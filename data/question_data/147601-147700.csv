,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How prove this inequality $b-a\ge \pi$,How prove this inequality,b-a\ge \pi,"let postive function $f(x)$ have two derivative on $(a,b),b>a$,and such   $$f''(x)+f(x)\ge 0,x\in(a,b)$$   and $f(a)=f(b)=0$,  show that $$b-a\ge \pi$$ if $$f''(x)+f(x)=0$$ then $$f(x)=C_{1}\cos{x}+C_{2}\sin{x}$$ but this problem is inequality,so then I  consider $$F(x)=f'^2(x)+f^2(x)\Longrightarrow F'(x)=2f'(x)f''(x)+2f(x)f'(x)=2f'(x)[f''(x)+f(x)]$$ But seem this also not solve this problem","let postive function $f(x)$ have two derivative on $(a,b),b>a$,and such   $$f''(x)+f(x)\ge 0,x\in(a,b)$$   and $f(a)=f(b)=0$,  show that $$b-a\ge \pi$$ if $$f''(x)+f(x)=0$$ then $$f(x)=C_{1}\cos{x}+C_{2}\sin{x}$$ but this problem is inequality,so then I  consider $$F(x)=f'^2(x)+f^2(x)\Longrightarrow F'(x)=2f'(x)f''(x)+2f(x)f'(x)=2f'(x)[f''(x)+f(x)]$$ But seem this also not solve this problem",,['analysis']
1,How find this limit $\lim_{n\to\infty}\left(\sqrt{n}\int_{0}^{1}(e^x(1-x))^ndx\right)$,How find this limit,\lim_{n\to\infty}\left(\sqrt{n}\int_{0}^{1}(e^x(1-x))^ndx\right),Question: Find this limit $$\lim_{n\to\infty}\left(\sqrt{n}\int_{0}^{1}(e^x(1-x))^ndx\right)$$ my idea: since $$e^{x}=\sum_{k=0}^{\infty}\frac{x^k}{k!}$$ so $$(1-x)e^x=\sum_{k=0}^{\infty}\dfrac{(1-x)x^k}{k!}$$ so $$\lim_{n\to\infty}\left(\sqrt{n}\int_{0}^{1}\sum_{k=0}^{\infty}\dfrac{x^k(1-x)}{k!}dx\right)$$ then I fell very ugly,Question: Find this limit $$\lim_{n\to\infty}\left(\sqrt{n}\int_{0}^{1}(e^x(1-x))^ndx\right)$$ my idea: since $$e^{x}=\sum_{k=0}^{\infty}\frac{x^k}{k!}$$ so $$(1-x)e^x=\sum_{k=0}^{\infty}\dfrac{(1-x)x^k}{k!}$$ so $$\lim_{n\to\infty}\left(\sqrt{n}\int_{0}^{1}\sum_{k=0}^{\infty}\dfrac{x^k(1-x)}{k!}dx\right)$$ then I fell very ugly,,['analysis']
2,BMO functions are $L^p$ Loc for all $1<p<\infty$,BMO functions are  Loc for all,L^p 1<p<\infty,"In order to motivate my question, I'd like to remember that if $\Omega$ is a bounded domain and $f \in L^q(\Omega)$ for some $q>1$, by Hölder inequality $f \in L^p(\Omega)$ for $p \in (1,q]$ with \begin{equation} \|f\|_{L^p(\Omega)} \le |\Omega|^{(1/p-1/q)} \|f\|_{L^q(\Omega)} \end{equation} Also, we easily see that if $f \in L^\infty(\Omega)$ then $f \in BMO(\Omega)$ with \begin{equation} \|f\|_{BMO(\Omega)} \le 2 \|f\|_{L^\infty(\Omega)} \end{equation} Where the BMO space can be seen here , where we also can see that if $f \in BMO(\Omega)$ then $f \in L^p_{Loc}(\Omega)$ for all $1<p<\infty$. But I can't see any proof of this fact. I'd like to see one. Better yet, I'd like to know if is it possible to bound the  $L^p$ Loc norm of $f$ by the $BMO$ norm of f. That is, is there a constant $C$ such that \begin{equation} \|f\|_{L^p_{Loc}(\Omega)} \le C\|f\|_{BMO(\Omega)} ? \end{equation} A reference is also valid. Thank you.","In order to motivate my question, I'd like to remember that if $\Omega$ is a bounded domain and $f \in L^q(\Omega)$ for some $q>1$, by Hölder inequality $f \in L^p(\Omega)$ for $p \in (1,q]$ with \begin{equation} \|f\|_{L^p(\Omega)} \le |\Omega|^{(1/p-1/q)} \|f\|_{L^q(\Omega)} \end{equation} Also, we easily see that if $f \in L^\infty(\Omega)$ then $f \in BMO(\Omega)$ with \begin{equation} \|f\|_{BMO(\Omega)} \le 2 \|f\|_{L^\infty(\Omega)} \end{equation} Where the BMO space can be seen here , where we also can see that if $f \in BMO(\Omega)$ then $f \in L^p_{Loc}(\Omega)$ for all $1<p<\infty$. But I can't see any proof of this fact. I'd like to see one. Better yet, I'd like to know if is it possible to bound the  $L^p$ Loc norm of $f$ by the $BMO$ norm of f. That is, is there a constant $C$ such that \begin{equation} \|f\|_{L^p_{Loc}(\Omega)} \le C\|f\|_{BMO(\Omega)} ? \end{equation} A reference is also valid. Thank you.",,"['analysis', 'reference-request', 'lp-spaces']"
3,theorem 1 chapter 2 - Evans PDE,theorem 1 chapter 2 - Evans PDE,,"My doubt is about the proof of the theorem 1  section 2.2.1  of the evans pde classic book. My doubt: Consider the function $$\Phi(x) = \begin{cases} - \frac{1}{2 \pi} \log |x| & \text{if $n= 2$} \\ \frac{1}{n(n-2)\alpha (n)} \frac{1}{|x|^{n-2}} &\text{if $n \geq 3$} \end{cases},$$ where $x \neq 0$ ( $x \in \Bbb R^n$ ). Consider $$u(x) = \int_{\Bbb R^n} \Phi (x-y) f(y) \, dy, $$ where $ f \in C^{2}_{c} (\Bbb R^n)$ . I am trying to prove that $$\frac{\partial u }{\partial x_i} (x) = \int_{R^n} \Phi (y) \frac{\partial f}{ \partial x_i} (x-y) \, dy.$$ My book does this: We have $$u(x) = \int_{\Bbb R^n} \Phi (x-y) f(y) \, dy  = \int_{\Bbb R^n} \Phi (y) f(x- y) \ dy.$$ Then for $h \in \Bbb R \setminus \{ 0 \}$ we have $$ \frac{u(x + h e_i) - u(x)}{h} = \int_{\Bbb R^n} \Phi (y)\: \frac{f(x  - y + h e_i) - f(x -y)}{h} \, dy$$ Evans says $$\frac{f(x + he_i - y ) - f (x-y)}{h} \to \frac{\partial f}{\partial x_i} (x-y)$$ uniformly as $h \to 0$ , and thus $$\frac{\partial u}{\partial  x_i} (x) = \int_{\Bbb R^n} \Phi (y) \frac{\partial f}{\partial x_i} (x-y) \, dy. \tag{*}$$ The only thing that I don't understand is the line $(*)$ . The uniform convergence I proved. Someone can help me to prove the line $(*)$ ? Thanks in advance","My doubt is about the proof of the theorem 1  section 2.2.1  of the evans pde classic book. My doubt: Consider the function where ( ). Consider where . I am trying to prove that My book does this: We have Then for we have Evans says uniformly as , and thus The only thing that I don't understand is the line . The uniform convergence I proved. Someone can help me to prove the line ? Thanks in advance","\Phi(x) = \begin{cases}
- \frac{1}{2 \pi} \log |x| & \text{if n= 2} \\
\frac{1}{n(n-2)\alpha (n)} \frac{1}{|x|^{n-2}} &\text{if n \geq 3}
\end{cases}, x \neq 0 x \in \Bbb R^n u(x) = \int_{\Bbb R^n} \Phi (x-y) f(y) \, dy,   f \in C^{2}_{c} (\Bbb R^n) \frac{\partial u }{\partial x_i} (x) = \int_{R^n} \Phi (y) \frac{\partial f}{ \partial x_i} (x-y) \, dy. u(x) = \int_{\Bbb R^n} \Phi (x-y) f(y) \, dy  = \int_{\Bbb R^n} \Phi (y) f(x- y) \ dy. h \in \Bbb R \setminus \{ 0 \}  \frac{u(x + h e_i) - u(x)}{h} = \int_{\Bbb R^n} \Phi (y)\: \frac{f(x  - y + h e_i) - f(x -y)}{h} \, dy \frac{f(x + he_i - y ) - f (x-y)}{h} \to \frac{\partial f}{\partial x_i} (x-y) h \to 0 \frac{\partial u}{\partial  x_i} (x) = \int_{\Bbb R^n} \Phi (y) \frac{\partial f}{\partial x_i} (x-y) \, dy. \tag{*} (*) (*)","['analysis', 'partial-differential-equations']"
4,Proving principle of the Iterated Suprema,Proving principle of the Iterated Suprema,,"Let $X$ and $Y$ be nonempty sets and let $h : X\times Y \to R$ have bounded range in $\mathbb{R}$. Let $F: X \to\mathbb{R}$ and $G : y \to \mathbb{R}$ be defined by $F(x):=\sup\{h(x,y) : y\in Y\}$, and $G(y) := \sup\{h(x,y) : x\in X\}$. Establish the Principle of the Iterated Suprema: $\sup\{h(x,y) :x \in X, y \in Y\} = \sup\{F(x) : x \in X\} = \sup\{G(y) : y \in Y\}$. This proof is quite long but im having trouble with it. Here is what I have so far. Proof Since $h(x,y)$ is bounded we know that $\sup\{h(x,y) :x \in X, y \in Y\}$ exists. Let $T=\sup\{h(x,y) :x \in X, y \in Y\}$. By definition we know that $T \geq h(x_0,y_0)$ for arbitrary $x_0 \in X$ and $ y_0 \in Y$ Since $h(x_0,y_0) \in\{h(x,y) :x \in X, y \in Y\}$ and $h(x_0,y_0) \in \{h(x_0,y) : y \in Y\}$ Since $x_0$ and $y_0$ are arbitrary it follows that T is a upper bound for $F(x_0)= \sup\{h(x_0,y) : y\in Y\}$. $\sup\{F(x) : x \in X\}$ exists since $F(x_0)$ was arbitrary. Thus $T \geq \sup\{F(x) : x \in X\}$ This is where I stopped at since I do not know if what I have done so far is correct and I am stuck proving $T \leq \sup\{F(x) : x \in X\}$ in order to show $T=\sup\{F(x) : x \in X\}$.","Let $X$ and $Y$ be nonempty sets and let $h : X\times Y \to R$ have bounded range in $\mathbb{R}$. Let $F: X \to\mathbb{R}$ and $G : y \to \mathbb{R}$ be defined by $F(x):=\sup\{h(x,y) : y\in Y\}$, and $G(y) := \sup\{h(x,y) : x\in X\}$. Establish the Principle of the Iterated Suprema: $\sup\{h(x,y) :x \in X, y \in Y\} = \sup\{F(x) : x \in X\} = \sup\{G(y) : y \in Y\}$. This proof is quite long but im having trouble with it. Here is what I have so far. Proof Since $h(x,y)$ is bounded we know that $\sup\{h(x,y) :x \in X, y \in Y\}$ exists. Let $T=\sup\{h(x,y) :x \in X, y \in Y\}$. By definition we know that $T \geq h(x_0,y_0)$ for arbitrary $x_0 \in X$ and $ y_0 \in Y$ Since $h(x_0,y_0) \in\{h(x,y) :x \in X, y \in Y\}$ and $h(x_0,y_0) \in \{h(x_0,y) : y \in Y\}$ Since $x_0$ and $y_0$ are arbitrary it follows that T is a upper bound for $F(x_0)= \sup\{h(x_0,y) : y\in Y\}$. $\sup\{F(x) : x \in X\}$ exists since $F(x_0)$ was arbitrary. Thus $T \geq \sup\{F(x) : x \in X\}$ This is where I stopped at since I do not know if what I have done so far is correct and I am stuck proving $T \leq \sup\{F(x) : x \in X\}$ in order to show $T=\sup\{F(x) : x \in X\}$.",,['analysis']
5,"$\Delta u = \operatorname{div}f \ \ \mbox{in} \ \ B_1, f \in L^2 \Rightarrow \nabla u \in L^2$",,"\Delta u = \operatorname{div}f \ \ \mbox{in} \ \ B_1, f \in L^2 \Rightarrow \nabla u \in L^2","I'm looking for results like, If $f \in L^p$  and  $$  \begin{array}{rclcl} \Delta u & = & \operatorname{div}f & \mbox{in} &  B_1\\ u&=&0& \mbox{on}& \partial B_1 \end{array} $$ then  $$ \int_{B_1} \!\left| \nabla u \right|^2 \le \int_{B_1}\!\left|\,f\right|^2 .$$ This is, $f \in L^2 \Rightarrow \nabla u \in L^2$. If  you know more generally, $f \in L^q \Rightarrow \nabla u \in L^q$. Is great if you can write the solution. If no, reference is good, preferable for non general cases of second order elliptic equations. But if you know only references for general case, I'm very grateful too.","I'm looking for results like, If $f \in L^p$  and  $$  \begin{array}{rclcl} \Delta u & = & \operatorname{div}f & \mbox{in} &  B_1\\ u&=&0& \mbox{on}& \partial B_1 \end{array} $$ then  $$ \int_{B_1} \!\left| \nabla u \right|^2 \le \int_{B_1}\!\left|\,f\right|^2 .$$ This is, $f \in L^2 \Rightarrow \nabla u \in L^2$. If  you know more generally, $f \in L^q \Rightarrow \nabla u \in L^q$. Is great if you can write the solution. If no, reference is good, preferable for non general cases of second order elliptic equations. But if you know only references for general case, I'm very grateful too.",,"['analysis', 'reference-request', 'partial-differential-equations', 'sobolev-spaces']"
6,Smoothening viscosity solution to classical solution,Smoothening viscosity solution to classical solution,,"Let $A$ be a linear second order differential operator with constant coefficients defined on real-valued functions of one-variable. Suppose that we have that for an upper-semicontinuous function $u:\mathbb{R}\to\mathbb{R}$ $$Au \leq 0\mbox{ holds in the viscosity sense.}$$ Let $u^{\epsilon}(x) = \int_{-1}^1u(x-\epsilon y)\varphi(y)\mbox{dy}$, where $\varphi$ is the standard mollifier. I would like to say that $$Au^{\epsilon}\leq 0\mbox{ in the classical sense}.$$ I am convinced that this must be true in this special and simple case. I have tried a few approaches, but as is often the case with viscosity theory, the devil is in the details and I cannot seem to write something convincing. Is anyone familiar with a general theorem along the lines of ``mollification preserves sub-solutions of linear const. coeff. pde''? Attempt at a solution: We suppose the contrary, that at some point $x_0,$ we have $Au^{\epsilon}(x_0) > 0.$ By continuity, we can extend this further to strict positivity on some open neighborhood $N$ of $x_0.$ By upper semi-continuity, I know that $u-u^{\epsilon}$ will achieve a maximum on closure  $\overline{N}$. If it is a local maximum, that is great, I'm done. However, if it is not, I am looking to construct another test function, say $\psi$ such that $A\psi \geq 0$ and $u - u^{\epsilon} - \psi$ achieves a local maximum inside $N$. Clearly, $\psi$ needs to depend somehow on the convergence of $u^{\epsilon}$ to $u$. But how to construct? Edit: problem statement has been amended to say constant coefficients. Thanks Willie Wong for pointing that out.","Let $A$ be a linear second order differential operator with constant coefficients defined on real-valued functions of one-variable. Suppose that we have that for an upper-semicontinuous function $u:\mathbb{R}\to\mathbb{R}$ $$Au \leq 0\mbox{ holds in the viscosity sense.}$$ Let $u^{\epsilon}(x) = \int_{-1}^1u(x-\epsilon y)\varphi(y)\mbox{dy}$, where $\varphi$ is the standard mollifier. I would like to say that $$Au^{\epsilon}\leq 0\mbox{ in the classical sense}.$$ I am convinced that this must be true in this special and simple case. I have tried a few approaches, but as is often the case with viscosity theory, the devil is in the details and I cannot seem to write something convincing. Is anyone familiar with a general theorem along the lines of ``mollification preserves sub-solutions of linear const. coeff. pde''? Attempt at a solution: We suppose the contrary, that at some point $x_0,$ we have $Au^{\epsilon}(x_0) > 0.$ By continuity, we can extend this further to strict positivity on some open neighborhood $N$ of $x_0.$ By upper semi-continuity, I know that $u-u^{\epsilon}$ will achieve a maximum on closure  $\overline{N}$. If it is a local maximum, that is great, I'm done. However, if it is not, I am looking to construct another test function, say $\psi$ such that $A\psi \geq 0$ and $u - u^{\epsilon} - \psi$ achieves a local maximum inside $N$. Clearly, $\psi$ needs to depend somehow on the convergence of $u^{\epsilon}$ to $u$. But how to construct? Edit: problem statement has been amended to say constant coefficients. Thanks Willie Wong for pointing that out.",,"['analysis', 'partial-differential-equations']"
7,Contraction Mapping question,Contraction Mapping question,,"Let X be the set of continuous real valued functions defined on $[0,\frac{1}{2}]$ with the metric $d(f,g):=\sup_{x\in[0,\frac{1}{2}]} |f(x)-g(x)|$. Define the map $\theta:X\rightarrow X$ such that $$\theta (f)(x)=\int_{0}^{x} \frac{1}{1+f(t)^2} dt$$. I need to show that $\theta$ is a contraction mapping and that the unique fixed point satisfies $f(0)=0$ and the differential equation $\frac{df}{dx}=\frac{1}{1+f(x)^2}$ So I'm pretty lost on this, I'm quite comfortable proving that things like $f(x)=1+\frac{1}{1+x^4}$ are contraction mapping but I'm a bit confused with this, so $$d(\theta(f),\theta(g))=\sup|\int_{0}^{x} \frac{1}{1+f(t)^2}-\frac{1}{1+g(t)^2} dt|$$ and so this is: $$=\sup|\int_{0}^{x} \frac{(g(t)-f(t))(g(t)+f(t))}{(1+f(t)^2)(1+g(t)^2)} dt|$$ but I am unsure where to go from here, I know that I need to get this to be something like: $$\leq\alpha\sup|f(x)-g(x)|$$ where $\alpha$ is the contraction constant? Thanks very much for any help","Let X be the set of continuous real valued functions defined on $[0,\frac{1}{2}]$ with the metric $d(f,g):=\sup_{x\in[0,\frac{1}{2}]} |f(x)-g(x)|$. Define the map $\theta:X\rightarrow X$ such that $$\theta (f)(x)=\int_{0}^{x} \frac{1}{1+f(t)^2} dt$$. I need to show that $\theta$ is a contraction mapping and that the unique fixed point satisfies $f(0)=0$ and the differential equation $\frac{df}{dx}=\frac{1}{1+f(x)^2}$ So I'm pretty lost on this, I'm quite comfortable proving that things like $f(x)=1+\frac{1}{1+x^4}$ are contraction mapping but I'm a bit confused with this, so $$d(\theta(f),\theta(g))=\sup|\int_{0}^{x} \frac{1}{1+f(t)^2}-\frac{1}{1+g(t)^2} dt|$$ and so this is: $$=\sup|\int_{0}^{x} \frac{(g(t)-f(t))(g(t)+f(t))}{(1+f(t)^2)(1+g(t)^2)} dt|$$ but I am unsure where to go from here, I know that I need to get this to be something like: $$\leq\alpha\sup|f(x)-g(x)|$$ where $\alpha$ is the contraction constant? Thanks very much for any help",,"['analysis', 'metric-spaces']"
8,"$f\in C[0,\infty]$ and $\lim\limits_{x\to \infty}f(x)=L<\infty$. Compute $\lim\limits_{n\to \infty} \int_{0}^{2} f(nx)dx$",and . Compute,"f\in C[0,\infty] \lim\limits_{x\to \infty}f(x)=L<\infty \lim\limits_{n\to \infty} \int_{0}^{2} f(nx)dx","I'd really love your help with this Let $f$ be a continuous function in $[0,\infty )$  and assume that $\lim\limits_{x\to \infty} f(x)=L<\infty$. I need to compute: $$\lim_{n\to \infty} \int_{0}^{2} f(nx)dx.$$ Because of the fact that $f$ is continuous I want to insert the $\lim$ into the integral (Am I allowed to? What conditions?), and basically to write : $\lim\limits_{n\to \infty} \int_{0}^{2} f(nx)dx=\int_{0}^{2}\lim\limits_{n\to \infty}  f(nx)dx=\int_{0}^{2}\ L  dx=2L$. I'm quite sure that it's wrong. What are my mistakes, and why am I not allowed to do so? what is the correct solution? Thanks a lot!","I'd really love your help with this Let $f$ be a continuous function in $[0,\infty )$  and assume that $\lim\limits_{x\to \infty} f(x)=L<\infty$. I need to compute: $$\lim_{n\to \infty} \int_{0}^{2} f(nx)dx.$$ Because of the fact that $f$ is continuous I want to insert the $\lim$ into the integral (Am I allowed to? What conditions?), and basically to write : $\lim\limits_{n\to \infty} \int_{0}^{2} f(nx)dx=\int_{0}^{2}\lim\limits_{n\to \infty}  f(nx)dx=\int_{0}^{2}\ L  dx=2L$. I'm quite sure that it's wrong. What are my mistakes, and why am I not allowed to do so? what is the correct solution? Thanks a lot!",,['analysis']
9,Proving an inequality with $\|x\|_p$ metrics?,Proving an inequality with  metrics?,\|x\|_p,"Let $1 \leq p < q \leq \infty$ and $x \in\mathbb{R}^n$. Show that $\|x\|_q \leq \|x\|_p \leq n^\frac{1}{p}\|x\|_q$, where $\|x\|_p$ is the metric $\left(\sum_{j=1}^n{|x_j|^p}\right)^\frac{1}{p}$. A hint is given: ""For the left-hand inequality do first the case where $\|x\|_p = 1$, and for the right-hand inequality do first the case $\|x\|_q = 1$."" So first I set $\|x\|_p = 1$ and got $\sum_{k = 1}^n|x_k|^q \leq 1 \leq n^\frac{q}{p}\sum_{k = 1}^n|x_k|^q$. This makes sense since $\frac{q}{p} > 1$. I left this and used the other half of the hint; I considered the inequality again and set $\|x\|_q = 1$ to obtain $1 \leq \sum_{k = 1}^n|x_k|^p \leq n$. I'm not quite as sure what this inequality means. My question is, how to extend these two inequalities to the more general cases where $\|x\|_p, \|x\|_q \neq 1$? And how can I make my two results relate to each other, since they seem to be two completely different cases? Perhaps this question is simpler than I am making it out to be and I might not be wording myself clearly but once I understand the general principle behind inequalities like this I will be able to do more complex ones on my own.","Let $1 \leq p < q \leq \infty$ and $x \in\mathbb{R}^n$. Show that $\|x\|_q \leq \|x\|_p \leq n^\frac{1}{p}\|x\|_q$, where $\|x\|_p$ is the metric $\left(\sum_{j=1}^n{|x_j|^p}\right)^\frac{1}{p}$. A hint is given: ""For the left-hand inequality do first the case where $\|x\|_p = 1$, and for the right-hand inequality do first the case $\|x\|_q = 1$."" So first I set $\|x\|_p = 1$ and got $\sum_{k = 1}^n|x_k|^q \leq 1 \leq n^\frac{q}{p}\sum_{k = 1}^n|x_k|^q$. This makes sense since $\frac{q}{p} > 1$. I left this and used the other half of the hint; I considered the inequality again and set $\|x\|_q = 1$ to obtain $1 \leq \sum_{k = 1}^n|x_k|^p \leq n$. I'm not quite as sure what this inequality means. My question is, how to extend these two inequalities to the more general cases where $\|x\|_p, \|x\|_q \neq 1$? And how can I make my two results relate to each other, since they seem to be two completely different cases? Perhaps this question is simpler than I am making it out to be and I might not be wording myself clearly but once I understand the general principle behind inequalities like this I will be able to do more complex ones on my own.",,"['analysis', 'metric-spaces']"
10,Estimating $\#\{\{\alpha k\} < 1/\sqrt{k} : k \leq n\}$ for irrational $\alpha$,Estimating  for irrational,\#\{\{\alpha k\} < 1/\sqrt{k} : k \leq n\} \alpha,"Suppose $\{\alpha n\}$ is the fractional part of $\alpha n$. Put $$A_{\alpha}(n) = \#\{\{\alpha k\} < 1/\sqrt{k} : k \leq n\}.$$ If $\alpha$ is irrational, can I find some constant $K$ such that $A_{\alpha}(n) < K \sqrt{n}$ for all $n$? Does the order of $A_{\alpha}(n)$ depend on $\alpha$? Suppose $1/\sqrt{k}$ is replaced by some function $f(k)$. What can I say about the number of $\{\alpha n\}$ less than $f(n)$ as $n$ tends to infinity?","Suppose $\{\alpha n\}$ is the fractional part of $\alpha n$. Put $$A_{\alpha}(n) = \#\{\{\alpha k\} < 1/\sqrt{k} : k \leq n\}.$$ If $\alpha$ is irrational, can I find some constant $K$ such that $A_{\alpha}(n) < K \sqrt{n}$ for all $n$? Does the order of $A_{\alpha}(n)$ depend on $\alpha$? Suppose $1/\sqrt{k}$ is replaced by some function $f(k)$. What can I say about the number of $\{\alpha n\}$ less than $f(n)$ as $n$ tends to infinity?",,"['analysis', 'irrational-numbers']"
11,"$\sum_{i,j=1}^nx_ix_j\frac{\partial^2f}{\partial x_i\partial x_j}=0$ and $\nabla f(0)=0$ implies constancy of $f$ in $B_1(0)$",and  implies constancy of  in,"\sum_{i,j=1}^nx_ix_j\frac{\partial^2f}{\partial x_i\partial x_j}=0 \nabla f(0)=0 f B_1(0)","Let $B_1(0)$ be the unit ball in $\mathbb R^n$ centered at the origin. Assume that the function $f\in  C^2(B_1(0))$ . Prove that $1)$ If $f$ satisfies $$\sum_{i,j=1}^nx_ix_j\frac{\partial^2f}{\partial x_i\partial x_j}=0$$ on $B_1(0)$ , and $\nabla f(0) = 0$ , then $f$ is constant in $B_1(0)$ . $2)$ If $f$ satisfies $$x_i\frac{\partial f}{\partial x_j}-x_j\frac{\partial f}{\partial x_i}=0,i,j=1,\cdots,n$$ on $B_1(0)$ ,then $f$ is constant on the sphere $\{x:x\in B_1(0),\vert x\vert=\frac{1}{2}\}$ . My intuition for the first problem is that if letting $L=\sum_{i=1}^{n}x_i\frac{\partial}{\partial x_i}$ , then we have $L^2f=Lf$ . But I don't know if this will proceed next, I think this maybe associated with the Hessian matrix or the maximum principle, but what property of Hessian matrix should I use here? Also this seems not the exactly the form of maximum principle so I don't know how to extend it. I don't know how to use the condition to proceed. Are there any solutions or suggestions of proceeding? Thanks in advance!","Let be the unit ball in centered at the origin. Assume that the function . Prove that If satisfies on , and , then is constant in . If satisfies on ,then is constant on the sphere . My intuition for the first problem is that if letting , then we have . But I don't know if this will proceed next, I think this maybe associated with the Hessian matrix or the maximum principle, but what property of Hessian matrix should I use here? Also this seems not the exactly the form of maximum principle so I don't know how to extend it. I don't know how to use the condition to proceed. Are there any solutions or suggestions of proceeding? Thanks in advance!","B_1(0) \mathbb R^n f\in  C^2(B_1(0)) 1) f \sum_{i,j=1}^nx_ix_j\frac{\partial^2f}{\partial x_i\partial x_j}=0 B_1(0) \nabla f(0) = 0 f B_1(0) 2) f x_i\frac{\partial f}{\partial x_j}-x_j\frac{\partial f}{\partial x_i}=0,i,j=1,\cdots,n B_1(0) f \{x:x\in B_1(0),\vert x\vert=\frac{1}{2}\} L=\sum_{i=1}^{n}x_i\frac{\partial}{\partial x_i} L^2f=Lf","['analysis', 'derivatives', 'partial-derivative', 'hessian-matrix']"
12,Expressing the $n-$th derivative of $f(x)=\frac{3x^2-6x+5}{x^3-5x^2+9x-5}$,Expressing the th derivative of,n- f(x)=\frac{3x^2-6x+5}{x^3-5x^2+9x-5},"In an attempt to express the $n-$ th derivative of the rational function $f(x)=\frac{3x^2-6x+5}{x^3-5x^2+9x-5}$ , I split it into $\left( \frac{1+2i}{(x-2-i)} + \frac{1-2i}{(x-2+i)} + \frac{1}{(x-1)} \right)$ , then using $y= \frac{1}{ax+b} \Rightarrow y_{n}=\frac{(-1)^n n! a^n}{(ax+b)^{n+1}}$ I ended up with: $$(-1)^n n! [(1+2i)(x-2-i)^{-n-1}+(1-2i)(x-2+i)^{-n-1} +(x-1)^{-n-1}]$$ Now I don’t know how to get back to $\mathbb{\mathbb{R}}$ from $\mathbb{\mathbb{C}}$ . What I'd like to get is a form that doesn't make use of trigonometric functions but uses the binomial theorem. Can anyone help me achieve this? Thank you so much in advance.","In an attempt to express the th derivative of the rational function , I split it into , then using I ended up with: Now I don’t know how to get back to from . What I'd like to get is a form that doesn't make use of trigonometric functions but uses the binomial theorem. Can anyone help me achieve this? Thank you so much in advance.",n- f(x)=\frac{3x^2-6x+5}{x^3-5x^2+9x-5} \left( \frac{1+2i}{(x-2-i)} + \frac{1-2i}{(x-2+i)} + \frac{1}{(x-1)} \right) y= \frac{1}{ax+b} \Rightarrow y_{n}=\frac{(-1)^n n! a^n}{(ax+b)^{n+1}} (-1)^n n! [(1+2i)(x-2-i)^{-n-1}+(1-2i)(x-2+i)^{-n-1} +(x-1)^{-n-1}] \mathbb{\mathbb{R}} \mathbb{\mathbb{C}},"['analysis', 'functions', 'derivatives', 'complex-numbers', 'binomial-theorem']"
13,Derivatives with rational numbers,Derivatives with rational numbers,,"A definite integral written entirely in terms of rational numbers and functions of rational numbers does not always equal a rational number. For example, with the function $1/x$ , $$ \int_1^2 \frac{dx}{x}  = \ln 2$$ I am curious whether or not this is also true for derivatives. Given a function $f$ for which the image of $\mathbb Q$ is a subset of $\mathbb Q$ , is the image of $\mathbb Q$ under $f'$ always a subset of $\mathbb Q$ ? I have verified that this is true for every function written as a combination of arithmetic operations, but I have not verified it for all rational functions.","A definite integral written entirely in terms of rational numbers and functions of rational numbers does not always equal a rational number. For example, with the function , I am curious whether or not this is also true for derivatives. Given a function for which the image of is a subset of , is the image of under always a subset of ? I have verified that this is true for every function written as a combination of arithmetic operations, but I have not verified it for all rational functions.","1/x 
\int_1^2 \frac{dx}{x}  = \ln 2 f \mathbb Q \mathbb Q \mathbb Q f' \mathbb Q","['analysis', 'rational-numbers']"
14,If $f^2(t) \le 1+2\int_0^tf(s)\mathrm{d}s$ prove $f(t)\le 1+t$,If  prove,f^2(t) \le 1+2\int_0^tf(s)\mathrm{d}s f(t)\le 1+t,"If $f(x)$ is positive and continuous on $[0,1]$ and $f^2(t) \le 1+2\int_0^tf(s)\mathrm{d}s$ , prove that $f(t)\le 1+t$ . Here's my thinking. $$f^2(t) \le 1+2\int_0^tf(s)\mathrm{d}s \Rightarrow f(t)\le \sqrt{1+2\int_0^tf(s)\mathrm{d}s} $$ $$\Rightarrow \int_0^t\frac{f(t)}{\sqrt{1+2\int_0^tf(s)\mathrm{d}s}}\mathrm{d}t = \sqrt{1+2\int_0^tf(s)\mathrm{d}s}-1 \le t$$ $$ \Rightarrow 2\int_0^tf(s)\mathrm{d}s\le t^2+2t$$ This comes from the integration of two sides of $f(t)\le 1+t$ . But I don't know what to do next.","If is positive and continuous on and , prove that . Here's my thinking. This comes from the integration of two sides of . But I don't know what to do next.","f(x) [0,1] f^2(t) \le 1+2\int_0^tf(s)\mathrm{d}s f(t)\le 1+t f^2(t) \le 1+2\int_0^tf(s)\mathrm{d}s \Rightarrow f(t)\le \sqrt{1+2\int_0^tf(s)\mathrm{d}s}  \Rightarrow \int_0^t\frac{f(t)}{\sqrt{1+2\int_0^tf(s)\mathrm{d}s}}\mathrm{d}t = \sqrt{1+2\int_0^tf(s)\mathrm{d}s}-1 \le t  \Rightarrow 2\int_0^tf(s)\mathrm{d}s\le t^2+2t f(t)\le 1+t","['analysis', 'inequality', 'integral-inequality']"
15,Weierstrass approximation theorem. Approximation of |x|.,Weierstrass approximation theorem. Approximation of |x|.,,"I'm studying a proof of the Weierstrass approximation theorem that requires an uniform approximation using polynomials of the function |x| in the interval $[-1,1]$ i.e. we need a sequence of polynomials that converge to |x| in the supreme norm. One way to do this is to define for $t\in[0,1]$ the following sequence: First set $P_0(t)=0$ for all t, and then using induction define $$P_{n+1}(t)=P_n(t)+\frac{1}{2}(t-P_n(t)^2).$$ The next step would be to prove using induction that for all $t\in[0,1]$, $$0\leq\sqrt{t}-P_n(t)\leq\frac{2\sqrt{t}}{2+n\sqrt{t}},$$ but i have not been able to do this. Can someone help me???","I'm studying a proof of the Weierstrass approximation theorem that requires an uniform approximation using polynomials of the function |x| in the interval $[-1,1]$ i.e. we need a sequence of polynomials that converge to |x| in the supreme norm. One way to do this is to define for $t\in[0,1]$ the following sequence: First set $P_0(t)=0$ for all t, and then using induction define $$P_{n+1}(t)=P_n(t)+\frac{1}{2}(t-P_n(t)^2).$$ The next step would be to prove using induction that for all $t\in[0,1]$, $$0\leq\sqrt{t}-P_n(t)\leq\frac{2\sqrt{t}}{2+n\sqrt{t}},$$ but i have not been able to do this. Can someone help me???",,"['analysis', 'absolute-value', 'approximation-theory', 'weierstrass-approximation']"
16,"Intuitively (or precisely), which functions can be approximated by straight lines?","Intuitively (or precisely), which functions can be approximated by straight lines?",,"I am trying to build up some intuition of what the various notions of analysis mean. By intuition of continuity is that it means the graph of the function is 'connected', but not necessarily in any nice way. This lead me to ask the question 'Given a function $f: \mathbb{R} \to \mathbb{R}$ and a point $x_0 \in \mathbb{R}$, what assumptions must we impose on $f$ so that we can say $f$ is 'approximately a line' in some neighbourhood of $x_0$?' To apply Taylor's theorem, we need that $f$ is twice continuously differentiable at $x_0$. However I am only interested in some tiny neighbourhood of $x_0$, so can we weaken these assumptions? We certainly can't weaken them all the way down to continuity at $x_0$; for example the function $g: \mathbb{R} \to \mathbb{R}$ defined by $$g(x) = \begin{cases} x \sin(\frac{1}{x}), \ x \neq 0 \\ 0, \qquad \quad \ x=0 \end{cases}$$ is continuous at $x_0 = 0$, but certainly cannot be approximated by a line there. Differentiability at $x_0$ also won't do; consider $h: \mathbb{R} \to \mathbb{R}$ given by $$h(x) = \begin{cases} x^2 \sin(\frac{1}{x}), \ x \neq 0 \\ 0, \qquad \quad \ \ \ x=0. \end{cases}$$ This is differentiable and hence continuous at $x_0 = 0$. Indeed it is differentiable everywhere, but it is not continuously differentiable at $0$; it's derivative is $h': \mathbb{R} \to \mathbb{R}$ given by $$h'(x) = \begin{cases} 2x\sin(\frac{1}{x}) - \cos(\frac{1}{x}), \ x \neq 0 \\ 0, \qquad \qquad \qquad \qquad x=0 \end{cases}$$ whose limit at $0$ doesn't exist. Going one step further, we consider $k: \mathbb{R} \to \mathbb{R}$ given by  $$k(x) = \begin{cases} x^3 \sin(\frac{1}{x}), \ x \neq 0 \\ 0, \qquad \quad \ \ \ x=0. \end{cases}$$ Again this is differentiable everywhere, but now it is continuously differentiable at $0$ (and so everywhere); it's derivative is $k': \mathbb{R} \to \mathbb{R}$ given by $$k'(x) = \begin{cases} 3x^2\sin(\frac{1}{x}) - x\cos(\frac{1}{x}), \ x \neq 0 \\ 0, \qquad \qquad \qquad \qquad \quad \ x=0 \end{cases}$$ which is continuous everywhere. However this function obviously still can't be approximated by a straight line near $0$ (since it takes on positive, negative, and zero values arbitrarily near the origin, and a straight line through the origin can only take on positive, negative, $\textit{or}$ zero values near the origin). At first I thought the problem was that the value of the derivatives of $g, h$ and $k$ around $x_0 = 0$ were unbounded, i.e. their 'slopes' grow without bound, however this is only the case for $g$; the derivative of $h$ near $0$ is approximately between $-1$ and $1$ and the derivative of $k$ near $0$ is approximately $0$. Then I thought the problem was that that the derivatives of $g, h,$ and $k$ had arbitrarily many sign changes near $x_0 = 0$, however this also can't be right. So now I'm just confused. Do we in fact need all the assumptions warranted by Taylor's theorem, i.e. twice continuously differentiable at $x_0$, to guarantee that a function is 'like a line' at a point? Do we actually need more hypotheses than Taylor's theorem in that we have to assume information about $f$ not at $x_0$, i.e. in some neighbourhood? Thanks in advance. I also wouldn't mind comments explaining your intuitions about related notions in analysis. tl;dr: what restrictions must we place on a function $f: \mathbb{R} \to \mathbb{R}$ so that when we 'zoom in close enough', it looks like a straight line?","I am trying to build up some intuition of what the various notions of analysis mean. By intuition of continuity is that it means the graph of the function is 'connected', but not necessarily in any nice way. This lead me to ask the question 'Given a function $f: \mathbb{R} \to \mathbb{R}$ and a point $x_0 \in \mathbb{R}$, what assumptions must we impose on $f$ so that we can say $f$ is 'approximately a line' in some neighbourhood of $x_0$?' To apply Taylor's theorem, we need that $f$ is twice continuously differentiable at $x_0$. However I am only interested in some tiny neighbourhood of $x_0$, so can we weaken these assumptions? We certainly can't weaken them all the way down to continuity at $x_0$; for example the function $g: \mathbb{R} \to \mathbb{R}$ defined by $$g(x) = \begin{cases} x \sin(\frac{1}{x}), \ x \neq 0 \\ 0, \qquad \quad \ x=0 \end{cases}$$ is continuous at $x_0 = 0$, but certainly cannot be approximated by a line there. Differentiability at $x_0$ also won't do; consider $h: \mathbb{R} \to \mathbb{R}$ given by $$h(x) = \begin{cases} x^2 \sin(\frac{1}{x}), \ x \neq 0 \\ 0, \qquad \quad \ \ \ x=0. \end{cases}$$ This is differentiable and hence continuous at $x_0 = 0$. Indeed it is differentiable everywhere, but it is not continuously differentiable at $0$; it's derivative is $h': \mathbb{R} \to \mathbb{R}$ given by $$h'(x) = \begin{cases} 2x\sin(\frac{1}{x}) - \cos(\frac{1}{x}), \ x \neq 0 \\ 0, \qquad \qquad \qquad \qquad x=0 \end{cases}$$ whose limit at $0$ doesn't exist. Going one step further, we consider $k: \mathbb{R} \to \mathbb{R}$ given by  $$k(x) = \begin{cases} x^3 \sin(\frac{1}{x}), \ x \neq 0 \\ 0, \qquad \quad \ \ \ x=0. \end{cases}$$ Again this is differentiable everywhere, but now it is continuously differentiable at $0$ (and so everywhere); it's derivative is $k': \mathbb{R} \to \mathbb{R}$ given by $$k'(x) = \begin{cases} 3x^2\sin(\frac{1}{x}) - x\cos(\frac{1}{x}), \ x \neq 0 \\ 0, \qquad \qquad \qquad \qquad \quad \ x=0 \end{cases}$$ which is continuous everywhere. However this function obviously still can't be approximated by a straight line near $0$ (since it takes on positive, negative, and zero values arbitrarily near the origin, and a straight line through the origin can only take on positive, negative, $\textit{or}$ zero values near the origin). At first I thought the problem was that the value of the derivatives of $g, h$ and $k$ around $x_0 = 0$ were unbounded, i.e. their 'slopes' grow without bound, however this is only the case for $g$; the derivative of $h$ near $0$ is approximately between $-1$ and $1$ and the derivative of $k$ near $0$ is approximately $0$. Then I thought the problem was that that the derivatives of $g, h,$ and $k$ had arbitrarily many sign changes near $x_0 = 0$, however this also can't be right. So now I'm just confused. Do we in fact need all the assumptions warranted by Taylor's theorem, i.e. twice continuously differentiable at $x_0$, to guarantee that a function is 'like a line' at a point? Do we actually need more hypotheses than Taylor's theorem in that we have to assume information about $f$ not at $x_0$, i.e. in some neighbourhood? Thanks in advance. I also wouldn't mind comments explaining your intuitions about related notions in analysis. tl;dr: what restrictions must we place on a function $f: \mathbb{R} \to \mathbb{R}$ so that when we 'zoom in close enough', it looks like a straight line?",,"['analysis', 'functions', 'derivatives', 'continuity', 'intuition']"
17,Prove that the set consisting of isolated points is finite.,Prove that the set consisting of isolated points is finite.,,1. Let A be a compact subset in $R^n$. Investigate whether the following assertion is true or not: If A consists of isolated points only then A is finite. I couldn't demonstrate my answer.We know that when A is a compact subset then it is closed and bounded. I was looking and I found that it resembles this question Is a closed subset of isolated points in a compact set necessarily finite? I am still not fully convinced.,1. Let A be a compact subset in $R^n$. Investigate whether the following assertion is true or not: If A consists of isolated points only then A is finite. I couldn't demonstrate my answer.We know that when A is a compact subset then it is closed and bounded. I was looking and I found that it resembles this question Is a closed subset of isolated points in a compact set necessarily finite? I am still not fully convinced.,,['analysis']
18,References for mathematical theory of summability of divergent series,References for mathematical theory of summability of divergent series,,"Once in a while, I can't help it to ask very broad questions. I have read (a portion of) Hardy's Divergent Series. Back then, I think besides in mathematics, divergent series and the need to assign values to them hadn't arisen. But nowadays, for example, almost all explanations of the 26 dimensions of Bosonic string theory deals with such an assignment. Also, the internet literally exploded on Youtube explaining these kinds of issues (they conveniently left out all the proofs). Questions: Are there (more modern) references for the mathematical theory of the summability of divergent series, and its philosophy (I.e. usage of such things in physics) ? I have seen divergent series of real numbers sum to a complex number with non-zero imaginary part. Is there something more to it (for example, are there known divergent series that lead to a quaternion) ? I am tagging this with meta, biglist and analysis, for obvious reasons.","Once in a while, I can't help it to ask very broad questions. I have read (a portion of) Hardy's Divergent Series. Back then, I think besides in mathematics, divergent series and the need to assign values to them hadn't arisen. But nowadays, for example, almost all explanations of the 26 dimensions of Bosonic string theory deals with such an assignment. Also, the internet literally exploded on Youtube explaining these kinds of issues (they conveniently left out all the proofs). Questions: Are there (more modern) references for the mathematical theory of the summability of divergent series, and its philosophy (I.e. usage of such things in physics) ? I have seen divergent series of real numbers sum to a complex number with non-zero imaginary part. Is there something more to it (for example, are there known divergent series that lead to a quaternion) ? I am tagging this with meta, biglist and analysis, for obvious reasons.",,"['analysis', 'reference-request', 'big-list', 'divergent-series']"
19,Estimating a finite sum of complex numbers (Rudin Lemma 6.3).,Estimating a finite sum of complex numbers (Rudin Lemma 6.3).,,"In Rudin's book, Real and Complex analysis, Lemma 6.3. states: If $z_1,...,z_N$ are complex numbers then, there is a subset $S$ of $\{1,...,N\}$ for which $$\left|\sum_{k\in S} z_k\right|\ge \frac{1}{\pi}\sum _{k=1}^N |z_k|.$$ In the proof he claims the following: Let $\theta_0$ be the value for which $$\sum _{k=1}^N |z_k|\cos^{+}(\alpha_k-\theta),$$ attains it's maximum. Therefore $$\sum _{k=1}^N |z_k|\cos^{+}(\alpha_k-\theta_0)\ge \frac{1}{2\pi}\int_{-\pi}^\pi\sum _{k=1}^N |z_k|\cos^{+}(\alpha_k-\eta)d\eta=\frac{1}{\pi}\sum _{k=1}^N |z_k|,\tag{1}$$ i.e. the maximum is bounded below by the avergae of the sum over $[-\pi,\pi]$. Could someone help me to understand why this is true. I can see, for example, that there is a constant $0<C<1$ such that inequality $(1)$ is true for $C$ instead of $\pi$, however, I fail to see why his claim is true. Moreover, is $1/\pi$ the best constant?","In Rudin's book, Real and Complex analysis, Lemma 6.3. states: If $z_1,...,z_N$ are complex numbers then, there is a subset $S$ of $\{1,...,N\}$ for which $$\left|\sum_{k\in S} z_k\right|\ge \frac{1}{\pi}\sum _{k=1}^N |z_k|.$$ In the proof he claims the following: Let $\theta_0$ be the value for which $$\sum _{k=1}^N |z_k|\cos^{+}(\alpha_k-\theta),$$ attains it's maximum. Therefore $$\sum _{k=1}^N |z_k|\cos^{+}(\alpha_k-\theta_0)\ge \frac{1}{2\pi}\int_{-\pi}^\pi\sum _{k=1}^N |z_k|\cos^{+}(\alpha_k-\eta)d\eta=\frac{1}{\pi}\sum _{k=1}^N |z_k|,\tag{1}$$ i.e. the maximum is bounded below by the avergae of the sum over $[-\pi,\pi]$. Could someone help me to understand why this is true. I can see, for example, that there is a constant $0<C<1$ such that inequality $(1)$ is true for $C$ instead of $\pi$, however, I fail to see why his claim is true. Moreover, is $1/\pi$ the best constant?",,['analysis']
20,"How prove $f(x)$ is monotonous , if $f'(x)=g[f(x)]$","How prove  is monotonous , if",f(x) f'(x)=g[f(x)],"Question: Let $f(x)$ be a derivative, and there exsit $g(x)$ be such that:   $$f'(x)=g[f(x)]$$   Show that $f(x)$ is monotonic. This problem is from Xie Hui Min analysis problems book in china ,and the  The author only give the hint: use contradiction. My idea: Assume there exist $x,y$ such that:  $$f(x)=f(y), x\neq y$$ Then: $$f'(x)=g(f(x))\Longrightarrow f'(y)=g(f(y))=g(f(x))=f'(x).$$ Then I can't continue. Thank you","Question: Let $f(x)$ be a derivative, and there exsit $g(x)$ be such that:   $$f'(x)=g[f(x)]$$   Show that $f(x)$ is monotonic. This problem is from Xie Hui Min analysis problems book in china ,and the  The author only give the hint: use contradiction. My idea: Assume there exist $x,y$ such that:  $$f(x)=f(y), x\neq y$$ Then: $$f'(x)=g(f(x))\Longrightarrow f'(y)=g(f(y))=g(f(x))=f'(x).$$ Then I can't continue. Thank you",,"['analysis', 'functions']"
21,Prove $ \left |\sin(x) - x + \frac{x^3}{3!} \right | < \frac{4}{15}$,Prove, \left |\sin(x) - x + \frac{x^3}{3!} \right | < \frac{4}{15},"Prove $ \left |\sin(x) - x + \dfrac{x^3}{3!} \right | < \dfrac{4}{15}$ $\forall x \in [-2,2]$ By Maclaurin's formula and Lagrange's remainder we have $\sin(x)  = x - \dfrac{x^3}{3!} + \dfrac{\sin(\xi)}{5!}x^5$ for some $0<\xi<2$ subbing this in we get $\left|\dfrac{\sin(\xi)}{5!}x^5 \right| \leq \left |\dfrac{x^5}{5!} \right| \leq \dfrac{2^5}{5!} = \dfrac{4}{15}$, but the question has $<$ rather than $\leq$ - where have I done wrong? edit: thinking the $\cos(\xi)$ should be there rather than $\sin(\xi)$","Prove $ \left |\sin(x) - x + \dfrac{x^3}{3!} \right | < \dfrac{4}{15}$ $\forall x \in [-2,2]$ By Maclaurin's formula and Lagrange's remainder we have $\sin(x)  = x - \dfrac{x^3}{3!} + \dfrac{\sin(\xi)}{5!}x^5$ for some $0<\xi<2$ subbing this in we get $\left|\dfrac{\sin(\xi)}{5!}x^5 \right| \leq \left |\dfrac{x^5}{5!} \right| \leq \dfrac{2^5}{5!} = \dfrac{4}{15}$, but the question has $<$ rather than $\leq$ - where have I done wrong? edit: thinking the $\cos(\xi)$ should be there rather than $\sin(\xi)$",,['analysis']
22,Conditions for Fubini's theorem,Conditions for Fubini's theorem,,"To preface this post, I have to admit that I have extremely little measure theory knowledge and I get lost when trying to read about Fubini's theorem for this reason. In the theorem statement for Fubini's theorem , it says that $$\int_{A\times B}|f(x,y)|d(x,y) < \infty.$$ I get that it is saying that $|f|$ - when integrated over the product measure - is finite but how does one go about checking that this is the case? Is it sufficient to show the following: $$\int_{A}\left(\int_{B}|f(x,y)|dy\right)dx < \infty$$ I found a resource (PDF warning) that talks about a corollary to Fubini's theorem that seems to suggest that this condition is sufficient (see Corollary 6.2.1 and the remark following it) however they restrict themselves to $\sigma$-finite measure spaces but the theorem statement on Wikipedia allows for general complete measure spaces. Is my assertion correct or am I way off the mark?","To preface this post, I have to admit that I have extremely little measure theory knowledge and I get lost when trying to read about Fubini's theorem for this reason. In the theorem statement for Fubini's theorem , it says that $$\int_{A\times B}|f(x,y)|d(x,y) < \infty.$$ I get that it is saying that $|f|$ - when integrated over the product measure - is finite but how does one go about checking that this is the case? Is it sufficient to show the following: $$\int_{A}\left(\int_{B}|f(x,y)|dy\right)dx < \infty$$ I found a resource (PDF warning) that talks about a corollary to Fubini's theorem that seems to suggest that this condition is sufficient (see Corollary 6.2.1 and the remark following it) however they restrict themselves to $\sigma$-finite measure spaces but the theorem statement on Wikipedia allows for general complete measure spaces. Is my assertion correct or am I way off the mark?",,"['analysis', 'measure-theory']"
23,Wedge product of differential and volume form,Wedge product of differential and volume form,,"Let $f(x)$ be a $C^1$ function defined on $\mathbb{R}^n$ and $\nabla f(x) \neq 0$ for any $x \in \mathbb{R}^n$. If $d\sigma$ is the volume form on hypersurface $f(x)=c$ induced from $\mathbb{R}^n$ then how to show the equality $$     df \wedge d\sigma = |\nabla f| \, dx^1 \wedge \ldots \wedge dx^n? $$ The main problem is that I don't know to what equal $d\sigma(X_p^1,\ldots,X_p^{n-1})$ for tangent vectors $X_p^1,\ldots,X_p^{n-1} \in T_p \mathbb{R}^n$. Any hint is welcome.","Let $f(x)$ be a $C^1$ function defined on $\mathbb{R}^n$ and $\nabla f(x) \neq 0$ for any $x \in \mathbb{R}^n$. If $d\sigma$ is the volume form on hypersurface $f(x)=c$ induced from $\mathbb{R}^n$ then how to show the equality $$     df \wedge d\sigma = |\nabla f| \, dx^1 \wedge \ldots \wedge dx^n? $$ The main problem is that I don't know to what equal $d\sigma(X_p^1,\ldots,X_p^{n-1})$ for tangent vectors $X_p^1,\ldots,X_p^{n-1} \in T_p \mathbb{R}^n$. Any hint is welcome.",,"['analysis', 'differential-forms']"
24,Maximum volume change for two sets with small Hausdorff metric in bounded part of $\mathbb{R}^n$,Maximum volume change for two sets with small Hausdorff metric in bounded part of,\mathbb{R}^n,"Given two subsets $S_1$, $S_2$ of a bounded part of $\mathbb{R}^n$, say $[-M,M]^n$. Is there a way to relate the difference in volume $vol(S_2)-vol(S_1)$ to the Hausdorff metric distance between the sets $S_1$ and $S_2$ given that $S_1 \subset S_2$? Intuitively I can see that the the fact that $S_1$ and $S_2$ lie in $[-M,M]^n$ is important since othwerwise one could just take the cirkels with radii $R$ and $R+\epsilon$. The Hausdorff metric between those sets is $\epsilon$ but as $R$ grows the difference in volume grows as well. I would suspect that in the settings above if $H(S_1,S_2)=m$, with $m$ small that the difference in volume must be smaller then some function of $m$. But I can't formulate anything general. Can anyone point me in the right direction or give a counter example?","Given two subsets $S_1$, $S_2$ of a bounded part of $\mathbb{R}^n$, say $[-M,M]^n$. Is there a way to relate the difference in volume $vol(S_2)-vol(S_1)$ to the Hausdorff metric distance between the sets $S_1$ and $S_2$ given that $S_1 \subset S_2$? Intuitively I can see that the the fact that $S_1$ and $S_2$ lie in $[-M,M]^n$ is important since othwerwise one could just take the cirkels with radii $R$ and $R+\epsilon$. The Hausdorff metric between those sets is $\epsilon$ but as $R$ grows the difference in volume grows as well. I would suspect that in the settings above if $H(S_1,S_2)=m$, with $m$ small that the difference in volume must be smaller then some function of $m$. But I can't formulate anything general. Can anyone point me in the right direction or give a counter example?",,"['analysis', 'metric-spaces', 'volume']"
25,Is there always a primitive m-th root of unity with imaginary part bigger than 1/2,Is there always a primitive m-th root of unity with imaginary part bigger than 1/2,,"Let $m$ be a positive integer. I need the existence of a primitive $m$-th root of unity $\zeta_m$ such that its imaginary part is strictly greater than $1/2$. We can write $\zeta_m = \exp(2\pi i a/m)$ for some $a$ coprime to $m$. The condition above boils down to $\sin(2 \pi a /m ) > 1/2$. This just means that $$ \frac{m}{12} <  a < \frac{5m}{12}.$$ So I'm looking for the existence of an integer $a$ coprime to $m$ such that $$ \frac{m}{12} < a < \frac{5m}{12}. $$ Is this always possible? Probably I need that $m> 12$. For $m=12$, there is no such $a$. It's ok if it doesn't work for a finite number of $m$.","Let $m$ be a positive integer. I need the existence of a primitive $m$-th root of unity $\zeta_m$ such that its imaginary part is strictly greater than $1/2$. We can write $\zeta_m = \exp(2\pi i a/m)$ for some $a$ coprime to $m$. The condition above boils down to $\sin(2 \pi a /m ) > 1/2$. This just means that $$ \frac{m}{12} <  a < \frac{5m}{12}.$$ So I'm looking for the existence of an integer $a$ coprime to $m$ such that $$ \frac{m}{12} < a < \frac{5m}{12}. $$ Is this always possible? Probably I need that $m> 12$. For $m=12$, there is no such $a$. It's ok if it doesn't work for a finite number of $m$.",,"['analysis', 'elementary-number-theory', 'modular-arithmetic']"
26,Deriving the Oversampling formula,Deriving the Oversampling formula,,"I am familiar with the Shanon Sampling theorem, which states that: Let $f \in L_1(\mathbb{R})$ and $supp(\mathcal{F}f) \subseteq [-B,B]$ ,then $f(x)=\sum_{n \in \mathbb{Z}} f(\frac{n}{2B}) sinc(2B(x-\frac{n}{2B}))$ in the sense that the RHS converges to $f$ in $L_2(\mathbb{R})$ . This states that a function can be recoverd from the values $(f(\frac{k}{2B}))_{k \in \mathbb{Z}}$ . Now I heard that one can ""oversample"" to get better convergence. I got a hint on how to do the prove: Use a function $g$ , such that $\mathcal{F}g(\xi)=1$ on $[-B,B]$ and $\mathcal{F}g(\xi)=0$ on $|\xi| > B'$ My approach: Now assume $supp(\mathcal{F}f) \subseteq [-B,B]$ , and consider some $B'$ >B. One can write $f(x)=\int_{-B}^{B} \mathcal{F}f(\xi) e^{2 \pi i \xi x}d\xi$ Now if I choose a function $g$ , such that $g(\xi)=1 for \xi \in [-B,B]$ and $g(\xi)=0 for |\xi|>B'$ the above equals to $=\int_{-B'}^{B'} \mathcal{F}f(\xi) e^{2 \pi i \xi x}g(\xi)dx $ If I now consider the Fourier series of f on $(-B',B')$ I further get $= \int_{-B'}^{B'}\sum_{n \in\mathbb{Z}} \frac{1}{2B'} f(-\frac{n}{2B'}) e^{\frac{2 \pi i n \xi}{2B'}} g(\xi) e^{2 \pi i x \xi} d \xi=$ $\sum_{n \in\mathbb{Z}} f(-\frac{n}{2B'}) \frac{1}{2B'}\mathcal{F}^{-1}(g)(x+\frac{n}{2B'})$ To sum it up, one expands $\mathcal{F}f$ as a Fourier series, recover Fourier coefficients and then do the inverse Fourier transform. Now it seems to come down to calculating $g$ . For that I also got a hint: Assume $g$ has the form $g=\frac{1}{2a} \chi_{[-a,a]} \ast ... \ast \frac{1}{2a} \chi_{[-a,a]} \ast \chi_{[-B-ka,B+ka]}$ , where $\chi_{[c,d]}$ denotes the indicator function on some intervall $[c,d]$ That's where I am stuck. Alternatively if someone has another prove this would be fine too.","I am familiar with the Shanon Sampling theorem, which states that: Let and ,then in the sense that the RHS converges to in . This states that a function can be recoverd from the values . Now I heard that one can ""oversample"" to get better convergence. I got a hint on how to do the prove: Use a function , such that on and on My approach: Now assume , and consider some >B. One can write Now if I choose a function , such that and the above equals to If I now consider the Fourier series of f on I further get To sum it up, one expands as a Fourier series, recover Fourier coefficients and then do the inverse Fourier transform. Now it seems to come down to calculating . For that I also got a hint: Assume has the form , where denotes the indicator function on some intervall That's where I am stuck. Alternatively if someone has another prove this would be fine too.","f \in L_1(\mathbb{R}) supp(\mathcal{F}f) \subseteq [-B,B] f(x)=\sum_{n \in \mathbb{Z}} f(\frac{n}{2B}) sinc(2B(x-\frac{n}{2B})) f L_2(\mathbb{R}) (f(\frac{k}{2B}))_{k \in \mathbb{Z}} g \mathcal{F}g(\xi)=1 [-B,B] \mathcal{F}g(\xi)=0 |\xi| > B' supp(\mathcal{F}f) \subseteq [-B,B] B' f(x)=\int_{-B}^{B} \mathcal{F}f(\xi) e^{2 \pi i \xi x}d\xi g g(\xi)=1 for \xi \in [-B,B] g(\xi)=0 for |\xi|>B' =\int_{-B'}^{B'} \mathcal{F}f(\xi) e^{2 \pi i \xi x}g(\xi)dx  (-B',B') = \int_{-B'}^{B'}\sum_{n \in\mathbb{Z}} \frac{1}{2B'} f(-\frac{n}{2B'}) e^{\frac{2 \pi i n \xi}{2B'}} g(\xi) e^{2 \pi i x \xi} d \xi= \sum_{n \in\mathbb{Z}} f(-\frac{n}{2B'}) \frac{1}{2B'}\mathcal{F}^{-1}(g)(x+\frac{n}{2B'}) \mathcal{F}f g g g=\frac{1}{2a} \chi_{[-a,a]} \ast ... \ast \frac{1}{2a} \chi_{[-a,a]} \ast \chi_{[-B-ka,B+ka]} \chi_{[c,d]} [c,d]","['analysis', 'fourier-analysis', 'sampling-theory']"
27,function with zero derivatives at certain points,function with zero derivatives at certain points,,"Let $f:\mathbb{R}_{\ge0}\rightarrow \mathbb{R}$ be a $\mathcal{C}^{\infty}$ function such that $\lim_{x\rightarrow \infty}f(x) = 0$ . If $f(0)=0$ , I know how to show that $f'(c)=0$ for some $c$ , and this is intuitively clear. My question is : Is it possible to find a sequence $x_n$ such that $f^{(n)}(x_n)=0$ for every $n$ ? It looks like a very strong statement, but I feel like once it is true for $f'$ it is also true for all the other derivatives of $f$ at some point since we can just ""repeat"" by induction. I am not able to construct a proof though...","Let be a function such that . If , I know how to show that for some , and this is intuitively clear. My question is : Is it possible to find a sequence such that for every ? It looks like a very strong statement, but I feel like once it is true for it is also true for all the other derivatives of at some point since we can just ""repeat"" by induction. I am not able to construct a proof though...",f:\mathbb{R}_{\ge0}\rightarrow \mathbb{R} \mathcal{C}^{\infty} \lim_{x\rightarrow \infty}f(x) = 0 f(0)=0 f'(c)=0 c x_n f^{(n)}(x_n)=0 n f' f,"['analysis', 'derivatives']"
28,Taylor series higher-order terms,Taylor series higher-order terms,,"I have carefully read the M.S.E. post here explaining the derivation of Taylor series, and I paid attention to the link in one of the comments, i.e., a dedicated blog post on Taylor series. In the linked blog post, the author explains how $\sin(x)$ can be approximated with a Taylor series expansion. Specifically, an example is given for a second-order Taylor approximation (denoted $T_2(x)$ ) where the expansion is carried around the point $x=\frac{\pi}{2}$ : here, the author explains that a curve can be defined using: a single point (i.e., $T_2(x=\frac{\pi}{2})=\sin(x=\frac{\pi}{2})=1$ ) the derivative at this point (i.e., we want the approximating polynomial to be $T_2'(x=\frac{\pi}{2})=\sin'(x=\frac{\pi}{2})=0$ ) The “curvature” at the same point, i.e., second derivative ( $T_2''(x=\frac{\pi}{2})=\sin''(x=\frac{\pi}{2})=-1)$ ) Solving for these conditions gives the following approximation: The third-order Taylor approximation, $T_3(x)$ is carried out around $x=0$ (i.e., the inflection point), by adding a condition on the third derivative at $x=0$ , in addition to slope and curvature at that point, leading to: My question: I conceptually understand how the two graphs above can be generated by information at a single point (i.e., $x=\frac{\pi}{2}$ for the first one and $x=0$ for the second one). But how can higher-order derivative terms in the Taylor expansion, all taken at the same point , provide information about local minima and maxima further away from the point of the approximation? I.e., how can higher-order derivatives taken (say) at $x=0$ see “behind the next turn”? I try to highlight this at the graph below: I cannot get my head around how information contained at $x=0$ can give the right point for the Taylor approximation at (say) $x=\frac{3\pi}{2}$ , for example.","I have carefully read the M.S.E. post here explaining the derivation of Taylor series, and I paid attention to the link in one of the comments, i.e., a dedicated blog post on Taylor series. In the linked blog post, the author explains how can be approximated with a Taylor series expansion. Specifically, an example is given for a second-order Taylor approximation (denoted ) where the expansion is carried around the point : here, the author explains that a curve can be defined using: a single point (i.e., ) the derivative at this point (i.e., we want the approximating polynomial to be ) The “curvature” at the same point, i.e., second derivative ( ) Solving for these conditions gives the following approximation: The third-order Taylor approximation, is carried out around (i.e., the inflection point), by adding a condition on the third derivative at , in addition to slope and curvature at that point, leading to: My question: I conceptually understand how the two graphs above can be generated by information at a single point (i.e., for the first one and for the second one). But how can higher-order derivative terms in the Taylor expansion, all taken at the same point , provide information about local minima and maxima further away from the point of the approximation? I.e., how can higher-order derivatives taken (say) at see “behind the next turn”? I try to highlight this at the graph below: I cannot get my head around how information contained at can give the right point for the Taylor approximation at (say) , for example.",\sin(x) T_2(x) x=\frac{\pi}{2} T_2(x=\frac{\pi}{2})=\sin(x=\frac{\pi}{2})=1 T_2'(x=\frac{\pi}{2})=\sin'(x=\frac{\pi}{2})=0 T_2''(x=\frac{\pi}{2})=\sin''(x=\frac{\pi}{2})=-1) T_3(x) x=0 x=0 x=\frac{\pi}{2} x=0 x=0 x=0 x=\frac{3\pi}{2},"['analysis', 'taylor-expansion']"
29,Hausdorff dimension of sets with positive Lebesgue measure,Hausdorff dimension of sets with positive Lebesgue measure,,"I am reading Hausdorff Dimension, Its Properties, and Its Surprises by Dierk Schleicher. Among the elementary properties of the Hausdorff dimension, the last one is: If $X\subset \Bbb R^n$ has finite positive $d$ -dimensional Lebesgue measure, then $\dim_H X = d$ . My work. It will be enough to show that $\mathcal H^s(X) = 0$ for all $s > d$ , and $\mathcal H^s(X) = \infty$ for all $s < d$ . As usual, $$H^s(X) = \lim_{\delta\to 0} H^s_\delta(X)$$ where $$\mathcal H^s_\delta(X) = \inf\left\{\sum_{i=1}^\infty |U_i|^s: \{U_i\} \text{ is a }\delta\text{-cover of }X  \right\}$$ I'm unable to relate the Lebesgue measure with coverings of $X$ , which would help me find a connection with $\mathcal H^s_\delta(X)$ for given $\delta > 0$ . Thanks a lot!","I am reading Hausdorff Dimension, Its Properties, and Its Surprises by Dierk Schleicher. Among the elementary properties of the Hausdorff dimension, the last one is: If has finite positive -dimensional Lebesgue measure, then . My work. It will be enough to show that for all , and for all . As usual, where I'm unable to relate the Lebesgue measure with coverings of , which would help me find a connection with for given . Thanks a lot!",X\subset \Bbb R^n d \dim_H X = d \mathcal H^s(X) = 0 s > d \mathcal H^s(X) = \infty s < d H^s(X) = \lim_{\delta\to 0} H^s_\delta(X) \mathcal H^s_\delta(X) = \inf\left\{\sum_{i=1}^\infty |U_i|^s: \{U_i\} \text{ is a }\delta\text{-cover of }X  \right\} X \mathcal H^s_\delta(X) \delta > 0,"['analysis', 'measure-theory', 'dimension-theory-analysis']"
30,Approximating a derivative: How to complete this proof of $f'(x_2) = \frac{f_0 - 8f_1 + 8f_3 - f_4}{12 h} + \frac{h^4}{30}f^\mathrm{V}(\xi)$?,Approximating a derivative: How to complete this proof of ?,f'(x_2) = \frac{f_0 - 8f_1 + 8f_3 - f_4}{12 h} + \frac{h^4}{30}f^\mathrm{V}(\xi),"Fix five equally spaced nodes as $x_i = x_0 + ih$ where $h > 0$ , $x_0\in\mathbb{R}$ , and $i = 0, 1, 2, 3, 4$ . Let us also denote $f_i := f(x_i)$ . Exercise. Assume that $f\in \operatorname{C^5}[x_0, x_4].$ Show that there exists some $\xi(x_2)=:\xi\in[x_0, x_4]$ such that $$f'(x_2) = \dfrac{f_0 - 8f_1 + 8f_3 - f_4}{12 h} + \dfrac{h^4}{30}f^\mathrm{V}(\xi).\label{E}\tag{E}$$ Solution. Using a method of undetermined coefficients and approximation by Taylor polynomials with Lagrangian remainders, I believe to have shown that $$f'(x) = \dfrac{f_0 - 8f_1 + 8f_3 - f_4}{12 h} + \frac{h^4}{30} \frac{16\, f^\mathrm{V}(\xi_2) - 4\, f^\mathrm{V}(\xi_1)}{12} \tag{1}$$ where $\xi_1, \xi_2 \in[x_0, x_4]$ , and $x:=x_2.$ Here is the more detailed explanation. (Skip forward to section named Question if you wish). First for $k = 1, 2$ using Taylor polynomials and Lagrangian remainders $$f(x\pm kh) = f(x) \pm f'(x)\, kh + f''(x)\, \frac{k^2 h^2}{2} \pm f'''(x)\, \frac{k^3 h^3}{6}  + f^\mathrm{IV}(x)\,\frac{k^4 h^4}{24} \pm f^\mathrm{V}(\xi_\pm^k)\,\frac{k^5 h^5}{120} \label{A1}\tag{A1}$$ where $\xi_\pm^k$ is between $x$ and $x \pm k h$ . Note also that $x_1 = x - h$ , $x_3 = x + h$ and so on. Let us view the expression $Af_0 + Bf_1 + Cf_3 + Df_4$ where $A, B, C, D$ are to be determined. After substituting $f_0, f_1, f_3, f_4$ from the earlier Taylor expansion $\eqref{A1}$ into this expression, one gets after further dividing both sides by $h$ that \begin{align*}\frac{Af_0 + Bf_1 + Cf_3 + Df_4}{h} = \, (&A + B + C + D)\,\frac{f(x)}{h} + (-2A - B + C + 2D)\, f'(x)\\ +&(4A + B + C + 4 D)\,f''(x)\, \frac{h}{2}  \\ +&\, (-8A -B + C + 8D)\,f'''(x)\, \frac{h^2}{6} + (16A + B + C + 16D)\, f^\mathrm{IV}(x)\,\frac{ h^3}{24}\\ +& \left[-32A\, f^\mathrm{V}(\xi_-^2) - B\, f^\mathrm{V}(\xi_-^1) + C\, f^\mathrm{V}(\xi_+^1) + 32D\, f^\mathrm{V}(\xi_+^2)\right]\,\frac{h^4}{120}. \label{A2}\tag{A2}\end{align*} Next we attempt to determine the coefficients $A, B, C, D$ in such a way that we are left with $f'(x)$ and $h^4$ terms on the RHS of $\eqref{A2}$ . This gives us the system $$ \begin{cases} 	A + B + C + D = 0,\\ 	-2A - B + C + 2D = 1, \\     4A + B + C + 4 D = 0, \\     -8A -B + C + 8D = 0,\\     16A + B + C + 16D = 0. \end{cases}\label{A3}\tag{A3} $$ The unique solution is $A = - D = \dfrac{1}{12}$ , $-B = C = \dfrac{2}{3}.$ If we denote the $h^4$ term by $-\mathcal R(x)$ , then substituting the values of the coefficients back into $\eqref{A2}$ , we get $$\dfrac{f_0 - 8f_1 + 8f_3 - f_4}{12 h} + \mathcal R(x) = f'(x).\label{A4}\tag{A4}$$ Comparing this to $\eqref{E}$ , what remains to be shown is that the expression $$\mathcal R(x) = \left[32A\, f^\mathrm{V}(\xi_-^2) + B\, f^\mathrm{V}(\xi_-^1) - C\, f^\mathrm{V}(\xi_+^1) - 32D\, f^\mathrm{V}(\xi_+^2)\right]\,\frac{h^4}{120}\label{A5}\tag{A5}$$ or, after substituting the solution coefficients and simplifying, that the expression $$\mathcal R(x) = \frac{h^4}{30} \frac{8\, f^\mathrm{V}(\xi_-^2) - 2\, f^\mathrm{V}(\xi_-^1) - 2\, f^\mathrm{V}(\xi_+^1) + 8\, f^\mathrm{V}(\xi_+^2)}{12}\label{A6}\tag{A6}$$ is somehow equal to $$\dfrac{h^4}{30}f^\mathrm{V}(\xi)\label{A7}\tag{A7}$$ for some $\xi\in[x_0, x_4]$ . Because $f^\mathrm{V}$ is continuous, by the intermediate value theorem we get \begin{align*} f^\mathrm{V}(\xi_-^1) + f^\mathrm{V}(\xi_+^1) = 2 f^\mathrm{V}(\xi_1),\label{A8}\tag{A8}\\ f^\mathrm{V}(\xi_-^2) + f^\mathrm{V}(\xi_+^2) = 2 f^\mathrm{V}(\xi_2),\label{A9}\tag{A9} \end{align*} where $\xi_1 \in(x - h, x + h)$ and $\xi_2 \in(x - 2h, x + 2h)$ . Therefore, $$\mathcal R(x) = \frac{h^4}{30} \frac{16\, f^\mathrm{V}(\xi_2) - 4\, f^\mathrm{V}(\xi_1)}{12}.\label{A10}\tag{A10}$$ Question. If I could show that for some $\xi\in [x_0, x_4]$ $$16f^\mathrm{V}(\xi_1) - 4f^\mathrm{V}(\xi_2) = 12f^\mathrm{V}(\xi),\label{Q}\tag{Q}$$ the proof would be complete. Is this achievable? If it isn't always possible to do, there is probably a mistake somewhere...","Fix five equally spaced nodes as where , , and . Let us also denote . Exercise. Assume that Show that there exists some such that Solution. Using a method of undetermined coefficients and approximation by Taylor polynomials with Lagrangian remainders, I believe to have shown that where , and Here is the more detailed explanation. (Skip forward to section named Question if you wish). First for using Taylor polynomials and Lagrangian remainders where is between and . Note also that , and so on. Let us view the expression where are to be determined. After substituting from the earlier Taylor expansion into this expression, one gets after further dividing both sides by that Next we attempt to determine the coefficients in such a way that we are left with and terms on the RHS of . This gives us the system The unique solution is , If we denote the term by , then substituting the values of the coefficients back into , we get Comparing this to , what remains to be shown is that the expression or, after substituting the solution coefficients and simplifying, that the expression is somehow equal to for some . Because is continuous, by the intermediate value theorem we get where and . Therefore, Question. If I could show that for some the proof would be complete. Is this achievable? If it isn't always possible to do, there is probably a mistake somewhere...","x_i = x_0 + ih h > 0 x_0\in\mathbb{R} i = 0, 1, 2, 3, 4 f_i := f(x_i) f\in \operatorname{C^5}[x_0, x_4]. \xi(x_2)=:\xi\in[x_0, x_4] f'(x_2) = \dfrac{f_0 - 8f_1 + 8f_3 - f_4}{12 h} + \dfrac{h^4}{30}f^\mathrm{V}(\xi).\label{E}\tag{E} f'(x) = \dfrac{f_0 - 8f_1 + 8f_3 - f_4}{12 h} + \frac{h^4}{30} \frac{16\, f^\mathrm{V}(\xi_2) - 4\, f^\mathrm{V}(\xi_1)}{12} \tag{1} \xi_1, \xi_2 \in[x_0, x_4] x:=x_2. k = 1, 2 f(x\pm kh) = f(x) \pm f'(x)\, kh + f''(x)\, \frac{k^2 h^2}{2} \pm f'''(x)\, \frac{k^3 h^3}{6}
 + f^\mathrm{IV}(x)\,\frac{k^4 h^4}{24} \pm f^\mathrm{V}(\xi_\pm^k)\,\frac{k^5 h^5}{120} \label{A1}\tag{A1} \xi_\pm^k x x \pm k h x_1 = x - h x_3 = x + h Af_0 + Bf_1 + Cf_3 + Df_4 A, B, C, D f_0, f_1, f_3, f_4 \eqref{A1} h \begin{align*}\frac{Af_0 + Bf_1 + Cf_3 + Df_4}{h} = \, (&A + B + C + D)\,\frac{f(x)}{h} + (-2A - B + C + 2D)\, f'(x)\\
+&(4A + B + C + 4 D)\,f''(x)\, \frac{h}{2}  \\
+&\, (-8A -B + C + 8D)\,f'''(x)\, \frac{h^2}{6} + (16A + B + C + 16D)\, f^\mathrm{IV}(x)\,\frac{ h^3}{24}\\
+& \left[-32A\, f^\mathrm{V}(\xi_-^2) - B\, f^\mathrm{V}(\xi_-^1) + C\, f^\mathrm{V}(\xi_+^1) + 32D\, f^\mathrm{V}(\xi_+^2)\right]\,\frac{h^4}{120}.
\label{A2}\tag{A2}\end{align*} A, B, C, D f'(x) h^4 \eqref{A2} 
\begin{cases}
	A + B + C + D = 0,\\
	-2A - B + C + 2D = 1, \\
    4A + B + C + 4 D = 0, \\
    -8A -B + C + 8D = 0,\\
    16A + B + C + 16D = 0.
\end{cases}\label{A3}\tag{A3}
 A = - D = \dfrac{1}{12} -B = C = \dfrac{2}{3}. h^4 -\mathcal R(x) \eqref{A2} \dfrac{f_0 - 8f_1 + 8f_3 - f_4}{12 h} + \mathcal R(x) = f'(x).\label{A4}\tag{A4} \eqref{E} \mathcal R(x) = \left[32A\, f^\mathrm{V}(\xi_-^2) + B\, f^\mathrm{V}(\xi_-^1) - C\, f^\mathrm{V}(\xi_+^1) - 32D\, f^\mathrm{V}(\xi_+^2)\right]\,\frac{h^4}{120}\label{A5}\tag{A5} \mathcal R(x) = \frac{h^4}{30} \frac{8\, f^\mathrm{V}(\xi_-^2) - 2\, f^\mathrm{V}(\xi_-^1) - 2\, f^\mathrm{V}(\xi_+^1) + 8\, f^\mathrm{V}(\xi_+^2)}{12}\label{A6}\tag{A6} \dfrac{h^4}{30}f^\mathrm{V}(\xi)\label{A7}\tag{A7} \xi\in[x_0, x_4] f^\mathrm{V} \begin{align*}
f^\mathrm{V}(\xi_-^1) + f^\mathrm{V}(\xi_+^1) = 2 f^\mathrm{V}(\xi_1),\label{A8}\tag{A8}\\
f^\mathrm{V}(\xi_-^2) + f^\mathrm{V}(\xi_+^2) = 2 f^\mathrm{V}(\xi_2),\label{A9}\tag{A9}
\end{align*} \xi_1 \in(x - h, x + h) \xi_2 \in(x - 2h, x + 2h) \mathcal R(x) = \frac{h^4}{30} \frac{16\, f^\mathrm{V}(\xi_2) - 4\, f^\mathrm{V}(\xi_1)}{12}.\label{A10}\tag{A10} \xi\in [x_0, x_4] 16f^\mathrm{V}(\xi_1) - 4f^\mathrm{V}(\xi_2) = 12f^\mathrm{V}(\xi),\label{Q}\tag{Q}","['analysis', 'derivatives', 'numerical-methods', 'solution-verification', 'numerical-calculus']"
31,Algebraic Topology Book for the Analyst,Algebraic Topology Book for the Analyst,,"I'm looking for a graduate level book on topology that takes most of its motivation from analysis and applied mathematics. I'm currently in an algebraic topology course but other than basic definitions and intuition I have learned absolutely nothing about algebraic topology! Now I need to go and relearn most of the topic but it's quite challenging because I find most books on topology unmotivated and uninteresting. Most examples in my class are showing that one sphere is not a different sphere or otherwise use a collection of classic topological objects which I take very little interest in. I am not trying to diss algebraic topology in any way, but what are some books on topology that stress spaces that are of greater interest to problems in analysis and probability theory? Ghrist's book ""Elementary Applied Topology"" looks good, but too cursory for what I'm after. And references on topological data analysis use persistent homology and other topics that are currently above my head. Some books that I'm aware of but have not read that seem like they may be good are: Lee ""Introduction to Topological Manifolds"", Dold ""Lectures in Algebraic Topology"", Rotman ""An Introduction to Algebraic Topology"", Edelsbrunner ""Computational Topology"", and Kaczynski ""Computational Homology"" If one of the above texts stands out as a good candidate for what I'm interested in, please let me know (It's impossible to read all of them before deciding). Books that I have read parts of and dislike include: Bredon, Massey, Hatcher, and May.","I'm looking for a graduate level book on topology that takes most of its motivation from analysis and applied mathematics. I'm currently in an algebraic topology course but other than basic definitions and intuition I have learned absolutely nothing about algebraic topology! Now I need to go and relearn most of the topic but it's quite challenging because I find most books on topology unmotivated and uninteresting. Most examples in my class are showing that one sphere is not a different sphere or otherwise use a collection of classic topological objects which I take very little interest in. I am not trying to diss algebraic topology in any way, but what are some books on topology that stress spaces that are of greater interest to problems in analysis and probability theory? Ghrist's book ""Elementary Applied Topology"" looks good, but too cursory for what I'm after. And references on topological data analysis use persistent homology and other topics that are currently above my head. Some books that I'm aware of but have not read that seem like they may be good are: Lee ""Introduction to Topological Manifolds"", Dold ""Lectures in Algebraic Topology"", Rotman ""An Introduction to Algebraic Topology"", Edelsbrunner ""Computational Topology"", and Kaczynski ""Computational Homology"" If one of the above texts stands out as a good candidate for what I'm interested in, please let me know (It's impossible to read all of them before deciding). Books that I have read parts of and dislike include: Bredon, Massey, Hatcher, and May.",,"['analysis', 'reference-request', 'algebraic-topology', 'book-recommendation']"
32,"Euclidean Algorithm proof exercise Terence Tao ""Analysis I""","Euclidean Algorithm proof exercise Terence Tao ""Analysis I""",,"I'm self-studying Analysis from Terence Tao's ""Analysis I"" and one of the exercises given is prove the following proposition Proposition 2.3.9 (Euclidean Algorithm). Let $n$ be a natural number and let $q$ be a positive natural number. Then there exist natural numbers $m$ , $r$ such that $0 \leq r < q$ and $n = mq + r$ . I'm fairly new to proving so I initially got stuck so I looked at the hint which was to fix $q$ and induct on $n$ . And now I currently have the following: Proof. We fix $q$ and use induction on $n$ . We first prove the base case $n=0$ . If we set $m=0$ and $r=0$ then we have $n = 0 \cdot q + 0 = 0$ but $0 \leq 0 < q$ , so we are done with the base case. Now suppose inductively that $n = m \cdot q + r$ for some natural numbers $m$ , $r$ such that $0 \leq r < q$ and $n = mq + r$ . We wish to show that there exist natural numbers $m'$ and $r'$ such that $n+1= m' \cdot  q + r'$ where $0\leq r'< q$ . From the inductive hypothesis we have $n+1 = mq + (r+1)$ . Since $r<q$ , $r+1 \leq q$ that is $r+1 = q$ or $r+1 <q$ . If $r+1 = q$ , we set $m' = m+1$ and $r'=0$ then $m' \cdot q + r' = (m+1) \cdot q + 0$ but $n+1 =(m+1) \cdot q + 0$ , so $n+1 = m' \cdot  q + r'$ and $0\leq r'< q$ . If however $r+1 <q$ then we set $m' = m$ and $r' = r+1$ then we have that $n+1 = m' \cdot  q + r'$ and $0\leq r'< q$ . This completes the induction. $$\tag*{$\Box$}$$ I'd be grateful for any corrections or suggestions for improvement.","I'm self-studying Analysis from Terence Tao's ""Analysis I"" and one of the exercises given is prove the following proposition Proposition 2.3.9 (Euclidean Algorithm). Let be a natural number and let be a positive natural number. Then there exist natural numbers , such that and . I'm fairly new to proving so I initially got stuck so I looked at the hint which was to fix and induct on . And now I currently have the following: Proof. We fix and use induction on . We first prove the base case . If we set and then we have but , so we are done with the base case. Now suppose inductively that for some natural numbers , such that and . We wish to show that there exist natural numbers and such that where . From the inductive hypothesis we have . Since , that is or . If , we set and then but , so and . If however then we set and then we have that and . This completes the induction. I'd be grateful for any corrections or suggestions for improvement.",n q m r 0 \leq r < q n = mq + r q n q n n=0 m=0 r=0 n = 0 \cdot q + 0 = 0 0 \leq 0 < q n = m \cdot q + r m r 0 \leq r < q n = mq + r m' r' n+1= m' \cdot  q + r' 0\leq r'< q n+1 = mq + (r+1) r<q r+1 \leq q r+1 = q r+1 <q r+1 = q m' = m+1 r'=0 m' \cdot q + r' = (m+1) \cdot q + 0 n+1 =(m+1) \cdot q + 0 n+1 = m' \cdot  q + r' 0\leq r'< q r+1 <q m' = m r' = r+1 n+1 = m' \cdot  q + r' 0\leq r'< q \tag*{\Box},"['analysis', 'proof-writing', 'solution-verification', 'natural-numbers']"
33,$f$ is continuous in $x$ and $y$ and maps compact set into compact set. Show that $f$ is a continuous function on $\mathbb R^2$.,is continuous in  and  and maps compact set into compact set. Show that  is a continuous function on .,f x y f \mathbb R^2,"Suppose $f$ is defined on $\mathbb{R}^2$ , $f$ is continuous in $x$ and $y$ respectively and $f$ maps compact set into compact set. Show that $f$ is a continuous function on $\mathbb R^2$ . Suppose $f$ is not continuous at $(x_0,\,y_0)$ , then there are $\varepsilon_0>0$ and a sequence $\{(x_n,\,y_n)\}$ converging to $(x_0,\,y_0)$ such that $$ |f(x_n,\,y_n)-f(x_0,\,y_0)|\geq\varepsilon_0. $$ Let $K=\{(x_n,\,y_n)\}_{n=0}^\infty$ . $K$ is compact. Thus, $f(K)$ is also compact. Then I don't know what to do and am unable to use the condition that $f$ is continuous in $x$ and $y$ respectively.","Suppose is defined on , is continuous in and respectively and maps compact set into compact set. Show that is a continuous function on . Suppose is not continuous at , then there are and a sequence converging to such that Let . is compact. Thus, is also compact. Then I don't know what to do and am unable to use the condition that is continuous in and respectively.","f \mathbb{R}^2 f x y f f \mathbb R^2 f (x_0,\,y_0) \varepsilon_0>0 \{(x_n,\,y_n)\} (x_0,\,y_0) 
|f(x_n,\,y_n)-f(x_0,\,y_0)|\geq\varepsilon_0.
 K=\{(x_n,\,y_n)\}_{n=0}^\infty K f(K) f x y",['analysis']
34,Countable partition in atoms,Countable partition in atoms,,"Let $\mu: \Sigma \to [0, \infty)$ a measure over $\Omega$. We say a set $A \in \Sigma$ is an atom if for all $B \in \Sigma$ with $B \subset A$, $\mu(B)=\mu(A)$ or $\mu(B)=0$. We say that $\mu$ is atomic if every set $A \in \Sigma$ with positive measure contains an atom with positive measure. So, I'm trying to prove that if $\mu$ is atomic, there exists $\{ A_n\}_{n \in \mathbb{N}}$ pairwise disjoint atoms such that its union covers $\Omega$. I can build a sequence this way: Let $A \in \Sigma$ be a set with positive measure. It contains an atom $A_1$.Then I pick up $\Omega \setminus A_1$. Again, it contains an atom $A_2$ which is disjoint from $A_1$. However, I do not know how to continue. I would appreciate any help.","Let $\mu: \Sigma \to [0, \infty)$ a measure over $\Omega$. We say a set $A \in \Sigma$ is an atom if for all $B \in \Sigma$ with $B \subset A$, $\mu(B)=\mu(A)$ or $\mu(B)=0$. We say that $\mu$ is atomic if every set $A \in \Sigma$ with positive measure contains an atom with positive measure. So, I'm trying to prove that if $\mu$ is atomic, there exists $\{ A_n\}_{n \in \mathbb{N}}$ pairwise disjoint atoms such that its union covers $\Omega$. I can build a sequence this way: Let $A \in \Sigma$ be a set with positive measure. It contains an atom $A_1$.Then I pick up $\Omega \setminus A_1$. Again, it contains an atom $A_2$ which is disjoint from $A_1$. However, I do not know how to continue. I would appreciate any help.",,"['analysis', 'measure-theory']"
35,Prove that $f$ is continuous in $\mathbb{R}$,Prove that  is continuous in,f \mathbb{R},Let  $f:\mathbb{R} \to \mathbb{R}$ be a monotone function such that $f(V)$ is open for every open set $V\subset \mathbb{R}$. Prove that $f$ is continuous in $\mathbb{R}$. Any hint for proving this I will appreciate.,Let  $f:\mathbb{R} \to \mathbb{R}$ be a monotone function such that $f(V)$ is open for every open set $V\subset \mathbb{R}$. Prove that $f$ is continuous in $\mathbb{R}$. Any hint for proving this I will appreciate.,,['analysis']
36,"If $m^*([-n,n] \cap E) + m^*([-n,n] \setminus E) = 2n$ for all $n$, then $E$ is Lebesgue measurable","If  for all , then  is Lebesgue measurable","m^*([-n,n] \cap E) + m^*([-n,n] \setminus E) = 2n n E","Let $E \subset \Bbb R$ and let $m^*$ denote the Lebesgue outer measure on $\Bbb R$ . Show that if for all $n \in \Bbb N$ , $m^*([-n,n] \cap E) + m^*([-n,n] \setminus E) = 2n$ , then $E$ is Lebesgue measurable. Hint: for all $n \in \Bbb N$ , there are measurable sets $F_n$ , $G_n$ such that $F_n \subset E \cap [-n,n] \subset G_n$ , and $m(G_n \setminus F_n) = 0$ . Given the hint, it is not difficult to conclude: put $F = \cup_n F_n$ and $G = \cup_n G_n$ . We have: $$m^*(E \setminus F) = m^* \left( \bigcup_n ((E \cap [-n,n]) \setminus F) \right) \le m (G \setminus F) \le m(\cup_n (G_n \setminus F_n)) = 0$$ So $E \setminus F$ is negligible, hence measurable. Therefore, $E = (E \setminus F) \cup F$ is measurable. How to prove the claim in the hint?","Let and let denote the Lebesgue outer measure on . Show that if for all , , then is Lebesgue measurable. Hint: for all , there are measurable sets , such that , and . Given the hint, it is not difficult to conclude: put and . We have: So is negligible, hence measurable. Therefore, is measurable. How to prove the claim in the hint?","E \subset \Bbb R m^* \Bbb R n \in \Bbb N m^*([-n,n] \cap E) + m^*([-n,n] \setminus E) = 2n E n \in \Bbb N F_n G_n F_n \subset E \cap [-n,n] \subset G_n m(G_n \setminus F_n) = 0 F = \cup_n F_n G = \cup_n G_n m^*(E \setminus F) = m^* \left( \bigcup_n ((E \cap [-n,n]) \setminus F) \right) \le m (G \setminus F) \le m(\cup_n (G_n \setminus F_n)) = 0 E \setminus F E = (E \setminus F) \cup F",['analysis']
37,strictly increasing functions and bijection,strictly increasing functions and bijection,,$f $  is strictly increasing     \begin{align}  & \rightarrow f^{-1} \text{is strictly increasing}\tag {i} \\ & \rightarrow f^{-1} \text{is strictly decreasing}\tag {ii}  \\ & \rightarrow f \quad \text{is injective}\tag {iii}\\ & \rightarrow f \quad \text{is surjective}\tag {iv}\\ & \rightarrow f^{-1} \text{is bijective}\tag {v}   \end{align} which statements are true? (iii)  $f$ is injective since $x<y\rightarrow f(x)<f(y)$ (iv) I believe it doesn't need to be surjective.   $f:\Bbb N\to \Bbb N$ with $f(x)=x+1$ (v) since $f$ isn't bijective we cannot talk about $f^{-1} $ function,$f $  is strictly increasing     \begin{align}  & \rightarrow f^{-1} \text{is strictly increasing}\tag {i} \\ & \rightarrow f^{-1} \text{is strictly decreasing}\tag {ii}  \\ & \rightarrow f \quad \text{is injective}\tag {iii}\\ & \rightarrow f \quad \text{is surjective}\tag {iv}\\ & \rightarrow f^{-1} \text{is bijective}\tag {v}   \end{align} which statements are true? (iii)  $f$ is injective since $x<y\rightarrow f(x)<f(y)$ (iv) I believe it doesn't need to be surjective.   $f:\Bbb N\to \Bbb N$ with $f(x)=x+1$ (v) since $f$ isn't bijective we cannot talk about $f^{-1} $ function,,"['analysis', 'functions', 'monotone-functions']"
38,"How would you interpret the following statement involving ""a.e.""?","How would you interpret the following statement involving ""a.e.""?",,"Here is an edited fragment from an exercise: Let $(X, \mathcal A, \mu)$ be a measure space, $(f_n)$ be a sequence of such and such functions. If $f(x)= \lim f_n(x)$ exists for almost every $x\in X$ then $f$ has such and such properties (in particular $f$ is measurable). I'm confused about the role of $f$. Is it already given in the statement? E.g. if in the proof, one would set $f$ to zero on some convenient set of measure zero, it wouldn't be OK? (A proof which I encountered does exactly that.) The question probably has little to do with measure theory but that a general ""if something shows up in the assumption, must it be fixed from there on?"" but I've added the context just in case.","Here is an edited fragment from an exercise: Let $(X, \mathcal A, \mu)$ be a measure space, $(f_n)$ be a sequence of such and such functions. If $f(x)= \lim f_n(x)$ exists for almost every $x\in X$ then $f$ has such and such properties (in particular $f$ is measurable). I'm confused about the role of $f$. Is it already given in the statement? E.g. if in the proof, one would set $f$ to zero on some convenient set of measure zero, it wouldn't be OK? (A proof which I encountered does exactly that.) The question probably has little to do with measure theory but that a general ""if something shows up in the assumption, must it be fixed from there on?"" but I've added the context just in case.",,"['analysis', 'measure-theory', 'proof-writing', 'fake-proofs', 'almost-everywhere']"
39,"Showing that $\sup_{(x,y)}f(x,y)=\sup_x\sup_yf(x,y)=\sup_y\sup_xf(x,y)$",Showing that,"\sup_{(x,y)}f(x,y)=\sup_x\sup_yf(x,y)=\sup_y\sup_xf(x,y)","Can anyone help me prove this: Let $X$ and $Y$ be nonempty sets and $f:X\times Y\to\Bbb R$ such that $f(X\times Y)$ is bounded. Prove the following statement: $\sup_{(x,y)}f(x,y)=\sup_x\sup_yf(x,y)=\sup_y\sup_xf(x,y)\;.$ thank you",Can anyone help me prove this: Let and be nonempty sets and such that is bounded. Prove the following statement: thank you,"X Y f:X\times Y\to\Bbb R f(X\times Y) \sup_{(x,y)}f(x,y)=\sup_x\sup_yf(x,y)=\sup_y\sup_xf(x,y)\;.",['analysis']
40,Can $\mathbb R$ be partitioned into a countable number of dense subsets with same cardinality?,Can  be partitioned into a countable number of dense subsets with same cardinality?,\mathbb R,"Is it possible to partition $\mathbb R$ into an countable number of disjoint dense subsets with the same cardinality? Furthermore, is it possible to partition the reals into an uncountable number of disjoint dense subsets with the same cardinality? This is a follow up question on an old question that was answered here. Can $\mathbb{R}$ be partitioned into $n$ dense sets with same cardinality? There, someone was able to construct a partition of $\mathbb R$ into $n$ dense subsets of the same cardinality.","Is it possible to partition into an countable number of disjoint dense subsets with the same cardinality? Furthermore, is it possible to partition the reals into an uncountable number of disjoint dense subsets with the same cardinality? This is a follow up question on an old question that was answered here. Can $\mathbb{R}$ be partitioned into $n$ dense sets with same cardinality? There, someone was able to construct a partition of into dense subsets of the same cardinality.",\mathbb R \mathbb R n,['analysis']
41,Prove that $\exists k\in \mathbb{N}^*$ such that $\|a-a_k\|<\varepsilon$,Prove that  such that,\exists k\in \mathbb{N}^* \|a-a_k\|<\varepsilon,"Let $E$ be a normed linear space, $C$ compact and $f:C\to C$ a function such that $\|f(x)-f(y)\|\geq \|x-y\|$ for all $x,y\in C$. Then $f$ is an isometry. Note: I'm having trouble trying to prove it! I feel stuck. My approach: Some hint says define: $a_0=a$, $a_n=f\circ ...\circ f(a)=f^{n}(a)$ and similar $b_n=f^{n}(b)$. Added: prove that for all $\varepsilon>0$ there is $k\in \mathbb{N}^*$ such that $\|a-a_k\|<\varepsilon$ and $\|b-b_k\|<\varepsilon$ How can I deduce from this that $\|a_1-b_1\|\leq \|a-b\|+2\varepsilon$? Because in that case I know that $\|f(a)-f(b)\|\leq \|a-b\|$ and since $a,b$ were arbitrary we are done! Please any help in proving this two things!! My attempt: $a_n$ have (by compacity) a subsequence which converges, say $a_{n_k}\to z\in C$. Can I say that $z=a$?? What I know is that for some $x_1,...x_p\in C$ we have that $C\subseteq \bigcup_{i=1}^p B(x_i,\varepsilon)$. Thanks I really feel stuck!","Let $E$ be a normed linear space, $C$ compact and $f:C\to C$ a function such that $\|f(x)-f(y)\|\geq \|x-y\|$ for all $x,y\in C$. Then $f$ is an isometry. Note: I'm having trouble trying to prove it! I feel stuck. My approach: Some hint says define: $a_0=a$, $a_n=f\circ ...\circ f(a)=f^{n}(a)$ and similar $b_n=f^{n}(b)$. Added: prove that for all $\varepsilon>0$ there is $k\in \mathbb{N}^*$ such that $\|a-a_k\|<\varepsilon$ and $\|b-b_k\|<\varepsilon$ How can I deduce from this that $\|a_1-b_1\|\leq \|a-b\|+2\varepsilon$? Because in that case I know that $\|f(a)-f(b)\|\leq \|a-b\|$ and since $a,b$ were arbitrary we are done! Please any help in proving this two things!! My attempt: $a_n$ have (by compacity) a subsequence which converges, say $a_{n_k}\to z\in C$. Can I say that $z=a$?? What I know is that for some $x_1,...x_p\in C$ we have that $C\subseteq \bigcup_{i=1}^p B(x_i,\varepsilon)$. Thanks I really feel stuck!",,['analysis']
42,Strong convexity and strong smoothness duality,Strong convexity and strong smoothness duality,,"A function $f$ is said to be strongly convex with respect to a norm $\|\cdot\|$ at a point $y$ if $f(x) \geq f(y) + \nabla f(y)^T(x-y) + \frac{1}{2}\|x-y\|^2.$ It is said to be strongly smooth with respect to a norm $\|\cdot\|$ at a point $x$ if $f(y) \leq f(x) + \nabla f(x)^T (y-x) + \frac{1}{2} \|y-x\|^2$. The Fenchel dual of a convex function $f: \mathbb{R}^n \to \mathbb{R} $ is defined as: $f^*(x) = \max_y x^T y - f(y)$. Now, it's a general fact that if $f$ is strongly convex with respect to some norm $\|\cdot\|$ everywhere, then $f^*$ is strongly smooth with respect to the dual norm $\|\cdot\|^*$ everywhere. However, I was wondering if it is also true pointwise. Specifically, if $f$ is strongly convex at its minimum $x_0$, can one say that $f^*$ is strongly smooth at 0?","A function $f$ is said to be strongly convex with respect to a norm $\|\cdot\|$ at a point $y$ if $f(x) \geq f(y) + \nabla f(y)^T(x-y) + \frac{1}{2}\|x-y\|^2.$ It is said to be strongly smooth with respect to a norm $\|\cdot\|$ at a point $x$ if $f(y) \leq f(x) + \nabla f(x)^T (y-x) + \frac{1}{2} \|y-x\|^2$. The Fenchel dual of a convex function $f: \mathbb{R}^n \to \mathbb{R} $ is defined as: $f^*(x) = \max_y x^T y - f(y)$. Now, it's a general fact that if $f$ is strongly convex with respect to some norm $\|\cdot\|$ everywhere, then $f^*$ is strongly smooth with respect to the dual norm $\|\cdot\|^*$ everywhere. However, I was wondering if it is also true pointwise. Specifically, if $f$ is strongly convex at its minimum $x_0$, can one say that $f^*$ is strongly smooth at 0?",,"['analysis', 'convex-analysis']"
43,"How prove there exist $(a,b)$ such $f'^2_{x}(a,b)+f'^2_{y}(a,b)-4(a^2+b^2)=0$",How prove there exist  such,"(a,b) f'^2_{x}(a,b)+f'^2_{y}(a,b)-4(a^2+b^2)=0","Question: let $D=\{(x,y):x^2+y^2<1\}$,and $f\in C^{1}(D)$,if   $$|f(x,y)|\le 1 ,((x,y)\in D)$$ show that:   $\exists (a,b)\in D,$$$f'^2_{x}(a,b)+f'^2_{y}(a,b)-4(a^2+b^2)=0$$ I only solve this problem: let $D=\{(x,y):x^2+y^2\le 1\}$,and $f\in C^{1}(D)$,if $$|f(x,y)|\le 1 ,((x,y)\in D)$$ show that:   $\exists (a,b)\in D,$$$f'^2_{x}(a,b)+f'^2_{y}(a,b)-16(a^2+b^2)=0$$ proof: let $$g(x,y)=f(x,y)+2(x^2+y^2)$$ if $$x^2+y^2=2,\Longrightarrow |g(x,y)|\ge 1$$   since $g(0,0)\le 1$   so   $g(x,y)$ is minimum when $x^2+y^2<1$   so $$g'_{x}(a)=0,g'_{y}(b)=0$$ so $$f'^2_{x}(a,b)+f'^2_{y}(a,b)-16(a^2+b^2)=0$$ But for the coefficient  is 4(and D is also different) ,so I   let $$g(x,y)=f(x,y)+(x^2+y^2)$$ if $x^2+y^2=1$. then $$0\le g(x,y)\le 2$$ then I can't works","Question: let $D=\{(x,y):x^2+y^2<1\}$,and $f\in C^{1}(D)$,if   $$|f(x,y)|\le 1 ,((x,y)\in D)$$ show that:   $\exists (a,b)\in D,$$$f'^2_{x}(a,b)+f'^2_{y}(a,b)-4(a^2+b^2)=0$$ I only solve this problem: let $D=\{(x,y):x^2+y^2\le 1\}$,and $f\in C^{1}(D)$,if $$|f(x,y)|\le 1 ,((x,y)\in D)$$ show that:   $\exists (a,b)\in D,$$$f'^2_{x}(a,b)+f'^2_{y}(a,b)-16(a^2+b^2)=0$$ proof: let $$g(x,y)=f(x,y)+2(x^2+y^2)$$ if $$x^2+y^2=2,\Longrightarrow |g(x,y)|\ge 1$$   since $g(0,0)\le 1$   so   $g(x,y)$ is minimum when $x^2+y^2<1$   so $$g'_{x}(a)=0,g'_{y}(b)=0$$ so $$f'^2_{x}(a,b)+f'^2_{y}(a,b)-16(a^2+b^2)=0$$ But for the coefficient  is 4(and D is also different) ,so I   let $$g(x,y)=f(x,y)+(x^2+y^2)$$ if $x^2+y^2=1$. then $$0\le g(x,y)\le 2$$ then I can't works",,['analysis']
44,"Simple proof? If $x$ lies outside a compact convex set, there exists a $y$ closer to every point in the set than $x$.","Simple proof? If  lies outside a compact convex set, there exists a  closer to every point in the set than .",x y x,"This seems rather obvious intuitively, but I can't find a simple proof. If $C$ is a compact, convex subset of $\mathbb{R}^n$ and $x \not \in C$, then there exists a point $y$ such that, for every $c$ in $C$, $\|c-y\| < \|c-x\|$. ( Related question asks to show that for every $x,c$ there exists such a $y$; this asks for a single $y$ that works for all $c$.) If it simplifies the proof, it's also interesting to prove the following instead: If $C$ is a finite subset of $\mathbb{R}^n$ and $x$ is not in the convex hull of $C$, then there exists a $y$ such that, for every $c \in C$, $\|c-y\| < \|c-x\|$. Here's an argument that I think works for the second one, but might be a pain to formalize: Let $S$ be a separating hyperplane between $x$ and $C$. Take the intersection of $S$ with the ball centered at a point $c \in C$ with radius $\|x-c\|$. This is nonempty: Otherwise, $c$ must lie on the ""other side"" of $S$, a contradiction. Do this for every $c \in C$ iteratively; at each step, the argument holds, so we are left with a nonempty set of points $y$ that satisfy the criteria. The problem with the proof is formalizing this notion that $c$ must lie on the other side of $S$. Can anyone help me fix/finish the proof, or suggest a better one? Also, I don't think this proof will extend easily to the first case, so that would be interesting to consider as well.","This seems rather obvious intuitively, but I can't find a simple proof. If $C$ is a compact, convex subset of $\mathbb{R}^n$ and $x \not \in C$, then there exists a point $y$ such that, for every $c$ in $C$, $\|c-y\| < \|c-x\|$. ( Related question asks to show that for every $x,c$ there exists such a $y$; this asks for a single $y$ that works for all $c$.) If it simplifies the proof, it's also interesting to prove the following instead: If $C$ is a finite subset of $\mathbb{R}^n$ and $x$ is not in the convex hull of $C$, then there exists a $y$ such that, for every $c \in C$, $\|c-y\| < \|c-x\|$. Here's an argument that I think works for the second one, but might be a pain to formalize: Let $S$ be a separating hyperplane between $x$ and $C$. Take the intersection of $S$ with the ball centered at a point $c \in C$ with radius $\|x-c\|$. This is nonempty: Otherwise, $c$ must lie on the ""other side"" of $S$, a contradiction. Do this for every $c \in C$ iteratively; at each step, the argument holds, so we are left with a nonempty set of points $y$ that satisfy the criteria. The problem with the proof is formalizing this notion that $c$ must lie on the other side of $S$. Can anyone help me fix/finish the proof, or suggest a better one? Also, I don't think this proof will extend easily to the first case, so that would be interesting to consider as well.",,"['analysis', 'convex-analysis']"
45,$\mathcal{H}$ is relatively compact iff every sequence in $\mathcal{H}$ has a convergent subsequence?,is relatively compact iff every sequence in  has a convergent subsequence?,\mathcal{H} \mathcal{H},"I'm trying to prove that Let $(Y,\rho_{Y}),(K,\rho_{K})$ a complete metric space and a compact metric space, respectively. Let, as well, $Z=\mathcal{C}^{0}(K,Y)$ the metric space of continuous fuctions, such that $K\longrightarrow Y$, with the uniform metric and $\mathcal{H}\subset Z$. $\mathcal{H}$ is relatively compact in $Z$ iff for every $\{f_{k}\}$, such that $f_{k}\in\mathcal{H}$, there is a subsequence $\{f_{k_{j}}\}$ that converges in $Z$, that is $f_{k_{j}}\rightarrow\varphi\in Z$. The proof I'm trying reads: Suppose $\mathcal{H}$ is relatively compact in $Z$. Then $\overline{\mathcal{H}}$ is compact. Therefore, every sequence in $\overline{\mathcal{H}}$ has a subsequence that converges in $\overline{\mathcal{H}}$. That, particularly, means that every sequence in $\mathcal{H}$, that is a sequence in $\overline{\mathcal{H}}$, has a subsequence that converges in $\overline{\mathcal{H}}$. Then, for every $\{f_{k}\}$, such that $f_{k}\in\mathcal{H}$, there is a subsequence $\{f_{k_{j}}\}$ that converges in $Z$, that is $f_{k_{j}}\rightarrow\varphi\in\overline{\mathcal{H}} \subseteq Z$. Conversely, suppose that for every $\{f_{k}\}$, such that $f_{k}\in\mathcal{H}$, there is a subsequence $\{f_{k_{j}}\}$ that converges in $Z$, that is $f_{k_{j}}\rightarrow\varphi\in Z$. We want to show that $\mathcal{H}$ is relatively compact, or what is the same, show that $\overline{\mathcal{H}}$ is compact. Now $\overline{\mathcal{H}}\subset Z$, but $Z$ is complete and $\overline{\mathcal{H}}$ is closed (since closure is closed), therefore $\overline{\mathcal{H}}$ is complete. Then it only remains to show that $\overline{\mathcal{H}}$ is totally bounded, for if it is then $\overline{\mathcal{H}}$ is compact. My question is how to show this using the hypothesis that every sequence has a subsequence that converges in $Z$? Thanks to the answers of @BrianMScott and @BenjaminLim. In the next lines I complete the proof I was doing with their useful hints, comments are welcome Proof: Suppose $\mathcal{H}$ is relatively compact in $Z$. Then $\overline{\mathcal{H}}$ is compact. Therefore, every sequence in $\overline{\mathcal{H}}$ has a subsequence that converges in $\overline{\mathcal{H}}$. That, particularly, means that every sequence in $\mathcal{H}$, that is a sequence in $\overline{\mathcal{H}}$, has a subsequence that converges in $\overline{\mathcal{H}}$. Then, for every $\{f_{k}\}$, such that $f_{k}\in\mathcal{H}$, there is a subsequence $\{f_{k_{j}}\}$ that converges in $Z$, that is $f_{k_{j}}\rightarrow\varphi\in\overline{\mathcal{H}} \subseteq Z$. Conversely, suppose that for every $\{f_{k}\}$, such that $f_{k}\in\mathcal{H}$, there is a subsequence $\{f_{k_{j}}\}$ that converges in $Z$, that is $f_{k_{j}}\rightarrow\varphi\in Z$. We want to show that $\mathcal{H}$ is relatively compact, or what is the same, show that $\overline{\mathcal{H}}$ is compact. Now $\overline{\mathcal{H}}\subset Z$, but $Z$ is complete and $\overline{\mathcal{H}}$ is closed (since closure is closed), therefore $\overline{\mathcal{H}}$ is complete. Then it only remains to show that $\overline{\mathcal{H}}$ is totally bounded, for if it is then $\overline{\mathcal{H}}$ is compact. But, for that, we need only to prove that $\mathcal{H}$ is totally bounded, since closure of a totally bounded set is totally bounded. Suppose that $\mathcal{H}$ is not totally bounded, then exists $\epsilon>0$ such that there isn't a finite covering of balls, for $\mathcal{H}$, of the form $\{B_{g_{i},\epsilon}^{\rho_{\infty}}\}$ with $g_{i}\in\mathcal{H}$. Therefore, we can choose a $f_{1}\in\mathcal{H}$ and there will be a $f_{2}\in\mathcal{H}$ such that $f_{2}\notin B_{f_{1},\epsilon}^{\rho_{\infty}}$. In the same way there will be a $f_{3}\notin B_{f_{1},\epsilon}^{\rho_{\infty}}\cup B_{f_{2},\epsilon}^{\rho_{\infty}}$ and in general there will be $f_{k}\notin\bigcup_{i=1}^{k-1}B_{f_{i},\epsilon}^{\rho}$. Hence, we have defined, inductively, a sequence that there is $\epsilon>0$ such that $\rho(f_{k},f_{\ell})\geq\epsilon$ for every $k\neq\ell$, since if $\rho(f_{k},f_{\ell})<\epsilon$ for sufficiently large $k,\ell$ therefore $f(k)\in B_{f_{\ell},\epsilon}^{\rho_{\infty}}$ which contradicts the former construction. Therefore, every subsequence is not Cauchy and therefore none of them has the chance to converge. But by hypothesis, every sequence in $\mathcal{H}$ has a convergent subsequence, and supposing that $\mathcal{H}$ was not totally bounded has led us to contradiction. Therefore $\mathcal{H}$ is totally bounded in $Z$ and, hence, $\overline{\mathcal{H}}$. It proves that, since $\overline{\mathcal{H}}$ is complete and totally bounded, $\overline{\mathcal{H}}$ is compact. Finally, $\mathcal{H}$ is relatively compact.","I'm trying to prove that Let $(Y,\rho_{Y}),(K,\rho_{K})$ a complete metric space and a compact metric space, respectively. Let, as well, $Z=\mathcal{C}^{0}(K,Y)$ the metric space of continuous fuctions, such that $K\longrightarrow Y$, with the uniform metric and $\mathcal{H}\subset Z$. $\mathcal{H}$ is relatively compact in $Z$ iff for every $\{f_{k}\}$, such that $f_{k}\in\mathcal{H}$, there is a subsequence $\{f_{k_{j}}\}$ that converges in $Z$, that is $f_{k_{j}}\rightarrow\varphi\in Z$. The proof I'm trying reads: Suppose $\mathcal{H}$ is relatively compact in $Z$. Then $\overline{\mathcal{H}}$ is compact. Therefore, every sequence in $\overline{\mathcal{H}}$ has a subsequence that converges in $\overline{\mathcal{H}}$. That, particularly, means that every sequence in $\mathcal{H}$, that is a sequence in $\overline{\mathcal{H}}$, has a subsequence that converges in $\overline{\mathcal{H}}$. Then, for every $\{f_{k}\}$, such that $f_{k}\in\mathcal{H}$, there is a subsequence $\{f_{k_{j}}\}$ that converges in $Z$, that is $f_{k_{j}}\rightarrow\varphi\in\overline{\mathcal{H}} \subseteq Z$. Conversely, suppose that for every $\{f_{k}\}$, such that $f_{k}\in\mathcal{H}$, there is a subsequence $\{f_{k_{j}}\}$ that converges in $Z$, that is $f_{k_{j}}\rightarrow\varphi\in Z$. We want to show that $\mathcal{H}$ is relatively compact, or what is the same, show that $\overline{\mathcal{H}}$ is compact. Now $\overline{\mathcal{H}}\subset Z$, but $Z$ is complete and $\overline{\mathcal{H}}$ is closed (since closure is closed), therefore $\overline{\mathcal{H}}$ is complete. Then it only remains to show that $\overline{\mathcal{H}}$ is totally bounded, for if it is then $\overline{\mathcal{H}}$ is compact. My question is how to show this using the hypothesis that every sequence has a subsequence that converges in $Z$? Thanks to the answers of @BrianMScott and @BenjaminLim. In the next lines I complete the proof I was doing with their useful hints, comments are welcome Proof: Suppose $\mathcal{H}$ is relatively compact in $Z$. Then $\overline{\mathcal{H}}$ is compact. Therefore, every sequence in $\overline{\mathcal{H}}$ has a subsequence that converges in $\overline{\mathcal{H}}$. That, particularly, means that every sequence in $\mathcal{H}$, that is a sequence in $\overline{\mathcal{H}}$, has a subsequence that converges in $\overline{\mathcal{H}}$. Then, for every $\{f_{k}\}$, such that $f_{k}\in\mathcal{H}$, there is a subsequence $\{f_{k_{j}}\}$ that converges in $Z$, that is $f_{k_{j}}\rightarrow\varphi\in\overline{\mathcal{H}} \subseteq Z$. Conversely, suppose that for every $\{f_{k}\}$, such that $f_{k}\in\mathcal{H}$, there is a subsequence $\{f_{k_{j}}\}$ that converges in $Z$, that is $f_{k_{j}}\rightarrow\varphi\in Z$. We want to show that $\mathcal{H}$ is relatively compact, or what is the same, show that $\overline{\mathcal{H}}$ is compact. Now $\overline{\mathcal{H}}\subset Z$, but $Z$ is complete and $\overline{\mathcal{H}}$ is closed (since closure is closed), therefore $\overline{\mathcal{H}}$ is complete. Then it only remains to show that $\overline{\mathcal{H}}$ is totally bounded, for if it is then $\overline{\mathcal{H}}$ is compact. But, for that, we need only to prove that $\mathcal{H}$ is totally bounded, since closure of a totally bounded set is totally bounded. Suppose that $\mathcal{H}$ is not totally bounded, then exists $\epsilon>0$ such that there isn't a finite covering of balls, for $\mathcal{H}$, of the form $\{B_{g_{i},\epsilon}^{\rho_{\infty}}\}$ with $g_{i}\in\mathcal{H}$. Therefore, we can choose a $f_{1}\in\mathcal{H}$ and there will be a $f_{2}\in\mathcal{H}$ such that $f_{2}\notin B_{f_{1},\epsilon}^{\rho_{\infty}}$. In the same way there will be a $f_{3}\notin B_{f_{1},\epsilon}^{\rho_{\infty}}\cup B_{f_{2},\epsilon}^{\rho_{\infty}}$ and in general there will be $f_{k}\notin\bigcup_{i=1}^{k-1}B_{f_{i},\epsilon}^{\rho}$. Hence, we have defined, inductively, a sequence that there is $\epsilon>0$ such that $\rho(f_{k},f_{\ell})\geq\epsilon$ for every $k\neq\ell$, since if $\rho(f_{k},f_{\ell})<\epsilon$ for sufficiently large $k,\ell$ therefore $f(k)\in B_{f_{\ell},\epsilon}^{\rho_{\infty}}$ which contradicts the former construction. Therefore, every subsequence is not Cauchy and therefore none of them has the chance to converge. But by hypothesis, every sequence in $\mathcal{H}$ has a convergent subsequence, and supposing that $\mathcal{H}$ was not totally bounded has led us to contradiction. Therefore $\mathcal{H}$ is totally bounded in $Z$ and, hence, $\overline{\mathcal{H}}$. It proves that, since $\overline{\mathcal{H}}$ is complete and totally bounded, $\overline{\mathcal{H}}$ is compact. Finally, $\mathcal{H}$ is relatively compact.",,['analysis']
46,An estimate of a series,An estimate of a series,,"Suppose $s$ is not an integer, let $\lambda(s)=\min_{n≥0}|s+n|$. Show that $\sum\limits_{n=1}^{\infty}(\frac{1}{n+s}-\frac{1}{n})\ll\frac{1}{\lambda(s)}+\log(|s|+2)$.","Suppose $s$ is not an integer, let $\lambda(s)=\min_{n≥0}|s+n|$. Show that $\sum\limits_{n=1}^{\infty}(\frac{1}{n+s}-\frac{1}{n})\ll\frac{1}{\lambda(s)}+\log(|s|+2)$.",,"['analysis', 'asymptotics', 'analytic-number-theory']"
47,Gowers norm - gap between $U_3$ and $U_4$ norms?,Gowers norm - gap between  and  norms?,U_3 U_4,"For a function $f$, it is known that $|f|_{U_2} \le |f|_{U_3} \le |f|_{U_4} \le\dots$ Is there an example for a function $f$ such that $|f|_{U_3} < |f|_{U_4}$ (i.e. they are not equal?). The bigger the gap between them, the better. I think there is a known construction of Gowers doing it but have no reference.","For a function $f$, it is known that $|f|_{U_2} \le |f|_{U_3} \le |f|_{U_4} \le\dots$ Is there an example for a function $f$ such that $|f|_{U_3} < |f|_{U_4}$ (i.e. they are not equal?). The bigger the gap between them, the better. I think there is a known construction of Gowers doing it but have no reference.",,"['number-theory', 'analysis', 'reference-request']"
48,Abelian theorem regarding Riesz summability,Abelian theorem regarding Riesz summability,,"This is my first time to post something here. If there is anything wrong, please inform me... Anyway, here is my question: Let $k$ be a nonnegative integer. We say a sequence $(a_n)$ is $(R, k)$-summable to $a$ if $ \displaystyle \lim_{x\to\infty} \sum_{n \leq x} a_n \left( 1 - \frac{\log n}{\log x} \right)^k = a,$ and we denote $\sum a_n = a \ (R, k)$. It is easy to show that $(R, 0)$-summability is equivalent to the ordinary summability, and $\sum a_n = a \ (R, k)$ implies $\sum a_n = a \ (R, j)$ for all $j \geq k$. My question here is like this: Let $\alpha (s) = \sum a_n n^{-s}$. If $\sum a_n = a \ (R, k)$, then does the limit $\lim_{s \to 0^+} \alpha (s)$ exist? If so, then does the limit coincide with $a$? I was able to prove that $\alpha (s)$ can by analytically continued for $\Re (s) > 0$, and for this continuation, we have $\alpha (s) \to a$ as $s \to 0^{+}$. But it is not immediate, or even not sure if this guarantees the convergence of $\sum a_n n^{-s}$ for $\Re (s) > 0$. Or is there any other way to proof of disprove that $\sum a_n n^{-s}$ exists for $\Re (s) > 0$?","This is my first time to post something here. If there is anything wrong, please inform me... Anyway, here is my question: Let $k$ be a nonnegative integer. We say a sequence $(a_n)$ is $(R, k)$-summable to $a$ if $ \displaystyle \lim_{x\to\infty} \sum_{n \leq x} a_n \left( 1 - \frac{\log n}{\log x} \right)^k = a,$ and we denote $\sum a_n = a \ (R, k)$. It is easy to show that $(R, 0)$-summability is equivalent to the ordinary summability, and $\sum a_n = a \ (R, k)$ implies $\sum a_n = a \ (R, j)$ for all $j \geq k$. My question here is like this: Let $\alpha (s) = \sum a_n n^{-s}$. If $\sum a_n = a \ (R, k)$, then does the limit $\lim_{s \to 0^+} \alpha (s)$ exist? If so, then does the limit coincide with $a$? I was able to prove that $\alpha (s)$ can by analytically continued for $\Re (s) > 0$, and for this continuation, we have $\alpha (s) \to a$ as $s \to 0^{+}$. But it is not immediate, or even not sure if this guarantees the convergence of $\sum a_n n^{-s}$ for $\Re (s) > 0$. Or is there any other way to proof of disprove that $\sum a_n n^{-s}$ exists for $\Re (s) > 0$?",,"['analysis', 'analytic-number-theory']"
49,Fourier transform of $\frac{1}{|x|}$ on $\mathbb{T}^{n}$,Fourier transform of  on,\frac{1}{|x|} \mathbb{T}^{n},"Let $|x|=\sqrt{x_1^2+\cdots+x_n^2} ~$ for $~x\in\mathbb{T}^n=[-\frac{\pi}{2},\frac{\pi}{2}]^n$ , $~n\geq 2$ . We can define the Fourier transform of $f(x)=\frac{1}{|x|}$ as $$ \hat{f}(k)= \int_{\mathbb{T}^n}\frac{e^{2ik\cdot x}}{|x|} dx$$ for $k\in\mathbb{Z}^n$ , since $f\in L^{1}(\mathbb{T}^n)$ if $n\geq 2$ . My question: is $\hat{f}\in l^{p}(\mathbb{Z^n})$ for $p=1$ , or other possible value of $p$ ? By Haursdorff-Young's inequality, we can partially answer this question. But I want some refined estimates. Another related function is $g(x)=\frac{1}{w(x)}$ , where $$ w(x)=\sqrt{\sin^{2}\left({x_1}\right)+\sin^{2}\left({x_2}\right)+\cdots+\sin^{2}\left({x_n}\right)}.$$ And I have the same question to $\hat{g}$ . In my point of view, the property of $\hat{g}$ is more important. If we can obtain the sharp upper decay of $\hat{f}$ or $\hat{g}$ , or we can prove that $\hat{g}\in l^1(\mathbb{Z^n})$ , that will be excellent.","Let for , . We can define the Fourier transform of as for , since if . My question: is for , or other possible value of ? By Haursdorff-Young's inequality, we can partially answer this question. But I want some refined estimates. Another related function is , where And I have the same question to . In my point of view, the property of is more important. If we can obtain the sharp upper decay of or , or we can prove that , that will be excellent.","|x|=\sqrt{x_1^2+\cdots+x_n^2} ~ ~x\in\mathbb{T}^n=[-\frac{\pi}{2},\frac{\pi}{2}]^n ~n\geq 2 f(x)=\frac{1}{|x|}  \hat{f}(k)= \int_{\mathbb{T}^n}\frac{e^{2ik\cdot x}}{|x|} dx k\in\mathbb{Z}^n f\in L^{1}(\mathbb{T}^n) n\geq 2 \hat{f}\in l^{p}(\mathbb{Z^n}) p=1 p g(x)=\frac{1}{w(x)}  w(x)=\sqrt{\sin^{2}\left({x_1}\right)+\sin^{2}\left({x_2}\right)+\cdots+\sin^{2}\left({x_n}\right)}. \hat{g} \hat{g} \hat{f} \hat{g} \hat{g}\in l^1(\mathbb{Z^n})","['analysis', 'fourier-analysis', 'fourier-series', 'fourier-transform', 'harmonic-analysis']"
50,Continuation of functions beyond natural boundaries,Continuation of functions beyond natural boundaries,,"The article Continuation of functions beyond natural boundaries by John L. Gammel states I am particularly interested in the convergence of the $[N/N+1]$ Padé approximants beyond the natural boundary, since, as is well known, Borel [2] has shown that there exists a kind of analytic continuation which differs from the usual kind (the theory of the usual kind is due to Weierstrass), and Borel made use of examples such as the ones studied here in showing that in some cases it is possible to continue functions beyond what Weierstrass called natural boundaries. I am interested in these examples because they seem to me suggestive of the direction in which comprehensive theorems about the domains in which Padé approximants converge and theorems about to what they converge are to be sought. where [2] references E. Borel, Lecons sur les fonctions monogènes d'une variable complexe , Gauthier-Villars, Paris, 1917. Unfortunately, it is in French and inaccessible to me. Does anyone know what technique of Borel Gammel is referring to? How is it possible to continue a function beyond its natural boundary?","The article Continuation of functions beyond natural boundaries by John L. Gammel states I am particularly interested in the convergence of the Padé approximants beyond the natural boundary, since, as is well known, Borel [2] has shown that there exists a kind of analytic continuation which differs from the usual kind (the theory of the usual kind is due to Weierstrass), and Borel made use of examples such as the ones studied here in showing that in some cases it is possible to continue functions beyond what Weierstrass called natural boundaries. I am interested in these examples because they seem to me suggestive of the direction in which comprehensive theorems about the domains in which Padé approximants converge and theorems about to what they converge are to be sought. where [2] references E. Borel, Lecons sur les fonctions monogènes d'une variable complexe , Gauthier-Villars, Paris, 1917. Unfortunately, it is in French and inaccessible to me. Does anyone know what technique of Borel Gammel is referring to? How is it possible to continue a function beyond its natural boundary?",[N/N+1],"['analysis', 'analytic-functions', 'analytic-continuation', 'pade-approximation', 'lacunary-series']"
51,Confusion with contraposition of an implication with quantifiers,Confusion with contraposition of an implication with quantifiers,,"I'm learning about functions and more specific, surjections and injections. However I'm a little bit confused regarding the use of quantifiers and ""contraposition"". Example: A function $ f: X \rightarrow Y$ is injective if $ \forall x_1, x_2 \in X: x_1 \not = x_2 \implies f(x_1) \not = f(x_2) $ , My book says this is equivalent to the contrapostion, that is: $ \forall x_1, x_2 \in X: f(x_1)  = f(x_2) \implies x_1 = x_2 $ . So in this case the quantifiers don't change. However if we look at this example, which is an excercise my professor wrote out for us. The original implication is: $$\forall B \in P(Y): \forall C \in P(Y): f^{-1}(B) \subset f^{-1}(C) \implies B \subset C \implies f $$ is surjective And our professor said that the contraposition of this implication is: $f$ is not surjective $$ \implies  \exists B \in P(Y): \exists C \in P(Y): \neg \left[ f^{-1}(B) \subset f^{-1}(C) \implies B \subset C \right] $$ So in this case the quantifiers do change. I can't seem to find the reason why sometimes they change and other times they don't. If anyone could clarify, I would highly appreciate it!","I'm learning about functions and more specific, surjections and injections. However I'm a little bit confused regarding the use of quantifiers and ""contraposition"". Example: A function is injective if , My book says this is equivalent to the contrapostion, that is: . So in this case the quantifiers don't change. However if we look at this example, which is an excercise my professor wrote out for us. The original implication is: is surjective And our professor said that the contraposition of this implication is: is not surjective So in this case the quantifiers do change. I can't seem to find the reason why sometimes they change and other times they don't. If anyone could clarify, I would highly appreciate it!"," f: X \rightarrow Y  \forall x_1, x_2 \in X: x_1 \not = x_2 \implies f(x_1) \not = f(x_2)   \forall x_1, x_2 \in X: f(x_1)  = f(x_2) \implies x_1 = x_2  \forall B \in P(Y): \forall C \in P(Y): f^{-1}(B) \subset f^{-1}(C) \implies B \subset C \implies f  f  \implies  \exists B \in P(Y): \exists C \in P(Y): \neg \left[ f^{-1}(B) \subset f^{-1}(C) \implies B \subset C \right] ","['analysis', 'logic', 'proof-writing', 'quantifiers']"
52,"Clarke's tangent cone, Bouligand's tangent cone, and set regularity","Clarke's tangent cone, Bouligand's tangent cone, and set regularity",,"For a set $C$ (which may not be convex) and a point $x\in C$ : Bouligand's Tangent cone is defined as $$ T(C,x) = \left\{v : \lim_{\theta\to 0_+} \inf \frac{d(x+\theta v, C)}{\theta} = 0\right\} $$ and where $d(x,C) = \min_{y\in C} \|x-y\|$ the distance from a point to a set. Clarke's tangent cone is $$ T_C(C,x) = \left\{ v : \lim_{y\to x, y\in C, \theta\to 0_+} \frac{d(y+\theta v,C)}{\theta} = 0 \right\} $$ A set is regular if $T(x,C) = T_C(x,C)$ for all $x\in C$ . My questions are a bit general, as I'm trying to build intuition. If a set is convex, then is it always regular? (Including possibly infinitely-dimensional sets? What if we restrict to finite-dimensional sets?) Would it be fair to say that here, both definitions boil down to the ""usual"" tangent cone definition, e.g. $$ T_0(C,x) = \lim_{r\to 0}\mathrm{cone}(\{y\in C: \|x-y\|\leq r\}) $$ Do funny things happen if $C$ is a low dimensional subspace (e.g. convex but unbounded and with empty interior?) Now assume that I have a set which is nonconvex, shaped like a cashew (e.g. no nonsmooth points.) Then it seems like the tangent cone at any point is just a halfspace, using either definition. Does this seem true? Now assume that I have a set which is ""pointy"" and nonconvex, like Pacman. In particular, take $x$ to be the point most inside Pacman's mouth. More precisely, consider $$ C = \{x : \|x\| \leq 1\} \cap \{x : \angle(x_2,x_1) > \alpha \text{ or }\angle (x_2,x_1) < \alpha\} $$ for some $\pi/2 > \alpha > 0$ ,  and take $x = 0$ . I suppose the tangent cone, using either definition, at this point, is the set $\{x : \angle(x_2,x_1) > \alpha \text{ or }\angle (x_2,x_1) < \alpha\}$ , and the normal cone, defined as the polar of the tangent cone, is empty (in both definitions). Does this sound sensible? Finally, the main question is: what is an example of a set which is not regular? I presume such sets must be nonconvex; can they also be finite-dimensional? What about compact / closed / bounded? Thanks for any discussion!","For a set (which may not be convex) and a point : Bouligand's Tangent cone is defined as and where the distance from a point to a set. Clarke's tangent cone is A set is regular if for all . My questions are a bit general, as I'm trying to build intuition. If a set is convex, then is it always regular? (Including possibly infinitely-dimensional sets? What if we restrict to finite-dimensional sets?) Would it be fair to say that here, both definitions boil down to the ""usual"" tangent cone definition, e.g. Do funny things happen if is a low dimensional subspace (e.g. convex but unbounded and with empty interior?) Now assume that I have a set which is nonconvex, shaped like a cashew (e.g. no nonsmooth points.) Then it seems like the tangent cone at any point is just a halfspace, using either definition. Does this seem true? Now assume that I have a set which is ""pointy"" and nonconvex, like Pacman. In particular, take to be the point most inside Pacman's mouth. More precisely, consider for some ,  and take . I suppose the tangent cone, using either definition, at this point, is the set , and the normal cone, defined as the polar of the tangent cone, is empty (in both definitions). Does this sound sensible? Finally, the main question is: what is an example of a set which is not regular? I presume such sets must be nonconvex; can they also be finite-dimensional? What about compact / closed / bounded? Thanks for any discussion!","C x\in C 
T(C,x) = \left\{v : \lim_{\theta\to 0_+} \inf \frac{d(x+\theta v, C)}{\theta} = 0\right\}
 d(x,C) = \min_{y\in C} \|x-y\| 
T_C(C,x) = \left\{ v : \lim_{y\to x, y\in C, \theta\to 0_+} \frac{d(y+\theta v,C)}{\theta} = 0 \right\}
 T(x,C) = T_C(x,C) x\in C 
T_0(C,x) = \lim_{r\to 0}\mathrm{cone}(\{y\in C: \|x-y\|\leq r\})
 C x 
C = \{x : \|x\| \leq 1\} \cap \{x : \angle(x_2,x_1) > \alpha \text{ or }\angle (x_2,x_1) < \alpha\}
 \pi/2 > \alpha > 0 x = 0 \{x : \angle(x_2,x_1) > \alpha \text{ or }\angle (x_2,x_1) < \alpha\}","['analysis', 'convex-analysis', 'non-convex-optimization']"
53,Integrating $1/x$ on unit sphere (quaternions),Integrating  on unit sphere (quaternions),1/x,"I have an issue with integrating over the unit sphere in $\mathbb{H}$ (quaternions). The integral is : $$\oint_{\mathbb{S}^2} \frac{1}{q} dq$$ with $$q\in \mathbb{H}, q=ia+jb+kc, (a,b,c) \in \mathbb{R}^3$$ yet we do not use the real numbers to have a 3-dimensional space. So I used that property : if we have $$ \begin{align*}   \gamma \colon & [a;b] \to \mathbb{C}\\   &t \mapsto \gamma(t). \end{align*} $$ then $$\int_\gamma f(\zeta) d\zeta = \int_a^b f(\gamma(\xi)) \gamma'(\xi) d\xi$$ Note that I supposed that we can adapt this property to quaternions. So, after changing this in a ""normal"" integral, I got : $$\int_{-\pi / 2}^{\pi / 2} \int_{- \pi}^{\pi} \frac{i \cos^2(\theta) \cos(\phi)-j \cos^2(\theta)\sin(\phi)-k\cos(\theta)\sin(\theta)\cos^2(\phi)+k\cos(\theta)\sin(\theta)\sin^2(\phi) }{i\cos(\theta)\cos(\phi) + j\cos(\theta)\sin(\phi)+k\sin(\theta)} d\phi d\theta$$ with using the parametrization of unit sphere : $$\mathbb{S}^2(\theta,\phi) : \left\{ \begin{align*}   x(t) &= \cos(\theta)\cos(\phi) \\   y(t) &= \cos(\theta)\sin(\phi) \\   z(t) &= \sin(\theta) \end{align*} \right\} $$ and so $$ \mathbb{S}^2(\theta,\phi)= \begin{align*}   &i \cos(\theta)\cos(\phi) \\   + &j \cos(\theta)\sin(\phi) \\   + &k \sin(\theta) \end{align*} $$ yet we do not use the real numbers. And the quaternion-adaptation for the property seen above : $$\int_{-\pi / 2}^{\pi / 2} \int_{- \pi}^{\pi} \frac{(d \mathbb{S}^2(\theta,\phi)/d\theta)(d \mathbb{S}^2(\theta,\phi)/d\phi)}{\mathbb{S}^2(\theta,\phi)} d\phi d\theta$$ And there's the issue : The result is divergent, however, we easily can see than $1/q$ has only one pole (or singularity) at $q=0$ yet $\mathbb{S}^2$ does not pass trought this point. I certainly make a mistake but I am not able to determine which even if I strongly think that the mistake is about the quaternion-adaptation of the seen property. Thanks you for helping.","I have an issue with integrating over the unit sphere in (quaternions). The integral is : with yet we do not use the real numbers to have a 3-dimensional space. So I used that property : if we have then Note that I supposed that we can adapt this property to quaternions. So, after changing this in a ""normal"" integral, I got : with using the parametrization of unit sphere : and so yet we do not use the real numbers. And the quaternion-adaptation for the property seen above : And there's the issue : The result is divergent, however, we easily can see than has only one pole (or singularity) at yet does not pass trought this point. I certainly make a mistake but I am not able to determine which even if I strongly think that the mistake is about the quaternion-adaptation of the seen property. Thanks you for helping.","\mathbb{H} \oint_{\mathbb{S}^2} \frac{1}{q} dq q\in \mathbb{H}, q=ia+jb+kc, (a,b,c) \in \mathbb{R}^3 
\begin{align*}
  \gamma \colon & [a;b] \to \mathbb{C}\\
  &t \mapsto \gamma(t).
\end{align*}
 \int_\gamma f(\zeta) d\zeta = \int_a^b f(\gamma(\xi)) \gamma'(\xi) d\xi \int_{-\pi / 2}^{\pi / 2} \int_{- \pi}^{\pi} \frac{i \cos^2(\theta) \cos(\phi)-j \cos^2(\theta)\sin(\phi)-k\cos(\theta)\sin(\theta)\cos^2(\phi)+k\cos(\theta)\sin(\theta)\sin^2(\phi) }{i\cos(\theta)\cos(\phi) + j\cos(\theta)\sin(\phi)+k\sin(\theta)} d\phi d\theta \mathbb{S}^2(\theta,\phi) : \left\{
\begin{align*}
  x(t) &= \cos(\theta)\cos(\phi) \\
  y(t) &= \cos(\theta)\sin(\phi) \\
  z(t) &= \sin(\theta)
\end{align*}
\right\}   \mathbb{S}^2(\theta,\phi)=
\begin{align*}
  &i \cos(\theta)\cos(\phi) \\
  + &j \cos(\theta)\sin(\phi) \\
  + &k \sin(\theta)
\end{align*}
 \int_{-\pi / 2}^{\pi / 2} \int_{- \pi}^{\pi} \frac{(d \mathbb{S}^2(\theta,\phi)/d\theta)(d \mathbb{S}^2(\theta,\phi)/d\phi)}{\mathbb{S}^2(\theta,\phi)} d\phi d\theta 1/q q=0 \mathbb{S}^2","['analysis', 'quaternions']"
54,Prove that $x_1\ln x_1+x_2\ln x_2 +x_1+x_2+2x_1^2+2x_2^2>0$,Prove that,x_1\ln x_1+x_2\ln x_2 +x_1+x_2+2x_1^2+2x_2^2>0,"Suppose we have $x_1\ln x_1+x_1^2=x_2\ln x_2 +x_2^2$ with $0<x_1<x_2$ . Then $x_1\ln x_1+x_2\ln x_2 +x_1+x_2+2x_1^2+2x_2^2>0$ ? I am not sure the conclusion is true or wrong. From math software, it seems it is true. I have no idea how to deal with the problem, I am willing to prove the statement is true, but no result. Any comment would be helpful.","Suppose we have with . Then ? I am not sure the conclusion is true or wrong. From math software, it seems it is true. I have no idea how to deal with the problem, I am willing to prove the statement is true, but no result. Any comment would be helpful.",x_1\ln x_1+x_1^2=x_2\ln x_2 +x_2^2 0<x_1<x_2 x_1\ln x_1+x_2\ln x_2 +x_1+x_2+2x_1^2+2x_2^2>0,"['calculus', 'analysis', 'inequality']"
55,Convergent Sequence + Limit is Compact using Sequential Compactness,Convergent Sequence + Limit is Compact using Sequential Compactness,,"Proposition: Let $(X,d)$ be a metric space and $\lbrace x_n \rbrace_{n=1}^\infty \subset X$ be a convergent sequence with $x_n \rightarrow x_0, n \rightarrow \infty$. Show that $K = \lbrace x_n \mid n \in \mathbb{N} \cup \lbrace 0 \rbrace \rbrace$ is a compact set. Question: It is easy to see how to do this with open covers via the standard definition of compactness. What I am wondering is how one would prove this theorem using sequential compactness (since it is equivalent to regular compactness for metric spaces), if it is even possible to do so.","Proposition: Let $(X,d)$ be a metric space and $\lbrace x_n \rbrace_{n=1}^\infty \subset X$ be a convergent sequence with $x_n \rightarrow x_0, n \rightarrow \infty$. Show that $K = \lbrace x_n \mid n \in \mathbb{N} \cup \lbrace 0 \rbrace \rbrace$ is a compact set. Question: It is easy to see how to do this with open covers via the standard definition of compactness. What I am wondering is how one would prove this theorem using sequential compactness (since it is equivalent to regular compactness for metric spaces), if it is even possible to do so.",,"['analysis', 'metric-spaces', 'compactness', 'alternative-proof']"
56,"If the radius of convergence of the power series $a$ is positive then it ""reciprocal"" power series have positive radius of convergence","If the radius of convergence of the power series  is positive then it ""reciprocal"" power series have positive radius of convergence",a,"Im trying to solve this exercise from the book Analysis I of Amann and Escher (page 216, exercise 9). Let $a=\sum a_k X^k\in\Bbb C[\![X]\!]$ with $a_0=1$ . (a) Show that there is some $b\in\Bbb C[\![X]\!]$ such that $ab=1$ . Provide a recursive algorithm for calculating the coefficients $b_k$ . (b) Show that the radius of convergence of $\rho_b$ of $b$ is positive if $\rho_a$ of $a$ is positive. The first part is easy, we have that $b_0=1$ and $$b_n=-a_n-\sum_{k=1}^{n-1}a_kb_{n-k},\quad\forall n\ge 1$$ (with the convention that the empty sum is zero). But Im stuck in the second part. To context the exercise: this exercise comes prior to any definition of continuity, derivative or analyticity in the book, then, from this context, I dont know how to prove it or if it is provable. My work so far: let $a:=\sum a_k X^k$ a formal power series with radius of convergence $\rho_a>0$ , then for $a(x):=\sum_{k=0}^\infty a_k x^k$ for $x\in\Bbb B(0,\rho_a)$ we have $$a_0=1\le\left|\sum_{k=0}^\infty a_kx^k\right|\le \sum_{k=0}^\infty |a_k|r^k=M<\infty,\quad\forall x\in\Bbb B(0,r),\text{ with }0<r<\rho_a$$ If we define $b:=\sum b_k X^k$ such that $ab=1$ then from above we have that $$\frac1M\le\left|\sum_{k=0}^\infty b_kx^k\right|\le 1,\quad\forall x\in\Bbb B(0,r),\text{ with }0<r<\rho_a$$ but from the last expression I cannot conclude that $b$ is absolutely convergent for all $|x|<\rho_b$ for some $\rho_b$ , hence I cannot conclude that $b$ have a positive radius of convergence. Some help will be appreciated, thank you.","Im trying to solve this exercise from the book Analysis I of Amann and Escher (page 216, exercise 9). Let with . (a) Show that there is some such that . Provide a recursive algorithm for calculating the coefficients . (b) Show that the radius of convergence of of is positive if of is positive. The first part is easy, we have that and (with the convention that the empty sum is zero). But Im stuck in the second part. To context the exercise: this exercise comes prior to any definition of continuity, derivative or analyticity in the book, then, from this context, I dont know how to prove it or if it is provable. My work so far: let a formal power series with radius of convergence , then for for we have If we define such that then from above we have that but from the last expression I cannot conclude that is absolutely convergent for all for some , hence I cannot conclude that have a positive radius of convergence. Some help will be appreciated, thank you.","a=\sum a_k X^k\in\Bbb C[\![X]\!] a_0=1 b\in\Bbb C[\![X]\!] ab=1 b_k \rho_b b \rho_a a b_0=1 b_n=-a_n-\sum_{k=1}^{n-1}a_kb_{n-k},\quad\forall n\ge 1 a:=\sum a_k X^k \rho_a>0 a(x):=\sum_{k=0}^\infty a_k x^k x\in\Bbb B(0,\rho_a) a_0=1\le\left|\sum_{k=0}^\infty a_kx^k\right|\le \sum_{k=0}^\infty |a_k|r^k=M<\infty,\quad\forall x\in\Bbb B(0,r),\text{ with }0<r<\rho_a b:=\sum b_k X^k ab=1 \frac1M\le\left|\sum_{k=0}^\infty b_kx^k\right|\le 1,\quad\forall x\in\Bbb B(0,r),\text{ with }0<r<\rho_a b |x|<\rho_b \rho_b b","['analysis', 'power-series', 'absolute-convergence']"
57,What exactly is a derivative?,What exactly is a derivative?,,"In calculus courses, we learn the classical derivative: $$f'(x)=\lim_{h\to 0} \frac{f(x+h)-f(x)}{h}$$ And the directional derivative: $$D_{\mathbf{v}}{f}(\mathbf{x}) = \lim_{h \rightarrow 0}{\frac{f(\mathbf{x} + h\mathbf{v}) - f(\mathbf{x})}{h}}$$ In which the partial derivative is just a slight variation of the previous one. But some days ago, I've read Stopple's Primer of Analytic Number Theory , I've read something about the difference operator: $$\Delta f(x)=f(x+1)-f(x)$$ Which at least for me, seems quite sems quite familiar with the previous examples. The only difference is that this is not really a limiting process - another one that lacks the limiting process but is also called a derivative is the arithmetic derivative . But there are other examples in which there is a limiting process, in non-newtonian calculus, for example, we have the geometric derivative : $$f^{*}(x) = \lim_{h \to 0}{ \left({f(x+h)\over{f(x)}}\right)^{1\over{h}} }$$ And the bigeometric derivative: $$f^{*}(x) = \lim_{h \to 0}{ \left({f((1+h)x)\over{f(x)}}\right)^{1\over{h}} } =  \lim_{k \to 1}{ \left({f(kx)\over{f(x)}}\right)^{1\over{\ln(k)}} }$$ And in this site , the authors argue that: ""There are infinitely many non-Newtonian calculi. Like the classical calculus, each of them possesses, among other things: a derivative, an integral, a natural average, a special class of functions having a constant derivative, and two Fundamental Theorems which reveal that the derivative and integral are 'inversely' related."" I remember from my calculus classes that the classical derivative is a comparison of a certain function with the slope of a straight line. It seems to my limited knowledge that these other derivatives are also comparisons with some other geometrical figures, perhaps? From this book : During the Renaissance many scholars, including Galileo, discussed the following problem: Two estimates, $10$ and $1000$ , are proposed as the value of a horse. Which estimate, if any, deviates more from the true value of $100$ ? The scholars who maintained that the deviations should be measures by the differences concludes that the estimate of $10$ was closer to the actual value. However, Galileo eventual maintained that the deviations should be measured by ratios, and he concluded that the two estimates deviated equally from the true value. Other examples are also Fréchet's derivatives and Gâteaux derivatives which I'm not exactly sure of what they are. As you can see further in the book, this yields the previously mentioned geometric derivative . So, assuming there is a class of kinds of derivatives, what binds them all? How can I look at anything and decide if it's a derivative or not? I presume that if something is a derivative, then it must have - just as the authors of the mentioned website said - an integral, a natural average, a special class of functions having a constant derivative, and two Fundamental Theorems which reveal that the derivative and integral are 'inversely' related. So, given any expresion, is it a derivative if I can come up with all these items? This seems a faint answer to me, I'd like to see if there is a better one.","In calculus courses, we learn the classical derivative: And the directional derivative: In which the partial derivative is just a slight variation of the previous one. But some days ago, I've read Stopple's Primer of Analytic Number Theory , I've read something about the difference operator: Which at least for me, seems quite sems quite familiar with the previous examples. The only difference is that this is not really a limiting process - another one that lacks the limiting process but is also called a derivative is the arithmetic derivative . But there are other examples in which there is a limiting process, in non-newtonian calculus, for example, we have the geometric derivative : And the bigeometric derivative: And in this site , the authors argue that: ""There are infinitely many non-Newtonian calculi. Like the classical calculus, each of them possesses, among other things: a derivative, an integral, a natural average, a special class of functions having a constant derivative, and two Fundamental Theorems which reveal that the derivative and integral are 'inversely' related."" I remember from my calculus classes that the classical derivative is a comparison of a certain function with the slope of a straight line. It seems to my limited knowledge that these other derivatives are also comparisons with some other geometrical figures, perhaps? From this book : During the Renaissance many scholars, including Galileo, discussed the following problem: Two estimates, and , are proposed as the value of a horse. Which estimate, if any, deviates more from the true value of ? The scholars who maintained that the deviations should be measures by the differences concludes that the estimate of was closer to the actual value. However, Galileo eventual maintained that the deviations should be measured by ratios, and he concluded that the two estimates deviated equally from the true value. Other examples are also Fréchet's derivatives and Gâteaux derivatives which I'm not exactly sure of what they are. As you can see further in the book, this yields the previously mentioned geometric derivative . So, assuming there is a class of kinds of derivatives, what binds them all? How can I look at anything and decide if it's a derivative or not? I presume that if something is a derivative, then it must have - just as the authors of the mentioned website said - an integral, a natural average, a special class of functions having a constant derivative, and two Fundamental Theorems which reveal that the derivative and integral are 'inversely' related. So, given any expresion, is it a derivative if I can come up with all these items? This seems a faint answer to me, I'd like to see if there is a better one.",f'(x)=\lim_{h\to 0} \frac{f(x+h)-f(x)}{h} D_{\mathbf{v}}{f}(\mathbf{x}) = \lim_{h \rightarrow 0}{\frac{f(\mathbf{x} + h\mathbf{v}) - f(\mathbf{x})}{h}} \Delta f(x)=f(x+1)-f(x) f^{*}(x) = \lim_{h \to 0}{ \left({f(x+h)\over{f(x)}}\right)^{1\over{h}} } f^{*}(x) = \lim_{h \to 0}{ \left({f((1+h)x)\over{f(x)}}\right)^{1\over{h}} } =  \lim_{k \to 1}{ \left({f(kx)\over{f(x)}}\right)^{1\over{\ln(k)}} } 10 1000 100 10,['analysis']
58,"Prove there is a unique $y:[0,1] \to \mathbb{R}$ solving $y(x) = e^x + \frac{y(x^2)}{2}$ for $x \in [0,1].$",Prove there is a unique  solving  for,"y:[0,1] \to \mathbb{R} y(x) = e^x + \frac{y(x^2)}{2} x \in [0,1].","The title is the problem statement, but to reiterate, Prove there is a unique $y:[0,1] \to \mathbb{R}$ solving $y(x) = e^x + \frac{y(x^2)}{2}$ for $x \in [0,1].$ Looking for hints/solutions, thanks in advance. Edit 6/10/16: Progress? If we define the operator $Ty(x) = e^x + \frac{y(x^2)}{2},$ then $|Ty(x) - Tz(x)| = 1/2|y(x^2) - z(x^2)|,$ which might be a contraction map (not exactly sure how)... then we could use a fixed point theorem to conclude that for some guess $y_0,$ the series  $T^N y_0(x) = \frac{y_0(x^{2^N})}{2} + \sum_{n=0}^N \frac{\exp(x^{2^n})}{2^n}$ converges to $y(x)$? Could anybody confirm if one is allowed to claim this $$ |Ty(x) - Tz(x)| = 1/2|y(x^2) - z(x^2)| $$  is a contraction map?  On one hand, it look like $$ |Ty - Tz| = 1/2|y - z|, $$ but on the other, the arguments are not the same for $y, z$ on both sides of the equation.","The title is the problem statement, but to reiterate, Prove there is a unique $y:[0,1] \to \mathbb{R}$ solving $y(x) = e^x + \frac{y(x^2)}{2}$ for $x \in [0,1].$ Looking for hints/solutions, thanks in advance. Edit 6/10/16: Progress? If we define the operator $Ty(x) = e^x + \frac{y(x^2)}{2},$ then $|Ty(x) - Tz(x)| = 1/2|y(x^2) - z(x^2)|,$ which might be a contraction map (not exactly sure how)... then we could use a fixed point theorem to conclude that for some guess $y_0,$ the series  $T^N y_0(x) = \frac{y_0(x^{2^N})}{2} + \sum_{n=0}^N \frac{\exp(x^{2^n})}{2^n}$ converges to $y(x)$? Could anybody confirm if one is allowed to claim this $$ |Ty(x) - Tz(x)| = 1/2|y(x^2) - z(x^2)| $$  is a contraction map?  On one hand, it look like $$ |Ty - Tz| = 1/2|y - z|, $$ but on the other, the arguments are not the same for $y, z$ on both sides of the equation.",,['analysis']
59,Rudin Theorem 1.20,Rudin Theorem 1.20,,"Can someone please explain help which proposition or axiom the following step comes from.  I will highlight it in the proof. $1.20\ (a)$ If $x \in R,\ y\in R$, and $x > 0$, then there is a positive integer $n$ such that $nx > y$. Pf Let $A = \{nx\ |\ n \in Z^{+}\}$.  Suppose on the contrary that there exists no such element that is greater than $y$.  Then $y$ is an upper bound and so there must exists an $\alpha = sup\ A$.  Then since $x > 0$, $\alpha - x < \alpha$, and $\alpha - x$ is not an upper bound of $A$.  Hence $$\alpha - x < mx$$ for some positive integer $m$.  But then $$\alpha < (m+1)x \in A.$$ It seems obvious that we can say that, but I don't see yet exactly which axiom or propositions allows us to make the last statements.  Can someone please help clear this up? Thanks.","Can someone please explain help which proposition or axiom the following step comes from.  I will highlight it in the proof. $1.20\ (a)$ If $x \in R,\ y\in R$, and $x > 0$, then there is a positive integer $n$ such that $nx > y$. Pf Let $A = \{nx\ |\ n \in Z^{+}\}$.  Suppose on the contrary that there exists no such element that is greater than $y$.  Then $y$ is an upper bound and so there must exists an $\alpha = sup\ A$.  Then since $x > 0$, $\alpha - x < \alpha$, and $\alpha - x$ is not an upper bound of $A$.  Hence $$\alpha - x < mx$$ for some positive integer $m$.  But then $$\alpha < (m+1)x \in A.$$ It seems obvious that we can say that, but I don't see yet exactly which axiom or propositions allows us to make the last statements.  Can someone please help clear this up? Thanks.",,['analysis']
60,Uniqueness of Fourier Coefficients,Uniqueness of Fourier Coefficients,,"I'm reading through Stein & Shakarchi's book on Fourier Analysis on my own, and have a question about the proof of the following theorem: Suppose that $f$ is an integrable function on the circle with $\hat{f}(n)=0$ for all $n \in \mathbb{Z}$. Then $f(\theta_{0})=0$ whenever f is continuous at the point $\theta_{0}$. They prove the theorem by contradiction - assuming that $f$ satisfies the hypotheses, $\theta_{0}=0$, and $f(0)>0$. They then construct a family of trigonometric polynomials $\{p_{k}\}$ that peak at $0$, resulting in $$\int p_{k}(\theta)f(\theta)\; d\theta \rightarrow \infty \;\; \text{as}\;\; k \rightarrow \infty.$$ Here, the peak functions are constructed in the following manner: Since $f$ is continuous at $0$, we can choose $0 < \delta \leq \pi/2$, so that $f(\theta)>f(0)/2$ whenever $|\theta|< \delta$. Let $p(\theta) = \epsilon + \cos\theta$ where $\epsilon >0$ is chosen so small that $|p(\theta)|< 1-\epsilon/2$, whenever $\delta \leq |\theta| \leq \pi$. Then choose a positive $\eta$ with $\eta < \delta$, so that $p(\theta) \geq 1 + \epsilon/2$, for $|\theta| < \eta$. Finally, we define $p_{k}(\theta) = [p(\theta)]^k$. It's not exactly clear to me how this gives a contradiction, since the Fourier coefficients of $f$ are given by $$\int f(\theta)e^{-in\theta}\; d\theta.$$ Could someone explain how the first integral implies there is a nonzero Fourier coefficient?","I'm reading through Stein & Shakarchi's book on Fourier Analysis on my own, and have a question about the proof of the following theorem: Suppose that $f$ is an integrable function on the circle with $\hat{f}(n)=0$ for all $n \in \mathbb{Z}$. Then $f(\theta_{0})=0$ whenever f is continuous at the point $\theta_{0}$. They prove the theorem by contradiction - assuming that $f$ satisfies the hypotheses, $\theta_{0}=0$, and $f(0)>0$. They then construct a family of trigonometric polynomials $\{p_{k}\}$ that peak at $0$, resulting in $$\int p_{k}(\theta)f(\theta)\; d\theta \rightarrow \infty \;\; \text{as}\;\; k \rightarrow \infty.$$ Here, the peak functions are constructed in the following manner: Since $f$ is continuous at $0$, we can choose $0 < \delta \leq \pi/2$, so that $f(\theta)>f(0)/2$ whenever $|\theta|< \delta$. Let $p(\theta) = \epsilon + \cos\theta$ where $\epsilon >0$ is chosen so small that $|p(\theta)|< 1-\epsilon/2$, whenever $\delta \leq |\theta| \leq \pi$. Then choose a positive $\eta$ with $\eta < \delta$, so that $p(\theta) \geq 1 + \epsilon/2$, for $|\theta| < \eta$. Finally, we define $p_{k}(\theta) = [p(\theta)]^k$. It's not exactly clear to me how this gives a contradiction, since the Fourier coefficients of $f$ are given by $$\int f(\theta)e^{-in\theta}\; d\theta.$$ Could someone explain how the first integral implies there is a nonzero Fourier coefficient?",,"['analysis', 'fourier-analysis', 'fourier-series']"
61,Why is the case $1/2<\rho\leq 1$ trivial in proving the following inequality?,Why is the case  trivial in proving the following inequality?,1/2<\rho\leq 1,"I'm studying Elliptic Partial Differential Equations by Q. Han and F. Lin. In Lemma 1.41 is given the elliptic equation $D_j(a_{ij}D_i u)=0$ where the coefficient matrix $(a_{ij})$ is constant positive definite and elliptic (with the bound $0<\lambda\leq\Lambda$). There is an energy estimate of its weak solution $u\in C^1(B_1)$ which reads $$\int_{B_\rho}u^2\leq c\rho^n\int_{B_1}u^2$$ where $c=c(\lambda,\Lambda)$. Is there any reason that the case $1/2<\rho\leq 1$ is trivial? I can hardly find out why it is evident (to the authors). If so, why does this inequality holds in this case?","I'm studying Elliptic Partial Differential Equations by Q. Han and F. Lin. In Lemma 1.41 is given the elliptic equation $D_j(a_{ij}D_i u)=0$ where the coefficient matrix $(a_{ij})$ is constant positive definite and elliptic (with the bound $0<\lambda\leq\Lambda$). There is an energy estimate of its weak solution $u\in C^1(B_1)$ which reads $$\int_{B_\rho}u^2\leq c\rho^n\int_{B_1}u^2$$ where $c=c(\lambda,\Lambda)$. Is there any reason that the case $1/2<\rho\leq 1$ is trivial? I can hardly find out why it is evident (to the authors). If so, why does this inequality holds in this case?",,['analysis']
62,Convergence Counterexamples,Convergence Counterexamples,,"I'm trying to compile a list of counterexamples for convergence implications (or rather, the lack of). I have an incomplete list and I hope to get it all together in one piece. I'm currently working with the 5 different notions of convergence: uniformly, pointwise, a.e., $L^1$ and in measure. I know that we have the following implications: uniformly $\implies$ pointwise $\implies$ a.e. uniformly $\implies$ in measure and finally, $L^1 \implies$ in measure. That means I should have counterexamples for the following: pointwise $\not\Rightarrow$ uniformly: $f_n(x)=x^n$ on $[0,1]$ pointwise $\not\Rightarrow$ in measure: $f_n(x)=\chi_{[n,\infty)}$ ,  or $f_n(x)=\chi_{[n,n+1]}$ pointwise $\not\Rightarrow L^1$ : $f_n = 1$ when $x\in \mathbb{Q}$ and $f_n(x) = 1/n$ otherwise a.e. $\not\Rightarrow$ pointwise: $\{r_n\}_{n\in\mathbb N}$ is an enumeration of $\mathbb Q$ , $f_n(x)=(x-r_n)^{-\frac{1}{2}}$ if $r_n<x\leq r_n+1$ and $f_n(x)=0$ otherwise a.e. $\not\Rightarrow$ uniformly: $f_n(x)=x^n\chi_{[0,1]}$ a.e. $\not\Rightarrow L^1$ : $f_n(x)=\chi_{[n,n+1]}$ a.e. $\not\Rightarrow$ in measure: $f_n(x)=\chi_{[n,n+1]}$ in measure $\not\Rightarrow L^1$ : $f_n(x)=n\chi_{[0,\frac{1}n]}$ in measure $\not\Rightarrow$ uniformly: $f_n=n\chi_{[0,\frac{1}{n^2}]}$ in measure $\not\Rightarrow$ pointwise: in measure $\not\Rightarrow$ a.e.: $f_1=\chi_{[0,1]},f_2=\chi_{[0,\frac{1}{2}]},f_3=\chi_{[\frac{1}{2},1]},f_4=\chi_{[0,\frac{1}{4}]},\cdots$ $L^1\not\Rightarrow$ uniformly: $f_n=n\chi_{[0,\frac{1}{n^2}]}$ $L^1\not\Rightarrow$ pointwise: $f_n = \chi_{[0,1/n]}$ $L^1\not\Rightarrow$ a.e.: $f_n=\chi_{I_n},I_n=[j2^{-k},(j+1)2^{-k}]$ for $n=2^k+j$ uniformly $\not\Rightarrow L^1$ : $f_n(x)=\frac{1}n\chi_{[0,n]}$ As people provide solutions, I will try to update this accordingly. :)","I'm trying to compile a list of counterexamples for convergence implications (or rather, the lack of). I have an incomplete list and I hope to get it all together in one piece. I'm currently working with the 5 different notions of convergence: uniformly, pointwise, a.e., and in measure. I know that we have the following implications: uniformly pointwise a.e. uniformly in measure and finally, in measure. That means I should have counterexamples for the following: pointwise uniformly: on pointwise in measure: ,  or pointwise : when and otherwise a.e. pointwise: is an enumeration of , if and otherwise a.e. uniformly: a.e. : a.e. in measure: in measure : in measure uniformly: in measure pointwise: in measure a.e.: uniformly: pointwise: a.e.: for uniformly : As people provide solutions, I will try to update this accordingly. :)","L^1 \implies \implies \implies L^1 \implies \not\Rightarrow f_n(x)=x^n [0,1] \not\Rightarrow f_n(x)=\chi_{[n,\infty)} f_n(x)=\chi_{[n,n+1]} \not\Rightarrow L^1 f_n = 1 x\in \mathbb{Q} f_n(x) = 1/n \not\Rightarrow \{r_n\}_{n\in\mathbb N} \mathbb Q f_n(x)=(x-r_n)^{-\frac{1}{2}} r_n<x\leq r_n+1 f_n(x)=0 \not\Rightarrow f_n(x)=x^n\chi_{[0,1]} \not\Rightarrow L^1 f_n(x)=\chi_{[n,n+1]} \not\Rightarrow f_n(x)=\chi_{[n,n+1]} \not\Rightarrow L^1 f_n(x)=n\chi_{[0,\frac{1}n]} \not\Rightarrow f_n=n\chi_{[0,\frac{1}{n^2}]} \not\Rightarrow \not\Rightarrow f_1=\chi_{[0,1]},f_2=\chi_{[0,\frac{1}{2}]},f_3=\chi_{[\frac{1}{2},1]},f_4=\chi_{[0,\frac{1}{4}]},\cdots L^1\not\Rightarrow f_n=n\chi_{[0,\frac{1}{n^2}]} L^1\not\Rightarrow f_n = \chi_{[0,1/n]} L^1\not\Rightarrow f_n=\chi_{I_n},I_n=[j2^{-k},(j+1)2^{-k}] n=2^k+j \not\Rightarrow L^1 f_n(x)=\frac{1}n\chi_{[0,n]}","['analysis', 'convergence-divergence', 'examples-counterexamples', 'big-list']"
63,Where to go after Advanced Calculus 2?,Where to go after Advanced Calculus 2?,,"I will be finishing up Advanced Calculus 2 soon and I would like to continue self studying Analysis. I want to learn Real and Complex Analysis, Measure Theory and all that other good stuff. but I am not exactly sure what book to use. My class used the text An Introduction to Analysis by William R. Wade, although I would consider this book rather easy because it's exercises are quite simple. The homework problems my professor gave were much more difficult and often took problems from Rudin's Principles of Mathematical Analysis book.  Another thing worth mention is that the whole reason I got into math was from reading about the first half of Micheal Spivak's Calculus completely on my own. So (I think) I can say that I feel more comfortable with the theorem-proof format than most students who went through a class similar to mine. Anyway, I think Rudin's Real and Complex Analysis would probably be too hard for me. Basically I have been looking at the Table of Contents from books on amazon. I saw the book Analysis by Leib and Loss, but it seems geared towards students of Physics.  The table of contents of  DiBenedetto's Real Analysis seem to be what I am looking for but I think the book may be too advanced for me as it looks like it just dives right into the deep end. Knapp's Basic Real Analysis is the book I am leaning towards, but since I have very little money, I would really like some advice before shelling out $70 on the book. Any help picking out a book appropriate for my level would be greatly appreciated.","I will be finishing up Advanced Calculus 2 soon and I would like to continue self studying Analysis. I want to learn Real and Complex Analysis, Measure Theory and all that other good stuff. but I am not exactly sure what book to use. My class used the text An Introduction to Analysis by William R. Wade, although I would consider this book rather easy because it's exercises are quite simple. The homework problems my professor gave were much more difficult and often took problems from Rudin's Principles of Mathematical Analysis book.  Another thing worth mention is that the whole reason I got into math was from reading about the first half of Micheal Spivak's Calculus completely on my own. So (I think) I can say that I feel more comfortable with the theorem-proof format than most students who went through a class similar to mine. Anyway, I think Rudin's Real and Complex Analysis would probably be too hard for me. Basically I have been looking at the Table of Contents from books on amazon. I saw the book Analysis by Leib and Loss, but it seems geared towards students of Physics.  The table of contents of  DiBenedetto's Real Analysis seem to be what I am looking for but I think the book may be too advanced for me as it looks like it just dives right into the deep end. Knapp's Basic Real Analysis is the book I am leaning towards, but since I have very little money, I would really like some advice before shelling out $70 on the book. Any help picking out a book appropriate for my level would be greatly appreciated.",,"['analysis', 'reference-request', 'soft-question', 'self-learning']"
64,Landau Notation Properties,Landau Notation Properties,,I would like to take the limit $x\to 0$ and prove or disprove the following statements concerning Landau notation. (a) $O(x^{3/2})\subset o(x)\subset O(x^{1/2})$ (b) $(1+O(x))^2=1+O(x^2)$ I know the deifiniton of the O-notation and also what it means but i have no idea how to show these kinds of inequalities. I would appreciate it if somebody can help me.,I would like to take the limit $x\to 0$ and prove or disprove the following statements concerning Landau notation. (a) $O(x^{3/2})\subset o(x)\subset O(x^{1/2})$ (b) $(1+O(x))^2=1+O(x^2)$ I know the deifiniton of the O-notation and also what it means but i have no idea how to show these kinds of inequalities. I would appreciate it if somebody can help me.,,"['analysis', 'inequality', 'asymptotics']"
65,Extending isometries between compact subspaces of Cantor space,Extending isometries between compact subspaces of Cantor space,,"Let $\omega$ be the set of natural numbers. $2^\omega$ is the Cantor space . Suppose $K$, $L \subset 2^\omega$ are compact, and there is an isometry $f: K \to L$.  Then how could one extend $f$ to an isometry from $2^\omega$ to $2^\omega$?  Here we are considering $2^\omega$ with the minimum difference metric, which gives the standard product topology; i.e. $ d(x,y) = 2^{-\min \{ n : x(n) \neq y(n) \}}. $","Let $\omega$ be the set of natural numbers. $2^\omega$ is the Cantor space . Suppose $K$, $L \subset 2^\omega$ are compact, and there is an isometry $f: K \to L$.  Then how could one extend $f$ to an isometry from $2^\omega$ to $2^\omega$?  Here we are considering $2^\omega$ with the minimum difference metric, which gives the standard product topology; i.e. $ d(x,y) = 2^{-\min \{ n : x(n) \neq y(n) \}}. $",,"['analysis', 'metric-spaces', 'descriptive-set-theory']"
66,wave equation with moving end,wave equation with moving end,,"Solve the wave equation with moving end: $$ \begin{aligned} u_{t t} &=c^{2} u_{x x} \text { for } x>0, t>0 \\ u(x, 0) &=\varphi(x), u_{t}(x, 0)=\psi(x) \text { for } x \geq 0 \\ u(0, t) &=f(t) \text { for } t \geq 0 \end{aligned} $$ Suppose $x_{0} \geq c t_{0}$ . Now $\left[x_{0}-c t_{0}, x_{0}+c t_{0}\right]$ lies entirely on the nonnegative part of the $x$ - axis, and not enough time has passed for the initial displacement $f(t)$ at time zero to reach any point in this interval. In this case, $u\left(x_{0}, t_{0}\right)$ is not influenced by $f$ and the d'Alembert formula holds. $$ u\left(x_{0}, t_{0}\right)=F\left(x_{0}-c t_{0}\right)+B\left(x_{0}+c t_{0}\right) \text { for } x_{0} \geq c t_{0} $$ $F$ and $B$ are the forward and backward waves. $\psi(x)$ are not defined for $x_{0}-c t_{0}<x<0$ ; hence $F\left(x-c t_{0}\right)$ is not defined. However, putting $x=0$ yields $$ u\left(0, t_{0}\right)=f\left(t_{0}\right)=F\left(-c t_{0}\right)+B\left(c t_{0}\right) . $$ This suggests that we can extend $F$ to this negative value by defining $$ F\left(-c t_{0}\right)=f\left(t_{0}\right)-B\left(c t_{0}\right) . $$ Both function values on the right are well defined. Further, since $t_{0}$ can be any positive number, we can think of $c t_{0}$ as any positive number and use this equation as a model to define $$ F(-x)=f\left(\frac{x}{c}\right)-B(x) $$ for any positive number $x$ . This extends $F$ to negative values. For simplicity, we are using the same symbol $F$ for the extended function. Now put, for $x_{0}-c t_{0}<0$ $$ \begin{aligned} F\left(x_{0}-c t_{0}\right) &=F\left(-\left(c t_{0}-x_{0}\right)\right) \\ &=f\left(\frac{c t_{0}-x_{0}}{c}\right)-B\left(c t_{0}-x_{0}\right) \end{aligned} $$ or $$ F\left(x_{0}-c t_{0}\right)=f\left(t_{0}-\frac{x_{0}}{c}\right)-B\left(c t_{0}-x_{0}\right) . $$ Substituting this into d'Alembert's solution, we have $$ u\left(x_{0}, t_{0}\right)=f\left(t_{0}-\frac{x_{0}}{c}\right)-B\left(c t_{0}-x_{0}\right)+B\left(x_{0}+c t_{0}\right) \text { for } x_{0}-c t_{0}<0 $$ In view of the definition of the backward wave $B$ , this equation can be written $$ \begin{aligned} u\left(x_{0}, t_{0}\right) &=f\left(t_{0}-\frac{x_{0}}{c}\right)+\frac{1}{2}\left(\varphi\left(x_{0}+c t_{0}\right)-\varphi\left(c t_{0}-x_{0}\right)\right) \\ &+\frac{1}{2 c} \int_{c t_{0}-x_{0}}^{x_{0}+c t_{0}} \psi(s) d s \text { for } x_{0}<c t_{0} \end{aligned} $$ We have used the zero subscript to discuss the solution at a particular point and maintain $x$ and $t$ as variables. However, we now drop the subscript and write the solution at any $(x, t)$ with $x \geq 0, t \geq 0$ : $$ \bbox[5px, border:2px solid black] {\begin{aligned} u(x, t) &=\frac{1}{2}(\varphi(x-c t)+\varphi(x+c t)) \\ &+\frac{1}{2 c} \int_{x-c t}^{x+c t} \psi(s) d s \text { for } x \geq c t\\ u(x, t) &=f\left(t-\frac{x}{c}\right)+\frac{1}{2}(\varphi(x+c t)-\varphi(c t-x)) \\ &+\frac{1}{2 c} \int_{c t-x}^{x+c t} \psi(s) d s \text { for } x<c t \end{aligned}} $$ I didn't understand the bolded lines, like why we split the domain in $x< ct \& x\geq ct$ ? And what was mean by, and not enough time has passed for the initial displacement $f(t)$ at time zero to reach any point in this interval. In this case, $u\left(x_{0}, t_{0}\right)$ is not influenced by $f$ and the d'Alembert formula holds. I copy the whole page of Beginning partial differential equations by Peter V. O'Neil , page: 134, section: 4.5 Any help will be appreciated, Thanks in advance.","Solve the wave equation with moving end: Suppose . Now lies entirely on the nonnegative part of the - axis, and not enough time has passed for the initial displacement at time zero to reach any point in this interval. In this case, is not influenced by and the d'Alembert formula holds. and are the forward and backward waves. are not defined for ; hence is not defined. However, putting yields This suggests that we can extend to this negative value by defining Both function values on the right are well defined. Further, since can be any positive number, we can think of as any positive number and use this equation as a model to define for any positive number . This extends to negative values. For simplicity, we are using the same symbol for the extended function. Now put, for or Substituting this into d'Alembert's solution, we have In view of the definition of the backward wave , this equation can be written We have used the zero subscript to discuss the solution at a particular point and maintain and as variables. However, we now drop the subscript and write the solution at any with : I didn't understand the bolded lines, like why we split the domain in ? And what was mean by, and not enough time has passed for the initial displacement at time zero to reach any point in this interval. In this case, is not influenced by and the d'Alembert formula holds. I copy the whole page of Beginning partial differential equations by Peter V. O'Neil , page: 134, section: 4.5 Any help will be appreciated, Thanks in advance.","
\begin{aligned}
u_{t t} &=c^{2} u_{x x} \text { for } x>0, t>0 \\
u(x, 0) &=\varphi(x), u_{t}(x, 0)=\psi(x) \text { for } x \geq 0 \\
u(0, t) &=f(t) \text { for } t \geq 0
\end{aligned}
 x_{0} \geq c t_{0} \left[x_{0}-c t_{0}, x_{0}+c t_{0}\right] x f(t) u\left(x_{0}, t_{0}\right) f 
u\left(x_{0}, t_{0}\right)=F\left(x_{0}-c t_{0}\right)+B\left(x_{0}+c t_{0}\right) \text { for } x_{0} \geq c t_{0}
 F B \psi(x) x_{0}-c t_{0}<x<0 F\left(x-c t_{0}\right) x=0 
u\left(0, t_{0}\right)=f\left(t_{0}\right)=F\left(-c t_{0}\right)+B\left(c t_{0}\right) .
 F 
F\left(-c t_{0}\right)=f\left(t_{0}\right)-B\left(c t_{0}\right) .
 t_{0} c t_{0} 
F(-x)=f\left(\frac{x}{c}\right)-B(x)
 x F F x_{0}-c t_{0}<0 
\begin{aligned}
F\left(x_{0}-c t_{0}\right) &=F\left(-\left(c t_{0}-x_{0}\right)\right) \\
&=f\left(\frac{c t_{0}-x_{0}}{c}\right)-B\left(c t_{0}-x_{0}\right)
\end{aligned}
 
F\left(x_{0}-c t_{0}\right)=f\left(t_{0}-\frac{x_{0}}{c}\right)-B\left(c t_{0}-x_{0}\right) .
 
u\left(x_{0}, t_{0}\right)=f\left(t_{0}-\frac{x_{0}}{c}\right)-B\left(c t_{0}-x_{0}\right)+B\left(x_{0}+c t_{0}\right) \text { for } x_{0}-c t_{0}<0
 B 
\begin{aligned}
u\left(x_{0}, t_{0}\right) &=f\left(t_{0}-\frac{x_{0}}{c}\right)+\frac{1}{2}\left(\varphi\left(x_{0}+c t_{0}\right)-\varphi\left(c t_{0}-x_{0}\right)\right) \\
&+\frac{1}{2 c} \int_{c t_{0}-x_{0}}^{x_{0}+c t_{0}} \psi(s) d s \text { for } x_{0}<c t_{0}
\end{aligned}
 x t (x, t) x \geq 0, t \geq 0  \bbox[5px, border:2px solid black]
{\begin{aligned}
u(x, t) &=\frac{1}{2}(\varphi(x-c t)+\varphi(x+c t)) \\
&+\frac{1}{2 c} \int_{x-c t}^{x+c t} \psi(s) d s \text { for } x \geq c t\\
u(x, t) &=f\left(t-\frac{x}{c}\right)+\frac{1}{2}(\varphi(x+c t)-\varphi(c t-x)) \\
&+\frac{1}{2 c} \int_{c t-x}^{x+c t} \psi(s) d s \text { for } x<c t
\end{aligned}}
 x< ct \& x\geq ct f(t) u\left(x_{0}, t_{0}\right) f","['analysis', 'partial-differential-equations', 'wave-equation']"
67,Polynomials cannot be small in a large set,Polynomials cannot be small in a large set,,"Let $P$ be a degree $d$ polynomial where $d\geq 1$ . Suppose $P$ has a root $x_0\in [0,1]$ , and suppose that $\max_{x\in [0,1]}|P(x)|=1$ . Is there a constant $C_d$ depending on $d$ only, such that for every $0<\epsilon<0.01$ $$ |\{x\in [0,1]:|P(x)|<\epsilon\}|\leq C_d\epsilon^{1/d}? $$ Note that this is easy if $d=1$ . Indeed, suppose $P(x)=a(x-x_0)$ . Since $\max_{x\in [0,1]}|P(x)|=1$ , there is $x_1\in [0,1]$ such that $|a(x_1-x_0)|=1$ . Solving the inequality $|P(x)|<\epsilon$ , we get $|x-x_0|<\epsilon/a$ , and thus $$ |\{x\in [0,1]:|P(x)|<\epsilon\}|\leq 2\epsilon/|a|=2\epsilon |x_1-x_0|\leq 2\epsilon. $$ But in higher dimensions, this seems much trickier.","Let be a degree polynomial where . Suppose has a root , and suppose that . Is there a constant depending on only, such that for every Note that this is easy if . Indeed, suppose . Since , there is such that . Solving the inequality , we get , and thus But in higher dimensions, this seems much trickier.","P d d\geq 1 P x_0\in [0,1] \max_{x\in [0,1]}|P(x)|=1 C_d d 0<\epsilon<0.01 
|\{x\in [0,1]:|P(x)|<\epsilon\}|\leq C_d\epsilon^{1/d}?
 d=1 P(x)=a(x-x_0) \max_{x\in [0,1]}|P(x)|=1 x_1\in [0,1] |a(x_1-x_0)|=1 |P(x)|<\epsilon |x-x_0|<\epsilon/a 
|\{x\in [0,1]:|P(x)|<\epsilon\}|\leq 2\epsilon/|a|=2\epsilon |x_1-x_0|\leq 2\epsilon.
","['analysis', 'measure-theory', 'polynomials']"
68,Confusion about differentiability of a function between finite dimensional Banach spaces,Confusion about differentiability of a function between finite dimensional Banach spaces,,"I'm a little bit confused about something that should actually be simple. If we have a function f between finite dimensional Banach spaces. Then we have the implications: If f is partially differentiable with continuous partial derivatives, then f is continuously differentiable, in particular, f is (totally) differentiable. However, the opposite implication is not true: There are functions that are differentiable, but don't have continuous partial derivatives. My confusion is about how continuous differentiability ties in. Since the derivative of f in any point is given as a linear function, this function between (finite dimensional!) Banach spaces should be continuous. But the function that maps any point to it's derivative doesn't have to be linear, so it doesn't have to be continuous either. Is that right? Otherwise (total) differentiability would imply continuous differentiability. So my last question is: Is, then, continuous differentiability equivalent to partial continuous differentiability? I feel silly even asking this, but I couldn't find any explicit explanation.","I'm a little bit confused about something that should actually be simple. If we have a function f between finite dimensional Banach spaces. Then we have the implications: If f is partially differentiable with continuous partial derivatives, then f is continuously differentiable, in particular, f is (totally) differentiable. However, the opposite implication is not true: There are functions that are differentiable, but don't have continuous partial derivatives. My confusion is about how continuous differentiability ties in. Since the derivative of f in any point is given as a linear function, this function between (finite dimensional!) Banach spaces should be continuous. But the function that maps any point to it's derivative doesn't have to be linear, so it doesn't have to be continuous either. Is that right? Otherwise (total) differentiability would imply continuous differentiability. So my last question is: Is, then, continuous differentiability equivalent to partial continuous differentiability? I feel silly even asking this, but I couldn't find any explicit explanation.",,['analysis']
69,Constructive proof of the Cauchy Schwarz inequality,Constructive proof of the Cauchy Schwarz inequality,,"The famous CS inequality states $$ \left| \left< x , y \right>\right|   \le \left\| x \right\| \cdot \left\| y \right\| $$ for $x,y$ in an inner product space $X$ over $\mathbb{K}$. Every proof I found involves some kind of case distinction; namely one may use w.l.o.g $\Vert x \Vert, \Vert y \Vert > 0$ since the inequality is trivial otherwise. However, I was looking for a constructive proof (i.e. without using the law of excluded middle) for the inequality. I will add some remarks to the question. 1) Law of excluded middle: This axiom states that $A \vee \neg  A$ is true. This is not considered an axiom in constructive mathematics. One might interpret this in the following way: Indirect proofs are not allowed. However, I find this not completely accurate. Precisely the implication $A \rightarrow \neg \neg A$ is true in constructive mathematics; the implication $ \neg \neg A \rightarrow A$ not in gerneral. 2) The definition of an inner product is the same as in ""classical"" mathematics. 3) The norm $\Vert \cdot \Vert$ is given by $\Vert x \Vert = \sqrt{\langle x, x\rangle }$.","The famous CS inequality states $$ \left| \left< x , y \right>\right|   \le \left\| x \right\| \cdot \left\| y \right\| $$ for $x,y$ in an inner product space $X$ over $\mathbb{K}$. Every proof I found involves some kind of case distinction; namely one may use w.l.o.g $\Vert x \Vert, \Vert y \Vert > 0$ since the inequality is trivial otherwise. However, I was looking for a constructive proof (i.e. without using the law of excluded middle) for the inequality. I will add some remarks to the question. 1) Law of excluded middle: This axiom states that $A \vee \neg  A$ is true. This is not considered an axiom in constructive mathematics. One might interpret this in the following way: Indirect proofs are not allowed. However, I find this not completely accurate. Precisely the implication $A \rightarrow \neg \neg A$ is true in constructive mathematics; the implication $ \neg \neg A \rightarrow A$ not in gerneral. 2) The definition of an inner product is the same as in ""classical"" mathematics. 3) The norm $\Vert \cdot \Vert$ is given by $\Vert x \Vert = \sqrt{\langle x, x\rangle }$.",,"['analysis', 'inequality']"
70,inequality related to roots of $(x-1)\log(x)=m$,inequality related to roots of,(x-1)\log(x)=m,"$f(x)=(x-1)\log {x}$, and $f(x_1)=f(x_2)=m, 0<x_1<x_2$. Show that $\frac{9}{5}+\log{(1+m)}<x_1+x_2<2+\frac{m}{2}$. If we apply Hermite-Hadamard inequality, it's easy to show $2<x_1+x_2$, but it's not strong enough.  I also tried to replace $m$ with $f(x_1)$, then it became super complicated. There should be some easy way to simplify $x_1, x_2$","$f(x)=(x-1)\log {x}$, and $f(x_1)=f(x_2)=m, 0<x_1<x_2$. Show that $\frac{9}{5}+\log{(1+m)}<x_1+x_2<2+\frac{m}{2}$. If we apply Hermite-Hadamard inequality, it's easy to show $2<x_1+x_2$, but it's not strong enough.  I also tried to replace $m$ with $f(x_1)$, then it became super complicated. There should be some easy way to simplify $x_1, x_2$",,"['analysis', 'inequality']"
71,Fourier Transform Dirac Delta,Fourier Transform Dirac Delta,,"I have recently learnt about tempered distributions, and how one can define the Fourier transform of a tempered distribution $v$ to be $\hat v$ so that $$\langle\hat v,\varphi\rangle=\langle v,\hat \varphi\rangle.$$ for all Schwartz function $\varphi$. Here, let's say I take the convention that the Fourier transform is $$\hat\varphi(\xi)=\frac{1}{\sqrt{2\pi}}\int_\mathbb R e^{-i x\xi}\varphi(x) \, dx.$$  In particular, one can show that the Fourier transform of $1$ is the Dirac delta $\delta_0$. Here, $1$ is in the sense of tempered distribution, defined by $$\langle 1,\varphi\rangle=\int\varphi(x)\,dx.$$ No problem there. However, I have come across a confusing usage of this fact: I was reading a proof of some estimate, the author claimed that ($g$ is just some function of $x$) $$\frac{1}{\sqrt{2\pi}}\int_\mathbb{R}\int_\mathbb{R} e^{-ixt}g(x)\,dt\,dx=\int_\mathbb{R}\delta(x)g(x)dx.$$ It looks like the author took the Fourier transform of $1$ ""at the point $x$"". But this is confusing to me because I don't understand what it means to take Fourier transform ""at a point"". I also don't understand what $\delta(x)$ means, and what it means to ""integrate"" it. If the context is helpful, I am reading this set of seminar notes. https://www.math.ucla.edu/~visan/Oberwolfach2012.pdf Namely, page 11 Lemma 2.11 (Local Smoothing). I simplified the notation above the make the question less complicated.","I have recently learnt about tempered distributions, and how one can define the Fourier transform of a tempered distribution $v$ to be $\hat v$ so that $$\langle\hat v,\varphi\rangle=\langle v,\hat \varphi\rangle.$$ for all Schwartz function $\varphi$. Here, let's say I take the convention that the Fourier transform is $$\hat\varphi(\xi)=\frac{1}{\sqrt{2\pi}}\int_\mathbb R e^{-i x\xi}\varphi(x) \, dx.$$  In particular, one can show that the Fourier transform of $1$ is the Dirac delta $\delta_0$. Here, $1$ is in the sense of tempered distribution, defined by $$\langle 1,\varphi\rangle=\int\varphi(x)\,dx.$$ No problem there. However, I have come across a confusing usage of this fact: I was reading a proof of some estimate, the author claimed that ($g$ is just some function of $x$) $$\frac{1}{\sqrt{2\pi}}\int_\mathbb{R}\int_\mathbb{R} e^{-ixt}g(x)\,dt\,dx=\int_\mathbb{R}\delta(x)g(x)dx.$$ It looks like the author took the Fourier transform of $1$ ""at the point $x$"". But this is confusing to me because I don't understand what it means to take Fourier transform ""at a point"". I also don't understand what $\delta(x)$ means, and what it means to ""integrate"" it. If the context is helpful, I am reading this set of seminar notes. https://www.math.ucla.edu/~visan/Oberwolfach2012.pdf Namely, page 11 Lemma 2.11 (Local Smoothing). I simplified the notation above the make the question less complicated.",,"['analysis', 'partial-differential-equations', 'fourier-analysis', 'distribution-theory']"
72,Examples of categorical adjunctions in analysis and differential geometry?,Examples of categorical adjunctions in analysis and differential geometry?,,"In a lot of introductory texts on category theory, it seems like the majority of examples come from algebraic topology, algebra, and logic. Are there any good examples of adjunctions in analysis and differential geometry (including Lie theory)? Obviously we have products, coproducts, etc., but those are common to a lot of categories we work with. Are there any examples more specific to analysis and differential geometry? (Crosspost to Math Overflow )","In a lot of introductory texts on category theory, it seems like the majority of examples come from algebraic topology, algebra, and logic. Are there any good examples of adjunctions in analysis and differential geometry (including Lie theory)? Obviously we have products, coproducts, etc., but those are common to a lot of categories we work with. Are there any examples more specific to analysis and differential geometry? (Crosspost to Math Overflow )",,"['analysis', 'differential-geometry', 'soft-question', 'category-theory']"
73,Loss of derivatives,Loss of derivatives,,"In many books on pdes the expression ""loss of derivatives"" is used when some estimates on solution are proved. Can someone clarify to me (maybe with an example) the meaning of this expression? For instance: $$\Vert e^{tD^\alpha\partial_x}u_0\Vert_{L_t^4L_x^\infty}\leq\Vert D^{\frac{1-\alpha}{4}}u_0\Vert_{L^2}$$ has a lost of $\frac{1-\alpha}{4}$ derivatives. Edit: Here is some more background and instances in which it appears. In the survey article by Ifrim and Tataru on local wellposedness for hyperbolic systems (""A key observation is that, whereas solving the linearized equation would cause a loss of derivatives, solving the paradifferential equation does not in general."") In the Encyclopedia of Math entry on nonlinear PDE (""The 'loss of one derivative' in the inversion of the second-order hyperbolic operator leads to principal obstacles in the study of non-linear hyperbolic equations"") This paper on 2nd order hyperbolic PDE (""...the Cauchy problem for $L$ is well-posed in $H^\infty$ with no loss of derivatives ."") I've heard it generally used to refer to when, in estimating some important quantity (usually an energy) $E$ , the norm $\|{f}\|_{H^{k + 1}}$ appears, when somehow we only have a priori estimates for $\|{f}\|_{H^{k}}$ .","In many books on pdes the expression ""loss of derivatives"" is used when some estimates on solution are proved. Can someone clarify to me (maybe with an example) the meaning of this expression? For instance: has a lost of derivatives. Edit: Here is some more background and instances in which it appears. In the survey article by Ifrim and Tataru on local wellposedness for hyperbolic systems (""A key observation is that, whereas solving the linearized equation would cause a loss of derivatives, solving the paradifferential equation does not in general."") In the Encyclopedia of Math entry on nonlinear PDE (""The 'loss of one derivative' in the inversion of the second-order hyperbolic operator leads to principal obstacles in the study of non-linear hyperbolic equations"") This paper on 2nd order hyperbolic PDE (""...the Cauchy problem for is well-posed in with no loss of derivatives ."") I've heard it generally used to refer to when, in estimating some important quantity (usually an energy) , the norm appears, when somehow we only have a priori estimates for .",\Vert e^{tD^\alpha\partial_x}u_0\Vert_{L_t^4L_x^\infty}\leq\Vert D^{\frac{1-\alpha}{4}}u_0\Vert_{L^2} \frac{1-\alpha}{4} L H^\infty E \|{f}\|_{H^{k + 1}} \|{f}\|_{H^{k}},"['analysis', 'partial-differential-equations', 'definition', 'regularity-theory-of-pdes']"
74,Find the points in the graph (my solution) - high school.,Find the points in the graph (my solution) - high school.,,"my math problem is on Swedish so i'll try my best to translate it so you can understand. I'd appreciate it if someone could point out if I did anything wrong and if there is anything that I should add that's essential to these kind of problems. (I'm a high school student in my second year) The function $f(x) = x^3 - 6x^2 + 9x + 2$ gives us the points A and B   on the graph and the second function, $g(x) = x^2 - 7x + 14$ gives C   and D, where C is a mutual point for these two functions. C has a   gradient that is -3. To start off with, I set the x for $f(x)$ to 0 in order to see where on the y-axis the point A crosses. $f(0) = 0^3 - 6*0^2 + 9*0 + 2 = 2$ $A(0,2)$ The next step is to figure out B. To be able to get the coordinates for this point, I have to derivate $f(x)$, and then set $f'(x)=0$. $f'(x) = 3x^2 - 12x + 9 = 0$ $3x^2 - 12x + 9 = 0$ Dividing everything with 3 to use the pq-formula. $x^2 - 4x + 3 = 0$ $x = 2  \pm{\sqrt {2^2-3}}$ $x_1 = 2+1 = 3$ $x_2 = 2-1 = 1$ With these 2 x-values I have to choose one that gives the biggest value for $f(x)$ point B is a Maximum point on the graph that has been provided. I can find this out with the help of Second Derivative. $f''(x) = 6x - 12$ $f''(3) = 6*3 - 12 = 6$ This is a minimum point because of $f'(3) = 0$ and $f''(3) > 0$. $f''(1) = 6*1 - 12 = -6$ This is the x-value that gives the maximum point that I am looking for. $f'(1) = 0$ and $f''(1) < 0$ $f(1) = 1^3 - 6*1^2 + 9*1 + 2 = 6$ $B(1,6)$ As stated earlier, C is a mutual point with a gradient -3. Derivating both functions and setting them equal to -3 and solving these will give us which x-value they have in common. $f'(x) = 3x^2 - 12x + 9 = -3$ $3x^2 - 12x + 9 = -3$ +3 on both sides to get everything on one side then dividing everything with 3 to get x^2 alone so I can use pq-formula. $x^2 - 4x + 4 = 0$ $x = 2  \pm {\sqrt {2^2-4}}$ $x = 2$ On to the second function: $g(x) = x^2 - 7x + 14$ I set $g'(x) = -3$ By derivating this function and moving -7 to the right side will give us: $2x = 4$ Dividing both sides with 2 $x = 2$ $f(2) = 2^3 - 6*2^2 + 9*2 + 2 = 4$ $g(2) = 2^2 - 7*2 + 14 = 4$ $f(2)=g(2)=4$ $C(2,4)$ To get the last point (D) I do the exact same way as I did for point B. I derivate the function and set g'(x) = 0 to solve x. To confirm that D is a Min.Point I derivate the first derivative and this gives me $g''(x) = 2$. We can see that $g''(x) > 0$. $g'(x) = 2x - 7 = 0$ $2x = 7$ $x = 3.5$ $g(3.5) = 3.5^2 - 7*3.5 + 14 = 1.75$ $D(3.5;1.75)$ Answer: $A(0.2)$ $B(1.6)$ $C(2.4)$ $D(3.5;1.75)$ $${}$$","my math problem is on Swedish so i'll try my best to translate it so you can understand. I'd appreciate it if someone could point out if I did anything wrong and if there is anything that I should add that's essential to these kind of problems. (I'm a high school student in my second year) The function $f(x) = x^3 - 6x^2 + 9x + 2$ gives us the points A and B   on the graph and the second function, $g(x) = x^2 - 7x + 14$ gives C   and D, where C is a mutual point for these two functions. C has a   gradient that is -3. To start off with, I set the x for $f(x)$ to 0 in order to see where on the y-axis the point A crosses. $f(0) = 0^3 - 6*0^2 + 9*0 + 2 = 2$ $A(0,2)$ The next step is to figure out B. To be able to get the coordinates for this point, I have to derivate $f(x)$, and then set $f'(x)=0$. $f'(x) = 3x^2 - 12x + 9 = 0$ $3x^2 - 12x + 9 = 0$ Dividing everything with 3 to use the pq-formula. $x^2 - 4x + 3 = 0$ $x = 2  \pm{\sqrt {2^2-3}}$ $x_1 = 2+1 = 3$ $x_2 = 2-1 = 1$ With these 2 x-values I have to choose one that gives the biggest value for $f(x)$ point B is a Maximum point on the graph that has been provided. I can find this out with the help of Second Derivative. $f''(x) = 6x - 12$ $f''(3) = 6*3 - 12 = 6$ This is a minimum point because of $f'(3) = 0$ and $f''(3) > 0$. $f''(1) = 6*1 - 12 = -6$ This is the x-value that gives the maximum point that I am looking for. $f'(1) = 0$ and $f''(1) < 0$ $f(1) = 1^3 - 6*1^2 + 9*1 + 2 = 6$ $B(1,6)$ As stated earlier, C is a mutual point with a gradient -3. Derivating both functions and setting them equal to -3 and solving these will give us which x-value they have in common. $f'(x) = 3x^2 - 12x + 9 = -3$ $3x^2 - 12x + 9 = -3$ +3 on both sides to get everything on one side then dividing everything with 3 to get x^2 alone so I can use pq-formula. $x^2 - 4x + 4 = 0$ $x = 2  \pm {\sqrt {2^2-4}}$ $x = 2$ On to the second function: $g(x) = x^2 - 7x + 14$ I set $g'(x) = -3$ By derivating this function and moving -7 to the right side will give us: $2x = 4$ Dividing both sides with 2 $x = 2$ $f(2) = 2^3 - 6*2^2 + 9*2 + 2 = 4$ $g(2) = 2^2 - 7*2 + 14 = 4$ $f(2)=g(2)=4$ $C(2,4)$ To get the last point (D) I do the exact same way as I did for point B. I derivate the function and set g'(x) = 0 to solve x. To confirm that D is a Min.Point I derivate the first derivative and this gives me $g''(x) = 2$. We can see that $g''(x) > 0$. $g'(x) = 2x - 7 = 0$ $2x = 7$ $x = 3.5$ $g(3.5) = 3.5^2 - 7*3.5 + 14 = 1.75$ $D(3.5;1.75)$ Answer: $A(0.2)$ $B(1.6)$ $C(2.4)$ $D(3.5;1.75)$ $${}$$",,"['analysis', 'optimization']"
75,English edition of Vol 9 of Dieudonné's Foundations of Modern Analysis?,English edition of Vol 9 of Dieudonné's Foundations of Modern Analysis?,,"I have found the first 8 volumes of Dieudonné's Foundations of Modern Analysis in English translation, but I'm having difficulty locating volume 9. I have searched the catalogues of numerous libraries including the British Library and Library of Congress. Does an English language edition of the 9th volume actually exist? The 9th volume of the original French language series ( Éléments d'Analyse ) comprises chapter 24 and is entitled Topologie algébrique et topologie différentielle élémentaire ( Algebraic Topology and Basic Differential Topology ). Thanks in advance for any help with this. Harry","I have found the first 8 volumes of Dieudonné's Foundations of Modern Analysis in English translation, but I'm having difficulty locating volume 9. I have searched the catalogues of numerous libraries including the British Library and Library of Congress. Does an English language edition of the 9th volume actually exist? The 9th volume of the original French language series ( Éléments d'Analyse ) comprises chapter 24 and is entitled Topologie algébrique et topologie différentielle élémentaire ( Algebraic Topology and Basic Differential Topology ). Thanks in advance for any help with this. Harry",,"['analysis', 'reference-request', 'algebraic-topology', 'differential-topology']"
76,Old Qualifying Exam Question (Real Analysis - Possibly Implicit Function Theorem),Old Qualifying Exam Question (Real Analysis - Possibly Implicit Function Theorem),,"The following is an old qualifying exam question that has stumped me: Let $f,g\in C^\infty(\mathbb{R}^3)$ be real-valued functions such that for some $x_0\in \mathbb{R}^3,$ we have $f(x_0) = g(x_0) = 0$ with $df(x_0)$ and $dg(x_0)$ linearly independent. Let $S_f$ and $S_g$ be the zero sets of $f$ and $g$ respectively. Show that if $h\in C^\infty(\mathbb{R}^3)$ vanishes on $S_f\cup S_g,$ then there exists a neighborhood $U\ni x_0$ such that in $U,$ we have $h(x) = f(x)g(x)H(x),$ for some $H\in C^\infty(U)$. My initial thought was that the implicit function theorem might be a good place to start, but I have not been able to make any progress on it. Any thoughts are much appreciated.","The following is an old qualifying exam question that has stumped me: Let $f,g\in C^\infty(\mathbb{R}^3)$ be real-valued functions such that for some $x_0\in \mathbb{R}^3,$ we have $f(x_0) = g(x_0) = 0$ with $df(x_0)$ and $dg(x_0)$ linearly independent. Let $S_f$ and $S_g$ be the zero sets of $f$ and $g$ respectively. Show that if $h\in C^\infty(\mathbb{R}^3)$ vanishes on $S_f\cup S_g,$ then there exists a neighborhood $U\ni x_0$ such that in $U,$ we have $h(x) = f(x)g(x)H(x),$ for some $H\in C^\infty(U)$. My initial thought was that the implicit function theorem might be a good place to start, but I have not been able to make any progress on it. Any thoughts are much appreciated.",,['analysis']
77,Analysis on Improper Integrals,Analysis on Improper Integrals,,"This question is from Munkres' Analysis on Manifolds, section 15 question 1. Let $f: \mathbb{R} \to \mathbb{R}$ be the function $f(x) = x$. Show that, given $\lambda \in \mathbb{R}$, there exists a sequence $C_N$ of compact rectifiable subsets of $\mathbb{R}$ whose union is $\mathbb{R}$, such that $C_N \subset \operatorname{Int} \, C_{N+1}$ for each $N$ and  $$\lim_{N \to \infty} \int_{C_N} f = \lambda. $$  Does the extended integral $\int_\mathbb{R} f$ exist? My guess is to have $C_0$ be $\lambda$ and not the point $-\lambda$.  Then $C_i$ be $[\lambda - i, \lambda +i]$ and $[-\lambda - i, -\lambda +i] \setminus -\lambda$.  This would cover $\mathbb{R}$ and the integral would hold.  I'm not sure if this makes sense though.  Also the extended integral wouldn't exist (would be infinity) but a theorem in the section says that if the normal integral exists, then the extended integral exists as well.","This question is from Munkres' Analysis on Manifolds, section 15 question 1. Let $f: \mathbb{R} \to \mathbb{R}$ be the function $f(x) = x$. Show that, given $\lambda \in \mathbb{R}$, there exists a sequence $C_N$ of compact rectifiable subsets of $\mathbb{R}$ whose union is $\mathbb{R}$, such that $C_N \subset \operatorname{Int} \, C_{N+1}$ for each $N$ and  $$\lim_{N \to \infty} \int_{C_N} f = \lambda. $$  Does the extended integral $\int_\mathbb{R} f$ exist? My guess is to have $C_0$ be $\lambda$ and not the point $-\lambda$.  Then $C_i$ be $[\lambda - i, \lambda +i]$ and $[-\lambda - i, -\lambda +i] \setminus -\lambda$.  This would cover $\mathbb{R}$ and the integral would hold.  I'm not sure if this makes sense though.  Also the extended integral wouldn't exist (would be infinity) but a theorem in the section says that if the normal integral exists, then the extended integral exists as well.",,"['analysis', 'improper-integrals']"
78,How to check the strong ergodicity of the $SL_2(\mathbb{Z})$-action on the torus?,How to check the strong ergodicity of the -action on the torus?,SL_2(\mathbb{Z}),"Suppose $\Gamma\subset SL_2(\mathbb{Z})$ is a non-amenable subgroup, especially, $\Gamma=SL_2(\mathbb{Z})$. Consider the natural action of $\Gamma$ on $S^1\times S^1=T^2$. How to check that this action is strongly ergodic? ($T^2$ is equipped with the Haar measure). Recall that an action $\Gamma\curvearrowright (X,\mu)$ is called strongly ergodic if for every sequence of measurable sets $A_n\subset X$ such that $\lim\limits_{n\to\infty}\mu(A_n\Delta\gamma A_n)=0,\forall \gamma\in \Gamma$, we have that $\lim\limits_{n\to\infty}\mu(A_n)(1-\mu(A_n))=0$. Note that since $T^2$ is considered as the Pontryagin dual of $\mathbb{Z}^2$, we know that for any $f\in T^2$, such that $f(n,m)=z_1^nz_2^m, (z_2,z_2)\in T^2\forall (n,m)\in\mathbb{Z}^2$, then, for $$\gamma=\begin{bmatrix}a&b\\c&d\end{bmatrix},$$ $\gamma f$ satisfies $(\gamma f)(n,m)=(z_1^az_2^c)^n(z_1^bz_2^d)^m$. But how to check the strong ergodicity condition? Since I do not know how to calculate $\mu(A_n\Delta\gamma A_n)$ in practice. Maybe we need to do some qualitative analysis instead of quantitive analysis, but how to proceed?","Suppose $\Gamma\subset SL_2(\mathbb{Z})$ is a non-amenable subgroup, especially, $\Gamma=SL_2(\mathbb{Z})$. Consider the natural action of $\Gamma$ on $S^1\times S^1=T^2$. How to check that this action is strongly ergodic? ($T^2$ is equipped with the Haar measure). Recall that an action $\Gamma\curvearrowright (X,\mu)$ is called strongly ergodic if for every sequence of measurable sets $A_n\subset X$ such that $\lim\limits_{n\to\infty}\mu(A_n\Delta\gamma A_n)=0,\forall \gamma\in \Gamma$, we have that $\lim\limits_{n\to\infty}\mu(A_n)(1-\mu(A_n))=0$. Note that since $T^2$ is considered as the Pontryagin dual of $\mathbb{Z}^2$, we know that for any $f\in T^2$, such that $f(n,m)=z_1^nz_2^m, (z_2,z_2)\in T^2\forall (n,m)\in\mathbb{Z}^2$, then, for $$\gamma=\begin{bmatrix}a&b\\c&d\end{bmatrix},$$ $\gamma f$ satisfies $(\gamma f)(n,m)=(z_1^az_2^c)^n(z_1^bz_2^d)^m$. But how to check the strong ergodicity condition? Since I do not know how to calculate $\mu(A_n\Delta\gamma A_n)$ in practice. Maybe we need to do some qualitative analysis instead of quantitive analysis, but how to proceed?",,"['analysis', 'dynamical-systems', 'ergodic-theory', 'group-actions']"
79,A question on measure space and measurable function,A question on measure space and measurable function,,"Let $(X,\mu)$ be a measure space, and $\mu(X)< + \infty$. Let $f$ be a nonnegative measurable function on $X$ and $E_n = \{x \in X|f(x) \geq n \}$. Then $\int_X f^2d \mu < + \infty$ if and only if $\sum_{n=1}^{\infty} n \mu(E_n) <+\infty$. Do I have to use simple functions that converge to $f$? Thanks a lot.","Let $(X,\mu)$ be a measure space, and $\mu(X)< + \infty$. Let $f$ be a nonnegative measurable function on $X$ and $E_n = \{x \in X|f(x) \geq n \}$. Then $\int_X f^2d \mu < + \infty$ if and only if $\sum_{n=1}^{\infty} n \mu(E_n) <+\infty$. Do I have to use simple functions that converge to $f$? Thanks a lot.",,"['analysis', 'measure-theory']"
80,Smoothness of radially defined functions.,Smoothness of radially defined functions.,,"Suppose that for each $v \in \mathbb{R}^2$ with $|v| = 1$, there is a smooth ($C^{\infty}$) function $f_v : [0, 1] \rightarrow \mathbb{R}$ such that $f_v(0) = 0$. Now, let $\bar{D}$ be the closed unit disk in $\mathbb{R}^2$ and define $\phi : \bar{D} \subseteq \mathbb{R}^2 \rightarrow \mathbb{R}$ by setting $\phi(0) = 0$ and, for all other $u \in \bar{D}$, $$\phi(u) := f_{u/|u|}(|u|).$$ QUESTION : Are there reasonable conditions to impose on the family $\{f_v\}_{v~\in~\mathbb{S}^1}$ that guarantee smoothness of $\phi$? Under these conditions, how would you prove it? I apologize for the imprecise use of the word reasonable here; but I'd be interested in pretty much any conditions, other than absolutely trivial ones, like ""take all $f_v$ identically zero"". Thanks!","Suppose that for each $v \in \mathbb{R}^2$ with $|v| = 1$, there is a smooth ($C^{\infty}$) function $f_v : [0, 1] \rightarrow \mathbb{R}$ such that $f_v(0) = 0$. Now, let $\bar{D}$ be the closed unit disk in $\mathbb{R}^2$ and define $\phi : \bar{D} \subseteq \mathbb{R}^2 \rightarrow \mathbb{R}$ by setting $\phi(0) = 0$ and, for all other $u \in \bar{D}$, $$\phi(u) := f_{u/|u|}(|u|).$$ QUESTION : Are there reasonable conditions to impose on the family $\{f_v\}_{v~\in~\mathbb{S}^1}$ that guarantee smoothness of $\phi$? Under these conditions, how would you prove it? I apologize for the imprecise use of the word reasonable here; but I'd be interested in pretty much any conditions, other than absolutely trivial ones, like ""take all $f_v$ identically zero"". Thanks!",,"['analysis', 'multivariable-calculus']"
81,Prove : $\int_0^1 \frac{ e^{\sin^2x}}{1+x^2}\mathrm dx > 1$,Prove :,\int_0^1 \frac{ e^{\sin^2x}}{1+x^2}\mathrm dx > 1,"I want to prove the following inequality : $$I = \int_0^1 \frac{ e^{\sin^2x}}{1+x^2}\mathrm dx > 1$$ Since I have been only introduced to elementary methods of integration, the indefinite integral appears non-solvable to me (I would be glad to see if there's a neat expression for indefinite integral). To evaluate the integral, I used the property $\int_a^b f(x) \mathrm dx = \int_a^b f(a+b-x) \mathrm dx$ , to get $I = \int_0^1 \frac{ e^{\sin^2(1-x)}}{x^2 - 2x + 2}\mathrm dx$ which looks much uglier. Further, the limits of integral are not symmetrical with respect to $x=0$ , hence the fact that the integrand is an even function is useless. Please tell that how can I approach this inequality. Thanks !","I want to prove the following inequality : Since I have been only introduced to elementary methods of integration, the indefinite integral appears non-solvable to me (I would be glad to see if there's a neat expression for indefinite integral). To evaluate the integral, I used the property , to get which looks much uglier. Further, the limits of integral are not symmetrical with respect to , hence the fact that the integrand is an even function is useless. Please tell that how can I approach this inequality. Thanks !",I = \int_0^1 \frac{ e^{\sin^2x}}{1+x^2}\mathrm dx > 1 \int_a^b f(x) \mathrm dx = \int_a^b f(a+b-x) \mathrm dx I = \int_0^1 \frac{ e^{\sin^2(1-x)}}{x^2 - 2x + 2}\mathrm dx x=0,"['calculus', 'inequality', 'definite-integrals']"
82,Bound for the gradient (or laplacian) of a strictly convex function from above,Bound for the gradient (or laplacian) of a strictly convex function from above,,"Let $V \in C^2(\mathbb{R}^d; \mathbb{R})$ a (strictly) convex function with $ \int_{\mathbb{R}^d} \mathrm{e}^{-V(x)} \, dx = 1.$ I am trying to show that $$ \int_{\mathbb{R}^d} |\nabla_x V(x) |^2\mathrm{e}^{-V(x)} \, dx < +\infty.$$ My ideas: (Due to strict convexity and $V$ is not constant and smooth on $\mathbb{R}^d$ we should have $ \lim_{|x| \to \infty} V(x) = + \infty.$ Perhabs this could be useful...) $$ \int_{\mathbb{R}^d} |\nabla_x V(x) |^2\mathrm{e}^{-V(x)} \, dx = - \int_{\mathbb{R}^d} \nabla_x V(x) \cdot \nabla_x\mathrm{e}^{-V(x)} \, dx  = \int_{\mathbb{R}^d} \Delta_x V(x) \cdot \mathrm{e}^{-V(x)} \, dx.$$ If I could estimate $\Delta_x V(x)$ in a certain way like: $$ \Delta_x V(x) \leq \alpha + \beta |\nabla_x V(x) |^2$$ with $\alpha > 0$ and $\beta \in [0;1) $ it would work. Does this hold for all (strictly) convex functions with $ \int_{\mathbb{R}^d} \mathrm{e}^{-V(x)} \, dx = 1$ ? The inequality does not hold for all strictly convex functions like for $-\mathrm{ln}(x)$ on $(0, +\infty).$ So I suppose we will need an additional condition. Or are there any other ideas? Maybe something like $$ |\nabla_x V(x) |^2 \leq a + b |V(x)|^2$$ would work too... Since V is convex and differentiable it fulfills: $$ V(x) \geq V(y) + \nabla^{\top} V(y) \cdot (x-y)$$ for all $x,y \in \mathbb{R}^d.$ That should be helpful.",Let a (strictly) convex function with I am trying to show that My ideas: (Due to strict convexity and is not constant and smooth on we should have Perhabs this could be useful...) If I could estimate in a certain way like: with and it would work. Does this hold for all (strictly) convex functions with ? The inequality does not hold for all strictly convex functions like for on So I suppose we will need an additional condition. Or are there any other ideas? Maybe something like would work too... Since V is convex and differentiable it fulfills: for all That should be helpful.,"V \in C^2(\mathbb{R}^d; \mathbb{R})  \int_{\mathbb{R}^d} \mathrm{e}^{-V(x)} \, dx = 1.  \int_{\mathbb{R}^d} |\nabla_x V(x) |^2\mathrm{e}^{-V(x)} \, dx < +\infty. V \mathbb{R}^d  \lim_{|x| \to \infty} V(x) = + \infty.  \int_{\mathbb{R}^d} |\nabla_x V(x) |^2\mathrm{e}^{-V(x)} \, dx = - \int_{\mathbb{R}^d} \nabla_x V(x) \cdot \nabla_x\mathrm{e}^{-V(x)} \, dx 
= \int_{\mathbb{R}^d} \Delta_x V(x) \cdot \mathrm{e}^{-V(x)} \, dx. \Delta_x V(x)  \Delta_x V(x) \leq \alpha + \beta |\nabla_x V(x) |^2 \alpha > 0 \beta \in [0;1)   \int_{\mathbb{R}^d} \mathrm{e}^{-V(x)} \, dx = 1 -\mathrm{ln}(x) (0, +\infty).  |\nabla_x V(x) |^2 \leq a + b |V(x)|^2  V(x) \geq V(y) + \nabla^{\top} V(y) \cdot (x-y) x,y \in \mathbb{R}^d.","['analysis', 'multivariable-calculus', 'convex-analysis']"
83,Is there any function such that the limit of its derivative divided by its value to the nth power diverges?,Is there any function such that the limit of its derivative divided by its value to the nth power diverges?,,"Recently, I have become intrigued with this functional: $$D_n=\lim \limits _{x\to \infty}\frac{f'(x)}{[f(x)]^n}.$$ In particular, provided that the function is both differentiable and increasing in magnitude for all $x$ , for which functions does $D_n$ diverge to infinity? First, I have considered $n=1$ . This has obvious solutions. For example, $e^{e^x}$ , which has derivative $e^x\cdot e^{e^x}$ , makes $D_1$ diverge to infinity, but not $D_2$ . Is there any function which satisfies the above conditions and makes $D_2$ diverge? What about a function which satisfies the above conditions and makes $D_n$ diverge for all values of $n$ ? As a reminder, here are the conditions for the functions: You must be able to choose an $x_0$ such that the function is differentiable for all $x>x_0$ . The absolute value of the function must be increasing for all $x$ on which it is defined. The function must be both defined on real numbers and real-valued.","Recently, I have become intrigued with this functional: In particular, provided that the function is both differentiable and increasing in magnitude for all , for which functions does diverge to infinity? First, I have considered . This has obvious solutions. For example, , which has derivative , makes diverge to infinity, but not . Is there any function which satisfies the above conditions and makes diverge? What about a function which satisfies the above conditions and makes diverge for all values of ? As a reminder, here are the conditions for the functions: You must be able to choose an such that the function is differentiable for all . The absolute value of the function must be increasing for all on which it is defined. The function must be both defined on real numbers and real-valued.",D_n=\lim \limits _{x\to \infty}\frac{f'(x)}{[f(x)]^n}. x D_n n=1 e^{e^x} e^x\cdot e^{e^x} D_1 D_2 D_2 D_n n x_0 x>x_0 x,[]
84,Lebesgue differentiation theorem and mollifiers,Lebesgue differentiation theorem and mollifiers,,"Let $F(x,u)$ be Caratheodory function. i.e. $F(x,\cdot)$ is continous for a.e. with respect to $x\in \mathbb{R}$ and $F(\cdot,u)$ measurable for $u.$ In addition, assume that $F(x,\cdot)$ is Lipschitz continuous and $F(x,0)=0.$ Now, suppose $u \in L^1 \cap L^{\infty}(\mathbb{R})$ and $\phi \in C_c(\mathbb{R})$ : how to prove the following statement? \begin{eqnarray} \lim \limits_{\epsilon \rightarrow 0} \int\limits_{\mathbb{R}^2} F(x,u(y))\eta_{\epsilon}(x-y) \phi(x)dx dy= \int\limits_{\mathbb{R}}F(x,u(x))\phi(x)dx \end{eqnarray} P.S.: I do understand that we have to apply Lebesgue differentiation theorem but I am looking for a proper justification. For example what guarantees the measurability of the set $$ \left\{x\in\Bbb R: \lim \limits_{\epsilon \rightarrow 0} \int\limits_{\mathbb{R}} F(x,u(y))\eta_{\epsilon}(x-y) dy=0\right\}\;? $$","Let be Caratheodory function. i.e. is continous for a.e. with respect to and measurable for In addition, assume that is Lipschitz continuous and Now, suppose and : how to prove the following statement? P.S.: I do understand that we have to apply Lebesgue differentiation theorem but I am looking for a proper justification. For example what guarantees the measurability of the set","F(x,u) F(x,\cdot) x\in \mathbb{R} F(\cdot,u) u. F(x,\cdot) F(x,0)=0. u \in L^1 \cap L^{\infty}(\mathbb{R}) \phi \in C_c(\mathbb{R}) \begin{eqnarray}
\lim \limits_{\epsilon \rightarrow 0} \int\limits_{\mathbb{R}^2} F(x,u(y))\eta_{\epsilon}(x-y) \phi(x)dx dy= \int\limits_{\mathbb{R}}F(x,u(x))\phi(x)dx
\end{eqnarray} 
\left\{x\in\Bbb R: \lim \limits_{\epsilon \rightarrow 0} \int\limits_{\mathbb{R}} F(x,u(y))\eta_{\epsilon}(x-y) dy=0\right\}\;?
","['analysis', 'measure-theory']"
85,'Conceptual' proof of Holder's inequality or Minkowski's inequality,'Conceptual' proof of Holder's inequality or Minkowski's inequality,,"Holder's inequality and Minkowski's inequality are two basic inequalities about $L^p$ spaces, stating that $||fg||_1 \leq ||f||_p||g||_q$ when $p, q$ are conjugate, and that $||f+g||_p \leq ||f||_p + ||g||_p.$ Holder's inequality is closely related to the fact that $L^p, L^q$ are each others' duals for $p, q$ conjugate (and $1 < p, q < \infty$ ). However, the only proofs I know of these inequalities are very 'algebra heavy' and I find it hard to think of what the 'ideas' of the proofs are, or how to compress them in my head beyond (for Holder) 'do funky algebra and apply Young's inequality' and (for Minkowski) 'do funky algebra and apply Holder's.' Is there some more intuitive/conceptual proof of either inequality? Say, some abstract way of seeing the dual of $L^p$ is $L^q$ in a way that allows you to deduce Holder, or some other proof that doesn't use any 'clever' algebraic manipulations (or, perhaps, any motivations for these clever manipulations?).","Holder's inequality and Minkowski's inequality are two basic inequalities about spaces, stating that when are conjugate, and that Holder's inequality is closely related to the fact that are each others' duals for conjugate (and ). However, the only proofs I know of these inequalities are very 'algebra heavy' and I find it hard to think of what the 'ideas' of the proofs are, or how to compress them in my head beyond (for Holder) 'do funky algebra and apply Young's inequality' and (for Minkowski) 'do funky algebra and apply Holder's.' Is there some more intuitive/conceptual proof of either inequality? Say, some abstract way of seeing the dual of is in a way that allows you to deduce Holder, or some other proof that doesn't use any 'clever' algebraic manipulations (or, perhaps, any motivations for these clever manipulations?).","L^p ||fg||_1 \leq ||f||_p||g||_q p, q ||f+g||_p \leq ||f||_p + ||g||_p. L^p, L^q p, q 1 < p, q < \infty L^p L^q","['analysis', 'intuition']"
86,Relation between $\rm{Diff}(\mathbb{R}^n) \times \rm{Diff}(\mathbb{R}^m)$ and $\rm{Diff}(\mathbb{R}^n \times \mathbb{R}^m)$,Relation between  and,\rm{Diff}(\mathbb{R}^n) \times \rm{Diff}(\mathbb{R}^m) \rm{Diff}(\mathbb{R}^n \times \mathbb{R}^m),"I am wondering if there is a relation between a map $(\varphi, \psi) \in \rm{Diff}(\mathbb{R}^n) \times \rm{Diff}(\mathbb{R}^m)$ and a map $\Psi \in \rm{Diff}(\mathbb{R}^n \times \mathbb{R}^m)$ . To be clear: Here I only mean local diffeomorphisms at $(0,0)$ . Is there an embedding of $\rm{Diff}(\mathbb{R}^n) \times \rm{Diff}(\mathbb{R}^m)$ into $\rm{Diff}(\mathbb{R}^n \times \mathbb{R}^m)$ , since maps $(\varphi, \psi)\in \rm{Diff}(\mathbb{R}^n) \times \rm{Diff}(\mathbb{R}^m)$ are of the form $(x,y) \mapsto (\varphi(x),\psi(y))$ and a map $\Psi \in \rm{Diff}(\mathbb{R}^n \times \mathbb{R}^m)$ is of the form $\Psi(x,y) = (\Psi_1(x,y), \Psi_2(x,y))$ . Can I understand the map $\Psi_1$ as a parametrized diffeomorphism, i.e. for all $y \in \mathbb R^m$ the map $x \mapsto \Psi_1(x,y)$ is a diffeomorphism in $\mathbb R^n$ and vice versa for $\Psi_2$ ? Thanks in advance, any help is appreciated. EDIT: So I figured that $\rm{Diff}(\mathbb{R}^n) \times \rm{Diff}(\mathbb{R}^m)$ is embedded in $\rm{Diff}(\mathbb{R}^n \times \mathbb{R}^m)$ . As for $(\varphi, \psi) \in \rm{Diff}(\mathbb{R}^n) \times \rm{Diff}(\mathbb{R}^m)$ , the map $\iota:(\varphi, \psi) \mapsto (\varphi \times \psi)$ is clearly injective and continuous, therefore an embedding and $(\varphi \times \psi)\in \rm{Diff}(\mathbb{R}^n \times \mathbb{R}^m)$ . Is this correct or am I missing something?","I am wondering if there is a relation between a map and a map . To be clear: Here I only mean local diffeomorphisms at . Is there an embedding of into , since maps are of the form and a map is of the form . Can I understand the map as a parametrized diffeomorphism, i.e. for all the map is a diffeomorphism in and vice versa for ? Thanks in advance, any help is appreciated. EDIT: So I figured that is embedded in . As for , the map is clearly injective and continuous, therefore an embedding and . Is this correct or am I missing something?","(\varphi, \psi) \in \rm{Diff}(\mathbb{R}^n) \times \rm{Diff}(\mathbb{R}^m) \Psi \in \rm{Diff}(\mathbb{R}^n \times \mathbb{R}^m) (0,0) \rm{Diff}(\mathbb{R}^n) \times \rm{Diff}(\mathbb{R}^m) \rm{Diff}(\mathbb{R}^n \times \mathbb{R}^m) (\varphi, \psi)\in \rm{Diff}(\mathbb{R}^n) \times \rm{Diff}(\mathbb{R}^m) (x,y) \mapsto (\varphi(x),\psi(y)) \Psi \in \rm{Diff}(\mathbb{R}^n \times \mathbb{R}^m) \Psi(x,y) = (\Psi_1(x,y), \Psi_2(x,y)) \Psi_1 y \in \mathbb R^m x \mapsto \Psi_1(x,y) \mathbb R^n \Psi_2 \rm{Diff}(\mathbb{R}^n) \times \rm{Diff}(\mathbb{R}^m) \rm{Diff}(\mathbb{R}^n \times \mathbb{R}^m) (\varphi, \psi) \in \rm{Diff}(\mathbb{R}^n) \times \rm{Diff}(\mathbb{R}^m) \iota:(\varphi, \psi) \mapsto (\varphi \times \psi) (\varphi \times \psi)\in \rm{Diff}(\mathbb{R}^n \times \mathbb{R}^m)","['analysis', 'diffeomorphism']"
87,Proof of some Riemann problem [closed],Proof of some Riemann problem [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 4 years ago . Improve this question I was wondering if anyone is aware of this proof about the Riemann Hypothesis. https://arxiv.org/pdf/1004.4143.pdf What I like about it is his analytic approach using the behaviour of general series at singular points. Since I haven't heard anything about it, I presume there must be some flaw somewhere, but I can not find it. To me it seems legit. In particular I want to point out his main result. I added short variations of the proofs which should be sufficient to capture the entire essence. The paper with that regard is somewhat cumbersome. He resums the $\zeta$ -function, compares it with its integral representation and obtains an expression valid for $\sigma>0$ ( $s=\sigma+it$ ) $$(s-1)\zeta(s) = \sum_{n=1}^\infty \frac{S_n(s)}{n+1} \tag{0}$$ where $S_n(s)=-\frac{1}{n} \sum_{k=1}^n \binom{n}{k} (-1)^k \, k^{1-s}$ . Then he defines a function $\zeta(s,x)$ valid for $|x|<1$ by $$(s-1)\zeta(s,x) = \sum_{n=1}^\infty \frac{S_n(s)}{n+1} \, x^n$$ which converges to (0) for $\sigma>0$ as $x\rightarrow 1$ . He uses a result for the asymptotic behaviour of $S_n(s)$ as $n$ gets large $$\frac{S_n(s)}{n+1} \sim \frac{1}{n(n+1)(\log n)^{1-s} \Gamma(s)} \left( 1 + {\cal O}\left(\frac{1}{\log n}\right) \right)\tag{as.1}$$ which probably inspired him to look for functions $\phi_s(x)=\sum_{n=0}^\infty \phi_n(s) \, x^n$ whose coefficients $\phi_n(s)$ have the same asymptotic behaviour with respect to $n$ . Such a function is $$\phi_s(x)=(1-x)\left(\frac{\log(1-x)}{-x}\right)^{s}$$ whose coefficients obey $$\phi_n(s) \sim \frac{-s}{n^2 (\log n)^{1-s}} \left( 1 + {\cal O}\left(\frac{1}{\log n}\right) \right)\tag{as.2}$$ as $n$ gets large. Since $\phi_s'(x)$ diverges to $\infty$ as $x\rightarrow 1$ it follows that $\zeta'(s,x)$ diverges to $\infty$ as $x\rightarrow 1$ . This allows him to establish the result (after a lot of details) $$\lim_{x\rightarrow 1} \frac{(s-1)\zeta'(s,x)}{\phi_s'(x)} = \lim_{n\rightarrow \infty} \frac{\frac{nS_n(s)}{n+1}}{n\phi_n(s)} = \frac{-1}{\Gamma(s+1)} \, . \tag{1}$$ In a similar but different way as in the pre-print this can be easily seen as follows. proof: Define $a_n\equiv\frac{nS_n(s)}{n+1}$ and $b_n\equiv n\phi_n(s)$ , so that $\lim_{n\rightarrow \infty}\frac{a_n}{b_n} = \frac{-1}{\Gamma(s+1)} \equiv c$ . This is equivalent to saying $\frac{a_n}{b_n}=c+\epsilon_n$ for some complex sequence $\epsilon_n$ that vanishes as $n\rightarrow \infty$ . From the asymptotics (as.1) and (as.2) it is seen that for large $n$ $$|\epsilon_n| = {\cal O}\left(1/\log n\right) \, .$$ To establish the result, $a_n = cb_n + \epsilon_n b_n$ is used to construct $$\sum_{n=1}^\infty a_n x^n = c \sum_{n=1}^\infty b_n x^n + \sum_{n=1}^\infty \epsilon_n b_n x^n$$ from which it then follows if we can show that $\sum_{n=1}^\infty |\epsilon_n b_n|$ converges. But from the asymptotics for $|\epsilon_n|$ and $|b_n|$ this sum can be bounded $$\sum_{n=2}^\infty |\epsilon_n b_n| \leq C|s| \sum_{n=2}^\infty \frac{1}{n(\log n)^{2-\sigma}}$$ for a suitable constant $C<\infty$ . The convergence of the right sum follows by the integral test for $0<\sigma<1$ . From this it can be deduced: If $s$ is a non-trivial zero, then $$\lim_{x\rightarrow 1} \frac{(s-1)\zeta(s,x)}{\phi_s(x)} = \frac{-1}{\Gamma(s+1)} \, . \tag{2}$$ proof: Define $\delta(x)\equiv \frac{(s-1)\zeta'(s,x)}{\phi_s'(x)} + \frac{1}{\Gamma(s+1)}$ , so that the previous result is compactly written as $\lim_{x\rightarrow 1} \delta(x) = 0$ . Let $s$ be a non-trivial zero and integrate (the previous proof showed that $\zeta'(s,x)$ has the same singularity at $x=1$ as $\phi_s'(x)$ which is manifestly integrable in a neighbourhood $\leq 1$ ) $$\int_x^1 \delta(t) \phi_s'(t) \, {\rm d}t = -(s-1)\zeta(s,x) - \frac{\phi_s(x)}{\Gamma(s+1)}$$ or $$-\frac{\int_x^1 \delta(t) \phi_s'(t) \, {\rm d}t}{\phi_s(x)} = \frac{(s-1)\zeta(s,x)}{\phi_s(x)} + \frac{1}{\Gamma(s+1)} \, .$$ It suffices to show that the LHS vanishes absolutely as $x\rightarrow 1$ , for this $$\left|\frac{\int_x^1 \delta(t) \phi_s'(t) \, {\rm d}t}{\phi_s(x)}\right| \leq \frac{\int_x^1 |\delta(t)| \, |\phi_s'(t)| \, {\rm d}t}{|\phi_s(x)|} \, .$$ The nominator ( $f(x)$ ) and denominator ( $g(x)$ ) on the RHS are differentiable on the open interval $(x,1)$ and finite at $x<1$ , so Rolle's theorem (for the function $h(t)=f(t) - \frac{f(x)}{g(x)} \, g(t)$ ) or equivalently L'Hospital can be applied. The RHS becomes in the limit $x\rightarrow 1$ $$\lim_{x\rightarrow 1} |\delta(x)| \, \frac{|\phi_s'(x)|}{-|\phi_s(x)|'} = 0 \cdot 1 = 0 \, .$$ In the final step he considers the ratio of (2) for the zero $s$ and $1-s$ $$ \lim_{x\rightarrow 1} \left|\frac{(s-1)\zeta(s,x)}{\phi_s(x)}\frac{\phi_{1-s}(x)}{(-s)\zeta(1-s,x)}\right| = \left|\frac{\Gamma(2-s)}{\Gamma(s+1)}\right| \tag{3}$$ or $$ \lim_{x\rightarrow 1} \left|\frac{\phi_{1-s}(x)}{\phi_s(x)}\right| = \lim_{x\rightarrow 1} \left( \frac{\log(1-x)}{-x} \right)^{1-2\sigma} = \left|\frac{\zeta(1-s)\Gamma(1-s)}{\zeta(s)\Gamma(s)}\right| $$ which is an equation only possible when $\sigma =1/2$ in which case the LHS is 1, which is indeed true for any real $t$ on the RHS when using the functional equation for the $\zeta$ -function $$\left| \frac{\zeta(1-s)\Gamma(1-s)}{\zeta(s)\Gamma(s)} \right| = \left| \frac{(2\pi)^{1-s}}{2\sin(\pi s/2)\Gamma(s)} \right| = 1 $$ and $\left|\Gamma(1/2+it)\right|^2 = \frac{\pi}{\cosh(\pi t)}$ . Wrong... As most probably have expected, the proof is fundamentally flawed. I've been thinking about this for some time now and am somewhat embarassed to say that the solution is so trivial. It would have saved me lots of time, if somebody here would have pointed it out earlier, since it could have been seen from my given exposition, but here we go... The problem with the proof lies within its formulation. It utterly overcomplicates matter where not necessary. This distracts from the relevant point and superficially makes it look as if it were correct, just like shenanigans try to distract their prey. While I have already simplified the steps significantly (I believe), it already more or less starts with Equation (1). It is not even wrong, but should have been more properly written as $$(s-1)\zeta'(s,x) = -\frac{\phi_s'(x)}{\Gamma(s+1)} + {\cal O}(1)$$ for $x\rightarrow 1$ , in order to keep track of error-terms. This shows that both functions share the same logarithmic singularity at $x=1$ . Now when integrating this Equation from $x$ to $1$ , it is not yet assumed that $s$ is a zero. This gives rise to the identity $$(s-1)\zeta(s,x)=(s-1)\zeta(s) - \frac{\phi_s(x)}{\Gamma(s+1)} + {\cal O}(1-x)$$ as $x\rightarrow 1$ , which is just the expansion of $(s-1)\zeta(s,x)$ about the singular point $x=1$ . Now if $s=s_0$ is a zero with $0<\Re(s_0)<1$ , then the constant term of the expansion vanishes identically, so that Equation (3) becomes $$\lim_{x\rightarrow 1} \left|\frac{(s_0-1)\zeta(s_0,x)}{(-s_0)\zeta(1-s_0,x)}\right| = \lim_{x\rightarrow 1} \left| \frac{\phi_{s_0}(x)}{\phi_{1-s_0}(x)} \frac{\Gamma(2-s_0)}{\Gamma(s_0+1)}\right| \\ \stackrel{!}{=} \lim_{s\rightarrow s_0} \left| \frac{(s-1)\zeta(s,1)}{(-s)\zeta(1-s,1)} \right| = \lim_{s\rightarrow s_0} \left| \frac{(s-1)\zeta(s)}{(-s)\zeta(1-s)} \right| \, .$$ This is where the flaw becomes apparent, because the interchange of limits is generally wrong and would have to be crucially justified. Evidently it would be the main part of the proof! I should also mention, that Equation (0) - even though he might have found it independently - can actually be traced back to a version from H. Hasse (1930).","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 4 years ago . Improve this question I was wondering if anyone is aware of this proof about the Riemann Hypothesis. https://arxiv.org/pdf/1004.4143.pdf What I like about it is his analytic approach using the behaviour of general series at singular points. Since I haven't heard anything about it, I presume there must be some flaw somewhere, but I can not find it. To me it seems legit. In particular I want to point out his main result. I added short variations of the proofs which should be sufficient to capture the entire essence. The paper with that regard is somewhat cumbersome. He resums the -function, compares it with its integral representation and obtains an expression valid for ( ) where . Then he defines a function valid for by which converges to (0) for as . He uses a result for the asymptotic behaviour of as gets large which probably inspired him to look for functions whose coefficients have the same asymptotic behaviour with respect to . Such a function is whose coefficients obey as gets large. Since diverges to as it follows that diverges to as . This allows him to establish the result (after a lot of details) In a similar but different way as in the pre-print this can be easily seen as follows. proof: Define and , so that . This is equivalent to saying for some complex sequence that vanishes as . From the asymptotics (as.1) and (as.2) it is seen that for large To establish the result, is used to construct from which it then follows if we can show that converges. But from the asymptotics for and this sum can be bounded for a suitable constant . The convergence of the right sum follows by the integral test for . From this it can be deduced: If is a non-trivial zero, then proof: Define , so that the previous result is compactly written as . Let be a non-trivial zero and integrate (the previous proof showed that has the same singularity at as which is manifestly integrable in a neighbourhood ) or It suffices to show that the LHS vanishes absolutely as , for this The nominator ( ) and denominator ( ) on the RHS are differentiable on the open interval and finite at , so Rolle's theorem (for the function ) or equivalently L'Hospital can be applied. The RHS becomes in the limit In the final step he considers the ratio of (2) for the zero and or which is an equation only possible when in which case the LHS is 1, which is indeed true for any real on the RHS when using the functional equation for the -function and . Wrong... As most probably have expected, the proof is fundamentally flawed. I've been thinking about this for some time now and am somewhat embarassed to say that the solution is so trivial. It would have saved me lots of time, if somebody here would have pointed it out earlier, since it could have been seen from my given exposition, but here we go... The problem with the proof lies within its formulation. It utterly overcomplicates matter where not necessary. This distracts from the relevant point and superficially makes it look as if it were correct, just like shenanigans try to distract their prey. While I have already simplified the steps significantly (I believe), it already more or less starts with Equation (1). It is not even wrong, but should have been more properly written as for , in order to keep track of error-terms. This shows that both functions share the same logarithmic singularity at . Now when integrating this Equation from to , it is not yet assumed that is a zero. This gives rise to the identity as , which is just the expansion of about the singular point . Now if is a zero with , then the constant term of the expansion vanishes identically, so that Equation (3) becomes This is where the flaw becomes apparent, because the interchange of limits is generally wrong and would have to be crucially justified. Evidently it would be the main part of the proof! I should also mention, that Equation (0) - even though he might have found it independently - can actually be traced back to a version from H. Hasse (1930).","\zeta \sigma>0 s=\sigma+it (s-1)\zeta(s) = \sum_{n=1}^\infty \frac{S_n(s)}{n+1} \tag{0} S_n(s)=-\frac{1}{n} \sum_{k=1}^n \binom{n}{k} (-1)^k \, k^{1-s} \zeta(s,x) |x|<1 (s-1)\zeta(s,x) = \sum_{n=1}^\infty \frac{S_n(s)}{n+1} \, x^n \sigma>0 x\rightarrow 1 S_n(s) n \frac{S_n(s)}{n+1} \sim \frac{1}{n(n+1)(\log n)^{1-s} \Gamma(s)} \left( 1 + {\cal O}\left(\frac{1}{\log n}\right) \right)\tag{as.1} \phi_s(x)=\sum_{n=0}^\infty \phi_n(s) \, x^n \phi_n(s) n \phi_s(x)=(1-x)\left(\frac{\log(1-x)}{-x}\right)^{s} \phi_n(s) \sim \frac{-s}{n^2 (\log n)^{1-s}} \left( 1 + {\cal O}\left(\frac{1}{\log n}\right) \right)\tag{as.2} n \phi_s'(x) \infty x\rightarrow 1 \zeta'(s,x) \infty x\rightarrow 1 \lim_{x\rightarrow 1} \frac{(s-1)\zeta'(s,x)}{\phi_s'(x)} = \lim_{n\rightarrow \infty} \frac{\frac{nS_n(s)}{n+1}}{n\phi_n(s)} = \frac{-1}{\Gamma(s+1)} \, . \tag{1} a_n\equiv\frac{nS_n(s)}{n+1} b_n\equiv n\phi_n(s) \lim_{n\rightarrow \infty}\frac{a_n}{b_n} = \frac{-1}{\Gamma(s+1)} \equiv c \frac{a_n}{b_n}=c+\epsilon_n \epsilon_n n\rightarrow \infty n |\epsilon_n| = {\cal O}\left(1/\log n\right) \, . a_n = cb_n + \epsilon_n b_n \sum_{n=1}^\infty a_n x^n = c \sum_{n=1}^\infty b_n x^n + \sum_{n=1}^\infty \epsilon_n b_n x^n \sum_{n=1}^\infty |\epsilon_n b_n| |\epsilon_n| |b_n| \sum_{n=2}^\infty |\epsilon_n b_n| \leq C|s| \sum_{n=2}^\infty \frac{1}{n(\log n)^{2-\sigma}} C<\infty 0<\sigma<1 s \lim_{x\rightarrow 1} \frac{(s-1)\zeta(s,x)}{\phi_s(x)} = \frac{-1}{\Gamma(s+1)} \, . \tag{2} \delta(x)\equiv \frac{(s-1)\zeta'(s,x)}{\phi_s'(x)} + \frac{1}{\Gamma(s+1)} \lim_{x\rightarrow 1} \delta(x) = 0 s \zeta'(s,x) x=1 \phi_s'(x) \leq 1 \int_x^1 \delta(t) \phi_s'(t) \, {\rm d}t = -(s-1)\zeta(s,x) - \frac{\phi_s(x)}{\Gamma(s+1)} -\frac{\int_x^1 \delta(t) \phi_s'(t) \, {\rm d}t}{\phi_s(x)} = \frac{(s-1)\zeta(s,x)}{\phi_s(x)} + \frac{1}{\Gamma(s+1)} \, . x\rightarrow 1 \left|\frac{\int_x^1 \delta(t) \phi_s'(t) \, {\rm d}t}{\phi_s(x)}\right| \leq \frac{\int_x^1 |\delta(t)| \, |\phi_s'(t)| \, {\rm d}t}{|\phi_s(x)|} \, . f(x) g(x) (x,1) x<1 h(t)=f(t) - \frac{f(x)}{g(x)} \, g(t) x\rightarrow 1 \lim_{x\rightarrow 1} |\delta(x)| \, \frac{|\phi_s'(x)|}{-|\phi_s(x)|'} = 0 \cdot 1 = 0 \, . s 1-s  \lim_{x\rightarrow 1} \left|\frac{(s-1)\zeta(s,x)}{\phi_s(x)}\frac{\phi_{1-s}(x)}{(-s)\zeta(1-s,x)}\right| = \left|\frac{\Gamma(2-s)}{\Gamma(s+1)}\right| \tag{3}  \lim_{x\rightarrow 1} \left|\frac{\phi_{1-s}(x)}{\phi_s(x)}\right| = \lim_{x\rightarrow 1} \left( \frac{\log(1-x)}{-x} \right)^{1-2\sigma} = \left|\frac{\zeta(1-s)\Gamma(1-s)}{\zeta(s)\Gamma(s)}\right|  \sigma =1/2 t \zeta \left| \frac{\zeta(1-s)\Gamma(1-s)}{\zeta(s)\Gamma(s)} \right| = \left| \frac{(2\pi)^{1-s}}{2\sin(\pi s/2)\Gamma(s)} \right| = 1  \left|\Gamma(1/2+it)\right|^2 = \frac{\pi}{\cosh(\pi t)} (s-1)\zeta'(s,x) = -\frac{\phi_s'(x)}{\Gamma(s+1)} + {\cal O}(1) x\rightarrow 1 x=1 x 1 s (s-1)\zeta(s,x)=(s-1)\zeta(s) - \frac{\phi_s(x)}{\Gamma(s+1)} + {\cal O}(1-x) x\rightarrow 1 (s-1)\zeta(s,x) x=1 s=s_0 0<\Re(s_0)<1 \lim_{x\rightarrow 1} \left|\frac{(s_0-1)\zeta(s_0,x)}{(-s_0)\zeta(1-s_0,x)}\right| = \lim_{x\rightarrow 1} \left| \frac{\phi_{s_0}(x)}{\phi_{1-s_0}(x)} \frac{\Gamma(2-s_0)}{\Gamma(s_0+1)}\right| \\ \stackrel{!}{=} \lim_{s\rightarrow s_0} \left| \frac{(s-1)\zeta(s,1)}{(-s)\zeta(1-s,1)} \right| = \lim_{s\rightarrow s_0} \left| \frac{(s-1)\zeta(s)}{(-s)\zeta(1-s)} \right| \, .","['analysis', 'riemann-zeta', 'divergent-series']"
88,Almost everywhere equal implies measurable,Almost everywhere equal implies measurable,,"I am still new to Lebesgue measure theory. I would like to get a verification on my proof. Statement: Assume a complete measure space. If $f$ is a measurable function and $f=g$ almost everywhere, then $g$ is measurable. Proof: Let $c$ be a real number and $N$ the null set such that $f=g$ on $N^C$ . Hence, $$g^{-1}(c,\infty)=[g^{-1}(c,\infty)\cap N]\cup[g^{-1}(c,\infty)\cap N^C].$$ The first RHS bracket is measurable since it is the subset of $N$ and the space is complete. The second is measurable because it is equal to $f^{-1}(c,\infty)\cap N^C$ , where $f^{-1}(c,\infty)$ is measurable by assumption and $N^C$ is measurable because $N$ is. Q.E.D. Are there missing details in the proof? Thanks a lot.","I am still new to Lebesgue measure theory. I would like to get a verification on my proof. Statement: Assume a complete measure space. If is a measurable function and almost everywhere, then is measurable. Proof: Let be a real number and the null set such that on . Hence, The first RHS bracket is measurable since it is the subset of and the space is complete. The second is measurable because it is equal to , where is measurable by assumption and is measurable because is. Q.E.D. Are there missing details in the proof? Thanks a lot.","f f=g g c N f=g N^C g^{-1}(c,\infty)=[g^{-1}(c,\infty)\cap N]\cup[g^{-1}(c,\infty)\cap N^C]. N f^{-1}(c,\infty)\cap N^C f^{-1}(c,\infty) N^C N","['analysis', 'measure-theory', 'lebesgue-measure']"
89,"What ""tools"" are available within pure mathematics to visualize more advanced topics? [closed]","What ""tools"" are available within pure mathematics to visualize more advanced topics? [closed]",,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 5 years ago . Improve this question What ""tools"" exist within mathematics to visualize concepts for more advanced areas of mathematics, particularly within Analysis, Topology and Algebra ? Furthermore, can one rigorously integrate such ""visual tools"" within proofs? As an example here are some that I've encountered so far:, I've seen commutative diagrams for compositions of maps. Another example is the following: If $X, W \subset \mathbf{R}^2$ are subsets of the plane, we can draw a picture of a map $f : X \to W$ by colouring $X$ with some pattern, and then colouring each point $(u,v)= f(x,y)$ by the same colour as $(x,y)$ as shown in the free book Stokes’s Theorem (Benjamin McKay) (on pages 3-4). Edit: By visualization, I mean anything that conveys information diagrammatically or without the use of words, which hold true for the examples I provided.","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 5 years ago . Improve this question What ""tools"" exist within mathematics to visualize concepts for more advanced areas of mathematics, particularly within Analysis, Topology and Algebra ? Furthermore, can one rigorously integrate such ""visual tools"" within proofs? As an example here are some that I've encountered so far:, I've seen commutative diagrams for compositions of maps. Another example is the following: If are subsets of the plane, we can draw a picture of a map by colouring with some pattern, and then colouring each point by the same colour as as shown in the free book Stokes’s Theorem (Benjamin McKay) (on pages 3-4). Edit: By visualization, I mean anything that conveys information diagrammatically or without the use of words, which hold true for the examples I provided.","X, W \subset \mathbf{R}^2 f : X \to W X (u,v)= f(x,y) (x,y)","['analysis', 'soft-question', 'visualization']"
90,The set of points reached exactly $n$ times is measurable,The set of points reached exactly  times is measurable,n,"Let $p:X \to Y$ be a measurable surjection and assume that for each $y \in Y$ the set $p^{-1}(y)$ is at most countable. Define $Y_n$ (for $n \in \mathbb{N} \cup \{ \infty \}$ ) to be the set of those $y \in Y$ for which there are exactly $n$ distinct $x \in X$ such that $p(x)=y$ . Is it clear that each $Y_n$ is measurable? EDIT: I forgot to add, $X,Y$ are assumed to be standard Borel spaces: i.e. sigma algebras are the sigma algebras of Borel set and $X$ and $Y$ are assumed to be complete, separable metric spaces","Let be a measurable surjection and assume that for each the set is at most countable. Define (for ) to be the set of those for which there are exactly distinct such that . Is it clear that each is measurable? EDIT: I forgot to add, are assumed to be standard Borel spaces: i.e. sigma algebras are the sigma algebras of Borel set and and are assumed to be complete, separable metric spaces","p:X \to Y y \in Y p^{-1}(y) Y_n n \in \mathbb{N} \cup \{ \infty \} y \in Y n x \in X p(x)=y Y_n X,Y X Y","['analysis', 'measure-theory', 'descriptive-set-theory', 'measurable-functions']"
91,Improper integrals over the reals and surreal numbers,Improper integrals over the reals and surreal numbers,,"Is it possible to assign improper integrals over the reals a surreal value in a consistent way? Are there any papers available on this? Note that I am not inquiring about formalizing integration over the surreal themselves, which I realize is still a somewhat open problem. Rather, I would like to identify an integral with a number $z$ in the surreals, e.g. $\int_{\mathbb{R}^+} x^2 dx = z, \; z\in \text{Surreals}$ with the ultimate purpose of comparing and performing algebra on the values of ""classically"" divergent integrals.","Is it possible to assign improper integrals over the reals a surreal value in a consistent way? Are there any papers available on this? Note that I am not inquiring about formalizing integration over the surreal themselves, which I realize is still a somewhat open problem. Rather, I would like to identify an integral with a number $z$ in the surreals, e.g. $\int_{\mathbb{R}^+} x^2 dx = z, \; z\in \text{Surreals}$ with the ultimate purpose of comparing and performing algebra on the values of ""classically"" divergent integrals.",,"['analysis', 'logic', 'field-theory', 'combinatorial-game-theory', 'surreal-numbers']"
92,Division of Distributions on $\mathbb{R}$,Division of Distributions on,\mathbb{R},"Let $H:\mathbb{R} \rightarrow \mathbb{C}$ be a function, with $H \in C^{\infty}(\mathbb{R})$, and $S \in \mathscr{D}'(R)$ a distribution such that there exists $x_0 \in \mathbb{R}$ which belongs to both the support of $H$ and the support of $S$. Assume that $H$ and all its derivatives vanish in $x_0$: \begin{equation} (D^{n}H)(x_0)=0 \qquad (n=0,1,2,\dots). \end{equation} I am trying to prove that in these hypotheses, the division of $S$ by $H$ is not possible. This means that there exists no distribution $T \in \mathscr{D}'(R)$ such that \begin{equation} H \cdot T = S. \end{equation} I could prove this statement only in a particular case (see Note (2) below), but I am quite convinced that it is true. Any help is welcome. Thank you very much in advance for your attention. NOTE (1). The statement above is a conjecture of mine, inspired by Schwartz, Théories des Distributions, Chapitre V, $\S 4$, p.126. He states that if $H$ is a function as described above, that is $H \in C^{\infty}(\mathbb{R})$ and $H$ vanishes with all its derivatives in a point $x_0$, then the division of a distribution $S$ by $H$ is not possible. Obviously, this is not true for a generic distribution $S$. To see this, assume that $H$ has as only zero $x_0$, and that $x_0$ does not lie in the support of $S$. For any non-empty open subset $\Omega$ of $\mathbb{R}$, define $S_{\Omega} \in \mathscr{D'}(\Omega)$ as \begin{equation} S_{\Omega}(\psi)=S(\psi) \quad (\psi \in \mathscr{D}(\Omega)). \end{equation} Then if $\Omega_1$ is the complement of the support of $S$, the zero distribution $T_1=0 \in \mathscr{D'}(\Omega_1)$ satisfies  \begin{equation} H \cdot T_1 = S_{\Omega_1}, \end{equation} since we have \begin{equation} T_1(H\psi) = S(\psi)=0 \quad \forall \psi \in \mathscr{D}(\Omega_1). \end{equation} Now put $\Omega_2=\mathbb{R} \backslash \{x_0 \}$. Since $1/H \in C^{\infty}(\Omega_2)$, $T_2=\frac{1}{H} \cdot S_{\Omega_2}$ is a well defined element of $\mathscr{D'}(\Omega_2)$ and we have \begin{equation} H \cdot T_2 = S_{\Omega_2}, \end{equation}  that is \begin{equation} T_2(H \psi) = S (\psi) \quad \forall \psi \in \mathscr{D}(\Omega_2). \end{equation} Now let $\xi_1 \in \mathscr{D}(\Omega_1)$ be such that $0 \leq \xi \leq 1$, and $\xi=1$ on an open set $V$ containing $x_0$, and define $\xi_2 = 1 - \xi_1$. Define  \begin{equation} T(\phi)=T_1(\xi_1 \phi)+T_2(\xi_2 \phi) \quad (\phi \in \mathscr{D}(\mathbb{R})). \end{equation} It is immediate to see that $T \in \mathscr{D'}(\mathbb{R})$ and that \begin{equation} H \cdot T = S. \end{equation} NOTE (2). I could prove the statement above in the specific case in which $S$ is the distribution defined by the constant function equal to one. In this case, take $\psi \in \mathscr{D}(R)$ such that  \begin{equation} \int_{\mathbb{R}} \psi(x) dx =1, \end{equation} and define the sequence of test functions \begin{equation} \psi_{m}(x)=m \psi(m(x-x_0)+x_0) \quad (x \in \mathbb{R}, m=1,2,3,\dots). \end{equation} It is easy to see that $S(\psi_m) =1$ for all $m$. Now set \begin{equation} \phi_m(x) = H(x) \psi_m(x) \quad (x \in \mathbb{R}, m=1,2,3,\dots). \end{equation} Define for any non-negative integers $n, m$ the function  \begin{equation} F_{n,m}(x) = \begin{cases} \frac{(D^n H)(x)}{(x-x_0)^m} & x \neq x_0, \\ 0 & x = x_0, \end{cases} \end{equation} We have $F_{n,m} \in C^{\infty}(\mathbb{R})$: in particular $F_{n,m}$ is continuous at $x_0$. From this observation and Leibniz formula we easily get that $\phi_m \rightarrow 0$ in $\mathscr{D}(\mathbb{R})$. So if there existed $T \in \mathscr{D'}(\mathbb{R})$ such that \begin{equation} H \cdot T = S, \end{equation} we should have $T(\phi_m) \rightarrow 0$. But we have $T(\phi_m)=T(H\psi_m)=S(\psi_m)=1$ for all $m$, a contradiction.","Let $H:\mathbb{R} \rightarrow \mathbb{C}$ be a function, with $H \in C^{\infty}(\mathbb{R})$, and $S \in \mathscr{D}'(R)$ a distribution such that there exists $x_0 \in \mathbb{R}$ which belongs to both the support of $H$ and the support of $S$. Assume that $H$ and all its derivatives vanish in $x_0$: \begin{equation} (D^{n}H)(x_0)=0 \qquad (n=0,1,2,\dots). \end{equation} I am trying to prove that in these hypotheses, the division of $S$ by $H$ is not possible. This means that there exists no distribution $T \in \mathscr{D}'(R)$ such that \begin{equation} H \cdot T = S. \end{equation} I could prove this statement only in a particular case (see Note (2) below), but I am quite convinced that it is true. Any help is welcome. Thank you very much in advance for your attention. NOTE (1). The statement above is a conjecture of mine, inspired by Schwartz, Théories des Distributions, Chapitre V, $\S 4$, p.126. He states that if $H$ is a function as described above, that is $H \in C^{\infty}(\mathbb{R})$ and $H$ vanishes with all its derivatives in a point $x_0$, then the division of a distribution $S$ by $H$ is not possible. Obviously, this is not true for a generic distribution $S$. To see this, assume that $H$ has as only zero $x_0$, and that $x_0$ does not lie in the support of $S$. For any non-empty open subset $\Omega$ of $\mathbb{R}$, define $S_{\Omega} \in \mathscr{D'}(\Omega)$ as \begin{equation} S_{\Omega}(\psi)=S(\psi) \quad (\psi \in \mathscr{D}(\Omega)). \end{equation} Then if $\Omega_1$ is the complement of the support of $S$, the zero distribution $T_1=0 \in \mathscr{D'}(\Omega_1)$ satisfies  \begin{equation} H \cdot T_1 = S_{\Omega_1}, \end{equation} since we have \begin{equation} T_1(H\psi) = S(\psi)=0 \quad \forall \psi \in \mathscr{D}(\Omega_1). \end{equation} Now put $\Omega_2=\mathbb{R} \backslash \{x_0 \}$. Since $1/H \in C^{\infty}(\Omega_2)$, $T_2=\frac{1}{H} \cdot S_{\Omega_2}$ is a well defined element of $\mathscr{D'}(\Omega_2)$ and we have \begin{equation} H \cdot T_2 = S_{\Omega_2}, \end{equation}  that is \begin{equation} T_2(H \psi) = S (\psi) \quad \forall \psi \in \mathscr{D}(\Omega_2). \end{equation} Now let $\xi_1 \in \mathscr{D}(\Omega_1)$ be such that $0 \leq \xi \leq 1$, and $\xi=1$ on an open set $V$ containing $x_0$, and define $\xi_2 = 1 - \xi_1$. Define  \begin{equation} T(\phi)=T_1(\xi_1 \phi)+T_2(\xi_2 \phi) \quad (\phi \in \mathscr{D}(\mathbb{R})). \end{equation} It is immediate to see that $T \in \mathscr{D'}(\mathbb{R})$ and that \begin{equation} H \cdot T = S. \end{equation} NOTE (2). I could prove the statement above in the specific case in which $S$ is the distribution defined by the constant function equal to one. In this case, take $\psi \in \mathscr{D}(R)$ such that  \begin{equation} \int_{\mathbb{R}} \psi(x) dx =1, \end{equation} and define the sequence of test functions \begin{equation} \psi_{m}(x)=m \psi(m(x-x_0)+x_0) \quad (x \in \mathbb{R}, m=1,2,3,\dots). \end{equation} It is easy to see that $S(\psi_m) =1$ for all $m$. Now set \begin{equation} \phi_m(x) = H(x) \psi_m(x) \quad (x \in \mathbb{R}, m=1,2,3,\dots). \end{equation} Define for any non-negative integers $n, m$ the function  \begin{equation} F_{n,m}(x) = \begin{cases} \frac{(D^n H)(x)}{(x-x_0)^m} & x \neq x_0, \\ 0 & x = x_0, \end{cases} \end{equation} We have $F_{n,m} \in C^{\infty}(\mathbb{R})$: in particular $F_{n,m}$ is continuous at $x_0$. From this observation and Leibniz formula we easily get that $\phi_m \rightarrow 0$ in $\mathscr{D}(\mathbb{R})$. So if there existed $T \in \mathscr{D'}(\mathbb{R})$ such that \begin{equation} H \cdot T = S, \end{equation} we should have $T(\phi_m) \rightarrow 0$. But we have $T(\phi_m)=T(H\psi_m)=S(\psi_m)=1$ for all $m$, a contradiction.",,"['real-analysis', 'analysis', 'distribution-theory']"
93,"Proof verification: $\frac{d(\nu_1\times \nu_2)}{d(\nu_1 \times \nu_2)}(x_1,x_2)=\frac{d\nu_1}{d\mu_1)}(x_1)\frac{d\nu_2}{d\mu_2}(x_2).$",Proof verification:,"\frac{d(\nu_1\times \nu_2)}{d(\nu_1 \times \nu_2)}(x_1,x_2)=\frac{d\nu_1}{d\mu_1)}(x_1)\frac{d\nu_2}{d\mu_2}(x_2).","This is exercise 3.12 from Folland's Real Analysis. It took me a long times to come up with a solution to this problem, and I'd appreciate it if anyone could verify if my answer is correct. For $j=1,2,$ let $\mu_j, \nu_j$ be $\sigma$-finite measures on $(X_j,\mathcal{M}_j)$ such that $\nu_j \ll \nu_j$. Then $\nu_1 \times \nu_2 \ll \mu_1 \times \mu_2$ and $$\frac{d(\nu_1\times \nu_2)}{d(\mu_1 \times \mu_2)}(x_1,x_2)=\frac{d\nu_1}{d\mu_1}(x_1)\frac{d\nu_2}{d\mu_2}(x_2).$$ The theorems I used from the text are captured at the bottom. Proof: First, assume that $\mu_1 \times \mu_2(E)=0.$ Then $\mu_1 \times \mu_2 (E)=\int \mu_1(E^{x_2})d\mu_2=0$. So $\mu_1(E^{x_2})=0$ $\mu_2$-a.e. Since $\nu_1 \ll \mu_1$, for a.e. on $X_2$, $\nu_1(E^{x_2})=0$, and hence $\nu_1 \times \nu_2 (E)=\int \nu_1(E^{x_2})d\nu_2=0$, which shows that $\nu_1 \times \nu_2 \ll \mu_1 \times \mu_2$. To prove the second part, by definition of the Radon-Nikodym derivative and its a.e.-uniqueness, it will suffice to show that for all $E \in \mathcal{M}\otimes \mathcal{N}$, we have $$\int_E \frac{d(\nu_1\times \nu_2)}{d(\mu_1 \times \mu_2)}d(\mu_1 \times \mu_2)=\nu_1 \times \nu_2 (E)=\int_E \frac{d\nu_1}{d\mu_1}(x_1)\frac{d\nu_2}{d\mu_2}(x_2)d(\mu_1 \times \mu_2).$$ To show this I need to use the Tonelli's theorem restricted on a measurable set. Namely, $\int_E f d(\mu \times \nu)=\int f\chi_E d(\mu \times \nu)=\int [\int f_x \chi_{E_x}d\nu]d\mu=\int[\int_{E_x} f_x d\nu] d\mu$. (I believe this is correct and my proof hinges on it, so tell me if it's wrong). Finally, we have $\nu_1 \times \nu_2(E)=\int \nu_2(E_{x_1})d\nu_1=\int \nu_2(E_{x_1})\frac{d\nu_1}{d\mu_1} d\mu_1=\int \int_{E_{x_1}}\frac{d\nu_2}{d\mu_2}d\mu_2 \frac{d\nu_1}{d\mu_1} d\mu_1 $. where the third equality follows from Prop 3.9(a) and the last equality follows from the definition of $d\nu_2/d\mu_2$. Also, $\int_E \frac{d\nu_1}{d\mu_1}(x_1)\frac{d\nu_2}{d\mu_2}(x_2) d(\mu_1 \times \mu_2)=\int \int_{E_{x_1}}\frac{d\nu_1}{d\mu_1}(x_1)\frac{d\nu_2}{d\mu_2}(x_2) d\mu_2(x_2) d\mu_1(x_1) $, from the Tonelli's theorem I've derived above. Now we have the desired equality and so the two derivatives are equal a.e. The problem doesn't state the equality in terms of a.e., but I believe it is just omitted. I'd appreciate any comments on this.","This is exercise 3.12 from Folland's Real Analysis. It took me a long times to come up with a solution to this problem, and I'd appreciate it if anyone could verify if my answer is correct. For $j=1,2,$ let $\mu_j, \nu_j$ be $\sigma$-finite measures on $(X_j,\mathcal{M}_j)$ such that $\nu_j \ll \nu_j$. Then $\nu_1 \times \nu_2 \ll \mu_1 \times \mu_2$ and $$\frac{d(\nu_1\times \nu_2)}{d(\mu_1 \times \mu_2)}(x_1,x_2)=\frac{d\nu_1}{d\mu_1}(x_1)\frac{d\nu_2}{d\mu_2}(x_2).$$ The theorems I used from the text are captured at the bottom. Proof: First, assume that $\mu_1 \times \mu_2(E)=0.$ Then $\mu_1 \times \mu_2 (E)=\int \mu_1(E^{x_2})d\mu_2=0$. So $\mu_1(E^{x_2})=0$ $\mu_2$-a.e. Since $\nu_1 \ll \mu_1$, for a.e. on $X_2$, $\nu_1(E^{x_2})=0$, and hence $\nu_1 \times \nu_2 (E)=\int \nu_1(E^{x_2})d\nu_2=0$, which shows that $\nu_1 \times \nu_2 \ll \mu_1 \times \mu_2$. To prove the second part, by definition of the Radon-Nikodym derivative and its a.e.-uniqueness, it will suffice to show that for all $E \in \mathcal{M}\otimes \mathcal{N}$, we have $$\int_E \frac{d(\nu_1\times \nu_2)}{d(\mu_1 \times \mu_2)}d(\mu_1 \times \mu_2)=\nu_1 \times \nu_2 (E)=\int_E \frac{d\nu_1}{d\mu_1}(x_1)\frac{d\nu_2}{d\mu_2}(x_2)d(\mu_1 \times \mu_2).$$ To show this I need to use the Tonelli's theorem restricted on a measurable set. Namely, $\int_E f d(\mu \times \nu)=\int f\chi_E d(\mu \times \nu)=\int [\int f_x \chi_{E_x}d\nu]d\mu=\int[\int_{E_x} f_x d\nu] d\mu$. (I believe this is correct and my proof hinges on it, so tell me if it's wrong). Finally, we have $\nu_1 \times \nu_2(E)=\int \nu_2(E_{x_1})d\nu_1=\int \nu_2(E_{x_1})\frac{d\nu_1}{d\mu_1} d\mu_1=\int \int_{E_{x_1}}\frac{d\nu_2}{d\mu_2}d\mu_2 \frac{d\nu_1}{d\mu_1} d\mu_1 $. where the third equality follows from Prop 3.9(a) and the last equality follows from the definition of $d\nu_2/d\mu_2$. Also, $\int_E \frac{d\nu_1}{d\mu_1}(x_1)\frac{d\nu_2}{d\mu_2}(x_2) d(\mu_1 \times \mu_2)=\int \int_{E_{x_1}}\frac{d\nu_1}{d\mu_1}(x_1)\frac{d\nu_2}{d\mu_2}(x_2) d\mu_2(x_2) d\mu_1(x_1) $, from the Tonelli's theorem I've derived above. Now we have the desired equality and so the two derivatives are equal a.e. The problem doesn't state the equality in terms of a.e., but I believe it is just omitted. I'd appreciate any comments on this.",,"['real-analysis', 'analysis', 'measure-theory', 'derivatives', 'proof-verification']"
94,Efficient of Newton Polynomial Evaluation,Efficient of Newton Polynomial Evaluation,,"The polynomial in Newton form having the coefficients $a_{0},a_{1},\ldots,a_{n}$ and centers $x_{1},x_{2},\ldots,x_{n}$ is the polynomial  $$ p(x) = a_{0} + a_{1} (x-x_{1}) +a_{2} (x-x_{1})(x-x_{2}) + \cdots + a_{n} (x-x_{1})\cdots (x-x_{n}). $$ Let $\alpha \in \mathbb{R}$ and set  $$\left\{ \begin{array}{ll} b_{n} &= a_{n}, \\ b_{i} & = a_{i} + b_{i+1} ( \alpha - x_{i+1}), \; \text{for } i=n-1,n-2,\ldots,0. \end{array} \right. $$ Then $b_{0} = p(\alpha)$. This is the generalized Horner's rule for evaluating Polynomial in Newton form. This algorithm takes at most 2n additions and n mutiplications. I am trying to prove that in the case $x_{i} \neq x_{j}, i \neq j$ and all the coeficients $a_{i}$ is nonzero and $\alpha \neq x_{i}$ for all $i$, this algorithm is optimal. I think that we may need to generalize the proof of Alexander Ostrowski in 1954 for Horner's rule (he proved that the number of additions required in evaluating arbitrary polynomial must be at least n, the degree of that polynomial), but I am struggling to do it. So my question is: Is this algorithm optimal in the case above?. Any help would be appreciated. Regards.","The polynomial in Newton form having the coefficients $a_{0},a_{1},\ldots,a_{n}$ and centers $x_{1},x_{2},\ldots,x_{n}$ is the polynomial  $$ p(x) = a_{0} + a_{1} (x-x_{1}) +a_{2} (x-x_{1})(x-x_{2}) + \cdots + a_{n} (x-x_{1})\cdots (x-x_{n}). $$ Let $\alpha \in \mathbb{R}$ and set  $$\left\{ \begin{array}{ll} b_{n} &= a_{n}, \\ b_{i} & = a_{i} + b_{i+1} ( \alpha - x_{i+1}), \; \text{for } i=n-1,n-2,\ldots,0. \end{array} \right. $$ Then $b_{0} = p(\alpha)$. This is the generalized Horner's rule for evaluating Polynomial in Newton form. This algorithm takes at most 2n additions and n mutiplications. I am trying to prove that in the case $x_{i} \neq x_{j}, i \neq j$ and all the coeficients $a_{i}$ is nonzero and $\alpha \neq x_{i}$ for all $i$, this algorithm is optimal. I think that we may need to generalize the proof of Alexander Ostrowski in 1954 for Horner's rule (he proved that the number of additions required in evaluating arbitrary polynomial must be at least n, the degree of that polynomial), but I am struggling to do it. So my question is: Is this algorithm optimal in the case above?. Any help would be appreciated. Regards.",,"['real-analysis', 'analysis', 'numerical-methods']"
95,Every Continuous Function Attains Maximum Implies Compact,Every Continuous Function Attains Maximum Implies Compact,,"Let $K$ be a compact metric space, and let $A \subset K$. Prove that $A$ is compact if and only if, for every continuous function $f:K \to \mathbb{R}$,  there exists a $q \in A$ so that $$f(q) = \max_{a \in A} \{f(a)\}.$$ i.e. $f$ attains its maximum on the restriction to $A$. Here's an attempt: The forward direction is a standard theorem. Suppose conversely every restriction attains its maximum on $A$. Let $\rho \in \overline{A}$. We shall show $\rho \in A$ and therefore $A$ is closed, and being a subset of a compact space, compact. Suppose $\rho \notin A$. Let $\{a_n\}_{n \in \mathbb{N}} \to \rho$. There exists a continuous function $f: K \to \mathbb{R}$ which attains its maximum at $\rho$ (for example, $d:A \to \mathbb{R}$ with $d: x \mapsto -d(x,\rho)$). In particular, $f: \overline{A} \to \mathbb{R}$ is continuous and has a maximum at $\rho$. Therefore, given $\epsilon >0$, there exists $\delta >0$ so that $f(B_\delta(\rho)) \subset B_\epsilon (f(\rho)).$ Choose any $a_{n_1} \in B_\delta(\rho)$. Then, we may take $0 < \tilde{\epsilon} < \epsilon$ and $0< \tilde{\delta}< d(a_{n_1},\rho)$ so that, given $a_{n_2} \in B_{\tilde{\delta}}$, $f(a_{n_2}) > f(a_{n_1})$. Repeating this process indefinitely, we see that $f$ cannot attain a maximum on $a$, a contradiction. We conclude $\rho \in A$, so that $A$ is compact. Any issues? Other methods of proof are welcome. In fact, it'd be great to see alternatives. I've never seen this result anywhere, so I thought it interesting.","Let $K$ be a compact metric space, and let $A \subset K$. Prove that $A$ is compact if and only if, for every continuous function $f:K \to \mathbb{R}$,  there exists a $q \in A$ so that $$f(q) = \max_{a \in A} \{f(a)\}.$$ i.e. $f$ attains its maximum on the restriction to $A$. Here's an attempt: The forward direction is a standard theorem. Suppose conversely every restriction attains its maximum on $A$. Let $\rho \in \overline{A}$. We shall show $\rho \in A$ and therefore $A$ is closed, and being a subset of a compact space, compact. Suppose $\rho \notin A$. Let $\{a_n\}_{n \in \mathbb{N}} \to \rho$. There exists a continuous function $f: K \to \mathbb{R}$ which attains its maximum at $\rho$ (for example, $d:A \to \mathbb{R}$ with $d: x \mapsto -d(x,\rho)$). In particular, $f: \overline{A} \to \mathbb{R}$ is continuous and has a maximum at $\rho$. Therefore, given $\epsilon >0$, there exists $\delta >0$ so that $f(B_\delta(\rho)) \subset B_\epsilon (f(\rho)).$ Choose any $a_{n_1} \in B_\delta(\rho)$. Then, we may take $0 < \tilde{\epsilon} < \epsilon$ and $0< \tilde{\delta}< d(a_{n_1},\rho)$ so that, given $a_{n_2} \in B_{\tilde{\delta}}$, $f(a_{n_2}) > f(a_{n_1})$. Repeating this process indefinitely, we see that $f$ cannot attain a maximum on $a$, a contradiction. We conclude $\rho \in A$, so that $A$ is compact. Any issues? Other methods of proof are welcome. In fact, it'd be great to see alternatives. I've never seen this result anywhere, so I thought it interesting.",,"['real-analysis', 'analysis', 'proof-verification']"
96,Equivalent definition of Cauchy sequence,Equivalent definition of Cauchy sequence,,"A sequence $x_i$ is Cauchy if for all $r>0$, there exists $n$ s.t. $i,j\geq n$ implies $d(x_i,x_j)<r$. My question is, is it equivalent to define Cauchy as follows? $x_i$ is Cauchy if for all $r>0$, there exists $n$ such that $i>n$ implies $d(x_n,x_i)<r$. If $x_n$ is Cauchy then it obviously satisfies my definition (just set $i=n$). If it satisfies my definition, then let's say we're given an $r$. By my definition, there exists $n$ s.t. $i\geq n$ implies $x_i\in B(x_n,r/2)$. So if $i,j\geq n$, then $x_i,x_j$ are both in this ball, so $d(x_i,x_j)<r$ by triangle inequality. I ask because my version seems easier to prove. You only have to consider $(n,i)$ s.t. $i>n$ instead of all pairs $(i,j)$ with $i,j\geq n$. It's also nicer to visualize because I can imagine all $x_i$ s.t. $i\geq n$ contained in this nice ball.","A sequence $x_i$ is Cauchy if for all $r>0$, there exists $n$ s.t. $i,j\geq n$ implies $d(x_i,x_j)<r$. My question is, is it equivalent to define Cauchy as follows? $x_i$ is Cauchy if for all $r>0$, there exists $n$ such that $i>n$ implies $d(x_n,x_i)<r$. If $x_n$ is Cauchy then it obviously satisfies my definition (just set $i=n$). If it satisfies my definition, then let's say we're given an $r$. By my definition, there exists $n$ s.t. $i\geq n$ implies $x_i\in B(x_n,r/2)$. So if $i,j\geq n$, then $x_i,x_j$ are both in this ball, so $d(x_i,x_j)<r$ by triangle inequality. I ask because my version seems easier to prove. You only have to consider $(n,i)$ s.t. $i>n$ instead of all pairs $(i,j)$ with $i,j\geq n$. It's also nicer to visualize because I can imagine all $x_i$ s.t. $i\geq n$ contained in this nice ball.",,"['analysis', 'metric-spaces', 'cauchy-sequences']"
97,Are stable manifold for gradient flows embedded submanifold?,Are stable manifold for gradient flows embedded submanifold?,,"Generally, the stable manifolds $W^s(p)$ of a diffeomorphism $\phi:M\to M$ is no embedded submanifold. The injective immersion $$ E^s:T_p^sM\to M $$ does not need to be a homeomorphism onto its image $W^s(p)$. This is a popular image that illustrates this fact: However, in the case that the diffeomorphism $\phi$ comes from a gradient flow, I think the stable manifold must be a submanifold. (More generally I think this is the case when there exists a strict Lyapunov function). Can anybody tell me how to prove this?","Generally, the stable manifolds $W^s(p)$ of a diffeomorphism $\phi:M\to M$ is no embedded submanifold. The injective immersion $$ E^s:T_p^sM\to M $$ does not need to be a homeomorphism onto its image $W^s(p)$. This is a popular image that illustrates this fact: However, in the case that the diffeomorphism $\phi$ comes from a gradient flow, I think the stable manifold must be a submanifold. (More generally I think this is the case when there exists a strict Lyapunov function). Can anybody tell me how to prove this?",,"['analysis', 'differential-geometry', 'manifolds', 'dynamical-systems']"
98,Divergence of the sequence $\sin(n!)$,Divergence of the sequence,\sin(n!),Does the sequence $\sin(n!)$ diverge(converge)? It seems the sequence diverges. I tried for a contradiction but with no success. Thanks for your cooperation.,Does the sequence $\sin(n!)$ diverge(converge)? It seems the sequence diverges. I tried for a contradiction but with no success. Thanks for your cooperation.,,['analysis']
99,Prove $\ell_1$ is complete,Prove  is complete,\ell_1,"I try to prove that $\ell_1$, the space of absolutely convergent sequences in $\mathbb{C}$ with norm $\| x \| = \sum_{k=1}^{\infty} |x_k|$, is complete. I already proved that, if $\{ x_n \}$ is a Cauchy sequence (with elements in $\ell_1$), then, for a fixed $k \in \mathbb{N}$, the sequence $\{ x_{n,k} \}$ is Cauchy in $\mathbb{C}$. The obvious way to define a limit of a Cauchy sequence $\{ x_n \}$ is to take the limits of $\{ x_{n,k}\}$, which exist since $\mathbb{C}$ is complete. Let's call this sequence $x$. But now we only know $x_n \to x$ pointwise. Problem: how do I prove $\lim_{n \to \infty} \| x_n - x \| = \lim_{n \to \infty} \sum_{k=1}^{\infty} |x_{n,k} - x_k| = 0$? I suppose I need to use the absolute convergence of $x_n$, but I do not see how. If I can prove the above, it follows that $x \in \ell_1$ therefore $\ell_1$ is complete. Edit: Knowing $x \in \ell_1$, to establish convergence, I know (for $n,m>N$) $\sum_{k=1}^K |x_{m,k} − x_{n,k} | \leq \|x_m−x_n\| < \epsilon$. I let $m \to \infty$, can I conclude $\sum_{k=1}^K |x_k − x_{n,k}| \leq \| x−x_n \| < \epsilon$? That seems odd. But so does $\sum^K_{k=1} | x_k − x_{n,k} | \leq \| x_m − x_n \|$. What can I say when $m \to \infty$?","I try to prove that $\ell_1$, the space of absolutely convergent sequences in $\mathbb{C}$ with norm $\| x \| = \sum_{k=1}^{\infty} |x_k|$, is complete. I already proved that, if $\{ x_n \}$ is a Cauchy sequence (with elements in $\ell_1$), then, for a fixed $k \in \mathbb{N}$, the sequence $\{ x_{n,k} \}$ is Cauchy in $\mathbb{C}$. The obvious way to define a limit of a Cauchy sequence $\{ x_n \}$ is to take the limits of $\{ x_{n,k}\}$, which exist since $\mathbb{C}$ is complete. Let's call this sequence $x$. But now we only know $x_n \to x$ pointwise. Problem: how do I prove $\lim_{n \to \infty} \| x_n - x \| = \lim_{n \to \infty} \sum_{k=1}^{\infty} |x_{n,k} - x_k| = 0$? I suppose I need to use the absolute convergence of $x_n$, but I do not see how. If I can prove the above, it follows that $x \in \ell_1$ therefore $\ell_1$ is complete. Edit: Knowing $x \in \ell_1$, to establish convergence, I know (for $n,m>N$) $\sum_{k=1}^K |x_{m,k} − x_{n,k} | \leq \|x_m−x_n\| < \epsilon$. I let $m \to \infty$, can I conclude $\sum_{k=1}^K |x_k − x_{n,k}| \leq \| x−x_n \| < \epsilon$? That seems odd. But so does $\sum^K_{k=1} | x_k − x_{n,k} | \leq \| x_m − x_n \|$. What can I say when $m \to \infty$?",,['analysis']
