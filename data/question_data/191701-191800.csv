,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What is the derivative of $x^{x^{x^{x^{.^{.^{.}}}}}}$ [duplicate],What is the derivative of  [duplicate],x^{x^{x^{x^{.^{.^{.}}}}}},"This question already has answers here : derivative of x^x^x... to infinity? (2 answers) Closed 7 years ago . Here is my attempt: Substituting y for infinite x powers: $$x^{x^{x^{x^{.^{.^{.}}}}}}=y → x^y=y $$ Giving: $$x=y^{\frac{1}{y}}$$ Take natural logs & differentiate with respect to $y$: $$ln(x)=ln(y^\frac{1}{y}) → ln(x)=\frac{1}{y}ln(y^\frac{1}{y})$$ $$\frac{1}{x}\frac{dx}{dy}=-\frac{1}{y^2}ln(y)+\frac{1}{y^2}$$ $$\frac{dx}{dy}=x\left(\frac{1-ln(y)}{y^2}\right)$$ Sub. in $y^{\frac{1}{y}}$ for $x$: $$\frac{dx}{dy}=y^{\frac{1}{y}}\left(\frac{1-ln(y)}{y^2}\right)$$ $$\frac{dx}{dy}=y^{\frac{1}{y}-2}\left(1-ln(y)\right)$$ Inverse $\frac{dx}{dy}$ to get $\frac{dy}{dx}$: $$\frac{dy}{dx}=\left[y^{\frac{1}{y}-2}\left(1-ln(y)\right)\right]^{-1}$$ Therefore:  $$\frac{dy}{dx}=\frac{y^{2-\frac{1}{y}}}{1-ln(y)}$$ Have I made a mistake anywhere? Have I made a false assumption? Please kindly provide some guidance, thanks.","This question already has answers here : derivative of x^x^x... to infinity? (2 answers) Closed 7 years ago . Here is my attempt: Substituting y for infinite x powers: $$x^{x^{x^{x^{.^{.^{.}}}}}}=y → x^y=y $$ Giving: $$x=y^{\frac{1}{y}}$$ Take natural logs & differentiate with respect to $y$: $$ln(x)=ln(y^\frac{1}{y}) → ln(x)=\frac{1}{y}ln(y^\frac{1}{y})$$ $$\frac{1}{x}\frac{dx}{dy}=-\frac{1}{y^2}ln(y)+\frac{1}{y^2}$$ $$\frac{dx}{dy}=x\left(\frac{1-ln(y)}{y^2}\right)$$ Sub. in $y^{\frac{1}{y}}$ for $x$: $$\frac{dx}{dy}=y^{\frac{1}{y}}\left(\frac{1-ln(y)}{y^2}\right)$$ $$\frac{dx}{dy}=y^{\frac{1}{y}-2}\left(1-ln(y)\right)$$ Inverse $\frac{dx}{dy}$ to get $\frac{dy}{dx}$: $$\frac{dy}{dx}=\left[y^{\frac{1}{y}-2}\left(1-ln(y)\right)\right]^{-1}$$ Therefore:  $$\frac{dy}{dx}=\frac{y^{2-\frac{1}{y}}}{1-ln(y)}$$ Have I made a mistake anywhere? Have I made a false assumption? Please kindly provide some guidance, thanks.",,"['calculus', 'derivatives', 'infinity', 'implicit-differentiation']"
1,Application of Derivatives rigorous proof,Application of Derivatives rigorous proof,,"Let $f:R\rightarrow R$ be a function such that all its successive derivatives exist in all $R$ and also $f(x)f''(x)\leq 0$ everywhere. If $\alpha$ and $\beta$ be two successive roots of $f(x)=0$. Then prove that $f'''(x)=0$ for atleast one $\gamma \in(\alpha,\beta)$. My Attempt: I began by taking an example $f(x)=(x-1)(2-x)$ and let $x=1$ and $x=2$ be its two successive roots.The statement is trivially true. Then I took the function $f(x)=e^{-x}(x-1)(2-x)$.The statement is true here also so on and so forth. But what would be the exact proof I wonder. It appears to be a question of Rolle's Theorem.","Let $f:R\rightarrow R$ be a function such that all its successive derivatives exist in all $R$ and also $f(x)f''(x)\leq 0$ everywhere. If $\alpha$ and $\beta$ be two successive roots of $f(x)=0$. Then prove that $f'''(x)=0$ for atleast one $\gamma \in(\alpha,\beta)$. My Attempt: I began by taking an example $f(x)=(x-1)(2-x)$ and let $x=1$ and $x=2$ be its two successive roots.The statement is trivially true. Then I took the function $f(x)=e^{-x}(x-1)(2-x)$.The statement is true here also so on and so forth. But what would be the exact proof I wonder. It appears to be a question of Rolle's Theorem.",,"['calculus', 'derivatives']"
2,Derivative of exponent,Derivative of exponent,,Looking to solve : $$ \frac{d}{dx}[2^{0.5x}]$$ The multiplication and X value in the exponent is confusing me. Help? Thanks!,Looking to solve : $$ \frac{d}{dx}[2^{0.5x}]$$ The multiplication and X value in the exponent is confusing me. Help? Thanks!,,"['calculus', 'derivatives', 'exponentiation']"
3,Prove the exterior derivative of the following (n-1) form is zero,Prove the exterior derivative of the following (n-1) form is zero,,Let $\omega(x)=\frac{1}{{\parallel x \parallel}^n}\displaystyle\sum_{i=1}^{n}(-1)^{i-1}x_{i} dx_{1} \wedge \dots \wedge \widehat{dx_{i}} \wedge \dots \wedge dx_{n}$ be a differential $(n-1)$ form on $\mathbb{R}^n \setminus\{0\}$. Where $\widehat{dx_{i}}$ means that the $i$th term is absent from the product. Prove that the exterior derivative $d\omega=0$. What I have so far is $d\omega(x)=\displaystyle\sum_{i=1}^{n}d(\frac{1}{{\parallel x \parallel}^n})(-1)^{i-1}dx_{i}\wedge dx_{1} \wedge \dots \wedge \widehat{dx_{i}} \wedge \dots \wedge dx_{n}$ $=\displaystyle\sum_{i=1}^{n}d(\frac{1}{{\parallel x \parallel}^n})(-1)^{i-1} (-1)^{i}dx_{1} \wedge \dots \wedge dx_{n}$ $=\displaystyle\sum_{i=1}^{n}d(\frac{1}{{\parallel x \parallel}^n})dx_{1} \wedge \dots \wedge dx_{n}$ However I am unsure on how to proceed from here. I'm also not sure if what I have done so far is even correct. Any help would be greatly appreciated.,Let $\omega(x)=\frac{1}{{\parallel x \parallel}^n}\displaystyle\sum_{i=1}^{n}(-1)^{i-1}x_{i} dx_{1} \wedge \dots \wedge \widehat{dx_{i}} \wedge \dots \wedge dx_{n}$ be a differential $(n-1)$ form on $\mathbb{R}^n \setminus\{0\}$. Where $\widehat{dx_{i}}$ means that the $i$th term is absent from the product. Prove that the exterior derivative $d\omega=0$. What I have so far is $d\omega(x)=\displaystyle\sum_{i=1}^{n}d(\frac{1}{{\parallel x \parallel}^n})(-1)^{i-1}dx_{i}\wedge dx_{1} \wedge \dots \wedge \widehat{dx_{i}} \wedge \dots \wedge dx_{n}$ $=\displaystyle\sum_{i=1}^{n}d(\frac{1}{{\parallel x \parallel}^n})(-1)^{i-1} (-1)^{i}dx_{1} \wedge \dots \wedge dx_{n}$ $=\displaystyle\sum_{i=1}^{n}d(\frac{1}{{\parallel x \parallel}^n})dx_{1} \wedge \dots \wedge dx_{n}$ However I am unsure on how to proceed from here. I'm also not sure if what I have done so far is even correct. Any help would be greatly appreciated.,,"['differential-geometry', 'derivatives', 'differential-forms', 'tensors', 'exterior-algebra']"
4,Directional derivative of determinant at the identity is the trace of the matrix?,Directional derivative of determinant at the identity is the trace of the matrix?,,"Let $f:A\mapsto \rm{det}(A)$, Prove that $\left(Df\right)_{{\rm id}}\left(H\right)={\rm tr}\left(H\right)$     for all $H\in\mathcal{L}\left(\mathbb{R}^{n}\to\mathbb{R}^{n}\right)$. The question appears also here: Directional derivative of the determinant but with no answers apart from that of the poster itself, and his solution uses some identities regarding the characteristic polynomial I do not understand. Additionally I think his approach assumes $H$ is invertible, which we are not given. Naturally I have also tried calculating the directional derivative, giving $$ \lim_{t\to0}\frac{\det(tH+\rm{id})-\det{\rm{id}}}{t}=\lim_{t\to0}\frac{\det(tH+\rm{id})-1}{t} $$ where I want to prove it inductively by extracting the first row to have $$\rm{det}(tH+\rm{id}) = \rm{det}\pmatrix{1\ 0\ \dots \ 0 \\(tH+\rm{id})_2\\\ \vdots \ \\(tH+\rm{id})_n} + t\rm{det}\pmatrix{H_{1,1}\ H_{1,2}\ \dots \ H_{1,n} \\(tH+\rm{id})_2\\\ \vdots \ \\(tH+\rm{id})_n} $$ (where $(A)_i$ is just the $i$th row of the matrix) where I want to say that the left determinant is the sum of $tH_{i,i}$ for $i\geq 2$ by induction which leaves me with showing the right side is $tH_{1,1}$, but I'm not sure how to proceed with that.","Let $f:A\mapsto \rm{det}(A)$, Prove that $\left(Df\right)_{{\rm id}}\left(H\right)={\rm tr}\left(H\right)$     for all $H\in\mathcal{L}\left(\mathbb{R}^{n}\to\mathbb{R}^{n}\right)$. The question appears also here: Directional derivative of the determinant but with no answers apart from that of the poster itself, and his solution uses some identities regarding the characteristic polynomial I do not understand. Additionally I think his approach assumes $H$ is invertible, which we are not given. Naturally I have also tried calculating the directional derivative, giving $$ \lim_{t\to0}\frac{\det(tH+\rm{id})-\det{\rm{id}}}{t}=\lim_{t\to0}\frac{\det(tH+\rm{id})-1}{t} $$ where I want to prove it inductively by extracting the first row to have $$\rm{det}(tH+\rm{id}) = \rm{det}\pmatrix{1\ 0\ \dots \ 0 \\(tH+\rm{id})_2\\\ \vdots \ \\(tH+\rm{id})_n} + t\rm{det}\pmatrix{H_{1,1}\ H_{1,2}\ \dots \ H_{1,n} \\(tH+\rm{id})_2\\\ \vdots \ \\(tH+\rm{id})_n} $$ (where $(A)_i$ is just the $i$th row of the matrix) where I want to say that the left determinant is the sum of $tH_{i,i}$ for $i\geq 2$ by induction which leaves me with showing the right side is $tH_{1,1}$, but I'm not sure how to proceed with that.",,"['real-analysis', 'derivatives', 'determinant']"
5,"Show that the equation of the tangent to the parabola $y^2=4ax$ at the point (p,q) is $qy=2a(x+p)$","Show that the equation of the tangent to the parabola  at the point (p,q) is",y^2=4ax qy=2a(x+p),"Question: Show that the equation of the tangent to the parabola $y^2=4ax$ at the point (p,q) is $qy=2a(x+p)$ These are my two approaches: First approach: If we have $(p,q)$ as $(x_1,y_1)$ $$y^2=4ax$$ $$ 2y \left( \frac{dy}{dx}\right) = 4a$$ $$ \frac{dy}{dx} = \frac{2a}{y} $$ At $y=q$ $$ \frac{dy}{dx} = \frac{2a}{q} $$ $$ m = \frac{2a}{q} $$ As equation of tangent is $$ (y-y_1) = m(x-x_1) $$ Plugging in values $$ (y-q) = \frac{2a}{q}(x-p) $$ $$ y-q = \frac{2ax}{q} - \frac{2ap}{q} $$ $$ y-q = \frac{2ax-2ap}{q} $$ $$ qy - q^2 = 2ax-2ap $$ $$ qy -q^2 = 2a(x-p) $$ But this does not satisfy the proof My second approach: If $(p,q)$ are points on $y^2=4ax$ then plugging into $$ y^2 = 4ax $$ at $x=p$ $$ y^2 = 4ap $$ $$ y = \sqrt{4ap} $$ at $y=q$ $$ q^2 = 4ax $$ $$ x = \frac{q^2}{4a} $$ Finding gradient: $$y^2=4ax$$ $$ 2y \left( \frac{dy}{dx}\right) = 4a$$ $$ \frac{dy}{dx} = \frac{2a}{y} $$ At $y = \sqrt{4ap} $ $$ \frac{dy}{dx}  = \frac{a}{\sqrt{ap}} $$ $$  (y-y_1) = m(x-x_1) $$ $$ y-\sqrt{4ap}=\frac{a}{\sqrt{ap}}(x-\frac{q^2}{4a})$$ But I don't think this gives me the proof as well...","Question: Show that the equation of the tangent to the parabola $y^2=4ax$ at the point (p,q) is $qy=2a(x+p)$ These are my two approaches: First approach: If we have $(p,q)$ as $(x_1,y_1)$ $$y^2=4ax$$ $$ 2y \left( \frac{dy}{dx}\right) = 4a$$ $$ \frac{dy}{dx} = \frac{2a}{y} $$ At $y=q$ $$ \frac{dy}{dx} = \frac{2a}{q} $$ $$ m = \frac{2a}{q} $$ As equation of tangent is $$ (y-y_1) = m(x-x_1) $$ Plugging in values $$ (y-q) = \frac{2a}{q}(x-p) $$ $$ y-q = \frac{2ax}{q} - \frac{2ap}{q} $$ $$ y-q = \frac{2ax-2ap}{q} $$ $$ qy - q^2 = 2ax-2ap $$ $$ qy -q^2 = 2a(x-p) $$ But this does not satisfy the proof My second approach: If $(p,q)$ are points on $y^2=4ax$ then plugging into $$ y^2 = 4ax $$ at $x=p$ $$ y^2 = 4ap $$ $$ y = \sqrt{4ap} $$ at $y=q$ $$ q^2 = 4ax $$ $$ x = \frac{q^2}{4a} $$ Finding gradient: $$y^2=4ax$$ $$ 2y \left( \frac{dy}{dx}\right) = 4a$$ $$ \frac{dy}{dx} = \frac{2a}{y} $$ At $y = \sqrt{4ap} $ $$ \frac{dy}{dx}  = \frac{a}{\sqrt{ap}} $$ $$  (y-y_1) = m(x-x_1) $$ $$ y-\sqrt{4ap}=\frac{a}{\sqrt{ap}}(x-\frac{q^2}{4a})$$ But I don't think this gives me the proof as well...",,"['derivatives', 'analytic-geometry', 'conic-sections']"
6,"What is the partial derivative of $f(x,y(x))$?",What is the partial derivative of ?,"f(x,y(x))","What is the total derivative of $f(x,y(x,z))$ with respect to $x$? Is it $$\frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}\frac{\partial y}{\partial x}?$$ If this is correct, what is $\frac{\partial f}{\partial x}$? It seems to me that partial $f$ partial $x$ is equal to the derivative of $f$ with respect to $x$. What is the difference? I mean, suppose $f(x,y(x,z))=x^2+y(x,z)$ and $y(x,z)=x^2+z.$ So we have $f(x,y(x,z))=2x^2+z.$ Hence $\frac{\partial f}{\partial x}=4x.$ On the other hand, $\frac{df}{dx}=\frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}\frac{\partial y}{\partial x}=4x+1\cdot 2x=6x.$ Am I making a mistake?","What is the total derivative of $f(x,y(x,z))$ with respect to $x$? Is it $$\frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}\frac{\partial y}{\partial x}?$$ If this is correct, what is $\frac{\partial f}{\partial x}$? It seems to me that partial $f$ partial $x$ is equal to the derivative of $f$ with respect to $x$. What is the difference? I mean, suppose $f(x,y(x,z))=x^2+y(x,z)$ and $y(x,z)=x^2+z.$ So we have $f(x,y(x,z))=2x^2+z.$ Hence $\frac{\partial f}{\partial x}=4x.$ On the other hand, $\frac{df}{dx}=\frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}\frac{\partial y}{\partial x}=4x+1\cdot 2x=6x.$ Am I making a mistake?",,"['calculus', 'derivatives']"
7,"Finding the formula for acceleration from $v=2s^3+5s$, where $s$ is the displacement at time $t$","Finding the formula for acceleration from , where  is the displacement at time",v=2s^3+5s s t,"This is the question: I first found $\frac{dv}{ds}=6s^2+5$, then I tried to find $\frac{ds}{dt}$ by messing about a little with implicit differentiation, but I had no luck and I therefore couldn't apply the chain rule (i.e. $a=\frac{dv}{dt}=\frac{ds}{dt}\frac{dv}{ds}$) to find acceleration. The back of my book tells me the answer is $(6s^2+5)(2s^3+5s)$, but I fail to see how this is true as it would imply that $\frac{ds}{dt}=v$, which I can't exactly understand. Can anyone tell me what I am doing wrong?","This is the question: I first found $\frac{dv}{ds}=6s^2+5$, then I tried to find $\frac{ds}{dt}$ by messing about a little with implicit differentiation, but I had no luck and I therefore couldn't apply the chain rule (i.e. $a=\frac{dv}{dt}=\frac{ds}{dt}\frac{dv}{ds}$) to find acceleration. The back of my book tells me the answer is $(6s^2+5)(2s^3+5s)$, but I fail to see how this is true as it would imply that $\frac{ds}{dt}=v$, which I can't exactly understand. Can anyone tell me what I am doing wrong?",,"['calculus', 'derivatives', 'mathematical-physics', 'applications', 'kinematics']"
8,"If $(u,v)$ is a point on $4x^2+a^2y^2=4a^2$,where $4<a^2<8$,that is farthest from $(0,-2)$ then $u+v$ is equal to?","If  is a point on ,where ,that is farthest from  then  is equal to?","(u,v) 4x^2+a^2y^2=4a^2 4<a^2<8 (0,-2) u+v","If $(u,v)$ is a point on $4x^2+a^2y^2=4a^2$,where $4<a^2<8$,that is farthest from $(0,-2)$ then $u+v$ is equal to? My Approach: I took a parametric point $(t,4-4t^2/a^2)$.And then tried to find the minima of the distance.But that is too lengthy method.Any other suggestions?","If $(u,v)$ is a point on $4x^2+a^2y^2=4a^2$,where $4<a^2<8$,that is farthest from $(0,-2)$ then $u+v$ is equal to? My Approach: I took a parametric point $(t,4-4t^2/a^2)$.And then tried to find the minima of the distance.But that is too lengthy method.Any other suggestions?",,[]
9,Taylor series for $f(x)=\cot(x)$,Taylor series for,f(x)=\cot(x),"Trying to expand $f(x)=\cot(x)$ to Taylor series (Maclaurin, actually).  But I keep ""adding up"" infinities when using the formula. (Because of $\cot(0)=\infty$) Could you perhaps give me a hint on how to proceed?","Trying to expand $f(x)=\cot(x)$ to Taylor series (Maclaurin, actually).  But I keep ""adding up"" infinities when using the formula. (Because of $\cot(0)=\infty$) Could you perhaps give me a hint on how to proceed?",,"['calculus', 'derivatives', 'taylor-expansion', 'singularity']"
10,Closed form for $n$-th derivative of $\sqrt{f(x)}$ for general $f(x)$,Closed form for -th derivative of  for general,n \sqrt{f(x)} f(x),"Let's assume we have an inifinitely differentiable real valued function $f(x)$, and we have a closed form expression for all its derivatives. Is it then possible to find a closed form for the $n$-th derivative of $\sqrt{f(x)}$?","Let's assume we have an inifinitely differentiable real valued function $f(x)$, and we have a closed form expression for all its derivatives. Is it then possible to find a closed form for the $n$-th derivative of $\sqrt{f(x)}$?",,"['calculus', 'derivatives', 'taylor-expansion']"
11,Finding Points of Inflection (Standard Deviation),Finding Points of Inflection (Standard Deviation),,Question: A Random Variable that is normally distributed with mean $\mu$ and standard deviation $\sigma$ has a probability density function of $$ f(x) = {1\over {\sigma}{\sqrt {2\pi}}} e^{(x-\mu)^2\over 2 \sigma^2} $$ Find the values of $x$ where the curve has points of inflection What I have attempted: $$ f(x) = {1\over {\sigma}{\sqrt {2\pi}}}e^{(x-\mu)^2\over 2{\sigma}^2} $$ $$ f'(x) = {1\over \sigma{\sqrt {2\pi}}}e^{(x-\mu)^2\over 2{\sigma}^2} \cdot {(x-\mu)\over \sigma^2} $$ $$ f''(x) = \left[({1\over \sigma{\sqrt {2\pi}}}e^{(x-\mu)^2\over 2\sigma^2} \cdot {(x-\mu)\over \sigma^2} \cdot {(x-\mu)\over \sigma^2}) + {1\over \sigma{\sqrt {2\pi}}}e^{(x-\mu)^2\over 2\sigma^2} \cdot {1\over \sigma^2}\right] = 0 $$ $$ ({1\over {\sigma}{\sqrt {2\pi}}}e^{(x-\mu)^2\over 2{\sigma}^2}) \left[ {(x-\mu)^2\over {\sigma}^4} + {1\over \sigma^2} \right] = 0 $$ $$ \left[ {(x-\mu)^2\over {\sigma}^4} + {1\over \sigma^2} \right] = 0 $$ $$ \left[ {(x-\mu)^2\over {\sigma}^4} + {\sigma^2\over \sigma^4} \right] = 0 $$ $$  {(x-\mu)^2} + {\sigma^2\ } = 0 $$ $$ x^2 - 2{\mu}x + \mu^2 + {\sigma^2\ } = 0  $$ $$ x={-b\pm\sqrt{b^2-4ac} \over 2a} $$ $$ x={2\mu\pm\sqrt{4\mu^2-4(\mu^2 + \sigma^2) } \over 2} $$ $$ x={2\mu\pm\sqrt{4\mu^2-4\mu^2 - 4 \sigma^2 } \over 2} $$ $$ x={2\mu\pm\sqrt{ - 4 \sigma^2 } \over 2} $$ $$ x={2\mu\pm\ 2\sigma i \over 2} $$ $$ x = \mu ± \sigma i $$ Is this correct? I'm doubting myself due to the fact I have an imaginary number...,Question: A Random Variable that is normally distributed with mean $\mu$ and standard deviation $\sigma$ has a probability density function of $$ f(x) = {1\over {\sigma}{\sqrt {2\pi}}} e^{(x-\mu)^2\over 2 \sigma^2} $$ Find the values of $x$ where the curve has points of inflection What I have attempted: $$ f(x) = {1\over {\sigma}{\sqrt {2\pi}}}e^{(x-\mu)^2\over 2{\sigma}^2} $$ $$ f'(x) = {1\over \sigma{\sqrt {2\pi}}}e^{(x-\mu)^2\over 2{\sigma}^2} \cdot {(x-\mu)\over \sigma^2} $$ $$ f''(x) = \left[({1\over \sigma{\sqrt {2\pi}}}e^{(x-\mu)^2\over 2\sigma^2} \cdot {(x-\mu)\over \sigma^2} \cdot {(x-\mu)\over \sigma^2}) + {1\over \sigma{\sqrt {2\pi}}}e^{(x-\mu)^2\over 2\sigma^2} \cdot {1\over \sigma^2}\right] = 0 $$ $$ ({1\over {\sigma}{\sqrt {2\pi}}}e^{(x-\mu)^2\over 2{\sigma}^2}) \left[ {(x-\mu)^2\over {\sigma}^4} + {1\over \sigma^2} \right] = 0 $$ $$ \left[ {(x-\mu)^2\over {\sigma}^4} + {1\over \sigma^2} \right] = 0 $$ $$ \left[ {(x-\mu)^2\over {\sigma}^4} + {\sigma^2\over \sigma^4} \right] = 0 $$ $$  {(x-\mu)^2} + {\sigma^2\ } = 0 $$ $$ x^2 - 2{\mu}x + \mu^2 + {\sigma^2\ } = 0  $$ $$ x={-b\pm\sqrt{b^2-4ac} \over 2a} $$ $$ x={2\mu\pm\sqrt{4\mu^2-4(\mu^2 + \sigma^2) } \over 2} $$ $$ x={2\mu\pm\sqrt{4\mu^2-4\mu^2 - 4 \sigma^2 } \over 2} $$ $$ x={2\mu\pm\sqrt{ - 4 \sigma^2 } \over 2} $$ $$ x={2\mu\pm\ 2\sigma i \over 2} $$ $$ x = \mu ± \sigma i $$ Is this correct? I'm doubting myself due to the fact I have an imaginary number...,,['derivatives']
12,Find the number of real roots of the derivative of $f(x)=(x-1)(x-2)(x-3)(x-4)(x-5)$ [duplicate],Find the number of real roots of the derivative of  [duplicate],f(x)=(x-1)(x-2)(x-3)(x-4)(x-5),"This question already has answers here : How to find root of derivative of any polynomial/equation? (4 answers) Closed 8 years ago . Find out the number of real roots of equation $f'(x) = 0$, where    $$f(x)=(x-1)(x-2)(x-3)(x-4)(x-5)$$ How can I differentiate this function without expanding it to the polynomial form. Am I underestimating some theory of equation concept associated with it? (I know the product rule approach and solving by simplifying but I want to know is there any other way to solve it)","This question already has answers here : How to find root of derivative of any polynomial/equation? (4 answers) Closed 8 years ago . Find out the number of real roots of equation $f'(x) = 0$, where    $$f(x)=(x-1)(x-2)(x-3)(x-4)(x-5)$$ How can I differentiate this function without expanding it to the polynomial form. Am I underestimating some theory of equation concept associated with it? (I know the product rule approach and solving by simplifying but I want to know is there any other way to solve it)",,"['calculus', 'derivatives', 'polynomials', 'roots']"
13,How to get nth derivative of $\arcsin x$,How to get nth derivative of,\arcsin x,"I want to calculate the nth derivative of $\arcsin x$ . I know $$ \frac{d}{dx}\arcsin x=\frac1{\sqrt{1-x^2}} $$ And $$  \frac{d^n}{dx^n} \frac1{\sqrt{1-x^2}} = \frac{d}{dx} \left(P_{n-1}(x) \frac1{\sqrt{1-x^2}}\right) = \left(-\frac{x}{(1-x^2)^{}} P_{n-1}(x) + \frac{dP_{n-1}}{dx}\right)\frac1{\sqrt{1-x^2}} = P_n(x) \frac1{\sqrt{1-x^2}} $$ Hence we have the recursive relation of $P_n$ : $$  P_{n}(x)=-\frac{x}{(1-x^2)^{}} P_{n-1}(x) + \frac{dP_{n-1}}{dx}, \:P_0(x) = 1 $$ My question is how to solve the recursive relation involving function and derivative. I think it should use the generating function, but not sure what it is.","I want to calculate the nth derivative of . I know And Hence we have the recursive relation of : My question is how to solve the recursive relation involving function and derivative. I think it should use the generating function, but not sure what it is.","\arcsin x 
\frac{d}{dx}\arcsin x=\frac1{\sqrt{1-x^2}}
  
\frac{d^n}{dx^n} \frac1{\sqrt{1-x^2}} = \frac{d}{dx} \left(P_{n-1}(x) \frac1{\sqrt{1-x^2}}\right) = \left(-\frac{x}{(1-x^2)^{}} P_{n-1}(x) + \frac{dP_{n-1}}{dx}\right)\frac1{\sqrt{1-x^2}} = P_n(x) \frac1{\sqrt{1-x^2}}
 P_n  
P_{n}(x)=-\frac{x}{(1-x^2)^{}} P_{n-1}(x) + \frac{dP_{n-1}}{dx}, \:P_0(x) = 1
","['calculus', 'derivatives', 'polynomials', 'generating-functions', 'recursion']"
14,Differential Notation Magic in Integration by u-Substitution [duplicate],Differential Notation Magic in Integration by u-Substitution [duplicate],,"This question already has answers here : Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? (27 answers) Closed 8 years ago . I'm really confused now. I always thought that the differential notation $\frac{df}{dx}$ was just that, a notation. But somehow when doing integration by u-substitution I'm told that you can turn something like this $\frac{du}{dx} = 2x\;$ into this $\;du = 2x\ dx$. But how is that even possible? I understand that the notation comes from the fact that $\frac{du}{dx}$ actually means the limit of the difference in $u$ over the difference in $x$, with $\Delta x$ approaching $0$. $$u'(x) = \frac{du}{dx} = \frac{du(x)}{dx} = \lim_{\Delta x\to 0} \frac{u(x+\Delta x)\ -\ u(x)}{(x+\Delta x) - x} = \lim_{\Delta x\to 0} \frac{u(x+\Delta x)\ -\ u(x)}{\Delta x}$$ So if $\frac{df}{dx}$ is just a notation for the limit mentioned above, then what is the underlying argument to say that you can treat $\frac{du}{dx}$ as if it were an actual fraction? Appreciate the help =)","This question already has answers here : Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? (27 answers) Closed 8 years ago . I'm really confused now. I always thought that the differential notation $\frac{df}{dx}$ was just that, a notation. But somehow when doing integration by u-substitution I'm told that you can turn something like this $\frac{du}{dx} = 2x\;$ into this $\;du = 2x\ dx$. But how is that even possible? I understand that the notation comes from the fact that $\frac{du}{dx}$ actually means the limit of the difference in $u$ over the difference in $x$, with $\Delta x$ approaching $0$. $$u'(x) = \frac{du}{dx} = \frac{du(x)}{dx} = \lim_{\Delta x\to 0} \frac{u(x+\Delta x)\ -\ u(x)}{(x+\Delta x) - x} = \lim_{\Delta x\to 0} \frac{u(x+\Delta x)\ -\ u(x)}{\Delta x}$$ So if $\frac{df}{dx}$ is just a notation for the limit mentioned above, then what is the underlying argument to say that you can treat $\frac{du}{dx}$ as if it were an actual fraction? Appreciate the help =)",,"['calculus', 'real-analysis', 'integration', 'derivatives']"
15,Derivative Notation Explanation,Derivative Notation Explanation,,"I am learning differential calculus on Khan Academy, but I am uncertain of a few things. By the way; I understand derivatives this far: $d^{\prime}(x)$ and this: $d^{\prime}(g(x))$ I am confused mainly about Leibniz's notation. What does the ""respect"" mean in ""derivative with respect to x"" and ""derivative of y with respect to x"" mean? Why does $\frac d{dx}f(x)$ have only a $d$ on top? I suspect there is a hidden variable not notated. Lastly, because this question is not as important, (but can help my understanding) what does $\frac {dx}{d f(x)}$ mean? For example, what is $\frac {dx}{d \sin(x)}\left(x^2\right)$? Other examples would be helpful. Thanks for all the help. I really don't want to wait for my senior year in high school.","I am learning differential calculus on Khan Academy, but I am uncertain of a few things. By the way; I understand derivatives this far: $d^{\prime}(x)$ and this: $d^{\prime}(g(x))$ I am confused mainly about Leibniz's notation. What does the ""respect"" mean in ""derivative with respect to x"" and ""derivative of y with respect to x"" mean? Why does $\frac d{dx}f(x)$ have only a $d$ on top? I suspect there is a hidden variable not notated. Lastly, because this question is not as important, (but can help my understanding) what does $\frac {dx}{d f(x)}$ mean? For example, what is $\frac {dx}{d \sin(x)}\left(x^2\right)$? Other examples would be helpful. Thanks for all the help. I really don't want to wait for my senior year in high school.",,"['calculus', 'derivatives', 'notation']"
16,Prove derivative is continuous - is this $\delta/\epsilon$ proof correct?,Prove derivative is continuous - is this  proof correct?,\delta/\epsilon,"I am aware of proofs of this fact, including those given at prove that $f'(a)=\lim_{x\rightarrow a}f'(x)$. . This question is not about how to prove it efficiently using MVT or any other clever method, but whether the proof I've sketched here is correct. This is not a duplicate, because I'm trying to get a review of my delta-epsilon reasoning. Suppose $f$ is differentiable on an open interval $I$, $c\in I$, and $\lim_{x\to c} f'(x) = L \neq\infty$. Prove that $f'$ is continuous at $c$. In other words, $f'$ cannot have a removable discontinuity. My intuition is this: if $x$ is close to $c$, then the quantity $\frac{f(x)-f(c)}{x-c}$ is close to $f'(x)$ and also to $f'(c)$, therefore they are close to each other. However, I'm having a hard time formalizing this. I figure that for any $\epsilon >0$, we can find a $\delta$ so that, whenever $x$ is $\delta$-close to $c$, that puts our difference quotient $\frac{\epsilon}{2}$-close to $f'(c)$. That's true because $f'(c)$ exists. Additionally, I can find another $\delta$ so that $f'(x)$ is $\frac{\epsilon}{2}$-close to $L$ when $x$ is $\delta$-close to $c$. That's true because the limit of $f'$ exists at $c$. Choosing the smaller of the two $\delta$'s, the triangle inequality gives us that $|f'(c)-L|<\epsilon$, and so they're equal? Does that work?","I am aware of proofs of this fact, including those given at prove that $f'(a)=\lim_{x\rightarrow a}f'(x)$. . This question is not about how to prove it efficiently using MVT or any other clever method, but whether the proof I've sketched here is correct. This is not a duplicate, because I'm trying to get a review of my delta-epsilon reasoning. Suppose $f$ is differentiable on an open interval $I$, $c\in I$, and $\lim_{x\to c} f'(x) = L \neq\infty$. Prove that $f'$ is continuous at $c$. In other words, $f'$ cannot have a removable discontinuity. My intuition is this: if $x$ is close to $c$, then the quantity $\frac{f(x)-f(c)}{x-c}$ is close to $f'(x)$ and also to $f'(c)$, therefore they are close to each other. However, I'm having a hard time formalizing this. I figure that for any $\epsilon >0$, we can find a $\delta$ so that, whenever $x$ is $\delta$-close to $c$, that puts our difference quotient $\frac{\epsilon}{2}$-close to $f'(c)$. That's true because $f'(c)$ exists. Additionally, I can find another $\delta$ so that $f'(x)$ is $\frac{\epsilon}{2}$-close to $L$ when $x$ is $\delta$-close to $c$. That's true because the limit of $f'$ exists at $c$. Choosing the smaller of the two $\delta$'s, the triangle inequality gives us that $|f'(c)-L|<\epsilon$, and so they're equal? Does that work?",,"['real-analysis', 'derivatives', 'proof-verification']"
17,Conditions for convergence of derivatives from pointwise convergence,Conditions for convergence of derivatives from pointwise convergence,,"Let $\{f_n\}_n$ be a sequence of functions $\mathbb R \rightarrow \mathbb R$ which converges pointwise to $f$, ie: $$f_n(x) \rightarrow f(x) \hspace{10pt}\hbox{for all $x$}.$$ What additional conditions are needed so that the derivatives at $0$ of $\{f_n\}_n$ converge to the derivatives at $0$ of $f$ ? One condition that would make sense is ""uniform convergence on all compact sets"" though I can't seem to find references","Let $\{f_n\}_n$ be a sequence of functions $\mathbb R \rightarrow \mathbb R$ which converges pointwise to $f$, ie: $$f_n(x) \rightarrow f(x) \hspace{10pt}\hbox{for all $x$}.$$ What additional conditions are needed so that the derivatives at $0$ of $\{f_n\}_n$ converge to the derivatives at $0$ of $f$ ? One condition that would make sense is ""uniform convergence on all compact sets"" though I can't seem to find references",,"['reference-request', 'derivatives', 'convergence-divergence', 'uniform-convergence']"
18,"Let $f:\mathbb{R} \to \mathbb{R}$ be $C^3$. Show the equivalence: $f^{(k)}(0) = 0$ for $k=0,1,2 \iff \lim_{x\to 0} \frac{f(x)}{x^3}$ exists",Let  be . Show the equivalence:  for  exists,"f:\mathbb{R} \to \mathbb{R} C^3 f^{(k)}(0) = 0 k=0,1,2 \iff \lim_{x\to 0} \frac{f(x)}{x^3}","Let $f:\mathbb{R} \to \mathbb{R}$ be  $C^3$. Show the equivalence: $$f^{(k)}(0) = 0 \quad k=0,1,2 \iff \lim_{x\to 0} \frac{f(x)}{x^3} \text{, exists.}$$ Trying: Since $f \in C^3$, implies $f, f', f''$ are differentiable (therefore continuous) and $f'''$ is continuous in all domain. Therefore we can approach $f$ with a order 3 Taylor polynomial, such as: $$f(a+x) = f(a) + f'(a)x + \frac{f''(a)}{2}x^2 + r(x)$$ Where $\lim_{x \to 0} \frac{r(x)}{x^3} = 0$. (can I assume that? isn't what I'm trying to prove? This is the definition of Infinitesimal Taylor Polynomial. ). $(\Rightarrow)$ Let's solve for $r(x)$ $$ r(x) = f(a+x) - f(a) - f'(a)x - \frac{f''(a)}{2}x^2 $$ Dividing by $x^3$ $$ \frac{r(x)}{x^3} = \frac{f(a+x)}{x^3} - \frac{f(a)}{x^3} - \frac{f'(a)}{x^3}x - \frac{f''(a)}{2x^3}x^2 $$ Lets take $a=0$ $$ \frac{r(x)}{x^3} = \frac{f(x)}{x^3} - \frac{f(0)}{x^3} - \frac{f'(0)}{x^3}x - \frac{f''(0)}{2x^3}x^2 $$ By hypothesis $f^{(k)}(0) = 0 \quad k=0,1,2$. Let's substitute: $$ \frac{r(x)}{x^3} = \frac{f(x)}{x^3} - \frac{0}{x^3} - \frac{0}{x^3}x - \frac{0}{2x^3}x^2 $$ Taking the limit $$ \lim_{x \to 0} \frac{r(x)}{x^3} = \lim_{x \to 0} \frac{f(x)}{x^3} - \lim_{x \to 0} \frac{0}{x^3} - \lim_{x \to 0} \frac{0}{x^3}x - \lim_{x \to 0} \frac{0}{2x^3}x^2 $$ Since $f$ is continuous $$ \lim_{x \to 0} \frac{r(x)}{x^3} = \lim_{x \to 0} \frac{f(x)}{x^3} - 0 - 0 - 0 $$ $$ \lim_{x \to 0} \frac{r(x)}{x^3} = \lim_{x \to 0} \frac{f(x)}{x^3} $$ Therefore $ r(x) = f(x) $ since the limit of $ \lim_{x \to 0} \frac{r(x)}{x^3} $ exists (is a Taylor Poly) we conclude that lim of $ \lim_{x \to 0} \frac{f(x)}{x^3} $ also exists. Is that correct? Also I don't have any idea in how to do the other direction.","Let $f:\mathbb{R} \to \mathbb{R}$ be  $C^3$. Show the equivalence: $$f^{(k)}(0) = 0 \quad k=0,1,2 \iff \lim_{x\to 0} \frac{f(x)}{x^3} \text{, exists.}$$ Trying: Since $f \in C^3$, implies $f, f', f''$ are differentiable (therefore continuous) and $f'''$ is continuous in all domain. Therefore we can approach $f$ with a order 3 Taylor polynomial, such as: $$f(a+x) = f(a) + f'(a)x + \frac{f''(a)}{2}x^2 + r(x)$$ Where $\lim_{x \to 0} \frac{r(x)}{x^3} = 0$. (can I assume that? isn't what I'm trying to prove? This is the definition of Infinitesimal Taylor Polynomial. ). $(\Rightarrow)$ Let's solve for $r(x)$ $$ r(x) = f(a+x) - f(a) - f'(a)x - \frac{f''(a)}{2}x^2 $$ Dividing by $x^3$ $$ \frac{r(x)}{x^3} = \frac{f(a+x)}{x^3} - \frac{f(a)}{x^3} - \frac{f'(a)}{x^3}x - \frac{f''(a)}{2x^3}x^2 $$ Lets take $a=0$ $$ \frac{r(x)}{x^3} = \frac{f(x)}{x^3} - \frac{f(0)}{x^3} - \frac{f'(0)}{x^3}x - \frac{f''(0)}{2x^3}x^2 $$ By hypothesis $f^{(k)}(0) = 0 \quad k=0,1,2$. Let's substitute: $$ \frac{r(x)}{x^3} = \frac{f(x)}{x^3} - \frac{0}{x^3} - \frac{0}{x^3}x - \frac{0}{2x^3}x^2 $$ Taking the limit $$ \lim_{x \to 0} \frac{r(x)}{x^3} = \lim_{x \to 0} \frac{f(x)}{x^3} - \lim_{x \to 0} \frac{0}{x^3} - \lim_{x \to 0} \frac{0}{x^3}x - \lim_{x \to 0} \frac{0}{2x^3}x^2 $$ Since $f$ is continuous $$ \lim_{x \to 0} \frac{r(x)}{x^3} = \lim_{x \to 0} \frac{f(x)}{x^3} - 0 - 0 - 0 $$ $$ \lim_{x \to 0} \frac{r(x)}{x^3} = \lim_{x \to 0} \frac{f(x)}{x^3} $$ Therefore $ r(x) = f(x) $ since the limit of $ \lim_{x \to 0} \frac{r(x)}{x^3} $ exists (is a Taylor Poly) we conclude that lim of $ \lim_{x \to 0} \frac{f(x)}{x^3} $ also exists. Is that correct? Also I don't have any idea in how to do the other direction.",,"['real-analysis', 'derivatives', 'taylor-expansion']"
19,Minimum Surface Area of a Closed Cylindrical Container,Minimum Surface Area of a Closed Cylindrical Container,,"This is a trivial question; but I just want to make sure: A closed cylindrical container has a capacity of $128\pi \,{\rm m}^3$. Determine the minimum surface area. The answer is $96\pi$. Volume of Cylinder, $V = \pi r^2 h = 128\pi$ (eq1) Surface Area of Cylinder, $SA = 2\pi(r^2 + rh)$ (eq2) Substitute eq(1) and eq(2); solve for the derivative of zero: $$\frac{\rm d}{{\rm d}r}( r^2 + 128/r) = 0,$$ solve for $r$. We get $r=4$. Put back into $V$ formula, $h = 2.54647$ Calculate Surface area: $52\pi$ Is it possible answer is wrong? I double checked with wolfram Alpha and all my derivatives are valid.","This is a trivial question; but I just want to make sure: A closed cylindrical container has a capacity of $128\pi \,{\rm m}^3$. Determine the minimum surface area. The answer is $96\pi$. Volume of Cylinder, $V = \pi r^2 h = 128\pi$ (eq1) Surface Area of Cylinder, $SA = 2\pi(r^2 + rh)$ (eq2) Substitute eq(1) and eq(2); solve for the derivative of zero: $$\frac{\rm d}{{\rm d}r}( r^2 + 128/r) = 0,$$ solve for $r$. We get $r=4$. Put back into $V$ formula, $h = 2.54647$ Calculate Surface area: $52\pi$ Is it possible answer is wrong? I double checked with wolfram Alpha and all my derivatives are valid.",,"['calculus', 'derivatives', 'volume']"
20,Prove that $(1+a)^x>1+ax$ when $x>1$ and $0<a<1$,Prove that  when  and,(1+a)^x>1+ax x>1 0<a<1,"Prove that $(1+a)^x>1+ax$ when $x>1$ and $0<a<1$ and $(1+a)^x<1+ax$ when $0<x<1$ and $0<a<1$ I was trying to do it the usual way which is to consider the function $h(x)=(1+a)^x-1-ax$. Note that when $x=1$, $h(x)=0$; taking the derivative $h´(x)=(1+a)^xln(1+a)-a$, and I´d like to prove that $h´(x)>0$ when $x>1$, and this would imply that $h(x)$ is an increasing function which would also imply that $h(x)>0$ But that´s my problem how can I guarantee/prove that $h´(x)>0$ when $x>1$ ? I would really appreciate your help","Prove that $(1+a)^x>1+ax$ when $x>1$ and $0<a<1$ and $(1+a)^x<1+ax$ when $0<x<1$ and $0<a<1$ I was trying to do it the usual way which is to consider the function $h(x)=(1+a)^x-1-ax$. Note that when $x=1$, $h(x)=0$; taking the derivative $h´(x)=(1+a)^xln(1+a)-a$, and I´d like to prove that $h´(x)>0$ when $x>1$, and this would imply that $h(x)$ is an increasing function which would also imply that $h(x)>0$ But that´s my problem how can I guarantee/prove that $h´(x)>0$ when $x>1$ ? I would really appreciate your help",,"['calculus', 'inequality', 'derivatives']"
21,"Prove that $\frac{f(x)}{x^n}=\frac{f^{(n)}(\theta x)}{n!},0<\theta <1$ if $f^{'}(0)=...=f^{(n-1)}(0)=0$ using Cauchy's mean value theorem",Prove that  if  using Cauchy's mean value theorem,"\frac{f(x)}{x^n}=\frac{f^{(n)}(\theta x)}{n!},0<\theta <1 f^{'}(0)=...=f^{(n-1)}(0)=0","I don't know how to apply theorem on the problem. By this theorem, if two functions $f$ and $g$ are defined on $[a,b]$ continuous on $[a,b]$, differentiable on $(a,b)$ and $g^{'}(x)\neq 0$ for every $x\in (a,b)$, then there exists point $c\in (a,b)$ so that $$\frac{f(b)-f(a)}{g(b)-g(a)}=\frac{f{'}(c)}{g^{'}(c)}$$ Can we define $f(x)=x^n$ and $g(x)=n!$? Can we define $\theta$ as a point $c$?","I don't know how to apply theorem on the problem. By this theorem, if two functions $f$ and $g$ are defined on $[a,b]$ continuous on $[a,b]$, differentiable on $(a,b)$ and $g^{'}(x)\neq 0$ for every $x\in (a,b)$, then there exists point $c\in (a,b)$ so that $$\frac{f(b)-f(a)}{g(b)-g(a)}=\frac{f{'}(c)}{g^{'}(c)}$$ Can we define $f(x)=x^n$ and $g(x)=n!$? Can we define $\theta$ as a point $c$?",,"['calculus', 'derivatives']"
22,Feynman Integration Problem,Feynman Integration Problem,,"$$ I = \frac{\pi^2}{8} - \int_0^1 \frac{\arctan(x)}{\sqrt{1-x^2}} \,dx $$   Evaluate $I$ $$ I = \frac{\pi^2}{8} - \int_0^1 \frac{\arctan(x)}{\sqrt{1-x^2}} \,dx$$ $$f(a) = \int_0^1 \frac{\arctan(ax)}{\sqrt{1-x^2}} \,dx$$ After differentiating and integrating, $$ f'(a) = \frac{\ln(a+\sqrt{1+a^2})}{a\sqrt{1+a^2}} $$ The final answer that we get is: $$ f(1) = \frac{\pi^2}{8} - \frac{\ln^2(1+\sqrt{2})}{2} $$ Could somebody please show me how this was done? $$$$ Also, how do we know how and where to introduce another variable (say $n$) to perform Feynman Integration? For example, in this case, how do we know that we should rewrite $$f(x) = \int_0^1 \frac{\arctan(x)}{\sqrt{1-x^2}} \,dx$$ as $$f(x,a) = \int_0^1 \frac{\arctan(ax)}{\sqrt{1-x^2}} \,dx$$ $$$$Please do help me!","$$ I = \frac{\pi^2}{8} - \int_0^1 \frac{\arctan(x)}{\sqrt{1-x^2}} \,dx $$   Evaluate $I$ $$ I = \frac{\pi^2}{8} - \int_0^1 \frac{\arctan(x)}{\sqrt{1-x^2}} \,dx$$ $$f(a) = \int_0^1 \frac{\arctan(ax)}{\sqrt{1-x^2}} \,dx$$ After differentiating and integrating, $$ f'(a) = \frac{\ln(a+\sqrt{1+a^2})}{a\sqrt{1+a^2}} $$ The final answer that we get is: $$ f(1) = \frac{\pi^2}{8} - \frac{\ln^2(1+\sqrt{2})}{2} $$ Could somebody please show me how this was done? $$$$ Also, how do we know how and where to introduce another variable (say $n$) to perform Feynman Integration? For example, in this case, how do we know that we should rewrite $$f(x) = \int_0^1 \frac{\arctan(x)}{\sqrt{1-x^2}} \,dx$$ as $$f(x,a) = \int_0^1 \frac{\arctan(ax)}{\sqrt{1-x^2}} \,dx$$ $$$$Please do help me!",,"['calculus', 'integration', 'derivatives', 'definite-integrals']"
23,differentiation of a matrix function,differentiation of a matrix function,,"In statistics, the residual sum of squares is given by the formula $$ \operatorname{RSS}(\beta) = (\mathbf{y} - \mathbf{X}\beta)^T(\mathbf{y} - \mathbf{X}\beta)$$ I know differentiation of scalar functions, but how to I perform derivatives on this wrt $\beta$? By the way, I am trying to take the minimum of RSS wrt to $\beta$, so I am setting the derivative equal to 0. I know somehow product rule has to hold. So here I have the first step $$-\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta) + (\mathbf{y}-\mathbf{X}\beta)^T(-\mathbf{X})= 0$$","In statistics, the residual sum of squares is given by the formula $$ \operatorname{RSS}(\beta) = (\mathbf{y} - \mathbf{X}\beta)^T(\mathbf{y} - \mathbf{X}\beta)$$ I know differentiation of scalar functions, but how to I perform derivatives on this wrt $\beta$? By the way, I am trying to take the minimum of RSS wrt to $\beta$, so I am setting the derivative equal to 0. I know somehow product rule has to hold. So here I have the first step $$-\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta) + (\mathbf{y}-\mathbf{X}\beta)^T(-\mathbf{X})= 0$$",,"['derivatives', 'matrix-calculus']"
24,Understanding higher dimensional derivatives,Understanding higher dimensional derivatives,,"I'm having trouble understanding higher dimensional derivatives. Suppose $f: \Bbb R \to \Bbb R$.  We say $f$ is differentiable at $x = c$ if $\lim \limits_{x \to c} \dfrac{f(x) - f(c)}{x - c}$ exists.  If the limit exists, we define $f'(c)$ as the value limit.  Since this limit is a number, we can equivalently define the derivative of $f(x)$ at $c$ as the number $q$ such that $\lim \limits_{x \to c} \dfrac{f(x) - f(c) - q(x - c)}{x - c} = 0$.  If such a $q$ exists, we define $f'(c)$ to be this $q$.  We can think of $q$ as a ""linear transformation"" from $\Bbb R \to \Bbb R$. Why is it useful to think of $q$ in this way?  What's the point of thinking about it as a linear transformation? I think it probably has something to do with the fact that $f(c) + q(x - c)$ is the tangent line . Now, we can define the derivative of $g: \Bbb R^{n} \to \Bbb R^{m}$ in the same way, that is, if there is some linear transformation $T_{g}$ such that $\lim \limits_{x \to c} \dfrac{||g(x) - g(c) - T_{g}(x - c) ||}{||x - c||} = 0$, then we say $T_{g}$ is the derivative of $g$. Again, what's the point of saying this is a linear transformation? I already know it has to be an $m \times n$ matrix based on the context, but I don't see why we care that it is a linear transformation.  Does it have something to do with the tangent plane?","I'm having trouble understanding higher dimensional derivatives. Suppose $f: \Bbb R \to \Bbb R$.  We say $f$ is differentiable at $x = c$ if $\lim \limits_{x \to c} \dfrac{f(x) - f(c)}{x - c}$ exists.  If the limit exists, we define $f'(c)$ as the value limit.  Since this limit is a number, we can equivalently define the derivative of $f(x)$ at $c$ as the number $q$ such that $\lim \limits_{x \to c} \dfrac{f(x) - f(c) - q(x - c)}{x - c} = 0$.  If such a $q$ exists, we define $f'(c)$ to be this $q$.  We can think of $q$ as a ""linear transformation"" from $\Bbb R \to \Bbb R$. Why is it useful to think of $q$ in this way?  What's the point of thinking about it as a linear transformation? I think it probably has something to do with the fact that $f(c) + q(x - c)$ is the tangent line . Now, we can define the derivative of $g: \Bbb R^{n} \to \Bbb R^{m}$ in the same way, that is, if there is some linear transformation $T_{g}$ such that $\lim \limits_{x \to c} \dfrac{||g(x) - g(c) - T_{g}(x - c) ||}{||x - c||} = 0$, then we say $T_{g}$ is the derivative of $g$. Again, what's the point of saying this is a linear transformation? I already know it has to be an $m \times n$ matrix based on the context, but I don't see why we care that it is a linear transformation.  Does it have something to do with the tangent plane?",,"['real-analysis', 'derivatives']"
25,"Differentiate $\,y = 9x^2 \sin x \tan x:$ Did I Solve This Correctly?",Differentiate  Did I Solve This Correctly?,"\,y = 9x^2 \sin x \tan x:","I'm posting my initial work up to this point.  Criticism welcomed! Using the formula $(fgh)' = f'gh+fg'h + fgh'$, differentiate$$y = 9x^2\sin x \tan x$$ $$\begin{align} y' &= 9\frac d{dx}(x^2)\sin x \tan x + 9x^2 \frac d{dx} (\sin x) \tan x + 9x^2\sin x \frac d{dx}(\tan x)\\ \\ & = 9(2x\sin x \tan x + 9x^2-\cos x \tan x + 9x^2\sin x \sec^2x\\ \\ &=9\Big(2x\sin x \tan x + x^2 -\cos x \tan x + x^2\sin x \sec^2 x\Big) \end{align}$$ Have I done it correctly up and until this point?","I'm posting my initial work up to this point.  Criticism welcomed! Using the formula $(fgh)' = f'gh+fg'h + fgh'$, differentiate$$y = 9x^2\sin x \tan x$$ $$\begin{align} y' &= 9\frac d{dx}(x^2)\sin x \tan x + 9x^2 \frac d{dx} (\sin x) \tan x + 9x^2\sin x \frac d{dx}(\tan x)\\ \\ & = 9(2x\sin x \tan x + 9x^2-\cos x \tan x + 9x^2\sin x \sec^2x\\ \\ &=9\Big(2x\sin x \tan x + x^2 -\cos x \tan x + x^2\sin x \sec^2 x\Big) \end{align}$$ Have I done it correctly up and until this point?",,"['calculus', 'derivatives']"
26,Show $f$ is not $1-1$,Show  is not,f 1-1,"Let $f:\Bbb R^2\to \Bbb R$ be a continuously differentiable function. Show that $f$ is not $1-1$. I know I will need to use the Inverse Function Theorem and consider some open set A with $g:A\to \Bbb R^2$ defined by $g(x,y)=\big(f(x,y),y\big)$.","Let $f:\Bbb R^2\to \Bbb R$ be a continuously differentiable function. Show that $f$ is not $1-1$. I know I will need to use the Inverse Function Theorem and consider some open set A with $g:A\to \Bbb R^2$ defined by $g(x,y)=\big(f(x,y),y\big)$.",,"['real-analysis', 'derivatives', 'continuity']"
27,Calculate the derivative and find its domain: $\;f(x)= \sqrt{\ln(x)+2}$,Calculate the derivative and find its domain:,\;f(x)= \sqrt{\ln(x)+2},I calculated the derivative as $$f'(x) = \frac{1}{2x \sqrt{2+\ln x}}$$ How do I find out the domain?,I calculated the derivative as $$f'(x) = \frac{1}{2x \sqrt{2+\ln x}}$$ How do I find out the domain?,,['derivatives']
28,Definition of reciprocal derivative,Definition of reciprocal derivative,,"Suppose I define $y(x)=x^3$. Then, $$\frac{\mathrm dy(x)}{\mathrm dx}=3x^2.$$ however I don't see how $\displaystyle \frac{\mathrm dx(y)}{\mathrm dy}$=$\frac{1}{3x^2}$ because I never explicitly define $x(y)$ which has implications for coupling across equations of variables. $x(y)$ and $y(x)$ leads to a possible recursive definition $x(y(x))$ ... $y(x(y))$  Could someone help me resolve this?  Thank you!","Suppose I define $y(x)=x^3$. Then, $$\frac{\mathrm dy(x)}{\mathrm dx}=3x^2.$$ however I don't see how $\displaystyle \frac{\mathrm dx(y)}{\mathrm dy}$=$\frac{1}{3x^2}$ because I never explicitly define $x(y)$ which has implications for coupling across equations of variables. $x(y)$ and $y(x)$ leads to a possible recursive definition $x(y(x))$ ... $y(x(y))$  Could someone help me resolve this?  Thank you!",,"['calculus', 'derivatives']"
29,Quaternion derivative w.r.t. its angle,Quaternion derivative w.r.t. its angle,,"The following quaternion represents a rotation by $\theta$ around the z-axis: \begin{align} q &= (\cos(\frac{1}{2}\theta), \vec{u}\cdot\sin(\frac{1}{2}\theta)),  \\ \vec{u}&=(0,0,1)^t \end{align} I'd like to take the derivative of this quaternion with respect to its angle $\theta$. In this paper I've read that first the exponential map representation of the quaternion has to be built: that would be \begin{align} q &= e^\vec{w} , \\\vec{w}&=(w_1,w_2,w_3)^t=(0,0,\frac{1}{2}\theta)^t \end{align}  Then I have to derive q with respect to $w_1,w_2$ and $w_3$ as follows: \begin{align} \frac{\partial q}{\partial \vec{w}}&=(\frac{\partial q}{\partial w_1},\frac{\partial q}{\partial w_2},\frac{\partial q}{\partial w_3}) \end{align} In my example, for every partial derivation I get a quaternion, which is respectively: which are 3 quaternions. Could this be wrong? I'm wondering because $w_1=w_2=0$. Does anybody have an idea how to deal with this? Later I'm aiming to compute a jacobian matrix (I've already computed it) with these derivatives, and as $\theta$ is in the denominator,  in the jacobian matrix there has to be $\theta\neq0$, which is senseless because $\theta$ has to include the posibility to be $0$. To provide more information, I've added the algorithm I'm working with: Could anyone help me with this? Thanks!","The following quaternion represents a rotation by $\theta$ around the z-axis: \begin{align} q &= (\cos(\frac{1}{2}\theta), \vec{u}\cdot\sin(\frac{1}{2}\theta)),  \\ \vec{u}&=(0,0,1)^t \end{align} I'd like to take the derivative of this quaternion with respect to its angle $\theta$. In this paper I've read that first the exponential map representation of the quaternion has to be built: that would be \begin{align} q &= e^\vec{w} , \\\vec{w}&=(w_1,w_2,w_3)^t=(0,0,\frac{1}{2}\theta)^t \end{align}  Then I have to derive q with respect to $w_1,w_2$ and $w_3$ as follows: \begin{align} \frac{\partial q}{\partial \vec{w}}&=(\frac{\partial q}{\partial w_1},\frac{\partial q}{\partial w_2},\frac{\partial q}{\partial w_3}) \end{align} In my example, for every partial derivation I get a quaternion, which is respectively: which are 3 quaternions. Could this be wrong? I'm wondering because $w_1=w_2=0$. Does anybody have an idea how to deal with this? Later I'm aiming to compute a jacobian matrix (I've already computed it) with these derivatives, and as $\theta$ is in the denominator,  in the jacobian matrix there has to be $\theta\neq0$, which is senseless because $\theta$ has to include the posibility to be $0$. To provide more information, I've added the algorithm I'm working with: Could anyone help me with this? Thanks!",,"['derivatives', 'quaternions']"
30,Second derivative of binomial distribution,Second derivative of binomial distribution,,"I try to prove that according to binomial distribution $P(X=k)={n \choose k}p^k(1-p)^{n-k}$ the maximum probability $P(X=k)$ is achieved at maximum likelihood, i.e. $p=\frac{k}{n}$. Let's apply $\log$ and take the first derivative. $$\log P(X=k) = \log {n \choose k}+k \log p+ (n-k)\log (1-p)$$ $$\frac{d \log P(X=k)}{dp} = \frac{k}{p}-\frac{n-k}{1-p}=0 \quad \iff \quad  p=\frac{k}{n}$$ The problem is to show that what I found is indeed the global maximum, i.e. I need to show that the second derivative is negative everywhere. I would appreciate if someone could help me with the second derivative. The second derivative $$\frac{d(\frac{k}{p}-\frac{n-k}{1-p}) }{dp} = -\frac{k}{p^2}-\frac{n-k}{(1-p)^2}$$ it's negative because $n>k$","I try to prove that according to binomial distribution $P(X=k)={n \choose k}p^k(1-p)^{n-k}$ the maximum probability $P(X=k)$ is achieved at maximum likelihood, i.e. $p=\frac{k}{n}$. Let's apply $\log$ and take the first derivative. $$\log P(X=k) = \log {n \choose k}+k \log p+ (n-k)\log (1-p)$$ $$\frac{d \log P(X=k)}{dp} = \frac{k}{p}-\frac{n-k}{1-p}=0 \quad \iff \quad  p=\frac{k}{n}$$ The problem is to show that what I found is indeed the global maximum, i.e. I need to show that the second derivative is negative everywhere. I would appreciate if someone could help me with the second derivative. The second derivative $$\frac{d(\frac{k}{p}-\frac{n-k}{1-p}) }{dp} = -\frac{k}{p^2}-\frac{n-k}{(1-p)^2}$$ it's negative because $n>k$",,"['probability', 'probability-distributions', 'derivatives']"
31,Why is the power rule for derivatives not valid here?,Why is the power rule for derivatives not valid here?,,"I am stuck on an exercise where I have to figure out the derivative of $y = \frac{\sqrt{20-x^2}}{4}$ . I realize that this equation can be rewritten as: $\frac{1}{4} \times \sqrt{20-x^2}$ , so when I factor out the constant $\frac{1}{4}$ on beforehand, this leaves me with $\sqrt{20-x^2}$ . This can be rewritten as a power $(20-x^2)^{0.5}$ . My idea was to simply apply the power rule to this equation to find the derivative. This leads to: $0.5(20-x^2)^{-0.5} = \frac{1}{2} \times \frac{1}{\sqrt{20-x^2}} = \frac{1}{2\sqrt{20-x^2}}$ However my solution is not valid, and wolfram alpha does something complicated using the chain rule instead. Please help me understand why.","I am stuck on an exercise where I have to figure out the derivative of . I realize that this equation can be rewritten as: , so when I factor out the constant on beforehand, this leaves me with . This can be rewritten as a power . My idea was to simply apply the power rule to this equation to find the derivative. This leads to: However my solution is not valid, and wolfram alpha does something complicated using the chain rule instead. Please help me understand why.",y = \frac{\sqrt{20-x^2}}{4} \frac{1}{4} \times \sqrt{20-x^2} \frac{1}{4} \sqrt{20-x^2} (20-x^2)^{0.5} 0.5(20-x^2)^{-0.5} = \frac{1}{2} \times \frac{1}{\sqrt{20-x^2}} = \frac{1}{2\sqrt{20-x^2}},"['calculus', 'derivatives']"
32,What exactly does the $d$ represent in $\frac{d}{dx}$?,What exactly does the  represent in ?,d \frac{d}{dx},"When taking the derivative, such as $\frac{d}{dx}$, what exactly does the $d$ represent? The best answer so far is in for example $\frac{dy}{dx}$, the $d$ stands for change in and what follows the equality sign describes how $y$ changes in relation to $x$. But say the derivative is $3x^2 + y$, how exactly does that represent the relation between the two variables? I find it hard to wrap my head around.","When taking the derivative, such as $\frac{d}{dx}$, what exactly does the $d$ represent? The best answer so far is in for example $\frac{dy}{dx}$, the $d$ stands for change in and what follows the equality sign describes how $y$ changes in relation to $x$. But say the derivative is $3x^2 + y$, how exactly does that represent the relation between the two variables? I find it hard to wrap my head around.",,"['calculus', 'derivatives', 'notation']"
33,Wolfram double solution to $\int{x \cdot \sin^2(x) dx}$,Wolfram double solution to,\int{x \cdot \sin^2(x) dx},"I calculated this integral : $$\int{x \cdot \sin^2(x) dx}$$ By parts, knowing that $\int{\sin^2(x) dx} = \frac{1}{2} \cdot x - \frac{1}{4} \cdot \sin(2x) +c$. So I can consider $\sin^2(x)$ a derivative of $\frac{1}{2} \cdot x - \frac{1}{4} \cdot \sin(2x)$, and I get this result: $$\frac{1}{4} \cdot x^2 - \frac{x}{4} \cdot \sin(2x) + \frac{1}{4} \cdot \sin^2(x) +c$$ I get the confirm on wolfram if I try to compute the derivative of $\frac{1}{4} \cdot x^2 - \frac{x}{4} \cdot \sin(2x) + \frac{1}{4} \cdot \sin^2(x) +c$, but here if I try to compute the integral of $\int{x \cdot \sin^2(x) dx}$ I get this result: $$\frac{x^2}{4} -\frac{1}{4} \cdot x \cdot \sin(2x) -\frac{1}{8} \cdot \cos(2x) $$ But $\frac{1}{8} \cdot \cos(2x)$ isn't equal to $\frac{1}{4} \cdot \sin^2(x)$, which is the correct result?","I calculated this integral : $$\int{x \cdot \sin^2(x) dx}$$ By parts, knowing that $\int{\sin^2(x) dx} = \frac{1}{2} \cdot x - \frac{1}{4} \cdot \sin(2x) +c$. So I can consider $\sin^2(x)$ a derivative of $\frac{1}{2} \cdot x - \frac{1}{4} \cdot \sin(2x)$, and I get this result: $$\frac{1}{4} \cdot x^2 - \frac{x}{4} \cdot \sin(2x) + \frac{1}{4} \cdot \sin^2(x) +c$$ I get the confirm on wolfram if I try to compute the derivative of $\frac{1}{4} \cdot x^2 - \frac{x}{4} \cdot \sin(2x) + \frac{1}{4} \cdot \sin^2(x) +c$, but here if I try to compute the integral of $\int{x \cdot \sin^2(x) dx}$ I get this result: $$\frac{x^2}{4} -\frac{1}{4} \cdot x \cdot \sin(2x) -\frac{1}{8} \cdot \cos(2x) $$ But $\frac{1}{8} \cdot \cos(2x)$ isn't equal to $\frac{1}{4} \cdot \sin^2(x)$, which is the correct result?",,"['integration', 'derivatives', 'wolfram-alpha']"
34,Finding the derivative of a relational problem,Finding the derivative of a relational problem,,"I am self studying some calculus and I have gotten really stuck! I thought I had the right idea but I keep getting the answer totally wrong. I am sure I am missing something important. Here is the problem: For the equation $6x^{\frac{1}{2}}+12y^{-\frac{1}{2}} = 3xy$, find an equation of the tangent line at the point $(1, 4)$. Here is my work: $$6x^{\frac{1}{2}}+12y^{-\frac{1}{2}} = 3xy$$ $$6(x^{\frac{1}{2}}+2y^{-\frac{1}{2}}) = 3xy$$ $$2(x^{\frac{1}{2}}+2y^{-\frac{1}{2}}) = xy$$ $$2x^{\frac{1}{2}}+4y^{-\frac{1}{2}} = xy$$ $$\ln(2x^{\frac{1}{2}})+\ln(4y^{-\frac{1}{2}}) = \ln(xy)$$ $$\ln2 + \ln x^{\frac{1}{2}}+\ln4 + \ln y^{-\frac{1}{2}} = \ln x + \ln y$$ $$\ln2 + \frac{1}{2}\ln x+\ln4 -\frac{1}{2} \ln y = \ln x + \ln y$$ $$\ln2 + \ln4 + \frac{1}{2}\ln x - \ln x =  \ln y + \frac{1}{2} \ln y$$ $$\frac{2}{3}\ln2 + \frac{2}{3}\ln4 - \frac{1}{3}\ln x = \ln y$$ $$e^{\frac{2}{3}\ln2 + \frac{2}{3}\ln4 - \frac{1}{3}\ln x} = y$$ $$\frac{2}{3}\ln2 + \frac{2}{3}\ln4 - \frac{1}{3}\ln x = \ln y$$ $$\frac{1}{3x} = y'\frac{1}{y}$$ $$y' = \frac{y}{3x}$$ $$y' = \frac{e^{\frac{2}{3}\ln2 + \frac{2}{3}\ln4 - \frac{1}{3}\ln x}}{3x}$$ $$y - 4 = \frac{4}{3}(x-1)$$ $$y = \frac{4}{3}x-\frac{4}{3} + 4$$ $$y = \frac{4}{3}x+\frac{8}{3}$$ The Solution I found was: $y = \frac{4}{3}x+\frac{8}{3}$ but this is wrong! Can you tell me where I have gone wrong?","I am self studying some calculus and I have gotten really stuck! I thought I had the right idea but I keep getting the answer totally wrong. I am sure I am missing something important. Here is the problem: For the equation $6x^{\frac{1}{2}}+12y^{-\frac{1}{2}} = 3xy$, find an equation of the tangent line at the point $(1, 4)$. Here is my work: $$6x^{\frac{1}{2}}+12y^{-\frac{1}{2}} = 3xy$$ $$6(x^{\frac{1}{2}}+2y^{-\frac{1}{2}}) = 3xy$$ $$2(x^{\frac{1}{2}}+2y^{-\frac{1}{2}}) = xy$$ $$2x^{\frac{1}{2}}+4y^{-\frac{1}{2}} = xy$$ $$\ln(2x^{\frac{1}{2}})+\ln(4y^{-\frac{1}{2}}) = \ln(xy)$$ $$\ln2 + \ln x^{\frac{1}{2}}+\ln4 + \ln y^{-\frac{1}{2}} = \ln x + \ln y$$ $$\ln2 + \frac{1}{2}\ln x+\ln4 -\frac{1}{2} \ln y = \ln x + \ln y$$ $$\ln2 + \ln4 + \frac{1}{2}\ln x - \ln x =  \ln y + \frac{1}{2} \ln y$$ $$\frac{2}{3}\ln2 + \frac{2}{3}\ln4 - \frac{1}{3}\ln x = \ln y$$ $$e^{\frac{2}{3}\ln2 + \frac{2}{3}\ln4 - \frac{1}{3}\ln x} = y$$ $$\frac{2}{3}\ln2 + \frac{2}{3}\ln4 - \frac{1}{3}\ln x = \ln y$$ $$\frac{1}{3x} = y'\frac{1}{y}$$ $$y' = \frac{y}{3x}$$ $$y' = \frac{e^{\frac{2}{3}\ln2 + \frac{2}{3}\ln4 - \frac{1}{3}\ln x}}{3x}$$ $$y - 4 = \frac{4}{3}(x-1)$$ $$y = \frac{4}{3}x-\frac{4}{3} + 4$$ $$y = \frac{4}{3}x+\frac{8}{3}$$ The Solution I found was: $y = \frac{4}{3}x+\frac{8}{3}$ but this is wrong! Can you tell me where I have gone wrong?",,"['calculus', 'derivatives']"
35,Notation of differential equations,Notation of differential equations,,"I have just started a course on differential equations, and unfortunetely enough for me, we immediately used notation foreign for me, for example: $$ x^2 \left(\dfrac{d^2y}{dx^2}\right)^2 = \sin( x)\;\textrm{ and}\;\; y \times \dfrac{d^2 y}{dx^2} = \sin(x)$$ were used as examples of non-linear ordinary differential equations. My questions are Is $\large \frac{dy}{dx}$ equal to $y'$? Also, what then is $\large\frac{d^2 y}{dx^2}\;$? I have looked up the formal mathematical definition but it somewhat confuses me, Why on earth is this notation used, instead of just using $\;'\;$ or $\;''\;$? It seems to me much more confusing and unnecessarily messy.","I have just started a course on differential equations, and unfortunetely enough for me, we immediately used notation foreign for me, for example: $$ x^2 \left(\dfrac{d^2y}{dx^2}\right)^2 = \sin( x)\;\textrm{ and}\;\; y \times \dfrac{d^2 y}{dx^2} = \sin(x)$$ were used as examples of non-linear ordinary differential equations. My questions are Is $\large \frac{dy}{dx}$ equal to $y'$? Also, what then is $\large\frac{d^2 y}{dx^2}\;$? I have looked up the formal mathematical definition but it somewhat confuses me, Why on earth is this notation used, instead of just using $\;'\;$ or $\;''\;$? It seems to me much more confusing and unnecessarily messy.",,"['derivatives', 'notation']"
36,A Lipschitz function with no directional derivatives at a point,A Lipschitz function with no directional derivatives at a point,,"I am stuck in constructing a function that is locally Lipschitz continuous at $x_0$ but it does not have directional differentiation at $x_0 $ in any direction. The definition of directional derivative is $$f'(x_0,h)=\lim_{t\downarrow 0}\frac{f(x_0+th)-f(x_0)}{t}$$","I am stuck in constructing a function that is locally Lipschitz continuous at $x_0$ but it does not have directional differentiation at $x_0 $ in any direction. The definition of directional derivative is $$f'(x_0,h)=\lim_{t\downarrow 0}\frac{f(x_0+th)-f(x_0)}{t}$$",,"['real-analysis', 'derivatives', 'examples-counterexamples', 'lipschitz-functions']"
37,Equivalent definition of Tangent Spaces,Equivalent definition of Tangent Spaces,,There are about 4 definitions of tangent spaces 1) using velocities of curve 2) via derivations 3)via cotangent spaces 4) as directional derivatives. I am not getting the intuition about what tangent space is and why are they equivalent.Most importantly I am having problems connecting the viwepoints.Please help me. Here is link to tangent space definition,There are about 4 definitions of tangent spaces 1) using velocities of curve 2) via derivations 3)via cotangent spaces 4) as directional derivatives. I am not getting the intuition about what tangent space is and why are they equivalent.Most importantly I am having problems connecting the viwepoints.Please help me. Here is link to tangent space definition,,"['calculus', 'real-analysis', 'differential-geometry', 'derivatives', 'differential-topology']"
38,"Show that if $f(x)=\int_{0}^x f(t)dt$, then $f=0$","Show that if , then",f(x)=\int_{0}^x f(t)dt f=0,"I am having a problem with a question. Can somebody help me please. Show that if $f(x)=\int_{0}^x f(t)dt$, then $f=0$ Thank you in advance","I am having a problem with a question. Can somebody help me please. Show that if $f(x)=\int_{0}^x f(t)dt$, then $f=0$ Thank you in advance",,"['calculus', 'integration', 'derivatives']"
39,Integrating $\int \frac{2x \ln x}{\sqrt{x^2-9}}\mathrm dx$ by parts.,Integrating  by parts.,\int \frac{2x \ln x}{\sqrt{x^2-9}}\mathrm dx,"We have to integrate $$\int\frac{2x \ln x}{\sqrt{(x^2-9)}} \mathrm dx$$ Is it right to use Integration by Parts ? I tried to substitute it with $$u = \log x,\: \mathrm du = \frac 1x \mathrm dx;$$ $$v = x^2 - 9,\: \mathrm dv = 2x \mathrm dx.$$ But then I'm stuck with substituting it within the original equation because from $\mathrm du = \dfrac 1x \mathrm dx,$ and $\mathrm dv = 2x \mathrm dx,$ there will be two $\mathrm dx$'s to substitute and from $\mathrm du = \dfrac 1x \mathrm dx,$ the $x$ will go to the denominator and I don't know what to do any more.","We have to integrate $$\int\frac{2x \ln x}{\sqrt{(x^2-9)}} \mathrm dx$$ Is it right to use Integration by Parts ? I tried to substitute it with $$u = \log x,\: \mathrm du = \frac 1x \mathrm dx;$$ $$v = x^2 - 9,\: \mathrm dv = 2x \mathrm dx.$$ But then I'm stuck with substituting it within the original equation because from $\mathrm du = \dfrac 1x \mathrm dx,$ and $\mathrm dv = 2x \mathrm dx,$ there will be two $\mathrm dx$'s to substitute and from $\mathrm du = \dfrac 1x \mathrm dx,$ the $x$ will go to the denominator and I don't know what to do any more.",,"['integration', 'derivatives', 'indefinite-integrals']"
40,Proving that $f(x)=c \cdot e^x$ is the only function such that $f'(x)=f(x)$ [duplicate],Proving that  is the only function such that  [duplicate],f(x)=c \cdot e^x f'(x)=f(x),"This question already has answers here : Closed 12 years ago . Possible Duplicate: Proof that $\exp(x)$ is the only function for which $f(x) = f'(x)$ Here's a question I got for homework: Let f a differentiable function such that $f(x)=f'(x)$ for all $x$.   Prove that there exist a $c \in \mathbb{R}$ such that $f(x) = c \cdot  e^x$ Hint: notice $\dfrac{f(x)}{e^x}$ So, as it turns out this hint was not enough. Any more hints? Thanks!","This question already has answers here : Closed 12 years ago . Possible Duplicate: Proof that $\exp(x)$ is the only function for which $f(x) = f'(x)$ Here's a question I got for homework: Let f a differentiable function such that $f(x)=f'(x)$ for all $x$.   Prove that there exist a $c \in \mathbb{R}$ such that $f(x) = c \cdot  e^x$ Hint: notice $\dfrac{f(x)}{e^x}$ So, as it turns out this hint was not enough. Any more hints? Thanks!",,"['calculus', 'derivatives']"
41,Matrix function derivative. Introduction,Matrix function derivative. Introduction,,"The author of this question was close to determining the derivative of the function of dual variable, when we consider matrices isomorphic (algebraically and topologically) to dual numbers: $$(a+\epsilon b) \sim \begin{bmatrix}     a       & 0  \\     b       & a  \\ \end{bmatrix}.$$ So, using the fact we can define the derivative (in the Fréchet sense) for functions $F$ for with an argument in the form of such a matrix and a value in the form of such a matrix: $$F\big(\begin{bmatrix}     x+s       & 0  \\     y+t       & x+s  \\ \end{bmatrix}\big)-F\big(\begin{bmatrix}     x       & 0  \\     y       & x  \\ \end{bmatrix}\big)=\begin{bmatrix}     u'       & 0  \\     v'      & u'  \\ \end{bmatrix}\begin{bmatrix}     s       & 0  \\     t       & s  \\ \end{bmatrix}+o\bigg(\bigg|\bigg|\begin{bmatrix}     s       & 0  \\     t       & s  \\ \end{bmatrix}\bigg|\bigg|\bigg),$$ where $\bigg|\bigg|\begin{bmatrix}     s       & 0  \\     t       & s  \\ \end{bmatrix}\bigg|\bigg|=\max\{|s|,|t|\}$ and all elements of all matrices are real. Therefore, the existence of such a matrix $\begin{bmatrix}     u'       & 0  \\     v'      & u'  \\ \end{bmatrix}$ (which we will call derivative at $\begin{bmatrix}     x       & 0  \\     y       & x  \\ \end{bmatrix}$ ) means differentiability of $F$ at $\begin{bmatrix}     x       & 0  \\     y       & x  \\ \end{bmatrix}$ . I'm interested in to what extent can this approach be generalized in defining a matrix-valued function of a matrix argument? I mean the case, when the derivative is an object of the same nature as variables (in opposed to the definition of the derivative of a function $f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ which is a (Jacobian) matrix). Can anyone share links to material with respect to such kind of derivatives?","The author of this question was close to determining the derivative of the function of dual variable, when we consider matrices isomorphic (algebraically and topologically) to dual numbers: So, using the fact we can define the derivative (in the Fréchet sense) for functions for with an argument in the form of such a matrix and a value in the form of such a matrix: where and all elements of all matrices are real. Therefore, the existence of such a matrix (which we will call derivative at ) means differentiability of at . I'm interested in to what extent can this approach be generalized in defining a matrix-valued function of a matrix argument? I mean the case, when the derivative is an object of the same nature as variables (in opposed to the definition of the derivative of a function which is a (Jacobian) matrix). Can anyone share links to material with respect to such kind of derivatives?","(a+\epsilon b) \sim \begin{bmatrix}
    a       & 0  \\
    b       & a  \\
\end{bmatrix}. F F\big(\begin{bmatrix}
    x+s       & 0  \\
    y+t       & x+s  \\
\end{bmatrix}\big)-F\big(\begin{bmatrix}
    x       & 0  \\
    y       & x  \\
\end{bmatrix}\big)=\begin{bmatrix}
    u'       & 0  \\
    v'      & u'  \\
\end{bmatrix}\begin{bmatrix}
    s       & 0  \\
    t       & s  \\
\end{bmatrix}+o\bigg(\bigg|\bigg|\begin{bmatrix}
    s       & 0  \\
    t       & s  \\
\end{bmatrix}\bigg|\bigg|\bigg), \bigg|\bigg|\begin{bmatrix}
    s       & 0  \\
    t       & s  \\
\end{bmatrix}\bigg|\bigg|=\max\{|s|,|t|\} \begin{bmatrix}
    u'       & 0  \\
    v'      & u'  \\
\end{bmatrix} \begin{bmatrix}
    x       & 0  \\
    y       & x  \\
\end{bmatrix} F \begin{bmatrix}
    x       & 0  \\
    y       & x  \\
\end{bmatrix} f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}","['derivatives', 'matrix-calculus', 'isometry', 'frechet-derivative', 'dual-numbers']"
42,$F(x) = f([x]) \cdot f(\{x\})$ . Find $f$,. Find,F(x) = f([x]) \cdot f(\{x\}) f,"Given a function $f: [0,\infty) \rightarrow (0,\infty)$ which admits primitives, and let $F$ be a primitive such that: $F(x) = f([x]) \cdot f(\{x\})$ for all x, and there exists $a>0$ s.t. $f(a)=F([a]) \cdot F(\{a\})$ , where $[x]$ refers to the integer part, and the other to the fractional part. Find every function $f$ that verifies these properties. I have only been able to find that small relations between $F$ and $f$ for intervals of the form $[n,n+1)$ and that $f$ seems to have a recursive formula for natural numbers, yet nothing that would point out a certain clear path..","Given a function which admits primitives, and let be a primitive such that: for all x, and there exists s.t. , where refers to the integer part, and the other to the fractional part. Find every function that verifies these properties. I have only been able to find that small relations between and for intervals of the form and that seems to have a recursive formula for natural numbers, yet nothing that would point out a certain clear path..","f: [0,\infty) \rightarrow (0,\infty) F F(x) = f([x]) \cdot f(\{x\}) a>0 f(a)=F([a]) \cdot F(\{a\}) [x] f F f [n,n+1) f","['real-analysis', 'derivatives', 'continuity', 'fractional-part']"
43,Differentiate using product rule,Differentiate using product rule,,"Differentiate $$R(t) = (4t + e^t)(4 - \sqrt t) $$ $$R'(t) = $$ When I used the product rule $$ f'(x) *g(x) + f(x)*g'(x) $$ I get $$(16 -4 \sqrt t + 4e^t - e^t\sqrt t) + (-(4t - e^t / ( 2\sqrt t )))$$ What do I do from here? When I input this as the answer is says incorrect. I've been stuck on this for hours, any help is appreciated. edit: here is a screenshot edit 2: everyone was correct and apparently I just cant type I had already found this answer hours ago, but apparently im stupid and type it incorrectly into the box and wasted hours I could have been doing something else. I feel so dumb","Differentiate When I used the product rule I get What do I do from here? When I input this as the answer is says incorrect. I've been stuck on this for hours, any help is appreciated. edit: here is a screenshot edit 2: everyone was correct and apparently I just cant type I had already found this answer hours ago, but apparently im stupid and type it incorrectly into the box and wasted hours I could have been doing something else. I feel so dumb",R(t) = (4t + e^t)(4 - \sqrt t)  R'(t) =   f'(x) *g(x) + f(x)*g'(x)  (16 -4 \sqrt t + 4e^t - e^t\sqrt t) + (-(4t - e^t / ( 2\sqrt t ))),"['calculus', 'derivatives']"
44,Closed form for the $n$-th derivative of $\csc^2$?,Closed form for the -th derivative of ?,n \csc^2,"To find a closed representation for the $n$ -th derivative of $f(t)=\csc^2(t)$ , I started with the Ansatz: $$ f^{(n)}(t) = \frac{p_n(\sin t, \cos t)}{\sin^{2+n}(t)} \tag1 $$ where $p_n$ is a polynomial (with integer coefficients) in two variables. This leads to the recursion $$ p_{n+1}=xy\frac{\partial p_n}{\partial x} -x^2\frac{\partial p_n}{\partial y} -(2+n)\,y\,p_n~;\qquad p_0=1 \tag2 $$ and I am stuck. Likely, someone has solved this before and has a pointer to the $n$ -th derivative of $\csc^2$ ? With ""closed representation"" , I mean something like a finite sum and without recursion. Note: This question is on $\cot$ and only comes up with a recursion. (It's a polynomial in just 1 variable so maybe simpler to turn into an explcit form.) Edit That question leads to Lemma 2.1 in: V.S. Adamchik, On the Hurwitz function for rational arguments , Applied Mathematics and Computation , Volume 187, Issue 1, 1 April 2007, Pages 3–12. It states for the $n$ -th derivative of $\cot z$ , $n>1$ : $$ \frac{d^n}{dz^n} \cot z = (2i)^n (\cot(z)-i)\sum_{j=1}^n \frac{k!}{2^k} \left\{ {n \atop k} \right\} (i\cot(z)-1)^k \\ $$ $$ \text{where } \left\{ {j \atop k} \right\} \text{ is the Stirling subset}\dots $$ but after the $\dots$ the text is truncated I cannot make 100% sense of it and how $k$ is computed etc. Maybe someone kows the missing bits.","To find a closed representation for the -th derivative of , I started with the Ansatz: where is a polynomial (with integer coefficients) in two variables. This leads to the recursion and I am stuck. Likely, someone has solved this before and has a pointer to the -th derivative of ? With ""closed representation"" , I mean something like a finite sum and without recursion. Note: This question is on and only comes up with a recursion. (It's a polynomial in just 1 variable so maybe simpler to turn into an explcit form.) Edit That question leads to Lemma 2.1 in: V.S. Adamchik, On the Hurwitz function for rational arguments , Applied Mathematics and Computation , Volume 187, Issue 1, 1 April 2007, Pages 3–12. It states for the -th derivative of , : but after the the text is truncated I cannot make 100% sense of it and how is computed etc. Maybe someone kows the missing bits.","n f(t)=\csc^2(t) 
f^{(n)}(t) = \frac{p_n(\sin t, \cos t)}{\sin^{2+n}(t)} \tag1
 p_n 
p_{n+1}=xy\frac{\partial p_n}{\partial x}
-x^2\frac{\partial p_n}{\partial y}
-(2+n)\,y\,p_n~;\qquad p_0=1 \tag2
 n \csc^2 \cot n \cot z n>1 
\frac{d^n}{dz^n} \cot z
= (2i)^n (\cot(z)-i)\sum_{j=1}^n \frac{k!}{2^k} \left\{ {n \atop k} \right\} (i\cot(z)-1)^k \\
 
\text{where } \left\{ {j \atop k} \right\} \text{ is the Stirling subset}\dots
 \dots k","['derivatives', 'closed-form']"
45,Directional derivative of proximal mapping of a convex function,Directional derivative of proximal mapping of a convex function,,"Let $f:\mathbb{R}^n\rightarrow\overline{\mathbb{R}}$ be a proper closed convex function that is locally Lipschitz continuous on its domain $D(f)$ . Define the proximal mapping of $f$ to be $$\textbf{prox}_{\lambda f}(x)=\arg\min_u\left\{f(u)+\frac{1}{2\lambda}\|u-x\|^2\right\}$$ In this context, it is not hard to see $\textbf{prox}_{\lambda f}(x)$ is a Lipschitz continuous function w.r.t. variable $x$ for any parameter $\lambda>0$ . I am wondering under what assumption on $f$ , will the directional derivative of proximal mapping, i.e., $$\textbf{prox}_{\lambda f}'(x;d) := \lim_{t\downarrow 0}\frac{\textbf{prox}_{\lambda f}(x+td)-\textbf{prox}_{\lambda f}(x)}{t}$$ exist? From the existing literature (Proposition 5.3.5, p. 141), I know if $f:=I_C$ is an indicator function of a closed convex set $C$ (hence proximal mapping reduces to projection mapping), then the directional derivative exists. To be more precise, for any $x\in C$ and $d\in\mathbb{R}^n$ , $$\textbf{proj}_C'(x;d) := \lim_{t\downarrow 0}\frac{\textbf{proj}_C(x+td)-\textbf{proj}_C(x)}{t} = \textbf{proj}_{T_C(x)}(d)$$ where $T_C(x)$ is the tangent cone of $C$ at $x$ . I am wondering if this is only special for projection operator or can be slightly generalized to proximal operator of convex functions with some nice properties or structures.","Let be a proper closed convex function that is locally Lipschitz continuous on its domain . Define the proximal mapping of to be In this context, it is not hard to see is a Lipschitz continuous function w.r.t. variable for any parameter . I am wondering under what assumption on , will the directional derivative of proximal mapping, i.e., exist? From the existing literature (Proposition 5.3.5, p. 141), I know if is an indicator function of a closed convex set (hence proximal mapping reduces to projection mapping), then the directional derivative exists. To be more precise, for any and , where is the tangent cone of at . I am wondering if this is only special for projection operator or can be slightly generalized to proximal operator of convex functions with some nice properties or structures.",f:\mathbb{R}^n\rightarrow\overline{\mathbb{R}} D(f) f \textbf{prox}_{\lambda f}(x)=\arg\min_u\left\{f(u)+\frac{1}{2\lambda}\|u-x\|^2\right\} \textbf{prox}_{\lambda f}(x) x \lambda>0 f \textbf{prox}_{\lambda f}'(x;d) := \lim_{t\downarrow 0}\frac{\textbf{prox}_{\lambda f}(x+td)-\textbf{prox}_{\lambda f}(x)}{t} f:=I_C C x\in C d\in\mathbb{R}^n \textbf{proj}_C'(x;d) := \lim_{t\downarrow 0}\frac{\textbf{proj}_C(x+td)-\textbf{proj}_C(x)}{t} = \textbf{proj}_{T_C(x)}(d) T_C(x) C x,"['derivatives', 'convex-analysis', 'convex-optimization', 'proximal-operators', 'non-smooth-analysis']"
46,On a proof of the non-differentiability of the Blancmange curve,On a proof of the non-differentiability of the Blancmange curve,,"In my nameless lecture notes, in the construction of a continuous, nowhere differentiable function (the Blancmange curve), we encounter the definition of the sequential derivative and other real-analysis concepts. I have some question about these concepts. Here's a brief background: Part 1: the Blancmange curve Let $$f_1(x)=\begin{cases} x\qquad \ \text{when } 0\leq x< 1/2,\\ 1-x \  \ \text{when } 1/2\leq x< 1. \end{cases}$$ and define $f_1(x)$ for other values of $x$ so that it becomes periodic with period $1$ . Then we let $$f_2(x)=\frac12 f_1(2x).$$ This is a scaling of $f_1$ ; we get a function with half the period and half the amplitude. In general $$f_{k+1}(x)=2^{-k}f_1(2^kx)\quad \text{for all }x\in\mathbb{R},\quad k=1,2,\ldots.$$ Now sum up all these functions and let $$T(x)=\sum_{k=1}^\infty f_k(x).$$ Since $0\leq f_k(x)\leq 2^{-k}$ , the convergence is uniform according to the Weierstrass $M$ -test and hence $T(x)$ is continuous. $T(x)$ (from Wikipedia)"" /> To understand the next part, here's a lemma from Abbott's Understanding Analysis (second edition), Lemma. Let $f$ be defined on an open interval $J$ and assume that $f$ is differentiable at some $a\in J$ . If $(a_n)$ and $(b_n)$ are sequences satisfying $a_n<a<b_n$ and $\lim a_n = a = \lim b_n$ , then $$f'(a) =\lim_{n\rightarrow\infty}\frac{f(b_n)-f(a_n)}{b_n-a_n}.$$ Part 2: the sequential derivative We shall show that $T(x)$ is not differentiable at any arbitrary point $c$ . For each integer $n$ , we let $m$ be the integer that satisfies $$\frac{m}{2^n}\leq c <\frac{m+1}{2^n},\tag{1}$$ and denote by $I_n$ the interval $[m2^{-n},(m+1)2^{-n}]$ . The length of this interval is $2^{-n}$ which tends to zero as $n\to\infty$ , and the interval contains $c$ . Hence if $T(x)$ is differentiable at $c$ , then the difference quotient $$d_n=\frac{T((m+1)2^{-n})-T(m2^{-n})}{2^{-n}}$$ equals $T'(c)+o(2^{-n})$ which has the limit $T'(c)$ as $n\to\infty$ . Note that my lecture notes have a non-strict inequality, i.e. $\frac{m}{2^n}\color{red}{\leq}c<\frac{m+1}{2^n}$ and I think this is problematic in the proof of the lemma provided here , since we get division by zero if $a_n=a$ , i.e. if $\frac{m}{2^n}=c$ . On the other hand, consider $a=c=\frac14$ and $n=2$ , then there is no integer $m$ satisfying $\frac{m}{4}<\frac14<\frac{m+1}{4}$ . So it seems like we need the non-strict inequality after all ( $m=1$ would do it if the first inequality was a non-strict inequality). This confuses me and I'd be grateful for a comment or two on this. Moreover I think $o(2^{-n})$ should be $o(1)$ (as suggested by this answer ). Part 3: proof We shall prove that $d_n$ has no limit. Consider the corresponding difference quotient for $f_k$ . If $k>n$ , then $f_k$ equals zero at both endpoints of $I_n$ , and then the difference quotient equals $0$ . This cannot happen if $k\leq n$ , because then the difference quotient equals $1$ or $-1$ , since $f_k$ is made up of line segments having these gradients. The sign depends only on the gradient of $f_k$ at the point $a$ . The difference quotient $d_n$ is equal to the sum of these numbers. The difference $d_n-d_{n-1}$ is a difference between two sums the terms of which are equal in pairs except the $n$ th terms. Hence $$d_n-d_{n-1}=\pm 1.$$ This is valid for all $n$ , and therefor the sequence $(d_n)$ has no limit. By ""...corresponding difference quotient for $f_k$ "" I assume they mean $$\frac{f_k((m+1)2^{-n})-f_k(m2^{-n})}{2^{-n}}.\tag{2}$$ Since $f_{k+1}(x)=2^{-k}f_1(2^kx)$ , the above fraction simplifies to $$\frac{2^{-k+1}f_1(2^{k-1}(m+1)2^{-n})-2^{-k+1}f_1(2^{k-1}m2^{-n})}{2^{-n}}=2^{-p}\left(f_1(2^{p}(m+1))-f_1(2^{p}m)\right),$$ where $p=k-1-n$ . When $k>n$ , then $p\geq 0$ and then $2^{p}(m+1)$ and $2^{p}m$ are integers where $f_1$ is $0$ . I have a hard time convincing myself of that $(2)$ equals $-1$ or $1$ when $k\leq n$ . Then $p<0$ , and $2^{p}(m+1)$ and $2^{p}m$ are rational numbers, but what is $f_1$ then? Finally, and probably related to the previous question, I  do not understand why $d_n-d_{n-1}=\pm 1$ . Since $T(x)$ is uniformly convergent on $\mathbb{R}$ and the difference quotient of $f_k$ equals $0$ for $k>n$ , we have $$\begin{align} d_n-d_{n-1}&=\frac{T((m+1)2^{-n})-T(m2^{-n})}{2^{-n}}-\frac{T((m+1)2^{-n+1})-T(m2^{-n+1})}{2^{-n+1}} \tag{3} \\ &= \frac{\sum_{k=1}^\infty f_k((m+1)2^{-n})-\sum_{k=1}^\infty f_k(m2^{-n})}{2^{-n}}-\frac{\sum_{k=1}^\infty f_k((m+1)2^{-n+1})-\sum_{k=1}^\infty f_k(m2^{-n+1})}{2^{-n+1}} \tag{4}\\ &=\sum_{k=1}^n \frac{ f_k((m+1)2^{-n})-f_k(m2^{-n})}{2^{-n}}-\sum_{k=1}^{n-1} \frac{ f_k((m+1)2^{-n+1})-f_k(m2^{-n+1})}{2^{-n+1}}, \tag{5} \end{align}$$ however, I do not see from this why the sums have equal terms up to the $n$ th term.","In my nameless lecture notes, in the construction of a continuous, nowhere differentiable function (the Blancmange curve), we encounter the definition of the sequential derivative and other real-analysis concepts. I have some question about these concepts. Here's a brief background: Part 1: the Blancmange curve Let and define for other values of so that it becomes periodic with period . Then we let This is a scaling of ; we get a function with half the period and half the amplitude. In general Now sum up all these functions and let Since , the convergence is uniform according to the Weierstrass -test and hence is continuous. $T(x)$ (from Wikipedia)"" /> To understand the next part, here's a lemma from Abbott's Understanding Analysis (second edition), Lemma. Let be defined on an open interval and assume that is differentiable at some . If and are sequences satisfying and , then Part 2: the sequential derivative We shall show that is not differentiable at any arbitrary point . For each integer , we let be the integer that satisfies and denote by the interval . The length of this interval is which tends to zero as , and the interval contains . Hence if is differentiable at , then the difference quotient equals which has the limit as . Note that my lecture notes have a non-strict inequality, i.e. and I think this is problematic in the proof of the lemma provided here , since we get division by zero if , i.e. if . On the other hand, consider and , then there is no integer satisfying . So it seems like we need the non-strict inequality after all ( would do it if the first inequality was a non-strict inequality). This confuses me and I'd be grateful for a comment or two on this. Moreover I think should be (as suggested by this answer ). Part 3: proof We shall prove that has no limit. Consider the corresponding difference quotient for . If , then equals zero at both endpoints of , and then the difference quotient equals . This cannot happen if , because then the difference quotient equals or , since is made up of line segments having these gradients. The sign depends only on the gradient of at the point . The difference quotient is equal to the sum of these numbers. The difference is a difference between two sums the terms of which are equal in pairs except the th terms. Hence This is valid for all , and therefor the sequence has no limit. By ""...corresponding difference quotient for "" I assume they mean Since , the above fraction simplifies to where . When , then and then and are integers where is . I have a hard time convincing myself of that equals or when . Then , and and are rational numbers, but what is then? Finally, and probably related to the previous question, I  do not understand why . Since is uniformly convergent on and the difference quotient of equals for , we have however, I do not see from this why the sums have equal terms up to the th term.","f_1(x)=\begin{cases}
x\qquad \ \text{when } 0\leq x< 1/2,\\
1-x \  \ \text{when } 1/2\leq x< 1.
\end{cases} f_1(x) x 1 f_2(x)=\frac12 f_1(2x). f_1 f_{k+1}(x)=2^{-k}f_1(2^kx)\quad \text{for all }x\in\mathbb{R},\quad k=1,2,\ldots. T(x)=\sum_{k=1}^\infty f_k(x). 0\leq f_k(x)\leq 2^{-k} M T(x) f J f a\in J (a_n) (b_n) a_n<a<b_n \lim a_n = a = \lim b_n f'(a) =\lim_{n\rightarrow\infty}\frac{f(b_n)-f(a_n)}{b_n-a_n}. T(x) c n m \frac{m}{2^n}\leq c <\frac{m+1}{2^n},\tag{1} I_n [m2^{-n},(m+1)2^{-n}] 2^{-n} n\to\infty c T(x) c d_n=\frac{T((m+1)2^{-n})-T(m2^{-n})}{2^{-n}} T'(c)+o(2^{-n}) T'(c) n\to\infty \frac{m}{2^n}\color{red}{\leq}c<\frac{m+1}{2^n} a_n=a \frac{m}{2^n}=c a=c=\frac14 n=2 m \frac{m}{4}<\frac14<\frac{m+1}{4} m=1 o(2^{-n}) o(1) d_n f_k k>n f_k I_n 0 k\leq n 1 -1 f_k f_k a d_n d_n-d_{n-1} n d_n-d_{n-1}=\pm 1. n (d_n) f_k \frac{f_k((m+1)2^{-n})-f_k(m2^{-n})}{2^{-n}}.\tag{2} f_{k+1}(x)=2^{-k}f_1(2^kx) \frac{2^{-k+1}f_1(2^{k-1}(m+1)2^{-n})-2^{-k+1}f_1(2^{k-1}m2^{-n})}{2^{-n}}=2^{-p}\left(f_1(2^{p}(m+1))-f_1(2^{p}m)\right), p=k-1-n k>n p\geq 0 2^{p}(m+1) 2^{p}m f_1 0 (2) -1 1 k\leq n p<0 2^{p}(m+1) 2^{p}m f_1 d_n-d_{n-1}=\pm 1 T(x) \mathbb{R} f_k 0 k>n \begin{align}
d_n-d_{n-1}&=\frac{T((m+1)2^{-n})-T(m2^{-n})}{2^{-n}}-\frac{T((m+1)2^{-n+1})-T(m2^{-n+1})}{2^{-n+1}}
\tag{3} \\ &= \frac{\sum_{k=1}^\infty f_k((m+1)2^{-n})-\sum_{k=1}^\infty f_k(m2^{-n})}{2^{-n}}-\frac{\sum_{k=1}^\infty f_k((m+1)2^{-n+1})-\sum_{k=1}^\infty f_k(m2^{-n+1})}{2^{-n+1}} \tag{4}\\ &=\sum_{k=1}^n \frac{
f_k((m+1)2^{-n})-f_k(m2^{-n})}{2^{-n}}-\sum_{k=1}^{n-1} \frac{
f_k((m+1)2^{-n+1})-f_k(m2^{-n+1})}{2^{-n+1}}, \tag{5} \end{align} n","['real-analysis', 'derivatives', 'continuity', 'proof-explanation', 'uniform-convergence']"
47,Prove that the function $g(x)=\begin{cases} \sin (\frac 1x)& x \neq 0 \\ 1& x=0 \end{cases}$ has no antiderivative,Prove that the function  has no antiderivative,g(x)=\begin{cases} \sin (\frac 1x)& x \neq 0 \\ 1& x=0 \end{cases},"Prove that the function $$g(x)=\begin{cases}     \sin (\frac 1x) & x \neq 0 \\ 1 & x=0 \end{cases}$$ is not a derivative of any function that is differentiable along the whole line. We want to prove that $g$ has no antiderivative. Assume the opposite - let $g$ have an antiderivative $G$ . Every antiderivative has the Darboux property, so for any $a,b$ in the domain of $G$ , if $G(a)G(b)<0$ , then there is $c\in (a,b)$ that $G(c)=0$ . I further tried to use mean value theorem to contradict the Darboux property assumption, but I didn't get anything meaningful. Could you please help me?","Prove that the function is not a derivative of any function that is differentiable along the whole line. We want to prove that has no antiderivative. Assume the opposite - let have an antiderivative . Every antiderivative has the Darboux property, so for any in the domain of , if , then there is that . I further tried to use mean value theorem to contradict the Darboux property assumption, but I didn't get anything meaningful. Could you please help me?","g(x)=\begin{cases}
    \sin (\frac 1x) & x \neq 0 \\ 1 & x=0
\end{cases} g g G a,b G G(a)G(b)<0 c\in (a,b) G(c)=0","['real-analysis', 'derivatives', 'mean-value-theorem']"
48,Application of Rolle's theorem for finding roots of a function and its derivative,Application of Rolle's theorem for finding roots of a function and its derivative,,"The question is as follows Consider a non-constant thrice differentiable function $f:\mathbb R \to \mathbb R$ , such that $\,f(x) = f(6-x) \, \forall\, x \in \mathbb R $ . And f'(0)=f'(2)=f'(5) = 0. If n denotes the minimum no. of roots of the equation $$ [f''(x)]^2+f'(x)f'''(x) =0$$ in [0,6]. Then the minimum value of n is? The answer is 12 and the approach used requires us to find the $x \in [0,6]$ where $f'(x) = 0$ , the solution only used the equation $f(x) = f(6-x)$ Differentiating both sides $f'(x) = -f'(6-x)$ Using this equation and the fact that $f'(0)=f'(2)=f'(5) = 0$ , it deduces that $f'(x) = 0$ at $x =0,1,2,3,4,5,6$ . My question is can we use the fact $f(x) = f(6-x)$ and when x = 0, $f(0) = f(6)$ , and then by rolle's theorem $\exists \, c \in (0,6)$ such that f'(c) = 0. and this c isn't necessarily in  {1,2,3,4,5}. And we could probably extend this and find distinct $c \in \mathbb R$ for every $x \in (0,3)$ by using f(x) = f(6-x). Is my assertion correct and does it work in this question as in since we require minimum n should such c's be ignored? Edit: 1.)I came across this question similar to the one, I have asked question , and even the accepted answer under this question doesn't use Rolle's theorem to deduce existence of a possible root for f'(x) different from {0,1,2,3,4,5,6} 2.) Corrected the equation $[f''(x)]^2-f'(x)f'''(x) =0$ to $[f''(x)]^2+f'(x)f'''(x) =0$","The question is as follows Consider a non-constant thrice differentiable function , such that . And f'(0)=f'(2)=f'(5) = 0. If n denotes the minimum no. of roots of the equation in [0,6]. Then the minimum value of n is? The answer is 12 and the approach used requires us to find the where , the solution only used the equation Differentiating both sides Using this equation and the fact that , it deduces that at . My question is can we use the fact and when x = 0, , and then by rolle's theorem such that f'(c) = 0. and this c isn't necessarily in  {1,2,3,4,5}. And we could probably extend this and find distinct for every by using f(x) = f(6-x). Is my assertion correct and does it work in this question as in since we require minimum n should such c's be ignored? Edit: 1.)I came across this question similar to the one, I have asked question , and even the accepted answer under this question doesn't use Rolle's theorem to deduce existence of a possible root for f'(x) different from {0,1,2,3,4,5,6} 2.) Corrected the equation to","f:\mathbb R \to \mathbb R \,f(x) = f(6-x) \, \forall\, x \in \mathbb R   [f''(x)]^2+f'(x)f'''(x) =0 x \in [0,6] f'(x) = 0 f(x) = f(6-x) f'(x) = -f'(6-x) f'(0)=f'(2)=f'(5) = 0 f'(x) = 0 x =0,1,2,3,4,5,6 f(x) = f(6-x) f(0) = f(6) \exists \, c \in (0,6) c \in \mathbb R x \in (0,3) [f''(x)]^2-f'(x)f'''(x) =0 [f''(x)]^2+f'(x)f'''(x) =0","['real-analysis', 'derivatives', 'mean-value-theorem']"
49,Can I apply power rule to the derivative of constant function?,Can I apply power rule to the derivative of constant function?,,"I just saw someone trying to show $\frac{d f(x)}{d x}=\frac{d k}{d x}=0$ by arguing that $$f(x)=k=k\cdot x^0;$$ thus, according to the power rule, $f'(x)=0\cdot k x^{-1}=0.$ I wonder if this is mathematically valid in terms of general application of the power rule. In other words, can I apply the power rule even to constant functions?","I just saw someone trying to show by arguing that thus, according to the power rule, I wonder if this is mathematically valid in terms of general application of the power rule. In other words, can I apply the power rule even to constant functions?",\frac{d f(x)}{d x}=\frac{d k}{d x}=0 f(x)=k=k\cdot x^0; f'(x)=0\cdot k x^{-1}=0.,['derivatives']
50,Showing that the Fourier transform of $f_n(t)=e^{\pi t^2}\frac{d^n}{dt^n}e^{-2\pi t^2}$ is $\hat{f_n} = (-i)^nf_n$,Showing that the Fourier transform of  is,f_n(t)=e^{\pi t^2}\frac{d^n}{dt^n}e^{-2\pi t^2} \hat{f_n} = (-i)^nf_n,"Let $n \in \mathbb{N}_0$ and define $f_n(t)=e^{\pi t^2}\frac{d^n}{dt^n}e^{-2\pi t^2}$ . I am tasked to show that $\hat{f_n} = (-i)^nf_n$ for $\hat{f_n}$ the Fourier transform of $f_n$ . My problem is that I am either not seeing the integral trick I am supposed to use or I fail to see the pattern in the derivatives of $e^{-2\pi t^2}$ as I don't know how to proceed beyond $$\hat{f_n}(y) = \int_{\mathbb{R}}\left(e^{\pi t^2}\frac{d^n}{dt^n}e^{-2\pi t^2}\right)e^{-2\pi i t y}dt$$ I am not looking for a complete solution per se, but rather a better intuition for this problem. Although if you happen to know some elegant and general method for dealing with this problem, feel free to share it! Thanks!","Let and define . I am tasked to show that for the Fourier transform of . My problem is that I am either not seeing the integral trick I am supposed to use or I fail to see the pattern in the derivatives of as I don't know how to proceed beyond I am not looking for a complete solution per se, but rather a better intuition for this problem. Although if you happen to know some elegant and general method for dealing with this problem, feel free to share it! Thanks!",n \in \mathbb{N}_0 f_n(t)=e^{\pi t^2}\frac{d^n}{dt^n}e^{-2\pi t^2} \hat{f_n} = (-i)^nf_n \hat{f_n} f_n e^{-2\pi t^2} \hat{f_n}(y) = \int_{\mathbb{R}}\left(e^{\pi t^2}\frac{d^n}{dt^n}e^{-2\pi t^2}\right)e^{-2\pi i t y}dt,"['calculus', 'integration', 'derivatives', 'fourier-analysis', 'fourier-transform']"
51,Why is $\frac{\partial^2 f}{\partial x\partial x}$ equal to $\frac{\partial^2 f}{\partial x^2}$ and not $\frac{\partial^2 f}{(\partial x)^2}$?,Why is  equal to  and not ?,\frac{\partial^2 f}{\partial x\partial x} \frac{\partial^2 f}{\partial x^2} \frac{\partial^2 f}{(\partial x)^2},"I know that this is a borderline pedantic question, but is there any other reason than a convention why usual calculus and differential equation texts say that $\frac{\partial^2 f}{\partial x\partial x} = \frac{\partial^2 f}{\partial x^2}$ and not $\frac{\partial^2 f}{\partial x\partial x} = \frac{\partial^2 f}{(\partial x)^2}$ ? Or are there some meaningful set of rules with which you can manipulate the differential forms $\partial x$ so that the $\frac{\partial}{\partial x}\frac{\partial f}{\partial x} = \frac{\partial^2 f}{\partial x^2}$ doesn't feel that arbitrary (although I suppose that this is a question of preference)?","I know that this is a borderline pedantic question, but is there any other reason than a convention why usual calculus and differential equation texts say that and not ? Or are there some meaningful set of rules with which you can manipulate the differential forms so that the doesn't feel that arbitrary (although I suppose that this is a question of preference)?",\frac{\partial^2 f}{\partial x\partial x} = \frac{\partial^2 f}{\partial x^2} \frac{\partial^2 f}{\partial x\partial x} = \frac{\partial^2 f}{(\partial x)^2} \partial x \frac{\partial}{\partial x}\frac{\partial f}{\partial x} = \frac{\partial^2 f}{\partial x^2},"['calculus', 'derivatives', 'notation', 'partial-derivative', 'math-history']"
52,"In calculus, how is taking $\ln$ on both sides justified?","In calculus, how is taking  on both sides justified?",\ln,"For example, if we want to differentiate $y=x^x,$ we can turn it into $e^{\ln(y)}=y=e^{x\ln(x)},$ seemingly ignore when $x<0,$ differentiate, then convert back. This method can make differentiation (and solving limits) easier, so is obviously important, but I can't find a general proof that justifies it. EDIT : $x^x$ has domain $\mathbb R^+,$ so the above was a bad example! Instead, consider $y:\mathbb{R}\to\mathbb{R}.$ If we want to differentiate or take the limit of $y,$ can we take $\ln$ of $y$ without considering when $y\le0$ for $x\in\mathbb{R}\:?$ Then, differentiate or take the limit of $\ln y$ with respect to $x,$ then solve the equation for $\frac{\mathrm dy}{\mathrm dx}$ or $y$ and get the answer? I'm guessing that since the domain was never explicitly stated, I missed that $y>0$ for $\ln y.$ So, if $x\in\mathbb{R}$ and we change $y=x$ to $\ln y=\ln x,$ we let $x=-x$ for $x<0,$ right?","For example, if we want to differentiate we can turn it into seemingly ignore when differentiate, then convert back. This method can make differentiation (and solving limits) easier, so is obviously important, but I can't find a general proof that justifies it. EDIT : has domain so the above was a bad example! Instead, consider If we want to differentiate or take the limit of can we take of without considering when for Then, differentiate or take the limit of with respect to then solve the equation for or and get the answer? I'm guessing that since the domain was never explicitly stated, I missed that for So, if and we change to we let for right?","y=x^x, e^{\ln(y)}=y=e^{x\ln(x)}, x<0, x^x \mathbb R^+, y:\mathbb{R}\to\mathbb{R}. y, \ln y y\le0 x\in\mathbb{R}\:? \ln y x, \frac{\mathrm dy}{\mathrm dx} y y>0 \ln y. x\in\mathbb{R} y=x \ln y=\ln x, x=-x x<0,","['calculus', 'derivatives', 'logarithms']"
53,what is the link between exponential/logarithmic function and 1/x,what is the link between exponential/logarithmic function and 1/x,,"Thanks to Kalid Azad's book (betterexplained.com), I understand exponential phenomena better. $ae^{rt}$ gives the growth of ' $a$ ' after ' $t$ ' unit of times and with a ""continuous"" growth rate of ' $r$ ' (so if $rt = 1$ like in $e^1$ the formula will output the new value of ' $a$ ' after a $100\%$ of growth during $1$ unit of time ' $t$ ', it can also be thought of as a $50\%$ growth during $2$ periods of times, etc...). The $\ln(x)$ function outputs the amount of time needed to have a certain growth of the quantity ' $1$ '. e.g. $\ln(2.71\dots) = 1$ (we need $1$ unit of time to transition from $1$ to $2.71\dots$ with $100\%$ continuous growth). My definitions are not $100\%$ mathematical and precise but I can't visualize and understand $\exp(x)$ or $\ln(x)$ without them (especially their applications in engineering stuff). The $\ln(x)$ function is the antiderivative of $\frac1x$ (or $\frac1x$ is the derivative of $\ln(x)$ ), and my question is what's the link between $\frac1x$ and the time needed to have a continuous growth of a rate "" $r$ "" and during $x$ unit of times. For example, why the derivative of the $\ln(x)$ , the function returning the time to achieve $100\%$ growth during a time unit, is the inverse of the time unit $x$ . What's the intuitive explanation of $\ln(x)$ being the antiderivative of $\frac1x$ ? $$e^x = e^1, e^2, e^3, e^4,\dots$$ $$\ln(e^x) = 1, 2, 3, 4,\dots$$ $$\frac1x = \frac1{e^1}, \frac1{e^2}, \frac1{e^3}, \frac1{e^4},\dots$$","Thanks to Kalid Azad's book (betterexplained.com), I understand exponential phenomena better. gives the growth of ' ' after ' ' unit of times and with a ""continuous"" growth rate of ' ' (so if like in the formula will output the new value of ' ' after a of growth during unit of time ' ', it can also be thought of as a growth during periods of times, etc...). The function outputs the amount of time needed to have a certain growth of the quantity ' '. e.g. (we need unit of time to transition from to with continuous growth). My definitions are not mathematical and precise but I can't visualize and understand or without them (especially their applications in engineering stuff). The function is the antiderivative of (or is the derivative of ), and my question is what's the link between and the time needed to have a continuous growth of a rate "" "" and during unit of times. For example, why the derivative of the , the function returning the time to achieve growth during a time unit, is the inverse of the time unit . What's the intuitive explanation of being the antiderivative of ?","ae^{rt} a t r rt = 1 e^1 a 100\% 1 t 50\% 2 \ln(x) 1 \ln(2.71\dots) = 1 1 1 2.71\dots 100\% 100\% \exp(x) \ln(x) \ln(x) \frac1x \frac1x \ln(x) \frac1x r x \ln(x) 100\% x \ln(x) \frac1x e^x = e^1, e^2, e^3, e^4,\dots \ln(e^x) = 1, 2, 3, 4,\dots \frac1x = \frac1{e^1}, \frac1{e^2}, \frac1{e^3}, \frac1{e^4},\dots","['integration', 'derivatives', 'logarithms', 'exponential-function', 'inverse-function']"
54,General form of integral possibly related to arctan function?,General form of integral possibly related to arctan function?,,"I am dealing with an integral which has the form: $$ I = \int_{-\gamma}^{+\gamma} \frac{\alpha}{\left(\beta^2 + \alpha^2 z^2 \right)^{\frac{n}{2}}} \,\text{d}z, $$ with real constants $\alpha$ and $\beta$ , and integer $n \geq 2$ . I think I am right in saying that for $n=2$ , this integral becomes $$ I = \frac{1}{\beta}\int_{-\gamma}^{+\gamma} \text{d}\arctan \left(\frac{\alpha}{\beta}z \right). $$ But I'm wondering whether there is a general result for any $n$ ? Appreciate your thoughts.","I am dealing with an integral which has the form: with real constants and , and integer . I think I am right in saying that for , this integral becomes But I'm wondering whether there is a general result for any ? Appreciate your thoughts.","
I = \int_{-\gamma}^{+\gamma} \frac{\alpha}{\left(\beta^2 + \alpha^2 z^2 \right)^{\frac{n}{2}}} \,\text{d}z,
 \alpha \beta n \geq 2 n=2 
I = \frac{1}{\beta}\int_{-\gamma}^{+\gamma} \text{d}\arctan \left(\frac{\alpha}{\beta}z \right).
 n","['real-analysis', 'calculus', 'integration', 'derivatives', 'trigonometric-integrals']"
55,Mean Value Theorem question (show that f is constant),Mean Value Theorem question (show that f is constant),,"Suppose that $f, g : R → R$ are functions such that $$|f(x) − f(y)| ≤ |g(x) − g(y)| \sqrt{|x − y|}$$ for any $x, y ∈ R$ . If $g$ is differentiable with bounded derivative on all $R$ , show that $f$ is constant. I know I am meant to be using MVT for this question, I attempted to use $g(x)$ as the function because we know it is differentiable (condition required for MVT): $$\frac{g(b)-g(a)}{b-a}=g'(c)$$ $$|g(b)-g(a)|=|g'(c)|b-a|$$ I am lost, not sure where to start the question.","Suppose that are functions such that for any . If is differentiable with bounded derivative on all , show that is constant. I know I am meant to be using MVT for this question, I attempted to use as the function because we know it is differentiable (condition required for MVT): I am lost, not sure where to start the question.","f, g : R → R |f(x) − f(y)| ≤ |g(x) − g(y)| \sqrt{|x − y|} x, y ∈ R g R f g(x) \frac{g(b)-g(a)}{b-a}=g'(c) |g(b)-g(a)|=|g'(c)|b-a|","['derivatives', 'continuity', 'mean-value-theorem']"
56,"If $F$ is differentiable at $x_0$, $f$ is continuous at $x_0$","If  is differentiable at ,  is continuous at",F x_0 f x_0,"just looking for a hint on how to proceed here. In Tao's Analysis I , exercise 11.9.3 is as follows: Let $a<b$ be real numbers, and let $f:[a,b]\rightarrow\mathbb{R}$ be a monotone increasing function. Let $F:[a,b]\rightarrow\mathbb{R}$ be the function $F(x):=\int_{[a,x]}f$ . Let $x_0\in(a,b)$ be given. Show that $F$ is differentiable at $x_0$ iff $f$ is continuous at $x_0$ . (Hint: one direction is taken care of by one of the Fundamental Theorems of Calculus. For the other, consider the left and right limits of $f$ and argue by contradiction.) From this section, we are also given the two Fundamental Theorems of Calculus as follows: (First Fundamental Theorem). Let $a<b$ be real numbers, and let $f:[a,b]\rightarrow\mathbb{R}$ be a Riemann integrable function. Let $F:[a,b]\rightarrow\mathbb{R}$ be the function $F(x):=\int_{[a,x]}f$ . Then $F$ is continuous. Furthermore, if $x_0\in[a,b]$ and $f$ is continuous, then $F$ is differentiable at $x_0$ , and $F'(x_0)=f(x_0)$ . (Second Fundamental Theorem). Let $a<b$ be real numbers, and let $f:[a,b]\rightarrow\mathbb{R}$ be a Riemann integrable function. If $F:[a,b]\rightarrow\mathbb{R}$ is an antiderivative of $f$ , then $\int_{[a,b]}f=F(b)-F(a)$ . It was proven in an earlier section that if $f$ is monotone on a closed interval, it is integrable on that interval. Following the hint given, the First Fundamental Theorem tells us if $f$ is continuous at $x_0$ , then $F(x):=\int_{[a,x]}f$ is differentiable at $x_0$ , so we are done in that respect. My trouble here is with the other direction. Following the hint again, suppose for the sake of contradiction that $F$ is differentiable at $x_0$ , but $f$ is not continuous at $x_0$ . Since $f$ is a function on $[a,b]$ , it is defined at $x_0$ . Since $f$ is monotone increasing, $f(x_0)\geq f(x)$ for all $x<x_0$ and $f(x_0)\leq f(x)$ for all $x>x_0$ . Therefore, $\lim\limits_{x\rightarrow x_0^-}f(x)\leq f(x_0)\leq\lim\limits_{x\rightarrow x_0^+}f(x)$ , and since $f$ is not continuous, $\lim\limits_{x\rightarrow x_0^-}f(x)<\lim\limits_{x\rightarrow x_0^+}f(x)$ . This is where I seem to be stuck. My intuition is that now I should be able to show that $$\lim\limits_{x\rightarrow x_0^-}\frac{F(x)-F(x_0)}{x-x_0}<\lim\limits_{x\rightarrow x_0^+} \frac{F(x)-F(x_0)}{x-x_0},$$ but I can't quite piece together the connection. I've simplified the expression for which we're taking a limit to $$\frac{F(x)-F(x_0)}{x-x_0}=\frac{\int_{[x,x_0]}f}{x-x_0},$$ but this hasn't made anything clear for me, either. Any hints on how to proceed would be much appreciated. Thanks!","just looking for a hint on how to proceed here. In Tao's Analysis I , exercise 11.9.3 is as follows: Let be real numbers, and let be a monotone increasing function. Let be the function . Let be given. Show that is differentiable at iff is continuous at . (Hint: one direction is taken care of by one of the Fundamental Theorems of Calculus. For the other, consider the left and right limits of and argue by contradiction.) From this section, we are also given the two Fundamental Theorems of Calculus as follows: (First Fundamental Theorem). Let be real numbers, and let be a Riemann integrable function. Let be the function . Then is continuous. Furthermore, if and is continuous, then is differentiable at , and . (Second Fundamental Theorem). Let be real numbers, and let be a Riemann integrable function. If is an antiderivative of , then . It was proven in an earlier section that if is monotone on a closed interval, it is integrable on that interval. Following the hint given, the First Fundamental Theorem tells us if is continuous at , then is differentiable at , so we are done in that respect. My trouble here is with the other direction. Following the hint again, suppose for the sake of contradiction that is differentiable at , but is not continuous at . Since is a function on , it is defined at . Since is monotone increasing, for all and for all . Therefore, , and since is not continuous, . This is where I seem to be stuck. My intuition is that now I should be able to show that but I can't quite piece together the connection. I've simplified the expression for which we're taking a limit to but this hasn't made anything clear for me, either. Any hints on how to proceed would be much appreciated. Thanks!","a<b f:[a,b]\rightarrow\mathbb{R} F:[a,b]\rightarrow\mathbb{R} F(x):=\int_{[a,x]}f x_0\in(a,b) F x_0 f x_0 f a<b f:[a,b]\rightarrow\mathbb{R} F:[a,b]\rightarrow\mathbb{R} F(x):=\int_{[a,x]}f F x_0\in[a,b] f F x_0 F'(x_0)=f(x_0) a<b f:[a,b]\rightarrow\mathbb{R} F:[a,b]\rightarrow\mathbb{R} f \int_{[a,b]}f=F(b)-F(a) f f x_0 F(x):=\int_{[a,x]}f x_0 F x_0 f x_0 f [a,b] x_0 f f(x_0)\geq f(x) x<x_0 f(x_0)\leq f(x) x>x_0 \lim\limits_{x\rightarrow x_0^-}f(x)\leq f(x_0)\leq\lim\limits_{x\rightarrow x_0^+}f(x) f \lim\limits_{x\rightarrow x_0^-}f(x)<\lim\limits_{x\rightarrow x_0^+}f(x) \lim\limits_{x\rightarrow x_0^-}\frac{F(x)-F(x_0)}{x-x_0}<\lim\limits_{x\rightarrow x_0^+} \frac{F(x)-F(x_0)}{x-x_0}, \frac{F(x)-F(x_0)}{x-x_0}=\frac{\int_{[x,x_0]}f}{x-x_0},","['real-analysis', 'integration', 'derivatives', 'monotone-functions']"
57,When is a local minimum a global minimum over a closed interval,When is a local minimum a global minimum over a closed interval,,"Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be  twice differentiable function. Suppose $a<c<b$ such that $f'(c)=0$ and $f''(c)>0$ . Then $c$ is a local minimum of $f$ . Does it follow that over $[a,b]$ , $c$ is also global minimum? It seems no. By EVT, there exists $y\in[a,b]$ such that $f(y)\leq f(x)$ for all $x\in[a,b]$ . Then $f(y)\leq f(c)$ . We also know that there exists $\delta>0$ such that for all $x\in(c-\delta,c+\delta)$ , $f(c)\leq f(x)$ . Since it's not necessary for $y$ to be in the interval $(c-\delta,c+\delta)$ , we may not have $f(c)\leq f(y)$ ; i.e. $f(c)$ is not necessarily a global min over $[a,b]$ . Is there anything we can say about the global minimum of $f$ over $[a,b]$ , once we find a local minimum in $(a,b)$ ?","Let be  twice differentiable function. Suppose such that and . Then is a local minimum of . Does it follow that over , is also global minimum? It seems no. By EVT, there exists such that for all . Then . We also know that there exists such that for all , . Since it's not necessary for to be in the interval , we may not have ; i.e. is not necessarily a global min over . Is there anything we can say about the global minimum of over , once we find a local minimum in ?","f:\mathbb{R}\rightarrow\mathbb{R} a<c<b f'(c)=0 f''(c)>0 c f [a,b] c y\in[a,b] f(y)\leq f(x) x\in[a,b] f(y)\leq f(c) \delta>0 x\in(c-\delta,c+\delta) f(c)\leq f(x) y (c-\delta,c+\delta) f(c)\leq f(y) f(c) [a,b] f [a,b] (a,b)","['real-analysis', 'derivatives', 'continuity', 'maxima-minima']"
58,Does $f'(x)\neq 0$ already imply that a function $f:\mathbb{R}\to\mathbb{R}$ is monotonic?,Does  already imply that a function  is monotonic?,f'(x)\neq 0 f:\mathbb{R}\to\mathbb{R},"I am observing a differentiable function $f: I\to\mathbb{R}$ where $I\subseteq\mathbb{R}$ is a connected interval, with the condition $f'(x)\neq 0$ for every $x\in I$ . Now I am asking myself the question how ""messy"" a derivative can look under these circumstances. I want to prove that such a function is already (strictly) monotonic (decreasing, or increasing). The example $f:\mathbb{R}\setminus\{0\}\to\mathbb{R}, x\mapsto 1/x$ shows that it is necessary to have a connected interval. I tried to prove it like this. Let $a,b\in I$ with $a<b$ . Now I want to show that $f(a)<f(b)$ or $f(a)>f(b)$ . Using the mean-value theorem, I have $\frac{f(a)-f(b)}{a-b}=f'(\xi)\neq 0$ . Hence positive or negative. Then $f(a)-f(b)$ is positive or negative. The problem is that I would have to prove that this relation never changes for every $a,b\in I$ . So once positive/negative means, always positive/negative, which is not immediate. I can not come up with a counterexample to this, but this looks like it should need continuously differentiable, as $f'(x)\neq 0$ implying that $f'(x)$ is already always positive/negative seems like an intermediate-value kind off argument, and I wonder if you can walve this additional condition, when we have a connected interval. With other words: How messy can a derivative really look? Thanks in advance.","I am observing a differentiable function where is a connected interval, with the condition for every . Now I am asking myself the question how ""messy"" a derivative can look under these circumstances. I want to prove that such a function is already (strictly) monotonic (decreasing, or increasing). The example shows that it is necessary to have a connected interval. I tried to prove it like this. Let with . Now I want to show that or . Using the mean-value theorem, I have . Hence positive or negative. Then is positive or negative. The problem is that I would have to prove that this relation never changes for every . So once positive/negative means, always positive/negative, which is not immediate. I can not come up with a counterexample to this, but this looks like it should need continuously differentiable, as implying that is already always positive/negative seems like an intermediate-value kind off argument, and I wonder if you can walve this additional condition, when we have a connected interval. With other words: How messy can a derivative really look? Thanks in advance.","f: I\to\mathbb{R} I\subseteq\mathbb{R} f'(x)\neq 0 x\in I f:\mathbb{R}\setminus\{0\}\to\mathbb{R}, x\mapsto 1/x a,b\in I a<b f(a)<f(b) f(a)>f(b) \frac{f(a)-f(b)}{a-b}=f'(\xi)\neq 0 f(a)-f(b) a,b\in I f'(x)\neq 0 f'(x)","['real-analysis', 'derivatives']"
59,Derivative of area integral w.r.t. parameter (Stokes' theorem?),Derivative of area integral w.r.t. parameter (Stokes' theorem?),,"Let $$g\left(p\right) = \iint_{R\left(p\right)} f\left(x\right)dA,$$ where $f$ is some continuous function on $\mathbb{R}^d$ , and $R\left(p\right)$ is some subset of $\mathbb{R}^d$ parameterized by $p$ . To give some examples, we could have $R\left(p\right) = \left\{x:\|x\|\leq p\right\}$ , or perhaps an ellipse $\left\{x:x^\top B x\leq p\right\}$ , or a parameter that can be changed to translate the shape (like moving the unit sphere in some direction), etc., but the problem is not restricted to these examples; however, we can assume that $R$ changes ""smoothly"" with $p$ . Note that the function $f$ being integrated has no dependence on $p$ . The parameter only affects the set over which $f$ is being integrated. I would like to understand $\frac{dg}{dp}$ . It seems like this should be related to Stokes' theorem or similar results, because if I change $p$ to $p+\Delta p$ , most of the $x$ values in $R\left(p\right)$ will continue to be in the modified set; the change will only occur for $x$ around the boundary. So, the ""net"" change in $g$ should be some sort of integral over the boundary of $f$ combined with the ""direction"" in which the boundary is moving. But I am a bit lost as to how to formalize this -- the usual formulations of Stokes' theorem, Green's theorem etc. use a different setting. Would appreciate any pointers.","Let where is some continuous function on , and is some subset of parameterized by . To give some examples, we could have , or perhaps an ellipse , or a parameter that can be changed to translate the shape (like moving the unit sphere in some direction), etc., but the problem is not restricted to these examples; however, we can assume that changes ""smoothly"" with . Note that the function being integrated has no dependence on . The parameter only affects the set over which is being integrated. I would like to understand . It seems like this should be related to Stokes' theorem or similar results, because if I change to , most of the values in will continue to be in the modified set; the change will only occur for around the boundary. So, the ""net"" change in should be some sort of integral over the boundary of combined with the ""direction"" in which the boundary is moving. But I am a bit lost as to how to formalize this -- the usual formulations of Stokes' theorem, Green's theorem etc. use a different setting. Would appreciate any pointers.","g\left(p\right) = \iint_{R\left(p\right)} f\left(x\right)dA, f \mathbb{R}^d R\left(p\right) \mathbb{R}^d p R\left(p\right) = \left\{x:\|x\|\leq p\right\} \left\{x:x^\top B x\leq p\right\} R p f p f \frac{dg}{dp} p p+\Delta p x R\left(p\right) x g f","['derivatives', 'contour-integration', 'area', 'differential-forms', 'stokes-theorem']"
60,Is my understanding of derivatives correct?,Is my understanding of derivatives correct?,,"(Full disclosure: I'm a complete amateur at differentiation, and just trying to understand derivatives through this question) My explanation of derivatives: Consider the curve $y=x^3$ : (Let us only consider the 1st quadrant for now for simplicity) Now, what is the slope of this curve? Answer: it doesn't have a fixed slope like that of a straight line; its slope is constantly changing. Now, a better question is, where exactly does the slope change? I'll attempt to answer this question now: Consider this graph: Now, the graph of the above function does not have a fixed slope. It has 3 well-defined slopes in the regions OA, AB & BC. Now, where exactly do the slopes of the function change? Answer: it changes 2 times:  first, at point A before the start of segment A, and second at point B before the start of segment BC. Okay, good. Now, what if we made the regions OA, AB & BC smaller. Okay, that's fine; however, it won't be the same function anymore: Now, there are 6 regions with well-defined slopes, OA, AB, BC, CD, DE & EF, and the slope changes 5 times at the points A, B, C, D & E. Now, if we keep making the regions smaller and smaller, we will get the function that we started with, $y=x^3$ : Now, back to our original question, where exactly does the slope of the graph change? The answer is, at each of the infinitely many points of the graph! You can visualize this in this way: in our previous examples, there were a finite number of points where the slope changed. However, if we keep making the regions smaller and smaller, the number of regions and the number of times the slope changes will get bigger and bigger, and at one point, the regions will lose their 2-dimensionality: it will become one-dimensional, just a point, and we will get $y=x^3$ . But the slopes are still changing, so we can understand that the slopes are changing at every point of the curve! Where else is the slope changing?! Now, you might object that how can a point have a slope? You need two points to calculate a slope; you only have one point. Answer: you are right. A point in isolation can't have any slope; however, a point that is part of a graph can have a well-defined slope. Get this, if points can't have slopes, how is the slope of a curve changing at every point? The fact that the slope of the curve is changing at every point is proof that a point can have a slope.  Okay, now I will define what the slope of a point is. The slope of a point that is part of the graph of a function is the slope of the line that is tangent to the graph of the function at that point...(i) Now, how do we calculate this slope of the tangent line? Luckily, earlier mathematicians have worked this out for us: $$f'(x)=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}$$ $f'(x)$ is the slope of the line tangent to the graph of $f$ at $x$ . $f'(x)$ is also known commonly as the derivative of $f$ at $x$ : this is just another name for the slope of the tangent line. Questions: Is (i) a legitimate definition in mathematics? Is my explanation correct? Do you disagree with any parts of my explanation? Bonus question: (If you don't want to answer this, it's fine! However, if you do, it might be helpful for me.) Was my thinking process similar to that of the fathers of calculus? If not, what was their thinking process?","(Full disclosure: I'm a complete amateur at differentiation, and just trying to understand derivatives through this question) My explanation of derivatives: Consider the curve : (Let us only consider the 1st quadrant for now for simplicity) Now, what is the slope of this curve? Answer: it doesn't have a fixed slope like that of a straight line; its slope is constantly changing. Now, a better question is, where exactly does the slope change? I'll attempt to answer this question now: Consider this graph: Now, the graph of the above function does not have a fixed slope. It has 3 well-defined slopes in the regions OA, AB & BC. Now, where exactly do the slopes of the function change? Answer: it changes 2 times:  first, at point A before the start of segment A, and second at point B before the start of segment BC. Okay, good. Now, what if we made the regions OA, AB & BC smaller. Okay, that's fine; however, it won't be the same function anymore: Now, there are 6 regions with well-defined slopes, OA, AB, BC, CD, DE & EF, and the slope changes 5 times at the points A, B, C, D & E. Now, if we keep making the regions smaller and smaller, we will get the function that we started with, : Now, back to our original question, where exactly does the slope of the graph change? The answer is, at each of the infinitely many points of the graph! You can visualize this in this way: in our previous examples, there were a finite number of points where the slope changed. However, if we keep making the regions smaller and smaller, the number of regions and the number of times the slope changes will get bigger and bigger, and at one point, the regions will lose their 2-dimensionality: it will become one-dimensional, just a point, and we will get . But the slopes are still changing, so we can understand that the slopes are changing at every point of the curve! Where else is the slope changing?! Now, you might object that how can a point have a slope? You need two points to calculate a slope; you only have one point. Answer: you are right. A point in isolation can't have any slope; however, a point that is part of a graph can have a well-defined slope. Get this, if points can't have slopes, how is the slope of a curve changing at every point? The fact that the slope of the curve is changing at every point is proof that a point can have a slope.  Okay, now I will define what the slope of a point is. The slope of a point that is part of the graph of a function is the slope of the line that is tangent to the graph of the function at that point...(i) Now, how do we calculate this slope of the tangent line? Luckily, earlier mathematicians have worked this out for us: is the slope of the line tangent to the graph of at . is also known commonly as the derivative of at : this is just another name for the slope of the tangent line. Questions: Is (i) a legitimate definition in mathematics? Is my explanation correct? Do you disagree with any parts of my explanation? Bonus question: (If you don't want to answer this, it's fine! However, if you do, it might be helpful for me.) Was my thinking process similar to that of the fathers of calculus? If not, what was their thinking process?",y=x^3 y=x^3 y=x^3 f'(x)=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h} f'(x) f x f'(x) f x,"['calculus', 'derivatives']"
61,I'm stuck in derivatives for finding Curvature of radius I know the formula,I'm stuck in derivatives for finding Curvature of radius I know the formula,,How can we find the radius of curvature for this equation $$(x^2+y^2)^2=a^2(x^2-y^2)$$ Here I know the formula but I just want to find y' and y'' I'm stuck in it here is what know $$\delta=\frac{(1+y'^2)^\frac{3}{2}}{y''}$$ i got y' $$y'=\frac{x}{y}\frac{a-2\sqrt{(x^2-y^2)}}{a+2\sqrt{(x^2-y^2)}}$$,How can we find the radius of curvature for this equation Here I know the formula but I just want to find y' and y'' I'm stuck in it here is what know i got y',(x^2+y^2)^2=a^2(x^2-y^2) \delta=\frac{(1+y'^2)^\frac{3}{2}}{y''} y'=\frac{x}{y}\frac{a-2\sqrt{(x^2-y^2)}}{a+2\sqrt{(x^2-y^2)}},"['calculus', 'derivatives', 'curvature']"
62,Understanding a Detail in Proof of Chain Rule,Understanding a Detail in Proof of Chain Rule,,"I am currently studying from Courant's Differential and Integral Calculus, and I have been trying to understand his proof of the chain rule, which uses the idea that the derivative is the best linear approximation of the function at a point. To give context for Courant's notation, I will write out his definition of a compound function before giving his proof of the chain rule: Definition of compound function: Let $\phi (x)$ be a function which is differentiable in an interval $x \in [a, b]$ and assumes all values in the interval $\phi \in [\alpha, \beta]$ . Consider a second differentiable function $g(\phi)$ of the independent variable $\phi$ , in which the variable $\phi$ ranges over the interval from $\alpha$ to $\beta$ . We can now regard the function $g(\phi) = g \{ \phi (x) \} = f(x)$ as a function of x in the interval $x \in [a, b]$ . Proof of Chain Rule: For any arbitrary $\Delta x \neq 0$ and corresponding values $\Delta \phi$ and $\Delta g$ there exist two quantities $\epsilon$ and $\eta$ , tending to 0 with $\Delta x$ , such that $$\Delta g = g^{'}(\phi)\Delta \phi + \epsilon\Delta\phi, \\ \Delta \phi = \phi^{'}(x)\Delta x + \eta\Delta x;$$ we have only to calculate $\eta$ from the second equation and, where $\Delta \phi \neq 0$ , $\epsilon$ from the first equation, while if $\Delta \phi$ = 0, we put $\epsilon = 0$ . If in the first of these equations we now substitute the value of $\Delta \phi$ from the second equation, we obtain $$\Delta g = g^{'}(\phi)\phi^{'}(x)\Delta x + \{\eta g^{'}(\phi) + \epsilon\phi^{'}(x) + \epsilon\eta\}\Delta x$$ which can be rewritten as $$\frac {\Delta g}{\Delta x} = g^{'}(\phi)\phi^{'}(x) + \{\eta g^{'}(\phi) + \epsilon\phi^{'}(x) + \epsilon\eta\}.$$ In this equation, we can let $\Delta x$ tend to $0$ , and the bracket on the right tends to zero with $\Delta x$ . The left hand side of our equation has a limit $f^{'}(x)$ , and this limit is equal to the first term on the right hand side: $$f^{'}(x)=g^{'}(\phi)\phi^{'}(x)$$ The part I am confused about in Courant's proof is why he goes out of his way to state: we have only to calculate $\eta$ from the second equation and, where $\Delta \phi \neq 0$ , $\epsilon$ from the first equation, while if $\Delta \phi$ = 0, we put $\epsilon = 0$ Why do we have to set $\epsilon$ to equal $0$ when $\Delta \phi$ is $0$ ? Why can't we set $\epsilon$ to equal to 1? From my naive perspective, I can't see an issue of setting $\epsilon$ to 1 when $\Delta \phi$ is $0$ . From what I understand, the entire proof is only based on the condition that $g(\phi)$ and $\phi(x)$ are differentiable, which is what allows us to write the expressions for $\Delta g$ and $\Delta \phi$ and let us claim that there exist two quantities $\epsilon$ and $\eta$ that tend to $0$ with $\Delta x$ . Is there something I am misunderstanding about Courant's proof or is my interpretation of his proof correct?","I am currently studying from Courant's Differential and Integral Calculus, and I have been trying to understand his proof of the chain rule, which uses the idea that the derivative is the best linear approximation of the function at a point. To give context for Courant's notation, I will write out his definition of a compound function before giving his proof of the chain rule: Definition of compound function: Let be a function which is differentiable in an interval and assumes all values in the interval . Consider a second differentiable function of the independent variable , in which the variable ranges over the interval from to . We can now regard the function as a function of x in the interval . Proof of Chain Rule: For any arbitrary and corresponding values and there exist two quantities and , tending to 0 with , such that we have only to calculate from the second equation and, where , from the first equation, while if = 0, we put . If in the first of these equations we now substitute the value of from the second equation, we obtain which can be rewritten as In this equation, we can let tend to , and the bracket on the right tends to zero with . The left hand side of our equation has a limit , and this limit is equal to the first term on the right hand side: The part I am confused about in Courant's proof is why he goes out of his way to state: we have only to calculate from the second equation and, where , from the first equation, while if = 0, we put Why do we have to set to equal when is ? Why can't we set to equal to 1? From my naive perspective, I can't see an issue of setting to 1 when is . From what I understand, the entire proof is only based on the condition that and are differentiable, which is what allows us to write the expressions for and and let us claim that there exist two quantities and that tend to with . Is there something I am misunderstanding about Courant's proof or is my interpretation of his proof correct?","\phi (x) x \in [a, b] \phi \in [\alpha, \beta] g(\phi) \phi \phi \alpha \beta g(\phi) = g \{ \phi (x) \} = f(x) x \in [a, b] \Delta x \neq 0 \Delta \phi \Delta g \epsilon \eta \Delta x \Delta g = g^{'}(\phi)\Delta \phi + \epsilon\Delta\phi, \\ \Delta \phi = \phi^{'}(x)\Delta x + \eta\Delta x; \eta \Delta \phi \neq 0 \epsilon \Delta \phi \epsilon = 0 \Delta \phi \Delta g = g^{'}(\phi)\phi^{'}(x)\Delta x + \{\eta g^{'}(\phi) + \epsilon\phi^{'}(x) + \epsilon\eta\}\Delta x \frac {\Delta g}{\Delta x} = g^{'}(\phi)\phi^{'}(x) + \{\eta g^{'}(\phi) + \epsilon\phi^{'}(x) + \epsilon\eta\}. \Delta x 0 \Delta x f^{'}(x) f^{'}(x)=g^{'}(\phi)\phi^{'}(x) \eta \Delta \phi \neq 0 \epsilon \Delta \phi \epsilon = 0 \epsilon 0 \Delta \phi 0 \epsilon \epsilon \Delta \phi 0 g(\phi) \phi(x) \Delta g \Delta \phi \epsilon \eta 0 \Delta x","['real-analysis', 'calculus', 'derivatives']"
63,Does the derivative of a polynomial over an ordered ring behave like a rate of change?,Does the derivative of a polynomial over an ordered ring behave like a rate of change?,,"Suppose I have an ordered commutative ring $R$ . I can define the collection $P$ of polynomial functions defined on $R$ as functions of the form $f(r) = p_0 + p_1 r + p_2 r^2 + \cdots$ where $p_0, p_1, p_2, \cdots$ are elements of $R$ . The collection $P$ is closed under the formal derivative operation that maps $f(r) = p_0 + p_1 r + p_2 r^2 + \cdots$ to $f'(r) = p_1 + (p_2 + p_2) r + \cdots$ Does this derivative satisfy any ""rate of change"" properties that play nicely with the ordering on $R$ ? For example, consider the elements $r_1 \leq r_2$ in $R$ and suppose that for any $r \in R$ where $r_1 \leq r \leq r_2$ we have that $f'(r) \geq 0$ . Does this imply that $f(r_2) - f(r_1) \geq 0$ ? If not, what additional assumptions do I need? Obviously this works when $R$ is the reals, but I am curious what assumptions I can drop.","Suppose I have an ordered commutative ring . I can define the collection of polynomial functions defined on as functions of the form where are elements of . The collection is closed under the formal derivative operation that maps to Does this derivative satisfy any ""rate of change"" properties that play nicely with the ordering on ? For example, consider the elements in and suppose that for any where we have that . Does this imply that ? If not, what additional assumptions do I need? Obviously this works when is the reals, but I am curious what assumptions I can drop.","R P R f(r) = p_0 + p_1 r + p_2 r^2 + \cdots p_0, p_1, p_2, \cdots R P f(r) = p_0 + p_1 r + p_2 r^2 + \cdots f'(r) = p_1 + (p_2 + p_2) r + \cdots R r_1 \leq r_2 R r \in R r_1 \leq r \leq r_2 f'(r) \geq 0 f(r_2) - f(r_1) \geq 0 R","['derivatives', 'polynomials', 'ring-theory', 'ordered-rings']"
64,How to show the total number of real solutions of the equation $x^4-2x^2+4=n$ for each set of real values of $n$ using differentiation?,How to show the total number of real solutions of the equation  for each set of real values of  using differentiation?,x^4-2x^2+4=n n,"How to show the total number of real solutions of the equation $x^4-2x^2+4=n$ for each set of real values of $n$ using differentiation? Trying to solve this, I converted the given equation as the following function $f(x) = x^4-2x^2+4-n$ Then I solve the derivative which will help me determine the critical numbers for x. $f'(x) = 4x^3-4x = 4x(x^2-1)$ Hence, the critical numbers will be $x = 0, \pm 1$ This means that I can identify the relative extremums. The relative maximum will be $(0, 4-n)$ The relative minimums are $(\pm1, 3-n)$ But I don't know how can I use those relative extremums to show the following which I found using a graphing tool. $n < 3$ implies no solution $n = 3$ implies $2$ solutions $3 < n < 4$ implies $4$ solutions $n = 4$ implies $3$ solutions $n > 4$ implies $2$ solutions How can I use, the local extremums or other concepts under differentiation to find the above results? If you have strategies other than derivatives, please share them here. Please feel free to share your ideas, comments, and suggestions about this matter. Thank you in advance!","How to show the total number of real solutions of the equation for each set of real values of using differentiation? Trying to solve this, I converted the given equation as the following function Then I solve the derivative which will help me determine the critical numbers for x. Hence, the critical numbers will be This means that I can identify the relative extremums. The relative maximum will be The relative minimums are But I don't know how can I use those relative extremums to show the following which I found using a graphing tool. implies no solution implies solutions implies solutions implies solutions implies solutions How can I use, the local extremums or other concepts under differentiation to find the above results? If you have strategies other than derivatives, please share them here. Please feel free to share your ideas, comments, and suggestions about this matter. Thank you in advance!","x^4-2x^2+4=n n f(x) = x^4-2x^2+4-n f'(x) = 4x^3-4x = 4x(x^2-1) x = 0, \pm 1 (0, 4-n) (\pm1, 3-n) n < 3 n = 3 2 3 < n < 4 4 n = 4 3 n > 4 2","['derivatives', 'solution-verification']"
65,Covariant derivative and $\nabla X$,Covariant derivative and,\nabla X,"If I consider the covariant derivative of a vector field $Y$ with respect to $X$ this is usually indicated as $\nabla_X Y$ , but what's about $\nabla X$ ? This is the covariant derivative of $X$ and I have read it is  tensor field of type (1,1) such that if $Y\in \chi(\cal{M})$ (vector field) and $w\in \chi^*(\cal{M})$ (covector) $$\nabla X(Y,w)=w(\nabla_XY)$$ Why it is true? I only know that given a smooth function $f$ then $\nabla_{f X} Y=f\nabla_X Y$ , but really I can't understand the relationship between $\nabla X$ and $\nabla_X Y$ and so why the expression above holds. Can you help me please?","If I consider the covariant derivative of a vector field with respect to this is usually indicated as , but what's about ? This is the covariant derivative of and I have read it is  tensor field of type (1,1) such that if (vector field) and (covector) Why it is true? I only know that given a smooth function then , but really I can't understand the relationship between and and so why the expression above holds. Can you help me please?","Y X \nabla_X Y \nabla X X Y\in \chi(\cal{M}) w\in \chi^*(\cal{M}) \nabla X(Y,w)=w(\nabla_XY) f \nabla_{f X} Y=f\nabla_X Y \nabla X \nabla_X Y","['derivatives', 'differential-geometry', 'vectors', 'tensors']"
66,Does convexity of $f(x)$ imply convexity of $f(e^x)$?,Does convexity of  imply convexity of ?,f(x) f(e^x),"Let $0<\epsilon<1$ , and let $f:[1-\epsilon,1] \to \mathbb{R}$ be a strictly decreasing, continuous, and strictly convex function. Suppose that $f_-'(1)=0$ . Define $g:[\log(1-\epsilon),0] \to \mathbb{R}$ by setting $g(x)=f(e^x)$ . Is $g$ convex on some half-neighbourhood of $0$ ? I specifically don't want to assume stronger differentiability assumptions . The differentiability assumptions do matter here: If we assume $f,g \in C^2$ and $f''(1)>0$ , then $g''(0)=f''(1)>0$ and $g$ is convex. If we remove the assumption $f_-'(1)=0$ , $g$ might be non-convex. Take $f(x)=1-x$ . Then $g(x)=1-e^x$ , so $g''(x)=-e^x<0$ .","Let , and let be a strictly decreasing, continuous, and strictly convex function. Suppose that . Define by setting . Is convex on some half-neighbourhood of ? I specifically don't want to assume stronger differentiability assumptions . The differentiability assumptions do matter here: If we assume and , then and is convex. If we remove the assumption , might be non-convex. Take . Then , so .","0<\epsilon<1 f:[1-\epsilon,1] \to \mathbb{R} f_-'(1)=0 g:[\log(1-\epsilon),0] \to \mathbb{R} g(x)=f(e^x) g 0 f,g \in C^2 f''(1)>0 g''(0)=f''(1)>0 g f_-'(1)=0 g f(x)=1-x g(x)=1-e^x g''(x)=-e^x<0","['real-analysis', 'calculus', 'derivatives', 'convex-analysis', 'examples-counterexamples']"
67,Why the chain rule does not work for this question?,Why the chain rule does not work for this question?,,"$f(x)=x^{14x}$ Find $f'(x)$ I used the chain rule and wrote it as $f(U)=U^{14x},U(x)=x $ , and get an answer : $14x(x)^{14x-1}$ But it is wrong .The right answer should be make $y=x^{14x}$ then $\ln y=\ln x^{14x}$ then $\ln y=14x\ln x$ then differentiate each side with respect to $x$ . Can anyone explain why my method is wrong ?","Find I used the chain rule and wrote it as , and get an answer : But it is wrong .The right answer should be make then then then differentiate each side with respect to . Can anyone explain why my method is wrong ?","f(x)=x^{14x} f'(x) f(U)=U^{14x},U(x)=x  14x(x)^{14x-1} y=x^{14x} \ln y=\ln x^{14x} \ln y=14x\ln x x","['calculus', 'derivatives', 'logarithms', 'chain-rule']"
68,"Function $f$ such that $|f(x)-f(y)|\leq \sqrt {|x-y|}, \forall x,y\in\Bbb R.$",Function  such that,"f |f(x)-f(y)|\leq \sqrt {|x-y|}, \forall x,y\in\Bbb R.","Let $f$ be a real function such that  such that $|f(x)-f(y)|\leq \sqrt {|x-y|}, \forall x,y\in\Bbb R.$ Does this condition imply that $f$ will be differentiable ? If Lipschitz order is greater than $1$ then function is constant so differentiable . If Lipschitz order is equal to $1$ then $ |x|$ is counterexample for differentiablity . For this question I have no idea . Please suggest me a counterexample or proof of differentiablity hint . Thank you.",Let be a real function such that  such that Does this condition imply that will be differentiable ? If Lipschitz order is greater than then function is constant so differentiable . If Lipschitz order is equal to then is counterexample for differentiablity . For this question I have no idea . Please suggest me a counterexample or proof of differentiablity hint . Thank you.,"f |f(x)-f(y)|\leq \sqrt {|x-y|}, \forall x,y\in\Bbb R. f 1 1  |x|","['real-analysis', 'derivatives', 'lipschitz-functions', 'holder-spaces']"
69,Proof of $k$th derivatives always being integers,Proof of th derivatives always being integers,k,Consider the function $f(x)$ defined such that $$f(x)=\frac{x^n(1-x)^n}{n!}$$ Then prove that the $k$ th derivatives $f^{(k)}(0)$ and $f^{(k)}(1)$ are always integers. Here $n$ and $k$ are integers and $n\ge1$ and $k\ge 0$ I used binomial theorem to prove that the result holds true for $x=0$ for all $k\le n$ which was trivial. But I'm not able to prove it for the general case as such. I also tried using induction..but that didn't work as well. Any research I do to find the answer online leads me to the proof of the irrationality of $\pi$ or $e^n$ where the above statement is taken as a starting point without the proof. Thanks for any answers!!,Consider the function defined such that Then prove that the th derivatives and are always integers. Here and are integers and and I used binomial theorem to prove that the result holds true for for all which was trivial. But I'm not able to prove it for the general case as such. I also tried using induction..but that didn't work as well. Any research I do to find the answer online leads me to the proof of the irrationality of or where the above statement is taken as a starting point without the proof. Thanks for any answers!!,f(x) f(x)=\frac{x^n(1-x)^n}{n!} k f^{(k)}(0) f^{(k)}(1) n k n\ge1 k\ge 0 x=0 k\le n \pi e^n,"['calculus', 'number-theory', 'elementary-number-theory', 'derivatives']"
70,"Proving that $f(x)=x\arccos\left(1-p+p\cos\left(\frac{2\pi}{x}\right)\right)$ is increasing on $[2,\infty)$ with $0<p<1$",Proving that  is increasing on  with,"f(x)=x\arccos\left(1-p+p\cos\left(\frac{2\pi}{x}\right)\right) [2,\infty) 0<p<1","The function $$f(x)=x\arccos\left(1-p+p\cos\left(\frac{2\pi}{x}\right)\right)$$ on $[2,\infty)$ with $0<p<1$ is clearly increasing by looking at its graph. I calculated its derivative to be $$f'(x)=\arccos\left(1-p+p\cos\left(\frac{2\pi}{x}\right)\right)-\frac{2\pi p}{x}\frac{\sin\left(\frac{2\pi}{x}\right)}{\sqrt{1-\left(1-p+p\cos\left(\frac{2\pi}{x}\right)\right)^{2}}}$$ but couldn't prove that it is positive on the interval. I tried to prove the equivalent result $$\frac{x}{x+h} > \frac{\arccos\left(1-p+p\cos\left(\frac{2\pi}{x+h}\right)\right)}{\arccos\left(1-p+p\cos\left(\frac{2\pi}{x}\right)\right)} $$ for all $x\ge2$ and $h>0$ but couldn't deal with the $\arccos$ terms. I also calculated the limit of $f(x)$ at infinity to be $2\pi \sqrt{p}$ if this can help somehow.",The function on with is clearly increasing by looking at its graph. I calculated its derivative to be but couldn't prove that it is positive on the interval. I tried to prove the equivalent result for all and but couldn't deal with the terms. I also calculated the limit of at infinity to be if this can help somehow.,"f(x)=x\arccos\left(1-p+p\cos\left(\frac{2\pi}{x}\right)\right) [2,\infty) 0<p<1 f'(x)=\arccos\left(1-p+p\cos\left(\frac{2\pi}{x}\right)\right)-\frac{2\pi p}{x}\frac{\sin\left(\frac{2\pi}{x}\right)}{\sqrt{1-\left(1-p+p\cos\left(\frac{2\pi}{x}\right)\right)^{2}}} \frac{x}{x+h} > \frac{\arccos\left(1-p+p\cos\left(\frac{2\pi}{x+h}\right)\right)}{\arccos\left(1-p+p\cos\left(\frac{2\pi}{x}\right)\right)}  x\ge2 h>0 \arccos f(x) 2\pi \sqrt{p}","['calculus', 'derivatives', 'monotone-functions']"
71,"Prove that there exists a line passing through M$(\alpha,\beta)$ which is tangent to the graph of $f$.",Prove that there exists a line passing through M which is tangent to the graph of .,"(\alpha,\beta) f","Let $f:[a,b]\rightarrow\Bbb{R}$ be a function continuous on $[a,b]$ and differentiable on $(a,b)$ . Let M $(\alpha,\beta)$ be a point on the line passing through the points $(a,f(a))$ and $(b,f(b))$ with $\alpha\notin [a,b]$ . Prove that there exists a line passing through M $(\alpha,\beta)$ which is tangent to the graph of $f$ . My Attempt: I think this is an applicaton of Rolle's or Lagrange's Mean Value theorems but I am not able to frame the proper function",Let be a function continuous on and differentiable on . Let M be a point on the line passing through the points and with . Prove that there exists a line passing through M which is tangent to the graph of . My Attempt: I think this is an applicaton of Rolle's or Lagrange's Mean Value theorems but I am not able to frame the proper function,"f:[a,b]\rightarrow\Bbb{R} [a,b] (a,b) (\alpha,\beta) (a,f(a)) (b,f(b)) \alpha\notin [a,b] (\alpha,\beta) f","['real-analysis', 'calculus', 'derivatives', 'tangent-line', 'rolles-theorem']"
72,Differentiability of $f(|x|)$ at zero,Differentiability of  at zero,f(|x|),"This is a self-answered question. I post it here since it wasn't obvious to me. Let $f:[0,\infty) \to \mathbb R $ be a function. Then the function $x \to f(|x|)$ is $k$ -times differentiable at zero if and only if $f$ is $k$ -times differentiable at zero and all the derivatives of $f$ of odd order up to $k$ vanish. I proved it via (a bit strange) ""coupled"" induction. Of course, any alternative proofs would be welcomed. (I feel that there might be a shorter way to phrase this coupling). To see that the vanishing of the odd order derivatives is necessary, consider the following: $x \to f(|x|)$ is an even function, so all its odd-order derivatives vanish at zero. However, for non-negative $x$ this map coincides with our original $f(x)$ , so its (odd) derivatives should be zero.","This is a self-answered question. I post it here since it wasn't obvious to me. Let be a function. Then the function is -times differentiable at zero if and only if is -times differentiable at zero and all the derivatives of of odd order up to vanish. I proved it via (a bit strange) ""coupled"" induction. Of course, any alternative proofs would be welcomed. (I feel that there might be a shorter way to phrase this coupling). To see that the vanishing of the odd order derivatives is necessary, consider the following: is an even function, so all its odd-order derivatives vanish at zero. However, for non-negative this map coincides with our original , so its (odd) derivatives should be zero.","f:[0,\infty) \to \mathbb R  x \to f(|x|) k f k f k x \to f(|x|) x f(x)","['real-analysis', 'calculus', 'derivatives']"
73,$h \in C^k$ implies $\frac{h(x)-h(0)}{x} \in C^{k-1}$,implies,h \in C^k \frac{h(x)-h(0)}{x} \in C^{k-1},"This is a self-answered question. I post it here since it wasn't obvious to me. (although I have seen similar questions-is it an exact duplicate?). Let $h:\mathbb{R}\to\mathbb{R}$ be $C^k$ and suppose that $h(0)=0$ . Define $$ F(x) = \begin{cases} \frac{h(x)}{x} & \text{if $x\neq 0$} \\ h'(0) & \text{if $x=0$}\end{cases} $$ Then $F$ is $C^{k-1}$ and $F^{(m)}(0)=\frac{h^{(m+1)}(0)}{m+1}$ for every $m \le k-1$ . Is there a way to deduce the equality $F^{(m)}(0)=\frac{h^{(m+1)}(0)}{m+1}$ without a long calculation via L'Hôpital's rule? Perhaps using approximation by Taylor's polynomials? I tried to that, but naive attempt didn't make it. (By inspecting the Taylor's polynomials, it is immediate that if $F \in C^{k-1}$ , then $F^{(m)}(0)=\frac{h^{(m+1)}(0)}{m+1}$ holds, but this does not imply the $C^{k-1}$ differentiability to begin with). Added: This result is sharp. Indeed $h(x)=x^{k+1}\text{sgn}(x)$ is $C^k$ , but $F(x)=x^{k}\text{sgn}(x)$ is only $C^{k-1}$ , and is not differentiable $k$ times at zero.","This is a self-answered question. I post it here since it wasn't obvious to me. (although I have seen similar questions-is it an exact duplicate?). Let be and suppose that . Define Then is and for every . Is there a way to deduce the equality without a long calculation via L'Hôpital's rule? Perhaps using approximation by Taylor's polynomials? I tried to that, but naive attempt didn't make it. (By inspecting the Taylor's polynomials, it is immediate that if , then holds, but this does not imply the differentiability to begin with). Added: This result is sharp. Indeed is , but is only , and is not differentiable times at zero.","h:\mathbb{R}\to\mathbb{R} C^k h(0)=0 
F(x) = \begin{cases} \frac{h(x)}{x} & \text{if x\neq 0} \\ h'(0) & \text{if x=0}\end{cases}
 F C^{k-1} F^{(m)}(0)=\frac{h^{(m+1)}(0)}{m+1} m \le k-1 F^{(m)}(0)=\frac{h^{(m+1)}(0)}{m+1} F \in C^{k-1} F^{(m)}(0)=\frac{h^{(m+1)}(0)}{m+1} C^{k-1} h(x)=x^{k+1}\text{sgn}(x) C^k F(x)=x^{k}\text{sgn}(x) C^{k-1} k","['real-analysis', 'calculus', 'derivatives']"
74,How to calculate using derivative definition,How to calculate using derivative definition,,"I am trying to find the derivative of $f$ at $x=1$ when $y=\sqrt{3x+1}$ using only the following derivative definition: $$f^\prime(1)=\lim_{x\to1}\frac{f(x)−f(1)}{x−1}$$ I think I need to get rid of the radical first, but I simply cannot find the right way. I am relearning math after many years, so please bear with me.","I am trying to find the derivative of at when using only the following derivative definition: I think I need to get rid of the radical first, but I simply cannot find the right way. I am relearning math after many years, so please bear with me.",f x=1 y=\sqrt{3x+1} f^\prime(1)=\lim_{x\to1}\frac{f(x)−f(1)}{x−1},"['derivatives', 'radicals']"
75,"$f(x,y) = \frac{x^3y}{x^4 + y^2}$ is not differentiable at $(0,0)$.",is not differentiable at .,"f(x,y) = \frac{x^3y}{x^4 + y^2} (0,0)","The directional derivative of $f: U \to \Bbb{R}^m$ at $p \in U$ in the direction $u$ is the limit, if it exists, $$\nabla_p f(u) = \lim_{t\to 0}\frac{f(p + tu) - f(p)}{t}.$$ (Often one requires that $|u| = 1$ .)} (a) If $f$ is differentiable at $p$ , why is it obvious that the directional derivative exists in each direction $u$ ?} (b) Show that the function $f : \Bbb{R}^2 \to \Bbb{R}$ defined by $$f(x,y) = \begin{cases} \frac{x^3y}{x^4 + y^2},& (x,y) \neq (0,0)\\ 0,& (x,y) = (0,0) \end{cases}.$$ has $\nabla_{(0,0)}f(u) = 0$ for all $u$ but is not differentiable at $(0, 0)$ . My only question about that problem is about the differentiability of $f$ . I cannot see how to prove that $f$ is not differentiable. I tried to show that the partials derivatives is not continuous, show that $\frac{f(x,y)}{|(x,y)|} \not\to (0,0)$ as $(x,y) \to (0,0)$ , but nothing worked. I appreciate any hints.","The directional derivative of at in the direction is the limit, if it exists, (Often one requires that .)} (a) If is differentiable at , why is it obvious that the directional derivative exists in each direction ?} (b) Show that the function defined by has for all but is not differentiable at . My only question about that problem is about the differentiability of . I cannot see how to prove that is not differentiable. I tried to show that the partials derivatives is not continuous, show that as , but nothing worked. I appreciate any hints.","f: U \to \Bbb{R}^m p \in U u \nabla_p f(u) = \lim_{t\to 0}\frac{f(p + tu) - f(p)}{t}. |u| = 1 f p u f : \Bbb{R}^2 \to \Bbb{R} f(x,y) = \begin{cases}
\frac{x^3y}{x^4 + y^2},& (x,y) \neq (0,0)\\
0,& (x,y) = (0,0)
\end{cases}. \nabla_{(0,0)}f(u) = 0 u (0, 0) f f \frac{f(x,y)}{|(x,y)|} \not\to (0,0) (x,y) \to (0,0)","['real-analysis', 'derivatives', 'frechet-derivative']"
76,"Find derivative $\frac{dy}{dx}$, given $y(x)=\sin^{-1}\left(\frac{5\sin x+4\cos x}{\sqrt{41}}\right)$","Find derivative , given",\frac{dy}{dx} y(x)=\sin^{-1}\left(\frac{5\sin x+4\cos x}{\sqrt{41}}\right),"Find $\dfrac{dy}{dx}$ if $y=\sin^{-1}\bigg[\dfrac{5\sin x+4\cos x}{\sqrt{41}}\bigg]$ My Attempt Put $\cos\theta=5/\sqrt{41}\implies\sin\theta=4/\sqrt{41}$ $$ y=\sin^{-1}\big[\sin(x+\theta)\big]\implies\sin y=\sin(x+\theta)\\ y=n\pi+(-1)^n(x+\theta)\\ \boxed{\frac{dy}{dx}=(-1)^n} $$ But my reference gives the solution $y'=1$ , am I missing something here ?","Find if My Attempt Put But my reference gives the solution , am I missing something here ?","\dfrac{dy}{dx} y=\sin^{-1}\bigg[\dfrac{5\sin x+4\cos x}{\sqrt{41}}\bigg] \cos\theta=5/\sqrt{41}\implies\sin\theta=4/\sqrt{41} 
y=\sin^{-1}\big[\sin(x+\theta)\big]\implies\sin y=\sin(x+\theta)\\
y=n\pi+(-1)^n(x+\theta)\\
\boxed{\frac{dy}{dx}=(-1)^n}
 y'=1","['derivatives', 'trigonometry', 'inverse-function']"
77,"Is there a better/faster way to take anti-derivatives of simple functions than ""reversing"" the derivative rules?","Is there a better/faster way to take anti-derivatives of simple functions than ""reversing"" the derivative rules?",,"Update : For what it's worth, I will wait another few hours to see if anyone else has a more comprehensive answer to my question. But if not, I will ""accept"" one of the two extant answers, both of which are very good although not quite as comprehensive as I had hoped. I am taking AP Calculus AB right now and we are learning about anti-derivatives (indefinite integrals) for Unit II. Before that, we learned some basic derivative rules for transformed functions, such as: $ [f (x + a)]]' = f'(x + a)$ $[f(ax)]'=a*f'(ax)$ (We also learned the derivatives of some elementary functions, e.g. polynomials, exponential functions, sine, and cosine.) For anti-derivatives we have likewise memorized (and proved) formulas for some basic functions, but unlike with derivatives we have not been taught very much at all about what to do with transformed functions. Consider, say, finding the anti-derivative $F(x)$ if $f(x)=1/(4x)$ . Or finding the anti-derivative $G(x)$ if $g(x)=\cos(4x)$ . (Or even worse, how about if $f(x)$ was actually $1/(4x-3)$ , and $g(x)$ was actually $\cos(4x-3)$ ?) I am not entirely sure how to systematically and carefully go about solving such problems. Should I try to learn integral u -substitution or any tricks like that (which we haven't covered in class yet), or am I better off just trying to intuitively ""reverse"" the differentiation rules as best I can? I want to figure out a relatively efficient method of integrating basic functions but right now am pretty confused. (Often attempting to reverse the differentiation rules kinda gives me a headache haha and I get utterly lost because it's hard to think about things backwards.)","Update : For what it's worth, I will wait another few hours to see if anyone else has a more comprehensive answer to my question. But if not, I will ""accept"" one of the two extant answers, both of which are very good although not quite as comprehensive as I had hoped. I am taking AP Calculus AB right now and we are learning about anti-derivatives (indefinite integrals) for Unit II. Before that, we learned some basic derivative rules for transformed functions, such as: (We also learned the derivatives of some elementary functions, e.g. polynomials, exponential functions, sine, and cosine.) For anti-derivatives we have likewise memorized (and proved) formulas for some basic functions, but unlike with derivatives we have not been taught very much at all about what to do with transformed functions. Consider, say, finding the anti-derivative if . Or finding the anti-derivative if . (Or even worse, how about if was actually , and was actually ?) I am not entirely sure how to systematically and carefully go about solving such problems. Should I try to learn integral u -substitution or any tricks like that (which we haven't covered in class yet), or am I better off just trying to intuitively ""reverse"" the differentiation rules as best I can? I want to figure out a relatively efficient method of integrating basic functions but right now am pretty confused. (Often attempting to reverse the differentiation rules kinda gives me a headache haha and I get utterly lost because it's hard to think about things backwards.)", [f (x + a)]]' = f'(x + a) [f(ax)]'=a*f'(ax) F(x) f(x)=1/(4x) G(x) g(x)=\cos(4x) f(x) 1/(4x-3) g(x) \cos(4x-3),"['calculus', 'integration', 'derivatives', 'indefinite-integrals', 'advice']"
78,Baby Rudin 5.26,Baby Rudin 5.26,,"Could someone help me out with the solution written in Roger Cooke's solutions for Baby Rudin 5.26? https://minds.wisconsin.edu/bitstream/handle/1793/67009/rudin%20ch%205.pdf?sequence=7&isAllowed=y The question is, Suppose $f$ is differentiable on $[a,b]$ , $f(a)=0$ and there is a real   number $A$ such that $|f'(x)| \le A|f(x)|$ on $[a,b]$ . Prove that $f(x)=0$ for all $x\in [a,b]$ . The solution says in its last paragraph, that But by definition of $M_0$ , this implies $M_0 \le \frac{M_0}{2}$ , so that $M_0\le 0$ ,  i.e. $M_0=0$ . I'm interpreting this as $M_0(x_o-a)A\ge M_0$ , is assured if $x_0 \equiv a+\frac{1}{2A}$ . But if $M_0$ is extremely large and $(x_0-a)$ is not so small, would this inequality still hold? More formally, Are $|f(x)| \le M_0$ and $|f(x)| \le M_0 (x_0-a) A$ , enough conditions for deriving $M_0(x_0-a)A\ge M_0$ ?","Could someone help me out with the solution written in Roger Cooke's solutions for Baby Rudin 5.26? https://minds.wisconsin.edu/bitstream/handle/1793/67009/rudin%20ch%205.pdf?sequence=7&isAllowed=y The question is, Suppose is differentiable on , and there is a real   number such that on . Prove that for all . The solution says in its last paragraph, that But by definition of , this implies , so that ,  i.e. . I'm interpreting this as , is assured if . But if is extremely large and is not so small, would this inequality still hold? More formally, Are and , enough conditions for deriving ?","f [a,b] f(a)=0 A |f'(x)| \le A|f(x)| [a,b] f(x)=0 x\in [a,b] M_0 M_0 \le \frac{M_0}{2} M_0\le 0 M_0=0 M_0(x_o-a)A\ge M_0 x_0 \equiv a+\frac{1}{2A} M_0 (x_0-a) |f(x)| \le M_0 |f(x)| \le M_0 (x_0-a) A M_0(x_0-a)A\ge M_0","['derivatives', 'proof-explanation']"
79,What Constitutes a Decreasing Interval?,What Constitutes a Decreasing Interval?,,"Problem: For what values of $x$ is $f(x)=x^4-4x^3$ increasing? decreasing? The first half of the answer in the book is: if $x<3$ , $f'(x)\leq 0$ and $f$ is decreasing There is also a note: Note that $f$ is decreasing at $x=0$ even though $f'(0)=0$ If $f'(0)=0$ , then $f$ at $x=0$ is neither increasing nor decreasing. So why is the book saying what it's saying?","Problem: For what values of is increasing? decreasing? The first half of the answer in the book is: if , and is decreasing There is also a note: Note that is decreasing at even though If , then at is neither increasing nor decreasing. So why is the book saying what it's saying?",x f(x)=x^4-4x^3 x<3 f'(x)\leq 0 f f x=0 f'(0)=0 f'(0)=0 f x=0,['calculus']
80,"Find $f$ if $f'(t) = 3^t - \frac{3}{t},$ $f(1) = 2,$ and $f(-1) = 1$.",Find  if   and .,"f f'(t) = 3^t - \frac{3}{t}, f(1) = 2, f(-1) = 1","This is problem #38 from section 4.9 of Stewarts Calculus Early Transcendentals 8th edition. I know how these problems are typically solved, but I don't know why in this problem there are two initial conditions on $f$ that seem to contradict each other. Here it is in the book to verify: Here's my work: $$f(t) = \frac{3^t}{\ln(3)} - 3\ln|t| + C.$$ Since $f(1) = 2,$ then $$2 = \frac{3}{\ln(3)} - 3\ln|1| + C,$$ so $$C = 2 - \frac{3}{\ln(3)} \approx -0.73072.$$ However, since $f(-1) = 1$ , then $$1 = \frac{3^{-1}}{\ln(3)} - 3\ln|-1| + C  = \frac{1}{3\ln(3)} - 3\ln(1) + C,$$ so $$C = 1 - \frac{1}{3\ln(3)} \approx 0.69657.$$ My initial thought was that it had something to do with natural log, but the antiderivative should have an absolute value, which should make it okay to plug in negative values. So what's going on here?","This is problem #38 from section 4.9 of Stewarts Calculus Early Transcendentals 8th edition. I know how these problems are typically solved, but I don't know why in this problem there are two initial conditions on that seem to contradict each other. Here it is in the book to verify: Here's my work: Since then so However, since , then so My initial thought was that it had something to do with natural log, but the antiderivative should have an absolute value, which should make it okay to plug in negative values. So what's going on here?","f f(t) = \frac{3^t}{\ln(3)} - 3\ln|t| + C. f(1) = 2, 2 = \frac{3}{\ln(3)} - 3\ln|1| + C, C = 2 - \frac{3}{\ln(3)} \approx -0.73072. f(-1) = 1 1 = \frac{3^{-1}}{\ln(3)} - 3\ln|-1| + C  = \frac{1}{3\ln(3)} - 3\ln(1) + C, C = 1 - \frac{1}{3\ln(3)} \approx 0.69657.","['calculus', 'integration']"
81,Find $9$'th derivative of $\frac{x^3 e^{2x^2}}{(1-x^2)^2}$,Find 'th derivative of,9 \frac{x^3 e^{2x^2}}{(1-x^2)^2},"How can I find $9$ 'th derivative at $0$ of $\displaystyle \frac{x^3 e^{2x^2}}{(1-x^2)^2}$ . Is there any tricky way to do that? This exercise comes from discrete mathematic's exam, so I think that tools like taylor  can't be used there.","How can I find 'th derivative at of . Is there any tricky way to do that? This exercise comes from discrete mathematic's exam, so I think that tools like taylor  can't be used there.",9 0 \displaystyle \frac{x^3 e^{2x^2}}{(1-x^2)^2},['discrete-mathematics']
82,Multi-variable Chain Rule in Quasi-linear PDEs,Multi-variable Chain Rule in Quasi-linear PDEs,,"In trying to understand the general solution of a first-order quasilinear PDE of the form $$a(x,y,z) \frac{\partial z}{\partial x}+b(x,y,z) \frac{\partial z}{\partial y}=c(x,y,z) $$ What approach is taken to differentiate $F(u,v)$ ? When I try to apply the chain rule I am finding $$\frac{dF}{dx} = \frac{\partial F}{\partial u}\frac{\partial u}{\partial x}+\frac{\partial F}{\partial v}\frac{\partial v}{\partial x}+\frac{\partial F}{\partial z}\frac{\partial z}{\partial x}$$ which is clearly not in agreement with the book derivation. Also, shouldn't the term $\frac{\partial z}{\partial x} = 0$ as z is not dependent on either x nor y?","In trying to understand the general solution of a first-order quasilinear PDE of the form What approach is taken to differentiate ? When I try to apply the chain rule I am finding which is clearly not in agreement with the book derivation. Also, shouldn't the term as z is not dependent on either x nor y?","a(x,y,z) \frac{\partial z}{\partial x}+b(x,y,z) \frac{\partial z}{\partial y}=c(x,y,z)  F(u,v) \frac{dF}{dx} = \frac{\partial F}{\partial u}\frac{\partial u}{\partial x}+\frac{\partial F}{\partial v}\frac{\partial v}{\partial x}+\frac{\partial F}{\partial z}\frac{\partial z}{\partial x} \frac{\partial z}{\partial x} = 0","['derivatives', 'differential-geometry', 'partial-differential-equations']"
83,Implicit derivative: why do we keep the $\frac{dy}{dx}$?,Implicit derivative: why do we keep the ?,\frac{dy}{dx},I just started learning calculus and I'm studying implicit derivatives and I have a question regarding the differenciation of the y variable. I'll use an example: Applying implicit derivative to $5y^2 = x^2$ $ \frac{d}{dx} (5y^2) = \frac{d}{dx} (x^2) $ $10y \frac{dy}{dx} = 2x$ Why do we keep the $ \frac{dy}{dx} $ after differentiating $5y^2$ ? The book I'm following does not explain the reason for this. Thanks in advance.,I just started learning calculus and I'm studying implicit derivatives and I have a question regarding the differenciation of the y variable. I'll use an example: Applying implicit derivative to Why do we keep the after differentiating ? The book I'm following does not explain the reason for this. Thanks in advance.,5y^2 = x^2  \frac{d}{dx} (5y^2) = \frac{d}{dx} (x^2)  10y \frac{dy}{dx} = 2x  \frac{dy}{dx}  5y^2,"['calculus', 'derivatives']"
84,Differentiate $e^{y/x} = 20x-y$,Differentiate,e^{y/x} = 20x-y,"I am trying to use implicit differentiation to differentiate $e^{y/x} = 20x-y$ . I get $\frac{20}{2 e^{y/x} \cdot \frac{x-y}{x^2}}$ , but according to the math website I'm using, ""WebWork"", this is wrong. I'm not sure how to handle it when an equation has two $\frac{dy}{dx}$ floating around, but I'm not sure this is the problem. Here are my steps: $$\frac{d}{dx}(e^{y/x}) = \frac{d}{dx}(20x-y)$$ Factoring both sides independently: $$\frac{d}{dx}(e^{y/x}) = e^{y/x}\cdot\frac{x-y}{x^2} \frac{dy}{dx}$$ $$\frac{d}{dx}(20x-y)=20-\frac{dy}{dx}$$ Finding $\frac{dy}{dx}$ $$e^{y/x} \cdot \frac{x-y}{x^2} \frac{dy}{dx} = 20 - \frac{dy}{dx}$$ $$\frac{dy}{dx}+\frac{x-y}{x^2} \frac{dy}{dx} = \frac{20}{e^{y/x}}$$ $$\frac{dy}{dx} + \frac{dy}{dx} = \frac{20}{e^{y/x}\cdot\frac{x-y}{x^2}}$$ $$2\frac{dy}{dx} = \frac{20}{e^{y/x}\cdot\frac{x-y}{x^2}}$$ $$\frac{dy}{dx} = \frac{20}{2e^{y/x}\cdot\frac{x-y}{x^2}}$$","I am trying to use implicit differentiation to differentiate . I get , but according to the math website I'm using, ""WebWork"", this is wrong. I'm not sure how to handle it when an equation has two floating around, but I'm not sure this is the problem. Here are my steps: Factoring both sides independently: Finding",e^{y/x} = 20x-y \frac{20}{2 e^{y/x} \cdot \frac{x-y}{x^2}} \frac{dy}{dx} \frac{d}{dx}(e^{y/x}) = \frac{d}{dx}(20x-y) \frac{d}{dx}(e^{y/x}) = e^{y/x}\cdot\frac{x-y}{x^2} \frac{dy}{dx} \frac{d}{dx}(20x-y)=20-\frac{dy}{dx} \frac{dy}{dx} e^{y/x} \cdot \frac{x-y}{x^2} \frac{dy}{dx} = 20 - \frac{dy}{dx} \frac{dy}{dx}+\frac{x-y}{x^2} \frac{dy}{dx} = \frac{20}{e^{y/x}} \frac{dy}{dx} + \frac{dy}{dx} = \frac{20}{e^{y/x}\cdot\frac{x-y}{x^2}} 2\frac{dy}{dx} = \frac{20}{e^{y/x}\cdot\frac{x-y}{x^2}} \frac{dy}{dx} = \frac{20}{2e^{y/x}\cdot\frac{x-y}{x^2}},"['calculus', 'derivatives', 'implicit-differentiation']"
85,Getting different answers when using product rule and limit substitution than I do with quotient rule,Getting different answers when using product rule and limit substitution than I do with quotient rule,,"I'm trying to differentiate $$y = \frac{x+1}{x-1}.$$ Using quotient rule: http://prntscr.com/mt90yc Using product: http://prntscr.com/mt91dd Using limit definition: http://prntscr.com/mt91ih I get $-2$ for product and limit, but I get $\frac{-2}{x^{2}-2x+1}$ using quotient.","I'm trying to differentiate Using quotient rule: http://prntscr.com/mt90yc Using product: http://prntscr.com/mt91dd Using limit definition: http://prntscr.com/mt91ih I get for product and limit, but I get using quotient.",y = \frac{x+1}{x-1}. -2 \frac{-2}{x^{2}-2x+1},"['calculus', 'derivatives']"
86,Simplifying the Derivative of a European Call Option,Simplifying the Derivative of a European Call Option,,"I know the title suggests finance, but I'm stuck on the mathematics of this. I need to take the following derivative: $$ -\frac{\delta C(X)}{\delta X}=-\frac{\delta}{\delta X} \Big[Se^{-dT}N(d_1)-Xe^{-rT}N(d_2)\Big] $$ where S, d, and T are constants, $$ d_1 = d_2 + \sigma \sqrt{T} = \frac{ln(\frac{S}{X})+(r-d+\frac{\sigma^2}{2})T}{\sigma \sqrt{T}} $$ $$ d_2 = \frac{ln(\frac{S}{X})+(r-d-\frac{\sigma^2}{2})T}{\sigma \sqrt{T}} $$ and $\sigma$ is a function of X such that $\sigma = \sigma(X)$ . Further, I need to show this derivative ultimately equals $$ -\frac{\delta C(X)}{\delta X} = e^{-rT}N(d_2) - Xe^{-rT}N'(d_2) \sqrt{T} \sigma'(X) $$ I have currently done the following $$ \begin{align} -\frac{\delta C(X)}{\delta X} &=-\frac{\delta}{\delta X} \Big[Se^{-dT}N(d_1)-Xe^{-rT}N(d_2)\Big] \\ &=-\frac{\delta}{\delta X} \Big[Se^{-dT}N(d_1)\Big]+\frac{\delta}{\delta X}\Big[Xe^{-rT}N(d_2)\Big] \\ &= -Se^{-dT}\frac{\delta}{\delta X} \Big[N(d_1)\Big]+e^{-rT}\frac{\delta}{\delta X}\Big[XN(d_2)\Big] \\ &= -Se^{-dT}\frac{\delta N(d_1)}{\delta d_1}\frac{\delta d_1}{\delta X} + Xe^{-rT}\frac{\delta N(d_2)}{\delta d_2}\frac{\delta d_2}{\delta X} + e^{-rT}N(d_2) \end{align} $$ Finding the partial derivatives $$ \frac{\delta N(d_1)}{\delta d_1} = N'(d_1) $$ $$ \frac{\delta N(d_2)}{\delta d_2} = N'(d_2) $$ $$ \frac{\delta d_1}{\delta X} = \frac{\delta d_2}{\delta X} + \frac{\delta}{\delta X}\Big[\sigma(X) \sqrt{T} \Big] = \frac{\delta d_2}{\delta X} + \sigma'(X) \sqrt{T} $$ $$ \begin{align} \frac{\delta d_2}{\delta X} &= \frac{\delta}{\delta X} \Bigg[ \frac{ln(\frac{S}{X})+(r-d-\frac{\sigma(X)^2}{2})T}{\sigma(X) \sqrt{T}} \Bigg] \\ &= \frac{\delta}{\delta X}\Bigg[\frac{ln(\frac{S}{X})}{\sigma(X) \sqrt{T}}\Bigg] + \frac{\delta}{\delta X}\Bigg[\frac{(r-d)T}{\sigma(X)\sqrt{T}}\Bigg] + \frac{\delta}{\delta X}\Bigg[\frac{\frac{\sigma(X)^2T}{2}}{\sigma(X)\sqrt{T}}\Bigg] \\ &= \frac{ \frac{\sigma(X)}{X}-\sigma'(X)ln(\frac{S}{X})}{\sigma(X)^2 \sqrt{T}} + (r-d)\sqrt{T} \frac{\sigma'(X)}{\sigma(X)^2} + \frac{\sigma'(X) \sqrt{T}}{2} \end{align} $$ Applying these partial derivatives to the derivation $$ \begin{align} -\frac{\delta C(X)}{\delta X} &= -Se^{dT}N'(d_1)\Bigg[ \frac{\delta d_2}{\delta X} + \sigma'(X) \sqrt{T} \Bigg] + Xe^{-rT}N'(d_2) \frac{\delta d_2}{\delta x} + e^{-rT}N(d_2) \\ &= -Se^{-dT}\Bigg[N(d_1)\frac{\delta d_2}{\delta X} + \sigma'(X) \sqrt{T} \Bigg] + Xe^{-rT}N'(d_2)\frac{\delta d_2}{\delta X} + e^{-rT}N(d_2) \end{align} $$ From here it appears that I need the first term to zero out and $\frac{\delta d_2}{\delta X} = \sqrt{T} \sigma'(X)$ , but after working with the formulae for a while I've gotten no closer to an answer.","I know the title suggests finance, but I'm stuck on the mathematics of this. I need to take the following derivative: where S, d, and T are constants, and is a function of X such that . Further, I need to show this derivative ultimately equals I have currently done the following Finding the partial derivatives Applying these partial derivatives to the derivation From here it appears that I need the first term to zero out and , but after working with the formulae for a while I've gotten no closer to an answer.","
-\frac{\delta C(X)}{\delta X}=-\frac{\delta}{\delta X}
\Big[Se^{-dT}N(d_1)-Xe^{-rT}N(d_2)\Big]
 
d_1 = d_2 + \sigma \sqrt{T} = \frac{ln(\frac{S}{X})+(r-d+\frac{\sigma^2}{2})T}{\sigma \sqrt{T}}
 
d_2 = \frac{ln(\frac{S}{X})+(r-d-\frac{\sigma^2}{2})T}{\sigma \sqrt{T}}
 \sigma \sigma = \sigma(X) 
-\frac{\delta C(X)}{\delta X} = e^{-rT}N(d_2) - Xe^{-rT}N'(d_2) \sqrt{T} \sigma'(X)
 
\begin{align}
-\frac{\delta C(X)}{\delta X} &=-\frac{\delta}{\delta X}
\Big[Se^{-dT}N(d_1)-Xe^{-rT}N(d_2)\Big] \\
&=-\frac{\delta}{\delta X}
\Big[Se^{-dT}N(d_1)\Big]+\frac{\delta}{\delta X}\Big[Xe^{-rT}N(d_2)\Big] \\
&= -Se^{-dT}\frac{\delta}{\delta X}
\Big[N(d_1)\Big]+e^{-rT}\frac{\delta}{\delta X}\Big[XN(d_2)\Big] \\
&= -Se^{-dT}\frac{\delta N(d_1)}{\delta d_1}\frac{\delta d_1}{\delta X} + Xe^{-rT}\frac{\delta N(d_2)}{\delta d_2}\frac{\delta d_2}{\delta X} + e^{-rT}N(d_2)
\end{align}
 
\frac{\delta N(d_1)}{\delta d_1} = N'(d_1)
 
\frac{\delta N(d_2)}{\delta d_2} = N'(d_2)
 
\frac{\delta d_1}{\delta X} = \frac{\delta d_2}{\delta X} + \frac{\delta}{\delta X}\Big[\sigma(X) \sqrt{T} \Big] = \frac{\delta d_2}{\delta X} + \sigma'(X) \sqrt{T}
 
\begin{align}
\frac{\delta d_2}{\delta X} &= \frac{\delta}{\delta X} \Bigg[ \frac{ln(\frac{S}{X})+(r-d-\frac{\sigma(X)^2}{2})T}{\sigma(X) \sqrt{T}} \Bigg] \\
&= \frac{\delta}{\delta X}\Bigg[\frac{ln(\frac{S}{X})}{\sigma(X) \sqrt{T}}\Bigg] + \frac{\delta}{\delta X}\Bigg[\frac{(r-d)T}{\sigma(X)\sqrt{T}}\Bigg] + \frac{\delta}{\delta X}\Bigg[\frac{\frac{\sigma(X)^2T}{2}}{\sigma(X)\sqrt{T}}\Bigg] \\
&= \frac{ \frac{\sigma(X)}{X}-\sigma'(X)ln(\frac{S}{X})}{\sigma(X)^2 \sqrt{T}} + (r-d)\sqrt{T} \frac{\sigma'(X)}{\sigma(X)^2} + \frac{\sigma'(X) \sqrt{T}}{2}
\end{align}
 
\begin{align}
-\frac{\delta C(X)}{\delta X} &= -Se^{dT}N'(d_1)\Bigg[ \frac{\delta d_2}{\delta X} + \sigma'(X) \sqrt{T} \Bigg] + Xe^{-rT}N'(d_2) \frac{\delta d_2}{\delta x} + e^{-rT}N(d_2) \\
&= -Se^{-dT}\Bigg[N(d_1)\frac{\delta d_2}{\delta X} + \sigma'(X) \sqrt{T} \Bigg] + Xe^{-rT}N'(d_2)\frac{\delta d_2}{\delta X} + e^{-rT}N(d_2)
\end{align}
 \frac{\delta d_2}{\delta X} = \sqrt{T} \sigma'(X)","['derivatives', 'normal-distribution', 'partial-derivative', 'finance']"
87,How do you differentiate with respect to y?,How do you differentiate with respect to y?,,"Find the gradient of $$z=x^y$$ I understand how to get it with respect to $x$ since $y$ is treated as a constant. But when trying to solve it with respect to $y$ , why is it incorrect to implicitly differentiate and use the product rule: $$\ln(z)=y\cdot ln(x)$$ $$\frac{(z_y)}{z}=\Bigr(y\cdot \frac{1}{x}\Bigr)+(1\cdot \ln(x))$$ $$z_y=z\Bigr(\frac{y}{x}+\ln(x)\Bigr)$$ $$z_y=x^y\Bigr(\frac{y}{x}+\ln(x)\Bigr)$$","Find the gradient of I understand how to get it with respect to since is treated as a constant. But when trying to solve it with respect to , why is it incorrect to implicitly differentiate and use the product rule:",z=x^y x y y \ln(z)=y\cdot ln(x) \frac{(z_y)}{z}=\Bigr(y\cdot \frac{1}{x}\Bigr)+(1\cdot \ln(x)) z_y=z\Bigr(\frac{y}{x}+\ln(x)\Bigr) z_y=x^y\Bigr(\frac{y}{x}+\ln(x)\Bigr),['derivatives']
88,A differentiation/derivative/calculus problem,A differentiation/derivative/calculus problem,,"The question is as follows: $$y=x^2/(x+1)$$ The normal to this curve at $x=1$ meets the $x$ -axis at point $M$ . The tangent to the curve at $x=-2$ meets the $y$ -axis at point $N$ . Find the area of triangle $MNO$ , where $O$ is the origin. PS- this is not a school h.w  so don't worry. And I  did try....for  a good 100  min...not even joking EDIT - The drawing is NOT EXACT, it is just to give an idea.","The question is as follows: The normal to this curve at meets the -axis at point . The tangent to the curve at meets the -axis at point . Find the area of triangle , where is the origin. PS- this is not a school h.w  so don't worry. And I  did try....for  a good 100  min...not even joking EDIT - The drawing is NOT EXACT, it is just to give an idea.",y=x^2/(x+1) x=1 x M x=-2 y N MNO O,"['calculus', 'derivatives']"
89,"Is a function's derivative continuous on $(a,b)$ if it is injective on $[a,b]$?",Is a function's derivative continuous on  if it is injective on ?,"(a,b) [a,b]","Say a differentiable function $f: I \rightarrow \mathbb{R}$ is defined on an open interval $I$ , and there is a closed bounded interval $[a,b] \subseteq I$ . If $f'$ is injective on $[a,b]$ , is it continuous on $(a,b)$ ?","Say a differentiable function is defined on an open interval , and there is a closed bounded interval . If is injective on , is it continuous on ?","f: I \rightarrow \mathbb{R} I [a,b] \subseteq I f' [a,b] (a,b)","['calculus', 'real-analysis', 'derivatives']"
90,Partial Derivative of a double finite summation.,Partial Derivative of a double finite summation.,,"How might i go about finding the derivative the following double sum $$\frac{\partial }{\partial x_i}\left(\sum _{k=1}^n\:\sum _{j=1}^n\:a_{kj}x_kx_j\right)\:$$ My inital idea was to end put the summation terms and $a_{kj}$ outside of the derivative, and then look for special cases where i=j i=k and i=j=k. However i'm quite stuck in the way of expressing it. Basically this is as far as i got. $$\frac{\partial }{\partial x_i}\left(\sum _{k=1}^n\:\sum _{j=1}^n\:a_{kj}x_kx_j\right)\: = \sum _{k=1}^n\:\sum _{j=1}^n\:a_{kj}\frac{\partial \:}{\partial \:x_i}\left(x_kx_j\right)\:$$","How might i go about finding the derivative the following double sum $$\frac{\partial }{\partial x_i}\left(\sum _{k=1}^n\:\sum _{j=1}^n\:a_{kj}x_kx_j\right)\:$$ My inital idea was to end put the summation terms and $a_{kj}$ outside of the derivative, and then look for special cases where i=j i=k and i=j=k. However i'm quite stuck in the way of expressing it. Basically this is as far as i got. $$\frac{\partial }{\partial x_i}\left(\sum _{k=1}^n\:\sum _{j=1}^n\:a_{kj}x_kx_j\right)\: = \sum _{k=1}^n\:\sum _{j=1}^n\:a_{kj}\frac{\partial \:}{\partial \:x_i}\left(x_kx_j\right)\:$$",,"['calculus', 'derivatives']"
91,Taking the derivative of $\sum_\limits{n=1}^{\infty}\arctan(\frac{x}{n^2})$,Taking the derivative of,\sum_\limits{n=1}^{\infty}\arctan(\frac{x}{n^2}),"Problem : Study the possibility of taking the derivative of the following series: $$\sum_\limits{n=1}^{\infty}\arctan\left(\frac{x}{n^2}\right)\:\:,x\in\mathbb{R}$$ I have studied the following theorem: Theorem : Suppose that $\sum_\limits{n=k}^{\infty}f_n$ converges uniformly to $F$ on $S=[a,b]$ . Assume that $F$ and $f_n\:\:,n\geqslant k$ , are integrable on $[a,b]$ . Then: $$\int_\limits{a}^{b}F(x)dx=\sum_\limits{n=k}^{\infty}\int_\limits{a}^{b}f_n(x)dx$$ Following the theorem I would need to check out if the derivative converges uniformly $\sum_\limits{n=1}^{\infty}(\frac{1}{1+\frac{x^2}{n^4}}\frac{2x}{n^4})$ I tried to apply Dirichlet to latter. Once I know that $\sum_\limits{n=1}^{\infty}\frac{2x}{n^4}$ by the integral test converges uniformly. However I was not able to apply it due to the fact that I could not prove $$\sum_\limits{n=1}^{\infty}\left(\frac{1}{1+\frac{x^2}{n^4}}\right)\leqslant M$$ Question : How should I solve the problem? How should I prove the series $\sum_\limits{n=1}^{\infty}\arctan\left(\frac{x}{n^2}\right)$ converge? Thanks in advance!","Problem : Study the possibility of taking the derivative of the following series: I have studied the following theorem: Theorem : Suppose that converges uniformly to on . Assume that and , are integrable on . Then: Following the theorem I would need to check out if the derivative converges uniformly I tried to apply Dirichlet to latter. Once I know that by the integral test converges uniformly. However I was not able to apply it due to the fact that I could not prove Question : How should I solve the problem? How should I prove the series converge? Thanks in advance!","\sum_\limits{n=1}^{\infty}\arctan\left(\frac{x}{n^2}\right)\:\:,x\in\mathbb{R} \sum_\limits{n=k}^{\infty}f_n F S=[a,b] F f_n\:\:,n\geqslant k [a,b] \int_\limits{a}^{b}F(x)dx=\sum_\limits{n=k}^{\infty}\int_\limits{a}^{b}f_n(x)dx \sum_\limits{n=1}^{\infty}(\frac{1}{1+\frac{x^2}{n^4}}\frac{2x}{n^4}) \sum_\limits{n=1}^{\infty}\frac{2x}{n^4} \sum_\limits{n=1}^{\infty}\left(\frac{1}{1+\frac{x^2}{n^4}}\right)\leqslant M \sum_\limits{n=1}^{\infty}\arctan\left(\frac{x}{n^2}\right)","['calculus', 'real-analysis', 'sequences-and-series', 'derivatives', 'uniform-convergence']"
92,Question on Local Maxima and Local Minima,Question on Local Maxima and Local Minima,,"Find the set of all the possible values of $a$ for which the function $f(x) = 5 + (a-2)x + (a-1)x^2 - x^3$ has a local minimum value at some $x < 1$ and local maximum value at some $x > 1$ The first derivative of f(x) is : $f'(x) = (a-2) + 2x(a-1) -3x^2$ I do know the first derivative test for local maxima and local minima, but I can't figure out how I could use monotonicity to find intervals of increase and decrease of $f'(x)$ The expression for $f'(x)$ might suggest the double derivative test is the key, considering $f''(x) = 2(a-1) - 6x$  for which the intervals where it is greater than zero and less than zero can be easily found, but then again I can't think of a way how I could find a $c$ such that $f'(c) = 0$.","Find the set of all the possible values of $a$ for which the function $f(x) = 5 + (a-2)x + (a-1)x^2 - x^3$ has a local minimum value at some $x < 1$ and local maximum value at some $x > 1$ The first derivative of f(x) is : $f'(x) = (a-2) + 2x(a-1) -3x^2$ I do know the first derivative test for local maxima and local minima, but I can't figure out how I could use monotonicity to find intervals of increase and decrease of $f'(x)$ The expression for $f'(x)$ might suggest the double derivative test is the key, considering $f''(x) = 2(a-1) - 6x$  for which the intervals where it is greater than zero and less than zero can be easily found, but then again I can't think of a way how I could find a $c$ such that $f'(c) = 0$.",,"['calculus', 'derivatives', 'maxima-minima']"
93,Rank-1 matrix derivative,Rank-1 matrix derivative,,"I am trying to prove that $$\displaystyle{\frac{\partial}{\partial x} \left( x\cdot x^\top  \right) = x\otimes \mathbb{I}+\mathbb{I}\otimes x}$$ The product $x\cdot x^\top$ is a rank-$1$ matrix. Thus, we actually have a derivative of a matrix by a vector. How do we handle such case? Can you please help prove the equality?","I am trying to prove that $$\displaystyle{\frac{\partial}{\partial x} \left( x\cdot x^\top  \right) = x\otimes \mathbb{I}+\mathbb{I}\otimes x}$$ The product $x\cdot x^\top$ is a rank-$1$ matrix. Thus, we actually have a derivative of a matrix by a vector. How do we handle such case? Can you please help prove the equality?",,"['derivatives', 'matrix-calculus', 'jacobian']"
94,Find $(f^{-1})'(3)$,Find,(f^{-1})'(3),"Let $f:\mathbb{R}\to\mathbb{R}\space \space \space f(x)=\left\{\begin{array}{lc}x^2-2x,&x\leq0\\-x^2-2x,&x>0\end{array}\right.$. Find $(f^{-1})'(3)$. My attempt: First, it's easy to see that $f(x)$ is injective $\forall x \in \mathbb{R}.$ By doing the first derivative table we see that $Im_f=\mathbb{R}.$ so then $f$ is surjective and then $f$ is bijective and it has an inverse function. $$f^{-1}(f(x))=x$$ differentiating both sides we get that $(f^{-1}(f(x)))'=\frac 1{f'(x)}$. $f(x)=3\implies x = -1$ unique solution so then $\boxed{(f^{-1})'(3)=\frac 1{f'(x)}=-\frac 14}$ Second solution: (which i don't really understand what happens here) I want to define $f^{-1}(x).$ First case ($x\leq0):$ $\implies f(x)=x^2-2x \space \space \space \forall x\leq0.$ $$f^{-1}(x)=\pm\sqrt{x+1}+1 \space\space\space \forall x\geq -1.$$ so then: $f^{-1}(x)=\pm\sqrt{x+1}+1 \space \space \space \forall \space x\in[-1,0]?$ Also when do we use the $+$ sign and the $-$ for the square root? Second case ($x>0$): $\implies f(x)=-(x+1)^2+1\implies f^{-1}(x)=\pm\sqrt{1-x}-1 \space \space \space \forall x\in(0,1]?$ same question about the sign of the square root. so then we get $$\boxed{f^{-1}(x)=\left\{\begin{array}{lc}\pm\sqrt{x+1}+1,&x\in\lbrack-1,0\rbrack\\\pm\sqrt{1-x}-1,&x\in(0,1\rbrack\end{array}\right.}$$ But by differentiating is we can't find $(f^{-1})'(3)$. But $f^{-1}(x)$ should be defined on all $\mathbb{R}$.. So... What I am doing wrong? What is really happening here? Also when I try to graph them: We can see that $f^{-1}(x)$ takes values only in $[-1,1]$.","Let $f:\mathbb{R}\to\mathbb{R}\space \space \space f(x)=\left\{\begin{array}{lc}x^2-2x,&x\leq0\\-x^2-2x,&x>0\end{array}\right.$. Find $(f^{-1})'(3)$. My attempt: First, it's easy to see that $f(x)$ is injective $\forall x \in \mathbb{R}.$ By doing the first derivative table we see that $Im_f=\mathbb{R}.$ so then $f$ is surjective and then $f$ is bijective and it has an inverse function. $$f^{-1}(f(x))=x$$ differentiating both sides we get that $(f^{-1}(f(x)))'=\frac 1{f'(x)}$. $f(x)=3\implies x = -1$ unique solution so then $\boxed{(f^{-1})'(3)=\frac 1{f'(x)}=-\frac 14}$ Second solution: (which i don't really understand what happens here) I want to define $f^{-1}(x).$ First case ($x\leq0):$ $\implies f(x)=x^2-2x \space \space \space \forall x\leq0.$ $$f^{-1}(x)=\pm\sqrt{x+1}+1 \space\space\space \forall x\geq -1.$$ so then: $f^{-1}(x)=\pm\sqrt{x+1}+1 \space \space \space \forall \space x\in[-1,0]?$ Also when do we use the $+$ sign and the $-$ for the square root? Second case ($x>0$): $\implies f(x)=-(x+1)^2+1\implies f^{-1}(x)=\pm\sqrt{1-x}-1 \space \space \space \forall x\in(0,1]?$ same question about the sign of the square root. so then we get $$\boxed{f^{-1}(x)=\left\{\begin{array}{lc}\pm\sqrt{x+1}+1,&x\in\lbrack-1,0\rbrack\\\pm\sqrt{1-x}-1,&x\in(0,1\rbrack\end{array}\right.}$$ But by differentiating is we can't find $(f^{-1})'(3)$. But $f^{-1}(x)$ should be defined on all $\mathbb{R}$.. So... What I am doing wrong? What is really happening here? Also when I try to graph them: We can see that $f^{-1}(x)$ takes values only in $[-1,1]$.",,"['calculus', 'real-analysis', 'derivatives', 'inverse', 'inverse-function']"
95,Bessel function and ODE.,Bessel function and ODE.,,"Define the Bessel function $J_{0}$ by   $$J_{0}(x) = \frac{1}{\pi}\int_{-1}^{1}\frac{\cos xt}{\sqrt{1-t^{2}}}\mathrm{d}t.$$   Prove that $J_{0}'' + (\frac{1}{x})J_{0}' + J_{0} = 0$ for all $x > 0$. The hint of my professor was to use the Leibniz Rule (permutation of partial derivatives), but I don't see how that can help. Can someone help me? I appreciate it!","Define the Bessel function $J_{0}$ by   $$J_{0}(x) = \frac{1}{\pi}\int_{-1}^{1}\frac{\cos xt}{\sqrt{1-t^{2}}}\mathrm{d}t.$$   Prove that $J_{0}'' + (\frac{1}{x})J_{0}' + J_{0} = 0$ for all $x > 0$. The hint of my professor was to use the Leibniz Rule (permutation of partial derivatives), but I don't see how that can help. Can someone help me? I appreciate it!",,"['real-analysis', 'derivatives']"
96,Derivatives of function defined by cases,Derivatives of function defined by cases,,"I have the following problem that I'm not sure how to tackle. For function $f$, find values for $a$ and $b$ such that $f'(3)$ exists. The function is defined as: $$  f(x)= \begin{cases} \hfill x^2-4x+8 & \text{if $x \leq 3$} \\ ax+b & \text{if $x \gt 3$} \\ \end{cases} $$ So I'm a bit at a loss here. Both cases of the function are polynomials, so I would assume that they are always derivable, and therefore $f'(3)$ would exist, but I'm not sure how to tackle it. Should I derive both equations and then use the results to create a lineal system of equations? How would you recommend me to proceed? Also, I'm thinking that maybe $f(x)$ is not continuous in $f(3)$ because the limit approaching by the right might be different from the limit approaching from the left. Does it mean it is not derivable and thus $f'(3)$ actually does not exist?","I have the following problem that I'm not sure how to tackle. For function $f$, find values for $a$ and $b$ such that $f'(3)$ exists. The function is defined as: $$  f(x)= \begin{cases} \hfill x^2-4x+8 & \text{if $x \leq 3$} \\ ax+b & \text{if $x \gt 3$} \\ \end{cases} $$ So I'm a bit at a loss here. Both cases of the function are polynomials, so I would assume that they are always derivable, and therefore $f'(3)$ would exist, but I'm not sure how to tackle it. Should I derive both equations and then use the results to create a lineal system of equations? How would you recommend me to proceed? Also, I'm thinking that maybe $f(x)$ is not continuous in $f(3)$ because the limit approaching by the right might be different from the limit approaching from the left. Does it mean it is not derivable and thus $f'(3)$ actually does not exist?",,"['calculus', 'derivatives']"
97,Interpolating $f$ and approximating its derivative,Interpolating  and approximating its derivative,f,"We are given $x_0 = 1, x_1 = \frac{4}{3}$ and $x_2 = 2$. Find a parabola which agrees with function $f(x) = (x + 1)\sin(x)$ in the given points. Afterwards derive a formula for the approximation of the $f'$ i.e. the derivative in $x_1$. What is the approximation of $f'(x_1)$ for function f? I did the first step using interpolation and got the polynomial $p(x) = -1.0647x^2 + 4.2390x - 1.4914.$ Now I do not know how to use this to approximate the derivative. Should I just compute the derivative of the polynomial $p$? What about the approximation then?","We are given $x_0 = 1, x_1 = \frac{4}{3}$ and $x_2 = 2$. Find a parabola which agrees with function $f(x) = (x + 1)\sin(x)$ in the given points. Afterwards derive a formula for the approximation of the $f'$ i.e. the derivative in $x_1$. What is the approximation of $f'(x_1)$ for function f? I did the first step using interpolation and got the polynomial $p(x) = -1.0647x^2 + 4.2390x - 1.4914.$ Now I do not know how to use this to approximate the derivative. Should I just compute the derivative of the polynomial $p$? What about the approximation then?",,"['derivatives', 'numerical-methods', 'approximation', 'interpolation']"
98,Intuition for why a integral with jump discontinuity is continuous but not differentiable?,Intuition for why a integral with jump discontinuity is continuous but not differentiable?,,"Suppose we have a function $f:[b,d]\to\mathbb{R}$ that has a jump discontinuity at some point $b<a<d$ and continuous otherwise. Define $F(x) = \int_b^x$ Then, from other answers on the site , I know that the jump discontinuity does not affect the value of $F(x)$ $F(x)$ is continuous (since the jump discontinuity did not affect the value of $F(x)$, and $F(x)$ would be continuous/differentiable otherwise) $F(x)$ is not differentiable (at $a$) When i think of a function not being differentiable I normally imagine a kink or a jump. But I don't see how/why $F(x)$ would have a kink/jump (if that is the problem for differentiability). It can't jump because the discontinuity doesn't affect the value of the integral. For a kink I have no intuition I tried computing an example where $$g:[-2,3]\to \mathbb{R} = \begin{cases} 1 & x\not = 0\\ 5 & x=0\end{cases}$$ Then we have, for$G(x)\equiv \int_{-2}^xg$ , that $$ G(x) = \begin{cases} \int_{-2}^{x} 1 = (x) +2 =2+x & x< 0\\ \\lim_{\epsilon\to 0} \bigg(\int_{-2}^{-\epsilon}1ds + \int_{\epsilon}^x 1 ds = -\epsilon +2 + x-\epsilon = 2+x-2\epsilon\bigg ) = 2+x & x\geq 0 \end{cases} $$ So the integral is $G(x) = 2+x$, which is continuous and differentiable. So I am messing up the calculation somewhere, but I don't see where? (perhaps I cannot write the integral as the limit as $\epsilon\to 0$, precisely because the discontinuity?)","Suppose we have a function $f:[b,d]\to\mathbb{R}$ that has a jump discontinuity at some point $b<a<d$ and continuous otherwise. Define $F(x) = \int_b^x$ Then, from other answers on the site , I know that the jump discontinuity does not affect the value of $F(x)$ $F(x)$ is continuous (since the jump discontinuity did not affect the value of $F(x)$, and $F(x)$ would be continuous/differentiable otherwise) $F(x)$ is not differentiable (at $a$) When i think of a function not being differentiable I normally imagine a kink or a jump. But I don't see how/why $F(x)$ would have a kink/jump (if that is the problem for differentiability). It can't jump because the discontinuity doesn't affect the value of the integral. For a kink I have no intuition I tried computing an example where $$g:[-2,3]\to \mathbb{R} = \begin{cases} 1 & x\not = 0\\ 5 & x=0\end{cases}$$ Then we have, for$G(x)\equiv \int_{-2}^xg$ , that $$ G(x) = \begin{cases} \int_{-2}^{x} 1 = (x) +2 =2+x & x< 0\\ \\lim_{\epsilon\to 0} \bigg(\int_{-2}^{-\epsilon}1ds + \int_{\epsilon}^x 1 ds = -\epsilon +2 + x-\epsilon = 2+x-2\epsilon\bigg ) = 2+x & x\geq 0 \end{cases} $$ So the integral is $G(x) = 2+x$, which is continuous and differentiable. So I am messing up the calculation somewhere, but I don't see where? (perhaps I cannot write the integral as the limit as $\epsilon\to 0$, precisely because the discontinuity?)",,"['integration', 'derivatives', 'continuity']"
99,$f$ is differentiable $\Rightarrow$ we can find $I \subset \mathbb{R}$ such that $f$ is monotonic on $I$,is differentiable  we can find  such that  is monotonic on,f \Rightarrow I \subset \mathbb{R} f I,"Let $f:\mathbb{R} \rightarrow \mathbb{R}$ Is it true that : $f$ is differentiable $\Leftrightarrow$ we can find $I \subset \mathbb{R}$ such that $f$ is monotonic on $I$ where $I = [a, b]$ ($a \ne b$ and $a, b \in \mathbb{R}$). If this is true I really don't know how to prove such result... My intuition is saying that this is true because by definition if $f$ is differentiable then the function has a tagent line on every point of $\mathbb{R}$ and hence is monotonic, but I am not sure .","Let $f:\mathbb{R} \rightarrow \mathbb{R}$ Is it true that : $f$ is differentiable $\Leftrightarrow$ we can find $I \subset \mathbb{R}$ such that $f$ is monotonic on $I$ where $I = [a, b]$ ($a \ne b$ and $a, b \in \mathbb{R}$). If this is true I really don't know how to prove such result... My intuition is saying that this is true because by definition if $f$ is differentiable then the function has a tagent line on every point of $\mathbb{R}$ and hence is monotonic, but I am not sure .",,"['real-analysis', 'derivatives']"
