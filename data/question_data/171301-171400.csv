,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Find the sample size for a two-sided 90% CI when no prior estimate of $\hat{p}$ is given,Find the sample size for a two-sided 90% CI when no prior estimate of  is given,\hat{p},"The following is not a homework problem, I'm preparing for my exam. I worked to derive the n from Wilson's CI and I assumed that $\hat p = 0$ since there are no prior estimates but I got a negative value which (obviously) is wrong.","The following is not a homework problem, I'm preparing for my exam. I worked to derive the n from Wilson's CI and I assumed that since there are no prior estimates but I got a negative value which (obviously) is wrong.",\hat p = 0,['probability']
1,Probability of two sets of items containing common subset of items,Probability of two sets of items containing common subset of items,,"I've recently been pondering about the following problem: Kyle and his best mate Chad head over to the game shop to purchase   some new games. After squabbling over which games to purchase, Kyle   and Chad eventually limit their options to ten games. Kyle and Chad   have individual orders of preference which are independent of each   other and which the other person does not have any information about. To ensure they choose a game as fairly as possible, they agree that   Kyle randomly removes five options from the list, and then Chad   randomly chooses a game from the remaining five. What is the   probability that the game which Chad chooses is not in the top three   of either person's list of preferences? The obvious thing to me is that the probability that Kyle's top three are in the five he chose is just ${7\choose 2}/{10\choose 5}$ . What I am unsure about is the probability that Chad's random choice is in his top three for a given sample of five. Is this a joint probability (in Chad's top three AND from Kyle's selection) or a conditional one (in Chad's top three GIVEN Kyle's selection)? How do I account for the intersection of their preferences? More generally, for two identical sets of $n$ items, if we select $m$ items out randomly from each set, how do we calculate the probability that the $m$ -element subsets share $k$ elements in common? Is there a well-known distribution for this? Thanks!","I've recently been pondering about the following problem: Kyle and his best mate Chad head over to the game shop to purchase   some new games. After squabbling over which games to purchase, Kyle   and Chad eventually limit their options to ten games. Kyle and Chad   have individual orders of preference which are independent of each   other and which the other person does not have any information about. To ensure they choose a game as fairly as possible, they agree that   Kyle randomly removes five options from the list, and then Chad   randomly chooses a game from the remaining five. What is the   probability that the game which Chad chooses is not in the top three   of either person's list of preferences? The obvious thing to me is that the probability that Kyle's top three are in the five he chose is just . What I am unsure about is the probability that Chad's random choice is in his top three for a given sample of five. Is this a joint probability (in Chad's top three AND from Kyle's selection) or a conditional one (in Chad's top three GIVEN Kyle's selection)? How do I account for the intersection of their preferences? More generally, for two identical sets of items, if we select items out randomly from each set, how do we calculate the probability that the -element subsets share elements in common? Is there a well-known distribution for this? Thanks!",{7\choose 2}/{10\choose 5} n m m k,"['probability', 'combinatorics', 'statistics']"
2,"Pearson product moment correlation coefficent, coefficient of determination and negative values","Pearson product moment correlation coefficent, coefficient of determination and negative values",,"here is the formula I have: $$R = \frac {    \frac {1}{n} \sum_{i=1}^n x_i y_i - \bar{x} \bar{y}   }            {    \sqrt{ \frac {1}{n} \sum_{i=1}^n (x_i - \bar{x})^2   \frac {1}{n} \sum_{i=1}^n (y_i - \bar{y})^2  }}$$ Now assuming we have a set of measurements whose sample correlation coefficient is $R$ . Assuming the relationship (regression curve) : $$ y = \beta_0 + \beta_1 x $$ ( i think we assume the simple linear model), then we have that, given that $\beta_i$ are the MLE of their respective coefficient : $$ R = \hat{\beta}_1 \frac{ \sqrt{ n \sum_{i=1}^n x_i^2 - (\sum_{i=1}^n x_i)^2 }   } {    \sqrt{  n \sum_{i=1}^n y_i^2 - (\sum_{i=1}^n y_i)^2 }} $$ which leads us to: $$ R^2 = \frac{\sum_{i=1}^n (y_i - \bar{y} )^2 - \sum_{i=1}^n(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2 }{\sum_{i=1}^n (y_i - \bar{y})^2} $$ which is I guess is a well-known formula. all of this can be found page 602 of the book ""Introduction to Mathematical Statistics and its applications"" by R.J. Larsen and M.L. Marx. Here is my question. I'm not sure how to tackle this last equation. $R^2$ should be positive, does that mean the numerator can't be   negative, why? Moreover, how do you interpret globally this expression?   Does it have some meaning? Since we are doing $$ \frac {A + B}{A},  $$ I'm assuming it has some interesting meaning.","here is the formula I have: Now assuming we have a set of measurements whose sample correlation coefficient is . Assuming the relationship (regression curve) : ( i think we assume the simple linear model), then we have that, given that are the MLE of their respective coefficient : which leads us to: which is I guess is a well-known formula. all of this can be found page 602 of the book ""Introduction to Mathematical Statistics and its applications"" by R.J. Larsen and M.L. Marx. Here is my question. I'm not sure how to tackle this last equation. should be positive, does that mean the numerator can't be   negative, why? Moreover, how do you interpret globally this expression?   Does it have some meaning? Since we are doing I'm assuming it has some interesting meaning.","R = \frac {    \frac {1}{n} \sum_{i=1}^n x_i y_i - \bar{x} \bar{y}   }           
{    \sqrt{ \frac {1}{n} \sum_{i=1}^n (x_i - \bar{x})^2   \frac {1}{n} \sum_{i=1}^n (y_i - \bar{y})^2  }} R  y = \beta_0 + \beta_1 x  \beta_i  R = \hat{\beta}_1 \frac{ \sqrt{ n \sum_{i=1}^n x_i^2 - (\sum_{i=1}^n x_i)^2 }   } {    \sqrt{  n \sum_{i=1}^n y_i^2 - (\sum_{i=1}^n y_i)^2 }}   R^2 = \frac{\sum_{i=1}^n (y_i - \bar{y} )^2 - \sum_{i=1}^n(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2 }{\sum_{i=1}^n (y_i - \bar{y})^2}  R^2  \frac {A + B}{A},
 ","['statistics', 'statistical-inference', 'regression', 'linear-regression']"
3,Multinomial: Probability of a Subset,Multinomial: Probability of a Subset,,"I have been trying to calculate the probability of getting only a certain subset of  events of a multinomial distribution given that a certain number of events occur. That is, I have a multiniomial distribution $$  f(x_1, x_2, ... ,x_k; n, p_1, p_2, ... ,p_k) = Pr[X_1 = x_1, X_2 = x_2, ..., X_k = x_k] =  \frac{n!}{x_1! x_2!...x_k!} {p_1}^{x_1}  {p_2}^{x_2} ... {p_k}^{x_k} $$ I want to calculate, as a example, $$ Pr[X_1 >0,   \mbox{ } X_2 >0,  X_5 > 0,   \mbox{ }  X_3 = 0,  \mbox{ } X_4 = 0,  \mbox{ } X_6 = 0,  \mbox{ } X_7 = 0, ..., X_k = 0  \mbox{ }| n = 8] $$ More generally, I want to be able to calculate the probability that event occurs such that $\{X_i, X_j, X_k\} >0$ and $\{X_m =0 \}$ for $m \not\in \{i,j,k \}$ given $n$ . Is there a way to do this? I only really need to be able to calculate it numerically, but I'm finding even that surprisingly difficult to accomplish.","I have been trying to calculate the probability of getting only a certain subset of  events of a multinomial distribution given that a certain number of events occur. That is, I have a multiniomial distribution I want to calculate, as a example, More generally, I want to be able to calculate the probability that event occurs such that and for given . Is there a way to do this? I only really need to be able to calculate it numerically, but I'm finding even that surprisingly difficult to accomplish."," 
f(x_1, x_2, ... ,x_k; n, p_1, p_2, ... ,p_k) = Pr[X_1 = x_1, X_2 = x_2, ..., X_k = x_k] =  \frac{n!}{x_1! x_2!...x_k!} {p_1}^{x_1}  {p_2}^{x_2} ... {p_k}^{x_k}
  Pr[X_1 >0,   \mbox{ } X_2 >0,  X_5 > 0,   \mbox{ }  X_3 = 0,  \mbox{ } X_4 = 0,  \mbox{ } X_6 = 0,  \mbox{ } X_7 = 0, ..., X_k = 0  \mbox{ }| n = 8]  \{X_i, X_j, X_k\} >0 \{X_m =0 \} m \not\in \{i,j,k \} n","['probability', 'probability-theory', 'statistics', 'probability-distributions', 'multinomial-coefficients']"
4,Why does $\int_{\theta}^{\infty}u(x)e^{-(x-\theta)}dx=-u(\theta) \implies Pr[u(X)=0]=1$?,Why does ?,\int_{\theta}^{\infty}u(x)e^{-(x-\theta)}dx=-u(\theta) \implies Pr[u(X)=0]=1,"I am trying to prove that a shifted expolential distribution is a complete set. i.e. $$X_1,...,X_n \sim_{iid} f_X(x;\theta)=e^{-(x-\theta)}, 0<\theta<x$$ and if $E[u(X)]=0$ then $$u(X) = 0$$ . The procedure I am taking is to first look at the expectation $$\int_{\theta}^{\infty} u(x)e^{-(x-\theta)}dx=0$$ and take the derivative on both sides with respect to $\theta$ . I see that the result will be $$\int_{\theta}^{\infty}u(x)e^{-(x-\theta)}dx=-u(\theta)$$ but I do not know how this leads to $u(X)=0$ . The notes that I am reading mentions that $Pr[u(X)=0]=1$ instead of $u(X)=0$ so I am thinking that I am missing something. I would appreciate your input.",I am trying to prove that a shifted expolential distribution is a complete set. i.e. and if then . The procedure I am taking is to first look at the expectation and take the derivative on both sides with respect to . I see that the result will be but I do not know how this leads to . The notes that I am reading mentions that instead of so I am thinking that I am missing something. I would appreciate your input.,"X_1,...,X_n \sim_{iid} f_X(x;\theta)=e^{-(x-\theta)}, 0<\theta<x E[u(X)]=0 u(X) = 0 \int_{\theta}^{\infty} u(x)e^{-(x-\theta)}dx=0 \theta \int_{\theta}^{\infty}u(x)e^{-(x-\theta)}dx=-u(\theta) u(X)=0 Pr[u(X)=0]=1 u(X)=0","['probability', 'statistics', 'parameter-estimation']"
5,Why can the normal distribution be used to describe general data?,Why can the normal distribution be used to describe general data?,,"I understand the assumptions underpinning and the motivation behind error distributions which led to the development of Laplace's and eventually Gauss's error distributions as explained in Saul Stahl's essay . It makes sense that larger random errors are less likely than smaller ones, and that the magnitudes of errors are the same regardless of direction, i.e. $\phi(x) = \phi(-x)$ . What I'm having a hard time reconciling is the widespread use of the normal distribution to describe generic data, not just errors. Why can we model human heights or test grades with a normal distribution? I understand The Central Limit Theorem allows us to use the normal distribution to approximate certain sampling distributions, e.g. $\overline{x}$ , as normal (usually when $n \geq 30$ ) - but why? What does an error curve have to do with CLT? Do random factors that affect natural phenomena to cancel each other out leading to (approximately) normal distributions with large samples?","I understand the assumptions underpinning and the motivation behind error distributions which led to the development of Laplace's and eventually Gauss's error distributions as explained in Saul Stahl's essay . It makes sense that larger random errors are less likely than smaller ones, and that the magnitudes of errors are the same regardless of direction, i.e. . What I'm having a hard time reconciling is the widespread use of the normal distribution to describe generic data, not just errors. Why can we model human heights or test grades with a normal distribution? I understand The Central Limit Theorem allows us to use the normal distribution to approximate certain sampling distributions, e.g. , as normal (usually when ) - but why? What does an error curve have to do with CLT? Do random factors that affect natural phenomena to cancel each other out leading to (approximately) normal distributions with large samples?",\phi(x) = \phi(-x) \overline{x} n \geq 30,"['statistics', 'normal-distribution']"
6,VC dimension of left-sided intervals,VC dimension of left-sided intervals,,"Consider the class of left-sided half intervals in $\mathbb{R}^d$ : $$\mathcal{S}^d:=\{(-\infty,t_1]\times(-\infty,t_2]\times\dots\times(-\infty,t_d]|(t_1,\dots,t_d)\in\mathbb{R}^d\}.$$ My question is how to show that the VC dimension of $\mathcal{S}^d$ is $d$ . It is easy to check that there exists $d$ points in $\mathbb{R}^d$ that can be shattered by $\mathcal{S}^d$ such that VC dim $\geq d$ , but I don't know how to clarify the opposite. Thanks for your reading.","Consider the class of left-sided half intervals in : My question is how to show that the VC dimension of is . It is easy to check that there exists points in that can be shattered by such that VC dim , but I don't know how to clarify the opposite. Thanks for your reading.","\mathbb{R}^d \mathcal{S}^d:=\{(-\infty,t_1]\times(-\infty,t_2]\times\dots\times(-\infty,t_d]|(t_1,\dots,t_d)\in\mathbb{R}^d\}. \mathcal{S}^d d d \mathbb{R}^d \mathcal{S}^d \geq d","['statistics', 'machine-learning']"
7,"Covariance of continuous functions, uniform and normal distribution","Covariance of continuous functions, uniform and normal distribution",,"For X~Uniform(1, 9.9) and Y|X = x~Normal(1.4, x^2) What is Cov(X, Y) equal to? What I tried was: E[XY] - E[X]E[Y] Where E[X] = 5.45 and E[Y] = 1.4 But for E[XY] I'm a bit clueless. I've considered: $$ \int_1^{9.9}\int_{-\infty}^{\infty} xx \frac{1}{9.9-1} \frac{e^-\frac{x^2}{2}}{\sqrt{2\pi}} dxdx = 1$$ or is E[XY] more like: $$ \int_1^{9.9}\int_{-\infty}^{\infty} xy \frac{1}{9.9-1} \frac{e^-\frac{(y-1.4)^2}{2x^2}}{\sqrt{2\pi x^2}} dydx = 7.63 $$ and now I realized that 7.63 is already equal to 5.45*1.4 so that would result in 0...","For X~Uniform(1, 9.9) and Y|X = x~Normal(1.4, x^2) What is Cov(X, Y) equal to? What I tried was: E[XY] - E[X]E[Y] Where E[X] = 5.45 and E[Y] = 1.4 But for E[XY] I'm a bit clueless. I've considered: or is E[XY] more like: and now I realized that 7.63 is already equal to 5.45*1.4 so that would result in 0...", \int_1^{9.9}\int_{-\infty}^{\infty} xx \frac{1}{9.9-1} \frac{e^-\frac{x^2}{2}}{\sqrt{2\pi}} dxdx = 1  \int_1^{9.9}\int_{-\infty}^{\infty} xy \frac{1}{9.9-1} \frac{e^-\frac{(y-1.4)^2}{2x^2}}{\sqrt{2\pi x^2}} dydx = 7.63 ,"['statistics', 'probability-distributions', 'normal-distribution', 'uniform-distribution', 'covariance']"
8,Median of 2 sets and median of the sum of the 2 sets,Median of 2 sets and median of the sum of the 2 sets,,"Suppose I have N individuals (N is odd for convenience). Associated with these individuals, two  sets (S1 and S2) which contain integer numbers.  E.g. S1 is the salary/hour and S2 is the bonus/hour: Individuals = {A,B,C,D,E} S1 = {A=9; B=11, C=13; D=15, E=20} S2 = {A=6; B=1, C=4; D=5, E=1} (Note that S1 is ordered while S2 is not). The medians of each set are respectively 13 & 4. Suppose now, that for each individual I do the sum of the two sets: S3 = {A=15; B=12, C=17; D=20, E=21}. The median of S3 is equal to 17. So, in this case, median(S1+S2) = median(S1)+median(S2). My question is, in which conditions this relation is true ? By doing tests, I came to the conclusion that if the median individual in set1 is the same as in set2 then the relationship is true... Is there other conditions ? How to prove it ? Thank you,","Suppose I have N individuals (N is odd for convenience). Associated with these individuals, two  sets (S1 and S2) which contain integer numbers.  E.g. S1 is the salary/hour and S2 is the bonus/hour: Individuals = {A,B,C,D,E} S1 = {A=9; B=11, C=13; D=15, E=20} S2 = {A=6; B=1, C=4; D=5, E=1} (Note that S1 is ordered while S2 is not). The medians of each set are respectively 13 & 4. Suppose now, that for each individual I do the sum of the two sets: S3 = {A=15; B=12, C=17; D=20, E=21}. The median of S3 is equal to 17. So, in this case, median(S1+S2) = median(S1)+median(S2). My question is, in which conditions this relation is true ? By doing tests, I came to the conclusion that if the median individual in set1 is the same as in set2 then the relationship is true... Is there other conditions ? How to prove it ? Thank you,",,['statistics']
9,Expectation of multiplied matrices,Expectation of multiplied matrices,,"In p.50 of Quadratic forms in random variables: theory and applications by Mathai and Provost (1992) $$\mathbb{E}\left[\mathbf{X}^{\operatorname{T}}A\mathbf{X}\right]=\cdots =\mathbb{E}\left[\operatorname{tr}\left(A\mathbf{X}\mathbf{X}^{\operatorname{T}} \right ) \right ]=\operatorname{tr}\left(A\left(\Sigma+\mathbf{\mu}\mathbf{\mu}^{\operatorname{T}} \right ) \right )$$ where $\mathbf{\mu}=\mathbb{E}\left[\mathbf{X}\right]$ and $\Sigma=\operatorname{cov}\left[\mathbf{X}\right]$ . (The notation has slightly changed but the meaning should be the same.) I am having trouble with the last two steps shown above. I understand that $$\mathbb{E}\left[\operatorname{tr}\left(A\mathbf{X}\mathbf{X}^{\operatorname{T}}\right)\right]=\operatorname{tr}\left(\mathbb{E}\left[A\mathbf{X}\mathbf{X}^{\operatorname{T}}\right]\right)$$ but I am not sure why $$\operatorname{tr\left(\mathbb{E}\left[A\mathbf{X}\mathbf{X}^{\operatorname{T}}\right]\right)}=\operatorname{tr}\left(A\left(\Sigma+\mathbf{\mu}\mathbf{\mu}^{\operatorname{T}}\right)\right)$$ Breaking this down, the above equality is implying that $$\mathbb{E}\left[A\mathbf{X}\mathbf{X}^{\operatorname{T}}\right]=A\left(\Sigma+\mathbf{\mu}\mathbf{\mu}^{\operatorname{T}}\right)$$ I understand that $$\Sigma=\operatorname{cov}\left[\mathbf{X}\right]=\mathbb{E}\left[\mathbf{X}\mathbf{X}^{\operatorname{T}}\right]-\mathbf{\mu}\mathbf{\mu}^{\operatorname{T}}\Leftrightarrow\mathbb{E}\left[\mathbf{X}\mathbf{X}^{\operatorname{T}}\right]=\Sigma+\mathbf{\mu}\mathbf{\mu}^{\operatorname{T}}$$ Then this is implying that $$\mathbb{E}\left[A\mathbf{X}\mathbf{X}^{\operatorname{T}}\right]=A\,\mathbb{E}\left[\mathbf{X}\mathbf{X}^{\operatorname{T}}\right]$$ I do not know why $$\mathbb{E}\left[A\mathbf{X}\mathbf{X}^{\operatorname{T}}\right]=A\,\mathbb{E}\left[\mathbf{X}\mathbf{X}^{\operatorname{T}}\right]$$ I would really appreciate it if anyone can provide a proof for this. Thank you!","In p.50 of Quadratic forms in random variables: theory and applications by Mathai and Provost (1992) where and . (The notation has slightly changed but the meaning should be the same.) I am having trouble with the last two steps shown above. I understand that but I am not sure why Breaking this down, the above equality is implying that I understand that Then this is implying that I do not know why I would really appreciate it if anyone can provide a proof for this. Thank you!","\mathbb{E}\left[\mathbf{X}^{\operatorname{T}}A\mathbf{X}\right]=\cdots =\mathbb{E}\left[\operatorname{tr}\left(A\mathbf{X}\mathbf{X}^{\operatorname{T}} \right ) \right ]=\operatorname{tr}\left(A\left(\Sigma+\mathbf{\mu}\mathbf{\mu}^{\operatorname{T}} \right ) \right ) \mathbf{\mu}=\mathbb{E}\left[\mathbf{X}\right] \Sigma=\operatorname{cov}\left[\mathbf{X}\right] \mathbb{E}\left[\operatorname{tr}\left(A\mathbf{X}\mathbf{X}^{\operatorname{T}}\right)\right]=\operatorname{tr}\left(\mathbb{E}\left[A\mathbf{X}\mathbf{X}^{\operatorname{T}}\right]\right) \operatorname{tr\left(\mathbb{E}\left[A\mathbf{X}\mathbf{X}^{\operatorname{T}}\right]\right)}=\operatorname{tr}\left(A\left(\Sigma+\mathbf{\mu}\mathbf{\mu}^{\operatorname{T}}\right)\right) \mathbb{E}\left[A\mathbf{X}\mathbf{X}^{\operatorname{T}}\right]=A\left(\Sigma+\mathbf{\mu}\mathbf{\mu}^{\operatorname{T}}\right) \Sigma=\operatorname{cov}\left[\mathbf{X}\right]=\mathbb{E}\left[\mathbf{X}\mathbf{X}^{\operatorname{T}}\right]-\mathbf{\mu}\mathbf{\mu}^{\operatorname{T}}\Leftrightarrow\mathbb{E}\left[\mathbf{X}\mathbf{X}^{\operatorname{T}}\right]=\Sigma+\mathbf{\mu}\mathbf{\mu}^{\operatorname{T}} \mathbb{E}\left[A\mathbf{X}\mathbf{X}^{\operatorname{T}}\right]=A\,\mathbb{E}\left[\mathbf{X}\mathbf{X}^{\operatorname{T}}\right] \mathbb{E}\left[A\mathbf{X}\mathbf{X}^{\operatorname{T}}\right]=A\,\mathbb{E}\left[\mathbf{X}\mathbf{X}^{\operatorname{T}}\right]","['linear-algebra', 'statistics']"
10,The distribution of $X_i-X_{(1)}$,The distribution of,X_i-X_{(1)},"If $X_1,...,X_n$ be a random sample from $Exp(1)$ and $X_{(1)}$ is the smallest exponential order statistic, how can I get the distribution of $X_i-X_{(1)}$ ?","If be a random sample from and is the smallest exponential order statistic, how can I get the distribution of ?","X_1,...,X_n Exp(1) X_{(1)} X_i-X_{(1)}","['probability-theory', 'statistics', 'exponential-distribution', 'order-statistics']"
11,Scaling vector with multivariate normal,Scaling vector with multivariate normal,,"If I have a vector $\mathbf{\bar{X}}=\frac{1}{N}\sum_{i=1}^N \mathbf{X}_i$ such that it is known by CLT that $\sqrt{N}(\mathbf{\bar{X}}-\bar{\mu})$ (here $\bar{\mu}$ is the mean vector) converges in distribution to a multivariate normal $N(\mathbf{\bar{0}},\Sigma)$ what are the parameters for the limiting distribution for just $\mathbf{\bar{X}}$ if its possible to use this fact? Im not sure how to adjust for the scaling in the multivariate case",If I have a vector such that it is known by CLT that (here is the mean vector) converges in distribution to a multivariate normal what are the parameters for the limiting distribution for just if its possible to use this fact? Im not sure how to adjust for the scaling in the multivariate case,"\mathbf{\bar{X}}=\frac{1}{N}\sum_{i=1}^N \mathbf{X}_i \sqrt{N}(\mathbf{\bar{X}}-\bar{\mu}) \bar{\mu} N(\mathbf{\bar{0}},\Sigma) \mathbf{\bar{X}}",['statistics']
12,How does $Y_n$ approach $\theta$ in probability here?,How does  approach  in probability here?,Y_n \theta,"From Statistical Inference by Casella and Berger: $\text{(Delta Method)}$ Let $Y_n$ be a sequence of random variables that   satisfies $\sqrt{n}(Y_n - \theta) \rightarrow n(0,\sigma^2)$ in   distribution.  For a given function $g$ and a specific value of $\theta$ , suppose that $g'(\theta)$ exists and is not $0$ .  Then $\sqrt{n}[g(Y_n) - g(\theta)] \rightarrow n(0, \sigma^2[g'(\theta)])$ in distribution. The taylor expansion of $g(Y_n)$ around $Y_n = \theta$ is $$g(Y_n) = g(\theta) + g'(\theta)(Y_n-\theta) + \text{ Remainder, }$$ where the remainder $\rightarrow 0$ as $Y_n \rightarrow \theta$ . Since $Y_n \rightarrow \theta$ in probability , it follows that the remainder $\rightarrow 0 $ in probability. Why does $Y_n \rightarrow \theta$ in probability?  This isn't assumed in the statement of the theorem, so why is this true?","From Statistical Inference by Casella and Berger: Let be a sequence of random variables that   satisfies in   distribution.  For a given function and a specific value of , suppose that exists and is not .  Then in distribution. The taylor expansion of around is where the remainder as . Since in probability , it follows that the remainder in probability. Why does in probability?  This isn't assumed in the statement of the theorem, so why is this true?","\text{(Delta Method)} Y_n \sqrt{n}(Y_n - \theta) \rightarrow n(0,\sigma^2) g \theta g'(\theta) 0 \sqrt{n}[g(Y_n) - g(\theta)] \rightarrow n(0, \sigma^2[g'(\theta)]) g(Y_n) Y_n = \theta g(Y_n) = g(\theta) + g'(\theta)(Y_n-\theta) + \text{ Remainder, } \rightarrow 0 Y_n \rightarrow \theta Y_n \rightarrow \theta \rightarrow 0  Y_n \rightarrow \theta","['probability-theory', 'statistics', 'proof-explanation', 'statistical-inference']"
13,A problem about finding the probability a random variable is bigger than another random variable,A problem about finding the probability a random variable is bigger than another random variable,,"I'm studying statistics through Khan Academy and it had this question: ""Yuki and Zana are on a swimming team. They often compete against each   other in the 100 meter freestyle race. Yuki's times in this race are   normally distributed with a mean of 80 seconds and a standard   deviation of 4.2 seconds. Zana's times are also   normally distributed with a mean of 85 seconds and a standard   deviation of 5.6 seconds. We can assume that their   times are independent. Suppose we choose a random 100-meter freestyle race and calculate the   difference between their times. Find the probability that Yuki's time is faster than Zana's."" Khan Academy's answer to the question is 0.7611 or approximately 0.76. I've spent hours working on this problem and I frustratingly keep on ending up with an answer of 0.2389. Can someone please help me on what I'm doing wrong?","I'm studying statistics through Khan Academy and it had this question: ""Yuki and Zana are on a swimming team. They often compete against each   other in the 100 meter freestyle race. Yuki's times in this race are   normally distributed with a mean of 80 seconds and a standard   deviation of 4.2 seconds. Zana's times are also   normally distributed with a mean of 85 seconds and a standard   deviation of 5.6 seconds. We can assume that their   times are independent. Suppose we choose a random 100-meter freestyle race and calculate the   difference between their times. Find the probability that Yuki's time is faster than Zana's."" Khan Academy's answer to the question is 0.7611 or approximately 0.76. I've spent hours working on this problem and I frustratingly keep on ending up with an answer of 0.2389. Can someone please help me on what I'm doing wrong?",,"['probability-theory', 'statistics', 'probability-distributions', 'random-variables', 'normal-distribution']"
14,Find the critical region and the power when $H_0$ is false.,Find the critical region and the power when  is false.,H_0,"In a sample $N(0, \sigma ^2)$ we have two hypothesis. $H_0: \sigma ^2 =16$ and $H_1: \sigma ^2 =4$ (a )For a sample of size n, find the form of the best critical region. (b) If  n = 10 and $\alpha = 0.10$ , find the critical region  and the power when $H_0$ is false. (c) Whith the next sample: 0.05, 1.58, 1.41, -0.01, -0.40, -1.39, -1.78, 1.03, 1.27, 0.23; ¿Which hypothesis should be accepted? a) I use the Neyman-Pearson Lemma: $$\frac{L(16)}{L(4)} =\frac{\prod_{i=0}^n 1/\sqrt{32\pi} e^{\frac{-x_i^2}{32}}}{\prod_{i=0}^n 1/\sqrt{8\pi } e^{\frac{-x_i^2}{8}}} \le K$$ $$\iff \sum_{i=0}^n{X_i^2 \le c}$$ then $\gamma: $ Reject $H_0$ if $ \sum_{i=0}^n{X_i^2 \le c}$ thus the critical region is $C^*=\{(X_1,...,X_n) |  \sum_{i=0}^n {X_i^2} \le c \}$ b)I think I should find the value of c using $P($ Reject $ H_0 | H_a)$ and $\pi_\gamma (4) $ Is this correct? In this case: $P($ Reject $ H_0 | H_a) =P(\sum_{i=0}^n {X_i^2} \le c | \sigma^2=4)$ . Since $X_i^2=4(\frac{X_i}{2})^2 \sim$ Gamma $(1/2, 8)$ then $\sum_{i=0}^n {X_i^2} \sim$ Gamma $(5,8)$ .  If $\alpha=.1 $ , c is the value of the quantile .1 c) $P($ Reject $ H_0 | H_0) =P(\sum_{i=0}^n {X_i^2} \le c | \sigma^2=16)$ using the same argument in b) I get: $\sum_{i=0}^n {X_i^2} \sim$ Gamma $(5,32)$ and  c=77.84, using the values $\sum_{i=0}^n {X_i^2}=12.47$ then $H_0 $ is rejected","In a sample we have two hypothesis. and (a )For a sample of size n, find the form of the best critical region. (b) If  n = 10 and , find the critical region  and the power when is false. (c) Whith the next sample: 0.05, 1.58, 1.41, -0.01, -0.40, -1.39, -1.78, 1.03, 1.27, 0.23; ¿Which hypothesis should be accepted? a) I use the Neyman-Pearson Lemma: then Reject if thus the critical region is b)I think I should find the value of c using Reject and Is this correct? In this case: Reject . Since Gamma then Gamma .  If , c is the value of the quantile .1 c) Reject using the same argument in b) I get: Gamma and  c=77.84, using the values then is rejected","N(0, \sigma ^2) H_0: \sigma ^2 =16 H_1: \sigma ^2 =4 \alpha = 0.10 H_0 \frac{L(16)}{L(4)} =\frac{\prod_{i=0}^n 1/\sqrt{32\pi} e^{\frac{-x_i^2}{32}}}{\prod_{i=0}^n 1/\sqrt{8\pi } e^{\frac{-x_i^2}{8}}} \le K \iff \sum_{i=0}^n{X_i^2 \le c} \gamma:  H_0  \sum_{i=0}^n{X_i^2 \le c} C^*=\{(X_1,...,X_n) |  \sum_{i=0}^n {X_i^2} \le c \} P(  H_0 | H_a) \pi_\gamma (4)  P(  H_0 | H_a) =P(\sum_{i=0}^n {X_i^2} \le c | \sigma^2=4) X_i^2=4(\frac{X_i}{2})^2 \sim (1/2, 8) \sum_{i=0}^n {X_i^2} \sim (5,8) \alpha=.1  P(  H_0 | H_0) =P(\sum_{i=0}^n {X_i^2} \le c | \sigma^2=16) \sum_{i=0}^n {X_i^2} \sim (5,32) \sum_{i=0}^n {X_i^2}=12.47 H_0 ","['statistics', 'statistical-inference', 'estimation', 'hypothesis-testing', 'parameter-estimation']"
15,Use the t-test to test below find its t-statistic,Use the t-test to test below find its t-statistic,,"Use the t-test to test $H_0 : \beta_1 = 0$ $H_1 : \beta_1 \neq 0$ at level significance $5 \%$ . Give test statistic and conclude within the context of the study Given $SSREG = 36460$ $s = 365$ $\hat\beta_1 = -5.98$ $n = 49$ attempt: $t^* = \frac{\hat\beta_1}{s(\hat\beta_1)}$ $s(\hat\beta_1) = \sqrt{\frac{s^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2}}$ $\sum_{i=1}^{n} (X_i - \bar{X})^2 = \frac{SSREG}{\hat\beta_1^2} = 1019.5$ so then $s(\hat\beta_1) = \sqrt{\frac{s^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2}} = \sqrt{\frac{365^2}{1019.5}} = 11.4$ and finally $t^* = \frac{(-5.98)}{11.4} = -0.52$ I have to show $|t^*| < t_{1 - \frac{\alpha}{2}, n - 2}$ or $|t^*| \geq t_{1 - \frac{\alpha}{2}, n - 2}$ but what is $\alpha$ equal to?",Use the t-test to test at level significance . Give test statistic and conclude within the context of the study Given attempt: so then and finally I have to show or but what is equal to?,"H_0 : \beta_1 = 0 H_1 : \beta_1 \neq 0 5 \% SSREG = 36460 s = 365 \hat\beta_1 = -5.98 n = 49 t^* = \frac{\hat\beta_1}{s(\hat\beta_1)} s(\hat\beta_1) = \sqrt{\frac{s^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2}} \sum_{i=1}^{n} (X_i - \bar{X})^2 = \frac{SSREG}{\hat\beta_1^2} = 1019.5 s(\hat\beta_1) = \sqrt{\frac{s^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2}} = \sqrt{\frac{365^2}{1019.5}} = 11.4 t^* = \frac{(-5.98)}{11.4} = -0.52 |t^*| < t_{1 - \frac{\alpha}{2}, n - 2} |t^*| \geq t_{1 - \frac{\alpha}{2}, n - 2} \alpha","['statistics', 'regression']"
16,Confidence interval of $\mu_1 - \mu_2$ of two samples,Confidence interval of  of two samples,\mu_1 - \mu_2,"We have two samples where both is $N(\mu_i, \sigma_i)$ , we assume that $\sigma_1 = \sigma_2$ . From the two samples we get the follow numbers: $n_1 = 4, \bar{x} = 1007.25, s_1=143.66$ and $n_2 = 4, \bar{x}=817.25, s_2=73.627$ Calculate the lower limit of a confidence interval for $\mu_1 - \mu_2$ where the confidence level is 95%. What I've done: I calculated $s$ to be: $s=114,1471$ . Then I use t-distribution: $\mu_1 - \mu_2 - \frac{s\cdot \lambda_{\frac{\alpha}{2}}(f)}{\sqrt{n}}$ which gives: $1007.25 - 817.75 - \frac{s\cdot \lambda_{0.025}(6)}{\sqrt{8}} = 90.123$ which apparently is wrong. What am I doing wrong here? The thing I feel most uncertain about here is what $f$ and $n$ is supposed to be when using two samples like this.","We have two samples where both is , we assume that . From the two samples we get the follow numbers: and Calculate the lower limit of a confidence interval for where the confidence level is 95%. What I've done: I calculated to be: . Then I use t-distribution: which gives: which apparently is wrong. What am I doing wrong here? The thing I feel most uncertain about here is what and is supposed to be when using two samples like this.","N(\mu_i, \sigma_i) \sigma_1 = \sigma_2 n_1 = 4, \bar{x} = 1007.25, s_1=143.66 n_2 = 4, \bar{x}=817.25, s_2=73.627 \mu_1 - \mu_2 s s=114,1471 \mu_1 - \mu_2 - \frac{s\cdot \lambda_{\frac{\alpha}{2}}(f)}{\sqrt{n}} 1007.25 - 817.75 - \frac{s\cdot \lambda_{0.025}(6)}{\sqrt{8}} = 90.123 f n","['statistics', 'confidence-interval']"
17,Find confidence levels from given intervals,Find confidence levels from given intervals,,"A set of $40$ data items, produces a confidence interval for the population mean of $94.93<\mu<105.07$ . If $\sum x^2 = 424 375$ , find the confidence level. So the idea of confidence intervals is still rather new to me and one that isn't fully clear in my mind, would someone be able to give me some hints about solving this and explain what they are doing to solve it. I get that the confidence interval is given by $$\bar{x}-z\frac{\sigma}{\sqrt{n}}<\mu<\bar{x}+z\frac{\sigma}{\sqrt{n}}$$ and that $z=\Phi^{-1}(c)$ where $c$ is the confidence level, however this is as far as my knowledge goes.","A set of data items, produces a confidence interval for the population mean of . If , find the confidence level. So the idea of confidence intervals is still rather new to me and one that isn't fully clear in my mind, would someone be able to give me some hints about solving this and explain what they are doing to solve it. I get that the confidence interval is given by and that where is the confidence level, however this is as far as my knowledge goes.",40 94.93<\mu<105.07 \sum x^2 = 424 375 \bar{x}-z\frac{\sigma}{\sqrt{n}}<\mu<\bar{x}+z\frac{\sigma}{\sqrt{n}} z=\Phi^{-1}(c) c,"['statistics', 'normal-distribution', 'confidence-interval']"
18,Calculating an average speed (simple),Calculating an average speed (simple),,"The other day I had to cover a distance of 241 km. The first 99 km I covered within 33 hours, so my speed was 3 km/h. The next 120 km I covered within 30 hours, so the speed was 4 km/h. Then it was 21 km within 3 hours at the speed of 7 km/h. And the very last kilometer I finished within one hour. Thus, I covered the whole distance within 67 hours (33+30+3+1=67). And my average speed was 3.59 km/h (241/67 = 3.59) However, if I sum up all the four above-mentioned speeds and divide them by four, I will get a different value: (3+4+7+1)/4 = 15/4 = 3.75 Why is it so? Why is this second average speed higher then the first one? What does it reflect?","The other day I had to cover a distance of 241 km. The first 99 km I covered within 33 hours, so my speed was 3 km/h. The next 120 km I covered within 30 hours, so the speed was 4 km/h. Then it was 21 km within 3 hours at the speed of 7 km/h. And the very last kilometer I finished within one hour. Thus, I covered the whole distance within 67 hours (33+30+3+1=67). And my average speed was 3.59 km/h (241/67 = 3.59) However, if I sum up all the four above-mentioned speeds and divide them by four, I will get a different value: (3+4+7+1)/4 = 15/4 = 3.75 Why is it so? Why is this second average speed higher then the first one? What does it reflect?",,"['calculus', 'statistics']"
19,"$U$~$N(3,16)$ $V$ ~$\chi_{9}^{2}$ U and V are independent random variables. Find $P(U-3<4.33\sqrt{V})$",~  ~ U and V are independent random variables. Find,"U N(3,16) V \chi_{9}^{2} P(U-3<4.33\sqrt{V})","$U$ ~ $N(3,16)$ $V$ ~ $\chi_{9}^{2}$ U and V are independent random variables. Find $P(U-3<4.33\sqrt{V})$ (The notes I'm working through don't seem to approach this rigorously...) The answer is $P(t_{9} <3\times\frac{4.33}{4})$ and I'd appreciate a rigorous solution. where $t_{9}$ is the t distribution with 9 degrees of freedom.",~ ~ U and V are independent random variables. Find (The notes I'm working through don't seem to approach this rigorously...) The answer is and I'd appreciate a rigorous solution. where is the t distribution with 9 degrees of freedom.,"U N(3,16) V \chi_{9}^{2} P(U-3<4.33\sqrt{V}) P(t_{9} <3\times\frac{4.33}{4}) t_{9}","['statistics', 'probability-distributions', 'statistical-inference', 'sampling']"
20,How do you find the probability that the sample mean is between 52 and 56?,How do you find the probability that the sample mean is between 52 and 56?,,"I was attempting to help a friend with a question and I am not sure if I am overthinking it, or simply missing an assumption I can make.  It goes like this: The average life of a battery is 50 hours with a standard deviation of 4 hours.  What is the probability that the mean of 16 samples is between 52 and 56? I thought I had this, but I am getting caught up on the fact that it is a small sample size and it is not stated that it is normally distributed. Question:  What method do you use to solve this? Other people suggested: $$Z = \frac{\bar{X}-\mu}{\sigma}$$ My problem here is we are assuming it is normal and we are not utilizing the given info about the sample size of 16. I suggested: $$Z = \frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}}$$ My problem here is we are again assuming it is normal and I am getting $2<Z<6$ , which gives about $2\%$ . Any clarifications will be greatlyappreciated!","I was attempting to help a friend with a question and I am not sure if I am overthinking it, or simply missing an assumption I can make.  It goes like this: The average life of a battery is 50 hours with a standard deviation of 4 hours.  What is the probability that the mean of 16 samples is between 52 and 56? I thought I had this, but I am getting caught up on the fact that it is a small sample size and it is not stated that it is normally distributed. Question:  What method do you use to solve this? Other people suggested: My problem here is we are assuming it is normal and we are not utilizing the given info about the sample size of 16. I suggested: My problem here is we are again assuming it is normal and I am getting , which gives about . Any clarifications will be greatlyappreciated!",Z = \frac{\bar{X}-\mu}{\sigma} Z = \frac{\bar{X}-\mu}{\frac{\sigma}{\sqrt{n}}} 2<Z<6 2\%,"['probability', 'statistics', 'normal-distribution', 'sampling']"
21,Intuition behind joint pdf with transformations with partitioned support,Intuition behind joint pdf with transformations with partitioned support,,"I'm trying to understand this part of Statistical Inference(Casella, Berger) regarding expressing the joint pdf of non-bijective transformations. More specifically, what is the intuition behind (4.3.6)? I understand the case of one-two-one transformations, but here, I cannot understand why the joint pdf becomes a summation. Thanks in advance!","I'm trying to understand this part of Statistical Inference(Casella, Berger) regarding expressing the joint pdf of non-bijective transformations. More specifically, what is the intuition behind (4.3.6)? I understand the case of one-two-one transformations, but here, I cannot understand why the joint pdf becomes a summation. Thanks in advance!",,"['probability', 'statistics', 'random-variables', 'intuition', 'jacobian']"
22,Why is the bottom limit of the conditional probability $x$ in Bayesian Statistics?,Why is the bottom limit of the conditional probability  in Bayesian Statistics?,x,"I am learning bayesian statistics and was stuck when trying to understand the following example: Romeo and Juliet start dating, but Juliet will be late on any date by a random amount X, uniformly distributed over the interval [0, $\theta$ ]. The parameter $\theta$ is unknown and is modelled as the value of a random variable $\Theta$ , uniformly distributed between zero and one hour. Assuming that Juliet was late by an amount $x$ on their first date, how should Romeo use this information to update the distribution of $\Theta$ ? The sample solution is as follows: $f_\Theta(\theta)$ = 1 if $0 \leq \theta \leq 1$ , 0 otherwise $f_{X|\Theta}(x|\theta) = \frac{1}{\theta}$ if $0 \leq x \leq \theta$ , 0 otherwise The posterior pdf is: $$ f_{\Theta|X} = \frac{f_{\Theta}(\theta)f_{X|\Theta}(x|\theta)}{\int_0^1{f_\Theta(\theta')f_{X|\Theta}(x|\theta')d\theta'}} $$ The following step is where I have a problem: $$ \frac{1/\theta}{\int_x^1{1/\theta'}d\theta'} $$ How did the limits for the integeral go from (0, 1) to (x, 1). I cannot find the justification for this step or why the limits is changing. Thank you for your help.","I am learning bayesian statistics and was stuck when trying to understand the following example: Romeo and Juliet start dating, but Juliet will be late on any date by a random amount X, uniformly distributed over the interval [0, ]. The parameter is unknown and is modelled as the value of a random variable , uniformly distributed between zero and one hour. Assuming that Juliet was late by an amount on their first date, how should Romeo use this information to update the distribution of ? The sample solution is as follows: = 1 if , 0 otherwise if , 0 otherwise The posterior pdf is: The following step is where I have a problem: How did the limits for the integeral go from (0, 1) to (x, 1). I cannot find the justification for this step or why the limits is changing. Thank you for your help.","\theta \theta \Theta x \Theta f_\Theta(\theta) 0 \leq \theta \leq 1 f_{X|\Theta}(x|\theta) = \frac{1}{\theta} 0 \leq x \leq \theta 
f_{\Theta|X} = \frac{f_{\Theta}(\theta)f_{X|\Theta}(x|\theta)}{\int_0^1{f_\Theta(\theta')f_{X|\Theta}(x|\theta')d\theta'}}
 
\frac{1/\theta}{\int_x^1{1/\theta'}d\theta'}
","['probability', 'statistics', 'statistical-inference', 'bayesian']"
23,Why couldn't we use x / average for standardization instead of z scores?,Why couldn't we use x / average for standardization instead of z scores?,,"As I was reviewing data standardization and z score theory, i had this intuition. Suppose you have the results of people who took two different tests: TEST A (mean=70%; std.dev=6%) +--------------+-------+---------+-------+ | Participant# | score | z-score | x/avg | +--------------+-------+---------+-------+ | 1            |    60 | -1.66   | 0.85  | | 2            |    65 | -0.83   | 0.92  | | 3            |    80 | 1.66    | 1.14  | | 4            |    90 | 3.33    | 1.28  | | 5            |    40 | -5.00   | 0.57  | | ...          |       |         |       | +--------------+-------+---------+-------+  TEST B (mean=75%;std.dev=7%) +--------------+-------+---------+-------+ | Participant# | score | z-score | x/avg | +--------------+-------+---------+-------+ | 1            |    60 | -2.14   | 0.8   | | 2            |    70 | -0.71   | 0.93  | | 3            |    80 | 0.71    | 1.06  | | 4            |    90 | 2.14    | 1.2   | | ...          |       |         |       | +--------------+-------+---------+-------+ We can see that participant #3 in test A has a higher z score than participant #3 in test B and so that he's relatively better than his counterpart. I can't find any info on the name of the measurement x/avg, but I have the intuition that it could be used as a proxy for standardized data. I am surely wrong as it is mentioned nowhere, but why?","As I was reviewing data standardization and z score theory, i had this intuition. Suppose you have the results of people who took two different tests: TEST A (mean=70%; std.dev=6%) +--------------+-------+---------+-------+ | Participant# | score | z-score | x/avg | +--------------+-------+---------+-------+ | 1            |    60 | -1.66   | 0.85  | | 2            |    65 | -0.83   | 0.92  | | 3            |    80 | 1.66    | 1.14  | | 4            |    90 | 3.33    | 1.28  | | 5            |    40 | -5.00   | 0.57  | | ...          |       |         |       | +--------------+-------+---------+-------+  TEST B (mean=75%;std.dev=7%) +--------------+-------+---------+-------+ | Participant# | score | z-score | x/avg | +--------------+-------+---------+-------+ | 1            |    60 | -2.14   | 0.8   | | 2            |    70 | -0.71   | 0.93  | | 3            |    80 | 0.71    | 1.06  | | 4            |    90 | 2.14    | 1.2   | | ...          |       |         |       | +--------------+-------+---------+-------+ We can see that participant #3 in test A has a higher z score than participant #3 in test B and so that he's relatively better than his counterpart. I can't find any info on the name of the measurement x/avg, but I have the intuition that it could be used as a proxy for standardized data. I am surely wrong as it is mentioned nowhere, but why?",,['statistics']
24,How to calculate similarity between two clusters?,How to calculate similarity between two clusters?,,"I have generated clusters for two different datasets (d1 and d2) with Hierarchical Clustering algorithm and I would like to calculate the similarity between the clusters generated for d1 and d2.  Example: Compare d1_1 to d2_1, where ""_x"" is the cluster number Is important to note that each cluster can have different number of objects, but all clusters have the same attributes types: **Atribute name**    **data type**  Attr1         -         nominal  Attr2         -         numerical Attr3         -         nominal Attr4         -         nominal  Attr5         -         numerical Example: Cluster 1 from d1: Car, 12, Diesel, Black, 24 Car, 13, Diesel, Blue, 2200 Truck, 12, Diesel, Black, 24 Car, 12, Diesel, Black, 82 Cluster 1 from d2: Car, 12, Gasoline, Black, 24 Truck, 12, Diesel, Black, 24 Car, 12, Diesel, Black, 82 If possible, I would like to have a value of similarity (between 2 clusters) between 0 and 1 or a percentage of similarity. I assume that two clusters are similar if they have close numbers (if numeric type) and equal values (in nominal type)","I have generated clusters for two different datasets (d1 and d2) with Hierarchical Clustering algorithm and I would like to calculate the similarity between the clusters generated for d1 and d2.  Example: Compare d1_1 to d2_1, where ""_x"" is the cluster number Is important to note that each cluster can have different number of objects, but all clusters have the same attributes types: **Atribute name**    **data type**  Attr1         -         nominal  Attr2         -         numerical Attr3         -         nominal Attr4         -         nominal  Attr5         -         numerical Example: Cluster 1 from d1: Car, 12, Diesel, Black, 24 Car, 13, Diesel, Blue, 2200 Truck, 12, Diesel, Black, 24 Car, 12, Diesel, Black, 82 Cluster 1 from d2: Car, 12, Gasoline, Black, 24 Truck, 12, Diesel, Black, 24 Car, 12, Diesel, Black, 82 If possible, I would like to have a value of similarity (between 2 clusters) between 0 and 1 or a percentage of similarity. I assume that two clusters are similar if they have close numbers (if numeric type) and equal values (in nominal type)",,"['statistics', 'machine-learning', 'clustering']"
25,Use MGF to show $\hat\beta$ is a consistent estimator of $\beta$,Use MGF to show  is a consistent estimator of,\hat\beta \beta,"Suppose that $X_1,....,X_n$ is a random sample from a gamma distribution with parameters $\alpha= 2, \beta$ . \begin{equation} f(x)= \frac{x e^{(-x/ \beta)}}{\beta^2},  x>0 \end{equation} (a) Find the maximum likelihood estimator $\hat\beta$ of $\beta$ and show it is unbiased. (b) Use the moment generating function (MGF) to show that $\hat\beta$ is a consistent estimator of $\beta$ . I Know the answer for part (a) $\hat\beta= \frac{1}{2n} \sum {X_i}$ and then show $E[\hat\beta]= \beta$ . I need help in part (b) I know how to show that an estimator is consistent by using convergence in probability definition or   Chebyshev's Inequality. but I have no idea how to use MGF to show  an estimator is consistent.",Suppose that is a random sample from a gamma distribution with parameters . (a) Find the maximum likelihood estimator of and show it is unbiased. (b) Use the moment generating function (MGF) to show that is a consistent estimator of . I Know the answer for part (a) and then show . I need help in part (b) I know how to show that an estimator is consistent by using convergence in probability definition or   Chebyshev's Inequality. but I have no idea how to use MGF to show  an estimator is consistent.,"X_1,....,X_n \alpha= 2, \beta \begin{equation}
f(x)= \frac{x e^{(-x/ \beta)}}{\beta^2},  x>0
\end{equation} \hat\beta \beta \hat\beta \beta \hat\beta= \frac{1}{2n} \sum {X_i} E[\hat\beta]= \beta","['statistics', 'convergence-divergence', 'statistical-inference']"
26,Assigning a value to how 'ordered' a vector is,Assigning a value to how 'ordered' a vector is,,"Consider a vector which goes in ascending order from (1, 2, 3 ... N). This vector is ordered 'correctly' and I would like to assign it a score of 1, which indicates a perfectly ascending ordered vector. On the other hand, a vector (N, N-1, N-2 ... 3, 2, 1) has the 'worst' possible score of 0. In between 0 and 1, the vector can be 'scrambled' and I would like a way to calculate how 'ordered' it is compared to the perfect case of (1, 2, 3 ... N). Does anyone know of a way to calculate such a score which measures the ordered-ness of a vector?","Consider a vector which goes in ascending order from (1, 2, 3 ... N). This vector is ordered 'correctly' and I would like to assign it a score of 1, which indicates a perfectly ascending ordered vector. On the other hand, a vector (N, N-1, N-2 ... 3, 2, 1) has the 'worst' possible score of 0. In between 0 and 1, the vector can be 'scrambled' and I would like a way to calculate how 'ordered' it is compared to the perfect case of (1, 2, 3 ... N). Does anyone know of a way to calculate such a score which measures the ordered-ness of a vector?",,['statistics']
27,Multivariate Kolmogorov distance bounded by Wasserstein distance,Multivariate Kolmogorov distance bounded by Wasserstein distance,,"I'm trying to find a bound for the multivariate Kolmogorov distance in terms of the Wasserstein distance. Denoting by $F$ and $G$ two cumulative distribution functions (cdf) on $\mathbb{R}^n$ the Kolmogorov and Wasserstein distances between $F$ and $G$ are given by \begin{align} d_{K}(F,G) &:= \sup_{\mathbf{x} \in \mathbb{R}^n} \left| F(\mathbf{x})- G(\mathbf{x}) \right|,\\ d_{W_1}(F,G) &:= \inf_{M_{F,G}} \left\{ \int_{\mathbb{R}^{n}\times\mathbb{R}^{n}} \Vert \mathbf{x}-\mathbf{y} \Vert dM_{F,G}(\mathbf{x},\mathbf{y}); M_{F,G} \text{ is a cdf with margins } F \text{ and } G \right\}, \end{align} where $\Vert \mathbf{x} \Vert$ is a vector norm on $\mathbb{R}^n$ . If $G$ has a density $g$ bounded by $C := \sup_{x\in\mathbb{R}} g(x)$ we have in the univariate case the following bound (see Lemma 1 in https://statweb.stanford.edu/~souravc/Lecture2.pdf ): \begin{align} d_{K}(F,G) \leq 2 \sqrt{C} \sqrt{d_{W_1}(F,G)}. \end{align} In the multivariate ( $n$ -dimensional) case I used the same approach to get the following result. If $G$ has a density $g$ bounded by $C := \sup_{\mathbf{x}\in\mathbb{R}^n} g(x)$ and $G$ is compactly supported on a set $D \subsetneq \mathbb{R}^n$ where $K = \max_{i=1,\ldots,n} \{ \sup_{\mathbf{x,y}\in D}|x_i-y_i| \}$ is the longest possible distance in one direction in $D$ then \begin{align} d_{K}(F,G) \leq 2 \sqrt{CnK^{n-1}} \sqrt{d_{W_1}(F,G)}. \end{align} However, I would like to get rid of the bounded support assumption for $G$ (density, smoothness or moment assumptions for $F$ and $G$ would be fine) and I was wondering if anyone knows of a less restrictive result of the same flavor?","I'm trying to find a bound for the multivariate Kolmogorov distance in terms of the Wasserstein distance. Denoting by and two cumulative distribution functions (cdf) on the Kolmogorov and Wasserstein distances between and are given by where is a vector norm on . If has a density bounded by we have in the univariate case the following bound (see Lemma 1 in https://statweb.stanford.edu/~souravc/Lecture2.pdf ): In the multivariate ( -dimensional) case I used the same approach to get the following result. If has a density bounded by and is compactly supported on a set where is the longest possible distance in one direction in then However, I would like to get rid of the bounded support assumption for (density, smoothness or moment assumptions for and would be fine) and I was wondering if anyone knows of a less restrictive result of the same flavor?","F G \mathbb{R}^n F G \begin{align}
d_{K}(F,G) &:= \sup_{\mathbf{x} \in \mathbb{R}^n} \left| F(\mathbf{x})- G(\mathbf{x}) \right|,\\
d_{W_1}(F,G) &:= \inf_{M_{F,G}} \left\{ \int_{\mathbb{R}^{n}\times\mathbb{R}^{n}} \Vert \mathbf{x}-\mathbf{y} \Vert dM_{F,G}(\mathbf{x},\mathbf{y}); M_{F,G} \text{ is a cdf with margins } F \text{ and } G \right\},
\end{align} \Vert \mathbf{x} \Vert \mathbb{R}^n G g C := \sup_{x\in\mathbb{R}} g(x) \begin{align}
d_{K}(F,G) \leq 2 \sqrt{C} \sqrt{d_{W_1}(F,G)}.
\end{align} n G g C := \sup_{\mathbf{x}\in\mathbb{R}^n} g(x) G D \subsetneq \mathbb{R}^n K = \max_{i=1,\ldots,n} \{ \sup_{\mathbf{x,y}\in D}|x_i-y_i| \} D \begin{align}
d_{K}(F,G) \leq 2 \sqrt{CnK^{n-1}} \sqrt{d_{W_1}(F,G)}.
\end{align} G F G","['probability-theory', 'statistics', 'inequality', 'multiple-integral', 'optimal-transport']"
28,Using Hotelling's T-statistic to find an elliptic confidence set,Using Hotelling's T-statistic to find an elliptic confidence set,,"The problem: We have samples of sizes ${n_1} = 25,{n_2} = 15,{n_3} = 30$ drawn independently from $N\left( {{\mu _i},{\sigma ^2}} \right),i = 1,2,3$ (normal distributions with same variance). We have ${\overline x _1} = 10.5,{\overline x _2} = 14,{\overline x _1} = 12,s_1^2 = 2.5,s_2^2 = 3,s_3^2 = 2.7$ (unbiased variance estimators). Find 95% elliptic confidence set for $\left( {{z_1},{z_2}} \right) = \left( {{\mu _1} + {\mu _2} - 2{\mu _3},{\mu _1} - {\mu _2}} \right)$ by using Hotelling's distribution. My attempt: We have $$\left( {{X_{1i}},{X_{2j}},{X_{3k}}} \right)\sim N\left( {\left( {\begin{array}{*{20}{c}}   {{\mu _1}} \\    {{\mu _2}} \\    {{\mu _3}}  \end{array}} \right),\left[ {\begin{array}{*{20}{c}}   {{\sigma ^2}}&0&0 \\    0&{{\sigma ^2}}&0 \\    0&0&{{\sigma ^2}}  \end{array}} \right]} \right) \Rightarrow \left( {{{\overline X }_1},{{\overline X }_2},{{\overline X }_3}} \right)\sim N\left( {\underbrace {\left( {\begin{array}{*{20}{c}}   {{\mu _1}} \\    {{\mu _2}} \\    {{\mu _3}}  \end{array}} \right)}_\mu ,\underbrace {\left[ {\begin{array}{*{20}{c}}   {{\sigma ^2}/{n_1}}&0&0 \\    0&{{\sigma ^2}/{n_2}}&0 \\    0&0&{{\sigma ^2}/{n_3}}  \end{array}} \right]}_\Sigma } \right)$$ So $$\left( {{Z_1},{Z_2}} \right) = \left( {{{\overline X }_1} + {{\overline X }_2} - 2{{\overline X }_3},{{\overline X }_1} - {{\overline X }_2}} \right) = \underbrace {\left( {\begin{array}{*{20}{c}}   1&1&{ - 2} \\    1&{ - 1}&0  \end{array}} \right)}_B\left( {\begin{array}{*{20}{c}}   {{{\overline X }_1}} \\    {{{\overline X }_2}} \\    {{{\overline X }_3}}  \end{array}} \right)\sim N\left( {B\mu ,B\Sigma B'} \right)$$ and $$\left( {{Z_1},{Z_2}} \right)\sim N\left( {\left( {\begin{array}{*{20}{c}}   {{\mu _1} + {\mu _2} - 2{\mu _3}} \\    {{\mu _1} - {\mu _2}}  \end{array}} \right),\left( {\begin{array}{*{20}{c}}   {{\sigma ^2}\left( {\frac{1}{{{n_1}}} + \frac{1}{{{n_2}}} + \frac{4}{{{n_3}}}} \right)}&{{\sigma ^2}\left( {\frac{1}{{{n_1}}} - \frac{1}{{{n_2}}}} \right)} \\    {{\sigma ^2}\left( {\frac{1}{{{n_1}}} - \frac{1}{{{n_2}}}} \right)}&{{\sigma ^2}\left( {\frac{1}{{{n_1}}} + \frac{1}{{{n_2}}}} \right)}  \end{array}} \right)} \right)$$ The only estimator for ${{\sigma ^2}}$ that comes to mind is the pooled variance ${\widehat \sigma ^2} = s_p^2 = \frac{{\sum\limits_{i = 1}^3 {\left( {{n_i} - 1} \right)s_i^2} }}{{\sum\limits_{i = 1}^3 {\left( {{n_i} - 1} \right)} }}$ , but I don't see how to get from there to Hotelling's distribution. I'm assuming that $S = s_p^2\left( {\begin{array}{*{20}{c}}   {\frac{1}{{{n_1}}} + \frac{1}{{{n_2}}} + \frac{4}{{{n_3}}}}&{\frac{1}{{{n_1}}} - \frac{1}{{{n_2}}}} \\    {\frac{1}{{{n_1}}} - \frac{1}{{{n_2}}}}&{\frac{1}{{{n_1}}} + \frac{1}{{{n_2}}}}  \end{array}} \right)$ does not follow a Wishart distribution. How would I find the required elliptic confidence set for the linear combination of expectations with unknown (and common) variance from those independent normally distributed samples? EDIT: We also know that $\sum\limits_{i = 1}^3 {\left( {{n_i} - 1} \right)\frac{{S_i^2}}{{{\sigma ^2}}}} \sim {\chi ^2}\left( {\sum\limits_{i = 1}^3 {\left( {{n_i} - 1} \right)} } \right)$ , which further makes me suspect that Hotelling's distribution will not play a role in the solution. Still, I don't see which test statistic to employ (I'm guessing ${\left( {\left( {{z_1},{z_2}} \right) - B\mu } \right)^\prime }{S^{ - 1}}\left( {\left( {{z_1},{z_2}} \right) - B\mu } \right)$ ) and which distribution would it follow. EDIT2: Also see here","The problem: We have samples of sizes drawn independently from (normal distributions with same variance). We have (unbiased variance estimators). Find 95% elliptic confidence set for by using Hotelling's distribution. My attempt: We have So and The only estimator for that comes to mind is the pooled variance , but I don't see how to get from there to Hotelling's distribution. I'm assuming that does not follow a Wishart distribution. How would I find the required elliptic confidence set for the linear combination of expectations with unknown (and common) variance from those independent normally distributed samples? EDIT: We also know that , which further makes me suspect that Hotelling's distribution will not play a role in the solution. Still, I don't see which test statistic to employ (I'm guessing ) and which distribution would it follow. EDIT2: Also see here","{n_1} = 25,{n_2} = 15,{n_3} = 30 N\left( {{\mu _i},{\sigma ^2}} \right),i = 1,2,3 {\overline x _1} = 10.5,{\overline x _2} = 14,{\overline x _1} = 12,s_1^2 = 2.5,s_2^2 = 3,s_3^2 = 2.7 \left( {{z_1},{z_2}} \right) = \left( {{\mu _1} + {\mu _2} - 2{\mu _3},{\mu _1} - {\mu _2}} \right) \left( {{X_{1i}},{X_{2j}},{X_{3k}}} \right)\sim N\left( {\left( {\begin{array}{*{20}{c}}
  {{\mu _1}} \\ 
  {{\mu _2}} \\ 
  {{\mu _3}} 
\end{array}} \right),\left[ {\begin{array}{*{20}{c}}
  {{\sigma ^2}}&0&0 \\ 
  0&{{\sigma ^2}}&0 \\ 
  0&0&{{\sigma ^2}} 
\end{array}} \right]} \right) \Rightarrow \left( {{{\overline X }_1},{{\overline X }_2},{{\overline X }_3}} \right)\sim N\left( {\underbrace {\left( {\begin{array}{*{20}{c}}
  {{\mu _1}} \\ 
  {{\mu _2}} \\ 
  {{\mu _3}} 
\end{array}} \right)}_\mu ,\underbrace {\left[ {\begin{array}{*{20}{c}}
  {{\sigma ^2}/{n_1}}&0&0 \\ 
  0&{{\sigma ^2}/{n_2}}&0 \\ 
  0&0&{{\sigma ^2}/{n_3}} 
\end{array}} \right]}_\Sigma } \right) \left( {{Z_1},{Z_2}} \right) = \left( {{{\overline X }_1} + {{\overline X }_2} - 2{{\overline X }_3},{{\overline X }_1} - {{\overline X }_2}} \right) = \underbrace {\left( {\begin{array}{*{20}{c}}
  1&1&{ - 2} \\ 
  1&{ - 1}&0 
\end{array}} \right)}_B\left( {\begin{array}{*{20}{c}}
  {{{\overline X }_1}} \\ 
  {{{\overline X }_2}} \\ 
  {{{\overline X }_3}} 
\end{array}} \right)\sim N\left( {B\mu ,B\Sigma B'} \right) \left( {{Z_1},{Z_2}} \right)\sim N\left( {\left( {\begin{array}{*{20}{c}}
  {{\mu _1} + {\mu _2} - 2{\mu _3}} \\ 
  {{\mu _1} - {\mu _2}} 
\end{array}} \right),\left( {\begin{array}{*{20}{c}}
  {{\sigma ^2}\left( {\frac{1}{{{n_1}}} + \frac{1}{{{n_2}}} + \frac{4}{{{n_3}}}} \right)}&{{\sigma ^2}\left( {\frac{1}{{{n_1}}} - \frac{1}{{{n_2}}}} \right)} \\ 
  {{\sigma ^2}\left( {\frac{1}{{{n_1}}} - \frac{1}{{{n_2}}}} \right)}&{{\sigma ^2}\left( {\frac{1}{{{n_1}}} + \frac{1}{{{n_2}}}} \right)} 
\end{array}} \right)} \right) {{\sigma ^2}} {\widehat \sigma ^2} = s_p^2 = \frac{{\sum\limits_{i = 1}^3 {\left( {{n_i} - 1} \right)s_i^2} }}{{\sum\limits_{i = 1}^3 {\left( {{n_i} - 1} \right)} }} S = s_p^2\left( {\begin{array}{*{20}{c}}
  {\frac{1}{{{n_1}}} + \frac{1}{{{n_2}}} + \frac{4}{{{n_3}}}}&{\frac{1}{{{n_1}}} - \frac{1}{{{n_2}}}} \\ 
  {\frac{1}{{{n_1}}} - \frac{1}{{{n_2}}}}&{\frac{1}{{{n_1}}} + \frac{1}{{{n_2}}}} 
\end{array}} \right) \sum\limits_{i = 1}^3 {\left( {{n_i} - 1} \right)\frac{{S_i^2}}{{{\sigma ^2}}}} \sim {\chi ^2}\left( {\sum\limits_{i = 1}^3 {\left( {{n_i} - 1} \right)} } \right) {\left( {\left( {{z_1},{z_2}} \right) - B\mu } \right)^\prime }{S^{ - 1}}\left( {\left( {{z_1},{z_2}} \right) - B\mu } \right)","['statistics', 'normal-distribution', 'confidence-interval']"
29,How is the continuous input probabalistic generative model derived from single class model?,How is the continuous input probabalistic generative model derived from single class model?,,"So for the single valued model we have: $p(C_1|\textbf{x}) = \frac{p(\textbf{x}|C_1)p(C_1)}{p(\textbf{x}|C_1)p(C_1)+p(\textbf{x}|C_2)p(C_2)}$ If we rearrange the terms, we can write this as a sigmoid function: $\frac{1}{1+exp(-a)}=\sigma(a)$ ...(4.57) where $a=ln \frac{p(\textbf{x}|C_1)p(C_1)}{p(\textbf{x}|C_2)p(C_2)}$ ...(4.58) Then we moved onto the continuous input case where we assumed $p(C_k|\textbf{x})$ was gaussian: $p(\textbf{x}|C_k) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp(-\frac{1}{2}(\textbf{x}-\mu_k)^T\Sigma^{-1}(\textbf{x}-\mu_k))$ ...(4.64) Then I became confused when the text said, using 4.57 and 4.58 we have: $p(C_1|\textbf{x}) = \sigma(\textbf{w}^T\textbf{x}+w_0)$ where: $\textbf{w} = \Sigma ^{-1}(\mu_1-\mu_2)$ $w_0=-\frac{1}{2}\mu_1^T\Sigma^{-1}\mu_1+\frac{1}{2}\mu_2^T\Sigma^{-1}\mu_2+ln\frac{p(C_1)}{p(C_2)}$ Is it saying that if I plug everything in the sigmoid I will recover from it $p(C_1|\textbf{x}) = \frac{p(\textbf{x}|C_1)p(C_1)}{p(\textbf{x}|C_1)p(C_1)+p(\textbf{x}|C_2)p(C_2)}$ but $p(x|C_1)$ and $p(x|C_2)$ are the normal distributions like in 4.64? Why can't we just use the Bayes theorem as is? Why do we have create a sigmoid function out of seemingly no where?","So for the single valued model we have: If we rearrange the terms, we can write this as a sigmoid function: ...(4.57) where ...(4.58) Then we moved onto the continuous input case where we assumed was gaussian: ...(4.64) Then I became confused when the text said, using 4.57 and 4.58 we have: where: Is it saying that if I plug everything in the sigmoid I will recover from it but and are the normal distributions like in 4.64? Why can't we just use the Bayes theorem as is? Why do we have create a sigmoid function out of seemingly no where?",p(C_1|\textbf{x}) = \frac{p(\textbf{x}|C_1)p(C_1)}{p(\textbf{x}|C_1)p(C_1)+p(\textbf{x}|C_2)p(C_2)} \frac{1}{1+exp(-a)}=\sigma(a) a=ln \frac{p(\textbf{x}|C_1)p(C_1)}{p(\textbf{x}|C_2)p(C_2)} p(C_k|\textbf{x}) p(\textbf{x}|C_k) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp(-\frac{1}{2}(\textbf{x}-\mu_k)^T\Sigma^{-1}(\textbf{x}-\mu_k)) p(C_1|\textbf{x}) = \sigma(\textbf{w}^T\textbf{x}+w_0) \textbf{w} = \Sigma ^{-1}(\mu_1-\mu_2) w_0=-\frac{1}{2}\mu_1^T\Sigma^{-1}\mu_1+\frac{1}{2}\mu_2^T\Sigma^{-1}\mu_2+ln\frac{p(C_1)}{p(C_2)} p(C_1|\textbf{x}) = \frac{p(\textbf{x}|C_1)p(C_1)}{p(\textbf{x}|C_1)p(C_1)+p(\textbf{x}|C_2)p(C_2)} p(x|C_1) p(x|C_2),"['statistics', 'machine-learning', 'bayesian']"
30,"The steps to get $\beta$ in Least Square Estimation, why $x_i$ was removed?","The steps to get  in Least Square Estimation, why  was removed?",\beta x_i,Please see the following steps Get $\beta$ in Least Square Estimation:,Please see the following steps Get in Least Square Estimation:,\beta,"['statistics', 'regression', 'least-squares']"
31,Probability and Statistical Modelling Proof (Negative Binomial into Poisson),Probability and Statistical Modelling Proof (Negative Binomial into Poisson),,"The Negative Binomial RV $X$ models the number of trials until the $r$ -th success in a sequence of independent Bernoulli Trials with probability of success $p$ in each trial. So, if $q = 1 - p$ , $$P(X = k)= \begin{pmatrix} k-1 \\r-1 \end{pmatrix}p^rq^{k-r},k=r,r+1,\dots.$$ Let $Y = X - r$ . Suppose that $r\to\infty$ and $q\to 0$ so that $rq\to\lambda$ . Show that, for fixed $m=0,1,\dots$ $$P(Y=m)\longrightarrow\frac{\lambda^m}{m!}e^{-\lambda}.$$ So what I did so far was Let $r=\frac{\lambda}{q}$ ,with $q\to 0$ Then $P(Y=m) = P(X=r+m)$ So I got $$P(X = r+m)=\begin{pmatrix}\frac{\lambda}{q}+m-1\\ \frac{\lambda}{q}-1 \end{pmatrix} p^{\frac{\lambda}{q}}q^{m}, m=0,1,\dots.$$ and I don't know what to do next, can someone help me out ?","The Negative Binomial RV models the number of trials until the -th success in a sequence of independent Bernoulli Trials with probability of success in each trial. So, if , Let . Suppose that and so that . Show that, for fixed So what I did so far was Let ,with Then So I got and I don't know what to do next, can someone help me out ?","X r p q = 1 - p P(X = k)= \begin{pmatrix} k-1 \\r-1 \end{pmatrix}p^rq^{k-r},k=r,r+1,\dots. Y = X - r r\to\infty q\to 0 rq\to\lambda m=0,1,\dots P(Y=m)\longrightarrow\frac{\lambda^m}{m!}e^{-\lambda}. r=\frac{\lambda}{q} q\to 0 P(Y=m) = P(X=r+m) P(X = r+m)=\begin{pmatrix}\frac{\lambda}{q}+m-1\\ \frac{\lambda}{q}-1 \end{pmatrix} p^{\frac{\lambda}{q}}q^{m}, m=0,1,\dots.","['probability', 'statistics', 'poisson-distribution', 'binomial-distribution', 'negative-binomial']"
32,Find the $P(\theta - 0.1 \leq MLE \leq \theta + 0.1)$,Find the,P(\theta - 0.1 \leq MLE \leq \theta + 0.1),I have a continuous random variable with density: $f(x|\theta) = \frac{\theta}{x^{\theta+1}}$ . I have calculated the MLE( $\hat\theta$ ) as: $\frac{n}{\sum_{i}^{n}log(x_i)}$ I am stuck on figuring out how to estimate $P(\theta - 0.1 \leq \hat\theta \leq \theta + 0.1)$ ? Thank you.,I have a continuous random variable with density: . I have calculated the MLE( ) as: I am stuck on figuring out how to estimate ? Thank you.,f(x|\theta) = \frac{\theta}{x^{\theta+1}} \hat\theta \frac{n}{\sum_{i}^{n}log(x_i)} P(\theta - 0.1 \leq \hat\theta \leq \theta + 0.1),"['probability', 'statistics', 'maximum-likelihood']"
33,Chi square goodness of fit test for Exp(1) in r,Chi square goodness of fit test for Exp(1) in r,,"Our teacher has posted the following task: We have 100 random numbers in our disposal that come from Exp(1) and the task is to perform the chi square test in order to check if the numbers come indeed from Exp(1)(he basically wants us to get more confortable with the r environment,so we have to look for everything by ourselves*). Although i know how to perform the test and answer about the null hypothesis based on the pvalue, i have no clue on how to separate the interval from zero to infinity(in the r environment)We need that in order to calculate the probabilities. I also have in mind that we need all our expected values to be bigger than 5. Any ideas? *we are amateurs who started using r one week ago,we are familiar with basic stuff only.","Our teacher has posted the following task: We have 100 random numbers in our disposal that come from Exp(1) and the task is to perform the chi square test in order to check if the numbers come indeed from Exp(1)(he basically wants us to get more confortable with the r environment,so we have to look for everything by ourselves*). Although i know how to perform the test and answer about the null hypothesis based on the pvalue, i have no clue on how to separate the interval from zero to infinity(in the r environment)We need that in order to calculate the probabilities. I also have in mind that we need all our expected values to be bigger than 5. Any ideas? *we are amateurs who started using r one week ago,we are familiar with basic stuff only.",,"['statistics', 'chi-squared']"
34,Bias and variance of IV estimation,Bias and variance of IV estimation,,"NOTE: I have asked this question on stat.stackoverflow but got no answers/comments. Hence I decide to ask it on the math.stackoverflow platform as well. I'm studying IV estimation by myself and have some confusion about the basics. Let $y=X\beta_0 + u$ be a linear model with endogenous variable $X$ , and $Z$ be an instrument, meaning that $Z$ and $u$ are uncorrelated, and $Z$ and $X$ are correlated. For simplicity let's say both $X$ and $Z$ are univariate. The IV estimation takes the form of $$ \widehat\beta = \beta_0 + (Z'X)^{-1}Z'u. $$ I understand that $\widehat\beta$ is consistent because $Z'u\overset{p}{\to} 0$ , $Z'X\overset{p}{\to} a\neq 0$ and therefore by Slutsky's theorem $\widehat\beta\overset{p}{\to}\beta_0$ . However, I'm having some trouble on other properties of $\widehat\beta$ . Is $\widehat\beta$ unbiased? For me it seems $\widehat\beta$ is not unbiased, because $Z'u$ and $Z'X$ are correlated (are they?) and therefore $\mathbb E[Z'u]=0$ does not mean $\mathbb E[(Z'X)^{-1}Z'u]=0$ ? If $\widehat\beta$ is indeed biased, how do we obtain the (asymptotic) variance of $\widehat\beta$ ? If we calculate by definition then $\mathbb E|\widehat\beta-\beta_0|^2 = \mathbb E[(Z'X)^{-1}Z'uu'Z(Z'X)^{-1}]$ but $u$ is not uncorrelated with $(Z'X)^{-1}Z'$ , and how do we simplify this expression?","NOTE: I have asked this question on stat.stackoverflow but got no answers/comments. Hence I decide to ask it on the math.stackoverflow platform as well. I'm studying IV estimation by myself and have some confusion about the basics. Let be a linear model with endogenous variable , and be an instrument, meaning that and are uncorrelated, and and are correlated. For simplicity let's say both and are univariate. The IV estimation takes the form of I understand that is consistent because , and therefore by Slutsky's theorem . However, I'm having some trouble on other properties of . Is unbiased? For me it seems is not unbiased, because and are correlated (are they?) and therefore does not mean ? If is indeed biased, how do we obtain the (asymptotic) variance of ? If we calculate by definition then but is not uncorrelated with , and how do we simplify this expression?","y=X\beta_0 + u X Z Z u Z X X Z 
\widehat\beta = \beta_0 + (Z'X)^{-1}Z'u.
 \widehat\beta Z'u\overset{p}{\to} 0 Z'X\overset{p}{\to} a\neq 0 \widehat\beta\overset{p}{\to}\beta_0 \widehat\beta \widehat\beta \widehat\beta Z'u Z'X \mathbb E[Z'u]=0 \mathbb E[(Z'X)^{-1}Z'u]=0 \widehat\beta \widehat\beta \mathbb E|\widehat\beta-\beta_0|^2 = \mathbb E[(Z'X)^{-1}Z'uu'Z(Z'X)^{-1}] u (Z'X)^{-1}Z'","['statistics', 'linear-regression']"
35,Suppose 55% of a group a students can pass an exam...,Suppose 55% of a group a students can pass an exam...,,"Suppose 55% of a grouo of students can pass an exam, if 100 students are randomly selected, what is the probability that at least 50 but less than 60 students pass the exam? Use histogram correction. This question was given to me as a review for an upcoming exam. My math: $\overline{x} = 100*0.55 = 55$ $\sigma = \sqrt{np(1-p)}= \sqrt{100*0.55(0.45)} = 4.97493$ What we need to calculate: $P( 50 \leq x \lt 60)$ $=P(X < 60) - P(X < 50)$ $=P(Z< \frac{\overline{x}-correction-X}{\sigma})-P(Z< \frac{\overline{x}-correction-X}{\sigma})$ $=P(Z< \frac{60-0.5-55}{4.97493})-P(Z< \frac{50-0.5-55}{4.97493})$ $=P(Z < 0.9045) - P(Z < -1.1055)$ $=0.81713-0.13447$ = 0.6826 or 68.26% Is there anything glaringly wrong with my calculations and did I use the histogram correction correct?","Suppose 55% of a grouo of students can pass an exam, if 100 students are randomly selected, what is the probability that at least 50 but less than 60 students pass the exam? Use histogram correction. This question was given to me as a review for an upcoming exam. My math: What we need to calculate: = 0.6826 or 68.26% Is there anything glaringly wrong with my calculations and did I use the histogram correction correct?",\overline{x} = 100*0.55 = 55 \sigma = \sqrt{np(1-p)}= \sqrt{100*0.55(0.45)} = 4.97493 P( 50 \leq x \lt 60) =P(X < 60) - P(X < 50) =P(Z< \frac{\overline{x}-correction-X}{\sigma})-P(Z< \frac{\overline{x}-correction-X}{\sigma}) =P(Z< \frac{60-0.5-55}{4.97493})-P(Z< \frac{50-0.5-55}{4.97493}) =P(Z < 0.9045) - P(Z < -1.1055) =0.81713-0.13447,"['probability', 'statistics']"
36,Is a distribution whose mean and variance are its only non-zero cumulants neccesarily Gaussian?,Is a distribution whose mean and variance are its only non-zero cumulants neccesarily Gaussian?,,"I have a distribution. Its cumulants are all zero except for its mean and variance. Does that necessarily mean that my distribution is Gaussian? Or in other words, are there non-Gaussian distributions whose only non-zero cumulants are mean and variance.","I have a distribution. Its cumulants are all zero except for its mean and variance. Does that necessarily mean that my distribution is Gaussian? Or in other words, are there non-Gaussian distributions whose only non-zero cumulants are mean and variance.",,"['probability', 'statistics', 'probability-distributions', 'cumulants']"
37,90% Confidence Interval,90% Confidence Interval,,"A random sample of $504$ out of $1600$ students in a school play soccer. How large should the sample of students be if a $90$ % confidence interval is desired with a margin of error of $2.2$ percentage points for the proportion of students that play soccer in the school? My main problem here is the interpretation of margin of error. The confidence interval I came up with was $\frac{504}{16000}+-1.645\sqrt{\frac{\left(\frac{8631}{40000}\right)}{n}}$ . The $+$ and $-$ back to back represent a ""plus or minus"". $1.645$ is the respective $z$ value for a $90$ % confidence interval and $\left(\frac{8631}{40000}\right)$ $=\frac{504}{1600}\left(1-\frac{504}{1600}\right)$ is the variance I calculated. I thought then that I needed to find a value for $1.645\sqrt{\frac{\left(\frac{8631}{40000}\right)}{n}}$ that was $2.2$ % of $\frac{504}{1600}$ . This means that I got a final $n$ value of approximately $12158$ , but I am not sure if this is what is required in this situation. If anyone knows if this is the correct approach or not, I would greatly appreciate the input!","A random sample of out of students in a school play soccer. How large should the sample of students be if a % confidence interval is desired with a margin of error of percentage points for the proportion of students that play soccer in the school? My main problem here is the interpretation of margin of error. The confidence interval I came up with was . The and back to back represent a ""plus or minus"". is the respective value for a % confidence interval and is the variance I calculated. I thought then that I needed to find a value for that was % of . This means that I got a final value of approximately , but I am not sure if this is what is required in this situation. If anyone knows if this is the correct approach or not, I would greatly appreciate the input!",504 1600 90 2.2 \frac{504}{16000}+-1.645\sqrt{\frac{\left(\frac{8631}{40000}\right)}{n}} + - 1.645 z 90 \left(\frac{8631}{40000}\right) =\frac{504}{1600}\left(1-\frac{504}{1600}\right) 1.645\sqrt{\frac{\left(\frac{8631}{40000}\right)}{n}} 2.2 \frac{504}{1600} n 12158,"['statistics', 'confidence-interval']"
38,Basic Chi square problem,Basic Chi square problem,,How do I calculate $P(S^2 > 1.8307(\mathrm{PopVariance}))$ if $n =11$ ? I think I should use the Chi square formula: $$X^2 = \frac{(n-1)s^2}{\mathrm{PopVariance}}$$ But I can't really understand the textbook application of this formula.,How do I calculate if ? I think I should use the Chi square formula: But I can't really understand the textbook application of this formula.,P(S^2 > 1.8307(\mathrm{PopVariance})) n =11 X^2 = \frac{(n-1)s^2}{\mathrm{PopVariance}},"['statistics', 'variance']"
39,Sum of random variables dependent on an ODE,Sum of random variables dependent on an ODE,,"Let us assume we divide a piece of membrane into $n$ parts. Every part contributes with a specific electric potential, which is represented by $X_i\ $ $(i=1,\dots,n)$ random variables. So with probability $p_i$ the part $X_i$ is sending an impulse (mostly this impulse should be in the range of $[0,1]$ ). All $X_i$ are independent - maybe we can assume a $N(0,1)$ distribution. The $X_i$ are dependent on the total membrane voltage $V(t)$ . I want to show that the standard deviation of the sum $$\sum_{i=1}^{n}X_i$$ grows proportional to $\sqrt{n}$ . I can show that the standard deviation grows proportional to $\sqrt{n}$ , when neglecting the dependency on $V(t)$ . But I don't know how to deal with the dependency on a function. Maybe it would be a good idea to model the dependency regarding $dV/dt$ ? Shall I look at $\sum_{i=1}^{n}V(X_i)$ ? I don't know how to handle this issue.","Let us assume we divide a piece of membrane into parts. Every part contributes with a specific electric potential, which is represented by random variables. So with probability the part is sending an impulse (mostly this impulse should be in the range of ). All are independent - maybe we can assume a distribution. The are dependent on the total membrane voltage . I want to show that the standard deviation of the sum grows proportional to . I can show that the standard deviation grows proportional to , when neglecting the dependency on . But I don't know how to deal with the dependency on a function. Maybe it would be a good idea to model the dependency regarding ? Shall I look at ? I don't know how to handle this issue.","n X_i\  (i=1,\dots,n) p_i X_i [0,1] X_i N(0,1) X_i V(t) \sum_{i=1}^{n}X_i \sqrt{n} \sqrt{n} V(t) dV/dt \sum_{i=1}^{n}V(X_i)","['probability', 'probability-theory', 'statistics']"
40,Rademacher complexity definition,Rademacher complexity definition,,"I am learning the Rademacher complexity and have a question on the definition of it. Let $\mathcal{H}$ be a hypothesis class and $m$ is the number of iid samples from a distribution $\mathcal{D}$ , say, $S=\{x_i\}_{i=1}^m$ . Then the Rademacher complexity is defined to be $$ R_s(\mathcal{H}) := \mathbb{E}_{\sigma} \sup_{h \in \mathcal{H}} \frac{1}{m}\left|\sum_{i=1}^m \sigma_i \ell(h,x_i) \right|, $$ where $\sigma_i$ 's are iid uniform on $\{-1, 1\}$ . It seems that the more appropreate definition should be $$ \tilde{R}_s(\mathcal{H}) := \mathbb{E}_{\sigma} \sup_{h \in \mathcal{H}}\left| \frac{1}{|S_+|}\sum_{i \in S_+} \ell(h,x_i) - \frac{1}{|S_-|}\sum_{i \in S_-} \ell(h,x_i)\right|, $$ where $S_+ = \{i \in [m] | \sigma_i = 1\}$ and $S_+ = \{i \in [m] | \sigma_i = -1\}$ . This is because the cross-validation should take the normalization constants into account. But I am not sure why the definition does not reflect the normalization constants. Any comments/suggestions/answers will be very appreciated.","I am learning the Rademacher complexity and have a question on the definition of it. Let be a hypothesis class and is the number of iid samples from a distribution , say, . Then the Rademacher complexity is defined to be where 's are iid uniform on . It seems that the more appropreate definition should be where and . This is because the cross-validation should take the normalization constants into account. But I am not sure why the definition does not reflect the normalization constants. Any comments/suggestions/answers will be very appreciated.","\mathcal{H} m \mathcal{D} S=\{x_i\}_{i=1}^m 
R_s(\mathcal{H}) := \mathbb{E}_{\sigma} \sup_{h \in \mathcal{H}} \frac{1}{m}\left|\sum_{i=1}^m \sigma_i \ell(h,x_i) \right|,
 \sigma_i \{-1, 1\} 
\tilde{R}_s(\mathcal{H}) := \mathbb{E}_{\sigma} \sup_{h \in \mathcal{H}}\left| \frac{1}{|S_+|}\sum_{i \in S_+} \ell(h,x_i) - \frac{1}{|S_-|}\sum_{i \in S_-} \ell(h,x_i)\right|,
 S_+ = \{i \in [m] | \sigma_i = 1\} S_+ = \{i \in [m] | \sigma_i = -1\}","['statistics', 'machine-learning']"
41,How to reconcile difference in notation used in probability and statistics by different authors,How to reconcile difference in notation used in probability and statistics by different authors,,"After learning probability for so many years, I still have trouble with the notation whenever I encounter a new reference. I have pinpointed my confusion to this ""two-culture"" of probability: one is the probability done by often-times engineers and people who write introductory textbooks on probability (e.g., Sheldon Ross), the other is done by statisticians and more recently practitioners of machine learning. For the former (engineers and textbook writers), random variables are denoted as $X$ , pdf are written as $f_X(x)$ , jointed pdf are written as $f_{X,Y}(x,y)$ , where $x,y$ are in the range of random variables $X$ and $Y$ . Example: Sheldon Ross, Papolis and Pillai, Leon-Garcia, Bertsekas and Tsitsiklis, Feller, Kobayashi I found some notes online to illustrate this first culture of probability For statistics and machine learning, random variables are not denoted as anything, they are suppressed, pdf is written as $p(x)$ , where $x$ is the realization of this underlying unspecified random variable, joint pdf are written as $p(x,y)$ . Capital letters are almost never used, always lowercase. However, sometimes lower case letter is used to denote a random variable, e.g., $x \sim \mathcal{N}(\mu, \Sigma)$ Example: Bishop, David MacKay, Hastie, Mohri I found some online notes to illustrate this second culture of probability Am I correct in my assessment? Can someone who is familiar with this ""two-culture"" of probability provide a possible reconciliation between these notations?","After learning probability for so many years, I still have trouble with the notation whenever I encounter a new reference. I have pinpointed my confusion to this ""two-culture"" of probability: one is the probability done by often-times engineers and people who write introductory textbooks on probability (e.g., Sheldon Ross), the other is done by statisticians and more recently practitioners of machine learning. For the former (engineers and textbook writers), random variables are denoted as , pdf are written as , jointed pdf are written as , where are in the range of random variables and . Example: Sheldon Ross, Papolis and Pillai, Leon-Garcia, Bertsekas and Tsitsiklis, Feller, Kobayashi I found some notes online to illustrate this first culture of probability For statistics and machine learning, random variables are not denoted as anything, they are suppressed, pdf is written as , where is the realization of this underlying unspecified random variable, joint pdf are written as . Capital letters are almost never used, always lowercase. However, sometimes lower case letter is used to denote a random variable, e.g., Example: Bishop, David MacKay, Hastie, Mohri I found some online notes to illustrate this second culture of probability Am I correct in my assessment? Can someone who is familiar with this ""two-culture"" of probability provide a possible reconciliation between these notations?","X f_X(x) f_{X,Y}(x,y) x,y X Y p(x) x p(x,y) x \sim \mathcal{N}(\mu, \Sigma)","['probability', 'statistics', 'soft-question', 'self-learning', 'machine-learning']"
42,Sample Space of a Geometric Distribution,Sample Space of a Geometric Distribution,,"Let's say I have an experiment where I repeatedly toss a coin; each toss is independent. I would like to define a random variable $X: \omega \rightarrow R$ . $X$ is the number of failures before the first success. I would like to visualize the sample space of this experiment, but I'm having trouble. Is the sample space an infinite set containing potentially infinitely long sequences?","Let's say I have an experiment where I repeatedly toss a coin; each toss is independent. I would like to define a random variable . is the number of failures before the first success. I would like to visualize the sample space of this experiment, but I'm having trouble. Is the sample space an infinite set containing potentially infinitely long sequences?",X: \omega \rightarrow R X,"['probability-theory', 'statistics', 'probability-distributions']"
43,Is $T_n$ an unbiased estimator of $\theta$? Prove your answer.,Is  an unbiased estimator of ? Prove your answer.,T_n \theta,"Let $(x_1, x_2, \dots, x_n)$ be an observed sample from a distribution with probability density function given by $$f(x) =\begin{cases}\displaystyle\frac{1}{5\theta+8}&\text{if }0 \leq x \leq 5 \theta + 8\\0&\text{otherwise. }\end{cases}$$ Where $\theta \in \mathbb R^+$ is an unknown parameter. It can be shown that $T_n = \frac{X_{(n)}-8}{5}$ is an $MLE$ of $\theta$ , where $X_{(n)}$ is the $n^{th}$ order statistic. $\big($ i.e. max $\{X_1, X_2, \dots, X_n\}$$\big)$ . (a) Is $T_n$ an unbiased estimator of $\theta$ ? Prove your answer. (b) Is $T_n$ asymptotically unbiased for $\theta$ ? Give reasons for your answer. (c) Is $T_n$ consistent for $\theta$ in probability? Prove your answer. ATTEMPT $(a)$ $$F_{X}(x) = \int_{0}^{x} f(t)dt = \int_{0}^{x} \frac{1}{5 \theta +8} dt = \frac{1}{5\theta+8}t|_{0}^{x} = \frac{1}{5\theta+8}x$$ $$F_{X_{(n)}}(x) = \left(\frac{x}{5\theta+8}\right)^n$$ $$f_{X_{(n)}}(x) = \frac{dF_{X_{(n)}}(x)}{dx} = \frac{n}{(5 \theta + 8)^n}x^{n-1}, 0 \leq x \leq 5 \theta + 8$$ $$\mathsf{E}T_n = \mathsf{E}\left[\left(\frac{X_{(n)} - 8}{5}\right)\right] = 1/5(\mathsf{E}(X_{(n)}) - 8)$$ $$\begin{align}\mathsf{E}(X_{(n)}) = \int_{0}^{5 \theta + 8} x f_{X_{(n)}}(x)dx &= \int_{0}^{5 \theta + 8}\frac{n}{(5 \theta + 8)^n}x^{n}dx \\ &= \frac{n}{(5 \theta + 8)^n(n+1)}x^{n+1}\bigg|_{0}^{5 \theta+8} = \frac{n(5 \theta + 8)^{n+1}}{(5 \theta + 8)^n(n+1)} = \frac{(5 \theta + 8)n}{n+1}\end{align}$$ $$\mathsf{E}T_n = 1/5(\mathsf{E}(X_{(n)}) - 8) = 1/5 \left (\frac{(5 \theta + 8)n}{n+1} - 8 \right) \neq \theta,$$ so $T_n$ is not an unbiased estimator of $\theta.$ $(b)$ Yes, because $\lim\limits_{n\to\infty} \mathsf{E}T_n = \lim\limits_{n\to\infty} 1/5 \left (\dfrac{(5 \theta + 8)n}{n+1} - 8 \right) = \theta$ $(c)$ $$\begin{align}\mathsf{E}(X^2_{(n)}) = \int_{0}^{5 \theta + 8} x^2 f_{X_{(n)}}(x)dx &= \int_{0}^{5 \theta + 8}\frac{n}{(5 \theta + 8)^n}x^{n+1}dx \\ & = \frac{n}{(5 \theta + 8)^n(n+2)}x^{n+2}\bigg|_{0}^{5 \theta+8} = \frac{n(5 \theta + 8)^{n+2}}{(5 \theta + 8)^n(n+2)}= \frac{(5 \theta + 8)^2n}{n+2}\end{align}$$ $$\begin{align}\lim\limits_{n\to\infty} \mathsf{Var}(X_{(n)}) = \lim\limits_{n\to\infty} \left(\mathsf{E}X^2_{(n)} - (\mathsf{E}X_{(n)})^2\right) = \lim\limits_{n\to\infty} \left(\frac{(5 \theta + 8)^2n}{n+2} -  \left(\frac{(5 \theta + 8)n}{n+1}\right)^2\right) = 0\end{align}$$ $T_n$ is consistent for $\theta$ in probability because $(1)$ $T_n$ is asymptotically unbiased for $\theta$ and $(2)$ $\lim\limits_{n\to\infty} \mathsf{Var}(X_{(n)}) = 0$ Is this correct?","Let be an observed sample from a distribution with probability density function given by Where is an unknown parameter. It can be shown that is an of , where is the order statistic. i.e. max . (a) Is an unbiased estimator of ? Prove your answer. (b) Is asymptotically unbiased for ? Give reasons for your answer. (c) Is consistent for in probability? Prove your answer. ATTEMPT so is not an unbiased estimator of Yes, because is consistent for in probability because is asymptotically unbiased for and Is this correct?","(x_1, x_2, \dots, x_n) f(x) =\begin{cases}\displaystyle\frac{1}{5\theta+8}&\text{if }0 \leq x \leq 5 \theta + 8\\0&\text{otherwise. }\end{cases} \theta \in \mathbb R^+ T_n = \frac{X_{(n)}-8}{5} MLE \theta X_{(n)} n^{th} \big( \{X_1, X_2, \dots, X_n\}\big) T_n \theta T_n \theta T_n \theta (a) F_{X}(x) = \int_{0}^{x} f(t)dt = \int_{0}^{x} \frac{1}{5 \theta +8} dt = \frac{1}{5\theta+8}t|_{0}^{x} = \frac{1}{5\theta+8}x F_{X_{(n)}}(x) = \left(\frac{x}{5\theta+8}\right)^n f_{X_{(n)}}(x) = \frac{dF_{X_{(n)}}(x)}{dx} = \frac{n}{(5 \theta + 8)^n}x^{n-1}, 0 \leq x \leq 5 \theta + 8 \mathsf{E}T_n = \mathsf{E}\left[\left(\frac{X_{(n)} - 8}{5}\right)\right] = 1/5(\mathsf{E}(X_{(n)}) - 8) \begin{align}\mathsf{E}(X_{(n)}) = \int_{0}^{5 \theta + 8} x f_{X_{(n)}}(x)dx &= \int_{0}^{5 \theta + 8}\frac{n}{(5 \theta + 8)^n}x^{n}dx \\ &= \frac{n}{(5 \theta + 8)^n(n+1)}x^{n+1}\bigg|_{0}^{5 \theta+8} = \frac{n(5 \theta + 8)^{n+1}}{(5 \theta + 8)^n(n+1)} = \frac{(5 \theta + 8)n}{n+1}\end{align} \mathsf{E}T_n = 1/5(\mathsf{E}(X_{(n)}) - 8) = 1/5 \left (\frac{(5 \theta + 8)n}{n+1} - 8 \right) \neq \theta, T_n \theta. (b) \lim\limits_{n\to\infty} \mathsf{E}T_n = \lim\limits_{n\to\infty} 1/5 \left (\dfrac{(5 \theta + 8)n}{n+1} - 8 \right) = \theta (c) \begin{align}\mathsf{E}(X^2_{(n)}) = \int_{0}^{5 \theta + 8} x^2 f_{X_{(n)}}(x)dx &= \int_{0}^{5 \theta + 8}\frac{n}{(5 \theta + 8)^n}x^{n+1}dx \\ & = \frac{n}{(5 \theta + 8)^n(n+2)}x^{n+2}\bigg|_{0}^{5 \theta+8} = \frac{n(5 \theta + 8)^{n+2}}{(5 \theta + 8)^n(n+2)}= \frac{(5 \theta + 8)^2n}{n+2}\end{align} \begin{align}\lim\limits_{n\to\infty} \mathsf{Var}(X_{(n)}) = \lim\limits_{n\to\infty} \left(\mathsf{E}X^2_{(n)} - (\mathsf{E}X_{(n)})^2\right) = \lim\limits_{n\to\infty} \left(\frac{(5 \theta + 8)^2n}{n+2} -  \left(\frac{(5 \theta + 8)n}{n+1}\right)^2\right) = 0\end{align} T_n \theta (1) T_n \theta (2) \lim\limits_{n\to\infty} \mathsf{Var}(X_{(n)}) = 0","['probability', 'statistics']"
44,Clarification about the $\epsilon$-net argument,Clarification about the -net argument,\epsilon,"I have been reading this paper : https://openreview.net/pdf?id=BJehNfW0- In Corollary D.1, they reference this paper : https://arxiv.org/pdf/1703.00573.pdf which in Theorem B.2 constructs an $\epsilon$ -net to get an upper bound for an expectation. I have little background in topology so I'm not sure what the authors mean when they say they use ""standard constructions"" to obtain $\log |X| \leq O(p\log(LL\varphi p/ε))$ (in theorem B.2). I'm also unsure as to where they get the probability $1 − \exp(−p)$ required for the proof from. Could someone more knowledgable clear this up for me? Thanks in Advance.","I have been reading this paper : https://openreview.net/pdf?id=BJehNfW0- In Corollary D.1, they reference this paper : https://arxiv.org/pdf/1703.00573.pdf which in Theorem B.2 constructs an -net to get an upper bound for an expectation. I have little background in topology so I'm not sure what the authors mean when they say they use ""standard constructions"" to obtain (in theorem B.2). I'm also unsure as to where they get the probability required for the proof from. Could someone more knowledgable clear this up for me? Thanks in Advance.",\epsilon \log |X| \leq O(p\log(LL\varphi p/ε)) 1 − \exp(−p),"['probability', 'general-topology', 'probability-theory', 'statistics', 'machine-learning']"
45,"Finding $P(B)$ if $A,B$ are independent and given values for $P(A),P(A\cup B)$",Finding  if  are independent and given values for,"P(B) A,B P(A),P(A\cup B)","I am trying to figure out the math to get to an answer that's given to me right away, which is $1/3$ . The question is asking what the probability of $B$ is with $P(A)=0.4$ and $P(A∪B)=0.6$ , with $A,B$ being independent events. I can't seem to come to one-third? Can someone explain it to me like I'm a three year old, since I have been on this for about a few hours? Thanks, and much appreciated! Below is the setup I keep using: $$P(A∪B)=P(A)+P(B)-P(A∩B)$$ which, when substituting in the given values and letting $P(B) = x$ , yields $$0.6 = 0.4 + x - P(A \cap B)$$ But I don't know what $P(A \cap B)$ is, I guess $0.4x$ ?","I am trying to figure out the math to get to an answer that's given to me right away, which is . The question is asking what the probability of is with and , with being independent events. I can't seem to come to one-third? Can someone explain it to me like I'm a three year old, since I have been on this for about a few hours? Thanks, and much appreciated! Below is the setup I keep using: which, when substituting in the given values and letting , yields But I don't know what is, I guess ?","1/3 B P(A)=0.4 P(A∪B)=0.6 A,B P(A∪B)=P(A)+P(B)-P(A∩B) P(B) = x 0.6 = 0.4 + x - P(A \cap B) P(A \cap B) 0.4x","['probability', 'statistics']"
46,Normal approximation of the Binomial distribution,Normal approximation of the Binomial distribution,,"So my text books says that if $X \sim B(n,p)$ with $np>5$ and $nq>5$ (where $q=1-p$ ) then $X$ can be approximated by a normal distribution  of $X\sim N(\mu,\sigma^2)$ with $\mu = E(X) = np$ and $\sigma^2 = Var(X) = npq$ So I understand that if n is very large then $X$ will roughly show a normal distribution and that $X$ will have $E(X) = np$ and $Var(X) = npq$ but why must $E(X)>5$ and $nq>5$ . and furthermore, what is $nq$ representing?","So my text books says that if with and (where ) then can be approximated by a normal distribution  of with and So I understand that if n is very large then will roughly show a normal distribution and that will have and but why must and . and furthermore, what is representing?","X \sim B(n,p) np>5 nq>5 q=1-p X X\sim N(\mu,\sigma^2) \mu = E(X) = np \sigma^2 = Var(X) = npq X X E(X) = np Var(X) = npq E(X)>5 nq>5 nq","['probability', 'statistics', 'normal-distribution', 'binomial-distribution']"
47,"How to spread probability in a ""gradient"", i.e. so that it decreases with distance from a point on a dimension?","How to spread probability in a ""gradient"", i.e. so that it decreases with distance from a point on a dimension?",,"I have data tasks which involve a little bit of math - which is really not my strong suit - hope you can help. Background Respondent has been given a question that they answer with options on a discrete 0:10 scale. One of the answers is ""the true"" response, so if each of the (11) options is assigned a (subjective) probability the sum of these probabilities must be 100%. So... Options on the scale: 0, 1 , 2 , 3, 4, 5, 6, 7, 8, 9, 10. $p(o_0)+p(o_1)+...p(o_i)...+p(o_9)+p(o_{10})$ = 1.0 = 100% What I plan to do is the following: First. If a respondent answers some option $o_i$ . I want to assign, to the respondent's option, a subjective probability, let's call it $p(o_k)$ . Then. Spread the remaining $1-p(o_k)$ probability over the other options; with the probability of each option declining as the distance from $o_k$ increase. All options must be assigned a non-zero probability. Problem What I can't solve is what function would give me the probability of an option on the scale, given: The probability of the respondent's option. The distance, on the scale, of an option from the respondent's option. That the sum of the probabilities of all options, on the scale, is 1. Example The respondent has answered option 4. I assign to option 4 a 25% chance of being true. Then there's a 75% probability of one of the other options is the true option. But what probability to assign each individual option if it is to decline with the distance (absolute difference) from 4? I hope this was sufficiently clear. Have a great day.","I have data tasks which involve a little bit of math - which is really not my strong suit - hope you can help. Background Respondent has been given a question that they answer with options on a discrete 0:10 scale. One of the answers is ""the true"" response, so if each of the (11) options is assigned a (subjective) probability the sum of these probabilities must be 100%. So... Options on the scale: 0, 1 , 2 , 3, 4, 5, 6, 7, 8, 9, 10. = 1.0 = 100% What I plan to do is the following: First. If a respondent answers some option . I want to assign, to the respondent's option, a subjective probability, let's call it . Then. Spread the remaining probability over the other options; with the probability of each option declining as the distance from increase. All options must be assigned a non-zero probability. Problem What I can't solve is what function would give me the probability of an option on the scale, given: The probability of the respondent's option. The distance, on the scale, of an option from the respondent's option. That the sum of the probabilities of all options, on the scale, is 1. Example The respondent has answered option 4. I assign to option 4 a 25% chance of being true. Then there's a 75% probability of one of the other options is the true option. But what probability to assign each individual option if it is to decline with the distance (absolute difference) from 4? I hope this was sufficiently clear. Have a great day.",p(o_0)+p(o_1)+...p(o_i)...+p(o_9)+p(o_{10}) o_i p(o_k) 1-p(o_k) o_k,"['probability', 'statistics', 'functions', 'exponential-function', 'bayesian']"
48,Why does the parameter estimator not depend on the parameter?,Why does the parameter estimator not depend on the parameter?,,"In the proof for the Cramer Rao Inequality, my book writes: $$E[\hat{\theta}] = \int{\hat{\theta}(\textbf{x})L(\theta;\textbf{x})d\textbf{x}=\theta}$$ Then differentiating both sides of this equation with respect to $\theta$ , and interchanging the order of integration and differentiation, gives $$\int\hat{\theta}\frac{\delta L}{\delta \theta}d \textbf{x}=1$$ This is because $\hat{\theta}$ does not depend on $\theta$ . But why does $\hat{\theta}$ not depend on $\theta$ ? Because i think $\hat{\theta}$ depends on the random sample which depends on $\theta$","In the proof for the Cramer Rao Inequality, my book writes: Then differentiating both sides of this equation with respect to , and interchanging the order of integration and differentiation, gives This is because does not depend on . But why does not depend on ? Because i think depends on the random sample which depends on",E[\hat{\theta}] = \int{\hat{\theta}(\textbf{x})L(\theta;\textbf{x})d\textbf{x}=\theta} \theta \int\hat{\theta}\frac{\delta L}{\delta \theta}d \textbf{x}=1 \hat{\theta} \theta \hat{\theta} \theta \hat{\theta} \theta,"['calculus', 'statistics', 'statistical-inference']"
49,$Var(f(X))$ when $Var(X)\to 0$,when,Var(f(X)) Var(X)\to 0,"I am a newcomer to measure-theoretic probability theory, and I have some confusion with how the notion of variance is handled in this framework. Intuitively speaking, for a measurable function $f:\mathbb{R}\to\mathbb{R}$ and a random variable $X$ , if $Var(X)=0$ , then $Var(f(X))$ should be $0$ , and when $Var(X)\to 0$ , the same should happen to $Var(f(X))$ . However, mathematically there seems to be some intricate issues involved with this intuition. For example, let's take $X\sim\mathcal{N}(0,\sigma^2)$ . The variance of $f(X)$ is defined by $$Var(f(X))=\int_{\mathbb{R}}f(x)^2\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x^2}{2\sigma^2}}dx - (\int_{\mathbb{R}}f(x)\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x^2}{2\sigma^2}}dx)^2$$ Two issues: $Var(f(X))$ is simply undefined when $\sigma=0$ !! When we let $\sigma\to0$ , by dominated convergence and with some manipulations, we can see that $Var(f(X))$ approaches $0$ . See, for example: Proof that the limit of the normal distribution for a standard deviation approximating 0 is the dirac delta function. . But this only works for $f$ belonging to the Schwartz space. So I am quite confused about this limitation, since we are restricted to such a small class of measurable functions, while intuitively $Var(f(X))$ should go to $0$ as $Var(X)\to 0$ for any $f$ that's not ""crazily behaving"". Now I am very confused about how variance in measure-theoretic probability works. When we talk about $\lim_{Var(X)\to0}Var(f(X))$ , are we really stuck with $f$ being a Schwartz function? What about issue 1.? Any help on clearing my confusion is greatly appreciated!","I am a newcomer to measure-theoretic probability theory, and I have some confusion with how the notion of variance is handled in this framework. Intuitively speaking, for a measurable function and a random variable , if , then should be , and when , the same should happen to . However, mathematically there seems to be some intricate issues involved with this intuition. For example, let's take . The variance of is defined by Two issues: is simply undefined when !! When we let , by dominated convergence and with some manipulations, we can see that approaches . See, for example: Proof that the limit of the normal distribution for a standard deviation approximating 0 is the dirac delta function. . But this only works for belonging to the Schwartz space. So I am quite confused about this limitation, since we are restricted to such a small class of measurable functions, while intuitively should go to as for any that's not ""crazily behaving"". Now I am very confused about how variance in measure-theoretic probability works. When we talk about , are we really stuck with being a Schwartz function? What about issue 1.? Any help on clearing my confusion is greatly appreciated!","f:\mathbb{R}\to\mathbb{R} X Var(X)=0 Var(f(X)) 0 Var(X)\to 0 Var(f(X)) X\sim\mathcal{N}(0,\sigma^2) f(X) Var(f(X))=\int_{\mathbb{R}}f(x)^2\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x^2}{2\sigma^2}}dx - (\int_{\mathbb{R}}f(x)\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{x^2}{2\sigma^2}}dx)^2 Var(f(X)) \sigma=0 \sigma\to0 Var(f(X)) 0 f Var(f(X)) 0 Var(X)\to 0 f \lim_{Var(X)\to0}Var(f(X)) f","['probability', 'probability-theory', 'statistics', 'measure-theory']"
50,How to show the following weak convergence using characteristic functions,How to show the following weak convergence using characteristic functions,,"Suppose $g:R\rightarrow R$ has at least three bounded continuous derivatives and let $X_i$ be $iid$ and in $L^2$ . Prove that: $\sqrt{n}[g(\overline{X_n}) - g(\mu)]\xrightarrow{w} N(0,g^{'}(\mu)^{2} \sigma ^2)$ i.e. weak convergence happens where $\overline{X_n} = \frac{\sum X_n}{n}$ , $\mu = EX_1$ , $\sigma^2=Var(X_1)$ My attempt: Let $\phi$ be the characteristic function/fourier transform. Weak convergence $\iff$ convergence of $\phi$ On the RHS, $N(0,g^{'}(\mu)^{2} \sigma ^2) \sim  g^{'}(\mu) \sigma N(0,1) = g^{'}(\mu) \sigma \mathscr{X}$ where $\mathscr{X}$ is standard normal $\phi_{g^{'}(\mu) \sigma  \mathscr{X}}(t) = \phi_{\mathscr{X}}(g^{'}(\mu) \sigma t) = e^{-\frac{g^{'}(\mu)^{2} \sigma ^2 t^2}{2}}$ because $\phi_{\mathscr{X}}(t) = e^{-\frac{t^2}{2}}$ On the LHS: The characteristic function is $E[e^{it\sqrt{n}g(\overline{X_n}) - it\sqrt{n}g(\mu)}] = e^{-it\sqrt{n}g(\mu)} E[e^{it\sqrt{n}g(\overline{X_n})}]$ Because RHS has $g^{'}(\mu)$ , I am thinking of expanding $e^{it\sqrt{n}g(\overline{X_n})}$ using Taylor's which is: $e^{it\sqrt{n}g(\overline{X_n})} \approx e^{it\sqrt{n}g(\mu)} + e^{it\sqrt{n}g(\mu)}it\sqrt{n}g'(\mu)(\overline{X_n}-\mu)+e^{it\sqrt{n}g(\mu)}it\sqrt{n}g'(\mu)[(it\sqrt{n}g'(\mu))^2 + it\sqrt{n}g^{''}(\mu)](\overline{X_n}-\mu)^2$ So $LHS = e^{-it\sqrt{n}g(\mu)} E[e^{it\sqrt{n}g(\overline{X_n})}] = 1+[(it\sqrt{n}g'(\mu))^2 + it\sqrt{n}g^{''}(\mu)]E[\overline{X_n}-\mu]^{2} =$ $1+[(it\sqrt{n}g'(\mu))^2 + it\sqrt{n}g^{''}(\mu)](\frac{\sigma^2}{n}+\mu^2)$ I am stuck at this point as $RHS$ has $e$ to the some power which means $LHS$ needs to have a $(1+x/n)^n$ kind of a situation which I don't. Basically, I am missing a power $n$ on the $LHS$ which should occur because of $\overline{X_n}$ but $g(\overline{X_n})$ kind of blocks that to happen. A hint is appreciated about the whole approach, thanks.","Suppose has at least three bounded continuous derivatives and let be and in . Prove that: i.e. weak convergence happens where , , My attempt: Let be the characteristic function/fourier transform. Weak convergence convergence of On the RHS, where is standard normal because On the LHS: The characteristic function is Because RHS has , I am thinking of expanding using Taylor's which is: So I am stuck at this point as has to the some power which means needs to have a kind of a situation which I don't. Basically, I am missing a power on the which should occur because of but kind of blocks that to happen. A hint is appreciated about the whole approach, thanks.","g:R\rightarrow R X_i iid L^2 \sqrt{n}[g(\overline{X_n}) - g(\mu)]\xrightarrow{w} N(0,g^{'}(\mu)^{2} \sigma ^2) \overline{X_n} = \frac{\sum X_n}{n} \mu = EX_1 \sigma^2=Var(X_1) \phi \iff \phi N(0,g^{'}(\mu)^{2} \sigma ^2) \sim  g^{'}(\mu) \sigma N(0,1) = g^{'}(\mu) \sigma \mathscr{X} \mathscr{X} \phi_{g^{'}(\mu) \sigma  \mathscr{X}}(t) = \phi_{\mathscr{X}}(g^{'}(\mu) \sigma t) = e^{-\frac{g^{'}(\mu)^{2} \sigma ^2 t^2}{2}} \phi_{\mathscr{X}}(t) = e^{-\frac{t^2}{2}} E[e^{it\sqrt{n}g(\overline{X_n}) - it\sqrt{n}g(\mu)}] = e^{-it\sqrt{n}g(\mu)} E[e^{it\sqrt{n}g(\overline{X_n})}] g^{'}(\mu) e^{it\sqrt{n}g(\overline{X_n})} e^{it\sqrt{n}g(\overline{X_n})} \approx e^{it\sqrt{n}g(\mu)} + e^{it\sqrt{n}g(\mu)}it\sqrt{n}g'(\mu)(\overline{X_n}-\mu)+e^{it\sqrt{n}g(\mu)}it\sqrt{n}g'(\mu)[(it\sqrt{n}g'(\mu))^2 + it\sqrt{n}g^{''}(\mu)](\overline{X_n}-\mu)^2 LHS = e^{-it\sqrt{n}g(\mu)} E[e^{it\sqrt{n}g(\overline{X_n})}] = 1+[(it\sqrt{n}g'(\mu))^2 + it\sqrt{n}g^{''}(\mu)]E[\overline{X_n}-\mu]^{2} = 1+[(it\sqrt{n}g'(\mu))^2 + it\sqrt{n}g^{''}(\mu)](\frac{\sigma^2}{n}+\mu^2) RHS e LHS (1+x/n)^n n LHS \overline{X_n} g(\overline{X_n})","['real-analysis', 'probability-theory']"
51,Distribution of $\frac{\overline X - \overline Y - (\mu_1 -\mu_2)}{\sqrt{\frac1{m+n-2} [(m-1)*s_1^2+(n-1)*s_2^2](\frac1{m}+\frac1{n})}}$,Distribution of,\frac{\overline X - \overline Y - (\mu_1 -\mu_2)}{\sqrt{\frac1{m+n-2} [(m-1)*s_1^2+(n-1)*s_2^2](\frac1{m}+\frac1{n})}},"I need to prove the following r.v has $t_{m+n-2}$ distribution $$\frac{\overline X - \overline Y - (\mu_1 -\mu_2)}{\sqrt{\frac1{m+n-2} [(m-1)*s_1^2+(n-1)*s_2^2](\frac1{m}+\frac1{n})}}$$ where $\overline X$ and $s_1^2$ are the mean sample and the quasivariance  of a s.r.s from a population $N(\mu_1,\sigma)$ and $\overline Y$ and $s_2^2$ are the  mean sample and the quasivariance of a s.r.s from a population $N(\mu_2,\sigma)$ ( the samples are independent). It's easy to see most of it. I can prove the numerator divided by $\sigma$ has distribution $N(0,1)$ and the denominator divided by $\sigma$ has distribution $\chi^2_{m+n-2}$ . However, I need to prove numerator and denominator are independent, and I don't know how to. Of course, $\overline X, \overline Y, s_1^2, s_2^2$ are pairwise independent but that doesn't guarantee anything , does it?","I need to prove the following r.v has distribution where and are the mean sample and the quasivariance  of a s.r.s from a population and and are the  mean sample and the quasivariance of a s.r.s from a population ( the samples are independent). It's easy to see most of it. I can prove the numerator divided by has distribution and the denominator divided by has distribution . However, I need to prove numerator and denominator are independent, and I don't know how to. Of course, are pairwise independent but that doesn't guarantee anything , does it?","t_{m+n-2} \frac{\overline X - \overline Y - (\mu_1 -\mu_2)}{\sqrt{\frac1{m+n-2} [(m-1)*s_1^2+(n-1)*s_2^2](\frac1{m}+\frac1{n})}} \overline X s_1^2 N(\mu_1,\sigma) \overline Y s_2^2 N(\mu_2,\sigma) \sigma N(0,1) \sigma \chi^2_{m+n-2} \overline X, \overline Y, s_1^2, s_2^2","['statistics', 'random-variables']"
52,Is an element of a vector a scalar or 1x1 vector or 1x1 matrix?,Is an element of a vector a scalar or 1x1 vector or 1x1 matrix?,,"In econometrics, by Hayashi, they defined the error vector of n observations in a $ (n \times K)$ regression funcntion as: $\epsilon = \begin{bmatrix}\epsilon_{1} \\\epsilon_{2} \\\vdots \\\epsilon_{n}\end{bmatrix}$ , where $\epsilon_{i}$ is the ith observation's error term, and K-dimension x vector of the ith observation as, $x_{i} = \begin{bmatrix}x_{i1} \\x_{i2} \\\vdots \\x_{ik}\end{bmatrix}$ The book says that the cross moment of two random variables E[xy] is zero means that these two random variables are orthogonal. It's not hard to see the point using [0,1] and [1,0] to check their cross product for orthogonality. But in the book it has a formula for strict exogeneity assumption: $E[x_{j}\epsilon_{i}] = \begin{bmatrix}x_{j1}\epsilon_{i} \\x_{j2}\epsilon_{i} \\\vdots \\x_{jk}\epsilon_{i}\end{bmatrix} = 0_{(K\times1)}$ So, here the $\epsilon_{i}$ is an element of the $\epsilon$ vector, and the cross moment for a $(k\times1)$ vector and an element of a vector, which are orthogonal, is a $(k\times1)$ 0 vector. My question is that, what is a cross moment of two random variables, is it the expected value of the inner product? And shall I view the $\epsilon_{i}$ as a scalar, or $(1\times1)$ vector, or $(1\times1)$ matrix? Thank you very much.","In econometrics, by Hayashi, they defined the error vector of n observations in a regression funcntion as: , where is the ith observation's error term, and K-dimension x vector of the ith observation as, The book says that the cross moment of two random variables E[xy] is zero means that these two random variables are orthogonal. It's not hard to see the point using [0,1] and [1,0] to check their cross product for orthogonality. But in the book it has a formula for strict exogeneity assumption: So, here the is an element of the vector, and the cross moment for a vector and an element of a vector, which are orthogonal, is a 0 vector. My question is that, what is a cross moment of two random variables, is it the expected value of the inner product? And shall I view the as a scalar, or vector, or matrix? Thank you very much.", (n \times K) \epsilon = \begin{bmatrix}\epsilon_{1} \\\epsilon_{2} \\\vdots \\\epsilon_{n}\end{bmatrix} \epsilon_{i} x_{i} = \begin{bmatrix}x_{i1} \\x_{i2} \\\vdots \\x_{ik}\end{bmatrix} E[x_{j}\epsilon_{i}] = \begin{bmatrix}x_{j1}\epsilon_{i} \\x_{j2}\epsilon_{i} \\\vdots \\x_{jk}\epsilon_{i}\end{bmatrix} = 0_{(K\times1)} \epsilon_{i} \epsilon (k\times1) (k\times1) \epsilon_{i} (1\times1) (1\times1),"['linear-algebra', 'statistics', 'vector-spaces', 'vectors', 'economics']"
53,Confidence interval for mean of non-normal distribution based on a small sample,Confidence interval for mean of non-normal distribution based on a small sample,,"The following problem appears in chapter 7 of Probability and Statistics for Engineers and Scientists by Ross: Suppose that a random sample of nine recently sold houses in a certain   city resulted in a sample mean price of \$222,000, with a sample   standard deviation of \$22,000. Give a 95 percent upper confidence   interval for the mean price of all recently sold houses in this city. If we were sampling from a normal distribution, then we could construct a confidence interval based on the fact that $\sqrt{n} (\bar X - \mu)/S$ has a $t$ -distribution. (Here $\bar X$ is the sample mean, $S$ is the sample standard deviation, and $n$ is the size of the sample.) Alternatively, if $n$ were large, then we could construct a confidence interval based on the fact that $\sqrt{n} (\bar X - \mu)/S$ is approximately normally distributed. However , in this problem we are sampling from a non-normal distribution and the sample size is small . So what can be done? I don't need a solution worked out in full detail -- I'd just like to know what approach the book expects us to take.","The following problem appears in chapter 7 of Probability and Statistics for Engineers and Scientists by Ross: Suppose that a random sample of nine recently sold houses in a certain   city resulted in a sample mean price of \$222,000, with a sample   standard deviation of \$22,000. Give a 95 percent upper confidence   interval for the mean price of all recently sold houses in this city. If we were sampling from a normal distribution, then we could construct a confidence interval based on the fact that has a -distribution. (Here is the sample mean, is the sample standard deviation, and is the size of the sample.) Alternatively, if were large, then we could construct a confidence interval based on the fact that is approximately normally distributed. However , in this problem we are sampling from a non-normal distribution and the sample size is small . So what can be done? I don't need a solution worked out in full detail -- I'd just like to know what approach the book expects us to take.",\sqrt{n} (\bar X - \mu)/S t \bar X S n n \sqrt{n} (\bar X - \mu)/S,"['statistics', 'confidence-interval']"
54,Combinations & Probability - Sport Club Teams Probability - Is my solution correct?,Combinations & Probability - Sport Club Teams Probability - Is my solution correct?,,"I'm revising permutations, combinations and probability for an upcoming exam and would really appreciate if someone could take a look at my procedure to solve this problem and let me know if it's correct. I have no answers to refer to. Hence, I have no way of knowing if I'm solving this correctly. Thank you in advance for your time! It is much much appreciated! A basketball club consist of 5 girls and 11 boys. For the next game 5 persons are randomly chosen to take the following positions: point guard, shooting guard, small forward, power forward and center. Specify a probability space for the experiment and calculate the probabilities of the following events: A: All players are girls B: All players are boys C: Small forward is the only girl D: Small forward and point guard are girls So we start with specifying the sample space. $B=[1,2,3...16]$ $1,2,3,4,5$ - girls; $6,7,8,9,10,11,12,13,14,15,16$ - boys $\Omega=[c1...c5] ⊂B$ , where ci≠cj for i≠j; $n=16$ $k=5$ A: All players are girls $\binom{5}{5}:\binom{16}{5}=1:4368=0.00022$ B: All players are boys $\binom{11}{5}:\binom{16}{5}=462:4368=0.1057$ C: Small forward is the only girl $\binom{5}{1}*\binom{11}{4}:\binom{16}{5}=(5*330):4368=0.377$ D: Small forward and point guard are girls $\binom{5}{2}*\binom{14}{3}:\binom{16}{5}=3640:4368 = 0.83$","I'm revising permutations, combinations and probability for an upcoming exam and would really appreciate if someone could take a look at my procedure to solve this problem and let me know if it's correct. I have no answers to refer to. Hence, I have no way of knowing if I'm solving this correctly. Thank you in advance for your time! It is much much appreciated! A basketball club consist of 5 girls and 11 boys. For the next game 5 persons are randomly chosen to take the following positions: point guard, shooting guard, small forward, power forward and center. Specify a probability space for the experiment and calculate the probabilities of the following events: A: All players are girls B: All players are boys C: Small forward is the only girl D: Small forward and point guard are girls So we start with specifying the sample space. - girls; - boys , where ci≠cj for i≠j; A: All players are girls B: All players are boys C: Small forward is the only girl D: Small forward and point guard are girls","B=[1,2,3...16] 1,2,3,4,5 6,7,8,9,10,11,12,13,14,15,16 \Omega=[c1...c5] ⊂B n=16 k=5 \binom{5}{5}:\binom{16}{5}=1:4368=0.00022 \binom{11}{5}:\binom{16}{5}=462:4368=0.1057 \binom{5}{1}*\binom{11}{4}:\binom{16}{5}=(5*330):4368=0.377 \binom{5}{2}*\binom{14}{3}:\binom{16}{5}=3640:4368 = 0.83","['probability', 'combinatorics', 'statistics', 'permutations', 'combinations']"
55,"Sufficient statistic for $N(\mu,1)$",Sufficient statistic for,"N(\mu,1)","Let $X_1,\ldots,X_n$ be a random sample from $N(\mu,1)$ , where $\mu$ is an unknown parameter. Show that $(\overline{x}/{S^2}, S^2)$ is a sufficient statistic for $\mu$ , where $S^2$ is the sample variance. My Approach $$ \sum_{i=1}^n (x_i-\mu)^2 = n(\bar x-\mu)^2 + \sum_{i=1}^n (x_i-\bar x)^2. $$ By Neyman Fisher Factorisation, \begin{align} f_{X_1,\ldots,X_n}(x_1,\ldots,x_n) \propto {} & \prod_{i=1}^n \frac 1 {(\sqrt2\pi)^n} \exp\left( \frac{-1} 2 \left( {x_i-\mu} \right)^2 \right) = \frac 1 {(\sqrt2\pi)^n} \exp\left( \frac{-1}{2} \sum_{i=1}^n (x_i-\mu)^2  \right) \\[10pt] = {} & (\sqrt2\pi)^{-n} \exp\left( \frac{-1}{2} \left( n(\bar x - \mu)^2 + \sum_{i=1}^n (x_i - \bar x)^2 \right) \right) \\[10pt] = {} & (\sqrt2\pi)^{-n} \exp\left( - \frac n {2} ((\bar x - \mu)^2 + s^2) \right). \end{align} How do I proceed from here?","Let be a random sample from , where is an unknown parameter. Show that is a sufficient statistic for , where is the sample variance. My Approach By Neyman Fisher Factorisation, How do I proceed from here?","X_1,\ldots,X_n N(\mu,1) \mu (\overline{x}/{S^2}, S^2) \mu S^2 
\sum_{i=1}^n (x_i-\mu)^2 = n(\bar x-\mu)^2 + \sum_{i=1}^n (x_i-\bar x)^2.
 \begin{align}
f_{X_1,\ldots,X_n}(x_1,\ldots,x_n) \propto {} & \prod_{i=1}^n \frac 1 {(\sqrt2\pi)^n} \exp\left( \frac{-1} 2 \left( {x_i-\mu} \right)^2 \right) = \frac 1 {(\sqrt2\pi)^n} \exp\left( \frac{-1}{2} \sum_{i=1}^n (x_i-\mu)^2  \right) \\[10pt]
= {} & (\sqrt2\pi)^{-n} \exp\left( \frac{-1}{2} \left( n(\bar x - \mu)^2 + \sum_{i=1}^n (x_i - \bar x)^2 \right) \right) \\[10pt]
= {} & (\sqrt2\pi)^{-n} \exp\left( - \frac n {2} ((\bar x - \mu)^2 + s^2) \right).
\end{align}","['statistics', 'statistical-inference']"
56,Sufficient statistics for a discrete distribution,Sufficient statistics for a discrete distribution,,"Let { $X_1, X_2,...,X_n$ } be a random sample from a population with the following pmf $$f(x|\theta,p)= \left\{ \begin{array}{lcc}              (1-p)p^{x-\theta}  &  x=\theta, \theta+1,... \\              \\ 0 &\mbox{ otherwise}, \\              \\               \end{array}    \right.$$ where $\theta$ is an positive integer and $0<p<1$ , both are unknown. Find a sufficient statistic for the parameter vector $(\theta,p)$ . My Approach Using Neyman-Fisher Factorization $$ f_X(x|\theta, p) = \prod_{i=1}^n (1-p)p^{x_i-\theta} = (1-p)^{n} p^{\sum_{i=1}^n x_i-n\theta} I_{(x_i \in {\theta,\theta+1,... })}. $$ This can be written $$ f_X(x|\theta,p) = g(T_1(x),T_2(x);\theta,p)\, h(x) \, , $$ where $g(T_1(x),T_2(x))=p^{\sum_{i=1}(x_i-n\theta)}$ and $h(x) = (1-p)^n$ , which demonstrates that $(X_{(1)},\sum_{i=1}^n X_i)$ is a sufficient statistic for the parameter $(\theta,p)$ . Is the above correct? What am I missing?","Let { } be a random sample from a population with the following pmf where is an positive integer and , both are unknown. Find a sufficient statistic for the parameter vector . My Approach Using Neyman-Fisher Factorization This can be written where and , which demonstrates that is a sufficient statistic for the parameter . Is the above correct? What am I missing?","X_1, X_2,...,X_n f(x|\theta,p)= \left\{ \begin{array}{lcc}
             (1-p)p^{x-\theta}  &  x=\theta, \theta+1,... \\
             \\ 0 &\mbox{ otherwise}, \\
             \\ 
             \end{array}
   \right. \theta 0<p<1 (\theta,p) 
f_X(x|\theta, p) = \prod_{i=1}^n (1-p)p^{x_i-\theta} = (1-p)^{n} p^{\sum_{i=1}^n x_i-n\theta} I_{(x_i \in {\theta,\theta+1,... })}.
 
f_X(x|\theta,p) = g(T_1(x),T_2(x);\theta,p)\, h(x) \, ,
 g(T_1(x),T_2(x))=p^{\sum_{i=1}(x_i-n\theta)} h(x) = (1-p)^n (X_{(1)},\sum_{i=1}^n X_i) (\theta,p)","['statistics', 'statistical-inference']"
57,How to proof variance of the sum $x+y$ is the sum of variance $\sigma_x^2$ of $x$ and the variance $\sigma_y^2$ of $y$?,How to proof variance of the sum  is the sum of variance  of  and the variance  of ?,x+y \sigma_x^2 x \sigma_y^2 y,"Show that the variance of the sum $x + y$ of the random variable $x$ and the random variable $y$ is the sum of the variance $\sigma_x^2$ of $x$ and the variance $\sigma_y^2$ of $y$ ? My attempt: Is my first time doing statistics and there are alot of terms i am quite unfamiliar with. Random variable: Value of a variable result from a random experiment: e.g. (the score when a die is rolled once) Variance( $\sigma^2$ ) : Average of squared deviations from the mean or square of standard deviation? I know variance of sum $x + y$ means $var(x+y) = $ E(x+y)^2 - $(E(x+y))^2$ But what does sum of the variance $\sigma_x^2$ of $x$ and the variance $\sigma_y^2$ of $y$ means? Edited: My 2nd attempt: Var(x + y ) = E $(x+y)^2$ - $(E(x+y)^2)$ = E( $x^2$ + 2xy + $y^2$ ) - [( $Ex)^2$ + 2ExEy + ( $Ey)^2$ ] = [E $x^2$ - (E $x)^2$ ] + [E $y^2$ - (E $y)^2$ ] + 2[Exy - ExEy] = Var x + Var y + 2cov (x,y) Now i assume(not sure is it correct) sum of the variance $\sigma_x^2$ of $x$ and the variance $\sigma_y^2$ of $y$ = var ( $\sigma_x^2$ ) + var( $\sigma_y^2$ )  = E( $\sigma_x^2$ ) -[E( $\sigma_x)]^2$ +  E( $\sigma_y^2$ ) -[E( $\sigma_y)]^2$ Since var(x) = $\sigma_x^2$ , =E(E( $x^2)$ - $[E(x)]^2$ ) - [ $E^2$ ((E( $x^2)$ - $[E(x)]^2$ )) + E(E( $y^2)$ - $[E(y)]^2$ ) - [ $E^2$ ((E( $y^2)$ I did not further solve it because i realise is wrong as the above does not have an expression that has an xy expression. So where have i done wrong?","Show that the variance of the sum of the random variable and the random variable is the sum of the variance of and the variance of ? My attempt: Is my first time doing statistics and there are alot of terms i am quite unfamiliar with. Random variable: Value of a variable result from a random experiment: e.g. (the score when a die is rolled once) Variance( ) : Average of squared deviations from the mean or square of standard deviation? I know variance of sum means E(x+y)^2 - But what does sum of the variance of and the variance of means? Edited: My 2nd attempt: Var(x + y ) = E - = E( + 2xy + ) - [( + 2ExEy + ( ] = [E - (E ] + [E - (E ] + 2[Exy - ExEy] = Var x + Var y + 2cov (x,y) Now i assume(not sure is it correct) sum of the variance of and the variance of = var ( ) + var( )  = E( ) -[E( +  E( ) -[E( Since var(x) = , =E(E( - ) - [ ((E( - )) + E(E( - ) - [ ((E( I did not further solve it because i realise is wrong as the above does not have an expression that has an xy expression. So where have i done wrong?",x + y x y \sigma_x^2 x \sigma_y^2 y \sigma^2 x + y var(x+y) =  (E(x+y))^2 \sigma_x^2 x \sigma_y^2 y (x+y)^2 (E(x+y)^2) x^2 y^2 Ex)^2 Ey)^2 x^2 x)^2 y^2 y)^2 \sigma_x^2 x \sigma_y^2 y \sigma_x^2 \sigma_y^2 \sigma_x^2 \sigma_x)]^2 \sigma_y^2 \sigma_y)]^2 \sigma_x^2 x^2) [E(x)]^2 E^2 x^2) [E(x)]^2 y^2) [E(y)]^2 E^2 y^2),"['statistics', 'variance', 'standard-deviation']"
58,Efficient computation of incremental standard deviation (removing first value),Efficient computation of incremental standard deviation (removing first value),,"I found this link about incremental standard deviation where it computes the standard deviation every time a new element was added to the dataset. Is there a similar method when adding a new value and removing the first value from the dataset? For example, [45, 26, 78, 45, 34, 56] - initial data set   [26, 78, 45, 34, 56, 74 *(new data)*] - first value 45 was removed","I found this link about incremental standard deviation where it computes the standard deviation every time a new element was added to the dataset. Is there a similar method when adding a new value and removing the first value from the dataset? For example, [45, 26, 78, 45, 34, 56] - initial data set   [26, 78, 45, 34, 56, 74 *(new data)*] - first value 45 was removed",,"['statistics', 'algorithms', 'computational-mathematics', 'standard-deviation']"
59,Population Cumulative Distribution Function with Simple Random Sampling,Population Cumulative Distribution Function with Simple Random Sampling,,"Link to textbook example: http://puu.sh/CEgk2/b5d08453fb.png Context to Example 5.4.1: http://puu.sh/CEgmt/c6390a8bba.png Hi, I was reading up on my textbook readings for my intro to stats class, and I got confused over an example that was provided in the textbook. I was wondering if I could get some help with understanding it. In the first picture, there is an equation that is equivalent to CDF. Please correct me if I'm understanding this wrong, but the first part of that equation says ""the probability of the measurement X of the first individual in the population being less than x is equal to..."". Then, the second part of the equation says ""the absolute value of the number of individuals from the population set such that the measurement of the population is less than or equal to x over 20"". Then, the third part is just the CDF. The thing that confuses me is that if the probability is being asked of the first individual of the population set being less than x (which is stated in the first part), why does the second part require me to account for other possible individuals from the population set which satisfy the X(pi) <= x?","Link to textbook example: http://puu.sh/CEgk2/b5d08453fb.png Context to Example 5.4.1: http://puu.sh/CEgmt/c6390a8bba.png Hi, I was reading up on my textbook readings for my intro to stats class, and I got confused over an example that was provided in the textbook. I was wondering if I could get some help with understanding it. In the first picture, there is an equation that is equivalent to CDF. Please correct me if I'm understanding this wrong, but the first part of that equation says ""the probability of the measurement X of the first individual in the population being less than x is equal to..."". Then, the second part of the equation says ""the absolute value of the number of individuals from the population set such that the measurement of the population is less than or equal to x over 20"". Then, the third part is just the CDF. The thing that confuses me is that if the probability is being asked of the first individual of the population set being less than x (which is stated in the first part), why does the second part require me to account for other possible individuals from the population set which satisfy the X(pi) <= x?",,['statistics']
60,Compute the Mean Squared Error of $𝑇$,Compute the Mean Squared Error of,𝑇,"The famous biologist Henk de Rijn has come up with a skill test for chimpanzees.  We assume that each chimpanzee has a probability $𝑝$ to pass the test, independent from previous attempts and from other monkeys.  Let $𝑋_𝑖$ be the number of attempts that chimpanzee $𝑖$ needs to pass. Furthermore, $𝑇 = \overline{X}_𝑛$ is an estimator for $1/𝑝$ .  Compute the mean squared error of $𝑇$ . So let $T$ be an estimator for a parameter $1∕𝑝$ . The $MSE$ of $T$ is $MSE(T)=\operatorname{Var}(T) + (E[T] − 1∕𝑝)^2$ ,  where $(E[T] − 1∕𝑝)^2$ is the bias. In this case the $E[T]=E[\overline{X}_𝑛]=\mu$ and $\operatorname{Var}(T)=\operatorname{Var}(\overline{X}_𝑛)=\dfrac{\sigma^2}{n}$ . Now my question is: which distribution should I use? I think that I think that is a Binomial but it also make sense to use a Geometric Distribution, so then the estimator $T$ will be unbiased and the $\displaystyle\operatorname{MSE}(T)=\operatorname{Var}(T)=\frac{1-p}{p^2}*\frac{1}{n}$ , is it right?","The famous biologist Henk de Rijn has come up with a skill test for chimpanzees.  We assume that each chimpanzee has a probability to pass the test, independent from previous attempts and from other monkeys.  Let be the number of attempts that chimpanzee needs to pass. Furthermore, is an estimator for .  Compute the mean squared error of . So let be an estimator for a parameter . The of is ,  where is the bias. In this case the and . Now my question is: which distribution should I use? I think that I think that is a Binomial but it also make sense to use a Geometric Distribution, so then the estimator will be unbiased and the , is it right?",𝑝 𝑋_𝑖 𝑖 𝑇 = \overline{X}_𝑛 1/𝑝 𝑇 T 1∕𝑝 MSE T MSE(T)=\operatorname{Var}(T) + (E[T] − 1∕𝑝)^2 (E[T] − 1∕𝑝)^2 E[T]=E[\overline{X}_𝑛]=\mu \operatorname{Var}(T)=\operatorname{Var}(\overline{X}_𝑛)=\dfrac{\sigma^2}{n} T \displaystyle\operatorname{MSE}(T)=\operatorname{Var}(T)=\frac{1-p}{p^2}*\frac{1}{n},['statistics']
61,A manufacturer of space shuttle light bulbs claims that the defect rate of the bulbs is $0.1\%$.,A manufacturer of space shuttle light bulbs claims that the defect rate of the bulbs is .,0.1\%,"A manufacturer of space shuttle light bulbs claims that the defect rate of the bulbs is $0.1\%$ . You suspect the defect rate is actually higher, so you have checked $1000$ identical light bulbs from this manufacturer and found out that 3 of them are defect. Formulate the relevant hypotheses and test statistic (including its distribution) and investigate the claim of the manufacturer at significance level $α = 0.05$ . In my opinion, is a Binomial Distribution $Bin(1000,0.001)$ . For the hypothesis, it's correct to assume that $H_0:\mu=0.001$ , $H_1:\mu>0.001$ ? Then how can I use the significance level with a Binomial Distribution? (I know how to solve it but with a Normal) :( For a $Bin(n,p)$ $P(X=k)=\frac{n!}{k!(n-k)!}\cdot p^k\cdot(1-p)^{n-k}$","A manufacturer of space shuttle light bulbs claims that the defect rate of the bulbs is . You suspect the defect rate is actually higher, so you have checked identical light bulbs from this manufacturer and found out that 3 of them are defect. Formulate the relevant hypotheses and test statistic (including its distribution) and investigate the claim of the manufacturer at significance level . In my opinion, is a Binomial Distribution . For the hypothesis, it's correct to assume that , ? Then how can I use the significance level with a Binomial Distribution? (I know how to solve it but with a Normal) :( For a","0.1\% 1000 α = 0.05 Bin(1000,0.001) H_0:\mu=0.001 H_1:\mu>0.001 Bin(n,p) P(X=k)=\frac{n!}{k!(n-k)!}\cdot p^k\cdot(1-p)^{n-k}",['statistics']
62,Use an appropriate statistical test to test the manufacturer’s claim at significance level $α=0.05$.,Use an appropriate statistical test to test the manufacturer’s claim at significance level .,α=0.05,"The manufacturer of the resistors claims that the expected maximum power at which the resistor fails is $0.25$ . You suspect this threshold is lower and therefore decide to measure the current and resistance at which the resistor breaks down. By an ingenious application of Ohm’s Law this results in a sample mean of $\overline{x}_{20} = 0.21$ and a sample variance of $s^2_{20} = 0.01$ for the power. You may assume that the measurements for the power are also normally distributed. Use an appropriate statistical test to test the manufacturer’s claim at significance level $α=0.05$ . I assume that $H_0$ : $\mu=0.25$ $H_1$ : $\mu<0.25$ $P(T\geq0.21|H_0)\leq\alpha$ $Z=\frac{0.21-0.25}{\frac{\sqrt{0.01}}{\sqrt{20}}}=-1.788$ $P(Z\leq-1.788)=P(Z\geq1.788)= 0.0368$ (from the N(0,1) table) I think that what I did is correct but I didn't use the value of $0.05$ for the significance level, how can I use it in this question? What I assumed is correct? Thanks for the help.","The manufacturer of the resistors claims that the expected maximum power at which the resistor fails is . You suspect this threshold is lower and therefore decide to measure the current and resistance at which the resistor breaks down. By an ingenious application of Ohm’s Law this results in a sample mean of and a sample variance of for the power. You may assume that the measurements for the power are also normally distributed. Use an appropriate statistical test to test the manufacturer’s claim at significance level . I assume that : : (from the N(0,1) table) I think that what I did is correct but I didn't use the value of for the significance level, how can I use it in this question? What I assumed is correct? Thanks for the help.",0.25 \overline{x}_{20} = 0.21 s^2_{20} = 0.01 α=0.05 H_0 \mu=0.25 H_1 \mu<0.25 P(T\geq0.21|H_0)\leq\alpha Z=\frac{0.21-0.25}{\frac{\sqrt{0.01}}{\sqrt{20}}}=-1.788 P(Z\leq-1.788)=P(Z\geq1.788)= 0.0368 0.05,['statistics']
63,Understanding why variance of the standard normal distribution equals one intuitively,Understanding why variance of the standard normal distribution equals one intuitively,,"Can anyone explain to me why the variance of the standard normal distribution is 1? I am trying to understand the mechanism behind standardising random variable. While I know minus the variable by the mean is like shifting the graph to make it centre at the origin, I don't know why dividing it by SD makes the variable having SD = 1 as well","Can anyone explain to me why the variance of the standard normal distribution is 1? I am trying to understand the mechanism behind standardising random variable. While I know minus the variable by the mean is like shifting the graph to make it centre at the origin, I don't know why dividing it by SD makes the variable having SD = 1 as well",,"['calculus', 'linear-algebra', 'statistics', 'linear-transformations', 'normal-distribution']"
64,How do I calculate the F value from the Null Hypothesis of equality of means,How do I calculate the F value from the Null Hypothesis of equality of means,,"This question comes from a practice exam, the following contains the question and the answer provided. However my teachers for this exam are non-respondent and I really need someone to please help explain this to me. I attach the answer below as well. In an agricultural trial, yields are recorded for samples of plants from four different varieties of a crop: Variety A: 15, 14, 12, 13 Variety B: 11, 18, 13 Variety C: 18, 25, 19, 20 Variety D: 19, 20, 24 It is revealed that varieties C and D were genetically modified (GM) plants. Varieties A and B were not. By fitting an appropriate model to the data, test the hypothesis: H0: The mean yields of the two GM varieties are equal and the mean yields of the two non-GM varieties are equal. My questions are: - Where this equation for F comes from. I understand the need to test for it, but in my experience it is calculated from doing (MS Variable / MS Error). What the values of p_f and p_r represent? My rationale for the latter is that they represent the upper limits of i from the ""appropriate model"". If anyone could explain this to me then that would be very much appreciated. Thank you.","This question comes from a practice exam, the following contains the question and the answer provided. However my teachers for this exam are non-respondent and I really need someone to please help explain this to me. I attach the answer below as well. In an agricultural trial, yields are recorded for samples of plants from four different varieties of a crop: Variety A: 15, 14, 12, 13 Variety B: 11, 18, 13 Variety C: 18, 25, 19, 20 Variety D: 19, 20, 24 It is revealed that varieties C and D were genetically modified (GM) plants. Varieties A and B were not. By fitting an appropriate model to the data, test the hypothesis: H0: The mean yields of the two GM varieties are equal and the mean yields of the two non-GM varieties are equal. My questions are: - Where this equation for F comes from. I understand the need to test for it, but in my experience it is calculated from doing (MS Variable / MS Error). What the values of p_f and p_r represent? My rationale for the latter is that they represent the upper limits of i from the ""appropriate model"". If anyone could explain this to me then that would be very much appreciated. Thank you.",,"['statistics', 'statistical-inference', 'hypothesis-testing']"
65,Total variation distance bounds on multivariate normals with different means and variances,Total variation distance bounds on multivariate normals with different means and variances,,"I'm trying to find a way to obtain an upper bound on the total variation distance between two multivariate normal distributions, i.e. $$ \vert \vert \mathcal{N}(\theta_1, a I) - \mathcal{N}(\theta_2, b I) \vert \vert_{TV} \le k \cdot \vert \vert \theta_1 - \theta_2 \vert \vert $$ where $\theta_1 \ne \theta_2$ and $a \ne b$ are constants and $k$ is some constant. Alternatively, some other bound that results in a scalar times any normed distance of $\theta_1$ and $\theta_2$ works. The problem I'm running into is I can't seem to find many ways to bound total variation distances between multivariate normal distributions. There are plenty of toy examples where either both distributions have the same mean or both distributions have the same variance in which it's easy to get an exact value, and I've computed the exact total variation distance between these two distributions, but I need something cleaner. Any help would be greatly appreciated!","I'm trying to find a way to obtain an upper bound on the total variation distance between two multivariate normal distributions, i.e. where and are constants and is some constant. Alternatively, some other bound that results in a scalar times any normed distance of and works. The problem I'm running into is I can't seem to find many ways to bound total variation distances between multivariate normal distributions. There are plenty of toy examples where either both distributions have the same mean or both distributions have the same variance in which it's easy to get an exact value, and I've computed the exact total variation distance between these two distributions, but I need something cleaner. Any help would be greatly appreciated!","
\vert \vert \mathcal{N}(\theta_1, a I) - \mathcal{N}(\theta_2, b I) \vert \vert_{TV} \le k \cdot \vert \vert \theta_1 - \theta_2 \vert \vert
 \theta_1 \ne \theta_2 a \ne b k \theta_1 \theta_2","['probability', 'probability-theory', 'statistics', 'markov-chains']"
66,What's the average life expectancy if only dying from accidents?,What's the average life expectancy if only dying from accidents?,,"So, I curious and trying to determine what sort life expectancy a human being would have if they were immortal (as in, no more senescence (aging)). Accidental deaths only. I've googled around and found numbers from a few hundred years to nearly 9000 years. Europe has some great statistics on accidental deaths - https://ec.europa.eu/eurostat/statistics-explained/index.php/Accidents_and_injuries_statistics#Deaths_from_accidents.2C_injuries_and_assault The headline is that 3.1% of deaths were accidents in 2015. However, it occurs to me that the best representative sample to use is people aged 15-25 who generally don't really die from illnesses and are in better health so less susceptible to things like falls (and more to traffic accidents). There's a graph on that page that shows accidental deaths are about 35% of all deaths for those age groups. Unfortunately I lack the mathematical chops to be able to take those numbers and merge them together meaningfully. So.... given the information above (and in the link if I've missed anything useful out), what's the approximate average European life expectancy if we ""solved"" senescence tomorrow?","So, I curious and trying to determine what sort life expectancy a human being would have if they were immortal (as in, no more senescence (aging)). Accidental deaths only. I've googled around and found numbers from a few hundred years to nearly 9000 years. Europe has some great statistics on accidental deaths - https://ec.europa.eu/eurostat/statistics-explained/index.php/Accidents_and_injuries_statistics#Deaths_from_accidents.2C_injuries_and_assault The headline is that 3.1% of deaths were accidents in 2015. However, it occurs to me that the best representative sample to use is people aged 15-25 who generally don't really die from illnesses and are in better health so less susceptible to things like falls (and more to traffic accidents). There's a graph on that page that shows accidental deaths are about 35% of all deaths for those age groups. Unfortunately I lack the mathematical chops to be able to take those numbers and merge them together meaningfully. So.... given the information above (and in the link if I've missed anything useful out), what's the approximate average European life expectancy if we ""solved"" senescence tomorrow?",,"['statistics', 'average']"
67,Simple error propagation calculation,Simple error propagation calculation,,"I have two elevation models that I subtract from each other, to find out how the topography changed over time. For the elevation models I have given the RMSE error of the horizontal and vertical uncertainty/error. I want to calculate the error of the resulting elevation model, that represents the differences between the two models. I know that I have to calculate a error propagation, but I am very unsure how to do it. The method does not need to be very exact or complex, I need a simple method to state that the resulting elevation model has an error of round about xy. The first model has a resolution of 0.05 m with an error of 0.03 m horizontally and 0.05 m vertically. The second model has a resolution of 1.0 m and an error of 0.15 m horizontally and 0.10 m vertically. So I would say, I have four variables: Variable 1: 0.05 m +/- 0.03 m Variable 2: 0.05 m +/- 0.05 m Variable 3: 1.00 m +/- 0.15 m Variable 4: 1.00 m +/- 0.10 m Is this correct? How can I further calculate the error? EDIT: I am not sure about the output variable but I think I would have two output variables. Variable 1: output resulting from Variable 1 and Variable 3. Maybe something like XX m +/- XX m or a percentage value. Variable 2: output resulting from Variable 2 and Variable 4","I have two elevation models that I subtract from each other, to find out how the topography changed over time. For the elevation models I have given the RMSE error of the horizontal and vertical uncertainty/error. I want to calculate the error of the resulting elevation model, that represents the differences between the two models. I know that I have to calculate a error propagation, but I am very unsure how to do it. The method does not need to be very exact or complex, I need a simple method to state that the resulting elevation model has an error of round about xy. The first model has a resolution of 0.05 m with an error of 0.03 m horizontally and 0.05 m vertically. The second model has a resolution of 1.0 m and an error of 0.15 m horizontally and 0.10 m vertically. So I would say, I have four variables: Variable 1: 0.05 m +/- 0.03 m Variable 2: 0.05 m +/- 0.05 m Variable 3: 1.00 m +/- 0.15 m Variable 4: 1.00 m +/- 0.10 m Is this correct? How can I further calculate the error? EDIT: I am not sure about the output variable but I think I would have two output variables. Variable 1: output resulting from Variable 1 and Variable 3. Maybe something like XX m +/- XX m or a percentage value. Variable 2: output resulting from Variable 2 and Variable 4",,"['statistics', 'error-propagation']"
68,Sufficient statistics for $p$ for a random sample from $\text{Ber}(p)$ distribution,Sufficient statistics for  for a random sample from  distribution,p \text{Ber}(p),"Let $X_1,X_2, X_3$ be a random sample from Bernoulli distribution $B(p)$ . Which of the following is sufficient statistic for $p$ ? $(A)\ \ X_{1}^{2}+X_{2}^{2}+X_{3}^{2}$ $(B) \ \ X_1+2X_{2}+X_{3}$ $(C)\ \ 2X_1-X_{2}-X_{3}$ $(D) \ \ X_1+X_{2}$ $(E)\ \ 3X_1+2X_{2}-4X_{3}$ We know $T=\sum_i^3 X_i$ is sufficient statistic for $p$ via Neymann Factorization theorem. The reasoning for different options. $(A)$ It's not a one-one function of sufficient statistic $T$ (If it was $T^2$ , was it sufficient statistic then? My answer is yes because it is one to one function of sufficient statistic our random variable is positive am I right?). $(B)X_1+X_{2}+X_{3}+X_{2}$ It contains original statistic therefor it is suffient statistic for $p$ . $(D)$ It doesn't include $X_3$ So it's not sufficient statistic for $p$ All other options include subtraction in it so I ruled out all of them. I think my reasoning is not very good I lack some intuition behind finding sufficient statistic. Correct me here, please.","Let be a random sample from Bernoulli distribution . Which of the following is sufficient statistic for ? We know is sufficient statistic for via Neymann Factorization theorem. The reasoning for different options. It's not a one-one function of sufficient statistic (If it was , was it sufficient statistic then? My answer is yes because it is one to one function of sufficient statistic our random variable is positive am I right?). It contains original statistic therefor it is suffient statistic for . It doesn't include So it's not sufficient statistic for All other options include subtraction in it so I ruled out all of them. I think my reasoning is not very good I lack some intuition behind finding sufficient statistic. Correct me here, please.","X_1,X_2, X_3 B(p) p (A)\ \ X_{1}^{2}+X_{2}^{2}+X_{3}^{2} (B) \ \ X_1+2X_{2}+X_{3} (C)\ \ 2X_1-X_{2}-X_{3} (D) \ \ X_1+X_{2} (E)\ \ 3X_1+2X_{2}-4X_{3} T=\sum_i^3 X_i p (A) T T^2 (B)X_1+X_{2}+X_{3}+X_{2} p (D) X_3 p","['statistics', 'statistical-inference', 'estimation', 'binomial-distribution']"
69,Simulation: Generate random numbers that cluster around an average?,Simulation: Generate random numbers that cluster around an average?,,"I want to simulate a simple event that has variable empirical result/outcome. Generate random numbers that cluster around an average For example, let's say we collect the data for how far people can throw a ball.  The data may or may not be distributed normally.  I want my code to generate a hypothetical throwing distance based on that distribution. For example, say the mean throw distance is 10 feet, with StdDev of 2 feet.  The simulator should generate most throws to be around 10 feet, but once in a while you can generate a 20 ft. distance.   There is a probability of each distance that can be calculated?   Any idea how I start to model this?  I'm not sure what to search for. Is this one approach?   Area under the curve?  If $f(x)$ is the ""bell curve"" function of throwing distance frequency distribution histogram, and $g(x) = \int_0^x f(x)dx$ is some kind of cumulative density function.    Generate a random number from 0 to 1 and and see where it intersects $g(x)$ ? Better yet, What do you think of the following? I think I can discard the distribution concept, and just model a bell-like frequency histogram in Excel.  Using 2 columns of data.  a1=3.  b1=34.  Etc.  (3, 34), (5, 45), (7,245)(10,350) (11,240), (12,145), (13,90), (14, 35), (15, 12) ............( 20, 1) In the 3rd column, I can create a cumulative total. From that, I can do a regression and get a function! Then I just take the inverse of that function and use it as a lookup function using f(x), where x is a random number from 0","I want to simulate a simple event that has variable empirical result/outcome. Generate random numbers that cluster around an average For example, let's say we collect the data for how far people can throw a ball.  The data may or may not be distributed normally.  I want my code to generate a hypothetical throwing distance based on that distribution. For example, say the mean throw distance is 10 feet, with StdDev of 2 feet.  The simulator should generate most throws to be around 10 feet, but once in a while you can generate a 20 ft. distance.   There is a probability of each distance that can be calculated?   Any idea how I start to model this?  I'm not sure what to search for. Is this one approach?   Area under the curve?  If is the ""bell curve"" function of throwing distance frequency distribution histogram, and is some kind of cumulative density function.    Generate a random number from 0 to 1 and and see where it intersects ? Better yet, What do you think of the following? I think I can discard the distribution concept, and just model a bell-like frequency histogram in Excel.  Using 2 columns of data.  a1=3.  b1=34.  Etc.  (3, 34), (5, 45), (7,245)(10,350) (11,240), (12,145), (13,90), (14, 35), (15, 12) ............( 20, 1) In the 3rd column, I can create a cumulative total. From that, I can do a regression and get a function! Then I just take the inverse of that function and use it as a lookup function using f(x), where x is a random number from 0",f(x) g(x) = \int_0^x f(x)dx g(x),"['calculus', 'integration', 'statistics']"
70,Bernoulli trials hypothesis test,Bernoulli trials hypothesis test,,"I'm studying about hypothesis test theory and I've reached the next exercise: Let $X\sim Ber(\theta)$ . Then, we know that for a sample of $n=10$ , we reject $H_0:\theta = 0.5$ , and accept $H_1: \theta = 0.75$ if $\overline{X} > 0.8$ . Find the test's power function. Compute the significance level. Compute the power of the hypothesis test. So, I know a few things that could be useful: For the first question I know that the power function is defined as $$\mathbb{P}[\text{Reject } \ H_0 \mid \mu]$$ where $\mu$ is the mean of the sample. So I need to compute: $$\mathbb{P}[\overline{X} > 0.8 \mid \mu]$$ So far, I know that $$\frac{\overline{X}-\mu}{\frac{\sigma}{\sqrt{n}}}\sim N(0,1)$$ And also that for independent $X_i \sim Ber(\theta)$ then $Y = \sum_{i=1}^{10}X_i \sim Bin(10,\theta)$ and $\mathbb{E}(Y) = 10\theta, \ \sigma_Y = \sqrt{10(\theta)(1-\theta)}$ . Maybe it is a simple exercise but I'm having trouble figuring out the value of $\mu$ . For example, for the third question, when I compute the power of the test I did it this way: \begin{align} \text{Power} &= \mathbb{P}[\overline{X} > 0.8 \mid \theta = 0.75]\\ &= \mathbb{P}\left[\frac{\overline{X} - (10*.75)}{\frac{\sqrt{10*0.75*0.25}}{\sqrt{10}}} > \frac{0.8-(10*0.75)}{\frac{\sqrt{10*0.75*0.25}}{\sqrt{10}}}\right]\\ &= \mathbb{P} [Z > -15.47]\\ &= 1-\phi(-15.47)\\ &= 1 \end{align} But it confuses me that the power of the test is $1$ . And when I compute the significance level $\alpha$ is equal to $1$ too. It seems I'm computing something wrong but I'm lost. Is my $\mu = 10*0.75$ computed the right way?","I'm studying about hypothesis test theory and I've reached the next exercise: Let . Then, we know that for a sample of , we reject , and accept if . Find the test's power function. Compute the significance level. Compute the power of the hypothesis test. So, I know a few things that could be useful: For the first question I know that the power function is defined as where is the mean of the sample. So I need to compute: So far, I know that And also that for independent then and . Maybe it is a simple exercise but I'm having trouble figuring out the value of . For example, for the third question, when I compute the power of the test I did it this way: But it confuses me that the power of the test is . And when I compute the significance level is equal to too. It seems I'm computing something wrong but I'm lost. Is my computed the right way?","X\sim Ber(\theta) n=10 H_0:\theta = 0.5 H_1: \theta = 0.75 \overline{X} > 0.8 \mathbb{P}[\text{Reject } \ H_0 \mid \mu] \mu \mathbb{P}[\overline{X} > 0.8 \mid \mu] \frac{\overline{X}-\mu}{\frac{\sigma}{\sqrt{n}}}\sim N(0,1) X_i \sim Ber(\theta) Y = \sum_{i=1}^{10}X_i \sim Bin(10,\theta) \mathbb{E}(Y) = 10\theta, \ \sigma_Y = \sqrt{10(\theta)(1-\theta)} \mu \begin{align}
\text{Power} &= \mathbb{P}[\overline{X} > 0.8 \mid \theta = 0.75]\\
&= \mathbb{P}\left[\frac{\overline{X} - (10*.75)}{\frac{\sqrt{10*0.75*0.25}}{\sqrt{10}}} > \frac{0.8-(10*0.75)}{\frac{\sqrt{10*0.75*0.25}}{\sqrt{10}}}\right]\\
&= \mathbb{P} [Z > -15.47]\\
&= 1-\phi(-15.47)\\
&= 1
\end{align} 1 \alpha 1 \mu = 10*0.75","['statistics', 'statistical-inference', 'hypothesis-testing']"
71,"Find Random Number Generator following the density $f (x) = \frac{1 + \alpha x}{2}$, $ −1 ≤ x ≤ 1$, $−1 ≤\alpha ≤ 1$","Find Random Number Generator following the density , ,",f (x) = \frac{1 + \alpha x}{2}  −1 ≤ x ≤ 1 −1 ≤\alpha ≤ 1,"How could random variables with the following density function be generated from a uniform random number generator? $f (x) = \frac{1 + \alpha x}{2}$ , $ −1 ≤ x ≤ 1$ , $−1 ≤\alpha ≤ 1$ . To find the Random number generator one first needs to find the distribution function and then the quantile function. However, I have troubles solving this on my own: \begin{align} F(x) & = \int_{-1}^x \frac{1+\alpha u}{2} du\\ & = \frac 12 \Big( (x+\frac{\alpha x^2}{2})-(-1+\frac{\alpha}{2})\Big)\\ & = \alpha x^2 + 2x + 2 - \alpha \end{align} I am not sure how to handle the $\alpha$ . For getting the quantile(=inverse) function i need to solve this last expression for x.","How could random variables with the following density function be generated from a uniform random number generator? , , . To find the Random number generator one first needs to find the distribution function and then the quantile function. However, I have troubles solving this on my own: I am not sure how to handle the . For getting the quantile(=inverse) function i need to solve this last expression for x.","f (x) = \frac{1 + \alpha x}{2}  −1 ≤ x ≤ 1 −1 ≤\alpha ≤ 1 \begin{align}
F(x) & = \int_{-1}^x \frac{1+\alpha u}{2} du\\
& = \frac 12 \Big( (x+\frac{\alpha x^2}{2})-(-1+\frac{\alpha}{2})\Big)\\
& = \alpha x^2 + 2x + 2 - \alpha
\end{align} \alpha","['probability', 'probability-theory', 'statistics', 'probability-distributions', 'inverse-function']"
72,how to prove chi-square statistics conforms to chi-square distribution with contingency table?,how to prove chi-square statistics conforms to chi-square distribution with contingency table?,,"chi-square test(principle used in C4.5's CVP Pruning), also called chi-square statistics, also called chi-square goodness-of fit How to prove $\sum_{i=1}^{i=r}\sum_{j=1}^{j=c}\frac{(x_{ij}-E_{ij} )^2}{E_{ij}}   = \chi^2_{(r-1)(c-1)}$ where $E_{ij}=\frac{N_i·N_j}{N}$ , $N$ is the total counts of the whole datasets. $N_i$ are the counts of the sub-datasets of the same-value of feature $N_j$ are the counts of the sub-datasets of the same-class please help,thanks～！ here is contingency table /------------------------------------------------ here are some references which are not clear: https://arxiv.org/pdf/1808.09171.pdf (not mention why $k-1$ is used in formula(5)) https://www.math.utah.edu/~davar/ps-pdf-files/Chisquared.pdf (Not mention why $\Theta<1$ from (9)->(10)) https://arxiv.org/pdf/1808.09171 (page 4th not mention what is X*with a line on it) http://personal.psu.edu/drh20/asymp/fall2006/lectures/ANGELchpt07.pdf (Page 109th,Not mention why $Cov(X_{ij},X_{il}=-p_ip_l)$ )","chi-square test(principle used in C4.5's CVP Pruning), also called chi-square statistics, also called chi-square goodness-of fit How to prove where , is the total counts of the whole datasets. are the counts of the sub-datasets of the same-value of feature are the counts of the sub-datasets of the same-class please help,thanks～！ here is contingency table /------------------------------------------------ here are some references which are not clear: https://arxiv.org/pdf/1808.09171.pdf (not mention why is used in formula(5)) https://www.math.utah.edu/~davar/ps-pdf-files/Chisquared.pdf (Not mention why from (9)->(10)) https://arxiv.org/pdf/1808.09171 (page 4th not mention what is X*with a line on it) http://personal.psu.edu/drh20/asymp/fall2006/lectures/ANGELchpt07.pdf (Page 109th,Not mention why )","\sum_{i=1}^{i=r}\sum_{j=1}^{j=c}\frac{(x_{ij}-E_{ij} )^2}{E_{ij}}   = \chi^2_{(r-1)(c-1)} E_{ij}=\frac{N_i·N_j}{N} N N_i N_j k-1 \Theta<1 Cov(X_{ij},X_{il}=-p_ip_l)","['probability', 'probability-theory', 'statistics', 'normal-distribution', 'chi-squared']"
73,Deriving the probability on the Probability of an Event - Chebyshev's Theorem,Deriving the probability on the Probability of an Event - Chebyshev's Theorem,,"Question: For the random variable X, where E(X) = 0 and Var(X) = 1, derive the upper bound on the probability of the event $\{|X-.6| > .2\}$ . My attempt: Using Chebyshev's Inequality- Prob( $|X-E(X)| > \epsilon) \leq \frac{Var(X)}{\epsilon^2} = \frac{1}{(.2)^2} = 25.$ A) With no distributional assumption is this correct? I am thinking that because the probability is greater than 1 then this is a trivial inequality. I have seen alternate definitions of Chebyshev's inequality where $\epsilon$ is $k \sigma$ and if k is greater than or equal to 1 then this happens. However, 25 seems like an unusual number to get. Am I missing something? B) As a follow up and for further understanding, how would the probability change if X was normally distributed with mean = 0 and variance = 1? Using the probability normal approximation integral I get the probability to be 3.96953... Although 25 and 3.96 are very different, is accuracy even a question since both probabilities are greater than 1?","Question: For the random variable X, where E(X) = 0 and Var(X) = 1, derive the upper bound on the probability of the event . My attempt: Using Chebyshev's Inequality- Prob( A) With no distributional assumption is this correct? I am thinking that because the probability is greater than 1 then this is a trivial inequality. I have seen alternate definitions of Chebyshev's inequality where is and if k is greater than or equal to 1 then this happens. However, 25 seems like an unusual number to get. Am I missing something? B) As a follow up and for further understanding, how would the probability change if X was normally distributed with mean = 0 and variance = 1? Using the probability normal approximation integral I get the probability to be 3.96953... Although 25 and 3.96 are very different, is accuracy even a question since both probabilities are greater than 1?",\{|X-.6| > .2\} |X-E(X)| > \epsilon) \leq \frac{Var(X)}{\epsilon^2} = \frac{1}{(.2)^2} = 25. \epsilon k \sigma,"['probability', 'statistics']"
74,Find the number of dentists so less than a specific number of patients are waiting at specific time t,Find the number of dentists so less than a specific number of patients are waiting at specific time t,,"Historical data: Am I right to think about this question from the perspective of Poisson Distribution? I think I have to find something like $P(X\le 3)$ , but I don't know how to find $\lambda$ , the mean number of occurrences in this case. In this case, we need to make sure there are less than $3$ patients at a specific time $t$ . How do you find the number of dentists so less than a specific number of patients are waiting at specific time t?","Historical data: Am I right to think about this question from the perspective of Poisson Distribution? I think I have to find something like , but I don't know how to find , the mean number of occurrences in this case. In this case, we need to make sure there are less than patients at a specific time . How do you find the number of dentists so less than a specific number of patients are waiting at specific time t?",P(X\le 3) \lambda 3 t,"['probability', 'probability-theory', 'statistics', 'probability-distributions']"
75,Estimating temperature most accurately with different thermometers,Estimating temperature most accurately with different thermometers,,"I recently came upon this semi-opened ended question and wanted to think through it with you guys. You have 5 measurements from 5 different thermometers, which are unbiased, but   each with a different variance. Out of those measurements, how would you ensure that you measure   the oven temperatures most accurately? I personally would take each thermometer and take the temperature $X$ number of times, lets say 40 times (total of $40*5$ times). Then I would calculate the sample $\mu$ and sample $\sigma$ and simulate a normal distribution for each thermometer. If I see that the measurements are normally distributed, I would use the thermometer with the lowest variance. I feel like my answer might be too simple and I'm wondering if I should go about it a different way.","I recently came upon this semi-opened ended question and wanted to think through it with you guys. You have 5 measurements from 5 different thermometers, which are unbiased, but   each with a different variance. Out of those measurements, how would you ensure that you measure   the oven temperatures most accurately? I personally would take each thermometer and take the temperature number of times, lets say 40 times (total of times). Then I would calculate the sample and sample and simulate a normal distribution for each thermometer. If I see that the measurements are normally distributed, I would use the thermometer with the lowest variance. I feel like my answer might be too simple and I'm wondering if I should go about it a different way.",X 40*5 \mu \sigma,"['statistics', 'normal-distribution', 'statistical-inference']"
76,Probability Function and Bounded Variation Function,Probability Function and Bounded Variation Function,,"Let $ X $ be a continuous random variable with probability function $ f_X(x) $ and distribution function $ F_X(x) $ . Assume $ f_X(x) $ is a piecewise continuous function. Is it possible to prove for an arbitrary random variable $ X $ , $ f_X(x) $ is a function of bounded variation? My attempt Let $ [a,b] \subset supp(X) $ , then we can divide $ f_X(x) $ into several subinterval where it only contains strictly increasing function, strictly decreasing function, or constant function. Let say we can divide $ (a,b) $ into $ n+1 $ subinterval with $ a = t_0 < t_2 < ... < t_n = b $ , where in every subinterval, $ f_X(x) $ is strictly increasing function, strictly decreasing function, or constant function. For arbitrary partition of $ [a,b] $ , then we can calculate variation of $ f_X(x) $ for $ x \in [a,b] $ as follows $$ V_a^b = \sup \sum_{i=0}^{n-1} | f(x_{i+1}) - f(x_i) | $$ $$ V_a^b = | f(x_{1}) - f(x_0) | + ... + | f(x_{n}) - f(x_{n-1}) | $$ Because $ 0 \leq f(x) \leq 1 $ then for every $ x_i $ and $ x_{i+1} $ , $ | f(x_{i+1}) - f(x_i) | \leq 1 $ , then the upper bound of variation of $ f_X(x) $ is $$ V_a^b \leq 1 + ... + 1 = n $$ The question still remains. How to prove $ n $ is a bounded constant? But my intuition is that, if $ f_X(x) $ is a probability function, then it is unlikely for a bounded interval, it contains infinitely number of monotone function, so $ n $ must be a bounded constant. But I cannot find any source that support my claim. I need help to support my claim (by explanation or reliable source) or maybe another way to solve it. Thank you!","Let be a continuous random variable with probability function and distribution function . Assume is a piecewise continuous function. Is it possible to prove for an arbitrary random variable , is a function of bounded variation? My attempt Let , then we can divide into several subinterval where it only contains strictly increasing function, strictly decreasing function, or constant function. Let say we can divide into subinterval with , where in every subinterval, is strictly increasing function, strictly decreasing function, or constant function. For arbitrary partition of , then we can calculate variation of for as follows Because then for every and , , then the upper bound of variation of is The question still remains. How to prove is a bounded constant? But my intuition is that, if is a probability function, then it is unlikely for a bounded interval, it contains infinitely number of monotone function, so must be a bounded constant. But I cannot find any source that support my claim. I need help to support my claim (by explanation or reliable source) or maybe another way to solve it. Thank you!"," X   f_X(x)   F_X(x)   f_X(x)   X   f_X(x)   [a,b] \subset supp(X)   f_X(x)   (a,b)   n+1   a = t_0 < t_2 < ... < t_n = b   f_X(x)   [a,b]   f_X(x)   x \in [a,b]   V_a^b = \sup \sum_{i=0}^{n-1} | f(x_{i+1}) - f(x_i) |   V_a^b = | f(x_{1}) - f(x_0) | + ... + | f(x_{n}) - f(x_{n-1}) |   0 \leq f(x) \leq 1   x_i   x_{i+1}   | f(x_{i+1}) - f(x_i) | \leq 1   f_X(x)   V_a^b \leq 1 + ... + 1 = n   n   f_X(x)   n ","['statistics', 'random-variables', 'bounded-variation']"
77,Deriving the odds ratio of a 3-way interaction logistic regression model,Deriving the odds ratio of a 3-way interaction logistic regression model,,"Suppose a logistic regression model has three binary explanatory variables $x_1$ , $x_2$ and $x_3$ used to estimate the probability of success. This model includes all three main effects, the three $2$ - way interactions, and the one $3$ -way interaction. Derive the odds ratio to examine the odds of a success across two levels of one of the explanatory variables, say $x_1$ , and derive its variance. I assume you start by: $\log(\frac{\pi}{1-\pi}) = \beta_1x_1 + \beta_2x_2 + \beta_3x_3 +\beta_4x_1x_2 + \beta_5x_1x_3 + \beta_6x_2x_3 + \beta_7x_1x_2x_3$ and then taking exponetials on both sides: $\frac{\pi}{1-\pi} = e^{\beta_1x_1 + \beta_2x_2 + \beta_3x_3 +\beta_4x_1x_2 + \beta_5x_1x_3 + \beta_6x_2x_3 + \beta_7x_1x_2x_3}$ However I am not sure how to proceed.","Suppose a logistic regression model has three binary explanatory variables , and used to estimate the probability of success. This model includes all three main effects, the three - way interactions, and the one -way interaction. Derive the odds ratio to examine the odds of a success across two levels of one of the explanatory variables, say , and derive its variance. I assume you start by: and then taking exponetials on both sides: However I am not sure how to proceed.",x_1 x_2 x_3 2 3 x_1 \log(\frac{\pi}{1-\pi}) = \beta_1x_1 + \beta_2x_2 + \beta_3x_3 +\beta_4x_1x_2 + \beta_5x_1x_3 + \beta_6x_2x_3 + \beta_7x_1x_2x_3 \frac{\pi}{1-\pi} = e^{\beta_1x_1 + \beta_2x_2 + \beta_3x_3 +\beta_4x_1x_2 + \beta_5x_1x_3 + \beta_6x_2x_3 + \beta_7x_1x_2x_3},"['probability', 'statistics', 'regression', 'logistic-regression']"
78,"If we change only one value of a data set, will the mean absolute deviation behave as the same way as standard deviation?","If we change only one value of a data set, will the mean absolute deviation behave as the same way as standard deviation?",,"I took the new data as b and the data removed as a and calculated the new mean and used that to find the new mean and deviation in terms of old. But it gets too complicated and there is no way to get the relation looking at the terms. Basically the question is, if after changing only one value of a data set, if the mean absolute deviation increases, will standard deviation always increase? Or is there any case where it can decrease too? EDIT: Taking absolute mean deviation about the mean. Basically the sum of absolute difference of every point in data set with the mean divided by the number of data points.","I took the new data as b and the data removed as a and calculated the new mean and used that to find the new mean and deviation in terms of old. But it gets too complicated and there is no way to get the relation looking at the terms. Basically the question is, if after changing only one value of a data set, if the mean absolute deviation increases, will standard deviation always increase? Or is there any case where it can decrease too? EDIT: Taking absolute mean deviation about the mean. Basically the sum of absolute difference of every point in data set with the mean divided by the number of data points.",,"['statistics', 'standard-deviation']"
79,Existence of a limiting sum of random variables,Existence of a limiting sum of random variables,,Consider that $X_i$ ‘s are independent exponentially distributed random variables with mean $1/i$ (and thus variance $1/i^2$ ).Then the sum of them seems to converge to a “random variable” with finite variance but unbounded mean. What’s the problem here? Why does not such random variable exist? Thanks,Consider that ‘s are independent exponentially distributed random variables with mean (and thus variance ).Then the sum of them seems to converge to a “random variable” with finite variance but unbounded mean. What’s the problem here? Why does not such random variable exist? Thanks,X_i 1/i 1/i^2,"['probability', 'statistics', 'random-variables']"
80,"Probability ""at-least"" question with large numbers","Probability ""at-least"" question with large numbers",,"The mean monthly rent of all 1000 one-bedroom condos for rent is   1560 dollars, with a standard deviation of 224 dollars. 67.7% of these condos have   monthly rents lower than 1600 dollars If 64 one-bedroom condos for rent are randomly selected from 1000   one-bedroom, what is the probability that at least 25 of the sampled   condos cost less than 1600 dollars per month to rent I'm not entirely sure that I'm approaching this question in the right way. I've checked that it follows an approximately normal distribution (which it does) and I've calculated the standard deviation of the sample and found that the $1600 value is between 1 and 2 standard deviations of the mean (mean = 1560, mean + 1SD =1588, mean +2SD = 1616) From there though, I'm not sure how to use the 68-95-99.7 rule to determine the probability (if its even possible, I honestly don't know if I'm on the right track with answering this question) Thanks for any help anyone can give!","The mean monthly rent of all 1000 one-bedroom condos for rent is   1560 dollars, with a standard deviation of 224 dollars. 67.7% of these condos have   monthly rents lower than 1600 dollars If 64 one-bedroom condos for rent are randomly selected from 1000   one-bedroom, what is the probability that at least 25 of the sampled   condos cost less than 1600 dollars per month to rent I'm not entirely sure that I'm approaching this question in the right way. I've checked that it follows an approximately normal distribution (which it does) and I've calculated the standard deviation of the sample and found that the $1600 value is between 1 and 2 standard deviations of the mean (mean = 1560, mean + 1SD =1588, mean +2SD = 1616) From there though, I'm not sure how to use the 68-95-99.7 rule to determine the probability (if its even possible, I honestly don't know if I'm on the right track with answering this question) Thanks for any help anyone can give!",,"['probability', 'statistics']"
81,Is every probability distribution,Is every probability distribution,,Say I have some arbitrary $N$ -dimensional probability distribution $P$ . I'd like to create a deterministic function that accepts a vector of $N$ uniformly distributed numbers and generates an $N$ element vector where the elements of this vector are distributed according to $P$ . Is this always possible?,Say I have some arbitrary -dimensional probability distribution . I'd like to create a deterministic function that accepts a vector of uniformly distributed numbers and generates an element vector where the elements of this vector are distributed according to . Is this always possible?,N P N N P,"['probability', 'statistics']"
82,Help with Likelihood Function Notation in Logistic Regression,Help with Likelihood Function Notation in Logistic Regression,,"Can someone please help me understand below notation I encountered while studying logistic regression? I am pretty sure I lack the mathematical maturity but would like to give it a try. $l(\beta_0,\beta_1) = \prod_{i:y_{i=1}} p(x_i) \prod_{i':y_{i'=0}} (1-p(x_i'))$ where $p(X)$ = $\frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}$ What does $\prod$ notation stand for? What does the index $i$ comprise of? What does the lower bound $i:y_{i=1}$ mean? What does the first equation mean on the whole? Please let me know if I missed providing any context information.",Can someone please help me understand below notation I encountered while studying logistic regression? I am pretty sure I lack the mathematical maturity but would like to give it a try. where = What does notation stand for? What does the index comprise of? What does the lower bound mean? What does the first equation mean on the whole? Please let me know if I missed providing any context information.,"l(\beta_0,\beta_1) = \prod_{i:y_{i=1}} p(x_i) \prod_{i':y_{i'=0}} (1-p(x_i')) p(X) \frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}} \prod i i:y_{i=1}",['statistics']
83,Pareto distribution moments,Pareto distribution moments,,"I have 2 questions where I have to use the Pareto distribution. A family of pdf’s that has been used to approximate the distribution of income, city population size, and size of firms is the Pareto family. The family has two parameters, $k$ and $y$ , both $> 0$ , and the pdf is: $$ f(x;k,\theta)=\frac{k\cdot\theta^k}{x^{k+1}} \ for \ x\geq\theta $$ and is $ 0 $ otherwise. a) Sketch the graph of $ f(x;k,\theta) $ b) Verify the the total area under the graph is $ 1 $ . c) For $ \theta<a<b $ , obtain an expression for the probability $P(a\leq{X}\leq{b}). $ For this question, I have integrated and proved part b) because all $k $ and $\theta$ cancel. I have also got an expression for part c) by just changing the bounds to $ a $ and $b $ . However, I have no idea how to graph the 3 variables, any thoughts? Let X have the Pareto pdf introduced in Exercise 1. a) If $k>1$ , compute $E(X)$ b) What can you say about $E(X)$ if $k=1?$ c) If $k>2$ , show that $ V(X)=k\theta^2(k-1)^{-2}(k-2)^{-1} $ . d) If $k=2$ , what can you say about $V(C)$ ? e) What conditions on $k$ are necessary to make $ E(X^n) $ finite? I got $E(X)=\int_\theta^\infty{x\frac{k\theta^k}{x^{k+1}}}dx=k\theta^k\int_\theta^\infty{x^{-k}}dx=-\frac{k\theta}{-k+1} $ . And I got $E(X^2)=\int_\theta^\infty{x^2\frac{k\theta^k}{x^{k+1}}}dx=k\theta^k\int_\theta^\infty{x^{-k+1}dx}=\frac{-k\theta^2}{-k+2}$","I have 2 questions where I have to use the Pareto distribution. A family of pdf’s that has been used to approximate the distribution of income, city population size, and size of firms is the Pareto family. The family has two parameters, and , both , and the pdf is: and is otherwise. a) Sketch the graph of b) Verify the the total area under the graph is . c) For , obtain an expression for the probability For this question, I have integrated and proved part b) because all and cancel. I have also got an expression for part c) by just changing the bounds to and . However, I have no idea how to graph the 3 variables, any thoughts? Let X have the Pareto pdf introduced in Exercise 1. a) If , compute b) What can you say about if c) If , show that . d) If , what can you say about ? e) What conditions on are necessary to make finite? I got . And I got","k y > 0  f(x;k,\theta)=\frac{k\cdot\theta^k}{x^{k+1}} \ for \ x\geq\theta   0   f(x;k,\theta)   1   \theta<a<b  P(a\leq{X}\leq{b}).  k  \theta  a  b  k>1 E(X) E(X) k=1? k>2  V(X)=k\theta^2(k-1)^{-2}(k-2)^{-1}  k=2 V(C) k  E(X^n)  E(X)=\int_\theta^\infty{x\frac{k\theta^k}{x^{k+1}}}dx=k\theta^k\int_\theta^\infty{x^{-k}}dx=-\frac{k\theta}{-k+1}  E(X^2)=\int_\theta^\infty{x^2\frac{k\theta^k}{x^{k+1}}}dx=k\theta^k\int_\theta^\infty{x^{-k+1}dx}=\frac{-k\theta^2}{-k+2}","['statistics', 'probability-distributions', 'expected-value']"
84,Using MGFs to determine independence of sum and difference of two variables $(X+Y$ and $X-Y)$,Using MGFs to determine independence of sum and difference of two variables  and,(X+Y X-Y),"I have a question for class that asks: Let $X$ and $Y$ be i.i.d. Unif $(0,1)$ . Are $X+Y$ and $X-Y$ independent? In an earlier part of the question I found that the covariance between (X+Y) and (X-Y) was 0, but I know this does not necessarily mean independence. My professor advisd that I use their MGFs, because I know that if they are independent then: $$ M_{(X+Y)+(X-Y)}(t) = M_{X+Y}(t) \cdot M_{X-Y}(t) $$ I think that if I let W = X+Y: $$ M_{W}(t) = M_{w_1}(t) + M_{W_2}(t) = M_{X}(t) + M_Y(t) $$ and if I let U = X-Y: $$ M_{U}(t) = M_{U_1}(t) + M_{U_2}(t) = M_{X}(t) + M_{-Y}(t) $$ (but I'm not really sure how to handle the negative). and finally, if I let Z = (X+Y)+(X-Y) = X + Y + X - Y = X + X: $$ M_Z(t) = M_{Z_1}(t) + M_{Z_2}(t) = M_{X}(t) + M_{X}(t) $$ I also know that because X and Y are i.i.d. their MGFs should be the same. So it looks like: $$ M_{(X+Y)+(X-Y)}(t) = 2 \cdot M_X(t) $$ and $$ M_{X+Y}(t) \cdot M_{X-Y}(t) = 3 \cdot M_{X}(t) + M_{-Y}(t) $$ BUT, if that negative signed in $M_{U}$ worked out differently, so that it was $M_X(t) - M_Y(t)$ when I know the MGFs are the same, then the whole thing would come out differently! Both MGFs would ultimately be $2 \cdot M_X(t)$ and they would be independent. (Based on a passage in my text, I think they're supposed to come out independent). So is this whole approach bad, or is it possible I mishandled the negative?","I have a question for class that asks: Let and be i.i.d. Unif . Are and independent? In an earlier part of the question I found that the covariance between (X+Y) and (X-Y) was 0, but I know this does not necessarily mean independence. My professor advisd that I use their MGFs, because I know that if they are independent then: I think that if I let W = X+Y: and if I let U = X-Y: (but I'm not really sure how to handle the negative). and finally, if I let Z = (X+Y)+(X-Y) = X + Y + X - Y = X + X: I also know that because X and Y are i.i.d. their MGFs should be the same. So it looks like: and BUT, if that negative signed in worked out differently, so that it was when I know the MGFs are the same, then the whole thing would come out differently! Both MGFs would ultimately be and they would be independent. (Based on a passage in my text, I think they're supposed to come out independent). So is this whole approach bad, or is it possible I mishandled the negative?","X Y (0,1) X+Y X-Y  M_{(X+Y)+(X-Y)}(t) = M_{X+Y}(t) \cdot M_{X-Y}(t)   M_{W}(t) = M_{w_1}(t) + M_{W_2}(t) = M_{X}(t) + M_Y(t)   M_{U}(t) = M_{U_1}(t) + M_{U_2}(t) = M_{X}(t) + M_{-Y}(t)   M_Z(t) = M_{Z_1}(t) + M_{Z_2}(t) = M_{X}(t) + M_{X}(t)   M_{(X+Y)+(X-Y)}(t) = 2 \cdot M_X(t)   M_{X+Y}(t) \cdot M_{X-Y}(t) = 3 \cdot M_{X}(t) + M_{-Y}(t)  M_{U} M_X(t) - M_Y(t) 2 \cdot M_X(t)","['probability', 'statistics', 'moment-generating-functions']"
85,Joint PDF of min and max of iid uniform distributions,Joint PDF of min and max of iid uniform distributions,,"$U_1$ and $U_2$ are identically independently distributed ~uni[0,1].  I'm trying to find the joint PDF between the $P_1$ =min( $U_1$ , $U_2$ ) and $P_2$ =max( $U_1$ , $U_2$ ).  I have found the marginal pdfs: $f_{p1}(y)=2y^2-4y+2$ $f_{p2}(x)=2x$ Since the min and max are not independent, I think the best course of action would be to find the conditional probability by conditioning on $U_1$ and then multiplying by the pdf of $U_1$ . $$\int_0^1 P(min(U_1, U_2)<p_{1} ,P(max(U_1, U_2)<p_{2}\ \ |\ \  U_1=u)\ f_u(u)  \;\mathrm{dx}$$ However, I am stuck now.","and are identically independently distributed ~uni[0,1].  I'm trying to find the joint PDF between the =min( , ) and =max( , ).  I have found the marginal pdfs: Since the min and max are not independent, I think the best course of action would be to find the conditional probability by conditioning on and then multiplying by the pdf of . However, I am stuck now.","U_1 U_2 P_1 U_1 U_2 P_2 U_1 U_2 f_{p1}(y)=2y^2-4y+2 f_{p2}(x)=2x U_1 U_1 \int_0^1 P(min(U_1, U_2)<p_{1} ,P(max(U_1, U_2)<p_{2}\ \ |\ \  U_1=u)\ f_u(u)  \;\mathrm{dx}","['probability', 'statistics', 'probability-distributions']"
86,P-value vs. Bayesian statistics,P-value vs. Bayesian statistics,,"Are there any theoretical considerations between $p$ -value and the Bayesian statistics? I mean say, any theorem regarding both of these two concepts at a time.","Are there any theoretical considerations between -value and the Bayesian statistics? I mean say, any theorem regarding both of these two concepts at a time.",p,"['probability-theory', 'statistics', 'statistical-inference', 'bayesian', 'p-value']"
87,Variance of $\int_0^1X(t)dt$,Variance of,\int_0^1X(t)dt,"Let $X(t)$ be a stationary random variable with expected value $E[X(t)] = m$ and  covariance function $r_X(\tau) = 2e^{-|\tau|}$ . I'm asked to calculate the variance of $\int_0^1X(t)dt$ , $$V[\int_0^1X(t)dt].$$ I've tried using the formula $$C[X,Y] = E[XY] - E[X]E[Y]$$ but I can't figure out $$E[\int_0^1X(t)dt\cdot\int_0^1X(t)dt].$$ How do I do it? Note; all the information given may not be necessary. The answer is supposedly $4e^{-1}.$","Let be a stationary random variable with expected value and  covariance function . I'm asked to calculate the variance of , I've tried using the formula but I can't figure out How do I do it? Note; all the information given may not be necessary. The answer is supposedly","X(t) E[X(t)] = m r_X(\tau) = 2e^{-|\tau|} \int_0^1X(t)dt V[\int_0^1X(t)dt]. C[X,Y] = E[XY] - E[X]E[Y] E[\int_0^1X(t)dt\cdot\int_0^1X(t)dt]. 4e^{-1}.","['statistics', 'stochastic-processes', 'random-variables', 'variance']"
88,Sum of probabilities of events vs. probability of at least one event,Sum of probabilities of events vs. probability of at least one event,,"$P_i$ is the probability of one event. As defined below, $a$ is the sum of all probabilities of events (that may or may not be independent), and $c$ is the overall probability that at least one event will happen. $a=\sum_{i=0}^n P_i $ $c=(1-\prod_{i=0}^n (1-P_i))$ $0\leq P_i \leq 1$ Take two events with probability of $0.5$ and $0.2$ for example: $$ a= 0.5 +0.2=0.7~, \qquad c = 1 - 0.5 \cdot 0.8=0.6$$ Is it true $\dfrac{a}{c}$ will increase as $n$ , the number of $P_i$ , increases ? It seems true to me based on sample results, but I can't think of a formal proof.","is the probability of one event. As defined below, is the sum of all probabilities of events (that may or may not be independent), and is the overall probability that at least one event will happen. Take two events with probability of and for example: Is it true will increase as , the number of , increases ? It seems true to me based on sample results, but I can't think of a formal proof.","P_i a c a=\sum_{i=0}^n P_i  c=(1-\prod_{i=0}^n (1-P_i)) 0\leq P_i \leq 1 0.5 0.2  a= 0.5 +0.2=0.7~, \qquad c = 1 - 0.5 \cdot 0.8=0.6 \dfrac{a}{c} n P_i","['calculus', 'probability', 'algebra-precalculus', 'statistics', 'inequality']"
89,"How to calculate statistical ""difference"" of two samples in 0-100 range?","How to calculate statistical ""difference"" of two samples in 0-100 range?",,"There are two groups of people, target and neutral There are two group of events X and others We're making assumption that people from target group react on event X very different to others. We have some sets of reaction numeric values on different type of events by those two groups. And after running Student's T-Test in excel I get 0.032 for target group and 0.55 for others so I can tell that target group react on X different but is it possible to measure how much different in 0-100 range? example of data set (not real data set) for both groups looks like 30, 20, 45, 15, 26 ... - reaction on X event 10, 14, 22, 8, 13 ... - reactions on other events What can I use to measure how much reaction X different (alike in T-Test) to reaction on other events in 0-100 range and is it possible in general?","There are two groups of people, target and neutral There are two group of events X and others We're making assumption that people from target group react on event X very different to others. We have some sets of reaction numeric values on different type of events by those two groups. And after running Student's T-Test in excel I get 0.032 for target group and 0.55 for others so I can tell that target group react on X different but is it possible to measure how much different in 0-100 range? example of data set (not real data set) for both groups looks like 30, 20, 45, 15, 26 ... - reaction on X event 10, 14, 22, 8, 13 ... - reactions on other events What can I use to measure how much reaction X different (alike in T-Test) to reaction on other events in 0-100 range and is it possible in general?",,['statistics']
90,"Probability of missing dart throw given uniform random radius (0,1) of board","Probability of missing dart throw given uniform random radius (0,1) of board",,"Given that the radius of a dart board is uniformly distributed between 0 and 1, I throw a dart uniformly at random within a circle of radius 1. What is the probability of not hitting the dart board? I have calculated the expected area of the dart board E(A) = π/3. Would it be correct to assume (1 - π/3)/π = 2/3 is the probability of not hitting the dart board? Or would I need to use integration given the continuous probability?","Given that the radius of a dart board is uniformly distributed between 0 and 1, I throw a dart uniformly at random within a circle of radius 1. What is the probability of not hitting the dart board? I have calculated the expected area of the dart board E(A) = π/3. Would it be correct to assume (1 - π/3)/π = 2/3 is the probability of not hitting the dart board? Or would I need to use integration given the continuous probability?",,"['probability', 'statistics', 'uniform-distribution']"
91,Likelihood Ratio Test for binomial proportions,Likelihood Ratio Test for binomial proportions,,"Suppose we wish to preform a simple likelihood ratio test for the parameters of two binomial distributions. Where the null hypothesis is that the two parameters are equal versus the alternative they are not. Now for the following example, to construct a normal test or T-test would be straightforward. However I am interested in comparing this to using the likelihood ratio test and chi-square distribution. Say we know for example $A∼Bin(40,p_{A})$ and $B∼Bin(20,p_{B})$ and we observe data from A and we observe $24$ successes hence we can then estimate $p=0.6$ and from B we observe data and observe $15$ successes hence $p=0.75$ How would one go about forming the likelihood test statistic?","Suppose we wish to preform a simple likelihood ratio test for the parameters of two binomial distributions. Where the null hypothesis is that the two parameters are equal versus the alternative they are not. Now for the following example, to construct a normal test or T-test would be straightforward. However I am interested in comparing this to using the likelihood ratio test and chi-square distribution. Say we know for example and and we observe data from A and we observe successes hence we can then estimate and from B we observe data and observe successes hence How would one go about forming the likelihood test statistic?","A∼Bin(40,p_{A}) B∼Bin(20,p_{B}) 24 p=0.6 15 p=0.75","['probability', 'statistics', 'hypothesis-testing']"
92,How much does the Top 1% have?,How much does the Top 1% have?,,"Please consider the following problem and my answer to it. The answer seems to low to me. Is my answer right? Thanks, Bob Problem: The median net worth of a certain population is $97K$ . To be in the top $5\%$ you need to have a net worth of $2387K$ . Assuming the population follows the Pareto distribution, what is the minimum net worth you need to be in top $1\%$ ? Answer: For the Pareto distribution we have: \begin{eqnarray*} P(X > x) &=& \Big( \frac{x}{x_m}  \Big) ^ \alpha \\ \end{eqnarray*} The above formula assumes that $x$ is greater than $0$ . Now, we need to find the two parameters $x_m$ and $\alpha$ . \begin{eqnarray*} \Big( \frac{97}{x_m}  \Big) ^ \alpha &=& 0.5 \\ \Big( \frac{2387}{x_m}  \Big) ^ \alpha &=& 0.95 \\ \end{eqnarray*} Using software, I find: \begin{eqnarray*} x_m &=& 3083 \\ \alpha &=& 0.200386 \\ \end{eqnarray*} Now we have our model. \begin{eqnarray*} 0.99 &=& \Big( \frac{x}{x_m}  \Big) ^ \alpha \\ 0.99 &=& \Big( \frac{x}{3083}  \Big) ^  {0.200386}\\ 0.9510821 &=& \Big( \frac{x}{3083}  \Big) \\ x &=& 3083( 0.9510821 ) \\ x &=& 2932.1861 \\ \end{eqnarray*} Here is an updated solution: For the Pareto distribution we have: \begin{eqnarray*} P(X > x) &=& \Big( \frac{x_m}{x}  \Big) ^ \alpha \\ \end{eqnarray*} The above formula assumes that $x$ is greater than $0$ . Now, we need to find the two parameters $x_m$ and $\alpha$ . \begin{eqnarray*} \Big( \frac{x_m}{97}  \Big) ^ \alpha &=& 0.5 \\ \Big( \frac{x_m}{2387}  \Big) ^ \alpha &=& 0.95 \\ \end{eqnarray*} Using software, I find: \begin{eqnarray*} x_m &=& 3083.33 \\ \alpha &=& -0.200386 \\ \end{eqnarray*} Now we have our model. \begin{eqnarray*} 0.99 &=& \Big( \frac{x_m}{x}  \Big) ^ \alpha \\ 0.99 &=& \Big( \frac{3083.33}{x}  \Big) ^ { -0.200386 } \\ 1.010101 &=& \Big( \frac{3083.33}{x}  \Big) ^ { 0.200386 } \\ 1.00514339 &=& \Big( \frac{3083.33}{x}  \Big) \\ x &=& 2932.50 \\ \end{eqnarray*} Is this answer right? My feeling is that it is still too low? If so, what did I do wrong?","Please consider the following problem and my answer to it. The answer seems to low to me. Is my answer right? Thanks, Bob Problem: The median net worth of a certain population is . To be in the top you need to have a net worth of . Assuming the population follows the Pareto distribution, what is the minimum net worth you need to be in top ? Answer: For the Pareto distribution we have: The above formula assumes that is greater than . Now, we need to find the two parameters and . Using software, I find: Now we have our model. Here is an updated solution: For the Pareto distribution we have: The above formula assumes that is greater than . Now, we need to find the two parameters and . Using software, I find: Now we have our model. Is this answer right? My feeling is that it is still too low? If so, what did I do wrong?","97K 5\% 2387K 1\% \begin{eqnarray*}
P(X > x) &=& \Big( \frac{x}{x_m}  \Big) ^ \alpha \\
\end{eqnarray*} x 0 x_m \alpha \begin{eqnarray*}
\Big( \frac{97}{x_m}  \Big) ^ \alpha &=& 0.5 \\
\Big( \frac{2387}{x_m}  \Big) ^ \alpha &=& 0.95 \\
\end{eqnarray*} \begin{eqnarray*}
x_m &=& 3083 \\
\alpha &=& 0.200386 \\
\end{eqnarray*} \begin{eqnarray*}
0.99 &=& \Big( \frac{x}{x_m}  \Big) ^ \alpha \\
0.99 &=& \Big( \frac{x}{3083}  \Big) ^  {0.200386}\\
0.9510821 &=& \Big( \frac{x}{3083}  \Big) \\
x &=& 3083( 0.9510821 ) \\
x &=& 2932.1861 \\
\end{eqnarray*} \begin{eqnarray*}
P(X > x) &=& \Big( \frac{x_m}{x}  \Big) ^ \alpha \\
\end{eqnarray*} x 0 x_m \alpha \begin{eqnarray*}
\Big( \frac{x_m}{97}  \Big) ^ \alpha &=& 0.5 \\
\Big( \frac{x_m}{2387}  \Big) ^ \alpha &=& 0.95 \\
\end{eqnarray*} \begin{eqnarray*}
x_m &=& 3083.33 \\
\alpha &=& -0.200386 \\
\end{eqnarray*} \begin{eqnarray*}
0.99 &=& \Big( \frac{x_m}{x}  \Big) ^ \alpha \\
0.99 &=& \Big( \frac{3083.33}{x}  \Big) ^ { -0.200386 } \\
1.010101 &=& \Big( \frac{3083.33}{x}  \Big) ^ { 0.200386 } \\
1.00514339 &=& \Big( \frac{3083.33}{x}  \Big) \\
x &=& 2932.50 \\
\end{eqnarray*}",['statistics']
93,Stuck on a term in $Var[\hat{\beta_o}]$ Proof,Stuck on a term in  Proof,Var[\hat{\beta_o}],"So I was trying to prove that $Var[\hat{\beta_o}]=\dfrac{\sigma^2n^{-1}\sum{(x_i)^2}}{\sum{(x_i-\bar{x}})^2}$ And I got stuck with the part $\dfrac{   -2\bar{x}}{\sum{(x_i-\bar{x})^2}}\sum[{(x_i  -     \bar{x})}E(u_i\bar{u})]$ Now, I need to show that this whole expression equals 0 but I don't know how to go about it. I'm mainly confused as to how I should be treating the $E(u_i\bar{u})$ term. I have read that $u_i$ and $u_j$ are uncorrelated so $E(u_iu_j)=0$ so long as $i \ne j$ . But I don't know how I can use that fact to reduce the $E(u_i\bar{u})$ part to something. The $u_i$ I'm assuming would go on forever i.e. $i=1,2,3........$ but the $\bar{u}$ would only have the values of $u$ that go from $1$ to $n$ like $u_1+u_2+....+u_n$ , so how should I go about this? Note: $u_i's$ are NOT the residual terms. These are the actual $u's$ from the line/function that models the relationship between $y$ and $x$ And $\bar{u}$ would be defined to be actual $u's$ from $1$ to $n$ whole divided by $n$ Can anyone help simplify this? Thank you.","So I was trying to prove that And I got stuck with the part Now, I need to show that this whole expression equals 0 but I don't know how to go about it. I'm mainly confused as to how I should be treating the term. I have read that and are uncorrelated so so long as . But I don't know how I can use that fact to reduce the part to something. The I'm assuming would go on forever i.e. but the would only have the values of that go from to like , so how should I go about this? Note: are NOT the residual terms. These are the actual from the line/function that models the relationship between and And would be defined to be actual from to whole divided by Can anyone help simplify this? Thank you.","Var[\hat{\beta_o}]=\dfrac{\sigma^2n^{-1}\sum{(x_i)^2}}{\sum{(x_i-\bar{x}})^2} \dfrac{   -2\bar{x}}{\sum{(x_i-\bar{x})^2}}\sum[{(x_i  -     \bar{x})}E(u_i\bar{u})] E(u_i\bar{u}) u_i u_j E(u_iu_j)=0 i \ne j E(u_i\bar{u}) u_i i=1,2,3........ \bar{u} u 1 n u_1+u_2+....+u_n u_i's u's y x \bar{u} u's 1 n n","['statistics', 'proof-verification', 'proof-writing', 'variance', 'expected-value']"
94,If T follows a t-distribution them prove that U=T^2 follows an F-distribution,If T follows a t-distribution them prove that U=T^2 follows an F-distribution,,Is this correct is there a more elegant way to do this,Is this correct is there a more elegant way to do this,,['statistics']
95,Correction factor for calculating standard error when drawing sample without replacement - derivation,Correction factor for calculating standard error when drawing sample without replacement - derivation,,When drawing a sample of size n from population of size N This relationship holds $$ SE\space when\space  drawing\space  sample\space  without\space  replacement\space  =\space  correction\space  factor\space  *\space  SE\space  when\space  drawing\space  with\space  replacement$$ where $$ correction\space factor = \sqrt{\frac{N-n}{N-1}} $$ N = size of population n = size of sample I would like to know how this relationship was derived.,When drawing a sample of size n from population of size N This relationship holds where N = size of population n = size of sample I would like to know how this relationship was derived., SE\space when\space  drawing\space  sample\space  without\space  replacement\space  =\space  correction\space  factor\space  *\space  SE\space  when\space  drawing\space  with\space  replacement  correction\space factor = \sqrt{\frac{N-n}{N-1}} ,"['statistics', 'statistical-inference', 'descriptive-statistics', 'standard-error']"
96,Probability density function of sum of uniform random variables,Probability density function of sum of uniform random variables,,"Let $X$ and $Y$ be independent random variables with density functions $f_X(a)  \begin{cases} 1\over2   & -1\le a\le1 \\ 0 & \text{otherwise} \end{cases}$ $f_Y(a)  \begin{cases} 1\over2   & 3\le a\le5 \\ 0 & \text{otherwise} \end{cases}$ Find the probability density function of $X + Y$ I have been trying to do this without convolution, but with using the standard double integral method of finding the CDF and then the PDF (as opposed to using a purely geometrical method) and I have been successful in the case of $2 \le a \le 4$ by using an integral of ${1\over 4}\int_3^a \int_{-1}^{a-y}dxdy$ However I have been having trouble in the other case of $4 \le a \le 6$ . My attempt was this: $${1\over 4}\int_3^5 \int_{-1}^{min \{1, a-y\}}\ dx \ dy$$ $${1\over 4}\left (\int_3^5 ({min \{1, a-y\}} +1) \ dy \right)$$ $${1\over 4}\left (\int_3^5 ({min \{1, a-y\}}\ dy+ \int_3^5 1\ dy \right)$$ At this point I said that since the minimum function depends on whether $y>a-1$ or $y<a-1$ , therefore there are two cases and thus $${1\over 4}\left (\int_3^{a-1}1 \ dy +\int_{a-1}^{5} \ y \ dy+ \int_3^5 1\ dy \right)$$ $${1\over 4}\left (a-4-\dfrac{a^2-2a-24}{2}+ 2\right)$$ Taking the derivative of this CDF yields ${1\over2} - {a\over4}$ , whereas the true solution is ${3\over2} - {a\over4}$ . Where am I going wrong?","Let and be independent random variables with density functions Find the probability density function of I have been trying to do this without convolution, but with using the standard double integral method of finding the CDF and then the PDF (as opposed to using a purely geometrical method) and I have been successful in the case of by using an integral of However I have been having trouble in the other case of . My attempt was this: At this point I said that since the minimum function depends on whether or , therefore there are two cases and thus Taking the derivative of this CDF yields , whereas the true solution is . Where am I going wrong?","X Y f_X(a) 
\begin{cases}
1\over2   & -1\le a\le1 \\
0 & \text{otherwise}
\end{cases} f_Y(a) 
\begin{cases}
1\over2   & 3\le a\le5 \\
0 & \text{otherwise}
\end{cases} X + Y 2 \le a \le 4 {1\over 4}\int_3^a \int_{-1}^{a-y}dxdy 4 \le a \le 6 {1\over 4}\int_3^5 \int_{-1}^{min \{1, a-y\}}\ dx \ dy {1\over 4}\left (\int_3^5 ({min \{1, a-y\}} +1) \ dy \right) {1\over 4}\left (\int_3^5 ({min \{1, a-y\}}\ dy+ \int_3^5 1\ dy \right) y>a-1 y<a-1 {1\over 4}\left (\int_3^{a-1}1 \ dy +\int_{a-1}^{5} \ y \ dy+ \int_3^5 1\ dy \right) {1\over 4}\left (a-4-\dfrac{a^2-2a-24}{2}+ 2\right) {1\over2} - {a\over4} {3\over2} - {a\over4}","['probability', 'statistics', 'probability-distributions']"
97,Root-$n$ consistency of an estimator implies its variance is $O(n^{-1})$,Root- consistency of an estimator implies its variance is,n O(n^{-1}),"(This is from a claim on page 20 of Hall's 1992 The Bootstrap and the Edgeworth Expansion.) Suppose we observe i.i.d. data $X_1,\ldots,X_n$ from some unknown CDF $F$ . Let $\theta_0=\theta(F)$ be a parameter that can be computed from the CDF and $\hat\theta$ an estimator. Hall then mentions: ""should $\hat\theta$ be $\sqrt{n}$ -consistent for $\theta_0$ , so that $E(\hat\theta-\theta_0)^2=O(n^{-1})$ ..."" Why is this true? Could you please in particular provide a rigorous argument (you can make further assumptions if necessary)? My interest in this study is to see how such an argument can be constructed. My progress so far is that I understand the claim intuitively, as follows: the $\sqrt{n}$ -consistency of $\hat\theta$ means there is some fixed positive number $V$ such that $$ \sqrt{n}(\hat\theta-\theta_0)\approx N(0,V)\implies E[(\hat\theta-\theta_0)^2]\approx\frac{1}{n}V=O(n^{-1}). $$","(This is from a claim on page 20 of Hall's 1992 The Bootstrap and the Edgeworth Expansion.) Suppose we observe i.i.d. data from some unknown CDF . Let be a parameter that can be computed from the CDF and an estimator. Hall then mentions: ""should be -consistent for , so that ..."" Why is this true? Could you please in particular provide a rigorous argument (you can make further assumptions if necessary)? My interest in this study is to see how such an argument can be constructed. My progress so far is that I understand the claim intuitively, as follows: the -consistency of means there is some fixed positive number such that","X_1,\ldots,X_n F \theta_0=\theta(F) \hat\theta \hat\theta \sqrt{n} \theta_0 E(\hat\theta-\theta_0)^2=O(n^{-1}) \sqrt{n} \hat\theta V 
\sqrt{n}(\hat\theta-\theta_0)\approx N(0,V)\implies E[(\hat\theta-\theta_0)^2]\approx\frac{1}{n}V=O(n^{-1}).
","['probability-theory', 'statistics', 'proof-writing', 'asymptotics']"
98,MLE and method of moments estimator (example),MLE and method of moments estimator (example),,"Let $X_1,X_2,\dots ,X_n$ be a random sample from the Gamma distribution $$ f(x,\theta)=\theta^2 x e^{-\theta x},\quad x>0$$ To find the Maximum Likelihood Estimator , we define the likelihood function as:  $$ L(\theta;x_i)=\prod_{i=1}^nf(x_i;\theta)$$ and we demand  $$\frac{\partial \ln[L(\theta;x_i)]}{\partial\theta}=0$$ For this example  $$L(\theta;x_i)=\theta^{2n}\cdot \prod_{i=1}^n x_i\cdot e^{-\theta \sum_{i=1}^nx_i}$$ $$\ln[L(\theta;x_i)]=2n\ln(\theta)+n\ln\bigg[\prod_{i=1}^nx_i\bigg]-\theta \sum_{i=1}^nx_i$$ So $$\frac{2n}{\theta}-\sum_{i=1}^n x_i=0\iff \hat{\theta}=\frac{2}{\bar{X}}$$ To find the estimator of $\theta$ using the method of moments $$\bar{X}=E(X)=\mu$$ $$E(X)=\int_0^\infty x f(x)dx=\theta^2\cdot\frac{2}{\theta^3}=\frac{2}{\theta}$$ $$\Rightarrow \tilde{\theta}=\frac{2}{\bar{X}}$$ Is there some kind of relation between the two? Why are the expressions of $\hat{\theta}$ and $\tilde{\theta}$ so similar?","Let $X_1,X_2,\dots ,X_n$ be a random sample from the Gamma distribution $$ f(x,\theta)=\theta^2 x e^{-\theta x},\quad x>0$$ To find the Maximum Likelihood Estimator , we define the likelihood function as:  $$ L(\theta;x_i)=\prod_{i=1}^nf(x_i;\theta)$$ and we demand  $$\frac{\partial \ln[L(\theta;x_i)]}{\partial\theta}=0$$ For this example  $$L(\theta;x_i)=\theta^{2n}\cdot \prod_{i=1}^n x_i\cdot e^{-\theta \sum_{i=1}^nx_i}$$ $$\ln[L(\theta;x_i)]=2n\ln(\theta)+n\ln\bigg[\prod_{i=1}^nx_i\bigg]-\theta \sum_{i=1}^nx_i$$ So $$\frac{2n}{\theta}-\sum_{i=1}^n x_i=0\iff \hat{\theta}=\frac{2}{\bar{X}}$$ To find the estimator of $\theta$ using the method of moments $$\bar{X}=E(X)=\mu$$ $$E(X)=\int_0^\infty x f(x)dx=\theta^2\cdot\frac{2}{\theta^3}=\frac{2}{\theta}$$ $$\Rightarrow \tilde{\theta}=\frac{2}{\bar{X}}$$ Is there some kind of relation between the two? Why are the expressions of $\hat{\theta}$ and $\tilde{\theta}$ so similar?",,"['statistics', 'probability-distributions', 'maximum-likelihood']"
99,Variance of $X-Y$ cannot be negative,Variance of  cannot be negative,X-Y,"Suppose we have 2 non independent variable X and Y. Since $$Var(X-Y)= Var(X) + Var (Y) -2Cov(X,Y)$$ Would it be possible for the above to be negative in the case when $$Var(X) + Var(Y) < 2 Cov(X,Y)$$?","Suppose we have 2 non independent variable X and Y. Since $$Var(X-Y)= Var(X) + Var (Y) -2Cov(X,Y)$$ Would it be possible for the above to be negative in the case when $$Var(X) + Var(Y) < 2 Cov(X,Y)$$?",,"['statistics', 'covariance']"
