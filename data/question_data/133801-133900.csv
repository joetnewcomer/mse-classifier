,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Pursuit Curve. Dog Chases Rabbit. Calculus 4.,Pursuit Curve. Dog Chases Rabbit. Calculus 4.,,"(a) In Example 1.21, assume that $a$ is less than $b$ (so that $k$ is less than $1$) and find $y$ as a function of $x$.  How far does the rabbit run before the dog catches him? (b)  Assume now that $a=b$, and find $y$ as a function of $x$.  How close does the dog come to the rabbit? Example 1.21 A rabbit begins at the origin and runs up the $y-axis$ with speed $a$ feet per second.  At the same time, a dog runs at speed $b$ from the point $(c,0)$ in pursuit of the rabbit.  What is the path of the dog? Solution:  At time $t$, measured from the instant both the rabbit and the dog start, the rabbit will be at the point $R=(0,at)$ and the dog at $D=(x,y)$.  We wish to solve for $y$ as a function of $x$. $$\frac{dy}{dx}=\frac{y-at}{x}$$ $$xy'-y=-at$$ $$xy''=-a\frac{dt}{dx}$$ Since the $s$ is a arc length along the path of the dog, it follows that $\frac{ds}{dt}=b$.  Hence, $$\frac{dt}{dx}=\frac{dt}{ds}\frac{ds}{dx}=\frac{-1}{b}\sqrt{1+(y')^2}$$ $$xy''=\frac{a}{b}\sqrt{1+(y')^2}$$ For convenience, we set $k=\frac{a}{b}$,   $y'=p$, and $y''=\frac{dp}{dx}$ $$\frac{dp}{\sqrt{1+p^2}}=k\frac{dx}{x}$$ $$\ln\left({p+\sqrt{1+p^2}}\right)=\ln\left(\frac{x}{c}\right)^k$$ Now, solve for $p$: $$\frac{dy}{dx}=p=\frac{1}{2}\Bigg(\left(\frac{x}{c}\right)^k-\left(\frac{c}{x}\right)^k\Bigg)$$ In order to continue the analysis, we need to know something about the relative sizes of $a$ and $b$.  Suppose, for example, that $a \lt$ $b$ (so $k\lt$ $1$), meaning that the dog will certainly catch the rabbit.  Then we can integrate the last equation to obtain: $$y(x)=\frac{1}{2}\Bigg\{\frac{c}{k+1}\left(\frac{x}{c}\right)^{k+1}-\frac{c}{1-k}\left(\frac{c}{x}\right)^{k-1}\Bigg\}+D$$ Again,  this is all I have to go on.  I need to answer questions (a) and (b) stated at the top.","(a) In Example 1.21, assume that $a$ is less than $b$ (so that $k$ is less than $1$) and find $y$ as a function of $x$.  How far does the rabbit run before the dog catches him? (b)  Assume now that $a=b$, and find $y$ as a function of $x$.  How close does the dog come to the rabbit? Example 1.21 A rabbit begins at the origin and runs up the $y-axis$ with speed $a$ feet per second.  At the same time, a dog runs at speed $b$ from the point $(c,0)$ in pursuit of the rabbit.  What is the path of the dog? Solution:  At time $t$, measured from the instant both the rabbit and the dog start, the rabbit will be at the point $R=(0,at)$ and the dog at $D=(x,y)$.  We wish to solve for $y$ as a function of $x$. $$\frac{dy}{dx}=\frac{y-at}{x}$$ $$xy'-y=-at$$ $$xy''=-a\frac{dt}{dx}$$ Since the $s$ is a arc length along the path of the dog, it follows that $\frac{ds}{dt}=b$.  Hence, $$\frac{dt}{dx}=\frac{dt}{ds}\frac{ds}{dx}=\frac{-1}{b}\sqrt{1+(y')^2}$$ $$xy''=\frac{a}{b}\sqrt{1+(y')^2}$$ For convenience, we set $k=\frac{a}{b}$,   $y'=p$, and $y''=\frac{dp}{dx}$ $$\frac{dp}{\sqrt{1+p^2}}=k\frac{dx}{x}$$ $$\ln\left({p+\sqrt{1+p^2}}\right)=\ln\left(\frac{x}{c}\right)^k$$ Now, solve for $p$: $$\frac{dy}{dx}=p=\frac{1}{2}\Bigg(\left(\frac{x}{c}\right)^k-\left(\frac{c}{x}\right)^k\Bigg)$$ In order to continue the analysis, we need to know something about the relative sizes of $a$ and $b$.  Suppose, for example, that $a \lt$ $b$ (so $k\lt$ $1$), meaning that the dog will certainly catch the rabbit.  Then we can integrate the last equation to obtain: $$y(x)=\frac{1}{2}\Bigg\{\frac{c}{k+1}\left(\frac{x}{c}\right)^{k+1}-\frac{c}{1-k}\left(\frac{c}{x}\right)^{k-1}\Bigg\}+D$$ Again,  this is all I have to go on.  I need to answer questions (a) and (b) stated at the top.",,['calculus']
1,Oscillation frequencies in an ODE,Oscillation frequencies in an ODE,,"Given the following ODE: $$\ddot{x}(t)+\sin(\omega t)x(t)=0$$ its solution can be expressed in terms of the Mathieu functions.  Plotting this solutions and assuming known the initial conditions it can be found a main oscillatory behaviour with another superimposed oscillation close to the maxima and minima of the solution. Now my question is: how can I find the frequencies of these oscillations knowing $\omega$? Or in other words, is there a formula linking the frequencies to $\omega$?","Given the following ODE: $$\ddot{x}(t)+\sin(\omega t)x(t)=0$$ its solution can be expressed in terms of the Mathieu functions.  Plotting this solutions and assuming known the initial conditions it can be found a main oscillatory behaviour with another superimposed oscillation close to the maxima and minima of the solution. Now my question is: how can I find the frequencies of these oscillations knowing $\omega$? Or in other words, is there a formula linking the frequencies to $\omega$?",,"['ordinary-differential-equations', 'special-functions']"
2,Solving differential equation for an expanding bubble,Solving differential equation for an expanding bubble,,I need to solve the equation  \begin{eqnarray} R^3  \frac{d } {dt} \left [        \frac{4}{3} \rho_{\rm ext} \left ( \frac{dR}{dt} \right )^2  \right ]+ 4 p R^2 \frac{d R} {dt}  =\frac{F_E}{4\pi} \end{eqnarray} Could you please help in this regard?,I need to solve the equation  \begin{eqnarray} R^3  \frac{d } {dt} \left [        \frac{4}{3} \rho_{\rm ext} \left ( \frac{dR}{dt} \right )^2  \right ]+ 4 p R^2 \frac{d R} {dt}  =\frac{F_E}{4\pi} \end{eqnarray} Could you please help in this regard?,,['fluid-dynamics']
3,Zeros of a solution between successive zeros of another solution,Zeros of a solution between successive zeros of another solution,,"Let $q$ be a real valued non-trivial solution solution of  $$ y'' +A(x)y = 0 \text{ on } a<x<b, $$  and let $w$ be a real valued non-trivial solution of  $$ y'' + B(x)y = 0 \text{ on } a<x<b. $$  Here $A$ and $B$ are real valued continuous functions satisfying  $$ B(x)>A(x) \text{ for } a<x<b. $$  How to show that if $x_1$ and $x_2$ are successive zeros of $q$ on $(a,b)$, then $w$ must vanish at some point $p \in (x_1, x_2)$? Partial answer: Let $q, w>0$ on $(x_1, x_2)$,then with $(wq'-qw')'= (B-A)qw$, and by integration from $x_1$ to $x_2$ we get $w(x_2)q'(x_2)-w(x_1)q'(x_1)> 0$. Somehow I want to show that that $q'(x_1)< 0$ or $q'(x_2)>0$, which will then contradict $q > 0$ on $(x_1, x_2)$","Let $q$ be a real valued non-trivial solution solution of  $$ y'' +A(x)y = 0 \text{ on } a<x<b, $$  and let $w$ be a real valued non-trivial solution of  $$ y'' + B(x)y = 0 \text{ on } a<x<b. $$  Here $A$ and $B$ are real valued continuous functions satisfying  $$ B(x)>A(x) \text{ for } a<x<b. $$  How to show that if $x_1$ and $x_2$ are successive zeros of $q$ on $(a,b)$, then $w$ must vanish at some point $p \in (x_1, x_2)$? Partial answer: Let $q, w>0$ on $(x_1, x_2)$,then with $(wq'-qw')'= (B-A)qw$, and by integration from $x_1$ to $x_2$ we get $w(x_2)q'(x_2)-w(x_1)q'(x_1)> 0$. Somehow I want to show that that $q'(x_1)< 0$ or $q'(x_2)>0$, which will then contradict $q > 0$ on $(x_1, x_2)$",,['ordinary-differential-equations']
4,inequality in a differential equation,inequality in a differential equation,,"Let $u:\mathbb{R}\to\mathbb{R}^3$ where $u(t)=(u_1(t),u_2(t), u_3(t))$ be a function that satisfies $$\frac{d}{dt}|u(t)|^2+|u|^2\le 1,\tag{1}$$where $|\cdot|$ is the Euclidean norm. According to Temam 's book paragraph 2.2 on page 32 number (2.10), inequality (1) implies $$|u(t)|^2\le|u(0)|^2\exp(-t)+1-\exp(-t),\tag{2}$$but I do not understand why (1) implies (2).","Let $u:\mathbb{R}\to\mathbb{R}^3$ where $u(t)=(u_1(t),u_2(t), u_3(t))$ be a function that satisfies $$\frac{d}{dt}|u(t)|^2+|u|^2\le 1,\tag{1}$$where $|\cdot|$ is the Euclidean norm. According to Temam 's book paragraph 2.2 on page 32 number (2.10), inequality (1) implies $$|u(t)|^2\le|u(0)|^2\exp(-t)+1-\exp(-t),\tag{2}$$but I do not understand why (1) implies (2).",,"['analysis', 'ordinary-differential-equations', 'dynamical-systems']"
5,Solve second order differential equation,Solve second order differential equation,,"Suppose that we have following second order differential equation $$\frac{d^2y}{dx^2}-\frac{dy}{dx} = 2(1-x).$$ When I saw this  equation in the book, it was said that solution is of the form $$y(x)=x(a_1+a_2 x).$$ My assumption about this is that because the right-hand side of the differential equation is the linear function $2(1-x)=2-2x$, it means  that a function $y(x)$ whose second and first derivative is linear must be  quadratic, or in other words $$y(x)=ax^2+bx+c.$$ Is this right? I have posted this question because I wanted to be sure that my assumption is correct. Thanks guys.","Suppose that we have following second order differential equation $$\frac{d^2y}{dx^2}-\frac{dy}{dx} = 2(1-x).$$ When I saw this  equation in the book, it was said that solution is of the form $$y(x)=x(a_1+a_2 x).$$ My assumption about this is that because the right-hand side of the differential equation is the linear function $2(1-x)=2-2x$, it means  that a function $y(x)$ whose second and first derivative is linear must be  quadratic, or in other words $$y(x)=ax^2+bx+c.$$ Is this right? I have posted this question because I wanted to be sure that my assumption is correct. Thanks guys.",,"['calculus', 'ordinary-differential-equations']"
6,A-stability of Heun method for ODEs,A-stability of Heun method for ODEs,,"I'm trying to determine the stability region of the Heun method for ODEs by using the equation $y' = ky$, where $k$ is a complex number, based on the method described here . If the Heun method is: $$y_{n+1} = y_n + 0.5\cdot h\bigl(f(t_n, y_n) + f(t_{n+1},y_n + 0.5\cdot h\cdot f(t_n, y_n)\bigr)$$ then when I insert $y' = zy$ for $f(t,y)$, my result simplifies to $$ y_{n+1} = (0.25\cdot h^2 \cdot z^2 + hz + 1)y_n $$ to judge from the wiki article, the stability region is then the area described by $$\\{z \in \mathbb C \mid 0.25h^2z^2 + hz + 1 < 1\\}$$ Am I doing this right? What would such a region look like? Can someone help me get the intuition for this? And then I guess the method is A-stable if that region includes wherever $\Re < 0$?","I'm trying to determine the stability region of the Heun method for ODEs by using the equation $y' = ky$, where $k$ is a complex number, based on the method described here . If the Heun method is: $$y_{n+1} = y_n + 0.5\cdot h\bigl(f(t_n, y_n) + f(t_{n+1},y_n + 0.5\cdot h\cdot f(t_n, y_n)\bigr)$$ then when I insert $y' = zy$ for $f(t,y)$, my result simplifies to $$ y_{n+1} = (0.25\cdot h^2 \cdot z^2 + hz + 1)y_n $$ to judge from the wiki article, the stability region is then the area described by $$\\{z \in \mathbb C \mid 0.25h^2z^2 + hz + 1 < 1\\}$$ Am I doing this right? What would such a region look like? Can someone help me get the intuition for this? And then I guess the method is A-stable if that region includes wherever $\Re < 0$?",,"['ordinary-differential-equations', 'numerical-methods', 'complex-numbers']"
7,Convergence radius of the solution of an ODE.,Convergence radius of the solution of an ODE.,,Sorry for the vague title... Here's the problem: Consider the ODE $$y'' + \frac{1}{\sqrt{t}}y=0.$$ Given a solution $y$ such that $y(t_0)=0$ for a fixed $t_0>0$ and $y'(t_0)\neq 0.$ What can i say about the convergence radius of the Taylor series of the solution? Thanks!,Sorry for the vague title... Here's the problem: Consider the ODE $$y'' + \frac{1}{\sqrt{t}}y=0.$$ Given a solution $y$ such that $y(t_0)=0$ for a fixed $t_0>0$ and $y'(t_0)\neq 0.$ What can i say about the convergence radius of the Taylor series of the solution? Thanks!,,"['ordinary-differential-equations', 'taylor-expansion']"
8,"MATLAB - ode45 ""ODEFUN"" format","MATLAB - ode45 ""ODEFUN"" format",,"Is someone able to explain to me exactly what the ""odefun"" called by the ""ode45"" ODE solver in MATLAB is supposed to do? My understanding is that you represent an n-order ODE as a system of n first-order ODEs and that, somehow, from this system, you create the ""odefun"" which ""ode45"" uses. My understanding is also that ""odefun"" should output a column array of derivative values at an the input independent variable value and that the function takes in a column array of values (but I'm not sure what these are). How do you actually represent the system of first-order ODEs in ""odefun""? Thanks in advance for your help.","Is someone able to explain to me exactly what the ""odefun"" called by the ""ode45"" ODE solver in MATLAB is supposed to do? My understanding is that you represent an n-order ODE as a system of n first-order ODEs and that, somehow, from this system, you create the ""odefun"" which ""ode45"" uses. My understanding is also that ""odefun"" should output a column array of derivative values at an the input independent variable value and that the function takes in a column array of values (but I'm not sure what these are). How do you actually represent the system of first-order ODEs in ""odefun""? Thanks in advance for your help.",,"['ordinary-differential-equations', 'matlab']"
9,Solving a second-order equation using Laplace Transforms,Solving a second-order equation using Laplace Transforms,,I'm trying to solve this second order differential equation using Laplace Transform. The Laplace transform of the equation is as follows: $$I(s) = \frac{E}{s^2+ \frac{R}{L}s + \frac{1}{LC}}$$ I'm having trouble trying to bring it back to the time domain. Should I be using partial fractions with quadratic factors or is there a easier method to go abut this? Any help would be much appreciated.,I'm trying to solve this second order differential equation using Laplace Transform. The Laplace transform of the equation is as follows: $$I(s) = \frac{E}{s^2+ \frac{R}{L}s + \frac{1}{LC}}$$ I'm having trouble trying to bring it back to the time domain. Should I be using partial fractions with quadratic factors or is there a easier method to go abut this? Any help would be much appreciated.,,"['ordinary-differential-equations', 'laplace-transform']"
10,Conditions for a trapping region.,Conditions for a trapping region.,,"Suppose I have an autonomous system of ordinary differential equations and I want to show that I have a trapping region: a region of phase space which trajectories can enter but can never leave. One way to do this is to show that the vector field points ""inward"" on the boundary of the region, which is how trapping regions are presented in many books. Is it possible to weaken this condition and just require that the vector field not point ""outward""? In other words, if the vector field is inward pointing or tangent to the boundary everywhere on the boundary, is that enough to have a trapping region? I am assuming that the system has unique solutions, otherwise there is the following counter-example: Let $\frac{dy}{dt} = -3 y^{2/3}$. Consider the region to be the positive real line. Then at the boundary of the region the vector field is zero, but there is still the solution $y=-t^3$ that goes from inside the region to outside.","Suppose I have an autonomous system of ordinary differential equations and I want to show that I have a trapping region: a region of phase space which trajectories can enter but can never leave. One way to do this is to show that the vector field points ""inward"" on the boundary of the region, which is how trapping regions are presented in many books. Is it possible to weaken this condition and just require that the vector field not point ""outward""? In other words, if the vector field is inward pointing or tangent to the boundary everywhere on the boundary, is that enough to have a trapping region? I am assuming that the system has unique solutions, otherwise there is the following counter-example: Let $\frac{dy}{dt} = -3 y^{2/3}$. Consider the region to be the positive real line. Then at the boundary of the region the vector field is zero, but there is still the solution $y=-t^3$ that goes from inside the region to outside.",,"['ordinary-differential-equations', 'dynamical-systems']"
11,Does Zorn Lemma imply the existence of a (not unique) maximal prolongation of any solution of an ode?,Does Zorn Lemma imply the existence of a (not unique) maximal prolongation of any solution of an ode?,,"Let be given a map $F:(x,y)\in\mathbb{R}\times\mathbb{R}^n\to F(x,y)\in\mathbb{R}^n$. Let us denote by $\mathcal{P}$ the set whose elements are the solutions of the ode $y'=F(x,y)$, i.e. the differentiable maps $u:J\to\mathbb{R}^n$, where $J\ $ is some open interval in $\mathbb{R}\ $, s.t. $u'(t)=F(t,u(t))$ for all $t\in J$. Let $\mathcal{P}$ be endowed with the ordering by extension. In order to prove that any element of $\mathcal{P}$ is extendable to a (not unique) maximal element, without particular hypothesis on $F$, I was wondering if the Zorn lemma can be used.","Let be given a map $F:(x,y)\in\mathbb{R}\times\mathbb{R}^n\to F(x,y)\in\mathbb{R}^n$. Let us denote by $\mathcal{P}$ the set whose elements are the solutions of the ode $y'=F(x,y)$, i.e. the differentiable maps $u:J\to\mathbb{R}^n$, where $J\ $ is some open interval in $\mathbb{R}\ $, s.t. $u'(t)=F(t,u(t))$ for all $t\in J$. Let $\mathcal{P}$ be endowed with the ordering by extension. In order to prove that any element of $\mathcal{P}$ is extendable to a (not unique) maximal element, without particular hypothesis on $F$, I was wondering if the Zorn lemma can be used.",,"['ordinary-differential-equations', 'axiom-of-choice']"
12,ODE problem shooting,ODE problem shooting,,"Please help me spot my mistake: I have an equation $$(u(x)^{-2} + 4u'(x)^2)^{\frac{1}{2}} - u'(x)\frac{d}{du'}(u(x)^{-2} + 4u'(x)^2)^{\frac{1}{2}} = k$$ where $k$ is  a constant. I am quite sure that if I take $u(x) = \sqrt{y(x)}$ I would have the brachistochrone equation, hence I am expecting a cycloid equation if I let $u(x) = \sqrt{y(x)}$ in the result, but I don't seem to get it :( My workings are as follows: $$u(x)^{-2} + 4u'(x)^2- 4u'(x)^2 = k \times (u(x)^{-2} + 4u'(x)^2)^{\frac{1}{2}}$$ $$\implies u(x)^{-4} = k^2 \times (u(x)^{-2} + 4u'(x)^2)$$ $$\implies u'(x)= \frac{1}{2k}\sqrt{u(x)^{-4} - k^2u(x)^{-2}}$$ $$\implies \int \frac{1}{u \sqrt{u^2 - k^2}} du = \int \frac{1}{2k} dx$$ Change variable: let $v = \frac{u}{k}$ $$\implies \int \frac{1}{v \sqrt{v^2 - 1}} dv = \frac{x+a}{2}$$, where $a$ is a constant  $$\implies \operatorname{arcsec}(v) =  \frac{x+a}{2} $$ $$\implies \operatorname{arcsec}\left(\frac{\sqrt{y}}{k}\right) = \frac{x+a}{2}$$ which does not seem to describe a cycloid... Help would be very much appreciated! Thanks.","Please help me spot my mistake: I have an equation $$(u(x)^{-2} + 4u'(x)^2)^{\frac{1}{2}} - u'(x)\frac{d}{du'}(u(x)^{-2} + 4u'(x)^2)^{\frac{1}{2}} = k$$ where $k$ is  a constant. I am quite sure that if I take $u(x) = \sqrt{y(x)}$ I would have the brachistochrone equation, hence I am expecting a cycloid equation if I let $u(x) = \sqrt{y(x)}$ in the result, but I don't seem to get it :( My workings are as follows: $$u(x)^{-2} + 4u'(x)^2- 4u'(x)^2 = k \times (u(x)^{-2} + 4u'(x)^2)^{\frac{1}{2}}$$ $$\implies u(x)^{-4} = k^2 \times (u(x)^{-2} + 4u'(x)^2)$$ $$\implies u'(x)= \frac{1}{2k}\sqrt{u(x)^{-4} - k^2u(x)^{-2}}$$ $$\implies \int \frac{1}{u \sqrt{u^2 - k^2}} du = \int \frac{1}{2k} dx$$ Change variable: let $v = \frac{u}{k}$ $$\implies \int \frac{1}{v \sqrt{v^2 - 1}} dv = \frac{x+a}{2}$$, where $a$ is a constant  $$\implies \operatorname{arcsec}(v) =  \frac{x+a}{2} $$ $$\implies \operatorname{arcsec}\left(\frac{\sqrt{y}}{k}\right) = \frac{x+a}{2}$$ which does not seem to describe a cycloid... Help would be very much appreciated! Thanks.",,['ordinary-differential-equations']
13,Vector Field/Differential Equation Correspondence,Vector Field/Differential Equation Correspondence,,"I have seen some examples (though I am currently looking for a good rigorous explanation and a source would be much appreciated) of taking a second order linear ODE and turning it into a linear system of 2 equations.  My question is, can you go the other way?  That is, given some continuous vector field, can we find a differential equation corresponding to it (of any order)? Thanks!","I have seen some examples (though I am currently looking for a good rigorous explanation and a source would be much appreciated) of taking a second order linear ODE and turning it into a linear system of 2 equations.  My question is, can you go the other way?  That is, given some continuous vector field, can we find a differential equation corresponding to it (of any order)? Thanks!",,['ordinary-differential-equations']
14,Solution of a Hamilton-Jacobi-Bellman (HJB) equation,Solution of a Hamilton-Jacobi-Bellman (HJB) equation,,"I am  trying to solve a ODE that arises from a Hamilton-Jacobi-Bellman (HJB) equation. The equation is $$\frac{1}{2}b^2(1-\rho_s^2)\psi''-\frac{1}{2}\left(\frac{\mu-r}{\sigma}\right)^2\frac{(\psi')^2}{\psi''}+[ru+\theta a+b\rho_s(\mu-r)(1-\frac{2}{\sigma})]\psi'=0,$$ where $\mu, r, \sigma, \theta, a, \rho_s, b$ are constant. I want to determine $\psi'(u)$ (so that I get an integral form for $\psi(u)$). I have tried guessing (trial and error method) forms of the solution but didn't get far. I also tried the Legendre transform, but could not get the linear form. These are the method that I have seen being used in with these problems.","I am  trying to solve a ODE that arises from a Hamilton-Jacobi-Bellman (HJB) equation. The equation is $$\frac{1}{2}b^2(1-\rho_s^2)\psi''-\frac{1}{2}\left(\frac{\mu-r}{\sigma}\right)^2\frac{(\psi')^2}{\psi''}+[ru+\theta a+b\rho_s(\mu-r)(1-\frac{2}{\sigma})]\psi'=0,$$ where $\mu, r, \sigma, \theta, a, \rho_s, b$ are constant. I want to determine $\psi'(u)$ (so that I get an integral form for $\psi(u)$). I have tried guessing (trial and error method) forms of the solution but didn't get far. I also tried the Legendre transform, but could not get the linear form. These are the method that I have seen being used in with these problems.",,"['ordinary-differential-equations', 'stochastic-processes']"
15,Transition probability density function for a non-trivial diffusion process.,Transition probability density function for a non-trivial diffusion process.,,"Let $\mu$ and $\sigma > 0$ and $\beta_1 \ge 0 $ and $\beta_2 \ge 0$ be real numbers. Consider a stochastic process $X_t$ that satisfies the following stochastic differential equation: \begin{equation} d X_t = \mu X_t^{\beta_1} dt + \sigma X_t^{\beta_2} dB_t \tag{1} \end{equation} where $B_t$ is the Brownian motion. My objective is to derive the transition probability density function $P_x\left( X_t \in dz\right) := P\left( X_t \in (z,z+dz) | X_0 = x \right)$ for the process in question. Here we will be using a result known from literature (see section 4.11 pages 149-161 in K Ito& H P McKean Jr, ""Diffusion Processes and their Sample Paths"") as below. Theorem: The Laplace transform (with respect to time) of the transition probability density in question  reads: \begin{equation} \int\limits_0^\infty e^{-\lambda t} P_x\left( X_t \in dz\right) dt = G_\lambda(z,x) \cdot m(dz) \tag{2} \end{equation} Here $m(z):= \left( 1/2 \sigma^2(z) S(z) \right)^{-1}$ is the speed measure  with $S(y) := \exp\left(- \int\limits_1^y 2 \mu(\xi)/\sigma^2(\xi) d\xi \right)$ being the scale measure. Then $G_\lambda​(z,x)$ is the Green's function of the operator $\lambda - {\mathfrak G}^{*}$ with ${\mathfrak G}^{*}$ being the infinitesimal generator, i.e. such an operator that its action on a trial function $f$ reads $\left({\mathfrak G}^{*} f\right)(x) = \lim\limits_{\epsilon \rightarrow 0} E \left[ f(X_\epsilon) - f(x) \right]/\epsilon$ subject to $X_0=x$ . Note that the Green's function is easily determined by the eigen-functions and it reads $G_\lambda(z,x) = S(x)/{\mathfrak W}_\lambda(x) \left( G_\lambda(x) F_\lambda(z) 1_{z < x} + F_\lambda(x) G_\lambda(z) 1_{z > x} \right)$ where $F_\lambda(z)$ and $G_\lambda(z)$ is a strictly increasing and a strictly decreasing eigen-function of the infinitesimal generator to an eigenvalue $\lambda$ and ${\mathfrak W}_\lambda(x) := F_\lambda^{'}(x) G_\lambda(x) - G_\lambda^{'}(x) F_\lambda(x)$ is the Wronskian of the generator in question. What have I managed to achieve?: I have found the result for $(\beta_1,\beta_2) = (1, \beta)$ with $\beta >1$ and I present the derivation below. Here the infinitesimal generator reads ${\mathfrak G}_z := \mu z \cdot d/dz + 1/2 \sigma^2 z^{2 \beta} d^2/ d z^2$ and its eigenfunctions  (compare my previous question on a similar topic ) read: \begin{eqnarray} F(z) &=& U\left(\frac{\lambda}{2(-1+\beta) \mu}, 1+ \frac{1}{2(-1+\beta)}, \frac{\mu z^{2-2\beta} }{(-1+\beta) \sigma^2} \right) \tag{3a} \\ G(z) &=& F_{1,1}\left(\frac{\lambda}{2(-1+\beta) \mu}, 1+ \frac{1}{2(-1+\beta)}, \frac{\mu z^{2-2\beta} }{(-1+\beta) \sigma^2} \right) \tag{3b}  \end{eqnarray} where $U$ is the confluent hypergeometric function. Note that $F^{'}(z) = \frac{\lambda  z^{1-2 \beta }    U\left(\frac{\lambda }{2 (\beta -1) \mu    }+1,2+\frac{1}{2 (\beta    -1)},\frac{z^{2-2 \beta } \mu }{s^2    (\beta -1)}\right)}{(\beta -1) s^2} >0$ and $G^{'}(z) = -\frac{2 \lambda  z^{1-2 \beta } \,    _1F_1\left(\frac{\lambda }{2 (\beta -1)    \mu }+1;2+\frac{1}{2 (\beta    -1)};\frac{z^{2-2 \beta } \mu }{s^2    (\beta -1)}\right)}{(2 \beta -1) s^2} < 0$ (note that from the integral representations we see that both $U$ and $F_{1,1}$ are positive --well up to a multiplicative constant in the second case) and as such $F(z)$ and $G(z)$ are strictly increasing and strictly decreasing as it should be. Now the Wronskian  is computed as follows: \begin{eqnarray} {\mathfrak W}_\lambda(x) &=& {\mathfrak W}_\lambda(1) \cdot \exp\left( -\int\limits_1^x  \frac{2\mu(\xi)}{\sigma^2(\xi)} d\xi \right) \tag{4a} \\ &=& {\mathfrak W}_\lambda(1) \cdot \exp\left( - \frac{2 \mu}{\sigma^2} \frac{x^{2-2\beta} -1}{2-2\beta}\right) \tag{4b} \\ &=& -\frac{2 (\beta -1) \Gamma    \left(1+\frac{1}{2 (\beta -1)}\right)    e^{\frac{\mu }{(\beta -1) \sigma ^2}}    \left(\frac{\mu }{(\beta -1) \sigma    ^2}\right)^{\frac{1}{2-2 \beta    }}}{\Gamma \left(\frac{\lambda }{(2    \beta -2) \mu }\right)} \cdot \exp\left( - \frac{2 \mu}{\sigma^2} \frac{x^{2-2\beta} -1}{2-2\beta}\right) \tag{4c} \end{eqnarray} The first line above simply follows form the first order differential equation being satisfied by the Wronskian. The second line follows form the fact that $\mu(\xi), \sigma(\xi) = \mu \cdot \xi, \sigma \cdot \xi^\beta$ . The step to the final line is the most laborious, but not really that mind boggling. Here one employs the integral representations in order to express the Wronskian at unity as a double integral in which one changes variables to obtain the neat final result. Now we compute the inverse Laplace transform as the Bromwich integral and we use Cauchy theorem (being applied to the usual contour in the negative complex half-plane-- the second & the third quadrants) to evaluate that integral. As seen from $(4c)$ the Laplace transform has infinitely many simple poles at $\lambda_n = (2\beta-2) \mu (-n)$ for $n=0,1,2,\cdots$ . Then the final result reads: \begin{eqnarray} &&\left. P_x \left( X_t \in dz \right)/dz = \frac{2 \mu  \left(\frac{\mu }{(\beta -1)    \sigma ^2}\right)^{\frac{1}{2 (\beta    -1)}}}{\sigma ^2 \Gamma    \left(1+\frac{1}{2 (\beta -1)}\right)} \cdot z^{-2 \beta } e^{-\frac{\mu  z^{2-2 \beta    }}{(\beta -1) \sigma ^2}} \cdot \right.\\ && \left. \sum\limits_{n=0}^\infty \frac{(-1)^n}{n!} % \, _1F_1\left(-n;1+\frac{1}{2 (\beta    -1)};\frac{\mu  (x \vee z)^{2-2 \beta    }}{(\beta -1) \sigma ^2}\right)    U\left(-n,1+\frac{1}{2 (\beta    -1)},\frac{\mu  (x \wedge z)^{2-2 \beta    }}{(\beta -1) \sigma ^2}\right) % \cdot e^{-2(-1+\beta) \cdot \mu \cdot n \cdot t} \right. \tag{5} \end{eqnarray} Now we have checked numerically that the result above matches with the solution known from literature (see equations (4) and (5) in V Linetsky, R Mendoza, The Constant Elasticity of Variance Model). Here we go: In[1991]:= (*Verify whether our solution matches that known from \ literature?*) Clear[mu, s, b, z, lmb, phi, v, W, W1, W0, x]; {mu, s, b} = {1/2, 1/3 + 3/10, 5/2}; {x, z} = RandomReal[{1, 3}, 2, WorkingPrecision -> 50]; t = 3/2; (*Our solution:*) res1 = (2  mu ((mu/((-1 + b) s^2))^((1/(2 (-1 + b))))) )/(    s^2 Gamma[1 + 1/(2 (-1 + b))])     E^(-((mu z^(2 - 2 b))/((-1 + b) s^2))) z^(-2 b)     Table[(-1)^n/      n! HypergeometricU[-n, 1 + 1/(2 (-1 + b)),        mu/((-1 + b) s^2) Min[x, z]^(2 - 2 b)] Hypergeometric1F1[-n,        1 + 1/(2 (-1 + b)),        mu/((-1 + b) s^2) Max[x, z]^(2 - 2 b)] E^(-         2 (-1 + b) mu n t), {n, 0, M}]; Total[res1] // N  (*Solution known from literature: Equations (4) & (5) page 2 in V \ Linetsky & R Mendoza The Constant Elasticity of Variance Model.Here \ the solution was obtained,using different techniques,meaning by \ re-scaling the current price and by changing time in the solution of \ a simpler problem,i.e.a problem with the drift parameter being set to \ zero.*)  Clear[tau]; tau[t_] := (Exp[2 mu (b - 1) t] - 1)/(2 mu (b - 1)); res2 = Exp[-mu t] ((Exp[-mu t] z)^(-2 b + 1/2) x^(1/2))/(    s^2 Abs[b - 1] tau[t])     BesselI[1/(2 Abs[ b - 1]), (x^(-b + 1) (Exp[-mu t] z)^(-b + 1))/(     s^2 (b - 1)^2 tau[       t])] Exp[-((x^(-2 b + 2) + (Exp[-mu t] z)^(-2 b + 2))/(      2 s^2 (b - 1)^2 tau[t]))]; res2 // N  Out[1996]= 0.0481819  Out[1999]= 0.0481819 Having said all this my question is how do we find the solution in the generic case, i.e. when $\beta_1 \neq 1$ ? Another obvious question would be to provide a literature reference, if it exists, on that problem solution.","Let and and and be real numbers. Consider a stochastic process that satisfies the following stochastic differential equation: where is the Brownian motion. My objective is to derive the transition probability density function for the process in question. Here we will be using a result known from literature (see section 4.11 pages 149-161 in K Ito& H P McKean Jr, ""Diffusion Processes and their Sample Paths"") as below. Theorem: The Laplace transform (with respect to time) of the transition probability density in question  reads: Here is the speed measure  with being the scale measure. Then is the Green's function of the operator with being the infinitesimal generator, i.e. such an operator that its action on a trial function reads subject to . Note that the Green's function is easily determined by the eigen-functions and it reads where and is a strictly increasing and a strictly decreasing eigen-function of the infinitesimal generator to an eigenvalue and is the Wronskian of the generator in question. What have I managed to achieve?: I have found the result for with and I present the derivation below. Here the infinitesimal generator reads and its eigenfunctions  (compare my previous question on a similar topic ) read: where is the confluent hypergeometric function. Note that and (note that from the integral representations we see that both and are positive --well up to a multiplicative constant in the second case) and as such and are strictly increasing and strictly decreasing as it should be. Now the Wronskian  is computed as follows: The first line above simply follows form the first order differential equation being satisfied by the Wronskian. The second line follows form the fact that . The step to the final line is the most laborious, but not really that mind boggling. Here one employs the integral representations in order to express the Wronskian at unity as a double integral in which one changes variables to obtain the neat final result. Now we compute the inverse Laplace transform as the Bromwich integral and we use Cauchy theorem (being applied to the usual contour in the negative complex half-plane-- the second & the third quadrants) to evaluate that integral. As seen from the Laplace transform has infinitely many simple poles at for . Then the final result reads: Now we have checked numerically that the result above matches with the solution known from literature (see equations (4) and (5) in V Linetsky, R Mendoza, The Constant Elasticity of Variance Model). Here we go: In[1991]:= (*Verify whether our solution matches that known from \ literature?*) Clear[mu, s, b, z, lmb, phi, v, W, W1, W0, x]; {mu, s, b} = {1/2, 1/3 + 3/10, 5/2}; {x, z} = RandomReal[{1, 3}, 2, WorkingPrecision -> 50]; t = 3/2; (*Our solution:*) res1 = (2  mu ((mu/((-1 + b) s^2))^((1/(2 (-1 + b))))) )/(    s^2 Gamma[1 + 1/(2 (-1 + b))])     E^(-((mu z^(2 - 2 b))/((-1 + b) s^2))) z^(-2 b)     Table[(-1)^n/      n! HypergeometricU[-n, 1 + 1/(2 (-1 + b)),        mu/((-1 + b) s^2) Min[x, z]^(2 - 2 b)] Hypergeometric1F1[-n,        1 + 1/(2 (-1 + b)),        mu/((-1 + b) s^2) Max[x, z]^(2 - 2 b)] E^(-         2 (-1 + b) mu n t), {n, 0, M}]; Total[res1] // N  (*Solution known from literature: Equations (4) & (5) page 2 in V \ Linetsky & R Mendoza The Constant Elasticity of Variance Model.Here \ the solution was obtained,using different techniques,meaning by \ re-scaling the current price and by changing time in the solution of \ a simpler problem,i.e.a problem with the drift parameter being set to \ zero.*)  Clear[tau]; tau[t_] := (Exp[2 mu (b - 1) t] - 1)/(2 mu (b - 1)); res2 = Exp[-mu t] ((Exp[-mu t] z)^(-2 b + 1/2) x^(1/2))/(    s^2 Abs[b - 1] tau[t])     BesselI[1/(2 Abs[ b - 1]), (x^(-b + 1) (Exp[-mu t] z)^(-b + 1))/(     s^2 (b - 1)^2 tau[       t])] Exp[-((x^(-2 b + 2) + (Exp[-mu t] z)^(-2 b + 2))/(      2 s^2 (b - 1)^2 tau[t]))]; res2 // N  Out[1996]= 0.0481819  Out[1999]= 0.0481819 Having said all this my question is how do we find the solution in the generic case, i.e. when ? Another obvious question would be to provide a literature reference, if it exists, on that problem solution.","\mu \sigma > 0 \beta_1 \ge 0  \beta_2 \ge 0 X_t \begin{equation}
d X_t = \mu X_t^{\beta_1} dt + \sigma X_t^{\beta_2} dB_t \tag{1}
\end{equation} B_t P_x\left( X_t \in dz\right) := P\left( X_t \in (z,z+dz) | X_0 = x \right) \begin{equation}
\int\limits_0^\infty e^{-\lambda t} P_x\left( X_t \in dz\right) dt = G_\lambda(z,x) \cdot m(dz) \tag{2}
\end{equation} m(z):= \left( 1/2 \sigma^2(z) S(z) \right)^{-1} S(y) := \exp\left(- \int\limits_1^y 2 \mu(\xi)/\sigma^2(\xi) d\xi \right) G_\lambda​(z,x) \lambda - {\mathfrak G}^{*} {\mathfrak G}^{*} f \left({\mathfrak G}^{*} f\right)(x) = \lim\limits_{\epsilon \rightarrow 0} E \left[ f(X_\epsilon) - f(x) \right]/\epsilon X_0=x G_\lambda(z,x) = S(x)/{\mathfrak W}_\lambda(x) \left( G_\lambda(x) F_\lambda(z) 1_{z < x} + F_\lambda(x) G_\lambda(z) 1_{z > x} \right) F_\lambda(z) G_\lambda(z) \lambda {\mathfrak W}_\lambda(x) := F_\lambda^{'}(x) G_\lambda(x) - G_\lambda^{'}(x) F_\lambda(x) (\beta_1,\beta_2) = (1, \beta) \beta >1 {\mathfrak G}_z := \mu z \cdot d/dz + 1/2 \sigma^2 z^{2 \beta} d^2/ d z^2 \begin{eqnarray}
F(z) &=& U\left(\frac{\lambda}{2(-1+\beta) \mu}, 1+ \frac{1}{2(-1+\beta)}, \frac{\mu z^{2-2\beta} }{(-1+\beta) \sigma^2} \right) \tag{3a} \\
G(z) &=& F_{1,1}\left(\frac{\lambda}{2(-1+\beta) \mu}, 1+ \frac{1}{2(-1+\beta)}, \frac{\mu z^{2-2\beta} }{(-1+\beta) \sigma^2} \right) \tag{3b} 
\end{eqnarray} U F^{'}(z) = \frac{\lambda  z^{1-2 \beta }
   U\left(\frac{\lambda }{2 (\beta -1) \mu
   }+1,2+\frac{1}{2 (\beta
   -1)},\frac{z^{2-2 \beta } \mu }{s^2
   (\beta -1)}\right)}{(\beta -1) s^2} >0 G^{'}(z) = -\frac{2 \lambda  z^{1-2 \beta } \,
   _1F_1\left(\frac{\lambda }{2 (\beta -1)
   \mu }+1;2+\frac{1}{2 (\beta
   -1)};\frac{z^{2-2 \beta } \mu }{s^2
   (\beta -1)}\right)}{(2 \beta -1) s^2} < 0 U F_{1,1} F(z) G(z) \begin{eqnarray}
{\mathfrak W}_\lambda(x) &=& {\mathfrak W}_\lambda(1) \cdot \exp\left( -\int\limits_1^x  \frac{2\mu(\xi)}{\sigma^2(\xi)} d\xi \right) \tag{4a} \\
&=& {\mathfrak W}_\lambda(1) \cdot \exp\left( - \frac{2 \mu}{\sigma^2} \frac{x^{2-2\beta} -1}{2-2\beta}\right) \tag{4b} \\
&=&
-\frac{2 (\beta -1) \Gamma
   \left(1+\frac{1}{2 (\beta -1)}\right)
   e^{\frac{\mu }{(\beta -1) \sigma ^2}}
   \left(\frac{\mu }{(\beta -1) \sigma
   ^2}\right)^{\frac{1}{2-2 \beta
   }}}{\Gamma \left(\frac{\lambda }{(2
   \beta -2) \mu }\right)}
\cdot
\exp\left( - \frac{2 \mu}{\sigma^2} \frac{x^{2-2\beta} -1}{2-2\beta}\right) \tag{4c}
\end{eqnarray} \mu(\xi), \sigma(\xi) = \mu \cdot \xi, \sigma \cdot \xi^\beta (4c) \lambda_n = (2\beta-2) \mu (-n) n=0,1,2,\cdots \begin{eqnarray}
&&\left. P_x \left( X_t \in dz \right)/dz =
\frac{2 \mu  \left(\frac{\mu }{(\beta -1)
   \sigma ^2}\right)^{\frac{1}{2 (\beta
   -1)}}}{\sigma ^2 \Gamma
   \left(1+\frac{1}{2 (\beta -1)}\right)}
\cdot
z^{-2 \beta } e^{-\frac{\mu  z^{2-2 \beta
   }}{(\beta -1) \sigma ^2}}
\cdot \right.\\
&& \left.
\sum\limits_{n=0}^\infty
\frac{(-1)^n}{n!}
%
\, _1F_1\left(-n;1+\frac{1}{2 (\beta
   -1)};\frac{\mu  (x \vee z)^{2-2 \beta
   }}{(\beta -1) \sigma ^2}\right)
   U\left(-n,1+\frac{1}{2 (\beta
   -1)},\frac{\mu  (x \wedge z)^{2-2 \beta
   }}{(\beta -1) \sigma ^2}\right)
%
\cdot
e^{-2(-1+\beta) \cdot \mu \cdot n \cdot t} \right. \tag{5}
\end{eqnarray} \beta_1 \neq 1","['ordinary-differential-equations', 'stochastic-processes', 'hypergeometric-function', 'greens-function', 'inverse-laplace']"
16,Expected Value Via ODE,Expected Value Via ODE,,"I recently learned about a technique to use ODEs to find the mathematical expectation. I decided to apply it to the following problem: Let $X_1$ , $X_2$ , $\ldots\simeq$ Uniform $(0,1)$ iid. Let $N$ be the first index $n$ where $X_n\neq \max\{X_1,X_2,\ldots,X_n\}$ . Find $E[N]$ . To solve, we generalize the problem by defining a function $m(z)$ . The function $m(z)$ is the expected number of uniform variables needed so that the monotonic condition no longer holds when initially starting at $z$ . Specifically, it is the average of the first index $n$ such that $X_n<X_{n-1}\geq\ldots X_1\geq z$ . Then the total law of expectation - where we condition on $X_1$ - yields $$m(x) = \int^1_x(1+m(z)) \mathrm{d}z$$ For now, let us assume that $m(x)$ is differentiable. Then differentiating both sides yields $$m'(x)=-(1+m(x))$$ or $$m'(x)+m(x)=-1$$ This is a linear ODE with general solution $m(x)=ce^{-x}-1$ . Plugging this equation back into the integral shows that $$\int^1_z(1+m(z))\;\mathrm{d}z=-ce^{-1}+ce^{-z}=ce^{-x}+1$$ or $c=e$ . All the remains to do is to evaluate at $x=0$ : $$m(0)=e^{1-0}-1=e-1$$ However, solving it through alternative means, I obtain $e$ . What went wrong?","I recently learned about a technique to use ODEs to find the mathematical expectation. I decided to apply it to the following problem: Let , , Uniform iid. Let be the first index where . Find . To solve, we generalize the problem by defining a function . The function is the expected number of uniform variables needed so that the monotonic condition no longer holds when initially starting at . Specifically, it is the average of the first index such that . Then the total law of expectation - where we condition on - yields For now, let us assume that is differentiable. Then differentiating both sides yields or This is a linear ODE with general solution . Plugging this equation back into the integral shows that or . All the remains to do is to evaluate at : However, solving it through alternative means, I obtain . What went wrong?","X_1 X_2 \ldots\simeq (0,1) N n X_n\neq \max\{X_1,X_2,\ldots,X_n\} E[N] m(z) m(z) z n X_n<X_{n-1}\geq\ldots X_1\geq z X_1 m(x) = \int^1_x(1+m(z)) \mathrm{d}z m(x) m'(x)=-(1+m(x)) m'(x)+m(x)=-1 m(x)=ce^{-x}-1 \int^1_z(1+m(z))\;\mathrm{d}z=-ce^{-1}+ce^{-z}=ce^{-x}+1 c=e x=0 m(0)=e^{1-0}-1=e-1 e","['ordinary-differential-equations', 'expected-value', 'conditional-expectation']"
17,Existence and Uniqueness of ODEs under weaker Lipschitz continuity condition.,Existence and Uniqueness of ODEs under weaker Lipschitz continuity condition.,,"It is well known that if $f:\mathbb{R}^n\to\mathbb{R}^n$ is locally Lipschitz at $x_0$ , i.e., there exists a neighborhood $U$ of $x_0$ and $L>0$ such that \begin{align*} \|f(x)-f(y)\|\leq L\|x-y\| \end{align*} for all $x,y\in U$ then the ODE $\dot{x}=f(x)$ with initial condition $x(0)=x_0$ has a unique solution. However, assume that we only know that $f$ satisfies the following weaker condition: there exists a neighborhood $U_2$ of $x_0$ and a constant $L_2>0$ such that \begin{align*} \|f(x)-f(x_0)\|\leq L_2\|x-x_0\| \end{align*} for all $x\in U_2$ . Does existence and uniqueness also hold here? If not, what is a counterexample?","It is well known that if is locally Lipschitz at , i.e., there exists a neighborhood of and such that for all then the ODE with initial condition has a unique solution. However, assume that we only know that satisfies the following weaker condition: there exists a neighborhood of and a constant such that for all . Does existence and uniqueness also hold here? If not, what is a counterexample?","f:\mathbb{R}^n\to\mathbb{R}^n x_0 U x_0 L>0 \begin{align*}
\|f(x)-f(y)\|\leq L\|x-y\|
\end{align*} x,y\in U \dot{x}=f(x) x(0)=x_0 f U_2 x_0 L_2>0 \begin{align*}
\|f(x)-f(x_0)\|\leq L_2\|x-x_0\|
\end{align*} x\in U_2",['ordinary-differential-equations']
18,Finding singular solution to a Lagrange equation,Finding singular solution to a Lagrange equation,,"I want to find a singular solution (not general) to the following differential equation: $$y=3xy'-7y'^3$$ One method for finding a singular solution is called p-discriminant and it consists in solving this system: $$\cases{F(x,y,y')=0 \\ \frac{\partial F}{\partial y'}=0}$$ Where F is the the differential equation. So the system looks like: $$\cases{y=3xy'-7y'^3 \\y'=\pm \sqrt{\frac{x}{7}}}$$ So if we solve the system we get $$y=\pm \frac{2x^{\frac{3}{2}}}{\sqrt{7}}$$ On first sight there is nothing wrong but if we differentiate we get that $$y'=\pm3\sqrt{\frac{x}{7}}=3y'$$ Which doesn't make any sense. Moreover the text book says that the only singular solution is $y=0$ which I have no idea how to conclude using the only the upper method(without guesswork). Where is my mistake?",I want to find a singular solution (not general) to the following differential equation: One method for finding a singular solution is called p-discriminant and it consists in solving this system: Where F is the the differential equation. So the system looks like: So if we solve the system we get On first sight there is nothing wrong but if we differentiate we get that Which doesn't make any sense. Moreover the text book says that the only singular solution is which I have no idea how to conclude using the only the upper method(without guesswork). Where is my mistake?,"y=3xy'-7y'^3 \cases{F(x,y,y')=0 \\ \frac{\partial F}{\partial y'}=0} \cases{y=3xy'-7y'^3 \\y'=\pm \sqrt{\frac{x}{7}}} y=\pm \frac{2x^{\frac{3}{2}}}{\sqrt{7}} y'=\pm3\sqrt{\frac{x}{7}}=3y' y=0",['ordinary-differential-equations']
19,"A question about solutions of O.D.E $(2x^2+2y^2+x)dx+(x^2+y^2+y)dy=0,\ x\geq 0$",A question about solutions of O.D.E,"(2x^2+2y^2+x)dx+(x^2+y^2+y)dy=0,\ x\geq 0","I solved firstly the O.D.E $$(*)\ \ \ \ (2x^2+2y^2+x)dx+(x^2+y^2+y)dy=0, x\geq 0, $$ by using the Integrating Factor $\rho (x,y)=\frac{1}{x^2+y^2}$ . Consequently, I proved that every solution $y$ of this equation is given by form: $$2x+\frac{1}{2}\ln(x^2+y^2)+y=c,\ x\geq 0 $$ with $c$ be a random parameter. $\bullet\ $ Question 1: Can I tell with certainty, that there exist a solution of $(*)$ such that $y'(x)\leq 0, \ \forall\ x\geq 0$ ? $\bullet\ $ Question 2: Can I tell with certainty, that for every solution $y$ of $(*)$ holds $\displaystyle\lim_{x\to+\infty}y(x)=-\infty$ ? About the 1st question, we can see that for every solution $y$ of $(*)$ we can rewrite $(*)$ as $y'=-\dfrac{2x^2+2y^2+x}{x^2+y^2+y},\ x\geq 0$ but i think it's not clearly that $y'(x)\leq 0, \ \forall\ x\geq 0$ About the 2nd question, I tried by contradiction by taking the limit as $x\to+\infty$ to the last equation. However, that didn't end up as I expected. Any help or suggestion it would be helpful, please. Thanks a lot.","I solved firstly the O.D.E by using the Integrating Factor . Consequently, I proved that every solution of this equation is given by form: with be a random parameter. Question 1: Can I tell with certainty, that there exist a solution of such that ? Question 2: Can I tell with certainty, that for every solution of holds ? About the 1st question, we can see that for every solution of we can rewrite as but i think it's not clearly that About the 2nd question, I tried by contradiction by taking the limit as to the last equation. However, that didn't end up as I expected. Any help or suggestion it would be helpful, please. Thanks a lot.","(*)\ \ \ \ (2x^2+2y^2+x)dx+(x^2+y^2+y)dy=0, x\geq 0,  \rho (x,y)=\frac{1}{x^2+y^2} y 2x+\frac{1}{2}\ln(x^2+y^2)+y=c,\ x\geq 0  c \bullet\  (*) y'(x)\leq 0, \ \forall\ x\geq 0 \bullet\  y (*) \displaystyle\lim_{x\to+\infty}y(x)=-\infty y (*) (*) y'=-\dfrac{2x^2+2y^2+x}{x^2+y^2+y},\ x\geq 0 y'(x)\leq 0, \ \forall\ x\geq 0 x\to+\infty","['calculus', 'ordinary-differential-equations', 'functions']"
20,"What are ""forward/backward"" solutions to first order difference/differential equations?","What are ""forward/backward"" solutions to first order difference/differential equations?",,"Recently, I've met for the first time with the so called ""forward/backward"" solutions to first order difference/differential equations. It seems that the subject goes back to Blanchard's ""Backward and Forward Solutions for Economies with Rational Expectations"" https://www.jstor.org/stable/1801627 . Is there some reference with simple examples for this type of calculations? Some textbook or lecture notes? For example : If I have the differential equation $x' = ax + b$ with $a, b$ real constants I get the following ""forward solution"" : $$ x(0) = \left(\frac{b}{a} + x(t)\right)e^{-at} - \frac{b}{a} $$ and the following ""backward solution"" : $$ x(0) = \left(\frac{b}{a} + x(-t)\right)e^{at} - \frac{b}{a} $$ Is that correct? I would appreciate any comments or references on the subject","Recently, I've met for the first time with the so called ""forward/backward"" solutions to first order difference/differential equations. It seems that the subject goes back to Blanchard's ""Backward and Forward Solutions for Economies with Rational Expectations"" https://www.jstor.org/stable/1801627 . Is there some reference with simple examples for this type of calculations? Some textbook or lecture notes? For example : If I have the differential equation with real constants I get the following ""forward solution"" : and the following ""backward solution"" : Is that correct? I would appreciate any comments or references on the subject","x' = ax + b a, b 
x(0) = \left(\frac{b}{a} + x(t)\right)e^{-at} - \frac{b}{a}
 
x(0) = \left(\frac{b}{a} + x(-t)\right)e^{at} - \frac{b}{a}
","['ordinary-differential-equations', 'analysis', 'reference-request', 'terminology', 'economics']"
21,"Solve a system of differential equations $\begin{cases} x'=-7x-18y-67e^{-t},x(0)=-1\\ y'=2x+5y+22e^{-t}, \; y(0)=-3 \end{cases}$",Solve a system of differential equations,"\begin{cases} x'=-7x-18y-67e^{-t},x(0)=-1\\ y'=2x+5y+22e^{-t}, \; y(0)=-3 \end{cases}","Apply the operational method to solve the Cauchy problem $$\begin{cases} x'=-7x-18y-67e^{-t},x(0)=-1\\ y'=2x+5y+22e^{-t}, \; y(0)=-3 \end{cases}$$ My attempt: $$x(t) = \frac{a \sin(4t)+ b\cos(4t)}{e^{4t}}\overset{x\in [0,1)}{\Rightarrow }x(t) = \frac{a \sin(4t)+ b\cos(4t)}{e^{4t}} + 12$$ It is usually assumed that $x$ is twice differentiable everywhere, so on the boundaries of each of the smooth definitions we need to match the limits $x(0), x(1), x'(0), x'(1)$ $$x(t) = \left\{\begin{matrix} \frac{A\sin(4t)+B\cos(4t)}{e^{4t}},&t<0\\ \frac{C\sin(4t)+D\cos(4t)}{e^{4t}} + 12,&0 < t\le 1\\ \frac{E\sin(4t)+F\cos(4t)}{e^{4t}},&t>1 \end{matrix}\right\} $$ Then you need to make sure that the initial conditions are met and all boundary conditions are met. $A=2, B=-2, C=-10, D=-14$ . My solution: $$\begin{cases} x'=-7x-18y-67e^{-t},x(0)=-1\\ y'=2x+5y+22e^{-t}, \; y(0)=-3 \end{cases} \Leftrightarrow \dot{\vec{r}}=\left[\begin{array}{cc} -7 & -18 \\ 2 & 5 \end{array}\right] \vec{r}+\left[\begin{array}{c} -67 \\ 22 \end{array}\right] e^{-t} ; \vec{r}(0)=\left[\begin{array}{l} -1 \\ -3 \end{array}\right]$$ \begin{multline*} \mathcal{L}[f(t)]=\int\limits\limits_0^{\infty} f(t) e^{-p t} d t=F(p) \Rightarrow \mathcal{L}\left[\frac{d f}{d t}\right]=\int\limits\limits_0^{\infty} \frac{d f}{d t} e^{-p t} d t=\\=\lim _{t \rightarrow \infty} f(t) e^{-p t}-f(0)+p \int\limits\limits_0^{\infty} f(t) e^{-p t} d t=p F(p)-f(0)  \end{multline*} \begin{multline*} \mathcal{L}[\vec{r}(t)]=\vec{R}(p) \Rightarrow \forall p>-1\left(p \hat{1}-\left[\begin{array}{cc} -7 & -18 \\ 2 & 5 \end{array}\right]\right) \vec{R}(p)=\left[\begin{array}{cc} p+7 & 18 \\ -2 & p-5 \end{array}\right] \vec{R}(p)=\\=\left[\begin{array}{c} -1 \\ -3 \end{array}\right]+\left[\begin{array}{c} -67 \\ 22 \end{array}\right] \int\limits\limits_0^{\infty} e^{-(p+1) t} d t=\left[\begin{array}{c} -1 \\ -3 \end{array}\right]+\frac{1}{p+1}\left[\begin{array}{c} -67 \\ 22 \end{array}\right]=-\frac{1}{p+1}\left[\begin{array}{c} p+68 \\ 3 p-19 \end{array}\right]  \end{multline*} \begin{multline*}\left|\begin{array}{cc} p+7 & 18 \\ -2 & p-5 \end{array}\right|=(p+7)(p-5)+36=p^2+2 p+1=(p+1)^2 \Rightarrow\\\Rightarrow \forall p>-1 \exists\left[\begin{array}{cc} p+7 & 18 \\ -2 & p-5 \end{array}\right]^{-1}=\frac{1}{(p+1)^2}\left[\begin{array}{cc} p-5 & -18 \\ 2 & p+7 \end{array}\right] \Rightarrow \vec{R}(p)=\\=-\frac{1}{p+1}\left[\begin{array}{cc} p+7 & 18 \\ -2 & p-5 \end{array}\right]^{-1}\left[\begin{array}{c} p+68 \\ 3 p-19 \end{array}\right]=-\frac{1}{(p+1)^3}\left[\begin{array}{cc} p-5 & -18 \\ 2 & p+7 \end{array}\right]\left[\begin{array}{c} p+68 \\ 3 p-19 \end{array}\right]=\\=-\frac{1}{(p+1)^3}\left[\begin{array}{c} p^2+9 p+2 \\ 3 p^2+4 p+3 \end{array}\right]=-\frac{1}{(p+1)^3}\left[\begin{array}{c} (p+1)^2+7(p+1)-6 \\ 3(p+1)^2-2(p+1)+2 \end{array}\right] \end{multline*} \begin{multline*} \mathcal{F}[f(t)]=\frac{1}{\sqrt{2 \pi}} \int\limits\limits_{-\infty}^{\infty} f(t) e^{-i \omega t} d t \Rightarrow F(p)=\mathcal{L}[f(t)]=\sqrt{2 \pi} \lim _{\omega \rightarrow-i p} \mathcal{F}[f(t) \eta(t)] \Rightarrow\\\Rightarrow f(t)=\mathcal{L}^{-1}[F(p)]=\frac{1}{2 \pi i} \int\limits\limits_{\sigma-i \infty}^{\sigma+i \infty} F(p) e^{p t} d p  \end{multline*} \begin{multline*} \left\{z \in \mathbb{C}\left(|z-\sigma| \leq \tau ; \arg (z-\sigma) \in\left[-\frac{\pi}{2}, \frac{\pi}{2}\right]\right)\right\}=\Sigma \Rightarrow \frac{1}{2 \pi i} \oint\limits_{\partial \Sigma} F(p) e^{p t} d p=\\=\frac{1}{2 \pi i}\left(\int\limits_{\sigma-i \tau}^{\sigma+i \tau} F(p) e^{p t} d p-i \tau \int\limits_{-\pi / 2}^{\pi / 2} F\left(\sigma+\tau e^{i \varphi}\right) \exp \left(\left(\sigma+\tau e^{i \varphi}\right) t+i \varphi\right) d \varphi\right)=\sum_{s \in\left\{z \in \Sigma \mid F(p) \notin C^1(z)\right\}}\operatorname{Res}\left(F(p) e^{p t}\right) \end{multline*} \begin{multline*} \lim _{\tau \rightarrow \infty} \tau F\left(\sigma+\tau e^{i \varphi}\right)=0 \Rightarrow f(t)=\frac{1}{2 \pi i} \int\limits_{\sigma-i \infty}^{\sigma+i \infty} F(p) e^{p t} d p=\\=\sum_{s \in\left\{z \in \mathbb{C}\left(\operatorname{Re}(z)>\sigma ; F(p) \notin C^1(z)\right)\right\}} \sum_{p=s}\operatorname{Res}\left(F(p) e^{p t}\right) \end{multline*} \begin{multline*} \vec{r}(t)=-\operatorname{Res}_{p=-1}\left(\vec{R}(p) e^{p t}\right)=-\operatorname{Res}_{p=-1}\left(\frac{1}{(p+1)^3}\left[\begin{array}{c} (p+1)^2+7(p+1)-6 \\ 3(p+1)^2-2(p+1)+2 \end{array}\right] e^{p t}\right)=\\=-\frac{1}{2 \pi i} \lim _{\varepsilon \rightarrow 0} \sum_{n=0}^{\infty} \frac{t^n}{n !} \oint\limits_{|p+1|=\varepsilon} \frac{p^n}{(p+1)^3}\left[\begin{array}{c} (p+1)^2+7(p+1)-6 \\ 3(p+1)^2-2(p+1)+2 \end{array}\right] d p=\\ =-\frac{1}{2 \pi i} \lim _{\varepsilon \rightarrow 0} \sum_{n=0}^{\infty} \sum_{k=0}^n(-1)^{n-k} C_n^k \frac{t^n}{n !} \oint\limits_{|p+1|=\varepsilon}\left[\begin{array}{c} (p+1)^{k-1}+7(p+1)^{k-2}-6(p+1)^{k-3} \\ 3(p+1)^{k-1}-2(p+1)^{k-2}+2(p+1)^{k-3} \end{array}\right] d p=\\=-\frac{1}{2 \pi} \lim _{\varepsilon \rightarrow 0} \sum_{k=0}^{\infty} \sum_{n=k}^{\infty}(-1)^{n-k} C_n^k \frac{t^n}{n !} \varepsilon^{k-2} \int\limits_0^{2 \pi}\left[\begin{array}{c} \varepsilon^2 e^{2 i \varphi}+7 \varepsilon e^{i \varphi}-6 \\ 3 \varepsilon^2 e^{2 i \varphi}-2 \varepsilon e^{i \varphi}+2 \end{array}\right] e^{i(k-2) \varphi} d \varphi=\\ =-\lim _{\varepsilon \rightarrow 0} \sum_{k=0}^{\infty} \sum_{n=k}^{\infty}(-1)^{n-k} C_n^k \frac{t^n}{n !}\left(\left[\begin{array}{l} 1 \\ 3 \end{array}\right] \varepsilon^k \delta_{k 0}+\left[\begin{array}{c} 7 \\ -2 \end{array}\right] \varepsilon^{k-1} \delta_{k 1}+\left[\begin{array}{c} -6 \\ 2 \end{array}\right] \varepsilon^{k-2} \delta_{k 2}\right)=\\=-\left(\left[\begin{array}{l} 1 \\ 3 \end{array}\right] \sum_{n=0}^{\infty}(-1)^n \frac{t^n}{n !}-\left[\begin{array}{c} 7 \\ -2 \end{array}\right] \sum_{n=1}^{\infty}(-1)^n \frac{t^n}{(n-1) !}+\left[\begin{array}{c} -3 \\ 1 \end{array}\right] \sum_{n=2}^{\infty}(-1)^n \frac{t^n}{(n-2) !}\right)=\\=\left[\begin{array}{c} 3 t^2-7 t-1 \\ -t^2+2 t-3 \end{array}\right] e^{-t}  \end{multline*}","Apply the operational method to solve the Cauchy problem My attempt: It is usually assumed that is twice differentiable everywhere, so on the boundaries of each of the smooth definitions we need to match the limits Then you need to make sure that the initial conditions are met and all boundary conditions are met. . My solution:","\begin{cases}
x'=-7x-18y-67e^{-t},x(0)=-1\\
y'=2x+5y+22e^{-t}, \; y(0)=-3
\end{cases} x(t) = \frac{a \sin(4t)+ b\cos(4t)}{e^{4t}}\overset{x\in [0,1)}{\Rightarrow }x(t) = \frac{a \sin(4t)+ b\cos(4t)}{e^{4t}} + 12 x x(0), x(1), x'(0), x'(1) x(t) = \left\{\begin{matrix}
\frac{A\sin(4t)+B\cos(4t)}{e^{4t}},&t<0\\
\frac{C\sin(4t)+D\cos(4t)}{e^{4t}} + 12,&0 < t\le 1\\
\frac{E\sin(4t)+F\cos(4t)}{e^{4t}},&t>1
\end{matrix}\right\}
 A=2, B=-2, C=-10, D=-14 \begin{cases}
x'=-7x-18y-67e^{-t},x(0)=-1\\
y'=2x+5y+22e^{-t}, \; y(0)=-3
\end{cases} \Leftrightarrow \dot{\vec{r}}=\left[\begin{array}{cc}
-7 & -18 \\
2 & 5
\end{array}\right] \vec{r}+\left[\begin{array}{c}
-67 \\
22
\end{array}\right] e^{-t} ; \vec{r}(0)=\left[\begin{array}{l}
-1 \\
-3
\end{array}\right] \begin{multline*}
\mathcal{L}[f(t)]=\int\limits\limits_0^{\infty} f(t) e^{-p t} d t=F(p) \Rightarrow \mathcal{L}\left[\frac{d f}{d t}\right]=\int\limits\limits_0^{\infty} \frac{d f}{d t} e^{-p t} d t=\\=\lim _{t \rightarrow \infty} f(t) e^{-p t}-f(0)+p \int\limits\limits_0^{\infty} f(t) e^{-p t} d t=p F(p)-f(0) 
\end{multline*} \begin{multline*}
\mathcal{L}[\vec{r}(t)]=\vec{R}(p) \Rightarrow \forall p>-1\left(p \hat{1}-\left[\begin{array}{cc}
-7 & -18 \\
2 & 5
\end{array}\right]\right) \vec{R}(p)=\left[\begin{array}{cc}
p+7 & 18 \\
-2 & p-5
\end{array}\right] \vec{R}(p)=\\=\left[\begin{array}{c}
-1 \\
-3
\end{array}\right]+\left[\begin{array}{c}
-67 \\
22
\end{array}\right] \int\limits\limits_0^{\infty} e^{-(p+1) t} d t=\left[\begin{array}{c}
-1 \\
-3
\end{array}\right]+\frac{1}{p+1}\left[\begin{array}{c}
-67 \\
22
\end{array}\right]=-\frac{1}{p+1}\left[\begin{array}{c}
p+68 \\
3 p-19
\end{array}\right] 
\end{multline*} \begin{multline*}\left|\begin{array}{cc}
p+7 & 18 \\
-2 & p-5
\end{array}\right|=(p+7)(p-5)+36=p^2+2 p+1=(p+1)^2 \Rightarrow\\\Rightarrow \forall p>-1 \exists\left[\begin{array}{cc}
p+7 & 18 \\
-2 & p-5
\end{array}\right]^{-1}=\frac{1}{(p+1)^2}\left[\begin{array}{cc}
p-5 & -18 \\
2 & p+7
\end{array}\right] \Rightarrow \vec{R}(p)=\\=-\frac{1}{p+1}\left[\begin{array}{cc}
p+7 & 18 \\
-2 & p-5
\end{array}\right]^{-1}\left[\begin{array}{c}
p+68 \\
3 p-19
\end{array}\right]=-\frac{1}{(p+1)^3}\left[\begin{array}{cc}
p-5 & -18 \\
2 & p+7
\end{array}\right]\left[\begin{array}{c}
p+68 \\
3 p-19
\end{array}\right]=\\=-\frac{1}{(p+1)^3}\left[\begin{array}{c}
p^2+9 p+2 \\
3 p^2+4 p+3
\end{array}\right]=-\frac{1}{(p+1)^3}\left[\begin{array}{c}
(p+1)^2+7(p+1)-6 \\
3(p+1)^2-2(p+1)+2
\end{array}\right]
\end{multline*} \begin{multline*}
\mathcal{F}[f(t)]=\frac{1}{\sqrt{2 \pi}} \int\limits\limits_{-\infty}^{\infty} f(t) e^{-i \omega t} d t \Rightarrow F(p)=\mathcal{L}[f(t)]=\sqrt{2 \pi} \lim _{\omega \rightarrow-i p} \mathcal{F}[f(t) \eta(t)] \Rightarrow\\\Rightarrow f(t)=\mathcal{L}^{-1}[F(p)]=\frac{1}{2 \pi i} \int\limits\limits_{\sigma-i \infty}^{\sigma+i \infty} F(p) e^{p t} d p 
\end{multline*} \begin{multline*}
\left\{z \in \mathbb{C}\left(|z-\sigma| \leq \tau ; \arg (z-\sigma) \in\left[-\frac{\pi}{2}, \frac{\pi}{2}\right]\right)\right\}=\Sigma \Rightarrow \frac{1}{2 \pi i} \oint\limits_{\partial \Sigma} F(p) e^{p t} d p=\\=\frac{1}{2 \pi i}\left(\int\limits_{\sigma-i \tau}^{\sigma+i \tau} F(p) e^{p t} d p-i \tau \int\limits_{-\pi / 2}^{\pi / 2} F\left(\sigma+\tau e^{i \varphi}\right) \exp \left(\left(\sigma+\tau e^{i \varphi}\right) t+i \varphi\right) d \varphi\right)=\sum_{s \in\left\{z \in \Sigma \mid F(p) \notin C^1(z)\right\}}\operatorname{Res}\left(F(p) e^{p t}\right)
\end{multline*} \begin{multline*}
\lim _{\tau \rightarrow \infty} \tau F\left(\sigma+\tau e^{i \varphi}\right)=0 \Rightarrow f(t)=\frac{1}{2 \pi i} \int\limits_{\sigma-i \infty}^{\sigma+i \infty} F(p) e^{p t} d p=\\=\sum_{s \in\left\{z \in \mathbb{C}\left(\operatorname{Re}(z)>\sigma ; F(p) \notin C^1(z)\right)\right\}} \sum_{p=s}\operatorname{Res}\left(F(p) e^{p t}\right)
\end{multline*} \begin{multline*}
\vec{r}(t)=-\operatorname{Res}_{p=-1}\left(\vec{R}(p) e^{p t}\right)=-\operatorname{Res}_{p=-1}\left(\frac{1}{(p+1)^3}\left[\begin{array}{c}
(p+1)^2+7(p+1)-6 \\
3(p+1)^2-2(p+1)+2
\end{array}\right] e^{p t}\right)=\\=-\frac{1}{2 \pi i} \lim _{\varepsilon \rightarrow 0} \sum_{n=0}^{\infty} \frac{t^n}{n !} \oint\limits_{|p+1|=\varepsilon} \frac{p^n}{(p+1)^3}\left[\begin{array}{c}
(p+1)^2+7(p+1)-6 \\
3(p+1)^2-2(p+1)+2
\end{array}\right] d p=\\ =-\frac{1}{2 \pi i} \lim _{\varepsilon \rightarrow 0} \sum_{n=0}^{\infty} \sum_{k=0}^n(-1)^{n-k} C_n^k \frac{t^n}{n !} \oint\limits_{|p+1|=\varepsilon}\left[\begin{array}{c}
(p+1)^{k-1}+7(p+1)^{k-2}-6(p+1)^{k-3} \\
3(p+1)^{k-1}-2(p+1)^{k-2}+2(p+1)^{k-3}
\end{array}\right] d p=\\=-\frac{1}{2 \pi} \lim _{\varepsilon \rightarrow 0} \sum_{k=0}^{\infty} \sum_{n=k}^{\infty}(-1)^{n-k} C_n^k \frac{t^n}{n !} \varepsilon^{k-2} \int\limits_0^{2 \pi}\left[\begin{array}{c}
\varepsilon^2 e^{2 i \varphi}+7 \varepsilon e^{i \varphi}-6 \\
3 \varepsilon^2 e^{2 i \varphi}-2 \varepsilon e^{i \varphi}+2
\end{array}\right] e^{i(k-2) \varphi} d \varphi=\\ =-\lim _{\varepsilon \rightarrow 0} \sum_{k=0}^{\infty} \sum_{n=k}^{\infty}(-1)^{n-k} C_n^k \frac{t^n}{n !}\left(\left[\begin{array}{l}
1 \\
3
\end{array}\right] \varepsilon^k \delta_{k 0}+\left[\begin{array}{c}
7 \\
-2
\end{array}\right] \varepsilon^{k-1} \delta_{k 1}+\left[\begin{array}{c}
-6 \\
2
\end{array}\right] \varepsilon^{k-2} \delta_{k 2}\right)=\\=-\left(\left[\begin{array}{l}
1 \\
3
\end{array}\right] \sum_{n=0}^{\infty}(-1)^n \frac{t^n}{n !}-\left[\begin{array}{c}
7 \\
-2
\end{array}\right] \sum_{n=1}^{\infty}(-1)^n \frac{t^n}{(n-1) !}+\left[\begin{array}{c}
-3 \\
1
\end{array}\right] \sum_{n=2}^{\infty}(-1)^n \frac{t^n}{(n-2) !}\right)=\\=\left[\begin{array}{c}
3 t^2-7 t-1 \\
-t^2+2 t-3
\end{array}\right] e^{-t} 
\end{multline*}","['real-analysis', 'calculus', 'ordinary-differential-equations']"
22,Prove the difference of 2 solutions of different initial values problems is increasing with time,Prove the difference of 2 solutions of different initial values problems is increasing with time,,"I'm trying to solve this problem. In fact, I'm not really sure how to solve this, but, with guesses, I do have an attempt which is the following: Since for the second coordinate(that is $y$ coordinate), $f_{2}$ only depends on $y$ and they have the same initial values, the second coordinate of the solution curve to be the same, so I just consider the first coordinate Suppose the solution in the first coordinate is $x(t,x_{0})$ . Here I use $x_{0}$ to denote the initial value which is a vector in $\mathbb{R}^2$ and $X$ to denote this coordinate (direction). Then I have the following calculation $$\begin{align} \frac{\partial x}{\partial t}(t,x_{0})&= f_{1}(x,y) \\ \\ \frac{\partial^2 x }{\partial X\partial t}(t,x_{0}) &= \frac{\partial f}{\partial x}(x,y) \cdot \frac{\partial x}{\partial X}(t,x_{0})  \end{align}$$ Then consider the ode $$\begin{align} \frac{\partial }{\partial t}\left( \frac{\partial x}{\partial X}(t,x_{0}) \right)=  \frac{\partial f_{1}}{\partial x}(x,y) \cdot \frac{\partial x}{\partial X}(t,x_{0})  \end{align}$$ This is a first order linear ode with respect to $\frac{\partial x}{\partial X}$ . By solving it, I can obtain $$\begin{align} \frac{\partial x}{\partial X}(t,x_{0})=C\exp\left( \int _{0}^t  \frac{\partial f_{1}}{\partial x} \ d s \right) \end{align}$$ So far, I think that if all things above are right, then as long as $C>0$ , with $g(t)=\int_{0}^t\frac{\partial f_{1}}{\partial x} \ d s>0$ , then I can say that $\frac{\partial x}{\partial X}$ is increasing as $t$ is increasing, and hence if I have 2 different solutions $$\begin{align} x(t,x_{1})< \tilde{x}(t,x_{2}) \end{align}$$ at $t=0$ . That is, $x_{1}<x_{2}$ , we can have, when $t>s$ $$\begin{align} \frac{\tilde{x}(t,x_{2})-x(t,x_{1})}{x_{2}-x_{1}}&> \frac{\tilde{x}(s,x_{2})-x(s,x_{1})}{x_{2}-x_{1}} \\  \tilde{x}(t,x_{2})-x(t,x_{1})&> \tilde{x}(s,x_{2})-x(s,x_{1}) \end{align}$$ which is done. Now I have 2 questions: 1 I strongly doubt my attempt. If the above is wrong can anyone show me how to approach this problem? 2 If the reasoning above is right, how should I compute the constant $C$ ? Thank you very much.","I'm trying to solve this problem. In fact, I'm not really sure how to solve this, but, with guesses, I do have an attempt which is the following: Since for the second coordinate(that is coordinate), only depends on and they have the same initial values, the second coordinate of the solution curve to be the same, so I just consider the first coordinate Suppose the solution in the first coordinate is . Here I use to denote the initial value which is a vector in and to denote this coordinate (direction). Then I have the following calculation Then consider the ode This is a first order linear ode with respect to . By solving it, I can obtain So far, I think that if all things above are right, then as long as , with , then I can say that is increasing as is increasing, and hence if I have 2 different solutions at . That is, , we can have, when which is done. Now I have 2 questions: 1 I strongly doubt my attempt. If the above is wrong can anyone show me how to approach this problem? 2 If the reasoning above is right, how should I compute the constant ? Thank you very much.","y f_{2} y x(t,x_{0}) x_{0} \mathbb{R}^2 X \begin{align}
\frac{\partial x}{\partial t}(t,x_{0})&= f_{1}(x,y) \\ \\ \frac{\partial^2 x }{\partial X\partial t}(t,x_{0}) &= \frac{\partial f}{\partial x}(x,y) \cdot \frac{\partial x}{\partial X}(t,x_{0}) 
\end{align} \begin{align}
\frac{\partial }{\partial t}\left( \frac{\partial x}{\partial X}(t,x_{0}) \right)=  \frac{\partial f_{1}}{\partial x}(x,y) \cdot \frac{\partial x}{\partial X}(t,x_{0}) 
\end{align} \frac{\partial x}{\partial X} \begin{align}
\frac{\partial x}{\partial X}(t,x_{0})=C\exp\left( \int _{0}^t  \frac{\partial f_{1}}{\partial x} \ d s \right)
\end{align} C>0 g(t)=\int_{0}^t\frac{\partial f_{1}}{\partial x} \ d s>0 \frac{\partial x}{\partial X} t \begin{align}
x(t,x_{1})< \tilde{x}(t,x_{2})
\end{align} t=0 x_{1}<x_{2} t>s \begin{align}
\frac{\tilde{x}(t,x_{2})-x(t,x_{1})}{x_{2}-x_{1}}&> \frac{\tilde{x}(s,x_{2})-x(s,x_{1})}{x_{2}-x_{1}} \\  \tilde{x}(t,x_{2})-x(t,x_{1})&> \tilde{x}(s,x_{2})-x(s,x_{1})
\end{align} C","['ordinary-differential-equations', 'solution-verification', 'dynamical-systems']"
23,Solve the following differential equation : $\frac{d^3y}{dx^3}+y=3+e^{-x}+5e^{2x}.$,Solve the following differential equation :,\frac{d^3y}{dx^3}+y=3+e^{-x}+5e^{2x}.,"Solve the following differential equation : $\frac{d^3y}{dx^3}+y=3+e^{-x}+5e^{2x}.$ My solution goes like this: Given, $\frac{d^3y}{dx^3}+y=3+e^{-x}+5e^{2x}$ . We write this, as $(D^3+1)y=3+e^{-x}+5e^{2x}.$ We first, calculate the value of the complementary function, i.e general solution of the differential equation, $(D^3+1)y=0.$ The roots of $f(x)=x^3+1$ are : $-1,a_1=\frac12+\frac{\sqrt 3i}{2},a_2=\frac12-\frac{\sqrt 3i}{2}.$ Thus, the complementary function $CF=c_1e^{-x}+c_2e^{a_1x}+c_3e^{a_2x},$ where $c_1,c_2,c_3$ are arbitary constants. Now, we evaluate the particular integral $y$ of the original equation $$\frac{d^3y}{dx^3}+y=3+e^{-x}+5e^{2x}$$ i.e \begin{align} &y=\frac{1}{D^3+1}(3+e^{-x}+5e^{2x}) \\ &\implies y=\frac{1}{D^3+1}(3)+\frac{1}{D^3+1}e^{-x}+5\frac{1}{D^3+1}(e^{2x}) \\ &\implies y = 3+\frac{1}{D+1}(\frac{1}{D^2-D+1}e^{-x})+5\frac{e^{2x}}{9} \\ &\implies y = 3+\frac{1}{D+1}\frac{e^{-x}}{3}+5\frac{e^{2x}}{9} \\ &\implies y= 3+\frac{xe^{-x}}{3}+5\frac{e^{2x}}{9}. \end{align} Thus, the particular integral is, $$y= 3+\frac{xe^{-x}}{3}+5\frac{e^{2x}}{9}.$$ Now, the complete solution of the given equation $\frac{d^3y}{dx^3}+y=3+e^{-x}+5e^{2x}$ is : $$y_1=CF+y=c_1e^{-x}+c_2e^{a_1x}+c_3e^{a_2x}+3+\frac{xe^{-x}}{3}+5\frac{e^{2x}}{9}.$$ Is the above solution correct? If not, where is it going wrong ? Is the method, valid?","Solve the following differential equation : My solution goes like this: Given, . We write this, as We first, calculate the value of the complementary function, i.e general solution of the differential equation, The roots of are : Thus, the complementary function where are arbitary constants. Now, we evaluate the particular integral of the original equation i.e Thus, the particular integral is, Now, the complete solution of the given equation is : Is the above solution correct? If not, where is it going wrong ? Is the method, valid?","\frac{d^3y}{dx^3}+y=3+e^{-x}+5e^{2x}. \frac{d^3y}{dx^3}+y=3+e^{-x}+5e^{2x} (D^3+1)y=3+e^{-x}+5e^{2x}. (D^3+1)y=0. f(x)=x^3+1 -1,a_1=\frac12+\frac{\sqrt 3i}{2},a_2=\frac12-\frac{\sqrt 3i}{2}. CF=c_1e^{-x}+c_2e^{a_1x}+c_3e^{a_2x}, c_1,c_2,c_3 y \frac{d^3y}{dx^3}+y=3+e^{-x}+5e^{2x} \begin{align}
&y=\frac{1}{D^3+1}(3+e^{-x}+5e^{2x}) \\
&\implies y=\frac{1}{D^3+1}(3)+\frac{1}{D^3+1}e^{-x}+5\frac{1}{D^3+1}(e^{2x}) \\
&\implies y = 3+\frac{1}{D+1}(\frac{1}{D^2-D+1}e^{-x})+5\frac{e^{2x}}{9} \\
&\implies y = 3+\frac{1}{D+1}\frac{e^{-x}}{3}+5\frac{e^{2x}}{9} \\
&\implies y= 3+\frac{xe^{-x}}{3}+5\frac{e^{2x}}{9}.
\end{align} y= 3+\frac{xe^{-x}}{3}+5\frac{e^{2x}}{9}. \frac{d^3y}{dx^3}+y=3+e^{-x}+5e^{2x} y_1=CF+y=c_1e^{-x}+c_2e^{a_1x}+c_3e^{a_2x}+3+\frac{xe^{-x}}{3}+5\frac{e^{2x}}{9}.","['ordinary-differential-equations', 'solution-verification']"
24,Is the fact that $e^{ix}$ and $\cos(x) + i \sin(x)$ have the same derivative and a point in common enough to imply they’re equal,Is the fact that  and  have the same derivative and a point in common enough to imply they’re equal,e^{ix} \cos(x) + i \sin(x),"If we let $$f(x) = e^{ix}$$ and $$g(x) = \cos(x) + i \sin(x),$$ then \begin{align*} f(0) &= g(0) = 1,\\ f’(x) &= if(x),\\ g’(x) &= ig(x). \end{align*} Is this enough to imply they’re the same - is it possible for any other function $k(x)$ to take the form $k’(x) = i\ k(x)$ and where $k(0) = 1$ .",If we let and then Is this enough to imply they’re the same - is it possible for any other function to take the form and where .,"f(x) = e^{ix} g(x) = \cos(x) + i \sin(x), \begin{align*}
f(0) &= g(0) = 1,\\
f’(x) &= if(x),\\
g’(x) &= ig(x).
\end{align*} k(x) k’(x) = i\ k(x) k(0) = 1","['ordinary-differential-equations', 'derivatives', 'complex-numbers']"
25,Solving non-linear fractional differential equation,Solving non-linear fractional differential equation,,"I want to solve the non-linear Caputo-type fractional equation of the form ( $0 < \alpha < 1$ ) $$ ^cD^{\alpha}_0 f(t) = af(t)^4 + bf(t) + c$$ I have found, the equation $^cD^{\alpha}_0 f(t) = af(t) + g(t)$ is the Cauchy-type equation and is solved. But, setting $g(t) = af(t)^4 + c$ doesn't work. Just for clarification, I want something like $f(t) =$ something in $t$ . (Numerical solutions / Hints is also fine).Thanks in advance! PS: If Riemann-Liouville or any other fractional derivatives will make it easier, that's also fine. PS2: I am a newbie to fractional calculus, so I have no clue other than this book. Ref: Kilbas, Anatoly A.; Srivastava, Hari M.; Trujillo, Juan J. , Theory and applications of fractional differential equations, North-Holland Mathematics Studies 204. Amsterdam: Elsevier (ISBN 0-444-51832-0/hbk). xv, 523 p. (2006). ZBL1092.45003 .","I want to solve the non-linear Caputo-type fractional equation of the form ( ) I have found, the equation is the Cauchy-type equation and is solved. But, setting doesn't work. Just for clarification, I want something like something in . (Numerical solutions / Hints is also fine).Thanks in advance! PS: If Riemann-Liouville or any other fractional derivatives will make it easier, that's also fine. PS2: I am a newbie to fractional calculus, so I have no clue other than this book. Ref: Kilbas, Anatoly A.; Srivastava, Hari M.; Trujillo, Juan J. , Theory and applications of fractional differential equations, North-Holland Mathematics Studies 204. Amsterdam: Elsevier (ISBN 0-444-51832-0/hbk). xv, 523 p. (2006). ZBL1092.45003 .",0 < \alpha < 1  ^cD^{\alpha}_0 f(t) = af(t)^4 + bf(t) + c ^cD^{\alpha}_0 f(t) = af(t) + g(t) g(t) = af(t)^4 + c f(t) = t,"['ordinary-differential-equations', 'fractional-calculus', 'fractional-differential-equations']"
26,"if $f$ and $g$ are solutions of linear homogenous ODE, prove $af+bg$ is also a solution","if  and  are solutions of linear homogenous ODE, prove  is also a solution",f g af+bg,"a) If $f$ and $g$ are solutions of a linear homogenous ODE on some interval, prove that $af+bg$ is also a solution (on that same interval) for any real $a$ and $b$ . I found a solution online that essentially says: Let, $y''+py'+qy=0$ be the linear homogenous ODE Now check that $af+bg$ is a solution by verifying $(af+bg)''+p(af+bg)'+q(af+bg)=af''+bg''+paf'+pbg'+qaf+qbg$ $=a(f''+pf'+qf)+b(g''+pg'+qg)=a*0+b*0$ (since $f$ and $g$ are solutions which means $f''+pf'+qf=0$ and $g''+pg'+qg=0$ ) My question is , why did the solution use a second order ODE? Can I do this with a first order? i.e.: $y'+py=0$ so $(af+bg)'+p(af+bg)=0$ $af'+bg'+paf+pbg=0$ $a(f'+pf)+b(g'+pg)=0$ Using the same reasoning as the solution above: $a*0+b*0=0$ b) If we drop the ""homogenous"" hypothesis, is this still true? Prove a counterexample or proof. My attempt at a solution $y''+py'+qy=r(x)$ $f''+pf'+qf=0$ and $g''+pg'+qg=0$ Also the general solution to a non-homogenous DE is: $y(x)=g(x)+h(x)$ where $h(x)$ is a particular solution and $g(x)$ is the general solution to the corresponding homogenous DE $(af+bg)''+p(af+bg)'+q(af+bg)=a(f''+pf'+qf)+b(g''+pg'+qg)=a*0+b*0=0$ This is the general solution to the corresponding DE (i.e. the $g(x)$ ) I'm stuck on how to proceed from here.","a) If and are solutions of a linear homogenous ODE on some interval, prove that is also a solution (on that same interval) for any real and . I found a solution online that essentially says: Let, be the linear homogenous ODE Now check that is a solution by verifying (since and are solutions which means and ) My question is , why did the solution use a second order ODE? Can I do this with a first order? i.e.: so Using the same reasoning as the solution above: b) If we drop the ""homogenous"" hypothesis, is this still true? Prove a counterexample or proof. My attempt at a solution and Also the general solution to a non-homogenous DE is: where is a particular solution and is the general solution to the corresponding homogenous DE This is the general solution to the corresponding DE (i.e. the ) I'm stuck on how to proceed from here.",f g af+bg a b y''+py'+qy=0 af+bg (af+bg)''+p(af+bg)'+q(af+bg)=af''+bg''+paf'+pbg'+qaf+qbg =a(f''+pf'+qf)+b(g''+pg'+qg)=a*0+b*0 f g f''+pf'+qf=0 g''+pg'+qg=0 y'+py=0 (af+bg)'+p(af+bg)=0 af'+bg'+paf+pbg=0 a(f'+pf)+b(g'+pg)=0 a*0+b*0=0 y''+py'+qy=r(x) f''+pf'+qf=0 g''+pg'+qg=0 y(x)=g(x)+h(x) h(x) g(x) (af+bg)''+p(af+bg)'+q(af+bg)=a(f''+pf'+qf)+b(g''+pg'+qg)=a*0+b*0=0 g(x),['ordinary-differential-equations']
27,Question about the Poincaré first return function,Question about the Poincaré first return function,,"Let $M$ be a compact and connected two-dimensional differentiable manifold of class $C^2$ . Let $\varphi: \mathbb R \times M \to M$ be a flow of class $C^2$ in $M$ . Let $\sigma:[-1,1]\to M$ be an embedding of class $C^2$ such that $\Sigma:=\sigma((-1,1))$ is transversal to every orbit of $\varphi$ . Let $V\subset [-1,1]$ be the set of points whose image has its first return in $\Sigma$ .. We define the real function $f:V\to (-1,1)$ as follows $v\in (-1,1)$ is carried by $\sigma$ to $\sigma(v )\in\Sigma$ then the first return to $\Sigma$ from $\sigma(v)\in\Sigma$ is $\varphi(t_{\sigma(v)},\sigma(v))$ now it we return to $(-1,1)$ by $\sigma^{-1}$ . So that $$ f(v):=\sigma^{-1}(\varphi(t_{\sigma(v)},\sigma(v))) $$ I'm trying to prove that $f'(v)\neq 0$ for all $v$ , but I can't see it. My ideas were to prove that $f$ is strictly monotone but I failed in the attempt. My second attempt was to get an explicit relationship between $f'(v)$ with $\sigma$ and $\varphi$ but I've been having a hard time getting anywhere. Maybe the answer is obvious but I need something concrete to justify it because I don't see it so clearly. Any suggestion will be of incredible help, please detail your answer a bit so I can understand it.","Let be a compact and connected two-dimensional differentiable manifold of class . Let be a flow of class in . Let be an embedding of class such that is transversal to every orbit of . Let be the set of points whose image has its first return in .. We define the real function as follows is carried by to then the first return to from is now it we return to by . So that I'm trying to prove that for all , but I can't see it. My ideas were to prove that is strictly monotone but I failed in the attempt. My second attempt was to get an explicit relationship between with and but I've been having a hard time getting anywhere. Maybe the answer is obvious but I need something concrete to justify it because I don't see it so clearly. Any suggestion will be of incredible help, please detail your answer a bit so I can understand it.","M C^2 \varphi: \mathbb R \times M \to M C^2 M \sigma:[-1,1]\to M C^2 \Sigma:=\sigma((-1,1)) \varphi V\subset [-1,1] \Sigma f:V\to (-1,1) v\in (-1,1) \sigma \sigma(v )\in\Sigma \Sigma \sigma(v)\in\Sigma \varphi(t_{\sigma(v)},\sigma(v)) (-1,1) \sigma^{-1} 
f(v):=\sigma^{-1}(\varphi(t_{\sigma(v)},\sigma(v)))
 f'(v)\neq 0 v f f'(v) \sigma \varphi","['ordinary-differential-equations', 'differential-topology', 'dynamical-systems']"
28,"Having trouble understanding the solution of $\frac{d^{2}y}{dx^{2}}+2\frac{dy}{dx}+\lambda y=0,$ $y(0)=y(1)=1$",Having trouble understanding the solution of,"\frac{d^{2}y}{dx^{2}}+2\frac{dy}{dx}+\lambda y=0, y(0)=y(1)=1","Good day. I was working on this problem from my lecture notes. ""Find eigenvalues and corresponding eigenfunctions for the BVP $\frac{d^{2}y}{dx^{2}}+2\frac{dy}{dx}+\lambda y=0,$ $y(0)=y(1)=1$ and verify their orthogonality by direct calculation"" There's also a solution provided, which I've followed along with. Though solving the whole exercise is the end goal, I'm just concerned with finding the eigenvalues for the time being. I understand how to set up the auxiliary equation, and how to solve the general system of the ODE for $\lambda < 1$ and $\lambda = 1$ , but I don't understand what the solution does to solve it for $\lambda > 1$ . ""For $\lambda > 1$ , say $\lambda = 1 + \mu^2, \mu = \sqrt{\lambda-1} > 0$ , the general solution is $y(x) = (A\cos(\mu x) + B\sin(\mu x))e^{-x},$ $y(0) = 0 \to A=0,$ then $y(1) = 0 \Rightarrow B = 0,$ or $\sin(\mu) = 0,$ $\mu = n\pi,$ $n = 1, 2, 3...,$ giving the result: $\lambda_{n} = 1 + n^2\pi^2,$ $\phi_n(x) = e^{-x}\sin(n\pi x),$ $n = 1,2,3,...,$ "" (It then checks orthogonality) I don't understand why it substitutes $\mu$ . I thought when dealing with complex roots, you need it to be in form $m = p \pm qi$ , to substitute into $y = e^{px}(C_{1}\cos(qx)+C_{2}\sin(qx))$ . I've thought about it for a bit, but the link isn't clear yet.","Good day. I was working on this problem from my lecture notes. ""Find eigenvalues and corresponding eigenfunctions for the BVP and verify their orthogonality by direct calculation"" There's also a solution provided, which I've followed along with. Though solving the whole exercise is the end goal, I'm just concerned with finding the eigenvalues for the time being. I understand how to set up the auxiliary equation, and how to solve the general system of the ODE for and , but I don't understand what the solution does to solve it for . ""For , say , the general solution is then or giving the result: "" (It then checks orthogonality) I don't understand why it substitutes . I thought when dealing with complex roots, you need it to be in form , to substitute into . I've thought about it for a bit, but the link isn't clear yet.","\frac{d^{2}y}{dx^{2}}+2\frac{dy}{dx}+\lambda y=0, y(0)=y(1)=1 \lambda < 1 \lambda = 1 \lambda > 1 \lambda > 1 \lambda = 1 + \mu^2, \mu = \sqrt{\lambda-1} > 0 y(x) = (A\cos(\mu x) + B\sin(\mu x))e^{-x}, y(0) = 0 \to A=0, y(1) = 0 \Rightarrow B = 0, \sin(\mu) = 0, \mu = n\pi, n = 1, 2, 3..., \lambda_{n} = 1 + n^2\pi^2, \phi_n(x) = e^{-x}\sin(n\pi x), n = 1,2,3,..., \mu m = p \pm qi y = e^{px}(C_{1}\cos(qx)+C_{2}\sin(qx))","['ordinary-differential-equations', 'eigenfunctions']"
29,Calculus of variations and integrating $\left(\frac{dy}{dx}\right)^2=\alpha^2(1+x)$,Calculus of variations and integrating,\left(\frac{dy}{dx}\right)^2=\alpha^2(1+x),"I'm trying to wrap my head around a step in the solution of an exercise that has me stumped. The exercise is the following : Given the functional $$S[y]=\int_0^{1}dx\sqrt{1+x+y'^2}, \quad y(0)=x_0, \quad y(1)=x_1$$ it is asked to show that $y(x)$ defined by $$y'(x)=k\sqrt{1+x+y'(x)^2},$$ where $k$ is a constant, makes the functional stationary. Then, by expressing $y'(x)$ in terms of $x$ , one should show that the solution is $$y(x)=x_0+\frac{x_1-x_0}{2^{3/2}-1}\left((1+x)^{3/2}-1\right).$$ It has been shown previously in the course that given the functional $S[y]=\int_a^{b}dxF(x,y'),\quad y(a)=A, \quad y(b)=B,$ we obtain the following differential equation for the stationary path $y$ of $S$ : $$\frac{\partial}{\partial y'}F(x,y')=k, \quad y(a)=A, \quad y(b)=B,$$ where $k$ is a constant. Now, the solution of the initial problem goes as follows: We have $F(x,v)=\sqrt{1+x+v^2}$ and the general equation (above) becomes $$v=k\sqrt{1+x+v^2}, \quad \text{where}\quad v=y'(x).$$ Rearranging and squaring, we obtain $$\left(\frac{dy}{dx}\right)^2=\alpha^2(1+x), \quad \alpha^2=\frac{k^2}{1-k^2}.$$ Integrating gives the solution $$y(x)-x_0=\alpha\int_0^xdx\sqrt{1+x}=\frac{2\alpha}{3}\left((1+x)^{3/2}-1\right).$$ It then goes on to find the value of $\alpha$ with the boundary conditions. My questioning mostly pertains to the integration part. I feel like it is very vague, and I'm not sure how to go about such an integration and come out with the desired result. What about the integration limits $0$ and $x$ ? Secondarily, I also wonder about the need to introduce the variable $v$ . Any help would be tremendously appreciated.","I'm trying to wrap my head around a step in the solution of an exercise that has me stumped. The exercise is the following : Given the functional it is asked to show that defined by where is a constant, makes the functional stationary. Then, by expressing in terms of , one should show that the solution is It has been shown previously in the course that given the functional we obtain the following differential equation for the stationary path of : where is a constant. Now, the solution of the initial problem goes as follows: We have and the general equation (above) becomes Rearranging and squaring, we obtain Integrating gives the solution It then goes on to find the value of with the boundary conditions. My questioning mostly pertains to the integration part. I feel like it is very vague, and I'm not sure how to go about such an integration and come out with the desired result. What about the integration limits and ? Secondarily, I also wonder about the need to introduce the variable . Any help would be tremendously appreciated.","S[y]=\int_0^{1}dx\sqrt{1+x+y'^2}, \quad y(0)=x_0, \quad y(1)=x_1 y(x) y'(x)=k\sqrt{1+x+y'(x)^2}, k y'(x) x y(x)=x_0+\frac{x_1-x_0}{2^{3/2}-1}\left((1+x)^{3/2}-1\right). S[y]=\int_a^{b}dxF(x,y'),\quad y(a)=A, \quad y(b)=B, y S \frac{\partial}{\partial y'}F(x,y')=k, \quad y(a)=A, \quad y(b)=B, k F(x,v)=\sqrt{1+x+v^2} v=k\sqrt{1+x+v^2}, \quad \text{where}\quad v=y'(x). \left(\frac{dy}{dx}\right)^2=\alpha^2(1+x), \quad \alpha^2=\frac{k^2}{1-k^2}. y(x)-x_0=\alpha\int_0^xdx\sqrt{1+x}=\frac{2\alpha}{3}\left((1+x)^{3/2}-1\right). \alpha 0 x v","['integration', 'ordinary-differential-equations', 'calculus-of-variations']"
30,When do solution to differential equations belonging to the same parametric family intersect?,When do solution to differential equations belonging to the same parametric family intersect?,,"General Problem I am interested in studying whether solutions to the Fokker-Planck equation: \begin{equation} \tag{1}\label{fp} \frac{\partial p(x, t)}{\partial t} = \textrm{div}(-p(x, t)\nabla\log q(x)) + \frac{1}{2}\Delta p, \quad p(x, 0) = p_0 \end{equation} (where $p_0$ and $q$ are probability densities), can intersect at some time $t^\star$ , when varying the choice of $q$ . More formally, I want to know if there exists a triplet $(q_1, q_2, t^\star)$ such that $p_1(x, t^\star) = p_2(x, t^\star)$ , for all $x$ , and where $p_1$ (resp. $p_2$ ) is the solution of \eqref{fp} with $q=q_1$ (resp. $q=q_2$ ), and $t^\star>0$ . I welcome any pointer to works/fields studying the intersection points of solutions of differential equations (a) belonging to some parametric family and (b) with the same initial conditions. Simpler Setting Looking first at simpler ordinary differential equations (ODEs), note that the following ODE: $$\tag{2}\label{ode1} \frac{\textrm{d}x}{\textrm{d}t} + ax = 0, \quad x(0) = 0, \, a \neq 0 $$ is such that no triplet $(a_1, a_2, t^\star)$ (with $a_1, a_2 \ne 0$ , and $t^\star > 0$ ) verifies $x_1(t^\star)=x_2(t^\star)$ , where $x_1$ (resp. $x_2$ ) is the solution to \eqref{ode1} with $a=a_1$ (resp. $a=a_2$ ). On the other hand, for the ODE: $$\tag{3}\label{ode2} \frac{\textrm{d}^2x}{\textrm{d}t^2} + k^2x = 0, \quad x(0) = 0, \, \frac{\textrm{d}x}{\textrm{d}t}\bigg \rvert_{t=0} = 1, \, k \in \mathbb N \setminus \{0\} $$ the triplet $(1, 2, 2\pi)$ verifies $x_1(2\pi) = x_2(2\pi)$ , where $x_1$ (resp. $x_2$ ) is the solution to \eqref{ode2} with $k=1$ (resp. $k=2$ ). As this simpler setting is related to my original problem, I would also welcome pointers to any work in this latter simpler setting. PS: I added the tag Gradient Flows since solutions to \eqref{fp} are also solutions to the Gradient Flow of $KL(\cdot||q)$ with initial condition $p_0$ .","General Problem I am interested in studying whether solutions to the Fokker-Planck equation: (where and are probability densities), can intersect at some time , when varying the choice of . More formally, I want to know if there exists a triplet such that , for all , and where (resp. ) is the solution of \eqref{fp} with (resp. ), and . I welcome any pointer to works/fields studying the intersection points of solutions of differential equations (a) belonging to some parametric family and (b) with the same initial conditions. Simpler Setting Looking first at simpler ordinary differential equations (ODEs), note that the following ODE: is such that no triplet (with , and ) verifies , where (resp. ) is the solution to \eqref{ode1} with (resp. ). On the other hand, for the ODE: the triplet verifies , where (resp. ) is the solution to \eqref{ode2} with (resp. ). As this simpler setting is related to my original problem, I would also welcome pointers to any work in this latter simpler setting. PS: I added the tag Gradient Flows since solutions to \eqref{fp} are also solutions to the Gradient Flow of with initial condition .","\begin{equation}
\tag{1}\label{fp}
\frac{\partial p(x, t)}{\partial t} = \textrm{div}(-p(x, t)\nabla\log q(x)) + \frac{1}{2}\Delta p, \quad p(x, 0) = p_0
\end{equation} p_0 q t^\star q (q_1, q_2, t^\star) p_1(x, t^\star) = p_2(x, t^\star) x p_1 p_2 q=q_1 q=q_2 t^\star>0 \tag{2}\label{ode1}
\frac{\textrm{d}x}{\textrm{d}t} + ax = 0, \quad x(0) = 0, \, a \neq 0
 (a_1, a_2, t^\star) a_1, a_2 \ne 0 t^\star > 0 x_1(t^\star)=x_2(t^\star) x_1 x_2 a=a_1 a=a_2 \tag{3}\label{ode2}
\frac{\textrm{d}^2x}{\textrm{d}t^2} + k^2x = 0, \quad x(0) = 0, \, \frac{\textrm{d}x}{\textrm{d}t}\bigg \rvert_{t=0} = 1, \, k \in \mathbb N \setminus \{0\}
 (1, 2, 2\pi) x_1(2\pi) = x_2(2\pi) x_1 x_2 k=1 k=2 KL(\cdot||q) p_0","['ordinary-differential-equations', 'partial-differential-equations', 'stochastic-processes', 'gradient-flows']"
31,Solving a nonlinear ODE,Solving a nonlinear ODE,,"$$  y''-i(\sin(x)y)'-i\omega y-\lvert y \rvert^2y'=0 , \quad y\rvert_{x=0}=0 \quad y'\rvert_{x=0}=0 $$ Hello, im looking for advice on how to solve this equation, im intrested in knowing what possible values of $\omega$ could be. Ive tried to get a solution by using the WKBJ method by assuming that $y$ is \begin{equation} y=\psi(x)\exp(iS(z)) \end{equation} And then subbing this solution into the above equation and then assuming that both $\psi$ and $S$ are real then having seperating that equation into real and imaginary parts but then I have no idea what to do with the nonlinear $y$ term. Is there any methods or ways anyone could suggest on how to approach this problem? I also tried to find a solution to this equation by assuming that $y$ is real and splitting the problem into real and imaginary part. The imaginary part of that equation is separable and gives a solution but how then do i use the real part? Thank you. Update: So i tried another way solving this equation i haven't tried implementing the boundary conditions yet but does this seem like a reasonable approach? $$  y''-i(\sin(x)y)'-i\omega y-\lvert y \rvert^2y'=0 , \quad y\rvert_{x=0}=0 \quad y'\rvert_{x=0}=0 $$ I then split this equation in to real and imaginary parts: $$ \textrm{real: } y'' - \lvert y\rvert^2 y' = 0\\ \textrm{Imaginary: } \omega y - (sin(x)y)' = 0  $$ The imaginary part can be solved to get $$ y' = \dfrac{\omega - cos(x)}{sin(x)}y \\ $$ which directly integrates to $$ \lvert y \rvert = \exp\left(\int\dfrac{\omega - cos(x)}{sinx(x)} \right) $$ I can rearrange the real part and integrate it to find that $$ \lvert y' \rvert = \exp\left(\int \lvert y\rvert^2 dx\right) $$ This integral $$ \int \lvert y\rvert^2 dx = \dfrac{2sin(x)}{\omega - cos(x)}\exp\left(2\int\dfrac{\omega - cos(x)}{sin(x)}dx\right) $$ combining all these together i get that $$ \lvert y \rvert = \left| \dfrac{sinx(x)}{\omega - cos(x)}\right|\exp\left(-\dfrac{2sin(x)}{\omega - cos(x)}\exp\left(2\int \dfrac{\omega-cos(x)}{sin(x)} dx\right)\right)  $$ The solution i found looks really messy so i cant help but feeling like ive done some bad maths somehow does this seem reasonable? Thanks to Gribouillis for the suggestion, this equation originally had a $dy/dt$ term but im looking for a steady state solution, numerical solutions found from timestepping were oscilatory so thats where the $\omega$ was coming from, i was hoping there was some sort of expression i could get from looking at the steady state. Thanks again to anyone who can help.","Hello, im looking for advice on how to solve this equation, im intrested in knowing what possible values of could be. Ive tried to get a solution by using the WKBJ method by assuming that is And then subbing this solution into the above equation and then assuming that both and are real then having seperating that equation into real and imaginary parts but then I have no idea what to do with the nonlinear term. Is there any methods or ways anyone could suggest on how to approach this problem? I also tried to find a solution to this equation by assuming that is real and splitting the problem into real and imaginary part. The imaginary part of that equation is separable and gives a solution but how then do i use the real part? Thank you. Update: So i tried another way solving this equation i haven't tried implementing the boundary conditions yet but does this seem like a reasonable approach? I then split this equation in to real and imaginary parts: The imaginary part can be solved to get which directly integrates to I can rearrange the real part and integrate it to find that This integral combining all these together i get that The solution i found looks really messy so i cant help but feeling like ive done some bad maths somehow does this seem reasonable? Thanks to Gribouillis for the suggestion, this equation originally had a term but im looking for a steady state solution, numerical solutions found from timestepping were oscilatory so thats where the was coming from, i was hoping there was some sort of expression i could get from looking at the steady state. Thanks again to anyone who can help.","
 y''-i(\sin(x)y)'-i\omega y-\lvert y \rvert^2y'=0 , \quad y\rvert_{x=0}=0 \quad y'\rvert_{x=0}=0
 \omega y \begin{equation}
y=\psi(x)\exp(iS(z))
\end{equation} \psi S y y 
 y''-i(\sin(x)y)'-i\omega y-\lvert y \rvert^2y'=0 , \quad y\rvert_{x=0}=0 \quad y'\rvert_{x=0}=0
 
\textrm{real: } y'' - \lvert y\rvert^2 y' = 0\\
\textrm{Imaginary: } \omega y - (sin(x)y)' = 0 
 
y' = \dfrac{\omega - cos(x)}{sin(x)}y \\
 
\lvert y \rvert = \exp\left(\int\dfrac{\omega - cos(x)}{sinx(x)} \right)
 
\lvert y' \rvert = \exp\left(\int \lvert y\rvert^2 dx\right)
 
\int \lvert y\rvert^2 dx = \dfrac{2sin(x)}{\omega - cos(x)}\exp\left(2\int\dfrac{\omega - cos(x)}{sin(x)}dx\right)
 
\lvert y \rvert = \left| \dfrac{sinx(x)}{\omega - cos(x)}\right|\exp\left(-\dfrac{2sin(x)}{\omega - cos(x)}\exp\left(2\int \dfrac{\omega-cos(x)}{sin(x)} dx\right)\right) 
 dy/dt \omega","['ordinary-differential-equations', 'mathematical-physics', 'eigenfunctions']"
32,Laurent expansion of Meijer's G function,Laurent expansion of Meijer's G function,,"I am considering the following equation (a Generalized hyper-geometric equation): $$\left(D-\beta_1\right)\left(D-\beta_2\right)f(x)-x\left(D+1-\alpha_1\right)\left(D+1-\alpha_2\right)f(x)=0$$ where in this case I specifically take $\alpha_1=\frac{1}{4}$ , $\alpha_2=\frac{5}{4}$ , $\beta_1=\frac{3}{4}$ , $\beta_2=-\frac{1}{4}$ . In fact this equation has the following two solutions: $$f_1(x)=x^{3/4}\,{}_2F_1\left(\frac{3}{2},\frac{1}{2};2;x\right)$$ The second solution is quite unfamiliar for me, which is denoted by MeijerG[{{}, {1/4, 5/4}}, {{-(1/4), 3/4}, {}}, x] in Mathematica. It has the following integral representation: $$f_2(x)=\frac{1}{2\pi i}\int_L\frac{\Gamma\left(s-\frac{1}{4}\right)\Gamma\left(\frac{3}{4}+s\right)}{\Gamma\left(s+\frac{1}{4}\right)\Gamma\left(\frac{5}{4}+s\right)}x^{-s}ds$$ with $L$ a proper contour. It can be seen that $\Gamma\left(s-\frac{1}{4}\right)\Gamma\left(\frac{3}{4}+s\right)$ has a simpole pole at $s=\frac{1}{4}$ , and has double poles at $s=-\frac{3}{4}$ , $s=-\frac{7}{4}$ , $s=-\frac{11}{4}$ ,...I sum up the residue to get a series expansion: $$f_2(x)=\frac{2}{\pi}x^{-1/4}+\frac{x^{3/4}}{\pi^2}\sum_{n=0}^{\infty}\frac{\Gamma\left(\frac{3}{2}+n\right)\Gamma\left(\frac{1}{2}+n\right)}{n!\Gamma\left(n+2\right)}\left(-\psi\left(-n-\frac{1}{2}\right)-\psi\left(\frac{1}{2}-n\right)-\ln x\right)x^{n}$$ where I use $\psi(z)=\frac{\Gamma^{\prime}(z)}{\Gamma(z)}$ The problem is, does this series expansion converge on the whole complex plane? Can I use this expansion to determine the monodromy around both $0$ and $\infty$ ? Thanks to the hint from @Mariusz Iwaniuk, I know that by using the following command in Mathematica: MeijerG[{{}, {1/4, 5/4}}, {{-(1/4), 3/4}, {}}, x] // FunctionExpand , the output may indicate that $f_2(x)$ vanishes outside the unit disk $0<|x|<1$ . However, another question comes out: since the equation is invariant under $x\to \frac{1}{x}$ , I can construct the third solution as $f_3(x)=f_2(1/x)$ , which is nonzero only near the $\infty$ . Then we just have three independent solutions at hand. What's the problem with my reasoning?","I am considering the following equation (a Generalized hyper-geometric equation): where in this case I specifically take , , , . In fact this equation has the following two solutions: The second solution is quite unfamiliar for me, which is denoted by MeijerG[{{}, {1/4, 5/4}}, {{-(1/4), 3/4}, {}}, x] in Mathematica. It has the following integral representation: with a proper contour. It can be seen that has a simpole pole at , and has double poles at , , ,...I sum up the residue to get a series expansion: where I use The problem is, does this series expansion converge on the whole complex plane? Can I use this expansion to determine the monodromy around both and ? Thanks to the hint from @Mariusz Iwaniuk, I know that by using the following command in Mathematica: MeijerG[{{}, {1/4, 5/4}}, {{-(1/4), 3/4}, {}}, x] // FunctionExpand , the output may indicate that vanishes outside the unit disk . However, another question comes out: since the equation is invariant under , I can construct the third solution as , which is nonzero only near the . Then we just have three independent solutions at hand. What's the problem with my reasoning?","\left(D-\beta_1\right)\left(D-\beta_2\right)f(x)-x\left(D+1-\alpha_1\right)\left(D+1-\alpha_2\right)f(x)=0 \alpha_1=\frac{1}{4} \alpha_2=\frac{5}{4} \beta_1=\frac{3}{4} \beta_2=-\frac{1}{4} f_1(x)=x^{3/4}\,{}_2F_1\left(\frac{3}{2},\frac{1}{2};2;x\right) f_2(x)=\frac{1}{2\pi i}\int_L\frac{\Gamma\left(s-\frac{1}{4}\right)\Gamma\left(\frac{3}{4}+s\right)}{\Gamma\left(s+\frac{1}{4}\right)\Gamma\left(\frac{5}{4}+s\right)}x^{-s}ds L \Gamma\left(s-\frac{1}{4}\right)\Gamma\left(\frac{3}{4}+s\right) s=\frac{1}{4} s=-\frac{3}{4} s=-\frac{7}{4} s=-\frac{11}{4} f_2(x)=\frac{2}{\pi}x^{-1/4}+\frac{x^{3/4}}{\pi^2}\sum_{n=0}^{\infty}\frac{\Gamma\left(\frac{3}{2}+n\right)\Gamma\left(\frac{1}{2}+n\right)}{n!\Gamma\left(n+2\right)}\left(-\psi\left(-n-\frac{1}{2}\right)-\psi\left(\frac{1}{2}-n\right)-\ln x\right)x^{n} \psi(z)=\frac{\Gamma^{\prime}(z)}{\Gamma(z)} 0 \infty f_2(x) 0<|x|<1 x\to \frac{1}{x} f_3(x)=f_2(1/x) \infty","['ordinary-differential-equations', 'laurent-series', 'hypergeometric-function', 'monodromy']"
33,Solution to $x'=x\sin(\frac{\pi}{x})$ is unique,Solution to  is unique,x'=x\sin(\frac{\pi}{x}),"I want to prove that the only solution to the ODE $$x'=\begin{cases} x\sin(\frac{\pi}{x}) \text{ if } x \neq 0 \\ 0 \text{ else}\end{cases}$$ with $x(0) = 0$ is unique (so it is the constant solution $x=0$ ). Since $f(x) = x\sin(\frac{\pi}{x})$ is not locally Lipschitz at $0$ we cannot use any general result about uniqueness. I have tried to prove that if $x$ is a solution then $x^2$ is locally decreasing around $0$ (a trick that has worked other times ) but the oscillations of the term $\sin{\frac{\pi}{x}}$ make impossible reaching conclusions about sign. I have noticed that since $f$ is odd, any solution $x$ must be an even function, this is $x(-t) = x(t)$ . The equation does not seem to be solvable so more direct approach are out of hand I think. Can you give me any hint?","I want to prove that the only solution to the ODE with is unique (so it is the constant solution ). Since is not locally Lipschitz at we cannot use any general result about uniqueness. I have tried to prove that if is a solution then is locally decreasing around (a trick that has worked other times ) but the oscillations of the term make impossible reaching conclusions about sign. I have noticed that since is odd, any solution must be an even function, this is . The equation does not seem to be solvable so more direct approach are out of hand I think. Can you give me any hint?",x'=\begin{cases} x\sin(\frac{\pi}{x}) \text{ if } x \neq 0 \\ 0 \text{ else}\end{cases} x(0) = 0 x=0 f(x) = x\sin(\frac{\pi}{x}) 0 x x^2 0 \sin{\frac{\pi}{x}} f x x(-t) = x(t),"['ordinary-differential-equations', 'dynamical-systems']"
34,Trapping Region for ODE System.,Trapping Region for ODE System.,,"I am working on the following problem, given the system of two differential equations $x′=2x+y−2x^3−3xy^2,$ $y′=−2x+4y−4y^3−2x^2y,$ So far, I have tackled similar problems by trying to find a trapping region that does not contain an equilibrium point in its interior. (Applying Poincare-Bendixson Theorem). Clearly $(0,0)$ is an equilibrium point. (It will need to be shown it is the only equilibrium point). Now using a Lyapunov function, I am trying to find a trapping region.","I am working on the following problem, given the system of two differential equations So far, I have tackled similar problems by trying to find a trapping region that does not contain an equilibrium point in its interior. (Applying Poincare-Bendixson Theorem). Clearly is an equilibrium point. (It will need to be shown it is the only equilibrium point). Now using a Lyapunov function, I am trying to find a trapping region.","x′=2x+y−2x^3−3xy^2, y′=−2x+4y−4y^3−2x^2y, (0,0)","['ordinary-differential-equations', 'dynamical-systems', 'lyapunov-functions', 'limit-cycles']"
35,How to solve $y''(t)=|y(t)|$,How to solve,y''(t)=|y(t)|,"Solve differential equation: $y''(t) = |y(t)|$ My attempt to solution: I could not find all solutions possibles to the problem. And I do not know how to continue. $1)$ We know that $y''(t) = |y(t)| \Rightarrow (y''(t))^2=(|y(t)|)^2 \Rightarrow (y''(t))^2=(y(t))^2$ $ \Rightarrow(y''(t) - y(t))(y''(t) + y(t))=0$ $2)$ Define: $y_1''(t)=y_1(t)$ and $y_2''(t)=-y_2(t)$ $3)$ So, we have three possibilites, First is: For all real $t$ , $y_1(t)$ is solution of $y''(t) = |y(t)|$ Second is:  For all real $t$ , $y_2(t)$ is solution  of $y''(t) = |y(t)|$ Third is: $y_1(t)$ is solution  of $y''(t) = |y(t)|$ for some values of $t$ , and $y_2(t)$ is solutions  of $y''(t) = |y(t)|$ for the remaining values of t. $4)$ I test the First situacion, and I find the condition for it: The general solution of $y_1''(t)=y_1(t)$ is $y_1(t) = C_1.e^{t}+C_2.e^{-t}$ So, if $y_1(t)$ is solution for all t, implies that $y_1''(t)=|y_1(t)| \ge 0$ $y_1''(t)= C_1.e^{t}+C_2.e^{-t} \ge 0$ for all real t The condition of $C_1, C_2$ is that $C_1 \ge 0$ and $C_2 \ge 0$ $5)$ I test the Second situation, and I find that is impossible: The general solution of $y_2''(t)=-y_2(t)$ is $y_2(t) = C_3.\cos{t}+C_4.\sin{t}$ So, if $y_2(t)$ is solution for all t, implies that $y_2''(t)=|y_2(t)| \ge 0$ $y_2''(t)= -C_3.\cos{t}-C_4.\sin{t} \ge 0$ for all real t It is impossible, unless $C_1 = C_2 = 0$ that implies $y(t) = 0$ for all real t. $6)$ The last possibility is the Third situation, and I could not to find this As $y(t)$ is a continuous function, when one of the solutions ceases to be valid and passes to another solution, both must be equal at these exact instants and differenciable. And I don't know how to find this. Also, I suspect we will find a multitude of solutions for the third case.","Solve differential equation: My attempt to solution: I could not find all solutions possibles to the problem. And I do not know how to continue. We know that Define: and So, we have three possibilites, First is: For all real , is solution of Second is:  For all real , is solution  of Third is: is solution  of for some values of , and is solutions  of for the remaining values of t. I test the First situacion, and I find the condition for it: The general solution of is So, if is solution for all t, implies that for all real t The condition of is that and I test the Second situation, and I find that is impossible: The general solution of is So, if is solution for all t, implies that for all real t It is impossible, unless that implies for all real t. The last possibility is the Third situation, and I could not to find this As is a continuous function, when one of the solutions ceases to be valid and passes to another solution, both must be equal at these exact instants and differenciable. And I don't know how to find this. Also, I suspect we will find a multitude of solutions for the third case.","y''(t) = |y(t)| 1) y''(t) = |y(t)| \Rightarrow (y''(t))^2=(|y(t)|)^2 \Rightarrow (y''(t))^2=(y(t))^2  \Rightarrow(y''(t) - y(t))(y''(t) + y(t))=0 2) y_1''(t)=y_1(t) y_2''(t)=-y_2(t) 3) t y_1(t) y''(t) = |y(t)| t y_2(t) y''(t) = |y(t)| y_1(t) y''(t) = |y(t)| t y_2(t) y''(t) = |y(t)| 4) y_1''(t)=y_1(t) y_1(t) = C_1.e^{t}+C_2.e^{-t} y_1(t) y_1''(t)=|y_1(t)| \ge 0 y_1''(t)= C_1.e^{t}+C_2.e^{-t} \ge 0 C_1, C_2 C_1 \ge 0 C_2 \ge 0 5) y_2''(t)=-y_2(t) y_2(t) = C_3.\cos{t}+C_4.\sin{t} y_2(t) y_2''(t)=|y_2(t)| \ge 0 y_2''(t)= -C_3.\cos{t}-C_4.\sin{t} \ge 0 C_1 = C_2 = 0 y(t) = 0 6) y(t)",['ordinary-differential-equations']
36,Extension of the solution,Extension of the solution,,"Let $H \in C[\mathbb{R}^{n}, \mathbb{R}]$ and $H(x) \to \infty$ as $|x | \to \infty$ . Suppose $f \in C[\mathbb{R}_{+} \times \mathbb{R}^{n}, \mathbb{R}^{n}]$ and for some $M >0$ : $$ \frac{\partial H}{\partial x} \cdot f(t,x) \leq 0, \quad |x| \geq M, t\in \mathbb{R}_{+} =[0, \infty) $$ Show that the solutions of ODE: $x' = f(t,x), x(t_{0}) = x_{0}$ exists on $[t_{0}, \infty)$ . My attempt is that I am trying to prove that $f(t,x)$ is bounded on the right maximal interval of existence. Using the integration-formula $$ x(t) = x_0 + \int_{t_0}^{t} f(w,x(w)) dw $$ to derive that $x(t)$ is bounded on the maximal interval. However, I was stuck using the assumption on $H(x)$ . I appreciate any help how to prove this.","Let and as . Suppose and for some : Show that the solutions of ODE: exists on . My attempt is that I am trying to prove that is bounded on the right maximal interval of existence. Using the integration-formula to derive that is bounded on the maximal interval. However, I was stuck using the assumption on . I appreciate any help how to prove this.","H \in C[\mathbb{R}^{n}, \mathbb{R}] H(x) \to \infty |x | \to \infty f \in C[\mathbb{R}_{+} \times \mathbb{R}^{n}, \mathbb{R}^{n}] M >0 
\frac{\partial H}{\partial x} \cdot f(t,x) \leq 0, \quad |x| \geq M, t\in \mathbb{R}_{+} =[0, \infty)
 x' = f(t,x), x(t_{0}) = x_{0} [t_{0}, \infty) f(t,x) 
x(t) = x_0 + \int_{t_0}^{t} f(w,x(w)) dw
 x(t) H(x)","['real-analysis', 'ordinary-differential-equations']"
37,Does the Green's Function for an IVP always converge while integrating?,Does the Green's Function for an IVP always converge while integrating?,,"I'm having some trouble solving an ODE using the Green's function method. The problem I'm working in is the simple harmonic oscillator equation $$ L[y(t)]=f(t)$$ $$ L = \frac{d^2}{dt^2}+\omega^2$$ $$y(0)=1,y'(0)=0$$ Now I'll construct my Green's function. I have two linear independent solutions for the homogeneous version ( $f(t)=0$ ) of my problem: $y_1(t)=\cos(\omega t)$ and $y_2(t)=\sin(\omega t)$ . I know that the Green's function satisfies $L[G(t,\xi)]=\delta(t-\xi)$ . For $t\neq\xi$ we have that $L[G(t,\xi)]=0$ . My function should be $$ G(t,\xi)=\left\{\begin{matrix}  A\cos(\omega t) + B \sin(\omega t),& t<\xi \\   C\cos(\omega t) + D \sin(\omega t),& \xi<t \end{matrix}\right. $$ Applying the initial conditions $G(0,\xi)=1,G'(0,\xi)=0$ we find that $A=1$ and $B=0$ . $$ G(t,\xi)=\left\{\begin{matrix}  \cos(\omega t),& t<\xi \\   C\cos(\omega t) + D \sin(\omega t),& \xi<t \end{matrix}\right. $$ Using the continuity of the function at $t=\xi$ and the discontinuity of the derivative at $t=\xi$ we should have $$C \cos(\omega \xi)+D \sin(\omega \xi)-\cos(\omega \xi)=0 \\ -C\sin(\omega \xi)+D\cos(\omega \xi)+\sin(\omega\xi)=\frac{1}{\omega}$$ Solving this system we find that $C=1-\dfrac{\sin(\omega\xi)}{\omega}, \;D=\dfrac{\cos(\omega\xi)}{\omega}$ . Finally, the Green's function should be $$ G(t,\xi)=\left\{\begin{matrix}  \cos(\omega t),& t<\xi \\   \dfrac{1}{\omega}\left[\omega-\sin(\omega\xi)\right]\cos(\omega t) + \dfrac{1}{\omega}\cos(\omega\xi)\sin(\omega t),& \xi<t \end{matrix}\right. $$ Now I should be able to find a solution for the ODE using the Green's function because $y(t)=\int_0^\infty f(\xi)\,G(t,\xi)\,d\xi$ I'm having trouble with this last step. Some of the functions I should try are $f(t)=e^{-t}$ and $f(t)=\cos(t)$ . For the case where the ""forcing function"" is a cosine, the integral does not converge. I tried the following \begin{align*} y(t) &=\int_0^\infty f(\xi)\,G(t,\xi)\,d\xi\\ &=\int_0^t \cos(\xi)\left[ \frac{1}{\omega}\left[\omega-\sin(\omega\xi)\right]\cos(\omega t) + \frac{1}{\omega}\cos(\omega\xi)\sin(\omega t)\right]d\xi\\ &\quad+\int_t^\infty \cos(\xi)\,\cos(\omega t)\,d\xi \end{align*} The first integral is not hard to compute and it give us an answer. But the second integral does not converge. I thought that the Green's function should give us a solution for this problem because this ODE has a solution that we can easily get with other methods. So, my question is: what is going wrong with my construction? Is my Green's function wrong or am I taking the integral in the wrong way with the wrong limits? Thanks in advance!","I'm having some trouble solving an ODE using the Green's function method. The problem I'm working in is the simple harmonic oscillator equation Now I'll construct my Green's function. I have two linear independent solutions for the homogeneous version ( ) of my problem: and . I know that the Green's function satisfies . For we have that . My function should be Applying the initial conditions we find that and . Using the continuity of the function at and the discontinuity of the derivative at we should have Solving this system we find that . Finally, the Green's function should be Now I should be able to find a solution for the ODE using the Green's function because I'm having trouble with this last step. Some of the functions I should try are and . For the case where the ""forcing function"" is a cosine, the integral does not converge. I tried the following The first integral is not hard to compute and it give us an answer. But the second integral does not converge. I thought that the Green's function should give us a solution for this problem because this ODE has a solution that we can easily get with other methods. So, my question is: what is going wrong with my construction? Is my Green's function wrong or am I taking the integral in the wrong way with the wrong limits? Thanks in advance!"," L[y(t)]=f(t)  L = \frac{d^2}{dt^2}+\omega^2 y(0)=1,y'(0)=0 f(t)=0 y_1(t)=\cos(\omega t) y_2(t)=\sin(\omega t) L[G(t,\xi)]=\delta(t-\xi) t\neq\xi L[G(t,\xi)]=0  G(t,\xi)=\left\{\begin{matrix}
 A\cos(\omega t) + B \sin(\omega t),& t<\xi \\ 
 C\cos(\omega t) + D \sin(\omega t),& \xi<t
\end{matrix}\right.  G(0,\xi)=1,G'(0,\xi)=0 A=1 B=0  G(t,\xi)=\left\{\begin{matrix}
 \cos(\omega t),& t<\xi \\ 
 C\cos(\omega t) + D \sin(\omega t),& \xi<t
\end{matrix}\right.  t=\xi t=\xi C \cos(\omega \xi)+D \sin(\omega \xi)-\cos(\omega \xi)=0 \\ -C\sin(\omega \xi)+D\cos(\omega \xi)+\sin(\omega\xi)=\frac{1}{\omega} C=1-\dfrac{\sin(\omega\xi)}{\omega}, \;D=\dfrac{\cos(\omega\xi)}{\omega}  G(t,\xi)=\left\{\begin{matrix}
 \cos(\omega t),& t<\xi \\ 
 \dfrac{1}{\omega}\left[\omega-\sin(\omega\xi)\right]\cos(\omega t) + \dfrac{1}{\omega}\cos(\omega\xi)\sin(\omega t),& \xi<t
\end{matrix}\right.  y(t)=\int_0^\infty f(\xi)\,G(t,\xi)\,d\xi f(t)=e^{-t} f(t)=\cos(t) \begin{align*}
y(t)
&=\int_0^\infty f(\xi)\,G(t,\xi)\,d\xi\\
&=\int_0^t \cos(\xi)\left[ \frac{1}{\omega}\left[\omega-\sin(\omega\xi)\right]\cos(\omega t) + \frac{1}{\omega}\cos(\omega\xi)\sin(\omega t)\right]d\xi\\
&\quad+\int_t^\infty \cos(\xi)\,\cos(\omega t)\,d\xi
\end{align*}","['ordinary-differential-equations', 'initial-value-problems', 'differential-operators', 'greens-function']"
38,ODE of continuous multivariate function,ODE of continuous multivariate function,,"This question is pretty similar to the one discussed here: ODE of multivariate function . The problem that we have is: $$ \frac{df(x, t)}{dt} = p(x) + c\int_{y}g(x, y)f(y,t) \, dy + cf(x,t) . $$ But this time, both $x$ and $t$ are continuous. The context of the problem and the background of me, as the author of both questions are the same, except that now I know how we can approach the previously mentioned problem. My questions are: Is this any specific general form (like Linear Separable ODEs) to which this problem belongs? Is this problem solvable? If so, how? Thanks in advance!","This question is pretty similar to the one discussed here: ODE of multivariate function . The problem that we have is: But this time, both and are continuous. The context of the problem and the background of me, as the author of both questions are the same, except that now I know how we can approach the previously mentioned problem. My questions are: Is this any specific general form (like Linear Separable ODEs) to which this problem belongs? Is this problem solvable? If so, how? Thanks in advance!","
\frac{df(x, t)}{dt} = p(x) + c\int_{y}g(x, y)f(y,t) \, dy + cf(x,t)
.
 x t","['calculus', 'ordinary-differential-equations', 'multivariable-calculus']"
39,"Differential equation: mixing salt into water problem where there is also leakage. Is my differential equation correct, and how to solve?","Differential equation: mixing salt into water problem where there is also leakage. Is my differential equation correct, and how to solve?",,"Initially, a tank contains $50$ litres of water and the tank water contains no salt. Salt is added to the water at time $t=0,$ at a constant rate of $5$ grams per minute. The salt does not change the volume of the water in the tank. The water in the tank is stirred constantly, so that the concentration of salt throughout the tank is uniform. Furthermore, the saltwater solution leaks out the tank a constant rate of $100$ ml per minute. What is the concentration of salt in the tank after $4$ hours? My attempt: Let $t$ be time in minutes, let $V(t)$ be the volume of water at time $t$ , and let $m(t)$ be the mass of salt dissolved in the water in the tank at time $t$ . Then, changing units of volume from litres into metres cubed, $V(t) = (0.05 - 0.0001t) m^3.$ Now, in order to find the rate of mass of salt leaving the tank with respect to time, we first should find the concentration of salt in the tank water. Concentration of salt in tank water = $\left(\frac{m(t)}{V(t)}\right)\ = \left(\frac{m(t)}{0.05 - 0.0001t}\right)\ g/m^3.\ $ Hence we need only need to find $\ m(240)\ $ and then the answer to the question is just $\left(\frac{m(240)}{0.05 - 0.0001\times 240}\right)\ g/m^3. $ Now I believe that the rate at which the mass of the salt leaks out the tank with respect to time is (concentration of salt in tank water in $g/m^3$ ) $\times$ (the rate at which water leaks out the tank in $m^3$ /second), although I am not certain of this. On the basis that this is all correct so far, the mass of salt in the water in the tank should satisfy the differential equation: $$\frac{dm}{dt} = 5 - 0.0001 \times \left(\frac{m(t)}{0.05 - 0.0001t}\right),$$ or to tidy up the final term, $$\frac{dm}{dt} = 5 - \frac{m(t)}{500 - t}.$$ I am not sure how to solve such an equation. Wolfram Alpha says that the general solution is: $$m(t) = c(t-500) + 5(t-500)\log(t-500).$$ However, this doesn't allow for the initial condition $m(0) = 0$ to be satisfied, so something has gone wrong here. Perhaps $\log(t-500)$ should be $\log\lvert t-500 \rvert$ ? Or Is it the differential equation itself wrong? I suspect so, in which case, can someone give me a hint as to how to figure out what the differential equation should be ? I am looking for pointers in the right direction, rather than a complete solution to the problem.","Initially, a tank contains litres of water and the tank water contains no salt. Salt is added to the water at time at a constant rate of grams per minute. The salt does not change the volume of the water in the tank. The water in the tank is stirred constantly, so that the concentration of salt throughout the tank is uniform. Furthermore, the saltwater solution leaks out the tank a constant rate of ml per minute. What is the concentration of salt in the tank after hours? My attempt: Let be time in minutes, let be the volume of water at time , and let be the mass of salt dissolved in the water in the tank at time . Then, changing units of volume from litres into metres cubed, Now, in order to find the rate of mass of salt leaving the tank with respect to time, we first should find the concentration of salt in the tank water. Concentration of salt in tank water = Hence we need only need to find and then the answer to the question is just Now I believe that the rate at which the mass of the salt leaks out the tank with respect to time is (concentration of salt in tank water in ) (the rate at which water leaks out the tank in /second), although I am not certain of this. On the basis that this is all correct so far, the mass of salt in the water in the tank should satisfy the differential equation: or to tidy up the final term, I am not sure how to solve such an equation. Wolfram Alpha says that the general solution is: However, this doesn't allow for the initial condition to be satisfied, so something has gone wrong here. Perhaps should be ? Or Is it the differential equation itself wrong? I suspect so, in which case, can someone give me a hint as to how to figure out what the differential equation should be ? I am looking for pointers in the right direction, rather than a complete solution to the problem.","50 t=0, 5 100 4 t V(t) t m(t) t V(t) = (0.05 - 0.0001t) m^3. \left(\frac{m(t)}{V(t)}\right)\ = \left(\frac{m(t)}{0.05 - 0.0001t}\right)\ g/m^3.\  \ m(240)\  \left(\frac{m(240)}{0.05 - 0.0001\times 240}\right)\ g/m^3.  g/m^3 \times m^3 \frac{dm}{dt} = 5 - 0.0001 \times \left(\frac{m(t)}{0.05 - 0.0001t}\right), \frac{dm}{dt} = 5 - \frac{m(t)}{500 - t}. m(t) = c(t-500) + 5(t-500)\log(t-500). m(0) = 0 \log(t-500) \log\lvert t-500 \rvert","['calculus', 'ordinary-differential-equations', 'problem-solving', 'initial-value-problems', 'related-rates']"
40,"Do we expect that ""unsolvable"" differential equations would have an analytical solution if we simply knew more math?","Do we expect that ""unsolvable"" differential equations would have an analytical solution if we simply knew more math?",,"In my engineering studies and while reading the book Chaos, I see a lot of mentions of complicated differential equations without solutions. For example, the equation $$\frac{dx}{dt}+\sin(x(t))=\sin(wt)$$ does not have an analytical solution as far as I know. Is there hope that if we had more functions at our disposal (for example, more functions like sine, hyperbolic sine, etc.) we would be able to find such a solution? Or is something like this fundamentally unsolvable for some reason? If it would be possible, are mathematicians working to discover these new mathematical terms? It fascinates me that we don't have the math to cleanly describe the three-body problem, for example, and it's hard to imagine that a clean solution wouldn't exist if we simply knew more.","In my engineering studies and while reading the book Chaos, I see a lot of mentions of complicated differential equations without solutions. For example, the equation does not have an analytical solution as far as I know. Is there hope that if we had more functions at our disposal (for example, more functions like sine, hyperbolic sine, etc.) we would be able to find such a solution? Or is something like this fundamentally unsolvable for some reason? If it would be possible, are mathematicians working to discover these new mathematical terms? It fascinates me that we don't have the math to cleanly describe the three-body problem, for example, and it's hard to imagine that a clean solution wouldn't exist if we simply knew more.",\frac{dx}{dt}+\sin(x(t))=\sin(wt),"['ordinary-differential-equations', 'partial-differential-equations']"
41,nonlinear ODE's involving heaviside step functions: The bouncy ball as an example,nonlinear ODE's involving heaviside step functions: The bouncy ball as an example,,"The dynamical equation for a ball bouncing on a plate (located at $x=0$ ) can be represented as $$ \ddot{x}(t) = -g - (k_1 x + k_2 \dot{x})H(-x), $$ where $H(x)$ is a heaviside step function and the collisions between the ball and the plate are represented with a spring-dashpot model. I would like to solve this equation for $x(0) = x_0$ and $\dot{x}(0) = v_0$ . One can solve this by calculating the velocity and time of the first collision, evaluating the Newtonian dynamics during the collision, then repeating this process iteratively through collisions. Obviously the resulting piecewise trajectory would be something like the following: This approach is somewhat unsatisfactory because I would ideally like to solve for the trajectory $x(t)$ at any arbitrary time, and this approach only yields a piecewise solution. A priori I do not know how many collisions have occurred up to a given time at which $x(t)$ is desired, so I would need to iterate an arbitrary number of times to find a solution. I wonder if there are specialized approaches to solve such ordinary differential equations analytically. The above equation is nonlinear and defined only in the sense of a distribution (since $H(x)$ is not exactly a function). Clearly the solution exhibits discontinuities. Are there any approaches I might read about to better understand such equations or solve such equations with more powerful tools than piecewise integration?","The dynamical equation for a ball bouncing on a plate (located at ) can be represented as where is a heaviside step function and the collisions between the ball and the plate are represented with a spring-dashpot model. I would like to solve this equation for and . One can solve this by calculating the velocity and time of the first collision, evaluating the Newtonian dynamics during the collision, then repeating this process iteratively through collisions. Obviously the resulting piecewise trajectory would be something like the following: This approach is somewhat unsatisfactory because I would ideally like to solve for the trajectory at any arbitrary time, and this approach only yields a piecewise solution. A priori I do not know how many collisions have occurred up to a given time at which is desired, so I would need to iterate an arbitrary number of times to find a solution. I wonder if there are specialized approaches to solve such ordinary differential equations analytically. The above equation is nonlinear and defined only in the sense of a distribution (since is not exactly a function). Clearly the solution exhibits discontinuities. Are there any approaches I might read about to better understand such equations or solve such equations with more powerful tools than piecewise integration?","x=0  \ddot{x}(t) = -g - (k_1 x + k_2 \dot{x})H(-x),  H(x) x(0) = x_0 \dot{x}(0) = v_0 x(t) x(t) H(x)","['ordinary-differential-equations', 'classical-mechanics', 'nonlinear-dynamics']"
42,Understanding the (Inverse) Flow of an ODE,Understanding the (Inverse) Flow of an ODE,,"Background The problem is taken from section 2.5 of this paper , by Raphael Danchin. We are concerned with the following Transport Equation: $\begin{cases} \partial_t a + v\cdot\nabla a + \lambda a = f, \ \text{in } \mathbb{R^+} \times \mathbb{R^d}, \\ a|_{t=0} = a_0, \ \text{in } \mathbb{R^d}. \end{cases}$ Here, the initial term $a_0 = a_0(x)$ , the source term $f=f(t,x)$ , the coeffictient $\lambda \geq 0$ , and the transport field $v=v(t,x)$ are all given. We assume $a\in X$ , for some Banach space $X$ , $f \in L^1_{\text{loc}}(\mathbb{R^+} ; X)$ , and $v$ is in a nice enough set (depending on choice of $X$ ) for there to be a unique solution. In particular, $v$ is at least integrable-in-time with $v(t)$ Lipschitz for all $t > 0$ , which ensures the existence of a flow, $\psi$ , with respect to $v$ . That is, $\psi$ satisfies \begin{align} \psi_0(x)  = x, \quad &  \forall x \in \mathbb{R^d} \\ \partial_t \psi_t(x)  = v(t, \psi_t(x)), \quad & \forall x \in \mathbb{R^d}, t\geq 0. \end{align} A flow is meant to satisfy other properties as well, namely being a bijection and forming a semigroup with respect to time. These properties however will not be assumed for this question, as they are part of what I want to prove. Finally, we claim that the flow, $\psi$ , allows us to write the following explicit formula for the solution, $a$ : $$ a(t,x) = e^{-\lambda t} a_0(\psi^{-1}_t (x)) + \int_0^t e^{-\lambda (t-\tau)} f(\tau, \psi_\tau (\psi^{-1}_t (x))) \text{d}\tau. $$ My Problem I have managed to derive the formula above for $a$ , by converting from Euclidean to Lagarangian coordinates (i.e. substituting $x \to \psi_t(x)$ ), which changes the material derivative into a simple time derivative: $$ \Big{(}\partial_t + v(t,x)\cdot\nabla \Big{)} a(t,x) = \frac{\text{d}}{\text{d}t} a(t,\psi_t(x)). $$ The problem is thus reduced to an ODE, which after solving gives us the formula above when we undo our substitution to return back to Euclidean coordinates. My issue is when I try to confirm the result in Euclidean coordinates. That is, I want to explicitly write out $$ \partial_t a(t,x) = \partial_t \Bigg{(}e^{-\lambda t} a_0(\psi^{-1}_t (x)) + \int_0^t e^{-\lambda (t-\tau)} f(\tau, \psi_\tau (\psi^{-1}_t (x))) \text{d}\tau \Bigg{)}, $$ etc, and get equality on both sides of the equation that way. To do this, I need all of the terms on the left hand side to cancel out nicely. In particular, it seems that I need the following identity for the time derivative of $\psi^{-1}$ : $$ \partial_t \Big{(} \psi^{-1}_t(x) \Big{)} = -v(t,x).$$ This seems natural enough, and I think I recall proving a result like this years ago, but I seem to be running into difficulty with it now. My Attempt Our first step is to see if we can write an explicit formula for $\psi_t(x)$ when $t$ is positive. I believe the following formula should work: $$ \psi_t(x) := x + \int^t_0 v(\tau, \psi_\tau (x)) \text{d}\tau, $$ as it satisfies the two properties laid out in the Background. We then need to try and derive an inverse function from this. We note that any inverse should have the property that $\psi^{-1}_t(\psi_t(x)) = x$ , for all $x\in\mathbb{R^d}$ . Then, by our above formula for $\psi_t$ : $$ x = \psi^{-1}_t(\psi_t(x)) = \psi_t(x) - \int^t_0 v(\tau, \psi_\tau (x)) \text{d}\tau. $$ Now we replace $\psi_t(x)$ with a generic Euclidean coordinate $y$ , to get $$ \psi^{-1}_t(y) = y - \int^t_0 v(\tau, \psi_\tau ( \psi^{-1}_t(y) )) \text{d}\tau. $$ Taking the time derivative and using Leibniz's rule, however, leaves us with a messy unwanted term: $$ \partial_t \psi^{-1}_t(y) = - v(t,y) - \int^t_0 \partial_t v(\tau, \psi_\tau ( \psi^{-1}_t(y) )) \text{d}\tau. $$ My question thus is whether there is a way to show that the messy integral above turns out to be $0,$ or if I've messed up somewhere Edit 01 Just as I was writing this question out, I realised that the property I'm looking for on $\psi$ is satisfied if we replace the $\tau$ in the integrand with $t$ . That is, if we set $$ \psi_t(x) := x + \int^t_0 v(\tau , \psi_t(x)) \text{d}\tau.$$ These two possible definitions of $\psi$ don't seem to be equivalent, unless perhaps $v(t,x)$ is constant wrt $x$ along flowlines. That is, $$ v(t,x) = v(t,x_0), \ \forall t\geq 0, \ x \in \{ x \in \mathbb{R^d} \ | \ x = \psi_\tau(x_0) , \ \text{some } \tau \geq 0 \}.$$ So now my question becomes: how do I settle on a definition of $\psi$ ? Are the properties I've listed so far insufficient to find a unique formula?","Background The problem is taken from section 2.5 of this paper , by Raphael Danchin. We are concerned with the following Transport Equation: Here, the initial term , the source term , the coeffictient , and the transport field are all given. We assume , for some Banach space , , and is in a nice enough set (depending on choice of ) for there to be a unique solution. In particular, is at least integrable-in-time with Lipschitz for all , which ensures the existence of a flow, , with respect to . That is, satisfies A flow is meant to satisfy other properties as well, namely being a bijection and forming a semigroup with respect to time. These properties however will not be assumed for this question, as they are part of what I want to prove. Finally, we claim that the flow, , allows us to write the following explicit formula for the solution, : My Problem I have managed to derive the formula above for , by converting from Euclidean to Lagarangian coordinates (i.e. substituting ), which changes the material derivative into a simple time derivative: The problem is thus reduced to an ODE, which after solving gives us the formula above when we undo our substitution to return back to Euclidean coordinates. My issue is when I try to confirm the result in Euclidean coordinates. That is, I want to explicitly write out etc, and get equality on both sides of the equation that way. To do this, I need all of the terms on the left hand side to cancel out nicely. In particular, it seems that I need the following identity for the time derivative of : This seems natural enough, and I think I recall proving a result like this years ago, but I seem to be running into difficulty with it now. My Attempt Our first step is to see if we can write an explicit formula for when is positive. I believe the following formula should work: as it satisfies the two properties laid out in the Background. We then need to try and derive an inverse function from this. We note that any inverse should have the property that , for all . Then, by our above formula for : Now we replace with a generic Euclidean coordinate , to get Taking the time derivative and using Leibniz's rule, however, leaves us with a messy unwanted term: My question thus is whether there is a way to show that the messy integral above turns out to be or if I've messed up somewhere Edit 01 Just as I was writing this question out, I realised that the property I'm looking for on is satisfied if we replace the in the integrand with . That is, if we set These two possible definitions of don't seem to be equivalent, unless perhaps is constant wrt along flowlines. That is, So now my question becomes: how do I settle on a definition of ? Are the properties I've listed so far insufficient to find a unique formula?","\begin{cases} \partial_t a + v\cdot\nabla a + \lambda a = f, \ \text{in } \mathbb{R^+} \times \mathbb{R^d}, \\
a|_{t=0} = a_0, \ \text{in } \mathbb{R^d}.
\end{cases} a_0 = a_0(x) f=f(t,x) \lambda \geq 0 v=v(t,x) a\in X X f \in L^1_{\text{loc}}(\mathbb{R^+} ; X) v X v v(t) t > 0 \psi v \psi \begin{align} \psi_0(x)  = x, \quad &  \forall x \in \mathbb{R^d} \\
\partial_t \psi_t(x)  = v(t, \psi_t(x)), \quad & \forall x \in \mathbb{R^d}, t\geq 0.
\end{align} \psi a  a(t,x) = e^{-\lambda t} a_0(\psi^{-1}_t (x)) + \int_0^t e^{-\lambda (t-\tau)} f(\tau, \psi_\tau (\psi^{-1}_t (x))) \text{d}\tau.  a x \to \psi_t(x)  \Big{(}\partial_t + v(t,x)\cdot\nabla \Big{)} a(t,x) = \frac{\text{d}}{\text{d}t} a(t,\psi_t(x)).   \partial_t a(t,x) = \partial_t \Bigg{(}e^{-\lambda t} a_0(\psi^{-1}_t (x)) + \int_0^t e^{-\lambda (t-\tau)} f(\tau, \psi_\tau (\psi^{-1}_t (x))) \text{d}\tau \Bigg{)},  \psi^{-1}  \partial_t \Big{(} \psi^{-1}_t(x) \Big{)} = -v(t,x). \psi_t(x) t  \psi_t(x) := x + \int^t_0 v(\tau, \psi_\tau (x)) \text{d}\tau,  \psi^{-1}_t(\psi_t(x)) = x x\in\mathbb{R^d} \psi_t  x = \psi^{-1}_t(\psi_t(x)) = \psi_t(x) - \int^t_0 v(\tau, \psi_\tau (x)) \text{d}\tau.  \psi_t(x) y  \psi^{-1}_t(y) = y - \int^t_0 v(\tau, \psi_\tau ( \psi^{-1}_t(y) )) \text{d}\tau.   \partial_t \psi^{-1}_t(y) = - v(t,y) - \int^t_0 \partial_t v(\tau, \psi_\tau ( \psi^{-1}_t(y) )) \text{d}\tau.  0, \psi \tau t  \psi_t(x) := x + \int^t_0 v(\tau , \psi_t(x)) \text{d}\tau. \psi v(t,x) x  v(t,x) = v(t,x_0), \ \forall t\geq 0, \ x \in \{ x \in \mathbb{R^d} \ | \ x = \psi_\tau(x_0) , \ \text{some } \tau \geq 0 \}. \psi","['real-analysis', 'integration', 'ordinary-differential-equations', 'derivatives', 'partial-differential-equations']"
43,"System of two ODEs, problem with finding eigenvectors","System of two ODEs, problem with finding eigenvectors",,"I am fairly new to systems of ODEs so bear with me. $$ \left\{  \begin{array}{c} x' = 2x-y \\  y' = x+2y \\  \end{array} \right.  $$ So, I know I have to find the eigenvalues, which is not a problem for me. The eigenvalues are $2 \pm i$ After inserting the first value in the matrix of the system and multiplying it by a column-matrix consisting of $C_1$ and $C_2$ , I get the system: $$ \left\{  \begin{array}{c} -iC_1 - C_2 = 0\\  C_1-iC_2=0 \\  \end{array} \right.  $$ Now, my question is as follows: If I express $C_2$ from the first equation I get $C_2 = -i C_1$ . After inserting that in the second equation I get that $C_1 - C_1 = 0$ , which leads me to think that $C_1$ is any real number. After choosing the number $1$ , I get that the eigenvector is $v_1 = (1,-i)$ However, WolframAlpha says that the eigenvector is $(i,1)$ , which leads me to think that I should have expressed $C_1$ from the second equation and substituted it into the first, which is when I get the same solution as in WolframAlpha. But does it matter what variable I express though? I'm also new to eigenvectors so I don't know a whole lot about them. Can anyone help?","I am fairly new to systems of ODEs so bear with me. So, I know I have to find the eigenvalues, which is not a problem for me. The eigenvalues are After inserting the first value in the matrix of the system and multiplying it by a column-matrix consisting of and , I get the system: Now, my question is as follows: If I express from the first equation I get . After inserting that in the second equation I get that , which leads me to think that is any real number. After choosing the number , I get that the eigenvector is However, WolframAlpha says that the eigenvector is , which leads me to think that I should have expressed from the second equation and substituted it into the first, which is when I get the same solution as in WolframAlpha. But does it matter what variable I express though? I'm also new to eigenvectors so I don't know a whole lot about them. Can anyone help?","
\left\{ 
\begin{array}{c}
x' = 2x-y \\ 
y' = x+2y \\ 
\end{array}
\right. 
 2 \pm i C_1 C_2 
\left\{ 
\begin{array}{c}
-iC_1 - C_2 = 0\\ 
C_1-iC_2=0 \\ 
\end{array}
\right. 
 C_2 C_2 = -i C_1 C_1 - C_1 = 0 C_1 1 v_1 = (1,-i) (i,1) C_1","['ordinary-differential-equations', 'systems-of-equations']"
44,Properties of the Solutions of $y' = y$,Properties of the Solutions of,y' = y,"In this section of my notes we take $E(x)$ as a solution of the initial value problem $y'=y , y(0)= 1 $ We show that $E(x+r)$ is also a solution to this problem and show $E(x+r)=E(x)E(r)$ . The notes then go on to prove that $E(x)>0$ . It is done as the following: Suppose $E(s) = 0$ for some $s  \in \mathbb{R}$ . Then, for arbitrary $x$ , we have $E(x + s) = E(x)E(s) = 0$ , so $E(x) ≡ 0$ . This contradicts $E(0) = 1$ . My question is, surely $E(x)$ doesn't have to be $0$ , surely it could be anything because for whatever value of $E(x)$ , since we've said $E(s)=0$ then $E(x)E(s)$ will always be $0$ no matter what the value of $E(x)$ . Also I'm a little confused why $E(0)=1$ . How do we know this? Thanks in advance!","In this section of my notes we take as a solution of the initial value problem We show that is also a solution to this problem and show . The notes then go on to prove that . It is done as the following: Suppose for some . Then, for arbitrary , we have , so . This contradicts . My question is, surely doesn't have to be , surely it could be anything because for whatever value of , since we've said then will always be no matter what the value of . Also I'm a little confused why . How do we know this? Thanks in advance!","E(x) y'=y , y(0)= 1  E(x+r) E(x+r)=E(x)E(r) E(x)>0 E(s) = 0 s  \in \mathbb{R} x E(x + s) = E(x)E(s) = 0 E(x) ≡ 0 E(0) = 1 E(x) 0 E(x) E(s)=0 E(x)E(s) 0 E(x) E(0)=1","['ordinary-differential-equations', 'initial-value-problems']"
45,Vector function going to zero faster than the norm of its variable uniformly in time,Vector function going to zero faster than the norm of its variable uniformly in time,,"I need to find some conditions on a function $h:\mathbb{R}^+\times \mathbb{R}^n\rightarrow\mathbb{R}^n$ to ensure that it goes, in norm, to zero faster than $\|z\|$ uniformly in time, i.e. $$ \lim_{\|z\|\to 0}\sup_{t\geq 0}\frac{\|h(t,z)\|}{\|z\|}=0.$$ I know that for any fixed time $t\in\mathbb{R}^+$ the limit goes to zero, but in general this convergence is not uniform in time. My function $h$ has even the property that $h(t,0)=0$ for any $t\in\mathbb{R}^+$ . Therefore the first property I thought for $h$ in order to satisfy this condition is that it is so that $\|h(t,x)\|\leq \|x\|^p$ where $p>1$ , which however does not give me relevant information about it. Do you suggest other possible conditions to ensure this kind of behaviour? If it might help, I need this condition in order to define the linearization of a non-autonomous dynamical system, where $h(t,z)$ is the nonlinear part of my ODE.","I need to find some conditions on a function to ensure that it goes, in norm, to zero faster than uniformly in time, i.e. I know that for any fixed time the limit goes to zero, but in general this convergence is not uniform in time. My function has even the property that for any . Therefore the first property I thought for in order to satisfy this condition is that it is so that where , which however does not give me relevant information about it. Do you suggest other possible conditions to ensure this kind of behaviour? If it might help, I need this condition in order to define the linearization of a non-autonomous dynamical system, where is the nonlinear part of my ODE.","h:\mathbb{R}^+\times \mathbb{R}^n\rightarrow\mathbb{R}^n \|z\|  \lim_{\|z\|\to 0}\sup_{t\geq 0}\frac{\|h(t,z)\|}{\|z\|}=0. t\in\mathbb{R}^+ h h(t,0)=0 t\in\mathbb{R}^+ h \|h(t,x)\|\leq \|x\|^p p>1 h(t,z)","['ordinary-differential-equations', 'limits', 'dynamical-systems', 'uniform-convergence']"
46,Travelling (soliton) Wave solution to 1D GPE equation,Travelling (soliton) Wave solution to 1D GPE equation,,"I have a non-dimensional version of the Gross-Pitaevskii equation: $$ 2i \frac{\partial \psi}{\partial t} = - \frac{\partial^2\psi}{\partial x^2} + \left|\psi \right|^2\psi -\psi, $$ where $\psi(x,t)$ is the wave-function for a BEC. My aim is to find a travelling wave solution. I suspect this solution is of the form: $$\psi(x,t) = f(x-ut)\,e^{-i\mu t}  ,$$ where $\mu, u$ are constants ( $u$ is the speed). From reading around online (specifically, the book ""A Primer on Quantum Fluids""), I think the final solution for $\psi(x,t)$ is of the general form: $$ \psi(x,t) = \tanh\left( (x-ct)\,+i\frac{u}{c} \right)\,e^{-i\mu t}. $$ I have tried to substitute my form of the solution into it to find $f$ , and so far I am at the current stage: $$ \frac{\partial^2f}{\partial x^2} - i\,2u\frac{\partial f}{\partial x} -f^3 +(1+2\mu)f = 0,$$ but after carrying on with this and applying some basic differential equation solver techniques, it doesn't seem to get me to the right place. It is also possible that I have made mistakes leading up to this point, and I'm not too sure whether that's the case or where to go from here. This particular solution is for dark solitons. I also need to find bright soliton solutions but this should proceed in a similar manner. Any help would be greatly appreciated!","I have a non-dimensional version of the Gross-Pitaevskii equation: where is the wave-function for a BEC. My aim is to find a travelling wave solution. I suspect this solution is of the form: where are constants ( is the speed). From reading around online (specifically, the book ""A Primer on Quantum Fluids""), I think the final solution for is of the general form: I have tried to substitute my form of the solution into it to find , and so far I am at the current stage: but after carrying on with this and applying some basic differential equation solver techniques, it doesn't seem to get me to the right place. It is also possible that I have made mistakes leading up to this point, and I'm not too sure whether that's the case or where to go from here. This particular solution is for dark solitons. I also need to find bright soliton solutions but this should proceed in a similar manner. Any help would be greatly appreciated!"," 2i \frac{\partial \psi}{\partial t} = - \frac{\partial^2\psi}{\partial x^2} + \left|\psi \right|^2\psi -\psi,  \psi(x,t) \psi(x,t) = f(x-ut)\,e^{-i\mu t}  , \mu, u u \psi(x,t)  \psi(x,t) = \tanh\left( (x-ct)\,+i\frac{u}{c} \right)\,e^{-i\mu t}.  f  \frac{\partial^2f}{\partial x^2} - i\,2u\frac{\partial f}{\partial x} -f^3 +(1+2\mu)f = 0,","['ordinary-differential-equations', 'partial-differential-equations', 'nonlinear-system', 'quantum-mechanics', 'wave-equation']"
47,First Order Differential Equation Problem - Calorie intake,First Order Differential Equation Problem - Calorie intake,,"UPDATE :  Thanks to Matthew for pointing out what to do next, this problem has been solved. Thanks everyone for your time and effort. I've been stuck on this DE problem for a few days now and was wondering if someone can point me in the right direction.  I know the solution is $101$ days. The daily calorie intake is taken to be a fixed quantity $C$ and is modelled by the DE. $$\frac{dt}{dm} = \frac{1}{a(C-bm)}$$ where $a$ and $b$ are both constants. A man is $90$ kg.  If he were to take no calories for $14$ days, his mass would reduce by $20$ %.  How long will it take him to reduce his mas by the same amount if he took $3/4$ of the calories required to keep his mass constant at $90$ kg? I'm missing something simple, but just can't figure it out. Thanks in advanced. So far I have the general solution to the DE as: $$\int dt = \int\frac{1}{a(C-bm)} dm$$ $$t + K = - \frac{1}{ab} ln|C-bm| $$ $$-tab - Kab = ln|C-bm| $$ $$|C-bm| = e^{-tab} e^{-Kab}$$ $$C-bm = Ae^{-tab}$$ where $$A=\pm e^{-Kab}$$ and $K$ is an arbitrary constant. Now, when $t=0$ $m=90$ kg, so $A=C-90b$ , therefore we now have $$C-bm = (C-90b)e^{-tab}$$ Transposing $$ bm= \frac{C-(C-90b)e^{-tab}}{b}$$ $$ m(t) = \frac{C+(90b-C)e^{-tab}}{b}$$ Next, $t=14$ , $m=72$ ( $20$ % reduction of $90$ ), $C=0$ $$72b = 90be^{-14ab}$$ $$\frac{72}{90} = e^{-14ab}$$ $$e^{14ab} = \frac{90}{72}$$ Thus $$ab=\frac{\ln\frac{90}{72}}{14}$$ $$ab=\frac{1}{14}\ln\Big(\frac{5}{4}\Big)$$ $$ab=0.0159$$ So substituting $ab$ back into the equation $$ m(t) = \frac{C+(90b-C)e^{-t0.0159}}{b}$$ So I have 3 unknows $C$ , $b$ and $t$ . I can seem to extract the required information to solve for $t$ . The next part is where I get stuck.  I don't know how to implement the $3/4$ of the calories required in my formula.","UPDATE :  Thanks to Matthew for pointing out what to do next, this problem has been solved. Thanks everyone for your time and effort. I've been stuck on this DE problem for a few days now and was wondering if someone can point me in the right direction.  I know the solution is days. The daily calorie intake is taken to be a fixed quantity and is modelled by the DE. where and are both constants. A man is kg.  If he were to take no calories for days, his mass would reduce by %.  How long will it take him to reduce his mas by the same amount if he took of the calories required to keep his mass constant at kg? I'm missing something simple, but just can't figure it out. Thanks in advanced. So far I have the general solution to the DE as: where and is an arbitrary constant. Now, when kg, so , therefore we now have Transposing Next, , ( % reduction of ), Thus So substituting back into the equation So I have 3 unknows , and . I can seem to extract the required information to solve for . The next part is where I get stuck.  I don't know how to implement the of the calories required in my formula.",101 C \frac{dt}{dm} = \frac{1}{a(C-bm)} a b 90 14 20 3/4 90 \int dt = \int\frac{1}{a(C-bm)} dm t + K = - \frac{1}{ab} ln|C-bm|  -tab - Kab = ln|C-bm|  |C-bm| = e^{-tab} e^{-Kab} C-bm = Ae^{-tab} A=\pm e^{-Kab} K t=0 m=90 A=C-90b C-bm = (C-90b)e^{-tab}  bm= \frac{C-(C-90b)e^{-tab}}{b}  m(t) = \frac{C+(90b-C)e^{-tab}}{b} t=14 m=72 20 90 C=0 72b = 90be^{-14ab} \frac{72}{90} = e^{-14ab} e^{14ab} = \frac{90}{72} ab=\frac{\ln\frac{90}{72}}{14} ab=\frac{1}{14}\ln\Big(\frac{5}{4}\Big) ab=0.0159 ab  m(t) = \frac{C+(90b-C)e^{-t0.0159}}{b} C b t t 3/4,"['calculus', 'ordinary-differential-equations']"
48,Help solving $2y'-4y\left(\frac{y''}{y'}\right)=\log\left(\frac{y''}{y'}\right)$,Help solving,2y'-4y\left(\frac{y''}{y'}\right)=\log\left(\frac{y''}{y'}\right),"I'm at the begginig of a differential equation course, and I'm having trouble finishing this problem. I'm asked to solve the equation $$2y'-4y\left(\frac{y''}{y'}\right)=\log\left(\frac{y''}{y'}\right). \ \ \ \text{(I)}$$ I'm used the variable change $p=y'$ , $\ \dot p=\frac{dp}{dy}$ , $ \ y''=\dot pp$ , I get the equation: $$p=2y\dot p+\frac{1}{2}\log \dot p. \ \ \ \text{(II)}$$ That last one is a Lagrange differential equation. I solved it finding a parametric solution using the variable change $\dot p = u$ , and using that I got the first order linear equation $$\frac{dy}{du}+\frac{2y}{u}=\frac{-1}{2u^2}.$$ Solving it by integrating factor, I finally found the parametric values for $y$ and $p$ , and they are $$ \left\{ \begin{array}{rcl}      y(u,C)&=&\frac{C}{u^2}-\frac{1}{2u}   \\ p(u,C)&=&\frac{2C}{u^2}-1+\frac{1}{2}\log u \end{array} \right. $$ This two equations are the solution to the equation (II), but I'm asked to solve the equation (I). Since $p=y'(x)$ in the original equation (I), I integrate the expression of $p$ with respect to $x$ , and I get $$y=\int\left(\frac{2C}{u^2}-1+\frac{1}{2}\log u\right)\ dx + K$$ $$\Downarrow$$ $$\boxed{y(x)=x\left(\frac{2C}{u^2}-1+\frac{1}{2}\log u\right) + K}$$ This may be my solution, but something feels wrong about it, since my original $y$ in equation (I) is some $y(x)$ , but what I get is a single equation $y(x,u)$ ( $C,K$ are constants), and I'm not sure if I'm forgetting another expression to make it have sense. I feel that the solution to (I) must be a parametric one, but I only get one equation for $y$ in function of $x$ and $u$ . Is the boxed equation the right solution? If not, where am I wrong? Any help will be appreciated.","I'm at the begginig of a differential equation course, and I'm having trouble finishing this problem. I'm asked to solve the equation I'm used the variable change , , , I get the equation: That last one is a Lagrange differential equation. I solved it finding a parametric solution using the variable change , and using that I got the first order linear equation Solving it by integrating factor, I finally found the parametric values for and , and they are This two equations are the solution to the equation (II), but I'm asked to solve the equation (I). Since in the original equation (I), I integrate the expression of with respect to , and I get This may be my solution, but something feels wrong about it, since my original in equation (I) is some , but what I get is a single equation ( are constants), and I'm not sure if I'm forgetting another expression to make it have sense. I feel that the solution to (I) must be a parametric one, but I only get one equation for in function of and . Is the boxed equation the right solution? If not, where am I wrong? Any help will be appreciated.","2y'-4y\left(\frac{y''}{y'}\right)=\log\left(\frac{y''}{y'}\right). \ \ \ \text{(I)} p=y' \ \dot p=\frac{dp}{dy}  \ y''=\dot pp p=2y\dot p+\frac{1}{2}\log \dot p. \ \ \ \text{(II)} \dot p = u \frac{dy}{du}+\frac{2y}{u}=\frac{-1}{2u^2}. y p 
\left\{
\begin{array}{rcl}
     y(u,C)&=&\frac{C}{u^2}-\frac{1}{2u}
  \\ p(u,C)&=&\frac{2C}{u^2}-1+\frac{1}{2}\log u
\end{array}
\right.
 p=y'(x) p x y=\int\left(\frac{2C}{u^2}-1+\frac{1}{2}\log u\right)\ dx + K \Downarrow \boxed{y(x)=x\left(\frac{2C}{u^2}-1+\frac{1}{2}\log u\right) + K} y y(x) y(x,u) C,K y x u","['ordinary-differential-equations', 'parametric']"
49,"Algorithm to solve $p(x)f''(x) + q(x)f(x) = 0$ where $p(x), q(x)$ are polynomials",Algorithm to solve  where  are polynomials,"p(x)f''(x) + q(x)f(x) = 0 p(x), q(x)","Consider the following ordinary differential equation. $$ p(x)f''(x) + q(x) f(x) = 0$$ Here, $p(x), q(x)$ are given polynomials in $x$ . For example, in my physics education, I am interested in solving the following. $$ 4x^2 f''(x) +(-15-8x^2-4x^4)f(x) = 0 $$ What are the general algorithms to solve these kind of differential equations? Any references are also very appreaciated. One of the ways that I know of would be assuming series solution and solve (recursively) for coefficients, but is there something else?","Consider the following ordinary differential equation. Here, are given polynomials in . For example, in my physics education, I am interested in solving the following. What are the general algorithms to solve these kind of differential equations? Any references are also very appreaciated. One of the ways that I know of would be assuming series solution and solve (recursively) for coefficients, but is there something else?"," p(x)f''(x) + q(x) f(x) = 0 p(x), q(x) x  4x^2 f''(x) +(-15-8x^2-4x^4)f(x) = 0 ",['ordinary-differential-equations']
50,Proving a certain nonlinear ODE has a solution on $t \geq 0$,Proving a certain nonlinear ODE has a solution on,t \geq 0,"I'm trying to prove that the following ODE admits a solution on the interval $[0, \infty )$ : $\begin{align*} \begin{cases} x_1^\prime &= x_2 - x_1^3 \\ x_2^\prime &= \frac{1}{2}x_1 - x_2 + d \sin t \end{cases} \end{align*}$ With initial condition $x(0) = x_0$ . The usual approaches seem to fail: the derivative isn't bounded on $\mathbb{R}^2$ and the function does not seem Lipschitz on all of that domain either. Using energy functions doesn't seem to help either. Any hints would be highly appreciated!",I'm trying to prove that the following ODE admits a solution on the interval : With initial condition . The usual approaches seem to fail: the derivative isn't bounded on and the function does not seem Lipschitz on all of that domain either. Using energy functions doesn't seem to help either. Any hints would be highly appreciated!,"[0, \infty ) \begin{align*}
\begin{cases}
x_1^\prime &= x_2 - x_1^3 \\
x_2^\prime &= \frac{1}{2}x_1 - x_2 + d \sin t
\end{cases}
\end{align*} x(0) = x_0 \mathbb{R}^2","['real-analysis', 'ordinary-differential-equations', 'nonlinear-system']"
51,Can a D.E. still be defined even if terms are unbounded?,Can a D.E. still be defined even if terms are unbounded?,,"$$(1 - x^2)y'' - 2xy' + l(l + 1)y = 0, -1 \leq x \leq 1$$ I want to go ahead and use the Sturm-Liouville theorem to prove that this equation's eigenstates are orthogonal, but it's not a given that $y'$ is bounded at $a$ or $b$ , only that the D.E. is defined over $[-1,1]$ . Is that sufficient?","I want to go ahead and use the Sturm-Liouville theorem to prove that this equation's eigenstates are orthogonal, but it's not a given that is bounded at or , only that the D.E. is defined over . Is that sufficient?","(1 - x^2)y'' - 2xy' + l(l + 1)y = 0, -1 \leq x \leq 1 y' a b [-1,1]","['ordinary-differential-equations', 'sturm-liouville']"
52,Textbook advice- Dynamical Systems and Differential Equations,Textbook advice- Dynamical Systems and Differential Equations,,"I am currently an undergrad math major taking a gap year (because my university is entirely online this semester).  For this year, I have signed up for a ""Directed Reading Program"" with a graduate student whose specialty involves dynamical systems.  For this program, I am supposed to read through a textbook that we can discuss.  Two of her suggestions were Nonlinear Dynamics and Chaos by Steven H. Strogatz and Differential Equations, Dynamical Systems, and an Introduction to Chaos by Hirsch, Smale, and Devaney. As I took a look at those books, I realized an additional reason why reading a book such as those could be useful: although I took a differential equations course at my local community college when I was in high school, I don't remember them all that well.  My university's math department is very theory-oriented, so I may never have the opportunity to take a DiffEQ course as an undergrad, though, as a math major who may want to go into something more applied, I feel as though a high level of comfort with differential equations would be nice to have.  Looking through the two textbooks online, neither appears to cover Laplace transforms, which I remember to have been an entire unit in my community-college course.  Because of this, I am having doubts about the efficacy of the two books with respect to giving me said comfort.  However, the books seem to be fantastic with respect to gaining a deeper understanding of the material, so I am not trying to criticize. Two questions: Between Strogatz and Hirsch/Smale/Devaney, which would you recommend? In light of the above (the lack of coverage of topics such as Laplace transforms), do you think I ought to, in addition to one of those two books, spend time with Ordinary Differential Equations by Tenenbaum and Pollard (which I got for Christmas or something awhile back but haven't spent much time with)?","I am currently an undergrad math major taking a gap year (because my university is entirely online this semester).  For this year, I have signed up for a ""Directed Reading Program"" with a graduate student whose specialty involves dynamical systems.  For this program, I am supposed to read through a textbook that we can discuss.  Two of her suggestions were Nonlinear Dynamics and Chaos by Steven H. Strogatz and Differential Equations, Dynamical Systems, and an Introduction to Chaos by Hirsch, Smale, and Devaney. As I took a look at those books, I realized an additional reason why reading a book such as those could be useful: although I took a differential equations course at my local community college when I was in high school, I don't remember them all that well.  My university's math department is very theory-oriented, so I may never have the opportunity to take a DiffEQ course as an undergrad, though, as a math major who may want to go into something more applied, I feel as though a high level of comfort with differential equations would be nice to have.  Looking through the two textbooks online, neither appears to cover Laplace transforms, which I remember to have been an entire unit in my community-college course.  Because of this, I am having doubts about the efficacy of the two books with respect to giving me said comfort.  However, the books seem to be fantastic with respect to gaining a deeper understanding of the material, so I am not trying to criticize. Two questions: Between Strogatz and Hirsch/Smale/Devaney, which would you recommend? In light of the above (the lack of coverage of topics such as Laplace transforms), do you think I ought to, in addition to one of those two books, spend time with Ordinary Differential Equations by Tenenbaum and Pollard (which I got for Christmas or something awhile back but haven't spent much time with)?",,"['ordinary-differential-equations', 'reference-request', 'dynamical-systems']"
53,"Interesting change of variable for a ""simple"" ODE","Interesting change of variable for a ""simple"" ODE",,"I'm working on a dynamical system governed by the following ODE in $\mathbb{R}^n$ : $$ \dot{x}(t) = f(A(t)x(t)+b). $$ Here $t\rightarrow A(t)\in\mathbb{R}^{n\times n}$ is a smooth (enough) matrix valued function and $b\in\mathbb{R}^n$ and $f:\mathbb{R}^n\rightarrow\mathbb{R}^n$ is smooth (enough) too. So in general the dynamics is just given by the composition between a non-linear transformation and an affine one. My question is if there is some famous/interesting change of variable which allows to rewrite this system in some useful way, i.e. to Hamiltonian system or gradient vector field or something more manageable. You can even tell me that something is a good choice when $A$ satisfies some particular property, it would still be very helpful. I see it is a quite general question and I am not saying what are my aims but, at the moment, I just want to see if in this setting there is some ""standard"" way to proceed. The easiest transformation, i.e. $z(t)=A(t)x(t)+b$ , does not seem to bring anywhere: $$ \dot{z}(t) = \dot{A}(t)A^{-1}(t)(z(t)-b)+A(t)f(z(t)),$$ assuming the invertibility of $A$ .","I'm working on a dynamical system governed by the following ODE in : Here is a smooth (enough) matrix valued function and and is smooth (enough) too. So in general the dynamics is just given by the composition between a non-linear transformation and an affine one. My question is if there is some famous/interesting change of variable which allows to rewrite this system in some useful way, i.e. to Hamiltonian system or gradient vector field or something more manageable. You can even tell me that something is a good choice when satisfies some particular property, it would still be very helpful. I see it is a quite general question and I am not saying what are my aims but, at the moment, I just want to see if in this setting there is some ""standard"" way to proceed. The easiest transformation, i.e. , does not seem to bring anywhere: assuming the invertibility of .","\mathbb{R}^n  \dot{x}(t) = f(A(t)x(t)+b).  t\rightarrow A(t)\in\mathbb{R}^{n\times n} b\in\mathbb{R}^n f:\mathbb{R}^n\rightarrow\mathbb{R}^n A z(t)=A(t)x(t)+b  \dot{z}(t) = \dot{A}(t)A^{-1}(t)(z(t)-b)+A(t)f(z(t)), A","['linear-algebra', 'ordinary-differential-equations', 'dynamical-systems', 'change-of-variable']"
54,"Trying to solve: $y'' + 2y' = \frac{(y')^2}{y + 1} + \frac{y'}{x} \ln(\frac{y+1}{y'})$, $y(1) = 1$, $y'(1) = 2/e$.","Trying to solve: , , .",y'' + 2y' = \frac{(y')^2}{y + 1} + \frac{y'}{x} \ln(\frac{y+1}{y'}) y(1) = 1 y'(1) = 2/e,"The problem is the following:  Solve $$y'' + 2y' = \frac{(y')^2}{(y + 1)} + \frac{y'}{x} \ln\left(\frac{y+1}{y'}\right),$$ given that $y(1) = 1$ , $y'(1) = \frac{2}{e}$ . My solution: After noting that $$\left(\ln\left(\frac{y+1}{y'}\right)\right)' = \frac{y'}{y + 1} - \frac{y''}{y'}$$ we divide the original equation by $y'$ , rearrange and get $$\frac{y'}{y + 1} - \frac{y''}{y'} + \frac{1}{x}\ln\left(\frac{y + 1}{y'}\right) = 2$$ Then we substitute $t(x) = \ln\left(\frac{y + 1}{y'}\right)$ . Now we have $$t' + \frac{t}{x} = 2$$ Particular solution to this equation is $t = x$ and for $t' + \frac{t}{x} = 0$ we have $t = \frac{C}{x}$ . So finally we have $t = x + \frac{C}{x}$ . Now we note that $t(1) = \ln\left(\frac{e(1 + 1)}{2}\right) = 1$ and conclude that $t = x$ and then solve for $y$ . For $y$ we have $$\frac{y + 1}{y'} = e^x$$ Solution for this equation is $\ln|y + 1| = -e^{-x} + C$ . After some manipulations we get $$y = \frac{C}{e^{e^{-x}}} - 1$$ and following the initial condition $y(1) = 1$ we get $C = 2e^{e^{-1}}$ . So the answer will be $$y = 2e^{e^{-1} - e^{-x}}$$ I have 2 questions: $\qquad 1)$ Is my solution correct? The answer looks terrible, so maybe I had an error somewhere. $\qquad 2)$ Is there any easier solution? I thought about finding $y''(1)$ , multiplying by $x/y'$ , taking derivative to remove $\ln$ -function and solve the resulting equation, but it seemed to be much harder than what I did in the solution above.","The problem is the following:  Solve given that , . My solution: After noting that we divide the original equation by , rearrange and get Then we substitute . Now we have Particular solution to this equation is and for we have . So finally we have . Now we note that and conclude that and then solve for . For we have Solution for this equation is . After some manipulations we get and following the initial condition we get . So the answer will be I have 2 questions: Is my solution correct? The answer looks terrible, so maybe I had an error somewhere. Is there any easier solution? I thought about finding , multiplying by , taking derivative to remove -function and solve the resulting equation, but it seemed to be much harder than what I did in the solution above.","y'' + 2y' = \frac{(y')^2}{(y + 1)} + \frac{y'}{x} \ln\left(\frac{y+1}{y'}\right), y(1) = 1 y'(1) = \frac{2}{e} \left(\ln\left(\frac{y+1}{y'}\right)\right)' = \frac{y'}{y + 1} - \frac{y''}{y'} y' \frac{y'}{y + 1} - \frac{y''}{y'} + \frac{1}{x}\ln\left(\frac{y + 1}{y'}\right) = 2 t(x) = \ln\left(\frac{y + 1}{y'}\right) t' + \frac{t}{x} = 2 t = x t' + \frac{t}{x} = 0 t = \frac{C}{x} t = x + \frac{C}{x} t(1) = \ln\left(\frac{e(1 + 1)}{2}\right) = 1 t = x y y \frac{y + 1}{y'} = e^x \ln|y + 1| = -e^{-x} + C y = \frac{C}{e^{e^{-x}}} - 1 y(1) = 1 C = 2e^{e^{-1}} y = 2e^{e^{-1} - e^{-x}} \qquad 1) \qquad 2) y''(1) x/y' \ln","['ordinary-differential-equations', 'solution-verification']"
55,Periodic orbits enclosing fixed points in a differential equation,Periodic orbits enclosing fixed points in a differential equation,,"Suppose we have a differential equation $\mathbf{\dot{x}}=\mathbf{f(x)}$ for $\mathbf{x}\in \Bbb{R}^2$ . Suppose further that there are three fixed points, of which one is a saddle and two are sinks. I am not sure how to determine examples of the following scenarios or to prove they do not exist: There exists a periodic orbit enclosing precisely one sink. There exists a periodic orbit enclosing all three fixed points. The index test does not rule out either of these possibilities and I am unsure how to construct examples demonstrating existence. Any help would be much appreciated!","Suppose we have a differential equation for . Suppose further that there are three fixed points, of which one is a saddle and two are sinks. I am not sure how to determine examples of the following scenarios or to prove they do not exist: There exists a periodic orbit enclosing precisely one sink. There exists a periodic orbit enclosing all three fixed points. The index test does not rule out either of these possibilities and I am unsure how to construct examples demonstrating existence. Any help would be much appreciated!",\mathbf{\dot{x}}=\mathbf{f(x)} \mathbf{x}\in \Bbb{R}^2,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes']"
56,How to show that Airy Function in Integral form obeys $y''=xy$?,How to show that Airy Function in Integral form obeys ?,y''=xy,"I wanna show that having Airy function defined as: $$ \mathrm {Ai} (x)={\dfrac {1}{\pi }}\int _{0}^{\infty }\cos \left({\dfrac {t^{3}}{3}}+xt\right)dt $$ Solves equation: $$y''=xy.$$ Edit: After clearing out that $k^3$ is not $x^3$ , which i have misread: I am stuck at the integral: $$  \int_0^\infty (k^2+x)\cos(kx+k^3) $$ which is clearly not converging but some how wiki says it right solution?","I wanna show that having Airy function defined as: Solves equation: Edit: After clearing out that is not , which i have misread: I am stuck at the integral: which is clearly not converging but some how wiki says it right solution?", \mathrm {Ai} (x)={\dfrac {1}{\pi }}\int _{0}^{\infty }\cos \left({\dfrac {t^{3}}{3}}+xt\right)dt  y''=xy. k^3 x^3   \int_0^\infty (k^2+x)\cos(kx+k^3) ,"['ordinary-differential-equations', 'improper-integrals', 'airy-functions']"
57,"Wronskian, Linear Dependence and Construction of ODE","Wronskian, Linear Dependence and Construction of ODE",,"I already understand that for second-order, linear, homogeneous ODE, if the Wronskian of two solutions to the ODE ( or functions that satisfy the ODE) is zero at a certain point, the functions are linearly dependent. At another place (source at bottom), it has been written that if Wronskian is zero at a certain point, it is not necessary that they are linearly dependent. As an example, for f(x) = x, g(x) = sin(x), we find W(f, g) = x cos(x) − sin(x) which is nonzero, for example, at x = π. Hence, x and sin(x) are Linearly Independent. Note that W(f, g) may be zero at some point such as x = 0. The only way I can resolve these two statements is by saying that you can't construct a second-order, linear, homogeneous ODE which has these two functions, x and sin(x) as it's solutions (i.e. they satisfy the ODE). I don't know how to formally prove this, except that I observe that sin(x) already is the solution of second-order, linear, homogeneous ODE, of which x is not a solution, so adding it might make a third-order or say non-linear ODE. So, how do we actually resolve these statements and can we prove that we can't construct an ODE as required above? EDIT: This question was solved earlier, but I have a doubt in the solution by mathcounterexamples.net now: why can we not change the coefficient of y'' to the function in the denominator (xcos(x) - sin(x)) ? Source: https://home.iitk.ac.in/~sghorai/TEACHING/MTH203/ode7.pdf or https://drive.google.com/file/d/1OGRE00YNB0kjVHam9ZSpsDQfba0PxkYG/view?usp=sharing Relevant statement: Theorem 3: Two solutions $y_1,y_2$ of $$ y'' + p(x)y' + q(x)y = 0, \quad x \in \mathcal I $$ are linearly dependent iff $W(y_1,y_2) = 0$ at a certain point $x_0 \in \mathcal I$ .","I already understand that for second-order, linear, homogeneous ODE, if the Wronskian of two solutions to the ODE ( or functions that satisfy the ODE) is zero at a certain point, the functions are linearly dependent. At another place (source at bottom), it has been written that if Wronskian is zero at a certain point, it is not necessary that they are linearly dependent. As an example, for f(x) = x, g(x) = sin(x), we find W(f, g) = x cos(x) − sin(x) which is nonzero, for example, at x = π. Hence, x and sin(x) are Linearly Independent. Note that W(f, g) may be zero at some point such as x = 0. The only way I can resolve these two statements is by saying that you can't construct a second-order, linear, homogeneous ODE which has these two functions, x and sin(x) as it's solutions (i.e. they satisfy the ODE). I don't know how to formally prove this, except that I observe that sin(x) already is the solution of second-order, linear, homogeneous ODE, of which x is not a solution, so adding it might make a third-order or say non-linear ODE. So, how do we actually resolve these statements and can we prove that we can't construct an ODE as required above? EDIT: This question was solved earlier, but I have a doubt in the solution by mathcounterexamples.net now: why can we not change the coefficient of y'' to the function in the denominator (xcos(x) - sin(x)) ? Source: https://home.iitk.ac.in/~sghorai/TEACHING/MTH203/ode7.pdf or https://drive.google.com/file/d/1OGRE00YNB0kjVHam9ZSpsDQfba0PxkYG/view?usp=sharing Relevant statement: Theorem 3: Two solutions of are linearly dependent iff at a certain point .","y_1,y_2 
y'' + p(x)y' + q(x)y = 0, \quad x \in \mathcal I
 W(y_1,y_2) = 0 x_0 \in \mathcal I","['linear-algebra', 'ordinary-differential-equations', 'wronskian']"
58,Show that if $x(t)$ satisfies $\frac{dx}{dt} = x^2-x^6$ and $x(0) >0$ then $\lim_{t\to\infty} x(t) = 1.$,Show that if  satisfies  and  then,x(t) \frac{dx}{dt} = x^2-x^6 x(0) >0 \lim_{t\to\infty} x(t) = 1.,"Show that if $x(t)$ satisfies $\frac{dx}{dt} = x^2-x^6$ and $x(0)  = c >0$ then $\lim_{t\to\infty} x(t) = 1.$ (Taken from ""Berkeley problems in mathematics"") Here's my attempt: The function $f(t,x) = x^2-x^6$ is continuous everywhere and locally Lipschitz. By Picard's theorem there exists an $\epsilon>0$ such that there is a unique solution $x(t)$ on $[- \epsilon, \epsilon].$ By differentiability of $x(t)$ with $x'(0) \neq 0$ ( assuming $x$ is non-constant) and as $x(0)>0$ the inverse function theorem applies and there is a local continuous inverse $x^{-1}(y)$ satisfying $$ \frac{dt}{dx} = \frac{1}{x^2-x^6} \implies x^{-1}(y) = \int^{y}_{c}\frac{d \xi}{\xi^2-\xi^6}. $$ As $\frac{1}{\xi ^2- \xi^6}$ ""blows up at $\xi=1$ rapidly enough"" we have $x^{-1}(1) = \infty$ and by continuity of $x$ , $ \lim_{t \to \infty}x(t) = 1$ . $\blacksquare$ Is this correct? Many thanks! EDIT: In response to enzotib: Assume $0 < c <1$ . The integral $x^{-1}(y)$ is continuous and non-zero on $(c,1)$ . Clearly $x^{-1}(c) = 0, $ and as shown $x^{-1}(1) = + \infty$ . By IVT, $ x^{-1} : (c,1) \to [0, +\infty)$ . Applying the inverse function theorem a second time we see that the unique solution on $[0,  \epsilon]$ can be extended to a unique solution on $[0, \infty)$ . Taking should now be justified. The case $c >1$ is much the same and the case $c =1$ forces $x(t) =1$ which solves the problem trivially.","Show that if satisfies and then (Taken from ""Berkeley problems in mathematics"") Here's my attempt: The function is continuous everywhere and locally Lipschitz. By Picard's theorem there exists an such that there is a unique solution on By differentiability of with ( assuming is non-constant) and as the inverse function theorem applies and there is a local continuous inverse satisfying As ""blows up at rapidly enough"" we have and by continuity of , . Is this correct? Many thanks! EDIT: In response to enzotib: Assume . The integral is continuous and non-zero on . Clearly and as shown . By IVT, . Applying the inverse function theorem a second time we see that the unique solution on can be extended to a unique solution on . Taking should now be justified. The case is much the same and the case forces which solves the problem trivially.","x(t) \frac{dx}{dt} = x^2-x^6 x(0)  = c >0 \lim_{t\to\infty} x(t) = 1. f(t,x) = x^2-x^6 \epsilon>0 x(t) [- \epsilon, \epsilon]. x(t) x'(0) \neq 0 x x(0)>0 x^{-1}(y) 
\frac{dt}{dx} = \frac{1}{x^2-x^6} \implies x^{-1}(y) = \int^{y}_{c}\frac{d \xi}{\xi^2-\xi^6}.
 \frac{1}{\xi ^2- \xi^6} \xi=1 x^{-1}(1) = \infty x  \lim_{t \to \infty}x(t) = 1 \blacksquare 0 < c <1 x^{-1}(y) (c,1) x^{-1}(c) = 0,  x^{-1}(1) = + \infty  x^{-1} : (c,1) \to [0, +\infty) [0,  \epsilon] [0, \infty) c >1 c =1 x(t) =1","['real-analysis', 'calculus']"
59,Critical Points of $dy/dx=0.2x^2\left(1-x/3\right)$,Critical Points of,dy/dx=0.2x^2\left(1-x/3\right),"I am trying to determine the critical points of the ODE $$\frac{dy}{dx}=0.2x^2\left(1-\frac{x}{3}\right).$$ Setting the right-hand-side to zero gives two solutions, namely $x=0$ and $x=3$ . I was wondering if the $x^2$ means that there's two critical points at $x=0$ . That is, there are two critical points at $x=0$ and one critical point at $x=3$ .","I am trying to determine the critical points of the ODE Setting the right-hand-side to zero gives two solutions, namely and . I was wondering if the means that there's two critical points at . That is, there are two critical points at and one critical point at .",\frac{dy}{dx}=0.2x^2\left(1-\frac{x}{3}\right). x=0 x=3 x^2 x=0 x=0 x=3,['ordinary-differential-equations']
60,Is there a linear dynamical system on the sphere with a two-point attractor?,Is there a linear dynamical system on the sphere with a two-point attractor?,,"Let $N=(1,0,0)$ denote the North Pole of $\mathbb S^2$ . Therefore, $-N$ denotes the South Pole. Question . Is there a skew-symmetric $M\in\mathbb R^{3\times 3}$ such that, letting $u(t,P)\in\mathbb S^2$ denote the solution to $$\frac{\mathrm{d}u}{\mathrm{d}t}= Mu,\qquad u(0)=P,$$ for an arbitrary $P\in\mathbb S^2$ , it holds that either $u(t,P)\to N$ or $u(t,P)\to -N$ as $t\to \infty$ ? The matrix $M$ must satisfy the skew-symmetry $M^T=-M$ in order for $u(t, P)$ to remain on $\mathbb S^2$ for all $t>0$ . In particular, the trace of $M$ must vanish. My intuition is that such a dynamical system cannot exist. Indeed, since $\operatorname*{tr}(M)=0$ , the dynamical system $u(t, \cdot)$ should be ""area-preserving"", in some sense. And this shouldn't be compatible with the requirement that, asymptotically, everything collapses to a couple of points, which is a set with zero area. However, if my intuition is wrong, I would be very happy to see an example.","Let denote the North Pole of . Therefore, denotes the South Pole. Question . Is there a skew-symmetric such that, letting denote the solution to for an arbitrary , it holds that either or as ? The matrix must satisfy the skew-symmetry in order for to remain on for all . In particular, the trace of must vanish. My intuition is that such a dynamical system cannot exist. Indeed, since , the dynamical system should be ""area-preserving"", in some sense. And this shouldn't be compatible with the requirement that, asymptotically, everything collapses to a couple of points, which is a set with zero area. However, if my intuition is wrong, I would be very happy to see an example.","N=(1,0,0) \mathbb S^2 -N M\in\mathbb R^{3\times 3} u(t,P)\in\mathbb S^2 \frac{\mathrm{d}u}{\mathrm{d}t}= Mu,\qquad u(0)=P, P\in\mathbb S^2 u(t,P)\to N u(t,P)\to -N t\to \infty M M^T=-M u(t, P) \mathbb S^2 t>0 M \operatorname*{tr}(M)=0 u(t, \cdot)","['linear-algebra', 'ordinary-differential-equations', 'dynamical-systems', 'spheres']"
61,Function that transforms a differential equation,Function that transforms a differential equation,,"What function $u$ can transform the equation $$fy''-4f'y'+gy=0$$ to an equation of the form $$v''+kv=0$$ . Here, $f,g,y,k,u,v$ are all functions of $x$ and $y=uv$ . Is there any theory for such type of transformations? By simple calculation, it is seen that $v''=fy''-4f'y'$ . But how to proceed further? Any hints? Thanks beforehand.","What function can transform the equation to an equation of the form . Here, are all functions of and . Is there any theory for such type of transformations? By simple calculation, it is seen that . But how to proceed further? Any hints? Thanks beforehand.","u fy''-4f'y'+gy=0 v''+kv=0 f,g,y,k,u,v x y=uv v''=fy''-4f'y'","['real-analysis', 'calculus', 'ordinary-differential-equations']"
62,Understanding the Poisson bracket for this system in $so(4)$,Understanding the Poisson bracket for this system in,so(4),"I have the following system of ODEs: \begin{align*}     \dot{x}_1 &= x_2x_3A_{32} + x_5 x_6 A_{65}\\     \dot{x}_2 &= x_1 x_3 A_{13} + x_4 x_6 A_{46}\\     \dot{x}_3 &= x_1 x_2 A_{21} + x_4 x_5 A_{54}\\     \dot{x}_4 &= x_3x_5 A_{35} + x_2 x_6 A_{62}\\     \dot{x}_5 &= x_3 x_4 A_{43} + x_1 x_6 A_{16}\\     \dot{x}_6 &= x_2 x_4 A_{24} + x_1 x_5 A_{51} \end{align*} where $A_{ij} = \frac{1}{I_i} - \frac{1}{I_j}$ . Questions: What is the Poisson bracket associated with the above system? If it is as I describe, then why is $\{H,H\} \neq 0$ ? Am I doing something wrong? My Work: For a smaller system: $$ \begin{cases} \dot{x}_{1} &= \alpha_{1} x_2 x_3,\\ \dot{x}_{2} &= \alpha_{2} x_3 x_1,\\ \dot{x}_{3} &= \alpha_{3} x_1 x_2 \end{cases} $$ where $\alpha_i = \frac{1}{I_k} - \frac{1}{I_j}$ s are some physical constants. I had been using the following definition $$ \{\mathbf{x},\mathbf{y\}} = (\nabla f(\mathbf{x}))^{T} \omega(\mathbf{x}) \nabla \mathbf{y} $$ where $\omega$ is a $n \times n$ matrix called the Poisson matrix. For this small system $$\omega(x) = \begin{pmatrix}0 & -x_{3} & x_{2} \\ x_{3} & 0 & -x_{1} \\ -x_{2} & x_{1} & 0  \end{pmatrix} \in so(3)$$ Using this definition we can confirm that the function $H = \frac{1}{2}\frac{x_1^2}{I_1} + \frac{1}{2}\frac{x_2^2}{I_2} + \frac{1}{2}\frac{x_3^2}{I_3}$ is the Hamiltonian of this system. This is done by recovering the system from the Poisson bracket, e.g, $$\dot{x}_1 = \{x_1, H\}$$ . There are two ways I done this computation. The first way keeping everything as matrices and taking the trace at the end: $(-,-) = tr(x^{T}\omega y)$ BUT I DONT KNOW WHY THIS WORKS. Why the trace? The basis for $so(3)$ are the three matrices $e_1 = \begin{pmatrix} 0 & 0 & 0\\ 0 & 0 & -1\\ 0 & 1 & 0 \end{pmatrix}$ $e_{2} = \begin{pmatrix}     0 & 0 & 1 \\ 0 & 0 & 0 \\ -1 & 0 & 0     \end{pmatrix}$ $e_{3} =  \begin{pmatrix}     0 & -1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 0     \end{pmatrix}$ Then by definition $\nabla x_1 = \sum_{i=1}^{3} \frac{\partial x_1}{x_i} \cdot e_i = e_1 = \begin{pmatrix}     0 & 0  & 0 \\ 0 & 0 & -1 \\ 0 & 1 & 0     \end{pmatrix}$ . Similarly, $\nabla H = \begin{pmatrix}     0 & -\frac{x_3}{I_3} & \frac{x_2}{I_2}\\     \frac{x_3}{I_3} & 0 & -\frac{x_1}{I_1}\\     -\frac{x_2}{I_2} & \frac{x_1}{I_1} & 0     \end{pmatrix}$ And so I then do the computation \begin{align*}     \{x_1, H\} &= \underbrace{\begin{pmatrix}    0 & 0 & 0\\    0 & 0 & 1\\    0 & -1 & 0    \end{pmatrix}}_{(\Delta_x f)^T} \underbrace{\begin{pmatrix}0 & -x_{3} & x_{2} \\ x_{3} & 0 & -x_{1} \\ -x_{2} & x_{1} & 0  \end{pmatrix}}_{\omega(x)} \underbrace{\begin{pmatrix}     0 & -\frac{x_3}{I_3} & \frac{x_2}{I_2}\\     \frac{x_3}{I_3} & 0 & -\frac{x_1}{I_1}\\     -\frac{x_2}{I_2} & \frac{x_1}{I_1} & 0     \end{pmatrix}}_{\Delta_x H}\\     &= \begin{pmatrix}     0 & 0 & 0 \\     -x_2 & x_2 & 0\\     -x_3 & 0 & x_1     \end{pmatrix}\begin{pmatrix}     0 & -\frac{x_3}{I_3} & \frac{x_2}{I_2} \\     \frac{x_3}{I_3} & 0 & -\frac{x_1}{I_1}\\     -\frac{x_2}{I_2} & \frac{x_1}{I_1} & 0     \end{pmatrix}\\     &= \underbrace{\begin{pmatrix}     0 & 0 & 0 \\     \frac{x_1 x_3}{I_3} & \frac{x_2 x_3}{I_3} & -\frac{x_2^2}{I_2} - \frac{x_1^2}{I_1} \\     -\frac{x_1x_2}{I_2} & \frac{x_3^2}{I_3} + \frac{x_1^2}{I_1} & -\frac{x_2 x_3}{I_2}     \end{pmatrix}}_{A} \end{align*} Taking the trace of the matrix above yields: \begin{align*}     tr(A) &= \frac{x_2x_3}{I_3} - \frac{x_2x_3}{I_2}\\     &= (\frac{1}{I_3} - \frac{1}{I_2})x_2 x_3 \end{align*} which is exactly the first equation of our little system! We can also recover $\dot{x}_2$ and $\dot{x}_3$ . But, again, I don't really know why we have to take the trace other than it works. The function $f = x_1^2 + x_2^2 + x_3^2$ is also a conserved quantity and is verified by showing $\{f,H\} = 0$ . So $f$ and $H$ are in involution. The second way to do the computation is a little different. I compute with: $(-,-) = (\nabla x)^T B (\nabla y)$ where $(x), (y)$ are written in terms of the basis and $B$ is made up of $b_{ij} = \{e_i, e_j\}$ . Turns out that $B = \omega$ here. Then $(\nabla x) = \begin{pmatrix}1 \\ 0 \\ 0\end{pmatrix}$ and $(\nabla y) = \begin{pmatrix}\frac{x_1}{I_1} \\ \frac{x_2}{I_2} \\ \frac{x_3}{I_3} \end{pmatrix}$ . So then doing the computation $$\begin{pmatrix}1; 0; 0\end{pmatrix} B \begin{pmatrix}\frac{x_1}{I_1} \\ \frac{x_2}{I_2} \\ \frac{x_3}{I_3} \end{pmatrix}$$ yields what we desire: $x_2x_3\alpha_1$ . So we see these two computations are equivalent. But why are they equivalent? The trace seems kinda arbitrary to me. Now, for the big system I have found three suspected conserved quantities. One of them being the Hamiltonian $H = \frac{1}{2}\frac{x_1^2}{I_1} + \frac{1}{2}\frac{x_2^2}{I_2} + \frac{1}{2}\frac{x_3^2}{I_3} + \frac{1}{2}\frac{x_4^2}{I_4} + \frac{1}{2}\frac{x_5^2}{I_5} + \frac{1}{2}\frac{x_6^2}{I_6} $ . I won't write the computations here, but I confirmed that we recover the big system. That is, I get $$\dot{x}_1 = \{x_1, H\}$$ where $\omega \in so(4)$ . HOWEVER , I have seen that $$\{H,H\} \neq 0$$ This is bad since it should be zero. I do not understand why it is not zero since I did the calculation showing we recover the system so that means the system is Hamiltonian doesn't it? Questions: What is the Poisson bracket associated with the big system? If it is as I describe, then why is $\{H,H\} \neq 0$ ? Am I doing something wrong?","I have the following system of ODEs: where . Questions: What is the Poisson bracket associated with the above system? If it is as I describe, then why is ? Am I doing something wrong? My Work: For a smaller system: where s are some physical constants. I had been using the following definition where is a matrix called the Poisson matrix. For this small system Using this definition we can confirm that the function is the Hamiltonian of this system. This is done by recovering the system from the Poisson bracket, e.g, . There are two ways I done this computation. The first way keeping everything as matrices and taking the trace at the end: BUT I DONT KNOW WHY THIS WORKS. Why the trace? The basis for are the three matrices Then by definition . Similarly, And so I then do the computation Taking the trace of the matrix above yields: which is exactly the first equation of our little system! We can also recover and . But, again, I don't really know why we have to take the trace other than it works. The function is also a conserved quantity and is verified by showing . So and are in involution. The second way to do the computation is a little different. I compute with: where are written in terms of the basis and is made up of . Turns out that here. Then and . So then doing the computation yields what we desire: . So we see these two computations are equivalent. But why are they equivalent? The trace seems kinda arbitrary to me. Now, for the big system I have found three suspected conserved quantities. One of them being the Hamiltonian . I won't write the computations here, but I confirmed that we recover the big system. That is, I get where . HOWEVER , I have seen that This is bad since it should be zero. I do not understand why it is not zero since I did the calculation showing we recover the system so that means the system is Hamiltonian doesn't it? Questions: What is the Poisson bracket associated with the big system? If it is as I describe, then why is ? Am I doing something wrong?","\begin{align*}
    \dot{x}_1 &= x_2x_3A_{32} + x_5 x_6 A_{65}\\
    \dot{x}_2 &= x_1 x_3 A_{13} + x_4 x_6 A_{46}\\
    \dot{x}_3 &= x_1 x_2 A_{21} + x_4 x_5 A_{54}\\
    \dot{x}_4 &= x_3x_5 A_{35} + x_2 x_6 A_{62}\\
    \dot{x}_5 &= x_3 x_4 A_{43} + x_1 x_6 A_{16}\\
    \dot{x}_6 &= x_2 x_4 A_{24} + x_1 x_5 A_{51}
\end{align*} A_{ij} = \frac{1}{I_i} - \frac{1}{I_j} \{H,H\} \neq 0 
\begin{cases}
\dot{x}_{1} &= \alpha_{1} x_2 x_3,\\
\dot{x}_{2} &= \alpha_{2} x_3 x_1,\\
\dot{x}_{3} &= \alpha_{3} x_1 x_2
\end{cases}
 \alpha_i = \frac{1}{I_k} - \frac{1}{I_j} 
\{\mathbf{x},\mathbf{y\}} = (\nabla f(\mathbf{x}))^{T} \omega(\mathbf{x}) \nabla \mathbf{y}
 \omega n \times n \omega(x) = \begin{pmatrix}0 & -x_{3} & x_{2} \\ x_{3} & 0 & -x_{1} \\ -x_{2} & x_{1} & 0  \end{pmatrix} \in so(3) H = \frac{1}{2}\frac{x_1^2}{I_1} + \frac{1}{2}\frac{x_2^2}{I_2} + \frac{1}{2}\frac{x_3^2}{I_3} \dot{x}_1 = \{x_1, H\} (-,-) = tr(x^{T}\omega y) so(3) e_1 = \begin{pmatrix}
0 & 0 & 0\\
0 & 0 & -1\\
0 & 1 & 0
\end{pmatrix} e_{2} = \begin{pmatrix}
    0 & 0 & 1 \\ 0 & 0 & 0 \\ -1 & 0 & 0
    \end{pmatrix} e_{3} =  \begin{pmatrix}
    0 & -1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 0
    \end{pmatrix} \nabla x_1 = \sum_{i=1}^{3} \frac{\partial x_1}{x_i} \cdot e_i = e_1 = \begin{pmatrix}
    0 & 0  & 0 \\ 0 & 0 & -1 \\ 0 & 1 & 0
    \end{pmatrix} \nabla H = \begin{pmatrix}
    0 & -\frac{x_3}{I_3} & \frac{x_2}{I_2}\\
    \frac{x_3}{I_3} & 0 & -\frac{x_1}{I_1}\\
    -\frac{x_2}{I_2} & \frac{x_1}{I_1} & 0
    \end{pmatrix} \begin{align*}
    \{x_1, H\} &= \underbrace{\begin{pmatrix}
   0 & 0 & 0\\
   0 & 0 & 1\\
   0 & -1 & 0
   \end{pmatrix}}_{(\Delta_x f)^T} \underbrace{\begin{pmatrix}0 & -x_{3} & x_{2} \\ x_{3} & 0 & -x_{1} \\ -x_{2} & x_{1} & 0  \end{pmatrix}}_{\omega(x)} \underbrace{\begin{pmatrix}
    0 & -\frac{x_3}{I_3} & \frac{x_2}{I_2}\\
    \frac{x_3}{I_3} & 0 & -\frac{x_1}{I_1}\\
    -\frac{x_2}{I_2} & \frac{x_1}{I_1} & 0
    \end{pmatrix}}_{\Delta_x H}\\
    &= \begin{pmatrix}
    0 & 0 & 0 \\
    -x_2 & x_2 & 0\\
    -x_3 & 0 & x_1
    \end{pmatrix}\begin{pmatrix}
    0 & -\frac{x_3}{I_3} & \frac{x_2}{I_2} \\
    \frac{x_3}{I_3} & 0 & -\frac{x_1}{I_1}\\
    -\frac{x_2}{I_2} & \frac{x_1}{I_1} & 0
    \end{pmatrix}\\
    &= \underbrace{\begin{pmatrix}
    0 & 0 & 0 \\
    \frac{x_1 x_3}{I_3} & \frac{x_2 x_3}{I_3} & -\frac{x_2^2}{I_2} - \frac{x_1^2}{I_1} \\
    -\frac{x_1x_2}{I_2} & \frac{x_3^2}{I_3} + \frac{x_1^2}{I_1} & -\frac{x_2 x_3}{I_2}
    \end{pmatrix}}_{A}
\end{align*} \begin{align*}
    tr(A) &= \frac{x_2x_3}{I_3} - \frac{x_2x_3}{I_2}\\
    &= (\frac{1}{I_3} - \frac{1}{I_2})x_2 x_3
\end{align*} \dot{x}_2 \dot{x}_3 f = x_1^2 + x_2^2 + x_3^2 \{f,H\} = 0 f H (-,-) = (\nabla x)^T B (\nabla y) (x), (y) B b_{ij} = \{e_i, e_j\} B = \omega (\nabla x) = \begin{pmatrix}1 \\ 0 \\ 0\end{pmatrix} (\nabla y) = \begin{pmatrix}\frac{x_1}{I_1} \\ \frac{x_2}{I_2} \\ \frac{x_3}{I_3} \end{pmatrix} \begin{pmatrix}1; 0; 0\end{pmatrix} B \begin{pmatrix}\frac{x_1}{I_1} \\ \frac{x_2}{I_2} \\ \frac{x_3}{I_3} \end{pmatrix} x_2x_3\alpha_1 H = \frac{1}{2}\frac{x_1^2}{I_1} + \frac{1}{2}\frac{x_2^2}{I_2} + \frac{1}{2}\frac{x_3^2}{I_3} + \frac{1}{2}\frac{x_4^2}{I_4} + \frac{1}{2}\frac{x_5^2}{I_5} + \frac{1}{2}\frac{x_6^2}{I_6}
 \dot{x}_1 = \{x_1, H\} \omega \in so(4) \{H,H\} \neq 0 \{H,H\} \neq 0","['ordinary-differential-equations', 'classical-mechanics']"
63,2nd order differential equation with non-constant coefficients,2nd order differential equation with non-constant coefficients,,"Consider the second order differential equation $$y''-x^2y=0 $$ where $y$ itself is a function of $x$ . I do not know how to solve this equation. I tried a series expansion and failed, and because the coefficients are not constant, I can not use the characteristic equation to solve it either. Hence, here I am, looking for any hints on how to solve this equation for $y$ . I know there are tons of questions already out there concerning second order differential equations looking like this one, and I looked through just about every one of them, however all the solutions provided seem to be very situational for the given DE, and I have yet to find a general method that I can use to solve the above. I though about reducing the order of the equation. Thanks!","Consider the second order differential equation where itself is a function of . I do not know how to solve this equation. I tried a series expansion and failed, and because the coefficients are not constant, I can not use the characteristic equation to solve it either. Hence, here I am, looking for any hints on how to solve this equation for . I know there are tons of questions already out there concerning second order differential equations looking like this one, and I looked through just about every one of them, however all the solutions provided seem to be very situational for the given DE, and I have yet to find a general method that I can use to solve the above. I though about reducing the order of the equation. Thanks!","y''-x^2y=0
 y x y",[]
64,Solve $xy\frac{dy}{dx} = \sqrt{x^2 - y^2 - x^2y^2 - 1}$,Solve,xy\frac{dy}{dx} = \sqrt{x^2 - y^2 - x^2y^2 - 1},"Can anyone please help me solving the below $$ xy\frac{dy}{dx} = \sqrt{x^2 - y^2 - x^2y^2 - 1} $$ If there would have been + instead of -, It would have been easier. I tried 2 ways as mentioned below : 1. Putting $x^2 = u, y^2 = v$ 2. Squaring both sides.Then, dividing both sides by $x^2y^2$ But didn't able to go much further. Any hint would be appreciated. Thanks in advance.","Can anyone please help me solving the below If there would have been + instead of -, It would have been easier. I tried 2 ways as mentioned below : 1. Putting 2. Squaring both sides.Then, dividing both sides by But didn't able to go much further. Any hint would be appreciated. Thanks in advance.","
xy\frac{dy}{dx} = \sqrt{x^2 - y^2 - x^2y^2 - 1}
 x^2 = u, y^2 = v x^2y^2",['ordinary-differential-equations']
65,Second order linear ODE with polynomial coefficients,Second order linear ODE with polynomial coefficients,,"I am currently stuck at solving an ODE of the form \begin{equation} 0=\psi''(x)+(\varepsilon-(\alpha x^2-\beta)^2)\psi(x) \end{equation} where $\alpha,\beta$ and $\varepsilon$ are real parameters. Anticipating $\psi\sim\mathrm{e}^{-\alpha x^3/3}$ for large $x$ , I made the Ansatz $\psi(x) = f(x)\mathrm{e}^{-\alpha x^3/3}$ and obtain the ODE \begin{equation} 0=f''(x)-2\alpha x^2 f'(x)+(\varepsilon-\beta^2+2\alpha\beta x^2-2\alpha x)f(x). \end{equation} Actually, I am looking for a normalized solution $\sim\mathrm{e}^{-\alpha|x|^3/3}$ , where extra care needs to be taken around $x=0$ . (I am a physics student and not so much concerned about mathematical rigor right now.) But any solution would be a first step of course. I tried a variety of things to solve the second equation: A powerseries approach $f(x)=\sum_{n=0}^\infty a_n x^n$ . I obtain \begin{equation}a_{n+2}=\frac{(\varepsilon-\beta^2)a_n-2\alpha(n+1)a_{n-1}+2\alpha\beta a_{n-2}}{(n+2)(n+1)}\end{equation} for $n\geq 2$ , which does not look very promising. I also checked A. Polyanins and V.F. Zaitsevs Handbook of Exact Solutions of Ordinary Differential Equations but failed to transform the above ODEs into one given in the book. Wolfram Mathematica was not helpful either. I transformed the equation into an system of first order linear ODEs with $g=f'$ . However such an approach is only helpful for constant coefficients, right? I am thankful for any ideas or hints. A particular solution would be very helpful of course, as it would allow me to reduce the order of the ODE and find all solutions. Note that, while $\alpha$ and $\beta$ are given, I expect constraints on $\varepsilon$ for (normalized) solutions to exist, i.e. I would also appreciate a solution for special values of $\varepsilon$ , e.g. $\varepsilon=\beta^2$ . Sorry for any grammatical errors. Thank you for your help!","I am currently stuck at solving an ODE of the form where and are real parameters. Anticipating for large , I made the Ansatz and obtain the ODE Actually, I am looking for a normalized solution , where extra care needs to be taken around . (I am a physics student and not so much concerned about mathematical rigor right now.) But any solution would be a first step of course. I tried a variety of things to solve the second equation: A powerseries approach . I obtain for , which does not look very promising. I also checked A. Polyanins and V.F. Zaitsevs Handbook of Exact Solutions of Ordinary Differential Equations but failed to transform the above ODEs into one given in the book. Wolfram Mathematica was not helpful either. I transformed the equation into an system of first order linear ODEs with . However such an approach is only helpful for constant coefficients, right? I am thankful for any ideas or hints. A particular solution would be very helpful of course, as it would allow me to reduce the order of the ODE and find all solutions. Note that, while and are given, I expect constraints on for (normalized) solutions to exist, i.e. I would also appreciate a solution for special values of , e.g. . Sorry for any grammatical errors. Thank you for your help!","\begin{equation}
0=\psi''(x)+(\varepsilon-(\alpha x^2-\beta)^2)\psi(x)
\end{equation} \alpha,\beta \varepsilon \psi\sim\mathrm{e}^{-\alpha x^3/3} x \psi(x) = f(x)\mathrm{e}^{-\alpha x^3/3} \begin{equation}
0=f''(x)-2\alpha x^2 f'(x)+(\varepsilon-\beta^2+2\alpha\beta x^2-2\alpha x)f(x).
\end{equation} \sim\mathrm{e}^{-\alpha|x|^3/3} x=0 f(x)=\sum_{n=0}^\infty a_n x^n \begin{equation}a_{n+2}=\frac{(\varepsilon-\beta^2)a_n-2\alpha(n+1)a_{n-1}+2\alpha\beta a_{n-2}}{(n+2)(n+1)}\end{equation} n\geq 2 g=f' \alpha \beta \varepsilon \varepsilon \varepsilon=\beta^2","['ordinary-differential-equations', 'physics', 'fundamental-solution']"
66,What allows us to use the Heaviside operator like a variable?,What allows us to use the Heaviside operator like a variable?,,"We were taught to use the Heaviside operator $D: \dfrac{d}{dx}$ to solve an ODE, for example, Consider $y'' + 3y' +2y = e^{-2x}$ $$\implies (D^2 + 3D + 2)y = e^{-2x}$$ $$\implies y = \dfrac{1}{D^2 + 3D + 2} e^{-2x}$$ $$\implies y = \dfrac{1}{(D+1)(D+2)} e^{-2x}$$ Now we substitute $-2$ in place of $D$ , in this case the denominator becomes zero, so we differentiate the denominator with respect to $D$ as if it's a variable and multiply $^*$ a factor $x$ . $$ y = \dfrac{x}{(D+1)+(D+2)} e^{-2x}$$ And then do the substitution $$\implies y = -xe^{-2x}$$ How is this possible, how are we able to treat an operator $\dfrac{d}{dx}$ like a variable? What does differentiate with respect to $D$ even mean? $*:$ I also don't understand why we multiply $x$ in the numerator Reference: https://www.youtube.com/playlist?list=PLEC88901EBADDD980","We were taught to use the Heaviside operator to solve an ODE, for example, Consider Now we substitute in place of , in this case the denominator becomes zero, so we differentiate the denominator with respect to as if it's a variable and multiply a factor . And then do the substitution How is this possible, how are we able to treat an operator like a variable? What does differentiate with respect to even mean? I also don't understand why we multiply in the numerator Reference: https://www.youtube.com/playlist?list=PLEC88901EBADDD980",D: \dfrac{d}{dx} y'' + 3y' +2y = e^{-2x} \implies (D^2 + 3D + 2)y = e^{-2x} \implies y = \dfrac{1}{D^2 + 3D + 2} e^{-2x} \implies y = \dfrac{1}{(D+1)(D+2)} e^{-2x} -2 D D ^* x  y = \dfrac{x}{(D+1)+(D+2)} e^{-2x} \implies y = -xe^{-2x} \dfrac{d}{dx} D *: x,"['calculus', 'ordinary-differential-equations', 'derivatives', 'partial-differential-equations', 'operator-theory']"
67,How Are Generalized Eigenvalues used in Differential Equations?,How Are Generalized Eigenvalues used in Differential Equations?,,"I understand the process for how Eigenvalues are involved in Differential Equations.  If you have Differential System of Equations like this $$ \vec{x}' = \begin{pmatrix} 2 & 1 \\ 0 & 1 \end{pmatrix}\vec{x} $$ The solution to that System of Differential Equations is a Linear Combination of e to the power of the eigenvalues times the corresponding eigenvectors. $$ \vec{x} = C_1e^{2t}\begin{pmatrix} 1 \\ 0 \end{pmatrix} + C_2e^t\begin{pmatrix} -1 \\ 1 \end{pmatrix} $$ However, what I am struggling with figuring out how Generalized Eigenvalues translate to the solutions in Differential Equations.  If you take this System of Differential Equations $$ \vec{x}' = \begin{pmatrix} 1 & 0 & 0 \\ 3 & 1 & 0 \\ 6 & 3 & 1 \end{pmatrix}\vec{x} $$ The solution to this Differential Equations is $$ \vec{x} = C_1e^t\begin{pmatrix} 1 \\ 3t \\ 6t + \frac{9}{2}t^2 \end{pmatrix} + C_2e^t\begin{pmatrix} 0 \\ 1 \\ 3t \end{pmatrix} + C_3e^t\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} $$ But the eigenvectors of this matrix (including the generalized eigenvectors) are $$ \vec{v}_1 = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix},\:\: \vec{w}_1 = \begin{pmatrix} 0 \\ 1 \\ 3 \end{pmatrix},\:\: \vec{w}_2 =  \begin{pmatrix} 1 \\ 1 \\ 3 \end{pmatrix} $$ These eigenvectors do share some similarities to the solution to the System of Equations.  However, the $$ 6t + \frac{9}{2}t^2$$ term in the solution is one I have no idea how generalized eigenvectors relate.  May someone please explain how these eigenvectors translate into Differential Equations?","I understand the process for how Eigenvalues are involved in Differential Equations.  If you have Differential System of Equations like this The solution to that System of Differential Equations is a Linear Combination of e to the power of the eigenvalues times the corresponding eigenvectors. However, what I am struggling with figuring out how Generalized Eigenvalues translate to the solutions in Differential Equations.  If you take this System of Differential Equations The solution to this Differential Equations is But the eigenvectors of this matrix (including the generalized eigenvectors) are These eigenvectors do share some similarities to the solution to the System of Equations.  However, the term in the solution is one I have no idea how generalized eigenvectors relate.  May someone please explain how these eigenvectors translate into Differential Equations?"," \vec{x}' = \begin{pmatrix} 2 & 1 \\ 0 & 1 \end{pmatrix}\vec{x}   \vec{x} = C_1e^{2t}\begin{pmatrix} 1 \\ 0 \end{pmatrix} + C_2e^t\begin{pmatrix} -1 \\ 1 \end{pmatrix}   \vec{x}' = \begin{pmatrix} 1 & 0 & 0 \\ 3 & 1 & 0 \\ 6 & 3 & 1 \end{pmatrix}\vec{x}   \vec{x} = C_1e^t\begin{pmatrix} 1 \\ 3t \\ 6t + \frac{9}{2}t^2 \end{pmatrix} + C_2e^t\begin{pmatrix} 0 \\ 1 \\ 3t \end{pmatrix} + C_3e^t\begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}   \vec{v}_1 = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix},\:\: \vec{w}_1 = \begin{pmatrix} 0 \\ 1 \\ 3 \end{pmatrix},\:\: \vec{w}_2 =  \begin{pmatrix} 1 \\ 1 \\ 3 \end{pmatrix}   6t + \frac{9}{2}t^2","['ordinary-differential-equations', 'eigenvalues-eigenvectors']"
68,Justification for Uniqueness of Solutions to Dispersive PDE,Justification for Uniqueness of Solutions to Dispersive PDE,,"For the sake of concreteness, we consider the linear Schrodinger equation $$ \partial_t u = i\Delta u, \ \ \ \ u(0, x) = u_0(x). $$ The solution is typically (at least, how I've seen it)  obtained by taking the Fourier transform of both sides, giving $\widehat{\partial_t u}(t, \xi) = -i|\xi|^2 \hat{u}(t,  \xi)$ . The next step is where I have questions. Assuming that everything is nice enough (for instance, in Tao's book, he assumes $u_0$ is Schwartz), dominated convergence gives $\widehat{\partial_t u}(t, \xi) =  \partial_t \hat{u}(t, \xi)$ , and then we get an ODE that solves to $$ \hat{u}(t, \xi) =e^{-i|\xi|^2}\hat{u}_0(\xi) \implies u(t, x) = e^{it\Delta}u_0(x). $$ This is then referred to as "" the solution to the Schrodinger equation, with initial data $u_0$ ."" My question : How do we know that there are no other solutions, that may not satisfy the right decay/smoothness criteria to  justify pulling the Fourier transform into the time derivative of $u$ ? I agree that there are no other solutions $u$ that are ""nice enough"" to justify this. But how do we rule out the existence of solutions $u$ such that $\partial_t \hat{u} \neq \widehat{\partial_t u}$ ? Any help is much appreciated!","For the sake of concreteness, we consider the linear Schrodinger equation The solution is typically (at least, how I've seen it)  obtained by taking the Fourier transform of both sides, giving . The next step is where I have questions. Assuming that everything is nice enough (for instance, in Tao's book, he assumes is Schwartz), dominated convergence gives , and then we get an ODE that solves to This is then referred to as "" the solution to the Schrodinger equation, with initial data ."" My question : How do we know that there are no other solutions, that may not satisfy the right decay/smoothness criteria to  justify pulling the Fourier transform into the time derivative of ? I agree that there are no other solutions that are ""nice enough"" to justify this. But how do we rule out the existence of solutions such that ? Any help is much appreciated!","
\partial_t u = i\Delta u, \ \ \ \ u(0, x) = u_0(x).
 \widehat{\partial_t u}(t, \xi) = -i|\xi|^2 \hat{u}(t,  \xi) u_0 \widehat{\partial_t u}(t, \xi) =  \partial_t \hat{u}(t, \xi) 
\hat{u}(t, \xi) =e^{-i|\xi|^2}\hat{u}_0(\xi) \implies u(t, x) = e^{it\Delta}u_0(x).
 u_0 u u u \partial_t \hat{u} \neq \widehat{\partial_t u}","['ordinary-differential-equations', 'analysis', 'partial-differential-equations', 'dispersive-pde']"
69,Second order homogeneous variable coefficient ODE (periodic perturbation),Second order homogeneous variable coefficient ODE (periodic perturbation),,"I have trouble solving the following ODE: $$y''(t)+[\alpha -i\beta \cos(\omega t)]y'(t)-\alpha^2 y(t)=0.$$ where $$\alpha\gg\beta, \alpha\gg\omega$$ I was wondering is there any approach to get an approximate solution to this ODE.  I have tried the exploratory solution like this： $$y(x)=Y(x)*e^{-i\frac{\beta}{\omega}\sin\omega t}$$ , but it's not work.","I have trouble solving the following ODE: where I was wondering is there any approach to get an approximate solution to this ODE.  I have tried the exploratory solution like this： , but it's not work.","y''(t)+[\alpha -i\beta \cos(\omega t)]y'(t)-\alpha^2 y(t)=0. \alpha\gg\beta, \alpha\gg\omega y(x)=Y(x)*e^{-i\frac{\beta}{\omega}\sin\omega t}","['ordinary-differential-equations', 'periodic-functions', 'perturbation-theory']"
70,Modeling with differential equations,Modeling with differential equations,,"I am struggling a bit to convert a word/modeling problem into a differential equation to then solve. The question is: Assume a town has a population of 100,000 citizens, within a week 10,000 people are  mysteriously ill. Assume that the rate of increase of the number who are ill is proportional to the number of people who have not yet fallen ill. How long will it be until half the town have fallen ill? I think it should end up being something like ds/dt=r(s-100000) where s is the number of people that have fallen sick and r is some rate for it to be proportional to.","I am struggling a bit to convert a word/modeling problem into a differential equation to then solve. The question is: Assume a town has a population of 100,000 citizens, within a week 10,000 people are  mysteriously ill. Assume that the rate of increase of the number who are ill is proportional to the number of people who have not yet fallen ill. How long will it be until half the town have fallen ill? I think it should end up being something like ds/dt=r(s-100000) where s is the number of people that have fallen sick and r is some rate for it to be proportional to.",,['ordinary-differential-equations']
71,Second order ODE with variable coefficients.,Second order ODE with variable coefficients.,,"Consider the following ODE: \begin{equation} (\cos x)y''-2(\sin x)y'-(\cos x)y=e^x \end{equation} The above equation is a second order linear ODE. However, I noticed that it doesn't have constant coefficients, so I cannot ""guess"" the solution is of the form $e^{\lambda x}$ . Im very confused by this, since I have never solved a 2nd order ODE with variable coefficients. The first part of the question says: (a) Show that the ODE is of the form: \begin{equation} \frac{d^2}{dx^2}(f(x)y)=e^x \end{equation} by finding the function f. (b) Hence, find the general soluion of he differential equation. For part (a) I have have no idea where to begin, but I tink that If i knew how to complete part (a) I would be able to find the general solution since I hae done things like that many times. Any help (part a especially) would be much appreciated. Thanks in advance.","Consider the following ODE: The above equation is a second order linear ODE. However, I noticed that it doesn't have constant coefficients, so I cannot ""guess"" the solution is of the form . Im very confused by this, since I have never solved a 2nd order ODE with variable coefficients. The first part of the question says: (a) Show that the ODE is of the form: by finding the function f. (b) Hence, find the general soluion of he differential equation. For part (a) I have have no idea where to begin, but I tink that If i knew how to complete part (a) I would be able to find the general solution since I hae done things like that many times. Any help (part a especially) would be much appreciated. Thanks in advance.","\begin{equation}
(\cos x)y''-2(\sin x)y'-(\cos x)y=e^x
\end{equation} e^{\lambda x} \begin{equation}
\frac{d^2}{dx^2}(f(x)y)=e^x
\end{equation}",['ordinary-differential-equations']
72,Method of Cauchy for particular solution of second order inhomogeneous differential equation,Method of Cauchy for particular solution of second order inhomogeneous differential equation,,"Consider $$ - u''(t) + p(t) u'(t) + q(t) u(t) = f(t), \quad t \in (a,b). $$ For $s \in (a,b)$ find coefficient functions $c_1$ and $c_2$ from the general solution of the homogenous equation $$ u_{\text{hom}}(t) = c_1 u_1(t) + c_2(t) $$ such that $u_{\text{hom}}(s) = 0$ and $u'_{\text{hom}}(s) = - f(s)$ .   Then $$ u_p(t) := \int_{t_0}^{t} c_1(s) u_1(t) + c_2(s) u_2(t) $$ is a particular solution of the inhomogeneous equation. And now I am supposed to find a particular solution to $$ t^2(1 - t) u''(t) + 2t (2 - t) u'(t) + 2(1 + t) u(t) = \frac{1}{t - 1}, \quad t \in (0,1). $$ I have found the solution for the homogeneous equation to be $$ u_{\text{hom}}(t) = c_1 t^{-2} + c_2 \cdot \frac{t^2 - 3t + 3}{3t} $$ But the instructions confuse me in particular because in one equation $c_i$ seem to be constants and the other functions and I con't know how they are connected or if that is the case at all. Can somebody please give me a hint on how to continue? Also, in the first equation of the yellow block, the highest derivate doesn't have a coefficient function, but since $p$ and $q$ are not relevant to the instructions that follow I don't have to divide my equation by $t^2(t - 1)$ , right? So taking the hints from the answer below I got: So my ""Wronski-Matrix"" is $$\begin{pmatrix} t^{-2} & \frac{t^2 -3t + 3}{3t} \\ -2 t^{-3} & \frac{1}{3} - t^{-2}\end{pmatrix},$$ solving I get $$C_1'(s) = \frac{t^3(t^2 - 3t + 3)}{(t - 3)^2(t - 1)}$$ and $$C_2'(t) = \frac{3t^2}{(1 - t)(t - 3)^2}.$$ Integrating those gives horrible terms , so what have I done wrong?","Consider For find coefficient functions and from the general solution of the homogenous equation such that and .   Then is a particular solution of the inhomogeneous equation. And now I am supposed to find a particular solution to I have found the solution for the homogeneous equation to be But the instructions confuse me in particular because in one equation seem to be constants and the other functions and I con't know how they are connected or if that is the case at all. Can somebody please give me a hint on how to continue? Also, in the first equation of the yellow block, the highest derivate doesn't have a coefficient function, but since and are not relevant to the instructions that follow I don't have to divide my equation by , right? So taking the hints from the answer below I got: So my ""Wronski-Matrix"" is solving I get and Integrating those gives horrible terms , so what have I done wrong?","
- u''(t) + p(t) u'(t) + q(t) u(t) = f(t), \quad t \in (a,b).
 s \in (a,b) c_1 c_2 
u_{\text{hom}}(t) = c_1 u_1(t) + c_2(t)
 u_{\text{hom}}(s) = 0 u'_{\text{hom}}(s) = - f(s) 
u_p(t)
:= \int_{t_0}^{t} c_1(s) u_1(t) + c_2(s) u_2(t)
 
t^2(1 - t) u''(t) + 2t (2 - t) u'(t) + 2(1 + t) u(t) = \frac{1}{t - 1}, \quad t \in (0,1).
 
u_{\text{hom}}(t) = c_1 t^{-2} + c_2 \cdot \frac{t^2 - 3t + 3}{3t}
 c_i p q t^2(t - 1) \begin{pmatrix} t^{-2} & \frac{t^2 -3t + 3}{3t} \\ -2 t^{-3} & \frac{1}{3} - t^{-2}\end{pmatrix}, C_1'(s) = \frac{t^3(t^2 - 3t + 3)}{(t - 3)^2(t - 1)} C_2'(t) = \frac{3t^2}{(1 - t)(t - 3)^2}.",['ordinary-differential-equations']
73,Solving a Non linear ODE,Solving a Non linear ODE,,"Can the following differential equation be solved analytically by some method $$   v(v’’’ + 3v’’ +52v’ + 50v) + 4(v’’’ + 3 v’’ + 52v’ + 100v) = 0,\\ v(0) = v’(0) = v’’(0) = 1. $$ The solutions to each linear DE in the parenthesis is known. On observation one can see that for the first DE the roots of characterstic equation are $-1, -1\pm7i$ . And the roots to the other are $-2,-0.5\pm i\sqrt{49.75}$ . I have tried solving it numerically on matlab and the solution looks like a sinusoidal wave squeezed between 2 exponentially decaying functions. And the solution decays to zero. Presently, I am trying if functions of the following form might be able to fit into the solution $$ v(x) = \alpha - \sqrt{\alpha^2 + (a_1 e^{-x}+ a_2e^{-2x})+(b_1e^{-x}+b_2e^{-2x})(A\cos\delta x + B\sin\delta x)} $$","Can the following differential equation be solved analytically by some method The solutions to each linear DE in the parenthesis is known. On observation one can see that for the first DE the roots of characterstic equation are . And the roots to the other are . I have tried solving it numerically on matlab and the solution looks like a sinusoidal wave squeezed between 2 exponentially decaying functions. And the solution decays to zero. Presently, I am trying if functions of the following form might be able to fit into the solution"," 
 v(v’’’ + 3v’’ +52v’ + 50v) + 4(v’’’ + 3 v’’ + 52v’ + 100v) = 0,\\
v(0) = v’(0) = v’’(0) = 1.
 -1, -1\pm7i -2,-0.5\pm i\sqrt{49.75} 
v(x) = \alpha - \sqrt{\alpha^2 + (a_1 e^{-x}+ a_2e^{-2x})+(b_1e^{-x}+b_2e^{-2x})(A\cos\delta x + B\sin\delta x)}
",['ordinary-differential-equations']
74,Extension of a vector field $X:A\subset \mathbb{R}^2 \to \mathbb{R}^2 $,Extension of a vector field,X:A\subset \mathbb{R}^2 \to \mathbb{R}^2 ,"Define $A=\{(x,y)\in \mathbb{R}^2;\ x\leq 0\ \text{or }  y \leq x^2 \}$ ( a representation of $A$ can be found here .) Let $X:A\to \mathbb{R}^2$ be a smooth function such that $X(\partial A) = (1,0)$ and $\tau:\mathbb{R}\to \mathbb{R}$ a diffeomorphism satisfying $\tau'>0$ and $\tau(0) = 0$ . My Question: Is it possible to extend $X$ to a fuction $\tilde{X}:\mathbb{R}^2\to \mathbb{R}^2$ , such that the orbit of $\tilde{X}$ startig at the point $(0,y)$ (with $y>0$ ) also passes through the point $(\tau(y),\tau(y)^2)$ . Just for the records, I am searching for a smooth planar vector field $\tilde{X}$ , such that $\left. \tilde{X}\right|_{A} = X$ and $\forall$ $y>0$ the solution $\varphi_y$ of the ODE \begin{align*}\dot{x} &= \tilde {X}(x) \\ x(0) &= (0,y).\\ \end{align*} satisfy the condition, $\exists$ $t_y \in \mathbb{R}$ such that $\varphi_y(t_y) = \left(\tau(y),\tau(y)^2 \right)$ . If the result is true just in the neighborhood of the origin it is good enough for me. I don't have any ideas on how to tackle this problem. Can anyone help?","Define ( a representation of can be found here .) Let be a smooth function such that and a diffeomorphism satisfying and . My Question: Is it possible to extend to a fuction , such that the orbit of startig at the point (with ) also passes through the point . Just for the records, I am searching for a smooth planar vector field , such that and the solution of the ODE satisfy the condition, such that . If the result is true just in the neighborhood of the origin it is good enough for me. I don't have any ideas on how to tackle this problem. Can anyone help?","A=\{(x,y)\in \mathbb{R}^2;\ x\leq 0\ \text{or }  y \leq x^2 \} A X:A\to \mathbb{R}^2 X(\partial A) = (1,0) \tau:\mathbb{R}\to \mathbb{R} \tau'>0 \tau(0) = 0 X \tilde{X}:\mathbb{R}^2\to \mathbb{R}^2 \tilde{X} (0,y) y>0 (\tau(y),\tau(y)^2) \tilde{X} \left. \tilde{X}\right|_{A} = X \forall y>0 \varphi_y \begin{align*}\dot{x} &= \tilde
{X}(x) \\
x(0) &= (0,y).\\
\end{align*} \exists t_y \in \mathbb{R} \varphi_y(t_y) = \left(\tau(y),\tau(y)^2 \right)","['real-analysis', 'ordinary-differential-equations', 'differential-topology', 'dynamical-systems']"
75,Estimate in proving that linear instability implies nonlinear instability,Estimate in proving that linear instability implies nonlinear instability,,"I am trying to prove the exercise on page 3 of http://depts.washington.edu/bdecon/workshop2012/g_stability.pdf . This question was already asked before here: Linear instability implies nonlinear instability . In the proof contained in the answer there, we let $Lv=\lambda v$ with $\mathcal{Re}\lambda > 0$ and let $\|\cdot\| = \|\cdot\|_{L^{\infty}([0,t),(X,\|\cdot\|))}$ . The following estimate is then made: $$\|u(t)-e^{Lt}\delta v\|\leq\|u(t)\|^2\int_0^te^{(\lambda+\epsilon)(t-s)}ds\leq\frac{2}{\mathcal{Re}\lambda}\|u(t)\|^2$$ for $\epsilon$ sufficiently small.  My question is, how is this second inequality true? This inequality seems to imply that $\int_0^te^{(\lambda+\epsilon)(t-s)}ds\leq\frac{2}{\mathcal{Re}\lambda}$ , but one can easily evaluate the integral to find that $\int_0^te^{(\lambda+\epsilon)(t-s)}ds=\frac{e^{(\lambda+\epsilon)t}-1}{\lambda+\epsilon}$ , which is growing exponentially in time for any $\epsilon >0$ . This is claimed to be true for all $t\geq 0$ , but obviously it cannot be true if the left hand side is growing exponentially and the right hand side is constant. If someone could explain what step I am missing here or provide their own proof of this claim, I would greatly appreciate it.","I am trying to prove the exercise on page 3 of http://depts.washington.edu/bdecon/workshop2012/g_stability.pdf . This question was already asked before here: Linear instability implies nonlinear instability . In the proof contained in the answer there, we let with and let . The following estimate is then made: for sufficiently small.  My question is, how is this second inequality true? This inequality seems to imply that , but one can easily evaluate the integral to find that , which is growing exponentially in time for any . This is claimed to be true for all , but obviously it cannot be true if the left hand side is growing exponentially and the right hand side is constant. If someone could explain what step I am missing here or provide their own proof of this claim, I would greatly appreciate it.","Lv=\lambda v \mathcal{Re}\lambda > 0 \|\cdot\| = \|\cdot\|_{L^{\infty}([0,t),(X,\|\cdot\|))} \|u(t)-e^{Lt}\delta v\|\leq\|u(t)\|^2\int_0^te^{(\lambda+\epsilon)(t-s)}ds\leq\frac{2}{\mathcal{Re}\lambda}\|u(t)\|^2 \epsilon \int_0^te^{(\lambda+\epsilon)(t-s)}ds\leq\frac{2}{\mathcal{Re}\lambda} \int_0^te^{(\lambda+\epsilon)(t-s)}ds=\frac{e^{(\lambda+\epsilon)t}-1}{\lambda+\epsilon} \epsilon >0 t\geq 0","['ordinary-differential-equations', 'analysis', 'partial-differential-equations', 'stability-theory']"
76,How to find the solution of this second order differential equation with the time-varying coefficients?,How to find the solution of this second order differential equation with the time-varying coefficients?,,"Does this second order differential equation with the time-varying coefficient fit into any general form? $${d^2x(t)\over dt^2}+\Big(k_1+k_2\cos(\omega t)\Big){dx(t)\over dt}+\Big({1\over 1+k_\Delta \sin(\omega t)}+k_3 \cos(\omega t)\Big) x(t)=F(t)$$ Some properties of the coefficients - $k_1$ and $k_2$ are in similar orders of magnitude and $k_\Delta<<1$ . How can we find the solution of this kind? Otherwise, how can we analyze the characteristics of a system governed by this? For instance, conditions for damping when we don't have any input excitation i.e., $F(t)=0$ , or when we have a harmonic excitation i.e., $F(t)=A \sin(\omega_0 t)$ ? Update 1 I understand that if we can get rid of the coefficient in $x'$ term (or remove the time dependency), then the equation can be reduced to a form of the Hill equation . Now, I am trying to do a variable substitution that is similar to this answer . Update 2 Thanks for the comments and suggestions so far.. From this answer , I understand that having a closed form of a solution would be really challenging (or even impossible). Now, I am trying to do some simplifications and my equation can be reduced to $${d^2x(t)\over dt^2}+\Big(k_1+k_2\cos(\omega t)\Big){dx(t)\over dt}+k_3  x(t)=F(t)$$ It looks much simpler than the initial version, however, still I could not find any general form similar to this. Analytical solvers could not solve it either. Now, I am only interested in analyzing the characteristics of the system behavior. For example, I can see from numerical simulations that $x(t)$ will have a growing (close to exponential) oscillations when $k_2/k_1>\rm{a~threshold}$ , and otherwise stable oscillations.  But I could not find such relation analytically. Any suggestions to move forward from here is highly appreciated.","Does this second order differential equation with the time-varying coefficient fit into any general form? Some properties of the coefficients - and are in similar orders of magnitude and . How can we find the solution of this kind? Otherwise, how can we analyze the characteristics of a system governed by this? For instance, conditions for damping when we don't have any input excitation i.e., , or when we have a harmonic excitation i.e., ? Update 1 I understand that if we can get rid of the coefficient in term (or remove the time dependency), then the equation can be reduced to a form of the Hill equation . Now, I am trying to do a variable substitution that is similar to this answer . Update 2 Thanks for the comments and suggestions so far.. From this answer , I understand that having a closed form of a solution would be really challenging (or even impossible). Now, I am trying to do some simplifications and my equation can be reduced to It looks much simpler than the initial version, however, still I could not find any general form similar to this. Analytical solvers could not solve it either. Now, I am only interested in analyzing the characteristics of the system behavior. For example, I can see from numerical simulations that will have a growing (close to exponential) oscillations when , and otherwise stable oscillations.  But I could not find such relation analytically. Any suggestions to move forward from here is highly appreciated.","{d^2x(t)\over dt^2}+\Big(k_1+k_2\cos(\omega t)\Big){dx(t)\over dt}+\Big({1\over 1+k_\Delta \sin(\omega t)}+k_3 \cos(\omega t)\Big)
x(t)=F(t) k_1 k_2 k_\Delta<<1 F(t)=0 F(t)=A \sin(\omega_0 t) x' {d^2x(t)\over dt^2}+\Big(k_1+k_2\cos(\omega t)\Big){dx(t)\over dt}+k_3 
x(t)=F(t) x(t) k_2/k_1>\rm{a~threshold}",['ordinary-differential-equations']
77,Quadratic first integrals,Quadratic first integrals,,"I started reading chapter II.16 of Solving Ordinary Differential Equations I in order to understand nummerical methods more. There, they state that symplectic methods don't always conserve the first integrals of the system. So for example, if a system is described by some Hamiltonian $H(p,q)$ then the nummerics don't need to conserve it. What is conserved (Theorem 16.7 of said book or Sanz-Serna 1988) is the quadratic first integral that is also a first integral of the system. Now, a quadratic first integral is a function of the shape $y^T C y$ for some symmetric $C$ and $y:=(p,q)$ . Now, for this to be the first integral we would need to have $H(y) : = y^T C y$ right? But then, the conservation of $H$ for most numerical cases wouldn't be possible! Take for example the Lennard-Jones potential. There we have: $$H(p, r) \approx \sum_ip_i^2 + \sum_{i,j} \frac{A}{r_{ij}^{12}}-\frac{B}{r_{ij}^6}$$ and we can't conserve this Hamiltonian because $y^TCy$ can only contain terms with $r$ to the power of $0,1,2$ and never negative. What is going on here? Is it only possible to (nummericly) conserve Hamiltonians of the shape: $$ H(p,q) = \sum_{i,j} a_{ij} p_i p_j + b_{ij}p_{i}q_{j} + c_{ij} q_{i}q_j $$ with the additional constraint that the constants $a,b,c$ need to be symmetric (so $a_{ij} = a_{ji}, b_{ij} = b_{ji}, c_{ij} = c_{ji}$ )? My specific application is to show that: $$H(p,q) = \sum_i |p_i| - \sum_{ij}\frac{1}{r_{ij}}$$ doesn't need to be preserved by leapfrog algorithm and can become unstable (see related question ) What conservation laws can be derived here? What C can I find that will be conserved?","I started reading chapter II.16 of Solving Ordinary Differential Equations I in order to understand nummerical methods more. There, they state that symplectic methods don't always conserve the first integrals of the system. So for example, if a system is described by some Hamiltonian then the nummerics don't need to conserve it. What is conserved (Theorem 16.7 of said book or Sanz-Serna 1988) is the quadratic first integral that is also a first integral of the system. Now, a quadratic first integral is a function of the shape for some symmetric and . Now, for this to be the first integral we would need to have right? But then, the conservation of for most numerical cases wouldn't be possible! Take for example the Lennard-Jones potential. There we have: and we can't conserve this Hamiltonian because can only contain terms with to the power of and never negative. What is going on here? Is it only possible to (nummericly) conserve Hamiltonians of the shape: with the additional constraint that the constants need to be symmetric (so )? My specific application is to show that: doesn't need to be preserved by leapfrog algorithm and can become unstable (see related question ) What conservation laws can be derived here? What C can I find that will be conserved?","H(p,q) y^T C y C y:=(p,q) H(y) : = y^T C y H H(p, r) \approx \sum_ip_i^2 + \sum_{i,j} \frac{A}{r_{ij}^{12}}-\frac{B}{r_{ij}^6} y^TCy r 0,1,2 
H(p,q) = \sum_{i,j} a_{ij} p_i p_j + b_{ij}p_{i}q_{j} + c_{ij} q_{i}q_j
 a,b,c a_{ij} = a_{ji}, b_{ij} = b_{ji}, c_{ij} = c_{ji} H(p,q) = \sum_i |p_i| - \sum_{ij}\frac{1}{r_{ij}}","['ordinary-differential-equations', 'numerical-methods', 'mathematical-physics', 'symplectic-linear-algebra']"
78,How to solve following linear differential-difference equation?,How to solve following linear differential-difference equation?,,"How to solve following linear differential-difference equation? $$\frac{da_{n}(t)}{dt}=i k na_{n}(t)+G\left\{n(n-1)a_{n-2}(t)-a_{n+2}(t) \right\},~n=0,1,2,\ldots~~~~~(1)$$ where, k and G is a constant. And $i$ is the imaginary unit. The initial conditions are $$a_{n}(0)=\delta_{mn}=\begin{cases} C_{0}~~(n=m) \\ 0~~(n\neq m) \end{cases}$$ where, $C_{0}$ is a constant. $a_{0}(t)$ and $a_{1}(t)$ is unknown function. I want to find $a_{n}(t)$ . If $k=0$ , then equation $(1)$ reduce to following equation: $$ \frac{da_{n}(t)}{dt}=G\left\{n(n-1)a_{n-2}(t)-a_{n+2}(t) \right\},~n=0,1,2,\ldots.~~~~~(\mathrm{A}) $$ The general solution of equation $(\mathrm{A})$ is $$ a_{n}(t)=C_{0}\frac{1}{\sqrt{\mathstrut 2\pi}}\left(\frac{n}{2} \right)!~\sqrt[]{\mathstrut 2^{n}}\left\{1+(-1)^{n+1} \right\}(\cosh{2Gt})^{-\frac{3}{2}}(\tanh{2Gt})^{-\frac{1}{2}(n-1)}.~~~~~(\mathrm{B}) $$ Equation $(\mathrm{B})$ satisfy following initial conditions: $$ a_{n}(0)=\begin{cases} C_{0}~~(n=1) \\ 0~~~~~(n\neq 1) \end{cases} ~~~~~~~~~(\mathrm{C}) $$ I have tried Laplace transform to equaton $(1)$ . Multiplying both sides of equation $(1)$ by $\mathrm{e}^{-st}$ and then Integrating  for the interval $0$ to $\infty$ to obtain $$ \int_{0}^{\infty}dt~\mathrm{e}^{-st} \frac{da_{n}(t)}{dt}=i k n\int_{0}^{\infty}dt~\mathrm{e}^{-st}a_{n}(t)+G\left\{n(n-1)\int_{0}^{\infty}dt~\mathrm{e}^{-st}a_{n-2}(t)-\int_{0}^{\infty}dt~\mathrm{e}^{-st}a_{n+2}(t) \right\}.~~~~~(2) $$ We define the $U_{n}(s)$ $$ U_{n}(s):=\int_{0}^{\infty}dt~\mathrm{e}^{-st}a_{n}(t).~~~~~(3) $$ Then equation $(2)$ to be $$ sU_{n}(s)-a_{n}(0)=iknU_{n}(s)+ G\left\{n(n-1)U_{n-2}(s)-U_{n+2}(s) \right\}.~~~~~(4) $$ where, we use following integration by parts $$ \int_{0}^{\infty}dt~\mathrm{e}^{-st}\frac{d a_{n}(t)}{dt}=sU_{n}(s)-a_{n}(0). $$ Let's solve equation $(4)$ by using Z-transform. First, we define unilateral Z-transform $W(s,z)$ as follows: $$ \mathcal{Z}[U_{n}(s)]= W(s,z):=\sum_{n=0}^{\infty} U_{n}(s)z^{-n}.~~~~~(5) $$ Noting following relations Differentiation & Time delay $$ \mathcal{Z}[n(n-1)U_{n-2}(s)]= 2z^{-2}W(s,z)-2z^{-1}\frac{\partial W(s,z)}{\partial z}+\frac{\partial^{2}W(s,z)}{\partial z^{2}}, ~~~~~~~~(6) $$ Time advance $$ \mathcal{Z}[U_{n+2}(s)]=z^{2}W(s,z)-z^{2}U_{0}(s)-zU_{1}(s), ~~~~~~~~~(7) $$ Using propaty of $a_{n}(0)=\delta_{mn}$ $$ \mathcal{Z}[a_{n}(0)]=C_{0}z^{-m}, ~~~~~~~~~~(8) $$ we can transform equation $(4)$ as follows: $$ Gz^{2}\frac{\partial^{2}W(s,z)}{\partial z^{2}}+(-2G-ikz^{2})z\frac{\partial W(s,z)}{\partial z} +\left\{z^{2}(-Gz^{2}-s)+2G \right\}W(s,z)+Gz^{3}\left\{zU_{0}(s)+U_{1}(s) \right\} +C_{0}z^{-m+2}=0. ~~~~~~(9) $$ Equation $(9)$ is some kind of Bessel equation. I'm trying to solve equation $(9)$ .","How to solve following linear differential-difference equation? where, k and G is a constant. And is the imaginary unit. The initial conditions are where, is a constant. and is unknown function. I want to find . If , then equation reduce to following equation: The general solution of equation is Equation satisfy following initial conditions: I have tried Laplace transform to equaton . Multiplying both sides of equation by and then Integrating  for the interval to to obtain We define the Then equation to be where, we use following integration by parts Let's solve equation by using Z-transform. First, we define unilateral Z-transform as follows: Noting following relations Differentiation & Time delay Time advance Using propaty of we can transform equation as follows: Equation is some kind of Bessel equation. I'm trying to solve equation .","\frac{da_{n}(t)}{dt}=i k na_{n}(t)+G\left\{n(n-1)a_{n-2}(t)-a_{n+2}(t) \right\},~n=0,1,2,\ldots~~~~~(1) i a_{n}(0)=\delta_{mn}=\begin{cases}
C_{0}~~(n=m)
\\
0~~(n\neq m)
\end{cases} C_{0} a_{0}(t) a_{1}(t) a_{n}(t) k=0 (1) 
\frac{da_{n}(t)}{dt}=G\left\{n(n-1)a_{n-2}(t)-a_{n+2}(t) \right\},~n=0,1,2,\ldots.~~~~~(\mathrm{A})
 (\mathrm{A}) 
a_{n}(t)=C_{0}\frac{1}{\sqrt{\mathstrut 2\pi}}\left(\frac{n}{2} \right)!~\sqrt[]{\mathstrut 2^{n}}\left\{1+(-1)^{n+1} \right\}(\cosh{2Gt})^{-\frac{3}{2}}(\tanh{2Gt})^{-\frac{1}{2}(n-1)}.~~~~~(\mathrm{B})
 (\mathrm{B}) 
a_{n}(0)=\begin{cases}
C_{0}~~(n=1)
\\
0~~~~~(n\neq 1)
\end{cases}
~~~~~~~~~(\mathrm{C})
 (1) (1) \mathrm{e}^{-st} 0 \infty 
\int_{0}^{\infty}dt~\mathrm{e}^{-st}
\frac{da_{n}(t)}{dt}=i k n\int_{0}^{\infty}dt~\mathrm{e}^{-st}a_{n}(t)+G\left\{n(n-1)\int_{0}^{\infty}dt~\mathrm{e}^{-st}a_{n-2}(t)-\int_{0}^{\infty}dt~\mathrm{e}^{-st}a_{n+2}(t) \right\}.~~~~~(2)
 U_{n}(s) 
U_{n}(s):=\int_{0}^{\infty}dt~\mathrm{e}^{-st}a_{n}(t).~~~~~(3)
 (2) 
sU_{n}(s)-a_{n}(0)=iknU_{n}(s)+
G\left\{n(n-1)U_{n-2}(s)-U_{n+2}(s) \right\}.~~~~~(4)
 
\int_{0}^{\infty}dt~\mathrm{e}^{-st}\frac{d a_{n}(t)}{dt}=sU_{n}(s)-a_{n}(0).
 (4) W(s,z) 
\mathcal{Z}[U_{n}(s)]=
W(s,z):=\sum_{n=0}^{\infty} U_{n}(s)z^{-n}.~~~~~(5)
 
\mathcal{Z}[n(n-1)U_{n-2}(s)]=
2z^{-2}W(s,z)-2z^{-1}\frac{\partial W(s,z)}{\partial z}+\frac{\partial^{2}W(s,z)}{\partial z^{2}},
~~~~~~~~(6)
 
\mathcal{Z}[U_{n+2}(s)]=z^{2}W(s,z)-z^{2}U_{0}(s)-zU_{1}(s),
~~~~~~~~~(7)
 a_{n}(0)=\delta_{mn} 
\mathcal{Z}[a_{n}(0)]=C_{0}z^{-m},
~~~~~~~~~~(8)
 (4) 
Gz^{2}\frac{\partial^{2}W(s,z)}{\partial z^{2}}+(-2G-ikz^{2})z\frac{\partial W(s,z)}{\partial z}
+\left\{z^{2}(-Gz^{2}-s)+2G \right\}W(s,z)+Gz^{3}\left\{zU_{0}(s)+U_{1}(s) \right\} +C_{0}z^{-m+2}=0.
~~~~~~(9)
 (9) (9)","['ordinary-differential-equations', 'recurrence-relations', 'delay-differential-equations']"
79,Not possible to find non-zero terms of series expansion?,Not possible to find non-zero terms of series expansion?,,"I've been asked to compute the first 3 nonzero terms of a power series expansion about x=0 for two linearly independent solutions to the ODE: $$(1+x^3)y''- 6xy =0 $$ I have tried to solve this many different ways and continue to get the same solution, which does not allow me to find three nonzero terms from two linearly independent solutions I have used $$y(x) = \sum_{n=0}^\infty a_nx^n $$ $$ y'(x)=\sum_{n=1}^\infty a_nnx^{n-1} $$ $$ y''(x)=\sum_{n=2}^\infty a_nn(n-1)x^{n-2} $$ to obtain the series form $$ \sum_{n=2}^\infty a_nn(n-1)x^{n-2} + x^3\sum_{n=2}^\infty a_nn(n-1)x^{n-2} -6x\sum_{n=0}^\infty a_nx^n$$ Add in x terms: $$ \sum_{n=2}^\infty a_nn(n-1)x^{n-2} + \sum_{n=2}^\infty a_nn(n-1)x^{n+1} -\sum_{n=0}^\infty 6a_nx^{n+1}$$ Set all the powers of x equal to n: $$ \sum_{n=0}^\infty a_{n+2}(n+2)(n+1)x^{n} + \sum_{n=3}^\infty a_{n-1}(n-1)(n-2)x^{n} -\sum_{n=1}^\infty 6a_{n-1}x^{n}$$ Peel off terms to have all series start at n=3: $$ 2a_2x^0+6a_3x^1-6a_0x^1+12a_4x^2-6a_1x^2$$ $$+$$ $$\sum_{n=3}^\infty [a_{n+2}(n+2)(n+1)+a_{n-1}(n-1)(n-2)-6a_{n-1}]x^n=0$$ From this I have deduced: $ x^0 : a_2=0 $ $ x^1 : a_3=a_0 $ $ x^2 : a_4=\frac{a_1}{2} $ $ x^n, n\geq3 : $ \begin{align}  & a_{n+2}=-\frac{a_{n-1}(n^2-3n+2-6)}{(n+2)(n+1)}\\  & = -\frac{a_{n-1}(n+1)(n-4)}{(n+1)(n+2)}\\  & = -\frac{a_{n-1}(n-4)}{n+2}\\ \end{align} Using the recursion equation above I obtained these terms: $n=3 : $ $$ a_5=\frac{a_2}{5}=0 $$ $n=4 : $ $$ a_6=a_3(4-4)=0 $$ $n=5 : $ $$ a_7=\frac{-a_4}{7}=\frac{-a_1}{14} $$ $n=6 : $ $$ a_8=\frac{-2a_5}{5}=\frac{-a_2}{20}=0$$ $n=7 : $ $$ a_9=\frac{-3a_6}{9}=0$$ $n=8 : $ $$ a_10=\frac{-4a_7}{12}=\frac{a_4}{21}=\frac{a_1}{42} $$ $n=9 : $ $$ a_11=\frac{-5a_8}{11}=\frac{a_2}{44}=0 $$ $n=10 : $ $$ a_12=\frac{-6a_9}{12}=\frac{-a_2}{2}=0 $$ $n=11 : $ $$ a_13=\frac{-7a_10}{13}=\frac{-7a_4}{273}=\frac{-a_1}{78} $$ So, every second and third term equal zer0.  My solution is: $$ y_1(x)=a_0(1+x^3) $$ $$ y_2(x)=a_1\big(x+\frac{x^4}{2}-\frac{x^7}{14}+\frac{x^{10}}{42} - ...) $$ Can someone please tell me what I am doing wrong here? I cannot come up with three non zero terms from $y_1$ as there are only two terms, and everything else is zero. When I try to apply the ratio test to this series solution I get inconclusive results as well which further makes me think my solution is incorrect... Any help or advice would be very greatly appreciated.","I've been asked to compute the first 3 nonzero terms of a power series expansion about x=0 for two linearly independent solutions to the ODE: I have tried to solve this many different ways and continue to get the same solution, which does not allow me to find three nonzero terms from two linearly independent solutions I have used to obtain the series form Add in x terms: Set all the powers of x equal to n: Peel off terms to have all series start at n=3: From this I have deduced: Using the recursion equation above I obtained these terms: So, every second and third term equal zer0.  My solution is: Can someone please tell me what I am doing wrong here? I cannot come up with three non zero terms from as there are only two terms, and everything else is zero. When I try to apply the ratio test to this series solution I get inconclusive results as well which further makes me think my solution is incorrect... Any help or advice would be very greatly appreciated.","(1+x^3)y''- 6xy =0  y(x) = \sum_{n=0}^\infty a_nx^n   y'(x)=\sum_{n=1}^\infty a_nnx^{n-1}   y''(x)=\sum_{n=2}^\infty a_nn(n-1)x^{n-2}   \sum_{n=2}^\infty a_nn(n-1)x^{n-2} + x^3\sum_{n=2}^\infty a_nn(n-1)x^{n-2} -6x\sum_{n=0}^\infty a_nx^n  \sum_{n=2}^\infty a_nn(n-1)x^{n-2} + \sum_{n=2}^\infty a_nn(n-1)x^{n+1} -\sum_{n=0}^\infty 6a_nx^{n+1}  \sum_{n=0}^\infty a_{n+2}(n+2)(n+1)x^{n} + \sum_{n=3}^\infty a_{n-1}(n-1)(n-2)x^{n} -\sum_{n=1}^\infty 6a_{n-1}x^{n}  2a_2x^0+6a_3x^1-6a_0x^1+12a_4x^2-6a_1x^2 + \sum_{n=3}^\infty [a_{n+2}(n+2)(n+1)+a_{n-1}(n-1)(n-2)-6a_{n-1}]x^n=0  x^0 : a_2=0   x^1 : a_3=a_0   x^2 : a_4=\frac{a_1}{2}   x^n, n\geq3 :  \begin{align}
 & a_{n+2}=-\frac{a_{n-1}(n^2-3n+2-6)}{(n+2)(n+1)}\\
 & = -\frac{a_{n-1}(n+1)(n-4)}{(n+1)(n+2)}\\
 & = -\frac{a_{n-1}(n-4)}{n+2}\\
\end{align} n=3 :   a_5=\frac{a_2}{5}=0  n=4 :   a_6=a_3(4-4)=0  n=5 :   a_7=\frac{-a_4}{7}=\frac{-a_1}{14}  n=6 :   a_8=\frac{-2a_5}{5}=\frac{-a_2}{20}=0 n=7 :   a_9=\frac{-3a_6}{9}=0 n=8 :   a_10=\frac{-4a_7}{12}=\frac{a_4}{21}=\frac{a_1}{42}  n=9 :   a_11=\frac{-5a_8}{11}=\frac{a_2}{44}=0  n=10 :   a_12=\frac{-6a_9}{12}=\frac{-a_2}{2}=0  n=11 :   a_13=\frac{-7a_10}{13}=\frac{-7a_4}{273}=\frac{-a_1}{78}   y_1(x)=a_0(1+x^3)   y_2(x)=a_1\big(x+\frac{x^4}{2}-\frac{x^7}{14}+\frac{x^{10}}{42} - ...)  y_1","['ordinary-differential-equations', 'convergence-divergence', 'power-series', 'taylor-expansion']"
80,Limit of solution of Cauchy problem,Limit of solution of Cauchy problem,,"I have the following Cauchy problem: \begin{align} y'(t) = \arctan(t^3(y-1)) \\ y(0) = \alpha \end{align} I want to study the limit of the solution on the boundary. This is what I have done so far: I know that the function is $C^\infty$ so it is Lipshitz and then I have uniqueness and existence of global solution for each $\alpha$ . The constant solution is y = 1. By uniqueness, other solutions cannot interest this line. If $\alpha < 1$ , then I have that the function increases monotonically up until $\alpha$ and then decreases monotonically to infinity. If $\alpha > 1$ , then I have that the function decreases monotonically down to $\alpha$ and then increases monotonically to infinity. This means that I always have the limit both at $-\infty$ and at $\infty$ for whatever alpha. However my problem now is actually finding the limit. Any suggestion?","I have the following Cauchy problem: I want to study the limit of the solution on the boundary. This is what I have done so far: I know that the function is so it is Lipshitz and then I have uniqueness and existence of global solution for each . The constant solution is y = 1. By uniqueness, other solutions cannot interest this line. If , then I have that the function increases monotonically up until and then decreases monotonically to infinity. If , then I have that the function decreases monotonically down to and then increases monotonically to infinity. This means that I always have the limit both at and at for whatever alpha. However my problem now is actually finding the limit. Any suggestion?","\begin{align}
y'(t) = \arctan(t^3(y-1)) \\
y(0) = \alpha
\end{align} C^\infty \alpha \alpha < 1 \alpha \alpha > 1 \alpha -\infty \infty","['ordinary-differential-equations', 'cauchy-problem']"
81,Solution to the parabolic cylinder equation,Solution to the parabolic cylinder equation,,"In the Gradshteyn & Ryzhik (7th ed.) the differential equation (9.255) leading to parabolic cylinder functions is $$\frac{d^2u}{dz^2}+(p+\frac{1}{2}-\frac{z^2}{4})u=0.$$ The solutions are $u=D_p(z),D_p(-z),D_{-p-1}(iz),D_{-p-1}(-iz)$ , where $D_p(z)$ is the parabolic cylinder function. These four solutions are linearly dependent. My question is why is there four solutions to the second order ODE? In my case $p$ is complex, and Mathematica gives solution in the form $C_1 D_p(z)+C_2 D_{-p-1}(iz)$ .","In the Gradshteyn & Ryzhik (7th ed.) the differential equation (9.255) leading to parabolic cylinder functions is The solutions are , where is the parabolic cylinder function. These four solutions are linearly dependent. My question is why is there four solutions to the second order ODE? In my case is complex, and Mathematica gives solution in the form .","\frac{d^2u}{dz^2}+(p+\frac{1}{2}-\frac{z^2}{4})u=0. u=D_p(z),D_p(-z),D_{-p-1}(iz),D_{-p-1}(-iz) D_p(z) p C_1 D_p(z)+C_2 D_{-p-1}(iz)","['ordinary-differential-equations', 'special-functions']"
82,Advantage of multi steps ODE methods over single step methods,Advantage of multi steps ODE methods over single step methods,,"I wanna know what's the advantage of multi step ODE methods such as Adams-Bashforth over ordinary single step methods such Runge–Kutta, accuracy/time wise.","I wanna know what's the advantage of multi step ODE methods such as Adams-Bashforth over ordinary single step methods such Runge–Kutta, accuracy/time wise.",,"['ordinary-differential-equations', 'numerical-methods', 'runge-kutta-methods']"
83,"Numerical methods for the matrix ODE $\dot L(t)=[L, \Pi_m(F\circ L)]$",Numerical methods for the matrix ODE,"\dot L(t)=[L, \Pi_m(F\circ L)]","Suppose $L$ is a lower-triangular matrix whose row sum is $0$ . I want to numerically solve the following matrix ODE $$\dot L(t) = [L, \Pi_m (F \circ L)], \qquad L(0)=L_0$$ where $[\cdot \,,\cdot]$ is the Lie bracket, i.e., $[A,B]=A B - B A$ . $L_0$ is lower-triangular and its row sum is also $0$ . $\circ$ is the entry-wise product, i.e., $(A \circ B)_{ij} = a_{ij} b_{ij}$ . $F_{ij}=-\frac{f(u_i)-f(u_j)}{u_i-u_j} $ for $i \ne j.$ $F_{ii}=-u(i)$ . $f(x)=\frac{x^2}{2}$ and $u=(u_1, u_2, \dots, u_n)$ is a sequence of some numbers, where $u_i<u_j.$ So, $F$ is a symmetric constant matrix. $\Pi_m(F\circ L)=F\circ L- \mbox{diag}((F\circ L)*e),$ where $e=(1 ~1~...1)^T, $ a column vector. So, the row sum of $\Pi_m(F\circ L)$ is also zero. I want to solve $L(t)$ numerically such that $L(t)$ preserves the property that the row sum is $0$ . For convenience, let $\Pi_m (F \circ L)=B(L(t))$ . By the forward Euler method, we have the following iteration equation: $L_{n+1}=L_n+ h\dot L=L_n +h (L_nB_n-B_nL_n).$ It is easy to prove that the product of two matrices that their row sums are $0$ preserve the property. So, the row sums of $B_nL_n$ and $L_nB_n$ are $0$ . Since the row sum of $L_0$ is $0$ , then we have the row sum of $L_n$ is $0$ for any $n \ge 0$ . So, we can conclude that the forward Euler method preserves the property (row sum is zero). However, I used the Matlab to implement the forward method, and the numerical result blew up for some initial condition.  However, my professor told me that the analytic solution would not blow up because he proved it analytically. My questions: Do we have the convergence of $L_n$ , i.e. $L_n$ converges to the exact solution $L(t)$ ? Do we have other methods that converge to the exact solution and preserve the property if the forward Euler does not converge?","Suppose is a lower-triangular matrix whose row sum is . I want to numerically solve the following matrix ODE where is the Lie bracket, i.e., . is lower-triangular and its row sum is also . is the entry-wise product, i.e., . for . and is a sequence of some numbers, where So, is a symmetric constant matrix. where a column vector. So, the row sum of is also zero. I want to solve numerically such that preserves the property that the row sum is . For convenience, let . By the forward Euler method, we have the following iteration equation: It is easy to prove that the product of two matrices that their row sums are preserve the property. So, the row sums of and are . Since the row sum of is , then we have the row sum of is for any . So, we can conclude that the forward Euler method preserves the property (row sum is zero). However, I used the Matlab to implement the forward method, and the numerical result blew up for some initial condition.  However, my professor told me that the analytic solution would not blow up because he proved it analytically. My questions: Do we have the convergence of , i.e. converges to the exact solution ? Do we have other methods that converge to the exact solution and preserve the property if the forward Euler does not converge?","L 0 \dot L(t) = [L, \Pi_m (F \circ L)], \qquad L(0)=L_0 [\cdot \,,\cdot] [A,B]=A B - B A L_0 0 \circ (A \circ B)_{ij} = a_{ij} b_{ij} F_{ij}=-\frac{f(u_i)-f(u_j)}{u_i-u_j}  i \ne j. F_{ii}=-u(i) f(x)=\frac{x^2}{2} u=(u_1, u_2, \dots, u_n) u_i<u_j. F \Pi_m(F\circ L)=F\circ L- \mbox{diag}((F\circ L)*e), e=(1 ~1~...1)^T,  \Pi_m(F\circ L) L(t) L(t) 0 \Pi_m (F \circ L)=B(L(t)) L_{n+1}=L_n+ h\dot L=L_n +h (L_nB_n-B_nL_n). 0 B_nL_n L_nB_n 0 L_0 0 L_n 0 n \ge 0 L_n L_n L(t)","['ordinary-differential-equations', 'numerical-methods', 'matlab']"
84,the finite solution first order differential equation,the finite solution first order differential equation,,"I have an equation: $xy'+ay=f(x)$ , where a is positive constant and $f(x)\rightarrow b<\infty$ as $x \rightarrow 0$ . I found the solution: $y=\frac{C}{|x|^a}+\frac{b}{a}-\frac{1}{|x|^a}\int\limits_0^x f'(t)\frac{t^a}{a}dt$ . And I need to show that there is only one finite solution as $x \rightarrow 0$ . To do that I have to show that $\frac{1}{|x|^a}\int\limits_0^x f'(t)\frac{t^a}{a}dt \rightarrow 0$ as $x \rightarrow 0$ . But don't know how to do it. Can anyone help me?","I have an equation: , where a is positive constant and as . I found the solution: . And I need to show that there is only one finite solution as . To do that I have to show that as . But don't know how to do it. Can anyone help me?",xy'+ay=f(x) f(x)\rightarrow b<\infty x \rightarrow 0 y=\frac{C}{|x|^a}+\frac{b}{a}-\frac{1}{|x|^a}\int\limits_0^x f'(t)\frac{t^a}{a}dt x \rightarrow 0 \frac{1}{|x|^a}\int\limits_0^x f'(t)\frac{t^a}{a}dt \rightarrow 0 x \rightarrow 0,['ordinary-differential-equations']
85,"""Damped"" wave equation with Fourier method","""Damped"" wave equation with Fourier method",,"The problem I was trying to solve is the following PDE problem $$\begin{cases} \partial_{tt}^2 u = \partial_{xx}^2 u -\gamma\partial_t u \\[5 pt] u(0,t)= u(\pi, t) = 0 \\[5 pt] u(x,0) = (\sin2x)^4 -{1\over 5}\sin 10x \\[5 pt] \partial_t u(x,t)|_{t=0}=0 \end{cases}\tag 1$$ with the Fourier series method. But I got stuck on the calculations. What I've done is, first thing first, to evaluate the initial condition to eliminate that fourth power, which can be easily done, and I've got $$ u(x,0) = {3\over 8}-{1\over 2}\cos4x +{1\over8}\cos8x-{1\over 5}\sin10x $$ This says to me that the solution ought to be of the form $$ u(x,t) = \sum_n a_n(t)\sin(nx)+b_n(t)\cos(nx) $$ or simply by using the complex exponential, which doesn't change much. I then used the ansatz in the PDE to get two ODE's for the coefficients $a_n(t), b_n(t)$ $$ \sum_n(a''_n(t)\sin(nx)+b''_n(t)\cos(nx))= \\ =-\sum_n(a_n(t)\sin(nx)+b_n(t)\cos(nx))-\gamma\sum_n(a'_n(t)\sin(nx)+b'_n(t)\cos(nx)) $$ and got, equating the coefficients $$ a''_n(t)= -\gamma a'_n(t)-a_n(t) \\ b''_n(t)= -\gamma b'_n(t)-b_n(t) $$ which are the same equations: the equation of a damped harmonic oscillator. To find the solution we search for the solutions of the polynomial equation $$ \lambda^2 +\gamma\lambda +1 = 0 $$ which are $$ \lambda_1 = -{1\over 2}\left(\gamma+\sqrt{\gamma^2-4}\right)\;\;\;\;\; \lambda_2 = -{1\over 2}\left(\gamma-\sqrt{\gamma^2-4}\right) $$ Clearly the solution for the ODE's depends on the value of the ""damping coefficient"" gamma $$ \gamma^2-4 \gt 0 \implies \color{red}{a_n(t) = c^a_1e^{\lambda_1 t}+c^a_2e^{\lambda_2 t}}\\ \gamma_2-4\lt 0 \implies \lambda_{1/2} = \mu\pm i\nu \implies \color{orange}{a_n(t) = c^a_1 e^{(\mu+i\nu)t}+c^a_2 e^{(\mu-i\nu)t}} \\ \gamma^2-4=0\implies \lambda_1=\lambda_2=\lambda \implies \color{green}{a_n(t)=c^a_1 e^{\lambda t}+c^a_2 t e^{\lambda t}} $$ and the same goes for $b_n(t)$. But then jumped to my mind that the solution would become very ugly! Knowing my professor I think that there could be a easier way to solving this. Question 1: In my solution, am I headed in the right way? Question 2: Is there a simpler method to solve this problem? Question 3: I thought about using Laplace transform but the initial condition make matter worse: could this be a viable way?","The problem I was trying to solve is the following PDE problem $$\begin{cases} \partial_{tt}^2 u = \partial_{xx}^2 u -\gamma\partial_t u \\[5 pt] u(0,t)= u(\pi, t) = 0 \\[5 pt] u(x,0) = (\sin2x)^4 -{1\over 5}\sin 10x \\[5 pt] \partial_t u(x,t)|_{t=0}=0 \end{cases}\tag 1$$ with the Fourier series method. But I got stuck on the calculations. What I've done is, first thing first, to evaluate the initial condition to eliminate that fourth power, which can be easily done, and I've got $$ u(x,0) = {3\over 8}-{1\over 2}\cos4x +{1\over8}\cos8x-{1\over 5}\sin10x $$ This says to me that the solution ought to be of the form $$ u(x,t) = \sum_n a_n(t)\sin(nx)+b_n(t)\cos(nx) $$ or simply by using the complex exponential, which doesn't change much. I then used the ansatz in the PDE to get two ODE's for the coefficients $a_n(t), b_n(t)$ $$ \sum_n(a''_n(t)\sin(nx)+b''_n(t)\cos(nx))= \\ =-\sum_n(a_n(t)\sin(nx)+b_n(t)\cos(nx))-\gamma\sum_n(a'_n(t)\sin(nx)+b'_n(t)\cos(nx)) $$ and got, equating the coefficients $$ a''_n(t)= -\gamma a'_n(t)-a_n(t) \\ b''_n(t)= -\gamma b'_n(t)-b_n(t) $$ which are the same equations: the equation of a damped harmonic oscillator. To find the solution we search for the solutions of the polynomial equation $$ \lambda^2 +\gamma\lambda +1 = 0 $$ which are $$ \lambda_1 = -{1\over 2}\left(\gamma+\sqrt{\gamma^2-4}\right)\;\;\;\;\; \lambda_2 = -{1\over 2}\left(\gamma-\sqrt{\gamma^2-4}\right) $$ Clearly the solution for the ODE's depends on the value of the ""damping coefficient"" gamma $$ \gamma^2-4 \gt 0 \implies \color{red}{a_n(t) = c^a_1e^{\lambda_1 t}+c^a_2e^{\lambda_2 t}}\\ \gamma_2-4\lt 0 \implies \lambda_{1/2} = \mu\pm i\nu \implies \color{orange}{a_n(t) = c^a_1 e^{(\mu+i\nu)t}+c^a_2 e^{(\mu-i\nu)t}} \\ \gamma^2-4=0\implies \lambda_1=\lambda_2=\lambda \implies \color{green}{a_n(t)=c^a_1 e^{\lambda t}+c^a_2 t e^{\lambda t}} $$ and the same goes for $b_n(t)$. But then jumped to my mind that the solution would become very ugly! Knowing my professor I think that there could be a easier way to solving this. Question 1: In my solution, am I headed in the right way? Question 2: Is there a simpler method to solve this problem? Question 3: I thought about using Laplace transform but the initial condition make matter worse: could this be a viable way?",,"['ordinary-differential-equations', 'partial-differential-equations', 'fourier-series', 'laplace-transform', 'wave-equation']"
86,Generating Function and Laguerre Polynomials.,Generating Function and Laguerre Polynomials.,,"The Laguerre polynomial of degree $n$ is $$L_n(x)= \sum_{k=0}^n \frac{(-1)^k \; n\,!}{(k\,!)^2 (n-k)\;!}x^k $$ I am expanding the generating function $$\phi(x,z)= \frac{e^{-xz/(1-z)}}{1-z} $$ to get an expression involving the Laguerre Polynomials ( given $|z|<1$ ) as following $$\phi(x,z)= \sum_{k=0}^\infty L_n(x) z^n \tag{1}$$ To proceed, I just started expanding $\phi(x,z)$ but got stuck in the middle. Here is my attempt: $$\phi(x,z)= \frac{e^{-xz/(1-z)}}{1-z}=\frac{1}{1-z}\;\;\sum_{k=0}^\infty \frac{1}{k\;!} \left(\frac{-xz}{1-z}\right)^k $$ $$=\sum_{k=0}^\infty\frac{(-1)^k}{k\;!}\frac{x^k\;z^k}{(1-z)^{1+k}} $$ Stuck from here. Probably I have to expand $(1-z)^{-(1+k)}$ but I have no idea how to do it. How to get from here to equation $(1)$.","The Laguerre polynomial of degree $n$ is $$L_n(x)= \sum_{k=0}^n \frac{(-1)^k \; n\,!}{(k\,!)^2 (n-k)\;!}x^k $$ I am expanding the generating function $$\phi(x,z)= \frac{e^{-xz/(1-z)}}{1-z} $$ to get an expression involving the Laguerre Polynomials ( given $|z|<1$ ) as following $$\phi(x,z)= \sum_{k=0}^\infty L_n(x) z^n \tag{1}$$ To proceed, I just started expanding $\phi(x,z)$ but got stuck in the middle. Here is my attempt: $$\phi(x,z)= \frac{e^{-xz/(1-z)}}{1-z}=\frac{1}{1-z}\;\;\sum_{k=0}^\infty \frac{1}{k\;!} \left(\frac{-xz}{1-z}\right)^k $$ $$=\sum_{k=0}^\infty\frac{(-1)^k}{k\;!}\frac{x^k\;z^k}{(1-z)^{1+k}} $$ Stuck from here. Probably I have to expand $(1-z)^{-(1+k)}$ but I have no idea how to do it. How to get from here to equation $(1)$.",,"['ordinary-differential-equations', 'polynomials', 'generating-functions']"
87,Assumption on traveling wave solutions of Fisher's equation,Assumption on traveling wave solutions of Fisher's equation,,"I have a question about Fisher's equation in a biology context. For example, in Fisher's equation $u_{t} = Du_{xx} + u(1-u)$, where $u$ is a density of cell, the logistic term explains that the capacity is 1. When we look for a traveling wave solution $U(x-ct)$ (or a heteroclinic orbit), $c$ is a speed, connecting $u=0$ and $u=1,$ do we assume that a traveling wave ansatz must satisfy $0\leq U(x-ct) \leq 1?$ Or, could it be $U(x-ct)>1?$ Thank you so much!","I have a question about Fisher's equation in a biology context. For example, in Fisher's equation $u_{t} = Du_{xx} + u(1-u)$, where $u$ is a density of cell, the logistic term explains that the capacity is 1. When we look for a traveling wave solution $U(x-ct)$ (or a heteroclinic orbit), $c$ is a speed, connecting $u=0$ and $u=1,$ do we assume that a traveling wave ansatz must satisfy $0\leq U(x-ct) \leq 1?$ Or, could it be $U(x-ct)>1?$ Thank you so much!",,"['ordinary-differential-equations', 'partial-differential-equations', 'mathematical-modeling', 'biology']"
88,Smooth curves that are submanifolds,Smooth curves that are submanifolds,,"Let $f:I\subseteq \mathbb{R} \to \mathbb{R}^n$ be a smooth function ($I$ is an interval in $\mathbb{R}$). What are the sufficient conditions (or sufficient and necessary) under which the image of this function (say $S$ which is indeed a curve in $\mathbb{R}^n$) is a submanifold? Is there any famous theorem on this topic? I tried to find such conditions in this way: according to the definition of submanifold, we need a 1-dimensional submanifold chart for $S$ at every point $x$ of $S$. Thus, there should exist open interval $W \subseteq \mathbb{R}$ and smooth function $G: W \to \mathbb{R}^n$. We need $G(W)=U \cap S$ where $U\subseteq \mathbb{R}^n$ is an open set and $x\in S \cap U$. Finally, we need the function $G$ to have the form $G(w)=A\left( {\begin{array}{*{20}{c}} w\\ g(w) \end{array}} \right)$ where $A$ is non-singular and $g:W\to\mathbb{R}^{n-1}$ is a smooth function (I used the definition from ""Ordinary Differential Equations with Applications by C. Chicone"" ). I think the sufficient condition is that $f$ should be a diffeomorphism (it should be smooth and have smooth inverse) and $I$ be an open interval. Actually, in this case, the function $f$ plays the role of the function $G$ in the definition of submanifold. Thank you, in advance, for your help! Incidentally, I do not understand why we need $U$ in the definition of submanifold to be open? I am saying this because $S$ can be a closed subset of $\mathbb{R}^n$ and thus $U\cap S $ may not be open. Indeed, my question is that whay the image of $W$ under $G$ i.e., $G(W)=U \cap S$, is always open? Thanks.","Let $f:I\subseteq \mathbb{R} \to \mathbb{R}^n$ be a smooth function ($I$ is an interval in $\mathbb{R}$). What are the sufficient conditions (or sufficient and necessary) under which the image of this function (say $S$ which is indeed a curve in $\mathbb{R}^n$) is a submanifold? Is there any famous theorem on this topic? I tried to find such conditions in this way: according to the definition of submanifold, we need a 1-dimensional submanifold chart for $S$ at every point $x$ of $S$. Thus, there should exist open interval $W \subseteq \mathbb{R}$ and smooth function $G: W \to \mathbb{R}^n$. We need $G(W)=U \cap S$ where $U\subseteq \mathbb{R}^n$ is an open set and $x\in S \cap U$. Finally, we need the function $G$ to have the form $G(w)=A\left( {\begin{array}{*{20}{c}} w\\ g(w) \end{array}} \right)$ where $A$ is non-singular and $g:W\to\mathbb{R}^{n-1}$ is a smooth function (I used the definition from ""Ordinary Differential Equations with Applications by C. Chicone"" ). I think the sufficient condition is that $f$ should be a diffeomorphism (it should be smooth and have smooth inverse) and $I$ be an open interval. Actually, in this case, the function $f$ plays the role of the function $G$ in the definition of submanifold. Thank you, in advance, for your help! Incidentally, I do not understand why we need $U$ in the definition of submanifold to be open? I am saying this because $S$ can be a closed subset of $\mathbb{R}^n$ and thus $U\cap S $ may not be open. Indeed, my question is that whay the image of $W$ under $G$ i.e., $G(W)=U \cap S$, is always open? Thanks.",,"['ordinary-differential-equations', 'differential-geometry', 'manifolds', 'differential-topology', 'smooth-manifolds']"
89,L2-orthogonality,L2-orthogonality,,"Consider the following functions:  $$ \psi_1(x) = \sinh(x)-\sin(x) \quad \quad\psi_2(x)=\cosh(x)-\cos(x)$$ $$\phi_a(x)=\psi_1(x)\psi_2(a)-\psi_1(a)\psi_2(x)$$ and define $\pi_k(x)=\phi_{a_k}(a_kx)$ where $a_k$ is a sequence such that $\phi'_{a_k}(a_k)=0=\phi'_{-a_k}(-a_k)$ I need to show that those functions are $L_2$-orthogonal $\langle \pi_i,\pi_j \rangle =\delta_{i,j}=\int_0^1\pi_i(x)\pi_j(x)dx$ I am sitting at this problem for quite some time and tried to ""brute-force"" my way throug the calculation (applying partial integration). Is there any quick way to see that those functions are orthogonal ?","Consider the following functions:  $$ \psi_1(x) = \sinh(x)-\sin(x) \quad \quad\psi_2(x)=\cosh(x)-\cos(x)$$ $$\phi_a(x)=\psi_1(x)\psi_2(a)-\psi_1(a)\psi_2(x)$$ and define $\pi_k(x)=\phi_{a_k}(a_kx)$ where $a_k$ is a sequence such that $\phi'_{a_k}(a_k)=0=\phi'_{-a_k}(-a_k)$ I need to show that those functions are $L_2$-orthogonal $\langle \pi_i,\pi_j \rangle =\delta_{i,j}=\int_0^1\pi_i(x)\pi_j(x)dx$ I am sitting at this problem for quite some time and tried to ""brute-force"" my way throug the calculation (applying partial integration). Is there any quick way to see that those functions are orthogonal ?",,"['ordinary-differential-equations', 'normed-spaces', 'lp-spaces', 'orthogonality']"
90,Uniform convergence of Fourier series proof,Uniform convergence of Fourier series proof,,"I want to prove that: If $f(x)$ is continuous with a period of $2\pi$ and its derivative $f^\prime(x)$ is piecewise continuous, then the Fourier series of $f(x)$ converges uniformly to $f(x)$. I'm familiar with the exact same proof (presented below) except for the fact that $f^\prime(x)$ has jump discontinuities. Don't these discontinuities affect the uniform convergence of $f$ ? What changes in the proof if $f^\prime$ is piecewise continuous?","I want to prove that: If $f(x)$ is continuous with a period of $2\pi$ and its derivative $f^\prime(x)$ is piecewise continuous, then the Fourier series of $f(x)$ converges uniformly to $f(x)$. I'm familiar with the exact same proof (presented below) except for the fact that $f^\prime(x)$ has jump discontinuities. Don't these discontinuities affect the uniform convergence of $f$ ? What changes in the proof if $f^\prime$ is piecewise continuous?",,"['ordinary-differential-equations', 'partial-differential-equations', 'fourier-series', 'uniform-convergence']"
91,Applications of First Order Differential Equations,Applications of First Order Differential Equations,,"Can I get help for this question please? Suppose that a tank containing a liquid is vented to the air at the top and has an outlet at the bottom through which the liquid can drain. It follows from Torricelli’s law in physics that if the outlet is opened at time $t = 0$ , then at each instant the depth of the liquid $h(t)$ and the area $A(h)$ of the liquid’s surface are related by $$A(h){dh\over dt} = −k\sqrt h$$ where $k$ is a positive constant that depends on such factors as the viscosity of the liquid and the cross-sectional area of the outlet. Assume that $h$ is in metres, $A(h)$ is in square metres, and $t$ is in seconds. Now a conic tank in the accompanying figure is filled to a depth of 1 metre at time $t = 0$ and suppose that the constant in Torricelli’s law is $k = 0.025$ . (a) Find the depth $h(t)$ at time t while the liquid is draining. (b) How many seconds will it take for the tank to drain completely? This is what I have tried so far: Since the base of the cone is just a circle, the area will then be: $$\mathrm{Area} = {\pi}r^2$$ I thus got the equation: $${\pi}r^2{dh \over dt}= -k {\sqrt h}$$ $${\pi}r^2{dh \over {\sqrt h}}= -k  dt$$ $$\int{1 \over {\sqrt h}} dh = \int {-k \over {\pi}r^2} dt$$ I then got my final equation as: $$h(t) = \left( {-2k \over {\pi}r^2}t + {c \over 2} \right)^2$$","Can I get help for this question please? Suppose that a tank containing a liquid is vented to the air at the top and has an outlet at the bottom through which the liquid can drain. It follows from Torricelli’s law in physics that if the outlet is opened at time , then at each instant the depth of the liquid and the area of the liquid’s surface are related by where is a positive constant that depends on such factors as the viscosity of the liquid and the cross-sectional area of the outlet. Assume that is in metres, is in square metres, and is in seconds. Now a conic tank in the accompanying figure is filled to a depth of 1 metre at time and suppose that the constant in Torricelli’s law is . (a) Find the depth at time t while the liquid is draining. (b) How many seconds will it take for the tank to drain completely? This is what I have tried so far: Since the base of the cone is just a circle, the area will then be: I thus got the equation: I then got my final equation as:",t = 0 h(t) A(h) A(h){dh\over dt} = −k\sqrt h k h A(h) t t = 0 k = 0.025 h(t) \mathrm{Area} = {\pi}r^2 {\pi}r^2{dh \over dt}= -k {\sqrt h} {\pi}r^2{dh \over {\sqrt h}}= -k  dt \int{1 \over {\sqrt h}} dh = \int {-k \over {\pi}r^2} dt h(t) = \left( {-2k \over {\pi}r^2}t + {c \over 2} \right)^2,"['calculus', 'ordinary-differential-equations', 'applications']"
92,The time complexity of solving an ODE,The time complexity of solving an ODE,,"I'm a teacher assistant in an ODE course, and I was thinking about the different methods to solve a linear ODE of order $n$ with constant coefficients, and how long they can get. For example you need to invert a matrix (a very costly operation) and do a bunch of integrals if you want to use variation of parameters, or if you want to use undetermined coefficients, finding the coefficients can be a long process (I don't know the actual computational cost though). I'd like to tell the students that you can't solve the equations faster than this, or that you can actually do it faster, at least theorically, so my question is: is there an upper/lower bound for the time complexity of analytically solving a linear ODE?","I'm a teacher assistant in an ODE course, and I was thinking about the different methods to solve a linear ODE of order $n$ with constant coefficients, and how long they can get. For example you need to invert a matrix (a very costly operation) and do a bunch of integrals if you want to use variation of parameters, or if you want to use undetermined coefficients, finding the coefficients can be a long process (I don't know the actual computational cost though). I'd like to tell the students that you can't solve the equations faster than this, or that you can actually do it faster, at least theorically, so my question is: is there an upper/lower bound for the time complexity of analytically solving a linear ODE?",,"['ordinary-differential-equations', 'computational-complexity']"
93,Finding the Right Initial Conditions for a Three-Body Problem Periodic Solution,Finding the Right Initial Conditions for a Three-Body Problem Periodic Solution,,"The planar restricted circular three-body problem is about computing the trajectory of a small mass, usually a speccraft, that is affected by two larger masses, Earth and the Moon. The differential equation for this problem is given as $$  y_1'' = y_1 + 2y_2' - \mu_2 \frac{y_1+\mu_1}{D_1} - \mu_1 \frac{y_1-\mu_2}{D_2} $$ $$  y_2'' = y_2 - 2y_1' - \mu_2 \frac{y_2}{D_1} -\mu_1 \frac{y_2}{D_2} $$ where $$\mu_1 = \frac{m_1}{m_1+m_2}$$ $$\mu_2 = 1-\mu_1$$ $$ D_1 = ((y_1+\mu_1)^2 + y_2^2 )^{3/2}$$ $$ D_1 = ((y_1-\mu_2)^2 + y_2^2 )^{3/2}$$ For these equations for a certain initial conditions a so-called Arenstorf orbit can be computed which is periodic. The initial conditions are given as (for example in Solving Ordinary Differential Equations I, Nonstiff Problems , pg. 130), $y_1(0) = 0.994$, $y_1'(0)=0$, $y_2'(0) = -2.0015851063790825224053786222$ Every textbook, lecture note repeats these values, which produces, after integration, a nice orbit around earth and moon. But noone seems to know where they come from. Were they computed analytically? Or a trial and error approach was used to find them, or a combination of both? If someone has code that can compute these values it would be much appreciated. Thanks,","The planar restricted circular three-body problem is about computing the trajectory of a small mass, usually a speccraft, that is affected by two larger masses, Earth and the Moon. The differential equation for this problem is given as $$  y_1'' = y_1 + 2y_2' - \mu_2 \frac{y_1+\mu_1}{D_1} - \mu_1 \frac{y_1-\mu_2}{D_2} $$ $$  y_2'' = y_2 - 2y_1' - \mu_2 \frac{y_2}{D_1} -\mu_1 \frac{y_2}{D_2} $$ where $$\mu_1 = \frac{m_1}{m_1+m_2}$$ $$\mu_2 = 1-\mu_1$$ $$ D_1 = ((y_1+\mu_1)^2 + y_2^2 )^{3/2}$$ $$ D_1 = ((y_1-\mu_2)^2 + y_2^2 )^{3/2}$$ For these equations for a certain initial conditions a so-called Arenstorf orbit can be computed which is periodic. The initial conditions are given as (for example in Solving Ordinary Differential Equations I, Nonstiff Problems , pg. 130), $y_1(0) = 0.994$, $y_1'(0)=0$, $y_2'(0) = -2.0015851063790825224053786222$ Every textbook, lecture note repeats these values, which produces, after integration, a nice orbit around earth and moon. But noone seems to know where they come from. Were they computed analytically? Or a trial and error approach was used to find them, or a combination of both? If someone has code that can compute these values it would be much appreciated. Thanks,",,"['calculus', 'ordinary-differential-equations', 'numerical-methods', 'classical-mechanics']"
94,How do I find the singular solution of the differential equation $y' = \frac{y^2 + 1}{xy + y}$?,How do I find the singular solution of the differential equation ?,y' = \frac{y^2 + 1}{xy + y},"I start out with the separable differential equation, $$y' =\frac{dy}{dx} = \frac{y^2 + 1}{xy + y} = \frac{y^2 + 1}{y(x+1)}$$ Thus, $\frac{1 }{x+1}dx = \frac{y }{y^2 + 1}dy$. Then integrating both sides of the equation, I get $$\ln(x+1) = \frac{1}{2}\ln(y^2 +1) + C$$ Now, $e^{\ln(x+1)}$ = $e^{\frac{1}{2}\ln(y^2 +1) + C}$. So... $$(x+1) = e^C(y^2 + 1)^{\frac{1}{2}}$$ I kind of wanted to know if this is indeed the correct general formula. And also, how would I find the singular solution, if there happens to be one in this case.","I start out with the separable differential equation, $$y' =\frac{dy}{dx} = \frac{y^2 + 1}{xy + y} = \frac{y^2 + 1}{y(x+1)}$$ Thus, $\frac{1 }{x+1}dx = \frac{y }{y^2 + 1}dy$. Then integrating both sides of the equation, I get $$\ln(x+1) = \frac{1}{2}\ln(y^2 +1) + C$$ Now, $e^{\ln(x+1)}$ = $e^{\frac{1}{2}\ln(y^2 +1) + C}$. So... $$(x+1) = e^C(y^2 + 1)^{\frac{1}{2}}$$ I kind of wanted to know if this is indeed the correct general formula. And also, how would I find the singular solution, if there happens to be one in this case.",,"['ordinary-differential-equations', 'singular-solution']"
95,Uniqueness of ODE as a consequence of the qualitative behaviour,Uniqueness of ODE as a consequence of the qualitative behaviour,,"I am interested in uniqueness results for ordinary differential equations. Let’s start with a basic example. Consider the initial value problem $\dot{u} = 2\sqrt{|u|}$ with $u(0) =0$. It is well known that this problem has infinitely many solutions. However if we make further assumptions about the behaviour of the solution we can get a unique solution to the right. Assuming for example that $u(t)>0$ for all $t >0$ we deduce that the unique solution is given by $u(t) = t^2$. We see, in some situations further knowledge of the behaviour of the solution is helpful to deduce uniqueness. The one dimensional case is for example discussed in this article . The special situation i am interested in is an ODE the form $\dot{u} = f(u)$ for a continuous function $f \in C(\mathbb{R}^n;\mathbb{R}^n)$. In this situation one can deduce the existence of solutions for a given initial value $u_0$ but  a priori there is no statement about uniqueness of this solution. In spirit of the introductory example it seems natural to ask if one gets uniqueness under additional assumptions. Let’s assume that there is compact subset $ K\subset \mathbb{R}^n$ such that the following assumptions hold. $f$ is locally lipschitz continuous in the interior of $K$. Every solution that starts in the interior of $K$ stays there for all time. Especially there is a unique global solution for such initial values. Given $u_0 \in K$ there is at least one short time solution of the initial value problem. Every of these solution fulfills $u(t) \in K$ for all $t \in (0,t_+)$. Therefore also for initial values on the boundary of $K$ once again one has a global solution. Can one deduce that given an initial value on the boundary the solution under above conditions is once again unique. Even tough we do only assume f to be continuous in a neighborhood of this initial value? Looking at some numerical calculations and the corresponding graphs i think this might be true. Some further questions: Do you know about a two dimensional system where one does not have uniqueness of solutions? And where one can rescue uniqueness with similiar assumptions? I do not see how to generalize the results given 1 to the multi dimensional case. Do you have an idea? Does the knowledge of a lyapunov function help? Some findings: Here one can find a theorem which is somehow what i was thinking of. So however one needs to verify a not so nice estimate on $f$ and its derivatives.","I am interested in uniqueness results for ordinary differential equations. Let’s start with a basic example. Consider the initial value problem $\dot{u} = 2\sqrt{|u|}$ with $u(0) =0$. It is well known that this problem has infinitely many solutions. However if we make further assumptions about the behaviour of the solution we can get a unique solution to the right. Assuming for example that $u(t)>0$ for all $t >0$ we deduce that the unique solution is given by $u(t) = t^2$. We see, in some situations further knowledge of the behaviour of the solution is helpful to deduce uniqueness. The one dimensional case is for example discussed in this article . The special situation i am interested in is an ODE the form $\dot{u} = f(u)$ for a continuous function $f \in C(\mathbb{R}^n;\mathbb{R}^n)$. In this situation one can deduce the existence of solutions for a given initial value $u_0$ but  a priori there is no statement about uniqueness of this solution. In spirit of the introductory example it seems natural to ask if one gets uniqueness under additional assumptions. Let’s assume that there is compact subset $ K\subset \mathbb{R}^n$ such that the following assumptions hold. $f$ is locally lipschitz continuous in the interior of $K$. Every solution that starts in the interior of $K$ stays there for all time. Especially there is a unique global solution for such initial values. Given $u_0 \in K$ there is at least one short time solution of the initial value problem. Every of these solution fulfills $u(t) \in K$ for all $t \in (0,t_+)$. Therefore also for initial values on the boundary of $K$ once again one has a global solution. Can one deduce that given an initial value on the boundary the solution under above conditions is once again unique. Even tough we do only assume f to be continuous in a neighborhood of this initial value? Looking at some numerical calculations and the corresponding graphs i think this might be true. Some further questions: Do you know about a two dimensional system where one does not have uniqueness of solutions? And where one can rescue uniqueness with similiar assumptions? I do not see how to generalize the results given 1 to the multi dimensional case. Do you have an idea? Does the knowledge of a lyapunov function help? Some findings: Here one can find a theorem which is somehow what i was thinking of. So however one needs to verify a not so nice estimate on $f$ and its derivatives.",,['ordinary-differential-equations']
96,"Existence and uniqueness of solution for the IVP : $y'(x) = [x-\cos(y(x))]^{2/5}, \space y(0) = 0$",Existence and uniqueness of solution for the IVP :,"y'(x) = [x-\cos(y(x))]^{2/5}, \space y(0) = 0","Exams Problem : Consider the IVP :   $$y'(x) = [x-\cos(y(x))]^{2/5}, \space y(0) = 0$$   Examine if there exists a solution for the given IVP in an area of $x=0$. Furthermore, examine if conclusions can be made about the uniqueness of the solution. Attempt : As I've went over similar posts, you need to always determine how you define the powers/roots in this case, since it's a detail fact that plays a big role. Let's assume that we consider the real-valued $5th$ root in this particular case. Then, let's consider the function : $$f(x,y) = [x-\cos(y)]^{2/5}=\sqrt[5]{(x-\cos(y))^{2}}$$ Then, the function $f(x,y)$ will be continuous $\forall (x,y) \in \mathbb R^2$ since it is $(x-\cos(y))^2 \geq 0$. This means that the function $f(x,y)$ will also be continuous in a domain, such as : $$D=\{(x,y) \in \mathbb R^2 :|x|\leq a, |y| \leq b\} \space \text{where} \space a,b > 0$$ Obviously, the Picard/Peano theorems can be applied and then truly, there exists a solution for the IVP around $x=0$ with $y(0)=0$. My question is regarding the uniqueness though : One way to determine the uniqueness, is showing that the derivative of $f(x,y)$ with respect to $y$ is bounded or in other words (and approach), Lipschitz Continuous. Simply then (still considering the real valued root branch case) : $$\frac{\partial}{\partial y}f(x,y)= \frac{2\sin(y)}{5\sqrt[5]{(x-\cos(y))^3}}$$ But in such case, we see that $f_y(x,y)$ is not continuous for every value, since it's not always $(x-\cos(y))^3 \geq 0$. This means that the derivative $f_y(x,y)$ won't be continuous in the domain $D$ (which is a standard step for the existence) and thus no conclusions can be made about the uniqueness. In another way, the derivative $f_y(x,y)$ is not bounded (as I think), so still no conclusions can be made. Question : Is the above explanation and reasoning correct though ? Is there a more formal way of showing that $f_y(x,y)$ is not bounded or is my approach mistaken and the derivative could be bounded - defined ?","Exams Problem : Consider the IVP :   $$y'(x) = [x-\cos(y(x))]^{2/5}, \space y(0) = 0$$   Examine if there exists a solution for the given IVP in an area of $x=0$. Furthermore, examine if conclusions can be made about the uniqueness of the solution. Attempt : As I've went over similar posts, you need to always determine how you define the powers/roots in this case, since it's a detail fact that plays a big role. Let's assume that we consider the real-valued $5th$ root in this particular case. Then, let's consider the function : $$f(x,y) = [x-\cos(y)]^{2/5}=\sqrt[5]{(x-\cos(y))^{2}}$$ Then, the function $f(x,y)$ will be continuous $\forall (x,y) \in \mathbb R^2$ since it is $(x-\cos(y))^2 \geq 0$. This means that the function $f(x,y)$ will also be continuous in a domain, such as : $$D=\{(x,y) \in \mathbb R^2 :|x|\leq a, |y| \leq b\} \space \text{where} \space a,b > 0$$ Obviously, the Picard/Peano theorems can be applied and then truly, there exists a solution for the IVP around $x=0$ with $y(0)=0$. My question is regarding the uniqueness though : One way to determine the uniqueness, is showing that the derivative of $f(x,y)$ with respect to $y$ is bounded or in other words (and approach), Lipschitz Continuous. Simply then (still considering the real valued root branch case) : $$\frac{\partial}{\partial y}f(x,y)= \frac{2\sin(y)}{5\sqrt[5]{(x-\cos(y))^3}}$$ But in such case, we see that $f_y(x,y)$ is not continuous for every value, since it's not always $(x-\cos(y))^3 \geq 0$. This means that the derivative $f_y(x,y)$ won't be continuous in the domain $D$ (which is a standard step for the existence) and thus no conclusions can be made about the uniqueness. In another way, the derivative $f_y(x,y)$ is not bounded (as I think), so still no conclusions can be made. Question : Is the above explanation and reasoning correct though ? Is there a more formal way of showing that $f_y(x,y)$ is not bounded or is my approach mistaken and the derivative could be bounded - defined ?",,"['ordinary-differential-equations', 'dynamical-systems', 'roots', 'stability-in-odes']"
97,Solution of heat equation.,Solution of heat equation.,,"Let $u(x,t)$ be the bounded solution of $\frac{\partial  u}{\partial t} -\frac{\partial ^2  u}{\partial x^2}=0$ with $u(x, 0)=\dfrac{e^ {2x} -1}{e^{2x} +1}.$ Then $\lim _{t \rightarrow \infty}u(1, t)=$ $$(A)-\frac{1}{2}~~~~~~(B)\frac{1}{2}~~~~~~(C)-1~~~~~~(D)1.$$ By separation of variable the solution of the above PDE is:    $$u(x, t)=\begin{cases} e^{- \lambda ^2 t}(c_1 \cos(\lambda x)+c_2 \sin(\lambda x)) & ~~~\text{if}~k=-{\lambda}^2<0 ,\\  e^{\lambda ^2 t}(c_1 e^{\lambda x}+c_2 e^{\lambda x}) & ~~~\text{if}~k={\lambda}^2>0 ,\\ c_1x+c_2 & ~~~\text{if}~k=0 .\\ \end{cases}$$   where $k$ is separation constant.   But when $t=0$, I cannot compare the solution with given $u(x,0)$, that's why I cannot find  $\lim _{t \rightarrow \infty}u(1, t).$ Please help.","Let $u(x,t)$ be the bounded solution of $\frac{\partial  u}{\partial t} -\frac{\partial ^2  u}{\partial x^2}=0$ with $u(x, 0)=\dfrac{e^ {2x} -1}{e^{2x} +1}.$ Then $\lim _{t \rightarrow \infty}u(1, t)=$ $$(A)-\frac{1}{2}~~~~~~(B)\frac{1}{2}~~~~~~(C)-1~~~~~~(D)1.$$ By separation of variable the solution of the above PDE is:    $$u(x, t)=\begin{cases} e^{- \lambda ^2 t}(c_1 \cos(\lambda x)+c_2 \sin(\lambda x)) & ~~~\text{if}~k=-{\lambda}^2<0 ,\\  e^{\lambda ^2 t}(c_1 e^{\lambda x}+c_2 e^{\lambda x}) & ~~~\text{if}~k={\lambda}^2>0 ,\\ c_1x+c_2 & ~~~\text{if}~k=0 .\\ \end{cases}$$   where $k$ is separation constant.   But when $t=0$, I cannot compare the solution with given $u(x,0)$, that's why I cannot find  $\lim _{t \rightarrow \infty}u(1, t).$ Please help.",,"['ordinary-differential-equations', 'partial-differential-equations', 'heat-equation']"
98,Determine canonical coordinates,Determine canonical coordinates,,"I am trying to learn how to solve an ODE using symmetry. I have got to the point where I have the generators, now I need to use them to get canonical coordinates, $r$ and $s$ where $r(x,y)$ and $s(x,y)$. If $\eta(x,y)=xy$ and $\xi(x,y)=x^2$ Then the equations to solve in order to determine $r$ and $s$ are : $x^2r_x + xyr_y =0$ and $x^2s_x + xys_y=1$ however, I don't know how to move forward from here. It has been eons since I have solved differential equations. Please can someone walk me through this? UPDATE: I have tried to refresh my memory wrt to solving PDEs.  Here is my attempt: For the first equation I have the following: divide by $x^2$ to get $\frac{\partial r}{\partial x} + \frac{y}{x}\frac{\partial r}{\partial y}=0$ so I need to solve $\frac{dy}{dx}=\frac{y}{x}$ if I separate variables and integrate I get $\ln y =\ln x+C$ $\ln(y/x)=C$ $\frac{y}{x}=C$ Does this mean which means $r(x,y) = \frac{y}{x}$ ? I need to know if this is correct and I need help solving the second equation to find $s(x,y)$. Thanks","I am trying to learn how to solve an ODE using symmetry. I have got to the point where I have the generators, now I need to use them to get canonical coordinates, $r$ and $s$ where $r(x,y)$ and $s(x,y)$. If $\eta(x,y)=xy$ and $\xi(x,y)=x^2$ Then the equations to solve in order to determine $r$ and $s$ are : $x^2r_x + xyr_y =0$ and $x^2s_x + xys_y=1$ however, I don't know how to move forward from here. It has been eons since I have solved differential equations. Please can someone walk me through this? UPDATE: I have tried to refresh my memory wrt to solving PDEs.  Here is my attempt: For the first equation I have the following: divide by $x^2$ to get $\frac{\partial r}{\partial x} + \frac{y}{x}\frac{\partial r}{\partial y}=0$ so I need to solve $\frac{dy}{dx}=\frac{y}{x}$ if I separate variables and integrate I get $\ln y =\ln x+C$ $\ln(y/x)=C$ $\frac{y}{x}=C$ Does this mean which means $r(x,y) = \frac{y}{x}$ ? I need to know if this is correct and I need help solving the second equation to find $s(x,y)$. Thanks",,"['ordinary-differential-equations', 'lie-algebras', 'symmetry']"
99,Finding the integral of a solution to a differential equation,Finding the integral of a solution to a differential equation,,Let $y$ be a solution to the differential equation   $$(1-x^2)y''-2xy'+6y=0$$ If $y(1)=2$ find the value of the integral   $\int_{-1}^1y^2~dx$. I have tried integrating the differential equation which just gives $$\int_{-1}^1\left[(1-x^2)y''-2xy'\right]dx+\int_{-1}^16y~dx\\=\int_{-1}^1\Big[\frac d{dx}(1-x^2)y'\Big]dx+\int_{-1}^16y~dx\\=\int_{-1}^16y~dx=c$$ for some constant of integration $c$. What should I do to find the value of the integral?,Let $y$ be a solution to the differential equation   $$(1-x^2)y''-2xy'+6y=0$$ If $y(1)=2$ find the value of the integral   $\int_{-1}^1y^2~dx$. I have tried integrating the differential equation which just gives $$\int_{-1}^1\left[(1-x^2)y''-2xy'\right]dx+\int_{-1}^16y~dx\\=\int_{-1}^1\Big[\frac d{dx}(1-x^2)y'\Big]dx+\int_{-1}^16y~dx\\=\int_{-1}^16y~dx=c$$ for some constant of integration $c$. What should I do to find the value of the integral?,,['ordinary-differential-equations']
