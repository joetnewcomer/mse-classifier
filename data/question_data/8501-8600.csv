,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"If limit of $|f|$ is divergent to $\infty$, then is $f$ divergent to either $\infty$ or $-\infty$?","If limit of  is divergent to , then is  divergent to either  or ?",|f| \infty f \infty -\infty,"Let $f$ be a real function and $a$ be a limit point of its domain. Suppose $\lim_{x\to a} |f(x)| = \infty$. How do i prove that if $f$ is continuous on its domain, its limit is either $\infty$ or $-\infty$? (And Its domain is connected) I think continuity is essential For example, a function $g:(0,\infty)\rightarrow \mathbb{R}$ such that $g(x)=1/x$ if $x$ is rational and $g(x)=-1/x$ is $x$ is irrational, does not have its limit at $0$.","Let $f$ be a real function and $a$ be a limit point of its domain. Suppose $\lim_{x\to a} |f(x)| = \infty$. How do i prove that if $f$ is continuous on its domain, its limit is either $\infty$ or $-\infty$? (And Its domain is connected) I think continuity is essential For example, a function $g:(0,\infty)\rightarrow \mathbb{R}$ such that $g(x)=1/x$ if $x$ is rational and $g(x)=-1/x$ is $x$ is irrational, does not have its limit at $0$.",,['real-analysis']
1,Arzela-Ascoli: why is boundedness required?,Arzela-Ascoli: why is boundedness required?,,"One of the ways the Arzela-Ascoli Theorem is stated is as follows: Given a compact space $X$ and a set $ M \subset C(X) := \{f: X \rightarrow \mathbb{R}, \| . \|_{\infty}\}$, the following are equivalent: 1) $M$ is bounded, closed and uniformly equicontinuous. 2) $M$ compact. Why is the condition on boundedness required, and does it not follow from uniform equicontinuity? It appears to me that uniform equicontinuity implies (pointwise) continuity of any function $f$ in $M$, and since $X$ is compact and $f$ is continuous, $f(X)$ is compact so $f$ takes its maximum and minimum on its image. In particular, it would follow that $\| f\|_{\infty} < \infty$, for every $f \in M$, so $M$ is bounded. Since the theorem is well-established, it seems to me that there must be some mistake in this argument.","One of the ways the Arzela-Ascoli Theorem is stated is as follows: Given a compact space $X$ and a set $ M \subset C(X) := \{f: X \rightarrow \mathbb{R}, \| . \|_{\infty}\}$, the following are equivalent: 1) $M$ is bounded, closed and uniformly equicontinuous. 2) $M$ compact. Why is the condition on boundedness required, and does it not follow from uniform equicontinuity? It appears to me that uniform equicontinuity implies (pointwise) continuity of any function $f$ in $M$, and since $X$ is compact and $f$ is continuous, $f(X)$ is compact so $f$ takes its maximum and minimum on its image. In particular, it would follow that $\| f\|_{\infty} < \infty$, for every $f \in M$, so $M$ is bounded. Since the theorem is well-established, it seems to me that there must be some mistake in this argument.",,"['real-analysis', 'functional-analysis']"
2,"Compute $\lim_{n\to\infty}\int_0^n \left(1+\frac{x}{2n}\right)^ne^{-x}\,dx$.",Compute .,"\lim_{n\to\infty}\int_0^n \left(1+\frac{x}{2n}\right)^ne^{-x}\,dx","I'm trying to teach myself some analysis (I'm currently studying algebra), and I'm a bit stuck on this question. It's strange because of the $n$ appearing as a limit of integration; I want to apply something like LDCT (I guess), but it doesn't seem that can be done directly. I have noticed that the change of variables $u=1+\frac{x}{2n}$ helps. With this, the problem becomes $$ \lim_{n\to\infty}\int_1^{3/2}2nu^ne^{-2n(u-1)}\,du. $$ This at least solves the issue of the integration limits. Let's let $f_n(u):=2nu^ne^{-2n(u-1)}$ for brevity. I believe it can be shown that $$ \lim_{n\to\infty}f_n(u)=\cases{\infty,\,u=1\\0,\,1<u\leq 3/2} $$ using L'Hopital's rule and the fact that $u^n$ intersects $e^{2n(u-1)}$ where $u=1$, and so the exponential function is larger than $u^n$ for $n>1$. I think I was also able to show that $\{f_n\}$ is eventually decreasing on $(1,3/2]$, and so Dini's Theorem says that the sequence is uniformly convergent to $0$ on $[u_0,3/2]$ for any $u_0\in (1,3/2]$. Since each $f_n$ is continuous on the closed and bounded interval $[u_0,3/2]$, each is bounded; as the convergence is uniform, the sequence is uniformly bounded. Thus, the Lebesgue Dominated Convergence Theorem says $$ \lim_{n\to\infty}\int_{u_0}^{3/2}2nu^ne^{-2n(u-1)}\,du=\int_{u_0}^{3/2}0\,du=0. $$ So it looks like I'm almost there, I just need to extend the lower limit all the way to $1$. I think this amounts to asking whether we can switch the order of the limits in $$\lim_{n\to\infty}\lim_{u_0\to 1^+}\int_{u_0}^{3/2}2nu^ne^{-2n(u-1)}\,du, $$ and (finally!) this is where I'm stuck. I feel like this step should be easy, and it's quite possible I'm missing something obvious. That happens a lot when I try to do analysis because of my practically nonexistent background.","I'm trying to teach myself some analysis (I'm currently studying algebra), and I'm a bit stuck on this question. It's strange because of the $n$ appearing as a limit of integration; I want to apply something like LDCT (I guess), but it doesn't seem that can be done directly. I have noticed that the change of variables $u=1+\frac{x}{2n}$ helps. With this, the problem becomes $$ \lim_{n\to\infty}\int_1^{3/2}2nu^ne^{-2n(u-1)}\,du. $$ This at least solves the issue of the integration limits. Let's let $f_n(u):=2nu^ne^{-2n(u-1)}$ for brevity. I believe it can be shown that $$ \lim_{n\to\infty}f_n(u)=\cases{\infty,\,u=1\\0,\,1<u\leq 3/2} $$ using L'Hopital's rule and the fact that $u^n$ intersects $e^{2n(u-1)}$ where $u=1$, and so the exponential function is larger than $u^n$ for $n>1$. I think I was also able to show that $\{f_n\}$ is eventually decreasing on $(1,3/2]$, and so Dini's Theorem says that the sequence is uniformly convergent to $0$ on $[u_0,3/2]$ for any $u_0\in (1,3/2]$. Since each $f_n$ is continuous on the closed and bounded interval $[u_0,3/2]$, each is bounded; as the convergence is uniform, the sequence is uniformly bounded. Thus, the Lebesgue Dominated Convergence Theorem says $$ \lim_{n\to\infty}\int_{u_0}^{3/2}2nu^ne^{-2n(u-1)}\,du=\int_{u_0}^{3/2}0\,du=0. $$ So it looks like I'm almost there, I just need to extend the lower limit all the way to $1$. I think this amounts to asking whether we can switch the order of the limits in $$\lim_{n\to\infty}\lim_{u_0\to 1^+}\int_{u_0}^{3/2}2nu^ne^{-2n(u-1)}\,du, $$ and (finally!) this is where I'm stuck. I feel like this step should be easy, and it's quite possible I'm missing something obvious. That happens a lot when I try to do analysis because of my practically nonexistent background.",,"['real-analysis', 'measure-theory', 'integration', 'limits']"
3,Bounded set has finite outer measure,Bounded set has finite outer measure,,"The definition of outer measure is as follows: $$m^*(E) = \inf \left\{ \sum\limits_{k=1}^{\infty} l(I_k) : A \subset \bigcup\limits_{k=1}^{\infty} I_k \right\}$$ If a set is bounded then for some $p\in E$ every $e \in E$, $d(e, p) \leq M$. In my mind I interpret this as saying that every point has a finite distance less than or equal to M.  However, I'm not sure how to translate that to the definition of outer measure.  My first idea was that the intervals $I_k$ are the lengths between a fixed $p$ and $e$.  Since they would all clearly be less than $M$, then they are all finite.  Thus the outer measure must be finite.","The definition of outer measure is as follows: $$m^*(E) = \inf \left\{ \sum\limits_{k=1}^{\infty} l(I_k) : A \subset \bigcup\limits_{k=1}^{\infty} I_k \right\}$$ If a set is bounded then for some $p\in E$ every $e \in E$, $d(e, p) \leq M$. In my mind I interpret this as saying that every point has a finite distance less than or equal to M.  However, I'm not sure how to translate that to the definition of outer measure.  My first idea was that the intervals $I_k$ are the lengths between a fixed $p$ and $e$.  Since they would all clearly be less than $M$, then they are all finite.  Thus the outer measure must be finite.",,"['real-analysis', 'measure-theory']"
4,Generating a $\sigma$-algebra from a $\sigma$-ring,Generating a -algebra from a -ring,\sigma \sigma,"Here is exercise 1.1(c) from Folland's Real Analysis : If $\mathcal{R}$ is a $\sigma$-ring, then $\mathcal{M}= \{ E\subset X : E \in \mathcal{R} \text{ or } E^c \in \mathcal{R} \} $ is a $\sigma$-algebra. Recall that a family of sets $\mathcal{R} \subseteq \mathcal{P}(X)$ is a $\sigma$-ring if it is closed under differences (i.e., if $E,F \in \mathcal{R}$ then $E \setminus F \in \mathcal{R}$) and countable unions. If I take $X = \mathbb{R}$, $\mathcal{R} = \{\{0\}\}$, which is definitely a $\sigma$-ring, then $\{0\}, \mathbb{R}\setminus \{0\} \in \mathcal{M}$, but $\mathbb{R} = \{0\} \cup \mathbb{R}\setminus \{0\} \not\in \mathcal{R}$, nor $\emptyset \in \mathcal{R}$. What's wrong here?","Here is exercise 1.1(c) from Folland's Real Analysis : If $\mathcal{R}$ is a $\sigma$-ring, then $\mathcal{M}= \{ E\subset X : E \in \mathcal{R} \text{ or } E^c \in \mathcal{R} \} $ is a $\sigma$-algebra. Recall that a family of sets $\mathcal{R} \subseteq \mathcal{P}(X)$ is a $\sigma$-ring if it is closed under differences (i.e., if $E,F \in \mathcal{R}$ then $E \setminus F \in \mathcal{R}$) and countable unions. If I take $X = \mathbb{R}$, $\mathcal{R} = \{\{0\}\}$, which is definitely a $\sigma$-ring, then $\{0\}, \mathbb{R}\setminus \{0\} \in \mathcal{M}$, but $\mathbb{R} = \{0\} \cup \mathbb{R}\setminus \{0\} \not\in \mathcal{R}$, nor $\emptyset \in \mathcal{R}$. What's wrong here?",,['real-analysis']
5,The distance function is continuous.,The distance function is continuous.,,"Let $S\subset\Bbb R$ not empty, define $f:\Bbb R\rightarrow\Bbb R$ such that $f(x)= \inf\{|x-s| ;s\in S\}$ then, prove that  $|f(x)-f(y)|\le|x-y| $ for any $x,y \in \Bbb R$","Let $S\subset\Bbb R$ not empty, define $f:\Bbb R\rightarrow\Bbb R$ such that $f(x)= \inf\{|x-s| ;s\in S\}$ then, prove that  $|f(x)-f(y)|\le|x-y| $ for any $x,y \in \Bbb R$",,"['real-analysis', 'analysis']"
6,Is a function globally Lipschitz continuous and $\mathcal{C}^1$ if and only if it is $\mathcal{C}^1$ and its total derivative is bounded?,Is a function globally Lipschitz continuous and  if and only if it is  and its total derivative is bounded?,\mathcal{C}^1 \mathcal{C}^1,"Is $f:\mathbb{R}^n\rightarrow\mathbb{R}^n$ globally Lipschitz continuous, i.e. there exists an $L>0$ such that $\frac{|f(x)-f(y)|}{|x-y|}\leq L$ for all $x,y\in\mathbb{R}^n$, and $\mathcal{C}^1$ if and only if $f$ is $\mathcal{C}^1$ and its total derivative is bounded? Based on intuition alone, I'm strongly inclined to believe that the answer is yes. However I'm having trouble coming up with a proof (probably because my grasp of multivariable calculus is far from great). Could someone give one if the statement is true, or provide a counter example if it is false? Thanks.","Is $f:\mathbb{R}^n\rightarrow\mathbb{R}^n$ globally Lipschitz continuous, i.e. there exists an $L>0$ such that $\frac{|f(x)-f(y)|}{|x-y|}\leq L$ for all $x,y\in\mathbb{R}^n$, and $\mathcal{C}^1$ if and only if $f$ is $\mathcal{C}^1$ and its total derivative is bounded? Based on intuition alone, I'm strongly inclined to believe that the answer is yes. However I'm having trouble coming up with a proof (probably because my grasp of multivariable calculus is far from great). Could someone give one if the statement is true, or provide a counter example if it is false? Thanks.",,"['real-analysis', 'multivariable-calculus']"
7,Limsups of nets,Limsups of nets,,"The limsup on sequences of extended real numbers is usually taken to be either of these two things, which are equivalent: the sup of all subsequential limits. The limit of the sup of the tail ends of the sequence. For the situation with nets, the same arguments guarantee the existence of the above quantities 1. and 2, as long as you understand that a subnet of a net must be precomposed with an increasing function that is also cofinal.  Also, one has that 1. $\leq$ 2., and I just can't see the reverse inequality.  Don't forget that one cannot imitate the argument for sequences because the following fact fails: Given a directed set, how do I construct a net that converges to $0$.","The limsup on sequences of extended real numbers is usually taken to be either of these two things, which are equivalent: the sup of all subsequential limits. The limit of the sup of the tail ends of the sequence. For the situation with nets, the same arguments guarantee the existence of the above quantities 1. and 2, as long as you understand that a subnet of a net must be precomposed with an increasing function that is also cofinal.  Also, one has that 1. $\leq$ 2., and I just can't see the reverse inequality.  Don't forget that one cannot imitate the argument for sequences because the following fact fails: Given a directed set, how do I construct a net that converges to $0$.",,"['real-analysis', 'analysis', 'limsup-and-liminf', 'nets']"
8,What is the precise definition of predual,What is the precise definition of predual,,"How does one define ""predual"" and the surrounding notions?  More specifically: Why must there be only one predual of $X$ when $X$ is a Banach space?  What is the correct notion of similarity here that gives this uniqueness?  Is it isomorphic homeomorphism of Banach spaces? I'm also interested in the corresponding algebraic statement.  Is it true that if $V$ is a vector space, then it has at most one predual? I have noticed from looking online that the predual of $B(H)$ is the trace class operators, and the predual of that is the compact operators, which strangely enough means that taking preduals doesn't always reduce the ""size"" of the space (I'm not able to be precise since I don't know the true meaning of the uniqueness of predual), even though in the algebraic setting, one always has the usual injection of a vector space into its dual.  I suppose that this discrepancy is because in the analytic definition of dual, we require continuity, so that the dual vector of a vector $x$ in $X$ when $X$ is a Banach space need not actually be in the continuous dual $X^*$ of $X$?","How does one define ""predual"" and the surrounding notions?  More specifically: Why must there be only one predual of $X$ when $X$ is a Banach space?  What is the correct notion of similarity here that gives this uniqueness?  Is it isomorphic homeomorphism of Banach spaces? I'm also interested in the corresponding algebraic statement.  Is it true that if $V$ is a vector space, then it has at most one predual? I have noticed from looking online that the predual of $B(H)$ is the trace class operators, and the predual of that is the compact operators, which strangely enough means that taking preduals doesn't always reduce the ""size"" of the space (I'm not able to be precise since I don't know the true meaning of the uniqueness of predual), even though in the algebraic setting, one always has the usual injection of a vector space into its dual.  I suppose that this discrepancy is because in the analytic definition of dual, we require continuity, so that the dual vector of a vector $x$ in $X$ when $X$ is a Banach space need not actually be in the continuous dual $X^*$ of $X$?",,"['linear-algebra', 'real-analysis', 'banach-spaces', 'operator-theory']"
9,Riemann Integration Problem,Riemann Integration Problem,,"Let $f$ be a continuous function on $[0,1]$ satisfying $$\int_0^1f(x)\,dx = 0$$ and $$\int_0^1xf(x)\,dx = 0.$$ Show that there exists $a$,$b$ in $[0,1]$ with $a < b$,  such that $f(a) = 0 =f(b)$. Existence of one point is clear to me but I cannot prove the existence of the other one. Thanks for any help.","Let $f$ be a continuous function on $[0,1]$ satisfying $$\int_0^1f(x)\,dx = 0$$ and $$\int_0^1xf(x)\,dx = 0.$$ Show that there exists $a$,$b$ in $[0,1]$ with $a < b$,  such that $f(a) = 0 =f(b)$. Existence of one point is clear to me but I cannot prove the existence of the other one. Thanks for any help.",,['real-analysis']
10,what's the ordinary derivative of the kronecker delta function?,what's the ordinary derivative of the kronecker delta function?,,"What's ordinary derivative of the kronecker delta function? I have used ""ordinary"" in order not to confuse the reader with the covariant derivative. I have tried the following:  $$\delta[x-n]=\frac{1}{2\pi}\int_{0}^{2\pi}e^{i(x-n)t}dt$$  but that doesn't work since. $x,n \in \mathbb{Z}$, while I look for the case $x \in \mathbb{R}$","What's ordinary derivative of the kronecker delta function? I have used ""ordinary"" in order not to confuse the reader with the covariant derivative. I have tried the following:  $$\delta[x-n]=\frac{1}{2\pi}\int_{0}^{2\pi}e^{i(x-n)t}dt$$  but that doesn't work since. $x,n \in \mathbb{Z}$, while I look for the case $x \in \mathbb{R}$",,"['calculus', 'real-analysis']"
11,Approximation of a bounded measurable function with step functions?,Approximation of a bounded measurable function with step functions?,,"I'm having trouble judging whether this statement is correct: For an arbitrary bounded measurable function $f$ defined on $[0,1]$, $\exists{}\ $a sequence of step functions $\{\phi_n\}$, such that $\{\phi_n\}$ converges to $f$ pointwisely a.e. on $[0,1]$. By the Simple Approximation Theorem, this is true if we are allowed to use simple functions. But I am curious whether this still holds when we restrict ourselves to step functions only. I have a feeling that this may not be true because for a measurable function, its domain may be too ""broken up"" to be fitted by step functions. But I don't know how to find a counter-example... So can anybody help me find a counterexample or confirm that this is correct? Thank you very much! Edit: By a step function I mean a (finite) linear combination of indicator functions for intervals.","I'm having trouble judging whether this statement is correct: For an arbitrary bounded measurable function $f$ defined on $[0,1]$, $\exists{}\ $a sequence of step functions $\{\phi_n\}$, such that $\{\phi_n\}$ converges to $f$ pointwisely a.e. on $[0,1]$. By the Simple Approximation Theorem, this is true if we are allowed to use simple functions. But I am curious whether this still holds when we restrict ourselves to step functions only. I have a feeling that this may not be true because for a measurable function, its domain may be too ""broken up"" to be fitted by step functions. But I don't know how to find a counter-example... So can anybody help me find a counterexample or confirm that this is correct? Thank you very much! Edit: By a step function I mean a (finite) linear combination of indicator functions for intervals.",,"['real-analysis', 'measure-theory', 'approximation']"
12,convergence with respect to integral norm but not pointwise,convergence with respect to integral norm but not pointwise,,"I want to give an example of a sequence of functions $f_1 \dots f_n$ that converges with respect to the metric $d(f,g) = \int_a^b |f(x) - g(x)| dx$ but does not converge pointwise. I'm thinking of a function $f_n$ that is piecewise triangle, whose area converges to some constant function, but doesn't converge pointwise. I just can't manage to formalize it.","I want to give an example of a sequence of functions $f_1 \dots f_n$ that converges with respect to the metric $d(f,g) = \int_a^b |f(x) - g(x)| dx$ but does not converge pointwise. I'm thinking of a function $f_n$ that is piecewise triangle, whose area converges to some constant function, but doesn't converge pointwise. I just can't manage to formalize it.",,['real-analysis']
13,Extending a continuous function defined on the rationals,Extending a continuous function defined on the rationals,,"Is there an elementary way of proving that for any continuous function $f:\mathbb{Q}\to[0,1]$ there is such an $x\in\mathbb{R}\setminus\mathbb{Q}$ that $f$ can be extended to a continuous function $\mathbb{Q}\cup\{x\}\to[0,1]$, without resorting to the fact that $\mathbb{Q}$ is not a $G_\delta$-set in $\mathbb{R}$ and the Theorem (4.3.20. in General Topology by Engelking): If $Y$ is a completely metrizable space, then every continuous mapping $f:A\to Y$ from a dense subset of a topological space $X$ to the space $Y$ is extendable to a continuous mapping $F:B\to Y$ defined on a $G_\delta$-set $B\subset X$ containing $A$. which seem an overkill to me in this case?","Is there an elementary way of proving that for any continuous function $f:\mathbb{Q}\to[0,1]$ there is such an $x\in\mathbb{R}\setminus\mathbb{Q}$ that $f$ can be extended to a continuous function $\mathbb{Q}\cup\{x\}\to[0,1]$, without resorting to the fact that $\mathbb{Q}$ is not a $G_\delta$-set in $\mathbb{R}$ and the Theorem (4.3.20. in General Topology by Engelking): If $Y$ is a completely metrizable space, then every continuous mapping $f:A\to Y$ from a dense subset of a topological space $X$ to the space $Y$ is extendable to a continuous mapping $F:B\to Y$ defined on a $G_\delta$-set $B\subset X$ containing $A$. which seem an overkill to me in this case?",,"['real-analysis', 'general-topology']"
14,Definition of the Derivative Using a Sequence,Definition of the Derivative Using a Sequence,,"I believe that I may have once seen a definition of the derivative that went along these lines: $$f'(c)=\lim_{n\to\infty}\frac{f(x_n)-f(c)}{x_n-c}$$ Here, $(x_n)$ is a sequence that converges to $c$. As I do not recall much about it, I may have left out one or two important assumptions. However, does this seem in any way correct? And if so, does this theorem/etc. have a name? Thanks in advance!","I believe that I may have once seen a definition of the derivative that went along these lines: $$f'(c)=\lim_{n\to\infty}\frac{f(x_n)-f(c)}{x_n-c}$$ Here, $(x_n)$ is a sequence that converges to $c$. As I do not recall much about it, I may have left out one or two important assumptions. However, does this seem in any way correct? And if so, does this theorem/etc. have a name? Thanks in advance!",,['real-analysis']
15,Verifying an inequality under certain conditions.,Verifying an inequality under certain conditions.,,"Let $\mu$ be a positive measure on a measure space $\Omega$. How can I show that for all non-negative measurable $f$ $$\exp\left(\int_\Omega f \; d\mu \right) \leq \int_\Omega \exp(f) \; d\mu$$ the above inequality is valid if $\mu(\Omega) = 1$ and not valid if $\mu(\Omega) \neq 1.$ This is what I've been able to come up with. Since $\exp$ is convex, if $\mu(\Omega) =1$, then the inequality is just Jensen's inequality. Do I have to say more? How do I show that the inequality is not valid if $\mu(\Omega) \neq 1$.","Let $\mu$ be a positive measure on a measure space $\Omega$. How can I show that for all non-negative measurable $f$ $$\exp\left(\int_\Omega f \; d\mu \right) \leq \int_\Omega \exp(f) \; d\mu$$ the above inequality is valid if $\mu(\Omega) = 1$ and not valid if $\mu(\Omega) \neq 1.$ This is what I've been able to come up with. Since $\exp$ is convex, if $\mu(\Omega) =1$, then the inequality is just Jensen's inequality. Do I have to say more? How do I show that the inequality is not valid if $\mu(\Omega) \neq 1$.",,"['real-analysis', 'measure-theory']"
16,$f(x)$ and $h(x)$ are absolutely continuous functions. Is $e^{f(x)} |h(x)|$ as well?,and  are absolutely continuous functions. Is  as well?,f(x) h(x) e^{f(x)} |h(x)|,"Given that functions $f(x)$ and $h(x)$ are absolutely continuous on $[0,1]$, I want to show that $e^{f(x)} |h(x)|$ is absolutely continuous as well. I know that (1) the product of two absolutely continuous function on $[0,1]$ is absolutely continuous.  (2) the composition of a Lipschitz continuous function and an absolutely continuous function is absolutely continuous. So $|h|$ is absolutely continuous, But the exponential function $e^x$ is not Lipschitz, so not absolutely continuous. What's the key to solve the problem here?","Given that functions $f(x)$ and $h(x)$ are absolutely continuous on $[0,1]$, I want to show that $e^{f(x)} |h(x)|$ is absolutely continuous as well. I know that (1) the product of two absolutely continuous function on $[0,1]$ is absolutely continuous.  (2) the composition of a Lipschitz continuous function and an absolutely continuous function is absolutely continuous. So $|h|$ is absolutely continuous, But the exponential function $e^x$ is not Lipschitz, so not absolutely continuous. What's the key to solve the problem here?",,['real-analysis']
17,Approximate a positive Sobolev function by positive smooth functions,Approximate a positive Sobolev function by positive smooth functions,,"Here is a problem that I have encountered in PDE book several times. But I have never seen a proof of it. I will be very grateful if someone could give me a proof. Question: Let $B$ be the unit ball in $\mathbb{R}^n$, $f$ a non-negative function in $H_0^1(B)$, prove that there exists a sequence of non-negative functions $\varphi_k\in C_c^\infty(B)$ such that $\varphi_k\rightarrow f$ in $H_0^1(B)$. Edit: What if we replace $B$ by a general domain $\Omega$? Edit II: Thanks to Hans's idea (which should work for any star-shaped domain), if the boundary of $\Omega$ suitably good (for example, it admits finite covering of star-shaped open sets), then using partition of unity we should be able to construct the desired approximation. Edit III: If I didn't make any mistake. L.C. Evans's Partial Differential Equations (First Edition) page 260 gives a proof for $C^1$ domain. Although he was actually proving something else, the key ingredient works in our situation!","Here is a problem that I have encountered in PDE book several times. But I have never seen a proof of it. I will be very grateful if someone could give me a proof. Question: Let $B$ be the unit ball in $\mathbb{R}^n$, $f$ a non-negative function in $H_0^1(B)$, prove that there exists a sequence of non-negative functions $\varphi_k\in C_c^\infty(B)$ such that $\varphi_k\rightarrow f$ in $H_0^1(B)$. Edit: What if we replace $B$ by a general domain $\Omega$? Edit II: Thanks to Hans's idea (which should work for any star-shaped domain), if the boundary of $\Omega$ suitably good (for example, it admits finite covering of star-shaped open sets), then using partition of unity we should be able to construct the desired approximation. Edit III: If I didn't make any mistake. L.C. Evans's Partial Differential Equations (First Edition) page 260 gives a proof for $C^1$ domain. Although he was actually proving something else, the key ingredient works in our situation!",,"['real-analysis', 'partial-differential-equations', 'banach-spaces', 'approximation', 'sobolev-spaces']"
18,Can f not be in L1 if its Fourier transform is in L-infinity?,Can f not be in L1 if its Fourier transform is in L-infinity?,,"Can $f\notin L^1$ if its Fourier transform $\hat f \in L^\infty$  ? This is a question about the endpoint of Pontryagin duality.  We know that if a function is in $L^1$, then its Fourier transform lies in $L^\infty$. This is very easy to show.  But what about the converse: Can the $L^1$ norm of $f$ be infinite even if the $L^\infty$ norm of its Fourier transform is finite? I assume that the answer to my question is YES, but I do not see how to handle this case. Any references?","Can $f\notin L^1$ if its Fourier transform $\hat f \in L^\infty$  ? This is a question about the endpoint of Pontryagin duality.  We know that if a function is in $L^1$, then its Fourier transform lies in $L^\infty$. This is very easy to show.  But what about the converse: Can the $L^1$ norm of $f$ be infinite even if the $L^\infty$ norm of its Fourier transform is finite? I assume that the answer to my question is YES, but I do not see how to handle this case. Any references?",,['real-analysis']
19,Convergence in $L^q$ of an $L^p$-bounded sequence for $q<p$,Convergence in  of an -bounded sequence for,L^q L^p q<p,"Let $1< p < \infty$, $\{f_n\}_n \subset L^p[0,1]$ s.t. $f_n:[0,1] \to \mathbb{R}$, $f_n \to f$ a.e., and $||f_n||_{L^p} \leq M < \infty$ for all $n$. Then, given $1 \leq q < p$, we want to show that $f_n \to f$ in $L^q$. I think that you can show this result by using the Rellich-Kondrachov theorem (i.e. $L^p \hookrightarrow L^q$ compactly for $q < p$) to extract a convergent subsequence in $L^q$. Then you can use the fact that $f_n \to f$ a.e. to show that the whole sequence must converge to $f$ in $L^q$. However, I was wondering if this approach might be a little overkill, and if I should be using some less overpowered tools to prove the statement (assuming the argument above is valid).","Let $1< p < \infty$, $\{f_n\}_n \subset L^p[0,1]$ s.t. $f_n:[0,1] \to \mathbb{R}$, $f_n \to f$ a.e., and $||f_n||_{L^p} \leq M < \infty$ for all $n$. Then, given $1 \leq q < p$, we want to show that $f_n \to f$ in $L^q$. I think that you can show this result by using the Rellich-Kondrachov theorem (i.e. $L^p \hookrightarrow L^q$ compactly for $q < p$) to extract a convergent subsequence in $L^q$. Then you can use the fact that $f_n \to f$ a.e. to show that the whole sequence must converge to $f$ in $L^q$. However, I was wondering if this approach might be a little overkill, and if I should be using some less overpowered tools to prove the statement (assuming the argument above is valid).",,['real-analysis']
20,under which conditions $\lim_{t\to \infty} \frac{1}{t}\int_0^t f(s)ds$ implies $\lim_{t\to \infty} f(t) $,under which conditions  implies,\lim_{t\to \infty} \frac{1}{t}\int_0^t f(s)ds \lim_{t\to \infty} f(t) ,"I am interested in knowing under which conditions $\lim_{t\to \infty} \frac{1}{t}\int_0^t f(s)ds$ implies that $\lim_{t\to \infty} f(t)$ exist. It is known that $\lim_{t\to \infty} \frac{1}{t}\int_0^t f(s)ds$ can exist, but $\lim_{t\to \infty} f(t) $ not. For instace $f(t) = \cos t$ . First, I wonder if it is enough that $f(t)$ is continuous and $\lim_{t\to \infty} \frac{1}{t}\int_0^t f(s)ds$ converges to the maximum value of the image. However, I found this counter-example: Let us define $f(t)$ by cases $f(t):=  1 -2(t-(10^{n-1}-1))$ if $t \in [10^{n-1}-1,10^{n-1}-\frac{1}{2}]$ , $f(t):= 2(t-(10^{n-1}-\frac{1}{2}))$ if $t \in [10^{n-1}-\frac{1}{2},10^{n-1}]$ , $f(t) := 1$ if $t \in [10^{n-1},10^n-1] $ Where $n = 1,2,3,\dots$ . We can provide a smooth example using the same argument and bump function. However $f(t)$ is not analytic. Since the proof relies that $f(t)$ is constant function in open interval. I wonder if $f(t)$ being analytic is enough. Is true the following statement? Let $f(t):[0,\infty] \to [-1,1]$ be an analytic function. If $\lim_{t\to \infty} \frac{1}{t}\int_0^t f(s)ds = 1$ , then $\lim_{t\to \infty} f(t) = 1$ .","I am interested in knowing under which conditions implies that exist. It is known that can exist, but not. For instace . First, I wonder if it is enough that is continuous and converges to the maximum value of the image. However, I found this counter-example: Let us define by cases if , if , if Where . We can provide a smooth example using the same argument and bump function. However is not analytic. Since the proof relies that is constant function in open interval. I wonder if being analytic is enough. Is true the following statement? Let be an analytic function. If , then .","\lim_{t\to \infty} \frac{1}{t}\int_0^t f(s)ds \lim_{t\to \infty} f(t) \lim_{t\to \infty} \frac{1}{t}\int_0^t f(s)ds \lim_{t\to \infty} f(t)  f(t) = \cos t f(t) \lim_{t\to \infty} \frac{1}{t}\int_0^t f(s)ds f(t) f(t):=  1 -2(t-(10^{n-1}-1)) t \in [10^{n-1}-1,10^{n-1}-\frac{1}{2}] f(t):= 2(t-(10^{n-1}-\frac{1}{2})) t \in [10^{n-1}-\frac{1}{2},10^{n-1}] f(t) := 1 t \in [10^{n-1},10^n-1]  n = 1,2,3,\dots f(t) f(t) f(t) f(t):[0,\infty] \to [-1,1] \lim_{t\to \infty} \frac{1}{t}\int_0^t f(s)ds = 1 \lim_{t\to \infty} f(t) = 1","['real-analysis', 'limits', 'average', 'analytic-functions']"
21,About the constant from the Marcinkiewicz Interpolation Theorem,About the constant from the Marcinkiewicz Interpolation Theorem,,"The Marcinkiewicz Interpolation Theorem states as following: Let $T$ be a linear operator of weak type $(p_0,q_0)$ with constant $C_0$ and of weak type $(p_1,q_1)$ with constant $C_1$ where $q_0\neq q_1$ . Now for any $t\in (0,1)$ , define $p_t,q_t$ by $$ \frac{1}{p_t} = \frac{1-t}{p_0}+\frac{t}{p_1},\quad \frac{1}{q_t} = \frac{1-t}{q_0} + \frac{t}{q_1}. $$ Then $T$ is strong type $(p_t,q_t)$ with constant $C_t$ . Moreover, this constant $C_t$ satisfies $$ C_t\leq C C_0^{1-t} C_1^t, $$ where $C=C(p_0, p_1, q_0, q_1, t)$ is bounded for $0<\epsilon \leq t\leq 1-\epsilon<1$ but tends to infinity as $t\to 0$ or $t\to 1$ . When we say an operator $T$ is of strong type $(p,q)$ with constant $C$ , it means $|| Tf||_q \leq C||f||_p$ . When we say an operator $T$ is of weak type $(p,q)$ with constant $C$ , it means $|| T f ||_{q,w}\leq C||f||_p $ . Every proof of this Theorem (for instance in Folland's ""Real Analysis"" or more accessibly in this writeup ) that I have found has worked by obtaining an inequality involving the distribution function of $Tf$ of the form $$ \lambda_{Tf}(2\alpha)\leq \lambda_{Tg_A}(\alpha)+\lambda_{Th_A}(\alpha). $$ Then by applying the weak type estimates of $T$ and some clever integral manipulation, one obtains a final inequality like: $$ ||Tf||_{q_t}\leq 2 q_t^{\frac{1}{q_t}}\left[C_0^{q_0}(p_0/p_t)^{\frac{q_0}{p_0}}|q_t-q_0|^{-1}+C_1^{q_1}(p_1/p_t)^{\frac{q_1}{p_1}}|q_t-q_1|^{-1}\right]^{\frac{1}{q_t}}=:C_t, $$ for all $f$ with $||f||_p=1$ . Then the general inequality follows from the linearity of $T$ because $|T(cf)|=c|T(f)|$ for $c>0$ . My question is as follows: I don't see how we can obtain from this that $C_t\leq C C_0^{1-t} C_1^t$ . For example, if we take $q_0=3$ , $q_1=6$ , $t=\frac{1}{2}$ , and $q_t=4$ which satisfies the hypothesis $$ \frac{1}{q_t} = \frac{1-t}{q_0} + \frac{t}{q_1}, $$ then (for some given choice of $p_0, p_1, p_t$ ) the quantity $$ \frac{\left[C_0^3 +C_1^6 \right]^{\frac{1}{4}}}{C_0^\frac{1}{2}C_1^\frac{1}{2}} $$ should remain bounded with respect to $C_0,C_1$ . But it clearly doesn't (Just take $C_0=1$ or something and ramp $C_1\to \infty$ ). So how do we obtain the inequality $C_t\leq C C_0^{1-t} C_1^t$ ? Is there an alternative proof of this?","The Marcinkiewicz Interpolation Theorem states as following: Let be a linear operator of weak type with constant and of weak type with constant where . Now for any , define by Then is strong type with constant . Moreover, this constant satisfies where is bounded for but tends to infinity as or . When we say an operator is of strong type with constant , it means . When we say an operator is of weak type with constant , it means . Every proof of this Theorem (for instance in Folland's ""Real Analysis"" or more accessibly in this writeup ) that I have found has worked by obtaining an inequality involving the distribution function of of the form Then by applying the weak type estimates of and some clever integral manipulation, one obtains a final inequality like: for all with . Then the general inequality follows from the linearity of because for . My question is as follows: I don't see how we can obtain from this that . For example, if we take , , , and which satisfies the hypothesis then (for some given choice of ) the quantity should remain bounded with respect to . But it clearly doesn't (Just take or something and ramp ). So how do we obtain the inequality ? Is there an alternative proof of this?","T (p_0,q_0) C_0 (p_1,q_1) C_1 q_0\neq q_1 t\in (0,1) p_t,q_t 
\frac{1}{p_t} = \frac{1-t}{p_0}+\frac{t}{p_1},\quad
\frac{1}{q_t} = \frac{1-t}{q_0} + \frac{t}{q_1}.
 T (p_t,q_t) C_t C_t 
C_t\leq C C_0^{1-t} C_1^t,
 C=C(p_0, p_1, q_0, q_1, t) 0<\epsilon \leq t\leq 1-\epsilon<1 t\to 0 t\to 1 T (p,q) C || Tf||_q \leq C||f||_p T (p,q) C || T f ||_{q,w}\leq C||f||_p  Tf 
\lambda_{Tf}(2\alpha)\leq \lambda_{Tg_A}(\alpha)+\lambda_{Th_A}(\alpha).
 T 
||Tf||_{q_t}\leq 2 q_t^{\frac{1}{q_t}}\left[C_0^{q_0}(p_0/p_t)^{\frac{q_0}{p_0}}|q_t-q_0|^{-1}+C_1^{q_1}(p_1/p_t)^{\frac{q_1}{p_1}}|q_t-q_1|^{-1}\right]^{\frac{1}{q_t}}=:C_t,
 f ||f||_p=1 T |T(cf)|=c|T(f)| c>0 C_t\leq C C_0^{1-t} C_1^t q_0=3 q_1=6 t=\frac{1}{2} q_t=4 
\frac{1}{q_t} = \frac{1-t}{q_0} + \frac{t}{q_1},
 p_0, p_1, p_t 
\frac{\left[C_0^3 +C_1^6 \right]^{\frac{1}{4}}}{C_0^\frac{1}{2}C_1^\frac{1}{2}}
 C_0,C_1 C_0=1 C_1\to \infty C_t\leq C C_0^{1-t} C_1^t","['real-analysis', 'functional-analysis', 'analysis', 'lp-spaces', 'interpolation']"
22,Prove that $\sum_{i=1}^{n} (x_i-\overline{x}_n)^4 \leq \sum_{i=1}^{n} x_i^4$,Prove that,\sum_{i=1}^{n} (x_i-\overline{x}_n)^4 \leq \sum_{i=1}^{n} x_i^4,"I'm trying to prove the following inequality: $\sum_{i=1}^{n} (x_i-\overline{x}_n)^4 \leq \sum_{i=1}^{n} x_i^4$ where $\overline{x}_n=\frac{1}{n}\sum_{i=1}^{n}x_i$ . This is my attempt: we can easily show that $\sum_{i=1}^{n} (x_i-\overline{x}_n)^2 \leq \sum_{i=1}^{n} x_i^2$ , then $\sum_{i=1}^{n} (x_i-\overline{x}_n)^4 \leq \bigg[\sum_{i=1}^{n}(x_i-\overline{x}_n)^2\bigg]^2\leq n^2 \bigg[\frac{1}{n}\sum_{i=1}^{n}x_i^2\bigg]^2$ Then, by using Jensen inequality, we find that $\sum_{i=1}^{n} (x_i-\overline{x}_n)^4 \leq n \sum_{i=1}^{n}x_i^4$ . The problem is that I can't find a way to get rid of the $n$ factor. Any suggestion is welcome.","I'm trying to prove the following inequality: where . This is my attempt: we can easily show that , then Then, by using Jensen inequality, we find that . The problem is that I can't find a way to get rid of the factor. Any suggestion is welcome.",\sum_{i=1}^{n} (x_i-\overline{x}_n)^4 \leq \sum_{i=1}^{n} x_i^4 \overline{x}_n=\frac{1}{n}\sum_{i=1}^{n}x_i \sum_{i=1}^{n} (x_i-\overline{x}_n)^2 \leq \sum_{i=1}^{n} x_i^2 \sum_{i=1}^{n} (x_i-\overline{x}_n)^4 \leq \bigg[\sum_{i=1}^{n}(x_i-\overline{x}_n)^2\bigg]^2\leq n^2 \bigg[\frac{1}{n}\sum_{i=1}^{n}x_i^2\bigg]^2 \sum_{i=1}^{n} (x_i-\overline{x}_n)^4 \leq n \sum_{i=1}^{n}x_i^4 n,"['real-analysis', 'inequality', 'summation']"
23,Let $g(y) := \lim\limits_{n\to \infty}x_{n}(y)$. Find $\int_{0}^{3}g(y)dy$.,Let . Find .,g(y) := \lim\limits_{n\to \infty}x_{n}(y) \int_{0}^{3}g(y)dy,"Let $f(x)$ be the function on $\mathbb{R}$ defined by $f(x)\!:=\sin(\pi x/2)$ . For $y$ in $\mathbb{R}$ , consider the sequence $\{x_{n}(y)\}_{n\geqslant0}$ defined by $$ x_{0}(y) := y\;\;\text{ and }\;\;x_{n+1}(y)=f(x_{n}(y))\;\text{ for all }\;n\geqslant1\\\text{and let }\,g(y):=\lim\limits_{n\to\infty}x_{n}(y)\,.$$ Find $\displaystyle\int_{0}^{3}g(y)\mathrm dy\,.$ My Attempt: Between $0$ and $3$ , $f$ has only two fixed points $0,1$ , so $g$ takes only value $0,1$ in $[0,3]$ . But how compute exactly $g$ in $[0,3]\,?$","Let be the function on defined by . For in , consider the sequence defined by Find My Attempt: Between and , has only two fixed points , so takes only value in . But how compute exactly in","f(x) \mathbb{R} f(x)\!:=\sin(\pi x/2) y \mathbb{R} \{x_{n}(y)\}_{n\geqslant0} 
x_{0}(y) := y\;\;\text{ and }\;\;x_{n+1}(y)=f(x_{n}(y))\;\text{ for all }\;n\geqslant1\\\text{and let }\,g(y):=\lim\limits_{n\to\infty}x_{n}(y)\,. \displaystyle\int_{0}^{3}g(y)\mathrm dy\,. 0 3 f 0,1 g 0,1 [0,3] g [0,3]\,?","['real-analysis', 'metric-spaces', 'fixed-point-theorems']"
24,Does weakly Riemann integrable implies strongly Riemann integrable in operator theory?,Does weakly Riemann integrable implies strongly Riemann integrable in operator theory?,,"Let $f:[0,1]\to X$ be a function where $X$ is a Banach algebra. We will say $f$ is strongly Riemann integrable if the limit of the sum $\sum_{i=1}^k f(t_i)(t_{i}-t_{i-1})$ as nets where the poset is the set of all partions $\{0=t_0<t_1<\cdots<t_k=1\}$ of $[0,1]$ with refinement as order. We say $f$ is weakly Riemann integrable means for all linear functionals $x^\ast$ on $X$ we have $x^\ast(f)$ is Riemann integrable. My question is are both definitions equivalent?",Let be a function where is a Banach algebra. We will say is strongly Riemann integrable if the limit of the sum as nets where the poset is the set of all partions of with refinement as order. We say is weakly Riemann integrable means for all linear functionals on we have is Riemann integrable. My question is are both definitions equivalent?,"f:[0,1]\to X X f \sum_{i=1}^k f(t_i)(t_{i}-t_{i-1}) \{0=t_0<t_1<\cdots<t_k=1\} [0,1] f x^\ast X x^\ast(f)","['real-analysis', 'functional-analysis', 'operator-theory']"
25,Does Pointwise Convergence imply L2 Convergence in L2?,Does Pointwise Convergence imply L2 Convergence in L2?,,"Its clear that convergence in $L^2$ does not necessarily imply pointwise convergence a.e but does this work the other way too? I considered the characteristic function $\chi_{[n, n+1]}$ which I guess pointwise converges to 0 in $L^2$ but not for $x \in (0,1)$ I think but I want the example for the other way around for something that converges pointwise but not in $L^2$ if the function in $L^2$ is bounded?",Its clear that convergence in does not necessarily imply pointwise convergence a.e but does this work the other way too? I considered the characteristic function which I guess pointwise converges to 0 in but not for I think but I want the example for the other way around for something that converges pointwise but not in if the function in is bounded?,"L^2 \chi_{[n, n+1]} L^2 x \in (0,1) L^2 L^2","['real-analysis', 'measure-theory']"
26,"Asymptotic expansion of $\small\sum_{k=1}^n H_k^{-1}$, where $\small H_k$ are harmonic numbers","Asymptotic expansion of , where  are harmonic numbers",\small\sum_{k=1}^n H_k^{-1} \small H_k,"I'm interested in an asymptotic expansion of $\,\sum_{k=1}^n H_k^{-1}$ for $\,n\to\infty$ , where $H_k=\sum_{m=1}^km^{-1}$ are harmonic numbers. I would like all terms in the expansion to use $H_n$ rather than $\log n$ to avoid irrational numbers where possible. So far, I have figured the two initial terms by replacing the harmonic numbers with their asymptotic expansion $H_n\sim\log n+\gamma+O\!\left(n^{-1}\right)$ and approximating the sum by the corresponding integral. If I haven't made any mistakes, then $$\sum_{k=1}^n H_k^{-1}\sim n\,H_n^{-1}+n\,H_n^{-2}+O\!\left(n\,H_n^{-3}\right).$$ But computations become more complicated after that, so I couldn't make any further progress. Could you propose an approach that allows to compute more terms, or find a general formula for them?","I'm interested in an asymptotic expansion of for , where are harmonic numbers. I would like all terms in the expansion to use rather than to avoid irrational numbers where possible. So far, I have figured the two initial terms by replacing the harmonic numbers with their asymptotic expansion and approximating the sum by the corresponding integral. If I haven't made any mistakes, then But computations become more complicated after that, so I couldn't make any further progress. Could you propose an approach that allows to compute more terms, or find a general formula for them?","\,\sum_{k=1}^n H_k^{-1} \,n\to\infty H_k=\sum_{m=1}^km^{-1} H_n \log n H_n\sim\log n+\gamma+O\!\left(n^{-1}\right) \sum_{k=1}^n H_k^{-1}\sim n\,H_n^{-1}+n\,H_n^{-2}+O\!\left(n\,H_n^{-3}\right).","['real-analysis', 'sequences-and-series', 'asymptotics', 'harmonic-numbers']"
27,"Finding recursive formulas for $\int_0^{\pi/2}\sin^nx\,\sin(nx)\,dx$ and $\int_0^{\pi/2}\sin^nx\,\cos(nx)\,dx$",Finding recursive formulas for  and,"\int_0^{\pi/2}\sin^nx\,\sin(nx)\,dx \int_0^{\pi/2}\sin^nx\,\cos(nx)\,dx","As part of my homework, the teacher tasked us with finding recursive formulas/recurrence relations for the following integrals (I guess without refrencing I in J or J in I): $$I_n =\int_{0}^{\frac{\pi}{2}} \sin^nx\cdot \sin(nx)\ dx \ \ \ \ \ (1) \\ J_n = \int_{0}^{\frac{\pi}{2}}\sin^nx\cdot \cos(nx) \ dx \ \ \ \ \ (2)$$ The constraints are to use integration by parts and elementary operations (such as writing $x^n$ as $x^{n-1}\cdot x$ and then integrating). The teacher also gave us $(1)$ but only with $\cos$ and $(2)$ but with $\sin$ and $\cos$ switched $(\cos^nx\cdot\sin(nx))$ . These I was able to solve either by started from the $n-1$ term which then turned into the $n$ term or by integrating by parts. However, these other two seem a lot harder and, for instance in the case of $(1)$ , I get stuck because I somehow reach an equation that includes $(2)$ : $$I_n = \int_{0}^{\frac{\pi}{2}} \sin^nx\cdot\sin(nx) \ dx = -\frac{\cos\left(\frac{n\pi}{2}\right)}{n}+ \int_{0}^{\frac{\pi}{2}}\sin^{n-1}x\cdot\cos((n-1)\cdot x)\ dx - \underbrace{\int_{0}^{\frac{\pi}{2}}\sin^{n-1}x\cdot\sin nx\cdot\sin x \ dx}_{=I_n} \iff \\ \iff 2\cdot I_n =  -\frac{\cos\left(\frac{n\pi}{2}\right)}{n}+ \underbrace{\int_{0}^{\frac{\pi}{2}}\sin^{n-1}x\cdot\cos((n-1)\cdot x)\ dx}_{=J_{n-1}}$$ Ideally, I would like just a hint if I'm close with my attempt. Thank you!","As part of my homework, the teacher tasked us with finding recursive formulas/recurrence relations for the following integrals (I guess without refrencing I in J or J in I): The constraints are to use integration by parts and elementary operations (such as writing as and then integrating). The teacher also gave us but only with and but with and switched . These I was able to solve either by started from the term which then turned into the term or by integrating by parts. However, these other two seem a lot harder and, for instance in the case of , I get stuck because I somehow reach an equation that includes : Ideally, I would like just a hint if I'm close with my attempt. Thank you!",I_n =\int_{0}^{\frac{\pi}{2}} \sin^nx\cdot \sin(nx)\ dx \ \ \ \ \ (1) \\ J_n = \int_{0}^{\frac{\pi}{2}}\sin^nx\cdot \cos(nx) \ dx \ \ \ \ \ (2) x^n x^{n-1}\cdot x (1) \cos (2) \sin \cos (\cos^nx\cdot\sin(nx)) n-1 n (1) (2) I_n = \int_{0}^{\frac{\pi}{2}} \sin^nx\cdot\sin(nx) \ dx = -\frac{\cos\left(\frac{n\pi}{2}\right)}{n}+ \int_{0}^{\frac{\pi}{2}}\sin^{n-1}x\cdot\cos((n-1)\cdot x)\ dx - \underbrace{\int_{0}^{\frac{\pi}{2}}\sin^{n-1}x\cdot\sin nx\cdot\sin x \ dx}_{=I_n} \iff \\ \iff 2\cdot I_n =  -\frac{\cos\left(\frac{n\pi}{2}\right)}{n}+ \underbrace{\int_{0}^{\frac{\pi}{2}}\sin^{n-1}x\cdot\cos((n-1)\cdot x)\ dx}_{=J_{n-1}},"['real-analysis', 'calculus', 'integration', 'trigonometry', 'trigonometric-integrals']"
28,Compute $\lim_{n\to\infty}(\frac{a_{n+1}}{\sqrt[n+1]{b_{n+1}}}-\frac{a_n}{\sqrt[n]{b_n}})$ given $(a_{n+1}-a_n)/n\to a$ and $b_{n+1}/(nb_n)\to b$,Compute  given  and,\lim_{n\to\infty}(\frac{a_{n+1}}{\sqrt[n+1]{b_{n+1}}}-\frac{a_n}{\sqrt[n]{b_n}}) (a_{n+1}-a_n)/n\to a b_{n+1}/(nb_n)\to b,"Let $(a_n)_{n\geq 1}$ and $(b_n)_{n\geq 1}$ be positive real sequences such that $$\lim_{n\to\infty}\frac{a_{n+1}-a_n}n=a\in \mathbb R_{>0}\qquad\text{and}\qquad \lim_{n\to\infty}\frac{b_{n+1}}{nb_n}=b\in \mathbb R_{>0}.$$ Compute $$\lim_{n\to\infty}\left(\frac{a_{n+1}}{\sqrt[n+1]{b_{n+1}}}-\frac{a_n}{\sqrt[n]{b_n}}\right).$$ This is W5 of József Wildt International Mathematical Competition, 2020 . I have made some progresses and got stuck. Using Cauchy's criterion , I have obtained that $\sqrt[n]{\frac{b_{n+1}}{b_1n!}}\to b$ and thus $$\sqrt[n]{\frac{b_n}{b_1n!}}=\sqrt[n]{\frac{b_{n+1}}{b_1n!}}\cdot\sqrt[n]{\frac{nb_n}{b_{n+1}}}\cdot\sqrt[n]{\frac1n}\to b,\qquad\text{as}\ \  n\to\infty.$$ Stirling's formula implies that $\lim_{n\to\infty}\frac{\sqrt[n]{b_n}}n=\frac be$ . Now it seems resonable to guess that $$\lim_{n\to\infty}\left(\frac{a_{n+1}}{\sqrt[n+1]{b_{n+1}}}-\frac{a_n}{\sqrt[n]{b_n}}\right)=\lim_{n\to\infty}\left(\frac{\frac{a_{n+1}}{n}}{\frac{\sqrt[n+1]{b_{n+1}}}{n}}-\frac{\frac{a_n}n}{\frac{\sqrt[n]{b_n}}n}\right)\overset{?}{=}\lim_{n\to\infty}\frac eb\frac{a_{n+1}-a_n}n=\frac{ae}b\tag{1};$$ or another possibility $$\lim_{n\to\infty}\left(\frac{a_{n+1}}{\sqrt[n+1]{b_{n+1}}}-\frac{a_n}{\sqrt[n]{b_n}}\right)=\lim_{n\to\infty}\left(\frac{\frac{a_{n+1}}{n+1}}{\frac{\sqrt[n+1]{b_{n+1}}}{n+1}}-\frac{\frac{a_n}n}{\frac{\sqrt[n]{b_n}}n}\right)\overset{?}{=}\frac eb\lim_{n\to\infty}\left(\frac{a_{n+1}}{n+1}-\frac{a_n}n\right),\tag{2}$$ where the limit $\lim_{n\to\infty}\left(\frac{a_{n+1}}{n+1}-\frac{a_n}n\right)$ can be obtained by $$\frac{a_{n+1}}{n+1}-\frac{a_n}n=\frac{a_{n+1}-a_n}n+\frac{a_{n+1}}{n+1}-\frac{a_{n+1}}n=\frac{a_{n+1}-a_n}n-\frac{a_{n+1}}{n(n+1)}$$ and Cesaro-Stolz: $\lim_{n\to\infty}\frac{a_{n+1}}{n(n+1)}=\lim_{n\to\infty}\frac{a_{n+1}-a_n}{n(n+1)-(n-1)n}=\frac a2$ , so $\lim_{n\to\infty}\left(\frac{a_{n+1}}{n+1}-\frac{a_n}n\right)=a-\frac a2=\frac a2$ . In $(1)$ and $(2)$ above, I don't know the eact reasons for both ' $?$ ', I just guess them. Therefore, when I looked at the same limit from two different points of view, namely $(1)$ and $(2)$ , I got different results, which makes me confused. I wonder which one is correct and what is the rigorous proof. Could someone help me to finish this problem?","Let and be positive real sequences such that Compute This is W5 of József Wildt International Mathematical Competition, 2020 . I have made some progresses and got stuck. Using Cauchy's criterion , I have obtained that and thus Stirling's formula implies that . Now it seems resonable to guess that or another possibility where the limit can be obtained by and Cesaro-Stolz: , so . In and above, I don't know the eact reasons for both ' ', I just guess them. Therefore, when I looked at the same limit from two different points of view, namely and , I got different results, which makes me confused. I wonder which one is correct and what is the rigorous proof. Could someone help me to finish this problem?","(a_n)_{n\geq 1} (b_n)_{n\geq 1} \lim_{n\to\infty}\frac{a_{n+1}-a_n}n=a\in \mathbb R_{>0}\qquad\text{and}\qquad \lim_{n\to\infty}\frac{b_{n+1}}{nb_n}=b\in \mathbb R_{>0}. \lim_{n\to\infty}\left(\frac{a_{n+1}}{\sqrt[n+1]{b_{n+1}}}-\frac{a_n}{\sqrt[n]{b_n}}\right). \sqrt[n]{\frac{b_{n+1}}{b_1n!}}\to b \sqrt[n]{\frac{b_n}{b_1n!}}=\sqrt[n]{\frac{b_{n+1}}{b_1n!}}\cdot\sqrt[n]{\frac{nb_n}{b_{n+1}}}\cdot\sqrt[n]{\frac1n}\to b,\qquad\text{as}\ \  n\to\infty. \lim_{n\to\infty}\frac{\sqrt[n]{b_n}}n=\frac be \lim_{n\to\infty}\left(\frac{a_{n+1}}{\sqrt[n+1]{b_{n+1}}}-\frac{a_n}{\sqrt[n]{b_n}}\right)=\lim_{n\to\infty}\left(\frac{\frac{a_{n+1}}{n}}{\frac{\sqrt[n+1]{b_{n+1}}}{n}}-\frac{\frac{a_n}n}{\frac{\sqrt[n]{b_n}}n}\right)\overset{?}{=}\lim_{n\to\infty}\frac eb\frac{a_{n+1}-a_n}n=\frac{ae}b\tag{1}; \lim_{n\to\infty}\left(\frac{a_{n+1}}{\sqrt[n+1]{b_{n+1}}}-\frac{a_n}{\sqrt[n]{b_n}}\right)=\lim_{n\to\infty}\left(\frac{\frac{a_{n+1}}{n+1}}{\frac{\sqrt[n+1]{b_{n+1}}}{n+1}}-\frac{\frac{a_n}n}{\frac{\sqrt[n]{b_n}}n}\right)\overset{?}{=}\frac eb\lim_{n\to\infty}\left(\frac{a_{n+1}}{n+1}-\frac{a_n}n\right),\tag{2} \lim_{n\to\infty}\left(\frac{a_{n+1}}{n+1}-\frac{a_n}n\right) \frac{a_{n+1}}{n+1}-\frac{a_n}n=\frac{a_{n+1}-a_n}n+\frac{a_{n+1}}{n+1}-\frac{a_{n+1}}n=\frac{a_{n+1}-a_n}n-\frac{a_{n+1}}{n(n+1)} \lim_{n\to\infty}\frac{a_{n+1}}{n(n+1)}=\lim_{n\to\infty}\frac{a_{n+1}-a_n}{n(n+1)-(n-1)n}=\frac a2 \lim_{n\to\infty}\left(\frac{a_{n+1}}{n+1}-\frac{a_n}n\right)=a-\frac a2=\frac a2 (1) (2) ? (1) (2)","['real-analysis', 'calculus', 'limits']"
29,"If $\int f=\int g$ and $\int_0^t f \geq \int_0^t g$ for all $t≥0$, then is $\int_0^t g^{-1} \geq \int_0^t f^{-1}$ for all $t\geq 0$?","If  and  for all , then is  for all ?",\int f=\int g \int_0^t f \geq \int_0^t g t≥0 \int_0^t g^{-1} \geq \int_0^t f^{-1} t\geq 0,"Suppose $f,g$ are continuous, integrable, decreasing, nonnegative-real-valued functions, each defined on some interval of nonnegative real numbers with left endpoint $0$ , and satisfying $\inf f = \inf g = 0$ . (The intervals can be open, half-open, or closed, and may be infinite.) The assumptions imply that $f$ and $g$ have similarly continuous, integrable, decreasing inverses (I mean inverse with respect to function composition) defined on similar intervals. Then suppose $$\int f = \int g,$$ where the integration is over their domains. Finally, suppose that for all $t$ for which both functions are defined, we have $$ \int_0^t f \geq \int_0^t g.$$ It seems to me that in this situation it follows that $$ \int_0^t g^{-1} \geq \int_0^t f^{-1} $$ for all $t$ for which both $f^{-1}$ and $g^{-1}$ are defined, as well. But I think this for an essentially hand-wavy reason: ""Since the functions are decreasing and trapped in the first quadrant, then if mass is moved downard, it has to go to the right."" Question: Is the claim correct, and if so, how is it properly proven?","Suppose are continuous, integrable, decreasing, nonnegative-real-valued functions, each defined on some interval of nonnegative real numbers with left endpoint , and satisfying . (The intervals can be open, half-open, or closed, and may be infinite.) The assumptions imply that and have similarly continuous, integrable, decreasing inverses (I mean inverse with respect to function composition) defined on similar intervals. Then suppose where the integration is over their domains. Finally, suppose that for all for which both functions are defined, we have It seems to me that in this situation it follows that for all for which both and are defined, as well. But I think this for an essentially hand-wavy reason: ""Since the functions are decreasing and trapped in the first quadrant, then if mass is moved downard, it has to go to the right."" Question: Is the claim correct, and if so, how is it properly proven?","f,g 0 \inf f = \inf g = 0 f g \int f = \int g, t  \int_0^t f \geq \int_0^t g.  \int_0^t g^{-1} \geq \int_0^t f^{-1}  t f^{-1} g^{-1}",['real-analysis']
30,$\int_{0}^{1} \frac{1}{(1+x^2)\sqrt{1-x^2}\sqrt{2+x^2} \sqrt{3+x^2} }\text{d}x=\frac{\pi}{8}$ and generalizations,and generalizations,\int_{0}^{1} \frac{1}{(1+x^2)\sqrt{1-x^2}\sqrt{2+x^2} \sqrt{3+x^2} }\text{d}x=\frac{\pi}{8},Someone shared this to me: $$\int_{0}^{1} \frac{1}{(1+x^2)\sqrt{1-x^2}\sqrt{4-x^4}\sqrt{9-x^4}   } \text{d} x -\int_{0}^{1} \frac{1}{(3+x^2)\sqrt{1-x^4} \sqrt{2+x^2}\sqrt{4+x^2}\sqrt{5+x^2}   }\text{d}x + \int_{0}^{1} \frac{1}{(1+x^2)\sqrt{1-x^2} \sqrt{2+x^2} \sqrt{3+x^2}\sqrt{4+x^2}\sqrt{5+x^2}    }\text{d}x =\frac{\pi\sqrt{3} }{24}$$ is correct. The integral has piqued my curiosity for a while but not worked out still. So I hope someone aids me and thank you for the great appreciation. The expression is in the form of $\pi\cdot\text{Algebraic}$ . A relatively notable integral is $$\int_{0}^{1} \frac{1}{(1+x^2)\sqrt{1-x^2}\sqrt{2+x^2}  \sqrt{3+x^2}  }\text{d}x=\frac{\pi}{8}$$,Someone shared this to me: is correct. The integral has piqued my curiosity for a while but not worked out still. So I hope someone aids me and thank you for the great appreciation. The expression is in the form of . A relatively notable integral is,"\int_{0}^{1} \frac{1}{(1+x^2)\sqrt{1-x^2}\sqrt{4-x^4}\sqrt{9-x^4}   }
\text{d} x
-\int_{0}^{1} \frac{1}{(3+x^2)\sqrt{1-x^4} \sqrt{2+x^2}\sqrt{4+x^2}\sqrt{5+x^2}   }\text{d}x
+ \int_{0}^{1} \frac{1}{(1+x^2)\sqrt{1-x^2} \sqrt{2+x^2}
\sqrt{3+x^2}\sqrt{4+x^2}\sqrt{5+x^2}    }\text{d}x
=\frac{\pi\sqrt{3} }{24} \pi\cdot\text{Algebraic} \int_{0}^{1} \frac{1}{(1+x^2)\sqrt{1-x^2}\sqrt{2+x^2} 
\sqrt{3+x^2}  }\text{d}x=\frac{\pi}{8}","['real-analysis', 'integration', 'definite-integrals', 'contour-integration']"
31,How prove this inequality with $\int_{0}^{1}(f(x))^2dx\le\frac{1}{\theta^4}\int_{0}^{1}(f''(x))^2dx$,How prove this inequality with,\int_{0}^{1}(f(x))^2dx\le\frac{1}{\theta^4}\int_{0}^{1}(f''(x))^2dx,"let $f(x)$ is smooth function，and such $$f(0)=f'(0)=f(1)=f'(1)=0$$ show that $$\int_{0}^{1}(f(x))^2dx\le\dfrac{1}{\theta^4}\int_{0}^{1}(f''(x))^2dx$$ where the $\theta $ is  smallest positive root $$\cos{\theta}\cosh{\theta}=1$$ ,and the $\dfrac{1}{\theta^4}$ is best coefficient. I know that this form of inequality is commonly called Wirtinger's inequality： links ,but this  my question conditions are quite many, and it doesn’t seem feasible to use a proof method like that Wirtinger's  inequality,","let is smooth function，and such show that where the is  smallest positive root ,and the is best coefficient. I know that this form of inequality is commonly called Wirtinger's inequality： links ,but this  my question conditions are quite many, and it doesn’t seem feasible to use a proof method like that Wirtinger's  inequality,",f(x) f(0)=f'(0)=f(1)=f'(1)=0 \int_{0}^{1}(f(x))^2dx\le\dfrac{1}{\theta^4}\int_{0}^{1}(f''(x))^2dx \theta  \cos{\theta}\cosh{\theta}=1 \dfrac{1}{\theta^4},"['real-analysis', 'inequality']"
32,Find the value of $\displaystyle \int \limits_{0}^{1}\frac{\ln^2{x}+2\ln{x}-2x+2}{\ln^2{x}-x\ln^2{x}} \mathrm dx$,Find the value of,\displaystyle \int \limits_{0}^{1}\frac{\ln^2{x}+2\ln{x}-2x+2}{\ln^2{x}-x\ln^2{x}} \mathrm dx,"I have a question which askes us to find the value of the integral: $\displaystyle \tag*{} \int \limits_{0}^{1}\frac{\ln^2{x}+2\ln{x}-2x+2}{\ln^2{x}-x\ln^2{x}} \mathrm dx$ I tried using differentiation under integral using some variable, I couldn't go any further, I even tried the substitution $\ln x=t$ , it just reduces the size of integral. I divided the integral with the denominator to get $3$ separate integrals but each of them diverges. Any help would be appreciated. Thanks.","I have a question which askes us to find the value of the integral: I tried using differentiation under integral using some variable, I couldn't go any further, I even tried the substitution , it just reduces the size of integral. I divided the integral with the denominator to get separate integrals but each of them diverges. Any help would be appreciated. Thanks.",\displaystyle \tag*{} \int \limits_{0}^{1}\frac{\ln^2{x}+2\ln{x}-2x+2}{\ln^2{x}-x\ln^2{x}} \mathrm dx \ln x=t 3,"['real-analysis', 'integration']"
33,What does the limit $\lim_{x \to a}\frac{(x-a)f'(x)}{f(x)}$ mean for non-polynomials,What does the limit  mean for non-polynomials,\lim_{x \to a}\frac{(x-a)f'(x)}{f(x)},"The limit $$\lim_{x \to a}\frac{(x-a)f'(x)}{f(x)}$$ has an important meaning for polynomials. Suppose $f(x)=(x-a)^nq(x)$ where $q(a)\neq 0$ . Then, the given limit is equivalent to $$\lim_{x \to a}\frac{\{n(x-a)^{n-1}q(x)+(x-a)^nq'(x)\}(x-a)}{(x-a)^nq(x)}=\lim_{x \to a}\frac{nq(x)+(x-a)q'(x)}{q(x)}=n+\lim_{x \to a}\frac{(x-a)q'(x)}{q(x)}=n$$ where the last limit comes from the fact that $q(a)\neq 0$ , but the numerator goes to zero. So, for polynomials, the value of the given limit would indicate the algebraic multiplicity of the polynomial of the term $(x-a)$ . We could also extend this concept to functions not quite polynomial, but ""looks"" like a polynomial, for example functions like $x-4\sqrt{x}+3$ . This function has the above limit value when $a=1$ as $$\lim_{x \to 1}\frac{(x-1)\times \{1-\frac{2}{\sqrt{x}}\}}{(\sqrt{x}-1)(\sqrt{x}-3)}=\frac{2\cdot (-1)}{1-3}=1$$ One could also extend to functions like $f(x)=x^\pi$ . The limit in this case would be, $$\lim_{x \to 0}\frac{x\times \pi x^{\pi-1}}{x^\pi}=\pi$$ So, the limit would similarly be the value of the ""degree of $(x-a)$ ""(which, if it is even a valid saying). (Note that we choose function $f(x)$ and the value $a$ for each limit. Yet since the limit has some meaning when $f(a)=0$ , I would omit the choice of $a$ whenever the root of $f(x)$ is unique.) My question is, how can I extend this concept to functions that are not in the form of $f(x^p)$ (where $f$ is a polynomial, $p$ is a real number)? Are there any meaning to this limit value when the function is a transcendental function? Are there any concepts that this value represents? (Edit) Here are some examples of the value of the limit for some functions. $f(x)=e^{-x}(x^3\sin x)$ at $a=0$ $$\lim_{x \to 0}\frac{xf'(x)}{f(x)}=\lim_{x \to 0}\frac{x\times e^{-x}(-x^3\sin x+3x^2\sin x+x^3\cos x)}{e^{-x}(x^3\sin x)}=4$$ $f(x)=\frac{\sin x}{x}$ at $a=0$ $$\lim_{x \to 0}\frac{xf'(x)}{f(x)}=\lim_{x \to 0}\frac{x\times \frac{x\cos x-\sin x}{x^2}}{\frac{\sin x}{x}}=0$$ $f(x)=\frac{\sin x}{x}$ at $a=\pi$ $$\lim_{x \to \pi}\frac{(x-\pi)f'(x)}{f(x)}=\lim_{x \to \pi}\frac{(x-\pi)\frac{x \cos x-\sin x}{x^2}}{\frac{\sin x}{x}}=1$$ $f(x)=e^x-1$ at $a=0$ $$\lim_{x \to 0}\frac{xf'(x)}{f(x)}=\lim_{x \to 0}\frac{x\times e^x}{e^x-1}=1$$","The limit has an important meaning for polynomials. Suppose where . Then, the given limit is equivalent to where the last limit comes from the fact that , but the numerator goes to zero. So, for polynomials, the value of the given limit would indicate the algebraic multiplicity of the polynomial of the term . We could also extend this concept to functions not quite polynomial, but ""looks"" like a polynomial, for example functions like . This function has the above limit value when as One could also extend to functions like . The limit in this case would be, So, the limit would similarly be the value of the ""degree of ""(which, if it is even a valid saying). (Note that we choose function and the value for each limit. Yet since the limit has some meaning when , I would omit the choice of whenever the root of is unique.) My question is, how can I extend this concept to functions that are not in the form of (where is a polynomial, is a real number)? Are there any meaning to this limit value when the function is a transcendental function? Are there any concepts that this value represents? (Edit) Here are some examples of the value of the limit for some functions. at at at at",\lim_{x \to a}\frac{(x-a)f'(x)}{f(x)} f(x)=(x-a)^nq(x) q(a)\neq 0 \lim_{x \to a}\frac{\{n(x-a)^{n-1}q(x)+(x-a)^nq'(x)\}(x-a)}{(x-a)^nq(x)}=\lim_{x \to a}\frac{nq(x)+(x-a)q'(x)}{q(x)}=n+\lim_{x \to a}\frac{(x-a)q'(x)}{q(x)}=n q(a)\neq 0 (x-a) x-4\sqrt{x}+3 a=1 \lim_{x \to 1}\frac{(x-1)\times \{1-\frac{2}{\sqrt{x}}\}}{(\sqrt{x}-1)(\sqrt{x}-3)}=\frac{2\cdot (-1)}{1-3}=1 f(x)=x^\pi \lim_{x \to 0}\frac{x\times \pi x^{\pi-1}}{x^\pi}=\pi (x-a) f(x) a f(a)=0 a f(x) f(x^p) f p f(x)=e^{-x}(x^3\sin x) a=0 \lim_{x \to 0}\frac{xf'(x)}{f(x)}=\lim_{x \to 0}\frac{x\times e^{-x}(-x^3\sin x+3x^2\sin x+x^3\cos x)}{e^{-x}(x^3\sin x)}=4 f(x)=\frac{\sin x}{x} a=0 \lim_{x \to 0}\frac{xf'(x)}{f(x)}=\lim_{x \to 0}\frac{x\times \frac{x\cos x-\sin x}{x^2}}{\frac{\sin x}{x}}=0 f(x)=\frac{\sin x}{x} a=\pi \lim_{x \to \pi}\frac{(x-\pi)f'(x)}{f(x)}=\lim_{x \to \pi}\frac{(x-\pi)\frac{x \cos x-\sin x}{x^2}}{\frac{\sin x}{x}}=1 f(x)=e^x-1 a=0 \lim_{x \to 0}\frac{xf'(x)}{f(x)}=\lim_{x \to 0}\frac{x\times e^x}{e^x-1}=1,"['real-analysis', 'calculus', 'limits']"
34,"Density of $\sin(\Bbb Z)$ in $[-1, 1]$ implies the same density for $\sin(\Bbb N)$",Density of  in  implies the same density for,"\sin(\Bbb Z) [-1, 1] \sin(\Bbb N)","I have succeeded in proving that $\sin(\Bbb Z)$ is dense in $[-1, 1]$ , however I would also like to prove that this result implies that $\sin(\Bbb N)$ is also dense in $[-1,1]$ . So far, I have tried to proceed by contradiction, noticing first that $$\sin(\Bbb Z) =  \sin(\Bbb N) \cup \sin(-\Bbb N) = \sin(\Bbb N) \cup -\sin(\Bbb N);$$ then there is at least a point in $[-1,1]$ such that one of its neighbourhoods contains infinitely many points of $\sin(-\Bbb N)$ but zero points of $\sin(\Bbb N)$ . However I haven't been able to go further since I do not understand which property of sine must be used in order to complete the proof (if the proof is actually possible given only that hypothesis). Any help is highly appreciated! EDIT I would like to clarify the reasoning I have done so far: (1) I proved that any additive subgroup of $(\Bbb R, +)$ either has a positive least element or is dense in $\Bbb R$ ; (2) I proved that the set $\{ n+m\alpha: n,m \in \Bbb Z\}$ , where $\alpha$ is irrational, is dense in $\Bbb R$ by showing that it is an additive subgroup of $(\Bbb R, +)$ which doesn't have a least positive element; (3) now it is easy to prove that $\sin(\Bbb Z)$ is dense in $[-1,1]$ by using the periodicity (let $\alpha = 2\pi$ ), the surjectivity in $[-1,1]$ and the sequential continuity of the sine function. In light of recent comments, I think it would be easier to show that $ A = \{ n+m\alpha: n \in \Bbb N,m \in \Bbb Z\}$ is dense in $\Bbb R$ .","I have succeeded in proving that is dense in , however I would also like to prove that this result implies that is also dense in . So far, I have tried to proceed by contradiction, noticing first that then there is at least a point in such that one of its neighbourhoods contains infinitely many points of but zero points of . However I haven't been able to go further since I do not understand which property of sine must be used in order to complete the proof (if the proof is actually possible given only that hypothesis). Any help is highly appreciated! EDIT I would like to clarify the reasoning I have done so far: (1) I proved that any additive subgroup of either has a positive least element or is dense in ; (2) I proved that the set , where is irrational, is dense in by showing that it is an additive subgroup of which doesn't have a least positive element; (3) now it is easy to prove that is dense in by using the periodicity (let ), the surjectivity in and the sequential continuity of the sine function. In light of recent comments, I think it would be easier to show that is dense in .","\sin(\Bbb Z) [-1, 1] \sin(\Bbb N) [-1,1] \sin(\Bbb Z) =  \sin(\Bbb N) \cup \sin(-\Bbb N) = \sin(\Bbb N) \cup -\sin(\Bbb N); [-1,1] \sin(-\Bbb N) \sin(\Bbb N) (\Bbb R, +) \Bbb R \{ n+m\alpha: n,m \in \Bbb Z\} \alpha \Bbb R (\Bbb R, +) \sin(\Bbb Z) [-1,1] \alpha = 2\pi [-1,1]  A = \{ n+m\alpha: n \in \Bbb N,m \in \Bbb Z\} \Bbb R","['real-analysis', 'general-topology', 'number-theory', 'trigonometry']"
35,"Must a monotone, differentiable function $[a, b] \rightarrow \mathbb{R}$ be absolutely continuous?","Must a monotone, differentiable function  be absolutely continuous?","[a, b] \rightarrow \mathbb{R}","If we assume that the derivative is continuous the answer is clearly yes. I suspect however that now the answer is no. This question arose when I wanted to investigate for which non-decreasing, right-continuous functions $g: \mathbb{R} \rightarrow \mathbb{R}$ the Lebesgue-Stjeltes measure associated to $g$ , $\mu_g$ , is given by $\mu_g(A)=\int_A g' d\mu$ for all Borel sets $A$ (1). If such a $g$ is automatically absolutely continuous then the second theorem of calculus for Lebesgue integration would imply that $\int_{[a,b]}g'(x)d\mu(x)=g(b)-g(a)$ and the uniqueness in Caratheodory's theorem would imply (1).","If we assume that the derivative is continuous the answer is clearly yes. I suspect however that now the answer is no. This question arose when I wanted to investigate for which non-decreasing, right-continuous functions the Lebesgue-Stjeltes measure associated to , , is given by for all Borel sets (1). If such a is automatically absolutely continuous then the second theorem of calculus for Lebesgue integration would imply that and the uniqueness in Caratheodory's theorem would imply (1).","g: \mathbb{R} \rightarrow \mathbb{R} g \mu_g \mu_g(A)=\int_A g' d\mu A g \int_{[a,b]}g'(x)d\mu(x)=g(b)-g(a)","['real-analysis', 'measure-theory', 'absolute-continuity']"
36,Is this a good argument on what makes the Lebesgue integral more general than the Riemann integral?,Is this a good argument on what makes the Lebesgue integral more general than the Riemann integral?,,"Ever since I learned about the Lebesgue integral I have tried to understand what exactly makes it more general than the Riemann integral. By this I mean what part of its definition makes it more general. Below I will only talk about positive functions defined on some interval $[a, b]\subset \mathbb{R}$ . Here is what I've come up with: when defining the Riemann integral, we use step functions, which are linear combinations of characteristic functions of intervals ; when defining the Lebesgue integral, we use measurable simple functions, which are linear combinations of characteristic functions of measurable sets. I think that this is the point of the theory that makes the Lebesgue integral more general. In particular, I think that this is what makes the function $$f(x)=\begin{cases} 1 & x\in [0, 1]\cap \mathbb{Q} \\ 0 & x\in [0, 1]\cap (\mathbb{R}\setminus \mathbb{Q}) \end{cases}$$ Lebesgue integrable and not Riemann integrable. Furthermore, I think that the reason why for the Riemann integral we use characteristic functions of intervals is because these are the only Jordan measurable subsets of $\mathbb{R}$ that are ""interesting"" (I can't really think of a Jordan measurable subset of $\mathbb{R}$ that is not an interval except some weird looking examples that end up having Jordan measure zero) and for the Riemann integral we wish to have a good connection with the Jordan measure. So, I think that we may also say that the reason why the Lebesgue integral is more general than the Riemann integrable is that the Lebesgue measure is more general than the Jordan measure. Are the above arguments correct? I have really thought about this a lot and I really want to make sure that I know what part of the definition of the Lebesgue integral grants it its generality.","Ever since I learned about the Lebesgue integral I have tried to understand what exactly makes it more general than the Riemann integral. By this I mean what part of its definition makes it more general. Below I will only talk about positive functions defined on some interval . Here is what I've come up with: when defining the Riemann integral, we use step functions, which are linear combinations of characteristic functions of intervals ; when defining the Lebesgue integral, we use measurable simple functions, which are linear combinations of characteristic functions of measurable sets. I think that this is the point of the theory that makes the Lebesgue integral more general. In particular, I think that this is what makes the function Lebesgue integrable and not Riemann integrable. Furthermore, I think that the reason why for the Riemann integral we use characteristic functions of intervals is because these are the only Jordan measurable subsets of that are ""interesting"" (I can't really think of a Jordan measurable subset of that is not an interval except some weird looking examples that end up having Jordan measure zero) and for the Riemann integral we wish to have a good connection with the Jordan measure. So, I think that we may also say that the reason why the Lebesgue integral is more general than the Riemann integrable is that the Lebesgue measure is more general than the Jordan measure. Are the above arguments correct? I have really thought about this a lot and I really want to make sure that I know what part of the definition of the Lebesgue integral grants it its generality.","[a, b]\subset \mathbb{R} f(x)=\begin{cases} 1 & x\in [0, 1]\cap \mathbb{Q} \\ 0 & x\in [0, 1]\cap (\mathbb{R}\setminus \mathbb{Q}) \end{cases} \mathbb{R} \mathbb{R}","['real-analysis', 'measure-theory']"
37,Why do so few authors package up the definition of a limit into a function?,Why do so few authors package up the definition of a limit into a function?,,"Occasionally, you will literally see people arguing for nonstandard analysis purely to unnest the quantifiers in the definition of a limit. By unnesting, I mean avoiding an exists quantifier that is required to be nested inside the scope of the for all epsilon quantifier, and instead having only foralls that can be made implicit and constraints on the corresponding variables. NSA does this by just constraining $N$ to be infinitely large or $x$ to be infinitely close to $x_0$ . But there's a much simpler way to clean things up so that the quantifiers are no longer nested. Just define functions: A sequence $a_n$ in a metric space is convergent with limit L if there is a function $N : \mathbf{R^+} \to \mathbf{N}$ such that $d(a_n,L) < \varepsilon$ for all $\varepsilon > 0$ and $n > N(\varepsilon)$ . A sequence $a_n$ in a metric space is a Cauchy series if there is a function $N : \mathbf{R^+} \to \mathbf{N}$ such that $d(a_n,a_m) < \varepsilon$ for all $\varepsilon > 0$ and $n,m > N(\varepsilon)$ . A function $f : X \to Y$ between metric spaces has the limit $L = \lim_{x \to x_0} f(x)$ if there is a function $\delta : \mathbf{R^+} \to \mathbf{R^+}$ so that $d(f(x),L) < \varepsilon$ for any $\varepsilon > 0$ and $d(x,x_0) < \delta(\varepsilon)$ . which lets you write the definitions with only foralls that can be floated out to top level so that you just have one inequality between top level variables implying the other. Furthermore, actually requiring that this function be defined (occasionally called the modulus of convergence), is both very convenient in epsilon delta proofs, and actually necessary to have for anything related to numerics/computability/constructive analysis or if you generally care about ""how many terms do I need"" questions. So this made me curious. Is there any fundamental reason why you would want to write the definition of a limit in terms of nested quantifiers? I'd say tradition, but afaik first order logic is quite a bit newer than the definition of a limit. How did the 19th century analysts state the definition?","Occasionally, you will literally see people arguing for nonstandard analysis purely to unnest the quantifiers in the definition of a limit. By unnesting, I mean avoiding an exists quantifier that is required to be nested inside the scope of the for all epsilon quantifier, and instead having only foralls that can be made implicit and constraints on the corresponding variables. NSA does this by just constraining to be infinitely large or to be infinitely close to . But there's a much simpler way to clean things up so that the quantifiers are no longer nested. Just define functions: A sequence in a metric space is convergent with limit L if there is a function such that for all and . A sequence in a metric space is a Cauchy series if there is a function such that for all and . A function between metric spaces has the limit if there is a function so that for any and . which lets you write the definitions with only foralls that can be floated out to top level so that you just have one inequality between top level variables implying the other. Furthermore, actually requiring that this function be defined (occasionally called the modulus of convergence), is both very convenient in epsilon delta proofs, and actually necessary to have for anything related to numerics/computability/constructive analysis or if you generally care about ""how many terms do I need"" questions. So this made me curious. Is there any fundamental reason why you would want to write the definition of a limit in terms of nested quantifiers? I'd say tradition, but afaik first order logic is quite a bit newer than the definition of a limit. How did the 19th century analysts state the definition?","N x x_0 a_n N : \mathbf{R^+} \to \mathbf{N} d(a_n,L) < \varepsilon \varepsilon > 0 n > N(\varepsilon) a_n N : \mathbf{R^+} \to \mathbf{N} d(a_n,a_m) < \varepsilon \varepsilon > 0 n,m > N(\varepsilon) f : X \to Y L = \lim_{x \to x_0} f(x) \delta : \mathbf{R^+} \to \mathbf{R^+} d(f(x),L) < \varepsilon \varepsilon > 0 d(x,x_0) < \delta(\varepsilon)","['real-analysis', 'limits', 'education', 'constructive-mathematics']"
38,Two equivalent definitions of the Cantor function,Two equivalent definitions of the Cantor function,,"I know two definitions of the Cantor function $c: [0,1] \to [0,1]$ . $$ c(x) =  \begin{cases} \sum_{n=1}^{\infty} \frac{a_{n}}{2^{n}}, \; x \in C\\ \sup_{y \leq x, \; y \in C} c(y), \; x \notin C \end{cases} $$ where $\{a_{n}\}$ is the ternary expansion of $x$ . I also know the recursive definition: $$ c(x) = \lim_{n \to \infty} c_{n}(x) $$ where $$ c_{n+1}(x) =  \begin{cases} \frac{1}{2} c_{n}(3x), & x\in [0, \frac{1}{3}); \\ \frac{1}{2}, & x\in[\frac{1}{3}, \frac{2}{3}];\\ \frac{1}{2}(1 + c_{n}(3x-2)), & x\in (\frac{2}{3}, 1].  \end{cases} $$ and $$ c_{0}(x) = x $$ I have seen and understood the proof that the recursive definition converges uniformly to some continuous function. This function is the Cantor function of course, but I cannot see how this is. Is there some easy way to demonstrate that this recursive definition of the function is the equivalent the function above it?","I know two definitions of the Cantor function . where is the ternary expansion of . I also know the recursive definition: where and I have seen and understood the proof that the recursive definition converges uniformly to some continuous function. This function is the Cantor function of course, but I cannot see how this is. Is there some easy way to demonstrate that this recursive definition of the function is the equivalent the function above it?","c: [0,1] \to [0,1] 
c(x) = 
\begin{cases}
\sum_{n=1}^{\infty} \frac{a_{n}}{2^{n}}, \; x \in C\\
\sup_{y \leq x, \; y \in C} c(y), \; x \notin C
\end{cases}
 \{a_{n}\} x 
c(x) = \lim_{n \to \infty} c_{n}(x)
 
c_{n+1}(x) = 
\begin{cases}
\frac{1}{2} c_{n}(3x), & x\in [0, \frac{1}{3}); \\
\frac{1}{2}, & x\in[\frac{1}{3}, \frac{2}{3}];\\
\frac{1}{2}(1 + c_{n}(3x-2)), & x\in (\frac{2}{3}, 1]. 
\end{cases}
 
c_{0}(x) = x
","['real-analysis', 'measure-theory', 'cantor-set']"
39,How to strengthen $ h \big( 2 h ( x ) \big) = h ( x ) + x $ to force $ h $ to be linear?,How to strengthen  to force  to be linear?, h \big( 2 h ( x ) \big) = h ( x ) + x   h ,"Let $ h : \mathbb R \to \mathbb R $ be an injective function such that $$ h \big( 2 h ( x ) \big) = h ( x ) + x $$ for all $ x \in \mathbb R $ , and $ h ( 0 ) = 0 $ . What would be an as mild as possible extra condition such that the only solution of the equation $ h ( x ) = - \frac x 2 $ is $ x = 0 $ ? Note that the function $ x \mapsto - \frac x 2 $ satisfies the conditions, so somehow we need to fully exclude this function. For example, both the extra condition $ h ( x ) \le x \ \forall x $ as well as the extra condition $ h ( x ) \ge x \ \forall x $ force $ h ( x ) = x \ \forall x $ , and thus $ 0 $ is the only solution to the equation in question. But what about other conditions as e.g. monotone or surjective?","Let be an injective function such that for all , and . What would be an as mild as possible extra condition such that the only solution of the equation is ? Note that the function satisfies the conditions, so somehow we need to fully exclude this function. For example, both the extra condition as well as the extra condition force , and thus is the only solution to the equation in question. But what about other conditions as e.g. monotone or surjective?"," h : \mathbb R \to \mathbb R  
h \big( 2 h ( x ) \big) = h ( x ) + x
  x \in \mathbb R   h ( 0 ) = 0   h ( x ) = - \frac x 2   x = 0   x \mapsto - \frac x 2   h ( x ) \le x \ \forall x   h ( x ) \ge x \ \forall x   h ( x ) = x \ \forall x   0 ","['real-analysis', 'functions', 'real-numbers', 'functional-equations']"
40,"Show $|f(1)|<\frac{\sqrt{5}}{2}\|f\|_{L^2([0,1])} $",Show,"|f(1)|<\frac{\sqrt{5}}{2}\|f\|_{L^2([0,1])} ","Let $f(x)$ be a twice continuously differentiable real-valued function on the interval [0,1]. If $$f''(x) +xf(x) = 0 \text{, } f'(0)=0 \text{, and } \int_0^1 f(x)dx = 0$$ show that $$ |f(1)|\leq \frac{\sqrt{5}}{2}\|f\|_{L^2([0,1])}$$ So far I have the following: Since $f''(x) = -xf(x) $ we have that $$f'(x) = \int_0^x f''(t)dt +f'(0) = \int_0^x -tf(t)dt$$ I see that we will need to use Cauchy-Schwarz at sometime, but I haven't gotten the $\sqrt{5}/2$ out of it yet. Any hints would be appreciated!","Let be a twice continuously differentiable real-valued function on the interval [0,1]. If show that So far I have the following: Since we have that I see that we will need to use Cauchy-Schwarz at sometime, but I haven't gotten the out of it yet. Any hints would be appreciated!","f(x) f''(x) +xf(x) = 0 \text{, } f'(0)=0 \text{, and } \int_0^1 f(x)dx = 0  |f(1)|\leq \frac{\sqrt{5}}{2}\|f\|_{L^2([0,1])} f''(x) = -xf(x)  f'(x) = \int_0^x f''(t)dt +f'(0) = \int_0^x -tf(t)dt \sqrt{5}/2","['real-analysis', 'lebesgue-integral', 'cauchy-schwarz-inequality']"
41,"Let $f$ be a real valued function. Prove the set of points $x\in\mathbb{R}$ such that $F(y)\leq F(x)\leq F(z)$ for all $x\leq z$, $y\leq x$ is Borel.","Let  be a real valued function. Prove the set of points  such that  for all ,  is Borel.",f x\in\mathbb{R} F(y)\leq F(x)\leq F(z) x\leq z y\leq x,"Question: Let $F:\mathbb{R}\rightarrow\mathbb{R}$ be any function (not necessarily measurable, continuous, or anything else.  Prove the set of points $x\in\mathbb{R}$ such that $F(y)\leq F(x)\leq F(z)$ for all $x\leq z$ , $y\leq x$ is Borel. This question was asked here: Prove a function is Borel set , so I am trying to go off the hint in the most recent comment, but I can't seem to quite wrap my head around it.  I like what the first comment said about getting a ""line"" of sorts, but I just can't seem to wrap my head around what I am trying to see graphically.  Any help would be greatly appreciated!","Question: Let be any function (not necessarily measurable, continuous, or anything else.  Prove the set of points such that for all , is Borel. This question was asked here: Prove a function is Borel set , so I am trying to go off the hint in the most recent comment, but I can't seem to quite wrap my head around it.  I like what the first comment said about getting a ""line"" of sorts, but I just can't seem to wrap my head around what I am trying to see graphically.  Any help would be greatly appreciated!",F:\mathbb{R}\rightarrow\mathbb{R} x\in\mathbb{R} F(y)\leq F(x)\leq F(z) x\leq z y\leq x,"['real-analysis', 'measure-theory', 'borel-sets']"
42,Lebesgue integral : Changing the $\sum$ and $\int$,Lebesgue integral : Changing the  and,\sum \int,"I want to calculate $\displaystyle\int_0^1 \dfrac{x}{1-x}(-\log x)\ dx$ in Lebesgue integral theory. I calculated this using the theorem below. Theorem Let $\{f_n \}_{n=1}^{\infty}$ be a sequence of functions that are Lebesgue-measurable and non negative. And let $f:=\sum_{n=1}^{\infty} f_n$ . Then, $\displaystyle\int f dx=\sum_{n=1}^{\infty} \displaystyle\int f_n (x) dx.$ My calculation \begin{align} \displaystyle\int_0^1 \dfrac{x}{1-x}(-\log x) \ dx &=\displaystyle\int \dfrac{x}{1-x}(-\log x) \cdot \chi_{(0,1)} (x) \ dx\\ &=\displaystyle\int \bigg(\sum_{k=1}^{\infty} x^k\bigg)\cdot (-\log x) \cdot \chi_{(0,1)} (x) \ dx\\ &=\displaystyle\int \sum_{k=1}^{\infty} \bigg(x^k (-\log x) \cdot \chi_{(0,1)} (x)\bigg) \ dx\\ &=\sum_{k=1}^{\infty}  \displaystyle\int \bigg(x^k (-\log x) \cdot \chi_{(0,1)} (x)\bigg) \ dx \ (\text{ using the Theorem) }\\ &=\sum_{k=1}^{\infty}  \displaystyle\int_0^1 x^k (-\log x)  \ dx \\ &=\sum_{k=1}^{\infty} \Bigg(\bigg[\dfrac{x^{k+1}}{k+1} (-\log x)\bigg]_0^1+ \displaystyle\int_0^1 \dfrac{x^{k}}{k+1} \ dx\Bigg) \ (\text{Integration by parts})\\ &=\sum_{k=1}^{\infty} \Bigg(0+ \dfrac{1}{(k+1)^2}\Bigg) =\sum_{k=1}^{\infty}\dfrac{1}{(k+1)^2}. \end{align} But I wonder if I can change $\sum$ and $\displaystyle\int.$ For each $k$ , $x^k(-\log x)\chi_{(0,1)}(x)$ is non negative but is it Lebesgue-measurable? And why?","I want to calculate in Lebesgue integral theory. I calculated this using the theorem below. Theorem Let be a sequence of functions that are Lebesgue-measurable and non negative. And let . Then, My calculation But I wonder if I can change and For each , is non negative but is it Lebesgue-measurable? And why?","\displaystyle\int_0^1 \dfrac{x}{1-x}(-\log x)\ dx \{f_n \}_{n=1}^{\infty} f:=\sum_{n=1}^{\infty} f_n \displaystyle\int f dx=\sum_{n=1}^{\infty} \displaystyle\int f_n (x) dx. \begin{align}
\displaystyle\int_0^1 \dfrac{x}{1-x}(-\log x) \ dx
&=\displaystyle\int \dfrac{x}{1-x}(-\log x) \cdot \chi_{(0,1)} (x) \ dx\\
&=\displaystyle\int \bigg(\sum_{k=1}^{\infty} x^k\bigg)\cdot (-\log x) \cdot \chi_{(0,1)} (x) \ dx\\
&=\displaystyle\int \sum_{k=1}^{\infty} \bigg(x^k (-\log x) \cdot \chi_{(0,1)} (x)\bigg) \ dx\\
&=\sum_{k=1}^{\infty}  \displaystyle\int \bigg(x^k (-\log x) \cdot \chi_{(0,1)} (x)\bigg) \ dx \ (\text{ using the Theorem) }\\
&=\sum_{k=1}^{\infty}  \displaystyle\int_0^1 x^k (-\log x)  \ dx \\
&=\sum_{k=1}^{\infty} \Bigg(\bigg[\dfrac{x^{k+1}}{k+1} (-\log x)\bigg]_0^1+ \displaystyle\int_0^1 \dfrac{x^{k}}{k+1} \ dx\Bigg) \ (\text{Integration by parts})\\
&=\sum_{k=1}^{\infty} \Bigg(0+ \dfrac{1}{(k+1)^2}\Bigg)
=\sum_{k=1}^{\infty}\dfrac{1}{(k+1)^2}.
\end{align} \sum \displaystyle\int. k x^k(-\log x)\chi_{(0,1)}(x)","['real-analysis', 'calculus', 'integration', 'lebesgue-integral']"
43,"If $f,f_m$ are Lipschitz with $\lim_{m\to+\infty} d(f_m(x),f(x)) = 0 $, is it true that the fixed point of $f_m$ converges to the fixed point of $f$?","If  are Lipschitz with , is it true that the fixed point of  converges to the fixed point of ?","f,f_m \lim_{m\to+\infty} d(f_m(x),f(x)) = 0  f_m f","I'm going through a proof in which at one point, the authors make the following statement : Let $(X,d)$ be a complete metric space and $f:X\to X$ and $f_m:X\to X$ Lipschitz with $lip(f)<1$ and $lip(f_m)<1\; \forall m\in\mathbb{N}$ . Let $x^*_m$ the fixed point of $f_m$ and $x^*$ the fixed point of $f$ . If for each $x\in X$ we have that $$\lim_{m\to+\infty} d(f_m(x),f(x)) = 0, $$ it follows that $$\lim_{m\to+\infty} d(x^*_m,x^*) = 0 .$$ I tried to figure out how they came up with this by an argument along the lines of \begin{align*}d(x^*_m,x^*) &= d(f_m(x^*_m),f(x^*)) \leq d(f_m(x^*_m),f(x_m^*))+ d(f(x_m^*),f(x^*))\\&\leq d(f_m(x^*_m),f(x_m^*))+lip(f)d(x_m^*,x^*). \end{align*} Then $$0\leq (1-lip(f))d(f(x_m^*),f(x^*))\leq d(f_m(x^*_m),f(x_m^*)).$$ But now we can't apply the first limit because $x_m^*$ depends on $m$ . Also another thing I tried is by using the fact that $$\lim_{n \to+\infty}d(f^{\circ n}(x),x^*)=0 \; \forall x\in X$$ Where $f^{\circ n} = \underbrace{f\circ f\circ \dots \circ f}_{\text {n times}}.$ Then for a fixed $x\in X$ \begin{equation*}d(x^*_m,x^*)\leq d(x^*_m,f^{\circ n}_m(x)) + d(f^{\circ n}_m(x),f^{\circ n}(x))+ d(f^{\circ n}(x),x^*),\; \forall n\in\mathbb{N} \end{equation*} Now let $\varepsilon > 0$ and set $N$ big enough so that $d(f^{\circ N}(x),x^*)<\varepsilon/3$ and $d(f_m^{\circ N}(x),x_m^*)< \varepsilon/3$ . Then \begin{align*} d(x^*_m,x^*) &\leq d(x^*_m,f^{\circ N}_m(x)) + d(f^{\circ N}_m(x),f^{\circ N}(x))+ d(f^{\circ N}(x),x^*)\\&<2\varepsilon/3 + d(f^{\circ N}_m(x),f^{\circ N}(x)). \end{align*} Now it can be shown that $\lim\limits_{m\to+\infty} d(f^{\circ N}_m(x),f^{\circ N}(x)) =0$ , so there is some $M$ such that $d(f^{\circ N}_m(x),f^{\circ N}(x)) <\varepsilon/3\; \forall m>M$ , from which we get $$d(x^*_m,x^*)<\varepsilon.$$ But I feel like something is not right in my second attempt. Is something missing from the statement?","I'm going through a proof in which at one point, the authors make the following statement : Let be a complete metric space and and Lipschitz with and . Let the fixed point of and the fixed point of . If for each we have that it follows that I tried to figure out how they came up with this by an argument along the lines of Then But now we can't apply the first limit because depends on . Also another thing I tried is by using the fact that Where Then for a fixed Now let and set big enough so that and . Then Now it can be shown that , so there is some such that , from which we get But I feel like something is not right in my second attempt. Is something missing from the statement?","(X,d) f:X\to X f_m:X\to X lip(f)<1 lip(f_m)<1\; \forall m\in\mathbb{N} x^*_m f_m x^* f x\in X \lim_{m\to+\infty} d(f_m(x),f(x)) = 0,  \lim_{m\to+\infty} d(x^*_m,x^*) = 0 . \begin{align*}d(x^*_m,x^*) &= d(f_m(x^*_m),f(x^*)) \leq d(f_m(x^*_m),f(x_m^*))+ d(f(x_m^*),f(x^*))\\&\leq d(f_m(x^*_m),f(x_m^*))+lip(f)d(x_m^*,x^*).
\end{align*} 0\leq (1-lip(f))d(f(x_m^*),f(x^*))\leq d(f_m(x^*_m),f(x_m^*)). x_m^* m \lim_{n
\to+\infty}d(f^{\circ n}(x),x^*)=0 \; \forall x\in X f^{\circ n} = \underbrace{f\circ f\circ \dots \circ f}_{\text {n times}}. x\in X \begin{equation*}d(x^*_m,x^*)\leq d(x^*_m,f^{\circ n}_m(x)) + d(f^{\circ n}_m(x),f^{\circ n}(x))+ d(f^{\circ n}(x),x^*),\; \forall n\in\mathbb{N}
\end{equation*} \varepsilon > 0 N d(f^{\circ N}(x),x^*)<\varepsilon/3 d(f_m^{\circ N}(x),x_m^*)< \varepsilon/3 \begin{align*}
d(x^*_m,x^*) &\leq d(x^*_m,f^{\circ N}_m(x)) + d(f^{\circ N}_m(x),f^{\circ N}(x))+ d(f^{\circ N}(x),x^*)\\&<2\varepsilon/3 + d(f^{\circ N}_m(x),f^{\circ N}(x)).
\end{align*} \lim\limits_{m\to+\infty} d(f^{\circ N}_m(x),f^{\circ N}(x)) =0 M d(f^{\circ N}_m(x),f^{\circ N}(x)) <\varepsilon/3\; \forall m>M d(x^*_m,x^*)<\varepsilon.","['real-analysis', 'metric-spaces', 'lipschitz-functions', 'fixed-points']"
44,Proof explanation: Every Cauchy sequence has a limit,Proof explanation: Every Cauchy sequence has a limit,,"I have a bit of a problem with understanding the parts of the proof as stated in my book: Lemma: In a complete ordered field, every Cauchy sequence has a limit. Proof: Let $\left(a_{n}\right)$ be a Cauchy sequence in $F$ . By the argument of lemma $9.15$ of chapter 9 (carried out in $F$ ) the sequence is bounded. Hence so is every subset of elements in the sequence. Define $$ b_{N}=\text { the least upper bound of  }\left\{a_{N}, a_{N+1}, a_{N+2}, \ldots\right\} $$ This exists by completeness. Clearly $$ b_{0} \geq b_{1} \geq b_{2} \geq \cdots $$ and the sequence $\left(b_{n}\right)$ is bounded below-say, by any lower bound for $\left(a_{n}\right)$ . Hence we can define $c=$ the greatest lower bound of $\left(b_{n}\right)$ . We claim that $c$ is the limit of the original sequence $\left(a_{n}\right)$ To prove this, let $\varepsilon>0$ . Suppose that there exist only finitely many values of $n$ with $$ c-\frac{1}{2} \varepsilon<a_{n}<c+\frac{1}{2} \varepsilon $$ Then we may choose $N$ such that for all $n>N$ , $$ a_{n} \leq c-\frac{1}{2} \varepsilon \text { or } a_{n} \geq c+\frac{1}{2} \varepsilon $$ But there exists $N_{1}>N$ such that if $m, n>N_{1}$ then $\left|a_{m}-a_{n}\right|<\frac{1}{2} \varepsilon$ . Hence $$ \text { for all } n>N_{1}, a_{n} \leq c-\frac{1}{2} \varepsilon $$ Or $$ \text { for all } n>N_{1}, a_{n} \geq c+\frac{1}{2} \varepsilon $$ The latter condition implies that there exists some $m$ with $a_{n}>b_{m}$ for all $n>N_{1}$ , which contradicts the definition of $b_{m}$ . But the former implies that we may change $b_{N_{1}}$ to $b_{N_{1}}-\frac{1}{2} \varepsilon$ , which again contradicts the definition of $b_{N_{1}}$ . It follows that for any $M$ there exists $m>M$ such that $$ c-\frac{1}{2} \varepsilon<a_{m}<c+\frac{1}{2}  \varepsilon $$ Since $\left(a_{n}\right)$ is Cauchy, there exists $M_{1}>M$ such that $\left|a_{n}-a_{m}\right|<\frac{1}{2} \varepsilon$ for $m, n>M_{1} .$ Hence for $n>M_{1}$ , $$ c-\varepsilon<a_{n}<c+\varepsilon $$ But this implies that $\lim a_{n}=c$ as claimed. Specifically I don't understand why the 2 bolded sentences are needed in the proof. Don't we already establish in the previous assumption ( Suppose that there exist only finitely... ) that for all $n>N,  a_{n} \leq c-\frac{1}{2} \varepsilon \text { or } a_{n} \geq c+\frac{1}{2} \varepsilon $ and from that, the paragraph afterwards ( The latter condition... ) follows? Why does the Cauchy criterion need to be invoked?","I have a bit of a problem with understanding the parts of the proof as stated in my book: Lemma: In a complete ordered field, every Cauchy sequence has a limit. Proof: Let be a Cauchy sequence in . By the argument of lemma of chapter 9 (carried out in ) the sequence is bounded. Hence so is every subset of elements in the sequence. Define This exists by completeness. Clearly and the sequence is bounded below-say, by any lower bound for . Hence we can define the greatest lower bound of . We claim that is the limit of the original sequence To prove this, let . Suppose that there exist only finitely many values of with Then we may choose such that for all , But there exists such that if then . Hence Or The latter condition implies that there exists some with for all , which contradicts the definition of . But the former implies that we may change to , which again contradicts the definition of . It follows that for any there exists such that Since is Cauchy, there exists such that for Hence for , But this implies that as claimed. Specifically I don't understand why the 2 bolded sentences are needed in the proof. Don't we already establish in the previous assumption ( Suppose that there exist only finitely... ) that for all and from that, the paragraph afterwards ( The latter condition... ) follows? Why does the Cauchy criterion need to be invoked?","\left(a_{n}\right) F 9.15 F  b_{N}=\text { the least upper bound of
 }\left\{a_{N}, a_{N+1}, a_{N+2}, \ldots\right\}   b_{0} \geq b_{1} \geq b_{2} \geq \cdots  \left(b_{n}\right) \left(a_{n}\right) c= \left(b_{n}\right) c \left(a_{n}\right) \varepsilon>0 n  c-\frac{1}{2} \varepsilon<a_{n}<c+\frac{1}{2} \varepsilon  N n>N  a_{n} \leq c-\frac{1}{2} \varepsilon \text { or } a_{n} \geq c+\frac{1}{2} \varepsilon  N_{1}>N m, n>N_{1} \left|a_{m}-a_{n}\right|<\frac{1}{2} \varepsilon  \text {
for all } n>N_{1}, a_{n} \leq c-\frac{1}{2} \varepsilon   \text
{ for all } n>N_{1}, a_{n} \geq c+\frac{1}{2} \varepsilon  m a_{n}>b_{m} n>N_{1} b_{m} b_{N_{1}} b_{N_{1}}-\frac{1}{2} \varepsilon b_{N_{1}} M m>M  c-\frac{1}{2} \varepsilon<a_{m}<c+\frac{1}{2}
 \varepsilon  \left(a_{n}\right) M_{1}>M \left|a_{n}-a_{m}\right|<\frac{1}{2} \varepsilon m, n>M_{1} . n>M_{1} 
c-\varepsilon<a_{n}<c+\varepsilon  \lim a_{n}=c n>N,  a_{n} \leq c-\frac{1}{2} \varepsilon \text { or } a_{n} \geq c+\frac{1}{2} \varepsilon ","['real-analysis', 'proof-explanation', 'cauchy-sequences']"
45,Can we get quantitative information from the Dominated (resp. Monotone) Convergence Theorem?,Can we get quantitative information from the Dominated (resp. Monotone) Convergence Theorem?,,"It's a common exercise in measure theory courses to solve problems like the following: Compute $$\lim_{n \to \infty} \int_0^\infty (1+ (x/n))^{-n} \sin(x/n)\,\mathrm{d}x.$$ (cf. Folland Ch 2.4, exercise 28a). The dominated convergence theorem makes short work of things like this, since (for large $n$ ) we get roughly exponential decay. A little bit of work turns this intuition into a pointwise bound, and then we swap $\lim \int = \int \lim$ to solve the problem. Often in computer science, combinatorics, etc. this is not enough. We're interested in the rate of convergence. That is, it's not enough to know $$ \lim_{n \to \infty} \int_0^\infty (1+ (x/n))^{-n} \sin(x/n)\ dx  = \int_0^\infty 0\  dx = 0 $$ We want to say that $$\int_0^\infty (1+ (x/n))^{-n} \sin(x/n)\ dx = O(1/n),$$ or something similar. I'm not well versed with computing integrals while using estimates of this kind, but I'm sure that people have thought about this. Is there a way to get these kinds of error estimates from the dominated (resp. monotone) convergence theorem? I would be happy to use this problem (or any others that you may feel are more emblematic of the technique) as a case study. I suspect one could argue by integrating some error bounds between $f_n$ and $f$ , and it may be helpful to look at, say $f_n \cdot \chi_{[0,n]}$ in order to make the error bounds more useful. This feels like it is kind of sidestepping the convergence theorems entirely, though, and I'm curious if there's a more elegant approach. Thanks in advance! ^_^","It's a common exercise in measure theory courses to solve problems like the following: Compute (cf. Folland Ch 2.4, exercise 28a). The dominated convergence theorem makes short work of things like this, since (for large ) we get roughly exponential decay. A little bit of work turns this intuition into a pointwise bound, and then we swap to solve the problem. Often in computer science, combinatorics, etc. this is not enough. We're interested in the rate of convergence. That is, it's not enough to know We want to say that or something similar. I'm not well versed with computing integrals while using estimates of this kind, but I'm sure that people have thought about this. Is there a way to get these kinds of error estimates from the dominated (resp. monotone) convergence theorem? I would be happy to use this problem (or any others that you may feel are more emblematic of the technique) as a case study. I suspect one could argue by integrating some error bounds between and , and it may be helpful to look at, say in order to make the error bounds more useful. This feels like it is kind of sidestepping the convergence theorems entirely, though, and I'm curious if there's a more elegant approach. Thanks in advance! ^_^","\lim_{n \to \infty} \int_0^\infty (1+ (x/n))^{-n} \sin(x/n)\,\mathrm{d}x. n \lim \int = \int \lim 
\lim_{n \to \infty} \int_0^\infty (1+ (x/n))^{-n} \sin(x/n)\ dx 
= \int_0^\infty 0\  dx
= 0
 \int_0^\infty (1+ (x/n))^{-n} \sin(x/n)\ dx = O(1/n), f_n f f_n \cdot \chi_{[0,n]}","['real-analysis', 'integration', 'measure-theory', 'reference-request', 'asymptotics']"
46,Mathematical constants and approximations of irrational numbers,Mathematical constants and approximations of irrational numbers,,"I found two examples where various constants have some surprising properties, related to the approximations of real numbers (you can convince yourself with Wolfram Alpha): The real number $\pi^{\pi^{1/\pi}}\approx5.19644$ has the same first $\lfloor \pi \rfloor$ decimals as the number $3\sqrt{\lfloor \pi\rfloor}\approx5.19615$ . If we replace $\pi$ with $e$ in the above phrase, we get the same result: $e^{e^{1/e}}\approx4.24044$ , $3\sqrt{\lfloor   e\rfloor}\approx4.24264$ . The Euler-Mascheroni constant, $\gamma\approx0.57721$ , has the same first $3$ decimals as $\frac{1}{\sqrt3}\approx0.57735$ . It is very unlikely that these examples do have a strong mathematical explanation, therefore they might be pure coincidence. For the first one, it seems obvious that if I change the $100^{th}$ decimal of $\pi$ , for example, the result still holds. Also, over a ,,small'' neighbourhood of $\pi$ or $e$ , the result is verified. Do you know other similar examples, where mathematical constants ""almost"" satisfy a short equation or appear in an unexpected way (I mean, not related to their usual definitions and applications)? You are more than welcome to post an answer, your effort will be appreciated! P.S. This question is mainly recreational and is the result of my pure imagination.","I found two examples where various constants have some surprising properties, related to the approximations of real numbers (you can convince yourself with Wolfram Alpha): The real number has the same first decimals as the number . If we replace with in the above phrase, we get the same result: , . The Euler-Mascheroni constant, , has the same first decimals as . It is very unlikely that these examples do have a strong mathematical explanation, therefore they might be pure coincidence. For the first one, it seems obvious that if I change the decimal of , for example, the result still holds. Also, over a ,,small'' neighbourhood of or , the result is verified. Do you know other similar examples, where mathematical constants ""almost"" satisfy a short equation or appear in an unexpected way (I mean, not related to their usual definitions and applications)? You are more than welcome to post an answer, your effort will be appreciated! P.S. This question is mainly recreational and is the result of my pure imagination.","\pi^{\pi^{1/\pi}}\approx5.19644 \lfloor \pi \rfloor 3\sqrt{\lfloor \pi\rfloor}\approx5.19615 \pi e e^{e^{1/e}}\approx4.24044 3\sqrt{\lfloor 
 e\rfloor}\approx4.24264 \gamma\approx0.57721 3 \frac{1}{\sqrt3}\approx0.57735 100^{th} \pi \pi e","['real-analysis', 'recreational-mathematics', 'real-numbers', 'floating-point']"
47,"Calculate $\int_0^1 \left\lfloor \frac{2020}{x} \right\rfloor - 2020\left\lfloor\frac1x\right\rfloor\,dx$",Calculate,"\int_0^1 \left\lfloor \frac{2020}{x} \right\rfloor - 2020\left\lfloor\frac1x\right\rfloor\,dx","Calculate the definite integral $$\int_0^1 \left\lfloor \frac{2020}{x} \right\rfloor - 2020\left\lfloor\frac1x\right\rfloor\,dx$$ My Attempt: Let's make a start. Write $[2020/x]=n\ge 2020$ then ${{2020}\over {n+1}}\le 1/x < {{2020}\over n}$ and ${n\over {2020}}\le 1/x <{{n+1}\over {2020}}$ . Hence $[1/x]=[n/2020]$ . For the integral we find $$\sum_{n=2020}^\infty \int_{2020/(n+1)}^{2020/n} (n-2020[n/2020]) dx=2020\sum_{2020}^\infty {1\over {n(n+1)}} (n-2020[n/2020]) $$ In order to get $[n/2020] = m$ we need $m \leq n/2020 < m+1$ or $2020m \leq n \leq 2020m + 2019$ . Now write: $$2020\sum_{n=2020}^\infty \frac{1}{n(n+1)} (n-2020\lfloor n/2020\rfloor) = 2020\sum_{m=1}^\infty \sum_{n=2020m}^{2020m+2019} \frac{1}{n(n+1)} (n-2020m)$$ $$ = 2020\sum_{m=1}^\infty \left[\left(\sum_{n=2020m}^{2020m+2019} \frac{1}{n+1}\right) - 2020m\left(\frac{1}{2020m} - \frac{1}{2020(m+1)}\right)\right]$$ $$ = 2020\sum_{m=1}^\infty \left[\left(\sum_{n=2020m}^{2020m+2019} \frac{1}{n+1}\right) - \frac{1}{m+1}\right]$$ Now an analysis of partial sums... $$\sum_{m=1}^N \left[\left(\sum_{n=2020m}^{2020m+2019} \frac{1}{n+1}\right) - \frac{1}{m+1}\right] = \sum_{k=2021}^{2020N+2020} \frac{1}{k} - \sum_{k=2}^{N+1} \frac1k$$ $$ = H_{2020N+2020} - H_{N+1} - H_{2020} + 1 \to \log(2020) - H_{2020} + 1$$ So the answer is $\boxed{2020\left(\log(2020) - H_{2020} + 1\right)}$ . However, I still wonder whether my answer is correct. Please help on this problem, thank you.","Calculate the definite integral My Attempt: Let's make a start. Write then and . Hence . For the integral we find In order to get we need or . Now write: Now an analysis of partial sums... So the answer is . However, I still wonder whether my answer is correct. Please help on this problem, thank you.","\int_0^1 \left\lfloor \frac{2020}{x} \right\rfloor - 2020\left\lfloor\frac1x\right\rfloor\,dx [2020/x]=n\ge 2020 {{2020}\over {n+1}}\le 1/x < {{2020}\over n} {n\over {2020}}\le 1/x <{{n+1}\over {2020}} [1/x]=[n/2020] \sum_{n=2020}^\infty \int_{2020/(n+1)}^{2020/n} (n-2020[n/2020]) dx=2020\sum_{2020}^\infty {1\over {n(n+1)}} (n-2020[n/2020])  [n/2020] = m m \leq n/2020 < m+1 2020m \leq n \leq 2020m + 2019 2020\sum_{n=2020}^\infty \frac{1}{n(n+1)} (n-2020\lfloor n/2020\rfloor) = 2020\sum_{m=1}^\infty \sum_{n=2020m}^{2020m+2019} \frac{1}{n(n+1)} (n-2020m)  = 2020\sum_{m=1}^\infty \left[\left(\sum_{n=2020m}^{2020m+2019} \frac{1}{n+1}\right) - 2020m\left(\frac{1}{2020m} - \frac{1}{2020(m+1)}\right)\right]  = 2020\sum_{m=1}^\infty \left[\left(\sum_{n=2020m}^{2020m+2019} \frac{1}{n+1}\right) - \frac{1}{m+1}\right] \sum_{m=1}^N \left[\left(\sum_{n=2020m}^{2020m+2019} \frac{1}{n+1}\right) - \frac{1}{m+1}\right] = \sum_{k=2021}^{2020N+2020} \frac{1}{k} - \sum_{k=2}^{N+1} \frac1k  = H_{2020N+2020} - H_{N+1} - H_{2020} + 1 \to \log(2020) - H_{2020} + 1 \boxed{2020\left(\log(2020) - H_{2020} + 1\right)}","['real-analysis', 'calculus', 'analysis', 'harmonic-numbers']"
48,Limit of $a_{n+1} = a_{n}(1+1/n^2)$,Limit of,a_{n+1} = a_{n}(1+1/n^2),"I read the following exercise : Let $(a_n)$ be a sequence such as : $a_1 = 2$ , $a_{n+1} = a_n.(1+1/n^2)$ Prove that $\lim_{n \rightarrow \infty} a_n$ exists. Hint : $\ln : \: (0, \infty) \rightarrow \mathbb{R} \:$ is a continuous, increasing function such as $\ln(1+x) \leq x$ and $\ln(ab) = \ln(a)+\ln(b)$ . It is quite clear that $a_n$ is increasing but then I don't really know how to proceed. Could you please help me ? Thanks.","I read the following exercise : Let be a sequence such as : , Prove that exists. Hint : is a continuous, increasing function such as and . It is quite clear that is increasing but then I don't really know how to proceed. Could you please help me ? Thanks.","(a_n) a_1 = 2 a_{n+1} = a_n.(1+1/n^2) \lim_{n \rightarrow \infty} a_n \ln : \: (0, \infty) \rightarrow \mathbb{R} \: \ln(1+x) \leq x \ln(ab) = \ln(a)+\ln(b) a_n","['real-analysis', 'sequences-and-series', 'limits']"
49,Proving $\lim_{x \to 1}\frac{x+1}{x-2} + x = -1$ using definition,Proving  using definition,\lim_{x \to 1}\frac{x+1}{x-2} + x = -1,"I got a question regarding my answer of proving limit using epsilon-delta, here's the question Prove $\lim_{x \to 1}\frac{x+1}{x-2} + x = -1$ Here's the answer I've come up so far let $f(x) = \frac{x+1}{x-2} + x$ by algebra manipulation, we get $|f(x) - (-1)| = |\frac{x+1}{x-2} + x +1| $ $=|\frac{x^2 - 1}{x-2}|$ $=|\frac{(x-1)(x+1)}{x-2}|$ let $|x-1| < 1$ , by triangle inequality we get $|x| < 2$ , then $|x + 1| < 3$ and $|x - 2| < 1$ now, using the definiton of limit, for every $\epsilon > 0$ , there exist $\delta = min\{1, \frac{\epsilon}{3}\}$ such that if $0 < |x - 1| < \delta$ then, $|f(x) - (-1)| = |\frac{x+1}{x-2} + x +1|$ $=|\frac{(x-1)(x+1)}{x-2}|$ $=|\frac{1 \cdot 3}{1}|$ $< \epsilon$ Is this correct? honestly I'm not sure on getting the upper bound of $|x-2|$ , so I used the assumption of $|x-1| < 1$ Any tips would help, thanks beforehand.","I got a question regarding my answer of proving limit using epsilon-delta, here's the question Prove Here's the answer I've come up so far let by algebra manipulation, we get let , by triangle inequality we get , then and now, using the definiton of limit, for every , there exist such that if then, Is this correct? honestly I'm not sure on getting the upper bound of , so I used the assumption of Any tips would help, thanks beforehand.","\lim_{x \to 1}\frac{x+1}{x-2} + x = -1 f(x) = \frac{x+1}{x-2} + x |f(x) - (-1)| = |\frac{x+1}{x-2} + x +1|  =|\frac{x^2 - 1}{x-2}| =|\frac{(x-1)(x+1)}{x-2}| |x-1| < 1 |x| < 2 |x + 1| < 3 |x - 2| < 1 \epsilon > 0 \delta = min\{1, \frac{\epsilon}{3}\} 0 < |x - 1| < \delta |f(x) - (-1)| = |\frac{x+1}{x-2} + x +1| =|\frac{(x-1)(x+1)}{x-2}| =|\frac{1 \cdot 3}{1}| < \epsilon |x-2| |x-1| < 1","['real-analysis', 'calculus', 'limits', 'epsilon-delta']"
50,"If $f$ is Riemann integrable but not continuous on $[0,1]$, does $\lim_{n \to \infty} \left( \int_0^1 |f(x)|^n dx \right)^{\frac{1}{n}}$ exists?","If  is Riemann integrable but not continuous on , does  exists?","f [0,1] \lim_{n \to \infty} \left( \int_0^1 |f(x)|^n dx \right)^{\frac{1}{n}}","Suppose $f: [0,1] \to \mathbb R$ is Riemann integrable on $[0,1]$ , but not continuous on $[0,1]$ . Let $$a_n = \left( \int_0^1 |f(x)|^n dx \right)^{\frac{1}{n}}$$ for $n \in \mathbb N$ . Does $\lim_{n\to\infty} a_n$ exists? If it does, what is it equal to? If $f$ is continuous, I know that $(a_n)$ converges to $M = \sup\{{|f(x)|: x \in [0,1]}\}$ . But the proofs that I found for this case rely the continuity of $f$ to show that $\liminf_{n\to\infty} a_n \geq M$ . My idea is to use $M = \sup\{|f(x)|: x\in C\}$ instead, where $C\subseteq [0,1]$ is the set of all points at which $f$ is continuous. Then, if my reasoning is correct, there must be a $c \in C$ such that $\lim_{x\to c^+} f(x) = M$ or $\lim_{x\to c^-} f(x) = M$ . Given an arbitrary $\varepsilon > 0$ , perhaps I can then construct an interval $I \subset [0,1]$ such that $|f(x)| \geq M-\varepsilon$ for all $x \in I$ . After that, the remaining parts should be similar to the continuous case. Nevertheless, I'm quite sure that there is some error in my line of reasoning, or maybe there is much more to be demanded for this argument to be complete. From what I read, the term $a_n$ is actually $\|f\|_n$ (the $L^n$ norm), so $(a_n)$ should converge to $\|f\|_\infty$ as $n \to \infty$ . However, my current understanding is limited to Riemann integration, without any knowledge whatsoever on measure theory and function spaces. Is there a way to prove the convergence of $(a_n)$ without resorting to measure theory, or even Lebesgue's Criterion?","Suppose is Riemann integrable on , but not continuous on . Let for . Does exists? If it does, what is it equal to? If is continuous, I know that converges to . But the proofs that I found for this case rely the continuity of to show that . My idea is to use instead, where is the set of all points at which is continuous. Then, if my reasoning is correct, there must be a such that or . Given an arbitrary , perhaps I can then construct an interval such that for all . After that, the remaining parts should be similar to the continuous case. Nevertheless, I'm quite sure that there is some error in my line of reasoning, or maybe there is much more to be demanded for this argument to be complete. From what I read, the term is actually (the norm), so should converge to as . However, my current understanding is limited to Riemann integration, without any knowledge whatsoever on measure theory and function spaces. Is there a way to prove the convergence of without resorting to measure theory, or even Lebesgue's Criterion?","f: [0,1] \to \mathbb R [0,1] [0,1] a_n = \left( \int_0^1 |f(x)|^n dx \right)^{\frac{1}{n}} n \in \mathbb N \lim_{n\to\infty} a_n f (a_n) M = \sup\{{|f(x)|: x \in [0,1]}\} f \liminf_{n\to\infty} a_n \geq M M = \sup\{|f(x)|: x\in C\} C\subseteq [0,1] f c \in C \lim_{x\to c^+} f(x) = M \lim_{x\to c^-} f(x) = M \varepsilon > 0 I \subset [0,1] |f(x)| \geq M-\varepsilon x \in I a_n \|f\|_n L^n (a_n) \|f\|_\infty n \to \infty (a_n)","['real-analysis', 'integration', 'limits', 'riemann-integration']"
51,The relation between uniform integrability and dominated convergence theorem in the case of counting measure,The relation between uniform integrability and dominated convergence theorem in the case of counting measure,,"I want to discover the analog between uniform integrability (UI) and the dominated convergence theorem (DCT) for infinite series. An infinite series is an integral with respect to counting measure. That is, $\sum_{k=0}^{\infty}f(k)=\int_{\mathbb{Z}_{\geq 0}}f(k)\mu(dk),$ where $\mu$ is the counting measure.  We can then say a function $f$ is integrable with respect to the counting measure if $\sum_{k=0}^{\infty}|f(k)|=\int_{\mathbb{Z}_{\geq 0}}|f(k)|\mu(dk)<\infty.$ My first step is to reformulate DCT and UI. For DCT, we are consider a sequence of $\mu-$ integrable function $f_{n}(k)$ such that $f_{n}(k)\longrightarrow f(k)$ as $n\rightarrow\infty$ , and for each $n$ , $|f_{n}(k)|\leq g(k)$ for some $\mu-$ integrable $g(k)$ . Then, $$\lim_{n\rightarrow\infty}\sum_{k=0}^{\infty}f_{n}(k)=\sum_{k=0}^{\infty}\lim_{n\rightarrow\infty}f_{n}(k)=\sum_{k=0}^{\infty}f(k).$$ However, I don't know how to formulate the UI. The definition UI I have is that: A family $\{f_{\alpha\}}$ of integrable function is uniform integrable if for any $\epsilon>0$ , there is a $\delta>0$ such that whenever $\lambda(A)<\delta$ , $\sup_{\alpha}\int_{A}|f_{\alpha}|\lambda(dx)<\epsilon,$ where $\lambda$ here is a general measure. I can reformulate this without problem, but I don't understand what does it mean for $\mu(A)<\delta$ when $\mu$ is a counting measure. I read some online notes saying that DCT is a consequence of UI and Egoroff Theorem, I understand this, since Egoroff will give you a set $A$ on which the convergence is uniform and UI can make the integral on $A^{c}$ to be negligible. But this requires the measure space to be finite. I am not sure if we can apply this to counting measure, i.e. is $(\mathbb{N},\mathcal{P}(\mathbb{N}),\mu)$ a finite measure space? Even I can answer this question, I don't want to stop here.I believe that  there must be something special in the case of counting measure, since $\mu(A)<\delta$ in the case of counting measure is still mysterious to me, but for now I don't know where to continue. Thank you! Edit: Example Okay, I worked some examples, but I still don't quite understand what fails. For example, consider the sequence $f_{n}(x)$ on the integer $X:=\{1,2,3,\cdots\}$ , defined by $$f_{n}(x)=\left\{   \begin{array}{ll} \frac{1}{n},\ \ \text{if}\ \ x=1,2,\cdots, n\\ 0,\ \ \text{if}\ \ x\geq n+1. \end{array} \right.$$ Note that $\sum_{k=1}^{\infty}f_{n}(x)=1$ for every $n$ , but $\lim_{n\rightarrow\infty}f_{n}(x)=0$ for every $x$ , so we cannot interchange the limit and summation. As suggested by the comment, uniform integrability is guaranteed in the case of counting measure, so What fails here? Edit: Potential Answer As Rivers Mcforge said, the example above does not satisfy the boundedness requirement in DCT. Along with the comment of Lorenzo, I found a connection between the boundedness requirement and tightness. As Lorenzo suggested, the sequence in above example is not $\mu-$ tight, and since we are over an infinite measure space, the Vitali convergence theorem needs the tightness. (The uniform integrability was given to us for free over our space, as suggested by both answers below). Eventually, I found that in the case of our counting measure space, a sequence that satisfies DCT will also satisfy Vitali. In other words, we can use Vitali to prove DCT (in the case of our counting measure space $(\mathbb{N},\mathcal{P}(\mathbb{N}),\mu)$ ). Indeed, recall that for any measure $\lambda$ and any measurable set $E$ (not necessarily of finite measure), if $f$ is $\lambda-$ integrable over $E$ , then for each $\epsilon>0$ , there is a set of finite measure $E_{0}$ for which $$\int_{E\setminus E_{0}}|f(x)|\lambda(dx)<\epsilon.$$ Now, suppose $f_{n}(x)$ is an sequence of functions on $X$ that converges $\mu-$ almost surely to $f(x)$ . It is for free that $f_{n}(x)$ is uniformly integrable. If $|f_{n}(x)|\leq g(x)$ for all $n$ and $x$ , where $g(x)$ is $\mu-$ integrable, then by the above recalled fact, for each $\epsilon>0$ , there is a subset $X_{0}$ of $X$ of finite measure for which $$\int_{E\setminus E_{0}}|f_{n}(x)|\mu(dx)\leq \int_{E\setminus E_{0}}|g(x)|\mu(dx)<\epsilon,\ \text{for all}\ n.$$ Hence, the family $\{f_{n}(x)\}$ is tight. Then, it follows from Vitali that we can interchange the summation (the $\mu-$ integral) with the limit of $n\rightarrow\infty.$ Thus, the confusion of DCT follows. I am not sure if tightness can conversely imply the boundedness requirement in DCT.","I want to discover the analog between uniform integrability (UI) and the dominated convergence theorem (DCT) for infinite series. An infinite series is an integral with respect to counting measure. That is, where is the counting measure.  We can then say a function is integrable with respect to the counting measure if My first step is to reformulate DCT and UI. For DCT, we are consider a sequence of integrable function such that as , and for each , for some integrable . Then, However, I don't know how to formulate the UI. The definition UI I have is that: A family of integrable function is uniform integrable if for any , there is a such that whenever , where here is a general measure. I can reformulate this without problem, but I don't understand what does it mean for when is a counting measure. I read some online notes saying that DCT is a consequence of UI and Egoroff Theorem, I understand this, since Egoroff will give you a set on which the convergence is uniform and UI can make the integral on to be negligible. But this requires the measure space to be finite. I am not sure if we can apply this to counting measure, i.e. is a finite measure space? Even I can answer this question, I don't want to stop here.I believe that  there must be something special in the case of counting measure, since in the case of counting measure is still mysterious to me, but for now I don't know where to continue. Thank you! Edit: Example Okay, I worked some examples, but I still don't quite understand what fails. For example, consider the sequence on the integer , defined by Note that for every , but for every , so we cannot interchange the limit and summation. As suggested by the comment, uniform integrability is guaranteed in the case of counting measure, so What fails here? Edit: Potential Answer As Rivers Mcforge said, the example above does not satisfy the boundedness requirement in DCT. Along with the comment of Lorenzo, I found a connection between the boundedness requirement and tightness. As Lorenzo suggested, the sequence in above example is not tight, and since we are over an infinite measure space, the Vitali convergence theorem needs the tightness. (The uniform integrability was given to us for free over our space, as suggested by both answers below). Eventually, I found that in the case of our counting measure space, a sequence that satisfies DCT will also satisfy Vitali. In other words, we can use Vitali to prove DCT (in the case of our counting measure space ). Indeed, recall that for any measure and any measurable set (not necessarily of finite measure), if is integrable over , then for each , there is a set of finite measure for which Now, suppose is an sequence of functions on that converges almost surely to . It is for free that is uniformly integrable. If for all and , where is integrable, then by the above recalled fact, for each , there is a subset of of finite measure for which Hence, the family is tight. Then, it follows from Vitali that we can interchange the summation (the integral) with the limit of Thus, the confusion of DCT follows. I am not sure if tightness can conversely imply the boundedness requirement in DCT.","\sum_{k=0}^{\infty}f(k)=\int_{\mathbb{Z}_{\geq 0}}f(k)\mu(dk), \mu f \sum_{k=0}^{\infty}|f(k)|=\int_{\mathbb{Z}_{\geq 0}}|f(k)|\mu(dk)<\infty. \mu- f_{n}(k) f_{n}(k)\longrightarrow f(k) n\rightarrow\infty n |f_{n}(k)|\leq g(k) \mu- g(k) \lim_{n\rightarrow\infty}\sum_{k=0}^{\infty}f_{n}(k)=\sum_{k=0}^{\infty}\lim_{n\rightarrow\infty}f_{n}(k)=\sum_{k=0}^{\infty}f(k). \{f_{\alpha\}} \epsilon>0 \delta>0 \lambda(A)<\delta \sup_{\alpha}\int_{A}|f_{\alpha}|\lambda(dx)<\epsilon, \lambda \mu(A)<\delta \mu A A^{c} (\mathbb{N},\mathcal{P}(\mathbb{N}),\mu) \mu(A)<\delta f_{n}(x) X:=\{1,2,3,\cdots\} f_{n}(x)=\left\{
  \begin{array}{ll}
\frac{1}{n},\ \ \text{if}\ \ x=1,2,\cdots, n\\
0,\ \ \text{if}\ \ x\geq n+1.
\end{array}
\right. \sum_{k=1}^{\infty}f_{n}(x)=1 n \lim_{n\rightarrow\infty}f_{n}(x)=0 x \mu- (\mathbb{N},\mathcal{P}(\mathbb{N}),\mu) \lambda E f \lambda- E \epsilon>0 E_{0} \int_{E\setminus E_{0}}|f(x)|\lambda(dx)<\epsilon. f_{n}(x) X \mu- f(x) f_{n}(x) |f_{n}(x)|\leq g(x) n x g(x) \mu- \epsilon>0 X_{0} X \int_{E\setminus E_{0}}|f_{n}(x)|\mu(dx)\leq \int_{E\setminus E_{0}}|g(x)|\mu(dx)<\epsilon,\ \text{for all}\ n. \{f_{n}(x)\} \mu- n\rightarrow\infty.","['real-analysis', 'sequences-and-series', 'measure-theory', 'lebesgue-integral']"
52,What is the difference between total variation and arc length?,What is the difference between total variation and arc length?,,"Let $f:[a,b] \rightarrow \mathbb R$ be $C^1$ . Is the length of $\{f(x): x \in [a,b] \}$ and the total variation of $f$ the same thing ? The definition are extremely similar to each others: The total variation of a real-valued (or more generally complex-valued) function $f$ , defined on an interval $[a, b] \subset \mathbb{R}$ is the quantity $$ V_{b}^{a}(f)=\sup _{\mathcal{P}} \sum_{i=0}^{n_{P}-1}\left|f\left(x_{i+1}\right)-f\left(x_{i}\right)\right| $$ where the supremum runs over the set of all partitions $\mathcal{P}=\left\{P=\left\{x_{0}, \ldots, x_{n_{P}}\right\} \mid P\right.$ is a partition of $\left.[a, b]\right\}$ of the given interval. Let $f:[a, b] \rightarrow \mathbb{R}^{n}$ be a continuously differentiable function. The length of the curve defined by $f$ can be defined as the limit of the sum of line segment lengths for a regular partition of $[a, b]$ as the number of segments approaches infinity. This means $$ L(f)=\lim _{N \rightarrow \infty} \sum_{i=1}^{N}\left|f\left(t_{i}\right)-f\left(t_{i-1}\right)\right| $$ where $t_{i}=a+i(b-a) / N=a+i \Delta t$ for $i=0,1, \ldots, N .$ This defintiton is equivalent to the standard definition of arc length as an integral: $$ \lim _{N \rightarrow \infty} \sum_{i=1}^{N}\left|f\left(t_{i}\right)-f\left(t_{i-1}\right)\right|=\lim _{N \rightarrow \infty} \sum_{i=1}^{N}\left|\frac{f\left(t_{i}\right)-f\left(t_{i-1}\right)}{\Delta t}\right| \Delta t=\int_{a}^{b}\left|f^{\prime}(t)\right| d t $$","Let be . Is the length of and the total variation of the same thing ? The definition are extremely similar to each others: The total variation of a real-valued (or more generally complex-valued) function , defined on an interval is the quantity where the supremum runs over the set of all partitions is a partition of of the given interval. Let be a continuously differentiable function. The length of the curve defined by can be defined as the limit of the sum of line segment lengths for a regular partition of as the number of segments approaches infinity. This means where for This defintiton is equivalent to the standard definition of arc length as an integral:","f:[a,b] \rightarrow \mathbb R C^1 \{f(x): x \in [a,b] \} f f [a, b] \subset \mathbb{R} 
V_{b}^{a}(f)=\sup _{\mathcal{P}} \sum_{i=0}^{n_{P}-1}\left|f\left(x_{i+1}\right)-f\left(x_{i}\right)\right|
 \mathcal{P}=\left\{P=\left\{x_{0}, \ldots, x_{n_{P}}\right\} \mid P\right. \left.[a, b]\right\} f:[a, b] \rightarrow \mathbb{R}^{n} f [a, b] 
L(f)=\lim _{N \rightarrow \infty} \sum_{i=1}^{N}\left|f\left(t_{i}\right)-f\left(t_{i-1}\right)\right|
 t_{i}=a+i(b-a) / N=a+i \Delta t i=0,1, \ldots, N . 
\lim _{N \rightarrow \infty} \sum_{i=1}^{N}\left|f\left(t_{i}\right)-f\left(t_{i-1}\right)\right|=\lim _{N \rightarrow \infty} \sum_{i=1}^{N}\left|\frac{f\left(t_{i}\right)-f\left(t_{i-1}\right)}{\Delta t}\right| \Delta t=\int_{a}^{b}\left|f^{\prime}(t)\right| d t
",['real-analysis']
53,Is the series $\sum_\limits{n=1}^\infty\frac{n}{n^3+1}$ convergent or divergent? [duplicate],Is the series  convergent or divergent? [duplicate],\sum_\limits{n=1}^\infty\frac{n}{n^3+1},"This question already has an answer here : Convergent series and divergent series $\sum\limits_{n=1}^\infty \frac{n}{n^3+1}$ and $\sum\limits_{n=1}^\infty \frac{n^2+1}{n^3+1}$ (1 answer) Closed 3 years ago . Is the series $$\sum_\limits{n=1}^\infty\frac{n}{n^3+1}$$ convergent or divergent? My answer is the following. Could anyone tell me if it is correct? Since $$0<\frac{n}{n^3+1}<\frac{1}{n^2}\;\;,\;\;\;\;\forall n\in\mathbb{N}$$ and the series $$\sum_\limits{n=1}^\infty\frac{1}{n^2}$$ is convergent, by applying comparison test, we get that the series $$\sum_\limits{n=1}^\infty\frac{n}{n^3+1}$$ is convergent too.","This question already has an answer here : Convergent series and divergent series $\sum\limits_{n=1}^\infty \frac{n}{n^3+1}$ and $\sum\limits_{n=1}^\infty \frac{n^2+1}{n^3+1}$ (1 answer) Closed 3 years ago . Is the series convergent or divergent? My answer is the following. Could anyone tell me if it is correct? Since and the series is convergent, by applying comparison test, we get that the series is convergent too.","\sum_\limits{n=1}^\infty\frac{n}{n^3+1} 0<\frac{n}{n^3+1}<\frac{1}{n^2}\;\;,\;\;\;\;\forall n\in\mathbb{N} \sum_\limits{n=1}^\infty\frac{1}{n^2} \sum_\limits{n=1}^\infty\frac{n}{n^3+1}","['real-analysis', 'calculus', 'sequences-and-series']"
54,General coefficient of a Taylor series of $\frac{1}{e^{(e^{1-x})+\frac{x}{2}}+e^{\frac{x}{2}}}$,General coefficient of a Taylor series of,\frac{1}{e^{(e^{1-x})+\frac{x}{2}}+e^{\frac{x}{2}}},Is it possible to write a general Taylor coefficient of the function $$\frac{1}{e^{(e^{1-x})+\frac{x}{2}}+e^{\frac{x}{2}}}?$$ Wolfram alpha produces this nasty thing.,Is it possible to write a general Taylor coefficient of the function Wolfram alpha produces this nasty thing.,\frac{1}{e^{(e^{1-x})+\frac{x}{2}}+e^{\frac{x}{2}}}?,"['real-analysis', 'calculus', 'power-series', 'taylor-expansion']"
55,Is the product of two Cesaro convergent series Cesaro convergent?,Is the product of two Cesaro convergent series Cesaro convergent?,,"Let $\{a_n \}_{n \geq 1}$ and $\{b_n \}_{n \geq 1}$ be two sequences of real numbers such that the infinite series $\sum\limits_{n=1}^{\infty} a_n$ and $\sum\limits_{n=1}^{\infty} b_n$ are both convergent in the Cesaro sense i.e. \begin{align*} \lim\limits_{n \to \infty} \frac 1 n \sum\limits_{k=1}^{n} s_k & < + \infty \\  \lim\limits_{n \to \infty} \frac 1 n \sum\limits_{k=1}^{n} t_k & < + \infty \end{align*} where $\{s_k \}_{k \geq 1}$ and $\{t_k\}_{k \geq 1}$ are sequences of partial sums of the series $\sum\limits_{n=1}^{\infty} a_n$ and $\sum\limits_{n=1}^{\infty} b_n$ respectively. Can I say that $\sum\limits_{n=1}^{\infty} a_n b_n$ is convergent in the Cesaro sense? If ""yes"" then what can I say about it's limit in terms of the limits of the given two series?","Let and be two sequences of real numbers such that the infinite series and are both convergent in the Cesaro sense i.e. where and are sequences of partial sums of the series and respectively. Can I say that is convergent in the Cesaro sense? If ""yes"" then what can I say about it's limit in terms of the limits of the given two series?",\{a_n \}_{n \geq 1} \{b_n \}_{n \geq 1} \sum\limits_{n=1}^{\infty} a_n \sum\limits_{n=1}^{\infty} b_n \begin{align*} \lim\limits_{n \to \infty} \frac 1 n \sum\limits_{k=1}^{n} s_k & < + \infty \\  \lim\limits_{n \to \infty} \frac 1 n \sum\limits_{k=1}^{n} t_k & < + \infty \end{align*} \{s_k \}_{k \geq 1} \{t_k\}_{k \geq 1} \sum\limits_{n=1}^{\infty} a_n \sum\limits_{n=1}^{\infty} b_n \sum\limits_{n=1}^{\infty} a_n b_n,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'proof-writing', 'cesaro-summable']"
56,"If sets $A, B$ in Euclidean space are closed sets, they have the same boundary and their interior's intersection is non-empty, can we say $A=B$?","If sets  in Euclidean space are closed sets, they have the same boundary and their interior's intersection is non-empty, can we say ?","A, B A=B","If sets $A, B$ in Euclidean space are closed sets, they have the same boundary and their interiors intersection is non-empty, can we say $A=B$ ? Any suggestions and comments are welcome!","If sets in Euclidean space are closed sets, they have the same boundary and their interiors intersection is non-empty, can we say ? Any suggestions and comments are welcome!","A, B A=B","['real-analysis', 'general-topology']"
57,Evaluate $\lim\limits_{n \to \infty}\sum_{k=n+1}^{2n}\frac{|\sin 1|+2|\sin 2|+\cdots+k|\sin k|}{k^3}$.,Evaluate .,\lim\limits_{n \to \infty}\sum_{k=n+1}^{2n}\frac{|\sin 1|+2|\sin 2|+\cdots+k|\sin k|}{k^3},"Evaluate $$\lim\limits_{n \to \infty}\sum_{k=n+1}^{2n}\frac{|\sin  1|+2|\sin  2|+\cdots+k|\sin k|}{k^3}.$$ At least, we can estimate an upper bound as follows: \begin{align*} \sum_{k=n+1}^{2n}\frac{|\sin 1|+2|\sin 2|+\cdots+k|\sin k|}{k^3}&\le\sum_{k=n+1}^{2n}\frac{1+2+\cdots+k}{k^3}\\&=\sum_{k=n+1}^{2n}\frac{k+1}{2k^2}\\&=\frac{1}{2}\sum_{k=n+1}^{2n}\frac{1}{k}+\frac{1}{2}\sum_{k=n+1}^{2n}\frac{1}{k^2}\\&\to \frac{1}{2}\ln 2(n \to \infty). \end{align*} But how to obtain the lower bound?","Evaluate At least, we can estimate an upper bound as follows: But how to obtain the lower bound?","\lim\limits_{n \to \infty}\sum_{k=n+1}^{2n}\frac{|\sin  1|+2|\sin
 2|+\cdots+k|\sin k|}{k^3}. \begin{align*}
\sum_{k=n+1}^{2n}\frac{|\sin
1|+2|\sin 2|+\cdots+k|\sin k|}{k^3}&\le\sum_{k=n+1}^{2n}\frac{1+2+\cdots+k}{k^3}\\&=\sum_{k=n+1}^{2n}\frac{k+1}{2k^2}\\&=\frac{1}{2}\sum_{k=n+1}^{2n}\frac{1}{k}+\frac{1}{2}\sum_{k=n+1}^{2n}\frac{1}{k^2}\\&\to \frac{1}{2}\ln 2(n \to \infty).
\end{align*}","['real-analysis', 'calculus', 'sequences-and-series', 'limits']"
58,Is it true that a multivariate function is differentiable if it's components are?,Is it true that a multivariate function is differentiable if it's components are?,,"For example, if I had $f : \mathbb{R}^n \to \mathbb{R} = x_1^2 + x_2^2 + ... + x_n^2$ , would it follow that $f$ is differentiable from the fact that $g: \mathbb{R} \to \mathbb{R} = x^2$ is differentiable? Why?","For example, if I had , would it follow that is differentiable from the fact that is differentiable? Why?",f : \mathbb{R}^n \to \mathbb{R} = x_1^2 + x_2^2 + ... + x_n^2 f g: \mathbb{R} \to \mathbb{R} = x^2,"['real-analysis', 'multivariable-calculus', 'derivatives']"
59,If series is absolutely convergent then $\sum \limits_{n\in I}a_n=\sum \limits_{k=1}^{\infty}\sum \limits_{n\in I_k}a_n.$,If series is absolutely convergent then,\sum \limits_{n\in I}a_n=\sum \limits_{k=1}^{\infty}\sum \limits_{n\in I_k}a_n.,"Suppose that the series $\sum \limits_{n=1}^{\infty}a_n$ is absolutely convergent and let $I\subseteq \mathbb{N}$ such that $I=\bigsqcup\limits_{k=1}^{\infty}I_k$ . Then show that $$\sum \limits_{n\in I}a_n=\sum \limits_{k=1}^{\infty}\sum \limits_{n\in I_k}a_n. \qquad (*)$$ I don't have any idea how to solve it. I do know that in any absolute convergent series permutation of terms does not change the sum and I guess it should be used somehow in order to prove equality $(*)$ . Can anyone show the rigorous proof of equality $(*)$ , please?","Suppose that the series is absolutely convergent and let such that . Then show that I don't have any idea how to solve it. I do know that in any absolute convergent series permutation of terms does not change the sum and I guess it should be used somehow in order to prove equality . Can anyone show the rigorous proof of equality , please?",\sum \limits_{n=1}^{\infty}a_n I\subseteq \mathbb{N} I=\bigsqcup\limits_{k=1}^{\infty}I_k \sum \limits_{n\in I}a_n=\sum \limits_{k=1}^{\infty}\sum \limits_{n\in I_k}a_n. \qquad (*) (*) (*),"['real-analysis', 'sequences-and-series']"
60,Growth of a function with every derivative everywhere increasing,Growth of a function with every derivative everywhere increasing,,"If a function $f: \mathbb{R} \to \mathbb{R}^{+}$ , is infinitely differentiable everywhere, and is such that every derivative is monotonically strictly increasing everywhere, then what is the slowest-growing function that $f$ could be? What if the derivatives are just monotonically nondecreasing everywhere? Feel free to add in additional properties (which you would specify) if this question is not well-formulated enough. This question just popped into my head. I have suspicions and will work on it too.","If a function , is infinitely differentiable everywhere, and is such that every derivative is monotonically strictly increasing everywhere, then what is the slowest-growing function that could be? What if the derivatives are just monotonically nondecreasing everywhere? Feel free to add in additional properties (which you would specify) if this question is not well-formulated enough. This question just popped into my head. I have suspicions and will work on it too.",f: \mathbb{R} \to \mathbb{R}^{+} f,"['real-analysis', 'calculus']"
61,log|x| is a tempered distribution,log|x| is a tempered distribution,,"How do I prove that $\log|x|$ is a tempered distribution on $\mathcal{S}(\mathbb{R}^n)$ , ie., I need to prove that the linear functional $$\phi \in \mathcal{S}(\mathbb{R}^n) \mapsto \int_{\mathbb{R}^n} \phi(x)\log|x|dx	$$ is continuous. It's suficcient to prove that $\phi_k \rightarrow 0$ in $\mathcal{S}(\mathbb{R}^n)$ implies that $\int_{\mathbb{R}^n} \phi_k(x)\log|x| \rightarrow 0$ as $k \rightarrow \infty$ , but I don't know estimate $\phi_k(x)\log|x|$ by an integrable function, so I could use dominated convergence theorem, or estimate it by a sum seminorms $\|\phi_k\|_{\alpha,\beta}$ . (this is the Example 2.3.5 (7) from Grafakos's Book - Classical Fourier Analysis - third edition)","How do I prove that is a tempered distribution on , ie., I need to prove that the linear functional is continuous. It's suficcient to prove that in implies that as , but I don't know estimate by an integrable function, so I could use dominated convergence theorem, or estimate it by a sum seminorms . (this is the Example 2.3.5 (7) from Grafakos's Book - Classical Fourier Analysis - third edition)","\log|x| \mathcal{S}(\mathbb{R}^n) \phi \in \mathcal{S}(\mathbb{R}^n) \mapsto \int_{\mathbb{R}^n} \phi(x)\log|x|dx	 \phi_k \rightarrow 0 \mathcal{S}(\mathbb{R}^n) \int_{\mathbb{R}^n} \phi_k(x)\log|x| \rightarrow 0 k \rightarrow \infty \phi_k(x)\log|x| \|\phi_k\|_{\alpha,\beta}","['real-analysis', 'functional-analysis', 'fourier-analysis']"
62,"$h:[0,1] \to\mathbb{R}$ continuous, and ivt","continuous, and ivt","h:[0,1] \to\mathbb{R}","The question is as follows: $$ \text{Supposd } h:[0,1] \rightarrow \mathbb{R} \text{ is continuous. Show that there exist } w \in [0,1] \text{ such that} \\h(w)= \frac{w+1}{2}h(0)+\frac{2w+2}{9}h(\frac{1}{2})+\frac{w+1}{12}h(1)$$ I know that I have to use Intermediate Value Theorem and I have to show $h(w)$ lies between $h(0)$ and $h(1)$ , but I have no idea how to prove it. I have tried to separate $h(w)$ into $\frac{w+1}{2}h(0)+\frac{w+1}{9}h(\frac12)+\frac{w+1}{9}h(\frac12)+\frac{w+1}{12}h(1)$ and then use IVT twice on the interval $[0,\frac12]$ and $[\frac12,1]$ but it seems not working. I have also tried to see if $h$ is an interpolation of 3 points but it also fails.","The question is as follows: I know that I have to use Intermediate Value Theorem and I have to show lies between and , but I have no idea how to prove it. I have tried to separate into and then use IVT twice on the interval and but it seems not working. I have also tried to see if is an interpolation of 3 points but it also fails."," \text{Supposd } h:[0,1] \rightarrow \mathbb{R} \text{ is continuous. Show that there exist } w \in [0,1] \text{ such that}
\\h(w)= \frac{w+1}{2}h(0)+\frac{2w+2}{9}h(\frac{1}{2})+\frac{w+1}{12}h(1) h(w) h(0) h(1) h(w) \frac{w+1}{2}h(0)+\frac{w+1}{9}h(\frac12)+\frac{w+1}{9}h(\frac12)+\frac{w+1}{12}h(1) [0,\frac12] [\frac12,1] h","['real-analysis', 'calculus', 'analysis', 'continuity']"
63,Fourier transform of measure on $\mathbb{T}$,Fourier transform of measure on,\mathbb{T},"Following is problem 8.39 from Folland. $\mu$ is a positive Borel measure on $\mathbb{T}=[0,1)$ with $\mu(\mathbb{T})=1$ , then for its Fourier transform $\hat{\mu}(k)=\int_{\mathbb{T}}e^{-2\pi ikx}d\mu(x)$ , prove that $|\hat{\mu}(k)|<1$ for any $k \neq 0$ unless $\mu$ is a linear combination of point mass. I don't even know how to start with. I was trying to use the Radon-Nikodym theorem and thus decompose the measure w.r.t Lebesgue measure but fail to push it further. Also, I can't see there's any general way to conclude to the point mass case... Any comment and help is appreciated.","Following is problem 8.39 from Folland. is a positive Borel measure on with , then for its Fourier transform , prove that for any unless is a linear combination of point mass. I don't even know how to start with. I was trying to use the Radon-Nikodym theorem and thus decompose the measure w.r.t Lebesgue measure but fail to push it further. Also, I can't see there's any general way to conclude to the point mass case... Any comment and help is appreciated.","\mu \mathbb{T}=[0,1) \mu(\mathbb{T})=1 \hat{\mu}(k)=\int_{\mathbb{T}}e^{-2\pi ikx}d\mu(x) |\hat{\mu}(k)|<1 k \neq 0 \mu",['real-analysis']
64,Strictly increasing function from $\mathbb{R}$ into $\mathbb{R}\backslash\mathbb{Q}$,Strictly increasing function from  into,\mathbb{R} \mathbb{R}\backslash\mathbb{Q},"It seems this question is duplicated...Still I would be grateful for any hint to my second question. Does there exist a strictly increasing function $f:\mathbb{R}\rightarrow\mathbb{R}\backslash\mathbb{Q}$ , i.e., from real numbers to irrational numbers? It cannot be surjective, since otherwise one can show that it is continuous and obtain a contradiction, but without assuming surjectivity I have not found a contradiction. I do have a candidate function with that property by translating decimal expansion to continued fraction. More precisely, define a function $f:(0,1)\rightarrow\mathbb{R}\backslash\mathbb{Q}$ as follows. Let $x\in(0,1)$ with decimal expansion $0.a_1a_2a_3...$ , define $f(x)$ to be $[0;1,1+a_1,1,1+a_2,...]$ . As far as I understand infinite continued fraction must represent irrational number, and it seems $f$ preserves order. Is this a correct example? More generally, what subset of reals can be the image of strictly increasing function?","It seems this question is duplicated...Still I would be grateful for any hint to my second question. Does there exist a strictly increasing function , i.e., from real numbers to irrational numbers? It cannot be surjective, since otherwise one can show that it is continuous and obtain a contradiction, but without assuming surjectivity I have not found a contradiction. I do have a candidate function with that property by translating decimal expansion to continued fraction. More precisely, define a function as follows. Let with decimal expansion , define to be . As far as I understand infinite continued fraction must represent irrational number, and it seems preserves order. Is this a correct example? More generally, what subset of reals can be the image of strictly increasing function?","f:\mathbb{R}\rightarrow\mathbb{R}\backslash\mathbb{Q} f:(0,1)\rightarrow\mathbb{R}\backslash\mathbb{Q} x\in(0,1) 0.a_1a_2a_3... f(x) [0;1,1+a_1,1,1+a_2,...] f","['real-analysis', 'functions', 'continued-fractions', 'monotone-functions']"
65,What axiom system for the complex numbers is categorical?,What axiom system for the complex numbers is categorical?,,"A theory is categorical if it has a unique model up to isomorphism.  First-order Peano arithmetic is not categorical, but second-order Peano arithmetic is categorical, with the natural numbers as its unique model.  The first-order theory of real closed fields is not categorical, but the second-order theory of Dedekind-complete ordered fields is categorical, with the real numbers as its unique model.  ZFC is not categorical, but Morse-Kelley Set Theory with an appropriate axiom about inaccessible cardinals is categorical. My question is, what theory of the complex numbers is categorical?  The first order theory of algebraically closed fields of characteristic zero is not categorical, because both the field of algebraic complex numbers and the field of complex numbers satisfy it.  So is there some second-order axiom we can add to this theory to make it categorical?","A theory is categorical if it has a unique model up to isomorphism.  First-order Peano arithmetic is not categorical, but second-order Peano arithmetic is categorical, with the natural numbers as its unique model.  The first-order theory of real closed fields is not categorical, but the second-order theory of Dedekind-complete ordered fields is categorical, with the real numbers as its unique model.  ZFC is not categorical, but Morse-Kelley Set Theory with an appropriate axiom about inaccessible cardinals is categorical. My question is, what theory of the complex numbers is categorical?  The first order theory of algebraically closed fields of characteristic zero is not categorical, because both the field of algebraic complex numbers and the field of complex numbers satisfy it.  So is there some second-order axiom we can add to this theory to make it categorical?",,"['real-analysis', 'logic', 'complex-numbers', 'model-theory', 'second-order-logic']"
66,Question about the relation between integration and differentiation (From Calculus by Apostol),Question about the relation between integration and differentiation (From Calculus by Apostol),,"I got stuck while doing exercise of the Apostol's Calculus, the exercise 28 of Section 5.5. Here's the question Given a function $f$ such that the integral $A(x) = \int_a^xf(t)dt$ exists for each $x$ in an interval $[a, b]$ . Let $c$ be a point in the open interval $(a, b)$ . Consider the following ten statements about this $f$ and this A: And there are five (a) ~ (e) statements on the left, and five ( $\alpha$ ) ~ ( $\epsilon$ ) statements on the right. The author asks the reader to decide the implicative relation from statements on the left to statements on the right. I thought I answered correctly but the solution at the end tells different. I don't know why this is wrong. (d) $f'(c)$ exists. $\implies$ ( $\epsilon$ ) $A'$ is continuous at c. This is my argument: By the Example 7 of Section 4.4, the differentiability of $f$ at c implies the continuity of $f$ at c. Since $f$ is differentiable at c, $f$ is continuous at c, so that $A'$ , which equals to $f$ , should continuous at c. But the solution at the end says (d) does not implies ( $\epsilon$ ). Sorry for the partializing the problem, it maybe tough to point out what is wrong.","I got stuck while doing exercise of the Apostol's Calculus, the exercise 28 of Section 5.5. Here's the question Given a function such that the integral exists for each in an interval . Let be a point in the open interval . Consider the following ten statements about this and this A: And there are five (a) ~ (e) statements on the left, and five ( ) ~ ( ) statements on the right. The author asks the reader to decide the implicative relation from statements on the left to statements on the right. I thought I answered correctly but the solution at the end tells different. I don't know why this is wrong. (d) exists. ( ) is continuous at c. This is my argument: By the Example 7 of Section 4.4, the differentiability of at c implies the continuity of at c. Since is differentiable at c, is continuous at c, so that , which equals to , should continuous at c. But the solution at the end says (d) does not implies ( ). Sorry for the partializing the problem, it maybe tough to point out what is wrong.","f A(x) = \int_a^xf(t)dt x [a, b] c (a, b) f \alpha \epsilon f'(c) \implies \epsilon A' f f f f A' f \epsilon","['real-analysis', 'calculus', 'integration', 'continuity']"
67,power of integral,power of integral,,I don't know how to put this properly but Is there any some kind of equality or inequality like in case we know that $|\int_a^b f| \leq \int_a^b |f|$ $\Big[\int_a^b f(x) \Big]^{T}$ where $T \in \mathbb{R}^{+}$ ( positive real number) and $f(x)$ is positive for all $x$,I don't know how to put this properly but Is there any some kind of equality or inequality like in case we know that where ( positive real number) and is positive for all,|\int_a^b f| \leq \int_a^b |f| \Big[\int_a^b f(x) \Big]^{T} T \in \mathbb{R}^{+} f(x) x,"['real-analysis', 'integration']"
68,"Numbers with ""known"" continued fractions","Numbers with ""known"" continued fractions",,"We know $$\frac{1+\sqrt{5}}{2} = 1+\cfrac{1}{1+\cfrac{1}{1+\cfrac{1}{1+\cdots}}}$$ and $$e = 2+\cfrac{1}{1+\cfrac{1}{2+\cfrac{1}{1+\frac{1}{4+\cdots}}}}.$$ There are similar formulas for any quadratic irrationals and some other numbers related to $e$ (such as some rational powers and $\frac{2}{e-1}$ , though those can be quite rich, and I would be interested in any remarkable examples). A bit less known is $$\frac{I_1(2)}{I_0(2)} = \cfrac{1}{1+\cfrac{1}{2+\cfrac{1}{3+\cdots}}},$$ where $I_{\alpha}$ is the modified Bessel function. In contrast, many numbers such as $\pi$ and algebraic irrationals of degree $>2$ are expected to have random-looking continued fractions. Are there any other examples of numbers whose representation as standard continued fraction is ""known"" (or depending on how you look at it, any other continued fractions we can compute)?","We know and There are similar formulas for any quadratic irrationals and some other numbers related to (such as some rational powers and , though those can be quite rich, and I would be interested in any remarkable examples). A bit less known is where is the modified Bessel function. In contrast, many numbers such as and algebraic irrationals of degree are expected to have random-looking continued fractions. Are there any other examples of numbers whose representation as standard continued fraction is ""known"" (or depending on how you look at it, any other continued fractions we can compute)?","\frac{1+\sqrt{5}}{2} = 1+\cfrac{1}{1+\cfrac{1}{1+\cfrac{1}{1+\cdots}}} e = 2+\cfrac{1}{1+\cfrac{1}{2+\cfrac{1}{1+\frac{1}{4+\cdots}}}}. e \frac{2}{e-1} \frac{I_1(2)}{I_0(2)} = \cfrac{1}{1+\cfrac{1}{2+\cfrac{1}{3+\cdots}}}, I_{\alpha} \pi >2","['real-analysis', 'number-theory', 'irrational-numbers', 'continued-fractions', 'diophantine-approximation']"
69,Limit $\lim\limits_{x\to\infty}\int\limits_0^\pi\cos\left(x\sin\theta \right)d\theta=0$,Limit,\lim\limits_{x\to\infty}\int\limits_0^\pi\cos\left(x\sin\theta \right)d\theta=0,Consider the function $f$ defined on $\mathbb{R}$ by $$f(x)=\int_0^\pi\cos\left(x\sin\theta  \right)d\theta.$$ I showed that this function satisfies the following differential equation: $$xf''(x)+f'(x)+xf(x)=0$$ this implies that $$f''(x)+\frac{f'(x)}{x} +f(x)=0$$ since $f'$ is bounded then $$\lim_{x\to\infty}f''(x)+f(x)=0$$ how to continue to prove that $$\lim_{x\to\infty}f(x)=0.$$ I am also interested if there's another method to prove it without the differential equation.,Consider the function defined on by I showed that this function satisfies the following differential equation: this implies that since is bounded then how to continue to prove that I am also interested if there's another method to prove it without the differential equation.,f \mathbb{R} f(x)=\int_0^\pi\cos\left(x\sin\theta  \right)d\theta. xf''(x)+f'(x)+xf(x)=0 f''(x)+\frac{f'(x)}{x} +f(x)=0 f' \lim_{x\to\infty}f''(x)+f(x)=0 \lim_{x\to\infty}f(x)=0.,"['real-analysis', 'integration', 'special-functions', 'bessel-functions']"
70,The convergence of $\sum_{n=1}^\infty (-1)^n\left(\frac{n}{e}\right)^n\frac{1}{n!}$,The convergence of,\sum_{n=1}^\infty (-1)^n\left(\frac{n}{e}\right)^n\frac{1}{n!},"I'm trying to figure out if the $\sum_{n=1}^\infty (-1)^n\left(\frac{n}{e}\right)^n\frac{1}{n!}$ converges or not. I've tried the Leibnitz test for alternating series, but it leads to Stirling's formula and I was wondering if there's any other way so I could avoid using it. I'll be grateful for any idea.","I'm trying to figure out if the converges or not. I've tried the Leibnitz test for alternating series, but it leads to Stirling's formula and I was wondering if there's any other way so I could avoid using it. I'll be grateful for any idea.",\sum_{n=1}^\infty (-1)^n\left(\frac{n}{e}\right)^n\frac{1}{n!},"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'power-series']"
71,Points of continuity is a Borel set?,Points of continuity is a Borel set?,,Let $f:X \to \mathbb R$ be a function where $X$ is a metric space. Is the set of points at which $f$ is continuous a Borel set? i.e. Is the set $\{ x \in X : f$ is continuous at $x \in X$ $\}$ a Borel set in $X$ ? (Maybe separability of $X$ is needed?),Let be a function where is a metric space. Is the set of points at which is continuous a Borel set? i.e. Is the set is continuous at a Borel set in ? (Maybe separability of is needed?),f:X \to \mathbb R X f \{ x \in X : f x \in X \} X X,"['real-analysis', 'measure-theory']"
72,"Is $\{n!\alpha\},n \in\mathbb{N}$ dense in $R$ ? where $\alpha$ is irrational",Is  dense in  ? where  is irrational,"\{n!\alpha\},n \in\mathbb{N} R \alpha","Is $\{n!\alpha\},n \in\mathbb{N}$ dense in $[0,1]$ ? where $\alpha$ is irrational. I know that $\{n\alpha\}$ is dense in $[0,1]$ ? I wanted to generalise it.So, first I thought about $\{n!\alpha\},n \in\mathbb{N}$ , but couldn't make any reasonable progress, although my intuition says answer will be no.","Is dense in ? where is irrational. I know that is dense in ? I wanted to generalise it.So, first I thought about , but couldn't make any reasonable progress, although my intuition says answer will be no.","\{n!\alpha\},n \in\mathbb{N} [0,1] \alpha \{n\alpha\} [0,1] \{n!\alpha\},n \in\mathbb{N}",[]
73,Find the limit of sequence $a_{n+1} -a_{n}$,Find the limit of sequence,a_{n+1} -a_{n},"Consider the sequence $a_{n}=\sqrt{n}+\sqrt[3]{n}+\cdots+ \sqrt[n]{n}.$ Find the limit of sequence $a_{n+1} -a_{n}.$ One of my attempts is to use the reciprocal of the Stolz-Cesaro lemma, which is true by imposing additional conditions. What is the valid solution?","Consider the sequence Find the limit of sequence One of my attempts is to use the reciprocal of the Stolz-Cesaro lemma, which is true by imposing additional conditions. What is the valid solution?",a_{n}=\sqrt{n}+\sqrt[3]{n}+\cdots+ \sqrt[n]{n}. a_{n+1} -a_{n}.,['real-analysis']
74,Show that $\{x\in\mathbb Q\mid x^2\leq 2\}$ has no supremum in $\mathbb Q$. Why my argument is not correct?,Show that  has no supremum in . Why my argument is not correct?,\{x\in\mathbb Q\mid x^2\leq 2\} \mathbb Q,"Let $A=\{x\in\mathbb Q\mid x^2\leq 2\}$ . Prove that $A$ has no supremum in $\mathbb Q$ . I had that to an exam, and I had a grade of $0/15$ . Could someone explain me why ? Proof : Let $M\in\mathbb Q$ s.t. $M=\sup(A)$ . Suppose $M<\sqrt 2$ . Then, by density of $\mathbb Q$ in $\mathbb R$ there is $x\in \mathbb Q$ s.t. $M<x<\sqrt 2$ . Since $x^2\leq 2$ , we have that $x\in A$ which is a contradiction. If $\sqrt 2<M$ . Still by density of $\mathbb Q$ in $\mathbb R$ , there is $y\in \mathbb Q$ s.t. $\sqrt 2<y<M$ . They $y$ is an upper bound of $A$ which contradict that $M$ is the smallest upper bound. Question : What's wrong in my argument ? (I'll normally see the corrector next week, but I would like to know why this is wrong because to me it looks completely correct).","Let . Prove that has no supremum in . I had that to an exam, and I had a grade of . Could someone explain me why ? Proof : Let s.t. . Suppose . Then, by density of in there is s.t. . Since , we have that which is a contradiction. If . Still by density of in , there is s.t. . They is an upper bound of which contradict that is the smallest upper bound. Question : What's wrong in my argument ? (I'll normally see the corrector next week, but I would like to know why this is wrong because to me it looks completely correct).",A=\{x\in\mathbb Q\mid x^2\leq 2\} A \mathbb Q 0/15 M\in\mathbb Q M=\sup(A) M<\sqrt 2 \mathbb Q \mathbb R x\in \mathbb Q M<x<\sqrt 2 x^2\leq 2 x\in A \sqrt 2<M \mathbb Q \mathbb R y\in \mathbb Q \sqrt 2<y<M y A M,"['real-analysis', 'proof-verification', 'supremum-and-infimum']"
75,"Find the increasing function $f:\mathbb{R} \to \mathbb{R}$ such that $\int_{0}^{x}f(t)\,dt=x^2$ for all $x\in \mathbb{R}$",Find the increasing function  such that  for all,"f:\mathbb{R} \to \mathbb{R} \int_{0}^{x}f(t)\,dt=x^2 x\in \mathbb{R}","Find the increasing function $f:\mathbb{R} \to \mathbb{R}$ such that $$\int_{0}^{x}f(t)\,dt=x^2\text{ for all }x\in \mathbb{R}$$ My solution: Let $F:\mathbb{R} \to \mathbb{R}$ , $F(x)=\int_{0}^{x}f(t)\,dt$ . Then it follows that $F$ is differentiable and that $F'(x)=2x$ for all $x\in \mathbb{R}$ . For $a\in \mathbb{R}$ we have that $$2a=F'(a)=\lim_{x\searrow a}\frac{F(x)-F(a)}{x-a}=\lim_{x\searrow a}\frac{\int_{a}^{x}f(t)\,dt}{x-a}\ge \lim_{x\searrow a} \frac{\int_{a}^{x}f(a)\,dt}{x-a}=\lim_{x\searrow a}\frac{(x-a)f(a)}{x-a}=f(a)$$ and $$2a=F'(a)=\lim_{x\nearrow a}\frac{F(x)-F(a)}{x-a}=\lim_{x\nearrow a}\frac{\int_{a}^{x}f(t)\,dt}{x-a}\le \lim_{x\nearrow a} \frac{\int_{a}^{x}f(a)\,dt}{x-a}=\lim_{x\nearrow a}\frac{(x-a)f(a)}{x-a}=f(a).$$ From these two relations it follows that $f(x)=2x$ for all $x\in \mathbb{R}$ . I think that my solution works, but what I would like to know is why we can't take $f$ to be decreasing. Assuming that it were decreasing, by reversing the inequalities from above we get that $f(x)=2x,\forall x \in \mathbb{R}$ , which is a contradiction. What I want to find out is if there is any other way to reach a contradiction in this case. More precisely, I would like to know what may have motivated the authors of this problem to consider $f$ to be increasing instead of decreasing(I doubt that they just tried both cases and chose the one that actually worked).","Find the increasing function such that My solution: Let , . Then it follows that is differentiable and that for all . For we have that and From these two relations it follows that for all . I think that my solution works, but what I would like to know is why we can't take to be decreasing. Assuming that it were decreasing, by reversing the inequalities from above we get that , which is a contradiction. What I want to find out is if there is any other way to reach a contradiction in this case. More precisely, I would like to know what may have motivated the authors of this problem to consider to be increasing instead of decreasing(I doubt that they just tried both cases and chose the one that actually worked).","f:\mathbb{R} \to \mathbb{R} \int_{0}^{x}f(t)\,dt=x^2\text{ for all }x\in \mathbb{R} F:\mathbb{R} \to \mathbb{R} F(x)=\int_{0}^{x}f(t)\,dt F F'(x)=2x x\in \mathbb{R} a\in \mathbb{R} 2a=F'(a)=\lim_{x\searrow a}\frac{F(x)-F(a)}{x-a}=\lim_{x\searrow a}\frac{\int_{a}^{x}f(t)\,dt}{x-a}\ge \lim_{x\searrow a} \frac{\int_{a}^{x}f(a)\,dt}{x-a}=\lim_{x\searrow a}\frac{(x-a)f(a)}{x-a}=f(a) 2a=F'(a)=\lim_{x\nearrow a}\frac{F(x)-F(a)}{x-a}=\lim_{x\nearrow a}\frac{\int_{a}^{x}f(t)\,dt}{x-a}\le \lim_{x\nearrow a} \frac{\int_{a}^{x}f(a)\,dt}{x-a}=\lim_{x\nearrow a}\frac{(x-a)f(a)}{x-a}=f(a). f(x)=2x x\in \mathbb{R} f f(x)=2x,\forall x \in \mathbb{R} f","['real-analysis', 'integration', 'functions']"
76,About Rudin's outline to proving that Lipschitz functions have converging Fourier Series,About Rudin's outline to proving that Lipschitz functions have converging Fourier Series,,"I'm trying to do the following exercise from Rudin's Real and Complex Analysis: Suppose $f\in C(T)$ and $f\in \text{Lip }\alpha$ for some $\alpha>0$ . Prove that the Fourier series of $f$ converges to $f(x)$ , by completing the following outline: It is enough to consider the case $x=0$ , $f(0)=0$ . The difference between the partial sums $s_n(f;0)$ and the integrals $$\frac{1}{\pi}\int_{-\pi}^{\pi}f(t)\frac{\sin nt}{t}\:dt$$ tends to $0$ as $n\to\infty$ . The function $f(t)/t$ is in $L^1(T)$ . Apply the Riemann-Lebesgue lemma. More careful reasoning shows that the convergence is actually uniform on $T$ . I already know how to prove the result as in Lipschitz Continuity and Hölder Continuity helps Fourier series to converge . However, I don't understand Rudin's outline. When he says that it is enough to consider the case $x=0$ , $f(0)=0$ , is it simply because it changes almost nothing to the proof? Since we can prove that $s_n(f;0)$ tends to $0$ by proving that $f(t)\cot(t/2)$ is in $L^1$ , it seems to me that Rudin's outline is only better if it is easier to prove that $f(t)/t\in L^1$ and that the difference between $s_n(f;0)$ and the integrals tends to $0$ . But I don't see any easy way to do the latter. Why is the convergence uniform and why does it matter? I would appreciate if anyone could answer those 3 questions.","I'm trying to do the following exercise from Rudin's Real and Complex Analysis: Suppose and for some . Prove that the Fourier series of converges to , by completing the following outline: It is enough to consider the case , . The difference between the partial sums and the integrals tends to as . The function is in . Apply the Riemann-Lebesgue lemma. More careful reasoning shows that the convergence is actually uniform on . I already know how to prove the result as in Lipschitz Continuity and Hölder Continuity helps Fourier series to converge . However, I don't understand Rudin's outline. When he says that it is enough to consider the case , , is it simply because it changes almost nothing to the proof? Since we can prove that tends to by proving that is in , it seems to me that Rudin's outline is only better if it is easier to prove that and that the difference between and the integrals tends to . But I don't see any easy way to do the latter. Why is the convergence uniform and why does it matter? I would appreciate if anyone could answer those 3 questions.",f\in C(T) f\in \text{Lip }\alpha \alpha>0 f f(x) x=0 f(0)=0 s_n(f;0) \frac{1}{\pi}\int_{-\pi}^{\pi}f(t)\frac{\sin nt}{t}\:dt 0 n\to\infty f(t)/t L^1(T) T x=0 f(0)=0 s_n(f;0) 0 f(t)\cot(t/2) L^1 f(t)/t\in L^1 s_n(f;0) 0,"['real-analysis', 'fourier-analysis', 'fourier-series']"
77,Calculate $\lim _{n\rightarrow +\infty} \sum _{k=0}^{n} \frac{\sqrt{2n^2+kn-k^2}}{n^2}$,Calculate,\lim _{n\rightarrow +\infty} \sum _{k=0}^{n} \frac{\sqrt{2n^2+kn-k^2}}{n^2},"Calculate $$\lim _{n\rightarrow +\infty} \sum _{k=0}^{n} \frac{\sqrt{2n^2+kn-k^2}}{n^2}$$ My try: $$\lim _{n\rightarrow +\infty} \sum _{k=0}^{n} \frac{\sqrt{2n^2+kn-k^2}}{n^2}=\lim _{n\rightarrow +\infty} \frac{1}{n} \sum _{k=0}^{n} \sqrt{2+\frac{k}{n}-(\frac{k}{n})^2}=\lim _{n\rightarrow +\infty} \frac{1}{n} \sum _{k=0}^{n} f(\frac{k}{n})=\int^{1}_{0} \sqrt{2+x-x^2} dx=\int^{1}_{0} \sqrt{-(x-\frac{1}{2})^2+\frac{9}{4}} dx=\int^{\frac{1}{2}}_{-\frac{1}{2}} \sqrt{\frac{9}{4}-u^2} du$$ In this sollution: $$f:[0,1]\rightarrow \mathbb R, f(x)=\sqrt{2+x-x^2}$$ $$u=x-\frac{1}{2}, du=dx$$ Unfortunatelly I don't know what I can do with $\int^{\frac{1}{2}}_{-\frac{1}{2}} \sqrt{\frac{9}{4}-u^2} du$ because my only idea is integration by substitution. But if I use for example $s=u^2$ then I have $ds=2udu$ so $du=\frac{ds}{2u}$ so I did not get rid of $ u $ which is problematic. Can you help me how to bypass this problem?",Calculate My try: In this sollution: Unfortunatelly I don't know what I can do with because my only idea is integration by substitution. But if I use for example then I have so so I did not get rid of which is problematic. Can you help me how to bypass this problem?,"\lim _{n\rightarrow +\infty} \sum _{k=0}^{n} \frac{\sqrt{2n^2+kn-k^2}}{n^2} \lim _{n\rightarrow +\infty} \sum _{k=0}^{n} \frac{\sqrt{2n^2+kn-k^2}}{n^2}=\lim _{n\rightarrow +\infty} \frac{1}{n} \sum _{k=0}^{n} \sqrt{2+\frac{k}{n}-(\frac{k}{n})^2}=\lim _{n\rightarrow +\infty} \frac{1}{n} \sum _{k=0}^{n} f(\frac{k}{n})=\int^{1}_{0} \sqrt{2+x-x^2} dx=\int^{1}_{0} \sqrt{-(x-\frac{1}{2})^2+\frac{9}{4}} dx=\int^{\frac{1}{2}}_{-\frac{1}{2}} \sqrt{\frac{9}{4}-u^2} du f:[0,1]\rightarrow \mathbb R, f(x)=\sqrt{2+x-x^2} u=x-\frac{1}{2}, du=dx \int^{\frac{1}{2}}_{-\frac{1}{2}} \sqrt{\frac{9}{4}-u^2} du s=u^2 ds=2udu du=\frac{ds}{2u}  u ",['real-analysis']
78,A measurable function is an infinite sum of simple measurable functions. Why measurable?,A measurable function is an infinite sum of simple measurable functions. Why measurable?,,"Let $\left(E,\mathcal{E}\right)$ a measurable space and let $f:E\to[0,+\infty]$ a measurable function. Then $$ f=\sum_{i=1}^{+\infty}a_{i}\chi_{A_{i}} $$ with $a_{i}\geq0$ and $A_{i}$ measurable. In an attempt to solve this problem, I stumbled upon the suggestion of Adam Hughes proposed here: A measurable function equal to a countable sum of characteristic functions? The solution is convincing, but I don't understand how to prove that $A_{i}$ are measurable. Briefly recapitulate for those who have not read the other post. For all $y\in\mathbb{R_+}$ we can find a set $S_{y}\subset\mathbb{N}^{*}$ such that $$ y=\sum_{k\in S_{y}}\frac{1}{k} $$ Now we define $$ \forall k\in\mathbb{N}^{*}\quad\quad A_{k}=\left\{ x\in E\mid k\in S_{f\left(x\right)}\right\}  $$ Then $$ \displaystyle f=\sum_{k\in\mathbb N^*} \frac{1}{k} \chi_{A_{k}} $$","Let a measurable space and let a measurable function. Then with and measurable. In an attempt to solve this problem, I stumbled upon the suggestion of Adam Hughes proposed here: A measurable function equal to a countable sum of characteristic functions? The solution is convincing, but I don't understand how to prove that are measurable. Briefly recapitulate for those who have not read the other post. For all we can find a set such that Now we define Then","\left(E,\mathcal{E}\right) f:E\to[0,+\infty] 
f=\sum_{i=1}^{+\infty}a_{i}\chi_{A_{i}}
 a_{i}\geq0 A_{i} A_{i} y\in\mathbb{R_+} S_{y}\subset\mathbb{N}^{*} 
y=\sum_{k\in S_{y}}\frac{1}{k}
 
\forall k\in\mathbb{N}^{*}\quad\quad A_{k}=\left\{ x\in E\mid k\in S_{f\left(x\right)}\right\} 
  \displaystyle
f=\sum_{k\in\mathbb N^*} \frac{1}{k} \chi_{A_{k}}
","['real-analysis', 'probability', 'measure-theory']"
79,Can this expression be simplified? (double integral over a sphere in $\mathbb{R}^3$.,Can this expression be simplified? (double integral over a sphere in .,\mathbb{R}^3,"Suppose $f:\mathbb{R}^3\to\mathbb{R}$ is an integrable function (I'm open to further restricting $f$ , e.g. $f$ can be assumed to be smooth). For some constants $0<r\leq R$ , is there a way to simplify the expression $$ \int_{\lvert y \rvert = R}\int_{\lvert x\rvert =r} f(x+y) \,\mathrm{d}S(x)\mathrm{d}S(y) $$ to an expression not involving surface integrals?","Suppose is an integrable function (I'm open to further restricting , e.g. can be assumed to be smooth). For some constants , is there a way to simplify the expression to an expression not involving surface integrals?","f:\mathbb{R}^3\to\mathbb{R} f f 0<r\leq R 
\int_{\lvert y \rvert = R}\int_{\lvert x\rvert =r} f(x+y) \,\mathrm{d}S(x)\mathrm{d}S(y)
","['real-analysis', 'calculus', 'integration']"
80,Calculate $\sum\limits_{i=0}^{n-1}\frac{1}{(i+1)(i+2)}$ in use of disturbance of sum,Calculate  in use of disturbance of sum,\sum\limits_{i=0}^{n-1}\frac{1}{(i+1)(i+2)},"I need calculate $$\sum_{i=0}^{n-1}\frac{1}{(i+1)(i+2)}$$ in use of disturbance of sum method: $$\boxed{\boxed{\sum_{k=m}^Ma_k+a_{M+1}=a_m+\sum_{k=m+1}^Ma_{k+1}}}  $$ My try I know how to solve this task in ""traditional"" way but I am trying to do it with that method: $$S_n=S_n$$ $$ \sum_{i=0}^{n-1}\frac{1}{(i+1)(i+2)} + \frac{1}{(n+1)(n+2)} = \frac{1}{2} + \sum_{i=1}^{n}\frac{1}{(i+1)(i+2)}  $$ ok - now I want to have the same piece as on the left side of equal. So i decide to use: $$  i' = i+1 $$ $$ \frac{1}{2} + \sum_{i=1}^{n}\frac{1}{(i+1)(i+2)} = \frac{1}{2} + \sum_{i=0}^{n-1}\frac{1}{i(i+1)} $$ but there I divide by 0... How to avoid that problem? Method I don't know if the name ""disturbance of sum"" is correct in english so I am going to give example of another task with this approach: $$ S_n=\sum_{k=0}^{n}q^k = ? $$ $$ S_n+q^{n+1}=1+\sum_{k=1}^{n+1}q^k=1+\sum_{k=0}^{n}q^{k+1}= 1+q\sum_{k=0}^{n}q^{k}=1+qS_n $$ $$(q-1)S_n=q^{n+1}-1 $$ $$ S_n=\frac{q^{n+1}-1}{q-1} $$","I need calculate in use of disturbance of sum method: My try I know how to solve this task in ""traditional"" way but I am trying to do it with that method: ok - now I want to have the same piece as on the left side of equal. So i decide to use: but there I divide by 0... How to avoid that problem? Method I don't know if the name ""disturbance of sum"" is correct in english so I am going to give example of another task with this approach:","\sum_{i=0}^{n-1}\frac{1}{(i+1)(i+2)} \boxed{\boxed{\sum_{k=m}^Ma_k+a_{M+1}=a_m+\sum_{k=m+1}^Ma_{k+1}}}
  S_n=S_n  \sum_{i=0}^{n-1}\frac{1}{(i+1)(i+2)} + \frac{1}{(n+1)(n+2)} = \frac{1}{2} + \sum_{i=1}^{n}\frac{1}{(i+1)(i+2)}     i' = i+1   \frac{1}{2} + \sum_{i=1}^{n}\frac{1}{(i+1)(i+2)} = \frac{1}{2} + \sum_{i=0}^{n-1}\frac{1}{i(i+1)}   S_n=\sum_{k=0}^{n}q^k = ?   S_n+q^{n+1}=1+\sum_{k=1}^{n+1}q^k=1+\sum_{k=0}^{n}q^{k+1}= 1+q\sum_{k=0}^{n}q^{k}=1+qS_n
 (q-1)S_n=q^{n+1}-1
  S_n=\frac{q^{n+1}-1}{q-1}
",['real-analysis']
81,K time differentiable function,K time differentiable function,,Is there any k time differentiable function such that $$f(f'(f''(f'''(......f^{(k)}(x))))=x$$ for all $x$ belongs to $\mathbb R$ ? EDIT:- What will the case be when the order of the functions taken are reversed?,Is there any k time differentiable function such that for all belongs to ? EDIT:- What will the case be when the order of the functions taken are reversed?,f(f'(f''(f'''(......f^{(k)}(x))))=x x \mathbb R,"['real-analysis', 'calculus', 'functional-analysis']"
82,Must compact bijections be continuous?,Must compact bijections be continuous?,,"We say a map $f:X \to Y$ is compact if compact sets are mapped to compact sets. If $f:X \to Y$ is a compact bijection, must it be continuous? The bijection condition is necessary. Otherwise consider an appropriately constructed piecewise constant function $\mathbb{R} \to \mathbb{R}$ as a counterexample. You might notice I never specified what exactly $X,Y$ are. Let us first consider simply the case where $X=\mathbb{R}^n$ , $Y=\mathbb{R}^m$ . If that holds, then work our way up to $X,Y$ being generic metric spaces. If that holds, then work our way up to $X,Y$ being topological spaces [EDIT: it's not true for general topological spaces, see the comments].","We say a map is compact if compact sets are mapped to compact sets. If is a compact bijection, must it be continuous? The bijection condition is necessary. Otherwise consider an appropriately constructed piecewise constant function as a counterexample. You might notice I never specified what exactly are. Let us first consider simply the case where , . If that holds, then work our way up to being generic metric spaces. If that holds, then work our way up to being topological spaces [EDIT: it's not true for general topological spaces, see the comments].","f:X \to Y f:X \to Y \mathbb{R} \to \mathbb{R} X,Y X=\mathbb{R}^n Y=\mathbb{R}^m X,Y X,Y","['real-analysis', 'general-topology', 'continuity', 'metric-spaces', 'compactness']"
83,"In mathematics, which word should I use, ""in"" or ""on"" or ""over""?","In mathematics, which word should I use, ""in"" or ""on"" or ""over""?",,Which option should I choose? $f:\mathbb{R}^3 \to \mathbb{R}$ is a function. Then we say that $f$ is bounded ____ $\mathbb{R}^3$ . a) on; b) in;   c) over,Which option should I choose? is a function. Then we say that is bounded ____ . a) on; b) in;   c) over,f:\mathbb{R}^3 \to \mathbb{R} f \mathbb{R}^3,"['real-analysis', 'terminology']"
84,"Prove that $f(x) = \sum_{n=1}^{\infty} x^n/n^2$ is continuous on $[0,1]$",Prove that  is continuous on,"f(x) = \sum_{n=1}^{\infty} x^n/n^2 [0,1]","I have a general idea of how to prove this but I could use some help with the details. Basically I see that $f(x)$ is the uniform limit of $f_k(x) = \sum_{n=1}^{k} x^n/n^2$ on $[0,1]$ . Each $f_k$ is continuous, so $f$ is as well since uniform convergence preserves continuity. Is this proof correct/does it seem sufficient? Thanks ahead of time.","I have a general idea of how to prove this but I could use some help with the details. Basically I see that is the uniform limit of on . Each is continuous, so is as well since uniform convergence preserves continuity. Is this proof correct/does it seem sufficient? Thanks ahead of time.","f(x) f_k(x) = \sum_{n=1}^{k} x^n/n^2 [0,1] f_k f","['real-analysis', 'proof-verification', 'proof-writing']"
85,Expected value estimate.,Expected value estimate.,,"If we have $P(a< x<b)=1$ , where $P$ denotes probability,and $0<a<b$ then $$E(X)E(\frac{1}{X})\le \frac{(a+b)^2}{4ab}$$ I have tried some trivial cases , uniform distribution and the case $P(x=a)=P(x=b)=\frac{1}{2}$ , I found that in the later case we get the maximum value of right hand side of the inequality we wanted,or it’s the extreme case.（Although the case doesn’t satisfy the condition, I don’t know if it is useful or not.) But I don’t know how to deal the general cases. Even though the special case:the maximum value of the following functional. $$\max_{\int_a^bf(x)dx=1}\int_a^b xf(x)dx\int_a^b\frac{1}{x}f(x)dx$$ Any suggestion will be appreciated.","If we have , where denotes probability,and then I have tried some trivial cases , uniform distribution and the case , I found that in the later case we get the maximum value of right hand side of the inequality we wanted,or it’s the extreme case.（Although the case doesn’t satisfy the condition, I don’t know if it is useful or not.) But I don’t know how to deal the general cases. Even though the special case:the maximum value of the following functional. Any suggestion will be appreciated.",P(a< x<b)=1 P 0<a<b E(X)E(\frac{1}{X})\le \frac{(a+b)^2}{4ab} P(x=a)=P(x=b)=\frac{1}{2} \max_{\int_a^bf(x)dx=1}\int_a^b xf(x)dx\int_a^b\frac{1}{x}f(x)dx,"['calculus', 'real-analysis', 'probability', 'analysis']"
86,Divergence of the solution.,Divergence of the solution.,,"I have two functions $f,g:\mathbb{R}\longrightarrow{\mathbb{R}}$ , both continuous and with $g$ bounded. I also have the following Cauchy problem $$ \begin{cases}x'=f(t)g(x)\\x(t_0)=x_0 \end{cases} $$ If $\phi$ is the solution of the system defined in $(-\infty, b)$ , I have $$ \lim_{t \to{+}b}{\phi'(t)=f(t)g(\phi(t))=f(b)k} $$ with $k \geq g(\phi(t))$ since g is bounded. And now the question: in my notes it claimed that $\phi(t)$ goes to $\infty$ in $b$ since is not defined there. Is it true? I don't see why $\phi \to \infty$ in $b$ . I think it is simply not defined but I don't see why it should go to infinity. Showing this I would get a contradiction since $\phi(t)'$ is bounded, hence proving that $\phi$ is defined in $\mathbb{R}$ Regards. Edit ------- Original problem: Let $f,g:\mathbb{R}\longrightarrow{\mathbb{R}}$ , both continuous and with $g$ bounded. Prove that for all $(t_0,x_0)\in \mathbb{R^2}$ , the maximal solutions of the Cauchy problem $$ \begin{cases}x'=f(t)g(x)\\x(t_0)=x_0 \end{cases} $$ are defined for all $\mathbb{R}$ . The attempt of solution is done by contradiction; supposing $\phi : (-\infty,b) \to \mathbb{R}$ is a solution, we want to show the derivative of $\phi$ is bounded (as I noted at the begining of this post) while $\phi$ goes to $\infty$ in $b$ . This last part is what I don't get. I don't see why is going to $\infty$ .","I have two functions , both continuous and with bounded. I also have the following Cauchy problem If is the solution of the system defined in , I have with since g is bounded. And now the question: in my notes it claimed that goes to in since is not defined there. Is it true? I don't see why in . I think it is simply not defined but I don't see why it should go to infinity. Showing this I would get a contradiction since is bounded, hence proving that is defined in Regards. Edit ------- Original problem: Let , both continuous and with bounded. Prove that for all , the maximal solutions of the Cauchy problem are defined for all . The attempt of solution is done by contradiction; supposing is a solution, we want to show the derivative of is bounded (as I noted at the begining of this post) while goes to in . This last part is what I don't get. I don't see why is going to .","f,g:\mathbb{R}\longrightarrow{\mathbb{R}} g 
\begin{cases}x'=f(t)g(x)\\x(t_0)=x_0
\end{cases}
 \phi (-\infty, b) 
\lim_{t \to{+}b}{\phi'(t)=f(t)g(\phi(t))=f(b)k}
 k \geq g(\phi(t)) \phi(t) \infty b \phi \to \infty b \phi(t)' \phi \mathbb{R} f,g:\mathbb{R}\longrightarrow{\mathbb{R}} g (t_0,x_0)\in \mathbb{R^2} 
\begin{cases}x'=f(t)g(x)\\x(t_0)=x_0
\end{cases}
 \mathbb{R} \phi : (-\infty,b) \to \mathbb{R} \phi \phi \infty b \infty","['real-analysis', 'ordinary-differential-equations']"
87,Compute $\lim_{n\rightarrow\infty}\sqrt[{n+1}]{(n+1)!}-\sqrt[n]{n!}$ [duplicate],Compute  [duplicate],\lim_{n\rightarrow\infty}\sqrt[{n+1}]{(n+1)!}-\sqrt[n]{n!},This question already has answers here : Computation of a limit involving factorial $\lim_{n \to \infty} \sqrt[n+1] {(n+1)!} - \sqrt[n] {(n)!} = \frac{1}{e}$ (4 answers) Closed 5 years ago . Evaluate $L=\lim_{n\rightarrow\infty}\sqrt[{n+1}]{(n+1)!}-\sqrt[n]{n!}$ How I approached it and where I get stuck: $$\lim_{n\rightarrow\infty}\sqrt[{n+1}]{(n+1)!}-\sqrt[n]{n!}=\lim_{n\rightarrow\infty}\frac{\sqrt[n]{n!}}nn(\frac{\sqrt[{n+1}]{(n+1)!}}{\sqrt[n]{n!}}-1)$$ Now: $$\lim_{n\rightarrow\infty}\frac{\sqrt[n]{n!}}n=\lim_{n\rightarrow\infty}\sqrt[n]{\frac{n!}{n^n}}=\lim_{n\rightarrow\infty}e^{\frac1n\sum_{k=1}^n\ln(\frac kn)}=e^{\int_0^1\ln(x)dx}=e^{-1}=\frac1e$$ So $L=\lim_{n\to\infty}\frac 1e\times n(\frac{\sqrt[{n+1}]{(n+1)!}}{\sqrt[n]{n!}}-1)$ . Now this is where I get stuck. What should I do?,This question already has answers here : Computation of a limit involving factorial $\lim_{n \to \infty} \sqrt[n+1] {(n+1)!} - \sqrt[n] {(n)!} = \frac{1}{e}$ (4 answers) Closed 5 years ago . Evaluate How I approached it and where I get stuck: Now: So . Now this is where I get stuck. What should I do?,L=\lim_{n\rightarrow\infty}\sqrt[{n+1}]{(n+1)!}-\sqrt[n]{n!} \lim_{n\rightarrow\infty}\sqrt[{n+1}]{(n+1)!}-\sqrt[n]{n!}=\lim_{n\rightarrow\infty}\frac{\sqrt[n]{n!}}nn(\frac{\sqrt[{n+1}]{(n+1)!}}{\sqrt[n]{n!}}-1) \lim_{n\rightarrow\infty}\frac{\sqrt[n]{n!}}n=\lim_{n\rightarrow\infty}\sqrt[n]{\frac{n!}{n^n}}=\lim_{n\rightarrow\infty}e^{\frac1n\sum_{k=1}^n\ln(\frac kn)}=e^{\int_0^1\ln(x)dx}=e^{-1}=\frac1e L=\lim_{n\to\infty}\frac 1e\times n(\frac{\sqrt[{n+1}]{(n+1)!}}{\sqrt[n]{n!}}-1),"['real-analysis', 'limits', 'limits-without-lhopital']"
88,No. of solutions of $f(x)=f'(x)$?,No. of solutions of ?,f(x)=f'(x),"Let $f:[0,1] \to \Bbb R$ be a fixed continuous function such that $f$ is differentiable on $(0,1)$ and $f(0)=f(1)$. Then the equation $f(x)=f'(x)$ admits No solution $x \in (0,1)$ More than one solution $x \in (0,1)$ Exactly one solution $x \in (0,1)$ At least one solution $x \in (0,1)$ As I have tried taking $f(x)=0$ on $[0,1]$ ruled out options 1 and 2 and by Rolle's Theorem there exists $c\in (0,1)$ such that $f'(c)=0$.  Then I thought to construct function $g(x)=f(x)-f'(x)$ to check zeros but I'm stuck because $f'(x)$ need to be continuous. Can anyone give some hint to proceed further?","Let $f:[0,1] \to \Bbb R$ be a fixed continuous function such that $f$ is differentiable on $(0,1)$ and $f(0)=f(1)$. Then the equation $f(x)=f'(x)$ admits No solution $x \in (0,1)$ More than one solution $x \in (0,1)$ Exactly one solution $x \in (0,1)$ At least one solution $x \in (0,1)$ As I have tried taking $f(x)=0$ on $[0,1]$ ruled out options 1 and 2 and by Rolle's Theorem there exists $c\in (0,1)$ such that $f'(c)=0$.  Then I thought to construct function $g(x)=f(x)-f'(x)$ to check zeros but I'm stuck because $f'(x)$ need to be continuous. Can anyone give some hint to proceed further?",,"['real-analysis', 'continuity']"
89,There are open sets $U$ and $V$ such that $U\cap V = \emptyset $ and $ U \cap \tau(V) =\emptyset $?,There are open sets  and  such that  and ?,U V U\cap V = \emptyset   U \cap \tau(V) =\emptyset ,"I am in a series of self-studies in the book Vector Analysis written by Klaus Janich. By page 15 of the book is made the affirmation (without explicit proof) that we can choose (under the considerations below) open sets $U$ and $V$ such that $U\cap V =\emptyset $ and $ U \cap \tau(V) =\emptyset $. In this page, $\tau:M\to M$ is a fixed-point-free involution ( i.e. a differentiable map with $\tau\circ\tau=\mathrm{id}_M$ and $\tau(x)\neq x$ for all $x\in M$) and $M$ a $m$-dimensional manifold. QUESTION. Under these considerations how can we prove that there are open sets $U\neq \emptyset$ and $V\neq \emptyset$ such that $U\cap V = \emptyset $ and $ U \cap \tau(V) =\emptyset $? In my attempts, the only things I have been able to prove are that $$ U\cap V=\emptyset\Longleftrightarrow \tau(U)\cap \tau(V)=\emptyset \qquad \mathrm{ and } \qquad  U\cap \tau(V)=\emptyset\Longleftrightarrow \tau(U)\cap V=\emptyset $$","I am in a series of self-studies in the book Vector Analysis written by Klaus Janich. By page 15 of the book is made the affirmation (without explicit proof) that we can choose (under the considerations below) open sets $U$ and $V$ such that $U\cap V =\emptyset $ and $ U \cap \tau(V) =\emptyset $. In this page, $\tau:M\to M$ is a fixed-point-free involution ( i.e. a differentiable map with $\tau\circ\tau=\mathrm{id}_M$ and $\tau(x)\neq x$ for all $x\in M$) and $M$ a $m$-dimensional manifold. QUESTION. Under these considerations how can we prove that there are open sets $U\neq \emptyset$ and $V\neq \emptyset$ such that $U\cap V = \emptyset $ and $ U \cap \tau(V) =\emptyset $? In my attempts, the only things I have been able to prove are that $$ U\cap V=\emptyset\Longleftrightarrow \tau(U)\cap \tau(V)=\emptyset \qquad \mathrm{ and } \qquad  U\cap \tau(V)=\emptyset\Longleftrightarrow \tau(U)\cap V=\emptyset $$",,"['real-analysis', 'general-topology', 'manifolds', 'self-learning']"
90,Notion of convergence on the space of compactly supported continuous functions,Notion of convergence on the space of compactly supported continuous functions,,"Let $E = C_c^0(\mathbb{R}^n;\mathbb{R}^m)$ be the space of compactly supported continuous functions on $\mathbb{R}^n$ with values on $\mathbb{R}^m$. There is a natural norm on this space: given $\varphi \in E$, we put  $$ \Vert \varphi \Vert = \sup_{x \in \mathbb{R}^m} \Vert \varphi(x) \Vert.$$ First question: is $E$ equipped with this norm a Banach space? In the book ""Sets of finite perimeter and geometric variational problems"", by Francesco Maggi, the author introduces the following notion of convergence on $E$: a sequence $(\varphi_k)_{k \in \mathbb{N}}$ in $E$ converges to a function $\varphi \in E$ if $\Vert \varphi_k - \varphi \Vert \to 0$ as $k \to \infty$ and if there is a compact set $K \subset \mathbb{R}^n$ such that $$ \text{supp}(\varphi) \cup \bigcup_{k \in \mathbb{N}} \text{supp}(\varphi_k) \subseteq K,$$ that is, if the supports of the functions do not ""escape"" to infinity. Second question: does this notion of convergence somehow generates a topology on $E$? Is it metrizable?","Let $E = C_c^0(\mathbb{R}^n;\mathbb{R}^m)$ be the space of compactly supported continuous functions on $\mathbb{R}^n$ with values on $\mathbb{R}^m$. There is a natural norm on this space: given $\varphi \in E$, we put  $$ \Vert \varphi \Vert = \sup_{x \in \mathbb{R}^m} \Vert \varphi(x) \Vert.$$ First question: is $E$ equipped with this norm a Banach space? In the book ""Sets of finite perimeter and geometric variational problems"", by Francesco Maggi, the author introduces the following notion of convergence on $E$: a sequence $(\varphi_k)_{k \in \mathbb{N}}$ in $E$ converges to a function $\varphi \in E$ if $\Vert \varphi_k - \varphi \Vert \to 0$ as $k \to \infty$ and if there is a compact set $K \subset \mathbb{R}^n$ such that $$ \text{supp}(\varphi) \cup \bigcup_{k \in \mathbb{N}} \text{supp}(\varphi_k) \subseteq K,$$ that is, if the supports of the functions do not ""escape"" to infinity. Second question: does this notion of convergence somehow generates a topology on $E$? Is it metrizable?",,"['real-analysis', 'general-topology', 'functional-analysis', 'banach-spaces']"
91,Strong maximum principle for subharmonic functions?,Strong maximum principle for subharmonic functions?,,"I think I just ""proved"" the Strong MP for sub harmonic functions. But I don't know where things went wrong. Suppose $U$ is a connected bounded region in $\Bbb R^n$, $u\in C^2(\bar U)$ and $\Delta u\ge 0$ in $U$. Then I try to prove in the following that if $u$ isn't constant, then $\max_{\bar U}u$ is only attained on the boundary $\partial U$. Proof: for any $x\in U$ and $r>0$ such that $B_r(x)\subset U$, consider the spherical mean  $$\phi(r):=\frac1{|\partial B_r(x)|}\int_{\partial B_r(x)} u(y)dS(y)=\frac1{|\partial B_1(0)|}\int_{\partial B_1(0)} u(x+rz)dS(z).$$ Then  $$ \begin{align} \phi'(r)&=\frac1{|\partial B_1(0)|}\int_{\partial B_1(0)} Du(x+rz)\cdot zdS(z)\\ &=\frac1{|\partial B_1(0)|}\int_{B_1(0)}\Delta u(x+rz)dz\ge 0 \end{align} $$ Hence $\phi(r)$ is monotonously increasing and in particular $\ge \phi(0^+)=u(x)$ when $r>0$. Hence, if at $x\in U$ is attained the max $M$, then the above mean value inequality forces $u=M$ on a small ball centred at $x$. So $E:=\{x\in U\mid u(x)=M\}$ is open in $U$, but is also closed by continuity. So either  $E$ is empty (max is only attained on the boundary) or $E$ is all of $U$ ($u$ is constant).","I think I just ""proved"" the Strong MP for sub harmonic functions. But I don't know where things went wrong. Suppose $U$ is a connected bounded region in $\Bbb R^n$, $u\in C^2(\bar U)$ and $\Delta u\ge 0$ in $U$. Then I try to prove in the following that if $u$ isn't constant, then $\max_{\bar U}u$ is only attained on the boundary $\partial U$. Proof: for any $x\in U$ and $r>0$ such that $B_r(x)\subset U$, consider the spherical mean  $$\phi(r):=\frac1{|\partial B_r(x)|}\int_{\partial B_r(x)} u(y)dS(y)=\frac1{|\partial B_1(0)|}\int_{\partial B_1(0)} u(x+rz)dS(z).$$ Then  $$ \begin{align} \phi'(r)&=\frac1{|\partial B_1(0)|}\int_{\partial B_1(0)} Du(x+rz)\cdot zdS(z)\\ &=\frac1{|\partial B_1(0)|}\int_{B_1(0)}\Delta u(x+rz)dz\ge 0 \end{align} $$ Hence $\phi(r)$ is monotonously increasing and in particular $\ge \phi(0^+)=u(x)$ when $r>0$. Hence, if at $x\in U$ is attained the max $M$, then the above mean value inequality forces $u=M$ on a small ball centred at $x$. So $E:=\{x\in U\mid u(x)=M\}$ is open in $U$, but is also closed by continuity. So either  $E$ is empty (max is only attained on the boundary) or $E$ is all of $U$ ($u$ is constant).",,"['real-analysis', 'ordinary-differential-equations', 'proof-verification', 'partial-differential-equations', 'harmonic-functions']"
92,Simplify a double integral,Simplify a double integral,,"I'm having trouble with proving that the double integral $$\int_{-1}^1\int_{x^2}^1 f\Big(\frac{y}{x}\Big)\mathrm{d}y \mathrm{d}x $$ can be simplified to:  $$\int_{-1}^1 f(t) \frac{t^2}{2} \mathrm{d}t + \int_{-1}^1 f\Big(\frac{1}{t}\Big) \frac{1}{2} \mathrm{d}t $$ This exercice is part of the section ""double integrals in polar coordinates"", but I can't see the link. Thank you in advance!","I'm having trouble with proving that the double integral $$\int_{-1}^1\int_{x^2}^1 f\Big(\frac{y}{x}\Big)\mathrm{d}y \mathrm{d}x $$ can be simplified to:  $$\int_{-1}^1 f(t) \frac{t^2}{2} \mathrm{d}t + \int_{-1}^1 f\Big(\frac{1}{t}\Big) \frac{1}{2} \mathrm{d}t $$ This exercice is part of the section ""double integrals in polar coordinates"", but I can't see the link. Thank you in advance!",,"['real-analysis', 'integration']"
93,Showing Bounded Derivative $\implies$ Lipschitz Function (Uniformly Continuous),Showing Bounded Derivative  Lipschitz Function (Uniformly Continuous),\implies,"Let $I$ be an interval in $\mathbb{R}$ (bounded or unbounded). Let $f: I \rightarrow \mathbb{R}$ be differentiable, with the property that $|f'(x)| \le M,  \forall x \in I$ . Show that $|f(x) - f(y)| \le M |x - y|$ , for all $x \in I$ . Essentially, I need to show that a function whose derivative is bounded is uniformly continuous. I've seen the answer posted at Prove that a function whose derivative is bounded is uniformly continuous. , but it seems that answer assumes the conclusion I need to arrive at. I'm trying to understand the solution posted above, but I don't quite see how they assume the conclusion I need to arrive at. Is the case similar with my question, where we can simply use the Mean Value Theorem? If not, how should I approach this problem? A bounded derivative means that the rate of change for the function will decrease and decrease, which means the function itself must also be decreasing (monotonically?). Is my line of thinking correct, or on the right track? I can't seem to put these thoughts into a formal proof. Any help on this problem would be very helpful. Thank you!","Let be an interval in (bounded or unbounded). Let be differentiable, with the property that . Show that , for all . Essentially, I need to show that a function whose derivative is bounded is uniformly continuous. I've seen the answer posted at Prove that a function whose derivative is bounded is uniformly continuous. , but it seems that answer assumes the conclusion I need to arrive at. I'm trying to understand the solution posted above, but I don't quite see how they assume the conclusion I need to arrive at. Is the case similar with my question, where we can simply use the Mean Value Theorem? If not, how should I approach this problem? A bounded derivative means that the rate of change for the function will decrease and decrease, which means the function itself must also be decreasing (monotonically?). Is my line of thinking correct, or on the right track? I can't seem to put these thoughts into a formal proof. Any help on this problem would be very helpful. Thank you!","I \mathbb{R} f: I \rightarrow \mathbb{R} |f'(x)| \le M,  \forall x \in I |f(x) - f(y)| \le M |x - y| x \in I","['real-analysis', 'derivatives', 'continuity', 'uniform-continuity']"
94,What is the limit function of the series $\sum_{n=1}^{\infty} \left(e^{-n^2}-e^{-(n+x)^2}\right)$?,What is the limit function of the series ?,\sum_{n=1}^{\infty} \left(e^{-n^2}-e^{-(n+x)^2}\right),"Consider the series $$\sum_{n=1}^{\infty}\left(e^{-n^2}-e^{-(n+x)^2}\right).$$ According to some theorems in my research, I found that it is convergent on $\mathbb{R}$, but I cannot find the limit function of this exactly. Anyone can help me to obtain this? thanks a lot.","Consider the series $$\sum_{n=1}^{\infty}\left(e^{-n^2}-e^{-(n+x)^2}\right).$$ According to some theorems in my research, I found that it is convergent on $\mathbb{R}$, but I cannot find the limit function of this exactly. Anyone can help me to obtain this? thanks a lot.",,"['real-analysis', 'sequences-and-series']"
95,"How to prove that: $\forall a \in \Bbb R,$ $\|f+a\|_p\ge \frac12\|f\|_p$ when $\int_\Omega f(x)dx= 0$",How to prove that:   when,"\forall a \in \Bbb R, \|f+a\|_p\ge \frac12\|f\|_p \int_\Omega f(x)dx= 0","Let $\Omega $ be any subset of $\Bbb R^d$ and $f\in L^p(\Omega)$ such that $$\int_\Omega f(x)dx= 0$$ Then, shows that for each real number  $a\in \Bbb R,$ we have $$\|f+a\|_{L^p(\Omega )}\ge \frac12\|f\|_{L^p(\Omega )}$$ This looks obvious but but spent already a lot of time on it. I noticed that it sufficed to prove the inequality  for $a>0 $. the case  $p=2$ is been trivial using quadratic. Any Hint or idea?","Let $\Omega $ be any subset of $\Bbb R^d$ and $f\in L^p(\Omega)$ such that $$\int_\Omega f(x)dx= 0$$ Then, shows that for each real number  $a\in \Bbb R,$ we have $$\|f+a\|_{L^p(\Omega )}\ge \frac12\|f\|_{L^p(\Omega )}$$ This looks obvious but but spent already a lot of time on it. I noticed that it sufficed to prove the inequality  for $a>0 $. the case  $p=2$ is been trivial using quadratic. Any Hint or idea?",,"['real-analysis', 'functional-analysis', 'measure-theory', 'inequality', 'lp-spaces']"
96,"Proof check - If two sequences A and B are equivalent, then the first one is a Cauchy sequence if and only if the second one is a Cauchy sequence too","Proof check - If two sequences A and B are equivalent, then the first one is a Cauchy sequence if and only if the second one is a Cauchy sequence too",,"Could someone check this proof? Definitions : Equivalence : Two sequences are equivalent iff, for any $\varepsilon > 0$,there exists an $N$ such that for $n \geq N$, $|a_n - b_n| < \varepsilon$. Cauchy Sequence : $(b_n)_{n=1}^\infty$  is a Cauchy sequence iff, for any $\varepsilon > 0$, there exists an $N$ such that $j,k \geq N$ implies $|a_j - a_k| < \varepsilon$. Proposition : Show that, if two sequences $(a_n)_{n=1}^\infty$  and $(b_n)_{n=1}^\infty$ are equivalent, then $(a_n)_{n=1}^\infty$ is a Cauchy sequence if and only if $(b_n)_{n=1}^\infty$ is a Cauchy sequence too. Proof : Let $(a_n)_{n=1}^\infty$  and $(b_n)_{n=1}^\infty$  be equivalent and assume $(a_n)_{n=1}^\infty$ is a Cauchy sequence. As both sequences are equivalent, for some n and for any $\varepsilon$ , we have $$|a_n - b_n| + | b_{n+1} - a_{n+1}| + |a_{n+1} - a_n | < \varepsilon$$ By the triangle inequality theorem, $$|a_n - b_n + b_{n+1} - a_{n+1} + a_{n+1} - a_n|  =  |b_{n+1} - b_n| <\varepsilon $$ We could show that, if  $(b_n)_{n=1}^\infty$ is a Cauchy sequence, then $(a_n)_{n=1}^\infty$ is a Cauchy sequence too, by a reciprocal argument. $\blacksquare$","Could someone check this proof? Definitions : Equivalence : Two sequences are equivalent iff, for any $\varepsilon > 0$,there exists an $N$ such that for $n \geq N$, $|a_n - b_n| < \varepsilon$. Cauchy Sequence : $(b_n)_{n=1}^\infty$  is a Cauchy sequence iff, for any $\varepsilon > 0$, there exists an $N$ such that $j,k \geq N$ implies $|a_j - a_k| < \varepsilon$. Proposition : Show that, if two sequences $(a_n)_{n=1}^\infty$  and $(b_n)_{n=1}^\infty$ are equivalent, then $(a_n)_{n=1}^\infty$ is a Cauchy sequence if and only if $(b_n)_{n=1}^\infty$ is a Cauchy sequence too. Proof : Let $(a_n)_{n=1}^\infty$  and $(b_n)_{n=1}^\infty$  be equivalent and assume $(a_n)_{n=1}^\infty$ is a Cauchy sequence. As both sequences are equivalent, for some n and for any $\varepsilon$ , we have $$|a_n - b_n| + | b_{n+1} - a_{n+1}| + |a_{n+1} - a_n | < \varepsilon$$ By the triangle inequality theorem, $$|a_n - b_n + b_{n+1} - a_{n+1} + a_{n+1} - a_n|  =  |b_{n+1} - b_n| <\varepsilon $$ We could show that, if  $(b_n)_{n=1}^\infty$ is a Cauchy sequence, then $(a_n)_{n=1}^\infty$ is a Cauchy sequence too, by a reciprocal argument. $\blacksquare$",,"['real-analysis', 'proof-verification', 'cauchy-sequences']"
97,$a_{n+1}=a_n^{n+1}+a_n+1$,,a_{n+1}=a_n^{n+1}+a_n+1,"Let $a_1>1$ and $$a_{n+1}=a_n^{n+1}+a_n+1, \: \forall n \geq 1$$ Edit: Prove that there is a real number $x \neq 0$ such that $$\lim_{n \to \infty}\frac{a_n}{x^{n!}}=1$$ I think that $x=a_1$ works, and I tried to use the Squeeze Theorem to prove that. From $a_{n+1}>a_n^{n+1}>a_{n-1}^{n(n+1)}>\dots>a_1^{(n+1)!}$ we get that $1<\frac{a_{n+1}}{a_1^{(n+1)!}}$ and then I tried to find another bound for $a_{n+1}$ such that $\frac{a_{n+1}}{a_1^{(n+1)!}}<b_{n+1} \to 1$, but nothing seems to work.","Let $a_1>1$ and $$a_{n+1}=a_n^{n+1}+a_n+1, \: \forall n \geq 1$$ Edit: Prove that there is a real number $x \neq 0$ such that $$\lim_{n \to \infty}\frac{a_n}{x^{n!}}=1$$ I think that $x=a_1$ works, and I tried to use the Squeeze Theorem to prove that. From $a_{n+1}>a_n^{n+1}>a_{n-1}^{n(n+1)}>\dots>a_1^{(n+1)!}$ we get that $1<\frac{a_{n+1}}{a_1^{(n+1)!}}$ and then I tried to find another bound for $a_{n+1}$ such that $\frac{a_{n+1}}{a_1^{(n+1)!}}<b_{n+1} \to 1$, but nothing seems to work.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
98,Prove that the sum of a convergent and a divergent sequence is divergent,Prove that the sum of a convergent and a divergent sequence is divergent,,"I wonder whether my proof is correct. The exercise is to prove that for $(x_n)$ a convergent sequence and $(y_n)$ a divergent sequence, $(x_n + y_n)$ diverges. Attempt (only for divergence to infinity): The sequence $(x_n)$ is convergent, thus bounded, so for every $k \in \mathbb{N}$: $x_k \geq M$ for $M = \inf x_n$. Let $\xi > 0$. Choose $N \in \mathbb{N}$ such that $y_n \geq \xi - M$ for every $n \geq N$. Then: $x_n + y_n \geq M + y_n \geq M + \xi - M \geq \xi$. In a same manner for divergence to infinity. Is this proof correct? Thank you!","I wonder whether my proof is correct. The exercise is to prove that for $(x_n)$ a convergent sequence and $(y_n)$ a divergent sequence, $(x_n + y_n)$ diverges. Attempt (only for divergence to infinity): The sequence $(x_n)$ is convergent, thus bounded, so for every $k \in \mathbb{N}$: $x_k \geq M$ for $M = \inf x_n$. Let $\xi > 0$. Choose $N \in \mathbb{N}$ such that $y_n \geq \xi - M$ for every $n \geq N$. Then: $x_n + y_n \geq M + y_n \geq M + \xi - M \geq \xi$. In a same manner for divergence to infinity. Is this proof correct? Thank you!",,"['real-analysis', 'proof-verification']"
99,"Describe the set of all elements $x,y \in H$, such that $\|x+y\|=\|x\|+\|y\|$.","Describe the set of all elements , such that .","x,y \in H \|x+y\|=\|x\|+\|y\|","The question is as follows: Let $H$ be a Hilbert space. Describe the set of all elements $x,y \in H$, such that $\|x+y\|=\|x\|+\|y\|$. $\textbf{An idea:}$ The set of all such elements will be in a unit sphere that contains a line segment $[x,y]$ where $x,y \in H$ and $x \neq y.$ Such elements are linearly independent, because suppose they are dependent and say $y = \beta x$ for some $\beta \in \mathbb{C}$. Then we have $1 = \|ax + (1-a)\beta x \| = \|a + (1-a)\beta\|$. Then for $a = 0$ we get $|\beta|=1$ and for $a = \frac{1}{2}$ we get $|1+\beta|=2$ which implies that $\beta = 1$ and so $x=y$, which is contradiction. Can you please let me know if I am wrong? And if I am wrong? Can you please let me know what is the correct answer? Thanks!","The question is as follows: Let $H$ be a Hilbert space. Describe the set of all elements $x,y \in H$, such that $\|x+y\|=\|x\|+\|y\|$. $\textbf{An idea:}$ The set of all such elements will be in a unit sphere that contains a line segment $[x,y]$ where $x,y \in H$ and $x \neq y.$ Such elements are linearly independent, because suppose they are dependent and say $y = \beta x$ for some $\beta \in \mathbb{C}$. Then we have $1 = \|ax + (1-a)\beta x \| = \|a + (1-a)\beta\|$. Then for $a = 0$ we get $|\beta|=1$ and for $a = \frac{1}{2}$ we get $|1+\beta|=2$ which implies that $\beta = 1$ and so $x=y$, which is contradiction. Can you please let me know if I am wrong? And if I am wrong? Can you please let me know what is the correct answer? Thanks!",,"['real-analysis', 'functional-analysis', 'hilbert-spaces']"
