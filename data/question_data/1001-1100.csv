,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Why is Lebesgue-Stieltjes a generalization of Riemann-Stieltjes? Moreover, is there an example where Lebesgue-Stieltjes is useful","Why is Lebesgue-Stieltjes a generalization of Riemann-Stieltjes? Moreover, is there an example where Lebesgue-Stieltjes is useful",,"I certainly have a question, but i don't know what the best title should be. Please edit the title if there is a better one :) And I believe, to get a better answer, it would be good to explain exactly what i feel and know.. ======== Riemann-Stieltjes Integral (Darboux Sum) can be defined for bounded integrand $f$ and nondecreasing $\alpha$ . ( $\alpha$ does not need to be continuous ). However, as far as i know, Lebesgue-Stieltjes Integral requires integrator to be right or left continuous. Moreover, even if there is a way to define measure associated to nondecreasing discontinuous function, integral w.r.t to this function cannot be identical to that of Riemann-Stieltjes. (I'll explain it below) Below is the result i have proved: Theorem Let $\alpha:\mathbb{R}$ be a monotonically increasing right-continuous function. Let $f:[a,b]\rightarrow \mathbb{R}$ be a RS-integrable function w.r.t $\alpha$ on $[a,b]$ . Let $\mu_\alpha$ be the LS-measure w.r.t $\alpha$ . Then, there exists $F\in L^1(\mu_\alpha)$ such that $\forall x\in(a,b], F(x)=f(x)$ and $\int_a^b f d\alpha = \int_{(a,b]} F d\mu_\alpha$ Since when one wants to use RS-integration, one's interest is exactly in $[a,b]$ . So rather than using full domain of $\alpha$ as illustrated in the above theorem, we can replace it by $\alpha(x)=\alpha(a)$ for $x<a$ . Then, this new $\alpha$ is continuous at $a$ . Thus $\int_a^b f d\alpha = \int_{[a,b]} F d\mu_\alpha$ . Hence, in the case $\alpha$ is right continuous, it can be understood that LS-integration is an extension of RS-integration. However, in general, if $\alpha$ is not right or left continuous, LS-integration cannot be a generalization of RS-integration. Let $\alpha$ be a nondecreasing function. Suppose there exists a measure $\mu$ such that for some type of interval $【a,b】$ , $\mu(【a,b】)=\alpha(b)-\alpha(a)$ . (【a,b】denotes one of open,closed,half-open and whatsoever that has an interval form.) Then, the continuity of $\mu$ shows that the only choice that $\mu$ could be a measure is that $\mu((a,b))=\alpha(b-)-\alpha(a+)$ . Indeed, we can construct such measure. Define $F(x)=\alpha(x+)$ . Then $F$ is right-continuous nondecreasing function and the LS-measure $\mu$ associated with $F$ is the measure such that $\mu((a,b))=\alpha(b-) - \alpha(a+)$ . In this case, RS-integral and LS-integral of a bounded function do not coincide in general. When $\alpha$ is continuous, LS seems more natural than RS to me. However, when $\alpha$ is not continuous, LS does not seem natural at all to me.. I'm an undergraduate, so i don't know many cases where integration is useful. I don't have any clue why one should know LS integration. Is there any natural-science example of a function that is only LS integrable but not RS integrable? Or is there any mathematical example that LS integration is useful? I understand how Lebesgue Integration flows better than Riemann Integration, but I don't see any reason why LS-integration is in need.","I certainly have a question, but i don't know what the best title should be. Please edit the title if there is a better one :) And I believe, to get a better answer, it would be good to explain exactly what i feel and know.. ======== Riemann-Stieltjes Integral (Darboux Sum) can be defined for bounded integrand and nondecreasing . ( does not need to be continuous ). However, as far as i know, Lebesgue-Stieltjes Integral requires integrator to be right or left continuous. Moreover, even if there is a way to define measure associated to nondecreasing discontinuous function, integral w.r.t to this function cannot be identical to that of Riemann-Stieltjes. (I'll explain it below) Below is the result i have proved: Theorem Let be a monotonically increasing right-continuous function. Let be a RS-integrable function w.r.t on . Let be the LS-measure w.r.t . Then, there exists such that and Since when one wants to use RS-integration, one's interest is exactly in . So rather than using full domain of as illustrated in the above theorem, we can replace it by for . Then, this new is continuous at . Thus . Hence, in the case is right continuous, it can be understood that LS-integration is an extension of RS-integration. However, in general, if is not right or left continuous, LS-integration cannot be a generalization of RS-integration. Let be a nondecreasing function. Suppose there exists a measure such that for some type of interval , . (【a,b】denotes one of open,closed,half-open and whatsoever that has an interval form.) Then, the continuity of shows that the only choice that could be a measure is that . Indeed, we can construct such measure. Define . Then is right-continuous nondecreasing function and the LS-measure associated with is the measure such that . In this case, RS-integral and LS-integral of a bounded function do not coincide in general. When is continuous, LS seems more natural than RS to me. However, when is not continuous, LS does not seem natural at all to me.. I'm an undergraduate, so i don't know many cases where integration is useful. I don't have any clue why one should know LS integration. Is there any natural-science example of a function that is only LS integrable but not RS integrable? Or is there any mathematical example that LS integration is useful? I understand how Lebesgue Integration flows better than Riemann Integration, but I don't see any reason why LS-integration is in need.","f \alpha \alpha \alpha:\mathbb{R} f:[a,b]\rightarrow \mathbb{R} \alpha [a,b] \mu_\alpha \alpha F\in L^1(\mu_\alpha) \forall x\in(a,b], F(x)=f(x) \int_a^b f d\alpha = \int_{(a,b]} F d\mu_\alpha [a,b] \alpha \alpha(x)=\alpha(a) x<a \alpha a \int_a^b f d\alpha = \int_{[a,b]} F d\mu_\alpha \alpha \alpha \alpha \mu 【a,b】 \mu(【a,b】)=\alpha(b)-\alpha(a) \mu \mu \mu((a,b))=\alpha(b-)-\alpha(a+) F(x)=\alpha(x+) F \mu F \mu((a,b))=\alpha(b-) - \alpha(a+) \alpha \alpha","['real-analysis', 'integration', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
1,Convergence of $a_n$ given $a_{\lfloor{x^n}\rfloor}$ converges to $0$ [duplicate],Convergence of  given  converges to  [duplicate],a_n a_{\lfloor{x^n}\rfloor} 0,"This question already has an answer here : Convergence of a sequence with assumption that exponential subsequences converge? (1 answer) Closed 5 years ago . Sequence $a_n$ has a property that for every real $x > 1$ , sequence $a_{\lfloor{x^n}\rfloor}$ converges to $0$ . Does that mean that $a_n$ converges to $0$ ? I have tried to find a counterexample, but failed at it, so I think it might be true, but I don't know how to prove it.","This question already has an answer here : Convergence of a sequence with assumption that exponential subsequences converge? (1 answer) Closed 5 years ago . Sequence has a property that for every real , sequence converges to . Does that mean that converges to ? I have tried to find a counterexample, but failed at it, so I think it might be true, but I don't know how to prove it.",a_n x > 1 a_{\lfloor{x^n}\rfloor} 0 a_n 0,"['real-analysis', 'limits', 'convergence-divergence']"
2,"""Converse"" of Taylor's theorem","""Converse"" of Taylor's theorem",,"Let $f:(a,b)\to\mathbb R$. We know that for every $c\in(a,b)$ we can write $f(t)=\sum_{i=0}^k a_i(c)(t-c)^i+o\left((t-c)^k\right)$ and $\forall i$ $a_i(c)$ is continuous (with respect to $c$). Can we conclude that $f$ is of class $C^k$?","Let $f:(a,b)\to\mathbb R$. We know that for every $c\in(a,b)$ we can write $f(t)=\sum_{i=0}^k a_i(c)(t-c)^i+o\left((t-c)^k\right)$ and $\forall i$ $a_i(c)$ is continuous (with respect to $c$). Can we conclude that $f$ is of class $C^k$?",,"['calculus', 'real-analysis', 'functions', 'derivatives']"
3,Roots of polynomials with bounded integer coefficients,Roots of polynomials with bounded integer coefficients,,"For $a\in\mathbb N$ , let $R_a\subseteq\mathbb{R}$ be the set of real roots of polynomials whose coefficients are integers with absolute value at most $a$ . $$R_a=\left\{r\in\mathbb{R}\middle|\sum_{i=0}^na_ir^i=0,n\in\mathbb N,a_i\in\mathbb Z,|a_i|\leq a, a_n\neq 0\right\}$$ (Note that the degrees of the allowed polynomials aren't bounded.) I want to know which numbers can be arbitrarily well approximated by such roots. In particular, is $R_a$ dense on some interval for any $a$ ? Discussion: If $r\in R_a$ and $r\neq 0$ then $1/r\in R_a$ , since by reversing the order of the coefficients of the polynomial of which $r$ was a root one obtains a polynomial of which $1/r$ is a root. In the comments, mathworker21 noted that by considering the polynomial $1 - aX -\dots-aX^n$ one can show that every nonzero $r\in R_a$ satisfies $|r|>\frac{1}{a+1}$ . Therefore we also have $|r|<a+1$ , so $R_a$ is bounded. We could also ask about the set $R_a^\mathbb C$ of the roots of such polynomials in $\mathbb C$ . John Baez has a page about such sets here , which contains some beautiful pictures. He's also written a paper , which gives a reference to a paper by Thierry Bousch in which it is proved that the roots of the Littlewood polynomials (those with coefficients $\pm1$ only) are dense on the annulus $2^{-1/4}<|z|<2^{1/4}$ . Since $\{-1,1\}\subseteq\{-1,0,1\}$ , this answers the complex analogue of my question: the set $R_1^\mathbb C$ is dense on some ball. Unfortunately the paper is written in French, so I can't tell if the proof generalises to the reals. (The real result isn't an immediate corollary of the complex version, because there might be a sequence in $R_a^\mathbb C$ tending to $x\in\mathbb R$ without there being such a sequence in $R_a^\mathbb C\cap\mathbb R$ .)","For , let be the set of real roots of polynomials whose coefficients are integers with absolute value at most . (Note that the degrees of the allowed polynomials aren't bounded.) I want to know which numbers can be arbitrarily well approximated by such roots. In particular, is dense on some interval for any ? Discussion: If and then , since by reversing the order of the coefficients of the polynomial of which was a root one obtains a polynomial of which is a root. In the comments, mathworker21 noted that by considering the polynomial one can show that every nonzero satisfies . Therefore we also have , so is bounded. We could also ask about the set of the roots of such polynomials in . John Baez has a page about such sets here , which contains some beautiful pictures. He's also written a paper , which gives a reference to a paper by Thierry Bousch in which it is proved that the roots of the Littlewood polynomials (those with coefficients only) are dense on the annulus . Since , this answers the complex analogue of my question: the set is dense on some ball. Unfortunately the paper is written in French, so I can't tell if the proof generalises to the reals. (The real result isn't an immediate corollary of the complex version, because there might be a sequence in tending to without there being such a sequence in .)","a\in\mathbb N R_a\subseteq\mathbb{R} a R_a=\left\{r\in\mathbb{R}\middle|\sum_{i=0}^na_ir^i=0,n\in\mathbb N,a_i\in\mathbb Z,|a_i|\leq a, a_n\neq 0\right\} R_a a r\in R_a r\neq 0 1/r\in R_a r 1/r 1 - aX -\dots-aX^n r\in R_a |r|>\frac{1}{a+1} |r|<a+1 R_a R_a^\mathbb C \mathbb C \pm1 2^{-1/4}<|z|<2^{1/4} \{-1,1\}\subseteq\{-1,0,1\} R_1^\mathbb C R_a^\mathbb C x\in\mathbb R R_a^\mathbb C\cap\mathbb R","['real-analysis', 'polynomials', 'roots', 'mathematical-french']"
4,A combination integral and series resulting the inverse tangent integral,A combination integral and series resulting the inverse tangent integral,,"$\def\Ti{{\rm{Ti}}_2}$I have been able to solve an integral problem, now I tried to use the other method to crack the integral and I have to prove the following expression \begin{equation} I=\sum_{n=1}^\infty \frac{z^{2n-1}}{2n-1}\int_0^{\Large\frac{\pi}{2}}\sin(2n-1)x\cot x\,dx=-\frac{1}{2}\Ti\left(\frac{1-z^2}{2z}\right)\qquad\quad\quad(\star) \end{equation} where $\Ti(\cdot)$ is the inverse tangent integral . I managed to evaluate the integral and I obtained (if I am not mistaken) \begin{equation} \int_0^{\Large\frac{\pi}{2}}\sin(2n-1)x\cot x\,dx=\frac{(-1)^{n-1}}{2n-1}+2\sum_{k=1}^{n-1}\frac{(-1)^{k-1}}{2k-1} \end{equation} then \begin{equation} I=\sum_{n=1}^\infty \frac{z^{2n-1}}{2n-1}\left[\frac{(-1)^{n-1}}{2n-1}+2\sum_{k=1}^{n-1}\frac{(-1)^{k-1}}{2k-1}\right]=\Ti\left(z\right)+2\sum_{n=1}^\infty \frac{z^{2n-1}}{2n-1}\sum_{k=1}^{n-1}\frac{(-1)^{k-1}}{2k-1} \end{equation} I am stuck in the last expression. Could anyone here please help me to prove the expression $(\star)$? Any help would be greatly appreciated. Thank you.","$\def\Ti{{\rm{Ti}}_2}$I have been able to solve an integral problem, now I tried to use the other method to crack the integral and I have to prove the following expression \begin{equation} I=\sum_{n=1}^\infty \frac{z^{2n-1}}{2n-1}\int_0^{\Large\frac{\pi}{2}}\sin(2n-1)x\cot x\,dx=-\frac{1}{2}\Ti\left(\frac{1-z^2}{2z}\right)\qquad\quad\quad(\star) \end{equation} where $\Ti(\cdot)$ is the inverse tangent integral . I managed to evaluate the integral and I obtained (if I am not mistaken) \begin{equation} \int_0^{\Large\frac{\pi}{2}}\sin(2n-1)x\cot x\,dx=\frac{(-1)^{n-1}}{2n-1}+2\sum_{k=1}^{n-1}\frac{(-1)^{k-1}}{2k-1} \end{equation} then \begin{equation} I=\sum_{n=1}^\infty \frac{z^{2n-1}}{2n-1}\left[\frac{(-1)^{n-1}}{2n-1}+2\sum_{k=1}^{n-1}\frac{(-1)^{k-1}}{2k-1}\right]=\Ti\left(z\right)+2\sum_{n=1}^\infty \frac{z^{2n-1}}{2n-1}\sum_{k=1}^{n-1}\frac{(-1)^{k-1}}{2k-1} \end{equation} I am stuck in the last expression. Could anyone here please help me to prove the expression $(\star)$? Any help would be greatly appreciated. Thank you.",,"['calculus', 'real-analysis', 'integration', 'sequences-and-series', 'definite-integrals']"
5,An alternative proof of Cauchy's Mean Value Theorem,An alternative proof of Cauchy's Mean Value Theorem,,"Let's focus on the following version of Cauchy's Mean Value Theorem: Cauchy's Mean Value Theorem : Let $f, g$ be functions defined on closed interval $[a, b]$ such that 1) Both $f, g$ are continuous on $[a, b]$ 2) Both $f, g$ are differentiable on $(a, b)$ 3) $g'$ does not vanish at any point in $(a, b)$ then there is a point $c \in (a, b)$ such that $$\frac{f(b) - f(a)}{g(b) - g(a)} = \frac{f'(c)}{g'(c)}$$ In most good textbooks it is mentioned that this theorem can't be derived from the usual Mean Value Theorem. Using MVT we can get $$\frac{f(b) - f(a)}{g(b) - g(a)} = \frac{(f(b) - f(a))/(b - a)}{(g(b) - g(a))/(b - a)} = \frac{f'(c_{1})}{g'(c_{2})}$$ and we can't guarantee that $c_{1} = c_{2}$ and hence the above strategy to prove Cauchy's theorem fails. However I beg to differ and I provide the following argument. The geometrical interpretation of MVT is that if we take any two points on a continuous and smooth curve (with some further restrictions on nature of curve) then between this portion of the curve there is a point at which the tangent to the curve is parallel to the chord joining these two points . Cauchy's theorem is obtained at once if we use the parametric equation of the curve with $x = g(t), y = f(t), dy/dx = f'(t)/g'(t)$ for $t \in (a, b) $ and the endpoints of the curve are $(g(a), f(a)), (g(b), f(b))$. Hence Cauchy's Theorem is derivable from MVT. The rigorous proof proceeds as follows. Since $g'$ does not vanish it is clear from Rolle's theorem that $g$ is one one function so that $g(a) \neq g(b)$. If $g(a) < g(b)$ then we can show that $g$ is strictly increasing. Let $a \leq c < d \leq b$. Clearly $g(c) \neq g(d) \neq g(a)$. Let us suppose $g(d) < g(a)$ so that $g(d) < g(a) < g(b)$ and hence by IVT there will be a point $x$ in $(d, b)$ with $g(x) = g(a)$ contrary to the fact that $g$ is one one. So we must have $g(a) < g(d)$. Similarly $g(c) < g(b)$. Now suppose that $g(c) > g(d)$ so that $g(a) < g(d) < g(c)$ and by IVT there is an $x \in (a, c)$ with $g(x) = g(d)$ contrary to the fact that $f$ is one one. Hence $g(c) < g(d)$. We have thus shown that $g$ is strictly increasing. If $g(a) > g(b)$ then we can show in same manner that $g$ is strictly decreasing. Hence by inverse function theorem $h = g^{-1}$ exists and is continuous and differentiable. Suppose $g$ is increasing and let $A = g(a), B = g(b)$ so that $a = h(A), b = h(B)$ and consider the function $F(x) = f(h(x))$ on $[A, B]$. By MVT there is a $C \in (A, B)$ such that $$F(B) - F(A) = (B - A)F'(C)$$ or $$f(h(B)) - f(h(A)) = (g(b) - g(a))f'(h(C))h'(C)$$ If $h(C) = c$ then $c \in (a, b)$ and $h'(C) = 1/g'(c)$ and we get $$\frac{f(b) - f(a)}{g(b) - g(a)} = \frac{f'(c)}{g'(c)}$$ I would like to get feedback on this proof above. I haven't found any issue with the argument and think it is perfectly valid.","Let's focus on the following version of Cauchy's Mean Value Theorem: Cauchy's Mean Value Theorem : Let $f, g$ be functions defined on closed interval $[a, b]$ such that 1) Both $f, g$ are continuous on $[a, b]$ 2) Both $f, g$ are differentiable on $(a, b)$ 3) $g'$ does not vanish at any point in $(a, b)$ then there is a point $c \in (a, b)$ such that $$\frac{f(b) - f(a)}{g(b) - g(a)} = \frac{f'(c)}{g'(c)}$$ In most good textbooks it is mentioned that this theorem can't be derived from the usual Mean Value Theorem. Using MVT we can get $$\frac{f(b) - f(a)}{g(b) - g(a)} = \frac{(f(b) - f(a))/(b - a)}{(g(b) - g(a))/(b - a)} = \frac{f'(c_{1})}{g'(c_{2})}$$ and we can't guarantee that $c_{1} = c_{2}$ and hence the above strategy to prove Cauchy's theorem fails. However I beg to differ and I provide the following argument. The geometrical interpretation of MVT is that if we take any two points on a continuous and smooth curve (with some further restrictions on nature of curve) then between this portion of the curve there is a point at which the tangent to the curve is parallel to the chord joining these two points . Cauchy's theorem is obtained at once if we use the parametric equation of the curve with $x = g(t), y = f(t), dy/dx = f'(t)/g'(t)$ for $t \in (a, b) $ and the endpoints of the curve are $(g(a), f(a)), (g(b), f(b))$. Hence Cauchy's Theorem is derivable from MVT. The rigorous proof proceeds as follows. Since $g'$ does not vanish it is clear from Rolle's theorem that $g$ is one one function so that $g(a) \neq g(b)$. If $g(a) < g(b)$ then we can show that $g$ is strictly increasing. Let $a \leq c < d \leq b$. Clearly $g(c) \neq g(d) \neq g(a)$. Let us suppose $g(d) < g(a)$ so that $g(d) < g(a) < g(b)$ and hence by IVT there will be a point $x$ in $(d, b)$ with $g(x) = g(a)$ contrary to the fact that $g$ is one one. So we must have $g(a) < g(d)$. Similarly $g(c) < g(b)$. Now suppose that $g(c) > g(d)$ so that $g(a) < g(d) < g(c)$ and by IVT there is an $x \in (a, c)$ with $g(x) = g(d)$ contrary to the fact that $f$ is one one. Hence $g(c) < g(d)$. We have thus shown that $g$ is strictly increasing. If $g(a) > g(b)$ then we can show in same manner that $g$ is strictly decreasing. Hence by inverse function theorem $h = g^{-1}$ exists and is continuous and differentiable. Suppose $g$ is increasing and let $A = g(a), B = g(b)$ so that $a = h(A), b = h(B)$ and consider the function $F(x) = f(h(x))$ on $[A, B]$. By MVT there is a $C \in (A, B)$ such that $$F(B) - F(A) = (B - A)F'(C)$$ or $$f(h(B)) - f(h(A)) = (g(b) - g(a))f'(h(C))h'(C)$$ If $h(C) = c$ then $c \in (a, b)$ and $h'(C) = 1/g'(c)$ and we get $$\frac{f(b) - f(a)}{g(b) - g(a)} = \frac{f'(c)}{g'(c)}$$ I would like to get feedback on this proof above. I haven't found any issue with the argument and think it is perfectly valid.",,"['calculus', 'real-analysis']"
6,Stokes' Theorem general case,Stokes' Theorem general case,,"With the following lemma : Lemma : Let $f_{+},f_{-} : U \longmapsto \mathbb{R}$ be $C^{1}$ maps with $f_{-} \leq 0 \leq f_{+}$ with $U \subset \mathbb{R}^{n}$ open and bounded with $C^{1}$ boundary. Let $\Omega : \lbrace (x,t) : x \in U \wedge f_{-}(x) < t < f_{+}(x) \rbrace$ and $u \in C^{1}(\overline{\Omega})$ . Let $y = (x,t)$ then we have $$\int_{\Omega} \partial_{t}u(y)dy = \int_{\Sigma_{+}\cup \Sigma_{-}}u(z) \cdot \langle e_{t},v_{ext}(z)\rangle d\sigma(z)$$ Where $\Sigma_{+} = graph(f_{+}),\Sigma_{-} = graph(f_{-})$ and $v_{ext}(z)$ is the normal tangent vector. I'd like to prove the general case, which means the following theorem : Theorem : Let $\Omega \subset \mathbb{R}^{m}$ open and bounded with $C^{1}$ boundary. Then given $u \in C^{1}({\overline{\Omega}})$ and said $v_{ext}(z)$ the normal tangent vector of the boundary in the point $z \in \partial \Omega$ in outgoing direction with respect to the ""inside"" of the manifold, we have $$\int_{\Omega}\partial_{i}u(y)dy = \int_{\partial \Omega}u(z)\cdot \langle e_{i},v_{ext}(z)\rangle d\sigma(z)$$ I do understand the proof of the lemma, what I don't understand is the general proof of the theorem which requires an easy Sard's theorem version to affirm that the image under the projection $p : \mathbb{R}^{n} \longmapsto \mathbb{R}^{n-1} \hspace{0.2cm} p(x_{1},\cdots,x_{n}) = (x_{1},\cdots,x_{n-1})$ of the point of the boundary, where the restriction of $p$ to the tangent space $T_{x}\partial \Omega$ is not invertible, has measure $0$ , in other words $p(\lbrace x \in \partial\Omega : \lvert p_{|_{T_{x}\partial\Omega}} \mbox{not invertible}\rbrace) \rvert = 0$ . Thanks to this observation we can proceed as if $\partial\Omega$ didn't contain ""critical"" points where $v_{ext} \perp e_{i}$ . Then called $U = p(\Omega)-p(\lbrace x \in \partial\Omega : \langle e_{i},v_{ext} \rangle = 0\rbrace)$ , open in $\mathbb{R}^{n-1}$ , and $V$ a connected component of $U$ , I should be able to prove that $\Omega \cap p^{-1}(V)$ is a finite union of open and disjoint sets which are of the form of the lemma already proven and conclude the theorem by additivity of the integral. Any solution and help on how to prove that $\Omega \cap p^{-1}(V)$ is a finite union of open and disjoint set which are of the form of the lemma already proven is well accepted. I know there are easier ways to prove the theorem for instance using partition of unity, a tool I'm not aware of. I would like to keep the proof as linear as possible, possibly using just general topology and the analysis required. As general as possible solutions using just general topology and the analysis concerning the second year of math university are well accepted too. Edit 1 : The definition of manifold I'm currently using is : Definition : A subset $M \subseteq \mathbb{R}^{n}$ is a differentiable manifold of class $C^{l}$ and dimension $k$ if $\forall \hspace{0.1cm} x \in M \hspace{0.1cm} \exists  \hspace{0.1cm} U \in \mathbb{R}^{k},V \in \mathbb{R}^{n}$ open subsets, with $x \in V$ and a diffeomorphism $\phi : U \longmapsto V \cap M$ of class $C^{l}$ . While the ""easy"" version of Sard's Theorem for manifolds is the following : Definition : Let $f : \Sigma \longmapsto \Gamma$ a function of class $C^{1}$ between manifolds of same dimension. A point $p \in \Sigma$ is said to be critical is $d_{p}f$ is not surjective. The immage of the critical point are said critical values . Theorem : Let $f : \Sigma \longmapsto \Gamma$ a function of class $C^{1}$ between manifolds. The critical values have null measure.","With the following lemma : Lemma : Let be maps with with open and bounded with boundary. Let and . Let then we have Where and is the normal tangent vector. I'd like to prove the general case, which means the following theorem : Theorem : Let open and bounded with boundary. Then given and said the normal tangent vector of the boundary in the point in outgoing direction with respect to the ""inside"" of the manifold, we have I do understand the proof of the lemma, what I don't understand is the general proof of the theorem which requires an easy Sard's theorem version to affirm that the image under the projection of the point of the boundary, where the restriction of to the tangent space is not invertible, has measure , in other words . Thanks to this observation we can proceed as if didn't contain ""critical"" points where . Then called , open in , and a connected component of , I should be able to prove that is a finite union of open and disjoint sets which are of the form of the lemma already proven and conclude the theorem by additivity of the integral. Any solution and help on how to prove that is a finite union of open and disjoint set which are of the form of the lemma already proven is well accepted. I know there are easier ways to prove the theorem for instance using partition of unity, a tool I'm not aware of. I would like to keep the proof as linear as possible, possibly using just general topology and the analysis required. As general as possible solutions using just general topology and the analysis concerning the second year of math university are well accepted too. Edit 1 : The definition of manifold I'm currently using is : Definition : A subset is a differentiable manifold of class and dimension if open subsets, with and a diffeomorphism of class . While the ""easy"" version of Sard's Theorem for manifolds is the following : Definition : Let a function of class between manifolds of same dimension. A point is said to be critical is is not surjective. The immage of the critical point are said critical values . Theorem : Let a function of class between manifolds. The critical values have null measure.","f_{+},f_{-} : U \longmapsto \mathbb{R} C^{1} f_{-} \leq 0 \leq f_{+} U \subset \mathbb{R}^{n} C^{1} \Omega : \lbrace (x,t) : x \in U \wedge f_{-}(x) < t < f_{+}(x) \rbrace u \in C^{1}(\overline{\Omega}) y = (x,t) \int_{\Omega} \partial_{t}u(y)dy = \int_{\Sigma_{+}\cup \Sigma_{-}}u(z) \cdot \langle e_{t},v_{ext}(z)\rangle d\sigma(z) \Sigma_{+} = graph(f_{+}),\Sigma_{-} = graph(f_{-}) v_{ext}(z) \Omega \subset \mathbb{R}^{m} C^{1} u \in C^{1}({\overline{\Omega}}) v_{ext}(z) z \in \partial \Omega \int_{\Omega}\partial_{i}u(y)dy = \int_{\partial \Omega}u(z)\cdot \langle e_{i},v_{ext}(z)\rangle d\sigma(z) p : \mathbb{R}^{n} \longmapsto \mathbb{R}^{n-1} \hspace{0.2cm} p(x_{1},\cdots,x_{n}) = (x_{1},\cdots,x_{n-1}) p T_{x}\partial \Omega 0 p(\lbrace x \in \partial\Omega : \lvert p_{|_{T_{x}\partial\Omega}} \mbox{not invertible}\rbrace) \rvert = 0 \partial\Omega v_{ext} \perp e_{i} U = p(\Omega)-p(\lbrace x \in \partial\Omega : \langle e_{i},v_{ext} \rangle = 0\rbrace) \mathbb{R}^{n-1} V U \Omega \cap p^{-1}(V) \Omega \cap p^{-1}(V) M \subseteq \mathbb{R}^{n} C^{l} k \forall \hspace{0.1cm} x \in M \hspace{0.1cm} \exists  \hspace{0.1cm} U \in \mathbb{R}^{k},V \in \mathbb{R}^{n} x \in V \phi : U \longmapsto V \cap M C^{l} f : \Sigma \longmapsto \Gamma C^{1} p \in \Sigma d_{p}f f : \Sigma \longmapsto \Gamma C^{1}","['real-analysis', 'general-topology', 'stokes-theorem', 'manifolds-with-boundary', 'divergence-theorem']"
7,Lagrange multipliers in Calculus of Variations,Lagrange multipliers in Calculus of Variations,,"I am trying to learn about Calculus of Variations and I am beginning to see some constrained optimization problems in the domain of functionals, by using Lagrange multipliers. It seems that things work like in the finite-dimensional calculus but I need some semi-formal explanations; which I failed to find anywhere. (Looks like this is a rare area of interest). In the finite dimensional calculus, when we wanted to optimize a function $f(x), f:\mathbb{R^n} \mapsto \mathbb{R}$ such that it obeys the constraint $g(x)=0$ , the idea was that $g(x)=0$ was a $n-1$ dimensional surface and $\nabla g(x)$ was normal to the surface's tangent hyperplane. In order to move on the surface, we had to move in directions $u$ where $\nabla g(x)^T \cdot u=0$ . This requires that at a constrained extremum it is needed that $\nabla f(x)^T \cdot u=0$ whenever $\nabla g(x)^T \cdot u=0$ , which requires that $\nabla f(x) = \lambda \nabla g(x)$ . Is this valid in a function space, like $C[a,b]$ as well? Can it be shown that if we have a nice behaving constraint functional $G[y(x)]=0$ in this space, the gradient $\nabla G[y(x)]$ is perpendicular to the function space surface $G[y(x)]=0$ ? We have the inner product $\int_{a}^{b}f(x)g(x)dx$ defined for this space which introduces angles and orthogonality between functions; so I intuitively think that this is true, but I wonder what the actual explanation is. Assuming $1$ is true, can it be said that at a constrained extremum, it should be $\nabla F[y(x)] = \lambda \nabla G[y(x)]$ , like in the finite dimensional case? Does one build a ""Lagrangian functional"" $L[y(x),\lambda]=F[y(x)] + \lambda G[y(x)]$ like in the finite dimensional case, where the problem turns into the unconstrained optimization of the weird thing $L[y(x),\lambda]$ , which depends both on a functional and a scalar variable? How does one maximizes such a thing then: This space is something like $C[a,b] \times \mathbb{R}$ . How are derivatives etc. defined here; for example can we define a Gateaux variation here like $$\lim_{\epsilon \to 0} \dfrac{L[y(x) + \epsilon h(x),\lambda + \epsilon] - L[y(x),\lambda]}{\epsilon}$$ which leads to unconstrained optimization? I need some explanations for such cases.","I am trying to learn about Calculus of Variations and I am beginning to see some constrained optimization problems in the domain of functionals, by using Lagrange multipliers. It seems that things work like in the finite-dimensional calculus but I need some semi-formal explanations; which I failed to find anywhere. (Looks like this is a rare area of interest). In the finite dimensional calculus, when we wanted to optimize a function such that it obeys the constraint , the idea was that was a dimensional surface and was normal to the surface's tangent hyperplane. In order to move on the surface, we had to move in directions where . This requires that at a constrained extremum it is needed that whenever , which requires that . Is this valid in a function space, like as well? Can it be shown that if we have a nice behaving constraint functional in this space, the gradient is perpendicular to the function space surface ? We have the inner product defined for this space which introduces angles and orthogonality between functions; so I intuitively think that this is true, but I wonder what the actual explanation is. Assuming is true, can it be said that at a constrained extremum, it should be , like in the finite dimensional case? Does one build a ""Lagrangian functional"" like in the finite dimensional case, where the problem turns into the unconstrained optimization of the weird thing , which depends both on a functional and a scalar variable? How does one maximizes such a thing then: This space is something like . How are derivatives etc. defined here; for example can we define a Gateaux variation here like which leads to unconstrained optimization? I need some explanations for such cases.","f(x), f:\mathbb{R^n} \mapsto \mathbb{R} g(x)=0 g(x)=0 n-1 \nabla g(x) u \nabla g(x)^T \cdot u=0 \nabla f(x)^T \cdot u=0 \nabla g(x)^T \cdot u=0 \nabla f(x) = \lambda \nabla g(x) C[a,b] G[y(x)]=0 \nabla G[y(x)] G[y(x)]=0 \int_{a}^{b}f(x)g(x)dx 1 \nabla F[y(x)] = \lambda \nabla G[y(x)] L[y(x),\lambda]=F[y(x)] + \lambda G[y(x)] L[y(x),\lambda] C[a,b] \times \mathbb{R} \lim_{\epsilon \to 0} \dfrac{L[y(x) + \epsilon h(x),\lambda + \epsilon] - L[y(x),\lambda]}{\epsilon}","['real-analysis', 'functional-analysis', 'multivariable-calculus', 'calculus-of-variations', 'lagrange-multiplier']"
8,Prove that $\lim_{x\rightarrow 0}\frac{f(x^2)-f(0)}{x}=0$ if $f:\mathbb{R}\rightarrow\mathbb{R}$ is differentiable at $x=0$,Prove that  if  is differentiable at,\lim_{x\rightarrow 0}\frac{f(x^2)-f(0)}{x}=0 f:\mathbb{R}\rightarrow\mathbb{R} x=0,Let the function $f:\mathbb{R}\rightarrow\mathbb{R}$ be differentiable at $x=0$. Prove that $\lim_{x\rightarrow 0}\frac{f(x^2)-f(0)}{x}=0$. The result is pretty obvious to me but I am having a difficult time arguing it precise enough for a proof. What I have so far is of course that since $f$ is differentiable; $$f'(0)=\lim_{x\rightarrow 0}\frac{f(x)-f(0)}{x}$$ exists. Any help would be greatly appreciated.,Let the function $f:\mathbb{R}\rightarrow\mathbb{R}$ be differentiable at $x=0$. Prove that $\lim_{x\rightarrow 0}\frac{f(x^2)-f(0)}{x}=0$. The result is pretty obvious to me but I am having a difficult time arguing it precise enough for a proof. What I have so far is of course that since $f$ is differentiable; $$f'(0)=\lim_{x\rightarrow 0}\frac{f(x)-f(0)}{x}$$ exists. Any help would be greatly appreciated.,,"['real-analysis', 'limits', 'derivatives']"
9,How can I prove that there is a function that is its own derivative? [closed],How can I prove that there is a function that is its own derivative? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question How can I prove that a function that is its own derivative exists? And how can I prove that this function is of the form $a(b^x)$?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question How can I prove that a function that is its own derivative exists? And how can I prove that this function is of the form $a(b^x)$?",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'derivatives']"
10,Why are subsets of compact sets not compact?,Why are subsets of compact sets not compact?,,"So much of the properties of compact sets are motivated by finite sets, to the point that thinking of compact sets as topologically finite sets may yield some deeper understanding. But finite sets have the intuitive property that every subset of a finite set is also compact(also finite), why is it that compact sets give this up? It is easy to state that they just do and provide the example $I=(0,1)$ and $K=[0,1]$ as an example, but the problem with that is it really only helps illuminate how compact sets work in the euclidean spaces. It isn't generally true in all spaces that open sets aren't compact, or that closed and bounded sets are compact. So there is a problem generalizing the ideas. The heart of my question is : Given a subset $E$ of a compact set $K$ why isn't it compact? Simple Answer : Because there is an open cover of $E$ which has no finite subcover The deeper question: Why?","So much of the properties of compact sets are motivated by finite sets, to the point that thinking of compact sets as topologically finite sets may yield some deeper understanding. But finite sets have the intuitive property that every subset of a finite set is also compact(also finite), why is it that compact sets give this up? It is easy to state that they just do and provide the example and as an example, but the problem with that is it really only helps illuminate how compact sets work in the euclidean spaces. It isn't generally true in all spaces that open sets aren't compact, or that closed and bounded sets are compact. So there is a problem generalizing the ideas. The heart of my question is : Given a subset of a compact set why isn't it compact? Simple Answer : Because there is an open cover of which has no finite subcover The deeper question: Why?","I=(0,1) K=[0,1] E K E","['real-analysis', 'general-topology', 'compactness']"
11,Evaluate $\int_0^\infty\frac{1-e^{-x}(1+x )}{x(e^{x}-1)(e^{x}+e^{-x})}dx$,Evaluate,\int_0^\infty\frac{1-e^{-x}(1+x )}{x(e^{x}-1)(e^{x}+e^{-x})}dx,"\begin{equation} \int_0^\infty\frac{1-e^{-x}(1+x )}{x(e^{x}-1)(e^{x}+e^{-x})}dx \end{equation} My colleague got this problem from his friend but he didn't know the answer so he asked my help. Unfortunately, after hours of tired effort I was unable to crack this integral. I was unable to find a way to evaluate it from online search either. I used to be good at solving this kind of problem but now I feel so embarrassed by my stupidity. I'm stuck and I badly need your help. It's a humbling request to ask people here being so kind to help me out. Thank you.","\begin{equation} \int_0^\infty\frac{1-e^{-x}(1+x )}{x(e^{x}-1)(e^{x}+e^{-x})}dx \end{equation} My colleague got this problem from his friend but he didn't know the answer so he asked my help. Unfortunately, after hours of tired effort I was unable to crack this integral. I was unable to find a way to evaluate it from online search either. I used to be good at solving this kind of problem but now I feel so embarrassed by my stupidity. I'm stuck and I badly need your help. It's a humbling request to ask people here being so kind to help me out. Thank you.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
12,How exactly can't $\delta$ depend on $x$ in the definition of uniform continuity?,How exactly can't  depend on  in the definition of uniform continuity?,\delta x,"I'm told that a function defined on an interval $[a,b]$ or $(a,b)$ is uniformly continuous if for each $\epsilon\gt 0$ there exists a $\delta\gt 0$ such that $|x-t|\lt \delta$ implies that $|f(x)-f(t)|\lt \epsilon$. Then it gives a little note saying that $\delta$ cannot depend on $x$, it can only depend on $\epsilon$. With ordinary continuity, the $\delta$ can depend on both $x$ and $\epsilon$. I'm just a little lost on why $|x-t|\lt \delta$ implies $|f(x)-f(t)|\lt \epsilon$, and how $\delta$ can't depend on $x$ but only $\epsilon$.","I'm told that a function defined on an interval $[a,b]$ or $(a,b)$ is uniformly continuous if for each $\epsilon\gt 0$ there exists a $\delta\gt 0$ such that $|x-t|\lt \delta$ implies that $|f(x)-f(t)|\lt \epsilon$. Then it gives a little note saying that $\delta$ cannot depend on $x$, it can only depend on $\epsilon$. With ordinary continuity, the $\delta$ can depend on both $x$ and $\epsilon$. I'm just a little lost on why $|x-t|\lt \delta$ implies $|f(x)-f(t)|\lt \epsilon$, and how $\delta$ can't depend on $x$ but only $\epsilon$.",,"['real-analysis', 'definition', 'continuity']"
13,How to simplify $\int{\sqrt[4]{1-8{{x}^{2}}+8{{x}^{4}}-4x\sqrt{{{x}^{2}}-1}+8{{x}^{3}}\sqrt{{{x}^{2}}-1}}dx}$?,How to simplify ?,\int{\sqrt[4]{1-8{{x}^{2}}+8{{x}^{4}}-4x\sqrt{{{x}^{2}}-1}+8{{x}^{3}}\sqrt{{{x}^{2}}-1}}dx},"I have been asked about the following integral: $$\int{\sqrt[4]{1-8{{x}^{2}}+8{{x}^{4}}-4x\sqrt{{{x}^{2}}-1}+8{{x}^{3}}\sqrt{{{x}^{2}}-1}}dx}$$ I think this is a joke of bad taste . I have tried every elementary method of integration which i know, also i tried integrating using Maple but as i suspected, the integrad doesn't have an anti derivative. Any ideas?","I have been asked about the following integral: $$\int{\sqrt[4]{1-8{{x}^{2}}+8{{x}^{4}}-4x\sqrt{{{x}^{2}}-1}+8{{x}^{3}}\sqrt{{{x}^{2}}-1}}dx}$$ I think this is a joke of bad taste . I have tried every elementary method of integration which i know, also i tried integrating using Maple but as i suspected, the integrad doesn't have an anti derivative. Any ideas?",,"['real-analysis', 'integration']"
14,Why are the rational numbers not continuous?,Why are the rational numbers not continuous?,,"I am trying to understand why the rational numbers are not continuous. Given two rational numbers $a$ and $b$, I can always find a number $c = \frac{a+b}{2}$ between these two numbers. So when I plot the rational numbers as a line, this is a steady line (unlike natural numbers, which are obviously not continuous). Why are they not continuous?","I am trying to understand why the rational numbers are not continuous. Given two rational numbers $a$ and $b$, I can always find a number $c = \frac{a+b}{2}$ between these two numbers. So when I plot the rational numbers as a line, this is a steady line (unlike natural numbers, which are obviously not continuous). Why are they not continuous?",,"['real-analysis', 'rational-numbers']"
15,What's the limit of the sequence $\lim\limits_{n \to\infty} \frac{n!}{n^n}$?,What's the limit of the sequence ?,\lim\limits_{n \to\infty} \frac{n!}{n^n},$$\lim_{n \to\infty} \frac{n!}{n^n}$$ I have a question: is it valid to use Stirling's Formula to prove convergence of the sequence?,$$\lim_{n \to\infty} \frac{n!}{n^n}$$ I have a question: is it valid to use Stirling's Formula to prove convergence of the sequence?,,"['real-analysis', 'sequences-and-series', 'limits', 'exponentiation', 'factorial']"
16,How to prove the Squeeze Theorem for sequences,How to prove the Squeeze Theorem for sequences,,"The formulation I'm looking at goes: If $\lbrace x_n\rbrace$, $\lbrace y_n\rbrace$ and $\lbrace z_n \rbrace$ are sequences such that $x_n \le y_n \le z_n$ for all $n \in \mathbb N$, and $x_n \to l$ and $z_n \to l$ for some $l \in \mathbb R$, then $y_n \to l$ also. So we have to use the definition of convergence to a limit for a sequence:  $$\forall \varepsilon > 0, \space \exists N_\varepsilon \in \mathbb N, \space \forall n \ge N_\varepsilon, \space |a_n - l| < \varepsilon$$ I've been trying to say something like: $|y_n - l| < |x_n - l| + |z_n - l| \le \frac\varepsilon 2 + \frac\varepsilon 2 = \varepsilon$ for every $\varepsilon > 0$, but I'm not sure how to get there or if there may be a better way to prove the theorem. Any help would be greatly appreciated.","The formulation I'm looking at goes: If $\lbrace x_n\rbrace$, $\lbrace y_n\rbrace$ and $\lbrace z_n \rbrace$ are sequences such that $x_n \le y_n \le z_n$ for all $n \in \mathbb N$, and $x_n \to l$ and $z_n \to l$ for some $l \in \mathbb R$, then $y_n \to l$ also. So we have to use the definition of convergence to a limit for a sequence:  $$\forall \varepsilon > 0, \space \exists N_\varepsilon \in \mathbb N, \space \forall n \ge N_\varepsilon, \space |a_n - l| < \varepsilon$$ I've been trying to say something like: $|y_n - l| < |x_n - l| + |z_n - l| \le \frac\varepsilon 2 + \frac\varepsilon 2 = \varepsilon$ for every $\varepsilon > 0$, but I'm not sure how to get there or if there may be a better way to prove the theorem. Any help would be greatly appreciated.",,"['real-analysis', 'limits']"
17,"What does it mean for rational numbers to be ""dense in the reals?""","What does it mean for rational numbers to be ""dense in the reals?""",,"What does it mean for rational numbers to be ""dense in the reals?"" I can't seem to find a decent explanation online...","What does it mean for rational numbers to be ""dense in the reals?"" I can't seem to find a decent explanation online...",,['real-analysis']
18,Integral $\int_0^{\pi/4} \frac{\ln \tan x}{\cos 2x} dx=-\frac{\pi^2}{8}.$,Integral,\int_0^{\pi/4} \frac{\ln \tan x}{\cos 2x} dx=-\frac{\pi^2}{8}.,"$$I:=\int_0^{\pi/4} \frac{\ln \tan x}{\cos 2x} dx=-\frac{\pi^2}{8}.$$  I am trying to see nice solutions to this integral.  I tried the following $$ I=\int_0^{\pi/4}\frac{\ln \sin x}{\cos 2x} dx-\int_0^{\pi/4} \frac{\ln \cos x }{\cos 2x}dx $$ but am not sure how to work with this denominator of $\cos 2x$.  If this helps: $$ \int_0^{\pi/4}\log \sin x \, dx=-\frac{1}{4}\big(2K+\pi \ln 2\big) $$ where K is the Catalan constant (note I am using Borwein convention not mathematica of using a C to define this constant.)  It is given by $$ K=\sum_{k=0}^\infty \frac{(-1)^k}{(2k+1)^2}=\beta(2)  $$ where $\beta(2)$ is the  Dirichlet beta function. However I cannot solve this integral either.  Thanks","$$I:=\int_0^{\pi/4} \frac{\ln \tan x}{\cos 2x} dx=-\frac{\pi^2}{8}.$$  I am trying to see nice solutions to this integral.  I tried the following $$ I=\int_0^{\pi/4}\frac{\ln \sin x}{\cos 2x} dx-\int_0^{\pi/4} \frac{\ln \cos x }{\cos 2x}dx $$ but am not sure how to work with this denominator of $\cos 2x$.  If this helps: $$ \int_0^{\pi/4}\log \sin x \, dx=-\frac{1}{4}\big(2K+\pi \ln 2\big) $$ where K is the Catalan constant (note I am using Borwein convention not mathematica of using a C to define this constant.)  It is given by $$ K=\sum_{k=0}^\infty \frac{(-1)^k}{(2k+1)^2}=\beta(2)  $$ where $\beta(2)$ is the  Dirichlet beta function. However I cannot solve this integral either.  Thanks",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
19,"Problems understanding proof of if $x + y = x + z$ then $y = z$ (Baby Rudin, Chapter 1, Proposition 1.14)","Problems understanding proof of if  then  (Baby Rudin, Chapter 1, Proposition 1.14)",x + y = x + z y = z,I'm having trouble with whether Rudin actually proves what he's tried to prove. Proposition 1.14; (page 6) The axioms of addition imply the following statements: a) if $x + y = x + z$ then $y = z$ The author's proof is as follows: $ y = (0 + y) = (x + -x) + y = -x + (x + \textbf{y})$ $$ = -x + (x + \textbf{z}) = (-x + x) + z = (0 + z) = z $$ I emphased the section which troubles me.  How does Rudin prove that $ y = z $ if he substituted $y = z$?,I'm having trouble with whether Rudin actually proves what he's tried to prove. Proposition 1.14; (page 6) The axioms of addition imply the following statements: a) if $x + y = x + z$ then $y = z$ The author's proof is as follows: $ y = (0 + y) = (x + -x) + y = -x + (x + \textbf{y})$ $$ = -x + (x + \textbf{z}) = (-x + x) + z = (0 + z) = z $$ I emphased the section which troubles me.  How does Rudin prove that $ y = z $ if he substituted $y = z$?,,"['real-analysis', 'proof-writing', 'proof-explanation']"
20,"Why do the reals need to be constructed? Do they somehow ""span"" the rationals, the roots, and the transcendentals like e and pi?","Why do the reals need to be constructed? Do they somehow ""span"" the rationals, the roots, and the transcendentals like e and pi?",,"Here is my question.  Why do the reals need to be ""constructed"" by this bizarre ""Dedekind cut"" or ""equivalence class of Cauchy sequences"" argument?  Why can't they simply be ""observed"" as consisting of all numbers that ""span"" some known sets of numbers? I am thinking here, in part, by analogy with linear algebra and with the complex numbers, where $i$, the square root of $-1$, is really all you need, in addition to the reals, to get all complex numbers as a spanning set of $1$ and $i$ over $\mathbb R$.  (Every complex number may be expressed as $a\cdot 1 + b\cdot i$ where $a, b\in\mathbb R$.) We have a couple of known transcendental numbers, $e$ and $\pi$.  We have all the rationals.  We have all the square roots most of which are irrational.  We have all the fractional roots of $e$ and $\pi$. We have the $e$th roots of all the numbers that exist, and the $\pi$th roots. Maybe we also have some other sets of transcendental numbers out there that we can use? What I am trying to ask is, are we using these ""Dedekind cuts"" and ""equivalence classes of Cauchy sequences"" just because we don't ""know enough real numbers yet"", because their characterization hasn't occurred to us yet, or do we already have enough real numbers in our arsenal, like e and pi, to make a ""spanning set"" without using equivalence classes of infinite sequences and the like, or, is it the case that we really have to use these kinds of constructions of the reals, for some deep mathematical reason? It just doesn't seem right.  Because you have to admit, by identifying ""sets of numbers"" like Dedekind cuts and equivalent classes of Cauchy sequences which are both sets of numbers, with actual numbers , mathematicians create (at least in my mind) some cause for doubt about what they are doing here with the reals.  A ""set"" seems like a strangely undefined term, which I understand, but not well, is subject to various kinds of paradoxes and levels of analysis problems.  (This last paragraph may be more of a separate question, about the validity of using sets of numbers as numbers, from the first question, which is more about why aren't there simpler ways to define or understand the real numbers in terms of numbers and operations we already understand.)","Here is my question.  Why do the reals need to be ""constructed"" by this bizarre ""Dedekind cut"" or ""equivalence class of Cauchy sequences"" argument?  Why can't they simply be ""observed"" as consisting of all numbers that ""span"" some known sets of numbers? I am thinking here, in part, by analogy with linear algebra and with the complex numbers, where $i$, the square root of $-1$, is really all you need, in addition to the reals, to get all complex numbers as a spanning set of $1$ and $i$ over $\mathbb R$.  (Every complex number may be expressed as $a\cdot 1 + b\cdot i$ where $a, b\in\mathbb R$.) We have a couple of known transcendental numbers, $e$ and $\pi$.  We have all the rationals.  We have all the square roots most of which are irrational.  We have all the fractional roots of $e$ and $\pi$. We have the $e$th roots of all the numbers that exist, and the $\pi$th roots. Maybe we also have some other sets of transcendental numbers out there that we can use? What I am trying to ask is, are we using these ""Dedekind cuts"" and ""equivalence classes of Cauchy sequences"" just because we don't ""know enough real numbers yet"", because their characterization hasn't occurred to us yet, or do we already have enough real numbers in our arsenal, like e and pi, to make a ""spanning set"" without using equivalence classes of infinite sequences and the like, or, is it the case that we really have to use these kinds of constructions of the reals, for some deep mathematical reason? It just doesn't seem right.  Because you have to admit, by identifying ""sets of numbers"" like Dedekind cuts and equivalent classes of Cauchy sequences which are both sets of numbers, with actual numbers , mathematicians create (at least in my mind) some cause for doubt about what they are doing here with the reals.  A ""set"" seems like a strangely undefined term, which I understand, but not well, is subject to various kinds of paradoxes and levels of analysis problems.  (This last paragraph may be more of a separate question, about the validity of using sets of numbers as numbers, from the first question, which is more about why aren't there simpler ways to define or understand the real numbers in terms of numbers and operations we already understand.)",,"['real-analysis', 'analysis']"
21,Hyper Derivative definition.,Hyper Derivative definition.,,Following some thoughts in real analysis and in analogy with the classical definition of the derivative $$ f^\prime(x) = \lim_{h\rightarrow 0}{f(x+h)-f(x)\over h} $$ I considered the following hyper-derivative definition: $$ f^\nabla(x) = \lim_{h\rightarrow 1}\log_h{f(x\cdot h)\over f(x)} $$ It is easily proved then that $$ (f\cdot g)^\nabla = f^\nabla + g^\nabla $$ I would like please if someone can give some expert opinion in the following: Is the hyper derivative definition well defined? or somedody can see a potential problem? Can someone provide a geometric interpretation for the hyper derivative definition?,Following some thoughts in real analysis and in analogy with the classical definition of the derivative $$ f^\prime(x) = \lim_{h\rightarrow 0}{f(x+h)-f(x)\over h} $$ I considered the following hyper-derivative definition: $$ f^\nabla(x) = \lim_{h\rightarrow 1}\log_h{f(x\cdot h)\over f(x)} $$ It is easily proved then that $$ (f\cdot g)^\nabla = f^\nabla + g^\nabla $$ I would like please if someone can give some expert opinion in the following: Is the hyper derivative definition well defined? or somedody can see a potential problem? Can someone provide a geometric interpretation for the hyper derivative definition?,,"['calculus', 'real-analysis', 'complex-analysis']"
22,How would one go about proving that the rationals are not the countable intersection of open sets?,How would one go about proving that the rationals are not the countable intersection of open sets?,,"I'm trying to prove that the rationals are not the countable intersection of open sets, but I still can't understand why $$\bigcap_{n \in \mathbf{N}} \left\{\left(q - \frac 1n, q + \frac 1n\right) : q \in \mathbf{Q}\right\}$$ isn't a counter-example. Any ideas? Thanks!","I'm trying to prove that the rationals are not the countable intersection of open sets, but I still can't understand why $$\bigcap_{n \in \mathbf{N}} \left\{\left(q - \frac 1n, q + \frac 1n\right) : q \in \mathbf{Q}\right\}$$ isn't a counter-example. Any ideas? Thanks!",,['real-analysis']
23,Second derivative of $\det(\mathbb{1}+tA)$?,Second derivative of ?,\det(\mathbb{1}+tA),"I am given a function $F : \mathbb{R} \to \mathbb{R}$ defined by $$F(t)=\det(\mathbb{1}+tA)$$ where $A \in \mathbb{R}^{n \times n}$. As far as I know, the following is true. $$\frac{d}{dt}\bigg|_{t=0} F(t) = \text{tr}~ A$$ However, how to find the second derivative? $$\frac{d^2}{dt^2}\bigg|_{t=0} F(t)$$","I am given a function $F : \mathbb{R} \to \mathbb{R}$ defined by $$F(t)=\det(\mathbb{1}+tA)$$ where $A \in \mathbb{R}^{n \times n}$. As far as I know, the following is true. $$\frac{d}{dt}\bigg|_{t=0} F(t) = \text{tr}~ A$$ However, how to find the second derivative? $$\frac{d^2}{dt^2}\bigg|_{t=0} F(t)$$",,"['real-analysis', 'linear-algebra', 'derivatives', 'determinant', 'matrix-calculus']"
24,"If $f(x)\to 0$ as $x\to\infty$ and $f''$ is bounded, show that $f'(x)\to0$ as $x\to\infty$","If  as  and  is bounded, show that  as",f(x)\to 0 x\to\infty f'' f'(x)\to0 x\to\infty,Let $f\colon\mathbb R\to\mathbb R$ be twice differentiable with $f(x)\to 0$ as $x\to\infty$ and $f''$ bounded. Show that $f'(x)\to0$ as $x\to\infty$. (This is inspired by a comment/answer to a different question),Let $f\colon\mathbb R\to\mathbb R$ be twice differentiable with $f(x)\to 0$ as $x\to\infty$ and $f''$ bounded. Show that $f'(x)\to0$ as $x\to\infty$. (This is inspired by a comment/answer to a different question),,['real-analysis']
25,"Function on $[a,b]$ that satisfies a Hölder condition of order $\alpha > 1 $ is constant",Function on  that satisfies a Hölder condition of order  is constant,"[a,b] \alpha > 1 ","I want to show that if a function $f:[a,b]\rightarrow \mathbb R$ satisfies a Hölder condition of order $\alpha > 1 $ then it is constant. The way I think of it is as follows: $$|f(x) - f(y)| < K|x-y|^\alpha$$ $$\frac{|f(x) - f(y)|} {|x-y]}  < K|x-y|^{\alpha -1}$$ $$\lim_{y\rightarrow x} \frac{|f(x) - f(y)|} {|x-y]} \le \lim_{y\rightarrow x}  K|x-y|^{\alpha -1} =0 $$ As the limit is $0$, we can remove the modulus, so we get: $$\lim_{y\rightarrow x} \frac{f(x) - f(y)} {x-y} = 0$$ So $f$ is derivable and $f'(x) = 0$ for all $x$ in $[a,b]$. Note that the reason we can add the limit $y\rightarrow x$ is because $[a,b]$ is closed in $\mathbb R$. However, the question gives as a hint using the mean value theorem. I am not sure why one should do that. You would first have to prove that $f$ is derivable in a similar manner to what I did, and then prove that $f$ is constant. Or is there a simpler way to do it and I am missing it? Also please inform me of any mistakes I did in the proof (if any)/ Thank you!","I want to show that if a function $f:[a,b]\rightarrow \mathbb R$ satisfies a Hölder condition of order $\alpha > 1 $ then it is constant. The way I think of it is as follows: $$|f(x) - f(y)| < K|x-y|^\alpha$$ $$\frac{|f(x) - f(y)|} {|x-y]}  < K|x-y|^{\alpha -1}$$ $$\lim_{y\rightarrow x} \frac{|f(x) - f(y)|} {|x-y]} \le \lim_{y\rightarrow x}  K|x-y|^{\alpha -1} =0 $$ As the limit is $0$, we can remove the modulus, so we get: $$\lim_{y\rightarrow x} \frac{f(x) - f(y)} {x-y} = 0$$ So $f$ is derivable and $f'(x) = 0$ for all $x$ in $[a,b]$. Note that the reason we can add the limit $y\rightarrow x$ is because $[a,b]$ is closed in $\mathbb R$. However, the question gives as a hint using the mean value theorem. I am not sure why one should do that. You would first have to prove that $f$ is derivable in a similar manner to what I did, and then prove that $f$ is constant. Or is there a simpler way to do it and I am missing it? Also please inform me of any mistakes I did in the proof (if any)/ Thank you!",,"['real-analysis', 'holder-spaces']"
26,What is the difference between real and complex manifolds?,What is the difference between real and complex manifolds?,,"Given an even-dimensional (smooth) manifold, what is the difference between its (real) smooth structure and its complex structure? I realize that in the real case, the overlap functions of charts need only be smooth, while in the complex case they need to be holomorphic. However, this doesn't provide a satisfactory answer- it begs the question of why holomorphicity is a stronger condition than smoothness in the first case. The answer to which is just that ""in the real case, limits can only go from either side, whereas in the complex case they can be taken from all sorts of directions""- but this doesn't not seem very rigorous. Any thoughts on what is really at work here?","Given an even-dimensional (smooth) manifold, what is the difference between its (real) smooth structure and its complex structure? I realize that in the real case, the overlap functions of charts need only be smooth, while in the complex case they need to be holomorphic. However, this doesn't provide a satisfactory answer- it begs the question of why holomorphicity is a stronger condition than smoothness in the first case. The answer to which is just that ""in the real case, limits can only go from either side, whereas in the complex case they can be taken from all sorts of directions""- but this doesn't not seem very rigorous. Any thoughts on what is really at work here?",,"['real-analysis', 'complex-analysis', 'differential-geometry']"
27,Converse of mean value theorem,Converse of mean value theorem,,"I am wondering if the following converse (or modification) of the mean value theorem holds. Suppose $f(\cdot)$ is continuously differentiable on $[a,b]$. Then for all $c \in (a,b)$ there exists $x$ and $y$ such that $$ f'(c)=\frac{f(y)-f(x)}{y-x} $$","I am wondering if the following converse (or modification) of the mean value theorem holds. Suppose $f(\cdot)$ is continuously differentiable on $[a,b]$. Then for all $c \in (a,b)$ there exists $x$ and $y$ such that $$ f'(c)=\frac{f(y)-f(x)}{y-x} $$",,"['real-analysis', 'derivatives', 'mean-value-theorem']"
28,"How to integrate $\int \frac{1}{\sin^4x + \cos^4 x} \,dx$?",How to integrate ?,"\int \frac{1}{\sin^4x + \cos^4 x} \,dx","How to integrate    $$\int \frac{1}{\sin^4x + \cos^4 x} \,dx$$ I tried the following approach: $$\int \frac{1}{\sin^4x + \cos^4 x} \,dx = \int \frac{1}{\sin^4x + (1-\sin^2x)^2} \,dx = \int \frac{1}{\sin^4x + 1- 2\sin^2x + \sin^4x} \,dx \\ = \frac{1}{2}\int \frac{1}{\sin^4x - \sin^2x + \frac{1}{2}} \,dx = \frac{1}{2}\int \frac{1}{(\sin^2x - \frac{1}{2})^2 + \frac{1}{4}} \,dx$$ The substitution $t = \tan\frac{x}{2}$ yields 4th degree polynomials and a $\sin$ substitution would produce polynomials and expressions with square roots while Wolfram Alpha's solution doesn't look that complicated. Another approach: $\sin^4x + \cos^4 x = (\sin^2 x + \cos^2x)(\sin^2 x + \cos^2 x) - 2\sin^2 x\cos^2 x = 1 - 2\sin^2 x\cos^2 x  = (1-\sqrt2\sin x \cos x)(1+\sqrt2\sin x \cos x)$ and then I tried substituting: $t = \sin x \cos x$ and got  $$\int\frac{t\,dt}{2(1-2t^2)\sqrt{1-4t^2}}$$ Another way would maybe be to make two integrals: $$\int \frac{1}{\sin^4x + \cos^4 x} \,dx = \int \frac{1}{(1-\sqrt2\sin x \cos x)(1+\sqrt2\sin x \cos x)} \,dx = \\ \frac{1}{2}\int \frac{1}{1-\sqrt2\sin x \cos x} \,dx +  \frac{1}{2}\int\frac{1}{1+\sqrt2\sin x \cos x} \,dx$$ ... and again I tried $t = \tan\frac{x}{2}$ (4th degree polynomial) and $t=\sqrt2 \sin x \cos x$ and I get $\frac{\sqrt 2}{2} \int \frac{\,dt}{(1-t)\sqrt{1-2t^2}}$ for the first one. Any hints?","How to integrate    $$\int \frac{1}{\sin^4x + \cos^4 x} \,dx$$ I tried the following approach: $$\int \frac{1}{\sin^4x + \cos^4 x} \,dx = \int \frac{1}{\sin^4x + (1-\sin^2x)^2} \,dx = \int \frac{1}{\sin^4x + 1- 2\sin^2x + \sin^4x} \,dx \\ = \frac{1}{2}\int \frac{1}{\sin^4x - \sin^2x + \frac{1}{2}} \,dx = \frac{1}{2}\int \frac{1}{(\sin^2x - \frac{1}{2})^2 + \frac{1}{4}} \,dx$$ The substitution $t = \tan\frac{x}{2}$ yields 4th degree polynomials and a $\sin$ substitution would produce polynomials and expressions with square roots while Wolfram Alpha's solution doesn't look that complicated. Another approach: $\sin^4x + \cos^4 x = (\sin^2 x + \cos^2x)(\sin^2 x + \cos^2 x) - 2\sin^2 x\cos^2 x = 1 - 2\sin^2 x\cos^2 x  = (1-\sqrt2\sin x \cos x)(1+\sqrt2\sin x \cos x)$ and then I tried substituting: $t = \sin x \cos x$ and got  $$\int\frac{t\,dt}{2(1-2t^2)\sqrt{1-4t^2}}$$ Another way would maybe be to make two integrals: $$\int \frac{1}{\sin^4x + \cos^4 x} \,dx = \int \frac{1}{(1-\sqrt2\sin x \cos x)(1+\sqrt2\sin x \cos x)} \,dx = \\ \frac{1}{2}\int \frac{1}{1-\sqrt2\sin x \cos x} \,dx +  \frac{1}{2}\int\frac{1}{1+\sqrt2\sin x \cos x} \,dx$$ ... and again I tried $t = \tan\frac{x}{2}$ (4th degree polynomial) and $t=\sqrt2 \sin x \cos x$ and I get $\frac{\sqrt 2}{2} \int \frac{\,dt}{(1-t)\sqrt{1-2t^2}}$ for the first one. Any hints?",,"['calculus', 'real-analysis', 'integration', 'indefinite-integrals']"
29,Putnam Exam Integral: $\lim_{n\to \infty} \int_0^1 \int_0^1...\int_0^1 \cos^2\big(\frac{\pi}{2n}(x_1+x_2+...x_n)\big)dx_1 dx_2...dx_n$,Putnam Exam Integral:,\lim_{n\to \infty} \int_0^1 \int_0^1...\int_0^1 \cos^2\big(\frac{\pi}{2n}(x_1+x_2+...x_n)\big)dx_1 dx_2...dx_n,I am trying to evaluate$$ \lim_{n\to \infty} \int_0^1 \int_0^1...\int_0^1 \cos^2\big(\frac{\pi}{2n}(x_1+x_2+...x_n)\big)dx_1 dx_2...dx_n. $$ This is from an old Putnam mathematics competition. Either 1965 or 1987 I forget. Should we re-write the $\cos^2$ term first or how should we approach it?  Thanks,I am trying to evaluate$$ \lim_{n\to \infty} \int_0^1 \int_0^1...\int_0^1 \cos^2\big(\frac{\pi}{2n}(x_1+x_2+...x_n)\big)dx_1 dx_2...dx_n. $$ This is from an old Putnam mathematics competition. Either 1965 or 1987 I forget. Should we re-write the $\cos^2$ term first or how should we approach it?  Thanks,,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'contest-math']"
30,definition of limsup of a function,definition of limsup of a function,,I already have some idea on what $\limsup_{x\rightarrow\infty} f(x)=\infty$ means but I would like to hear other ideas from the math stack exchange community. Maybe some intuition would be helpful.,I already have some idea on what means but I would like to hear other ideas from the math stack exchange community. Maybe some intuition would be helpful.,\limsup_{x\rightarrow\infty} f(x)=\infty,"['real-analysis', 'limits', 'functions']"
31,Compute the limit of $\frac1{\sqrt{n}}\left(1^1 \cdot 2^2 \cdot3^3\cdots n^n\right)^{1/n^2}$,Compute the limit of,\frac1{\sqrt{n}}\left(1^1 \cdot 2^2 \cdot3^3\cdots n^n\right)^{1/n^2},Compute the following limit: $$\lim_{n\to\infty}\frac{{\left(1^1 \cdot 2^2 \cdot3^3\cdots n^n\right)}^\frac{1}{n^2}}{\sqrt{n}}  $$ I'm interested in almost any approaching way for this limit. Thanks.,Compute the following limit: $$\lim_{n\to\infty}\frac{{\left(1^1 \cdot 2^2 \cdot3^3\cdots n^n\right)}^\frac{1}{n^2}}{\sqrt{n}}  $$ I'm interested in almost any approaching way for this limit. Thanks.,,"['calculus', 'real-analysis', 'limits']"
32,Is the AM-GM inequality the only obstruction for getting a specific sum and product?,Is the AM-GM inequality the only obstruction for getting a specific sum and product?,,"This might be silly, but here it goes. Let $P,S>0$ be positive real numbers that satisfy $\frac{S}{n} \ge \sqrt[n]{P}$ . Does there exist a sequence of positive real numbers $a_1,\dots,a_n$ such that $S=\sum a_i,P=\prod a_i$ ? Clearly, $\frac{S}{n} \ge \sqrt[n]{P}$ is a necessary condition, due to the AM-GM inequality. But is it sufficient? For $n=2$ , the answer is positive, as can be seen by analysing the discriminant of the associated quadratic equation. (In fact, the solvability criterion for the quadratic, namely- the non-negativity of the discriminant, is equivalent to the AM-GM inequality for the sum and the product). What about $n>3$ ?","This might be silly, but here it goes. Let be positive real numbers that satisfy . Does there exist a sequence of positive real numbers such that ? Clearly, is a necessary condition, due to the AM-GM inequality. But is it sufficient? For , the answer is positive, as can be seen by analysing the discriminant of the associated quadratic equation. (In fact, the solvability criterion for the quadratic, namely- the non-negativity of the discriminant, is equivalent to the AM-GM inequality for the sum and the product). What about ?","P,S>0 \frac{S}{n} \ge \sqrt[n]{P} a_1,\dots,a_n S=\sum a_i,P=\prod a_i \frac{S}{n} \ge \sqrt[n]{P} n=2 n>3","['real-analysis', 'polynomials', 'systems-of-equations', 'a.m.-g.m.-inequality']"
33,Integral representation of Euler's constant,Integral representation of Euler's constant,,"Prove that : $$ \gamma=-\int_0^{1}\ln \ln \left ( \frac{1}{x} \right) \ \mathrm{d}x.$$ where $\gamma$ is Euler's constant ($\gamma \approx 0.57721$). This integral was mentioned in Wikipedia as in Mathworld , but the solutions I've got uses corollaries from this theorem . Can you give me a simple solution (not using much advanced theorems) or at least some hints.","Prove that : $$ \gamma=-\int_0^{1}\ln \ln \left ( \frac{1}{x} \right) \ \mathrm{d}x.$$ where $\gamma$ is Euler's constant ($\gamma \approx 0.57721$). This integral was mentioned in Wikipedia as in Mathworld , but the solutions I've got uses corollaries from this theorem . Can you give me a simple solution (not using much advanced theorems) or at least some hints.",,"['real-analysis', 'integration', 'definite-integrals', 'euler-mascheroni-constant']"
34,Does a polynomial that's bounded below have a global minimum?,Does a polynomial that's bounded below have a global minimum?,,"Must a polynomial function $f \in \mathbb{R}[x_1, \ldots, x_n]$ that's lower bounded by some $\lambda \in \mathbb{R}$ have a global minimum over $\mathbb{R}^n$?","Must a polynomial function $f \in \mathbb{R}[x_1, \ldots, x_n]$ that's lower bounded by some $\lambda \in \mathbb{R}$ have a global minimum over $\mathbb{R}^n$?",,"['real-analysis', 'multivariable-calculus', 'polynomials', 'maxima-minima']"
35,Calculation of $\int_{0}^{\frac{\pi}{4}}\tan^{-1}\sqrt{\frac{\cos 2x }{2 \cos^2 x}}dx$,Calculation of,\int_{0}^{\frac{\pi}{4}}\tan^{-1}\sqrt{\frac{\cos 2x }{2 \cos^2 x}}dx,Calculate $$ \int_{0}^{\frac{\pi}{4}}\tan^{-1}\sqrt{\frac{\cos 2x }{2 \cos^2 x}}dx$$ $\bf{My\; Try::}$ Let $\displaystyle I = \int_{0}^{\frac{\pi}{4}}\tan^{-1}\sqrt{\frac{\cos 2x }{2\cos^2 x}}dx = \int_{0}^{\frac{\pi}{4}}\frac{\pi}{2}-\int_{0}^{\frac{\pi}{4}}\cot^{-1}\sqrt{\frac{\cos 2x}{2\cos^2 x}}dx$ Using The formula $\displaystyle \tan^{-1}(x)+\cot^{-1}(x) = \frac{\pi}{2}\Rightarrow \tan^{-1}(x) = \frac{\pi}{2}-\cot^{-1}(x).$ Now Let $\displaystyle J = \int_{0}^{\frac{\pi}{4}}\cot^{-1}\sqrt{\frac{\cos 2x}{2\cos^2 x}}dx = \int_{0}^{\frac{\pi}{4}}\cot^{-1}\sqrt{\frac{\cos^2 x-\sin^2 x}{2\cos^2 x}}dx = \int_{0}^{\frac{\pi}{4}}\cot^{-1}\sqrt{\frac{1}{2}-\frac{\tan^2 x}{2}}dx$ Now How can I solve after that? Help required. Thanks,Calculate $$ \int_{0}^{\frac{\pi}{4}}\tan^{-1}\sqrt{\frac{\cos 2x }{2 \cos^2 x}}dx$$ $\bf{My\; Try::}$ Let $\displaystyle I = \int_{0}^{\frac{\pi}{4}}\tan^{-1}\sqrt{\frac{\cos 2x }{2\cos^2 x}}dx = \int_{0}^{\frac{\pi}{4}}\frac{\pi}{2}-\int_{0}^{\frac{\pi}{4}}\cot^{-1}\sqrt{\frac{\cos 2x}{2\cos^2 x}}dx$ Using The formula $\displaystyle \tan^{-1}(x)+\cot^{-1}(x) = \frac{\pi}{2}\Rightarrow \tan^{-1}(x) = \frac{\pi}{2}-\cot^{-1}(x).$ Now Let $\displaystyle J = \int_{0}^{\frac{\pi}{4}}\cot^{-1}\sqrt{\frac{\cos 2x}{2\cos^2 x}}dx = \int_{0}^{\frac{\pi}{4}}\cot^{-1}\sqrt{\frac{\cos^2 x-\sin^2 x}{2\cos^2 x}}dx = \int_{0}^{\frac{\pi}{4}}\cot^{-1}\sqrt{\frac{1}{2}-\frac{\tan^2 x}{2}}dx$ Now How can I solve after that? Help required. Thanks,,"['calculus', 'real-analysis', 'integration', 'trigonometry', 'definite-integrals']"
36,product of two uniformly continuous functions is uniformly continuous,product of two uniformly continuous functions is uniformly continuous,,"Suppose that $f$ and $g$ are uniformly continuous functions defined on $(a,b)$. Prove that $fg$ is also uniformly continuous on $(a,b)$. My attempt: Since $f$ is uniformly continuous on $(a,b)$, for all $\epsilon>0$, we have $\delta_f(\epsilon)>0$ such that for all $x,y \in (a,b)$, $|x-y|<\delta_f$, $|f(x)-f(y)|<\epsilon$ Since $g$ is uniformly continuous on $(a,b)$, for all $\epsilon>0$, we have $\delta_g(\epsilon)>0$ such that for all $x,y \in (a,b)$, $|x-y|<\delta_g$, $|g(x)-g(y)|<\epsilon$ Notice that $$|f(x)g(x)-f(y)g(y)|=|f(x)g(x)-f(x)g(y)+f(x)g(y)-f(y)g(y)| \leq |f(x)||g(x)-g(y)| + |g(y)||f(x)-f(y)|$$ Here I don't know how to bound $|f(x)|$ and $|g(y)|$. I have proven that uniformly continuous functions preserve boundedness of an interval , i.e. $f$ is bounded on $(a,b)$. Can anyone help me?","Suppose that $f$ and $g$ are uniformly continuous functions defined on $(a,b)$. Prove that $fg$ is also uniformly continuous on $(a,b)$. My attempt: Since $f$ is uniformly continuous on $(a,b)$, for all $\epsilon>0$, we have $\delta_f(\epsilon)>0$ such that for all $x,y \in (a,b)$, $|x-y|<\delta_f$, $|f(x)-f(y)|<\epsilon$ Since $g$ is uniformly continuous on $(a,b)$, for all $\epsilon>0$, we have $\delta_g(\epsilon)>0$ such that for all $x,y \in (a,b)$, $|x-y|<\delta_g$, $|g(x)-g(y)|<\epsilon$ Notice that $$|f(x)g(x)-f(y)g(y)|=|f(x)g(x)-f(x)g(y)+f(x)g(y)-f(y)g(y)| \leq |f(x)||g(x)-g(y)| + |g(y)||f(x)-f(y)|$$ Here I don't know how to bound $|f(x)|$ and $|g(y)|$. I have proven that uniformly continuous functions preserve boundedness of an interval , i.e. $f$ is bounded on $(a,b)$. Can anyone help me?",,"['real-analysis', 'uniform-continuity']"
37,Challenging integral: Evaluate $\int_0^1\frac{\ln^3(1-x)\operatorname{Li}_3(x)}{x}dx$,Challenging integral: Evaluate,\int_0^1\frac{\ln^3(1-x)\operatorname{Li}_3(x)}{x}dx,"How to evaluate $$I=\int_0^1\frac{\ln^3(1-x)\operatorname{Li}_3(x)}{x}dx\ ?$$ I came across this integral $I$ while I was trying to compute two advanced sums of weight 7. The problem with my approach is that when I tried to evaluate $I_5$ (shown below), the main integral $I$ appeared there which cancels out from both sides, so any idea how to evaluate $I_5$ or $I$ ? Thanks. Here is my trial: Using the two generalized integral expressions of the polylogrithmic function which can be found in the book (Almost) Impossible Integrals, Sums and series page 4. $$\int_0^1\frac{x\ln^n(u)}{1-xu}du=(-1)^n n!\operatorname{Li}_{n+1} (x)\Longrightarrow \operatorname{Li}_{3}(x)=\frac12\int_0^1\frac{x\ln^2(u)}{1-xu}du\tag{1}$$ $$\small{u\int_0^1\frac{\ln^n(x)}{1-u+ux}dx=(-1)^{n-1}n!\operatorname{Li}_{n+1}\left(\frac{u}{u-1}\right)\Longrightarrow\int_0^1\frac{\ln^3x}{1-u+ux}dx=\frac6u\operatorname{Li}_{3}\left(\frac{u}{u-1}\right)}\tag{2}$$ We have \begin{align} I&=\int_0^1\frac{\ln^3(1-x)\operatorname{Li}_3(x)}{x}dx\overset{\text{use} (1)}{=}\frac12\int_0^1\frac{\ln^3(1-x)}{x}\left(\int_0^1\frac{x\ln^2u}{1-xu}du\right)dx\\ &=\frac12\int_0^1\ln^2u\left(\frac{\ln^3(1-x)}{1-xu}dx\right)\ du\overset{1-x\ \mapsto\ x}{=}\frac12\int_0^1\ln^2u\left(\int_0^1\frac{\ln^3x}{1-u+ux}dx\right)\ du\\ &\overset{\text{use}\ (2)}{=}3\int_0^1\frac{\ln^2u}{u}\operatorname{Li}_4\left(\frac{u}{u-1}\right)du\overset{IBP}{=}-\int_0^1\frac{\ln^3u}{u(1-u)}\operatorname{Li}_3\left(\frac{u}{u-1}\right)du \end{align} Now we need the trilogarithmic identity: $$\operatorname{Li}_3\left(\frac{x-1}{x}\right)=\zeta(2)\ln x-\frac12\ln^2x\ln(1-x)+\frac16\ln^3x-\operatorname{Li}_3(1-x)-\operatorname{Li}_3(x)+\zeta(3)$$ set $1-x=u$ to get $$\small{\operatorname{Li}_3\left(\frac{u}{u-1}\right)=\zeta(2)\ln(1-u)-\frac12\ln^2(1-u)\ln u+\frac16\ln^3(1-u)-\operatorname{Li}_3(u)-\operatorname{Li}_3(1-u)+\zeta(3)}$$ Going back to our integral \begin{align} I&=\small{-\int_0^1\frac{\ln^3u}{u(1-u)}\left(\zeta(2)\ln(1-u)-\frac12\ln^2(1-u)\ln x+\frac16\ln^3(1-u)-\operatorname{Li}_3(u)-\operatorname{Li}_3(1-u)+\zeta(3)\right)du}\\ &=-\zeta(2)\underbrace{\int_0^1\frac{\ln^3u\ln(1-u)}{u(1-u)}du}_{\Large I_1}+\frac12\underbrace{\int_0^1\frac{\ln^4u\ln^2(1-u)}{u(1-u)}du}_{\Large I_2}-\frac16\underbrace{\int_0^1\frac{\ln^3u\ln^3(1-u)}{u(1-u)}du}_{\Large I_3}\\ &\quad+\underbrace{\int_0^1\frac{\ln^3u\operatorname{Li}_3(u)}{u(1-u)}\ du}_{\Large I_4}+\underbrace{\int_0^1\frac{\ln^3u}{u(1-u)}\left(\operatorname{Li}_3(1-u)-\zeta(3)\right)du}_{\Large I_5} \end{align} \begin{align} I_1=\int_0^1\frac{\ln^3u\ln(1-u)}{u(1-u)}du=-\sum_{n=1}^\infty H_n\int_0^1 u^{n-1}\ln^3udu=6\sum_{n=1}^\infty\frac{H_n}{n^4} \end{align} . \begin{align} I_2&=\int_0^1\frac{\ln^4u\ln^2(1-u)}{u(1-u)}du=\sum_{n=1}^\infty\left(H_n^2-H_n^{(2)}\right)\int_0^1 u^{n-1}\ln^4udu\\ &=24\sum_{n=1}^\infty\frac{H_n^2-H_n^{(2)}}{n^5}=24\sum_{n=1}^\infty\frac{H_n^2}{n^5}-24\sum_{n=1}^\infty\frac{H_n^{(2)}}{n^5} \end{align} \begin{align} I_3&=\int_0^1\frac{\ln^3u\ln^3(1-u)}{u(1-u)}du=\int_0^1\frac{\ln^3u\ln^3(1-u)}{u}du+\underbrace{\int_0^1\frac{\ln^3u\ln^3(1-u)}{1-u}du}_{1-x\ \mapsto\ x}\\ &=2\int_0^1\frac{\ln^3u\ln^3(1-u)}{u}\ du\overset{IBP}{=}\frac32\int_0^1\frac{\ln^4u\ln^2(1-u)}{1-u}du\\ &=\frac32\sum_{n=1}^\infty\left(H_n^2-H_n^{(2)}\right)\int_0^1 u^n\ln^4udu, \quad \text{reindex}\\ &=\frac32\sum_{n=1}^\infty\left(H_n^2-H_n^{(2)}-\frac{2H_n}{n}+\frac2{n^2}\right)\int_0^1 u^{n-1}\ln^4u du\\ &=\frac32\sum_{n=1}^\infty\left(H_n^2-H_n^{(2)}-\frac{2H_n}{n}+\frac2{n^2}\right)\left(\frac{24}{n^5}\right)\\ &=36\sum_{n=1}^\infty\frac{H_n^2}{n^5}-36\sum_{n=1}^\infty\frac{H_n^{(2)}}{n^5}-72\sum_{n=1}^\infty\frac{H_n}{n^6}+72\zeta(7) \end{align} . \begin{align} I_4&=\int_0^1\frac{\ln^3u\operatorname{Li}_3(u)}{u(1-u)}du=\sum_{n=1}^\infty H_n^{(3)}\int_0^1 u^{n-1}\ln^3u du=-6\sum_{n=1}^\infty\frac{H_n^{(3)}}{n^4} \end{align} \begin{align} I_5&=\int_0^1\frac{\ln^3u}{u(1-u)}\left(\operatorname{Li}_3(1-u)-\zeta(3)\right)du\\ &=\underbrace{\int_0^1\frac{\ln^3u}{u}\left(\operatorname{Li}_3(1-u)-\zeta(3)\right)du}_{IBP}+\underbrace{\int_0^1\frac{\ln^3u}{1-u}\left(\operatorname{Li}_3(1-u)-\zeta(3)\right)\ du}_{1-u\ \mapsto\ u}\\ &=\frac14\int_0^1\frac{\ln^4u\operatorname{Li}_2(1-u)}{1-u}du+\underbrace{\int_0^1\frac{\ln^3(1-u)\operatorname{Li}_3(u)}{u}du}_{\large \text{our main integral}}-\zeta(3)\int_0^1\frac{\ln^3u}{1-u}du\\ &=\frac14\int_0^1\frac{\ln^4u\operatorname{Li}_2(1-u)}{1-u}du+I+6\zeta(3)\zeta(4) \end{align} In my solution here I came across the remaining integral and here is the result: $$\frac14\int_0^1\frac{\ln^4u\operatorname{Li}_2(1-u)}{1-u}du=6\zeta(2)\zeta(5)+36\zeta(7)-30\sum_{n=1}^\infty\frac{H_n}{n^6}-6\sum_{n=1}^\infty\frac{H_n^{(2)}}{n^5}$$ Then $$I_5=I+6\zeta(3)\zeta(4)+6\zeta(2)\zeta(5)+36\zeta(7)-30\sum_{n=1}^\infty\frac{H_n}{n^6}-6\sum_{n=1}^\infty\frac{H_n^{(2)}}{n^5}$$ . Note: We can not use the two sums $\sum_{n=1}^\infty\frac{H_n^3}{n^4}$ and $\sum_{n=1}^\infty\frac{H_nH_n^{(2)}} {n^4}$ in our solution because the integral $I$ is the key to evaluate these two sums.","How to evaluate I came across this integral while I was trying to compute two advanced sums of weight 7. The problem with my approach is that when I tried to evaluate (shown below), the main integral appeared there which cancels out from both sides, so any idea how to evaluate or ? Thanks. Here is my trial: Using the two generalized integral expressions of the polylogrithmic function which can be found in the book (Almost) Impossible Integrals, Sums and series page 4. We have Now we need the trilogarithmic identity: set to get Going back to our integral . . In my solution here I came across the remaining integral and here is the result: Then . Note: We can not use the two sums and in our solution because the integral is the key to evaluate these two sums.","I=\int_0^1\frac{\ln^3(1-x)\operatorname{Li}_3(x)}{x}dx\ ? I I_5 I I_5 I \int_0^1\frac{x\ln^n(u)}{1-xu}du=(-1)^n n!\operatorname{Li}_{n+1}
(x)\Longrightarrow \operatorname{Li}_{3}(x)=\frac12\int_0^1\frac{x\ln^2(u)}{1-xu}du\tag{1} \small{u\int_0^1\frac{\ln^n(x)}{1-u+ux}dx=(-1)^{n-1}n!\operatorname{Li}_{n+1}\left(\frac{u}{u-1}\right)\Longrightarrow\int_0^1\frac{\ln^3x}{1-u+ux}dx=\frac6u\operatorname{Li}_{3}\left(\frac{u}{u-1}\right)}\tag{2} \begin{align}
I&=\int_0^1\frac{\ln^3(1-x)\operatorname{Li}_3(x)}{x}dx\overset{\text{use} (1)}{=}\frac12\int_0^1\frac{\ln^3(1-x)}{x}\left(\int_0^1\frac{x\ln^2u}{1-xu}du\right)dx\\
&=\frac12\int_0^1\ln^2u\left(\frac{\ln^3(1-x)}{1-xu}dx\right)\ du\overset{1-x\ \mapsto\ x}{=}\frac12\int_0^1\ln^2u\left(\int_0^1\frac{\ln^3x}{1-u+ux}dx\right)\ du\\
&\overset{\text{use}\ (2)}{=}3\int_0^1\frac{\ln^2u}{u}\operatorname{Li}_4\left(\frac{u}{u-1}\right)du\overset{IBP}{=}-\int_0^1\frac{\ln^3u}{u(1-u)}\operatorname{Li}_3\left(\frac{u}{u-1}\right)du
\end{align} \operatorname{Li}_3\left(\frac{x-1}{x}\right)=\zeta(2)\ln x-\frac12\ln^2x\ln(1-x)+\frac16\ln^3x-\operatorname{Li}_3(1-x)-\operatorname{Li}_3(x)+\zeta(3) 1-x=u \small{\operatorname{Li}_3\left(\frac{u}{u-1}\right)=\zeta(2)\ln(1-u)-\frac12\ln^2(1-u)\ln u+\frac16\ln^3(1-u)-\operatorname{Li}_3(u)-\operatorname{Li}_3(1-u)+\zeta(3)} \begin{align}
I&=\small{-\int_0^1\frac{\ln^3u}{u(1-u)}\left(\zeta(2)\ln(1-u)-\frac12\ln^2(1-u)\ln x+\frac16\ln^3(1-u)-\operatorname{Li}_3(u)-\operatorname{Li}_3(1-u)+\zeta(3)\right)du}\\
&=-\zeta(2)\underbrace{\int_0^1\frac{\ln^3u\ln(1-u)}{u(1-u)}du}_{\Large I_1}+\frac12\underbrace{\int_0^1\frac{\ln^4u\ln^2(1-u)}{u(1-u)}du}_{\Large I_2}-\frac16\underbrace{\int_0^1\frac{\ln^3u\ln^3(1-u)}{u(1-u)}du}_{\Large I_3}\\
&\quad+\underbrace{\int_0^1\frac{\ln^3u\operatorname{Li}_3(u)}{u(1-u)}\ du}_{\Large I_4}+\underbrace{\int_0^1\frac{\ln^3u}{u(1-u)}\left(\operatorname{Li}_3(1-u)-\zeta(3)\right)du}_{\Large I_5}
\end{align} \begin{align}
I_1=\int_0^1\frac{\ln^3u\ln(1-u)}{u(1-u)}du=-\sum_{n=1}^\infty H_n\int_0^1 u^{n-1}\ln^3udu=6\sum_{n=1}^\infty\frac{H_n}{n^4}
\end{align} \begin{align}
I_2&=\int_0^1\frac{\ln^4u\ln^2(1-u)}{u(1-u)}du=\sum_{n=1}^\infty\left(H_n^2-H_n^{(2)}\right)\int_0^1 u^{n-1}\ln^4udu\\
&=24\sum_{n=1}^\infty\frac{H_n^2-H_n^{(2)}}{n^5}=24\sum_{n=1}^\infty\frac{H_n^2}{n^5}-24\sum_{n=1}^\infty\frac{H_n^{(2)}}{n^5}
\end{align} \begin{align}
I_3&=\int_0^1\frac{\ln^3u\ln^3(1-u)}{u(1-u)}du=\int_0^1\frac{\ln^3u\ln^3(1-u)}{u}du+\underbrace{\int_0^1\frac{\ln^3u\ln^3(1-u)}{1-u}du}_{1-x\ \mapsto\ x}\\
&=2\int_0^1\frac{\ln^3u\ln^3(1-u)}{u}\ du\overset{IBP}{=}\frac32\int_0^1\frac{\ln^4u\ln^2(1-u)}{1-u}du\\
&=\frac32\sum_{n=1}^\infty\left(H_n^2-H_n^{(2)}\right)\int_0^1 u^n\ln^4udu, \quad \text{reindex}\\
&=\frac32\sum_{n=1}^\infty\left(H_n^2-H_n^{(2)}-\frac{2H_n}{n}+\frac2{n^2}\right)\int_0^1 u^{n-1}\ln^4u du\\
&=\frac32\sum_{n=1}^\infty\left(H_n^2-H_n^{(2)}-\frac{2H_n}{n}+\frac2{n^2}\right)\left(\frac{24}{n^5}\right)\\
&=36\sum_{n=1}^\infty\frac{H_n^2}{n^5}-36\sum_{n=1}^\infty\frac{H_n^{(2)}}{n^5}-72\sum_{n=1}^\infty\frac{H_n}{n^6}+72\zeta(7)
\end{align} \begin{align}
I_4&=\int_0^1\frac{\ln^3u\operatorname{Li}_3(u)}{u(1-u)}du=\sum_{n=1}^\infty H_n^{(3)}\int_0^1 u^{n-1}\ln^3u du=-6\sum_{n=1}^\infty\frac{H_n^{(3)}}{n^4}
\end{align} \begin{align}
I_5&=\int_0^1\frac{\ln^3u}{u(1-u)}\left(\operatorname{Li}_3(1-u)-\zeta(3)\right)du\\
&=\underbrace{\int_0^1\frac{\ln^3u}{u}\left(\operatorname{Li}_3(1-u)-\zeta(3)\right)du}_{IBP}+\underbrace{\int_0^1\frac{\ln^3u}{1-u}\left(\operatorname{Li}_3(1-u)-\zeta(3)\right)\ du}_{1-u\ \mapsto\ u}\\
&=\frac14\int_0^1\frac{\ln^4u\operatorname{Li}_2(1-u)}{1-u}du+\underbrace{\int_0^1\frac{\ln^3(1-u)\operatorname{Li}_3(u)}{u}du}_{\large \text{our main integral}}-\zeta(3)\int_0^1\frac{\ln^3u}{1-u}du\\
&=\frac14\int_0^1\frac{\ln^4u\operatorname{Li}_2(1-u)}{1-u}du+I+6\zeta(3)\zeta(4)
\end{align} \frac14\int_0^1\frac{\ln^4u\operatorname{Li}_2(1-u)}{1-u}du=6\zeta(2)\zeta(5)+36\zeta(7)-30\sum_{n=1}^\infty\frac{H_n}{n^6}-6\sum_{n=1}^\infty\frac{H_n^{(2)}}{n^5} I_5=I+6\zeta(3)\zeta(4)+6\zeta(2)\zeta(5)+36\zeta(7)-30\sum_{n=1}^\infty\frac{H_n}{n^6}-6\sum_{n=1}^\infty\frac{H_n^{(2)}}{n^5} \sum_{n=1}^\infty\frac{H_n^3}{n^4} \sum_{n=1}^\infty\frac{H_nH_n^{(2)}} {n^4} I","['real-analysis', 'calculus', 'integration', 'definite-integrals', 'harmonic-numbers']"
38,Does $f(0)=0$ and $\left|f^\prime(x)\right|\leq\left|f(x)\right|$ imply $f(x)=0$?,Does  and  imply ?,f(0)=0 \left|f^\prime(x)\right|\leq\left|f(x)\right| f(x)=0,"Let $f:\mathbb{R}\to\mathbb{R}$ be a function such that $f(0)=0$ for all real numbers $x$, $\left|f^\prime(x)\right|\leq\left|f(x)\right|$. Can $f$ be a function other than the constant zero function? I coudn't find any other function satisfying the property. The bound on $f^\prime(x)$ may mean that $f(x)$ may not change too much but does it mean that $f$ is constant? I thought for a while and found that $f^\prime(0)=0$ and by using mean value theorem, if $x\neq0$ then there's a real number $y$ between $0$ and $x$ such that $\left|f(x)\right|=\left|xf^\prime(y)\right|\leq\left|xf(y)\right|$. Anything further?","Let $f:\mathbb{R}\to\mathbb{R}$ be a function such that $f(0)=0$ for all real numbers $x$, $\left|f^\prime(x)\right|\leq\left|f(x)\right|$. Can $f$ be a function other than the constant zero function? I coudn't find any other function satisfying the property. The bound on $f^\prime(x)$ may mean that $f(x)$ may not change too much but does it mean that $f$ is constant? I thought for a while and found that $f^\prime(0)=0$ and by using mean value theorem, if $x\neq0$ then there's a real number $y$ between $0$ and $x$ such that $\left|f(x)\right|=\left|xf^\prime(y)\right|\leq\left|xf(y)\right|$. Anything further?",,"['calculus', 'real-analysis']"
39,Diffeomorphism from Inverse function theorem,Diffeomorphism from Inverse function theorem,,"I often heard that it is possible to show by using the inverse function theorem that if a function is smooth (i.e. arbitrarily often differentiable), a bijection between open sets, and has a non-singular jacobian, then it is a smooth diffeomorphism. but somehow the inverse function theorem that I know and that wikipedia seems to know, only states that if it is a continuously differentiable bijection with nonzero jacobian, then its inverse function is also continuously differentiable. But how do you get from this, to the statement that I proposed above? I don't see the implication.","I often heard that it is possible to show by using the inverse function theorem that if a function is smooth (i.e. arbitrarily often differentiable), a bijection between open sets, and has a non-singular jacobian, then it is a smooth diffeomorphism. but somehow the inverse function theorem that I know and that wikipedia seems to know, only states that if it is a continuously differentiable bijection with nonzero jacobian, then its inverse function is also continuously differentiable. But how do you get from this, to the statement that I proposed above? I don't see the implication.",,['real-analysis']
40,Uniform continuity of $f(x) = x \sin{\frac{1}{x}}$ for $x \neq 0$ and $f(0) = 0.$,Uniform continuity of  for  and,f(x) = x \sin{\frac{1}{x}} x \neq 0 f(0) = 0.,"For the $f(x) = x \sin{\frac{1}{x}}$ for $x \neq 0$ and $f(0) = 0,$ my text book asks the following questions. (b) Why is $f$ uniformly continuous on any bounded subset of $\mathbb{R}$? (c) Is $f$ uniformly continuous on $\mathbb{R}$?? The graph for the function is this. For the question (b), if I take subset between $[0.2,0.6]$ or the subset where the slope is steep, I don't think the function is uniformly continuous because I think for a given $\epsilon>0$, there is no unique $\delta >0$ for the bounded subset. Therefore, it also cannot be uniformly continuous on $\mathbb{R}.$ However, the questions sounds like the function is uniformly continuous and the book says that it is uniformly continuous. The answer on the book says something but I need more explanation. Thanks.","For the $f(x) = x \sin{\frac{1}{x}}$ for $x \neq 0$ and $f(0) = 0,$ my text book asks the following questions. (b) Why is $f$ uniformly continuous on any bounded subset of $\mathbb{R}$? (c) Is $f$ uniformly continuous on $\mathbb{R}$?? The graph for the function is this. For the question (b), if I take subset between $[0.2,0.6]$ or the subset where the slope is steep, I don't think the function is uniformly continuous because I think for a given $\epsilon>0$, there is no unique $\delta >0$ for the bounded subset. Therefore, it also cannot be uniformly continuous on $\mathbb{R}.$ However, the questions sounds like the function is uniformly continuous and the book says that it is uniformly continuous. The answer on the book says something but I need more explanation. Thanks.",,"['real-analysis', 'uniform-continuity']"
41,"If $f$ is continuous and $\,f\big(\frac{1}2(x+y)\big) \le \frac{1}{2}\big(\,f(x)+f(y)\big)$, then $f$ is convex [duplicate]","If  is continuous and , then  is convex [duplicate]","f \,f\big(\frac{1}2(x+y)\big) \le \frac{1}{2}\big(\,f(x)+f(y)\big) f","This question already has answers here : Midpoint-Convex and Continuous Implies Convex (6 answers) Closed 2 years ago . Let $\,\,f :\mathbb R \to \mathbb R$ be a continuous function such that $$ f\Big(\dfrac{x+y}2\Big) \le \dfrac{1}{2}\big(\,f(x)+f(y)\big) ,\,\, \text{for all}\,\, x,y \in \mathbb R, $$ then  how do we prove that $f$ is convex that is $$ f\big(tx+(1-t)y\big)\le tf(x)+(1-t)f(y) , \forall x,y \in \mathbb R , t\in (0,1)? $$ I can prove it for dyadic rational's $t=\dfrac k {2^n}$ and then argue by continuity ; but I would like a more direct proof . Thanks in Advance","This question already has answers here : Midpoint-Convex and Continuous Implies Convex (6 answers) Closed 2 years ago . Let $\,\,f :\mathbb R \to \mathbb R$ be a continuous function such that $$ f\Big(\dfrac{x+y}2\Big) \le \dfrac{1}{2}\big(\,f(x)+f(y)\big) ,\,\, \text{for all}\,\, x,y \in \mathbb R, $$ then  how do we prove that $f$ is convex that is $$ f\big(tx+(1-t)y\big)\le tf(x)+(1-t)f(y) , \forall x,y \in \mathbb R , t\in (0,1)? $$ I can prove it for dyadic rational's $t=\dfrac k {2^n}$ and then argue by continuity ; but I would like a more direct proof . Thanks in Advance",,"['real-analysis', 'analysis', 'continuity', 'convex-analysis', 'functional-inequalities']"
42,Monotonicity of function of two variables,Monotonicity of function of two variables,,"I have a function of two variables, which I wish to check for monotonicity in the entire function domain. I cant find any formal definition of increasing or decreasing function for multi variable case. Can anybody please guide? Thanks in advance.","I have a function of two variables, which I wish to check for monotonicity in the entire function domain. I cant find any formal definition of increasing or decreasing function for multi variable case. Can anybody please guide? Thanks in advance.",,"['real-analysis', 'multivariable-calculus']"
43,"Integral $\int_0^{\pi/2} \ln(1+\alpha\sin^2 x)\, dx=\pi \ln \frac{1+\sqrt{1+\alpha}}{2}$",Integral,"\int_0^{\pi/2} \ln(1+\alpha\sin^2 x)\, dx=\pi \ln \frac{1+\sqrt{1+\alpha}}{2}","$$ I_1:=\int_0^{\pi/2} \ln(1+\alpha\sin^2 x)\, dx=\pi \ln \frac{1+\sqrt{1+\alpha}}{2}, \qquad \alpha \geq -1. $$ I am trying to prove this integral $I_1$.  We can write  $$ \int_0^{\pi/2} \ln(\alpha(1/\alpha+\sin^2 x))dx=\int_0^{\pi/2} \left(\ln \alpha+\ln (\frac{1}{\alpha}+\sin^2 x)\right)dx=\frac{\pi}{2} \ln \alpha+I_2 $$ where $$ I_2=\int_0^{\pi/2}\ln (\frac{1}{\alpha}+\sin^2 x) \,dx $$ however I am not sure what that will do for us....  I also tried differentiating wrt $\alpha$ but didn't get placed.  How can we prove $I_1$ result?  Thanks","$$ I_1:=\int_0^{\pi/2} \ln(1+\alpha\sin^2 x)\, dx=\pi \ln \frac{1+\sqrt{1+\alpha}}{2}, \qquad \alpha \geq -1. $$ I am trying to prove this integral $I_1$.  We can write  $$ \int_0^{\pi/2} \ln(\alpha(1/\alpha+\sin^2 x))dx=\int_0^{\pi/2} \left(\ln \alpha+\ln (\frac{1}{\alpha}+\sin^2 x)\right)dx=\frac{\pi}{2} \ln \alpha+I_2 $$ where $$ I_2=\int_0^{\pi/2}\ln (\frac{1}{\alpha}+\sin^2 x) \,dx $$ however I am not sure what that will do for us....  I also tried differentiating wrt $\alpha$ but didn't get placed.  How can we prove $I_1$ result?  Thanks",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
44,Continuous differentiability implies Lipschitz continuity,Continuous differentiability implies Lipschitz continuity,,"Here's a statement in Zygmund's Measure and Integral on page 17: If $f$ has a continuous derivative on $[a,b]$, then (by the mean-value theorem) $f$ satisfies a Lipschitz condition on $[a,b]$. This does not seem obvious to me. How can I show it? Also, what does a continuous derivative imply? Can we conclude the function is differentiable? If so, how can I prove it?","Here's a statement in Zygmund's Measure and Integral on page 17: If $f$ has a continuous derivative on $[a,b]$, then (by the mean-value theorem) $f$ satisfies a Lipschitz condition on $[a,b]$. This does not seem obvious to me. How can I show it? Also, what does a continuous derivative imply? Can we conclude the function is differentiable? If so, how can I prove it?",,"['real-analysis', 'derivatives', 'continuity', 'lipschitz-functions']"
45,Approximating continuous functions with polynomials,Approximating continuous functions with polynomials,,"Given $\epsilon \gt 0$ and $f \in C^{0}[0,1]$, Weierstrass says that I can find at least one (how many? probably a lot?) polynomial $P$ which approximates f uniformly: $$\sup_{x \in [0,1]} |f(x) - P(x)| \lt \epsilon$$ This means that, under the sup norm $||.||_{\infty}$, the polynomials are dense in $C^{0}[0,1]$. So, in analogy to approximating irrationals with the rationals, I would like to know: What can we say about the order of $P$? Or, turning this around, given that $P$ is of order $n$, how small can $\epsilon$ get? I'm betting this should depend in some way on the properties of $f$: the intuition is that smoother functions should be somehow ""better"" approximated by lower-order polynomials, and less-well-behaved functions should require higher-order polynomials. But I am not sure how to formalize this. This is probably all well-understood, but I'm not well-read on approximation theory. Any guidance would be wonderful.","Given $\epsilon \gt 0$ and $f \in C^{0}[0,1]$, Weierstrass says that I can find at least one (how many? probably a lot?) polynomial $P$ which approximates f uniformly: $$\sup_{x \in [0,1]} |f(x) - P(x)| \lt \epsilon$$ This means that, under the sup norm $||.||_{\infty}$, the polynomials are dense in $C^{0}[0,1]$. So, in analogy to approximating irrationals with the rationals, I would like to know: What can we say about the order of $P$? Or, turning this around, given that $P$ is of order $n$, how small can $\epsilon$ get? I'm betting this should depend in some way on the properties of $f$: the intuition is that smoother functions should be somehow ""better"" approximated by lower-order polynomials, and less-well-behaved functions should require higher-order polynomials. But I am not sure how to formalize this. This is probably all well-understood, but I'm not well-read on approximation theory. Any guidance would be wonderful.",,"['real-analysis', 'reference-request', 'polynomials', 'approximation-theory']"
46,Evaluate $\lim_{n\to\infty} \prod_{k=1}^n \frac{2k}{2k-1}\int_{-1}^{\infty} \frac{{\left(\cos{x}\right)}^{2n}}{2^x} \; dx$,Evaluate,\lim_{n\to\infty} \prod_{k=1}^n \frac{2k}{2k-1}\int_{-1}^{\infty} \frac{{\left(\cos{x}\right)}^{2n}}{2^x} \; dx,"Problem 9 in the JHMT 2013 Calculus Test asks to evaluate $$\lim_{n\to\infty} \prod_{k=1}^n \frac{2k}{2k-1}\int_{-1}^{\infty} \frac{{\left(\cos{x}\right)}^{2n}}{2^x} \; dx$$ The answer is $\pi\cdot 2^\pi /(2^{\pi}-1)$ . How can I show this?  I know that the infinite product diverges and the limit cannot be moved into the integral, but I don't know what to do.  Maybe I can represent the integral as a summation?","Problem 9 in the JHMT 2013 Calculus Test asks to evaluate The answer is . How can I show this?  I know that the infinite product diverges and the limit cannot be moved into the integral, but I don't know what to do.  Maybe I can represent the integral as a summation?",\lim_{n\to\infty} \prod_{k=1}^n \frac{2k}{2k-1}\int_{-1}^{\infty} \frac{{\left(\cos{x}\right)}^{2n}}{2^x} \; dx \pi\cdot 2^\pi /(2^{\pi}-1),"['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'infinite-product']"
47,Infinite Series $\sum\limits_{n=1}^\infty\frac{H_{2n+1}}{n^2}$,Infinite Series,\sum\limits_{n=1}^\infty\frac{H_{2n+1}}{n^2},"How can I prove that  $$\sum_{n=1}^\infty\frac{H_{2n+1}}{n^2}=\frac{11}{4}\zeta(3)+\zeta(2)+4\log(2)-4$$ I think this post can help me, but I'm not sure.","How can I prove that  $$\sum_{n=1}^\infty\frac{H_{2n+1}}{n^2}=\frac{11}{4}\zeta(3)+\zeta(2)+4\log(2)-4$$ I think this post can help me, but I'm not sure.",,"['real-analysis', 'sequences-and-series', 'closed-form', 'harmonic-numbers', 'polylogarithm']"
48,"Two definitions of ""Bounded Variation Function""","Two definitions of ""Bounded Variation Function""",,"As far as I know, a function $f$ defined on an interval $[a, b]$ is said to be of bounded variation if  $$\tag{1}V_a^b(f)=\sup\left\{\sum_{P} \lvert f(x_{j+1})-f(x_j)\rvert \ :\ P\ \text{partition of }[a, b]\right\}<\infty.$$ Today I discovered that another definition is in use for a function defined in an open subset $\Omega$ of $\mathbb{R}^n$, namely ( cfr. Wikipedia ) we say that $f$ is of bounded variation if $$\tag{2}V(f; \Omega)=\sup\left\{ \int_{\Omega}f(x)\,\text{div}\,\phi (x)\, dx\ :\ \phi\in C^1_c(\Omega),\ \lVert \phi\rVert_{\infty}\le 1 \right\}<\infty.$$ Even if the cited Wikipedia article treats the two definitions as if they were equivalent when $\Omega=(a, b)$, this does not seem to me to be the case. The Dirichlet function $\chi_{\mathbb{Q}\cap [0, 1]}$ is not of bounded variation in $(0, 1)$ in the sense of definition (1) but it is in the sense of definition (2). Question . What is the precise relationship between the two definitions? Thank you for reading.","As far as I know, a function $f$ defined on an interval $[a, b]$ is said to be of bounded variation if  $$\tag{1}V_a^b(f)=\sup\left\{\sum_{P} \lvert f(x_{j+1})-f(x_j)\rvert \ :\ P\ \text{partition of }[a, b]\right\}<\infty.$$ Today I discovered that another definition is in use for a function defined in an open subset $\Omega$ of $\mathbb{R}^n$, namely ( cfr. Wikipedia ) we say that $f$ is of bounded variation if $$\tag{2}V(f; \Omega)=\sup\left\{ \int_{\Omega}f(x)\,\text{div}\,\phi (x)\, dx\ :\ \phi\in C^1_c(\Omega),\ \lVert \phi\rVert_{\infty}\le 1 \right\}<\infty.$$ Even if the cited Wikipedia article treats the two definitions as if they were equivalent when $\Omega=(a, b)$, this does not seem to me to be the case. The Dirichlet function $\chi_{\mathbb{Q}\cap [0, 1]}$ is not of bounded variation in $(0, 1)$ in the sense of definition (1) but it is in the sense of definition (2). Question . What is the precise relationship between the two definitions? Thank you for reading.",,"['real-analysis', 'bounded-variation']"
49,How can we find and categorize the subgroups of $\mathbb{R}$?,How can we find and categorize the subgroups of ?,\mathbb{R},"$\newcommand{\R}{\Bbb R}\newcommand{\Q}{\Bbb Q}\newcommand{\Z}{\Bbb Z}$ What are all the subgroups of R =  $(\R, +)$ and how can we categorize them? I started thinking about this question last night after looking at the structure of the cosets of $\R / \Q$ What do the cosets of $\mathbb{R} / \mathbb{Q}$ look like? . I did some searching on SO and google but didn't find anything giving a full categorization (or even a partial one) of the subgroups of $\R$. Here are the subgroups that I came up with so far: $\Z$ (there are no finite subgroups and $\Z$ is the universal smallest subgroup I think) n$\Z$ eg 2$\Z$ all even numbers a$\Z$ where a is any real number, including a in $\Q$ which ""nest"" nicely in each other $\Z$[a] - group generated by adding one real a to $\Z$ n$\Z$[a] which equals $\Z$[na] and so is just a case of the one above Dyadic rationals eg a numbers of the form a/2 b or similar subgroups such as a/3 b , a/2 b 7 c etc $\Q$ $\Q$[a] $\Q$[a in A] where A is a subset of $\R$ - could be finite, countable or uncountable. Group generated by adding all elements of A to $\Q$ eg  $\Q[\sqrt2]$ It is clear that the ""n$\Z$ subgroups"" n$\Z$ and m$\Z$ are related according to the gcd(n,m) Also when H is a subgroup of R looking at the structure of the cosets of R / H. eg for H any of the Z subgroups we get R / H homomorphic to [0,1) or the circle. For H one of the Q subgroups it is more complex and I currently don't have ideas on the larger subgroup cosets I am not clear how ""big"" a subgroup H can get before it becomes the whole of R. I do know that if it contains any interval then it is the whole of R. But what about H with dimension less than 1? I am aware of one question on SO about the proper measurable subgroups of R having 0 measure Proper Measurable subgroups of $\mathbb R$ , one on dense subgroups Subgroup of $\mathbb{R}$ either dense or has a least positive element? and one on the subgroups of Q How to find all subgroups of $(\mathbb{Q},+)$ but that is all my searching found so far. Why is this question interesting? 1) there seem to be so many subgroups and they are related in many groupings 2) I think the subgroups related to the structure of the reals in some subtle ways 3) I know the result for the complete classification of all finite subgroups was a major result so wondering what has been done in this basic uncountable case. If anyone has any insight, intuition, info, papers or theorems on subgroups of R and how they are interrelated that would be interesting.","$\newcommand{\R}{\Bbb R}\newcommand{\Q}{\Bbb Q}\newcommand{\Z}{\Bbb Z}$ What are all the subgroups of R =  $(\R, +)$ and how can we categorize them? I started thinking about this question last night after looking at the structure of the cosets of $\R / \Q$ What do the cosets of $\mathbb{R} / \mathbb{Q}$ look like? . I did some searching on SO and google but didn't find anything giving a full categorization (or even a partial one) of the subgroups of $\R$. Here are the subgroups that I came up with so far: $\Z$ (there are no finite subgroups and $\Z$ is the universal smallest subgroup I think) n$\Z$ eg 2$\Z$ all even numbers a$\Z$ where a is any real number, including a in $\Q$ which ""nest"" nicely in each other $\Z$[a] - group generated by adding one real a to $\Z$ n$\Z$[a] which equals $\Z$[na] and so is just a case of the one above Dyadic rationals eg a numbers of the form a/2 b or similar subgroups such as a/3 b , a/2 b 7 c etc $\Q$ $\Q$[a] $\Q$[a in A] where A is a subset of $\R$ - could be finite, countable or uncountable. Group generated by adding all elements of A to $\Q$ eg  $\Q[\sqrt2]$ It is clear that the ""n$\Z$ subgroups"" n$\Z$ and m$\Z$ are related according to the gcd(n,m) Also when H is a subgroup of R looking at the structure of the cosets of R / H. eg for H any of the Z subgroups we get R / H homomorphic to [0,1) or the circle. For H one of the Q subgroups it is more complex and I currently don't have ideas on the larger subgroup cosets I am not clear how ""big"" a subgroup H can get before it becomes the whole of R. I do know that if it contains any interval then it is the whole of R. But what about H with dimension less than 1? I am aware of one question on SO about the proper measurable subgroups of R having 0 measure Proper Measurable subgroups of $\mathbb R$ , one on dense subgroups Subgroup of $\mathbb{R}$ either dense or has a least positive element? and one on the subgroups of Q How to find all subgroups of $(\mathbb{Q},+)$ but that is all my searching found so far. Why is this question interesting? 1) there seem to be so many subgroups and they are related in many groupings 2) I think the subgroups related to the structure of the reals in some subtle ways 3) I know the result for the complete classification of all finite subgroups was a major result so wondering what has been done in this basic uncountable case. If anyone has any insight, intuition, info, papers or theorems on subgroups of R and how they are interrelated that would be interesting.",,"['real-analysis', 'group-theory', 'topological-groups', 'rational-numbers']"
50,Showing $\left|\frac{a+b}{2}\right|^p+\left|\frac{a-b}{2}\right|^p\leq\frac{1}{2}|a|^p+\frac{1}{2}|b|^p$,Showing,\left|\frac{a+b}{2}\right|^p+\left|\frac{a-b}{2}\right|^p\leq\frac{1}{2}|a|^p+\frac{1}{2}|b|^p,"For $a,b \in \mathbb R$, $p\geq2$ I try to show $$\left|\frac{a+b}{2}\right|^p+\left|\frac{a-b}{2}\right|^p\leq\frac{1}{2}|a|^p+\frac{1}{2}|b|^p.$$ Is this a popular inequality (At least I could not find it in the list of popular inequalities from wikipedia)? It seems to be related to convexity but I did not succeed to show it. A related inequality seems to be for $p \geq 1,a,b\geq0$ $$\left(\frac{a+b}{2}\right)^p\leq \frac{1}{2}a^p+\frac{1}{2}b^p,$$ which directly follows from the convexity of $x^p$ for positive numbers.","For $a,b \in \mathbb R$, $p\geq2$ I try to show $$\left|\frac{a+b}{2}\right|^p+\left|\frac{a-b}{2}\right|^p\leq\frac{1}{2}|a|^p+\frac{1}{2}|b|^p.$$ Is this a popular inequality (At least I could not find it in the list of popular inequalities from wikipedia)? It seems to be related to convexity but I did not succeed to show it. A related inequality seems to be for $p \geq 1,a,b\geq0$ $$\left(\frac{a+b}{2}\right)^p\leq \frac{1}{2}a^p+\frac{1}{2}b^p,$$ which directly follows from the convexity of $x^p$ for positive numbers.",,"['real-analysis', 'inequality', 'convex-analysis']"
51,Positivity and Interchange of Summation,Positivity and Interchange of Summation,,"Suppose I have $\sum\limits_{n = 1}^{\infty}\sum\limits_{m = 1}^{\infty} a_{m, n}$ where $a_{m, n} \geq 0$ for all $m$ and $n$. Can I interchange the two summations? If so why?","Suppose I have $\sum\limits_{n = 1}^{\infty}\sum\limits_{m = 1}^{\infty} a_{m, n}$ where $a_{m, n} \geq 0$ for all $m$ and $n$. Can I interchange the two summations? If so why?",,"['real-analysis', 'sequences-and-series']"
52,For each $y \in \mathbb{R}$ either no $x$ with $f(x) = y$ or two such values of $x$. Show that $f$ is discontinuous.,For each  either no  with  or two such values of . Show that  is discontinuous.,y \in \mathbb{R} x f(x) = y x f,"This is from Apostol's Mathematical Analysis. Let $f$ be a function defined in $[0, 1]$ such that for each real number $y$ either there is no value of $x \in [0, 1]$ such that $f(x) = y$ or there are exactly two such values of $x$ . Prove that a) $f$ must be discontinuous on $[0, 1]$ . b) $f$ has an infinite number of discontinuities in $[0, 1]$ . Long back I had tried to solve the problem. I reasoned for a) as follows: Suppose that $f$ is continuous and let $m, M$ be its minimum and maximum values on $[0, 1]$ . Clearly if $m = M$ then $f$ is constant and clearly for every $x \in [0, 1]$ we have $f(x) = m = M$ which contradicts given property of $f$ . So we must have $m < M$ . Clearly there are now four distinct points $a, b, c, d$ in $[0, 1]$ such that $f(a) = f(b) = m, f(c) = f(d) = M$ . By checking possible linear orderings of $a, b, c, d$ I could find a find a value of $y \in (m, M)$ for which there exist more than 2 values of $x$ such that $f(x) = y$ . However this approach looks very clumsy and I suspect much simpler and easier proof is possible. Part b) seemed bit complex and my only guess was that probably $f$ was discontinuous on each subinterval of $[0, 1]$ but I could not prove this claim (may be the claim is wrong!). Perhaps there is a simpler solution to this part of the problem. Apostol's also asks to provide an example of such a function. Any hints on the overall problem will be appreciated. Update : After looking at Marc's answer, I think I have found a way to show that there are infinitely many discontinuities. Earlier the solution was a part of the question itself but it made the question look quite lengthy hence I moved it to a separate answer.","This is from Apostol's Mathematical Analysis. Let be a function defined in such that for each real number either there is no value of such that or there are exactly two such values of . Prove that a) must be discontinuous on . b) has an infinite number of discontinuities in . Long back I had tried to solve the problem. I reasoned for a) as follows: Suppose that is continuous and let be its minimum and maximum values on . Clearly if then is constant and clearly for every we have which contradicts given property of . So we must have . Clearly there are now four distinct points in such that . By checking possible linear orderings of I could find a find a value of for which there exist more than 2 values of such that . However this approach looks very clumsy and I suspect much simpler and easier proof is possible. Part b) seemed bit complex and my only guess was that probably was discontinuous on each subinterval of but I could not prove this claim (may be the claim is wrong!). Perhaps there is a simpler solution to this part of the problem. Apostol's also asks to provide an example of such a function. Any hints on the overall problem will be appreciated. Update : After looking at Marc's answer, I think I have found a way to show that there are infinitely many discontinuities. Earlier the solution was a part of the question itself but it made the question look quite lengthy hence I moved it to a separate answer.","f [0, 1] y x \in [0, 1] f(x) = y x f [0, 1] f [0, 1] f m, M [0, 1] m = M f x \in [0, 1] f(x) = m = M f m < M a, b, c, d [0, 1] f(a) = f(b) = m, f(c) = f(d) = M a, b, c, d y \in (m, M) x f(x) = y f [0, 1]","['calculus', 'real-analysis']"
53,"Improper Integral $\int\limits_0^{\frac12}(2x - 1)^6\log^2(2\sin\pi x)\,dx$",Improper Integral,"\int\limits_0^{\frac12}(2x - 1)^6\log^2(2\sin\pi x)\,dx","How can I find a closed form for the following integral  $$\int_0^{\frac12}(2x - 1)^6\log^2(2\sin\pi x)\,dx?$$","How can I find a closed form for the following integral  $$\int_0^{\frac12}(2x - 1)^6\log^2(2\sin\pi x)\,dx?$$",,"['calculus', 'real-analysis', 'integration', 'improper-integrals', 'closed-form']"
54,"Can a function ""grow too fast"" to be real analytic?","Can a function ""grow too fast"" to be real analytic?",,"Does there exist a continuous function $\: f : \mathbf{R} \to \mathbf{R} \:$ such that for all real analytic functions $\: g : \mathbf{R} \to \mathbf{R} \:$, for all real numbers $x$, there exists a real number $y$ such that $\: x < y \:$ and $\: g(y) < f(y) \:$?","Does there exist a continuous function $\: f : \mathbf{R} \to \mathbf{R} \:$ such that for all real analytic functions $\: g : \mathbf{R} \to \mathbf{R} \:$, for all real numbers $x$, there exists a real number $y$ such that $\: x < y \:$ and $\: g(y) < f(y) \:$?",,['real-analysis']
55,Third degree Taylor series of $f(x) = e^x \cos{x} $,Third degree Taylor series of,f(x) = e^x \cos{x} ,"Suppose you have the function: $$f(x) = e^x  \cos{x} $$ and you need to find the 3rd degree Taylor Series representation. The way I have been taught to do this is to express each separate function as a power series and multiply as necessary for the 3rd degree. For example for  $$ \cos x =\sum_{n=0}^\infty (-1)^n\frac{  x^{2n}}{(2n)!} = 1-\frac{x^2}{2!}+\frac{x^4}{4!}+\cdots  \text{ and }  e^x =\sum_{n=0}^\infty \frac{x^n}{n!} = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!}+\cdots $$ multiply the terms on the right of each until you get the 3rd degree. Logically, I am happy. However, I have not seen a theorem or any rule that says you can just multiply series in this way. Doing it this way, is there a guarantee that I will always get the power series representation of $f(x)$? Additionally, if instead of multiplying, functions were being added? Would the above hold true - take the series of each function and add up the necessary terms?","Suppose you have the function: $$f(x) = e^x  \cos{x} $$ and you need to find the 3rd degree Taylor Series representation. The way I have been taught to do this is to express each separate function as a power series and multiply as necessary for the 3rd degree. For example for  $$ \cos x =\sum_{n=0}^\infty (-1)^n\frac{  x^{2n}}{(2n)!} = 1-\frac{x^2}{2!}+\frac{x^4}{4!}+\cdots  \text{ and }  e^x =\sum_{n=0}^\infty \frac{x^n}{n!} = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!}+\cdots $$ multiply the terms on the right of each until you get the 3rd degree. Logically, I am happy. However, I have not seen a theorem or any rule that says you can just multiply series in this way. Doing it this way, is there a guarantee that I will always get the power series representation of $f(x)$? Additionally, if instead of multiplying, functions were being added? Would the above hold true - take the series of each function and add up the necessary terms?",,"['real-analysis', 'sequences-and-series', 'power-series', 'taylor-expansion']"
56,$\lim_{n\to \infty}f(nx)=0$ implies $\lim_{x\to \infty}f(x)=0$ [duplicate],implies  [duplicate],\lim_{n\to \infty}f(nx)=0 \lim_{x\to \infty}f(x)=0,"This question already has an answer here : A classical problem about limit of continuous function at infinity and its connection with Baire Category Theorem (1 answer) Closed 8 years ago . Can anyone help me with this problem? Let $f:[0,\infty)\longrightarrow \mathbb R$ be a continuous function such that for each $x>0$, we have $\lim_{n\to \infty}f(nx)=0$. Then prove that $\lim_{x\to \infty}f(x)=0$. Our teacher told first to prove Baire's theorem, and then show that this is a consequence of that theorem. I proved Baire's theorem, and I spend a few hours thinking on how Baire's theorem is related to this problem, but I couldn't find anything. I'd really appreciate your help.","This question already has an answer here : A classical problem about limit of continuous function at infinity and its connection with Baire Category Theorem (1 answer) Closed 8 years ago . Can anyone help me with this problem? Let $f:[0,\infty)\longrightarrow \mathbb R$ be a continuous function such that for each $x>0$, we have $\lim_{n\to \infty}f(nx)=0$. Then prove that $\lim_{x\to \infty}f(x)=0$. Our teacher told first to prove Baire's theorem, and then show that this is a consequence of that theorem. I proved Baire's theorem, and I spend a few hours thinking on how Baire's theorem is related to this problem, but I couldn't find anything. I'd really appreciate your help.",,"['real-analysis', 'limits', 'baire-category']"
57,What's the purpose of the two different definitions used for limit?,What's the purpose of the two different definitions used for limit?,,"In my study on the concept of limit, I've come across two different definitions: Let's assume that $a \in \bar D$, i.e. $a$ belongs to the closure of the domain of function $f$. Then $\lim_{x\rightarrow a}f(x)=b$ $\forall_{\epsilon>0}\exists_{\delta>0}\forall_{x}\ \ x\in D     \land|x-a|<\delta\implies |f(x)-b|<\epsilon$ $\forall_{\epsilon>0}\exists_{\delta>0}\forall_{x}\ \ x\in D     \land(0<|x-a|<\delta)\implies |f(x)-b|<\epsilon$ I've seen definition 1. being used mainly in book written in continental Europe, while definition 2 in books written in USA or UK. I may be wrong though. The point is that these definitions are clearly very different. For example, if $f$ is equal to $1$ in every point in $\mathcal{R}$ except in $a$, where it's equal to $2$, then according to def.1. the limit doesn't exist at $a$, but according to def.2. it does exist. Another difference I've notice is in proving the existence of composition of limits, in def1. it will be less demanding than in def2. Why would so many books use these different definitions? Edit: Because many of this forum may not be accquainted with def.1, you can check the book called ""Multidimensional Real Analysis"" by Duistermaat and Kolk, published by Cambridge University Press, where this is given as Definition 1.3.1 . Here's an image of the book: And clearly it's not a typo. (you can check the errata at the book's site.)","In my study on the concept of limit, I've come across two different definitions: Let's assume that $a \in \bar D$, i.e. $a$ belongs to the closure of the domain of function $f$. Then $\lim_{x\rightarrow a}f(x)=b$ $\forall_{\epsilon>0}\exists_{\delta>0}\forall_{x}\ \ x\in D     \land|x-a|<\delta\implies |f(x)-b|<\epsilon$ $\forall_{\epsilon>0}\exists_{\delta>0}\forall_{x}\ \ x\in D     \land(0<|x-a|<\delta)\implies |f(x)-b|<\epsilon$ I've seen definition 1. being used mainly in book written in continental Europe, while definition 2 in books written in USA or UK. I may be wrong though. The point is that these definitions are clearly very different. For example, if $f$ is equal to $1$ in every point in $\mathcal{R}$ except in $a$, where it's equal to $2$, then according to def.1. the limit doesn't exist at $a$, but according to def.2. it does exist. Another difference I've notice is in proving the existence of composition of limits, in def1. it will be less demanding than in def2. Why would so many books use these different definitions? Edit: Because many of this forum may not be accquainted with def.1, you can check the book called ""Multidimensional Real Analysis"" by Duistermaat and Kolk, published by Cambridge University Press, where this is given as Definition 1.3.1 . Here's an image of the book: And clearly it's not a typo. (you can check the errata at the book's site.)",,"['calculus', 'real-analysis', 'limits', 'definition']"
58,Can I exchange limit and differentiation for a sequence of smooth functions?,Can I exchange limit and differentiation for a sequence of smooth functions?,,"Let $(f_n)_{n\in \mathbb N}$ be a sequence of smooth functions converging to some $f$. Under what circumstances can I exchange limit and derivative?, i.e. $$\lim_{n\rightarrow \infty} \frac{\partial f_n(x)}{\partial x} = \frac{\partial f(x)}{\partial x}$$","Let $(f_n)_{n\in \mathbb N}$ be a sequence of smooth functions converging to some $f$. Under what circumstances can I exchange limit and derivative?, i.e. $$\lim_{n\rightarrow \infty} \frac{\partial f_n(x)}{\partial x} = \frac{\partial f(x)}{\partial x}$$",,"['real-analysis', 'sequences-and-series', 'analysis', 'limits', 'derivatives']"
59,Simple function approximation of a function in $L^p$,Simple function approximation of a function in,L^p,"I know that, in general, that any function $f \in L^p(X,\mathcal{M},\mu)$ can be approximated arbitrarily well by a simple function $\sum_{k=1}^n \lambda_k \chi_{E_k}$ where $a_k \in \mathbb{C}, E_k \in \mathcal{M}$. My question concerns the special case $X=[a,b]\subset \mathbb{R}$ equipped with the Borel $\sigma$-algebra and the Lebesgue measure. Is it then possible to approximate a function $f\in L^p([a,b])$ arbitrarily well with simple functions of the form $\sum_{k=1}^m \alpha_k \chi_{A_k}$ where each $A_k$ is an interval ? Thank you in advance","I know that, in general, that any function $f \in L^p(X,\mathcal{M},\mu)$ can be approximated arbitrarily well by a simple function $\sum_{k=1}^n \lambda_k \chi_{E_k}$ where $a_k \in \mathbb{C}, E_k \in \mathcal{M}$. My question concerns the special case $X=[a,b]\subset \mathbb{R}$ equipped with the Borel $\sigma$-algebra and the Lebesgue measure. Is it then possible to approximate a function $f\in L^p([a,b])$ arbitrarily well with simple functions of the form $\sum_{k=1}^m \alpha_k \chi_{A_k}$ where each $A_k$ is an interval ? Thank you in advance",,"['real-analysis', 'functional-analysis', 'normed-spaces']"
60,Fourier transform of fourier transform?,Fourier transform of fourier transform?,,"I have the definition of Fourier transform $$\hat f(\lambda) = \int_{\infty}^\infty f(t) \exp(- i \lambda t) dt$$ and have proved the following lemmas: $\hat E(x) = \sqrt{2 \pi} E(x)$ where $E(x) = \exp(- \tfrac{1}{2} x^2)$ Let $f_y(x) = f(x-y)$ then $\hat f_y(\lambda) = \hat f(\lambda) \exp(- i \lambda y)$ Let $\varphi(\lambda) = R E (R \lambda)$ then $\hat \varphi (\lambda) = \hat E(\frac{\lambda}{R})$ $\int \hat f(x) g(x) dx = \int f(x) \hat g(x)$ $\widehat{f\star g} = \hat f \cdot \hat g$ and I want to show that $$\hat {\hat f}(x) = 2 \pi f(-x).$$ I think the idea of the proof is to use the third lemma with a shift of $f$ anda scale of $E$, so that when you take the limit of the scale it tends towards $1$ and you are just left with $f$ but I just can't make any thing work out correctly. I think we also have $f \star \varphi \to f$ as $R \to \infty$.","I have the definition of Fourier transform $$\hat f(\lambda) = \int_{\infty}^\infty f(t) \exp(- i \lambda t) dt$$ and have proved the following lemmas: $\hat E(x) = \sqrt{2 \pi} E(x)$ where $E(x) = \exp(- \tfrac{1}{2} x^2)$ Let $f_y(x) = f(x-y)$ then $\hat f_y(\lambda) = \hat f(\lambda) \exp(- i \lambda y)$ Let $\varphi(\lambda) = R E (R \lambda)$ then $\hat \varphi (\lambda) = \hat E(\frac{\lambda}{R})$ $\int \hat f(x) g(x) dx = \int f(x) \hat g(x)$ $\widehat{f\star g} = \hat f \cdot \hat g$ and I want to show that $$\hat {\hat f}(x) = 2 \pi f(-x).$$ I think the idea of the proof is to use the third lemma with a shift of $f$ anda scale of $E$, so that when you take the limit of the scale it tends towards $1$ and you are just left with $f$ but I just can't make any thing work out correctly. I think we also have $f \star \varphi \to f$ as $R \to \infty$.",,"['real-analysis', 'integration', 'fourier-analysis']"
61,How badly can Dini's theorem fail if the p.w. limit isn't continuous?,How badly can Dini's theorem fail if the p.w. limit isn't continuous?,,"Dini's theorem is commonly seen in real analysis courses (possibly with the requirement that $X$ be a compact metric space if topological spaces are still off in the future), but suppose one wanted to give an example of how much it can fail without the requirement that the pointwise limit of the sequence of functions is continuous. The problem therefore is: Exhibit a sequence of continuous functions $f_n: [0,1] \to [0,1]$ pointwise monotonically decreasing to a function $f: [0,1] \to [0,1]$ such that the set of points where $f$ is discontinuous has measure $1$ . This could for example be used to demonstrate how poorly behaved the Riemann integral is with respect to pointwise limits since the sequence $(f_n)$ is pretty much as nice as you can possibly get without being uniformly convergent and $f$ is obviously Lebesgue integrable, but it ""maximally fails"" to satisfy Lebesgue's criterion for Riemann integrability. Additionally, it would demonstrate the existence of comeagre Lebesgue null sets because the set of points where $f$ is continuous is comeagre by the Baire-Osgood theorem (does anyone have a good free online reference for this? (EDIT: I wrote one myself )).","Dini's theorem is commonly seen in real analysis courses (possibly with the requirement that be a compact metric space if topological spaces are still off in the future), but suppose one wanted to give an example of how much it can fail without the requirement that the pointwise limit of the sequence of functions is continuous. The problem therefore is: Exhibit a sequence of continuous functions pointwise monotonically decreasing to a function such that the set of points where is discontinuous has measure . This could for example be used to demonstrate how poorly behaved the Riemann integral is with respect to pointwise limits since the sequence is pretty much as nice as you can possibly get without being uniformly convergent and is obviously Lebesgue integrable, but it ""maximally fails"" to satisfy Lebesgue's criterion for Riemann integrability. Additionally, it would demonstrate the existence of comeagre Lebesgue null sets because the set of points where is continuous is comeagre by the Baire-Osgood theorem (does anyone have a good free online reference for this? (EDIT: I wrote one myself )).","X f_n: [0,1] \to [0,1] f: [0,1] \to [0,1] f 1 (f_n) f f","['real-analysis', 'measure-theory']"
62,"If $f(x)$ is continuous on $[a,b]$ and $M=\max \; |f(x)|$, is $M=\lim \limits_{n\to\infty} \left(\int_a^b|f(x)|^n\,\mathrm dx\right)^{1/n}$?","If  is continuous on  and , is ?","f(x) [a,b] M=\max \; |f(x)| M=\lim \limits_{n\to\infty} \left(\int_a^b|f(x)|^n\,\mathrm dx\right)^{1/n}","Let $f(x)$ be a continuous real-valued function on $[a,b]$ and $M=\max\{|f(x)| \; :\; x \in [a,b]\}$. Is it true that: $$ M= \lim_{n\to\infty}\left(\int_a^b|f(x)|^n\,\mathrm dx\right)^{1/n} ? $$ Thanks!","Let $f(x)$ be a continuous real-valued function on $[a,b]$ and $M=\max\{|f(x)| \; :\; x \in [a,b]\}$. Is it true that: $$ M= \lim_{n\to\infty}\left(\int_a^b|f(x)|^n\,\mathrm dx\right)^{1/n} ? $$ Thanks!",,['real-analysis']
63,Taylor expansion for vector-valued function?,Taylor expansion for vector-valued function?,,"Let $f:\mathbb{R}^m \to \mathbb{R}^n$. Is it possible to do a Taylor expansion of $f$ around $\theta\in\mathbb{R}^m$? I am hoping for something like $$f\left(\theta\right) = f\left(\theta_0\right) + A \left(\theta - \theta_0\right) + \left(\theta - \theta'\right)^T \text{something} \left(\theta - \theta'\right)$$ where $A$ is a $n\times m$ matrix, and its rows are gradient of $f_i$ ($i$-the entry of vector $f$) with respect to vector $\theta$. But what should ""something"" be? For example, let us consider the simple case where $f :\mathbb{R}^m \to \mathbb{R}^2$. Denote $f = \left(f_1,f_2\right)$. Then $$f_1\left(\theta_n\right) = f_1(\theta_0) + \nabla f_1\left(\theta_0\right)(\theta_n - \theta_0) + (\theta_n - \theta_0)^T H_1(\theta') (\theta_n - \theta_0) $$ $$f_2\left(\theta_n\right) = f_2(\theta_0) + \nabla f_2\left(\theta_0\right)(\theta_n - \theta_0) + (\theta_n - \theta_0)^T H_2(\theta') (\theta_n - \theta_0) $$ If I try to put this in matrix form, I will get something like $$f\left(\theta_0\right) = f(\theta_0) + \begin{pmatrix} \nabla f_1(\theta_0)\\ \nabla f_2(\theta_0)\end{pmatrix} (\theta_n - \theta_0) + ???$$","Let $f:\mathbb{R}^m \to \mathbb{R}^n$. Is it possible to do a Taylor expansion of $f$ around $\theta\in\mathbb{R}^m$? I am hoping for something like $$f\left(\theta\right) = f\left(\theta_0\right) + A \left(\theta - \theta_0\right) + \left(\theta - \theta'\right)^T \text{something} \left(\theta - \theta'\right)$$ where $A$ is a $n\times m$ matrix, and its rows are gradient of $f_i$ ($i$-the entry of vector $f$) with respect to vector $\theta$. But what should ""something"" be? For example, let us consider the simple case where $f :\mathbb{R}^m \to \mathbb{R}^2$. Denote $f = \left(f_1,f_2\right)$. Then $$f_1\left(\theta_n\right) = f_1(\theta_0) + \nabla f_1\left(\theta_0\right)(\theta_n - \theta_0) + (\theta_n - \theta_0)^T H_1(\theta') (\theta_n - \theta_0) $$ $$f_2\left(\theta_n\right) = f_2(\theta_0) + \nabla f_2\left(\theta_0\right)(\theta_n - \theta_0) + (\theta_n - \theta_0)^T H_2(\theta') (\theta_n - \theta_0) $$ If I try to put this in matrix form, I will get something like $$f\left(\theta_0\right) = f(\theta_0) + \begin{pmatrix} \nabla f_1(\theta_0)\\ \nabla f_2(\theta_0)\end{pmatrix} (\theta_n - \theta_0) + ???$$",,"['real-analysis', 'taylor-expansion']"
64,Rational function approximation to square root,Rational function approximation to square root,,"I've already received a lot of help with this problem (thank you all), and I've written up an answer with all of the information I've got in it as to why these approximations are so good. My question is why the following rational function is such a good approximation for the square root of $x$? $$\sqrt{x}\approx\frac{256+1792x+1120 x^{2}+112 x^{3}+x^{4}}{1024+1792x+448 x^{2}+16 x^{3}}$$ The following image illustrates that for $3\leqslant x\leqslant 7$ the two curves are almost identical: More generally why does the following formula hold with such accuracy?: $$\sqrt{n}\approx\frac{m^{8}+28 m^{6}n+70 m^{4}n^{2}+28 m^{2}n^{3}+n^{4}}{8 m^{7}+56 m^{5}n+56m^{3}n^{2}+8m n^{3}}$$ for any real number ( not just integer) $n\geqslant 1$ and: $$m=\left\lfloor\frac{1+\sqrt{4n-3}}{2}\right\rfloor$$ The following graph shows that for $0\leqslant n\leqslant10000$ the two curves cannot be separated by eye: And this is the logarithm of the error in that approximation which decreases cyclically (presumably due to the floor in the definition of $m$) but then starts to become slightly less accurate stepwise as $n$ increases: ( Edit : Several people have mentioned Pade approximants below and that the first formula is a Pade approximant, which explains why it is such a good approximation. I would really especially like to know why the second formula is such a good approximation on the wide range I have shown in the second graph (is it related to Pade approximants also?), and I would really love to know why the process I outline in the section below of how I got these functions gives Pade approximants at all from ratios of binomial coefficients . At first glance, it seems as if there might be something deep going on here?) How I found these and what I already know: While messing around with maths once I noticed that $(3+\sqrt{8})^{8}\approx1331714$ with very good accuracy. Since the binomial theorem gives that $(3+\sqrt{8})^{8}=665857+235416\sqrt{8}$ and since $1331714=2\times665857$ I saw that that gave the approximation: $$\sqrt{8}\approx\frac{665857}{235416}$$ which is quite accurate. I found this intriguing, but exploring further I found that similar near integer results were produced by: $$(1+\sqrt{2})^{8}\approx1154$$ $$(2+\sqrt{3})^{8}\approx37634$$ $$(2+\sqrt{4})^{8}=65536$$ $$(2+\sqrt{5})^{8}\approx103682$$ $$(2+\sqrt{6})^{8}\approx153632$$ $$(3+\sqrt{7})^{8}\approx1032224$$ and so on, and I found that in general I could get a very accurate near integer from any (at least small) expression of the form $(m+\sqrt{n})^{8}$ where as $n$ increased over the integers $m$ increased as: $$m=\left\lfloor\frac{1+\sqrt{4n-3}}{2}\right\rfloor$$ When these expressions were expanded out they were all in the form $a+b\sqrt{n}\approx2a$ where: $$a=m^{8}+28 m^{6}n+70 m^{4}n^{2}+28 m^{2}n^{3}+n^{4}$$ $$b=8 m^{7}+56 m^{5}n+56m^{3}n^{2}+8m n^{3}$$ From this you get my original equations that I posted. Now there is nothing particularly interesting about the eighth powers I have been working with; ninth powers work as well (slightly more accurately) and seventh powers are slightly less accurate, giving a similar formula: $$\sqrt{n}\approx\frac{m^{7}+21 m^{5}n+35 m^{3} n^{2}+7m n^{3}}{7 m^{6}n+35 m^{4} n^{2}+21 m^{2} n^{3}+n^{4}}$$ Thus some of the reason behind these formulae must be the fact that the $m+\sqrt{n}$ have powers tending to integers, and just about the only thing I have been able to find out about why this might work is that some of these numbers are Pisot-Vijayaraghavan numbers (i.e. their powers tend towards integers). However, this does not explain the phenomenon, since I do not know why the effect would be greatest for $m=\left\lfloor\frac{1+\sqrt{4n-3}}{2}\right\rfloor$ or importantly why the convergence should be of the form $(m+\sqrt{n})^{c}=a+b\sqrt{n}\approx 2a$ (which is where the approximation comes from) or why the formula derived should also hold for non-integral $n$. Rather the important part seems to lie in the formulation as: $$\left(\frac{Even Binomial Terms}{Odd Binomial Terms}\right)^{\pm 1}$$ and perhaps in the expression $m=\left\lfloor\frac{1+\sqrt{4n-3}}{2}\right\rfloor$. So my question is: why do these formulae work?","I've already received a lot of help with this problem (thank you all), and I've written up an answer with all of the information I've got in it as to why these approximations are so good. My question is why the following rational function is such a good approximation for the square root of $x$? $$\sqrt{x}\approx\frac{256+1792x+1120 x^{2}+112 x^{3}+x^{4}}{1024+1792x+448 x^{2}+16 x^{3}}$$ The following image illustrates that for $3\leqslant x\leqslant 7$ the two curves are almost identical: More generally why does the following formula hold with such accuracy?: $$\sqrt{n}\approx\frac{m^{8}+28 m^{6}n+70 m^{4}n^{2}+28 m^{2}n^{3}+n^{4}}{8 m^{7}+56 m^{5}n+56m^{3}n^{2}+8m n^{3}}$$ for any real number ( not just integer) $n\geqslant 1$ and: $$m=\left\lfloor\frac{1+\sqrt{4n-3}}{2}\right\rfloor$$ The following graph shows that for $0\leqslant n\leqslant10000$ the two curves cannot be separated by eye: And this is the logarithm of the error in that approximation which decreases cyclically (presumably due to the floor in the definition of $m$) but then starts to become slightly less accurate stepwise as $n$ increases: ( Edit : Several people have mentioned Pade approximants below and that the first formula is a Pade approximant, which explains why it is such a good approximation. I would really especially like to know why the second formula is such a good approximation on the wide range I have shown in the second graph (is it related to Pade approximants also?), and I would really love to know why the process I outline in the section below of how I got these functions gives Pade approximants at all from ratios of binomial coefficients . At first glance, it seems as if there might be something deep going on here?) How I found these and what I already know: While messing around with maths once I noticed that $(3+\sqrt{8})^{8}\approx1331714$ with very good accuracy. Since the binomial theorem gives that $(3+\sqrt{8})^{8}=665857+235416\sqrt{8}$ and since $1331714=2\times665857$ I saw that that gave the approximation: $$\sqrt{8}\approx\frac{665857}{235416}$$ which is quite accurate. I found this intriguing, but exploring further I found that similar near integer results were produced by: $$(1+\sqrt{2})^{8}\approx1154$$ $$(2+\sqrt{3})^{8}\approx37634$$ $$(2+\sqrt{4})^{8}=65536$$ $$(2+\sqrt{5})^{8}\approx103682$$ $$(2+\sqrt{6})^{8}\approx153632$$ $$(3+\sqrt{7})^{8}\approx1032224$$ and so on, and I found that in general I could get a very accurate near integer from any (at least small) expression of the form $(m+\sqrt{n})^{8}$ where as $n$ increased over the integers $m$ increased as: $$m=\left\lfloor\frac{1+\sqrt{4n-3}}{2}\right\rfloor$$ When these expressions were expanded out they were all in the form $a+b\sqrt{n}\approx2a$ where: $$a=m^{8}+28 m^{6}n+70 m^{4}n^{2}+28 m^{2}n^{3}+n^{4}$$ $$b=8 m^{7}+56 m^{5}n+56m^{3}n^{2}+8m n^{3}$$ From this you get my original equations that I posted. Now there is nothing particularly interesting about the eighth powers I have been working with; ninth powers work as well (slightly more accurately) and seventh powers are slightly less accurate, giving a similar formula: $$\sqrt{n}\approx\frac{m^{7}+21 m^{5}n+35 m^{3} n^{2}+7m n^{3}}{7 m^{6}n+35 m^{4} n^{2}+21 m^{2} n^{3}+n^{4}}$$ Thus some of the reason behind these formulae must be the fact that the $m+\sqrt{n}$ have powers tending to integers, and just about the only thing I have been able to find out about why this might work is that some of these numbers are Pisot-Vijayaraghavan numbers (i.e. their powers tend towards integers). However, this does not explain the phenomenon, since I do not know why the effect would be greatest for $m=\left\lfloor\frac{1+\sqrt{4n-3}}{2}\right\rfloor$ or importantly why the convergence should be of the form $(m+\sqrt{n})^{c}=a+b\sqrt{n}\approx 2a$ (which is where the approximation comes from) or why the formula derived should also hold for non-integral $n$. Rather the important part seems to lie in the formulation as: $$\left(\frac{Even Binomial Terms}{Odd Binomial Terms}\right)^{\pm 1}$$ and perhaps in the expression $m=\left\lfloor\frac{1+\sqrt{4n-3}}{2}\right\rfloor$. So my question is: why do these formulae work?",,"['real-analysis', 'algebra-precalculus', 'binomial-coefficients', 'approximation', 'elementary-functions']"
65,"Construct a Borel set on R such that it intersect every open interval with non-zero non-""full"" measure","Construct a Borel set on R such that it intersect every open interval with non-zero non-""full"" measure",,"This is from problem $8$, Chapter II of Rudin's Real and Complex Analysis. The problem asks for a Borel set $M$ on $R$, such that for any interval $I$, $M \cap I$ has measure greater than $0$ and less than $m(I)$. I was thinking of taking the Cantor approach: taking $R$ to be the union of $[a,b]$ with $a$ and $b$ rationals, and for each $[a,b]$ we construct Cantor sets inside it. During theconstruction of each Cantor set, in order to have positive measure on it, we need to take off smaller and smaller intervals from it, namely the proportion goes to $0$. As a result, these Cantor sets are extremely ""dense"" on their ends. If for an interval $I$ it intersects with the Cantor set on $[a,b]$ while $b-a>>m(I)$, we shall expect the measure of intersection to be rather close to $m(I)$ and then we lose control on these cases. Is there any way to fix this or shall I consider other approaches? Thank you","This is from problem $8$, Chapter II of Rudin's Real and Complex Analysis. The problem asks for a Borel set $M$ on $R$, such that for any interval $I$, $M \cap I$ has measure greater than $0$ and less than $m(I)$. I was thinking of taking the Cantor approach: taking $R$ to be the union of $[a,b]$ with $a$ and $b$ rationals, and for each $[a,b]$ we construct Cantor sets inside it. During theconstruction of each Cantor set, in order to have positive measure on it, we need to take off smaller and smaller intervals from it, namely the proportion goes to $0$. As a result, these Cantor sets are extremely ""dense"" on their ends. If for an interval $I$ it intersects with the Cantor set on $[a,b]$ while $b-a>>m(I)$, we shall expect the measure of intersection to be rather close to $m(I)$ and then we lose control on these cases. Is there any way to fix this or shall I consider other approaches? Thank you",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
66,"Function $f(x)=\int_0^\infty\left|\sin(t)\cdot\sin(x\,t)\cdot e^{-t}\right|\,dt$",Function,"f(x)=\int_0^\infty\left|\sin(t)\cdot\sin(x\,t)\cdot e^{-t}\right|\,dt","Let $$f(x)=\int_0^\infty\Big|\sin(t)\cdot\sin(x\,t)\cdot e^{-t}\Big|\,dt,$$ where $|\dots|$ denotes the absolute value . We are concerned only with positive values of $x$ (i.e. let the domain of the function be $\mathbb{R}^+$). The graph of this function looks roughly as follows (modulo possible errors in numeric algorithms used to plot it): The function values at rational points can be evaluated in a closed form, e.g. $$f\left(\frac32\right)=\frac2{145}\left(24+\frac{27\,\sqrt3\,\left(e^{\frac\pi3}+e^{-\frac\pi3}\right)-24\,\left(e^{\frac\pi3}-e^{-\frac\pi3}\right)-4}{e^\pi-e^{-\pi}}\right).$$ Questions: Is the function $f(x)$ continuous? Is it smooth? Is it analytic ? How many local extrema does it have? What is the $\lim\limits_{x\,\to\,\infty}f(x)$, if it exists? Can we find a closed-form value of $f(x)$ at an explicit irrational point (given by a closed-form expression)? Is there a general formula for values of $f(x)$ at rational points?","Let $$f(x)=\int_0^\infty\Big|\sin(t)\cdot\sin(x\,t)\cdot e^{-t}\Big|\,dt,$$ where $|\dots|$ denotes the absolute value . We are concerned only with positive values of $x$ (i.e. let the domain of the function be $\mathbb{R}^+$). The graph of this function looks roughly as follows (modulo possible errors in numeric algorithms used to plot it): The function values at rational points can be evaluated in a closed form, e.g. $$f\left(\frac32\right)=\frac2{145}\left(24+\frac{27\,\sqrt3\,\left(e^{\frac\pi3}+e^{-\frac\pi3}\right)-24\,\left(e^{\frac\pi3}-e^{-\frac\pi3}\right)-4}{e^\pi-e^{-\pi}}\right).$$ Questions: Is the function $f(x)$ continuous? Is it smooth? Is it analytic ? How many local extrema does it have? What is the $\lim\limits_{x\,\to\,\infty}f(x)$, if it exists? Can we find a closed-form value of $f(x)$ at an explicit irrational point (given by a closed-form expression)? Is there a general formula for values of $f(x)$ at rational points?",,"['calculus', 'real-analysis', 'definite-integrals', 'continuity', 'analyticity']"
67,Characterization of sets of differentiability,Characterization of sets of differentiability,,"If $f : \mathbb{R} \to \mathbb{R}$, define $C(f) = \{ x : f \text{ is continuous at } x \}$ and $D(f) = \{ x : f \text{ is differentiable at } x \}$. I have seen it proved that: $C(f)$ is a $G_\delta$ set. For any $G_\delta$ set $A \subset \mathbb{R}$, there exists a function $f : \mathbb{R} \to \mathbb{R}$ such that $C(f)=A$. I suspect there is a related characterization for $D(f)$. I see that a related question was asked at Continuous functions are differentiable on a measurable set? , where the accepted answer implies that $D(f)$ is a $G_{\delta \sigma \delta}$ set. Is this optimal, that is, does there exist $f$ such that $D(f)$ is $G_{\delta \sigma \delta}$ and not $G_{\delta \sigma}$? If so, can an example be given? Conversely, given any $G_{\delta \sigma \delta}$ set $A \subset \mathbb{R}$, does there exist $f$ such that $D(f)=A$?","If $f : \mathbb{R} \to \mathbb{R}$, define $C(f) = \{ x : f \text{ is continuous at } x \}$ and $D(f) = \{ x : f \text{ is differentiable at } x \}$. I have seen it proved that: $C(f)$ is a $G_\delta$ set. For any $G_\delta$ set $A \subset \mathbb{R}$, there exists a function $f : \mathbb{R} \to \mathbb{R}$ such that $C(f)=A$. I suspect there is a related characterization for $D(f)$. I see that a related question was asked at Continuous functions are differentiable on a measurable set? , where the accepted answer implies that $D(f)$ is a $G_{\delta \sigma \delta}$ set. Is this optimal, that is, does there exist $f$ such that $D(f)$ is $G_{\delta \sigma \delta}$ and not $G_{\delta \sigma}$? If so, can an example be given? Conversely, given any $G_{\delta \sigma \delta}$ set $A \subset \mathbb{R}$, does there exist $f$ such that $D(f)=A$?",,['real-analysis']
68,Distance minimizers in $L^1$ and $L^{\infty}$,Distance minimizers in  and,L^1 L^{\infty},"If $H$ is a Hilbert space, we have the Hilbert Projection Theorem , which tells us that given a nonempty, closed, convex subset $K \subset H$, and a point $x \in H$, there is a unique point $y \in K$ which minimizes $\lVert x-y \rVert$. In the $L^{p}(X,\mathcal{X},\mu)$ spaces, for $1 < p < \infty$, we get the same result, even though these are not Hilbert spaces for $p\neq 2$ (assuming that $(X,\mathcal{X})$ is sufficiently non-trivial). This can be proved using the Hanner inequalities. I am interested in the case of $L^1$ or $L^\infty$. It is easy to construct examples where distance minimizers (in some closed, convex, nonempty subset) exist, but they are not unique. However, I am wondering whether or not existence can fail as well. I have thought about this a fair bit, and tried searching online, but I could not resolve this question. Can anyone share any insight? Thanks.","If $H$ is a Hilbert space, we have the Hilbert Projection Theorem , which tells us that given a nonempty, closed, convex subset $K \subset H$, and a point $x \in H$, there is a unique point $y \in K$ which minimizes $\lVert x-y \rVert$. In the $L^{p}(X,\mathcal{X},\mu)$ spaces, for $1 < p < \infty$, we get the same result, even though these are not Hilbert spaces for $p\neq 2$ (assuming that $(X,\mathcal{X})$ is sufficiently non-trivial). This can be proved using the Hanner inequalities. I am interested in the case of $L^1$ or $L^\infty$. It is easy to construct examples where distance minimizers (in some closed, convex, nonempty subset) exist, but they are not unique. However, I am wondering whether or not existence can fail as well. I have thought about this a fair bit, and tried searching online, but I could not resolve this question. Can anyone share any insight? Thanks.",,"['real-analysis', 'functional-analysis', 'lp-spaces']"
69,The absolute value of a Riemann integrable function is Riemann integrable.,The absolute value of a Riemann integrable function is Riemann integrable.,,"This is an exercise in Bartle & Sherbert's Introduction to Real Analysis second edition. They ask to show that if $I=[a,b]$ is a closed bounded interval and that $f:I\to\mathbb{R}$ is (Riemann) integrable on $I$, then $|f|$ is integrable on $I$. Of course we know that the composition of an integrable function with a continuous function is integrable, but here they ask to prove it directly using the inequality $$|f(x)|-|f(y)|\leq|f(x)-f(y)|,\quad\forall x,y\in I.$$ By the Riemann's Criterion, $\epsilon>0$ given, there exists a partition $P$ of $I$ such that the difference between the upper and lower sum is less than $\epsilon$, i.e.  $$U_f(P)-L_f(P)<\epsilon.$$ So if we can show that $$U_{|f|}(P)-L_{|f|}(P)\leq U_f(P)-L_f(P)$$ then we would be done. But I can't see how to prove this last inequality.","This is an exercise in Bartle & Sherbert's Introduction to Real Analysis second edition. They ask to show that if $I=[a,b]$ is a closed bounded interval and that $f:I\to\mathbb{R}$ is (Riemann) integrable on $I$, then $|f|$ is integrable on $I$. Of course we know that the composition of an integrable function with a continuous function is integrable, but here they ask to prove it directly using the inequality $$|f(x)|-|f(y)|\leq|f(x)-f(y)|,\quad\forall x,y\in I.$$ By the Riemann's Criterion, $\epsilon>0$ given, there exists a partition $P$ of $I$ such that the difference between the upper and lower sum is less than $\epsilon$, i.e.  $$U_f(P)-L_f(P)<\epsilon.$$ So if we can show that $$U_{|f|}(P)-L_{|f|}(P)\leq U_f(P)-L_f(P)$$ then we would be done. But I can't see how to prove this last inequality.",,"['real-analysis', 'integration', 'absolute-value', 'riemann-integration', 'riemann-sum']"
70,Proof of the L'Hôpital Rule for $\frac{\infty}{\infty}$,Proof of the L'Hôpital Rule for,\frac{\infty}{\infty},"I ask for the proof of the L'Hôpital rule for the indeterminate form $\frac{\infty}{\infty}$ utilizing the rule for the form $\frac{0}{0}$. Theorem: Let $f,g:(a,b)\to \mathbb{R}$ be two differentiable functions such as that:  $\forall x\in(a,b)\ \ g(x)\neq 0\text{ and }g^{\prime}(x)\neq 0$ and $\lim_{x\to a^+}f(x)=\lim_{x\to a^+}g(x)=+\infty$ If the limit $$\lim_{x\to a^+}\frac{f^{\prime}(x)}{g^{\prime}(x)}$$ exists and is finite, then  $$\lim_{x\to a^+}\frac{f(x)}{g(x)}=\lim_{x\to a^+}\frac{f^{\prime}(x)}{g^{\prime}(x)}$$ My attempt:  Since $\lim_{x\to a^+}f(x)=+\infty$, $$\exists \delta>0:a<x<a+\delta<b\Rightarrow f(x)>0\Rightarrow f(x)\neq 0$$  Let $F,G:(a,a+\delta)\to \mathbb{R}$, $F(x)=\frac{1}{f(x)}$, $G(x)=\frac{1}{g(x)}$. Then by the hypothesis $\lim_{x\to a^+}F(x)=\lim_{x\to a^+}G(x)=0$, $$\forall x\in(a,b)\ \ G(x)\neq 0\text{ and }G^{\prime}(x)=-\frac{1}{g^2(x)}g^{\prime}(x)\neq 0$$ The question is, does the limit $$\lim_{x\to a^+}\frac{F^{\prime}(x)}{G^{\prime}(x)}=\lim_{x\to a^+}\frac{-\frac{1}{f^2(x)}f^{\prime}(x)}{-\frac{1}{g^2(x)}g^{\prime}(X)}=\lim_{x\to a^+}\frac{g^2(x)f^{\prime}(x)}{f^2(x)g^{\prime}(x)}$$ exist? The limit $$\lim_{x\to a^+}\frac{f^{\prime}(x)}{g^{\prime}(x)}$$ exists by the hypothesis but we don't know if the limit $\displaystyle\lim_{x\to a^+}\frac{g^2(x)}{f^2(x)}$ exists to deduce that the limit $$\lim_{x\to a^+}\frac{F^{\prime}(x)}{G^{\prime}(x)}$$ exists to use the L'Hôpital Rule for the form $\frac{0}{0}$. EDIT: After discussing it with other users in the site, we came to the conclusion that this proof is only partial and can't logically be continued to yield the Theorem.  As a result, the rule for the $\frac{0}{0}$ form can't be used to proove the rule for the  $\frac{\infty}{\infty}$ form. Mr. Tavares and myself have already given two different proofs (with the pretty much the same main idea) of the Theorem in question using Cauchy's Mean Value Theorem. You can read them below. You can also read the proof Rudin gives for a stronger version of the Theorem (that does not suppose that $\lim_{x\to a^+}f(x)=+\infty$) in his book Principle of Meathematical Analysis. If you have any objections in either proofs please let me know. Thank you.","I ask for the proof of the L'Hôpital rule for the indeterminate form $\frac{\infty}{\infty}$ utilizing the rule for the form $\frac{0}{0}$. Theorem: Let $f,g:(a,b)\to \mathbb{R}$ be two differentiable functions such as that:  $\forall x\in(a,b)\ \ g(x)\neq 0\text{ and }g^{\prime}(x)\neq 0$ and $\lim_{x\to a^+}f(x)=\lim_{x\to a^+}g(x)=+\infty$ If the limit $$\lim_{x\to a^+}\frac{f^{\prime}(x)}{g^{\prime}(x)}$$ exists and is finite, then  $$\lim_{x\to a^+}\frac{f(x)}{g(x)}=\lim_{x\to a^+}\frac{f^{\prime}(x)}{g^{\prime}(x)}$$ My attempt:  Since $\lim_{x\to a^+}f(x)=+\infty$, $$\exists \delta>0:a<x<a+\delta<b\Rightarrow f(x)>0\Rightarrow f(x)\neq 0$$  Let $F,G:(a,a+\delta)\to \mathbb{R}$, $F(x)=\frac{1}{f(x)}$, $G(x)=\frac{1}{g(x)}$. Then by the hypothesis $\lim_{x\to a^+}F(x)=\lim_{x\to a^+}G(x)=0$, $$\forall x\in(a,b)\ \ G(x)\neq 0\text{ and }G^{\prime}(x)=-\frac{1}{g^2(x)}g^{\prime}(x)\neq 0$$ The question is, does the limit $$\lim_{x\to a^+}\frac{F^{\prime}(x)}{G^{\prime}(x)}=\lim_{x\to a^+}\frac{-\frac{1}{f^2(x)}f^{\prime}(x)}{-\frac{1}{g^2(x)}g^{\prime}(X)}=\lim_{x\to a^+}\frac{g^2(x)f^{\prime}(x)}{f^2(x)g^{\prime}(x)}$$ exist? The limit $$\lim_{x\to a^+}\frac{f^{\prime}(x)}{g^{\prime}(x)}$$ exists by the hypothesis but we don't know if the limit $\displaystyle\lim_{x\to a^+}\frac{g^2(x)}{f^2(x)}$ exists to deduce that the limit $$\lim_{x\to a^+}\frac{F^{\prime}(x)}{G^{\prime}(x)}$$ exists to use the L'Hôpital Rule for the form $\frac{0}{0}$. EDIT: After discussing it with other users in the site, we came to the conclusion that this proof is only partial and can't logically be continued to yield the Theorem.  As a result, the rule for the $\frac{0}{0}$ form can't be used to proove the rule for the  $\frac{\infty}{\infty}$ form. Mr. Tavares and myself have already given two different proofs (with the pretty much the same main idea) of the Theorem in question using Cauchy's Mean Value Theorem. You can read them below. You can also read the proof Rudin gives for a stronger version of the Theorem (that does not suppose that $\lim_{x\to a^+}f(x)=+\infty$) in his book Principle of Meathematical Analysis. If you have any objections in either proofs please let me know. Thank you.",,"['real-analysis', 'limits']"
71,Proving rigorously the supremum of a set,Proving rigorously the supremum of a set,,"Suppose $\emptyset \neq A \subset \mathbb{R} $.  Let $A = [\,0,2).\,\,$   Prove that $\sup A = 2$ This is my attempt: $A$ is the half open interval $[\,0,2)$ and so all the $x_i \in A$ look like $0 \leq x_i < 2$ so clearly $2$ is an upper bound. To show it is the ${\it least}$ upper bound, suppose that $2 \neq \sup A$, that is there exists a number $M < 2$ for some real $M$ qualifying as $\sup A$.  Certainly this $M \in [0,2) $ so $ M > 0 \Rightarrow 2 -M > 0$. By the Archimedean Principle, for all real numbers $r > 0\,\, \exists\,\, n \in \mathbb{N}$ such that $0 < \frac{1}{n} < r $. By the Approximation Property of Suprema, there exists $a \in [0,2)$ such that $\sup A - \epsilon < a \leq \sup A$, where $\epsilon > 0$. Suppose $\sup A = M < 2$.  Then the above gives $M - \epsilon < a < 2\,\,\,\,\forall \epsilon > 0$.  Also, by Archimedean, we have $0 < \frac{1}{n} < 2-M$, so choose $\epsilon = 2-M$.  Then $M - (2-M) < a < 2 \,\Rightarrow 2(M-1) < a < 2$ We can assume $M - 1>0$ and so $2(M-1) > 2$  This results in a contradiction in the previous inequality.  Hence $M < 2$ cannot be the supremum. I realise there is probably a simpler way, but is what I have written all good?","Suppose $\emptyset \neq A \subset \mathbb{R} $.  Let $A = [\,0,2).\,\,$   Prove that $\sup A = 2$ This is my attempt: $A$ is the half open interval $[\,0,2)$ and so all the $x_i \in A$ look like $0 \leq x_i < 2$ so clearly $2$ is an upper bound. To show it is the ${\it least}$ upper bound, suppose that $2 \neq \sup A$, that is there exists a number $M < 2$ for some real $M$ qualifying as $\sup A$.  Certainly this $M \in [0,2) $ so $ M > 0 \Rightarrow 2 -M > 0$. By the Archimedean Principle, for all real numbers $r > 0\,\, \exists\,\, n \in \mathbb{N}$ such that $0 < \frac{1}{n} < r $. By the Approximation Property of Suprema, there exists $a \in [0,2)$ such that $\sup A - \epsilon < a \leq \sup A$, where $\epsilon > 0$. Suppose $\sup A = M < 2$.  Then the above gives $M - \epsilon < a < 2\,\,\,\,\forall \epsilon > 0$.  Also, by Archimedean, we have $0 < \frac{1}{n} < 2-M$, so choose $\epsilon = 2-M$.  Then $M - (2-M) < a < 2 \,\Rightarrow 2(M-1) < a < 2$ We can assume $M - 1>0$ and so $2(M-1) > 2$  This results in a contradiction in the previous inequality.  Hence $M < 2$ cannot be the supremum. I realise there is probably a simpler way, but is what I have written all good?",,"['real-analysis', 'proof-verification', 'proof-writing', 'supremum-and-infimum']"
72,Radon-Nikodym derivative of product measure,Radon-Nikodym derivative of product measure,,"For $j=1,2$, let $\nu_{j},\mu_{j}$ be $\sigma$-finite measures on $(X_{j},\mathcal{M}_{j})$ such that $\nu_{j}\ll\mu_{j}$. I want to show that $\nu_{1}\times\nu_{2}\ll\mu_{1}\times\mu_{2}$ and that $\frac{d(\nu_{1}\times\nu_{2})}{d(\mu_{1}\times\mu_{2})}(x_{1},x_{2})=\frac{d\nu_{1}}{d\mu_{1}}(x_{1})\frac{d\nu_{2}}{d\mu_{2}}(x_{2})$. I tried to show that if $E\in\mathcal{M}_{1}\otimes\mathcal{M}_{2}$ and $\mu_{1}\times\mu_{2}(E)=0$, then $\nu_{1}\times\nu_{2}(E)=0$. This criterion holds for measurable rectangles so then I tried considering $\{E\in\mathcal{M}_{1}\otimes\mathcal{M}_{2}|\mu_{1}\times\mu_{2}(E)>0\;\text{or}\;\nu_{1}\times\nu_{2}(E)=0\}$ and show that it is a $\sigma$-algebra containing all measurable rectangles but I guess it didn't quite work.","For $j=1,2$, let $\nu_{j},\mu_{j}$ be $\sigma$-finite measures on $(X_{j},\mathcal{M}_{j})$ such that $\nu_{j}\ll\mu_{j}$. I want to show that $\nu_{1}\times\nu_{2}\ll\mu_{1}\times\mu_{2}$ and that $\frac{d(\nu_{1}\times\nu_{2})}{d(\mu_{1}\times\mu_{2})}(x_{1},x_{2})=\frac{d\nu_{1}}{d\mu_{1}}(x_{1})\frac{d\nu_{2}}{d\mu_{2}}(x_{2})$. I tried to show that if $E\in\mathcal{M}_{1}\otimes\mathcal{M}_{2}$ and $\mu_{1}\times\mu_{2}(E)=0$, then $\nu_{1}\times\nu_{2}(E)=0$. This criterion holds for measurable rectangles so then I tried considering $\{E\in\mathcal{M}_{1}\otimes\mathcal{M}_{2}|\mu_{1}\times\mu_{2}(E)>0\;\text{or}\;\nu_{1}\times\nu_{2}(E)=0\}$ and show that it is a $\sigma$-algebra containing all measurable rectangles but I guess it didn't quite work.",,['real-analysis']
73,Does the everywhere differentiability of $f$ imply it is absolutely continuous on a compact interval?,Does the everywhere differentiability of  imply it is absolutely continuous on a compact interval?,f,"Suppose $f$ is differentiable everywhere on $[0,1]$. Must $f$ be absolutely continuous on $[0,1]$? I know this is true if $f'$ is integrable but I'm not sure in this more general case.","Suppose $f$ is differentiable everywhere on $[0,1]$. Must $f$ be absolutely continuous on $[0,1]$? I know this is true if $f'$ is integrable but I'm not sure in this more general case.",,"['real-analysis', 'measure-theory']"
74,Spreading points in the unit interval to maximize the product of pairwise distances,Spreading points in the unit interval to maximize the product of pairwise distances,,"This is prompted by question 15312 , but moved to the reals.  It must be solved already.  Pick n points $x_i \in [0,1]$ to maximize $\prod_{i < j} (x_i - x_j)$.  A little playing shows you don't want them evenly distributed-they need to push out to the ends.  With four points, Alpha says to use $\{0,\frac{1}{2}\pm\frac{1}{2\sqrt{5}},1\}$ and with five, $\{0,\frac{1}{2}-\frac{\sqrt{\frac{3}{7}}}{2},\frac{1}{2},\frac{1}{2}+\frac{\sqrt{\frac{3}{7}}}{2},1\}$","This is prompted by question 15312 , but moved to the reals.  It must be solved already.  Pick n points $x_i \in [0,1]$ to maximize $\prod_{i < j} (x_i - x_j)$.  A little playing shows you don't want them evenly distributed-they need to push out to the ends.  With four points, Alpha says to use $\{0,\frac{1}{2}\pm\frac{1}{2\sqrt{5}},1\}$ and with five, $\{0,\frac{1}{2}-\frac{\sqrt{\frac{3}{7}}}{2},\frac{1}{2},\frac{1}{2}+\frac{\sqrt{\frac{3}{7}}}{2},1\}$",,"['real-analysis', 'analysis']"
75,Do differentiable functions preserve measure zero sets? Measurable sets?,Do differentiable functions preserve measure zero sets? Measurable sets?,,"Consider Lebesgue measure on $\mathbb{R}$ and let $f:\mathbb{R}\to\mathbb{R}$ be differentiable.  Does $f$ necessarily preserve measure zero sets?  Does $f$ necessarily preserve measurable sets? Note that if $f$ is $C^1$ then $f$ preserves measure zero sets since $C^1$ functions are locally Lipschitz.  Therefore $C^1$ functions also preserve measurable sets since a measurable set is the union of an $F_\sigma$ set and an measure zero set, and continuous functions preserve $F_\sigma$ sets.  More generally if $f$ is absolutely continuous on each interval then $f$ preserves both measure zero sets and measurable sets. However, I'm not sure about the differentiable case.  I would guess that the answer to both questions is no.  I'm interested in a counter example or proof in each case.","Consider Lebesgue measure on $\mathbb{R}$ and let $f:\mathbb{R}\to\mathbb{R}$ be differentiable.  Does $f$ necessarily preserve measure zero sets?  Does $f$ necessarily preserve measurable sets? Note that if $f$ is $C^1$ then $f$ preserves measure zero sets since $C^1$ functions are locally Lipschitz.  Therefore $C^1$ functions also preserve measurable sets since a measurable set is the union of an $F_\sigma$ set and an measure zero set, and continuous functions preserve $F_\sigma$ sets.  More generally if $f$ is absolutely continuous on each interval then $f$ preserves both measure zero sets and measurable sets. However, I'm not sure about the differentiable case.  I would guess that the answer to both questions is no.  I'm interested in a counter example or proof in each case.",,"['real-analysis', 'measure-theory', 'examples-counterexamples']"
76,Various kinds of derivatives,Various kinds of derivatives,,"Let $f\colon \mathbb{R}\to \mathbb{R}$ be a measurable function. Let us introduce the following notions of ""derivative"" of $f$ . Classical derivative . The unique function $f'_c$ defined pointwise by the following: $$\lim_{h\to 0} \frac{ f(x+h)-f(x)}{h}- f'_c(x)=0,\qquad \forall x \in \mathbb{R},$$ provided that the limit exists at all points. $L^p$ derivative . For a fixed $p\in (1, \infty)$ , the unique function $f'_p$ such that $$\lim_{h\to 0} \int_{-\infty}^\infty \left\lvert \frac{f(x+h)-f(x)}{h}-f'_p(x)\right\rvert^p\, dx=0,$$ provided that $f\in L^p$ and that such a function $f'_p$ exists. Distributional derivative . The unique distribution $f'_d$ such that $$\int_{-\infty}^\infty f(x)\phi'(x)\, dx=-\langle f'_d, \phi\rangle, \qquad \forall \phi \in C^\infty_0(\mathbb{R}),$$ provided that $f$ defines a distribution (i.e., $f\in L^1_{\mathrm{loc}}$ ). The vague version of my question is: to what extent are these definitions mutually consistent? More precisely: Suppose that $f'_c$ exists (at all points) and $f'_c \in L^p$ . Is it true that $f'_p$ exists and $f'_c=f'_p$ ? Suppose that $f$ defines a distribution, that $f'_c$ exists at all points and that $f'_c$ defines a distribution. Is it true that $f'_d=f'_c$ ? Suppose that $f'_p$ exists. Is it true that $f'_p=f'_d$ ? Suppose that $f'_d$ is a continuous function. Is is true that $f'_c$ exists and $f'_c=f'_d$ ? ( Suggested by Tomasz in comments ) Suppose that $f'_d\in L^p$ . Is it true that $f'_p$ exists and that $f'_p=f'_d$ ? P.S. : Some information on this topic, and especially on question 3, can be found on the book An introduction to nonlinear dispersive equations by F.Linares and G.Ponce, Springer Universitext. Look for Exercise 1.9 on page 21.","Let be a measurable function. Let us introduce the following notions of ""derivative"" of . Classical derivative . The unique function defined pointwise by the following: provided that the limit exists at all points. derivative . For a fixed , the unique function such that provided that and that such a function exists. Distributional derivative . The unique distribution such that provided that defines a distribution (i.e., ). The vague version of my question is: to what extent are these definitions mutually consistent? More precisely: Suppose that exists (at all points) and . Is it true that exists and ? Suppose that defines a distribution, that exists at all points and that defines a distribution. Is it true that ? Suppose that exists. Is it true that ? Suppose that is a continuous function. Is is true that exists and ? ( Suggested by Tomasz in comments ) Suppose that . Is it true that exists and that ? P.S. : Some information on this topic, and especially on question 3, can be found on the book An introduction to nonlinear dispersive equations by F.Linares and G.Ponce, Springer Universitext. Look for Exercise 1.9 on page 21.","f\colon \mathbb{R}\to \mathbb{R} f f'_c \lim_{h\to 0} \frac{ f(x+h)-f(x)}{h}- f'_c(x)=0,\qquad \forall x \in \mathbb{R}, L^p p\in (1, \infty) f'_p \lim_{h\to 0} \int_{-\infty}^\infty \left\lvert \frac{f(x+h)-f(x)}{h}-f'_p(x)\right\rvert^p\, dx=0, f\in L^p f'_p f'_d \int_{-\infty}^\infty f(x)\phi'(x)\, dx=-\langle f'_d, \phi\rangle, \qquad \forall \phi \in C^\infty_0(\mathbb{R}), f f\in L^1_{\mathrm{loc}} f'_c f'_c \in L^p f'_p f'_c=f'_p f f'_c f'_c f'_d=f'_c f'_p f'_p=f'_d f'_d f'_c f'_c=f'_d f'_d\in L^p f'_p f'_p=f'_d","['real-analysis', 'measure-theory', 'distribution-theory', 'lp-spaces']"
77,Understanding the proof of $l_2$ being complete.,Understanding the proof of  being complete.,l_2,"Let $l_2$ be the collection of bounded real sequences $x = (x_n)$ for which $\sum_{n=1}^{\infty}|x_n|^2<\infty$. I have to prove that $l_2$ is complete (that every Cauchy sequence in $l_2$ converges to a point in $l_2$). Proof: Let $(f_n)$ be a sequence in $l_2$, where now we write $f_n = (f_n(k))_{k=1}^{\infty}$, and suppose that $(f_n)$ is Cauchy in $l_2$. That is, suppose that for each $\epsilon > 0$ there is a $n_0$ such that $||f_n - f_m||_2 < \epsilon$ whenever $m,n \geq n_0$. We now want to show that $(f_n)$ converges, in the metric of $l_2$, to some $f\in l_2$. 1) First show that $f(k) = lim_{n\to \infty}\,f_n(k)$ exists in $\mathbb{R}$ for each k: To see why, note that $|\,f_n(k) - f_m(k)|\leq ||\,f_n - f_m||_2$ for any k, and hence $(f_n(k))_{k=1}^{\infty}$ is Cauchy in $\mathbb{R}$ for each k. Thus, $f$ is the obvious candidate for the limit of $(f_n)$, but we still have to show that the convergence takes place in the metric space $l_2$; that is, we need to show that $f\in l_2$ and that $||\,f_n - f||_2 \to 0$ (as $n \to \infty$). 2) Now show that $f\in l_2$; that is, $||\,f||_2 < \infty$. We know that $(f_n)$ is bounded in $l_2$; say, $||f_n||\leq B$ for all n. Thus, for any fixed $N < \infty$, we have: $\sum\limits_{k=1}^{N}|\,f(k)|^2 = lim_{n\to\infty}\sum\limits_{k=1}^{N}|\,f_n(k)|^2 \leq B^2$. Since this holds for any N, we get that $||\,f||_2\leq B$. 3) Now we repeat step 2 (more or less) to show that $f_n \to f$ in $l_2$. Given any $\epsilon > 0$ choose $n_0$ such that $||\,f_n - f_m||_2 < \epsilon$ whenever $m,n > n_0$. Then, for any N and any $n\geq n_0$, $\sum\limits_{k=1}^{N}|\,f(k) - f_n(k)|^2 = lim_{n \to \infty}\sum\limits_{k = 1}^{N}|\,f_m(k) - f_n(k)|^2 \leq \epsilon^2$. Since this holds for any N, we have $||\,f - f_n||_2 \leq \epsilon$ for all $n\geq n_0$. That is, $f_n \to f$ in $l_2$. Questions: I think I understand the general idea of the proof. You take a sequence and suppose it's Cauchy. You then have to show that it has a limit and thus converges, and show that the limit lies in $l_2$, which would mean that any Cauchy sequence in $l_2$ is convergent to a point in $l_2$. So why is step 3 necessary? I must be mistaken, but it seems to me that in step 1 and 2 it has already been proven that the chosen sequence has a limit and that it lies in $l_2$. I don't understand the notation that is used in this proof: ""Let $(f_n)$ be a sequence in $l_2$, where we now write $f_n = (f_n(k))_{k=1}^{\infty}$"". Why is the letter $k$ added to the sequence and what does it mean? If you have a sequence $x_n$, the subscript is the argument right? Can't you write $x_n$ like $x\,(n)$? Why does $|\,f_n(k) - f_m(k)|\leq||\,f_n - f_m||_2$ imply that $f_n(k)$ has a limit? Perhaps this will be clear once I understand the use of the letter $k$. Thanks in advance!","Let $l_2$ be the collection of bounded real sequences $x = (x_n)$ for which $\sum_{n=1}^{\infty}|x_n|^2<\infty$. I have to prove that $l_2$ is complete (that every Cauchy sequence in $l_2$ converges to a point in $l_2$). Proof: Let $(f_n)$ be a sequence in $l_2$, where now we write $f_n = (f_n(k))_{k=1}^{\infty}$, and suppose that $(f_n)$ is Cauchy in $l_2$. That is, suppose that for each $\epsilon > 0$ there is a $n_0$ such that $||f_n - f_m||_2 < \epsilon$ whenever $m,n \geq n_0$. We now want to show that $(f_n)$ converges, in the metric of $l_2$, to some $f\in l_2$. 1) First show that $f(k) = lim_{n\to \infty}\,f_n(k)$ exists in $\mathbb{R}$ for each k: To see why, note that $|\,f_n(k) - f_m(k)|\leq ||\,f_n - f_m||_2$ for any k, and hence $(f_n(k))_{k=1}^{\infty}$ is Cauchy in $\mathbb{R}$ for each k. Thus, $f$ is the obvious candidate for the limit of $(f_n)$, but we still have to show that the convergence takes place in the metric space $l_2$; that is, we need to show that $f\in l_2$ and that $||\,f_n - f||_2 \to 0$ (as $n \to \infty$). 2) Now show that $f\in l_2$; that is, $||\,f||_2 < \infty$. We know that $(f_n)$ is bounded in $l_2$; say, $||f_n||\leq B$ for all n. Thus, for any fixed $N < \infty$, we have: $\sum\limits_{k=1}^{N}|\,f(k)|^2 = lim_{n\to\infty}\sum\limits_{k=1}^{N}|\,f_n(k)|^2 \leq B^2$. Since this holds for any N, we get that $||\,f||_2\leq B$. 3) Now we repeat step 2 (more or less) to show that $f_n \to f$ in $l_2$. Given any $\epsilon > 0$ choose $n_0$ such that $||\,f_n - f_m||_2 < \epsilon$ whenever $m,n > n_0$. Then, for any N and any $n\geq n_0$, $\sum\limits_{k=1}^{N}|\,f(k) - f_n(k)|^2 = lim_{n \to \infty}\sum\limits_{k = 1}^{N}|\,f_m(k) - f_n(k)|^2 \leq \epsilon^2$. Since this holds for any N, we have $||\,f - f_n||_2 \leq \epsilon$ for all $n\geq n_0$. That is, $f_n \to f$ in $l_2$. Questions: I think I understand the general idea of the proof. You take a sequence and suppose it's Cauchy. You then have to show that it has a limit and thus converges, and show that the limit lies in $l_2$, which would mean that any Cauchy sequence in $l_2$ is convergent to a point in $l_2$. So why is step 3 necessary? I must be mistaken, but it seems to me that in step 1 and 2 it has already been proven that the chosen sequence has a limit and that it lies in $l_2$. I don't understand the notation that is used in this proof: ""Let $(f_n)$ be a sequence in $l_2$, where we now write $f_n = (f_n(k))_{k=1}^{\infty}$"". Why is the letter $k$ added to the sequence and what does it mean? If you have a sequence $x_n$, the subscript is the argument right? Can't you write $x_n$ like $x\,(n)$? Why does $|\,f_n(k) - f_m(k)|\leq||\,f_n - f_m||_2$ imply that $f_n(k)$ has a limit? Perhaps this will be clear once I understand the use of the letter $k$. Thanks in advance!",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'continuity']"
78,Continuity almost everywhere,Continuity almost everywhere,,"Let $$f(x) = \left\{\begin{array}{ll} 1 & \text{if }x\in\mathbb{Q},\\ 0 & \text{if }x\notin\mathbb{Q}. \end{array}\right.$$ $$g(x) = \left\{\begin{array}{ll} \frac{1}{n} & \text{if }x = \frac{m}{n}\in\mathbb{Q},\\ 0 & \text{if }x\notin\mathbb{Q}. \end{array}\right.$$ I have some difficulties with understanding of continuity almost everywhere . Honestly, I thought that continuity almost everywhere means that I could take any continuous function and change (or even make them undefined) its values for countable set of points from domain, say, for $\mathbb{Q}$. Based on such thinking, both $f(x), g(x)$ are continuous almost everywhere, because I could get them as a result of change of countable values of $h(x)  \equiv 0$ The fact is that only $g(x)$ is continuous almost everywhere, while $f(x)$ is discontinuous. Could you explain what went wrong in my understanding?","Let $$f(x) = \left\{\begin{array}{ll} 1 & \text{if }x\in\mathbb{Q},\\ 0 & \text{if }x\notin\mathbb{Q}. \end{array}\right.$$ $$g(x) = \left\{\begin{array}{ll} \frac{1}{n} & \text{if }x = \frac{m}{n}\in\mathbb{Q},\\ 0 & \text{if }x\notin\mathbb{Q}. \end{array}\right.$$ I have some difficulties with understanding of continuity almost everywhere . Honestly, I thought that continuity almost everywhere means that I could take any continuous function and change (or even make them undefined) its values for countable set of points from domain, say, for $\mathbb{Q}$. Based on such thinking, both $f(x), g(x)$ are continuous almost everywhere, because I could get them as a result of change of countable values of $h(x)  \equiv 0$ The fact is that only $g(x)$ is continuous almost everywhere, while $f(x)$ is discontinuous. Could you explain what went wrong in my understanding?",,"['real-analysis', 'measure-theory']"
79,Find $\lim_{n \to \infty}\frac{x_n}{\sqrt{n}}$ where $x_{n+1}=x_n+\frac{n}{x_1+x_2+\cdots+x_n}$,Find  where,\lim_{n \to \infty}\frac{x_n}{\sqrt{n}} x_{n+1}=x_n+\frac{n}{x_1+x_2+\cdots+x_n},"Assume a positive sequence $\{x_n\} $ satisfies $$x_{n+1}=x_n+\frac{n}{x_1+x_2+\cdots+x_n}.$$ Find $\lim\limits_{n \to \infty}\dfrac{x_n}{\sqrt{n}}$ . Assume the limit we want is $L$ . Then by Stolz theorem, one can obtain \begin{align*} L&=\lim_{n \to \infty}\frac{x_n}{\sqrt{n}}=\lim_{n \to \infty}\frac{x_{n+1}-x_n}{\sqrt{n+1}-\sqrt{n}}=\lim_{n \to \infty}\frac{2n\sqrt{n}}{x_1+x_2+\cdots+x_n}\\ &=\lim_{n \to \infty}\frac{2n\sqrt{n}-2(n-1)\sqrt{n-1}}{x_n}=\lim_{n \to \infty}\frac{3\sqrt{n}}{x_n}=\frac{3}{L}, \end{align*} which implies $L=\sqrt{3}$ . But how to prove the limit exists?","Assume a positive sequence satisfies Find . Assume the limit we want is . Then by Stolz theorem, one can obtain which implies . But how to prove the limit exists?","\{x_n\}  x_{n+1}=x_n+\frac{n}{x_1+x_2+\cdots+x_n}. \lim\limits_{n \to \infty}\dfrac{x_n}{\sqrt{n}} L \begin{align*}
L&=\lim_{n \to \infty}\frac{x_n}{\sqrt{n}}=\lim_{n \to \infty}\frac{x_{n+1}-x_n}{\sqrt{n+1}-\sqrt{n}}=\lim_{n \to \infty}\frac{2n\sqrt{n}}{x_1+x_2+\cdots+x_n}\\
&=\lim_{n \to \infty}\frac{2n\sqrt{n}-2(n-1)\sqrt{n-1}}{x_n}=\lim_{n \to \infty}\frac{3\sqrt{n}}{x_n}=\frac{3}{L},
\end{align*} L=\sqrt{3}","['real-analysis', 'sequences-and-series', 'limits', 'recurrence-relations']"
80,How does one prove L'Hôpital's rule?,How does one prove L'Hôpital's rule?,,"L'Hôpital's rule  can be stated as follows: Let $f, g$ be differentiable real functions defined on a deleted one-sided neighbourhood $^{(1)}$ of $a$ , where $a$ can be any real number or $\pm \infty$ . Suppose that both $f,g$ converge to $0$ or that both $f,g$ converge to $+\infty$ as $x \to a^{\pm}$ ( $\pm$ depending on the side of the deleted neighbourhood). If $$\frac{f'(x)}{g'(x)} \to L,$$ then $$\frac{f(x)}{g(x)} \to L,$$ where $L$ can be any real number or $\pm \infty$ . This is an ubiquitous tool for computations of limits, and some books avoid proving it or just prove it in some special cases. Since we don't seem to have a consistent reference for its statement and proof in MathSE and it is a theorem which is often misapplied (see here for an example), it seems valuable to have a question which could serve as such a reference. This is an attempt at that. $^{(1)}$ E.g., if $a=1$ , then $(1,3)$ is such a neighbourhood.","L'Hôpital's rule  can be stated as follows: Let be differentiable real functions defined on a deleted one-sided neighbourhood of , where can be any real number or . Suppose that both converge to or that both converge to as ( depending on the side of the deleted neighbourhood). If then where can be any real number or . This is an ubiquitous tool for computations of limits, and some books avoid proving it or just prove it in some special cases. Since we don't seem to have a consistent reference for its statement and proof in MathSE and it is a theorem which is often misapplied (see here for an example), it seems valuable to have a question which could serve as such a reference. This is an attempt at that. E.g., if , then is such a neighbourhood.","f, g ^{(1)} a a \pm \infty f,g 0 f,g +\infty x \to a^{\pm} \pm \frac{f'(x)}{g'(x)} \to L, \frac{f(x)}{g(x)} \to L, L \pm \infty ^{(1)} a=1 (1,3)","['real-analysis', 'calculus', 'derivatives']"
81,Finite-dimensional subspace normed vector space is closed,Finite-dimensional subspace normed vector space is closed,,"I know that probably this question has already been answered, but I'd like to present my attempt of solution. Let $(E,\|\cdot\|)$ be a normed vector space and let $F\subseteq E$ be a finite-dimensional subspace. Suppose that $\dim_{\mathbb{R}} F=n$ . First of all I proved the following lemma. Lemma 1 Every finite-dimensional normed vector space is complete. Then, I noticed that $(F,\|\cdot\|_{F})$ is a finite-dimensional normed vector space, where $\|\cdot\|_{F}$ is the induced norm. So, $F$ is complete by Lemma 1 . Lemma 2 If $(E,\|\cdot\|)$ is a normed vector space such that $\dim_{\mathbb{R}}{E}=n$ , then $E$ is algebraically and topologically isomorphic to $\mathbb{R}^{n}$ . By Lemma 2 , I can view $F$ as a complete subspace of $\mathbb{R}^n$ and, since $\mathbb{R}^n$ is itself complete, I can conclude that $F$ is closed. Here I used the fact that every complete subspace of a complete space is closed. Is there something wrong?","I know that probably this question has already been answered, but I'd like to present my attempt of solution. Let be a normed vector space and let be a finite-dimensional subspace. Suppose that . First of all I proved the following lemma. Lemma 1 Every finite-dimensional normed vector space is complete. Then, I noticed that is a finite-dimensional normed vector space, where is the induced norm. So, is complete by Lemma 1 . Lemma 2 If is a normed vector space such that , then is algebraically and topologically isomorphic to . By Lemma 2 , I can view as a complete subspace of and, since is itself complete, I can conclude that is closed. Here I used the fact that every complete subspace of a complete space is closed. Is there something wrong?","(E,\|\cdot\|) F\subseteq E \dim_{\mathbb{R}} F=n (F,\|\cdot\|_{F}) \|\cdot\|_{F} F (E,\|\cdot\|) \dim_{\mathbb{R}}{E}=n E \mathbb{R}^{n} F \mathbb{R}^n \mathbb{R}^n F","['real-analysis', 'general-topology', 'functional-analysis', 'vector-spaces', 'normed-spaces']"
82,Countability of local maxima on continuous real-valued functions,Countability of local maxima on continuous real-valued functions,,"I am working through a bank of previous exams and couldn't figure a problem out to my satisfaction. Let $f(x) : \mathbb{R} \to \mathbb{R}\,$ be a continuous function. Show that $f$ can have at most countably many strict local maxima. Assume that $f$ is not monotone on any interval. Then show that the local   maxima of $f$ are dense in   $\mathbb{R}$.","I am working through a bank of previous exams and couldn't figure a problem out to my satisfaction. Let $f(x) : \mathbb{R} \to \mathbb{R}\,$ be a continuous function. Show that $f$ can have at most countably many strict local maxima. Assume that $f$ is not monotone on any interval. Then show that the local   maxima of $f$ are dense in   $\mathbb{R}$.",,"['real-analysis', 'analysis']"
83,Can we have a power density but not a natural density?,Can we have a power density but not a natural density?,,"For $M \subset \mathbb{N}$ (in this post I follow the convention $\min \mathbb{N} = 1$) and $\alpha \in [0,1]$ define $$S_{M,\alpha}(x) = \sum_{\substack{n\in M \\ n \leqslant x}} \frac{1}{n^{\alpha}}$$ on $[1,+\infty)$. For sets $A \subset B \subset \mathbb{N}$ with $S_{B,\alpha}(x) \to +\infty$ as $x\to +\infty$, we define the upper $\alpha$-density of $A$ in $B$ as $$\overline{D}_{\alpha}(A;B) = \limsup_{x\to +\infty} \frac{S_{A,\alpha}(x)}{S_{B,\alpha}(x)}.$$ The lower $\alpha$-density $\underline{D}_{\alpha}(A;B)$ is defined analogously (using $\liminf$ instead of $\limsup$). We say that $A$ has an $\alpha$-density in $B$ if $\overline{D}_{\alpha}(A;B) = \underline{D}_{\alpha}(A;B)$, and then denote that value with $D_{\alpha}(A;B)$. For $\alpha = 0$ we have the familiar natural density (also called asymptotic density) and for $\alpha = 1$ the logarithmic density. The cases $\alpha \in (0,1)$ shall be called ""power densities"". Via summation by parts, it is straightforward to show that when $B$ is substantial, i.e. $\lim\limits_{x\to +\infty} S_{B,1}(x) = +\infty$, $$\underline{D}_{\alpha}(A;B) \leqslant \underline{D}_{\beta}(A;B) \leqslant \overline{D}_{\beta}(A;B) \leqslant \overline{D}_{\alpha}(A;B)$$ for all $A \subset B$ and $0 \leqslant \alpha < \beta \leqslant 1$. So if $A$ has an $\alpha$-density in $B$ for some $\alpha < 1$, then $D_{\beta}(A;B)$ exists for all $\beta \in [\alpha,1]$ and all these densities coincide. And for example the set $A_1$ of positive integers whose first (decimal) digit is $1$ has logarithmic density $\frac{\log 2}{\log 10}$ in $\mathbb{N}$, but $\underline{D}_{\alpha}(A_1;\mathbb{N}) < \overline{D}_{\alpha}(A_1;\mathbb{N})$ for all $\alpha \in [0,1)$. So the question arises whether there is a substantial set $B$ and a subset $A$ such that $A$ has no natural density in $B$, but $D_{\alpha}(A;B)$ exists for some $\alpha \in (0,1)$. Again via summation by parts it is easy to see that for well-behaved $B$ - e.g. $\underline{D}_0(B;\mathbb{N}) > 0$, or such that $\frac{S_{B,0}(x)}{x}$ tends to $0$ in a nice fashion, like for the set of primes - the existence of $D_{\alpha}(A;B)$ for an $\alpha < 1$ implies the existence of $D_0(A;B)$. But I did not find a proof of that for arbitrary substantial $B$, nor did I manage to find an example where $D_{\alpha}(A;B)$ exists for some $\alpha \in (0,1)$ and $D_0(A;B)$ doesn't exist. So: Do there exist $A \subset B \subset \mathbb{N}$ such that $B$ is substantial, $A$ doesn't have a natural density in $B$ and $D_{\alpha}(A;B)$ exists for an $\alpha \in (0,1)$?","For $M \subset \mathbb{N}$ (in this post I follow the convention $\min \mathbb{N} = 1$) and $\alpha \in [0,1]$ define $$S_{M,\alpha}(x) = \sum_{\substack{n\in M \\ n \leqslant x}} \frac{1}{n^{\alpha}}$$ on $[1,+\infty)$. For sets $A \subset B \subset \mathbb{N}$ with $S_{B,\alpha}(x) \to +\infty$ as $x\to +\infty$, we define the upper $\alpha$-density of $A$ in $B$ as $$\overline{D}_{\alpha}(A;B) = \limsup_{x\to +\infty} \frac{S_{A,\alpha}(x)}{S_{B,\alpha}(x)}.$$ The lower $\alpha$-density $\underline{D}_{\alpha}(A;B)$ is defined analogously (using $\liminf$ instead of $\limsup$). We say that $A$ has an $\alpha$-density in $B$ if $\overline{D}_{\alpha}(A;B) = \underline{D}_{\alpha}(A;B)$, and then denote that value with $D_{\alpha}(A;B)$. For $\alpha = 0$ we have the familiar natural density (also called asymptotic density) and for $\alpha = 1$ the logarithmic density. The cases $\alpha \in (0,1)$ shall be called ""power densities"". Via summation by parts, it is straightforward to show that when $B$ is substantial, i.e. $\lim\limits_{x\to +\infty} S_{B,1}(x) = +\infty$, $$\underline{D}_{\alpha}(A;B) \leqslant \underline{D}_{\beta}(A;B) \leqslant \overline{D}_{\beta}(A;B) \leqslant \overline{D}_{\alpha}(A;B)$$ for all $A \subset B$ and $0 \leqslant \alpha < \beta \leqslant 1$. So if $A$ has an $\alpha$-density in $B$ for some $\alpha < 1$, then $D_{\beta}(A;B)$ exists for all $\beta \in [\alpha,1]$ and all these densities coincide. And for example the set $A_1$ of positive integers whose first (decimal) digit is $1$ has logarithmic density $\frac{\log 2}{\log 10}$ in $\mathbb{N}$, but $\underline{D}_{\alpha}(A_1;\mathbb{N}) < \overline{D}_{\alpha}(A_1;\mathbb{N})$ for all $\alpha \in [0,1)$. So the question arises whether there is a substantial set $B$ and a subset $A$ such that $A$ has no natural density in $B$, but $D_{\alpha}(A;B)$ exists for some $\alpha \in (0,1)$. Again via summation by parts it is easy to see that for well-behaved $B$ - e.g. $\underline{D}_0(B;\mathbb{N}) > 0$, or such that $\frac{S_{B,0}(x)}{x}$ tends to $0$ in a nice fashion, like for the set of primes - the existence of $D_{\alpha}(A;B)$ for an $\alpha < 1$ implies the existence of $D_0(A;B)$. But I did not find a proof of that for arbitrary substantial $B$, nor did I manage to find an example where $D_{\alpha}(A;B)$ exists for some $\alpha \in (0,1)$ and $D_0(A;B)$ doesn't exist. So: Do there exist $A \subset B \subset \mathbb{N}$ such that $B$ is substantial, $A$ doesn't have a natural density in $B$ and $D_{\alpha}(A;B)$ exists for an $\alpha \in (0,1)$?",,"['real-analysis', 'analytic-number-theory', 'summation-by-parts']"
84,Fekete's lemma for real functions,Fekete's lemma for real functions,,"The following result, which I know under the name Fekete's lemma is quite often useful. It was, for example, used in this answer: Existence of a limit associated to an almost subadditive sequence . If $(a_n)_{n=0}^\infty$ is a subadditive sequence of real numbers, i.e., $$(\forall m,n) a_{m+n} \le a_m + a_n,$$  then $$\lim\limits_{n\to\infty} \frac{a_n}n = \inf_n \frac{a_n}n.$$ Some references are given in Wikipedia article, the original Fekete's paper is available here . Basically the exponential version (for submultiplicative sequences) can be shown in a similar way as Satz II in this paper. I was wondering, whether some analogous claim is true for functions. I.e. something like: Whenever $f:{(0,\infty)}\to{\mathbb R}$ fulfills $$(\forall x,y)f(x+y) \le f(x)+f(y)$$ (i.e., it is subadditive), then  $$\lim\limits_{x\to\infty} \frac{f(x)}x = \inf_x \frac{f(x)}x.$$ (In particular, the above limit exists -- if we include the possibility $-\infty$.) Clearly, this is not true without any additional assumptions on $f$. (E.g. if $f$ is any non-linear solution of Cauchy's equation , then $\liminf \frac{f(x)}x < \limsup \frac{f(x)}x$ and $f$ is both subadditive and superadditive. Probably even much simpler examples can be given.) On the other hand, if $f$ is well-behaved, the above claim is true. If I assume that $f$ is bounded on intervals of the form $(0,x]$, then I can basically repeat the proof which is given for sequences here . So my question is: Under what assumptions on $f$ the above claim is true. Can you give some references for this claim? EDIT: I found a result which shows that measurability of $f$ is sufficient and added this result as an answer. I think this is sufficient for most applications and my guess is that there is not much space to improve this result. However, I will wait a little bit before accepting my own answer - just in case someone would like to add some interesting information or further useful references. I have accepted my own answer, but if you have some interesting information which you can add, I'll be very glad to learn about it.","The following result, which I know under the name Fekete's lemma is quite often useful. It was, for example, used in this answer: Existence of a limit associated to an almost subadditive sequence . If $(a_n)_{n=0}^\infty$ is a subadditive sequence of real numbers, i.e., $$(\forall m,n) a_{m+n} \le a_m + a_n,$$  then $$\lim\limits_{n\to\infty} \frac{a_n}n = \inf_n \frac{a_n}n.$$ Some references are given in Wikipedia article, the original Fekete's paper is available here . Basically the exponential version (for submultiplicative sequences) can be shown in a similar way as Satz II in this paper. I was wondering, whether some analogous claim is true for functions. I.e. something like: Whenever $f:{(0,\infty)}\to{\mathbb R}$ fulfills $$(\forall x,y)f(x+y) \le f(x)+f(y)$$ (i.e., it is subadditive), then  $$\lim\limits_{x\to\infty} \frac{f(x)}x = \inf_x \frac{f(x)}x.$$ (In particular, the above limit exists -- if we include the possibility $-\infty$.) Clearly, this is not true without any additional assumptions on $f$. (E.g. if $f$ is any non-linear solution of Cauchy's equation , then $\liminf \frac{f(x)}x < \limsup \frac{f(x)}x$ and $f$ is both subadditive and superadditive. Probably even much simpler examples can be given.) On the other hand, if $f$ is well-behaved, the above claim is true. If I assume that $f$ is bounded on intervals of the form $(0,x]$, then I can basically repeat the proof which is given for sequences here . So my question is: Under what assumptions on $f$ the above claim is true. Can you give some references for this claim? EDIT: I found a result which shows that measurability of $f$ is sufficient and added this result as an answer. I think this is sufficient for most applications and my guess is that there is not much space to improve this result. However, I will wait a little bit before accepting my own answer - just in case someone would like to add some interesting information or further useful references. I have accepted my own answer, but if you have some interesting information which you can add, I'll be very glad to learn about it.",,"['real-analysis', 'reference-request']"
85,Theorem about multiplicity set of continuous functions.,Theorem about multiplicity set of continuous functions.,,"Relevant Theorems: $(a)$ There is no continuous function $f$ on $\mathbb{R}$ which takes on every value exactly twice. $(b)$ There is no continuous function $f$ on $\mathbb{R}$ which takes on each value either $0$ times or $2$ times. $(c)$ Find a continuous function $f$ on $\mathbb{R}$ which takes on every value exactly $3$ times. $(d)$ There is no continuous function $f$ on $\mathbb{R}$ which takes on every value exactly $2n$ times , for all $n \in \mathbb{N}$ . Source: Spivak's ""Calculus"" Definitions: Fix a function $f:\mathbb{R}\to\mathbb{R}$ such that $f$ obtains each value only finite (possibly $0$ ) number of times. We say $E \subset \mathbb{N}$ is ""the multiplicity set"" of $f$ if $1$ . For every $n \in E$ , there exists at least one value that $f$ takes exactly $n$ times. $2$ . If $f$ takes on a value exactly $n$ times, then $n \in E$ In other words, $n \in E$ iff $n$ is the multiplicity of some value under $f$ . A subset $E $ of $N$ is said to be ""constructable"" iff there exists a continuous function $f$ on $\mathbb{R}$ such that $E$ is the multiplicity set of $f$ . The problem: With these definitions, it seems like the theorems from Spivak are just telling that the sets $\{2\},\{0,2\},\{2n\}$ cannot be constructed, and, we can construct $\{3\}$ . After this, we can also construct sets like $\{0,1,4\},\{0,2,4\},\{0,1,2,4\},\{4,5\}$ etc. So far, I have got one theorem about the construction of some sets. For every continuous function $f$ on $\mathbb{R}$ with multiplicity set $E$ , if $E$ has maximum $2n$ then $E$ includes $0$ and some member of $\{1,..,n\}$ . This theorem tells us that sets like $\{0,3,4\}$ are impossible, but doesn't tell anything about sets with odd maximums. So far I don't have anymore theorems to decide whether a set is constructable or not. Hence, my question is: If $E$ is a finite subset of $\mathbb{N}$ , then is there a way to effectively & efficiently decide whether or not $E$ is constructable or not?","Relevant Theorems: There is no continuous function on which takes on every value exactly twice. There is no continuous function on which takes on each value either times or times. Find a continuous function on which takes on every value exactly times. There is no continuous function on which takes on every value exactly times , for all . Source: Spivak's ""Calculus"" Definitions: Fix a function such that obtains each value only finite (possibly ) number of times. We say is ""the multiplicity set"" of if . For every , there exists at least one value that takes exactly times. . If takes on a value exactly times, then In other words, iff is the multiplicity of some value under . A subset of is said to be ""constructable"" iff there exists a continuous function on such that is the multiplicity set of . The problem: With these definitions, it seems like the theorems from Spivak are just telling that the sets cannot be constructed, and, we can construct . After this, we can also construct sets like etc. So far, I have got one theorem about the construction of some sets. For every continuous function on with multiplicity set , if has maximum then includes and some member of . This theorem tells us that sets like are impossible, but doesn't tell anything about sets with odd maximums. So far I don't have anymore theorems to decide whether a set is constructable or not. Hence, my question is: If is a finite subset of , then is there a way to effectively & efficiently decide whether or not is constructable or not?","(a) f \mathbb{R} (b) f \mathbb{R} 0 2 (c) f \mathbb{R} 3 (d) f \mathbb{R} 2n n \in \mathbb{N} f:\mathbb{R}\to\mathbb{R} f 0 E \subset \mathbb{N} f 1 n \in E f n 2 f n n \in E n \in E n f E  N f \mathbb{R} E f \{2\},\{0,2\},\{2n\} \{3\} \{0,1,4\},\{0,2,4\},\{0,1,2,4\},\{4,5\} f \mathbb{R} E E 2n E 0 \{1,..,n\} \{0,3,4\} E \mathbb{N} E","['real-analysis', 'continuity']"
86,Reconciling several different definitions of Radon measures,Reconciling several different definitions of Radon measures,,"Upon reviewing some basic real analysis I have encountered two different definitions for Radon measure. Let the underlying space $X$ be locally compact and Hausdorff. Folland's Real Analysis gives the definition A Radon measure is a Borel measure that is finite on all compact sets, outer regular on Borel sets, and inner regular on open sets. Folland goes on to prove that a Radon measure is inner regular on $\sigma$-finite sets, and remarks that full inner regularity is too much to ask for, especially in the context of the Riesz representation theorem for positive linear functionals on $C_c(X)$. Folland's approach seems to match the approach taken by Rudin, if I recall. However, I've heard from others, as well as Wikipedia , that a Radon measure is defined as a Borel measure that is locally finite (which means finite on compact sets for LCH spaces) and inner regular, and no mention of outer regularity. Neither definition seems to connect well with Bourbaki's approach of defining Radon measures as positive linear functionals on $C_c(X)$, because, at least according to Wikipedia's article on the Riesz representation theorem , a positive linear functional on $C_c(X)$ uniquely corresponds to a regular Borel measure, which is stronger than Radon in either of the two definitions given above. Sadly I do not have any more advanced analysis treatises to compare against, so I was hoping somebody could clear up this discrepancy.","Upon reviewing some basic real analysis I have encountered two different definitions for Radon measure. Let the underlying space $X$ be locally compact and Hausdorff. Folland's Real Analysis gives the definition A Radon measure is a Borel measure that is finite on all compact sets, outer regular on Borel sets, and inner regular on open sets. Folland goes on to prove that a Radon measure is inner regular on $\sigma$-finite sets, and remarks that full inner regularity is too much to ask for, especially in the context of the Riesz representation theorem for positive linear functionals on $C_c(X)$. Folland's approach seems to match the approach taken by Rudin, if I recall. However, I've heard from others, as well as Wikipedia , that a Radon measure is defined as a Borel measure that is locally finite (which means finite on compact sets for LCH spaces) and inner regular, and no mention of outer regularity. Neither definition seems to connect well with Bourbaki's approach of defining Radon measures as positive linear functionals on $C_c(X)$, because, at least according to Wikipedia's article on the Riesz representation theorem , a positive linear functional on $C_c(X)$ uniquely corresponds to a regular Borel measure, which is stronger than Radon in either of the two definitions given above. Sadly I do not have any more advanced analysis treatises to compare against, so I was hoping somebody could clear up this discrepancy.",,"['real-analysis', 'functional-analysis', 'measure-theory']"
87,"Simple recurrences converging to $\log 2, \pi, e, \sqrt{2}$ and so on",Simple recurrences converging to  and so on,"\log 2, \pi, e, \sqrt{2}","See my question at the bottom of this post. The recurrence $P(n) x_{n+2} = Q(n)x_{n+1} - R(n)x_n$ , where $P(n), Q(n), R(n)$ are  polynomials of degree $1$ , sometimes leads to interesting results. Probably the most basic cases are: For $\log\alpha$ : $$P(n) = \alpha (n+2), Q(n) = (2\alpha-1)(n+1)+\alpha, R(n)=(\alpha-1)(n+1)$$ $$\mbox{with } x_1=\frac{\alpha-1}{\alpha}, x_2 = \frac{(\alpha-1) (3\alpha-1)}{2\alpha^2}$$ We have $\lim_{n\rightarrow\infty} x_n = \log\alpha$ . The convergence is fastest when $\alpha$ is close to $1$ . The related recurrence $$P(n) = 1, Q(n) = (2\alpha-1)(n+1)+\alpha, R(n)=(\alpha-1)\alpha(n+1)^2$$ $$\mbox{with } x_1=\alpha-1, x_2=(\alpha-1)(3\alpha-1)$$ yields $$\lim_{n\rightarrow\infty} \frac{x_n}{\alpha^n n!} = \log\alpha$$ and in addition $x_n$ is an integer if $\alpha>0$ is an integer. For $\exp \alpha$ : $$P(n) = n+2, Q(n) = n+2+\alpha, R(n)=\alpha$$ $$\mbox{with } x_0=1, x_1 = 1+\alpha$$ We have $\lim_{n\rightarrow\infty} x_n = \exp\alpha$ . The related recurrence $$P(n) = 1, Q(n) = n+2+\alpha, R(n)=\alpha(n+1)$$ $$\mbox{with } x_0=1, x_1=1+\alpha$$ yields $$\lim_{n\rightarrow\infty} \frac{x_n}{n!} = \exp\alpha$$ and in addition $x_n$ is an integer if $\alpha$ is an integer. For $\sqrt{2}$ : $$P(n) = 4(n+2), Q(n) = 6n+11, R(n)=2n+3$$ $$\mbox{with } x_0=1, x_1 = \frac{5}{4}$$ We have $\lim_{n\rightarrow\infty} x_n = \sqrt{2}$ . The related recurrence $$P(n) = n+2, Q(n) = 2(6n+11), R(n)=16(2n+3)$$ $$\mbox{with } x_0=1, x_1=10$$ yields $$\lim_{n\rightarrow\infty} \frac{x_n}{8^n} = \sqrt{2}$$ and in addition $x_n$ is an integer. Comment These formulas (and tons of other similar formulas) are easy to obtain, yet I could not find any reference in the literature. It would be interesting to see if one is available for $\gamma$ (the Euler Mascheroni constant), but I don't think so. Also, what happens when you change the initial conditions? What if you replace the recurrence by its equivalent differential equation, for instance $$(x+2) f(x) - (x+2+\alpha) f'(x) + \alpha f''(x) =0$$ corresponding to the case $\exp\alpha$ ? Generalization to arbitrary initial values As an example, here is what happens to the very first formula (the $\log \alpha$ case), if we change the initial conditions $x_1=\frac{\alpha-1}{\alpha}, x_2 = \frac{(\alpha-1) (3\alpha-1)}{2\alpha^2}$ to arbitrary values $x_1 = A, x_2=B$ , assuming here that $\alpha=2$ : $$\lim_{n\rightarrow\infty} x_n = (5-8\log \alpha)\cdot A + (8\log \alpha -4) \cdot B.$$ You may try proving this formula. It was obtained empirically, I haven't proved it. And it works only if $\alpha = 2$ . For $\alpha \neq 2$ , and also for the case $\sqrt{2}$ , a general formula is $$\lim_{n\rightarrow\infty} x_n = c_1 A + c_2 B$$ where $c_1, c_2$ are constants not depending on the initial conditions. This might be a general property of these converging linear recurrences (at least those involving polynomials of degree one). Another property, shared by the converging systems described here, is as follows: $$A = B \Rightarrow \lim_{n\rightarrow\infty} x_n = A.$$ This implies that $c_1+c_2 = 1$ . How to obtain these recursions? The case $\sqrt{2}$ can be derived from this other question . To me, it is the most interesting case as it allows you to study the digits of $\sqrt{2}$ in base 2. Some of these recursions can be computed with WolframAlpha, see here for the exponential case, and here for $\sqrt{2}$ . Numerous other recurrences, with much faster convergence, can be derived from combinatorial sums featured in this WA article . My question I am looking for some literature on these linear, non-homogeneous second order recurrences involving polynomials of degree $1$ . Also, I will accept any answer for a recurrence that yields $\pi$ . Should be easy, using formulas (37) or (38) in this article as a starting point. If you find my question too easy, here is one that could be much less easy: change the initial conditions to $x_0=A, x_1=B$ in any of these formulas, and see if you can get convergence to a known mathematical constant.","See my question at the bottom of this post. The recurrence , where are  polynomials of degree , sometimes leads to interesting results. Probably the most basic cases are: For : We have . The convergence is fastest when is close to . The related recurrence yields and in addition is an integer if is an integer. For : We have . The related recurrence yields and in addition is an integer if is an integer. For : We have . The related recurrence yields and in addition is an integer. Comment These formulas (and tons of other similar formulas) are easy to obtain, yet I could not find any reference in the literature. It would be interesting to see if one is available for (the Euler Mascheroni constant), but I don't think so. Also, what happens when you change the initial conditions? What if you replace the recurrence by its equivalent differential equation, for instance corresponding to the case ? Generalization to arbitrary initial values As an example, here is what happens to the very first formula (the case), if we change the initial conditions to arbitrary values , assuming here that : You may try proving this formula. It was obtained empirically, I haven't proved it. And it works only if . For , and also for the case , a general formula is where are constants not depending on the initial conditions. This might be a general property of these converging linear recurrences (at least those involving polynomials of degree one). Another property, shared by the converging systems described here, is as follows: This implies that . How to obtain these recursions? The case can be derived from this other question . To me, it is the most interesting case as it allows you to study the digits of in base 2. Some of these recursions can be computed with WolframAlpha, see here for the exponential case, and here for . Numerous other recurrences, with much faster convergence, can be derived from combinatorial sums featured in this WA article . My question I am looking for some literature on these linear, non-homogeneous second order recurrences involving polynomials of degree . Also, I will accept any answer for a recurrence that yields . Should be easy, using formulas (37) or (38) in this article as a starting point. If you find my question too easy, here is one that could be much less easy: change the initial conditions to in any of these formulas, and see if you can get convergence to a known mathematical constant.","P(n) x_{n+2} = Q(n)x_{n+1} - R(n)x_n P(n), Q(n), R(n) 1 \log\alpha P(n) = \alpha (n+2), Q(n) = (2\alpha-1)(n+1)+\alpha, R(n)=(\alpha-1)(n+1) \mbox{with } x_1=\frac{\alpha-1}{\alpha}, x_2 = \frac{(\alpha-1) (3\alpha-1)}{2\alpha^2} \lim_{n\rightarrow\infty} x_n = \log\alpha \alpha 1 P(n) = 1, Q(n) = (2\alpha-1)(n+1)+\alpha, R(n)=(\alpha-1)\alpha(n+1)^2 \mbox{with } x_1=\alpha-1, x_2=(\alpha-1)(3\alpha-1) \lim_{n\rightarrow\infty} \frac{x_n}{\alpha^n n!} = \log\alpha x_n \alpha>0 \exp \alpha P(n) = n+2, Q(n) = n+2+\alpha, R(n)=\alpha \mbox{with } x_0=1, x_1 = 1+\alpha \lim_{n\rightarrow\infty} x_n = \exp\alpha P(n) = 1, Q(n) = n+2+\alpha, R(n)=\alpha(n+1) \mbox{with } x_0=1, x_1=1+\alpha \lim_{n\rightarrow\infty} \frac{x_n}{n!} = \exp\alpha x_n \alpha \sqrt{2} P(n) = 4(n+2), Q(n) = 6n+11, R(n)=2n+3 \mbox{with } x_0=1, x_1 = \frac{5}{4} \lim_{n\rightarrow\infty} x_n = \sqrt{2} P(n) = n+2, Q(n) = 2(6n+11), R(n)=16(2n+3) \mbox{with } x_0=1, x_1=10 \lim_{n\rightarrow\infty} \frac{x_n}{8^n} = \sqrt{2} x_n \gamma (x+2) f(x) - (x+2+\alpha) f'(x) + \alpha f''(x) =0 \exp\alpha \log \alpha x_1=\frac{\alpha-1}{\alpha}, x_2 = \frac{(\alpha-1) (3\alpha-1)}{2\alpha^2} x_1 = A, x_2=B \alpha=2 \lim_{n\rightarrow\infty} x_n = (5-8\log \alpha)\cdot A + (8\log \alpha -4) \cdot B. \alpha = 2 \alpha \neq 2 \sqrt{2} \lim_{n\rightarrow\infty} x_n = c_1 A + c_2 B c_1, c_2 A = B \Rightarrow \lim_{n\rightarrow\infty} x_n = A. c_1+c_2 = 1 \sqrt{2} \sqrt{2} \sqrt{2} 1 \pi x_0=A, x_1=B","['real-analysis', 'limits', 'recurrence-relations', 'irrational-numbers']"
88,Practicality of the Lebesgue integral,Practicality of the Lebesgue integral,,"I am getting pretty frustrated with the Lebesgue integral mainly because it seems highly impractical to calculate anything non-trivial. Whenever I look for a concrete calculation all I see are encomiums about how wonderful it is and then invariably the only concrete calculation is the Dirichlet function where unsurprisingly the measures are easy to calculate. When a run of the mill function is offered to be calculated ,I have seen one of two kinds of  responses: 1) The answer involves a trick that can't be generalized 2) The answer given is  "" The function is Riemann integrable so just use that"" i.e forget about the Lebesgue integral. The only method I have seen that aspires to practicality is to use the monotone convergence theorem i.e get a bunch of simple functions whose limit is the function you want to integrate. Integrate them and take the limit. I have tried this for $x^2$ and I run into hard sums which are summed by guess what...help of the Riemann integral.(could be that I chose an inconvenient set of simple functions but that partly proves my point-very easy to make things complicated) So is the Lebesgue integral mostly used in formal situations and then occasionally some highly pathological function is pulled to justify the work? Are there examples where the Lebesgue integral is of practical importance and there can be no recourse to the Riemann integral?  Highly discontinuous functions are not welcome.","I am getting pretty frustrated with the Lebesgue integral mainly because it seems highly impractical to calculate anything non-trivial. Whenever I look for a concrete calculation all I see are encomiums about how wonderful it is and then invariably the only concrete calculation is the Dirichlet function where unsurprisingly the measures are easy to calculate. When a run of the mill function is offered to be calculated ,I have seen one of two kinds of  responses: 1) The answer involves a trick that can't be generalized 2) The answer given is  "" The function is Riemann integrable so just use that"" i.e forget about the Lebesgue integral. The only method I have seen that aspires to practicality is to use the monotone convergence theorem i.e get a bunch of simple functions whose limit is the function you want to integrate. Integrate them and take the limit. I have tried this for $x^2$ and I run into hard sums which are summed by guess what...help of the Riemann integral.(could be that I chose an inconvenient set of simple functions but that partly proves my point-very easy to make things complicated) So is the Lebesgue integral mostly used in formal situations and then occasionally some highly pathological function is pulled to justify the work? Are there examples where the Lebesgue integral is of practical importance and there can be no recourse to the Riemann integral?  Highly discontinuous functions are not welcome.",,"['real-analysis', 'lebesgue-integral']"
89,Prove $s$ is a supremum iff for all $\epsilon>0$ there exists $a\in A$ such that $a>s-\epsilon$.,Prove  is a supremum iff for all  there exists  such that .,s \epsilon>0 a\in A a>s-\epsilon,"Let $A\subseteq\Bbb{R}$ is a nonempty set and $s\in \Bbb{R}$ is an upper bound. Prove $s$ is the supremum iff for all $\epsilon>0$ there exists $a\in A$ such that $a>s-\epsilon$. This is important to me to know I am correct in my attempt, because I always grant more when I do try. Attempt: Suppose for all $\epsilon>0$ there exists $a\in A$ such that $a>s-\epsilon$. Assume $s$ is not the supremum. Then, since the supremum exists, let $s_1$ be the supremum. By the density of the real number,there exists $c$ such that $s_1<c<s$. Let us pick $\epsilon_1$ such that $s=c+\epsilon_1$ $\color{red}\Rightarrow c=s-\epsilon_1$ $\color{red}{\Rightarrow} s_1<s-\epsilon_1<s$ $\color{red}{\Rightarrow}$ $s>s_1+\epsilon_1$. Since for $\epsilon_1$ there exists $a\in A$ such that $a>s-\epsilon_1$. In particular $a>s_1+\epsilon_1-\epsilon_1$ meaning $a>s_1$. A contradiction. Therefore, $s$ is the supremum. Now, suppose, $s$ is the supremum. Assume there exists $\epsilon$ such that for all $a\in A$, $a<s-\epsilon$. That would mean that $s-\epsilon$ is an upper bound that is smaller than $s$. A contradiction. Therefore, for all $\epsilon>0$ there exists $a\in A$ such that $a>s-\epsilon$.","Let $A\subseteq\Bbb{R}$ is a nonempty set and $s\in \Bbb{R}$ is an upper bound. Prove $s$ is the supremum iff for all $\epsilon>0$ there exists $a\in A$ such that $a>s-\epsilon$. This is important to me to know I am correct in my attempt, because I always grant more when I do try. Attempt: Suppose for all $\epsilon>0$ there exists $a\in A$ such that $a>s-\epsilon$. Assume $s$ is not the supremum. Then, since the supremum exists, let $s_1$ be the supremum. By the density of the real number,there exists $c$ such that $s_1<c<s$. Let us pick $\epsilon_1$ such that $s=c+\epsilon_1$ $\color{red}\Rightarrow c=s-\epsilon_1$ $\color{red}{\Rightarrow} s_1<s-\epsilon_1<s$ $\color{red}{\Rightarrow}$ $s>s_1+\epsilon_1$. Since for $\epsilon_1$ there exists $a\in A$ such that $a>s-\epsilon_1$. In particular $a>s_1+\epsilon_1-\epsilon_1$ meaning $a>s_1$. A contradiction. Therefore, $s$ is the supremum. Now, suppose, $s$ is the supremum. Assume there exists $\epsilon$ such that for all $a\in A$, $a<s-\epsilon$. That would mean that $s-\epsilon$ is an upper bound that is smaller than $s$. A contradiction. Therefore, for all $\epsilon>0$ there exists $a\in A$ such that $a>s-\epsilon$.",,"['calculus', 'real-analysis', 'proof-verification', 'solution-verification']"
90,Residue Proof of Fourier's Theorem Dirichlet Conditions,Residue Proof of Fourier's Theorem Dirichlet Conditions,,"Whittaker gives two proofs of Fourier's theorem, assuming Dirichlet's conditions. One proof is Dirichlet's proof, which involves directly summing the partial sums, is found in many books. The other proof is an absolutely stunning proof of Fourier's theorem in terms of residues, treating the partial sums as the residues of a meromorphic function and showing that, on taking the limit, we end up with Dirichlet's conditions. My question is about understanding the latter half of the residue proof, given here . The jist of the proof is to consider a trigonometric series with real coefficients, assume the coefficients are Fourier coefficients of a function $f$, and then simplify the partial sum \begin{align} S_k(f) &= a_0 + \sum_{m=1}^k (a_m \cos(mz) + b_m \sin(mz)) \\ &= \frac{1}{2 \pi} \int_0^{2 \pi} f(t)dt  + \frac{1}{\pi} \sum_{m=1}^k  \int_0^{2 \pi} f(t)\cos[m(z-t)] dt \\ &= \sum_{m=-k}^k  \frac{1}{2\pi} \int_0^{2 \pi} f(t)e^{im(z-t)} dt \\ &=  \sum_{m=-k}^k  \frac{1}{2\pi} \int_0^z f(t)e^{im(z-t)} dt + \sum_{m=-k}^k  \frac{1}{2\pi} \int_z^{2 \pi} f(t)e^{im(z-t)} dt \\ &= U_k + V_k. \end{align} Next we try to turn $U_k$ into the sum of the residues of a meromorphic function derived from this, so try to modify it: \begin{align} U_k(z) &= \sum_{m=-k}^k  \frac{1}{2\pi} \int_0^z f(t)e^{im(z-t)} dt  \\ &= \sum_{m=-k}^k  \frac{w}{2\pi w} \int_0^z f(t)e^{w(z-t)} dt  |_{w = im, m \neq 0} \\ &= \sum_{m=-k}^k  \frac{w}{1 + 2\pi w  - 1} \int_0^z f(t)e^{w(z-t)} dt |_{w = im, m \neq 0} \\ &\to  \frac{1}{1 + 2\pi w + \dots - 1} \int_0^z f(t)e^{w(z-t)} dt  \\ &=  \frac{1}{e^{2 \pi w} - 1} \int_0^z f(t)e^{w(z-t)} dt \end{align} to find $$\phi(w) =  \frac{1}{e^{2 \pi w} - 1} \int_0^z f(t)e^{w(z-t)} dt$$ so that, if $C_k$ is a circle in the $w$ plane containing $0,i,-i,2i,-2i,\dots,ki,-ki$ and no more poles, say of radius $k+1/2$, we see $$ \frac{1}{2 \pi i} \int_{C_k} \phi(w) dw = U_k.$$ From this we integrate over the boundary explicitly via $w = (k + 1/2)e^{i\theta}$ so that $U_k$ reduces to $$U_k = \frac{1}{2 \pi} \int_0^{2 \pi} w \phi(w) d \theta$$ and from here on we are supposed to end up with Dirichlet's conditions. Can anybody explain the rest of the proof? Since this aspect of the proof seems to be the crux of other flawed proofs, need to make sure I get the rest of it with no hand-waving, seems unmotivated.","Whittaker gives two proofs of Fourier's theorem, assuming Dirichlet's conditions. One proof is Dirichlet's proof, which involves directly summing the partial sums, is found in many books. The other proof is an absolutely stunning proof of Fourier's theorem in terms of residues, treating the partial sums as the residues of a meromorphic function and showing that, on taking the limit, we end up with Dirichlet's conditions. My question is about understanding the latter half of the residue proof, given here . The jist of the proof is to consider a trigonometric series with real coefficients, assume the coefficients are Fourier coefficients of a function $f$, and then simplify the partial sum \begin{align} S_k(f) &= a_0 + \sum_{m=1}^k (a_m \cos(mz) + b_m \sin(mz)) \\ &= \frac{1}{2 \pi} \int_0^{2 \pi} f(t)dt  + \frac{1}{\pi} \sum_{m=1}^k  \int_0^{2 \pi} f(t)\cos[m(z-t)] dt \\ &= \sum_{m=-k}^k  \frac{1}{2\pi} \int_0^{2 \pi} f(t)e^{im(z-t)} dt \\ &=  \sum_{m=-k}^k  \frac{1}{2\pi} \int_0^z f(t)e^{im(z-t)} dt + \sum_{m=-k}^k  \frac{1}{2\pi} \int_z^{2 \pi} f(t)e^{im(z-t)} dt \\ &= U_k + V_k. \end{align} Next we try to turn $U_k$ into the sum of the residues of a meromorphic function derived from this, so try to modify it: \begin{align} U_k(z) &= \sum_{m=-k}^k  \frac{1}{2\pi} \int_0^z f(t)e^{im(z-t)} dt  \\ &= \sum_{m=-k}^k  \frac{w}{2\pi w} \int_0^z f(t)e^{w(z-t)} dt  |_{w = im, m \neq 0} \\ &= \sum_{m=-k}^k  \frac{w}{1 + 2\pi w  - 1} \int_0^z f(t)e^{w(z-t)} dt |_{w = im, m \neq 0} \\ &\to  \frac{1}{1 + 2\pi w + \dots - 1} \int_0^z f(t)e^{w(z-t)} dt  \\ &=  \frac{1}{e^{2 \pi w} - 1} \int_0^z f(t)e^{w(z-t)} dt \end{align} to find $$\phi(w) =  \frac{1}{e^{2 \pi w} - 1} \int_0^z f(t)e^{w(z-t)} dt$$ so that, if $C_k$ is a circle in the $w$ plane containing $0,i,-i,2i,-2i,\dots,ki,-ki$ and no more poles, say of radius $k+1/2$, we see $$ \frac{1}{2 \pi i} \int_{C_k} \phi(w) dw = U_k.$$ From this we integrate over the boundary explicitly via $w = (k + 1/2)e^{i\theta}$ so that $U_k$ reduces to $$U_k = \frac{1}{2 \pi} \int_0^{2 \pi} w \phi(w) d \theta$$ and from here on we are supposed to end up with Dirichlet's conditions. Can anybody explain the rest of the proof? Since this aspect of the proof seems to be the crux of other flawed proofs, need to make sure I get the rest of it with no hand-waving, seems unmotivated.",,"['real-analysis', 'complex-analysis', 'fourier-analysis', 'fourier-series', 'residue-calculus']"
91,A Proof with no words that $\sqrt{2+\sqrt{2+\sqrt{2+\cdots}}}=2$,A Proof with no words that,\sqrt{2+\sqrt{2+\sqrt{2+\cdots}}}=2,"Question What are the words to describe the method in the image below? (from Nelsen's Proofs without Words II ) Attempt I was thinking and could define the sequence $u_1=2; u_{n+1}=f\circ g^{−1}(u_n)$ where $f(x)=\sqrt x$ and $g(x)=x−2$ , as suggested by image, and thus, by the graph, it suggests that the succession defined in this way, is increasing and that $u_n<2$ , therefore increased, which was concluded to be convergent. Once it converges, let $l\in\mathbb R$ be its limit. As $\lim u_n=\lim u_{n+1}$ , since for the limit we are only interested in terms starting from a certain order, and that $f\circ g^{−1}$ is a continuous function, a composition of continuous functions is continuous, in the respective domains, we can conclude $\lim u_{n+1}=\lim f(g^{−1}(u_n))\iff l=\sqrt{l+2}$ and thus $l=−1$ or $l=2$ , but $−1$ does not belong to the domain of the function. What do you think ? Am I complicating? I believe there will be an easier way out, although that's the idea. Thanks in advance.","Question What are the words to describe the method in the image below? (from Nelsen's Proofs without Words II ) Attempt I was thinking and could define the sequence where and , as suggested by image, and thus, by the graph, it suggests that the succession defined in this way, is increasing and that , therefore increased, which was concluded to be convergent. Once it converges, let be its limit. As , since for the limit we are only interested in terms starting from a certain order, and that is a continuous function, a composition of continuous functions is continuous, in the respective domains, we can conclude and thus or , but does not belong to the domain of the function. What do you think ? Am I complicating? I believe there will be an easier way out, although that's the idea. Thanks in advance.",u_1=2; u_{n+1}=f\circ g^{−1}(u_n) f(x)=\sqrt x g(x)=x−2 u_n<2 l\in\mathbb R \lim u_n=\lim u_{n+1} f\circ g^{−1} \lim u_{n+1}=\lim f(g^{−1}(u_n))\iff l=\sqrt{l+2} l=−1 l=2 −1,"['real-analysis', 'dynamical-systems', 'nested-radicals', 'proof-without-words', 'cobweb-diagram']"
92,Mathematical meaning of certain integrals in physics,Mathematical meaning of certain integrals in physics,,"While studying on texts of physics I notice that differentiation under the integral sign is usually introduced without any comment on the conditions permitting to do so. In that case, I take care of thinking about what the author is assuming and the usual assumption made in physics that all the functions are of class $C^\infty$, at least piecewise on compact subsets, often is enough to guarantee the liceity of freely commutating the derivative and integral signs. While studying the derivation of Ampère's law from the Biot-Savart law, someting has surprised me in this proof which seems to be ubiquitous on line and in cartaceous texts. In fact the magnetic field in a point $\mathbf{x}$ is $$\mathbf{B}(\mathbf{x}):=\frac{\mu_0}{4\pi}\iiint_V\mathbf{J}(\mathbf{l})\times\frac{\mathbf{x}-\mathbf{l}}{\|\mathbf{x}-\mathbf{l}\|^3}d^3l=\frac{\mu_0}{4\pi}\iiint_V\nabla_x\times\left[\frac{\mathbf{J}(\mathbf{l})}{\|\mathbf{x}-\mathbf{l}\|}\right]d^3l$$where I would prove the identity of the integrands at both members by considering the derivatives as... well, ordinary derivatives. I keep Wikipedia's notation except for $\mathbf{x}$, which is more common as a variable, and the norm sign, for which I have always seen $\|\cdot\|$ elsewhere. Then we can notice that the proof uses a differentiation under the integral sign (at $(1)$ below): since $\nabla_x\times\left[\nabla_x\times\left[\frac{\mathbf{J}(\mathbf{l})}{\|\mathbf{x}-\mathbf{l}\|}\right]\right]=\nabla_x\left[\nabla_x\cdot\left[\frac{\mathbf{J}(\mathbf{l})}{\|\mathbf{x}-\mathbf{l}\|}\right]\right]-\nabla_x^2\left[\frac{\mathbf{J}(\mathbf{l})}{\|\mathbf{x}-\mathbf{l}\|}\right]=\nabla_x\left[\mathbf{J}(\mathbf{l})\cdot\nabla_x\left[\frac{1}{\|\mathbf{x}-\mathbf{l}\|}\right]\right]$ $-\nabla^2\left[\frac{1}{\|\mathbf{x}-\mathbf{l}\|}\right]\mathbf{J}(\mathbf{l})$, where I would calculate the derivatives as ordinarily understood, again, we have that$$\nabla_x\times\mathbf{B}(\mathbf{x})=\nabla_x\times\left[\frac{\mu_0}{4\pi}\iiint_V\nabla_x\times\left[\frac{\mathbf{J}(\mathbf{l})}{\|\mathbf{x}-\mathbf{l}\|}\right]d^3l\right]$$$$=\frac{\mu_0}{4\pi}\iiint_V\nabla_x\times\left[\nabla_x\times\left[\frac{\mathbf{J}(\mathbf{l})}{\|\mathbf{x}-\mathbf{l}\|}\right]\right]d^3l\quad(1)$$$$=\frac{\mu_0}{4\pi}\iiint_V\nabla_x\left[\mathbf{J}(\mathbf{l})\cdot\nabla_x\left[\frac{1}{\|\mathbf{x}-\mathbf{l}\|}\right]\right]-\nabla_x^2\left[\frac{1}{\|\mathbf{x}-\mathbf{l}\|}\right]\mathbf{J}(\mathbf{l})\,d^3l$$and then the integral is split as licit for Riemann, and Lebesgue, integrals when both integrands are integrable, and the gradient and integral signs are commutated in the first of the two resulting integrals to get$$\frac{\mu_0}{4\pi}\nabla_x\iiint_V\mathbf{J}(\mathbf{l})\cdot\nabla_x\left[\frac{1}{\|\mathbf{x}-\mathbf{l}\|}\right]d^3l-\frac{\mu_0}{4\pi}\iiint_V\nabla_l^2\left[\frac{1}{\|\mathbf{x}-\mathbf{l}\|}\right]\mathbf{J}(\mathbf{l})\,d^3l$$where the first addend is $\mathbf{0}$ (I do not understand how it is calculated, but that is not the main focus of my question) and where the identity $\nabla_l^2\left[\frac{1}{\|\mathbf{x}-\mathbf{l}\|}\right]=-4\pi\delta(\mathbf{x}-\mathbf{l})$, where the derivatives are this time intended as derivatives of a distribution, is used to get$$-\frac{\mu_0}{4\pi}\iiint_V\nabla_l^2\left[\frac{1}{\|\mathbf{x}-\mathbf{l}\|}\right]\mathbf{J}(\mathbf{l})\,d^3l=\mu_0\mathbf{J}(\mathbf{x}).$$ Everything of my reasoning seemed to me to work by assuming $V\subset\mathbb{R}^3$ to be compact and such that $\mathbf{x}\notin V$ and intending the integral $\iiint...d^3l$ to be a Riemann (or Lebesgue, which, in that case, I think to be the same) integral, but at this last step I see that it was not what I thought. What are, then, the integrals appearing in such calculations? They cannot be Riemann integrals, as far as I understand, because then it must be $\mathbf{x}\notin V$ and then $\iiint_V\nabla_l^2\left[\frac{1}{\|\mathbf{x}-\mathbf{l}\|}\right]\mathbf{J}(\mathbf{l})\,d^3l=\mathbf{0}$, and they cannot be Lebesgue integrals, because, even with $\mathbf{x}\in V$, then $\iiint_V\nabla_l^2\left[\frac{1}{\|\mathbf{x}-\mathbf{l}\|}\right]\mathbf{J}(\mathbf{l})\,d^3l$ $=\int_{V\setminus\{\mathbf{x}\}}\nabla_l^2\left[\frac{1}{\|\mathbf{x}-\mathbf{l}\|}\right]\mathbf{J}(\mathbf{l})\,d\mu_{\mathbf{l}}$ $=\mathbf{0}$, even if $\mathbf{J}(\mathbf{x})$ is not null. What else if not Riemann or Lebesgue integrals? Why is the commutation of the integral and differential operators licit and what do the derivatives mean in such a context? If we intend them to represent functionals as in the context of functional analysis (which is the only one that I know of where Dirac's $\delta$ is defined), which function ($\varphi$, to use the notation used here )  is the argument of the functional and what does the functional maps it to? What are the derivatives expressed by $\nabla$? Since theorems such as Stokes' are usually applied when integrating $\nabla\times\mathbf{B}$, I would believe that they are the ordinary derivatives of elementary multivariate calculus, but then the $\delta$, which is a tool of the theory of distributions, pops up in the outline of proof , and in the theory of distributions there exist derivatives of distributions which are a very different thing, but they are taken, as far as I know, with respect to the variables written as ""variables of integration"" in the distribution integral notation, while we start with $\nabla_r\times \mathbf{B}$ with $r$ , while the integral appears with $d^3l$... Or is that one of those cases , whose set I have been told not to be empty, where physics methods , at least at the didactic level, are not as rigourous as mathematics would require? I admit that I was rather inclined to think so until a user of PSE told me , without explaining how to interpretate the integrals and justify the steps, that the quoted proof is rigourous. I heartily thank any answerer.","While studying on texts of physics I notice that differentiation under the integral sign is usually introduced without any comment on the conditions permitting to do so. In that case, I take care of thinking about what the author is assuming and the usual assumption made in physics that all the functions are of class $C^\infty$, at least piecewise on compact subsets, often is enough to guarantee the liceity of freely commutating the derivative and integral signs. While studying the derivation of Ampère's law from the Biot-Savart law, someting has surprised me in this proof which seems to be ubiquitous on line and in cartaceous texts. In fact the magnetic field in a point $\mathbf{x}$ is $$\mathbf{B}(\mathbf{x}):=\frac{\mu_0}{4\pi}\iiint_V\mathbf{J}(\mathbf{l})\times\frac{\mathbf{x}-\mathbf{l}}{\|\mathbf{x}-\mathbf{l}\|^3}d^3l=\frac{\mu_0}{4\pi}\iiint_V\nabla_x\times\left[\frac{\mathbf{J}(\mathbf{l})}{\|\mathbf{x}-\mathbf{l}\|}\right]d^3l$$where I would prove the identity of the integrands at both members by considering the derivatives as... well, ordinary derivatives. I keep Wikipedia's notation except for $\mathbf{x}$, which is more common as a variable, and the norm sign, for which I have always seen $\|\cdot\|$ elsewhere. Then we can notice that the proof uses a differentiation under the integral sign (at $(1)$ below): since $\nabla_x\times\left[\nabla_x\times\left[\frac{\mathbf{J}(\mathbf{l})}{\|\mathbf{x}-\mathbf{l}\|}\right]\right]=\nabla_x\left[\nabla_x\cdot\left[\frac{\mathbf{J}(\mathbf{l})}{\|\mathbf{x}-\mathbf{l}\|}\right]\right]-\nabla_x^2\left[\frac{\mathbf{J}(\mathbf{l})}{\|\mathbf{x}-\mathbf{l}\|}\right]=\nabla_x\left[\mathbf{J}(\mathbf{l})\cdot\nabla_x\left[\frac{1}{\|\mathbf{x}-\mathbf{l}\|}\right]\right]$ $-\nabla^2\left[\frac{1}{\|\mathbf{x}-\mathbf{l}\|}\right]\mathbf{J}(\mathbf{l})$, where I would calculate the derivatives as ordinarily understood, again, we have that$$\nabla_x\times\mathbf{B}(\mathbf{x})=\nabla_x\times\left[\frac{\mu_0}{4\pi}\iiint_V\nabla_x\times\left[\frac{\mathbf{J}(\mathbf{l})}{\|\mathbf{x}-\mathbf{l}\|}\right]d^3l\right]$$$$=\frac{\mu_0}{4\pi}\iiint_V\nabla_x\times\left[\nabla_x\times\left[\frac{\mathbf{J}(\mathbf{l})}{\|\mathbf{x}-\mathbf{l}\|}\right]\right]d^3l\quad(1)$$$$=\frac{\mu_0}{4\pi}\iiint_V\nabla_x\left[\mathbf{J}(\mathbf{l})\cdot\nabla_x\left[\frac{1}{\|\mathbf{x}-\mathbf{l}\|}\right]\right]-\nabla_x^2\left[\frac{1}{\|\mathbf{x}-\mathbf{l}\|}\right]\mathbf{J}(\mathbf{l})\,d^3l$$and then the integral is split as licit for Riemann, and Lebesgue, integrals when both integrands are integrable, and the gradient and integral signs are commutated in the first of the two resulting integrals to get$$\frac{\mu_0}{4\pi}\nabla_x\iiint_V\mathbf{J}(\mathbf{l})\cdot\nabla_x\left[\frac{1}{\|\mathbf{x}-\mathbf{l}\|}\right]d^3l-\frac{\mu_0}{4\pi}\iiint_V\nabla_l^2\left[\frac{1}{\|\mathbf{x}-\mathbf{l}\|}\right]\mathbf{J}(\mathbf{l})\,d^3l$$where the first addend is $\mathbf{0}$ (I do not understand how it is calculated, but that is not the main focus of my question) and where the identity $\nabla_l^2\left[\frac{1}{\|\mathbf{x}-\mathbf{l}\|}\right]=-4\pi\delta(\mathbf{x}-\mathbf{l})$, where the derivatives are this time intended as derivatives of a distribution, is used to get$$-\frac{\mu_0}{4\pi}\iiint_V\nabla_l^2\left[\frac{1}{\|\mathbf{x}-\mathbf{l}\|}\right]\mathbf{J}(\mathbf{l})\,d^3l=\mu_0\mathbf{J}(\mathbf{x}).$$ Everything of my reasoning seemed to me to work by assuming $V\subset\mathbb{R}^3$ to be compact and such that $\mathbf{x}\notin V$ and intending the integral $\iiint...d^3l$ to be a Riemann (or Lebesgue, which, in that case, I think to be the same) integral, but at this last step I see that it was not what I thought. What are, then, the integrals appearing in such calculations? They cannot be Riemann integrals, as far as I understand, because then it must be $\mathbf{x}\notin V$ and then $\iiint_V\nabla_l^2\left[\frac{1}{\|\mathbf{x}-\mathbf{l}\|}\right]\mathbf{J}(\mathbf{l})\,d^3l=\mathbf{0}$, and they cannot be Lebesgue integrals, because, even with $\mathbf{x}\in V$, then $\iiint_V\nabla_l^2\left[\frac{1}{\|\mathbf{x}-\mathbf{l}\|}\right]\mathbf{J}(\mathbf{l})\,d^3l$ $=\int_{V\setminus\{\mathbf{x}\}}\nabla_l^2\left[\frac{1}{\|\mathbf{x}-\mathbf{l}\|}\right]\mathbf{J}(\mathbf{l})\,d\mu_{\mathbf{l}}$ $=\mathbf{0}$, even if $\mathbf{J}(\mathbf{x})$ is not null. What else if not Riemann or Lebesgue integrals? Why is the commutation of the integral and differential operators licit and what do the derivatives mean in such a context? If we intend them to represent functionals as in the context of functional analysis (which is the only one that I know of where Dirac's $\delta$ is defined), which function ($\varphi$, to use the notation used here )  is the argument of the functional and what does the functional maps it to? What are the derivatives expressed by $\nabla$? Since theorems such as Stokes' are usually applied when integrating $\nabla\times\mathbf{B}$, I would believe that they are the ordinary derivatives of elementary multivariate calculus, but then the $\delta$, which is a tool of the theory of distributions, pops up in the outline of proof , and in the theory of distributions there exist derivatives of distributions which are a very different thing, but they are taken, as far as I know, with respect to the variables written as ""variables of integration"" in the distribution integral notation, while we start with $\nabla_r\times \mathbf{B}$ with $r$ , while the integral appears with $d^3l$... Or is that one of those cases , whose set I have been told not to be empty, where physics methods , at least at the didactic level, are not as rigourous as mathematics would require? I admit that I was rather inclined to think so until a user of PSE told me , without explaining how to interpretate the integrals and justify the steps, that the quoted proof is rigourous. I heartily thank any answerer.",,"['real-analysis', 'integration', 'functional-analysis', 'multivariable-calculus', 'physics']"
93,"Is the sequence $a_{n+1}=a_n-\frac{1}{a_n}$, $a_0=2$ bounded?","Is the sequence ,  bounded?",a_{n+1}=a_n-\frac{1}{a_n} a_0=2,"The sequence again for convenience $$ a_{n+1}=a_n-\frac{1}{a_n},\;a_0=2 $$ My friend asked me this question and I do not know how to tackle it. It's clear it does not have a limit, but I am not sure whether it is unbounded; it seems to oscillate with a large amplitude when you simulate it numerically. I also can't seem to get anywhere with generating functions, but I also don't know how to use them for nonlinear recurrence relations.","The sequence again for convenience $$ a_{n+1}=a_n-\frac{1}{a_n},\;a_0=2 $$ My friend asked me this question and I do not know how to tackle it. It's clear it does not have a limit, but I am not sure whether it is unbounded; it seems to oscillate with a large amplitude when you simulate it numerically. I also can't seem to get anywhere with generating functions, but I also don't know how to use them for nonlinear recurrence relations.",,"['real-analysis', 'sequences-and-series', 'recurrence-relations']"
94,Build $\mathbb{R}$ from $\mathcal{P}(\mathbb{N})$,Build  from,\mathbb{R} \mathcal{P}(\mathbb{N}),"It's well known that $\mathbb{R}$ has the same cardinality as $\mathcal{P}(\mathbb{N})$; but I would fain know if there is a way to construct $(\mathbb{R}, +,\cdot, \leq )$ using only definitions that rely upon $\mathcal{P}(\mathbb{N})$'s elements and their respective properties.","It's well known that $\mathbb{R}$ has the same cardinality as $\mathcal{P}(\mathbb{N})$; but I would fain know if there is a way to construct $(\mathbb{R}, +,\cdot, \leq )$ using only definitions that rely upon $\mathcal{P}(\mathbb{N})$'s elements and their respective properties.",,"['real-analysis', 'elementary-set-theory']"
95,"How to prove $e^x\left|\int_x^{x+1}\sin(e^t) \,\mathrm d t\right|\le 1.4$?",How to prove ?,"e^x\left|\int_x^{x+1}\sin(e^t) \,\mathrm d t\right|\le 1.4","Related question asked by me on MathOverflow: How to prove $e^x\left|\int_x^{x+1}\sin(e^t) \,\mathrm d t\right|\le 1.4$ ? This is a follow-up question to the question How to prove $ \mathrm{e}^x\left|\int_x^{x+1}\sin\mathrm e^t \mathrm d t\right|\leqslant 2$ ? , in which a weaker bound is proven using a nice trick. Now my question is how to maximize and minimize $$f(x)=e^x\int_x^{x+1}\sin(e^t) \,\mathrm d t$$ or at least to prove $-1.4\le f(x)\le 1.4$ . Some observations, using the substitution $y=e^t$ : $$f(x)=e^x \int_{e^x}^{e^{x+1}} \frac{\sin(y)}y\,\mathrm dy=g(e^x),$$ where I have defined $$g(z)=z \int_z^{e z} \frac{\sin(y)}y\,\mathrm dy = z (\operatorname{Si}(e z)-\operatorname{Si}(z)).$$ ( $\operatorname{Si}$ is the Sine integral .) So the question reduces to: What are the maxima/minima of $g(z)$ for $z\geq 0$ ? Using the series of $\mathrm{Si}(z)$ , we get $$g(z)=\sum_{k=1}^\infty (-1)^{k-1} \frac{z^{2k}(e^{2k-1}-1)}{(2k-1)!\cdot(2k-1)}$$ and here is a plot of $g(z)$ , which seems to be periodic:","Related question asked by me on MathOverflow: How to prove ? This is a follow-up question to the question How to prove ? , in which a weaker bound is proven using a nice trick. Now my question is how to maximize and minimize or at least to prove . Some observations, using the substitution : where I have defined ( is the Sine integral .) So the question reduces to: What are the maxima/minima of for ? Using the series of , we get and here is a plot of , which seems to be periodic:","e^x\left|\int_x^{x+1}\sin(e^t) \,\mathrm d t\right|\le 1.4  \mathrm{e}^x\left|\int_x^{x+1}\sin\mathrm e^t \mathrm d t\right|\leqslant 2 f(x)=e^x\int_x^{x+1}\sin(e^t) \,\mathrm d t -1.4\le f(x)\le 1.4 y=e^t f(x)=e^x \int_{e^x}^{e^{x+1}} \frac{\sin(y)}y\,\mathrm dy=g(e^x), g(z)=z \int_z^{e z} \frac{\sin(y)}y\,\mathrm dy = z (\operatorname{Si}(e z)-\operatorname{Si}(z)). \operatorname{Si} g(z) z\geq 0 \mathrm{Si}(z) g(z)=\sum_{k=1}^\infty (-1)^{k-1} \frac{z^{2k}(e^{2k-1}-1)}{(2k-1)!\cdot(2k-1)} g(z)","['real-analysis', 'optimization', 'closed-form', 'maxima-minima']"
96,Do inequalities that hold for infinite sums hold for integrals too?,Do inequalities that hold for infinite sums hold for integrals too?,,"Let $\mathbb{R}_{\geq0}$ denote the set of non-negative reals and $+\infty$, and $\mathbb{Z}^+$ denote the set of positive integers. I will also let $\lambda$ denote the Lebesgue measure on $\mathbb{R}$ . Let there be a function $T:\mathbb{R}_{\geq0}^{\mathbb{Z}^+}\rightarrow \mathbb{R}_{\geq0}$ such that: For any functions $c:\mathbb{Z}^+\rightarrow \mathbb{R}_{\geq0},x:\mathbb{Z}^+\times\mathbb{Z}^+\rightarrow \mathbb{R}_{\geq0}$, we have:$$T((\sum_{k=1}^{\infty}c_kx(n,k))_{n\in \mathbb{Z}^+ })\leq\sum_{k=1}^{\infty}c_kT((x(n,k))_{n\in \mathbb{Z}^+})$$ For any sequence of non-negative Lebesgue measurable functions $\{f_n\}_{n\in \mathbb{Z}^+}$, the function $T(\{f_n\}_{n\in \mathbb{Z}^+})$ that sends $x$ to $T((f_n(x))_{n\in \mathbb{Z}^+})$ is Lebesgue measurable. An example of a function $T$ that satisfies these conditions is the function that sends $(x_n)_{n\in Z^+}$ to $\sum_{n\in Z^+}x_n$. Question: Does it follow that $$T((\int_{\mathbb{R}}f_n\,d\lambda)_{n\in \mathbb{Z}^+})\leq \int_{\mathbb{R}}T(\{f_n\}_{n\in \mathbb{Z}^+})\,d\lambda$$ I am intrested to know the answer to this question. I am also very interested to see any counterexamples (if there are any). Thank you.","Let $\mathbb{R}_{\geq0}$ denote the set of non-negative reals and $+\infty$, and $\mathbb{Z}^+$ denote the set of positive integers. I will also let $\lambda$ denote the Lebesgue measure on $\mathbb{R}$ . Let there be a function $T:\mathbb{R}_{\geq0}^{\mathbb{Z}^+}\rightarrow \mathbb{R}_{\geq0}$ such that: For any functions $c:\mathbb{Z}^+\rightarrow \mathbb{R}_{\geq0},x:\mathbb{Z}^+\times\mathbb{Z}^+\rightarrow \mathbb{R}_{\geq0}$, we have:$$T((\sum_{k=1}^{\infty}c_kx(n,k))_{n\in \mathbb{Z}^+ })\leq\sum_{k=1}^{\infty}c_kT((x(n,k))_{n\in \mathbb{Z}^+})$$ For any sequence of non-negative Lebesgue measurable functions $\{f_n\}_{n\in \mathbb{Z}^+}$, the function $T(\{f_n\}_{n\in \mathbb{Z}^+})$ that sends $x$ to $T((f_n(x))_{n\in \mathbb{Z}^+})$ is Lebesgue measurable. An example of a function $T$ that satisfies these conditions is the function that sends $(x_n)_{n\in Z^+}$ to $\sum_{n\in Z^+}x_n$. Question: Does it follow that $$T((\int_{\mathbb{R}}f_n\,d\lambda)_{n\in \mathbb{Z}^+})\leq \int_{\mathbb{R}}T(\{f_n\}_{n\in \mathbb{Z}^+})\,d\lambda$$ I am intrested to know the answer to this question. I am also very interested to see any counterexamples (if there are any). Thank you.",,"['real-analysis', 'measure-theory', 'inequality', 'integration', 'integral-inequality']"
97,Control over bump function's second derivative,Control over bump function's second derivative,,"The usual constructions of bump functions supported on an interval $[a,b]$ are based on the non-analytic function $f(x) = e^{\frac{1}{(x-a)(x-b)}}$. However, the $n$-th order derivatives of these functions are very high (in fact, they grow exponentially with $n$). I wonder if one can control some of the derivatives (at the expense, for example, of the other ones). In my application, I'd like to construct the following function: Question: Is it possible to construct a bump function $\phi: \mathbb{R}  \rightarrow \mathbb{R}$ which satisfies: $$ \phi(x)  = \begin{cases} x^2 \,\,\,, |x| \leq 1\\ 0\,\,\,, |x|\geq  2\\ \end{cases} $$ and  $$ \phi''(x) \leq 2 \,\,\, \forall x \in [1,2] ? $$ To give some context, this function is useful in the so-called virial-like identities in Nonlinear Dispersive PDE's. If $u_0 \in H^1\left(\mathbb{R}^N\right)$ is such that $|\cdot |u_0 \in L^2\left(\mathbb{R}^N\right)$, the solution $u(x,t)$ of the equation: $$ \begin{cases} \partial_t u + \Delta u + |u|^{p-1}u = 0\\ u(x,0) = u_0(x) \end{cases} $$ Also satisfies $u(\cdot,t) \in H^1\left(\mathbb{R}^N\right)$ and $|\cdot|u(\cdot,t) \in L^2\left(\mathbb{R}^N\right)$ for all t in the existence time of the solution. Moreover, we have the inequality: $$ \frac{d^2}{dt^2} \int |x|^2 |u(x,t)|^2 dx \leq C E[u_0] $$ which allows us to prove blow-up in some cases. However, if one drops the decay assumption, the term $|x|^2$ in the integral must be replaced for some compactly supported function. I'd like to use the function $\phi$ above, and the bound on the second derivative would be useful on controlling some error terms. The obvious try, that is multipliyng $|x|^2$ by a function constant (and equal to 1) in some neighborhood of the origin and constant (and equal to 0) away from the origin doesn't seem to work.","The usual constructions of bump functions supported on an interval $[a,b]$ are based on the non-analytic function $f(x) = e^{\frac{1}{(x-a)(x-b)}}$. However, the $n$-th order derivatives of these functions are very high (in fact, they grow exponentially with $n$). I wonder if one can control some of the derivatives (at the expense, for example, of the other ones). In my application, I'd like to construct the following function: Question: Is it possible to construct a bump function $\phi: \mathbb{R}  \rightarrow \mathbb{R}$ which satisfies: $$ \phi(x)  = \begin{cases} x^2 \,\,\,, |x| \leq 1\\ 0\,\,\,, |x|\geq  2\\ \end{cases} $$ and  $$ \phi''(x) \leq 2 \,\,\, \forall x \in [1,2] ? $$ To give some context, this function is useful in the so-called virial-like identities in Nonlinear Dispersive PDE's. If $u_0 \in H^1\left(\mathbb{R}^N\right)$ is such that $|\cdot |u_0 \in L^2\left(\mathbb{R}^N\right)$, the solution $u(x,t)$ of the equation: $$ \begin{cases} \partial_t u + \Delta u + |u|^{p-1}u = 0\\ u(x,0) = u_0(x) \end{cases} $$ Also satisfies $u(\cdot,t) \in H^1\left(\mathbb{R}^N\right)$ and $|\cdot|u(\cdot,t) \in L^2\left(\mathbb{R}^N\right)$ for all t in the existence time of the solution. Moreover, we have the inequality: $$ \frac{d^2}{dt^2} \int |x|^2 |u(x,t)|^2 dx \leq C E[u_0] $$ which allows us to prove blow-up in some cases. However, if one drops the decay assumption, the term $|x|^2$ in the integral must be replaced for some compactly supported function. I'd like to use the function $\phi$ above, and the bound on the second derivative would be useful on controlling some error terms. The obvious try, that is multipliyng $|x|^2$ by a function constant (and equal to 1) in some neighborhood of the origin and constant (and equal to 0) away from the origin doesn't seem to work.",,"['real-analysis', 'functional-analysis', 'partial-differential-equations']"
98,Proof that the set of irrational numbers is dense in reals,Proof that the set of irrational numbers is dense in reals,,"I'm being asked to prove that the set of irrational number is dense in the real numbers.  While I do understand the general idea of the proof: Given an interval $(x,y)$, choose a positive rational number (say) $z=\sqrt{2}$.  By density of rationals, there exists a rational number $p$ in the interval $(x/z, y/z)$, which essentially means that $$\frac{x}{\sqrt{2}} < p < \frac{y}{\sqrt{2}}.$$  I find that $pz$ is irrational, since it is the product of a rational and irrational number. However, my instructions besides writing that proof down is to specifically verify that $y=xz$ is irrational.  What does this have to do with anything and how does it prove denseness of irrationals?  Regardless, assume that $x$ is a nonzero rational number and that $z$ is irrational. For the sake of contradiction, assume that $y=xz$ is rational. This should mean that $y/x$ rational as well, and therefore that $z$ is rational, a contradiction.","I'm being asked to prove that the set of irrational number is dense in the real numbers.  While I do understand the general idea of the proof: Given an interval $(x,y)$, choose a positive rational number (say) $z=\sqrt{2}$.  By density of rationals, there exists a rational number $p$ in the interval $(x/z, y/z)$, which essentially means that $$\frac{x}{\sqrt{2}} < p < \frac{y}{\sqrt{2}}.$$  I find that $pz$ is irrational, since it is the product of a rational and irrational number. However, my instructions besides writing that proof down is to specifically verify that $y=xz$ is irrational.  What does this have to do with anything and how does it prove denseness of irrationals?  Regardless, assume that $x$ is a nonzero rational number and that $z$ is irrational. For the sake of contradiction, assume that $y=xz$ is rational. This should mean that $y/x$ rational as well, and therefore that $z$ is rational, a contradiction.",,"['real-analysis', 'analysis']"
99,Can I always order a countable set of numbers?,Can I always order a countable set of numbers?,,"Suppose I have a countable subset of the real numbers $A$ . Then since it is countable there is a function $f: \mathbb N \to A$ which is bijective. This function is of course a sequence, so we can write every element $x$ of $A$ as $x_i = f(i)$ . So $A= \{ x_i \mid i \in \mathbb N \}$ . Of course no two $x_i$ and $x_j$ are the same. I can now come and reorder the $x_i$ 's (instead of naming the $x \in A$ as $x_i$ I can name it $x_j$ ) such that they are decreasing $$x_1 >x_2>\cdots. $$ (Trivial??) Question: I CAN do that right? It seems right, since any $x_i$ and $x_j$ are different, then one is bigger than the other, thus I can do this comparison with all of them and order them. If that is the case then take as $A= \Bbb Q \cap (0,1)$ for example. What we said above is that since $A$ is countable I can write $A= \Bbb Q \cap (0,1)=\{ x_i \mid i \in \mathbb N \}$ with $x_1 >x_2>\cdots$ . But that is absurd, there is no biggest rational in $(0,1)$ . It's late, what am I missing? EDIT. Also suppose a case where there IS a biggest number such as $A=\Bbb Q \cap (0,0.9]$ where the biggest rational is $0.9$ . Then we can do $ 0.9=x_1 <x_2 <x_3<\cdots$ . We have no idea what $x_2$ is, but there is a rational closest to $0.9$ than all others??","Suppose I have a countable subset of the real numbers . Then since it is countable there is a function which is bijective. This function is of course a sequence, so we can write every element of as . So . Of course no two and are the same. I can now come and reorder the 's (instead of naming the as I can name it ) such that they are decreasing (Trivial??) Question: I CAN do that right? It seems right, since any and are different, then one is bigger than the other, thus I can do this comparison with all of them and order them. If that is the case then take as for example. What we said above is that since is countable I can write with . But that is absurd, there is no biggest rational in . It's late, what am I missing? EDIT. Also suppose a case where there IS a biggest number such as where the biggest rational is . Then we can do . We have no idea what is, but there is a rational closest to than all others??","A f: \mathbb N \to A x A x_i = f(i) A= \{ x_i \mid i \in \mathbb N \} x_i x_j x_i x \in A x_i x_j x_1 >x_2>\cdots.  x_i x_j A= \Bbb Q \cap (0,1) A A= \Bbb Q \cap (0,1)=\{ x_i \mid i \in \mathbb N \} x_1 >x_2>\cdots (0,1) A=\Bbb Q \cap (0,0.9] 0.9  0.9=x_1 <x_2 <x_3<\cdots x_2 0.9","['real-analysis', 'elementary-set-theory', 'order-theory']"
