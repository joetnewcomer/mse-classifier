,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Prove that if $A$ is symmetric and has a LU-decomposition then $A=LDU' \Rightarrow U'=L^T$,Prove that if  is symmetric and has a LU-decomposition then,A A=LDU' \Rightarrow U'=L^T,"Suppose the matriz $A$ has a LU-decomposition, in other words, suppose there exists matrices $L$ and $U$ such that $A=LU$ where $L$ is lower triangular and $U$ is upper triangular. We can to prove that $A$ has a LDU'-decomposition, where $D$ is a diagonal (to do so, put $d_{ii}=u_{ii}$ and $u'_{kj}=u_{kj}/u_{kk}$). Prove that if $A$ is symmetric then $A=LDU' \Rightarrow  U'=L^T$, where $L^T$ is the transpose of matrix $L$. Yes, $A$ is invertible. Therfore, $LDU'=U'^TDL^T\Rightarrow U'=L^T$. Here is the solution (question 9) .","Suppose the matriz $A$ has a LU-decomposition, in other words, suppose there exists matrices $L$ and $U$ such that $A=LU$ where $L$ is lower triangular and $U$ is upper triangular. We can to prove that $A$ has a LDU'-decomposition, where $D$ is a diagonal (to do so, put $d_{ii}=u_{ii}$ and $u'_{kj}=u_{kj}/u_{kk}$). Prove that if $A$ is symmetric then $A=LDU' \Rightarrow  U'=L^T$, where $L^T$ is the transpose of matrix $L$. Yes, $A$ is invertible. Therfore, $LDU'=U'^TDL^T\Rightarrow U'=L^T$. Here is the solution (question 9) .",,"['matrices', 'numerical-methods']"
1,The nature of eigenvectors of a given eigenvalue,The nature of eigenvectors of a given eigenvalue,,"I am working on a problem  where I have an ($n \times n $) matrix A and an eigenvalue of A, $\lambda$, where $\lambda$ has geometric multiplicity 1. The right and left eigenvectors of A corresponding to $\lambda$ are component-wise positive. How can I show that there are no other component-wise non-negative eigenvectors?, with the exception of scalar multiples of these?","I am working on a problem  where I have an ($n \times n $) matrix A and an eigenvalue of A, $\lambda$, where $\lambda$ has geometric multiplicity 1. The right and left eigenvectors of A corresponding to $\lambda$ are component-wise positive. How can I show that there are no other component-wise non-negative eigenvectors?, with the exception of scalar multiples of these?",,"['matrices', 'eigenvalues-eigenvectors', 'control-theory']"
2,Derivative of $(uA+C)^{-1}\mathbf{b}$ w.r.t. $u\in\mathbb{R}$,Derivative of  w.r.t.,(uA+C)^{-1}\mathbf{b} u\in\mathbb{R},"Given that $(uA+C)\mathbf{x}=\mathbf{b}$ where only $u\in \mathbb{R}$ and $\mathbf{x}\in\mathbb{R}^n$ are unknowns, and where $(uA+C)\in\mathbb{R}^{n\times n}$ is an invertible matrix, how can I determine $\frac{d\mathbf{x}}{du}$ ? I rewrite the equation to $$\mathbf{x}=(uA+C)^{-1}\mathbf{b}$$ and wonder whether there is any way to find/simplify $$\frac{d}{du}(uA+C)^{-1}\mathbf{b}$$ Background In my particular case $(uA+C)\mathbf{x} = \mathbf{b}$ comes from $$ \begin{bmatrix} -x_1 & -y_1 & -1 & 0 & 0 & 0 & x_1x_1' & y_1x_1' & x_1' \\ 0 & 0 & 0 & -x_1 & -y_1 & -1 & x_1y_1' & y_1y_1' & y_1' \\ -x_2 & -y_2 & -1 & 0 & 0 & 0 & x_2x_2' & y_2x_2' & x_2' \\ 0 & 0 & 0 & -x_2 & -y_2 & -1 & x_2y_2' & y_2y_2' & y_2' \\ -x_3 & -y_3 & -1 & 0 & 0 & 0 & x_3x_3' & y_3x_3' & x_3' \\ 0 & 0 & 0 & -x_3 & -y_3 & -1 & x_3y_3' & y_3y_3' & y_3' \\ -x_4 & -y_4 & -1 & 0 & 0 & 0 & x_4x_4' & y_4x_4' & x_4' \\ 0 & 0 & 0 & -x_4 & -y_4 & -1 & x_4y_4' & y_4y_4' & y_4' \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\ \end{bmatrix} \begin{bmatrix}h1 \\ h2 \\ h3 \\ h4 \\ h5 \\ h6 \\ h7 \\ h8 \\h9 \end{bmatrix} = \begin{bmatrix}0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\1 \end{bmatrix}$$ (which comes from here ) where $u$ is one of $x_1'$ , $y_1'$ , $x_2'$ , $y_2'$ , $x_3'$ , $y_3'$ , $x_4'$ , $y_4'$ . For example, for $u\equiv x_1'$ we have $$ A = \begin{bmatrix} 0 & 0 & 0 & 0 & 0 & 0 & x_1 & y_1 & 1 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ \end{bmatrix} $$ Edit I vaguely remember a technique called implicit differentiation which I feel may be useful: $$ \frac{d}{du}(uA+C)\mathbf{x}=\frac{d}{du}\mathbf{b} $$ $$ \frac{d}{du}uA\mathbf{x}+\frac{d}{du}C\mathbf{x}=\mathbf{0} $$ $$ A\frac{d}{du}u\mathbf{x}+C\frac{d\mathbf{x}}{du}=\mathbf{0} $$ $$ A(\mathbf{x}+u\frac{d\mathbf{x}}{du})+C\frac{d\mathbf{x}}{du}=\mathbf{0} $$ $$ A\mathbf{x}+(uA+C)\frac{d\mathbf{x}}{du}=\mathbf{0} $$ $$ \frac{d\mathbf{x}}{du}=-(uA+C)^{-1}A\mathbf{x} $$ ... did I just solve it; is this correct?","Given that where only and are unknowns, and where is an invertible matrix, how can I determine ? I rewrite the equation to and wonder whether there is any way to find/simplify Background In my particular case comes from (which comes from here ) where is one of , , , , , , , . For example, for we have Edit I vaguely remember a technique called implicit differentiation which I feel may be useful: ... did I just solve it; is this correct?","(uA+C)\mathbf{x}=\mathbf{b} u\in \mathbb{R} \mathbf{x}\in\mathbb{R}^n (uA+C)\in\mathbb{R}^{n\times n} \frac{d\mathbf{x}}{du} \mathbf{x}=(uA+C)^{-1}\mathbf{b} \frac{d}{du}(uA+C)^{-1}\mathbf{b} (uA+C)\mathbf{x} = \mathbf{b}  \begin{bmatrix}
-x_1 & -y_1 & -1 & 0 & 0 & 0 & x_1x_1' & y_1x_1' & x_1' \\
0 & 0 & 0 & -x_1 & -y_1 & -1 & x_1y_1' & y_1y_1' & y_1' \\
-x_2 & -y_2 & -1 & 0 & 0 & 0 & x_2x_2' & y_2x_2' & x_2' \\
0 & 0 & 0 & -x_2 & -y_2 & -1 & x_2y_2' & y_2y_2' & y_2' \\
-x_3 & -y_3 & -1 & 0 & 0 & 0 & x_3x_3' & y_3x_3' & x_3' \\
0 & 0 & 0 & -x_3 & -y_3 & -1 & x_3y_3' & y_3y_3' & y_3' \\
-x_4 & -y_4 & -1 & 0 & 0 & 0 & x_4x_4' & y_4x_4' & x_4' \\
0 & 0 & 0 & -x_4 & -y_4 & -1 & x_4y_4' & y_4y_4' & y_4' \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\
\end{bmatrix} \begin{bmatrix}h1 \\ h2 \\ h3 \\ h4 \\ h5 \\ h6 \\ h7 \\ h8 \\h9 \end{bmatrix} = \begin{bmatrix}0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\1 \end{bmatrix} u x_1' y_1' x_2' y_2' x_3' y_3' x_4' y_4' u\equiv x_1'  A = \begin{bmatrix}
0 & 0 & 0 & 0 & 0 & 0 & x_1 & y_1 & 1 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
\end{bmatrix}   \frac{d}{du}(uA+C)\mathbf{x}=\frac{d}{du}\mathbf{b}   \frac{d}{du}uA\mathbf{x}+\frac{d}{du}C\mathbf{x}=\mathbf{0}   A\frac{d}{du}u\mathbf{x}+C\frac{d\mathbf{x}}{du}=\mathbf{0}   A(\mathbf{x}+u\frac{d\mathbf{x}}{du})+C\frac{d\mathbf{x}}{du}=\mathbf{0}   A\mathbf{x}+(uA+C)\frac{d\mathbf{x}}{du}=\mathbf{0}   \frac{d\mathbf{x}}{du}=-(uA+C)^{-1}A\mathbf{x} ","['matrices', 'multivariable-calculus', 'matrix-equations', 'matrix-calculus']"
3,"Convexity of $\{x\in \mathbb R^n,\;x^TAx\leq t\}$ for all $t$ implies that $A$ is p.s.d.",Convexity of  for all  implies that  is p.s.d.,"\{x\in \mathbb R^n,\;x^TAx\leq t\} t A","Let $A\in \mathbb R^{n\times n}$ be symmetric and such that for all $t\in \mathbb R$ , the set $\{x\in \mathbb R^n,\;x^TAx\leq t\}$ is convex. Prove that $A$ is positive semi-definite. It suffices to prove that the eigenvalues of $A$ are $\geq 0$ . Consider $\lambda$ an eigenvalue of $A$ and $x$ an eigenvector with unit norm. Then the hypothesis rewrites as $(\lambda\leq t )\;\wedge (y^TAy\leq t) \implies \forall \mu \in [0,1], (1-\mu)^2\lambda +2 \mu(1-\mu)x^TAy + \mu^2y^TAy\leq t$ . If $y$ is an eigenvector with unit norm associated to some other eigenvalue $\lambda'$ with $\lambda'\leq t$ , this rewrites as $(1-\mu)^2\lambda + \mu^2 \lambda'\leq t$ . This is redundant as it already follows from $\lambda\leq t$ and $\lambda'\leq t$ . I don't see how to prove $\lambda\geq 0$ from $\forall \mu \in [0,1], (1-\mu)^2\lambda +2 \mu(1-\mu)x^TAy + \mu^2y^TAy\leq t$ .","Let be symmetric and such that for all , the set is convex. Prove that is positive semi-definite. It suffices to prove that the eigenvalues of are . Consider an eigenvalue of and an eigenvector with unit norm. Then the hypothesis rewrites as . If is an eigenvector with unit norm associated to some other eigenvalue with , this rewrites as . This is redundant as it already follows from and . I don't see how to prove from .","A\in \mathbb R^{n\times n} t\in \mathbb R \{x\in \mathbb R^n,\;x^TAx\leq t\} A A \geq 0 \lambda A x (\lambda\leq t )\;\wedge (y^TAy\leq t) \implies \forall \mu \in [0,1], (1-\mu)^2\lambda +2 \mu(1-\mu)x^TAy + \mu^2y^TAy\leq t y \lambda' \lambda'\leq t (1-\mu)^2\lambda + \mu^2 \lambda'\leq t \lambda\leq t \lambda'\leq t \lambda\geq 0 \forall \mu \in [0,1], (1-\mu)^2\lambda +2 \mu(1-\mu)x^TAy + \mu^2y^TAy\leq t","['matrices', 'convex-analysis', 'quadratic-forms', 'symmetric-matrices', 'positive-semidefinite']"
4,Symmetric determinant,Symmetric determinant,,"So, basically i want to prove that the value of $$\begin{vmatrix} ax-by-cz & ay+bx & cx+az\\ ay+bx & by-cz-ax & bz+cy\\ cx+az & bz+cy & cz-ax-by\\ \end{vmatrix}$$ is equal to $$(x^2+y^2+z^2)(a^2+b^2+c^2)(ax+by+cz)$$ $\mathbf {What}$ $\mathbf {I} $ $\mathbf{have} $ $\mathbf{tried}$ : I have noticed it is a symmetric matrix but cannot proceed on that thought further. Next, I have tried multiplying row $1,2,3$ with $yz, xz, xy$ respectively with no luck. I also did some transformations but they were of no use again. Thanks in advance.","So, basically i want to prove that the value of $$\begin{vmatrix} ax-by-cz & ay+bx & cx+az\\ ay+bx & by-cz-ax & bz+cy\\ cx+az & bz+cy & cz-ax-by\\ \end{vmatrix}$$ is equal to $$(x^2+y^2+z^2)(a^2+b^2+c^2)(ax+by+cz)$$ $\mathbf {What}$ $\mathbf {I} $ $\mathbf{have} $ $\mathbf{tried}$ : I have noticed it is a symmetric matrix but cannot proceed on that thought further. Next, I have tried multiplying row $1,2,3$ with $yz, xz, xy$ respectively with no luck. I also did some transformations but they were of no use again. Thanks in advance.",,"['matrices', 'determinant']"
5,Determinant of a specific $n\times n$ matrix [duplicate],Determinant of a specific  matrix [duplicate],n\times n,"This question already has answers here : If $a_{ij}=\max(i,j)$, calculate the determinant of $A$ (7 answers) Closed 6 years ago . Let $M_n=   \begin{bmatrix}     1 & 2 & 3 & ... & n\\     2 & 2 & 3 & ... & n\\     3 & 3 & 3 & ... & n\\     \vdots & \vdots & \vdots & \ddots & n\\     n & n & n & n & n   \end{bmatrix}$ What's $\det(M_n)$ ? Seems like it is one of these exercises virtually impossible to do without the one trick, which is why I ask this question. Computing the first few determinants yields: $\det(M_1)=1$ $\det(M_2)=-2$ $\det(M_3)=3$ so if I were to make a wild guess, I would say $\det(M_n)=n(-1)^{n+1}$ The induction process didn't work too well for me because I can't find the determinant once a row and a column have been added to pad the matrix to dimension $n+1$. I tried to use the well-known formula for the determinant by cutting the inside in four parts ($M_n$ in the upper-left, $(n+1)$ in the lower-right, and the $(n+1)$ row and column, but at the end I must compute the determinant of a sum of matrices which didn't help... I don't know the name of this specific matrix, so maybe this is a repost, but if this is the case, I can't find the original one I'd be grateful if someone could provide some hints.","This question already has answers here : If $a_{ij}=\max(i,j)$, calculate the determinant of $A$ (7 answers) Closed 6 years ago . Let $M_n=   \begin{bmatrix}     1 & 2 & 3 & ... & n\\     2 & 2 & 3 & ... & n\\     3 & 3 & 3 & ... & n\\     \vdots & \vdots & \vdots & \ddots & n\\     n & n & n & n & n   \end{bmatrix}$ What's $\det(M_n)$ ? Seems like it is one of these exercises virtually impossible to do without the one trick, which is why I ask this question. Computing the first few determinants yields: $\det(M_1)=1$ $\det(M_2)=-2$ $\det(M_3)=3$ so if I were to make a wild guess, I would say $\det(M_n)=n(-1)^{n+1}$ The induction process didn't work too well for me because I can't find the determinant once a row and a column have been added to pad the matrix to dimension $n+1$. I tried to use the well-known formula for the determinant by cutting the inside in four parts ($M_n$ in the upper-left, $(n+1)$ in the lower-right, and the $(n+1)$ row and column, but at the end I must compute the determinant of a sum of matrices which didn't help... I don't know the name of this specific matrix, so maybe this is a repost, but if this is the case, I can't find the original one I'd be grateful if someone could provide some hints.",,"['matrices', 'determinant']"
6,Does $A$ and $A^TA$ have the same eigenvalues,Does  and  have the same eigenvalues,A A^TA,Do matrices $A$ and $A^TA$ have the same eigenvalues? I think they do but I can't find anything online about it,Do matrices $A$ and $A^TA$ have the same eigenvalues? I think they do but I can't find anything online about it,,"['matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
7,Cayley–Menger determinant.,Cayley–Menger determinant.,,"Consider $n$-simplex. I read that Cayley–Menger determinant is a squared-volume of simplex. But I don't know how to get it. Actually after some attempts I get this : $\displaystyle V_s^2 = \frac 1 {(m!)^2}G(v_1\ldots v_n)$, where $G(v_1\dots v_n)$ is Gramian . But how can we transform Gramian to Cayley–Menger matrix? Any ideas?","Consider $n$-simplex. I read that Cayley–Menger determinant is a squared-volume of simplex. But I don't know how to get it. Actually after some attempts I get this : $\displaystyle V_s^2 = \frac 1 {(m!)^2}G(v_1\ldots v_n)$, where $G(v_1\dots v_n)$ is Gramian . But how can we transform Gramian to Cayley–Menger matrix? Any ideas?",,"['matrices', 'determinant', 'simplex']"
8,Minimization of Frobenius Norm and Schur Complement,Minimization of Frobenius Norm and Schur Complement,,"There is a famous problem Optimization of Frobenius Norm and Nuclear Norm ; however, this is not I want to ask (about proximal operator). Suppose I have an easy optimization problem: $$\min_Q \|Q-Q_N\|_F$$ where $\|\cdot\|_F$ is the Frobenius norm: $\|X\|_F = (\operatorname{tr}(X^TX))^{\frac{1}{2}}$ . We know we can consider the following: $$\mathcal{A}(Q,t) = \begin{bmatrix}I & Q-Q_N\\ (Q-Q_N)^T & tI \end{bmatrix}\succeq 0$$ By Schur complement we have the following $$tI-(Q-Q_N)^T(Q-Q_N)\succeq 0$$ If $Q\in \mathbb{R}$ , the above becomes $$t-\|Q-Q_N\|^2\geq 0 \Rightarrow t\geq \|Q-Q_N\|^2 \geq 0$$ So the original problem is equivalent to \begin{align} &\min_{t,Q} & &t \\ & s.t. & & \|Q-Q_N\|^2 \geq 0 \end{align} or \begin{align} &\min_{t,Q} & &t \\ & s.t. & & \mathcal{A}(t,Q)\succeq 0 \end{align} The second formulation is a SDP My question is: if $Q\in \mathbb{R}^{n\times n}$ how to obtain the above convex optimization problem (minimize over $t$ ) with $\|\cdot\|^2$ replaced by $\|\cdot\|_F^2$ Is there any method except the vectorization of matrices ?","There is a famous problem Optimization of Frobenius Norm and Nuclear Norm ; however, this is not I want to ask (about proximal operator). Suppose I have an easy optimization problem: where is the Frobenius norm: . We know we can consider the following: By Schur complement we have the following If , the above becomes So the original problem is equivalent to or The second formulation is a SDP My question is: if how to obtain the above convex optimization problem (minimize over ) with replaced by Is there any method except the vectorization of matrices ?","\min_Q \|Q-Q_N\|_F \|\cdot\|_F \|X\|_F = (\operatorname{tr}(X^TX))^{\frac{1}{2}} \mathcal{A}(Q,t) = \begin{bmatrix}I & Q-Q_N\\ (Q-Q_N)^T & tI \end{bmatrix}\succeq 0 tI-(Q-Q_N)^T(Q-Q_N)\succeq 0 Q\in \mathbb{R} t-\|Q-Q_N\|^2\geq 0 \Rightarrow t\geq \|Q-Q_N\|^2 \geq 0 \begin{align}
&\min_{t,Q} & &t \\
& s.t. & & \|Q-Q_N\|^2 \geq 0
\end{align} \begin{align}
&\min_{t,Q} & &t \\
& s.t. & & \mathcal{A}(t,Q)\succeq 0
\end{align} Q\in \mathbb{R}^{n\times n} t \|\cdot\|^2 \|\cdot\|_F^2","['matrices', 'convex-optimization', 'normed-spaces', 'linear-matrix-inequality', 'schur-complement']"
9,Matrix derivative of $\mbox{Tr} (\mathbf{AXB})$,Matrix derivative of,\mbox{Tr} (\mathbf{AXB}),The Matrix Cookbook says that: $\frac{\partial}{\partial\mathbf{X}} Tr\{\mathbf{A}\mathbf{X}\mathbf{B}\} = \mathbf{A^T}\mathbf{B^T}$ I can't seem to get this. I know that: $\frac{\partial}{\partial\mathbf{X}} Tr\{F(\mathbf{X})\} = f(\mathbf{A}\mathbf{X}\mathbf{B})^T$ Where $f$ is the scalar derivative of $F$. So when I apply the rule: $\partial \mathbf{XY} = \partial \mathbf{X} \mathbf{Y} + \mathbf{X} \partial \mathbf{Y}$ I do: Let: $\mathbf{C} = \mathbf{X}\mathbf{B}$ Then: $\mathbf{A}\mathbf{X}\mathbf{B} = \mathbf{A}\mathbf{C}$. $\frac{\partial}{\partial\mathbf{X}} \mathbf{A}\mathbf{C} = \frac{\partial}{\partial\mathbf{X}} \mathbf{A} \mathbf{XB} + \mathbf{A} \frac{\partial}{\partial\mathbf{X}}\mathbf{X}\mathbf{B} = \mathbf{AB}$ But then: $(\mathbf{AB})^{T} = \mathbf{B^TA^T} \neq \mathbf{A^{T}B^{T}}$ Am I confusing the notion of scalar derivative and matrix derivative? How can I verify the Cookbook's claim?,The Matrix Cookbook says that: $\frac{\partial}{\partial\mathbf{X}} Tr\{\mathbf{A}\mathbf{X}\mathbf{B}\} = \mathbf{A^T}\mathbf{B^T}$ I can't seem to get this. I know that: $\frac{\partial}{\partial\mathbf{X}} Tr\{F(\mathbf{X})\} = f(\mathbf{A}\mathbf{X}\mathbf{B})^T$ Where $f$ is the scalar derivative of $F$. So when I apply the rule: $\partial \mathbf{XY} = \partial \mathbf{X} \mathbf{Y} + \mathbf{X} \partial \mathbf{Y}$ I do: Let: $\mathbf{C} = \mathbf{X}\mathbf{B}$ Then: $\mathbf{A}\mathbf{X}\mathbf{B} = \mathbf{A}\mathbf{C}$. $\frac{\partial}{\partial\mathbf{X}} \mathbf{A}\mathbf{C} = \frac{\partial}{\partial\mathbf{X}} \mathbf{A} \mathbf{XB} + \mathbf{A} \frac{\partial}{\partial\mathbf{X}}\mathbf{X}\mathbf{B} = \mathbf{AB}$ But then: $(\mathbf{AB})^{T} = \mathbf{B^TA^T} \neq \mathbf{A^{T}B^{T}}$ Am I confusing the notion of scalar derivative and matrix derivative? How can I verify the Cookbook's claim?,,"['matrices', 'derivatives', 'matrix-calculus', 'trace', 'scalar-fields']"
10,Matrix Multiplication $1 \times n$ and $n \times 1$ Case,Matrix Multiplication  and  Case,1 \times n n \times 1,"I have seen a matrix multiplication with programming and other means such as the convention of dot product where $[3][2]=6$. However, I am unsure if it is technically correct to have it $[3][2]=[6]$. If you have a matrix $A$ which is $ 1 \times n$  and another matrix $B$ which is $n \times 1$ where $n$ is an element of the set of natural numbers, then $AB$ is another matrix which is $1 \times 1$ (like $[6]$) or is it not a matrix at all (like $6$)?","I have seen a matrix multiplication with programming and other means such as the convention of dot product where $[3][2]=6$. However, I am unsure if it is technically correct to have it $[3][2]=[6]$. If you have a matrix $A$ which is $ 1 \times n$  and another matrix $B$ which is $n \times 1$ where $n$ is an element of the set of natural numbers, then $AB$ is another matrix which is $1 \times 1$ (like $[6]$) or is it not a matrix at all (like $6$)?",,['matrices']
11,How to find the determinant of this matrix [duplicate],How to find the determinant of this matrix [duplicate],,This question already has answers here : Determinant of circulant-like matrix (2 answers) Closed 5 years ago . I've got to calculate determinant for this matrix: $$\begin{bmatrix} a_{ 1 } & 0 & 0 & \cdots  & 0 & b_{ n } \\ b_1 & a_2 & 0 & \cdots  & 0 & 0 \\ 0 & b_2 & a_3 & \cdots  & 0 & 0 \\ \vdots  & \vdots &  & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & a_{n-1} & 0 \\ 0 & 0 &  0& \cdots & b_{n-1} & a_n \end{bmatrix}$$ Is there any clever way to find out the determinant of above matrix? Thanks in advance.,This question already has answers here : Determinant of circulant-like matrix (2 answers) Closed 5 years ago . I've got to calculate determinant for this matrix: $$\begin{bmatrix} a_{ 1 } & 0 & 0 & \cdots  & 0 & b_{ n } \\ b_1 & a_2 & 0 & \cdots  & 0 & 0 \\ 0 & b_2 & a_3 & \cdots  & 0 & 0 \\ \vdots  & \vdots &  & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \cdots & a_{n-1} & 0 \\ 0 & 0 &  0& \cdots & b_{n-1} & a_n \end{bmatrix}$$ Is there any clever way to find out the determinant of above matrix? Thanks in advance.,,"['matrices', 'determinant']"
12,Doubt in the defn of exponential operator.,Doubt in the defn of exponential operator.,,"definition Let $A$ be an $n\times n$ matrix. Then for $t\in \mathbb R$, $$e^{At}=\sum_{k=0}^\infty \frac{A^kt^k}{k!}\tag{1}$$ But in this definition, What they are meaning by the term $A^kt^k$, If I give this matrix $$A=\begin{pmatrix}1&-1\\2&3\end{pmatrix},$$ what can we say about the $(1)$, means  How it will be?","definition Let $A$ be an $n\times n$ matrix. Then for $t\in \mathbb R$, $$e^{At}=\sum_{k=0}^\infty \frac{A^kt^k}{k!}\tag{1}$$ But in this definition, What they are meaning by the term $A^kt^k$, If I give this matrix $$A=\begin{pmatrix}1&-1\\2&3\end{pmatrix},$$ what can we say about the $(1)$, means  How it will be?",,"['matrices', 'exponential-function']"
13,How can we calculate the exponential form of a rotation matrix,How can we calculate the exponential form of a rotation matrix,,Considering the rotation matrix:  $$ A(\theta) = \left( \begin{array}{cc} \cos\space\theta & -\sin \space\theta \\ \sin \space\theta & \cos\space\theta \\ \end{array} \right) $$ How can I calculate $(A(\theta))^n$ where n ≥ 1 ? I'm not sure what to do nor how to write it.,Considering the rotation matrix:  $$ A(\theta) = \left( \begin{array}{cc} \cos\space\theta & -\sin \space\theta \\ \sin \space\theta & \cos\space\theta \\ \end{array} \right) $$ How can I calculate $(A(\theta))^n$ where n ≥ 1 ? I'm not sure what to do nor how to write it.,,"['matrices', 'matrix-equations']"
14,Power of an adjacency matrix,Power of an adjacency matrix,,"For any graph $\mathcal{G}$ , what do the powers of its adjacency matrix $A$ represent? As an example, what does element (u, v) of matrix $A^2$ correspond to?","For any graph , what do the powers of its adjacency matrix represent? As an example, what does element (u, v) of matrix correspond to?",\mathcal{G} A A^2,"['matrices', 'graph-theory', 'adjacency-matrix']"
15,"Proving algebraically that $\mathbb RP ^3\cong SO(3,\mathbb R)$",Proving algebraically that,"\mathbb RP ^3\cong SO(3,\mathbb R)","I am giving a simple introductory course on algebraic geometry and I plan to mention that $$\mathbb RP ^3\cong SO(3,\mathbb R).$$ I know a rather simple proof of this  using the fact that $\mathbb RP^3$ is topologically a $3$-dimensional ball with an identification of opposite points on the boundary. I would like to know if one can prove this fact in some symmetric ""algebraic way"". $\bf Added.$ Giulio Bresciani made a good remark, that $SO(3,\mathbb R)$ is naturally an affine variety, and since $\mathbb RP ^3$ is projective, they are not isomorphic. In order to rectify this situation, let as compactify $SO(3,\mathbb R)$ by adding to it points on infinity (an empty set), by adding to $\mathbb R^9$ (where $SO(3,\mathbb R)$ sits) the space $\mathbb RP^8$. The new question is then: is there some nice birational map from $\mathbb RP^3$ to $SO(3,\mathbb R)$, which is as well a diffeo on the set of points.","I am giving a simple introductory course on algebraic geometry and I plan to mention that $$\mathbb RP ^3\cong SO(3,\mathbb R).$$ I know a rather simple proof of this  using the fact that $\mathbb RP^3$ is topologically a $3$-dimensional ball with an identification of opposite points on the boundary. I would like to know if one can prove this fact in some symmetric ""algebraic way"". $\bf Added.$ Giulio Bresciani made a good remark, that $SO(3,\mathbb R)$ is naturally an affine variety, and since $\mathbb RP ^3$ is projective, they are not isomorphic. In order to rectify this situation, let as compactify $SO(3,\mathbb R)$ by adding to it points on infinity (an empty set), by adding to $\mathbb R^9$ (where $SO(3,\mathbb R)$ sits) the space $\mathbb RP^8$. The new question is then: is there some nice birational map from $\mathbb RP^3$ to $SO(3,\mathbb R)$, which is as well a diffeo on the set of points.",,"['matrices', 'algebraic-geometry', 'projective-geometry']"
16,What's a matrix?,What's a matrix?,,"What is a matrix exactly? What are matrices used for? I have read some of the Wikipedia article, but since my math knowledge is pretty basic, I didn't understand much. Could you explain to me in simple language what matrices are, and what they are used for? For example, I know what a rotation matrix is and what it's used for, but don't know why it's called a 'matrix'. For me it's just a formula ($x' = x\cos\theta - y\sin\theta$, $y' = x\sin\theta + y\cos\theta$), that I can use to get the new position of a point after a rotation. Don't know why it's called a matrix and what matrices are and do in general. Thanks","What is a matrix exactly? What are matrices used for? I have read some of the Wikipedia article, but since my math knowledge is pretty basic, I didn't understand much. Could you explain to me in simple language what matrices are, and what they are used for? For example, I know what a rotation matrix is and what it's used for, but don't know why it's called a 'matrix'. For me it's just a formula ($x' = x\cos\theta - y\sin\theta$, $y' = x\sin\theta + y\cos\theta$), that I can use to get the new position of a point after a rotation. Don't know why it's called a matrix and what matrices are and do in general. Thanks",,['matrices']
17,Trace of matrices inequality,Trace of matrices inequality,,"If I have two matrices, $\mathbf{A}$ which is symmetric and postive definite, and $\mathbf{B}$ symmetric, positive definite, and all entries in $\mathbf{B}$ are between 0 and 1, with the diagonal entries all equal to 1. I can't seem to see why tr($\mathbf{AB}$) $\leq$ tr($\mathbf{A}$). Thanks.","If I have two matrices, $\mathbf{A}$ which is symmetric and postive definite, and $\mathbf{B}$ symmetric, positive definite, and all entries in $\mathbf{B}$ are between 0 and 1, with the diagonal entries all equal to 1. I can't seem to see why tr($\mathbf{AB}$) $\leq$ tr($\mathbf{A}$). Thanks.",,"['matrices', 'trace']"
18,Prove that the matrix is totally unimodular,Prove that the matrix is totally unimodular,,"Is there any (theoretic) way I can prove the matrix is totally unimodular? I have tested it by Matlab and know it is TU, however I cannot prove it. -1 -1 -1 -1  0  0  0  0  0  0  0  0  0  0  0  0 -1 -1 -1 -1  0  0  0  0  0  0  0  0  0  0  0  0 -1 -1 -1 -1  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  0  0  0  1  0  0  0  1  0  0  0  0  1  0  0  0  1  0  0  0  1  0  0  0  0  1  0  0  0  1  0  0  0  1  0  0  0  0  1  0  0  0  1  0  0  0  1","Is there any (theoretic) way I can prove the matrix is totally unimodular? I have tested it by Matlab and know it is TU, however I cannot prove it. -1 -1 -1 -1  0  0  0  0  0  0  0  0  0  0  0  0 -1 -1 -1 -1  0  0  0  0  0  0  0  0  0  0  0  0 -1 -1 -1 -1  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  0  0  0  1  0  0  0  1  0  0  0  0  1  0  0  0  1  0  0  0  1  0  0  0  0  1  0  0  0  1  0  0  0  1  0  0  0  0  1  0  0  0  1  0  0  0  1",,"['matrices', 'determinant', 'integer-programming', 'total-unimodularity']"
19,Connection between irreducibly diagonally dominant matrices and positive definiteness?,Connection between irreducibly diagonally dominant matrices and positive definiteness?,,"I am attempting to prove a proposition that I found in Cottle's ""The Linear Complementarity Problem"" book in which the proof has been omitted. I will start by introducing some definitions. ${\bf Def.}$ A matrix $A\in\mathbb{R}^{n\times n}$ is said to be irreducible if and only if for any two distinct indices $1\le i,j\le n$, there is a sequence of nonzero elements of $A$ of the form $$ \{a_{ii_1},a_{i_1i_2},\ldots,a_{i_mj}\}. $$ ${\bf Def.}$ A matrix $A\in\mathbb{R}^{n\times n}$ is irreducibly diagonally dominant if it is irreducible, weakly diagonally dominant, $|A_{ii}| \ge \sum_{j\neq i}|A_{ij}|$ for all $i$, and there is at least one row or column where strict diagonal dominance holds, that is $\exists$ $i$ such that $|A_{ii}| > \sum_{j\neq i}|A_{ij}|$. The statement is as follows. ${\bf Prop.}$ Let $A\in\mathbb{R}^{n\times n}$ be symmetric, strictly or irreducibly diagonally dominant, and $A_{ii}>0$ for all $i$, then $A$ is positive definite. ${\bf Pf.}$ (attempt, sketch) My attempt used the Gershgorin Circle Theorem which allowed me to show that the eigenvalues of the matrix were all nonnegative, with one strictly positive. However, I do not know how to use irreducibility or the fact that $A$ has strictly positive diagonal entries to conclude that it is positive definite.","I am attempting to prove a proposition that I found in Cottle's ""The Linear Complementarity Problem"" book in which the proof has been omitted. I will start by introducing some definitions. ${\bf Def.}$ A matrix $A\in\mathbb{R}^{n\times n}$ is said to be irreducible if and only if for any two distinct indices $1\le i,j\le n$, there is a sequence of nonzero elements of $A$ of the form $$ \{a_{ii_1},a_{i_1i_2},\ldots,a_{i_mj}\}. $$ ${\bf Def.}$ A matrix $A\in\mathbb{R}^{n\times n}$ is irreducibly diagonally dominant if it is irreducible, weakly diagonally dominant, $|A_{ii}| \ge \sum_{j\neq i}|A_{ij}|$ for all $i$, and there is at least one row or column where strict diagonal dominance holds, that is $\exists$ $i$ such that $|A_{ii}| > \sum_{j\neq i}|A_{ij}|$. The statement is as follows. ${\bf Prop.}$ Let $A\in\mathbb{R}^{n\times n}$ be symmetric, strictly or irreducibly diagonally dominant, and $A_{ii}>0$ for all $i$, then $A$ is positive definite. ${\bf Pf.}$ (attempt, sketch) My attempt used the Gershgorin Circle Theorem which allowed me to show that the eigenvalues of the matrix were all nonnegative, with one strictly positive. However, I do not know how to use irreducibility or the fact that $A$ has strictly positive diagonal entries to conclude that it is positive definite.",,"['matrices', 'gershgorin-sets']"
20,Is $\mathrm{GL}_n(\mathbb C)$ divisible?,Is  divisible?,\mathrm{GL}_n(\mathbb C),"A group $G$ (possibly non-abelian) is divisible when for all $k\in \Bbb N$ and $g\in G$ there exists $h\in G$ such that $g=h^k.$ Is the group $\mathrm{GL}_n(\mathbb C)$ divisible? Or more precisely, for which $n$ is it divisible? (It clearly is for $n=0$ and $n=1$.)","A group $G$ (possibly non-abelian) is divisible when for all $k\in \Bbb N$ and $g\in G$ there exists $h\in G$ such that $g=h^k.$ Is the group $\mathrm{GL}_n(\mathbb C)$ divisible? Or more precisely, for which $n$ is it divisible? (It clearly is for $n=0$ and $n=1$.)",,"['group-theory', 'matrices', 'complex-numbers']"
21,Change in singular values of matrix after left-multiply with a diagonal matrix,Change in singular values of matrix after left-multiply with a diagonal matrix,,"Say that we have an SVD for a matrix $X = U \Sigma V^T$, giving trace norm $||X||_{tr} = ||\Sigma||_{tr} = \sum \Sigma_{ii}$. I am wondering what happens to the SVD and/or trace norm if we left multiply with a diagonal matrix that simply scales the row of $X$? Is it possible to write the SVD of $DX$ or the trace norm of $DX$ in terms of the diagonal of $D$ and the SVD of $X$?","Say that we have an SVD for a matrix $X = U \Sigma V^T$, giving trace norm $||X||_{tr} = ||\Sigma||_{tr} = \sum \Sigma_{ii}$. I am wondering what happens to the SVD and/or trace norm if we left multiply with a diagonal matrix that simply scales the row of $X$? Is it possible to write the SVD of $DX$ or the trace norm of $DX$ in terms of the diagonal of $D$ and the SVD of $X$?",,"['matrices', 'eigenvalues-eigenvectors', 'normed-spaces', 'trace', 'svd']"
22,Matrix non-identity,Matrix non-identity,,"Let $M,N$ be $n\times n$ matrices. Then why is it that $MN-NM=I_n$ cannot be true, where $I_n$ is the $n\times n$ identity matrix? I am thinking of perhaps there is an argument using determinants? (Of course I am probably way out.) Thanks.","Let $M,N$ be $n\times n$ matrices. Then why is it that $MN-NM=I_n$ cannot be true, where $I_n$ is the $n\times n$ identity matrix? I am thinking of perhaps there is an argument using determinants? (Of course I am probably way out.) Thanks.",,['matrices']
23,Positivity of a matrix built on Pascal's triangle,Positivity of a matrix built on Pascal's triangle,,"For an integer $n$ , let $T_n$ be the $(n+1) \times (n+1)$ Toeplitz matrix built on the $2n$ th row of Pascal's triangle, i.e., its $(i,j)$ entry equals $\binom{2n}{n+i-j}$ . For example, $$ T_2 =\begin{pmatrix} 6 & 4 & 1 \\ 4 & 6 & 4 \\ 1 & 4 & 6 \end{pmatrix}. $$ Numerical experiments indicate that $T_n$ is always positive definite. How to prove this claim?","For an integer , let be the Toeplitz matrix built on the th row of Pascal's triangle, i.e., its entry equals . For example, Numerical experiments indicate that is always positive definite. How to prove this claim?","n T_n (n+1) \times (n+1) 2n (i,j) \binom{2n}{n+i-j}  T_2 =\begin{pmatrix} 6 & 4 & 1 \\ 4 & 6 & 4 \\ 1 & 4 & 6 \end{pmatrix}.  T_n","['matrices', 'binomial-coefficients', 'positive-definite', 'toeplitz-matrices']"
24,Calculate kernel space of matrix map and its orthonormal space [closed],Calculate kernel space of matrix map and its orthonormal space [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question The question asks: a) What is kernel space of linear map defined by $$ M = \begin{bmatrix} 1 & 2 & 3 \\ 2 & 4 & 6 \\ 3 & 6 & 9 \\ \end{bmatrix} $$ b) Give orthonormal basis for map's kernel from a) My attempt of a) I attempted to find the kernel space, but I am not sure if I am right, furthermore, I don't know how to approach b). Thank you for any help","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question The question asks: a) What is kernel space of linear map defined by b) Give orthonormal basis for map's kernel from a) My attempt of a) I attempted to find the kernel space, but I am not sure if I am right, furthermore, I don't know how to approach b). Thank you for any help","
M =
\begin{bmatrix}
1 & 2 & 3 \\
2 & 4 & 6 \\
3 & 6 & 9 \\
\end{bmatrix}
","['matrices', 'vector-spaces']"
25,Prove or disprove that the matrix is invertible,Prove or disprove that the matrix is invertible,,Let $P$ be a $n \times n $ matrix with integer entries. Let $q$ be a non integer and $Q = P + qI$ where $I$ is identity matrix. Prove or disprove that $Q$ is invertible. This is easy if $P$ is a $2 \times 2$ matrix. In this case it is easy to see that determinant of $Q$ is non zero. Let $P$ be a matrix \begin{pmatrix} a & b \\ c & d \\ \end{pmatrix} then we see that $\det Q = (a + q)(d + q) - bc$ . If $q$ is an irrational then the determinant is obviously non zero. If $q$ is a rational then let $q = \frac mn$ where gcd of $m$ and $n$ is $1$ . Now it is easy to see that if $$ (an+m)(dn+m) = n^2 bc $$ then any prime which divides $n$ doesn't divide $m$ which implies that it doesn't divide LHS as well. So determinant of $Q$ is non zero so we conclude that $Q$ is invertible. Now how to prove for higher dimension matrices?,Let be a matrix with integer entries. Let be a non integer and where is identity matrix. Prove or disprove that is invertible. This is easy if is a matrix. In this case it is easy to see that determinant of is non zero. Let be a matrix then we see that . If is an irrational then the determinant is obviously non zero. If is a rational then let where gcd of and is . Now it is easy to see that if then any prime which divides doesn't divide which implies that it doesn't divide LHS as well. So determinant of is non zero so we conclude that is invertible. Now how to prove for higher dimension matrices?,"P n \times n  q Q = P + qI I Q P 2 \times 2 Q P \begin{pmatrix}
a & b \\
c & d \\
\end{pmatrix} \det Q = (a + q)(d + q) - bc q q q = \frac mn m n 1 
(an+m)(dn+m) = n^2 bc
 n m Q Q",['matrices']
26,Definition of a matrix by matching two vectors,Definition of a matrix by matching two vectors,,"There are two vectors: $\boldsymbol{\hat{y}} = (\hat{y_1}, \hat{y_2}, \dots, \hat{y_n})$ $\boldsymbol{{y}} = ({y_1}, {y_2}, \dots, {y_n})$ The vectors $\boldsymbol{\hat{y}}$ and $\boldsymbol{{y}}$ have some matches, but also some values that do not match. From these vectors I would like to create a matrix, which contains like the identity matrix in the diagonal ones, but only if the values match, otherwise zero. For example : $\boldsymbol{\hat{y}} = (1.0, 2.0, 3.0, 4.0)$ $\boldsymbol{{y}} = (1.0, 2.2, 3.0, 4.0)$ From these vectors results: $$ M= \left(\begin{matrix}1&0&0&0\\0&0&0&0\\0&0&1&0\\0&0&0&1\end{matrix}\right)$$ Is there a way to describe the definition of $M$ mathematically?","There are two vectors: The vectors and have some matches, but also some values that do not match. From these vectors I would like to create a matrix, which contains like the identity matrix in the diagonal ones, but only if the values match, otherwise zero. For example : From these vectors results: Is there a way to describe the definition of mathematically?","\boldsymbol{\hat{y}} = (\hat{y_1}, \hat{y_2}, \dots, \hat{y_n}) \boldsymbol{{y}} = ({y_1}, {y_2}, \dots, {y_n}) \boldsymbol{\hat{y}} \boldsymbol{{y}} \boldsymbol{\hat{y}} = (1.0, 2.0, 3.0, 4.0) \boldsymbol{{y}} = (1.0, 2.2, 3.0, 4.0)  M= \left(\begin{matrix}1&0&0&0\\0&0&0&0\\0&0&1&0\\0&0&0&1\end{matrix}\right) M","['matrices', 'vectors']"
27,Maximum of det(AX) as a function of Tr(AX),Maximum of det(AX) as a function of Tr(AX),,"Let $A$ = $\begin{pmatrix} 3&0&1\\ -1&2&-1\\ -2&-2&1 \end{pmatrix}$ and $X$ square matrix of order 3 diagonalizable and meets $AX=XA$ . When $Tr(AX) = d$ , I need to find the maximum of $det(AX)$ as a function of $d$ . All eigenvalues of $X$ are positive. What I have so far : I know that $Tr(AX) = \sum_k{\lambda_k}$ and $Det(AX) = \prod_k{\lambda_k}$ with $\lambda_k$ eigenvalues of $AX$ . So to maximize $Det(AX)$ I can maximize $Ln(Det(AX)) = \sum_k{Ln(\lambda_k)}$ . Therefore I have $\dfrac{\partial Ln(Det(AX))}{\partial \lambda_k} = \dfrac{1}{\lambda_k}$ . But I don't know how to continue.","Let = and square matrix of order 3 diagonalizable and meets . When , I need to find the maximum of as a function of . All eigenvalues of are positive. What I have so far : I know that and with eigenvalues of . So to maximize I can maximize . Therefore I have . But I don't know how to continue.","A \begin{pmatrix}
3&0&1\\
-1&2&-1\\
-2&-2&1
\end{pmatrix} X AX=XA Tr(AX) = d det(AX) d X Tr(AX) = \sum_k{\lambda_k} Det(AX) = \prod_k{\lambda_k} \lambda_k AX Det(AX) Ln(Det(AX)) = \sum_k{Ln(\lambda_k)} \dfrac{\partial Ln(Det(AX))}{\partial \lambda_k} = \dfrac{1}{\lambda_k}","['matrices', 'determinant', 'diagonalization']"
28,Writing a matrix as a product of two matrices,Writing a matrix as a product of two matrices,,"Consider the matrix $$ A = \begin{pmatrix} 0 & y & -x\\ y & y^2 & -xy\\ -x & -xy & x^2  \end{pmatrix}. $$ Is it possible to find matrices $X = X(x)$ and $Y=Y(y)$ such that $A = XY$ (or $A = YX$ )? A possibly unrelated observation of mine is that if we consider the vector $v = \begin{pmatrix}y\\-x\end{pmatrix}$ , then we can write $A$ in block form as $$ A = \begin{pmatrix} 0 & v^t\\ v & vv^t \end{pmatrix}, $$ which allows us to write $$ A = \begin{pmatrix} 1 & 1\\ 0 & v \end{pmatrix}\cdot \begin{pmatrix} -1 & 0\\ 1 & v^t \end{pmatrix}, $$ but this is not really what I want since now the factors depend on both $x$ and $y$ . EDIT: As suggested in the comments, setting $z = -x$ yields $$ A = \begin{pmatrix} 0 & y & z\\ y & y^2 & yz\\ z & yz & z^2  \end{pmatrix}. $$","Consider the matrix Is it possible to find matrices and such that (or )? A possibly unrelated observation of mine is that if we consider the vector , then we can write in block form as which allows us to write but this is not really what I want since now the factors depend on both and . EDIT: As suggested in the comments, setting yields"," A = \begin{pmatrix}
0 & y & -x\\
y & y^2 & -xy\\
-x & -xy & x^2 
\end{pmatrix}.  X = X(x) Y=Y(y) A = XY A = YX v = \begin{pmatrix}y\\-x\end{pmatrix} A  A = \begin{pmatrix}
0 & v^t\\
v & vv^t
\end{pmatrix},   A = \begin{pmatrix}
1 & 1\\
0 & v
\end{pmatrix}\cdot
\begin{pmatrix}
-1 & 0\\
1 & v^t
\end{pmatrix},  x y z = -x  A = \begin{pmatrix}
0 & y & z\\
y & y^2 & yz\\
z & yz & z^2 
\end{pmatrix}. ","['matrices', 'matrix-decomposition']"
29,eigenvalues of $AA^T$ and $A^TA$ [duplicate],eigenvalues of  and  [duplicate],AA^T A^TA,This question already has answers here : Let $A$ be an $m \times n$ matrix. Show $A^TA$ and $AA^T$ have the same eigenvalues (4 answers) Closed 5 years ago . Is it true (and under which conditions) that the products of an non-square matrix $A$ and its transpose and vice versa (so the product of the transpose and $A$) share the same eigenvalues (multiplicities omitted)?,This question already has answers here : Let $A$ be an $m \times n$ matrix. Show $A^TA$ and $AA^T$ have the same eigenvalues (4 answers) Closed 5 years ago . Is it true (and under which conditions) that the products of an non-square matrix $A$ and its transpose and vice versa (so the product of the transpose and $A$) share the same eigenvalues (multiplicities omitted)?,,"['matrices', 'eigenvalues-eigenvectors', 'transpose']"
30,Eigenvalues of a $2 \times 2$ block matrix where every block is an identity matrix,Eigenvalues of a  block matrix where every block is an identity matrix,2 \times 2,"I want to consider the following matrix: \begin{bmatrix}\boldsymbol{I}_n & \boldsymbol{I}_n \\\boldsymbol{I}_n & \boldsymbol{I}_n\end{bmatrix} By doing several numerical examples, I recognized that this matrix has $n$ eigenvalues equal to zero and $n$ eigenvalues equal to $2$. Is there any way to prove this for an arbitrary number $n$?","I want to consider the following matrix: \begin{bmatrix}\boldsymbol{I}_n & \boldsymbol{I}_n \\\boldsymbol{I}_n & \boldsymbol{I}_n\end{bmatrix} By doing several numerical examples, I recognized that this matrix has $n$ eigenvalues equal to zero and $n$ eigenvalues equal to $2$. Is there any way to prove this for an arbitrary number $n$?",,"['matrices', 'eigenvalues-eigenvectors', 'block-matrices']"
31,Why is Matrix Addition defined as element by element addition?,Why is Matrix Addition defined as element by element addition?,,"Matrix multiplication is defined as row by column multiplication. It represents linear transformation. Why isn't matrix addition defined in a similar way: row by column addition. Given to matrices A and B (of the same size). What transformation, if any, does (element by element) addition  A+B represent? What would row by column addition represent? How about the Hadamard product (element by element matrix multiplication) represent?","Matrix multiplication is defined as row by column multiplication. It represents linear transformation. Why isn't matrix addition defined in a similar way: row by column addition. Given to matrices A and B (of the same size). What transformation, if any, does (element by element) addition  A+B represent? What would row by column addition represent? How about the Hadamard product (element by element matrix multiplication) represent?",,['matrices']
32,$PGL(n)$ is a closed subgroup of $GL(n^2)$,is a closed subgroup of,PGL(n) GL(n^2),"Assume the base field is complex numbers. Let $PGL(n)$ be the projective linear group. I heard it is a closed algebraic subgroup of $GL(n^2)$, but I am having trouble proving it. What I tried is embedding a matrix in $PGL(n)$ using the ratios of its entries, but this works bad when there is $0$ in the matrix. Similar question has been asked here. How to realise $\mathrm{PGL}_2$ as a closed subgroup of some $\mathrm{GL}_n$ explicitly? Thanks for the help.","Assume the base field is complex numbers. Let $PGL(n)$ be the projective linear group. I heard it is a closed algebraic subgroup of $GL(n^2)$, but I am having trouble proving it. What I tried is embedding a matrix in $PGL(n)$ using the ratios of its entries, but this works bad when there is $0$ in the matrix. Similar question has been asked here. How to realise $\mathrm{PGL}_2$ as a closed subgroup of some $\mathrm{GL}_n$ explicitly? Thanks for the help.",,"['matrices', 'group-theory', 'algebraic-groups']"
33,Exponential of a symmetric tridiagonal Toeplitz matrix,Exponential of a symmetric tridiagonal Toeplitz matrix,,Let $\alpha$ be a (strictly) positive real number. Consider the following tridiagonal Toeplitz matrix $$ A=\alpha\begin{bmatrix} 0 & 1 & 0 &\cdots & 0\\ 1 & 0 & 1 &\ddots & \vdots \\ 0 & 1 & 0 & \ddots & 0\\ \vdots & \ddots & \ddots & \ddots & 1 \\ 0 & \cdots & 0 & 1 & 0 \end{bmatrix}. $$ My question. Does there exist a closed-form expression for $\exp(A)$? I played around a little bit with the truncated series $\sum_{k=0}^N \frac{A^k}{k!}$ but I didn't manage to provide an answer to my question. Pointers to the literature are also welcome!,Let $\alpha$ be a (strictly) positive real number. Consider the following tridiagonal Toeplitz matrix $$ A=\alpha\begin{bmatrix} 0 & 1 & 0 &\cdots & 0\\ 1 & 0 & 1 &\ddots & \vdots \\ 0 & 1 & 0 & \ddots & 0\\ \vdots & \ddots & \ddots & \ddots & 1 \\ 0 & \cdots & 0 & 1 & 0 \end{bmatrix}. $$ My question. Does there exist a closed-form expression for $\exp(A)$? I played around a little bit with the truncated series $\sum_{k=0}^N \frac{A^k}{k!}$ but I didn't manage to provide an answer to my question. Pointers to the literature are also welcome!,,"['matrices', 'reference-request', 'matrix-exponential', 'tridiagonal-matrices', 'toeplitz-matrices']"
34,Real logarithms of special orthogonal matrices,Real logarithms of special orthogonal matrices,,"In continuation to the following question : Let $P\in\text{SO}(2)$ and let $X$ be a real matrix such that $P = e^X$. Is $X$ necessarily skew-symmetric? No. The above-mentioned thread gives counter examples. However, are there counter examples in which $P$ is neither $I_2$ nor $-I_2$? More generally, in higher dimension, what can be said about special orthogonal matrices that have non-skew-symmetric real logarithms?","In continuation to the following question : Let $P\in\text{SO}(2)$ and let $X$ be a real matrix such that $P = e^X$. Is $X$ necessarily skew-symmetric? No. The above-mentioned thread gives counter examples. However, are there counter examples in which $P$ is neither $I_2$ nor $-I_2$? More generally, in higher dimension, what can be said about special orthogonal matrices that have non-skew-symmetric real logarithms?",,"['matrices', 'lie-groups', 'matrix-exponential']"
35,Metric for how symmetric a matrix is,Metric for how symmetric a matrix is,,"Given a square NxN matrix A, what is a measure of how symmetric A is? I can get the symmetric and antisymmetric parts of A as: $A_{sym}=\frac{1}{2}(A+A^{T})$ and $A_{anti}=\frac{1}{2}(A-A^{T})$ Is there some commonly used function, $F(A,A_{sym},A_{anti})$, that gives a measure of how symmetric a matrix is? E.g. something like the ratio of the determinants of $A_{sym}$ and $A_{anti}$?","Given a square NxN matrix A, what is a measure of how symmetric A is? I can get the symmetric and antisymmetric parts of A as: $A_{sym}=\frac{1}{2}(A+A^{T})$ and $A_{anti}=\frac{1}{2}(A-A^{T})$ Is there some commonly used function, $F(A,A_{sym},A_{anti})$, that gives a measure of how symmetric a matrix is? E.g. something like the ratio of the determinants of $A_{sym}$ and $A_{anti}$?",,"['matrices', 'eigenvalues-eigenvectors', 'determinant', 'symmetric-matrices']"
36,The lower bound of the smallest eigenvalue of a symmetric positive definite matrix,The lower bound of the smallest eigenvalue of a symmetric positive definite matrix,,"I encounter a symmetric positive definite matrix whose features are all diagonal entries are $1$. all the other entries are in $[0, 1)$, but the matrix is not diagonally dominant. Now I am looking for a positive lower bound of the smallest eigenvalue, expressed by trace and Frobenius norm. I have seen a lot of papers related to this topic. Especially, the result in this paper is very close to my goal. But that expression still involves the maximum eigenvalue and determinant. I have seen the answer of Lower bound on the smallest eigenvalue . I'm happy if the answer posted in that is correct. But I think it's wrong. Does that kind of lower bound exist? Anyone could give me any tips? Thanks so much!","I encounter a symmetric positive definite matrix whose features are all diagonal entries are $1$. all the other entries are in $[0, 1)$, but the matrix is not diagonally dominant. Now I am looking for a positive lower bound of the smallest eigenvalue, expressed by trace and Frobenius norm. I have seen a lot of papers related to this topic. Especially, the result in this paper is very close to my goal. But that expression still involves the maximum eigenvalue and determinant. I have seen the answer of Lower bound on the smallest eigenvalue . I'm happy if the answer posted in that is correct. But I think it's wrong. Does that kind of lower bound exist? Anyone could give me any tips? Thanks so much!",,"['matrices', 'eigenvalues-eigenvectors', 'positive-definite']"
37,Moore Penrose Inverse for symmetric matrix [closed],Moore Penrose Inverse for symmetric matrix [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Is there any proof that a MPI of symetric matrix is symmetric matrix? Basically I need that Moore-Penroses invers of positive semidefinite matrix is positive semidefinite. I can show that x^T(A+)x >=0. But A+ also need to be symmetric for positive semidefinite. Thank you for your help.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Is there any proof that a MPI of symetric matrix is symmetric matrix? Basically I need that Moore-Penroses invers of positive semidefinite matrix is positive semidefinite. I can show that x^T(A+)x >=0. But A+ also need to be symmetric for positive semidefinite. Thank you for your help.",,['matrices']
38,Why is $\nabla \log{\det{X}} = X^{-1}$? Where did the trace go?,Why is ? Where did the trace go?,\nabla \log{\det{X}} = X^{-1},"I am studying Boyd & Vandenberghe's Convex Optimization and encountered a problem on page 642. According to the definition, the derivative $Df(x)$ has the form: $$f(x)+Df(x)(z-x)$$ and when $f$ is real-valued (i.e., $f : \Bbb R^n \to \Bbb R$ ), the gradient is $$\nabla{f(x)}=Df(x)^{T}$$ See the original text below: But when discussing the gradient of function $f(X)=\log{\det{X}}$ , author said ""we can identify $X^{-1}$ as the gradient of $f$ at $X$ "", please see below: Where did trace $\mbox{tr}(\cdot)$ go?","I am studying Boyd & Vandenberghe's Convex Optimization and encountered a problem on page 642. According to the definition, the derivative has the form: and when is real-valued (i.e., ), the gradient is See the original text below: But when discussing the gradient of function , author said ""we can identify as the gradient of at "", please see below: Where did trace go?",Df(x) f(x)+Df(x)(z-x) f f : \Bbb R^n \to \Bbb R \nabla{f(x)}=Df(x)^{T} f(X)=\log{\det{X}} X^{-1} f X \mbox{tr}(\cdot),"['matrices', 'derivatives', 'matrix-calculus', 'scalar-fields']"
39,Are $B=PAP^{-1}$ and $B=P^{-1}AP$ equivalent?,Are  and  equivalent?,B=PAP^{-1} B=P^{-1}AP,"Im looking at the solution to one of my questions. Basically, we started off with a matrix $A$ (in the elementary basis) which we want to convert into a diagonal matrix $B$ of another basis. Question 1: The teacher uses $B=PAP^{-1}$ and $B=P^{-1}AP$ interchangeably, can I really do that if $P$ is the basis of $B$? I would think they would give different values for $B$ wouldn't they? Question 2: Since Matrix $A$ is in the elementary basis, isn't $BP=A$? Why can't we find $B$ by $AP^{-1}$ and why do we have $BP=P^{-1}A$?","Im looking at the solution to one of my questions. Basically, we started off with a matrix $A$ (in the elementary basis) which we want to convert into a diagonal matrix $B$ of another basis. Question 1: The teacher uses $B=PAP^{-1}$ and $B=P^{-1}AP$ interchangeably, can I really do that if $P$ is the basis of $B$? I would think they would give different values for $B$ wouldn't they? Question 2: Since Matrix $A$ is in the elementary basis, isn't $BP=A$? Why can't we find $B$ by $AP^{-1}$ and why do we have $BP=P^{-1}A$?",,"['matrices', 'transformation']"
40,Can a constant be considered as 1x1 matrix?,Can a constant be considered as 1x1 matrix?,,"A constant $c$ can be considered as 1 x 1 matrix $\;$ $( c )$ , it makes sense in terms of matrix inverse, matrix addition etc but the multiplication of a constant is possible to any matrix ( by multiplying all entries of matrix with it ) but if we consider constant as 1 x 1 matrix ,its multiplication with other matrices should not make sense ( as $m$ x $n$ matrix can be multiplies by $n$ x $k$ matrix only ). So why is constant multiplication defined the way it is and can we consider a constant as a 1 x 1 matrix ?","A constant $c$ can be considered as 1 x 1 matrix $\;$ $( c )$ , it makes sense in terms of matrix inverse, matrix addition etc but the multiplication of a constant is possible to any matrix ( by multiplying all entries of matrix with it ) but if we consider constant as 1 x 1 matrix ,its multiplication with other matrices should not make sense ( as $m$ x $n$ matrix can be multiplies by $n$ x $k$ matrix only ). So why is constant multiplication defined the way it is and can we consider a constant as a 1 x 1 matrix ?",,['matrices']
41,Eigenvalues of 3D rotation matrix,Eigenvalues of 3D rotation matrix,,"I'm having some trouble calculating the eigenvalues for this rotation matrix, I know that you subtract a $\lambda$ from each diagonal term and take the determinant and solve the equation for $\lambda$ but I think I'm having some trouble with trig identities since I can't seem to find the correct answer. Since this is a rotation matrix i know that the $|\lambda|$ must = 1 I just can't seem to get it to work out. Attempt: $\begin{bmatrix}\cos(\theta)&0&-\sin(\theta)\\0&1&0\\\sin(\theta)&0&\cos(\theta)\end{bmatrix}$ $\begin{bmatrix}\cos(\theta)-\lambda&0&-\sin(\theta)\\0&1-\lambda&0\\\sin(\theta)&0&\cos(\theta)-\lambda\end{bmatrix}$ $((\cos(\theta)-\lambda)*det\begin{bmatrix}1-\lambda&0\\0&\cos(\theta)-\lambda\end{bmatrix})$ - $0$ +$((-\sin(\theta)*det\begin{bmatrix}0&1-\lambda\\\sin(\theta)&0\end{bmatrix})$ $=0$ $(\cos(\theta)-\lambda)*[(1-\lambda)(\cos(\theta)-\lambda)]$ - $0$ +$(-\sin(\theta)*[0-(1-\lambda)(\sin(\theta)]$ = $0$ $(\cos(\theta) - \lambda)(\cos(\theta)-\lambda-\lambda\cos(\theta)+\lambda^2)-[\sin(\theta)(-(\sin(\theta)-\lambda\sin(\theta)]$ =$0$ $[-\lambda^3+\lambda^2+\lambda^2(2\cos(\theta))-\lambda(\cos^2(\theta))-\lambda(2\cos(\theta))+\cos^2(\theta)]+[\sin(\theta)-\lambda\sin^2(\theta)]=0$ combining like terms i end up with $[-\lambda^3+\lambda^2(1+2\cos(\theta))-\lambda(\cos^2(\theta)+2\cos(\theta)+\sin^2(\theta))+\cos^2(\theta)+\sin^2(\theta)$=$0$ I think this is correct so far, but its very possible i made a sign error or something like that. I can't seem to solve for $\lambda$ in any way that i can see.I think I'm very close, just not really good with my trig identities. Help would be greatly appreciated, Thanks a lot!","I'm having some trouble calculating the eigenvalues for this rotation matrix, I know that you subtract a $\lambda$ from each diagonal term and take the determinant and solve the equation for $\lambda$ but I think I'm having some trouble with trig identities since I can't seem to find the correct answer. Since this is a rotation matrix i know that the $|\lambda|$ must = 1 I just can't seem to get it to work out. Attempt: $\begin{bmatrix}\cos(\theta)&0&-\sin(\theta)\\0&1&0\\\sin(\theta)&0&\cos(\theta)\end{bmatrix}$ $\begin{bmatrix}\cos(\theta)-\lambda&0&-\sin(\theta)\\0&1-\lambda&0\\\sin(\theta)&0&\cos(\theta)-\lambda\end{bmatrix}$ $((\cos(\theta)-\lambda)*det\begin{bmatrix}1-\lambda&0\\0&\cos(\theta)-\lambda\end{bmatrix})$ - $0$ +$((-\sin(\theta)*det\begin{bmatrix}0&1-\lambda\\\sin(\theta)&0\end{bmatrix})$ $=0$ $(\cos(\theta)-\lambda)*[(1-\lambda)(\cos(\theta)-\lambda)]$ - $0$ +$(-\sin(\theta)*[0-(1-\lambda)(\sin(\theta)]$ = $0$ $(\cos(\theta) - \lambda)(\cos(\theta)-\lambda-\lambda\cos(\theta)+\lambda^2)-[\sin(\theta)(-(\sin(\theta)-\lambda\sin(\theta)]$ =$0$ $[-\lambda^3+\lambda^2+\lambda^2(2\cos(\theta))-\lambda(\cos^2(\theta))-\lambda(2\cos(\theta))+\cos^2(\theta)]+[\sin(\theta)-\lambda\sin^2(\theta)]=0$ combining like terms i end up with $[-\lambda^3+\lambda^2(1+2\cos(\theta))-\lambda(\cos^2(\theta)+2\cos(\theta)+\sin^2(\theta))+\cos^2(\theta)+\sin^2(\theta)$=$0$ I think this is correct so far, but its very possible i made a sign error or something like that. I can't seem to solve for $\lambda$ in any way that i can see.I think I'm very close, just not really good with my trig identities. Help would be greatly appreciated, Thanks a lot!",,"['matrices', 'eigenvalues-eigenvectors', 'rotations', 'matrix-equations']"
42,Can someone please provide an intuition behind cramer's rule?,Can someone please provide an intuition behind cramer's rule?,,"See question. I usually get concepts like this very quickly (no studying required), but this one looks like Chinese. Can someone please help me understand a brief intuition behind Cramer's rule for 2x2 and 3x3 matrices? TYVM","See question. I usually get concepts like this very quickly (no studying required), but this one looks like Chinese. Can someone please help me understand a brief intuition behind Cramer's rule for 2x2 and 3x3 matrices? TYVM",,"['matrices', 'systems-of-equations', 'matrix-equations']"
43,set of symmetric positive definite matrix open?,set of symmetric positive definite matrix open?,,"I consider a collection of symmetric positive definite matrices of the same dimension. I've learned it's an open set but have no clue about the proof. Also, can the symmetry condition be dropped? Thanks.","I consider a collection of symmetric positive definite matrices of the same dimension. I've learned it's an open set but have no clue about the proof. Also, can the symmetry condition be dropped? Thanks.",,"['matrices', 'functional-analysis']"
44,Number of flops required to invert a matrix,Number of flops required to invert a matrix,,I have an n-by-n upper triangular matrix $R$ and I can calculate its inverse by back substitution. I cannot make myself see why it needs $O(n^{3}/3)$ flops to do so. Can you explain?,I have an n-by-n upper triangular matrix $R$ and I can calculate its inverse by back substitution. I cannot make myself see why it needs $O(n^{3}/3)$ flops to do so. Can you explain?,,"['matrices', 'inverse', 'computational-complexity']"
45,A formula in 'The Matrix Cookbook',A formula in 'The Matrix Cookbook',,"In section 9.4 (Idempotent Matrices), the book says that : if $A$ is idempotent, which means that $AA = A$, then $f(sI + tA) = (I-A)f(s) + Af(s+t)$ but I don't understand the meaning of this formula, can anyone tell me where does it come from or show me how to proof this formula? Thanks.","In section 9.4 (Idempotent Matrices), the book says that : if $A$ is idempotent, which means that $AA = A$, then $f(sI + tA) = (I-A)f(s) + Af(s+t)$ but I don't understand the meaning of this formula, can anyone tell me where does it come from or show me how to proof this formula? Thanks.",,['matrices']
46,Proof with binomial coefficient and kronecker delta,Proof with binomial coefficient and kronecker delta,,"I want to prove that $$ \sum_{k=i}^n \binom{n}{k}\binom{k}{i}(-1)^{n-k}=\delta_{n,i} $$ Where $\delta_{n,i}$ is the Kronecker Delta, i.e. $\delta_{n,i}=0$ if $n \neq i$ and $\delta_{n,i}=1$ if $i=n$. I have an intuitive proof with matrices representing the linear transformation $F(x)=(1+x)^n$ and $G(x)=(1-x)^n$ and their product.  But I'd prefer an accurate proof, if it exists. Thanks.","I want to prove that $$ \sum_{k=i}^n \binom{n}{k}\binom{k}{i}(-1)^{n-k}=\delta_{n,i} $$ Where $\delta_{n,i}$ is the Kronecker Delta, i.e. $\delta_{n,i}=0$ if $n \neq i$ and $\delta_{n,i}=1$ if $i=n$. I have an intuitive proof with matrices representing the linear transformation $F(x)=(1+x)^n$ and $G(x)=(1-x)^n$ and their product.  But I'd prefer an accurate proof, if it exists. Thanks.",,"['matrices', 'summation', 'binomial-coefficients']"
47,Conjugate elements of $GL_2(\mathbb{R})$,Conjugate elements of,GL_2(\mathbb{R}),"Decide whether or not the two matrices $A= \begin{pmatrix} 3 & 0 \\ 0 & 2 \end{pmatrix}$ and $B= \begin{pmatrix} 1 & 1 \\ -2 & 4 \end{pmatrix}$ are conjugate elements of the general linear group $GL_2(\mathbb{R})$ So I need to show that there exists a matrix $X= \begin{pmatrix} a & b \\ c & d \end{pmatrix} \in GL_2(\mathbb{R})$ such that $a,b,c,d \in \mathbb{R}$ and such that $ad-bc \neq 0$ I solved the system $XAX^{-1}=B$ and found that the the matrix is of the form $X= \begin{pmatrix} a & 0 \\ b & 0 \end{pmatrix}$ which is not invertible hence A and B are not conjugate elements. Is it correct?  Is there an easier way to solve this type of question ?","Decide whether or not the two matrices $A= \begin{pmatrix} 3 & 0 \\ 0 & 2 \end{pmatrix}$ and $B= \begin{pmatrix} 1 & 1 \\ -2 & 4 \end{pmatrix}$ are conjugate elements of the general linear group $GL_2(\mathbb{R})$ So I need to show that there exists a matrix $X= \begin{pmatrix} a & b \\ c & d \end{pmatrix} \in GL_2(\mathbb{R})$ such that $a,b,c,d \in \mathbb{R}$ and such that $ad-bc \neq 0$ I solved the system $XAX^{-1}=B$ and found that the the matrix is of the form $X= \begin{pmatrix} a & 0 \\ b & 0 \end{pmatrix}$ which is not invertible hence A and B are not conjugate elements. Is it correct?  Is there an easier way to solve this type of question ?",,['group-theory']
48,Invertability of Singular 2x2 Matrix with all same real values.,Invertability of Singular 2x2 Matrix with all same real values.,,"Question: Let set G = { matrix [{a a},{a a}] such that a is real but not 0 } represent the set of 2x2 matrices with same elements of the reals excluding a = 0, show that G is a group under matrix multiplication. Explain why each element of G has an inverse even though the matrices have 0 determinants. Thoughts: I can show closure, the existence of a unique identity and associativity; but the inverse has me lost. How can a 2x2 singular matrix with identical elements from the reals (excluding zero) be a group under matrix multiplication when no inverse exist that I can see (though the book asserts otherwise)? Problem is from Contemporary Abstract Algebra (Gallian 8th Ed) Ch2 Problem 52.","Question: Let set G = { matrix [{a a},{a a}] such that a is real but not 0 } represent the set of 2x2 matrices with same elements of the reals excluding a = 0, show that G is a group under matrix multiplication. Explain why each element of G has an inverse even though the matrices have 0 determinants. Thoughts: I can show closure, the existence of a unique identity and associativity; but the inverse has me lost. How can a 2x2 singular matrix with identical elements from the reals (excluding zero) be a group under matrix multiplication when no inverse exist that I can see (though the book asserts otherwise)? Problem is from Contemporary Abstract Algebra (Gallian 8th Ed) Ch2 Problem 52.",,"['group-theory', 'matrices', 'abelian-groups', 'matrix-equations']"
49,How to solve this determinant: $a_{ij}=|i-j|+1$?,How to solve this determinant: ?,a_{ij}=|i-j|+1,"I have to solve determinant of the following form: $$a_{ij}=|i-j|+1$$ It looks like this: $$ \begin{pmatrix} 1 & 2 & 3 & 4 & \cdots & n \\ 2 & 1 & 2 & 3 & \cdots & n-1 \\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ n & n-1 & n-2 & n-3 & \cdots & 1 \end{pmatrix} $$ It looks something like Toeplitz matrix, but I haven't found any method of solving it. I would appreciate also a kind of hint that would help. EDIT: OEIS gives a formula for absolute value: $(n+1)\cdot2^{n-2}$ http://oeis.org/A001792 Thanks in advance!","I have to solve determinant of the following form: $$a_{ij}=|i-j|+1$$ It looks like this: $$ \begin{pmatrix} 1 & 2 & 3 & 4 & \cdots & n \\ 2 & 1 & 2 & 3 & \cdots & n-1 \\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ n & n-1 & n-2 & n-3 & \cdots & 1 \end{pmatrix} $$ It looks something like Toeplitz matrix, but I haven't found any method of solving it. I would appreciate also a kind of hint that would help. EDIT: OEIS gives a formula for absolute value: $(n+1)\cdot2^{n-2}$ http://oeis.org/A001792 Thanks in advance!",,"['matrices', 'determinant']"
50,Proof that a strongly connected digraph has an irreducible adjacency matrix,Proof that a strongly connected digraph has an irreducible adjacency matrix,,I need to prove that a strongly connected digraph has an irreducible adjacency matrix. If anybody would be willing to give an advice on how to tackle this problem I would be thankful.,I need to prove that a strongly connected digraph has an irreducible adjacency matrix. If anybody would be willing to give an advice on how to tackle this problem I would be thankful.,,"['matrices', 'graph-theory', 'proof-writing']"
51,What indexes do the subgroups of $\mathrm{GL}_n(\Bbb C)$ have?,What indexes do the subgroups of  have?,\mathrm{GL}_n(\Bbb C),"Let $B_n\subset\mathrm{GL}_n(\Bbb C)$ be the group of invertible upper-triangular matrices. What is the index $[\mathrm{GL}_n(\Bbb C):B_n]?$ (By index I mean the cardinality of a coset space.) In this answer , it is shown that the index is infinite. It also follows from the conjunction of this and this answer. But which cardinal is it? I'm almost certain that it's $\mathfrak c,$ but I have no idea how to prove it. More generally, which cardinals are the indexes $[\mathrm{GL}_n(\Bbb C):G],$ where $G$ runs over the set of subgroups of $\mathrm{GL}_n(\Bbb C)?$ According to the latter two linked answers, there shouldn't be any finite cardinals there. I suspect all of these cardinals are actually $\mathfrak c$, but I wouldn't know how to prove it either (and this suspicion is much weaker than the previous one).","Let $B_n\subset\mathrm{GL}_n(\Bbb C)$ be the group of invertible upper-triangular matrices. What is the index $[\mathrm{GL}_n(\Bbb C):B_n]?$ (By index I mean the cardinality of a coset space.) In this answer , it is shown that the index is infinite. It also follows from the conjunction of this and this answer. But which cardinal is it? I'm almost certain that it's $\mathfrak c,$ but I have no idea how to prove it. More generally, which cardinals are the indexes $[\mathrm{GL}_n(\Bbb C):G],$ where $G$ runs over the set of subgroups of $\mathrm{GL}_n(\Bbb C)?$ According to the latter two linked answers, there shouldn't be any finite cardinals there. I suspect all of these cardinals are actually $\mathfrak c$, but I wouldn't know how to prove it either (and this suspicion is much weaker than the previous one).",,"['group-theory', 'matrices', 'complex-numbers', 'cardinals']"
52,2D transformation matrix to make a trapezoid out of a rectangle,2D transformation matrix to make a trapezoid out of a rectangle,,"In most vector graphic software libraries I can use (3x3) matrices to transform 2D geometry (e.g. scale, rotate, skew). How does a matrix need to look like to transform a 2D rectangle to a symmetrical trapezoid (or  equilateral triangle if parameters are taken to the extreme)? What parts of the matrix define which parameters?","In most vector graphic software libraries I can use (3x3) matrices to transform 2D geometry (e.g. scale, rotate, skew). How does a matrix need to look like to transform a 2D rectangle to a symmetrical trapezoid (or  equilateral triangle if parameters are taken to the extreme)? What parts of the matrix define which parameters?",,"['geometry', 'matrices']"
53,How do I calculate the derivative of matrix?,How do I calculate the derivative of matrix?,,"I'd like to expand a real, symmetric and positive definite Matrix $M$ into a Taylor series. I'm trying to do $$ M (T) = M(T_0) + \frac{\partial M}{\partial T} (T-T_0) + \cdots$$ $T$ is a algebraic vector of parameters (e.g. temperatures at finite element nodes). I'm only interested in the first order term, i.e. the derivative of $M$ at $T_0$. My Professor tried to do this, but used unclear notation.","I'd like to expand a real, symmetric and positive definite Matrix $M$ into a Taylor series. I'm trying to do $$ M (T) = M(T_0) + \frac{\partial M}{\partial T} (T-T_0) + \cdots$$ $T$ is a algebraic vector of parameters (e.g. temperatures at finite element nodes). I'm only interested in the first order term, i.e. the derivative of $M$ at $T_0$. My Professor tried to do this, but used unclear notation.",,"['calculus', 'matrices', 'numerical-methods']"
54,Is there something wrong with this question concerning Groups,Is there something wrong with this question concerning Groups,,"We consider the group $G = SL(2, 3) $ i.e, the set of $2 \times2$ matrices with determinant 1 and addition and multiplication are performed modulo 3 even in the determinant formula. One can show that $|G| = 24$ a) Let $\alpha = \begin{pmatrix} 2 & 2\\ 2 & 1 \end{pmatrix}$ show that $\alpha \in G$ and find its inverse $\alpha^{-1}$ . b) Let H = { $ \begin{pmatrix} a & b\\ 0 & a \end{pmatrix} $ where a, b $\in$ {0,1,2}, a $\neq 0$ }. Show that H is a subgroup and find a familiar group that is Isomorphic to H. c) The subgroup H contains two elements of order 3. Find 8 elements of order 3 in G. HINT : H only contains upper triangular matrices; also use conjugation. It's trivial to show H $\leq$ G and I found it's Isomorphic to $D_{6}$ . It's the last part I have found issues in my understanding. I wrote out all the elements of H: $ \begin{pmatrix} 1 & 0\\ 0 & 1 \end{pmatrix}$ $ \begin{pmatrix} 2 & 0\\0 & 2 \end{pmatrix} $ $ \begin{pmatrix} 2 & 1\\0 & 2 \end{pmatrix} $ $ \begin{pmatrix} 2 & 2\\0 & 2 \end{pmatrix} $ $ \begin{pmatrix} 1 & 1\\0 & 1 \end{pmatrix} $ $ \begin{pmatrix} 1 & 2\\0 & 1 \end{pmatrix} $ Four of these have an order of 3. Not just two of them. Or is my understanding off?","We consider the group i.e, the set of matrices with determinant 1 and addition and multiplication are performed modulo 3 even in the determinant formula. One can show that a) Let show that and find its inverse . b) Let H = { where a, b {0,1,2}, a }. Show that H is a subgroup and find a familiar group that is Isomorphic to H. c) The subgroup H contains two elements of order 3. Find 8 elements of order 3 in G. HINT : H only contains upper triangular matrices; also use conjugation. It's trivial to show H G and I found it's Isomorphic to . It's the last part I have found issues in my understanding. I wrote out all the elements of H: Four of these have an order of 3. Not just two of them. Or is my understanding off?","G = SL(2, 3)  2 \times2 |G| = 24 \alpha = \begin{pmatrix}
2 & 2\\
2 & 1
\end{pmatrix} \alpha \in G \alpha^{-1}  \begin{pmatrix}
a & b\\
0 & a
\end{pmatrix}  \in \neq 0 \leq D_{6}  \begin{pmatrix} 1 & 0\\ 0 & 1 \end{pmatrix}  \begin{pmatrix} 2 & 0\\0 & 2 \end{pmatrix}   \begin{pmatrix} 2 & 1\\0 & 2 \end{pmatrix}   \begin{pmatrix} 2 & 2\\0 & 2 \end{pmatrix}   \begin{pmatrix} 1 & 1\\0 & 1 \end{pmatrix}   \begin{pmatrix} 1 & 2\\0 & 1 \end{pmatrix} ","['matrices', 'group-theory', 'finite-groups', 'inverse']"
55,Is every simply-connected nilpotent Lie group algebraic?,Is every simply-connected nilpotent Lie group algebraic?,,"Say a Lie group is a matrix group if it is a closed Lie subgroup of some general linear group. Say a Lie group is algebraic if it is the group of $\mathbb R$ -points of a real linear algebraic group. Clearly every algebraic Lie group is a matrix group, but the converse is false: the simplest counter-example I can think of is the group $\text{GL}_n^+$ of positive-determinant $n\times n$ invertible matrices. By Ado's theorem and the Lie algebra correspondence, every simply-connected nilpotent Lie group is a matrix group. But one could ask for a stronger property, that such groups are algebraic. Is this true? In other words, is every simply-connected nilpotent Lie group algebraic?","Say a Lie group is a matrix group if it is a closed Lie subgroup of some general linear group. Say a Lie group is algebraic if it is the group of -points of a real linear algebraic group. Clearly every algebraic Lie group is a matrix group, but the converse is false: the simplest counter-example I can think of is the group of positive-determinant invertible matrices. By Ado's theorem and the Lie algebra correspondence, every simply-connected nilpotent Lie group is a matrix group. But one could ask for a stronger property, that such groups are algebraic. Is this true? In other words, is every simply-connected nilpotent Lie group algebraic?",\mathbb R \text{GL}_n^+ n\times n,"['matrices', 'lie-groups', 'algebraic-groups', 'nilpotent-groups']"
56,Closed-form solution for the determinant of a Vandermonde-like matrix,Closed-form solution for the determinant of a Vandermonde-like matrix,,"I'm trying to find a closed-form solution $\forall$ odd integer $n\ge 3$ for the determinant of a matrix with some structure on it. After some manipulation, I've reduced it to the following matrix: $\small\begin{bmatrix}\boldsymbol{t_{1}^{n}-t_{a}^{n}} & \boldsymbol{t_{2}^{n}-t_{a}^{n}} & \boldsymbol{\cdots} & \boldsymbol{t_{a-1}^{n}-t_{a}^{n}} & nt_{1}^{n-1} & \cdots & nt_{a-1}^{n-1} & nt_{a}^{n-1}\\ \boldsymbol{t_{1}^{n-1}-t_{a}^{n-1}} & \boldsymbol{t_{2}^{n-1}-t_{a}^{n-1}} & \boldsymbol{\cdots} & \boldsymbol{t_{a-1}^{n-1}-t_{a}^{n-1}} & (n-1)t_{1}^{n-2} & \cdots & (n-1)t_{a-1}^{n-2} & (n-1)t_{a}^{n-2}\\ \boldsymbol{\vdots} & \boldsymbol{\vdots} & \boldsymbol{\ddots} & \boldsymbol{\vdots} & \vdots & \ddots & \vdots & \vdots\\ \boldsymbol{t_{1}^{2}-t_{a}^{2}} & \boldsymbol{t_{2}^{2}-t_{a}^{2}} & \boldsymbol{\cdots} & \boldsymbol{t_{a-1}^{2}-t_{a}^{2}} & 2t_{1} & \cdots & 2t_{a-1} & 2t_{a}\\ \boldsymbol{t_{1}-t_{a}} & \boldsymbol{t_{2}-t_{a}} & \boldsymbol{\cdots} & \boldsymbol{t_{a-1}-t_{a}} & 1 & \cdots & 1 & 1 \end{bmatrix}_{n\times n}$ where $a:=\frac{n+1}{2}$ , the bold block is $n\times(\frac{n+1}{2}-1)$ , and the non-bold block is $n \times \frac{n+1}{2}$ . Although it has some similarities with the Vandermonde Matrix or some generalizations , it's not the same. Using some values of n, its determinant looks pretty simple, which leads me to think that there should be a closed-form solution: $n=3$ : $$ det\left(  \left[\begin{array}{ccc} {t_{1}}^3-{t_{2}}^3 & 3\,{t_{1}}^2 & 3\,{t_{2}}^2\\ {t_{1}}^2-{t_{2}}^2 & 2\,t_{1} & 2\,t_{2}\\ t_{1}-t_{2} & 1 & 1 \end{array}\right] \right)= -{\left(t_{1}-t_{2}\right)}^4 $$ $n=5$ : $$ det\left(      \left[\begin{array}{ccccc} {t_{1}}^5-{t_{3}}^5 & {t_{2}}^5-{t_{3}}^5 & 5\,{t_{1}}^4 & 5\,{t_{2}}^4 & 5\,{t_{3}}^4\\ {t_{1}}^4-{t_{3}}^4 & {t_{2}}^4-{t_{3}}^4 & 4\,{t_{1}}^3 & 4\,{t_{2}}^3 & 4\,{t_{3}}^3\\ {t_{1}}^3-{t_{3}}^3 & {t_{2}}^3-{t_{3}}^3 & 3\,{t_{1}}^2 & 3\,{t_{2}}^2 & 3\,{t_{3}}^2\\ {t_{1}}^2-{t_{3}}^2 & {t_{2}}^2-{t_{3}}^2 & 2\,t_{1} & 2\,t_{2} & 2\,t_{3}\\ t_{1}-t_{3} & t_{2}-t_{3} & 1 & 1 & 1 \end{array}\right] \right)= -{\left(t_{1}-t_{2}\right)}^4\,{\left(t_{1}-t_{3}\right)}^4\,{\left(t_{2}-t_{3}\right)}^4 $$ I was wondering if there is a known closed-form solution for this determinant, or if it could be found using the determinant of a generalized Vandermonde matrix Thanks!","I'm trying to find a closed-form solution odd integer for the determinant of a matrix with some structure on it. After some manipulation, I've reduced it to the following matrix: where , the bold block is , and the non-bold block is . Although it has some similarities with the Vandermonde Matrix or some generalizations , it's not the same. Using some values of n, its determinant looks pretty simple, which leads me to think that there should be a closed-form solution: : : I was wondering if there is a known closed-form solution for this determinant, or if it could be found using the determinant of a generalized Vandermonde matrix Thanks!","\forall n\ge 3 \small\begin{bmatrix}\boldsymbol{t_{1}^{n}-t_{a}^{n}} & \boldsymbol{t_{2}^{n}-t_{a}^{n}} & \boldsymbol{\cdots} & \boldsymbol{t_{a-1}^{n}-t_{a}^{n}} & nt_{1}^{n-1} & \cdots & nt_{a-1}^{n-1} & nt_{a}^{n-1}\\
\boldsymbol{t_{1}^{n-1}-t_{a}^{n-1}} & \boldsymbol{t_{2}^{n-1}-t_{a}^{n-1}} & \boldsymbol{\cdots} & \boldsymbol{t_{a-1}^{n-1}-t_{a}^{n-1}} & (n-1)t_{1}^{n-2} & \cdots & (n-1)t_{a-1}^{n-2} & (n-1)t_{a}^{n-2}\\
\boldsymbol{\vdots} & \boldsymbol{\vdots} & \boldsymbol{\ddots} & \boldsymbol{\vdots} & \vdots & \ddots & \vdots & \vdots\\
\boldsymbol{t_{1}^{2}-t_{a}^{2}} & \boldsymbol{t_{2}^{2}-t_{a}^{2}} & \boldsymbol{\cdots} & \boldsymbol{t_{a-1}^{2}-t_{a}^{2}} & 2t_{1} & \cdots & 2t_{a-1} & 2t_{a}\\
\boldsymbol{t_{1}-t_{a}} & \boldsymbol{t_{2}-t_{a}} & \boldsymbol{\cdots} & \boldsymbol{t_{a-1}-t_{a}} & 1 & \cdots & 1 & 1
\end{bmatrix}_{n\times n} a:=\frac{n+1}{2} n\times(\frac{n+1}{2}-1) n \times \frac{n+1}{2} n=3 
det\left( 
\left[\begin{array}{ccc} {t_{1}}^3-{t_{2}}^3 & 3\,{t_{1}}^2 & 3\,{t_{2}}^2\\ {t_{1}}^2-{t_{2}}^2 & 2\,t_{1} & 2\,t_{2}\\ t_{1}-t_{2} & 1 & 1 \end{array}\right]
\right)=
-{\left(t_{1}-t_{2}\right)}^4
 n=5 
det\left( 
    \left[\begin{array}{ccccc} {t_{1}}^5-{t_{3}}^5 & {t_{2}}^5-{t_{3}}^5 & 5\,{t_{1}}^4 & 5\,{t_{2}}^4 & 5\,{t_{3}}^4\\ {t_{1}}^4-{t_{3}}^4 & {t_{2}}^4-{t_{3}}^4 & 4\,{t_{1}}^3 & 4\,{t_{2}}^3 & 4\,{t_{3}}^3\\ {t_{1}}^3-{t_{3}}^3 & {t_{2}}^3-{t_{3}}^3 & 3\,{t_{1}}^2 & 3\,{t_{2}}^2 & 3\,{t_{3}}^2\\ {t_{1}}^2-{t_{3}}^2 & {t_{2}}^2-{t_{3}}^2 & 2\,t_{1} & 2\,t_{2} & 2\,t_{3}\\ t_{1}-t_{3} & t_{2}-t_{3} & 1 & 1 & 1 \end{array}\right]
\right)=
-{\left(t_{1}-t_{2}\right)}^4\,{\left(t_{1}-t_{3}\right)}^4\,{\left(t_{2}-t_{3}\right)}^4
","['matrices', 'determinant', 'closed-form', 'matrix-decomposition']"
57,Spectrum of a matrix that has only one $1$ in each row,Spectrum of a matrix that has only one  in each row,1,"As in the title, I'm searching for the spectrum of a matrix which has only one $1$ in each row (and zeros as other entries) and also that is not necessary a permutation matrix. For example, $$A =\begin{bmatrix} 1 & 0 &0 \\ 1 &0 &0\\ 0 & 1 &0 \end{bmatrix}$$ Is it true that the specrum of such a thing only contains eigenvalues $\lambda$ of $|\lambda|=1$ or $\lambda = 0$ ?","As in the title, I'm searching for the spectrum of a matrix which has only one in each row (and zeros as other entries) and also that is not necessary a permutation matrix. For example, Is it true that the specrum of such a thing only contains eigenvalues of or ?","1 A =\begin{bmatrix}
1 & 0 &0 \\
1 &0 &0\\
0 & 1 &0
\end{bmatrix} \lambda |\lambda|=1 \lambda = 0","['matrices', 'eigenvalues-eigenvectors', 'stochastic-matrices']"
58,Limit of powers of $3\times3$ matrix,Limit of powers of  matrix,3\times3,"Consider the matrix $$A = \begin{bmatrix} \frac{1}{2} &\frac{1}{2} & 0\\ 0& \frac{3}{4} & \frac{1}{4}\\ 0& \frac{1}{4} & \frac{3}{4} \end{bmatrix}$$ What is $\lim_{n→\infty}$$A^n$ ? A) $\begin{bmatrix} 0 & 0 & 0\\ 0& 0 & 0\\ 0 & 0 & 0 \end{bmatrix}$ B) $\begin{bmatrix} \frac{1}{4} &\frac{1}{2} & \frac{1}{2}\\ \frac{1}{4}& \frac{1}{2} & \frac{1}{2}\\ \frac{1}{4}& \frac{1}{2} & \frac{1}{2}\end{bmatrix}$ C) $\begin{bmatrix} \frac{1}{2} &\frac{1}{4} & \frac{1}{4}\\ \frac{1}{2}& \frac{1}{4} & \frac{1}{4}\\ \frac{1}{2}& \frac{1}{4} & \frac{1}{4}\end{bmatrix}$ D) $\begin{bmatrix} 0 &\frac{1}{2} & \frac{1}{2}\\ 0 & \frac{1}{2} & \frac{1}{2}\\ 0 & \frac{1}{2} & \frac{1}{2}\end{bmatrix}$ E) The limit exists, but it is none of the above The given answer is D). How does one arrive at this result?","Consider the matrix What is ? A) B) C) D) E) The limit exists, but it is none of the above The given answer is D). How does one arrive at this result?",A = \begin{bmatrix} \frac{1}{2} &\frac{1}{2} & 0\\ 0& \frac{3}{4} & \frac{1}{4}\\ 0& \frac{1}{4} & \frac{3}{4} \end{bmatrix} \lim_{n→\infty}A^n \begin{bmatrix} 0 & 0 & 0\\ 0& 0 & 0\\ 0 & 0 & 0 \end{bmatrix} \begin{bmatrix} \frac{1}{4} &\frac{1}{2} & \frac{1}{2}\\ \frac{1}{4}& \frac{1}{2} & \frac{1}{2}\\ \frac{1}{4}& \frac{1}{2} & \frac{1}{2}\end{bmatrix} \begin{bmatrix} \frac{1}{2} &\frac{1}{4} & \frac{1}{4}\\ \frac{1}{2}& \frac{1}{4} & \frac{1}{4}\\ \frac{1}{2}& \frac{1}{4} & \frac{1}{4}\end{bmatrix} \begin{bmatrix} 0 &\frac{1}{2} & \frac{1}{2}\\ 0 & \frac{1}{2} & \frac{1}{2}\\ 0 & \frac{1}{2} & \frac{1}{2}\end{bmatrix},['matrices']
59,About matrices whose row and column sums are 0,About matrices whose row and column sums are 0,,"I think the following theorem is true but I'm not sure Let $(A_{ij})$ be a nonzero $m\times n$ matrix of real numbers such that the sum of the entries of each row and each column is $0$ . Then there are indices $i_1,j_1,i_2,j_2$ with $i_1\neq i_2$ and $j_1\neq j_2$ such that \begin{equation} A_{i_1j_1}>0, A_{i_2j_2}>0, A_{i_1j_2}<0 \text{ and } A_{i_2j_1}<0 \end{equation} I'm looking for a proof or counterexample.",I think the following theorem is true but I'm not sure Let be a nonzero matrix of real numbers such that the sum of the entries of each row and each column is . Then there are indices with and such that I'm looking for a proof or counterexample.,"(A_{ij}) m\times n 0 i_1,j_1,i_2,j_2 i_1\neq i_2 j_1\neq j_2 \begin{equation}
A_{i_1j_1}>0, A_{i_2j_2}>0, A_{i_1j_2}<0 \text{ and } A_{i_2j_1}<0
\end{equation}",['matrices']
60,Proving $S= S^3$,Proving,S= S^3,"Let $\mathrm{A,B,C,D}$ be (not necessarily square) real matrices such that $\mathrm{A^T = BCD , B^T= CDA, C^T = DAB, D^T = ABC}$ for the matrix $\mathrm{S= ABCD}$ prove that $\mathrm{S= S^3}$ Attempt: $\mathrm{S= ABCD \implies S^T = D^TC^T B^T A^T = (ABC)(DAB)(CDA)(BCD)\\ (S^T)^T = (D^TC^TB^TA^T)^3 \implies S^3 = (S^T)}$ (using reversal law) Where have I gone wrong? Or is it that $\mathrm{S= S^T}$, how can we show that?","Let $\mathrm{A,B,C,D}$ be (not necessarily square) real matrices such that $\mathrm{A^T = BCD , B^T= CDA, C^T = DAB, D^T = ABC}$ for the matrix $\mathrm{S= ABCD}$ prove that $\mathrm{S= S^3}$ Attempt: $\mathrm{S= ABCD \implies S^T = D^TC^T B^T A^T = (ABC)(DAB)(CDA)(BCD)\\ (S^T)^T = (D^TC^TB^TA^T)^3 \implies S^3 = (S^T)}$ (using reversal law) Where have I gone wrong? Or is it that $\mathrm{S= S^T}$, how can we show that?",,"['matrices', 'symmetric-matrices']"
61,What is $A$ in this case?,What is  in this case?,A,"Suppose  $A$  is  any  $3×3$  non-singular matrix and $(A−3I)(A−5I)=O$, where $I=I_3$ and $O=O_3$.  If $αA+βA^{−1}=4I$, then $α+β$ is equal to ___ . This seems like a simple and straightforward question, I expanded out the given condition, $$(A−3I)(A−5I)=O \\\implies A^2 -8A +15I = O $$ and then pre-multipied $A^{-1}$ to both sides which  gives: $$A^{-1} \cdot (A^2 -8A +15I )= A^{-1} \cdot O \\\implies A - 8I +15A^{-1} = O \\\text{or}\ \frac{1}2A +\frac{15}{2}A^{-1} = 4I$$ Comparing with what was given in the question,$ \alpha = \frac{1}2$ and $ \beta = \frac{15}2$ which gives the sum to be 8. But then I thought, how is this possible? From the equation $(A−3I)(A−5I)=O$, it seems $A$ can be either $3I$ or $5I$ (definitely not both at the same time), when I substiuted $ A = 3I$ in $αA+βA^{−1}=4I$, I get both α and β in the same equation, if the other possibility is substituted, we get another equation which gives the same result as above when the two are solved simultaneously. But this can't be possible? What is actually the value of $A$ then, since I'm getting such a contradiction?","Suppose  $A$  is  any  $3×3$  non-singular matrix and $(A−3I)(A−5I)=O$, where $I=I_3$ and $O=O_3$.  If $αA+βA^{−1}=4I$, then $α+β$ is equal to ___ . This seems like a simple and straightforward question, I expanded out the given condition, $$(A−3I)(A−5I)=O \\\implies A^2 -8A +15I = O $$ and then pre-multipied $A^{-1}$ to both sides which  gives: $$A^{-1} \cdot (A^2 -8A +15I )= A^{-1} \cdot O \\\implies A - 8I +15A^{-1} = O \\\text{or}\ \frac{1}2A +\frac{15}{2}A^{-1} = 4I$$ Comparing with what was given in the question,$ \alpha = \frac{1}2$ and $ \beta = \frac{15}2$ which gives the sum to be 8. But then I thought, how is this possible? From the equation $(A−3I)(A−5I)=O$, it seems $A$ can be either $3I$ or $5I$ (definitely not both at the same time), when I substiuted $ A = 3I$ in $αA+βA^{−1}=4I$, I get both α and β in the same equation, if the other possibility is substituted, we get another equation which gives the same result as above when the two are solved simultaneously. But this can't be possible? What is actually the value of $A$ then, since I'm getting such a contradiction?",,['matrices']
62,Is the spectral norm of the Jacobian of an $M$-Lipschitz function bounded by $M$?,Is the spectral norm of the Jacobian of an -Lipschitz function bounded by ?,M M,"Well, the title pretty much says everything. I have a function $f: \mathbb{R}^n \mapsto \mathbb{R}^n$, which is $M-$Lipschitz with respect to the vector $L^2$ norm, i.e. $$||f(x)-f(y)||_2\leq M ||x-y||_2~\forall~x,y \in \mathbb{R}^n~.$$  Let $J_f: \mathbb{R}^n \mapsto \mathbb{R}^{n\times n}$ denote the Jacobian function of $f$, i.e. $$\left(J_f(x)\right)_{i,j} = \frac{\partial}{\partial x_j} f_i(x)~.$$ Assume that $J_f(x)$ is symmetric for every $x$. My question is, is the following inequality true (and why, if so)? $$||J_f(x)||_2 \leq M~\forall~x \in \mathbb{R}^n~,$$ where the last norm $||\cdot||_2$ refers to the spectral norm, i.e. the largest absolute singular value.","Well, the title pretty much says everything. I have a function $f: \mathbb{R}^n \mapsto \mathbb{R}^n$, which is $M-$Lipschitz with respect to the vector $L^2$ norm, i.e. $$||f(x)-f(y)||_2\leq M ||x-y||_2~\forall~x,y \in \mathbb{R}^n~.$$  Let $J_f: \mathbb{R}^n \mapsto \mathbb{R}^{n\times n}$ denote the Jacobian function of $f$, i.e. $$\left(J_f(x)\right)_{i,j} = \frac{\partial}{\partial x_j} f_i(x)~.$$ Assume that $J_f(x)$ is symmetric for every $x$. My question is, is the following inequality true (and why, if so)? $$||J_f(x)||_2 \leq M~\forall~x \in \mathbb{R}^n~,$$ where the last norm $||\cdot||_2$ refers to the spectral norm, i.e. the largest absolute singular value.",,"['matrices', 'multivariable-calculus', 'lipschitz-functions', 'jacobian', 'spectral-norm']"
63,Understanding Kantorovich's inequality,Understanding Kantorovich's inequality,,"I'm looking for a proof of the Kantorovich inequality, namely: $$    \langle  Ax,x\rangle \langle A^{-1}x,x\rangle \leq \frac{1}{4}\left(K(A)+\frac{1}{K(A)}\right)$$ Where $ K(A)= \lVert  A\rVert_2 \lVert  A^{-1}\rVert_2 $ and $A $ is an Hermitian positive definite  matrix and $x$  a vector with the accurate size. Or alternatively $$   \langle  Ax,x\rangle \langle A^{-1}x,x\rangle \leq \frac{1}{4}\left(\left(\frac{\beta}{\alpha}\right)^{2}+\left(\frac{\alpha}{\beta}\right)^{2}\right)$$ where $0 < \alpha = \lambda_1 \leq \cdots \leq \lambda_n = \beta$ are the eigenvalues of $ A$. There are a lot of proofs on internet but this is the one that I found easier to understand . Nevertheless, I'm stuck figuring out something: They use $ f,g: [\alpha,\beta]\to \mathbb{R} $ two convex function with $ f$ positive and $f(t)\leq g(t) $ for every $t \in [\alpha, \beta] $. Then they claim that $ F=f(A)$ and $ G=g(A)$ are well defined and hermitian positive definite. I don't even understand why they mean by $ f(A)$. If any of you guys can suggest me alternative documentation (other proof) or help me with this one, I would be very grateful","I'm looking for a proof of the Kantorovich inequality, namely: $$    \langle  Ax,x\rangle \langle A^{-1}x,x\rangle \leq \frac{1}{4}\left(K(A)+\frac{1}{K(A)}\right)$$ Where $ K(A)= \lVert  A\rVert_2 \lVert  A^{-1}\rVert_2 $ and $A $ is an Hermitian positive definite  matrix and $x$  a vector with the accurate size. Or alternatively $$   \langle  Ax,x\rangle \langle A^{-1}x,x\rangle \leq \frac{1}{4}\left(\left(\frac{\beta}{\alpha}\right)^{2}+\left(\frac{\alpha}{\beta}\right)^{2}\right)$$ where $0 < \alpha = \lambda_1 \leq \cdots \leq \lambda_n = \beta$ are the eigenvalues of $ A$. There are a lot of proofs on internet but this is the one that I found easier to understand . Nevertheless, I'm stuck figuring out something: They use $ f,g: [\alpha,\beta]\to \mathbb{R} $ two convex function with $ f$ positive and $f(t)\leq g(t) $ for every $t \in [\alpha, \beta] $. Then they claim that $ F=f(A)$ and $ G=g(A)$ are well defined and hermitian positive definite. I don't even understand why they mean by $ f(A)$. If any of you guys can suggest me alternative documentation (other proof) or help me with this one, I would be very grateful",,"['matrices', 'inequality', 'numerical-methods', 'proof-explanation']"
64,Is there a way of determining if two matrices have any relations?,Is there a way of determining if two matrices have any relations?,,"If I have two matrices A and B, is there a way to determine if ANY relations exist? e.g. $AAB^{-1}=B^{-1}ABBA^{-1}, BABAB^{-1}B^{-1}A^{-1}=ABAAB$, etc Basically that every permutation of A, B and their inverses yields a unique matrix (apart from cases where a matrix is adjacent to it's inverse, which of course simplify) Thanks!","If I have two matrices A and B, is there a way to determine if ANY relations exist? e.g. $AAB^{-1}=B^{-1}ABBA^{-1}, BABAB^{-1}B^{-1}A^{-1}=ABAAB$, etc Basically that every permutation of A, B and their inverses yields a unique matrix (apart from cases where a matrix is adjacent to it's inverse, which of course simplify) Thanks!",,"['matrices', 'relations']"
65,Calculating the probability of reaching each absorbing state in Markov Chain,Calculating the probability of reaching each absorbing state in Markov Chain,,"I'm starting with a Matrix that looks like this: [[ 0 , 1/2, 0,  0 ,  0 , 1/2],   [4/9,  0 , 0, 3/9, 2/9,  0 ],   [ 0 ,  0 , 1,  0 ,  0 ,  0 ],   [ 0 ,  0 , 0,  1 ,  0 ,  0 ],   [ 0 ,  0 , 0,  0 ,  1 ,  0 ],   [ 0 ,  0 , 0,  0 ,  0 ,  1 ]] And I'm being asked to calculate the probability of reaching each of the absorbing states C, D, E, and F starting from A. Obviously C is 0, and the other probabilities are given as: A to D -> 3/14 A to E -> 1/7 A to F -> 9/14 but I don't know what steps or formulas I need to arrive at those values. I don't really know where to start with this so any help would be greatly appreciated!","I'm starting with a Matrix that looks like this: [[ 0 , 1/2, 0,  0 ,  0 , 1/2],   [4/9,  0 , 0, 3/9, 2/9,  0 ],   [ 0 ,  0 , 1,  0 ,  0 ,  0 ],   [ 0 ,  0 , 0,  1 ,  0 ,  0 ],   [ 0 ,  0 , 0,  0 ,  1 ,  0 ],   [ 0 ,  0 , 0,  0 ,  0 ,  1 ]] And I'm being asked to calculate the probability of reaching each of the absorbing states C, D, E, and F starting from A. Obviously C is 0, and the other probabilities are given as: A to D -> 3/14 A to E -> 1/7 A to F -> 9/14 but I don't know what steps or formulas I need to arrive at those values. I don't really know where to start with this so any help would be greatly appreciated!",,"['matrices', 'markov-chains']"
66,Group of symmetric invertible matrices,Group of symmetric invertible matrices,,"All $2 \times 2$ symmetric invertible matrices form an infinite abelian group under matrix multiplication. Is the above statement true? I know it has identity, associative property and inverses exist. Product of invertible matrices is invertible and product of symmetric matrices is symmetric only if the matrices commute. Hence the answer should be no. They don't even form a Group. Is my argument correct?","All $2 \times 2$ symmetric invertible matrices form an infinite abelian group under matrix multiplication. Is the above statement true? I know it has identity, associative property and inverses exist. Product of invertible matrices is invertible and product of symmetric matrices is symmetric only if the matrices commute. Hence the answer should be no. They don't even form a Group. Is my argument correct?",,"['matrices', 'group-theory']"
67,Find permutation matrix that minimizes a Frobenius norm,Find permutation matrix that minimizes a Frobenius norm,,"Is there an efficient way to solve (maybe approximately) the following optimization problem $$\left\| A - X B X^T \right \|_F \xrightarrow[]{X}\min$$ where $X$ is a permutation matrix and $A$ , $B$ are dense square symmetric real matrices?","Is there an efficient way to solve (maybe approximately) the following optimization problem where is a permutation matrix and , are dense square symmetric real matrices?",\left\| A - X B X^T \right \|_F \xrightarrow[]{X}\min X A B,"['matrices', 'optimization', 'permutations', 'discrete-optimization', 'permutation-matrices']"
68,Powering a sum of two easily powered matrices,Powering a sum of two easily powered matrices,,"I am currently studying matrices and in order to understand them better I want to know why I can't do certain things in my calculations. This question is just about that. The task is to calculate $A^n$ if $$         A=\begin{bmatrix}         a & b \\         0 & c \\         \end{bmatrix} $$ I started of by calculating smaller powers, $A^2$, $A^3$, but I did not recognize the pattern at first. I tried an alternative approach, writing the matrix in a form of a sum of two matrices that will be easier do power. $         A=\begin{bmatrix}         a & 0 \\         0 & c \\         \end{bmatrix} $ $ +\begin{bmatrix}                             0 & b \\                             0 & 0 \\                             \end{bmatrix}  $ Let's denote these matrices as $C=\begin{bmatrix}         a & 0 \\         0 & c \\         \end{bmatrix} $  and $D=\begin{bmatrix}                             0 & b \\                             0 & 0 \\                             \end{bmatrix} $ When we apply Binomial Theorem, we get: $$A^n = (C+D)^n=\binom{n}{0}C^n + \binom{n}{1}C^{n-1}D + \binom{n}{2}C^{n-2}D^2 \dots + \binom{n}{n-1}CD^{n-1} + \binom{n}{n}D^n  $$ I tested powering both $C$ and $D$ for smaller powers to see if there is a pattern. As it turns out: $C^n = \begin{bmatrix}         a^n & 0 \\         0 & c^n \\         \end{bmatrix}$ and $  D^n = \begin{bmatrix}                             0 & 0 \\                             0 & 0 \\                             \end{bmatrix}$ Every matrix multiplied by zero-matrix $O$ is equal to zero, which leaves us with: $$A^n = C^n  $$ which is not the correct solution to the problem. What interests me is: which step did I do wrong and why ? I am aware that it would have been easier to recognize the pattern before turning to Binomial Theorem, but I want to know why is this particular method of solving wrong.","I am currently studying matrices and in order to understand them better I want to know why I can't do certain things in my calculations. This question is just about that. The task is to calculate $A^n$ if $$         A=\begin{bmatrix}         a & b \\         0 & c \\         \end{bmatrix} $$ I started of by calculating smaller powers, $A^2$, $A^3$, but I did not recognize the pattern at first. I tried an alternative approach, writing the matrix in a form of a sum of two matrices that will be easier do power. $         A=\begin{bmatrix}         a & 0 \\         0 & c \\         \end{bmatrix} $ $ +\begin{bmatrix}                             0 & b \\                             0 & 0 \\                             \end{bmatrix}  $ Let's denote these matrices as $C=\begin{bmatrix}         a & 0 \\         0 & c \\         \end{bmatrix} $  and $D=\begin{bmatrix}                             0 & b \\                             0 & 0 \\                             \end{bmatrix} $ When we apply Binomial Theorem, we get: $$A^n = (C+D)^n=\binom{n}{0}C^n + \binom{n}{1}C^{n-1}D + \binom{n}{2}C^{n-2}D^2 \dots + \binom{n}{n-1}CD^{n-1} + \binom{n}{n}D^n  $$ I tested powering both $C$ and $D$ for smaller powers to see if there is a pattern. As it turns out: $C^n = \begin{bmatrix}         a^n & 0 \\         0 & c^n \\         \end{bmatrix}$ and $  D^n = \begin{bmatrix}                             0 & 0 \\                             0 & 0 \\                             \end{bmatrix}$ Every matrix multiplied by zero-matrix $O$ is equal to zero, which leaves us with: $$A^n = C^n  $$ which is not the correct solution to the problem. What interests me is: which step did I do wrong and why ? I am aware that it would have been easier to recognize the pattern before turning to Binomial Theorem, but I want to know why is this particular method of solving wrong.",,"['matrices', 'binomial-theorem']"
69,Inequality between 2 norm and 1 norm of a matrix,Inequality between 2 norm and 1 norm of a matrix,,"When reading Golub's ""Matrix Computations"", I came across a series of norm inequalities. While I could prove a lot of them, this one has me stuck: $$ \frac{1}{\sqrt{m}} ||A||_1 \le ||A||_2 \le \sqrt{n}||A||_1$$ where $A \in \mathbb{R}^{m\times n},$ $||A||_1 = \displaystyle \max_{j \in [1,n]} \sum_{i=1}^m |a_{ij}|$ and $||A||_2 = \max_{||x||=1}||Ax||_2$, where $||Ax||_2$ is the standard Euclidean norm of the vector $Ax$. I know that $||A||_2 = \max \sigma(A)$ (maximum singular value of $A$), in case that's useful. Help?","When reading Golub's ""Matrix Computations"", I came across a series of norm inequalities. While I could prove a lot of them, this one has me stuck: $$ \frac{1}{\sqrt{m}} ||A||_1 \le ||A||_2 \le \sqrt{n}||A||_1$$ where $A \in \mathbb{R}^{m\times n},$ $||A||_1 = \displaystyle \max_{j \in [1,n]} \sum_{i=1}^m |a_{ij}|$ and $||A||_2 = \max_{||x||=1}||Ax||_2$, where $||Ax||_2$ is the standard Euclidean norm of the vector $Ax$. I know that $||A||_2 = \max \sigma(A)$ (maximum singular value of $A$), in case that's useful. Help?",,"['matrices', 'normed-spaces']"
70,Change from one cartesian co-ordinate system to another by translation and rotation.,Change from one cartesian co-ordinate system to another by translation and rotation.,,"There are two reasons for me to ask this question: I want to know if my understanding on this issue is correct. To clarify a doubt I have. I want to change the co-ordinate system of a set of points (Old cartesian coordinates system to New cartesian co-ordinate system). This transformation will involve Translation as well as Rotation. This is what I plan to do: (Kindly refer to the image attached here ) With respect to this image I have a set of points which are in the XYZ coordinate system (Red). I want to change it with respect to the axes UVW (Purple). In order to do so, I have understood that there are two steps involved: Translation and Rotation. When I translate, I only change the origin. (say, I want the UVW origin at (5,6,7). Then, for all points in my data, the x co-ordinates will be subtracted by 5, y by 6 and z by 7. By doing so. I get a set of Translated data.) Now I have to apply a rotation transform (on the Translated data). The Rotation matrix is shown in the image. The values Ux, Uy and Uz are the co-ordinates of a point on the U axis which has unit distance from origin. Similarly, the values Vx, Vy and Vz are the coordinates of a point on the V axis which has a unit distance from origin. (I want to know if I am right here.) Given this information, how can I calculate the value of Wx, Wy and Wx (a point lying on the W axis with unit distance from the origin)? I will know the co-ordinates of the origin {Obviously, (0,0,0), and I have the co-ordinates of points lying at unit distance from origin on the U and V axes. Also, U, V an W are orthonormal.} How can I calculate the 3rd row of rotation matrix R? Is my approach correct? My ultimate goal is to transform the co-ordinate system. I can calculate points on the U and V axes, but not on the W axis. Keeping this in mind, please provide me with inputs. (As far as I know, with the Point U (Ux, Uy, Uz), Point V (Vx, Vy and Vz) and the origin, I can define a plane. With this, I can find a point which is at unit distance from (0, 0, 0) in a direction along the plane's normal. How should I do this?) (Also, if it serves any purpose, I would like to let you know that I am using MATLAB.)","There are two reasons for me to ask this question: I want to know if my understanding on this issue is correct. To clarify a doubt I have. I want to change the co-ordinate system of a set of points (Old cartesian coordinates system to New cartesian co-ordinate system). This transformation will involve Translation as well as Rotation. This is what I plan to do: (Kindly refer to the image attached here ) With respect to this image I have a set of points which are in the XYZ coordinate system (Red). I want to change it with respect to the axes UVW (Purple). In order to do so, I have understood that there are two steps involved: Translation and Rotation. When I translate, I only change the origin. (say, I want the UVW origin at (5,6,7). Then, for all points in my data, the x co-ordinates will be subtracted by 5, y by 6 and z by 7. By doing so. I get a set of Translated data.) Now I have to apply a rotation transform (on the Translated data). The Rotation matrix is shown in the image. The values Ux, Uy and Uz are the co-ordinates of a point on the U axis which has unit distance from origin. Similarly, the values Vx, Vy and Vz are the coordinates of a point on the V axis which has a unit distance from origin. (I want to know if I am right here.) Given this information, how can I calculate the value of Wx, Wy and Wx (a point lying on the W axis with unit distance from the origin)? I will know the co-ordinates of the origin {Obviously, (0,0,0), and I have the co-ordinates of points lying at unit distance from origin on the U and V axes. Also, U, V an W are orthonormal.} How can I calculate the 3rd row of rotation matrix R? Is my approach correct? My ultimate goal is to transform the co-ordinate system. I can calculate points on the U and V axes, but not on the W axis. Keeping this in mind, please provide me with inputs. (As far as I know, with the Point U (Ux, Uy, Uz), Point V (Vx, Vy and Vz) and the origin, I can define a plane. With this, I can find a point which is at unit distance from (0, 0, 0) in a direction along the plane's normal. How should I do this?) (Also, if it serves any purpose, I would like to let you know that I am using MATLAB.)",,"['matrices', 'geometry', 'transformation', '3d']"
71,Visual representation of matrices,Visual representation of matrices,,"I am used to seeing most basic mathematical objects being visually represented (for instance, a curve in the plane divided by the xy axis; the same goes for complex numbers, vectors, and so on....), However, I never saw a visual representation of a matrix. I do not mean the disposition in rows and columns, of course.  I would like to know whether they can be graphically represented in some intuitive way. If so, how? Could you illustrate it, say, with a 2x2 quadratic matrix? Thanks a lot in advance","I am used to seeing most basic mathematical objects being visually represented (for instance, a curve in the plane divided by the xy axis; the same goes for complex numbers, vectors, and so on....), However, I never saw a visual representation of a matrix. I do not mean the disposition in rows and columns, of course.  I would like to know whether they can be graphically represented in some intuitive way. If so, how? Could you illustrate it, say, with a 2x2 quadratic matrix? Thanks a lot in advance",,"['matrices', 'visualization']"
72,Show that $A^TA$ has at least one positive eigenvalue if $A$ is not all-zero,Show that  has at least one positive eigenvalue if  is not all-zero,A^TA A,"I need some help on showing that $A^TA$ has at least one positive eigenvalue if $A$ is not all-zero. $A$ is rectangular and can have dependent columns in general. I can show that it cannot have negative eigenvalues. Here is what I have now. $$ A^TA\vec x=\lambda \vec x\\ \vec x^TA^TA\vec x=\vec x^T\lambda \vec x\\ ||A\vec x||^2=\lambda||\vec x||^2 $$ Because the two norms are strictly non-negative, $\lambda$ has to be non-negative. However, I cannot guarantee that $||A\vec x||\neq 0$ for at least one $\vec x$. If all eigenvectors of $A^TA$ lies in the null-space of $A$ (which I feel should not be the case), then $\lambda$ is indeed zero for all eigenvectors. So is my feeling wrong or how do I prove it?","I need some help on showing that $A^TA$ has at least one positive eigenvalue if $A$ is not all-zero. $A$ is rectangular and can have dependent columns in general. I can show that it cannot have negative eigenvalues. Here is what I have now. $$ A^TA\vec x=\lambda \vec x\\ \vec x^TA^TA\vec x=\vec x^T\lambda \vec x\\ ||A\vec x||^2=\lambda||\vec x||^2 $$ Because the two norms are strictly non-negative, $\lambda$ has to be non-negative. However, I cannot guarantee that $||A\vec x||\neq 0$ for at least one $\vec x$. If all eigenvectors of $A^TA$ lies in the null-space of $A$ (which I feel should not be the case), then $\lambda$ is indeed zero for all eigenvectors. So is my feeling wrong or how do I prove it?",,"['matrices', 'eigenvalues-eigenvectors', 'symmetry']"
73,Generating the special linear group of 2 by 2 matrices over the integers.,Generating the special linear group of 2 by 2 matrices over the integers.,,"Our Number Theory professor claimed that the special linear group $\text{SL}_2(\mathbb{Z})$ is generated by just two matrices: $$ M_1=\begin{pmatrix} 0& -1\\ 1& 0 \\\end{pmatrix}  $$ $$ M_2=\begin{pmatrix} 1& 1\\ 1& 0 \\\end{pmatrix} $$ He commented that the proof is outside the scope of the syllabus. When I pressed him to give a hint, he made the following cryptic comment: Consider the action of the group $\text{SL}_2(\mathbb{Z})$ on the complex upper half plane $\mathbb{H}$ by the action:   $$\begin{pmatrix} a& b\\ c& d \\\end{pmatrix}: z \to \frac{az+b}{cz+d}$$   Show that 'some region' in $\mathbb{H}$ is a 'fundamental domain' and that finishes the proof. I don't follow the argument at all. He drew a region that looked like a semicircle with a rectangular patch in between. This is the 'some region' in the block quote. I don't know how the complex plane helps. I don't know what a 'fundamental domain' means in this problem. And most of all, I don't see how this approaches the proof at all :( Would someone explain this argument to me? I will be happy with references, papers or a general hand waving argument as well. Thank you.","Our Number Theory professor claimed that the special linear group $\text{SL}_2(\mathbb{Z})$ is generated by just two matrices: $$ M_1=\begin{pmatrix} 0& -1\\ 1& 0 \\\end{pmatrix}  $$ $$ M_2=\begin{pmatrix} 1& 1\\ 1& 0 \\\end{pmatrix} $$ He commented that the proof is outside the scope of the syllabus. When I pressed him to give a hint, he made the following cryptic comment: Consider the action of the group $\text{SL}_2(\mathbb{Z})$ on the complex upper half plane $\mathbb{H}$ by the action:   $$\begin{pmatrix} a& b\\ c& d \\\end{pmatrix}: z \to \frac{az+b}{cz+d}$$   Show that 'some region' in $\mathbb{H}$ is a 'fundamental domain' and that finishes the proof. I don't follow the argument at all. He drew a region that looked like a semicircle with a rectangular patch in between. This is the 'some region' in the block quote. I don't know how the complex plane helps. I don't know what a 'fundamental domain' means in this problem. And most of all, I don't see how this approaches the proof at all :( Would someone explain this argument to me? I will be happy with references, papers or a general hand waving argument as well. Thank you.",,"['matrices', 'number-theory', 'complex-numbers']"
74,Block diagonalizing two matrices simultaneously,Block diagonalizing two matrices simultaneously,,"There are two matrices $A$ and $B$ which can not be diagonalized simultaneously. Is it possible to block diagonalize them? What if the matrices have an special pattern?  Physics of the problem is that, I have $2$ layers of atoms where $A$ is connectivity within the layer $1$ itself and $B$ is connectivity between layer $1$ and $2$. By block diagoanl matrix I mean a matrix which its diagonals are some matrices with size $M \times M$ (where M is much smaller than the actual size of matrix but larger than 1) For example a $3 \times 3$ block diagonal matrix [ \begin{array}{ccc} [a] & [0] & [0] \\ [0] & [b] & [0]\\ [0] & [0] & [c] \end{array}  ] I have attached the figures of sparsity pattern of an example $A$ and $B$.","There are two matrices $A$ and $B$ which can not be diagonalized simultaneously. Is it possible to block diagonalize them? What if the matrices have an special pattern?  Physics of the problem is that, I have $2$ layers of atoms where $A$ is connectivity within the layer $1$ itself and $B$ is connectivity between layer $1$ and $2$. By block diagoanl matrix I mean a matrix which its diagonals are some matrices with size $M \times M$ (where M is much smaller than the actual size of matrix but larger than 1) For example a $3 \times 3$ block diagonal matrix [ \begin{array}{ccc} [a] & [0] & [0] \\ [0] & [b] & [0]\\ [0] & [0] & [c] \end{array}  ] I have attached the figures of sparsity pattern of an example $A$ and $B$.",,"['matrices', 'eigenvalues-eigenvectors', 'diagonalization', 'block-matrices', 'eigenfunctions']"
75,Frobenius norm bound,Frobenius norm bound,,"Is there any way to bound the Frobenius norm of a product of square matrices $A,B$ and a vector $x$ in the following way: $$ \|ABx\|≤ \|Ax\|\text{ and }\|B\|. $$",Is there any way to bound the Frobenius norm of a product of square matrices and a vector in the following way:,"A,B x 
\|ABx\|≤ \|Ax\|\text{ and }\|B\|.
",['matrices']
76,The distance between inverse matrix,The distance between inverse matrix,,"I'd like to know if there are any relations between this $$||A^{-1}-B^{-1}||$$ and this $$||A-B||?$$ My supervisor is quite confident there must be one under certain conditions, but so far I've found nothing. I only managed to rewrite the expression in the following form $$tr(X^{-1}Y),$$ where Y is a positive semidefinite matrix. There are some inequalities for that case, but they ask quite a lot, can not afford :-) p.s.: for clarification $A$ actually converges to $B$, that is $A=(A_n)_{n\ge1}:\lim A_n = B$. $||A||$ is a generic matrix norm, Frobenius norm e.g.","I'd like to know if there are any relations between this $$||A^{-1}-B^{-1}||$$ and this $$||A-B||?$$ My supervisor is quite confident there must be one under certain conditions, but so far I've found nothing. I only managed to rewrite the expression in the following form $$tr(X^{-1}Y),$$ where Y is a positive semidefinite matrix. There are some inequalities for that case, but they ask quite a lot, can not afford :-) p.s.: for clarification $A$ actually converges to $B$, that is $A=(A_n)_{n\ge1}:\lim A_n = B$. $||A||$ is a generic matrix norm, Frobenius norm e.g.",,"['matrices', 'matrix-calculus']"
77,condition number after scaling matrix,condition number after scaling matrix,,"Maybe a well-known question. Let $\Sigma$ represent a real symmetric positive definite matrix, i.e. a covariance matrix. Which diagonal matrix $D$ with positive diagonal minimizes the condition number $\frac{\lambda_{\max}(D\Sigma D)}{\lambda_{\min}(D\Sigma D)}$? In contrast to some other literature on preconditioners, the pre- and post multiply matrix $D$ are the same here. There could be a clue here if the pre- en post-D would be allowed to differ. Unfortunately, I have no access to eq. (A.74). Numerical experiments suggest that the identity matrix is always optimal if $\Sigma$ is two dimensional. For larger dimensions, the optimum is nontrivial.","Maybe a well-known question. Let $\Sigma$ represent a real symmetric positive definite matrix, i.e. a covariance matrix. Which diagonal matrix $D$ with positive diagonal minimizes the condition number $\frac{\lambda_{\max}(D\Sigma D)}{\lambda_{\min}(D\Sigma D)}$? In contrast to some other literature on preconditioners, the pre- and post multiply matrix $D$ are the same here. There could be a clue here if the pre- en post-D would be allowed to differ. Unfortunately, I have no access to eq. (A.74). Numerical experiments suggest that the identity matrix is always optimal if $\Sigma$ is two dimensional. For larger dimensions, the optimum is nontrivial.",,"['matrices', 'numerical-methods', 'numerical-linear-algebra']"
78,PCA and image compression,PCA and image compression,,"I have two questions related to principal component analysis (PCA): How do you prove that the principal components matrix forms an orthonormal basis? Are the eigenvalues always orthogonal? On the meaning of PCA. For my assignment, I have to compute the first 5 principal components for twenty-four $60 \times 50$ images. I've done that. But then it asks me to show those $5$ principal components as images and comment on what I see. I can see stuff, but I don't understand what is important about what I am seeing. Assuming I am doing everything right, what I should I be seeing/commenting on?","I have two questions related to principal component analysis (PCA): How do you prove that the principal components matrix forms an orthonormal basis? Are the eigenvalues always orthogonal? On the meaning of PCA. For my assignment, I have to compute the first 5 principal components for twenty-four images. I've done that. But then it asks me to show those principal components as images and comment on what I see. I can see stuff, but I don't understand what is important about what I am seeing. Assuming I am doing everything right, what I should I be seeing/commenting on?",60 \times 50 5,"['matrices', 'eigenvalues-eigenvectors', 'svd', 'image-processing', 'principal-component-analysis']"
79,square matrix is not invertible if at least one row or column is zero,square matrix is not invertible if at least one row or column is zero,,"How to show that a square matrix is not invertible if at least one row or column is zero ? I can show if a row is zero, the result C of $AB=C$ can not be the identity matrix because there is a zero row. But for the column case ? Assume I don't know something about determinants.","How to show that a square matrix is not invertible if at least one row or column is zero ? I can show if a row is zero, the result C of $AB=C$ can not be the identity matrix because there is a zero row. But for the column case ? Assume I don't know something about determinants.",,['matrices']
80,Derivatives of a the Matrix diagonal function,Derivatives of a the Matrix diagonal function,,"If A is a not diagonal but symetric matrix and diag() is a function such that returns the diagonal, i.e. diag(A) is a matrix of zeros except on the diagonal. Im interested in the derivative of vec(diag(A)) with respect to x where A is a function of the vector x. So, how do I do This in the best way? Thanks in advance!","If A is a not diagonal but symetric matrix and diag() is a function such that returns the diagonal, i.e. diag(A) is a matrix of zeros except on the diagonal. Im interested in the derivative of vec(diag(A)) with respect to x where A is a function of the vector x. So, how do I do This in the best way? Thanks in advance!",,['matrices']
81,2x2 Matrices and Differences of Fractions,2x2 Matrices and Differences of Fractions,,"Consider the difference of two arbitrary fractions, $\frac{a}{b}$ and $\frac{c}{d}$. $$\frac{a}{b}-\frac{c}{d}=\frac{ad-bc}{bd}$$ The numerator is the determinant of the 2x2 matrix $$ \left( \begin{array}{ccc} a & c \\ b & d \\ \end{array} \right)$$ Is there any reason for this? Are the two related in any way?","Consider the difference of two arbitrary fractions, $\frac{a}{b}$ and $\frac{c}{d}$. $$\frac{a}{b}-\frac{c}{d}=\frac{ad-bc}{bd}$$ The numerator is the determinant of the 2x2 matrix $$ \left( \begin{array}{ccc} a & c \\ b & d \\ \end{array} \right)$$ Is there any reason for this? Are the two related in any way?",,"['matrices', 'soft-question', 'fractions']"
82,Lebesgue measure on normal matrices,Lebesgue measure on normal matrices,,"Consider the space of $n\times n$ complex matrices, and equip it with its Lebesgue measure $dX$, seen as a $2n^2$-dimensional real vector space [edit: or better, a complex vector space (see the answer of Leonid Kovalev below)]. I was wondering if their is an explicit formula for the restriction of the form $dX$ to the submanifold of normal matrices, that is the matrices which satisfy $X^*X=XX^*$, where $*$ stands for the transconjugation. Thanks in advance. EDIT : After the comment of GEdgar  : Is there some way to put an associated Riemaniann metric to the Lebesgue measure then ? And to compute it explicitly ?","Consider the space of $n\times n$ complex matrices, and equip it with its Lebesgue measure $dX$, seen as a $2n^2$-dimensional real vector space [edit: or better, a complex vector space (see the answer of Leonid Kovalev below)]. I was wondering if their is an explicit formula for the restriction of the form $dX$ to the submanifold of normal matrices, that is the matrices which satisfy $X^*X=XX^*$, where $*$ stands for the transconjugation. Thanks in advance. EDIT : After the comment of GEdgar  : Is there some way to put an associated Riemaniann metric to the Lebesgue measure then ? And to compute it explicitly ?",,"['matrices', 'differential-geometry', 'geometric-measure-theory']"
83,Knowledge about Graph Spectral Theory and correlation between a graph Weighted Adjacency matrix and its eigenvalues,Knowledge about Graph Spectral Theory and correlation between a graph Weighted Adjacency matrix and its eigenvalues,,"I know that this question is some sort of bridge between Informatics and Mathematics, not knowing the best place where to post this question, I opted for this place because of the type of answer I want (which concerns Math more than anything). Consider to have a graph represented by a collection of nodes and connections. The best way to describe the graph is through the adjacency matrix (AM): a matrix that has 0 or 1 if one node is connected to another (we consider non-directed graphs, so we have all bidirectional connections). Does anyone know whether the eigenvalues of the matrix implies something for the graph??? properties, topology.... I ask this question because I've almost finished studying Markov Chains. In a chain, the matrix of transition probabilities P (for discrete time markov chains), or the matrix of transition frequencies Q (for cont-time markov chains), can be evaluated (their eigenvalues) in order to inspect whether the chain is ergodic or not (with a parallelism to Control Theory: eigenvalues in the unitary circle or in the negative half-plane). I am trying to find something similar for graphs. Thank you","I know that this question is some sort of bridge between Informatics and Mathematics, not knowing the best place where to post this question, I opted for this place because of the type of answer I want (which concerns Math more than anything). Consider to have a graph represented by a collection of nodes and connections. The best way to describe the graph is through the adjacency matrix (AM): a matrix that has 0 or 1 if one node is connected to another (we consider non-directed graphs, so we have all bidirectional connections). Does anyone know whether the eigenvalues of the matrix implies something for the graph??? properties, topology.... I ask this question because I've almost finished studying Markov Chains. In a chain, the matrix of transition probabilities P (for discrete time markov chains), or the matrix of transition frequencies Q (for cont-time markov chains), can be evaluated (their eigenvalues) in order to inspect whether the chain is ergodic or not (with a parallelism to Control Theory: eigenvalues in the unitary circle or in the negative half-plane). I am trying to find something similar for graphs. Thank you",,"['matrices', 'graph-theory', 'eigenvalues-eigenvectors']"
84,"Is $SL(2,\Bbb R)$ generated by $SO(2)$ and a single upper triangular element?",Is  generated by  and a single upper triangular element?,"SL(2,\Bbb R) SO(2)","Consider the subgroup $\Gamma < \operatorname{SL}(2,\mathbb{R})$ generated by the element $$ \begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix} $$ and all elements of $\operatorname{SO}(2)$ . Is $\Gamma = \operatorname{SL}(2,\mathbb{R})$ ? Some calculations show that $$ \begin{pmatrix} 1 & 0 \\ 1 & 1 \end{pmatrix} \in \Gamma $$ and hence $\operatorname{SL}(2,\mathbb{Z}) \subseteq \Gamma$ .",Consider the subgroup generated by the element and all elements of . Is ? Some calculations show that and hence .,"\Gamma < \operatorname{SL}(2,\mathbb{R}) 
\begin{pmatrix} 1 & 1 \\ 0 & 1 \end{pmatrix}
 \operatorname{SO}(2) \Gamma = \operatorname{SL}(2,\mathbb{R}) 
\begin{pmatrix} 1 & 0 \\ 1 & 1 \end{pmatrix} \in \Gamma
 \operatorname{SL}(2,\mathbb{Z}) \subseteq \Gamma","['matrices', 'group-theory', 'lie-groups', 'matrix-decomposition']"
85,Gradient of $x'\left(\sum_i x_i A_i\right)^{-1}x$,Gradient of,x'\left(\sum_i x_i A_i\right)^{-1}x,"I want to compute the gradient of $$f(x)=x'\left(\sum_i x_i A_i\right)^{-1}x$$ where $x$ is a $n\times 1$ column vector and $A_i$ is a square matrix. One option is to differentiate with respect to a single $x_j$ and then reconstitute the gradient but this gets very messy. The other option is to use matrix calculus rules. One can write $$ C:=\sum_i x_i A_i = (x'\otimes I_n)B$$ where $B$ is a block column vector: $B=\left[A_{1},\dots,A_{n}\right]'.$ Then using the rules for differentiating a matrix inverse $$ df=2C^{-1}xdx-x'C^{-1} \left(dC\right) C^{-1}x. $$ Finally, $$ dC=(dx'\otimes I_n)B. $$ But I am stuck there. How can I move from this to an expression for $\frac{df}{dx}$ ? EDIT: Corrected some mistakes that commenters noticed.","I want to compute the gradient of where is a column vector and is a square matrix. One option is to differentiate with respect to a single and then reconstitute the gradient but this gets very messy. The other option is to use matrix calculus rules. One can write where is a block column vector: Then using the rules for differentiating a matrix inverse Finally, But I am stuck there. How can I move from this to an expression for ? EDIT: Corrected some mistakes that commenters noticed.","f(x)=x'\left(\sum_i x_i A_i\right)^{-1}x x n\times 1 A_i x_j  C:=\sum_i x_i A_i = (x'\otimes I_n)B B B=\left[A_{1},\dots,A_{n}\right]'. 
df=2C^{-1}xdx-x'C^{-1} \left(dC\right) C^{-1}x.
 
dC=(dx'\otimes I_n)B.
 \frac{df}{dx}","['calculus', 'matrices', 'multivariable-calculus', 'matrix-calculus']"
86,Matrix derivative of $f(X^T Y)$ w.r.t X,Matrix derivative of  w.r.t X,f(X^T Y),"Given $X\in \mathbb{R}^{m\times n}$ , $Y\in \mathbb{R}^{m\times k}$ , $X^\top Y=Z\in \mathbb{R}^{n\times k}$ , and $f:\mathbb{R}^{n\times k} \to \mathbb{R}$ , we have the following: \begin{equation} f(X^\top Y)=f(Z) \end{equation} What is the derivative of $f$ with respect to $X$ , i.e., what is $\frac{\partial f}{\partial X}$ ? I tried to ""expect"" $\frac{\partial f}{\partial X}$ just by trying to match the dimension $m,n,k$ as follows: \begin{equation} \begin{aligned} \frac{\partial f}{\partial X} =\frac{\partial f}{\partial Z}\frac{\partial Z}{\partial X} =Y\left(\frac{\partial f}{\partial Z}\right)^\top \end{aligned} \end{equation} Since $\frac{\partial Z}{\partial X}$ should be a fourth-order tensor , I tried to calculate $\frac{\partial Z}{\partial X}$ by using $\frac {\partial AXB}{\partial X}=B^T\otimes A$ like this: \begin{equation} \begin{aligned} \frac{\partial Z}{\partial X}=\frac{\partial (X^\top Y)}{\partial X} {=\left(\frac{\partial ((X^\top) Y)}{\partial (X^\top)}\right)^\top} {=\left(Y^\top \otimes I \right)^\top} {=(Y^\top)^\top \otimes I^\top} {=Y \otimes I} \end{aligned} \end{equation} Where $\otimes$ is the Kronecker product and $I$ is the $n\times n$ identity matrix. From this, we have: \begin{equation} \begin{aligned} \frac{\partial f}{\partial X} =\frac{\partial f}{\partial Z}\frac{\partial Z}{\partial X} =\frac{\partial f}{\partial Z}\left(Y \otimes I\right) \end{aligned} \end{equation} From above equations, this should be true: \begin{equation} \begin{aligned} \frac{\partial f}{\partial X} =\frac{\partial f}{\partial Z}\left(Y \otimes I\right) =Y\left(\frac{\partial f}{\partial Z}\right)^\top \end{aligned} \end{equation} The last equation seemed counter intuitive. I can not figure it out. Do I calculate $\frac{\partial f}{\partial X}$ correctly ? If I did it wrong, how should I calculate $\frac{\partial f}{\partial X}$ instead?","Given , , , and , we have the following: What is the derivative of with respect to , i.e., what is ? I tried to ""expect"" just by trying to match the dimension as follows: Since should be a fourth-order tensor , I tried to calculate by using like this: Where is the Kronecker product and is the identity matrix. From this, we have: From above equations, this should be true: The last equation seemed counter intuitive. I can not figure it out. Do I calculate correctly ? If I did it wrong, how should I calculate instead?","X\in \mathbb{R}^{m\times n} Y\in \mathbb{R}^{m\times k} X^\top Y=Z\in \mathbb{R}^{n\times k} f:\mathbb{R}^{n\times k} \to \mathbb{R} \begin{equation}
f(X^\top Y)=f(Z)
\end{equation} f X \frac{\partial f}{\partial X} \frac{\partial f}{\partial X} m,n,k \begin{equation}
\begin{aligned}
\frac{\partial f}{\partial X}
=\frac{\partial f}{\partial Z}\frac{\partial Z}{\partial X}
=Y\left(\frac{\partial f}{\partial Z}\right)^\top
\end{aligned}
\end{equation} \frac{\partial Z}{\partial X} \frac{\partial Z}{\partial X} \frac {\partial AXB}{\partial X}=B^T\otimes A \begin{equation}
\begin{aligned}
\frac{\partial Z}{\partial X}=\frac{\partial (X^\top Y)}{\partial X}
{=\left(\frac{\partial ((X^\top) Y)}{\partial (X^\top)}\right)^\top}
{=\left(Y^\top \otimes I \right)^\top}
{=(Y^\top)^\top \otimes I^\top}
{=Y \otimes I}
\end{aligned}
\end{equation} \otimes I n\times n \begin{equation}
\begin{aligned}
\frac{\partial f}{\partial X}
=\frac{\partial f}{\partial Z}\frac{\partial Z}{\partial X}
=\frac{\partial f}{\partial Z}\left(Y \otimes I\right)
\end{aligned}
\end{equation} \begin{equation}
\begin{aligned}
\frac{\partial f}{\partial X}
=\frac{\partial f}{\partial Z}\left(Y \otimes I\right)
=Y\left(\frac{\partial f}{\partial Z}\right)^\top
\end{aligned}
\end{equation} \frac{\partial f}{\partial X} \frac{\partial f}{\partial X}","['matrices', 'derivatives', 'matrix-calculus']"
87,4D Left Isoclinic Rotations as double rotations,4D Left Isoclinic Rotations as double rotations,,"According to this page, 4D left-isoclinic rotation matrices are double rotations of the same angle (and same sign). But in the same page, in the context of quaternions, it is mentioned that left-isoclinic matrices have the form: $$R=\begin{bmatrix}a&-b&-c&-d \\ b&a&-d&c\\c&d&a&-b\\d&-c&b&a\end{bmatrix}$$ where $a,b,c,d\in\mathbb{R}$ such that $$a^2+b^2+c^2+d^2=1.$$ My question is, how can we view $R$ as a double rotation? I mean, how can we find the common angle? If two of $b,c,d$ are $0$ then it is easy to see. But what if this is not the case?","According to this page, 4D left-isoclinic rotation matrices are double rotations of the same angle (and same sign). But in the same page, in the context of quaternions, it is mentioned that left-isoclinic matrices have the form: where such that My question is, how can we view as a double rotation? I mean, how can we find the common angle? If two of are then it is easy to see. But what if this is not the case?","R=\begin{bmatrix}a&-b&-c&-d \\ b&a&-d&c\\c&d&a&-b\\d&-c&b&a\end{bmatrix} a,b,c,d\in\mathbb{R} a^2+b^2+c^2+d^2=1. R b,c,d 0","['matrices', 'rotations']"
88,Calculate Homography with and without SVD,Calculate Homography with and without SVD,,"I've rendered an example for this question.  With the trick of https://math.stackexchange.com/a/2619023/741822 I was able to calculate what seems to be the homography matrix (it passed 2 tests on $p_5$ and $p_6$ ). Unfortunately, I'm not able to get the result via SVD which is why I created that question 3D Data $p_0 = (0|0|0)$ $p_1 = (-7|0|0)$ $p_2 = (3|-6|0)$ $p_3 = (7|-4|0)$ $p_4 = (3|2|0)$ $p_5 = (6|1|0)$ $p_6 = (4|7|0)$ 2D Data $p_0 = (700|288)$ $p_1 = (93|63)$ $p_2 = (293|868)$ $p_3 = (1207|998)$ $p_4 = (1218|309)$ $p_5 = (1540|502)$ $p_6 = (1679|128)$ Calculation General Formula $PH = \begin{bmatrix} -x_1 \quad -y_1 \quad -1 \quad 0 \quad 0 \quad 0 \quad x_1x_1' \quad y_1x_1' \quad x_1' \\ 0 \quad 0 \quad 0 \quad -x_1 \quad -y_1 \quad -1 \quad x_1y_1' \quad y_1y_1' \quad y_1' \\ -x_2 \quad -y_2 \quad -1 \quad 0 \quad 0 \quad 0 \quad x_2x_2' \quad y_2x_2' \quad x_2' \\ 0 \quad 0 \quad 0 \quad -x_2 \quad -y_2 \quad -1 \quad x_2y_2' \quad y_2y_2' \quad y_2' \\ -x_3 \quad -y_3 \quad -1 \quad 0 \quad 0 \quad 0 \quad x_3x_3' \quad y_3x_3' \quad x_3' \\ 0 \quad 0 \quad 0 \quad -x_3 \quad -y_3 \quad -1 \quad x_3y_3' \quad y_3y_3' \quad y_3' \\ -x_4 \quad -y_4 \quad -1 \quad 0 \quad 0 \quad 0 \quad x_4x_4' \quad y_4x_4' \quad x_4' \\ 0 \quad 0 \quad 0 \quad -x_4 \quad -y_4 \quad -1 \quad x_4y_4' \quad y_4y_4' \quad y_4' \\ \end{bmatrix} \begin{bmatrix}h1 \\ h2 \\ h3 \\ h4 \\ h5 \\ h6 \\ h7 \\ h8 \\h9 \end{bmatrix} = 0$ Add new row to get a $9 \times9$ matrix $PH = \begin{bmatrix} -x_1 & -y_1 & -1 & 0 & 0 & 0 & x_1x_1' & y_1x_1' & x_1' \\ 0 & 0 & 0 & -x_1 & -y_1 & -1 & x_1y_1' & y_1y_1' & y_1' \\ -x_2 & -y_2 & -1 & 0 & 0 & 0 & x_2x_2' & y_2x_2' & x_2' \\ 0 & 0 & 0 & -x_2 & -y_2 & -1 & x_2y_2' & y_2y_2' & y_2' \\ -x_3 & -y_3 & -1 & 0 & 0 & 0 & x_3x_3' & y_3x_3' & x_3' \\ 0 & 0 & 0 & -x_3 & -y_3 & -1 & x_3y_3' & y_3y_3' & y_3' \\ -x_4 & -y_4 & -1 & 0 & 0 & 0 & x_4x_4' & y_4x_4' & x_4' \\ 0 & 0 & 0 & -x_4 & -y_4 & -1 & x_4y_4' & y_4y_4' & y_4' \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\ \end{bmatrix} \begin{bmatrix}h1 \\ h2 \\ h3 \\ h4 \\ h5 \\ h6 \\ h7 \\ h8 \\h9 \end{bmatrix} = \begin{bmatrix}0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\1 \end{bmatrix}$ Fill in values $P = \displaystyle \left[\begin{matrix}-93 & -63 & -1 & 0 & 0 & 0 & -651 & -441 & -7\\0 & 0 & 0 & -93 & -63 & -1 & 0 & 0 & 0\\-293 & -868 & -1 & 0 & 0 & 0 & 879 & 2604 & 3\\0 & 0 & 0 & -293 & -868 & -1 & -1758 & -5208 & -6\\-1207 & -998 & -1 & 0 & 0 & 0 & 8449 & 6986 & 7\\0 & 0 & 0 & -1207 & -998 & -1 & -4828 & -3992 & -4\\-1218 & -309 & -1 & 0 & 0 & 0 & 3654 & 927 & 3\\0 & 0 & 0 & -1218 & -309 & -1 & 2436 & 618 & 2\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\end{matrix}\right]$ Invert matrix $P$ and multiply with homogeneous vector to get matrix $H$ $H = P ^{-1} * \displaystyle \left[\begin{matrix}0\\0\\0\\0\\0\\0\\0\\0\\1\end{matrix}\right]$ Fill in and calculate result $H= \displaystyle \left[\begin{matrix}\frac{13724393133269}{1982067999646851}\\\frac{28367198390048}{1982067999646851}\\- \frac{5924445050578537}{660689333215617}\\\frac{11075363038922}{1982067999646851}\\- \frac{29759444826748}{1982067999646851}\\\frac{281612087155126}{660689333215617}\\\frac{2748231707}{1982067999646851}\\\frac{1890176860316}{1982067999646851}\\1\end{matrix}\right]$ Convert to $3 \times 3$ matrix $H= \displaystyle \left[\begin{matrix}\frac{13724393133269}{1982067999646851} & \frac{28367198390048}{1982067999646851} & - \frac{5924445050578537}{660689333215617}\\\frac{11075363038922}{1982067999646851} & - \frac{29759444826748}{1982067999646851} & \frac{281612087155126}{660689333215617}\\\frac{2748231707}{1982067999646851} & \frac{1890176860316}{1982067999646851} & 1\end{matrix}\right]$ In decimal numbers $H= \displaystyle \left[\begin{matrix}0.0069243 & 0.014312 & -8.9671\\0.0055878 & -0.015014 & 0.42624\\1.3865 \cdot 10^{-6} & 0.00095364 & 1.0\end{matrix}\right]$ Check General Formula to calculate point with the homography matrix $\left[\begin{array}{c}{x^{\prime} * \lambda} \\ {y^{\prime} * \lambda} \\ {\lambda}\end{array}\right]=\left[\begin{array}{lll}{h_{11}} & {h_{12}} & {h_{13}} \\ {h_{21}} & {h_{22}} & {h_{23}} \\ {h_{31}} & {h_{32}} & {h_{33}}\end{array}\right] \cdot\left[\begin{array}{l}{x} \\ {y} \\ {1}\end{array}\right]$ Check with point $p_5$ $(6|1|0)$ in 3d, and $(1540|502)$ in 2d $\displaystyle \left[\begin{matrix}0.00692 & 0.0143 & -8.97\\0.00559 & -0.015 & 0.426\\1.39 \cdot 10^{-6} & 0.000954 & 1.0\end{matrix}\right] * \displaystyle \left[\begin{matrix}1540\\502\\1\end{matrix}\right] = \displaystyle \left[\begin{matrix}8.8809\\1.4942\\1.4809\end{matrix}\right]$ $x^{\prime} = 8.8809 / 1.4809 \approx 6$ $y^{\prime} = 1.4942 / 1.4809 \approx 1$ Check with point $p_6$ $(4|7|0)$ in 3d, and $(1679|128)$ in 2d $\displaystyle \left[\begin{matrix}0.00692 & 0.0143 & -8.97\\0.00559 & -0.015 & 0.426\\1.39 \cdot 10^{-6} & 0.000954 & 1.0\end{matrix}\right] * \displaystyle \left[\begin{matrix}1679\\128\\1\end{matrix}\right] = \displaystyle \left[\begin{matrix}4.4907\\7.8863\\1.1244\end{matrix}\right]$ $x^{\prime} = 4.4907 / 1.1244 \approx 4$ $y^{\prime} = 7.8863 / 1.1244 \approx 7$ SVD Also described in the same linked question https://math.stackexchange.com/a/1289595/741822 , the last Vector of $V$ of the SVD result should be the homography matrix . Unfortunately, when I calculate it, I get something which doesn`t compare to the outcome before. Im using a python script and the svd_r function described here http://mpmath.org/doc/current/matrices.html to do the job from sympy import * from mpmath import mp from IPython.display import display  x_1 = [93,-7] y_1 = [63,0] x_2 = [293,3] y_2 = [868,-6] x_3 = [1207,7] y_3 = [998,-4] x_4 = [1218,3] y_4 = [309,2]  P = Matrix([  [x_1[0]*-1 ,y_1[0]*-1 ,-1 ,0 ,0 ,0 ,x_1[0]*x_1[1] ,y_1[0]*x_1[1] ,x_1[1]],  [0 ,0 ,0 ,x_1[0]*-1 , y_1[0]*-1 ,-1 ,x_1[0]*y_1[1] ,y_1[0]*y_1[1] ,y_1[1]],  [x_2[0]*-1 ,y_2[0]*-1 ,-1 ,0 ,0 ,0 ,x_2[0]*x_2[1] ,y_2[0]*x_2[1] ,x_2[1]],  [0 ,0 ,0 ,x_2[0]*-1 , y_2[0]*-1 ,-1 ,x_2[0]*y_2[1] ,y_2[0]*y_2[1] ,y_2[1]],  [x_3[0]*-1 ,y_3[0]*-1 ,-1 ,0 ,0 ,0 ,x_3[0]*x_3[1] ,y_3[0]*x_3[1] ,x_3[1]],  [0 ,0 ,0 ,x_3[0]*-1 , y_3[0]*-1 ,-1 ,x_3[0]*y_3[1] ,y_3[0]*y_3[1] ,y_3[1]],  [x_4[0]*-1 ,y_4[0]*-1 ,-1 ,0 ,0 ,0 ,x_4[0]*x_4[1] ,y_4[0]*x_4[1] ,x_4[1]],  [0 ,0 ,0 ,x_4[0]*-1 , y_4[0]*-1 ,-1 ,x_4[0]*y_4[1] ,y_4[0]*y_4[1] ,y_4[1]],  ])  U, S, V = mp.svd_r(P) display(U) display(S) display(V) which gives me $\displaystyle \left[\begin{matrix}0.0873968428008291 & 0.0682224411681068 & 7.78775669283991 \cdot 10^{-5} & -0.0307611970870902 & -0.047164344898438 & -4.31617782272004 \cdot 10^{-5} & -0.734415741368853 & -0.667210380053951 & -0.000763782447496494\\-0.1843309239413 & -0.0151678791434499 & -8.61313928411903 \cdot 10^{-5} & -0.177510439386387 & -0.196972727023475 & -0.000286352439937897 & 0.63453250533834 & -0.702034214967351 & -0.000453769669195275\\0.178502576804415 & 0.311156872128848 & 0.000325321190586577 & 0.77104600249753 & 0.446971881836657 & 0.000626740830458092 & 0.180594126282578 & -0.210730877512211 & -0.000435956847243289\\-0.669632242817859 & -0.535613720571974 & -0.000939878794635243 & 0.478237602951333 & -0.137967801791192 & 0.000272230101249859 & -0.129682866265852 & -0.0120304219769976 & -0.000973739592376422\\0.686925430229282 & -0.552023064176252 & 0.000248883448694341 & 0.254558652994286 & -0.38595799679894 & -0.000493796526964928 & 0.0865993870367576 & -0.046238819234477 & -0.00158707187572933\\-0.0790313367881196 & 0.55378718349052 & 0.0023403071836456 & 0.28199111075643 & -0.768889925477268 & -3.75757014301404 \cdot 10^{-5} & -0.0327206087187983 & 0.123639937181458 & 0.00038524047574376\\0.000450569782323263 & -0.0016191693558424 & 0.111042503882465 & 0.000924901054479592 & -0.000202764323620636 & 0.00454096268914545 & -0.00017046030825094 & -0.00107219827445236 & 0.993802818493512\\0.000319779779068856 & -0.000370212652047504 & 0.0463933646371045 & -0.000563323598922266 & -0.000441631139009571 & 0.998875291759086 & 0.000114547728791255 & -0.000112905332478902 & -0.0097483164699673\end{matrix}\right]$ No data in that matrix compares to the calculated homography matrix above. Creating a matrix with the last column and doing the check will end in $\approx (0|0)$ $\displaystyle \left[\begin{matrix}-0.000763782447496494 & -0.000453769669195275 & -0.000435956847243289\\-0.000973739592376422 & -0.00158707187572933 & 0.00038524047574376\\0.993802818493512 & -0.0097483164699673 & 1\end{matrix}\right] * \displaystyle \left[\begin{matrix}1679\\128\\1\end{matrix}\right] = \displaystyle \left[\begin{matrix}-1.3409\\-1.8377\\1668.3\end{matrix}\right]$ What am I doing wrong here? Am I missing a step when working with SVD or is it that the 1st result (via the trick) just works by pure chance?","I've rendered an example for this question.  With the trick of https://math.stackexchange.com/a/2619023/741822 I was able to calculate what seems to be the homography matrix (it passed 2 tests on and ). Unfortunately, I'm not able to get the result via SVD which is why I created that question 3D Data 2D Data Calculation General Formula Add new row to get a matrix Fill in values Invert matrix and multiply with homogeneous vector to get matrix Fill in and calculate result Convert to matrix In decimal numbers Check General Formula to calculate point with the homography matrix Check with point in 3d, and in 2d Check with point in 3d, and in 2d SVD Also described in the same linked question https://math.stackexchange.com/a/1289595/741822 , the last Vector of of the SVD result should be the homography matrix . Unfortunately, when I calculate it, I get something which doesn`t compare to the outcome before. Im using a python script and the svd_r function described here http://mpmath.org/doc/current/matrices.html to do the job from sympy import * from mpmath import mp from IPython.display import display  x_1 = [93,-7] y_1 = [63,0] x_2 = [293,3] y_2 = [868,-6] x_3 = [1207,7] y_3 = [998,-4] x_4 = [1218,3] y_4 = [309,2]  P = Matrix([  [x_1[0]*-1 ,y_1[0]*-1 ,-1 ,0 ,0 ,0 ,x_1[0]*x_1[1] ,y_1[0]*x_1[1] ,x_1[1]],  [0 ,0 ,0 ,x_1[0]*-1 , y_1[0]*-1 ,-1 ,x_1[0]*y_1[1] ,y_1[0]*y_1[1] ,y_1[1]],  [x_2[0]*-1 ,y_2[0]*-1 ,-1 ,0 ,0 ,0 ,x_2[0]*x_2[1] ,y_2[0]*x_2[1] ,x_2[1]],  [0 ,0 ,0 ,x_2[0]*-1 , y_2[0]*-1 ,-1 ,x_2[0]*y_2[1] ,y_2[0]*y_2[1] ,y_2[1]],  [x_3[0]*-1 ,y_3[0]*-1 ,-1 ,0 ,0 ,0 ,x_3[0]*x_3[1] ,y_3[0]*x_3[1] ,x_3[1]],  [0 ,0 ,0 ,x_3[0]*-1 , y_3[0]*-1 ,-1 ,x_3[0]*y_3[1] ,y_3[0]*y_3[1] ,y_3[1]],  [x_4[0]*-1 ,y_4[0]*-1 ,-1 ,0 ,0 ,0 ,x_4[0]*x_4[1] ,y_4[0]*x_4[1] ,x_4[1]],  [0 ,0 ,0 ,x_4[0]*-1 , y_4[0]*-1 ,-1 ,x_4[0]*y_4[1] ,y_4[0]*y_4[1] ,y_4[1]],  ])  U, S, V = mp.svd_r(P) display(U) display(S) display(V) which gives me No data in that matrix compares to the calculated homography matrix above. Creating a matrix with the last column and doing the check will end in What am I doing wrong here? Am I missing a step when working with SVD or is it that the 1st result (via the trick) just works by pure chance?",p_5 p_6 p_0 = (0|0|0) p_1 = (-7|0|0) p_2 = (3|-6|0) p_3 = (7|-4|0) p_4 = (3|2|0) p_5 = (6|1|0) p_6 = (4|7|0) p_0 = (700|288) p_1 = (93|63) p_2 = (293|868) p_3 = (1207|998) p_4 = (1218|309) p_5 = (1540|502) p_6 = (1679|128) PH = \begin{bmatrix} -x_1 \quad -y_1 \quad -1 \quad 0 \quad 0 \quad 0 \quad x_1x_1' \quad y_1x_1' \quad x_1' \\ 0 \quad 0 \quad 0 \quad -x_1 \quad -y_1 \quad -1 \quad x_1y_1' \quad y_1y_1' \quad y_1' \\ -x_2 \quad -y_2 \quad -1 \quad 0 \quad 0 \quad 0 \quad x_2x_2' \quad y_2x_2' \quad x_2' \\ 0 \quad 0 \quad 0 \quad -x_2 \quad -y_2 \quad -1 \quad x_2y_2' \quad y_2y_2' \quad y_2' \\ -x_3 \quad -y_3 \quad -1 \quad 0 \quad 0 \quad 0 \quad x_3x_3' \quad y_3x_3' \quad x_3' \\ 0 \quad 0 \quad 0 \quad -x_3 \quad -y_3 \quad -1 \quad x_3y_3' \quad y_3y_3' \quad y_3' \\ -x_4 \quad -y_4 \quad -1 \quad 0 \quad 0 \quad 0 \quad x_4x_4' \quad y_4x_4' \quad x_4' \\ 0 \quad 0 \quad 0 \quad -x_4 \quad -y_4 \quad -1 \quad x_4y_4' \quad y_4y_4' \quad y_4' \\ \end{bmatrix} \begin{bmatrix}h1 \\ h2 \\ h3 \\ h4 \\ h5 \\ h6 \\ h7 \\ h8 \\h9 \end{bmatrix} = 0 9 \times9 PH = \begin{bmatrix} -x_1 & -y_1 & -1 & 0 & 0 & 0 & x_1x_1' & y_1x_1' & x_1' \\ 0 & 0 & 0 & -x_1 & -y_1 & -1 & x_1y_1' & y_1y_1' & y_1' \\ -x_2 & -y_2 & -1 & 0 & 0 & 0 & x_2x_2' & y_2x_2' & x_2' \\ 0 & 0 & 0 & -x_2 & -y_2 & -1 & x_2y_2' & y_2y_2' & y_2' \\ -x_3 & -y_3 & -1 & 0 & 0 & 0 & x_3x_3' & y_3x_3' & x_3' \\ 0 & 0 & 0 & -x_3 & -y_3 & -1 & x_3y_3' & y_3y_3' & y_3' \\ -x_4 & -y_4 & -1 & 0 & 0 & 0 & x_4x_4' & y_4x_4' & x_4' \\ 0 & 0 & 0 & -x_4 & -y_4 & -1 & x_4y_4' & y_4y_4' & y_4' \\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\ \end{bmatrix} \begin{bmatrix}h1 \\ h2 \\ h3 \\ h4 \\ h5 \\ h6 \\ h7 \\ h8 \\h9 \end{bmatrix} = \begin{bmatrix}0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\1 \end{bmatrix} P = \displaystyle \left[\begin{matrix}-93 & -63 & -1 & 0 & 0 & 0 & -651 & -441 & -7\\0 & 0 & 0 & -93 & -63 & -1 & 0 & 0 & 0\\-293 & -868 & -1 & 0 & 0 & 0 & 879 & 2604 & 3\\0 & 0 & 0 & -293 & -868 & -1 & -1758 & -5208 & -6\\-1207 & -998 & -1 & 0 & 0 & 0 & 8449 & 6986 & 7\\0 & 0 & 0 & -1207 & -998 & -1 & -4828 & -3992 & -4\\-1218 & -309 & -1 & 0 & 0 & 0 & 3654 & 927 & 3\\0 & 0 & 0 & -1218 & -309 & -1 & 2436 & 618 & 2\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\end{matrix}\right] P H H = P ^{-1} * \displaystyle \left[\begin{matrix}0\\0\\0\\0\\0\\0\\0\\0\\1\end{matrix}\right] H= \displaystyle \left[\begin{matrix}\frac{13724393133269}{1982067999646851}\\\frac{28367198390048}{1982067999646851}\\- \frac{5924445050578537}{660689333215617}\\\frac{11075363038922}{1982067999646851}\\- \frac{29759444826748}{1982067999646851}\\\frac{281612087155126}{660689333215617}\\\frac{2748231707}{1982067999646851}\\\frac{1890176860316}{1982067999646851}\\1\end{matrix}\right] 3 \times 3 H= \displaystyle \left[\begin{matrix}\frac{13724393133269}{1982067999646851} & \frac{28367198390048}{1982067999646851} & - \frac{5924445050578537}{660689333215617}\\\frac{11075363038922}{1982067999646851} & - \frac{29759444826748}{1982067999646851} & \frac{281612087155126}{660689333215617}\\\frac{2748231707}{1982067999646851} & \frac{1890176860316}{1982067999646851} & 1\end{matrix}\right] H= \displaystyle \left[\begin{matrix}0.0069243 & 0.014312 & -8.9671\\0.0055878 & -0.015014 & 0.42624\\1.3865 \cdot 10^{-6} & 0.00095364 & 1.0\end{matrix}\right] \left[\begin{array}{c}{x^{\prime} * \lambda} \\ {y^{\prime} * \lambda} \\ {\lambda}\end{array}\right]=\left[\begin{array}{lll}{h_{11}} & {h_{12}} & {h_{13}} \\ {h_{21}} & {h_{22}} & {h_{23}} \\ {h_{31}} & {h_{32}} & {h_{33}}\end{array}\right] \cdot\left[\begin{array}{l}{x} \\ {y} \\ {1}\end{array}\right] p_5 (6|1|0) (1540|502) \displaystyle \left[\begin{matrix}0.00692 & 0.0143 & -8.97\\0.00559 & -0.015 & 0.426\\1.39 \cdot 10^{-6} & 0.000954 & 1.0\end{matrix}\right] * \displaystyle \left[\begin{matrix}1540\\502\\1\end{matrix}\right] = \displaystyle \left[\begin{matrix}8.8809\\1.4942\\1.4809\end{matrix}\right] x^{\prime} = 8.8809 / 1.4809 \approx 6 y^{\prime} = 1.4942 / 1.4809 \approx 1 p_6 (4|7|0) (1679|128) \displaystyle \left[\begin{matrix}0.00692 & 0.0143 & -8.97\\0.00559 & -0.015 & 0.426\\1.39 \cdot 10^{-6} & 0.000954 & 1.0\end{matrix}\right] * \displaystyle \left[\begin{matrix}1679\\128\\1\end{matrix}\right] = \displaystyle \left[\begin{matrix}4.4907\\7.8863\\1.1244\end{matrix}\right] x^{\prime} = 4.4907 / 1.1244 \approx 4 y^{\prime} = 7.8863 / 1.1244 \approx 7 V \displaystyle \left[\begin{matrix}0.0873968428008291 & 0.0682224411681068 & 7.78775669283991 \cdot 10^{-5} & -0.0307611970870902 & -0.047164344898438 & -4.31617782272004 \cdot 10^{-5} & -0.734415741368853 & -0.667210380053951 & -0.000763782447496494\\-0.1843309239413 & -0.0151678791434499 & -8.61313928411903 \cdot 10^{-5} & -0.177510439386387 & -0.196972727023475 & -0.000286352439937897 & 0.63453250533834 & -0.702034214967351 & -0.000453769669195275\\0.178502576804415 & 0.311156872128848 & 0.000325321190586577 & 0.77104600249753 & 0.446971881836657 & 0.000626740830458092 & 0.180594126282578 & -0.210730877512211 & -0.000435956847243289\\-0.669632242817859 & -0.535613720571974 & -0.000939878794635243 & 0.478237602951333 & -0.137967801791192 & 0.000272230101249859 & -0.129682866265852 & -0.0120304219769976 & -0.000973739592376422\\0.686925430229282 & -0.552023064176252 & 0.000248883448694341 & 0.254558652994286 & -0.38595799679894 & -0.000493796526964928 & 0.0865993870367576 & -0.046238819234477 & -0.00158707187572933\\-0.0790313367881196 & 0.55378718349052 & 0.0023403071836456 & 0.28199111075643 & -0.768889925477268 & -3.75757014301404 \cdot 10^{-5} & -0.0327206087187983 & 0.123639937181458 & 0.00038524047574376\\0.000450569782323263 & -0.0016191693558424 & 0.111042503882465 & 0.000924901054479592 & -0.000202764323620636 & 0.00454096268914545 & -0.00017046030825094 & -0.00107219827445236 & 0.993802818493512\\0.000319779779068856 & -0.000370212652047504 & 0.0463933646371045 & -0.000563323598922266 & -0.000441631139009571 & 0.998875291759086 & 0.000114547728791255 & -0.000112905332478902 & -0.0097483164699673\end{matrix}\right] \approx (0|0) \displaystyle \left[\begin{matrix}-0.000763782447496494 & -0.000453769669195275 & -0.000435956847243289\\-0.000973739592376422 & -0.00158707187572933 & 0.00038524047574376\\0.993802818493512 & -0.0097483164699673 & 1\end{matrix}\right] * \displaystyle \left[\begin{matrix}1679\\128\\1\end{matrix}\right] = \displaystyle \left[\begin{matrix}-1.3409\\-1.8377\\1668.3\end{matrix}\right],"['matrices', 'projective-geometry', 'svd']"
89,"Does one of the orthogonal groups $O^\pm(2n,2)$ always have an element of order $2n+1$",Does one of the orthogonal groups  always have an element of order,"O^\pm(2n,2) 2n+1","For arbitrary $n\ge 1$ , is it true that (at least) one of the orthogonal groups $O^\pm(2n,2)$ , consisting of $2n\times 2n$ matrices over the field of order $2$ , has an element of order $2n+1$ ? A quick check in MAGMA shows that it is true for $n\le 7$ (code still running for $n=8$ ). There is not an obvious pattern in the matrices found, but there are many solutions in each case, so some nice construction may be possible. EDIT: Reducing to $2n+1$ prime We can reduce to $2n+1$ prime as follows. This means that it is sufficient to prove that $2n+1$ divides the order of one of $O^\pm(2n,2)$ when $2n+1$ is prime. A quick check shows that this is true for primes $p<1000$ . Suppose the above is true when $2n+1$ is prime and $2n+1$ is the smallest non-prime for which the above does not hold. Then $2n+1=(2u+1)(2v+1)$ for some $u,v>1$ and $O^\epsilon(2u,2)$ has an element $M$ of order $2u+1$ for some $\epsilon\in\{\pm\}$ . One can check then that the following is an element of $O^{\epsilon'}(2n,2)$ (in the appropriate basis for some $\epsilon'\in\{\pm\}$ ) and has order $2n+1$ contrary to assumption: $$\left(\begin{matrix} \mathbf{0}_{2u} & M & \mathbf{0}_{2u} & \cdots & \cdots & \cdots & \mathbf{0}_{2u} & \mathbf{0}_{2v} \\ \mathbf{0}_{2u} & \mathbf{0}_{2u} & \mathbf{1}_{2u} & \mathbf{0}_{2u} & \cdots & \cdots & \mathbf{0}_{2u} & \mathbf{0}_{2v} \\ \mathbf{0}_{2u} & \cdots & \mathbf{0}_{2u} & \mathbf{1}_{2u} & \mathbf{0}_{2u} & \cdots & \mathbf{0}_{2u} & \mathbf{0}_{2v} \\ \vdots & \ddots & \ddots & \ddots & \ddots & \ddots & \vdots & \vdots \\ \mathbf{0}_{2u} & \cdots & \cdots & \cdots & \cdots & \mathbf{0}_{2u} & \mathbf{1}_{2u} & \mathbf{0}_{2v} \\ \mathbf{1}_{2u} & \mathbf{0}_{2u} & \cdots & \cdots & \cdots & \cdots & \mathbf{0}_{2u} & \mathbf{0}_{2v} \\ \mathbf{0}_{2u} & \cdots & \cdots & \cdots & \cdots & \cdots & \mathbf{0}_{2u} & \mathbf{1}_{2v}\\ \end{matrix}\right)$$","For arbitrary , is it true that (at least) one of the orthogonal groups , consisting of matrices over the field of order , has an element of order ? A quick check in MAGMA shows that it is true for (code still running for ). There is not an obvious pattern in the matrices found, but there are many solutions in each case, so some nice construction may be possible. EDIT: Reducing to prime We can reduce to prime as follows. This means that it is sufficient to prove that divides the order of one of when is prime. A quick check shows that this is true for primes . Suppose the above is true when is prime and is the smallest non-prime for which the above does not hold. Then for some and has an element of order for some . One can check then that the following is an element of (in the appropriate basis for some ) and has order contrary to assumption:","n\ge 1 O^\pm(2n,2) 2n\times 2n 2 2n+1 n\le 7 n=8 2n+1 2n+1 2n+1 O^\pm(2n,2) 2n+1 p<1000 2n+1 2n+1 2n+1=(2u+1)(2v+1) u,v>1 O^\epsilon(2u,2) M 2u+1 \epsilon\in\{\pm\} O^{\epsilon'}(2n,2) \epsilon'\in\{\pm\} 2n+1 \left(\begin{matrix}
\mathbf{0}_{2u} & M & \mathbf{0}_{2u} & \cdots & \cdots & \cdots & \mathbf{0}_{2u} & \mathbf{0}_{2v} \\
\mathbf{0}_{2u} & \mathbf{0}_{2u} & \mathbf{1}_{2u} & \mathbf{0}_{2u} & \cdots & \cdots & \mathbf{0}_{2u} & \mathbf{0}_{2v} \\
\mathbf{0}_{2u} & \cdots & \mathbf{0}_{2u} & \mathbf{1}_{2u} & \mathbf{0}_{2u} & \cdots & \mathbf{0}_{2u} & \mathbf{0}_{2v} \\
\vdots & \ddots & \ddots & \ddots & \ddots & \ddots & \vdots & \vdots \\
\mathbf{0}_{2u} & \cdots & \cdots & \cdots & \cdots & \mathbf{0}_{2u} & \mathbf{1}_{2u} & \mathbf{0}_{2v} \\
\mathbf{1}_{2u} & \mathbf{0}_{2u} & \cdots & \cdots & \cdots & \cdots & \mathbf{0}_{2u} & \mathbf{0}_{2v} \\
\mathbf{0}_{2u} & \cdots & \cdots & \cdots & \cdots & \cdots & \mathbf{0}_{2u} & \mathbf{1}_{2v}\\
\end{matrix}\right)","['matrices', 'group-theory', 'finite-groups', 'finite-fields']"
90,Why do eigenvectors arise as the solution of PCA?,Why do eigenvectors arise as the solution of PCA?,,I have very limited knowledge of linear algebra and therefore I don't have an geometrical intuition behind PCA. Why the eigen vectors (which are simply defined as vectors whose direction doesn't change after a linear transformation) are also the directions that maximize variance? I have seen PCA definition using Lagrange multipliers but I would like to have the geometrical intuition behind it. Thanks,I have very limited knowledge of linear algebra and therefore I don't have an geometrical intuition behind PCA. Why the eigen vectors (which are simply defined as vectors whose direction doesn't change after a linear transformation) are also the directions that maximize variance? I have seen PCA definition using Lagrange multipliers but I would like to have the geometrical intuition behind it. Thanks,,"['matrices', 'statistics', 'eigenvalues-eigenvectors', 'principal-component-analysis']"
91,Product of two Hermitian matrices,Product of two Hermitian matrices,,"According to Wikipedia: The product of two Hermitian matrices $A$ and $B$ is Hermitian if and only   if $AB = BA$. So if I understood correctly, if $C=AB$, then C will be Hermitian if and only if $AB=BA$. But... I've been able to create a matrix $S$ then did $R=SS^H$, and $R$ turned out to be Hermitian, even though $SS^H \neq S^HS$. So I'm clearly misunderstanding that property I quoted. Could anyone help me? Thank you!","According to Wikipedia: The product of two Hermitian matrices $A$ and $B$ is Hermitian if and only   if $AB = BA$. So if I understood correctly, if $C=AB$, then C will be Hermitian if and only if $AB=BA$. But... I've been able to create a matrix $S$ then did $R=SS^H$, and $R$ turned out to be Hermitian, even though $SS^H \neq S^HS$. So I'm clearly misunderstanding that property I quoted. Could anyone help me? Thank you!",,"['matrices', 'hermitian-matrices']"
92,Show that $|\det(A_n)|=n^{n/2}$,Show that,|\det(A_n)|=n^{n/2},For k $\ge2$ we recursively define $A_{2^k}$ as $\begin{bmatrix} A_{2^{k-1}} & A_{2^{k-1}} \\ A_{2^{k-1}} & -A_{2^{k-1}} \end{bmatrix}$ and $A_1=[1]$ The problem is to show that $|\det(A_n)|=n^{n/2}$ My attempt: we do an induction on $k$ $|\det(A_2)|=2=2^{2/2}$. Induction hypothesis: $|\det(A_{n})|=n^{n/2}$ and we want to show that $|\det(A_{2n})|=(2n)^n$ using block matrix properties $|\det(A_{2n})|=|\det(\begin{bmatrix} A_{n} & A_{n} \\ A_{n} & -A_{n} \end{bmatrix})|=|\det(-A)\det(A+AA^{-1}A)|=|2^n\det(A_n)^2|=|2^nn^n|=(2n)^n$ Can somone confirm that there are no flaws in the reasoning please?,For k $\ge2$ we recursively define $A_{2^k}$ as $\begin{bmatrix} A_{2^{k-1}} & A_{2^{k-1}} \\ A_{2^{k-1}} & -A_{2^{k-1}} \end{bmatrix}$ and $A_1=[1]$ The problem is to show that $|\det(A_n)|=n^{n/2}$ My attempt: we do an induction on $k$ $|\det(A_2)|=2=2^{2/2}$. Induction hypothesis: $|\det(A_{n})|=n^{n/2}$ and we want to show that $|\det(A_{2n})|=(2n)^n$ using block matrix properties $|\det(A_{2n})|=|\det(\begin{bmatrix} A_{n} & A_{n} \\ A_{n} & -A_{n} \end{bmatrix})|=|\det(-A)\det(A+AA^{-1}A)|=|2^n\det(A_n)^2|=|2^nn^n|=(2n)^n$ Can somone confirm that there are no flaws in the reasoning please?,,['matrices']
93,Applying the chain rule to a function mapping matrices to matrices,Applying the chain rule to a function mapping matrices to matrices,,"Given a function $F:\text{Mat}(n,n)\rightarrow\text{Mat}(n,n)$, I'd like to get $\frac{d}{dM_{ij}}F(M)^2$ using matrix calculus. I already derived it by simply writing it out explicitly for my choice of $F$, but this doesn't seem to generalize well if I want to take second derivatives of higher powers of $F(M)$ so I wanted to finally get my head around matrix calculus. To get it in matrix notation I noted that $$\frac{d}{dM_{ij}}F(M)^2 = \left(\frac{d}{dM} F(M)^2\right)[e^{ij}]$$ for $e^{ij}\in\text{Mat}(n,n)$ such that $(e^{ij})_{xy} = \delta_{i=x}\delta_{j=y}$. Now if $F$ were the identity, then I'd get $Me^{ij}+e^{ij}M$ (or more generally $\sum_{l=0}^{p-1}M^le^{ij}M^{p-l-1}$ for $F(M)^p$ instead of $F(M)^2$). Using the chain rule, would I get $$\left(F(M)e^{ij}+e^{ij}F(M)\right)\left(\frac{d}{dM}F(M)[e^{ij}]\right) ?$$ Optional information : It seems weird since I multiply(/pass as an argument) with $e^{ij}$ twice, which doesn't really make a lot of sense in the scalar case (e.g. $n=1$), but I don't know how to make use of the $(\frac{d}{dX} X^2)[Y]=XY+YX$ otherwise for symmetric $M$ and my choice of $F$ (which maps positive $M$ to positive $F(M)$) the trace of this object coincides with my direct computation (not sure if that's just a coincidence caused by some of the involved symmetries though). What I looked up so far: Most of the notes I could find online only dealt with scalar valued functions of matrices or vectors. http://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf (p15, eq 136) seems to be one exception, but they go back to pretty direct computations which is basically what I did so far. Instead I was thinking about getting something along the lines of Matrix chain rule question: what is $\frac{d}{dX} f(S)$ where $S = (A+X)^{-1}$ (just set $f=(F(M)^2)_{xy}$), but this doesn't seem to work if I don't know how to invert $F$.","Given a function $F:\text{Mat}(n,n)\rightarrow\text{Mat}(n,n)$, I'd like to get $\frac{d}{dM_{ij}}F(M)^2$ using matrix calculus. I already derived it by simply writing it out explicitly for my choice of $F$, but this doesn't seem to generalize well if I want to take second derivatives of higher powers of $F(M)$ so I wanted to finally get my head around matrix calculus. To get it in matrix notation I noted that $$\frac{d}{dM_{ij}}F(M)^2 = \left(\frac{d}{dM} F(M)^2\right)[e^{ij}]$$ for $e^{ij}\in\text{Mat}(n,n)$ such that $(e^{ij})_{xy} = \delta_{i=x}\delta_{j=y}$. Now if $F$ were the identity, then I'd get $Me^{ij}+e^{ij}M$ (or more generally $\sum_{l=0}^{p-1}M^le^{ij}M^{p-l-1}$ for $F(M)^p$ instead of $F(M)^2$). Using the chain rule, would I get $$\left(F(M)e^{ij}+e^{ij}F(M)\right)\left(\frac{d}{dM}F(M)[e^{ij}]\right) ?$$ Optional information : It seems weird since I multiply(/pass as an argument) with $e^{ij}$ twice, which doesn't really make a lot of sense in the scalar case (e.g. $n=1$), but I don't know how to make use of the $(\frac{d}{dX} X^2)[Y]=XY+YX$ otherwise for symmetric $M$ and my choice of $F$ (which maps positive $M$ to positive $F(M)$) the trace of this object coincides with my direct computation (not sure if that's just a coincidence caused by some of the involved symmetries though). What I looked up so far: Most of the notes I could find online only dealt with scalar valued functions of matrices or vectors. http://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf (p15, eq 136) seems to be one exception, but they go back to pretty direct computations which is basically what I did so far. Instead I was thinking about getting something along the lines of Matrix chain rule question: what is $\frac{d}{dX} f(S)$ where $S = (A+X)^{-1}$ (just set $f=(F(M)^2)_{xy}$), but this doesn't seem to work if I don't know how to invert $F$.",,"['matrices', 'matrix-calculus', 'chain-rule']"
94,finding the generators of a matrix in $SL_2(\mathbb{Z})$,finding the generators of a matrix in,SL_2(\mathbb{Z}),"I know the two generators of $SL_2(\mathbb{Z})$ are $S=\begin{bmatrix}      0 & -1  \\     1& 0  \end{bmatrix}$ and $T=\begin{bmatrix}     1 & 1  \\     0& 1 \end{bmatrix}$ . Furthermore, $T^n =\begin{bmatrix}     1 & n  \\     0& 1 \end{bmatrix}$ . Suppose I have any matrix $A$ in $SL_2(\mathbb{Z})$ . I want to find its generators. Here is what I have done so far: I am trying to find the $a,b,c,d \in \mathbb{N}$ such that $A=S^a T^b S^{-c} T^{-d} \iff A T^d S^c= S^a T^b$ . Also, since $S^4=1$ , I assumed $1 \leq a,c \leq 4$ . In general, I tried to  compare the coefficients of $A$ for every possible values of $a$ and $c$ , without success. Where is my mistake?","I know the two generators of are and . Furthermore, . Suppose I have any matrix in . I want to find its generators. Here is what I have done so far: I am trying to find the such that . Also, since , I assumed . In general, I tried to  compare the coefficients of for every possible values of and , without success. Where is my mistake?","SL_2(\mathbb{Z}) S=\begin{bmatrix} 
    0 & -1  \\
    1& 0 
\end{bmatrix} T=\begin{bmatrix}
    1 & 1  \\
    0& 1
\end{bmatrix} T^n =\begin{bmatrix}
    1 & n  \\
    0& 1
\end{bmatrix} A SL_2(\mathbb{Z}) a,b,c,d \in \mathbb{N} A=S^a T^b S^{-c} T^{-d} \iff A T^d S^c= S^a T^b S^4=1 1 \leq a,c \leq 4 A a c","['matrices', 'modular-forms']"
95,Non-singularity of a square matrix,Non-singularity of a square matrix,,The diagonal elements of a square matrix $M$ are odd integers while the off diagonals are even integers. Then can we say that $M$ must be non-singular? We can easily comment on the singularity of the matrix $M$. Let us take the identity matrix of order $2$ which is non-singular.So can I say the $M$ must be non-singular for every such case? please help me to clear this idea.,The diagonal elements of a square matrix $M$ are odd integers while the off diagonals are even integers. Then can we say that $M$ must be non-singular? We can easily comment on the singularity of the matrix $M$. Let us take the identity matrix of order $2$ which is non-singular.So can I say the $M$ must be non-singular for every such case? please help me to clear this idea.,,"['matrices', 'singularity']"
96,Inequality: Norm of difference in exponential of matrices,Inequality: Norm of difference in exponential of matrices,,"Let $A$ and $B$ be two matrices. Then, $$ \|e^{A+B}- e^A\| \leq \|B\|e^{\|A\|+\|B\|}. $$ Can anyone give me a proof or reference for this inequality, please? Thank you.","Let $A$ and $B$ be two matrices. Then, $$ \|e^{A+B}- e^A\| \leq \|B\|e^{\|A\|+\|B\|}. $$ Can anyone give me a proof or reference for this inequality, please? Thank you.",,"['matrices', 'inequality', 'reference-request', 'normed-spaces', 'matrix-exponential']"
97,Is every unitary matrix of the form $e^{A}$ where $A$ is skew-adjoint?,Is every unitary matrix of the form  where  is skew-adjoint?,e^{A} A,"If (and only if) $A$ is a skew-adjoint matrix, then $e^{A}$ is unitary.  Can this be reversed?  In other words, can every unitary matrix be written as a matrix exponential?","If (and only if) $A$ is a skew-adjoint matrix, then $e^{A}$ is unitary.  Can this be reversed?  In other words, can every unitary matrix be written as a matrix exponential?",,"['matrices', 'operator-theory']"
98,The spectral norm of a positive definite matrix is its largest eigenvalue,The spectral norm of a positive definite matrix is its largest eigenvalue,,"Let $A$ be a symmetric positive definite matrix. Show that $$\lambda_n=\max \left\{\frac{\|Ax\|}{\|x\|}: x\ne 0 \right\}$$ is the largest eigenvalue of $A$ . My try is $\frac{\|Ax\|}{\|x\|}\le \lambda,\forall \lambda$ as a not eigenvalue of $A$ , and the equality occurs when $\lambda$ is an eigenvalue. So $\lambda$ is maximum? May be I am not even understanding the question.","Let be a symmetric positive definite matrix. Show that is the largest eigenvalue of . My try is as a not eigenvalue of , and the equality occurs when is an eigenvalue. So is maximum? May be I am not even understanding the question.","A \lambda_n=\max \left\{\frac{\|Ax\|}{\|x\|}: x\ne 0 \right\} A \frac{\|Ax\|}{\|x\|}\le \lambda,\forall \lambda A \lambda \lambda","['matrices', 'symmetric-matrices', 'positive-definite', 'matrix-norms', 'spectral-norm']"
99,Why is this determinant zero ? (block matrix),Why is this determinant zero ? (block matrix),,"I have two non-singular matrices $P_1$ and $P_2$ such that their sum $P_1+P_2$ is also non-singular. The calculations I need to do lead me to the following block matrix: $$\begin{pmatrix} (P_1+P_2)^{-1} & (P_1+P_2)^{-1}-P_2^{-1} \\ (P_1+P_2)^{-1}-P_1^{-1} & (P_1+P_2)^{-1}\end{pmatrix}$$ Its determinant appears to be always null (i tried with random-generated $P_1$ and $P_2$) but I do not find any reason why, even though it is easy to prove it when $P_1$ and $P_2$ are $1\times 1$. To follow up with this question, can we find a linear combination of these matrix that is zero ? Edit : this weaker version of my problem has been solved. It also solves this stronger version : let's consider the whole problem then. We have $P_1$, $P_2$, ... $P_n$ non-singular such that every sum of these is also non-singular. The block matrix is now $$ \begin{pmatrix} \Sigma^{-1} & \Sigma^{-1}-A_1 & \dots & \Sigma^{-1}-A_1 \\ \Sigma^{-1}-A_2 & \Sigma^{-1} & \dots & \Sigma^{-1}-A_2 \\ \vdots & \vdots & \ddots & \vdots \\ \Sigma^{-1}-A_n & \Sigma^{-1}-A_n & \dots & \Sigma^{-1} \end{pmatrix}$$ Where $\Sigma = \displaystyle \sum_{i}P_i$ and $A_k = \displaystyle \left(\sum_{i\neq k}P_i\right)^{-1}$. Multypling by $(P_1, P_2,\dots,P_n)$ gives zero","I have two non-singular matrices $P_1$ and $P_2$ such that their sum $P_1+P_2$ is also non-singular. The calculations I need to do lead me to the following block matrix: $$\begin{pmatrix} (P_1+P_2)^{-1} & (P_1+P_2)^{-1}-P_2^{-1} \\ (P_1+P_2)^{-1}-P_1^{-1} & (P_1+P_2)^{-1}\end{pmatrix}$$ Its determinant appears to be always null (i tried with random-generated $P_1$ and $P_2$) but I do not find any reason why, even though it is easy to prove it when $P_1$ and $P_2$ are $1\times 1$. To follow up with this question, can we find a linear combination of these matrix that is zero ? Edit : this weaker version of my problem has been solved. It also solves this stronger version : let's consider the whole problem then. We have $P_1$, $P_2$, ... $P_n$ non-singular such that every sum of these is also non-singular. The block matrix is now $$ \begin{pmatrix} \Sigma^{-1} & \Sigma^{-1}-A_1 & \dots & \Sigma^{-1}-A_1 \\ \Sigma^{-1}-A_2 & \Sigma^{-1} & \dots & \Sigma^{-1}-A_2 \\ \vdots & \vdots & \ddots & \vdots \\ \Sigma^{-1}-A_n & \Sigma^{-1}-A_n & \dots & \Sigma^{-1} \end{pmatrix}$$ Where $\Sigma = \displaystyle \sum_{i}P_i$ and $A_k = \displaystyle \left(\sum_{i\neq k}P_i\right)^{-1}$. Multypling by $(P_1, P_2,\dots,P_n)$ gives zero",,['matrices']
