,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Lattice generated by vectors orthogonal to an integer vector,Lattice generated by vectors orthogonal to an integer vector,,"Given a non-zero vector $\boldsymbol{v}$ composed of integers, imagine the set of all non-zero integer vectors $\boldsymbol{u}$, such that $\boldsymbol{u} \cdot \boldsymbol{v} = 0$, i.e., the integer vectors orthogonal the original vector.  The set $S = \{\boldsymbol{u} : \boldsymbol{u} \cdot \boldsymbol{v} = 0\}$ seems to form a $dim(\boldsymbol{v})-1$ dimensional lattice.  Specifically, it's clear that for any two elements of $S$, their linear combination is also in $S$.  However, because S is a subset of the lattice of all integer vectors, it's also a lattice. It there a name for this lattice?  Additionally, how can it's basis vectors be computed?","Given a non-zero vector $\boldsymbol{v}$ composed of integers, imagine the set of all non-zero integer vectors $\boldsymbol{u}$, such that $\boldsymbol{u} \cdot \boldsymbol{v} = 0$, i.e., the integer vectors orthogonal the original vector.  The set $S = \{\boldsymbol{u} : \boldsymbol{u} \cdot \boldsymbol{v} = 0\}$ seems to form a $dim(\boldsymbol{v})-1$ dimensional lattice.  Specifically, it's clear that for any two elements of $S$, their linear combination is also in $S$.  However, because S is a subset of the lattice of all integer vectors, it's also a lattice. It there a name for this lattice?  Additionally, how can it's basis vectors be computed?",,"['linear-algebra', 'algorithms', 'integer-lattices', 'vector-lattices']"
1,Prove that if $b \ne 0$ then the set of solutions to $Ax=b$ is not a subspace,Prove that if  then the set of solutions to  is not a subspace,b \ne 0 Ax=b,"I realize that I will probably have to prove that the solution set does not contain the zero vector.  I've been trying to prove this, but I am not sure how to. This is what I have so far, but it doesn't sound very proofy.  I'm new to proofs and math, so I don't know if this is right or not. My attempt? Assume that $A$ is a non-zero invertible matrix.  Then $0 = A^{-1}b$ is impossible unless $b=0$.  Therefore, the set is not a subspace.","I realize that I will probably have to prove that the solution set does not contain the zero vector.  I've been trying to prove this, but I am not sure how to. This is what I have so far, but it doesn't sound very proofy.  I'm new to proofs and math, so I don't know if this is right or not. My attempt? Assume that $A$ is a non-zero invertible matrix.  Then $0 = A^{-1}b$ is impossible unless $b=0$.  Therefore, the set is not a subspace.",,['linear-algebra']
2,Prove rank(AP) = rank(A) if P is an invertible n × n matrix and A is any m × n matrix?,Prove rank(AP) = rank(A) if P is an invertible n × n matrix and A is any m × n matrix?,,I know how to prove But what about we have different size of AP matrix?,I know how to prove But what about we have different size of AP matrix?,,"['linear-algebra', 'matrices', 'matrix-rank']"
3,Determinant of rank-one perturbation of a diagonal matrix,Determinant of rank-one perturbation of a diagonal matrix,,"Let $A$ be a rank-one perturbation of a diagonal matrix, i. e. $A = D + s^T s$, where $D = \DeclareMathOperator{diag}{diag} \diag\{\lambda_1,\ldots,\lambda_n\}$, $s = [s_1,\ldots,s_n] \neq 0$. Is there a way to easily compute its determinant? One the one hand, $s^Ts$ has rank one so that it has only one non-zero eigenvalue which is equal to its trace $|s|^2 = s_1^2+\cdots+s_n^2$. On the other hand, if $D$ was a scalar operator (i.e. all $\lambda_i$'s were equal) then all eigenvalues of $A$ would be shifts of the eigenvalues of $s^T s$ by $\lambda$. Thus one eigenvalue would be equal to $\lambda+|s|^2$ and the others to $\lambda$. Hence in this case we would obtain $\det A = \lambda^{n-1} (\lambda+|s|^2)$. But is it possible to generalize these considerations to the case of diagonal non-scalar $D$?","Let $A$ be a rank-one perturbation of a diagonal matrix, i. e. $A = D + s^T s$, where $D = \DeclareMathOperator{diag}{diag} \diag\{\lambda_1,\ldots,\lambda_n\}$, $s = [s_1,\ldots,s_n] \neq 0$. Is there a way to easily compute its determinant? One the one hand, $s^Ts$ has rank one so that it has only one non-zero eigenvalue which is equal to its trace $|s|^2 = s_1^2+\cdots+s_n^2$. On the other hand, if $D$ was a scalar operator (i.e. all $\lambda_i$'s were equal) then all eigenvalues of $A$ would be shifts of the eigenvalues of $s^T s$ by $\lambda$. Thus one eigenvalue would be equal to $\lambda+|s|^2$ and the others to $\lambda$. Hence in this case we would obtain $\det A = \lambda^{n-1} (\lambda+|s|^2)$. But is it possible to generalize these considerations to the case of diagonal non-scalar $D$?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
4,Upper triangular matrix representation for a linear operator,Upper triangular matrix representation for a linear operator,,"I just the read the proof that every linear operator over finite-dimensional complex vector space has an upper triangular matrix with respect to some basis of $V$ (pretty neat fact!). I am using Linear Algebra Done Right by Sheldon Axler. My question is about the ending of the proof. So before I ask it, here is the verbatim proof: I can't help but notice that we are proving something stronger. Namely, $Tv_k\in \textrm{span}(u_1, u_2, ..., u_m, v_k)$ for every $1\leq k\leq n$. Correct? But the author just goes straight into saying that $Tv_k\in \textrm{span}(u_1, u_2, ..., u_m, v_1, v_2, ..., v_k)$. So my main question is: Is my observation correct? If so, does this slightly stronger result leads to any useful fact? As far as I can tell, the matrix will have many more zeros in this case.","I just the read the proof that every linear operator over finite-dimensional complex vector space has an upper triangular matrix with respect to some basis of $V$ (pretty neat fact!). I am using Linear Algebra Done Right by Sheldon Axler. My question is about the ending of the proof. So before I ask it, here is the verbatim proof: I can't help but notice that we are proving something stronger. Namely, $Tv_k\in \textrm{span}(u_1, u_2, ..., u_m, v_k)$ for every $1\leq k\leq n$. Correct? But the author just goes straight into saying that $Tv_k\in \textrm{span}(u_1, u_2, ..., u_m, v_1, v_2, ..., v_k)$. So my main question is: Is my observation correct? If so, does this slightly stronger result leads to any useful fact? As far as I can tell, the matrix will have many more zeros in this case.",,['linear-algebra']
5,maximum eigenvalue of a diagonal plus rank-one matrix,maximum eigenvalue of a diagonal plus rank-one matrix,,I want to compute the maximum eigenvalue of a diagonal plus rank-one matrix. Are there fast algorithms for the computation of the largest eigenvalue? What is the computational complexity of those algorithms?,I want to compute the maximum eigenvalue of a diagonal plus rank-one matrix. Are there fast algorithms for the computation of the largest eigenvalue? What is the computational complexity of those algorithms?,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'algorithms', 'numerical-linear-algebra']"
6,"Is perspective transform affine? If it is, why it's impossible to perspective a square by an affine transform, given by matrix and shift vector?","Is perspective transform affine? If it is, why it's impossible to perspective a square by an affine transform, given by matrix and shift vector?",,"I'm a bit confused. I want to program a perspective transformation and thought that it is an affine one, but seemingly it is not. As an example, I want to perspective a square into a quadrilateral (as shown below), but it seems impossible to represent such a transform as a matrix multiplication+shift: 1) What I can't understand is that by definition affine transform is the one, that preserves all the staight lines. Can you provide an example of straight line, which is not preserved in this case? 2) How do I represent perspective transforms as this one numerically? Thank you.","I'm a bit confused. I want to program a perspective transformation and thought that it is an affine one, but seemingly it is not. As an example, I want to perspective a square into a quadrilateral (as shown below), but it seems impossible to represent such a transform as a matrix multiplication+shift: 1) What I can't understand is that by definition affine transform is the one, that preserves all the staight lines. Can you provide an example of straight line, which is not preserved in this case? 2) How do I represent perspective transforms as this one numerically? Thank you.",,"['linear-algebra', 'transformation', 'affine-geometry']"
7,Concept of Linearity,Concept of Linearity,,"I hear so many terms involving the word ""linear"". Linear function, linear equation, linear system, linear operator, linear transformation, linear mapping, linear space, linear algebra, linear electrical circuits, linear filters, linear electrical elements, linear approximation, linear optimization. I'm getting crazy trying to understand the application of the ""linear concept"" to all this aspects (function, equation, mapping, system, operator, transformation, algebra, etc.) and I wish to know the one essence that is to be linear. What something has to be to be linear? If I say something is linear, what do I know for sure about that something (no matter what the something is)? I heard a definition of linearity is by homogeinity (scaling the input results in a scaled output) and addition (summing the inputs results in summing the outputs). Can I apply this simple definition to all the branches (operator, mapping, system, transformation, algebra, ...) I mentioned ? Do they all behave like a line ? y = ax + b, for example, is a line but doesn't behave like a line because y is not linear.","I hear so many terms involving the word ""linear"". Linear function, linear equation, linear system, linear operator, linear transformation, linear mapping, linear space, linear algebra, linear electrical circuits, linear filters, linear electrical elements, linear approximation, linear optimization. I'm getting crazy trying to understand the application of the ""linear concept"" to all this aspects (function, equation, mapping, system, operator, transformation, algebra, etc.) and I wish to know the one essence that is to be linear. What something has to be to be linear? If I say something is linear, what do I know for sure about that something (no matter what the something is)? I heard a definition of linearity is by homogeinity (scaling the input results in a scaled output) and addition (summing the inputs results in summing the outputs). Can I apply this simple definition to all the branches (operator, mapping, system, transformation, algebra, ...) I mentioned ? Do they all behave like a line ? y = ax + b, for example, is a line but doesn't behave like a line because y is not linear.",,"['linear-algebra', 'functions']"
8,Minimal polynomial and invariant subspaces.,Minimal polynomial and invariant subspaces.,,"Let $T$ be a linear operator on the $n$ dimensional vector space $\mathbb{V}$. Suppose that $\mathbb{V} = \sum_{i=1}^{k}W_i$ where each $W_i$ is $T$ - invariant. Let $\mu_{T_i}$ be the minimal polynomial of the operator restricted to $W_i$. If the minimal polynomials of all restrictions are coprime, that is if $\gcd(\mu_{T_i}, \mu_{T_j}) = 1$ for $i\neq j$, is it true that the subspaces $W_i$ are independent? That is, $\mathbb{V} = \bigoplus_{i=1}^k W_i$. Perhaps more generally, is it true that if the minimal polynomials of two subspaces are coprime, then the two subspaces are independent? The converse is easily seen to be false, the identity operator is a clear counterexample. I haven't been able to find an easy counterexample to the above statement however. A proof or counterexample would be appreciated.","Let $T$ be a linear operator on the $n$ dimensional vector space $\mathbb{V}$. Suppose that $\mathbb{V} = \sum_{i=1}^{k}W_i$ where each $W_i$ is $T$ - invariant. Let $\mu_{T_i}$ be the minimal polynomial of the operator restricted to $W_i$. If the minimal polynomials of all restrictions are coprime, that is if $\gcd(\mu_{T_i}, \mu_{T_j}) = 1$ for $i\neq j$, is it true that the subspaces $W_i$ are independent? That is, $\mathbb{V} = \bigoplus_{i=1}^k W_i$. Perhaps more generally, is it true that if the minimal polynomials of two subspaces are coprime, then the two subspaces are independent? The converse is easily seen to be false, the identity operator is a clear counterexample. I haven't been able to find an easy counterexample to the above statement however. A proof or counterexample would be appreciated.",,['linear-algebra']
9,"Let $A,B\in M_3(\mathbb{C})$ be invertible matrices such that $AB=BA=X$, $A^{T}+A=B^{T}+B=X^{T}+X$. Then, is $\det(X - I) = 0$?","Let  be invertible matrices such that , . Then, is ?","A,B\in M_3(\mathbb{C}) AB=BA=X A^{T}+A=B^{T}+B=X^{T}+X \det(X - I) = 0","Let $A,B\in M_3(\mathbb{C})$ be invertible matrices such that $AB=BA=X$ , $A^{T}+A=B^{T}+B=X^{T}+X$ , then: (A) $A=B$ (B) $\det(A-I)=0$ (C) $\det(B-I)=0$ (D) $\det(X-I)=0$ My working: $AB+(AB)^T=X+X^T\implies AB+B^TA^T=B+B^T\implies (A-I)B+B^T(A^T-I)=O_3$ $\implies (A-I)B+((A-I)B)^T=O_3\implies (A-I)B$ is skew symmetric matrix of order $3$ $\implies \det(A-I)=0$ . Similarly $\det (B-I)=0$ . How to check option D any counter example for D? Further progress: $A^T+A=B^T+B\iff A= \begin{pmatrix} \alpha & a_1 & a_2 \\ x-a_1 & \beta & a_3 \\ y-a_2 & z-a_3 & \gamma \end{pmatrix}$ and $B =\begin{pmatrix} \alpha & b_1 & b_2 \\ x-b_1 & \beta & b_3 \\ y-b_2 & z-b_3 & \gamma \end{pmatrix}$ . Counter example for (A): $A=I_3$ and $B= \begin{pmatrix} 1 & b_1 & b_2 \\ -b_1 & 1 & b_3 \\ -b_2 & -b_3 & 1 \end{pmatrix}\implies AB=BA=B\implies X=B$ . We can see that all conditions meet out and $A\ne B$ for at least one $b_i\ne0$ .","Let be invertible matrices such that , , then: (A) (B) (C) (D) My working: is skew symmetric matrix of order . Similarly . How to check option D any counter example for D? Further progress: and . Counter example for (A): and . We can see that all conditions meet out and for at least one .","A,B\in M_3(\mathbb{C}) AB=BA=X A^{T}+A=B^{T}+B=X^{T}+X A=B \det(A-I)=0 \det(B-I)=0 \det(X-I)=0 AB+(AB)^T=X+X^T\implies AB+B^TA^T=B+B^T\implies (A-I)B+B^T(A^T-I)=O_3 \implies (A-I)B+((A-I)B)^T=O_3\implies (A-I)B 3 \implies \det(A-I)=0 \det (B-I)=0 A^T+A=B^T+B\iff A= \begin{pmatrix} \alpha & a_1 & a_2 \\ x-a_1 & \beta & a_3 \\ y-a_2 & z-a_3 & \gamma \end{pmatrix} B =\begin{pmatrix} \alpha & b_1 & b_2 \\ x-b_1 & \beta & b_3 \\ y-b_2 & z-b_3 & \gamma \end{pmatrix} A=I_3 B= \begin{pmatrix} 1 & b_1 & b_2 \\ -b_1 & 1 & b_3 \\ -b_2 & -b_3 & 1 \end{pmatrix}\implies AB=BA=B\implies X=B A\ne B b_i\ne0","['linear-algebra', 'matrices', 'skew-symmetric-matrices']"
10,"Number of $n \times n$ $\{0,1\}$-matrices $A$ such that $A^2$ is the transpose of $A$?",Number of  -matrices  such that  is the transpose of ?,"n \times n \{0,1\} A A^2 A","While browsing the OEIS, I saw the above nice question. The few terms provided suggest that they were found by brute force. We conjecture that the number of $\{0, 1\}$ -matrices, which also satisfy the given condition, can be determined from the recurrence $$ 2\, a(n - 1)  +  (n^2 - 3 n + 2) \, a(n - 3)  \quad (n \ge 3), $$ starting $ a(0) = 1,\, a(1) = 2,\, a(2) = 4. $ Or, equivalently,  that $$ a(n) = \sum_{k=0}^{\lfloor n / 3 \rfloor} \frac{ 2^{n - 3 k}  \,  n!  }{3^k \,  (n - 3 k)! \, k!} \quad (n \ge 0) . $$ Can anyone prove this? The sequence is OEIS A336614 .","While browsing the OEIS, I saw the above nice question. The few terms provided suggest that they were found by brute force. We conjecture that the number of -matrices, which also satisfy the given condition, can be determined from the recurrence starting Or, equivalently,  that Can anyone prove this? The sequence is OEIS A336614 .","\{0, 1\}  2\, a(n - 1)  +  (n^2 - 3 n + 2) \, a(n - 3)  \quad (n \ge 3),   a(0) = 1,\, a(1) = 2,\, a(2) = 4.   a(n) = \sum_{k=0}^{\lfloor n / 3 \rfloor} \frac{ 2^{n - 3 k}  \,  n!  }{3^k \,  (n - 3 k)! \, k!} \quad (n \ge 0) . ","['linear-algebra', 'sequences-and-series', 'combinatorics']"
11,Grassmannian as a quotient of orthogonal or general linear group,Grassmannian as a quotient of orthogonal or general linear group,,"I'm trying to understand some different ways to construct the Grassmannian of a real vector space, but I'm having trouble getting some of the notation and definitions. One definition that I often see given, such as on Wikipedia , is $$Gr(r,n) = O(n)/(O(r) \times O(n–r))$$ where $Gr(r,n)$ is the Grassmannian of $r$ -dimensional subspaces of $\Bbb R^n$ . My first question is: what does this quotient notation mean? $O(n)$ is the group of $n\times n$ orthogonal matrices, whereas $O(r)$ and $O(n-r)$ are the groups of $r \times r$ and $(n-r) \times (n-r)$ orthogonal matrices. How are we viewing the cartesian product of $O(r) \times O(n-r)$ as a group of $n \times n$ matrices? A different way to construct this is also given by Wikipedia: First, recall that the general linear group GL(V) acts transitively on   the r-dimensional subspaces of V. Therefore, if H is the stabilizer of   any of the subspaces under this action, we have $$Gr(r,V) = GL(V)/H$$ This is also somewhat puzzling to me. You start with the general linear group of invertible $r \times r$ matrices, which acts transitively on subspaces, meaning for any two subspaces $a$ and $b$ there is some element of $GL(V)$ that maps $a$ to $b$ . But then we quotient by what stabilizer, exactly, to construct the Grassmannian as a quotient space? The only stabilizer that preserves all subspaces are simple scalar transformations; if we mod by those we get the projective linear group $PGL(V)$ , not the Grassmannian. If on the other hand, they mean that we quotient by all such $H$ that are a stabilizer for any subspace, this means that we are quotienting by every element of $GL(V)$ that has any real eigenspace, which is almost the entire group, except perhaps for rotation matrices in even dimensions, where all eigenvalues are complex. How is this thing supposed to be constructed? And how do these yield any type of metric structure?","I'm trying to understand some different ways to construct the Grassmannian of a real vector space, but I'm having trouble getting some of the notation and definitions. One definition that I often see given, such as on Wikipedia , is where is the Grassmannian of -dimensional subspaces of . My first question is: what does this quotient notation mean? is the group of orthogonal matrices, whereas and are the groups of and orthogonal matrices. How are we viewing the cartesian product of as a group of matrices? A different way to construct this is also given by Wikipedia: First, recall that the general linear group GL(V) acts transitively on   the r-dimensional subspaces of V. Therefore, if H is the stabilizer of   any of the subspaces under this action, we have This is also somewhat puzzling to me. You start with the general linear group of invertible matrices, which acts transitively on subspaces, meaning for any two subspaces and there is some element of that maps to . But then we quotient by what stabilizer, exactly, to construct the Grassmannian as a quotient space? The only stabilizer that preserves all subspaces are simple scalar transformations; if we mod by those we get the projective linear group , not the Grassmannian. If on the other hand, they mean that we quotient by all such that are a stabilizer for any subspace, this means that we are quotienting by every element of that has any real eigenspace, which is almost the entire group, except perhaps for rotation matrices in even dimensions, where all eigenvalues are complex. How is this thing supposed to be constructed? And how do these yield any type of metric structure?","Gr(r,n) = O(n)/(O(r) \times O(n–r)) Gr(r,n) r \Bbb R^n O(n) n\times n O(r) O(n-r) r \times r (n-r) \times (n-r) O(r) \times O(n-r) n \times n Gr(r,V) = GL(V)/H r \times r a b GL(V) a b PGL(V) H GL(V)","['linear-algebra', 'group-theory', 'algebraic-geometry', 'quotient-spaces', 'grassmannian']"
12,"Prove: if two homogeneous systems of linear equations in two unknowns have the same solutions, then they are equivalent.","Prove: if two homogeneous systems of linear equations in two unknowns have the same solutions, then they are equivalent.",,"From Exercise 6 of Sec 1.2 of Linear Algebra by K.Hoffman and R.Kunze. Equivalence is defined as follows: Two systems of linear equations are equivalent if each equation in   each system is a linear combination of the equations in the other   system. I did some research, but most answers I found on math.stackexchange.com either use some serious math like ranks or null spaces, or are merely ""intuitive"". I'm curious how to formalize the proof, with concepts previously defined. Thx. More specifically -- $A_{11}x_1+A_{12}x_2=0$, $A_{21}x_1+A_{22}x_2=0$; $B_{11}x_1+B_{12}x_2=0$, $B_{21}x_1+B_{22}x_2=0$; We are asked to find $c_{11},c_{12},c_{21},c_{22}$ such that: $A_{11}c_{11}+A_{21}c_{12}=B_{11}$; $A_{12}c_{11}+A_{22}c_{12}=B_{12}$; $A_{11}c_{21}+A_{21}c_{22}=B_{21}$; $A_{12}c_{21}+A_{22}c_{22}=B_{22}$; (Sorry about the mess...I don't know how to type systems of equations...) I can't convince myself in formal language the existence of such $c$s. Since this problem appears at the beginning of the book, I assume there's a rather rookie proof. I suppose I have to express the $c$s in terms of the given quantities. But how?","From Exercise 6 of Sec 1.2 of Linear Algebra by K.Hoffman and R.Kunze. Equivalence is defined as follows: Two systems of linear equations are equivalent if each equation in   each system is a linear combination of the equations in the other   system. I did some research, but most answers I found on math.stackexchange.com either use some serious math like ranks or null spaces, or are merely ""intuitive"". I'm curious how to formalize the proof, with concepts previously defined. Thx. More specifically -- $A_{11}x_1+A_{12}x_2=0$, $A_{21}x_1+A_{22}x_2=0$; $B_{11}x_1+B_{12}x_2=0$, $B_{21}x_1+B_{22}x_2=0$; We are asked to find $c_{11},c_{12},c_{21},c_{22}$ such that: $A_{11}c_{11}+A_{21}c_{12}=B_{11}$; $A_{12}c_{11}+A_{22}c_{12}=B_{12}$; $A_{11}c_{21}+A_{21}c_{22}=B_{21}$; $A_{12}c_{21}+A_{22}c_{22}=B_{22}$; (Sorry about the mess...I don't know how to type systems of equations...) I can't convince myself in formal language the existence of such $c$s. Since this problem appears at the beginning of the book, I assume there's a rather rookie proof. I suppose I have to express the $c$s in terms of the given quantities. But how?",,['linear-algebra']
13,Why is the kernel of an integral transform called kernel?,Why is the kernel of an integral transform called kernel?,,"I mean, in mathematics things with the same name are usually related. So, what is the relationship between the kernel of an integral transform and the kernel of an linear transformation? If it is none, why the kernel of an integral transform is called like that? There is another post here that says that there is no relationship but I don't think that is true.","I mean, in mathematics things with the same name are usually related. So, what is the relationship between the kernel of an integral transform and the kernel of an linear transformation? If it is none, why the kernel of an integral transform is called like that? There is another post here that says that there is no relationship but I don't think that is true.",,"['linear-algebra', 'integral-transforms']"
14,What kind of vector spaces have exactly one basis?,What kind of vector spaces have exactly one basis?,,"Here is the question as an exercise in the book Linear Algebra Done Right, Chapter 2 Find all vector spaces that have exactly one basis.","Here is the question as an exercise in the book Linear Algebra Done Right, Chapter 2 Find all vector spaces that have exactly one basis.",,['linear-algebra']
15,Is the norm operator between normed spaces ever induced from an inner product?,Is the norm operator between normed spaces ever induced from an inner product?,,"Assume $(V,\| \|_V),(W,\| \|_W)$ are both finite dimensional normed spaces. We have the induced operator norm on $Hom(V,W)$. When does it occur that this norm is actually induced from some inner product? As observed by a comment of user225318, If $dimV=dimW=1$ then the answer can clearly be positive. It can be seen that in the case where $dimV=1   (V=\mathbb{R})$ and the norm on $W$ is induced by an inner product, the answer is positive. So let us require $dimV>1$ or that $\| \|_W$ is not induced by an inner product. To state this more clearly, I would be happy to find a complete characterization, i.e necessary & sufficient conditions on the dimensions of $V,W$ and on their respective norms that are equivalent to the operator norm being induced by an inner product.","Assume $(V,\| \|_V),(W,\| \|_W)$ are both finite dimensional normed spaces. We have the induced operator norm on $Hom(V,W)$. When does it occur that this norm is actually induced from some inner product? As observed by a comment of user225318, If $dimV=dimW=1$ then the answer can clearly be positive. It can be seen that in the case where $dimV=1   (V=\mathbb{R})$ and the norm on $W$ is induced by an inner product, the answer is positive. So let us require $dimV>1$ or that $\| \|_W$ is not induced by an inner product. To state this more clearly, I would be happy to find a complete characterization, i.e necessary & sufficient conditions on the dimensions of $V,W$ and on their respective norms that are equivalent to the operator norm being induced by an inner product.",,"['linear-algebra', 'normed-spaces', 'inner-products']"
16,"In a complex vector space, $\langle Tx,x \rangle=0 \implies T = 0$","In a complex vector space,","\langle Tx,x \rangle=0 \implies T = 0","Suppose $T$ is a linear operator on a complex inner product space. Is it a theorem that if $\langle Tx,x\rangle=0$ for all $x$ in the space then $T=0$. The theorem fails in the real case, as seen for instance by rotation by $\pi/2$ on $\mathbb{R}^2$. Is there anything deeper behind this fact, or can it mostly be looked at as a quirk of the conjugate-linearity of the complex inner product? If anyone is interested in looking up this proof, it is theorem 9.2 in Roman's Linear Algebra , third edition.","Suppose $T$ is a linear operator on a complex inner product space. Is it a theorem that if $\langle Tx,x\rangle=0$ for all $x$ in the space then $T=0$. The theorem fails in the real case, as seen for instance by rotation by $\pi/2$ on $\mathbb{R}^2$. Is there anything deeper behind this fact, or can it mostly be looked at as a quirk of the conjugate-linearity of the complex inner product? If anyone is interested in looking up this proof, it is theorem 9.2 in Roman's Linear Algebra , third edition.",,"['linear-algebra', 'complex-numbers', 'inner-products']"
17,"Let $a_1, ...,a_n , b_1,...b_n$ be $2n$ distinct elements of a field , then is the matrix $\Big(\dfrac1{a_i-b_j}\Big)_{ij}$ non-singular?","Let  be  distinct elements of a field , then is the matrix  non-singular?","a_1, ...,a_n , b_1,...b_n 2n \Big(\dfrac1{a_i-b_j}\Big)_{ij}","Let $a_1, ...,a_n , b_1,...b_n$ be $2n$ distinct elements  of a field and define $$h_{ij}:=\dfrac1{a_i-b_j} , \forall i,j=1,2 ,\dots,n. $$  Is the $n \times n$ matrix $H:=(h_{ij})$ non-singular ?","Let $a_1, ...,a_n , b_1,...b_n$ be $2n$ distinct elements  of a field and define $$h_{ij}:=\dfrac1{a_i-b_j} , \forall i,j=1,2 ,\dots,n. $$  Is the $n \times n$ matrix $H:=(h_{ij})$ non-singular ?",,"['linear-algebra', 'matrices', 'vector-spaces', 'linear-transformations']"
18,What is the motivation/application of dual spaces and transposes?,What is the motivation/application of dual spaces and transposes?,,"I've always been baffled as to where transposes come from. I found this question , but the answer isn't satisfying to me - the idea seems to be ""dual spaces are important, and you can define transposes using those"". This leaves two questions: Why are dual spaces important? Whatever it is that we want to do with dual spaces, how does the transpose help us accomplish that? For point (1) my linear algebra teacher told me something else that I didn't find quite satisfying, which is that if you're interested in linear transformations, then dual spaces are the ""simplest kind"" of linear transformation. This is quite vague though... what actual problems might we want to solve in which the concept of the dual space would arise naturally? And how would the concept of a transpose arise naturally from those?","I've always been baffled as to where transposes come from. I found this question , but the answer isn't satisfying to me - the idea seems to be ""dual spaces are important, and you can define transposes using those"". This leaves two questions: Why are dual spaces important? Whatever it is that we want to do with dual spaces, how does the transpose help us accomplish that? For point (1) my linear algebra teacher told me something else that I didn't find quite satisfying, which is that if you're interested in linear transformations, then dual spaces are the ""simplest kind"" of linear transformation. This is quite vague though... what actual problems might we want to solve in which the concept of the dual space would arise naturally? And how would the concept of a transpose arise naturally from those?",,['linear-algebra']
19,Help on the relationship of a basis and a dual basis,Help on the relationship of a basis and a dual basis,,"If $B_1 = \{v_1,\ldots,v_n\}$ and $B_2 = \{v_1',\ldots,v_n'\}$ are bases for a vector space $V$ , and $D_1= \{\delta v_1,\ldots, \delta v_n\}$ and $D_2 = \{\delta v_1',\ldots, \delta v_n'\}$ are the corresponding dual bases of $V^*$ prove that, if $P$ is the change-of-basis matrix from $B_1$ to $B_2$ , then $(P^{-1})^T$ is the change-of-basis from $D_1$ to $D_2$ (this is a theorem from Schaum's outline of theory and problems of linear algebra) My knowledge of linear algebra is not great, but so far I have that \begin{align} \text{For any }v \in V, [v]_{B_1}&=P[v]_{B_2}\\ \text{so } (\alpha_1,\ldots,\alpha_n)(v_1,\ldots,v_n)^T&=P(\beta_1,\ldots,\beta_n)(v_1',\ldots,v_n')^T \end{align} for some $\alpha_1,\ldots,\alpha_n,\beta_1,\ldots,\beta_n \in F$ (where $F$ is a field) How do I use the fact that $D_1$ , $D_2$ are dual basis to further this proof? And also, where does the transpose of $P^{-1}$ come in? Can I use that $D_1 = \delta B_1$ and $D_2 = \delta B_2$ (where I am guessing $\delta$ is the Kronecker delta?) Any hints or suggestions are greatly appreciated, I know that my knowledge is quite limited with regard to this, links to text or sources that could explain a problem like this would be great.","If and are bases for a vector space , and and are the corresponding dual bases of prove that, if is the change-of-basis matrix from to , then is the change-of-basis from to (this is a theorem from Schaum's outline of theory and problems of linear algebra) My knowledge of linear algebra is not great, but so far I have that for some (where is a field) How do I use the fact that , are dual basis to further this proof? And also, where does the transpose of come in? Can I use that and (where I am guessing is the Kronecker delta?) Any hints or suggestions are greatly appreciated, I know that my knowledge is quite limited with regard to this, links to text or sources that could explain a problem like this would be great.","B_1 = \{v_1,\ldots,v_n\} B_2 = \{v_1',\ldots,v_n'\} V D_1= \{\delta v_1,\ldots, \delta v_n\} D_2 = \{\delta v_1',\ldots, \delta v_n'\} V^* P B_1 B_2 (P^{-1})^T D_1 D_2 \begin{align}
\text{For any }v \in V, [v]_{B_1}&=P[v]_{B_2}\\
\text{so } (\alpha_1,\ldots,\alpha_n)(v_1,\ldots,v_n)^T&=P(\beta_1,\ldots,\beta_n)(v_1',\ldots,v_n')^T
\end{align} \alpha_1,\ldots,\alpha_n,\beta_1,\ldots,\beta_n \in F F D_1 D_2 P^{-1} D_1 = \delta B_1 D_2 = \delta B_2 \delta",['linear-algebra']
20,Inequality of Positive-definite matrix.,Inequality of Positive-definite matrix.,,"In this question matrix $A$ is positive-definite if and only if $\forall x\ne0 :x^TAx>0$. ($A$ is not necessarily symmetric) Let $D$ be a positive-definite matrix such that it has block form: $$D=\left( \begin{array}{cc} A & C  \\ C^T & B  \end{array} \right)$$ How can we prove that $\det D\leq\det A\det B$? EDIT1: From the perspective of the first answer I want to sum up something. It's true that $$D=\left( \begin{array}{cc} A & C  \\ C^T & B  \end{array} \right)=\left( \begin{array}{cc} I & 0  \\ C^TA^{-1} & I  \end{array} \right)\left( \begin{array}{cc} A & C  \\ 0 & B-C^TA^{-1}C  \end{array} \right)$$ so $\det D=\det A\det (B-C^TA^{-1}C)$. It's also true that $\det A>0$ because every real eigenvalues of $A$ is greater than $0$ and all complex eigenvalues exist in pairs. Now we need to prove that $$\det B>\det(B-C^TA^{-1}C)$$ The usual way when $A$ and $B$ are symmetric haven't worked yet because: (1) we don't know for sure if $(B-C^TA^{-1}C)$ is definite positive and (2) we don't know for sure if $\det(M+N)>\det(N)$ if $M$ and $N$ are definite positive. I also want to point out that definite positive matrices (in this question) can have complex eigenvalues. It would be great if you answer with details, not with references since almost every references consider positive-definite matrices to be symmetric.","In this question matrix $A$ is positive-definite if and only if $\forall x\ne0 :x^TAx>0$. ($A$ is not necessarily symmetric) Let $D$ be a positive-definite matrix such that it has block form: $$D=\left( \begin{array}{cc} A & C  \\ C^T & B  \end{array} \right)$$ How can we prove that $\det D\leq\det A\det B$? EDIT1: From the perspective of the first answer I want to sum up something. It's true that $$D=\left( \begin{array}{cc} A & C  \\ C^T & B  \end{array} \right)=\left( \begin{array}{cc} I & 0  \\ C^TA^{-1} & I  \end{array} \right)\left( \begin{array}{cc} A & C  \\ 0 & B-C^TA^{-1}C  \end{array} \right)$$ so $\det D=\det A\det (B-C^TA^{-1}C)$. It's also true that $\det A>0$ because every real eigenvalues of $A$ is greater than $0$ and all complex eigenvalues exist in pairs. Now we need to prove that $$\det B>\det(B-C^TA^{-1}C)$$ The usual way when $A$ and $B$ are symmetric haven't worked yet because: (1) we don't know for sure if $(B-C^TA^{-1}C)$ is definite positive and (2) we don't know for sure if $\det(M+N)>\det(N)$ if $M$ and $N$ are definite positive. I also want to point out that definite positive matrices (in this question) can have complex eigenvalues. It would be great if you answer with details, not with references since almost every references consider positive-definite matrices to be symmetric.",,"['linear-algebra', 'matrices']"
21,Matrix notation of an ellipse.,Matrix notation of an ellipse.,,"When I was reading a paper related to computer vision, I came across the following notation, where an ellipse is represented by the equation $\mathbf{x}^TM\mathbf{x} = 1$ , where the ellipse parameter M, is given by a $2\times2$ matrix where, $M = \begin{bmatrix}a & b\\ b & c \end{bmatrix}$ I want to know firstly, how does this equation represent an ellipse, and further how does this ellipse gets normalized to a circle with the affine transformation, $\mathbf{x}' = M^{1/2}\mathbf{x}$ .","When I was reading a paper related to computer vision, I came across the following notation, where an ellipse is represented by the equation , where the ellipse parameter M, is given by a matrix where, I want to know firstly, how does this equation represent an ellipse, and further how does this ellipse gets normalized to a circle with the affine transformation, .","\mathbf{x}^TM\mathbf{x} = 1 2\times2 M = \begin{bmatrix}a & b\\
b & c
\end{bmatrix} \mathbf{x}' = M^{1/2}\mathbf{x}","['linear-algebra', 'conic-sections', 'symmetric-matrices']"
22,Left and Right inverses of linear operators,Left and Right inverses of linear operators,,"Let $X$ and $U$ be vector spaces over a field $F$, and let $T : X \to U$. (a) If there exists an operator $S : U \to X$ such that  $S(T(x)) =x$ for all $x \in X$, then $S$ is called a left inverse of $T$. (b) If there exists an operator $S : U \to X$ such that  $T(S(u)) =u$ for all $u \in U$, then $S$ is called a right inverse of $T$. I'm trying to prove the following theorem. Let $X$ and $U$ be vector spaces over a field $F$, and let $T: X\to U$ be linear. (a) There exists a left inverse $S$ of $T$ iff $T$ is injective. (b) There exists a right inverse $S$ of $T$ iff $T$ is surjective. So far my study on linear algebra has been largely restricted to finite dimensional vector spaces but this problem, I think applies to general vector spaces. How can I solve this without resorting to basis?","Let $X$ and $U$ be vector spaces over a field $F$, and let $T : X \to U$. (a) If there exists an operator $S : U \to X$ such that  $S(T(x)) =x$ for all $x \in X$, then $S$ is called a left inverse of $T$. (b) If there exists an operator $S : U \to X$ such that  $T(S(u)) =u$ for all $u \in U$, then $S$ is called a right inverse of $T$. I'm trying to prove the following theorem. Let $X$ and $U$ be vector spaces over a field $F$, and let $T: X\to U$ be linear. (a) There exists a left inverse $S$ of $T$ iff $T$ is injective. (b) There exists a right inverse $S$ of $T$ iff $T$ is surjective. So far my study on linear algebra has been largely restricted to finite dimensional vector spaces but this problem, I think applies to general vector spaces. How can I solve this without resorting to basis?",,['linear-algebra']
23,Iterated duals of a vector space,Iterated duals of a vector space,,"Let $K$ be a field and $\mathcal U$ a universe such that $K\in\mathcal U$ . (Here, ""universe"" means ""uncountable Grothendieck universe"".) Let $\mathcal C$ be the category of $K$ -vector spaces belonging to $\mathcal U$ , and let $i\ge0$ be an integer. If $i$ is even, put $\mathcal C_i:=\mathcal C$ ; if $i$ is odd, put $\mathcal C_i:=\mathcal C^\text{op}$ . Let $F_i:\mathcal C_i\to\mathcal C$ be the $i$ -th dual functor. For integers $i,j\ge0$ of same parity, $\operatorname{Hom}(F_i,F_j)$ is a $K$ -vector space. In particular $$ d(K,\mathcal U,i,j):=\dim\operatorname{Hom}(F_i,F_j) $$ is a well-defined cardinal. Can one compute this cardinal? Does $d(K,\mathcal U,i,j)$ depend on $K$ and $\mathcal U$ ? Is $d(K,\mathcal U,i,j)$ finite? Have these questions been asked before? Edit. As an illustration, here is a proof of the equality $d(K,\mathcal U,2,0)=0$ . This case can be handled without universes. Assume that, for each vector space $V$ (over the field $K$ chosen once and for all), we have a linear map $\theta_V:V^{**}\to V$ , and suppose that, for each linear map $f:V\to W$ , we have $$ f\circ\theta_V=\theta_W\circ f^{**}. $$ We claim: $\theta_V=0$ for all $V$ . Proof. As a general notation, put $V_1:=V^*,V_2:=V^{**},f_2:=f^{**}$ , and, for each vector space $V$ , let $\varepsilon_V:V\to V_2$ be the natural map. In particular, the above display becomes $$ f\circ\theta_V=\theta_W\circ f_2. $$ It is easy to see that there is a scalar $\lambda\in K$ such that $\theta_V\circ\varepsilon_V=\lambda\operatorname{id}_V$ for all $V$ , and that we can assume either $\lambda=0$ or $\lambda=1$ . Case $\lambda=0$ . Since $\varepsilon_K$ is an isomorphism, we get $\theta_K=0$ . For $v_1$ in $V_1$ we have $v_1\circ\theta_V=\theta_K\circ v_{12}=0$ . Since $v_1$ is arbitrary, this implies $\theta_V=0$ . Case $\lambda=1$ . We are seeking a contradiction. For $v_1$ in $V_1$ we have $$ v_{12}=\varepsilon_K\circ\theta_K\circ v_{12}=\varepsilon_K\circ v_1\circ\theta_V, $$ that is $$ \big(v_{12}(v_2)\big)(k_1)=\bigg(\varepsilon_K\Big(v_1\big(\theta_V(v_2)\big)\Big)\bigg)(k_1) $$ for $v_2$ in $V_2$ and $k_1$ in $K_1$ . The last equality can be rewritten as $$ v_2(k_1\circ v_1)=k_1\Big(v_1\big(\theta_V(v_2)\big)\Big). $$ Taking the identity of $K$ as $k_1$ , we get $$ v_2(v_1)=v_1\big(\theta_V(v_2)\big) $$ for all $v_1$ in $V_1$ and all $v_2$ in $V_2$ . Let $V$ be infinite dimensional. By the Erdös-Kaplansky Theorem, there is a nonzero $v_2$ in $V_2$ such that $\theta_V(v_2)=0$ . Since $v_2\ne0$ , there is a $v_1$ in $V_1$ such that $v_2(v_1)\ne0$ , contradiction.","Let be a field and a universe such that . (Here, ""universe"" means ""uncountable Grothendieck universe"".) Let be the category of -vector spaces belonging to , and let be an integer. If is even, put ; if is odd, put . Let be the -th dual functor. For integers of same parity, is a -vector space. In particular is a well-defined cardinal. Can one compute this cardinal? Does depend on and ? Is finite? Have these questions been asked before? Edit. As an illustration, here is a proof of the equality . This case can be handled without universes. Assume that, for each vector space (over the field chosen once and for all), we have a linear map , and suppose that, for each linear map , we have We claim: for all . Proof. As a general notation, put , and, for each vector space , let be the natural map. In particular, the above display becomes It is easy to see that there is a scalar such that for all , and that we can assume either or . Case . Since is an isomorphism, we get . For in we have . Since is arbitrary, this implies . Case . We are seeking a contradiction. For in we have that is for in and in . The last equality can be rewritten as Taking the identity of as , we get for all in and all in . Let be infinite dimensional. By the Erdös-Kaplansky Theorem, there is a nonzero in such that . Since , there is a in such that , contradiction.","K \mathcal U K\in\mathcal U \mathcal C K \mathcal U i\ge0 i \mathcal C_i:=\mathcal C i \mathcal C_i:=\mathcal C^\text{op} F_i:\mathcal C_i\to\mathcal C i i,j\ge0 \operatorname{Hom}(F_i,F_j) K 
d(K,\mathcal U,i,j):=\dim\operatorname{Hom}(F_i,F_j)
 d(K,\mathcal U,i,j) K \mathcal U d(K,\mathcal U,i,j) d(K,\mathcal U,2,0)=0 V K \theta_V:V^{**}\to V f:V\to W 
f\circ\theta_V=\theta_W\circ f^{**}.
 \theta_V=0 V V_1:=V^*,V_2:=V^{**},f_2:=f^{**} V \varepsilon_V:V\to V_2 
f\circ\theta_V=\theta_W\circ f_2.
 \lambda\in K \theta_V\circ\varepsilon_V=\lambda\operatorname{id}_V V \lambda=0 \lambda=1 \lambda=0 \varepsilon_K \theta_K=0 v_1 V_1 v_1\circ\theta_V=\theta_K\circ v_{12}=0 v_1 \theta_V=0 \lambda=1 v_1 V_1 
v_{12}=\varepsilon_K\circ\theta_K\circ v_{12}=\varepsilon_K\circ v_1\circ\theta_V,
 
\big(v_{12}(v_2)\big)(k_1)=\bigg(\varepsilon_K\Big(v_1\big(\theta_V(v_2)\big)\Big)\bigg)(k_1)
 v_2 V_2 k_1 K_1 
v_2(k_1\circ v_1)=k_1\Big(v_1\big(\theta_V(v_2)\big)\Big).
 K k_1 
v_2(v_1)=v_1\big(\theta_V(v_2)\big)
 v_1 V_1 v_2 V_2 V v_2 V_2 \theta_V(v_2)=0 v_2\ne0 v_1 V_1 v_2(v_1)\ne0","['linear-algebra', 'category-theory', 'dual-spaces']"
24,"trace , determinant and which of the following are true(NBHM-$2014$)","trace , determinant and which of the following are true(NBHM-)",2014,"Let $A \in M_2(\mathbb R)$ be a matrix which is not a diagonal matrix . Which of the following statements are true?? a. If $tr(A)=-1$ and $detA=1$ , then $A^3=I$ . b. If $A^3=I$ , then $tr(A)=-1$ and $det(A)=1$ . c. If $A^3=I$ , then $A$ is diagonalizable over $\mathbb R$ . For (a), it is clear that $A$ will satisfy $\lambda^2+\lambda+1=0$ giving $A^2+A+I=0$ . Multiplying $A$ through out gives $A^3+A^2+A=0\implies A^3=-A^2-A=I$ For (b), the only possibilities of eigen values are $1, \omega, \omega^2$ . Now if the eigen values are only $1$ and $1$ then $A$ will satisfy $(\lambda-1)^2=0$ . We already know that $A^3=I$ . From these two facts it is not difficult to see that $A=kI$ . Hence the only possible eigen values can be $\omega, \omega^2$ . Hence (b) is true. For(c), $A$ is definitely diagonalizable over $\mathbb C$ . Is there any condition which would force a matrix to be diagonalizable over $\mathbb R$ when it is already diagonalizable over $\mathbb C$ ?","Let be a matrix which is not a diagonal matrix . Which of the following statements are true?? a. If and , then . b. If , then and . c. If , then is diagonalizable over . For (a), it is clear that will satisfy giving . Multiplying through out gives For (b), the only possibilities of eigen values are . Now if the eigen values are only and then will satisfy . We already know that . From these two facts it is not difficult to see that . Hence the only possible eigen values can be . Hence (b) is true. For(c), is definitely diagonalizable over . Is there any condition which would force a matrix to be diagonalizable over when it is already diagonalizable over ?","A \in M_2(\mathbb R) tr(A)=-1 detA=1 A^3=I A^3=I tr(A)=-1 det(A)=1 A^3=I A \mathbb R A \lambda^2+\lambda+1=0 A^2+A+I=0 A A^3+A^2+A=0\implies A^3=-A^2-A=I 1, \omega, \omega^2 1 1 A (\lambda-1)^2=0 A^3=I A=kI \omega, \omega^2 A \mathbb C \mathbb R \mathbb C","['linear-algebra', 'matrices', 'determinant', 'diagonalization', 'trace']"
25,Do $T$-invariant subspaces necessarily have a $T$-invariant complement?,Do -invariant subspaces necessarily have a -invariant complement?,T T,"Suppose $T$ is a linear operator on some vector space $V$, and suppose $U$ is a $T$-invariant subspace of $V$. Does there necessarily exist a complement (a subspace $U^c$ such that $V=U\oplus U^c$) in $V$ which is also $T$-invariant? I'm curious because I'm wondering if, given such $U$, it is always possible to decompose the linear operator $T$ into the sum of its restrictions onto $U$ and $U^c$, but I don't know if such a $T$-invariant $U^c$ exists.","Suppose $T$ is a linear operator on some vector space $V$, and suppose $U$ is a $T$-invariant subspace of $V$. Does there necessarily exist a complement (a subspace $U^c$ such that $V=U\oplus U^c$) in $V$ which is also $T$-invariant? I'm curious because I'm wondering if, given such $U$, it is always possible to decompose the linear operator $T$ into the sum of its restrictions onto $U$ and $U^c$, but I don't know if such a $T$-invariant $U^c$ exists.",,['linear-algebra']
26,Using permutation matrix to get LU-Factorization with $A=UL$,Using permutation matrix to get LU-Factorization with,A=UL,"Let $Q$ be the $n$x$n$ permutation matrix $$Q= \begin{bmatrix} 0&0&...&0&1\\ 0&0&...&1&0\\ .& \\ .&\\ .&\\ 0&0&...&0&0\\ 1&0&...&0&0\\ \end{bmatrix}$$ If $L \in \mathbb{R}$ is lower triangular, what is the structure of $QLQ$? Use this to show that one can factorize $A=UL$ where $U$ is unit upper triangular and $L$ is lower triangular. I can see that $QLQ = L^T$ And $Q^2=I$ So here's what I am doing $A=LU$ $QAQ=QLUQ = QLQ^2UQ =QLQQUQ = UL$ But now I am left with $QAQ = UL$ rather than $A=UL$ But does that matter? It seems like it does as the factorization I get would be for solving $A^Tx=b$ rather than $Ax=b$ So have I missed something, is there a way to get the factorization $A=UL$ or have I actually got it but it's the case that I am misinterpreting my answer?","Let $Q$ be the $n$x$n$ permutation matrix $$Q= \begin{bmatrix} 0&0&...&0&1\\ 0&0&...&1&0\\ .& \\ .&\\ .&\\ 0&0&...&0&0\\ 1&0&...&0&0\\ \end{bmatrix}$$ If $L \in \mathbb{R}$ is lower triangular, what is the structure of $QLQ$? Use this to show that one can factorize $A=UL$ where $U$ is unit upper triangular and $L$ is lower triangular. I can see that $QLQ = L^T$ And $Q^2=I$ So here's what I am doing $A=LU$ $QAQ=QLUQ = QLQ^2UQ =QLQQUQ = UL$ But now I am left with $QAQ = UL$ rather than $A=UL$ But does that matter? It seems like it does as the factorization I get would be for solving $A^Tx=b$ rather than $Ax=b$ So have I missed something, is there a way to get the factorization $A=UL$ or have I actually got it but it's the case that I am misinterpreting my answer?",,"['linear-algebra', 'matrices', 'numerical-methods']"
27,What is the intuition for using definiteness to compare matrices?,What is the intuition for using definiteness to compare matrices?,,"If $a$ and $b$ are two numbers on the real line, we compare $a$ and $b$ by knowing which of them comes first as we move from $-\infty$ to $\infty$ on the real line. However when $A$ and $B$ are matrices, the comparison is through definiteness. We say $A \succ B$ iff $A-B$ is positive definite. Positive definiteness of $A$ means $x^TAx>0\ \forall x$; essentially the function $f(x)=x^TAx$ takes the form of a bowl with its base at origin. How does this ""bowl"" help in comparing two matrices? What is the intuition behind using definiteness in matrices for ordering?","If $a$ and $b$ are two numbers on the real line, we compare $a$ and $b$ by knowing which of them comes first as we move from $-\infty$ to $\infty$ on the real line. However when $A$ and $B$ are matrices, the comparison is through definiteness. We say $A \succ B$ iff $A-B$ is positive definite. Positive definiteness of $A$ means $x^TAx>0\ \forall x$; essentially the function $f(x)=x^TAx$ takes the form of a bowl with its base at origin. How does this ""bowl"" help in comparing two matrices? What is the intuition behind using definiteness in matrices for ordering?",,"['linear-algebra', 'matrices']"
28,Why is QR factorization useful and important?,Why is QR factorization useful and important?,,Why do we need QR factorization? Is this used in any particular field?,Why do we need QR factorization? Is this used in any particular field?,,['linear-algebra']
29,Solving $n$-queens with determinants,Solving -queens with determinants,n,"I keep reading about a proposed method of finding solutions to the $n$-queens problem using determinants, but I can't find any specific details anywhere. Can somebody explain to me how to find solutions to $n$-queens using determinants or point me in the right direction?","I keep reading about a proposed method of finding solutions to the $n$-queens problem using determinants, but I can't find any specific details anywhere. Can somebody explain to me how to find solutions to $n$-queens using determinants or point me in the right direction?",,"['linear-algebra', 'combinatorics', 'recreational-mathematics', 'determinant']"
30,Determinant of matrix with $2$'s and the pattern $3\ 1\ 3$,Determinant of matrix with 's and the pattern,2 3\ 1\ 3,"Let for $n\ge 1, D_n$ be the determinant of the $n\times n$ matrix $A_n$ where the entries along the main diagonal (i.e. of the form $(i,i)$ for $1\leq i\leq n$ ) are all $3$ , entries of the form $(i,i+1)$ are equal to $1$ , entries of the form $(i, i+2)$ are equal to $3$ , and all other entries are $2$ . Find, with proof, a formula for the determinant of $D_n$ . For instance, the matrix $A_n$ for $n=4$ is shown below. $$A_4 =\begin{pmatrix} 3 & 1 & 3 & 2 \\ 2 & 3 & 1 & 3\\ 2 & 2 & 3 & 1\\ 2 & 2 & 2 & 3\end{pmatrix}.$$ I found the following formula after a bit of experimentation, but I wasn't able to prove it: $$D_n = 1 + 12\lfloor \frac{n}6\rfloor + \begin{cases}2,&\text{ if $n\equiv 1\bmod 6$}\\ 6,&\text{ if $n\equiv 2\bmod 6$}\\ 10, &\text{ if $n\equiv 3\bmod 6$}\\ 12, &\text{ if $n\equiv 4 $ or $5\bmod 6$}\\ 0,&\text{ otherwise}\end{cases}.$$ One can define $D_0 := 1$ . I know $D_2 = 7$ and using row operations. So far, I've come up with the following sequence of operations, but I seem to be stuck after the last step. In the description below, $R_i$ represents the $i$ th row and $C_i$ represents the ith column of $A_n$ . Suppose $n > 2$ . Perform the operation $R_i \mapsto R_i - R_n$ for $2\leq i < n.$ Then the resulting matrix has all zeroes for entries of the form $(i, j)$ for $i < j$ that are not on the last row. The entries on the last row are $n-1$ $2$ 's followed by a $3$ . Also, entries along the main diagonal are all $1$ 's and the only entry of the form $(i,i+1)$ where $2\leq i <n$ that does not equal $-1$ is $(n-1,n)$ , and that entry equals $-2$ . Entries of the form $(i,i+2)$ where $2\leq i < n-2$ are equal to one while entry $(n-2,n)$ equals $0$ if it exists. Now perform the operations $C_i \mapsto C_i - C_1$ for $2\leq i < n$ . This removes all the $2$ 's on the last row except for the first, and the entries on the top row are $3, -2,0$ , followed by $-1$ 's and ending with $2$ (the sequence stops whenever $n$ is reached, so if $n=3$ , the entries would be $3,-2,0$ ). Finally, perform the operations $R_i \mapsto R_i + R_{i+1}$ for $2\leq i\le n-2$ . After performing these operations, for $n=5$ , the following matrix is obtained: $$\begin{pmatrix} 3& -2 & 0 & -1 & 2\\ 0 & 1 & 0 & 0 & 2\\ 0 & 0 & 1 & 0  & -2\\ 0 & 0 & 0 & 1 & -2\\ 2 & 0 & 0 & 0 & 3 \end{pmatrix}.$$ I can't seem to ""zero out"" the $2$ in the bottom left corner, despite trying various row and column operations. I think I should split the proof into cases based on the remainder when $n$ is divided by $6$ , which is suggested by the formula at the beginning. I tried considering other approaches such as finding eigenvalues and deriving a recurrence relation for $D_n$ , but those approaches didn't seem to help much.","Let for be the determinant of the matrix where the entries along the main diagonal (i.e. of the form for ) are all , entries of the form are equal to , entries of the form are equal to , and all other entries are . Find, with proof, a formula for the determinant of . For instance, the matrix for is shown below. I found the following formula after a bit of experimentation, but I wasn't able to prove it: One can define . I know and using row operations. So far, I've come up with the following sequence of operations, but I seem to be stuck after the last step. In the description below, represents the th row and represents the ith column of . Suppose . Perform the operation for Then the resulting matrix has all zeroes for entries of the form for that are not on the last row. The entries on the last row are 's followed by a . Also, entries along the main diagonal are all 's and the only entry of the form where that does not equal is , and that entry equals . Entries of the form where are equal to one while entry equals if it exists. Now perform the operations for . This removes all the 's on the last row except for the first, and the entries on the top row are , followed by 's and ending with (the sequence stops whenever is reached, so if , the entries would be ). Finally, perform the operations for . After performing these operations, for , the following matrix is obtained: I can't seem to ""zero out"" the in the bottom left corner, despite trying various row and column operations. I think I should split the proof into cases based on the remainder when is divided by , which is suggested by the formula at the beginning. I tried considering other approaches such as finding eigenvalues and deriving a recurrence relation for , but those approaches didn't seem to help much.","n\ge 1, D_n n\times n A_n (i,i) 1\leq i\leq n 3 (i,i+1) 1 (i, i+2) 3 2 D_n A_n n=4 A_4 =\begin{pmatrix} 3 & 1 & 3 & 2 \\
2 & 3 & 1 & 3\\
2 & 2 & 3 & 1\\
2 & 2 & 2 & 3\end{pmatrix}. D_n = 1 + 12\lfloor \frac{n}6\rfloor + \begin{cases}2,&\text{ if n\equiv 1\bmod 6}\\
6,&\text{ if n\equiv 2\bmod 6}\\
10, &\text{ if n\equiv 3\bmod 6}\\
12, &\text{ if n\equiv 4  or 5\bmod 6}\\
0,&\text{ otherwise}\end{cases}. D_0 := 1 D_2 = 7 R_i i C_i A_n n > 2 R_i \mapsto R_i - R_n 2\leq i < n. (i, j) i < j n-1 2 3 1 (i,i+1) 2\leq i <n -1 (n-1,n) -2 (i,i+2) 2\leq i < n-2 (n-2,n) 0 C_i \mapsto C_i - C_1 2\leq i < n 2 3, -2,0 -1 2 n n=3 3,-2,0 R_i \mapsto R_i + R_{i+1} 2\leq i\le n-2 n=5 \begin{pmatrix}
3& -2 & 0 & -1 & 2\\
0 & 1 & 0 & 0 & 2\\
0 & 0 & 1 & 0  & -2\\
0 & 0 & 0 & 1 & -2\\
2 & 0 & 0 & 0 & 3
\end{pmatrix}. 2 n 6 D_n","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
31,Is the matrix $A^4+A^3-3A^2-3A$ invertible?,Is the matrix  invertible?,A^4+A^3-3A^2-3A,"Let $A$ be a real $5 \times 5$ matrix satisfying $A^3-4A^2+5A-2I=O$ . Is the matrix $A^4+A^3-3A^2-3A$ invertible? Consider the polynomial $t^3-4t^2+5t-2=(t-1)^2(t-2)$ . The minimal polynomial of $A$ divides $(t-1)^2(t-2)$ . Let $p(t)$ be the minimal polynomial of $A$ . If $p(x)=t-1$ or $t-2$ , then $A$ is a scalar multiple of the identity and $A^4+A^3-3A^2-3A=-4I \text{ or }6I$ , so $A^4+A^3-3A^2-3A$ is invertible. If $p(t)=(t-1)^2(t-2)$ or $(t-1)(t-2)$ , then $A$ is similar to a upper triangular matrix $J$ with diagonal entries $1,2$ . Then $A^4+A^3-3A^2-3A=P(J^4+J^3-3J^2-3J)P^{-1}$ and $\det(A^4+A^3-3A^2-3A)=\det(J^4+J^3-3J^2-3J)$ . Since $J^4+J^3-3J^2-3J$ is upper triangular and the diagonal entries do not vanish, $\det(J^4+J^3-3J^2-3J) \ne 0$ . If $p(t)=(t-1)^2$ , then by the same reasoning, $A^4+A^3-3A^2-3A$ is invertible. Is there more efficient way to solve this kind of problems or I have to discuss any possible situations every time?","Let be a real matrix satisfying . Is the matrix invertible? Consider the polynomial . The minimal polynomial of divides . Let be the minimal polynomial of . If or , then is a scalar multiple of the identity and , so is invertible. If or , then is similar to a upper triangular matrix with diagonal entries . Then and . Since is upper triangular and the diagonal entries do not vanish, . If , then by the same reasoning, is invertible. Is there more efficient way to solve this kind of problems or I have to discuss any possible situations every time?","A 5 \times 5 A^3-4A^2+5A-2I=O A^4+A^3-3A^2-3A t^3-4t^2+5t-2=(t-1)^2(t-2) A (t-1)^2(t-2) p(t) A p(x)=t-1 t-2 A A^4+A^3-3A^2-3A=-4I \text{ or }6I A^4+A^3-3A^2-3A p(t)=(t-1)^2(t-2) (t-1)(t-2) A J 1,2 A^4+A^3-3A^2-3A=P(J^4+J^3-3J^2-3J)P^{-1} \det(A^4+A^3-3A^2-3A)=\det(J^4+J^3-3J^2-3J) J^4+J^3-3J^2-3J \det(J^4+J^3-3J^2-3J) \ne 0 p(t)=(t-1)^2 A^4+A^3-3A^2-3A","['linear-algebra', 'solution-verification']"
32,If $A^k$ commutes with $B$ then $A$ commutes with $B$.,If  commutes with  then  commutes with .,A^k B A B,Let $A$ and $B$ are two $n \times n$ Complex matrices. Assume that $(A-I)^n=0$ and $A^kB=BA^k$ for some $k \in \mathbb{N}$ . Then I want to prove that $AB= BA$ . Clearly $1$ is the only eigen value of $A$ and also $A^k$ and $B$ are simultaneously triangulable. But how do I get down to $A$ to commute with $B$ . Any help will be appreciated. Thanks.,Let and are two Complex matrices. Assume that and for some . Then I want to prove that . Clearly is the only eigen value of and also and are simultaneously triangulable. But how do I get down to to commute with . Any help will be appreciated. Thanks.,A B n \times n (A-I)^n=0 A^kB=BA^k k \in \mathbb{N} AB= BA 1 A A^k B A B,"['linear-algebra', 'abstract-algebra']"
33,Remove any number and the remaining numbers can be partitioned into two subsets of equal sum; prove all numbers are equal. [duplicate],Remove any number and the remaining numbers can be partitioned into two subsets of equal sum; prove all numbers are equal. [duplicate],,"This question already has answers here : After removing any part the rest can be split evenly. Consequences? (3 answers) Closed 5 years ago . Supposed I have a list of $n$ real numbers, where $n$ is odd. The list is constructed such that I can remove any arbitrary number from the list, and the remaining numbers can be partitioned into two equal-sized subsets with equal sums. Prove that all numbers in the list are equal. This should be somehow related to linear algebra. A way I could think of to interpret this is that the list is essentially a $1 \times n$ row, and there exist $n$ $n \times 1$ vectors with one zero in some entry and $1$'s and $-1$'s in other entries with the entries summing to $0$, such that the product of the row and the column vector is $[0]$. In other words, the entries/$1 \times 1$ columns in the row are linearly dependent once we remove any arbitrary entry/column. I'm not sure how this is/could be related to the proof though. Thanks in advance!","This question already has answers here : After removing any part the rest can be split evenly. Consequences? (3 answers) Closed 5 years ago . Supposed I have a list of $n$ real numbers, where $n$ is odd. The list is constructed such that I can remove any arbitrary number from the list, and the remaining numbers can be partitioned into two equal-sized subsets with equal sums. Prove that all numbers in the list are equal. This should be somehow related to linear algebra. A way I could think of to interpret this is that the list is essentially a $1 \times n$ row, and there exist $n$ $n \times 1$ vectors with one zero in some entry and $1$'s and $-1$'s in other entries with the entries summing to $0$, such that the product of the row and the column vector is $[0]$. In other words, the entries/$1 \times 1$ columns in the row are linearly dependent once we remove any arbitrary entry/column. I'm not sure how this is/could be related to the proof though. Thanks in advance!",,"['linear-algebra', 'number-theory', 'real-numbers']"
34,How to find the volume of a tetrahedron?,How to find the volume of a tetrahedron?,,"For this question, how do we find the volume of a tetrahedron in $\Bbb R^4$ if we have a 4 by 3 matrix? The cross product and determinant only work for square matrices. I'm not sure what to do after we subtract the points from the emanating point. Here is what I have so far. Can anyone please help me out? Find the volume of the tetrahedron in $\Bbb R^4$ with vertices $(1,0,0,1),(-1,2,0,1), (3,0,1,1), and (-1,4,0,1)$. $(-1,2,0,1)-(1,0,0,1) = [-2,2,0,0]$ $(3,0,1,1)-(1,0,0,1) = [2,0,1,0]$ $(-1,4,0,1)-(1,0,0,1) = [-2,4,0,0]$","For this question, how do we find the volume of a tetrahedron in $\Bbb R^4$ if we have a 4 by 3 matrix? The cross product and determinant only work for square matrices. I'm not sure what to do after we subtract the points from the emanating point. Here is what I have so far. Can anyone please help me out? Find the volume of the tetrahedron in $\Bbb R^4$ with vertices $(1,0,0,1),(-1,2,0,1), (3,0,1,1), and (-1,4,0,1)$. $(-1,2,0,1)-(1,0,0,1) = [-2,2,0,0]$ $(3,0,1,1)-(1,0,0,1) = [2,0,1,0]$ $(-1,4,0,1)-(1,0,0,1) = [-2,4,0,0]$",,"['linear-algebra', 'determinant']"
35,Is a nonsingular matrix not the same as an invertible matrix?,Is a nonsingular matrix not the same as an invertible matrix?,,"Over an arbitrary ring $R$, a matrix $A$ is said to be invertible if it has an inverse with entries in the same ring. This happens iff $\det A$ is a unit of $R$. I've always thought that the terms ""invertible"" and ""nonsingular"" are synonymous. But I think the following problem (from Artin) suggests that they are not (at least over an arbitrary ring): Let $\varphi: \mathbb Z^k\to \mathbb Z^k$ be a homomorphism given by multiplication by an integer matrix $A$. Show that the image of $\varphi$ is of finite index if and only if $A$ is nonsingular and that if so, then the index is equal to $|\det A|.$ If I understand correctly, in this context ""nonsingular"" means $\det A\ne 0$. And this is not the same as invertible since if $A$ is invertible, then $\varphi$ is bijective, and the image is the whole $\mathbb Z^k$. So, am I correct in saying that a matrix $A$ over a ring $R$ is by definition nonsingular if $\det A\ne 0$? And being nonsingular does not imply being invertible (unless the underlying ring is a field)?","Over an arbitrary ring $R$, a matrix $A$ is said to be invertible if it has an inverse with entries in the same ring. This happens iff $\det A$ is a unit of $R$. I've always thought that the terms ""invertible"" and ""nonsingular"" are synonymous. But I think the following problem (from Artin) suggests that they are not (at least over an arbitrary ring): Let $\varphi: \mathbb Z^k\to \mathbb Z^k$ be a homomorphism given by multiplication by an integer matrix $A$. Show that the image of $\varphi$ is of finite index if and only if $A$ is nonsingular and that if so, then the index is equal to $|\det A|.$ If I understand correctly, in this context ""nonsingular"" means $\det A\ne 0$. And this is not the same as invertible since if $A$ is invertible, then $\varphi$ is bijective, and the image is the whole $\mathbb Z^k$. So, am I correct in saying that a matrix $A$ over a ring $R$ is by definition nonsingular if $\det A\ne 0$? And being nonsingular does not imply being invertible (unless the underlying ring is a field)?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'determinant']"
36,"If $p+q+r=0$, find the value of the determinant","If , find the value of the determinant",p+q+r=0,"If $p+q+r=0$ , prove that the value of the determinant $$ \Delta= \begin{vmatrix} pa & qb &rc \\  qc & ra &pb\\  rb& pc & qa  \\  \end{vmatrix} =-pqr \begin{vmatrix} a & b &c \\  b & c &a\\  c& a & b  \\  \end{vmatrix}$$ My Try:Since $p+q+r=0$ we have $$a(p+q+r)+b(p+q+r)+c(p+q+r)=0$$ $\implies$ $$(ap+qc+rb)+(qb+ra+pc)+(rc+pb+qa)=0 \tag{1}$$ Now applying $C_1 \to C_1+C_2+C_3$ and then applying $R_1 \to R_1+R_2+R_3$ for $\Delta$ we get $$\Delta=  \begin{vmatrix} 0 & qb &rc \\  qc+ra+pb & ra &pb\\  rb+pc+qa& pc & qa  \\  \end{vmatrix}$$ Any clue here?","If , prove that the value of the determinant My Try:Since we have Now applying and then applying for we get Any clue here?","p+q+r=0  \Delta= \begin{vmatrix}
pa & qb &rc \\ 
qc & ra &pb\\ 
rb& pc & qa  \\ 
\end{vmatrix} =-pqr \begin{vmatrix}
a & b &c \\ 
b & c &a\\ 
c& a & b  \\ 
\end{vmatrix} p+q+r=0 a(p+q+r)+b(p+q+r)+c(p+q+r)=0 \implies (ap+qc+rb)+(qb+ra+pc)+(rc+pb+qa)=0 \tag{1} C_1 \to C_1+C_2+C_3 R_1 \to R_1+R_2+R_3 \Delta \Delta=  \begin{vmatrix}
0 & qb &rc \\ 
qc+ra+pb & ra &pb\\ 
rb+pc+qa& pc & qa  \\ 
\end{vmatrix}","['linear-algebra', 'matrices', 'determinant']"
37,Cholesky decomposition when deleting one row and one and column.,Cholesky decomposition when deleting one row and one and column.,,"I've thought about this problem for days but could not find a good answer. Given Cholesky decomposition of a symmetric positive semidefinite matrix $A = LL^T$. Now, suppose that we delete the $i$-th row and the $i$-th column of $A$ to obtain $A'$ ($A'$ is also a symmetric positive semidefinite matrix), and the Cholesky decomposition of this new matrix is $A' = L'(L')^T$. Is there any efficient way to obtain $L'$ from $L$? Note : In case we delete the last row and the last column of $A$, the problem becomes simple, we just delete the last row and last column of $L$ to obtain $L'$. Other cases, for me, are not trivial. Thank you so much.","I've thought about this problem for days but could not find a good answer. Given Cholesky decomposition of a symmetric positive semidefinite matrix $A = LL^T$. Now, suppose that we delete the $i$-th row and the $i$-th column of $A$ to obtain $A'$ ($A'$ is also a symmetric positive semidefinite matrix), and the Cholesky decomposition of this new matrix is $A' = L'(L')^T$. Is there any efficient way to obtain $L'$ from $L$? Note : In case we delete the last row and the last column of $A$, the problem becomes simple, we just delete the last row and last column of $L$ to obtain $L'$. Other cases, for me, are not trivial. Thank you so much.",,['linear-algebra']
38,Why is that an $n \times n$ matrix have $n$ eigenvalues?,Why is that an  matrix have  eigenvalues?,n \times n n,"This is a silly question, but I can't find a good argument why an $n \times n$ matrix will have $n$ eigenvalues (not necessarily distinct). In my linear algebra class the professor takes for granted that the number of eigenvalues is equal to the dimension of the matrix. For example, given eigs($A$) = $\{1, 2, 3, 4, 5\}$, he will write a $5 \times 5$ dimensional matrix for $A$ How can we show this trivial fact?","This is a silly question, but I can't find a good argument why an $n \times n$ matrix will have $n$ eigenvalues (not necessarily distinct). In my linear algebra class the professor takes for granted that the number of eigenvalues is equal to the dimension of the matrix. For example, given eigs($A$) = $\{1, 2, 3, 4, 5\}$, he will write a $5 \times 5$ dimensional matrix for $A$ How can we show this trivial fact?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
39,Prove that $ND = DN$ where $D$ is a diagonalizable and $N$ is a nilpotent matrix.,Prove that  where  is a diagonalizable and  is a nilpotent matrix.,ND = DN D N,"Let $A$ be an $n \times n$ complex matrix. Prove that there exist a diagonalizable matrix $D$ and a nilpotent matrix $N$ such that a. A = D + N b. DN = ND and show that these matrices are uniquely determined. I think I've solved the part a but don't have an idea to continue. Here is what I've tried: Let $D = \begin{bmatrix} a_{11} & 0 \\ * & a_{nn} \\ \end{bmatrix}$ and $N = \begin{bmatrix} 0 & * \\ 0 & 0 \end{bmatrix}$. Since D is a lower triangular matrix, its determinant is equal to product of its diagonal entries and hence its characteristic polynomial is $(x-a_{11})...(x-a_{nn}).$ Then D is diagonalizable. Similarly, characteristic polynomial of $N$ is $x^n$ then by Cayley-Hamilton $N^n = 0$. Update: My assumption for D to be diagonalizable was wrong, eigenvalues need not to be distinct. Update after answers : Thank you all for your help. It was just a question asked in the end of the chapter ""Canonical Forms"". So I just know Jordan form, Rational form, etc. I don't know anything about the Lie Algebra, Semi simple matrices, Representation theory, Perfect field mentioned in the answers. Honestly, answers didn't help me to understand the solution but they seem useful so maybe they help other people. Thanks.","Let $A$ be an $n \times n$ complex matrix. Prove that there exist a diagonalizable matrix $D$ and a nilpotent matrix $N$ such that a. A = D + N b. DN = ND and show that these matrices are uniquely determined. I think I've solved the part a but don't have an idea to continue. Here is what I've tried: Let $D = \begin{bmatrix} a_{11} & 0 \\ * & a_{nn} \\ \end{bmatrix}$ and $N = \begin{bmatrix} 0 & * \\ 0 & 0 \end{bmatrix}$. Since D is a lower triangular matrix, its determinant is equal to product of its diagonal entries and hence its characteristic polynomial is $(x-a_{11})...(x-a_{nn}).$ Then D is diagonalizable. Similarly, characteristic polynomial of $N$ is $x^n$ then by Cayley-Hamilton $N^n = 0$. Update: My assumption for D to be diagonalizable was wrong, eigenvalues need not to be distinct. Update after answers : Thank you all for your help. It was just a question asked in the end of the chapter ""Canonical Forms"". So I just know Jordan form, Rational form, etc. I don't know anything about the Lie Algebra, Semi simple matrices, Representation theory, Perfect field mentioned in the answers. Honestly, answers didn't help me to understand the solution but they seem useful so maybe they help other people. Thanks.",,"['linear-algebra', 'matrices', 'diagonalization', 'jordan-normal-form']"
40,A Representation Theory Problem in Putnam Competition,A Representation Theory Problem in Putnam Competition,,"The following was the B6 problem of 1985 Putnam Competition: Suppose $G$ is a finite group (under matrix multiplication) of real $n\times n$ matrices $\{M_i\}, 1\leq i\leq r$. Suppose that $$\sum_{i=1}^r tr (M_i)=0$$, prove that $$\sum_{i=1}^rM_i=0.$$ Here is an official proof from the committee which I didn't understand: Lemma: Let $G$ be a finite group of order $r$. Let $\rho: G\rightarrow GL(V)$ be a representation of $G$ on some finite dimensional vector space $V$. Then $$\sum_{g\in G}tr \rho_g$$ is a non-negative integer divisible by $r$, and is zero iff $$\sum_{g\in G}\rho_g=0$$. Proof: Let $\chi_1,\cdots, \chi_s$ be the irreducible characters of $G$ and $\chi= \sum_{i=1}^s a_i\chi_i$ and $\psi=\sum_{i=1}^sb_i\chi_i$ be arbitrary characters. Then by the orthogonality relations of characters, we have $$\frac{1}{|G|}\sum_{g\in G}\chi(g)\overline{\psi (g)}=\sum_{i=1}^sa_ib_i$$. Applying this to the character of $\rho$ and the trivial character $\mathbb{1}$ shows that $\frac{1}{|G|} \sum_{g\in G}tr \rho_g$ equals the multiplicity of $\mathbb{1}$ in $\rho$, which is a non-negative integer. Now suppose that the matrix $S=\sum_{g\in G}\rho_g$ is non-zero. Choose $v\in V$ with $Sv\not=0$. The relation $\rho_hS=S$ shows that $Sv$ is fixed by $\rho_h$ for all $h\in G$. In other words, $Sv$ spans a trivial subrepresentation of $\rho$, so the non-negative integer of the previous paragraph is positive.  QED We now return to the problem at hand. ""Unfortunately the $M_i$ do not necessarily define a representation of $G$, since the $M_i$ need not be invertible."" Instead we need to apply the lemma to the action of $G$ on $\mathbb{C}^n/K$ for some subspace $K$ ... I do not understand wthe sentence in """". Isn't the set of $M_i$'s form  a group under multiplication? why they need not to be invertible? The above proof is copied from Kedlaya, Poonen and Vakil's Putnam competition 1985-2000. Thanks for helping","The following was the B6 problem of 1985 Putnam Competition: Suppose $G$ is a finite group (under matrix multiplication) of real $n\times n$ matrices $\{M_i\}, 1\leq i\leq r$. Suppose that $$\sum_{i=1}^r tr (M_i)=0$$, prove that $$\sum_{i=1}^rM_i=0.$$ Here is an official proof from the committee which I didn't understand: Lemma: Let $G$ be a finite group of order $r$. Let $\rho: G\rightarrow GL(V)$ be a representation of $G$ on some finite dimensional vector space $V$. Then $$\sum_{g\in G}tr \rho_g$$ is a non-negative integer divisible by $r$, and is zero iff $$\sum_{g\in G}\rho_g=0$$. Proof: Let $\chi_1,\cdots, \chi_s$ be the irreducible characters of $G$ and $\chi= \sum_{i=1}^s a_i\chi_i$ and $\psi=\sum_{i=1}^sb_i\chi_i$ be arbitrary characters. Then by the orthogonality relations of characters, we have $$\frac{1}{|G|}\sum_{g\in G}\chi(g)\overline{\psi (g)}=\sum_{i=1}^sa_ib_i$$. Applying this to the character of $\rho$ and the trivial character $\mathbb{1}$ shows that $\frac{1}{|G|} \sum_{g\in G}tr \rho_g$ equals the multiplicity of $\mathbb{1}$ in $\rho$, which is a non-negative integer. Now suppose that the matrix $S=\sum_{g\in G}\rho_g$ is non-zero. Choose $v\in V$ with $Sv\not=0$. The relation $\rho_hS=S$ shows that $Sv$ is fixed by $\rho_h$ for all $h\in G$. In other words, $Sv$ spans a trivial subrepresentation of $\rho$, so the non-negative integer of the previous paragraph is positive.  QED We now return to the problem at hand. ""Unfortunately the $M_i$ do not necessarily define a representation of $G$, since the $M_i$ need not be invertible."" Instead we need to apply the lemma to the action of $G$ on $\mathbb{C}^n/K$ for some subspace $K$ ... I do not understand wthe sentence in """". Isn't the set of $M_i$'s form  a group under multiplication? why they need not to be invertible? The above proof is copied from Kedlaya, Poonen and Vakil's Putnam competition 1985-2000. Thanks for helping",,"['linear-algebra', 'representation-theory', 'contest-math']"
41,$\det(I+\epsilon V)=1+\operatorname{trace}(V)\epsilon+O(\epsilon^2)$,,\det(I+\epsilon V)=1+\operatorname{trace}(V)\epsilon+O(\epsilon^2),"How to show that $$\det(I+\epsilon V)=1+\operatorname{trace}(V)\epsilon+O(\epsilon^2)$$ for any $n\times n$ real matrix $V$? This is used a lot in the theory Lie groups, but I never saw a proof of it.","How to show that $$\det(I+\epsilon V)=1+\operatorname{trace}(V)\epsilon+O(\epsilon^2)$$ for any $n\times n$ real matrix $V$? This is used a lot in the theory Lie groups, but I never saw a proof of it.",,['linear-algebra']
42,Diagonalizing Quadratic Forms. Linear Algebra,Diagonalizing Quadratic Forms. Linear Algebra,,"I have a question that reads: Diagonalize the quadratic form $A(x,y) = 3x^2 -12xy + 7y^2$ by completing the square. What is diagonalization?  Is that when I should find the eigenvector matrix, say, $S$, find it's inverse $S^{-1}$, and then multiply it by some matrix in order to obtain a diagonal matrix that is easy to find powers of? I think that's what it is. Problem is, I guess I don't understand what it means by ""completing the square"".  I mean I know what completing the square is in, say, high school algebra, but I don't understand what it means in this context. Any clarification is greatly appreciated. Thanks!","I have a question that reads: Diagonalize the quadratic form $A(x,y) = 3x^2 -12xy + 7y^2$ by completing the square. What is diagonalization?  Is that when I should find the eigenvector matrix, say, $S$, find it's inverse $S^{-1}$, and then multiply it by some matrix in order to obtain a diagonal matrix that is easy to find powers of? I think that's what it is. Problem is, I guess I don't understand what it means by ""completing the square"".  I mean I know what completing the square is in, say, high school algebra, but I don't understand what it means in this context. Any clarification is greatly appreciated. Thanks!",,"['linear-algebra', 'quadratic-forms', 'diagonalization']"
43,Lower and upper bound for the largest eigenvalue,Lower and upper bound for the largest eigenvalue,,"We will call a matrix positive matrix if all elements in the matrix are positive, and we will denote the largest eigenvalue with $\lambda_{\max}$ , what is exist because of the Perron–Frobenius theorem . Theorem. Let $A$ be a positive square matrix. Then the minimal row sum is a lower bound and the maximal row sum is an upper bound of $\lambda_{\max}$ . My questions. Is there a name for this theorem and can anybody say books or papers what refer to it? How to prove it?","We will call a matrix positive matrix if all elements in the matrix are positive, and we will denote the largest eigenvalue with , what is exist because of the Perron–Frobenius theorem . Theorem. Let be a positive square matrix. Then the minimal row sum is a lower bound and the maximal row sum is an upper bound of . My questions. Is there a name for this theorem and can anybody say books or papers what refer to it? How to prove it?",\lambda_{\max} A \lambda_{\max},"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'positive-matrices']"
44,Why is the special linear group generated by elementary matrices that add a multiple of row $j$ to row $i$? [duplicate],Why is the special linear group generated by elementary matrices that add a multiple of row  to row ? [duplicate],j i,"This question already has an answer here : Transvection matrices generate $ \operatorname{SL}_n(\mathbb{R}) $ (1 answer) Closed 5 years ago . The general linear group is generated by elementary matrices that add a multiple of row $j$ to row $i$ and elementary matrices that multiply row $i$ by a scalar. This is because you can write an invertible matrix as the product of elementary matrices, and a row swap matrix can be written as a product of the other two types of elementary matrix. So far, I've worked out that all matrices that are products of this type of elementary matrix are in the special linear group, but I don't know how to proceed.","This question already has an answer here : Transvection matrices generate $ \operatorname{SL}_n(\mathbb{R}) $ (1 answer) Closed 5 years ago . The general linear group is generated by elementary matrices that add a multiple of row $j$ to row $i$ and elementary matrices that multiply row $i$ by a scalar. This is because you can write an invertible matrix as the product of elementary matrices, and a row swap matrix can be written as a product of the other two types of elementary matrix. So far, I've worked out that all matrices that are products of this type of elementary matrix are in the special linear group, but I don't know how to proceed.",,"['linear-algebra', 'group-theory']"
45,Product of complex Hermitian positive semidefinite matrices with trace zero,Product of complex Hermitian positive semidefinite matrices with trace zero,,"Let $A$ and $B$ are both $n\times n$ complex Hermitian positive semidefinite matrices, then whether $\mathrm{trace}(AB)=0$ implies $AB=0$? When $A$ and $B$ are real  Hermitian positive semidefinite matrices, this is indeed true, is it true for the complex case?","Let $A$ and $B$ are both $n\times n$ complex Hermitian positive semidefinite matrices, then whether $\mathrm{trace}(AB)=0$ implies $AB=0$? When $A$ and $B$ are real  Hermitian positive semidefinite matrices, this is indeed true, is it true for the complex case?",,"['linear-algebra', 'matrices']"
46,Duality of a finitely generated projective modules,Duality of a finitely generated projective modules,,"Let $M$ and $N$ be a finitely generated projective module over a ring $R$. Suppose we have a non degenerate bilinear pairing $\langle \ \cdot \ ,\ \cdot\ \rangle: M \times N \to R$. I want to show $M$ is isomorphic to the dual $N^*$ of $N$. The injectivity of $M$ into $N^*$ follows from the non degeneracy of the pairing by definiting a map $x \mapsto \langle x, \cdot\rangle$. What I cannot prove is surjectivity. If I impose a condition that $R$ is an injective module then I think surjectivity also follows. But I want to prove it without any condition on $R$ Any help is appriciated.","Let $M$ and $N$ be a finitely generated projective module over a ring $R$. Suppose we have a non degenerate bilinear pairing $\langle \ \cdot \ ,\ \cdot\ \rangle: M \times N \to R$. I want to show $M$ is isomorphic to the dual $N^*$ of $N$. The injectivity of $M$ into $N^*$ follows from the non degeneracy of the pairing by definiting a map $x \mapsto \langle x, \cdot\rangle$. What I cannot prove is surjectivity. If I impose a condition that $R$ is an injective module then I think surjectivity also follows. But I want to prove it without any condition on $R$ Any help is appriciated.",,['linear-algebra']
47,Reference for densities and pseudoforms and non-tensorial representations of $\operatorname{GL}(n)$ and associated vector bundles,Reference for densities and pseudoforms and non-tensorial representations of  and associated vector bundles,\operatorname{GL}(n),"I'm looking for a reference that will set me straight on a few things. It started out with densities. In John Lee's book, ""Introduction to Smooth Manifolds"", densities on vector spaces are functions that satisfy $$\mu(Tv_1,\dotsc,Tv_n)=|\det\; T\,|\;\mu(v_1,\dotsc,v_n)$$ for $T:V\to V$. They're like volume forms composed with absolute values. Densities are defined on manifolds and transform the obvious way under changes of coordinates. They're nice because you can integrate them in the absence of an orientation. And they're natural because the structure group is an irreducible representation of $\operatorname{GL}(n)$. This representation is a little bit unfamiliar in that it is not constructed out of tensor products of the fundamental rep and its dual. One of the reasons that symmetric and antisymmetric tensors on a manifold are important is because their structure groups belong to irreducible representations of $\operatorname{GL}(n)$. Googling for more information on the subject let me to an old USENET thread where I learned among other things that things that pick up a sign under negative determinant change of basis are called pseudoforms, not densities. Any (symmetric, antisymmetric, form, vector, etc.) can be turned ""pseudo"" by tensoring it with the determinant bundle. Densities are things with an exponent in their change of basis formulas, and these also form an irrep of $\operatorname{GL}(n)$. Jet bundles are another example of a bundle whose structure group is not a tensor product I'm left with some questions What are densities? Are they what Lee says, or what the USENET guys say? Are there differing conventions? If you use the word ""density"" for Lee's definition, then what do you call the thing with the exponent? Are there other nontensorial reps than density and pseudoforms? What about jet bundles? What about a rep of the universal cover of $\operatorname{GL}(n)$? I know that for $\operatorname{SO}(n)$ this gives us spinor reps. What about $\operatorname{GL}(n)$? And what about $\operatorname{SO}(n)$ and subgroups of $\operatorname{GL}(n)$? I would like a good reference text that covers this material in depth. Especially as it relates to differential topology on smooth manifolds. I did find some a book on the representation theory of Lie groups which proved the irreps of $\operatorname{GL}(n,\mathbb{C})$, but it was difficult to understand.","I'm looking for a reference that will set me straight on a few things. It started out with densities. In John Lee's book, ""Introduction to Smooth Manifolds"", densities on vector spaces are functions that satisfy $$\mu(Tv_1,\dotsc,Tv_n)=|\det\; T\,|\;\mu(v_1,\dotsc,v_n)$$ for $T:V\to V$. They're like volume forms composed with absolute values. Densities are defined on manifolds and transform the obvious way under changes of coordinates. They're nice because you can integrate them in the absence of an orientation. And they're natural because the structure group is an irreducible representation of $\operatorname{GL}(n)$. This representation is a little bit unfamiliar in that it is not constructed out of tensor products of the fundamental rep and its dual. One of the reasons that symmetric and antisymmetric tensors on a manifold are important is because their structure groups belong to irreducible representations of $\operatorname{GL}(n)$. Googling for more information on the subject let me to an old USENET thread where I learned among other things that things that pick up a sign under negative determinant change of basis are called pseudoforms, not densities. Any (symmetric, antisymmetric, form, vector, etc.) can be turned ""pseudo"" by tensoring it with the determinant bundle. Densities are things with an exponent in their change of basis formulas, and these also form an irrep of $\operatorname{GL}(n)$. Jet bundles are another example of a bundle whose structure group is not a tensor product I'm left with some questions What are densities? Are they what Lee says, or what the USENET guys say? Are there differing conventions? If you use the word ""density"" for Lee's definition, then what do you call the thing with the exponent? Are there other nontensorial reps than density and pseudoforms? What about jet bundles? What about a rep of the universal cover of $\operatorname{GL}(n)$? I know that for $\operatorname{SO}(n)$ this gives us spinor reps. What about $\operatorname{GL}(n)$? And what about $\operatorname{SO}(n)$ and subgroups of $\operatorname{GL}(n)$? I would like a good reference text that covers this material in depth. Especially as it relates to differential topology on smooth manifolds. I did find some a book on the representation theory of Lie groups which proved the irreps of $\operatorname{GL}(n,\mathbb{C})$, but it was difficult to understand.",,"['linear-algebra', 'reference-request', 'differential-geometry', 'representation-theory', 'tensors']"
48,Practical applications of eigenvalues/eigenvectors in computer science,Practical applications of eigenvalues/eigenvectors in computer science,,"What are the most important/popular applications of eigenvalues and eigenvectors in practical terms, in fields such as computer science and computer graphics? Wikipedia does mention some but doesn't really elaborate much.","What are the most important/popular applications of eigenvalues and eigenvectors in practical terms, in fields such as computer science and computer graphics? Wikipedia does mention some but doesn't really elaborate much.",,"['linear-algebra', 'computer-science', 'eigenvalues-eigenvectors', 'applications']"
49,Equivalent Definitions of Positive Definite Matrix,Equivalent Definitions of Positive Definite Matrix,,"As Wikipedia tells us, a real $n \times n$ symmetric matrix $G = [g_{ij}]$ is positive definite if $v^TGv >0$ for all $0 \neq v \in \mathbb{R}^n$. By a well-known theorem of linear algebra it can be shown that $G$ is positive definite if and only if the eigenvalues of $G$ are positive. Therefore, this gives us two distinct ways to say what it means for a matrix to be positive definite. In Amann and Escher's Analysis II, exercise 7.1.8 seems to provide yet another way recognize a positive definite matrix. In this exercise, $G$ is defined to be positive definite if there exists a positive number $\gamma$ such that $$ \sum\limits_{i,j = 1}^n g_{ij}v^iv^j \geq \gamma |v|^2 $$ I have not before seen this characterization of a positive definite matrix and I have not been successful at demonstrating that this characterization is equivalent to the other two characterizations listed above. Can anyone provide a hint how one might proceed to demonstrate this apparent equivalence or suggest a reference that discusses it?","As Wikipedia tells us, a real $n \times n$ symmetric matrix $G = [g_{ij}]$ is positive definite if $v^TGv >0$ for all $0 \neq v \in \mathbb{R}^n$. By a well-known theorem of linear algebra it can be shown that $G$ is positive definite if and only if the eigenvalues of $G$ are positive. Therefore, this gives us two distinct ways to say what it means for a matrix to be positive definite. In Amann and Escher's Analysis II, exercise 7.1.8 seems to provide yet another way recognize a positive definite matrix. In this exercise, $G$ is defined to be positive definite if there exists a positive number $\gamma$ such that $$ \sum\limits_{i,j = 1}^n g_{ij}v^iv^j \geq \gamma |v|^2 $$ I have not before seen this characterization of a positive definite matrix and I have not been successful at demonstrating that this characterization is equivalent to the other two characterizations listed above. Can anyone provide a hint how one might proceed to demonstrate this apparent equivalence or suggest a reference that discusses it?",,"['linear-algebra', 'reference-request']"
50,The calculation of $\dim(U + V + W)$,The calculation of,\dim(U + V + W),Is there a (valid) formula for $\dim(U + V + W)$? I know from MO that $$\begin{align*} \dim(U + V + W) &= \dim(U) + \dim(V) + \dim(W)\\ &\qquad\mathop{-} \dim(U \cap V) - \dim(U \cap W) - \dim(V \cap W)\\&\qquad \mathop{+} \dim(U \cap V \cap  W) \end{align*}$$  is wrong. Can we relate $\dim(U + V + W)$ with the cardinality of some of their quotient spaces? (sorry if this is a dummy question but I'm not any familiar with quotient spaces).,Is there a (valid) formula for $\dim(U + V + W)$? I know from MO that $$\begin{align*} \dim(U + V + W) &= \dim(U) + \dim(V) + \dim(W)\\ &\qquad\mathop{-} \dim(U \cap V) - \dim(U \cap W) - \dim(V \cap W)\\&\qquad \mathop{+} \dim(U \cap V \cap  W) \end{align*}$$  is wrong. Can we relate $\dim(U + V + W)$ with the cardinality of some of their quotient spaces? (sorry if this is a dummy question but I'm not any familiar with quotient spaces).,,['linear-algebra']
51,"Div, curl and linear algebra","Div, curl and linear algebra",,"I came across this post lying dormant on some online forum . I am putting it here verbatim, it seems to me worth a lot. By Prof. S. D. Agashe, IIT Bombay (Source: Vector Calculus, by Durgaprasanna Bhattacharyya, University Studies Series,Griffith Prize Thesis, 1918, published by the University of Calcutta, India, 1920, 90 pp) Chapter IV: The Linear Vector Function, article 15, p.24: ""The most general vector expression linear in $r$ can contain terms only of three possible types, $r$ , $(a\cdot r)b$ and $c\times r$ , $a$ , $b$ , $c$ being constant unit vectors. Since $r$ , $(a\cdot r)b$ and $c\times r$ are in general non-coplanar, it follows from the theorem of the parallelepiped of vectors that the most general linear vector expression can be written in the form $\lambda \cdot r + \mu (a\cdot r)b + \nu (c\times r)$ , where $\lambda, \mu, \nu$ are scalar constants"". Bhattacharyya does not prove this. Has anyone seen a similar result and its proof? Bhattacharyya uses this to show that the divergence of the linear function is ( $3 \lambda + a\cdot b$ ), that the curl is ( $a \times b + 2c$ ). He goes on to define div and curl of a differentiable function as the div and curl of the (linear) derivative function. The div and curl of a linear function are defined in terms of certain surface integrals. I am excited about this result because it seems to provide an excellent route to div and curl, as Bhattacharyya himself remarks. Sorry for a rather long and ""technical"" communication.","I came across this post lying dormant on some online forum . I am putting it here verbatim, it seems to me worth a lot. By Prof. S. D. Agashe, IIT Bombay (Source: Vector Calculus, by Durgaprasanna Bhattacharyya, University Studies Series,Griffith Prize Thesis, 1918, published by the University of Calcutta, India, 1920, 90 pp) Chapter IV: The Linear Vector Function, article 15, p.24: ""The most general vector expression linear in can contain terms only of three possible types, , and , , , being constant unit vectors. Since , and are in general non-coplanar, it follows from the theorem of the parallelepiped of vectors that the most general linear vector expression can be written in the form , where are scalar constants"". Bhattacharyya does not prove this. Has anyone seen a similar result and its proof? Bhattacharyya uses this to show that the divergence of the linear function is ( ), that the curl is ( ). He goes on to define div and curl of a differentiable function as the div and curl of the (linear) derivative function. The div and curl of a linear function are defined in terms of certain surface integrals. I am excited about this result because it seems to provide an excellent route to div and curl, as Bhattacharyya himself remarks. Sorry for a rather long and ""technical"" communication.","r r (a\cdot r)b c\times r a b c r (a\cdot r)b c\times r \lambda \cdot r + \mu (a\cdot r)b + \nu (c\times r) \lambda, \mu, \nu 3 \lambda + a\cdot b a \times b + 2c","['linear-algebra', 'multivariable-calculus']"
52,Point reflection across a line,Point reflection across a line,,"Let's say that we have three points: $p = (x_p,y_p)$, $q = (x_q,y_q)$ and $a = (x_a,y_a)$. How can i find point $b$ which is reflection of $a$ across a line drawn through $p$ and $q$? I know it's simple to calculate, when we have $p$, $q$ etc. But I want to do this in my program, and I'm not sure, how to compute this. OK, I've found solution by myself (but answers in this topic really helped me). Suppose, that we have a line $Ax+By+C=0$, and $A^2+B^2 \not= 0$. $M (a,b)$ reflection across the line is point: $M' (\frac{aB^2-aA^2-2bAB-2AC}{A^2+B^2}, \frac{bA^2-bB^2-2aAB-2BC}{A^2+B^2})$ In my case, we don't have line, but only 2 points. How we can find $A,B,C$? It's simple: Let's say, that $p=(p_x,p_y)$ and $q = (q_x,q_y)$. Line equation is: $(y-p_y)(q_x-p_x) - (q_y-p_y)(x-p_x) = 0$ After some calculations we have: $y(q_x-p_x) - x(q_y-p_y) - (p_y(q_x-p_x)+p_x(p_y-q_y)) = 0$ So: $A = p_y-q_y$, $B = q_x-p_x$ and $C = -p_y(q_x-p_x)-p_x(p_y-q_y)$. That's all.","Let's say that we have three points: $p = (x_p,y_p)$, $q = (x_q,y_q)$ and $a = (x_a,y_a)$. How can i find point $b$ which is reflection of $a$ across a line drawn through $p$ and $q$? I know it's simple to calculate, when we have $p$, $q$ etc. But I want to do this in my program, and I'm not sure, how to compute this. OK, I've found solution by myself (but answers in this topic really helped me). Suppose, that we have a line $Ax+By+C=0$, and $A^2+B^2 \not= 0$. $M (a,b)$ reflection across the line is point: $M' (\frac{aB^2-aA^2-2bAB-2AC}{A^2+B^2}, \frac{bA^2-bB^2-2aAB-2BC}{A^2+B^2})$ In my case, we don't have line, but only 2 points. How we can find $A,B,C$? It's simple: Let's say, that $p=(p_x,p_y)$ and $q = (q_x,q_y)$. Line equation is: $(y-p_y)(q_x-p_x) - (q_y-p_y)(x-p_x) = 0$ After some calculations we have: $y(q_x-p_x) - x(q_y-p_y) - (p_y(q_x-p_x)+p_x(p_y-q_y)) = 0$ So: $A = p_y-q_y$, $B = q_x-p_x$ and $C = -p_y(q_x-p_x)-p_x(p_y-q_y)$. That's all.",,"['linear-algebra', 'analytic-geometry', 'reflection']"
53,Invertibility of a linear combination of self-adjoint operators,Invertibility of a linear combination of self-adjoint operators,,"Assume a bounded linear operator $T:\mathcal{H}\to\mathcal{H},$ where $\mathcal{H}$ is an infinite dimensional separable Hilbert space, is invertible. Let $$A={1\over 2}(T+T^*),\quad B={1\over 2i}(T-T^*)$$ Does there exist   a real constant $r$ such that the operator $A+rB$ is injective ? The operators $A$ and $B$ are self-adjoint and $T=A+iB.$ The assumptions imply that $\ker A\cap \ker B=\{0\}.$ Assume  that $\ker (A-rB)\neq \{0\}$ for any positive value of $r.$ Let $0\neq x_r\in\ker (A-rB).$ Then for $s\neq r$ we get $$\langle Ax_r,x_s\rangle =r\langle Bx_r,x_s\rangle,\quad \langle x_r,Ax_s\rangle =s\langle x_r,Bx_s\rangle$$ Therefore $$\langle Ax_r,x_s\rangle=0,\quad s\neq r$$ The conclusion is valid if one of the operators $A$ or $B$ is nonnegative. Indeed, consider the case $A\ge 0.$ Then $$0=\langle Ax_r,x_s\rangle =\langle A^{1/2}x_r,A^{1/2}x_s\rangle \quad s\neq r$$ The family of nonzero orthogonal vectors $\{A^{1/2}x_r\}_{r>0}$ is uncountable, which leads to a contradiction. The conclusion is valid also for any  normal invertible operator $T.$ In that case the operators $A$ and $B$ commute. We have $$\ker(A^2-r^2B^2)=\ker [(A+rB)(A-rB)]\supset \ker (A-rB)$$ The operator $A^2$ is nonnegative, hence there is $r\neq 0$ such that $\ker(A^2-r^2B^2)=\{0\}.$ Thus $A-rB$ is injective. We cannot expect the invertibility of the operator $A+rB.$ I was able (spoiler) to get an example of an invertible operator $T$ such $A+rB$ is not invertible for any real value $r.$ Let $\mathcal{H}=L^2(0,\pi)$ and $(Tf)(x)=e^{ix}f(x).$ Then $(Af)(x)=\cos x\,f(x) $ and $(Bf)(x)=\sin x\, f(x).$ Thus $[(A+rB)f](x)=(\cos x+r\sin x)\,f(x).$ The function $x\mapsto \cos x+r\sin x$ is not bounded away from $0,$ Therefore the operator $A+rB$ is not invertible.","Assume a bounded linear operator where is an infinite dimensional separable Hilbert space, is invertible. Let Does there exist   a real constant such that the operator is injective ? The operators and are self-adjoint and The assumptions imply that Assume  that for any positive value of Let Then for we get Therefore The conclusion is valid if one of the operators or is nonnegative. Indeed, consider the case Then The family of nonzero orthogonal vectors is uncountable, which leads to a contradiction. The conclusion is valid also for any  normal invertible operator In that case the operators and commute. We have The operator is nonnegative, hence there is such that Thus is injective. We cannot expect the invertibility of the operator I was able (spoiler) to get an example of an invertible operator such is not invertible for any real value Let and Then and Thus The function is not bounded away from Therefore the operator is not invertible.","T:\mathcal{H}\to\mathcal{H}, \mathcal{H} A={1\over 2}(T+T^*),\quad B={1\over 2i}(T-T^*) r A+rB A B T=A+iB. \ker A\cap \ker B=\{0\}. \ker (A-rB)\neq \{0\} r. 0\neq x_r\in\ker (A-rB). s\neq r \langle Ax_r,x_s\rangle =r\langle Bx_r,x_s\rangle,\quad \langle x_r,Ax_s\rangle =s\langle x_r,Bx_s\rangle \langle Ax_r,x_s\rangle=0,\quad s\neq r A B A\ge 0. 0=\langle Ax_r,x_s\rangle =\langle A^{1/2}x_r,A^{1/2}x_s\rangle \quad s\neq r \{A^{1/2}x_r\}_{r>0} T. A B \ker(A^2-r^2B^2)=\ker [(A+rB)(A-rB)]\supset \ker (A-rB) A^2 r\neq 0 \ker(A^2-r^2B^2)=\{0\}. A-rB A+rB. T A+rB r. \mathcal{H}=L^2(0,\pi) (Tf)(x)=e^{ix}f(x). (Af)(x)=\cos x\,f(x)  (Bf)(x)=\sin x\, f(x). [(A+rB)f](x)=(\cos x+r\sin x)\,f(x). x\mapsto \cos x+r\sin x 0, A+rB","['linear-algebra', 'functional-analysis']"
54,Does $\sum\limits_{k=1}^n\frac{a_i-a_k}{a_i+a_k}\cdot\frac{a_j-a_k}{a_j+a_k}=0$ for all $i\neq j$ imply $a_1=a_2=\cdots=a_n$?,Does  for all  imply ?,\sum\limits_{k=1}^n\frac{a_i-a_k}{a_i+a_k}\cdot\frac{a_j-a_k}{a_j+a_k}=0 i\neq j a_1=a_2=\cdots=a_n,"$$\forall i,\forall j\neq i,\quad\sum_{k=1}^n\frac{a_i-a_k}{a_i+a_k}\cdot\frac{a_j-a_k}{a_j+a_k}=0.$$ We can't have two different $a_i=0$ because of the denominators; but we can allow one $a_i=0$ , if the terms $k=i$ and $k=j$ are excluded from the sum. For $n=3$ , these equations are easy to solve: \begin{align*} (1,2):\quad&\frac{a_1-a_3}{a_1+a_3}\cdot\frac{a_2-a_3}{a_2+a_3}=0\\ (1,3):\quad&\frac{a_1-a_2}{a_1+a_2}\cdot\frac{a_3-a_2}{a_3+a_2}=0\\ (2,3):\quad&\frac{a_2-a_1}{a_2+a_1}\cdot\frac{a_3-a_1}{a_3+a_1}=0. \end{align*} Indeed we just get $a_1=a_2=a_3$ . For $n=4$ , the first of $6$ equations is $$(1,2):\quad\frac{a_1-a_3}{a_1+a_3}\cdot\frac{a_2-a_3}{a_2+a_3}+\frac{a_1-a_4}{a_1+a_4}\cdot\frac{a_2-a_4}{a_2+a_4}=0.$$ (For the other $5$ , just permute the indices.) I multiplied to clear the denominators, then added equations $(1,2)$ and $(3,4)$ to get $$4(a_1a_2-a_3a_4)^2=0$$ and thus $$a_1a_2=a_3a_4,\quad a_1a_3=a_2a_4,\quad a_1a_4=a_2a_3.$$ These imply that $a_1^2=a_2^2=a_3^2=a_4^2$ ; and we can't have $a_i=-a_j$ , again because of the denominators. So $a_1=a_2=a_3=a_4$ . Does this continue for $n\geq5$ ? If the variables are non-negative real numbers, then we can arrange them in order, $a_1\geq a_2\geq a_3\geq\cdots\geq a_n\geq0$ ; equation $(1,2)$ is then a sum of non-negative terms, so each term must vanish, which gives $a_2=a_3=\cdots=a_n$ . Then equation $(2,3)$ has only its first term remaining, which gives $a_1=a_2$ . What if some of the variables are negative, or complex numbers? We might define $b_{ij}=\dfrac{a_i-a_j}{a_i+a_j}$ to simplify the equations to $\sum_kb_{ik}b_{jk}=0$ . Collecting these into an antisymmetric matrix $B$ , we see that the system of equations is just saying that $$BB^T=-B^2=B^TB=D$$ is some diagonal matrix. But I don't think this tells us enough about $B$ itself. The defining equation for $b_{ij}$ can be rearranged to $$a_j=\frac{1-b_{ij}}{1+b_{ij}}a_i$$ so in particular $$a_3=\frac{1-b_{13}}{1+b_{13}}a_1=\frac{1-b_{23}}{1+b_{23}}a_2=\frac{1-b_{23}}{1+b_{23}}\cdot\frac{1-b_{12}}{1+b_{12}}a_1;$$ cancelling $a_1$ , $$(1+b_{31})(1+b_{23})(1+b_{12})=(1-b_{31})(1-b_{23})(1-b_{12});$$ expanding, $$2b_{12}b_{23}b_{31}+2b_{12}+2b_{23}+2b_{31}=0.$$ In this process I divided by some things that might be $0$ , but this resulting cubic equation is valid nonetheless. I think we can dispense with $a_i$ now. In summary, we need to solve the system of equations \begin{align*} \forall i,\forall j,\quad&b_{ij}+b_{ji}=0\\ \forall i,\forall j,\forall k,\quad&b_{ij}b_{jk}b_{ki}+b_{ij}+b_{jk}+b_{ki}=0\\ \forall i,\forall j\neq i,\quad&\sum_kb_{ik}b_{jk}=0. \end{align*} Is the only solution $b_{ij}=0$ ?","We can't have two different because of the denominators; but we can allow one , if the terms and are excluded from the sum. For , these equations are easy to solve: Indeed we just get . For , the first of equations is (For the other , just permute the indices.) I multiplied to clear the denominators, then added equations and to get and thus These imply that ; and we can't have , again because of the denominators. So . Does this continue for ? If the variables are non-negative real numbers, then we can arrange them in order, ; equation is then a sum of non-negative terms, so each term must vanish, which gives . Then equation has only its first term remaining, which gives . What if some of the variables are negative, or complex numbers? We might define to simplify the equations to . Collecting these into an antisymmetric matrix , we see that the system of equations is just saying that is some diagonal matrix. But I don't think this tells us enough about itself. The defining equation for can be rearranged to so in particular cancelling , expanding, In this process I divided by some things that might be , but this resulting cubic equation is valid nonetheless. I think we can dispense with now. In summary, we need to solve the system of equations Is the only solution ?","\forall i,\forall j\neq i,\quad\sum_{k=1}^n\frac{a_i-a_k}{a_i+a_k}\cdot\frac{a_j-a_k}{a_j+a_k}=0. a_i=0 a_i=0 k=i k=j n=3 \begin{align*}
(1,2):\quad&\frac{a_1-a_3}{a_1+a_3}\cdot\frac{a_2-a_3}{a_2+a_3}=0\\
(1,3):\quad&\frac{a_1-a_2}{a_1+a_2}\cdot\frac{a_3-a_2}{a_3+a_2}=0\\
(2,3):\quad&\frac{a_2-a_1}{a_2+a_1}\cdot\frac{a_3-a_1}{a_3+a_1}=0.
\end{align*} a_1=a_2=a_3 n=4 6 (1,2):\quad\frac{a_1-a_3}{a_1+a_3}\cdot\frac{a_2-a_3}{a_2+a_3}+\frac{a_1-a_4}{a_1+a_4}\cdot\frac{a_2-a_4}{a_2+a_4}=0. 5 (1,2) (3,4) 4(a_1a_2-a_3a_4)^2=0 a_1a_2=a_3a_4,\quad a_1a_3=a_2a_4,\quad a_1a_4=a_2a_3. a_1^2=a_2^2=a_3^2=a_4^2 a_i=-a_j a_1=a_2=a_3=a_4 n\geq5 a_1\geq a_2\geq a_3\geq\cdots\geq a_n\geq0 (1,2) a_2=a_3=\cdots=a_n (2,3) a_1=a_2 b_{ij}=\dfrac{a_i-a_j}{a_i+a_j} \sum_kb_{ik}b_{jk}=0 B BB^T=-B^2=B^TB=D B b_{ij} a_j=\frac{1-b_{ij}}{1+b_{ij}}a_i a_3=\frac{1-b_{13}}{1+b_{13}}a_1=\frac{1-b_{23}}{1+b_{23}}a_2=\frac{1-b_{23}}{1+b_{23}}\cdot\frac{1-b_{12}}{1+b_{12}}a_1; a_1 (1+b_{31})(1+b_{23})(1+b_{12})=(1-b_{31})(1-b_{23})(1-b_{12}); 2b_{12}b_{23}b_{31}+2b_{12}+2b_{23}+2b_{31}=0. 0 a_i \begin{align*}
\forall i,\forall j,\quad&b_{ij}+b_{ji}=0\\
\forall i,\forall j,\forall k,\quad&b_{ij}b_{jk}b_{ki}+b_{ij}+b_{jk}+b_{ki}=0\\
\forall i,\forall j\neq i,\quad&\sum_kb_{ik}b_{jk}=0.
\end{align*} b_{ij}=0","['linear-algebra', 'systems-of-equations', 'rational-functions']"
55,Why does the universal enveloping algebra not have zero-divisors?,Why does the universal enveloping algebra not have zero-divisors?,,"Let $\mathfrak{g}$ be a finite-dimensional Lie algebra, and denote by $U(\mathfrak{g})$ its universal enveloping algebra. It appears to be a consequence of the Poincaré-Birkhoff-Witt Theorem that $U(\mathfrak{g})$ has no zero-divisors. All sources I look at consider this to be either obvious or an easy exercise. But to be honest I'm baffled by this problem. Attempt 1: Take a basis $e_1,\ldots,e_n$ of $\mathfrak{g}$ , so that $U(\mathfrak{g})$ is generated by all terms of the form $e_1^{k_1} \cdots e_n^{k_n}$ . Take two elements $x$ and $y$ in $U(\mathfrak{g})$ , and suppose $x y = 0$ . Our goal is to show that $x$ or $y$ is trivial. Thanks to Poincaré-Birkhoff-Witt, $x$ and $y$ are a finitely sum of the form $$x = \sum_{k_1,\ldots,k_n} a_{k_1\cdots k_n} e_1^{k_1} \cdots e_n^{k_n}$$ and $$y = \sum_{k_1,\ldots,k_n} b_{k_1 \cdots k_n} e_1^{k_1} \cdots e_n^{k_n}$$ We can then expand the product $xy$ , and for each product of the form $$e_1^{k_1} \cdots e_n^{k_n} e_1^{k'_1} \cdots e_n^{k'_n}$$ we can use the rule $e_i e_j + e_j e_i = [e_i,e_j]$ finitely many times to find an expression of it with respect to the basis that PBW gives us. Working all this out, the expression for $xy$ entirely gets out of hand, and it is not at all clear that it won't vanish for non-trivial $x$ and $y$ . Attempt 2: It is often hinted that you should use the associated graded ring of $U(\mathfrak{g})$ in some way. Many times it is stated that, as a consequence of PBW, this graded ring is a polynomial ring, and that therefore there aren't zero divisors. Both logical steps elude me, and I have no idea how to proceed in this direction.","Let be a finite-dimensional Lie algebra, and denote by its universal enveloping algebra. It appears to be a consequence of the Poincaré-Birkhoff-Witt Theorem that has no zero-divisors. All sources I look at consider this to be either obvious or an easy exercise. But to be honest I'm baffled by this problem. Attempt 1: Take a basis of , so that is generated by all terms of the form . Take two elements and in , and suppose . Our goal is to show that or is trivial. Thanks to Poincaré-Birkhoff-Witt, and are a finitely sum of the form and We can then expand the product , and for each product of the form we can use the rule finitely many times to find an expression of it with respect to the basis that PBW gives us. Working all this out, the expression for entirely gets out of hand, and it is not at all clear that it won't vanish for non-trivial and . Attempt 2: It is often hinted that you should use the associated graded ring of in some way. Many times it is stated that, as a consequence of PBW, this graded ring is a polynomial ring, and that therefore there aren't zero divisors. Both logical steps elude me, and I have no idea how to proceed in this direction.","\mathfrak{g} U(\mathfrak{g}) U(\mathfrak{g}) e_1,\ldots,e_n \mathfrak{g} U(\mathfrak{g}) e_1^{k_1} \cdots e_n^{k_n} x y U(\mathfrak{g}) x y = 0 x y x y x = \sum_{k_1,\ldots,k_n} a_{k_1\cdots k_n} e_1^{k_1} \cdots e_n^{k_n} y = \sum_{k_1,\ldots,k_n} b_{k_1 \cdots k_n} e_1^{k_1} \cdots e_n^{k_n} xy e_1^{k_1} \cdots e_n^{k_n} e_1^{k'_1} \cdots e_n^{k'_n} e_i e_j + e_j e_i = [e_i,e_j] xy x y U(\mathfrak{g})","['linear-algebra', 'abstract-algebra']"
56,Limit of matrix inverse: $\lim_{\lambda \to \infty} (A + \lambda I)^{-1} = \mathbf{0}$?,Limit of matrix inverse: ?,\lim_{\lambda \to \infty} (A + \lambda I)^{-1} = \mathbf{0},"Let matrix $A \in \mathbb{R}^{n\times n}$ be positive semidefinite. Is it then true to that $$ (A + \lambda I)^{-1} \to \mathbf{0} \quad (\lambda \to \infty) \quad ? $$ If so, is the fact that $A$ is positive definite irrelevant here? My thoughts so far: $$ (A + \lambda I)^{-1} = \Big(\lambda( \frac{1}{\lambda}A + I ) \Big)^{-1} = \frac{1}{\lambda} \Big(\frac{1}{\lambda}A + I \Big)^{-1} $$ I think that $\lim_{\lambda \to \infty} \Big( \frac{1}{\lambda}A + I \Big)^{-1} = I^{-1} = I$ , but I don't know if I can just pass the $\lim$ through the inverse $(\cdot)^{-1}$ like that. If this is the case, then $$ \lim_{\lambda \to \infty} (A + \lambda I)^{-1} = \lim_{\lambda \to \infty} (1/\lambda) \lim_{\lambda \to \infty} (A/\lambda + I)^{-1} = 0 \cdot I = \mathbf{0} $$ as I'd like to show. Where this comes from: I'm trying to justify a claim made in an econometrics lecture. Namely, $$ \textrm{Var}(\hat{\beta}^{\textrm{ridge}}) = \sigma^2 (X^{T}X + \lambda I)^{-1} X^T X [(X^T X + \lambda I)^{-1}]^T \to \mathbf{0} $$ where $\hat{\beta}^\textrm{ridge}$ is the ridge estimator in a linear model, $X \in \mathbb{R}^{n \times p}$ is the design matrix, and the equality is known. The limit, however, wasn't justified.","Let matrix be positive semidefinite. Is it then true to that If so, is the fact that is positive definite irrelevant here? My thoughts so far: I think that , but I don't know if I can just pass the through the inverse like that. If this is the case, then as I'd like to show. Where this comes from: I'm trying to justify a claim made in an econometrics lecture. Namely, where is the ridge estimator in a linear model, is the design matrix, and the equality is known. The limit, however, wasn't justified.","A \in \mathbb{R}^{n\times n} 
(A + \lambda I)^{-1} \to \mathbf{0} \quad (\lambda \to \infty) \quad ?
 A 
(A + \lambda I)^{-1} = \Big(\lambda( \frac{1}{\lambda}A + I ) \Big)^{-1} = \frac{1}{\lambda} \Big(\frac{1}{\lambda}A + I \Big)^{-1}
 \lim_{\lambda \to \infty} \Big( \frac{1}{\lambda}A + I \Big)^{-1} = I^{-1} = I \lim (\cdot)^{-1} 
\lim_{\lambda \to \infty} (A + \lambda I)^{-1} = \lim_{\lambda \to \infty} (1/\lambda) \lim_{\lambda \to \infty} (A/\lambda + I)^{-1} = 0 \cdot I = \mathbf{0}
 
\textrm{Var}(\hat{\beta}^{\textrm{ridge}}) = \sigma^2 (X^{T}X + \lambda I)^{-1} X^T X [(X^T X + \lambda I)^{-1}]^T \to \mathbf{0}
 \hat{\beta}^\textrm{ridge} X \in \mathbb{R}^{n \times p}","['linear-algebra', 'matrices', 'limits']"
57,"Idea is correct, proof lacks rigor, coefficient of $t$ in $\det(I+tA)$","Idea is correct, proof lacks rigor, coefficient of  in",t \det(I+tA),"The goal of this exercise is to find the coefficient of $t$ in the polynomial $\det(I+tA)$ where $I, A \in Mat_n(\mathbb R)$. I've thought about this long and hard as I am combinatorically challenged (it's the most difficult branch of mathematics for certain) and I'm pretty sure I have a convincing argument, but it feels handwavey and lacks rigor. I'd like someone to review it and possibly suggest way to make it less verbal and more rigorous. My argument uses Laplace expansion, initially using the first column of $I+tA$. $\det(I+tA) = (1+ta_{1,1})\begin{vmatrix}1+ta_{2,2} & ta_{2,3} & \dots & ta_{2,n}\\ta_{3,2} & 1+ta_{3, 3} & \dots & ta_{3,n}\\ \vdots & \vdots & \dots & \vdots \\ ta_{n, 2} & ta_{n, 3} & \dots & 1+ta_{n,n}\end{vmatrix} -ta_{2,1}\det(B_2) + ta_{3,1}\det(B_3)-\dots+(-1)^{n+1}ta_{n, 1}\det(B_n)$ Where $B_i$ are some sub matrices with accordance to Laplace. Notice that since we used the first column in our original expansion, the first row of every $B_i$ is just $\begin{pmatrix}ta_{1,2} \dots ta_{1,n}\end{pmatrix} = t\begin{pmatrix}a_{1,2} \dots a_{1,n}\end{pmatrix} $ if we define $B_i'$ to be exactly the same as $B_i$ except the first row is divided by $t$, then by determinant rules $\det(B_i) = t\det(B_i')$, and so: $-ta_{2,1}\det(B_2) + ta_{3,1}\det(B_3)-\dots+(-1)^{n+1}ta_{n, 1}\det(B_n) = -t^2a_{2,1}\det(B_2') + t^2a_{3,1}\det(B_3')-\dots+(-1)^{n+1}t^2a_{n, 1}\det(B_n')$ and they only contain non linear terms of $t$. So they don't matter to us at all. We can just focus on the first determinant. But now we can just use the exact same argument on $\begin{vmatrix}1+ta_{2,2} & ta_{2,3} & \dots & ta_{2,n}\\ta_{3,2} & 1+ta_{3, 3} & \dots & ta_{3,n}\\ \vdots & \vdots & \dots & \vdots \\ ta_{n, 2} & ta_{n, 3} & \dots & 1+ta_{n,n}\end{vmatrix}$! The only thing that will matter is the first element and its leading minor. The others can be discarded as they are higher order of $t$. We will use it again and again until at the end we are only left with $\prod_{i=1}^{n}(1+ta_{i,i})$. The answer is hidden somewhere in this product. Since we are only interested in linear terms, we are not allowed to multiply $ta_{i,i}$ with $ta_{j,j}$. So every $ta_{i,i}$ can only be multiplied by $1$. So overall we should have $ta_{1,1} + ta_{2,2} + \dots ta_{n,n} = tTrace(A)$. Is this result correct? is this ""proof"" valid? It feels too wishy washy and verbal.","The goal of this exercise is to find the coefficient of $t$ in the polynomial $\det(I+tA)$ where $I, A \in Mat_n(\mathbb R)$. I've thought about this long and hard as I am combinatorically challenged (it's the most difficult branch of mathematics for certain) and I'm pretty sure I have a convincing argument, but it feels handwavey and lacks rigor. I'd like someone to review it and possibly suggest way to make it less verbal and more rigorous. My argument uses Laplace expansion, initially using the first column of $I+tA$. $\det(I+tA) = (1+ta_{1,1})\begin{vmatrix}1+ta_{2,2} & ta_{2,3} & \dots & ta_{2,n}\\ta_{3,2} & 1+ta_{3, 3} & \dots & ta_{3,n}\\ \vdots & \vdots & \dots & \vdots \\ ta_{n, 2} & ta_{n, 3} & \dots & 1+ta_{n,n}\end{vmatrix} -ta_{2,1}\det(B_2) + ta_{3,1}\det(B_3)-\dots+(-1)^{n+1}ta_{n, 1}\det(B_n)$ Where $B_i$ are some sub matrices with accordance to Laplace. Notice that since we used the first column in our original expansion, the first row of every $B_i$ is just $\begin{pmatrix}ta_{1,2} \dots ta_{1,n}\end{pmatrix} = t\begin{pmatrix}a_{1,2} \dots a_{1,n}\end{pmatrix} $ if we define $B_i'$ to be exactly the same as $B_i$ except the first row is divided by $t$, then by determinant rules $\det(B_i) = t\det(B_i')$, and so: $-ta_{2,1}\det(B_2) + ta_{3,1}\det(B_3)-\dots+(-1)^{n+1}ta_{n, 1}\det(B_n) = -t^2a_{2,1}\det(B_2') + t^2a_{3,1}\det(B_3')-\dots+(-1)^{n+1}t^2a_{n, 1}\det(B_n')$ and they only contain non linear terms of $t$. So they don't matter to us at all. We can just focus on the first determinant. But now we can just use the exact same argument on $\begin{vmatrix}1+ta_{2,2} & ta_{2,3} & \dots & ta_{2,n}\\ta_{3,2} & 1+ta_{3, 3} & \dots & ta_{3,n}\\ \vdots & \vdots & \dots & \vdots \\ ta_{n, 2} & ta_{n, 3} & \dots & 1+ta_{n,n}\end{vmatrix}$! The only thing that will matter is the first element and its leading minor. The others can be discarded as they are higher order of $t$. We will use it again and again until at the end we are only left with $\prod_{i=1}^{n}(1+ta_{i,i})$. The answer is hidden somewhere in this product. Since we are only interested in linear terms, we are not allowed to multiply $ta_{i,i}$ with $ta_{j,j}$. So every $ta_{i,i}$ can only be multiplied by $1$. So overall we should have $ta_{1,1} + ta_{2,2} + \dots ta_{n,n} = tTrace(A)$. Is this result correct? is this ""proof"" valid? It feels too wishy washy and verbal.",,"['linear-algebra', 'proof-verification', 'proof-writing', 'determinant', 'trace']"
58,Are almost all $k$-tuples of vectors in high dimensional space almost orthogonal?,Are almost all -tuples of vectors in high dimensional space almost orthogonal?,k,"I would like to know, in high dimensions, if you pick $k$ vectors (not necessarily distinct) are they likely to be $\textit{almost}$ orthogonal. I make this precise below but I am also open to other interpretation of the question. Since we only care about orthogonality, we can assume the vectors are unit vectors. Thus I want to consider a set $\textbf{V}=\{x_1,...,x_k\}\subset S^n$, where $S^n$ is the unit sphere. We can thus view $\textbf{V}\in \underbrace{S^n\times\cdots\times S^n}_{k-times}=\mathcal{S}_{n,k}$. Let $\mathcal{O}\subset\mathcal{S}_{n,k}$ denote the subset of $k$-tuples of orthogonal vectors. Let $\mathcal{U}_\epsilon=\{x\in\mathcal{S}_{n,k} : d(x,\mathcal{O}) <\epsilon\}$ be the $\epsilon$ neighborhood of $\mathcal{O}$ in $\mathcal{S}_{n,k}$ (using, say, the $l^2$ product metric, I don't think this matters). Let $\mu$ be the obvious probability product measure on $\mathcal{S}_{n,k}$. My questions is then: Is it true that $$\forall \epsilon > 0\textrm{ and } \forall k\textrm{ }\lim_{n\rightarrow\infty}\mu(\mathcal{U}_\epsilon) = 1$$ If not what can we say about this limit?","I would like to know, in high dimensions, if you pick $k$ vectors (not necessarily distinct) are they likely to be $\textit{almost}$ orthogonal. I make this precise below but I am also open to other interpretation of the question. Since we only care about orthogonality, we can assume the vectors are unit vectors. Thus I want to consider a set $\textbf{V}=\{x_1,...,x_k\}\subset S^n$, where $S^n$ is the unit sphere. We can thus view $\textbf{V}\in \underbrace{S^n\times\cdots\times S^n}_{k-times}=\mathcal{S}_{n,k}$. Let $\mathcal{O}\subset\mathcal{S}_{n,k}$ denote the subset of $k$-tuples of orthogonal vectors. Let $\mathcal{U}_\epsilon=\{x\in\mathcal{S}_{n,k} : d(x,\mathcal{O}) <\epsilon\}$ be the $\epsilon$ neighborhood of $\mathcal{O}$ in $\mathcal{S}_{n,k}$ (using, say, the $l^2$ product metric, I don't think this matters). Let $\mu$ be the obvious probability product measure on $\mathcal{S}_{n,k}$. My questions is then: Is it true that $$\forall \epsilon > 0\textrm{ and } \forall k\textrm{ }\lim_{n\rightarrow\infty}\mu(\mathcal{U}_\epsilon) = 1$$ If not what can we say about this limit?",,"['linear-algebra', 'matrices', 'probability-theory', 'measure-theory']"
59,Show that the collection of matrices which commute with every idempotent matrix are the scalar matrices,Show that the collection of matrices which commute with every idempotent matrix are the scalar matrices,,Show that the collection of $n\times n$ real matrices which commute with every idempotent matrix are the scalar matrices . Let $\mathcal P$ denote set of all idempotent matrices . Let $A=\{B:BP=PB\forall P\in \mathcal P\}$. So I need to show that $A=\{cI:c\in \Bbb R\}$. I am feeling totally confused on this. Will you kindly give some hints here?,Show that the collection of $n\times n$ real matrices which commute with every idempotent matrix are the scalar matrices . Let $\mathcal P$ denote set of all idempotent matrices . Let $A=\{B:BP=PB\forall P\in \mathcal P\}$. So I need to show that $A=\{cI:c\in \Bbb R\}$. I am feeling totally confused on this. Will you kindly give some hints here?,,"['linear-algebra', 'matrices']"
60,How many $3 \times 3$ integer matrices are orthogonal?,How many  integer matrices are orthogonal?,3 \times 3,"Let $S$ be the set of $3 \times 3$ matrices $\rm A$ with integer entries such that $$\rm AA^{\top} = I_3$$ What is $|S|$ (cardinality of $S$)? The answer is supposed to be 48. Here is my proof and I wish to know if it is correct. So, I am going to exploit the fact that the matrix A in a set will be orthognal, so if the matrix is of the form \begin{bmatrix}         a_{11} & a_{12} & a_{13} \\         a_{21} & a_{22} & a_{23} \\         a_{31} & a_{32} & a_{33} \\         \end{bmatrix} Then each column and row will have exactly one non-zero element which will be +1 or -1. Thus, I have split possibilities for the first column into three cases and counted the possibilities in each case as follows :- $$a_{11} \neq 0$$ or $$ a_{21} \neq 0$$ or $$ a_{31} \neq 0$$ In case 1), we obviously have two possibilities(+1 or -1) so we consider the one where the entry is +1. Now, notice that the moment we choose the next non-zero entry, all the places for non-zero entries will be decided because of the rule 'each column and row will have exactly one non-zero element'. Meaning, if b and c are remaining two non-zero entries, we only have two possibilities left  \begin{bmatrix}         1 & 0 & 0 \\         0 & b & 0 \\         0 & 0 & c \\         \end{bmatrix} or \begin{bmatrix}         1 & 0 & 0 \\         0 & 0 & c \\         0 & b & 0 \\         \end{bmatrix} Using the fact that b and c are simply $$\pm1$$ In each of the above matrices, we get 4 possibilities for each of the matricies. Thus, 8 possibilities in totality. Basically, we are getting 8 possibilities on the assumption that $$a_{11} = 1$$ Thus, we get 16 possibilities on the case that $$a_{11} \neq 0$$ Following, the second and third cases analogously, we get a total of 16 possibilities in each of them and 48 possibilities in total. Source :- Tata Institute of Fundamental Research Graduate School Admissions 2016","Let $S$ be the set of $3 \times 3$ matrices $\rm A$ with integer entries such that $$\rm AA^{\top} = I_3$$ What is $|S|$ (cardinality of $S$)? The answer is supposed to be 48. Here is my proof and I wish to know if it is correct. So, I am going to exploit the fact that the matrix A in a set will be orthognal, so if the matrix is of the form \begin{bmatrix}         a_{11} & a_{12} & a_{13} \\         a_{21} & a_{22} & a_{23} \\         a_{31} & a_{32} & a_{33} \\         \end{bmatrix} Then each column and row will have exactly one non-zero element which will be +1 or -1. Thus, I have split possibilities for the first column into three cases and counted the possibilities in each case as follows :- $$a_{11} \neq 0$$ or $$ a_{21} \neq 0$$ or $$ a_{31} \neq 0$$ In case 1), we obviously have two possibilities(+1 or -1) so we consider the one where the entry is +1. Now, notice that the moment we choose the next non-zero entry, all the places for non-zero entries will be decided because of the rule 'each column and row will have exactly one non-zero element'. Meaning, if b and c are remaining two non-zero entries, we only have two possibilities left  \begin{bmatrix}         1 & 0 & 0 \\         0 & b & 0 \\         0 & 0 & c \\         \end{bmatrix} or \begin{bmatrix}         1 & 0 & 0 \\         0 & 0 & c \\         0 & b & 0 \\         \end{bmatrix} Using the fact that b and c are simply $$\pm1$$ In each of the above matrices, we get 4 possibilities for each of the matricies. Thus, 8 possibilities in totality. Basically, we are getting 8 possibilities on the assumption that $$a_{11} = 1$$ Thus, we get 16 possibilities on the case that $$a_{11} \neq 0$$ Following, the second and third cases analogously, we get a total of 16 possibilities in each of them and 48 possibilities in total. Source :- Tata Institute of Fundamental Research Graduate School Admissions 2016",,"['linear-algebra', 'combinatorics', 'matrices', 'proof-verification', 'orthogonal-matrices']"
61,A non-Vandermonde matrix with Vandermonde-like determinant?,A non-Vandermonde matrix with Vandermonde-like determinant?,,"This question is related to the previous one . Consider $n$ variables $x_1,x_2,\ldots,x_n$ and the following $n\times n$ matrix: $$ A=\begin{bmatrix} 1 & \cdots &  1 \\  x_2 + x_3 + \dots + x_n & \dots &  x_1 + x_2 + \dots + x_{n-1} \\ x_2{x_3}  + x_2{x_4}+ \dots + x_{n-1}x_n & \dots &   x_1{x_2}  + x_1{x_3}+ \dots + x_{n-2}x_{n-1 } \\ \vdots & \dots & \vdots\\ x_2 x_3 \dots x_n & \dots &  x_1 x_2 \dots x_{n-1} \\ \end{bmatrix}. $$  When $i>1$, the element $a_{ij}$ is the sum of all possible products of $i-1$ variables $x_k$'s with distinct indices, except that $x_j$ is not participating in any term on column $j$. Formally, $$ a_{ij}=\sum_{k_1<\cdots<k_{i-1} \text{ and they are } \ne j} x_{k_1}x_{k_2}\cdots x_{k_{i-1}}. $$ Of course, when some $x_i=x_j$, $A$ has two equal columns and it becomes singular, but is this the only possibility for $\det A=0$?","This question is related to the previous one . Consider $n$ variables $x_1,x_2,\ldots,x_n$ and the following $n\times n$ matrix: $$ A=\begin{bmatrix} 1 & \cdots &  1 \\  x_2 + x_3 + \dots + x_n & \dots &  x_1 + x_2 + \dots + x_{n-1} \\ x_2{x_3}  + x_2{x_4}+ \dots + x_{n-1}x_n & \dots &   x_1{x_2}  + x_1{x_3}+ \dots + x_{n-2}x_{n-1 } \\ \vdots & \dots & \vdots\\ x_2 x_3 \dots x_n & \dots &  x_1 x_2 \dots x_{n-1} \\ \end{bmatrix}. $$  When $i>1$, the element $a_{ij}$ is the sum of all possible products of $i-1$ variables $x_k$'s with distinct indices, except that $x_j$ is not participating in any term on column $j$. Formally, $$ a_{ij}=\sum_{k_1<\cdots<k_{i-1} \text{ and they are } \ne j} x_{k_1}x_{k_2}\cdots x_{k_{i-1}}. $$ Of course, when some $x_i=x_j$, $A$ has two equal columns and it becomes singular, but is this the only possibility for $\det A=0$?",,"['linear-algebra', 'matrices', 'determinant', 'symmetric-polynomials']"
62,Showing that similar matrices have the same minimal polynomial,Showing that similar matrices have the same minimal polynomial,,"I am in the process of proving the title. The hint says, for any polynomial $f$, we have $$f(P^{-1}AP) = P^{-1}f(A)P.$$ A is an $n \times n$ matrix over $F$ while $P$ is an invertible matrix such that the above matrix multiplication $P^{-1}AP$ makes sense. Why is this true? Thank you.","I am in the process of proving the title. The hint says, for any polynomial $f$, we have $$f(P^{-1}AP) = P^{-1}f(A)P.$$ A is an $n \times n$ matrix over $F$ while $P$ is an invertible matrix such that the above matrix multiplication $P^{-1}AP$ makes sense. Why is this true? Thank you.",,"['linear-algebra', 'matrices']"
63,Infinite direct product of fields.,Infinite direct product of fields.,,"Let $F$ be a field, and consider the infinite direct product$$F \times F \times F \times F \times \dots,$$i.e. $\prod_{i=0}^\infty F$, i.e. the direct product of a countable number of copies of $F$. The theory of bases for vector spaces shows that this direct product has a basis, and so is isomorphic to a direct sum of copies of $F$. How many copies of $F$ do I need $($i.e. what is the dimension of this direct product as an $F$-vector space$)$? Is it possible to exhibit this isomorphism between an infinite direct product and an infintie direct sum explicitly?","Let $F$ be a field, and consider the infinite direct product$$F \times F \times F \times F \times \dots,$$i.e. $\prod_{i=0}^\infty F$, i.e. the direct product of a countable number of copies of $F$. The theory of bases for vector spaces shows that this direct product has a basis, and so is isomorphic to a direct sum of copies of $F$. How many copies of $F$ do I need $($i.e. what is the dimension of this direct product as an $F$-vector space$)$? Is it possible to exhibit this isomorphism between an infinite direct product and an infintie direct sum explicitly?",,"['linear-algebra', 'abstract-algebra']"
64,"why similarity over $\bar{\mathbb{F}}$ of $A,B\in M_n(\mathbb{F})$ implies similarity over $\mathbb{F}$?",why similarity over  of  implies similarity over ?,"\bar{\mathbb{F}} A,B\in M_n(\mathbb{F}) \mathbb{F}","A classic problem in linear algebra is to determine if two matrices $A,B\in M_n(\mathbb{F})$ are similar one to another. When $\mathbb{F}=\bar{\mathbb{F}}$, we know that $A,B$ are similar if and only if they have the same Jordan form. What about the the case where $\mathbb{F}\neq \bar{\mathbb{F}}$? In this case I saw that there is a theorem saying that for two matrices $A,B\in M_n(\mathbb{F})$, if they are similar over $\bar{\mathbb{F}}$ then they are similar over $\mathbb{F}$. However, I can't find any ""natural"" proof for that. I want to ask if someone know such a proof?","A classic problem in linear algebra is to determine if two matrices $A,B\in M_n(\mathbb{F})$ are similar one to another. When $\mathbb{F}=\bar{\mathbb{F}}$, we know that $A,B$ are similar if and only if they have the same Jordan form. What about the the case where $\mathbb{F}\neq \bar{\mathbb{F}}$? In this case I saw that there is a theorem saying that for two matrices $A,B\in M_n(\mathbb{F})$, if they are similar over $\bar{\mathbb{F}}$ then they are similar over $\mathbb{F}$. However, I can't find any ""natural"" proof for that. I want to ask if someone know such a proof?",,"['linear-algebra', 'matrices', 'field-theory', 'extension-field', 'similar-matrices']"
65,"If every vector is an eigenvector, the operator must be a scalar multiple of the identity operator? [duplicate]","If every vector is an eigenvector, the operator must be a scalar multiple of the identity operator? [duplicate]",,"This question already has answers here : Demonstration: If all vectors of $V$ are eigenvectors of $T$, then there is one $\lambda$ such that $T(v) = \lambda v$ for all $v \in V$. (3 answers) Closed 5 years ago . I am posed with the following question: Suppose that $T\in \mathcal{L}(V)$, where $V$ is a finite-dimensional vector space,  is such that every vector in $V$ is an eigenvector of $T$. Prove that T is a scalar multiple of the identity function. My attempt goes as follows: If $T$ is a scalar multiple of the identity function, then $Tv=av$ for all $v\in V$, where $a$ doesn't depend on $v$. We will start off assuming that that may not necessarily be true, and work our way to the result. From the problem statement, we know that $Tv=a_j v$, where $a_j$ may depend on the choice of $v$. To show that it doesn't, consider two non-zero vectors $v_1$ and $v_2$ both in $V$ (It would be pointless to consider zero vectors). Consider $T(v_1+v_2)$. Also, let us first consider the case where $v_2$ is not a scalar multiple of $v_1$ (so $v_1$ & $v_2$ form a linearly independent set). On one hand we have $$\begin{align*} T(v_1+v_2)&=\alpha (v_1+v_2)\\ &= \alpha v_1 + \alpha v_2 \end{align*}$$ Which is true because of the the assumption that every vector is an eigenvector. On the other hand, we have $$\begin{align*} T(v_1+v_2)&=T(v_1)+T(v_2)\\ &=a_1 v_1 + a_2 v_2 \end{align*}$$ So we are left with the following equality $$a_1 v_1 + a_2 v_2=\alpha v_1 +  \alpha v_2$$ $$\implies (a_1 - \alpha)v_1 + (a_2 - \alpha)v_2 = 0$$ Because $v_1$ & $v_2$ are linearly independent, $a_1=a_2=\alpha$. Now consider $\beta v$, a scalar multiple of $v$. $$\begin{align*} T(\beta v)&= \beta T(v)\\ &= \beta (a v)\\ &= a (\beta v) \end{align*} $$ Because every vector in $V$ that is not equal to an arbitrary vector $v$ is either a scalar multiple of it or the sum of $v$ and some other vector in $V$, $Tv=av$ for all $v\in V$. Is my prove valid? Please criticize my proof holistically. P.S: Is the same proof valid for infinite-dimensional vector spaces? I ask because such truth would lie in the validity of the last paragraph, and I don't know if it is true in infinite-dimensional vector spaces.","This question already has answers here : Demonstration: If all vectors of $V$ are eigenvectors of $T$, then there is one $\lambda$ such that $T(v) = \lambda v$ for all $v \in V$. (3 answers) Closed 5 years ago . I am posed with the following question: Suppose that $T\in \mathcal{L}(V)$, where $V$ is a finite-dimensional vector space,  is such that every vector in $V$ is an eigenvector of $T$. Prove that T is a scalar multiple of the identity function. My attempt goes as follows: If $T$ is a scalar multiple of the identity function, then $Tv=av$ for all $v\in V$, where $a$ doesn't depend on $v$. We will start off assuming that that may not necessarily be true, and work our way to the result. From the problem statement, we know that $Tv=a_j v$, where $a_j$ may depend on the choice of $v$. To show that it doesn't, consider two non-zero vectors $v_1$ and $v_2$ both in $V$ (It would be pointless to consider zero vectors). Consider $T(v_1+v_2)$. Also, let us first consider the case where $v_2$ is not a scalar multiple of $v_1$ (so $v_1$ & $v_2$ form a linearly independent set). On one hand we have $$\begin{align*} T(v_1+v_2)&=\alpha (v_1+v_2)\\ &= \alpha v_1 + \alpha v_2 \end{align*}$$ Which is true because of the the assumption that every vector is an eigenvector. On the other hand, we have $$\begin{align*} T(v_1+v_2)&=T(v_1)+T(v_2)\\ &=a_1 v_1 + a_2 v_2 \end{align*}$$ So we are left with the following equality $$a_1 v_1 + a_2 v_2=\alpha v_1 +  \alpha v_2$$ $$\implies (a_1 - \alpha)v_1 + (a_2 - \alpha)v_2 = 0$$ Because $v_1$ & $v_2$ are linearly independent, $a_1=a_2=\alpha$. Now consider $\beta v$, a scalar multiple of $v$. $$\begin{align*} T(\beta v)&= \beta T(v)\\ &= \beta (a v)\\ &= a (\beta v) \end{align*} $$ Because every vector in $V$ that is not equal to an arbitrary vector $v$ is either a scalar multiple of it or the sum of $v$ and some other vector in $V$, $Tv=av$ for all $v\in V$. Is my prove valid? Please criticize my proof holistically. P.S: Is the same proof valid for infinite-dimensional vector spaces? I ask because such truth would lie in the validity of the last paragraph, and I don't know if it is true in infinite-dimensional vector spaces.",,['linear-algebra']
66,Solutions of $XA=XAX$.,Solutions of .,XA=XAX,"All matrices are real and $n \times n$. The matrix $A$ is given. I am interested in solving $XA=XAX$. In particular, I would like some characterization of matrices that satisfy this equation. For instance, a useful characterization would be: any valid $X$ is related in some concrete way to the row space of $A$, or any valid $X$ is related in some way to the eigenvectors of $A$. I want to describe the set of admissible $X$es in terms of some known decomposition of $A$ or some property of $A$. In case this leads to a larger set of admissible solutions, I am also interested, as a separate problem, in the complex relaxation, i.e. $A$ is real but $X$ can be complex. I apologize if my question is very trivial - I have no experience solving matrix equations like that. In case my question is a special case of some long-established theory, please just point me to a book or an article.","All matrices are real and $n \times n$. The matrix $A$ is given. I am interested in solving $XA=XAX$. In particular, I would like some characterization of matrices that satisfy this equation. For instance, a useful characterization would be: any valid $X$ is related in some concrete way to the row space of $A$, or any valid $X$ is related in some way to the eigenvectors of $A$. I want to describe the set of admissible $X$es in terms of some known decomposition of $A$ or some property of $A$. In case this leads to a larger set of admissible solutions, I am also interested, as a separate problem, in the complex relaxation, i.e. $A$ is real but $X$ can be complex. I apologize if my question is very trivial - I have no experience solving matrix equations like that. In case my question is a special case of some long-established theory, please just point me to a book or an article.",,"['linear-algebra', 'matrices', 'matrix-equations', 'control-theory']"
67,Maximum number of linearly independent anti commuting matrices,Maximum number of linearly independent anti commuting matrices,,"Given n x n matrices, my book says the maximum size of a set of linearly independent mutually anti commuting matrices is $n^2-1$. I don't understand why this is true. Would appreciate any tips to prove this is the case.","Given n x n matrices, my book says the maximum size of a set of linearly independent mutually anti commuting matrices is $n^2-1$. I don't understand why this is true. Would appreciate any tips to prove this is the case.",,"['linear-algebra', 'matrices']"
68,Subspace spanned by eigenvectors is a subalgebra,Subspace spanned by eigenvectors is a subalgebra,,"Let $L$ be a Lie algebra over an algebraically closed field and let $x\in L$. Prove that the subspace of $L$ spanned by the eigenvectors of $\operatorname{ad}x$ is a subalgebra. Suppose the eigenvectors of $\operatorname{ad}x$ are $y_1,\ldots,y_n$ with eigenvalues $a_1,\ldots,a_n\in F$. So $[xy_i]=a_iy_i$ for $i=1,\ldots,n$. Let $z_1,z_2$ be in the subspace spanned by the eigenvectors of $\operatorname{ad}x$. Suppose $z_1=b_1y_1+\ldots+b_ny_n$ and $z_2=c_1y_1+\ldots+c_ny_n$. We must prove that $[z_1z_2]$ is a linear combination of the $y_i$'s. We have $[z_1z_2] = \sum_{i\neq j}b_ic_i[y_iy_j]$. What then?","Let $L$ be a Lie algebra over an algebraically closed field and let $x\in L$. Prove that the subspace of $L$ spanned by the eigenvectors of $\operatorname{ad}x$ is a subalgebra. Suppose the eigenvectors of $\operatorname{ad}x$ are $y_1,\ldots,y_n$ with eigenvalues $a_1,\ldots,a_n\in F$. So $[xy_i]=a_iy_i$ for $i=1,\ldots,n$. Let $z_1,z_2$ be in the subspace spanned by the eigenvectors of $\operatorname{ad}x$. Suppose $z_1=b_1y_1+\ldots+b_ny_n$ and $z_2=c_1y_1+\ldots+c_ny_n$. We must prove that $[z_1z_2]$ is a linear combination of the $y_i$'s. We have $[z_1z_2] = \sum_{i\neq j}b_ic_i[y_iy_j]$. What then?",,"['linear-algebra', 'lie-algebras']"
69,Repertoire method for solving recursions,Repertoire method for solving recursions,,"I am trying to solve this four parameter recurrence from exercise 1.16 in Concrete Mathematics: \[ g(1)=\alpha \] \[ g(2n+j)=3g(n)+\gamma n+\beta_j \] \[ \mbox{for}\ j=0,1\ \mbox{and}\ n\geq1 \] I have assumed the closed form to be: $$g(n) = A(n)\alpha+B(n)\gamma+C(n)\beta_0+D(n)\beta_1$$ Next i plugged $g(n)=1$ in the recurrence equations from which I obtained $\alpha=0 ,\beta_0=-2,\beta_1=-2$ and $\gamma=0$ Substituting these values back into the closed form, I got: $$A(n)-2C(n)-2D(n)=1$$ Similarly plugging $g(n)=n$, I got $\alpha=1,\beta_0=0,\beta_1=1$ and $\gamma=-1$ and plugging this back into the closed form, we get: $$A(n)-B(n)+D(n) = n$$ Also, from the text in chapter 1, a recursion of general form $$f(j)=\alpha_j$$ $$f(dn+j) = cf(n)+\beta_j$$ has a radix representation of $$f((b_mb_{m-1}...b_1b_0)_d) = (\alpha_{b_m}\beta_{b_m-1}...\beta_{b_1}\beta_{b_0})_c$$ Applying the generalization to the current problem we have $$A(n)\alpha+C(n)\beta_0+D(n)\beta_1=(\alpha\beta_{b_m-1}...\beta_{b_1}\beta_{b_0})_3$$ where $n=(1b_{m-1}...b_1b_0)_2$ I am unable to see how to proceed further from here. Any help will be appreciated :)","I am trying to solve this four parameter recurrence from exercise 1.16 in Concrete Mathematics: \[ g(1)=\alpha \] \[ g(2n+j)=3g(n)+\gamma n+\beta_j \] \[ \mbox{for}\ j=0,1\ \mbox{and}\ n\geq1 \] I have assumed the closed form to be: $$g(n) = A(n)\alpha+B(n)\gamma+C(n)\beta_0+D(n)\beta_1$$ Next i plugged $g(n)=1$ in the recurrence equations from which I obtained $\alpha=0 ,\beta_0=-2,\beta_1=-2$ and $\gamma=0$ Substituting these values back into the closed form, I got: $$A(n)-2C(n)-2D(n)=1$$ Similarly plugging $g(n)=n$, I got $\alpha=1,\beta_0=0,\beta_1=1$ and $\gamma=-1$ and plugging this back into the closed form, we get: $$A(n)-B(n)+D(n) = n$$ Also, from the text in chapter 1, a recursion of general form $$f(j)=\alpha_j$$ $$f(dn+j) = cf(n)+\beta_j$$ has a radix representation of $$f((b_mb_{m-1}...b_1b_0)_d) = (\alpha_{b_m}\beta_{b_m-1}...\beta_{b_1}\beta_{b_0})_c$$ Applying the generalization to the current problem we have $$A(n)\alpha+C(n)\beta_0+D(n)\beta_1=(\alpha\beta_{b_m-1}...\beta_{b_1}\beta_{b_0})_3$$ where $n=(1b_{m-1}...b_1b_0)_2$ I am unable to see how to proceed further from here. Any help will be appreciated :)",,"['linear-algebra', 'functions', 'recurrence-relations', 'recursive-algorithms', 'recursion']"
70,Monotone matrix,Monotone matrix,,"A real matrix $A$ is called monotone if $Ax\geq 0$ implies $x \geq 0$. If inverse of $A$ exists and is real, then prove that $A$ is monotone if and only if inverse of $A \geq 0$. ($x\geq 0$ means $x$ is a column vector whose all entries are non-negative. $A \geq 0$ means $A$ is a square matrix whose all entries are non-negative.)","A real matrix $A$ is called monotone if $Ax\geq 0$ implies $x \geq 0$. If inverse of $A$ exists and is real, then prove that $A$ is monotone if and only if inverse of $A \geq 0$. ($x\geq 0$ means $x$ is a column vector whose all entries are non-negative. $A \geq 0$ means $A$ is a square matrix whose all entries are non-negative.)",,"['linear-algebra', 'matrices']"
71,Tensor products over field do not commute with inverse limits?,Tensor products over field do not commute with inverse limits?,,"In the question: Inverse limit of modules and tensor product , Matt E gives an example where inverse limits and tensor products do not commute over the base ring $\mathbb{Z}$. He then goes on to show that it does hold if one takes a limit over modules of finite length and tensors with a finitely presented module. Are there counterexamples in the category of vector spaces over a field $k$ (not necessarily finite dimensional of course)?","In the question: Inverse limit of modules and tensor product , Matt E gives an example where inverse limits and tensor products do not commute over the base ring $\mathbb{Z}$. He then goes on to show that it does hold if one takes a limit over modules of finite length and tensors with a finitely presented module. Are there counterexamples in the category of vector spaces over a field $k$ (not necessarily finite dimensional of course)?",,"['linear-algebra', 'examples-counterexamples', 'tensor-products']"
72,Primary decomposition problem,Primary decomposition problem,,"Let $T$ be a linear operator on a finite dimensional space $V$, and let $p=p_{1}^{r_{1}} \cdots p_{k}^{r_{k}} $ be the minimal polynomial for $T$, and let $V= W_{1} \oplus\cdots\oplus W_{k}$ be the primary decomposition for $T$, i.e., $W_{j}$ is the null space of $p_{j}(T)^{r_{j}}$. Let $W$ be any subspace of $V$ which is invariant under $T$. Prove that  $W= (W \cap W_{1})\oplus (W \cap W_{2})\oplus \cdots \oplus (W \cap W_{k})$.","Let $T$ be a linear operator on a finite dimensional space $V$, and let $p=p_{1}^{r_{1}} \cdots p_{k}^{r_{k}} $ be the minimal polynomial for $T$, and let $V= W_{1} \oplus\cdots\oplus W_{k}$ be the primary decomposition for $T$, i.e., $W_{j}$ is the null space of $p_{j}(T)^{r_{j}}$. Let $W$ be any subspace of $V$ which is invariant under $T$. Prove that  $W= (W \cap W_{1})\oplus (W \cap W_{2})\oplus \cdots \oplus (W \cap W_{k})$.",,['linear-algebra']
73,Recovering a set of vectors from their Gram matrix,Recovering a set of vectors from their Gram matrix,,"Suppose $\{ v_1, \ldots, v_k \}$ is a set of vectors in $\mathbb{R}^n$. The associated $k\times k$ Gram matrix is given by $$ G = [v_i \cdot v_j]_{i,j} $$ It's (apparently) well known that the Gram matrix of a set of vectors determines the vectors up to an isometry of $\mathbb{R}^n$ (e.g. [1]) My question is: does anyone know of a reference for an algorithm that performs the reconstruction? More precisely, I'm looking for an algorithm that takes $G$ as input and outputs $\{ Av_1, \ldots, Av_k \}$ for some $A \in SO(n)$. [1] http://mathworld.wolfram.com/GramMatrix.html","Suppose $\{ v_1, \ldots, v_k \}$ is a set of vectors in $\mathbb{R}^n$. The associated $k\times k$ Gram matrix is given by $$ G = [v_i \cdot v_j]_{i,j} $$ It's (apparently) well known that the Gram matrix of a set of vectors determines the vectors up to an isometry of $\mathbb{R}^n$ (e.g. [1]) My question is: does anyone know of a reference for an algorithm that performs the reconstruction? More precisely, I'm looking for an algorithm that takes $G$ as input and outputs $\{ Av_1, \ldots, Av_k \}$ for some $A \in SO(n)$. [1] http://mathworld.wolfram.com/GramMatrix.html",,"['linear-algebra', 'reference-request']"
74,Determinant of the transpose via exterior products,Determinant of the transpose via exterior products,,"Let $V$ be a finite-dimensional vector space over $F$ and let $\tau:V \to V$ be a linear operator. Here's my definition of the determinant: If $t:U \to U$ is a linear operator and $\dim(U)=n$ then $\det(t)$ is the unique number satisfying   $$tu_1 \wedge \cdots \wedge tu_n = \det(t) u_1 \wedge \cdots \wedge u_n$$   for all $u_1,\dots,u_n \in U$. Define the transpose $\tau^T : V^* \to V^*$ of $\tau$ by $(\tau^Tf)(v)=f \tau v$. Is there a way of proving that $\det(\tau^T)=\det(\tau)$ without choosing a basis? It's clear that $$\det(\tau) v_1 \wedge\cdots\wedge v_n = \tau v_1 \wedge\cdots\wedge \tau v_n$$ and $$\det(\tau^T) f_1 \wedge\cdots\wedge f_n = \tau^T f_1 \wedge\cdots\wedge \tau^T f_n$$ for all $v_1,\dots,v_n \in V$ and $f_1,\dots,f_n \in V^*$, but how do I ""join"" them together?","Let $V$ be a finite-dimensional vector space over $F$ and let $\tau:V \to V$ be a linear operator. Here's my definition of the determinant: If $t:U \to U$ is a linear operator and $\dim(U)=n$ then $\det(t)$ is the unique number satisfying   $$tu_1 \wedge \cdots \wedge tu_n = \det(t) u_1 \wedge \cdots \wedge u_n$$   for all $u_1,\dots,u_n \in U$. Define the transpose $\tau^T : V^* \to V^*$ of $\tau$ by $(\tau^Tf)(v)=f \tau v$. Is there a way of proving that $\det(\tau^T)=\det(\tau)$ without choosing a basis? It's clear that $$\det(\tau) v_1 \wedge\cdots\wedge v_n = \tau v_1 \wedge\cdots\wedge \tau v_n$$ and $$\det(\tau^T) f_1 \wedge\cdots\wedge f_n = \tau^T f_1 \wedge\cdots\wedge \tau^T f_n$$ for all $v_1,\dots,v_n \in V$ and $f_1,\dots,f_n \in V^*$, but how do I ""join"" them together?",,"['linear-algebra', 'determinant', 'exterior-algebra']"
75,Eigenspace of the companion matrix of a monic polynomial,Eigenspace of the companion matrix of a monic polynomial,,How do I prove that the eigenspace of an $n\times n$ companion matrix  $$ C_p=\begin{bmatrix} 0 & 1 & 0 &\cdots & 0\\ 0 & 0 & 1 &\cdots & 0 \\ \vdots&\vdots &\vdots&\ddots&\vdots\\ 0 & 0 & 0 &\cdots &1 \\ -\alpha_0 &-\alpha_1 &-\alpha_2 &\cdots&-\alpha_{n-1} \end{bmatrix} $$ equals $\operatorname{Span}\{v_{\lambda} \} $ where $v_{\lambda}$ is an eigenvector of the companion matrix w.r.t. the eigenvalue $\lambda$: $$ v_{\lambda} =  \begin{bmatrix} 1 \\ \lambda\\ \lambda^{2} \\ \vdots\\ \lambda^{n-1} \end{bmatrix}. $$,How do I prove that the eigenspace of an $n\times n$ companion matrix  $$ C_p=\begin{bmatrix} 0 & 1 & 0 &\cdots & 0\\ 0 & 0 & 1 &\cdots & 0 \\ \vdots&\vdots &\vdots&\ddots&\vdots\\ 0 & 0 & 0 &\cdots &1 \\ -\alpha_0 &-\alpha_1 &-\alpha_2 &\cdots&-\alpha_{n-1} \end{bmatrix} $$ equals $\operatorname{Span}\{v_{\lambda} \} $ where $v_{\lambda}$ is an eigenvector of the companion matrix w.r.t. the eigenvalue $\lambda$: $$ v_{\lambda} =  \begin{bmatrix} 1 \\ \lambda\\ \lambda^{2} \\ \vdots\\ \lambda^{n-1} \end{bmatrix}. $$,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'companion-matrices']"
76,Is there a nice way to classify the ideals of the ring of lower triangular matrices?,Is there a nice way to classify the ideals of the ring of lower triangular matrices?,,"Suppose $T$ is the subset of $M_2(\mathbb{Z})$ of lower triangular matrices, those of form $\begin{pmatrix} a & 0 \\ b & c\end{pmatrix}$. So $T$ is a subring. Now I know that the ideals of $M_2(\mathbb{Z})$ are all of the form $M_2(I)$ for $I$ and ideal of $\mathbb{Z}$. However, is there a nice way to describe all the ideals in $T$ specifically, or does it not behave quite as nicely?","Suppose $T$ is the subset of $M_2(\mathbb{Z})$ of lower triangular matrices, those of form $\begin{pmatrix} a & 0 \\ b & c\end{pmatrix}$. So $T$ is a subring. Now I know that the ideals of $M_2(\mathbb{Z})$ are all of the form $M_2(I)$ for $I$ and ideal of $\mathbb{Z}$. However, is there a nice way to describe all the ideals in $T$ specifically, or does it not behave quite as nicely?",,"['linear-algebra', 'matrices', 'ideals']"
77,Bolzano Weierstrass theorem in a finite dimensional normed space,Bolzano Weierstrass theorem in a finite dimensional normed space,,"The problem may have a very simple answer, but it is confusing me a bit now. Let $(\mathbf{V},\lVert\cdot\rVert)$ be a finite dimensional normed vector space. A subset $\mathbf{U}$ of $\mathbf{V}$ is said to be bounded, if there is a real $M$ such that for any member $u$ of $\mathbf{U}$, we have: $\lVert u\rVert\lt M$. . Also, convergence of a sequence in $\mathbf{V}$ is defined with respect to the metric $\lVert\cdot\rVert$. Is it true that every bounded sequence of vectors in $\mathbf{V}$ admits a convergent subsequence? If not, please give a counterexample with $\mathbf{V}$ finite dimensional.","The problem may have a very simple answer, but it is confusing me a bit now. Let $(\mathbf{V},\lVert\cdot\rVert)$ be a finite dimensional normed vector space. A subset $\mathbf{U}$ of $\mathbf{V}$ is said to be bounded, if there is a real $M$ such that for any member $u$ of $\mathbf{U}$, we have: $\lVert u\rVert\lt M$. . Also, convergence of a sequence in $\mathbf{V}$ is defined with respect to the metric $\lVert\cdot\rVert$. Is it true that every bounded sequence of vectors in $\mathbf{V}$ admits a convergent subsequence? If not, please give a counterexample with $\mathbf{V}$ finite dimensional.",,['linear-algebra']
78,Is the dual space of a subspace of a vector space simply its orthogonal complement?,Is the dual space of a subspace of a vector space simply its orthogonal complement?,,"Is the dual space of a subspace of a vector space simply its orthogonal complement?  My professor seems to use the terms interchangeably, but from Wikipedia they seem to be quite distinct concepts (like dual spaces are much more complicated and involve functionals).","Is the dual space of a subspace of a vector space simply its orthogonal complement?  My professor seems to use the terms interchangeably, but from Wikipedia they seem to be quite distinct concepts (like dual spaces are much more complicated and involve functionals).",,['linear-algebra']
79,Is there a infinite dimensional topological vector space with the weight of topology larger than the algebraical dimension?,Is there a infinite dimensional topological vector space with the weight of topology larger than the algebraical dimension?,,"A topological vector space is a vector space endowed with a topology such that the sum of vectors and the multiplication by scalars are continuous. The weight of the topology $\tau$ is the smallest cardinal $\alpha$ such that there is a basis $\mathcal B$ of cardinality $\alpha$ for $\tau$ . By algebraical dimension, I mean the classical cardinality of its Hamel basis. So, my question is: is there some infinite dimensional topological vector space $V$ such that the weight of $V$ is larger than the dimension of $V$ ? If useful, I'm okay with using some axioms such as the Continuum Hypothesis. To give some context, my interest in this question arose from a theorem that holds if the weight of the topology is less than or equal to the dimension. So, I want to know if there are counterexamples if the weight is greater than the dimension, which includes knowing whether there are spaces with this property.","A topological vector space is a vector space endowed with a topology such that the sum of vectors and the multiplication by scalars are continuous. The weight of the topology is the smallest cardinal such that there is a basis of cardinality for . By algebraical dimension, I mean the classical cardinality of its Hamel basis. So, my question is: is there some infinite dimensional topological vector space such that the weight of is larger than the dimension of ? If useful, I'm okay with using some axioms such as the Continuum Hypothesis. To give some context, my interest in this question arose from a theorem that holds if the weight of the topology is less than or equal to the dimension. So, I want to know if there are counterexamples if the weight is greater than the dimension, which includes knowing whether there are spaces with this property.",\tau \alpha \mathcal B \alpha \tau V V V,"['linear-algebra', 'general-topology', 'functional-analysis', 'set-theory', 'topological-vector-spaces']"
80,Calculate the determinant of $n^\text{th}$ order,Calculate the determinant of  order,n^\text{th},"Calculate the determinant of $n^\text{th}$ order: $$ \begin{vmatrix} 1 + a_1  & 1 + a_1^2 & \dots & 1 + a_1^n \\  1 + a_2  & 1 + a_2^2 & \dots & 1 + a_2^n \\   \vdots  & \vdots   & \ddots & \vdots \\  1 + a_n  & 1 + a_n^2 & \dots & 1 + a_n^n \\  \end{vmatrix} $$ So whenever any two of the variables are equal the determinant becomes $0$ . Therefore, it has $$\prod_{1 \le k < i \le n} (a_i - a_k)$$ as a factor. But I haven't been able to find the rest of the factors. Any help is appreciated.","Calculate the determinant of order: So whenever any two of the variables are equal the determinant becomes . Therefore, it has as a factor. But I haven't been able to find the rest of the factors. Any help is appreciated.","n^\text{th} 
\begin{vmatrix}
1 + a_1  & 1 + a_1^2 & \dots & 1 + a_1^n \\ 
1 + a_2  & 1 + a_2^2 & \dots & 1 + a_2^n \\  
\vdots  & \vdots   & \ddots & \vdots \\ 
1 + a_n  & 1 + a_n^2 & \dots & 1 + a_n^n \\ 
\end{vmatrix}
 0 \prod_{1 \le k < i \le n} (a_i - a_k)","['linear-algebra', 'matrices', 'determinant']"
81,Finding $\mbox{tr}(A)$ from the condition $\mbox{tr}(A^2) = \mbox{tr}(A^3) = \mbox{tr}(A^4)$?,Finding  from the condition ?,\mbox{tr}(A) \mbox{tr}(A^2) = \mbox{tr}(A^3) = \mbox{tr}(A^4),"Let $A$ be an $n \times n$ matrix with real eigenvalues such that $$\mbox{tr}(A^2) = \mbox{tr}(A^3) = \mbox{tr}(A^4)$$ Then what would be $\mbox{tr}(A)$? I thought of finding $\sum_{i=1}^{n} \lambda_{i}$ from $$\sum_{i=1}^{n} \lambda_{i}^2 = \sum_{i=1}^{n} \lambda_{i}^3 = \sum_{i=1}^{n} \lambda_{i}^4$$ after this, I could try $\sum_{i=1}^{n} \lambda_{i}^2 - \lambda_{i}^3 = 0$ and $\sum_{i=1}^{n} \lambda_{i}^3 - \lambda_{i}^4 = 0$, how can I proceed with this? Also in another way it can also be put like this - finding $\sum_{i=1}^{n} a_{i}$ where $a_{i} \in \Bbb{R}$ given that $\sum_{i=1}^{n} a_{i}^2 = \sum_{i=1}^{n} a_{i}^3 = \sum_{i=1}^{n} a_{i}^4$?","Let $A$ be an $n \times n$ matrix with real eigenvalues such that $$\mbox{tr}(A^2) = \mbox{tr}(A^3) = \mbox{tr}(A^4)$$ Then what would be $\mbox{tr}(A)$? I thought of finding $\sum_{i=1}^{n} \lambda_{i}$ from $$\sum_{i=1}^{n} \lambda_{i}^2 = \sum_{i=1}^{n} \lambda_{i}^3 = \sum_{i=1}^{n} \lambda_{i}^4$$ after this, I could try $\sum_{i=1}^{n} \lambda_{i}^2 - \lambda_{i}^3 = 0$ and $\sum_{i=1}^{n} \lambda_{i}^3 - \lambda_{i}^4 = 0$, how can I proceed with this? Also in another way it can also be put like this - finding $\sum_{i=1}^{n} a_{i}$ where $a_{i} \in \Bbb{R}$ given that $\sum_{i=1}^{n} a_{i}^2 = \sum_{i=1}^{n} a_{i}^3 = \sum_{i=1}^{n} a_{i}^4$?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'trace']"
82,Equivalence of Two Norm and Infinity Norm,Equivalence of Two Norm and Infinity Norm,,How could you show that: $$\|x\|_\infty \le \|x\|_2 \le \sqrt{n} \|x\|_\infty. $$ I was able to show the left hand side but got stuck showing the right hand side. What would be the best way to approach it? For the LHS: $$\|x\|_\infty = \max\limits_{j}|x_j| \le \sqrt{\sum_i {x_i^2}} = \|x\|_2 $$ .,How could you show that: I was able to show the left hand side but got stuck showing the right hand side. What would be the best way to approach it? For the LHS: .,\|x\|_\infty \le \|x\|_2 \le \sqrt{n} \|x\|_\infty.  \|x\|_\infty = \max\limits_{j}|x_j| \le \sqrt{\sum_i {x_i^2}} = \|x\|_2 ,['linear-algebra']
83,Is there any intuitive relationship between $A A^{T}$ and $A^{T} A$?,Is there any intuitive relationship between  and ?,A A^{T} A^{T} A,"I’m struggling to understand the intuition behind the following arguments. If anyone please can point me in the right direction (in particular, a book that discuss this sort of construction). Let $S_1 = A A^{T}$ , $S_2 = A^{T} A$ , $\Pi_1$ the projection onto the range of $S_1$ , $\Pi_2$ the projection onto the range of $S_2$ . Define $\tilde{A} = \lim_{\epsilon \downarrow 0} (\epsilon I + S_1)^{-1} \Pi_1$ , $T = A^{T} \tilde{A}$ then $TA = \Pi_2$ , $AT = \Pi_1$ . Let $M = \begin{pmatrix} S_1 & 0 \\ 0 & I_d \end{pmatrix}$ a $2d \times 2d$ matrix, $\Sigma = (T, I - \Pi_2)$ a $d \times 2d$ matrix, then $\Sigma M \Sigma^T = I_d$ , $A\Sigma M = S_1$ . I can prove every claim, it’s just that I don’t understand the intuition behind it at all. It seems to me that there is something to it that I can’t quite put my finger on. I think the crux lies in the relationship between $S_1$ and $S_2$ , but I might be wrong. This construction comes out of nowhere in a book that only uses it as an auxiliary result, and I can’t find any reference to this sort of thing anywhere. It feels like it should be something general and applicable elsewhere.","I’m struggling to understand the intuition behind the following arguments. If anyone please can point me in the right direction (in particular, a book that discuss this sort of construction). Let , , the projection onto the range of , the projection onto the range of . Define , then , . Let a matrix, a matrix, then , . I can prove every claim, it’s just that I don’t understand the intuition behind it at all. It seems to me that there is something to it that I can’t quite put my finger on. I think the crux lies in the relationship between and , but I might be wrong. This construction comes out of nowhere in a book that only uses it as an auxiliary result, and I can’t find any reference to this sort of thing anywhere. It feels like it should be something general and applicable elsewhere.","S_1 = A A^{T} S_2 = A^{T} A \Pi_1 S_1 \Pi_2 S_2 \tilde{A} = \lim_{\epsilon \downarrow 0} (\epsilon I + S_1)^{-1} \Pi_1 T = A^{T} \tilde{A} TA = \Pi_2 AT = \Pi_1 M = \begin{pmatrix} S_1 & 0 \\ 0 & I_d \end{pmatrix} 2d \times 2d \Sigma = (T, I - \Pi_2) d \times 2d \Sigma M \Sigma^T = I_d A\Sigma M = S_1 S_1 S_2","['linear-algebra', 'positive-definite']"
84,How to prove PCA using induction?,How to prove PCA using induction?,,"In Deep Learning (Goodfellow, et al) , the optimization objective of PCA is formulated as $$D^* = \arg\min_D ||X - XDD^T||_F^2 \quad \text{s.t.} \quad D^T D=I$$ The book gives the proof of the $1$ -dimension case, i.e. $$\arg\min_{d} || X - X dd^T||_F^2 \quad \text{s.t.} \quad d^T d = 1$$ equals the eigenvector of $X^TX$ with the largest eigenvalue. And the author says the general case (when $D$ is an $m \times l$ matrix, where $l>1$ ) can be easily proved by induction. Could anyone please show me how I can prove that using induction? I know that when $D^T D = I$ : $$D^* = \arg\min_D ||X - XDD^T||_F^2 = \arg\min_D tr D^T X^T X D$$ and $$tr D^T X^T X D = \left(\sum_{i=1}^{l-1} \left(d^{(i)}\right)^T X^TX d^{(i)}\right) + \left(d^{(l)}\right)^T X^TX d^{(l)}$$ where the left-hand side of the addition reaches maximum when $d^{(i)}$ is the $ith$ largest eigenvector of $X^T X$ according to induction hypothesis. But how can I be sure that the result of the addition in a whole is also maximal?","In Deep Learning (Goodfellow, et al) , the optimization objective of PCA is formulated as The book gives the proof of the -dimension case, i.e. equals the eigenvector of with the largest eigenvalue. And the author says the general case (when is an matrix, where ) can be easily proved by induction. Could anyone please show me how I can prove that using induction? I know that when : and where the left-hand side of the addition reaches maximum when is the largest eigenvector of according to induction hypothesis. But how can I be sure that the result of the addition in a whole is also maximal?",D^* = \arg\min_D ||X - XDD^T||_F^2 \quad \text{s.t.} \quad D^T D=I 1 \arg\min_{d} || X - X dd^T||_F^2 \quad \text{s.t.} \quad d^T d = 1 X^TX D m \times l l>1 D^T D = I D^* = \arg\min_D ||X - XDD^T||_F^2 = \arg\min_D tr D^T X^T X D tr D^T X^T X D = \left(\sum_{i=1}^{l-1} \left(d^{(i)}\right)^T X^TX d^{(i)}\right) + \left(d^{(l)}\right)^T X^TX d^{(l)} d^{(i)} ith X^T X,"['linear-algebra', 'matrices', 'optimization', 'principal-component-analysis']"
85,What really is the modern definition of Euclidean spaces?,What really is the modern definition of Euclidean spaces?,,"What is the modern definition of Euclidean spaces ? I read the Wikipedia article about the topic , but I still don't get it. Is a Euclidean space something that satisfies the traditional Euclid's axioms, or Hilbert's axioms? or is it defined to be an inner product space? or is it defined to be a set on which we can somehow define the notion of ""length"" and ""angle""? or is it defined to be an affine space? If a Euclidean space is defined as in case 2 (i.e. as an inner product space), then do we still need Euclid's axioms or Hilbert's axioms? For example, there's a Hilbert's axiom For every two points A, B there exists a line a that contains each of the points A, B. but in terms of inner product space terminology, it can be directly proved by trivial pre-calculus technique, right?","What is the modern definition of Euclidean spaces ? I read the Wikipedia article about the topic , but I still don't get it. Is a Euclidean space something that satisfies the traditional Euclid's axioms, or Hilbert's axioms? or is it defined to be an inner product space? or is it defined to be a set on which we can somehow define the notion of ""length"" and ""angle""? or is it defined to be an affine space? If a Euclidean space is defined as in case 2 (i.e. as an inner product space), then do we still need Euclid's axioms or Hilbert's axioms? For example, there's a Hilbert's axiom For every two points A, B there exists a line a that contains each of the points A, B. but in terms of inner product space terminology, it can be directly proved by trivial pre-calculus technique, right?",,"['linear-algebra', 'general-topology', 'geometry', 'hilbert-spaces', 'inner-products']"
86,Reflection relating two subspaces,Reflection relating two subspaces,,"Let $S_1, S_2 \subseteq \mathbb{R}^n$ be two linear $k$-dimensional subspaces. Does there always exist a hyperplane $H$ such that $S_1 = R_H S_2$, where $R_H$ denotes the orthogonal reflection across $H$? Geometric intuition leads me to suspect that this is obviously true, but a proof is eluding me. Edit: As @zvbxrpl pointed out below, my intuition was wrong and the claim is false. However, the claim may still be true with ""hyperplane"" replaced by some linear subspace -- @zvbxrpl attempted to show this, but unfortunately there was an error in the proof. Is the claim true if ""hyperplane"" is replaced by some linear subspace? Edit 2: Studiosus has given an affirmative answer to the above revised question using Riemannian geometry. This is nice, but I am interested in a constructive, elementary answer using only linear algebra.","Let $S_1, S_2 \subseteq \mathbb{R}^n$ be two linear $k$-dimensional subspaces. Does there always exist a hyperplane $H$ such that $S_1 = R_H S_2$, where $R_H$ denotes the orthogonal reflection across $H$? Geometric intuition leads me to suspect that this is obviously true, but a proof is eluding me. Edit: As @zvbxrpl pointed out below, my intuition was wrong and the claim is false. However, the claim may still be true with ""hyperplane"" replaced by some linear subspace -- @zvbxrpl attempted to show this, but unfortunately there was an error in the proof. Is the claim true if ""hyperplane"" is replaced by some linear subspace? Edit 2: Studiosus has given an affirmative answer to the above revised question using Riemannian geometry. This is nice, but I am interested in a constructive, elementary answer using only linear algebra.",,"['linear-algebra', 'geometry', 'riemannian-geometry']"
87,Why does the gradient of matrix product $AB$ w.r.t. $A$ equal $B^T$?,Why does the gradient of matrix product  w.r.t.  equal ?,AB A B^T,"The below passage is from p. 215 of Deep Learning by Goodfellow, Bengio and Courville. For example, we might use a matrix multiplication operation to create   a variable $C = AB$. Suppose that the gradient of a scalar $z$ with   respect to $C$ is given by $G$. The matrix multiplication operation is   responsible for defining two back-propagation rules, one for each of   its input arguments. If we call the bprop method to request the   gradient with respect to $A$ given that the gradient on the output is   $G$ , then the bprop method of the matrix multiplication operation   must state that the gradient with respect to A is given by $GB^T$. They are applying chain rule to compute the gradient of scalar $z = f(C)$ with respect to $A$.  I am unfamiliar with the idea of computing the gradient of a product of matrices with respect to a matrix.  What does this mean, and why is the result transposed?","The below passage is from p. 215 of Deep Learning by Goodfellow, Bengio and Courville. For example, we might use a matrix multiplication operation to create   a variable $C = AB$. Suppose that the gradient of a scalar $z$ with   respect to $C$ is given by $G$. The matrix multiplication operation is   responsible for defining two back-propagation rules, one for each of   its input arguments. If we call the bprop method to request the   gradient with respect to $A$ given that the gradient on the output is   $G$ , then the bprop method of the matrix multiplication operation   must state that the gradient with respect to A is given by $GB^T$. They are applying chain rule to compute the gradient of scalar $z = f(C)$ with respect to $A$.  I am unfamiliar with the idea of computing the gradient of a product of matrices with respect to a matrix.  What does this mean, and why is the result transposed?",,"['linear-algebra', 'multivariable-calculus', 'derivatives', 'matrix-calculus']"
88,Proving a norm is lipschitz,Proving a norm is lipschitz,,"Let $M\in\mathbb{R}^{n\times n}$. Define the function $f\colon\mathbb{R}^n\to\mathbb{R}$ by $f(x)=\Vert Mx\Vert$. Show that $f$ is Lipschitz. Let $x,y\in\mathbb{R}^n$, then we want to find a $L>0$ such that $$\Vert f(x)-f(y)\Vert \le L\Vert x-y\Vert$$ We have \begin{align} \Vert f(x)-f(y)\Vert &= \big\Vert \Vert Mx\Vert-\Vert My\Vert\big\Vert\\ &\le \Vert Mx- My\Vert&\text{reverse triangle ineq.}\\ &\le \Vert M\Vert\Vert x-y\Vert\\ \end{align} Taking $L=\Vert M\Vert$ we find $f$ is Lipschitz. I have a few questions here, some related to Lipschitz continuity and others related to norms. Firstly, is this working correct? I notice that we take $L=\Vert M\Vert$ but could we take anything greater than our chosen $L$, in other words, is the Lipschitz constant unique? On a side note, is $\Vert \Vert Mx\Vert \Vert=\Vert Mx\Vert$? In general can we say that $\Vert\Vert a\Vert\Vert = \Vert a\Vert$?","Let $M\in\mathbb{R}^{n\times n}$. Define the function $f\colon\mathbb{R}^n\to\mathbb{R}$ by $f(x)=\Vert Mx\Vert$. Show that $f$ is Lipschitz. Let $x,y\in\mathbb{R}^n$, then we want to find a $L>0$ such that $$\Vert f(x)-f(y)\Vert \le L\Vert x-y\Vert$$ We have \begin{align} \Vert f(x)-f(y)\Vert &= \big\Vert \Vert Mx\Vert-\Vert My\Vert\big\Vert\\ &\le \Vert Mx- My\Vert&\text{reverse triangle ineq.}\\ &\le \Vert M\Vert\Vert x-y\Vert\\ \end{align} Taking $L=\Vert M\Vert$ we find $f$ is Lipschitz. I have a few questions here, some related to Lipschitz continuity and others related to norms. Firstly, is this working correct? I notice that we take $L=\Vert M\Vert$ but could we take anything greater than our chosen $L$, in other words, is the Lipschitz constant unique? On a side note, is $\Vert \Vert Mx\Vert \Vert=\Vert Mx\Vert$? In general can we say that $\Vert\Vert a\Vert\Vert = \Vert a\Vert$?",,"['linear-algebra', 'continuity', 'normed-spaces', 'lipschitz-functions']"
89,"Given a matrix $A$ such that $A^{\ell}$ is a constant matrix, must $A$ be a constant matrix?","Given a matrix  such that  is a constant matrix, must  be a constant matrix?",A A^{\ell} A,"This problem originates from an exercise in Richard Stanley's Algebraic Combinatorics .  The exercise in the text (Chapter 3, Exercise 2(a)) asks Let $G$ be a finite graph (allowing loops and multiple edges).  Suppose that there is some $\ell> 0$ such that the number of walks of length $\ell$ from any fixed vertex $u$ to any fixed vertex $v$ is independent of $u$ and $v$.  Show that $G$ has the same number $k$ of edges between any two vertices (including $k$ loops at each vertex. The hypothesis of the problem (that the number of walks of length $\ell$ between any two vertices is the same) tells us that the adjacency matrix $A(G)$ of $G$ raised to the $\ell$ power is a constant matrix $$ (A(G))^{\ell} = \begin{pmatrix} c & c & \cdots & c \\ c & c & \cdots & c \\ \vdots & \vdots & \ddots & \vdots \\ c & c & \cdots & c \end{pmatrix} $$ for some constant $C$.  We would like to conclude that this means the adjacency matrix itself is a constant matrix (hence, the number of walks of length 1 between any two vertices is the same, i.e., the number of edges between any two vertices is the same). Update in response to comments below: In this case we also have that $A(G)$ is a symmetric matrix which would eliminate some trivial counter examples. Does this result follow from something in linear algebra?  What is the proof?  If not, is there some other approach that might be more fruitful?","This problem originates from an exercise in Richard Stanley's Algebraic Combinatorics .  The exercise in the text (Chapter 3, Exercise 2(a)) asks Let $G$ be a finite graph (allowing loops and multiple edges).  Suppose that there is some $\ell> 0$ such that the number of walks of length $\ell$ from any fixed vertex $u$ to any fixed vertex $v$ is independent of $u$ and $v$.  Show that $G$ has the same number $k$ of edges between any two vertices (including $k$ loops at each vertex. The hypothesis of the problem (that the number of walks of length $\ell$ between any two vertices is the same) tells us that the adjacency matrix $A(G)$ of $G$ raised to the $\ell$ power is a constant matrix $$ (A(G))^{\ell} = \begin{pmatrix} c & c & \cdots & c \\ c & c & \cdots & c \\ \vdots & \vdots & \ddots & \vdots \\ c & c & \cdots & c \end{pmatrix} $$ for some constant $C$.  We would like to conclude that this means the adjacency matrix itself is a constant matrix (hence, the number of walks of length 1 between any two vertices is the same, i.e., the number of edges between any two vertices is the same). Update in response to comments below: In this case we also have that $A(G)$ is a symmetric matrix which would eliminate some trivial counter examples. Does this result follow from something in linear algebra?  What is the proof?  If not, is there some other approach that might be more fruitful?",,"['linear-algebra', 'matrices', 'algebraic-combinatorics']"
90,Validity of this Determinant property.,Validity of this Determinant property.,,"The property states that, ""If to each element of any row (or column) of a matrix, product of a scalar and a corresponding element of any other row (or column) is added, the determinant of the new matrix is same as that of the original matrix"" First of all, I calculated the determinant of the original third-order non-zero matrix, say $A$. Input: I first tested this property by adding the ""product of a scalar $k_1$ and the corresponding elements of any column, say $C_2$"" to only one column, say $C_1$, of the original matrix and then calculated the determinant of the new  matrix $B$. Result: $\det{B}=\det{A}$ (Property verified) Input: Then, I tested this property by adding the ""product of a scalar $k_1$ and the corresponding elements of column $C_2$"" to $C_1$ and by adding the ""product of a different scalar $k_2$ and the corresponding elements of another column $C_3$ to the column $C_2$ as well. A new matrix appeared, say $C$. Result: $\det {C} = \det {A}$ (Property verified) Input: Then, ultimately, I tested this property by taking matrix $C$ and adding the ""product of a new scalar $k_3$ and the corresponding elements of the column $C_1$"" to column $C_3$. A new matrix $D$ came up. Result: $\det {D}= (\det {A})(1+k_1k_2k_3)$ $ \implies \det {D} \neq \det {A}$, unless $k_1=0$ or $k_2=0$ or $k_3=0$ or $k_1=k_2=k_3=0$  (Property invalid) Note: In first two inputs, changes are made with matrix A directly however in third input changes are made to C in order to save time. I think, my understanding to this property is flawed. Ensure me if it is the case.","The property states that, ""If to each element of any row (or column) of a matrix, product of a scalar and a corresponding element of any other row (or column) is added, the determinant of the new matrix is same as that of the original matrix"" First of all, I calculated the determinant of the original third-order non-zero matrix, say $A$. Input: I first tested this property by adding the ""product of a scalar $k_1$ and the corresponding elements of any column, say $C_2$"" to only one column, say $C_1$, of the original matrix and then calculated the determinant of the new  matrix $B$. Result: $\det{B}=\det{A}$ (Property verified) Input: Then, I tested this property by adding the ""product of a scalar $k_1$ and the corresponding elements of column $C_2$"" to $C_1$ and by adding the ""product of a different scalar $k_2$ and the corresponding elements of another column $C_3$ to the column $C_2$ as well. A new matrix appeared, say $C$. Result: $\det {C} = \det {A}$ (Property verified) Input: Then, ultimately, I tested this property by taking matrix $C$ and adding the ""product of a new scalar $k_3$ and the corresponding elements of the column $C_1$"" to column $C_3$. A new matrix $D$ came up. Result: $\det {D}= (\det {A})(1+k_1k_2k_3)$ $ \implies \det {D} \neq \det {A}$, unless $k_1=0$ or $k_2=0$ or $k_3=0$ or $k_1=k_2=k_3=0$  (Property invalid) Note: In first two inputs, changes are made with matrix A directly however in third input changes are made to C in order to save time. I think, my understanding to this property is flawed. Ensure me if it is the case.",,"['linear-algebra', 'matrices', 'determinant']"
91,Proving that cows have same weight without weighing it!,Proving that cows have same weight without weighing it!,,"My friend gave me this problem and I have no clue how to go about it: A peasant  has $2n + 1$ cows. When he puts aside any of his cows, the remaining $2n$, can be divided into two sub-flocks of $n$ cows and each having the same total weight. How can we prove that the cows all have the same weight? How can I approach this?","My friend gave me this problem and I have no clue how to go about it: A peasant  has $2n + 1$ cows. When he puts aside any of his cows, the remaining $2n$, can be divided into two sub-flocks of $n$ cows and each having the same total weight. How can we prove that the cows all have the same weight? How can I approach this?",,"['linear-algebra', 'combinatorics', 'contest-math']"
92,Finding the symplectic matrix in Williamson's theorem,Finding the symplectic matrix in Williamson's theorem,,"tl;dr: How do I construct the symplectic matrix in Williamson's theorem? I am interested in a constructive proof/version of Williamson's theorem in symplectic linear algebra. Maybe I'm just missing a simple step, so here is what I know: Let us fix the symplectic form $J=\begin{pmatrix} 0_n & 1_n \\ -1_n & 0_n\end{pmatrix}$. Recall: Theorem: Let $M\in\mathbb{R}^{2n\times 2n}$ be a positive-definite matrix, then there exists a symplectic matrix $S\in Sp(2n,\mathbb{R})$ and a diagonal matrix $D\in\mathbb{R}^{n\times n}$ such that $$M=S^T\tilde{D}S$$ where $\tilde{D}=\operatorname{diag}(D,D)$ is diagonal. My basic interest is in how to construct the $S$ (i.e. write a matlab program that does this). This theorem is cited and used in various cases, however one cannot find many proofs despite the original. Here is the one proof I found (the most important step for me is unjustified). Let us denote by $\langle \cdot,\cdot \rangle_M:=\langle \cdot, M\cdot \rangle$ the symmetric bilinear form defined by $M$ and by $\sigma(\cdot,\cdot)=\langle \cdot, J\cdot \rangle$ the symplectic form. The theorem asserts that there is an $M$-orthogonal and symplectic basis. The failure of the basis to be $M$-orthonormal is then given by $D$. In order to find such a basis, it seems a good idea to look at $JM$. As this is a real matrix, which is antisymmetric with respect to $\langle\cdot,\cdot\rangle_M$, its eigenvalues come in pairs $\pm i\lambda_j$ ($j=1,\ldots n$) and the corresponding eigenvectors in pairs $e_j\pm if_j$ with real $e_j,f_j$ (the proof in the link considers the eigenvalues/eigenvectors of $M^{-1}J$ instead). The claim in the proof is now that $\{e_j,f_j\}_j$ forms an $M$-orthonormal basis. If this is the case, one can easily see that $\delta_{jk}=\langle e_j,Me_k\rangle=-\lambda_k\sigma(e_j,f_k)$ and $0=\langle e_j,Mf_k\rangle=\lambda_k\sigma(e_j,e_k)$ and similarly for $f_k$, hence the basis is indeed $M$-orthogonal and symplectic after normalization by $\sqrt{\lambda_j}$. The matrix $S$ should then consist of the normalized $f_j$ and $e_j$ as columns. Q1: Why is $\{e_j,f_j\}_j$ an orthonormal basis w.r.t to $M$? Matlab suggests that most of the times, it is, but I seem to miss something fundamental, because I don't see it. Why most of the times? Eigenvalue multiplicities seem to play a role here: Q2: If $\lambda_j\neq \lambda_k$ always, then Q1 seems to be true. If there are some eigenvalue multiplicities, this seems to be wrong, probably because of the non-uniqueness of eigenvectors. How can I fix this? I suspect that some Gram-Schmidt wrt. $M$ is necessary, but since I can't answer Q1, I can't see whether this does the trick.","tl;dr: How do I construct the symplectic matrix in Williamson's theorem? I am interested in a constructive proof/version of Williamson's theorem in symplectic linear algebra. Maybe I'm just missing a simple step, so here is what I know: Let us fix the symplectic form $J=\begin{pmatrix} 0_n & 1_n \\ -1_n & 0_n\end{pmatrix}$. Recall: Theorem: Let $M\in\mathbb{R}^{2n\times 2n}$ be a positive-definite matrix, then there exists a symplectic matrix $S\in Sp(2n,\mathbb{R})$ and a diagonal matrix $D\in\mathbb{R}^{n\times n}$ such that $$M=S^T\tilde{D}S$$ where $\tilde{D}=\operatorname{diag}(D,D)$ is diagonal. My basic interest is in how to construct the $S$ (i.e. write a matlab program that does this). This theorem is cited and used in various cases, however one cannot find many proofs despite the original. Here is the one proof I found (the most important step for me is unjustified). Let us denote by $\langle \cdot,\cdot \rangle_M:=\langle \cdot, M\cdot \rangle$ the symmetric bilinear form defined by $M$ and by $\sigma(\cdot,\cdot)=\langle \cdot, J\cdot \rangle$ the symplectic form. The theorem asserts that there is an $M$-orthogonal and symplectic basis. The failure of the basis to be $M$-orthonormal is then given by $D$. In order to find such a basis, it seems a good idea to look at $JM$. As this is a real matrix, which is antisymmetric with respect to $\langle\cdot,\cdot\rangle_M$, its eigenvalues come in pairs $\pm i\lambda_j$ ($j=1,\ldots n$) and the corresponding eigenvectors in pairs $e_j\pm if_j$ with real $e_j,f_j$ (the proof in the link considers the eigenvalues/eigenvectors of $M^{-1}J$ instead). The claim in the proof is now that $\{e_j,f_j\}_j$ forms an $M$-orthonormal basis. If this is the case, one can easily see that $\delta_{jk}=\langle e_j,Me_k\rangle=-\lambda_k\sigma(e_j,f_k)$ and $0=\langle e_j,Mf_k\rangle=\lambda_k\sigma(e_j,e_k)$ and similarly for $f_k$, hence the basis is indeed $M$-orthogonal and symplectic after normalization by $\sqrt{\lambda_j}$. The matrix $S$ should then consist of the normalized $f_j$ and $e_j$ as columns. Q1: Why is $\{e_j,f_j\}_j$ an orthonormal basis w.r.t to $M$? Matlab suggests that most of the times, it is, but I seem to miss something fundamental, because I don't see it. Why most of the times? Eigenvalue multiplicities seem to play a role here: Q2: If $\lambda_j\neq \lambda_k$ always, then Q1 seems to be true. If there are some eigenvalue multiplicities, this seems to be wrong, probably because of the non-uniqueness of eigenvectors. How can I fix this? I suspect that some Gram-Schmidt wrt. $M$ is necessary, but since I can't answer Q1, I can't see whether this does the trick.",,"['linear-algebra', 'bilinear-form', 'symplectic-linear-algebra']"
93,Proving that linear functionals are linearly independent if and only if their kernels are different,Proving that linear functionals are linearly independent if and only if their kernels are different,,"Here is a problem I'm trying to solve: Let's assume that $\dim X < \infty$. Show that 2 non-zero linear functionals $a^*, b^* \in X^*$ are linearly independent if and only if $\ker a^* \neq \ker b^*$. I think I managed to do the implication from left side to right side: ($\rightarrow$) $a^*, b^*$ are linearly independent if and only if for all $x \in X$ we have $\alpha a^*(x) + \beta b^*(x) = 0$ if and only if $\alpha=\beta=0$ where $\alpha, \beta$ are scalars. But if $\ker a^* = \ker b^*$, then for some $x' \in \ker a^*$ we have $a^*(x') = b^*(x')=0$ so for $\alpha=\beta=1$ we get $a^*(x') + b^*(x') = 0$ which is a contradition. So $\ker a^* \neq \ker b^*$ But I really don't know where to start when trying to prove the implication in the other way. The information that $\ker a^* \neq \ker b^*$ seems like it's not enough to say whether $a^*,b^*$ are linearly independent. So, is there a way to prove that using only that information?","Here is a problem I'm trying to solve: Let's assume that $\dim X < \infty$. Show that 2 non-zero linear functionals $a^*, b^* \in X^*$ are linearly independent if and only if $\ker a^* \neq \ker b^*$. I think I managed to do the implication from left side to right side: ($\rightarrow$) $a^*, b^*$ are linearly independent if and only if for all $x \in X$ we have $\alpha a^*(x) + \beta b^*(x) = 0$ if and only if $\alpha=\beta=0$ where $\alpha, \beta$ are scalars. But if $\ker a^* = \ker b^*$, then for some $x' \in \ker a^*$ we have $a^*(x') = b^*(x')=0$ so for $\alpha=\beta=1$ we get $a^*(x') + b^*(x') = 0$ which is a contradition. So $\ker a^* \neq \ker b^*$ But I really don't know where to start when trying to prove the implication in the other way. The information that $\ker a^* \neq \ker b^*$ seems like it's not enough to say whether $a^*,b^*$ are linearly independent. So, is there a way to prove that using only that information?",,"['linear-algebra', 'functional-analysis']"
94,Notation in formula for tensor product of Hadamard matrix,Notation in formula for tensor product of Hadamard matrix,,"I'm having trouble understanding the notation used in a linear algebra exercise (it's exercise 2.33 of Nielsen and Chuang's ""Quantum Computation and Quantum Information"", page 74). The exercise gives the Hadamard matrix $$H = \frac{1}{\sqrt{2}} \Bigl[ (|0\rangle + |1\rangle)\langle 0| + (|0\rangle - |1\rangle)\langle 1|  \Bigr]$$ and asks us to show that $$H^{\otimes n} = \frac{1}{\sqrt{2^n}} \sum_{x,y} (-1)^{x \cdot y} |x \rangle \langle y|$$ My understanding is that $x$ and $y$ in the sum run through the basis vectors for the tensor product space, but my problem is that I don't understand what they're supposed to mean in the exponent of $(-1)$. I thought I was supposed to interpret them in base $2$ (""$00$"" as $0$, ""$10$"" as $2$, etc.), but then then the formula doesn't seem to work. For example, taking $n=2$ and computing the tensor product using the matrix representation of $H$: $$H^{\otimes 2} = H \otimes H = \frac{1}{2} \left[ \begin{array}{rr}  1 \left[ \begin{array}{rr} 1&1 \\ 1&-1 \end{array} \right] &  1 \left[ \begin{array}{rr} 1&1 \\ 1&-1 \end{array} \right] \\  1 \left[ \begin{array}{rr} 1&1 \\ 1&-1 \end{array} \right] & -1 \left[ \begin{array}{rr} 1&1 \\ 1&-1 \end{array} \right] \end{array} \right] = \frac{1}{2} \left[ \begin{array}{rrrr}  1 &  1 &  1 &  1 \\  1 & -1 &  1 & -1 \\  1 &  1 & -1 & -1 \\  1 & -1 & -1 &  1 \end{array} \right]$$ But, from the formula given in the book (and my probably mistaken interpretation of $x$ and $y$), the coefficient at, say, $|10 \rangle \langle 10|$ should be $(-1)^{2 \cdot 2} = 1$, but the corresponding coefficient in the matrix is $-1$. I feel I'm being dumb and missing something very simple, but I can't see it. What am I doing wrong?","I'm having trouble understanding the notation used in a linear algebra exercise (it's exercise 2.33 of Nielsen and Chuang's ""Quantum Computation and Quantum Information"", page 74). The exercise gives the Hadamard matrix $$H = \frac{1}{\sqrt{2}} \Bigl[ (|0\rangle + |1\rangle)\langle 0| + (|0\rangle - |1\rangle)\langle 1|  \Bigr]$$ and asks us to show that $$H^{\otimes n} = \frac{1}{\sqrt{2^n}} \sum_{x,y} (-1)^{x \cdot y} |x \rangle \langle y|$$ My understanding is that $x$ and $y$ in the sum run through the basis vectors for the tensor product space, but my problem is that I don't understand what they're supposed to mean in the exponent of $(-1)$. I thought I was supposed to interpret them in base $2$ (""$00$"" as $0$, ""$10$"" as $2$, etc.), but then then the formula doesn't seem to work. For example, taking $n=2$ and computing the tensor product using the matrix representation of $H$: $$H^{\otimes 2} = H \otimes H = \frac{1}{2} \left[ \begin{array}{rr}  1 \left[ \begin{array}{rr} 1&1 \\ 1&-1 \end{array} \right] &  1 \left[ \begin{array}{rr} 1&1 \\ 1&-1 \end{array} \right] \\  1 \left[ \begin{array}{rr} 1&1 \\ 1&-1 \end{array} \right] & -1 \left[ \begin{array}{rr} 1&1 \\ 1&-1 \end{array} \right] \end{array} \right] = \frac{1}{2} \left[ \begin{array}{rrrr}  1 &  1 &  1 &  1 \\  1 & -1 &  1 & -1 \\  1 &  1 & -1 & -1 \\  1 & -1 & -1 &  1 \end{array} \right]$$ But, from the formula given in the book (and my probably mistaken interpretation of $x$ and $y$), the coefficient at, say, $|10 \rangle \langle 10|$ should be $(-1)^{2 \cdot 2} = 1$, but the corresponding coefficient in the matrix is $-1$. I feel I'm being dumb and missing something very simple, but I can't see it. What am I doing wrong?",,"['linear-algebra', 'quantum-computation']"
95,Some questions about the pseudoinverse of a matrix,Some questions about the pseudoinverse of a matrix,,"For every mxn-matrix A with real entries, there exist a unique nxm-matrix B, also   with real entries, such that $$ABA = A$$ $$BAB = B$$ $$AB = (AB)^T$$ $$BA = (BA)^T$$ B is called the pseudoinverse of A. There is also a complex version, but I am only interested in the real one. Now my questions : If A has rational entries, must B also have rational entries ? How can I calculate the pseudoinverse of a matrix with PARI/GP ? Is there a simple method to calculate the pseudoinverse by hand for small matrices ? Under which condition are the entries of the pseudoinverse integers ? I know some special properties, for instance, that for invertible square  matrices A, the pseudoinverse is simply $A^{-1}$ , or that the pseudoinverse of a zero-matrix is its transposition, but I have not much experience with general pseudoinverses.","For every mxn-matrix A with real entries, there exist a unique nxm-matrix B, also   with real entries, such that $$ABA = A$$ $$BAB = B$$ $$AB = (AB)^T$$ $$BA = (BA)^T$$ B is called the pseudoinverse of A. There is also a complex version, but I am only interested in the real one. Now my questions : If A has rational entries, must B also have rational entries ? How can I calculate the pseudoinverse of a matrix with PARI/GP ? Is there a simple method to calculate the pseudoinverse by hand for small matrices ? Under which condition are the entries of the pseudoinverse integers ? I know some special properties, for instance, that for invertible square  matrices A, the pseudoinverse is simply $A^{-1}$ , or that the pseudoinverse of a zero-matrix is its transposition, but I have not much experience with general pseudoinverses.",,"['linear-algebra', 'matrices', 'inverse', 'pseudoinverse']"
96,Non-orthogonal projections summing to 1 in infinite-dimensional space,Non-orthogonal projections summing to 1 in infinite-dimensional space,,"Consider projection operators $\rho_1,\ldots,\rho_k$ defined on vector space $V$ over field of characteristic $0$, such that $$ \rho_1+\cdots+\rho_k = 1 $$ Projections $\rho, \pi$ are said to be orthogonal , if $\rho\circ\pi=\pi\circ\rho=0$. Question: Are  $\rho_1,\ldots,\rho_k$ necessarily pairwise orthogonal? If $V$ has finite dimension, the answer is yes. I expect it not to be the case in general, but I can't seem to come up with a counterexample. Bonus question: What about possibly infinite set of projections, where  $$ \sum_{\rho\in P} \rho = 1 $$ is understood as ""for each $v\in V$, only finitely many of $\rho v$ are nonzero, and their sum is $v$"" ?","Consider projection operators $\rho_1,\ldots,\rho_k$ defined on vector space $V$ over field of characteristic $0$, such that $$ \rho_1+\cdots+\rho_k = 1 $$ Projections $\rho, \pi$ are said to be orthogonal , if $\rho\circ\pi=\pi\circ\rho=0$. Question: Are  $\rho_1,\ldots,\rho_k$ necessarily pairwise orthogonal? If $V$ has finite dimension, the answer is yes. I expect it not to be the case in general, but I can't seem to come up with a counterexample. Bonus question: What about possibly infinite set of projections, where  $$ \sum_{\rho\in P} \rho = 1 $$ is understood as ""for each $v\in V$, only finitely many of $\rho v$ are nonzero, and their sum is $v$"" ?",,['linear-algebra']
97,What is a coordinate system?,What is a coordinate system?,,"What's a coordinate system? I was watching a Khan video about coordinates with respect to orthonormal bases . It is mentioned that orthonormal bases make for ""good coordinate systems"". I didn't quite digest that - up to this point, I was under the impression that a ""coordinate system"" was just a set of vectors: $\mathbb{R}^2$ would be the set of all vectors with two real numbers, for instance. Now I am told that a basis is also a coordinate system. So clearly my understanding of ""coordinate system"" is not clear. What IS a coordinate system, exactly? Can any set of vectors be a coordinate system? Is a vector space and a subspace considered coordinate systems? (I guess yes, because $\mathbb{R}^n$ is a vector space).","What's a coordinate system? I was watching a Khan video about coordinates with respect to orthonormal bases . It is mentioned that orthonormal bases make for ""good coordinate systems"". I didn't quite digest that - up to this point, I was under the impression that a ""coordinate system"" was just a set of vectors: $\mathbb{R}^2$ would be the set of all vectors with two real numbers, for instance. Now I am told that a basis is also a coordinate system. So clearly my understanding of ""coordinate system"" is not clear. What IS a coordinate system, exactly? Can any set of vectors be a coordinate system? Is a vector space and a subspace considered coordinate systems? (I guess yes, because $\mathbb{R}^n$ is a vector space).",,"['linear-algebra', 'coordinate-systems']"
98,Moore-Penrose Pseudo-inverse of a matrix on adding 1 new row/column,Moore-Penrose Pseudo-inverse of a matrix on adding 1 new row/column,,"Given that I know the pseudo-inverse of a matrix(not necessarily a square matrix), how to calculate the pseudo-inverse of the matrix I get by adding a single row/column to the original matrix? i.e, Is there any way to compute the MP inverse of [A v] if I know the MP inverse of A? (The new matrix is just the original matrix A with an additional column v)","Given that I know the pseudo-inverse of a matrix(not necessarily a square matrix), how to calculate the pseudo-inverse of the matrix I get by adding a single row/column to the original matrix? i.e, Is there any way to compute the MP inverse of [A v] if I know the MP inverse of A? (The new matrix is just the original matrix A with an additional column v)",,"['linear-algebra', 'matrices', 'inverse', 'svd']"
99,Adding Elements to Diagonal of Symmetric Matrix to Ensure Positive Definiteness.,Adding Elements to Diagonal of Symmetric Matrix to Ensure Positive Definiteness.,,"I have a symmetric matrix $A$, which has zeroes all along the diagonal i.e. $A_{ii}=0$. I cannot change the off diagonal elements of this matrix, I can only change the diagonal elements. I need this matrix to be positive definite. One way to ensure this is as follows: Let $\lambda'$ by the absolute value     of the most negative eigenvalue and transform $A\mapsto A + \lambda'I_{na}$. Notice this leaves the off-diagonal elements unchanged, but now it is positive definite: $(A+\lambda'I_{na})x_{i}=(\lambda_{i}+\lambda')x_{i}=\lambda_{i}^{new}x_{i}$, where $(x_{i},\lambda_{i})$ are an eigenpair. The eigenvalues of the new matrix formed by adding $\lambda'$ to the diagonal are all positive. I fear that this solution is sub-optimal - in my application I want to add only as much as I need to and no more. I would like to know if I can ensure positive definiteness by more generally performing $A+D$ where $D$ is some diagonal matrix. [Application: Statistics. $A$ is related to the covariance of some augmented data. I want it to be as small as possible so as to reduce the leverage of the missing data. EDIT: Changed $X_{a}^{T}X_{a}$ to $A$. I was ahead of myself, once I get my desired positive definite matrix I want to set $A_{pd}=A+D=X^{T}_{a}X_{a}$ and take the cholesky decomposition to get $X_{a}$.","I have a symmetric matrix $A$, which has zeroes all along the diagonal i.e. $A_{ii}=0$. I cannot change the off diagonal elements of this matrix, I can only change the diagonal elements. I need this matrix to be positive definite. One way to ensure this is as follows: Let $\lambda'$ by the absolute value     of the most negative eigenvalue and transform $A\mapsto A + \lambda'I_{na}$. Notice this leaves the off-diagonal elements unchanged, but now it is positive definite: $(A+\lambda'I_{na})x_{i}=(\lambda_{i}+\lambda')x_{i}=\lambda_{i}^{new}x_{i}$, where $(x_{i},\lambda_{i})$ are an eigenpair. The eigenvalues of the new matrix formed by adding $\lambda'$ to the diagonal are all positive. I fear that this solution is sub-optimal - in my application I want to add only as much as I need to and no more. I would like to know if I can ensure positive definiteness by more generally performing $A+D$ where $D$ is some diagonal matrix. [Application: Statistics. $A$ is related to the covariance of some augmented data. I want it to be as small as possible so as to reduce the leverage of the missing data. EDIT: Changed $X_{a}^{T}X_{a}$ to $A$. I was ahead of myself, once I get my desired positive definite matrix I want to set $A_{pd}=A+D=X^{T}_{a}X_{a}$ and take the cholesky decomposition to get $X_{a}$.",,"['linear-algebra', 'optimization', 'convex-optimization', 'semidefinite-programming']"
