,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Area of intersection of an ellipsoid and plane,Area of intersection of an ellipsoid and plane,,"The problem I am working on asks: Ellipsoid $\frac{x^2}{3}+\frac{y^2}{2}+\frac{z^2}{1}=1$ intersect with plane $x+y+z=0$ , what is the area of the intersection? My idea is basically as same as this answer , and using Lagrange's multiplier to find maximum and minimum of $x^2+y^2+z^2$ . So here is the function constructed: $$L(x,y,z,\lambda,\mu)=x^2+y^2+z^2+\lambda(\frac{x^2}{3}+\frac{y^2}{2}+\frac{z^2}{1}-1)+\mu(x+y+z)$$ Then taking derivative: $$L_x(x,y,z,\lambda,\mu)=2x+\frac{2}{3}{\lambda}x+\mu=0$$ $$L_y(x,y,z,\lambda,\mu)=2y+{\lambda}y+\mu=0$$ $$L_z(x,y,z,\lambda,\mu)=2z+2{\lambda}z+\mu=0$$ $$L_\lambda(x,y,z,\lambda,\mu)=\frac{x^2}{3}+\frac{y^2}{2}+\frac{z^2}{1}-1=0$$ $$L_\mu(x,y,z,\lambda,\mu)=x+y+z=0$$ I failed to find the solution of this on my self, so I had to find answer in the solution manual. It says ""Observe this five equation, we can learn that $\lambda+x^2+y^2+z^2=0$ "". But I am quite confused where this conclusion come from, I tried to do something like eliminate $\lambda$ from these equations but get nothing. Also, I do not understand why in this particular case, the Lagrange's multiplier $\lambda$ is equal to the opposite of the function being optimized (i.e. $x^2+y^2+z^2$ ), will this also works on any ellipsoid intersect with any plane? Could anyone expand this in detail? Things below, I believe is not related to my problem, however, I think I'd better describe the full solution in case anyone find it useful. Eliminate $\mu$ from first three equations, combine with the last equation, we get $\left[ \begin{array}{ccc}   6+2\lambda&-6-3\lambda&0\\   0&2+\lambda&-2-2\lambda\\   1&1&1 \end{array} \right]\left[\begin{array}{c}   x\\y\\z \end{array}\right]=\left[\begin{array}{c}   0\\0\\0 \end{array}\right]$ , because none of $x$ , $y$ , $z$ equal zero (otherwise the fourth equation is not satisfied) the determinate of the matrix must be $0$ , using Vieta theorem, $\lambda_1\lambda_2=3$ . Because of the equation I am asking, two of the half-axis equals $\sqrt{|\lambda_1|}$ and $\sqrt{|\lambda_2|}$ , the area is found.","The problem I am working on asks: Ellipsoid intersect with plane , what is the area of the intersection? My idea is basically as same as this answer , and using Lagrange's multiplier to find maximum and minimum of . So here is the function constructed: Then taking derivative: I failed to find the solution of this on my self, so I had to find answer in the solution manual. It says ""Observe this five equation, we can learn that "". But I am quite confused where this conclusion come from, I tried to do something like eliminate from these equations but get nothing. Also, I do not understand why in this particular case, the Lagrange's multiplier is equal to the opposite of the function being optimized (i.e. ), will this also works on any ellipsoid intersect with any plane? Could anyone expand this in detail? Things below, I believe is not related to my problem, however, I think I'd better describe the full solution in case anyone find it useful. Eliminate from first three equations, combine with the last equation, we get , because none of , , equal zero (otherwise the fourth equation is not satisfied) the determinate of the matrix must be , using Vieta theorem, . Because of the equation I am asking, two of the half-axis equals and , the area is found.","\frac{x^2}{3}+\frac{y^2}{2}+\frac{z^2}{1}=1 x+y+z=0 x^2+y^2+z^2 L(x,y,z,\lambda,\mu)=x^2+y^2+z^2+\lambda(\frac{x^2}{3}+\frac{y^2}{2}+\frac{z^2}{1}-1)+\mu(x+y+z) L_x(x,y,z,\lambda,\mu)=2x+\frac{2}{3}{\lambda}x+\mu=0 L_y(x,y,z,\lambda,\mu)=2y+{\lambda}y+\mu=0 L_z(x,y,z,\lambda,\mu)=2z+2{\lambda}z+\mu=0 L_\lambda(x,y,z,\lambda,\mu)=\frac{x^2}{3}+\frac{y^2}{2}+\frac{z^2}{1}-1=0 L_\mu(x,y,z,\lambda,\mu)=x+y+z=0 \lambda+x^2+y^2+z^2=0 \lambda \lambda x^2+y^2+z^2 \mu \left[
\begin{array}{ccc}
  6+2\lambda&-6-3\lambda&0\\
  0&2+\lambda&-2-2\lambda\\
  1&1&1
\end{array}
\right]\left[\begin{array}{c}
  x\\y\\z
\end{array}\right]=\left[\begin{array}{c}
  0\\0\\0
\end{array}\right] x y z 0 \lambda_1\lambda_2=3 \sqrt{|\lambda_1|} \sqrt{|\lambda_2|}","['geometry', 'multivariable-calculus', 'lagrange-multiplier']"
1,"Is the cartesian product of a bounded set with a set of points, both in $\mathbb{R}^{n}$ Jordan-Measurable over $\mathbb{R}^{2n}$?","Is the cartesian product of a bounded set with a set of points, both in  Jordan-Measurable over ?",\mathbb{R}^{n} \mathbb{R}^{2n},"Let $A=\{\vec{x_{1}},\dots,\vec{x_{k}}\}\subset\mathbb{R}^{n}$ . And let $B\subset\mathbb{R}^{n}$ a bounded set. Then $A\times B$ is Jordan-Measurable on $\mathbb{R}^{2n}$ ? In the beginning, I thought that the counterexample could be $\{\frac{1}{2}\}\times ((\mathbb{Q}\times\mathbb{Q})\bigcap([0,1]\times [0,1]))$ But then I realized this doesn't works. I started to think that maybe this is true. I will appreciate any hint. $\vec{x_{i}}$ is a vector on $\mathbb{R}^{n}$","Let . And let a bounded set. Then is Jordan-Measurable on ? In the beginning, I thought that the counterexample could be But then I realized this doesn't works. I started to think that maybe this is true. I will appreciate any hint. is a vector on","A=\{\vec{x_{1}},\dots,\vec{x_{k}}\}\subset\mathbb{R}^{n} B\subset\mathbb{R}^{n} A\times B \mathbb{R}^{2n} \{\frac{1}{2}\}\times ((\mathbb{Q}\times\mathbb{Q})\bigcap([0,1]\times [0,1])) \vec{x_{i}} \mathbb{R}^{n}","['real-analysis', 'calculus', 'measure-theory', 'multivariable-calculus']"
2,Calculating second partial derivate of a multivariable function.,Calculating second partial derivate of a multivariable function.,,"I have a doubt resolving Apostol Vol.2, sec 22, prob 4: The equations $u =f(x, y), x = X(s, t), y = Y(s, t)$ define $u$ as a function of $s$ and $t$ , say $u = F(s, t).$ Find formulas for the partial derivatives $\frac{\partial^2F}{\partial s\partial t}$ and $\frac{\partial^2F}{\partial t^2}$ I was able to get the formula for $\frac{\partial^2F}{\partial t^2}$ , but for $\frac{\partial^2F}{\partial s\partial t}$ I seem to have troubles, here's my solution: $\frac{\partial^2F}{\partial s\partial t}=\frac{\partial f}{\partial x}\frac{\partial^2X}{\partial s\partial t}+\frac{\partial^2f}{\partial x^2}\frac{\partial X}{\partial t}\frac{\partial X}{\partial s}+\frac{\partial^2f}{\partial y\partial x}\frac{\partial Y}{\partial s}\frac{\partial X}{\partial t}+\frac{\partial f}{\partial y}\frac{\partial^2Y}{\partial s\partial t}+\frac{\partial^2f}{\partial x\partial y}\frac{\partial X}{\partial s}\frac{\partial Y}{\partial t}+\frac{\partial^2f}{\partial y^2}\frac{\partial Y}{\partial s}\frac{\partial Y}{\partial t}$ But I think this is wrong given a solution I found on the internet, is my solution right or where could I have gone wrong? Thanks.","I have a doubt resolving Apostol Vol.2, sec 22, prob 4: The equations define as a function of and , say Find formulas for the partial derivatives and I was able to get the formula for , but for I seem to have troubles, here's my solution: But I think this is wrong given a solution I found on the internet, is my solution right or where could I have gone wrong? Thanks.","u =f(x, y), x = X(s, t), y = Y(s, t) u s t u = F(s, t). \frac{\partial^2F}{\partial s\partial t} \frac{\partial^2F}{\partial t^2} \frac{\partial^2F}{\partial t^2} \frac{\partial^2F}{\partial s\partial t} \frac{\partial^2F}{\partial s\partial t}=\frac{\partial f}{\partial x}\frac{\partial^2X}{\partial s\partial t}+\frac{\partial^2f}{\partial x^2}\frac{\partial X}{\partial t}\frac{\partial X}{\partial s}+\frac{\partial^2f}{\partial y\partial x}\frac{\partial Y}{\partial s}\frac{\partial X}{\partial t}+\frac{\partial f}{\partial y}\frac{\partial^2Y}{\partial s\partial t}+\frac{\partial^2f}{\partial x\partial y}\frac{\partial X}{\partial s}\frac{\partial Y}{\partial t}+\frac{\partial^2f}{\partial y^2}\frac{\partial Y}{\partial s}\frac{\partial Y}{\partial t}","['multivariable-calculus', 'partial-derivative', 'chain-rule']"
3,"Determine the point on the plane $4x-2y+z=1$ that is closest to the point $(-2, -1, 5)$",Determine the point on the plane  that is closest to the point,"4x-2y+z=1 (-2, -1, 5)","Determine the point on the plane $4x-2y+z=1$ that is closest to the point $(-2, -1, 5)$ . This question is from Pauls's Online Math Notes. He starts by defining a distance function: $z = 1 - 4x + 2y$ $d(x, y) = \sqrt{(x + 2)^2 + (y + 1)^2 + (-4 -4x + 2y)^2}$ However, at this point, to make the calculus simpler he finds the partial derivatives of $d^2$ instead of $d$ . Why does this give you the same answer?","Determine the point on the plane that is closest to the point . This question is from Pauls's Online Math Notes. He starts by defining a distance function: However, at this point, to make the calculus simpler he finds the partial derivatives of instead of . Why does this give you the same answer?","4x-2y+z=1 (-2, -1, 5) z = 1 - 4x + 2y d(x, y) = \sqrt{(x + 2)^2 + (y + 1)^2 + (-4 -4x + 2y)^2} d^2 d","['multivariable-calculus', 'partial-derivative']"
4,The volume as a probability measure: Stuck in the proof,The volume as a probability measure: Stuck in the proof,,"I have troubles proving the fact that if we define $Vol(A)=\idotsint 1_{A}(x_1,\dots,x_n) \,dx_1 \dots dx_n$ and $\mathbb{P}(B)=\frac{Vol(A\bigcap B)}{Vol(A)}$ then $\mathbb{P}(\bigcup\limits_{n=1}^{\infty} B_{n})= \sum_{n=1}^{\infty} P(B_{k})$ if $B_{1}, B_{2}\dots$ are disjoint. I'm starting with multivariable calculus, so I don't know how to formalize this. I know that this happens because the function $1_{A}=1$ iff what we want to integrate is part of the set, and it makes sense that is the sum because we are not ""counting twice"" the same points, because the fact that our $B_{k}$ sets are disjoints, but as I already said, I don't know how to work with integrals in $\mathbb{R}^{n}$ so it is hard to me to write this proof in a formal way. I will appreciate any help","I have troubles proving the fact that if we define and then if are disjoint. I'm starting with multivariable calculus, so I don't know how to formalize this. I know that this happens because the function iff what we want to integrate is part of the set, and it makes sense that is the sum because we are not ""counting twice"" the same points, because the fact that our sets are disjoints, but as I already said, I don't know how to work with integrals in so it is hard to me to write this proof in a formal way. I will appreciate any help","Vol(A)=\idotsint 1_{A}(x_1,\dots,x_n) \,dx_1 \dots dx_n \mathbb{P}(B)=\frac{Vol(A\bigcap B)}{Vol(A)} \mathbb{P}(\bigcup\limits_{n=1}^{\infty} B_{n})= \sum_{n=1}^{\infty} P(B_{k}) B_{1}, B_{2}\dots 1_{A}=1 B_{k} \mathbb{R}^{n}","['real-analysis', 'probability', 'probability-theory', 'measure-theory', 'multivariable-calculus']"
5,Partial derivatives of a function of 1 independent variable,Partial derivatives of a function of 1 independent variable,,"I have a function $f$ of one variable $t$ , but I will write it in a funny way: using a second function $g$ as an intermediary: $$ f(g(t),t)=tg(t)  $$ where $$ g(t)=t $$ . The exact definitions of $g$ and $f$ don't matter too much, anything will do the trick. Before I proceed, note that I (knowing the definition of $g$ ) could write $f$ in several ways (dropping the parentheses): $$ f=tg\qquad  f=t^2\qquad  f=g^2\qquad  f=t^3/g $$ you can see that this could go on forever. The issue arrives when I want to take partial derivatives. Going in the same order as before: $$ \frac{\partial f}{\partial t}=g\qquad  \frac{\partial f}{\partial t}=2t\qquad  \frac{\partial f}{\partial t}=0\qquad  \frac{\partial f}{\partial t}=3t^2/g $$ and I could do the same with partials w.r.t $g$ . Now I recognize that because $f$ is only a function of $t$ , I should even be taking partials with respect to it, but by the way I defined $f$ using $g$ , the multivariable chain rule: $$ \frac{df}{dt}=\frac{\partial f}{\partial g}\frac{dg}{dt}+\frac{\partial f}{\partial t} $$ still requires a definition of the partial w.r.t. $t$ . It should be noted that the total derivative w.r.t. $t$ (which should be $2t$ as $f(t)=t^2$ ) is retrieved from the multivariable chain rule if we keep the definitions of $f$ as a function of $g$ and $t$ consistent across the equation, i.e. if we just pick a definition and stick to it, it doesn't matter one, the total derivative will work. What's going on here, all I am doing is variable manipulations, but somehow the calculus seems intrinsically tied to the particular definitions of $f$ in terms of the dependent variable that I just made up. In a sense that is obvious. But Still. Am I doing something I am not allowed to do. Am I miscalculating something. Am I misinterpreting something. Obviously the partials aren't well defined if the variables are not independent. But there is more to it than that. Though this came up in the context of Lagrangian mechanics, where we regularly evaluate partials w.r.t. ""functions"" that solely depend on $t$ (I suspect there is something variation-y about that stuff though), the problem is easy to state, only relying on beginner-level calculus, and has me stumped. Any help is appreciated :)","I have a function of one variable , but I will write it in a funny way: using a second function as an intermediary: where . The exact definitions of and don't matter too much, anything will do the trick. Before I proceed, note that I (knowing the definition of ) could write in several ways (dropping the parentheses): you can see that this could go on forever. The issue arrives when I want to take partial derivatives. Going in the same order as before: and I could do the same with partials w.r.t . Now I recognize that because is only a function of , I should even be taking partials with respect to it, but by the way I defined using , the multivariable chain rule: still requires a definition of the partial w.r.t. . It should be noted that the total derivative w.r.t. (which should be as ) is retrieved from the multivariable chain rule if we keep the definitions of as a function of and consistent across the equation, i.e. if we just pick a definition and stick to it, it doesn't matter one, the total derivative will work. What's going on here, all I am doing is variable manipulations, but somehow the calculus seems intrinsically tied to the particular definitions of in terms of the dependent variable that I just made up. In a sense that is obvious. But Still. Am I doing something I am not allowed to do. Am I miscalculating something. Am I misinterpreting something. Obviously the partials aren't well defined if the variables are not independent. But there is more to it than that. Though this came up in the context of Lagrangian mechanics, where we regularly evaluate partials w.r.t. ""functions"" that solely depend on (I suspect there is something variation-y about that stuff though), the problem is easy to state, only relying on beginner-level calculus, and has me stumped. Any help is appreciated :)","f t g 
f(g(t),t)=tg(t) 
 
g(t)=t
 g f g f 
f=tg\qquad 
f=t^2\qquad 
f=g^2\qquad 
f=t^3/g
 
\frac{\partial f}{\partial t}=g\qquad 
\frac{\partial f}{\partial t}=2t\qquad 
\frac{\partial f}{\partial t}=0\qquad 
\frac{\partial f}{\partial t}=3t^2/g
 g f t f g 
\frac{df}{dt}=\frac{\partial f}{\partial g}\frac{dg}{dt}+\frac{\partial f}{\partial t}
 t t 2t f(t)=t^2 f g t f t","['calculus', 'multivariable-calculus']"
6,Calculate the volume determined by the paraboloid $4z=x^2+y^2$ and the planes $z=1$ and $z= \dfrac{1}{2}$. Also $y≤x$.,Calculate the volume determined by the paraboloid  and the planes  and . Also .,4z=x^2+y^2 z=1 z= \dfrac{1}{2} y≤x,"Calculate the volume determined by the paraboloid $4z=x^2+y^2$ and the planes $z=1$ and $z= \dfrac{1}{2}$ . Also $y≤x$ . I made a variable change: $x=r\cos \theta $ and $y=r\sin \theta$ then $z=\dfrac{r}{2}$ with $\dfrac{1}{4}≤r≤\dfrac{1}{2}$ and here is my first doubt: is $\theta$ going from $\dfrac{- \pi}{4}$ to $\dfrac{\pi}{4}$ ? because if I have two circles in the $xy$ plane centered on $(0,0)$ and I draw the line $y = x$ then the values ​​of $x$ less than or equal to $y$ are swept by the angle that goes from $\dfrac{- \pi}{4}$ to $\dfrac{\pi}{4}$ . Finally, I would like to consult what intergral I should use to obtain this volume and if it can be obtained only by a triple integral. Because I have seen cases in which a volume is obtained through a double integral and I don't understand how this is possible and when I can do it. I appreciate all help!","Calculate the volume determined by the paraboloid and the planes and . Also . I made a variable change: and then with and here is my first doubt: is going from to ? because if I have two circles in the plane centered on and I draw the line then the values ​​of less than or equal to are swept by the angle that goes from to . Finally, I would like to consult what intergral I should use to obtain this volume and if it can be obtained only by a triple integral. Because I have seen cases in which a volume is obtained through a double integral and I don't understand how this is possible and when I can do it. I appreciate all help!","4z=x^2+y^2 z=1 z= \dfrac{1}{2} y≤x x=r\cos \theta  y=r\sin \theta z=\dfrac{r}{2} \dfrac{1}{4}≤r≤\dfrac{1}{2} \theta \dfrac{- \pi}{4} \dfrac{\pi}{4} xy (0,0) y = x x y \dfrac{- \pi}{4} \dfrac{\pi}{4}","['calculus', 'integration', 'analysis', 'multivariable-calculus', 'volume']"
7,Bound angle between vectors on n-simplex,Bound angle between vectors on n-simplex,,"I'm trying to bound from above (tight as I can find) the angle between two vectors ( $\boldsymbol{x}, \boldsymbol{y} $ ) on the standard n-simplex in $\Bbb R^{n+1}$ ( $\sum_{k=0}^{n} x_k = 1$ , $x_k\geq0$ , and the same for $y_k$ ), given that distance between them is less than a constant $\epsilon $ (i.e $\| \boldsymbol{x} -\boldsymbol{y}\| \leq \epsilon)$ . My try so far: In general, the angle between vectors is: \begin{align}  \theta = \arccos \frac{\mathbf x \cdot \mathbf y}{\left\| \mathbf x \right\| \, \left\| \mathbf y \right\|} \end{align} and on the stranded simplex it satisfies $ 0\leq \theta \leq \pi/2$ , since the inner proudct in non-negative for vectors on the stranded simplex. $arccos$ is monotonic decreasing, so in order to bound (tightly) the angle, it suffice to bound from below the following function: \begin{align}  \frac{\mathbf x \cdot \mathbf y}{\left\| \mathbf x \right\| \, \left\| \mathbf y \right\|} \end{align} under the constraints that $\boldsymbol{x}$ and $\boldsymbol{y}$ are on the simplex, and the distance between them is at most $\epsilon$ . I am not sure how to do it, or how to procced from here. Thanks in advance!","I'm trying to bound from above (tight as I can find) the angle between two vectors ( ) on the standard n-simplex in ( , , and the same for ), given that distance between them is less than a constant (i.e . My try so far: In general, the angle between vectors is: and on the stranded simplex it satisfies , since the inner proudct in non-negative for vectors on the stranded simplex. is monotonic decreasing, so in order to bound (tightly) the angle, it suffice to bound from below the following function: under the constraints that and are on the simplex, and the distance between them is at most . I am not sure how to do it, or how to procced from here. Thanks in advance!","\boldsymbol{x}, \boldsymbol{y}  \Bbb R^{n+1} \sum_{k=0}^{n} x_k = 1 x_k\geq0 y_k \epsilon  \| \boldsymbol{x} -\boldsymbol{y}\| \leq \epsilon) \begin{align} 
\theta = \arccos \frac{\mathbf x \cdot \mathbf y}{\left\| \mathbf x \right\| \, \left\| \mathbf y \right\|}
\end{align}  0\leq \theta \leq \pi/2 arccos \begin{align} 
\frac{\mathbf x \cdot \mathbf y}{\left\| \mathbf x \right\| \, \left\| \mathbf y \right\|}
\end{align} \boldsymbol{x} \boldsymbol{y} \epsilon","['calculus', 'linear-algebra', 'geometry', 'multivariable-calculus', 'differential-geometry']"
8,Surface area of an oblate spheroid using gaussian quadrature,Surface area of an oblate spheroid using gaussian quadrature,,"I want to compute the surface area of an oblate spheroid using gaussian quadrature, the parametrization of the oblate spheroid is given by: $$x = a \cdot \sin\theta \cdot \cos \phi \\ y = a \cdot \sin\theta \cdot \sin \phi \\ z = b \cdot \cos\theta $$ Where $b<a$ , in order to compute this integral I am using a Gauss-legendre quadrature to compute the points and the weights for the integral for the first octant on the spheroid so $\theta = [0,\pi/2]$ , $\phi = [0,\pi/2]$ . So using the weights and the nodes of a Gauss-Legendre quadrature of order $n$ I can define the weights and the nodes in $\theta$ and $\phi$ by: $$X_{\theta/\phi} = \frac12 \cdot(X_{\text{Gauss-Leg}} + 1)\cdot \frac{\pi}2 \\ W_{\theta/\phi} = \frac12\cdot W_{\text{Gauss-Leg}} \cdot\frac{\pi}2$$ So having this defined, I can compute the surface integral as: $$\int_{0}^{\pi/2}\int_{0}^{\pi/2} dS = \sum\sum W_{\theta}\cdot W_{\phi}\cdot dS $$ Where I think $dS = a^2\cdot b \cdot\rho^2 \cdot \sin\theta \cdot d\theta \cdot d\phi$ . I find a bit confusing the $\rho$ here but I have tried using the standard definition of $\rho$ as: $$\rho = \sqrt{x^2 + y^2 + z^2} = \sqrt{a^2\cdot \sin^2\theta + b^2 \cdot \cos^2\theta} $$ So the expresion that I am using for solving this integral is: $$\int_{0}^{\pi/2}\int_{0}^{\pi/2} dS = \sum\sum W_{\theta}\cdot W_{\phi}\cdot a^2\cdot b \cdot(a^2\cdot \sin^2\theta + b^2 \cdot \cos^2\theta) \cdot \sin\theta $$ So I am doing something wrong since the surface of the oblate spheroid can be computed using: $$S =  2\pi \cdot \left(a^2 + \frac{b^2}{\sin(ae)} \ln\Bigl(\frac{1 + \sin(ae)}{\cos(ae)} \Bigr) \right)$$ with $ae = \arccos(b/a)$ . And the obtained result is not the same than the analytical surface. I have made an small python script that computes both results and prints them: import numpy as np from scipy.special import roots_legendre  #Define a and b b = 2. a = 100.  #Compute the Weights and nodes  x_phi, w_phi = roots_legendre(150) x_theta, w_theta = roots_legendre(100)  #Translate them x_phi = 0.5 * (x_phi + 1.) * np.pi/2. x_theta = 0.5 * (x_theta + 1.) * np.pi/2. w_phi = 0.5 * w_phi * np.pi/2. w_theta = 0.5 * w_theta * np.pi/2.  #Compute the integral  integral = 0 for i in xrange(len(x_phi)):     for j in xrange(len(x_theta)):         integral += w_phi[i] * w_theta[j] * a**2 * b  * (a**2 * np.sin(x_theta[j])**2 + b**2 * np.cos(x_theta[j])**2) * np.sin(x_theta[j])   print(""Estimated int: %f"" %(8*integral))  ae = np.arccos(b/a) surface = 2*np.pi*(a**2 + b**2/np.sin(ae) * np.log((1+np.sin(ae))/np.cos(ae)))  print(""Real int: %f"" %(surface)) So what am I doing wrong? (I have to mention that this is just a simple test, what I really want to do is to compute the surface integral of any arbitrary function on this spheroid)","I want to compute the surface area of an oblate spheroid using gaussian quadrature, the parametrization of the oblate spheroid is given by: Where , in order to compute this integral I am using a Gauss-legendre quadrature to compute the points and the weights for the integral for the first octant on the spheroid so , . So using the weights and the nodes of a Gauss-Legendre quadrature of order I can define the weights and the nodes in and by: So having this defined, I can compute the surface integral as: Where I think . I find a bit confusing the here but I have tried using the standard definition of as: So the expresion that I am using for solving this integral is: So I am doing something wrong since the surface of the oblate spheroid can be computed using: with . And the obtained result is not the same than the analytical surface. I have made an small python script that computes both results and prints them: import numpy as np from scipy.special import roots_legendre  #Define a and b b = 2. a = 100.  #Compute the Weights and nodes  x_phi, w_phi = roots_legendre(150) x_theta, w_theta = roots_legendre(100)  #Translate them x_phi = 0.5 * (x_phi + 1.) * np.pi/2. x_theta = 0.5 * (x_theta + 1.) * np.pi/2. w_phi = 0.5 * w_phi * np.pi/2. w_theta = 0.5 * w_theta * np.pi/2.  #Compute the integral  integral = 0 for i in xrange(len(x_phi)):     for j in xrange(len(x_theta)):         integral += w_phi[i] * w_theta[j] * a**2 * b  * (a**2 * np.sin(x_theta[j])**2 + b**2 * np.cos(x_theta[j])**2) * np.sin(x_theta[j])   print(""Estimated int: %f"" %(8*integral))  ae = np.arccos(b/a) surface = 2*np.pi*(a**2 + b**2/np.sin(ae) * np.log((1+np.sin(ae))/np.cos(ae)))  print(""Real int: %f"" %(surface)) So what am I doing wrong? (I have to mention that this is just a simple test, what I really want to do is to compute the surface integral of any arbitrary function on this spheroid)","x = a \cdot \sin\theta \cdot \cos \phi \\
y = a \cdot \sin\theta \cdot \sin \phi \\
z = b \cdot \cos\theta  b<a \theta = [0,\pi/2] \phi = [0,\pi/2] n \theta \phi X_{\theta/\phi} = \frac12 \cdot(X_{\text{Gauss-Leg}} + 1)\cdot \frac{\pi}2 \\
W_{\theta/\phi} = \frac12\cdot W_{\text{Gauss-Leg}} \cdot\frac{\pi}2 \int_{0}^{\pi/2}\int_{0}^{\pi/2} dS = \sum\sum W_{\theta}\cdot W_{\phi}\cdot dS  dS = a^2\cdot b \cdot\rho^2 \cdot \sin\theta \cdot d\theta \cdot d\phi \rho \rho \rho = \sqrt{x^2 + y^2 + z^2} = \sqrt{a^2\cdot \sin^2\theta + b^2 \cdot \cos^2\theta}  \int_{0}^{\pi/2}\int_{0}^{\pi/2} dS = \sum\sum W_{\theta}\cdot W_{\phi}\cdot a^2\cdot b \cdot(a^2\cdot \sin^2\theta + b^2 \cdot \cos^2\theta) \cdot \sin\theta  S =  2\pi \cdot \left(a^2 + \frac{b^2}{\sin(ae)} \ln\Bigl(\frac{1 + \sin(ae)}{\cos(ae)} \Bigr) \right) ae = \arccos(b/a)","['multivariable-calculus', 'numerical-methods', 'surface-integrals', 'quadrature']"
9,"$F$ is differentiable at $x_0$, but $S=\{ x \in \mathbb{R}^3 : F(x)=c\}$ is not smooth at $x_0$","is differentiable at , but  is not smooth at",F x_0 S=\{ x \in \mathbb{R}^3 : F(x)=c\} x_0,"Consider the surface defined implicitly by $x^3 - x^2y^2 + z^2 = 0$ . Does it have a well-defined tangent plane at the origin? Why or why not? The polynomial function $$F(x,y,z)=x^3 - x^2y^2 + z^2$$ is $C^1$ in its domain; in particular, it's differentiable at the origin. I thought that this was sufficient to claim that its level set at height $0$ is smooth and has a well-defined tangent plane at the origin. But it's not. Is this simply because $\nabla F(0,0,0)=0$ ? If that wasn't the case, the Implicit Function Theorem would imply that $S$ is the graph of some $C^1$ function near the origin, right? So, to be extremely methodical when answering theses questions, (1) If $\nabla F (x_0) \neq 0$ , then the level set is smooth at $x_0$ iff $F$ is $C^1$ at $x_0$ ; (2) If $\nabla F (x_0) = 0$ , we have to do algebra on the equation that defines the level set...? Or what?","Consider the surface defined implicitly by . Does it have a well-defined tangent plane at the origin? Why or why not? The polynomial function is in its domain; in particular, it's differentiable at the origin. I thought that this was sufficient to claim that its level set at height is smooth and has a well-defined tangent plane at the origin. But it's not. Is this simply because ? If that wasn't the case, the Implicit Function Theorem would imply that is the graph of some function near the origin, right? So, to be extremely methodical when answering theses questions, (1) If , then the level set is smooth at iff is at ; (2) If , we have to do algebra on the equation that defines the level set...? Or what?","x^3 - x^2y^2 + z^2 = 0 F(x,y,z)=x^3 - x^2y^2 + z^2 C^1 0 \nabla F(0,0,0)=0 S C^1 \nabla F (x_0) \neq 0 x_0 F C^1 x_0 \nabla F (x_0) = 0","['multivariable-calculus', 'smooth-functions']"
10,"Surface integral of $\left(0, \frac{-2yz}{r^4}, \frac{-r^2 + 2y^2}{r^4} \right)$ over an ellipse",Surface integral of  over an ellipse,"\left(0, \frac{-2yz}{r^4}, \frac{-r^2 + 2y^2}{r^4} \right)","On $\mathbb{R}^3 \setminus \{0\}$ , define the vector field $$F(x,y,z) = \left(0,  \frac{-2yz}{r^4}, \frac{-r^2 + 2y^2}{r^4} \right), $$ where $r = \sqrt{x^2+y^2+z^2}$ . Compute the integral of $F$ over the surface $$S: x^2 + y^2 + \frac{z^2}{4} = 1, z > 0. $$ The orientation does not matter. This question seems evil. How can one compute this in an easy way? Parametrizing the surface with spherical coordinates leads to a very difficult integral. And we cannot apply the divergence theorem, since we would have to close the surface, and then the domain enclosed by it will contain the origin.","On , define the vector field where . Compute the integral of over the surface The orientation does not matter. This question seems evil. How can one compute this in an easy way? Parametrizing the surface with spherical coordinates leads to a very difficult integral. And we cannot apply the divergence theorem, since we would have to close the surface, and then the domain enclosed by it will contain the origin.","\mathbb{R}^3 \setminus \{0\} F(x,y,z) = \left(0,  \frac{-2yz}{r^4}, \frac{-r^2 + 2y^2}{r^4} \right),  r = \sqrt{x^2+y^2+z^2} F S: x^2 + y^2 + \frac{z^2}{4} = 1, z > 0. ","['multivariable-calculus', 'vector-fields', 'surface-integrals']"
11,Directional derivative at local maximum,Directional derivative at local maximum,,"Suppose that $u(x, y)$ has a local maximum at $(0,0) .$ Show that for any direction $\frac{\partial u(0,0)}{\partial \vec{V}}=0$ . So I want to show that directional derivative is zero. So finally I get ${u(ah,bh) - u(0,0)}$ / $h$ as limit $h$ tends to zero ; how to proceed after that.",Suppose that has a local maximum at Show that for any direction . So I want to show that directional derivative is zero. So finally I get / as limit tends to zero ; how to proceed after that.,"u(x, y) (0,0) . \frac{\partial u(0,0)}{\partial \vec{V}}=0 {u(ah,bh) - u(0,0)} h h","['multivariable-calculus', 'derivatives', 'maxima-minima']"
12,Shortcut for finding local extrema of a multivariable function,Shortcut for finding local extrema of a multivariable function,,"I am trying to find the local extrema of this function: $f(x,y)=e^{\frac{1}{x^2 + 2 + \cos^2 y-2 \cos y}}$ I know what I am supposed to do (finding critical points and studying the Hessian). But, since the derivatives of this function are quite long, I was wondering if there might be a more efficient way to do it. I have thought that, since $g(t)=e^t $ strictly increasing and $h(t)=\frac{1}{t}$ is decreasing, I may just study the critical points of the function $l(x,y)=x^2 + 2 + \cos^2 y-2\cos y$ . However, I am not sure whether this would be correct, my main concern is with saddle points. It would be very helpful if anyone could tell me if that way of doing things is correct and, in case it is not correct, why so. Thanks","I am trying to find the local extrema of this function: I know what I am supposed to do (finding critical points and studying the Hessian). But, since the derivatives of this function are quite long, I was wondering if there might be a more efficient way to do it. I have thought that, since strictly increasing and is decreasing, I may just study the critical points of the function . However, I am not sure whether this would be correct, my main concern is with saddle points. It would be very helpful if anyone could tell me if that way of doing things is correct and, in case it is not correct, why so. Thanks","f(x,y)=e^{\frac{1}{x^2 + 2 + \cos^2 y-2 \cos y}} g(t)=e^t  h(t)=\frac{1}{t} l(x,y)=x^2 + 2 + \cos^2 y-2\cos y","['multivariable-calculus', 'maxima-minima']"
13,will in any case $\frac{\partial f}{\partial x}$ is Reciprocal of $\frac{\partial x}{\partial f}$,will in any case  is Reciprocal of,\frac{\partial f}{\partial x} \frac{\partial x}{\partial f},If $y$ is a Single Variable function of $x$ we have: $$\frac{dy}{dx}=\frac{1}{\frac{dx}{dy}}$$ Now coming to partial Derivatives i have tested for Polar Coordinates $$x=r \cos t$$ $$y=r \sin t$$ We have: $$\frac{\partial x}{\partial r}=\cos t=\frac{x}{\sqrt{x^2+y^2}}$$ Where as: $$\frac{\partial r}{\partial x}=\frac{\partial \left(\sqrt{x^2+y^2}\right)}{\partial x}=\frac{x}{\sqrt{x^2+y^2}}$$ Which are actually same? So is it not true about reciprocal relation in Partial derivatives or in which situations: $$\frac{\partial f}{\partial x}=\frac{1} {\frac{\partial x}{\partial f}}$$,If is a Single Variable function of we have: Now coming to partial Derivatives i have tested for Polar Coordinates We have: Where as: Which are actually same? So is it not true about reciprocal relation in Partial derivatives or in which situations:,y x \frac{dy}{dx}=\frac{1}{\frac{dx}{dy}} x=r \cos t y=r \sin t \frac{\partial x}{\partial r}=\cos t=\frac{x}{\sqrt{x^2+y^2}} \frac{\partial r}{\partial x}=\frac{\partial \left(\sqrt{x^2+y^2}\right)}{\partial x}=\frac{x}{\sqrt{x^2+y^2}} \frac{\partial f}{\partial x}=\frac{1} {\frac{\partial x}{\partial f}},"['multivariable-calculus', 'derivatives', 'chain-rule']"
14,Use Fubini's theorem to reverse the order of the double integral,Use Fubini's theorem to reverse the order of the double integral,,"Use Fubini's theorem to reverse the order of the double integral (the reversed integral may split into a sum of multiple pieces): a) $\int_0 ^4 \int_0 ^{\sqrt{x}} f(x,y) dy dx$ b) $\int_0 ^2 \int_x ^{3} f(x,y) dy dx$ c) $\int_{-1} ^2 \int_0 ^{1-y^2} f(x,y) dx dy$ In part $c)$ be careful about signs! My attempt a) Boundary of number at $x$ : $0\leq x\leq 4$ Boundary of function at $y$ : $0\leq y\leq \sqrt{x}.$ Now consider, boundary of number at $y$ : $0\leq y\leq 2$ boundary of function at $x$ : $y^2\leq x\leq 4.$ So by the Fubini's $\int_0 ^4 \int_0 ^{\sqrt{x}} f(x,y) dy dx=\int_0 ^2 \int_{y^2}^4 f(x,y) dx dy.$ b) Boundary of number at $x$ : $0\leq x\leq 2$ , boundary of function at $y$ : $x\leq y\leq 3.$ Now, consider boundary of number at $y$ : $0\leq y\leq 3$ boundary of function at $x$ : $0\leq x\leq y$ So, by the Fubini's theorem: $\int_0 ^2 \int_x ^{3} f(x,y) dy dx=\int_0 ^3 \int_0 ^{y} f(x,y) dy dx.$ c) I couldn't draw graph, may you draw and add here? May you check $a), b)$ and may you help for $c)$ ?","Use Fubini's theorem to reverse the order of the double integral (the reversed integral may split into a sum of multiple pieces): a) b) c) In part be careful about signs! My attempt a) Boundary of number at : Boundary of function at : Now consider, boundary of number at : boundary of function at : So by the Fubini's b) Boundary of number at : , boundary of function at : Now, consider boundary of number at : boundary of function at : So, by the Fubini's theorem: c) I couldn't draw graph, may you draw and add here? May you check and may you help for ?","\int_0 ^4 \int_0 ^{\sqrt{x}} f(x,y) dy dx \int_0 ^2 \int_x ^{3} f(x,y) dy dx \int_{-1} ^2 \int_0 ^{1-y^2} f(x,y) dx dy c) x 0\leq x\leq 4 y 0\leq y\leq \sqrt{x}. y 0\leq y\leq 2 x y^2\leq x\leq 4. \int_0 ^4 \int_0 ^{\sqrt{x}} f(x,y) dy dx=\int_0 ^2 \int_{y^2}^4 f(x,y) dx dy. x 0\leq x\leq 2 y x\leq y\leq 3. y 0\leq y\leq 3 x 0\leq x\leq y \int_0 ^2 \int_x ^{3} f(x,y) dy dx=\int_0 ^3 \int_0 ^{y} f(x,y) dy dx. a), b) c)",['real-analysis']
15,Does Euler's Theorem for homogeneous functions require continuous differentiability?,Does Euler's Theorem for homogeneous functions require continuous differentiability?,,"Euler's Theorem for homogeneous functions states that $f: \mathbb{R}^n \to \mathbb{R}$ is homogeneous of degree $k$ ( $f(cx) = c^k f(x)$ for all $c > 0$ , $x \in \mathbb{R}^n$ ), if and only if the partial derivatives of $f$ satisfy $$ k f(x) = \sum_{i=1}^n x_i \frac{\partial f(x)}{\partial x_i} $$ Clearly this theorem requires differentiability of $f$ . However, it seems to often be stated with the requirement of continuous differentiability, e.g., here . On the other hand, it is also sometimes stated without this requirement, and I don't see how any of the proofs actually use continuity of partial derivatives (e.g., the proof here ). I would greatly appreciate if anyone could clear this up!","Euler's Theorem for homogeneous functions states that is homogeneous of degree ( for all , ), if and only if the partial derivatives of satisfy Clearly this theorem requires differentiability of . However, it seems to often be stated with the requirement of continuous differentiability, e.g., here . On the other hand, it is also sometimes stated without this requirement, and I don't see how any of the proofs actually use continuity of partial derivatives (e.g., the proof here ). I would greatly appreciate if anyone could clear this up!","f: \mathbb{R}^n \to \mathbb{R} k f(cx) = c^k f(x) c > 0 x \in \mathbb{R}^n f 
k f(x) = \sum_{i=1}^n x_i \frac{\partial f(x)}{\partial x_i}
 f","['calculus', 'multivariable-calculus']"
16,Let $F=\cos(y^{2}+z^{2})i+\sin(z^{2}+x^{2})j+e^{x^{2}+y^{2}}k$ be a vectore field on $\mathbb R^{3}$.,Let  be a vectore field on .,F=\cos(y^{2}+z^{2})i+\sin(z^{2}+x^{2})j+e^{x^{2}+y^{2}}k \mathbb R^{3},"Let $F=\cos(y^{2}+z^{2})i+\sin(z^{2}+x^{2})j+e^{x^{2}+y^{2}}k$ be a vectore field on $\mathbb R^{3}$ . Calculate $\int_{S}F.ds$ , where the surface $S$ is defined by $x^{2}+y^{2}=e^{z}\cos z~$ , $~~0\leq z \leq \pi/2$ and oriented upward. Is my solution is correct?","Let be a vectore field on . Calculate , where the surface is defined by , and oriented upward. Is my solution is correct?",F=\cos(y^{2}+z^{2})i+\sin(z^{2}+x^{2})j+e^{x^{2}+y^{2}}k \mathbb R^{3} \int_{S}F.ds S x^{2}+y^{2}=e^{z}\cos z~ ~~0\leq z \leq \pi/2,"['multivariable-calculus', 'solution-verification', 'surface-integrals', 'stokes-theorem']"
17,Calculating the line integral for the circle and the square,Calculating the line integral for the circle and the square,,"Consider the region $S$ bounded between the square with corners at the points (4,4),(-4,4),(-4,-4) and (4,-4) (oriented counterclockwise), and the circle of radius 1 centered at (-1,0) (oriented clockwise) and $$ F(x,y)=\left(\frac{-y}{(x+1)^2+y^2}, \frac{x+1}{(x+1)^2+y^2}\right) $$ and calculate $$\int_{ds} F\cdot dr$$ (Hint for calculating the line integral: Use the definition $\tan^{-1} a + \tan^{-1} a^{-1} = \frac{\pi}{2}$ . Let $P(x,y)=\frac{-y}{(x+1)^2+y^2}$ and $Q(x,y)=\frac{x+1}{(x+1)^2+y^2}$ I can't use the Green's Theorem because there is a singularity at the point $(−1,0)$ in $P$ and $Q$ . So I want to calculate the line integral for the circle and the square In the image I represented the curves to be integrated with their respective orientations but when I calculated the line integral of the circle I obtained that it diverges so I don't know how to continue the exercise. I leave below how to calculate this integral: $$ \begin{split} I &= \int_{ds} F\cdot dr \\   &= \int_{0}^{2 \pi} \frac{-\sin t (-\sin t) dt}                            {(\cos t+1+1)^2+\sin^2 t}    +  \frac{(\cos t+1+1)\cos t dt}{(\cos t+1+1)^2+\sin^2 t} \\   &= \left[\frac{-1}{\sin t} - \tan^2 t +t\right]_{0}^{2 \pi}   \to \infty \end{split} $$ At this point I don't know how to solve the exercise in any other way so help would be appreciated! :)","Consider the region bounded between the square with corners at the points (4,4),(-4,4),(-4,-4) and (4,-4) (oriented counterclockwise), and the circle of radius 1 centered at (-1,0) (oriented clockwise) and and calculate (Hint for calculating the line integral: Use the definition . Let and I can't use the Green's Theorem because there is a singularity at the point in and . So I want to calculate the line integral for the circle and the square In the image I represented the curves to be integrated with their respective orientations but when I calculated the line integral of the circle I obtained that it diverges so I don't know how to continue the exercise. I leave below how to calculate this integral: At this point I don't know how to solve the exercise in any other way so help would be appreciated! :)","S 
F(x,y)=\left(\frac{-y}{(x+1)^2+y^2}, \frac{x+1}{(x+1)^2+y^2}\right)
 \int_{ds} F\cdot dr \tan^{-1} a + \tan^{-1} a^{-1} = \frac{\pi}{2} P(x,y)=\frac{-y}{(x+1)^2+y^2} Q(x,y)=\frac{x+1}{(x+1)^2+y^2} (−1,0) P Q 
\begin{split}
I &= \int_{ds} F\cdot dr \\
  &= \int_{0}^{2 \pi} \frac{-\sin t (-\sin t) dt}
                           {(\cos t+1+1)^2+\sin^2 t}
   +  \frac{(\cos t+1+1)\cos t dt}{(\cos t+1+1)^2+\sin^2 t} \\
  &= \left[\frac{-1}{\sin t} - \tan^2 t +t\right]_{0}^{2 \pi}
  \to \infty
\end{split}
","['calculus', 'integration', 'multivariable-calculus', 'surface-integrals', 'greens-theorem']"
18,Finding the new region after changing variables for a double integral.,Finding the new region after changing variables for a double integral.,,"I would like to solve the following double integral using the transformation: $u=x+y$ , $v=x/y$ $$\int _0^1\int _y^2\frac{\left(x+y\right)}{x^2}\:e^{\left(x+y\right)}dxdy$$ What I've reached so far: $$J=-\frac{u}{\left(v+1\right)^2}$$ , $$x=\frac{uv}{v+1}$$ , $$y=\frac{u}{v+1}$$ the only problem I'm facing now is to find the new region so I can set up my double integral, but what I get is: $$u=0,u=v+1,uv=2\left(v+1\right),v=1 $$ which doesn't look like a region that I can integrate over. Final note: I know that the integral doesn't converge anyway , and I'm not the one who chose this substitution (homework problem)","I would like to solve the following double integral using the transformation: , What I've reached so far: , , the only problem I'm facing now is to find the new region so I can set up my double integral, but what I get is: which doesn't look like a region that I can integrate over. Final note: I know that the integral doesn't converge anyway , and I'm not the one who chose this substitution (homework problem)","u=x+y v=x/y \int _0^1\int _y^2\frac{\left(x+y\right)}{x^2}\:e^{\left(x+y\right)}dxdy J=-\frac{u}{\left(v+1\right)^2} x=\frac{uv}{v+1} y=\frac{u}{v+1} u=0,u=v+1,uv=2\left(v+1\right),v=1 ","['calculus', 'multivariable-calculus', 'multiple-integral', 'change-of-variable']"
19,Visualising the divergence of a vector field,Visualising the divergence of a vector field,,"My main doubt is : ""How do you visualize finding out the divergence of a vector field?"" I will first tell you what I think can be a nice way of visualizing it. (There is obviously a mistake in my method as we get a contradiction which I will discuss  below) Step 1 : Sketch the vector field (length of the vector is proportional to its magnitude) Step 2 : Place a particle at the tail of all the vectors visible in the sketch Step 3 : Assume that the vectors indicate the velocity of the particles Step 4 : Let the particles move for one second. At the end of unit time(one second), the particles will reach the arrow head of the vector Step 5 : See whether the density has increased in the surrounding of the point, or has decreased If the density has increased, it has negative divergence and vice versa. 1-D case I hope you can understand my one dimensional (1/r^2) vector field sketch and my method of calculating the divergence at a point p More particles enter the rectangular surrounding of p than leave. 2-D case Here I have made the diagram for 2 dimensions. As we can see from the diagrams, more particles come into the neighbourhood of p. This suggests negative divergence at p and that is indeed the case. The math checks out. Now, doing the same for 3 dimensional vector field (1/r^2) gives us trouble. We know by calculating that the divergence of this field is 0, but it does not look like that according to my method of visualisation. 3-D case The dotted circular line shows the spherical neighbourhood of the point p. The solid ellipse depicts a circular intersection when sliced by a plane passing through origin . We can break the entire spherical surrounding as infinite such circular discs. Now we know the 2-D case. More points enter the disc than leave. This means more points enter the sphere than leave. This should make the divergence at p negative, but it turns out to be 0. SO MY QUESTION IS, ""WHAT IS THE MISTAKE IN MY METHOD OF VISUALISATION? IS THERE A BETTER WAY TO VISUALISE THE DIVERGENCE AT A POINT?""","My main doubt is : ""How do you visualize finding out the divergence of a vector field?"" I will first tell you what I think can be a nice way of visualizing it. (There is obviously a mistake in my method as we get a contradiction which I will discuss  below) Step 1 : Sketch the vector field (length of the vector is proportional to its magnitude) Step 2 : Place a particle at the tail of all the vectors visible in the sketch Step 3 : Assume that the vectors indicate the velocity of the particles Step 4 : Let the particles move for one second. At the end of unit time(one second), the particles will reach the arrow head of the vector Step 5 : See whether the density has increased in the surrounding of the point, or has decreased If the density has increased, it has negative divergence and vice versa. 1-D case I hope you can understand my one dimensional (1/r^2) vector field sketch and my method of calculating the divergence at a point p More particles enter the rectangular surrounding of p than leave. 2-D case Here I have made the diagram for 2 dimensions. As we can see from the diagrams, more particles come into the neighbourhood of p. This suggests negative divergence at p and that is indeed the case. The math checks out. Now, doing the same for 3 dimensional vector field (1/r^2) gives us trouble. We know by calculating that the divergence of this field is 0, but it does not look like that according to my method of visualisation. 3-D case The dotted circular line shows the spherical neighbourhood of the point p. The solid ellipse depicts a circular intersection when sliced by a plane passing through origin . We can break the entire spherical surrounding as infinite such circular discs. Now we know the 2-D case. More points enter the disc than leave. This means more points enter the sphere than leave. This should make the divergence at p negative, but it turns out to be 0. SO MY QUESTION IS, ""WHAT IS THE MISTAKE IN MY METHOD OF VISUALISATION? IS THERE A BETTER WAY TO VISUALISE THE DIVERGENCE AT A POINT?""",,"['multivariable-calculus', 'vector-analysis', 'vector-fields', 'divergence-operator']"
20,How do we know that the divergence of a vector field exists?,How do we know that the divergence of a vector field exists?,,"How do we prove that the limit of $${\displaystyle \left.\operatorname {div} \mathbf {F} \right|_{\mathbf {x_{0}} }=\lim _{V\rightarrow 0}{1 \over |V|}} \unicode{x222F}_{\displaystyle \scriptstyle S(V)} {\displaystyle \mathbf {F} \cdot \mathbf {\hat {n}} \,dS}$$ exists? $$$$ I understand how to intuitively derive the formula for divergence in Cartesian coordinates given that the limit exists (since we can then choose an easy shape for the volume), but I don't know how to prove that the limit exists in the first place.","How do we prove that the limit of exists? I understand how to intuitively derive the formula for divergence in Cartesian coordinates given that the limit exists (since we can then choose an easy shape for the volume), but I don't know how to prove that the limit exists in the first place.","{\displaystyle \left.\operatorname {div} \mathbf {F} \right|_{\mathbf {x_{0}} }=\lim _{V\rightarrow 0}{1 \over |V|}} \unicode{x222F}_{\displaystyle \scriptstyle S(V)} {\displaystyle \mathbf {F} \cdot \mathbf {\hat {n}} \,dS} ","['multivariable-calculus', 'vector-analysis', 'surface-integrals', 'divergence-operator']"
21,Flux across the surface of a cone,Flux across the surface of a cone,,"My textbook asks the following question: Find the flux of $F(x,y,z) = \langle x,y,z \rangle$ across the surface of the cone $ z^2 = x^2 + y^2 $ , for $ 0 \leq z \leq 1 $ (normal vectors point upward). I tried to solve this using both Divergence Thm. and directly. Using Divergence Thm., I got: $\iint_S F\cdot n dS = \iiint_E divFdV = \int_{0}^{2\pi}\int_{0}^{1}\int_{0}^{r} (3)rdzdrd\theta = 2\pi$ However, when performing it directly, I obtained the following: $\iint_S F\cdot n dS = \int_{0}^{2\pi}\int_{0}^1 (\langle rcos\theta, rsin\theta, r \rangle \cdot \langle -cos\theta, -sin\theta, 1 \rangle )rdrd\theta = 0 $ There is no solution listed for the problem. Are either of these correct or are neither of them correct?","My textbook asks the following question: Find the flux of across the surface of the cone , for (normal vectors point upward). I tried to solve this using both Divergence Thm. and directly. Using Divergence Thm., I got: However, when performing it directly, I obtained the following: There is no solution listed for the problem. Are either of these correct or are neither of them correct?","F(x,y,z) = \langle x,y,z \rangle  z^2 = x^2 + y^2   0 \leq z \leq 1  \iint_S F\cdot n dS = \iiint_E divFdV = \int_{0}^{2\pi}\int_{0}^{1}\int_{0}^{r} (3)rdzdrd\theta = 2\pi \iint_S F\cdot n dS = \int_{0}^{2\pi}\int_{0}^1 (\langle rcos\theta, rsin\theta, r \rangle \cdot \langle -cos\theta, -sin\theta, 1 \rangle )rdrd\theta = 0 ","['calculus', 'multivariable-calculus', 'surface-integrals']"
22,Question about differentation,Question about differentation,,"I can't understand a passage: given $f\in C^2([a,b]\times\mathbb{R}\times\mathbb{R}$ ), $f=f(x,u,\xi)$ , $u \in C^2[a,b]$ , we have $\frac{d}{dx}[f(x,u,u')-u'f_\xi(x,u,u')] = f_x(x,u,u') + u'\Big[f_u(x,u,u')-\dfrac{d}{dx}[f_\xi(x,u,u')]\Big]$ I don't understand why, I thought: $\frac{d}{dx}[f(x,u,u')-u'f_\xi(x,u,u')] = f_x(x,u,u') -u''f_\xi(x,u,u') - u'\dfrac{d}{dx}[f_\xi(x,u,u')]$ Where am I wrong? Thank you.","I can't understand a passage: given ), , , we have I don't understand why, I thought: Where am I wrong? Thank you.","f\in C^2([a,b]\times\mathbb{R}\times\mathbb{R} f=f(x,u,\xi) u \in C^2[a,b] \frac{d}{dx}[f(x,u,u')-u'f_\xi(x,u,u')] = f_x(x,u,u') + u'\Big[f_u(x,u,u')-\dfrac{d}{dx}[f_\xi(x,u,u')]\Big] \frac{d}{dx}[f(x,u,u')-u'f_\xi(x,u,u')] = f_x(x,u,u') -u''f_\xi(x,u,u') - u'\dfrac{d}{dx}[f_\xi(x,u,u')]",['multivariable-calculus']
23,Limit along the line,Limit along the line,,"I came across the following passage: Limit of a function $f$ along the line $l: (x_0  +t\cos a, y_0+t\sin a)$ , $t\in \mathbb{R}, a\in [0,\pi]$ , through the point $(x_0, y_0)$ is the following limit: $$\lim_{t\to 0}f(x_0  +t\cos a, y_0+t\sin a)$$ I don't understand the representation of the line $l$ . Is the collection of points $(x_0  +t\cos a, y_0+t\sin a)$ a set of points that satisfies the equation of the line $l$ ? I'm familiar with polar coordinates and I've tried representing a point $(x,y)$ and plugging in the equation $y=mx+n$ but I don't get anything similar to the stuff in the passage above.","I came across the following passage: Limit of a function along the line , , through the point is the following limit: I don't understand the representation of the line . Is the collection of points a set of points that satisfies the equation of the line ? I'm familiar with polar coordinates and I've tried representing a point and plugging in the equation but I don't get anything similar to the stuff in the passage above.","f l: (x_0  +t\cos a, y_0+t\sin a) t\in \mathbb{R}, a\in [0,\pi] (x_0, y_0) \lim_{t\to 0}f(x_0  +t\cos a, y_0+t\sin a) l (x_0  +t\cos a, y_0+t\sin a) l (x,y) y=mx+n",['limits']
24,"Is $(0,0)$ a saddle point for the given function?",Is  a saddle point for the given function?,"(0,0)","I need to find the critical points for the function $f(x) = 3(x^2 + y^2) - 2(x^3 - y^3) + 6xy$ and also test whether they are maxima/minima/saddle point. Now the only critical point is (0,0) however at (0,0) $rt - s^2  =0$ then second derivative test fails, If I take the line $y = -x$ then $f(x, -x) = -4x^3$ then clearly for along the neighborhood of $(0,0)$ $f$ has both positive and negative values . Hence , $(0,0)$ is a  saddle point . Is my solution and answer correct ? Can someone please verify ? Thank you.","I need to find the critical points for the function and also test whether they are maxima/minima/saddle point. Now the only critical point is (0,0) however at (0,0) then second derivative test fails, If I take the line then then clearly for along the neighborhood of has both positive and negative values . Hence , is a  saddle point . Is my solution and answer correct ? Can someone please verify ? Thank you.","f(x) = 3(x^2 + y^2) - 2(x^3 - y^3) + 6xy rt - s^2  =0 y = -x f(x, -x) = -4x^3 (0,0) f (0,0)",['multivariable-calculus']
25,"Find $\bigcup_{r\in R}\ \{(x,y): (x-r)^2 + (y+2r)^2 < r^2+1\}$",Find,"\bigcup_{r\in R}\ \{(x,y): (x-r)^2 + (y+2r)^2 < r^2+1\}","I have to obtain an union: $\bigcup_{r\in R}\ \{(x,y): (x-r)^2 + (y+2r)^2 < r^2+1\}$ I know this is a series of circles that is limited by two hyperbolic functions that are symmetrical in relation to $y = -2x$ . But how to get an union? I tried putting $y= \frac{1}{2}x$ into the circle's equation to get the intersection points but it works only for $r = 1$ . It would require putting $y = \frac{1}{2}x + b, \forall b \in R$ but it doesn't look as a significant simplification. I have also thought of representing an intersection as a translation from $(x, -2x)$ by a vector $\pm\ [\frac{\sqrt{5}}{5} \sqrt{r^2+1}, \frac{2\sqrt{5}}{5}\sqrt{r^2+1}]$ but it hasn't turned out to be usefull aswell. I'm running out of ideas. I'd be really thankful for any kind of help.",I have to obtain an union: I know this is a series of circles that is limited by two hyperbolic functions that are symmetrical in relation to . But how to get an union? I tried putting into the circle's equation to get the intersection points but it works only for . It would require putting but it doesn't look as a significant simplification. I have also thought of representing an intersection as a translation from by a vector but it hasn't turned out to be usefull aswell. I'm running out of ideas. I'd be really thankful for any kind of help.,"\bigcup_{r\in R}\ \{(x,y): (x-r)^2 + (y+2r)^2 < r^2+1\} y = -2x y= \frac{1}{2}x r = 1 y = \frac{1}{2}x + b, \forall b \in R (x, -2x) \pm\ [\frac{\sqrt{5}}{5} \sqrt{r^2+1}, \frac{2\sqrt{5}}{5}\sqrt{r^2+1}]","['multivariable-calculus', 'envelope']"
26,multivariate Ito isometry,multivariate Ito isometry,,"I wonder whether there exists a straightforward extension of the Ito isometry to multidimensional processes. In the one-dimensional case the Ito isometry can be written as $\mathbb{E}[ (\int_0^T X_t \; \mathrm{d}W_t)^2 ] = \mathbb{E}[ (\int_0^T X_t^2 \;\mathrm{d}_t) ]$ . If now $X_t$ is a vector of random variables instead, do I get something along these lines: $\mathbb{E}[ (\int_0^T X_t \; \mathrm{d}W_t) (\int_0^T X_t^\top \; \mathrm{d}W_t^\top) ] = \mathbb{E}[ (\int_0^T X_t X_t^\top \;\mathrm{d}_t) ]$ ????","I wonder whether there exists a straightforward extension of the Ito isometry to multidimensional processes. In the one-dimensional case the Ito isometry can be written as . If now is a vector of random variables instead, do I get something along these lines: ????",\mathbb{E}[ (\int_0^T X_t \; \mathrm{d}W_t)^2 ] = \mathbb{E}[ (\int_0^T X_t^2 \;\mathrm{d}_t) ] X_t \mathbb{E}[ (\int_0^T X_t \; \mathrm{d}W_t) (\int_0^T X_t^\top \; \mathrm{d}W_t^\top) ] = \mathbb{E}[ (\int_0^T X_t X_t^\top \;\mathrm{d}_t) ],"['multivariable-calculus', 'stochastic-calculus', 'brownian-motion', 'isometry', 'stochastic-differential-equations']"
27,Extending surface independence of Stokes' theorem in elementary calculus to the Stokes' theorem in differential geometry,Extending surface independence of Stokes' theorem in elementary calculus to the Stokes' theorem in differential geometry,,"Okay so I discovered, here When is a surface integral equal to double integral over projection? A verification of Stokes' Theorem. Intuition and relation to Green's Theorem. (and also here Applying Stokes' theorem - what surface? ), a view of Stokes' Theorem, at least in elementary calculus, as not only Given a surface $\Sigma$ , let $C$ be its boundary curve. If (insert assumptions), then $\int_C = \int \int_{\Sigma}$ . but also Given a curve $C$ , we have for any surface $\Sigma$ with $C$ as its boundary curve that if (insert assumptions), then $\int_C = \int \int_{\Sigma}$ . Thus, Given a surface $\Sigma$ , if (insert assumptions), then $\int \int_{\Sigma} = \int \int_{\Sigma'}$ for any $\Sigma'$ that has the same boundary curve as $\Sigma$ as long as (insert assumptions). Question : How can we extend this view to Stokes' theorem in differential geometry? If I have 2 distinct smooth oriented $n$ -manifolds with boundary $M$ and $N$ that actually turn out have the same manifold boundary $\partial M=\partial N$ , then, under assumptions (insert here), I want to use Stokes' Theorem to say something like $$\int_M d \omega=\int_{\partial M} \omega = \int_{\partial N} \omega=\int_N d \omega \tag{A}$$ Not sure what $\omega$ would be though. It's has to be a smooth differential $(n−1)$ -form with compact support on both $N$ and $M$ . Also, if we're going to be strict about it, then $(A)$ should look more like $$\int_M d \omega=\int_{\partial M} \iota_M^{*}\omega = \int_{\partial N} \iota_N^{*}\omega=\int_N d \omega \tag{B}$$ where $(\cdot)^{*}$ denotes pullback and $\iota_M: \partial M \to M$ and $\iota_N: \partial N \to N$ are inclusion maps in which case I'm not sure we can have the same ' $\omega$ ' since we can't exactly have $\omega: M \to \wedge(T^{*}M)$ to be also $\omega: N \to \wedge(T^{*}N)$ . Some examples that might make sense out of $\omega$ are when $M$ is a submanifold with boundary of $N$ (in which case I guess $M$ is open in $N$ extending from the case for submanifolds without boundary of codimension zero) or when they have a submanifold with boundary in common or something. In the former example (assuming it works), we could have the ' $\omega$ ' on $N$ to be just $\omega$ and then the ' $\omega$ ' on $M$ is $\omega|_{M}$ , in which case I hope that $\omega(M) = \omega|_{M}(M)$ , a subset of $\wedge(T^{*}N)$ , is a vector subbundle that is bundle isomorphic to the cotangent bundle $\wedge(T^{*}M)$ . I think the latter example can work similarly.","Okay so I discovered, here When is a surface integral equal to double integral over projection? A verification of Stokes' Theorem. Intuition and relation to Green's Theorem. (and also here Applying Stokes' theorem - what surface? ), a view of Stokes' Theorem, at least in elementary calculus, as not only Given a surface , let be its boundary curve. If (insert assumptions), then . but also Given a curve , we have for any surface with as its boundary curve that if (insert assumptions), then . Thus, Given a surface , if (insert assumptions), then for any that has the same boundary curve as as long as (insert assumptions). Question : How can we extend this view to Stokes' theorem in differential geometry? If I have 2 distinct smooth oriented -manifolds with boundary and that actually turn out have the same manifold boundary , then, under assumptions (insert here), I want to use Stokes' Theorem to say something like Not sure what would be though. It's has to be a smooth differential -form with compact support on both and . Also, if we're going to be strict about it, then should look more like where denotes pullback and and are inclusion maps in which case I'm not sure we can have the same ' ' since we can't exactly have to be also . Some examples that might make sense out of are when is a submanifold with boundary of (in which case I guess is open in extending from the case for submanifolds without boundary of codimension zero) or when they have a submanifold with boundary in common or something. In the former example (assuming it works), we could have the ' ' on to be just and then the ' ' on is , in which case I hope that , a subset of , is a vector subbundle that is bundle isomorphic to the cotangent bundle . I think the latter example can work similarly.",\Sigma C \int_C = \int \int_{\Sigma} C \Sigma C \int_C = \int \int_{\Sigma} \Sigma \int \int_{\Sigma} = \int \int_{\Sigma'} \Sigma' \Sigma n M N \partial M=\partial N \int_M d \omega=\int_{\partial M} \omega = \int_{\partial N} \omega=\int_N d \omega \tag{A} \omega (n−1) N M (A) \int_M d \omega=\int_{\partial M} \iota_M^{*}\omega = \int_{\partial N} \iota_N^{*}\omega=\int_N d \omega \tag{B} (\cdot)^{*} \iota_M: \partial M \to M \iota_N: \partial N \to N \omega \omega: M \to \wedge(T^{*}M) \omega: N \to \wedge(T^{*}N) \omega M N M N \omega N \omega \omega M \omega|_{M} \omega(M) = \omega|_{M}(M) \wedge(T^{*}N) \wedge(T^{*}M),['multivariable-calculus']
28,"Solid region $S$ bounded by parabolic cylinder $z+x^2 = 2$ and planes $z=0$, $y=0$ and $y=x$.","Solid region  bounded by parabolic cylinder  and planes ,  and .",S z+x^2 = 2 z=0 y=0 y=x,"The problem There exists a (unique, I guess) solid region $S$ bounded by parabolic cylinder $z+x^2 = 2$ and planes $z=0$ , $y=0$ and $y=x$ . Sketch the part of $S$ in the first octant of $\mathbb R^3$ . Set up the bounds for the triple integral $$I_S = \int\int\int _S f dz dy dx$$ for any suitable $f: \mathbb R^3 \to \mathbb R$ (uniformly continuous, Riemann integrable, Lebesgue integrable or whatever you need). Questions : Question : For 1, how is $S$ not completely in the first octant (except possibly for some of its boundary)? I think $S$ is a cut out of an infinite loaf of bread, with the bottom as $z=0$ cut and the top as the $z+x^2=2$ , where the infinite loaf is cut by $y=0$ and $y=x$ . Looks like first octant to me. Question : For 2, this is what I got at least for $S$ in the first octant, denoted $S_1$ . Is this correct? $z$ is from $0$ to $2-x^2$ . So we get $$I_{S_1} = \int\int_{R_1} \int_{z=0}^{z=2-x^2} f dy dx$$ for some region $R_1$ . Afterwards, I let $g = \int_{z=0}^{z=2-x^2} f dz$ , $g: \mathbb R^2 \to \mathbb R$ and pretend we're in the $xy$ -plane ( $z=0$ ) and then set up the bounds for the double integral $$I_{S_1} = \int\int_{R_1} g dy dx$$ over $R_1$ which appears to be a triangle bounded by $y=x$ , $x=\sqrt{2}$ and $y=0$ . Therefore, $$I_{S_1} = \int_{x=0}^{x=\sqrt{2}} \int_{y=0}^{y=x} g dy dx = \int_{x=0}^{x=\sqrt{2}} \int_{y=0}^{y=x} \int_{z=0}^{z=2-x^2} f dz dy dx$$ Question : If $S$ is not completely in the first octant (except possibly for some of its boundary), then why, and what's the integral? See below. Update Based on Quanto's answer, in particular the term 'third octant' (which I hope means negative y, negative x and positive z based on the '2' in the 'gray code' here ), I realised my (implicit) mistake in extending my answer to Question (2). Here's what I think to do: Forget $y=x$ and $y=0$ for now. The solid region bounded by $z=0$ and $z+x^2=2$ is $S^\# = \{(x,y,z) \in \mathbb R^3| z \ge 0, z=2-x^2\}$ . I see this as an infinite hollow loaf of bread where the bread is shaped parabolically and where the $y$ -axis is the centre of the base of the infinite loaf. Observe $S^\#$ can also be written $S^\# = \{(x,y,z) \in \mathbb R^3| z=2-x^2, x^2 \le 2 \}$ , i.e. the condition ' $z \ge 0$ ' is equivalent to the condition ' $x^2 \le 2$ '. Flattening $S^\#$ , in the sense of projecting $S^\#$ onto $z=0$ , gives us $\tilde A = \{(x,y,z) \in \mathbb R^3| z=0, x^2 \le 2 \}$ which is pretty much the same as (In higher maths: 'pretty much the same as' means 'homeomorphic, as topological manifolds with boundary, to' or something) the set $A = \{(x,y) \in \mathbb R^2|x^2 \le 2 \}$ of 2 parallel vertical lines and the space in between them. Now we bring back $y=x$ and $y=0$ . It appears my mistake was that I implicitly assumed 'bounded by $y=0$ ' meant 'is contained in $y \ge 0$ '. Observe that $A$ is cut up by $y=x$ and $y=0$ into 4 spaces (Let these spaces include the boundaries), 2 of which are bounded. Denote these bounded spaces as $A_1$ and $A_2$ . One of $A_1$ and $A_2$ without its boundary is completely in the first quadrant. The other without its boundary is completely in the third quadrant. Let $A_1$ be the former and $A_2$ be the latter. The part $S_1$ (without boundary) of $S$ in the first octant projects to $A_1$ (without boundary). $A_2$ (without boundary) indeed corresponds to a non-empty part $S_2$ (without boundary) of $S$ not in the first octant. Just as $A_2$ (without boundary) is in the third quadrant, so do we have $S_2$ (without boundary) to be in the third octant. Now, the bounds for $z$ in $I_{S_2}$ are the same as in $I_{S_1}$ , I think: $$I_{S_2} = \int\int_{R_2}\int_{z=0}^{z=2-x^2} f dy dx = \int\int_{R_2} g dy dx$$ Now, $R_2$ is the region in $\mathbb R^2$ which appears to be a triangle bounded by $y=x$ , $x=-\sqrt{2}$ (rather than $x=\sqrt{2}$ !) and $y=0$ . Therefore, $$I_{S_2} = \int_{x=-\sqrt{2}}^{x=0} \int_{y=x}^{y=0} g dy dx = \int_{x=-\sqrt{2}}^{x=0} \int_{y=x}^{y=0} \int_{z=0}^{z=2-x^2} f dz dy dx$$ Finally, I believe that $S = S_1 \cup S_2$ and that $S_1 \cap S_2$ is empty except for boundary points. Therefore, $$I_S = I_{S_1} + I_{S_2} = \int \int \int_{S_1} f dz dy dx + \int \int \int_{S_2} f dz dy dx$$","The problem There exists a (unique, I guess) solid region bounded by parabolic cylinder and planes , and . Sketch the part of in the first octant of . Set up the bounds for the triple integral for any suitable (uniformly continuous, Riemann integrable, Lebesgue integrable or whatever you need). Questions : Question : For 1, how is not completely in the first octant (except possibly for some of its boundary)? I think is a cut out of an infinite loaf of bread, with the bottom as cut and the top as the , where the infinite loaf is cut by and . Looks like first octant to me. Question : For 2, this is what I got at least for in the first octant, denoted . Is this correct? is from to . So we get for some region . Afterwards, I let , and pretend we're in the -plane ( ) and then set up the bounds for the double integral over which appears to be a triangle bounded by , and . Therefore, Question : If is not completely in the first octant (except possibly for some of its boundary), then why, and what's the integral? See below. Update Based on Quanto's answer, in particular the term 'third octant' (which I hope means negative y, negative x and positive z based on the '2' in the 'gray code' here ), I realised my (implicit) mistake in extending my answer to Question (2). Here's what I think to do: Forget and for now. The solid region bounded by and is . I see this as an infinite hollow loaf of bread where the bread is shaped parabolically and where the -axis is the centre of the base of the infinite loaf. Observe can also be written , i.e. the condition ' ' is equivalent to the condition ' '. Flattening , in the sense of projecting onto , gives us which is pretty much the same as (In higher maths: 'pretty much the same as' means 'homeomorphic, as topological manifolds with boundary, to' or something) the set of 2 parallel vertical lines and the space in between them. Now we bring back and . It appears my mistake was that I implicitly assumed 'bounded by ' meant 'is contained in '. Observe that is cut up by and into 4 spaces (Let these spaces include the boundaries), 2 of which are bounded. Denote these bounded spaces as and . One of and without its boundary is completely in the first quadrant. The other without its boundary is completely in the third quadrant. Let be the former and be the latter. The part (without boundary) of in the first octant projects to (without boundary). (without boundary) indeed corresponds to a non-empty part (without boundary) of not in the first octant. Just as (without boundary) is in the third quadrant, so do we have (without boundary) to be in the third octant. Now, the bounds for in are the same as in , I think: Now, is the region in which appears to be a triangle bounded by , (rather than !) and . Therefore, Finally, I believe that and that is empty except for boundary points. Therefore,","S z+x^2 = 2 z=0 y=0 y=x S \mathbb R^3 I_S = \int\int\int _S f dz dy dx f: \mathbb R^3 \to \mathbb R S S z=0 z+x^2=2 y=0 y=x S S_1 z 0 2-x^2 I_{S_1} = \int\int_{R_1} \int_{z=0}^{z=2-x^2} f dy dx R_1 g = \int_{z=0}^{z=2-x^2} f dz g: \mathbb R^2 \to \mathbb R xy z=0 I_{S_1} = \int\int_{R_1} g dy dx R_1 y=x x=\sqrt{2} y=0 I_{S_1} = \int_{x=0}^{x=\sqrt{2}} \int_{y=0}^{y=x} g dy dx = \int_{x=0}^{x=\sqrt{2}} \int_{y=0}^{y=x} \int_{z=0}^{z=2-x^2} f dz dy dx S y=x y=0 z=0 z+x^2=2 S^\# = \{(x,y,z) \in \mathbb R^3| z \ge 0, z=2-x^2\} y S^\# S^\# = \{(x,y,z) \in \mathbb R^3| z=2-x^2, x^2 \le 2 \} z \ge 0 x^2 \le 2 S^\# S^\# z=0 \tilde A = \{(x,y,z) \in \mathbb R^3| z=0, x^2 \le 2 \} A = \{(x,y) \in \mathbb R^2|x^2 \le 2 \} y=x y=0 y=0 y \ge 0 A y=x y=0 A_1 A_2 A_1 A_2 A_1 A_2 S_1 S A_1 A_2 S_2 S A_2 S_2 z I_{S_2} I_{S_1} I_{S_2} = \int\int_{R_2}\int_{z=0}^{z=2-x^2} f dy dx = \int\int_{R_2} g dy dx R_2 \mathbb R^2 y=x x=-\sqrt{2} x=\sqrt{2} y=0 I_{S_2} = \int_{x=-\sqrt{2}}^{x=0} \int_{y=x}^{y=0} g dy dx = \int_{x=-\sqrt{2}}^{x=0} \int_{y=x}^{y=0} \int_{z=0}^{z=2-x^2} f dz dy dx S = S_1 \cup S_2 S_1 \cap S_2 I_S = I_{S_1} + I_{S_2} = \int \int \int_{S_1} f dz dy dx + \int \int \int_{S_2} f dz dy dx",['calculus']
29,Help getting new bounds for a change of variable,Help getting new bounds for a change of variable,,"I have the domain $D: x^2-y^2=1, x^2-y^2=4,y=0, y=\frac{x}{2}$ where $x\geq 0$ . I have to calculate this double integral: $$\iint_D \left(1-\left(\frac{y}{x}\right)^4\right)e^{x^2-y^2} dxdy $$ So my first idea was, to use these new variables: $$u=\frac{y}{x}, v=x^2-y^2;  $$ $$J=\frac{1}{2(u^2-1) } $$ So this would give me : $$\iint (1-u^4)e^v\frac{1}{2(u^2-1)}dudv = -\frac{1}{2}\iint (1+u^2)e^v dudv$$ But I have a problem getting the bounds for $u$ . The other one is easy, $v\in [1,4]$ , and I got $u=\frac{1}{u}$ . Is it safe to assume: $u\in[0,1]$ ? I am unsure about this one. Did I do the above steps correctly? Any help/insight would be appreciated.","I have the domain where . I have to calculate this double integral: So my first idea was, to use these new variables: So this would give me : But I have a problem getting the bounds for . The other one is easy, , and I got . Is it safe to assume: ? I am unsure about this one. Did I do the above steps correctly? Any help/insight would be appreciated.","D: x^2-y^2=1, x^2-y^2=4,y=0, y=\frac{x}{2} x\geq 0 \iint_D \left(1-\left(\frac{y}{x}\right)^4\right)e^{x^2-y^2} dxdy  u=\frac{y}{x}, v=x^2-y^2; 
 J=\frac{1}{2(u^2-1) }  \iint (1-u^4)e^v\frac{1}{2(u^2-1)}dudv = -\frac{1}{2}\iint (1+u^2)e^v dudv u v\in [1,4] u=\frac{1}{u} u\in[0,1]","['calculus', 'integration', 'multivariable-calculus', 'change-of-variable']"
30,Why are double derivatives always same? [duplicate],Why are double derivatives always same? [duplicate],,"This question already has answers here : Why can partial derivatives be exchanged? (2 answers) Closed 4 years ago . When taking double/triple derivatives of multivariable functions, we see that no matter which order we take the derivative in, as long as we take the derivative od the function with respect to the same number of variables, the same number of times, our answer is the same? Is there an intuitive explanation for this? I cannot seem to understand how.","This question already has answers here : Why can partial derivatives be exchanged? (2 answers) Closed 4 years ago . When taking double/triple derivatives of multivariable functions, we see that no matter which order we take the derivative in, as long as we take the derivative od the function with respect to the same number of variables, the same number of times, our answer is the same? Is there an intuitive explanation for this? I cannot seem to understand how.",,[]
31,Laplacian of Gravitational Potential,Laplacian of Gravitational Potential,,"It is well known that the Laplacian of the gravitational potential $(V)$ is zero. One can easily show that by differentiating, as shown in the image. But at the same time one can show via Poisson's Equation that Laplacian, $V = -4\pi G\rho$ , where $\rho$ is the density at the point. Obviously if $\rho=0$ (For Example, outside the earth) this matches the result from the differentiation, but otherwise not. Where/why does this approach of differentiating fail?","It is well known that the Laplacian of the gravitational potential is zero. One can easily show that by differentiating, as shown in the image. But at the same time one can show via Poisson's Equation that Laplacian, , where is the density at the point. Obviously if (For Example, outside the earth) this matches the result from the differentiation, but otherwise not. Where/why does this approach of differentiating fail?",(V) V = -4\pi G\rho \rho \rho=0,"['integration', 'multivariable-calculus', 'physics', 'laplacian']"
32,How does the curvature change when a surface is translated by its normal,How does the curvature change when a surface is translated by its normal,,"This is not a homework problem, but a problem from an old exam, published online, which I am trying to solve. Consider a regular parameterized surface $X:U\to\mathcal{R}^3$ with Gauss map $N:X(U) \to S^2$ and principal curvatures $\kappa_1 = 1/r_1$ and $\kappa_2 = 1/r_2$ . Let $r\in \mathcal{R}$ and define a regular parametrized surface $X^r:U\to\mathcal{R}^3$ by $$X^r(u,v) = X(u,v)+rN(u,v).$$ Show that the principal curvatures of $X^r$ are $k_1(r)=1/(r_1-r)$ and $k_2(r)=1/(r_2-r)$ , respectively. In principle what we need to do is to determine the second and first fundamental form of $X^r$ in terms of the corresponding forms of $X$ . We could then calculate the Gaussian curvature $K$ , the mean curvature $K$ , and calculate the principal curvatures $\kappa = H \pm \sqrt{H^2-K}$ . However, the calculations for this approach seem to get very messy, so I suspect it should be done differently. Any suggestions? Using $\langle X_u,N \rangle = \langle X_v,N \rangle = 0$ , one can easily see that $$X^r_u \times X^r_v = X_u \times X_v + r^2N_u \times N_v.$$ If we choose the $+$ sign in calculating the normal from the cross product of the tangent basis vectors, and using the fact that $N_u \times N_v = -N_{uv} \times N \perp X_u \times X_v$ , we get the Gauss map $$N^r = \frac{|X_u\times X_v|N+r^2N_u\times N_v}{\sqrt{|X_u\times X_v|^2 + r^4|N_u\times N_v|^2 }}.$$ From this, I see no reasonably simple way to get at the second fundamental form of $X^r$ in terms of the second (and first) fundamental form of $X$ .","This is not a homework problem, but a problem from an old exam, published online, which I am trying to solve. Consider a regular parameterized surface with Gauss map and principal curvatures and . Let and define a regular parametrized surface by Show that the principal curvatures of are and , respectively. In principle what we need to do is to determine the second and first fundamental form of in terms of the corresponding forms of . We could then calculate the Gaussian curvature , the mean curvature , and calculate the principal curvatures . However, the calculations for this approach seem to get very messy, so I suspect it should be done differently. Any suggestions? Using , one can easily see that If we choose the sign in calculating the normal from the cross product of the tangent basis vectors, and using the fact that , we get the Gauss map From this, I see no reasonably simple way to get at the second fundamental form of in terms of the second (and first) fundamental form of .","X:U\to\mathcal{R}^3 N:X(U) \to S^2 \kappa_1 = 1/r_1 \kappa_2 = 1/r_2 r\in \mathcal{R} X^r:U\to\mathcal{R}^3 X^r(u,v) = X(u,v)+rN(u,v). X^r k_1(r)=1/(r_1-r) k_2(r)=1/(r_2-r) X^r X K K \kappa = H \pm \sqrt{H^2-K} \langle X_u,N \rangle = \langle X_v,N \rangle = 0 X^r_u \times X^r_v = X_u \times X_v + r^2N_u \times N_v. + N_u \times N_v = -N_{uv} \times N \perp X_u \times X_v N^r = \frac{|X_u\times X_v|N+r^2N_u\times N_v}{\sqrt{|X_u\times X_v|^2 + r^4|N_u\times N_v|^2 }}. X^r X","['multivariable-calculus', 'differential-geometry', 'surfaces']"
33,Studying the Continuity of a function,Studying the Continuity of a function,,"Let $U \subset \mathbb{R}^N$ be an open set, let $f : U \times [a, b] \to \mathbb{R}$ be a continuous function. Consider the function $$g(x):= \int_a^b f(x,y) \,dy$$ with $x \in U$ . i) Prove that $g$ is continuous in $U$ . ii) consider the function $f : [−1, 1] \times  [−1, 1] \to \mathbb{R}$ defined by $$f (x, y) :=\begin{cases} \frac {|y|−|x|} {y^2}&\text{ if $|x|<|y|$}\\ 0 &\text{ if $|x|\geq |y|$} \end{cases}$$ Let $g(y):= \int_{-1}^1 f(x,y) \,dx$ for $y \in [-1,1]$ Study the continuity of $g$ . So I am a bit stuck on how to prove the continuity from the basics: i know how to prove that if $f$ is continuous on $[a,b]$ , then $g=\int f$ is continuous on $[a,b]$ but i assume because of the different notation and dimension here, i have to prove a different way? In addition, how would i study the continuity?","Let be an open set, let be a continuous function. Consider the function with . i) Prove that is continuous in . ii) consider the function defined by Let for Study the continuity of . So I am a bit stuck on how to prove the continuity from the basics: i know how to prove that if is continuous on , then is continuous on but i assume because of the different notation and dimension here, i have to prove a different way? In addition, how would i study the continuity?","U \subset \mathbb{R}^N f : U \times [a, b] \to \mathbb{R} g(x):= \int_a^b f(x,y) \,dy x \in U g U f : [−1, 1] \times  [−1, 1] \to \mathbb{R} f (x, y) :=\begin{cases} \frac {|y|−|x|} {y^2}&\text{ if |x|<|y|}\\
0 &\text{ if |x|\geq |y|}
\end{cases} g(y):= \int_{-1}^1 f(x,y) \,dx y \in [-1,1] g f [a,b] g=\int f [a,b]","['real-analysis', 'integration', 'analysis', 'multivariable-calculus']"
34,"Expectations, Double Integrals and Jensen's Inequality","Expectations, Double Integrals and Jensen's Inequality",,"Consider two random variables distributed $v\backsim G(.)$ and $c \backsim F(.)$ with pdfs $g(.)$ and $f(.)$ . Let the supports of $c$ and $v$ be $[x,y]$ . Let $x<a=E(v)<b<y$ , so $[a,b]\subset\lbrack x,y]$ . Now consider a strictly concave (twice differentiable and continuous) function $u(.)$ , with $u^{\prime}(.)>0$ , $u^{^{\prime\prime}}(.)<0$ , and $u(0)=0$ (passes through the origin). Establish sufficient conditions such that the expression $\int_{a}^{b}u(E(v)-c)f(c)dc-\int_{a}^{b}\int_{x}^{y}% u(E(v)-v)g(v)f(c)dvdc\geq0$ $\forall$ $v,c$ , where $E(v)=\int_{x}^{y}vg(v)dv.$ Things I've tried: $\int_{0}^{\bar{v}}u(E(v)-v)g(v)dv\leq0$ by Jensen's inequality. To see this, let $E(v)-v=t$ . But $E(t)=E_{v}[E(v)-v]=0$ , and so $E(u(t))\leq u(E(t))=0$ , since $u(0)=0$ by assumption. Clearly, $\int_{a}^{b}u(E(v)-c)f(c)dc\leq0$ , since we are integrating the integrand $(E(v)-c)$ from $a=E(v)$ to $b$ . Intuitively, a variant of Jensen's inequality should apply if $c$ and $v$ are i.i.d. Let $c$ and $v$ be i.i.d. with identical supports. Then the integrands are the same, and we have the expression $\int_{a}^{b}% u(E(v)-v)f(c)dc-\int_{a}^{b}\int_{x}^{y}u(E(v)-v)g(v)f(c)dvdc$ . However, we can't apply Jensen's inequality directly since $\int_{a}^{b}u(E(v)-v)f(c)dc$ is not $u(E(x))$ , even if we ""factor out"" the outer integrals. $\int_{x}% ^{y}u(.)g(v)dv$ seems to be a form of $E(u(x))$ . At a loss as to what to do here. Any help would be greatly appreciated. Thank you!","Consider two random variables distributed and with pdfs and . Let the supports of and be . Let , so . Now consider a strictly concave (twice differentiable and continuous) function , with , , and (passes through the origin). Establish sufficient conditions such that the expression , where Things I've tried: by Jensen's inequality. To see this, let . But , and so , since by assumption. Clearly, , since we are integrating the integrand from to . Intuitively, a variant of Jensen's inequality should apply if and are i.i.d. Let and be i.i.d. with identical supports. Then the integrands are the same, and we have the expression . However, we can't apply Jensen's inequality directly since is not , even if we ""factor out"" the outer integrals. seems to be a form of . At a loss as to what to do here. Any help would be greatly appreciated. Thank you!","v\backsim G(.) c \backsim F(.) g(.) f(.) c v [x,y] x<a=E(v)<b<y [a,b]\subset\lbrack x,y] u(.) u^{\prime}(.)>0 u^{^{\prime\prime}}(.)<0 u(0)=0 \int_{a}^{b}u(E(v)-c)f(c)dc-\int_{a}^{b}\int_{x}^{y}%
u(E(v)-v)g(v)f(c)dvdc\geq0 \forall v,c E(v)=\int_{x}^{y}vg(v)dv. \int_{0}^{\bar{v}}u(E(v)-v)g(v)dv\leq0 E(v)-v=t E(t)=E_{v}[E(v)-v]=0 E(u(t))\leq
u(E(t))=0 u(0)=0 \int_{a}^{b}u(E(v)-c)f(c)dc\leq0 (E(v)-c) a=E(v) b c v c v \int_{a}^{b}%
u(E(v)-v)f(c)dc-\int_{a}^{b}\int_{x}^{y}u(E(v)-v)g(v)f(c)dvdc \int_{a}^{b}u(E(v)-v)f(c)dc u(E(x)) \int_{x}%
^{y}u(.)g(v)dv E(u(x))","['probability', 'probability-theory', 'multivariable-calculus', 'probability-distributions', 'jensen-inequality']"
35,Find the directional derivative in the direction of a parametric vector,Find the directional derivative in the direction of a parametric vector,,"Find the directional derivative at $(1,0,0)$ of the function $$f(x,y,z) = x^2 + ye^z)$$ in the direction of the tangent vector at $g(0)$ to the curve $\mathbb{R}^3$ defined parametrically by $$g(t) = (3t^2 + t + 1, 2t , t^2)$$ $$\begin{align} \nabla f(x,y,z) &= (2x, e^z, ye^z) \\ \frac{\partial f}{\partial g}(1,0,0) &= (2x, e^z, ye^z) \cdot (et^2 + t + 1, 2t, t^2) \\ &= 6t^2 + 2t + 2 + 2t \\ &= 6t^2 + 4t + 2 \end{align}$$ My textbook says that the answer is $2\sqrt{5}$ , which I don't think makes any sense and I think the book is wrong, but would like some other input on the matter, thank you","Find the directional derivative at of the function in the direction of the tangent vector at to the curve defined parametrically by My textbook says that the answer is , which I don't think makes any sense and I think the book is wrong, but would like some other input on the matter, thank you","(1,0,0) f(x,y,z) = x^2 + ye^z) g(0) \mathbb{R}^3 g(t) = (3t^2 + t + 1, 2t , t^2) \begin{align}
\nabla f(x,y,z) &= (2x, e^z, ye^z) \\
\frac{\partial f}{\partial g}(1,0,0) &= (2x, e^z, ye^z) \cdot (et^2 + t + 1, 2t, t^2) \\
&= 6t^2 + 2t + 2 + 2t \\
&= 6t^2 + 4t + 2
\end{align} 2\sqrt{5}","['multivariable-calculus', 'partial-derivative']"
36,"Prove that $\Phi(y, u, v) := \int_{u}^{v} f(x, y) dx$ is a $C^1$ function.(differentiation under the integral sign)",Prove that  is a  function.(differentiation under the integral sign),"\Phi(y, u, v) := \int_{u}^{v} f(x, y) dx C^1","I am reading ""A Course in Analysis vol.3"" by Kazuo Matsuzaka. The author wrote the following fact without a proof. I think it is easy to show that $D_2 \phi(y, u, v) = f(v, y)$ and $D_3 \phi(y, u, v) = -f(u, y)$ are continuous. But I cannot show that $D_1 \phi(y, u, v) = \int_{u}^{v} D_2 f(x, y) dx$ is continuous. Please tell me the proof. Let $f(x, y)$ be a continuous function from $S = [a, b] \times I \subset \mathbb{R}^2$ to $\mathbb{R}$ , where $I$ is an interval in $\mathbb{R}$ . Suppose that $D_2 f(x, y)$ exists in $S$ and is continuous in $S$ . Let $\Phi(y, u, v) := \int_{u}^{v} f(x, y) dx$ be a function from $I \times [a, b] \times [a, b]$ to $\mathbb{R}$ . Then, $\Phi(y, u, v)$ is a $C^1$ function.","I am reading ""A Course in Analysis vol.3"" by Kazuo Matsuzaka. The author wrote the following fact without a proof. I think it is easy to show that and are continuous. But I cannot show that is continuous. Please tell me the proof. Let be a continuous function from to , where is an interval in . Suppose that exists in and is continuous in . Let be a function from to . Then, is a function.","D_2 \phi(y, u, v) = f(v, y) D_3 \phi(y, u, v) = -f(u, y) D_1 \phi(y, u, v) = \int_{u}^{v} D_2 f(x, y) dx f(x, y) S = [a, b] \times I \subset \mathbb{R}^2 \mathbb{R} I \mathbb{R} D_2 f(x, y) S S \Phi(y, u, v) := \int_{u}^{v} f(x, y) dx I \times [a, b] \times [a, b] \mathbb{R} \Phi(y, u, v) C^1","['multivariable-calculus', 'partial-derivative']"
37,Formalising path connectedness,Formalising path connectedness,,"Let $L := \{(x,0) \in \mathbb{R}^2 : x \geq 0 \}$ .  Prove that $\mathbb{R}^2 \setminus L$ is path connected. Let $P := \{(x,y) \in \mathbb{R}^2 : y = x^2 \}$ .  Prove that $\mathbb{R}^2 \setminus L$ is path connected. I can visualise this pictorially but I'm struggling to find a rigorous proof, especially for number 1. For 1 we can draw as a straight line for points $p,q > 0$ or $p,q < 0$ . When $p$ and $q$ have different signs then we can draw a line to the origin and to $q$ .","Let .  Prove that is path connected. Let .  Prove that is path connected. I can visualise this pictorially but I'm struggling to find a rigorous proof, especially for number 1. For 1 we can draw as a straight line for points or . When and have different signs then we can draw a line to the origin and to .","L := \{(x,0) \in \mathbb{R}^2 : x \geq 0 \} \mathbb{R}^2 \setminus L P := \{(x,y) \in \mathbb{R}^2 : y = x^2 \} \mathbb{R}^2 \setminus L p,q > 0 p,q < 0 p q q","['general-topology', 'multivariable-calculus']"
38,flux through a line complex analysis,flux through a line complex analysis,,"I have a velocity field, say $$ \mathbb{v}=\left\langle\frac{-2xy}{(x^2+y^2)^2},\frac{x^2-y^2}{(x^2+y^2)^2}\right\rangle $$ First, I have to find the complex velocity potential, then I have to calculate the flux across a straight line from $2i$ to $1$ in the direction right to left. So for the complex velocity potential, since this velocity field is both irrotational and incompressible $\phi$ and $\psi$ exists. thus the complex velocity is $$ \Psi(x,y)=\phi(x,y)+i\psi(x,y) $$ where the complex velocity potential here is $$ \Psi(x,y)=\frac{x-y}{x^2+y^2}-i\frac{x^2-y^2}{x^2+y^2}. $$ I am not sure that this is correct. Lastly, the flux of $$\mathbb{v}=\int_\gamma\big(V\cdot n)d\vec{s}=\int_\gamma Pdy-Qdx$$ but this does not seem right, as I have a variable of two unknowns but only a 1D argument/input? Maybe if I use greens theorem? """"""""Edit"""""""""" So if I do this into a surface integral using greens theorem $$\int Pdy-Qdx = \iint \Big(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\Big)dxdy$$ But how would I know which limits I can use for which variables and how would one know the limit for the other variable, lets say that I use $2i$ to $1$ for $x$ how would I know what to use for $y$ ?","I have a velocity field, say First, I have to find the complex velocity potential, then I have to calculate the flux across a straight line from to in the direction right to left. So for the complex velocity potential, since this velocity field is both irrotational and incompressible and exists. thus the complex velocity is where the complex velocity potential here is I am not sure that this is correct. Lastly, the flux of but this does not seem right, as I have a variable of two unknowns but only a 1D argument/input? Maybe if I use greens theorem? """"""""Edit"""""""""" So if I do this into a surface integral using greens theorem But how would I know which limits I can use for which variables and how would one know the limit for the other variable, lets say that I use to for how would I know what to use for ?","
\mathbb{v}=\left\langle\frac{-2xy}{(x^2+y^2)^2},\frac{x^2-y^2}{(x^2+y^2)^2}\right\rangle
 2i 1 \phi \psi 
\Psi(x,y)=\phi(x,y)+i\psi(x,y)
 
\Psi(x,y)=\frac{x-y}{x^2+y^2}-i\frac{x^2-y^2}{x^2+y^2}.
 \mathbb{v}=\int_\gamma\big(V\cdot n)d\vec{s}=\int_\gamma Pdy-Qdx \int Pdy-Qdx = \iint \Big(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\Big)dxdy 2i 1 x y","['complex-analysis', 'multivariable-calculus']"
39,Finding the value of function defined as the limit of multivariable function,Finding the value of function defined as the limit of multivariable function,,"Suppose we have a function $f(x,y) = \frac{\cos{x}-\cos{y}}{2(x^2+y^2)}$ and we define $F(x) := \lim_{y \to 0} f(x,y)$ How would I go about finding $F(0)$ ? I am unclear whether to plug the value in first, and then take the limit, eg. $F(0) = \lim_{y \to 0} f(0,y) = \frac{1}{4}$ or take the limit first, and then plug in the value. $F(x) = \lim_{y \to 0} f(x,y) = \frac{\cos{x}-1}{2(x)^2} $ $F(0) = \frac{\cos{0}-1}{2(0)^2} $ In this case, the function is undefined. I have attempted to compute the value in Maple, and I've gotten the following f := (x, y) -> (cos(x) - cos(y))/(2*y^2 + 2*x^2) F := x -> limit(f(x, y), y = 0) F(0) = 1/4 This would suggest the first approach being correct, but I am still unsure whether this is actually true and why it is correct.","Suppose we have a function and we define How would I go about finding ? I am unclear whether to plug the value in first, and then take the limit, eg. or take the limit first, and then plug in the value. In this case, the function is undefined. I have attempted to compute the value in Maple, and I've gotten the following f := (x, y) -> (cos(x) - cos(y))/(2*y^2 + 2*x^2) F := x -> limit(f(x, y), y = 0) F(0) = 1/4 This would suggest the first approach being correct, but I am still unsure whether this is actually true and why it is correct.","f(x,y) = \frac{\cos{x}-\cos{y}}{2(x^2+y^2)} F(x) := \lim_{y \to 0} f(x,y) F(0) F(0) = \lim_{y \to 0} f(0,y) = \frac{1}{4} F(x) = \lim_{y \to 0} f(x,y) = \frac{\cos{x}-1}{2(x)^2}  F(0) = \frac{\cos{0}-1}{2(0)^2} ","['limits', 'multivariable-calculus']"
40,"2D LOTUS: joint PDF on unit square $\{ (x, y) : x, y \in [0, 1] \}$",2D LOTUS: joint PDF on unit square,"\{ (x, y) : x, y \in [0, 1] \}","My textbook, Introduction to Probability by Blitzstein and Hwang, presents the following example: Example 7.2.2 (Expected distance between two Uniforms). Let $X$ and $Y$ be i.i.d. Unif $(0, 1)$ r.v.s. Find $E(|X - Y|)$ . Solution : Since the joint PDF is 1 on the unit square $\{ (x, y) : x, y \in [0, 1] \}$ , 2D LOTUS gives $$\begin{align} E(|X - Y|) &= \int_0^1 \int_0^1 |x - y| \ dx dy \\ &= \int_0^1 \int_y^1 (x - y) \ dxdy + \int_0^1 \int_0^y (y - x) \ dxdy \\ &= 2 \int_0^1 \int_y^1 (x - y) \ dxdy = 1/3 \end{align}$$ First we broke up the integral into two parts so we could eliminate the absolute value; then we used symmetry. The textbook defines the 2D LOTUS as follows: Theorem 7.2.1 (2D LOTUS). Let $g$ be a function from $\mathbb{R}^2$ to $\mathbb{R}$ . If $X$ and $Y$ are discrete, then $$E(g(X, Y)) = \sum_x \sum_y g(x, y) P(X = x, Y = y).$$ If $X$ and $Y$ are continuous with joint PDF $f_{X, Y}$ , then $$E(g(X, Y)) = \int_{-\infty}^\infty \int_{-\infty}^\infty g(x, y) f_{X, Y}(x, y) \ dx dy.$$ LOTUS means Law of the Unconscious Statistician . My multiple integral knowledge is rusty, so I would appreciate it if people could please take the time to explain what's going on for each step here: $$\begin{align} E(|X - Y|) &= \int_0^1 \int_0^1 |x - y| \ dx dy \\ &= \int_0^1 \int_y^1 (x - y) \ dxdy + \int_0^1 \int_0^y (y - x) \ dxdy \\ &= 2 \int_0^1 \int_y^1 (x - y) \ dxdy = 1/3 \end{align}$$ Thank you. EDIT: The PDF of a continuous uniform random variable is $$f(x) = {\begin{cases}{\frac {1}{b-a}}&\mathrm {for} \ a\leq x\leq b,\\[8pt]0&\mathrm {for} \ x<a\ \mathrm {or} \ x>b\end{cases}}$$ Since $X$ and $Y$ are independent, my understanding is that the product of their individual PDFs is equal to the joint PDF: $$p_{X, Y}(x, y) = p_X(x) p_Y(y) = (1)(1) = 1$$","My textbook, Introduction to Probability by Blitzstein and Hwang, presents the following example: Example 7.2.2 (Expected distance between two Uniforms). Let and be i.i.d. Unif r.v.s. Find . Solution : Since the joint PDF is 1 on the unit square , 2D LOTUS gives First we broke up the integral into two parts so we could eliminate the absolute value; then we used symmetry. The textbook defines the 2D LOTUS as follows: Theorem 7.2.1 (2D LOTUS). Let be a function from to . If and are discrete, then If and are continuous with joint PDF , then LOTUS means Law of the Unconscious Statistician . My multiple integral knowledge is rusty, so I would appreciate it if people could please take the time to explain what's going on for each step here: Thank you. EDIT: The PDF of a continuous uniform random variable is Since and are independent, my understanding is that the product of their individual PDFs is equal to the joint PDF:","X Y (0, 1) E(|X - Y|) \{ (x, y) : x, y \in [0, 1] \} \begin{align} E(|X - Y|) &= \int_0^1 \int_0^1 |x - y| \ dx dy \\ &= \int_0^1 \int_y^1 (x - y) \ dxdy + \int_0^1 \int_0^y (y - x) \ dxdy \\ &= 2 \int_0^1 \int_y^1 (x - y) \ dxdy = 1/3 \end{align} g \mathbb{R}^2 \mathbb{R} X Y E(g(X, Y)) = \sum_x \sum_y g(x, y) P(X = x, Y = y). X Y f_{X, Y} E(g(X, Y)) = \int_{-\infty}^\infty \int_{-\infty}^\infty g(x, y) f_{X, Y}(x, y) \ dx dy. \begin{align} E(|X - Y|) &= \int_0^1 \int_0^1 |x - y| \ dx dy \\ &= \int_0^1 \int_y^1 (x - y) \ dxdy + \int_0^1 \int_0^y (y - x) \ dxdy \\ &= 2 \int_0^1 \int_y^1 (x - y) \ dxdy = 1/3 \end{align} f(x) = {\begin{cases}{\frac {1}{b-a}}&\mathrm {for} \ a\leq x\leq b,\\[8pt]0&\mathrm {for} \ x<a\ \mathrm {or} \ x>b\end{cases}} X Y p_{X, Y}(x, y) = p_X(x) p_Y(y) = (1)(1) = 1","['multivariable-calculus', 'probability-distributions', 'uniform-distribution']"
41,Show $ F^{\prime}\left(t_{0}\right)=D f_{p+t_{0} v}(v)=D_{v} f\left(p+t_{0} v\right) $,Show, F^{\prime}\left(t_{0}\right)=D f_{p+t_{0} v}(v)=D_{v} f\left(p+t_{0} v\right) ,"Suppose $f: \mathbb{R}^{m} \rightarrow \mathbb{R}^{n}$ is differentiable and given $p, v \in \mathbb{R}^{m}$ define $F(t)=f(p+t v)$ for $t \in \mathbb{R}$ . Show that for all $t_{0} \in \mathbb{R}$ we have $$ F^{\prime}\left(t_{0}\right)=D f_{p+t_{0} v}(v)=D_{v} f\left(p+t_{0} v\right) $$ My attempt. $F^{\prime}\left(t_{0}\right)=f'(p+t_0v)$ , $D_{v} f\left(p+t_{0} v\right)=\lim_{t\to 0} \dfrac {f(p+t_0v+tu)-f(p+t_0v)} {t},$ What is the $D f_{p+t_{0} v}(v)$ ? And how can I show that equality? $$ F^{\prime}\left(t_{0}\right)=D f_{p+t_{0} v}(v)=D_{v} f\left(p+t_{0} v\right) $$ May you help? Thanks...","Suppose is differentiable and given define for . Show that for all we have My attempt. , What is the ? And how can I show that equality? May you help? Thanks...","f: \mathbb{R}^{m} \rightarrow \mathbb{R}^{n} p, v \in \mathbb{R}^{m} F(t)=f(p+t v) t \in \mathbb{R} t_{0} \in \mathbb{R} 
F^{\prime}\left(t_{0}\right)=D f_{p+t_{0} v}(v)=D_{v} f\left(p+t_{0} v\right)
 F^{\prime}\left(t_{0}\right)=f'(p+t_0v) D_{v} f\left(p+t_{0} v\right)=\lim_{t\to 0} \dfrac {f(p+t_0v+tu)-f(p+t_0v)} {t}, D f_{p+t_{0} v}(v) 
F^{\prime}\left(t_{0}\right)=D f_{p+t_{0} v}(v)=D_{v} f\left(p+t_{0} v\right)
",['real-analysis']
42,"For each set, determine if it is compact, and if not, why not","For each set, determine if it is compact, and if not, why not",,"For each set, determine if it is compact, and if not, why not(it's not closed or not bounded $?$ ) $1.\{(x,y):1<e^{x^2+y^4}(x^4+y^2+1)<2\}$ $2. \{(x,y,z):|x|+|y|+|z|≤3\}$ $3. ⋃_{n∈\mathbb{Z}^+}[0,2−\frac{1}{n}]⊂\mathbb{R}$ $4. \{(x,e^x):0≤x≤10\}$ $5. \{(x,y):1≤e^{x^2+y^4}(x^4+y^2+1)≤2\}$ $6. \{(x,y):y=\sin(x)\}$ I checked the definition of compect set Def. compect set A set $S⊆\mathbb{R}^n$ is said to be compact if every sequence in $S$ has a subsequence that converges to a limit in $S$ . Seems like if the set isn't compact, then it's either not bounded or not closed, Take the contrapositive have bounded and closed implies compact $\dots$ My attempts $1.$ seems bounded but not closed so not compact $2.$ closed and compact so compact $3.$ union of closed set is closed and closed set are bounded so compact $4.$ bounded so compact $5.$ bounded so compact $6.$ not bounded so not compact Which might not be correct $\dots$ I'm a little confused about those concepts, Any help would be appreciated.","For each set, determine if it is compact, and if not, why not(it's not closed or not bounded ) I checked the definition of compect set Def. compect set A set is said to be compact if every sequence in has a subsequence that converges to a limit in . Seems like if the set isn't compact, then it's either not bounded or not closed, Take the contrapositive have bounded and closed implies compact My attempts seems bounded but not closed so not compact closed and compact so compact union of closed set is closed and closed set are bounded so compact bounded so compact bounded so compact not bounded so not compact Which might not be correct I'm a little confused about those concepts, Any help would be appreciated.","? 1.\{(x,y):1<e^{x^2+y^4}(x^4+y^2+1)<2\} 2. \{(x,y,z):|x|+|y|+|z|≤3\} 3. ⋃_{n∈\mathbb{Z}^+}[0,2−\frac{1}{n}]⊂\mathbb{R} 4. \{(x,e^x):0≤x≤10\} 5. \{(x,y):1≤e^{x^2+y^4}(x^4+y^2+1)≤2\} 6. \{(x,y):y=\sin(x)\} S⊆\mathbb{R}^n S S \dots 1. 2. 3. 4. 5. 6. \dots",['multivariable-calculus']
43,Connected components of intersection of curve with Ball,Connected components of intersection of curve with Ball,,"Given a smooth curve $\gamma\colon\left[0,1\right]\to\mathbb R^n$ , does the intersection $\mathrm{im}\,\gamma\cap B_1^n\left(0\right)$ have finitely many connected components? (Here $B^n_1\left(0\right)$ is the unit ball in $\mathbb R^n$ centred at $0\in\mathbb R^n$ ).","Given a smooth curve , does the intersection have finitely many connected components? (Here is the unit ball in centred at ).","\gamma\colon\left[0,1\right]\to\mathbb R^n \mathrm{im}\,\gamma\cap B_1^n\left(0\right) B^n_1\left(0\right) \mathbb R^n 0\in\mathbb R^n","['general-topology', 'multivariable-calculus', 'differential-topology']"
44,Some basic doubts about partial differentiation - PART II,Some basic doubts about partial differentiation - PART II,,"This is a continuation of Some basic doubts about partial differentiation . Doubt #3 : : Let us say that we have $f=f(x_1,x_2,....,x_n)$ where $x_i \in R$ . Is the following equation always true ? $$df=\frac {\partial f}{\partial x_1}dx_1+....+\frac {\partial f}{\partial x_n}dx_n$$ Doubt #4 : : Let $h=h(x,y,z)$ where $z$ itself is a function of the independent variables $x$ and $y$ , i.e., $z=z(x,y)$ . Which of the following equations is true ? $$ dh=\frac{\partial h}{\partial x}dx + \frac{\partial h}{\partial y}dy + \frac{\partial h}{\partial z}dz\,\,\,\,(1)$$ $$dh=\frac{\partial h}{\partial x}dx + \frac{\partial h}{\partial y}dy  \,\,\,\,(2)$$ I wrote Equation $(1)$ thinking that $h$ depends on $x, y$ and $z$ . I wrote Equation $(2)$ thinking that because $z$ can be expressed in terms of $x$ and $y$ , $h(x,y,z)$ is basically a function of only $x$ and $y$ . But I can not decide which of these two equations is actually correct. $\frac {\partial h}{\partial z}$ means differentiating $h$ wrt $z$ while holding $x$ and $y$ constant. But if $x$ and $y$ are constant, doesn't that mean that $z=z(x,y)$ is also a constant ? Does $\frac {\partial h}{\partial z}$ even mean anything here ? NOTE : As you can clearly see by the nature of my doubts, I am just a beginner in multivariable calculus. So please answer in simple terms. Thanks in advance :-).","This is a continuation of Some basic doubts about partial differentiation . Doubt #3 : : Let us say that we have where . Is the following equation always true ? Doubt #4 : : Let where itself is a function of the independent variables and , i.e., . Which of the following equations is true ? I wrote Equation thinking that depends on and . I wrote Equation thinking that because can be expressed in terms of and , is basically a function of only and . But I can not decide which of these two equations is actually correct. means differentiating wrt while holding and constant. But if and are constant, doesn't that mean that is also a constant ? Does even mean anything here ? NOTE : As you can clearly see by the nature of my doubts, I am just a beginner in multivariable calculus. So please answer in simple terms. Thanks in advance :-).","f=f(x_1,x_2,....,x_n) x_i \in R df=\frac {\partial f}{\partial x_1}dx_1+....+\frac {\partial f}{\partial x_n}dx_n h=h(x,y,z) z x y z=z(x,y)  dh=\frac{\partial h}{\partial x}dx + \frac{\partial h}{\partial y}dy + \frac{\partial h}{\partial z}dz\,\,\,\,(1) dh=\frac{\partial h}{\partial x}dx + \frac{\partial h}{\partial y}dy  \,\,\,\,(2) (1) h x, y z (2) z x y h(x,y,z) x y \frac {\partial h}{\partial z} h z x y x y z=z(x,y) \frac {\partial h}{\partial z}","['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
45,Solving an inequality with three variables,Solving an inequality with three variables,,"Find the largest integer $\lambda$ such that : $$\frac{\lambda (xyz)}{x+y+z} \le(x+y)^2 + (x+y+4z)^2.$$ Here $x,y$ and $z$ are positive real numbers The answer is only achieved using calculus. Can anyone use AM-GM and get $\lambda$ ?",Find the largest integer such that : Here and are positive real numbers The answer is only achieved using calculus. Can anyone use AM-GM and get ?,"\lambda \frac{\lambda (xyz)}{x+y+z} \le(x+y)^2 + (x+y+4z)^2. x,y z \lambda","['multivariable-calculus', 'inequality', 'optimization', 'a.m.-g.m.-inequality']"
46,Demonstrate divergence and rotational.,Demonstrate divergence and rotational.,,"Show that $$DIV(A)=\lim_{\Delta s\rightarrow0}\frac{\displaystyle\int\int_{\Delta v}A\cdot nds}{\Delta v}$$ and, $$ROT(A)\cdot n=\lim_{\Delta s\rightarrow 0}\frac{\displaystyle\oint_{C}A\cdot dr}{\Delta s}$$ Is there a demonstration for these results? One suggestion they gave me is to use the mean value theorem for integrals. Any suggestions?","Show that and, Is there a demonstration for these results? One suggestion they gave me is to use the mean value theorem for integrals. Any suggestions?",DIV(A)=\lim_{\Delta s\rightarrow0}\frac{\displaystyle\int\int_{\Delta v}A\cdot nds}{\Delta v} ROT(A)\cdot n=\lim_{\Delta s\rightarrow 0}\frac{\displaystyle\oint_{C}A\cdot dr}{\Delta s},"['calculus', 'multivariable-calculus', 'electromagnetism']"
47,Directional derivative of the $\ell_1$ norm,Directional derivative of the  norm,\ell_1,"The following paragraph is taken from Nocedal and Wright's Numerical Optimization (p. 628 of the second edition). Consider for instance the $\ell_1$ norm function $f(x)=\|x\|_1$ . We have from the definition (A.51) that $$ D(\|x\|_1;p) =\lim_{\epsilon\to0}\frac{\|x+\epsilon p\|_1-\|x\|_1}\epsilon =\lim_{\epsilon\to0}\frac{\sum_{i=1}^n|x_i+\epsilon p_i|-\sum_{i=1}^n|x_i|}\epsilon. $$ If $x_i>0$ , we have $|x_i+\epsilon p_i|=|x_i|+\epsilon p_i$ for all $\epsilon$ sufficiently small. If $x_i<0$ , we have $|x_i+\epsilon p_i|=|x_i|-\epsilon p_i$ , while if $x_i=0$ , we have $|x_i+\epsilon p_i|=\epsilon|p_i|$ . Therefore, we have $$ D(\|x\|_1;p) =\sum_{i|x_i<0}-p_i+\sum_{i|x_i>0}p_i+\sum_{i|x_i=0}|p_i|, $$ so the directional derivative of this function exists for any $x$ and $p$ . How do they obtain that $|x_i+\epsilon p_i|=\epsilon|p_i|$ when $x_i=0$ ? It seems that it should be $|x_i+\epsilon p_i|=|\epsilon||p_i|$ . Then the limits when $\epsilon\downarrow0$ and $\epsilon\uparrow0$ do not coincide and hence the derivative does not exist. Is it true that the $\ell_1$ norm has directional derivatives for any $x$ and $p$ ? Any help is much appreciated!","The following paragraph is taken from Nocedal and Wright's Numerical Optimization (p. 628 of the second edition). Consider for instance the norm function . We have from the definition (A.51) that If , we have for all sufficiently small. If , we have , while if , we have . Therefore, we have so the directional derivative of this function exists for any and . How do they obtain that when ? It seems that it should be . Then the limits when and do not coincide and hence the derivative does not exist. Is it true that the norm has directional derivatives for any and ? Any help is much appreciated!","\ell_1 f(x)=\|x\|_1 
D(\|x\|_1;p)
=\lim_{\epsilon\to0}\frac{\|x+\epsilon p\|_1-\|x\|_1}\epsilon
=\lim_{\epsilon\to0}\frac{\sum_{i=1}^n|x_i+\epsilon p_i|-\sum_{i=1}^n|x_i|}\epsilon.
 x_i>0 |x_i+\epsilon p_i|=|x_i|+\epsilon p_i \epsilon x_i<0 |x_i+\epsilon p_i|=|x_i|-\epsilon p_i x_i=0 |x_i+\epsilon p_i|=\epsilon|p_i| 
D(\|x\|_1;p)
=\sum_{i|x_i<0}-p_i+\sum_{i|x_i>0}p_i+\sum_{i|x_i=0}|p_i|,
 x p |x_i+\epsilon p_i|=\epsilon|p_i| x_i=0 |x_i+\epsilon p_i|=|\epsilon||p_i| \epsilon\downarrow0 \epsilon\uparrow0 \ell_1 x p","['multivariable-calculus', 'derivatives']"
48,"For which values of $a\in \mathbb{R}$ is there a max or min for $f(x,y) = (x-1)^2 +2a(x-1)y +y^2 +y^4$ in $(1,0)$?",For which values of  is there a max or min for  in ?,"a\in \mathbb{R} f(x,y) = (x-1)^2 +2a(x-1)y +y^2 +y^4 (1,0)","Given $a \in \mathbb{R}$ , and $$f(x,y)=(x-1)^2 +2a(x-1)y + y^2+y^4$$ determine which values for $a$ that will make $f(x,y)$ have a maximum or minimum in $(1,0)$ . My attempt so far is as follows: I calculate the partial derivatives $f_{x} = 2ay + 2(x-1)$ , $f_{y} = 4y^3 +2y +2a(x-1)$ , $f_{xx} = 2$ , $f_{yy} = 12y^2 +2$ , $f_{xy} = 2a$ . And aquire the charateristic polynomial for the point $(1,0)$ : $$Q(h,k)= 2h^2 + 4ahk + 2k^2$$ From here I am unsure about how to determine which values for $a$ that will make the polynomial tell me the point is a max or min. Another approach, which I am less sure about being correct is using the Hessian matrix. It becomes $$\begin{bmatrix} 2a & 2\\\ 2 & 2a\end{bmatrix}$$ which has the determinant $4a^2-4$ which is $>0$ for all $a>1$ and $a<-1$ . Is this approach correct? Please answer both how to do it using the characteristic polynomial and if the method with the Hessian matrix is correct.","Given , and determine which values for that will make have a maximum or minimum in . My attempt so far is as follows: I calculate the partial derivatives , , , , . And aquire the charateristic polynomial for the point : From here I am unsure about how to determine which values for that will make the polynomial tell me the point is a max or min. Another approach, which I am less sure about being correct is using the Hessian matrix. It becomes which has the determinant which is for all and . Is this approach correct? Please answer both how to do it using the characteristic polynomial and if the method with the Hessian matrix is correct.","a \in \mathbb{R} f(x,y)=(x-1)^2 +2a(x-1)y + y^2+y^4 a f(x,y) (1,0) f_{x} = 2ay + 2(x-1) f_{y} = 4y^3 +2y +2a(x-1) f_{xx} = 2 f_{yy} = 12y^2 +2 f_{xy} = 2a (1,0) Q(h,k)= 2h^2 + 4ahk + 2k^2 a \begin{bmatrix} 2a & 2\\\
2 & 2a\end{bmatrix} 4a^2-4 >0 a>1 a<-1","['multivariable-calculus', 'quadratic-forms', 'hessian-matrix']"
49,Interpretation of higher order multivariate differential.,Interpretation of higher order multivariate differential.,,"I'm having trouble with this definition of the higher order differential that I'm presented: If $f: U \subset \mathbb{R}^n \rightarrow \mathbb{R}^m$ is k-times continuously differentiable in a neighbourhood of $x_0 \in U$ , then the differential of order k $d^{(k)}f(x_0):\mathbb{R}^n \times \mathbb{R}^n \times ... \times \mathbb{R}^n \rightarrow \mathbb{R}^m$ is explained as symmetric k-linear mapping through: $d^{(k)}f(x_0)(v_1,...,v_k) = \partial_{v_1},...,\partial_{v_k}f(x_0), (v_1,...,v_k \in \mathbb{R}^n)$ The definition continues by providing examples for different orders: $d^{(1)}f(x_0)v = \partial_vf(x_0) = lim_{t \to 0} \frac{f(x_0+tv)-f(x_0)}{t} = \sum_{i=1}^n \frac{\partial f}{\partial x_i}(x_0) v^i$ for $v = \left( \begin{array}{c} v^1\\ \vdots\\ v^n\\ \end{array} \right) \in \mathbb{R}^n$ . finally arriving at: $d^{(k)}f(x_0)(v_1,...,v_k) = \sum_{i_1,...,i_k = 1}^n \frac{\partial^kf(x_0)}{\partial x_{i1}...\partial x_{ik}} v_1^{(i_1)}...v_k^{(i_k)}$ for $v_i = \left( \begin{array}{c} v_i^1\\ \vdots\\ v_i^n\\ \end{array} \right), 1 \leq i \leq k$ . Im confused by the very first definition, as I can't imagine what $d^{(k)}f(x_0)(v_1,...,v_k)$ is even aiming for. I know the following lines look like the directional derivatives for different $v$ , but why would we bring them up in the definition of the higher order differential? Any explanation of the definition is highly appreciated. Also, I think an example (or a link to an example) would really help me.","I'm having trouble with this definition of the higher order differential that I'm presented: If is k-times continuously differentiable in a neighbourhood of , then the differential of order k is explained as symmetric k-linear mapping through: The definition continues by providing examples for different orders: for . finally arriving at: for . Im confused by the very first definition, as I can't imagine what is even aiming for. I know the following lines look like the directional derivatives for different , but why would we bring them up in the definition of the higher order differential? Any explanation of the definition is highly appreciated. Also, I think an example (or a link to an example) would really help me.","f: U \subset \mathbb{R}^n \rightarrow \mathbb{R}^m x_0 \in U d^{(k)}f(x_0):\mathbb{R}^n \times \mathbb{R}^n \times ... \times \mathbb{R}^n \rightarrow \mathbb{R}^m d^{(k)}f(x_0)(v_1,...,v_k) = \partial_{v_1},...,\partial_{v_k}f(x_0), (v_1,...,v_k \in \mathbb{R}^n) d^{(1)}f(x_0)v = \partial_vf(x_0) = lim_{t \to 0} \frac{f(x_0+tv)-f(x_0)}{t} = \sum_{i=1}^n \frac{\partial f}{\partial x_i}(x_0) v^i v = \left(
\begin{array}{c}
v^1\\
\vdots\\
v^n\\
\end{array}
\right) \in \mathbb{R}^n d^{(k)}f(x_0)(v_1,...,v_k) = \sum_{i_1,...,i_k = 1}^n \frac{\partial^kf(x_0)}{\partial x_{i1}...\partial x_{ik}} v_1^{(i_1)}...v_k^{(i_k)} v_i = \left(
\begin{array}{c}
v_i^1\\
\vdots\\
v_i^n\\
\end{array}
\right), 1 \leq i \leq k d^{(k)}f(x_0)(v_1,...,v_k) v","['real-analysis', 'multivariable-calculus', 'derivatives', 'differential']"
50,From $f(t)=o(\|t\|^2)$ to $\frac{\partial f}{\partial y}(t) = o (\|t\|)$,From  to,f(t)=o(\|t\|^2) \frac{\partial f}{\partial y}(t) = o (\|t\|),"Suppose that $f:\mathbb{R}^2\to\mathbb{R}$ satisfies $f(0,0)=0$ and $f(t)=o(\|t\|^2)$ , $t\to (0,0)$ . If the first and second partial derivatives all exist, is it true that $$\frac{\partial f}{\partial y}(t) = o (\|t\|)\, , \, t\to (0,0)$$ $$\frac{\partial f}{\partial x}(t) = o (\|t\|)\, , \, t\to (0,0)$$ ? My attempt Put $\phi(s)=f(su)$ where $u=(1,0)$ . Then we have $$\phi(s)=\phi(0)+\phi'(0) s + \frac12 \phi''(0)s^2 +o(s^2)\, , \, s\to 0$$ Via $f(t)=o(\|t\|^2)$ , $t\to (0,0)$ we have $\phi(s)=o(s^2)$ and hence $\phi(0)=\phi'(0)=\phi''(0)=0$ . Thus $$\frac{\partial f}{\partial x}(su)=o(s)\, , \, s\to 0$$ But I get confused whether we can say $$\frac{\partial f}{\partial x}(t) = o (\|t\|)\, , \, t\to (0,0)$$ Any hints? Thanks in advance! Added The reason why I asked this question is to try to find a possible method to solve the following problem . Suppose that $f:\mathbb{R}^n\to\mathbb{R}$ with $f(0)=0$ satisfying $$f(x)=o(\|x\|^m)\,,\,x\to0$$ If all the $m$ -order  partial derivatives exist, prove that the partial derivatives equal $0$ at the origin (or give a counterexample). Actually based on the answer that was put forward, it is certain that this method does not work. I also want to ask for some hints for the new posted question. Thanks in advance!","Suppose that satisfies and , . If the first and second partial derivatives all exist, is it true that ? My attempt Put where . Then we have Via , we have and hence . Thus But I get confused whether we can say Any hints? Thanks in advance! Added The reason why I asked this question is to try to find a possible method to solve the following problem . Suppose that with satisfying If all the -order  partial derivatives exist, prove that the partial derivatives equal at the origin (or give a counterexample). Actually based on the answer that was put forward, it is certain that this method does not work. I also want to ask for some hints for the new posted question. Thanks in advance!","f:\mathbb{R}^2\to\mathbb{R} f(0,0)=0 f(t)=o(\|t\|^2) t\to (0,0) \frac{\partial f}{\partial y}(t) = o (\|t\|)\, , \, t\to (0,0) \frac{\partial f}{\partial x}(t) = o (\|t\|)\, , \, t\to (0,0) \phi(s)=f(su) u=(1,0) \phi(s)=\phi(0)+\phi'(0) s + \frac12 \phi''(0)s^2 +o(s^2)\, , \, s\to 0 f(t)=o(\|t\|^2) t\to (0,0) \phi(s)=o(s^2) \phi(0)=\phi'(0)=\phi''(0)=0 \frac{\partial f}{\partial x}(su)=o(s)\, , \, s\to 0 \frac{\partial f}{\partial x}(t) = o (\|t\|)\, , \, t\to (0,0) f:\mathbb{R}^n\to\mathbb{R} f(0)=0 f(x)=o(\|x\|^m)\,,\,x\to0 m 0","['multivariable-calculus', 'partial-derivative']"
51,How might one formulate integration over an ordered probability simplex?,How might one formulate integration over an ordered probability simplex?,,"Given a probability simplex ( $x_i \geq 0$ , $i=1,\ldots,n$ ) \begin{equation} \sum_{i=1}^n x_i=1, \end{equation} I want to attempt integrations of various (""operator-monotone""-based https://pdfs.semanticscholar.org/d393/21f142432eddd2af0d3bd07235a63aca2019.pdf ) functions over that subsection for which \begin{equation} x_i \geq x_{i-1}. \end{equation} Further, I'm particularly interested in such integrations ( $n=4$ ) with the additional (""absolute separability"") constraint (eq. (3) in https://arxiv.org/pdf/quant-ph/0502170.pdf ) \begin{equation} x_1 \leq x_3+2 \sqrt{x_2 x_4}. \end{equation} What are my options for setting up the integrations (coordinate transformations might be of interest)? The particular integrations I have in mind are seemingly very challenging, and I may (probably) have to resort to numerical methods.","Given a probability simplex ( , ) I want to attempt integrations of various (""operator-monotone""-based https://pdfs.semanticscholar.org/d393/21f142432eddd2af0d3bd07235a63aca2019.pdf ) functions over that subsection for which Further, I'm particularly interested in such integrations ( ) with the additional (""absolute separability"") constraint (eq. (3) in https://arxiv.org/pdf/quant-ph/0502170.pdf ) What are my options for setting up the integrations (coordinate transformations might be of interest)? The particular integrations I have in mind are seemingly very challenging, and I may (probably) have to resort to numerical methods.","x_i \geq 0 i=1,\ldots,n \begin{equation}
\sum_{i=1}^n x_i=1,
\end{equation} \begin{equation}
x_i \geq x_{i-1}.
\end{equation} n=4 \begin{equation}
x_1 \leq x_3+2 \sqrt{x_2 x_4}.
\end{equation}","['probability', 'integration', 'multivariable-calculus', 'simplex']"
52,Limit in multivariable calculus,Limit in multivariable calculus,,"Find $\lim_{(x,y) \to 0} x \ln (xy) ?$ Problem for this problem is already solved here on MSE.  But can't we find the limit using polar coordinates. As $\lim _{r \to 0} r \cos \theta \ln( r^{2} \cos \theta \sin \theta) = 0$ so limit should be zero. But the limit of this function doesn't exist. What is wrong with this method $?$",Find Problem for this problem is already solved here on MSE.  But can't we find the limit using polar coordinates. As so limit should be zero. But the limit of this function doesn't exist. What is wrong with this method,"\lim_{(x,y) \to 0} x \ln (xy) ? \lim _{r \to 0} r \cos \theta \ln( r^{2} \cos \theta \sin \theta) = 0 ?","['calculus', 'limits', 'multivariable-calculus']"
53,Is this a correct application of Stokes' theorem?,Is this a correct application of Stokes' theorem?,,"The problem: Compute the line integral $$\int_{\gamma} F \,\mathrm d r$$ where $F$ is a vector field $$F(x, y, z) = \left( y^2+z^4+x, 2xy-z^2+x, x^{18}y^6+z^{2016} \right)$$ and $\gamma$ is the bound of the intersection $T_1 \cap T_2$ where $$T_1 = \big\{(x, y, z) \in \mathbb{R}^3 \mid x^2+y^2\leq16 \big\}$$ and $$T_2 = \big\{(x, y, z)\in \mathbb{R}^3 \mid z=4, 0\leq y \leq 3 \big\}$$ The orientation of the curve is clockwise if looked at from the point $(0, 0, 100)$ . Since the given vector field seems highly complicated I immediately assumed that Stokes' theorem should be used. So the curve that we're integrating over is the circle $x^2+y^2\leq16$ on the height of $z=4$ where $0\leq y \leq 3$ . If looked at from the top it looks something like this: Now the surface that this encloses can be parametrized as: $$S: r(x, y)=(x, y, 4), \text{where } (x, y)\in D=\big\{(x, y)\in \mathbb{R^2}|x^2+y^2\leq16, 0 \leq y \leq 3 \big\}$$ The normal vector of this surface is $r'_x \times r'_y=(0, 0, 1)$ . Since, after applying Stokes' theorem, I'm going to be doing a scalar product of $(\nabla \times F)(r(x, y)) \cdot (r'_x \times r'_y) = (\nabla \times F)(r(x, y)) \cdot (0, 0, 1)$ , I've concluded that I don't care about the first two components of $\nabla \times F$ , therefore I've computed it as: $$\nabla \times F=(P, Q, \frac{d}{dx}(2xy-z^2+x) - \frac{d}{dy}(y^2+z^4+x)) = (P, Q, 2y+1-2y) = (P, Q, 1) = G$$ By the way, the $P$ and $Q$ are not meant to represent the original components of the vector field $F$ but rather just placeholders. And since Stokes' theorem says that the curve that encloses the surface should be oriented such that, when traversing the curve, the surface remains ""on the left side"", we'll add a minus sign, getting: $$\int_{\gamma}Fdr = - \iint_S{(\nabla \times F)dS} = -\iint_S{GdS} = -\iint_D{G(r(x, y)) \cdot (0, 0, 1)dxdy} = -\iint_D{(P(r(x, y), Q(r(x, y), 1) \cdot (0, 0, 1)}dxdy = -\iint_Ddxdy = - A(D)$$ Computing the area of the region $D$ gives some really uncomfortable results. I've done it by subtracting the area of the little 'cap' on the top (the part that gets cut off by the line $y=3$ ) from the area of the half-circle. Furthermore I've gotten the area of the cap by subtracting the area of the triangle formed by points $(0, 0), (\sqrt{7}, 3), (-\sqrt{7}, 3)$ from the area of the ""pizza-slice"" (don't know the English word for it) enclosed by the circle and lines $y=\frac{\pm \sqrt{7}}{3}$ . All that process got me to a really ugly solution: $$= 8\pi\bigg(1-\pi+2\arctan\bigg(\frac{3}{\sqrt7}\bigg)\bigg) + 3\sqrt7$$ So the final solution should be $-1$ times that. This is the first problem in which I've actually applied Stokes' theorem and you can see why I'm being skeptical about whether I've done it correctly. So, does my process seem correct? (also concerned about the orientation part aside from the very complicated result). Thanks.","The problem: Compute the line integral where is a vector field and is the bound of the intersection where and The orientation of the curve is clockwise if looked at from the point . Since the given vector field seems highly complicated I immediately assumed that Stokes' theorem should be used. So the curve that we're integrating over is the circle on the height of where . If looked at from the top it looks something like this: Now the surface that this encloses can be parametrized as: The normal vector of this surface is . Since, after applying Stokes' theorem, I'm going to be doing a scalar product of , I've concluded that I don't care about the first two components of , therefore I've computed it as: By the way, the and are not meant to represent the original components of the vector field but rather just placeholders. And since Stokes' theorem says that the curve that encloses the surface should be oriented such that, when traversing the curve, the surface remains ""on the left side"", we'll add a minus sign, getting: Computing the area of the region gives some really uncomfortable results. I've done it by subtracting the area of the little 'cap' on the top (the part that gets cut off by the line ) from the area of the half-circle. Furthermore I've gotten the area of the cap by subtracting the area of the triangle formed by points from the area of the ""pizza-slice"" (don't know the English word for it) enclosed by the circle and lines . All that process got me to a really ugly solution: So the final solution should be times that. This is the first problem in which I've actually applied Stokes' theorem and you can see why I'm being skeptical about whether I've done it correctly. So, does my process seem correct? (also concerned about the orientation part aside from the very complicated result). Thanks.","\int_{\gamma} F \,\mathrm d r F F(x, y, z) = \left( y^2+z^4+x, 2xy-z^2+x, x^{18}y^6+z^{2016} \right) \gamma T_1 \cap T_2 T_1 = \big\{(x, y, z) \in \mathbb{R}^3 \mid x^2+y^2\leq16 \big\} T_2 = \big\{(x, y, z)\in \mathbb{R}^3 \mid z=4, 0\leq y \leq 3 \big\} (0, 0, 100) x^2+y^2\leq16 z=4 0\leq y \leq 3 S: r(x, y)=(x, y, 4), \text{where } (x, y)\in D=\big\{(x, y)\in \mathbb{R^2}|x^2+y^2\leq16, 0 \leq y \leq 3 \big\} r'_x \times r'_y=(0, 0, 1) (\nabla \times F)(r(x, y)) \cdot (r'_x \times r'_y) = (\nabla \times F)(r(x, y)) \cdot (0, 0, 1) \nabla \times F \nabla \times F=(P, Q, \frac{d}{dx}(2xy-z^2+x) - \frac{d}{dy}(y^2+z^4+x)) = (P, Q, 2y+1-2y) = (P, Q, 1) = G P Q F \int_{\gamma}Fdr = - \iint_S{(\nabla \times F)dS} = -\iint_S{GdS} = -\iint_D{G(r(x, y)) \cdot (0, 0, 1)dxdy} = -\iint_D{(P(r(x, y), Q(r(x, y), 1) \cdot (0, 0, 1)}dxdy = -\iint_Ddxdy = - A(D) D y=3 (0, 0), (\sqrt{7}, 3), (-\sqrt{7}, 3) y=\frac{\pm \sqrt{7}}{3} = 8\pi\bigg(1-\pi+2\arctan\bigg(\frac{3}{\sqrt7}\bigg)\bigg) + 3\sqrt7 -1","['integration', 'multivariable-calculus', 'surface-integrals', 'line-integrals', 'stokes-theorem']"
54,How to find the volume enclosed by intersection of three orthogonal cylinders?,How to find the volume enclosed by intersection of three orthogonal cylinders?,,"If I have three cylinders $$x^2 + y^2 =1 $$ $$ x^2 + z^2 =1  $$ $$y^2 + z^2 =1$$ and I need to find the volume contained in their intersection. I know the figure I will get is a steinmetz solid looking like this . Now I was looking for the solution and I found this thread Intersection of Three Cylinders of equal radius . In this thread Mr. John Hughes explains beautifully about the figure. Here is the image . What I want is to compute the volume of this solid. I can infer from my research and little of intuition that the region PCB belongs to $x^2 + y^2 =1$ , region PAC belongs to $y^2 + z^2 =1$ and region PAB belongs to $x^2 + z^2 =1$ . If I try to find the volume I would set an integral like this $$\iiint_E dx~ dy~ dz $$ where $E$ is the whole region ABC along with P. Now, the problem is the bounds of the integrals. I'm having no idea of how to get the bound. I can (with little confidence) say that $y$ is bound by the curve PCB so I may write $$0 \leq y \leq +\sqrt{1-x^2}$$ and $$ 0 \leq x \leq 1$$ . I'm stuck here, I need help with as much elaboration possible from basics. I know this type of question has been asked multiple times and each time a different think has been asked, so I'm going with the tradition. Please help me in cartesian coordinates only and with this diagram, if possible. Curvilinear coordinates's bounds would also work but please add a through explanation to it. Thank you. I hope someone will surely help me through this. I apologize if I'm asking a conventional thing, I apologize if I'm too demanding , I apologize if I'm wrong in my very essence of the concept.","If I have three cylinders and I need to find the volume contained in their intersection. I know the figure I will get is a steinmetz solid looking like this . Now I was looking for the solution and I found this thread Intersection of Three Cylinders of equal radius . In this thread Mr. John Hughes explains beautifully about the figure. Here is the image . What I want is to compute the volume of this solid. I can infer from my research and little of intuition that the region PCB belongs to , region PAC belongs to and region PAB belongs to . If I try to find the volume I would set an integral like this where is the whole region ABC along with P. Now, the problem is the bounds of the integrals. I'm having no idea of how to get the bound. I can (with little confidence) say that is bound by the curve PCB so I may write and . I'm stuck here, I need help with as much elaboration possible from basics. I know this type of question has been asked multiple times and each time a different think has been asked, so I'm going with the tradition. Please help me in cartesian coordinates only and with this diagram, if possible. Curvilinear coordinates's bounds would also work but please add a through explanation to it. Thank you. I hope someone will surely help me through this. I apologize if I'm asking a conventional thing, I apologize if I'm too demanding , I apologize if I'm wrong in my very essence of the concept.",x^2 + y^2 =1   x^2 + z^2 =1   y^2 + z^2 =1 x^2 + y^2 =1 y^2 + z^2 =1 x^2 + z^2 =1 \iiint_E dx~ dy~ dz  E y 0 \leq y \leq +\sqrt{1-x^2}  0 \leq x \leq 1,"['multivariable-calculus', 'volume']"
55,Calculating partial derivatives.,Calculating partial derivatives.,,"Find the values of $n$ so that the function $v=r^n(3\cos^2\theta-1)$ satisfies the relation $$\dfrac{\partial}{\partial r}\bigg(r^2\dfrac{\partial v}{\partial r}\bigg)+\dfrac{1}{\sin\theta}\dfrac{\partial v}{\partial\theta}\bigg(\sin\theta\dfrac{\partial v}{\partial\theta}\bigg)=0$$ I got, $$\dfrac{\partial}{\partial r}\bigg(r^2\dfrac{\partial v}{\partial r}\bigg)=n(n+1)r^n(3\cos^2\theta-1)$$ Also, $$\dfrac{1}{\sin\theta}\dfrac{\partial v}{\partial\theta}\bigg(\sin\theta\dfrac{\partial v}{\partial\theta}\bigg)=36\sin^2\theta\cdot\cos^2\theta \cdot r^{2n}$$ Adding them together and equating it to zero gives, $$n(n+1)(3\cos^2\theta-1)+9\sin^22\theta\cdot r^n=0$$ I don't know how to get $n$ from three unknowns, please help.","Find the values of so that the function satisfies the relation I got, Also, Adding them together and equating it to zero gives, I don't know how to get from three unknowns, please help.",n v=r^n(3\cos^2\theta-1) \dfrac{\partial}{\partial r}\bigg(r^2\dfrac{\partial v}{\partial r}\bigg)+\dfrac{1}{\sin\theta}\dfrac{\partial v}{\partial\theta}\bigg(\sin\theta\dfrac{\partial v}{\partial\theta}\bigg)=0 \dfrac{\partial}{\partial r}\bigg(r^2\dfrac{\partial v}{\partial r}\bigg)=n(n+1)r^n(3\cos^2\theta-1) \dfrac{1}{\sin\theta}\dfrac{\partial v}{\partial\theta}\bigg(\sin\theta\dfrac{\partial v}{\partial\theta}\bigg)=36\sin^2\theta\cdot\cos^2\theta \cdot r^{2n} n(n+1)(3\cos^2\theta-1)+9\sin^22\theta\cdot r^n=0 n,"['calculus', 'multivariable-calculus', 'trigonometry', 'partial-derivative']"
56,Partial derivatives of $\ln(x+y)$,Partial derivatives of,\ln(x+y),"I am having trouble with because in my guide the answer is different. I want to calculate the total derivative of the function: $f(x,y)=\ln(x+y)$ By definition: The Total derivative/Chain rule for functions of functions. If $\omega=f(x,y)$ a continuous function. Then the total derivative is: $$\frac{\partial \omega }{\partial t} = \frac{\partial \omega }{\partial x}\frac{d x }{d t} +  \frac{\partial \omega }{\partial y}\frac{d y }{d t}  $$ For our case $t=x+y$ and $$\omega= f(x,y)=\ln(x+y)$$ I got: $$\frac{1}{x+y}$$ But in the book, the correct answer is: $$\frac{x+y}{x+y}$$ What am I doing wrong?","I am having trouble with because in my guide the answer is different. I want to calculate the total derivative of the function: By definition: The Total derivative/Chain rule for functions of functions. If a continuous function. Then the total derivative is: For our case and I got: But in the book, the correct answer is: What am I doing wrong?","f(x,y)=\ln(x+y) \omega=f(x,y) \frac{\partial \omega }{\partial t} = \frac{\partial \omega }{\partial x}\frac{d x }{d t} +  \frac{\partial \omega }{\partial y}\frac{d y }{d t}   t=x+y \omega= f(x,y)=\ln(x+y) \frac{1}{x+y} \frac{x+y}{x+y}","['calculus', 'multivariable-calculus']"
57,"Differentiation of a function of three variables, dependent on a function of two variables. Chain rule","Differentiation of a function of three variables, dependent on a function of two variables. Chain rule",,"Question: Let $ f(s,t) $ be a differentiable function of two variables and let $h(x,y,z)=z\cdot f(\frac{x}{z}, \frac{y}{z})$ . Simplify the expression $(x,y,z) \cdot \nabla h$ I am having trouble understanding how to go about this problem. My solution attempt: $\frac{\partial f}{\partial x} = \frac{\partial f}{\partial s}\cdot \frac{\partial s}{\partial x} + \frac{\partial f}{\partial t}\cdot \frac{\partial t}{\partial x} = \frac{\partial f}{\partial s}\cdot\frac{1}{z}$ $\frac{\partial f}{\partial y} = \frac{\partial f}{\partial s}\cdot \frac{\partial s}{\partial y} + \frac{\partial f}{\partial t}\cdot \frac{\partial t}{\partial y} = \frac{\partial f}{\partial s}\cdot\frac{1}{z}$ $\frac{\partial f}{\partial z} = \frac{\partial f}{\partial s}\cdot \frac{\partial s}{\partial z} + \frac{\partial f}{\partial t}\cdot \frac{\partial t}{\partial z} = \frac{\partial f}{\partial s}\cdot\frac{-x}{z^2} + \frac{\partial f}{\partial t}\cdot\frac{-y}{z^2}$ This should in turn give $\nabla h = (\frac{\partial f}{\partial s}, \frac{\partial f}{\partial t}, f(\frac{x}{z},\frac{y}{z}) + \frac{\partial f}{\partial s} \cdot \frac{-x}{z^2} + \frac{\partial f}{\partial t} \cdot \frac{-y}{z^2})$ And therefore $(x, y, z) \cdot \nabla h = x \cdot \frac{\partial f}{\partial s} + y \cdot \frac{\partial f}{\partial t} + z\cdot (f( \frac{x}{z} , \frac{y}{z} ) + \frac{\partial f}{\partial s} \cdot \frac{-x}{z^2} + \frac{\partial f}{\partial t} \cdot \frac{-y}{z^2}) = \frac{\partial f}{\partial s \cdot z} + \frac{\partial f}{\partial t \cdot z} + z \cdot f( \frac{x}{z}, \frac{y}{z})$ The answer is supposed to be $zf$ , but I cannot figure out how to reach that conclusion. Could someone perhaps point me in the right direction; what am I doing wrong?","Question: Let be a differentiable function of two variables and let . Simplify the expression I am having trouble understanding how to go about this problem. My solution attempt: This should in turn give And therefore The answer is supposed to be , but I cannot figure out how to reach that conclusion. Could someone perhaps point me in the right direction; what am I doing wrong?"," f(s,t)  h(x,y,z)=z\cdot f(\frac{x}{z}, \frac{y}{z}) (x,y,z) \cdot \nabla h \frac{\partial f}{\partial x} = \frac{\partial f}{\partial s}\cdot \frac{\partial s}{\partial x} + \frac{\partial f}{\partial t}\cdot \frac{\partial t}{\partial x} = \frac{\partial f}{\partial s}\cdot\frac{1}{z} \frac{\partial f}{\partial y} = \frac{\partial f}{\partial s}\cdot \frac{\partial s}{\partial y} + \frac{\partial f}{\partial t}\cdot \frac{\partial t}{\partial y} = \frac{\partial f}{\partial s}\cdot\frac{1}{z} \frac{\partial f}{\partial z} = \frac{\partial f}{\partial s}\cdot \frac{\partial s}{\partial z} + \frac{\partial f}{\partial t}\cdot \frac{\partial t}{\partial z} = \frac{\partial f}{\partial s}\cdot\frac{-x}{z^2} + \frac{\partial f}{\partial t}\cdot\frac{-y}{z^2} \nabla h = (\frac{\partial f}{\partial s}, \frac{\partial f}{\partial t}, f(\frac{x}{z},\frac{y}{z}) + \frac{\partial f}{\partial s} \cdot \frac{-x}{z^2} + \frac{\partial f}{\partial t} \cdot \frac{-y}{z^2}) (x, y, z) \cdot \nabla h = x \cdot \frac{\partial f}{\partial s} + y \cdot \frac{\partial f}{\partial t} + z\cdot (f( \frac{x}{z} , \frac{y}{z} ) + \frac{\partial f}{\partial s} \cdot \frac{-x}{z^2} + \frac{\partial f}{\partial t} \cdot \frac{-y}{z^2}) = \frac{\partial f}{\partial s \cdot z} + \frac{\partial f}{\partial t \cdot z} + z \cdot f( \frac{x}{z}, \frac{y}{z}) zf","['multivariable-calculus', 'chain-rule']"
58,How visualize a Volume,How visualize a Volume,,"I need to calculate some volumes, but I can't because I'm not able to visualize them.I 'm not able to set the integrals. I'm going to show my problem: I have that volume $V:=\{\ \underline{x} \in \mathbb{R^3} : x,y,z \ge 0 ,\ z \le 2-x^2-y,\ z \le x+2y \} \\ \underline{x}:=(x,y,z)$ How can I see it? Should I only use algebra and abandon any visual approach? If yes how? Thanks in advance P.S. Obviously the tools like Geogebra 3D I don't take them into consideration, I would like to succeed in this without using these tools. I would also like to add that generally if I can see the volume I have no problems with the integral calculation. The problem is all there, sometimes I can see the volume at other times absolutely not!","I need to calculate some volumes, but I can't because I'm not able to visualize them.I 'm not able to set the integrals. I'm going to show my problem: I have that volume How can I see it? Should I only use algebra and abandon any visual approach? If yes how? Thanks in advance P.S. Obviously the tools like Geogebra 3D I don't take them into consideration, I would like to succeed in this without using these tools. I would also like to add that generally if I can see the volume I have no problems with the integral calculation. The problem is all there, sometimes I can see the volume at other times absolutely not!","V:=\{\ \underline{x} \in \mathbb{R^3} : x,y,z \ge 0 ,\ z \le 2-x^2-y,\ z \le x+2y \} \\ \underline{x}:=(x,y,z)","['integration', 'multivariable-calculus', 'volume', 'visualization']"
59,How to change the limits of given integral,How to change the limits of given integral,,"I need to evaluate the following integral : $$\int_{ -1}^1\int_{1+x}^1\cos\left(x+y\right)e^{(y-x)}dydx$$ I know that I need to change the variables by using substitution $u = x+y$ and $v =y-x$ but I am confused about changing the limits of the new integral , I am trying to get the limits by drawing the graph of given limits in (x,y) and then , draw the corresponding graph in u-v plane using given equation, But I am still not getting it . Can please someone explain on how to change limits of this integral, and how should I proceed while tackling such problems of the same kind ? Thank you","I need to evaluate the following integral : I know that I need to change the variables by using substitution and but I am confused about changing the limits of the new integral , I am trying to get the limits by drawing the graph of given limits in (x,y) and then , draw the corresponding graph in u-v plane using given equation, But I am still not getting it . Can please someone explain on how to change limits of this integral, and how should I proceed while tackling such problems of the same kind ? Thank you",\int_{ -1}^1\int_{1+x}^1\cos\left(x+y\right)e^{(y-x)}dydx u = x+y v =y-x,['integration']
60,Deriving governing equation for heat transfer: why are $\Delta y$ and $\Delta z$ included?,Deriving governing equation for heat transfer: why are  and  included?,\Delta y \Delta z,"My textbook preliminarily describes the governing equation for heat transfer as follows: $$\text{Energy In - Energy Out + Energy Generated = Energy Stored}$$ It then goes on to derive the heat transfer equation by only considering heat flow in the $x$ -direction: $$\text{Energy in during time $\Delta t$} = (q''_x \Delta y \Delta z + [u \Delta y \Delta z \rho c_p(T - T_R)]_x) \Delta t$$ $$\text{Energy out during time $\Delta t$} = (q''_{x + \Delta x} \Delta y \Delta z + [u \Delta y \Delta z \rho c_p(T - T_R)]_{x + \Delta x}) \Delta t$$ $$\text{Energy generated during time $\Delta t$} = Q \Delta x \Delta y \Delta z \Delta t$$ $$\text{Energy stored during time $\Delta t$} = \Delta x \Delta y \Delta z \rho c_p \Delta T$$ If we are only considering heat flow in the $x$ -direction, then why are all $\Delta y$ and $\Delta z$ included in these equations? What purpose do they serve? I would greatly appreciate it if people could please take the time to clarify this.","My textbook preliminarily describes the governing equation for heat transfer as follows: It then goes on to derive the heat transfer equation by only considering heat flow in the -direction: If we are only considering heat flow in the -direction, then why are all and included in these equations? What purpose do they serve? I would greatly appreciate it if people could please take the time to clarify this.",\text{Energy In - Energy Out + Energy Generated = Energy Stored} x \text{Energy in during time \Delta t} = (q''_x \Delta y \Delta z + [u \Delta y \Delta z \rho c_p(T - T_R)]_x) \Delta t \text{Energy out during time \Delta t} = (q''_{x + \Delta x} \Delta y \Delta z + [u \Delta y \Delta z \rho c_p(T - T_R)]_{x + \Delta x}) \Delta t \text{Energy generated during time \Delta t} = Q \Delta x \Delta y \Delta z \Delta t \text{Energy stored during time \Delta t} = \Delta x \Delta y \Delta z \rho c_p \Delta T x \Delta y \Delta z,"['multivariable-calculus', 'partial-differential-equations', 'mathematical-modeling', 'heat-equation']"
61,Evaluate $I = \iint_S x~dS$ over the part of $2z = x^2$ that lies in the first octant part of $x^2 + y^2 = 1$.,Evaluate  over the part of  that lies in the first octant part of .,I = \iint_S x~dS 2z = x^2 x^2 + y^2 = 1,"Evaluate $I = \iint_S x~dS$ over the part of $2z = x^2$ that lies in the first octant part of $x^2 + y^2 = 1$ . In my attempt at solving this problem I begin by expressing $S$ a system of equations and inequalities and the paramterizing it. $$ S: \begin{cases}   2z = x^2 \Leftrightarrow z = x^2/2 \\   x^2 + y^2 \leq 1 \\   x,y,z \geq 0 \end{cases} \quad \Leftrightarrow \quad \mathbf{r}(r,\theta) =  \begin{bmatrix}   r\cos\theta \\   r\sin\theta \\   r^2 \cos^2(\theta)/2 \end{bmatrix}, ~r \in [0,1], ~ t\in[0, \pi/2] $$ I arrive at this paramterization by first paramterizing the closed disk $x^2 + y^2 \leq 1$ , and then substituting $x = r\cos\theta$ into $z=x^2/2$ to paramterize $z$ . After coming up with this parameterization I proceed to expressing the area element $dS$ in terms of $r$ and $\theta$ . $$dS = \Big\lvert \frac{d\mathbf{r}}{dr} \times \frac{d\mathbf{r}}{d\theta} \Big\rvert dr d\theta = \dots = \sqrt{\frac{r^2\cos^2(\theta)(r^2 \cos 2\theta - r^2 - 4)}{-2}} dr d\theta$$ My book does not paramterize $S$ and instead immediately finds a significantly easier expression for $dS$ , namely $$dS = \sqrt{1+x^2} dx dy.$$ How should one think to come up with this simpler expression for $dS$ and was my approach to solving the problem, not only ineffective, but wrong?","Evaluate over the part of that lies in the first octant part of . In my attempt at solving this problem I begin by expressing a system of equations and inequalities and the paramterizing it. I arrive at this paramterization by first paramterizing the closed disk , and then substituting into to paramterize . After coming up with this parameterization I proceed to expressing the area element in terms of and . My book does not paramterize and instead immediately finds a significantly easier expression for , namely How should one think to come up with this simpler expression for and was my approach to solving the problem, not only ineffective, but wrong?","I = \iint_S x~dS 2z = x^2 x^2 + y^2 = 1 S 
S:
\begin{cases}
  2z = x^2 \Leftrightarrow z = x^2/2 \\
  x^2 + y^2 \leq 1 \\
  x,y,z \geq 0
\end{cases}
\quad
\Leftrightarrow
\quad
\mathbf{r}(r,\theta) = 
\begin{bmatrix}
  r\cos\theta \\
  r\sin\theta \\
  r^2 \cos^2(\theta)/2
\end{bmatrix},
~r \in [0,1], ~ t\in[0, \pi/2]
 x^2 + y^2 \leq 1 x = r\cos\theta z=x^2/2 z dS r \theta dS = \Big\lvert \frac{d\mathbf{r}}{dr} \times \frac{d\mathbf{r}}{d\theta} \Big\rvert dr d\theta = \dots = \sqrt{\frac{r^2\cos^2(\theta)(r^2 \cos 2\theta - r^2 - 4)}{-2}} dr d\theta S dS dS = \sqrt{1+x^2} dx dy. dS","['multivariable-calculus', 'surface-integrals']"
62,"Compute $ \iint_{\mathbb{R}^2} \ln(\sqrt{x^2+y^2})\Delta(e^{-3x^2-4y^2})\,dx\,dy$",Compute," \iint_{\mathbb{R}^2} \ln(\sqrt{x^2+y^2})\Delta(e^{-3x^2-4y^2})\,dx\,dy","Compute $$ \iint_{\mathbb{R}^2} \ln(\sqrt{x^2+y^2})\Delta(e^{-3x^2-4y^2})\,dxdy$$ knowing that $\Delta \ln(\sqrt{x^2+y^2})=0$ on $\mathbb{R}^2\setminus \{(0,0)\}$ . I defined $g(x,y)=\ln(\sqrt{x^2+y^2})$ and $f(x,y)=e^{-3x^2-4y^2}$ and after some calculations I have found: $$g \Delta f= \nabla \cdot (g \nabla f) - \nabla \cdot (f \nabla g)+f \Delta g$$ I don't know how to proceed. I tried using divergence theorem but I failed to apply it.",Compute knowing that on . I defined and and after some calculations I have found: I don't know how to proceed. I tried using divergence theorem but I failed to apply it.," \iint_{\mathbb{R}^2} \ln(\sqrt{x^2+y^2})\Delta(e^{-3x^2-4y^2})\,dxdy \Delta \ln(\sqrt{x^2+y^2})=0 \mathbb{R}^2\setminus \{(0,0)\} g(x,y)=\ln(\sqrt{x^2+y^2}) f(x,y)=e^{-3x^2-4y^2} g \Delta f= \nabla \cdot (g \nabla f) - \nabla \cdot (f \nabla g)+f \Delta g","['integration', 'multivariable-calculus']"
63,Integral of some anti/symmetric functions of two variables,Integral of some anti/symmetric functions of two variables,,"Consider $f$ and $g$ , two scalar functions of one variable defined over a the interval [- $a$ , $a$ ], with $a$ real number. $f$ is symmetric, $f(x)=f(-x)$ and $g$ antisymmetric $g(-x)=-g(x)$ . We consider the following integral: $$\int_{-a}^a \mathrm{d}x\int_{-a}^a \mathrm{d}y\; f(x)^2f(y)V(|x-y|)g(y),$$ where $V$ is just another scalar function and $|x|$ is the absolute value of $x$ . I am under impression that this kind of integrals are always equal to zero. I am working with a numerical program that has to do this kind of integrals many times with different "" $g,f$ "", and I always get a negligible value (possibly zero). My questions are: Is this always zero? If so why? is this the result of some theorem? If it is not zero for that interval, would it be zero for the domain $(-\infty,\infty)$ ? If any of this is false, would any other condition make it true? I have manipulated a few simple sinusoidal functions to see if it is true (and it seems to be) but I am under impression that there is something more general. There has to be a manipulation that would make everything more evident. Edit: $f$ , $g$ and $V$ are bounded","Consider and , two scalar functions of one variable defined over a the interval [- , ], with real number. is symmetric, and antisymmetric . We consider the following integral: where is just another scalar function and is the absolute value of . I am under impression that this kind of integrals are always equal to zero. I am working with a numerical program that has to do this kind of integrals many times with different "" "", and I always get a negligible value (possibly zero). My questions are: Is this always zero? If so why? is this the result of some theorem? If it is not zero for that interval, would it be zero for the domain ? If any of this is false, would any other condition make it true? I have manipulated a few simple sinusoidal functions to see if it is true (and it seems to be) but I am under impression that there is something more general. There has to be a manipulation that would make everything more evident. Edit: , and are bounded","f g a a a f f(x)=f(-x) g g(-x)=-g(x) \int_{-a}^a \mathrm{d}x\int_{-a}^a \mathrm{d}y\; f(x)^2f(y)V(|x-y|)g(y), V |x| x g,f (-\infty,\infty) f g V","['integration', 'multivariable-calculus', 'symmetry']"
64,Very simple question about Stokes theorem / divergence theorem,Very simple question about Stokes theorem / divergence theorem,,"I know these two theorems: $\iiint_V(\nabla\cdot\mathbf F)dV= \iint_S(\mathbf F\cdot\mathbf n)dS $ , with $V\subset\mathbb R^3$ compact, $S=\partial V,\ \mathbf F\in\mathfrak X(\Bbb R^3)$ and $\mathbf n$ is the normal unitary vector field for $S$ (divergence theorem). $\int_{\partial\Omega}\omega=\int_{\Omega}d\omega$ , with $\omega$ a differential form on the compact manifold $\Omega$ (Stokes theorem). However I'm given this simple exercise: Find the flux of $\xi=3x\mathbf i+2y\mathbf j\in\mathfrak X(\Bbb R^3)$ through the surface of $S=\{(x \ y\ z)^t:x^2+y^2+z^2=9\}$ , oriented with a normal unitary vector field pointing outward $S$ . Now, if I apply the divergence theorem, since div $(\xi)=5$ , I have $\iiint_V5dV=5\cdot\frac 4 3\pi\cdot 9=$ $=\frac {20} 3\pi$ . However if I try to find the flux with Stokes, I obtain a different result: since $\mathbf n=\frac1 3(x\mathbf i+y\mathbf j+z\mathbf k)$ , $\mathbf F\cdot\mathbf n=x^2+\frac 2 3 y^2$ , and so $\iint_S(\mathbf F\cdot\mathbf n)dS =\int_S(x^2+\frac 2 3 y^2)(\frac 1 3xdy\wedge dz-\frac 1 3ydx\wedge dz+\frac 1 3zdx\wedge dy)$ . Clearly if I derive this differential $2-$ form I don't obtain the form $5dx\wedge dy \wedge dz$ , so actually this two methods give me two different results. Where am I wrong? Thank you in advance","I know these two theorems: , with compact, and is the normal unitary vector field for (divergence theorem). , with a differential form on the compact manifold (Stokes theorem). However I'm given this simple exercise: Find the flux of through the surface of , oriented with a normal unitary vector field pointing outward . Now, if I apply the divergence theorem, since div , I have . However if I try to find the flux with Stokes, I obtain a different result: since , , and so . Clearly if I derive this differential form I don't obtain the form , so actually this two methods give me two different results. Where am I wrong? Thank you in advance","\iiint_V(\nabla\cdot\mathbf F)dV= \iint_S(\mathbf F\cdot\mathbf n)dS  V\subset\mathbb R^3 S=\partial V,\ \mathbf F\in\mathfrak X(\Bbb R^3) \mathbf n S \int_{\partial\Omega}\omega=\int_{\Omega}d\omega \omega \Omega \xi=3x\mathbf i+2y\mathbf j\in\mathfrak X(\Bbb R^3) S=\{(x \ y\ z)^t:x^2+y^2+z^2=9\} S (\xi)=5 \iiint_V5dV=5\cdot\frac 4 3\pi\cdot 9= =\frac {20} 3\pi \mathbf n=\frac1 3(x\mathbf i+y\mathbf j+z\mathbf k) \mathbf F\cdot\mathbf n=x^2+\frac 2 3 y^2 \iint_S(\mathbf F\cdot\mathbf n)dS =\int_S(x^2+\frac 2 3 y^2)(\frac 1 3xdy\wedge dz-\frac 1 3ydx\wedge dz+\frac 1 3zdx\wedge dy) 2- 5dx\wedge dy \wedge dz","['multivariable-calculus', 'differential-geometry']"
65,$\int_\gamma F_1dx + F_2dy = \int_\gamma F_2dx - F_1dy = 0$ then there is a harmonic function $u$ such that $F = \nabla u$,then there is a harmonic function  such that,\int_\gamma F_1dx + F_2dy = \int_\gamma F_2dx - F_1dy = 0 u F = \nabla u,"Let $G \subset \mathbb{R}^2$ and let $F = (F_1,F_2)$ be a smooth (which means it is $C^1)$ vector field in $G$ . $F$ satisfies $$\int_\gamma F_1dx + F_2dy = \int_\gamma F_2dx - F_1dy = 0$$ for every closed and smooth path $\gamma$ . I need to prove that there exists a harmonic function $u: G \to \mathbb{R}$ such that $F = \nabla u$ . I wasn't sure what to do, so I just started trying different things. I think there is a theorem that if for every closed path $\gamma$ it satisfies $\int_\gamma \omega = 0$ , then $\omega$ is an exact differential form. If I write $\omega_1 = F_1dx + F_2dy$ and $\omega_2 = F_2dx - F_1dy$ then both are exact and there exist $u_1,u_2: G \to \mathbb{R}$ such that $\omega_1 = du_1$ and $\omega_2 = du_2$ From here I am not so sure what to do. I think I can also use Green's theorem here, but I am not sure how it helps. Help would be appreciated.","Let and let be a smooth (which means it is vector field in . satisfies for every closed and smooth path . I need to prove that there exists a harmonic function such that . I wasn't sure what to do, so I just started trying different things. I think there is a theorem that if for every closed path it satisfies , then is an exact differential form. If I write and then both are exact and there exist such that and From here I am not so sure what to do. I think I can also use Green's theorem here, but I am not sure how it helps. Help would be appreciated.","G \subset \mathbb{R}^2 F = (F_1,F_2) C^1) G F \int_\gamma F_1dx + F_2dy = \int_\gamma F_2dx - F_1dy = 0 \gamma u: G \to \mathbb{R} F = \nabla u \gamma \int_\gamma \omega = 0 \omega \omega_1 = F_1dx + F_2dy \omega_2 = F_2dx - F_1dy u_1,u_2: G \to \mathbb{R} \omega_1 = du_1 \omega_2 = du_2","['multivariable-calculus', 'differential-forms', 'vector-fields']"
66,How to prove the formula for the joint PDF of two transformed jointly continuous random variables?,How to prove the formula for the joint PDF of two transformed jointly continuous random variables?,,"In my notes, there is a theorem stated (see below), without proof. I have no idea how to prove this, because I've only just started learning multivariable calculus, and therefore lack a strong enough understanding of multivariable calculus at this point in time. Furthermore, I cannot find a proof anywhere online. So I was wondering if anyone knows how to prove this result, and explain each step? Or failing that, what are the prerequisites for proving this result?","In my notes, there is a theorem stated (see below), without proof. I have no idea how to prove this, because I've only just started learning multivariable calculus, and therefore lack a strong enough understanding of multivariable calculus at this point in time. Furthermore, I cannot find a proof anywhere online. So I was wondering if anyone knows how to prove this result, and explain each step? Or failing that, what are the prerequisites for proving this result?",,"['probability', 'multivariable-calculus', 'probability-distributions', 'transformation', 'jacobian']"
67,Show that the set $S$ is not a submanifold,Show that the set  is not a submanifold,S,"The definition of submanifold in my class is as follows. Let $d$ and $n$ natural numbers, such that $0 \leq d \leq n$ , let $l \in \mathbb{N} \cup \{\infty \} $ , and let $X$ a normed vector space of dimension $n$ . A subset $M \subset X$ is a $d$ -dimensional $C^l$ -submanifold of $X$ if it satisfies: For all $p \in M$ exist a open subset $U \subset X$ whit $p \in U$ , a open $V \subset \mathbb{R}^n$ , and a $C^l$ -diffeomorphism $\phi:U \rightarrow V$ with $$\phi(U \cap M)=V \cap (\mathbb{R}^d \times\{0\}).$$ I have to prove that the set $S$ is not a submanifold of $\mathbb{R}^2$ , where $S:=\{(x,y) \in \mathbb{R}^2~|~y\neq 0 \Rightarrow 1/y \in \mathbb{N} \}$ . My problem is that I believe I have demonstrated that $S$ is a submanifold 1-dimensional of $\mathbb{R}^2$ . Then $S$ can be a submanifold of $\mathbb{R}^2$ ?","The definition of submanifold in my class is as follows. Let and natural numbers, such that , let , and let a normed vector space of dimension . A subset is a -dimensional -submanifold of if it satisfies: For all exist a open subset whit , a open , and a -diffeomorphism with I have to prove that the set is not a submanifold of , where . My problem is that I believe I have demonstrated that is a submanifold 1-dimensional of . Then can be a submanifold of ?","d n 0 \leq d \leq n l \in \mathbb{N} \cup \{\infty \}  X n M \subset X d C^l X p \in M U \subset X p \in U V \subset \mathbb{R}^n C^l \phi:U \rightarrow V \phi(U \cap M)=V \cap (\mathbb{R}^d \times\{0\}). S \mathbb{R}^2 S:=\{(x,y) \in \mathbb{R}^2~|~y\neq 0 \Rightarrow 1/y \in \mathbb{N} \} S \mathbb{R}^2 S \mathbb{R}^2","['analysis', 'multivariable-calculus', 'manifolds', 'submanifold']"
68,"At $x_0 \in \partial \Omega$, $\text{Hess}u(x_0) = \Delta u(x_0) (\nu \otimes \nu)$, $\nu$ outer unit normal","At , ,  outer unit normal",x_0 \in \partial \Omega \text{Hess}u(x_0) = \Delta u(x_0) (\nu \otimes \nu) \nu,"Let $x_0 \in \partial \Omega$ and $\nu(x_{0})$ is the exterior unit normal at $x_0$ , with $\nu_1(x_0) > 0$ , where $u \in C^{2}(\overline{\Omega_{\epsilon}})$ and $\Omega_{\epsilon}=\Omega \cap \{|x-x_0|<\epsilon\}$ , $u>0$ in $\Omega$ and $u=0$ on $\partial \Omega \cap \{|x-x_0|<\epsilon\}$ . We know that $u_{x_1}(x_0) = 0$ . Why does the following hold? $$ \Delta u = - f(0) > 0 \implies u_{x_{i},x_{j}}=-f(0)\nu_{i}\nu_{j} \quad \text{ at } x_0 $$ Moreover, in their paper, Gidas, Ni and Nirenberg claim, right before what inspires the question, that $\nabla u(x_0) = 0$ . It's a mistake, isn't it? Up to my understanding, we only know that $u_1(x_0) = 0$ . This expression appears specifically in the last part of the demonstration of Lemma 2.1 of this article https://projecteuclid.org/download/pdf_1/euclid.cmp/1103905359 of Gidas-Ni-Nirenberg's and I think that it is related to the second directional derivative and Hessian matrix.","Let and is the exterior unit normal at , with , where and , in and on . We know that . Why does the following hold? Moreover, in their paper, Gidas, Ni and Nirenberg claim, right before what inspires the question, that . It's a mistake, isn't it? Up to my understanding, we only know that . This expression appears specifically in the last part of the demonstration of Lemma 2.1 of this article https://projecteuclid.org/download/pdf_1/euclid.cmp/1103905359 of Gidas-Ni-Nirenberg's and I think that it is related to the second directional derivative and Hessian matrix.","x_0 \in \partial \Omega \nu(x_{0}) x_0 \nu_1(x_0) > 0 u \in C^{2}(\overline{\Omega_{\epsilon}}) \Omega_{\epsilon}=\Omega \cap \{|x-x_0|<\epsilon\} u>0 \Omega u=0 \partial \Omega \cap \{|x-x_0|<\epsilon\} u_{x_1}(x_0) = 0 
\Delta u = - f(0) > 0 \implies u_{x_{i},x_{j}}=-f(0)\nu_{i}\nu_{j} \quad \text{ at } x_0
 \nabla u(x_0) = 0 u_1(x_0) = 0","['multivariable-calculus', 'partial-differential-equations', 'maximum-principle', 'hessian-matrix', 'elliptic-equations']"
69,"Calculate $\iiint_\omega (x+y+z)^2\,dxdydz$.",Calculate .,"\iiint_\omega (x+y+z)^2\,dxdydz","Problem : Calculate $$\displaystyle\iiint_\Omega (x+y+z)^2\,dxdydz,$$ where $\Omega$ is the domain bounded by the sphere $x^2+y^2+z^2\le 3a^2$ and the paraboloid $x^2+y^2\le 2az$ , where $z\ge 0$ . Progress : So, by sketching the domain, one can see that the domain includes a part from sphere and a part from the paraboloid, so we need to calculate the integral on two seperate part, then add the to get the result. To calculate the integral in the sphere part, I've tried using spherical coordinate. Using spherical coordinate, I made the following substitution: $$\begin{cases} x = r\sin\theta\cos\varphi\\ y = r\sin\theta\sin\varphi\\ z = r\cos\theta \end{cases}$$ with $0\le \varphi\le 2\pi$ , $0\le\theta\le\alpha$ , where $\alpha =\arccos\left(\dfrac{1}{\sqrt{3}}\right)$ (one can get the bound for angle $\theta$ by working out the intersection of the paraboloid and the sphere) and $\dfrac{a}{\cos\theta}\le r\le\dfrac{a\sqrt 2}{\sin\theta}$ . Now, the expression $(x+y+z)^2$ become $$r^2(1+\sin^2\theta\sin 2\varphi + \sin 2\theta+\sin\varphi+\sin 2\theta\cos\varphi).$$ Now, we multiply the Jacobian determinant and the original integral become $$\int^{2\pi}_0d\varphi\int^\alpha_0d\theta\int^\tfrac{a\sqrt 2}{\sin\theta}_\tfrac{a}{\cos\theta}r^4\sin^2\theta(1+\sin^2\theta\sin 2\varphi + \sin 2\theta+\sin\varphi+\sin 2\theta\cos\varphi)dr.$$ As you can see, this one is tedious and almost impossible to calculate it. Do you have any idea to solve this problem?","Problem : Calculate where is the domain bounded by the sphere and the paraboloid , where . Progress : So, by sketching the domain, one can see that the domain includes a part from sphere and a part from the paraboloid, so we need to calculate the integral on two seperate part, then add the to get the result. To calculate the integral in the sphere part, I've tried using spherical coordinate. Using spherical coordinate, I made the following substitution: with , , where (one can get the bound for angle by working out the intersection of the paraboloid and the sphere) and . Now, the expression become Now, we multiply the Jacobian determinant and the original integral become As you can see, this one is tedious and almost impossible to calculate it. Do you have any idea to solve this problem?","\displaystyle\iiint_\Omega (x+y+z)^2\,dxdydz, \Omega x^2+y^2+z^2\le 3a^2 x^2+y^2\le 2az z\ge 0 \begin{cases} x = r\sin\theta\cos\varphi\\ y = r\sin\theta\sin\varphi\\ z = r\cos\theta \end{cases} 0\le \varphi\le 2\pi 0\le\theta\le\alpha \alpha =\arccos\left(\dfrac{1}{\sqrt{3}}\right) \theta \dfrac{a}{\cos\theta}\le r\le\dfrac{a\sqrt 2}{\sin\theta} (x+y+z)^2 r^2(1+\sin^2\theta\sin 2\varphi + \sin 2\theta+\sin\varphi+\sin 2\theta\cos\varphi). \int^{2\pi}_0d\varphi\int^\alpha_0d\theta\int^\tfrac{a\sqrt 2}{\sin\theta}_\tfrac{a}{\cos\theta}r^4\sin^2\theta(1+\sin^2\theta\sin 2\varphi + \sin 2\theta+\sin\varphi+\sin 2\theta\cos\varphi)dr.","['calculus', 'integration', 'multivariable-calculus', 'multiple-integral']"
70,How to plot a phase space,How to plot a phase space,,"I was trying to study the function $x(t)=\int_0^t e^{-s^2}ds$ , which I did successfully using high school maths. After that, I decided that I wanted to try to study it using some multivariable calculus, but couldn't really do it, so I'm asking for the proper way to face this: the problem is equivalent to the ODE $x'(t)=e^{-t^2}$ given some initial value. Let $y(t)=t$ and therefore $y'(t)=1$ . Now I want to study the phase space of the system $\begin{cases} x'=e^{-y^2}\\ y'=1 \end{cases}$ . How can I do it? There are no equilibrium points and the Jacobian's only eigenvalue is $0$ . Also, the linearized system is $\begin{cases}x'=(-2y_0e^{-y_0^2})\cdot y \\ y'=0\end{cases}\;$ which makes no sense since in $(0, 0)$ it gives $x'=0, y'=0$ whereas in the original one it is $x'=1, y'= 1$ . What's the proper way to study this system?","I was trying to study the function , which I did successfully using high school maths. After that, I decided that I wanted to try to study it using some multivariable calculus, but couldn't really do it, so I'm asking for the proper way to face this: the problem is equivalent to the ODE given some initial value. Let and therefore . Now I want to study the phase space of the system . How can I do it? There are no equilibrium points and the Jacobian's only eigenvalue is . Also, the linearized system is which makes no sense since in it gives whereas in the original one it is . What's the proper way to study this system?","x(t)=\int_0^t e^{-s^2}ds x'(t)=e^{-t^2} y(t)=t y'(t)=1 \begin{cases} x'=e^{-y^2}\\ y'=1 \end{cases} 0 \begin{cases}x'=(-2y_0e^{-y_0^2})\cdot y \\ y'=0\end{cases}\; (0, 0) x'=0, y'=0 x'=1, y'= 1","['ordinary-differential-equations', 'multivariable-calculus', 'dynamical-systems']"
71,Sandwich rule and limits,Sandwich rule and limits,,"Let $f\colon A\subseteq \mathbb{R^n}$ $\rightarrow$ $\mathbb{R}^m$ and $h:A \subseteq \mathbb{R^n} \rightarrow \mathbb{R^{\geq0}}$ Assume $ > 0 \leq |f(x)| \leq h(x)$ for $x$ in a punctured ball of $a$ . Show that   if $h(x) \rightarrow 0$ as $x \rightarrow a$ then $f(x)\rightarrow   0$ as $x \rightarrow a$ . My Proof : Assume $h(x) \rightarrow 0$ as $x \rightarrow a$ . Let $\epsilon >0$ . So $\exists$ $\delta>0$ such that whenever we have $0 < |x-a| < \delta$ we have then $|h(x)| < \epsilon$ . Because $|f(x)|,h(x)\geq 0$ it follows that if I set $\delta'=\delta$ then whenever $0<|x-a|<\delta'$ we have $||f(x)||=|f(x)| <\epsilon$ . Is this proof correct? I would very much like feedback.",Let and Assume for in a punctured ball of . Show that   if as then as . My Proof : Assume as . Let . So such that whenever we have we have then . Because it follows that if I set then whenever we have . Is this proof correct? I would very much like feedback.,"f\colon A\subseteq \mathbb{R^n} \rightarrow \mathbb{R}^m h:A \subseteq \mathbb{R^n} \rightarrow \mathbb{R^{\geq0}} 
> 0 \leq |f(x)| \leq h(x) x a h(x) \rightarrow 0 x \rightarrow a f(x)\rightarrow
  0 x \rightarrow a h(x) \rightarrow 0 x \rightarrow a \epsilon >0 \exists \delta>0 0 < |x-a| < \delta |h(x)| < \epsilon |f(x)|,h(x)\geq 0 \delta'=\delta 0<|x-a|<\delta' ||f(x)||=|f(x)| <\epsilon","['real-analysis', 'limits']"
72,Why do we choose the limits for double integrals in this way?,Why do we choose the limits for double integrals in this way?,,"This is a general question which I have been asking (to no avail) for a while around my class. I will ask my question using an example. Evaluate $I=\displaystyle\int_{0}^{1}\mathrm{d}x\displaystyle\int_{\sqrt{x}}^{1}e^{y^3}\mathrm{d}y$ . Where the solution provided by the lecturer is We cannot directly integrate $e^{y^3}$ with respects to $y$ , therefore we express $I$ as a double integral over a region $D$ , that is $$I=\iint_{D}e^{y^3}\mathrm{d}x\,\mathrm{d}y.$$ We can describe $D=\{(x,y):0\leqslant y\leqslant 1, 0\leqslant x\leqslant y^2\}$ : The lecturer goes on to solve the integral using these limits and everything is fine. My question is: why do we choose $0\leqslant x\leqslant y^2$ ? What is the difference between this and $y^2\leqslant x\leqslant 1$ ? The same can be asked for the limits of the first integral: what is the difference between $\sqrt{x}\leqslant y\leqslant 1$ and $0\leqslant y\leqslant \sqrt{x}$ ?","This is a general question which I have been asking (to no avail) for a while around my class. I will ask my question using an example. Evaluate . Where the solution provided by the lecturer is We cannot directly integrate with respects to , therefore we express as a double integral over a region , that is We can describe : The lecturer goes on to solve the integral using these limits and everything is fine. My question is: why do we choose ? What is the difference between this and ? The same can be asked for the limits of the first integral: what is the difference between and ?","I=\displaystyle\int_{0}^{1}\mathrm{d}x\displaystyle\int_{\sqrt{x}}^{1}e^{y^3}\mathrm{d}y e^{y^3} y I D I=\iint_{D}e^{y^3}\mathrm{d}x\,\mathrm{d}y. D=\{(x,y):0\leqslant y\leqslant 1, 0\leqslant x\leqslant y^2\} 0\leqslant x\leqslant y^2 y^2\leqslant x\leqslant 1 \sqrt{x}\leqslant y\leqslant 1 0\leqslant y\leqslant \sqrt{x}","['integration', 'multivariable-calculus']"
73,Evaluate $\iint_E xy\ dx\ dy$ over the region $E$ common to the circles $x^2+y^2=x$ and $x^2+y^2=y$ using change of variable,Evaluate  over the region  common to the circles  and  using change of variable,\iint_E xy\ dx\ dy E x^2+y^2=x x^2+y^2=y,"I have solved the problem just by taking limits in $y$ first, then in $x$ and got the answer ${1\over 96}$ i.e. $$\iint_E xy\ dy\ dx=\int_{x=0}^{1/2}\int_{y=\left(1-\sqrt{1-4x^2}\right)/{2}}^{\sqrt{x-x^2}}xy\ dy\ dx={1\over 96}.$$ But the author asks to use the change of variable- $${x^2+y^2\over x}=u$$ and $${x^2+y^2\over y}=v$$ But I cannot get the limits of $u$ and $v$ , I am also facing problem to find the Jacobian as we need to find $x$ and $y$ explicitly. Can anybody solve the problem using the change of variable? Thanks for the assistance in advance.","I have solved the problem just by taking limits in first, then in and got the answer i.e. But the author asks to use the change of variable- and But I cannot get the limits of and , I am also facing problem to find the Jacobian as we need to find and explicitly. Can anybody solve the problem using the change of variable? Thanks for the assistance in advance.",y x {1\over 96} \iint_E xy\ dy\ dx=\int_{x=0}^{1/2}\int_{y=\left(1-\sqrt{1-4x^2}\right)/{2}}^{\sqrt{x-x^2}}xy\ dy\ dx={1\over 96}. {x^2+y^2\over x}=u {x^2+y^2\over y}=v u v x y,"['calculus', 'integration', 'multivariable-calculus']"
74,Question about notation: The order notation,Question about notation: The order notation,,"For example, when we Taylor expand say $e^x$ about $x=0$ , we would write $$e^x = 1+x+\frac 12 x^2 + \frac 16 x^3 + \mathcal O(x^4) \qquad \qquad \text{as } x \rightarrow 0$$ with the use of the ""Big-O"" notation $\mathcal O$ . I was just wondering, is there a generalisation for this to ""multiple variables""? For example, if I Taylor expand $e^{x+y}$ about $(x,y) = (0,0)$ , we get $$e^{x+y} = 1 + x + y + \frac 12 x^2 + xy + \frac 12 y^2 + \text{higher order terms} \qquad \qquad \text{as } (x,y) \rightarrow (0,0)$$ where ""higher order terms"" in this case would refer to cubic or higher terms in $x,y$ , namely $x^3$ , $x^2y$ , $xy^2$ , $y^3$ or beyond. You can't just write something like $\mathcal O(x^3)$ or $\mathcal O((xy)^3)$ because there are terms like $x^2y$ that don't contain the $x^3$ . So...what's the correct way to express this?","For example, when we Taylor expand say about , we would write with the use of the ""Big-O"" notation . I was just wondering, is there a generalisation for this to ""multiple variables""? For example, if I Taylor expand about , we get where ""higher order terms"" in this case would refer to cubic or higher terms in , namely , , , or beyond. You can't just write something like or because there are terms like that don't contain the . So...what's the correct way to express this?","e^x x=0 e^x = 1+x+\frac 12 x^2 + \frac 16 x^3 + \mathcal O(x^4) \qquad \qquad \text{as } x \rightarrow 0 \mathcal O e^{x+y} (x,y) = (0,0) e^{x+y} = 1 + x + y + \frac 12 x^2 + xy + \frac 12 y^2 + \text{higher order terms} \qquad \qquad \text{as } (x,y) \rightarrow (0,0) x,y x^3 x^2y xy^2 y^3 \mathcal O(x^3) \mathcal O((xy)^3) x^2y x^3","['multivariable-calculus', 'notation', 'asymptotics', 'taylor-expansion']"
75,"definite integral of logarithmic functions. Like this one $ \int_0^{\pi/2} \ln\left(a^2\cos^2 \theta + b^2\sin^2 \theta\right)\, \mathrm{d}{\theta}$",definite integral of logarithmic functions. Like this one," \int_0^{\pi/2} \ln\left(a^2\cos^2 \theta + b^2\sin^2 \theta\right)\, \mathrm{d}{\theta}","Just playing with graph plotter I found some formulas for definite integrals of logarithmic functions such as $$             \int \limits_0^{\pi/2} \ln\left(a^2\cos^2 \theta + b^2\sin^2 \theta\right)\, \mathrm{d}{\theta} = \pi \ln \left(\frac{a+b}{2}\right),         $$ $$         \int \limits_0^{\pi/2} \ln\left(4a^4\sin^4 \theta + b^4\cos^4\theta\right)\,  \mathrm{d}{\theta}= \pi\ln\left(\frac{2a^2+b^2+2ab}{4}\right)     $$ or even $$\int \limits_{0}^{1} \ln\left(a^2t^2 + b^2 (1-t)^2 \right) \, \mathrm{d}{t} = \frac{2a^2\ln a + \pi ab + 2b^2 \ln b}{a^2+b^2} - 2, $$ I hope you got the idea. The formulas look relatively simple, like it is not impossible to prove, but I have no idea what method can be used. Not being a good mathematician, but rather the one who solves integrals by guessing in graph plotter, I asked a friend and he came up with an elegant way to prove the first one (partial derivatives along $a+b=\mathrm{const}$ are zero), but it does not seem to work well for the others as the contour lines have more complicated shapes. Do you have any helpful thought about that? At least, where might some similar integrals have appeared before, what book/paper to search for? Thank you.","Just playing with graph plotter I found some formulas for definite integrals of logarithmic functions such as or even I hope you got the idea. The formulas look relatively simple, like it is not impossible to prove, but I have no idea what method can be used. Not being a good mathematician, but rather the one who solves integrals by guessing in graph plotter, I asked a friend and he came up with an elegant way to prove the first one (partial derivatives along are zero), but it does not seem to work well for the others as the contour lines have more complicated shapes. Do you have any helpful thought about that? At least, where might some similar integrals have appeared before, what book/paper to search for? Thank you.","
            \int \limits_0^{\pi/2} \ln\left(a^2\cos^2 \theta + b^2\sin^2 \theta\right)\, \mathrm{d}{\theta} = \pi \ln \left(\frac{a+b}{2}\right),
         
        \int \limits_0^{\pi/2} \ln\left(4a^4\sin^4 \theta + b^4\cos^4\theta\right)\,  \mathrm{d}{\theta}= \pi\ln\left(\frac{2a^2+b^2+2ab}{4}\right)
     \int \limits_{0}^{1} \ln\left(a^2t^2 + b^2 (1-t)^2 \right) \, \mathrm{d}{t} = \frac{2a^2\ln a + \pi ab + 2b^2 \ln b}{a^2+b^2} - 2,  a+b=\mathrm{const}",['multivariable-calculus']
76,Implicit Function Theorem with $x^2+y^2+z^2=\psi(ax+by+cz)$,Implicit Function Theorem with,x^2+y^2+z^2=\psi(ax+by+cz),"Given the equation $x^2+y^2+z^2=\psi(ax+by+cz)$ , with $a,b,c\in\mathbb{R},\ c\neq 0$ , and $\psi:\mathbb{R}\to\mathbb{R}$ that satisfies $\psi\in C^2,\ \psi(0)=0,\ \psi'(0)\neq0$ , prove that in a neighborhood of $(0,0,0)$ the solutions of the equation can be expressed as $(x,y,f(x,y))$ , with $f$ differentiable in that neighborhood, and find $\frac{\partial f}{\partial x}(0,0)$ and $\frac{\partial f}{\partial y}(0,0)$ . My idea is to define a function $F:\mathbb{R^3}\to\mathbb{R},\ F(x,y,z)=(x^2+y^2+z^2-\psi(ax+by+cz))$ . In the first part we have to prove that $F$ verifies the Implicit Function Theorem (IFT) in $p$ = $(0,0,0)$ : $F(0,0,0)=\psi(0)=0,\ F\in C^2(\mathbb R)$ (so it's $C^2$ 'near' $p$ ), and the third requirement is that $\frac{\partial F}{\partial z}=0$ , $\frac{\partial F}{\partial z}=2z-c\psi'\neq 0$ when evaluated in $p$ , because $c\neq0$ and $\psi'(0)\neq0$ , so applying the IFT we are done with the first part of the exercice. We can express $F$ as $F(x,y,f(x,y))=(x^2+y^2+(f(x,y))^2-\psi(ax+by+cf(x,y)))$ . Now we want to compute $\frac{\partial f}{\partial x}(0,0)$ and $\frac{\partial f}{\partial y}(0,0)$ : $\frac{\partial F}{\partial x}(x,y)=2x+2f(x,y)\frac{\partial f}{\partial x}(x,y)-(a+c\frac{\partial f}{\partial x}(x,y))\frac{\partial\psi}{\partial x}\to\frac{\partial F}{\partial x}(0,0)=-(a+c\frac{\partial f}{\partial x}(0,0))\frac{\partial\psi}{\partial x}$ But I don't know how to continue. Could you help me? Is the first part correctly done? Thanks!","Given the equation , with , and that satisfies , prove that in a neighborhood of the solutions of the equation can be expressed as , with differentiable in that neighborhood, and find and . My idea is to define a function . In the first part we have to prove that verifies the Implicit Function Theorem (IFT) in = : (so it's 'near' ), and the third requirement is that , when evaluated in , because and , so applying the IFT we are done with the first part of the exercice. We can express as . Now we want to compute and : But I don't know how to continue. Could you help me? Is the first part correctly done? Thanks!","x^2+y^2+z^2=\psi(ax+by+cz) a,b,c\in\mathbb{R},\ c\neq 0 \psi:\mathbb{R}\to\mathbb{R} \psi\in C^2,\ \psi(0)=0,\ \psi'(0)\neq0 (0,0,0) (x,y,f(x,y)) f \frac{\partial f}{\partial x}(0,0) \frac{\partial f}{\partial y}(0,0) F:\mathbb{R^3}\to\mathbb{R},\ F(x,y,z)=(x^2+y^2+z^2-\psi(ax+by+cz)) F p (0,0,0) F(0,0,0)=\psi(0)=0,\ F\in C^2(\mathbb R) C^2 p \frac{\partial F}{\partial z}=0 \frac{\partial F}{\partial z}=2z-c\psi'\neq 0 p c\neq0 \psi'(0)\neq0 F F(x,y,f(x,y))=(x^2+y^2+(f(x,y))^2-\psi(ax+by+cf(x,y))) \frac{\partial f}{\partial x}(0,0) \frac{\partial f}{\partial y}(0,0) \frac{\partial F}{\partial x}(x,y)=2x+2f(x,y)\frac{\partial f}{\partial x}(x,y)-(a+c\frac{\partial f}{\partial x}(x,y))\frac{\partial\psi}{\partial x}\to\frac{\partial F}{\partial x}(0,0)=-(a+c\frac{\partial f}{\partial x}(0,0))\frac{\partial\psi}{\partial x}","['multivariable-calculus', 'functions', 'implicit-differentiation', 'implicit-function-theorem']"
77,Periodic orbit in vector field with positive divergence,Periodic orbit in vector field with positive divergence,,"The Dulac-Bendixson Theorem states that a vector field with positive divergence defined on a 2-connected set cannot have more than one periodic orbit. I am looking for an exemple of a field defined on an open set minus one point, such that the field has positive divergence everywhere and the differential equation defined by such field has a periodic orbit. Many thanks.","The Dulac-Bendixson Theorem states that a vector field with positive divergence defined on a 2-connected set cannot have more than one periodic orbit. I am looking for an exemple of a field defined on an open set minus one point, such that the field has positive divergence everywhere and the differential equation defined by such field has a periodic orbit. Many thanks.",,"['ordinary-differential-equations', 'multivariable-calculus']"
78,If D$f(0)\neq 0$ then I need to show that a change of variable exists,If D then I need to show that a change of variable exists,f(0)\neq 0,"I don´t know how to start this problem. Could someone help me please? Let $f$ : $\mathbb {R}^N$ $\to$ $\mathbb {R}$ and $f$ is a function of class $C^1$ in $\mathbb {R}^N$ . Suppose D $f(0) \neq 0$ , then I need to show that a change of variable $\varphi$ : $\mathbb {R}^N$ $\to$ $\mathbb {R}^N$ exists such that: $\varphi(0)= 0$ , D $\varphi(0)$ is the identity. $f(\varphi(x)) = f(0) + Df(0)\cdot$ x,  for a small x","I don´t know how to start this problem. Could someone help me please? Let : and is a function of class in . Suppose D , then I need to show that a change of variable : exists such that: , D is the identity. x,  for a small x",f \mathbb {R}^N \to \mathbb {R} f C^1 \mathbb {R}^N f(0) \neq 0 \varphi \mathbb {R}^N \to \mathbb {R}^N \varphi(0)= 0 \varphi(0) f(\varphi(x)) = f(0) + Df(0)\cdot,"['calculus', 'multivariable-calculus']"
79,Problem 2-37(a) Spivak's Calculus on Manifolds,Problem 2-37(a) Spivak's Calculus on Manifolds,,"2-37(a) Let $f: \Bbb{R}^2 \rightarrow \Bbb{R}$ be a continuously differentiable function. Show that $f$ is not $1$ - $1$ . Hint : If for example, $D_1f(x,y) \neq 0$ for all $(x,y)$ in some open set $A$ , consider $g:A \rightarrow \Bbb{R}^2$ defined by $g(x,y)=(f(x,y),y)$ . My Attempt: For each $y \in \Bbb{R}$ , define $ h_y:\Bbb{R} \rightarrow \Bbb{R}$ s.t. $h_y(x)= f(x,y)$ . $\therefore D_1f(x,y)=h_y'(x)$ . If $D_1f(x_1,y_1)= 0$ for some $(x_1,y_1)$ , $\implies h_{y_1}'(x_1)=0 \implies h_{y_1}$ is not $1$ - $1$ i.e. there exist $x_2,x_3 \in \Bbb{R}$ s.t. $h_{y_1}(x_2)=h_{y_1}(x_3) \implies f(x_2,y_1)=f(x_3,y_1)$ . Thus $D_1f(x,y)$ must be nonzero for all $(x,y)$ in some open set $A$ if it is $1$ - $1$ . Assume $D_1f(x,y) \neq 0$ for all $(x,y)$ in some open set $A$ . consider $g:A \rightarrow \Bbb{R}^2$ defined by $g(x,y)=(f(x,y),y)$ . $g$ is continuously differentiable. Now $$\det\, g'(x,y)=\det\, \begin{pmatrix}  D_1f(x,y) & D_2f(x,y) \\  \frac{\partial y} {\partial x}   & \frac{\partial y} {\partial y} \end{pmatrix} \neq 0$$ $\therefore g^{-1}$ exists and is differentiable and $(g^{-1})'(x,y)=[g'(g^{-1}(x,y))]^{-1}$ Note: $f(g^{-1}(x,y))=x$ Now $$ [g'(g^{-1}(x,y))]^{-1}=\begin{pmatrix}   \frac{\partial f(g^{-1}(x,y)) } {\partial x}& \frac{\partial f(g^{-1}(x,y)) } {\partial y}\\ \frac{\partial y} {\partial x}& \frac{\partial y} {\partial y} \end{pmatrix}^{-1}=\begin{pmatrix}   1& 0\\ 0&1 \end{pmatrix}$$ Thus $(g^{-1})'$ is the identity matrix I don't know how to proceed. Can someone give a hint ?","2-37(a) Let be a continuously differentiable function. Show that is not - . Hint : If for example, for all in some open set , consider defined by . My Attempt: For each , define s.t. . . If for some , is not - i.e. there exist s.t. . Thus must be nonzero for all in some open set if it is - . Assume for all in some open set . consider defined by . is continuously differentiable. Now exists and is differentiable and Note: Now Thus is the identity matrix I don't know how to proceed. Can someone give a hint ?","f: \Bbb{R}^2 \rightarrow \Bbb{R} f 1 1 D_1f(x,y) \neq 0 (x,y) A g:A \rightarrow \Bbb{R}^2 g(x,y)=(f(x,y),y) y \in \Bbb{R}  h_y:\Bbb{R} \rightarrow \Bbb{R} h_y(x)= f(x,y) \therefore D_1f(x,y)=h_y'(x) D_1f(x_1,y_1)= 0 (x_1,y_1) \implies h_{y_1}'(x_1)=0 \implies h_{y_1} 1 1 x_2,x_3 \in \Bbb{R} h_{y_1}(x_2)=h_{y_1}(x_3) \implies f(x_2,y_1)=f(x_3,y_1) D_1f(x,y) (x,y) A 1 1 D_1f(x,y) \neq 0 (x,y) A g:A \rightarrow \Bbb{R}^2 g(x,y)=(f(x,y),y) g \det\, g'(x,y)=\det\,
\begin{pmatrix} 
D_1f(x,y) & D_2f(x,y) \\
 \frac{\partial y} {\partial x}   & \frac{\partial y} {\partial y}
\end{pmatrix} \neq 0 \therefore g^{-1} (g^{-1})'(x,y)=[g'(g^{-1}(x,y))]^{-1} f(g^{-1}(x,y))=x  [g'(g^{-1}(x,y))]^{-1}=\begin{pmatrix} 
 \frac{\partial f(g^{-1}(x,y)) } {\partial x}& \frac{\partial f(g^{-1}(x,y)) } {\partial y}\\
\frac{\partial y} {\partial x}& \frac{\partial y} {\partial y}
\end{pmatrix}^{-1}=\begin{pmatrix} 
 1& 0\\
0&1
\end{pmatrix} (g^{-1})'","['real-analysis', 'multivariable-calculus']"
80,Finding an Integral Surface,Finding an Integral Surface,,"Consider finding the integral surface of $$x^2 p + xy q = xyz-2y^2$$ which passes through the line $x=y e^y$ in the $z=0$ plane. Attempt In Lagrange's subsidiary form $$\frac{dx}{x^2}=\frac{dy}{xy}=\frac{dz}{(xyz-2y^2)}$$ Firstly consider $$\frac{dx}{x^2}=\frac{dy}{xy}$$ One can trivially show that $$a = \frac{x}{y}$$ where $a$ is an arbitrary constant. Now, consider $$\frac{dx}{x^2}=\frac{dz}{(xyz-2y^2)}$$ which may be written as $$\frac{dz}{dx}=\frac{(xyz-2y^2)}{x^2} \equiv \frac{z}{a}-\frac{2}{a^2}$$ having used $a=x/y$ from before. (After this point I am unsure of my working...) $$\frac{dz}{dx}=\frac{az-2}{a^2} \implies\frac{dz}{(az-2)}=\frac{dx}{a^2}$$ As $a$ is a function of $a(x,y)$ , albeit an arbitrary constant, is my solution above sensical or have a made a mistake? I understand that to find the integral surface the general solution is of the form $F(a,b)$ where has so far been determined to be $a=x/y$ . How can I find this over arbitrary constant $b$ ?","Consider finding the integral surface of which passes through the line in the plane. Attempt In Lagrange's subsidiary form Firstly consider One can trivially show that where is an arbitrary constant. Now, consider which may be written as having used from before. (After this point I am unsure of my working...) As is a function of , albeit an arbitrary constant, is my solution above sensical or have a made a mistake? I understand that to find the integral surface the general solution is of the form where has so far been determined to be . How can I find this over arbitrary constant ?","x^2 p + xy q = xyz-2y^2 x=y e^y z=0 \frac{dx}{x^2}=\frac{dy}{xy}=\frac{dz}{(xyz-2y^2)} \frac{dx}{x^2}=\frac{dy}{xy} a = \frac{x}{y} a \frac{dx}{x^2}=\frac{dz}{(xyz-2y^2)} \frac{dz}{dx}=\frac{(xyz-2y^2)}{x^2} \equiv \frac{z}{a}-\frac{2}{a^2} a=x/y \frac{dz}{dx}=\frac{az-2}{a^2} \implies\frac{dz}{(az-2)}=\frac{dx}{a^2} a a(x,y) F(a,b) a=x/y b","['ordinary-differential-equations', 'multivariable-calculus', 'partial-differential-equations', 'characteristics']"
81,Second derivative of multivariable implicit function,Second derivative of multivariable implicit function,,"Find $d^2z$ of the following function $$\frac{x}{z}=  \ln{\frac{z}{y}}+1$$ Finding the total derivative of the left and right side we arrive at the expression: $$\frac{zdx-xdz}{z^2}=\frac{y}{z} \cdot \frac{ydz-zdy}{y^2} \iff yzdx-xydz-yzdz+z^2dy=0$$ Differentiating again this expression: $$y(x+z) d^2z=zdzdy+(zdy-xdy)dz - y^2dz^2$$ Also $dz=\frac{z(ydx+zdy)}{y(x+z)}$ so by substituting in the last equation we get: $$d^2z= - \frac{z^2(ydx-xdy)^2}{y^2(x+z)^3}$$ I have couple of questions regarding this solution. I've managed to replicate taking the first derivative from both sides of the equation and arrived at the same result. What I noticed, is that during this we do not consider $z$ as a function of $x$ and $y$ . Next when taking the second derivative I didn't get the extra term $y(x+z)d^2z = yx d^2z +z d^2z$ . This term seems to appear by differentiating all the $dz$ parts in $yzdx-xydz-yzdz+z^2dy=0$ . Perhaps it is from somewhere else that we get the $d^2z$ term but I don't know what I'm missing.","Find of the following function Finding the total derivative of the left and right side we arrive at the expression: Differentiating again this expression: Also so by substituting in the last equation we get: I have couple of questions regarding this solution. I've managed to replicate taking the first derivative from both sides of the equation and arrived at the same result. What I noticed, is that during this we do not consider as a function of and . Next when taking the second derivative I didn't get the extra term . This term seems to appear by differentiating all the parts in . Perhaps it is from somewhere else that we get the term but I don't know what I'm missing.","d^2z \frac{x}{z}=
 \ln{\frac{z}{y}}+1 \frac{zdx-xdz}{z^2}=\frac{y}{z} \cdot \frac{ydz-zdy}{y^2} \iff yzdx-xydz-yzdz+z^2dy=0 y(x+z) d^2z=zdzdy+(zdy-xdy)dz - y^2dz^2 dz=\frac{z(ydx+zdy)}{y(x+z)} d^2z= - \frac{z^2(ydx-xdy)^2}{y^2(x+z)^3} z x y y(x+z)d^2z = yx d^2z +z d^2z dz yzdx-xydz-yzdz+z^2dy=0 d^2z","['multivariable-calculus', 'implicit-differentiation']"
82,A Double Partial Derivative and Integral,A Double Partial Derivative and Integral,,"Consider the following differential equation $$\frac{\partial z}{\partial \phi} . \frac{\partial z }{\partial \mu} = 1$$ My attempt at finding the solution for the complete integral of this is $$\int \int dz\,dz = \int \int d \mu \,d\phi$$ $$\int z\,dz = \int (\mu +A) \, d\phi$$ $$\frac{z^2}{2} = \mu \phi  + A \phi + B$$ $$z = \sqrt{2\phi\mu +2A\phi +2B}$$ where $A$ and $B$ are arbitrary constants. However, referring to my professor's solution, the family of solutions should be given by $$z=A\phi +\frac{1}{A}\mu + B$$ which is obviously not in agreement with my own findings. Can anyone see where I am going wrong?","Consider the following differential equation My attempt at finding the solution for the complete integral of this is where and are arbitrary constants. However, referring to my professor's solution, the family of solutions should be given by which is obviously not in agreement with my own findings. Can anyone see where I am going wrong?","\frac{\partial z}{\partial \phi} . \frac{\partial z }{\partial \mu} = 1 \int \int dz\,dz = \int \int d \mu \,d\phi \int z\,dz = \int (\mu +A) \, d\phi \frac{z^2}{2} = \mu \phi  + A \phi + B z = \sqrt{2\phi\mu +2A\phi +2B} A B z=A\phi +\frac{1}{A}\mu + B","['calculus', 'integration', 'ordinary-differential-equations', 'multivariable-calculus', 'partial-differential-equations']"
83,Proving that a function satisfies the wave equation (chain rule and iterated partial derivatives),Proving that a function satisfies the wave equation (chain rule and iterated partial derivatives),,"I'm really anxious about getting this exercise right: Given  the function $\phi = f(x-t) + g(x+t)$ , with $f,g$ one variable functions in $C^2$ , prove that $\phi$ satisfies the wave equation $\frac{\partial^2\phi}{\partial t^2} = \frac{\partial^2\phi}{\partial x^2}$ I started by defining $a, b: \mathbb{R^2} \to \mathbb{R}$ , $a(x,t)=x-t, b(x,t)=x+t$ . Then we have $$\frac{\partial^2\phi}{\partial t^2} =  \frac{d}{dt} \bigg(\frac{\partial \phi}{\partial t}\bigg) = \frac{d}{dt} \bigg(\frac{df}{da} \cdot \frac{\partial a}{\partial t} + \frac{dg}{db} \cdot \frac{\partial b}{\partial t}\bigg)$$ $$ = \frac{d}{dt} \bigg(\frac{df}{da}\bigg) \cdot \frac{\partial a}{\partial t} + \frac{df}{dt} \cdot \frac{\partial^2 a}{\partial t^2} + \frac{d}{dt} \bigg( \frac{dg}{db} \bigg) \cdot \frac{\partial b}{\partial t} + \frac{dg}{db} \cdot \frac{\partial^2 b}{\partial t^2}$$ But we know that $\frac{\partial a}{\partial t} = -1, \frac{\partial b}{\partial t}=1$ and $\frac{\partial^2 a}{\partial t^2} = \frac{\partial^2 b}{\partial t^2} = 0$ . Therefore $$= \frac{d}{dt} \bigg( \frac{dg}{db} \bigg) - \frac{d}{dt} \bigg(\frac{df}{da}\bigg) = \frac{d^2g}{db^2} + \frac{d^2f}{da^2}$$ The only difference when we differentiate w.r.t. $x$ is that $\frac{\partial a}{\partial x} = 1$ . And this doesn't make a difference because the negative sign of $\frac{\partial a}{\partial t}$ was cancelled out. Hence $\frac{\partial^2\phi}{\partial t^2} = \frac{\partial^2\phi}{\partial x^2}$ .","I'm really anxious about getting this exercise right: Given  the function , with one variable functions in , prove that satisfies the wave equation I started by defining , . Then we have But we know that and . Therefore The only difference when we differentiate w.r.t. is that . And this doesn't make a difference because the negative sign of was cancelled out. Hence .","\phi = f(x-t) + g(x+t) f,g C^2 \phi \frac{\partial^2\phi}{\partial t^2} = \frac{\partial^2\phi}{\partial x^2} a, b: \mathbb{R^2} \to \mathbb{R} a(x,t)=x-t, b(x,t)=x+t \frac{\partial^2\phi}{\partial t^2} =
 \frac{d}{dt} \bigg(\frac{\partial \phi}{\partial t}\bigg) = \frac{d}{dt} \bigg(\frac{df}{da} \cdot \frac{\partial a}{\partial t} + \frac{dg}{db} \cdot \frac{\partial b}{\partial t}\bigg)  = \frac{d}{dt} \bigg(\frac{df}{da}\bigg) \cdot \frac{\partial a}{\partial t} + \frac{df}{dt} \cdot \frac{\partial^2 a}{\partial t^2} + \frac{d}{dt} \bigg( \frac{dg}{db} \bigg) \cdot \frac{\partial b}{\partial t} + \frac{dg}{db} \cdot \frac{\partial^2 b}{\partial t^2} \frac{\partial a}{\partial t} = -1, \frac{\partial b}{\partial t}=1 \frac{\partial^2 a}{\partial t^2} = \frac{\partial^2 b}{\partial t^2} = 0 = \frac{d}{dt} \bigg( \frac{dg}{db} \bigg) - \frac{d}{dt} \bigg(\frac{df}{da}\bigg) = \frac{d^2g}{db^2} + \frac{d^2f}{da^2} x \frac{\partial a}{\partial x} = 1 \frac{\partial a}{\partial t} \frac{\partial^2\phi}{\partial t^2} = \frac{\partial^2\phi}{\partial x^2}","['multivariable-calculus', 'proof-verification', 'partial-derivative', 'chain-rule']"
84,Green's theorem over an annulus,Green's theorem over an annulus,,"I need help with this problem: Verify Green's Theorem in the plane where $S$ is the annulus $\{(x,y)\in\mathbb{R^2}|a^2\leq x^2+y^2\leq b^2\}$ and $F(x,y)=\left(\frac{-y}{\sqrt{x^2+y^2}},\frac{x}{\sqrt{x^2+y^2}}\right)$ $F(x,y)=\left(\frac{-y}{x^2+y^2},\frac{x}{x^2+y^2}\right)$ $F(x,y)=\left(\frac{x}{x^2+y^2},\frac{y}{x^2+y^2}\right)$ I was able to compute the line integral $\int_{\partial S^+} F\cdot d\mathbf{r}$ , but I'm having problems with the double integral $\iint_S\left(\frac{\partial F_2}{\partial x}-\frac{\partial F_1}{\partial y}\right)dA$ . My problem is that I don't know which limist should I use. I know that I could use polar coordinates, but this problem is from a chapter before change of variables, so I think I'm not supposed to solve it like that. I just need help with getting the limits of integration right, since I think it is easy to compute the double integral after that.","I need help with this problem: Verify Green's Theorem in the plane where is the annulus and I was able to compute the line integral , but I'm having problems with the double integral . My problem is that I don't know which limist should I use. I know that I could use polar coordinates, but this problem is from a chapter before change of variables, so I think I'm not supposed to solve it like that. I just need help with getting the limits of integration right, since I think it is easy to compute the double integral after that.","S \{(x,y)\in\mathbb{R^2}|a^2\leq x^2+y^2\leq b^2\} F(x,y)=\left(\frac{-y}{\sqrt{x^2+y^2}},\frac{x}{\sqrt{x^2+y^2}}\right) F(x,y)=\left(\frac{-y}{x^2+y^2},\frac{x}{x^2+y^2}\right) F(x,y)=\left(\frac{x}{x^2+y^2},\frac{y}{x^2+y^2}\right) \int_{\partial S^+} F\cdot d\mathbf{r} \iint_S\left(\frac{\partial F_2}{\partial x}-\frac{\partial F_1}{\partial y}\right)dA","['integration', 'multivariable-calculus', 'vector-analysis', 'greens-theorem']"
85,Question about a certain part of Weierstrass extreme value theorem,Question about a certain part of Weierstrass extreme value theorem,,"The lemma says that if $A \in \mathbb{R}^{n}$ is compact and $f:A \to \mathbb{R}$ is continuous in $A$ , then there's a real $M$ such that $\forall X \in A,f(X) \leq M$ . My textbook's proof goes this way: if $f(X)$ didn't have such an upper bound, then there would be a sequence $\{X_k\}_{k \in \mathbb{N}} \subseteq	 A$ such that $\forall k \in \mathbb{N},f(X_k) \geq k$ . But since $A$ is compact, there's a convergent subsequence $\{X_{k_j}\}$ such that $X_{k_j} \to P \in A$ . Therefore $f(X_{k_j}) \geq k_j$ . But this is impossible since $f$ is continuous in $A$ . Does this last step amount to showing that $f(P)$ , if defined, would have to be greater than every natural number (which contradicts the Archimedean property)? I also don't understand how to justify that last step. A previous lemma stated that convergent sequences of $A$ whose images under $f$ were bounded implied that the image of the limit of those sequences was also bounded. But in this case we're ""pushing forward"" the bound every time. How's this justified?","The lemma says that if is compact and is continuous in , then there's a real such that . My textbook's proof goes this way: if didn't have such an upper bound, then there would be a sequence such that . But since is compact, there's a convergent subsequence such that . Therefore . But this is impossible since is continuous in . Does this last step amount to showing that , if defined, would have to be greater than every natural number (which contradicts the Archimedean property)? I also don't understand how to justify that last step. A previous lemma stated that convergent sequences of whose images under were bounded implied that the image of the limit of those sequences was also bounded. But in this case we're ""pushing forward"" the bound every time. How's this justified?","A \in \mathbb{R}^{n} f:A \to \mathbb{R} A M \forall X \in A,f(X) \leq M f(X) \{X_k\}_{k \in \mathbb{N}} \subseteq	 A \forall k \in \mathbb{N},f(X_k) \geq k A \{X_{k_j}\} X_{k_j} \to P \in A f(X_{k_j}) \geq k_j f A f(P) A f","['real-analysis', 'multivariable-calculus', 'extreme-value-theorem']"
86,Why do we take magnitude into account when calculating the directional derivative?,Why do we take magnitude into account when calculating the directional derivative?,,"Given that the directional derivative is defined formally as: $$ \nabla_\vec{v}\, f\left(\vec{x}\right) = \lim_{h \to 0} \frac{f\left(\vec{x} + h\vec{v}\right) - f\left(\vec{x}\right)}{h|\vec{v}|} $$ It's not exactly clear why the magnitude should matter here if we are taking some step $h\vec{v}$ if $h \to 0$ . If the directional derivative can be calculated via $\nabla f\left(\vec{x}\right) \cdot \vec{v}$ , any scalar to $\vec{v}$ will arbitrarily scale the magnitude of the directional derivative - but if we take the limit definition, shouldn't magnitude not matter as we are investigating how $f$ changes with some infinitesimal movement in the direction of $\vec{v}$ ? One explanation was to imagine a function $f(x, y)$ representing the altitude via a surface, with a person moving around it. If a person were to move with double the velocity in a certain direction, they should have double to amount in change of height. But I can't reconcile this with the formal definition: why should doubling the velocity change how much $f$ changes if you move infinitesimally in the direction of the velocity?","Given that the directional derivative is defined formally as: It's not exactly clear why the magnitude should matter here if we are taking some step if . If the directional derivative can be calculated via , any scalar to will arbitrarily scale the magnitude of the directional derivative - but if we take the limit definition, shouldn't magnitude not matter as we are investigating how changes with some infinitesimal movement in the direction of ? One explanation was to imagine a function representing the altitude via a surface, with a person moving around it. If a person were to move with double the velocity in a certain direction, they should have double to amount in change of height. But I can't reconcile this with the formal definition: why should doubling the velocity change how much changes if you move infinitesimally in the direction of the velocity?","
\nabla_\vec{v}\, f\left(\vec{x}\right) = \lim_{h \to 0} \frac{f\left(\vec{x} + h\vec{v}\right) - f\left(\vec{x}\right)}{h|\vec{v}|}
 h\vec{v} h \to 0 \nabla f\left(\vec{x}\right) \cdot \vec{v} \vec{v} f \vec{v} f(x, y) f","['multivariable-calculus', 'derivatives']"
87,"Find the volume of the solid bounced by the planes $z=0$,$z=y$ and $x^2+y^2=1$","Find the volume of the solid bounced by the planes , and",z=0 z=y x^2+y^2=1,"So I do the following: $$\int_{-1}^{1}\int_0^{\sqrt{1-x^2}} \int_0^{y}  \,dzdydx$$ , but the answer gives me $\frac{2}{3}$ , as it graphs a cylinder it should be the half of the half of a cylinder of height and radius 1: ( $\frac{\pi}{4}$ )","So I do the following: , but the answer gives me , as it graphs a cylinder it should be the half of the half of a cylinder of height and radius 1: ( )","\int_{-1}^{1}\int_0^{\sqrt{1-x^2}} \int_0^{y}  \,dzdydx \frac{2}{3} \frac{\pi}{4}","['integration', 'multivariable-calculus']"
88,"Rudin Principles of Mathematical Analysis Chapter 10, Exercise 8","Rudin Principles of Mathematical Analysis Chapter 10, Exercise 8",,"I'm working on exercises of chapter 10 in Baby Rudin. I refer to R. Cooke's solutions manual to Baby Rudin while I'm solving those exercises.( https://minds.wisconsin.edu/handle/1793/67009 ) But I think there is a wrong solution for Chap 10, exercise 8. Baby Rudin, chap 10, ex 8 the wrong part of a solution for chap 10, ex 8 Using Theorem 10.9 in Baby Rudin, which is about change of variables on a multiple integral, I think we should represent a integrand on the right side with a mapping T, not an inverse of T. Could you guys check if I'm right or that solution is right?","I'm working on exercises of chapter 10 in Baby Rudin. I refer to R. Cooke's solutions manual to Baby Rudin while I'm solving those exercises.( https://minds.wisconsin.edu/handle/1793/67009 ) But I think there is a wrong solution for Chap 10, exercise 8. Baby Rudin, chap 10, ex 8 the wrong part of a solution for chap 10, ex 8 Using Theorem 10.9 in Baby Rudin, which is about change of variables on a multiple integral, I think we should represent a integrand on the right side with a mapping T, not an inverse of T. Could you guys check if I'm right or that solution is right?",,"['real-analysis', 'analysis', 'multivariable-calculus', 'multiple-integral', 'change-of-variable']"
89,Hessian matrix equal to zero,Hessian matrix equal to zero,,"The function $x\times y + e^{-x\times y}$ has the points that form $x$ -axis and $y$ -axis as critical points, how can I prove that they are points of minimum, the Hessian matrix in those points is equal to zero.","The function has the points that form -axis and -axis as critical points, how can I prove that they are points of minimum, the Hessian matrix in those points is equal to zero.",x\times y + e^{-x\times y} x y,[]
90,How to prove the integral converges?,How to prove the integral converges?,,"Let $(\mathbf M'.\hat{\mathbf n})$ and $f(R,\theta)$ be a continuous function of $R$ and let $f(0,\theta)=0$ . Then how shall we prove the following improper integral converges: $\displaystyle\lim \limits_{a \to 0} \int^{2 \pi}_0\int^b_a (\mathbf M'.\hat{\mathbf n})(\hat{\mathbf r})\ \sqrt{{f_x}^2+{f_y}^2+1}\ \dfrac{1}{R^2+[f(R,\theta)]^2}\ R\ dR\ d\theta \tag 1$ Edit: It has been said in an answer to use the following inequality: $0 \leq \frac{R}{R^2 + f^2} \leq \left( \frac{R}{R^2} = \frac{1}{R} \right)$ However I get my integral as diverging: $\displaystyle\lim \limits_{a \to 0} \int^b_a \dfrac{dR}{R}=-\lim \limits_{a \to 0} \left[    \ln |R|   \right]^b_a=\lim \limits_{a \to 0} \left( \ln \dfrac{ |a|}{|b|}  \right)=\ln (0)=??$ As I am getting $\ln (0)$ , there might be something wrong in my above calculation. Please show where am I wrong and also how the above improper integral $(1)$ converges.","Let and be a continuous function of and let . Then how shall we prove the following improper integral converges: Edit: It has been said in an answer to use the following inequality: However I get my integral as diverging: As I am getting , there might be something wrong in my above calculation. Please show where am I wrong and also how the above improper integral converges.","(\mathbf M'.\hat{\mathbf n}) f(R,\theta) R f(0,\theta)=0 \displaystyle\lim \limits_{a \to 0} \int^{2 \pi}_0\int^b_a
(\mathbf M'.\hat{\mathbf n})(\hat{\mathbf r})\ \sqrt{{f_x}^2+{f_y}^2+1}\
\dfrac{1}{R^2+[f(R,\theta)]^2}\ R\ dR\ d\theta \tag 1 0 \leq \frac{R}{R^2 + f^2} \leq \left( \frac{R}{R^2} = \frac{1}{R} \right) \displaystyle\lim \limits_{a \to 0} \int^b_a \dfrac{dR}{R}=-\lim \limits_{a \to 0} \left[    \ln |R|   \right]^b_a=\lim \limits_{a \to 0} \left( \ln \dfrac{ |a|}{|b|}  \right)=\ln (0)=?? \ln (0) (1)","['calculus', 'multivariable-calculus', 'convergence-divergence', 'improper-integrals', 'polar-coordinates']"
91,What does it mean for a function $f$ to be defined on a disk? (Clairaut's theorem),What does it mean for a function  to be defined on a disk? (Clairaut's theorem),f,"Clairaut's theorem states: ""Suppose $f$ is defined on a disk $D$ that contains the point $(a,b)$ . If the functions..."" My question is just about the first part of this. What does it mean for a function to be defined on a disk? Does it just mean that that disc (which is a always a subset and always(?) a subspace of $\mathbb{R}^2 $ ) is the domain of the function in question? In that case, doesn't defining $f$ only make sense for a function of two variables and nothing else?","Clairaut's theorem states: ""Suppose is defined on a disk that contains the point . If the functions..."" My question is just about the first part of this. What does it mean for a function to be defined on a disk? Does it just mean that that disc (which is a always a subset and always(?) a subspace of ) is the domain of the function in question? In that case, doesn't defining only make sense for a function of two variables and nothing else?","f D (a,b) \mathbb{R}^2  f","['calculus', 'linear-algebra', 'multivariable-calculus', 'functions', 'definition']"
92,Alternating functions and differential forms,Alternating functions and differential forms,,"Can someone help me? Any sugestion will be helpful. According to Courant and John in Introduction to Calculus and Analysis, Vol. 2 chapter 5 section 5.1, if we take: \begin{equation} L = f(x,y)dy - g(x,y)dx \end{equation} the differential form of L will be: \begin{equation} dL = dfdy - dgdx \end{equation} \begin{equation} dL = (f_xdx +f_ydy)dy - (g_xdx + g_ydy)dx \end{equation} \begin{equation} dL = f_xdxdy + f_ydydy - g_xdxdx - g_ydydx \end{equation} By alternating differential forms we know that $dxdy = -dydx$ and $ dxdx = dydy = 0$ . So, the answer will be: \begin{equation} dL = (f_x + g_y)dxdy \end{equation} I know that alternating functions have an important role in the calculus of determinants, as well in vectorial product. But, I do not understand the relation here, I tried to reach the answer performing a determinant, and the best I did was this: \begin{equation} \begin{vmatrix} f_xdx & dx \\ g_xdx & dy \end{vmatrix} + \begin{vmatrix} f_ydy & dx \\ g_ydy & dy \end{vmatrix} \end{equation} But it makes no sense to me. So, why to use differential alternating form ? Why it is applicable in this situation ? This doubt has been tormenting me for a long time.","Can someone help me? Any sugestion will be helpful. According to Courant and John in Introduction to Calculus and Analysis, Vol. 2 chapter 5 section 5.1, if we take: the differential form of L will be: By alternating differential forms we know that and . So, the answer will be: I know that alternating functions have an important role in the calculus of determinants, as well in vectorial product. But, I do not understand the relation here, I tried to reach the answer performing a determinant, and the best I did was this: But it makes no sense to me. So, why to use differential alternating form ? Why it is applicable in this situation ? This doubt has been tormenting me for a long time.","\begin{equation}
L = f(x,y)dy - g(x,y)dx
\end{equation} \begin{equation}
dL = dfdy - dgdx
\end{equation} \begin{equation}
dL = (f_xdx +f_ydy)dy - (g_xdx + g_ydy)dx
\end{equation} \begin{equation}
dL = f_xdxdy + f_ydydy - g_xdxdx - g_ydydx
\end{equation} dxdy = -dydx  dxdx = dydy = 0 \begin{equation}
dL = (f_x + g_y)dxdy
\end{equation} \begin{equation}
\begin{vmatrix} f_xdx & dx \\ g_xdx & dy \end{vmatrix} + \begin{vmatrix} f_ydy & dx \\ g_ydy & dy \end{vmatrix}
\end{equation}","['linear-algebra', 'multivariable-calculus', 'vector-analysis', 'differential-forms', 'calculus-of-variations']"
93,Values of $a$ and $b$ that maximize the value of $\int^b_a(x-x^2)dx$,Values of  and  that maximize the value of,a b \int^b_a(x-x^2)dx,"Values of $a$ and $b$ that maximize the value of $$\int^b_a(x-x^2)dx$$ . Let \begin{align}F(a,b)=\int^b_a(x-x^2)dx.\end{align} Then, by Fundamental Theorem of Calculus \begin{align}&\frac{\partial F}{\partial b}=b-b^2\\&\frac{\partial F}{\partial a}=-(a-a^2)\end{align} Setting these to zero, yields $b=0 \;\text{or}\;1$ and $a=0 \;\text{or}\;1.$ Assuming that $a<b$ , then $a=0$ and $b=1$ are the values. Please, I'm I right? If not, can you please, provide a correct one?","Values of and that maximize the value of . Let Then, by Fundamental Theorem of Calculus Setting these to zero, yields and Assuming that , then and are the values. Please, I'm I right? If not, can you please, provide a correct one?","a b \int^b_a(x-x^2)dx \begin{align}F(a,b)=\int^b_a(x-x^2)dx.\end{align} \begin{align}&\frac{\partial F}{\partial b}=b-b^2\\&\frac{\partial F}{\partial a}=-(a-a^2)\end{align} b=0 \;\text{or}\;1 a=0 \;\text{or}\;1. a<b a=0 b=1","['integration', 'algebra-precalculus', 'multivariable-calculus']"
94,Intuition for formula in proof of Poincaré Lemma,Intuition for formula in proof of Poincaré Lemma,,"In Poincaré lemma for star shaped domain , the potential $f$ of a vector field ${\bf F}$ is given explicitly for $$f(x) = \int_0^1 {\bf F}(tx)\cdot x\,dt.$$ Can be given some (physical...) intuition for this formula?","In Poincaré lemma for star shaped domain , the potential of a vector field is given explicitly for Can be given some (physical...) intuition for this formula?","f {\bf F} f(x) = \int_0^1 {\bf F}(tx)\cdot x\,dt.","['multivariable-calculus', 'intuition']"
95,"Do we need to consider ""direction"" while discussing regions of integration?","Do we need to consider ""direction"" while discussing regions of integration?",,"Consider the following double integral from Thomas' Calculus $$\int_0^3 \int_1^{4-2u} \frac{4-2u}{v^2} \ dv\ du;$$ sketch the region of integration. If we represent $u$ on the horizontal axis and $v$ on the vertical axis and sketch the limits, we get For $0 \leq u \leq \frac{3}{2}$ , we have $1 \leq 4-2u$ (blue region); for $\frac{3}{2} \leq u \leq 3$ , we have $1 \geq 4-2u$ (orange region). Is this sketch correct? Do we have to consider the direction when discussing the region of integration? I think this is a higher dimensional parallel to the convention that $$\int_a^b f\left( x \right) \ dx = -\int_b^a f\left( x \right) \ dx$$ Can someone point me to a reference with more details on this? If you find the presentation of my question confusing, I am very sorry.","Consider the following double integral from Thomas' Calculus sketch the region of integration. If we represent on the horizontal axis and on the vertical axis and sketch the limits, we get For , we have (blue region); for , we have (orange region). Is this sketch correct? Do we have to consider the direction when discussing the region of integration? I think this is a higher dimensional parallel to the convention that Can someone point me to a reference with more details on this? If you find the presentation of my question confusing, I am very sorry.",\int_0^3 \int_1^{4-2u} \frac{4-2u}{v^2} \ dv\ du; u v 0 \leq u \leq \frac{3}{2} 1 \leq 4-2u \frac{3}{2} \leq u \leq 3 1 \geq 4-2u \int_a^b f\left( x \right) \ dx = -\int_b^a f\left( x \right) \ dx,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
96,What differential equation corresponds to this vector field?,What differential equation corresponds to this vector field?,,"Here is a vector field: $$ \vec F(x,y)=\{\sin(x),\sin(y)\}, $$ where $x,y \in (0,\pi).$ How do you find the differential equation, that when solved gives the integral curves for this vector field? I made this plot on WolframAlpha:","Here is a vector field: where How do you find the differential equation, that when solved gives the integral curves for this vector field? I made this plot on WolframAlpha:"," \vec F(x,y)=\{\sin(x),\sin(y)\},  x,y \in (0,\pi).","['ordinary-differential-equations', 'multivariable-calculus', 'vector-fields']"
97,Prove that $V$ is a compact volume and compute its volume,Prove that  is a compact volume and compute its volume,V,"The following problem is something I thought about myself, trying to improve my integration and multivariable calculus skills. I would be glad to hear your review about it (maybe I forgot to mention something relevant to the problem), and of course a way to solve it (since I couldn't do it myself). Let $f:\mathbb{R}^3\rightarrow\mathbb{R}$ be a continuously differentiable scalar function, which is defined on a smooth (differentiable) and simple curve $\{\bar\gamma(t) \mid t\in[a,b]\}$ in $\mathbb{R}^3$ , such that $f(x,y,z)>0$ for every $(x,y,z)\in\mathbb{R}^3$ , with the exception of $f(\bar\gamma(a))=f(\bar\gamma(b))=0$ . The curve $\bar\gamma(t)$ is not closed ( $\bar\gamma(a)\neq\bar\gamma(b)$ ). For every point $t_0\in[a,b]$ on the curve, we will define and construct a circle $C_{t_0}$ , such that the circle is laying on the plane that is perpendicular to the curve at that point (meaning that the normal of the plane is the tangent vector to the curve at that point). The radius of $C_{t_0}$ is $f(\bar\gamma(t_0))$ , and its center is $P=\bar\gamma(t_0)$ . Now we will define a volume $V\subset\mathbb{R}^3$ such that: $$\partial V=\bigcup_{t\in[a,b]}C_{t}$$ Prove (or maybe disprove?) that $V$ is compact, and suggest and construct a way to compute its volume. Thanks! P.S.: A multivariable calculus solution would be the best for me, but of course anything else would also be great.","The following problem is something I thought about myself, trying to improve my integration and multivariable calculus skills. I would be glad to hear your review about it (maybe I forgot to mention something relevant to the problem), and of course a way to solve it (since I couldn't do it myself). Let be a continuously differentiable scalar function, which is defined on a smooth (differentiable) and simple curve in , such that for every , with the exception of . The curve is not closed ( ). For every point on the curve, we will define and construct a circle , such that the circle is laying on the plane that is perpendicular to the curve at that point (meaning that the normal of the plane is the tangent vector to the curve at that point). The radius of is , and its center is . Now we will define a volume such that: Prove (or maybe disprove?) that is compact, and suggest and construct a way to compute its volume. Thanks! P.S.: A multivariable calculus solution would be the best for me, but of course anything else would also be great.","f:\mathbb{R}^3\rightarrow\mathbb{R} \{\bar\gamma(t) \mid t\in[a,b]\} \mathbb{R}^3 f(x,y,z)>0 (x,y,z)\in\mathbb{R}^3 f(\bar\gamma(a))=f(\bar\gamma(b))=0 \bar\gamma(t) \bar\gamma(a)\neq\bar\gamma(b) t_0\in[a,b] C_{t_0} C_{t_0} f(\bar\gamma(t_0)) P=\bar\gamma(t_0) V\subset\mathbb{R}^3 \partial V=\bigcup_{t\in[a,b]}C_{t} V","['calculus', 'multivariable-calculus', 'differential-geometry', 'differential-topology']"
98,What does a gradient multiplied by the inverse of the Hessian mean?,What does a gradient multiplied by the inverse of the Hessian mean?,,"In the paper VIME: Variational Information Maximizing Exploration , the authors suggest taking a single second-order step to efficiently optimize a variant of evidence lower bound They say To optimize Eq. (12) efﬁciently, we only take a single second-order step. This way, the gradient is rescaled according to the curvature of the KL divergence at the origin. As such, we compute $D_{KL} [q(\theta; \phi + \lambda\Delta\phi)\Vert q(\theta; \phi)]$ , with the update step $\Delta\phi$ deﬁned as $$ \Delta\phi=H^{-1}(l)\nabla_\phi l(q(\theta;\phi),s_t) \tag{13} $$ in which $H(l)$ is the Hessian of $l(q(\theta;\phi),s_t)$ . I'm quite confused about how they obtain Eq. $(13)$ : why would they multiply the gradient by the inverse of the Hessian?","In the paper VIME: Variational Information Maximizing Exploration , the authors suggest taking a single second-order step to efficiently optimize a variant of evidence lower bound They say To optimize Eq. (12) efﬁciently, we only take a single second-order step. This way, the gradient is rescaled according to the curvature of the KL divergence at the origin. As such, we compute , with the update step deﬁned as in which is the Hessian of . I'm quite confused about how they obtain Eq. : why would they multiply the gradient by the inverse of the Hessian?","D_{KL} [q(\theta; \phi + \lambda\Delta\phi)\Vert q(\theta; \phi)] \Delta\phi 
\Delta\phi=H^{-1}(l)\nabla_\phi l(q(\theta;\phi),s_t) \tag{13}
 H(l) l(q(\theta;\phi),s_t) (13)","['calculus', 'multivariable-calculus', 'optimization']"
99,"Prove that $\lim\limits_{(x,y)\to (0,0)} \frac{\sin(xy)}{\sin(x)\sin(y)}$ exists, using $(\epsilon ,\delta)$-argument","Prove that  exists, using -argument","\lim\limits_{(x,y)\to (0,0)} \frac{\sin(xy)}{\sin(x)\sin(y)} (\epsilon ,\delta)","I need to prove that $\lim\limits_{(x,y)\to (0,0)} \frac{\sin(xy)}{\sin(x)\sin(y)}$ exists using the $ϵ-δ$ limit definition as $(x,y)→(0,0)$ . I know that the limit exist and is equal to $1$ . I worked on it using the squeeze theorem, but we didn't see it in class so I can't use it, the only thing I can use is the $ϵ-δ$ definition and I have no idea how to do it.","I need to prove that exists using the limit definition as . I know that the limit exist and is equal to . I worked on it using the squeeze theorem, but we didn't see it in class so I can't use it, the only thing I can use is the definition and I have no idea how to do it.","\lim\limits_{(x,y)\to (0,0)} \frac{\sin(xy)}{\sin(x)\sin(y)} ϵ-δ (x,y)→(0,0) 1 ϵ-δ","['limits', 'multivariable-calculus', 'limits-without-lhopital', 'epsilon-delta']"
