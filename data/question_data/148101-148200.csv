,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"An identity $ \int_{\partial B(0,1)}u(x_0+aw)u(x_0+cw)$ with a harmonic function $u$",An identity  with a harmonic function," \int_{\partial B(0,1)}u(x_0+aw)u(x_0+cw) u","This is Question 2.18 from Gilbarg and Trudinger, chapter 2. We are given that $\Omega$ is open bounded smooth boundary. Now fix $x_0\in \Omega$ and a constant $c>0$ such that $B(x_0,c)\subset\subset \Omega$ . Next, given $u$ such that $\Delta u=0$ in $\Omega$ and another $2$ positive constants $a$ and $b$ such that $a<b<c$ and $b^2=ac$ . Then, the question asks us to prove the following simple and beautiful equation. $$ \int_{\partial B(0,1)}u(x_0+aw)u(x_0+cw)\,dSw=\int_{\partial B(0,1)}u^2(x_0+bw)\,dSw $$ My try: Define $$ v(w):=u(x_0+aw)u(x_0+cw)- u^2(x_0+bw)$$ If I can prove $v$ is harmonic, then by Mean Value Theorem I would be done, since $$ 0=v(0)=\int_{\partial B(0,1)} v(w)\,dSw$$ Hence, by $v\in C^2$ , I just compute $\Delta v$ and obtain that, after simplification, $$ \Delta v(w)=2b^2(\nabla u(x_0+aw)\cdot\nabla u(x_0+cw)-\nabla u(x_0+bw)\cdot\nabla u(x_0+bw)) $$ I can not go further from here. Any hint would be very welcome!","This is Question 2.18 from Gilbarg and Trudinger, chapter 2. We are given that is open bounded smooth boundary. Now fix and a constant such that . Next, given such that in and another positive constants and such that and . Then, the question asks us to prove the following simple and beautiful equation. My try: Define If I can prove is harmonic, then by Mean Value Theorem I would be done, since Hence, by , I just compute and obtain that, after simplification, I can not go further from here. Any hint would be very welcome!","\Omega x_0\in \Omega c>0 B(x_0,c)\subset\subset \Omega u \Delta u=0 \Omega 2 a b a<b<c b^2=ac  \int_{\partial B(0,1)}u(x_0+aw)u(x_0+cw)\,dSw=\int_{\partial B(0,1)}u^2(x_0+bw)\,dSw   v(w):=u(x_0+aw)u(x_0+cw)- u^2(x_0+bw) v  0=v(0)=\int_{\partial B(0,1)} v(w)\,dSw v\in C^2 \Delta v  \Delta v(w)=2b^2(\nabla u(x_0+aw)\cdot\nabla u(x_0+cw)-\nabla u(x_0+bw)\cdot\nabla u(x_0+bw)) ","['analysis', 'partial-differential-equations', 'harmonic-functions', 'spherical-coordinates', 'mean-value-theorem']"
1,"A Partition of $[0, 1)$",A Partition of,"[0, 1)","Consider a sequence of disjoint nonempty intervals $[\ell_{n}, r_{n}) \subseteq [0, 1)$, $n \in \mathbb{N}$, such that $$\sum_{n = 1}^{\infty} (r_{n} - \ell_{n}) = 1,\qquad 0 < r_{n + 1} - \ell_{n + 1} \leq r_{n} - \ell_{n} < 1. $$ My question is, need the set $$A=[0, 1) \setminus \bigcup_{n \in \mathbb{N}} [\ell_{n}, r_{n})$$ be countable? All constructions I've been able to come up with are such that $A$ is countable. Obviously, $A$ has Lebesgue measure $0$, but that does not imply that $A$ is countable.","Consider a sequence of disjoint nonempty intervals $[\ell_{n}, r_{n}) \subseteq [0, 1)$, $n \in \mathbb{N}$, such that $$\sum_{n = 1}^{\infty} (r_{n} - \ell_{n}) = 1,\qquad 0 < r_{n + 1} - \ell_{n + 1} \leq r_{n} - \ell_{n} < 1. $$ My question is, need the set $$A=[0, 1) \setminus \bigcup_{n \in \mathbb{N}} [\ell_{n}, r_{n})$$ be countable? All constructions I've been able to come up with are such that $A$ is countable. Obviously, $A$ has Lebesgue measure $0$, but that does not imply that $A$ is countable.",,"['analysis', 'lebesgue-measure']"
2,Question about the Ascoli-Arzelá Theorem proof,Question about the Ascoli-Arzelá Theorem proof,,"Ascoli-Arzelá Thoerem : Let $K$ be a compact space and $M$ be a metric space and $C(K,M)$ be the set of continuous functions from $K$ to $M$. $H \subset C(K,M) $ is relatively compact if and only if $H$ is equicontinuous and $H(x) : = \{ f(x): f\in H\}$ is relatively compact. I want to understand the step in the implication ($\Leftarrow$). In order to show that $H$ is relatively compact I'd like to know how to prove that $\overline{H}$ is complete. $\underline{Ideas:} $ First one. Let $\overline{f}_n $ be a Cauchy sequence in $ \overline{H}$. Then for any $n$, let $f_n \in H$  such that  \begin{equation} d(f_n,\overline{f}_n) < 1/n \end{equation} As by hypothesis $\overline{H(x)}$ is compact and therefore complete, $(f_n(x))_n$ is a Cauchy sequence and we can define $f(x):= \lim_{n} f_n(x)$. There is a result that if $\overline{f}_n$ is equicontinuous  and converges punctually to $f$, then $f$ is continuous and converges uniformly. I can see that  $\overline{f}_n$ and converges punctually to $f$ but I can't see that $ \overline{f}_n $ is equicontinuous. The place where I saw this Idea says that this is by construction. I can see by construction the party that I alredy said. How to  see this? Second one. It suffices to show that $f_n \rightarrow f$ uniformly since $f$  will be continuous as uniform limit  of continuous functions. As $\overline{f}_n$ is a Cauchy sequence for all $\varepsilon >0 $ there is $n_0$ such that $n>n_0$ impllies \begin{equation} d(\overline{f}_n,\overline{f}_m) : = \sup_{x \in K} d(f_n(x),f_m(x)) < \varepsilon. \end{equation} Then, by triangle inequality and taking the limit as $m \rightarrow \infty$ in \begin{equation} d(\overline{f}_n,f) \le d(\overline{f}_n,\overline{f}_m) + d(\overline{f}_m, f) \end{equation} to obtain  \begin{equation} d(\overline{f}_n,f) \le \varepsilon + \lim_{m} d(\overline{f}_m, f). \end{equation} Then we only need to prove that \begin{equation} \lim_{m\rightarrow \infty} \sup_{x \in K} d(\overline{ f}_n(x),f(x)). \end{equation} Note that we can use the same idea to  show that $C(K,M)$ is  complete (I do not know if this is true, for example is true if $M = \mathbb{R}$). Because  if this is true $\overline{H}$ is complete because is the closure of a set contained in a in a complete space is compact. Any help will be good. I will be very grateful.","Ascoli-Arzelá Thoerem : Let $K$ be a compact space and $M$ be a metric space and $C(K,M)$ be the set of continuous functions from $K$ to $M$. $H \subset C(K,M) $ is relatively compact if and only if $H$ is equicontinuous and $H(x) : = \{ f(x): f\in H\}$ is relatively compact. I want to understand the step in the implication ($\Leftarrow$). In order to show that $H$ is relatively compact I'd like to know how to prove that $\overline{H}$ is complete. $\underline{Ideas:} $ First one. Let $\overline{f}_n $ be a Cauchy sequence in $ \overline{H}$. Then for any $n$, let $f_n \in H$  such that  \begin{equation} d(f_n,\overline{f}_n) < 1/n \end{equation} As by hypothesis $\overline{H(x)}$ is compact and therefore complete, $(f_n(x))_n$ is a Cauchy sequence and we can define $f(x):= \lim_{n} f_n(x)$. There is a result that if $\overline{f}_n$ is equicontinuous  and converges punctually to $f$, then $f$ is continuous and converges uniformly. I can see that  $\overline{f}_n$ and converges punctually to $f$ but I can't see that $ \overline{f}_n $ is equicontinuous. The place where I saw this Idea says that this is by construction. I can see by construction the party that I alredy said. How to  see this? Second one. It suffices to show that $f_n \rightarrow f$ uniformly since $f$  will be continuous as uniform limit  of continuous functions. As $\overline{f}_n$ is a Cauchy sequence for all $\varepsilon >0 $ there is $n_0$ such that $n>n_0$ impllies \begin{equation} d(\overline{f}_n,\overline{f}_m) : = \sup_{x \in K} d(f_n(x),f_m(x)) < \varepsilon. \end{equation} Then, by triangle inequality and taking the limit as $m \rightarrow \infty$ in \begin{equation} d(\overline{f}_n,f) \le d(\overline{f}_n,\overline{f}_m) + d(\overline{f}_m, f) \end{equation} to obtain  \begin{equation} d(\overline{f}_n,f) \le \varepsilon + \lim_{m} d(\overline{f}_m, f). \end{equation} Then we only need to prove that \begin{equation} \lim_{m\rightarrow \infty} \sup_{x \in K} d(\overline{ f}_n(x),f(x)). \end{equation} Note that we can use the same idea to  show that $C(K,M)$ is  complete (I do not know if this is true, for example is true if $M = \mathbb{R}$). Because  if this is true $\overline{H}$ is complete because is the closure of a set contained in a in a complete space is compact. Any help will be good. I will be very grateful.",,"['analysis', 'reference-request', 'metric-spaces']"
3,Can we characterize the space of functions which is real analytic but not real entire?,Can we characterize the space of functions which is real analytic but not real entire?,,"A complex valued function $F,$ defined on  an open set $E$ in the plane $\mathbb R^{2}$ , is said to be real-analytic in $E$ if to every point $(s_{0}, t_{0})$ in there corresponds an expansion with complex coefficients $$F(s, t)= \sum_{n,m=0}^{\infty} a_{nm}(s-s_{0})^{m} (t-t_{0})^{n},$$ which converges absolutely for all $(s,t)$ in some neighbourhood of $(s_{0}, t_{0}).$ If $F$ is defined in the whole plane $\mathbb R^{2}$ by a series $$F(s, t)= \sum_{n,m=0}^{\infty} a_{nm}s^{m} t^{n},$$ which converges absolutely for every $(s,t),$ the we call $F$ real-entire . Let us introduce temporary notations, $$RA(\mathbb R^{2}):=\text{The space of real analytic functions on $\mathbb R^{2}$},$$ and $$RE(\mathbb R^{2}):=\text{The space of real entire functions on $\mathbb R^{2}$}$$ Note . We note that, $RE(\mathbb R^{2}) \subset RA(\mathbb R^{2}).$ Example . There exists $$f(s,t) = \frac{1}{(1+s^{2}) (1+t^{2})}, (s,t \in \mathbb R).$$ is real- analytic in the whole plane $\mathbb R^{2}$ but not real-entire; that is, $f\in RA(\mathbb R^{2})$ but $f\notin RE(\mathbb R^{2}).$ My naive questions are : (1) How one can construct few more examples $f$ so that $f\in  RA(\mathbb R^{2})$ but $f\notin RE(\mathbb R^{2})$ ? (2) Can we think of some well-known function space say $E$ , so that, $E\subset RA(\mathbb R^{2})\setminus RE(\mathbb R^{2})$ ? (3) Can we expect to characterize the set $RA(\mathbb R^{2})\setminus RE(\mathbb R^{2})$ (=The space of functions which is real analytic but not real entire) ? Thanks,","A complex valued function defined on  an open set in the plane , is said to be real-analytic in if to every point in there corresponds an expansion with complex coefficients which converges absolutely for all in some neighbourhood of If is defined in the whole plane by a series which converges absolutely for every the we call real-entire . Let us introduce temporary notations, and Note . We note that, Example . There exists is real- analytic in the whole plane but not real-entire; that is, but My naive questions are : (1) How one can construct few more examples so that but ? (2) Can we think of some well-known function space say , so that, ? (3) Can we expect to characterize the set (=The space of functions which is real analytic but not real entire) ? Thanks,","F, E \mathbb R^{2} E (s_{0}, t_{0}) F(s, t)= \sum_{n,m=0}^{\infty} a_{nm}(s-s_{0})^{m} (t-t_{0})^{n}, (s,t) (s_{0}, t_{0}). F \mathbb R^{2} F(s, t)= \sum_{n,m=0}^{\infty} a_{nm}s^{m} t^{n}, (s,t), F RA(\mathbb R^{2}):=\text{The space of real analytic functions on \mathbb R^{2}}, RE(\mathbb R^{2}):=\text{The space of real entire functions on \mathbb R^{2}} RE(\mathbb R^{2}) \subset RA(\mathbb R^{2}). f(s,t) = \frac{1}{(1+s^{2}) (1+t^{2})}, (s,t \in \mathbb R). \mathbb R^{2} f\in RA(\mathbb R^{2}) f\notin RE(\mathbb R^{2}). f f\in  RA(\mathbb R^{2}) f\notin RE(\mathbb R^{2}) E E\subset RA(\mathbb R^{2})\setminus RE(\mathbb R^{2}) RA(\mathbb R^{2})\setminus RE(\mathbb R^{2})","['real-analysis', 'analysis', 'power-series', 'intuition', 'analyticity']"
4,Why define the Lebesgue-Integral just for measurable functions?,Why define the Lebesgue-Integral just for measurable functions?,,"Usually, the Lebesgue integral, for example on Wikipedia , is defined for non-negative measureable functions as $$  \int_E f \, d\mu := \sup\left\{ \int_E s \, d\mu : 0 \le s \le f, s \text{ simple } \right\}. $$ But why suppose that $f$ should be measureable? This is not used in the definition here, so we could define this supremum for arbitrary functions $f : X \to [0,\infty)$?","Usually, the Lebesgue integral, for example on Wikipedia , is defined for non-negative measureable functions as $$  \int_E f \, d\mu := \sup\left\{ \int_E s \, d\mu : 0 \le s \le f, s \text{ simple } \right\}. $$ But why suppose that $f$ should be measureable? This is not used in the definition here, so we could define this supremum for arbitrary functions $f : X \to [0,\infty)$?",,"['analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
5,Riemann Sums as in Königsberger Analysis 1,Riemann Sums as in Königsberger Analysis 1,,"Intro : I must take a small detour here which is only relevant if you do not know the book itself and care about my background. I am working with Königsberger Analysis I (can be found on Springerlink). Currently I am in Chapter 11 which focusses on integration. Königsberger takes the following approach to introduce Integral Calculus: He defines step functions $\varphi: [a,b] \to \mathbb{C}$ such that for all $x \in (x_{k-1},x_k)$ the step function $\varphi$ is constant $c_k$ and then defines the Integral of step functions: $$\int_a^b \varphi(x)dx := \sum_{k=1}^n c_k \Delta x_k $$ He defines regulated functions $f:I \to \mathbb{C}$ on an Intervall $I$ such that for all $x \in (a.b)$ the left-sided limit and the right-sided limit exists. He introduces the reader to the 'approximation theorem' Approximation theorem : A function $f$ on a compact Intervall $[a,b]$ is a regulated function if and only if for all $\epsilon > 0$ there exists a step function $\varphi $ such that $|f(x)-\varphi(x)| \leq \epsilon$ for all $x \in [a,b]$ Corollary to the theorem he shows that for regulated functions $f$ the following limit always exists and defines this as the integral of $f$ over $[a,b]$ $$ \int_a^b f(x) dx := \lim_{n \to \infty} \int_a^b \varphi_n(x)dx  $$ My problem : I believe to understand the above topics and associated proofs 'quite well'. However in the last section of the chapter Königsberger tries to make the connection of regulated functions to the Riemann Sum with the following theorem: Theorem : Let $f: [a,b] \to \mathbb{C}$ be a regulated function. Then for all $\epsilon > 0$ there exists a $\delta >0$ such that for all partitions $Z$ of $[a,b]$ with $\max \Delta x_k \leq \delta$ and arbitrary $\xi_k \in [x_{k-1},x_k]$ the following holds: $$ \left| \sum_{k=1}^n f( \xi_k) \Delta x_k - \int_a^b f(x) dx \right| \leq \epsilon $$ I struggle with the first part of the proof. (Page 216) Proof : Königsberger suggest to first verify the theorem for step functions rather than regulated functions and then use the approximation theorem. He says that showing it for step functions should be done by induction after the number of ""jump points"" $m$ (translator suggests saltus and jump discontinuity ) and that it is very easy. I only care about the first induction steps $m=0$ and $m=1$ which Königsberger describes as trivial, for $m=1$ he mentions to choose $\delta:= \epsilon / 4 \| \varphi\|$ and my problem is I really don't see how he obtains these things. After this I can complete the proof on my own because he's very thorough from that point on.","Intro : I must take a small detour here which is only relevant if you do not know the book itself and care about my background. I am working with Königsberger Analysis I (can be found on Springerlink). Currently I am in Chapter 11 which focusses on integration. Königsberger takes the following approach to introduce Integral Calculus: He defines step functions $\varphi: [a,b] \to \mathbb{C}$ such that for all $x \in (x_{k-1},x_k)$ the step function $\varphi$ is constant $c_k$ and then defines the Integral of step functions: $$\int_a^b \varphi(x)dx := \sum_{k=1}^n c_k \Delta x_k $$ He defines regulated functions $f:I \to \mathbb{C}$ on an Intervall $I$ such that for all $x \in (a.b)$ the left-sided limit and the right-sided limit exists. He introduces the reader to the 'approximation theorem' Approximation theorem : A function $f$ on a compact Intervall $[a,b]$ is a regulated function if and only if for all $\epsilon > 0$ there exists a step function $\varphi $ such that $|f(x)-\varphi(x)| \leq \epsilon$ for all $x \in [a,b]$ Corollary to the theorem he shows that for regulated functions $f$ the following limit always exists and defines this as the integral of $f$ over $[a,b]$ $$ \int_a^b f(x) dx := \lim_{n \to \infty} \int_a^b \varphi_n(x)dx  $$ My problem : I believe to understand the above topics and associated proofs 'quite well'. However in the last section of the chapter Königsberger tries to make the connection of regulated functions to the Riemann Sum with the following theorem: Theorem : Let $f: [a,b] \to \mathbb{C}$ be a regulated function. Then for all $\epsilon > 0$ there exists a $\delta >0$ such that for all partitions $Z$ of $[a,b]$ with $\max \Delta x_k \leq \delta$ and arbitrary $\xi_k \in [x_{k-1},x_k]$ the following holds: $$ \left| \sum_{k=1}^n f( \xi_k) \Delta x_k - \int_a^b f(x) dx \right| \leq \epsilon $$ I struggle with the first part of the proof. (Page 216) Proof : Königsberger suggest to first verify the theorem for step functions rather than regulated functions and then use the approximation theorem. He says that showing it for step functions should be done by induction after the number of ""jump points"" $m$ (translator suggests saltus and jump discontinuity ) and that it is very easy. I only care about the first induction steps $m=0$ and $m=1$ which Königsberger describes as trivial, for $m=1$ he mentions to choose $\delta:= \epsilon / 4 \| \varphi\|$ and my problem is I really don't see how he obtains these things. After this I can complete the proof on my own because he's very thorough from that point on.",,"['real-analysis', 'analysis', 'self-learning']"
6,Using paraboloidal coordinates,Using paraboloidal coordinates,,"I have the 3-dimensional paraboloidal coordinates $$s_{\pm}=\sqrt{x^2+y^2+z^2}\pm z$$ $$\phi=ArcTan(y/x)$$ with the inverse transformation $$x=\sqrt{s_+ \cdot s_-}\cdot cos(\phi)$$ $$y=\sqrt{s_+ \cdot s_-}\cdot sin(\phi)$$ $$z=\frac{s_+ - s_-}{2}$$ Questions: Properties in integration: Suppose I have $ I= \int_{0}^{\infty} ds_+ \int_{0}^{\infty} ds_- f(s_+,s_-) \cdot \delta(s_+ - t)$, can I naively apply the Dirac-Delta and get $I=\int_{0}^{\infty} ds_- f(t,s_-)$ ? Properties in differentiation: Suppose I have $D=\frac{\partial}{\partial s_-} e^{2\cdot s_-} \cdot s_+$, can I naively ignore the $s_+$ Term and get $D=2\cdot e^{2\cdot s_-} \cdot s_+$ ? Every hint will be much appreciated!","I have the 3-dimensional paraboloidal coordinates $$s_{\pm}=\sqrt{x^2+y^2+z^2}\pm z$$ $$\phi=ArcTan(y/x)$$ with the inverse transformation $$x=\sqrt{s_+ \cdot s_-}\cdot cos(\phi)$$ $$y=\sqrt{s_+ \cdot s_-}\cdot sin(\phi)$$ $$z=\frac{s_+ - s_-}{2}$$ Questions: Properties in integration: Suppose I have $ I= \int_{0}^{\infty} ds_+ \int_{0}^{\infty} ds_- f(s_+,s_-) \cdot \delta(s_+ - t)$, can I naively apply the Dirac-Delta and get $I=\int_{0}^{\infty} ds_- f(t,s_-)$ ? Properties in differentiation: Suppose I have $D=\frac{\partial}{\partial s_-} e^{2\cdot s_-} \cdot s_+$, can I naively ignore the $s_+$ Term and get $D=2\cdot e^{2\cdot s_-} \cdot s_+$ ? Every hint will be much appreciated!",,"['analysis', 'coordinate-systems']"
7,"$f: [0,\infty) \to \mathbb{R}$ is continuous and $\displaystyle \lim_{x \to \infty}f(x) = L < \infty$. Prove that $f$ is uniformly continuous.",is continuous and . Prove that  is uniformly continuous.,"f: [0,\infty) \to \mathbb{R} \displaystyle \lim_{x \to \infty}f(x) = L < \infty f","Suppose that $f: [0,\infty) \to \mathbb{R}$ is a continuous function and $\displaystyle \lim_{x \to \infty}f(x)$ exists. Prove that $f$ is uniformly continuous on $[0,\infty)$. Here's my solution, but because the problem has been given in a math contest preparation program I'm afraid that there's some point that I might be missing. Since $\displaystyle \lim_{x \to \infty}f(x)$ exists, there exists $L \in \mathbb{R}$ such that: $\forall \epsilon>0, \exists M>0 \text{ such that } x > M \implies |f(x)-L|<\epsilon/2$ $\forall \epsilon>0, \exists M>0 \text{ such that } y > M \implies |f(y)-L|<\epsilon/2$ Therefore, $\forall x,y \in (M,\infty)$ we have shown that: $|f(x)-f(y)|<\epsilon$. Now, let's consider $f$ on $[0,M]$. Since $[0,M]$ is a compact interval and $f$ is continuous, $f$ is uniformly continuous on $[0,M]$: $\forall \epsilon>0, \exists \delta_1>0, \forall x,y \in [0,M]: |x-y|<\delta_1 \implies |f(x)-f(y)|<\epsilon$. Now let us focus our attention at $x=M$. Since $f: [0,\infty) \to \mathbb{R}$ is continuous, it is continuous at $x=M$ as well. Therefore: $\epsilon>0, \delta_2>0, \forall x: |x-M|<\delta_2 \implies |f(x)-f(M)|<\epsilon/2$ So, for any $x,y \in (M-2\delta_2,M+2\delta_2)$ we have $|f(x)-f(y)|<\epsilon$ Now if we set $\delta=\min\{\delta_1,\delta_2\}$ we see that all the $3$ cases above become true and we conclude that $\forall x,y \in [0,\infty): |x-y|<\delta \implies |f(x)-f(y)|<\epsilon$. This proves that $f$ is uniformly continuous on $[0,\infty)$. Is my nonsense considered a proof?","Suppose that $f: [0,\infty) \to \mathbb{R}$ is a continuous function and $\displaystyle \lim_{x \to \infty}f(x)$ exists. Prove that $f$ is uniformly continuous on $[0,\infty)$. Here's my solution, but because the problem has been given in a math contest preparation program I'm afraid that there's some point that I might be missing. Since $\displaystyle \lim_{x \to \infty}f(x)$ exists, there exists $L \in \mathbb{R}$ such that: $\forall \epsilon>0, \exists M>0 \text{ such that } x > M \implies |f(x)-L|<\epsilon/2$ $\forall \epsilon>0, \exists M>0 \text{ such that } y > M \implies |f(y)-L|<\epsilon/2$ Therefore, $\forall x,y \in (M,\infty)$ we have shown that: $|f(x)-f(y)|<\epsilon$. Now, let's consider $f$ on $[0,M]$. Since $[0,M]$ is a compact interval and $f$ is continuous, $f$ is uniformly continuous on $[0,M]$: $\forall \epsilon>0, \exists \delta_1>0, \forall x,y \in [0,M]: |x-y|<\delta_1 \implies |f(x)-f(y)|<\epsilon$. Now let us focus our attention at $x=M$. Since $f: [0,\infty) \to \mathbb{R}$ is continuous, it is continuous at $x=M$ as well. Therefore: $\epsilon>0, \delta_2>0, \forall x: |x-M|<\delta_2 \implies |f(x)-f(M)|<\epsilon/2$ So, for any $x,y \in (M-2\delta_2,M+2\delta_2)$ we have $|f(x)-f(y)|<\epsilon$ Now if we set $\delta=\min\{\delta_1,\delta_2\}$ we see that all the $3$ cases above become true and we conclude that $\forall x,y \in [0,\infty): |x-y|<\delta \implies |f(x)-f(y)|<\epsilon$. This proves that $f$ is uniformly continuous on $[0,\infty)$. Is my nonsense considered a proof?",,['analysis']
8,Calculus and infinitesimals,Calculus and infinitesimals,,"In the definition of reimann integral, why do we put a 'dx' inside the integral sign when practically it serves no purpose except maybe telling what variable you are talking about. Then in some physics classes, i have seen people writing stuff like $f(x)/ g(y) = dy/dx$ as $f(x) dx = g(y) dy $.  Simply asking what is this 'dx' supposed to mean and how can you treat it like a function and sometimes even 'cancel' it from LHS and RHS(when these symbols are somehow present on both sides of equality).","In the definition of reimann integral, why do we put a 'dx' inside the integral sign when practically it serves no purpose except maybe telling what variable you are talking about. Then in some physics classes, i have seen people writing stuff like $f(x)/ g(y) = dy/dx$ as $f(x) dx = g(y) dy $.  Simply asking what is this 'dx' supposed to mean and how can you treat it like a function and sometimes even 'cancel' it from LHS and RHS(when these symbols are somehow present on both sides of equality).",,"['calculus', 'real-analysis', 'analysis']"
9,Density of a set of functions in Schwartz space,Density of a set of functions in Schwartz space,,"I have a difficulty doing the following problem: Let $S(\mathbb{R}^n)$ be the Schwartz space. I need to determine whether the following set of functions $A$: $$A= \{f\in S(\mathbb{R}^n): \text{supp}(\hat{f}) \text{ is compact and } 0\not\in \text{supp}(\hat{f})\}$$ is dense in $L^p(\mathbb{R}^n)$, where $\hat{f}$ is the Fourier transform of $f$. I think that the above space is dense in $L^p$ when $p\in (1,\infty)$, but not when $p=1$ or $\infty$, but I don't know how to prove it. Any help is appreciated.","I have a difficulty doing the following problem: Let $S(\mathbb{R}^n)$ be the Schwartz space. I need to determine whether the following set of functions $A$: $$A= \{f\in S(\mathbb{R}^n): \text{supp}(\hat{f}) \text{ is compact and } 0\not\in \text{supp}(\hat{f})\}$$ is dense in $L^p(\mathbb{R}^n)$, where $\hat{f}$ is the Fourier transform of $f$. I think that the above space is dense in $L^p$ when $p\in (1,\infty)$, but not when $p=1$ or $\infty$, but I don't know how to prove it. Any help is appreciated.",,"['real-analysis', 'analysis', 'fourier-analysis', 'harmonic-analysis']"
10,Is there a way to interpret summation by parts as integration by parts with counting measure?,Is there a way to interpret summation by parts as integration by parts with counting measure?,,"I find it difficult to remember the different forms of summation by parts: where the indices begin, end, whether to take forward/backward differences, etc. For example, Wikipedia has one form $$\sum_{k=m}^n f_k(g_{k+1}-g_k) = \left[f_{n+1}g_{n+1} - f_m g_m\right] - \sum_{k=m}^n g_{k+1}(f_{k+1}- f_k)$$ and I've seen another $$\sum_{k=m}^n f_k(g_k-g_{k-1}) = [f_n g_n - f_mg_{m-1}] - \sum_{k=m}^{n-1} g_k (f_{k+1} - f_k)$$ Even though I can derive one from the other, it takes some time. So I'm wondering if all this can be easily seen in a general setting with counting measure. Or perhaps if there are some nice heuristics which can help me remember the different forms quickly.","I find it difficult to remember the different forms of summation by parts: where the indices begin, end, whether to take forward/backward differences, etc. For example, Wikipedia has one form $$\sum_{k=m}^n f_k(g_{k+1}-g_k) = \left[f_{n+1}g_{n+1} - f_m g_m\right] - \sum_{k=m}^n g_{k+1}(f_{k+1}- f_k)$$ and I've seen another $$\sum_{k=m}^n f_k(g_k-g_{k-1}) = [f_n g_n - f_mg_{m-1}] - \sum_{k=m}^{n-1} g_k (f_{k+1} - f_k)$$ Even though I can derive one from the other, it takes some time. So I'm wondering if all this can be easily seen in a general setting with counting measure. Or perhaps if there are some nice heuristics which can help me remember the different forms quickly.",,['analysis']
11,"The image of a Banach space under a continuous, linear, open map is a Banach space.","The image of a Banach space under a continuous, linear, open map is a Banach space.",,"This is an exercise from Royden's Real Analysis . Suppose $X$ is a Banach space, there is a continuous, linear, open map from $X$ onto a normed linear space $Y$. Show that $Y$ is Banach.","This is an exercise from Royden's Real Analysis . Suppose $X$ is a Banach space, there is a continuous, linear, open map from $X$ onto a normed linear space $Y$. Show that $Y$ is Banach.",,['analysis']
12,"Exercise from Rogers and Williams's Diffusions, Markov processes and martingales","Exercise from Rogers and Williams's Diffusions, Markov processes and martingales",,"I'm stuck trying to do an exercise (see below) in the first volume of the book by Rogers and Williams and any help would be great (my actual question is right at the end). Let $E$ be a locally compact Hausdorff space with a countable base, let $\cal{E}$ denote the Borel sigma-algebra on $E$, $(C_0(E),||\cdot||)$ denote the space of continuous real valued functions on $E$ which vanish at infinity (with supremum norm) and $(\rm b \cal{E},||\cdot||)$ that of bounded Borel measurable functions on $E$ (again with supremum norm) . An extension of Riesz representation theorem gives the following Theorem 1 : A bounded linear functional $\phi$ on $C_0(E)$ may be written uniquely in the form $$\phi(f)=\mu(f):=\int f(x)\mu(dx)$$ where $\mu$ is a signed measure on $E$ of finite total variation. Exercise : Derive the following theorem from Theorem 1 using the Monotone Class Theorem. Theorem 2 : Suppose that $V:C_0(E)\to \rm b\cal{E}$ is a bounded linear operator such that $0\leq f\leq 1$ implies $0\leq Vf\leq1$. Then there exists a unique kernel $N:E\times\cal{E}\to\mathbb{R}$ such that $i)$ for every $x$ in $E$ and $f$ in $C_0(E)$ $$Vf(x)=Nf(x):=\int N(x,dy)f(y),$$ $ii)$ for every $x$ in $E$ $N(x,\cdot)$ is a (non-negative) measure on $(E,\cal{E})$ such that $N(x,E)\leq 1$, $iii)$ for every $B$ in $\cal{E}$, $N(\cdot,B)$ is $\cal{E}$-measurable. Since for each $x$ in $E$, the map $f\mapsto Vf(x)$ is a linear function on $C_0(E)$, Theorem gives us signed measure $\mu_x$ of finite total variation $\mu_x$ such that $V f (x) = \mu_x(f)$. It seems to me that the way to start is to set $N(x,\cdot):=\mu_x(\cdot)$ for every $x$ in $E$ so that $N$ satisfies $i)$. But from there I'm unsure how to proceed; my immediate reaction was to come up with a way to approximate (w.r.t. to the supremum norm) $\rm b\cal{E}$ functions (in particular, indicator functions) with $C_0(E)$ functions. Then I realised that since $C_0(E)$ is complete, we can't do this. Furthermore, the exercise explicitly states that one can proof Theorem 2 using the Monotone Class Theorem, so there's something I'm missing. Could someone please give a hint on how to proceed, maybe what $\pi$-system and what space of functions I should be thinking of when trying to apply the Monotone Class Theorem?","I'm stuck trying to do an exercise (see below) in the first volume of the book by Rogers and Williams and any help would be great (my actual question is right at the end). Let $E$ be a locally compact Hausdorff space with a countable base, let $\cal{E}$ denote the Borel sigma-algebra on $E$, $(C_0(E),||\cdot||)$ denote the space of continuous real valued functions on $E$ which vanish at infinity (with supremum norm) and $(\rm b \cal{E},||\cdot||)$ that of bounded Borel measurable functions on $E$ (again with supremum norm) . An extension of Riesz representation theorem gives the following Theorem 1 : A bounded linear functional $\phi$ on $C_0(E)$ may be written uniquely in the form $$\phi(f)=\mu(f):=\int f(x)\mu(dx)$$ where $\mu$ is a signed measure on $E$ of finite total variation. Exercise : Derive the following theorem from Theorem 1 using the Monotone Class Theorem. Theorem 2 : Suppose that $V:C_0(E)\to \rm b\cal{E}$ is a bounded linear operator such that $0\leq f\leq 1$ implies $0\leq Vf\leq1$. Then there exists a unique kernel $N:E\times\cal{E}\to\mathbb{R}$ such that $i)$ for every $x$ in $E$ and $f$ in $C_0(E)$ $$Vf(x)=Nf(x):=\int N(x,dy)f(y),$$ $ii)$ for every $x$ in $E$ $N(x,\cdot)$ is a (non-negative) measure on $(E,\cal{E})$ such that $N(x,E)\leq 1$, $iii)$ for every $B$ in $\cal{E}$, $N(\cdot,B)$ is $\cal{E}$-measurable. Since for each $x$ in $E$, the map $f\mapsto Vf(x)$ is a linear function on $C_0(E)$, Theorem gives us signed measure $\mu_x$ of finite total variation $\mu_x$ such that $V f (x) = \mu_x(f)$. It seems to me that the way to start is to set $N(x,\cdot):=\mu_x(\cdot)$ for every $x$ in $E$ so that $N$ satisfies $i)$. But from there I'm unsure how to proceed; my immediate reaction was to come up with a way to approximate (w.r.t. to the supremum norm) $\rm b\cal{E}$ functions (in particular, indicator functions) with $C_0(E)$ functions. Then I realised that since $C_0(E)$ is complete, we can't do this. Furthermore, the exercise explicitly states that one can proof Theorem 2 using the Monotone Class Theorem, so there's something I'm missing. Could someone please give a hint on how to proceed, maybe what $\pi$-system and what space of functions I should be thinking of when trying to apply the Monotone Class Theorem?",,"['analysis', 'functions', 'stochastic-processes']"
13,"Give an example of a uniformly continuous function $f$ on $[0, 1]$ that is differentiable on $(0, 1)$ but for which $f'$ is not bounded on $(0, 1)$",Give an example of a uniformly continuous function  on  that is differentiable on  but for which  is not bounded on,"f [0, 1] (0, 1) f' (0, 1)","Any help would be appreciated! Would $f(x) = \sqrt(x)$ work? Give an example of a uniformly continuous function $f$ on $[0, 1]$ that is differentiable on $(0, 1)$ but for which $f'$ is not bounded on $(0, 1)$","Any help would be appreciated! Would $f(x) = \sqrt(x)$ work? Give an example of a uniformly continuous function $f$ on $[0, 1]$ that is differentiable on $(0, 1)$ but for which $f'$ is not bounded on $(0, 1)$",,"['real-analysis', 'analysis', 'continuity']"
14,The projection on the first factor is a bijection,The projection on the first factor is a bijection,,"I require some clarification and hints on the following Problem: \begin{align}f: X \longrightarrow Y \end{align} The image $p$ is defined as: \begin{align} p: G_f &\longrightarrow X  \\ (x,y) &\longmapsto x\end{align} With $G_f=\lbrace (x,y) \in X \times Y : f(x)=y \rbrace$ Question: Is the function $p$ a bijection? The part I struggle the most with is that this is the first time that I deal with a 2-tuple, so please check if my approach is correct. In the general case, to show if a function is injective choose $x,x' \in X$ such that $f(x)=f(x')$ and show that if $f(x)=f(x')$ then $x=x'$ Here is my approach to this problem: Let $(x,y) \in G_f \subset X \times Y$ and $(x',y') \in G_f \subset X \times Y$ be two 2-tuples such that $p(x,y)=x=p(x',y')$ then $(x,y)=(x',y')$ is an ordered pair, therefore $x=x'$ and $y=y'$ Is this correct? For surjectivity, no ideas so far. Surjectivity: $ \forall y \in Y \exists x \in X : f(x)=y $","I require some clarification and hints on the following Problem: \begin{align}f: X \longrightarrow Y \end{align} The image $p$ is defined as: \begin{align} p: G_f &\longrightarrow X  \\ (x,y) &\longmapsto x\end{align} With $G_f=\lbrace (x,y) \in X \times Y : f(x)=y \rbrace$ Question: Is the function $p$ a bijection? The part I struggle the most with is that this is the first time that I deal with a 2-tuple, so please check if my approach is correct. In the general case, to show if a function is injective choose $x,x' \in X$ such that $f(x)=f(x')$ and show that if $f(x)=f(x')$ then $x=x'$ Here is my approach to this problem: Let $(x,y) \in G_f \subset X \times Y$ and $(x',y') \in G_f \subset X \times Y$ be two 2-tuples such that $p(x,y)=x=p(x',y')$ then $(x,y)=(x',y')$ is an ordered pair, therefore $x=x'$ and $y=y'$ Is this correct? For surjectivity, no ideas so far. Surjectivity: $ \forall y \in Y \exists x \in X : f(x)=y $",,['analysis']
15,Fundamental Optimization question consisting of two parts.,Fundamental Optimization question consisting of two parts.,,"A) Find all extrema of $$f(x)=\sum_{k=1}^{n} x_{k}^{2} $$ subject to the constraint $\sum_{k=1}^{n}\vert x_k\vert^p=1$ B) prove that $$\frac{1}{n^{(2-p)/(2p)}}(\sum \vert x_k\vert^p)^{(1/p)}\le (\sum x_k^2)^{1/p} \le (\sum \vert x_k\vert^p)^{(1/p)}$$ for $k=1,\dots ,n$ $\forall x_1, \dots x_n \in \Bbb R \ \& \  n\in \Bbb N \ \& \ 1\le p\le 2. $ There exists its solution. But this is too complicated,  not simple and clear. So I dont understand it. Please help me how should I solve this in a clear way?","A) Find all extrema of $$f(x)=\sum_{k=1}^{n} x_{k}^{2} $$ subject to the constraint $\sum_{k=1}^{n}\vert x_k\vert^p=1$ B) prove that $$\frac{1}{n^{(2-p)/(2p)}}(\sum \vert x_k\vert^p)^{(1/p)}\le (\sum x_k^2)^{1/p} \le (\sum \vert x_k\vert^p)^{(1/p)}$$ for $k=1,\dots ,n$ $\forall x_1, \dots x_n \in \Bbb R \ \& \  n\in \Bbb N \ \& \ 1\le p\le 2. $ There exists its solution. But this is too complicated,  not simple and clear. So I dont understand it. Please help me how should I solve this in a clear way?",,"['calculus', 'real-analysis', 'analysis', 'optimization']"
16,Dirichlet Problem with piecewise smooth boundary,Dirichlet Problem with piecewise smooth boundary,,"Suppose a domain $ \Omega \subset \mathbb{R^2} $ with $ \partial \Omega $. For $ f \in C^{\infty}(\mathbb{R^2}) $, the dirichlet problem is to find $ u $ with $ \Delta u = 0 $ in $ \Omega $, and $ f = u $ on $ \partial \Omega $. What are the existence and regularity theorems for classical solutions $ u $ in the case that $ \partial \Omega $ is not smooth, but only piecewise smooth? I am not sure exactly where to look for these results beyond just knowing the standard references, and I am a little overwhelmed by this. I would grateful if someone could put point me in a better direction.","Suppose a domain $ \Omega \subset \mathbb{R^2} $ with $ \partial \Omega $. For $ f \in C^{\infty}(\mathbb{R^2}) $, the dirichlet problem is to find $ u $ with $ \Delta u = 0 $ in $ \Omega $, and $ f = u $ on $ \partial \Omega $. What are the existence and regularity theorems for classical solutions $ u $ in the case that $ \partial \Omega $ is not smooth, but only piecewise smooth? I am not sure exactly where to look for these results beyond just knowing the standard references, and I am a little overwhelmed by this. I would grateful if someone could put point me in a better direction.",,"['analysis', 'reference-request', 'partial-differential-equations']"
17,The difference between analysis and real analysis,The difference between analysis and real analysis,,"I do not know the difference between analysis and real analysis. When I study analysis, is it alright if I use the books on real analysis?","I do not know the difference between analysis and real analysis. When I study analysis, is it alright if I use the books on real analysis?",,"['real-analysis', 'analysis']"
18,Harmonic functions in $\mathbb{R}^d$,Harmonic functions in,\mathbb{R}^d,"I want to establish the equivalence of the 3 standard definitions, and that harmonic functions are $C^\infty$.  The 3 definitions are: Mean value property and continuous. $C^2$ and $0$ Laplacian. Mean value property on arbitrarily small balls and continuous. The only help I think I need here is proof of the existence and uniqueness of solution for the Dirichlet problem on the ball. (That would help me to get 3 implies 2, along with the maximal property for harmonic functions of type 3, which should be easy to prove.)  2 implies 1 by using arguments to take a derivative under the integral, and then using gauss' divergence theorem.  1 implies 3 obviously.  I also need help seeing that harmonic functions must be $C^\infty$.  I am not used to methods that don't involve complex analysis, as must be used here. I know that there is a theory of plurisubharmonic functions.  As a bonus question, do those tend to be useful outside of complex analysis, and are they ever discussed for odd dimension?  For example, I have never seen them discussed in harmonic analysis, nor do they seem to be useful in relation to Brownian motions, which is why I am learning the d-dimensional version of harmonic function theory now. Edit: Come to think of it, I'd also like some help proving the open mapping property and maximal properties for harmonic functions.  Please only assume definition 3 here, because I will use it and a connectedness argument to establish 3 implies 2.  The precise statement of definition 3 is as in Greene and Krantz but for d dimensions: $f$ is ""harmonic (3)"" if $f$ is continuous and $\forall x \in U$ the domain of $f$ there exists $\epsilon>0$ such that all balls of radius $\epsilon$ or less centered at $x$ are contained in U, and $f$ satisfies the MVP for that ball/spherical shell.","I want to establish the equivalence of the 3 standard definitions, and that harmonic functions are $C^\infty$.  The 3 definitions are: Mean value property and continuous. $C^2$ and $0$ Laplacian. Mean value property on arbitrarily small balls and continuous. The only help I think I need here is proof of the existence and uniqueness of solution for the Dirichlet problem on the ball. (That would help me to get 3 implies 2, along with the maximal property for harmonic functions of type 3, which should be easy to prove.)  2 implies 1 by using arguments to take a derivative under the integral, and then using gauss' divergence theorem.  1 implies 3 obviously.  I also need help seeing that harmonic functions must be $C^\infty$.  I am not used to methods that don't involve complex analysis, as must be used here. I know that there is a theory of plurisubharmonic functions.  As a bonus question, do those tend to be useful outside of complex analysis, and are they ever discussed for odd dimension?  For example, I have never seen them discussed in harmonic analysis, nor do they seem to be useful in relation to Brownian motions, which is why I am learning the d-dimensional version of harmonic function theory now. Edit: Come to think of it, I'd also like some help proving the open mapping property and maximal properties for harmonic functions.  Please only assume definition 3 here, because I will use it and a connectedness argument to establish 3 implies 2.  The precise statement of definition 3 is as in Greene and Krantz but for d dimensions: $f$ is ""harmonic (3)"" if $f$ is continuous and $\forall x \in U$ the domain of $f$ there exists $\epsilon>0$ such that all balls of radius $\epsilon$ or less centered at $x$ are contained in U, and $f$ satisfies the MVP for that ball/spherical shell.",,"['analysis', 'partial-differential-equations', 'harmonic-functions']"
19,Algebraic transformations to continuously extend functions,Algebraic transformations to continuously extend functions,,"Lately I was browsing through my analysis lecture notes (since right know I'm somewhat rusty in analysis) and the proof that $x \mapsto \frac{1}{x}$ is differentiable at every $x'\neq 0$ captured my attention. The easy proof is based on the following algebraic manipulation  $$\frac{\frac{1}{x+h}-\frac{1}{x}}{h}=\frac{1}{h}\frac{-h}{x^{2}+xh}=\frac{-1}{x^{2}+xh}.$$ Letting $h$ tend to zero, we get the limit that we sought. What aroused my interest was the idea that we can (continuously) extend a (continuous) function to a larger domain by simple algebraic manipulations (since in essence this is what we do, when calculating a derivative -- continuously extending a function): The second equality from above is key: By dividing by $h$ the domain changes from $\mathbb{R}\setminus \{0\}$ to $\mathbb{R}$ (if we consider the functions $h\mapsto \frac{1}{h}\frac{-h}{x^{2}+xh}$ and $h\mapsto \frac{-1}{x^{2}+xh}$, for $x\in \mathbb{R}\setminus \{0\}$). Questions: 1. Is there some general theory of ""algebraic transformations"" that allow the extension  of functions (even if it extends the functions only for one single point, as for in the case of calculating derivatives) ? (I don't know what algebraic geometry deals with, but to the uneducated ear it sounds like this would be it) 2. What other kinds of purely algebraic tricks, like above, do you know, that allow you (in the context of finding derivatives) to continuously extend functions $h\mapsto \frac{f(x+h)-f(x)}{h}$ ? [From the examples of my notes (and 2 books I browsed through) the only tricks seems to be: $  \quad$- Manipulating the numerator long enough, until you can factor an $h$ out, so that  you can write $\frac{f(x+h)-f(x)}{h}=\frac{h}{h}\cdot s_x(h)$ for some function $s_x$, since the functions $h\mapsto \frac{h}{h}$ is preventing you from continuously  extending $h\mapsto \frac{f(x+h)-f(x)}{h}$ from $\mathbb{R}\setminus \{0\}$ to $\mathbb{R}$ (and the function $h\mapsto \frac{h}{h}$ is trival to continuously extend to $\mathbb{R}$). $ \quad$- Writing $f$ as the product of $f=p_1 \cdot p_2$ and then adding and substracting again $p_1(x)\cdot p_2 (x+h)$ as in the proof that $(p_1 \cdot p_2)'=p_1' p_2 + p_1 p_2'$. (Estimating $\frac{f(x+h)-f(x)}{h}$ etc. - anything that does not involve pure algebraic transformations to extend it - I'm not interested in. Also, very simple algebraic manipulations, like rearranging terms as in the proof that $(\frac{1}{f})'=\frac{-f}{f^2}$, don't count.]","Lately I was browsing through my analysis lecture notes (since right know I'm somewhat rusty in analysis) and the proof that $x \mapsto \frac{1}{x}$ is differentiable at every $x'\neq 0$ captured my attention. The easy proof is based on the following algebraic manipulation  $$\frac{\frac{1}{x+h}-\frac{1}{x}}{h}=\frac{1}{h}\frac{-h}{x^{2}+xh}=\frac{-1}{x^{2}+xh}.$$ Letting $h$ tend to zero, we get the limit that we sought. What aroused my interest was the idea that we can (continuously) extend a (continuous) function to a larger domain by simple algebraic manipulations (since in essence this is what we do, when calculating a derivative -- continuously extending a function): The second equality from above is key: By dividing by $h$ the domain changes from $\mathbb{R}\setminus \{0\}$ to $\mathbb{R}$ (if we consider the functions $h\mapsto \frac{1}{h}\frac{-h}{x^{2}+xh}$ and $h\mapsto \frac{-1}{x^{2}+xh}$, for $x\in \mathbb{R}\setminus \{0\}$). Questions: 1. Is there some general theory of ""algebraic transformations"" that allow the extension  of functions (even if it extends the functions only for one single point, as for in the case of calculating derivatives) ? (I don't know what algebraic geometry deals with, but to the uneducated ear it sounds like this would be it) 2. What other kinds of purely algebraic tricks, like above, do you know, that allow you (in the context of finding derivatives) to continuously extend functions $h\mapsto \frac{f(x+h)-f(x)}{h}$ ? [From the examples of my notes (and 2 books I browsed through) the only tricks seems to be: $  \quad$- Manipulating the numerator long enough, until you can factor an $h$ out, so that  you can write $\frac{f(x+h)-f(x)}{h}=\frac{h}{h}\cdot s_x(h)$ for some function $s_x$, since the functions $h\mapsto \frac{h}{h}$ is preventing you from continuously  extending $h\mapsto \frac{f(x+h)-f(x)}{h}$ from $\mathbb{R}\setminus \{0\}$ to $\mathbb{R}$ (and the function $h\mapsto \frac{h}{h}$ is trival to continuously extend to $\mathbb{R}$). $ \quad$- Writing $f$ as the product of $f=p_1 \cdot p_2$ and then adding and substracting again $p_1(x)\cdot p_2 (x+h)$ as in the proof that $(p_1 \cdot p_2)'=p_1' p_2 + p_1 p_2'$. (Estimating $\frac{f(x+h)-f(x)}{h}$ etc. - anything that does not involve pure algebraic transformations to extend it - I'm not interested in. Also, very simple algebraic manipulations, like rearranging terms as in the proof that $(\frac{1}{f})'=\frac{-f}{f^2}$, don't count.]",,"['real-analysis', 'analysis', 'algebraic-geometry', 'derivatives']"
20,A question related to Wave Equation,A question related to Wave Equation,,"Let $L>0$. Suppose $f, g$ are $C^2$ functions on $\mathbb{R}$ such that  $$f(t)+f(-t)+\int_{-t}^t g(s)\,ds=0$$ and  $$f(L+t)+f(L-t)+\int_{L-t}^{L+t} g(s)\,ds=0$$ for all $t\in \mathbb{R}.$ Does it follow that $f, g$ are odd periodic functions of period $2L$?","Let $L>0$. Suppose $f, g$ are $C^2$ functions on $\mathbb{R}$ such that  $$f(t)+f(-t)+\int_{-t}^t g(s)\,ds=0$$ and  $$f(L+t)+f(L-t)+\int_{L-t}^{L+t} g(s)\,ds=0$$ for all $t\in \mathbb{R}.$ Does it follow that $f, g$ are odd periodic functions of period $2L$?",,"['analysis', 'partial-differential-equations', 'fourier-series']"
21,Inequality concerning nonnegative numbers (related to Hanner's inequalities),Inequality concerning nonnegative numbers (related to Hanner's inequalities),,"I've been having a look at how Lieb and Loss (in their textbook on Analysis) prove Hanner's inequalities and have been trying to get a handle of the geometric intuition involved. In doing so I've been trying to prove that: $(a+b)^p + (a-b)^p \geq 2a^p +p(p-1)a^{p-2}b^2$ For any $1\leq p\leq 2$ and $0<b<a$. This is exercise 4 in chapter two of the aforementioned book. Can anyone give me any tips (or some geometric intuition)? Also can anyone give me any explanation of why uniform convexity is important, and what it might mean geometrically? Edit: for the latter question the following is useful but I would appreciate any more comments. https://mathoverflow.net/questions/27691/hanners-inequalities-the-intuition-behind-them","I've been having a look at how Lieb and Loss (in their textbook on Analysis) prove Hanner's inequalities and have been trying to get a handle of the geometric intuition involved. In doing so I've been trying to prove that: $(a+b)^p + (a-b)^p \geq 2a^p +p(p-1)a^{p-2}b^2$ For any $1\leq p\leq 2$ and $0<b<a$. This is exercise 4 in chapter two of the aforementioned book. Can anyone give me any tips (or some geometric intuition)? Also can anyone give me any explanation of why uniform convexity is important, and what it might mean geometrically? Edit: for the latter question the following is useful but I would appreciate any more comments. https://mathoverflow.net/questions/27691/hanners-inequalities-the-intuition-behind-them",,"['real-analysis', 'analysis']"
22,Uniqueness of Bounded Solutions to Dirichlet's Problem in the Half-Space,Uniqueness of Bounded Solutions to Dirichlet's Problem in the Half-Space,,"Title basically says everything.  Prove that if $u\in C^{2}(\mathbb{R}^{n}_{+})\cap C(\bar{\mathbb{R}^{n}_{+}})$ is a bounded solution of the BVP $$\left\{\begin{array} -\Delta u=0&\text{in}\;\mathbb{R}^{n}_{+}\\ u=g&\;\text{on}\;\partial\mathbb{R}^{n}_{+}, \end{array}\right.$$ then it is unique. Various tools I have in mind are maximum principle, mean value formulas, Liouville's theorem, ""energy"" functionals, and Harnack's inequality, uniqueness of Green's function, Hopf's lemma, etc....but in all my scratch work to prove the problem, I keep running into technical difficulties in working with the boundary at infinity. I arrived at a proof by using a result from Evans exercise #2.5.10 (Schwarz reflection principle): Proof. Consider the ball $B_{R}(0)$ and suppose $u_{R}$ is a bounded solution to the above problem, but posed on the domain $B_{R}^{+}(0):=\{x:x\in B_{R}(0), x_{n}>0\}.$  Let $v_{R}$ be another bounded solution and define $w_{R}:v_{R}-u_{R}.$  Then $w_{R}=0$ on $\partial B^{+}_{R}(0)\cap\{x_{n}=0\}$, and the Schwarz reflection principle states that the odd extension of $w_{R}$ to $B^{-}_{R}(0)$ is harmonic in all of $B_{R}(0)$ (the proof of this is trivial if we assume $w\in C^{2}(\bar{B^{+}_{R}(0)})$ by using the mean-value formulas, and only a little more difficult under the present assumptions by using Poisson's formula for the ball).  Now, $u_{R},v_{R}$ both being bounded implies $w_{R}$ is also bounded.  Sending $R\to\infty$, we find that $w:=\lim_{R\to\infty}w_{R}$ is a bounded and harmonic in $\mathbb{R}^{n}$, from which it follows $w_{R}\equiv0$ by Liouville's theorem and the fact that $w=0$ on the hyperplane $x_{n}=0.$ Let $u_{1}$ be a bounded solution to the problem above.  Suppose $u_{2}$ is another solution and define $$w:=u_{1}-u_{2}.$$  Then $w=0$ on $\partial\mathbb{R}^{n}_{+}.$","Title basically says everything.  Prove that if $u\in C^{2}(\mathbb{R}^{n}_{+})\cap C(\bar{\mathbb{R}^{n}_{+}})$ is a bounded solution of the BVP $$\left\{\begin{array} -\Delta u=0&\text{in}\;\mathbb{R}^{n}_{+}\\ u=g&\;\text{on}\;\partial\mathbb{R}^{n}_{+}, \end{array}\right.$$ then it is unique. Various tools I have in mind are maximum principle, mean value formulas, Liouville's theorem, ""energy"" functionals, and Harnack's inequality, uniqueness of Green's function, Hopf's lemma, etc....but in all my scratch work to prove the problem, I keep running into technical difficulties in working with the boundary at infinity. I arrived at a proof by using a result from Evans exercise #2.5.10 (Schwarz reflection principle): Proof. Consider the ball $B_{R}(0)$ and suppose $u_{R}$ is a bounded solution to the above problem, but posed on the domain $B_{R}^{+}(0):=\{x:x\in B_{R}(0), x_{n}>0\}.$  Let $v_{R}$ be another bounded solution and define $w_{R}:v_{R}-u_{R}.$  Then $w_{R}=0$ on $\partial B^{+}_{R}(0)\cap\{x_{n}=0\}$, and the Schwarz reflection principle states that the odd extension of $w_{R}$ to $B^{-}_{R}(0)$ is harmonic in all of $B_{R}(0)$ (the proof of this is trivial if we assume $w\in C^{2}(\bar{B^{+}_{R}(0)})$ by using the mean-value formulas, and only a little more difficult under the present assumptions by using Poisson's formula for the ball).  Now, $u_{R},v_{R}$ both being bounded implies $w_{R}$ is also bounded.  Sending $R\to\infty$, we find that $w:=\lim_{R\to\infty}w_{R}$ is a bounded and harmonic in $\mathbb{R}^{n}$, from which it follows $w_{R}\equiv0$ by Liouville's theorem and the fact that $w=0$ on the hyperplane $x_{n}=0.$ Let $u_{1}$ be a bounded solution to the problem above.  Suppose $u_{2}$ is another solution and define $$w:=u_{1}-u_{2}.$$  Then $w=0$ on $\partial\mathbb{R}^{n}_{+}.$",,"['analysis', 'partial-differential-equations']"
23,Continuous Coercive Functional not Bounded Below,Continuous Coercive Functional not Bounded Below,,"Let $X$ be a infinite dimensional Banach space. How to construct a example of a continuous function $f:X\rightarrow\mathbb{R}$ such that $f$ is coercive, but is not bounded below. $f$ is coercive if $\|u\|\rightarrow \infty$ then $f(u)\rightarrow\infty$ $f$ is bounded below if there exist a constant $C$ such that $f(u)\geq C$ for all $u\in X$","Let $X$ be a infinite dimensional Banach space. How to construct a example of a continuous function $f:X\rightarrow\mathbb{R}$ such that $f$ is coercive, but is not bounded below. $f$ is coercive if $\|u\|\rightarrow \infty$ then $f(u)\rightarrow\infty$ $f$ is bounded below if there exist a constant $C$ such that $f(u)\geq C$ for all $u\in X$",,"['analysis', 'banach-spaces']"
24,Viscosity Solutions,Viscosity Solutions,,"I am looking for the references or simple direct proofs of existence and uniqueness of viscosity solution for two problems: $1.$ Let $\Omega$ be bounded domain, $u=0$ on $\delta\Omega$ and $$|Du|-f(x,u)=0$$ where $f\ge 0$ and $f(x,r)<f(x,s)$ for all $x\in \Omega$ and $r<s.$ $2.$ $-D\cdot a(Du)=f$ where $a$ is a smooth vector-valued function that satisfies monotonicity condition $(a(Du)-a(Dv))\cdot (Du-Dv)\ge 0.$ We can assume whatever we need for the domain in $R^n,$ $u=0$ on the boundary.","I am looking for the references or simple direct proofs of existence and uniqueness of viscosity solution for two problems: $1.$ Let $\Omega$ be bounded domain, $u=0$ on $\delta\Omega$ and $$|Du|-f(x,u)=0$$ where $f\ge 0$ and $f(x,r)<f(x,s)$ for all $x\in \Omega$ and $r<s.$ $2.$ $-D\cdot a(Du)=f$ where $a$ is a smooth vector-valued function that satisfies monotonicity condition $(a(Du)-a(Dv))\cdot (Du-Dv)\ge 0.$ We can assume whatever we need for the domain in $R^n,$ $u=0$ on the boundary.",,"['analysis', 'partial-differential-equations']"
25,total variation of a measure,total variation of a measure,,I ran across this question in my analysis textbook.  I just cannot prove this. Suppose $\mu$ is a complex measure on $X$ such that $\mu(X)=1$ and $|\mu|(X)=1$.  Show $\mu$ is a positive real measure.,I ran across this question in my analysis textbook.  I just cannot prove this. Suppose $\mu$ is a complex measure on $X$ such that $\mu(X)=1$ and $|\mu|(X)=1$.  Show $\mu$ is a positive real measure.,,['analysis']
26,Equivalent definition of lower semi-continuity,Equivalent definition of lower semi-continuity,,"I have some problems about solving an exercise: Prove that one function $f \colon [a,b] \to \mathbb{R}$ is lower semi-continuous if and only if, for all $x \in [a,b]$, we have $$f(x)=\sup \{g(x) \mid g \in C[a,b] \text{ and } g \le f  \text{ over } [a,b] \}\;.$$ Assuming true that formula, I had no problems showing that $f^{-1}((t,\infty \ ])$ is open, using the property of supremum and the continuity of $g$'s. I have difficulties proving the opposite implication. Beacuse $f \ge g$, we have that $f(x) \ge \sup{g(x)}$, but I am not able to show the other inequality. Thank you.","I have some problems about solving an exercise: Prove that one function $f \colon [a,b] \to \mathbb{R}$ is lower semi-continuous if and only if, for all $x \in [a,b]$, we have $$f(x)=\sup \{g(x) \mid g \in C[a,b] \text{ and } g \le f  \text{ over } [a,b] \}\;.$$ Assuming true that formula, I had no problems showing that $f^{-1}((t,\infty \ ])$ is open, using the property of supremum and the continuity of $g$'s. I have difficulties proving the opposite implication. Beacuse $f \ge g$, we have that $f(x) \ge \sup{g(x)}$, but I am not able to show the other inequality. Thank you.",,"['real-analysis', 'analysis']"
27,Inequality for Fourier transform of measure,Inequality for Fourier transform of measure,,I am having trouble with the following question. Let $\mu$ be finite measure on $\mathbb{R}$ and let  $\hat{\mu}(\xi) = \int_{-\infty}^\infty e^{-ix \xi} d\mu(x)$ be its Fourier transform. Prove that $$|\mu(\{x\})| \le \limsup_{|\xi| \rightarrow \infty} |\hat{\mu}(\xi)|$$,I am having trouble with the following question. Let $\mu$ be finite measure on $\mathbb{R}$ and let  $\hat{\mu}(\xi) = \int_{-\infty}^\infty e^{-ix \xi} d\mu(x)$ be its Fourier transform. Prove that $$|\mu(\{x\})| \le \limsup_{|\xi| \rightarrow \infty} |\hat{\mu}(\xi)|$$,,"['analysis', 'measure-theory', 'fourier-analysis', 'limsup-and-liminf']"
28,Continuity of $f\circ f \circ \cdots$,Continuity of,f\circ f \circ \cdots,"Let $f:D\to D$, where $D\subseteq \mathbb{R}^n$, be a continuous function. Under what conditions is $f\circ f \circ \cdots$ continuous? Here, $\circ$ stands for the composition operator and sometimes the notation $f^2=f\circ f$ is used. So in this notation, when is $\lim_n f^n$ continuous? There is a counterexample I can think of: $f(t)=t^\alpha$ over $D=[0,1]$ for $\alpha>1$. Then $f^n(t)=t^{\alpha^n}$ and the point-wise limit of $f^n$ is $\left(\lim_n f^n\right)(t)=0$ for $t\in[0,1)$ and $\left(\lim_n f^n\right)(1)=1$ which is not continuous. One think one could suggest is that $f^n$ be a uniformly convergent sequence of functions. But what should this imply for $f$?","Let $f:D\to D$, where $D\subseteq \mathbb{R}^n$, be a continuous function. Under what conditions is $f\circ f \circ \cdots$ continuous? Here, $\circ$ stands for the composition operator and sometimes the notation $f^2=f\circ f$ is used. So in this notation, when is $\lim_n f^n$ continuous? There is a counterexample I can think of: $f(t)=t^\alpha$ over $D=[0,1]$ for $\alpha>1$. Then $f^n(t)=t^{\alpha^n}$ and the point-wise limit of $f^n$ is $\left(\lim_n f^n\right)(t)=0$ for $t\in[0,1)$ and $\left(\lim_n f^n\right)(1)=1$ which is not continuous. One think one could suggest is that $f^n$ be a uniformly convergent sequence of functions. But what should this imply for $f$?",,['analysis']
29,How to solve $y=\frac{(x-\sin x)}{ (1-\cos x)}$,How to solve,y=\frac{(x-\sin x)}{ (1-\cos x)},"I only see numerical approaches to solve this equation. Is there an analytical solution to solve $x$ as a function of $y$ for the range $(0,2 \pi)$? If there is no solution, is it possible to proof it?","I only see numerical approaches to solve this equation. Is there an analytical solution to solve $x$ as a function of $y$ for the range $(0,2 \pi)$? If there is no solution, is it possible to proof it?",,['analysis']
30,The set of diffeomorphisms preserving some metric.,The set of diffeomorphisms preserving some metric.,,"Let $M$ be a finite-dimensional, smooth manifold. Call a diffeomorphism $f : M \rightarrow M$ diagonalizable if there exists a Riemannian metric $g$ on $M$ such that $f : (M, g) \rightarrow (M, g)$ is an isometry. I have some questions regarding such objects. a) Is the set Diag(M) of all diagonalizable diffeomorphisms a group under composition? b) Note that, in order to be diagonalizable, a diffeomorphism must possess the following well-known property of isometries: $$\text{If}~f(p) = p~\text{and}~df(p) = \mathrm{Id}_{T_pM}, \text{for some $p \in M$, then}~f = \mathrm{Id}_M. (*)$$ Is $(*)$ also a sufficient condition? In other words, given a diffeomorphism $f \in \mathrm{Diff}(M)$ satisfying $(*)$, is there a Riemannian metric for which $f$ is an isometry? Maybe this is too general, because any diffeomorphism not fixing any point satisfies $(*)$, but I don't know the answer. My motivation here is to know how large is the set of diffeomorphisms that could be isometries within the set of all diffeomorphisms. I apologize if I'm missing some standard notation and/or vocabulary here. I'd appreciate, as always, any references. Thanks in advance.","Let $M$ be a finite-dimensional, smooth manifold. Call a diffeomorphism $f : M \rightarrow M$ diagonalizable if there exists a Riemannian metric $g$ on $M$ such that $f : (M, g) \rightarrow (M, g)$ is an isometry. I have some questions regarding such objects. a) Is the set Diag(M) of all diagonalizable diffeomorphisms a group under composition? b) Note that, in order to be diagonalizable, a diffeomorphism must possess the following well-known property of isometries: $$\text{If}~f(p) = p~\text{and}~df(p) = \mathrm{Id}_{T_pM}, \text{for some $p \in M$, then}~f = \mathrm{Id}_M. (*)$$ Is $(*)$ also a sufficient condition? In other words, given a diffeomorphism $f \in \mathrm{Diff}(M)$ satisfying $(*)$, is there a Riemannian metric for which $f$ is an isometry? Maybe this is too general, because any diffeomorphism not fixing any point satisfies $(*)$, but I don't know the answer. My motivation here is to know how large is the set of diffeomorphisms that could be isometries within the set of all diffeomorphisms. I apologize if I'm missing some standard notation and/or vocabulary here. I'd appreciate, as always, any references. Thanks in advance.",,"['analysis', 'differential-geometry', 'riemannian-geometry']"
31,Showing properties of discontinuous points of a strictly increasing function [duplicate],Showing properties of discontinuous points of a strictly increasing function [duplicate],,"This question already has answers here : How to show that a set of discontinuous points of an increasing function is at most countable (4 answers) Closed 2 years ago . Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be strictly monotonically increasing. (i) Is $f$ not continuous at $p \in \mathbb{R}$, there exists a non-empty, open interval $(a_p, b_p) \subset \mathbb{R}$ such that $f(x)\leq a_p$ for all $x < p$ and $f(x) \geq b_p$ for all $x > p$. (ii) The set of discontinuity points $$ \{ p \in \mathbb{R} | f \; \mbox{is not continuous at} \; p \}$$ is countable. For (i) I could be way off, but I am picturing a graph with $p$ on the $x$-axis for which the value $f(p)$ on the $y$-axis is undefined. Am I correct to interpret the open interval $(a_p, b_p)$ as an interval on the $y$-axis which should be contained within the distance between two $f(x)$'s (one for $x<p$ and the other for $x>p$)? If that is so far correct, there could be 2 types of discontinuous points $p$, a jump or removable type. For the jump it would be easier to show that somehow the interval $(a_p, b_p)$ is smaller than the vertical jump... For a removable discontinuity $c$, I would think that the interval could contain just the $y$-axis value $\displaystyle \lim_{x\to c}f(x)$ but I don't really know how to express that the upper and lower bounds would be just above and below that... With (ii) I am currently trying to understand a proof, what exactly does the notation $f(p-)$ or $f(p+)$ mean in this context? Is it simply the value when approached from the left or the right(respectively)?","This question already has answers here : How to show that a set of discontinuous points of an increasing function is at most countable (4 answers) Closed 2 years ago . Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be strictly monotonically increasing. (i) Is $f$ not continuous at $p \in \mathbb{R}$, there exists a non-empty, open interval $(a_p, b_p) \subset \mathbb{R}$ such that $f(x)\leq a_p$ for all $x < p$ and $f(x) \geq b_p$ for all $x > p$. (ii) The set of discontinuity points $$ \{ p \in \mathbb{R} | f \; \mbox{is not continuous at} \; p \}$$ is countable. For (i) I could be way off, but I am picturing a graph with $p$ on the $x$-axis for which the value $f(p)$ on the $y$-axis is undefined. Am I correct to interpret the open interval $(a_p, b_p)$ as an interval on the $y$-axis which should be contained within the distance between two $f(x)$'s (one for $x<p$ and the other for $x>p$)? If that is so far correct, there could be 2 types of discontinuous points $p$, a jump or removable type. For the jump it would be easier to show that somehow the interval $(a_p, b_p)$ is smaller than the vertical jump... For a removable discontinuity $c$, I would think that the interval could contain just the $y$-axis value $\displaystyle \lim_{x\to c}f(x)$ but I don't really know how to express that the upper and lower bounds would be just above and below that... With (ii) I am currently trying to understand a proof, what exactly does the notation $f(p-)$ or $f(p+)$ mean in this context? Is it simply the value when approached from the left or the right(respectively)?",,['analysis']
32,Upper semicontinuity of sequence of Hausdorff measures,Upper semicontinuity of sequence of Hausdorff measures,,"This is exercise 12.2 of Measure theory and integration of Michael Taylor. Let $(d_j)$ be a sequence of metrics on a compact space $X$ such that there exists $c,C>0$ with $$ cd_0(x,y) \leq d_j(x,y) \leq Cd_0(x,y) $$ for all $x,y\in X$ . Suppose also that $(d_j)$ converge uniformly to a metric $d$ . I want to show that, for any $S$ Borel subset, $\delta > 0$ and $\varepsilon > 0$ , $$ \mathcal{H}_{d_j,\lambda\delta}^r(S) \leq \mathcal{H}_{d, \delta}^r(S) + \varepsilon$$ with $j$ big enough and $\lambda = C/c$ and $$ \mathcal{H}^r_{d,\delta}(S) = \inf\{ \sum (\operatorname{diam}_d U_i)^r \mid S \subset \cup U_i, \operatorname{diam} U_i \leq \delta \} $$ the approximate $\delta$ -Hausdorff measure. However when I try I always get a factor $(c/C)^r$ and I don't know how to get ride of it as it is not clear if we can rescale our cover. $$ \begin{aligned} \mathcal{H}^r_{d,\delta}(S) + \varepsilon &\geq \sum (\operatorname{diam}_d U_i)^r \\ &\geq \sum \left( \frac{c}{C} \operatorname{diam}_{d_j} U_i\right)^r \end{aligned} $$","This is exercise 12.2 of Measure theory and integration of Michael Taylor. Let be a sequence of metrics on a compact space such that there exists with for all . Suppose also that converge uniformly to a metric . I want to show that, for any Borel subset, and , with big enough and and the approximate -Hausdorff measure. However when I try I always get a factor and I don't know how to get ride of it as it is not clear if we can rescale our cover.","(d_j) X c,C>0  cd_0(x,y) \leq d_j(x,y) \leq Cd_0(x,y)  x,y\in X (d_j) d S \delta > 0 \varepsilon > 0  \mathcal{H}_{d_j,\lambda\delta}^r(S) \leq \mathcal{H}_{d, \delta}^r(S) + \varepsilon j \lambda = C/c  \mathcal{H}^r_{d,\delta}(S) = \inf\{ \sum (\operatorname{diam}_d U_i)^r \mid S \subset \cup U_i, \operatorname{diam} U_i \leq \delta \}  \delta (c/C)^r 
\begin{aligned}
\mathcal{H}^r_{d,\delta}(S) + \varepsilon &\geq
\sum (\operatorname{diam}_d U_i)^r \\
&\geq \sum \left( \frac{c}{C} \operatorname{diam}_{d_j} U_i\right)^r
\end{aligned}
","['real-analysis', 'analysis', 'measure-theory', 'metric-spaces', 'hausdorff-measure']"
33,"If $f\in L^1(\Bbb R)$, what is $\lim_{t\to\infty}\int_{-\infty}^\infty |f(x-t)-f(x)|\, dx = ?$","If , what is","f\in L^1(\Bbb R) \lim_{t\to\infty}\int_{-\infty}^\infty |f(x-t)-f(x)|\, dx = ?","Given $f\in L^1(\Bbb R)$ , compute the limit $$\lim_{t\to\infty}\int_{-\infty}^\infty |f(x-t)-f(x)|\, dx.$$ My work. Of course, $$\lim_{t\to\infty}\int_{-\infty}^\infty |f(x-t)-f(x)|\, dx \le 2\|f\|_1 < \infty.$$ We know that step functions are dense in $L^1(\Bbb R)$ . My first thought is to find an expression for $f = \mathbf{1}_{(a,b)}$ , i.e., the indicator function of an interval. For simplicity, take $a = 0, b= 1$ . Then, $$ \begin{align*} \int_{-\infty}^\infty |f(x-t)-f(x)|\, dx &= \int_0^1 1\, dx + \int_t^{t+1} 1\, dx = 2 = 2\|f\|_1  \end{align*}$$ as $\|f\|_1 = 1$ . It is easy to see that a similar equality holds for $f = \mathbf{1}_{(a,b)}$ in general. If I can show that $$\lim_{t\to\infty}\int_{-\infty}^\infty |f(x-t)-f(x)|\, dx = 2\|f\|_1$$ for functions $f = \sum_{i=1}^n a_i \mathbf{1}_{E_i}$ where $E_i$ 's are disjoint intervals, then I can hope to use an approximation argument for the result. Suppose the result holds for all step functions. Then, given $f\in L^1(\Bbb R)$ , there is a sequence $\{f_n\}_{n\ge 1}$ of step functions such that $\|f - f_n\|_1 \to 0$ . We have $$\lim_{t\to\infty}\int_{-\infty}^\infty |f_n(x-t)-f_n(x)|\, dx = 2\|f_n\|_1$$ for every $n\ge 1$ . Certainly, $\|f_n\|_1 \to \|f\|_1$ as $n\to \infty$ . The hard part is showing that $$\lim_{n\to\infty} \lim_{t\to\infty}\int_{-\infty}^\infty |f_n(x-t)-f_n(x)|\, dx = \lim_{t\to\infty}\int_{-\infty}^\infty |f(x-t)-f(x)|\, dx.$$ Thanks for your help!","Given , compute the limit My work. Of course, We know that step functions are dense in . My first thought is to find an expression for , i.e., the indicator function of an interval. For simplicity, take . Then, as . It is easy to see that a similar equality holds for in general. If I can show that for functions where 's are disjoint intervals, then I can hope to use an approximation argument for the result. Suppose the result holds for all step functions. Then, given , there is a sequence of step functions such that . We have for every . Certainly, as . The hard part is showing that Thanks for your help!","f\in L^1(\Bbb R) \lim_{t\to\infty}\int_{-\infty}^\infty |f(x-t)-f(x)|\, dx. \lim_{t\to\infty}\int_{-\infty}^\infty |f(x-t)-f(x)|\, dx \le 2\|f\|_1 < \infty. L^1(\Bbb R) f = \mathbf{1}_{(a,b)} a = 0, b= 1 
\begin{align*}
\int_{-\infty}^\infty |f(x-t)-f(x)|\, dx &= \int_0^1 1\, dx + \int_t^{t+1} 1\, dx = 2 = 2\|f\|_1 
\end{align*} \|f\|_1 = 1 f = \mathbf{1}_{(a,b)} \lim_{t\to\infty}\int_{-\infty}^\infty |f(x-t)-f(x)|\, dx = 2\|f\|_1 f = \sum_{i=1}^n a_i \mathbf{1}_{E_i} E_i f\in L^1(\Bbb R) \{f_n\}_{n\ge 1} \|f - f_n\|_1 \to 0 \lim_{t\to\infty}\int_{-\infty}^\infty |f_n(x-t)-f_n(x)|\, dx = 2\|f_n\|_1 n\ge 1 \|f_n\|_1 \to \|f\|_1 n\to \infty \lim_{n\to\infty} \lim_{t\to\infty}\int_{-\infty}^\infty |f_n(x-t)-f_n(x)|\, dx = \lim_{t\to\infty}\int_{-\infty}^\infty |f(x-t)-f(x)|\, dx.","['real-analysis', 'analysis']"
34,If $f:\mathbb{R}\to\mathbb{R}$ is strictly increasing and convex in $\mathbb{R}$ prove $\lim_{x\to\infty} f(x)=\infty$,If  is strictly increasing and convex in  prove,f:\mathbb{R}\to\mathbb{R} \mathbb{R} \lim_{x\to\infty} f(x)=\infty,"The way I did it was using the definition of my highschool book for convexity (which is not a definition and more of a criterion) which says that: If $f$ is continuous on a closed interval $I$ and if $f'$ is strictly increasing in the open interval $I$ then $f$ is convex in all of $I$ so when asked this question in the context of my class I had to answer like so: $f$ convex in $\mathbb{R}$ iff $f'$ strictly increasing in $\mathbb{R}$ and since $f$ is strictly increasing we have $f'(x)\geq0$ $\forall x \in \mathbb{R}$ and $\exists x_0$ such that $f'(x_0)>0$ . Finding the tangent line at $x_0$ gives us: $y=xf'(x_0)-x_0f'(x_0)+f(x_0)$ Since $f$ is convex, $f$ is above its tangent line everywhere except on $x_0$ so we get $$f(x)\geq xf'(x_0)-x_0f'(x_0)+f(x_0)\quad \forall x \in\mathbb{R}$$ Also $$\lim_{x\to\infty}[xf'(x_0)-x_0f'(x_0)+f(x_0)]=\lim_{x\to\infty}xf'(x_0)=\infty $$ Which implies $$\lim_{x\to\infty}f(x)=\infty$$ This approach is also mentioned in this MSE post while also using the Mean Value Theorem which yearns a similar result. What I'm looking for is other more rigorous ways of proving this, using any theorem and any definition from real analysis or anywhere else. Since the actual definition of convexity doesn't require differentiability I'm very interested to see other ways to approach this and since we don't use the $\varepsilon$ - $\delta$ definition of limits I would imagine a rigorous proof would look very different.","The way I did it was using the definition of my highschool book for convexity (which is not a definition and more of a criterion) which says that: If is continuous on a closed interval and if is strictly increasing in the open interval then is convex in all of so when asked this question in the context of my class I had to answer like so: convex in iff strictly increasing in and since is strictly increasing we have and such that . Finding the tangent line at gives us: Since is convex, is above its tangent line everywhere except on so we get Also Which implies This approach is also mentioned in this MSE post while also using the Mean Value Theorem which yearns a similar result. What I'm looking for is other more rigorous ways of proving this, using any theorem and any definition from real analysis or anywhere else. Since the actual definition of convexity doesn't require differentiability I'm very interested to see other ways to approach this and since we don't use the - definition of limits I would imagine a rigorous proof would look very different.",f I f' I f I f \mathbb{R} f' \mathbb{R} f f'(x)\geq0 \forall x \in \mathbb{R} \exists x_0 f'(x_0)>0 x_0 y=xf'(x_0)-x_0f'(x_0)+f(x_0) f f x_0 f(x)\geq xf'(x_0)-x_0f'(x_0)+f(x_0)\quad \forall x \in\mathbb{R} \lim_{x\to\infty}[xf'(x_0)-x_0f'(x_0)+f(x_0)]=\lim_{x\to\infty}xf'(x_0)=\infty  \lim_{x\to\infty}f(x)=\infty \varepsilon \delta,"['calculus', 'analysis', 'convex-analysis']"
35,"how to prove volume is less or equal the outer measure of any closed interval, $v(I)\leq m^*(I)$","how to prove volume is less or equal the outer measure of any closed interval,",v(I)\leq m^*(I),"Exercise 26: Let $$ I=\left\{(x, y) \in \mathbb{R}^2 \mid a \leq x \leq b, c \leq y \leq d\right\} $$ be a closed interval in $\mathbb{R}^2$ . Let $$ \begin{aligned} &a=a_0<a_1<\ldots<a_m=b \quad \text { and } \\ &c=c_0<c_1<\ldots<c_n=d . \end{aligned} $$ For $i=1,2, \ldots, m$ and $j=1,2, \ldots, n$ , define the rectangle $I_{i j}$ by $$ I_{i j}=\left\{(x, y) \in \mathbb{R}^2 \mid a_{i-1} \leq x \leq a_i, c_{j-1} \leq y \leq c_j\right\} . $$ (This can be thought of as subdividing $I$ into subrectangles along the vertical lines $x=a_1, x=a_2, \ldots, x=a_{m-1}$ and the horizontal lines $y=c_1, y=c_2, \ldots, y=c_{n-1}$ .) Using the definition of volume, prove $$ \sum_{i=1}^m \sum_{j=1}^n v\left(I_{i j}\right)=v(I) . $$ Exercise 27: Let $$I=\left\{(x, y) \in \mathbb{R}^2 \mid a \leq x \leq b, c \leq y \leq d\right\}$$ be a closed interval in $\mathbb R^2$ . Let $J_1, J_2, \ldots, J_n$ be a finite collection of closed intervals that cover $I$ . That is, $$ I \subseteq \bigcup_{k=1}^n J_k . $$ By carefully subdividing $I$ and the $J_k$ 's into subrectangles, and use the previous exercise to show that $$ v(I) \leq \sum_{k=1}^n v\left(J_k\right) . $$ I start with Exercise 26 (thanks to @Balajisb pointing out the telescoping sum), $$ \begin{align} \sum_{i=1}^m \sum_{j=1}^n v\left(I_{i j}\right)&=\sum_{i=1}^m (v(I_{i1})+v(I_{i2})+\cdots+v(I_{in}))\\ &= \sum_{i=1}^m (a_i-a_{i-1}) ((c_1-c_0)+\cdots+(c_n-c_{n-1}))\\ &= ((c_1-c_0)+\cdots+(c_n-c_{n-1})) \sum_{i=1}^m (a_i-a_{i-1})\\ &= (c_n-c_0) ((a_1-a_0)+\cdots+(a_m-a_{m-1})) \\ &= (d-c)(b-a) \\ &= v(I) \end{align} $$ Now, from here I couldn't see how exercise 26 can be used in exercise 27. What they mean by ""By carefully subdividing $I$ and the $J_k$ 's into subrectangles..."" ? Any help will be appreciated. TIA.","Exercise 26: Let be a closed interval in . Let For and , define the rectangle by (This can be thought of as subdividing into subrectangles along the vertical lines and the horizontal lines .) Using the definition of volume, prove Exercise 27: Let be a closed interval in . Let be a finite collection of closed intervals that cover . That is, By carefully subdividing and the 's into subrectangles, and use the previous exercise to show that I start with Exercise 26 (thanks to @Balajisb pointing out the telescoping sum), Now, from here I couldn't see how exercise 26 can be used in exercise 27. What they mean by ""By carefully subdividing and the 's into subrectangles..."" ? Any help will be appreciated. TIA.","
I=\left\{(x, y) \in \mathbb{R}^2 \mid a \leq x \leq b, c \leq y \leq d\right\}
 \mathbb{R}^2 
\begin{aligned}
&a=a_0<a_1<\ldots<a_m=b \quad \text { and } \\
&c=c_0<c_1<\ldots<c_n=d .
\end{aligned}
 i=1,2, \ldots, m j=1,2, \ldots, n I_{i j} 
I_{i j}=\left\{(x, y) \in \mathbb{R}^2 \mid a_{i-1} \leq x \leq a_i, c_{j-1} \leq y \leq c_j\right\} .
 I x=a_1, x=a_2, \ldots, x=a_{m-1} y=c_1, y=c_2, \ldots, y=c_{n-1} 
\sum_{i=1}^m \sum_{j=1}^n v\left(I_{i j}\right)=v(I) .
 I=\left\{(x, y) \in \mathbb{R}^2 \mid a \leq x \leq b, c \leq y \leq d\right\} \mathbb R^2 J_1, J_2, \ldots, J_n I 
I \subseteq \bigcup_{k=1}^n J_k .
 I J_k 
v(I) \leq \sum_{k=1}^n v\left(J_k\right) .
 
\begin{align}
\sum_{i=1}^m \sum_{j=1}^n v\left(I_{i j}\right)&=\sum_{i=1}^m (v(I_{i1})+v(I_{i2})+\cdots+v(I_{in}))\\
&= \sum_{i=1}^m (a_i-a_{i-1}) ((c_1-c_0)+\cdots+(c_n-c_{n-1}))\\
&= ((c_1-c_0)+\cdots+(c_n-c_{n-1})) \sum_{i=1}^m (a_i-a_{i-1})\\
&= (c_n-c_0) ((a_1-a_0)+\cdots+(a_m-a_{m-1})) \\
&= (d-c)(b-a) \\
&= v(I)
\end{align}
 I J_k","['analysis', 'measure-theory', 'lebesgue-measure', 'volume']"
36,Textbook for graduate analysis over a general complete valued field?,Textbook for graduate analysis over a general complete valued field?,,"At my university, the standard sequence of first-year graduate analysis courses is taken by students who plan to go into analysis, differential equations, and applied math. These students know why they have to learn analysis. The first-year graduate analysis courses are also taken by students who plan to go into algebraic topology, algebraic number theory, and algebraic geometry. These students sometimes are disinterested in learning classical analysis (over the real or complex numbers), because their perspective is not yet broad enough to see the fundamental importance of analysis and/or how it is related to the subjects they plan to specialize in. My efforts to explain to these students why they ought to care about analysis have generally not been successful. My colleagues and I are considering the possibility of running our first-year graduate analysis courses as courses in analysis over a general complete valued field. Over the archimedean fields, we would recover classical analysis over the real and complex numbers; over the non-archimedean fields, we would recover many cases of interest to the students who plan to study algebra, particularly analysis over p-adic fields. We hope the resulting courses would be of interest and of use to all our students. But it is not yet clear to us that this idea makes sense; perhaps some topics (e.g. integration theory) are simply too different in the non-archimedean case to treat them, at this level, alongside the classical treatment. There are many good references on functional analysis which work over non-archimedean fields, or which allow the base complete valued field to be archimedean or non-archimedean. However, I do not know of a graduate analysis textbook (not just functional analysis, but rather roughly the contents of a standard first year of graduate analysis) which works over a general complete valued field. Do you know of any such textbook? Thanks.","At my university, the standard sequence of first-year graduate analysis courses is taken by students who plan to go into analysis, differential equations, and applied math. These students know why they have to learn analysis. The first-year graduate analysis courses are also taken by students who plan to go into algebraic topology, algebraic number theory, and algebraic geometry. These students sometimes are disinterested in learning classical analysis (over the real or complex numbers), because their perspective is not yet broad enough to see the fundamental importance of analysis and/or how it is related to the subjects they plan to specialize in. My efforts to explain to these students why they ought to care about analysis have generally not been successful. My colleagues and I are considering the possibility of running our first-year graduate analysis courses as courses in analysis over a general complete valued field. Over the archimedean fields, we would recover classical analysis over the real and complex numbers; over the non-archimedean fields, we would recover many cases of interest to the students who plan to study algebra, particularly analysis over p-adic fields. We hope the resulting courses would be of interest and of use to all our students. But it is not yet clear to us that this idea makes sense; perhaps some topics (e.g. integration theory) are simply too different in the non-archimedean case to treat them, at this level, alongside the classical treatment. There are many good references on functional analysis which work over non-archimedean fields, or which allow the base complete valued field to be archimedean or non-archimedean. However, I do not know of a graduate analysis textbook (not just functional analysis, but rather roughly the contents of a standard first year of graduate analysis) which works over a general complete valued field. Do you know of any such textbook? Thanks.",,['analysis']
37,Interpolation of $L^p$ spaces (from book by Caffarelli-Cabre),Interpolation of  spaces (from book by Caffarelli-Cabre),L^p,"In the book ""Fully nonlinear elliptic equations"" by L. Caffarelli and X. Cabre, Theorem 4.8 (b) we want to prove the following maximum principle, for any $p>0$ , \begin{align*}     \sup_{Q_{1/2}}\,u\leq C(p)\left(\|u^+\|_{L^p(Q_{3/4})}+\|f\|_{L^d(Q_1)}\right), \end{align*} where $C(p)$ is a constant depending only on $d$ , $\lambda$ , $\Lambda$ and $p$ . The proof consists of proving this estimate por a particular $p=\varepsilon$ , which I have no problem with. Then, the authors claim that the general case $p>0$ follows easily by interpolation, but I don't know how. So, for the purpose of this question, we can assume this inequality holds for a fixed $p=\varepsilon$ , and the question is how to generalize this for any $p>0$ . Note that $Q_l\subset \mathbb{R}^d$ is a cube of side length $l$ . This is my work so far: If $p>\varepsilon$ we can apply  Holder inequality and get \begin{align*} \|u^+\|_{L^\varepsilon(Q_{3/4})}\leq |Q_{3/4}|^\frac{1}{r}\|u^+\|_{L^p(Q_{3/4})}, \quad \frac{1}{r}+\frac{1}{p}=\frac{1}{\varepsilon}.  \end{align*} Hence \begin{align*} \sup_{Q_{1/2}}u\leq& C\left(|Q_{3/4}|^\frac{1}{r}\|u^+\|_{L^p(Q_{3/4})}+\|f\|_{L^d(Q_1)}\right)\\ \leq &C\left(\|u^+\|_{L^p(Q_{3/4})}+\|f\|_{L^d(Q_1)}\right), \end{align*} since $ |Q_{3/4}|^\frac{1}{r}<1$ . The proof for any $p<\varepsilon$ follows by interpolation (I don't know how). Interpolation of $L^p$ spaces: I only know the generalized Holder inequality: If $0<p_0<p_1\leq \infty$ and $g\in L^{p_0}(X)\cap L^{p_1}(X)$ , then for every $\theta\in(0,1)$ and \begin{align*} \frac{1}{p_\theta}=\frac{\theta}{p_0}+\frac{1-\theta}{p_1},  \end{align*} it holds \begin{align*} \|g\|_{L^{p_\theta}(X)}\leq \|g\|_{L^{p_0}(X)}^{\theta}\|g\|_{L^{p_1}(X)}^{1-\theta}. \end{align*} If we choose $p_\theta=\varepsilon$ and $p_1=+\infty$ then \begin{align*} p=\theta \varepsilon\in (0,\varepsilon) \end{align*} and \begin{align*} \|u\|_{L^{\varepsilon}(Q_{3/4})}\leq \|u\|_{L^{p}(Q_{3/4})}^{\theta}\|u\|_{L^{\infty}(Q_{3/4})}^{1-\theta}. \end{align*} By Young's inequality, we have for every $\delta>0$ \begin{align*} \|u\|_{L^{p}(Q_{3/4})}^{\theta}\|u\|_{L^{\infty}(Q_{3/4})}^{1-\theta}\leq &\frac{\left(\delta^{-1} \|u\|_{L^{p}(Q_{3/4})}^{\theta}\right)^r   }{r}+\frac{\left(\delta\|u\|_{L^{\infty}(Q_{3/4})}^{1-\theta}\right)^s}{s}\\ =&\frac{\delta^{-r} \|u\|_{L^{p}(Q_{3/4})} }{r}+\frac{\left(\delta\|u\|_{L^{\infty}(Q_{3/4})}^{1-\theta}\right)^s}{s}, \end{align*} where \begin{align*} \frac{1}{r}+\frac{1}{s}=1 \quad \mbox{ and } \quad r=\frac{1}{\theta}. \end{align*} Combining with our estimate for $p=\varepsilon$ we get \begin{align*} \sup_{Q_{1/2}}u-C\frac{\left(\delta\|u\|_{L^{\infty}(Q_{3/4})}^{1-\theta}\right)^s}{s}\leq C\left(\frac{\delta^{-r} \|u\|_{L^{p}(Q_{3/4})} }{r}+\|f\|_{L^d(Q_1)}\right). \end{align*} Problem: I cant take $\delta$ universally small such that \begin{align*} C\frac{\left(\delta\|u\|_{L^{\infty}(Q_{3/4})}^{1-\theta}\right)^s}{s}\leq \frac{1}{2}\sup_{Q_{1/2}}u \end{align*} because the domains are different.","In the book ""Fully nonlinear elliptic equations"" by L. Caffarelli and X. Cabre, Theorem 4.8 (b) we want to prove the following maximum principle, for any , where is a constant depending only on , , and . The proof consists of proving this estimate por a particular , which I have no problem with. Then, the authors claim that the general case follows easily by interpolation, but I don't know how. So, for the purpose of this question, we can assume this inequality holds for a fixed , and the question is how to generalize this for any . Note that is a cube of side length . This is my work so far: If we can apply  Holder inequality and get Hence since . The proof for any follows by interpolation (I don't know how). Interpolation of spaces: I only know the generalized Holder inequality: If and , then for every and it holds If we choose and then and By Young's inequality, we have for every where Combining with our estimate for we get Problem: I cant take universally small such that because the domains are different.","p>0 \begin{align*}
    \sup_{Q_{1/2}}\,u\leq C(p)\left(\|u^+\|_{L^p(Q_{3/4})}+\|f\|_{L^d(Q_1)}\right),
\end{align*} C(p) d \lambda \Lambda p p=\varepsilon p>0 p=\varepsilon p>0 Q_l\subset \mathbb{R}^d l p>\varepsilon \begin{align*}
\|u^+\|_{L^\varepsilon(Q_{3/4})}\leq |Q_{3/4}|^\frac{1}{r}\|u^+\|_{L^p(Q_{3/4})}, \quad \frac{1}{r}+\frac{1}{p}=\frac{1}{\varepsilon}. 
\end{align*} \begin{align*}
\sup_{Q_{1/2}}u\leq& C\left(|Q_{3/4}|^\frac{1}{r}\|u^+\|_{L^p(Q_{3/4})}+\|f\|_{L^d(Q_1)}\right)\\
\leq &C\left(\|u^+\|_{L^p(Q_{3/4})}+\|f\|_{L^d(Q_1)}\right),
\end{align*}  |Q_{3/4}|^\frac{1}{r}<1 p<\varepsilon L^p 0<p_0<p_1\leq \infty g\in L^{p_0}(X)\cap L^{p_1}(X) \theta\in(0,1) \begin{align*}
\frac{1}{p_\theta}=\frac{\theta}{p_0}+\frac{1-\theta}{p_1}, 
\end{align*} \begin{align*}
\|g\|_{L^{p_\theta}(X)}\leq \|g\|_{L^{p_0}(X)}^{\theta}\|g\|_{L^{p_1}(X)}^{1-\theta}.
\end{align*} p_\theta=\varepsilon p_1=+\infty \begin{align*}
p=\theta \varepsilon\in (0,\varepsilon)
\end{align*} \begin{align*}
\|u\|_{L^{\varepsilon}(Q_{3/4})}\leq \|u\|_{L^{p}(Q_{3/4})}^{\theta}\|u\|_{L^{\infty}(Q_{3/4})}^{1-\theta}.
\end{align*} \delta>0 \begin{align*}
\|u\|_{L^{p}(Q_{3/4})}^{\theta}\|u\|_{L^{\infty}(Q_{3/4})}^{1-\theta}\leq &\frac{\left(\delta^{-1} \|u\|_{L^{p}(Q_{3/4})}^{\theta}\right)^r   }{r}+\frac{\left(\delta\|u\|_{L^{\infty}(Q_{3/4})}^{1-\theta}\right)^s}{s}\\
=&\frac{\delta^{-r} \|u\|_{L^{p}(Q_{3/4})} }{r}+\frac{\left(\delta\|u\|_{L^{\infty}(Q_{3/4})}^{1-\theta}\right)^s}{s},
\end{align*} \begin{align*}
\frac{1}{r}+\frac{1}{s}=1 \quad \mbox{ and } \quad r=\frac{1}{\theta}.
\end{align*} p=\varepsilon \begin{align*}
\sup_{Q_{1/2}}u-C\frac{\left(\delta\|u\|_{L^{\infty}(Q_{3/4})}^{1-\theta}\right)^s}{s}\leq C\left(\frac{\delta^{-r} \|u\|_{L^{p}(Q_{3/4})} }{r}+\|f\|_{L^d(Q_1)}\right).
\end{align*} \delta \begin{align*}
C\frac{\left(\delta\|u\|_{L^{\infty}(Q_{3/4})}^{1-\theta}\right)^s}{s}\leq \frac{1}{2}\sup_{Q_{1/2}}u
\end{align*}","['analysis', 'lp-spaces', 'maximum-principle']"
38,"Show that if $f_n\to f_1$ uniformly and $f_n\to f_2$ in $L^p$, then $f_1=f_2$ almost everywhere.","Show that if  uniformly and  in , then  almost everywhere.",f_n\to f_1 f_n\to f_2 L^p f_1=f_2,"Let $(X,\Sigma _X,\mu )$ be a measurable space, $(f_n)_{n\in\mathbb{N}}$ a sequence of $L^p(X)$ with $p\in [1,\infty ]$ and $f_1,f_2:X\to \mathbb{R}$ two measurable functions. Show that if $f_n\to f_1$ uniformly and $f_n\to f_2$ in $L^p$ , then $f_1=f_2$ almost everywhere. I proved that $|f_1(x)-f_2(x)|\leq \limsup _{n\to\infty }|f_n(x)-f_2(x)|$ for all $x\in X$ . I also know that there's an increasing sequence $(k_n)_{n\in\mathbb{N}}$ of $\mathbb{N}$ such that $\lim_{n\to\infty }|f_{k_n}(x)-f_2(x)|=\limsup_{n\to\infty}|f_n(x)-f_2(x)|$ for all $x\in X$ . I was able to prove what I asked if we assume that $|f_{k_n}(x)-f_2(x)|\leq \limsup_{n\to\infty}|f_n(x)-f_2(x)|$ for all $n\in\mathbb{N}$ and $x\in X$ . However I don't know how to prove the last inequality. Thank you for your attention!","Let be a measurable space, a sequence of with and two measurable functions. Show that if uniformly and in , then almost everywhere. I proved that for all . I also know that there's an increasing sequence of such that for all . I was able to prove what I asked if we assume that for all and . However I don't know how to prove the last inequality. Thank you for your attention!","(X,\Sigma _X,\mu ) (f_n)_{n\in\mathbb{N}} L^p(X) p\in [1,\infty ] f_1,f_2:X\to \mathbb{R} f_n\to f_1 f_n\to f_2 L^p f_1=f_2 |f_1(x)-f_2(x)|\leq \limsup _{n\to\infty }|f_n(x)-f_2(x)| x\in X (k_n)_{n\in\mathbb{N}} \mathbb{N} \lim_{n\to\infty }|f_{k_n}(x)-f_2(x)|=\limsup_{n\to\infty}|f_n(x)-f_2(x)| x\in X |f_{k_n}(x)-f_2(x)|\leq \limsup_{n\to\infty}|f_n(x)-f_2(x)| n\in\mathbb{N} x\in X","['real-analysis', 'analysis', 'measure-theory', 'lp-spaces', 'measurable-functions']"
39,Does the Existence of Derivatives along All Smooth Paths Guarantee Multivariable Differentiability?,Does the Existence of Derivatives along All Smooth Paths Guarantee Multivariable Differentiability?,,"This question is inspired by other relevant questions on MSE regarding continuity of partial derivatives and differentiability , existence of partial derivatives and differentiability , and limits along arbitrary smooth curves . I'm trying to get a handle on the extra bit of weirdness introduced by multivariable functions when it comes to differentiability and continuity. The applicable definition here is: Definition (Cain & Herod, 1997) : Let $f: D \rightarrow \mathbb{R}^p$ , where $D \subset \mathbb{R}^n$ and let $\mathbf{x}$ be an interior point of $D$ . Then $f$ is differentiable if there exists a linear function $L$ such that $\lim\limits_{\mathbf{h} \rightarrow \mathbf{0}} \dfrac{\|f(\mathbf{x} + \mathbf{h}) - f(\mathbf{x}) - L(\mathbf{h})\|}{\|\mathbf{h}\|} = \mathbf{0}$ So, given that the multivariable limit in the definition above requires that the limit exists (and is zero) independent of the path, can I interpret this as translating to the requirement that the derivative along any smooth path through $\mathbf{x}$ must exist? That is, if we define a so-called path derivative $D_\mathbf{p}f$ : $D_{\mathbf{p}}f : \mathbf{x} \mapsto  \lim\limits_{t \rightarrow t_0} \dfrac{f(\mathbf{x} + \mathbf{p}(t)) - f(\mathbf{x})}{\|\mathbf{p}(t)\|}$ where $\mathbf{p}$ defines any path with $t_0$ such that $\lim\limits_{t \rightarrow t_0} \mathbf{p}(t) = \mathbf{0}$ , If a path is defined as smooth if it is representable by an infinitely differentiable vector function, is differentiability equivalent to the existence of all path derivatives where $\mathbf{p}$ defines a smooth path? Reading this answer , I initially suspected that this path derivative condition was identical to the total derivative which defines differentiability, but I wonder what kind of wrinkle the requirement that $\mathbf{p}$ defines a smooth path introduces. Proving that differentiability implies the existence of all such path derivatives is straightforward enough. I can't see if there's a way to prove differentiability assuming the existence of all path derivatives, however... How does this condition compare to the condition of the existence of all partial derivatives and $n-1$ continuous partial derivatives in some $n$ -ball containing $\mathbf{x}$ (quoted here) ? Is it stronger/weaker? At the very least, I think that this condition is not useless. For example, if you can find a smooth path for which the path derivative does not exist, then $f$ is not differentiable. In reference to this question –where one partial derivative is not defined along some line with $\mathbf{x}$ deleted–the notion of path derivatives implies that such a function is automatically not differentiable.","This question is inspired by other relevant questions on MSE regarding continuity of partial derivatives and differentiability , existence of partial derivatives and differentiability , and limits along arbitrary smooth curves . I'm trying to get a handle on the extra bit of weirdness introduced by multivariable functions when it comes to differentiability and continuity. The applicable definition here is: Definition (Cain & Herod, 1997) : Let , where and let be an interior point of . Then is differentiable if there exists a linear function such that So, given that the multivariable limit in the definition above requires that the limit exists (and is zero) independent of the path, can I interpret this as translating to the requirement that the derivative along any smooth path through must exist? That is, if we define a so-called path derivative : where defines any path with such that , If a path is defined as smooth if it is representable by an infinitely differentiable vector function, is differentiability equivalent to the existence of all path derivatives where defines a smooth path? Reading this answer , I initially suspected that this path derivative condition was identical to the total derivative which defines differentiability, but I wonder what kind of wrinkle the requirement that defines a smooth path introduces. Proving that differentiability implies the existence of all such path derivatives is straightforward enough. I can't see if there's a way to prove differentiability assuming the existence of all path derivatives, however... How does this condition compare to the condition of the existence of all partial derivatives and continuous partial derivatives in some -ball containing (quoted here) ? Is it stronger/weaker? At the very least, I think that this condition is not useless. For example, if you can find a smooth path for which the path derivative does not exist, then is not differentiable. In reference to this question –where one partial derivative is not defined along some line with deleted–the notion of path derivatives implies that such a function is automatically not differentiable.",f: D \rightarrow \mathbb{R}^p D \subset \mathbb{R}^n \mathbf{x} D f L \lim\limits_{\mathbf{h} \rightarrow \mathbf{0}} \dfrac{\|f(\mathbf{x} + \mathbf{h}) - f(\mathbf{x}) - L(\mathbf{h})\|}{\|\mathbf{h}\|} = \mathbf{0} \mathbf{x} D_\mathbf{p}f D_{\mathbf{p}}f : \mathbf{x} \mapsto  \lim\limits_{t \rightarrow t_0} \dfrac{f(\mathbf{x} + \mathbf{p}(t)) - f(\mathbf{x})}{\|\mathbf{p}(t)\|} \mathbf{p} t_0 \lim\limits_{t \rightarrow t_0} \mathbf{p}(t) = \mathbf{0} \mathbf{p} \mathbf{p} n-1 n \mathbf{x} f \mathbf{x},"['real-analysis', 'analysis', 'multivariable-calculus', 'partial-derivative']"
40,Find solutions of $F(x)=\frac32 F\left(\sqrt[3]{x^2}-1\right)$,Find solutions of,F(x)=\frac32 F\left(\sqrt[3]{x^2}-1\right),"I need to construct a continuous function $F:\mathbb R\to \mathbb R$ with $F(0)=0$ and $F(x)=\frac32 F\left(\sqrt[3]{x^2}-1\right)$ for every $x\in\mathbb R$ . I found $F(x)\equiv0$ is a solution. Moreover, if F is a polynomial, I could prove that $F$ must be zero function. My question is: Is there another such function? I think it must be have, but I could not construct them.","I need to construct a continuous function with and for every . I found is a solution. Moreover, if F is a polynomial, I could prove that must be zero function. My question is: Is there another such function? I think it must be have, but I could not construct them.",F:\mathbb R\to \mathbb R F(0)=0 F(x)=\frac32 F\left(\sqrt[3]{x^2}-1\right) x\in\mathbb R F(x)\equiv0 F,"['calculus', 'analysis', 'functional-equations']"
41,A Gronwall-type inequality for $L^p$ norms,A Gronwall-type inequality for  norms,L^p,"Here is a problem from my homework, which asks me to show a Gronwall-type inequality. Let $1\leq\beta<\gamma\leq\infty, 0<T\leq\infty$ and let $f\in L^\rho(0,T)$ , where $1\leq\rho<\infty$ is defined by $\frac1\rho=\frac1\beta-\frac1\gamma$ . If $\eta\geq0$ and $\varphi\in L_{\text{loc}}^\gamma([0,T))$ satisfy $$\|\varphi\|_{L^\gamma(0,t)}\leq\eta+\|f\varphi\|_{L^\beta(0,t)},$$ for all $0<t<T$ . Prove that $$\|\varphi\|_{L^\gamma(0,t)}\leq\eta\Phi(\|f\|_{L^\rho(0,t)}),$$ for all $0<t<T$ , where $\Phi(s)=2\Gamma(3+2s)$ and $\Gamma$ is the Gamma function. I cannot see why the Gamma function appears here. Applying Hölder's inequality to the assumption gives that $$\|\varphi\|_{L^\gamma(0,t)}\leq\eta+\|f\|_{L^\rho(0,t)}\|\varphi\|_{L^\gamma(0,t)}.$$ Now we can do the iteration, but this process fails to give the desired result. Any help would be appreciated.","Here is a problem from my homework, which asks me to show a Gronwall-type inequality. Let and let , where is defined by . If and satisfy for all . Prove that for all , where and is the Gamma function. I cannot see why the Gamma function appears here. Applying Hölder's inequality to the assumption gives that Now we can do the iteration, but this process fails to give the desired result. Any help would be appreciated.","1\leq\beta<\gamma\leq\infty, 0<T\leq\infty f\in L^\rho(0,T) 1\leq\rho<\infty \frac1\rho=\frac1\beta-\frac1\gamma \eta\geq0 \varphi\in L_{\text{loc}}^\gamma([0,T)) \|\varphi\|_{L^\gamma(0,t)}\leq\eta+\|f\varphi\|_{L^\beta(0,t)}, 0<t<T \|\varphi\|_{L^\gamma(0,t)}\leq\eta\Phi(\|f\|_{L^\rho(0,t)}), 0<t<T \Phi(s)=2\Gamma(3+2s) \Gamma \|\varphi\|_{L^\gamma(0,t)}\leq\eta+\|f\|_{L^\rho(0,t)}\|\varphi\|_{L^\gamma(0,t)}.","['real-analysis', 'analysis', 'inequality', 'partial-differential-equations']"
42,Minimums of $-g(x)\cos(2 \pi h(x))$?,Minimums of ?,-g(x)\cos(2 \pi h(x)),"Let $f(x) = -g(x)\cos(2 \pi h(x))$ , where $h(x)$ and $g(x)$ are both continuous and invertible functions. Let the ""attraction basin"" of a minima of $f(x)$ be defined as the set of points which lead to that minima when gradient descent is performed. Basically,  the region around the minima where if you ""let a ball go"", it would ""roll down"" to the minima. (Not sure how better to explain it, if this is confusing or ambiguous let me know). Finally, let $L(f(x))$ be a function that returns the local minima of the attraction basin of x. For example, if $f(x) = -\cos(2\pi x)$ , then minimas occur at integer coordinates so $L(f(x)) = round(x)$ . If $x = 0.4$ , then $L(f(0.4)) = round(0.4) = 0$ . If you performed gradient descent at 0.4, you would reach 0. I am trying to figure out how to make this work for $f(x) = -g(x)\cos(2\pi h(x))$ . Without the $g(x)$ , it is simple and $L(f(x)) = h^{-1}(round(h(x)))$ , because $h(x)$ is invertible. However, with the $g(x)$ , I am stumped. How can I figure out $L(f(x))$ for that? In other words, how can I predict where the minimas and their attraction basins are? I know that $f'(x) = 2\pi g(x) h'(x) \sin(2\pi h(x)) - g'(x) \cos(2\pi h(x))$ , and the minimas come when $f'(x) = 0$ . What are necessary conditions on $g(x)$ such that we can we predict the minimas of $f(x)$ ? Any help is very, very much appreciated. Thanks!","Let , where and are both continuous and invertible functions. Let the ""attraction basin"" of a minima of be defined as the set of points which lead to that minima when gradient descent is performed. Basically,  the region around the minima where if you ""let a ball go"", it would ""roll down"" to the minima. (Not sure how better to explain it, if this is confusing or ambiguous let me know). Finally, let be a function that returns the local minima of the attraction basin of x. For example, if , then minimas occur at integer coordinates so . If , then . If you performed gradient descent at 0.4, you would reach 0. I am trying to figure out how to make this work for . Without the , it is simple and , because is invertible. However, with the , I am stumped. How can I figure out for that? In other words, how can I predict where the minimas and their attraction basins are? I know that , and the minimas come when . What are necessary conditions on such that we can we predict the minimas of ? Any help is very, very much appreciated. Thanks!",f(x) = -g(x)\cos(2 \pi h(x)) h(x) g(x) f(x) L(f(x)) f(x) = -\cos(2\pi x) L(f(x)) = round(x) x = 0.4 L(f(0.4)) = round(0.4) = 0 f(x) = -g(x)\cos(2\pi h(x)) g(x) L(f(x)) = h^{-1}(round(h(x))) h(x) g(x) L(f(x)) f'(x) = 2\pi g(x) h'(x) \sin(2\pi h(x)) - g'(x) \cos(2\pi h(x)) f'(x) = 0 g(x) f(x),"['real-analysis', 'analysis', 'optimization', 'maxima-minima']"
43,Difference quotient for Hölder continuous functions,Difference quotient for Hölder continuous functions,,"Let $\Omega\subset\mathbb{R}^n$ be a bounded open set and $u\in C^\alpha_{\mathrm{loc}}(\Omega)$ . For $h>0$ , $1\leq k\leq n$ , let $$D_k^hu(x)=\frac{u(x+he_k)-u(x)}{h}$$ where $e_k$ is the $k$ -th coordinate vector. Suppose for each $\Omega_0\Subset\Omega$ and $k$ , there is a constant $C$ s.t. $\|D^h_ku\|_{C^\alpha(\Omega_0)}\leq C$ for all small $h$ . Then $u\in C^{1,\alpha}_{\mathrm{loc}}(\Omega)$ and in fact $\|D_ku\|_{C^\alpha(\Omega_0)}\leq C$ . Is this statement true? If so, how can I prove it? By Arzelà–Ascoli we know that there is a sequence $h_j\to0$ such that $D^{h_j}_ku$ converges uniformly. But why does this even imply $u\in C^1$ ?","Let be a bounded open set and . For , , let where is the -th coordinate vector. Suppose for each and , there is a constant s.t. for all small . Then and in fact . Is this statement true? If so, how can I prove it? By Arzelà–Ascoli we know that there is a sequence such that converges uniformly. But why does this even imply ?","\Omega\subset\mathbb{R}^n u\in C^\alpha_{\mathrm{loc}}(\Omega) h>0 1\leq k\leq n D_k^hu(x)=\frac{u(x+he_k)-u(x)}{h} e_k k \Omega_0\Subset\Omega k C \|D^h_ku\|_{C^\alpha(\Omega_0)}\leq C h u\in C^{1,\alpha}_{\mathrm{loc}}(\Omega) \|D_ku\|_{C^\alpha(\Omega_0)}\leq C h_j\to0 D^{h_j}_ku u\in C^1","['analysis', 'partial-differential-equations', 'regularity-theory-of-pdes', 'holder-spaces', 'function-spaces']"
44,$\nabla u=0~ a.e.\Rightarrow u=\text{constant}~ a.e. $ on Riemannian manifold,on Riemannian manifold,\nabla u=0~ a.e.\Rightarrow u=\text{constant}~ a.e. ,"Let $M$ be a Riemannian manifold and $\Omega\subset M$ be a connected open set. Is it true that for $u\in W^{1, 2}(\Omega)$ $$\nabla u=0~ a.e.\Rightarrow u=\text{constant}~ a.e.?$$ I know that this true if $M=\mathbb{R}^{n}$ by mollification, but does this also hold in this general setting? Thanks in advance!","Let be a Riemannian manifold and be a connected open set. Is it true that for I know that this true if by mollification, but does this also hold in this general setting? Thanks in advance!","M \Omega\subset M u\in W^{1, 2}(\Omega) \nabla u=0~ a.e.\Rightarrow u=\text{constant}~ a.e.? M=\mathbb{R}^{n}",['analysis']
45,"Maximize $\min(abh,cdg)k+\min(abj,fdg)l+\min(ebh,cdi)l+\min(ebj,fdi)m$ subject to $a+e=c+f=1$",Maximize  subject to,"\min(abh,cdg)k+\min(abj,fdg)l+\min(ebh,cdi)l+\min(ebj,fdi)m a+e=c+f=1","Let $a,b,c,\ldots,m\ge0$ with $a,c,e,f\le1$ . I want to maximize $$\varphi(a,c,e,f):=\min(abh,cdg)k+\min(abj,fdg)l+\min(ebh,cdi)l+\min(ebj,fdi)m$$ over all choices of $a,c,e,f$ subject to $a+e=c+f=1$ . Unfortunately, I don't have much to contribute, since I'm not familiar with this kind of problems. I could imagine that the solution is simple, but I might be wrong. It might be useful to note that $2\min(x,y)=x+y-|x-y|$ for all $x,y\ge0$ .","Let with . I want to maximize over all choices of subject to . Unfortunately, I don't have much to contribute, since I'm not familiar with this kind of problems. I could imagine that the solution is simple, but I might be wrong. It might be useful to note that for all .","a,b,c,\ldots,m\ge0 a,c,e,f\le1 \varphi(a,c,e,f):=\min(abh,cdg)k+\min(abj,fdg)l+\min(ebh,cdi)l+\min(ebj,fdi)m a,c,e,f a+e=c+f=1 2\min(x,y)=x+y-|x-y| x,y\ge0","['analysis', 'optimization', 'nonlinear-optimization', 'maxima-minima']"
46,A nifty triangle inequality trick,A nifty triangle inequality trick,,"I was reading the proof of a theorem and at a point of the argument the author just posed an inequality which I was unable to derive, but is probably just a smart application of the triangle inequality. The problem is as follows. Let $f,g:\mathbb{R} \to \mathbb{R}$ be uniformly continuous and $\epsilon, \delta >0$ such that if $x,y\in \mathbb{R}$ are st $|x-y| < \delta$ , that then $|f(x)-f(y)|, |g(x)-g(y)| <\epsilon$ . Let $I \subset \mathbb{R}$ be an interval with $diam(I) < \delta$ The author then states without proof that $\sup_{x,y\in I} |f(x)-g(y)|^p < |f(t)-g(t)+2\epsilon|^p$ for all $t \in I$ ., $p\geq 1$ It feels as if this should be easy to prove, but I have been stuck at it for quite some time now. Any help would be greatly appreciated.","I was reading the proof of a theorem and at a point of the argument the author just posed an inequality which I was unable to derive, but is probably just a smart application of the triangle inequality. The problem is as follows. Let be uniformly continuous and such that if are st , that then . Let be an interval with The author then states without proof that for all ., It feels as if this should be easy to prove, but I have been stuck at it for quite some time now. Any help would be greatly appreciated.","f,g:\mathbb{R} \to \mathbb{R} \epsilon, \delta >0 x,y\in \mathbb{R} |x-y| < \delta |f(x)-f(y)|, |g(x)-g(y)| <\epsilon I \subset \mathbb{R} diam(I) < \delta \sup_{x,y\in I} |f(x)-g(y)|^p < |f(t)-g(t)+2\epsilon|^p t \in I p\geq 1","['analysis', 'inequality']"
47,"On Convolution: Show that $g_a *g_b = g_{\min(a,b)}$",On Convolution: Show that,"g_a *g_b = g_{\min(a,b)}","For $a>0$, I have been given following  the functions  $$f_a(x)=\frac{a}{π(x^2+a^2)}$$  and  $$g_a(x)=\frac{\sin(ax)}{π x}~~x\neq0,\qquad g_a(0)= \frac{a}{π}. $$ Question Show that, $$f_a *f_b =  f_{a+b}$$ and $$g_a *g_b =  g_{\min(a,b)}$$ I was able to prove that $f_a *f_b =  f_{a+b}$ through Fourier Transform. Can anyone help to show that $g_a *g_b =  g_{\min(a,b)}$ or any hint?","For $a>0$, I have been given following  the functions  $$f_a(x)=\frac{a}{π(x^2+a^2)}$$  and  $$g_a(x)=\frac{\sin(ax)}{π x}~~x\neq0,\qquad g_a(0)= \frac{a}{π}. $$ Question Show that, $$f_a *f_b =  f_{a+b}$$ and $$g_a *g_b =  g_{\min(a,b)}$$ I was able to prove that $f_a *f_b =  f_{a+b}$ through Fourier Transform. Can anyone help to show that $g_a *g_b =  g_{\min(a,b)}$ or any hint?",,"['calculus', 'real-analysis', 'analysis', 'fourier-analysis', 'convolution']"
48,Proof without mean value theorem that continuously partially differentiable implies differentiability,Proof without mean value theorem that continuously partially differentiable implies differentiability,,"For simplicity we consider the case of $2$ factors. Let $E_1,E_2,F$ be Banach spaces and $f:X\to F$, where $X\subseteq E:=E_1\times E_2$ is open. By first partial derivative we mean the derivaitve of $f(\cdot,e_2)$ when we fix $e_2\in E_2$. Second (or other) partial derivatives are defined analogously. I'm trying to prove the following theorem from H. Cartan's Differential Calculus : This is the proof provided in the book: where the mean value theorem used is this: My question: Can we avoid the use of the mean value theorem? I think proving $(3.7.2)$ doesn't need the mean value theorem. Here is my proof: That $f$ is differentiable at $x_0\in E$ with derivative $\partial f(x_0)$ is equivalent to saying that $$f(x)=f(x_0)+\partial f(x_0)(x-x_0)+r(x)\|x-x_0\|,$$ where $r:X\to F$ is continuous at $x_0$ with $r(x_0)=0$. Therefore, we have (again considering the case $n=2$ for simplicity) $$f(x_1,x_2)-f(a_1,x_2)-\frac{\partial f}{\partial x_1}(a_1,a_2)(x_1-a_1)=\Big(\frac{\partial f}{\partial x_1}(a_1,x_2)-\frac{\partial f}{\partial x_1}(a_1,a_2)\Big)(x_1-a_1)+r(x_1,x_2)\|x_1-a_1\|.$$ Since $\frac{\partial f}{\partial x_1}$ and $r$ are continuous at $(a_1,a_2)$, we can choose $\delta>0$ such that, for all $\|x_1-a_1\|+\|x_2-a_2\|<\delta$ (using the $\|\!\cdot\!\|_1$ product norm), we have $\big\|\frac{\partial f}{\partial x_1}(a_1,x_2)-\frac{\partial f}{\partial x_1}(a_1,a_2)\big\|\leq\varepsilon/2$ (operator norm) and $\|r(x_1,x_2)\|\leq\varepsilon/2$, so $$\Big\|f(x_1,x_2)-f(a_1,x_2)-\frac{\partial f}{\partial x_1}(a_1,a_2)(x_1-a_1)\Big\|\leq\varepsilon\|x_1-a_1\|.$$ Is my proof correct? If so, why is the author using the mean value theorem (which does not really simplify the proof in my eyes)?","For simplicity we consider the case of $2$ factors. Let $E_1,E_2,F$ be Banach spaces and $f:X\to F$, where $X\subseteq E:=E_1\times E_2$ is open. By first partial derivative we mean the derivaitve of $f(\cdot,e_2)$ when we fix $e_2\in E_2$. Second (or other) partial derivatives are defined analogously. I'm trying to prove the following theorem from H. Cartan's Differential Calculus : This is the proof provided in the book: where the mean value theorem used is this: My question: Can we avoid the use of the mean value theorem? I think proving $(3.7.2)$ doesn't need the mean value theorem. Here is my proof: That $f$ is differentiable at $x_0\in E$ with derivative $\partial f(x_0)$ is equivalent to saying that $$f(x)=f(x_0)+\partial f(x_0)(x-x_0)+r(x)\|x-x_0\|,$$ where $r:X\to F$ is continuous at $x_0$ with $r(x_0)=0$. Therefore, we have (again considering the case $n=2$ for simplicity) $$f(x_1,x_2)-f(a_1,x_2)-\frac{\partial f}{\partial x_1}(a_1,a_2)(x_1-a_1)=\Big(\frac{\partial f}{\partial x_1}(a_1,x_2)-\frac{\partial f}{\partial x_1}(a_1,a_2)\Big)(x_1-a_1)+r(x_1,x_2)\|x_1-a_1\|.$$ Since $\frac{\partial f}{\partial x_1}$ and $r$ are continuous at $(a_1,a_2)$, we can choose $\delta>0$ such that, for all $\|x_1-a_1\|+\|x_2-a_2\|<\delta$ (using the $\|\!\cdot\!\|_1$ product norm), we have $\big\|\frac{\partial f}{\partial x_1}(a_1,x_2)-\frac{\partial f}{\partial x_1}(a_1,a_2)\big\|\leq\varepsilon/2$ (operator norm) and $\|r(x_1,x_2)\|\leq\varepsilon/2$, so $$\Big\|f(x_1,x_2)-f(a_1,x_2)-\frac{\partial f}{\partial x_1}(a_1,a_2)(x_1-a_1)\Big\|\leq\varepsilon\|x_1-a_1\|.$$ Is my proof correct? If so, why is the author using the mean value theorem (which does not really simplify the proof in my eyes)?",,"['calculus', 'analysis', 'multivariable-calculus', 'proof-verification']"
49,"Explanation of ""without loss of generality"" in an application of Inverse Function Theorem.","Explanation of ""without loss of generality"" in an application of Inverse Function Theorem.",,"Let $U$ be an open subset of $\mathbb R^{n+m}=\mathbb R^n\times \mathbb R^m$ and $g:U\to\mathbb R^m$ a $C^1$ function. Let $p=(x_0,y_0)\in U$ be a point such that $$g'(p):\mathbb R^{n+m}\to \mathbb R^m\text{ is surjective}\tag{$*$}$$ The book I'm reading says (A) Without loss of generality, we can assume that the restriction $g'(p)\big |_{\{0\}\times \mathbb R^m}$ is an isomorphism. Question: Why there is no loss of generality in this assumption? (I'm interested in the explicit proof that the general case can be reduced to this one.) Context: The book proves the Inverse Function Theorem, in which the condition ""$g'(p)\big |_{\{0\}\times \mathbb R^m}$ is an isomorphism"" is an assumption, and then proves the Lagrange Multiplier Method as an application. Because of (A), which appears in the proof of the Lagrange Multiplier Method, the Inverse Function Theorem implies the following: (B) There are a neighborhood $A\subset \mathbb R^n$ of $x_0$, a neighborhood $V\subset U$ of $(x_0,y_0)=p$ and a $C^1$ function $\xi:A\to\mathbb R^m$ such that   $$(x,\xi(x))\in V\quad\text{and}\quad g(x,\xi(x))=g(p),\qquad \forall\ x\in A$$ My try: I know that, from $(*)$, there exists an $m$-dimenisonal subspace $X$ of $\mathbb R^{n+m}$ such that the restriction $g'(p)\big |_{X}$ is an isomorphism. So, my question is: how to rigorously pass from $X$ to $\{0\}\times \mathbb R^m$? Well, I know that there exists a bijective linear map $h:\{0\}\times \mathbb R^{m}\to X$. Let $H:\mathbb R^{n+m}\to\mathbb R^{n+m}$ be a bijective linear extension of $h$. Take $x_p=(x_p^1,x_p^2)\in \mathbb R^{n+m}$ such that $H(x_p)=p$. Define $\tilde{g}:H^{-1}(U)\to\mathbb R^{m}$ by $\tilde{g}(y)=g(H(y))$. Then, $\tilde{g}'(p)\big |_{\{0\}\times \mathbb R^m}$ is an isomorphism. Is it correct? If so: (C) There are a neighborhood $\tilde{A}\subset \mathbb R^n$ of $x_p^1$, a neighborhood $\tilde{V}\subset H^{-1}( U)$ of $(x_p^1,x_2^p)=x_p$ and a $C^1$ function $\tilde{\xi}:\tilde{A}\to\mathbb R^m$ such that   $$(y,\tilde{\xi}(y))\in \tilde{V}\quad\text{and}\quad \tilde{g}(y,\tilde{\xi}(y))=\tilde{g}(x_p),\qquad \forall\ y\in \tilde{A}.$$ To finish my argumment, I have to obtain (B) from (C). Is it possible? I suspect that we should define $V=F(\tilde{V})$. But how to define $A$ and $\xi$ form $\tilde{A}$ and $\tilde{\xi}$?","Let $U$ be an open subset of $\mathbb R^{n+m}=\mathbb R^n\times \mathbb R^m$ and $g:U\to\mathbb R^m$ a $C^1$ function. Let $p=(x_0,y_0)\in U$ be a point such that $$g'(p):\mathbb R^{n+m}\to \mathbb R^m\text{ is surjective}\tag{$*$}$$ The book I'm reading says (A) Without loss of generality, we can assume that the restriction $g'(p)\big |_{\{0\}\times \mathbb R^m}$ is an isomorphism. Question: Why there is no loss of generality in this assumption? (I'm interested in the explicit proof that the general case can be reduced to this one.) Context: The book proves the Inverse Function Theorem, in which the condition ""$g'(p)\big |_{\{0\}\times \mathbb R^m}$ is an isomorphism"" is an assumption, and then proves the Lagrange Multiplier Method as an application. Because of (A), which appears in the proof of the Lagrange Multiplier Method, the Inverse Function Theorem implies the following: (B) There are a neighborhood $A\subset \mathbb R^n$ of $x_0$, a neighborhood $V\subset U$ of $(x_0,y_0)=p$ and a $C^1$ function $\xi:A\to\mathbb R^m$ such that   $$(x,\xi(x))\in V\quad\text{and}\quad g(x,\xi(x))=g(p),\qquad \forall\ x\in A$$ My try: I know that, from $(*)$, there exists an $m$-dimenisonal subspace $X$ of $\mathbb R^{n+m}$ such that the restriction $g'(p)\big |_{X}$ is an isomorphism. So, my question is: how to rigorously pass from $X$ to $\{0\}\times \mathbb R^m$? Well, I know that there exists a bijective linear map $h:\{0\}\times \mathbb R^{m}\to X$. Let $H:\mathbb R^{n+m}\to\mathbb R^{n+m}$ be a bijective linear extension of $h$. Take $x_p=(x_p^1,x_p^2)\in \mathbb R^{n+m}$ such that $H(x_p)=p$. Define $\tilde{g}:H^{-1}(U)\to\mathbb R^{m}$ by $\tilde{g}(y)=g(H(y))$. Then, $\tilde{g}'(p)\big |_{\{0\}\times \mathbb R^m}$ is an isomorphism. Is it correct? If so: (C) There are a neighborhood $\tilde{A}\subset \mathbb R^n$ of $x_p^1$, a neighborhood $\tilde{V}\subset H^{-1}( U)$ of $(x_p^1,x_2^p)=x_p$ and a $C^1$ function $\tilde{\xi}:\tilde{A}\to\mathbb R^m$ such that   $$(y,\tilde{\xi}(y))\in \tilde{V}\quad\text{and}\quad \tilde{g}(y,\tilde{\xi}(y))=\tilde{g}(x_p),\qquad \forall\ y\in \tilde{A}.$$ To finish my argumment, I have to obtain (B) from (C). Is it possible? I suspect that we should define $V=F(\tilde{V})$. But how to define $A$ and $\xi$ form $\tilde{A}$ and $\tilde{\xi}$?",,"['analysis', 'multivariable-calculus', 'proof-explanation', 'lagrange-multiplier', 'inverse-function-theorem']"
50,Is there a quick way to arrive to this partial fraction decomposition?,Is there a quick way to arrive to this partial fraction decomposition?,,"I'm reading a book where the author claims without showing the work that after we go through with the algebra of partial fractions we arrive to the formula (note $|z|<1$): $$\frac{1}{(1-z)(1-z^2)(1-z^3)} = \frac{1}{6}\frac{1}{(1-z)^3}+\frac{1}{4}\frac{1}{(1-z)^2}+\frac{1}{4}\frac{1}{(1-z^2)}+\frac{1}{3}\frac{1}{(1-z^3)}$$ Naturally, I'm trying to reproduce the result, but I'm taking a very naive approach, namely I've expressed the LHS as a product of irreducible factors over $\mathbb{R}$ and I am trying to determine the coefficients: $$\frac{1}{(1-z)^3}\frac{1}{(1+z)}\frac{1}{(z^2+z+1)} = \frac{A}{(1-z)}+\frac{B}{(1-z)^2}+\frac{C}{(1-z)^3}+\frac{D}{(1+z)}+\frac{Ex+F}{(z^2+z+1)}$$ This seems to have some drawbacks though, because on top of being long winded, once I do obtain all the coefficients I will have to recombine some of the terms to arrive at the author's answer. My question is then: is there some sort of trick which we can use here which I am not aware of, or do we have to suffer through the algebra patiently?","I'm reading a book where the author claims without showing the work that after we go through with the algebra of partial fractions we arrive to the formula (note $|z|<1$): $$\frac{1}{(1-z)(1-z^2)(1-z^3)} = \frac{1}{6}\frac{1}{(1-z)^3}+\frac{1}{4}\frac{1}{(1-z)^2}+\frac{1}{4}\frac{1}{(1-z^2)}+\frac{1}{3}\frac{1}{(1-z^3)}$$ Naturally, I'm trying to reproduce the result, but I'm taking a very naive approach, namely I've expressed the LHS as a product of irreducible factors over $\mathbb{R}$ and I am trying to determine the coefficients: $$\frac{1}{(1-z)^3}\frac{1}{(1+z)}\frac{1}{(z^2+z+1)} = \frac{A}{(1-z)}+\frac{B}{(1-z)^2}+\frac{C}{(1-z)^3}+\frac{D}{(1+z)}+\frac{Ex+F}{(z^2+z+1)}$$ This seems to have some drawbacks though, because on top of being long winded, once I do obtain all the coefficients I will have to recombine some of the terms to arrive at the author's answer. My question is then: is there some sort of trick which we can use here which I am not aware of, or do we have to suffer through the algebra patiently?",,"['calculus', 'analysis', 'partial-fractions']"
51,Give an example such that $(x_ny_n)$ converges but $(x_n)$ and $(y_n)$ diverges.,Give an example such that  converges but  and  diverges.,(x_ny_n) (x_n) (y_n),"Give an example of sequences $(x_n)$ and $(y_n)$ in $\mathbb{R}$ such that $(x_ny_n)$ converges but $(x_n)$ and $(y_n)$ diverges. My answer: Let $x_n=(-1)^n$ and $y_n=(-1)^n$. Then, $x_ny_n=(-1)^n(-1)^n=(-1)^{2n}$. So, $(-1)^{2n}$ convergence to $1$ and $(x_n)$, $(y_n)$ diverges. Can you check my answer?","Give an example of sequences $(x_n)$ and $(y_n)$ in $\mathbb{R}$ such that $(x_ny_n)$ converges but $(x_n)$ and $(y_n)$ diverges. My answer: Let $x_n=(-1)^n$ and $y_n=(-1)^n$. Then, $x_ny_n=(-1)^n(-1)^n=(-1)^{2n}$. So, $(-1)^{2n}$ convergence to $1$ and $(x_n)$, $(y_n)$ diverges. Can you check my answer?",,['analysis']
52,Poincaré's inequality for vector fields on a surface,Poincaré's inequality for vector fields on a surface,,"$\newcommand{\Ric}{\text{Ric}}$ Let $M$ be a smooth closed oriented Riemannian surface . I am searching for a reference (or a sketch of proof) for the following inequality: $$ \int_M |  \nabla V|^2 \ge  \int_{M} \Ric(V,V)=\int_{M} K|V|^2, \tag{1}$$ for every vector field $V \in \Gamma(TM)$ , where $ \nabla$ is the Levi-Civita connection, and the integration is against the Riemannian volume form. ( $K$ is the Gauss curvature). I guess some kind of Bochner identity is needed. I am also interested to know if this inequality holds for manifolds of higher dimensions. BTW, specializing for the case of the round $2$ -sphere, we get $$ \int_{\mathbb{S}^2} |  \nabla V|^2 \ge  \int_{\mathbb{S}^2} |V|^2. \tag{2}$$ A proof of this specific case can be found here .","Let be a smooth closed oriented Riemannian surface . I am searching for a reference (or a sketch of proof) for the following inequality: for every vector field , where is the Levi-Civita connection, and the integration is against the Riemannian volume form. ( is the Gauss curvature). I guess some kind of Bochner identity is needed. I am also interested to know if this inequality holds for manifolds of higher dimensions. BTW, specializing for the case of the round -sphere, we get A proof of this specific case can be found here .","\newcommand{\Ric}{\text{Ric}} M  \int_M |  \nabla V|^2 \ge  \int_{M} \Ric(V,V)=\int_{M} K|V|^2, \tag{1} V \in \Gamma(TM)  \nabla K 2  \int_{\mathbb{S}^2} |  \nabla V|^2 \ge  \int_{\mathbb{S}^2} |V|^2. \tag{2}","['analysis', 'riemannian-geometry', 'curvature', 'integral-inequality', 'geometric-inequalities']"
53,Uniform convergence of Fourier series in terms of modulus of continuity,Uniform convergence of Fourier series in terms of modulus of continuity,,"In the section Uniform Convergence in https://en.wikipedia.org/wiki/Convergence_of_Fourier_series it is given the result of D. Jackson which states that if $f \in C^p$ is $2\pi$-periodic and $f^{(p)}$ has modulus of continuity $\omega$, then  $$ |f(x) - (S_N f)(x)| \leq K \dfrac{\log N}{N^p}\omega(2\pi/N) $$ for $K > 0$ a constant independent of $N,f,p$. Here $S_N f$ denotes the $N$-th partial sum of the Fourier series expansion of $f$. I have been unable to find the proof of this and was wondering if anyone would be kind enough to show it? Thanks!","In the section Uniform Convergence in https://en.wikipedia.org/wiki/Convergence_of_Fourier_series it is given the result of D. Jackson which states that if $f \in C^p$ is $2\pi$-periodic and $f^{(p)}$ has modulus of continuity $\omega$, then  $$ |f(x) - (S_N f)(x)| \leq K \dfrac{\log N}{N^p}\omega(2\pi/N) $$ for $K > 0$ a constant independent of $N,f,p$. Here $S_N f$ denotes the $N$-th partial sum of the Fourier series expansion of $f$. I have been unable to find the proof of this and was wondering if anyone would be kind enough to show it? Thanks!",,"['calculus', 'analysis', 'fourier-analysis', 'fourier-series', 'uniform-convergence']"
54,Prove directly from the definition that $({1\over2}+\frac{1}{2^2}+...+\frac{1}{2^n})_n $ is Cauchy,Prove directly from the definition that  is Cauchy,({1\over2}+\frac{1}{2^2}+...+\frac{1}{2^n})_n ,Prove directly from the definition that $({1\over2}+\frac{1}{2^2}+...+\frac{1}{2^n})_n$ is cauchy I know from the definition of Cauchy that |$x_n$-$x_m$|< ϵ but how do you do this with |$\frac{1}{2^n}- \frac{1}{2^m}$| what I've tried: if $n\gt m$ then $$ |\frac{1}{2^n}- \frac{1}{2^m}| \le |\frac{2^m-2^n}{2^n2^m}| \le |\frac{2^m +2^n}{2^{n+m}}| \le \frac{2^n+2^n}{2^{2n}}= \frac{1}{2^{n-1}}     \le \frac{1}{2^{N-1}} \le \epsilon   $$ and rearrange to get N $ \ge 1+ \frac{ln(\epsilon)}{ln(2)}$ is this correct?,Prove directly from the definition that $({1\over2}+\frac{1}{2^2}+...+\frac{1}{2^n})_n$ is cauchy I know from the definition of Cauchy that |$x_n$-$x_m$|< ϵ but how do you do this with |$\frac{1}{2^n}- \frac{1}{2^m}$| what I've tried: if $n\gt m$ then $$ |\frac{1}{2^n}- \frac{1}{2^m}| \le |\frac{2^m-2^n}{2^n2^m}| \le |\frac{2^m +2^n}{2^{n+m}}| \le \frac{2^n+2^n}{2^{2n}}= \frac{1}{2^{n-1}}     \le \frac{1}{2^{N-1}} \le \epsilon   $$ and rearrange to get N $ \ge 1+ \frac{ln(\epsilon)}{ln(2)}$ is this correct?,,"['real-analysis', 'analysis', 'cauchy-sequences']"
55,Lerch's theorem,Lerch's theorem,,"In the context of a stats exercice, I need to show that, if for some $h$ measurable and for every real $\mu$, $\displaystyle {\int_{-\infty}^{+\infty} h\left(x \right)\cdot \exp\left(-x^2\right)\cdot \exp\left(-\mu x\right)\,\mathrm{d}x = 0}$, then $h=0\:a.e.$ The correction simply mentions that the Laplace transform is a bijection (between what spaces?), so h is $0\:a.e.$ But the only injectivity statement for the LT I could find was for well-behaved functions. Is it even true in my case ? Can anyone point me to such a proof ? Thanks EDIT: I changed the title since the result holds for continuous, of exponential order functions (Lerch).","In the context of a stats exercice, I need to show that, if for some $h$ measurable and for every real $\mu$, $\displaystyle {\int_{-\infty}^{+\infty} h\left(x \right)\cdot \exp\left(-x^2\right)\cdot \exp\left(-\mu x\right)\,\mathrm{d}x = 0}$, then $h=0\:a.e.$ The correction simply mentions that the Laplace transform is a bijection (between what spaces?), so h is $0\:a.e.$ But the only injectivity statement for the LT I could find was for well-behaved functions. Is it even true in my case ? Can anyone point me to such a proof ? Thanks EDIT: I changed the title since the result holds for continuous, of exponential order functions (Lerch).",,"['analysis', 'statistics', 'laplace-transform']"
56,inverse of function $xe^x$,inverse of function,xe^x,"Let W the Lambert function that is the inverse function of $xe^x$.Is true that if $c > 1$, then $w(cx)\leq cw(x)$ for $x\geq 0$? Can I apply the property $ln(x)-ln(ln(x))\leq W$ for $x\geq e$?","Let W the Lambert function that is the inverse function of $xe^x$.Is true that if $c > 1$, then $w(cx)\leq cw(x)$ for $x\geq 0$? Can I apply the property $ln(x)-ln(ln(x))\leq W$ for $x\geq e$?",,"['real-analysis', 'analysis', 'functions', 'exponential-function', 'special-functions']"
57,"Prob. 24, Chap. 5 in Baby Rudin: For $\alpha>1$, let $f(x) = (x+\alpha/x)/2$, $g(x) = (\alpha+x)/(1+x)$ have $\sqrt{\alpha}$ as their only fixed point","Prob. 24, Chap. 5 in Baby Rudin: For , let ,  have  as their only fixed point",\alpha>1 f(x) = (x+\alpha/x)/2 g(x) = (\alpha+x)/(1+x) \sqrt{\alpha},"Here is Prob. 24, Chap. 5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: The process described in part (c) of Exercise 22 can of course also be applied to functions that map $(0, \infty)$ to $(0, \infty)$. Fix some $\alpha > 1$, and put $$ f(x) = \frac{1}{2} \left( x + \frac{\alpha}{x} \right), \qquad g(x) = \frac{\alpha+x}{1+x}. $$   Both $f$ and $g$ have $\sqrt{\alpha}$ as their only fixed point in $(0, \infty)$. Try to explain, on the basis of properties of $f$ and $g$, why the convergence in Exercise 16, Chap. 3, is so much more rapid than it is in Exercise 17. (Compare $f^\prime$ and $g^\prime$, draw the zig-zags suggested in Exercise 22.) Do the same when $0 < \alpha < 1$. Here are the links to my posts here at Math SE on Prob. 22, Chap. 5, Prob. 16, Chap. 3, and Prob. 17, Chap. 3, in Baby Rudin, 3rd edition: Prob. 22, Chap. 5 in Baby Rudin: Fixed Points of Real Functions Prob. 16, Chap. 3 in Baby Rudin: $x_{n+1} = (x_n + \alpha/x_n)/2$, with $x_1 > \sqrt{\alpha}$, $\alpha > 0$ Prob. 17, Chap. 3 in Baby Rudin: For $\alpha > 1$, how to obtain these inequalities from this recurrence relation? My Attempt: We note that, for $0 < x < \infty$, $$ g(x) = 1 + \frac{\alpha-1}{x+1},$$   and so     $$f^\prime(x) = \frac{1}{2} \left( 1 - \frac{\alpha}{x^2} \right), \qquad g^\prime(x) = - \frac{\alpha-1}{(x+1)^2},$$    and for $\alpha > 1$, we see that, if $x > \sqrt{\alpha}$, then $x^2 > \alpha$, and so    $$0 <  f^\prime(x) < \frac{1}{2}.  $$   which implies (by part (c) of Prob. 22, Chap. 5, in Baby Rudin, 3rd edition) that the sequence $x_0 > \sqrt{\alpha}$, $x_{n+1} = f \left( x_n \right)$, for $n = 0, 1, 2, 3, \ldots$, does converge to the only fixed point of $f$, which is $\sqrt{\alpha}$. In fact, for all $n$, we have    $$ \left| x_{n+1} - x_n \right| \leq \frac{1}{2} \left| x_n - x_{n-1} \right| \leq \cdots \leq \frac{1}{2^n} \left| x_1 - x_0 \right|,  $$    and so for any $m < n$, we have    $$ \begin{align} \left| x_n - x_m \right| &\leq \left| x_n - x_{n-1} \right| + \cdots+ \left| x_{m+1} - x_m \right| \\ &\leq \left( \frac{1}{2^{n-1}} + \cdots + \frac{1}{2^m} \right) \left| x_1 - x_0 \right| \\ &= \frac{1}{2^m} \left( 1 + \cdots + \frac{1}{2^{n-m-1}} \right) \left| x_1 - x_0 \right| \\ &= \frac{1}{2^m} \frac{ 1 - \frac{1}{2^{n-m}} }{ 1 - \frac{1}{2} } \left| x_1 - x_0 \right| \\ &= \left( \frac{1}{2^{m-1}} - \frac{1}{2^{n-1}} \right) \left| x_1 - x_0 \right|, \end{align} $$   and upon letting $n \to \infty$, while keeping $m$ fixed, we obtain   $$ \left| x_m - x \right| \leq \frac{1}{2^{m-1}} \left| x_1 - x_0 \right| = \frac{1}{2^{m-1}} \left| \frac{1}{2} \left(x_0 + \frac{\alpha}{x_0} \right) - x_0 \right| = \frac{1}{2^m } \left( x_0 - \frac{\alpha}{x_0} \right), $$   for $m = 1, 2, 3, \ldots$, which gives the rate of convergence of this recursive algorithm. And, the similar situation occurs for $0 < \alpha < 1$ as well, provided we choose $x_0 > \sqrt{\alpha}$. Is my analysis correct? Or, have I erred anywhere or missed something of substance? And, what about $g$? How to analyze it? How to show what Rudin has demanded to be shown?","Here is Prob. 24, Chap. 5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: The process described in part (c) of Exercise 22 can of course also be applied to functions that map $(0, \infty)$ to $(0, \infty)$. Fix some $\alpha > 1$, and put $$ f(x) = \frac{1}{2} \left( x + \frac{\alpha}{x} \right), \qquad g(x) = \frac{\alpha+x}{1+x}. $$   Both $f$ and $g$ have $\sqrt{\alpha}$ as their only fixed point in $(0, \infty)$. Try to explain, on the basis of properties of $f$ and $g$, why the convergence in Exercise 16, Chap. 3, is so much more rapid than it is in Exercise 17. (Compare $f^\prime$ and $g^\prime$, draw the zig-zags suggested in Exercise 22.) Do the same when $0 < \alpha < 1$. Here are the links to my posts here at Math SE on Prob. 22, Chap. 5, Prob. 16, Chap. 3, and Prob. 17, Chap. 3, in Baby Rudin, 3rd edition: Prob. 22, Chap. 5 in Baby Rudin: Fixed Points of Real Functions Prob. 16, Chap. 3 in Baby Rudin: $x_{n+1} = (x_n + \alpha/x_n)/2$, with $x_1 > \sqrt{\alpha}$, $\alpha > 0$ Prob. 17, Chap. 3 in Baby Rudin: For $\alpha > 1$, how to obtain these inequalities from this recurrence relation? My Attempt: We note that, for $0 < x < \infty$, $$ g(x) = 1 + \frac{\alpha-1}{x+1},$$   and so     $$f^\prime(x) = \frac{1}{2} \left( 1 - \frac{\alpha}{x^2} \right), \qquad g^\prime(x) = - \frac{\alpha-1}{(x+1)^2},$$    and for $\alpha > 1$, we see that, if $x > \sqrt{\alpha}$, then $x^2 > \alpha$, and so    $$0 <  f^\prime(x) < \frac{1}{2}.  $$   which implies (by part (c) of Prob. 22, Chap. 5, in Baby Rudin, 3rd edition) that the sequence $x_0 > \sqrt{\alpha}$, $x_{n+1} = f \left( x_n \right)$, for $n = 0, 1, 2, 3, \ldots$, does converge to the only fixed point of $f$, which is $\sqrt{\alpha}$. In fact, for all $n$, we have    $$ \left| x_{n+1} - x_n \right| \leq \frac{1}{2} \left| x_n - x_{n-1} \right| \leq \cdots \leq \frac{1}{2^n} \left| x_1 - x_0 \right|,  $$    and so for any $m < n$, we have    $$ \begin{align} \left| x_n - x_m \right| &\leq \left| x_n - x_{n-1} \right| + \cdots+ \left| x_{m+1} - x_m \right| \\ &\leq \left( \frac{1}{2^{n-1}} + \cdots + \frac{1}{2^m} \right) \left| x_1 - x_0 \right| \\ &= \frac{1}{2^m} \left( 1 + \cdots + \frac{1}{2^{n-m-1}} \right) \left| x_1 - x_0 \right| \\ &= \frac{1}{2^m} \frac{ 1 - \frac{1}{2^{n-m}} }{ 1 - \frac{1}{2} } \left| x_1 - x_0 \right| \\ &= \left( \frac{1}{2^{m-1}} - \frac{1}{2^{n-1}} \right) \left| x_1 - x_0 \right|, \end{align} $$   and upon letting $n \to \infty$, while keeping $m$ fixed, we obtain   $$ \left| x_m - x \right| \leq \frac{1}{2^{m-1}} \left| x_1 - x_0 \right| = \frac{1}{2^{m-1}} \left| \frac{1}{2} \left(x_0 + \frac{\alpha}{x_0} \right) - x_0 \right| = \frac{1}{2^m } \left( x_0 - \frac{\alpha}{x_0} \right), $$   for $m = 1, 2, 3, \ldots$, which gives the rate of convergence of this recursive algorithm. And, the similar situation occurs for $0 < \alpha < 1$ as well, provided we choose $x_0 > \sqrt{\alpha}$. Is my analysis correct? Or, have I erred anywhere or missed something of substance? And, what about $g$? How to analyze it? How to show what Rudin has demanded to be shown?",,"['real-analysis', 'analysis', 'convergence-divergence', 'numerical-methods', 'recursive-algorithms']"
58,"Prob. 2, Chap. 5 in Baby Rudin: How is the inverse function differentiable?","Prob. 2, Chap. 5 in Baby Rudin: How is the inverse function differentiable?",,"Here is Prob. 2, Chap. 5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f^\prime(x) > 0$ in $(a, b)$. Prove that $f$ is strictly increasing in $(a, b)$, and let $g$ be its inverse function. Prove that $g$ is differentiable, and that    $$ g^\prime\left( f(x) \right) = \frac{1}{f^\prime(x)} \qquad \qquad \qquad (a < x < b). $$ My effort: Let $x_1$ and $x_2$ be any two real numbers such that    $$ a < x_1 < x_2 < b.$$    Then $f$ is differentiable on the interval $\left[ x_1, x_2 \right]$ and hence on the segment $\left( x_1, x_2 \right)$, and $f$ is of course continuous on the interval $\left[ x_1, x_2 \right]$. So, by the Mean-Value Theorem, we can find a point $x \in \left( x_1, x_2 \right)$, such that    $$ f\left( x_2 \right) - f\left( x_1 \right) = \left( x_2 - x_1 \right) f^\prime (x) > 0, $$   and so    $$  f\left( x_1 \right) <  f\left( x_2 \right) \qquad \qquad \qquad ( a< x_1 < x_2 < b), $$   and therefore $f$ is strictly increasing on $(a, b)$. As $f$ is strictly increasing on $(a, b)$, so it is injective, and therefore the inverse function $g$ exists; this $g$ is a mapping of $\mathrm{range} f$ into (rather onto) $(a, b)$, and is defined by    $$ g \left(  y \right) = x \ \mbox{ for all } \ y = f(x) \in \mathrm{range} f.$$    Thus the mapping $h$ of $(a, b)$ into $(a, b)$, given by    $$ h(x) = g \left( f(x) \right) \qquad \qquad \qquad ( a< x < b),$$   is the identity mapping. So    $$h^\prime(x) = 1 \qquad \qquad \qquad (a < x < b). \tag{1} $$   But by Theorem 5.5 in Baby Rudin (i.e. the Chain Rule), we know that if $g^\prime$ exists at each point in the range of $f$, then we obtain    $$h^\prime(x) = g^\prime\left( f(x) \right) f^\prime(x) \qquad \qquad \qquad (a < x < b). \tag{2} $$   From (1) and (2), we can conclude that    $$ g^\prime\left( f(x) \right) f^\prime(x) = 1 \qquad \qquad \qquad (a < x < b), $$   and since $f^\prime(x) > 0$ for all $x \in (a, b)$, therefore    $$ g^\prime\left( f(x) \right) = \frac{1}{f^\prime(x)  } \qquad \qquad \qquad (a < x < b). $$ Is my reasoning so far correct? If so, then how to show that the function $g$ is indeed differentiable (at each point in the range of $f$)? Or, is there a problem in my reasoning above? P.S.: After reading the above answers, I have arrived at this solution to my original problem and would be grateful for the appraisal thereof of the Math SE community. As $f$ is differentiable at each point in $(a, b)$, so $f$ is continuous in $(a, b)$, and as $f^\prime(x) > 0$ for every $x \in (a, b)$, so $f$ is strictly increasing on $(a, b)$. Thus $f$ is a continuous bijective mapping of $(a, b)$ onto the range of $f$. We now show that the range of $f$ is also an open interval. Suppose $y_1$ and $y_2$ are any two points in the range of $f$ such that $y_1 < y_2$, and suppose that $y$ is any real number such that $y_1 < y < y_2$. As $y_1$ and $y_2$ are in the range of $f$, so there exist some points $x_1$ and $x_2$ in $(a, b)$ such that $f\left(x_1 \right) = y_1$ and $f \left( x_2 \right) = y_2$; moreover, as $y_1 < y_2$ and as $f$ is strictly increasing, so we must also have $x_1 < x_2$, for otherwise we would obtain $f\left( x_1 \right) \geq f\left( x_2 \right)$. Now as $f$ is continuous on the interval $\left[ x_1, x_2 \right]$ and as $$y_1 = f \left( x_1 \right) < y < f \left( x_2 \right) = y_2,$$ so by the intermediate-value theorem for continuous functions, there is some point $x \in \left( x_1, x_2 \right)$ such that $y = f(x)$. Thus we have shown that, for any points $y_1$ and $y_2$ in the range of $f$ such that $y_1 < y_2$, the segment $\left( y_1, y_2 \right)$ is also contained in the range of $f$. That is, the range of $f$ is an interval. We now show that the range of $f$ is an open interval. For any point $y_0$ in the range of $f$, we have a unique point $x_0 \in (a, b)$ such that $y_0 = f\left(x_0\right)$. But as $a < x_0 < b$, so we also have $$ a < \frac{a+x_0}{2} < x_0 < \frac{x_0+b}{2} < b,$$ and therefore $$ f \left( \frac{a+x_0}{2} \right) < f\left( x_0 \right) < f \left( \frac{x_0+b}{2} \right);$$ that is, for any point $y_0$ in the range of $f$, we have points $f\left(\frac{a+x_0}{2} \right)$ and $f \left( \frac{x_0+b}{2} \right)$ in the range of $f$ such that $$ f\left(\frac{a+x_0}{2} \right) < y_0 < f \left( \frac{x_0+b}{2} \right).$$ So the range of $f$ has no maximum element and no minimum element. Hence the range of $f$ is a possibly infinite (on either side) open interval, say $(c, d)$. Let the function $g \colon (c, d) \to (a, b)$ be the inverse of the function $f \colon (a, b) \to (c, d)$. We show that $g$ is continuous by showing that the inverse image under $g$ of every open set in $(a, b)$ is open in $(c, d)$. For this it suffices to show that the inverse image under $g$ of every open interval $(u, v) \subset (a, b)$ is open in $(c, d)$. But as $f$ and $g$ are bijective and are the inverses of each other, so $$g^{-1} \left[ (u, v) \right] = f \left[ (u, v) \right]. $$   We now show that $$f \left[ (u, v) \right] = \left( f(u), f(v) \right).$$    Suppose $y \in f \left[ (u, v) \right]$. Then $y = f(x)$ for a (unique) point $x \in (u, v)$. As $a < u < x < v < b$ and as $f$ is strictly increasing on $(a, b)$, so we must have $f(u) < f(x) < f(v)$, that is, $y \in \left( f(u), f(v) \right)$. Conversely, suppose that $y \in \left( f(u), f(v) \right)$. Then $f(u) < y < f(v)$, and $f$ is continuous on the closed interval $[u, v]$; so (by the intermediate-value theorem for continuous functions)  there is some (unique) point $x \in (u, v)$ such that $y = f(x)$, which implies that $y \in f\left[ (u, v) \right]$. Therefore, $f \left[ (u, v) \right] = \left( f(u), f(v) \right)$, which is open in $(c, d)$, as required. Thus the inverse function $g \colon (c, d) \to (a, b)$ is continuous whenever $f^\prime(x) > 0$ for all $x \in (a, b)$. Now let $y_0 \in (c, d)$. Then there is a unique point $x_0 \in (a, b)$ such that $y_0 = f\left( x_0 \right)$, which implies that $g \left( y_0 \right) = x_0$. Since $g$ is continuous at $y_0$, so $$ \lim_{y \to y_0} g(y) = g\left( y_0 \right); $$    that is, $$\lim_{y \to y_0} x = x_0,  \ \mbox{ where } \ x = g(y); $$   thus, as $y \to y_0$ in $(c, d)$, the point $x = g(y) \to x_0$ in $(a, b)$. Now as $f^\prime\left( x_0 \right) \neq 0$, so we find that    $$g^\prime \left( y_0 \right) = \lim_{y \to y_0 } \frac{ g(y) - g\left( y_0 \right) }{ y - y_0 } = \lim_{x \to x_0 }  \frac{ x - x_0 }{ f(x) - f \left( x_0 \right) } = \lim_{x \to x_0 } \frac{ 1 }{ \frac{ f(x) - f\left( x_0 \right) }{ x - x_0 } } = \frac{1}{ f^\prime \left( x_0 \right) },$$    which shows that $g^\prime$ exists at each point $y_0 \in (c, d)$, and also that $$ g^\prime \left( y_0 \right) = \frac{1}{ f^\prime \left( x_0 \right) },$$ as required. Have I finally managed to get this solution correct?","Here is Prob. 2, Chap. 5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f^\prime(x) > 0$ in $(a, b)$. Prove that $f$ is strictly increasing in $(a, b)$, and let $g$ be its inverse function. Prove that $g$ is differentiable, and that    $$ g^\prime\left( f(x) \right) = \frac{1}{f^\prime(x)} \qquad \qquad \qquad (a < x < b). $$ My effort: Let $x_1$ and $x_2$ be any two real numbers such that    $$ a < x_1 < x_2 < b.$$    Then $f$ is differentiable on the interval $\left[ x_1, x_2 \right]$ and hence on the segment $\left( x_1, x_2 \right)$, and $f$ is of course continuous on the interval $\left[ x_1, x_2 \right]$. So, by the Mean-Value Theorem, we can find a point $x \in \left( x_1, x_2 \right)$, such that    $$ f\left( x_2 \right) - f\left( x_1 \right) = \left( x_2 - x_1 \right) f^\prime (x) > 0, $$   and so    $$  f\left( x_1 \right) <  f\left( x_2 \right) \qquad \qquad \qquad ( a< x_1 < x_2 < b), $$   and therefore $f$ is strictly increasing on $(a, b)$. As $f$ is strictly increasing on $(a, b)$, so it is injective, and therefore the inverse function $g$ exists; this $g$ is a mapping of $\mathrm{range} f$ into (rather onto) $(a, b)$, and is defined by    $$ g \left(  y \right) = x \ \mbox{ for all } \ y = f(x) \in \mathrm{range} f.$$    Thus the mapping $h$ of $(a, b)$ into $(a, b)$, given by    $$ h(x) = g \left( f(x) \right) \qquad \qquad \qquad ( a< x < b),$$   is the identity mapping. So    $$h^\prime(x) = 1 \qquad \qquad \qquad (a < x < b). \tag{1} $$   But by Theorem 5.5 in Baby Rudin (i.e. the Chain Rule), we know that if $g^\prime$ exists at each point in the range of $f$, then we obtain    $$h^\prime(x) = g^\prime\left( f(x) \right) f^\prime(x) \qquad \qquad \qquad (a < x < b). \tag{2} $$   From (1) and (2), we can conclude that    $$ g^\prime\left( f(x) \right) f^\prime(x) = 1 \qquad \qquad \qquad (a < x < b), $$   and since $f^\prime(x) > 0$ for all $x \in (a, b)$, therefore    $$ g^\prime\left( f(x) \right) = \frac{1}{f^\prime(x)  } \qquad \qquad \qquad (a < x < b). $$ Is my reasoning so far correct? If so, then how to show that the function $g$ is indeed differentiable (at each point in the range of $f$)? Or, is there a problem in my reasoning above? P.S.: After reading the above answers, I have arrived at this solution to my original problem and would be grateful for the appraisal thereof of the Math SE community. As $f$ is differentiable at each point in $(a, b)$, so $f$ is continuous in $(a, b)$, and as $f^\prime(x) > 0$ for every $x \in (a, b)$, so $f$ is strictly increasing on $(a, b)$. Thus $f$ is a continuous bijective mapping of $(a, b)$ onto the range of $f$. We now show that the range of $f$ is also an open interval. Suppose $y_1$ and $y_2$ are any two points in the range of $f$ such that $y_1 < y_2$, and suppose that $y$ is any real number such that $y_1 < y < y_2$. As $y_1$ and $y_2$ are in the range of $f$, so there exist some points $x_1$ and $x_2$ in $(a, b)$ such that $f\left(x_1 \right) = y_1$ and $f \left( x_2 \right) = y_2$; moreover, as $y_1 < y_2$ and as $f$ is strictly increasing, so we must also have $x_1 < x_2$, for otherwise we would obtain $f\left( x_1 \right) \geq f\left( x_2 \right)$. Now as $f$ is continuous on the interval $\left[ x_1, x_2 \right]$ and as $$y_1 = f \left( x_1 \right) < y < f \left( x_2 \right) = y_2,$$ so by the intermediate-value theorem for continuous functions, there is some point $x \in \left( x_1, x_2 \right)$ such that $y = f(x)$. Thus we have shown that, for any points $y_1$ and $y_2$ in the range of $f$ such that $y_1 < y_2$, the segment $\left( y_1, y_2 \right)$ is also contained in the range of $f$. That is, the range of $f$ is an interval. We now show that the range of $f$ is an open interval. For any point $y_0$ in the range of $f$, we have a unique point $x_0 \in (a, b)$ such that $y_0 = f\left(x_0\right)$. But as $a < x_0 < b$, so we also have $$ a < \frac{a+x_0}{2} < x_0 < \frac{x_0+b}{2} < b,$$ and therefore $$ f \left( \frac{a+x_0}{2} \right) < f\left( x_0 \right) < f \left( \frac{x_0+b}{2} \right);$$ that is, for any point $y_0$ in the range of $f$, we have points $f\left(\frac{a+x_0}{2} \right)$ and $f \left( \frac{x_0+b}{2} \right)$ in the range of $f$ such that $$ f\left(\frac{a+x_0}{2} \right) < y_0 < f \left( \frac{x_0+b}{2} \right).$$ So the range of $f$ has no maximum element and no minimum element. Hence the range of $f$ is a possibly infinite (on either side) open interval, say $(c, d)$. Let the function $g \colon (c, d) \to (a, b)$ be the inverse of the function $f \colon (a, b) \to (c, d)$. We show that $g$ is continuous by showing that the inverse image under $g$ of every open set in $(a, b)$ is open in $(c, d)$. For this it suffices to show that the inverse image under $g$ of every open interval $(u, v) \subset (a, b)$ is open in $(c, d)$. But as $f$ and $g$ are bijective and are the inverses of each other, so $$g^{-1} \left[ (u, v) \right] = f \left[ (u, v) \right]. $$   We now show that $$f \left[ (u, v) \right] = \left( f(u), f(v) \right).$$    Suppose $y \in f \left[ (u, v) \right]$. Then $y = f(x)$ for a (unique) point $x \in (u, v)$. As $a < u < x < v < b$ and as $f$ is strictly increasing on $(a, b)$, so we must have $f(u) < f(x) < f(v)$, that is, $y \in \left( f(u), f(v) \right)$. Conversely, suppose that $y \in \left( f(u), f(v) \right)$. Then $f(u) < y < f(v)$, and $f$ is continuous on the closed interval $[u, v]$; so (by the intermediate-value theorem for continuous functions)  there is some (unique) point $x \in (u, v)$ such that $y = f(x)$, which implies that $y \in f\left[ (u, v) \right]$. Therefore, $f \left[ (u, v) \right] = \left( f(u), f(v) \right)$, which is open in $(c, d)$, as required. Thus the inverse function $g \colon (c, d) \to (a, b)$ is continuous whenever $f^\prime(x) > 0$ for all $x \in (a, b)$. Now let $y_0 \in (c, d)$. Then there is a unique point $x_0 \in (a, b)$ such that $y_0 = f\left( x_0 \right)$, which implies that $g \left( y_0 \right) = x_0$. Since $g$ is continuous at $y_0$, so $$ \lim_{y \to y_0} g(y) = g\left( y_0 \right); $$    that is, $$\lim_{y \to y_0} x = x_0,  \ \mbox{ where } \ x = g(y); $$   thus, as $y \to y_0$ in $(c, d)$, the point $x = g(y) \to x_0$ in $(a, b)$. Now as $f^\prime\left( x_0 \right) \neq 0$, so we find that    $$g^\prime \left( y_0 \right) = \lim_{y \to y_0 } \frac{ g(y) - g\left( y_0 \right) }{ y - y_0 } = \lim_{x \to x_0 }  \frac{ x - x_0 }{ f(x) - f \left( x_0 \right) } = \lim_{x \to x_0 } \frac{ 1 }{ \frac{ f(x) - f\left( x_0 \right) }{ x - x_0 } } = \frac{1}{ f^\prime \left( x_0 \right) },$$    which shows that $g^\prime$ exists at each point $y_0 \in (c, d)$, and also that $$ g^\prime \left( y_0 \right) = \frac{1}{ f^\prime \left( x_0 \right) },$$ as required. Have I finally managed to get this solution correct?",,"['calculus', 'real-analysis', 'analysis', 'derivatives', 'inverse-function']"
59,"How can I show that $\left|\sin \frac{s}{2}\right| \geq \frac{|s|}{\pi}, s \in [- \pi , \pi]$?",How can I show that ?,"\left|\sin \frac{s}{2}\right| \geq \frac{|s|}{\pi}, s \in [- \pi , \pi]","How can I show that $$\left|\sin \frac{s}{2}\right| \geq \frac{|s|}{\pi}$$ $s \in [- \pi , \pi]$, using that $\psi : x \mapsto \sin x$ is a concave function on $[0 , \pi]$? By definition of concave function, $$ \psi(t \, x + (1 - t) \, y) \geq t \, \psi(x) + (1 - t) \, \psi(y) $$ for each $x , y \in [0 , \pi]$ and for all $t \in [0 , 1]$. My thought was obtain that using just the definition but I think it is not possible but I am not sure. I have drown that and it's obvious but I want to prove that analytically. Can you help me, please? Thank you very much.","How can I show that $$\left|\sin \frac{s}{2}\right| \geq \frac{|s|}{\pi}$$ $s \in [- \pi , \pi]$, using that $\psi : x \mapsto \sin x$ is a concave function on $[0 , \pi]$? By definition of concave function, $$ \psi(t \, x + (1 - t) \, y) \geq t \, \psi(x) + (1 - t) \, \psi(y) $$ for each $x , y \in [0 , \pi]$ and for all $t \in [0 , 1]$. My thought was obtain that using just the definition but I think it is not possible but I am not sure. I have drown that and it's obvious but I want to prove that analytically. Can you help me, please? Thank you very much.",,"['real-analysis', 'analysis', 'inequality', 'absolute-value', 'convexity-inequality']"
60,"Prob. 12, Chap. 4 in Baby Rudin: A uniformly continuous function of a uniformly continuous function is uniformly continuous","Prob. 12, Chap. 4 in Baby Rudin: A uniformly continuous function of a uniformly continuous function is uniformly continuous",,"Here is Prob. 12, Chap. 4 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: A uniformly continuous function of a uniformly continuous function is uniformly continuous. State this more precisely and prove it. Here is my effort: Theorem:** Let $\left(X, d_X\right)$ , $\left(Y, d_Y \right)$ , and $\left( Z, d_Z \right)$ be metric spaces, let $f$ be a uniformly continuous mapping of $X$ into $Y$ , let $g$ be a uniformly continuous mapping of $f(X)$ into $Z$ , and let $h = g \circ f$ . Then $h$ is a uniformly continuous mapping of $X$ into $Z$ . Proof:** Let $\varepsilon$ be a given real number such that $\varepsilon > 0$ . Since $g$ is a unifromly continuous mapping of $f(X)$ into $Z$ , we can find a real number $\eta > 0$ such that $$\tag{1} d_Z \left( g \left( y_1 \right), g \left( y_2 \right) \right) < \varepsilon$$ for any points $y_1$ and $y_2$ in $f(X)$ for which $$\tag{2} d_Y \left( y_1, y_2 \right) < \eta.$$ Now as $f$ is a uniformly continuous mapping of $X$ into $Y$ , so, corresponding to the real number $\eta > 0$ in particular, we can find a real number $\delta > 0$ such that $$ \tag{3} d_Y \left( f \left(x_1 \right), f \left( x_2 \right)  \right) < \eta$$ for any points $x_1$ and $x_2$ in $X$ for which $$ \tag{4} d_X \left( x_1, x_2 \right) < \delta.$$ So, we can conclude from (1), (2), (3), (4) above that, for any points $x_1$ and $x_2$ in $X$ which satisfy $$d_X \left( x_1, x_2 \right) < \delta,$$ the following is true. $$ d_Z \left( h \left(x_1 \right), h\left( x_2 \right) \right)  = d_Z \left( g\left( f\left( x_1 \right) \right), g\left( f\left( x_2 \right) \right) \right) < \varepsilon, $$ from which it follows that $h = g \circ f$ is a uniformly continuous mapping of $X$ into $Z$ . Have I managed to get the statement of the theorem right? If so, then is my proof correct?","Here is Prob. 12, Chap. 4 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: A uniformly continuous function of a uniformly continuous function is uniformly continuous. State this more precisely and prove it. Here is my effort: Theorem:** Let , , and be metric spaces, let be a uniformly continuous mapping of into , let be a uniformly continuous mapping of into , and let . Then is a uniformly continuous mapping of into . Proof:** Let be a given real number such that . Since is a unifromly continuous mapping of into , we can find a real number such that for any points and in for which Now as is a uniformly continuous mapping of into , so, corresponding to the real number in particular, we can find a real number such that for any points and in for which So, we can conclude from (1), (2), (3), (4) above that, for any points and in which satisfy the following is true. from which it follows that is a uniformly continuous mapping of into . Have I managed to get the statement of the theorem right? If so, then is my proof correct?","\left(X, d_X\right) \left(Y, d_Y \right) \left( Z, d_Z \right) f X Y g f(X) Z h = g \circ f h X Z \varepsilon \varepsilon > 0 g f(X) Z \eta > 0 \tag{1} d_Z \left( g \left( y_1 \right), g \left( y_2 \right) \right) < \varepsilon y_1 y_2 f(X) \tag{2} d_Y \left( y_1, y_2 \right) < \eta. f X Y \eta > 0 \delta > 0  \tag{3} d_Y \left( f \left(x_1 \right), f \left( x_2 \right)  \right) < \eta x_1 x_2 X  \tag{4} d_X \left( x_1, x_2 \right) < \delta. x_1 x_2 X d_X \left( x_1, x_2 \right) < \delta,  d_Z \left( h \left(x_1 \right), h\left( x_2 \right) \right)  = d_Z \left( g\left( f\left( x_1 \right) \right), g\left( f\left( x_2 \right) \right) \right) < \varepsilon,  h = g \circ f X Z","['real-analysis', 'analysis', 'metric-spaces', 'continuity', 'uniform-continuity']"
61,Proof about continuity of a function involving the Banach fixed point theorem,Proof about continuity of a function involving the Banach fixed point theorem,,"Be $X$ and $\Lambda$ metric spaces, with $X$ complete, and $f\in C(X\times\Lambda,X)$. Suppose that exists some $\alpha\in[0,1)$ and, for each $\lambda\in\Lambda$, some $q(\lambda)\in[0,\alpha]$ such that   $$d(f(x,\lambda),f(y,\lambda))\le q(\lambda) d(x,y),\quad\forall x,y\in X$$   By the Banach fixed point theorem, for each $\lambda\in\Lambda$, $f(\cdot,\lambda)$ has a unique fixed point $x(\lambda)$. Prove that $[\lambda\mapsto x(\lambda)]\in C(\Lambda,X)$. Im totally stuck with this exercise, I dont have a clue about what to do... I tried to show the continuity of $h$ defined as $$h:\Lambda\to X,\quad \lambda\mapsto x(\lambda)$$ trough the $\epsilon-\delta$ definition of continuity of $f$ and the information of the problem but I cant do it. Geometrically is easy to see it veracity because the function $h$ is just the intersection of $f$ with the plane defined by the set $\{\langle x,y\rangle\in X\times X:x=y\}\times\lambda$. My work: from the continuity of $f$ we have that for any fixed point $x:=f(x,\lambda_x)$ for any $\epsilon>0$ exists a $\delta>0$ such that $$d(\langle y,\lambda\rangle,\langle x,\lambda_x\rangle)<\delta\implies d(f(y,\lambda),x)<\epsilon$$ If we set $\langle y,\lambda\rangle=\langle x_0,\lambda_x\rangle$ then for the sequence defined as $$x_n:=f(x_{n-1},\lambda_x)$$ that converges to the fixed point $x$, we can rewrite the above as $$d(\langle x_0,\lambda_x\rangle,\langle x,\lambda_x\rangle)<\delta\implies d(x_1,x)<\epsilon\tag{1}$$ and from the contraction of $g_\lambda:=f(x,\lambda)$ for fixed $\lambda\in\Lambda$ we knows that $$d(f(x_0,\lambda_x),x)=d(x_1,x)\le q(\lambda_x) d(x_0,x)\tag{2}$$ for a fixed point $x$ (with any $0\le q(\lambda)<1$, hence the contraction). Moreover: we can suppose that the metric in $X\times\Lambda$ is the standard product metric, then: $$d(\langle a,b\rangle,\langle c,d\rangle)=\max\{d(a,c),d(b,d)\}\tag{3}$$ Then applying $(3)$ in $(1)$ we get $$d(x_0,x)<\delta\implies d(x_1,x)<\epsilon$$ But as I said Im stuck, I dont know how to show the desired continuity of $h$. Probably the last two (or three) identities are useless, I just take them to see if I can get something from there. Some help will be appreciated, thank you.","Be $X$ and $\Lambda$ metric spaces, with $X$ complete, and $f\in C(X\times\Lambda,X)$. Suppose that exists some $\alpha\in[0,1)$ and, for each $\lambda\in\Lambda$, some $q(\lambda)\in[0,\alpha]$ such that   $$d(f(x,\lambda),f(y,\lambda))\le q(\lambda) d(x,y),\quad\forall x,y\in X$$   By the Banach fixed point theorem, for each $\lambda\in\Lambda$, $f(\cdot,\lambda)$ has a unique fixed point $x(\lambda)$. Prove that $[\lambda\mapsto x(\lambda)]\in C(\Lambda,X)$. Im totally stuck with this exercise, I dont have a clue about what to do... I tried to show the continuity of $h$ defined as $$h:\Lambda\to X,\quad \lambda\mapsto x(\lambda)$$ trough the $\epsilon-\delta$ definition of continuity of $f$ and the information of the problem but I cant do it. Geometrically is easy to see it veracity because the function $h$ is just the intersection of $f$ with the plane defined by the set $\{\langle x,y\rangle\in X\times X:x=y\}\times\lambda$. My work: from the continuity of $f$ we have that for any fixed point $x:=f(x,\lambda_x)$ for any $\epsilon>0$ exists a $\delta>0$ such that $$d(\langle y,\lambda\rangle,\langle x,\lambda_x\rangle)<\delta\implies d(f(y,\lambda),x)<\epsilon$$ If we set $\langle y,\lambda\rangle=\langle x_0,\lambda_x\rangle$ then for the sequence defined as $$x_n:=f(x_{n-1},\lambda_x)$$ that converges to the fixed point $x$, we can rewrite the above as $$d(\langle x_0,\lambda_x\rangle,\langle x,\lambda_x\rangle)<\delta\implies d(x_1,x)<\epsilon\tag{1}$$ and from the contraction of $g_\lambda:=f(x,\lambda)$ for fixed $\lambda\in\Lambda$ we knows that $$d(f(x_0,\lambda_x),x)=d(x_1,x)\le q(\lambda_x) d(x_0,x)\tag{2}$$ for a fixed point $x$ (with any $0\le q(\lambda)<1$, hence the contraction). Moreover: we can suppose that the metric in $X\times\Lambda$ is the standard product metric, then: $$d(\langle a,b\rangle,\langle c,d\rangle)=\max\{d(a,c),d(b,d)\}\tag{3}$$ Then applying $(3)$ in $(1)$ we get $$d(x_0,x)<\delta\implies d(x_1,x)<\epsilon$$ But as I said Im stuck, I dont know how to show the desired continuity of $h$. Probably the last two (or three) identities are useless, I just take them to see if I can get something from there. Some help will be appreciated, thank you.",,"['analysis', 'continuity', 'banach-fixed-point']"
62,Visual Explanation of the Dominated Convergence Theorem,Visual Explanation of the Dominated Convergence Theorem,,"From a pedagogical perspective, is there a way to explain to the Dominated Convergence Theorem using visual examples? This may sound tasteless for mathematicians but many times, engineering students do not have exposure to the ways of thinking that mathematicians are comfortable with. I was hoping that with examples of plots of sequence of functions $f_n(x)$, there must be an easy way of explaining this beautiful result. Your help and creative ways to explaining this theorem are appreciated.","From a pedagogical perspective, is there a way to explain to the Dominated Convergence Theorem using visual examples? This may sound tasteless for mathematicians but many times, engineering students do not have exposure to the ways of thinking that mathematicians are comfortable with. I was hoping that with examples of plots of sequence of functions $f_n(x)$, there must be an easy way of explaining this beautiful result. Your help and creative ways to explaining this theorem are appreciated.",,"['analysis', 'education']"
63,Calculate the integral $\int_{-\infty}^\infty\frac{dy}{(1+y^2)(1+[x-y]^2)}$,Calculate the integral,\int_{-\infty}^\infty\frac{dy}{(1+y^2)(1+[x-y]^2)},"Calculate the integral for $x\in\mathbb{R}$   $$\int_{-\infty}^\infty\frac{dy}{(1+y^2)(1+[x-y]^2)}$$where $[\ ]$ is the floor function. By using the fitting tool of MATLAB I'm almost certain that the answer is $$\frac{2\pi}{(x-0.5)^2+4}$$compared with the result of an easier integral $$\int_{-\infty}^{\infty}\frac{dy}{(1+y^2)(1+(x-y)^2)}=\frac{2\pi}{x^2+4}$$ Noting that the wanting integral is $f(x)*f([x])$ if $f(x)=1/(1+x^2)$, I tried to calculate $\mathcal{F}^{-1}(\mathcal{F}(f(x))\cdot\mathcal{F}(f([x])))$ by convolution theorem. But things didn't get any simpler.","Calculate the integral for $x\in\mathbb{R}$   $$\int_{-\infty}^\infty\frac{dy}{(1+y^2)(1+[x-y]^2)}$$where $[\ ]$ is the floor function. By using the fitting tool of MATLAB I'm almost certain that the answer is $$\frac{2\pi}{(x-0.5)^2+4}$$compared with the result of an easier integral $$\int_{-\infty}^{\infty}\frac{dy}{(1+y^2)(1+(x-y)^2)}=\frac{2\pi}{x^2+4}$$ Noting that the wanting integral is $f(x)*f([x])$ if $f(x)=1/(1+x^2)$, I tried to calculate $\mathcal{F}^{-1}(\mathcal{F}(f(x))\cdot\mathcal{F}(f([x])))$ by convolution theorem. But things didn't get any simpler.",,"['calculus', 'real-analysis', 'analysis']"
64,Characterizations of $e^x$,Characterizations of,e^x,"I've been thinking about the following problem for a while: (AFAIK) the 'exponential function', $e^x$ can be characterized as the unique solution to the following differential equation with initial conditions specified: $f'(x) = f(x)$ , $f(0) = 1$. Prior to learning this, I thought of $e^x$ in the following way: I first thought of $e$ to be the supremum of the sequence $u_n =(1+ \frac{1}{n})^n$, and since $u_n$ can be shown to be monotone increasing and bounded above, this is the same as defining it to be $e = \lim_{n \to \infty} (1+\frac{1}{n})^n$ I then showed that via this characterization, we can also show that $e= lim_{h \to 0} (1+h)^{\frac{1}{h}}$, which allows us to show that the function $e^x$ has a nice property, namely, that its own derivative is itself, which immediately makes it $C^{\infty}$. Using this, as well as the Lagrange form of the remainder for the Taylor polynomial, I was then able to show that $\displaystyle e^x = \sum_{i=0}^{\infty} \frac{x^i}{i!}$ I then wanted to try and show that the characterization of $e^x$ as the unique function that satisfies: $f'(x) = f(x)$ , $f(0) = 1$. and the characterization I previously thought about were equivalent. So I started with showing that $f'(x) = f(x)$ , $f(0) = 1$ implied my previous characterization, as my previous characterization is a solution to $f'(x) = f(x)$ , $f(0) = 1$ I noted that as $f$ was defined at $0$, and that $f$ had a first derivative at $0$, it can be inferred that $f$ is continuous at $0$, and exists in some neighborhood, lets say $(-k,k)$ of $0$. Now I also realized if $f$ is a solution to this differential equation, it is $C^{\infty}$, and all of its derivatives must exist in the neighborhood $(-k,k)$ of $0$. Then I applied the Lagrange form of the remainder to this function to show that the error as this function is approximated with $\sum_{i=0}^n \frac{x^i}{i!}$ tends to $0$ as $n \to \infty$. So, in $(-k,k)$, the function must be $e^x$, as $e^x$ can also be made arbitrarily close to $\sum_{i=0}^n \frac{x^i}{i!}$ given $n$ large enough. Okay, so I understand that $f(x) = e^x$ in $(-k,k)$, and I realize that $\sum_{i=0}^{\infty} \frac{x^i}{i!}$ has an interval of convergence of $\mathbb{R}$, where it is uniformly convergent, and so $f$ even has a smooth uniformly convergent continuation outside $(-k,k)$, which is also just $e^x$. But this is what I am struggling to understand, why does $f$ need to be defined on all of $\mathbb{R}$, as in, why does $f$ need to actually be the same function as $e^x$, same domain and all? Surely $f$ could be defined on any arbitrarily small interval $(-k,k)$ around zero, and still fulfill its defining differential equation, but not be defined outside of $(-k,k)$, so while within $(-k,k)$ it is $e^x$, it may not exist outside $(-k,k)$ as itself, but only as a smooth unique continuation of itself (namely, $e^x$). Another problem I have related to this is when talking about solutions to the following differential equation (again with specified initial conditions): $f''(x) = -kf(x)$, $f(0) = A$, $f'(0) = 0$, I realize that $f(x)$ is essentially the cosine function, and that this is in fact the characterizing equation of simple harmonic motion, which a pendulum approximates when it swings back and forth with small incline angle. I could show this in a similar way, by talking about how $f$ is again $C^{\infty}$ and then showing that within whatever interval its defined in its equal to a function that is basically cosine, but now my question is, what makes us believe that $f(x)$, which can be thought of as the displacement of a particle doing simple harmonic motion, is defined outside a small open interval containing $t=0$? How can we model these particles with cosine in real life if you can't directly conclude all solutions to $f''(x) = -kf(x)$, $f(0) = A$ have to be defined themselves on all of $\mathbb{R}$ (and not as themselves on $(-k,k)$ and then as a unique smooth continuation (cosine) on $\mathbb{R} \setminus (-k,k)$). I would really appreciate it if someone were to help me understand this better, and I am sorry if I have been unclear","I've been thinking about the following problem for a while: (AFAIK) the 'exponential function', $e^x$ can be characterized as the unique solution to the following differential equation with initial conditions specified: $f'(x) = f(x)$ , $f(0) = 1$. Prior to learning this, I thought of $e^x$ in the following way: I first thought of $e$ to be the supremum of the sequence $u_n =(1+ \frac{1}{n})^n$, and since $u_n$ can be shown to be monotone increasing and bounded above, this is the same as defining it to be $e = \lim_{n \to \infty} (1+\frac{1}{n})^n$ I then showed that via this characterization, we can also show that $e= lim_{h \to 0} (1+h)^{\frac{1}{h}}$, which allows us to show that the function $e^x$ has a nice property, namely, that its own derivative is itself, which immediately makes it $C^{\infty}$. Using this, as well as the Lagrange form of the remainder for the Taylor polynomial, I was then able to show that $\displaystyle e^x = \sum_{i=0}^{\infty} \frac{x^i}{i!}$ I then wanted to try and show that the characterization of $e^x$ as the unique function that satisfies: $f'(x) = f(x)$ , $f(0) = 1$. and the characterization I previously thought about were equivalent. So I started with showing that $f'(x) = f(x)$ , $f(0) = 1$ implied my previous characterization, as my previous characterization is a solution to $f'(x) = f(x)$ , $f(0) = 1$ I noted that as $f$ was defined at $0$, and that $f$ had a first derivative at $0$, it can be inferred that $f$ is continuous at $0$, and exists in some neighborhood, lets say $(-k,k)$ of $0$. Now I also realized if $f$ is a solution to this differential equation, it is $C^{\infty}$, and all of its derivatives must exist in the neighborhood $(-k,k)$ of $0$. Then I applied the Lagrange form of the remainder to this function to show that the error as this function is approximated with $\sum_{i=0}^n \frac{x^i}{i!}$ tends to $0$ as $n \to \infty$. So, in $(-k,k)$, the function must be $e^x$, as $e^x$ can also be made arbitrarily close to $\sum_{i=0}^n \frac{x^i}{i!}$ given $n$ large enough. Okay, so I understand that $f(x) = e^x$ in $(-k,k)$, and I realize that $\sum_{i=0}^{\infty} \frac{x^i}{i!}$ has an interval of convergence of $\mathbb{R}$, where it is uniformly convergent, and so $f$ even has a smooth uniformly convergent continuation outside $(-k,k)$, which is also just $e^x$. But this is what I am struggling to understand, why does $f$ need to be defined on all of $\mathbb{R}$, as in, why does $f$ need to actually be the same function as $e^x$, same domain and all? Surely $f$ could be defined on any arbitrarily small interval $(-k,k)$ around zero, and still fulfill its defining differential equation, but not be defined outside of $(-k,k)$, so while within $(-k,k)$ it is $e^x$, it may not exist outside $(-k,k)$ as itself, but only as a smooth unique continuation of itself (namely, $e^x$). Another problem I have related to this is when talking about solutions to the following differential equation (again with specified initial conditions): $f''(x) = -kf(x)$, $f(0) = A$, $f'(0) = 0$, I realize that $f(x)$ is essentially the cosine function, and that this is in fact the characterizing equation of simple harmonic motion, which a pendulum approximates when it swings back and forth with small incline angle. I could show this in a similar way, by talking about how $f$ is again $C^{\infty}$ and then showing that within whatever interval its defined in its equal to a function that is basically cosine, but now my question is, what makes us believe that $f(x)$, which can be thought of as the displacement of a particle doing simple harmonic motion, is defined outside a small open interval containing $t=0$? How can we model these particles with cosine in real life if you can't directly conclude all solutions to $f''(x) = -kf(x)$, $f(0) = A$ have to be defined themselves on all of $\mathbb{R}$ (and not as themselves on $(-k,k)$ and then as a unique smooth continuation (cosine) on $\mathbb{R} \setminus (-k,k)$). I would really appreciate it if someone were to help me understand this better, and I am sorry if I have been unclear",,"['real-analysis', 'analysis', 'power-series', 'taylor-expansion', 'euler-mascheroni-constant']"
65,Whether a real number is a dyadic rational iff its binary expansion terminates?,Whether a real number is a dyadic rational iff its binary expansion terminates?,,"In self-studying a textbook on computability theory, I found that many of the exercises depend on the following factlet: A dyadic rational is a rational number whose denominator is a power of two, i.e. a rational number of the form $\frac{a}{2^b}$. A real number is a dyadic rational if and only if its binary expansion terminates. I have the following for the forward direction: The binary expansion of a number between $0$ and $1$ is of the form     \begin{equation*}     0.x_1x_2x_3\cdots = \sum_{k=1}^{\infty}x_k2^{-k} = \sum_{k=1}^{\infty}\frac{x_k}{2^k}   \end{equation*}     Suppose a number $0 < x < 1$ has a terminating binary expansion.     Then its expansion is of the form $0.x_1\cdots x_k$, where $x_k$ is the last $1$ digit.     Then     \begin{equation*}     x = \frac{x_1}{2^1} + \frac{x_2}{2^2} + \ldots + \frac{x_k}{2^k} = \frac{x_12^{k-1} + x_22^{k-2} + \ldots + x_k}{2^k}   \end{equation*}     Since this is base-$2$, each $x_i$ must be either $0$ or $1$, whence it follows that the denominator and numerator are integers, and the denominator is a power of two, which means $x$ is a dyadic rational. For the converse, I have the following idea, but do not have the background to write up a rigorous proof (in particular, I cannot imagine how to deal with the ambiguity of infinite $1$s versus infinite $0$s at some point in the expansion): Conversely, we must show that if $0 < \frac{a}{2^b} < 1$ is a dyadic rational, then its binary expansion terminates.     Every dyadic rational can be represented as the finite sum/product $\left(\frac{1}{2} + \ldots + \frac{1}{2}\right)\frac{1}{2}\cdot\ldots\cdot\frac{1}{2}$.     The binary expansion of the sum of two numbers with terminating binary expansions terminates, same for the product; and it follows that the binary expansion of a dyadic rational terminates. I have not yet formally tackled (but have vague, possibly incorrect, intuition of) the construction of real numbers, Cauchy convergence and proof by induction (which I gather could be used somehow...), but I need to convince myself of the factlet and its possible pitfalls to continue with the material for the time being (namely, Cantor's diagonalization proofs). Any detailed hints or full-on proofs would be greatly appreciated. (I have found this question but the hint in the answer seems unhelpful given my lack of background.) EDIT : Observe that $a$ is a finite integer, and so can be written as the finite-term sum $x_12^{k-1} + x_22^{k-2} + \ldots + x_k$, where $x_i \in \{0, 1\}$.     Since $\frac{a}{2^b} < 1$, it follows that $a < 2^b$.     By the definition of binary expansion, $x_1 = 1$, whence $2^{k-1} \le a < 2^b$, and we have $k-1<b$.     But $1 \le i \le k$, whence $k-i<b$.     Then we can divide each term by $2^b$, obtaining:     \begin{equation*}     \frac{a}{2^b} = \frac{x_1}{2^{b-k+1}} + \frac{x_2}{2^{b-k+2}} + \ldots + \frac{x_k}{2^b},   \end{equation*}     which gives us a finite binary expansion; this completes the proof.","In self-studying a textbook on computability theory, I found that many of the exercises depend on the following factlet: A dyadic rational is a rational number whose denominator is a power of two, i.e. a rational number of the form $\frac{a}{2^b}$. A real number is a dyadic rational if and only if its binary expansion terminates. I have the following for the forward direction: The binary expansion of a number between $0$ and $1$ is of the form     \begin{equation*}     0.x_1x_2x_3\cdots = \sum_{k=1}^{\infty}x_k2^{-k} = \sum_{k=1}^{\infty}\frac{x_k}{2^k}   \end{equation*}     Suppose a number $0 < x < 1$ has a terminating binary expansion.     Then its expansion is of the form $0.x_1\cdots x_k$, where $x_k$ is the last $1$ digit.     Then     \begin{equation*}     x = \frac{x_1}{2^1} + \frac{x_2}{2^2} + \ldots + \frac{x_k}{2^k} = \frac{x_12^{k-1} + x_22^{k-2} + \ldots + x_k}{2^k}   \end{equation*}     Since this is base-$2$, each $x_i$ must be either $0$ or $1$, whence it follows that the denominator and numerator are integers, and the denominator is a power of two, which means $x$ is a dyadic rational. For the converse, I have the following idea, but do not have the background to write up a rigorous proof (in particular, I cannot imagine how to deal with the ambiguity of infinite $1$s versus infinite $0$s at some point in the expansion): Conversely, we must show that if $0 < \frac{a}{2^b} < 1$ is a dyadic rational, then its binary expansion terminates.     Every dyadic rational can be represented as the finite sum/product $\left(\frac{1}{2} + \ldots + \frac{1}{2}\right)\frac{1}{2}\cdot\ldots\cdot\frac{1}{2}$.     The binary expansion of the sum of two numbers with terminating binary expansions terminates, same for the product; and it follows that the binary expansion of a dyadic rational terminates. I have not yet formally tackled (but have vague, possibly incorrect, intuition of) the construction of real numbers, Cauchy convergence and proof by induction (which I gather could be used somehow...), but I need to convince myself of the factlet and its possible pitfalls to continue with the material for the time being (namely, Cantor's diagonalization proofs). Any detailed hints or full-on proofs would be greatly appreciated. (I have found this question but the hint in the answer seems unhelpful given my lack of background.) EDIT : Observe that $a$ is a finite integer, and so can be written as the finite-term sum $x_12^{k-1} + x_22^{k-2} + \ldots + x_k$, where $x_i \in \{0, 1\}$.     Since $\frac{a}{2^b} < 1$, it follows that $a < 2^b$.     By the definition of binary expansion, $x_1 = 1$, whence $2^{k-1} \le a < 2^b$, and we have $k-1<b$.     But $1 \le i \le k$, whence $k-i<b$.     Then we can divide each term by $2^b$, obtaining:     \begin{equation*}     \frac{a}{2^b} = \frac{x_1}{2^{b-k+1}} + \frac{x_2}{2^{b-k+2}} + \ldots + \frac{x_k}{2^b},   \end{equation*}     which gives us a finite binary expansion; this completes the proof.",,"['analysis', 'elementary-number-theory']"
66,dominated convergence for functions $\mathbb R^n\to\mathbb R^m$?,dominated convergence for functions ?,\mathbb R^n\to\mathbb R^m,"I do know the dominated convergence theorem for functions $f:\mathbb R^n\to\mathbb R$. Now let $U\subset\mathbb R^n$ and $f: U\to\mathbb R^m$. Is there any dominated convergence theorem for 'vectorial' functions? Clearly one could integrate each component and apply the dominated convergence theorem for each component but can you apply it too without using this fact? Especially what a about the dominated function, can you use a norm $|\cdot|$ and somenthing like $|f|\leq |g|$?","I do know the dominated convergence theorem for functions $f:\mathbb R^n\to\mathbb R$. Now let $U\subset\mathbb R^n$ and $f: U\to\mathbb R^m$. Is there any dominated convergence theorem for 'vectorial' functions? Clearly one could integrate each component and apply the dominated convergence theorem for each component but can you apply it too without using this fact? Especially what a about the dominated function, can you use a norm $|\cdot|$ and somenthing like $|f|\leq |g|$?",,['real-analysis']
67,$\phi_{\epsilon} \ast \mu \rightarrow \mu$?,?,\phi_{\epsilon} \ast \mu \rightarrow \mu,"Let $\phi$ be a non-negative function on $\mathbb{R}$ with $\int_{\mathbb{R}} \phi = 1$. Define $\phi_{\epsilon}(x)=\epsilon^{-1}\phi(\epsilon^{-1}x)$ for $x \in \mathbb{R}, \epsilon > 0$. For $f \in L^1$, $\phi_{\epsilon} \ast f \rightarrow f$ in $L^1$ as $\epsilon \rightarrow 0$. (cf. Theorem 8.14 of Folland's Real Analysis). Can we replace $f$ by a probability measure $\mu$ to get something like $\phi_{\epsilon} \ast \mu \rightarrow \mu$ weakly as $\epsilon \rightarrow 0$? If so, can you show me (or point me to a reference containing) the proof? If $\mu$ has a density function $f \in L^1$, the conjecture is true because $g (\phi_{\epsilon} \ast f) \rightarrow g f$ in $L^1$ for any $g \in L^{\infty}$.","Let $\phi$ be a non-negative function on $\mathbb{R}$ with $\int_{\mathbb{R}} \phi = 1$. Define $\phi_{\epsilon}(x)=\epsilon^{-1}\phi(\epsilon^{-1}x)$ for $x \in \mathbb{R}, \epsilon > 0$. For $f \in L^1$, $\phi_{\epsilon} \ast f \rightarrow f$ in $L^1$ as $\epsilon \rightarrow 0$. (cf. Theorem 8.14 of Folland's Real Analysis). Can we replace $f$ by a probability measure $\mu$ to get something like $\phi_{\epsilon} \ast \mu \rightarrow \mu$ weakly as $\epsilon \rightarrow 0$? If so, can you show me (or point me to a reference containing) the proof? If $\mu$ has a density function $f \in L^1$, the conjecture is true because $g (\phi_{\epsilon} \ast f) \rightarrow g f$ in $L^1$ for any $g \in L^{\infty}$.",,"['real-analysis', 'analysis', 'measure-theory', 'fourier-analysis']"
68,Show that $X$ is countable.,Show that  is countable.,X,"Let $X \subset \mathbb{R}_{\geq 0 }$. Suppose there exists $C>0$ such that for any finite subset $\{x_1,x_2,...,x_n\}\subset X$ $\sum_{i=1}^{n}x_i \leq C$. Show that $X$ is countable. I am quite lost in trying to solve this exercise. I've only thought about $\mathcal{P}_{<\infty}(X)=\{A\subset X:\#(A)<\infty\}$, and I found out that $\#(\mathcal{P}_{<\infty}(X))=\aleph_{0}$ if $\#(X)=\aleph_{0}$ and $(\mathcal{P}_{<\infty}(X))=\mathfrak{c}$ if  $\#(X)=\mathfrak{c}$. However I can't go any further, and I don't even know if this is useful somehow. Any hint?","Let $X \subset \mathbb{R}_{\geq 0 }$. Suppose there exists $C>0$ such that for any finite subset $\{x_1,x_2,...,x_n\}\subset X$ $\sum_{i=1}^{n}x_i \leq C$. Show that $X$ is countable. I am quite lost in trying to solve this exercise. I've only thought about $\mathcal{P}_{<\infty}(X)=\{A\subset X:\#(A)<\infty\}$, and I found out that $\#(\mathcal{P}_{<\infty}(X))=\aleph_{0}$ if $\#(X)=\aleph_{0}$ and $(\mathcal{P}_{<\infty}(X))=\mathfrak{c}$ if  $\#(X)=\mathfrak{c}$. However I can't go any further, and I don't even know if this is useful somehow. Any hint?",,"['real-analysis', 'analysis', 'elementary-set-theory']"
69,Area form a differential form?,Area form a differential form?,,"I just read that $\omega_x( \eta, \zeta) := \langle x, \eta \times \zeta \rangle $ is the area form on the sphere, where $x \in \mathbb{S}^2$ and $\eta,\zeta \in T_xM.$ All I see is that this is basically a determinant which is of couse somehow related to a volume, but why is this called the area form on the sphere?","I just read that $\omega_x( \eta, \zeta) := \langle x, \eta \times \zeta \rangle $ is the area form on the sphere, where $x \in \mathbb{S}^2$ and $\eta,\zeta \in T_xM.$ All I see is that this is basically a determinant which is of couse somehow related to a volume, but why is this called the area form on the sphere?",,"['real-analysis', 'analysis', 'manifolds', 'differential-topology', 'differential-forms']"
70,"$f$ convex and concave, then $f=ax+b$","convex and concave, then",f f=ax+b,"Let $f$ be a real function defined on some interval $I$. Assuming that $f$ both convex and concave on $I$, i.e, for any $x,y\in I$ one has $$f(\lambda x+(1-\lambda)y)=\lambda f(x)+(1-\lambda)f(y),\, \, \lambda\in (0,1) .$$ I would like to show that $f$ is of the form  $f=ax+b$ for some $a,b$. I was able to prove it when $f$ is differentiable, using the relation  $$f'(x)=f'(y).$$ Anyway, I was not able to provide a general proof (without assuming that $f$ is differentiable, and without assuming that $0\in I$). Any answer will be will be appreciated. Edit: It is little bit different from tte other question How to prove convex+concave=affine? . Here $f$ is defined on some interval, so $o$ not necessary in the domain. Please remove the duplicate message if this possible","Let $f$ be a real function defined on some interval $I$. Assuming that $f$ both convex and concave on $I$, i.e, for any $x,y\in I$ one has $$f(\lambda x+(1-\lambda)y)=\lambda f(x)+(1-\lambda)f(y),\, \, \lambda\in (0,1) .$$ I would like to show that $f$ is of the form  $f=ax+b$ for some $a,b$. I was able to prove it when $f$ is differentiable, using the relation  $$f'(x)=f'(y).$$ Anyway, I was not able to provide a general proof (without assuming that $f$ is differentiable, and without assuming that $0\in I$). Any answer will be will be appreciated. Edit: It is little bit different from tte other question How to prove convex+concave=affine? . Here $f$ is defined on some interval, so $o$ not necessary in the domain. Please remove the duplicate message if this possible",,['real-analysis']
71,How to use the inverse function theorem to find a local inverse?,How to use the inverse function theorem to find a local inverse?,,"I have a function $F(x,y)=(x^2 - y^2, xy)$ and I need to show that it has an inverse. How do I find the inverse of this function using the inverse function theorem? I did this a while ago and now I can't find it in my notes. I recall having to make $u = x^2 - y^2$ and $v = xy$ but then I'm lost. Thank you for your assistance.","I have a function $F(x,y)=(x^2 - y^2, xy)$ and I need to show that it has an inverse. How do I find the inverse of this function using the inverse function theorem? I did this a while ago and now I can't find it in my notes. I recall having to make $u = x^2 - y^2$ and $v = xy$ but then I'm lost. Thank you for your assistance.",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus']"
72,Bounded second derivative implies square root of f is Lipschitz.,Bounded second derivative implies square root of f is Lipschitz.,,"Can you help me with this exercise? Let $f \in C^2(\mathbb{R}) $ a function $ f(x) > 0, \forall x \in \mathbb{R} $ and $\|f''\|_\infty < \infty $  , prove that $\sqrt f$ is Lipschitz continuous. My attempt: i tried assuming that $f'' \ge 0$ , then $f'$ is increasing  and the following limits exist: $$ \lim_{x \to +\infty} f'(x) , \lim_{x \to +\infty} f(x)$$ then i can calculate using L'Hôpital's rule ( $f(x)$ is definitely increasing or decreasing): $$ L=\lim_{x \to +\infty} |(\sqrt f(x))'| =  \lim_{x \to +\infty} |\frac{f'(x)}{2\sqrt f}| = \lim_{x \to +\infty} |\frac{f''(x)\sqrt f}{f'(x)}| \leq  \frac{\|f''\|_\infty}{2L}$$ and so L must be finite, similar with $-\infty$ limit, so $(\sqrt f(x))'$ is bounded. However I'm not sure that $\lim_{x \to +\infty}|\frac{f''(x)\sqrt f}{f'(x)}|$ always exists. Anyway I can't solve the other cases.","Can you help me with this exercise? Let $f \in C^2(\mathbb{R}) $ a function $ f(x) > 0, \forall x \in \mathbb{R} $ and $\|f''\|_\infty < \infty $  , prove that $\sqrt f$ is Lipschitz continuous. My attempt: i tried assuming that $f'' \ge 0$ , then $f'$ is increasing  and the following limits exist: $$ \lim_{x \to +\infty} f'(x) , \lim_{x \to +\infty} f(x)$$ then i can calculate using L'Hôpital's rule ( $f(x)$ is definitely increasing or decreasing): $$ L=\lim_{x \to +\infty} |(\sqrt f(x))'| =  \lim_{x \to +\infty} |\frac{f'(x)}{2\sqrt f}| = \lim_{x \to +\infty} |\frac{f''(x)\sqrt f}{f'(x)}| \leq  \frac{\|f''\|_\infty}{2L}$$ and so L must be finite, similar with $-\infty$ limit, so $(\sqrt f(x))'$ is bounded. However I'm not sure that $\lim_{x \to +\infty}|\frac{f''(x)\sqrt f}{f'(x)}|$ always exists. Anyway I can't solve the other cases.",,"['real-analysis', 'analysis', 'lipschitz-functions']"
73,Closed form as sum and combinatorial of Fibonacci numbers,Closed form as sum and combinatorial of Fibonacci numbers,,"How can I prove that the Fibonacci numbers that are defined as $F_n=F_{n-1}+F_{n-2}, \; n \geq 2$ and $F_0=0,\ F_1=1,\ F_2=1$ have the form: $$F_n=\sum_{k=0}^{n-1} \binom{n-1-k}{k}, \; n\ge 2 $$ I am aware of the gen. function that is: $$\frac{x}{1-x-x^2} =\sum_{n=0}^{\infty}F_n x^n $$ but I cannot extract the other formula.","How can I prove that the Fibonacci numbers that are defined as $F_n=F_{n-1}+F_{n-2}, \; n \geq 2$ and $F_0=0,\ F_1=1,\ F_2=1$ have the form: $$F_n=\sum_{k=0}^{n-1} \binom{n-1-k}{k}, \; n\ge 2 $$ I am aware of the gen. function that is: $$\frac{x}{1-x-x^2} =\sum_{n=0}^{\infty}F_n x^n $$ but I cannot extract the other formula.",,"['analysis', 'fibonacci-numbers']"
74,Most Suitable Book after Kline's Calculus?,Most Suitable Book after Kline's Calculus?,,"I've been working through Morris Kline's Calculus: An Intuitive and Physical Approach and it's an absolutely excellent book for self-studying applied single-variable (and some multi-variable) calculus but I'm starting to wonder what the best book to continue with would be? I wouldn't want to just review single-variable calculus in rigorous form as an introduction to analysis but I'm also not sure if going straight into Baby Rudin/Apostol Vol. II or anything of that sort is any wiser. Or perhaps it is, having the physical intuitions of single-variable calculus as imparted by Kline? I'm not even sure.","I've been working through Morris Kline's Calculus: An Intuitive and Physical Approach and it's an absolutely excellent book for self-studying applied single-variable (and some multi-variable) calculus but I'm starting to wonder what the best book to continue with would be? I wouldn't want to just review single-variable calculus in rigorous form as an introduction to analysis but I'm also not sure if going straight into Baby Rudin/Apostol Vol. II or anything of that sort is any wiser. Or perhaps it is, having the physical intuitions of single-variable calculus as imparted by Kline? I'm not even sure.",,"['calculus', 'real-analysis', 'analysis', 'self-learning', 'learning']"
75,Reference for a Cantor set in the plane formed from series of roots of unity,Reference for a Cantor set in the plane formed from series of roots of unity,,"This is a long shot, but I'm looking for a particular article that I once read, and I'm trying to find it again. It deals with a certain Cantor set in the plane. The set could be written as something like this: $$C=\sum_{n=2}^\infty\frac{1}{n^2}\exp\frac{2\pi i\mathbb Z}{n}.$$ In other words, $C$ contains all sums of series whose $n$th term is an $n$th root of unity multiplied by a quickly decreasing sequence of scales. The scales might not have been $1/n^2$; that's just a guess. The set is composed of two side-by-side blobs, each of which is a triangle of three smaller blobs, each of which is a diamond of four blobs, each of which is a ring of five blobs, each of which is a ring of six blobs, etc. It's sort of a disconnected multi-gasket... fractal thingy. The article definitely included a computer-generated diagram of $C$, maybe two or three. I want to say that it had a preprint on arXiv, but I'm not sure. I don't think it was particularly focused on $C$, so the definition and the diagram(s) would occur somewhere in the middle of the article, and the abstract probably doesn't mention $C$ at all. I don't remember anything else about the context, including the article's mathematical content! I know why I'm suddenly interested in $C$: it's a seemingly rare example of a Cantor set that doesn't contain any ""corner point"": a point whose Bouligand tangent cone is contained in an open half-space. But I don't remember why the author(s) introduced the example in the first place. When I saw the article, I was just searching for Cantor sets with interesting geometry, not for any particular result. Does anyone know the reference I'm looking for? If not, maybe there are suggestions on how one would search for such a thing? Can you divine what the context must have been?","This is a long shot, but I'm looking for a particular article that I once read, and I'm trying to find it again. It deals with a certain Cantor set in the plane. The set could be written as something like this: $$C=\sum_{n=2}^\infty\frac{1}{n^2}\exp\frac{2\pi i\mathbb Z}{n}.$$ In other words, $C$ contains all sums of series whose $n$th term is an $n$th root of unity multiplied by a quickly decreasing sequence of scales. The scales might not have been $1/n^2$; that's just a guess. The set is composed of two side-by-side blobs, each of which is a triangle of three smaller blobs, each of which is a diamond of four blobs, each of which is a ring of five blobs, each of which is a ring of six blobs, etc. It's sort of a disconnected multi-gasket... fractal thingy. The article definitely included a computer-generated diagram of $C$, maybe two or three. I want to say that it had a preprint on arXiv, but I'm not sure. I don't think it was particularly focused on $C$, so the definition and the diagram(s) would occur somewhere in the middle of the article, and the abstract probably doesn't mention $C$ at all. I don't remember anything else about the context, including the article's mathematical content! I know why I'm suddenly interested in $C$: it's a seemingly rare example of a Cantor set that doesn't contain any ""corner point"": a point whose Bouligand tangent cone is contained in an open half-space. But I don't remember why the author(s) introduced the example in the first place. When I saw the article, I was just searching for Cantor sets with interesting geometry, not for any particular result. Does anyone know the reference I'm looking for? If not, maybe there are suggestions on how one would search for such a thing? Can you divine what the context must have been?",,"['analysis', 'reference-request', 'fractals']"
76,An analysis problem about convergence,An analysis problem about convergence,,"Suppose that $f$ is a continuous function from $[a,b]$ to $[a,b]$. Let $x_0\in [a,b]$, and define by induction that $x_{n+1}=f(x_n)$. Show that $$\lim_{n \rightarrow \infty} (x_{n+1}-x_n)=0$$ implies $$\lim_{n \rightarrow \infty}x_n$$ exists. （This problem is from a analysis book, and the author tells us the answer can be found in American Mathematical Monthly, volume 83(1976), page 273, which I have no access to, so you can either offer the reference or give the sketch of the proof. Thanks!）","Suppose that $f$ is a continuous function from $[a,b]$ to $[a,b]$. Let $x_0\in [a,b]$, and define by induction that $x_{n+1}=f(x_n)$. Show that $$\lim_{n \rightarrow \infty} (x_{n+1}-x_n)=0$$ implies $$\lim_{n \rightarrow \infty}x_n$$ exists. （This problem is from a analysis book, and the author tells us the answer can be found in American Mathematical Monthly, volume 83(1976), page 273, which I have no access to, so you can either offer the reference or give the sketch of the proof. Thanks!）",,['analysis']
77,"I'm stuck with this.. (number 9,6 and 3)","I'm stuck with this.. (number 9,6 and 3)",,"Hello guys/girls I was bored and I just played around with math. I am stuck and it's about raised numbers. (9, 6 and 3) So this is how you calculate it. (same method for all numbers) Raise 3, 6 and 9 each from 1 to 10. If the product is more than one digit then add up the digits until there is only one digit. Look at the result... Number 3. 3^1 = 3 3^2 = 9 3^3 = 27 = 2+7 = 9 3^4 = 81 = 8+1 = 9 3^5 = 243 = 2+4+3 = 9 3^6 = 729 = 7+2+9 = 18 = 1+8 = 9 3^7 = 2187 = 2+1+8+7 = 18 = 1+8 = 9 3^8 = 6561 = 6+5+6+1 = 18 = 1+8 = 9 3^9 = 19683 = 1+9+6+8+3 = 27 = 2+7 = 9 3^10 = 59049 = 5+9+0+4+9 = 27 = 2+7 = 9* Number 6. I'm not gonna write plus signs now, because I think you've got the Idea 6^1 = 6 6^2 = 36 = 9 6^3 = 216 = 9 6^4 = 1296 = 18 = 9 6^5 = 7776 = 27 = 9 6^6 = 46656 = 27 = 9 6^7 = 279936 = 36 = 9 6^8 = 1679616 = 36 = 9 6^9 = 10077696 = 36 = 9 6^10 = 60466176 = 36 = 9 Number 9. 9^1 = 9 9^2 = 81 = 9 9^3 = 729 = 18 = 9 9^4 = 6561 = 18 = 9 9^5 = 59049 = 27 = 9 9^6 = 531441 = 18 = 9 9^7 = 4782969 = 45 = 9 9^8 = 43046721 = 27 = 9 9^9 = 387420489 = 45 = 9 9^10 = 3486784401 = 45 = 9 So to my question, why is the sum 9 for number 3 and 6 and not only for the 9 itself?  I know that 3, 6 and 9 is relative to each other but i'm still confused. (3+6+9 = 18 = 1+8 = 9) And is there an equation for this?  Thanks for reading :)","Hello guys/girls I was bored and I just played around with math. I am stuck and it's about raised numbers. (9, 6 and 3) So this is how you calculate it. (same method for all numbers) Raise 3, 6 and 9 each from 1 to 10. If the product is more than one digit then add up the digits until there is only one digit. Look at the result... Number 3. 3^1 = 3 3^2 = 9 3^3 = 27 = 2+7 = 9 3^4 = 81 = 8+1 = 9 3^5 = 243 = 2+4+3 = 9 3^6 = 729 = 7+2+9 = 18 = 1+8 = 9 3^7 = 2187 = 2+1+8+7 = 18 = 1+8 = 9 3^8 = 6561 = 6+5+6+1 = 18 = 1+8 = 9 3^9 = 19683 = 1+9+6+8+3 = 27 = 2+7 = 9 3^10 = 59049 = 5+9+0+4+9 = 27 = 2+7 = 9* Number 6. I'm not gonna write plus signs now, because I think you've got the Idea 6^1 = 6 6^2 = 36 = 9 6^3 = 216 = 9 6^4 = 1296 = 18 = 9 6^5 = 7776 = 27 = 9 6^6 = 46656 = 27 = 9 6^7 = 279936 = 36 = 9 6^8 = 1679616 = 36 = 9 6^9 = 10077696 = 36 = 9 6^10 = 60466176 = 36 = 9 Number 9. 9^1 = 9 9^2 = 81 = 9 9^3 = 729 = 18 = 9 9^4 = 6561 = 18 = 9 9^5 = 59049 = 27 = 9 9^6 = 531441 = 18 = 9 9^7 = 4782969 = 45 = 9 9^8 = 43046721 = 27 = 9 9^9 = 387420489 = 45 = 9 9^10 = 3486784401 = 45 = 9 So to my question, why is the sum 9 for number 3 and 6 and not only for the 9 itself?  I know that 3, 6 and 9 is relative to each other but i'm still confused. (3+6+9 = 18 = 1+8 = 9) And is there an equation for this?  Thanks for reading :)",,['analysis']
78,"Expressing $\frac{d}{dt}\left(\int_{D(t)}u(x,t)dx\right)-\int_{D(t)}u_t(x,t)dx$ as a surface integral?",Expressing  as a surface integral?,"\frac{d}{dt}\left(\int_{D(t)}u(x,t)dx\right)-\int_{D(t)}u_t(x,t)dx","the following question was the last problem on the Fall 2010 qualifying exam at UCLA. Define $D(t)=\{x^2+y^2\leq r^2(t)\}\subseteq\mathbb{R}^2$ where $r(t)\colon\mathbb{R}\to\mathbb{R}$ is continuously differentiable. For given smooth, nonnegative function $u(x,t)\colon\mathbb{R}^2\times\mathbb{R}\to\mathbb{R}$, express the following quantity in terms of a surface integral: $$\frac{d}{dt}\left(\int_{D(t)}u(x,t)dx\right)-\int_{D(t)}u_t(x,t)dx.$$ Any calculus theorem can be used without proof. Does anyone have an idea to express this? My group and I were stumped.","the following question was the last problem on the Fall 2010 qualifying exam at UCLA. Define $D(t)=\{x^2+y^2\leq r^2(t)\}\subseteq\mathbb{R}^2$ where $r(t)\colon\mathbb{R}\to\mathbb{R}$ is continuously differentiable. For given smooth, nonnegative function $u(x,t)\colon\mathbb{R}^2\times\mathbb{R}\to\mathbb{R}$, express the following quantity in terms of a surface integral: $$\frac{d}{dt}\left(\int_{D(t)}u(x,t)dx\right)-\int_{D(t)}u_t(x,t)dx.$$ Any calculus theorem can be used without proof. Does anyone have an idea to express this? My group and I were stumped.",,"['analysis', 'differential-geometry']"
79,"What are the differences between the Lebesgue measure on the Hilbert cube $[0,1]^\mathbb{N}$ and the standard Lebesgue measure on $[0,1]^n$?",What are the differences between the Lebesgue measure on the Hilbert cube  and the standard Lebesgue measure on ?,"[0,1]^\mathbb{N} [0,1]^n","There are no Lebesgue measure on infinite dimensional Banach space. However, there is a Lebesgue measure on the Hilbert cube $[0,1]^\mathbb{N}$. What are the differences between this measure and the finite-dimensional Lebesgue measure on $[0,1]^n$?","There are no Lebesgue measure on infinite dimensional Banach space. However, there is a Lebesgue measure on the Hilbert cube $[0,1]^\mathbb{N}$. What are the differences between this measure and the finite-dimensional Lebesgue measure on $[0,1]^n$?",,"['analysis', 'measure-theory']"
80,question about Lebesgue's integral,question about Lebesgue's integral,,"let $f(x)$ be a bounded measurable function defined on $\mathbb{R}$, then define $$F(x)=\int_0^xf(t)dt,\ \ x\in\mathbb{R}$$ We can see that $F(x)$ is a absolutely continuous function and by some classical results we know $F$ is a.s. differentiable and the integral of its derivative in an interval $[a, b]$ equals to the difference $F(b)-F(a)$. I would like to know whether we have $$F'(x)=f(x),\ \ a.s.$$ Since we have $$\int_0^xf(t)dt=\int_0^xF'(t)dt,\ \ \forall x\in\mathbb{R}$$ for two measurable functions. How could we conclude that $$F'(x)=f(x),\ \ a.s.$$ Many thanks!","let $f(x)$ be a bounded measurable function defined on $\mathbb{R}$, then define $$F(x)=\int_0^xf(t)dt,\ \ x\in\mathbb{R}$$ We can see that $F(x)$ is a absolutely continuous function and by some classical results we know $F$ is a.s. differentiable and the integral of its derivative in an interval $[a, b]$ equals to the difference $F(b)-F(a)$. I would like to know whether we have $$F'(x)=f(x),\ \ a.s.$$ Since we have $$\int_0^xf(t)dt=\int_0^xF'(t)dt,\ \ \forall x\in\mathbb{R}$$ for two measurable functions. How could we conclude that $$F'(x)=f(x),\ \ a.s.$$ Many thanks!",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral']"
81,The definition of $p$ capacity of a set $A\subset\mathbb{R}^n$,The definition of  capacity of a set,p A\subset\mathbb{R}^n,"I am having a bit of difficult understanding the definition of the $p$-capacity of a set $A\subset\mathbb{R}^n$ and I was wondering if anyone would be able to clarify whether I have the right idea or not. This is what I understand so far. Formerly the definition of the $p$-capacity of a set is presented as follows. Fix $1\leq p<n$. Define, \begin{equation} K^p\equiv\{f:\mathbb{R^n}\rightarrow\mathbb{R}\ \vert\ f\geq 0, f\in L^{p^{\ast}}(\mathbb{R}^n), Df\in L^{p}(\mathbb{R}^n;\mathbb{R}^n)\}. \end{equation} If $A\subset\mathbb{R}^n$ we define the quantity \begin{equation} \text{Cap}_p(A)    \equiv   \inf\left\{\int_{\mathbb{R}^n}\vert Df\vert^p\text{ d}x\ \middle|\   f\in K^p, A\subset\text{int}\{f\geq 1\}\right\} \end{equation} as the $p$-capacity of $A$ (denoted by Cap$_p(A)$). My thought process on this is as follows: Pick an $f\in K^p$, and look at int$\{f\geq 1\}$. At this point, we ask ourselves : is $A\subset\text{int}\{f\geq 1\}$? If not then repeat step 1. Compute the quantity $\int_{\mathbb{R}^n}\vert Df\vert^p\text{ d}x$ Save it in a list $L$. Repeat the process until all possible $f$ from $K^p$ have been considered. Taking the 'smallest' number from the list $L$ defines the $p$-capacity of $A$. Is this a correct way to think about the definition of Cap$_p(A)$? I am aware of the distinction between minimum and infimum which is why I 've put 'smallest' in inverted commas.","I am having a bit of difficult understanding the definition of the $p$-capacity of a set $A\subset\mathbb{R}^n$ and I was wondering if anyone would be able to clarify whether I have the right idea or not. This is what I understand so far. Formerly the definition of the $p$-capacity of a set is presented as follows. Fix $1\leq p<n$. Define, \begin{equation} K^p\equiv\{f:\mathbb{R^n}\rightarrow\mathbb{R}\ \vert\ f\geq 0, f\in L^{p^{\ast}}(\mathbb{R}^n), Df\in L^{p}(\mathbb{R}^n;\mathbb{R}^n)\}. \end{equation} If $A\subset\mathbb{R}^n$ we define the quantity \begin{equation} \text{Cap}_p(A)    \equiv   \inf\left\{\int_{\mathbb{R}^n}\vert Df\vert^p\text{ d}x\ \middle|\   f\in K^p, A\subset\text{int}\{f\geq 1\}\right\} \end{equation} as the $p$-capacity of $A$ (denoted by Cap$_p(A)$). My thought process on this is as follows: Pick an $f\in K^p$, and look at int$\{f\geq 1\}$. At this point, we ask ourselves : is $A\subset\text{int}\{f\geq 1\}$? If not then repeat step 1. Compute the quantity $\int_{\mathbb{R}^n}\vert Df\vert^p\text{ d}x$ Save it in a list $L$. Repeat the process until all possible $f$ from $K^p$ have been considered. Taking the 'smallest' number from the list $L$ defines the $p$-capacity of $A$. Is this a correct way to think about the definition of Cap$_p(A)$? I am aware of the distinction between minimum and infimum which is why I 've put 'smallest' in inverted commas.",,"['analysis', 'partial-differential-equations', 'definition', 'geometric-measure-theory']"
82,Cauchy's functional equation - a generalisation? (do additive maps have to be continuous?),Cauchy's functional equation - a generalisation? (do additive maps have to be continuous?),,"If a map $f : \mathbb{R} \to \mathbb{R}$ is additive, in the sense that $f(x + y) = f(x) + f(y)$, then it is simple to show that $f$ is $\mathbb{Q}$-linear, buy it does not need to be $\mathbb{R}$-linear in general. This is a classical problem, known as Cauchy's functional equation . It turns out that as soon as the function is even remotely regular (continuous, bounded on an interval, or measurable), on can show that it has to be linear, and thus of the form $f(x) = cx$ for some $c \in \mathbb{R}$. It turns out that after some work, one can prove a similar claim about maps of the torus $T = \mathbb{R}/\mathbb{Z}$: if $f :\ T \to T$ obeys $f(x+y) = f(x) + f(y)$ and is measurable, then it is of the form $f(x) = cx$, where $c \in \mathbb{Z}$. I think that it follows that additive maps on $T^m$, or $\mathbb{R}^n$, or even $\mathbb{R}^n \times T^m$, are necessarily  ""linear"". I believe it can be shown by applying the $1$-dimensional result to $t \mapsto \pi f(tv)$, where $\pi$ are projections and $v$ are vectors. (For instance, for $T^2$, write $f(t_1,t_2) = (f_{11}(t_{1}) + f_{12}(t_2),  f_{21}(t_{1}) + f_{22}(t_2))$ and reason for each $f_{ij}$ independently). I would like ask two things, and I would be very grateful for either. Is the result for the torus and/or multidimensional case generally known, and if so what would be a good keyword for further search or a reference? Is there an easy proof for the torus? (my reasoning for the torus goes very much like the standard one for $\mathbb{R}$ I know of, but perhaps there is a clever reduction from one to the other?) If I am not mistaken about the multidimensional case, it would follow that a an additive map of a commutative Lie group into itself is automatically continuous, and even has a particularly nice form. Is it true that an additive map of a general Lie group into itself is automatically continuous? Edit/Answer It turns out that the related issues have been studied with much success and are relatively well understood (at least insofar as there are strong results on the topic). The phenomenon in question is known as Automatic Continuity, and has been studied by the Polish mathematical school, and Andre Weil. There are theorems that assure continuity of Baire measurable homomorphisms between Polish group (Banach) and Haar measurable homomorphism from a locally compact Polish group into a Polish group (Weil). I encourage anyone interested to consult this excellent question/answer pair on MathOverflow.","If a map $f : \mathbb{R} \to \mathbb{R}$ is additive, in the sense that $f(x + y) = f(x) + f(y)$, then it is simple to show that $f$ is $\mathbb{Q}$-linear, buy it does not need to be $\mathbb{R}$-linear in general. This is a classical problem, known as Cauchy's functional equation . It turns out that as soon as the function is even remotely regular (continuous, bounded on an interval, or measurable), on can show that it has to be linear, and thus of the form $f(x) = cx$ for some $c \in \mathbb{R}$. It turns out that after some work, one can prove a similar claim about maps of the torus $T = \mathbb{R}/\mathbb{Z}$: if $f :\ T \to T$ obeys $f(x+y) = f(x) + f(y)$ and is measurable, then it is of the form $f(x) = cx$, where $c \in \mathbb{Z}$. I think that it follows that additive maps on $T^m$, or $\mathbb{R}^n$, or even $\mathbb{R}^n \times T^m$, are necessarily  ""linear"". I believe it can be shown by applying the $1$-dimensional result to $t \mapsto \pi f(tv)$, where $\pi$ are projections and $v$ are vectors. (For instance, for $T^2$, write $f(t_1,t_2) = (f_{11}(t_{1}) + f_{12}(t_2),  f_{21}(t_{1}) + f_{22}(t_2))$ and reason for each $f_{ij}$ independently). I would like ask two things, and I would be very grateful for either. Is the result for the torus and/or multidimensional case generally known, and if so what would be a good keyword for further search or a reference? Is there an easy proof for the torus? (my reasoning for the torus goes very much like the standard one for $\mathbb{R}$ I know of, but perhaps there is a clever reduction from one to the other?) If I am not mistaken about the multidimensional case, it would follow that a an additive map of a commutative Lie group into itself is automatically continuous, and even has a particularly nice form. Is it true that an additive map of a general Lie group into itself is automatically continuous? Edit/Answer It turns out that the related issues have been studied with much success and are relatively well understood (at least insofar as there are strong results on the topic). The phenomenon in question is known as Automatic Continuity, and has been studied by the Polish mathematical school, and Andre Weil. There are theorems that assure continuity of Baire measurable homomorphisms between Polish group (Banach) and Haar measurable homomorphism from a locally compact Polish group into a Polish group (Weil). I encourage anyone interested to consult this excellent question/answer pair on MathOverflow.",,"['analysis', 'functions', 'continuity', 'lie-groups', 'functional-equations']"
83,Expressing $\sin\pi/n$ in terms of radicals of integers,Expressing  in terms of radicals of integers,\sin\pi/n,"Are values of $$\sin \frac{\pi}{n}$$ where $n$ is a positive integer all expressible in terms of radicals of integers? If not, what is the first $n$ for which it is not?","Are values of $$\sin \frac{\pi}{n}$$ where $n$ is a positive integer all expressible in terms of radicals of integers? If not, what is the first $n$ for which it is not?",,"['abstract-algebra', 'analysis', 'trigonometry']"
84,Bounding $\liminf_{n} n |f^n(x)-x|$,Bounding,\liminf_{n} n |f^n(x)-x|,"I solved an exercise in which the first part asks to prove that for any measure preserving measurable transformation $f:[0,1]\rightarrow [0,1]$ we have $$\liminf_{n} n |f^n(x)-x| \leq 1, \ \mbox{a.e.}$$ I can't prove the second part of the exercise: Let $\omega=(\sqrt{5}-1)/2$ and let $f:[0,1]\rightarrow[0,1]$ defined as $f(x)= (x+\omega) \pmod{1}$. Use this transformation to prove that there is no $c<\frac{1}{\sqrt{5}}$ such that $$\liminf_{n} n |f^n(x)-x| \leq c$$ Thank you guys in advance! (I'm sorry about the mistakes!)","I solved an exercise in which the first part asks to prove that for any measure preserving measurable transformation $f:[0,1]\rightarrow [0,1]$ we have $$\liminf_{n} n |f^n(x)-x| \leq 1, \ \mbox{a.e.}$$ I can't prove the second part of the exercise: Let $\omega=(\sqrt{5}-1)/2$ and let $f:[0,1]\rightarrow[0,1]$ defined as $f(x)= (x+\omega) \pmod{1}$. Use this transformation to prove that there is no $c<\frac{1}{\sqrt{5}}$ such that $$\liminf_{n} n |f^n(x)-x| \leq c$$ Thank you guys in advance! (I'm sorry about the mistakes!)",,"['analysis', 'measure-theory']"
85,Infinitely valued functions,Infinitely valued functions,,Is it possible to define a multiple integral or multiple sums to infinite order ? Something like $\int\int\int\int\cdots$ where there are infinite number of integrals or $\sum\sum\sum\sum\cdots$ . Does infinite valued functions exist (Something like $R^\infty \rightarrow R^n$ ) ?,Is it possible to define a multiple integral or multiple sums to infinite order ? Something like $\int\int\int\int\cdots$ where there are infinite number of integrals or $\sum\sum\sum\sum\cdots$ . Does infinite valued functions exist (Something like $R^\infty \rightarrow R^n$ ) ?,,['analysis']
86,How to prove that $x \rightarrow e^{1/x}$ is not a restriction of any real distribution to $ \mathbb {R}_+$?,How to prove that  is not a restriction of any real distribution to ?,x \rightarrow e^{1/x}  \mathbb {R}_+,"This is an excercise 2.2 from Hormander, vol. I: Does there exist a distribution $u$ on $\mathbb{R}$ with the restriction $x \rightarrow e^{1/x}$ to $\mathbb{R}_+$? The answer, provided in the book, is ""No"". I am trying to ""cook up"" appropriate test function(s) such that $ \int \phi(x)e^{1/x} \leq C\sum_{\alpha \leq k} \sup\left|\partial^{\alpha}\phi\right|$ for no $k$, and I'm not sure at all what function(s) to take. What is the appropriate function? Is there a general method to come up with just right test functions?","This is an excercise 2.2 from Hormander, vol. I: Does there exist a distribution $u$ on $\mathbb{R}$ with the restriction $x \rightarrow e^{1/x}$ to $\mathbb{R}_+$? The answer, provided in the book, is ""No"". I am trying to ""cook up"" appropriate test function(s) such that $ \int \phi(x)e^{1/x} \leq C\sum_{\alpha \leq k} \sup\left|\partial^{\alpha}\phi\right|$ for no $k$, and I'm not sure at all what function(s) to take. What is the appropriate function? Is there a general method to come up with just right test functions?",,"['analysis', 'distribution-theory']"
87,Does such a subset has a nonempty interior?,Does such a subset has a nonempty interior?,,"Let $(a_n)_{n=1}^\infty$ be a sequence such that $0\leq a_n \leq 1$, $\sum_{n=1}^\infty a_n=1$ and let $card \{a_n: n \in \mathbb{N} \}=\infty$.  Let's consider the set $$S=\{ \sum_{n\in I} a_n: I \subset \mathbb{N} \}.$$  It may happens that $S=[0,1]$ for example for $(\frac{1}{2},\frac{1}{2^2},\frac{1}{2^3}...)$, or $(\frac{1}{3}, \frac{1}{3}, \frac{1}{3^2}, \frac{1}{3^2},...)$. Under what condition $S$ contain an interval? Is it then $S=[0,1]$?","Let $(a_n)_{n=1}^\infty$ be a sequence such that $0\leq a_n \leq 1$, $\sum_{n=1}^\infty a_n=1$ and let $card \{a_n: n \in \mathbb{N} \}=\infty$.  Let's consider the set $$S=\{ \sum_{n\in I} a_n: I \subset \mathbb{N} \}.$$  It may happens that $S=[0,1]$ for example for $(\frac{1}{2},\frac{1}{2^2},\frac{1}{2^3}...)$, or $(\frac{1}{3}, \frac{1}{3}, \frac{1}{3^2}, \frac{1}{3^2},...)$. Under what condition $S$ contain an interval? Is it then $S=[0,1]$?",,"['analysis', 'metric-spaces']"
88,How to solve $f(x)=\frac{1}{2} f(3x) + \cos x$?,How to solve ?,f(x)=\frac{1}{2} f(3x) + \cos x,"Using the Banach fixed-point theorem, I can prove that there exists exactly one continuous function $f:[0,1]→\mathbb{R}$ that satisfies $$f(x) = \frac{1}{2}f(3x)+\cos x.$$ I currently don't know how to solve this $f$ precisely. My attempt is similar to Picard iteration. Define $$Tf = \frac{1}{2}f(3x)+\cos x.$$ Let $f_0 = 2$ , and $f_{n+1} = Tf_n$ . I can calculate that $$f_n = \frac{1}{2^{n-1}}\cos 3^{n-1}x + \cdots + \frac{1}{2}\cos 3x+ \cos x + \frac{1}{2^{n-1}}.$$ But what is the limit of this $f_n$ ? Thank you for any solutions or hints to this problem. Edit: Adding the plots of $f_1,f_3$ and $f_5$ in the interval $x\in[-\pi,\pi]$ , JL","Using the Banach fixed-point theorem, I can prove that there exists exactly one continuous function that satisfies I currently don't know how to solve this precisely. My attempt is similar to Picard iteration. Define Let , and . I can calculate that But what is the limit of this ? Thank you for any solutions or hints to this problem. Edit: Adding the plots of and in the interval , JL","f:[0,1]→\mathbb{R} f(x) = \frac{1}{2}f(3x)+\cos x. f Tf = \frac{1}{2}f(3x)+\cos x. f_0 = 2 f_{n+1} = Tf_n f_n = \frac{1}{2^{n-1}}\cos 3^{n-1}x + \cdots + \frac{1}{2}\cos 3x+ \cos x + \frac{1}{2^{n-1}}. f_n f_1,f_3 f_5 x\in[-\pi,\pi]","['real-analysis', 'calculus', 'analysis', 'functional-equations']"
89,"If for any $\epsilon >0$, there exists a $\delta >0$, such that for any $|x-x_0| < \delta,~ (L-\epsilon)^k \leq f(x) \leq (L+\epsilon)^k,$ [closed]","If for any , there exists a , such that for any  [closed]","\epsilon >0 \delta >0 |x-x_0| < \delta,~ (L-\epsilon)^k \leq f(x) \leq (L+\epsilon)^k,","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 months ago . Improve this question Claim: If for any $\epsilon >0$ , there exists a $\delta >0$ , such that for any $|x-x_0| < \delta$ , it holds $(L-\epsilon)^k \leq f(x) \leq (L+\epsilon)^k,$ where $k$ and $L$ are positive constants. Then we have $\lim_{x \rightarrow x_0} f(x) =L^k.$ The following is my proof. Since $\lim_{\xi \rightarrow 0} (L+ \xi)^k = L^k,$ for any $\epsilon >0$ , there exists a $\delta' >0$ , such that for any $0 < \xi < \delta'$ , it holds $(L+\xi)^k - L^k < \epsilon.$ Then for $\delta'/2>0$ , there exists $\delta >0$ , such that when $|x-x_0| < \delta$ , it holds $ f(x) \leq (L+ \delta'/2)^k,$ and therefore, $f(x) - L^k \leq (L+ \delta'/2)^k - L^k < \epsilon.$ . The other direction can use the same method. Is my proof correct?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 months ago . Improve this question Claim: If for any , there exists a , such that for any , it holds where and are positive constants. Then we have The following is my proof. Since for any , there exists a , such that for any , it holds Then for , there exists , such that when , it holds and therefore, . The other direction can use the same method. Is my proof correct?","\epsilon >0 \delta >0 |x-x_0| < \delta (L-\epsilon)^k \leq f(x) \leq (L+\epsilon)^k, k L \lim_{x \rightarrow x_0} f(x) =L^k. \lim_{\xi \rightarrow 0} (L+ \xi)^k = L^k, \epsilon >0 \delta' >0 0 < \xi < \delta' (L+\xi)^k - L^k < \epsilon. \delta'/2>0 \delta >0 |x-x_0| < \delta  f(x) \leq (L+ \delta'/2)^k, f(x) - L^k \leq (L+ \delta'/2)^k - L^k < \epsilon.","['real-analysis', 'analysis']"
90,For which $\{a_k\}_{k=1}^\infty$ does $\sum_{k=1}^\infty \frac{1}{a_k} f(x+a_k)$ converge absolutely for almost every $x\in \Bbb R$?,For which  does  converge absolutely for almost every ?,\{a_k\}_{k=1}^\infty \sum_{k=1}^\infty \frac{1}{a_k} f(x+a_k) x\in \Bbb R,"Question: Let $f\in L^1(\Bbb R)$ . For which increasing sequences $\{a_k\}_{k=1}^\infty$ of positive real numbers does $$\sum_{k=1}^\infty \frac{1}{a_k} f(x+a_k)$$ converge absolutely for almost every $x\in \Bbb R$ ? I believe this might be true for many such sequences, with possible restrictions on how fast $a_k$ grows to infinity. I have sketched the proof for $a_k = \sqrt{k}$ below, which I shall try to generalize. The key step seems to involve the ""inverse"" sequence. Let $a_k = \phi(k)$ for $k\in \Bbb N$ , where $\phi:[1,\infty) \to [1,\infty)$ is a strictly increasing surjective function. Then, $\phi^{-1}: [1,\infty) \to [1,\infty)$ exists. I refer to $b_k = \phi^{-1}(k)$ as the ""inverse"" sequence of $a_k.$ Toy Case: One can do the following if $a_k = \sqrt k$ . It is enough to show that the series converges a.e. on $[n,n+1]$ , for all $n\in \Bbb Z$ . WLOG, let $n=0$ . Now, it is enough to show $$\int_0^1 \sum_k \frac{1}{\sqrt k} |f(x+\sqrt k)|\,dx < \infty$$ as $\int_0^1 |g| < \infty \implies g$ is finite a.e. on $[0,1]$ . Using the Monotone Convergence Theorem, it is enough to show $$ \sum_k \frac{1}{\sqrt k} \int_0^1|f(x+\sqrt k)|\,dx < \infty.$$ For fixed $m \in \Bbb N$ and $m^2 \le k < (m+1)^2$ , $$\frac{1}{\sqrt k} \int_0^1|f(x+\sqrt k)|\,dx \le \frac{1}{m} \sup_{y\in [m, m+1)} \int_0^1 |f(x+y)|\, dx \le \frac1m \int_m^{m+2} |f(x)|\, dx$$ so that $$ \begin{align*} \sum_k \frac{1}{\sqrt k} \int_0^1|f(x+\sqrt k)|\,dx &\le \sum_{m=1}^\infty \sum_{k=m^2}^{(m+1)^2-1} \frac1m \int_m^{m+2} |f(x)|\, dx\\ &= 2 \sum_{m=1}^\infty \int_m^{m+2} |f(x)|\, dx + \sum_{m=1}^\infty \frac 1 m \int_m^{m+2} |f(x)|\, dx\\ &\le 3 \sum_{m=1}^\infty \int_m^{m+2} |f(x)|\, dx\\ &\le 6\|f\|_1 < \infty. \end{align*}$$ Generalization: I propose the following hypotheses on $\{a_k\}_{k=1}^\infty$ . Let $\phi:[1,\infty) \to [1,\infty)$ be a strictly increasing surjective function, and $a_k := \phi(k)$ for all $k\in \Bbb N$ . Let $\{b_k\}$ be the inverse sequence of $a_k$ , defined using $\phi^{-1}:[1,\infty) \to [1,\infty)$ . Lastly, assume there exists $\beta > 0$ such that $b_{k+1} - b_k \le \beta k$ for all $k\in \mathbb N$ . Once again, it is enough to show that the series converges a.e. on $[n,n+1]$ , for all $n\in \Bbb Z$ . WLOG, let $n=0$ . Now, it is enough to show $$\int_0^1 \sum_k \frac{|f(x+a_k)|}{a_k} \,dx < \infty$$ as $\int_0^1 |g| < \infty \implies g$ is finite a.e. on $[0,1]$ . Using the Monotone Convergence Theorem, it is enough to show $$ \sum_k \frac{1}{a_k} \int_0^1|f(x+a_k)|\,dx < \infty.$$ For fixed $m \in \Bbb N$ and $b_m \le k < b_{m+1}$ , we have $m = \phi(b_m) \le a_k < \phi(b_{m+1}) = m +1$ . $$\frac{1}{a_k} \int_0^1|f(x+a_k)|\,dx \le \frac{1}{m} \sup_{y\in [m, m+1)} \int_0^1 |f(x+y)|\, dx \le \frac1m \int_m^{m+2} |f(x)|\, dx$$ so that $$\sum_k \frac{1}{a_k} \int_0^1|f(x+a_k)|\,dx \le \sum_{m=1}^\infty \sum_{k=b_m}^{b_{m+1}-1} \frac1m \int_m^{m+2} |f(x)|\, dx = \sum_{m=1}^\infty \frac{b_{m+1}-b_m}{m} \int_m^{m+2} |f(x)|\, dx .$$ As $b_{m+1} - b_m \le \beta m$ for each $m\ge 1$ , we have $$\sum_k \frac{1}{a_k} \int_0^1|f(x+a_k)|\,dx \le \beta \sum_{m=1}^\infty\int_m^{m+2} |f(x)|\, dx \le 2\beta \|f\|_1 < \infty.$$ Other Thoughts: In an ideal world, I'd like to know necessary and sufficient conditions on $\{a_k\}_{k=1}^\infty$ so that $\sum_{k=1}^\infty \frac{1}{a_k} f(x+a_k)$ converges absolutely for almost every $x\in \Bbb R$ . For sequences growing slower than $\sqrt k$ , e.g. $a_k = k^{1/3}$ , the ""inverse"" sequences grow faster. In this case, $b_k = k^3$ , and there is no constant $\beta > 0$ such that $b_{k+1} - b_k \le \beta k$ for all $k \in \Bbb N$ . As a result, I'd expect the series $$\sum_{k=1}^\infty \frac{1}{k^{1/3}} f(x+k^{1/3})$$ to not converge absolutely on a set of positive measure. I do not know if this is true. Thank you!","Question: Let . For which increasing sequences of positive real numbers does converge absolutely for almost every ? I believe this might be true for many such sequences, with possible restrictions on how fast grows to infinity. I have sketched the proof for below, which I shall try to generalize. The key step seems to involve the ""inverse"" sequence. Let for , where is a strictly increasing surjective function. Then, exists. I refer to as the ""inverse"" sequence of Toy Case: One can do the following if . It is enough to show that the series converges a.e. on , for all . WLOG, let . Now, it is enough to show as is finite a.e. on . Using the Monotone Convergence Theorem, it is enough to show For fixed and , so that Generalization: I propose the following hypotheses on . Let be a strictly increasing surjective function, and for all . Let be the inverse sequence of , defined using . Lastly, assume there exists such that for all . Once again, it is enough to show that the series converges a.e. on , for all . WLOG, let . Now, it is enough to show as is finite a.e. on . Using the Monotone Convergence Theorem, it is enough to show For fixed and , we have . so that As for each , we have Other Thoughts: In an ideal world, I'd like to know necessary and sufficient conditions on so that converges absolutely for almost every . For sequences growing slower than , e.g. , the ""inverse"" sequences grow faster. In this case, , and there is no constant such that for all . As a result, I'd expect the series to not converge absolutely on a set of positive measure. I do not know if this is true. Thank you!","f\in L^1(\Bbb R) \{a_k\}_{k=1}^\infty \sum_{k=1}^\infty \frac{1}{a_k} f(x+a_k) x\in \Bbb R a_k a_k = \sqrt{k} a_k = \phi(k) k\in \Bbb N \phi:[1,\infty) \to [1,\infty) \phi^{-1}: [1,\infty) \to [1,\infty) b_k = \phi^{-1}(k) a_k. a_k = \sqrt k [n,n+1] n\in \Bbb Z n=0 \int_0^1 \sum_k \frac{1}{\sqrt k} |f(x+\sqrt k)|\,dx < \infty \int_0^1 |g| < \infty \implies g [0,1]  \sum_k \frac{1}{\sqrt k} \int_0^1|f(x+\sqrt k)|\,dx < \infty. m \in \Bbb N m^2 \le k < (m+1)^2 \frac{1}{\sqrt k} \int_0^1|f(x+\sqrt k)|\,dx \le \frac{1}{m} \sup_{y\in [m, m+1)} \int_0^1 |f(x+y)|\, dx \le \frac1m \int_m^{m+2} |f(x)|\, dx 
\begin{align*}
\sum_k \frac{1}{\sqrt k} \int_0^1|f(x+\sqrt k)|\,dx &\le \sum_{m=1}^\infty \sum_{k=m^2}^{(m+1)^2-1} \frac1m \int_m^{m+2} |f(x)|\, dx\\
&= 2 \sum_{m=1}^\infty \int_m^{m+2} |f(x)|\, dx + \sum_{m=1}^\infty \frac 1 m \int_m^{m+2} |f(x)|\, dx\\
&\le 3 \sum_{m=1}^\infty \int_m^{m+2} |f(x)|\, dx\\ &\le 6\|f\|_1 < \infty.
\end{align*} \{a_k\}_{k=1}^\infty \phi:[1,\infty) \to [1,\infty) a_k := \phi(k) k\in \Bbb N \{b_k\} a_k \phi^{-1}:[1,\infty) \to [1,\infty) \beta > 0 b_{k+1} - b_k \le \beta k k\in \mathbb N [n,n+1] n\in \Bbb Z n=0 \int_0^1 \sum_k \frac{|f(x+a_k)|}{a_k} \,dx < \infty \int_0^1 |g| < \infty \implies g [0,1]  \sum_k \frac{1}{a_k} \int_0^1|f(x+a_k)|\,dx < \infty. m \in \Bbb N b_m \le k < b_{m+1} m = \phi(b_m) \le a_k < \phi(b_{m+1}) = m +1 \frac{1}{a_k} \int_0^1|f(x+a_k)|\,dx \le \frac{1}{m} \sup_{y\in [m, m+1)} \int_0^1 |f(x+y)|\, dx \le \frac1m \int_m^{m+2} |f(x)|\, dx \sum_k \frac{1}{a_k} \int_0^1|f(x+a_k)|\,dx \le \sum_{m=1}^\infty \sum_{k=b_m}^{b_{m+1}-1} \frac1m \int_m^{m+2} |f(x)|\, dx = \sum_{m=1}^\infty \frac{b_{m+1}-b_m}{m} \int_m^{m+2} |f(x)|\, dx . b_{m+1} - b_m \le \beta m m\ge 1 \sum_k \frac{1}{a_k} \int_0^1|f(x+a_k)|\,dx \le \beta \sum_{m=1}^\infty\int_m^{m+2} |f(x)|\, dx \le 2\beta \|f\|_1 < \infty. \{a_k\}_{k=1}^\infty \sum_{k=1}^\infty \frac{1}{a_k} f(x+a_k) x\in \Bbb R \sqrt k a_k = k^{1/3} b_k = k^3 \beta > 0 b_{k+1} - b_k \le \beta k k \in \Bbb N \sum_{k=1}^\infty \frac{1}{k^{1/3}} f(x+k^{1/3})","['real-analysis', 'analysis', 'lp-spaces', 'absolute-convergence']"
91,Inequality regarding a function and its Fourier transform,Inequality regarding a function and its Fourier transform,,"Does a continuous compactly supported function $f : \mathbb R \to \mathbb R$ satisfy the inequality $$\lvert f(x)\rvert+\lvert \widehat{f}(x)\rvert \leq C(1+\lvert x\rvert)^{-1-\epsilon},\quad x \in \mathbb{R}$$ for some $C,\epsilon >0$ ? Here, $\hat f$ is the Fourier transform of $f$ . Presumably everything is defined since $f \in L^1(\mathbb{R})$ . Edit 1 : For any $\epsilon >0\,$ I can bound $\lvert f(x)\rvert \leq \max \left(1,\frac{\Vert f\Vert_\infty}{\min_K \left(1+\lvert x \rvert\right)^{-1-\epsilon}}\right)$ , then I want to use the following (but my function is just $\mathcal{C}^0$ ...) Theorem (Paley-Wiener) If $f \in \mathcal{C}_o^\infty$ and $f(z)=0$ for $ \lvert z \rvert > R$ , then $$\lvert \widehat{f}(z) \rvert \leq C_n(1+ \lvert z\rvert)^{-n}e^{2\pi\lvert \text{Im}(z)\rvert R} \quad \forall n \in \mathbb{Z}^+.$$ I am referring to what they say in page 4 of the paper https://www.sciencedirect.com/science/article/pii/S0022247X19309813","Does a continuous compactly supported function satisfy the inequality for some ? Here, is the Fourier transform of . Presumably everything is defined since . Edit 1 : For any I can bound , then I want to use the following (but my function is just ...) Theorem (Paley-Wiener) If and for , then I am referring to what they say in page 4 of the paper https://www.sciencedirect.com/science/article/pii/S0022247X19309813","f : \mathbb R \to \mathbb R \lvert f(x)\rvert+\lvert \widehat{f}(x)\rvert \leq C(1+\lvert x\rvert)^{-1-\epsilon},\quad x \in \mathbb{R} C,\epsilon >0 \hat f f f \in L^1(\mathbb{R}) \epsilon >0\, \lvert f(x)\rvert \leq \max \left(1,\frac{\Vert f\Vert_\infty}{\min_K \left(1+\lvert x \rvert\right)^{-1-\epsilon}}\right) \mathcal{C}^0 f \in \mathcal{C}_o^\infty f(z)=0  \lvert z \rvert > R \lvert \widehat{f}(z) \rvert \leq C_n(1+ \lvert z\rvert)^{-n}e^{2\pi\lvert \text{Im}(z)\rvert R} \quad \forall n \in \mathbb{Z}^+.","['analysis', 'inequality', 'fourier-analysis', 'lp-spaces']"
92,Reference for Sobolev estimate on bounded domain with a boundary term on the right side.,Reference for Sobolev estimate on bounded domain with a boundary term on the right side.,,"Could someone point me to a reference for the proof of the following Sobolev estimate? $$ \| u\|_{ L^{2d/(d-2)}(\Omega)} \leqslant C\left(\| f \|_{L^{2 d /(d+2)}(\Omega)}+\|g\|_{\partial \Omega}\right) $$ for all $u \in W^{2,2 d /(d+2)}(\Omega)$ such that $\Delta u=f$ and $g=u$ on $\partial \Omega$ , and $d>2$ and $\Omega$ is a smooth domain of $\mathbf{R}^d$ . Thanks in advance to anyone who could offer help.","Could someone point me to a reference for the proof of the following Sobolev estimate? for all such that and on , and and is a smooth domain of . Thanks in advance to anyone who could offer help.","
\| u\|_{ L^{2d/(d-2)}(\Omega)} \leqslant C\left(\| f \|_{L^{2 d /(d+2)}(\Omega)}+\|g\|_{\partial \Omega}\right)
 u \in W^{2,2 d /(d+2)}(\Omega) \Delta u=f g=u \partial \Omega d>2 \Omega \mathbf{R}^d","['analysis', 'partial-differential-equations', 'reference-request']"
93,Is there any counter example for this $ f $?,Is there any counter example for this ?, f ,"First sorry about the grammar and words errors in my post since English is not my native tongue. Here $ f $ is a countinuous function on $ x=0 $ , and the limit $$ \lim_{x \to 0} \frac{f(\tan x) - f(x)}{x^3} $$ exists. My question is: is $f$ differentiable on $x = 0$ ? My opinion is NO since it restricts the derivative's definition. So I want to find a counter-example for this claim. I searched around the math.se, and found this: $f$ differentiable at $0\iff\lim_{x\to 0}\frac{f(2x)-f(x)}{x}$ exists . I draw my wrong proposition back and sorry for my carelessness.","First sorry about the grammar and words errors in my post since English is not my native tongue. Here is a countinuous function on , and the limit exists. My question is: is differentiable on ? My opinion is NO since it restricts the derivative's definition. So I want to find a counter-example for this claim. I searched around the math.se, and found this: $f$ differentiable at $0\iff\lim_{x\to 0}\frac{f(2x)-f(x)}{x}$ exists . I draw my wrong proposition back and sorry for my carelessness."," f   x=0  
\lim_{x \to 0} \frac{f(\tan x) - f(x)}{x^3}
 f x = 0","['analysis', 'derivatives']"
94,$A_1 \cup A_2$ is not a sigma Algebra,is not a sigma Algebra,A_1 \cup A_2,"Let $X\neq \emptyset$ und $A_1, A_2$ be two sigma Algebras over $X$ with $A_1\not \subseteq A_2$ and $A_2\not \subseteq A_1$ . Show, that $A_1 \cup A_2$ cannot be a Sigma Algebra over $X$ . My Idea: Because $A_1\not \subseteq A_2$ and $A_2\not \subseteq A_1$ , I find $y\in A_1$ with $y\not \in A_2$ and $z\in A_2$ with $z\not \in A_1$ . Also $y^c\not\in A_2$ and $z^c\not\in A_1$ . Now, I think I have to show that $y\cup z \not \in A_1 \cup A_2$ . Does anyone have a hint?","Let und be two sigma Algebras over with and . Show, that cannot be a Sigma Algebra over . My Idea: Because and , I find with and with . Also and . Now, I think I have to show that . Does anyone have a hint?","X\neq \emptyset A_1, A_2 X A_1\not \subseteq A_2 A_2\not \subseteq A_1 A_1 \cup A_2 X A_1\not \subseteq A_2 A_2\not \subseteq A_1 y\in A_1 y\not \in A_2 z\in A_2 z\not \in A_1 y^c\not\in A_2 z^c\not\in A_1 y\cup z \not \in A_1 \cup A_2","['analysis', 'measure-theory']"
95,Is calculus still being developed?,Is calculus still being developed?,,"I have heard a podcast episode about calculus, with a mathematician and a journalist, on which he asked the mathematician if there still were developments being made in calculus. He answered that no, but that there were in (real) analysis. Which made me even more sense since that advanced calculus refers to (real) analysis, that real analysis is a famous next-step after calculus, and that some advanced cool stuff I have seen were said to be of analysis. So, is calculus basically (emphasis on basically ) what is learnt in calc1–calc3, being more of it real analysis? Could you, like, say that you learn t calculus? P.S.: I am not a mathematician, nor am I in any college. I also do not know if this type of question is allowed — pardon me if it be not.","I have heard a podcast episode about calculus, with a mathematician and a journalist, on which he asked the mathematician if there still were developments being made in calculus. He answered that no, but that there were in (real) analysis. Which made me even more sense since that advanced calculus refers to (real) analysis, that real analysis is a famous next-step after calculus, and that some advanced cool stuff I have seen were said to be of analysis. So, is calculus basically (emphasis on basically ) what is learnt in calc1–calc3, being more of it real analysis? Could you, like, say that you learn t calculus? P.S.: I am not a mathematician, nor am I in any college. I also do not know if this type of question is allowed — pardon me if it be not.",,"['calculus', 'analysis', 'soft-question']"
96,"$\zeta(1 + 2/x)$ has a strange, nearly linear behaviour","has a strange, nearly linear behaviour",\zeta(1 + 2/x),"I was messing around with some infinite sums in $\ell^p$ spaces and I encountered a strange result: $\zeta\left(1 + \frac{2}{x}\right)$ looks like it is linear in $x$ for $x > 1$ ! A simple linear regression gives me $\zeta\left(1 + \frac{2}{x}\right)\approx 0.593413 + 0.499801 x$ . The greatest difference appears to be relatively small. Is this true? If so, how could I show this? And if not, why does it appear to be nearly linear?","I was messing around with some infinite sums in spaces and I encountered a strange result: looks like it is linear in for ! A simple linear regression gives me . The greatest difference appears to be relatively small. Is this true? If so, how could I show this? And if not, why does it appear to be nearly linear?",\ell^p \zeta\left(1 + \frac{2}{x}\right) x x > 1 \zeta\left(1 + \frac{2}{x}\right)\approx 0.593413 + 0.499801 x,"['analysis', 'riemann-zeta', 'linear-approximation']"
97,Does twice continuously differentiable imply twice Fréchet differentiable?,Does twice continuously differentiable imply twice Fréchet differentiable?,,"For simplicity assume that $f:X\rightarrow\mathbb{R}$ for some open set $X\subseteq\mathbb{R}^{n}$ . If $f$ is twice continuously differentiable, does that imply that $f$ is twice Fréchet (or totally) differentiable? I know that once continuously differentiable implies once Fréchet differentiable (Theorem 6.2 in Munkres 'Analysis on manifolds'). Does simply applying this to the derivative of $f$ give that the implication in my question is true? My second question is how does the second Fréchet derivative of $f$ differ from the Fréchet derivative of the gradient mapping of $f$ (i.e. the Hessian). The issue is probably that I'm just still struggling with the concepts... Edit: So the definition I have for $k$ times continuously differentiable is that all partial derivatives of order less than or equal to $k$ exists and are continuous. On wikipedia they say that this is equivalent to that the $k$ th Fréchet derivative exists and is continuous. Edit 2: And I now found relevant proofs in 'Differential Calculus' by Cartan.","For simplicity assume that for some open set . If is twice continuously differentiable, does that imply that is twice Fréchet (or totally) differentiable? I know that once continuously differentiable implies once Fréchet differentiable (Theorem 6.2 in Munkres 'Analysis on manifolds'). Does simply applying this to the derivative of give that the implication in my question is true? My second question is how does the second Fréchet derivative of differ from the Fréchet derivative of the gradient mapping of (i.e. the Hessian). The issue is probably that I'm just still struggling with the concepts... Edit: So the definition I have for times continuously differentiable is that all partial derivatives of order less than or equal to exists and are continuous. On wikipedia they say that this is equivalent to that the th Fréchet derivative exists and is continuous. Edit 2: And I now found relevant proofs in 'Differential Calculus' by Cartan.",f:X\rightarrow\mathbb{R} X\subseteq\mathbb{R}^{n} f f f f f k k k,"['calculus', 'analysis', 'derivatives']"
98,Is square of a VMO function in VMO,Is square of a VMO function in VMO,,"We say a function $f\in L^1_{loc}(\mathbb{R})$ is in $\mathrm{BMO}(\mathbb{R})$ if $$\|f\|_{\mathrm{BMO}}=\sup_{I}\frac{1}{|I|}\int\limits_I |f(y)-f_I|\, dy<\infty$$ for all intervals $I\subset\mathbb{R},$ where $f_I$ is the average value of $f$ : $$f_I=\frac{1}{|I|}\int_I f(y)\, dy.$$ $f\in \mathrm{VMO}(\mathbb{R})$ if $f \in\mathrm{BMO}$ and $$\lim_{|I|\rightarrow 0}\frac{1}{|I|}\int_I|f(y)-f_I|\, dy\rightarrow 0. $$ For more details, see Bounded mean oscillation . Some facts that maybe useful UC $\cap$ BMO $\subset$ VMO, where UC means uniformly continuous functions in $\mathbb{R}$ . VMO is the closure of UC $\cap$ BMO in BMO. My question is 1.If $f\in $ VMO, is $f^2\in$ VMO? (If yes, next question is trivial; if not, please see next question) 2.If $\omega\in A_\infty$ , A_p weight and $\log \omega\in$ VMO, is $(\log \omega)^2 \in$ VMO? My try: Divide $f=f_1+f_2$ , where $f_1\in \mathrm{UC}\cap \mathrm{BMO},\ \|f_2\|_{BMO}<\epsilon$ . But I can't go on next. I am not sure it is correct, if not, could you give me a counterexample? Thanks very much.","We say a function is in if for all intervals where is the average value of : if and For more details, see Bounded mean oscillation . Some facts that maybe useful UC BMO VMO, where UC means uniformly continuous functions in . VMO is the closure of UC BMO in BMO. My question is 1.If VMO, is VMO? (If yes, next question is trivial; if not, please see next question) 2.If , A_p weight and VMO, is VMO? My try: Divide , where . But I can't go on next. I am not sure it is correct, if not, could you give me a counterexample? Thanks very much.","f\in L^1_{loc}(\mathbb{R}) \mathrm{BMO}(\mathbb{R}) \|f\|_{\mathrm{BMO}}=\sup_{I}\frac{1}{|I|}\int\limits_I |f(y)-f_I|\, dy<\infty I\subset\mathbb{R}, f_I f f_I=\frac{1}{|I|}\int_I f(y)\, dy. f\in \mathrm{VMO}(\mathbb{R}) f \in\mathrm{BMO} \lim_{|I|\rightarrow 0}\frac{1}{|I|}\int_I|f(y)-f_I|\, dy\rightarrow 0.  \cap \subset \mathbb{R} \cap f\in  f^2\in \omega\in A_\infty \log \omega\in (\log \omega)^2 \in f=f_1+f_2 f_1\in \mathrm{UC}\cap \mathrm{BMO},\ \|f_2\|_{BMO}<\epsilon","['real-analysis', 'analysis', 'harmonic-analysis']"
99,Continuous function nowhere Differentiable,Continuous function nowhere Differentiable,,"Let $D(x):\mathbb{R}\rightarrow\mathbb{R}$ be: $$ D(x)=\sum^\infty_{k=1}\frac{1}{k!}\sin((k+1)!x)$$ Prove that $D(x)$ is nowhere differentiable. What I've done is that I supposed that exist a $x\in \mathbb{R}$ where $D(x)$ is differentiables, so: $$\lim_{h\rightarrow0} \frac{D(x+h)-D(x)}{h}=a \Rightarrow$$ $$\lim_{h\rightarrow0} \frac{\sum^\infty_{k=1}\frac{1}{k!}\sin[(k+1)!(x+h)]-\sum^\infty_{k=1}\frac{1}{k!}\sin((k+1)!x)}{h}=$$ $$\lim_{h\rightarrow0} \frac{\sum^\infty_{k=1}\frac{1}{k!}[\sin[(k+1)!(x+h)]-\sin((k+1)!x)]}{h}=$$ $$\sum^\infty_{k=1}\frac{1}{k!}\lim_{h\rightarrow0}\frac{ 2\sin(\frac{(k+1)!(x+h)-(k+1)!x}{2})\cos(\frac{(k+1)!(x+h)+(k+1)!x}{2})}{h}=$$ $$\sum^\infty_{k=1}\frac{1}{k!}\lim_{h\rightarrow0}\frac{ 2\sin(\frac{(k+1)!h}{2})\cos(\frac{(k+1)!(2x+h)}{2})}{h}=\frac{0} {0} $$ Using L'hopital: $$ \sum^\infty_{k=1}\frac{1}{k!}\lim_{h\rightarrow0}\frac{\cos(\frac{(k+1)!h}{2})(\frac{(k+1)!}{2})\cos(\frac{(k+1)!(2x+h)}{2})-\sin(\frac{(k+1)!h}{2})(\frac{(k+1)!}{2})\sin(\frac{(k+1)!(2x+h)}{2})}{1} =$$ $$ \sum^\infty_{k=1}\frac{1}{k!}\lim_{h\rightarrow0}[(\frac{(k+1)!}{2})\cos(\frac{(k+1)!(2x+h)}{2})] =$$ $$ \sum^\infty_{k=1}\frac{1}{k!}(\frac{(k+1)!}{2})\cos((k+1)!(x)) = \sum^\infty_{k=1}\frac{k+1}{2}\cos((k+1)!(x)) $$ I don't know how to continue.","Let be: Prove that is nowhere differentiable. What I've done is that I supposed that exist a where is differentiables, so: Using L'hopital: I don't know how to continue.","D(x):\mathbb{R}\rightarrow\mathbb{R}  D(x)=\sum^\infty_{k=1}\frac{1}{k!}\sin((k+1)!x) D(x) x\in \mathbb{R} D(x) \lim_{h\rightarrow0} \frac{D(x+h)-D(x)}{h}=a \Rightarrow \lim_{h\rightarrow0} \frac{\sum^\infty_{k=1}\frac{1}{k!}\sin[(k+1)!(x+h)]-\sum^\infty_{k=1}\frac{1}{k!}\sin((k+1)!x)}{h}= \lim_{h\rightarrow0} \frac{\sum^\infty_{k=1}\frac{1}{k!}[\sin[(k+1)!(x+h)]-\sin((k+1)!x)]}{h}= \sum^\infty_{k=1}\frac{1}{k!}\lim_{h\rightarrow0}\frac{ 2\sin(\frac{(k+1)!(x+h)-(k+1)!x}{2})\cos(\frac{(k+1)!(x+h)+(k+1)!x}{2})}{h}= \sum^\infty_{k=1}\frac{1}{k!}\lim_{h\rightarrow0}\frac{ 2\sin(\frac{(k+1)!h}{2})\cos(\frac{(k+1)!(2x+h)}{2})}{h}=\frac{0}
{0}   \sum^\infty_{k=1}\frac{1}{k!}\lim_{h\rightarrow0}\frac{\cos(\frac{(k+1)!h}{2})(\frac{(k+1)!}{2})\cos(\frac{(k+1)!(2x+h)}{2})-\sin(\frac{(k+1)!h}{2})(\frac{(k+1)!}{2})\sin(\frac{(k+1)!(2x+h)}{2})}{1} =  \sum^\infty_{k=1}\frac{1}{k!}\lim_{h\rightarrow0}[(\frac{(k+1)!}{2})\cos(\frac{(k+1)!(2x+h)}{2})] =  \sum^\infty_{k=1}\frac{1}{k!}(\frac{(k+1)!}{2})\cos((k+1)!(x)) = \sum^\infty_{k=1}\frac{k+1}{2}\cos((k+1)!(x)) ","['real-analysis', 'analysis', 'differential']"
