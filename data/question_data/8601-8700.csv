,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Need to show that $\int_a ^b f(x)g(x)dx=0 \implies f\equiv 0$,Need to show that,\int_a ^b f(x)g(x)dx=0 \implies f\equiv 0,"The question below is inspired by this one. Assume $f$ is continuous on $[a,b]$ and $\int_a ^b f(x)g(x)dx=0 $ for   every  function $g$ possessing continuous derivative such that   $g(a)=g(b)=0$. Is it true that $f\equiv 0$ on the interval $[a,b]$? The hint I was given is to cook up a smooth function with compact support out of $e^{-1/x^2}$. Expanded version of the hint was this: set $f(x)=e^{-1/x^2}$ for $x$ positive and $f(x)=0$ for $x$ non-positive and consider $\int_0^xf(t)f(1-t)dt$. I have no idea what lies behind this hint (i.e., how could one come up with such an integral) and do not know how to use the hint either.","The question below is inspired by this one. Assume $f$ is continuous on $[a,b]$ and $\int_a ^b f(x)g(x)dx=0 $ for   every  function $g$ possessing continuous derivative such that   $g(a)=g(b)=0$. Is it true that $f\equiv 0$ on the interval $[a,b]$? The hint I was given is to cook up a smooth function with compact support out of $e^{-1/x^2}$. Expanded version of the hint was this: set $f(x)=e^{-1/x^2}$ for $x$ positive and $f(x)=0$ for $x$ non-positive and consider $\int_0^xf(t)f(1-t)dt$. I have no idea what lies behind this hint (i.e., how could one come up with such an integral) and do not know how to use the hint either.",,"['calculus', 'real-analysis', 'integration', 'derivatives']"
1,"If $(M,d)$ has an uncountable discrete subset then $M$ is not separable?",If  has an uncountable discrete subset then  is not separable?,"(M,d) M","I was thinking a bit about the proof of the fact that $\ell^{\infty}$ is not separable. And from the proof I saw, which uses a subspace which is discrete and uncountable, I thought I can prove it for any space which has the property. Let's take $M$ a metric space with a discrete subspace $X$. Let $D$ be a dense subset of $M$. For every $x\in X$, we consider the ball $B_x=B_{\frac{1}{3}}(x)$. Then since $D$ is dense, for every such $x$, $B_x\cap D\neq \varnothing$, so let's take an arbitrary fixed element $a(x)\in B_x\cap D$ for each $x$. Since $x\neq y\Rightarrow B_x\cap B_y=\varnothing$ (because $d(x,y)=1$), so $(B_x\cap D)\cap (B_y\cap D)=\varnothing\Rightarrow a(x)\neq a(y)$, so $x\mapsto a(x)$ is injective, thus $D$ is uncountable and $M$ is not separable. Is that right? Edit: Now I see that this can be solved using this since the subset $D$ would not be separable.","I was thinking a bit about the proof of the fact that $\ell^{\infty}$ is not separable. And from the proof I saw, which uses a subspace which is discrete and uncountable, I thought I can prove it for any space which has the property. Let's take $M$ a metric space with a discrete subspace $X$. Let $D$ be a dense subset of $M$. For every $x\in X$, we consider the ball $B_x=B_{\frac{1}{3}}(x)$. Then since $D$ is dense, for every such $x$, $B_x\cap D\neq \varnothing$, so let's take an arbitrary fixed element $a(x)\in B_x\cap D$ for each $x$. Since $x\neq y\Rightarrow B_x\cap B_y=\varnothing$ (because $d(x,y)=1$), so $(B_x\cap D)\cap (B_y\cap D)=\varnothing\Rightarrow a(x)\neq a(y)$, so $x\mapsto a(x)$ is injective, thus $D$ is uncountable and $M$ is not separable. Is that right? Edit: Now I see that this can be solved using this since the subset $D$ would not be separable.",,"['real-analysis', 'metric-spaces', 'separable-spaces']"
2,Continuity points of the function $f$ defined by $f(p/q)=\sqrt{(1+p^2)/(1+q^2)}$ and $f(x)=x$ if $x$ is irrational or zero,Continuity points of the function  defined by  and  if  is irrational or zero,f f(p/q)=\sqrt{(1+p^2)/(1+q^2)} f(x)=x x,"Let $f:[-1,1]\rightarrow\mathbb{R}$ defined by $f(x)=x$ for every $x\in[-1,1]\cap \mathbb{Q}^{c})\cup\{0\}$ and $f(x)=\sqrt{\frac{1+p^{2}}{1+q^{2}}}$ for every $x=\frac{p}{q} \in[-1,1]$. Then, $f$ is continuous on $((0,1)\cap\mathbb{Q}^{c})\cup\{0,1\}$. How to find the continuity and discontinuity points? Is there any theorem to check the continuity and continuous point for this type of function. Proving the continuity using $\epsilon-\delta$ definition like the proof of Thomae function is lengthy. Please help me to deduce the conclusion with less time.","Let $f:[-1,1]\rightarrow\mathbb{R}$ defined by $f(x)=x$ for every $x\in[-1,1]\cap \mathbb{Q}^{c})\cup\{0\}$ and $f(x)=\sqrt{\frac{1+p^{2}}{1+q^{2}}}$ for every $x=\frac{p}{q} \in[-1,1]$. Then, $f$ is continuous on $((0,1)\cap\mathbb{Q}^{c})\cup\{0,1\}$. How to find the continuity and discontinuity points? Is there any theorem to check the continuity and continuous point for this type of function. Proving the continuity using $\epsilon-\delta$ definition like the proof of Thomae function is lengthy. Please help me to deduce the conclusion with less time.",,['real-analysis']
3,A continuous nowhere differentiable function,A continuous nowhere differentiable function,,"I am self-reviewing some basic analysis (undergraduate level), and I stumped upon this question from Abbott's book Understanding Analysis : Let $g$ be a function such that $$g(x) = \sum_{n=0}^{\infty}\frac{1}{2^n} h(2^n x)$$ where $$h(x) = |x|$$ is defined on the interval $[-1,1]$ and we additionally require that $h(x) = h(x+2)$ . The two-part question is the following a). Show that $g(x)$ attains maximum on the interval $[0,2]$ , say $M$ , and find $M$ . b). Define the set $D = \{x \in [0, 2] : g(x) = M\}$ . Is the set $D$ finite, countable, or uncountable? Since $g(x)$ is continuous on $\mathbb{R}$ , it is obvious that $g(x)$ will attain maximum and minimum on $[0,2]$ , because the closed interval is compact. But I cannot figure out the rest of the problem. Any thought is appreciated.","I am self-reviewing some basic analysis (undergraduate level), and I stumped upon this question from Abbott's book Understanding Analysis : Let be a function such that where is defined on the interval and we additionally require that . The two-part question is the following a). Show that attains maximum on the interval , say , and find . b). Define the set . Is the set finite, countable, or uncountable? Since is continuous on , it is obvious that will attain maximum and minimum on , because the closed interval is compact. But I cannot figure out the rest of the problem. Any thought is appreciated.","g g(x) = \sum_{n=0}^{\infty}\frac{1}{2^n} h(2^n x) h(x) = |x| [-1,1] h(x) = h(x+2) g(x) [0,2] M M D = \{x \in [0, 2] : g(x) = M\} D g(x) \mathbb{R} g(x) [0,2]","['real-analysis', 'sequences-and-series', 'continuity']"
4,Amateur proof verification: Between two consecutive roots of $f'$ there is at most one root of $f$,Amateur proof verification: Between two consecutive roots of  there is at most one root of,f' f,"i need someone to verify if i'm doing anything wrong on proving the following theorem (i'm new to real analysis and formal proofs). Also, suggestions on how to write it better would be appreciated. $f: I\rightarrow R$ differentiable. Beteween two consecutives roots of $f'$ there is at most one root of $f$ I think i can see why this is true. Informal Attempt: Let $g$ be a constraint of $f$ to the interval $[a,b]$, $a<b$, $g: [a,b]\rightarrow R$. if $f'(a)=g'(a)$ and $f'(b)=g'(b)$ are consecutive zeros of $f'$, they are the sole zeros of $g'$. By the Weierstrass extreme value theorem, since $g$ is continuous and $[a,b]$ is a compact set, we know $g$ obtains its extreme values. Since the only two zeroes of $g'$ are, by its very definition, $a$ and $b$, they must be these extreme values. From this point on, i know that, by ""looking"" at the graph of $g$, it must intersect the $x$ axis at most once, otherwise there would be other  $g'(x)=0$. How can i write this down formally? Have i missed anything? After proving that result for $g$, I intended to apply it to $f$ and get to the final result. Thanks for your attention.","i need someone to verify if i'm doing anything wrong on proving the following theorem (i'm new to real analysis and formal proofs). Also, suggestions on how to write it better would be appreciated. $f: I\rightarrow R$ differentiable. Beteween two consecutives roots of $f'$ there is at most one root of $f$ I think i can see why this is true. Informal Attempt: Let $g$ be a constraint of $f$ to the interval $[a,b]$, $a<b$, $g: [a,b]\rightarrow R$. if $f'(a)=g'(a)$ and $f'(b)=g'(b)$ are consecutive zeros of $f'$, they are the sole zeros of $g'$. By the Weierstrass extreme value theorem, since $g$ is continuous and $[a,b]$ is a compact set, we know $g$ obtains its extreme values. Since the only two zeroes of $g'$ are, by its very definition, $a$ and $b$, they must be these extreme values. From this point on, i know that, by ""looking"" at the graph of $g$, it must intersect the $x$ axis at most once, otherwise there would be other  $g'(x)=0$. How can i write this down formally? Have i missed anything? After proving that result for $g$, I intended to apply it to $f$ and get to the final result. Thanks for your attention.",,"['calculus', 'real-analysis', 'functions', 'proof-verification', 'proof-writing']"
5,"Let $L(x)=\int_1^x \frac1t dt$ prove that $L(xy)=L(x)+L(y),\quad x,y>0$ and some other similar problems. (Without using logarithms)",Let  prove that  and some other similar problems. (Without using logarithms),"L(x)=\int_1^x \frac1t dt L(xy)=L(x)+L(y),\quad x,y>0","I'm looking at the following problem: Let $L:(0,\infty)\rightarrow\mathbb{R}$ be defined by $$L(x)=\int_1^x\frac1t dt, \qquad x>0$$ a) Prove: $L$ strictly increasing and differentiable on $(0,\infty)$ . b) Show that $L(2)>0$ c) Prove that $L(xy)=L(x)+L(y),\quad x,y>0$ d) Show that for $n\in \mathbb{Z}$ , $L(2^n)=nL(2)$ e) Prove that $L(x)$ is both injective and surjective. Now, a and b are simple enough (I'll still include them) but starting from c I'm having some serious trouble. It'd be easy to note that $L(x)$ is the logarithmic function, so all these things hold, but since we haven't discussed logarithms in class, I suppose this exercise is built to derive some of its properties. My work: a) Note how $\frac 1t$ is continuous on $(0,\infty)$ ; it follow immediately that $L(x)$ is differentiable and $L'(x)=1/x$ . Note also how $1/x>0$ for all $x>0$ . Because of this, $L(x)$ is strictly increasing. b) Consider $L(1)=\int_1^1\frac1t dt=0$ . (Fundamental theorem of calculus) Since $L(x)$ is strictly increasing, $L(2)>L(1)=0$ . c) No significant results, as I don't want to use the logarithm for this. d) Let $n\geq 0$ , then this follows immediately from c) . For $n<0$ I've tried to play around with the fundamental theorem of calculus but didn't get to any significant result. e) Note how $L(x)$ is strictly increasing and thus injective. To prove that $L(x)$ is on to, we have to take an arbitrary $y\in\mathbb{R}$ and show that $L(x)=y$ for some $x\in (0,\infty)$ . I can intuitively see how this would be the case and that it could probably be proven without use of logarithms, but find it difficult to come up with any sort of formal argument. Now I'm very curious to know whether these can be proven without making use of logarithms, and if so, what their proofs look like.","I'm looking at the following problem: Let be defined by a) Prove: strictly increasing and differentiable on . b) Show that c) Prove that d) Show that for , e) Prove that is both injective and surjective. Now, a and b are simple enough (I'll still include them) but starting from c I'm having some serious trouble. It'd be easy to note that is the logarithmic function, so all these things hold, but since we haven't discussed logarithms in class, I suppose this exercise is built to derive some of its properties. My work: a) Note how is continuous on ; it follow immediately that is differentiable and . Note also how for all . Because of this, is strictly increasing. b) Consider . (Fundamental theorem of calculus) Since is strictly increasing, . c) No significant results, as I don't want to use the logarithm for this. d) Let , then this follows immediately from c) . For I've tried to play around with the fundamental theorem of calculus but didn't get to any significant result. e) Note how is strictly increasing and thus injective. To prove that is on to, we have to take an arbitrary and show that for some . I can intuitively see how this would be the case and that it could probably be proven without use of logarithms, but find it difficult to come up with any sort of formal argument. Now I'm very curious to know whether these can be proven without making use of logarithms, and if so, what their proofs look like.","L:(0,\infty)\rightarrow\mathbb{R} L(x)=\int_1^x\frac1t dt, \qquad x>0 L (0,\infty) L(2)>0 L(xy)=L(x)+L(y),\quad x,y>0 n\in \mathbb{Z} L(2^n)=nL(2) L(x) L(x) \frac 1t (0,\infty) L(x) L'(x)=1/x 1/x>0 x>0 L(x) L(1)=\int_1^1\frac1t dt=0 L(x) L(2)>L(1)=0 n\geq 0 n<0 L(x) L(x) y\in\mathbb{R} L(x)=y x\in (0,\infty)","['real-analysis', 'integration', 'logarithms']"
6,How can I prove that the mean of squared data points is greater than the square of the mean of the data points?,How can I prove that the mean of squared data points is greater than the square of the mean of the data points?,,"The inequality I'm trying to prove is $$\frac{1}{n}\sum_{i=1}^nx_i^2 \geq \frac{1}{n^2}\left(\sum_{i=1}^nx_i\right)^2.$$ I tried simply expanding out the RHS but I can't seem to count the terms properly. I can reduce the inequality to $$(n-1)\sum_{i=1}^nx_i^2 - 2\sum_{i\neq j}x_ix_j \geq 0$$ but there appear to be $2(n-1)!$ terms in this $i\neq j$ sum and only $n(n-1)$ terms in the sum of the squares, which doesn't seem right to me. I've also ""assumed"" that $x_1 \leq\dots\leq x_n$ which allows me to set up the inequalities $x_1^2 \leq x_1x_2 \leq x_2^2$ etc. but my counting is letting me down. Is there an easier way to do this?","The inequality I'm trying to prove is $$\frac{1}{n}\sum_{i=1}^nx_i^2 \geq \frac{1}{n^2}\left(\sum_{i=1}^nx_i\right)^2.$$ I tried simply expanding out the RHS but I can't seem to count the terms properly. I can reduce the inequality to $$(n-1)\sum_{i=1}^nx_i^2 - 2\sum_{i\neq j}x_ix_j \geq 0$$ but there appear to be $2(n-1)!$ terms in this $i\neq j$ sum and only $n(n-1)$ terms in the sum of the squares, which doesn't seem right to me. I've also ""assumed"" that $x_1 \leq\dots\leq x_n$ which allows me to set up the inequalities $x_1^2 \leq x_1x_2 \leq x_2^2$ etc. but my counting is letting me down. Is there an easier way to do this?",,"['real-analysis', 'probability', 'means']"
7,"Fourier transform of non-vanishing $f \in C^\infty([-1,1])$: does it always look like $\frac{\sin x}{x}$?",Fourier transform of non-vanishing : does it always look like ?,"f \in C^\infty([-1,1]) \frac{\sin x}{x}","Given a smooth non-vanishing real even function $f \in C^\infty([-1,1])$ we can compute the Fourier transform (of its product with the indicator of the unit interval): $$   \widehat f(\xi)=\int_{-1}^1 e^{i\xi x}f(x) \, dx. $$ I'm plotting some examples, and it turns out that the qualitative behavior of $\widehat f$ is similar to that of $\frac{\sin x}{x}$ (which corresponds to $f(x) = 1$): $\hspace{11em}$ Is it always the case or it is possible to find such a function $f$ (smooth, $f(x) \neq 0$ for any $x$, even) for which the Fourier transform $\widehat f$ will be qualitatively different? In particular, is it possible to find such $f$ that $\widehat f$ does not have any zeros on the real line $\mathbb R$? More generally, if $f \in C^\infty(\overline B{}^d)$, where $\overline B{}^d$ denotes the unit ball in $\mathbb R^d$, $d \geq 2$, we can put $$   \widehat f(\xi)=\int_{\overline B{}^d} e^{i\xi x}f(x) \, dx. $$ If $f$ is spherically symmetric (i.e. $f(x)=f_0(|x|)$ for some scalar $f_0$) and $f(x)>0$ on $\overline B{}^d$, can we conclude that qualitatively $\widehat f$ behaves like  $$   \widehat{ 1_{\overline B{}^d}}(\xi) = |\xi|^{-\frac{d}{2}} J_{\frac d 2}{|\xi|} \quad ? $$","Given a smooth non-vanishing real even function $f \in C^\infty([-1,1])$ we can compute the Fourier transform (of its product with the indicator of the unit interval): $$   \widehat f(\xi)=\int_{-1}^1 e^{i\xi x}f(x) \, dx. $$ I'm plotting some examples, and it turns out that the qualitative behavior of $\widehat f$ is similar to that of $\frac{\sin x}{x}$ (which corresponds to $f(x) = 1$): $\hspace{11em}$ Is it always the case or it is possible to find such a function $f$ (smooth, $f(x) \neq 0$ for any $x$, even) for which the Fourier transform $\widehat f$ will be qualitatively different? In particular, is it possible to find such $f$ that $\widehat f$ does not have any zeros on the real line $\mathbb R$? More generally, if $f \in C^\infty(\overline B{}^d)$, where $\overline B{}^d$ denotes the unit ball in $\mathbb R^d$, $d \geq 2$, we can put $$   \widehat f(\xi)=\int_{\overline B{}^d} e^{i\xi x}f(x) \, dx. $$ If $f$ is spherically symmetric (i.e. $f(x)=f_0(|x|)$ for some scalar $f_0$) and $f(x)>0$ on $\overline B{}^d$, can we conclude that qualitatively $\widehat f$ behaves like  $$   \widehat{ 1_{\overline B{}^d}}(\xi) = |\xi|^{-\frac{d}{2}} J_{\frac d 2}{|\xi|} \quad ? $$",,"['calculus', 'real-analysis', 'fourier-analysis']"
8,Prove that $\pi$ is irrational help,Prove that  is irrational help,\pi,"I am working through Cartwright's proof for $\pi$ being irrational; specifically, the problem states: Given $$A_n=\int_{-1}^{1}(1-x^2)^n\cos\left(\frac {\pi x}{2}\right)\,dx$$ Prove: $$A_n=\frac {8n(2n-1)A_{n-1}-16n(n-1)A_{n-2}}{\pi ^2}.$$ I must prove it using integration by parts and have gotten really close to the answer but I am stuck at this step: $$A_n=\frac{-16n(n-1)}{\pi^2}\int_{-1}^{1}x^2(1-x^2)^{n-2}\cos\left(\frac {\pi x}{2}\right)+\frac {8n}{\pi^2}A_{n-1}.$$ Specifically, I'm wondering how one could get rid of the $x^2$ term out of the integral to get $A_{n-2}.$ Thanks in advance!","I am working through Cartwright's proof for $\pi$ being irrational; specifically, the problem states: Given $$A_n=\int_{-1}^{1}(1-x^2)^n\cos\left(\frac {\pi x}{2}\right)\,dx$$ Prove: $$A_n=\frac {8n(2n-1)A_{n-1}-16n(n-1)A_{n-2}}{\pi ^2}.$$ I must prove it using integration by parts and have gotten really close to the answer but I am stuck at this step: $$A_n=\frac{-16n(n-1)}{\pi^2}\int_{-1}^{1}x^2(1-x^2)^{n-2}\cos\left(\frac {\pi x}{2}\right)+\frac {8n}{\pi^2}A_{n-1}.$$ Specifically, I'm wondering how one could get rid of the $x^2$ term out of the integral to get $A_{n-2}.$ Thanks in advance!",,['real-analysis']
9,Infinite Sigma algebra contains infinite sequence of nonempty disjoint sets,Infinite Sigma algebra contains infinite sequence of nonempty disjoint sets,,"I realize this question has been answered but I wonder if this argument would work. Proof: If the infinite Sigma algebra X contains an infinite sequence of strictly nested nonempty sets, then we are done. Otherwise, take the set of all maximal strict nests in X, then the smallest element of these nests are disjoint from each other. If there are infinitely many then we are done. Otherwise, delete those small elements from each set in X and repeat the process above. Since the process can be repeated infinitely many times and the resulting smallest set in each nest form a disjoint set, the proof is finished.","I realize this question has been answered but I wonder if this argument would work. Proof: If the infinite Sigma algebra X contains an infinite sequence of strictly nested nonempty sets, then we are done. Otherwise, take the set of all maximal strict nests in X, then the smallest element of these nests are disjoint from each other. If there are infinitely many then we are done. Otherwise, delete those small elements from each set in X and repeat the process above. Since the process can be repeated infinitely many times and the resulting smallest set in each nest form a disjoint set, the proof is finished.",,"['real-analysis', 'measure-theory', 'elementary-set-theory', 'proof-verification']"
10,Help with using a Cauchy criterion for Riemann Integrability to show that a continuous function is Riemann Integrable,Help with using a Cauchy criterion for Riemann Integrability to show that a continuous function is Riemann Integrable,,"Let $f:[a,b]\rightarrow\mathbb{R}$.  A tagged partition, $\mathcal{P}$ of $[a,b]$ is a set of ordered pairs defined as $$\mathcal{P}:=\{([x_{k−1},x_k]),t_k)\}^n_{k=0},$$ where $a=x_0<...<x_n=b$ and the ""tags"" $t_k∈[x_{k−1},x_k]$, where $\mathbb{P}_{[a,b]}$ is the set of all tagged partitions over $[a,b]$. $$\|\mathcal{P}\|:=\sup\{x_k-x_{k-1}|1\leq k\leq n\}$$ is the mesh of the partition. The Riemann sum of $f$ over $[a,b]$ w.r.t $\mathcal{P}$ is defined as $$S(f,\mathcal{P}):=\sum\limits^n_{k=1}f(t_k)(x_k−x_{k−1})$$ and $f$ is said to be Riemann integrable with $\int_a^bf=L$ iff $$(\forall\epsilon>0)(\exists\delta>0)(\forall \mathcal{P}\in\mathbb{P}_{[a,b]})\bigg(\|\mathcal{P}\|<\delta\Rightarrow |S(f,\mathcal{P})-L|<\epsilon\bigg)$$ A Cauchy integrability criterion which is a straightforward exercise says that $f$ Riemann integrable on $[a,b]$ iff $$(\forall\epsilon>0)(\exists\delta>0)(\forall \mathcal{P}\in\mathbb{P}_{[a,b]})(\forall \mathcal{Q}\in\mathbb{P}_{[a,b]})\bigg(\|\mathcal{P}\|<\delta\wedge\|Q\|<\delta\Rightarrow |S(f,\mathcal{P})-S(f,\mathcal{Q})|<\epsilon\bigg).$$ My question is that how can we use the above definitions and Cauchy criterion to show that if $f$ is continuous on $[a,b]$ then $f$ is intgrable on $[a,b]$? If we use the Darboux definition for integrability which uses the upper and lower Riemann sums, then we have an analogous Cauachy integrabilityly condition with the difference of the upper and lower sums. With this and the fact that a continuous function is uniformly continuous on a compact set it is a very simple task to show that continuity is a sufficient condition for integrability.  However in this situation we do not have much flexibility as we need to take two different partitions so there is not much we can do with $|S(f,\mathcal{P})-S(f,\mathcal{Q})|$, and taking the common refinement partition of $\mathcal{P}\cup\mathcal{Q}$ leads nowhere.  So how do we in this setup up and not using upper and lower sums show that continuity implies integrability? So any input and assistance is greatly needed and appreciated.  Also if you have other proofs that don't use the Darboux definition of integrability it will be most welcomed. Thanks in advance","Let $f:[a,b]\rightarrow\mathbb{R}$.  A tagged partition, $\mathcal{P}$ of $[a,b]$ is a set of ordered pairs defined as $$\mathcal{P}:=\{([x_{k−1},x_k]),t_k)\}^n_{k=0},$$ where $a=x_0<...<x_n=b$ and the ""tags"" $t_k∈[x_{k−1},x_k]$, where $\mathbb{P}_{[a,b]}$ is the set of all tagged partitions over $[a,b]$. $$\|\mathcal{P}\|:=\sup\{x_k-x_{k-1}|1\leq k\leq n\}$$ is the mesh of the partition. The Riemann sum of $f$ over $[a,b]$ w.r.t $\mathcal{P}$ is defined as $$S(f,\mathcal{P}):=\sum\limits^n_{k=1}f(t_k)(x_k−x_{k−1})$$ and $f$ is said to be Riemann integrable with $\int_a^bf=L$ iff $$(\forall\epsilon>0)(\exists\delta>0)(\forall \mathcal{P}\in\mathbb{P}_{[a,b]})\bigg(\|\mathcal{P}\|<\delta\Rightarrow |S(f,\mathcal{P})-L|<\epsilon\bigg)$$ A Cauchy integrability criterion which is a straightforward exercise says that $f$ Riemann integrable on $[a,b]$ iff $$(\forall\epsilon>0)(\exists\delta>0)(\forall \mathcal{P}\in\mathbb{P}_{[a,b]})(\forall \mathcal{Q}\in\mathbb{P}_{[a,b]})\bigg(\|\mathcal{P}\|<\delta\wedge\|Q\|<\delta\Rightarrow |S(f,\mathcal{P})-S(f,\mathcal{Q})|<\epsilon\bigg).$$ My question is that how can we use the above definitions and Cauchy criterion to show that if $f$ is continuous on $[a,b]$ then $f$ is intgrable on $[a,b]$? If we use the Darboux definition for integrability which uses the upper and lower Riemann sums, then we have an analogous Cauachy integrabilityly condition with the difference of the upper and lower sums. With this and the fact that a continuous function is uniformly continuous on a compact set it is a very simple task to show that continuity is a sufficient condition for integrability.  However in this situation we do not have much flexibility as we need to take two different partitions so there is not much we can do with $|S(f,\mathcal{P})-S(f,\mathcal{Q})|$, and taking the common refinement partition of $\mathcal{P}\cup\mathcal{Q}$ leads nowhere.  So how do we in this setup up and not using upper and lower sums show that continuity implies integrability? So any input and assistance is greatly needed and appreciated.  Also if you have other proofs that don't use the Darboux definition of integrability it will be most welcomed. Thanks in advance",,"['real-analysis', 'continuity', 'riemann-sum', 'riemann-integration']"
11,Dense transfer of a set with positive Lebesgue measure: is it conull?,Dense transfer of a set with positive Lebesgue measure: is it conull?,,"I'm facing a problem in measure theory and I need to prove the following conjecture to move on. Attention: I'm not sure the following statement is true. Let $A \subset \mathbb{R}$ be a measurable set such that $m(A)>0$ and $H$ be a countable, dense subset of $\mathbb{R}$. If $A+H=\{a+h: a \in A, h \in H\}$, prove that $m((A+H)^c)=0$. I'm totally stuck. It's easy to see that $A+H=\displaystyle{\bigcup_{h \in H} A+h}$, so it's definitely a measurable set, but that's the only progress I've been able to make. Any help would be greatly appreciated!","I'm facing a problem in measure theory and I need to prove the following conjecture to move on. Attention: I'm not sure the following statement is true. Let $A \subset \mathbb{R}$ be a measurable set such that $m(A)>0$ and $H$ be a countable, dense subset of $\mathbb{R}$. If $A+H=\{a+h: a \in A, h \in H\}$, prove that $m((A+H)^c)=0$. I'm totally stuck. It's easy to see that $A+H=\displaystyle{\bigcup_{h \in H} A+h}$, so it's definitely a measurable set, but that's the only progress I've been able to make. Any help would be greatly appreciated!",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
12,Prove that $\lim_{n \rightarrow \infty} \int_0^\infty f_n(x) dx = \int_0^\infty f(x) dx$.,Prove that .,\lim_{n \rightarrow \infty} \int_0^\infty f_n(x) dx = \int_0^\infty f(x) dx,"This problem is from Chapter 7 of Rudin's Principles of Mathematical Analysis . Suppose $g$ and $f_n$ ($n = 1,2,3,...)$ are defined on $(0,\infty)$, are Riemann-integrable on $[t,T]$ whenever $0 < t < T < \infty$, $|f_n| \leq g, f_n \to f$ uniformly on every compact subset of $(0,\infty)$, and $$\int_{0}^{\infty}g(x) dx < \infty.$$ Prove that $$\lim_{n\rightarrow \infty} \int_{0}^\infty f_n(x) dx = \int_{0}^{\infty} f(x) dx.$$ My attempt at the solution: Since $|f_n| \leq g$ and $\int_0^\infty g(x) dx < \infty$, it follows that $\int_0^\infty f_n(x) dx < \infty$ (and also $\int_0^\infty f(x) dx < \infty$ by uniform convergence). Also, $$ \lim_{n \to \infty} \int_{0}^\infty f_n(t) dt = \lim_{n \to \infty} \lim_{T \to \infty} \int_{0}^T f_n(t) dt. $$ If we could interchange the two limits $\lim_{n \to \infty} \lim_{T \to \infty}$, then we would have our result due to the fact that $f_n \in \mathscr{R}$ and $f_n \to f$ (Theorem 7.16). My question: What is the formal justification for being able to interchange the two limits? I have had a few ideas, but none of them have seemed any good.","This problem is from Chapter 7 of Rudin's Principles of Mathematical Analysis . Suppose $g$ and $f_n$ ($n = 1,2,3,...)$ are defined on $(0,\infty)$, are Riemann-integrable on $[t,T]$ whenever $0 < t < T < \infty$, $|f_n| \leq g, f_n \to f$ uniformly on every compact subset of $(0,\infty)$, and $$\int_{0}^{\infty}g(x) dx < \infty.$$ Prove that $$\lim_{n\rightarrow \infty} \int_{0}^\infty f_n(x) dx = \int_{0}^{\infty} f(x) dx.$$ My attempt at the solution: Since $|f_n| \leq g$ and $\int_0^\infty g(x) dx < \infty$, it follows that $\int_0^\infty f_n(x) dx < \infty$ (and also $\int_0^\infty f(x) dx < \infty$ by uniform convergence). Also, $$ \lim_{n \to \infty} \int_{0}^\infty f_n(t) dt = \lim_{n \to \infty} \lim_{T \to \infty} \int_{0}^T f_n(t) dt. $$ If we could interchange the two limits $\lim_{n \to \infty} \lim_{T \to \infty}$, then we would have our result due to the fact that $f_n \in \mathscr{R}$ and $f_n \to f$ (Theorem 7.16). My question: What is the formal justification for being able to interchange the two limits? I have had a few ideas, but none of them have seemed any good.",,"['real-analysis', 'analysis', 'uniform-convergence']"
13,How to characterize functions that map convex sets to convex sets?,How to characterize functions that map convex sets to convex sets?,,"Let $f: \mathbb{R}^n \to \mathbb{R}^n$. What is a necessary and sufficient condition for the following? If $C$ is a convex subset of $\mathbb{R}^n$, then so is $f(C)$. It's easy to find various sufficient conditions (I won't attempt an exhaustive list here), but I've been unable to find interesting necessary conditions. If it helps, I'm happy to assume that $C$ is closed and/or bounded. If a necessary and sufficient condition is known for $\mathbb{R}^n$, can it be extended to general vector spaces? Alternatively, if such a condition is unknown, could someone explain why the problem is difficult?","Let $f: \mathbb{R}^n \to \mathbb{R}^n$. What is a necessary and sufficient condition for the following? If $C$ is a convex subset of $\mathbb{R}^n$, then so is $f(C)$. It's easy to find various sufficient conditions (I won't attempt an exhaustive list here), but I've been unable to find interesting necessary conditions. If it helps, I'm happy to assume that $C$ is closed and/or bounded. If a necessary and sufficient condition is known for $\mathbb{R}^n$, can it be extended to general vector spaces? Alternatively, if such a condition is unknown, could someone explain why the problem is difficult?",,"['real-analysis', 'reference-request', 'convex-analysis']"
14,"Finding the value of limit when $k\in(-1,1)$",Finding the value of limit when,"k\in(-1,1)","My question arises from How find this $\lim_{n\to\infty}n^2\left(\frac{1^k+2^k+\cdots+n^k}{n^{k+1}}-\frac{1}{k+1}-\frac{1}{2n}\right)$ . $\\$ 1 .If $k\in(1,+\infty),$ then $$\lim_{n\to\infty}n^2\left(\frac{1^k+2^k+\cdots+n^k}{n^{k+1}}-\frac{1}{k+1}-\frac{1}{2n}\right)=\frac{k}{12};$$ 2 . If $k=1,$ then $$\lim_{n\to\infty}n^2\left(\frac{1^k+2^k+\cdots+n^k}{n^{k+1}}-\frac{1}{k+1}-\frac{1}{2n}\right)=0;$$ 3 .If $k\in(-\infty,-1),$ then $$\lim_{n\to\infty}n^2\left(\frac{1^k+2^k+\cdots+n^k}{n^{k+1}}-\frac{1}{k+1}-\frac{1}{2n}\right)=+\infty.$$ When $k\in (-1,1),$ I guess we could get $$\lim_{n\to\infty}n^2\left(\frac{1^k+2^k+\cdots+n^k}{n^{k+1}}-\frac{1}{k+1}-\frac{1}{2n}\right)=-\infty.$$ Euler-Maclaurin Summation Formula is at work in this case,but I need more convenient method to solve the problem.  Can anyone give me any hints on how to start it? Any help will be appreciated.","My question arises from How find this . 1 .If then 2 . If then 3 .If then When I guess we could get Euler-Maclaurin Summation Formula is at work in this case,but I need more convenient method to solve the problem.  Can anyone give me any hints on how to start it? Any help will be appreciated.","\lim_{n\to\infty}n^2\left(\frac{1^k+2^k+\cdots+n^k}{n^{k+1}}-\frac{1}{k+1}-\frac{1}{2n}\right) \\ k\in(1,+\infty), \lim_{n\to\infty}n^2\left(\frac{1^k+2^k+\cdots+n^k}{n^{k+1}}-\frac{1}{k+1}-\frac{1}{2n}\right)=\frac{k}{12}; k=1, \lim_{n\to\infty}n^2\left(\frac{1^k+2^k+\cdots+n^k}{n^{k+1}}-\frac{1}{k+1}-\frac{1}{2n}\right)=0; k\in(-\infty,-1), \lim_{n\to\infty}n^2\left(\frac{1^k+2^k+\cdots+n^k}{n^{k+1}}-\frac{1}{k+1}-\frac{1}{2n}\right)=+\infty. k\in (-1,1), \lim_{n\to\infty}n^2\left(\frac{1^k+2^k+\cdots+n^k}{n^{k+1}}-\frac{1}{k+1}-\frac{1}{2n}\right)=-\infty.","['real-analysis', 'limits']"
15,Converge uniformly on open interval implies on closed interval,Converge uniformly on open interval implies on closed interval,,"Suppose $f_n(x)$ is defined on $[a,b]$, and it converges uniformly to $f(x)$ on $(a,b)$. And the sequences $f_n(a)$ and $f_n(b)$ both converge (say, to points $c$ and $d$ respectively). I want to prove $f_n(x)$ is uniformly convergent on $[a,b]$. I know that when the goal is to proving convergence, we can combine the points $c,d$ and $f(x)$ to a new function, then the convergence is justified. But I do not know how to proceed for uniform convergence.","Suppose $f_n(x)$ is defined on $[a,b]$, and it converges uniformly to $f(x)$ on $(a,b)$. And the sequences $f_n(a)$ and $f_n(b)$ both converge (say, to points $c$ and $d$ respectively). I want to prove $f_n(x)$ is uniformly convergent on $[a,b]$. I know that when the goal is to proving convergence, we can combine the points $c,d$ and $f(x)$ to a new function, then the convergence is justified. But I do not know how to proceed for uniform convergence.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'uniform-convergence', 'cauchy-sequences']"
16,smoothness of solution to heat equation + differentiation under integral sign,smoothness of solution to heat equation + differentiation under integral sign,,"I am reading Evan's PDE book, and I need some help understanding the following result [Theorem 1 on pg47 of the book] Let $g$ be continuous and essentially bounded function on $\mathbb{R}^n$ and let $K$ be the heat kernel. Then, the function $u$ which is a convolution of $g$ and $K$ is $C^{\infty}$ . The proof of this theorem goes as: Since $K$ is infinitely differentiable, with uniformly bounded derivatives of all orders, on $[\delta, \infty)$ for each $\delta > 0$ , we see that $u$ is $C^{\infty}$ I am not really understanding this proof. (1) Am I correct that the uniform boundedness of derivatives of all orders means that: There exists a constant $M$ such that for every non-negative integer $\alpha$ and multi-index $\beta$ , $|\frac{\partial^\alpha}{\partial t^{\alpha}} D^{\beta} K(x,t)| \leq M$ for every $x$ and $t$ in $\mathbb{R}^n \times [\delta, \infty)$ ? If so, how do I know that the derivatives are uniformly bounded? I know that each derivative is bounded since $t\geq \delta > 0 $ , but how do I show the existence of the uniform constant $M$ ? (2) Why does the uniform boundedness of all derivatives allow us to differentiate under the integral sign? If I let $\Delta f_h$ to denote the difference quotient for the corresponding derivative $Df$ , then I can write $$\int |\Delta f_h(x-y)| g(y) dy =  \int |Df(x-y+c)| g(y) dy$$ for some $c$ between $x-y$ and $x-y+h$ by the mean value theorem, so I have as my dominating function $M |g(y)|$ where $M$ is the uniform bound constant, but since $g$ is only bounded and not necessarily integrable, so I cannot apply the dominated convergence theorem. What am I doing wrong? It was never intuiviely clear to me when differentiation under the integral is allowed and when it is not allowed. For example, (3) suppose that I have a function $f(x,y)$ in $\mathbb{R}^2$ and assume further that we know $\frac{\partial}{\partial x} f(x,y)$ exists and is integrable in $y$ over $\mathbb{R}$ . Then, is it always the case $\dfrac{d}{dx} \int_{\mathbb{R}} f(x,y)dy = \int_{\mathbb{R}} \frac{\partial}{\partial x} f(x,y) dy$ ?","I am reading Evan's PDE book, and I need some help understanding the following result [Theorem 1 on pg47 of the book] Let be continuous and essentially bounded function on and let be the heat kernel. Then, the function which is a convolution of and is . The proof of this theorem goes as: Since is infinitely differentiable, with uniformly bounded derivatives of all orders, on for each , we see that is I am not really understanding this proof. (1) Am I correct that the uniform boundedness of derivatives of all orders means that: There exists a constant such that for every non-negative integer and multi-index , for every and in ? If so, how do I know that the derivatives are uniformly bounded? I know that each derivative is bounded since , but how do I show the existence of the uniform constant ? (2) Why does the uniform boundedness of all derivatives allow us to differentiate under the integral sign? If I let to denote the difference quotient for the corresponding derivative , then I can write for some between and by the mean value theorem, so I have as my dominating function where is the uniform bound constant, but since is only bounded and not necessarily integrable, so I cannot apply the dominated convergence theorem. What am I doing wrong? It was never intuiviely clear to me when differentiation under the integral is allowed and when it is not allowed. For example, (3) suppose that I have a function in and assume further that we know exists and is integrable in over . Then, is it always the case ?","g \mathbb{R}^n K u g K C^{\infty} K [\delta, \infty) \delta > 0 u C^{\infty} M \alpha \beta |\frac{\partial^\alpha}{\partial t^{\alpha}} D^{\beta} K(x,t)| \leq M x t \mathbb{R}^n \times [\delta, \infty) t\geq \delta > 0  M \Delta f_h Df \int |\Delta f_h(x-y)| g(y) dy =  \int |Df(x-y+c)| g(y) dy c x-y x-y+h M |g(y)| M g f(x,y) \mathbb{R}^2 \frac{\partial}{\partial x} f(x,y) y \mathbb{R} \dfrac{d}{dx} \int_{\mathbb{R}} f(x,y)dy = \int_{\mathbb{R}} \frac{\partial}{\partial x} f(x,y) dy","['real-analysis', 'measure-theory', 'partial-differential-equations']"
17,Is $f(x)=\mu(V+x)$ continuous?,Is  continuous?,f(x)=\mu(V+x),"Suppose $V$ is open in $R^k$ and $\mu$ is a finite postive Borel measure on $R^k$. Is the function $f(x)=\mu(V+x)$ continuous? lower semicontinuous? upper semicontinuous? Clearly $f$ cannot be guaranteed to be continuous or even upper semicontinuous. If we let $$\mu_{x_0}(E)=\begin{cases} 1 & x_0\in E \\ 0 & x_0\not\in E \end{cases}$$ and $V= B_r(x_0)$, we then find that it is strictly lower semicontinuous. But I am finding it hard to: Find an example where $f$ isn't lower semicontinuous. Find a proof that $f$ must be lower semicontinuous. Any Ideas as to how .","Suppose $V$ is open in $R^k$ and $\mu$ is a finite postive Borel measure on $R^k$. Is the function $f(x)=\mu(V+x)$ continuous? lower semicontinuous? upper semicontinuous? Clearly $f$ cannot be guaranteed to be continuous or even upper semicontinuous. If we let $$\mu_{x_0}(E)=\begin{cases} 1 & x_0\in E \\ 0 & x_0\not\in E \end{cases}$$ and $V= B_r(x_0)$, we then find that it is strictly lower semicontinuous. But I am finding it hard to: Find an example where $f$ isn't lower semicontinuous. Find a proof that $f$ must be lower semicontinuous. Any Ideas as to how .",,"['real-analysis', 'measure-theory']"
18,Riemann and Darboux Integral of a product of two functions,Riemann and Darboux Integral of a product of two functions,,"I'm studying the Darboux definition of integrability, which I completely explained here . There's an exercise that asks me to prove that the Darboux Integrability is equivalent to Riemann Integrability, but this Riemann integral is defined as the following: It first defines a 'pointed' partition (I don't know how to say it in ensligh) by the following: a 'pointed' partition $[a,b]$ is a pair $P^*=(E,ξ)$, where $P=\{t_0, t_1, \cdots, t_n\}$ is a partition of $[a,b]$ and $ξ = (ξ_1, \cdots, ξ_n)$ is a list of $n$ chosen numbers such that $t_{i-1}\le ξ_i\le t_i$ for each $i=1,\cdots ,n$. Now, the Riemann Integral is defined as: $$\sum(f,P^*) = \sum_{i=1}^n f(ξ_i)(t_i-t_{i-1})$$ (I didn't understand the notation for the left hand side of the equation, by the way) Finally, I'm asked to prove the following: given $f,g:[a,b]\to \mathbb{R}$ integrable functions, for the entire partition $P=\{t_0, \cdots, t_n\}$ of $[a,b]$ let $P^* = (P,ξ)$ and $P^{\#} = (P, η)$ be pointed partitions of $P$, then: $$\lim_{|P|\to 0}\sum f(ξ_i)g(η_i)(t_i-t_{i-1}) = \int_a^b f(x)g(x) \ dx$$ I guess here I need to prove that the Riemann Integral of the product of two functions if the darboux integral of $f(x)g(x)$, but it seems too obvious, I just need to verify that $f,g$ are integrable, then their product is too, isn't it? I'm pretty sure this should be a hard question. Is there another interpretation that I'm missing?","I'm studying the Darboux definition of integrability, which I completely explained here . There's an exercise that asks me to prove that the Darboux Integrability is equivalent to Riemann Integrability, but this Riemann integral is defined as the following: It first defines a 'pointed' partition (I don't know how to say it in ensligh) by the following: a 'pointed' partition $[a,b]$ is a pair $P^*=(E,ξ)$, where $P=\{t_0, t_1, \cdots, t_n\}$ is a partition of $[a,b]$ and $ξ = (ξ_1, \cdots, ξ_n)$ is a list of $n$ chosen numbers such that $t_{i-1}\le ξ_i\le t_i$ for each $i=1,\cdots ,n$. Now, the Riemann Integral is defined as: $$\sum(f,P^*) = \sum_{i=1}^n f(ξ_i)(t_i-t_{i-1})$$ (I didn't understand the notation for the left hand side of the equation, by the way) Finally, I'm asked to prove the following: given $f,g:[a,b]\to \mathbb{R}$ integrable functions, for the entire partition $P=\{t_0, \cdots, t_n\}$ of $[a,b]$ let $P^* = (P,ξ)$ and $P^{\#} = (P, η)$ be pointed partitions of $P$, then: $$\lim_{|P|\to 0}\sum f(ξ_i)g(η_i)(t_i-t_{i-1}) = \int_a^b f(x)g(x) \ dx$$ I guess here I need to prove that the Riemann Integral of the product of two functions if the darboux integral of $f(x)g(x)$, but it seems too obvious, I just need to verify that $f,g$ are integrable, then their product is too, isn't it? I'm pretty sure this should be a hard question. Is there another interpretation that I'm missing?",,"['calculus', 'real-analysis', 'integration']"
19,Outer measure of rationals between 0 and 1. Royden.,Outer measure of rationals between 0 and 1. Royden.,,"I've been studying Royden and I have found something that looks like a contradiction to me, and can't find where it is that my reasoning is wrong. On Chapter 3 he defines the outer measure $m^*$ of a set $A$ as: $m^*A = \inf_{A \subset I_n} \sum l(I_n)$ where $\{I_n\}$ is a countable collection of open intervals, and $l$ is the length. What troubles me is that corollary 3 on chapter 3 says: ""If $A$ is countable, $m^*A = 0$"". And right bellow problem 5 states: ""Let $A$ be the set of rational numbers between 0 and 1, and let $\{I_n\}$ be a finite collection of open intervals covering $A$. Then $\sum l(I_n) \geq 1$"". My question is, how can every open covering of the rationals between 0 and 1 have length grater or equal to 1 if at the same time, by virtue of being countable, the outer measure of the rationals is 0 (corollary 3) and hence: $m^*(\mathbb{Q}\cap[0,1]) = \inf_{\mathbb{Q} \cap [0,1] \subset I_n}\sum l(I_n) = 0$. Isn't there a contradiction in saying $\inf_{\mathbb{Q} \cap [0,1] \subset I_n} \sum l(I_n) = 0$, and for all $\{I_n\}$, $\sum l(I_n) \ge 1$?","I've been studying Royden and I have found something that looks like a contradiction to me, and can't find where it is that my reasoning is wrong. On Chapter 3 he defines the outer measure $m^*$ of a set $A$ as: $m^*A = \inf_{A \subset I_n} \sum l(I_n)$ where $\{I_n\}$ is a countable collection of open intervals, and $l$ is the length. What troubles me is that corollary 3 on chapter 3 says: ""If $A$ is countable, $m^*A = 0$"". And right bellow problem 5 states: ""Let $A$ be the set of rational numbers between 0 and 1, and let $\{I_n\}$ be a finite collection of open intervals covering $A$. Then $\sum l(I_n) \geq 1$"". My question is, how can every open covering of the rationals between 0 and 1 have length grater or equal to 1 if at the same time, by virtue of being countable, the outer measure of the rationals is 0 (corollary 3) and hence: $m^*(\mathbb{Q}\cap[0,1]) = \inf_{\mathbb{Q} \cap [0,1] \subset I_n}\sum l(I_n) = 0$. Isn't there a contradiction in saying $\inf_{\mathbb{Q} \cap [0,1] \subset I_n} \sum l(I_n) = 0$, and for all $\{I_n\}$, $\sum l(I_n) \ge 1$?",,"['real-analysis', 'outer-measure']"
20,Apostol Bolzano-Weierstrass Theorem,Apostol Bolzano-Weierstrass Theorem,,"Theorem . If a bounded set $S$ in $\mathbb{R}^n$ contains infinitely many points, then there is at least one point in $\mathbb{R}^n$ which is an accumulation point of $S$. Proof . (for $\mathbb{R}^1$) Since $S$ is bounded, it lies in some interval $[−a,a]$. At least one of the subintervals $[−a,0]$ or $[0,a]$ contains an infinite subset of $S$. Call one such subinterval $[a_1,b_1]$. Bisect $[a_1,b_1]$ and obtain a subinterval $[a_2,b_2]$ containing an infinite subset of $S$, and continue this process. In this way a countable collection of intervals is obtained, the $nth$ interval $[a_n,b_n]$ being of length $b_n -a_n = a/2^{n-1}$. Clearly the $\sup$ of the left endpoints $a_n$ and the $\inf$ of the right endpoints $b_n$ must be equal, say to $x$. The point $x$ will be an accumulation point of $S$ because, if $r$ is any positive number, the interval $[a_n,b_n]$ will be contained in $B(x;r)$ as soon as $n$ is large enough so that $b_n−a_n<r/2$. The interval $B(x;r)$ contains a point of $S$ distinct from $x$ and hence $x$ is an accumulation point of $S$. My Questions Why is it obvious that the $\sup$ of the left endpoint $a_n$ is equal to the $\inf$ of the right endpoint $b_n$? Also how does one consider the $\sup$ or $\inf$ of a single number? I don't understand why we need $b_n−a_n<r/2$. Instead, why do we need to halve $r$?  If $b_n−a_n<r$ then wouldn't $[a_n,b_n]$ still be contained in $B(x;r)$?","Theorem . If a bounded set $S$ in $\mathbb{R}^n$ contains infinitely many points, then there is at least one point in $\mathbb{R}^n$ which is an accumulation point of $S$. Proof . (for $\mathbb{R}^1$) Since $S$ is bounded, it lies in some interval $[−a,a]$. At least one of the subintervals $[−a,0]$ or $[0,a]$ contains an infinite subset of $S$. Call one such subinterval $[a_1,b_1]$. Bisect $[a_1,b_1]$ and obtain a subinterval $[a_2,b_2]$ containing an infinite subset of $S$, and continue this process. In this way a countable collection of intervals is obtained, the $nth$ interval $[a_n,b_n]$ being of length $b_n -a_n = a/2^{n-1}$. Clearly the $\sup$ of the left endpoints $a_n$ and the $\inf$ of the right endpoints $b_n$ must be equal, say to $x$. The point $x$ will be an accumulation point of $S$ because, if $r$ is any positive number, the interval $[a_n,b_n]$ will be contained in $B(x;r)$ as soon as $n$ is large enough so that $b_n−a_n<r/2$. The interval $B(x;r)$ contains a point of $S$ distinct from $x$ and hence $x$ is an accumulation point of $S$. My Questions Why is it obvious that the $\sup$ of the left endpoint $a_n$ is equal to the $\inf$ of the right endpoint $b_n$? Also how does one consider the $\sup$ or $\inf$ of a single number? I don't understand why we need $b_n−a_n<r/2$. Instead, why do we need to halve $r$?  If $b_n−a_n<r$ then wouldn't $[a_n,b_n]$ still be contained in $B(x;r)$?",,"['real-analysis', 'analysis']"
21,What benefits do real numbers bring to the theory of rational numbers?,What benefits do real numbers bring to the theory of rational numbers?,,"Complex numbers make it easier to find real solutions of real polynomial equations. Algebraic topology makes it easier to prove theorems of (very) elementary topology (e.g. the invariance of domain theorem). In that sense, what are theorems purely about rational numbers whose proofs are greatly helped by the introduction of real numbers? By ""purely"" I mean: not about Cauchy sequences, Dedekind cuts, etc. of rational numbers. (This is of course a meta-mathematical statement and therefore imprecise by nature.) ""No, there is no such thing, because..."" would also be a valuable answer.","Complex numbers make it easier to find real solutions of real polynomial equations. Algebraic topology makes it easier to prove theorems of (very) elementary topology (e.g. the invariance of domain theorem). In that sense, what are theorems purely about rational numbers whose proofs are greatly helped by the introduction of real numbers? By ""purely"" I mean: not about Cauchy sequences, Dedekind cuts, etc. of rational numbers. (This is of course a meta-mathematical statement and therefore imprecise by nature.) ""No, there is no such thing, because..."" would also be a valuable answer.",,"['real-analysis', 'soft-question', 'real-numbers']"
22,Inequality of absolute values of complex sums,Inequality of absolute values of complex sums,,"Let $c_1,\dots, c_n\geq 0$ and $x,\dots,x_n\in\mathbb R$. Then $$\left\lvert \sum_k \frac{c_k x}{(x_k-i x)^2}\right\rvert\leq \left\lvert \sum_k \frac{c_k }{x_k-i x}\right\rvert$$ seems to hold for all real $x\not=0$. Why is this the case? It's trivial for $n=1$, but already for $n=2$ writing out these absolute values becomes rather messy. Am I overlooking something?","Let $c_1,\dots, c_n\geq 0$ and $x,\dots,x_n\in\mathbb R$. Then $$\left\lvert \sum_k \frac{c_k x}{(x_k-i x)^2}\right\rvert\leq \left\lvert \sum_k \frac{c_k }{x_k-i x}\right\rvert$$ seems to hold for all real $x\not=0$. Why is this the case? It's trivial for $n=1$, but already for $n=2$ writing out these absolute values becomes rather messy. Am I overlooking something?",,"['real-analysis', 'complex-analysis', 'inequality']"
23,Null set vs Measure zero set,Null set vs Measure zero set,,"In the context of Lebesgue measure on $\mathbb{R}^n$, is null set the same thing as a set of measure zero? I understand that null set implies measure zero, not sure about the other direction. Update: By null set I mean https://en.m.wikipedia.org/wiki/Null_set a set that can be covered by a countable union of intervals(balls)  of arbitrarily small total length. To be precise: Is a set with measure zero coverable by countable union of balls of arbitrarily small total length?","In the context of Lebesgue measure on $\mathbb{R}^n$, is null set the same thing as a set of measure zero? I understand that null set implies measure zero, not sure about the other direction. Update: By null set I mean https://en.m.wikipedia.org/wiki/Null_set a set that can be covered by a countable union of intervals(balls)  of arbitrarily small total length. To be precise: Is a set with measure zero coverable by countable union of balls of arbitrarily small total length?",,"['real-analysis', 'lebesgue-measure']"
24,Outer measure of a nested sequence of non-measurable sets,Outer measure of a nested sequence of non-measurable sets,,"Let $\bigcup_{n=1}^\infty E_n=E$ and $ E_{n}  \subseteq  E_{n+1} $ then $\lim\limits_{n\mapsto \infty} \mu^*(E_n) = \mu^*(E) $ even if each $E_n$ is a non-measurable set, where $\mu^*$ is Lebesgue outer measure. $E$ is a bounded set.Proof sketch please? This theorem allows the short proofs of Dominated convergence theorem, Vitali Convergence Theorem, Monotone Convergence Theorem , Egorov's theorem and Luzin's theorem without dwelling much on the machinery of measure theory.","Let and then even if each is a non-measurable set, where is Lebesgue outer measure. is a bounded set.Proof sketch please? This theorem allows the short proofs of Dominated convergence theorem, Vitali Convergence Theorem, Monotone Convergence Theorem , Egorov's theorem and Luzin's theorem without dwelling much on the machinery of measure theory.",\bigcup_{n=1}^\infty E_n=E  E_{n}  \subseteq  E_{n+1}  \lim\limits_{n\mapsto \infty} \mu^*(E_n) = \mu^*(E)  E_n \mu^* E,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
25,Why is continuity permissible at endpoints but not differentiability? [duplicate],Why is continuity permissible at endpoints but not differentiability? [duplicate],,"This question already has answers here : Differentiablility over closed intervals (3 answers) Closed 5 years ago . Differentiable at endpoints? Does differentiation only work on open sets? Admittedly, there are some questions and answers as to why a function defined on a closed interval is not differentiable on the interval's endpoints. But I find no answer as to why, in spite of this, continuity can be defined on an interval's endpoints. For differentiability, the intuition is that the neighborhood of $x$ that allows $\lim_{x \rightarrow c}\frac{f(x) - f(c)}{x - c} = L\quad$ must (or is this our definition) be populated from both the left ($x < c\;$) and the right ($x > c\;$). Hence, differentiability isn't defined on endpoints of an interval. Why does such a requirement not exist for continuity? A function $f$ is continuous at $c$ if $$ \forall \epsilon, \exists \delta, |x - c| < \delta \rightarrow |f(x) - f(c)| < \epsilon$$ Continuity seems not to care whether points appear left or right of $c$ -- just that, if they exist in some chosen neighborhood, they satisfy the condition above. Here is an example of this ""dichotomy""","This question already has answers here : Differentiablility over closed intervals (3 answers) Closed 5 years ago . Differentiable at endpoints? Does differentiation only work on open sets? Admittedly, there are some questions and answers as to why a function defined on a closed interval is not differentiable on the interval's endpoints. But I find no answer as to why, in spite of this, continuity can be defined on an interval's endpoints. For differentiability, the intuition is that the neighborhood of $x$ that allows $\lim_{x \rightarrow c}\frac{f(x) - f(c)}{x - c} = L\quad$ must (or is this our definition) be populated from both the left ($x < c\;$) and the right ($x > c\;$). Hence, differentiability isn't defined on endpoints of an interval. Why does such a requirement not exist for continuity? A function $f$ is continuous at $c$ if $$ \forall \epsilon, \exists \delta, |x - c| < \delta \rightarrow |f(x) - f(c)| < \epsilon$$ Continuity seems not to care whether points appear left or right of $c$ -- just that, if they exist in some chosen neighborhood, they satisfy the condition above. Here is an example of this ""dichotomy""",,"['real-analysis', 'derivatives', 'continuity']"
26,Why is the image of the implicit function in the implicit function theorem not open?,Why is the image of the implicit function in the implicit function theorem not open?,,"We have a continuously differentiable function $f$ from $\mathbb{R}^{n+m}$ to $\mathbb{R}^n$, and we find a continuously differentiable function $g$ which maps points from $\mathbb{R}^m$ into $\mathbb{R}^n$ (this function defines $x \in \mathbb{R}^n$ ""implicitly"" in terms of $y \in \mathbb{R}^m $). What regularity properties must the image of $g$ have as a direct result of the proof of the implicit function theorem (if any) in order to allow meaningful differentiable geometry (e.g. manifolds) to ""happen""? In the proof of the Inverse (not implicit) function theorem, we need to care about the properties of the image of $f$. Yet for some reason the properties of the image of $g$ in the Implicit (not inverse) function theorem don't matter? Why is that? What properties, if any, of $g$ in the implicit function theorem, then, are important for the definitions of manifolds and tangent spaces and bundles? This question seems to address a related issue, but the discussion seems to assume that the image of g is open. However, Rudin does not mention this in his proof, nor does it seem to necessarily follow from his proof (we only show the existence of two open sets in $\mathbb{R}^{n+m}$ and one in $\mathbb{R}^m$, but I'm asking about the image of the latter as a set in $\mathbb{R}^n$). EDIT: "" Isn't this unsatisfactory? How can we use the conclusions of the implicit function theorem if the implicit function is neither a homeomorphism nor a diffeomorphism? "" --- I realize now that it can't be a diffeomorphism except for the case where $n=m$, because otherwise we would have a contradiction. Context: This is in some sense the sequel to a question I asked here . In that question I asked why we wanted the image of the continuously differentiable function $f$ to be open. The answer seemed to come down to having a sufficiently meaningful/useful definition of derivative apply to the inverse function so that we had in particular continuity of the inverse function and a homeomorphism. That having the image be open is necessary for this seems somewhat unclear, but appears to be justifiable using Brouwer's invariance of domain theorem. However, in Rudin's proof of the implicit function theorem, it does not seem like it is necessary that the image of the ""implicit function"" $g$ be open. Why don't we have the same problem for the implicit function theorem? Wouldn't we need the image to be open for the inverse image of some set under $g$ to be a manifold? (Assuming it is supposed to somehow be a manifold?)","We have a continuously differentiable function $f$ from $\mathbb{R}^{n+m}$ to $\mathbb{R}^n$, and we find a continuously differentiable function $g$ which maps points from $\mathbb{R}^m$ into $\mathbb{R}^n$ (this function defines $x \in \mathbb{R}^n$ ""implicitly"" in terms of $y \in \mathbb{R}^m $). What regularity properties must the image of $g$ have as a direct result of the proof of the implicit function theorem (if any) in order to allow meaningful differentiable geometry (e.g. manifolds) to ""happen""? In the proof of the Inverse (not implicit) function theorem, we need to care about the properties of the image of $f$. Yet for some reason the properties of the image of $g$ in the Implicit (not inverse) function theorem don't matter? Why is that? What properties, if any, of $g$ in the implicit function theorem, then, are important for the definitions of manifolds and tangent spaces and bundles? This question seems to address a related issue, but the discussion seems to assume that the image of g is open. However, Rudin does not mention this in his proof, nor does it seem to necessarily follow from his proof (we only show the existence of two open sets in $\mathbb{R}^{n+m}$ and one in $\mathbb{R}^m$, but I'm asking about the image of the latter as a set in $\mathbb{R}^n$). EDIT: "" Isn't this unsatisfactory? How can we use the conclusions of the implicit function theorem if the implicit function is neither a homeomorphism nor a diffeomorphism? "" --- I realize now that it can't be a diffeomorphism except for the case where $n=m$, because otherwise we would have a contradiction. Context: This is in some sense the sequel to a question I asked here . In that question I asked why we wanted the image of the continuously differentiable function $f$ to be open. The answer seemed to come down to having a sufficiently meaningful/useful definition of derivative apply to the inverse function so that we had in particular continuity of the inverse function and a homeomorphism. That having the image be open is necessary for this seems somewhat unclear, but appears to be justifiable using Brouwer's invariance of domain theorem. However, in Rudin's proof of the implicit function theorem, it does not seem like it is necessary that the image of the ""implicit function"" $g$ be open. Why don't we have the same problem for the implicit function theorem? Wouldn't we need the image to be open for the inverse image of some set under $g$ to be a manifold? (Assuming it is supposed to somehow be a manifold?)",,"['real-analysis', 'differential-geometry', 'manifolds', 'smooth-manifolds', 'implicit-function-theorem']"
27,Non-Theoretical Applications of Partitions of Unity,Non-Theoretical Applications of Partitions of Unity,,"I am studying partitions of unity in Munkres' $\textit{Analysis on Manifolds}$ book. Are partitions of unity just theoretical tools, i.e. used to prove theorems, or do people actually apply them concretely to compute integrals in practice?","I am studying partitions of unity in Munkres' $\textit{Analysis on Manifolds}$ book. Are partitions of unity just theoretical tools, i.e. used to prove theorems, or do people actually apply them concretely to compute integrals in practice?",,"['real-analysis', 'integration']"
28,Sequence of continuous function converging pointwise to Thomae's function,Sequence of continuous function converging pointwise to Thomae's function,,"Recall that Thomae's function (also called Popcorn function) $f\colon\mathbb{R}\to\mathbb{R}$ is defined as  $$ f(x) = \begin{cases} \frac{1}{q} & \text{ if } x=\frac{p}{q} \neq 0 \text{ is rational, } \gcd(p,q)=1 \text{ and } q> 0\\ 0 &\text{ otherwise.} \end{cases} $$ In particular, it is a standard exercise to show that $f$ is continuous at every irrational, and discontinuous at every rational. Find a sequence of continuous functions $(f_n)_{n\in\mathbb{N}}$ that converges pointwise to $f$ on $[0,1]$.","Recall that Thomae's function (also called Popcorn function) $f\colon\mathbb{R}\to\mathbb{R}$ is defined as  $$ f(x) = \begin{cases} \frac{1}{q} & \text{ if } x=\frac{p}{q} \neq 0 \text{ is rational, } \gcd(p,q)=1 \text{ and } q> 0\\ 0 &\text{ otherwise.} \end{cases} $$ In particular, it is a standard exercise to show that $f$ is continuous at every irrational, and discontinuous at every rational. Find a sequence of continuous functions $(f_n)_{n\in\mathbb{N}}$ that converges pointwise to $f$ on $[0,1]$.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'continuity']"
29,Jensen's inequality in measure theory,Jensen's inequality in measure theory,,"Here cites its original claim from http://www.math.tau.ac.il/~ostrover/Teaching/18125.pdf . Theorem 3.1 Jensen's Inequality Let $(X,\mathcal{M},\mu)$ be a probability space (a measure space with $\mu(X) = 1)$ , $f: X \to \mathbb R \in L^1(X, \mu)$ , and $\psi: \mathbb R \to \mathbb R$ be a convex function, then $$\psi\int_X f d\mu \le \int_X (\psi \circ f)d\mu$$ . Proof: Since $\psi$ is convex, at each $x_0 \in \mathbb R$ , there exist $a,b \in \mathbb R$ such that $\psi(x_0) = ax_0 + b$ and $\psi(x) \ge ax + b, \forall x \in \mathbb R$ , (here, $y = ax + b$ defines a supporting plane of the epigraph of $\psi$ at $x_0$ ). Let $x_0 = \int_X fdµ$ , then we have $$\psi(\int_Xf d\mu) = \psi(x_0) = ax_0+b=a\int_Xf\mu + b = \int(af+b)d\mu \le \int(\psi\circ f)d\mu$$ , q. e. d. Generally I have two questions. First is how could the $a, b$ be guaranteed to exist? The other is if the $\mu$ is not from probability space and just Lebesgue measure or even general measure, will this Jensen inequality still hold (I think the requirement is $\mu$ should be finite measure at least)?","Here cites its original claim from http://www.math.tau.ac.il/~ostrover/Teaching/18125.pdf . Theorem 3.1 Jensen's Inequality Let be a probability space (a measure space with , , and be a convex function, then . Proof: Since is convex, at each , there exist such that and , (here, defines a supporting plane of the epigraph of at ). Let , then we have , q. e. d. Generally I have two questions. First is how could the be guaranteed to exist? The other is if the is not from probability space and just Lebesgue measure or even general measure, will this Jensen inequality still hold (I think the requirement is should be finite measure at least)?","(X,\mathcal{M},\mu) \mu(X) = 1) f: X \to \mathbb R \in L^1(X, \mu) \psi: \mathbb R \to \mathbb R \psi\int_X f d\mu \le \int_X (\psi \circ f)d\mu \psi x_0 \in \mathbb R a,b \in \mathbb R \psi(x_0) = ax_0 + b \psi(x) \ge ax + b, \forall x \in \mathbb R y = ax + b \psi x_0 x_0 = \int_X fdµ \psi(\int_Xf d\mu) = \psi(x_0) = ax_0+b=a\int_Xf\mu + b = \int(af+b)d\mu \le \int(\psi\circ f)d\mu a, b \mu \mu","['real-analysis', 'measure-theory', 'convex-analysis']"
30,"Composition of a continuous function and a discontinuous function, can be continous.","Composition of a continuous function and a discontinuous function, can be continous.",,"Okay, I think I found an example of a continuous function $f$ composed with a discontinuous function $g$, that make a continuous function $h$. Okay let: $f:[0,1]\to [0,1)$ where $f(x)=\begin{cases}x \quad \textrm{if} \quad x\in[0,1)\\ 0 \quad \textrm{if} \quad x=1\end{cases}$ $g:[0,1)\to \mathbb{R^2}$ where $g(x)= (\cos(2\pi x),\sin(2\pi x))$ I am thinking the $h(x)=g(f(x))$ is continuous because, the only discontinuity that could occur is at $x=1$ which doesn't because $\lim\limits_{x\to 1}h(x)=(1,0)=h(1)$. But, I am sort of confused as to if my justification is correct or not.","Okay, I think I found an example of a continuous function $f$ composed with a discontinuous function $g$, that make a continuous function $h$. Okay let: $f:[0,1]\to [0,1)$ where $f(x)=\begin{cases}x \quad \textrm{if} \quad x\in[0,1)\\ 0 \quad \textrm{if} \quad x=1\end{cases}$ $g:[0,1)\to \mathbb{R^2}$ where $g(x)= (\cos(2\pi x),\sin(2\pi x))$ I am thinking the $h(x)=g(f(x))$ is continuous because, the only discontinuity that could occur is at $x=1$ which doesn't because $\lim\limits_{x\to 1}h(x)=(1,0)=h(1)$. But, I am sort of confused as to if my justification is correct or not.",,['real-analysis']
31,$\sigma$-algebra produced by a subclass of a class.,-algebra produced by a subclass of a class.,\sigma,"im studying the book 'probability & measure' by Patrick Billingsley. in chapter 2 there's an exercise 2.9 say's: show that: If $B\in\sigma(A)$, then there exists a countable subclass $A_B$ of $A$ such that $B\in\sigma(A_B)$.","im studying the book 'probability & measure' by Patrick Billingsley. in chapter 2 there's an exercise 2.9 say's: show that: If $B\in\sigma(A)$, then there exists a countable subclass $A_B$ of $A$ such that $B\in\sigma(A_B)$.",,"['real-analysis', 'probability-theory']"
32,Showing that a function is uniformly continuous but not Lipschitz,Showing that a function is uniformly continuous but not Lipschitz,,"If $g(x):= \sqrt x $ for $x \in [0,1]$, show that there does not exist a constant $K$ such that $|g(x)| \leq K|x|$ $ \forall x \in [0,1]$ Conclude that the uniformly continuous function $g$ is not a Lipschitz function on interval $[0,1]$. Necessary definitions: Let $A \subseteq \Bbb R$. A function $f: A \to \Bbb R$  is uniformly continuous when: Given $\epsilon > 0$ and $u \in A$ there is a $\delta(\epsilon, u) > 0$ such that $ \forall x \in A$ and $|x - u| < \delta(\epsilon,u)$ $\implies$ $|f(x) - f(u)| < \epsilon$ A function $f$ is considered Lipschitz if $ \exists$ a constant $K > 0$ such that $ \forall x,u \in A$ $|f(x) - f(u)| \leq K|x-u|$. Here is the beginning of my proof, I am having some difficulty showing that such a constant does not exist. Intuitively it makes sense however showing this geometrically evades me. Proof (attempt): Suppose $g(x): = \sqrt x$ for $x \ in [0,1]$ Assume $g(x)$ is Lipschitz. $g(x)$ Lipschitz $\implies$ $\exists$ constant $K > 0$ such that $|f(x) - f(u)| \leq K|x-u|$ $\forall x,u \in [0,1]$. Evaluating geometrically: $\frac{|f(x) - f(u)|}{ |x-u|}$ = $\frac{ \sqrt x - 1}{|x-u|}$ $ \leq K$ I was hoping to assume the function is Lipschitz and encounter a contradiction however this is where I'm stuck. Can anyone nudge me in the right direction?","If $g(x):= \sqrt x $ for $x \in [0,1]$, show that there does not exist a constant $K$ such that $|g(x)| \leq K|x|$ $ \forall x \in [0,1]$ Conclude that the uniformly continuous function $g$ is not a Lipschitz function on interval $[0,1]$. Necessary definitions: Let $A \subseteq \Bbb R$. A function $f: A \to \Bbb R$  is uniformly continuous when: Given $\epsilon > 0$ and $u \in A$ there is a $\delta(\epsilon, u) > 0$ such that $ \forall x \in A$ and $|x - u| < \delta(\epsilon,u)$ $\implies$ $|f(x) - f(u)| < \epsilon$ A function $f$ is considered Lipschitz if $ \exists$ a constant $K > 0$ such that $ \forall x,u \in A$ $|f(x) - f(u)| \leq K|x-u|$. Here is the beginning of my proof, I am having some difficulty showing that such a constant does not exist. Intuitively it makes sense however showing this geometrically evades me. Proof (attempt): Suppose $g(x): = \sqrt x$ for $x \ in [0,1]$ Assume $g(x)$ is Lipschitz. $g(x)$ Lipschitz $\implies$ $\exists$ constant $K > 0$ such that $|f(x) - f(u)| \leq K|x-u|$ $\forall x,u \in [0,1]$. Evaluating geometrically: $\frac{|f(x) - f(u)|}{ |x-u|}$ = $\frac{ \sqrt x - 1}{|x-u|}$ $ \leq K$ I was hoping to assume the function is Lipschitz and encounter a contradiction however this is where I'm stuck. Can anyone nudge me in the right direction?",,"['real-analysis', 'uniform-continuity', 'lipschitz-functions']"
33,"If $f\in C[0,1]$ and $A\subset[0,1]$ is finite, can $f$ be approximated uniformly by polynomials that coincide with $f$ on $A$?","If  and  is finite, can  be approximated uniformly by polynomials that coincide with  on ?","f\in C[0,1] A\subset[0,1] f f A","Let $f\colon[0,1]\to\mathbb{R}$ be continuous and $A$ a finite subset of $[0,1]$. Given $\epsilon>0$ is there a polynomial $p$ such that $$ |f(x)-p(x)|\le\epsilon\quad\forall x\in[0,1]\quad\text{and}\quad p(a)=f(a)\quad\forall a\in A? $$ This question arised while I was writing notes on Weierstrass's approximation theorem. By considering the interpolating polynomial of $f$ on $A$, we may asume without loss of generality that $f(a)=0$ for all $a\in A$. There are three situations in which I know the answer to be in the positive. $A$ is a singleton, say $A=\{a\}$. There is a polynomial $q$ such that $|f(x)-q(x)|\le\epsilon/2$ for all $x\in[0,1]$. Since $f(0)=0$ we have $|q(a)|\le\epsilon/2$. Take $p(x)=q(x)-q(a)$. $A=\{0,1\}$. There is a polynomial $q$ such that $|f(x)-q(x)|\le\epsilon/2$ for all $x\in[0,1]$. Then $|q(0)|,|q(1)|\le\epsilon/2$. Take $p(x)=q(x)-q(0)(1-x)-q(1)\,x$. $f$ is differentiable at all $a\in A$ (with he usual understanding that if $a=0$ or $1$ we mean one-sided differentiability.) Let $P_A=\prod_{a\in A}(x-a)$ and $$ g(x)=\begin{cases}f(x)/P_A(x) & \text{if }x\notin A,\\f'(x)\Bigl(\prod\limits_{a\in A,a\ne x}(x-a)\Bigr)^{-1} & \text{if }x\in A.\end{cases} $$ The $g$ is continuous on $[0,1]$. Let $M$ be a bound of $P_A$ on $[0,1]$. There exists a polynomial $q$ such that $|g(x)-q(x)|\le\epsilon/M$ for all $x\in[0,1]$. Let $p(x)=P_A(x)\,q(x)$. Do you know the answer for the general case?","Let $f\colon[0,1]\to\mathbb{R}$ be continuous and $A$ a finite subset of $[0,1]$. Given $\epsilon>0$ is there a polynomial $p$ such that $$ |f(x)-p(x)|\le\epsilon\quad\forall x\in[0,1]\quad\text{and}\quad p(a)=f(a)\quad\forall a\in A? $$ This question arised while I was writing notes on Weierstrass's approximation theorem. By considering the interpolating polynomial of $f$ on $A$, we may asume without loss of generality that $f(a)=0$ for all $a\in A$. There are three situations in which I know the answer to be in the positive. $A$ is a singleton, say $A=\{a\}$. There is a polynomial $q$ such that $|f(x)-q(x)|\le\epsilon/2$ for all $x\in[0,1]$. Since $f(0)=0$ we have $|q(a)|\le\epsilon/2$. Take $p(x)=q(x)-q(a)$. $A=\{0,1\}$. There is a polynomial $q$ such that $|f(x)-q(x)|\le\epsilon/2$ for all $x\in[0,1]$. Then $|q(0)|,|q(1)|\le\epsilon/2$. Take $p(x)=q(x)-q(0)(1-x)-q(1)\,x$. $f$ is differentiable at all $a\in A$ (with he usual understanding that if $a=0$ or $1$ we mean one-sided differentiability.) Let $P_A=\prod_{a\in A}(x-a)$ and $$ g(x)=\begin{cases}f(x)/P_A(x) & \text{if }x\notin A,\\f'(x)\Bigl(\prod\limits_{a\in A,a\ne x}(x-a)\Bigr)^{-1} & \text{if }x\in A.\end{cases} $$ The $g$ is continuous on $[0,1]$. Let $M$ be a bound of $P_A$ on $[0,1]$. There exists a polynomial $q$ such that $|g(x)-q(x)|\le\epsilon/M$ for all $x\in[0,1]$. Let $p(x)=P_A(x)\,q(x)$. Do you know the answer for the general case?",,"['real-analysis', 'approximation-theory']"
34,why is the limit as n goes to infinity of $(1+\frac{1}{n}+\frac{200}{n^2})^n = e$?,why is the limit as n goes to infinity of ?,(1+\frac{1}{n}+\frac{200}{n^2})^n = e,"I know that $$\lim_{n\to\infty}\left(1+\frac{1}n\right)^n = e .$$ But why does $$\lim_{n\to\infty}\left(1+\frac{1}n+\frac{a}{n^b}\right)^n = e ? \quad where\quad  b\gt1$$   better yet, how can I conclude something like:   $$\lim_{n\to\infty}\left(1+\frac{1}n+\sum_{k=2}^\infty \frac{700^k}{k!n^k}\right)^n = e $$   Why do all the terms in the sigma not contribute anything to limit? This is from a statistics course where we have to evaluate a similar expression but I have studied and done most of the exercises of the chapter on sequences and series of real numbers in Rudin's principles of math. analysis","I know that $$\lim_{n\to\infty}\left(1+\frac{1}n\right)^n = e .$$ But why does $$\lim_{n\to\infty}\left(1+\frac{1}n+\frac{a}{n^b}\right)^n = e ? \quad where\quad  b\gt1$$   better yet, how can I conclude something like:   $$\lim_{n\to\infty}\left(1+\frac{1}n+\sum_{k=2}^\infty \frac{700^k}{k!n^k}\right)^n = e $$   Why do all the terms in the sigma not contribute anything to limit? This is from a statistics course where we have to evaluate a similar expression but I have studied and done most of the exercises of the chapter on sequences and series of real numbers in Rudin's principles of math. analysis",,"['real-analysis', 'limits', 'exponential-function']"
35,Rudin's definition on measurable function,Rudin's definition on measurable function,,"In the definition of measurable function in Rudin's book, he defines measurable function from a measurable space $X$ to a topological space $Y$ as the inverse image of every open set in the range space is measurable in the domain space, the definition is equivalent to the common definition of measurable function when $Y$ is equipped with borel set ( the smallest $\sigma-$ algebra containing open sets). However, if the $\sigma-$ algebra containing open sets on $Y$ is bigger than Borel set, then this definition gives a broader range of measurable function, can anyone tell me why Rudin defines like that?","In the definition of measurable function in Rudin's book, he defines measurable function from a measurable space $X$ to a topological space $Y$ as the inverse image of every open set in the range space is measurable in the domain space, the definition is equivalent to the common definition of measurable function when $Y$ is equipped with borel set ( the smallest $\sigma-$ algebra containing open sets). However, if the $\sigma-$ algebra containing open sets on $Y$ is bigger than Borel set, then this definition gives a broader range of measurable function, can anyone tell me why Rudin defines like that?",,['real-analysis']
36,Big $\mathcal{O}$ notation,Big  notation,\mathcal{O},I'm learning about the big $\mathcal{O}$ notation and I'm a bit confused. Why is it that we can write things like $$\displaystyle x+\frac{x^3}{3}+\frac{x^5}{5}+\mathcal{O}(x^6)$$ when there's no $x^6$ term? Wouldn't it make sense to write $$\displaystyle x+\frac{x^3}{3}+\frac{x^5}{5}+\mathcal{O}(x^7)$$ instead? This is for the $\tanh^{-1}{x}$ series if it makes a difference.,I'm learning about the big $\mathcal{O}$ notation and I'm a bit confused. Why is it that we can write things like $$\displaystyle x+\frac{x^3}{3}+\frac{x^5}{5}+\mathcal{O}(x^6)$$ when there's no $x^6$ term? Wouldn't it make sense to write $$\displaystyle x+\frac{x^3}{3}+\frac{x^5}{5}+\mathcal{O}(x^7)$$ instead? This is for the $\tanh^{-1}{x}$ series if it makes a difference.,,"['real-analysis', 'asymptotics']"
37,I would like to calculate $\lim_ {n \to \infty} {\frac{n+\lfloor \sqrt{n} \rfloor^2}{n-\lfloor \sqrt{n} \rfloor}}$,I would like to calculate,\lim_ {n \to \infty} {\frac{n+\lfloor \sqrt{n} \rfloor^2}{n-\lfloor \sqrt{n} \rfloor}},"I would like to calculate the following limit: $$\lim_ {n \to \infty} {\frac{n+\lfloor \sqrt{n} \rfloor^2}{n-\lfloor \sqrt{n} \rfloor}}$$ where $\lfloor x \rfloor$ is floor of $x$ and $x ∈ R$. Now I know the result is $2$, but I am having trouble getting to it. Any ideas would be greatly appreciated.","I would like to calculate the following limit: $$\lim_ {n \to \infty} {\frac{n+\lfloor \sqrt{n} \rfloor^2}{n-\lfloor \sqrt{n} \rfloor}}$$ where $\lfloor x \rfloor$ is floor of $x$ and $x ∈ R$. Now I know the result is $2$, but I am having trouble getting to it. Any ideas would be greatly appreciated.",,"['calculus', 'real-analysis', 'limits', 'ceiling-and-floor-functions']"
38,"Let $f: \Bbb R \to [0,\infty)$ be a continuous function such that $g(x)=(f(x))^2$ is uniformly continuous. Which of the following is always true?",Let  be a continuous function such that  is uniformly continuous. Which of the following is always true?,"f: \Bbb R \to [0,\infty) g(x)=(f(x))^2","A. $f$ is bounded B. $f$ may not be uniformly continuous C. $f$ is uniformly continuous D. $f$ is unbounded. Let $f(x)=\sqrt x$. Then $g(x)=x$ is uniformly continuous and unbounded. Hence option A can not be true. Let $f(x)=1$. Then $g(x)=1$ is uniformly continuous and bounded. Hence the option D can not be true. How should I check uniform continuity? EDIT: Since $g$ is uniformly continuous, hence for a given $\epsilon \gt 0$, $\exists \delta \gt 0$ depending only upon $\epsilon$ such that $|g(x)-g(y)| \lt \epsilon$ whenever $|x-y| \lt \delta$  $\forall x,y$. $\Rightarrow |\sqrt g(x) -\sqrt g(y)||\sqrt g(x)+\sqrt g(y)|\lt \epsilon$ whenever $|x-y| \lt \delta$. $\Rightarrow |f(x)-f(y)||f(x)+f(y)|\lt \epsilon$ whenever $|x-y| \lt \delta$. What should be my next step now? How can I use the continuity of $f$?","A. $f$ is bounded B. $f$ may not be uniformly continuous C. $f$ is uniformly continuous D. $f$ is unbounded. Let $f(x)=\sqrt x$. Then $g(x)=x$ is uniformly continuous and unbounded. Hence option A can not be true. Let $f(x)=1$. Then $g(x)=1$ is uniformly continuous and bounded. Hence the option D can not be true. How should I check uniform continuity? EDIT: Since $g$ is uniformly continuous, hence for a given $\epsilon \gt 0$, $\exists \delta \gt 0$ depending only upon $\epsilon$ such that $|g(x)-g(y)| \lt \epsilon$ whenever $|x-y| \lt \delta$  $\forall x,y$. $\Rightarrow |\sqrt g(x) -\sqrt g(y)||\sqrt g(x)+\sqrt g(y)|\lt \epsilon$ whenever $|x-y| \lt \delta$. $\Rightarrow |f(x)-f(y)||f(x)+f(y)|\lt \epsilon$ whenever $|x-y| \lt \delta$. What should be my next step now? How can I use the continuity of $f$?",,"['real-analysis', 'continuity', 'uniform-continuity']"
39,Functional equation $f(xy)=f(x)+f(y)$ and continuity,Functional equation  and continuity,f(xy)=f(x)+f(y),"Prove that if $f:(0,\infty)→\mathbb{R}$ satisfying $f(xy)=f(x)+f(y)$, and if $f$ is continuous at $x=1$, then $f$ is continuous for $x>0$. I let $x=1$ and I find that $f(x)=f(x)+f(1)$ which implies that $f(1)=0$. So, $\lim_{x\to1}f(x)=0$, but how can I use this to prove continuity of $f$ for every $x \in \mathbb R$? Any help would appreciated. Thanks","Prove that if $f:(0,\infty)→\mathbb{R}$ satisfying $f(xy)=f(x)+f(y)$, and if $f$ is continuous at $x=1$, then $f$ is continuous for $x>0$. I let $x=1$ and I find that $f(x)=f(x)+f(1)$ which implies that $f(1)=0$. So, $\lim_{x\to1}f(x)=0$, but how can I use this to prove continuity of $f$ for every $x \in \mathbb R$? Any help would appreciated. Thanks",,"['calculus', 'real-analysis', 'functional-equations']"
40,Proving a function is not Riemann integrable,Proving a function is not Riemann integrable,,"Prove that the bounded function $f$ defined by $f(x)=0$ if $x$ is irrational and $f(x)=1$ if $x$ is rational is not Riemann integrable on $[0,1]$. I was given the hint to use the inverse definition of Riemann integrable and consider the cases of the partition being all rationals, and all irrationals between $[0,1]$, but I'm not too sure how to go about it.","Prove that the bounded function $f$ defined by $f(x)=0$ if $x$ is irrational and $f(x)=1$ if $x$ is rational is not Riemann integrable on $[0,1]$. I was given the hint to use the inverse definition of Riemann integrable and consider the cases of the partition being all rationals, and all irrationals between $[0,1]$, but I'm not too sure how to go about it.",,['real-analysis']
41,"Is there a function, continuous on the irrationals, with rational values, nowhere locally constant?","Is there a function, continuous on the irrationals, with rational values, nowhere locally constant?",,"Question. Let $\mathbb A=\mathbb R\!\smallsetminus\!\mathbb Q$ be the irrational numbers. Is there a continuous function $\,f:\mathbb A\to\mathbb Q$, which is nowhere locally constant? – i.e., for every $a<b$, the restriction $f\,\big|_{\,(a,b)\cap\mathbb A}$ is not constant. Clearly, there exist many non-constant such functions, which are continuous, but fail to be nowhere locally constant . One failed attempt: Let $\mathbb Q=\{q_n\}_{n\in\mathbb N}$, and set  $$ f(x)=\sum_{n\in\mathbb N} a_n\mathrm{sgn}(x-q_n),\tag{1} $$ where $\{a_n\}_{n\in\mathbb N}\subset\mathbb Q^+$, with $\sum_{n\in\mathbb N}a_n<\infty$. Clearly $f$ is continuous in $\mathbb A$, and it does not take necessarily only rational values. EDIT. If there exists a sequence $\{a_n\}_{n\in\mathbb N}\subset\mathbb Q^+$, such that $\sum_{n\in S}a_n\in\mathbb Q$, for every $S\subset\mathbb N$, then the function $f$, defined by $(1)$ possesses the sought for properties.","Question. Let $\mathbb A=\mathbb R\!\smallsetminus\!\mathbb Q$ be the irrational numbers. Is there a continuous function $\,f:\mathbb A\to\mathbb Q$, which is nowhere locally constant? – i.e., for every $a<b$, the restriction $f\,\big|_{\,(a,b)\cap\mathbb A}$ is not constant. Clearly, there exist many non-constant such functions, which are continuous, but fail to be nowhere locally constant . One failed attempt: Let $\mathbb Q=\{q_n\}_{n\in\mathbb N}$, and set  $$ f(x)=\sum_{n\in\mathbb N} a_n\mathrm{sgn}(x-q_n),\tag{1} $$ where $\{a_n\}_{n\in\mathbb N}\subset\mathbb Q^+$, with $\sum_{n\in\mathbb N}a_n<\infty$. Clearly $f$ is continuous in $\mathbb A$, and it does not take necessarily only rational values. EDIT. If there exists a sequence $\{a_n\}_{n\in\mathbb N}\subset\mathbb Q^+$, such that $\sum_{n\in S}a_n\in\mathbb Q$, for every $S\subset\mathbb N$, then the function $f$, defined by $(1)$ possesses the sought for properties.",,"['real-analysis', 'continuity', 'irrational-numbers', 'rational-numbers', 'baire-category']"
42,If $A$ is closed and $B$ is compact in $\mathbb R^n$ then $A+B=\{a+b : a \in A \text{ and } b \in B\}$ is closed.,If  is closed and  is compact in  then  is closed.,A B \mathbb R^n A+B=\{a+b : a \in A \text{ and } b \in B\},"If $A$ is closed and $B$ is compact in $\mathbb R^n$ then $A+B=\{a+b : a \in A \text{ and } b \in B\}$ is closed. (In other words, the vector/Minkowski sum of a closed set and a compact set is closed.) What I've tried so far:  Let $ c_n $ be a sequence in $A+B$; $c_n =a_n+b_n$ where $a_n \in A$ and $b_n \in B$. Since $B$ is compact, there exists a subsequence $(b_{n_k})$ which converges $b$ which is in $B$. Now I'm stuck in how to show that the subsequence  $(a_{n_k})$ converges to some number in $A$ so that $(c_{n_k})$ converges to the sum of two limits in $A+B$.","If $A$ is closed and $B$ is compact in $\mathbb R^n$ then $A+B=\{a+b : a \in A \text{ and } b \in B\}$ is closed. (In other words, the vector/Minkowski sum of a closed set and a compact set is closed.) What I've tried so far:  Let $ c_n $ be a sequence in $A+B$; $c_n =a_n+b_n$ where $a_n \in A$ and $b_n \in B$. Since $B$ is compact, there exists a subsequence $(b_{n_k})$ which converges $b$ which is in $B$. Now I'm stuck in how to show that the subsequence  $(a_{n_k})$ converges to some number in $A$ so that $(c_{n_k})$ converges to the sum of two limits in $A+B$.",,"['real-analysis', 'compactness', 'sumset']"
43,"a question of theorem 3.13 in Real and Complex Analysis, Rudin","a question of theorem 3.13 in Real and Complex Analysis, Rudin",,"Here is the Theorem 3.13 in rudin's real and complex analysis book， Theorem 3.13 Let $S$ be the class of all complex,measureable,simple   functions on $X$ such that \begin{equation} \mu(\{x:s(x)\neq  0\})<\infty  \tag{1} \end{equation} If $1\leq p<\infty$,then $S$ is   dense in $L^{p}(\mu)$. Proof : First,it is clear that $S\subset L^{p}(\mu)$.Suppose $f\geq 0,f\in L^{p}(\mu)$,and let $\{s_{n}\}$ be   as in Theorem 1.17.Since $0\leq s_{n}\le f$, we have $s_{n}\in  L^{p}(\mu)$, hence $s_{n}\in S$,Since $|f-s_{n}|^{p}\leq f^{p}$,the    dominated convergence theorem show that $\|f-s_n\|_{p}\to 0$ as $n\to  \infty$.Thus $f$ is in the $L^{p}$-closure of $S$.The general case ($f$    complex) follows from this. Here I have a question: Why we have $s_{n}\in L^{p}(\mu)$? in the proof of rudin,he claim that $s_{n}$ is defined as Theorem 1.17.but in Theorem 1.17,he first put $\delta_{n}=2^{-n}$.then to each positive integer $n$ and each real number $t$ corresponds a unique integer $k=k_{n}(t)$ that satisfie $k\delta_n\leq t<(k+1)\delta_{n}$.Define $$\varphi_{n}(t)=\left\{   \begin{array}{ll}     k_{n}(t)\delta_{n}, & \hbox{if $0\leq t<n$;} \\     n, & \hbox{if $n\leq t\leq \infty$.}   \end{array} \right.$$ then define $s_{n}=\varphi\circ f$. but I can't see this definitition can show that $s_{n}\in S$.in fact we need to show that \begin{equation} \mu(\{x:s_{n}(x)\neq 0\})<\infty   \end{equation} and I can't figure it out.Can anyone who have read this book help me? Thank you in advance.","Here is the Theorem 3.13 in rudin's real and complex analysis book， Theorem 3.13 Let $S$ be the class of all complex,measureable,simple   functions on $X$ such that \begin{equation} \mu(\{x:s(x)\neq  0\})<\infty  \tag{1} \end{equation} If $1\leq p<\infty$,then $S$ is   dense in $L^{p}(\mu)$. Proof : First,it is clear that $S\subset L^{p}(\mu)$.Suppose $f\geq 0,f\in L^{p}(\mu)$,and let $\{s_{n}\}$ be   as in Theorem 1.17.Since $0\leq s_{n}\le f$, we have $s_{n}\in  L^{p}(\mu)$, hence $s_{n}\in S$,Since $|f-s_{n}|^{p}\leq f^{p}$,the    dominated convergence theorem show that $\|f-s_n\|_{p}\to 0$ as $n\to  \infty$.Thus $f$ is in the $L^{p}$-closure of $S$.The general case ($f$    complex) follows from this. Here I have a question: Why we have $s_{n}\in L^{p}(\mu)$? in the proof of rudin,he claim that $s_{n}$ is defined as Theorem 1.17.but in Theorem 1.17,he first put $\delta_{n}=2^{-n}$.then to each positive integer $n$ and each real number $t$ corresponds a unique integer $k=k_{n}(t)$ that satisfie $k\delta_n\leq t<(k+1)\delta_{n}$.Define $$\varphi_{n}(t)=\left\{   \begin{array}{ll}     k_{n}(t)\delta_{n}, & \hbox{if $0\leq t<n$;} \\     n, & \hbox{if $n\leq t\leq \infty$.}   \end{array} \right.$$ then define $s_{n}=\varphi\circ f$. but I can't see this definitition can show that $s_{n}\in S$.in fact we need to show that \begin{equation} \mu(\{x:s_{n}(x)\neq 0\})<\infty   \end{equation} and I can't figure it out.Can anyone who have read this book help me? Thank you in advance.",,['real-analysis']
44,"prove that if $\lim \limits_{n \to \infty}F( a_n)=\ell$, then $\lim \limits_{x \to \infty}F( x)=\ell$","prove that if , then",\lim \limits_{n \to \infty}F( a_n)=\ell \lim \limits_{x \to \infty}F( x)=\ell,"Let $a_n$ be a strictly increasing sequence of positive real numbers ($a_n>0$) and $\lim \limits_{n \to \infty} a_n=\infty.$ Suppose that the sequence $(a_{n+1}-a_n)_n$ is bounded. Let $F:\mathbb{R}^+\rightarrow \mathbb R$ be a differentiable function. Ssuppose further that $\lim \limits_{x \to \infty}F'(x)=0$ and $\lim \limits_{n \to \infty}F( a_n)=\ell$. (Note that $\ell$ is a real number). Prove that: $$\lim \limits_{x \to \infty}F( x)=\ell$$ I'm stuck, I've tried to use the MVT but it gets me nowhere! can anyone help me solve this problem?","Let $a_n$ be a strictly increasing sequence of positive real numbers ($a_n>0$) and $\lim \limits_{n \to \infty} a_n=\infty.$ Suppose that the sequence $(a_{n+1}-a_n)_n$ is bounded. Let $F:\mathbb{R}^+\rightarrow \mathbb R$ be a differentiable function. Ssuppose further that $\lim \limits_{x \to \infty}F'(x)=0$ and $\lim \limits_{n \to \infty}F( a_n)=\ell$. (Note that $\ell$ is a real number). Prove that: $$\lim \limits_{x \to \infty}F( x)=\ell$$ I'm stuck, I've tried to use the MVT but it gets me nowhere! can anyone help me solve this problem?",,"['calculus', 'real-analysis', 'algebra-precalculus', 'limits']"
45,Why is this function measurable?,Why is this function measurable?,,"If $f$ is Borel measurable on $\mathbb{R}$, why is then $f(x-y)$ Bore-measurable on $\mathbb{R}^2$? I tried showing that the set $\{(x,y): x-y \in f^{-1}(a,\infty)\}$, is measurable, we know that $f^{-1}(a,\infty)$ is in $\mathcal{B}$, but I am not quite sure how to finish the argument. Any tips?","If $f$ is Borel measurable on $\mathbb{R}$, why is then $f(x-y)$ Bore-measurable on $\mathbb{R}^2$? I tried showing that the set $\{(x,y): x-y \in f^{-1}(a,\infty)\}$, is measurable, we know that $f^{-1}(a,\infty)$ is in $\mathcal{B}$, but I am not quite sure how to finish the argument. Any tips?",,"['real-analysis', 'measure-theory']"
46,Path From Positive Dedekind Cuts to Reals?,Path From Positive Dedekind Cuts to Reals?,,"Don't spend a lot of time on this. I'm certain I could bang it out myself; but maybe there's an answer out there that someone already knows. Say we use Dedekind cuts to construct the reals. Addition is fine and the proof that a set bounded above has a supremum is just lovely. Then we get to defining multiplication and all hell breaks loose - not that there's anything deep or really difficult about it, but at the very least the elegance is lost in a mass of special  cases. So I say to myself this is just because of the historical accident that people figured out how to extend the positive rationals to the rationals before considering completeness; why not  get completeness first? So the plan is to use Dedekind cuts of positive rationals to construct the positive reals, and then go from there to the reals. (In all of this ""positive"" may mean strictly positive or non-negative, whatever works.) This leads to at least one question : For what values of whatever and the construction can one prove the following theorem? Theorem If $F$ is an ordered field then the set of positive elements is a whatever . Conversely, given a whatever $P$, the construction gives an ordered field $F$ such that $P$ is (isomorphic to) the set of positive elements of $F$. Or if there is no such general theorem we could be more specific. Say $P$ is the set of positive reals. I can think of at least two constructions that do in fact lead from $P$ to $\Bbb R$; the question is what do I need to prove about $P$ in order to prove that the result is in fact $\Bbb R$. One construction would be to say that a real is just an element of $P$ with a plus or minus sign attached. This seems likely to lead to the sort of special-casing that we're trying to avoid (already when we think about $0$ we see we need to add a special clause to the effect that $+0=-0$; blech). Another construction is to regard $(a,b)\in P^2$ as representing the real $a-b$. So we'd define $(a,b)\sim(c,d)$ if $a+d=c+b$, we'd define the sum and product of elements of $P^2$ in the obvious way (in particular $(a,b)(c,d)=(ac+bd,ad+bc)$), show these operations lift to the quotient $P^2/\sim$ and be on our way. I kind of like the second construction. What do I need to prove about $P$ to show that $P^2/\sim$ is an ordered field with positive cone $P$?","Don't spend a lot of time on this. I'm certain I could bang it out myself; but maybe there's an answer out there that someone already knows. Say we use Dedekind cuts to construct the reals. Addition is fine and the proof that a set bounded above has a supremum is just lovely. Then we get to defining multiplication and all hell breaks loose - not that there's anything deep or really difficult about it, but at the very least the elegance is lost in a mass of special  cases. So I say to myself this is just because of the historical accident that people figured out how to extend the positive rationals to the rationals before considering completeness; why not  get completeness first? So the plan is to use Dedekind cuts of positive rationals to construct the positive reals, and then go from there to the reals. (In all of this ""positive"" may mean strictly positive or non-negative, whatever works.) This leads to at least one question : For what values of whatever and the construction can one prove the following theorem? Theorem If $F$ is an ordered field then the set of positive elements is a whatever . Conversely, given a whatever $P$, the construction gives an ordered field $F$ such that $P$ is (isomorphic to) the set of positive elements of $F$. Or if there is no such general theorem we could be more specific. Say $P$ is the set of positive reals. I can think of at least two constructions that do in fact lead from $P$ to $\Bbb R$; the question is what do I need to prove about $P$ in order to prove that the result is in fact $\Bbb R$. One construction would be to say that a real is just an element of $P$ with a plus or minus sign attached. This seems likely to lead to the sort of special-casing that we're trying to avoid (already when we think about $0$ we see we need to add a special clause to the effect that $+0=-0$; blech). Another construction is to regard $(a,b)\in P^2$ as representing the real $a-b$. So we'd define $(a,b)\sim(c,d)$ if $a+d=c+b$, we'd define the sum and product of elements of $P^2$ in the obvious way (in particular $(a,b)(c,d)=(ac+bd,ad+bc)$), show these operations lift to the quotient $P^2/\sim$ and be on our way. I kind of like the second construction. What do I need to prove about $P$ to show that $P^2/\sim$ is an ordered field with positive cone $P$?",,"['real-analysis', 'abstract-algebra', 'real-numbers']"
47,Limit of $n(n^{1\over n}-1)$,Limit of,n(n^{1\over n}-1),How can you prove that $a_n=n(n^{1\over n}-1)$ diverges to $\infty $ without L'Hopital's rule. My guess would be by comparison but I can't find any sequence diverging to $\infty$ that's smaller than $a_n$ Any suggestions?,How can you prove that $a_n=n(n^{1\over n}-1)$ diverges to $\infty $ without L'Hopital's rule. My guess would be by comparison but I can't find any sequence diverging to $\infty$ that's smaller than $a_n$ Any suggestions?,,"['real-analysis', 'sequences-and-series']"
48,Double integral with a product of dilog $\int _0^1\int _0^1\text{Li}_2(x y) \text{Li}_2((1-y) x)\ dx \ dy$,Double integral with a product of dilog,\int _0^1\int _0^1\text{Li}_2(x y) \text{Li}_2((1-y) x)\ dx \ dy,"One of the integrals I came across these days (during my studies) is $$\int _0^1\int _0^1\text{Li}_2(x y) \text{Li}_2((1-y) x) \ dx \ dy$$  that can be turned into a series, or can be approached by using the integration by parts, but these ways do not look like as a promising way to go, or I might be wrong. I would like to know  your vision  on these integrals, not asking for full solutions, just feel comfortable to share ideas that is the thing I'm most interested in. I'm looking forward to your ideas! And one more thing, Mathematica shows that $$\int \text{Li}_2(x y) \text{Li}_2((1-y) x) \, dx$$ $$=\frac{\log \left(\frac{2 y-1}{(x (y-1)+1) y}\right) \log ^2\left(\frac{1-x y}{x (y-1)+1}\right)}{2 (y-1) y}+\frac{\log (x-x y) \log ^2\left(\frac{1-x y}{x (y-1)+1}\right)}{2 (y-1) y}-\frac{\log \left(\frac{x (2 y-1)}{x (y-1)+1}\right) \log ^2\left(\frac{1-x y}{x (y-1)+1}\right)}{2 (y-1) y}+\frac{\log (x y) \log (1-x y) \log \left(\frac{1-x y}{x (y-1)+1}\right)}{(y-1) y}+\frac{\text{Li}_2\left(\frac{(y-1) (x y-1)}{(x (y-1)+1) y}\right) \log \left(\frac{1-x y}{x (y-1)+1}\right)}{(y-1) y}+\frac{\text{Li}_2(1-x y) \log \left(\frac{1-x y}{x (y-1)+1}\right)}{(y-1) y}-\frac{\log (1-x y) \log (x-x y) \log \left(\frac{1-x y}{x (y-1)+1}\right)}{(y-1) y}-\frac{\text{Li}_2\left(\frac{1-x y}{x (y-1)+1}\right) \log \left(\frac{1-x y}{x (y-1)+1}\right)}{(y-1) y}-\frac{\text{Li}_2(x (y-1)+1) \log \left(\frac{1-x y}{x (y-1)+1}\right)}{(y-1) y}+\frac{6 x y}{y-1}+\frac{3 x \log (x (y-1)+1)}{y-1}+\frac{2 \log (x (y-1)+1) \log \left(-\frac{(y-1) (x y-1)}{2 y-1}\right)}{(y-1) y}+\frac{2 x y \log (x (y-1)+1) \log (1-x y)}{y-1}+\frac{\log (x (y-1)+1) \log (x y) \log (1-x y)}{(y-1) y}+\frac{2 \log \left(\frac{(x (y-1)+1) y}{2 y-1}\right) \log (1-x y)}{y-1}+\frac{3 x \log (1-x y)}{y-1}+\frac{3 \log (1-x y)}{y-1}+\frac{\log ^2(1-x y) \log (x-x y)}{2 (y-1) y}+\frac{\log (1-x y) \text{Li}_2(x (y-1)+1)}{(y-1) y}+\frac{x y \log (x (y-1)+1) \text{Li}_2(x y)}{y-1}+\frac{\log (x (y-1)+1) \text{Li}_2(x y)}{y-1}+\frac{x \text{Li}_2(x y)}{y-1}+\frac{2 \text{Li}_2\left(\frac{(x (y-1)+1) y}{2 y-1}\right)}{(y-1) y}+\frac{2 \text{Li}_2\left(\frac{(y-1) (1-x y)}{2 y-1}\right)}{y-1}+\frac{\log (x (y-1)+1) \text{Li}_2(1-x y)}{(y-1) y}+\frac{x y \log (1-x y) \text{Li}_2(x-x y)}{y-1}+\frac{\log (1-x y) \text{Li}_2(x-x y)}{(y-1) y}+\frac{x y \text{Li}_2(x y) \text{Li}_2(x-x y)}{y-1}+\frac{x \text{Li}_2(x-x y)}{y-1}+\frac{\text{Li}_3\left(\frac{1-x y}{x (y-1)+1}\right)}{(y-1) y}+\frac{6 x}{y-1}-\frac{3 x y \log (x (y-1)+1)}{y-1}-\frac{3 \log (x (y-1)+1)}{y-1}-\frac{2 \log (x (y-1)+1) \log \left(-\frac{(y-1) (x y-1)}{2 y-1}\right)}{y-1}-\frac{3 x y \log (1-x y)}{y-1}-\frac{2 x \log (x (y-1)+1) \log (1-x y)}{y-1}-\frac{x y \text{Li}_2(x y)}{y-1}-\frac{x \log (x (y-1)+1) \text{Li}_2(x y)}{y-1}-\frac{2 \text{Li}_2\left(\frac{(x (y-1)+1) y}{2 y-1}\right)}{y-1}-\frac{x y \text{Li}_2(x-x y)}{y-1}-\frac{x \log (1-x y) \text{Li}_2(x-x y)}{y-1}-\frac{\log (1-x y) \text{Li}_2(x-x y)}{y-1}-\frac{x \text{Li}_2(x y) \text{Li}_2(x-x y)}{y-1}-\frac{3 \log (1-x y)}{(y-1) y}-\frac{\text{Li}_3(x (y-1)+1)}{(y-1) y}-\frac{\text{Li}_3\left(\frac{(y-1) (x y-1)}{(x (y-1)+1) y}\right)}{(y-1) y}-\frac{\text{Li}_3(1-x y)}{(y-1) y}+\frac{2}{(y-1) y}-\frac{\log (x y) \log ^2(1-x y)}{2 (y-1) y}.$$","One of the integrals I came across these days (during my studies) is $$\int _0^1\int _0^1\text{Li}_2(x y) \text{Li}_2((1-y) x) \ dx \ dy$$  that can be turned into a series, or can be approached by using the integration by parts, but these ways do not look like as a promising way to go, or I might be wrong. I would like to know  your vision  on these integrals, not asking for full solutions, just feel comfortable to share ideas that is the thing I'm most interested in. I'm looking forward to your ideas! And one more thing, Mathematica shows that $$\int \text{Li}_2(x y) \text{Li}_2((1-y) x) \, dx$$ $$=\frac{\log \left(\frac{2 y-1}{(x (y-1)+1) y}\right) \log ^2\left(\frac{1-x y}{x (y-1)+1}\right)}{2 (y-1) y}+\frac{\log (x-x y) \log ^2\left(\frac{1-x y}{x (y-1)+1}\right)}{2 (y-1) y}-\frac{\log \left(\frac{x (2 y-1)}{x (y-1)+1}\right) \log ^2\left(\frac{1-x y}{x (y-1)+1}\right)}{2 (y-1) y}+\frac{\log (x y) \log (1-x y) \log \left(\frac{1-x y}{x (y-1)+1}\right)}{(y-1) y}+\frac{\text{Li}_2\left(\frac{(y-1) (x y-1)}{(x (y-1)+1) y}\right) \log \left(\frac{1-x y}{x (y-1)+1}\right)}{(y-1) y}+\frac{\text{Li}_2(1-x y) \log \left(\frac{1-x y}{x (y-1)+1}\right)}{(y-1) y}-\frac{\log (1-x y) \log (x-x y) \log \left(\frac{1-x y}{x (y-1)+1}\right)}{(y-1) y}-\frac{\text{Li}_2\left(\frac{1-x y}{x (y-1)+1}\right) \log \left(\frac{1-x y}{x (y-1)+1}\right)}{(y-1) y}-\frac{\text{Li}_2(x (y-1)+1) \log \left(\frac{1-x y}{x (y-1)+1}\right)}{(y-1) y}+\frac{6 x y}{y-1}+\frac{3 x \log (x (y-1)+1)}{y-1}+\frac{2 \log (x (y-1)+1) \log \left(-\frac{(y-1) (x y-1)}{2 y-1}\right)}{(y-1) y}+\frac{2 x y \log (x (y-1)+1) \log (1-x y)}{y-1}+\frac{\log (x (y-1)+1) \log (x y) \log (1-x y)}{(y-1) y}+\frac{2 \log \left(\frac{(x (y-1)+1) y}{2 y-1}\right) \log (1-x y)}{y-1}+\frac{3 x \log (1-x y)}{y-1}+\frac{3 \log (1-x y)}{y-1}+\frac{\log ^2(1-x y) \log (x-x y)}{2 (y-1) y}+\frac{\log (1-x y) \text{Li}_2(x (y-1)+1)}{(y-1) y}+\frac{x y \log (x (y-1)+1) \text{Li}_2(x y)}{y-1}+\frac{\log (x (y-1)+1) \text{Li}_2(x y)}{y-1}+\frac{x \text{Li}_2(x y)}{y-1}+\frac{2 \text{Li}_2\left(\frac{(x (y-1)+1) y}{2 y-1}\right)}{(y-1) y}+\frac{2 \text{Li}_2\left(\frac{(y-1) (1-x y)}{2 y-1}\right)}{y-1}+\frac{\log (x (y-1)+1) \text{Li}_2(1-x y)}{(y-1) y}+\frac{x y \log (1-x y) \text{Li}_2(x-x y)}{y-1}+\frac{\log (1-x y) \text{Li}_2(x-x y)}{(y-1) y}+\frac{x y \text{Li}_2(x y) \text{Li}_2(x-x y)}{y-1}+\frac{x \text{Li}_2(x-x y)}{y-1}+\frac{\text{Li}_3\left(\frac{1-x y}{x (y-1)+1}\right)}{(y-1) y}+\frac{6 x}{y-1}-\frac{3 x y \log (x (y-1)+1)}{y-1}-\frac{3 \log (x (y-1)+1)}{y-1}-\frac{2 \log (x (y-1)+1) \log \left(-\frac{(y-1) (x y-1)}{2 y-1}\right)}{y-1}-\frac{3 x y \log (1-x y)}{y-1}-\frac{2 x \log (x (y-1)+1) \log (1-x y)}{y-1}-\frac{x y \text{Li}_2(x y)}{y-1}-\frac{x \log (x (y-1)+1) \text{Li}_2(x y)}{y-1}-\frac{2 \text{Li}_2\left(\frac{(x (y-1)+1) y}{2 y-1}\right)}{y-1}-\frac{x y \text{Li}_2(x-x y)}{y-1}-\frac{x \log (1-x y) \text{Li}_2(x-x y)}{y-1}-\frac{\log (1-x y) \text{Li}_2(x-x y)}{y-1}-\frac{x \text{Li}_2(x y) \text{Li}_2(x-x y)}{y-1}-\frac{3 \log (1-x y)}{(y-1) y}-\frac{\text{Li}_3(x (y-1)+1)}{(y-1) y}-\frac{\text{Li}_3\left(\frac{(y-1) (x y-1)}{(x (y-1)+1) y}\right)}{(y-1) y}-\frac{\text{Li}_3(1-x y)}{(y-1) y}+\frac{2}{(y-1) y}-\frac{\log (x y) \log ^2(1-x y)}{2 (y-1) y}.$$",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'polylogarithm']"
49,How do I evaluate this limit :$\displaystyle \lim_{x\to \infty} (1+\cos x)^\frac{1}{\cos x}$?,How do I evaluate this limit :?,\displaystyle \lim_{x\to \infty} (1+\cos x)^\frac{1}{\cos x},I would like to know if this :$$ \lim_{x\to \infty} (1+\cos x)^\frac{1}{\cos x}$$ does exist and how do i evaluate it ?. Note : I have tried to use the standard limit : $$ \lim_{z\to \infty} \left(1+\frac{1}{z}\right)^z=e$$ using $\cos x=1/z $ but i can't  succeed Thank you for any help .,I would like to know if this :$$ \lim_{x\to \infty} (1+\cos x)^\frac{1}{\cos x}$$ does exist and how do i evaluate it ?. Note : I have tried to use the standard limit : $$ \lim_{z\to \infty} \left(1+\frac{1}{z}\right)^z=e$$ using $\cos x=1/z $ but i can't  succeed Thank you for any help .,,"['real-analysis', 'limits', 'convergence-divergence']"
50,$\lim\limits_{n\to\infty} \sup \sqrt[n]{n^k}$ for $k \in \mathbb{N}$?,for ?,\lim\limits_{n\to\infty} \sup \sqrt[n]{n^k} k \in \mathbb{N},"What is $$\lim\limits_{n\to\infty} \sup \sqrt[n]{n^k} \ \ \ \text{where } k \in \mathbb{N}\text ?$$ I've seen the proof that $\lim\limits_{n\to\infty} \sqrt[n]n = 1$. I believe the answer is $1$ for all $k$ because $\sqrt[n]n$ approaches $1$, so a sequence that approaches $1$ raised to an exponent, I believe, should also approach $1$. I don't know how to prove this, mainly because of the $\lim\limits_{n\to\infty}\sup$ concept. I learned $\lim\limits_{n\to\infty}\sup s_n$ as the $\sup$ of all subsequential limits of $s_n$. Intuitively I know it's the $\inf$ of all $\sup$s of the sequence as $n\to\infty$. But I don't know how to use that definition /intuition to solve problems like this.","What is $$\lim\limits_{n\to\infty} \sup \sqrt[n]{n^k} \ \ \ \text{where } k \in \mathbb{N}\text ?$$ I've seen the proof that $\lim\limits_{n\to\infty} \sqrt[n]n = 1$. I believe the answer is $1$ for all $k$ because $\sqrt[n]n$ approaches $1$, so a sequence that approaches $1$ raised to an exponent, I believe, should also approach $1$. I don't know how to prove this, mainly because of the $\lim\limits_{n\to\infty}\sup$ concept. I learned $\lim\limits_{n\to\infty}\sup s_n$ as the $\sup$ of all subsequential limits of $s_n$. Intuitively I know it's the $\inf$ of all $\sup$s of the sequence as $n\to\infty$. But I don't know how to use that definition /intuition to solve problems like this.",,['real-analysis']
51,$ \lim_{n\to+\infty} \frac{1\times 3\times \ldots \times (2n+1)}{2\times 4\times \ldots\times 2n}\times\frac{1}{\sqrt{n}}$,, \lim_{n\to+\infty} \frac{1\times 3\times \ldots \times (2n+1)}{2\times 4\times \ldots\times 2n}\times\frac{1}{\sqrt{n}},"Knowing that :    $$I_n=\int_0^{\frac{\pi}{2}}\cos^n(t) \, dt$$ $$I_{2n}=\frac{1\times 3\times \ldots \times (2n-1)}{2\times 4\times \ldots\times 2n}\times\dfrac{\pi}{2}\quad \forall n\geq 1$$   $$I_{n}\sim \sqrt{\dfrac{\pi}{2n}}$$ Calculate:    $$ \lim_{n\to+\infty} \frac{1\times 3\times \ldots \times (2n+1)}{2\times 4\times \ldots\times 2n}\times\dfrac{1}{\sqrt{n}}$$ Indeed, $$I_{2n}=\frac{1\times 3\times \ldots \times (2n-1)}{2\times 4\times \ldots\times 2n}\times\dfrac{\pi}{2}\quad \forall n\geq 1 \\ \frac{1\times 3\times \ldots \times (2n-1)}{2\times 4\times \ldots\times 2n}=\dfrac{2}{\pi}\times I_{2n}$$ then $$ \begin{align*} \frac{1\times 3\times \cdots \times (2n+1)}{2\times 4\times \cdots\times 2n}\times\dfrac{1}{\sqrt{n}}&=\dfrac{2}{\pi}\times I_{2n}\times (2n+1)\times\dfrac{1}{\sqrt{n}}\\ &=\dfrac{2}{\pi}\times (2n+1)\times\dfrac{1}{\sqrt{n}} \times \sqrt{ \dfrac{2n\times I^{2}_{2n}}{2n}  } \\ &=\dfrac{2}{\pi}\times (2n+1)\times\dfrac{1}{ \sqrt{2}\times n} \times \sqrt{ 2n\times I^{2}_{2n}   } \\ \end{align*} $$ or $$(2n)I^{2}_{2n}\sim \dfrac{\pi}{2}$$ then \begin{align*} \frac{1\times 3\times \cdots \times (2n+1)}{2\times 4\times \cdots\times 2n}\times\dfrac{1}{\sqrt{n}}&=\dfrac{2}{\pi}\times (2n+1)\times\dfrac{1}{ \sqrt{2}\times n} \times \sqrt{ 2n\times I^{2}_{2n}   } \\ &\sim\dfrac{2}{\pi}\times (2n+1)\times\dfrac{1}{ \sqrt{2}\times n} \times  \sqrt{ \dfrac{\pi}{2}   } \\ \end{align*} $$ I'm stuck here i think that i can go ahead \begin{align*} \frac{1\times 3\times \cdots \times (2n+1)}{2\times 4\times \cdots\times 2n}\times\dfrac{1}{\sqrt{n}}&\sim \dfrac{2}{\pi}\times (2n+1)\times\dfrac{1}{ \sqrt{2}\times n} \times  \sqrt{ \dfrac{\pi}{2}   } \\ &\sim  \dfrac{2}{\pi}\times\dfrac{2n}{ \sqrt{2}\times n} \times  \sqrt{ \dfrac{\pi}{2}   } \\ &\sim \frac{ 2\sqrt{\pi} }{\pi}=\dfrac{2}{\sqrt{\pi}} \end{align*} am i right ? if that so is there any other way","Knowing that :    $$I_n=\int_0^{\frac{\pi}{2}}\cos^n(t) \, dt$$ $$I_{2n}=\frac{1\times 3\times \ldots \times (2n-1)}{2\times 4\times \ldots\times 2n}\times\dfrac{\pi}{2}\quad \forall n\geq 1$$   $$I_{n}\sim \sqrt{\dfrac{\pi}{2n}}$$ Calculate:    $$ \lim_{n\to+\infty} \frac{1\times 3\times \ldots \times (2n+1)}{2\times 4\times \ldots\times 2n}\times\dfrac{1}{\sqrt{n}}$$ Indeed, $$I_{2n}=\frac{1\times 3\times \ldots \times (2n-1)}{2\times 4\times \ldots\times 2n}\times\dfrac{\pi}{2}\quad \forall n\geq 1 \\ \frac{1\times 3\times \ldots \times (2n-1)}{2\times 4\times \ldots\times 2n}=\dfrac{2}{\pi}\times I_{2n}$$ then $$ \begin{align*} \frac{1\times 3\times \cdots \times (2n+1)}{2\times 4\times \cdots\times 2n}\times\dfrac{1}{\sqrt{n}}&=\dfrac{2}{\pi}\times I_{2n}\times (2n+1)\times\dfrac{1}{\sqrt{n}}\\ &=\dfrac{2}{\pi}\times (2n+1)\times\dfrac{1}{\sqrt{n}} \times \sqrt{ \dfrac{2n\times I^{2}_{2n}}{2n}  } \\ &=\dfrac{2}{\pi}\times (2n+1)\times\dfrac{1}{ \sqrt{2}\times n} \times \sqrt{ 2n\times I^{2}_{2n}   } \\ \end{align*} $$ or $$(2n)I^{2}_{2n}\sim \dfrac{\pi}{2}$$ then \begin{align*} \frac{1\times 3\times \cdots \times (2n+1)}{2\times 4\times \cdots\times 2n}\times\dfrac{1}{\sqrt{n}}&=\dfrac{2}{\pi}\times (2n+1)\times\dfrac{1}{ \sqrt{2}\times n} \times \sqrt{ 2n\times I^{2}_{2n}   } \\ &\sim\dfrac{2}{\pi}\times (2n+1)\times\dfrac{1}{ \sqrt{2}\times n} \times  \sqrt{ \dfrac{\pi}{2}   } \\ \end{align*} $$ I'm stuck here i think that i can go ahead \begin{align*} \frac{1\times 3\times \cdots \times (2n+1)}{2\times 4\times \cdots\times 2n}\times\dfrac{1}{\sqrt{n}}&\sim \dfrac{2}{\pi}\times (2n+1)\times\dfrac{1}{ \sqrt{2}\times n} \times  \sqrt{ \dfrac{\pi}{2}   } \\ &\sim  \dfrac{2}{\pi}\times\dfrac{2n}{ \sqrt{2}\times n} \times  \sqrt{ \dfrac{\pi}{2}   } \\ &\sim \frac{ 2\sqrt{\pi} }{\pi}=\dfrac{2}{\sqrt{\pi}} \end{align*} am i right ? if that so is there any other way",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'contest-math']"
52,Theorema egregium violated in dimension $n \ge 4$?,Theorema egregium violated in dimension ?,n \ge 4,"Gauß showed that for surfaces in $\mathbb{R}^3$ the Gaussian curvature ( = sectional curvature) is invariant under local isometries. This is known as the thema egregium. Now in another question ( click me ) I defined i.e. $3$ types of spaces: 1.) Locally symmetric spaces. They were Riemannian manifolds with the property that $\nabla R=0$ everywhere. 2.) Symmetric spaces. They were path-conn. Riemannian manifolds such that there is for each $p \in M$ a global isometry $f_p: M \rightarrow M$ such that $f(\gamma(t)) = f(\gamma(-t))$ for all geodesics $\gamma: (-\varepsilon,\varepsilon) \rightarrow M$ satisfying $\gamma(0)=p$ and $Df_p(p) = -id.$ 3.) Homogenous spaces. They were spaces admitting for every $p,q\in M$ a global isometry $\phi_{p,q}: M \rightarrow M$ such that $\phi(p)=q$ and as it turned out, although all of them have some kind of isometries, they don't have to have constant curvature at all ( as I was told in the answer I got or rather in the comments below the answer in the previously linked thread.) Now, I want to understand whether this means that the Theorema egregium is wrong in dimension $n \ge 4$ or whether I am missing something here? I mean especially the condition we have for homogenous spaces looks like something that should preserve sectional curvature, but it does apparently not. Homogenous spaces admit global isometries between any two points. Why doesn't this mean that the curvature tensor is the same for these two points and is therefore constant on the homogenous space? If anything is unclear, please let me know.","Gauß showed that for surfaces in the Gaussian curvature ( = sectional curvature) is invariant under local isometries. This is known as the thema egregium. Now in another question ( click me ) I defined i.e. types of spaces: 1.) Locally symmetric spaces. They were Riemannian manifolds with the property that everywhere. 2.) Symmetric spaces. They were path-conn. Riemannian manifolds such that there is for each a global isometry such that for all geodesics satisfying and 3.) Homogenous spaces. They were spaces admitting for every a global isometry such that and as it turned out, although all of them have some kind of isometries, they don't have to have constant curvature at all ( as I was told in the answer I got or rather in the comments below the answer in the previously linked thread.) Now, I want to understand whether this means that the Theorema egregium is wrong in dimension or whether I am missing something here? I mean especially the condition we have for homogenous spaces looks like something that should preserve sectional curvature, but it does apparently not. Homogenous spaces admit global isometries between any two points. Why doesn't this mean that the curvature tensor is the same for these two points and is therefore constant on the homogenous space? If anything is unclear, please let me know.","\mathbb{R}^3 3 \nabla R=0 p \in M f_p: M \rightarrow M f(\gamma(t)) = f(\gamma(-t)) \gamma: (-\varepsilon,\varepsilon) \rightarrow M \gamma(0)=p Df_p(p) = -id. p,q\in M \phi_{p,q}: M \rightarrow M \phi(p)=q n \ge 4","['real-analysis', 'differential-geometry', 'manifolds', 'differential-topology', 'riemannian-geometry']"
53,A difficult integral about function $\ln x$ and $\ln\ln x$,A difficult integral about function  and,\ln x \ln\ln x,"Some days ago, I met a difficult integral $$\int_0^1 {\left( {1 + \ln x} \right)\ln \left( {1 + x} \right)\ln \ln \frac{1}{x} \,{\rm{d}} x} .$$ I considered $$\int_0^1 {{x^n}\ln \ln \frac{1}{x}\,{\rm{d}} x}  = \int_0^\infty  {{e^{ - \left( {n + 1} \right)x}}\ln x\,{\rm{d}} x}  =  - \frac{{\gamma  + \ln \left( {n + 1} \right)}}{{n + 1}}.$$ But I have no idea to continue it! Could you show me how to compute it?","Some days ago, I met a difficult integral $$\int_0^1 {\left( {1 + \ln x} \right)\ln \left( {1 + x} \right)\ln \ln \frac{1}{x} \,{\rm{d}} x} .$$ I considered $$\int_0^1 {{x^n}\ln \ln \frac{1}{x}\,{\rm{d}} x}  = \int_0^\infty  {{e^{ - \left( {n + 1} \right)x}}\ln x\,{\rm{d}} x}  =  - \frac{{\gamma  + \ln \left( {n + 1} \right)}}{{n + 1}}.$$ But I have no idea to continue it! Could you show me how to compute it?",,"['calculus', 'real-analysis', 'integration', 'analysis']"
54,Motivation of Lebesgue differentiation theorem,Motivation of Lebesgue differentiation theorem,,"Fundamental theorem of calculus states that the derivative of the integral is the original function, meaning that $$ f(x)=\frac{d}{dx}\int_{a}^{x}f(y)dy.\tag{*} $$ To motivate the statement of the Lebesgue differentiation theorem, observe that (*) may be written in terms of symmetric differences as $$ f(x)=\lim_{r\to 0^+}\frac{1}{2r}\int_{x-r}^{x+r}f(y)dy.\tag{**} $$ An $n$-dimensional version of (**) is $$ f(x)=\lim_{r\to 0^+}\frac{1}{|B(x,r)|}\int_{B(x,r)}f(y)dy.\tag{***} $$ where the integral is with respect $n$-dimensional Lebesgue measure. The Lebesgue differentiation theorem states that (***) holds pointwise $\mu$-a.e. for any locally integrable function $f$. My question is how could we write (**) by using (*) ? If we define $F(x)=\int_{a}^{x}f(y)dy$. The quotient  $$ \frac{F(x+r)-F(x)}{r}=\frac{\int_{a}^{x+r}f(y)dy-\int_{a}^{x}f(y)dy}{r}=\frac{1}{r}\int_{x}^{x+r}f(y)dy $$ How could we say that  $$\frac{1}{r}\int_{x}^{x+r}f(y)dy\overset{?}{=}\frac{1}{2r}\int_{x-r}^{x+r}f(y)dy$$","Fundamental theorem of calculus states that the derivative of the integral is the original function, meaning that $$ f(x)=\frac{d}{dx}\int_{a}^{x}f(y)dy.\tag{*} $$ To motivate the statement of the Lebesgue differentiation theorem, observe that (*) may be written in terms of symmetric differences as $$ f(x)=\lim_{r\to 0^+}\frac{1}{2r}\int_{x-r}^{x+r}f(y)dy.\tag{**} $$ An $n$-dimensional version of (**) is $$ f(x)=\lim_{r\to 0^+}\frac{1}{|B(x,r)|}\int_{B(x,r)}f(y)dy.\tag{***} $$ where the integral is with respect $n$-dimensional Lebesgue measure. The Lebesgue differentiation theorem states that (***) holds pointwise $\mu$-a.e. for any locally integrable function $f$. My question is how could we write (**) by using (*) ? If we define $F(x)=\int_{a}^{x}f(y)dy$. The quotient  $$ \frac{F(x+r)-F(x)}{r}=\frac{\int_{a}^{x+r}f(y)dy-\int_{a}^{x}f(y)dy}{r}=\frac{1}{r}\int_{x}^{x+r}f(y)dy $$ How could we say that  $$\frac{1}{r}\int_{x}^{x+r}f(y)dy\overset{?}{=}\frac{1}{2r}\int_{x-r}^{x+r}f(y)dy$$",,"['calculus', 'real-analysis', 'analysis']"
55,Can a function be continuous at the end points of its (closed interval) domain?,Can a function be continuous at the end points of its (closed interval) domain?,,"Assume $f$ has a domain of $[a, b]$. Is it possible that $f$ is continuous at $x = a$ and $x = b$? If the definition of continuity is that the left and right limits are equal to the function at the given point, then this fails at $a$ since the left limit is undefined, and fails at $b$ since the right limit is undefined there. On the other hand there are other questions on this site that imply that it is possible for a function to be continuous on a closed interval, so perhaps this is simply an incorrect definition of continuity that I have seen in some high school text books.","Assume $f$ has a domain of $[a, b]$. Is it possible that $f$ is continuous at $x = a$ and $x = b$? If the definition of continuity is that the left and right limits are equal to the function at the given point, then this fails at $a$ since the left limit is undefined, and fails at $b$ since the right limit is undefined there. On the other hand there are other questions on this site that imply that it is possible for a function to be continuous on a closed interval, so perhaps this is simply an incorrect definition of continuity that I have seen in some high school text books.",,"['real-analysis', 'continuity']"
56,For which $x\in \mathbb{R}$ does $\sum_{n=1}^\infty \left(\frac{x^{2n}}{n} - \frac{n^{2x}}{x}\right)$ converge?,For which  does  converge?,x\in \mathbb{R} \sum_{n=1}^\infty \left(\frac{x^{2n}}{n} - \frac{n^{2x}}{x}\right),"I have to study for which values of $x \in \mathbb{R}$ the following series converges: $$\sum_{n=1}^\infty \left(\frac{x^{2n}}{n} - \frac{n^{2x}}{x}\right)$$ I was only able to say that the necessary condition for the convergence of the series, $\left(\frac{x^{2n}}{n} - \frac{n^{2x}}{x}\right) \to 0$, is satisfied iff $-1<x<0$, but then I'm stuck. How would you complete the problem?","I have to study for which values of $x \in \mathbb{R}$ the following series converges: $$\sum_{n=1}^\infty \left(\frac{x^{2n}}{n} - \frac{n^{2x}}{x}\right)$$ I was only able to say that the necessary condition for the convergence of the series, $\left(\frac{x^{2n}}{n} - \frac{n^{2x}}{x}\right) \to 0$, is satisfied iff $-1<x<0$, but then I'm stuck. How would you complete the problem?",,"['calculus', 'real-analysis', 'sequences-and-series']"
57,What is the largest function whose integral still converges?,What is the largest function whose integral still converges?,,"Let C be the set of all functions $f(x)$ whose integral converges, i.e. for some constant $x_0$: $$\int_{x_0}^\infty f(x) dx < \infty$$ While playing with integrals in Wolfram Alpha , I noticed the following pattern: $$\frac{1}{x} \notin C \hspace{2cm} \frac{1}{x^2} \in C$$ $$\frac{1}{x \cdot \ln x} \notin C \hspace{2cm} \frac{1}{x \cdot \ln^2 x} \in C$$ $$\frac{1}{x \cdot \ln x \cdot \ln \ln x} \notin C \hspace{2cm} \frac{1}{x \cdot \ln x \cdot \ln^2 \ln x} \in C$$ Note that the functions at the left become asymptotically smaller, the functions on the right become asymptotically larger, and the gap between them becomes ""narrower"". This raises the following question: what is the ""largest function"" in $C$ and what is the ""smallest function"" not in $C$? Formally: Is there a function $f_{max}\in C$ such that, for every other function $g\in C$: $$\lim_{x\to\infty}{g(x) \over f_{max}(x)}=0$$ Is there a function $f_{min}\notin C$ such that, for every other function $g\notin C$: $$\lim_{x\to\infty}{f_{min}(x) \over g(x)}=0$$","Let C be the set of all functions $f(x)$ whose integral converges, i.e. for some constant $x_0$: $$\int_{x_0}^\infty f(x) dx < \infty$$ While playing with integrals in Wolfram Alpha , I noticed the following pattern: $$\frac{1}{x} \notin C \hspace{2cm} \frac{1}{x^2} \in C$$ $$\frac{1}{x \cdot \ln x} \notin C \hspace{2cm} \frac{1}{x \cdot \ln^2 x} \in C$$ $$\frac{1}{x \cdot \ln x \cdot \ln \ln x} \notin C \hspace{2cm} \frac{1}{x \cdot \ln x \cdot \ln^2 \ln x} \in C$$ Note that the functions at the left become asymptotically smaller, the functions on the right become asymptotically larger, and the gap between them becomes ""narrower"". This raises the following question: what is the ""largest function"" in $C$ and what is the ""smallest function"" not in $C$? Formally: Is there a function $f_{max}\in C$ such that, for every other function $g\in C$: $$\lim_{x\to\infty}{g(x) \over f_{max}(x)}=0$$ Is there a function $f_{min}\notin C$ such that, for every other function $g\notin C$: $$\lim_{x\to\infty}{f_{min}(x) \over g(x)}=0$$",,"['real-analysis', 'integration', 'functional-analysis', 'limits', 'improper-integrals']"
58,If $\lim_{n\to \infty}a_n = a\in \mathbb{R}$ . Prove that $\limsup_{n\to \infty}a_n x_n=a\limsup_{n\to \infty}x_n$ .,If  . Prove that  .,\lim_{n\to \infty}a_n = a\in \mathbb{R} \limsup_{n\to \infty}a_n x_n=a\limsup_{n\to \infty}x_n,"Note: $x_n$ is a sequence which is not necessarily convergent. The following was my attempt. Since $\lim_{n\to \infty}a_n=a$ then $\limsup_{n\to \infty}a_n=a$ . Also $\sup(a_nx_n)=\sup(a_n)\sup(x_n)$. Therefore, $\limsup_{n\to \infty}a_nx_n=\limsup_{n\to \infty}a_n\limsup_{n\to \infty}x_n=a\limsup_{n\to \infty}x_n$. I am not sure if this is correct. So could someone please show me how it is done. Thanks","Note: $x_n$ is a sequence which is not necessarily convergent. The following was my attempt. Since $\lim_{n\to \infty}a_n=a$ then $\limsup_{n\to \infty}a_n=a$ . Also $\sup(a_nx_n)=\sup(a_n)\sup(x_n)$. Therefore, $\limsup_{n\to \infty}a_nx_n=\limsup_{n\to \infty}a_n\limsup_{n\to \infty}x_n=a\limsup_{n\to \infty}x_n$. I am not sure if this is correct. So could someone please show me how it is done. Thanks",,"['calculus', 'real-analysis', 'sequences-and-series', 'limsup-and-liminf']"
59,Evan's Proof to Converse of Mean Value Property.,Evan's Proof to Converse of Mean Value Property.,,"The theorem state: If $u \in C^2(U)$ satisfies $$ u(x) = \frac{1}{|\partial B(x,r)|}\int_{\partial B(x,r)} u(y) dS(y)$$ for each ball $B(x,r) \in U$, then u is harmonic. The issue that I have is with the proof of the theorem. He asserts to show it by contradiction, to assume that $\Delta u > 0$. Then for $$\phi(r) =  \frac{1}{|\partial B(x,r)|}\int_{\partial B(x,r)} u(y) dS(y)$$ $$ 0 = \phi ' (r) = \frac{r}{n}\frac{1}{|B(x,r)|} \int_{B(x,r)} \Delta u(y) dy >0$$ a contradiction. My issue is that I'm pretty sure that we show $\phi ' (r) = 0$ in the opposite direction by using the fact that $u$ is harmonic in the first place, so I don't see how we can use that fact here, especially when we're assuming the opposite. I feel like there is something very sly going on here. Can someone explain the proof?","The theorem state: If $u \in C^2(U)$ satisfies $$ u(x) = \frac{1}{|\partial B(x,r)|}\int_{\partial B(x,r)} u(y) dS(y)$$ for each ball $B(x,r) \in U$, then u is harmonic. The issue that I have is with the proof of the theorem. He asserts to show it by contradiction, to assume that $\Delta u > 0$. Then for $$\phi(r) =  \frac{1}{|\partial B(x,r)|}\int_{\partial B(x,r)} u(y) dS(y)$$ $$ 0 = \phi ' (r) = \frac{r}{n}\frac{1}{|B(x,r)|} \int_{B(x,r)} \Delta u(y) dy >0$$ a contradiction. My issue is that I'm pretty sure that we show $\phi ' (r) = 0$ in the opposite direction by using the fact that $u$ is harmonic in the first place, so I don't see how we can use that fact here, especially when we're assuming the opposite. I feel like there is something very sly going on here. Can someone explain the proof?",,"['real-analysis', 'partial-differential-equations']"
60,Proving the usual distance metric in $\mathbb{R}$ is complete,Proving the usual distance metric in  is complete,\mathbb{R},"If we allow the metric to be $d(x,y)=|x-y|$, we must prove that this is complete. Now, I have proven all properties of a metric space. However, I don't particularly now where to begin to prove that this is complete. I understand that we need every Cauchy sequence in this space to converge inside the space itself. Now, I know the definition of Cauchy convergence, but I don't particularly know how to even begin by showing that this is complete. I have much harder metrics to prove but I would like to try to understand this simpler space before I move onto more difficult ones. Since we know that the output of this distance will always be some constant in $\mathbb{R}$, and since we are working with $\mathbb{R}$, then it must always converge to some real number, and thus its complete?","If we allow the metric to be $d(x,y)=|x-y|$, we must prove that this is complete. Now, I have proven all properties of a metric space. However, I don't particularly now where to begin to prove that this is complete. I understand that we need every Cauchy sequence in this space to converge inside the space itself. Now, I know the definition of Cauchy convergence, but I don't particularly know how to even begin by showing that this is complete. I have much harder metrics to prove but I would like to try to understand this simpler space before I move onto more difficult ones. Since we know that the output of this distance will always be some constant in $\mathbb{R}$, and since we are working with $\mathbb{R}$, then it must always converge to some real number, and thus its complete?",,"['real-analysis', 'metric-spaces', 'cauchy-sequences']"
61,Selecting the Real Analysis Textbooks,Selecting the Real Analysis Textbooks,,"I am a sophomore in the US with double majors in mathematics and microbiology. I am interested in self-studying real analysis since it will help me with my current research in computational microbiology, prepare for upcoming math research (starting this Fall) on analytic number theory, and prepare for the real analysis course I will take this Fall and Putnam competition. I just finished Calculus with Analytic Geometry by G. Simmons, How to Prove It by Daniel Velleman, and How to Solve It by G. Polya. I also read some portions of Apostol's Calculus Vol. I to get a deeper view on calculus theories. (I was originally planning to read Apostol's Calculus Vol. I and Spivak's Calculus first, but I think it would be a better idea to start with real analysis since it covers all the ideas in those ""advanced calculus"" textbooks and much more.) My current plan is to start with one ""dumbed-down"" real analysis textbook and one ""comprehensive, detailed, and intermediate"" textbook, and advance into Rudin's Principles of Mathematical Analysis (required textbook for my real analysis course) starting this Summer, and use it in accordance with other real analysis textbooks. Could you help me on selecting one book from each category? Elementary Real Analysis textbooks: Elementary Analysis: The Theory of Calculus (Kenneth Ross) Understanding Analysis (Steven Abbott) The Way of Analysis (Robert Strichartz) Real Mathematical Analysis (Charles Pugh) Intermediate, detailed Real Analysis textbooks: Mathematical Analysis (Tom Apostol) Undergraduate Analysis (Serge Lang) Introduction to Real Analysis (Bartle, Sherbert) Elements of Real Analysis (Bartle, Sherbert) Mathematical Analysis I (Vladimir Zorich)","I am a sophomore in the US with double majors in mathematics and microbiology. I am interested in self-studying real analysis since it will help me with my current research in computational microbiology, prepare for upcoming math research (starting this Fall) on analytic number theory, and prepare for the real analysis course I will take this Fall and Putnam competition. I just finished Calculus with Analytic Geometry by G. Simmons, How to Prove It by Daniel Velleman, and How to Solve It by G. Polya. I also read some portions of Apostol's Calculus Vol. I to get a deeper view on calculus theories. (I was originally planning to read Apostol's Calculus Vol. I and Spivak's Calculus first, but I think it would be a better idea to start with real analysis since it covers all the ideas in those ""advanced calculus"" textbooks and much more.) My current plan is to start with one ""dumbed-down"" real analysis textbook and one ""comprehensive, detailed, and intermediate"" textbook, and advance into Rudin's Principles of Mathematical Analysis (required textbook for my real analysis course) starting this Summer, and use it in accordance with other real analysis textbooks. Could you help me on selecting one book from each category? Elementary Real Analysis textbooks: Elementary Analysis: The Theory of Calculus (Kenneth Ross) Understanding Analysis (Steven Abbott) The Way of Analysis (Robert Strichartz) Real Mathematical Analysis (Charles Pugh) Intermediate, detailed Real Analysis textbooks: Mathematical Analysis (Tom Apostol) Undergraduate Analysis (Serge Lang) Introduction to Real Analysis (Bartle, Sherbert) Elements of Real Analysis (Bartle, Sherbert) Mathematical Analysis I (Vladimir Zorich)",,['real-analysis']
62,Example of disjoint union of sets which does not have additive measure,Example of disjoint union of sets which does not have additive measure,,"I had a question about the additivity property of the outer measure. Can someone provide an example of a disjoint union of sets which doesn't have an outer measure equal to the sum of the outer measure of each set (in $\mathbb{R}^n$)? That is: $m_*(E_1\bigcup E_2)\neq m_*(E_1) + m_*(E_2)$, where $E_1\bigcap E_2=\emptyset$ and $d(E_1,E_2)$ possibly equal to zero. The theorem states this holds in general if $d(E_1,E_2)> 0$, but I can't find a counterexample where it doesn't if $d(E_1,E_2)=0$. Thanks!","I had a question about the additivity property of the outer measure. Can someone provide an example of a disjoint union of sets which doesn't have an outer measure equal to the sum of the outer measure of each set (in $\mathbb{R}^n$)? That is: $m_*(E_1\bigcup E_2)\neq m_*(E_1) + m_*(E_2)$, where $E_1\bigcap E_2=\emptyset$ and $d(E_1,E_2)$ possibly equal to zero. The theorem states this holds in general if $d(E_1,E_2)> 0$, but I can't find a counterexample where it doesn't if $d(E_1,E_2)=0$. Thanks!",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
63,"Bound on $f(0)^2$ by integrals of $f^2$ and $(f')^2$ on $[0,1]$.",Bound on  by integrals of  and  on .,"f(0)^2 f^2 (f')^2 [0,1]","Let $f$ be a function which is $C^1((0,1))\cap C([0,1])$. I would like to be able to show $$  \frac{1}{2}f(0)^2 \leq \int_0^1 f(x)^2dx + \int_0^1f'(x)^2dx $$ where we are assuming that $f$ is a real-valued function. We have attempted to use Young's inequality to reduce to $\frac{d}{dx}[f(x)^2]$, but this does not work. Thanks in advance for any ideas! edit: Counterexamples are welcome, obviously, but we believe this at the moment.","Let $f$ be a function which is $C^1((0,1))\cap C([0,1])$. I would like to be able to show $$  \frac{1}{2}f(0)^2 \leq \int_0^1 f(x)^2dx + \int_0^1f'(x)^2dx $$ where we are assuming that $f$ is a real-valued function. We have attempted to use Young's inequality to reduce to $\frac{d}{dx}[f(x)^2]$, but this does not work. Thanks in advance for any ideas! edit: Counterexamples are welcome, obviously, but we believe this at the moment.",,"['calculus', 'real-analysis', 'integral-inequality']"
64,Riesz Representation Theorem for $\ell^p$,Riesz Representation Theorem for,\ell^p,"Let $ 1 \leq p < \infty$, with $q$ the conjugate of $p$, and let $T \in \ell^{p*}$. Then for some sequence $g \in \ell^q,$ $T(f)=\sum_{\mathbb{N}} fg$ for all $f \in \ell^p$. I am trying to prove this from elementary principles, so no referring to counting measures and deriving the result as an appendage of more general versions of the Riesz Representation Theorem. I do, however, have the RRT for $L^p$ at my disposal, but I can't make heads or tails of trying to extend an arbitrary functional on $\ell^p$ to one on $L^p$. My approach is, then, to force the theorem by defining $g=\sum T(e_n)$ where $e_n$ is the sequence (in $L^p$) with a $1$ in the $n$th place and zero otherwise. Then if $f=\sum a_k e_n$, $$T(f)=\sum a_k T(e_n)$$ by linearity, and this is bounded as $T$ is bounded. Hence for all $f \in \ell^p$,    $$\sum f(k)T(e_n) \in \ell^1$$ Here's where I hit the snag. How can I conclude $T(e_n) \in \ell^q$, without begging the question and using the RRT? I've attempted writing some function in $l^p$ from it, something like $T(e_n)^{(q-1)}*\frac{1}{n}$, and proving this part by contradiction, but I can't get anything to work. Thanks in advance for any advice!","Let $ 1 \leq p < \infty$, with $q$ the conjugate of $p$, and let $T \in \ell^{p*}$. Then for some sequence $g \in \ell^q,$ $T(f)=\sum_{\mathbb{N}} fg$ for all $f \in \ell^p$. I am trying to prove this from elementary principles, so no referring to counting measures and deriving the result as an appendage of more general versions of the Riesz Representation Theorem. I do, however, have the RRT for $L^p$ at my disposal, but I can't make heads or tails of trying to extend an arbitrary functional on $\ell^p$ to one on $L^p$. My approach is, then, to force the theorem by defining $g=\sum T(e_n)$ where $e_n$ is the sequence (in $L^p$) with a $1$ in the $n$th place and zero otherwise. Then if $f=\sum a_k e_n$, $$T(f)=\sum a_k T(e_n)$$ by linearity, and this is bounded as $T$ is bounded. Hence for all $f \in \ell^p$,    $$\sum f(k)T(e_n) \in \ell^1$$ Here's where I hit the snag. How can I conclude $T(e_n) \in \ell^q$, without begging the question and using the RRT? I've attempted writing some function in $l^p$ from it, something like $T(e_n)^{(q-1)}*\frac{1}{n}$, and proving this part by contradiction, but I can't get anything to work. Thanks in advance for any advice!",,"['real-analysis', 'functional-analysis', 'lp-spaces', 'riesz-representation-theorem']"
65,Computing $\sum_{n=1}^{\infty} \frac{\psi\left(\frac{n+1}{2}\right)}{ \binom{2n}{n}}$,Computing,\sum_{n=1}^{\infty} \frac{\psi\left(\frac{n+1}{2}\right)}{ \binom{2n}{n}},"Here is an interesting series I played with, namely $$\sum_{n=1}^{\infty} \frac{\displaystyle\psi\left(\frac{n+1}{2}\right)}{\displaystyle \binom{2n}{n}} \approx -0.245969181104090562617616399148$$ where $\psi(x)$ is digamma function and the closed form I got you may see here , but that was possible because I had some luck. However, the closed form doesn't look that friendly and maybe we can improve that. It's also possible that a different approach leads to a shorter closed form, just the intuition. I'd be interested in an approach that only uses series manipulations, I couldn't do that.","Here is an interesting series I played with, namely $$\sum_{n=1}^{\infty} \frac{\displaystyle\psi\left(\frac{n+1}{2}\right)}{\displaystyle \binom{2n}{n}} \approx -0.245969181104090562617616399148$$ where $\psi(x)$ is digamma function and the closed form I got you may see here , but that was possible because I had some luck. However, the closed form doesn't look that friendly and maybe we can improve that. It's also possible that a different approach leads to a shorter closed form, just the intuition. I'd be interested in an approach that only uses series manipulations, I couldn't do that.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
66,How to find a conserved quantity in this differential equation.,How to find a conserved quantity in this differential equation.,,Consider the system: $$\ddot x = x^3 -x$$ What is the method to follow to find a conserved quantity for this system? So far what I have is: $\dot x = y$ and $\dot y = x^3 - x$ and I can find the Jacobian of the system and find and classify fixed points. How would I find a conserved quantity?,Consider the system: $$\ddot x = x^3 -x$$ What is the method to follow to find a conserved quantity for this system? So far what I have is: $\dot x = y$ and $\dot y = x^3 - x$ and I can find the Jacobian of the system and find and classify fixed points. How would I find a conserved quantity?,,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'dynamical-systems']"
67,Find a smooth function with prescribed moments,Find a smooth function with prescribed moments,,"In several contexts I’ve encountered variants of the following problem : let $m_0,m_1,m_2$ be real numbers such that $0 < m_1 < m_0$ and $\frac{m_1^2}{m_0} <m_2 < m_1$. Then, show that there is a nonnegative smooth function $f : [0,1] \to {\mathbb R}^{+}$ such that $$ \int_{0}^{1} f(x)dx=m_0, \  \int_{0}^{1} xf(x)dx=m_1, \ \int_{0}^{1} x^2f(x)dx=m_2. $$ The easiest version is when ""smooth"" means just ""piecewise continuous"" ; in that case the only proof I know uses staged functions with complicated parameters. Is there a proof that avoids such tedious computations ? I guess that one can always find a ${\cal C}^{\infty}$ solution $f$, and even an analytic one, but I have no clear idea on how to proceed. My (very vague) thoughts : Bump functions ? Integration by parts, transforming the problem into an easier interpolation problem ? EDIT (09/24/2014) : In answer to Han de Bruijn’s comment, if one works on a general $[a,b]$ instead of $[0,1]$, so that $m_k=\int_a^b x^k f(x)dx$, then the inequalities become $$ am_0 < m_1 < bm_0, \ \frac{m_1^2}{m_0} < m_2 < (a+b)m_1-abm_0 $$ Note that the inequation $\frac{m_1^2}{m_0} < m_2$ (which remarkably contains no $a$ or $b$) comes from the Cauchy-Schwarz inequality.","In several contexts I’ve encountered variants of the following problem : let $m_0,m_1,m_2$ be real numbers such that $0 < m_1 < m_0$ and $\frac{m_1^2}{m_0} <m_2 < m_1$. Then, show that there is a nonnegative smooth function $f : [0,1] \to {\mathbb R}^{+}$ such that $$ \int_{0}^{1} f(x)dx=m_0, \  \int_{0}^{1} xf(x)dx=m_1, \ \int_{0}^{1} x^2f(x)dx=m_2. $$ The easiest version is when ""smooth"" means just ""piecewise continuous"" ; in that case the only proof I know uses staged functions with complicated parameters. Is there a proof that avoids such tedious computations ? I guess that one can always find a ${\cal C}^{\infty}$ solution $f$, and even an analytic one, but I have no clear idea on how to proceed. My (very vague) thoughts : Bump functions ? Integration by parts, transforming the problem into an easier interpolation problem ? EDIT (09/24/2014) : In answer to Han de Bruijn’s comment, if one works on a general $[a,b]$ instead of $[0,1]$, so that $m_k=\int_a^b x^k f(x)dx$, then the inequalities become $$ am_0 < m_1 < bm_0, \ \frac{m_1^2}{m_0} < m_2 < (a+b)m_1-abm_0 $$ Note that the inequation $\frac{m_1^2}{m_0} < m_2$ (which remarkably contains no $a$ or $b$) comes from the Cauchy-Schwarz inequality.",,"['real-analysis', 'integration', 'interpolation']"
68,$\int f = \lim\int f$ but $\int_{E}f\neq\lim\int_{E} f_{n}$,but,\int f = \lim\int f \int_{E}f\neq\lim\int_{E} f_{n},"This is exercise 2.13 in Folland's Real Analysis textbook Let $(X, \mathcal{M})$ be a measurable space. Suppose $\{f_{n}\}\subset L^{+}$, $f_{n}\to f$ pointwise, and $\int  f=\lim\int f_{n}<\infty$. Then $\int_{E} f = \lim \int_{E} f_{n}$ for    all $E\in\mathcal{M}$. However, this need not be true if $\int  f=\lim\int f_{n}=\infty$. The first part already been asked and answered here in MSE. As for the counter-example, let $f_{n}=n\chi_{(0, 1/n)}+\chi_{(1,\infty)}$, $f=\chi_{(1, \infty)}$ and $E=(0, 1)$. Then, $f_{n}\to f$ pointwise, $\int f = \lim\int f_{n}=\infty$, yet $\int_{E} f=0\neq 1=\lim \int_{E} f_{n}$. So the exercise is solved. Now my question is: Can we find  $\{f_{n}\}\subset L^{+}$, $f_{n}\to f$ pointwise, and $\int  f=\lim\int f_{n}=\infty$, and a subset $E$ such that $\lim\int_{E} f_{n}=\infty$ but $\int_{E} f <\infty$? Notation: Here $L^{+}$ is the set of all Borel measurable functions from a space $X$ to $[0, \infty]$. (In the above example, I used $X=\mathbb{R}$ for the domain).","This is exercise 2.13 in Folland's Real Analysis textbook Let $(X, \mathcal{M})$ be a measurable space. Suppose $\{f_{n}\}\subset L^{+}$, $f_{n}\to f$ pointwise, and $\int  f=\lim\int f_{n}<\infty$. Then $\int_{E} f = \lim \int_{E} f_{n}$ for    all $E\in\mathcal{M}$. However, this need not be true if $\int  f=\lim\int f_{n}=\infty$. The first part already been asked and answered here in MSE. As for the counter-example, let $f_{n}=n\chi_{(0, 1/n)}+\chi_{(1,\infty)}$, $f=\chi_{(1, \infty)}$ and $E=(0, 1)$. Then, $f_{n}\to f$ pointwise, $\int f = \lim\int f_{n}=\infty$, yet $\int_{E} f=0\neq 1=\lim \int_{E} f_{n}$. So the exercise is solved. Now my question is: Can we find  $\{f_{n}\}\subset L^{+}$, $f_{n}\to f$ pointwise, and $\int  f=\lim\int f_{n}=\infty$, and a subset $E$ such that $\lim\int_{E} f_{n}=\infty$ but $\int_{E} f <\infty$? Notation: Here $L^{+}$ is the set of all Borel measurable functions from a space $X$ to $[0, \infty]$. (In the above example, I used $X=\mathbb{R}$ for the domain).",,"['real-analysis', 'measure-theory']"
69,Folland's proof of the Hahn Decomposition. Minor error?,Folland's proof of the Hahn Decomposition. Minor error?,,"Theorem 3.3 of Folland's Real Analysis (ed 2) is the Hahn decomposition theorem. In the proof he assumes that the signed measure $\nu$ he is considering does not take the value $-\infty$. Then he argues: Let $m$ be the supremum of $\nu(E)$ as $E$ ranges over all positive   sets; thus there is a sequence $\{P_j\}$ of positive sets such that   $\nu(P_j) \to m$. Let $P = \cup_1^\infty P_j$. By Lemma 3.2 and   Proposition 3.1, $P$ is positive and $\nu(P)=m$; in particular, $m < \infty.$ The justification for the claim that $m<\infty$ is not clear to me. Could it be the case that Folland wanted to assume that $\nu$ does not take on the value $\infty$ in which case I think that the rest of the proof would work. Or am I missing something?","Theorem 3.3 of Folland's Real Analysis (ed 2) is the Hahn decomposition theorem. In the proof he assumes that the signed measure $\nu$ he is considering does not take the value $-\infty$. Then he argues: Let $m$ be the supremum of $\nu(E)$ as $E$ ranges over all positive   sets; thus there is a sequence $\{P_j\}$ of positive sets such that   $\nu(P_j) \to m$. Let $P = \cup_1^\infty P_j$. By Lemma 3.2 and   Proposition 3.1, $P$ is positive and $\nu(P)=m$; in particular, $m < \infty.$ The justification for the claim that $m<\infty$ is not clear to me. Could it be the case that Folland wanted to assume that $\nu$ does not take on the value $\infty$ in which case I think that the rest of the proof would work. Or am I missing something?",,"['real-analysis', 'measure-theory']"
70,"an integral estimate from Stein's book, Singular Integral","an integral estimate from Stein's book, Singular Integral",,"I am reading the Stein's book Singular Integrals and Differentiability Properties of Functions . In the text (page 40), he states that $$ \int_{|x|\geq 2|y|}\Big|\frac{1}{|x-y|^n}-\frac{1}{|x|^n}\Big| dx\leq C $$ where $x,y\in \mathbb{R^n}$ and $C$ is a constant only depending on the dimension $n$. Could you please explain me how to get this estimate? Thanks in advance.","I am reading the Stein's book Singular Integrals and Differentiability Properties of Functions . In the text (page 40), he states that $$ \int_{|x|\geq 2|y|}\Big|\frac{1}{|x-y|^n}-\frac{1}{|x|^n}\Big| dx\leq C $$ where $x,y\in \mathbb{R^n}$ and $C$ is a constant only depending on the dimension $n$. Could you please explain me how to get this estimate? Thanks in advance.",,"['real-analysis', 'integration']"
71,Why do we consider measurable function when dealing with abstract integration?,Why do we consider measurable function when dealing with abstract integration?,,"Let $[0,\infty]$ be equipped with the order topology. (That is, it is a subspace of the standard topology on the extended real) Let $(X,\mathfrak{M},\mu)$ be a measure space. Let $f:X\rightarrow [0,\infty]$ be a measurable function, which means that the inverse of each borel set of $[0,\infty]$ under $f$ is an element of $\mathfrak{M}$. In measure theory, we define it's integration as the supremum of $\sum_{x\in s(X)} x\mu(s^{-1}(x))$ where $s$ is a simple measurable fuction such that $s≦f$. The point is, the role of $\mu$ only acts on those simple functions $s$, not $f$ directly. What would go wrong if one defines integration for an arbitrary function $f$? That is, what's wrong with defining an integration of an arbitrary function $f$ as the supremum of $\sum_{x\in s(X)} x\mu(s^{-1}(x))$ over simple meadurable functions $s≦f$?","Let $[0,\infty]$ be equipped with the order topology. (That is, it is a subspace of the standard topology on the extended real) Let $(X,\mathfrak{M},\mu)$ be a measure space. Let $f:X\rightarrow [0,\infty]$ be a measurable function, which means that the inverse of each borel set of $[0,\infty]$ under $f$ is an element of $\mathfrak{M}$. In measure theory, we define it's integration as the supremum of $\sum_{x\in s(X)} x\mu(s^{-1}(x))$ where $s$ is a simple measurable fuction such that $s≦f$. The point is, the role of $\mu$ only acts on those simple functions $s$, not $f$ directly. What would go wrong if one defines integration for an arbitrary function $f$? That is, what's wrong with defining an integration of an arbitrary function $f$ as the supremum of $\sum_{x\in s(X)} x\mu(s^{-1}(x))$ over simple meadurable functions $s≦f$?",,"['real-analysis', 'measure-theory']"
72,How to show that $e^x$ is differentiable?,How to show that  is differentiable?,e^x,"I tried to search for a few minutes but I didn't find this question so I hope it's not a duplicate. So I want to show that $(e^x)' = e^x$. To do that, I must proof that the limit: $$\lim_{h\to0}\frac{f(x+h) - f(x)}{h} = \lim_{h\to0}\frac{e^{x+h} - e^x}{h}$$ exists and equals to $e^x$. So I have: $$\frac{e^{x+h} - e^x}{h} = \frac{e^x \cdot e^h - e^x}{h} = e^x \bigg(\frac{e^h - 1}{h}\bigg) \\ e^h - 1 = z \implies e^h = 1+z >0 \implies h = \ln(1+z)$$ Because of the continuity of the $\ln$ and $e^x$ functions, we have: $$h\to 0 \iff e^h \to 1 \implies z\to0 \\ z > 0 \implies \frac{1}{z} \to +\infty \\ z < 0 \implies \frac{1}{z} \to -\infty \\ \frac{e^h - 1}{h} = \frac{z}{\ln(1+z)} = \frac{1}{\frac{1}{z}\ln(1+z)} = \bigg[ y = \frac{1}{z} \bigg] =  \frac{1}{y\ln(1+\frac{1}{y})} = \frac{1}{\ln(1+\frac{1}{y})^y} \\ h \to 0 \implies |y| \to +\infty \implies \bigg(1+\frac{1}{y}\bigg)^y \to e \implies \\ \lim_{h\to0}\frac{e^h - 1}{h} = \frac{1}{\ln e} = 1 \implies (e^x)' = e^x \cdot 1 = e^x$$ But how do I prove that $\lim_{\pm\infty}(1 + \frac{1}{x})^x = e$ ? We defined $e$ like this: $$\lim_{n\to\infty} \bigg(1 + \frac{1}{n}\bigg)^n = e \space \,, \space n\in\mathbb{N}$$ I thought of the sandwich theorem: $\forall x \geq 1 \,, x\in\mathbb{R} \,, \lfloor x \rfloor \leq x < \lfloor x \rfloor + 1$ and $\lfloor x \rfloor , \lfloor x \rfloor + 1 \in \mathbb{N} $ and if I denote $\mathbb{N} \ni n = \lfloor x \rfloor$ I have: $$ \bigg(1 + \frac{1}{n+1}\bigg)^n \leq  \bigg(1 + \frac{1}{x}\bigg)^x \leq \bigg(1 + \frac{1}{n}\bigg)^{n+1} \\   \lim_{n\to+\infty}\bigg(1 + \frac{1}{n+1}\bigg)^n = \frac{\lim_{n\to+\infty}\bigg(1 + \frac{1}{n+1}\bigg)^{n+1}}{\lim_{n\to+\infty}\bigg(1 + \frac{1}{n+1}\bigg)} = \frac{e}{1+0} = e \\  \lim_{n\to+\infty}\bigg(1 + \frac{1}{n}\bigg)^{n+1} =  \lim_{n\to+\infty}\bigg(1 + \frac{1}{n}\bigg)^n \cdot \lim_{n\to+\infty}\bigg(1 + \frac{1}{n}\bigg) = e \cdot (1 + 0) = e$$ and now I could apply the theorem but then again, how do I show that this: $\bigg(1 + \frac{1}{n+1}\bigg)^n \leq  \bigg(1 + \frac{1}{x}\bigg)^x \leq \bigg(1 + \frac{1}{n}\bigg)^{n+1}$ is true? I mean, I can say that (left side): $$ n+1 > x \implies \frac{1}{n+1} < \frac{1}{x} \implies 1 + \frac{1}{n+1} < 1 + \frac{1}{x} \\ \forall y\in\mathbb{R_+} \space n \leq x \implies y^n \leq y^x \\ \implies \bigg(1 + \frac{1}{n+1}\bigg)^n \leq  \bigg(1 + \frac{1}{x}\bigg)^x$$ and by analogy the right side but is this a proof in terms of limits? and that's just $+\infty$ for $x \geq 1$, what do I do for $-\infty$ ? For $x\in\langle 0, 1\rangle$, a substitution $y = \frac{1}{x}$ gives me: $$\lim_{y\to 0}\bigg(1 + y\bigg)^\frac{1}{y}$$ I'll stop here to see if I'm on the right track, any hints, suggestions, edits, comments and answers are welcome!","I tried to search for a few minutes but I didn't find this question so I hope it's not a duplicate. So I want to show that $(e^x)' = e^x$. To do that, I must proof that the limit: $$\lim_{h\to0}\frac{f(x+h) - f(x)}{h} = \lim_{h\to0}\frac{e^{x+h} - e^x}{h}$$ exists and equals to $e^x$. So I have: $$\frac{e^{x+h} - e^x}{h} = \frac{e^x \cdot e^h - e^x}{h} = e^x \bigg(\frac{e^h - 1}{h}\bigg) \\ e^h - 1 = z \implies e^h = 1+z >0 \implies h = \ln(1+z)$$ Because of the continuity of the $\ln$ and $e^x$ functions, we have: $$h\to 0 \iff e^h \to 1 \implies z\to0 \\ z > 0 \implies \frac{1}{z} \to +\infty \\ z < 0 \implies \frac{1}{z} \to -\infty \\ \frac{e^h - 1}{h} = \frac{z}{\ln(1+z)} = \frac{1}{\frac{1}{z}\ln(1+z)} = \bigg[ y = \frac{1}{z} \bigg] =  \frac{1}{y\ln(1+\frac{1}{y})} = \frac{1}{\ln(1+\frac{1}{y})^y} \\ h \to 0 \implies |y| \to +\infty \implies \bigg(1+\frac{1}{y}\bigg)^y \to e \implies \\ \lim_{h\to0}\frac{e^h - 1}{h} = \frac{1}{\ln e} = 1 \implies (e^x)' = e^x \cdot 1 = e^x$$ But how do I prove that $\lim_{\pm\infty}(1 + \frac{1}{x})^x = e$ ? We defined $e$ like this: $$\lim_{n\to\infty} \bigg(1 + \frac{1}{n}\bigg)^n = e \space \,, \space n\in\mathbb{N}$$ I thought of the sandwich theorem: $\forall x \geq 1 \,, x\in\mathbb{R} \,, \lfloor x \rfloor \leq x < \lfloor x \rfloor + 1$ and $\lfloor x \rfloor , \lfloor x \rfloor + 1 \in \mathbb{N} $ and if I denote $\mathbb{N} \ni n = \lfloor x \rfloor$ I have: $$ \bigg(1 + \frac{1}{n+1}\bigg)^n \leq  \bigg(1 + \frac{1}{x}\bigg)^x \leq \bigg(1 + \frac{1}{n}\bigg)^{n+1} \\   \lim_{n\to+\infty}\bigg(1 + \frac{1}{n+1}\bigg)^n = \frac{\lim_{n\to+\infty}\bigg(1 + \frac{1}{n+1}\bigg)^{n+1}}{\lim_{n\to+\infty}\bigg(1 + \frac{1}{n+1}\bigg)} = \frac{e}{1+0} = e \\  \lim_{n\to+\infty}\bigg(1 + \frac{1}{n}\bigg)^{n+1} =  \lim_{n\to+\infty}\bigg(1 + \frac{1}{n}\bigg)^n \cdot \lim_{n\to+\infty}\bigg(1 + \frac{1}{n}\bigg) = e \cdot (1 + 0) = e$$ and now I could apply the theorem but then again, how do I show that this: $\bigg(1 + \frac{1}{n+1}\bigg)^n \leq  \bigg(1 + \frac{1}{x}\bigg)^x \leq \bigg(1 + \frac{1}{n}\bigg)^{n+1}$ is true? I mean, I can say that (left side): $$ n+1 > x \implies \frac{1}{n+1} < \frac{1}{x} \implies 1 + \frac{1}{n+1} < 1 + \frac{1}{x} \\ \forall y\in\mathbb{R_+} \space n \leq x \implies y^n \leq y^x \\ \implies \bigg(1 + \frac{1}{n+1}\bigg)^n \leq  \bigg(1 + \frac{1}{x}\bigg)^x$$ and by analogy the right side but is this a proof in terms of limits? and that's just $+\infty$ for $x \geq 1$, what do I do for $-\infty$ ? For $x\in\langle 0, 1\rangle$, a substitution $y = \frac{1}{x}$ gives me: $$\lim_{y\to 0}\bigg(1 + y\bigg)^\frac{1}{y}$$ I'll stop here to see if I'm on the right track, any hints, suggestions, edits, comments and answers are welcome!",,"['real-analysis', 'analysis', 'derivatives']"
73,Analyzing Integral of a product using Cauchy Mean Value Theorem?,Analyzing Integral of a product using Cauchy Mean Value Theorem?,,"Prove that if the functions $g,h:[a,b] \rightarrow \mathbb{R}$ are continuous, with $h(x) \geq 0$ for all $x\in[a,b]$, then there is a point $c$ in $(a,b)$ such that                   $\int_a^bh(x)g(x)dx=g(c)\int_a^bh(x)dx$. At first I tried to use the Cauchy Mean Value Theorem, by letting $A(x) =\int_a^xh(t)g(t)dt$ and $B(x) =\int_a^xh(t)dt$.  Then by CMVT, we have that there exists a point $c$ such that  $$\frac{ \int_a^bh(x)g(x)dx }{ \int_a^bh(x)dx}= \frac{h(c)g(c)}{h(c)}=g(c) $$, and the result follows But this only holds if $h(x)$ is never $0$, which we dont have.  So is this the wrong way to do this?","Prove that if the functions $g,h:[a,b] \rightarrow \mathbb{R}$ are continuous, with $h(x) \geq 0$ for all $x\in[a,b]$, then there is a point $c$ in $(a,b)$ such that                   $\int_a^bh(x)g(x)dx=g(c)\int_a^bh(x)dx$. At first I tried to use the Cauchy Mean Value Theorem, by letting $A(x) =\int_a^xh(t)g(t)dt$ and $B(x) =\int_a^xh(t)dt$.  Then by CMVT, we have that there exists a point $c$ such that  $$\frac{ \int_a^bh(x)g(x)dx }{ \int_a^bh(x)dx}= \frac{h(c)g(c)}{h(c)}=g(c) $$, and the result follows But this only holds if $h(x)$ is never $0$, which we dont have.  So is this the wrong way to do this?",,['real-analysis']
74,"Does $f$ exist such that $f(1)<0 , f(5)>3 $ and $ f'(x)\le e^{-f(x)}$",Does  exist such that  and,"f f(1)<0 , f(5)>3   f'(x)\le e^{-f(x)}","Does there exist a continuously differentiable function $f:[1,5]\rightarrow\mathbb{R}$ such that $f(1)<0 , f(5)>3$ and $f'(x)\le e^{-f(x)}$ ? My Attempt : If such $f$ exists the mean value theorem states: $\exists c \in(1,5) :$ $$f(5)-f(1)=f'(c)(5-1)$$ Now $$f'(c)(5-1)\le 4e^{-f(c)}\implies f(5)-f(1)\le 4e^{-f(c)}$$ and $$f(5)-f(1)>3$$ Therefore $$3<4e^{-f(c)} \implies -f(c)>\ln(3/4)$$ $$\implies f(c)<-\ln(3/4)$$ This dosen't result in the nice contradiction I was searching for, does such a function exist?","Does there exist a continuously differentiable function $f:[1,5]\rightarrow\mathbb{R}$ such that $f(1)<0 , f(5)>3$ and $f'(x)\le e^{-f(x)}$ ? My Attempt : If such $f$ exists the mean value theorem states: $\exists c \in(1,5) :$ $$f(5)-f(1)=f'(c)(5-1)$$ Now $$f'(c)(5-1)\le 4e^{-f(c)}\implies f(5)-f(1)\le 4e^{-f(c)}$$ and $$f(5)-f(1)>3$$ Therefore $$3<4e^{-f(c)} \implies -f(c)>\ln(3/4)$$ $$\implies f(c)<-\ln(3/4)$$ This dosen't result in the nice contradiction I was searching for, does such a function exist?",,['real-analysis']
75,Let X be a metric space in which every infinite subset has a limit point. Prove that X is compact.,Let X be a metric space in which every infinite subset has a limit point. Prove that X is compact.,,"Let $X$ be a metric space in which every infinite subset has a limit point. Prove that $X$ is compact. The following is my proof I'd like to know if it is correct. Proof: I will use the fact that if $X$ is a metric space in which every infinite subset has a limit point, then $X$ has a countable base, so every open cover of $X$ has a countable subcover $\{G_n\}$ . To lead contradiction, suppose that there is no finite subcollection of $\{G_n\}$ which covers $X$ . Then the complement $F_n$ of $G_1 \cup \dots \cup G_n$ is nonempty for each $n$ , $\bigcap F_n$ is empty because if we assume it is nonempty then there is a point $x\in \bigcap F_n$ for which $x\in F_n$ for every $n$ . Since $F_n = (G_1 \cup \cdots \cup G_n)^c$ this means that $x\notin G_1 \cup \cdots G_n$ for every $n$ which contradicts the assumption that $\{G_n\}$ is a countable cover of $X$ . So we can choose $x_1$ that doesn't belong to $G_1$ and $x_2$ distinct from $x_1$ that doesn't belong to $G_1 \cup G_2$ and recursively, get a sequence of distinct points $\{x_n\}$ such that each $x_n$ is from $F_n$ . Let's call this set $E$ . We can take distinct points here so $E$ is infinite (since if it is finite then $E=\{x_1,\dots, x_k\}$ . Then each $x_i$ is covered by some $G_m$ , we can take a union of at most $k$ such covers and since it is only a finite union, by assumption, there exists another point outside the union that belongs to $E$ . This is true because $F_n$ is a decreasing sequence of sets and so for the largest set $F_n$ from which we extract points in the set $E$ , we must have $x_n \notin G_{k_1} \cup \cdots \cup G_{k_n}$ ). This is a contradiction since we have another point in the set $E$ that is equal to $x_n$ (since the recursive process is finite so at some point we have repetitive points) and $x_n$ must belong to some $G_{k_j}$ .This proves that $E$ is infinite. Now by hypothesis, $E$ has a limit point $z$ . Then, $z$ belongs to some open cover $G_m$ and so there is an open ball $B(z;d)\subset G_m$ . But then, by our construction, all $x_n$ for which $n \ge m$ , do not belong to $B(z;d)$ since if it does then $x_n$ belongs to the union of $G_1,\dots,G_n$ and so $x_n \notin F_n$ , which contradicts the way we defined $x_n$ . So there is an open ball containing $z$ which doesn't contain infinitely many points of $E$ , which contradicts the assumption that it is a limit point. Thus, by way of contradiction, there must be a finite subcover. Now the proof is not yet fully complete. All we have shown is that if $X$ has a countable cover then it always has a finite subcover. We want this to hold for all open covers. However, this is dealt since from Exercise 23 we know that $X$ has a countable base and this allows us to always extract a countable subcover of $X$ from any given open cover. QED. This is my proof but I have a couple points that I'm not sure of. First, how do I guarantee that I can select all distinct points of $x_n$ by the above process? Also, is my proof that $E$ must be an infinite set correct? Is there anything incorrect about the rest of the proof? Thanks.","Let be a metric space in which every infinite subset has a limit point. Prove that is compact. The following is my proof I'd like to know if it is correct. Proof: I will use the fact that if is a metric space in which every infinite subset has a limit point, then has a countable base, so every open cover of has a countable subcover . To lead contradiction, suppose that there is no finite subcollection of which covers . Then the complement of is nonempty for each , is empty because if we assume it is nonempty then there is a point for which for every . Since this means that for every which contradicts the assumption that is a countable cover of . So we can choose that doesn't belong to and distinct from that doesn't belong to and recursively, get a sequence of distinct points such that each is from . Let's call this set . We can take distinct points here so is infinite (since if it is finite then . Then each is covered by some , we can take a union of at most such covers and since it is only a finite union, by assumption, there exists another point outside the union that belongs to . This is true because is a decreasing sequence of sets and so for the largest set from which we extract points in the set , we must have ). This is a contradiction since we have another point in the set that is equal to (since the recursive process is finite so at some point we have repetitive points) and must belong to some .This proves that is infinite. Now by hypothesis, has a limit point . Then, belongs to some open cover and so there is an open ball . But then, by our construction, all for which , do not belong to since if it does then belongs to the union of and so , which contradicts the way we defined . So there is an open ball containing which doesn't contain infinitely many points of , which contradicts the assumption that it is a limit point. Thus, by way of contradiction, there must be a finite subcover. Now the proof is not yet fully complete. All we have shown is that if has a countable cover then it always has a finite subcover. We want this to hold for all open covers. However, this is dealt since from Exercise 23 we know that has a countable base and this allows us to always extract a countable subcover of from any given open cover. QED. This is my proof but I have a couple points that I'm not sure of. First, how do I guarantee that I can select all distinct points of by the above process? Also, is my proof that must be an infinite set correct? Is there anything incorrect about the rest of the proof? Thanks.","X X X X X \{G_n\} \{G_n\} X F_n G_1 \cup \dots \cup G_n n \bigcap F_n x\in \bigcap F_n x\in F_n n F_n = (G_1 \cup \cdots \cup G_n)^c x\notin G_1 \cup \cdots G_n n \{G_n\} X x_1 G_1 x_2 x_1 G_1 \cup G_2 \{x_n\} x_n F_n E E E=\{x_1,\dots, x_k\} x_i G_m k E F_n F_n E x_n \notin G_{k_1} \cup \cdots \cup G_{k_n} E x_n x_n G_{k_j} E E z z G_m B(z;d)\subset G_m x_n n \ge m B(z;d) x_n G_1,\dots,G_n x_n \notin F_n x_n z E X X X x_n E","['real-analysis', 'general-topology', 'solution-verification', 'metric-spaces', 'compactness']"
76,eulers original derivation for the Euler–Maclaurin formula?,eulers original derivation for the Euler–Maclaurin formula?,,Please does someone know a good description of how Euler did derive his summation formula?  Thank you!,Please does someone know a good description of how Euler did derive his summation formula?  Thank you!,,"['calculus', 'real-analysis', 'reference-request', 'math-history', 'euler-maclaurin']"
77,$\int_0^1 f(x)^2\le 1$ and $\int_0^1 f'(x)^2\le 1$ $\Rightarrow$ $\left|f(x)\right|\le \sqrt3$,and,\int_0^1 f(x)^2\le 1 \int_0^1 f'(x)^2\le 1 \Rightarrow \left|f(x)\right|\le \sqrt3,"Let $f:[0, 1]\rightarrow \mathbb{R}$ be a function that is continous on $[0,1]$ and derivable on $(0, 1)$. If $\int_0^1 f(x)^2\le 1$ and $\int_0^1 f'(x)^2\le 1$, show that $\left|f(x)\right|\le \sqrt3$ for all $x\in [0, 1]$. Using Cauchy-Schwarz inequality I've shown that $\left|f(x)-f(0)\right|\le \sqrt{x}$ and $\left|f(x)^2-f(0)^2\right|\le 2$. Any other ideas?","Let $f:[0, 1]\rightarrow \mathbb{R}$ be a function that is continous on $[0,1]$ and derivable on $(0, 1)$. If $\int_0^1 f(x)^2\le 1$ and $\int_0^1 f'(x)^2\le 1$, show that $\left|f(x)\right|\le \sqrt3$ for all $x\in [0, 1]$. Using Cauchy-Schwarz inequality I've shown that $\left|f(x)-f(0)\right|\le \sqrt{x}$ and $\left|f(x)^2-f(0)^2\right|\le 2$. Any other ideas?",,"['calculus', 'real-analysis', 'integration', 'inequality']"
78,Outer and inner approximation of set with finite outer measure,Outer and inner approximation of set with finite outer measure,,"I was wondering if somebody could help me out with a solution for the following problem (taken from Royden's Real Analysis, 4e. (ch. 2.4, prob. 18): Let $E$ have finite outer measure. Show that there is an $F_\sigma$ set $F$ and a $G_\delta$ set $G$ such that $F \subset E \subset G$ and $m^*[F]=m^*[E]=m^*[G]$. So far I constructed a candidate set $G$ (for each $n \in \mathbb{N}$ there is a countable open interval cover $\{ I_{n,k} \}$ such that $\Sigma_k \ \ell(I_{n,k}) < m^*[E] + \frac{1}{n}$; then set $G:=\cap_n \cup_k I_{n,k}$). Since $G$ is measurable, I can approximate it with an $F_\sigma$ set $F \subset G$ such that $m^*[F]=m^*[G]$, but of course there is no guarantee that $F \subset E$. Is there anyway to salvage this proof by toying with $F$ so that $F \subset E$ but still remains an $F_\sigma$ set? Or should I approach another way? Thanks in advance for any help!","I was wondering if somebody could help me out with a solution for the following problem (taken from Royden's Real Analysis, 4e. (ch. 2.4, prob. 18): Let $E$ have finite outer measure. Show that there is an $F_\sigma$ set $F$ and a $G_\delta$ set $G$ such that $F \subset E \subset G$ and $m^*[F]=m^*[E]=m^*[G]$. So far I constructed a candidate set $G$ (for each $n \in \mathbb{N}$ there is a countable open interval cover $\{ I_{n,k} \}$ such that $\Sigma_k \ \ell(I_{n,k}) < m^*[E] + \frac{1}{n}$; then set $G:=\cap_n \cup_k I_{n,k}$). Since $G$ is measurable, I can approximate it with an $F_\sigma$ set $F \subset G$ such that $m^*[F]=m^*[G]$, but of course there is no guarantee that $F \subset E$. Is there anyway to salvage this proof by toying with $F$ so that $F \subset E$ but still remains an $F_\sigma$ set? Or should I approach another way? Thanks in advance for any help!",,"['real-analysis', 'measure-theory']"
79,Cantor construction is continuous,Cantor construction is continuous,,"I define a function $f:\mathbb{R}\to\mathbb{R}$ as follows: $f(x)=0$ for $x\le 0$. $f(x)=1$ for $x\ge1$. $f(x)=\dfrac12$ for $x\in\left[\dfrac13,\dfrac23\right]$. $f(x)=\dfrac14$ for $x\in\left[\dfrac19,\dfrac29\right]$, $f(x)=\dfrac34$ for $x\in\left[\dfrac79,\dfrac89\right]$. and so on. So this function has been defined on $\mathbb{R}$, except for the Cantor set. How can we fill in the function on the Cantor set, so that we get a continuous function?","I define a function $f:\mathbb{R}\to\mathbb{R}$ as follows: $f(x)=0$ for $x\le 0$. $f(x)=1$ for $x\ge1$. $f(x)=\dfrac12$ for $x\in\left[\dfrac13,\dfrac23\right]$. $f(x)=\dfrac14$ for $x\in\left[\dfrac19,\dfrac29\right]$, $f(x)=\dfrac34$ for $x\in\left[\dfrac79,\dfrac89\right]$. and so on. So this function has been defined on $\mathbb{R}$, except for the Cantor set. How can we fill in the function on the Cantor set, so that we get a continuous function?",,['real-analysis']
80,Is there a formula for the Haar measure on a product of groups?,Is there a formula for the Haar measure on a product of groups?,,"Let $ (G_{n})_{n \in \mathbb{N}} $ be a sequence of locally compact topological groups with a corresponding sequence $ (\mu_{n})_{n \in \mathbb{N}} $ of Haar measures. Is there a way to construct a Haar measure on the product group $ \displaystyle \prod_{n \in \mathbb{N}} G_{n} $ using $ (\mu_{n})_{n \in \mathbb{N}} $? Is the task easier if the product is finite, or if all of the $ G_{n} $’s are compact?","Let $ (G_{n})_{n \in \mathbb{N}} $ be a sequence of locally compact topological groups with a corresponding sequence $ (\mu_{n})_{n \in \mathbb{N}} $ of Haar measures. Is there a way to construct a Haar measure on the product group $ \displaystyle \prod_{n \in \mathbb{N}} G_{n} $ using $ (\mu_{n})_{n \in \mathbb{N}} $? Is the task easier if the product is finite, or if all of the $ G_{n} $’s are compact?",,"['real-analysis', 'functional-analysis', 'measure-theory', 'harmonic-analysis', 'locally-compact-groups']"
81,Continuous Function,Continuous Function,,"Let  $ f: \Bbb R^2 \to \Bbb R $ such that : $ \forall _{y_0 \in \Bbb R\ }:  $ function $ x \to f(x,y_0) $ continuous function and increasing $ \forall _{x_0 \in \Bbb R\ }:  $ function $ y \to f(x_0,y) $ continuous function I mean the continuity of one variable. Prove the continuity of a function $f: \Bbb R^2 \to \Bbb R $ I know definition, but I can not do. I wanted to use the definitions of Cauchy.","Let  $ f: \Bbb R^2 \to \Bbb R $ such that : $ \forall _{y_0 \in \Bbb R\ }:  $ function $ x \to f(x,y_0) $ continuous function and increasing $ \forall _{x_0 \in \Bbb R\ }:  $ function $ y \to f(x_0,y) $ continuous function I mean the continuity of one variable. Prove the continuity of a function $f: \Bbb R^2 \to \Bbb R $ I know definition, but I can not do. I wanted to use the definitions of Cauchy.",,['real-analysis']
82,"A set of the plane recursively including ""crosses""...","A set of the plane recursively including ""crosses""...",,"Definition: Call ""cross with center in $(x,y) \in \mathbb R^2$"" a set of $\mathbb R^2$ given by $(I_1(x)\times\{y\}) \cup (\{x\}\times I_2(y))$ where $I_1(x) \subseteq \mathbb R$ is a neighbourhood of $x$ and $I_2(y) \subseteq \mathbb R$ is a neighbourhood of $y$. Problem: Let $A \subseteq \mathbb R^2$ be a set such that for any $z \in A$ there exists a cross with center in $z$ which is all included in $A$. Is it true that $A$ must include a nonempty open set? ( Warm up exercise: prove that $A$ can actually be not open.)","Definition: Call ""cross with center in $(x,y) \in \mathbb R^2$"" a set of $\mathbb R^2$ given by $(I_1(x)\times\{y\}) \cup (\{x\}\times I_2(y))$ where $I_1(x) \subseteq \mathbb R$ is a neighbourhood of $x$ and $I_2(y) \subseteq \mathbb R$ is a neighbourhood of $y$. Problem: Let $A \subseteq \mathbb R^2$ be a set such that for any $z \in A$ there exists a cross with center in $z$ which is all included in $A$. Is it true that $A$ must include a nonempty open set? ( Warm up exercise: prove that $A$ can actually be not open.)",,"['real-analysis', 'general-topology']"
83,Is there a reference for compact imbedding of Hölder space?,Is there a reference for compact imbedding of Hölder space?,,"Suppose $0<\alpha <\beta$. Then, the Hölder space $C^\beta$ is compactly imbedded to $C^\alpha$. See the Wikipedia article Hölder condition . However, I could not find precise reference from some books on functional analysis. Can anybody  indicate a precise reference for this theorem? If possible, I would like to know a reference on the similar result on parabolic Hölder space.","Suppose $0<\alpha <\beta$. Then, the Hölder space $C^\beta$ is compactly imbedded to $C^\alpha$. See the Wikipedia article Hölder condition . However, I could not find precise reference from some books on functional analysis. Can anybody  indicate a precise reference for this theorem? If possible, I would like to know a reference on the similar result on parabolic Hölder space.",,"['real-analysis', 'functional-analysis', 'compactness', 'holder-spaces']"
84,Is the intersection of an arbitrary collection of semirings a semiring?,Is the intersection of an arbitrary collection of semirings a semiring?,,"A semiring (of sets) is a nonempty class $\mathcal{P}$ of subsets of the whole space $X$ that is closed under intersections and is such that any difference of two sets in $\mathcal{P}$ can be expressed as a finite disjoint union of sets in $\mathcal{P}$. Motivation for this question: for any class $\mathcal{E}$ of subsets of the whole space $X$, does there exist a unique smallest semiring containing $\mathcal{E}$? In other words, does there exist such a thing as the semiring generated by $\mathcal{E}$? I believe the answer is ""no,"" but I don't really have a good reason why. The actual question: Normally, to prove the existence of rings (or fields, I'll just stick to rings for simplicity) generated by a set $\mathcal{E}$, we first show that the intersection of an arbitrary collection of rings is again a ring. My hunch is that this result does not hold for semirings, and is the reason why there (possibly) does not exist such a thing as a semiring generated by $\mathcal{E}$. Let $\mathcal{P}_{\gamma}$ be a semiring for every $\gamma$ in some index set $\Gamma$. Define $\mathcal{P} = \cap \{\mathcal{P}_{\gamma}: \gamma \in \Gamma\}$. Then if $E, F \in \mathcal{P}$, we have that $E,F \in \mathcal{P}_{\gamma}$ for every $\gamma$. Then by the definition of semiring for each $\gamma$ there exists a disjoint collection of sets $\{E_{n_{\gamma}}\} \in \mathcal{P}_{\gamma}$ such that $E-F = \cup E_{n_{\gamma}}$ (for each $\gamma$ this sequence may be different). But for some reason I'm having trouble seeing why I can't take this disjoint sequence $\{E_{n_{\gamma}}\}$ to be common across $\gamma$. If I can, then it follows that an intersection of an arbitrary collection of semirings is a semiring, and then it follows that generated semirings exist. But I'm becoming convinced that they don't exist. Can anyone point out where I'm tripping up?","A semiring (of sets) is a nonempty class $\mathcal{P}$ of subsets of the whole space $X$ that is closed under intersections and is such that any difference of two sets in $\mathcal{P}$ can be expressed as a finite disjoint union of sets in $\mathcal{P}$. Motivation for this question: for any class $\mathcal{E}$ of subsets of the whole space $X$, does there exist a unique smallest semiring containing $\mathcal{E}$? In other words, does there exist such a thing as the semiring generated by $\mathcal{E}$? I believe the answer is ""no,"" but I don't really have a good reason why. The actual question: Normally, to prove the existence of rings (or fields, I'll just stick to rings for simplicity) generated by a set $\mathcal{E}$, we first show that the intersection of an arbitrary collection of rings is again a ring. My hunch is that this result does not hold for semirings, and is the reason why there (possibly) does not exist such a thing as a semiring generated by $\mathcal{E}$. Let $\mathcal{P}_{\gamma}$ be a semiring for every $\gamma$ in some index set $\Gamma$. Define $\mathcal{P} = \cap \{\mathcal{P}_{\gamma}: \gamma \in \Gamma\}$. Then if $E, F \in \mathcal{P}$, we have that $E,F \in \mathcal{P}_{\gamma}$ for every $\gamma$. Then by the definition of semiring for each $\gamma$ there exists a disjoint collection of sets $\{E_{n_{\gamma}}\} \in \mathcal{P}_{\gamma}$ such that $E-F = \cup E_{n_{\gamma}}$ (for each $\gamma$ this sequence may be different). But for some reason I'm having trouble seeing why I can't take this disjoint sequence $\{E_{n_{\gamma}}\}$ to be common across $\gamma$. If I can, then it follows that an intersection of an arbitrary collection of semirings is a semiring, and then it follows that generated semirings exist. But I'm becoming convinced that they don't exist. Can anyone point out where I'm tripping up?",,"['real-analysis', 'measure-theory']"
85,question about continuity: using polar coordinates,question about continuity: using polar coordinates,,"Given a function $f\colon\mathbb R^2\rightarrow \mathbb R$ I want to study continuity. So I know the $\varepsilon-\delta$ and sequence criterion. Now we had polar coordinates in lectures: set $x=r\cos\theta$ and $y=r\sin\theta$ and then consider $r\rightarrow 0$ for continuity in $(0,0)$. This transformation seems very usefull for expressions like e.g. $\frac{xy^2}{x^2+y^2}$ but don't I approach the function only on all straight lines in $(0,0)^t$ ? And for continuity I have to approach $(0,0)^t$ however I want to which I don't do using polar coordinates. So why can I still use polar coordinates? Thanks for helping. Add: I don't want a solution for the continuity of the example above. Clearly $|\frac{xy^2}{x^2+y^2}|\leq |y|<\epsilon$ and so continuity in $(0,0)$ with $f(0,0)=0$ which I also get with polar coordinates since $\cos^2\theta+\sin^2\theta=1$.","Given a function $f\colon\mathbb R^2\rightarrow \mathbb R$ I want to study continuity. So I know the $\varepsilon-\delta$ and sequence criterion. Now we had polar coordinates in lectures: set $x=r\cos\theta$ and $y=r\sin\theta$ and then consider $r\rightarrow 0$ for continuity in $(0,0)$. This transformation seems very usefull for expressions like e.g. $\frac{xy^2}{x^2+y^2}$ but don't I approach the function only on all straight lines in $(0,0)^t$ ? And for continuity I have to approach $(0,0)^t$ however I want to which I don't do using polar coordinates. So why can I still use polar coordinates? Thanks for helping. Add: I don't want a solution for the continuity of the example above. Clearly $|\frac{xy^2}{x^2+y^2}|\leq |y|<\epsilon$ and so continuity in $(0,0)$ with $f(0,0)=0$ which I also get with polar coordinates since $\cos^2\theta+\sin^2\theta=1$.",,"['real-analysis', 'analysis', 'multivariable-calculus', 'polar-coordinates']"
86,Is there an integral form of Newton's method?,Is there an integral form of Newton's method?,,"Warning : This seems like a silly sort of question, not the kind I'd ask out loud. The contraction mapping theorem is a basic tool for proving existence of, and finding solutions to, equations. Given an algebraic (read: not differential) equation $f(x)=0$ where $f : \mathbb{R} \rightarrow \mathbb{R}$ is sufficiently smooth, often it is possible prove that the mapping \begin{equation} \Phi_f(x) = x - \frac{f(x)}{f'(x)} \end{equation} is a contraction on some complete metric space by restricting $x$ to an interval. This yields existence of a solution to the original equation. This is Newton's method , which has both theoretical and practical significance. Consider the ordinary differential equation $\dot{\mathbf{x}}=\mathbf{v}(\mathbf{x},t)$. The Picard mapping \begin{equation*} \Psi(\phi)(t) = \mathbf{x}_0 + \int_0^t \mathbf{v}(\phi(\tau),\tau) \; d\tau \end{equation*} is a contraction on a function space, under suitable conditions on $\mathbf{v}$ . This yields an existence result for the given ODE with data $\mathbf{x}_0$. A very similar map appears in the study of certain nonlinear partial differential equations (e.g. Duhamel's principle applied to semilinear equations). Contrasted with the numerical solution of an algebraic equation, the contraction in these cases isn't of much practical use. Clearly the derivative is useful for proving existence of algebraic equations, and similarly for the integral and differential equations. In line with the title, have I missed a theoretical application of the integral to solve algebraic equations or the derivative to solve differential equations? If not, is there a moral reason why we shouldn't expect to find such applications?","Warning : This seems like a silly sort of question, not the kind I'd ask out loud. The contraction mapping theorem is a basic tool for proving existence of, and finding solutions to, equations. Given an algebraic (read: not differential) equation $f(x)=0$ where $f : \mathbb{R} \rightarrow \mathbb{R}$ is sufficiently smooth, often it is possible prove that the mapping \begin{equation} \Phi_f(x) = x - \frac{f(x)}{f'(x)} \end{equation} is a contraction on some complete metric space by restricting $x$ to an interval. This yields existence of a solution to the original equation. This is Newton's method , which has both theoretical and practical significance. Consider the ordinary differential equation $\dot{\mathbf{x}}=\mathbf{v}(\mathbf{x},t)$. The Picard mapping \begin{equation*} \Psi(\phi)(t) = \mathbf{x}_0 + \int_0^t \mathbf{v}(\phi(\tau),\tau) \; d\tau \end{equation*} is a contraction on a function space, under suitable conditions on $\mathbf{v}$ . This yields an existence result for the given ODE with data $\mathbf{x}_0$. A very similar map appears in the study of certain nonlinear partial differential equations (e.g. Duhamel's principle applied to semilinear equations). Contrasted with the numerical solution of an algebraic equation, the contraction in these cases isn't of much practical use. Clearly the derivative is useful for proving existence of algebraic equations, and similarly for the integral and differential equations. In line with the title, have I missed a theoretical application of the integral to solve algebraic equations or the derivative to solve differential equations? If not, is there a moral reason why we shouldn't expect to find such applications?",,"['real-analysis', 'ordinary-differential-equations', 'soft-question', 'partial-differential-equations']"
87,Show that the tangent space of the diagonal is the diagonal of the product of tangent space,Show that the tangent space of the diagonal is the diagonal of the product of tangent space,,"I'm stuck on this question for quite a few days and still haven't got a clue what to do. The question is as follows: If $\Delta$ is the diagonal of $X\times X$ where $X$ is a manifold, show that its tangent space $T_{(x,x)}(\Delta)$ is the diagonal of $T_x(X)\times T_x(X)$. Because this question follows a previous part, so I constructed a map $$f:X\longrightarrow X\times X$$ such that f(x)=(x,x). Therefore I have $X\overset{f}{\longrightarrow} X\times X$. Then we take the derivative map $$T_x(X)\overset{df_x}{\longrightarrow} T_{(x,x)}(X,X)$$ However this does not give me the tangent space of the diagonal...","I'm stuck on this question for quite a few days and still haven't got a clue what to do. The question is as follows: If $\Delta$ is the diagonal of $X\times X$ where $X$ is a manifold, show that its tangent space $T_{(x,x)}(\Delta)$ is the diagonal of $T_x(X)\times T_x(X)$. Because this question follows a previous part, so I constructed a map $$f:X\longrightarrow X\times X$$ such that f(x)=(x,x). Therefore I have $X\overset{f}{\longrightarrow} X\times X$. Then we take the derivative map $$T_x(X)\overset{df_x}{\longrightarrow} T_{(x,x)}(X,X)$$ However this does not give me the tangent space of the diagonal...",,"['real-analysis', 'analysis', 'differential-geometry', 'differential-topology']"
88,"If $B\times \{0\}$ is a Borel set in the plane, then $B$ is a Borel set in $\mathbb{R}$.","If  is a Borel set in the plane, then  is a Borel set in .",B\times \{0\} B \mathbb{R},"I'm trying to figure out how to prove the following ""obvious"" fact: Let $B\times \{0\}\subset \mathbb{R}^2$ be a Borel set, then $B\subset \mathbb{R}$ is a Borel set. The problem here is that I can't see a nice constructive way of expressing a Borel set, so I just can't intersect stuff with $\mathbb{R}\times\{0\}$ in some countable union/intersection, which is what I tried by starting by taking the countable family sets $[a,b]\times [c,d]$, $a,b,c,d\in \mathbb{Q}$ as the generators of the Borel sets of the plane. Is there a nice technique of approaching results like this?","I'm trying to figure out how to prove the following ""obvious"" fact: Let $B\times \{0\}\subset \mathbb{R}^2$ be a Borel set, then $B\subset \mathbb{R}$ is a Borel set. The problem here is that I can't see a nice constructive way of expressing a Borel set, so I just can't intersect stuff with $\mathbb{R}\times\{0\}$ in some countable union/intersection, which is what I tried by starting by taking the countable family sets $[a,b]\times [c,d]$, $a,b,c,d\in \mathbb{Q}$ as the generators of the Borel sets of the plane. Is there a nice technique of approaching results like this?",,"['real-analysis', 'measure-theory']"
89,A point is in the boundary of $E$ if and only if it belongs to the closure of both $E$ and its complement.,A point is in the boundary of  if and only if it belongs to the closure of both  and its complement.,E E,"Let $E$ be a subset of a metric space $(S,d)$. Prove that: A point is in the boundary of $E$ if and only if it belongs to the closure of both $E$ and its complement. Here is what I thought: I'm first trying to understand what I need to prove. The boundary of $E$ is the set $E^- -E ^{\circ}$. It a point belongs to both $E^-$ and $(S-E)^-$, then it belongs to $E^- \cap (S-E)^-$. Therefore I think I need to prove that: $$E^- -E ^{\circ}=E^- \cap (S-E)^-$$ And now I'm not sure if there any set theory rules I could use. Intuitive I would say that I need to prove: $S-E ^\circ =(S-E)^-$. Is this correct ? And I can kind of feel that this last statement is right, but I can't prove this rigoursly. Could anybody help me how I can prove this statement more rigourisly ?","Let $E$ be a subset of a metric space $(S,d)$. Prove that: A point is in the boundary of $E$ if and only if it belongs to the closure of both $E$ and its complement. Here is what I thought: I'm first trying to understand what I need to prove. The boundary of $E$ is the set $E^- -E ^{\circ}$. It a point belongs to both $E^-$ and $(S-E)^-$, then it belongs to $E^- \cap (S-E)^-$. Therefore I think I need to prove that: $$E^- -E ^{\circ}=E^- \cap (S-E)^-$$ And now I'm not sure if there any set theory rules I could use. Intuitive I would say that I need to prove: $S-E ^\circ =(S-E)^-$. Is this correct ? And I can kind of feel that this last statement is right, but I can't prove this rigoursly. Could anybody help me how I can prove this statement more rigourisly ?",,"['real-analysis', 'metric-spaces']"
90,Calculating $\lim_{n\to\infty}\sum_{k=1}^{\infty}\frac{(-1)^{k}\arctan(n^{2}k)}{n^{\alpha}+k^{3/2}}$,Calculating,\lim_{n\to\infty}\sum_{k=1}^{\infty}\frac{(-1)^{k}\arctan(n^{2}k)}{n^{\alpha}+k^{3/2}},"I am studying for my exam in real analysis and I am having difficulties with some of the material, I know that the following should be solved by using the counting measure and LDCT, but I don't know how. For $\alpha>0,$ Calculate   $$\lim_{n\to\infty}\sum_{k=1}^{\infty}\frac{(-1)^{k}\arctan(n^{2}k)}{n^{\alpha}+k^{3/2}}$$ I would greatly appreciate it if in the answers you can include all the details about the theorems used (since there is a high importance for the arguments for why we can do what we do in each step, and I don't understand the material good enough to be able to understand that some step is actually not trivial and uses some theorem)","I am studying for my exam in real analysis and I am having difficulties with some of the material, I know that the following should be solved by using the counting measure and LDCT, but I don't know how. For $\alpha>0,$ Calculate   $$\lim_{n\to\infty}\sum_{k=1}^{\infty}\frac{(-1)^{k}\arctan(n^{2}k)}{n^{\alpha}+k^{3/2}}$$ I would greatly appreciate it if in the answers you can include all the details about the theorems used (since there is a high importance for the arguments for why we can do what we do in each step, and I don't understand the material good enough to be able to understand that some step is actually not trivial and uses some theorem)",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
91,Convergence of a Recursive Sequence,Convergence of a Recursive Sequence,,"A friend gave me the following problem: Let $c$ be any positive real number.  Define a sequence recursively by   $$a_0=c,\;\text{and }\; a_n=c^{a_{n-1}}\;\text{for }\;n=1,2,\ldots$$   For what values of $c$ does this sequence converge? The problem is trickier than it first seems, as I believe there are values $c>1$ for which the sequence converges, and also values $0<c<1$ for which the sequence diverges.  This is supposed to be able to be solved by a first year calculus student, so elementary methods are preferred. A followup question: Suppose the answer to the first question is some set $D\subset\mathbb{R}^+$.  Then we have a well-defined function $f:D\rightarrow\mathbb{R}$ given by $f(c)=L_c$ where $L_c$ is the limit of the sequence defined above.  Is $f$ continuous?  Is $f$ differentiable? Thanks!","A friend gave me the following problem: Let $c$ be any positive real number.  Define a sequence recursively by   $$a_0=c,\;\text{and }\; a_n=c^{a_{n-1}}\;\text{for }\;n=1,2,\ldots$$   For what values of $c$ does this sequence converge? The problem is trickier than it first seems, as I believe there are values $c>1$ for which the sequence converges, and also values $0<c<1$ for which the sequence diverges.  This is supposed to be able to be solved by a first year calculus student, so elementary methods are preferred. A followup question: Suppose the answer to the first question is some set $D\subset\mathbb{R}^+$.  Then we have a well-defined function $f:D\rightarrow\mathbb{R}$ given by $f(c)=L_c$ where $L_c$ is the limit of the sequence defined above.  Is $f$ continuous?  Is $f$ differentiable? Thanks!",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
92,Locally or Globally Lipschitz-functions,Locally or Globally Lipschitz-functions,,"Determine if the following function satisfies a local or a a uniform Lipschitz condition. The definition of locally Lipschitz and globally lipschitz are as follows: (i) We say that f is (uniformly) Lipschitz with respect to y, or simply in y, on    $A\subset U$ if there is a constant L such that $||f(t,x)-f(t,y)||\leq L ||x-y||$, whenever $(t,x)$ and $(t,y)$ are in A. (ii) f is locally Lipschitz in y if for every $(t_0, y_0) \in (c,d) \times U$, there exists a neighborhood V of $(t_0, y_0)$, (i.e. $V = \{f(t,y) \in(c,d) \times U : ||t-t_0|<a \ \text{and} \ |y-y_0|\leq b\}$) and a constant K = K(V) such that $||f(t,x)-f(t,y)||\leq K||x-y||$ for any $(t,x),(t,y) \in V$ 1) $t^2|y|$. So for this problem, I did the following: $|f(t,x)-f(t,y)|=|(t^2|x|-t^2|y|)|=t^2||x|-|y||\leq t^2|x-y|$. This shows from the second definition that this function is locally lipschitz because as we vary t, our bound changes (the $t^2$ is the K from the definition). Is this correct? How would I go about showing that this is\is not uniformly lipschitz? 2) $\frac{y}{1+y^2+t^2}$ For this one, I tried to follow a similar method. $|\frac{x}{1+x^2+t^2}-\frac{y}{1+y^2+t^2}|=|\frac{t^2(x-y)-x^2y+x(y^2+1)-y}{(1+x^2+t^2)(1+y^2+t^2)}| \leq |t^2(x-y)-x^2y+x(y^2+1)-y|$. Then I got stuck.","Determine if the following function satisfies a local or a a uniform Lipschitz condition. The definition of locally Lipschitz and globally lipschitz are as follows: (i) We say that f is (uniformly) Lipschitz with respect to y, or simply in y, on    $A\subset U$ if there is a constant L such that $||f(t,x)-f(t,y)||\leq L ||x-y||$, whenever $(t,x)$ and $(t,y)$ are in A. (ii) f is locally Lipschitz in y if for every $(t_0, y_0) \in (c,d) \times U$, there exists a neighborhood V of $(t_0, y_0)$, (i.e. $V = \{f(t,y) \in(c,d) \times U : ||t-t_0|<a \ \text{and} \ |y-y_0|\leq b\}$) and a constant K = K(V) such that $||f(t,x)-f(t,y)||\leq K||x-y||$ for any $(t,x),(t,y) \in V$ 1) $t^2|y|$. So for this problem, I did the following: $|f(t,x)-f(t,y)|=|(t^2|x|-t^2|y|)|=t^2||x|-|y||\leq t^2|x-y|$. This shows from the second definition that this function is locally lipschitz because as we vary t, our bound changes (the $t^2$ is the K from the definition). Is this correct? How would I go about showing that this is\is not uniformly lipschitz? 2) $\frac{y}{1+y^2+t^2}$ For this one, I tried to follow a similar method. $|\frac{x}{1+x^2+t^2}-\frac{y}{1+y^2+t^2}|=|\frac{t^2(x-y)-x^2y+x(y^2+1)-y}{(1+x^2+t^2)(1+y^2+t^2)}| \leq |t^2(x-y)-x^2y+x(y^2+1)-y|$. Then I got stuck.",,"['real-analysis', 'ordinary-differential-equations']"
93,There isn't a sequence converging pointwise to this function.,There isn't a sequence converging pointwise to this function.,,"I'm trying to solve this question: Show that there isn't a sequence of continuous functions $f_n:[0,1]\to  \mathbb R$ converges pointwise to the function $f:[0,1]\to \mathbb R$   such that $f(x)=0$ for $x$ rational and $f(x)=1$, for $x$ irrational. Of course there isn't a sequence $f_n:[0,1]\to  \mathbb R$ converges uniformly to the function $f:[0,1]\to \mathbb R$ because $f$ is discontinuous, but when the convergence is pointwise? I need help here Thanks in advance","I'm trying to solve this question: Show that there isn't a sequence of continuous functions $f_n:[0,1]\to  \mathbb R$ converges pointwise to the function $f:[0,1]\to \mathbb R$   such that $f(x)=0$ for $x$ rational and $f(x)=1$, for $x$ irrational. Of course there isn't a sequence $f_n:[0,1]\to  \mathbb R$ converges uniformly to the function $f:[0,1]\to \mathbb R$ because $f$ is discontinuous, but when the convergence is pointwise? I need help here Thanks in advance",,"['real-analysis', 'sequences-and-series']"
94,Continuity of the (real) $\Gamma$ function.,Continuity of the (real)  function.,\Gamma,"Consider the real valued function $$\Gamma(x)=\int_0^{\infty}t^{x-1}e^{-t}dt$$ where the above integral means the Lebesgue integral with the Lebesgue measure in $\mathbb R$. The domain of the function is $\{x\in\mathbb R\,:\, x>0\}$, and now I'm trying to study the continuity. The function  $$t^{x-1}e^{-t}$$ is positive and bounded if $x\in[a,b]$, for $0<a<b$, so using the dominated convergence theorem in $[a,b]$, I have: $$\lim_{x\to x_0}\Gamma(x)=\lim_{x\to x_0}\int_0^{\infty}t^{x-1}e^{-t}dt=\int_0^{\infty}\lim_{x\to x_0}t^{x-1}e^{-t}dt=\Gamma(x_0)$$ Reassuming $\Gamma$ is continuous in every interval $[a,b]$; so can I conclude that $\Gamma$ is continuous on all its domain?","Consider the real valued function $$\Gamma(x)=\int_0^{\infty}t^{x-1}e^{-t}dt$$ where the above integral means the Lebesgue integral with the Lebesgue measure in $\mathbb R$. The domain of the function is $\{x\in\mathbb R\,:\, x>0\}$, and now I'm trying to study the continuity. The function  $$t^{x-1}e^{-t}$$ is positive and bounded if $x\in[a,b]$, for $0<a<b$, so using the dominated convergence theorem in $[a,b]$, I have: $$\lim_{x\to x_0}\Gamma(x)=\lim_{x\to x_0}\int_0^{\infty}t^{x-1}e^{-t}dt=\int_0^{\infty}\lim_{x\to x_0}t^{x-1}e^{-t}dt=\Gamma(x_0)$$ Reassuming $\Gamma$ is continuous in every interval $[a,b]$; so can I conclude that $\Gamma$ is continuous on all its domain?",,"['real-analysis', 'gamma-function', 'lebesgue-integral']"
95,Lebesgue integral and a parametrized family of functions,Lebesgue integral and a parametrized family of functions,,"The main advantages of the Lebesgue integration are due to the theorems that ""control"" the (pointwise) limit of a sequence of measurable functions. In particular, I refer to the monotone convergence theorem, the Fatou's lemma and  the dominated convergence theorem. Now let  $t\in\mathbb R$ be a real parameter; if $\{f_t\}$ is a family of real valued and measurable functions (clearly the index $t$ runs in an uncountable set), and we now that  $$\displaystyle \lim_{t\to t_0} f_t(x)=f(x),$$ what about the quantity $\int f\,d\mu$? In this context are there some theorems like the monotone convergence theorem, the Fatou's lemma and the dominated convergence theorem? Under which hypothesis can I assert that $$\int f\,d\mu=\lim_{t\to t_0}\int f_t\,d\mu\;\;\;\textrm{?}$$","The main advantages of the Lebesgue integration are due to the theorems that ""control"" the (pointwise) limit of a sequence of measurable functions. In particular, I refer to the monotone convergence theorem, the Fatou's lemma and  the dominated convergence theorem. Now let  $t\in\mathbb R$ be a real parameter; if $\{f_t\}$ is a family of real valued and measurable functions (clearly the index $t$ runs in an uncountable set), and we now that  $$\displaystyle \lim_{t\to t_0} f_t(x)=f(x),$$ what about the quantity $\int f\,d\mu$? In this context are there some theorems like the monotone convergence theorem, the Fatou's lemma and the dominated convergence theorem? Under which hypothesis can I assert that $$\int f\,d\mu=\lim_{t\to t_0}\int f_t\,d\mu\;\;\;\textrm{?}$$",,"['real-analysis', 'lebesgue-integral']"
96,Determining limit points and proving there are no more of them,Determining limit points and proving there are no more of them,,"Determine all limits points of the following sequences: $\displaystyle a_n=\begin{cases}2^{-n},&n\text{ even},\\3^{1/n},&n\text{ odd}.\end{cases}$ $\displaystyle b_n=n+\frac{2(-1)^nn^2+3}{2n+1}$ $\displaystyle c_n=\sin\left(\frac{n\pi}{4}\right)$ $\displaystyle d_n=\frac{n+1}{n}\cdot i^n$ Explain for each sequence why the specified points are limit points and why there are no other limit points except than the ones you determined. I am famous for doing everything way too difficult, so I would both like to know whether my results are correct and how to improve them considering length and comprehensibility. Furthermore I don't know how to prove that I found all limit points - any help/hints? For the proof that there is at least one limit point I use the Bolzano-Weierstrass theorem which states that every bounded sequence has at least one converging subsequence. Sequence 1 Just by looking at the sequence we see, that $|a_n|\le\sqrt{3}$ and therefore at least one converging subsequence with the related limit points does exist. We take subsequences $(a_{n_k})_{k\in\mathbb{N}_0}$ with $n_k=2k$ such that $\displaystyle(a_{2k})_{k\in\mathbb{N}}=\frac{1}{2^{2k}}$ and we know that this is a null sequence and $0$ is therefore one limit point. $n_k=2k+1$ such that $\displaystyle(a_{2k+1})_{k\in\mathbb{N}}=\sqrt[2k+1]{3}$ and we know that the $n$-th square root is convergent and therefore $\lim_{n\to\infty}\sqrt[n]{3}=1$ which is the second (and last) limit point. Sequence 2 We can definitely see, that $|b_n|\ge 0$ and is therefore bounded and there exsists at least one limit point. We do look once again at subsequences $(b_{n_k})_{k\in\mathbb{N}_0}$ with the following two cases: $n_k=2k$ such that we get $$2k+\frac{2(2k)^2+3}{2(2k)+1}=2k+\frac{8k^2+3}{4k+1}=\frac{16k^2+2k+3}{4k+1}\longrightarrow +\infty$$ which is no limit point. $n_k=2k+1$ such that $$2k+1+\frac{-2(2k+1)^2+3}{2(2k+1)+1}=2k+1+\frac{-8k^2-8k+1}{4k+3}=\frac{k(2+4/k)}{k(4+3/k)}\longrightarrow \frac{1}{2}$$ which is our one and only limit point for this sequence. Sequence 3 The basic properties of $\sin$ imply that $\displaystyle\left|\sin\left(\frac{n\pi}{4}\right)\right|\le 1$ and we should find some limit points. In fact we will find five of them. Assuming that we know that $\sin(0)=0$, $\cos(0)=1$, $\sin(\pi)=1$, $\cos(\pi)=-1$, $\sin(\pi/4)=1/\sqrt{2}$, $\sin(-\pi/4)=-1/\sqrt{2}$, $\sin(x+2k\pi)=\sin(x)$, $\cos(x+2k\pi)=\cos(x)$ and $\sin(x+\pi/2)=\cos(x)$ we again do look at subsequences $(c_{n_k})_{k\in\mathbb{N}_0}$ where we distinguisch between 8 cases: $n_k=8k:$ $$\sin\left(\frac{8k\pi}{4}\right)=\sin(2k\pi)=0$$ $n_k=8k+1:$ $$\sin\left(\frac{(8k+1)\pi}{4}\right)=\sin\left(\frac{\pi}{4}+2k\pi\right)=\sin\left(\frac{\pi}{4}\right)=\frac{1}{\sqrt{2}}$$ $n_k=8k+2:$ $$\sin\left(\frac{(8k+2)\pi}{4}\right)=\sin\left(\frac{(4k+1)\pi}{2}\right)=\cos(2k\pi)=\cos(0)=1$$ $n_k=8k+3:$ $$\sin\left(\frac{(8k+3)\pi}{4}\right)=\sin\left(\frac{3\pi}{4}+2k\pi\right)=\sin\left(\frac{3\pi}{4}\right)=\sin\left(\frac{\pi}{4}\right)=\frac{1}{\sqrt{2}}$$ $n_k=8k+4:$ $$\sin\left(\frac{(8k+4)\pi}{4}\right)=\sin\left((2k+1)\pi\right)=\sin(\pi)=0$$ $n_k=8k+5:$ $$\sin\left(\frac{(8k+5)\pi}{4}\right)=\sin\left(\frac{5\pi}{4}+2k\pi\right)=\sin\left(\frac{5\pi}{4}\right)=-\frac{1}{\sqrt{2}}$$ $n_k=8k+6:$ $$\sin\left(\frac{(8k+6)\pi}{4}\right)=\sin\left(\frac{(4k+3)\pi}{2}\right)=\sin\left(2k\pi+\frac{3\pi}{2}\right)=\sin\left(\frac{3\pi}{2}\right)=\cos(\pi)=-1$$ $n_k=8k+7:$ $$\sin\left(\frac{(8k+7)\pi}{4}\right)=\sin\left(-\frac{\pi}{4}+2k\pi\right)=\sin\left(-\frac{\pi}{4}\right)=-\frac{1}{\sqrt{2}}$$ Therefore we have five limit points: $0,\pm 1,\pm 1/\sqrt{2}$. Sequence 4 This sequence is apparently bounded by the unit circle and therefore we know that $|d_n|\ge 1$. We distinguish here four different cases: $\frac{4k+1}{4k}\cdot 1\longrightarrow 1$ $\frac{4k+2}{4k+1}\cdot i\longrightarrow i$ $\frac{4k+3}{4k+2}\cdot (-1)\longrightarrow (-1)$ $\frac{4k+4}{4k+3}\cdot (-i)\longrightarrow (-i)$ This leads to the result that each $i^k$ is a limit point.","Determine all limits points of the following sequences: $\displaystyle a_n=\begin{cases}2^{-n},&n\text{ even},\\3^{1/n},&n\text{ odd}.\end{cases}$ $\displaystyle b_n=n+\frac{2(-1)^nn^2+3}{2n+1}$ $\displaystyle c_n=\sin\left(\frac{n\pi}{4}\right)$ $\displaystyle d_n=\frac{n+1}{n}\cdot i^n$ Explain for each sequence why the specified points are limit points and why there are no other limit points except than the ones you determined. I am famous for doing everything way too difficult, so I would both like to know whether my results are correct and how to improve them considering length and comprehensibility. Furthermore I don't know how to prove that I found all limit points - any help/hints? For the proof that there is at least one limit point I use the Bolzano-Weierstrass theorem which states that every bounded sequence has at least one converging subsequence. Sequence 1 Just by looking at the sequence we see, that $|a_n|\le\sqrt{3}$ and therefore at least one converging subsequence with the related limit points does exist. We take subsequences $(a_{n_k})_{k\in\mathbb{N}_0}$ with $n_k=2k$ such that $\displaystyle(a_{2k})_{k\in\mathbb{N}}=\frac{1}{2^{2k}}$ and we know that this is a null sequence and $0$ is therefore one limit point. $n_k=2k+1$ such that $\displaystyle(a_{2k+1})_{k\in\mathbb{N}}=\sqrt[2k+1]{3}$ and we know that the $n$-th square root is convergent and therefore $\lim_{n\to\infty}\sqrt[n]{3}=1$ which is the second (and last) limit point. Sequence 2 We can definitely see, that $|b_n|\ge 0$ and is therefore bounded and there exsists at least one limit point. We do look once again at subsequences $(b_{n_k})_{k\in\mathbb{N}_0}$ with the following two cases: $n_k=2k$ such that we get $$2k+\frac{2(2k)^2+3}{2(2k)+1}=2k+\frac{8k^2+3}{4k+1}=\frac{16k^2+2k+3}{4k+1}\longrightarrow +\infty$$ which is no limit point. $n_k=2k+1$ such that $$2k+1+\frac{-2(2k+1)^2+3}{2(2k+1)+1}=2k+1+\frac{-8k^2-8k+1}{4k+3}=\frac{k(2+4/k)}{k(4+3/k)}\longrightarrow \frac{1}{2}$$ which is our one and only limit point for this sequence. Sequence 3 The basic properties of $\sin$ imply that $\displaystyle\left|\sin\left(\frac{n\pi}{4}\right)\right|\le 1$ and we should find some limit points. In fact we will find five of them. Assuming that we know that $\sin(0)=0$, $\cos(0)=1$, $\sin(\pi)=1$, $\cos(\pi)=-1$, $\sin(\pi/4)=1/\sqrt{2}$, $\sin(-\pi/4)=-1/\sqrt{2}$, $\sin(x+2k\pi)=\sin(x)$, $\cos(x+2k\pi)=\cos(x)$ and $\sin(x+\pi/2)=\cos(x)$ we again do look at subsequences $(c_{n_k})_{k\in\mathbb{N}_0}$ where we distinguisch between 8 cases: $n_k=8k:$ $$\sin\left(\frac{8k\pi}{4}\right)=\sin(2k\pi)=0$$ $n_k=8k+1:$ $$\sin\left(\frac{(8k+1)\pi}{4}\right)=\sin\left(\frac{\pi}{4}+2k\pi\right)=\sin\left(\frac{\pi}{4}\right)=\frac{1}{\sqrt{2}}$$ $n_k=8k+2:$ $$\sin\left(\frac{(8k+2)\pi}{4}\right)=\sin\left(\frac{(4k+1)\pi}{2}\right)=\cos(2k\pi)=\cos(0)=1$$ $n_k=8k+3:$ $$\sin\left(\frac{(8k+3)\pi}{4}\right)=\sin\left(\frac{3\pi}{4}+2k\pi\right)=\sin\left(\frac{3\pi}{4}\right)=\sin\left(\frac{\pi}{4}\right)=\frac{1}{\sqrt{2}}$$ $n_k=8k+4:$ $$\sin\left(\frac{(8k+4)\pi}{4}\right)=\sin\left((2k+1)\pi\right)=\sin(\pi)=0$$ $n_k=8k+5:$ $$\sin\left(\frac{(8k+5)\pi}{4}\right)=\sin\left(\frac{5\pi}{4}+2k\pi\right)=\sin\left(\frac{5\pi}{4}\right)=-\frac{1}{\sqrt{2}}$$ $n_k=8k+6:$ $$\sin\left(\frac{(8k+6)\pi}{4}\right)=\sin\left(\frac{(4k+3)\pi}{2}\right)=\sin\left(2k\pi+\frac{3\pi}{2}\right)=\sin\left(\frac{3\pi}{2}\right)=\cos(\pi)=-1$$ $n_k=8k+7:$ $$\sin\left(\frac{(8k+7)\pi}{4}\right)=\sin\left(-\frac{\pi}{4}+2k\pi\right)=\sin\left(-\frac{\pi}{4}\right)=-\frac{1}{\sqrt{2}}$$ Therefore we have five limit points: $0,\pm 1,\pm 1/\sqrt{2}$. Sequence 4 This sequence is apparently bounded by the unit circle and therefore we know that $|d_n|\ge 1$. We distinguish here four different cases: $\frac{4k+1}{4k}\cdot 1\longrightarrow 1$ $\frac{4k+2}{4k+1}\cdot i\longrightarrow i$ $\frac{4k+3}{4k+2}\cdot (-1)\longrightarrow (-1)$ $\frac{4k+4}{4k+3}\cdot (-i)\longrightarrow (-i)$ This leads to the result that each $i^k$ is a limit point.",,"['real-analysis', 'sequences-and-series', 'limits']"
97,Prove that every linear map on a finite dimensional vector space is continuous [duplicate],Prove that every linear map on a finite dimensional vector space is continuous [duplicate],,"This question already has answers here : ""Every linear mapping on a finite dimensional space is continuous"" (3 answers) Closed 8 years ago . Need to know how to prove that every linear map is continuous.  working with finite dimensional vector spaces. It's one of the problems I'm currently working on as revision for an upcoming test. I know that that a map is continuous if the preimage of any open set is open but can't seem to figure out the full proof.","This question already has answers here : ""Every linear mapping on a finite dimensional space is continuous"" (3 answers) Closed 8 years ago . Need to know how to prove that every linear map is continuous.  working with finite dimensional vector spaces. It's one of the problems I'm currently working on as revision for an upcoming test. I know that that a map is continuous if the preimage of any open set is open but can't seem to figure out the full proof.",,"['real-analysis', 'functional-analysis', 'continuity']"
98,Integral Inequality $|f''(x)/f(x)|$,Integral Inequality,|f''(x)/f(x)|,"Let $f$ be a $C^2$ function in $[0,1]$ such that $f(0)=f(1)=0$ and $f(x)\neq 0\,\forall x\in(0,1).$ Prove that $$\int_0^1 \left|\frac{f{''}(x)}{f(x)}\right|dx\ge4$$","Let $f$ be a $C^2$ function in $[0,1]$ such that $f(0)=f(1)=0$ and $f(x)\neq 0\,\forall x\in(0,1).$ Prove that $$\int_0^1 \left|\frac{f{''}(x)}{f(x)}\right|dx\ge4$$",,"['calculus', 'real-analysis', 'integration', 'inequality', 'functional-equations']"
99,Pre-image of a measurable set A is always measurable?,Pre-image of a measurable set A is always measurable?,,"Let $f$ be a continuous function on a set $E$.  Is it always true that $f^{-1}(A)$ is always measurable if $A$ is measurable? I say no.  We know that $\bar{\psi(x)}:=\frac{\phi(x)+x}{2}$ where $\phi$ is the Cantor (or Cantor-Lebesgue) function and $\bar{\psi}:[0,1] \rightarrow [0,1]$.  We know this function maps a measurable subset of $C$ (the Cantor set) into a non-measurable set $W$. A few observations about $\bar{\psi(x)}$.  It is strictly increasing and continuous, so it has a continuous inverse $\bar{\psi(x)}^{-1}$.  Thus $\bar{\psi(x)}^{-1}(W)=c \subset C$; that is, $\bar{\psi(x)}^{-1}(W)$ is non-measurable, but c is. Is this correct?","Let $f$ be a continuous function on a set $E$.  Is it always true that $f^{-1}(A)$ is always measurable if $A$ is measurable? I say no.  We know that $\bar{\psi(x)}:=\frac{\phi(x)+x}{2}$ where $\phi$ is the Cantor (or Cantor-Lebesgue) function and $\bar{\psi}:[0,1] \rightarrow [0,1]$.  We know this function maps a measurable subset of $C$ (the Cantor set) into a non-measurable set $W$. A few observations about $\bar{\psi(x)}$.  It is strictly increasing and continuous, so it has a continuous inverse $\bar{\psi(x)}^{-1}$.  Thus $\bar{\psi(x)}^{-1}(W)=c \subset C$; that is, $\bar{\psi(x)}^{-1}(W)$ is non-measurable, but c is. Is this correct?",,"['real-analysis', 'measure-theory']"
