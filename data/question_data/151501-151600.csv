,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Fourier tranform of the Euclidean norm,Fourier tranform of the Euclidean norm,,"where can I find the Fourier transform of the power of the Euclidean norm?, that is: $$\mathcal{F}[\|x\|^{p}](\omega) = \int_{\mathbb{R}^{d}}\exp(-2\pi i \langle\omega, x\rangle) \|x\|^{p} dx$$ Thank you!","where can I find the Fourier transform of the power of the Euclidean norm?, that is: $$\mathcal{F}[\|x\|^{p}](\omega) = \int_{\mathbb{R}^{d}}\exp(-2\pi i \langle\omega, x\rangle) \|x\|^{p} dx$$ Thank you!",,['analysis']
1,"Let the sequence of functions $f_n(x)$ equal $1$ if $x ∈ [n, n + 1)$ and $0$ otherwise. Why doesn't $f_n$ converge uniformly?",Let the sequence of functions  equal  if  and  otherwise. Why doesn't  converge uniformly?,"f_n(x) 1 x ∈ [n, n + 1) 0 f_n","Let the sequence of functions $f_n(x)$ equal $1$ if $x ∈ [n, n + 1)$ and $0$ otherwise. How can I use the deﬁnition of uniform convergence to show that $f_n$ does not converges uniformly? If a function is uniformly convergent, then for all $\epsilon > 0$ there exists an integer $N$ such that for all $n \geq N$ adn for all $x$, $|f_n(x) - f(x)| < \epsilon$.  Therefore we wish to find some $\epsilon > 0$ such that for each $n$ there is an $x ∈ [n, n + 1)$ such that $|f_n(x) - f(x)| \geq \epsilon$.  But how can I do this ?","Let the sequence of functions $f_n(x)$ equal $1$ if $x ∈ [n, n + 1)$ and $0$ otherwise. How can I use the deﬁnition of uniform convergence to show that $f_n$ does not converges uniformly? If a function is uniformly convergent, then for all $\epsilon > 0$ there exists an integer $N$ such that for all $n \geq N$ adn for all $x$, $|f_n(x) - f(x)| < \epsilon$.  Therefore we wish to find some $\epsilon > 0$ such that for each $n$ there is an $x ∈ [n, n + 1)$ such that $|f_n(x) - f(x)| \geq \epsilon$.  But how can I do this ?",,"['real-analysis', 'analysis', 'uniform-convergence']"
2,completion of $C^{\infty}\left(S\right)$ is $L^2(S)$?,completion of  is ?,C^{\infty}\left(S\right) L^2(S),"I have the space of infinitely derivable functions, i.e. $C^{\infty}\left(S\right)$ with the following inner product $$\left\langle f,g\right\rangle =\intop_{0}^{2\pi}\intop_{0}^{2\pi}\left(\overline{f\left(\theta,\varphi\right)}g\left(\theta,\varphi\right)\sin\theta\right)d\theta d\varphi,$$ I would like to understand what does it mean that the completion of this space is $L^2(S)$? Edit : If it's possible I would also like to understand why $L^2(S)$ is the completion of $C^{\infty}\left(S\right)$ with this inner product. I found that this might be a quite general statement so I'd like to know if there's a theorem of some kind abou it. Thanks","I have the space of infinitely derivable functions, i.e. $C^{\infty}\left(S\right)$ with the following inner product $$\left\langle f,g\right\rangle =\intop_{0}^{2\pi}\intop_{0}^{2\pi}\left(\overline{f\left(\theta,\varphi\right)}g\left(\theta,\varphi\right)\sin\theta\right)d\theta d\varphi,$$ I would like to understand what does it mean that the completion of this space is $L^2(S)$? Edit : If it's possible I would also like to understand why $L^2(S)$ is the completion of $C^{\infty}\left(S\right)$ with this inner product. I found that this might be a quite general statement so I'd like to know if there's a theorem of some kind abou it. Thanks",,"['functional-analysis', 'analysis', 'hilbert-spaces', 'banach-spaces']"
3,Interval is homeomorphic to a circumference less a point,Interval is homeomorphic to a circumference less a point,,"Let $f:\left(-1,1\right)\longrightarrow \Bbb S$ \ $\lbrace-1\rbrace$ be a map defined by $t\mapsto e^{i\pi t}$ where $\Bbb S=\lbrace z\in\Bbb C: \|z\| = 1\rbrace$ Is $f$ an homeomorphism? I proved that $f$ is continuous function and bijective and  $f^{-1}:\Bbb S$ \ $\lbrace-1\rbrace\longrightarrow\left(-1,1\right)$ it´s $$f^{-1}\left(z\right)=f^{-1}\left(e^{i\theta}\right)=\frac{\theta}{\pi} \text{ if } \theta\in\left[0,\pi\right) \text{ or } \frac{\theta-2\pi}{\pi} \text{ if } \theta\in \left(\pi,2\pi\right)$$ Can You help me please?","Let $f:\left(-1,1\right)\longrightarrow \Bbb S$ \ $\lbrace-1\rbrace$ be a map defined by $t\mapsto e^{i\pi t}$ where $\Bbb S=\lbrace z\in\Bbb C: \|z\| = 1\rbrace$ Is $f$ an homeomorphism? I proved that $f$ is continuous function and bijective and  $f^{-1}:\Bbb S$ \ $\lbrace-1\rbrace\longrightarrow\left(-1,1\right)$ it´s $$f^{-1}\left(z\right)=f^{-1}\left(e^{i\theta}\right)=\frac{\theta}{\pi} \text{ if } \theta\in\left[0,\pi\right) \text{ or } \frac{\theta-2\pi}{\pi} \text{ if } \theta\in \left(\pi,2\pi\right)$$ Can You help me please?",,"['real-analysis', 'general-topology', 'analysis']"
4,Are these two metric spaces complete?,Are these two metric spaces complete?,,"Let $J$ be an arbitrary non-empty set of indices, and let $\mathbb{R}^J$ denote the set of all the $J$-tuples of real numbers (i.e. the set of all the functions $x \colon J \to \mathbb{R}$, where we denote $x(\alpha)$ by $x_\alpha$, for each $\alpha \in J$). Now let the function $\tilde{\rho} \colon \mathbb{R}^J \times \mathbb{R}^J \to \mathbb{R}$ be defined as follows:  $$ \tilde{\rho}( x, y) \colon= \sup \left\{ \ \min \left\{ \ \vert x_\alpha - y_\alpha \vert, \ 1 \ \right\} \ \colon \ \alpha \in J \ \right\} \ \mbox{ for all } \ x \colon= \left(x_\alpha \right)_{\alpha \in J}, \ y \colon= \left(y_\alpha \right)_{\alpha \in J} \in \mathbb{R}^J.$$ This function $\tilde{\rho}$ is of course a metric on $\mathbb{R}^J$, called the uniform metric. Is the metric space $\left( \ \mathbb{R}^J, \  \tilde{\rho} \ \right)$ complete? My effort: Let $\left( x_n \right)_{n \in \mathbb{N}}$ be a Cauchy sequence in $\left( \ \mathbb{R}^J, \  \tilde{\rho} \ \right)$, and let $x_n \colon= \left( x_{\alpha n} \right)_{\alpha \in J}$, for each $n = 1, 2, 3, \ldots$. Then, given any real number $\epsilon > 0$, we can find a natural number $N = N(\epsilon)$ such that  $$ \tilde{\rho}(x_m, x_n ) < \epsilon \ \mbox{ for all } \ m, n \in \mathbb{N} \ \mbox{ such that } \ m> N \ \mbox{ and } \ n> N. $$ Let's assume that $0< \epsilon < 1$. Let $\beta \in J$. Then for all $m, n \in \mathbb{N}$ such that $m > N$ and $n > N$, we have  $$\epsilon > \tilde{\rho}(x_m, x_n) \geq  \min \left\{ \  \left\vert x_{\beta m} - x_{\beta n} \right\vert, \ 1 \ \right\} = \left\vert x_{\beta m} - x_{\beta n} \right\vert,$$ which shows that the sequence $\left(x_{\beta n} \right)_{n \in \mathbb{N}}$ is a Cauchy sequence in $\mathbb{R}$, and so this sequence converges to some real number $x_\beta$. Now since our $\beta \in J$ was arbitrary, let us define $x \colon= \left( x_\alpha \right)_{\alpha \in J}$. Is this logic correct so far? If so, then how to proceed from here, especially if the index set  $J$ is infinite? Now let's take $J$ to be the set $\mathbb{N}$ of natural numbers, and let's denote $\mathbb{R}^J$ by $\mathbb{R}^\omega$ and define the function $D \colon \mathbb{R}^\omega \times \mathbb{R}^\omega \to \mathbb{R}$ by the formula $$ D(x, y) \colon = \sup \left\{ \frac{ \min \left\{ \ \left\vert x_\alpha - y_\alpha  \right\vert, \ 1 \ \right\}}{\alpha } \ \colon \ \alpha  \in \mathbb{N} \ \right\} \ \mbox{ for all } \ x \colon= \left( x_\alpha  \right)_{\alpha \in \mathbb{N}}, \ y \colon= \left( y_\alpha \right)_{ \alpha \in \mathbb{N}} \in \mathbb{R}^\omega.$$ Then $D$ is also a metric on $\mathbb{R}^\omega$, and the topology induced by $D$ is the same as the product topology on $\mathbb{R}^\omega$. Is the metric space $\left( \ \mathbb{R}^\omega, \ D \ \right)$ complete? Let $\left(x_n \right)_{n \in \mathbb{N}}$, where $x_n \colon= \left( x_{\alpha n} \right)_{\alpha \in \mathbb{N}}$ for each $n \in \mathbb{N}$, be a Cauchy sequence in $\left( \ \mathbb{R}^\omega, \ D \ \right)$. Let $\beta$ be a fixed natural number. Then, given any real number $\epsilon > 0$, we can find a natural number $N = N(\epsilon)$ such that $$ D(x_m, x_n) < \frac{ \epsilon}{ \beta} \ \mbox{ for all } \ m, n \in \mathbb{N} \ \mbox{ such that } \ m > N \ \mbox{ and } \  n > N. $$ Let's assume that $0 < \epsilon < 1$. Then for all natural numbers $m$ and $n$ such that $m, n > N$, we have  $$\left\vert x_{\beta m} - x_{\beta n} \right\vert < \epsilon,$$ from which it follows that the sequence $\left( x_{\beta n} \right)_{ n \in \mathbb{N}}$ is a Cauchy sequence of real numbers and therefore this sequence converges to some real number $x_\beta$. Since our $\beta \in \mathbb{N}$ was arbitrary, we can define an element $x \in \mathbb{R}^\omega$ as $$x \colon= \left( x_\alpha \right)_{\alpha \in \mathbb{N}}.$$ Is my logic correct so far? If so, then how to proceed from here? What if, in either of the above two cases, we replace $\mathbb{R}$ by an arbitrary complete metric space (and replace $\vert \cdot \vert$ by the corresponding distance of course)? Do we still have a metric in either case? And, if so, is (are) this (these) metric(s) complete?","Let $J$ be an arbitrary non-empty set of indices, and let $\mathbb{R}^J$ denote the set of all the $J$-tuples of real numbers (i.e. the set of all the functions $x \colon J \to \mathbb{R}$, where we denote $x(\alpha)$ by $x_\alpha$, for each $\alpha \in J$). Now let the function $\tilde{\rho} \colon \mathbb{R}^J \times \mathbb{R}^J \to \mathbb{R}$ be defined as follows:  $$ \tilde{\rho}( x, y) \colon= \sup \left\{ \ \min \left\{ \ \vert x_\alpha - y_\alpha \vert, \ 1 \ \right\} \ \colon \ \alpha \in J \ \right\} \ \mbox{ for all } \ x \colon= \left(x_\alpha \right)_{\alpha \in J}, \ y \colon= \left(y_\alpha \right)_{\alpha \in J} \in \mathbb{R}^J.$$ This function $\tilde{\rho}$ is of course a metric on $\mathbb{R}^J$, called the uniform metric. Is the metric space $\left( \ \mathbb{R}^J, \  \tilde{\rho} \ \right)$ complete? My effort: Let $\left( x_n \right)_{n \in \mathbb{N}}$ be a Cauchy sequence in $\left( \ \mathbb{R}^J, \  \tilde{\rho} \ \right)$, and let $x_n \colon= \left( x_{\alpha n} \right)_{\alpha \in J}$, for each $n = 1, 2, 3, \ldots$. Then, given any real number $\epsilon > 0$, we can find a natural number $N = N(\epsilon)$ such that  $$ \tilde{\rho}(x_m, x_n ) < \epsilon \ \mbox{ for all } \ m, n \in \mathbb{N} \ \mbox{ such that } \ m> N \ \mbox{ and } \ n> N. $$ Let's assume that $0< \epsilon < 1$. Let $\beta \in J$. Then for all $m, n \in \mathbb{N}$ such that $m > N$ and $n > N$, we have  $$\epsilon > \tilde{\rho}(x_m, x_n) \geq  \min \left\{ \  \left\vert x_{\beta m} - x_{\beta n} \right\vert, \ 1 \ \right\} = \left\vert x_{\beta m} - x_{\beta n} \right\vert,$$ which shows that the sequence $\left(x_{\beta n} \right)_{n \in \mathbb{N}}$ is a Cauchy sequence in $\mathbb{R}$, and so this sequence converges to some real number $x_\beta$. Now since our $\beta \in J$ was arbitrary, let us define $x \colon= \left( x_\alpha \right)_{\alpha \in J}$. Is this logic correct so far? If so, then how to proceed from here, especially if the index set  $J$ is infinite? Now let's take $J$ to be the set $\mathbb{N}$ of natural numbers, and let's denote $\mathbb{R}^J$ by $\mathbb{R}^\omega$ and define the function $D \colon \mathbb{R}^\omega \times \mathbb{R}^\omega \to \mathbb{R}$ by the formula $$ D(x, y) \colon = \sup \left\{ \frac{ \min \left\{ \ \left\vert x_\alpha - y_\alpha  \right\vert, \ 1 \ \right\}}{\alpha } \ \colon \ \alpha  \in \mathbb{N} \ \right\} \ \mbox{ for all } \ x \colon= \left( x_\alpha  \right)_{\alpha \in \mathbb{N}}, \ y \colon= \left( y_\alpha \right)_{ \alpha \in \mathbb{N}} \in \mathbb{R}^\omega.$$ Then $D$ is also a metric on $\mathbb{R}^\omega$, and the topology induced by $D$ is the same as the product topology on $\mathbb{R}^\omega$. Is the metric space $\left( \ \mathbb{R}^\omega, \ D \ \right)$ complete? Let $\left(x_n \right)_{n \in \mathbb{N}}$, where $x_n \colon= \left( x_{\alpha n} \right)_{\alpha \in \mathbb{N}}$ for each $n \in \mathbb{N}$, be a Cauchy sequence in $\left( \ \mathbb{R}^\omega, \ D \ \right)$. Let $\beta$ be a fixed natural number. Then, given any real number $\epsilon > 0$, we can find a natural number $N = N(\epsilon)$ such that $$ D(x_m, x_n) < \frac{ \epsilon}{ \beta} \ \mbox{ for all } \ m, n \in \mathbb{N} \ \mbox{ such that } \ m > N \ \mbox{ and } \  n > N. $$ Let's assume that $0 < \epsilon < 1$. Then for all natural numbers $m$ and $n$ such that $m, n > N$, we have  $$\left\vert x_{\beta m} - x_{\beta n} \right\vert < \epsilon,$$ from which it follows that the sequence $\left( x_{\beta n} \right)_{ n \in \mathbb{N}}$ is a Cauchy sequence of real numbers and therefore this sequence converges to some real number $x_\beta$. Since our $\beta \in \mathbb{N}$ was arbitrary, we can define an element $x \in \mathbb{R}^\omega$ as $$x \colon= \left( x_\alpha \right)_{\alpha \in \mathbb{N}}.$$ Is my logic correct so far? If so, then how to proceed from here? What if, in either of the above two cases, we replace $\mathbb{R}$ by an arbitrary complete metric space (and replace $\vert \cdot \vert$ by the corresponding distance of course)? Do we still have a metric in either case? And, if so, is (are) this (these) metric(s) complete?",,"['real-analysis', 'general-topology', 'analysis', 'metric-spaces', 'complete-spaces']"
5,Question about the Jacobian of a function,Question about the Jacobian of a function,,"Let $f:U\rightarrow V$ , $U$ and $V$ open subsets of $\mathbb{R}^2$, be a smooth function. Let $Jf_p$ be the jacobian of $f$ in the point $p\in U$ and set $M_p:=\sup\{|df_pv|:\|v\|=1\}$ and $m_p:=\inf\{|df_pv|:\|v\|=1\}$. If fear I'm missing something quite basic, but I can't understand why is it true $Jf_p=M_pm_p$. Can you give some hints? Thank you!","Let $f:U\rightarrow V$ , $U$ and $V$ open subsets of $\mathbb{R}^2$, be a smooth function. Let $Jf_p$ be the jacobian of $f$ in the point $p\in U$ and set $M_p:=\sup\{|df_pv|:\|v\|=1\}$ and $m_p:=\inf\{|df_pv|:\|v\|=1\}$. If fear I'm missing something quite basic, but I can't understand why is it true $Jf_p=M_pm_p$. Can you give some hints? Thank you!",,"['real-analysis', 'analysis', 'differential-geometry', 'analytic-geometry']"
6,relations between the derivatives of second rank,relations between the derivatives of second rank,,"I have some questions while learning about partial derivatives, I didn't found answers neither in books I have nor in the internet. Thanks in advance for your help. The well-known theorem states that if $f:R^2\to R$ has continuous $\frac{\partial^2f}{\partial x\partial y}$ and $\frac{\partial^2f}{\partial y\partial x}$, then $\frac{\partial^2f}{\partial x\partial y}=\frac{\partial^2f}{\partial y\partial x}$. But is there any connection between $\frac{\partial^2f}{\partial x^2}$ and $\frac{\partial^2f}{\partial y^2}$ and the mixed derivatives? I mean does any of basic informations (existence, continuity) about $\frac{\partial^2f}{\partial x^2}$ and $\frac{\partial^2f}{\partial y^2}$ imply the same about mixed derivatives? So the questions are: does the existence of $\frac{\partial^2f}{\partial x^2}$ and $\frac{\partial^2f}{\partial y^2}$ imply the existence of $\frac{\partial^2f}{\partial x\partial y}$ and $\frac{\partial^2f}{\partial y\partial x}$ ? does the continuity of $\frac{\partial^2f}{\partial x^2}$ and $\frac{\partial^2f}{\partial y^2}$ imply the continuity of $\frac{\partial^2f}{\partial x\partial y}$ and $\frac{\partial^2f}{\partial y\partial x}$ ?","I have some questions while learning about partial derivatives, I didn't found answers neither in books I have nor in the internet. Thanks in advance for your help. The well-known theorem states that if $f:R^2\to R$ has continuous $\frac{\partial^2f}{\partial x\partial y}$ and $\frac{\partial^2f}{\partial y\partial x}$, then $\frac{\partial^2f}{\partial x\partial y}=\frac{\partial^2f}{\partial y\partial x}$. But is there any connection between $\frac{\partial^2f}{\partial x^2}$ and $\frac{\partial^2f}{\partial y^2}$ and the mixed derivatives? I mean does any of basic informations (existence, continuity) about $\frac{\partial^2f}{\partial x^2}$ and $\frac{\partial^2f}{\partial y^2}$ imply the same about mixed derivatives? So the questions are: does the existence of $\frac{\partial^2f}{\partial x^2}$ and $\frac{\partial^2f}{\partial y^2}$ imply the existence of $\frac{\partial^2f}{\partial x\partial y}$ and $\frac{\partial^2f}{\partial y\partial x}$ ? does the continuity of $\frac{\partial^2f}{\partial x^2}$ and $\frac{\partial^2f}{\partial y^2}$ imply the continuity of $\frac{\partial^2f}{\partial x\partial y}$ and $\frac{\partial^2f}{\partial y\partial x}$ ?",,"['analysis', 'derivatives']"
7,$L^p \subset L^q$ for $p\neq q$.,for .,L^p \subset L^q p\neq q,"Let $1\leq p \leq q \leq \infty$. It's well known that on a finite measure space $(X,\mathcal{M}, \mu)$, we have the inclusion $L^q(X,\mathcal{M}, \mu) \subset L^p(X,\mathcal{M}, \mu)$. Questions regarding this have already been addressed ( $L^p$ and $L^q$ space inclusion ). But we have also have special cases, for instance if $\mu$ is the counting measure then $L^p(X,\mathcal{M}, \mu) \subset L^q(X,\mathcal{M}, \mu)$. On this site I also found this: When $L^p \subset L^q$ for $p <q$. . So here's my question: what are some other cases of $L^p$ space inclusion? Also, what is the nature of this inclusion? Are there examples/criteria where $L^p$ is, say, open, closed (complete), dense, compact, etc. in $L^q$ for $p\neq q$. Of course, here we're viewing $L^p$ functions with the $L^q$ norm.","Let $1\leq p \leq q \leq \infty$. It's well known that on a finite measure space $(X,\mathcal{M}, \mu)$, we have the inclusion $L^q(X,\mathcal{M}, \mu) \subset L^p(X,\mathcal{M}, \mu)$. Questions regarding this have already been addressed ( $L^p$ and $L^q$ space inclusion ). But we have also have special cases, for instance if $\mu$ is the counting measure then $L^p(X,\mathcal{M}, \mu) \subset L^q(X,\mathcal{M}, \mu)$. On this site I also found this: When $L^p \subset L^q$ for $p <q$. . So here's my question: what are some other cases of $L^p$ space inclusion? Also, what is the nature of this inclusion? Are there examples/criteria where $L^p$ is, say, open, closed (complete), dense, compact, etc. in $L^q$ for $p\neq q$. Of course, here we're viewing $L^p$ functions with the $L^q$ norm.",,"['real-analysis', 'analysis', 'functional-analysis', 'lp-spaces']"
8,Why is a $\sigma$-algebra defined as such?,Why is a -algebra defined as such?,\sigma,"We know that a $\sigma$-algebra is a collection of sets closed under countable set operations. My question is: how was it determined that this is the right collection? i.e., how was it determined that these are the sets that reasonably should be measurable. As an analogue, we know that the definition of a topology comes from the behaviour of the collection of open sets in $\mathbb R^n$, and the fact that continuity may be rephrased as a statement about open sets. That is, the generalization of taking the essential property of behaviour of open sets and continuity is intuitive. I've never heard nor read an explanation in the same spirit of this one for a $\sigma$-algebra. I'd assume in the same sense, this comes from what subsets of $\mathbb R^n$ we should expect to be Lebesgue Measurable , and then generalizing to arbitrary measures. Or Is there some way to show this is the maximal collection of sets for any measure? That is, does the definition of a $\sigma$-algebra come from some maximality property, or does it come from some generalization of Lebesgue Measurable Sets? I've done a bit of research on the topic and come up short of an answer, so any discussion of the subject would be appreciated.","We know that a $\sigma$-algebra is a collection of sets closed under countable set operations. My question is: how was it determined that this is the right collection? i.e., how was it determined that these are the sets that reasonably should be measurable. As an analogue, we know that the definition of a topology comes from the behaviour of the collection of open sets in $\mathbb R^n$, and the fact that continuity may be rephrased as a statement about open sets. That is, the generalization of taking the essential property of behaviour of open sets and continuity is intuitive. I've never heard nor read an explanation in the same spirit of this one for a $\sigma$-algebra. I'd assume in the same sense, this comes from what subsets of $\mathbb R^n$ we should expect to be Lebesgue Measurable , and then generalizing to arbitrary measures. Or Is there some way to show this is the maximal collection of sets for any measure? That is, does the definition of a $\sigma$-algebra come from some maximality property, or does it come from some generalization of Lebesgue Measurable Sets? I've done a bit of research on the topic and come up short of an answer, so any discussion of the subject would be appreciated.",,"['real-analysis', 'general-topology', 'analysis', 'measure-theory', 'math-history']"
9,Compute Gradient from Jacobian,Compute Gradient from Jacobian,,"I have some trouble understanding a formula from a report : https://www.samba.org/tridge/UAV/madgwick_internal_report.pdf It is formula (20) (Page 7). Could you tell me where it comes from?  I can't find anything that resembles in litterature... Here is an image of the formula: http://www.les-mathematiques.net/phorum/addon.php?4,module=embed_images,url=http%3A%2F%2Fs21.postimg.org%2Ft89ej68k7%2Fpourlesmaths.png The author claims that ""Equation (20) computes the gradient of the solution surface defined by the objective function and its Jacobian""and I don't even understand what he means by gradient since f is a function that goes from R^4 into R^3. Thanks in advance for your answer","I have some trouble understanding a formula from a report : https://www.samba.org/tridge/UAV/madgwick_internal_report.pdf It is formula (20) (Page 7). Could you tell me where it comes from?  I can't find anything that resembles in litterature... Here is an image of the formula: http://www.les-mathematiques.net/phorum/addon.php?4,module=embed_images,url=http%3A%2F%2Fs21.postimg.org%2Ft89ej68k7%2Fpourlesmaths.png The author claims that ""Equation (20) computes the gradient of the solution surface defined by the objective function and its Jacobian""and I don't even understand what he means by gradient since f is a function that goes from R^4 into R^3. Thanks in advance for your answer",,"['analysis', 'vector-analysis']"
10,$C^{\infty}_{loc}$-convergence,-convergence,C^{\infty}_{loc},"Let $\Omega \subset \mathbb{R}^{n}$ be some open set. Let $f_{n},f\in C^{\infty}(\Omega)$. My question is: What does the following phrase mean? $f_{n}$ converges to $f$ in $C^{\infty}_{loc}(\Omega)$. What is the exact definition of such a convergence.","Let $\Omega \subset \mathbb{R}^{n}$ be some open set. Let $f_{n},f\in C^{\infty}(\Omega)$. My question is: What does the following phrase mean? $f_{n}$ converges to $f$ in $C^{\infty}_{loc}(\Omega)$. What is the exact definition of such a convergence.",,"['real-analysis', 'analysis', 'functional-analysis', 'functions']"
11,"With regards to a comment on Problem 5.14 from Evans PDE, absolute values.","With regards to a comment on Problem 5.14 from Evans PDE, absolute values.",,"See here . Problem 5.14, Evans. Let $U$ be bounded with a $C^1$ boundary. Show that a ''typical''   function $u \in L^p(U) \ (1 \leq p < \infty)$ does not have a trace on   $\partial U$. More precisely, prove there does not exist a bounded   linear operator \begin{equation} T:L^p(U) \to L^p(\partial U) \end{equation} such that $Tu = \left. u \right|_{\partial U}$ whenever $u \in C(\overline{U}) \cap L^p(U)$. In the link, Sally offers the following solution. For completeness, I'll expand the idea in this comment . The construction  does not require $C^1$ boundary, and works in every bounded domain. Let    $$ u_n(x) = (1-n\operatorname{dist}(x,\partial U))^+$$   which is a continuous function on $\overline{U}$. Since the sequence $u_n^p$ is decreasing, it is dominated by $u_1^p$, which is integrable. Hence    $$ \lim_{n\to\infty}\int_U u_n^p = \int_U \lim_{n\to\infty} u_n^p = 0 $$    On the other hand, $$\int_{\partial U}u_n^p = \int_{\partial U}1\not\to0$$   which yields the claim. Cookie comments the following in response, which didn't get a response back. And we shouldn't use absolute values either because if $\text{dist}(x, \partial U) > {1\over n}$, the function $u_n$ would be positive due to the absolute value and not $0$. If we just use the ""$+$"" like you introduced, then the negative part of the function $u_n$ would just simply be $0$. I hope that it is correct. Could anyone help resolve this issue?","See here . Problem 5.14, Evans. Let $U$ be bounded with a $C^1$ boundary. Show that a ''typical''   function $u \in L^p(U) \ (1 \leq p < \infty)$ does not have a trace on   $\partial U$. More precisely, prove there does not exist a bounded   linear operator \begin{equation} T:L^p(U) \to L^p(\partial U) \end{equation} such that $Tu = \left. u \right|_{\partial U}$ whenever $u \in C(\overline{U}) \cap L^p(U)$. In the link, Sally offers the following solution. For completeness, I'll expand the idea in this comment . The construction  does not require $C^1$ boundary, and works in every bounded domain. Let    $$ u_n(x) = (1-n\operatorname{dist}(x,\partial U))^+$$   which is a continuous function on $\overline{U}$. Since the sequence $u_n^p$ is decreasing, it is dominated by $u_1^p$, which is integrable. Hence    $$ \lim_{n\to\infty}\int_U u_n^p = \int_U \lim_{n\to\infty} u_n^p = 0 $$    On the other hand, $$\int_{\partial U}u_n^p = \int_{\partial U}1\not\to0$$   which yields the claim. Cookie comments the following in response, which didn't get a response back. And we shouldn't use absolute values either because if $\text{dist}(x, \partial U) > {1\over n}$, the function $u_n$ would be positive due to the absolute value and not $0$. If we just use the ""$+$"" like you introduced, then the negative part of the function $u_n$ would just simply be $0$. I hope that it is correct. Could anyone help resolve this issue?",,"['real-analysis', 'analysis', 'functional-analysis', 'partial-differential-equations', 'lp-spaces']"
12,"How to prove that for all $k\in\mathbb N$, $h(kx)=kh(x)$ and $h(x+y)\le h(x)+h(y)$?","How to prove that for all ,  and ?",k\in\mathbb N h(kx)=kh(x) h(x+y)\le h(x)+h(y),"Suppose $X$ is a commutative monoid and $f:X\to\mathbb R\cup\{\infty\}$ a function and $$g(x)=\inf\left\{\sum_{i=1}^nf(x_i)~\middle\vert~\sum_{i=1}^nx_i=x,n\in\mathbb N\right\}$$ $$h(x)=\inf\left\{\frac{g(mx)}m ~\middle\vert~ m\in\mathbb N\right\}$$ then how to prove that for all $k\in\mathbb N$, $h(kx)=kh(x)$ and $h(x+y)\le h(x)+h(y)$? I proved that $g(x+y)\le g(x)+g(y)$, so I can write $$h(kx)=\inf\left\{\frac{g(mkx)}m ~\middle\vert~ m\in\mathbb N\right\}=k\inf\left\{\frac{g(mkx)}{mk} ~\middle\vert~ m\in\mathbb N\right\}$$ since $g(x+y)\le g(x)+g(y)$, we have $\frac{g(mkx)}{mk}\le\frac{kg(mx)}{mk}$ So, $$h(kx)\le k\frac{g(mx)}m$$ therefore $$h(kx)\le kh(x)$$ on the other hand $$kh(x)=\inf\left\{\frac{kg(mx)}m ~\middle\vert~ m\in\mathbb N\right\}$$ and since $\frac{kg(mx)}m\le\frac{km}mg(x)$ we have $$kh(x)\le kg(x)$$ Any hint to continue?","Suppose $X$ is a commutative monoid and $f:X\to\mathbb R\cup\{\infty\}$ a function and $$g(x)=\inf\left\{\sum_{i=1}^nf(x_i)~\middle\vert~\sum_{i=1}^nx_i=x,n\in\mathbb N\right\}$$ $$h(x)=\inf\left\{\frac{g(mx)}m ~\middle\vert~ m\in\mathbb N\right\}$$ then how to prove that for all $k\in\mathbb N$, $h(kx)=kh(x)$ and $h(x+y)\le h(x)+h(y)$? I proved that $g(x+y)\le g(x)+g(y)$, so I can write $$h(kx)=\inf\left\{\frac{g(mkx)}m ~\middle\vert~ m\in\mathbb N\right\}=k\inf\left\{\frac{g(mkx)}{mk} ~\middle\vert~ m\in\mathbb N\right\}$$ since $g(x+y)\le g(x)+g(y)$, we have $\frac{g(mkx)}{mk}\le\frac{kg(mx)}{mk}$ So, $$h(kx)\le k\frac{g(mx)}m$$ therefore $$h(kx)\le kh(x)$$ on the other hand $$kh(x)=\inf\left\{\frac{kg(mx)}m ~\middle\vert~ m\in\mathbb N\right\}$$ and since $\frac{kg(mx)}m\le\frac{km}mg(x)$ we have $$kh(x)\le kg(x)$$ Any hint to continue?",,"['real-analysis', 'linear-algebra']"
13,Convergence radius and two-times-differentiability of power series.,Convergence radius and two-times-differentiability of power series.,,"I wanted to compute the radius of convergence for the following the power series $$\sum_{n=1}^{\infty} a_nz^n$$ with $(i) \, a_n = n!, \, (ii) \, a_n = \sqrt[\leftroot{-3}\uproot{3}n]{n}$ Then I need to determine which power series defines a function that is twice differentiable and compute the second derivative. For the convergence radius I had no problem, and used once the Alembert criterion, once the Cauchy criterion and found that the convergence radius for $a_n = n!$ is 0, and the one for $a_n = 1$. Now I'm tempted to say that therefore the first power series gives a non differentiable function, since it doesn't converge. Is that right? Because I'm not sure about it. Then for the second I was told that since the convergence radius is $1$, it's differentiable at $]-1,1[$, which I can understand, but that it's also automatically differentiable twice at $]-1,1[$. And that part I don't undertand. Why? Has that something to do with the radius being specifically $1$? And how to conclude in general? For the last question, about derivating twice, isn't it just derivate the sum? So I'd get: $$f''(x) = \sum_{n = 2}^{\infty} n(n-1)a_nz^{n-2}$$","I wanted to compute the radius of convergence for the following the power series $$\sum_{n=1}^{\infty} a_nz^n$$ with $(i) \, a_n = n!, \, (ii) \, a_n = \sqrt[\leftroot{-3}\uproot{3}n]{n}$ Then I need to determine which power series defines a function that is twice differentiable and compute the second derivative. For the convergence radius I had no problem, and used once the Alembert criterion, once the Cauchy criterion and found that the convergence radius for $a_n = n!$ is 0, and the one for $a_n = 1$. Now I'm tempted to say that therefore the first power series gives a non differentiable function, since it doesn't converge. Is that right? Because I'm not sure about it. Then for the second I was told that since the convergence radius is $1$, it's differentiable at $]-1,1[$, which I can understand, but that it's also automatically differentiable twice at $]-1,1[$. And that part I don't undertand. Why? Has that something to do with the radius being specifically $1$? And how to conclude in general? For the last question, about derivating twice, isn't it just derivate the sum? So I'd get: $$f''(x) = \sum_{n = 2}^{\infty} n(n-1)a_nz^{n-2}$$",,"['analysis', 'power-series']"
14,Second order Taylor expansion with Lagrangian form of the remainder,Second order Taylor expansion with Lagrangian form of the remainder,,"I have a vector-valued smooth function $\boldsymbol{f}:\mathbb{R}^m \rightarrow \mathbb{R}^n$ and I want to use the second order Taylor expansion to bound $$\sum_{i=1}^{n}(f_i(\boldsymbol{v} +\boldsymbol{h})-f_i(\boldsymbol{v})-\boldsymbol{h}^{T} \nabla f_i(\boldsymbol{v}))^2.$$ (It's squared euclidean norm.) Taylor's theorem for each coordinate implies $$\sum_{i=1}^{n}(f_i(\boldsymbol{v} +\boldsymbol{h})-f_i(\boldsymbol{v})-\boldsymbol{h}^{T} \nabla f_i(\boldsymbol{v}))^2 \le \frac14 \sum_{i=1}^{n} \sup_{\xi_i \in [v,v+h]}|\boldsymbol{h}^T\nabla^2 f_i(\boldsymbol{\xi_i})\boldsymbol{h}|^2.$$ But can we change the order of sum and supremum? In other words, can we write that $$\sum_{i=1}^{n}(f_i(\boldsymbol{v} +\boldsymbol{h})-f_i(\boldsymbol{v})-\boldsymbol{h}^{T} \nabla f_i(\boldsymbol{v}))^2 \le \frac14 \sup_{\xi \in [v,v+h]} \sum_{i=1}^{n} |\boldsymbol{h}^T\nabla^2 f_i(\boldsymbol{\xi})\boldsymbol{h}|^2.$$ Thanks!","I have a vector-valued smooth function $\boldsymbol{f}:\mathbb{R}^m \rightarrow \mathbb{R}^n$ and I want to use the second order Taylor expansion to bound $$\sum_{i=1}^{n}(f_i(\boldsymbol{v} +\boldsymbol{h})-f_i(\boldsymbol{v})-\boldsymbol{h}^{T} \nabla f_i(\boldsymbol{v}))^2.$$ (It's squared euclidean norm.) Taylor's theorem for each coordinate implies $$\sum_{i=1}^{n}(f_i(\boldsymbol{v} +\boldsymbol{h})-f_i(\boldsymbol{v})-\boldsymbol{h}^{T} \nabla f_i(\boldsymbol{v}))^2 \le \frac14 \sum_{i=1}^{n} \sup_{\xi_i \in [v,v+h]}|\boldsymbol{h}^T\nabla^2 f_i(\boldsymbol{\xi_i})\boldsymbol{h}|^2.$$ But can we change the order of sum and supremum? In other words, can we write that $$\sum_{i=1}^{n}(f_i(\boldsymbol{v} +\boldsymbol{h})-f_i(\boldsymbol{v})-\boldsymbol{h}^{T} \nabla f_i(\boldsymbol{v}))^2 \le \frac14 \sup_{\xi \in [v,v+h]} \sum_{i=1}^{n} |\boldsymbol{h}^T\nabla^2 f_i(\boldsymbol{\xi})\boldsymbol{h}|^2.$$ Thanks!",,"['calculus', 'analysis', 'functional-analysis']"
15,Is it possible to get a closed-form for $\int_0^1\frac{(1-x)^{n+2k-2}}{(1+x)^{2k-1}}dx$?,Is it possible to get a closed-form for ?,\int_0^1\frac{(1-x)^{n+2k-2}}{(1+x)^{2k-1}}dx,"It is know that $$H_n=-n\int_0^1(1-t)^{n-1}\log (t)dt,$$ see [1], where $H_n=1+1/2+\ldots+1/n$ it the nth harmonic number . Then I believe that can be used for $x>0$  $$\frac{1}{2}\log(x)=\sum_{k=1}^{\infty}\frac{(-1)^{2k-1}}{2k-1}\left(\frac{x-1}{1+x}\right)^{2k-1},$$ see for example [2], to show  $$H_n=2n\sum_{k=1}^{\infty}\frac{1}{2k-1}\int_0^1\frac{(1-x)^{n+2k-2}}{(1+x)^{2k-1}}dx.$$ Question. It is possible to get a closed-form for    $$\int_0^1\frac{(1-x)^{n+2k-2}}{(1+x)^{2k-1}}dx?$$ If you want give a justification for previous expression for $H_n$, and know how comoute previous definite integral I am wait your answer. Thanks in advance. References: [1] Furdui, LA GACETA de la Real Sociedad Matemática Española. Third paragraph in page 699 , see here . [2] Hyslop, Infinite Series, Dover Publications (2006).","It is know that $$H_n=-n\int_0^1(1-t)^{n-1}\log (t)dt,$$ see [1], where $H_n=1+1/2+\ldots+1/n$ it the nth harmonic number . Then I believe that can be used for $x>0$  $$\frac{1}{2}\log(x)=\sum_{k=1}^{\infty}\frac{(-1)^{2k-1}}{2k-1}\left(\frac{x-1}{1+x}\right)^{2k-1},$$ see for example [2], to show  $$H_n=2n\sum_{k=1}^{\infty}\frac{1}{2k-1}\int_0^1\frac{(1-x)^{n+2k-2}}{(1+x)^{2k-1}}dx.$$ Question. It is possible to get a closed-form for    $$\int_0^1\frac{(1-x)^{n+2k-2}}{(1+x)^{2k-1}}dx?$$ If you want give a justification for previous expression for $H_n$, and know how comoute previous definite integral I am wait your answer. Thanks in advance. References: [1] Furdui, LA GACETA de la Real Sociedad Matemática Española. Third paragraph in page 699 , see here . [2] Hyslop, Infinite Series, Dover Publications (2006).",,['sequences-and-series']
16,Sequence of compactly supported functions approximating $x^2$,Sequence of compactly supported functions approximating,x^2,"I encountered this question as part of a proof I am working on and was wondering whether anybody has an explicit way to construct these sequences: 1.) Is there a sequence of positive compactly supported functions $f_n \in C_c^2(\mathbb{R_{\ge 0}})$ with $||f_n''|| \le 2$ such that $f_n(x) \rightarrow x^2$ for every $x \in \mathbb{R}.$ 2.) Is there a sequence of positive compactly supported functions $f_n \in C_c^2(\mathbb{R_{\ge 0}})$ with $|f_n''(x)| \le x^2$ and $f_n(x) \rightarrow \frac{x^4}{12}$? I mean figuratively, you need to construct functions that approximate the respective function on some interval $[-n,n]$ quite well and decay to $0$ on $[-m-n,-n] \cup [n,n+m]$ in a way that is not too fast. But how one could do this explicitly is really the big issue here.","I encountered this question as part of a proof I am working on and was wondering whether anybody has an explicit way to construct these sequences: 1.) Is there a sequence of positive compactly supported functions $f_n \in C_c^2(\mathbb{R_{\ge 0}})$ with $||f_n''|| \le 2$ such that $f_n(x) \rightarrow x^2$ for every $x \in \mathbb{R}.$ 2.) Is there a sequence of positive compactly supported functions $f_n \in C_c^2(\mathbb{R_{\ge 0}})$ with $|f_n''(x)| \le x^2$ and $f_n(x) \rightarrow \frac{x^4}{12}$? I mean figuratively, you need to construct functions that approximate the respective function on some interval $[-n,n]$ quite well and decay to $0$ on $[-m-n,-n] \cup [n,n+m]$ in a way that is not too fast. But how one could do this explicitly is really the big issue here.",,"['calculus', 'real-analysis']"
17,show that $ f_h \in L^1(R) $ and $ \lim_{h \to 0} f_h(x) = f(x) $ in $ L^1(R) $,show that  and  in, f_h \in L^1(R)   \lim_{h \to 0} f_h(x) = f(x)   L^1(R) ,"If $ f \in L^1(R) $ and set  $ f_h(x)= \frac{1}{2h} \int_{x-h}^{x+h}f(t)dt, h>0 $ then show that  $ f_h \in L^1(R) $ and $ \lim_{h \to 0} f_h(x) = f(x) $ in $ L^1(R) $ To prove f_h is integrable in R we have to this integral is finite almost everywhere. I just wondering how to do that? looking for some hints","If $ f \in L^1(R) $ and set  $ f_h(x)= \frac{1}{2h} \int_{x-h}^{x+h}f(t)dt, h>0 $ then show that  $ f_h \in L^1(R) $ and $ \lim_{h \to 0} f_h(x) = f(x) $ in $ L^1(R) $ To prove f_h is integrable in R we have to this integral is finite almost everywhere. I just wondering how to do that? looking for some hints",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
18,Existence of derivative of a given function,Existence of derivative of a given function,,"Given $$F(x)=\frac{1}{2x}\int_{-x}^xf(t)dt,$$ where $f:\mathbb{R}\to\mathbb{R}$ is continuous and $f'(0)$ exists, how can I prove that $F'(0)$ also exists?","Given $$F(x)=\frac{1}{2x}\int_{-x}^xf(t)dt,$$ where $f:\mathbb{R}\to\mathbb{R}$ is continuous and $f'(0)$ exists, how can I prove that $F'(0)$ also exists?",,"['real-analysis', 'integration', 'analysis', 'derivatives']"
19,Impossible to show this analytically (without numerical methods)?,Impossible to show this analytically (without numerical methods)?,,"Let $f(x):=-xe^{-x^2}$ then we have that $\inf_{x \in \mathbb{R}} f(x)=-\frac{1}{\sqrt{2e}} \approx -0.429$ see wolframalpha Now define $g(x):=f(x)-0.01 f'''(x)$ then we have that $\inf_{x \in \mathbb{R}}g(x) \approx - 0.408$ see wolframalpha Now I want to show analytically that $\inf_{x \in \mathbb{R}} f(x)< \inf_{x \in \mathbb{R}} g(x).$ But I have the feeling that this is impossible as the method of calculating extrema via $g'(x)=0$ leads to a fifth-order polynomial equation. So is it really not possible to show this fact analytically or is there a work-around? In particular, I am not interested in the precise values, I only want to show that the infimum of $f$ is strictly smaller than the one of $g$. You maybe also want to have a look at the graph where the one with the lower peak (red) is of course $f$ and the other one $g$. If anything is unclear, please let me know.","Let $f(x):=-xe^{-x^2}$ then we have that $\inf_{x \in \mathbb{R}} f(x)=-\frac{1}{\sqrt{2e}} \approx -0.429$ see wolframalpha Now define $g(x):=f(x)-0.01 f'''(x)$ then we have that $\inf_{x \in \mathbb{R}}g(x) \approx - 0.408$ see wolframalpha Now I want to show analytically that $\inf_{x \in \mathbb{R}} f(x)< \inf_{x \in \mathbb{R}} g(x).$ But I have the feeling that this is impossible as the method of calculating extrema via $g'(x)=0$ leads to a fifth-order polynomial equation. So is it really not possible to show this fact analytically or is there a work-around? In particular, I am not interested in the precise values, I only want to show that the infimum of $f$ is strictly smaller than the one of $g$. You maybe also want to have a look at the graph where the one with the lower peak (red) is of course $f$ and the other one $g$. If anything is unclear, please let me know.",,"['calculus', 'real-analysis']"
20,$L^\infty$ is complete - proof from exercises in Royden & Fitzpatrick's Real Analysis.,is complete - proof from exercises in Royden & Fitzpatrick's Real Analysis.,L^\infty,"I've been reading up on some Analysis for my comp exams, and I couldn't find in my texts a proof of $L^\infty$ being Banach. Someone pointed me to the following exercise in Royden & Fitzpatrick. Now, I've found other proofs of $L^\infty$ being Banach online, but this problem is now really bothering me: Let ${f_n}$ be a sequence in $L^\infty(E)$ and $\sum_{k=1}^\infty a_k$ a convergent series of positive numbers such that $||f_{k+1}-f_k||_\infty \leq a_k$ for all $k$. Then, $\exists$ a subset $E_0$ of $E$ of measure zero and $|f_{n+k}(x)-f_k(x)|\leq||f_{n+k}-f_k||_\infty \leq \sum_{j=n}^\infty a_j$ for all $k,n$ and all $x\in E$~$E_0$ a) Conclude that there is a function $f\in L^\infty(E)$ such that ${f_n}$->$f$ uniformly on $E$~$E_0$.  b) Now, show that $L^\infty(E)$ is a Banach space. ATTEMPT: Thanks to the comments. The first part of the inequality: $|f_{n+k}(x)-f_k(x)|\leq||f_{n+k}-f_k||_\infty \leq \sum_{j=n}^\infty a_j$ is immediate from the definition of the sup norm (keeping in mind that Lp spaces are really equivalence classes where equality means values coincide a.e.) and the second part is obvious by summing up the left and right-hand sides of the given inequality. Now, does this inequality give me uniform convergence with any indices $l,q>n$ and $\epsilon =  \sum_{j=n}^\infty a_j $? If so, why does all this give me b), that $L^\infty$ is complete? Any help is much appreciated.","I've been reading up on some Analysis for my comp exams, and I couldn't find in my texts a proof of $L^\infty$ being Banach. Someone pointed me to the following exercise in Royden & Fitzpatrick. Now, I've found other proofs of $L^\infty$ being Banach online, but this problem is now really bothering me: Let ${f_n}$ be a sequence in $L^\infty(E)$ and $\sum_{k=1}^\infty a_k$ a convergent series of positive numbers such that $||f_{k+1}-f_k||_\infty \leq a_k$ for all $k$. Then, $\exists$ a subset $E_0$ of $E$ of measure zero and $|f_{n+k}(x)-f_k(x)|\leq||f_{n+k}-f_k||_\infty \leq \sum_{j=n}^\infty a_j$ for all $k,n$ and all $x\in E$~$E_0$ a) Conclude that there is a function $f\in L^\infty(E)$ such that ${f_n}$->$f$ uniformly on $E$~$E_0$.  b) Now, show that $L^\infty(E)$ is a Banach space. ATTEMPT: Thanks to the comments. The first part of the inequality: $|f_{n+k}(x)-f_k(x)|\leq||f_{n+k}-f_k||_\infty \leq \sum_{j=n}^\infty a_j$ is immediate from the definition of the sup norm (keeping in mind that Lp spaces are really equivalence classes where equality means values coincide a.e.) and the second part is obvious by summing up the left and right-hand sides of the given inequality. Now, does this inequality give me uniform convergence with any indices $l,q>n$ and $\epsilon =  \sum_{j=n}^\infty a_j $? If so, why does all this give me b), that $L^\infty$ is complete? Any help is much appreciated.",,"['real-analysis', 'analysis', 'functional-analysis', 'lp-spaces']"
21,"The Frechet derivative of $\sin(xy)$ at $(\pi,0)$.",The Frechet derivative of  at .,"\sin(xy) (\pi,0)","Is the Frechet derivative of $\sin(xy)$ at the point $(\pi,0)$ equal to the the linear function $\phi(x,y)=\pi y$? The Frechet derivative of $xy$ at $(a,b)$ is $bx+ay$ and so the Frechet derivative of $\sin(xy)$ must $\cos(ab)(bx+ay)$. Is it correct? However the limit with which the Frechet derivative is defiend: $$\lim_{(s,t)\to (0,0)}\frac{\sin((a+s)(b+t))-\sin(ab)-\cos(ab)(bs+at)}{\sqrt{s^2+t^2}}$$ does not exists when $(a,b)=(\pi,0)$: $$\lim_{(s,t)\to (0,0)}\frac{\sin(\pi t+st)-\pi t}{\sqrt{s^2+t^2}}$$","Is the Frechet derivative of $\sin(xy)$ at the point $(\pi,0)$ equal to the the linear function $\phi(x,y)=\pi y$? The Frechet derivative of $xy$ at $(a,b)$ is $bx+ay$ and so the Frechet derivative of $\sin(xy)$ must $\cos(ab)(bx+ay)$. Is it correct? However the limit with which the Frechet derivative is defiend: $$\lim_{(s,t)\to (0,0)}\frac{\sin((a+s)(b+t))-\sin(ab)-\cos(ab)(bs+at)}{\sqrt{s^2+t^2}}$$ does not exists when $(a,b)=(\pi,0)$: $$\lim_{(s,t)\to (0,0)}\frac{\sin(\pi t+st)-\pi t}{\sqrt{s^2+t^2}}$$",,"['real-analysis', 'analysis']"
22,How can we evaluate the following limit?,How can we evaluate the following limit?,,"How can this problem be solved?   $$ \lim_{(n,r) \rightarrow (\infty, \infty)} \frac{\prod\limits_{k=1}^{r} \left( \sum\limits_{i=1}^{n} i^{2k-1} \right)}{n^{r+1} \prod\limits_{k=1}^{r-1} \left( \sum\limits_{i=1}^{n} i^{2k} \right)} $$","How can this problem be solved?   $$ \lim_{(n,r) \rightarrow (\infty, \infty)} \frac{\prod\limits_{k=1}^{r} \left( \sum\limits_{i=1}^{n} i^{2k-1} \right)}{n^{r+1} \prod\limits_{k=1}^{r-1} \left( \sum\limits_{i=1}^{n} i^{2k} \right)} $$",,"['calculus', 'analysis']"
23,"Prove that $f(x,y)=(x+2y+y^2+|xy|,2x+y+x^2+|xy|)$ is differentiable at $(0,0)$.",Prove that  is differentiable at .,"f(x,y)=(x+2y+y^2+|xy|,2x+y+x^2+|xy|) (0,0)","Define $f:\Bbb R^2\to \Bbb R^2; ~~~(x,y)\longrightarrow(x+2y+y^2+|xy|,2x+y+x^2+|xy|)$ Prove that $f$ is differentiable at $(0,0)$ . In order to prove that $f$ is differentiable at $(0,0)$ we must have $\lim _{|h|\to 0}\dfrac{|f(h)-f(0)|}{|h|}= 0$ But here $\lim_{h^2+k^2\to 0}\dfrac{h+2k+k^2+|hk|}{h^2+k^2}$ does not exist. Is my approach wrong? Or the problem? Please help.",Define Prove that is differentiable at . In order to prove that is differentiable at we must have But here does not exist. Is my approach wrong? Or the problem? Please help.,"f:\Bbb R^2\to \Bbb R^2; ~~~(x,y)\longrightarrow(x+2y+y^2+|xy|,2x+y+x^2+|xy|) f (0,0) f (0,0) \lim _{|h|\to 0}\dfrac{|f(h)-f(0)|}{|h|}= 0 \lim_{h^2+k^2\to 0}\dfrac{h+2k+k^2+|hk|}{h^2+k^2}","['calculus', 'analysis', 'multivariable-calculus', 'derivatives']"
24,Cauchy Sequence in Normed Space,Cauchy Sequence in Normed Space,,"Let $(E, ||\cdot ||)$ be a normed space and let $(x_n)$ be a sequence in $E$. Show that the following conditions are equivalent: (a) $(x_n)$ is a Cauchy sequence. (b) For every increasing function $p: \mathbb{N} \to \mathbb{N}$ we have $||x_{p(n+1)} - x_{p(n)}||\to 0$ as $n \to \infty$. I've encountered this problem in my graduate level analysis class. The definition I have for Cauchy sequence is: $\forall  \varepsilon >0$  $\exists \, n_0$ such that $\forall m,n>n_0$, $||x_n-x_m||< \varepsilon$ (or $||x_n-x_m||\to 0$ as $n,m\to \infty$). Thank you for any assistance here.","Let $(E, ||\cdot ||)$ be a normed space and let $(x_n)$ be a sequence in $E$. Show that the following conditions are equivalent: (a) $(x_n)$ is a Cauchy sequence. (b) For every increasing function $p: \mathbb{N} \to \mathbb{N}$ we have $||x_{p(n+1)} - x_{p(n)}||\to 0$ as $n \to \infty$. I've encountered this problem in my graduate level analysis class. The definition I have for Cauchy sequence is: $\forall  \varepsilon >0$  $\exists \, n_0$ such that $\forall m,n>n_0$, $||x_n-x_m||< \varepsilon$ (or $||x_n-x_m||\to 0$ as $n,m\to \infty$). Thank you for any assistance here.",,"['real-analysis', 'analysis', 'normed-spaces', 'cauchy-sequences']"
25,Showing that a subset of $\ell^1$ is totally bounded,Showing that a subset of  is totally bounded,\ell^1,"Let $A=\{ f \in \ell^1\, :\,\text{ for each natural number}\, n\,\text{we have}\, |f(n)|<1/2^n \}$. Show that it is totally bounded in $\ell^1$ and find the interior of $A$ in $\ell^1$ the closure of $A$ in $\ell^1$ To prove that $A$ is totally bounded it suffices to show A is compact in $\ell^1$, right? But I do not know how to prove it. And also I would like to know how to find interior and closure of $A$.","Let $A=\{ f \in \ell^1\, :\,\text{ for each natural number}\, n\,\text{we have}\, |f(n)|<1/2^n \}$. Show that it is totally bounded in $\ell^1$ and find the interior of $A$ in $\ell^1$ the closure of $A$ in $\ell^1$ To prove that $A$ is totally bounded it suffices to show A is compact in $\ell^1$, right? But I do not know how to prove it. And also I would like to know how to find interior and closure of $A$.",,"['real-analysis', 'analysis', 'functional-analysis', 'metric-spaces', 'compactness']"
26,"If $g$ is integrable over $[a,b]$ then $f(x)=\int_{a}^{x}g(x)\,{\rm d}x$ is absolutely continuous on $[a,b]$",If  is integrable over  then  is absolutely continuous on,"g [a,b] f(x)=\int_{a}^{x}g(x)\,{\rm d}x [a,b]","From the definition this flows naturally up until near the end: For all $\epsilon>0$, I am looking for a $\delta$ such that $\sum_{i=1}^{n}|x_{i+1}-x_i|<\delta \implies \sum_{i=1}^{n}|f(x_{i+1})-f(x_i)|<\epsilon$ for all subintervals $(x_{k+1},x_k)$ of $[a,b]$. Now, $$\sum_{i=1}^{n}|f(x_{i+1})-f(x_i)|=\sum_{i=1}^{n}\left|\int^{x_{i+1}}_{a}g(x)\,{\rm d}x-\int_{a}^{x_i}g(x)\,{\rm d}x\right|\\=\sum_{i=1}^{n}\left|\int^{x_{i+1}}_{x_i}g(x)\,{\rm d}x\right|\leq \sum_{i=1}^{n}\int^{x_{i+1}}_{x_i}|g(x)|\,{\rm d}x=\int^{b}_{a}|g(x)|\,{\rm d}x$$ I am not sure how to go on from here. How does this sequence of implications help me find a $\delta$? All help is appreciated.","From the definition this flows naturally up until near the end: For all $\epsilon>0$, I am looking for a $\delta$ such that $\sum_{i=1}^{n}|x_{i+1}-x_i|<\delta \implies \sum_{i=1}^{n}|f(x_{i+1})-f(x_i)|<\epsilon$ for all subintervals $(x_{k+1},x_k)$ of $[a,b]$. Now, $$\sum_{i=1}^{n}|f(x_{i+1})-f(x_i)|=\sum_{i=1}^{n}\left|\int^{x_{i+1}}_{a}g(x)\,{\rm d}x-\int_{a}^{x_i}g(x)\,{\rm d}x\right|\\=\sum_{i=1}^{n}\left|\int^{x_{i+1}}_{x_i}g(x)\,{\rm d}x\right|\leq \sum_{i=1}^{n}\int^{x_{i+1}}_{x_i}|g(x)|\,{\rm d}x=\int^{b}_{a}|g(x)|\,{\rm d}x$$ I am not sure how to go on from here. How does this sequence of implications help me find a $\delta$? All help is appreciated.",,"['real-analysis', 'integration', 'analysis', 'continuity', 'lebesgue-measure']"
27,Exponential function as a series,Exponential function as a series,,"I have to prove that exponential function is equals to $\displaystyle \sum_{n=0}^\infty \frac{x^n}{n!}$ for $x<0$, I have the part for positive numbers, any help for this part? maybe something with $-x>0$ and $e^{-x}=\dfrac{1}{e^x}$ and Cauchy product? Thank you so much.","I have to prove that exponential function is equals to $\displaystyle \sum_{n=0}^\infty \frac{x^n}{n!}$ for $x<0$, I have the part for positive numbers, any help for this part? maybe something with $-x>0$ and $e^{-x}=\dfrac{1}{e^x}$ and Cauchy product? Thank you so much.",,"['sequences-and-series', 'analysis']"
28,Show $f$ is integrable,Show  is integrable,f,"Let $f$ be such that $\int_0^{\infty} |f(s)|e^s ds< \infty.$ Now, I want to argue that for $x,y$ sufficiently large and $\lambda < 1$ fixed we have that $$\int_0^{\infty} \int_x^y e^{\lambda z} e^{s} |f(s+z)|dz ds$$ can be made arbitrarily small for $\lambda <1.$ In other words: $\forall \varepsilon >0 \exists N: \left(x,y >N \Rightarrow \int_0^{\infty} \int_x^y e^{\lambda z} e^{s} |f(s+z)|dz ds< \varepsilon \right)$ But how can I show this rigorously? Does anybody have an idea","Let $f$ be such that $\int_0^{\infty} |f(s)|e^s ds< \infty.$ Now, I want to argue that for $x,y$ sufficiently large and $\lambda < 1$ fixed we have that $$\int_0^{\infty} \int_x^y e^{\lambda z} e^{s} |f(s+z)|dz ds$$ can be made arbitrarily small for $\lambda <1.$ In other words: $\forall \varepsilon >0 \exists N: \left(x,y >N \Rightarrow \int_0^{\infty} \int_x^y e^{\lambda z} e^{s} |f(s+z)|dz ds< \varepsilon \right)$ But how can I show this rigorously? Does anybody have an idea",,"['real-analysis', 'integration', 'analysis', 'lebesgue-integral']"
29,Homogeneous Littlewood-Paley decomposition,Homogeneous Littlewood-Paley decomposition,,I have a question concerning Littlewood-paley-theory. Suppose we have test functions $\psi_k$ supported in annuli $\{2^{k-1}\leq\vert\xi\vert\leq2^{k+1}\}$ such that $\sum_{k\in\mathbb{Z}}\psi_k(\xi)=1$ for $\xi\neq0$. Define the homogeneous Littlewood-Paley projections $P_k$ of a tempered distribution by $\widehat{P_ku}=\psi_k\hat u$. I'd like to show that the equality $$ u=\sum_{k\in\mathbb{Z}}P_ku\qquad \text{in} \qquad \mathcal{S}'$$ holds true modulo a polynomial. Unfortunately I'm not able to show this. That's why I'm hoping for your help. Greets Lukas,I have a question concerning Littlewood-paley-theory. Suppose we have test functions $\psi_k$ supported in annuli $\{2^{k-1}\leq\vert\xi\vert\leq2^{k+1}\}$ such that $\sum_{k\in\mathbb{Z}}\psi_k(\xi)=1$ for $\xi\neq0$. Define the homogeneous Littlewood-Paley projections $P_k$ of a tempered distribution by $\widehat{P_ku}=\psi_k\hat u$. I'd like to show that the equality $$ u=\sum_{k\in\mathbb{Z}}P_ku\qquad \text{in} \qquad \mathcal{S}'$$ holds true modulo a polynomial. Unfortunately I'm not able to show this. That's why I'm hoping for your help. Greets Lukas,,"['real-analysis', 'analysis', 'fourier-analysis', 'distribution-theory', 'littlewood-paley-theory']"
30,"Show that $cos(x)=0$ has a solution in $(\sqrt{2},1.6)$ using only the series definition.",Show that  has a solution in  using only the series definition.,"cos(x)=0 (\sqrt{2},1.6)","How do I show that $$\cos(x) = \sum\limits_{k=0}^{\infty} \left(\frac{(-1)^k x^{2k}}{(2k)!}\right) \qquad = 0$$ has a solution in the interval $(\sqrt{2},1.6)$ without using any 'external' results? My thought is to show that it's positive at $x=\sqrt{2}$ and negative at $x = 1.6$ and use the IVT.  I think I can show it's positive at $\sqrt{2}$ (by grouping terms into positive pairs), but I'm struggling to show it's negative (in the vicinity of) 1.6.","How do I show that $$\cos(x) = \sum\limits_{k=0}^{\infty} \left(\frac{(-1)^k x^{2k}}{(2k)!}\right) \qquad = 0$$ has a solution in the interval $(\sqrt{2},1.6)$ without using any 'external' results? My thought is to show that it's positive at $x=\sqrt{2}$ and negative at $x = 1.6$ and use the IVT.  I think I can show it's positive at $\sqrt{2}$ (by grouping terms into positive pairs), but I'm struggling to show it's negative (in the vicinity of) 1.6.",,"['real-analysis', 'sequences-and-series', 'analysis', 'trigonometry']"
31,Basic neighborhoods in weak topology,Basic neighborhoods in weak topology,,"I am trying to visualize the basic neighborhoods of the form $V(x_0;\varepsilon,f_1,...,f_n) = \bigcap_{j=1}^n \{ x \in E : |f_j(x-x_0)|<\varepsilon \}$ where $x_0 \in E$, $\varepsilon>0$ and $f_1,...,f_n \in E'$ on the weak topology of a normed infinite-dimensional vector space $E$. I had imagined some way ""to see"" these neighborhoods like open stripes bounded in a ""finite number of directions"", just like the vertical open stripes are the standard visualization of the basic neighborhoods in the product topology on a infinite product of topological spaces. In this sense can I say that these basic neighborhoods are rotated stripes?","I am trying to visualize the basic neighborhoods of the form $V(x_0;\varepsilon,f_1,...,f_n) = \bigcap_{j=1}^n \{ x \in E : |f_j(x-x_0)|<\varepsilon \}$ where $x_0 \in E$, $\varepsilon>0$ and $f_1,...,f_n \in E'$ on the weak topology of a normed infinite-dimensional vector space $E$. I had imagined some way ""to see"" these neighborhoods like open stripes bounded in a ""finite number of directions"", just like the vertical open stripes are the standard visualization of the basic neighborhoods in the product topology on a infinite product of topological spaces. In this sense can I say that these basic neighborhoods are rotated stripes?",,"['analysis', 'functional-analysis', 'topological-vector-spaces']"
32,Showing that $f^{(n)}(0)=0$ for $f(x)=e^{-1/x^2}$ if $x\neq 0$ and $f(0)=0$.,Showing that  for  if  and .,f^{(n)}(0)=0 f(x)=e^{-1/x^2} x\neq 0 f(0)=0,"Define $f$ as follows: $f(x)=e^{-1/x^2}$ if $x\neq 0$ and $f(0)=0$. Show that $f^{(n)}(0)$ is continuous for all $x$ and $f^{(n)}(0)=0$. $n=1,2,\dots$. To show this, I have shown that for $x\neq 0$, we have $f^{(n)}(x)=e^{-1/x^2}P_{3n}(1/x),$ where $P_{3n}(t)$ is a real polynomial of degree $3n$. Hence, to show that $f^{(n)}(0)=0,$ I will use induction. $n=1$ case is trivial. So suppose that $n=k$ holds. Then as $n=k+1$, we have $\frac{f^{(k)}(x)-f^{(k)}(0)}{x-0}=\frac{f^{(k)}(x)}{x}=\frac{e^{-1/x^2}P_{3k}(1/x)}{x}$. Now I need to show that the limit of the above fraction as $x\to 0$ tends to $0$. For the case $x\to 0^{+}$, I can argue as follows. Replace $t=1/x$ in the above fraction, then we get $\frac{e^{-1/x^2}P_{3k}(1/x)}{x}=\frac{tP_{3k}(t)}{e^{t^2}}=(\frac{tP_{3k}(t)}{e^t})(\frac{e^t}{e^{t^2}})\to 0$ as $t\to \infty$, since we have $\lim_{x\to \infty}\frac{P(x)}{e^x}=0$. However, I have trouble showing the case for $x\to 0^{-}$, since $\lim_{\to -\infty}\frac{P(x)}{e^x}$ does not exist for all polynomials. How can I solve this problem? I would greatly appreciate any help. (added) My attempt: $\lim_{x\to 0^{-}}\frac{e^{-1/x^2}P_{3k}(1/x)}{x}=\lim_{x\to 0^{-}}e^{-1/x^2}P_{3k+1}(1/x)=\lim_{x\to 0^{+}}e^{-1/x^2}P_{3k+1}(-1/x)=\lim_{t\to \infty}e^{-t^2}P_{3k+1}(-t)=\lim_{t\to\infty}\frac{P_{3k+1}(-t)}{e^t}\cdot \frac{e^t}{e^{t^2}}=0\cdot 0=0$ $\lim_{t\to\infty}\frac{P_{3k+1}(-t)}{e^t}=0$, since $|\frac{P_{3k+1}(-t)}{e^t}|\le\frac{P_{3k+1}(|-t|)}{e^t} \to 0$ as $t\to \infty$.","Define $f$ as follows: $f(x)=e^{-1/x^2}$ if $x\neq 0$ and $f(0)=0$. Show that $f^{(n)}(0)$ is continuous for all $x$ and $f^{(n)}(0)=0$. $n=1,2,\dots$. To show this, I have shown that for $x\neq 0$, we have $f^{(n)}(x)=e^{-1/x^2}P_{3n}(1/x),$ where $P_{3n}(t)$ is a real polynomial of degree $3n$. Hence, to show that $f^{(n)}(0)=0,$ I will use induction. $n=1$ case is trivial. So suppose that $n=k$ holds. Then as $n=k+1$, we have $\frac{f^{(k)}(x)-f^{(k)}(0)}{x-0}=\frac{f^{(k)}(x)}{x}=\frac{e^{-1/x^2}P_{3k}(1/x)}{x}$. Now I need to show that the limit of the above fraction as $x\to 0$ tends to $0$. For the case $x\to 0^{+}$, I can argue as follows. Replace $t=1/x$ in the above fraction, then we get $\frac{e^{-1/x^2}P_{3k}(1/x)}{x}=\frac{tP_{3k}(t)}{e^{t^2}}=(\frac{tP_{3k}(t)}{e^t})(\frac{e^t}{e^{t^2}})\to 0$ as $t\to \infty$, since we have $\lim_{x\to \infty}\frac{P(x)}{e^x}=0$. However, I have trouble showing the case for $x\to 0^{-}$, since $\lim_{\to -\infty}\frac{P(x)}{e^x}$ does not exist for all polynomials. How can I solve this problem? I would greatly appreciate any help. (added) My attempt: $\lim_{x\to 0^{-}}\frac{e^{-1/x^2}P_{3k}(1/x)}{x}=\lim_{x\to 0^{-}}e^{-1/x^2}P_{3k+1}(1/x)=\lim_{x\to 0^{+}}e^{-1/x^2}P_{3k+1}(-1/x)=\lim_{t\to \infty}e^{-t^2}P_{3k+1}(-t)=\lim_{t\to\infty}\frac{P_{3k+1}(-t)}{e^t}\cdot \frac{e^t}{e^{t^2}}=0\cdot 0=0$ $\lim_{t\to\infty}\frac{P_{3k+1}(-t)}{e^t}=0$, since $|\frac{P_{3k+1}(-t)}{e^t}|\le\frac{P_{3k+1}(|-t|)}{e^t} \to 0$ as $t\to \infty$.",,"['calculus', 'real-analysis', 'analysis']"
33,Proving with completeness axiom,Proving with completeness axiom,,"Suppose we claim that if there is a set $E := \{a \in \mathbb{R} : a < \epsilon, \forall \epsilon \in \mathbb{Q}^{+} \}$, then it must be true that $a \leq 0$. I aim to prove this using only the ordered field axioms and the completeness axiom (as per the ""request"" by my professor). I am going to prove it by contradiction. So suppose $\exists a>0$ so that $0 < a < \epsilon, \forall \epsilon \in \mathbb{Q}^{+}$. Now since $a < \epsilon, \forall \epsilon \in \mathbb{Q}^{+}$, we have $a < \frac{\epsilon}{2}$ since $\frac{\epsilon}{2} \in \mathbb{Q}^{+}$. So $2a < \epsilon$. It follows that we can always find $a' \in E$ such that $a<a'$ for any $a \in E$. Now here is my problem. I know that the last sentence in the previous paragraph somehow contradicts the completeness axiom. But I cannot see why. Do you have any suggestions?","Suppose we claim that if there is a set $E := \{a \in \mathbb{R} : a < \epsilon, \forall \epsilon \in \mathbb{Q}^{+} \}$, then it must be true that $a \leq 0$. I aim to prove this using only the ordered field axioms and the completeness axiom (as per the ""request"" by my professor). I am going to prove it by contradiction. So suppose $\exists a>0$ so that $0 < a < \epsilon, \forall \epsilon \in \mathbb{Q}^{+}$. Now since $a < \epsilon, \forall \epsilon \in \mathbb{Q}^{+}$, we have $a < \frac{\epsilon}{2}$ since $\frac{\epsilon}{2} \in \mathbb{Q}^{+}$. So $2a < \epsilon$. It follows that we can always find $a' \in E$ such that $a<a'$ for any $a \in E$. Now here is my problem. I know that the last sentence in the previous paragraph somehow contradicts the completeness axiom. But I cannot see why. Do you have any suggestions?",,[]
34,Properties of inverse functions,Properties of inverse functions,,"In order for a certain proof to work, I need a function and its inverse to satisfy the following property: if  $f(x) \ge k$, then $x \le f^{-1}(k)$. Is there a more general term for this? To provide some perspective, I am using $f$ to map distance to similarity. E.g. $1/(1+x)$ is one function that I am using for this purpose. So effectively the idea is that if the similarity is above some threshold, then the distance should also be below the same threshold, converted back to distance.","In order for a certain proof to work, I need a function and its inverse to satisfy the following property: if  $f(x) \ge k$, then $x \le f^{-1}(k)$. Is there a more general term for this? To provide some perspective, I am using $f$ to map distance to similarity. E.g. $1/(1+x)$ is one function that I am using for this purpose. So effectively the idea is that if the similarity is above some threshold, then the distance should also be below the same threshold, converted back to distance.",,"['real-analysis', 'linear-algebra', 'analysis']"
35,Measuring the set-theoretical complexity of sets/spaces encountered in general analysis,Measuring the set-theoretical complexity of sets/spaces encountered in general analysis,,"In analysis, it is common to encounter subsets of $\mathbb R$ (or even $\mathbb R^n$) which appear to be ""well-behaved"", especially with regard to properties like being measurable, compactness, etc. It is core to descriptive set theory (DST) that one is able to impose a classification of such subsets by closure properties of varying degree. Loosely speaking, for example, one obtains the Borel hierarchy by applying a certain transfinite induction on open/closed sets indexed by ordinal numbers $\alpha_k < \omega_1$, taking unions at limit stages, and so on. On the face of it, it appears that the complexity of the sets used in real analysis and measure theory are of low complexity in the Borel hierarchy, and sets in functional analysis (I am thinking spaces of functions, etc) are of a strictly higher complexity, but I am unsure where or how their relative complexity is classified in the known hierarchies. My main question can be split into some sub-questions: 1) What are the most ""complicated"" sets which one encounters in general analysis, and how may one translate complexity classification in DST to concrete problems in real analysis? 2) Is there any reasonable method in DST to measure the set-theoretic complexity of general spaces? (Here I am thinking along the lines of $L^p$ and Sobolev spaces). I apologize if (2) is a silly question, but a nice reference noting the recent applications of DST to functional analysis is found here http://www.math.uiuc.edu/~anush/Notes/dst_lectures.pdf in the introduction, namely in connection with a background for classification problems in a concrete setting. More references would be appreciated.","In analysis, it is common to encounter subsets of $\mathbb R$ (or even $\mathbb R^n$) which appear to be ""well-behaved"", especially with regard to properties like being measurable, compactness, etc. It is core to descriptive set theory (DST) that one is able to impose a classification of such subsets by closure properties of varying degree. Loosely speaking, for example, one obtains the Borel hierarchy by applying a certain transfinite induction on open/closed sets indexed by ordinal numbers $\alpha_k < \omega_1$, taking unions at limit stages, and so on. On the face of it, it appears that the complexity of the sets used in real analysis and measure theory are of low complexity in the Borel hierarchy, and sets in functional analysis (I am thinking spaces of functions, etc) are of a strictly higher complexity, but I am unsure where or how their relative complexity is classified in the known hierarchies. My main question can be split into some sub-questions: 1) What are the most ""complicated"" sets which one encounters in general analysis, and how may one translate complexity classification in DST to concrete problems in real analysis? 2) Is there any reasonable method in DST to measure the set-theoretic complexity of general spaces? (Here I am thinking along the lines of $L^p$ and Sobolev spaces). I apologize if (2) is a silly question, but a nice reference noting the recent applications of DST to functional analysis is found here http://www.math.uiuc.edu/~anush/Notes/dst_lectures.pdf in the introduction, namely in connection with a background for classification problems in a concrete setting. More references would be appreciated.",,"['analysis', 'functional-analysis', 'soft-question', 'set-theory', 'descriptive-set-theory']"
36,Extending a smooth function of constant rank,Extending a smooth function of constant rank,,"Let's denote $\mathbb{H}^m = \{(x_1, \ldots, x_m) \in \mathbb{R}^m\ |\ x_m \geq 0\}$. For an open subset $U \subset \mathbb{H}^m$, a function $f : U \to \mathbb{R}^n$ is called smooth if it can be locally extended to smooth functions (defined on open subsets of $\mathbb{R}^m$). By a simple partition of unity argument, this is equivalent to there being a smooth extension $\widetilde{f} : \widetilde{U} \to \mathbb{R}^n$, where $\widetilde{U} \supset U$ is an open subset of $\mathbb{R}^m$. Let us suppose now that $f$ has differential of constant rank equal to $k$ (by taking unilateral partial derivatives, it is clear that the differential of smooth extensions of $f$ doesn't depend on the choice of the extension throughout $U$, so we therefore may talk about the differential of $f$ on the boundary as well). My question is: can we guarantee the existence of a smooth extension $\widetilde{f}$ of $f$ that also has differential of constant rank $k$?","Let's denote $\mathbb{H}^m = \{(x_1, \ldots, x_m) \in \mathbb{R}^m\ |\ x_m \geq 0\}$. For an open subset $U \subset \mathbb{H}^m$, a function $f : U \to \mathbb{R}^n$ is called smooth if it can be locally extended to smooth functions (defined on open subsets of $\mathbb{R}^m$). By a simple partition of unity argument, this is equivalent to there being a smooth extension $\widetilde{f} : \widetilde{U} \to \mathbb{R}^n$, where $\widetilde{U} \supset U$ is an open subset of $\mathbb{R}^m$. Let us suppose now that $f$ has differential of constant rank equal to $k$ (by taking unilateral partial derivatives, it is clear that the differential of smooth extensions of $f$ doesn't depend on the choice of the extension throughout $U$, so we therefore may talk about the differential of $f$ on the boundary as well). My question is: can we guarantee the existence of a smooth extension $\widetilde{f}$ of $f$ that also has differential of constant rank $k$?",,"['analysis', 'multivariable-calculus', 'smooth-manifolds']"
37,$|f(x)-f(y)| \geq \frac{|x-y|}{2}$,,|f(x)-f(y)| \geq \frac{|x-y|}{2},"Let $f:\mathbb{R} \to \mathbb{R}$ be a continuous function such that $|f(x)-f(y)| \geq \frac{|x-y|}{2}$ then prove $f$ is onto. I can prove it just using IVT, but looking for some short solution which is using some good argument.","Let $f:\mathbb{R} \to \mathbb{R}$ be a continuous function such that $|f(x)-f(y)| \geq \frac{|x-y|}{2}$ then prove $f$ is onto. I can prove it just using IVT, but looking for some short solution which is using some good argument.",,"['analysis', 'continuity']"
38,Basic examples of functions in Hörmander class,Basic examples of functions in Hörmander class,,"The Hörmander class $S_{\rho,\delta}^m$ (with $\rho,\delta\in[0,1]$) consists of smooth functions $p(x,\xi)$ with $$|D_x^\beta D_\xi^\alpha p(x,\xi)|\leq C_{\alpha\beta}(1+|\xi|^2)^{(m-\rho|\alpha|+\delta|\beta|)/2}.$$ I'm a newcomer to pseudodifferential calculus so my question is: what are the basic examples to have in mind when thinking about this class? Relatedly, why is this class a natural thing to consider? The only example apparent to me is that if $$p(x,\xi)=\sum_{|\alpha|\leq k} a_\alpha(x)\xi^\alpha$$ with $a_\alpha\in C_c^\infty$, then $p\in S_{0,\delta}^m$ for any $\delta\geq 0.$","The Hörmander class $S_{\rho,\delta}^m$ (with $\rho,\delta\in[0,1]$) consists of smooth functions $p(x,\xi)$ with $$|D_x^\beta D_\xi^\alpha p(x,\xi)|\leq C_{\alpha\beta}(1+|\xi|^2)^{(m-\rho|\alpha|+\delta|\beta|)/2}.$$ I'm a newcomer to pseudodifferential calculus so my question is: what are the basic examples to have in mind when thinking about this class? Relatedly, why is this class a natural thing to consider? The only example apparent to me is that if $$p(x,\xi)=\sum_{|\alpha|\leq k} a_\alpha(x)\xi^\alpha$$ with $a_\alpha\in C_c^\infty$, then $p\in S_{0,\delta}^m$ for any $\delta\geq 0.$",,"['analysis', 'partial-differential-equations', 'pseudo-differential-operators']"
39,Methods to Minimize Functions and Integrals over $\mathbb{N}$.,Methods to Minimize Functions and Integrals over .,\mathbb{N},"In a paper I'm writing, I have to minimize a messy function $f(\mu,n)$ where $\mu \in \mathbb{R}$ and $n \in \mathbb{N}$. That is, given $\mu \in \mathbb{R}$, I need to minimize the one variable function $f_\mu (n)$. Moreover, I have a couple messy integral expressions involving floor and ceiling functions, say $$\int_{n\mu}^{\left \lceil{n \mu}\right \rceil} f(t) \, dt,$$ again I need to minimize this for $n \in \mathbb{N}$ given $\mu$. My question is,: What are some methods for minimizing functions and integrals over subsets of $\mathbb{R}$, specifically $\mathbb{N}$? Say, given a messy quadratic in $n$ and $\mu$, what is the best way to minimize such a function over $n \in \mathbb{N}$? To specify, I'm looking for methods that avoid using computational resources, such as Matlab, these should be purely analytical. Also, any references would be remarkably useful.","In a paper I'm writing, I have to minimize a messy function $f(\mu,n)$ where $\mu \in \mathbb{R}$ and $n \in \mathbb{N}$. That is, given $\mu \in \mathbb{R}$, I need to minimize the one variable function $f_\mu (n)$. Moreover, I have a couple messy integral expressions involving floor and ceiling functions, say $$\int_{n\mu}^{\left \lceil{n \mu}\right \rceil} f(t) \, dt,$$ again I need to minimize this for $n \in \mathbb{N}$ given $\mu$. My question is,: What are some methods for minimizing functions and integrals over subsets of $\mathbb{R}$, specifically $\mathbb{N}$? Say, given a messy quadratic in $n$ and $\mu$, what is the best way to minimize such a function over $n \in \mathbb{N}$? To specify, I'm looking for methods that avoid using computational resources, such as Matlab, these should be purely analytical. Also, any references would be remarkably useful.",,"['real-analysis', 'integration', 'analysis', 'optimization']"
40,A question on the Banach fixed point theorem.,A question on the Banach fixed point theorem.,,"Suppose $f:(X,\tilde{d})\rightarrow(X,d)$ be a continuous function satisfying \begin{eqnarray}d(f(x),f(y))\leq \lambda d(x,y),\end{eqnarray} $\lambda > 1$. Let $\tilde{d}(x,y)=\lambda d(x,y)$. I observed that the topologies due to $d$ and $\tilde{d}$ are equivalent. Hence the ""contraction"" condition now reads as \begin{eqnarray}d(f(x),f(y))\leq \tilde{d}(x,y).\end{eqnarray} Can it concluded form here that a fixed point exists for $f$?. In other words I was wondering whether the Banach fixed point theorem holds if the metrics in the domain and the range spaces are equivalent and not exactly the same?.","Suppose $f:(X,\tilde{d})\rightarrow(X,d)$ be a continuous function satisfying \begin{eqnarray}d(f(x),f(y))\leq \lambda d(x,y),\end{eqnarray} $\lambda > 1$. Let $\tilde{d}(x,y)=\lambda d(x,y)$. I observed that the topologies due to $d$ and $\tilde{d}$ are equivalent. Hence the ""contraction"" condition now reads as \begin{eqnarray}d(f(x),f(y))\leq \tilde{d}(x,y).\end{eqnarray} Can it concluded form here that a fixed point exists for $f$?. In other words I was wondering whether the Banach fixed point theorem holds if the metrics in the domain and the range spaces are equivalent and not exactly the same?.",,"['real-analysis', 'analysis', 'functional-analysis', 'fixed-point-theorems']"
41,"Convexity increases the ""cost"" of long steps","Convexity increases the ""cost"" of long steps",,"Let $V(n)$ be a non-decreasing, convex function on $\mathbb{N}$ such that $V(0)=0$, $V(1)=1$. Let $(r_i)_{i=1}^{N}$ and $(r^{\prime}_i)_{i=1}^{N^{\prime}}$, $N^{\prime} > N$, be two sequences of positive integers such that $\sum\limits^N_{i=1} r_i = \sum\limits^{N^{\prime}}_{i=1} r^{\prime}_i = L$, for a fixed constant $L$. Prove that, $$ \sum\limits_{i=1}^{N} V(r_i) \geq \sum\limits_{i=1}^{N^{\prime}} V(r^{\prime}_i), $$ i.e., if we interpret $V(n)$ as the ''cost'' required for performing a jump of length $n$, the previous expression tells us that convexity of the cost function implies that it is always more convenient to perform ""small"" than ""large"" jumps to cover a given distance.","Let $V(n)$ be a non-decreasing, convex function on $\mathbb{N}$ such that $V(0)=0$, $V(1)=1$. Let $(r_i)_{i=1}^{N}$ and $(r^{\prime}_i)_{i=1}^{N^{\prime}}$, $N^{\prime} > N$, be two sequences of positive integers such that $\sum\limits^N_{i=1} r_i = \sum\limits^{N^{\prime}}_{i=1} r^{\prime}_i = L$, for a fixed constant $L$. Prove that, $$ \sum\limits_{i=1}^{N} V(r_i) \geq \sum\limits_{i=1}^{N^{\prime}} V(r^{\prime}_i), $$ i.e., if we interpret $V(n)$ as the ''cost'' required for performing a jump of length $n$, the previous expression tells us that convexity of the cost function implies that it is always more convenient to perform ""small"" than ""large"" jumps to cover a given distance.",,"['real-analysis', 'analysis', 'convex-analysis']"
42,Understanding tensors,Understanding tensors,,"Locally in a chart, a tensor field looks like $$T= T^{i_1,...i_n}_{j_1,...,j_m} dx^{j_1} \otimes...\otimes dx^{j_m} \otimes \partial_{i_1} \otimes ... \otimes \partial_{i_n},$$ where $T^{i_1,...i_n}_{j_1,...,j_m}$ are smooth functions. Now, I want to see that this is then a $C^{\infty}$ multilinear map  $T: \Omega_1(M) \times... \Omega_1(M) \times \Gamma(TM) \times... \times \Gamma(TM) \rightarrow C^{\infty}(M,\mathbb{R}).$ The converse direction is actually clear to me. Now assume we would plug in a vector field multiplied by a function in the first component of the local tensor field representation (which corresponds to a tensor $dx^{j_1}$, then there would be a term $dx^{j_1}(f \partial_i)$, since $\partial_i$ is a locally a basis of tangent spaces and $f$ is here a smooth function. Now, how is this $C^{\infty}$ linear? Or is this defined to be $f dx^{j_1}(\partial_i)$?  The thing is that I don't think so, i.e. $\partial_i$ is of course not(!) $C^{\infty}$ linear (as it is a derivative), so $\partial_{i_k}(f dx^{i})\neq f \partial_{i_k}(dx^{i})$ as far as I know. By the way: I know that $dx^i$ is just the dual basis element to $\partial_i$, but I don't get this $C^{\infty}$ linearity. Edit: Thinking about it, I feel as if a pointwise argument could save me here: $\partial_{i_k}(f dx^{i})|_p = \partial_{i_k}|_p(f(p)dx^{i}(p)) = f(p) \delta_{i_k,i}$ for all points $p \in M$. So it holds for the smooth function. Is this the trick? If so, then this notation is totally confusing in this case. But maybe somebody could still explain why the vector field $\partial_{i_k}$ does in this case not act on $f$, cause I still feel as if I have not understood what I am doing here.?","Locally in a chart, a tensor field looks like $$T= T^{i_1,...i_n}_{j_1,...,j_m} dx^{j_1} \otimes...\otimes dx^{j_m} \otimes \partial_{i_1} \otimes ... \otimes \partial_{i_n},$$ where $T^{i_1,...i_n}_{j_1,...,j_m}$ are smooth functions. Now, I want to see that this is then a $C^{\infty}$ multilinear map  $T: \Omega_1(M) \times... \Omega_1(M) \times \Gamma(TM) \times... \times \Gamma(TM) \rightarrow C^{\infty}(M,\mathbb{R}).$ The converse direction is actually clear to me. Now assume we would plug in a vector field multiplied by a function in the first component of the local tensor field representation (which corresponds to a tensor $dx^{j_1}$, then there would be a term $dx^{j_1}(f \partial_i)$, since $\partial_i$ is a locally a basis of tangent spaces and $f$ is here a smooth function. Now, how is this $C^{\infty}$ linear? Or is this defined to be $f dx^{j_1}(\partial_i)$?  The thing is that I don't think so, i.e. $\partial_i$ is of course not(!) $C^{\infty}$ linear (as it is a derivative), so $\partial_{i_k}(f dx^{i})\neq f \partial_{i_k}(dx^{i})$ as far as I know. By the way: I know that $dx^i$ is just the dual basis element to $\partial_i$, but I don't get this $C^{\infty}$ linearity. Edit: Thinking about it, I feel as if a pointwise argument could save me here: $\partial_{i_k}(f dx^{i})|_p = \partial_{i_k}|_p(f(p)dx^{i}(p)) = f(p) \delta_{i_k,i}$ for all points $p \in M$. So it holds for the smooth function. Is this the trick? If so, then this notation is totally confusing in this case. But maybe somebody could still explain why the vector field $\partial_{i_k}$ does in this case not act on $f$, cause I still feel as if I have not understood what I am doing here.?",,"['analysis', 'differential-geometry', 'differential-topology', 'tensors']"
43,Showing that $f$ is $C^\infty$,Showing that  is,f C^\infty,"Question: Let $f: U \to \mathbb R$ be a continuous function, with $U \subset \mathbb R^2$ open, such that $$(x^2  +y^4)f(x,y) + f(x,y)^3 = 1,\, \,\, \forall (x,y) \in U$$   Show that $f$ is of class $C^\infty$. Attempt: Define $F(x,y,z) = (x^2 + y^4)z + z^3 - 1$. Then $$F_z(x,y,z) = (x^2 + y^4) + 3z^2 > 0$$ for any $(x,y,z) \in \mathbb R^3- \{0\}$, and if we fix $(x_0,y_0) \in U$, such that $(x_0,y_0) \neq 0$, then taking $z_0 = f(x_0,y_0) \in \mathbb R$, thus it follows that $F(x_0,y_0,z_0) = 0$. Now by the Implicit Function Theorem we have that $z$ is a defined as a function of $x$ and $y$, such that $$F(x,y,z) = F(x,y,\xi (x,y)) = 0 \tag{1}$$ for every $(x,y,z) \in B \times J$, here $B \subset \mathbb R^2$ and $J \subset \mathbb R$. Clearly $F$ is of class $C^\infty$ then $z = \xi (x,y)$ is of class $C^\infty$. As $f$ is continuous there exists $\delta > 0$ sufficiently small such that $f(B) \subseteq J$, then we may conclude by $(1)$ and by hypothesis $F(x,y,f(x,y)) = 0$, that  $f(x,y) = \xi(x,y)$, for every $x \in B$, it follows then that $f$ is of class $C^\infty$.","Question: Let $f: U \to \mathbb R$ be a continuous function, with $U \subset \mathbb R^2$ open, such that $$(x^2  +y^4)f(x,y) + f(x,y)^3 = 1,\, \,\, \forall (x,y) \in U$$   Show that $f$ is of class $C^\infty$. Attempt: Define $F(x,y,z) = (x^2 + y^4)z + z^3 - 1$. Then $$F_z(x,y,z) = (x^2 + y^4) + 3z^2 > 0$$ for any $(x,y,z) \in \mathbb R^3- \{0\}$, and if we fix $(x_0,y_0) \in U$, such that $(x_0,y_0) \neq 0$, then taking $z_0 = f(x_0,y_0) \in \mathbb R$, thus it follows that $F(x_0,y_0,z_0) = 0$. Now by the Implicit Function Theorem we have that $z$ is a defined as a function of $x$ and $y$, such that $$F(x,y,z) = F(x,y,\xi (x,y)) = 0 \tag{1}$$ for every $(x,y,z) \in B \times J$, here $B \subset \mathbb R^2$ and $J \subset \mathbb R$. Clearly $F$ is of class $C^\infty$ then $z = \xi (x,y)$ is of class $C^\infty$. As $f$ is continuous there exists $\delta > 0$ sufficiently small such that $f(B) \subseteq J$, then we may conclude by $(1)$ and by hypothesis $F(x,y,f(x,y)) = 0$, that  $f(x,y) = \xi(x,y)$, for every $x \in B$, it follows then that $f$ is of class $C^\infty$.",,"['analysis', 'proof-verification', 'implicit-function-theorem']"
44,Prove $c_0$ is a banach space.,Prove  is a banach space.,c_0,"The subspace of null sequences $c_0$ consists of all sequences whose limit is zero.  Prove that $c_0$ is a closed subspace of $C$ (The space of convergent sequences), and so again a Banach space. There's something I don't understand. I know we have to prove that every Cauchy sequence on $c_0$ is convergent on $C$ in order to prove $c_0$ is closed on $C$. But, that Cauchy sequence will be a sequence of sequences? Because the elements of $C$ and $c_0$ are sequences. I'm really confused.","The subspace of null sequences $c_0$ consists of all sequences whose limit is zero.  Prove that $c_0$ is a closed subspace of $C$ (The space of convergent sequences), and so again a Banach space. There's something I don't understand. I know we have to prove that every Cauchy sequence on $c_0$ is convergent on $C$ in order to prove $c_0$ is closed on $C$. But, that Cauchy sequence will be a sequence of sequences? Because the elements of $C$ and $c_0$ are sequences. I'm really confused.",,"['analysis', 'functional-analysis', 'banach-spaces']"
45,Proving that the set of differentiable functions with $\left|f'(t)\right|\leq K$ is dense in the set of Lipschitz continuous functions?,Proving that the set of differentiable functions with  is dense in the set of Lipschitz continuous functions?,\left|f'(t)\right|\leq K,"Let $M_K$be the set of all continuous functions $f$ in $C_{[a,b]}$ satisfying a Lipschitz condition, i.e., the set of all $f$ such that $$ \left| f(t_1)-f(t_2)\right| \leq K \left| t_1-t_2\right| $$ for all $t_1,t_2 \in [a,b]$, where $K$ is a fixed positive number. I would like to prove that $M_K$ is the closure of the set of all differentiable functions on $[a,b]$ such that $\left|f'(t)\right|\leq K$. My Try: I have proved that $M_K$ is a closed set and know that any such differentiable function is in $M_K$. Now I need to prove that for any $f\in M_K$ there exists a sequence of such differentiable functions $\{g_n\}$ such that $g_n\rightarrow f$. Any help? By $C_{[a,b]}$, I mean the set of all continuous functions on the interval $[a,b]$ with distance: $$ d(f,g) = \max_{a\leq t\leq b} \left|f(t)-g(t)\right| $$","Let $M_K$be the set of all continuous functions $f$ in $C_{[a,b]}$ satisfying a Lipschitz condition, i.e., the set of all $f$ such that $$ \left| f(t_1)-f(t_2)\right| \leq K \left| t_1-t_2\right| $$ for all $t_1,t_2 \in [a,b]$, where $K$ is a fixed positive number. I would like to prove that $M_K$ is the closure of the set of all differentiable functions on $[a,b]$ such that $\left|f'(t)\right|\leq K$. My Try: I have proved that $M_K$ is a closed set and know that any such differentiable function is in $M_K$. Now I need to prove that for any $f\in M_K$ there exists a sequence of such differentiable functions $\{g_n\}$ such that $g_n\rightarrow f$. Any help? By $C_{[a,b]}$, I mean the set of all continuous functions on the interval $[a,b]$ with distance: $$ d(f,g) = \max_{a\leq t\leq b} \left|f(t)-g(t)\right| $$",,"['real-analysis', 'analysis']"
46,Characterization of subsets of $\mathbb{R}^n$ of the form $X+Y$,Characterization of subsets of  of the form,\mathbb{R}^n X+Y,"The following comes from the mathematical tripos exam at Cambridge: Let $X,Y \subset \mathbb{R}^n$, and define $X+Y = \{x+y : x \in X, y \in Y\}$ Prove or disprove each of the following: (i) If each of $X,Y$ is closed and bounded, $X+Y$ is closed and bounded. (ii) If $X$ is bounded and closed, and $Y$ is closed, $X+Y$ is closed. (iii) If $X$ and $Y$ are closed, $X+Y$ is closed. (iv) If $X$ is open and $Y$ is closed, $X+Y$ is open. Attempt: (i) If $X,Y$ are closed and bounded, then $\mathbb{R}^n \setminus X$ and $\mathbb{R}^n \setminus Y$ are open and unbounded. We need show $\mathbb{R}^n \setminus(X+Y)$ is also open and unbounded. Given any element $x+y \in X+Y$ we construct $B_{\tilde{\epsilon}}(x+y)$ in a natural way. Since there exists $\epsilon_1$ and $\epsilon_2$ so that $B_{\epsilon_1}(x) \subset \mathbb{R}^n \setminus X$ and $B_{\epsilon_2} (y) \subset \mathbb{R}^n \setminus Y$, we see that $$B_{\epsilon_1}(x) + B_{\epsilon_2}(y) \subset \mathbb{R}^n \setminus  (X+Y)$$ Let $\tilde{\epsilon}  = \min\{\epsilon_1,\epsilon_2\}$. By the triangle inequality, we have $$B_{\tilde{\epsilon}} (x+y) \subset \mathbb{R}^n \setminus (X+Y)$$ so that this set is open. The fact that both $\mathbb{R}^n \setminus X$ and $\mathbb{R}^n \setminus Y$ are unbounded implies that they are unbounded in at least one coordinate. We have two cases: Suppose $\mathbb{R}^n \setminus X$ is unbounded in $i$, $(x_1,\ldots x_i, \ldots, x_n)$ and suppose $\mathbb{R}^n \setminus Y$ is unbounded in $j \neq i$. Then, clearly the sum $(x_1+y_1,\ldots, x_i + y_i, \ldots, x_j + y_j, \ldots, x_n+y_n)$ is unbounded. Now, suppose both sets are unbounded in $i$. Then, call the $A$ set of $x_i$ and $B$ the set of $y_i$. Either $A = (a,\infty)$ or $A=(-\infty,a)$ or $A=(-\infty,a) \cup (a,\infty)$ for some $a$. Likewise for $B$ with some $b$. Checking all of these cases, we see that $A+B$ is unbounded and therefore $\mathbb{R}^n \setminus (X+Y)$ is open and unbounded and so $X+Y$ is closed and bounded.  We conclude that, for $\{X_i\}_{i=1}^{n}$ closed and bounded, $$\sum_{i=1}^{n} X_i$$ is also closed and bounded. (ii) Using the same argument as (i) (I think?) we can conclude that $X+Y$ is closed. Is it valid to do this component-wise and use the facts that $\sup(\pi_i(X+Y) = \sup(\pi_i(X)) + \sup(\pi_i(Y))$ and $\inf(\pi_i(X+Y)) = \inf(\pi_i(X)) + \inf(\pi_i(Y))$ and thus the sum is bounded above and below by $[\inf(\pi_i(X+Y)), \sup(\pi_i(X+Y)]$ and since $X$ and $Y$ are closed, we have $$\pi_i(X+Y) = [\inf(\pi_i(X+Y)), \sup(\pi_i(X+Y)]$$ Where $\pi_i: \mathbb{R}^n \to \mathbb{R}$ is the canonical projection onto $i$th coordinate. That is, each $i$th level-cut is closed, if we fix the other coordinates. (iv) Not sure where to begin. Any hints would be wonderful on all four parts. I'm pretty sure Bolzano-Weierstrass would simplify (i) quite a bit, but I wanted to try to use the closed argument in (i) in (ii) and (iii).","The following comes from the mathematical tripos exam at Cambridge: Let $X,Y \subset \mathbb{R}^n$, and define $X+Y = \{x+y : x \in X, y \in Y\}$ Prove or disprove each of the following: (i) If each of $X,Y$ is closed and bounded, $X+Y$ is closed and bounded. (ii) If $X$ is bounded and closed, and $Y$ is closed, $X+Y$ is closed. (iii) If $X$ and $Y$ are closed, $X+Y$ is closed. (iv) If $X$ is open and $Y$ is closed, $X+Y$ is open. Attempt: (i) If $X,Y$ are closed and bounded, then $\mathbb{R}^n \setminus X$ and $\mathbb{R}^n \setminus Y$ are open and unbounded. We need show $\mathbb{R}^n \setminus(X+Y)$ is also open and unbounded. Given any element $x+y \in X+Y$ we construct $B_{\tilde{\epsilon}}(x+y)$ in a natural way. Since there exists $\epsilon_1$ and $\epsilon_2$ so that $B_{\epsilon_1}(x) \subset \mathbb{R}^n \setminus X$ and $B_{\epsilon_2} (y) \subset \mathbb{R}^n \setminus Y$, we see that $$B_{\epsilon_1}(x) + B_{\epsilon_2}(y) \subset \mathbb{R}^n \setminus  (X+Y)$$ Let $\tilde{\epsilon}  = \min\{\epsilon_1,\epsilon_2\}$. By the triangle inequality, we have $$B_{\tilde{\epsilon}} (x+y) \subset \mathbb{R}^n \setminus (X+Y)$$ so that this set is open. The fact that both $\mathbb{R}^n \setminus X$ and $\mathbb{R}^n \setminus Y$ are unbounded implies that they are unbounded in at least one coordinate. We have two cases: Suppose $\mathbb{R}^n \setminus X$ is unbounded in $i$, $(x_1,\ldots x_i, \ldots, x_n)$ and suppose $\mathbb{R}^n \setminus Y$ is unbounded in $j \neq i$. Then, clearly the sum $(x_1+y_1,\ldots, x_i + y_i, \ldots, x_j + y_j, \ldots, x_n+y_n)$ is unbounded. Now, suppose both sets are unbounded in $i$. Then, call the $A$ set of $x_i$ and $B$ the set of $y_i$. Either $A = (a,\infty)$ or $A=(-\infty,a)$ or $A=(-\infty,a) \cup (a,\infty)$ for some $a$. Likewise for $B$ with some $b$. Checking all of these cases, we see that $A+B$ is unbounded and therefore $\mathbb{R}^n \setminus (X+Y)$ is open and unbounded and so $X+Y$ is closed and bounded.  We conclude that, for $\{X_i\}_{i=1}^{n}$ closed and bounded, $$\sum_{i=1}^{n} X_i$$ is also closed and bounded. (ii) Using the same argument as (i) (I think?) we can conclude that $X+Y$ is closed. Is it valid to do this component-wise and use the facts that $\sup(\pi_i(X+Y) = \sup(\pi_i(X)) + \sup(\pi_i(Y))$ and $\inf(\pi_i(X+Y)) = \inf(\pi_i(X)) + \inf(\pi_i(Y))$ and thus the sum is bounded above and below by $[\inf(\pi_i(X+Y)), \sup(\pi_i(X+Y)]$ and since $X$ and $Y$ are closed, we have $$\pi_i(X+Y) = [\inf(\pi_i(X+Y)), \sup(\pi_i(X+Y)]$$ Where $\pi_i: \mathbb{R}^n \to \mathbb{R}$ is the canonical projection onto $i$th coordinate. That is, each $i$th level-cut is closed, if we fix the other coordinates. (iv) Not sure where to begin. Any hints would be wonderful on all four parts. I'm pretty sure Bolzano-Weierstrass would simplify (i) quite a bit, but I wanted to try to use the closed argument in (i) in (ii) and (iii).",,"['real-analysis', 'analysis', 'proof-verification', 'proof-writing']"
47,Connections between Cesaro summation and Borel summation of series,Connections between Cesaro summation and Borel summation of series,,"Let  $\sum_{n=0}^\infty x_n$ be a given series of numbers, let $S_n=\sum_{k=0}^n x_k$, $n=0,1,2,...$, let $g\in \mathbb R$. We say that this series is convergent to $g$ in the sense of Cesaro if $$ \frac{S_0+S_1+..+S_n}{n+1}\rightarrow g $$ as $n\rightarrow \infty$. We say that this series is convergent to $g$ in the sense of  Borel  if $$ e^{-x}\sum_{n=0}^\infty S_n \frac{x^n}{n!} \rightarrow g $$  as  $x\rightarrow +\infty$. What is connection between this convergence: Is it true that if a series is convergent to some $g$ in the Cesaro sense then it is convergent to the same $g$ in the Borel sense? Is it true that if a series is convergent simultanuously in  both  Borel and Cesaro sense, to $g$ and $h$ respectively, then $g=h$?","Let  $\sum_{n=0}^\infty x_n$ be a given series of numbers, let $S_n=\sum_{k=0}^n x_k$, $n=0,1,2,...$, let $g\in \mathbb R$. We say that this series is convergent to $g$ in the sense of Cesaro if $$ \frac{S_0+S_1+..+S_n}{n+1}\rightarrow g $$ as $n\rightarrow \infty$. We say that this series is convergent to $g$ in the sense of  Borel  if $$ e^{-x}\sum_{n=0}^\infty S_n \frac{x^n}{n!} \rightarrow g $$  as  $x\rightarrow +\infty$. What is connection between this convergence: Is it true that if a series is convergent to some $g$ in the Cesaro sense then it is convergent to the same $g$ in the Borel sense? Is it true that if a series is convergent simultanuously in  both  Borel and Cesaro sense, to $g$ and $h$ respectively, then $g=h$?",,['analysis']
48,For which values of $x$ is the following series convergent: $\sum_0^\infty \frac{1}{n^x}\arctan\Bigl(\bigl(\frac{x-4}{x-1}\bigr)^n\Bigr)$,For which values of  is the following series convergent:,x \sum_0^\infty \frac{1}{n^x}\arctan\Bigl(\bigl(\frac{x-4}{x-1}\bigr)^n\Bigr),For which values of $x$ is the following series convergent? $$\sum_{n=1}^{\infty} \frac{1}{n^x}\arctan\Biggl(\biggl(\frac{x-4}{x-1}\biggr)^n\Biggr)$$,For which values of $x$ is the following series convergent? $$\sum_{n=1}^{\infty} \frac{1}{n^x}\arctan\Biggl(\biggl(\frac{x-4}{x-1}\biggr)^n\Biggr)$$,,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis']"
49,A necessary condition to $F'(x)=f(x)$ for a continuous function $f$,A necessary condition to  for a continuous function,F'(x)=f(x) f,"Theorem: Consider , $$F(x)=\int_a^xf(t)\,dt$$ If the function $f:[a,b]\to \mathbb R$ is continuous then , $F(x)$ is differentiable and $F'(x)=f(x).$ I know that the continuity condition of $f$ is sufficient condition. That means there exists a discontinuous function $f$ for which this $F'(x)=f(x)$ . My Question: Does there exist a necessary condition for this ? $$OR$$ After imposing which extra condition on $f$ it is necessary that $F'(x)=f(x)$ ?","Theorem: Consider , If the function is continuous then , is differentiable and I know that the continuity condition of is sufficient condition. That means there exists a discontinuous function for which this . My Question: Does there exist a necessary condition for this ? After imposing which extra condition on it is necessary that ?","F(x)=\int_a^xf(t)\,dt f:[a,b]\to \mathbb R F(x) F'(x)=f(x). f f F'(x)=f(x) OR f F'(x)=f(x)","['real-analysis', 'integration', 'analysis']"
50,Please check my demonstration of de l'hopital's rule,Please check my demonstration of de l'hopital's rule,,"I have demostrate the de l'hopital theorem but in some steps I'm not 100% sure; The theorem I demostrate is for: $\lim_{x\rightarrow a+}  \frac{f'(x)}{g'(x)}=L \implies\lim_{x\rightarrow a+}  \frac{f(x)}{g(x)} $ with $ g,f \rightarrow \infty $ Here I report only the demostration not the hypothesis. Demostration For a fixed $\epsilon \ \exists \ x_1 \ | \ \forall \ \xi \in (a,x_1) \ L-\epsilon<\frac{f'(\xi)}{g'(\xi)}<L+\epsilon $ For each x $\in$ $(a,x_1)$ for cauchy theorem$\frac{f'(\xi)}{g'(\xi)}=\frac{f(x)-f(x_1)}{g(x)-g(x_1)}$; So :  $L-\epsilon<\frac{f'(\xi)}{g'(\xi)}=\frac{f(x)-f(x_1)}{g(x)-g(x_1)}<L+\epsilon $  $\\\\$ with $x<\xi<x_1$ $L-\epsilon<\frac{f'(\xi)}{g'(\xi)}=\frac{f(x)(1-\frac{f(x_1)}{f(x)})}{g(x)(1-\frac{f(g_1)}{g(x)})}<L+\epsilon $ I call $\beta=\frac{f(x)(1-\frac{f(x_1)}{f(x)})}{g(x)(1-\frac{f(g_1)}{g(x)})}$ and I assume  that $f,g\rightarrow\infty$ : I consider the same $\epsilon$ used at the beginning so $ \exists \ \delta \ | \ \forall \ x \in (a,a+\delta) \ 1-\epsilon<\beta<1+\epsilon $ So for $a<x<min\ \left \{ a+\delta,x_1 \right \} \ $: $(L-\epsilon)(1-\epsilon)<\frac{f(x)}{g(x)}<(L+\epsilon)(1+\epsilon) $","I have demostrate the de l'hopital theorem but in some steps I'm not 100% sure; The theorem I demostrate is for: $\lim_{x\rightarrow a+}  \frac{f'(x)}{g'(x)}=L \implies\lim_{x\rightarrow a+}  \frac{f(x)}{g(x)} $ with $ g,f \rightarrow \infty $ Here I report only the demostration not the hypothesis. Demostration For a fixed $\epsilon \ \exists \ x_1 \ | \ \forall \ \xi \in (a,x_1) \ L-\epsilon<\frac{f'(\xi)}{g'(\xi)}<L+\epsilon $ For each x $\in$ $(a,x_1)$ for cauchy theorem$\frac{f'(\xi)}{g'(\xi)}=\frac{f(x)-f(x_1)}{g(x)-g(x_1)}$; So :  $L-\epsilon<\frac{f'(\xi)}{g'(\xi)}=\frac{f(x)-f(x_1)}{g(x)-g(x_1)}<L+\epsilon $  $\\\\$ with $x<\xi<x_1$ $L-\epsilon<\frac{f'(\xi)}{g'(\xi)}=\frac{f(x)(1-\frac{f(x_1)}{f(x)})}{g(x)(1-\frac{f(g_1)}{g(x)})}<L+\epsilon $ I call $\beta=\frac{f(x)(1-\frac{f(x_1)}{f(x)})}{g(x)(1-\frac{f(g_1)}{g(x)})}$ and I assume  that $f,g\rightarrow\infty$ : I consider the same $\epsilon$ used at the beginning so $ \exists \ \delta \ | \ \forall \ x \in (a,a+\delta) \ 1-\epsilon<\beta<1+\epsilon $ So for $a<x<min\ \left \{ a+\delta,x_1 \right \} \ $: $(L-\epsilon)(1-\epsilon)<\frac{f(x)}{g(x)}<(L+\epsilon)(1+\epsilon) $",,"['calculus', 'real-analysis', 'analysis', 'proof-verification']"
51,Why does $a\cdot \cos(kx)+b\cdot\sin(kx)=\sqrt{a^2+b^2}\sin(kx+\phi)$ hold?,Why does  hold?,a\cdot \cos(kx)+b\cdot\sin(kx)=\sqrt{a^2+b^2}\sin(kx+\phi),"I've just bought a book to learn how Laplace, Fourier- and z-transformation works and stumbled over \begin{align}  a_k\cos(kx)+b_k\sin(kx)&=A_k \sin(kx+\varphi_k)\\ &= A_k[\sin(kx)\cos(\varphi_k)+\cos(kx)\sin(\varphi_k)]  \end{align}   Equating coefficients leads to   $A_k \cos(\varphi) = b_k$ and $A_k \sin(\varphi)=a_k$. It follows:   $A_k = \sqrt{a_k^2 + b_k^2}$ and $\tan(\varphi_k) = \frac{a_k}{b_k}$ I understand the part with equating the coefficients and why $\tan(\varphi_k) = \frac{a_k}{b_k}$ is true (because $\tan(\alpha) = \frac{\sin{\alpha}}{\cos(\alpha)}$ if $\cos(\alpha) \neq 0$). But I don't understand how they follow $A_k = \sqrt{a_k^2 + b_k^2}$. My thought was \begin{align} A_k \cos(\varphi) = b_k \land A_k \sin(\varphi)=a_k\\ \Rightarrow (A_k \cos(\varphi))^2 + (A_k \sin(\varphi))^2 = b_k^2 + a_k^2\\ \Leftrightarrow A_k^2 (\cos(\varphi)^2 + \sin(\varphi)^2) = a_k^2 + b_k^2\\ \Leftrightarrow A_k^2 = a_k^2 + b_k^2\\ \Leftrightarrow A_k = \pm \sqrt{a_k^2 + b_k^2}\\ \end{align} Why is $A_k = - \sqrt{a_k^2 + b_k^2}$ not possible?","I've just bought a book to learn how Laplace, Fourier- and z-transformation works and stumbled over \begin{align}  a_k\cos(kx)+b_k\sin(kx)&=A_k \sin(kx+\varphi_k)\\ &= A_k[\sin(kx)\cos(\varphi_k)+\cos(kx)\sin(\varphi_k)]  \end{align}   Equating coefficients leads to   $A_k \cos(\varphi) = b_k$ and $A_k \sin(\varphi)=a_k$. It follows:   $A_k = \sqrt{a_k^2 + b_k^2}$ and $\tan(\varphi_k) = \frac{a_k}{b_k}$ I understand the part with equating the coefficients and why $\tan(\varphi_k) = \frac{a_k}{b_k}$ is true (because $\tan(\alpha) = \frac{\sin{\alpha}}{\cos(\alpha)}$ if $\cos(\alpha) \neq 0$). But I don't understand how they follow $A_k = \sqrt{a_k^2 + b_k^2}$. My thought was \begin{align} A_k \cos(\varphi) = b_k \land A_k \sin(\varphi)=a_k\\ \Rightarrow (A_k \cos(\varphi))^2 + (A_k \sin(\varphi))^2 = b_k^2 + a_k^2\\ \Leftrightarrow A_k^2 (\cos(\varphi)^2 + \sin(\varphi)^2) = a_k^2 + b_k^2\\ \Leftrightarrow A_k^2 = a_k^2 + b_k^2\\ \Leftrightarrow A_k = \pm \sqrt{a_k^2 + b_k^2}\\ \end{align} Why is $A_k = - \sqrt{a_k^2 + b_k^2}$ not possible?",,"['analysis', 'trigonometry']"
52,"Example 5, Sec. 24 in Munkres' TOPOLOGY, 2nd ed: Is this map always continuous?","Example 5, Sec. 24 in Munkres' TOPOLOGY, 2nd ed: Is this map always continuous?",,"Let $(X, \Vert \cdot \Vert)$ be a given normed space that has elements other than the zero vector $\theta_X$. And let $T \colon X-\{\theta_X \} \to X$ be defined by  $$T(x) \colon= \frac{1}{\Vert x \Vert} x \ \ \ \mbox{ for all } \ x \in X, \ x \neq \theta_X.$$ Then how to determine if $T$ is continuous? Is $T$ always continuous or discontinuous? If $X = \mathbb{R}^n$, then $T$ is of course continuous. Right? After some thought: Let $x$ be an arbitrary element of $X-\{\theta_X\}$. Let $x_n$ be a sequence in $X-\{\theta_X\}$ such that $x_n$ converges to $x$. Then the sequence $\Vert x_n \Vert$ of non-zero real numbers converges in the usual metric space $\mathbb{R}$ to the non-zero real number $\Vert x \Vert$. So the sequence $\frac{1}{\Vert x_n \Vert}$ of reciprocals converges in $\mathbb{R}$ to $\frac{1}{\Vert x \Vert}$. Therefor, the image sequence $T(x_n)$ converges in $X$ to $T(x)$. Hence $T$ is continuous at $x$. Is there anything wrong about this reasoning?","Let $(X, \Vert \cdot \Vert)$ be a given normed space that has elements other than the zero vector $\theta_X$. And let $T \colon X-\{\theta_X \} \to X$ be defined by  $$T(x) \colon= \frac{1}{\Vert x \Vert} x \ \ \ \mbox{ for all } \ x \in X, \ x \neq \theta_X.$$ Then how to determine if $T$ is continuous? Is $T$ always continuous or discontinuous? If $X = \mathbb{R}^n$, then $T$ is of course continuous. Right? After some thought: Let $x$ be an arbitrary element of $X-\{\theta_X\}$. Let $x_n$ be a sequence in $X-\{\theta_X\}$ such that $x_n$ converges to $x$. Then the sequence $\Vert x_n \Vert$ of non-zero real numbers converges in the usual metric space $\mathbb{R}$ to the non-zero real number $\Vert x \Vert$. So the sequence $\frac{1}{\Vert x_n \Vert}$ of reciprocals converges in $\mathbb{R}$ to $\frac{1}{\Vert x \Vert}$. Therefor, the image sequence $T(x_n)$ converges in $X$ to $T(x)$. Hence $T$ is continuous at $x$. Is there anything wrong about this reasoning?",,"['real-analysis', 'general-topology', 'analysis', 'continuity', 'normed-spaces']"
53,Derivative limit is uniformly convergent,Derivative limit is uniformly convergent,,"If we consider the sequence of functions: $g_{n}(x)=\frac{f(x+h_{n})-f(x)}{h_{n}}$ where $h_{n}>0$ is a sequence of real numbers converging to $0$, and $f$ is a $C^{1}$ function. How can you show that the sequence {$g_{n}$} is uniformly convergent?","If we consider the sequence of functions: $g_{n}(x)=\frac{f(x+h_{n})-f(x)}{h_{n}}$ where $h_{n}>0$ is a sequence of real numbers converging to $0$, and $f$ is a $C^{1}$ function. How can you show that the sequence {$g_{n}$} is uniformly convergent?",,"['analysis', 'derivatives', 'convergence-divergence']"
54,"If a sequence $f(x_n)$ goes to its minimum, will $x_n$ go to the point at which $f$ achieve the minimum?","If a sequence  goes to its minimum, will  go to the point at which  achieve the minimum?",f(x_n) x_n f,"I have a continuous function $f$ that is defined on a compact set. And $f(x_0)$ is its minimum. If I have a sequence $x_n$ such that $f(x_n)\to f(x_0)$, how can I show that $x_n\to x_0$? I tried proving by contradiction but I got lost. Anyone could help me? Thank you very much. Update:$f$ only achieves minimum at $x_0$.","I have a continuous function $f$ that is defined on a compact set. And $f(x_0)$ is its minimum. If I have a sequence $x_n$ such that $f(x_n)\to f(x_0)$, how can I show that $x_n\to x_0$? I tried proving by contradiction but I got lost. Anyone could help me? Thank you very much. Update:$f$ only achieves minimum at $x_0$.",,"['real-analysis', 'analysis']"
55,Geometric Interpretation of Fractional Derivatives,Geometric Interpretation of Fractional Derivatives,,I was looking for a geometrical interpretations of fractional derivatives and fractional integrals . I would be glad to see any kind of intuitive and preferably visual interpretation of the objects of fractional calculus . Can anyone recommend the source or share personal opinion on the topic?,I was looking for a geometrical interpretations of fractional derivatives and fractional integrals . I would be glad to see any kind of intuitive and preferably visual interpretation of the objects of fractional calculus . Can anyone recommend the source or share personal opinion on the topic?,,"['calculus', 'analysis', 'fractional-calculus', 'geometric-interpretation']"
56,"Show that a sequence (($x_n, y_n$)) in $X \times Y$ is $e$-Cauchy if the component sequences ($x_n$) and ($y_n$) are $d_X$-Cauchy and $d_Y$ -Cauchy.",Show that a sequence (()) in  is -Cauchy if the component sequences () and () are -Cauchy and  -Cauchy.,"x_n, y_n X \times Y e x_n y_n d_X d_Y","How to solve this? Let $(X, d_X)$ and $(Y, d_Y)$ be metric spaces and let $e$ be a product metric on $X\times Y$. Show that a sequence (($x_n, y_n$)) in $X \times Y$ is $e$-Cauchy if the component sequences ($x_n$) and ($y_n$) are $d_X$-Cauchy and $d_Y$ -Cauchy, respectively.","How to solve this? Let $(X, d_X)$ and $(Y, d_Y)$ be metric spaces and let $e$ be a product metric on $X\times Y$. Show that a sequence (($x_n, y_n$)) in $X \times Y$ is $e$-Cauchy if the component sequences ($x_n$) and ($y_n$) are $d_X$-Cauchy and $d_Y$ -Cauchy, respectively.",,"['real-analysis', 'general-topology', 'analysis']"
57,"Injectivity of $T:C[0,1]\rightarrow C[0,1]$ where $T(x)(t):=\int_0^t x(s)ds $",Injectivity of  where,"T:C[0,1]\rightarrow C[0,1] T(x)(t):=\int_0^t x(s)ds ","Prob. 2.7-9 in Erwin Kreyszig's ""Introductory Functional Analysis with Applications"" : Is this map injective? Let $C[0,1]$ denote the normed space of all (real or complex-valued) functions defined and continuous on the closed interval $[0,1]$ on the real line, with the maximum norm given by  $$ \Vert x \Vert_{C[0,1]} = \max_{t \in [0,1]} \vert x(t) \vert \ \ \ \mbox{ for all } \ x \in C[0,1]. $$ Let $T \colon C[0,1] \to C[0,1]$ be defined as follows: for each $x \in C[0,1]$, let  $T(x) \colon [0,1] \to K$, where $K = \mathbb{R}$ or $\mathbb{C}$, be defined by $$  \left( T(x) \right)(t) \colon= \ \int_0^t \ x(\tau) \ \mathrm{d} \tau  \ \ \ \mbox{ for all } \ t \in [0,1]. $$ Then $T$ is a bounded linear operator with range consisting of all those continuously differentiable functions on $[0,1]$ that vanish at $t=0$. Am I right? Is $T$ injective? How to determine if $T$ is injective or not?","Prob. 2.7-9 in Erwin Kreyszig's ""Introductory Functional Analysis with Applications"" : Is this map injective? Let $C[0,1]$ denote the normed space of all (real or complex-valued) functions defined and continuous on the closed interval $[0,1]$ on the real line, with the maximum norm given by  $$ \Vert x \Vert_{C[0,1]} = \max_{t \in [0,1]} \vert x(t) \vert \ \ \ \mbox{ for all } \ x \in C[0,1]. $$ Let $T \colon C[0,1] \to C[0,1]$ be defined as follows: for each $x \in C[0,1]$, let  $T(x) \colon [0,1] \to K$, where $K = \mathbb{R}$ or $\mathbb{C}$, be defined by $$  \left( T(x) \right)(t) \colon= \ \int_0^t \ x(\tau) \ \mathrm{d} \tau  \ \ \ \mbox{ for all } \ t \in [0,1]. $$ Then $T$ is a bounded linear operator with range consisting of all those continuously differentiable functions on $[0,1]$ that vanish at $t=0$. Am I right? Is $T$ injective? How to determine if $T$ is injective or not?",,"['real-analysis', 'analysis', 'functional-analysis', 'operator-theory', 'normed-spaces']"
58,How to find the limit properly?,How to find the limit properly?,,I would like to find or solve the limit of: $$\lim_{n \to \infty} \frac{80^{(n+1)/4}}{37^{(2n+3)/4}}$$ My idea was somehow non-intuitive: $$\lim_{n \to \infty} \frac{80^{(n+1)/4}}{37^{(2n+3)/4}} \leq  \lim_{n \to \infty} \frac{3^{4(n+1)/4}} {6^{2(2n+3)/4}}= \lim_{n \to \infty}(\frac{3^{4n+4}}{6^{4n+6}})^{1/4} = \lim_{n \to \infty}(\frac{1^{4n+4}}{3^{4n+6}})^{1/4}=0$$ But it seems wrong somehow...,I would like to find or solve the limit of: $$\lim_{n \to \infty} \frac{80^{(n+1)/4}}{37^{(2n+3)/4}}$$ My idea was somehow non-intuitive: $$\lim_{n \to \infty} \frac{80^{(n+1)/4}}{37^{(2n+3)/4}} \leq  \lim_{n \to \infty} \frac{3^{4(n+1)/4}} {6^{2(2n+3)/4}}= \lim_{n \to \infty}(\frac{3^{4n+4}}{6^{4n+6}})^{1/4} = \lim_{n \to \infty}(\frac{1^{4n+4}}{3^{4n+6}})^{1/4}=0$$ But it seems wrong somehow...,,['analysis']
59,Convergent series? Gamma/power function,Convergent series? Gamma/power function,,"Is it true to use as a general rule of thumb that the Gamma function always ""kills"" power function in a series? I mean: $$\sum_{n=1}^{\infty} \frac{C^n}{\Gamma(n)^p}<\infty$$ no matter the constant $C>0$ and power $p>0$ in the gamma function as long as they are fixed and independent of $n$. Am I correct? Now I wonder what happens with $$\sum_{n=1}^{\infty} C^n \frac{\Gamma( a_n )^p}{\Gamma(b_n)^q}\, ?$$ How can we decide when the series is convergent? For instance if $a_n=b_n$ and $q>p>0$ I guess the series converges right? Is there a general rule/criterion one can use here? Actually, I have $a_n= an$ and $b_n=bn+c$ with $a,b,c>0$. I mean $$\sum_{n=1}^{\infty} C^n \frac{\Gamma( an )^p}{\Gamma(bn+c)^q}\, ?$$ What would be the conditions on $p,q,a,b,c$ so that the sum converges? Thanks a lot! :D","Is it true to use as a general rule of thumb that the Gamma function always ""kills"" power function in a series? I mean: $$\sum_{n=1}^{\infty} \frac{C^n}{\Gamma(n)^p}<\infty$$ no matter the constant $C>0$ and power $p>0$ in the gamma function as long as they are fixed and independent of $n$. Am I correct? Now I wonder what happens with $$\sum_{n=1}^{\infty} C^n \frac{\Gamma( a_n )^p}{\Gamma(b_n)^q}\, ?$$ How can we decide when the series is convergent? For instance if $a_n=b_n$ and $q>p>0$ I guess the series converges right? Is there a general rule/criterion one can use here? Actually, I have $a_n= an$ and $b_n=bn+c$ with $a,b,c>0$. I mean $$\sum_{n=1}^{\infty} C^n \frac{\Gamma( an )^p}{\Gamma(bn+c)^q}\, ?$$ What would be the conditions on $p,q,a,b,c$ so that the sum converges? Thanks a lot! :D",,"['real-analysis', 'sequences-and-series', 'analysis', 'summation', 'divergent-series']"
60,$\int_{a}^{b} f(x) = \int_{a}^{b}A dx - \int_{a}^{b} B dx$,,\int_{a}^{b} f(x) = \int_{a}^{b}A dx - \int_{a}^{b} B dx,"Let $f:[a,b] \rightarrow \mathbb{R}$ be an integrable function. Define A = \begin{cases} f(x), & \mbox{if } f(x) \ge 0\mbox{} \\ 0, & \mbox{if } f(x) < 0\mbox{ } \end{cases} Define B = \begin{cases} 0, & \mbox{if } f(x) \ge 0\mbox{} \\ f(x), & \mbox{if } f(x) < 0\mbox{ } \end{cases} How to prove that $A,B:[a,b] \rightarrow \mathbb{R}$  are integrable and  $$\int_{a}^{b} f(x) = \int_{a}^{b}A dx - \int_{a}^{b} B dx$$ Please help!","Let $f:[a,b] \rightarrow \mathbb{R}$ be an integrable function. Define A = \begin{cases} f(x), & \mbox{if } f(x) \ge 0\mbox{} \\ 0, & \mbox{if } f(x) < 0\mbox{ } \end{cases} Define B = \begin{cases} 0, & \mbox{if } f(x) \ge 0\mbox{} \\ f(x), & \mbox{if } f(x) < 0\mbox{ } \end{cases} How to prove that $A,B:[a,b] \rightarrow \mathbb{R}$  are integrable and  $$\int_{a}^{b} f(x) = \int_{a}^{b}A dx - \int_{a}^{b} B dx$$ Please help!",,"['calculus', 'integration', 'analysis']"
61,Limit of sequences and integers,Limit of sequences and integers,,"If $a$ is a non zero real number , $x \ge 1$ is a rational number and $(r_n)$ is a sequence of positive integers such that $\lim _{n \to \infty}ax^n-r_n=0$ , then is it true that $x$ is an integer ?","If $a$ is a non zero real number , $x \ge 1$ is a rational number and $(r_n)$ is a sequence of positive integers such that $\lim _{n \to \infty}ax^n-r_n=0$ , then is it true that $x$ is an integer ?",,"['real-analysis', 'sequences-and-series']"
62,Give the result analytically $\int_{0}^{\pi} \sin^{2m} \theta \ln{\sin\theta} \cos{2n\theta} d\theta$,Give the result analytically,\int_{0}^{\pi} \sin^{2m} \theta \ln{\sin\theta} \cos{2n\theta} d\theta,"Question as title showed, where m and n are positive integers. Many thanks in advance.","Question as title showed, where m and n are positive integers. Many thanks in advance.",,"['calculus', 'analysis']"
63,"Show that $2\nabla \sqrt f\,+\,x \sqrt f=0$ (a.e.). $\implies$ $\sqrt f\in \mathcal C_0$. (Derivatives are in weak sense)",Show that  (a.e.).  . (Derivatives are in weak sense),"2\nabla \sqrt f\,+\,x \sqrt f=0 \implies \sqrt f\in \mathcal C_0","Show that $2\nabla \sqrt f\,+\,x \sqrt f=0$ (a.e.). $\implies$ $\sqrt f\in \mathcal C_0$. (Derivatives are in weak sense) Given that $f\in L^1(\mathbb R^d),f\geq 0,\int_{\mathbb R^d}f=1, \int_{\mathbb R^d}f(x)|x|^2\leq d$ As $f\in L^1(\mathbb R^d)$ so $\lim_{|x|\rightarrow \infty}\sqrt f(x)=0$ only thing remaining to show is continuity. Author feels that it has to be shown by using Sobolev inequality and bootstrap arguments. Regards. Harish","Show that $2\nabla \sqrt f\,+\,x \sqrt f=0$ (a.e.). $\implies$ $\sqrt f\in \mathcal C_0$. (Derivatives are in weak sense) Given that $f\in L^1(\mathbb R^d),f\geq 0,\int_{\mathbb R^d}f=1, \int_{\mathbb R^d}f(x)|x|^2\leq d$ As $f\in L^1(\mathbb R^d)$ so $\lim_{|x|\rightarrow \infty}\sqrt f(x)=0$ only thing remaining to show is continuity. Author feels that it has to be shown by using Sobolev inequality and bootstrap arguments. Regards. Harish",,"['analysis', 'partial-differential-equations', 'sobolev-spaces']"
64,Classes of measures that are closed under multiplication,Classes of measures that are closed under multiplication,,"Consider the space $\mathcal M$ of all finite complex Borel measures on a segment with norm $\|\mu\|=\int d\,|\mu|$. Assume that a norm-closed linear subspace $\mathcal M_0$ of $\mathcal M$ has the following property: if $\mu\in\mathcal M_0$ and $f\in L^1(|\mu|)$, then $f\mu\in \mathcal M_0$. Can $\mathcal M_0$ be characterized as the family of all measures that vanish on a certain fixed collection of subsets of the segment? Example : the class of all measures $\mu$ such that $d\mu=f\,dx$ for some $f\in L^1$ coincides with the class of measures that vanish on all subsets of zero Lebesgue measure. Update : For any fixed measure $\mu$, one can define $\mathcal M_0$ as the class of all measures that are absolutely continuous wrt $\mu$. Then the class of subsets in question is the class of subsets of zero $\mu$-measure.","Consider the space $\mathcal M$ of all finite complex Borel measures on a segment with norm $\|\mu\|=\int d\,|\mu|$. Assume that a norm-closed linear subspace $\mathcal M_0$ of $\mathcal M$ has the following property: if $\mu\in\mathcal M_0$ and $f\in L^1(|\mu|)$, then $f\mu\in \mathcal M_0$. Can $\mathcal M_0$ be characterized as the family of all measures that vanish on a certain fixed collection of subsets of the segment? Example : the class of all measures $\mu$ such that $d\mu=f\,dx$ for some $f\in L^1$ coincides with the class of measures that vanish on all subsets of zero Lebesgue measure. Update : For any fixed measure $\mu$, one can define $\mathcal M_0$ as the class of all measures that are absolutely continuous wrt $\mu$. Then the class of subsets in question is the class of subsets of zero $\mu$-measure.",,"['analysis', 'functional-analysis', 'measure-theory']"
65,Finding the tangent line through the origin,Finding the tangent line through the origin,,"Find the tangent line to: $$f(x) = \sqrt{x-1}$$ that passes through the origin $(0, 0)$. $$f'(x) = \frac{1}{2\sqrt{x-1}}$$ The line will be tangent at $(a, b)$ so then: $$f'(a) = \frac{1}{2\sqrt{a-1}}$$ Which is the slope. $$y - \sqrt{a-1} = \frac{1}{2\sqrt{a-1}}(x - a)$$ Through $(0, 0)$ $$\sqrt{a-1} = \frac{-a}{2\sqrt{a-1}}$$ $$a-1 = -\frac{a}{2}$$ Am I heading the RIGHT direction? I dont need an answer, just advice.","Find the tangent line to: $$f(x) = \sqrt{x-1}$$ that passes through the origin $(0, 0)$. $$f'(x) = \frac{1}{2\sqrt{x-1}}$$ The line will be tangent at $(a, b)$ so then: $$f'(a) = \frac{1}{2\sqrt{a-1}}$$ Which is the slope. $$y - \sqrt{a-1} = \frac{1}{2\sqrt{a-1}}(x - a)$$ Through $(0, 0)$ $$\sqrt{a-1} = \frac{-a}{2\sqrt{a-1}}$$ $$a-1 = -\frac{a}{2}$$ Am I heading the RIGHT direction? I dont need an answer, just advice.",,"['calculus', 'real-analysis', 'analysis', 'derivatives']"
66,"$\int_0^\infty f^k (t)\cdot t^j \,dt$",,"\int_0^\infty f^k (t)\cdot t^j \,dt","Compute the $$\int_0^\infty f^{(k)}(t)\cdot t^j \,dt$$ for $$0\le j\le k$$ function values of f   $$(f\in C_0 ^{\infty}[0,\infty))$$ Any ideas how to compute this integral?","Compute the $$\int_0^\infty f^{(k)}(t)\cdot t^j \,dt$$ for $$0\le j\le k$$ function values of f   $$(f\in C_0 ^{\infty}[0,\infty))$$ Any ideas how to compute this integral?",,"['integration', 'analysis', 'fourier-analysis']"
67,Derivation of multivariable functions,Derivation of multivariable functions,,"I know that the derivative of $f(x)$ defined as: $\, f(x)=\dfrac{1}{2} \cdot \left|\left|g(x) \right|\right|^2_2$ is $\nabla f(x) = J_g(X)^T \cdot g(x)$ where $g:\mathbb{R}^n \rightarrow \mathbb{R}^m$, $f:\mathbb{R}^n \rightarrow \mathbb{R}$, and $\left|\left|x \right|\right|_p$ is the $p$-norm of $x$ : $\left|\left|x \right|\right|_p=\left( \sum\limits_{i=1}^n \left|x_i \right|^p \right)^\frac{1}{p}$ First of all why is the norm sign removed after derivation? Secondly, what is the derivative in for example this case where the norm and exponent differ: $f(x)=\dfrac{1}{2} \cdot \left|\left|g(x) \right|\right|^8_3$ Could you please explain the reason behind the two cases?","I know that the derivative of $f(x)$ defined as: $\, f(x)=\dfrac{1}{2} \cdot \left|\left|g(x) \right|\right|^2_2$ is $\nabla f(x) = J_g(X)^T \cdot g(x)$ where $g:\mathbb{R}^n \rightarrow \mathbb{R}^m$, $f:\mathbb{R}^n \rightarrow \mathbb{R}$, and $\left|\left|x \right|\right|_p$ is the $p$-norm of $x$ : $\left|\left|x \right|\right|_p=\left( \sum\limits_{i=1}^n \left|x_i \right|^p \right)^\frac{1}{p}$ First of all why is the norm sign removed after derivation? Secondly, what is the derivative in for example this case where the norm and exponent differ: $f(x)=\dfrac{1}{2} \cdot \left|\left|g(x) \right|\right|^8_3$ Could you please explain the reason behind the two cases?",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'derivatives']"
68,How to find the norm of $ \Lambda x = \sum_{n=1}^{\infty} \frac{x_n}{n\sqrt{6}}$ in $\ell^2$,How to find the norm of  in, \Lambda x = \sum_{n=1}^{\infty} \frac{x_n}{n\sqrt{6}} \ell^2,"Suppose that $(x_n)$ is a sequence in $\ell^2$, i.e. $\displaystyle \sum_{i=1}^{\infty} x_n^2 < \infty$. Define: $$\Lambda x = \sum_{n=1}^{\infty} \frac{x_n}{n\sqrt{6}}$$ Find $\| \Lambda \|_2$ Using the Cauchy-Schwarz inequality, it's easy to see that for any sequence $(x_n)$ in $\ell^2$ such that $\|(x_n)\|=1$, we have $\displaystyle |\Lambda x| \leq \frac{\pi}{6}$: $$|\Lambda x|^2 = |\sum_{n=1}^{\infty} \frac{x_n}{n\sqrt{6}}|^2 \leq |\sum_{n=1}^{\infty} x_n^2|\cdot|\sum_{n=1}^{\infty}\frac{1}{6n^2}|=\frac{\pi^2}{36}$$ How can I show that $\| \Lambda \|$ is indeed equal to $\displaystyle \frac{\pi}{6}$?","Suppose that $(x_n)$ is a sequence in $\ell^2$, i.e. $\displaystyle \sum_{i=1}^{\infty} x_n^2 < \infty$. Define: $$\Lambda x = \sum_{n=1}^{\infty} \frac{x_n}{n\sqrt{6}}$$ Find $\| \Lambda \|_2$ Using the Cauchy-Schwarz inequality, it's easy to see that for any sequence $(x_n)$ in $\ell^2$ such that $\|(x_n)\|=1$, we have $\displaystyle |\Lambda x| \leq \frac{\pi}{6}$: $$|\Lambda x|^2 = |\sum_{n=1}^{\infty} \frac{x_n}{n\sqrt{6}}|^2 \leq |\sum_{n=1}^{\infty} x_n^2|\cdot|\sum_{n=1}^{\infty}\frac{1}{6n^2}|=\frac{\pi^2}{36}$$ How can I show that $\| \Lambda \|$ is indeed equal to $\displaystyle \frac{\pi}{6}$?",,"['real-analysis', 'analysis', 'functional-analysis', 'lp-spaces']"
69,Prove existence of Borel set related to the function $f(x)=2x \mod 1$,Prove existence of Borel set related to the function,f(x)=2x \mod 1,"Let $I=[0,1)$ and $f(x)=2x \mod 1$. Prove that for every $\epsilon>0$ there is $E\subset I$ Borel set s.a $m(I/E)<\epsilon$ and $\lim_{N\to\infty} \sup \left\{|\frac{1}{N}\sum_{j=0}^{N-1} 1_{[0,1/2)}(f^j(x))-1/2|:x\in E\right\}=0$. $f^j$ means $f\circ f\circ f\circ ...$, and $1_{[0,1/2)}(f^j(x))$ is $1$ if $f^j(x)\in [0,1/2)$ and $0$ otherwise. Could you help me with this question? 1. this is homework. thought it would be fair to mention it. I did spend a lot of time on it though. My thoughts are that given a $j$, in order for $f^j(x)$ to be $1$, $x$ has to lie in one of $j$ subsets of $[0,1/2)$ of length $1/j$ (am i right? did i understand $f$ correctly?) Also, one could show that $\lim \int =0$ and therefore deduce that the integrand is $0$ a.e. Lebesgue sets are the complement of the Borel sigma algebra, so if i could find any measurable set satisfies it, I could also find a Borel subset of it close enough to it. Thanks! P.s I understand that it has something to do with ergodic theory. I'm not allowed to use any ergodic theorems - purely measure theory. P.s2: I am allowed to use this, if you should find it useful: Let $(X,M,\mu)$ be a measure space, and $f_k:M\to\mathbb R$ function series. assume $\int f_k^2 <A$ and $\langle f_i,f_j\rangle=0$ for $i\not=j$. Then $\lim_{N\to\infty} \frac{1}{N}\sum_{k=1}^N f_k(x)=0 $ a.e.","Let $I=[0,1)$ and $f(x)=2x \mod 1$. Prove that for every $\epsilon>0$ there is $E\subset I$ Borel set s.a $m(I/E)<\epsilon$ and $\lim_{N\to\infty} \sup \left\{|\frac{1}{N}\sum_{j=0}^{N-1} 1_{[0,1/2)}(f^j(x))-1/2|:x\in E\right\}=0$. $f^j$ means $f\circ f\circ f\circ ...$, and $1_{[0,1/2)}(f^j(x))$ is $1$ if $f^j(x)\in [0,1/2)$ and $0$ otherwise. Could you help me with this question? 1. this is homework. thought it would be fair to mention it. I did spend a lot of time on it though. My thoughts are that given a $j$, in order for $f^j(x)$ to be $1$, $x$ has to lie in one of $j$ subsets of $[0,1/2)$ of length $1/j$ (am i right? did i understand $f$ correctly?) Also, one could show that $\lim \int =0$ and therefore deduce that the integrand is $0$ a.e. Lebesgue sets are the complement of the Borel sigma algebra, so if i could find any measurable set satisfies it, I could also find a Borel subset of it close enough to it. Thanks! P.s I understand that it has something to do with ergodic theory. I'm not allowed to use any ergodic theorems - purely measure theory. P.s2: I am allowed to use this, if you should find it useful: Let $(X,M,\mu)$ be a measure space, and $f_k:M\to\mathbb R$ function series. assume $\int f_k^2 <A$ and $\langle f_i,f_j\rangle=0$ for $i\not=j$. Then $\lim_{N\to\infty} \frac{1}{N}\sum_{k=1}^N f_k(x)=0 $ a.e.",,"['analysis', 'measure-theory', 'ergodic-theory']"
70,Show that a particular sequence ($x_{n+1}= \frac{1}{4 - x_{n}}$) converges,Show that a particular sequence () converges,x_{n+1}= \frac{1}{4 - x_{n}},"Would someone be able to verify that my proof that the following sequence converges is sound? We have a sequence $\left ( x_{n} \right )_{n\geq i}$ where $x_{1} = 3$ and $x_{n+1}= \frac{1}{4 - x_{n}}$. We wish to show that this sequence converges (and then determine its limit). So, I applied the monotone convergence theorem, showing it is bounded and monotone decreasing by induction and hence converges. So, the base case is fine, showing that $\frac{1}{4} \leq x_{2} = 1 < x_{1} = 3 \leq 3$. Then I made the assumption that $\frac{1}{4} \leq x_{k+1} < x_{k} \leq 3$. Now, we want to show: $\frac{1}{4} \leq x_{k+2} < x_{k+1} \leq 3$ (I think this is correct?) So, it is probably possible to do this as one big inequality if you are good at that, however, for simplicity, I split it and showed $x_{k+2} < x_{k+1}$ first and then that $\frac{1}{4} \leq x_{k+2}$ and $x_{k+1} \leq 3$. So: $x_{k+1} < x_{k}$ (by our assumption) $ \Rightarrow   4 - x_{k} < 4 - x_{k+1}  $ $ \Rightarrow   \frac{1}{4 - x_{k+1}} < \frac{1}{4 - x_{k}}  $ (both terms less than 3 by our assumption, so, we can divide without changing inequality sign) $ \Rightarrow  x_{k+2} < x_{k+1}   $ $ \Rightarrow$ monotone decreasing So, now that we have shown that the sequence is decreasing, we just need to show both bounds, like so: $\frac{1}{4} \leq x_{k+1}$ and $x_{k} \leq 3$ (by our assumption) $ \Rightarrow 4 - x_{k+1} \leq 4 - \frac{1}{4}$ and $ 4 - 3 \leq 4 - x_{k}$ $ \Rightarrow \frac{1}{\frac{15}{4}} \leq \frac{1}{4-x_{k+1}} $ and $ \frac{1}{4 - x_{k}} \leq 1$ $ \Rightarrow \frac{1}{4} < \frac{4}{15} \leq x_{k+2}$ and $ x_{k+1} \leq 1 < 3$ Thus, combining both parts of the proof together once again we form the inequality: $ \frac{1}{4} < x_{k+2} < x_{k+1} < 3$ Hence, by induction, we have shown that the sequence is bounded below by $\frac{1}{4}$, is strictly decreasing and is bounded above by 3, thus, the sequence converges by the monotone convergence theorem. Does anyone spot any errors in my proof? Thanks!","Would someone be able to verify that my proof that the following sequence converges is sound? We have a sequence $\left ( x_{n} \right )_{n\geq i}$ where $x_{1} = 3$ and $x_{n+1}= \frac{1}{4 - x_{n}}$. We wish to show that this sequence converges (and then determine its limit). So, I applied the monotone convergence theorem, showing it is bounded and monotone decreasing by induction and hence converges. So, the base case is fine, showing that $\frac{1}{4} \leq x_{2} = 1 < x_{1} = 3 \leq 3$. Then I made the assumption that $\frac{1}{4} \leq x_{k+1} < x_{k} \leq 3$. Now, we want to show: $\frac{1}{4} \leq x_{k+2} < x_{k+1} \leq 3$ (I think this is correct?) So, it is probably possible to do this as one big inequality if you are good at that, however, for simplicity, I split it and showed $x_{k+2} < x_{k+1}$ first and then that $\frac{1}{4} \leq x_{k+2}$ and $x_{k+1} \leq 3$. So: $x_{k+1} < x_{k}$ (by our assumption) $ \Rightarrow   4 - x_{k} < 4 - x_{k+1}  $ $ \Rightarrow   \frac{1}{4 - x_{k+1}} < \frac{1}{4 - x_{k}}  $ (both terms less than 3 by our assumption, so, we can divide without changing inequality sign) $ \Rightarrow  x_{k+2} < x_{k+1}   $ $ \Rightarrow$ monotone decreasing So, now that we have shown that the sequence is decreasing, we just need to show both bounds, like so: $\frac{1}{4} \leq x_{k+1}$ and $x_{k} \leq 3$ (by our assumption) $ \Rightarrow 4 - x_{k+1} \leq 4 - \frac{1}{4}$ and $ 4 - 3 \leq 4 - x_{k}$ $ \Rightarrow \frac{1}{\frac{15}{4}} \leq \frac{1}{4-x_{k+1}} $ and $ \frac{1}{4 - x_{k}} \leq 1$ $ \Rightarrow \frac{1}{4} < \frac{4}{15} \leq x_{k+2}$ and $ x_{k+1} \leq 1 < 3$ Thus, combining both parts of the proof together once again we form the inequality: $ \frac{1}{4} < x_{k+2} < x_{k+1} < 3$ Hence, by induction, we have shown that the sequence is bounded below by $\frac{1}{4}$, is strictly decreasing and is bounded above by 3, thus, the sequence converges by the monotone convergence theorem. Does anyone spot any errors in my proof? Thanks!",,"['real-analysis', 'analysis']"
71,"Radius of convergence, prove that $\sum\limits_{n=0}^{\infty} a_n z^n$ converges absolutely for every $z \in \mathbb{C}$ with $|z| < R$","Radius of convergence, prove that  converges absolutely for every  with",\sum\limits_{n=0}^{\infty} a_n z^n z \in \mathbb{C} |z| < R,"A proof of this is given in my lecture notes as follows: We define $R$ to be $\sup \{|z| \in \mathbb{R} : \sum |c_k z^k|$ converges $\}$ when the supremum exists. Prove that $\sum |c_k z^k|$ converges for $|z| < R$ where $R$ is the radius of convergence. Proof: Fix $z$ with $|z| < R$ and pick $S$ such that $|z| < S < R$ . Then $\epsilon = R − S > 0$ and by Approximation Property for supremum we can find $\rho$ with $R−\epsilon = S < \rho < R$ such that $\sum |c_k \rho ^k|$ converges. But since $|z| < \rho$ , this implies $\sum |c_k z^k|$ converges, by the simple Comparison Test. I don't understand two things. Firstly, why do they pick $S$ and then find $\rho$ ? Can't they just pick an $S$ that satisfy the extra conditions of $\rho$ , that is, choose an $S$ that satisfies $R - \epsilon < S < R$ by the Approximation Property? Secondly, why can they they say that there will definitely be a value between $R- \epsilon$ and $R$ , calling it $\rho$ , such that $\sum |c_k \rho ^k|$ converges. Why isn't it possible that no such value exists between $R - \epsilon$ and $R$ ?","A proof of this is given in my lecture notes as follows: We define to be converges when the supremum exists. Prove that converges for where is the radius of convergence. Proof: Fix with and pick such that . Then and by Approximation Property for supremum we can find with such that converges. But since , this implies converges, by the simple Comparison Test. I don't understand two things. Firstly, why do they pick and then find ? Can't they just pick an that satisfy the extra conditions of , that is, choose an that satisfies by the Approximation Property? Secondly, why can they they say that there will definitely be a value between and , calling it , such that converges. Why isn't it possible that no such value exists between and ?",R \sup \{|z| \in \mathbb{R} : \sum |c_k z^k| \} \sum |c_k z^k| |z| < R R z |z| < R S |z| < S < R \epsilon = R − S > 0 \rho R−\epsilon = S < \rho < R \sum |c_k \rho ^k| |z| < \rho \sum |c_k z^k| S \rho S \rho S R - \epsilon < S < R R- \epsilon R \rho \sum |c_k \rho ^k| R - \epsilon R,"['analysis', 'proof-verification', 'power-series']"
72,"Investigate whether there exists a function $f : [a,b]\rightarrow R$ that is continuous and that takes exactly twice each of its values. [duplicate]",Investigate whether there exists a function  that is continuous and that takes exactly twice each of its values. [duplicate],"f : [a,b]\rightarrow R","This question already has answers here : A function takes every function value twice - proof it is not continuous (3 answers) Prove that no function exists such that... (2 answers) Closed 9 years ago . Let a < b. Investigate whether there exists a function $f : [a,b]\rightarrow R$ that is continuous and that takes exactly twice each of its values.","This question already has answers here : A function takes every function value twice - proof it is not continuous (3 answers) Prove that no function exists such that... (2 answers) Closed 9 years ago . Let a < b. Investigate whether there exists a function $f : [a,b]\rightarrow R$ that is continuous and that takes exactly twice each of its values.",,['analysis']
73,Is the limit function $f$ continuous if $f_n(x_n)\to f(x)$? [duplicate],Is the limit function  continuous if ? [duplicate],f f_n(x_n)\to f(x),"This question already has answers here : If $f_n(x_n) \to f(x)$ whenever $x_n \to x$, show that $f$ is continuous (4 answers) Closed 9 years ago . Let $I\subset\mathbb{R}$ be an interval and let $(f_n)$ be a sequence of continuous real-valued functions on $I$. Consider the following statements: $f_n\to f$ uniformly; For every sequence $(x_n)$ in $I$ converging to $x\in I$, we have $f_n(x_n)\to f(x)$; $f:\ I\longrightarrow\mathbb{R}$ is continuous. Certainly (1) implies (2) and (3). If $I$ is compact, (2) and (3) together imply (1), otherwise counterexamples can be found. My interest is on the relationship between (2) and (3), specifically does (2) imply (3)? My intuition says no, since (2) would appear to be a strictly weaker statement than (1). However, (2) is certainly stronger than pointwise convergence of $(f_n)$, and so far I have been unable to think of a counterexample. Can anyone think of a counterexample (or proof)","This question already has answers here : If $f_n(x_n) \to f(x)$ whenever $x_n \to x$, show that $f$ is continuous (4 answers) Closed 9 years ago . Let $I\subset\mathbb{R}$ be an interval and let $(f_n)$ be a sequence of continuous real-valued functions on $I$. Consider the following statements: $f_n\to f$ uniformly; For every sequence $(x_n)$ in $I$ converging to $x\in I$, we have $f_n(x_n)\to f(x)$; $f:\ I\longrightarrow\mathbb{R}$ is continuous. Certainly (1) implies (2) and (3). If $I$ is compact, (2) and (3) together imply (1), otherwise counterexamples can be found. My interest is on the relationship between (2) and (3), specifically does (2) imply (3)? My intuition says no, since (2) would appear to be a strictly weaker statement than (1). However, (2) is certainly stronger than pointwise convergence of $(f_n)$, and so far I have been unable to think of a counterexample. Can anyone think of a counterexample (or proof)",,"['real-analysis', 'analysis']"
74,$L^\infty(S^1)$ is not separable,is not separable,L^\infty(S^1),"Let $S^1$ be the unit circle and $L^\infty(S^1)$ the space of measurable functions $f:S^1\to\mathbb{C}$ such that $\|f\|_\infty<\infty$. (In fact $L^\infty(S^1)$ consists of equivalence classes of functions, where $f\sim g$ is they are equal almost everywhere.) How to show that $L^\infty(S^1)$ is not separable? $L^\infty(S^1)$ can be thought as the space of $2\pi$ periodic functions. I have been able to show that the space of periodic functions (of arbitrary period) is not separable by showing that the collection of functions of the form $$f_s(x):=e^{isx}$$ for $s\in\mathbb{R}$ is uncountable and satisfies $$\|f_s-f_t\|_\infty=2\delta_{st},$$ but I fail to adapt this to the case of $2\pi$ periodic functions. Is there another approach?","Let $S^1$ be the unit circle and $L^\infty(S^1)$ the space of measurable functions $f:S^1\to\mathbb{C}$ such that $\|f\|_\infty<\infty$. (In fact $L^\infty(S^1)$ consists of equivalence classes of functions, where $f\sim g$ is they are equal almost everywhere.) How to show that $L^\infty(S^1)$ is not separable? $L^\infty(S^1)$ can be thought as the space of $2\pi$ periodic functions. I have been able to show that the space of periodic functions (of arbitrary period) is not separable by showing that the collection of functions of the form $$f_s(x):=e^{isx}$$ for $s\in\mathbb{R}$ is uncountable and satisfies $$\|f_s-f_t\|_\infty=2\delta_{st},$$ but I fail to adapt this to the case of $2\pi$ periodic functions. Is there another approach?",,"['analysis', 'lp-spaces']"
75,harmonic conjugate is unique up to a constant,harmonic conjugate is unique up to a constant,,"$u: U \rightarrow \mathbb{R}$ is harmonic function. If I let $w$ and $v$ both be harmonic conjugate of u, combining this with Cauchy Riemann equations, I can reach conclusion that w and v differ by a constant after some manipulations. However, the problem states that U is connected. (i.e. U is connected. Show that harmonic conjugates differ by a constant) But, I didn't use the fact that U is connected in my proof. Could someone clarify to me where I need the fact that U is connected? Thank you in advance.","$u: U \rightarrow \mathbb{R}$ is harmonic function. If I let $w$ and $v$ both be harmonic conjugate of u, combining this with Cauchy Riemann equations, I can reach conclusion that w and v differ by a constant after some manipulations. However, the problem states that U is connected. (i.e. U is connected. Show that harmonic conjugates differ by a constant) But, I didn't use the fact that U is connected in my proof. Could someone clarify to me where I need the fact that U is connected? Thank you in advance.",,"['complex-analysis', 'analysis']"
76,Finding the zeroes of a function,Finding the zeroes of a function,,"How do I find the zeroes of the function $S$, below? I want to find the zeroes of $$S = 4\psi_2$$ Where $$\dot{\psi}_1=2\psi_2$$ $$\dot{\psi}_2=-2\psi_1$$ and I have that(from below) $$\begin{pmatrix}\psi_1 \\ \psi_2\end{pmatrix} = Ae^{2it}\begin{pmatrix}i \\ 1\end{pmatrix}+Be^{-2it}\begin{pmatrix}-i \\ 1\end{pmatrix}$$ I have two equilibrium points that are centre nodes at $(2,0)$ and $(-2,0)$ So I suppose this is equivalent to finding the zeroes of the function: $$S = 4(Ae^{2it}+Be^{-2it})$$ And eigenvector: $\underset{\sim}{v}_1=\begin{pmatrix}i \\ 1\end{pmatrix}$ for $\lambda=2i$ And eigenvector: $\underset{\sim}{v}_2=\begin{pmatrix}-i \\1\end{pmatrix}$ for $\lambda = -2i$","How do I find the zeroes of the function $S$, below? I want to find the zeroes of $$S = 4\psi_2$$ Where $$\dot{\psi}_1=2\psi_2$$ $$\dot{\psi}_2=-2\psi_1$$ and I have that(from below) $$\begin{pmatrix}\psi_1 \\ \psi_2\end{pmatrix} = Ae^{2it}\begin{pmatrix}i \\ 1\end{pmatrix}+Be^{-2it}\begin{pmatrix}-i \\ 1\end{pmatrix}$$ I have two equilibrium points that are centre nodes at $(2,0)$ and $(-2,0)$ So I suppose this is equivalent to finding the zeroes of the function: $$S = 4(Ae^{2it}+Be^{-2it})$$ And eigenvector: $\underset{\sim}{v}_1=\begin{pmatrix}i \\ 1\end{pmatrix}$ for $\lambda=2i$ And eigenvector: $\underset{\sim}{v}_2=\begin{pmatrix}-i \\1\end{pmatrix}$ for $\lambda = -2i$",,"['calculus', 'analysis', 'systems-of-equations']"
77,"Existence of unique solution on $(-\delta,\delta)$ for $f(x)=1+x+\displaystyle\int^x_0\sin(tf(t))dt$",Existence of unique solution on  for,"(-\delta,\delta) f(x)=1+x+\displaystyle\int^x_0\sin(tf(t))dt","The following was a question previously given in a test at my university: Show that there exists some $\delta>0$ for which there is a unique   continuous function $f:(-\delta,\delta)\to\mathbb{R}$ satisfying   $$f(x)=1+x+\int^x_0\sin(tf(t))dt$$ and give a value of $\delta$ for   which this is true. ($\mathbb{R}$ has the Euclidean metric) I'm really not sure what to do with this question. If it were on a closed interval I think I could use the Banach contraction theorem to show that some mapping $Q$ with $Qf=f$ if and only if $f$ satisfies the above equation is a contraction, then finish. Or, I think Picard's theorem might've worked too, but the fact that it's an open interval has got me pretty confused. How can I solve this?","The following was a question previously given in a test at my university: Show that there exists some $\delta>0$ for which there is a unique   continuous function $f:(-\delta,\delta)\to\mathbb{R}$ satisfying   $$f(x)=1+x+\int^x_0\sin(tf(t))dt$$ and give a value of $\delta$ for   which this is true. ($\mathbb{R}$ has the Euclidean metric) I'm really not sure what to do with this question. If it were on a closed interval I think I could use the Banach contraction theorem to show that some mapping $Q$ with $Qf=f$ if and only if $f$ satisfies the above equation is a contraction, then finish. Or, I think Picard's theorem might've worked too, but the fact that it's an open interval has got me pretty confused. How can I solve this?",,"['real-analysis', 'analysis']"
78,"There is no holomorphic function in $\Omega=\{0<r<\lvert z\rvert <R\}$ with real part $u(x,y)=\frac{1}{2}\log(x^2+y^2)$",There is no holomorphic function in  with real part,"\Omega=\{0<r<\lvert z\rvert <R\} u(x,y)=\frac{1}{2}\log(x^2+y^2)","Consider $u(x,y)=\dfrac{\text{log}(x^2+y^2)}{2}$ on $\Omega=\{0<r<|z|<R\}.$ Show there is no holomorphic function on $\Omega$ whose real part is $u.$ My attempt: I understand that $u$ is real part of $\text{log}(z)$ and $\text{log}(z)$ is not well defined on $\Omega.$ How do I use this fact and identity theorem to show there isn't any holomorphic function on $\Omega$ whose real part is $u \ ?$ Thank you.","Consider $u(x,y)=\dfrac{\text{log}(x^2+y^2)}{2}$ on $\Omega=\{0<r<|z|<R\}.$ Show there is no holomorphic function on $\Omega$ whose real part is $u.$ My attempt: I understand that $u$ is real part of $\text{log}(z)$ and $\text{log}(z)$ is not well defined on $\Omega.$ How do I use this fact and identity theorem to show there isn't any holomorphic function on $\Omega$ whose real part is $u \ ?$ Thank you.",,"['complex-analysis', 'analysis', 'harmonic-functions']"
79,Compactness and uniform equicontinuous family,Compactness and uniform equicontinuous family,,"Suppose $\mathcal{F} \subset C(A)$ be a family of continuous functions with domain $A$.    If $\mathcal{F}$ is pointwise equicontinuous, is it true that $\cal F$ is uniformly equicontinuous? I guess the answer is NO. Indeed, if the domain $A$ is compact set, then the $\cal F$ is uniformly equicontinuous. But I'm not sure if the compactness assumption drops. Here is my thinking about constructing a counterexample: (but not sure...) Suppose $A:=(0,1]$, I want to say $\cal F$ is NOT uniformly equicontinuous; i.e., there exists $\varepsilon>0$ s.t. $\forall \delta>0$, there exists $x,y \in A$, $f \in \mathcal{F}$ such that $|x-y| < \delta$ but $|f(x) - f(y)| \ge \varepsilon$ Take $\varepsilon=1$, and choose $x=1/n, y=2/n$ (then $x,y \in A$); define a function $f_n(x):= 2n x$ ($f_n \in \mathcal{F} \subset C(A)$ ), then I have  $$|x-y| = |1/n - 2/n|<1/n := \delta,$$ but $|f_n(x) - f_n(y)| = |2 -4 |=2 \ge 1$ But I didn't see very clear what the role of compactness played.. Thanks you.","Suppose $\mathcal{F} \subset C(A)$ be a family of continuous functions with domain $A$.    If $\mathcal{F}$ is pointwise equicontinuous, is it true that $\cal F$ is uniformly equicontinuous? I guess the answer is NO. Indeed, if the domain $A$ is compact set, then the $\cal F$ is uniformly equicontinuous. But I'm not sure if the compactness assumption drops. Here is my thinking about constructing a counterexample: (but not sure...) Suppose $A:=(0,1]$, I want to say $\cal F$ is NOT uniformly equicontinuous; i.e., there exists $\varepsilon>0$ s.t. $\forall \delta>0$, there exists $x,y \in A$, $f \in \mathcal{F}$ such that $|x-y| < \delta$ but $|f(x) - f(y)| \ge \varepsilon$ Take $\varepsilon=1$, and choose $x=1/n, y=2/n$ (then $x,y \in A$); define a function $f_n(x):= 2n x$ ($f_n \in \mathcal{F} \subset C(A)$ ), then I have  $$|x-y| = |1/n - 2/n|<1/n := \delta,$$ but $|f_n(x) - f_n(y)| = |2 -4 |=2 \ge 1$ But I didn't see very clear what the role of compactness played.. Thanks you.",,"['real-analysis', 'analysis']"
80,A density question,A density question,,"Let $\theta \in \mathbb{R} \setminus \mathbb{Q}$. Is the set $\{ (2n+1) \theta \bmod 1: n \in \mathbb{N} \}$ dense in $[0,1]$?","Let $\theta \in \mathbb{R} \setminus \mathbb{Q}$. Is the set $\{ (2n+1) \theta \bmod 1: n \in \mathbb{N} \}$ dense in $[0,1]$?",,"['real-analysis', 'combinatorics', 'analysis', 'irrational-numbers']"
81,Holder Inequality,Holder Inequality,,"I have a problem with the demonstration of this inequality ('m following Royden) : If $p$ and $q$ are nonnegative numbers such that $\frac{1}{p}+\frac{1}{q}$ and if $f \in L^p$ and $g \in L^q$, then $f\cdot g \in L^1$ and $$\int |fg| \leqslant ||f||_p \cdot ||g||_q$$  Without loss of generality suppose $f,g \geqslant 0$ Set $h(x)=g(x)^{q-1}=g(x)^{p/q}$,since $q-1=q/p$ also $g(x)=h(x)^{q-1}=h(x)^{p/q}$ Thus for $t \in \bf{R}$: $$ptf(x)g(x)= ptf(x)h(x)^{p-1} \leqslant (h(x)+tf(x))^p-h(x)$$ (because of lemma). Hence $$pt \int fg \leqslant \int |h+tf|^p-\int h^p = ||h+tf||^p-||h||^p$$ and $$pt \int fg \leqslant (||h||+t||f||)^p-||h||^p$$ Now is the passage that I do not understand: Differentiating both sides with respect to t at t=0, we get $$p \int fg \leqslant p ||f|| \cdot ||h||_p^{p-1}=p||f|| \cdot ||g||$$ I do not see why I can derive from both sides of an INEQUALITY..","I have a problem with the demonstration of this inequality ('m following Royden) : If $p$ and $q$ are nonnegative numbers such that $\frac{1}{p}+\frac{1}{q}$ and if $f \in L^p$ and $g \in L^q$, then $f\cdot g \in L^1$ and $$\int |fg| \leqslant ||f||_p \cdot ||g||_q$$  Without loss of generality suppose $f,g \geqslant 0$ Set $h(x)=g(x)^{q-1}=g(x)^{p/q}$,since $q-1=q/p$ also $g(x)=h(x)^{q-1}=h(x)^{p/q}$ Thus for $t \in \bf{R}$: $$ptf(x)g(x)= ptf(x)h(x)^{p-1} \leqslant (h(x)+tf(x))^p-h(x)$$ (because of lemma). Hence $$pt \int fg \leqslant \int |h+tf|^p-\int h^p = ||h+tf||^p-||h||^p$$ and $$pt \int fg \leqslant (||h||+t||f||)^p-||h||^p$$ Now is the passage that I do not understand: Differentiating both sides with respect to t at t=0, we get $$p \int fg \leqslant p ||f|| \cdot ||h||_p^{p-1}=p||f|| \cdot ||g||$$ I do not see why I can derive from both sides of an INEQUALITY..",,"['analysis', 'inequality', 'banach-spaces', 'lp-spaces', 'integral-inequality']"
82,Family of sequences in a Hilbert space with certain property,Family of sequences in a Hilbert space with certain property,,"Suppose $\mathcal{F}$ is a family of sequences on the unit sphere of $l_2$ with the following property: For any sequence $\varepsilon_n\downarrow 0$ but which is not eventually identically $0$, there exists $(x_n)\in\mathcal{F}$ and $z\in S_{l_2}$ such that, for any $n$, $\langle z, x_n\rangle\leq \varepsilon_n$. Can we find a sequence $(y_n)\in\mathcal{F}$ and $z\in S_{l_2}$ such that for any $n$ $\langle z, y_n\rangle=0$?","Suppose $\mathcal{F}$ is a family of sequences on the unit sphere of $l_2$ with the following property: For any sequence $\varepsilon_n\downarrow 0$ but which is not eventually identically $0$, there exists $(x_n)\in\mathcal{F}$ and $z\in S_{l_2}$ such that, for any $n$, $\langle z, x_n\rangle\leq \varepsilon_n$. Can we find a sequence $(y_n)\in\mathcal{F}$ and $z\in S_{l_2}$ such that for any $n$ $\langle z, y_n\rangle=0$?",,"['analysis', 'functional-analysis', 'hilbert-spaces']"
83,Proving that $\limsup_{n\to\infty}\frac{1}{n}\sum_{m=1}^n s_m\leq \limsup_{n\to\infty}s_n.$,Proving that,\limsup_{n\to\infty}\frac{1}{n}\sum_{m=1}^n s_m\leq \limsup_{n\to\infty}s_n.,"I am reviewing for my first year analysis exam and am stuck on a problem. Let $\sigma_n=\frac{1}{n}\sum_{m=1}^n s_m$.  I am trying to show that, if $(s_n)$ is a bounded sequence of real numbers, $$\limsup_{n\to\infty}\sigma_n\leq \limsup_{n\to\infty}s_n.$$ I understand what's supposed to happen here, but I'm having trouble showing it.  This is what I have so far: I know from a previous exercise that $s_n\rightarrow s$ implies that $\sigma_n\rightarrow s$. I'll call this fact $\star$. If $(\sigma_{n_k})_{k\in K}$ is a convergent subsequence of $(\sigma_n)$, let $(s_{n_i})_{i\in I\subseteq K}$ be a convergent subsequence of $(s_{n_k})_{k\in K}$.  We know this must exist because $(s_{n_k})_{k\in K}$ is bounded (because $(s_n)$ is bounded).  Let $I_k=\{i\in I:i \leq k\}$, $I_k^\prime =\{i\in K\setminus I:i \leq k\}$, and $m_k=|I_k|$.  Then $$\sigma_{n_k}=\frac{m_k}{n_k}\left(\frac{1}{m_k}\sum_{i\in I_k}s_{n_i}\right)+\frac{n_k-m_k}{n_k}\left(\frac{1}{n_k-m_k}\sum_{i\in I_k^\prime}s_{n_i}\right)$$ Because $s_{n_i}$ is convergent by assumption, $\frac{1}{m_k}\sum_{i\in I_k}s_{n_i}$ is convergent by $\star$. But, now I don't know where to go.  The coefficients are a little ugly and I don't know what the other sum might converge to.  What should I do next?  Or am I taking the wrong approach?","I am reviewing for my first year analysis exam and am stuck on a problem. Let $\sigma_n=\frac{1}{n}\sum_{m=1}^n s_m$.  I am trying to show that, if $(s_n)$ is a bounded sequence of real numbers, $$\limsup_{n\to\infty}\sigma_n\leq \limsup_{n\to\infty}s_n.$$ I understand what's supposed to happen here, but I'm having trouble showing it.  This is what I have so far: I know from a previous exercise that $s_n\rightarrow s$ implies that $\sigma_n\rightarrow s$. I'll call this fact $\star$. If $(\sigma_{n_k})_{k\in K}$ is a convergent subsequence of $(\sigma_n)$, let $(s_{n_i})_{i\in I\subseteq K}$ be a convergent subsequence of $(s_{n_k})_{k\in K}$.  We know this must exist because $(s_{n_k})_{k\in K}$ is bounded (because $(s_n)$ is bounded).  Let $I_k=\{i\in I:i \leq k\}$, $I_k^\prime =\{i\in K\setminus I:i \leq k\}$, and $m_k=|I_k|$.  Then $$\sigma_{n_k}=\frac{m_k}{n_k}\left(\frac{1}{m_k}\sum_{i\in I_k}s_{n_i}\right)+\frac{n_k-m_k}{n_k}\left(\frac{1}{n_k-m_k}\sum_{i\in I_k^\prime}s_{n_i}\right)$$ Because $s_{n_i}$ is convergent by assumption, $\frac{1}{m_k}\sum_{i\in I_k}s_{n_i}$ is convergent by $\star$. But, now I don't know where to go.  The coefficients are a little ugly and I don't know what the other sum might converge to.  What should I do next?  Or am I taking the wrong approach?",,"['real-analysis', 'analysis', 'proof-verification']"
84,Cesàro summability and $\sum n \lvert a_n\rvert ^2 < \infty$ implies convergence,Cesàro summability and  implies convergence,\sum n \lvert a_n\rvert ^2 < \infty,"How can I prove that if $\sum_{n=1}^\infty a_n$ is Cesàro summable and if $\sum_{n=1}^\infty n |a_n|^2 < \infty$, then $\sum_{n=1}^\infty a_n$ converges?","How can I prove that if $\sum_{n=1}^\infty a_n$ is Cesàro summable and if $\sum_{n=1}^\infty n |a_n|^2 < \infty$, then $\sum_{n=1}^\infty a_n$ converges?",,"['calculus', 'sequences-and-series', 'analysis', 'convergence-divergence', 'cesaro-summable']"
85,Am I going about this the right way?,Am I going about this the right way?,,"Show that for any $α ∈ R$, there exist infinitely many rational numbers $\frac{m}{n}$ with $|α − \frac{m}{n^2}| < \frac{1}{n}$. So we know that $-1≤\frac{1}{n}≤1$ which implies $\frac{1}{n^2}≤1$. Case $1$: if $m=n$ then $\frac{m}{n^2} = \frac{1}{n}$ so obviously we get $|α − \frac{m}{n^2}| < \frac{1}{n}$. Case 2: if $m< n$ that implies $m< n^2$ which implies $\frac{m}{n^2}<1$ and if $n< n^2$ then $\frac{m}{n^2}<\frac{1}{n}$ so again $|α − \frac{m}{n^2} | < \frac{1}{n}$ makes sense. I'm having trouble seeing how $m > n$ would come up with the same conclusion. (Am I going about this proof the right way?)","Show that for any $α ∈ R$, there exist infinitely many rational numbers $\frac{m}{n}$ with $|α − \frac{m}{n^2}| < \frac{1}{n}$. So we know that $-1≤\frac{1}{n}≤1$ which implies $\frac{1}{n^2}≤1$. Case $1$: if $m=n$ then $\frac{m}{n^2} = \frac{1}{n}$ so obviously we get $|α − \frac{m}{n^2}| < \frac{1}{n}$. Case 2: if $m< n$ that implies $m< n^2$ which implies $\frac{m}{n^2}<1$ and if $n< n^2$ then $\frac{m}{n^2}<\frac{1}{n}$ so again $|α − \frac{m}{n^2} | < \frac{1}{n}$ makes sense. I'm having trouble seeing how $m > n$ would come up with the same conclusion. (Am I going about this proof the right way?)",,['analysis']
86,Closure and subbasis,Closure and subbasis,,"Let $X$ be a topological space and $A \subset X$ with a subbasis $S$. Does it then hold that $x \in \overline{A}: \Leftrightarrow \forall s \in S: (x \in s \Rightarrow s \cap A \neq  \emptyset).$ This is certainly true if $S$ is a basis, but I suspect it is wrong for a subbasis, am I right?","Let $X$ be a topological space and $A \subset X$ with a subbasis $S$. Does it then hold that $x \in \overline{A}: \Leftrightarrow \forall s \in S: (x \in s \Rightarrow s \cap A \neq  \emptyset).$ This is certainly true if $S$ is a basis, but I suspect it is wrong for a subbasis, am I right?",,['real-analysis']
87,"If $f$ is differentiable everywhere, is $f'$ the weak derivative?","If  is differentiable everywhere, is  the weak derivative?",f f',"Let $f \in C^0([0,T])$ be such that $f'$ exists in the classical sense everywhere, but $f'$ may not be continuous. Is it true that $f'$ is the weak derivative of $f$ too, if it exists? I know this is true if $f \in C^1$, but I don't have that..","Let $f \in C^0([0,T])$ be such that $f'$ exists in the classical sense everywhere, but $f'$ may not be continuous. Is it true that $f'$ is the weak derivative of $f$ too, if it exists? I know this is true if $f \in C^1$, but I don't have that..",,"['real-analysis', 'analysis', 'distribution-theory', 'weak-derivatives']"
88,Correctness of proof that an ordered field S that has the supremum property also has the infimum property,Correctness of proof that an ordered field S that has the supremum property also has the infimum property,,"First question I have is how would you describe the relationship between an ordered field and an ordered set and continue the proof by treating the field as a set?  I want to say that right in the beginning to make things clear but I don't know how to say it. Here's the rest of the proof: Let A be a nonempty subset of S s.t.  it is bounded above and D be the set of all lower bounds for A. Fix a to be elements of A.  Then for each d in subset D d is a lower bound for A so d $\leq$ a  for all a in A. Since d is bounded above by A it has the supremum $\gamma$ .  Since every a is an upper bound for d, $\gamma \leq a$ for each element a in subset A.  So $\gamma$ is a lower bound for A and is clearly an infimum since it is an upper bound for the set of lower bounds.","First question I have is how would you describe the relationship between an ordered field and an ordered set and continue the proof by treating the field as a set?  I want to say that right in the beginning to make things clear but I don't know how to say it. Here's the rest of the proof: Let A be a nonempty subset of S s.t.  it is bounded above and D be the set of all lower bounds for A. Fix a to be elements of A.  Then for each d in subset D d is a lower bound for A so d $\leq$ a  for all a in A. Since d is bounded above by A it has the supremum $\gamma$ .  Since every a is an upper bound for d, $\gamma \leq a$ for each element a in subset A.  So $\gamma$ is a lower bound for A and is clearly an infimum since it is an upper bound for the set of lower bounds.",,"['real-analysis', 'analysis', 'proof-verification']"
89,"Closure of $C_0^{\infty}$ in $W^{k,p}(\Omega)$",Closure of  in,"C_0^{\infty} W^{k,p}(\Omega)","Why is it that in the definition of $W_0^{k,p}(\Omega)$ for $\Omega$ with boundary smooth enough, we only have $D^{\alpha}u$ for all $0\leq|\alpha|\leq k-1$ vanishing at the boundary and not $D^{\alpha}u$ with $|\alpha|=k$? Thanks a lot!","Why is it that in the definition of $W_0^{k,p}(\Omega)$ for $\Omega$ with boundary smooth enough, we only have $D^{\alpha}u$ for all $0\leq|\alpha|\leq k-1$ vanishing at the boundary and not $D^{\alpha}u$ with $|\alpha|=k$? Thanks a lot!",,"['analysis', 'functional-analysis', 'sobolev-spaces']"
90,Quadratic formula with complex coefficients,Quadratic formula with complex coefficients,,"Let $a,b$ and $c$ be complex numbers. I'm trying to prove that this version of the usual quadratic formula: $$z=\frac{-b+(b^2-4ac)^{\frac{1}{2}}} {2a}$$ solves the quadratic equation $$az^2+bz+c=0$$ This seemed fairly easy to do, but I came across some doubts. This is what I've done so far, please correct me if I made a mistake anywhere: $$az^2+bz+c=0$$ $$z^2+\frac b a z+\frac c a=0$$ $$(z+\frac b {2a})^ 2=\frac {b^2} {4a^2} -\frac c a$$ Now, if we were working with real numbers, I would just take square root on both sides of the equation and that would be all. The problem is, the complex function $w^{\frac 1 2}$ is multivaluated. I don't quite know how to work around it without reducing the image set in order to work with an injective function. Please, if somebody know how to finish this proof, I would really appreciate it. PS: I apologize in advance for the misspelling/structure mistakes of this post.","Let $a,b$ and $c$ be complex numbers. I'm trying to prove that this version of the usual quadratic formula: $$z=\frac{-b+(b^2-4ac)^{\frac{1}{2}}} {2a}$$ solves the quadratic equation $$az^2+bz+c=0$$ This seemed fairly easy to do, but I came across some doubts. This is what I've done so far, please correct me if I made a mistake anywhere: $$az^2+bz+c=0$$ $$z^2+\frac b a z+\frac c a=0$$ $$(z+\frac b {2a})^ 2=\frac {b^2} {4a^2} -\frac c a$$ Now, if we were working with real numbers, I would just take square root on both sides of the equation and that would be all. The problem is, the complex function $w^{\frac 1 2}$ is multivaluated. I don't quite know how to work around it without reducing the image set in order to work with an injective function. Please, if somebody know how to finish this proof, I would really appreciate it. PS: I apologize in advance for the misspelling/structure mistakes of this post.",,"['complex-analysis', 'analysis', 'complex-numbers']"
91,"If $T$ is self-adjoint, is the set of power series in $T$ closed?","If  is self-adjoint, is the set of power series in  closed?",T T,"If $T$ is a bounded self-adjoint operator on a Hilbert space, is the set of convergent power series in $T$ closed in the norm topology? I ask because I'm reading some spectral theorems and I was wondering what the space $\overline{ \mathbb{C}[T] }$ is like.  It would be nice if it were just the set of convergent power series in $T$. Edit: Actually this seems sort of unlikely since then every complex valued continuous functions on the spectrum of $T$ would be given by a power series, but I still haven't seen enough examples of spectra to rule this out. Even a self adjoint $T$ whose spectrum has a limit point would suffice to rule it out.","If $T$ is a bounded self-adjoint operator on a Hilbert space, is the set of convergent power series in $T$ closed in the norm topology? I ask because I'm reading some spectral theorems and I was wondering what the space $\overline{ \mathbb{C}[T] }$ is like.  It would be nice if it were just the set of convergent power series in $T$. Edit: Actually this seems sort of unlikely since then every complex valued continuous functions on the spectrum of $T$ would be given by a power series, but I still haven't seen enough examples of spectra to rule this out. Even a self adjoint $T$ whose spectrum has a limit point would suffice to rule it out.",,"['analysis', 'functional-analysis', 'operator-theory']"
92,Functions for which $\mathcal{F}g = f \ast f$,Functions for which,\mathcal{F}g = f \ast f,"Suppose one is given $f \in L^{2}(\mathbb{R})$, my question is whether or not there exists a $g \in L^{1}(\mathbb{R})$ such that $f \ast f = \mathcal{F}g$ where $\mathcal{F}$ is the Fourier transform. Conversely, suppose one is given a $g \in L^{1}(\mathbb{R})$, then does there exist an $f \in L^{2}(\mathbb{R})$ such that $\mathcal{F}g = f \ast f$? Formally, the answer for the first question should be $g := \mathcal{F}^{-1}(f \ast f)$. But of course all I know is that $f \ast f \in L^{\infty}(\mathbb{R})$.","Suppose one is given $f \in L^{2}(\mathbb{R})$, my question is whether or not there exists a $g \in L^{1}(\mathbb{R})$ such that $f \ast f = \mathcal{F}g$ where $\mathcal{F}$ is the Fourier transform. Conversely, suppose one is given a $g \in L^{1}(\mathbb{R})$, then does there exist an $f \in L^{2}(\mathbb{R})$ such that $\mathcal{F}g = f \ast f$? Formally, the answer for the first question should be $g := \mathcal{F}^{-1}(f \ast f)$. But of course all I know is that $f \ast f \in L^{\infty}(\mathbb{R})$.",,"['real-analysis', 'analysis', 'fourier-analysis']"
93,Extending a measurable map $f: G \to \mathbb{R}/\mathbb{Z}$ to a continuous group homomorphism,Extending a measurable map  to a continuous group homomorphism,f: G \to \mathbb{R}/\mathbb{Z},"Let $G$ be a compact abelian metrizable group (where the group operation is written as $+$) and $\mu$ is the Haar measure on $G$. Suppose we have a measurable function $f: G \rightarrow \mathbb{T}\cong \mathbb{R}/\mathbb{Z}$ such that $f(x + y) = f(x)+f(y)$ for all $(x, y)\in Z$, where $Z\subset G\times G$ with $(\mu\times\mu)(Z)=1$. Question: Can we find a continuous, group homomorphism $\phi: G \rightarrow \mathbb{T}\cong \mathbb{R}/\mathbb{Z}$, i.e., $\phi$ lies in the dual group of $G$, such that $\phi(x)=f(x)$  almost everywhere? Remarks: 1, This is essentially a question asked here by someone else with some change . Since no answer appeared, I think it is OK to ask it again here. Note that $Z$ is not necessarily of product type. 2, It seems to be able to extend $f$ to a continuous, almost everywhere group homomorphism, but I do not see how to get a group homomorphism.(this claim seems  Not true) 3, Any help, suggestions, references are appreciated, thanks in advance!","Let $G$ be a compact abelian metrizable group (where the group operation is written as $+$) and $\mu$ is the Haar measure on $G$. Suppose we have a measurable function $f: G \rightarrow \mathbb{T}\cong \mathbb{R}/\mathbb{Z}$ such that $f(x + y) = f(x)+f(y)$ for all $(x, y)\in Z$, where $Z\subset G\times G$ with $(\mu\times\mu)(Z)=1$. Question: Can we find a continuous, group homomorphism $\phi: G \rightarrow \mathbb{T}\cong \mathbb{R}/\mathbb{Z}$, i.e., $\phi$ lies in the dual group of $G$, such that $\phi(x)=f(x)$  almost everywhere? Remarks: 1, This is essentially a question asked here by someone else with some change . Since no answer appeared, I think it is OK to ask it again here. Note that $Z$ is not necessarily of product type. 2, It seems to be able to extend $f$ to a continuous, almost everywhere group homomorphism, but I do not see how to get a group homomorphism.(this claim seems  Not true) 3, Any help, suggestions, references are appreciated, thanks in advance!",,"['real-analysis', 'analysis', 'measure-theory', 'reference-request', 'topological-groups']"
94,Is set with this property is homeomorphic to Cantor set?,Is set with this property is homeomorphic to Cantor set?,,"(1) $A$ is nonempty subset of $\mathbb{R}$. (2) For all $x<y \in A$ there is $z \notin A$ such that $x<z<y$. (3) $A$ is perfect. Then is there homeomorphism between $A$ and cantor set? Intuitively, I think this is right because $A$ is uncountable because of (3), and set is almost like distinct many points. Which is almost like Cantor set.","(1) $A$ is nonempty subset of $\mathbb{R}$. (2) For all $x<y \in A$ there is $z \notin A$ such that $x<z<y$. (3) $A$ is perfect. Then is there homeomorphism between $A$ and cantor set? Intuitively, I think this is right because $A$ is uncountable because of (3), and set is almost like distinct many points. Which is almost like Cantor set.",,['analysis']
95,Disconnected Topoological Space with Intermediate Value Property,Disconnected Topoological Space with Intermediate Value Property,,"Does There exist a disconnected topological space with intermediate value property? Intermediate Value Property states that 'a topological space X is said to have intermediate value property if for every continuous function f: X to Y (where Y is ordered set with order topology) the following is true: If a, b belongs to X and there exist r in Y s.t. r lies between f(a) and f(b) then there exist c in X s.t. f(c) = r.","Does There exist a disconnected topological space with intermediate value property? Intermediate Value Property states that 'a topological space X is said to have intermediate value property if for every continuous function f: X to Y (where Y is ordered set with order topology) the following is true: If a, b belongs to X and there exist r in Y s.t. r lies between f(a) and f(b) then there exist c in X s.t. f(c) = r.",,"['general-topology', 'analysis']"
96,Uniformly bounded sequence of $L^{2}$ functions and a limit,Uniformly bounded sequence of  functions and a limit,L^{2},"Let $f_{n}: \mathbb{R}^{d} \rightarrow \mathbb{R}$ such that $\sup_{n}\|f_{n}\|_{L^{2}} < \infty$. Furthermore suppose $f_{n} \rightarrow f$ pointwise almost everywhere for some $f$. The problem I am working on is to show that $$\int_{\mathbb{R}^{d}}||f_{n}|^{2} - |f_{n} - f|^{2} - |f|^{2}|\, dx \rightarrow 0$$ as $n \rightarrow \infty$. First I noticed that $||f_{n}|^{2} - |f_{n} - f|^{2} - |f|^{2}| \rightarrow 0$ pointwise almost everywhere. However, I can't think of any function which dominates this expression.","Let $f_{n}: \mathbb{R}^{d} \rightarrow \mathbb{R}$ such that $\sup_{n}\|f_{n}\|_{L^{2}} < \infty$. Furthermore suppose $f_{n} \rightarrow f$ pointwise almost everywhere for some $f$. The problem I am working on is to show that $$\int_{\mathbb{R}^{d}}||f_{n}|^{2} - |f_{n} - f|^{2} - |f|^{2}|\, dx \rightarrow 0$$ as $n \rightarrow \infty$. First I noticed that $||f_{n}|^{2} - |f_{n} - f|^{2} - |f|^{2}| \rightarrow 0$ pointwise almost everywhere. However, I can't think of any function which dominates this expression.",,"['real-analysis', 'analysis']"
97,Calculating $\int_0^\pi \sin^2t\;dt$ using the residue theorem,Calculating  using the residue theorem,\int_0^\pi \sin^2t\;dt,"I want to use the residue theorem to calculate $$I:=\int_0^\pi \sin^2t\;dt$$ Since $\sin^2$ is an even function, we've got $$I=\frac{1}{2}\int_0^{2\pi}\sin^2t\;dt$$ The solution of this exercise defines $$f(z):=\frac{1}{iz}\left(\frac{1}{2i}\left(z-\frac{1}{z}\right)\right)^2=-\frac{1}{4i}\left(z-\frac{2}{z}+\frac{1}{z^3}\right)$$ concludes that $$\operatorname{res}(f,0)=\left(-\frac{1}{4i}\right)\cdot (-2)=\frac{1}{2i}$$ and $$I=\frac{1}{2}\left\{2\pi i\cdot \frac{1}{2i}\right\}=\frac{\pi}{2}$$ I don't understand what they're doing; especially $f$ seems to have no relationship to the original integrand. My approach was to try to write the integral as a contour integral: I hoped something like $$\frac{1}{2i}\int_{|z|=1}\frac{\sin^2z}{z}dz$$ would be worth a consideration. However, this integral is $0$; so, not exactly $I$.","I want to use the residue theorem to calculate $$I:=\int_0^\pi \sin^2t\;dt$$ Since $\sin^2$ is an even function, we've got $$I=\frac{1}{2}\int_0^{2\pi}\sin^2t\;dt$$ The solution of this exercise defines $$f(z):=\frac{1}{iz}\left(\frac{1}{2i}\left(z-\frac{1}{z}\right)\right)^2=-\frac{1}{4i}\left(z-\frac{2}{z}+\frac{1}{z^3}\right)$$ concludes that $$\operatorname{res}(f,0)=\left(-\frac{1}{4i}\right)\cdot (-2)=\frac{1}{2i}$$ and $$I=\frac{1}{2}\left\{2\pi i\cdot \frac{1}{2i}\right\}=\frac{\pi}{2}$$ I don't understand what they're doing; especially $f$ seems to have no relationship to the original integrand. My approach was to try to write the integral as a contour integral: I hoped something like $$\frac{1}{2i}\int_{|z|=1}\frac{\sin^2z}{z}dz$$ would be worth a consideration. However, this integral is $0$; so, not exactly $I$.",,"['complex-analysis', 'analysis', 'contour-integration', 'residue-calculus']"
98,Does a function that is twice weakly differentiable have a version that is classically differentiable?,Does a function that is twice weakly differentiable have a version that is classically differentiable?,,"I have been wondering about the idea of functions that are weakly differentiable. My intuition tells me that the weak derivative allows one to differentiate functions that either have a removable discontinuity or have a kink. Functions with a jump that cannot be ""repaired"" are not weakly differentiable. Furthermore, it seems that if we take the weak derivative of a non-classically differentiable function $f$ with a kink at some point $x$, then the weak derivative will have a jump at $x$ since the limits of the classical derivative approaching $x$ from the left and the right will differ. It would therefore follow that the weak derivative of $f$ is not weakly differentiable. Is this idea correct? If so, does that imply that in order for the second weak derivative to exist, the function is equivalent to (in the Lebesgue sense) a classically differentiable function?","I have been wondering about the idea of functions that are weakly differentiable. My intuition tells me that the weak derivative allows one to differentiate functions that either have a removable discontinuity or have a kink. Functions with a jump that cannot be ""repaired"" are not weakly differentiable. Furthermore, it seems that if we take the weak derivative of a non-classically differentiable function $f$ with a kink at some point $x$, then the weak derivative will have a jump at $x$ since the limits of the classical derivative approaching $x$ from the left and the right will differ. It would therefore follow that the weak derivative of $f$ is not weakly differentiable. Is this idea correct? If so, does that imply that in order for the second weak derivative to exist, the function is equivalent to (in the Lebesgue sense) a classically differentiable function?",,"['calculus', 'analysis', 'functional-analysis']"
99,lower bound of modified Besselfunction,lower bound of modified Besselfunction,,i'm looking for an lower bound for the modified Bessel function of the first kind $I_\nu(x)$ of a +ive real argument. There should be one of the form $$ce^{x^\alpha} \le I_\nu(x)$$.,i'm looking for an lower bound for the modified Bessel function of the first kind $I_\nu(x)$ of a +ive real argument. There should be one of the form $$ce^{x^\alpha} \le I_\nu(x)$$.,,"['real-analysis', 'analysis', 'inequality', 'special-functions', 'approximation']"
