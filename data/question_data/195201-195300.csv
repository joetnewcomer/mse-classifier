,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Are there any continuous time-limited Linear and Time-Invariant (LIT) functions with unbounded derivative?,Are there any continuous time-limited Linear and Time-Invariant (LIT) functions with unbounded derivative?,,"Are there any continuous time-limited Linear and Time-Invariant (LIT) functions with unbounded derivative? Let think about a continuous and time-limited function $q(t)$ that is representing a classical mechanics phenomena, which can be represented as the output of a LIT system with impulse response $h(t)$ . Now, I want to know which is the maximum rate of change that can possible achieve the function $q(t)$ , so as a worst case I test this system against a discontinuous jump-alike change by using as input the unitary standard step function $\theta(t)$ , so: $$q(t) = h(t)\circledast\theta(t)$$ related through the convolution operator as every LIT system. Now, I believe that since $q(t)$ is continuous and time limited, and $\theta(t)$ is not a compacted-supported function, the only alternative to made $q(t)$ as it is, is by an $h(t)$ function that is also continuous and time-limited ( please correct me if I am wrong, this is the most important assumption on the presented line-of-thought ). With this, since $q(t)$ and $h(t)$ are both continuous and compact-supported (because they are both time-limited), they are also bounded functions $\sup_t |q(t)| < \infty$ and $\sup_t |h(t)| < \infty$ . And also remember, that since $q(t)$ and $h(t)$ are both time-limited, it implies that both are also of unlimited bandwidth (i.e., they aren´t band-limited functions). Now, following the exercise n° 4.49 of the book ""Signals and Systems, 2nd Edition"" (Alan V. Oppenheim, Alan S. Willsky, with S. Hamid) [1] , and the following properties: $\frac{d}{dt}\Big(x(t) \circledast y(t) \Big) = \frac{dx(t)}{dt} \circledast y(t) = x(t) \circledast \frac{dy(t)}{dt}$ as is shown on Wiki : Algebraic Properties --> Relation with differentitation. The derivative of the unitary step function is the Dirac's delta function $\theta'(t) = \delta(t)$ , as is shown on Wiki . and $f(t) = f(t) \circledast \delta(t)$ as is shown on Wiki : Algebraic Properties --> Multiplicative identity. So, I will have that $$q'(t) = h(t) \circledast \theta'(t) = h(t) \circledast \delta(t) = h(t)$$ and since $|h(t)|< \infty$ is bounded if my first assumptions are right, I will have that the maximum rate of change of $$\sup_t |q'(t)| = sup_t |h(t)| < \infty$$ so even they are functions of unlimited bandwidth they will ALWAYS have a bounded derivative : A) Does this means that every continuous and time-limited LIT functions have a bounded maximum rate of change $\sup_t |q'(t)|< \infty$ ? By construction, I already know that if impulse response $h(t)$ is continuous and time-limited then the maximum rate of change of $q(t)$ will be bounded, if the analysis against the unitary step $\theta(t)$ is general enough ( Is that so? ). But in the other way, if $q(t)$ is continuous and time-limited, I feel that it could not be necessarily implying that $h(t)$ is also continuous and time-limited, since $q(t) = h(t) \circledast \theta(t) = \int_{-\infty}^t h(t)\,dt$ and I am not sure if the integral of a discontinuous function could lead to a continuous function, hope you can explain this too . Added later: I have just notice that any function can be described as: $$q(t) = \int_{-\infty}^t q'(t)\,dt = q'(t)\circledast \theta(t)$$ so requiring that the impulse response function $h(t) \equiv q'(t)$ to be continuous and time-limited is already requiring that $|q'(t)|<\infty$ been bounded. Other interesting facts from exercise 4.49: Since from the properties of the Laplace Transform of $q(t)$ given by $Q(s)$ , I will have that: $$ \lim_{t \to \infty} q(t) = q(\infty) = \lim_{s \to 0} s\,Q(s) = \lim_{s \to 0} \int_{-\infty}^\infty \frac{dq(t)}{dt}e^{-st}dt = \int_{-\infty}^\infty q'(t)\,dt = \int_{-\infty}^\infty h(t)\,dt$$ so I can model the ""minimum possible response time to the step function"" $\Delta t_{\min}$ as: $$ \sup_t |q'(t)| = \Bigg| \frac{q(\infty)}{\Delta t_{min}}\Bigg| \Rightarrow \Delta t_{min} = \frac{|q(\infty)|}{\sup_t|q'(t)|} = \frac{|\int_{-\infty}^\infty h(t)\,dt|}{\sup_t |h(t)|} $$ so I could define an ""effective Bandwidth"" for this system response to the step function using as maximum frequency the quantity $f_{max} = 1/ \Delta t_{min}$ : $$B_W = 2 f_{max} = \frac{2}{\Delta t_{min}} = \frac{2\sup_t |h(t)|}{|\int_{-\infty}^\infty h(t)\,dt|}$$ with Uncertainty relation $$ B_W \cdot \Delta t_{min} > 2$$ .","Are there any continuous time-limited Linear and Time-Invariant (LIT) functions with unbounded derivative? Let think about a continuous and time-limited function that is representing a classical mechanics phenomena, which can be represented as the output of a LIT system with impulse response . Now, I want to know which is the maximum rate of change that can possible achieve the function , so as a worst case I test this system against a discontinuous jump-alike change by using as input the unitary standard step function , so: related through the convolution operator as every LIT system. Now, I believe that since is continuous and time limited, and is not a compacted-supported function, the only alternative to made as it is, is by an function that is also continuous and time-limited ( please correct me if I am wrong, this is the most important assumption on the presented line-of-thought ). With this, since and are both continuous and compact-supported (because they are both time-limited), they are also bounded functions and . And also remember, that since and are both time-limited, it implies that both are also of unlimited bandwidth (i.e., they aren´t band-limited functions). Now, following the exercise n° 4.49 of the book ""Signals and Systems, 2nd Edition"" (Alan V. Oppenheim, Alan S. Willsky, with S. Hamid) [1] , and the following properties: as is shown on Wiki : Algebraic Properties --> Relation with differentitation. The derivative of the unitary step function is the Dirac's delta function , as is shown on Wiki . and as is shown on Wiki : Algebraic Properties --> Multiplicative identity. So, I will have that and since is bounded if my first assumptions are right, I will have that the maximum rate of change of so even they are functions of unlimited bandwidth they will ALWAYS have a bounded derivative : A) Does this means that every continuous and time-limited LIT functions have a bounded maximum rate of change ? By construction, I already know that if impulse response is continuous and time-limited then the maximum rate of change of will be bounded, if the analysis against the unitary step is general enough ( Is that so? ). But in the other way, if is continuous and time-limited, I feel that it could not be necessarily implying that is also continuous and time-limited, since and I am not sure if the integral of a discontinuous function could lead to a continuous function, hope you can explain this too . Added later: I have just notice that any function can be described as: so requiring that the impulse response function to be continuous and time-limited is already requiring that been bounded. Other interesting facts from exercise 4.49: Since from the properties of the Laplace Transform of given by , I will have that: so I can model the ""minimum possible response time to the step function"" as: so I could define an ""effective Bandwidth"" for this system response to the step function using as maximum frequency the quantity : with Uncertainty relation .","q(t) h(t) q(t) \theta(t) q(t) = h(t)\circledast\theta(t) q(t) \theta(t) q(t) h(t) q(t) h(t) \sup_t |q(t)| < \infty \sup_t |h(t)| < \infty q(t) h(t) \frac{d}{dt}\Big(x(t) \circledast y(t) \Big) = \frac{dx(t)}{dt} \circledast y(t) = x(t) \circledast \frac{dy(t)}{dt} \theta'(t) = \delta(t) f(t) = f(t) \circledast \delta(t) q'(t) = h(t) \circledast \theta'(t) = h(t) \circledast \delta(t) = h(t) |h(t)|< \infty \sup_t |q'(t)| = sup_t |h(t)| < \infty \sup_t |q'(t)|< \infty h(t) q(t) \theta(t) q(t) h(t) q(t) = h(t) \circledast \theta(t) = \int_{-\infty}^t h(t)\,dt q(t) = \int_{-\infty}^t q'(t)\,dt = q'(t)\circledast \theta(t) h(t) \equiv q'(t) |q'(t)|<\infty q(t) Q(s)  \lim_{t \to \infty} q(t) = q(\infty) = \lim_{s \to 0} s\,Q(s) = \lim_{s \to 0} \int_{-\infty}^\infty \frac{dq(t)}{dt}e^{-st}dt = \int_{-\infty}^\infty q'(t)\,dt = \int_{-\infty}^\infty h(t)\,dt \Delta t_{\min}  \sup_t |q'(t)| = \Bigg| \frac{q(\infty)}{\Delta t_{min}}\Bigg| \Rightarrow \Delta t_{min} = \frac{|q(\infty)|}{\sup_t|q'(t)|} = \frac{|\int_{-\infty}^\infty h(t)\,dt|}{\sup_t |h(t)|}  f_{max} = 1/ \Delta t_{min} B_W = 2 f_{max} = \frac{2}{\Delta t_{min}} = \frac{2\sup_t |h(t)|}{|\int_{-\infty}^\infty h(t)\,dt|}  B_W \cdot \Delta t_{min} > 2","['derivatives', 'fourier-analysis', 'physics', 'signal-processing', 'finite-duration']"
1,Are there functions $f(t)$ with $||f'(t)||_\infty < \infty$ such as their Fourier transform $F(w)$ makes $\int_{-\infty}^\infty|wF(w)|dw \to \infty$??,Are there functions  with  such as their Fourier transform  makes ??,f(t) ||f'(t)||_\infty < \infty F(w) \int_{-\infty}^\infty|wF(w)|dw \to \infty,"Are there any time-limited and continuous one-variable functions $f(t)$ with bounded derivative $||f'(t)||_\infty < \infty$ (not meaning here they are also necessarily differentiable), such as their Fourier transform $F(w)$ makes diverge the following integral $\int\limits_{-\infty}^\infty|iwF(w)+f(t_F)\,e^{-iwt_F}-f(t_0)\,e^{-iwt_0}|\,dw \to \infty$ ?? Or these kind of functions are an empty set (for each of the following scenarios)? The different terms from the questions of the tittle are just for avoiding the effects of the discontinuity on the edges of the compact-support $\partial t = \{t_0,\,t_F\}$ (starting and ending times), since they introduce Dirac's Delta functions $\delta(t)$ in the derivative $f'(t)$ (""artificially"" in my opinion, since to model time limited phenomena I am interested only in what is happening ""within"" the compact support). If you feel uncomfortable with them, just assume also that the functions $f(t)$ begins and finishes at zero $f(t_0)=f(t_F)=0$ . From the following, I will use both definitions as equivalent since the problem is avoidable (I explained one way to overcome it here ). Please keep it in mind, or it will make harder to find counterexamples since this edges-discontinuities will make the standard $\int_{-\infty}^\infty|wF(w)|dw$ always diverge, since the derivative will be unbounded because of these delta functions, as I will explain now. I am trying to understand the figure of the integral $\int_{-\infty}^\infty|wF(w)|dw$ which is an upper bound for the maximum rate of change of the function $f(t)$ : $$ \sup\limits_t \left| f'(t)\right| \leq \int_{-\infty}^\infty|wF(w)|dw$$ It has an individual name? (as the Dirichlet Energy, as example), this for being able to look for its properties by myself. Any references are welcome. Directly from the inequality I know that if the derivative is unbounded $||f'(t)||_\infty \to \infty \Rightarrow \int_{-\infty}^\infty|wF(w)|dw \to \infty$ will always diverge, and conversely, if ""this"" integral is bounded $\int_{-\infty}^\infty|wF(w)|dw < \infty \Rightarrow ||f'(t)||_\infty < \infty$ the maximum rate of change will be bounded (even when time-limited functions has unlimited bandwidth on the frequencies), but I want to know if there exists any cases of functions that lie in-between these two scenarios (I have already looked unsuccessfully for counterexamples by myself). I am specially interested in these five scenarios (from less to more restrictive - I believe): General time-limited and continuous one-variable functions $f(t)$ , as is already asked Time-limited continuous one-variable functions which are also absolutely integrable $\int\limits_{t_0}^{t_F}|f(t)|\,dt < \infty$ and energy finite $\int\limits_{t_0}^{t_F}|f(t)|^2 dt < \infty$ Functions that fulfill (1) and (2) and are also have their absolute value of its Fourier Transform bounded $\int\limits_{-\infty}^{\infty} |F(w)| dw < \infty$ Functions that fulfill (1) to (3) and also have finite Dirichlet Energy $\int\limits_{t_0}^{t_F} |f'(t)|^2 dt < \infty$ Functions that fulfill (1) to (4) and there also of bounded total variation $V_{[t_0,\,t_F]}(f(t)) < \infty$ I want to know if any of these intermediate conditions stages makes the integral $\int_{-\infty}^\infty|wF(w)|dw$ becomes bounded, or if are totally unrelated . Please notice that neither of these conditions are requiring to $f(t)$ to be differentiable. But I am not interested in ""bad-behaved"" things like nowhere-differentiable functions as Brownian motions, or fractals, or Cantor or Weierstrass functions, and things like that (at least not this time) Any counterexample will be welcome either. Beforehand, thanks you very much.","Are there any time-limited and continuous one-variable functions with bounded derivative (not meaning here they are also necessarily differentiable), such as their Fourier transform makes diverge the following integral ?? Or these kind of functions are an empty set (for each of the following scenarios)? The different terms from the questions of the tittle are just for avoiding the effects of the discontinuity on the edges of the compact-support (starting and ending times), since they introduce Dirac's Delta functions in the derivative (""artificially"" in my opinion, since to model time limited phenomena I am interested only in what is happening ""within"" the compact support). If you feel uncomfortable with them, just assume also that the functions begins and finishes at zero . From the following, I will use both definitions as equivalent since the problem is avoidable (I explained one way to overcome it here ). Please keep it in mind, or it will make harder to find counterexamples since this edges-discontinuities will make the standard always diverge, since the derivative will be unbounded because of these delta functions, as I will explain now. I am trying to understand the figure of the integral which is an upper bound for the maximum rate of change of the function : It has an individual name? (as the Dirichlet Energy, as example), this for being able to look for its properties by myself. Any references are welcome. Directly from the inequality I know that if the derivative is unbounded will always diverge, and conversely, if ""this"" integral is bounded the maximum rate of change will be bounded (even when time-limited functions has unlimited bandwidth on the frequencies), but I want to know if there exists any cases of functions that lie in-between these two scenarios (I have already looked unsuccessfully for counterexamples by myself). I am specially interested in these five scenarios (from less to more restrictive - I believe): General time-limited and continuous one-variable functions , as is already asked Time-limited continuous one-variable functions which are also absolutely integrable and energy finite Functions that fulfill (1) and (2) and are also have their absolute value of its Fourier Transform bounded Functions that fulfill (1) to (3) and also have finite Dirichlet Energy Functions that fulfill (1) to (4) and there also of bounded total variation I want to know if any of these intermediate conditions stages makes the integral becomes bounded, or if are totally unrelated . Please notice that neither of these conditions are requiring to to be differentiable. But I am not interested in ""bad-behaved"" things like nowhere-differentiable functions as Brownian motions, or fractals, or Cantor or Weierstrass functions, and things like that (at least not this time) Any counterexample will be welcome either. Beforehand, thanks you very much.","f(t) ||f'(t)||_\infty < \infty F(w) \int\limits_{-\infty}^\infty|iwF(w)+f(t_F)\,e^{-iwt_F}-f(t_0)\,e^{-iwt_0}|\,dw \to \infty \partial t = \{t_0,\,t_F\} \delta(t) f'(t) f(t) f(t_0)=f(t_F)=0 \int_{-\infty}^\infty|wF(w)|dw \int_{-\infty}^\infty|wF(w)|dw f(t)  \sup\limits_t \left| f'(t)\right| \leq \int_{-\infty}^\infty|wF(w)|dw ||f'(t)||_\infty \to \infty \Rightarrow \int_{-\infty}^\infty|wF(w)|dw \to \infty \int_{-\infty}^\infty|wF(w)|dw < \infty \Rightarrow ||f'(t)||_\infty < \infty f(t) \int\limits_{t_0}^{t_F}|f(t)|\,dt < \infty \int\limits_{t_0}^{t_F}|f(t)|^2 dt < \infty \int\limits_{-\infty}^{\infty} |F(w)| dw < \infty \int\limits_{t_0}^{t_F} |f'(t)|^2 dt < \infty V_{[t_0,\,t_F]}(f(t)) < \infty \int_{-\infty}^\infty|wF(w)|dw f(t)","['real-analysis', 'complex-analysis', 'derivatives', 'fourier-analysis', 'fourier-transform']"
2,Understanding numerator/denominator layout in matrix-calculus,Understanding numerator/denominator layout in matrix-calculus,,"This is a distilled version of this question. Consider the following: $$ \begin{align} z & = f(\mathbf{y}) \\ \mathbf{y} & = g(\mathbf{x}) \\ \text{where, } & z \in \mathbb{R} \text{, and} \\ & \mathbf{y}, \mathbf{x} \text{ are two $(1, m)$ dimensional vectors, i.e. row-vectors} \end{align} $$ Using numerator-layout , what is the dimension of the derivative $\frac{\mathrm{d}z}{\mathrm{d}\mathbf{y}}$ ? Should it be a column-vector of dimension $(m, 1)$ , because $\mathbf{y}$ is a row-vector of dimension $(1, m)$ ( Source ) But, using this notation causes issues while computing the derivative $\frac{\mathrm{d} z}{\mathrm{d} \mathbf{x}} = \frac{\mathrm{d} z}{\mathrm{d} \mathbf{y}} \frac{\mathrm{d} \mathbf{y}}{\mathrm{d} \mathbf{x}}$ ; since, $\frac{\mathrm{d} \mathbf{y}}{\mathrm{d} \mathbf{x}}$ would be an $(m, m)$ matrix, while $\frac{\mathrm{d} z}{\mathrm{d} \mathbf{y}}$ is an $(m, 1)$ vector. However, this notation does serve well when computing the derivatives of the form $\frac{\mathrm{d} h(\mathbf{X})}{\mathrm{d}\mathbf{X}}$ , where $\mathbf{X}$ is a matrix of dimension $(m, n)$ ; and $h(\mathbf{X})$ is a scalar-valued function. Or should it be a row-vector, because according to the numerator-layout the derivative has the dimensions --> $\text{numerator-dimension} \times (\text{denominator-dimension})^\intercal = (1,1)\times(m, 1)$ ( Source ) Also, (for this point) is my understanding even correct? PS: also, is there any definitive guide from which I can learn matrix-calculus from the first principals. Although, the following sources are good, they still leave a lot of gaps: Matrix-Calculus The Matrix-cookbook Old and New Matrix Algebra Useful for Statistics , by T. P. Minka The Matrix Calculus You Need For Deep Learning Matrix Differentiation Matrix Calculus The Guide for Matrix Calculus , by R.P. Pacelli.","This is a distilled version of this question. Consider the following: Using numerator-layout , what is the dimension of the derivative ? Should it be a column-vector of dimension , because is a row-vector of dimension ( Source ) But, using this notation causes issues while computing the derivative ; since, would be an matrix, while is an vector. However, this notation does serve well when computing the derivatives of the form , where is a matrix of dimension ; and is a scalar-valued function. Or should it be a row-vector, because according to the numerator-layout the derivative has the dimensions --> ( Source ) Also, (for this point) is my understanding even correct? PS: also, is there any definitive guide from which I can learn matrix-calculus from the first principals. Although, the following sources are good, they still leave a lot of gaps: Matrix-Calculus The Matrix-cookbook Old and New Matrix Algebra Useful for Statistics , by T. P. Minka The Matrix Calculus You Need For Deep Learning Matrix Differentiation Matrix Calculus The Guide for Matrix Calculus , by R.P. Pacelli.","
\begin{align}
z & = f(\mathbf{y}) \\
\mathbf{y} & = g(\mathbf{x}) \\
\text{where, } & z \in \mathbb{R} \text{, and} \\
& \mathbf{y}, \mathbf{x} \text{ are two (1, m) dimensional vectors, i.e. row-vectors}
\end{align}
 \frac{\mathrm{d}z}{\mathrm{d}\mathbf{y}} (m, 1) \mathbf{y} (1, m) \frac{\mathrm{d} z}{\mathrm{d} \mathbf{x}} = \frac{\mathrm{d} z}{\mathrm{d} \mathbf{y}} \frac{\mathrm{d} \mathbf{y}}{\mathrm{d} \mathbf{x}} \frac{\mathrm{d} \mathbf{y}}{\mathrm{d} \mathbf{x}} (m, m) \frac{\mathrm{d} z}{\mathrm{d} \mathbf{y}} (m, 1) \frac{\mathrm{d} h(\mathbf{X})}{\mathrm{d}\mathbf{X}} \mathbf{X} (m, n) h(\mathbf{X}) \text{numerator-dimension} \times (\text{denominator-dimension})^\intercal = (1,1)\times(m, 1)","['derivatives', 'vector-analysis', 'matrix-calculus', 'machine-learning', 'neural-networks']"
3,Counterexample of multiplicative Landau inequality in finite interval,Counterexample of multiplicative Landau inequality in finite interval,,"Question: Is there a sequence of functions $(f_n) \subseteq C^2(I)$ such that $$\lim_{n\to \infty}\dfrac {\|f_n'\|} {\|f_n\|^{\frac 1 2}\|f_n''\|^{\frac 1 2}} = \infty$$ where $I$ is the unit interval and the norms are all sup-norms? Landau Inequality states for any $f \in C^2(\mathbb R)$ the following holds $$\|f'\| \le 2 \|f\|^{\frac 1 2}\|f''\|^{\frac 1 2}.$$ There is a generalization of this inequality for $f \in C^2(\mathbb R^+)$ . For $f \in C^2(I)$ , the Gorny's Inequality states $$\|f'\| \le C \|f\|^{\frac 1 2}\max\{\|f''\|, 2\|f\|\}^{\frac 1 2}.$$ However, the right term of the above inequality is not the same as in Landau Inequality. I have also found this paper ( Landau-Kolmogorov inequality on a finite interval ), which says for $f \in C^2(I)$ $$\|f'\|_{[\delta, 1-\delta]} \le C \|f\|^{\frac 1 2}\|f''\|^{\frac 1 2}$$ for some $\delta > 0$ . The Wikipedia page of Landau–Kolmogorov inequality says there are some generalizations of Landau Inequality in finite interval but its expression is very vague, and it provides no references. I wonder whether there is really a constant $C$ that for every $f \in C^2(I)$ $$\|f'\| \le C \|f\|^{\frac 1 2}\|f''\|^{\frac 1 2}.$$","Question: Is there a sequence of functions such that where is the unit interval and the norms are all sup-norms? Landau Inequality states for any the following holds There is a generalization of this inequality for . For , the Gorny's Inequality states However, the right term of the above inequality is not the same as in Landau Inequality. I have also found this paper ( Landau-Kolmogorov inequality on a finite interval ), which says for for some . The Wikipedia page of Landau–Kolmogorov inequality says there are some generalizations of Landau Inequality in finite interval but its expression is very vague, and it provides no references. I wonder whether there is really a constant that for every","(f_n) \subseteq C^2(I) \lim_{n\to \infty}\dfrac {\|f_n'\|} {\|f_n\|^{\frac 1 2}\|f_n''\|^{\frac 1 2}} = \infty I f \in C^2(\mathbb R) \|f'\| \le 2 \|f\|^{\frac 1 2}\|f''\|^{\frac 1 2}. f \in C^2(\mathbb R^+) f \in C^2(I) \|f'\| \le C \|f\|^{\frac 1 2}\max\{\|f''\|, 2\|f\|\}^{\frac 1 2}. f \in C^2(I) \|f'\|_{[\delta, 1-\delta]} \le C \|f\|^{\frac 1 2}\|f''\|^{\frac 1 2} \delta > 0 C f \in C^2(I) \|f'\| \le C \|f\|^{\frac 1 2}\|f''\|^{\frac 1 2}.","['real-analysis', 'calculus', 'derivatives', 'inequality']"
4,How to use differentials to estimate the relativistic increase in mass from at 90% speed of light to 92%,How to use differentials to estimate the relativistic increase in mass from at 90% speed of light to 92%,,"the question below is from a section on differentials in an old calculus text I am teaching myself with (Calculus, Varberg & Purcell, 6th edition.) The text gives an answer (9.47%) but does not explain how it is derived. That is what I wish to learn. I understand how to calculate the answer using the equation, but not how to estimate it using differentials. I believe dv = .02, and I have to multiply that by the derivative of the equation below to find dm. But I am confused by the c^2. (which is in fact a constant, further confounding me.)  Must I first express v in terms of c? (for example .9c^2 / c^2, in order to differentiate?  Or possibly I should try implicit differentiation?  Here's the question- any help would be greatly appreciated.  thank you! ""Einstein's Special Theory of Relativity says that the mass m of an object moving at a velocity v is given by the formula: m = m0 (1- v^2/c^2)^(-1/2) Here m0 is the rest mass (mass at velocity 0) and c is the speed of light. Use differentials to estimate the percentage increase of the object as its velocity increases from v=0.90c to 0.92c.""","the question below is from a section on differentials in an old calculus text I am teaching myself with (Calculus, Varberg & Purcell, 6th edition.) The text gives an answer (9.47%) but does not explain how it is derived. That is what I wish to learn. I understand how to calculate the answer using the equation, but not how to estimate it using differentials. I believe dv = .02, and I have to multiply that by the derivative of the equation below to find dm. But I am confused by the c^2. (which is in fact a constant, further confounding me.)  Must I first express v in terms of c? (for example .9c^2 / c^2, in order to differentiate?  Or possibly I should try implicit differentiation?  Here's the question- any help would be greatly appreciated.  thank you! ""Einstein's Special Theory of Relativity says that the mass m of an object moving at a velocity v is given by the formula: m = m0 (1- v^2/c^2)^(-1/2) Here m0 is the rest mass (mass at velocity 0) and c is the speed of light. Use differentials to estimate the percentage increase of the object as its velocity increases from v=0.90c to 0.92c.""",,['derivatives']
5,Inequalities for $\mathcal C^1$ functions satisfying $f^\prime(x) \leq af(x)+b$,Inequalities for  functions satisfying,\mathcal C^1 f^\prime(x) \leq af(x)+b,"Let $f : \mathbb{R} \to \mathbb{R}$ be a class $C^1$ function which vanishes in $0$ and which satisfies: $$\forall x \geq 0,\quad  f'(x) \leq af(x)+b \quad [a > 0, b \geqslant 0] $$ We try to show that: $$\forall x \geq 0, \quad f(x) \leq {b \frac{e^{ax}-1}{a}}$$ and $$f'(x) \leq be^{ax}$$ Could you direct me to the demo please? I don't see how to start ... Thank you",Let be a class function which vanishes in and which satisfies: We try to show that: and Could you direct me to the demo please? I don't see how to start ... Thank you,"f : \mathbb{R} \to \mathbb{R} C^1 0 \forall x \geq 0,\quad  f'(x) \leq af(x)+b \quad [a > 0, b \geqslant 0]  \forall x \geq 0, \quad f(x) \leq {b \frac{e^{ax}-1}{a}} f'(x) \leq be^{ax}","['derivatives', 'inequality', 'exponential-function']"
6,Why there is no $y_i$ term in $\frac{d^{2} J(\boldsymbol{\alpha})}{d \alpha_{i}^{2}}$?,Why there is no  term in ?,y_i \frac{d^{2} J(\boldsymbol{\alpha})}{d \alpha_{i}^{2}},$$\frac{d^{2} J(\boldsymbol{\alpha})}{d \alpha_{i}^{2}}=\lambda^{-1} \mathbf{x}_{i}^{\mathrm{T}} \mathbf{x}_{i}+\frac{1}{\alpha_{i}\left(1-\alpha_{i}\right)}$$ I cannot understand why there is no $y_i$ term in $\frac{d^{2} J(\boldsymbol{\alpha})}{d \alpha_{i}^{2}}.$ $$\mathbf{w}(\boldsymbol{\alpha})=\lambda^{-1} \sum_{i} \alpha_{i} y_{i} \mathbf{x}_{i}$$ $$J(\alpha)=\frac{1}{2 \lambda} \sum_{i j} \alpha_{i} \alpha_{j} y_{i} y_{j} \mathbf{x}_{j}^{\mathrm{T}} \mathbf{x}_{i}-\sum_{i} H\left(\alpha_{i}\right)$$ \begin{aligned} \frac{d J(\boldsymbol{\alpha})}{d \alpha_{i}} &=\lambda^{-1} y_{i} \sum_{j} \alpha_{j} y_{j} \mathbf{x}_{j}^{\mathrm{T}} \mathbf{x}_{i}+\log \frac{\alpha_{i}}{1-\alpha_{i}} \\ &=y_{i} \mathbf{w}(\boldsymbol{\alpha})^{\mathrm{T}} \mathbf{x}_{i}+\log \frac{\alpha_{i}}{1-\alpha_{i}} \end{aligned},I cannot understand why there is no term in,"\frac{d^{2} J(\boldsymbol{\alpha})}{d \alpha_{i}^{2}}=\lambda^{-1} \mathbf{x}_{i}^{\mathrm{T}} \mathbf{x}_{i}+\frac{1}{\alpha_{i}\left(1-\alpha_{i}\right)} y_i \frac{d^{2} J(\boldsymbol{\alpha})}{d \alpha_{i}^{2}}. \mathbf{w}(\boldsymbol{\alpha})=\lambda^{-1} \sum_{i} \alpha_{i} y_{i} \mathbf{x}_{i} J(\alpha)=\frac{1}{2 \lambda} \sum_{i j} \alpha_{i} \alpha_{j} y_{i} y_{j} \mathbf{x}_{j}^{\mathrm{T}} \mathbf{x}_{i}-\sum_{i} H\left(\alpha_{i}\right) \begin{aligned}
\frac{d J(\boldsymbol{\alpha})}{d \alpha_{i}} &=\lambda^{-1} y_{i} \sum_{j} \alpha_{j} y_{j} \mathbf{x}_{j}^{\mathrm{T}} \mathbf{x}_{i}+\log \frac{\alpha_{i}}{1-\alpha_{i}} \\
&=y_{i} \mathbf{w}(\boldsymbol{\alpha})^{\mathrm{T}} \mathbf{x}_{i}+\log \frac{\alpha_{i}}{1-\alpha_{i}}
\end{aligned}","['derivatives', 'matrix-calculus']"
7,"Is it possible to exchange the derivative sign with the integral sign in $\;\frac{d}{dy}(\int_0^\infty F(x)\frac{e^{-x/y}}{y}\,dx)\;$?",Is it possible to exchange the derivative sign with the integral sign in ?,"\;\frac{d}{dy}(\int_0^\infty F(x)\frac{e^{-x/y}}{y}\,dx)\;","I have to compute the following derivative $$\frac{d}{dy}\left(\int_0^\infty F(x)\frac{e^{-x/y}}{y}\,dx\right)$$ where $F\colon\mathbb R^+\to\mathbb R^+$ . I would like to pass the derivative sign inside the improper integral in order to conclude that \begin{align}\frac{d}{dy}\left(\int_0^\infty F(x)\frac{e^{-x/y}}{y}\,dx\right)= \int_0^\infty F(x)\frac{\partial}{\partial y}\left(\frac{e^{-x/y}}{y}\right)\,dx. \end{align} The problem is that the interval of the integral is not bounded so I cannot apply the classic theory. Do you know if there is any theorem that allows to switch the derivative and the integral sign when the interval of the integral is unbounded? Thank you",I have to compute the following derivative where . I would like to pass the derivative sign inside the improper integral in order to conclude that The problem is that the interval of the integral is not bounded so I cannot apply the classic theory. Do you know if there is any theorem that allows to switch the derivative and the integral sign when the interval of the integral is unbounded? Thank you,"\frac{d}{dy}\left(\int_0^\infty F(x)\frac{e^{-x/y}}{y}\,dx\right) F\colon\mathbb R^+\to\mathbb R^+ \begin{align}\frac{d}{dy}\left(\int_0^\infty F(x)\frac{e^{-x/y}}{y}\,dx\right)=
\int_0^\infty F(x)\frac{\partial}{\partial y}\left(\frac{e^{-x/y}}{y}\right)\,dx.
\end{align}","['real-analysis', 'integration', 'derivatives']"
8,Binomial rule and repeatedly differentiating,Binomial rule and repeatedly differentiating,,"Exercise from book: Show that for all positive integers n $$ \sum_{k=0}^{n}(-1)^{k}\left(\begin{array}{l} n \\ k \end{array}\right) k^{j}= \begin{cases}0, & j=0,1, \ldots, n-1 \\ (-1)^{n} n ! j=n\end{cases} $$ and hint from author: Hint. Expand $(1-x)^{n}$ by the binomial rule. Repeatedly differentiate, but with a twist. My question: what does mean differentiate with a twist??","Exercise from book: Show that for all positive integers n and hint from author: Hint. Expand by the binomial rule. Repeatedly differentiate, but with a twist. My question: what does mean differentiate with a twist??","
\sum_{k=0}^{n}(-1)^{k}\left(\begin{array}{l}
n \\
k
\end{array}\right) k^{j}= \begin{cases}0, & j=0,1, \ldots, n-1 \\
(-1)^{n} n ! j=n\end{cases}
 (1-x)^{n}","['real-analysis', 'derivatives', 'binomial-theorem']"
9,derivative of an implicit matrix function,derivative of an implicit matrix function,,"Let $\mathbf{V}$ be an $N \times N$ real symmetric (or complex Hermitian) positive definite matrix, such that $\mathrm{det}(\mathbf{V})=1$ . By means of the implicit function theorem, its first top-left entry $[V]_{1,1} \triangleq v_{11} $ can be expressed as: Real symmetric (positive definite) $\mathbf{V}$ : \begin{equation*} [V]_{1,1} \triangleq v_{11} = g_\mathbb{R}(v_{1,2},\ldots,v_{N,N}), \end{equation*} where $v_{1,2},\ldots,v_{N,N}$ are the $N \times (N+1)/2-1$ entries of the upper triangular submatrix except $v_{11}$ , Complex Hermitian (positive definite) $\mathbf{V}$ : \begin{equation*} [V]_{1,1} \triangleq v_{11} = g_\mathbb{C}(v_{1,2},v_{2,1}\ldots,v_{N,N}), \end{equation*} where $v_{1,2},v_{2,1},\ldots,v_{N,N}$ are the $N^2 - 1$ entries of $\mathbf{V}$ except $v_{11}$ , and $g_{\mathbb{R}}$ , $g_{\mathbb{C}}$ are differentiable functions (but I haven't its explicit expression). How can I explicitly evaluate the following (column) vectors ? \begin{equation*} 	\mathbf{s}_{\mathbb{C}} = \frac{\partial g([\mathbf{V}]_{2,1},\ldots,[\mathbf{V}]_{N,N})}{\partial \underline{\mathrm{vec}}(\mathbf{V})} \end{equation*} for $\mathbf{V} \in \mathbb{C}^{N \times N}$ (Hermitian positive definite) and where $\mathrm{vec}(\mathbf{V}) \triangleq [v_{11},\underline{\mathrm{vec}}(\mathbf{V})^T]^T$ and \begin{equation*} 	\mathbf{s}_{\mathbb{R}} = \frac{\partial g([\mathbf{V}]_{2,1},\ldots,[\mathbf{V}]_{N,N})}{\partial \underline{\mathrm{vech}}(\mathbf{V})} \end{equation*} for $\mathbf{V} \in \mathbb{R}^{N \times N}$ (symmetric positive definite) and where $\mathrm{vech}(\mathbf{V}) \triangleq [v_{11},\underline{\mathrm{vech}}(\mathbf{V})^T]^T$ and $\mathrm{vech}$ is the (column-wise) vectorization of the upper triangular part of $\mathbf{V}$ . Thanks!","Let be an real symmetric (or complex Hermitian) positive definite matrix, such that . By means of the implicit function theorem, its first top-left entry can be expressed as: Real symmetric (positive definite) : where are the entries of the upper triangular submatrix except , Complex Hermitian (positive definite) : where are the entries of except , and , are differentiable functions (but I haven't its explicit expression). How can I explicitly evaluate the following (column) vectors ? for (Hermitian positive definite) and where and for (symmetric positive definite) and where and is the (column-wise) vectorization of the upper triangular part of . Thanks!","\mathbf{V} N \times N \mathrm{det}(\mathbf{V})=1 [V]_{1,1} \triangleq v_{11}  \mathbf{V} \begin{equation*}
[V]_{1,1} \triangleq v_{11} = g_\mathbb{R}(v_{1,2},\ldots,v_{N,N}),
\end{equation*} v_{1,2},\ldots,v_{N,N} N \times (N+1)/2-1 v_{11} \mathbf{V} \begin{equation*}
[V]_{1,1} \triangleq v_{11} = g_\mathbb{C}(v_{1,2},v_{2,1}\ldots,v_{N,N}),
\end{equation*} v_{1,2},v_{2,1},\ldots,v_{N,N} N^2 - 1 \mathbf{V} v_{11} g_{\mathbb{R}} g_{\mathbb{C}} \begin{equation*}
	\mathbf{s}_{\mathbb{C}} = \frac{\partial g([\mathbf{V}]_{2,1},\ldots,[\mathbf{V}]_{N,N})}{\partial \underline{\mathrm{vec}}(\mathbf{V})}
\end{equation*} \mathbf{V} \in \mathbb{C}^{N \times N} \mathrm{vec}(\mathbf{V}) \triangleq [v_{11},\underline{\mathrm{vec}}(\mathbf{V})^T]^T \begin{equation*}
	\mathbf{s}_{\mathbb{R}} = \frac{\partial g([\mathbf{V}]_{2,1},\ldots,[\mathbf{V}]_{N,N})}{\partial \underline{\mathrm{vech}}(\mathbf{V})}
\end{equation*} \mathbf{V} \in \mathbb{R}^{N \times N} \mathrm{vech}(\mathbf{V}) \triangleq [v_{11},\underline{\mathrm{vech}}(\mathbf{V})^T]^T \mathrm{vech} \mathbf{V}","['linear-algebra', 'matrices', 'derivatives', 'matrix-calculus']"
10,Proving $\int_0^\infty e^{-t^2} \cos (tx) \ dt$ is differentiable.,Proving  is differentiable.,\int_0^\infty e^{-t^2} \cos (tx) \ dt,"Prove that $f$ defined on $\mathbb R$ and given by $f(x)=\displaystyle\int_0^\infty e^{-t^2} \cos (tx) \ dt$ is differentiable. Fix $a\in \mathbb R,$ then \begin{align} \lim_{h\to 0} \dfrac{f(a+h)-f(a)}{h} &=\lim_{h\to 0} \int_0^\infty e^{-t^2}\cdot  \dfrac{\cos(at+ht)-\cos(at)}{h}\ dt. \end{align} If I can use LCT, I can change the $\lim$ and $\int$ since the integrand is not grater than $|te^{-t^2}|$ and $te^{-t^2}$ is Lebesgue-integrable on $[0, \infty]$ and come to the conclusion. But this problem doesn't suppose the knowledge about Lebesgue integral theory. Thus this seems to be solved without using the  knowledge about Lebesgue integral theory. Is there the reason why I can change $\lim$ and $\int$ without Lebesgue integral theory, or the reason why $f$ is differentiable ?","Prove that defined on and given by is differentiable. Fix then If I can use LCT, I can change the and since the integrand is not grater than and is Lebesgue-integrable on and come to the conclusion. But this problem doesn't suppose the knowledge about Lebesgue integral theory. Thus this seems to be solved without using the  knowledge about Lebesgue integral theory. Is there the reason why I can change and without Lebesgue integral theory, or the reason why is differentiable ?","f \mathbb R f(x)=\displaystyle\int_0^\infty e^{-t^2} \cos (tx) \ dt a\in \mathbb R, \begin{align}
\lim_{h\to 0} \dfrac{f(a+h)-f(a)}{h}
&=\lim_{h\to 0} \int_0^\infty e^{-t^2}\cdot  \dfrac{\cos(at+ht)-\cos(at)}{h}\ dt.
\end{align} \lim \int |te^{-t^2}| te^{-t^2} [0, \infty] \lim \int f","['real-analysis', 'calculus', 'derivatives']"
11,Are there any explicit solutions known for this ODE?,Are there any explicit solutions known for this ODE?,,"I am considering the following ODE, $$u''+\frac{5}{r}u'+K\left(\frac{2}{1+4r^2}\right)^2u=0$$ where $K>0$ and $u=u(r):\mathbb{R}_{+}\to \mathbb{R}.$ Can we deduce that any solution $u$ to the above ODE decays as follows $$u(r)\leq \frac{C}{r^{\alpha}}?$$","I am considering the following ODE, where and Can we deduce that any solution to the above ODE decays as follows",u''+\frac{5}{r}u'+K\left(\frac{2}{1+4r^2}\right)^2u=0 K>0 u=u(r):\mathbb{R}_{+}\to \mathbb{R}. u u(r)\leq \frac{C}{r^{\alpha}}?,"['real-analysis', 'ordinary-differential-equations', 'derivatives', 'sturm-liouville']"
12,Determining if process $Z$ is a martingale.,Determining if process  is a martingale.,Z,"I have to determine whether a process $Z$ defined by $Z_0 = 0$ and $Z_t = W_t^5 - 10 \int_0^t W_u^3 du$ is a martingale. Where $W_t$ is a standard Brownian motion. I thought the best way to start was to determine $dZ_t$ and see what we get. So, I first used Ito's lemma to compute the first term. After some work I got $5W_t^4dW_t + 10 W_t^3dt$ . Then I wanted to compute $10 \int_0^t W_u^3 du$ , however I didn't know how to compute it. Maybe I can use Ito's lemma again, but I don't know how to do it. Could someone help me?","I have to determine whether a process defined by and is a martingale. Where is a standard Brownian motion. I thought the best way to start was to determine and see what we get. So, I first used Ito's lemma to compute the first term. After some work I got . Then I wanted to compute , however I didn't know how to compute it. Maybe I can use Ito's lemma again, but I don't know how to do it. Could someone help me?",Z Z_0 = 0 Z_t = W_t^5 - 10 \int_0^t W_u^3 du W_t dZ_t 5W_t^4dW_t + 10 W_t^3dt 10 \int_0^t W_u^3 du,"['derivatives', 'martingales', 'stochastic-differential-equations']"
13,finding derivative in Gronwall's lemma integral form proof,finding derivative in Gronwall's lemma integral form proof,,"taken from answer here and wiki proof Let: $v(s) = \exp\biggl({-}\int_a^s\beta(r)\,\mathrm{d}r\biggr)\int_a^s\beta(r)u(r)\,\mathrm{d}r,\qquad s\in I.$ Using the product rule, the chain rule, the derivative of the exponential function and the fundamental theorem of calculus, we obtain for the derivative $[\exp(-\int_a^s\beta(r)\,dr)(\int_a^s\beta(r)u(r)dr)]'=$ applying $(uv)'=u'v+uv'$ $(e^{-\int_a^s\beta(r)\,dr})'(\int_a^s\beta(r)u(r)dr)+(e^{-\int_a^s\beta(r)\,dr})(\int_a^s\beta(r)u(r)dr)'=$ Chain rule on the first summand exp term, Newton Leibniz formula to the right term $(e^{-\int_a^s\beta(r)\,dr})(-\int_a^s\beta(r)\,dr)'(\int_a^s\beta(r)u(r)dr)\quad+\quad(e^{-\int_a^s\beta(r)\,dr})(\beta(s)u(s)-\beta(a)u(a))=$ Newton Leibniz formula for the derivative of integral on the first summands factor $(e^{-\int_a^s\beta(r)\,dr})(-\beta(s) + \beta(a))(\int_a^s\beta(r)u(r)dr)\quad+\quad(e^{-\int_a^s\beta(r)\,dr})(\beta(s)u(s)-\beta(a)u(a))=$ pulling out exp term $(e^{-\int_a^s\beta(r)\,dr})[(-\beta(s) + \beta(a))(\int_a^s\beta(r)u(r)dr)\quad+\quad\beta(s)u(s)-\beta(a)u(a)]=$ $\beta(s),\beta(a)$ out $(e^{-\int_a^s\beta(r)\,dr})[\beta(s)(u(s)-\int_a^s\beta(r)u(r)dr)+\beta(a)(-u(s)+\int_a^s\beta(r)u(r)dr)]$ However, wiki has no term with $\beta(a)(-u(s)+\int_a^s\beta(r)u(r)dr)$ $v'(s) = \biggl(\underbrace{u(s)-\int_a^s\beta(r)u(r)\,\mathrm{d}r}_{\le\,\alpha(s)}\biggr)\beta(s)\exp\biggl({-}\int_a^s\beta(r)\mathrm{d}r\biggr), \qquad s\in I,$ are there any assumptions or given conditions so that the second summand vanishes?","taken from answer here and wiki proof Let: Using the product rule, the chain rule, the derivative of the exponential function and the fundamental theorem of calculus, we obtain for the derivative applying Chain rule on the first summand exp term, Newton Leibniz formula to the right term Newton Leibniz formula for the derivative of integral on the first summands factor pulling out exp term out However, wiki has no term with are there any assumptions or given conditions so that the second summand vanishes?","v(s) = \exp\biggl({-}\int_a^s\beta(r)\,\mathrm{d}r\biggr)\int_a^s\beta(r)u(r)\,\mathrm{d}r,\qquad s\in I. [\exp(-\int_a^s\beta(r)\,dr)(\int_a^s\beta(r)u(r)dr)]'= (uv)'=u'v+uv' (e^{-\int_a^s\beta(r)\,dr})'(\int_a^s\beta(r)u(r)dr)+(e^{-\int_a^s\beta(r)\,dr})(\int_a^s\beta(r)u(r)dr)'= (e^{-\int_a^s\beta(r)\,dr})(-\int_a^s\beta(r)\,dr)'(\int_a^s\beta(r)u(r)dr)\quad+\quad(e^{-\int_a^s\beta(r)\,dr})(\beta(s)u(s)-\beta(a)u(a))= (e^{-\int_a^s\beta(r)\,dr})(-\beta(s) + \beta(a))(\int_a^s\beta(r)u(r)dr)\quad+\quad(e^{-\int_a^s\beta(r)\,dr})(\beta(s)u(s)-\beta(a)u(a))= (e^{-\int_a^s\beta(r)\,dr})[(-\beta(s) + \beta(a))(\int_a^s\beta(r)u(r)dr)\quad+\quad\beta(s)u(s)-\beta(a)u(a)]= \beta(s),\beta(a) (e^{-\int_a^s\beta(r)\,dr})[\beta(s)(u(s)-\int_a^s\beta(r)u(r)dr)+\beta(a)(-u(s)+\int_a^s\beta(r)u(r)dr)] \beta(a)(-u(s)+\int_a^s\beta(r)u(r)dr) v'(s) = \biggl(\underbrace{u(s)-\int_a^s\beta(r)u(r)\,\mathrm{d}r}_{\le\,\alpha(s)}\biggr)\beta(s)\exp\biggl({-}\int_a^s\beta(r)\mathrm{d}r\biggr),
\qquad s\in I,","['calculus', 'integration', 'derivatives', 'proof-explanation', 'chain-rule']"
14,Total Derivative : How to correct the practical definition ? a+h out of set.,Total Derivative : How to correct the practical definition ? a+h out of set.,,"Definition : Total Derivative over an open set of a $\mathbb{R}$ vectorial space : Let E and F be two $\mathbb{R}$ vectorial spaces. Let $U \subset E$ be an open subset of E. Let $f:U \rightarrow F$ be an application and $x_0 \in U$ . $f$ is said to be totally differentiable in $x_0$ if and only if there exist a continuous linear map $L:E \rightarrow F$ that respects $$f(x_0+h)=f(x_0)+L(h)+o(\Vert h \Vert)$$ It is the definition i find in most books i have , but it is very ambiguous to me , 1)First of all, $h$ needs to be defined. It cannot be said in E (without any other condition) , or in U(without any other condition) , because else it would in all case contradict the definition of $f$ (first case , if we define h to be in E , no reason $x_0+h$ to be in U. second case if we define h to be in U , no reason $x_0+h$ to be in U.) A clear example is to consider $A\rightarrow A^{-1}$ from $GL_n(\mathbb{R})\rightarrow GL_n(\mathbb{R})$ . It is consider to be a differentiable function for a lot of author. But the sum of two inversible matrix has no reason to remain an inversible matrix. So what i understand is, we tinker it with hands , saying ""well consider h close enough from $x_0$ and everything will be fine as your set is open"" But L is unique , and so if i say Definition : Total Derivative over an open set of a $\mathbb{R}$ vectorial space : Let E and F be two $\mathbb{R}$ vectorial spaces. Let $U \subset E$ be an open subset of E. Let $f:U \rightarrow F$ be an application and $x_0 \in U$ . f is said to be totally differentiable in $x_0$ if and only if there exist a continuous linear map $L:E \rightarrow F$ that respects for all $h\in U$ such that $x_0+h\in U$ $$f(x_0+h)=f(x_0)+L(h)+o(\Vert h \Vert)$$ I feel like breaking the definition. Can someone give a reformulation of the definition that defines h properly and is well defined ?","Definition : Total Derivative over an open set of a vectorial space : Let E and F be two vectorial spaces. Let be an open subset of E. Let be an application and . is said to be totally differentiable in if and only if there exist a continuous linear map that respects It is the definition i find in most books i have , but it is very ambiguous to me , 1)First of all, needs to be defined. It cannot be said in E (without any other condition) , or in U(without any other condition) , because else it would in all case contradict the definition of (first case , if we define h to be in E , no reason to be in U. second case if we define h to be in U , no reason to be in U.) A clear example is to consider from . It is consider to be a differentiable function for a lot of author. But the sum of two inversible matrix has no reason to remain an inversible matrix. So what i understand is, we tinker it with hands , saying ""well consider h close enough from and everything will be fine as your set is open"" But L is unique , and so if i say Definition : Total Derivative over an open set of a vectorial space : Let E and F be two vectorial spaces. Let be an open subset of E. Let be an application and . f is said to be totally differentiable in if and only if there exist a continuous linear map that respects for all such that I feel like breaking the definition. Can someone give a reformulation of the definition that defines h properly and is well defined ?",\mathbb{R} \mathbb{R} U \subset E f:U \rightarrow F x_0 \in U f x_0 L:E \rightarrow F f(x_0+h)=f(x_0)+L(h)+o(\Vert h \Vert) h f x_0+h x_0+h A\rightarrow A^{-1} GL_n(\mathbb{R})\rightarrow GL_n(\mathbb{R}) x_0 \mathbb{R} \mathbb{R} U \subset E f:U \rightarrow F x_0 \in U x_0 L:E \rightarrow F h\in U x_0+h\in U f(x_0+h)=f(x_0)+L(h)+o(\Vert h \Vert),"['derivatives', 'differential', 'frechet-derivative']"
15,"Let $f:[0,2\pi]\rightarrow[-1,1]$ satisfy $f(\theta)=\sum_{r=0}^n(a_r\sin(r\theta)+b_r\cos(r\theta))$ for $a_h,b_i\in \mathbb R$. If $|f(x)|=1$...",Let  satisfy  for . If ...,"f:[0,2\pi]\rightarrow[-1,1] f(\theta)=\sum_{r=0}^n(a_r\sin(r\theta)+b_r\cos(r\theta)) a_h,b_i\in \mathbb R |f(x)|=1","Let $f:[0,2\pi]\rightarrow[-1,1]$ satisfy $f(\theta)=\sum_{r=0}^n(a_r\sin(r\theta)+b_r\cos(r\theta))$ for $a_h,b_i\in\mathbb R$ . If $|f(x)|=1$ for exactly $2n$ distinct values in $[0,2\pi)$ , then prove that the number of distinct solutions of $(f''(x))^2+f'(x)f'''(x)=0$ can be $4n,4n-1$ or $4n-2$ . I know that $-\sqrt{a^2+b^2}+c\leq a\cos(\theta)+b\sin(\theta)+c\leq \sqrt{a^2+b^2}+c$ . But this same formula can't be extended to the entire series since it's not necessary that $\sin(2\theta)$ and $\sin(\theta)$ give the minimum value for the same value of $\theta$ . If we prove that $1$ is the maximum value of $f(\theta)$ , we can prove that $f'(x)=0$ has $2n$ roots. This means $f''(x)$ has $2n-1$ roots (by Rolle's theorem). $f'(x)=\sum_{r=0}^nra_r\cos(r\theta)-rb_r\sin(r\theta)$ $f''(x)=\sum_{r=0}^n-r^2a_r\sin(r\theta)-r^2b_r\cos(r\theta)$ $f'''(x)=\sum_{r=0}^n -r^3a_r\cos(r\theta)+r^3b_r\sin(r\theta)$","Let satisfy for . If for exactly distinct values in , then prove that the number of distinct solutions of can be or . I know that . But this same formula can't be extended to the entire series since it's not necessary that and give the minimum value for the same value of . If we prove that is the maximum value of , we can prove that has roots. This means has roots (by Rolle's theorem).","f:[0,2\pi]\rightarrow[-1,1] f(\theta)=\sum_{r=0}^n(a_r\sin(r\theta)+b_r\cos(r\theta)) a_h,b_i\in\mathbb R |f(x)|=1 2n [0,2\pi) (f''(x))^2+f'(x)f'''(x)=0 4n,4n-1 4n-2 -\sqrt{a^2+b^2}+c\leq a\cos(\theta)+b\sin(\theta)+c\leq \sqrt{a^2+b^2}+c \sin(2\theta) \sin(\theta) \theta 1 f(\theta) f'(x)=0 2n f''(x) 2n-1 f'(x)=\sum_{r=0}^nra_r\cos(r\theta)-rb_r\sin(r\theta) f''(x)=\sum_{r=0}^n-r^2a_r\sin(r\theta)-r^2b_r\cos(r\theta) f'''(x)=\sum_{r=0}^n -r^3a_r\cos(r\theta)+r^3b_r\sin(r\theta)","['real-analysis', 'ordinary-differential-equations', 'derivatives', 'trigonometry']"
16,Determine and classify stationary points of the function $f (x) = x(1− \ln x)$,Determine and classify stationary points of the function,f (x) = x(1− \ln x),I'm stuck on what to do after finding the derivative. So far I have done: $$f(x)=x(1-\ln x)$$ $$f'(x)=1(1-\ln x)\frac1x\tag{Applying the chain rule}$$ $$f'(x)=\frac1x(1-\ln x)$$ (as the original question differentiated) From there where do i proceed and determine and classify all the stationary points?,I'm stuck on what to do after finding the derivative. So far I have done: (as the original question differentiated) From there where do i proceed and determine and classify all the stationary points?,f(x)=x(1-\ln x) f'(x)=1(1-\ln x)\frac1x\tag{Applying the chain rule} f'(x)=\frac1x(1-\ln x),['calculus']
17,Evaluate $a$ and $b$ s.t. the piecewise function is differentiable at two points,Evaluate  and  s.t. the piecewise function is differentiable at two points,a b,"Evaluate $a$ and $b$ s.t. the piecewise function is differentiable at $x=1$ and $x=2$ at the same time. \begin{equation*} f(x) = \left\{         \begin{array}{ll}             ax^2+bx+1 & \quad x ≥ 1 \\             -1 & \quad x < 1         \end{array}     \right. \end{equation*} So first, we gotta prove it's continuous at these points. $$f(1)=a+b+1$$ $$\lim_{x \to 1^-} -1=-1$$ $$\lim_{x \to 1^+} ax^2+bx+1=a+b+1$$ $-1=a+b+1 ⟹ a+b=-2$ How do I continue this for $x=2$ ? And then for differentbility? Im kinda new to this and need a little help.","Evaluate and s.t. the piecewise function is differentiable at and at the same time. So first, we gotta prove it's continuous at these points. How do I continue this for ? And then for differentbility? Im kinda new to this and need a little help.","a b x=1 x=2 \begin{equation*}
f(x) = \left\{
        \begin{array}{ll}
            ax^2+bx+1 & \quad x ≥ 1 \\
            -1 & \quad x < 1
        \end{array}
    \right.
\end{equation*} f(1)=a+b+1 \lim_{x \to 1^-} -1=-1 \lim_{x \to 1^+} ax^2+bx+1=a+b+1 -1=a+b+1 ⟹ a+b=-2 x=2","['calculus', 'derivatives', 'continuity']"
18,How do I propagate errors back to the previous layers for convolutional neural network?,How do I propagate errors back to the previous layers for convolutional neural network?,,"I'm trying to figure out the backward propagation for convolutional neural network. I drew the following figure to illustrate the forward propagation of a 3-layer model. Someone may argue the flatten should be counted as another layer, well, that would make me write a few more lines of derivation without giving any good. To simplify the whole process, I decided to consider binary classification and just one training example which is also an input image, so the output of the 1st layer would be $a^{[1]} = ReLU (x * w^{[1]} + b^{[1]}) \tag{1}$ where the $*$ denotes the convolution operation, $w^{[1]}$ denotes the kernel/filter used in this convolution operation, $b^{[1]}$ denotes the bias/intercept for layer [1] . Given the input image $x$ is size of (8, 8), the filter size of (3, 3), so the output of layer [1] $a^{[1]}$ is size of (6, 6). $m$ denotes the number of training examples, so $m=1$ here, and the last dimension of (m, 8, 8, 1) means the color channel which is also 1 here. There are 10 learnable params in layer [1] , 9 for the kernel, 1 for the bias and the activation function is ReLU in this model. There is no learnable params or activation function in layer [2] though, I still use $a$ to denote the output of this layer. So $a^{[2]}$ is size of (3, 3), as the kernel size = (2, 2). $a^{[3]}$ denotes the output of the last layer of the model and could be computed using this formula $a^{[3]} = \sigma{(z^{[3]})} \tag{2}$ where $\sigma{(\cdot)}$ denotes the sigmoid function and $z^{[3]} = a^{[2]} \cdot w^{[3]} + b^{[3]} \tag{3}$ so, there are another 10 params to learn, 9 for $w^{[3]}$ and 1 for $b^{[3]}$ . the loss function could be $$ \mathcal{L}(a^{[3]}, y) =  - [y \log a^{[3]} + (1-y)  \log(1-a^{[3]})] \tag{4}$$ I clearly understand part of the backward propagation for this model, which is $$ \dfrac{d\mathcal{L}(a^{[3]}, y)}{da^{[3]}} = -\dfrac{y}{a^{[3]}} + \dfrac{1-y}{1-a^{[3]}} \tag{5} $$ $$ \dfrac{da^{[3]}}{dz^{[3]}} = a^{[3]}\,(1-a^{[3]}) \tag{6} $$ \begin{align} \frac{d\mathcal{L}(a^{[3]}, y)}{dz^{[3]}} = a^{[3]} - y \tag{7} \end{align} \begin{align} \frac{d\mathcal{L}(a^{[3]}, y)}{da^{[2]}} = \frac{d\mathcal{L}(a^{[3]}, y)}{dz^{[3]}} \cdot w^{[3]T} \tag{8} \end{align} the part of equation (3) could be rewritten as $ a^{[2]} \cdot w^{[3]}  = a^{[2]}_{1,1} \ w^{[3]}_{1} + a^{[2]}_{1,2} \ w^{[3]}_{2}+ a^{[2]}_{1,3} \ w^{[3]}_{3} \\ + a^{[2]}_{2,1} \ w^{[3]}_{4} + a^{[2]}_{2,2} \ w^{[3]}_5+ a^{[2]}_{2,3} \ w^{[3]}_{6} \\ + a^{[2]}_{3,1} \ w^{[3]}_{7} + a^{[2]}_{3,2} \ w^{[3]}_{8}+ a^{[2]}_{3,3} \ w^{[3]}_{9} \tag{9} $ where $ a^{[2]}_{1,1} = \max(a^{[1]}_{1,1}, a^{[1]}_{1,2}, a^{[1]}_{2,1}, a^{[1]}_{2,2}) \tag{10} $ $ a^{[2]}_{p,r} = \max(a^{[1]}_{2p-1,2r-1}, a^{[1]}_{2p-1,2r}, a^{[1]}_{2p,2r-1}, a^{[1]}_{2p,2r}) \tag{11} $ How do I propagate errors back to layer [2] and [1] ? I'm now blocked at the step of propagating errors from $a^{[2]}$ to $a^{[1]}$ . If $a^{[1]}_{1,1}$ is the output of equation (10), is it correct to use the formula below to compute $ \dfrac{d\mathcal{L}(a^{[3]}, y)}{da^{[1]}} $ ? $ \dfrac{\partial \mathcal{L}(a^{[3]}, y)}{\partial a^{[1]}_{1,1}} =  \dfrac{\partial \mathcal{L}(a^{[3]}, y)}{\partial a^{[2]}_{1,1}} = \dfrac{\partial \mathcal{L}(a^{[3]}, y)}{\partial z^{[3]}} \ w^{[3]}_1 = (a^{[3]} - y)\ w^{[3]}_1 $ If yes, what about the other 3 guys? I went through a post for this though, I still don't know what should I do to complete the formulas after (8). Could someone give me a hint? Note: I know what the convolution operation and cross-correlation are in deep learning. I'd just like to know the backward propagation part.","I'm trying to figure out the backward propagation for convolutional neural network. I drew the following figure to illustrate the forward propagation of a 3-layer model. Someone may argue the flatten should be counted as another layer, well, that would make me write a few more lines of derivation without giving any good. To simplify the whole process, I decided to consider binary classification and just one training example which is also an input image, so the output of the 1st layer would be where the denotes the convolution operation, denotes the kernel/filter used in this convolution operation, denotes the bias/intercept for layer [1] . Given the input image is size of (8, 8), the filter size of (3, 3), so the output of layer [1] is size of (6, 6). denotes the number of training examples, so here, and the last dimension of (m, 8, 8, 1) means the color channel which is also 1 here. There are 10 learnable params in layer [1] , 9 for the kernel, 1 for the bias and the activation function is ReLU in this model. There is no learnable params or activation function in layer [2] though, I still use to denote the output of this layer. So is size of (3, 3), as the kernel size = (2, 2). denotes the output of the last layer of the model and could be computed using this formula where denotes the sigmoid function and so, there are another 10 params to learn, 9 for and 1 for . the loss function could be I clearly understand part of the backward propagation for this model, which is the part of equation (3) could be rewritten as where How do I propagate errors back to layer [2] and [1] ? I'm now blocked at the step of propagating errors from to . If is the output of equation (10), is it correct to use the formula below to compute ? If yes, what about the other 3 guys? I went through a post for this though, I still don't know what should I do to complete the formulas after (8). Could someone give me a hint? Note: I know what the convolution operation and cross-correlation are in deep learning. I'd just like to know the backward propagation part.","a^{[1]} = ReLU (x * w^{[1]} + b^{[1]}) \tag{1} * w^{[1]} b^{[1]} x a^{[1]} m m=1 a a^{[2]} a^{[3]} a^{[3]} = \sigma{(z^{[3]})} \tag{2} \sigma{(\cdot)} z^{[3]} = a^{[2]} \cdot w^{[3]} + b^{[3]} \tag{3} w^{[3]} b^{[3]}  \mathcal{L}(a^{[3]}, y) =  - [y \log a^{[3]} + (1-y)  \log(1-a^{[3]})] \tag{4} 
\dfrac{d\mathcal{L}(a^{[3]}, y)}{da^{[3]}} = -\dfrac{y}{a^{[3]}} + \dfrac{1-y}{1-a^{[3]}} \tag{5}
 
\dfrac{da^{[3]}}{dz^{[3]}} = a^{[3]}\,(1-a^{[3]}) \tag{6}
 \begin{align}
\frac{d\mathcal{L}(a^{[3]}, y)}{dz^{[3]}}
= a^{[3]} - y \tag{7}
\end{align} \begin{align}
\frac{d\mathcal{L}(a^{[3]}, y)}{da^{[2]}}
= \frac{d\mathcal{L}(a^{[3]}, y)}{dz^{[3]}} \cdot w^{[3]T}
\tag{8}
\end{align} 
a^{[2]} \cdot w^{[3]} 
= a^{[2]}_{1,1} \ w^{[3]}_{1} + a^{[2]}_{1,2} \ w^{[3]}_{2}+ a^{[2]}_{1,3} \ w^{[3]}_{3} \\
+ a^{[2]}_{2,1} \ w^{[3]}_{4} + a^{[2]}_{2,2} \ w^{[3]}_5+ a^{[2]}_{2,3} \ w^{[3]}_{6} \\
+ a^{[2]}_{3,1} \ w^{[3]}_{7} + a^{[2]}_{3,2} \ w^{[3]}_{8}+ a^{[2]}_{3,3} \ w^{[3]}_{9} \tag{9}
 
a^{[2]}_{1,1} = \max(a^{[1]}_{1,1}, a^{[1]}_{1,2}, a^{[1]}_{2,1}, a^{[1]}_{2,2}) \tag{10}
 
a^{[2]}_{p,r} = \max(a^{[1]}_{2p-1,2r-1}, a^{[1]}_{2p-1,2r}, a^{[1]}_{2p,2r-1}, a^{[1]}_{2p,2r}) \tag{11}
 a^{[2]} a^{[1]} a^{[1]}_{1,1} 
\dfrac{d\mathcal{L}(a^{[3]}, y)}{da^{[1]}}
 
\dfrac{\partial \mathcal{L}(a^{[3]}, y)}{\partial a^{[1]}_{1,1}} = 
\dfrac{\partial \mathcal{L}(a^{[3]}, y)}{\partial a^{[2]}_{1,1}} =
\dfrac{\partial \mathcal{L}(a^{[3]}, y)}{\partial z^{[3]}} \ w^{[3]}_1 = (a^{[3]} - y)\ w^{[3]}_1
","['calculus', 'derivatives', 'machine-learning', 'convolution']"
19,Rate of Change of Volume with respect to Surface Area [duplicate],Rate of Change of Volume with respect to Surface Area [duplicate],,"This question already has answers here : Why is the derivative of a circle's area its perimeter (and similarly for spheres)? (8 answers) Closed 2 years ago . This question explains why the derivative of the volume of a sphere is equal to its surface area. This is based on the logic using the differential. However, to what extent (and when can this be generalized). For example A. The derivative of the volume of a cube with side s does not equal its surface area. B. The derivative of the volume of a cube with side 2r does equal its surface area. What is going on here under the hood? Can anyone explain without using Maths beyond Calc I/II. Also, I am not so interested in understanding why the derivative of the area is equal to the circumfence, as in understanding which shapes in general exhibit this property and why. Note: This paper explains this question, but is quite technical. Summarizing the main results in less technical language would be useful.","This question already has answers here : Why is the derivative of a circle's area its perimeter (and similarly for spheres)? (8 answers) Closed 2 years ago . This question explains why the derivative of the volume of a sphere is equal to its surface area. This is based on the logic using the differential. However, to what extent (and when can this be generalized). For example A. The derivative of the volume of a cube with side s does not equal its surface area. B. The derivative of the volume of a cube with side 2r does equal its surface area. What is going on here under the hood? Can anyone explain without using Maths beyond Calc I/II. Also, I am not so interested in understanding why the derivative of the area is equal to the circumfence, as in understanding which shapes in general exhibit this property and why. Note: This paper explains this question, but is quite technical. Summarizing the main results in less technical language would be useful.",,"['calculus', 'geometry', 'derivatives', 'volume', 'related-rates']"
20,How to use Leibniz integral rule If f goes to infinity?,How to use Leibniz integral rule If f goes to infinity?,,"If $F(t)$ is defined as follows: $$ F(t) = \int_0^t \frac{y(\tau)}{\sqrt{t-\tau}}d\tau $$ I want to calculate $F'(t)$ using Libniz integral rule. Use $f(t,\tau)$ to denote the part in the integration: $$ f(t,\tau) = \frac{y(\tau)}{\sqrt{t-\tau}} $$ I know that $y(t)$ is a finite value, so $f(t,t)$ is $+\infty$ , use following formula ( https://en.wikipedia.org/wiki/Leibniz_integral_rule ), we would have the result equals to $+\infty - \infty$ , which is not what I want. $$ {\frac {d}{dx}}\left(\int _{0}^{x}f(x,t)dt\right)=f{\big (}x,x{\big )}+\int _{0}^{x}{\frac {\partial }{\partial x}}f(x,t)dt $$ it looks like in this case Leibniz integral rule is not applicable. Is there a way to find $F'(t)$ ?","If is defined as follows: I want to calculate using Libniz integral rule. Use to denote the part in the integration: I know that is a finite value, so is , use following formula ( https://en.wikipedia.org/wiki/Leibniz_integral_rule ), we would have the result equals to , which is not what I want. it looks like in this case Leibniz integral rule is not applicable. Is there a way to find ?","F(t) 
F(t) = \int_0^t \frac{y(\tau)}{\sqrt{t-\tau}}d\tau
 F'(t) f(t,\tau) 
f(t,\tau) = \frac{y(\tau)}{\sqrt{t-\tau}}
 y(t) f(t,t) +\infty +\infty - \infty 
{\frac {d}{dx}}\left(\int _{0}^{x}f(x,t)dt\right)=f{\big (}x,x{\big )}+\int _{0}^{x}{\frac {\partial }{\partial x}}f(x,t)dt
 F'(t)","['calculus', 'integration', 'derivatives']"
21,Sum function of a power series derivative,Sum function of a power series derivative,,"If $$f(x)=\sum_{n=1}^\infty \frac{2^n}{n} x^n\space\space\space x\in\left]-\rho,\rho\right[$$ how is $$f'(x)=\frac{2}{1-2x}$$ I've found that $$\vert\rho\vert=\frac{1}{2}$$ and I know that $$\sum_{n=0}^\infty{x^n}=\frac{1}{1-x}\space\space\space x\in\left]-1,1\right[$$ (think I need to use that).",If how is I've found that and I know that (think I need to use that).,"f(x)=\sum_{n=1}^\infty \frac{2^n}{n} x^n\space\space\space x\in\left]-\rho,\rho\right[ f'(x)=\frac{2}{1-2x} \vert\rho\vert=\frac{1}{2} \sum_{n=0}^\infty{x^n}=\frac{1}{1-x}\space\space\space x\in\left]-1,1\right[","['derivatives', 'power-series']"
22,Exchange of the one-sided derivative and the one-sided limit of a real variable,Exchange of the one-sided derivative and the one-sided limit of a real variable,,"Let $f$ be a real-valued continuous function on $[0, 1)$ . Suppose that $f$ is infinitely-differentiable on $(0 , 1)$ . Is there some sufficient condition for $f$ to be right-differentiable at $0$ with $$f_{+}'(0) = \lim_{x \to 0^{+}} f'(x),$$ where I use the notation $f_{+}'(0) := \lim_{\Delta x \to 0^+} \frac{f(\Delta x) - f(0)}{\Delta x}$ for the right derivative of $f$ at $0$ ?",Let be a real-valued continuous function on . Suppose that is infinitely-differentiable on . Is there some sufficient condition for to be right-differentiable at with where I use the notation for the right derivative of at ?,"f [0, 1) f (0 , 1) f 0 f_{+}'(0) = \lim_{x \to 0^{+}} f'(x), f_{+}'(0) := \lim_{\Delta x \to 0^+} \frac{f(\Delta x) - f(0)}{\Delta x} f 0",['limits']
23,"For all $x\not =0 \in\mathbb{R}^m$ and $h\in\mathbb{R}^m$, who is $\xi'(x)\cdot h$?","For all  and , who is ?",x\not =0 \in\mathbb{R}^m h\in\mathbb{R}^m \xi'(x)\cdot h,"If $f,g:\mathbb{R}^m\rightarrow\mathbb{R}^n$ are differentiable with $f\in C^1$ and $\xi:\mathbb{R}^m\rightarrow \mathbb{R}$ where $$\xi(x)=\langle g(x),\int_0^{|x|}f(tx)dt\rangle,$$ with $|x|=\sqrt{\langle x,x\rangle}$ , then for all $x\not =0 \in\mathbb{R}^m$ and $h\in\mathbb{R}^m$ , who is $\xi'(x)\cdot h$ ? I don't know how to proceed with the second term. What I've done is: $\forall x\not =0 \in\mathbb{R}^m$ and $h\in\mathbb{R}^m$ , $$\xi'(x)\cdot h=\lim_{w\to 0}{\frac{\xi(x+wh)-\xi(x)}{w}}=$$ $$=\lim_{w\to 0}\left\langle\frac{g(x+wh)-g(x)}{w},\frac{\int_{0}^{|x+wh|}f(tx+twh)dt-\int_{0}^{|x|}f(tx)dt}{w}\right\rangle=$$ $$=\left\langle g'(x)\cdot h,\lim_{w\to0}\left(\frac{\int_{0}^{|x+wh|}f(tx+twh)dt-\int_{0}^{|x|}f(tx)dt}{w}\right)\right\rangle$$ And after that, how could I establish that $$\lim_{h\to 0}\frac{r_\xi(h)}{|h|}=\lim_{h\to 0}\frac{\xi(x+h)-\xi(x)-\xi'(x)\cdot h}{|h|}=0?$$","If are differentiable with and where with , then for all and , who is ? I don't know how to proceed with the second term. What I've done is: and , And after that, how could I establish that","f,g:\mathbb{R}^m\rightarrow\mathbb{R}^n f\in C^1 \xi:\mathbb{R}^m\rightarrow \mathbb{R} \xi(x)=\langle g(x),\int_0^{|x|}f(tx)dt\rangle, |x|=\sqrt{\langle x,x\rangle} x\not =0 \in\mathbb{R}^m h\in\mathbb{R}^m \xi'(x)\cdot h \forall x\not =0 \in\mathbb{R}^m h\in\mathbb{R}^m \xi'(x)\cdot h=\lim_{w\to 0}{\frac{\xi(x+wh)-\xi(x)}{w}}= =\lim_{w\to 0}\left\langle\frac{g(x+wh)-g(x)}{w},\frac{\int_{0}^{|x+wh|}f(tx+twh)dt-\int_{0}^{|x|}f(tx)dt}{w}\right\rangle= =\left\langle g'(x)\cdot h,\lim_{w\to0}\left(\frac{\int_{0}^{|x+wh|}f(tx+twh)dt-\int_{0}^{|x|}f(tx)dt}{w}\right)\right\rangle \lim_{h\to 0}\frac{r_\xi(h)}{|h|}=\lim_{h\to 0}\frac{\xi(x+h)-\xi(x)-\xi'(x)\cdot h}{|h|}=0?","['integration', 'analysis', 'derivatives', 'inner-products']"
24,"Find the directional derivative of $f_{\vec u}(5,2)$ for the function $xy+y^3$.",Find the directional derivative of  for the function .,"f_{\vec u}(5,2) xy+y^3","Find the directional derivative of $f_{\vec u}(5,2)$ for the function $f(x,y)=xy+y^3$ with $\frac{(3i-4j)}{5}$ ? I got $-\frac{17}{5}$ (as the answer) from the derived formula below. $  \left(2\right)\left(\frac{3}{5}\right)+\left(5\right)+3\left(2\right)^2\left(-\frac{4}{5}\right)$ Incase the above is not readable here is the problem on symbolab. It is in the form $ai+bj$ where $a=y$ and $b=x+3y^2$ , as can be seen in the Symbolab, I already plugged in the $x$ and $y$ values which is how I got to my final answer of $-\frac{17}{5}$ . $a$ and $b$ were found by taking the partial derivative of $x$ and $y$ via Symbolab. Obviously my answer was wrong, I am not sure where I went wrong.","Find the directional derivative of for the function with ? I got (as the answer) from the derived formula below. Incase the above is not readable here is the problem on symbolab. It is in the form where and , as can be seen in the Symbolab, I already plugged in the and values which is how I got to my final answer of . and were found by taking the partial derivative of and via Symbolab. Obviously my answer was wrong, I am not sure where I went wrong.","f_{\vec u}(5,2) f(x,y)=xy+y^3 \frac{(3i-4j)}{5} -\frac{17}{5}   \left(2\right)\left(\frac{3}{5}\right)+\left(5\right)+3\left(2\right)^2\left(-\frac{4}{5}\right) ai+bj a=y b=x+3y^2 x y -\frac{17}{5} a b x y","['calculus', 'derivatives', 'vector-spaces', 'vectors']"
25,Integrate $I=\int_0^\infty \frac{e^{-ax}\sin bx}{x}\mathrm dx$ [duplicate],Integrate  [duplicate],I=\int_0^\infty \frac{e^{-ax}\sin bx}{x}\mathrm dx,"This question already has answers here : Evaluating $\int_0^\infty e^{-ax}\frac{\sin{(bx)}}{x}dx $ (2 answers) Show that $\int_0^\infty e^{-x}\cos x \text{dx}=\int_0^\infty e^{-x} \sin x \text{dx}$ (5 answers) Closed 2 years ago . If $a > 0$ then show $$\int_0^\infty \frac{e^{-ax}\sin bx}{x} \mathrm dx=\tan^{-1}\frac{b}{a},$$ hence also show that $$(i) \quad \int_0^\infty \frac{\sin bx}{x}\mathrm dx=\frac{\pi}{2} \text{ when } b>0,$$ $$(ii) \quad\int_0^\infty \frac{\sin bx}{x}\mathrm dx=-\frac{\pi}{2} \text{ when } b<0.$$ $$I=\int_0^\infty \frac{e^{-ax}\sin bx}{x}\mathrm dx .$$ $$\frac{dI}{db}=\int_0^\infty \frac{e^{-ax}}{x} \frac{\partial}{\partial b} (\sin bx)\mathrm dx   =\int_0^\infty \frac{e^{-ax}}{x}x\cos bx\mathrm dx   =\frac{a}{a^2+b^2}.$$ But, how is $$e \int_0^\infty \frac{e^{-ax}}{x}x\cos bx\mathrm dx = \frac{a}{a^2 +b^2}.$$ I was solving the problem by differentiating inside the integral sign. There's an $x$ in denominator. So, It's not my answer . Note : I am doing differentiation under integral sign. Question: Show that $\int_0^\infty e^{-ax}\cos bx\mathrm dx=\frac{a}{a^2 +b^2}$ (recent linked question didn't prove it. They were just telling to prove what I will get if I integrate by parts. And, another one prove my question is equal to 1/2. Thats why I am rejecting them. There's no possible answer which matches with mine.)","This question already has answers here : Evaluating $\int_0^\infty e^{-ax}\frac{\sin{(bx)}}{x}dx $ (2 answers) Show that $\int_0^\infty e^{-x}\cos x \text{dx}=\int_0^\infty e^{-x} \sin x \text{dx}$ (5 answers) Closed 2 years ago . If then show hence also show that But, how is I was solving the problem by differentiating inside the integral sign. There's an in denominator. So, It's not my answer . Note : I am doing differentiation under integral sign. Question: Show that (recent linked question didn't prove it. They were just telling to prove what I will get if I integrate by parts. And, another one prove my question is equal to 1/2. Thats why I am rejecting them. There's no possible answer which matches with mine.)","a > 0 \int_0^\infty \frac{e^{-ax}\sin bx}{x} \mathrm dx=\tan^{-1}\frac{b}{a}, (i) \quad \int_0^\infty \frac{\sin bx}{x}\mathrm dx=\frac{\pi}{2} \text{ when } b>0, (ii) \quad\int_0^\infty \frac{\sin bx}{x}\mathrm dx=-\frac{\pi}{2} \text{ when } b<0. I=\int_0^\infty \frac{e^{-ax}\sin bx}{x}\mathrm dx . \frac{dI}{db}=\int_0^\infty \frac{e^{-ax}}{x} \frac{\partial}{\partial b} (\sin bx)\mathrm dx 
 =\int_0^\infty \frac{e^{-ax}}{x}x\cos bx\mathrm dx 
 =\frac{a}{a^2+b^2}. e \int_0^\infty \frac{e^{-ax}}{x}x\cos bx\mathrm dx = \frac{a}{a^2 +b^2}. x \int_0^\infty e^{-ax}\cos bx\mathrm dx=\frac{a}{a^2 +b^2}","['calculus', 'integration']"
26,Hypothesis for differentiating under the integral,Hypothesis for differentiating under the integral,,"I've been trying to understand when it is allowed to move a derivative inside the integral, especifically the requirements on the derivative of the function being integrated. $$\frac{d}{dx}\int_a^bf(x,y)dy=\int_a^b\frac{\partial}{\partial x}f(x,y)dy$$ In some sources I find that it just requires that $\frac{\partial}{\partial x}f(x,y)$ be continuous in some neighbourhood while in others it is also required that there exists a function $g(y)$ such that $|\frac{\partial}{\partial x}f(x,y)|<g(y)$ . Can someone please explain to me if/when the second hypothesisis not necessary?","I've been trying to understand when it is allowed to move a derivative inside the integral, especifically the requirements on the derivative of the function being integrated. In some sources I find that it just requires that be continuous in some neighbourhood while in others it is also required that there exists a function such that . Can someone please explain to me if/when the second hypothesisis not necessary?","\frac{d}{dx}\int_a^bf(x,y)dy=\int_a^b\frac{\partial}{\partial x}f(x,y)dy \frac{\partial}{\partial x}f(x,y) g(y) |\frac{\partial}{\partial x}f(x,y)|<g(y)","['calculus', 'integration', 'derivatives', 'proof-explanation']"
27,How to differentiate this matrix multiplication?,How to differentiate this matrix multiplication?,,"(Sorry for poor question I don't know how to write a math equation) $U_i$ for $i=1,2,...,d$ and $M, S$ are $C^{(N \times N)}$ matrices. And $\alpha_i$ for $i=1,2,...,d$ and $t$ are complex scalars. $H=\sum_{i=1}^d\alpha_iU_i$ and $Q=\exp(jtH)$ where $j$ is the imaginary unit - $j^2=-1$ . Let $f(\alpha_1,\alpha_2,...,\alpha_d)=\operatorname{tr}[MQSQ^H]$ Where $^H$ is the Hermitian transpose. Then how can we describe the partial derivative of $f$ with respect to $\alpha_i$ ?",(Sorry for poor question I don't know how to write a math equation) for and are matrices. And for and are complex scalars. and where is the imaginary unit - . Let Where is the Hermitian transpose. Then how can we describe the partial derivative of with respect to ?,"U_i i=1,2,...,d M, S C^{(N \times N)} \alpha_i i=1,2,...,d t H=\sum_{i=1}^d\alpha_iU_i Q=\exp(jtH) j j^2=-1 f(\alpha_1,\alpha_2,...,\alpha_d)=\operatorname{tr}[MQSQ^H] ^H f \alpha_i","['calculus', 'matrices', 'derivatives', 'matrix-calculus', 'differential']"
28,Is there a common shorthand for the $n^{\text{th}}$ antiderivative of a function?,Is there a common shorthand for the  antiderivative of a function?,n^{\text{th}},The antiderivative of a function $f(x)$ is commonly written as $F(x)$ . $n^{\text{th}}$ derivative is commonly written as $f^{(n)}(x)$ . Is there something for the $n^{\text{th}}$ antiderivative?,The antiderivative of a function is commonly written as . derivative is commonly written as . Is there something for the antiderivative?,f(x) F(x) n^{\text{th}} f^{(n)}(x) n^{\text{th}},"['integration', 'derivatives', 'notation', 'soft-question']"
29,"Prove or disprove: $1+\frac{\ln^c(ax)-\ln^ca}{\ln^c(2a)-\ln^ca}-x\leq 0$, where $1<x$, $a>2$, and $c$ is chosen so that $f'(2)=0$","Prove or disprove: , where , , and  is chosen so that",1+\frac{\ln^c(ax)-\ln^ca}{\ln^c(2a)-\ln^ca}-x\leq 0 1<x a>2 c f'(2)=0,Define: $$f(x)=1+\frac{\ln^{c}\left(ax\right)-\ln^{c}\left(a\right)}{\ln^{c}\left(2a\right)-\ln^{c}\left(a\right)}-x$$ where $1< x$ and $a>2$ . Assume further that the parameter $c$ is chosen so that $f'(2)=0$ . The derivation of $c$ involves the Lambert's function. Claim : $$f(x)\leq 0$$ My attempt: We have : $$f'(x)=\frac{c(\ln(ax))^{c-1}}{x\left(\ln^{c}\left(2a\right)-\ln^{c}\left(a\right)\right)}-1$$ We substitute $x=\frac{1}{y^{c-1}a}$ The inequality have the form : $$\ln(u)u=p$$ Wich is just the Lambert's function .See the solution in this link . I cannot proceed further . How to (dis)prove the first inequality ? Thanks in advance,Define: where and . Assume further that the parameter is chosen so that . The derivation of involves the Lambert's function. Claim : My attempt: We have : We substitute The inequality have the form : Wich is just the Lambert's function .See the solution in this link . I cannot proceed further . How to (dis)prove the first inequality ? Thanks in advance,f(x)=1+\frac{\ln^{c}\left(ax\right)-\ln^{c}\left(a\right)}{\ln^{c}\left(2a\right)-\ln^{c}\left(a\right)}-x 1< x a>2 c f'(2)=0 c f(x)\leq 0 f'(x)=\frac{c(\ln(ax))^{c-1}}{x\left(\ln^{c}\left(2a\right)-\ln^{c}\left(a\right)\right)}-1 x=\frac{1}{y^{c-1}a} \ln(u)u=p,"['derivatives', 'inequality', 'logarithms']"
30,problem with implicit derivative using ln,problem with implicit derivative using ln,,"I have the following expression: $(xy)^{x^{2}}=(\tan y)^{xy^{3}}$ With $y$ being an implicit and differentiable function of $x$ . I want to find an expression for $y'$ . My first attempt is to use Ln function: $x^{2}\ln(xy)=xy^{3}\ln(\tan y)$ . But now I have two options: a) I use implicit differentiation (and other rules of differentiation) on the above equation. b) I rewrite the above expression as $x\ln(xy)=y^{3}\ln(\tan y)$ , and then use implicit differentiation . In my opinion, the two options should lead to the same final result. But for my surprise, this is not the case. For a: $ y'=\dfrac{y^{3}\ln(\tan y)-x-2x\ln(xy)}{\dfrac{x^{2}}{y}-3xy^{2}\ln(\tan y)-xy^{3}\dfrac{\sec^{2}y}{\tan y}}$ For b: $ y'=\dfrac{-1-2\ln(xy)}{\dfrac{x}{y}-3y^{2}\ln(\tan y)- y^{3}\dfrac{\sec^{2}y}{\tan y}}$ What am I doing wrong? Which option is correct?","I have the following expression: With being an implicit and differentiable function of . I want to find an expression for . My first attempt is to use Ln function: . But now I have two options: a) I use implicit differentiation (and other rules of differentiation) on the above equation. b) I rewrite the above expression as , and then use implicit differentiation . In my opinion, the two options should lead to the same final result. But for my surprise, this is not the case. For a: For b: What am I doing wrong? Which option is correct?",(xy)^{x^{2}}=(\tan y)^{xy^{3}} y x y' x^{2}\ln(xy)=xy^{3}\ln(\tan y) x\ln(xy)=y^{3}\ln(\tan y)  y'=\dfrac{y^{3}\ln(\tan y)-x-2x\ln(xy)}{\dfrac{x^{2}}{y}-3xy^{2}\ln(\tan y)-xy^{3}\dfrac{\sec^{2}y}{\tan y}}  y'=\dfrac{-1-2\ln(xy)}{\dfrac{x}{y}-3y^{2}\ln(\tan y)- y^{3}\dfrac{\sec^{2}y}{\tan y}},"['derivatives', 'implicit-differentiation']"
31,Show derivative function is bounded to prove uniform continuity,Show derivative function is bounded to prove uniform continuity,,"I've been trying to solve this question: Let $f(x) = \frac {x}{x+sin x}$ , prove that $f$ is uniformly continuous in $(0,\infty)$ I've been trying to prove that $f'$ is bounded in $(0,\infty)$ , but reached: $f'(x)= \frac {sin x - xcosx}{(x+sin x)^2}$ I'm not sure how to go on from here to show that $f'$ is truly bounded. Would appreciate any help! Thanks!","I've been trying to solve this question: Let , prove that is uniformly continuous in I've been trying to prove that is bounded in , but reached: I'm not sure how to go on from here to show that is truly bounded. Would appreciate any help! Thanks!","f(x) = \frac {x}{x+sin x} f (0,\infty) f' (0,\infty) f'(x)= \frac {sin x - xcosx}{(x+sin x)^2} f'","['calculus', 'limits', 'derivatives', 'continuity', 'uniform-continuity']"
32,Variational Problem has no minimizer,Variational Problem has no minimizer,,"I am given the problem $$ \min_{u \in H_0^1([0, 1])} \int^1_0 xu'(x)^2~\mathrm{d}x $$ and supposed to prove that the problem has no minimizer. But the integrand is bounded below by zero, and thus so is the functional that we want to minimize. The zero function is admissible and therefore $0$ is a minimizer. Where is my mistake? Or is there a problem with the task?","I am given the problem and supposed to prove that the problem has no minimizer. But the integrand is bounded below by zero, and thus so is the functional that we want to minimize. The zero function is admissible and therefore is a minimizer. Where is my mistake? Or is there a problem with the task?","
\min_{u \in H_0^1([0, 1])} \int^1_0 xu'(x)^2~\mathrm{d}x
 0","['real-analysis', 'derivatives', 'optimization', 'sobolev-spaces', 'calculus-of-variations']"
33,"If $x=ye^y$, find $\frac{dx}{dy}$, and use it to find $\frac{dy}{dx}$","If , find , and use it to find",x=ye^y \frac{dx}{dy} \frac{dy}{dx},"If $x=ye^y$ , then explicit differentiation to find $\frac{dx}{dy}$ , and the implicit differentiation to find $\frac{dy}{dx}$ yield consistent results. Explain if the above line is True or False. (Ans: True) But why? When we can clearly see using wolframalpha , explicit differentiation is $\dfrac{dx}{dy} = e^y \times (1 + y)$ . Implicit differentiation using wolframalpha , is $y'(x) = \dfrac{dy}{dx} = \dfrac{1}{e^y (y+1)}$ They seem clearly different, and not consistent result because they are reciprocal(?) of each other?","If , then explicit differentiation to find , and the implicit differentiation to find yield consistent results. Explain if the above line is True or False. (Ans: True) But why? When we can clearly see using wolframalpha , explicit differentiation is . Implicit differentiation using wolframalpha , is They seem clearly different, and not consistent result because they are reciprocal(?) of each other?",x=ye^y \frac{dx}{dy} \frac{dy}{dx} \dfrac{dx}{dy} = e^y \times (1 + y) y'(x) = \dfrac{dy}{dx} = \dfrac{1}{e^y (y+1)},"['calculus', 'algebra-precalculus', 'derivatives', 'implicit-differentiation']"
34,Intermediate problem using Chain Rule,Intermediate problem using Chain Rule,,"If $y=\frac{d}{dx} [\sin \sqrt{1+\cos (x)}]$ than, differentiate $x$ . $$\frac{d}{dx} [\sin \sqrt{1+\cos (x)}]$$ $$=\frac{d}{dx} [\sin (1+\cos (x))^{\frac{1}{2}}]$$ $$=\frac{1}{2} \cos (1+\cos (x))^{-\frac{1}{2}} (-\sin x)$$ $$=-\frac{\sin (x)}{2\cos (1+\cos (x))^\frac{1}{2}}$$ I found that the answer is wrong. I found the answer which was solved using $$\frac{dx}{dy}=\frac{dx}{du} \frac{du}{dy} $$ But, I want to figure it out using I was trying to solve above question as I did for this $$\frac{d}{dx}[\tan(x^2+1)]=\sec^2 (x^2+1) \cdot 2x$$","If than, differentiate . I found that the answer is wrong. I found the answer which was solved using But, I want to figure it out using I was trying to solve above question as I did for this",y=\frac{d}{dx} [\sin \sqrt{1+\cos (x)}] x \frac{d}{dx} [\sin \sqrt{1+\cos (x)}] =\frac{d}{dx} [\sin (1+\cos (x))^{\frac{1}{2}}] =\frac{1}{2} \cos (1+\cos (x))^{-\frac{1}{2}} (-\sin x) =-\frac{\sin (x)}{2\cos (1+\cos (x))^\frac{1}{2}} \frac{dx}{dy}=\frac{dx}{du} \frac{du}{dy}  \frac{d}{dx}[\tan(x^2+1)]=\sec^2 (x^2+1) \cdot 2x,['calculus']
35,Determine the extrema of this function,Determine the extrema of this function,,"Let $b_i \in \mathbb{R}$ , $f:\mathbb{R} \rightarrow \mathbb{R}$ $f(x) = \sum_{i=1}^{n} (x-b_i)^2$ So i have calculated the first derivative $f’(x) = 2 \sum_{i=1}^{n} (x-b_i)$ and the second $f”(x)=2$ , so the extrema would be a minimum? I have tried to calculate the extrema, $2 \sum_{i=1}^{n} (x-b_i)= 2(nx-\sum_{i=1}^{n} b_i)$ $nx= \sum_{i=1}^{n} b_i$ , so the local minimum would be at $x=\frac{\sum_{i=1}^{n} b_i}{n}$ . Is my approach correct?","Let , So i have calculated the first derivative and the second , so the extrema would be a minimum? I have tried to calculate the extrema, , so the local minimum would be at . Is my approach correct?",b_i \in \mathbb{R} f:\mathbb{R} \rightarrow \mathbb{R} f(x) = \sum_{i=1}^{n} (x-b_i)^2 f’(x) = 2 \sum_{i=1}^{n} (x-b_i) f”(x)=2 2 \sum_{i=1}^{n} (x-b_i)= 2(nx-\sum_{i=1}^{n} b_i) nx= \sum_{i=1}^{n} b_i x=\frac{\sum_{i=1}^{n} b_i}{n},"['real-analysis', 'calculus', 'derivatives', 'optimization']"
36,Definitive integral from derivative,Definitive integral from derivative,,"I have a simple differential equation: $$\frac{\partial p(x,y)}{\partial y} = -g$$ $y$ is defined from $-1$ to $\zeta$ , and I also have a boundary condition, which is: $$y=\zeta:p=sinh(x)$$ I am 100% sure that the problem is stated correctly, but it looks to me that these two conditions: $y=\zeta:p=sinh(x)$ $y$ is defined from $-1$ to $\zeta$ contradict each other, because using the first condition I am getting: $p=-gy + F(x)$ and $F(x)=sinh(x)+g \zeta$ , so the full solution would be $p(x,y)=-gy+sinh(x)+g\zeta$ . But by taking the definite integral from $-1$ to $\zeta$ I am getting $\int_{-1}^{\zeta} \frac{\partial p(x,y)}{\partial y} dy = p(x,y)=-g(\zeta+1)$ So, can someone tell me where is my mistake?","I have a simple differential equation: is defined from to , and I also have a boundary condition, which is: I am 100% sure that the problem is stated correctly, but it looks to me that these two conditions: is defined from to contradict each other, because using the first condition I am getting: and , so the full solution would be . But by taking the definite integral from to I am getting So, can someone tell me where is my mistake?","\frac{\partial p(x,y)}{\partial y} = -g y -1 \zeta y=\zeta:p=sinh(x) y=\zeta:p=sinh(x) y -1 \zeta p=-gy + F(x) F(x)=sinh(x)+g \zeta p(x,y)=-gy+sinh(x)+g\zeta -1 \zeta \int_{-1}^{\zeta} \frac{\partial p(x,y)}{\partial y} dy = p(x,y)=-g(\zeta+1)","['derivatives', 'definite-integrals']"
37,Can I view the Taylor polynomial as a generalization of the derivative?,Can I view the Taylor polynomial as a generalization of the derivative?,,"We know that we can regard a derivative as the ""best linear approximation"" of a function $f: (a,b) \to \mathbb{R}$ at a given point $x_0 \in (a,b)$ . I was wondering whether we can view the Taylor polynomial as ""the best polynomial approximation"". Another person has already asked a quite similar question , but I am not very satisfied with the answer because the norm that was defined in the first answer seems a bit arbitrary. I would suggest to define a Taylor polynomial in the following way: Let $f:(a,b) \to \mathbb{R}$ be a real-valued function. A $k$ -th order polynomial $p_k$ is a Taylor polynomial of order $k$ of $f$ at $x_0 \in (a,b)$ , if there exists a function $r_k: (a,b) \to \mathbb{R}$ such that $$ f(x) = p_k (x) + r_k (x) \quad \text{ with } \quad \lim\limits_{x \to x_0} \frac{r_k(x)}{(x-x_0)^k} = 0.$$ I already know that if $f$ is $k$ times differentiable at $x_0$ , then this definition coincides with the usual one. My question is: Is it necessary for $f$ to be differentiable $k$ times at $x_0$ ? If not, does it have major implications on things like uniqueness, etc?","We know that we can regard a derivative as the ""best linear approximation"" of a function at a given point . I was wondering whether we can view the Taylor polynomial as ""the best polynomial approximation"". Another person has already asked a quite similar question , but I am not very satisfied with the answer because the norm that was defined in the first answer seems a bit arbitrary. I would suggest to define a Taylor polynomial in the following way: Let be a real-valued function. A -th order polynomial is a Taylor polynomial of order of at , if there exists a function such that I already know that if is times differentiable at , then this definition coincides with the usual one. My question is: Is it necessary for to be differentiable times at ? If not, does it have major implications on things like uniqueness, etc?","f: (a,b) \to \mathbb{R} x_0 \in (a,b) f:(a,b) \to \mathbb{R} k p_k k f x_0 \in (a,b) r_k: (a,b) \to \mathbb{R}  f(x) = p_k (x) + r_k (x) \quad \text{ with } \quad \lim\limits_{x \to x_0} \frac{r_k(x)}{(x-x_0)^k} = 0. f k x_0 f k x_0","['real-analysis', 'calculus', 'derivatives', 'taylor-expansion']"
38,Differential equation for ethanol concentration in nth tank,Differential equation for ethanol concentration in nth tank,,"I have the following problem to solve. At first, I write the differential equation for the 0th tank as: I go ahead and solve it and get: Now, for the second tank, the differential equation becomes: Solving it gives me that: However, according to a later statement in the same excercise: This means that I'm missing a factor of 1/2 for the equation of x1, which we can see if we substitute n = 1 in the given equation. Can anyone help me where it goes wrong in my differential equations? Thank you!","I have the following problem to solve. At first, I write the differential equation for the 0th tank as: I go ahead and solve it and get: Now, for the second tank, the differential equation becomes: Solving it gives me that: However, according to a later statement in the same excercise: This means that I'm missing a factor of 1/2 for the equation of x1, which we can see if we substitute n = 1 in the given equation. Can anyone help me where it goes wrong in my differential equations? Thank you!",,"['ordinary-differential-equations', 'derivatives']"
39,Effects of variables' change on total change in a product function,Effects of variables' change on total change in a product function,,"Prelude My topic is related to existing posts found here and here ; although, my questions are different. My post explains my objective and what has and has not worked for me. I have met my objective and provided my solution. My questions pertain to why the solution works in the hopes that I may apply my one-off solution here more readily to other instances. Background I am comparing two numbers that are products of multiple variables. My objective is to determine how much of the total change is attributable to each variable's change. As a simple example, consider: x1 = a1 * b1 = 10 * 10 = 100  x2 = a2 * b2 = 11 * 10 = 110 How much of the total difference (110 - 100 = 10) is attributable to the change in a and b? My first approach was to use a binary (on/off) approach to determine what happens if each variables' change were the only change to occur. $$\Delta = \Delta a * b_1 + \Delta b * a_1$$ $$\Delta = (11 - 10) * 10 + (10 - 10) * 10$$ $$10 = 10 + 0$$ As shown, the change in ""a"" is responsible for 10 (100%) of the total change. However, as additional variables are added and/or change, this appears to stop working. Consider a more complicated example. Year A B C D Total (A * B * C * D) Y1 32 200 25 365 58,400,000 Y2 33 215 28 360 71,517,600 $\Delta$ 1 15 3 -5 13,117,600 If I use the previously described approach ( $\Delta = \Delta A * B_1 * C_1 * D_1 + ...$ ), the sum of the individual effects does not equal the total change. A B C D Total Change (A+B+C+D) 1,825,000 4,380,000 7,008,000 -800,000 12,413,000 As shown, if A were the only variable to change, it would increase the value by 1.8 million, and so on.  The sum total of isolated challenges in this method is approximately 12.4 million, which is less than the observed total of 13.1 million. My next attempt tried the reverse.  What would happen if each individual variable were the only variable not to change ( $\Delta = \Delta A * B_2 * C_2 * D_2 + ...$ )? A B C D Total Change (A+B+C+D) 2,167,200 4,989,600 7,662,600 -993,300 13,826,100 The sum total for this approach (approximately 13.8 million) is too great. The true answer exists somewhere between these two methods.  And through trial and error, I found a solution. Solution Let T be the total product for a given year, and $\Delta$ T be the total difference between T year 1 and T year 2 . Objective: Total change is equal to the change caused by each variable. $\Delta T = \Delta_A(T) + \Delta_B(T) + \Delta_C(T) + \Delta_D(T)$ Solution: $\Delta_A(T) = (A_2 - A_1) * B_1 * C_2 * D_1$ $\Delta_B(T) = (B_2 - B_1) * A_2 * C_2 * D_1$ $\Delta_C(T) = (C_2 - C_1) * A_1 * B_1 * D_1$ $\Delta_D(T) = (D_2 - D_1) * A_2 * B_2 * C_2$ Check: A B C D Total Change (A+B+C+D) 2,044,000 5,058,900 7,008,000 -993,300 13,117,600 As shown, the sum total of effects for the variables is equal to $\Delta$ T. Objective [presumably]  met. The Questions Notice in the solution that function differs for each variable. The effect of $\Delta$ A is $\Delta$ A * the year 1 value for B and D, but year 2 value for C. The effect of $\Delta$ B is $\Delta$ B * the year 1 value for D, but year 2 value for A and C. The effect of $\Delta$ C is $\Delta$ C * the year 1 value for A, B and D. The effect of $\Delta$ D is $\Delta$ D * the year 2 value for A, B and C. The functions are specific to the variable. Year 1 variables are applied to $\Delta$ C, and year 2 variables are applied to $\Delta$ D, these are not interchangeable functions. If year 2 variables are applied to $\Delta$ C, and year 1 variables are applied to $\Delta$ D, the sum total no longer equals the observed change ( $\Delta$ T). Q1: Why does this work? Why do we reference different years for different variables? Q2: Is there a way (other than trial and error) to determine the appropriate function (i.e., what would the function be for an added variable ""E""?) Q3: Does this solution actually isolate variables if they do not use a consistent approach? Is this really an accurate depiction of variables' effect on total change or just a coincidental / convenient distribution? Thanks, Andrew","Prelude My topic is related to existing posts found here and here ; although, my questions are different. My post explains my objective and what has and has not worked for me. I have met my objective and provided my solution. My questions pertain to why the solution works in the hopes that I may apply my one-off solution here more readily to other instances. Background I am comparing two numbers that are products of multiple variables. My objective is to determine how much of the total change is attributable to each variable's change. As a simple example, consider: x1 = a1 * b1 = 10 * 10 = 100  x2 = a2 * b2 = 11 * 10 = 110 How much of the total difference (110 - 100 = 10) is attributable to the change in a and b? My first approach was to use a binary (on/off) approach to determine what happens if each variables' change were the only change to occur. As shown, the change in ""a"" is responsible for 10 (100%) of the total change. However, as additional variables are added and/or change, this appears to stop working. Consider a more complicated example. Year A B C D Total (A * B * C * D) Y1 32 200 25 365 58,400,000 Y2 33 215 28 360 71,517,600 1 15 3 -5 13,117,600 If I use the previously described approach ( ), the sum of the individual effects does not equal the total change. A B C D Total Change (A+B+C+D) 1,825,000 4,380,000 7,008,000 -800,000 12,413,000 As shown, if A were the only variable to change, it would increase the value by 1.8 million, and so on.  The sum total of isolated challenges in this method is approximately 12.4 million, which is less than the observed total of 13.1 million. My next attempt tried the reverse.  What would happen if each individual variable were the only variable not to change ( )? A B C D Total Change (A+B+C+D) 2,167,200 4,989,600 7,662,600 -993,300 13,826,100 The sum total for this approach (approximately 13.8 million) is too great. The true answer exists somewhere between these two methods.  And through trial and error, I found a solution. Solution Let T be the total product for a given year, and T be the total difference between T year 1 and T year 2 . Objective: Total change is equal to the change caused by each variable. Solution: Check: A B C D Total Change (A+B+C+D) 2,044,000 5,058,900 7,008,000 -993,300 13,117,600 As shown, the sum total of effects for the variables is equal to T. Objective [presumably]  met. The Questions Notice in the solution that function differs for each variable. The effect of A is A * the year 1 value for B and D, but year 2 value for C. The effect of B is B * the year 1 value for D, but year 2 value for A and C. The effect of C is C * the year 1 value for A, B and D. The effect of D is D * the year 2 value for A, B and C. The functions are specific to the variable. Year 1 variables are applied to C, and year 2 variables are applied to D, these are not interchangeable functions. If year 2 variables are applied to C, and year 1 variables are applied to D, the sum total no longer equals the observed change ( T). Q1: Why does this work? Why do we reference different years for different variables? Q2: Is there a way (other than trial and error) to determine the appropriate function (i.e., what would the function be for an added variable ""E""?) Q3: Does this solution actually isolate variables if they do not use a consistent approach? Is this really an accurate depiction of variables' effect on total change or just a coincidental / convenient distribution? Thanks, Andrew",\Delta = \Delta a * b_1 + \Delta b * a_1 \Delta = (11 - 10) * 10 + (10 - 10) * 10 10 = 10 + 0 \Delta \Delta = \Delta A * B_1 * C_1 * D_1 + ... \Delta = \Delta A * B_2 * C_2 * D_2 + ... \Delta \Delta T = \Delta_A(T) + \Delta_B(T) + \Delta_C(T) + \Delta_D(T) \Delta_A(T) = (A_2 - A_1) * B_1 * C_2 * D_1 \Delta_B(T) = (B_2 - B_1) * A_2 * C_2 * D_1 \Delta_C(T) = (C_2 - C_1) * A_1 * B_1 * D_1 \Delta_D(T) = (D_2 - D_1) * A_2 * B_2 * C_2 \Delta \Delta \Delta \Delta \Delta \Delta \Delta \Delta \Delta \Delta \Delta \Delta \Delta \Delta,['derivatives']
40,Continuity and Differentiability at end point of an interval,Continuity and Differentiability at end point of an interval,,"Let $ f(x) = x(\sqrt {x} + \sqrt{x+1})  $ The problem had asked to check continuity and Differentiability at $ x = 0 $ First of all I noticed that the Natural Domain of the function is non negative Real Numbers i.e zero is the least point of interval Secondly I noted that $f(0)=0 $ And that the Right Hand Limit is also zero , implying that $f(x)$ is continuous at $0$ . My lecturer had told that to check Differentiability on the end point of an interval we need that the Right Hand Derivative must be finite . When I evaluate the  Right Hand Derivative it comes to be equal to $1$ (and hence finite, and therefore differentiable ) $$  \lim_{h\to 0} \frac {f(0+h) - f(0)}{h} (where  h>0) $$ Which evaluates to $$ \lim_{h\to 0} \frac {h(\sqrt {h} + \sqrt {1+h}) - 0}{h} , $$ the h cancels out and we are left with 1 . But the problem book says that the function is not differentiable at $x=0$ as it not defined from the Left side . I am confused on which is correct ?","Let The problem had asked to check continuity and Differentiability at First of all I noticed that the Natural Domain of the function is non negative Real Numbers i.e zero is the least point of interval Secondly I noted that And that the Right Hand Limit is also zero , implying that is continuous at . My lecturer had told that to check Differentiability on the end point of an interval we need that the Right Hand Derivative must be finite . When I evaluate the  Right Hand Derivative it comes to be equal to (and hence finite, and therefore differentiable ) Which evaluates to the h cancels out and we are left with 1 . But the problem book says that the function is not differentiable at as it not defined from the Left side . I am confused on which is correct ?"," f(x) = x(\sqrt {x} + \sqrt{x+1})    x = 0  f(0)=0  f(x) 0 1   \lim_{h\to 0} \frac {f(0+h) - f(0)}{h} (where  h>0)   \lim_{h\to 0} \frac {h(\sqrt {h} + \sqrt {1+h}) - 0}{h} ,  x=0",['calculus']
41,"f(x,y) is continuous if the partial derivatives of f(x,y) exist and one of them is bounded below?","f(x,y) is continuous if the partial derivatives of f(x,y) exist and one of them is bounded below?",,"I had a test and I couldn't solve this problem: f is continuous if the partial derivatives of f(x,y) exist and one of them is bounded below. I know that if f is differentiable then f must be continuous. But this isn't a necessary condition. How do you do that?","I had a test and I couldn't solve this problem: f is continuous if the partial derivatives of f(x,y) exist and one of them is bounded below. I know that if f is differentiable then f must be continuous. But this isn't a necessary condition. How do you do that?",,"['derivatives', 'continuity']"
42,Where did I go wrong with my differentiation here?,Where did I go wrong with my differentiation here?,,"This question involves the product rule and power of a function rule for differentiation. I often make silly algebra errors, but here I cannot find them. The answer is supposed to be y'=22. Thanks! When $x=-1$ , $y=(2x+1)^5(3x+2)^4$ . Hence, the derivative, $$y'=5(2x+1)^4 \cdot (2)(3x+2)^4 + (2x+1)^5 \cdot 4(3x+2)^3 \cdot 3$$ Plugging in $x=-1$ , $$y'= 5(2(-1)+1)^4 \cdot (2)(3(-1)+2)^4 + (2(-1)+1)^5 \cdot 4(3x+2)^3 \cdot 3\\ y' = 10-12\\ y' = -2$$","This question involves the product rule and power of a function rule for differentiation. I often make silly algebra errors, but here I cannot find them. The answer is supposed to be y'=22. Thanks! When , . Hence, the derivative, Plugging in ,","x=-1 y=(2x+1)^5(3x+2)^4 y'=5(2x+1)^4 \cdot (2)(3x+2)^4 + (2x+1)^5 \cdot 4(3x+2)^3 \cdot 3 x=-1 y'= 5(2(-1)+1)^4 \cdot (2)(3(-1)+2)^4 + (2(-1)+1)^5 \cdot 4(3x+2)^3 \cdot 3\\
y' = 10-12\\
y' = -2","['calculus', 'derivatives']"
43,Where can I find the proof of this theorem about the smoothness of the initial data?,Where can I find the proof of this theorem about the smoothness of the initial data?,,"What to follow is a reference from the text Differential forms by Victor Guillemin and Peter Haine So clearly the theorems $2.2.4$ and $2.2.5$ follow directely by the Cauchy theorem for ordinary differential equation provided that the vector field if locally lipschitz or of class $C^r$ for $r\ge 1$ since any function of calss $C^1$ is locally lipschitz. However how prove the last theorem? In particular I know that if the vector field $\vec v$ is of class $C^r$ for $r\ge 1$ then the solution of the correspondent system of differential equations is of class $C^r$ too but unfortunately this did not help me to prove the theorem. So could someone idicates where I can find the proof of the last theorem, please? I point out I did NOT study lebesgue integration and measure theory so that I ask courteously to not use them, thanks. MY PROOF ATTEMPT So by the regularity theorem we know that the function $\gamma_p$ is of class $C^\infty$ so that if $A_p$ is an open neighborhood of any $p\in V$ contained in $V$ then the set $J:=\gamma^{-1}_p[A_p]$ is open an open interval containing $a\in I$ and so if $\pi$ is the projection of $V\times I$ onto $I$ the statement follows proving that $$ h(q,t)=[\gamma_p\circ\pi](q,t) $$ for any $(q,t)\in A_p\times J$ because the set $A_p\times J$ is open and the function $\gamma_p\circ\pi$ is of class $C^\infty$ being a composition of such functions. So how prove the last equality? Is effectively it hold?","What to follow is a reference from the text Differential forms by Victor Guillemin and Peter Haine So clearly the theorems and follow directely by the Cauchy theorem for ordinary differential equation provided that the vector field if locally lipschitz or of class for since any function of calss is locally lipschitz. However how prove the last theorem? In particular I know that if the vector field is of class for then the solution of the correspondent system of differential equations is of class too but unfortunately this did not help me to prove the theorem. So could someone idicates where I can find the proof of the last theorem, please? I point out I did NOT study lebesgue integration and measure theory so that I ask courteously to not use them, thanks. MY PROOF ATTEMPT So by the regularity theorem we know that the function is of class so that if is an open neighborhood of any contained in then the set is open an open interval containing and so if is the projection of onto the statement follows proving that for any because the set is open and the function is of class being a composition of such functions. So how prove the last equality? Is effectively it hold?","2.2.4 2.2.5 C^r r\ge 1 C^1 \vec v C^r r\ge 1 C^r \gamma_p C^\infty A_p p\in V V J:=\gamma^{-1}_p[A_p] a\in I \pi V\times I I 
h(q,t)=[\gamma_p\circ\pi](q,t)
 (q,t)\in A_p\times J A_p\times J \gamma_p\circ\pi C^\infty","['real-analysis', 'calculus', 'integration', 'ordinary-differential-equations', 'derivatives']"
44,Related rates of change involving trig functions,Related rates of change involving trig functions,,"Question: A train is moving along a straight track at $75 \text{km h}^{-1}$ heading east. A camera is positioned $2$ km north of the train track and is focused on the train. a) Find the rate of change of the distance between the camera and the train when the train is $4$ km from the camera. b) Find the rate at which the camera is rotating when the train is 4km from the camera. Give your answer in degrees per second correct to the nearest tenth of a degree. This is a question that has caused my extension maths group a bit of confusion! We have solved part a of the question and we believe the answer is $65$ km/h (2sf). This is correct. Part b is the one we are unsure about. We have set up the problem using a couple of variables and applied various differentiation techniques. The answers we are getting, however, are varied and even our teacher has said that he needs a bit more time to work out which one is correct! I am quite eager to find out whether my technique is correct. The different answers we have got are: $2.6 \times 10^{-3}$ radians/second (then we can convert this to degrees per second as the question asks) $2.6$ degrees/second I appreciate that by putting this question out there, I may generate different responses, so I will wait to see if a several people start getting the same answer before accepting if it is correct. Best wishes!","Question: A train is moving along a straight track at heading east. A camera is positioned km north of the train track and is focused on the train. a) Find the rate of change of the distance between the camera and the train when the train is km from the camera. b) Find the rate at which the camera is rotating when the train is 4km from the camera. Give your answer in degrees per second correct to the nearest tenth of a degree. This is a question that has caused my extension maths group a bit of confusion! We have solved part a of the question and we believe the answer is km/h (2sf). This is correct. Part b is the one we are unsure about. We have set up the problem using a couple of variables and applied various differentiation techniques. The answers we are getting, however, are varied and even our teacher has said that he needs a bit more time to work out which one is correct! I am quite eager to find out whether my technique is correct. The different answers we have got are: radians/second (then we can convert this to degrees per second as the question asks) degrees/second I appreciate that by putting this question out there, I may generate different responses, so I will wait to see if a several people start getting the same answer before accepting if it is correct. Best wishes!",75 \text{km h}^{-1} 2 4 65 2.6 \times 10^{-3} 2.6,"['calculus', 'derivatives', 'trigonometry', 'implicit-differentiation', 'related-rates']"
45,Confusion over continuity on surfaces and L'Hospital's rule,Confusion over continuity on surfaces and L'Hospital's rule,,"I am really confused and was hoping someone could help clear up my understanding. Suppose I have two continuous differentiable surfaces $f,g:\mathbb{R}^N\rightarrow\mathbb{R}$ and there exists a point $x_0$ such that $f(x_0)=g(x_0)=0$ . Further $g(x)$ is non-zero everywhere else in the domain. I am interested in the surface $\frac{f}{g}$ , which must be continuous everywhere where $g$ is non-zero. Suppose I find a curve $C_1:[0,1]\rightarrow\mathbb{R}^N$ such that $x_0=C_1(0)$ and $x_1=C_1(1)$ for some $x_1 \in \mathbb{R}^N$ and I am able to show that: $$\lim_{y \rightarrow 0}\frac{f(C_1(y))}{g(C_1(y))}=k$$ The question which is confusing me is this: Does the existence of the above limit imply that if I were to find any curve function, say $C_2:[0,1]\rightarrow\mathbb{R}^N$ such that $x_0=C_2(0)$ and $x_2=C_2(1)$ for some $x_2 \in \mathbb{R}^N$ , it would be true that: $$\lim_{y \rightarrow 0}\frac{f(C_2(y))}{g(C_2(y))}=k$$ i.e. is the limit now the same along all curves? Or in other words, if I defined $h(x)=\frac{f(x)}{g(x)}$ for all $x\neq x_0$ and $h(x_0)=k$ would $h$ be continuous? My intuition for why the answer to the above should be ""yes, $h$ would be continuous"" is this: $x_0$ should be a ""removable"" discontinuous point for $\frac{f}{g}$ and therefore it seems it should have a well-defined limit at $x_0$ from all sides and that limit should be equal. My intuition for why the answer to the above should be ""no, $h$ is not continuous"" is this: Suppose additionally that $C_1$ and $C_2$ were differentiable everywhere and that $f$ and $g$ had a non-zero directional derivative along $C_1$ and $C_2$ . By L'hopitals rule, we would have: $$k=\lim_{y\rightarrow 0}\frac{(f \circ C_1) (y)}{(g \circ C_1) (y)}=\lim_{y\rightarrow 0}\frac{(f \circ C_1)' (y)}{(g \circ C_1)' (y)}=\lim_{x\rightarrow x_0}\frac{\nabla_{C_1} f(x)}{\nabla_{C_1} g(x)}$$ and: $$k=\lim_{y\rightarrow 0}\frac{(f \circ C_2) (y)}{(g \circ C_2) (y)}=\lim_{y\rightarrow 0}\frac{(f \circ C_2)' (y)}{(g \circ C_2)' (y)}=\lim_{x\rightarrow x_0}\frac{\nabla_{C_2} f(x)}{\nabla_{C_2} g(x)}$$ Thus: $$\lim_{x\rightarrow x_0}\frac{\nabla_{C_1} f(x)}{\nabla_{C_1} g(x)}=\lim_{x\rightarrow x_0}\frac{\nabla_{C_2} f(x)}{\nabla_{C_2} g(x)}$$ This seems like quite a strong statement and its hard for me to believe it is true for every curve $C_2$ such that $g$ has non-zero directional derivative along $C_2$ ... Apologies for the very long post but I am really very confused and any help clearing up my understanding here would be very appreciated!","I am really confused and was hoping someone could help clear up my understanding. Suppose I have two continuous differentiable surfaces and there exists a point such that . Further is non-zero everywhere else in the domain. I am interested in the surface , which must be continuous everywhere where is non-zero. Suppose I find a curve such that and for some and I am able to show that: The question which is confusing me is this: Does the existence of the above limit imply that if I were to find any curve function, say such that and for some , it would be true that: i.e. is the limit now the same along all curves? Or in other words, if I defined for all and would be continuous? My intuition for why the answer to the above should be ""yes, would be continuous"" is this: should be a ""removable"" discontinuous point for and therefore it seems it should have a well-defined limit at from all sides and that limit should be equal. My intuition for why the answer to the above should be ""no, is not continuous"" is this: Suppose additionally that and were differentiable everywhere and that and had a non-zero directional derivative along and . By L'hopitals rule, we would have: and: Thus: This seems like quite a strong statement and its hard for me to believe it is true for every curve such that has non-zero directional derivative along ... Apologies for the very long post but I am really very confused and any help clearing up my understanding here would be very appreciated!","f,g:\mathbb{R}^N\rightarrow\mathbb{R} x_0 f(x_0)=g(x_0)=0 g(x) \frac{f}{g} g C_1:[0,1]\rightarrow\mathbb{R}^N x_0=C_1(0) x_1=C_1(1) x_1 \in \mathbb{R}^N \lim_{y \rightarrow 0}\frac{f(C_1(y))}{g(C_1(y))}=k C_2:[0,1]\rightarrow\mathbb{R}^N x_0=C_2(0) x_2=C_2(1) x_2 \in \mathbb{R}^N \lim_{y \rightarrow 0}\frac{f(C_2(y))}{g(C_2(y))}=k h(x)=\frac{f(x)}{g(x)} x\neq x_0 h(x_0)=k h h x_0 \frac{f}{g} x_0 h C_1 C_2 f g C_1 C_2 k=\lim_{y\rightarrow 0}\frac{(f \circ C_1) (y)}{(g \circ C_1) (y)}=\lim_{y\rightarrow 0}\frac{(f \circ C_1)' (y)}{(g \circ C_1)' (y)}=\lim_{x\rightarrow x_0}\frac{\nabla_{C_1} f(x)}{\nabla_{C_1} g(x)} k=\lim_{y\rightarrow 0}\frac{(f \circ C_2) (y)}{(g \circ C_2) (y)}=\lim_{y\rightarrow 0}\frac{(f \circ C_2)' (y)}{(g \circ C_2)' (y)}=\lim_{x\rightarrow x_0}\frac{\nabla_{C_2} f(x)}{\nabla_{C_2} g(x)} \lim_{x\rightarrow x_0}\frac{\nabla_{C_1} f(x)}{\nabla_{C_1} g(x)}=\lim_{x\rightarrow x_0}\frac{\nabla_{C_2} f(x)}{\nabla_{C_2} g(x)} C_2 g C_2","['limits', 'derivatives', 'continuity', 'surfaces']"
46,Derivative of $E\left(\int_t^T S_udu|\mathcal{F}_t\right)$,Derivative of,E\left(\int_t^T S_udu|\mathcal{F}_t\right),"Calculate the derivarive of $E\left(\int_t^T S_udu|\mathcal{F}_t\right)$ in function of $t$ $$\frac{d}{dt}E\left(\int_t^T S_udu|\mathcal{F}_t\right)$$ where $S_t$ is a stochastic process( I try to avoid specifying its form). My attempt: We have $$E\left(\int_t^T S_udu|\mathcal{F}_t\right) = \int_t^T E\left(S_u|\mathcal{F}_t\right)du$$ then $$\frac{d}{dt}E\left(\int_t^T S_udu|\mathcal{F}_t\right) = \frac{d}{dt}\left(\int_t^T E\left(S_u|\mathcal{F}_t\right)du\right) $$ But after that, I don't know how to calculate $\frac{d}{dt}\left(\int_t^T E\left(S_u|\mathcal{F}_{\color{red}{t}}\right)du\right) $ because of the subscript in red color.","Calculate the derivarive of in function of where is a stochastic process( I try to avoid specifying its form). My attempt: We have then But after that, I don't know how to calculate because of the subscript in red color.",E\left(\int_t^T S_udu|\mathcal{F}_t\right) t \frac{d}{dt}E\left(\int_t^T S_udu|\mathcal{F}_t\right) S_t E\left(\int_t^T S_udu|\mathcal{F}_t\right) = \int_t^T E\left(S_u|\mathcal{F}_t\right)du \frac{d}{dt}E\left(\int_t^T S_udu|\mathcal{F}_t\right) = \frac{d}{dt}\left(\int_t^T E\left(S_u|\mathcal{F}_t\right)du\right)  \frac{d}{dt}\left(\int_t^T E\left(S_u|\mathcal{F}_{\color{red}{t}}\right)du\right) ,"['derivatives', 'stochastic-calculus', 'stochastic-integrals', 'stochastic-analysis']"
47,Derivation of Product Rule for Finite Differences without Shift Operator,Derivation of Product Rule for Finite Differences without Shift Operator,,"Let $h(x)=f(x)g(x)$ , and let the $\Delta$ be the forward difference operator on functions be $$\Delta f := \frac{f_{j+1}-f_j}{\Delta x}$$ where $f_j = f(j\Delta x)$ Now I can derive the product rule for finite differences in terms of $\Delta f$ and $\Delta g$ easily $$\Delta h = \frac{h_{j+1}-h_j}{\Delta x} = \frac{f_{j+1}g_{j+1}-f_jg_j}{\Delta x} = \frac{f_{j+1}g_{j+1}\color{red}{-f_jg_{j+1} }{\color{blue}{+f_jg_{j+1}}}-f_jg_j}{\Delta x}=g_{j+1}\Delta f+f_j\Delta g$$ However, you can see in the first term that $g$ is shifted forward by $1$ step. But on Wikipedia for the finite differences the rule is express with a second order term: $$\Delta(fg) = f\Delta g + g \Delta f + \Delta f \Delta g$$ I have tried converting my expression to this with all sorts of algebra, and I even tried using the wiki's formula to go backwards to my equation, but nothing has worked. It's clear there must be some clever trick to convert the shift operator in terms of $f,\Delta f,\Delta g$ but I can't see how. Any ideas?","Let , and let the be the forward difference operator on functions be where Now I can derive the product rule for finite differences in terms of and easily However, you can see in the first term that is shifted forward by step. But on Wikipedia for the finite differences the rule is express with a second order term: I have tried converting my expression to this with all sorts of algebra, and I even tried using the wiki's formula to go backwards to my equation, but nothing has worked. It's clear there must be some clever trick to convert the shift operator in terms of but I can't see how. Any ideas?","h(x)=f(x)g(x) \Delta \Delta f := \frac{f_{j+1}-f_j}{\Delta x} f_j = f(j\Delta x) \Delta f \Delta g \Delta h = \frac{h_{j+1}-h_j}{\Delta x} = \frac{f_{j+1}g_{j+1}-f_jg_j}{\Delta x} = \frac{f_{j+1}g_{j+1}\color{red}{-f_jg_{j+1} }{\color{blue}{+f_jg_{j+1}}}-f_jg_j}{\Delta x}=g_{j+1}\Delta f+f_j\Delta g g 1 \Delta(fg) = f\Delta g + g \Delta f + \Delta f \Delta g f,\Delta f,\Delta g","['calculus', 'derivatives', 'finite-differences', 'discrete-calculus']"
48,Figuring out Mechanics of How Chain Rule was Applied,Figuring out Mechanics of How Chain Rule was Applied,,"How do I go from system $(1)$ to system $(2)$ below? \begin{gather} \begin{aligned} S &= K e^{x}\\[7pt] V(S,t) &= K v(x, \tau)\\ \tau &= (T-t) \frac{\sigma^{2}}{2} \end{aligned}\tag{1} \end{gather} \begin{gather} \begin{aligned} \frac{d}{dt}V &= -K \frac{\sigma^{2}}{2} \frac{d}{d \tau} v\\ \frac{d}{dS}V &= \frac{K}{S} \frac{d}{dx} v \end{aligned}\tag{2} \end{gather}",How do I go from system to system below?,"(1) (2) \begin{gather}
\begin{aligned}
S &= K e^{x}\\[7pt]
V(S,t) &= K v(x, \tau)\\
\tau &= (T-t) \frac{\sigma^{2}}{2}
\end{aligned}\tag{1}
\end{gather} \begin{gather}
\begin{aligned}
\frac{d}{dt}V &= -K \frac{\sigma^{2}}{2} \frac{d}{d \tau} v\\
\frac{d}{dS}V &= \frac{K}{S} \frac{d}{dx} v
\end{aligned}\tag{2}
\end{gather}","['calculus', 'derivatives', 'chain-rule', 'change-of-variable']"
49,"How should we use differential ""dx"" in deriving Tabular formula","How should we use differential ""dx"" in deriving Tabular formula",,"I am learning repeated integration with the  Tabular method. I see textbooks or web sites prove the method, but  most of the times, they  either do not explain each step or just skip with (...). I try to prove it myself  by going through  each step.  I am not sure if my manipulations of differential "" $dx$ "" are legitimate, and in general how to justify them. I started with  with  (uv)'=u'v+vu' $\frac {d(uv)} {dx}  = \frac {du} {dx}v+ u\frac {dv} {dx}$ $\to$ (Q1: Can I  simply multiply both sides  by $dx$ ?) $d(uv)= vdu+udv$ $\int uv =\int udv+\int vdu$ $\int udv=\int uv -\int vdu$ $    $ (I) Set $\frac {du} {dx}=u^1$ , $\frac {dv^{-1}}{dx}=v$ $\to$ $du= u^1dx$ , $v=\frac {dv^{(-1)}} {dx}$ $\int vdu= \int u^1dx$$\frac {dv^{(-1)}} {dx}$ $\to$ $\int u^1dv^{-1}$ (Q2:  Can I directly cancel $dx$ from  both numerator and denominator?) $\int udv= uv -\int u^1dv^{-1}  $ Repeating similar steps $\to$ $\int udv=$$\sum_{i=0}^{n-1} (-1)^iu^i v^{-i} +\int (-1)^nu^n dv^{-n}$ $    $ $\int udv=$$\sum_{i=0}^{n} (-1)^iu^i v^{-i} +\int (-1)^{n+1}u^{n+1} dv^{-(n+1)}$ $    $ (II) which is equivalent to common expression $\int udv=$$\sum_{i=0}^{n} (-1)^iu^i v^{-i} +\int (-1)^{n+1}u^{n+1} v^{-(n)}dx$ $    $ (II')","I am learning repeated integration with the  Tabular method. I see textbooks or web sites prove the method, but  most of the times, they  either do not explain each step or just skip with (...). I try to prove it myself  by going through  each step.  I am not sure if my manipulations of differential "" "" are legitimate, and in general how to justify them. I started with  with  (uv)'=u'v+vu' (Q1: Can I  simply multiply both sides  by ?) (I) Set , , (Q2:  Can I directly cancel from  both numerator and denominator?) Repeating similar steps (II) which is equivalent to common expression (II')","dx \frac {d(uv)} {dx} 
= \frac {du} {dx}v+ u\frac {dv} {dx} \to dx d(uv)= vdu+udv \int uv =\int udv+\int vdu \int udv=\int uv -\int vdu      \frac {du} {dx}=u^1 \frac {dv^{-1}}{dx}=v \to du= u^1dx v=\frac {dv^{(-1)}} {dx} \int vdu= \int u^1dx\frac {dv^{(-1)}} {dx} \to \int u^1dv^{-1} dx \int udv= uv -\int u^1dv^{-1}   \to \int udv=\sum_{i=0}^{n-1} (-1)^iu^i v^{-i} +\int (-1)^nu^n dv^{-n}      \int udv=\sum_{i=0}^{n} (-1)^iu^i v^{-i} +\int (-1)^{n+1}u^{n+1} dv^{-(n+1)}      \int udv=\sum_{i=0}^{n} (-1)^iu^i v^{-i} +\int (-1)^{n+1}u^{n+1} v^{-(n)}dx     ","['integration', 'derivatives']"
50,"Leibniz question on notation, specifically using with vertical point bars?","Leibniz question on notation, specifically using with vertical point bars?",,"I'm trying to understand Leibniz notation.  I have read the Wiki page on the notation and numerous others, but think I'm overlooking something crucial to understanding. I have been told that: $$ y=f(x) $$ $$ f'(x) = \frac {dy}{dx} $$ Question 1 : In the above form, I'm assuming the strings of symbols $x$ (LHS) and $dx$ (RHS) should be treated as different objects, and that $dx$ represents both the denominator of the difference quotient $\lim_{h\rightarrow0}{\frac {f(a+h) - f(a)}{(a+h) - a}}$ at some point $a$ and that $x$ is the independent variable which to infinitesimally change.  Is that correct?  To eliminate symbol confusion, is the following form equivalent? a. $$ f'(a) = \frac {dy}{dx} \Big|_{a}= \frac {dy}{dx} = \lim_{h\rightarrow0} {\frac {f(a+h) - f(a)} {(a+h) -a}} $$ An example of the chain rule : b. $$  f(x) = (f_1 \circ f_2 \circ f_3)(x)  $$ $$ y = f_1(u), u = f_2(v), v = f_3(x) $$ $$  f'(a)  = \frac {dy} {du} \Big|_{f_2(f_3(a))}\frac {du} {dv} \Big|_{f_3(a)} \frac {dv} {dx} \Big|_{a}  $$ $$  \begin{align*} f'(a) &= (f_1' \circ (f_2 \circ f_3))(a) \cdot (f_2' \circ f_3)(a) \cdot f_3'(a)  \end{align*} $$ Question 2 : I understand the vertical bars $\Big|_{z}$ provide a point from which to take the derivative.  I observe there is notation with and without bars and have not seen any history of Leibniz using them.  I'm assuming they help reduce back references to the function definitions.  Does anyone know why used?","I'm trying to understand Leibniz notation.  I have read the Wiki page on the notation and numerous others, but think I'm overlooking something crucial to understanding. I have been told that: Question 1 : In the above form, I'm assuming the strings of symbols (LHS) and (RHS) should be treated as different objects, and that represents both the denominator of the difference quotient at some point and that is the independent variable which to infinitesimally change.  Is that correct?  To eliminate symbol confusion, is the following form equivalent? a. An example of the chain rule : b. Question 2 : I understand the vertical bars provide a point from which to take the derivative.  I observe there is notation with and without bars and have not seen any history of Leibniz using them.  I'm assuming they help reduce back references to the function definitions.  Does anyone know why used?","
y=f(x)
 
f'(x) = \frac {dy}{dx}
 x dx dx \lim_{h\rightarrow0}{\frac {f(a+h) - f(a)}{(a+h) - a}} a x 
f'(a) = \frac {dy}{dx} \Big|_{a}= \frac {dy}{dx} = \lim_{h\rightarrow0} {\frac {f(a+h) - f(a)} {(a+h) -a}}
  
f(x) = (f_1 \circ f_2 \circ f_3)(x) 
 
y = f_1(u), u = f_2(v), v = f_3(x)
  
f'(a) 
= \frac {dy} {du} \Big|_{f_2(f_3(a))}\frac {du} {dv} \Big|_{f_3(a)} \frac {dv} {dx} \Big|_{a} 
  
\begin{align*}
f'(a) &= (f_1' \circ (f_2 \circ f_3))(a) \cdot (f_2' \circ f_3)(a) \cdot f_3'(a) 
\end{align*}
 \Big|_{z}","['calculus', 'derivatives', 'notation']"
51,When can we affirm this equality about integral?,When can we affirm this equality about integral?,,"Suppose we have $f(x) = \int G(x_{0},x_{1},...)\,dx_{0}\,dx_{1}\dotsb dx_{n}$ When can we affirm that $$df = G(x_{0},x_{1},...)? \tag1$$ Basically, I am having trouble to understand how to deal with differentials of functions, intuitively I thought that we can do that: $$\delta f(x) = \int \sum\left(\frac{\partial G}{\partial dx_{i}}\right)\,dx_{0}\,dx_{1} \dotsb dx_{n} \tag2$$ But I am not sure how $(2)$ reduces to $(1)$ .","Suppose we have When can we affirm that Basically, I am having trouble to understand how to deal with differentials of functions, intuitively I thought that we can do that: But I am not sure how reduces to .","f(x) = \int G(x_{0},x_{1},...)\,dx_{0}\,dx_{1}\dotsb dx_{n} df = G(x_{0},x_{1},...)? \tag1 \delta f(x) = \int \sum\left(\frac{\partial G}{\partial dx_{i}}\right)\,dx_{0}\,dx_{1}
\dotsb dx_{n} \tag2 (2) (1)","['calculus', 'integration', 'derivatives']"
52,Derivative of a summation with variable in the upper limit,Derivative of a summation with variable in the upper limit,,"I wish to derive the following equation with respect to its upper limit, but i'm not sure how to proceed. $${ d \over dv} \sum_{t=1}^{T-t_0(v)} f(t)$$ This link offered some insight, as well as this link , altough i did not quite understand the solution of this second one. I thought of a solution like this. Create a function using the following integral $$ G(v,T,t) =  \int_1^{T-t_0(v)} f(t) \, dt $$ Then take the derivative of this function with respect to v : $${ d \over dv} G(v,T,t)$$ This is correct?","I wish to derive the following equation with respect to its upper limit, but i'm not sure how to proceed. This link offered some insight, as well as this link , altough i did not quite understand the solution of this second one. I thought of a solution like this. Create a function using the following integral Then take the derivative of this function with respect to v : This is correct?","{ d \over dv} \sum_{t=1}^{T-t_0(v)} f(t) 
G(v,T,t) = 
\int_1^{T-t_0(v)} f(t) \, dt
 { d \over dv} G(v,T,t)","['integration', 'derivatives', 'summation']"
53,"Using Mathematical Induction to Prove that $f^n (x) = 3^n e^{3x}$, where $f^n (x)$ is the $n$th derivative of $f$, and $n$ is a natural number.","Using Mathematical Induction to Prove that , where  is the th derivative of , and  is a natural number.",f^n (x) = 3^n e^{3x} f^n (x) n f n,"Here is the question that I am not sure if I am doing right. Let $f(x) = e^{3x}$ and suppose $f^n (x)$ represents the $n$ th derivative of $f(x)$ with respect to $x$ . Use mathematical induction to prove $f^n (x) = 3^n e^{3x}$ , where $n$ is a natural number. And here is my attempt: First, we have to show that the base case is true. So for $n = 1$ , We know that $f'(x) = 3e^{3x}$ . We want to see if $f^1 (x)$ is the same. So \begin{align*} f'(x) &= f^1 (x) \\ 3e^{3x} &= 3^1 e^{3x} \\ 3e^{3x} &= 3e^{3x} \end{align*} Since LS $\equiv$ RS for $n = 1$ , the base case is true. For the inductive step, we want to assume that $f^n(x) = 3^n e^{3x}$ is true for $n = k$ , such that $f^k(x) = 3^k e^{3x}$ . Then we want to show that this is also true for $n = k + 1$ . \begin{align*} f^{k+1}(x) &= 3^{k + 1}e^{3x} \\ &= (3)^k(3)^1e^{3x} \\ &= (3)^1(3^k e^{3x}) \\ &= 3f^k(x) \end{align*} I don't know if this is correct or not, or if I am on the right track. Some help or some suggestions would be appreciated!","Here is the question that I am not sure if I am doing right. Let and suppose represents the th derivative of with respect to . Use mathematical induction to prove , where is a natural number. And here is my attempt: First, we have to show that the base case is true. So for , We know that . We want to see if is the same. So Since LS RS for , the base case is true. For the inductive step, we want to assume that is true for , such that . Then we want to show that this is also true for . I don't know if this is correct or not, or if I am on the right track. Some help or some suggestions would be appreciated!","f(x) = e^{3x} f^n (x) n f(x) x f^n (x) = 3^n e^{3x} n n = 1 f'(x) = 3e^{3x} f^1 (x) \begin{align*}
f'(x) &= f^1 (x) \\
3e^{3x} &= 3^1 e^{3x} \\
3e^{3x} &= 3e^{3x}
\end{align*} \equiv n = 1 f^n(x) = 3^n e^{3x} n = k f^k(x) = 3^k e^{3x} n = k + 1 \begin{align*}
f^{k+1}(x) &= 3^{k + 1}e^{3x} \\
&= (3)^k(3)^1e^{3x} \\
&= (3)^1(3^k e^{3x}) \\
&= 3f^k(x)
\end{align*}","['calculus', 'derivatives', 'proof-writing']"
54,Green function derivation,Green function derivation,,"My professor derived the free-space dyadic Green function from the general Green function: $$\tag{1} \stackrel{\leftrightarrow}{\boldsymbol{G}}=\left[\stackrel{\leftrightarrow}{\mathbb{I}}+\frac{1}{k^{2}} \nabla \nabla\right] G_{0}(R)$$ With the scalar Green function: $$\tag{2} G_{0}(R)=\frac{e^{i k R}}{4 \pi R}, \quad R \equiv\left|\boldsymbol{r}-\boldsymbol{r}^{\prime}\right|$$ In the derivation, he states the following: $$\tag{3} \nabla \hat{\mathbf{R}}=\nabla\left(\frac{\mathbf{R}}{R}\right)=\frac{\nabla \mathbf{R}}{R}+\mathbf{R} \nabla \frac{1}{R}$$ where $\boldsymbol{R}=\boldsymbol{r}-\boldsymbol{r}^{\prime}, \quad \widehat{\boldsymbol{R}}=\frac{\boldsymbol{R}}{R}$ . He says we can see from eq. (3) that $\nabla \mathbf{R}=\mathbb{I}$ , but I don't see that? Can someone explain how he gets to that conclusion?","My professor derived the free-space dyadic Green function from the general Green function: With the scalar Green function: In the derivation, he states the following: where . He says we can see from eq. (3) that , but I don't see that? Can someone explain how he gets to that conclusion?","\tag{1}
\stackrel{\leftrightarrow}{\boldsymbol{G}}=\left[\stackrel{\leftrightarrow}{\mathbb{I}}+\frac{1}{k^{2}} \nabla \nabla\right] G_{0}(R) \tag{2}
G_{0}(R)=\frac{e^{i k R}}{4 \pi R}, \quad R \equiv\left|\boldsymbol{r}-\boldsymbol{r}^{\prime}\right| \tag{3}
\nabla \hat{\mathbf{R}}=\nabla\left(\frac{\mathbf{R}}{R}\right)=\frac{\nabla \mathbf{R}}{R}+\mathbf{R} \nabla \frac{1}{R} \boldsymbol{R}=\boldsymbol{r}-\boldsymbol{r}^{\prime}, \quad \widehat{\boldsymbol{R}}=\frac{\boldsymbol{R}}{R} \nabla \mathbf{R}=\mathbb{I}","['calculus', 'algebra-precalculus', 'derivatives', 'proof-explanation']"
55,How Jacobian is defined for the function of a matrix?,How Jacobian is defined for the function of a matrix?,,"Let $f: \mathbb{R}^{m\times n} \rightarrow \mathbb{R}^m$ where $f(W)=Wx$ . Question1: How is the Jacobian matrix defined for a vector-valued function whose variable is a matrix? Question2: Using the answer to the above, how one can generalized the Jacobian formula of a composition functions whose variables are matrices? By generalization I mean how the Jacobian of a function like $g(x)=h(u(x))$ where $x \in \mathbb{R}^n$ , $u: \mathbb{R}^n \rightarrow \mathbb{R}^l$ , and $h: \mathbb{R}^l \rightarrow \mathbb{R}^m$ as follows: $$ J_x(g) = J_u(h) J_x(u) $$ and $J_x(g)$ is the Jacobian of $g$ with respect to $x$ can be handle when $x$ becomes a matrix. The above simply says the Jacobian of a composition function is the product of the Jacobians. How can one mimic this when the variable is a matrix? My thoughts: For the given function $f(W)=Wx$ one can write the following: $$ f(W)=Wx = \begin{bmatrix} W_{1\bullet}x\\ \vdots\\ W_{m\bullet}x \end{bmatrix} $$ where $W_{m\bullet}$ is the m-th row of $W$ . Now the Jacobian could be either $\begin{bmatrix} x^T\\ \vdots\\ x^T \end{bmatrix} \in \mathbb{R}^{m \times n} $ or $\begin{bmatrix} x& \dots& x \end{bmatrix} \in \mathbb{R}^{n \times m}. $","Let where . Question1: How is the Jacobian matrix defined for a vector-valued function whose variable is a matrix? Question2: Using the answer to the above, how one can generalized the Jacobian formula of a composition functions whose variables are matrices? By generalization I mean how the Jacobian of a function like where , , and as follows: and is the Jacobian of with respect to can be handle when becomes a matrix. The above simply says the Jacobian of a composition function is the product of the Jacobians. How can one mimic this when the variable is a matrix? My thoughts: For the given function one can write the following: where is the m-th row of . Now the Jacobian could be either or","f: \mathbb{R}^{m\times n} \rightarrow \mathbb{R}^m f(W)=Wx g(x)=h(u(x)) x \in \mathbb{R}^n u: \mathbb{R}^n \rightarrow \mathbb{R}^l h: \mathbb{R}^l \rightarrow \mathbb{R}^m 
J_x(g) = J_u(h) J_x(u)
 J_x(g) g x x f(W)=Wx 
f(W)=Wx
=
\begin{bmatrix}
W_{1\bullet}x\\
\vdots\\
W_{m\bullet}x
\end{bmatrix}
 W_{m\bullet} W \begin{bmatrix}
x^T\\
\vdots\\
x^T
\end{bmatrix}
\in \mathbb{R}^{m \times n}
 \begin{bmatrix}
x&
\dots&
x
\end{bmatrix}
\in \mathbb{R}^{n \times m}.
","['linear-algebra', 'derivatives']"
56,"By differentiation or otherwise, prove that $(c(z))^2 +(s(z))^2 = 1$","By differentiation or otherwise, prove that",(c(z))^2 +(s(z))^2 = 1,"Assume that the two power series $s(z) = \sum a_nz^n$ and $c(z) =\sum b_nz^n$ are convergent for all $z\in \mathbb{C},$ and that they satisfy the relations $s'(z) = c(z), c'(z) =-s(z).$ Deduce the identities $$ a_n = -a_{n-2}/(n(n-1)) , b_n=-a_{n-2}/(n(n-1)) $$ (To me, it looks like there is a typo for $a_n$ ) If further $s(0)=0, c(0) =1,$ determine $s(z)$ and $c(z)$ completely. By differentiation or otherwise, prove that $(c(z))^2 +(s(z))^2 = 1$ To me it looks like $s(z) = sin(z)$ and $c(z) = cos(z)$ but I don't know what to do with this information. Also while taking the derivate of the power series in an attempt to deduce the identities, I have $(n+1)a_{n+1}=b_n$ and it does not seem like I have the correct identity","Assume that the two power series and are convergent for all and that they satisfy the relations Deduce the identities (To me, it looks like there is a typo for ) If further determine and completely. By differentiation or otherwise, prove that To me it looks like and but I don't know what to do with this information. Also while taking the derivate of the power series in an attempt to deduce the identities, I have and it does not seem like I have the correct identity","s(z) = \sum a_nz^n c(z) =\sum b_nz^n z\in \mathbb{C}, s'(z) = c(z), c'(z) =-s(z).  a_n = -a_{n-2}/(n(n-1)) , b_n=-a_{n-2}/(n(n-1))
 a_n s(0)=0, c(0) =1, s(z) c(z) (c(z))^2 +(s(z))^2 = 1 s(z) = sin(z) c(z) = cos(z) (n+1)a_{n+1}=b_n","['complex-analysis', 'derivatives']"
57,Classifying the point $x=2$ for $f(x)=\cos(|x-2|)$,Classifying the point  for,x=2 f(x)=\cos(|x-2|),What is the point $x=2$ for this function? $$f(x)=\cos(|x-2|)$$ I calculated $$\lim_{x \to2^+} \frac{-\sin(|x-2|)|x-2|}{x-2}=0^-$$ $$\lim_{x \to2^-} \frac{-\sin(|x-2|)|x-2|}{x-2}=0^+$$ and it's a corner point. If I calculate the limit of the incremental ratio $$\lim_{h \to0^+} \frac{\cos(|h|)}{h}=+\infty$$ $$\lim_{h \to0^-} \frac{\cos(|h|)}{h}=-\infty$$ it's a cusp. Can it just be a stationary point?,What is the point for this function? I calculated and it's a corner point. If I calculate the limit of the incremental ratio it's a cusp. Can it just be a stationary point?,x=2 f(x)=\cos(|x-2|) \lim_{x \to2^+} \frac{-\sin(|x-2|)|x-2|}{x-2}=0^- \lim_{x \to2^-} \frac{-\sin(|x-2|)|x-2|}{x-2}=0^+ \lim_{h \to0^+} \frac{\cos(|h|)}{h}=+\infty \lim_{h \to0^-} \frac{\cos(|h|)}{h}=-\infty,"['limits', 'derivatives']"
58,"Applying the chain-rule to the function $u(x+sb, t+s)$ where $b \in \mathbb{R}^n$ is a constant vector and $s \in \mathbb{R}$",Applying the chain-rule to the function  where  is a constant vector and,"u(x+sb, t+s) b \in \mathbb{R}^n s \in \mathbb{R}","I want to apply the chain-rule to the function $z(s) := u(x + sb, t+s)$ where $x = (x_1, \cdots, x_n) \in \mathbb{R}^n$ and $b \in \mathbb{R}^n$ , $s \in \mathbb{R}$ are constants. I am supposed to get this: \begin{align} z(s) &= u(x+sb,t+s)\\[5pt] z'(s) &= u_{x_{1}}(x+sb,t+s)b_{1} + \dotsb + u_{x_{n}}(x+sb,t+s)b_{n} + u_{t}(x+sb,t+s)\cdot 1 \end{align} but I do not know how this is done concretely. Here is my attempt. I set $\xi(s) = x+sb$ and $\gamma(s) = t+s$ . Then, $z(s) : = u(\xi(s), \gamma(s))$ . The chain-rule then says: $$\frac{dz}{ds} = \frac{du}{d\xi}\frac{d\xi}{ds} + \frac{du}{d\gamma}\frac{d\gamma}{ds}.$$ What is $\frac{du}{d\xi}$ ? Since $\xi$ is a vector then I think it is the sum $u_{\xi_1} + \cdots + u_{\xi_n}$ . Then what is $u_{\xi_1}$ ? This is my confusion that I hope someone can clear for me.","I want to apply the chain-rule to the function where and , are constants. I am supposed to get this: but I do not know how this is done concretely. Here is my attempt. I set and . Then, . The chain-rule then says: What is ? Since is a vector then I think it is the sum . Then what is ? This is my confusion that I hope someone can clear for me.","z(s) := u(x + sb, t+s) x = (x_1, \cdots, x_n) \in \mathbb{R}^n b \in \mathbb{R}^n s \in \mathbb{R} \begin{align}
z(s) &= u(x+sb,t+s)\\[5pt]
z'(s) &= u_{x_{1}}(x+sb,t+s)b_{1} + \dotsb + u_{x_{n}}(x+sb,t+s)b_{n} + u_{t}(x+sb,t+s)\cdot 1
\end{align} \xi(s) = x+sb \gamma(s) = t+s z(s) : = u(\xi(s), \gamma(s)) \frac{dz}{ds} = \frac{du}{d\xi}\frac{d\xi}{ds} + \frac{du}{d\gamma}\frac{d\gamma}{ds}. \frac{du}{d\xi} \xi u_{\xi_1} + \cdots + u_{\xi_n} u_{\xi_1}","['calculus', 'derivatives', 'chain-rule']"
59,Proving slope inequality when function is concave up,Proving slope inequality when function is concave up,,"I'm trying to prove the following question: Suppose that $f$ is concave up on an interval $I$ and $x_1, x_2, x_3$ and $x_4$ are numbers in $I$ with $x_1 < x_2 < x_3 < x_4$ . Show that the slope of the  secant line through the points at $x=x_1$ and $x=x_3$ is less than the slope of the  secant line through the points at $x=x_2$ and $x=x_4$ . Now using concavity theorem , I have been able to derive the following equations: $\dfrac{f(x_3) - f(x_1)}{x_3 - x_1} < \dfrac{f(x_4) - f(x_3)}{x_4 - x_3}$ $\dfrac{f(x_2) - f(x_1)}{x_2 - x_1} < \dfrac{f(x_4) - f(x_2)}{x_4 - x_2}$ Rewriting the above equations as: $m_1 < \dfrac{f(x_4) - f(x_3)}{x_4 - x_3}$ $\dfrac{f(x_2) - f(x_1)}{x_2 - x_1} < m_2$ My objective is to prove that $m_1 < m_2$ I also know that: $\dfrac{f(x_2) - f(x_1)}{x_2 - x_1} < \dfrac{f(x_3) - f(x_4)}{x_3 - x_4}$ Now i'm stuck at this point. I'm not exactly sure how to proceed so as to prove $m_1 < m_2$ .","I'm trying to prove the following question: Suppose that is concave up on an interval and and are numbers in with . Show that the slope of the  secant line through the points at and is less than the slope of the  secant line through the points at and . Now using concavity theorem , I have been able to derive the following equations: Rewriting the above equations as: My objective is to prove that I also know that: Now i'm stuck at this point. I'm not exactly sure how to proceed so as to prove .","f I x_1, x_2, x_3 x_4 I x_1 < x_2 < x_3 < x_4 x=x_1 x=x_3 x=x_2 x=x_4 \dfrac{f(x_3) - f(x_1)}{x_3 - x_1} < \dfrac{f(x_4) - f(x_3)}{x_4 - x_3} \dfrac{f(x_2) - f(x_1)}{x_2 - x_1} < \dfrac{f(x_4) - f(x_2)}{x_4 - x_2} m_1 < \dfrac{f(x_4) - f(x_3)}{x_4 - x_3} \dfrac{f(x_2) - f(x_1)}{x_2 - x_1} < m_2 m_1 < m_2 \dfrac{f(x_2) - f(x_1)}{x_2 - x_1} < \dfrac{f(x_3) - f(x_4)}{x_3 - x_4} m_1 < m_2","['calculus', 'derivatives', 'slope']"
60,Computing the gradient of Cross Entropy Loss,Computing the gradient of Cross Entropy Loss,,"The categorical cross entropy loss is expressed as: $$L(y,t) = -\sum_{k=1}^{K}t_k\ln{y_k}$$ where $t$ is a one-hot encoded vector. $y_k$ is the softmax function defined as: $$y_k = \frac{e^{z_k}}{\sum_{j=1}^{K}e^{z_j}}$$ I want to compute the gradient, $\nabla_z$ , of the loss function with respect to the input of the output node. What I know: I understand how to compute the partial derivative of L with respect to a selected node (say, $z_k$ ). This yields the following expression: $$\frac{\partial L}{\partial z_k} = y_k - t_k$$ But I am not sure how to generalize this to the entire vector, $z$ . In essence, I know how to compute $\frac{\partial L}{\partial z_k}$ when $k = j$ and $k \neq j$ , but I don't know how to calculate the gradient, $\nabla_z$ .","The categorical cross entropy loss is expressed as: where is a one-hot encoded vector. is the softmax function defined as: I want to compute the gradient, , of the loss function with respect to the input of the output node. What I know: I understand how to compute the partial derivative of L with respect to a selected node (say, ). This yields the following expression: But I am not sure how to generalize this to the entire vector, . In essence, I know how to compute when and , but I don't know how to calculate the gradient, .","L(y,t) = -\sum_{k=1}^{K}t_k\ln{y_k} t y_k y_k = \frac{e^{z_k}}{\sum_{j=1}^{K}e^{z_j}} \nabla_z z_k \frac{\partial L}{\partial z_k} = y_k - t_k z \frac{\partial L}{\partial z_k} k = j k \neq j \nabla_z",['derivatives']
61,Term-by-term antidifferentiation and power series representation of antiderivative,Term-by-term antidifferentiation and power series representation of antiderivative,,"I have solved the following exercise and I would like to know If I have made any mistakes (NOTE: I can't use integrals, since they are defined in a later chapter): Assume $f(x)=\sum_{n=0}^{\infty} a_n x^n$ converges on $(-R,R)$ . (a) $F(x)=\sum_{n=0}^{\infty}\frac{a_n}{n+1}x^{n+1}$ is defined on $(-R,R)$ and satisfies $F'(x)=f(x)$ . (b) Antiderivatives are not unique. If $g$ is an arbitrary function satisfying $g'(x)=f(x)$ on $(-R,R)$ find a power series representation for $g$ . My solution: (a) Let $x\in (-R,R)$ : then the series $\sum_{n=0}^{\infty} a_n x^{n+1}$ is convergent in $(-R,R)$ since $\sum_{n=0}^{\infty} a_n x^n$ is convergent there by hypothesis and since $(\frac{1}{n+1})_{n=0}^{\infty}$ is a non-negative and decreasing sequence by Abel's Test we have that $\sum_{n=0}^{\infty}\frac{a_n}{n+1}x^{n+1}$ is convergent in $(-R,R)$ too so $F(x)$ is defined on $(-R,R)$ . Since by Theorem 6.5.2 * $\sum_{n=0}^{\infty} a_n x^n$ converges uniformly on every interval $[-c,c]\subseteq (-R,R)$ , $0\in [-c,c]$ and $F(0)=\sum_{n=0}^{\infty}\frac{a_n}{n+1}0^{n+1}=0$ is convergent, by Term-by-Term Differentiability Theorem we have $F'(x)=\sum_{n=0}^{\infty} F'_n(x)=\sum_{n=0}^{\infty} a_n x^n=f(x)$ on each $[-c,c]\subseteq (-R,R)$ . (b) $g(x)=C +\sum_{n=0}^{\infty}\frac{a_n}{n+1}x^{n+1}$ ( $C$ costant) since $g'(x)=0+(\sum_{n=0}^{\infty}\frac{a_n}{n+1}x^{n+1})'\stackrel{\text{(a)}}{=}\sum_{n=0}^{\infty}a_n x^n=f(x)$ , as desired. *Theorem 6.5.2 If a power series converges absolutely at $x_0$ then it converges uniformly on the interval $[-|x_0|,|x_0|]$","I have solved the following exercise and I would like to know If I have made any mistakes (NOTE: I can't use integrals, since they are defined in a later chapter): Assume converges on . (a) is defined on and satisfies . (b) Antiderivatives are not unique. If is an arbitrary function satisfying on find a power series representation for . My solution: (a) Let : then the series is convergent in since is convergent there by hypothesis and since is a non-negative and decreasing sequence by Abel's Test we have that is convergent in too so is defined on . Since by Theorem 6.5.2 * converges uniformly on every interval , and is convergent, by Term-by-Term Differentiability Theorem we have on each . (b) ( costant) since , as desired. *Theorem 6.5.2 If a power series converges absolutely at then it converges uniformly on the interval","f(x)=\sum_{n=0}^{\infty} a_n x^n (-R,R) F(x)=\sum_{n=0}^{\infty}\frac{a_n}{n+1}x^{n+1} (-R,R) F'(x)=f(x) g g'(x)=f(x) (-R,R) g x\in (-R,R) \sum_{n=0}^{\infty} a_n x^{n+1} (-R,R) \sum_{n=0}^{\infty} a_n x^n (\frac{1}{n+1})_{n=0}^{\infty} \sum_{n=0}^{\infty}\frac{a_n}{n+1}x^{n+1} (-R,R) F(x) (-R,R) \sum_{n=0}^{\infty} a_n x^n [-c,c]\subseteq (-R,R) 0\in [-c,c] F(0)=\sum_{n=0}^{\infty}\frac{a_n}{n+1}0^{n+1}=0 F'(x)=\sum_{n=0}^{\infty} F'_n(x)=\sum_{n=0}^{\infty} a_n x^n=f(x) [-c,c]\subseteq (-R,R) g(x)=C +\sum_{n=0}^{\infty}\frac{a_n}{n+1}x^{n+1} C g'(x)=0+(\sum_{n=0}^{\infty}\frac{a_n}{n+1}x^{n+1})'\stackrel{\text{(a)}}{=}\sum_{n=0}^{\infty}a_n x^n=f(x) x_0 [-|x_0|,|x_0|]","['real-analysis', 'sequences-and-series', 'derivatives', 'solution-verification', 'power-series']"
62,Number of roots of the equation $f(x)= \int_0^x (t-1)(t-2)(t-3)(t-4)dt =0$,Number of roots of the equation,f(x)= \int_0^x (t-1)(t-2)(t-3)(t-4)dt =0,"I have the following question before me: Find the number of roots of the equation $f(x)= \int_0^x (t-1)(t-2)(t-3)(t-4)dt =0$ in the interval $[0,5]$ . $0$ is clearly one of the roots. But how can I find other roots, if any? I tried evaluating the integral and came up with a fifth degree polynomial in $x$ having no constant term. The equation seemed quite daunting to me. How can I get the roots quicker? Please suggest.","I have the following question before me: Find the number of roots of the equation in the interval . is clearly one of the roots. But how can I find other roots, if any? I tried evaluating the integral and came up with a fifth degree polynomial in having no constant term. The equation seemed quite daunting to me. How can I get the roots quicker? Please suggest.","f(x)= \int_0^x (t-1)(t-2)(t-3)(t-4)dt =0 [0,5] 0 x","['integration', 'derivatives', 'definite-integrals']"
63,"Does dx in dA=dx*dy represent change, or is it a notation that denotes an infinitesimal arbitrary length? Change vs. an arbitrary physical length","Does dx in dA=dx*dy represent change, or is it a notation that denotes an infinitesimal arbitrary length? Change vs. an arbitrary physical length",,"I'm not sure how to explain this, but I have a gap in my understanding of infinitesimals/differentials. I've so far had calc 1 and 2, and have been taught that dy/dx represents a slope, which represents a change. So, my current understanding is if I had a function dy/dx(x) and plugged in x, I could deduce how much of a change in y I could expect, if I moved a unit x, at that particular x of interest. Where my understanding falls apart is, I don't see how dx in a slope, which represents change, would also represent ""change"" in dA=dx*dy. I mean in the classical meaning of the word change. I know we could say ""well in the same way delta X is a change in x and represents length"", but I don't view it as such. Not sure if this is related, but here's another thing bothering me, say we have P=dF/dA. Why dF and not only F? I understand dA would represent this infinitesimal area, but what does dF represent then? It's supposed to be a change in F, right? Why do we need a change? I'd really appreciate someone clearing this up or me, because I'm expected to pass a fluid mechanics test while our uni doesn't even offer a calc 3 course. You can understand my frustration. Thanks","I'm not sure how to explain this, but I have a gap in my understanding of infinitesimals/differentials. I've so far had calc 1 and 2, and have been taught that dy/dx represents a slope, which represents a change. So, my current understanding is if I had a function dy/dx(x) and plugged in x, I could deduce how much of a change in y I could expect, if I moved a unit x, at that particular x of interest. Where my understanding falls apart is, I don't see how dx in a slope, which represents change, would also represent ""change"" in dA=dx*dy. I mean in the classical meaning of the word change. I know we could say ""well in the same way delta X is a change in x and represents length"", but I don't view it as such. Not sure if this is related, but here's another thing bothering me, say we have P=dF/dA. Why dF and not only F? I understand dA would represent this infinitesimal area, but what does dF represent then? It's supposed to be a change in F, right? Why do we need a change? I'd really appreciate someone clearing this up or me, because I'm expected to pass a fluid mechanics test while our uni doesn't even offer a calc 3 course. You can understand my frustration. Thanks",,"['calculus', 'derivatives', 'differential', 'infinitesimals']"
64,Find the derivative of the function $f(x)=(P(x))^a$ where $a\in\Bbb{R}$ and $P(x)$ is a real polynomial with no real roots,Find the derivative of the function  where  and  is a real polynomial with no real roots,f(x)=(P(x))^a a\in\Bbb{R} P(x),"I need help to clarify this statement. I found it in a friend's old homework notebook and I think that the answer is $$f'(x)=aP'(x)\cdot(P(x))^{a-1}$$ However, I just don't get why the fact of $P(x)$ having no real roots is relevant. Is there a way to state the answer using only $P(x)$ instead of $P'(x)$ ? Am I missing something? I think his teacher was trying to make this problem look more complex than it is, but I just wanted to check if maybe I am the one who is mistaken.","I need help to clarify this statement. I found it in a friend's old homework notebook and I think that the answer is However, I just don't get why the fact of having no real roots is relevant. Is there a way to state the answer using only instead of ? Am I missing something? I think his teacher was trying to make this problem look more complex than it is, but I just wanted to check if maybe I am the one who is mistaken.",f'(x)=aP'(x)\cdot(P(x))^{a-1} P(x) P(x) P'(x),"['algebra-precalculus', 'derivatives', 'polynomials']"
65,Intuition behind stolz caesaro theorem [duplicate],Intuition behind stolz caesaro theorem [duplicate],,"This question already has answers here : How to understand intuitively the Stolz-Cesaro Theorem for sequences? (3 answers) Closed 3 years ago . The stolz caesaro theorem seems to be a discrete analogue of L'hopitals rule. I can understand l'hopitals rule via the taylor series, that is: $$ \lim_{x \to a} \frac{P(x)}{Q(x)} = \frac{ P(a) + \frac{dP}{dx}|_a (x-a) + O ( (x-a)^2)  }{  Q(a) + \frac{dQ}{dx}|_a (x-a) + O ( (x-a)^2)}$$ Now if $P(a) = Q(a) = 0$ , the limit becomes ratio of first order term/ the ratio of values nearby to the point of interest. So, Is there a similar 'series' intuition for stolz caesaro theorem?","This question already has answers here : How to understand intuitively the Stolz-Cesaro Theorem for sequences? (3 answers) Closed 3 years ago . The stolz caesaro theorem seems to be a discrete analogue of L'hopitals rule. I can understand l'hopitals rule via the taylor series, that is: Now if , the limit becomes ratio of first order term/ the ratio of values nearby to the point of interest. So, Is there a similar 'series' intuition for stolz caesaro theorem?", \lim_{x \to a} \frac{P(x)}{Q(x)} = \frac{ P(a) + \frac{dP}{dx}|_a (x-a) + O ( (x-a)^2)  }{  Q(a) + \frac{dQ}{dx}|_a (x-a) + O ( (x-a)^2)} P(a) = Q(a) = 0,['derivatives']
66,"Continuity, differentiability and double-differentiability of $\sum_{k=1}^{\infty} \frac{\sin(x/k)}{k}$ and $\sum_{k=1}^{\infty}\frac{\sin(kx)}{k^3}$","Continuity, differentiability and double-differentiability of  and",\sum_{k=1}^{\infty} \frac{\sin(x/k)}{k} \sum_{k=1}^{\infty}\frac{\sin(kx)}{k^3},I have solved the following exercise except for the last point and I would like to know if I have made any mistakes and I would appreciate any hint about how to complete the last question in part (b) about $g$ being twice-differentiable; thanks. Consider $f(x)=\sum_{k=1}^{\infty} \frac{\sin(x/k)}{k}$ and $g(x)=\sum_{k=1}^{\infty}\frac{\sin(kx)}{k^3}$ . (a) Where is $f$ defined? Continuous? Differentiable? Twice-differentiable? (b) Show that $g(x)$ is differentiable and that $g'(x)$ is continuous (c) Can we determine if $g(x)$ is twice-differentiable? My solution: (a) f defined Let $x\in\mathbb{R}$ : then $|\frac{1}{k}\sin(\frac{x}{k})|\leq\frac{1}{k}|\sin(\frac{x}{k})|\leq\frac{1}{k}\frac{|x|}{k}=\frac{|x|}{k^2}$ (where we have used the fact that $|\sin(x)|\leq |x|$ for all $x\in\mathbb{R}$ ) and since $\sum_{k=1}^{\infty}\frac{|x|}{k^2}=|x|\sum_{k=1}^{\infty}\frac{1}{k^2}<\infty$ by Comparison Test it must be $\sum_{k=1}^{\infty}|\frac{1}{k}\sin(\frac{x}{k})|<\infty$ which implies $\sum_{k=1}^{\infty}\frac{1}{k}\sin(\frac{x}{k})<\infty$ by Absolute Convergence Test. $f$ is thus well-defined for all $x\in\mathbb{R}$ . f continuous and differentiable $|f'_k(x)|=|\frac{1}{k^2}\cos(\frac{x}{k})|\leq\frac{1}{k^2}$ and $\sum_{k=1}^{\infty}\frac{1}{k^2}<\infty$ so $\sum_{k=1}^{\infty} f'_k(x)$ converges uniformly on $\mathbb{R}$ by Weierstrass M-test and since $\sum_{n=1}^{\infty}f_n(0)=0$ by Term-by-Term Differentiability Theorem we have that $f$ si differentiable (and hence continuous) on $\mathbb{R}$ and $f'(x)=\sum_{k=1}^{\infty}f'_k(x)=\sum_{k=1}^{\infty}\frac{1}{k^2}\cos(\frac{x}{k})$ . f twice-differentiable $|f''_k(x)|=|-\sin(\frac{x}{k})\frac{1}{k^3}|\leq\frac{1}{k^3}$ and $\sum_{k=1}^{\infty}\frac{1}{k^3}<\infty$ so $\sum_{k=1}^{\infty}f''_k(x)$ converges uniformly on $\mathbb{R}$ by Weierstrass M-Test and since $\sum_{k=1}^{\infty} f'_k(x)=\sum_{k=1}^{\infty}\frac{1}{k}\cos(\frac{0}{k})=\sum_{k=1}^{\infty}\frac{1}{k^2}<\infty$ by Term-by-Term Differentiability Theorem we have that $f'(x)$ is differentiable on $\mathbb{R}$ and $f''(x)=\sum_{k=1}^{\infty}f''_k(x)=\sum_{k=1}^{\infty}-\frac{1}{k^3}\sin(\frac{x}{k})$ . (b) g differentiable with g' continuous $|g'_k(x)|=|\frac{\cos(kx)}{k^2}|\leq\frac{1}{k^2}$ and $\sum_{k=1}^{\infty}\frac{1}{k^2}<\infty$ so $\sum_{k=1}^{\infty}g'_k(x)$ converges uniformly on $\mathbb{R}$ by Weierstrass M-Test and since $\sum_{k=1}^{\infty}\frac{\sin(k\cdot 0)}{k^3}=0$ by Term-by-Term Differentiability Theorem $\sum_{k=1}^{\infty}g_k(x)$ converges uniformly to a differentiable function $g(x)=\sum_{k=1}^{\infty}g_k(x)=\sum_{k=1}^{\infty}\frac{\sin(kx)}{k^3}$ on $\mathbb{R}$ and $g'(x)=\sum_{k=1}^{\infty}g'_k(x)=\frac{\cos(kx)}{k^2}$ and since each $g'_k$ is continuous by Term-by-Term Continuity Theorem $g'$ is continuous too. g twice-differentiable? Here I have tried the estimates $|g''_k(x)|=|-\frac{\sin(kx)}{k}|\leq\frac{1}{k}$ which tells me nothing and neither does the estimate $|g''_k(x)|=|-\frac{\sin(kx)}{k}|\leq\frac{k|x|}{k}=|x|$ since in both of these cases I can't use Weierstrass M-Test so I don't see how I could prove that $g$ is twice differentiable.,I have solved the following exercise except for the last point and I would like to know if I have made any mistakes and I would appreciate any hint about how to complete the last question in part (b) about being twice-differentiable; thanks. Consider and . (a) Where is defined? Continuous? Differentiable? Twice-differentiable? (b) Show that is differentiable and that is continuous (c) Can we determine if is twice-differentiable? My solution: (a) f defined Let : then (where we have used the fact that for all ) and since by Comparison Test it must be which implies by Absolute Convergence Test. is thus well-defined for all . f continuous and differentiable and so converges uniformly on by Weierstrass M-test and since by Term-by-Term Differentiability Theorem we have that si differentiable (and hence continuous) on and . f twice-differentiable and so converges uniformly on by Weierstrass M-Test and since by Term-by-Term Differentiability Theorem we have that is differentiable on and . (b) g differentiable with g' continuous and so converges uniformly on by Weierstrass M-Test and since by Term-by-Term Differentiability Theorem converges uniformly to a differentiable function on and and since each is continuous by Term-by-Term Continuity Theorem is continuous too. g twice-differentiable? Here I have tried the estimates which tells me nothing and neither does the estimate since in both of these cases I can't use Weierstrass M-Test so I don't see how I could prove that is twice differentiable.,g f(x)=\sum_{k=1}^{\infty} \frac{\sin(x/k)}{k} g(x)=\sum_{k=1}^{\infty}\frac{\sin(kx)}{k^3} f g(x) g'(x) g(x) x\in\mathbb{R} |\frac{1}{k}\sin(\frac{x}{k})|\leq\frac{1}{k}|\sin(\frac{x}{k})|\leq\frac{1}{k}\frac{|x|}{k}=\frac{|x|}{k^2} |\sin(x)|\leq |x| x\in\mathbb{R} \sum_{k=1}^{\infty}\frac{|x|}{k^2}=|x|\sum_{k=1}^{\infty}\frac{1}{k^2}<\infty \sum_{k=1}^{\infty}|\frac{1}{k}\sin(\frac{x}{k})|<\infty \sum_{k=1}^{\infty}\frac{1}{k}\sin(\frac{x}{k})<\infty f x\in\mathbb{R} |f'_k(x)|=|\frac{1}{k^2}\cos(\frac{x}{k})|\leq\frac{1}{k^2} \sum_{k=1}^{\infty}\frac{1}{k^2}<\infty \sum_{k=1}^{\infty} f'_k(x) \mathbb{R} \sum_{n=1}^{\infty}f_n(0)=0 f \mathbb{R} f'(x)=\sum_{k=1}^{\infty}f'_k(x)=\sum_{k=1}^{\infty}\frac{1}{k^2}\cos(\frac{x}{k}) |f''_k(x)|=|-\sin(\frac{x}{k})\frac{1}{k^3}|\leq\frac{1}{k^3} \sum_{k=1}^{\infty}\frac{1}{k^3}<\infty \sum_{k=1}^{\infty}f''_k(x) \mathbb{R} \sum_{k=1}^{\infty} f'_k(x)=\sum_{k=1}^{\infty}\frac{1}{k}\cos(\frac{0}{k})=\sum_{k=1}^{\infty}\frac{1}{k^2}<\infty f'(x) \mathbb{R} f''(x)=\sum_{k=1}^{\infty}f''_k(x)=\sum_{k=1}^{\infty}-\frac{1}{k^3}\sin(\frac{x}{k}) |g'_k(x)|=|\frac{\cos(kx)}{k^2}|\leq\frac{1}{k^2} \sum_{k=1}^{\infty}\frac{1}{k^2}<\infty \sum_{k=1}^{\infty}g'_k(x) \mathbb{R} \sum_{k=1}^{\infty}\frac{\sin(k\cdot 0)}{k^3}=0 \sum_{k=1}^{\infty}g_k(x) g(x)=\sum_{k=1}^{\infty}g_k(x)=\sum_{k=1}^{\infty}\frac{\sin(kx)}{k^3} \mathbb{R} g'(x)=\sum_{k=1}^{\infty}g'_k(x)=\frac{\cos(kx)}{k^2} g'_k g' |g''_k(x)|=|-\frac{\sin(kx)}{k}|\leq\frac{1}{k} |g''_k(x)|=|-\frac{\sin(kx)}{k}|\leq\frac{k|x|}{k}=|x| g,"['real-analysis', 'derivatives', 'solution-verification', 'uniform-convergence', 'sequence-of-function']"
67,"Prove that the function $ \zeta(x)=\sum^{\infty} \frac{1}{n^{x}}, x>1 $ is infinitely differentiable in $(1, \infty) . n=1$",Prove that the function  is infinitely differentiable in," \zeta(x)=\sum^{\infty} \frac{1}{n^{x}}, x>1  (1, \infty) . n=1","Prove that if $a>1$ and $k \geq 1,$ then a) $\sum_{n=2}^{\infty} \frac{(\log n)^{k}}{n^{a}}<\infty$ b)  Prove that the function $$ \zeta(x)=\sum^{\infty} \frac{1}{n^{x}}, x>1 $$ is infinitely differentiable in $(1, \infty) . n=1$ Hint: Use part (a) to prove part (b). You can use part (a) to prove (b), even if you do not know how to prove (a). My try: a)  As $\frac{(\log n)^{k}}{n^{a}}$ in monotonically decreasing because $\ln n$ grows slowly than $n^{a}$ . Is there a rigorous way to prove it? I tried in this way that $\frac{d}{dx}\left(\frac{(\log x)^{k}}{x^{a}}\right)=\frac{\frac{k}{x}(\log x)^{k-1}-a x^{a-1}}{x^{2 a}}=\frac{k(\log x)^{k-1}-a x^{a}}{x^{2 a+1}}$ and in this way inductively I was trying to see that $f(x)<f(y)$ whenn $x>y$ for some large $y$ . But not convinced fully. Thus we can use Cauchy's Condensation test, considering $f(n)=\frac{(\log n)^{k}}{n^{a}}$ . $\begin{aligned} 2^{n} f\left(2^{n}\right)=& \frac{2^{n}\left(\log 2^{n}\right)^{k}}{\left(2^{n}\right)^{a}}=\frac{\sin n^{k}(\log 2)^{k}}{2^{n(a-1)}} \\ &=\frac{n^{k} c}{2^{d n}} ; \text { where }\left(\log {2}\right)^{k}=c \text { finite constant }, d=a-1>0 \text { finite constant .}\end{aligned}$ Now, $\sum f(n)$ converges iff $\sum 2^{n} f\left(2^{n}\right)$ converges iff $\sum \frac{n^{k}}{2^{d n}}$ converges. By Ratio test the last series converges and hence the result. For part b) I can see that $h(x)=\frac{1}{n^{x}}=n^{-x}$ then $h'(x)=-n^{-x}\log n$ and $h''(x)=n^{-x}(\log n)^2$ and so on. Then I can see that the series with term by term differentiation of $\zeta(x)$ is convergent. How to conclude my answer from there?","Prove that if and then a) b)  Prove that the function is infinitely differentiable in Hint: Use part (a) to prove part (b). You can use part (a) to prove (b), even if you do not know how to prove (a). My try: a)  As in monotonically decreasing because grows slowly than . Is there a rigorous way to prove it? I tried in this way that and in this way inductively I was trying to see that whenn for some large . But not convinced fully. Thus we can use Cauchy's Condensation test, considering . Now, converges iff converges iff converges. By Ratio test the last series converges and hence the result. For part b) I can see that then and and so on. Then I can see that the series with term by term differentiation of is convergent. How to conclude my answer from there?","a>1 k \geq 1, \sum_{n=2}^{\infty} \frac{(\log n)^{k}}{n^{a}}<\infty 
\zeta(x)=\sum^{\infty} \frac{1}{n^{x}}, x>1
 (1, \infty) . n=1 \frac{(\log n)^{k}}{n^{a}} \ln n n^{a} \frac{d}{dx}\left(\frac{(\log x)^{k}}{x^{a}}\right)=\frac{\frac{k}{x}(\log x)^{k-1}-a x^{a-1}}{x^{2 a}}=\frac{k(\log x)^{k-1}-a x^{a}}{x^{2 a+1}} f(x)<f(y) x>y y f(n)=\frac{(\log n)^{k}}{n^{a}} \begin{aligned} 2^{n} f\left(2^{n}\right)=& \frac{2^{n}\left(\log 2^{n}\right)^{k}}{\left(2^{n}\right)^{a}}=\frac{\sin n^{k}(\log 2)^{k}}{2^{n(a-1)}} \\ &=\frac{n^{k} c}{2^{d n}} ; \text { where }\left(\log {2}\right)^{k}=c \text { finite constant }, d=a-1>0 \text { finite constant .}\end{aligned} \sum f(n) \sum 2^{n} f\left(2^{n}\right) \sum \frac{n^{k}}{2^{d n}} h(x)=\frac{1}{n^{x}}=n^{-x} h'(x)=-n^{-x}\log n h''(x)=n^{-x}(\log n)^2 \zeta(x)","['real-analysis', 'sequences-and-series', 'analysis', 'derivatives', 'power-series']"
68,Doubt about derivatives.,Doubt about derivatives.,,"$$f(x+y^2) = f(x) + f(y)^2$$ Hello everybody. I am doing this exercise, I need to find all functions real valued $f: \mathbb{R} \to \mathbb{R}$ that satisfy the condition above. I have already found some type of functions with a little of guess and check, but I would like to go beyond. I thought that would be a good idea to derivate both sides, but I am a little confused how to do it. As it was said, it is a function from $\mathbb{R}$ to $\mathbb{R}$ , not from $\mathbb{R}^2$ to $\mathbb{R}$ , so it makes me thought that would be a simple derivative, but how to do it? Can I say that $\frac{df(x+y^2)}{d(x+y^2)} = \frac{df(u)}{du} = \frac{df(x)}{dx}$ ? So that it would lead us to: $f' = f' + 2f(x)f'$","Hello everybody. I am doing this exercise, I need to find all functions real valued that satisfy the condition above. I have already found some type of functions with a little of guess and check, but I would like to go beyond. I thought that would be a good idea to derivate both sides, but I am a little confused how to do it. As it was said, it is a function from to , not from to , so it makes me thought that would be a simple derivative, but how to do it? Can I say that ? So that it would lead us to:",f(x+y^2) = f(x) + f(y)^2 f: \mathbb{R} \to \mathbb{R} \mathbb{R} \mathbb{R} \mathbb{R}^2 \mathbb{R} \frac{df(x+y^2)}{d(x+y^2)} = \frac{df(u)}{du} = \frac{df(x)}{dx} f' = f' + 2f(x)f',"['calculus', 'derivatives']"
69,"if $F'(ax) = G(x)$, then $F'(nax) = nG(nx)$?","if , then ?",F'(ax) = G(x) F'(nax) = nG(nx),"A question I found stated that: Given $$\frac{d}{dx}F(ax) = G(x)$$ then $$\frac{d}{dx}F(2ax) = \dots$$ with choices: 1.) G(2ax)           2.) G(ax)           3.) 2G(2x)           4.) 2G(x) I got the third option as the answer by assuming that $F(ax) = \sin(ax)$ and therefore $F'(ax)= G(x) = a\cos(ax)$ and hence: $F(2ax) = \sin(2ax)$ and therefore $F'(2ax)= 2a\cos(2ax) = 2G(2x)$ I then tested with $F(ax) = (ax)^2$ and $F(ax) = \log(ax)$ with same result as above. Is there a general rule to proof the statement: if $F'(ax) = G(x)$ , then $F'(nax) = nG(nx)$ ?","A question I found stated that: Given then with choices: 1.) G(2ax)           2.) G(ax)           3.) 2G(2x)           4.) 2G(x) I got the third option as the answer by assuming that and therefore and hence: and therefore I then tested with and with same result as above. Is there a general rule to proof the statement: if , then ?",\frac{d}{dx}F(ax) = G(x) \frac{d}{dx}F(2ax) = \dots F(ax) = \sin(ax) F'(ax)= G(x) = a\cos(ax) F(2ax) = \sin(2ax) F'(2ax)= 2a\cos(2ax) = 2G(2x) F(ax) = (ax)^2 F(ax) = \log(ax) F'(ax) = G(x) F'(nax) = nG(nx),['derivatives']
70,Finding the area not by integral,Finding the area not by integral,,"Original problem. Find the area under $y= \sqrt{x}$ in the range $\left [ 0, 1 \right ]$ My friend, she wants to use total area instead of calculating the integeral. So I tried something: Dividing the area by $n$ parts with each part is $\Delta x= \frac{1}{n}$ . The part $k$ is $\left [ \left ( k- 1 \right )\frac{1}{n}, \frac{k}{n} \right ]$ , we calculate the total area by formula: $$S_{n}= \sum_{k= 1}^{n}\frac{1}{n}\sqrt{\frac{k}{n}}= \frac{1}{n^{3/2}}\sum_{k= 1}^{n}k^{1/2}$$ How should I do next ?? Thank you....","Original problem. Find the area under in the range My friend, she wants to use total area instead of calculating the integeral. So I tried something: Dividing the area by parts with each part is . The part is , we calculate the total area by formula: How should I do next ?? Thank you....","y= \sqrt{x} \left [ 0, 1 \right ] n \Delta x= \frac{1}{n} k \left [ \left ( k- 1 \right )\frac{1}{n}, \frac{k}{n} \right ] S_{n}= \sum_{k= 1}^{n}\frac{1}{n}\sqrt{\frac{k}{n}}= \frac{1}{n^{3/2}}\sum_{k= 1}^{n}k^{1/2}","['calculus', 'integration']"
71,Computing the derivative of a matrix-valued function [closed],Computing the derivative of a matrix-valued function [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Suppose $\mathbf{A} :\mathbb{R}^m\to\mathbb{R}^{m\times m}$ such that $\mathbf{A}(\theta)$ is a symmetric matrix. Let $\mathbf{A}(\theta)$ have the eigen-decomposition $\mathbf{A}(\theta) = \mathbf{U}(\theta)~\mathrm{diag}(\lambda_1(\theta),\ldots,\lambda_m(\theta))~\mathbf{U}(\theta)^\top$ . If I have a function $f:\mathbb{R}\to\mathbb{R}$ and I define \begin{align} f(\mathbf{A}(\theta)) = \mathbf{U}(\theta) ~\mathrm{diag}(f(\lambda_1(\theta)),\ldots,f(\lambda_m(\theta)))~\mathbf{U}(\theta)^\top \end{align} How does one compute $\frac{\partial}{\partial\theta_i} f(\mathbf{A}(\theta))$ ?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Suppose such that is a symmetric matrix. Let have the eigen-decomposition . If I have a function and I define How does one compute ?","\mathbf{A} :\mathbb{R}^m\to\mathbb{R}^{m\times m} \mathbf{A}(\theta) \mathbf{A}(\theta) \mathbf{A}(\theta) = \mathbf{U}(\theta)~\mathrm{diag}(\lambda_1(\theta),\ldots,\lambda_m(\theta))~\mathbf{U}(\theta)^\top f:\mathbb{R}\to\mathbb{R} \begin{align}
f(\mathbf{A}(\theta)) = \mathbf{U}(\theta) ~\mathrm{diag}(f(\lambda_1(\theta)),\ldots,f(\lambda_m(\theta)))~\mathbf{U}(\theta)^\top
\end{align} \frac{\partial}{\partial\theta_i} f(\mathbf{A}(\theta))","['derivatives', 'eigenvalues-eigenvectors', 'matrix-calculus']"
72,Partial Derivatives : Given $f(x) = Ax^3 + By^3 - Cx - Dy + E$,Partial Derivatives : Given,f(x) = Ax^3 + By^3 - Cx - Dy + E,"Given $f(x) = Ax^3 + By^3 - Cx - Dy + E$ Propose any value for $A, B, C, D$ and $E$ so that these will give three(3) critical points. Please help. My choice of value only gives me two(2) critical point therefore it is not accurate. I randomly pick value for $A, B, C, D$ and $E$ , then I differentiate the equation. I find $\frac{\partial}{\partial x} (Ax^3 + By^3 - Cx - Dy + E )$ and $\frac{\partial}{\partial y}(Ax^3 + By^3 - Cx - Dy + E )$ . Both equation that I differentiated is then equal = $0$ Then I determine the value of $x$ and $y$ which will give me the critical points p/s: Very sorry I did not know how to use the math format","Given Propose any value for and so that these will give three(3) critical points. Please help. My choice of value only gives me two(2) critical point therefore it is not accurate. I randomly pick value for and , then I differentiate the equation. I find and . Both equation that I differentiated is then equal = Then I determine the value of and which will give me the critical points p/s: Very sorry I did not know how to use the math format","f(x) = Ax^3 + By^3 - Cx - Dy + E A, B, C, D E A, B, C, D E \frac{\partial}{\partial x} (Ax^3 + By^3 - Cx - Dy + E ) \frac{\partial}{\partial y}(Ax^3 + By^3 - Cx - Dy + E ) 0 x y","['derivatives', 'partial-differential-equations']"
73,Methods of solving an initial-value problem for a separable first-order ODE,Methods of solving an initial-value problem for a separable first-order ODE,,"In Schaum's outline of Differential Equations it is said that the initial-value problem of $A(x) d x+B(y) d y=0 $ ; $y\left(x_{0}\right)=y_{0}$ can be solved either $(1)$ integrating and applying the initial condition to evaluate $c$ or by using $(2)$ : $$ \int A(x) d x+\int B(y) d y=c; \quad y\left(x_{0}\right)=y_{0} \tag{1} $$ $$ \int_{x_{0}}^{x} A(x) d x+\int_{y_{0}}^{y} B(y) d y=0 \tag{2} $$ It is also said that this second method may not determine the solution of the initial-value problem uniquely; thal is, it may have many solutions, of which only one will satisfy the initial-value problem. As I see it, both methods are equivalent: if $\widetilde A(x)$ and $\widetilde B(y)$ are the primitive functions of $A(x)$ and $B(y)$ , then $$(1) \rightarrow \widetilde A(x) + \widetilde B(y) =c \rightarrow c= \widetilde A(x_0) + \widetilde B(y_0)\rightarrow \\\widetilde A(x)- \widetilde A(x_0) + \widetilde B(y)- \widetilde B(y_0)=0$$ Which is the same result we get from $(2)$ : $$(2) \rightarrow \widetilde A(x)- \widetilde A(x_0) + \widetilde B(y)- \widetilde B(y_0)=0$$ Why does then the book say that the second method may may not determine the solution of the initial-value problem uniquely ?","In Schaum's outline of Differential Equations it is said that the initial-value problem of ; can be solved either integrating and applying the initial condition to evaluate or by using : It is also said that this second method may not determine the solution of the initial-value problem uniquely; thal is, it may have many solutions, of which only one will satisfy the initial-value problem. As I see it, both methods are equivalent: if and are the primitive functions of and , then Which is the same result we get from : Why does then the book say that the second method may may not determine the solution of the initial-value problem uniquely ?","A(x) d x+B(y) d y=0  y\left(x_{0}\right)=y_{0} (1) c (2) 
\int A(x) d x+\int B(y) d y=c; \quad y\left(x_{0}\right)=y_{0} \tag{1}
 
\int_{x_{0}}^{x} A(x) d x+\int_{y_{0}}^{y} B(y) d y=0 \tag{2}
 \widetilde A(x) \widetilde B(y) A(x) B(y) (1) \rightarrow \widetilde A(x) + \widetilde B(y) =c \rightarrow c= \widetilde A(x_0) + \widetilde B(y_0)\rightarrow \\\widetilde A(x)- \widetilde A(x_0) + \widetilde B(y)- \widetilde B(y_0)=0 (2) (2) \rightarrow \widetilde A(x)- \widetilde A(x_0) + \widetilde B(y)- \widetilde B(y_0)=0","['calculus', 'integration', 'ordinary-differential-equations', 'derivatives']"
74,Is my explanation to this derivative problem correct?,Is my explanation to this derivative problem correct?,,"Basically, I am trying to prove that number 13 relates to the graph of B, but I was wondering if my explanation was correct. Here is the problem and here is what I wrote. From the interval of - ∞ to zero, the function is decreasing, so f ' < 0. The interval from zero to + ∞, the function is increasing, so f ' > 0. The graph that corresponds to this is B because the interval from - ∞ to zero, f ' is below zero on the y-axis and the interval from zero to + ∞, f ' is above zero on the y-axis. Also, for this problem, is there a more efficient way of saying where the function is increasing or decreasing, rather than saying ""in this interval, the function is increasing/decreasing,"" since there are 5 intervals and doesn't give the points on the axis? ,","Basically, I am trying to prove that number 13 relates to the graph of B, but I was wondering if my explanation was correct. Here is the problem and here is what I wrote. From the interval of - ∞ to zero, the function is decreasing, so f ' < 0. The interval from zero to + ∞, the function is increasing, so f ' > 0. The graph that corresponds to this is B because the interval from - ∞ to zero, f ' is below zero on the y-axis and the interval from zero to + ∞, f ' is above zero on the y-axis. Also, for this problem, is there a more efficient way of saying where the function is increasing or decreasing, rather than saying ""in this interval, the function is increasing/decreasing,"" since there are 5 intervals and doesn't give the points on the axis? ,",,"['calculus', 'derivatives']"
75,Find y in terms of $t$,Find y in terms of,t,"So I am given this equation for the rate of a reaction: $$\frac{dy}{dt}=y^2-8y+15$$ when $t=0, y=0$ [ what does this mean? ] How can I proceed to find $y$ in terms of $t$ ? I checked out this site but I still don't get how you're supposed to find $y$ in terms of $t$ from it. Some guidance would be appreciated.",So I am given this equation for the rate of a reaction: when [ what does this mean? ] How can I proceed to find in terms of ? I checked out this site but I still don't get how you're supposed to find in terms of from it. Some guidance would be appreciated.,"\frac{dy}{dt}=y^2-8y+15 t=0, y=0 y t y t","['calculus', 'derivatives', 'implicit-differentiation']"
76,Proof that the minimum of a bounded differentiable real function occurs at a stationary point or at an endpoint,Proof that the minimum of a bounded differentiable real function occurs at a stationary point or at an endpoint,,I couldn't find a proof for this well-known result on this site... The minimum of a bounded differentiable real function occurs at a stationary point or at an endpoint. I prove the contrapositive to this statement in my answer below. Alternative proofs are most welcome.,I couldn't find a proof for this well-known result on this site... The minimum of a bounded differentiable real function occurs at a stationary point or at an endpoint. I prove the contrapositive to this statement in my answer below. Alternative proofs are most welcome.,,"['real-analysis', 'derivatives', 'solution-verification']"
77,Unique property of differentiable functions,Unique property of differentiable functions,,"Recently I was going through the chain-rule derivative proof in Stewart Calculus - It's a great approach to avoid the objections of $\Delta$$u$$=0$ . But I don't understand why do we need to prove that $\epsilon$ is a continuous function of $\Delta$$x$ . The book claims it to be a property of a differentiable function, but what is the significance of this property and how are we helped here by using it? Also, does it really help in some later stages of calculus? If so, then how? It'd be really helpful if someone pointed out why it was necessary to use $\epsilon$ as a continuous function here.","Recently I was going through the chain-rule derivative proof in Stewart Calculus - It's a great approach to avoid the objections of . But I don't understand why do we need to prove that is a continuous function of . The book claims it to be a property of a differentiable function, but what is the significance of this property and how are we helped here by using it? Also, does it really help in some later stages of calculus? If so, then how? It'd be really helpful if someone pointed out why it was necessary to use as a continuous function here.",\Deltau=0 \epsilon \Deltax \epsilon,"['derivatives', 'chain-rule']"
78,"Prove that if $f$ is differentiable at 0, then the function $f(|x|)$ is differentiable at 0 if and only if $f'(0)=0$","Prove that if  is differentiable at 0, then the function  is differentiable at 0 if and only if",f f(|x|) f'(0)=0,"I have the following exercise: Prove that if $f$ is differentiable at 0, then the function $f(|x|)$ is differentiable at 0 if and only if $f'(0)=0$ This problem is somewhat similar and the accepted solution seems correct to me. But I would like to know if there is a different way to solve it, also, the difference to the exercise here is that we do not know that $f(0) = 0$ . I hope you can help me, thank you.","I have the following exercise: Prove that if is differentiable at 0, then the function is differentiable at 0 if and only if This problem is somewhat similar and the accepted solution seems correct to me. But I would like to know if there is a different way to solve it, also, the difference to the exercise here is that we do not know that . I hope you can help me, thank you.",f f(|x|) f'(0)=0 f(0) = 0,"['real-analysis', 'derivatives']"
79,proving that $f'(x) > g'(x)$ implies $f$ and $g$ cannot intersect more than once.,proving that  implies  and  cannot intersect more than once.,f'(x) > g'(x) f g,"I'm not completely certain if the statement is true, but intuitively I think it is. My sort of intuitive explanation would be if there is a point where $f$ and $g$ are equal, then the fact that $f'$ is greater than $g'$ would prevent them from intersecting again. I was thinking of doing something with the mean value theorem and a proof by contradiction: If we assume $f$ and $g$ intersect at two points, say $a$ and $b$ , then $f(a)=g(a)$ and $f(b)=g(b)$ . By the MVT, there must be points $c_f$ and $c_g$ such that $f'(c_f)=\frac{f(b)-f(a)}{b-a}$ and $f'(c_g)=\frac{g(b)-g(a)}{b-a}$ , which implies $f'(c_f)=g'(c_g)$ . But I'm not sure how much this would help since it would only lead to a contradiction if $c_f=c_g$ . Is there a way to continue the proof from here or some other way to prove this statement?","I'm not completely certain if the statement is true, but intuitively I think it is. My sort of intuitive explanation would be if there is a point where and are equal, then the fact that is greater than would prevent them from intersecting again. I was thinking of doing something with the mean value theorem and a proof by contradiction: If we assume and intersect at two points, say and , then and . By the MVT, there must be points and such that and , which implies . But I'm not sure how much this would help since it would only lead to a contradiction if . Is there a way to continue the proof from here or some other way to prove this statement?",f g f' g' f g a b f(a)=g(a) f(b)=g(b) c_f c_g f'(c_f)=\frac{f(b)-f(a)}{b-a} f'(c_g)=\frac{g(b)-g(a)}{b-a} f'(c_f)=g'(c_g) c_f=c_g,['derivatives']
80,ODE $y(x)=xy'(x)-\sqrt{y'(x)-1}$,ODE,y(x)=xy'(x)-\sqrt{y'(x)-1},"I've following differential equation... $$y(x)=xy'(x)-\sqrt{y'(x)-1}$$ I recognised that it's an Clairaut's equation and then I wanted to find the general and the singular solution. First I reformed the ODE like that... $$\frac{d}{dx}y=\frac{d}{dx}dy'-\frac{d}{dx}\sqrt{y'-1}$$ $$y'=xy''+y'-\frac{1}{2\sqrt{y'-1}}*y''$$ $$0=y''(x-\frac{1}{2\sqrt{y'-1}})$$ I can then deduce, that the equation is true if $y''=0$ or $x-\frac{1}{2\sqrt{y'-1}}=0$ I solved 1) like that... $$\int y'' dx = 0 \Leftrightarrow y'=C$$ In our original equation, that gives us... $$y=Cx-\sqrt{C-1}$$ Then I solved 2) $$x-\frac{1}{2\sqrt{y'-1}}=0 \Leftrightarrow y'=\frac{1}{4x^2}+1$$ $$\int y'\, dx=\int \frac{1}{4x^2}+1 \, dx$$ $$y = -\frac{1}{2x}+x+C$$ But I've drawn graphs of these two equations and 1), aka the general solution, doesn't touch 2), aka the singular solution. So i probably have a mistake somewhere, don't I? Can someone point it out to me?","I've following differential equation... I recognised that it's an Clairaut's equation and then I wanted to find the general and the singular solution. First I reformed the ODE like that... I can then deduce, that the equation is true if or I solved 1) like that... In our original equation, that gives us... Then I solved 2) But I've drawn graphs of these two equations and 1), aka the general solution, doesn't touch 2), aka the singular solution. So i probably have a mistake somewhere, don't I? Can someone point it out to me?","y(x)=xy'(x)-\sqrt{y'(x)-1} \frac{d}{dx}y=\frac{d}{dx}dy'-\frac{d}{dx}\sqrt{y'-1} y'=xy''+y'-\frac{1}{2\sqrt{y'-1}}*y'' 0=y''(x-\frac{1}{2\sqrt{y'-1}}) y''=0 x-\frac{1}{2\sqrt{y'-1}}=0 \int y'' dx = 0 \Leftrightarrow y'=C y=Cx-\sqrt{C-1} x-\frac{1}{2\sqrt{y'-1}}=0 \Leftrightarrow y'=\frac{1}{4x^2}+1 \int y'\, dx=\int \frac{1}{4x^2}+1 \, dx y = -\frac{1}{2x}+x+C","['integration', 'ordinary-differential-equations', 'derivatives']"
81,Product of derivations,Product of derivations,,"Let $A$ be an algebra over a field $K$ . If $D_1$ and $D_2$ are derivations of $A$ , show that $D_1 \circ D_2$ is not necessarily a derivation (it is if $D_1$ or $D_2 = 0$ ), but $D_1 \circ D_2−D_2 \circ D_1$ is always a derivation of $A$ . It's a problem from Tu, Loring W's An introduction to manifolds . Obviously, the composition of linear maps is linear, but how can I check the Leibniz rule? Meanwhile, why it defines to be $D_1 \circ D_2−D_2 \circ D_1$ ? Are there any motivations or backgrounds?","Let be an algebra over a field . If and are derivations of , show that is not necessarily a derivation (it is if or ), but is always a derivation of . It's a problem from Tu, Loring W's An introduction to manifolds . Obviously, the composition of linear maps is linear, but how can I check the Leibniz rule? Meanwhile, why it defines to be ? Are there any motivations or backgrounds?",A K D_1 D_2 A D_1 \circ D_2 D_1 D_2 = 0 D_1 \circ D_2−D_2 \circ D_1 A D_1 \circ D_2−D_2 \circ D_1,"['abstract-algebra', 'derivatives', 'differential-geometry', 'manifolds']"
82,"Let $F(x)=\int_0^x\left(t\int_1^tf(u)\,du\right)\,dt$. Show that $F''(x)$ is increasing on $(0, 1)$.",Let . Show that  is increasing on .,"F(x)=\int_0^x\left(t\int_1^tf(u)\,du\right)\,dt F''(x) (0, 1)","Question: Suppose that $f(x)$ is continuous and increasing on $[-1, 2]$ with $f(x)>0$ . Let $F(x)=\int_0^x\left(t\int_1^tf(u)\,du\right)\,dt$ . Show that $F''(x)$ is increasing on $(0, 1)$ . This is an exercise about the Fundamental Theorem of Calculus. Below is my attempt. My idea is to show that $F'''(x)\geq0$ for all $x\in(0, 1)$ . $$\begin{align}F(x)=\int_0^x\left(t\int_1^tf(u)\,du\right)\,dt&\implies F'(x)=x\int_1^xf(u)\,du\\ &\implies F''(x)=\int_1^xf(u)\,du+xf(x)\\ &\implies F'''(x)=2f(x)+xf'(x) \end{align}$$ Here I want to say that $f'(x)\geq0$ for all $x\in(0, 1)$ since $f(x)$ is increasing on $(0, 1)$ . However, I am not sure whether $f'(x)$ exists for all $x\in(0, 1)$ . The question only states that $f(x)$ is ""continuous"", not ""differentiable"". How should I continue? Thanks in advance.","Question: Suppose that is continuous and increasing on with . Let . Show that is increasing on . This is an exercise about the Fundamental Theorem of Calculus. Below is my attempt. My idea is to show that for all . Here I want to say that for all since is increasing on . However, I am not sure whether exists for all . The question only states that is ""continuous"", not ""differentiable"". How should I continue? Thanks in advance.","f(x) [-1, 2] f(x)>0 F(x)=\int_0^x\left(t\int_1^tf(u)\,du\right)\,dt F''(x) (0, 1) F'''(x)\geq0 x\in(0, 1) \begin{align}F(x)=\int_0^x\left(t\int_1^tf(u)\,du\right)\,dt&\implies F'(x)=x\int_1^xf(u)\,du\\
&\implies F''(x)=\int_1^xf(u)\,du+xf(x)\\
&\implies F'''(x)=2f(x)+xf'(x)
\end{align} f'(x)\geq0 x\in(0, 1) f(x) (0, 1) f'(x) x\in(0, 1) f(x)","['calculus', 'integration']"
83,How to solve the limit $\lim_{x \to \infty} \frac{e^{\log(x)^{c_1}}}{2^{c_2 x}}$,How to solve the limit,\lim_{x \to \infty} \frac{e^{\log(x)^{c_1}}}{2^{c_2 x}},"$$ \lim_{x \to \infty} \frac{e^{\log(x)^{c_1}}}{2^{c_2 x}} $$ I tried to use L'hopital rule by differentiating top and bottom, but it ended up getting more and more complicated. Here, $c_1$ and $c_2$ are constants.","I tried to use L'hopital rule by differentiating top and bottom, but it ended up getting more and more complicated. Here, and are constants.","
\lim_{x \to \infty} \frac{e^{\log(x)^{c_1}}}{2^{c_2 x}}
 c_1 c_2","['calculus', 'limits', 'derivatives']"
84,Does power rule holds for time functions of unit quaternions?,Does power rule holds for time functions of unit quaternions?,,"I'm trying to understand quaternion calculus. Let's suppose I have a time function of unit quaternion: $q(t) = (\cos \theta(t), (\sin \theta(t)) \mathbf{a}(t)), \mathbf{a}(t) \in \mathbb{R}^3$ , $||\mathbf{a}(t)|| = 1$ $\forall$ $t$ to the power $f(t) \in \mathbb{R}$ . Would someone clarify if $\dfrac{d\left(q(t)^{f(t)}\right)}{dt}$ $$= \left(−\sin(fθ) \dfrac{d(f\theta)}{dt}, \mathbf{a}\cos(fθ)\dfrac{d(f\theta)}{dt} + \dfrac{d(\mathbf{a})}{dt}\sin(fθ)\right)$$ or $$ = q(t)^{f(t)} \times \dfrac{d}{dt} \left[f(t) \ln q(t)\right] = q(t)^{f(t)} \times \dfrac{d}{dt} \left[f(t) \cdot (0, \theta(t) \mathbf{a}(t)) \right] \\ = (\cos (f\theta), (\sin (f\theta))\mathbf{a}) \times (0, f'\theta\mathbf{a} + f\theta'\mathbf{a} + f\theta\mathbf{a}') $$ or both are correct? (BTW, do we have a general formula for $\mathbf{a} \times \mathbf{a}'$ ?) Thanks a lot.","I'm trying to understand quaternion calculus. Let's suppose I have a time function of unit quaternion: , to the power . Would someone clarify if or or both are correct? (BTW, do we have a general formula for ?) Thanks a lot.","q(t) = (\cos \theta(t), (\sin \theta(t)) \mathbf{a}(t)), \mathbf{a}(t) \in \mathbb{R}^3 ||\mathbf{a}(t)|| = 1 \forall t f(t) \in \mathbb{R} \dfrac{d\left(q(t)^{f(t)}\right)}{dt} = \left(−\sin(fθ) \dfrac{d(f\theta)}{dt}, \mathbf{a}\cos(fθ)\dfrac{d(f\theta)}{dt} + \dfrac{d(\mathbf{a})}{dt}\sin(fθ)\right) 
= q(t)^{f(t)} \times \dfrac{d}{dt} \left[f(t) \ln q(t)\right] = q(t)^{f(t)} \times \dfrac{d}{dt} \left[f(t) \cdot (0, \theta(t) \mathbf{a}(t)) \right] \\
= (\cos (f\theta), (\sin (f\theta))\mathbf{a}) \times (0, f'\theta\mathbf{a} + f\theta'\mathbf{a} + f\theta\mathbf{a}')  \mathbf{a} \times \mathbf{a}'","['derivatives', 'quaternions']"
85,Substitution of variable in a second order differential equation [closed],Substitution of variable in a second order differential equation [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I am trying to make a substitution of a new variable into a second order differential equation. However, I am not sure how to do this, I have seen examples online but they do not seem to be applicable to my case. I would like to know the procedure followed in order to have my original equation be written in terms of t instead of r. Here is the equation: This is the new variable I would like the equation to be in terms of:","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I am trying to make a substitution of a new variable into a second order differential equation. However, I am not sure how to do this, I have seen examples online but they do not seem to be applicable to my case. I would like to know the procedure followed in order to have my original equation be written in terms of t instead of r. Here is the equation: This is the new variable I would like the equation to be in terms of:",,"['calculus', 'ordinary-differential-equations', 'derivatives']"
86,Which step in deriving the derivative of $sec(x)$ is wrong?,Which step in deriving the derivative of  is wrong?,sec(x),I can't see any errors on the steps. But Step 3 makes me doubt my answer.,I can't see any errors on the steps. But Step 3 makes me doubt my answer.,,"['calculus', 'derivatives']"
87,Prove that $f'(c)=(f(c)-f(a))/(c-a)$ isn't always correct,Prove that  isn't always correct,f'(c)=(f(c)-f(a))/(c-a),"I must find a function $f:[a,b]->R$ derived such that there is no c in $(a,b)$ for which: $$f'(c)=(f(c)-f(a))/(c-a)$$ My professor said that $f(x)=x^3$ will work but I think that he is wrong, because $f'(-0.5)$ proves that the claim is correct and not the opposite. can someone tell me who is wrong here and what example may work?","I must find a function derived such that there is no c in for which: My professor said that will work but I think that he is wrong, because proves that the claim is correct and not the opposite. can someone tell me who is wrong here and what example may work?","f:[a,b]->R (a,b) f'(c)=(f(c)-f(a))/(c-a) f(x)=x^3 f'(-0.5)","['calculus', 'derivatives']"
88,Derivative of a differentiable function,Derivative of a differentiable function,,"Given a function $f:[a,b] \rightarrow \mathbb{R}$ such that $f$ is differentiable in $(a,b)$ and continuous at $0$ and $1$ . Let $g:\mathbb{R} \rightarrow \mathbb{R}$ be the line which passes through $(a,f(a))$ and $(b,f(b))$ . I was wondering how to show that for every $s \notin [a,b]$ , there exists $t \in [a,b]$ such that the tangent line of $f(x)$ at the point $(t,f(t))$ passes throught $(s,g(s))$ ? (I was thinking of using Mean Value Theorem, but I failed to prove it by only using MVT.)","Given a function such that is differentiable in and continuous at and . Let be the line which passes through and . I was wondering how to show that for every , there exists such that the tangent line of at the point passes throught ? (I was thinking of using Mean Value Theorem, but I failed to prove it by only using MVT.)","f:[a,b] \rightarrow \mathbb{R} f (a,b) 0 1 g:\mathbb{R} \rightarrow \mathbb{R} (a,f(a)) (b,f(b)) s \notin [a,b] t \in [a,b] f(x) (t,f(t)) (s,g(s))","['real-analysis', 'derivatives', 'continuity']"
89,Black and Scholes d1 derivation,Black and Scholes d1 derivation,,"I viewed this derivation on the website and didn't understand the variable substitution made in order to find d1 in the calculus (line 6): \begin{align*} F(t,s) & = e^{-r(T-t)}\int^\infty_{-\infty} \max\left[se^z-K,0\right]f(z)\,dz \\ \,\, & = e^{-r(T-t)}\left(\int^{\ln \frac{K}{s}}_{-\infty} 0\cdot f(z)\,dz + \int^{\infty}_{\ln\frac{K}{s}} \left(se^z-K\right)\,f(z)\,dz\right) \\ \,\, & = e^{-r(T-t)}\int^{\infty}_{\ln\frac{K}{s}}\left(se^z-K\right)\,f(z)\,dz \\ \,\, & = e^{-r(T-t)} \left(s\int^{\infty}_{\ln\frac{K}{s}} e^zf(z)\,dz -K\int^{\infty}_{\ln\frac{K}{s}}f(z)\,dz \right) \\ \,\, & = e^{-r(T-t)} \left(s\int^{\infty}_{\ln\frac{K}{s}} e^zf(z)\,dz -K\int^{\infty}_{\ln\frac{K}{s}}f(z)\,dz \right) \\ \,\, & = \frac{e^{-r(T-t)}}{\sqrt{2\pi}} \left(s\int^{\infty}_{\ln\frac{K}{s}}  e^{\left(r-\frac{\sigma^2}{2}\right)\left(T-t\right) + \sigma\sqrt{T-t}y}e^{-\frac{y^2}{2}}\,dy -K\int^{\infty}_{\ln\frac{K}{s}}e^{-\frac{z^2}{2}}\,dz \right)  \\ \,\, & = \frac{e^{-r(T-t)}}{\sqrt{2\pi}} \left(s\int^{\infty}_{\ln\frac{K}{s}} e^{\left(r-\frac{\sigma^2}{2}\right)\left(T-t\right) + \sigma\sqrt{T-t}y-\frac{y^2}{2}}\,dy\right) -Ke^{-r(T-t)}\Phi\left(-\frac{\ln\frac{K}{s}- \left(r-\frac{\sigma^2}{2}\right)\left(T-t\right)}{\sigma\sqrt{T-t}}\right) \\ \,\, & = \frac{e^{-r(T-t)}}{\sqrt{2\pi}}  e^{\left(r-\frac{\sigma^2}{2}\right)\left(T-t\right)}\left(s\int^{\infty}_{\ln\frac{K}{s}} e^{\sigma\sqrt{T-t}y-\frac{y^2}{2}}\,dy\right) -Ke^{-r(T-t)}\Phi\left(-\frac{\ln\frac{K}{s}- \left(r-\frac{\sigma^2}{2}\right)\left(T-t\right)}{\sigma\sqrt{T-t}}\right) \\ \,\, & =    \frac{e^{-\frac{\sigma^2}{2}\left(T-t\right)}}{\sqrt{2\pi}}  \left(s\int^{\infty}_{\ln\frac{K}{s}} e^{-\frac{1}{2}\left(y^2-2\sigma\sqrt{T-t} y+\sigma^2\left(T-t\right)\right)}e^{\frac{1}{2}\sigma^2\left(T-t\right)}\,dy\right)-Ke^{-r(T-t)}\Phi\left(\frac{\ln\frac{s}{K}+\left(r-\frac{\sigma^2}{2}\right)\left(T-t\right)}{\sigma\sqrt{T-t}}\right) \\ \,\, & = \frac{e^{-\frac{\sigma^2}{2}\left(T-t\right)}}{\sqrt{2\pi}}   \left(s\int^{\infty}_{\ln\frac{K}{s}} e^{-\frac{1}{2}\left(y-\sigma\sqrt{T-t}\right)^2+\frac{1}{2}\sigma^2\left(T-t\right)}\,dz\right)-Ke^{-r(T-t)}\Phi\left(\frac{\ln\frac{s}{K}+\left(r-\frac{\sigma^2}{2}\right)\left(T-t\right)}{\sigma\sqrt{T-t}}\right) \\ \,\, & = \frac{e^{-\frac{\sigma^2}{2}\left(T-t\right)}e^{\frac{\sigma^2}{2}\left(T-t\right)}}{\sqrt{2\pi}}   \left(s\int^{\infty}_{\ln\frac{K}{s}} e^{-\frac{1}{2}\left(y-\sigma\sqrt{T-t}\right)^2}\,dz\right)-Ke^{-r(T-t)}\Phi\left(\frac{\ln\frac{s}{K}+\left(r-\frac{\sigma^2}{2}\right)\left(T-t\right)}{\sigma\sqrt{T-t}}\right) \\ \,\ & = \frac{1}{\sqrt{2\pi}}   \left(s\int^{\infty}_{\ln\frac{K}{s}} e^{-\frac{1}{2}\left(y-\sigma\sqrt{T-t}\right)^2}\,dz\right)-Ke^{-r(T-t)}\Phi\left(\frac{\ln\frac{s}{K}+\left(r-\frac{\sigma^2}{2}\right)\left(T-t\right)}{\sigma\sqrt{T-t}}\right) \\ \,\, & = s\Phi\left(-\frac{\ln\frac{K}{s}-\left(r-\frac{\sigma^2}{2}\right)\left(T-t\right)}{\sigma\sqrt{T-t}}+\sigma\sqrt{T-t}\right)-Ke^{-r(T-t)}\Phi\left(\frac{\ln\frac{s}{K}+\left(r-\frac{\sigma^2}{2}\right)\left(T-t\right)}{\sigma\sqrt{T-t}}\right) \\ \,\, & = s\Phi\left(\frac{\ln\frac{s}{K}+\left(r+\frac{\sigma^2}{2}\right)\left(T-t\right)}{\sigma\sqrt{T-t}}\right)-Ke^{-r(T-t)}\Phi\left(\frac{\ln\frac{s}{K}+\left(r-\frac{\sigma^2}{2}\right)\left(T-t\right)}{\sigma\sqrt{T-t}}\right) \end{align*} Could someone help me please",I viewed this derivation on the website and didn't understand the variable substitution made in order to find d1 in the calculus (line 6): Could someone help me please,"\begin{align*}
F(t,s) & = e^{-r(T-t)}\int^\infty_{-\infty} \max\left[se^z-K,0\right]f(z)\,dz \\
\,\, & = e^{-r(T-t)}\left(\int^{\ln \frac{K}{s}}_{-\infty} 0\cdot f(z)\,dz + \int^{\infty}_{\ln\frac{K}{s}} \left(se^z-K\right)\,f(z)\,dz\right) \\
\,\, & = e^{-r(T-t)}\int^{\infty}_{\ln\frac{K}{s}}\left(se^z-K\right)\,f(z)\,dz \\
\,\, & = e^{-r(T-t)} \left(s\int^{\infty}_{\ln\frac{K}{s}} e^zf(z)\,dz -K\int^{\infty}_{\ln\frac{K}{s}}f(z)\,dz \right) \\
\,\, & = e^{-r(T-t)} \left(s\int^{\infty}_{\ln\frac{K}{s}} e^zf(z)\,dz -K\int^{\infty}_{\ln\frac{K}{s}}f(z)\,dz \right) \\
\,\, & = \frac{e^{-r(T-t)}}{\sqrt{2\pi}} \left(s\int^{\infty}_{\ln\frac{K}{s}}  e^{\left(r-\frac{\sigma^2}{2}\right)\left(T-t\right) + \sigma\sqrt{T-t}y}e^{-\frac{y^2}{2}}\,dy -K\int^{\infty}_{\ln\frac{K}{s}}e^{-\frac{z^2}{2}}\,dz \right)  \\
\,\, & = \frac{e^{-r(T-t)}}{\sqrt{2\pi}} \left(s\int^{\infty}_{\ln\frac{K}{s}} e^{\left(r-\frac{\sigma^2}{2}\right)\left(T-t\right) + \sigma\sqrt{T-t}y-\frac{y^2}{2}}\,dy\right) -Ke^{-r(T-t)}\Phi\left(-\frac{\ln\frac{K}{s}- \left(r-\frac{\sigma^2}{2}\right)\left(T-t\right)}{\sigma\sqrt{T-t}}\right) \\
\,\, & = \frac{e^{-r(T-t)}}{\sqrt{2\pi}}  e^{\left(r-\frac{\sigma^2}{2}\right)\left(T-t\right)}\left(s\int^{\infty}_{\ln\frac{K}{s}} e^{\sigma\sqrt{T-t}y-\frac{y^2}{2}}\,dy\right) -Ke^{-r(T-t)}\Phi\left(-\frac{\ln\frac{K}{s}- \left(r-\frac{\sigma^2}{2}\right)\left(T-t\right)}{\sigma\sqrt{T-t}}\right) \\
\,\, & =    \frac{e^{-\frac{\sigma^2}{2}\left(T-t\right)}}{\sqrt{2\pi}}  \left(s\int^{\infty}_{\ln\frac{K}{s}} e^{-\frac{1}{2}\left(y^2-2\sigma\sqrt{T-t} y+\sigma^2\left(T-t\right)\right)}e^{\frac{1}{2}\sigma^2\left(T-t\right)}\,dy\right)-Ke^{-r(T-t)}\Phi\left(\frac{\ln\frac{s}{K}+\left(r-\frac{\sigma^2}{2}\right)\left(T-t\right)}{\sigma\sqrt{T-t}}\right) \\
\,\, & = \frac{e^{-\frac{\sigma^2}{2}\left(T-t\right)}}{\sqrt{2\pi}}   \left(s\int^{\infty}_{\ln\frac{K}{s}} e^{-\frac{1}{2}\left(y-\sigma\sqrt{T-t}\right)^2+\frac{1}{2}\sigma^2\left(T-t\right)}\,dz\right)-Ke^{-r(T-t)}\Phi\left(\frac{\ln\frac{s}{K}+\left(r-\frac{\sigma^2}{2}\right)\left(T-t\right)}{\sigma\sqrt{T-t}}\right) \\
\,\, & = \frac{e^{-\frac{\sigma^2}{2}\left(T-t\right)}e^{\frac{\sigma^2}{2}\left(T-t\right)}}{\sqrt{2\pi}}   \left(s\int^{\infty}_{\ln\frac{K}{s}} e^{-\frac{1}{2}\left(y-\sigma\sqrt{T-t}\right)^2}\,dz\right)-Ke^{-r(T-t)}\Phi\left(\frac{\ln\frac{s}{K}+\left(r-\frac{\sigma^2}{2}\right)\left(T-t\right)}{\sigma\sqrt{T-t}}\right) \\
\,\ & = \frac{1}{\sqrt{2\pi}}   \left(s\int^{\infty}_{\ln\frac{K}{s}} e^{-\frac{1}{2}\left(y-\sigma\sqrt{T-t}\right)^2}\,dz\right)-Ke^{-r(T-t)}\Phi\left(\frac{\ln\frac{s}{K}+\left(r-\frac{\sigma^2}{2}\right)\left(T-t\right)}{\sigma\sqrt{T-t}}\right) \\
\,\, & = s\Phi\left(-\frac{\ln\frac{K}{s}-\left(r-\frac{\sigma^2}{2}\right)\left(T-t\right)}{\sigma\sqrt{T-t}}+\sigma\sqrt{T-t}\right)-Ke^{-r(T-t)}\Phi\left(\frac{\ln\frac{s}{K}+\left(r-\frac{\sigma^2}{2}\right)\left(T-t\right)}{\sigma\sqrt{T-t}}\right) \\
\,\, & = s\Phi\left(\frac{\ln\frac{s}{K}+\left(r+\frac{\sigma^2}{2}\right)\left(T-t\right)}{\sigma\sqrt{T-t}}\right)-Ke^{-r(T-t)}\Phi\left(\frac{\ln\frac{s}{K}+\left(r-\frac{\sigma^2}{2}\right)\left(T-t\right)}{\sigma\sqrt{T-t}}\right)
\end{align*}","['integration', 'derivatives', 'finance']"
90,How can I find the value of the derivative at a certain value given the graph?,How can I find the value of the derivative at a certain value given the graph?,,"I need to find the values of $$\lim_{h\rightarrow 0^+} \frac{f(2+h)-f(2)}{h}$$ I know that the value of $f(2)=-1$ I think I need to come up with an equation for the graph, which I can't come up with. I thought it was $|x-2|-1$ but that isn't it. How can I solve this?","I need to find the values of I know that the value of I think I need to come up with an equation for the graph, which I can't come up with. I thought it was but that isn't it. How can I solve this?",\lim_{h\rightarrow 0^+} \frac{f(2+h)-f(2)}{h} f(2)=-1 |x-2|-1,"['calculus', 'derivatives']"
91,Uniform bound for derivatives of holomorphic function on compact set,Uniform bound for derivatives of holomorphic function on compact set,,"I am trying to prove the following statement. Given $U \subseteq \mathbb{C}$ open, a compact set $K \subset U$ , and $j \in \mathbb{N}$ , show that there exists a constant $C > 0$ such that for any holomorphic $f : U \to \mathbb{C}$ and $z \in K$ we have \begin{equation*} 	\lvert f^{(j)}(z) \rvert \leq C \sup_{w \in U} \lvert f(w) \rvert. \end{equation*} Here is my attempt. In what follows, $D(P,r)$ and $\overline{D(P, r)}$ will denote the open and closed disks of radius $r$ centered at $P$ , respectively. Proof attempt. Let $z \in K \subset U$ . Since $U$ is open, we can find $r_z > 0$ such that $\overline{D(z, r_z)} \subseteq U$ . Cover $K$ with $\{D(z, r_z) \mid z \in K\}$ ; since $K$ is compact, we can find finitely many $z_i \in K$ such that $\{D(z_i, r_i) \mid 1 \leq i \leq n, r_i := r_{z_i}\}$ covers $K$ . Now let $f : U \to \mathbb{C}$ be holomorphic and let $z \in K$ . Then $z \in D(z_i, r_i)$ for some $1 \leq i \leq n$ , so by Cauchy's integral formula and a bound on the path integral we have \begin{equation*} 	\lvert f^{(j)}(z) \vert = \left \vert \frac{j!}{2 \pi i} \oint_{\partial D(z_i, r_i)} \frac{f(w) \ dw}{(w - z)^{j + 1}} \right \vert \leq \frac{j!}{2 \pi} \sup_{w \in \partial D(z_i, r_i)} \left \vert \frac{f(w)}{(w - z)^{j+1}} \right \vert \cdot 2 \pi r_i. \end{equation*} We get \begin{equation*} 	\left \vert w - z \right \vert = \vert (w - z_i) - (z - z_i) \vert \geq \vert \vert w - z_i \vert - \vert z_i - z \vert \vert = \vert r_i - \vert z_i - z \vert \vert \end{equation*} for $w \in \partial D(z_i, r_i)$ , so \begin{equation*} 	\vert f^{(j)}(z) \vert \leq r_i \cdot j! \sup_{w \in \partial D(z_i, r_i)} \left \vert \frac{f(w)}{(w - z)^{j+1}} \right \vert \leq r_i j! \frac{\sup_{w \in \partial D(z_i, r_i)} \left \vert f(w) \right \vert}{\vert r_i - \vert z_i - z \vert \vert^{j+1}} \leq \frac{j!}{r_i^j} \frac{1}{\left \vert 1 - \frac{\left \vert z_i - z\right \vert}{r_i}\right \vert^{j+1}} \sup_{w \in U} \vert f(w) \vert. \end{equation*} This is as far as I could go. The triangle inequality and the bound I have on $z_i - z$ work against me: I can bound it above by $r_i$ and below by $0$ , and both are useless with the reciprocal. If I could get a bound $M_{i,j}$ for \begin{equation*} 	\frac{1}{\left \vert 1 - \frac{\left \vert z_i - z \right \vert}{r_i}\right \vert^{j+1}} \end{equation*} that did not depend on $z$ then I could take $C = \max \{M_{i,j} \cdot j!/r_i^j \mid 1 \leq i \leq n\}$ and the statement would follow. However, I see no way of getting a useful bound. Any suggestion or hint would be greatly appreciated.","I am trying to prove the following statement. Given open, a compact set , and , show that there exists a constant such that for any holomorphic and we have Here is my attempt. In what follows, and will denote the open and closed disks of radius centered at , respectively. Proof attempt. Let . Since is open, we can find such that . Cover with ; since is compact, we can find finitely many such that covers . Now let be holomorphic and let . Then for some , so by Cauchy's integral formula and a bound on the path integral we have We get for , so This is as far as I could go. The triangle inequality and the bound I have on work against me: I can bound it above by and below by , and both are useless with the reciprocal. If I could get a bound for that did not depend on then I could take and the statement would follow. However, I see no way of getting a useful bound. Any suggestion or hint would be greatly appreciated.","U \subseteq \mathbb{C} K \subset U j \in \mathbb{N} C > 0 f : U \to \mathbb{C} z \in K \begin{equation*}
	\lvert f^{(j)}(z) \rvert \leq C \sup_{w \in U} \lvert f(w) \rvert.
\end{equation*} D(P,r) \overline{D(P, r)} r P z \in K \subset U U r_z > 0 \overline{D(z, r_z)} \subseteq U K \{D(z, r_z) \mid z \in K\} K z_i \in K \{D(z_i, r_i) \mid 1 \leq i \leq n, r_i := r_{z_i}\} K f : U \to \mathbb{C} z \in K z \in D(z_i, r_i) 1 \leq i \leq n \begin{equation*}
	\lvert f^{(j)}(z) \vert = \left \vert \frac{j!}{2 \pi i} \oint_{\partial D(z_i, r_i)} \frac{f(w) \ dw}{(w - z)^{j + 1}} \right \vert \leq \frac{j!}{2 \pi} \sup_{w \in \partial D(z_i, r_i)} \left \vert \frac{f(w)}{(w - z)^{j+1}} \right \vert \cdot 2 \pi r_i.
\end{equation*} \begin{equation*}
	\left \vert w - z \right \vert = \vert (w - z_i) - (z - z_i) \vert \geq \vert \vert w - z_i \vert - \vert z_i - z \vert \vert = \vert r_i - \vert z_i - z \vert \vert
\end{equation*} w \in \partial D(z_i, r_i) \begin{equation*}
	\vert f^{(j)}(z) \vert \leq r_i \cdot j! \sup_{w \in \partial D(z_i, r_i)} \left \vert \frac{f(w)}{(w - z)^{j+1}} \right \vert \leq r_i j! \frac{\sup_{w \in \partial D(z_i, r_i)} \left \vert f(w) \right \vert}{\vert r_i - \vert z_i - z \vert \vert^{j+1}} \leq \frac{j!}{r_i^j} \frac{1}{\left \vert 1 - \frac{\left \vert z_i - z\right \vert}{r_i}\right \vert^{j+1}} \sup_{w \in U} \vert f(w) \vert.
\end{equation*} z_i - z r_i 0 M_{i,j} \begin{equation*}
	\frac{1}{\left \vert 1 - \frac{\left \vert z_i - z \right \vert}{r_i}\right \vert^{j+1}}
\end{equation*} z C = \max \{M_{i,j} \cdot j!/r_i^j \mid 1 \leq i \leq n\}","['complex-analysis', 'derivatives', 'upper-lower-bounds', 'cauchy-integral-formula']"
92,Partial derivatives of a complex function?,Partial derivatives of a complex function?,,"In real algebra, if I have a differentiable function $f:\mathbb{R}^2 \rightarrow \mathbb{R}^2$ , say $f(x,y)=[u,v]$ , I can calculate four different partial derivatives $\frac{\partial u}{\partial x},\frac{\partial u}{\partial y},\frac{\partial v}{\partial x},\frac{\partial v}{\partial y}$ . If I interpret $x$ and $y$ as spatial coordinates, I could  - for example - create two separate quiver plots : one illustrating the partial derivatives of $u$ and one illustrating the partial derivatives of $v$ . In complex algebra, it seems like we should encounter a similar behaviour: Assume that I have $z=x+iy$ and $w=u+iv$ , and that my function $f:\mathbb{C}\rightarrow \mathbb{C}$ is (say) a Mobius transformation $$M(z)=\frac{az+b}{cz+d}$$ whose derivative is: $$\frac{\partial w}{\partial z}=\frac{\partial M(z)}{\partial z}=\frac{ad-bc}{(cz+d)^2}$$ Now here is my question: The derivative I obtain will be a single complex number. Is it possible to extract the partial derivatives $\frac{\partial u}{\partial x},\frac{\partial u}{\partial y},\frac{\partial v}{\partial x},\frac{\partial v}{\partial y}$ ? If yes, how would I go about this?","In real algebra, if I have a differentiable function , say , I can calculate four different partial derivatives . If I interpret and as spatial coordinates, I could  - for example - create two separate quiver plots : one illustrating the partial derivatives of and one illustrating the partial derivatives of . In complex algebra, it seems like we should encounter a similar behaviour: Assume that I have and , and that my function is (say) a Mobius transformation whose derivative is: Now here is my question: The derivative I obtain will be a single complex number. Is it possible to extract the partial derivatives ? If yes, how would I go about this?","f:\mathbb{R}^2 \rightarrow \mathbb{R}^2 f(x,y)=[u,v] \frac{\partial u}{\partial x},\frac{\partial u}{\partial y},\frac{\partial v}{\partial x},\frac{\partial v}{\partial y} x y u v z=x+iy w=u+iv f:\mathbb{C}\rightarrow \mathbb{C} M(z)=\frac{az+b}{cz+d} \frac{\partial w}{\partial z}=\frac{\partial M(z)}{\partial z}=\frac{ad-bc}{(cz+d)^2} \frac{\partial u}{\partial x},\frac{\partial u}{\partial y},\frac{\partial v}{\partial x},\frac{\partial v}{\partial y}","['complex-analysis', 'derivatives', 'partial-derivative', 'mobius-transformation']"
93,Are endpoints critical points?,Are endpoints critical points?,,"In the function $f(x)=\max\{\sin (x),\cos (x)\}$ for all $x$ belonging to $(0,2π)$ , can we count end points of the domain as critical points, since a function is not differentiable at endpoints?","In the function for all belonging to , can we count end points of the domain as critical points, since a function is not differentiable at endpoints?","f(x)=\max\{\sin (x),\cos (x)\} x (0,2π)","['calculus', 'derivatives', 'maxima-minima', 'domain-theory']"
94,"Find a function $g$ that makes $f$ continuous, but not differentiable","Find a function  that makes  continuous, but not differentiable",g f,"Consider the function $f:[-1,1] \to \mathbb{R}$ defined piecewise by \begin{align*}f(x) = \begin{cases}0: -1 \leq x < 0 \\ g(x): 0\leq x \leq 1 \end{cases}\end{align*} where $g:[0,1] \to \mathbb{R}$ is some function. Suppose $g$ is a non-zero function given by $$g(x) = a_0 + a_1x + a_2x^2 + a_3x^3.$$ Find non-negative real numbers $a_0, a_1, a_2, a_3$ (not all of which are 0) such that $f$ is continuous but not differentiable. I've been able to find some examples of general continuous, but not differentiable functions, but I have not been able to find one that fits this specific example AND to supply non-negative real numbers that makes it true. My problem is I can't preserve continuity at $f(0)$ but make $f(x)$ non-differentiable given the restriction of the $a_i$ 's.","Consider the function defined piecewise by where is some function. Suppose is a non-zero function given by Find non-negative real numbers (not all of which are 0) such that is continuous but not differentiable. I've been able to find some examples of general continuous, but not differentiable functions, but I have not been able to find one that fits this specific example AND to supply non-negative real numbers that makes it true. My problem is I can't preserve continuity at but make non-differentiable given the restriction of the 's.","f:[-1,1] \to \mathbb{R} \begin{align*}f(x) = \begin{cases}0: -1 \leq x < 0 \\ g(x): 0\leq x \leq 1 \end{cases}\end{align*} g:[0,1] \to \mathbb{R} g g(x) = a_0 + a_1x + a_2x^2 + a_3x^3. a_0, a_1, a_2, a_3 f f(0) f(x) a_i","['real-analysis', 'derivatives', 'continuity', 'piecewise-continuity']"
95,"If $f(x)$ is differentiable over $(a,b)$, does that mean $f'(x)$ is continuous over $(a,b)$? [duplicate]","If  is differentiable over , does that mean  is continuous over ? [duplicate]","f(x) (a,b) f'(x) (a,b)","This question already has answers here : Discontinuous derivative. [duplicate] (2 answers) Closed 3 years ago . I was drawing smooth curves through points $(a,f(a))$ and $(b,f(b))$ in an attempt to come up with a proof of the Mean Value Theorem. As I was doing that, I noticed that I couldn't draw the curve in such a way that its derivative would be discontinous. I tried to think up a function that is differentiable but whose derivative is discontinous but I got nothing. I did a quick Google search for the same question that I've asked here but all I could find was that differentiability of a function implies continuity of the same which I already know. If $f(x)$ is differentiable over $(a,b)$ , does that mean $f'(x)$ is continuous over $(a,b)$ ? If not, what are some examples of functions where this is not the case?","This question already has answers here : Discontinuous derivative. [duplicate] (2 answers) Closed 3 years ago . I was drawing smooth curves through points and in an attempt to come up with a proof of the Mean Value Theorem. As I was doing that, I noticed that I couldn't draw the curve in such a way that its derivative would be discontinous. I tried to think up a function that is differentiable but whose derivative is discontinous but I got nothing. I did a quick Google search for the same question that I've asked here but all I could find was that differentiability of a function implies continuity of the same which I already know. If is differentiable over , does that mean is continuous over ? If not, what are some examples of functions where this is not the case?","(a,f(a)) (b,f(b)) f(x) (a,b) f'(x) (a,b)","['calculus', 'derivatives', 'continuity']"
96,How to calculate an integral containing a nonlinear function of a derivative?,How to calculate an integral containing a nonlinear function of a derivative?,,"Consider the integral $I_1=\displaystyle \int_0^\tau \frac{dx(t)}{dt} dt$ that contains the derivative $\dot{x}$ . Because $\dot{x}$ is not inside a non linear expression, one calculates that it equals $\displaystyle \int_{x(0)}^{x(\tau)} dx(t) = x(\tau)-x(0)$ . But how can one calculate an integral such as $I_2=\displaystyle \int_0^\tau \sqrt{\frac{dx(t)}{dt}} dt$ ? I tried partial integration, but this does not work.","Consider the integral that contains the derivative . Because is not inside a non linear expression, one calculates that it equals . But how can one calculate an integral such as ? I tried partial integration, but this does not work.",I_1=\displaystyle \int_0^\tau \frac{dx(t)}{dt} dt \dot{x} \dot{x} \displaystyle \int_{x(0)}^{x(\tau)} dx(t) = x(\tau)-x(0) I_2=\displaystyle \int_0^\tau \sqrt{\frac{dx(t)}{dt}} dt,"['integration', 'derivatives']"
97,Interpreting the meaning of change in a variable,Interpreting the meaning of change in a variable,,"If $z$ is a function depending on some other variables, let's say $x$ and $y$ , I have learnt that we can write $$\delta z=\frac{\partial{z}}{\partial{x}}\delta{x}+\frac{\partial{z}}{\partial{y}}\delta{y}$$ Or if a function simply depends on a single variable, for example if $y$ depends on $x$ , then we can write $$\delta y=\frac{\mathrm{d}y}{\mathrm{d}x}\delta x$$ I want to know why it can be written like this. I know that $\delta y$ represents some finite change in $y$ and $\mathrm{d}y$ also represents infinitesimal change in $y$ , but how are they related is confusing me. Any help would be appreciated!","If is a function depending on some other variables, let's say and , I have learnt that we can write Or if a function simply depends on a single variable, for example if depends on , then we can write I want to know why it can be written like this. I know that represents some finite change in and also represents infinitesimal change in , but how are they related is confusing me. Any help would be appreciated!",z x y \delta z=\frac{\partial{z}}{\partial{x}}\delta{x}+\frac{\partial{z}}{\partial{y}}\delta{y} y x \delta y=\frac{\mathrm{d}y}{\mathrm{d}x}\delta x \delta y y \mathrm{d}y y,"['calculus', 'derivatives', 'partial-derivative', 'change-of-variable']"
98,"If $P(x)$ is a polynomial of degree three in $x$, and $y^2 = P(x)$, show that $\frac{D(y^3D^2y)}{y^2}$ is constant","If  is a polynomial of degree three in , and , show that  is constant",P(x) x y^2 = P(x) \frac{D(y^3D^2y)}{y^2},"If $P(x)$ is a polynomial of degree three in $x$ , and $y^2 = P(x)$ , show that $$\frac{D(y^3D^2y)}{y^2}$$ is a constant, where D denotes the derivative operator. I have tried expressing the expression above in terms of $P$ and its derivatives (in the hopes of showing the derivative is $0$ ) but I couldn't manage to do that.","If is a polynomial of degree three in , and , show that is a constant, where D denotes the derivative operator. I have tried expressing the expression above in terms of and its derivatives (in the hopes of showing the derivative is ) but I couldn't manage to do that.",P(x) x y^2 = P(x) \frac{D(y^3D^2y)}{y^2} P 0,['ordinary-differential-equations']
99,A twice differentiable function satisfying a differential equation,A twice differentiable function satisfying a differential equation,,"The question is : Let $f:\mathbb{R} \to \mathbb{R}$ be twice differentiable function satisfying $f(x)+f''(x)=-xg(x)f'(x), x\in \mathbb{R} $ where $g(x) \ge 0, \forall x\in \mathbb{R}$ Which of the following is\are true? $(1)$ If $f(0)=f'(0)=1$ , then $f(3)\lt 3$ $(2)$ If $f(0)=f'(0)=2$ , then $f(4)\lt 4$ $(3)$ If $f(0)=f'(0)=3$ , then $f(3)=5$ $(4)$ If $f(0)=f'(0)=3$ , then $f(3)=6$ My thoughts:- I will first discuss about $(3)$ and $(4)$ Let $g(x)=0$ Then with some computation , we can show $f(x)=3(\sin x+\cos x)$ as a suitable candidate to discard $(3)$ and $(4)$ Here , for option $(3)$ $f(3)=5$ $\Rightarrow \sin 3+\cos 3=\frac 53$ On squaring both sides $1+\sin 6=\frac{25}9$ $\sin 6=\frac {16}9 \gt 1$ , a contradiction Similarly $f(3)= 6$ will give the contradiction $\sin 3+\cos 3=2$ ( implying $\sin 3=\cos 3=1$ which is an impossibility) . Thus we are left with $(1)$ and $(2)$ Note:A slight variant of the above example satisfies the condition in $(1)$ and $(2)$ I tried with simple examples like $g(x)=1 $ and $f(x)=x$ or like quadratics but couldn't reach conclusions . Please help with the options $(1)$ and $(2)$ . Thanks for your time.","The question is : Let be twice differentiable function satisfying where Which of the following is\are true? If , then If , then If , then If , then My thoughts:- I will first discuss about and Let Then with some computation , we can show as a suitable candidate to discard and Here , for option On squaring both sides , a contradiction Similarly will give the contradiction ( implying which is an impossibility) . Thus we are left with and Note:A slight variant of the above example satisfies the condition in and I tried with simple examples like and or like quadratics but couldn't reach conclusions . Please help with the options and . Thanks for your time.","f:\mathbb{R} \to \mathbb{R} f(x)+f''(x)=-xg(x)f'(x), x\in \mathbb{R}  g(x) \ge 0, \forall x\in \mathbb{R} (1) f(0)=f'(0)=1 f(3)\lt 3 (2) f(0)=f'(0)=2 f(4)\lt 4 (3) f(0)=f'(0)=3 f(3)=5 (4) f(0)=f'(0)=3 f(3)=6 (3) (4) g(x)=0 f(x)=3(\sin x+\cos x) (3) (4) (3) f(3)=5 \Rightarrow \sin 3+\cos 3=\frac 53 1+\sin 6=\frac{25}9 \sin 6=\frac {16}9 \gt 1 f(3)= 6 \sin 3+\cos 3=2 \sin 3=\cos 3=1 (1) (2) (1) (2) g(x)=1  f(x)=x (1) (2)","['real-analysis', 'ordinary-differential-equations', 'derivatives', 'solution-verification', 'sturm-liouville']"
