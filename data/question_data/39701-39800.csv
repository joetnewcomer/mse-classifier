,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Left and right ideals of $R=\left\{\bigl(\begin{smallmatrix}a&b\\0&c \end{smallmatrix}\bigr) : a\in\mathbb Z, \ b,c\in\mathbb Q\right\}$",Left and right ideals of,"R=\left\{\bigl(\begin{smallmatrix}a&b\\0&c \end{smallmatrix}\bigr) : a\in\mathbb Z, \ b,c\in\mathbb Q\right\}","If $$R=\left\{ \begin{pmatrix} a &b\\ 0 & c \end{pmatrix} \ : \ a \in \mathbb{Z}, \ b,c \in \mathbb{Q}\right\} $$ under usual addition and multiplication, then what are the left and right ideals of $R$?","If $$R=\left\{ \begin{pmatrix} a &b\\ 0 & c \end{pmatrix} \ : \ a \in \mathbb{Z}, \ b,c \in \mathbb{Q}\right\} $$ under usual addition and multiplication, then what are the left and right ideals of $R$?",,"['abstract-algebra', 'ring-theory', 'examples-counterexamples']"
1,Impact of Riemannian Geometry on Group Theory,Impact of Riemannian Geometry on Group Theory,,"In the Wikipedia page for Riemannian Geometry , it mentions that the field had made a ""profound impact on group theory"". What are some examples of this? Looking around a bit (including on that page), it seems more like it is the other way around (i.e. group theory informs Riemannian manifold theory). E.g. analyzing a manifold with its fundamental group, or with Lie theory. (But perhaps these can be viewed inversely.)","In the Wikipedia page for Riemannian Geometry , it mentions that the field had made a ""profound impact on group theory"". What are some examples of this? Looking around a bit (including on that page), it seems more like it is the other way around (i.e. group theory informs Riemannian manifold theory). E.g. analyzing a manifold with its fundamental group, or with Lie theory. (But perhaps these can be viewed inversely.)",,"['abstract-algebra', 'group-theory', 'differential-geometry', 'manifolds', 'riemannian-geometry']"
2,Fibonacci cycles for finite groups,Fibonacci cycles for finite groups,,"For two elements $A$ and $B$ of a finite group $G$ of order $n$, we may define the Fibonacci cycle $Fib(A,B)$ as the sequence $$Fib(A,B) =\{A,B,BA,BAB,BABBA,BABBABAB, \dots \}$$ Here, starting from the third element, each element in the sequence is obtained by applying the group operation to the preceding two elements. If $A$ and $B$ are both selected to be the identity element $E$, we get a trivial sequence with period 1: $Fib(E,E) =\{E,E,E, \dots \}$. Any other choice for $A$ and $B$ will lead to a Fibonacci cycle with larger period. For instance, if $G$ is the cyclic group of order 3, with elements $E$, $L$ and $R$, a period 8 Fibonacci sequence results: $$Fib(E,L) = \{E,L,L,R,E,R,R,L,E,L,\dots \}$$ For groups of order 3 it is not possible to create larger cycles. This follows from the fact that the periods of all non-trivial cycles for a given group of order $n$ add up to $n^2-1$. The question now arises: what is the maximum period $\tau_n$ obtainable for finite groups of order $n$?","For two elements $A$ and $B$ of a finite group $G$ of order $n$, we may define the Fibonacci cycle $Fib(A,B)$ as the sequence $$Fib(A,B) =\{A,B,BA,BAB,BABBA,BABBABAB, \dots \}$$ Here, starting from the third element, each element in the sequence is obtained by applying the group operation to the preceding two elements. If $A$ and $B$ are both selected to be the identity element $E$, we get a trivial sequence with period 1: $Fib(E,E) =\{E,E,E, \dots \}$. Any other choice for $A$ and $B$ will lead to a Fibonacci cycle with larger period. For instance, if $G$ is the cyclic group of order 3, with elements $E$, $L$ and $R$, a period 8 Fibonacci sequence results: $$Fib(E,L) = \{E,L,L,R,E,R,R,L,E,L,\dots \}$$ For groups of order 3 it is not possible to create larger cycles. This follows from the fact that the periods of all non-trivial cycles for a given group of order $n$ add up to $n^2-1$. The question now arises: what is the maximum period $\tau_n$ obtainable for finite groups of order $n$?",,"['abstract-algebra', 'combinatorics', 'group-theory', 'elementary-number-theory', 'finite-groups']"
3,Finding Symmetry Group $S_3$ in a function,Finding Symmetry Group  in a function,S_3,"I was considering functions $f: \Bbb{C} \rightarrow \Bbb{C}$ and I defined the following instrument (I call it the Symmetry Group of a function) $$ \text{Sym}(f) = \left< m(x)|f(m(x))=f(x) \right> $$ An intuitive example is to consider $\text{Sym}(e^x)$ and observe that $$m(x) = x + 2i \pi $$  has the property that $$ e^{m(x)} = e^{x+2i\pi}=e^x e^{2i \pi} = e^x $$ And the group generated by $m(x)$ under composition is the set of functions $$ x + 2i\pi k, k \in \Bbb{Z}$$ Which is isomorphic to $\Bbb{Z}$ under function composition. So one can then say that $$\text{Sym}(e^x) \cong \Bbb{Z}$$ What I was curious about was if there are any elementary functions such that $$ \text{Sym}(g(x)) \cong S_3$$ In attempt to build one I considered $$ g(x) = x^{\frac{-1 + i \sqrt{3}}{2}} +  x^{\left( {\frac{-1 + i \sqrt{3}}{2}}\right)^2} + x + \frac{1}{x} +x^{-\frac{-1 + i \sqrt{3}}{2}}+  x^{-\left({\frac{-1 + i \sqrt{3}}{2}}\right)^2} $$ G has as a generator for its symmetries the functions $L_1 = \frac{1}{x}$ and $L_2 =  x^{\frac{-1 + i \sqrt{3}}{2}}$ Which can be observed as $$G(L_1) = G(L_2) = G(x)$$ But the problem is that $L_1(L_2) = L_2(L_1)$ so clearly this isn't a generating set for $S_3$. It's not obvious at this point, how to go about making a function that has $S_3$ as its underlying symmetry group Some Examples: $\Bbb{Z}_2$ can be realized as $\text{Sym}\left(x + \frac{1}{x}\right)$ as this function is invariant under the substitutions $x \rightarrow x$ and $x\rightarrow \frac{1}{x}$ The proof arises from the following: Suppose we wish to find all transformations $T$ $x$ such that $$ x + \frac{1}{x} = T(x) + \frac{1}{T(x)}$$ We can the derive that $$ T(x)^2 - \left(x+ \frac{1}{x}\right)T(x) + 1 = 0$$ Which yields that $$ T(x) = \frac{x + \frac{1}{x} \pm \sqrt{x^2+2+\frac{2}{x^2}-4}}{2}$$ simplifying to $$ T(x)  = \frac{x + \frac{1}{x} \pm (x-\frac{1}{x})}{2}$$ and that gives $T(x) = x, T(x) = \frac{1}{x}$ observe the these transformations form a group of order $2$ so they must be isomorphic to $\Bbb{Z}_2$ And in general I conjecture that: $\Bbb{Z}_n$ can be realized as $$\text{Sym} \left( x + x^{\sqrt[n]{1}_1} + x^{\sqrt[n]{1}_2} + ... x^{\sqrt[n]{1}_{n-1}}\right)$$","I was considering functions $f: \Bbb{C} \rightarrow \Bbb{C}$ and I defined the following instrument (I call it the Symmetry Group of a function) $$ \text{Sym}(f) = \left< m(x)|f(m(x))=f(x) \right> $$ An intuitive example is to consider $\text{Sym}(e^x)$ and observe that $$m(x) = x + 2i \pi $$  has the property that $$ e^{m(x)} = e^{x+2i\pi}=e^x e^{2i \pi} = e^x $$ And the group generated by $m(x)$ under composition is the set of functions $$ x + 2i\pi k, k \in \Bbb{Z}$$ Which is isomorphic to $\Bbb{Z}$ under function composition. So one can then say that $$\text{Sym}(e^x) \cong \Bbb{Z}$$ What I was curious about was if there are any elementary functions such that $$ \text{Sym}(g(x)) \cong S_3$$ In attempt to build one I considered $$ g(x) = x^{\frac{-1 + i \sqrt{3}}{2}} +  x^{\left( {\frac{-1 + i \sqrt{3}}{2}}\right)^2} + x + \frac{1}{x} +x^{-\frac{-1 + i \sqrt{3}}{2}}+  x^{-\left({\frac{-1 + i \sqrt{3}}{2}}\right)^2} $$ G has as a generator for its symmetries the functions $L_1 = \frac{1}{x}$ and $L_2 =  x^{\frac{-1 + i \sqrt{3}}{2}}$ Which can be observed as $$G(L_1) = G(L_2) = G(x)$$ But the problem is that $L_1(L_2) = L_2(L_1)$ so clearly this isn't a generating set for $S_3$. It's not obvious at this point, how to go about making a function that has $S_3$ as its underlying symmetry group Some Examples: $\Bbb{Z}_2$ can be realized as $\text{Sym}\left(x + \frac{1}{x}\right)$ as this function is invariant under the substitutions $x \rightarrow x$ and $x\rightarrow \frac{1}{x}$ The proof arises from the following: Suppose we wish to find all transformations $T$ $x$ such that $$ x + \frac{1}{x} = T(x) + \frac{1}{T(x)}$$ We can the derive that $$ T(x)^2 - \left(x+ \frac{1}{x}\right)T(x) + 1 = 0$$ Which yields that $$ T(x) = \frac{x + \frac{1}{x} \pm \sqrt{x^2+2+\frac{2}{x^2}-4}}{2}$$ simplifying to $$ T(x)  = \frac{x + \frac{1}{x} \pm (x-\frac{1}{x})}{2}$$ and that gives $T(x) = x, T(x) = \frac{1}{x}$ observe the these transformations form a group of order $2$ so they must be isomorphic to $\Bbb{Z}_2$ And in general I conjecture that: $\Bbb{Z}_n$ can be realized as $$\text{Sym} \left( x + x^{\sqrt[n]{1}_1} + x^{\sqrt[n]{1}_2} + ... x^{\sqrt[n]{1}_{n-1}}\right)$$",,"['abstract-algebra', 'group-theory', 'complex-analysis', 'finite-groups', 'functional-equations']"
4,Examples for Burnside problem.,Examples for Burnside problem.,,"What are some examples for Burnside Problem- example of an infinite finitely generated torsion group - except Grigorchuk group. I have studies Grigorchuk group as an counterexample which was first given to settle this question, but wondering are there other examples. What are they? Any sources to them are welcome.  Thanks","What are some examples for Burnside Problem- example of an infinite finitely generated torsion group - except Grigorchuk group. I have studies Grigorchuk group as an counterexample which was first given to settle this question, but wondering are there other examples. What are they? Any sources to them are welcome.  Thanks",,"['abstract-algebra', 'group-theory', 'examples-counterexamples', 'combinatorial-group-theory']"
5,"Classification of finitely generated multigraded modules over $K[x_1,\ldots,x_n]$?",Classification of finitely generated multigraded modules over ?,"K[x_1,\ldots,x_n]","Let $K$ be a field and $R=K[x_1,\ldots,x_n]=\bigoplus_{a\in\mathbb{N}^n}Kx^a$ the multigraded   polynomial ring. Have   finitely-generated multigraded $R$-modules been classified? Are they   of the form $R^r\oplus\bigoplus_{i=1}^sR/Rx^{a_i}$ for some (unique?)   $a_1,\ldots,a_s\in\mathbb{N}^n$? I was thinking along the following lines. If an $\mathbb{N}^n$-graded $R$-module $M$ is generated by $v_1,\ldots,v_r$, then we may assume that every $v_i$ is homogenous of degree $a_i$ (otherwise each of these $v_i$ is a further finite combination of homogenous vectors). Let $R^{[a]}$ be the $R$-module $R$ with the grading shifted by $a\!\in\!\mathbb{N}^n$. Thus the map $R^r\!=\!\bigoplus_{i=1}^rR^{[a_i]}\rightarrow M$ that sends $e_i\mapsto v_i$ is a surjective graded morphism, so its kernel $A$ is a graded submodule and $M\cong R^r/A$, i.e. $$\textstyle{A=\bigoplus_{b\in\mathbb{N}^n}(R^r)_b\cap A=\bigoplus_{b\in\mathbb{N}^n}\prod_iKx^{a_i+b}\cap A}.$$ Hence $A$ is generated by $u_1,\ldots,u_s$ where every component of $u_i$ is $\alpha_{ji}x^{a_i+b_j}$ for some $\alpha_{ji}\!\in\!K$. Thus our module is isomorphic to the cokernel of the matrix $$\left[\begin{matrix} \alpha_{11}x^{a_1+b_1} &\alpha_{12}x^{a_1+b_2}&\ldots&\alpha_{1s}x^{a_1+b_s}\\ \alpha_{21}x^{a_2+b_1} &\alpha_{22}x^{a_2+b_2}&\ldots&\alpha_{2s}x^{a_2+b_s}\\ \vdots&\vdots&\vdots&\vdots\\ \alpha_{r1}x^{a_r+b_1} &\alpha_{r2}x^{a_r+b_2}&\ldots&\alpha_{rs}x^{a_r+b_s}\\ \end{matrix}\right].$$ Now we can do row and column operations, without changing the isomorphism type of the module. But I'm not sure how the above can be transformed into $$\left[\begin{matrix} x^{c_1} &&&\\  &x^{c_2}&&\\ &&\ddots&\\  &&&x^{c_s}\\ \end{matrix}\right].$$ If my conjecture is not valid, I ask for a counterexample. For instance, if $n=2$ and $R=K[x,y]$, are $Coker\left[\begin{smallmatrix} x^2y &xy\\ & xy^2\\ \end{smallmatrix}\right]$ or $Coker\left[\begin{smallmatrix} x &y\\ \end{smallmatrix}\right]$  or $Coker\left[\begin{smallmatrix} x \\y \end{smallmatrix}\right]$ not of the above form? Is there some other classification of   $\mathbb{N}^n$-graded $K[x_1,\ldots,x_n]$-modules? Maybe they are of the    form $\bigoplus_{i=1}^sR^{[a_i]}/\langle x^a; a\!\in\!A_i\rangle$ for some   (unique?) $a_1,\ldots,a_s\!\in\!\mathbb{N}^n$ and $A_1,\ldots,A_s\!\subseteq\!\mathbb{N}^n$? By Monomial Ideals (Herzog & Hibi, 2011) , Dickson’s lemma 2.1.1, every $A_i$ may be finite. Also, elements of $A_i$ are assumed to be incomparable w.r.t. the componentwise partial order on $\mathbb{N}^n$.","Let $K$ be a field and $R=K[x_1,\ldots,x_n]=\bigoplus_{a\in\mathbb{N}^n}Kx^a$ the multigraded   polynomial ring. Have   finitely-generated multigraded $R$-modules been classified? Are they   of the form $R^r\oplus\bigoplus_{i=1}^sR/Rx^{a_i}$ for some (unique?)   $a_1,\ldots,a_s\in\mathbb{N}^n$? I was thinking along the following lines. If an $\mathbb{N}^n$-graded $R$-module $M$ is generated by $v_1,\ldots,v_r$, then we may assume that every $v_i$ is homogenous of degree $a_i$ (otherwise each of these $v_i$ is a further finite combination of homogenous vectors). Let $R^{[a]}$ be the $R$-module $R$ with the grading shifted by $a\!\in\!\mathbb{N}^n$. Thus the map $R^r\!=\!\bigoplus_{i=1}^rR^{[a_i]}\rightarrow M$ that sends $e_i\mapsto v_i$ is a surjective graded morphism, so its kernel $A$ is a graded submodule and $M\cong R^r/A$, i.e. $$\textstyle{A=\bigoplus_{b\in\mathbb{N}^n}(R^r)_b\cap A=\bigoplus_{b\in\mathbb{N}^n}\prod_iKx^{a_i+b}\cap A}.$$ Hence $A$ is generated by $u_1,\ldots,u_s$ where every component of $u_i$ is $\alpha_{ji}x^{a_i+b_j}$ for some $\alpha_{ji}\!\in\!K$. Thus our module is isomorphic to the cokernel of the matrix $$\left[\begin{matrix} \alpha_{11}x^{a_1+b_1} &\alpha_{12}x^{a_1+b_2}&\ldots&\alpha_{1s}x^{a_1+b_s}\\ \alpha_{21}x^{a_2+b_1} &\alpha_{22}x^{a_2+b_2}&\ldots&\alpha_{2s}x^{a_2+b_s}\\ \vdots&\vdots&\vdots&\vdots\\ \alpha_{r1}x^{a_r+b_1} &\alpha_{r2}x^{a_r+b_2}&\ldots&\alpha_{rs}x^{a_r+b_s}\\ \end{matrix}\right].$$ Now we can do row and column operations, without changing the isomorphism type of the module. But I'm not sure how the above can be transformed into $$\left[\begin{matrix} x^{c_1} &&&\\  &x^{c_2}&&\\ &&\ddots&\\  &&&x^{c_s}\\ \end{matrix}\right].$$ If my conjecture is not valid, I ask for a counterexample. For instance, if $n=2$ and $R=K[x,y]$, are $Coker\left[\begin{smallmatrix} x^2y &xy\\ & xy^2\\ \end{smallmatrix}\right]$ or $Coker\left[\begin{smallmatrix} x &y\\ \end{smallmatrix}\right]$  or $Coker\left[\begin{smallmatrix} x \\y \end{smallmatrix}\right]$ not of the above form? Is there some other classification of   $\mathbb{N}^n$-graded $K[x_1,\ldots,x_n]$-modules? Maybe they are of the    form $\bigoplus_{i=1}^sR^{[a_i]}/\langle x^a; a\!\in\!A_i\rangle$ for some   (unique?) $a_1,\ldots,a_s\!\in\!\mathbb{N}^n$ and $A_1,\ldots,A_s\!\subseteq\!\mathbb{N}^n$? By Monomial Ideals (Herzog & Hibi, 2011) , Dickson’s lemma 2.1.1, every $A_i$ may be finite. Also, elements of $A_i$ are assumed to be incomparable w.r.t. the componentwise partial order on $\mathbb{N}^n$.",,"['abstract-algebra', 'commutative-algebra', 'homological-algebra', 'graded-modules']"
6,Putting down axioms for some symbols. Playing with their consequences qualitatively and symbolically. Building theories. The book?,Putting down axioms for some symbols. Playing with their consequences qualitatively and symbolically. Building theories. The book?,,"I am interested in the design and building of theories. By building theories, I mean putting down axioms of various kinds, over various fields, exploring their perhaps interesting, or probably boring, consequences. I would like to know if there are books: perhaps with a historical bent, characterizing the thoughts that went behind the design of a particular system of axioms for a catalogue of theories; perhaps with a non-historical bent, just exploring axiomatization in general; perhaps with a ""fun"" bent, attempting to recreate a well established mathematical theory from scratch; perhaps that are clearly a text on abstract algebra, but one that explores qualitatively the various consequences of choosing particular axioms? Does such a book exist, or am I dreaming?","I am interested in the design and building of theories. By building theories, I mean putting down axioms of various kinds, over various fields, exploring their perhaps interesting, or probably boring, consequences. I would like to know if there are books: perhaps with a historical bent, characterizing the thoughts that went behind the design of a particular system of axioms for a catalogue of theories; perhaps with a non-historical bent, just exploring axiomatization in general; perhaps with a ""fun"" bent, attempting to recreate a well established mathematical theory from scratch; perhaps that are clearly a text on abstract algebra, but one that explores qualitatively the various consequences of choosing particular axioms? Does such a book exist, or am I dreaming?",,"['abstract-algebra', 'reference-request', 'logic', 'model-theory']"
7,Real forms of complex vector spaces and $\mathbb{C}$-algebra,Real forms of complex vector spaces and -algebra,\mathbb{C},"A real form $W$ of a complex vector space $V$ is a real subspace s.t. $\mathbb{C}\otimes_{\mathbb{R}}W \cong V$ by $a\otimes x \longrightarrow ax$, or equivalently there is an $\mathbb{R}$-basis of $W$ that is also a $\mathbb{C}$-basis of $V$. There is a fact that every complex vector space has a real form. For example, $\mathbb{R}^n$, $\mathbb{R}[x]$ and $M_n(\mathbb{R})$ are real forms of $\mathbb{C}^n$, $\mathbb{C}[x]$ and $M_n(\mathbb{C})$ respectively. The last two examples are not only real vector spaces but real subalgebras. So, I'm wondering whether any complex associative algebra $A$ has a real form, i.e. a real subalgebra $B$ s.t. $\mathbb{C}\otimes_{\mathbb{R}}B \cong A$ as $\mathbb{C}$-algbras? Is there any counterexample?","A real form $W$ of a complex vector space $V$ is a real subspace s.t. $\mathbb{C}\otimes_{\mathbb{R}}W \cong V$ by $a\otimes x \longrightarrow ax$, or equivalently there is an $\mathbb{R}$-basis of $W$ that is also a $\mathbb{C}$-basis of $V$. There is a fact that every complex vector space has a real form. For example, $\mathbb{R}^n$, $\mathbb{R}[x]$ and $M_n(\mathbb{R})$ are real forms of $\mathbb{C}^n$, $\mathbb{C}[x]$ and $M_n(\mathbb{C})$ respectively. The last two examples are not only real vector spaces but real subalgebras. So, I'm wondering whether any complex associative algebra $A$ has a real form, i.e. a real subalgebra $B$ s.t. $\mathbb{C}\otimes_{\mathbb{R}}B \cong A$ as $\mathbb{C}$-algbras? Is there any counterexample?",,"['abstract-algebra', 'ring-theory', 'noncommutative-algebra']"
8,Automorphisms of the ring $\Bbb Z[x]$ of polynomials with integer coefficients,Automorphisms of the ring  of polynomials with integer coefficients,\Bbb Z[x],"I am trying to find the automorphisms of the polynomial ring $\mathbb{Z}[x]$. So far, I have shown that if $\varphi$ is an automorphism, then $\varphi(n)=n$ for $n \in \mathbb{Z}$, and $\varphi(x)$ is a polynomial of degree one, say $mx+n$. If we consider what gets sent to $x$ by $\varphi$, we see that $m$ must be a unit of $\mathbb{Z}$, so $m=\pm 1$. So I think the automorphisms fix $\mathbb{Z}$ and send the indeterminate variable $x \mapsto x+n$ or $x \mapsto -x+n$ for $n \in \mathbb{Z}$. Is this correct?","I am trying to find the automorphisms of the polynomial ring $\mathbb{Z}[x]$. So far, I have shown that if $\varphi$ is an automorphism, then $\varphi(n)=n$ for $n \in \mathbb{Z}$, and $\varphi(x)$ is a polynomial of degree one, say $mx+n$. If we consider what gets sent to $x$ by $\varphi$, we see that $m$ must be a unit of $\mathbb{Z}$, so $m=\pm 1$. So I think the automorphisms fix $\mathbb{Z}$ and send the indeterminate variable $x \mapsto x+n$ or $x \mapsto -x+n$ for $n \in \mathbb{Z}$. Is this correct?",,"['abstract-algebra', 'ring-theory']"
9,"Result due to Cohn, unique division ring whose unit group is a given group?","Result due to Cohn, unique division ring whose unit group is a given group?",,"This is a result of Cohn I would like to understand. Let $G$ be a group, $g\in G$, and $\varphi$ a map of the subset $H=\{x\in G:x\neq 1\}$ into itself which has the properties $\varphi(yxy^{-1})=y(\varphi x)y^{-1}$ for $x\in H$, $y\in G$. $\varphi^2(x)=x$ $\varphi(x^{-1})=g(\varphi x)x^{-1}$ $\varphi(xy^{-1})=(\varphi(\varphi(x)\varphi(y^{-1})))\varphi(y^{-1})$ for $x,y\in H, x\neq y$. Then there exists a unique division ring $D$ such that $D^*=G$ for the set of nonzero elements $D^*$ of $D$, and in $G$, $\varphi x=1-x$, $x\in H$, $g=-1$. I did notice that the first three properties agree with $\varphi x=1-x$ for $x\in H$. However, for the fourth property to be satisfied, would require $$ \begin{align*} 1-xy^{-1} &= (1-(1-x)(1-y^{-1}))(1-y^{-1})\\ &= x+y^{-1}-2xy^{-1}-y^{-2}+xy^{-2} \end{align*} $$ which seems like an odd relation to have. I thought about this, but really had no idea where to begin. Is there a reference for this result, or even a sketch I could try to work through? Thank you.","This is a result of Cohn I would like to understand. Let $G$ be a group, $g\in G$, and $\varphi$ a map of the subset $H=\{x\in G:x\neq 1\}$ into itself which has the properties $\varphi(yxy^{-1})=y(\varphi x)y^{-1}$ for $x\in H$, $y\in G$. $\varphi^2(x)=x$ $\varphi(x^{-1})=g(\varphi x)x^{-1}$ $\varphi(xy^{-1})=(\varphi(\varphi(x)\varphi(y^{-1})))\varphi(y^{-1})$ for $x,y\in H, x\neq y$. Then there exists a unique division ring $D$ such that $D^*=G$ for the set of nonzero elements $D^*$ of $D$, and in $G$, $\varphi x=1-x$, $x\in H$, $g=-1$. I did notice that the first three properties agree with $\varphi x=1-x$ for $x\in H$. However, for the fourth property to be satisfied, would require $$ \begin{align*} 1-xy^{-1} &= (1-(1-x)(1-y^{-1}))(1-y^{-1})\\ &= x+y^{-1}-2xy^{-1}-y^{-2}+xy^{-2} \end{align*} $$ which seems like an odd relation to have. I thought about this, but really had no idea where to begin. Is there a reference for this result, or even a sketch I could try to work through? Thank you.",,"['abstract-algebra', 'group-theory', 'ring-theory']"
10,Question about all the homomorphisms from $\mathbb{Z}$ to $\mathbb{Z}$,Question about all the homomorphisms from  to,\mathbb{Z} \mathbb{Z},"An exercise in ""A first course in Abstract Algebra"" asked the following: Describe all ring homomorphisms from the ring $\mathbb{Z},+,\cdot$ to itself. I observed that for any such ring homomorphism the following has to hold: $$\varphi(1) = \varphi(1\cdot 1) = \varphi(1) \cdot \varphi(1)$$ In $\mathbb{Z}$ only two numbers exists so that their square equals itself: 0 and 1. When $\varphi(1) = 0$ then $\varphi = 0$ hence $\forall n \in \mathbb{Z}$: $\varphi(n) = \varphi(n \cdot 1) = \varphi(n) \cdot \varphi(1) = \varphi(n) \cdot 0 = 0$. Now, when $\varphi(1) = 1$ I showed that  $\varphi(n) = n$ using induction Base case: $n = 1$, which is true by our assumption Induction hypothesis: $\varphi(m) = m$ for $m < n$ Induction step: $\varphi(n) = \varphi((n-1) + 1) = \varphi(n-1) + \varphi(1) = n-1 + 1 = n$ Now I wonder whether you could show that $\varphi(n) = n$ when $\varphi(1) = 1$ without using induction, which seems overkill for this exercise. EDIT: Forgot about the negative n's. Since $\varphi$ is also a group homomorphism under $\mathbb{Z},+$, we know that $\varphi(-n) = -\varphi(n)$. Thus,  $$\varphi(-n) = -\varphi(n) = -n$$","An exercise in ""A first course in Abstract Algebra"" asked the following: Describe all ring homomorphisms from the ring $\mathbb{Z},+,\cdot$ to itself. I observed that for any such ring homomorphism the following has to hold: $$\varphi(1) = \varphi(1\cdot 1) = \varphi(1) \cdot \varphi(1)$$ In $\mathbb{Z}$ only two numbers exists so that their square equals itself: 0 and 1. When $\varphi(1) = 0$ then $\varphi = 0$ hence $\forall n \in \mathbb{Z}$: $\varphi(n) = \varphi(n \cdot 1) = \varphi(n) \cdot \varphi(1) = \varphi(n) \cdot 0 = 0$. Now, when $\varphi(1) = 1$ I showed that  $\varphi(n) = n$ using induction Base case: $n = 1$, which is true by our assumption Induction hypothesis: $\varphi(m) = m$ for $m < n$ Induction step: $\varphi(n) = \varphi((n-1) + 1) = \varphi(n-1) + \varphi(1) = n-1 + 1 = n$ Now I wonder whether you could show that $\varphi(n) = n$ when $\varphi(1) = 1$ without using induction, which seems overkill for this exercise. EDIT: Forgot about the negative n's. Since $\varphi$ is also a group homomorphism under $\mathbb{Z},+$, we know that $\varphi(-n) = -\varphi(n)$. Thus,  $$\varphi(-n) = -\varphi(n) = -n$$",,"['abstract-algebra', 'ring-theory', 'self-learning']"
11,What does $\mathrm{Spec}(\mathbb{Z})\times_{\mathrm{Spec}(\mathbb{F}_{1})}\mathrm{Spec}(\mathbb{Z})$ look like?,What does  look like?,\mathrm{Spec}(\mathbb{Z})\times_{\mathrm{Spec}(\mathbb{F}_{1})}\mathrm{Spec}(\mathbb{Z}),"See also: What does $\mathrm{Spét}(H\mathbb{Z})\times_{\mathrm{Spét}(\mathbb{S})}\mathrm{Spét}(H\mathbb{Z})$ look like? . $\newcommand{\F}{\mathbb{F}}\newcommand{\N}{\mathbb{N}}\newcommand{\Z}{\mathbb{Z}}$ One of the most mysterious objects in mathematics is the elusive ""field with one element"", and coming with it is the arithmetic curve $\mathrm{Spec}(\mathbb{Z})\times_{\mathrm{Spec}(\mathbb{F}_{1})}\mathrm{Spec}(\mathbb{Z})\cong\mathrm{Spec}(\mathbb{Z}\otimes_{\mathbb{F}_{1}}\mathbb{Z})$ . I want to know what such a thing would look like, and hence am trying to work it out in one particular model for geometry over $\mathbb{F}_1$ , that of binoids . Here are some definitions (for the question, it suffices to know 1–3 only). A binoid is a commutative monoid $M$ together with an absorbing element $0$ . An ideal of $M$ is a subset $I$ such that $0\in I$ . If $a\in I$ and $r\in M$ , then $ra\in I$ . An ideal $I$ of $M$ is prime if it is proper and whenever $ab\in I$ then $a\in I$ or $b\in I$ . The spectrum $\mathrm{Spec}(M)$ of a binoid $M$ is the set of all prime ideals of $M$ . The Zariski topology on $\mathrm{Spec}(M)$ is the topology generated by the collection $\{D(I)\}$ with $D(I)=\mathrm{Spec}(M)\setminus V(I)$ , where $$V(I)=\{\mathfrak{p}\in\mathrm{Spec}(M):I\subset\mathfrak{p}\}.$$ A distinguished open of $\mathrm{Spec}(M)$ is a set of the form $D_f=D(\{f\})$ for some $f\in A$ . These form a basis for the Zariski topology on $\mathrm{Spec}(M)$ . A binoidal space is a pair $(X,\mathcal{O}_X)$ with $X$ a topological space and $\mathcal{O}_X$ a sheaf of binoids on $X$ . An affine binoid scheme is a binoidal space of the form $(\mathrm{Spec}(M),\mathcal{O}_{M})$ , where $\mathcal{O}_{M}$ is defined on the distinguished opens by $$\mathcal{O}_{M}(D_f)=M_f.$$ For example, every ring $R$ has an associated binoid, given by forgetting the addition of $R$ . We have also a tensor product of binoids , and the tensor product $\mathbb{N}\otimes_{\mathbb{F}_{1}}\mathbb{N}$ is isomorphic to a countable direct sum of the multiplicative monoid of positive natural numbers, $(\mathbb{N}\setminus\{0\},\cdot,1)$ , adjoined with an absorbing element $\{0\}$ . It looks like this: The binoid $\Z\otimes_{\F_{1}}\Z\cong(\Z\setminus\{0\},\cdot)^{\oplus{\N}}\sqcup\{0\}$ is pictured in the same way, we just add negative numbers. What I'd like to ask is: What are the $\mathrm{Spec}$ 's of the main objects involved here, including $\mathrm{Spec}(\mathbb{N})$ and $\mathrm{Spec}(\mathbb{Z})$ (where $\mathbb{N}=(\mathbb{N},\cdot,1)$ and similarly for $\mathbb{Z}$ ), and, above all, \begin{align*} \mathrm{Spec}(\mathbb{N})\times_{\mathrm{Spec}(\mathbb{F}_{1})}\mathrm{Spec}(\mathbb{N}) &\cong \mathrm{Spec}(\mathbb{N}\otimes_{\mathbb{F}_{1}}\mathbb{N}),\\ \mathrm{Spec}(\mathbb{Z})\times_{\mathrm{Spec}(\mathbb{F}_{1})}\mathrm{Spec}(\mathbb{Z}) &\cong \mathrm{Spec}(\mathbb{Z}\otimes_{\mathbb{F}_{1}}\mathbb{Z}), \end{align*} the sets of prime ideals of the binoids $\N\otimes_{\F_{1}}\N\cong(\N\setminus\{0\},\cdot)^{\oplus{\N}}\sqcup\{0\}$ and $\Z\otimes_{\F_{1}}\Z\cong(\Z\setminus\{0\},\cdot)^{\oplus{\N}}\sqcup\{0\}$ ?","See also: What does look like? . One of the most mysterious objects in mathematics is the elusive ""field with one element"", and coming with it is the arithmetic curve . I want to know what such a thing would look like, and hence am trying to work it out in one particular model for geometry over , that of binoids . Here are some definitions (for the question, it suffices to know 1–3 only). A binoid is a commutative monoid together with an absorbing element . An ideal of is a subset such that . If and , then . An ideal of is prime if it is proper and whenever then or . The spectrum of a binoid is the set of all prime ideals of . The Zariski topology on is the topology generated by the collection with , where A distinguished open of is a set of the form for some . These form a basis for the Zariski topology on . A binoidal space is a pair with a topological space and a sheaf of binoids on . An affine binoid scheme is a binoidal space of the form , where is defined on the distinguished opens by For example, every ring has an associated binoid, given by forgetting the addition of . We have also a tensor product of binoids , and the tensor product is isomorphic to a countable direct sum of the multiplicative monoid of positive natural numbers, , adjoined with an absorbing element . It looks like this: The binoid is pictured in the same way, we just add negative numbers. What I'd like to ask is: What are the 's of the main objects involved here, including and (where and similarly for ), and, above all, the sets of prime ideals of the binoids and ?","\mathrm{Spét}(H\mathbb{Z})\times_{\mathrm{Spét}(\mathbb{S})}\mathrm{Spét}(H\mathbb{Z}) \newcommand{\F}{\mathbb{F}}\newcommand{\N}{\mathbb{N}}\newcommand{\Z}{\mathbb{Z}} \mathrm{Spec}(\mathbb{Z})\times_{\mathrm{Spec}(\mathbb{F}_{1})}\mathrm{Spec}(\mathbb{Z})\cong\mathrm{Spec}(\mathbb{Z}\otimes_{\mathbb{F}_{1}}\mathbb{Z}) \mathbb{F}_1 M 0 M I 0\in I a\in I r\in M ra\in I I M ab\in I a\in I b\in I \mathrm{Spec}(M) M M \mathrm{Spec}(M) \{D(I)\} D(I)=\mathrm{Spec}(M)\setminus V(I) V(I)=\{\mathfrak{p}\in\mathrm{Spec}(M):I\subset\mathfrak{p}\}. \mathrm{Spec}(M) D_f=D(\{f\}) f\in A \mathrm{Spec}(M) (X,\mathcal{O}_X) X \mathcal{O}_X X (\mathrm{Spec}(M),\mathcal{O}_{M}) \mathcal{O}_{M} \mathcal{O}_{M}(D_f)=M_f. R R \mathbb{N}\otimes_{\mathbb{F}_{1}}\mathbb{N} (\mathbb{N}\setminus\{0\},\cdot,1) \{0\} \Z\otimes_{\F_{1}}\Z\cong(\Z\setminus\{0\},\cdot)^{\oplus{\N}}\sqcup\{0\} \mathrm{Spec} \mathrm{Spec}(\mathbb{N}) \mathrm{Spec}(\mathbb{Z}) \mathbb{N}=(\mathbb{N},\cdot,1) \mathbb{Z} \begin{align*}
\mathrm{Spec}(\mathbb{N})\times_{\mathrm{Spec}(\mathbb{F}_{1})}\mathrm{Spec}(\mathbb{N}) &\cong \mathrm{Spec}(\mathbb{N}\otimes_{\mathbb{F}_{1}}\mathbb{N}),\\
\mathrm{Spec}(\mathbb{Z})\times_{\mathrm{Spec}(\mathbb{F}_{1})}\mathrm{Spec}(\mathbb{Z}) &\cong \mathrm{Spec}(\mathbb{Z}\otimes_{\mathbb{F}_{1}}\mathbb{Z}),
\end{align*} \N\otimes_{\F_{1}}\N\cong(\N\setminus\{0\},\cdot)^{\oplus{\N}}\sqcup\{0\} \Z\otimes_{\F_{1}}\Z\cong(\Z\setminus\{0\},\cdot)^{\oplus{\N}}\sqcup\{0\}","['abstract-algebra', 'algebraic-geometry', 'semigroups', 'monoid']"
12,Ordering on $R[\sqrt{n}]$ for an ordered ring $R$,Ordering on  for an ordered ring,R[\sqrt{n}] R,"I'm interested in showing that if $R$ is an ordered ring (with ordering $\leq$ ), and $n \geq 0$ , then $R[\sqrt{n}]$ is also an ordered ring. In the reals, $0 \leq a_1 + a_n\sqrt n$ iff either (1) $0 \leq a_1$ and $na_n^2 \leq a_1^2$ , or (2) $0 \leq a_n$ and $a_1^2 \leq na_n^2$ .  So it seems natural to define the set $P$ of nonnegative elements of $R[\sqrt{n}]$ by the rule $a_1 + a_n\sqrt{n} \in P$ iff (1) or (2) above hold However, I'm having trouble proving that $x \in P$ and $y \in P$ implies $x + y \in P$ .  It seems like it should be elementary, but I'm finding myself running around in circles. I've been trying to split the proof into cases, depending on whether $x$ and $y$ satisfy rules (1) or (2).  The cases where they both satisfy (1) or both satisfy (2) are easy, but the casework is getting overwhelming when $x$ satisfies (1) and $y$ satisfies (2). I wonder if anyone can suggest something I'm missing, or maybe point me to a more general theorem that would help (although I'd like to avoid getting too esoteric since my eventual goal is to formulate the proof in coq). Thanks! Update: I’ve made progress but am still stuck.  Unless I’m mistaken, the only case I need is to show that $0 \leq (x_1 + x_n\sqrt{n}) + (y_1 + y_n\sqrt{n})$ when the following conditions all hold: $0 \leq x_1$ $0 \leq -x_n$ $nx_n^2 \leq x_1^2$ $0 \leq -y_1$ $0 \leq y_n$ $y_1^2 \leq ny_n^2$ $0 \leq x_1 + y_1$ $0 \leq -x_n - y_n$ Combining these assumptions gives $0 \leq y_1^2 \leq ny_n^2 \leq n x_n^2 \leq x_1^2$ . In this case I need to show that $n(x_n + y_n)^2 \leq (x_1 + y_1)^2$ . I’ve tried playing with fractions, derivatives, absolute values, and just can’t solve this case.  Any help would be appreciated.","I'm interested in showing that if is an ordered ring (with ordering ), and , then is also an ordered ring. In the reals, iff either (1) and , or (2) and .  So it seems natural to define the set of nonnegative elements of by the rule iff (1) or (2) above hold However, I'm having trouble proving that and implies .  It seems like it should be elementary, but I'm finding myself running around in circles. I've been trying to split the proof into cases, depending on whether and satisfy rules (1) or (2).  The cases where they both satisfy (1) or both satisfy (2) are easy, but the casework is getting overwhelming when satisfies (1) and satisfies (2). I wonder if anyone can suggest something I'm missing, or maybe point me to a more general theorem that would help (although I'd like to avoid getting too esoteric since my eventual goal is to formulate the proof in coq). Thanks! Update: I’ve made progress but am still stuck.  Unless I’m mistaken, the only case I need is to show that when the following conditions all hold: Combining these assumptions gives . In this case I need to show that . I’ve tried playing with fractions, derivatives, absolute values, and just can’t solve this case.  Any help would be appreciated.",R \leq n \geq 0 R[\sqrt{n}] 0 \leq a_1 + a_n\sqrt n 0 \leq a_1 na_n^2 \leq a_1^2 0 \leq a_n a_1^2 \leq na_n^2 P R[\sqrt{n}] a_1 + a_n\sqrt{n} \in P x \in P y \in P x + y \in P x y x y 0 \leq (x_1 + x_n\sqrt{n}) + (y_1 + y_n\sqrt{n}) 0 \leq x_1 0 \leq -x_n nx_n^2 \leq x_1^2 0 \leq -y_1 0 \leq y_n y_1^2 \leq ny_n^2 0 \leq x_1 + y_1 0 \leq -x_n - y_n 0 \leq y_1^2 \leq ny_n^2 \leq n x_n^2 \leq x_1^2 n(x_n + y_n)^2 \leq (x_1 + y_1)^2,"['abstract-algebra', 'ordered-rings']"
13,When does there exist a section of $GL_n(\mathbb Z_p) \rightarrow GL_n(\mathbb F_p)$?,When does there exist a section of ?,GL_n(\mathbb Z_p) \rightarrow GL_n(\mathbb F_p),"There is a reduction map $f: GL_n(\mathbb Z_p) \rightarrow GL_n(\mathbb F_p)$ for any prime $p$ , when does there exist a group homomorphism $g: GL_n(\mathbb F_p) \rightarrow GL_n(\mathbb Z_p)$ such that $f\circ g=id$ ? If $n=1$ this is always possible, and if $p-1>n$ this is always impossible as $\mathbb F_p \not \subseteq GL_n(\mathbb Z_p)$ by considering the minimal polynomial.","There is a reduction map for any prime , when does there exist a group homomorphism such that ? If this is always possible, and if this is always impossible as by considering the minimal polynomial.",f: GL_n(\mathbb Z_p) \rightarrow GL_n(\mathbb F_p) p g: GL_n(\mathbb F_p) \rightarrow GL_n(\mathbb Z_p) f\circ g=id n=1 p-1>n \mathbb F_p \not \subseteq GL_n(\mathbb Z_p),['abstract-algebra']
14,The Disk and the Punctured Disk,The Disk and the Punctured Disk,,"Can you explane me why $$D =  \operatorname{Spec}\mathbb{C}[[t]]$$ is the disk and $$D^{\times} =  \operatorname{Spec}\mathbb{C}((t))$$ is the punctured disk? Or give me some links on intelligible books, lectures, etc... Thanks a lot!","Can you explane me why $$D =  \operatorname{Spec}\mathbb{C}[[t]]$$ is the disk and $$D^{\times} =  \operatorname{Spec}\mathbb{C}((t))$$ is the punctured disk? Or give me some links on intelligible books, lectures, etc... Thanks a lot!",,"['abstract-algebra', 'general-topology', 'algebraic-geometry']"
15,Directly indecomposable rings,Directly indecomposable rings,,"Is every ring the (possibly infinite) direct product of directly indecomposable rings? I believe the answer is no, but I'm not positive and don't know any explicit examples. A reduction: If $R$ is a unital, associative ring, then define $B(R)$ to be set of all central idempotents of $R$, $B(R) = \{ e \in R: e^2 = e, er=re ~(\forall r \in R) \}$. $B(R)$ is a ring under the operations $e \oplus f = e+f -ef$ and the standard multiplication from $R$. If $R$ was a direct product of directly indecomposable rings $R_i =  Re_i$ for $i \in I$, then the elements of $B(R)$ are exactly the $e_J$ for $J \subseteq I$; $e_J \oplus e_K = e_{J \oplus K}$ where $J \oplus K$ is symmetric-difference; and $e_J \cdot e_K = e_{J \cap K}$. In particular, $B(R) \cong \mathbb{Z}/2\mathbb{Z}^I$ is what is called a complete boolean algebra. Hence any ring in which $B(R)$ is not a complete boolean algebra is an example. If $B(B(R)) = B(R)$, then I think this will basically work, though I still wouldn't mind the details being stated clearly (especially in algebraic language). Anderson–Fuller page 102 provides an example to those who understand the topology of $\mathbb{Q}$, but I'm not such a person. I think the finite-cofinite boolean algebra is likely more my speed, but I'm not sure it is not isomorphic to a complete boolean algebra. (I think the struck out portions might be wrong; I'd appreciate corrections.)","Is every ring the (possibly infinite) direct product of directly indecomposable rings? I believe the answer is no, but I'm not positive and don't know any explicit examples. A reduction: If $R$ is a unital, associative ring, then define $B(R)$ to be set of all central idempotents of $R$, $B(R) = \{ e \in R: e^2 = e, er=re ~(\forall r \in R) \}$. $B(R)$ is a ring under the operations $e \oplus f = e+f -ef$ and the standard multiplication from $R$. If $R$ was a direct product of directly indecomposable rings $R_i =  Re_i$ for $i \in I$, then the elements of $B(R)$ are exactly the $e_J$ for $J \subseteq I$; $e_J \oplus e_K = e_{J \oplus K}$ where $J \oplus K$ is symmetric-difference; and $e_J \cdot e_K = e_{J \cap K}$. In particular, $B(R) \cong \mathbb{Z}/2\mathbb{Z}^I$ is what is called a complete boolean algebra. Hence any ring in which $B(R)$ is not a complete boolean algebra is an example. If $B(B(R)) = B(R)$, then I think this will basically work, though I still wouldn't mind the details being stated clearly (especially in algebraic language). Anderson–Fuller page 102 provides an example to those who understand the topology of $\mathbb{Q}$, but I'm not such a person. I think the finite-cofinite boolean algebra is likely more my speed, but I'm not sure it is not isomorphic to a complete boolean algebra. (I think the struck out portions might be wrong; I'd appreciate corrections.)",,"['abstract-algebra', 'ring-theory']"
16,Ideal of ideal needs not to be an ideal,Ideal of ideal needs not to be an ideal,,"Suppose I is an ideal of a ring R and J is an ideal of I, is there any counter example showing J need not to be an ideal of R? The hint given in the book is to consider polynomial ring with coefficient from a field, thanks","Suppose I is an ideal of a ring R and J is an ideal of I, is there any counter example showing J need not to be an ideal of R? The hint given in the book is to consider polynomial ring with coefficient from a field, thanks",,"['abstract-algebra', 'ideals']"
17,How to calculate separable closures of an algebraic extension?,How to calculate separable closures of an algebraic extension?,,"In my study of fields, the notion of the separability of an algebraic field extension is one of the more slippery concepts I have encountered thusfar. What is particularly vexing to me is the notion of a separable closure. Let $E/F$ be an algebraic field extension. The separable closure of $F$ in $E$ is defined to be the subfield $E_{sep}=\{\alpha \in E \; | \; \alpha $ is separable over $ F \}$ . While the concept of a separable closure is easy enough to understand based on the definition, the actual calculation of a separable closure $E_{sep}$ for a given algebraic field extension $E/F$ is not obvious to me. For example, I was recently assigned the following problem: Let $k=\mathbb{F}_2(t)$ , $\alpha$ be a root of $X^4+tX^2+t\in k[X]$ in an algebraic closure of $k$ and $K=k(\alpha)$ , where $\mathbb{F_2}=\mathbb{Z}/2\mathbb{Z}$ . Determine $K_{sep}$ and $[K_{sep}:K]$ . My first thought is that since the separability of an element $a$ of the algebraic field extension $E/F$ is defined based on the structure of its minimal polynomial $m_{a,F}(X) \in F[X]$ , we can use this to determine the elements of $E_{sep}$ . An element $e \in E$ is inseparable if and only if $m_{e,F}(X)=g(X^p)$ for some separable $g(X) \in F[X]$ . So the separable elements of $E$ are those whose minimal polynomial is not an element of $F[X^p]$ . This, however, does not seems to be a tractable idea. My questions are How does one approach the problem of calculation the separable closure of an algebraic extension, like, for example, the one above? If there is no general, one-size-fits-all method, what are some examples of techniques used to deduce the separable closure? Note: any questions asked herein are not current homework questions.","In my study of fields, the notion of the separability of an algebraic field extension is one of the more slippery concepts I have encountered thusfar. What is particularly vexing to me is the notion of a separable closure. Let be an algebraic field extension. The separable closure of in is defined to be the subfield is separable over . While the concept of a separable closure is easy enough to understand based on the definition, the actual calculation of a separable closure for a given algebraic field extension is not obvious to me. For example, I was recently assigned the following problem: Let , be a root of in an algebraic closure of and , where . Determine and . My first thought is that since the separability of an element of the algebraic field extension is defined based on the structure of its minimal polynomial , we can use this to determine the elements of . An element is inseparable if and only if for some separable . So the separable elements of are those whose minimal polynomial is not an element of . This, however, does not seems to be a tractable idea. My questions are How does one approach the problem of calculation the separable closure of an algebraic extension, like, for example, the one above? If there is no general, one-size-fits-all method, what are some examples of techniques used to deduce the separable closure? Note: any questions asked herein are not current homework questions.","E/F F E E_{sep}=\{\alpha \in E \; | \; \alpha   F \} E_{sep} E/F k=\mathbb{F}_2(t) \alpha X^4+tX^2+t\in k[X] k K=k(\alpha) \mathbb{F_2}=\mathbb{Z}/2\mathbb{Z} K_{sep} [K_{sep}:K] a E/F m_{a,F}(X) \in F[X] E_{sep} e \in E m_{e,F}(X)=g(X^p) g(X) \in F[X] E F[X^p]","['abstract-algebra', 'field-theory', 'extension-field', 'separable-extension']"
18,Are complex numbers the only field that leads to the equivalence of analyticity and differentiability?,Are complex numbers the only field that leads to the equivalence of analyticity and differentiability?,,"One of the most beloved properties of complex numbers is that doing analysis on complex functions leads to the fact that every continuously differentiable complex function is analytic (and if it is $C^1$ it is automatically $C^{\infty}$ ) which is naturally something very comfortable to have. I was wondering what made the complex numbers so special; certainly, the fact that they constitute a field sets them apart from, for example, $\mathbb{R}^2$ , as a setting for doing analysis on it. But are they unique in that regard? A central fact about analysis on complex functions seems to be that Green's Theorem leads to Cauchy's Theorem, and then the topology of complex numbers leads to Cauchy's integral formula, from whence the equivalence of continuous differentiability and analiticity can be obtained. So maybe they are unique, because for other fields where a similar concept of differentiation this line of reasoning would not hold; but there might be other routes. So my question is as follows: let $\mathbb{F}$ be a field, and, for a function from $\mathbb{F}$ to $\mathbb{F}$ , $g$ , let a similar notion for differentiation be established (ie $g$ is differentiable on $a \in \mathbb{F} $ if $g(a+h)-g(a)=f_a\cdot h+o(h)$ , for $h \in \mathbb{F}, $ and $f_a$ is the derivative). Suppose that every continuously differentiable function in an open set (in some topology one might bestow onto $\mathbb{F}$ ) is analytic in that open set. Does that force $\mathbb{F}$ to be the complex numbers, with the Euclidean topology? Or might there be other fields, with differentent topologies, which have this property? EDIT: I would like to better frame the question following the sugestions in the comments. Firstly, let $\mathbb{F}$ be a topological field such that the operations are continuous given some arbitrary topology. Further, let $g: \mathbb{F} \rightarrow \mathbb{F}$ be a differentiable function in an open set $U$ in the following sense: $$\forall a \in U, \exists g_a \in \mathbb{F} \, \text{s.t} \hspace{2mm}g(a+h)-g(a)=g_a \cdot h+o(h)$$ for some $h \in \mathbb{F}$ and where $o(h)$ is the standard little oh definition. Suppose we have that $g$ differentiable in some open set $U$ $\iff$ $g$ is analytic in $U$ . Does this restrict the topology and the algebra of $\mathbb{F}$ enough to conclude that it should be $\mathbb{C}$ with the euclidean topology? If not, up to where could we constrain them, in view of maybe arriving at some necessary or even sufficient conditions for a topological field to have that property?","One of the most beloved properties of complex numbers is that doing analysis on complex functions leads to the fact that every continuously differentiable complex function is analytic (and if it is it is automatically ) which is naturally something very comfortable to have. I was wondering what made the complex numbers so special; certainly, the fact that they constitute a field sets them apart from, for example, , as a setting for doing analysis on it. But are they unique in that regard? A central fact about analysis on complex functions seems to be that Green's Theorem leads to Cauchy's Theorem, and then the topology of complex numbers leads to Cauchy's integral formula, from whence the equivalence of continuous differentiability and analiticity can be obtained. So maybe they are unique, because for other fields where a similar concept of differentiation this line of reasoning would not hold; but there might be other routes. So my question is as follows: let be a field, and, for a function from to , , let a similar notion for differentiation be established (ie is differentiable on if , for and is the derivative). Suppose that every continuously differentiable function in an open set (in some topology one might bestow onto ) is analytic in that open set. Does that force to be the complex numbers, with the Euclidean topology? Or might there be other fields, with differentent topologies, which have this property? EDIT: I would like to better frame the question following the sugestions in the comments. Firstly, let be a topological field such that the operations are continuous given some arbitrary topology. Further, let be a differentiable function in an open set in the following sense: for some and where is the standard little oh definition. Suppose we have that differentiable in some open set is analytic in . Does this restrict the topology and the algebra of enough to conclude that it should be with the euclidean topology? If not, up to where could we constrain them, in view of maybe arriving at some necessary or even sufficient conditions for a topological field to have that property?","C^1 C^{\infty} \mathbb{R}^2 \mathbb{F} \mathbb{F} \mathbb{F} g g a \in \mathbb{F}  g(a+h)-g(a)=f_a\cdot h+o(h) h \in \mathbb{F},  f_a \mathbb{F} \mathbb{F} \mathbb{F} g: \mathbb{F} \rightarrow \mathbb{F} U \forall a \in U, \exists g_a \in \mathbb{F} \, \text{s.t} \hspace{2mm}g(a+h)-g(a)=g_a \cdot h+o(h) h \in \mathbb{F} o(h) g U \iff g U \mathbb{F} \mathbb{C}","['abstract-algebra', 'complex-analysis']"
19,"Why is the number of elements in a group called ""order""?","Why is the number of elements in a group called ""order""?",,"This is a question that I have for a long time, Why is the number of elements in a group called ""order""? I mean, the word ""order"" in Spanish (which is my language) has a very strong meaning in terms of ""ordering"", but it does not refer to quantities. What was the motivation for this?","This is a question that I have for a long time, Why is the number of elements in a group called ""order""? I mean, the word ""order"" in Spanish (which is my language) has a very strong meaning in terms of ""ordering"", but it does not refer to quantities. What was the motivation for this?",,"['abstract-algebra', 'group-theory', 'terminology']"
20,"Calculate an abstract algebra ""sum"" $\frac 12 * \frac13 *...*\frac 1{1000}$.","Calculate an abstract algebra ""sum"" .",\frac 12 * \frac13 *...*\frac 1{1000},"Let $x*y= \frac {x+y}{1+xy}$, $x,y\in(-1,1)$. Calculate $\frac 12 * \frac13 *...*\frac 1{1000}$. My attempt: First I tried to find some inductive formula but I get something like this: $\frac 12*\frac 13=\frac 57$ $\frac 57*\frac 14=\frac 9{11}$ $\frac 9{11}*\frac 15=\frac 78$ didn't got anywhere... Then I thought maybe if I let $G=(-1,1)$ then $(G,*)$ is an abelian group. That means I have to find another group that can be an isomorphism with this one and try to calculate using that composition. So I found that:  $$f:(0,\infty)\to (-1,1),f(x)=\frac {x-1}{x+1}$$ is an isomorphism with $G$ from $(\mathbb{R},\times)$ to $(G,*).$ Then: $f(xy)=f(x)*f(y)$. So: $$f^{-1}:(-1,1)\to(0,\infty), f^{-1}(x)=-\frac {x+1}{x-1}.$$ Is is true that $f^{-1}(x*y)=f^{-1}(x)f^{-1}(y)?$ If for $f$ happens this then it happens for $f^{-1}$ too? If this is true then I think I solved my exercise because: $$f^{-1}\left(\frac 12*\frac 13*\ldots*\frac 1{1000}\right)=f\left(\frac {1}{2}\right) f\left(\frac{1}{3}\right)\ldots f\left(\frac{1}{1000}\right)= \frac {\frac 32}{-\frac 12}\frac {\frac 43}{-\frac 23}...\frac {\frac {1000}{999}}{-\frac {998}{999}}\frac {\frac {1001}{1000}}{-\frac {999}{1000}}=500\cdot 1001=500500.$$ then $\frac 12*\frac 13*...*\frac 1{1000}=f(500500)=\frac {500499}{500501}$","Let $x*y= \frac {x+y}{1+xy}$, $x,y\in(-1,1)$. Calculate $\frac 12 * \frac13 *...*\frac 1{1000}$. My attempt: First I tried to find some inductive formula but I get something like this: $\frac 12*\frac 13=\frac 57$ $\frac 57*\frac 14=\frac 9{11}$ $\frac 9{11}*\frac 15=\frac 78$ didn't got anywhere... Then I thought maybe if I let $G=(-1,1)$ then $(G,*)$ is an abelian group. That means I have to find another group that can be an isomorphism with this one and try to calculate using that composition. So I found that:  $$f:(0,\infty)\to (-1,1),f(x)=\frac {x-1}{x+1}$$ is an isomorphism with $G$ from $(\mathbb{R},\times)$ to $(G,*).$ Then: $f(xy)=f(x)*f(y)$. So: $$f^{-1}:(-1,1)\to(0,\infty), f^{-1}(x)=-\frac {x+1}{x-1}.$$ Is is true that $f^{-1}(x*y)=f^{-1}(x)f^{-1}(y)?$ If for $f$ happens this then it happens for $f^{-1}$ too? If this is true then I think I solved my exercise because: $$f^{-1}\left(\frac 12*\frac 13*\ldots*\frac 1{1000}\right)=f\left(\frac {1}{2}\right) f\left(\frac{1}{3}\right)\ldots f\left(\frac{1}{1000}\right)= \frac {\frac 32}{-\frac 12}\frac {\frac 43}{-\frac 23}...\frac {\frac {1000}{999}}{-\frac {998}{999}}\frac {\frac {1001}{1000}}{-\frac {999}{1000}}=500\cdot 1001=500500.$$ then $\frac 12*\frac 13*...*\frac 1{1000}=f(500500)=\frac {500499}{500501}$",,['abstract-algebra']
21,Intuitive/geometric way of thinking about effective divisors?,Intuitive/geometric way of thinking about effective divisors?,,What is the motivation/intuition/geometric way of thinking about an effective divisor? I know that a divisor is effective if all its coefficients are non-negative. We write $D \ge 0$ for effective divisors. We can extend this in the obvious manner to get a partial ordering of divisors. I would consider any explanation that introduces the concepts of Weil and/or Cartier divisors to be obfuscating the underlying intuition of the definition in more formalism...,What is the motivation/intuition/geometric way of thinking about an effective divisor? I know that a divisor is effective if all its coefficients are non-negative. We write $D \ge 0$ for effective divisors. We can extend this in the obvious manner to get a partial ordering of divisors. I would consider any explanation that introduces the concepts of Weil and/or Cartier divisors to be obfuscating the underlying intuition of the definition in more formalism...,,"['abstract-algebra', 'geometry']"
22,Conditions on cycle types for permutations to generate $S_n$,Conditions on cycle types for permutations to generate,S_n,"Consider the following result of Dedekind: For any polynomial $p \in \mathbb{Z}[x]$ and any prime $q$ not dividing the discriminant of $p$ , if $p$ factors modulo $q$ into a product of irreducible polynomials with degrees $d_1, \ldots, d_s$ , then the Galois group $\text{Gal}(p)$ contains a permutation with cycle structure $(d_1, \ldots, d_s)$ . In particular, it is known that in $S_r$ , $r$ prime, is generated by a $2$ -cycle and an $r$ -cycle, so if one can find appropriate primes, one can use Dedekind's result to show quickly that a given polynomial of prime degree has full Galois group $S_5$ . This condition is rather limited, however, and this motivates the following question: Suppose we know the cycle types of some elements of a group $G \leq S_n$ . Under what conditions on those cycle types can we conclude that $G$ is actually the full group $S_n$ ? This line of thinking was motivated, by the way, by this question , which asks about the solvability of the polynomials $$p_n(x) := x^n - \sum_{k = 0}^{n - 1} x^k$$ that arise when studying the so-called $n$ -nacci sequences . Two of the answers there report CAS computations giving that the Galois group of $p_n$ is $S_n$ for $n \leq 12$ , and one speculates that that holds for all $n$ . For $n = 5$ , one can use the primes $q = 3, 5$ to show that the Galois group contains $2$ - and $5$ -cycles, and hence by the above fact $\text{Gal}(p_5) \cong S_5$ . One can do the same, for example, with $n = 13$ and $q = 5, 17$ to conclude that $\text{Gal}(p_{13}) \cong S_{13}$ , extending the other answers there.","Consider the following result of Dedekind: For any polynomial and any prime not dividing the discriminant of , if factors modulo into a product of irreducible polynomials with degrees , then the Galois group contains a permutation with cycle structure . In particular, it is known that in , prime, is generated by a -cycle and an -cycle, so if one can find appropriate primes, one can use Dedekind's result to show quickly that a given polynomial of prime degree has full Galois group . This condition is rather limited, however, and this motivates the following question: Suppose we know the cycle types of some elements of a group . Under what conditions on those cycle types can we conclude that is actually the full group ? This line of thinking was motivated, by the way, by this question , which asks about the solvability of the polynomials that arise when studying the so-called -nacci sequences . Two of the answers there report CAS computations giving that the Galois group of is for , and one speculates that that holds for all . For , one can use the primes to show that the Galois group contains - and -cycles, and hence by the above fact . One can do the same, for example, with and to conclude that , extending the other answers there.","p \in \mathbb{Z}[x] q p p q d_1, \ldots, d_s \text{Gal}(p) (d_1, \ldots, d_s) S_r r 2 r S_5 G \leq S_n G S_n p_n(x) := x^n - \sum_{k = 0}^{n - 1} x^k n p_n S_n n \leq 12 n n = 5 q = 3, 5 2 5 \text{Gal}(p_5) \cong S_5 n = 13 q = 5, 17 \text{Gal}(p_{13}) \cong S_{13}","['abstract-algebra', 'group-theory', 'galois-theory']"
23,Most general version of Hensel's Lemma,Most general version of Hensel's Lemma,,"Roughly speaking, Hensel's Lemma states that a polynomial $f \in O[X]$ over a certain local ring $(O,\mathfrak{m})$ which factors over the residue field $O/\mathfrak{m}$ into coprime polynomials also factors over $O$ in a compatible way. However, there are different versions of the lemma with different requirements on $O$. For example the statement holds true if $O$ is complete with respect to the $\mathfrak{m}$-adic topology, and it also holds true if $O$ is the valuation ring of a nontrivial non-archimedean absolute value on some field $K$, which is complete with respect to this absolute value. Is there a more general version of Hensel's Lemma which implies both of the above statements?","Roughly speaking, Hensel's Lemma states that a polynomial $f \in O[X]$ over a certain local ring $(O,\mathfrak{m})$ which factors over the residue field $O/\mathfrak{m}$ into coprime polynomials also factors over $O$ in a compatible way. However, there are different versions of the lemma with different requirements on $O$. For example the statement holds true if $O$ is complete with respect to the $\mathfrak{m}$-adic topology, and it also holds true if $O$ is the valuation ring of a nontrivial non-archimedean absolute value on some field $K$, which is complete with respect to this absolute value. Is there a more general version of Hensel's Lemma which implies both of the above statements?",,"['abstract-algebra', 'commutative-algebra', 'algebraic-number-theory']"
24,Any abstract algebra book with programming (homework) assignment?,Any abstract algebra book with programming (homework) assignment?,,"All: I had studied abstract algebra long time ago. Now, I would like to review some material, particularly about Galois theory (and its application). Can anyone recommend an abstract algebra book which cover Galois theory (and its applications)? I have been a software engineer for past many years. Ideally, I would like an algebra book with programming assignments or exercise (help me to understand concept.)  For example, homework assignments to write a program to verify Galois theory, or to construct 'solvable Group"", or anything like that. (Of course, I can think some random questions, but I prefer a text book with well designed, and meaningful home works). I felt that I was not good at deriving formula anymore, I would like to use my programming skills to help me to understand subject, do more hand-on exercise and calculations.","All: I had studied abstract algebra long time ago. Now, I would like to review some material, particularly about Galois theory (and its application). Can anyone recommend an abstract algebra book which cover Galois theory (and its applications)? I have been a software engineer for past many years. Ideally, I would like an algebra book with programming assignments or exercise (help me to understand concept.)  For example, homework assignments to write a program to verify Galois theory, or to construct 'solvable Group"", or anything like that. (Of course, I can think some random questions, but I prefer a text book with well designed, and meaningful home works). I felt that I was not good at deriving formula anymore, I would like to use my programming skills to help me to understand subject, do more hand-on exercise and calculations.",,"['abstract-algebra', 'reference-request', 'galois-theory', 'math-software', 'computational-algebra']"
25,faithfully flat ring extensions where primes extend to primes,faithfully flat ring extensions where primes extend to primes,,"I am interested in unital ring homomorphisms (and classes thereof) $R \rightarrow S$ of commutative rings that have the following pair of properties: $S$ is faithfully flat as an $R$-module, and For any prime ideal $P$ of $R$, $PS$ is a prime ideal of $S$. I know of the following examples: $R \rightarrow R[X]$, where $X$ is an indeterminate over $R$ (or any set of algebraically independent indeterminates over $R$) $R \rightarrow R[[X]]$, where $X$ is an analytic indeterminate over $R$ (or any finite set of such) If $R$ is a field, take any (unital) ring map $R \rightarrow S$ such that $S$ is an integral domain. If $R$ is a Noetherian valuation domain, the map $R \rightarrow \hat{R}$ will work. Whenever $g: R \rightarrow S$ is an example of this and $W$ is a multiplicatively closed subset of $S$ such that $m S_W \neq S_W$ for any maximal ideal $m$ of $R$, then the induced map $R \rightarrow S_W$ works as well.  (Some other localizations will be fine too.) Then there are cases that sometimes work.  For instance, one might expect $R \rightarrow \hat{R}$ to work, where $(R,m)$ is an arbitrary Noetherian local ring and $\hat{R}$ is its $m$-adic completion.  (As noted above, this works for valuation rings.)  However, it only works when $R/p$ is analytically irreducible for every prime ideal $p$ of $R$.  It fails, for instance, when $R$ is the localization of $k[x,y]$ at $(x,y)$ when $k$ is a field of characteristic other than $2$.  (This is because the element $x^2-y^2-y^3$ is prime in $R$ but not in $\hat{R}$.) Do people know of any other important examples?","I am interested in unital ring homomorphisms (and classes thereof) $R \rightarrow S$ of commutative rings that have the following pair of properties: $S$ is faithfully flat as an $R$-module, and For any prime ideal $P$ of $R$, $PS$ is a prime ideal of $S$. I know of the following examples: $R \rightarrow R[X]$, where $X$ is an indeterminate over $R$ (or any set of algebraically independent indeterminates over $R$) $R \rightarrow R[[X]]$, where $X$ is an analytic indeterminate over $R$ (or any finite set of such) If $R$ is a field, take any (unital) ring map $R \rightarrow S$ such that $S$ is an integral domain. If $R$ is a Noetherian valuation domain, the map $R \rightarrow \hat{R}$ will work. Whenever $g: R \rightarrow S$ is an example of this and $W$ is a multiplicatively closed subset of $S$ such that $m S_W \neq S_W$ for any maximal ideal $m$ of $R$, then the induced map $R \rightarrow S_W$ works as well.  (Some other localizations will be fine too.) Then there are cases that sometimes work.  For instance, one might expect $R \rightarrow \hat{R}$ to work, where $(R,m)$ is an arbitrary Noetherian local ring and $\hat{R}$ is its $m$-adic completion.  (As noted above, this works for valuation rings.)  However, it only works when $R/p$ is analytically irreducible for every prime ideal $p$ of $R$.  It fails, for instance, when $R$ is the localization of $k[x,y]$ at $(x,y)$ when $k$ is a field of characteristic other than $2$.  (This is because the element $x^2-y^2-y^3$ is prime in $R$ but not in $\hat{R}$.) Do people know of any other important examples?",,"['abstract-algebra', 'commutative-algebra']"
26,Orientation on finite dimensional vector spaces over finite fields.,Orientation on finite dimensional vector spaces over finite fields.,,"For finite-dimensional $\mathbb R$-vector spaces, we define an orientation to be an equivalence class of ordered bases, where $B_1 \sim B_2$ iff the change of basis matrix $A$ taking $B_{1}$ to $B_{2}$ has positive determinant. Then there are two equivalence classes, one which we call, ""positive"" and the other we call, ""negative"". I wanted to know if any work has been done to extend this to finite-dimensional vector spaces over finite fields. My idea was to do everything as above, but replace the condition $\det A>0$ with $\det A$ is a quadratic residue. This should split the bases into two equivalence classes just as above. I played around with these and noticed some interesting things. For example let $q=p^{f}$ and $\mathbb F_{q}$ be an $\mathbb F_{p}$ vector space. Unlike the case for $\mathbb R$, if $q \not\equiv 3 \pmod 4$ then switching any two vectors in your basis doesn't change the equivalence class, because the determinant of the corresponding change of basis matrix is $-1$, which is a quadratic residue in this case. Has any work been done on this, or are there any other definitions or related concepts that might be of interest?","For finite-dimensional $\mathbb R$-vector spaces, we define an orientation to be an equivalence class of ordered bases, where $B_1 \sim B_2$ iff the change of basis matrix $A$ taking $B_{1}$ to $B_{2}$ has positive determinant. Then there are two equivalence classes, one which we call, ""positive"" and the other we call, ""negative"". I wanted to know if any work has been done to extend this to finite-dimensional vector spaces over finite fields. My idea was to do everything as above, but replace the condition $\det A>0$ with $\det A$ is a quadratic residue. This should split the bases into two equivalence classes just as above. I played around with these and noticed some interesting things. For example let $q=p^{f}$ and $\mathbb F_{q}$ be an $\mathbb F_{p}$ vector space. Unlike the case for $\mathbb R$, if $q \not\equiv 3 \pmod 4$ then switching any two vectors in your basis doesn't change the equivalence class, because the determinant of the corresponding change of basis matrix is $-1$, which is a quadratic residue in this case. Has any work been done on this, or are there any other definitions or related concepts that might be of interest?",,"['abstract-algebra', 'number-theory', 'vector-spaces', 'differential-topology', 'finite-fields']"
27,Emil Artin's proof for Wedderburn's Little Theorem,Emil Artin's proof for Wedderburn's Little Theorem,,"I am looking through different proofs for Wedderburn's Little Theorem, which states that every finite division ring is necessarily a field. I would like to read Emil Artin's proof for this theorem: Emil Artin, Über einen Satz von Herm J. H. Maclagan Wedderburn, Hamb. Abh. 5 (1928), 245-250. I have found the paper, but unfortunately, I can't read German. Does anyone know if there is a translation for this paper? If such translation exists, I would love to know where it can be found. Thanks!","I am looking through different proofs for Wedderburn's Little Theorem, which states that every finite division ring is necessarily a field. I would like to read Emil Artin's proof for this theorem: Emil Artin, Über einen Satz von Herm J. H. Maclagan Wedderburn, Hamb. Abh. 5 (1928), 245-250. I have found the paper, but unfortunately, I can't read German. Does anyone know if there is a translation for this paper? If such translation exists, I would love to know where it can be found. Thanks!",,"['abstract-algebra', 'reference-request']"
28,Let $z$ be a complex number. The number $1$ is written on a board. You perform a series of moves. When can you make terms tend to $0$?,Let  be a complex number. The number  is written on a board. You perform a series of moves. When can you make terms tend to ?,z 1 0,"Let $z$ be a complex number. The number $1$ is written on a board. You perform a series of moves, where in each move you may either replace the number $w$ written on the board with $zw$ , or replace the number $w$ with a different complex number $w'$ so that $$\max(\lvert\operatorname{Re} w\rvert, \lvert\operatorname{Im} w\rvert) = \max(\lvert\operatorname{Re} w'\rvert, \lvert\operatorname{Im} w'\rvert).$$ After some time, a positive real number less than $0.001$ is written on the board. The set of all $z$ for which this is possible forms a region $A$ in the complex plane. What is the area of $A$ ? So I'm baffled on where to begin this problem. I suppose defining $z$ = a + bi. I do not know where to go from here. I understand the max notation and the $\rm{Re}$ (Real) and $\rm{Im}$ (Imaginary) notation. This problem was sent to me by my math coach, so this may be from a book.","Let be a complex number. The number is written on a board. You perform a series of moves, where in each move you may either replace the number written on the board with , or replace the number with a different complex number so that After some time, a positive real number less than is written on the board. The set of all for which this is possible forms a region in the complex plane. What is the area of ? So I'm baffled on where to begin this problem. I suppose defining = a + bi. I do not know where to go from here. I understand the max notation and the (Real) and (Imaginary) notation. This problem was sent to me by my math coach, so this may be from a book.","z 1 w zw w w' \max(\lvert\operatorname{Re} w\rvert, \lvert\operatorname{Im} w\rvert) = \max(\lvert\operatorname{Re} w'\rvert, \lvert\operatorname{Im} w'\rvert). 0.001 z A A z \rm{Re} \rm{Im}","['abstract-algebra', 'complex-analysis', 'geometry', 'complex-numbers']"
29,Embedding of valued fields,Embedding of valued fields,,"I am reading this Lecture notes on the model theory of valued fields. On p.71, he states the following lemma: 6.12. Lemma. If $(K, v, \Gamma)$ is $\aleph_{1}$ -saturated, then the discrete valued field $(\dot{k}, v, \mathbb{Z} 1)$ is complete. which is left as an exercise. I cannot prove it on my own. My attempt: I know that $\mathcal{M}$ is saturated if every type over a subset of smaller cardinality than M is realized. But I do not know how to actually use that. I would apprecaite any help or reference to a paper or a book where this lemma is proven.","I am reading this Lecture notes on the model theory of valued fields. On p.71, he states the following lemma: 6.12. Lemma. If is -saturated, then the discrete valued field is complete. which is left as an exercise. I cannot prove it on my own. My attempt: I know that is saturated if every type over a subset of smaller cardinality than M is realized. But I do not know how to actually use that. I would apprecaite any help or reference to a paper or a book where this lemma is proven.","(K, v, \Gamma) \aleph_{1} (\dot{k}, v, \mathbb{Z} 1) \mathcal{M}","['abstract-algebra', 'logic', 'field-theory', 'extension-field']"
30,How to efficiently compute the minimal polynomial of a number expressed in radicals?,How to efficiently compute the minimal polynomial of a number expressed in radicals?,,"Preamble: I want to calculate the minimal polynomial of a number of the form $$x=\sum_{i=1}^k \pm a_i^{1/k_i}$$ Where the $a_i$ are algebraic numbers also of this form with a finite expression and $k_i$ are positive integers. I can think of three approaches, all with serious problems: Using the resultant repeatedly: Apply the algorithm recursively until you arrive at a root of a rational number, therefore we can assume that we have the minimal polynomial of all $a_i$ . Then compute the minimal polynomial of $\pm a_1^{1/k_1}\pm a_2^{1/k_2}$ using the resultant of their minimal polynomials then repeat for that with $a_3, a_4...a_k$ . Let $d_i$ be the degree of the minimal polynomial of $a_i^{1/k_i}$ . In general this method requires computing the determinant of square matrices of orders $d_1+d_2$ , $d_1+d_2+d_3,\ldots$ up to $d_1+d_2+\ldots+d_k$ . According to wikipedia calculating the determinant of a matrix of order $n$ has complexity of the order $O(n^{2.373})$ but practically it's faster to use the LU decomposition method (this is what, for example, the blaze c++ library does) which has a complexity of $O(n^3)$ . If we have $d_1=d_2=...=d$ then the complexity is $O((2d)^3+(3d)^3...(kd)^3)=O(d^3k^4)$ . Using an Integer relation algorithm: If we know that the degree of $x$ is $n$ then we can run for example the LLL algorithm on ${1,x,x^2...x^n}$ . Problems with that include: Determination of $n$ : in the previous example we can get an upper bound of $d^k$ (yikes) so running the LLL algorithm on this would have complexity $O(d^k)$ assuming I'm interpreting wikipedia correctly. If $n$ does not match an upper bound then we either need to run the algorithm again or factor the polynomial. Precision: if we calculate these values outwards starting at the rational constants (for example $\sqrt[3]{2-\sqrt{2}}\approx\sqrt[3]{0.5858}\approx 0.8367$ ) we run into the problem that each operation has a tendency to exacerbate the errors of the previous ones. This is particularly problematic when subtracting two large numbers whose difference is small. So we'd need bounds on the error $x^i\pm\epsilon$ where the algorithm still returns the correct result. Edit: since writing this, I found this blog post by Jack Coughlin which details how to estimate this error. This greatly mitigates the problem. Afterwards we'd need to check if the polynomial found really has $x$ as a root and whether it's irreducible (this is only necessary if we don't know $n$ in advance, then we'd have to use an upper bound). Trying some symbolic manipulation, especially as a middle step of another algorithm: In some cases isolating the term with the highest root might be helpful. For example $$x=\sqrt{a}+\sqrt[3]{b}$$ $$x^3-3\sqrt a x^2+3ax-a\sqrt a = b$$ $$(x^3+3ax-b)^2=9ax^4-6a^2x^2+a^3$$ I get the impression that this isn't efficient, so maybe we can use a lookup table for small exponents with little nesting. Applications Of course I want to implement this in a computer so it does all the number crunching for me. For example, recently I was trying to do a search over graphs composed of triangular lattices so what I did is set the vertices of the graph as points in $\mathbb C$ so I had to check whether several numbers of the form $\eta(a+b\omega)+\mu$ where $w=\frac{1-\sqrt{-3}}2$ are equal to each other with sympy but since it stores expressions ""as is"" there's no trivial way to check equality, so I needed to check if the minimal polynomial of $\eta_1(a_1+b_1\omega)+\mu_1-\eta_2(a_2+b_2\omega)-\mu_2$ was equal to the identity polynomial, which takes forever . Sympy uses the first algorithm by default and alternatively something involving Gröbner bases which I did not understand. Of course the object-oriented-programming approach to solving this problem is declaring a class which represents an algebraically closed structure that holds all the numbers you're gonna need (and you better foresee all of them!). For example in c++: // a member of a real or imaginary quadratic field // a+b*sqrt(n) where a,b,n are integers class quadratic_int {   int a;   int b;   int n;    quadratic_int operator+(quadratic_int p) const {     // implement addition   }   // ... }; This will be fast to perform operations on, but if you have a number which is not of the form $a+b\sqrt n$ you need to rewrite everything from scratch, plus if $n\equiv 1 \pmod 4$ , $a$ and $b$ need to be half integers, and if you wanted to perform operations on the field $\mathbb Q[\sqrt 2]$ instead the class should probably use a template type instead of ints...all of this complexity means that by the end the class will be over a hundred lines long. If we could compute minimal polynomials fast though, we could have a class called algebraic which internally represents a number as its minimal polynomial together with a way of distinguishing it from the other roots. Then its usage could be something like: algebraic a {""x^2-2"", 0}; // the 0th root of x^2-2 i.e. sqrt(2) algebraic b {""x^3-3"", 0}; // in 3^(1/3)  // x = 2^(1/2)+3^(1/3) // so x is an algebraic number of degree 6 // the internal representation of x holds the polynomial // t^6-6t^4-6t^3+12t^2-36t+1 // as a list of 7 integers (1,0,-6,-6,12,-36,1) plus the number of the root // which is an integer from 0 to 5 auto x = a + b; Where the 0th, 1st, etc roots are ordered say by absolute value and then by complex argument. This way we still get fast arithmetic without having to rewrite our class every time we need to work on a different algebraic structure. Statement of the question: What is the fastest algorithm to calculate the minimal polynomial of a number expressed in radicals? It should have good asymptotic complexity in the number of terms/the degree of the input. Performance for small entries is not so important because a look-up table can be implemented.","Preamble: I want to calculate the minimal polynomial of a number of the form Where the are algebraic numbers also of this form with a finite expression and are positive integers. I can think of three approaches, all with serious problems: Using the resultant repeatedly: Apply the algorithm recursively until you arrive at a root of a rational number, therefore we can assume that we have the minimal polynomial of all . Then compute the minimal polynomial of using the resultant of their minimal polynomials then repeat for that with . Let be the degree of the minimal polynomial of . In general this method requires computing the determinant of square matrices of orders , up to . According to wikipedia calculating the determinant of a matrix of order has complexity of the order but practically it's faster to use the LU decomposition method (this is what, for example, the blaze c++ library does) which has a complexity of . If we have then the complexity is . Using an Integer relation algorithm: If we know that the degree of is then we can run for example the LLL algorithm on . Problems with that include: Determination of : in the previous example we can get an upper bound of (yikes) so running the LLL algorithm on this would have complexity assuming I'm interpreting wikipedia correctly. If does not match an upper bound then we either need to run the algorithm again or factor the polynomial. Precision: if we calculate these values outwards starting at the rational constants (for example ) we run into the problem that each operation has a tendency to exacerbate the errors of the previous ones. This is particularly problematic when subtracting two large numbers whose difference is small. So we'd need bounds on the error where the algorithm still returns the correct result. Edit: since writing this, I found this blog post by Jack Coughlin which details how to estimate this error. This greatly mitigates the problem. Afterwards we'd need to check if the polynomial found really has as a root and whether it's irreducible (this is only necessary if we don't know in advance, then we'd have to use an upper bound). Trying some symbolic manipulation, especially as a middle step of another algorithm: In some cases isolating the term with the highest root might be helpful. For example I get the impression that this isn't efficient, so maybe we can use a lookup table for small exponents with little nesting. Applications Of course I want to implement this in a computer so it does all the number crunching for me. For example, recently I was trying to do a search over graphs composed of triangular lattices so what I did is set the vertices of the graph as points in so I had to check whether several numbers of the form where are equal to each other with sympy but since it stores expressions ""as is"" there's no trivial way to check equality, so I needed to check if the minimal polynomial of was equal to the identity polynomial, which takes forever . Sympy uses the first algorithm by default and alternatively something involving Gröbner bases which I did not understand. Of course the object-oriented-programming approach to solving this problem is declaring a class which represents an algebraically closed structure that holds all the numbers you're gonna need (and you better foresee all of them!). For example in c++: // a member of a real or imaginary quadratic field // a+b*sqrt(n) where a,b,n are integers class quadratic_int {   int a;   int b;   int n;    quadratic_int operator+(quadratic_int p) const {     // implement addition   }   // ... }; This will be fast to perform operations on, but if you have a number which is not of the form you need to rewrite everything from scratch, plus if , and need to be half integers, and if you wanted to perform operations on the field instead the class should probably use a template type instead of ints...all of this complexity means that by the end the class will be over a hundred lines long. If we could compute minimal polynomials fast though, we could have a class called algebraic which internally represents a number as its minimal polynomial together with a way of distinguishing it from the other roots. Then its usage could be something like: algebraic a {""x^2-2"", 0}; // the 0th root of x^2-2 i.e. sqrt(2) algebraic b {""x^3-3"", 0}; // in 3^(1/3)  // x = 2^(1/2)+3^(1/3) // so x is an algebraic number of degree 6 // the internal representation of x holds the polynomial // t^6-6t^4-6t^3+12t^2-36t+1 // as a list of 7 integers (1,0,-6,-6,12,-36,1) plus the number of the root // which is an integer from 0 to 5 auto x = a + b; Where the 0th, 1st, etc roots are ordered say by absolute value and then by complex argument. This way we still get fast arithmetic without having to rewrite our class every time we need to work on a different algebraic structure. Statement of the question: What is the fastest algorithm to calculate the minimal polynomial of a number expressed in radicals? It should have good asymptotic complexity in the number of terms/the degree of the input. Performance for small entries is not so important because a look-up table can be implemented.","x=\sum_{i=1}^k \pm a_i^{1/k_i} a_i k_i a_i \pm a_1^{1/k_1}\pm a_2^{1/k_2} a_3, a_4...a_k d_i a_i^{1/k_i} d_1+d_2 d_1+d_2+d_3,\ldots d_1+d_2+\ldots+d_k n O(n^{2.373}) O(n^3) d_1=d_2=...=d O((2d)^3+(3d)^3...(kd)^3)=O(d^3k^4) x n {1,x,x^2...x^n} n d^k O(d^k) n \sqrt[3]{2-\sqrt{2}}\approx\sqrt[3]{0.5858}\approx 0.8367 x^i\pm\epsilon x n x=\sqrt{a}+\sqrt[3]{b} x^3-3\sqrt a x^2+3ax-a\sqrt a = b (x^3+3ax-b)^2=9ax^4-6a^2x^2+a^3 \mathbb C \eta(a+b\omega)+\mu w=\frac{1-\sqrt{-3}}2 \eta_1(a_1+b_1\omega)+\mu_1-\eta_2(a_2+b_2\omega)-\mu_2 a+b\sqrt n n\equiv 1 \pmod 4 a b \mathbb Q[\sqrt 2]","['abstract-algebra', 'numerical-methods', 'computational-mathematics', 'minimal-polynomials', 'computer-algebra-systems']"
31,find total number of maximal ideals in $\mathbb{Q}[x]/\langle x^4-1\rangle$.,find total number of maximal ideals in .,\mathbb{Q}[x]/\langle x^4-1\rangle,"find total number of maximal ideals in $\mathbb{Q}[x]/\langle x^4-1\rangle$. Let $J=\langle x^4-1\rangle$, $R=\mathbb{Q}[x]$. I want to use $(R/J)/(I/J)\simeq R/I$, where $I $ is ideal of $R$ which contain $J$. Then $R/I$ is field, and $R$ is a principal ideal domain. Let $I=\langle f(x) \rangle$ hence $f(x)$ must be irreducible in $R$, so only choice for $f(x)$ are $x-1,x+1,x^2+1$. So answer should be $3$. Is it right explanation? and better method thanks in advance","find total number of maximal ideals in $\mathbb{Q}[x]/\langle x^4-1\rangle$. Let $J=\langle x^4-1\rangle$, $R=\mathbb{Q}[x]$. I want to use $(R/J)/(I/J)\simeq R/I$, where $I $ is ideal of $R$ which contain $J$. Then $R/I$ is field, and $R$ is a principal ideal domain. Let $I=\langle f(x) \rangle$ hence $f(x)$ must be irreducible in $R$, so only choice for $f(x)$ are $x-1,x+1,x^2+1$. So answer should be $3$. Is it right explanation? and better method thanks in advance",,"['abstract-algebra', 'proof-verification', 'ring-theory', 'field-theory']"
32,Brauer Group of $\mathbb{Q}_2$,Brauer Group of,\mathbb{Q}_2,"I have read that the Brauer Group of any local field is $\mathbb{Q}/\mathbb{Z}$, and I want to see this for $\mathbb{Q}_2$. I'm confused because it appears there are $3$ division algebras of dimension $4$, namely $\mathbb{Q}_2(u, v)/(u^2=a, v^2=b, uv=-vu)$ for $a, b$ are any pair among $2, 3, 5$. What am I missing?","I have read that the Brauer Group of any local field is $\mathbb{Q}/\mathbb{Z}$, and I want to see this for $\mathbb{Q}_2$. I'm confused because it appears there are $3$ division algebras of dimension $4$, namely $\mathbb{Q}_2(u, v)/(u^2=a, v^2=b, uv=-vu)$ for $a, b$ are any pair among $2, 3, 5$. What am I missing?",,"['abstract-algebra', 'field-theory', 'class-field-theory']"
33,Only five solvable quintic equations of the form $x^5+ax^2+b=0$? What are their solutions?,Only five solvable quintic equations of the form ? What are their solutions?,x^5+ax^2+b=0,"According to Wikipedia there are only five solvable quintic equations of the form $x^5+ax^2+b=0,~~a,b \in \mathbb{Q}$ (up to a scaling constant $s$ ). $$x^5-2s^3x^2-\frac{s^5}{5}=0 $$ $$ x^5-100s^3x^2-1000s^5=0 $$ $$x^5-5s^3x^2-3s^5=0  $$ $$x^5-5s^3x^2+15s^5=0  $$ $$ x^5-25s^3x^2-300s^5=0 $$ But the source of this claim is a web-page (even if it's Harvard), not an article. Thus, my questions are: Why is this true, and what is the original source of this knowledge? What are the solutions to these equations (in radical form, or possibly trigonometric/hyperbolic form)? Edit I found the proper citation on the linked Web-Page (for which I sincerely thank the author, if they ever visit this post). The paper is On Solvable Quintics $X^5+aX+b$ and $X^5+aX^2+b$ by Blair K. Spearman and Kenneth S. Williams and the full text is available with open-access. I will see if my questions are answered by this paper and update the post.","According to Wikipedia there are only five solvable quintic equations of the form (up to a scaling constant ). But the source of this claim is a web-page (even if it's Harvard), not an article. Thus, my questions are: Why is this true, and what is the original source of this knowledge? What are the solutions to these equations (in radical form, or possibly trigonometric/hyperbolic form)? Edit I found the proper citation on the linked Web-Page (for which I sincerely thank the author, if they ever visit this post). The paper is On Solvable Quintics and by Blair K. Spearman and Kenneth S. Williams and the full text is available with open-access. I will see if my questions are answered by this paper and update the post.","x^5+ax^2+b=0,~~a,b \in \mathbb{Q} s x^5-2s^3x^2-\frac{s^5}{5}=0   x^5-100s^3x^2-1000s^5=0  x^5-5s^3x^2-3s^5=0   x^5-5s^3x^2+15s^5=0    x^5-25s^3x^2-300s^5=0  X^5+aX+b X^5+aX^2+b","['abstract-algebra', 'polynomials', 'roots', 'irreducible-polynomials']"
34,Example of algebraic structure that is non distributive for BOTH distributive laws and how to do computation in them?,Example of algebraic structure that is non distributive for BOTH distributive laws and how to do computation in them?,,"(Apologies if this one sounds like I have not done much research, or I did not aware already have an answer, but I have been searching ev er yw h er e and all of these structures presented here, even including the highly exotic division by zero proposals such as Wheels and Meadows , they all seemed to either retain the distributive law (both side) or at least retain the right distributive law So my question then becomes Q1 Any famous or widely used example of an algebraic structure where BOTH right and left distributive law fails? Q2. Suppose I have an algebraic structure on the set $S$ with the following $$S=\left\{a,b,c,d\right\}$$ with the axiom $$a^2=a$$ with right addition defined as $$+:(a_1,a_2)\rightarrow(a_1+a_2),a_1,a_2 \in S$$ and some left operation $$\circ : (k,a)\rightarrow (k^2 a), k,a \in S$$ I am then interested in computing this entry in the Cayley table for $\circ$ $$a \circ(b+c)$$ How should I approach it since I don't have distributive laws (BOTH left and right) that allow me to simplify this expression to this and apply the axioms I know about $S$ $$a \circ b+a \circ c$$","(Apologies if this one sounds like I have not done much research, or I did not aware already have an answer, but I have been searching ev er yw h er e and all of these structures presented here, even including the highly exotic division by zero proposals such as Wheels and Meadows , they all seemed to either retain the distributive law (both side) or at least retain the right distributive law So my question then becomes Q1 Any famous or widely used example of an algebraic structure where BOTH right and left distributive law fails? Q2. Suppose I have an algebraic structure on the set with the following with the axiom with right addition defined as and some left operation I am then interested in computing this entry in the Cayley table for How should I approach it since I don't have distributive laws (BOTH left and right) that allow me to simplify this expression to this and apply the axioms I know about","S S=\left\{a,b,c,d\right\} a^2=a +:(a_1,a_2)\rightarrow(a_1+a_2),a_1,a_2 \in S \circ : (k,a)\rightarrow (k^2 a), k,a \in S \circ a \circ(b+c) S a \circ b+a \circ c",['abstract-algebra']
35,Cogroup structures on the profinite completion of the integers,Cogroup structures on the profinite completion of the integers,,"Let $\mathsf{ProFinGrp}$ be the category of profinite groups (with continuous homomorphisms). This is equivalent to the Pro-category of $\mathsf{FinGrp}$. Notice that $\widehat{\mathbb{Z}} = \lim_{n>0} \mathbb{Z}/n\mathbb{Z}$ is a cogroup object, since it represents the forgetful functor $U : \mathsf{ProFinGrp} \to \mathsf{Set}$. Although $\mathsf{ProFinGrp}$ has no coproducts (?), the coproduct $\widehat{\mathbb{Z}} \sqcup \widehat{\mathbb{Z}}$ exists, it coincides by formal nonsense with $\widehat{F_2}$, where $F_2$ is the free group on two generators $x,y$. Let us denote the generator of $\mathbb{Z}=F_1$ by $x$. Then the comultiplication of $\widehat{F_1}$ is given by $\widehat{F_1} \to \widehat{F_2}$, $x \mapsto x * y$. There is another cogroup structure on $\widehat{F_1}$, corresponding to the opposite group. The corresponding comultiplication is $\widehat{F_1} \to \widehat{F_2}$, $x \mapsto y * x$. Question. Are these two the only cogroup structures on $\widehat{\mathbb{Z}}$? Here is a down-to-earth description what a cogroup structure on $\widehat{\mathbb{Z}}$ is: It is an element $m(x,y) \in \widehat{F_2}$ (a sort of ""profinite word"" in $x$ and $y$) such that $m(x,1)=m(1,x)=x$ in $\widehat{F_1}$ $m(x,m(y,z))=m(m(x,y),z)$ in $\widehat{F_3}$ There is some $i \in \widehat{F_1}$ such that $m(x,i(x))=m(i(x),x)=1$. This is very similar to the definition of a one-dimensional formal group law (which is a cogroup structure on $R[[t]]$ in the category of complete topological $R$-algebras). Background: An affirmative answer would answer math.SE/656279 . It is known that $\mathbb{Z} \in \mathsf{Grp}$ has only two cogroup structures.","Let $\mathsf{ProFinGrp}$ be the category of profinite groups (with continuous homomorphisms). This is equivalent to the Pro-category of $\mathsf{FinGrp}$. Notice that $\widehat{\mathbb{Z}} = \lim_{n>0} \mathbb{Z}/n\mathbb{Z}$ is a cogroup object, since it represents the forgetful functor $U : \mathsf{ProFinGrp} \to \mathsf{Set}$. Although $\mathsf{ProFinGrp}$ has no coproducts (?), the coproduct $\widehat{\mathbb{Z}} \sqcup \widehat{\mathbb{Z}}$ exists, it coincides by formal nonsense with $\widehat{F_2}$, where $F_2$ is the free group on two generators $x,y$. Let us denote the generator of $\mathbb{Z}=F_1$ by $x$. Then the comultiplication of $\widehat{F_1}$ is given by $\widehat{F_1} \to \widehat{F_2}$, $x \mapsto x * y$. There is another cogroup structure on $\widehat{F_1}$, corresponding to the opposite group. The corresponding comultiplication is $\widehat{F_1} \to \widehat{F_2}$, $x \mapsto y * x$. Question. Are these two the only cogroup structures on $\widehat{\mathbb{Z}}$? Here is a down-to-earth description what a cogroup structure on $\widehat{\mathbb{Z}}$ is: It is an element $m(x,y) \in \widehat{F_2}$ (a sort of ""profinite word"" in $x$ and $y$) such that $m(x,1)=m(1,x)=x$ in $\widehat{F_1}$ $m(x,m(y,z))=m(m(x,y),z)$ in $\widehat{F_3}$ There is some $i \in \widehat{F_1}$ such that $m(x,i(x))=m(i(x),x)=1$. This is very similar to the definition of a one-dimensional formal group law (which is a cogroup structure on $R[[t]]$ in the category of complete topological $R$-algebras). Background: An affirmative answer would answer math.SE/656279 . It is known that $\mathbb{Z} \in \mathsf{Grp}$ has only two cogroup structures.",,"['abstract-algebra', 'group-theory', 'category-theory', 'profinite-groups', 'formal-groups']"
36,How many Groups there are on a finite set?,How many Groups there are on a finite set?,,"Let say cardinality of set S is $n=|S|$. We know that there are $n^{n^2}$ all binary operations on that set. To find out how many groups can be created by this set and by those operations, we need not only to know how many associative operations there are on that finite set. But also this set and given operation must satisfy specific axioms: closure, associativity, identity and invertibility. So how find out how many different groups can be created on that finite countable set?","Let say cardinality of set S is $n=|S|$. We know that there are $n^{n^2}$ all binary operations on that set. To find out how many groups can be created by this set and by those operations, we need not only to know how many associative operations there are on that finite set. But also this set and given operation must satisfy specific axioms: closure, associativity, identity and invertibility. So how find out how many different groups can be created on that finite countable set?",,"['abstract-algebra', 'group-theory', 'finite-groups', 'binary-operations', 'groups-enumeration']"
37,"All non abelian groups of order $56$, when $\mathbb Z_7\triangleleft G$","All non abelian groups of order , when",56 \mathbb Z_7\triangleleft G,"I am calculating all non abelian groups of order $56$ when $\mathbb Z_7$ is a normal subgroup ( Given in the exercise $7$ of §5.5, Dummit Foote). I have searched for related posts on this site but couldn't able to find something that discusses on this particular topic. It would be great if someone provide me link on this site discussing on this topic . However, here is my progress. Let $S$ be the Sylow $2$ subgroup and throughout the discussion I will call $\mathbb Z_7=\left<d\right>$ , where $d=1$ . My calculations are as follows : $\boxed{\text{case }(1) :  S=\mathbb{Z_2}\times\mathbb{Z_2}\times\mathbb{Z_2}=\left<a,b,c\right>}$ Since all elements of $\mathbb{Z_2}\times\mathbb{Z_2}\times\mathbb{Z_2}=\left<a,b,c\right>$ has order $2$ , so for any homomorphism $\phi\colon\left<a,b,c\right>\to Aut(\mathbb Z_7)$ , each of $\phi(a),\phi(b),\phi(c)$ has order either $1$ or $2$ . The element of $Aut(\mathbb Z_7)$ having order $1$ is the identity  automorphism $\alpha_1\colon1\mapsto1$ and that having order $2$ is the automorphism $\alpha_2\colon1\mapsto6$ because $1\mapsto6\mapsto6\cdot6=36=1$ in $\mathbb Z_7$ . So the possible homomorphisms are : $\begin{array}{c|c}\phi_1&\phi_2\\\hline  a\mapsto\alpha_1 & a\mapsto  \alpha_1\\b\mapsto\alpha_1&  b\mapsto\alpha_2\\c\mapsto \alpha_2&c\mapsto\alpha_2\end{array}\tag*{}$ In case of the homomorphism $\phi_1$ , $a\cdot d=d, b\cdot d=d, c\cdot d=d^{-1}$ , where the dot represents the action on $d$ . $\therefore ada^{-1}=d,bdb^{-1}=d,cdc^{-1}=d^{-1}\text{ with  }a^2=b^2=c^2=d^7=1$ Since $a$ and $b$ centralize both $c$ and $d$ , so in this case the group must be $\begin{align}G_{\phi_1}&=\left<a|a^2=1\right>\times\left<b|b^2=1\right>\times\left<c,  d|c^2=d^7=1,cdc^{-1}=d^{-1}\right>\\&=\mathbb Z_2\times\mathbb Z_2\times D_{14}\\&=\mathbb Z_2\times D_{28}\end{align}$ In case of the homomorphism $\phi_2$ , $a\cdot d=d, b\cdot d=d^{-1}, c\cdot d=d^{-1}$ . $\therefore  ada^{-1}=d,bdb^{-1}=d^{-1},cdc^{-1}=d^{-1}\text{ with   }a^2=b^2=c^2=d^7=1$ . But the group $G_{\phi_2}$ formed by these relations is exactly same as the above group : if we invert all the elements of $G_{\phi_1}$ , we get $G_{\phi_2}$ . So $$G_{\phi_1}\cong G_{\phi_2}$$ $\boxed{\text{case }(2) : S=\mathbb{Z_4}\times\mathbb{Z_2}=\left<a,b\right>}$ By the same argument in case $1$ , the possible homomorphisms $\mathbb Z_4\times\mathbb Z_2\to Aut(\mathbb Z_7)$ are : $\begin{array}{c|c|c}\psi_1&\psi_2&\psi_3\\\hline a\mapsto\alpha_1& a\mapsto\alpha_2&a\mapsto \alpha_2\\b\mapsto\alpha_2& b\mapsto\alpha_2&b\mapsto \alpha_1\end{array}\tag*{}$ In case of the homomorphism $\psi_1$ , $G_{\psi_1}=\left<a,b,d|a^4=b^2=d^7=1, ab=ba ,ada^{-1}=d, bdb^{-1}=d^{-1}\right>$ Hence it  is easily seen to be $$G_{\psi_1}\cong\mathbb Z_4\times D_{14}$$ In case of the homomorphism $\psi_2$ , $G_{\psi_2}=\left<a,b,d|a^4=b^2=d^7=1,ab=ba ,ada^{-1}=d^{-1},bdb^{-1}=d^{-1}\right>$ Now, $(ab) \cdot (d)=(ab)d(ab)^{-1}=a(bdb^{-1})a^{-1}=ad^{-1}a^{-1}=d$ . And order of $ab$ is $4$ and $ab$ centralises both $b$ and $d$ . So the relations determined by $\psi_2$ are same as the relations determined by $\psi_1$ . So, $G_{\psi_2}=\left<ab\right>\times\left<b,d\right>=\mathbb Z_4\times D_{14}\cong G_{\psi_1}$ And the homomorphism $\psi_3$ produces the group $\begin{align}G_{\psi_3}&=\left<a,b,d|a^4=b^2=d^7=1, ab=ba ,ada^{-1}=d^{-1}, bdb^{-1}=d\right>\\&=\left<b|b^2=1\right>\times\left<a,d|a^4=d^7=1,ada^{-1}=d^{-1}\right>\\&=\mathbb Z_2\times\left<a,d|a^4=d^7=1,ada^{-1}=d^{-1}\right>\end{align}$ $\boxed{\text{case }(3) : S=\mathbb{Z_8}=\left<a\right>}$ The only possible homomorphism $\pi\colon\mathbb Z_8\to Aut(\mathbb Z_7)$ that gives rise to a non abelian group is : $\pi\colon a\mapsto\alpha_2$ . Here the group is $G_\pi=\mathbb Z_7\rtimes_{\pi}\mathbb Z_8$ . $\boxed{\text{case }(4) : S=\mathbb Q_8=\left<i ,j\right>}$ Since here $S$ is non abelian, we need to take the trivial homomorphism $S\to Aut(\mathbb Z_7)$ into account. The possible homomorphisms are : $\begin{array}{c|c|c|c}\theta_1&\theta_2&\theta_3&\theta_4(\text{ trivial })\\\hline  i\mapsto\alpha_1& i\mapsto\alpha_2&i\mapsto \alpha_2&i\mapsto\alpha_1\\j\mapsto\alpha_2& j\mapsto\alpha_1&j\mapsto \alpha_2&j\mapsto\alpha_1\end{array}\tag*{}$ In case of the homomorphism $\theta_1$ , the relations between the elements are : $idi^{-1}=d, jdj^{-1}=d^{-1}, i^2=j^2=-1, d^7=1, ij=-ji$ I don't know the name of this  group $G_{\theta_1}$ . I would like to know if there is some name or explicit notation for this group. The group $G_{\theta_2}$ formed by the homomorphism $\theta_2$ is isomorphic to $G_{\theta_1}$ because we are just interchanging the roles of $i$ and $j$ . In case of the homomorphism $\theta_3$ , we have the relations : $idi^{-1}=d^{-1}, jdj^{-1}=d^{-1}, i^2=j^2=-1, d^7=1, ij=-ji$ . Now in the relation $j\cdot d=jdj^{-1}= d^{-1}$ , if we hit by $i$ , $(ij)d(ij)^{-1}=id^{-1}i^{-1}=d$ . So writing $ij=k$ the relations determined by $\theta_3$ are $idi^{-1}=d^{-1}, kdk^{-1}=d$ , which is exactly the same as the group determined by $\theta_1$ or $\theta_2$ . So $G_{\theta_1}\cong G_{\theta_2}\cong G_{\theta_3}$ . The trivial homomorphism $\theta_4\colon\mathbb Q_8\to Aut(\mathbb Z_7)$ corresponds to $G_{\theta_4}=\mathbb Q_8\times\mathbb Z_7$ $\boxed{\text{case }(5) : S=D_8=\left<r ,s\right>}$ Here the possible homomorphisms are : $\begin{array}{c|c|c|c}\eta_1&\eta_2&\eta_3&\eta_4(\text{ trivial })\\\hline r\mapsto\alpha_1& r\mapsto\alpha_2&r\mapsto \alpha_2&r\mapsto\alpha_1\\s\mapsto\alpha_2&   s\mapsto\alpha_1&s\mapsto \alpha_2&s\mapsto\alpha_1\end{array}\tag*{}$ The corresponding groups are as alex mentioned in the comment : $\begin{align}G_{\eta_1}&=\{r, s,d|r^4=s^2=d^7=1,srs=r^{-1},rdr^{-1}=d,sds=d^{-1}\}\\&=\{r,s,d|(rd) ^{28}=s^2=1,s(rd)\cdot s(rd)=s(dr)\cdot s(rd)=sd(rsr)d=sd(s)d=(sds)d=d^{-1}d=1\}\\&=D_{56}\end{align}$ $G_{\eta_2}=\{r, s,d|r^4=s^2=d^7=1,srs=r^{-1},rdr^{-1}=d^{-1},sds=d\}$ and $G_{\eta_3}=\{r, s, d|r^4=s^2=d^7=1,srs=r^{-1},rdr^{-1}=d^{-1},sds=d^{-1}\}$ In $G_{\eta_2}$ , if we hit the relation $s\cdot d=sds^{-1}=d$ by $r$ , we get, $(rs)\cdot d=(rs)d(rs)^{-1}=r(sds^{-1})r^{-1}=rdr^{-1}=d^{-1}$ So $G_{\eta_2}=\{r, s,d|r^4=s^2=d^7=1,srs=r^{-1},rdr^{-1}=d^{-1},(rs)d(rs)^{-1}=d^{-1}\}\cong G_{\eta_3}$ And here too I come up with the group $G_{\eta_2}$ for the first time. So it would be great if someone please help me to understand these groups. The trivial homomorphism $\eta_4\colon D_8\to Aut(\mathbb Z_7)$ gives rise to the group $G_{\eta_4}$ which is the direct product $D_8\times\mathbb Z_7$ Most importantly, I would like to know whether there are any flaws in my calculations. Thank you.","I am calculating all non abelian groups of order when is a normal subgroup ( Given in the exercise of §5.5, Dummit Foote). I have searched for related posts on this site but couldn't able to find something that discusses on this particular topic. It would be great if someone provide me link on this site discussing on this topic . However, here is my progress. Let be the Sylow subgroup and throughout the discussion I will call , where . My calculations are as follows : Since all elements of has order , so for any homomorphism , each of has order either or . The element of having order is the identity  automorphism and that having order is the automorphism because in . So the possible homomorphisms are : In case of the homomorphism , , where the dot represents the action on . Since and centralize both and , so in this case the group must be In case of the homomorphism , . . But the group formed by these relations is exactly same as the above group : if we invert all the elements of , we get . So By the same argument in case , the possible homomorphisms are : In case of the homomorphism , Hence it  is easily seen to be In case of the homomorphism , Now, . And order of is and centralises both and . So the relations determined by are same as the relations determined by . So, And the homomorphism produces the group The only possible homomorphism that gives rise to a non abelian group is : . Here the group is . Since here is non abelian, we need to take the trivial homomorphism into account. The possible homomorphisms are : In case of the homomorphism , the relations between the elements are : I don't know the name of this  group . I would like to know if there is some name or explicit notation for this group. The group formed by the homomorphism is isomorphic to because we are just interchanging the roles of and . In case of the homomorphism , we have the relations : . Now in the relation , if we hit by , . So writing the relations determined by are , which is exactly the same as the group determined by or . So . The trivial homomorphism corresponds to Here the possible homomorphisms are : The corresponding groups are as alex mentioned in the comment : and In , if we hit the relation by , we get, So And here too I come up with the group for the first time. So it would be great if someone please help me to understand these groups. The trivial homomorphism gives rise to the group which is the direct product Most importantly, I would like to know whether there are any flaws in my calculations. Thank you.","56 \mathbb Z_7 7 S 2 \mathbb Z_7=\left<d\right> d=1 \boxed{\text{case }(1) :  S=\mathbb{Z_2}\times\mathbb{Z_2}\times\mathbb{Z_2}=\left<a,b,c\right>} \mathbb{Z_2}\times\mathbb{Z_2}\times\mathbb{Z_2}=\left<a,b,c\right> 2 \phi\colon\left<a,b,c\right>\to Aut(\mathbb Z_7) \phi(a),\phi(b),\phi(c) 1 2 Aut(\mathbb Z_7) 1 \alpha_1\colon1\mapsto1 2 \alpha_2\colon1\mapsto6 1\mapsto6\mapsto6\cdot6=36=1 \mathbb Z_7 \begin{array}{c|c}\phi_1&\phi_2\\\hline
 a\mapsto\alpha_1 & a\mapsto  \alpha_1\\b\mapsto\alpha_1&
 b\mapsto\alpha_2\\c\mapsto \alpha_2&c\mapsto\alpha_2\end{array}\tag*{} \phi_1 a\cdot d=d, b\cdot d=d, c\cdot d=d^{-1} d \therefore ada^{-1}=d,bdb^{-1}=d,cdc^{-1}=d^{-1}\text{ with  }a^2=b^2=c^2=d^7=1 a b c d \begin{align}G_{\phi_1}&=\left<a|a^2=1\right>\times\left<b|b^2=1\right>\times\left<c,  d|c^2=d^7=1,cdc^{-1}=d^{-1}\right>\\&=\mathbb Z_2\times\mathbb Z_2\times D_{14}\\&=\mathbb Z_2\times D_{28}\end{align} \phi_2 a\cdot d=d, b\cdot d=d^{-1}, c\cdot d=d^{-1} \therefore
 ada^{-1}=d,bdb^{-1}=d^{-1},cdc^{-1}=d^{-1}\text{ with 
 }a^2=b^2=c^2=d^7=1 G_{\phi_2} G_{\phi_1} G_{\phi_2} G_{\phi_1}\cong G_{\phi_2} \boxed{\text{case }(2) : S=\mathbb{Z_4}\times\mathbb{Z_2}=\left<a,b\right>} 1 \mathbb Z_4\times\mathbb Z_2\to Aut(\mathbb Z_7) \begin{array}{c|c|c}\psi_1&\psi_2&\psi_3\\\hline a\mapsto\alpha_1& a\mapsto\alpha_2&a\mapsto \alpha_2\\b\mapsto\alpha_2& b\mapsto\alpha_2&b\mapsto \alpha_1\end{array}\tag*{} \psi_1 G_{\psi_1}=\left<a,b,d|a^4=b^2=d^7=1, ab=ba ,ada^{-1}=d, bdb^{-1}=d^{-1}\right> G_{\psi_1}\cong\mathbb Z_4\times D_{14} \psi_2 G_{\psi_2}=\left<a,b,d|a^4=b^2=d^7=1,ab=ba ,ada^{-1}=d^{-1},bdb^{-1}=d^{-1}\right> (ab) \cdot (d)=(ab)d(ab)^{-1}=a(bdb^{-1})a^{-1}=ad^{-1}a^{-1}=d ab 4 ab b d \psi_2 \psi_1 G_{\psi_2}=\left<ab\right>\times\left<b,d\right>=\mathbb Z_4\times D_{14}\cong G_{\psi_1} \psi_3 \begin{align}G_{\psi_3}&=\left<a,b,d|a^4=b^2=d^7=1, ab=ba ,ada^{-1}=d^{-1}, bdb^{-1}=d\right>\\&=\left<b|b^2=1\right>\times\left<a,d|a^4=d^7=1,ada^{-1}=d^{-1}\right>\\&=\mathbb Z_2\times\left<a,d|a^4=d^7=1,ada^{-1}=d^{-1}\right>\end{align} \boxed{\text{case }(3) : S=\mathbb{Z_8}=\left<a\right>} \pi\colon\mathbb Z_8\to Aut(\mathbb Z_7) \pi\colon a\mapsto\alpha_2 G_\pi=\mathbb Z_7\rtimes_{\pi}\mathbb Z_8 \boxed{\text{case }(4) : S=\mathbb Q_8=\left<i ,j\right>} S S\to Aut(\mathbb Z_7) \begin{array}{c|c|c|c}\theta_1&\theta_2&\theta_3&\theta_4(\text{ trivial })\\\hline
 i\mapsto\alpha_1& i\mapsto\alpha_2&i\mapsto
\alpha_2&i\mapsto\alpha_1\\j\mapsto\alpha_2& j\mapsto\alpha_1&j\mapsto
\alpha_2&j\mapsto\alpha_1\end{array}\tag*{} \theta_1 idi^{-1}=d, jdj^{-1}=d^{-1}, i^2=j^2=-1, d^7=1, ij=-ji G_{\theta_1} G_{\theta_2} \theta_2 G_{\theta_1} i j \theta_3 idi^{-1}=d^{-1}, jdj^{-1}=d^{-1}, i^2=j^2=-1, d^7=1, ij=-ji j\cdot d=jdj^{-1}= d^{-1} i (ij)d(ij)^{-1}=id^{-1}i^{-1}=d ij=k \theta_3 idi^{-1}=d^{-1}, kdk^{-1}=d \theta_1 \theta_2 G_{\theta_1}\cong G_{\theta_2}\cong G_{\theta_3} \theta_4\colon\mathbb Q_8\to Aut(\mathbb Z_7) G_{\theta_4}=\mathbb Q_8\times\mathbb Z_7 \boxed{\text{case }(5) : S=D_8=\left<r ,s\right>} \begin{array}{c|c|c|c}\eta_1&\eta_2&\eta_3&\eta_4(\text{ trivial })\\\hline r\mapsto\alpha_1& r\mapsto\alpha_2&r\mapsto \alpha_2&r\mapsto\alpha_1\\s\mapsto\alpha_2& 
 s\mapsto\alpha_1&s\mapsto \alpha_2&s\mapsto\alpha_1\end{array}\tag*{} \begin{align}G_{\eta_1}&=\{r, s,d|r^4=s^2=d^7=1,srs=r^{-1},rdr^{-1}=d,sds=d^{-1}\}\\&=\{r,s,d|(rd) ^{28}=s^2=1,s(rd)\cdot s(rd)=s(dr)\cdot s(rd)=sd(rsr)d=sd(s)d=(sds)d=d^{-1}d=1\}\\&=D_{56}\end{align} G_{\eta_2}=\{r, s,d|r^4=s^2=d^7=1,srs=r^{-1},rdr^{-1}=d^{-1},sds=d\} G_{\eta_3}=\{r, s, d|r^4=s^2=d^7=1,srs=r^{-1},rdr^{-1}=d^{-1},sds=d^{-1}\} G_{\eta_2} s\cdot d=sds^{-1}=d r (rs)\cdot d=(rs)d(rs)^{-1}=r(sds^{-1})r^{-1}=rdr^{-1}=d^{-1} G_{\eta_2}=\{r, s,d|r^4=s^2=d^7=1,srs=r^{-1},rdr^{-1}=d^{-1},(rs)d(rs)^{-1}=d^{-1}\}\cong G_{\eta_3} G_{\eta_2} \eta_4\colon D_8\to Aut(\mathbb Z_7) G_{\eta_4} D_8\times\mathbb Z_7","['abstract-algebra', 'finite-groups', 'solution-verification', 'sylow-theory', 'semidirect-product']"
38,On group varieties and numbers,On group varieties and numbers,,"Suppose $\mathfrak{U}$ is a group variety. Let’s define $N_{\mathfrak{U}} \subset \mathbb{N}$ as a such set of numbers, that for any finite group $G$ , $|G| \in N_{\mathfrak{U}}$ implies $G \in \mathfrak{U}$ . Examples: If $\mathfrak{O}$ is the variety of all groups, then $N_{\mathfrak{O}} = \mathbb{N}$ . If $\mathfrak{B}_m$ is the variety of all groups of exponent $m$ , then $N_{\mathfrak{B}_m}$ is the set of all divisors of $m$ If $\mathfrak{N}_c$ is the variety of all groups of nilpotency class $c$ , then $N_{\mathfrak{N}_c}$ is the set of all numbers $n=p_1^{e_1}\cdots p_m^{e_m}$ with $p_i^k\not\equiv 1(\mod p_j)$ for $i,j\in\{1,\ldots,m\}$ and $1\leqslant k\leqslant e_i$ , and $e_i \leq c + 1$ for $i\in\{1,\ldots,m\}$ . If $\mathfrak{U}$ and $\mathfrak{V}$ are two varieties, then $N_{\mathfrak{U}\cap\mathfrak{V}} = N_{\mathfrak{U}} \cap N_{\mathfrak{V}}$ My question is: Does there exist some number-theoretic characterisation of all such subsets $N \subset \mathbb{N}$ , such that $N = N_{\mathfrak{U}}$ for some variety $\mathfrak{U}$ ? Any $N_{\mathfrak{U}}$ satisfies the property: If $a \in N_{\mathfrak{U}}$ and $b | a$ , then $b \in N_{\mathfrak{U}}$ Suppose $|G| = b$ and $G \notin \mathfrak{U}$ . Then $G \times C_{\frac{a}{b}} \notin \mathfrak{U}$ . If $\exists n \in \mathbb{N}$ , such that $\forall k \in \mathbb{N}$ $n^k \in N_{\mathfrak{U}}$ , then $\mathfrak{U} = \mathfrak{O}$ . By previous lemma, we can assume without loss of generality, that $n = p$ is prime. The only variety, that contains all $p$ -groups for a fixed prime $p$ is $\mathfrak{O}$ However, I am not sure, whether those two conditions are sufficient to characterise all such sets or not. This question was inspired by this MO question","Suppose is a group variety. Let’s define as a such set of numbers, that for any finite group , implies . Examples: If is the variety of all groups, then . If is the variety of all groups of exponent , then is the set of all divisors of If is the variety of all groups of nilpotency class , then is the set of all numbers with for and , and for . If and are two varieties, then My question is: Does there exist some number-theoretic characterisation of all such subsets , such that for some variety ? Any satisfies the property: If and , then Suppose and . Then . If , such that , then . By previous lemma, we can assume without loss of generality, that is prime. The only variety, that contains all -groups for a fixed prime is However, I am not sure, whether those two conditions are sufficient to characterise all such sets or not. This question was inspired by this MO question","\mathfrak{U} N_{\mathfrak{U}} \subset \mathbb{N} G |G| \in N_{\mathfrak{U}} G \in \mathfrak{U} \mathfrak{O} N_{\mathfrak{O}} = \mathbb{N} \mathfrak{B}_m m N_{\mathfrak{B}_m} m \mathfrak{N}_c c N_{\mathfrak{N}_c} n=p_1^{e_1}\cdots p_m^{e_m} p_i^k\not\equiv 1(\mod p_j) i,j\in\{1,\ldots,m\} 1\leqslant k\leqslant e_i e_i \leq c + 1 i\in\{1,\ldots,m\} \mathfrak{U} \mathfrak{V} N_{\mathfrak{U}\cap\mathfrak{V}} = N_{\mathfrak{U}} \cap N_{\mathfrak{V}} N \subset \mathbb{N} N = N_{\mathfrak{U}} \mathfrak{U} N_{\mathfrak{U}} a \in N_{\mathfrak{U}} b | a b \in N_{\mathfrak{U}} |G| = b G \notin \mathfrak{U} G \times C_{\frac{a}{b}} \notin \mathfrak{U} \exists n \in \mathbb{N} \forall k \in \mathbb{N} n^k \in N_{\mathfrak{U}} \mathfrak{U} = \mathfrak{O} n = p p p \mathfrak{O}","['abstract-algebra', 'group-theory', 'number-theory', 'finite-groups', 'universal-algebra']"
39,"Suppose $I\oplus K$ is a free module, then ""$KI\subseteq K\cap I$""","Suppose  is a free module, then """"",I\oplus K KI\subseteq K\cap I,"I have been staring at this proof in the Proceedings of the AMS, and I don't follow the author's logic. Here's the setup: $I$ is an ideal in a ring $R$, and it is projective as a right $R$ module. Therefore it is a summand in a free right $R$ module, $F=I\oplus K$. Now, the line of reasoning continues (verbatim except for symbol changes): Suppose $K\neq 0$. Since $F$ is isomorphic to a direct sum of copies of $R$, it has canonical multiplication. Let $\operatorname{Ann}_F(I)$ be the annihilator of $I$ in $F$. Then $KI\subseteq K\cap I=0$, so $\operatorname{Ann}_F(I)\neq 0$. Now if $K$ was a right ideal of $R$, then $KI\subseteq K\cap I$ would be easy to understand. The only set $K$ and $I$ are comparable in $F$, so $(I\oplus 0)\cap( 0\oplus K)=0\oplus 0$ in the direct sum.  But the left side is apparently multiplying $K$ through the $F$ module action, so that we're actually talking about $(0\oplus K)I$. Why would one say that's a subset of $I\oplus 0$? Sure, every element of $(0\oplus K)I$, when expressed as a tuple in $F$ has entries in $I$, but as far as I can see, that doesn't mean anything about its membership in $I\oplus 0$. I should also say that the ring $R$ is right self-injective and the ideal $I$ has zero left annihilator in $R$, but I'm not sure that it makes a difference. The author appeals to neither property in the line of thought above. In fact, that $I$ has zero left annihilator immediately allows you to say that $Ann_F(I)=0$, but since the whole point of this is to derive a contradiction, I need to see if there's any merit in what the author has claimed. I haven't managed to cook up a counterexample yet, mostly because I have a hard time realizing projective ideals as summands in free modules. Am I missing some observation or is my doubt justified? I have a sneaking suspicion that a cognitive error occurred on the author's part.","I have been staring at this proof in the Proceedings of the AMS, and I don't follow the author's logic. Here's the setup: $I$ is an ideal in a ring $R$, and it is projective as a right $R$ module. Therefore it is a summand in a free right $R$ module, $F=I\oplus K$. Now, the line of reasoning continues (verbatim except for symbol changes): Suppose $K\neq 0$. Since $F$ is isomorphic to a direct sum of copies of $R$, it has canonical multiplication. Let $\operatorname{Ann}_F(I)$ be the annihilator of $I$ in $F$. Then $KI\subseteq K\cap I=0$, so $\operatorname{Ann}_F(I)\neq 0$. Now if $K$ was a right ideal of $R$, then $KI\subseteq K\cap I$ would be easy to understand. The only set $K$ and $I$ are comparable in $F$, so $(I\oplus 0)\cap( 0\oplus K)=0\oplus 0$ in the direct sum.  But the left side is apparently multiplying $K$ through the $F$ module action, so that we're actually talking about $(0\oplus K)I$. Why would one say that's a subset of $I\oplus 0$? Sure, every element of $(0\oplus K)I$, when expressed as a tuple in $F$ has entries in $I$, but as far as I can see, that doesn't mean anything about its membership in $I\oplus 0$. I should also say that the ring $R$ is right self-injective and the ideal $I$ has zero left annihilator in $R$, but I'm not sure that it makes a difference. The author appeals to neither property in the line of thought above. In fact, that $I$ has zero left annihilator immediately allows you to say that $Ann_F(I)=0$, but since the whole point of this is to derive a contradiction, I need to see if there's any merit in what the author has claimed. I haven't managed to cook up a counterexample yet, mostly because I have a hard time realizing projective ideals as summands in free modules. Am I missing some observation or is my doubt justified? I have a sneaking suspicion that a cognitive error occurred on the author's part.",,"['abstract-algebra', 'ring-theory', 'modules']"
40,"Do there exist polynomials $f,g$ such that $\mathbb{C}[a,b,c]\le\mathbb{C}[f,g]$ for $a,b,c$ given polynomials?",Do there exist polynomials  such that  for  given polynomials?,"f,g \mathbb{C}[a,b,c]\le\mathbb{C}[f,g] a,b,c","I want to prove something bigger than the problem in the title and I want to create a lemma that is useful for the solution of the problem. But I am unable to prove (or give a counterexample) the ""lemma"": Suppose that $a,b,c$ are given polynomials in $\mathbb{C}[x]$ such that $\mathbb{C}[a,b,c] \subsetneq \mathbb{C}[x]$. There exist polynomials $f,g \in \mathbb{C}[x]$ such that $\mathbb{C}[a,b,c]\le\mathbb{C}[f,g]$ where $\mathbb{C}[f,g]\subsetneq \mathbb{C}[x]$? (The symbol $\le$ obviously means subring.) Does anyone have any idea to prove this if it is true? Or give a counterexample if it is false?","I want to prove something bigger than the problem in the title and I want to create a lemma that is useful for the solution of the problem. But I am unable to prove (or give a counterexample) the ""lemma"": Suppose that $a,b,c$ are given polynomials in $\mathbb{C}[x]$ such that $\mathbb{C}[a,b,c] \subsetneq \mathbb{C}[x]$. There exist polynomials $f,g \in \mathbb{C}[x]$ such that $\mathbb{C}[a,b,c]\le\mathbb{C}[f,g]$ where $\mathbb{C}[f,g]\subsetneq \mathbb{C}[x]$? (The symbol $\le$ obviously means subring.) Does anyone have any idea to prove this if it is true? Or give a counterexample if it is false?",,"['abstract-algebra', 'commutative-algebra', 'polynomials']"
41,Can any torsion-free abelian group be embedded in a direct sum of copies of $\mathbb Q$?,Can any torsion-free abelian group be embedded in a direct sum of copies of ?,\mathbb Q,"I'm trying to solve this for a problem and I need to know if what I have done is right: Let $B$ be a torsion-free abelian group. Then we consider the set $$A=\{(b,n):b\in B,n\in \mathbb Z,n\neq 0\}$$ and define $$(b,n)\sim (a,m) \text{ iff } bm=an.$$ This yields an equivalence relation, now you can define addition of classes by $(b,n)+(a,m)=(am+bn,nm)$ . Then $(A,+)$ is a torsion-free abelian group and $B$ can be embedded into $(A,+)$ , but also $(A,+)$ is divisible, so $(A,+)$ can be embedded in a direct sum of copies of $\mathbb Q$ . Thank you for your time.","I'm trying to solve this for a problem and I need to know if what I have done is right: Let be a torsion-free abelian group. Then we consider the set and define This yields an equivalence relation, now you can define addition of classes by . Then is a torsion-free abelian group and can be embedded into , but also is divisible, so can be embedded in a direct sum of copies of . Thank you for your time.","B A=\{(b,n):b\in B,n\in \mathbb Z,n\neq 0\} (b,n)\sim (a,m) \text{ iff } bm=an. (b,n)+(a,m)=(am+bn,nm) (A,+) B (A,+) (A,+) (A,+) \mathbb Q","['abstract-algebra', 'group-theory']"
42,"Given $n\in \mathbb N$, is there a free module with a basis of size $m$, $\forall m\geq n$?","Given , is there a free module with a basis of size , ?",n\in \mathbb N m \forall m\geq n,"Exercise IV.2.12 of Hungerford's Algebra asks to show the following: If $F$ is a free module over a ring with identity such that $F$ has a basis of finite cardinality $n > 1$ and another basis of cardinality $n + 1$ , then $F$ has a basis of cardinality $m$ for every $m > n\ (m \in \mathbb N)$ . This is easy to prove using induction and the fact that if $M$ is an $R$ -module with a basis of size $n$ , then $M\simeq\bigoplus_{k=1}^nR$ as $R$ -modules. My question is, for each $n\geq 1$ is there a ring $R$ with identity and a free $R$ -module $M$ such that $M$ has a basis of size $m$ for all $m\geq n$ and $M$ does not have a basis of size $k$ for all $k<n$ ? In another exercise of the same section the case $n=1$ is established as follows: Let $K$ be a ring with identity and $F$ a free $K$ -module with an infinite denumerable basis $\{ e_1,e_2,\ldots\}$ . Put $R =$ Hom $_K(F,F)$ . Then the author shows that $R$ has a basis of size $2$ as an $R$ -module; namely $\{f_1,f_2\}$ , where $f_1(e_{2n})=e_n, f_1(e_{2n-1})=0,f_2(e_{2n})=0,f_2(e_{2n-1})=e_n$ , and of course $R$ has a basis of size $1$ ; $\{1_R\}$ . But I don't know what to do when $n\geq 2$ . I thank beforehand any help.","Exercise IV.2.12 of Hungerford's Algebra asks to show the following: If is a free module over a ring with identity such that has a basis of finite cardinality and another basis of cardinality , then has a basis of cardinality for every . This is easy to prove using induction and the fact that if is an -module with a basis of size , then as -modules. My question is, for each is there a ring with identity and a free -module such that has a basis of size for all and does not have a basis of size for all ? In another exercise of the same section the case is established as follows: Let be a ring with identity and a free -module with an infinite denumerable basis . Put Hom . Then the author shows that has a basis of size as an -module; namely , where , and of course has a basis of size ; . But I don't know what to do when . I thank beforehand any help.","F F n > 1 n + 1 F m m > n\ (m \in \mathbb N) M R n M\simeq\bigoplus_{k=1}^nR R n\geq 1 R R M M m m\geq n M k k<n n=1 K F K \{ e_1,e_2,\ldots\} R = _K(F,F) R 2 R \{f_1,f_2\} f_1(e_{2n})=e_n, f_1(e_{2n-1})=0,f_2(e_{2n})=0,f_2(e_{2n-1})=e_n R 1 \{1_R\} n\geq 2","['abstract-algebra', 'modules']"
43,Has this algebraic structure been named or studied?,Has this algebraic structure been named or studied?,,"Apologies if this is is not very well-defined or exposes my ignorance; I know comparatively little about abstract algebra. The structure of certain programming languages can be described with the algebraic structure $(S,\cdot,\verb|^|)$ where $\cdot:S\times S\rightarrow S$ is associative and unital, and $\verb|^| : S \rightarrow S$ i.e., a monoid with an extra unary operation.  Unfortunately, nothing much can be said about $\verb|^|$ except that: $\verb|^|$ distributes over $\cdot$ , i.e., $\verb|^|(a\cdot b) = \verb|^|a\cdot \verb|^|b$ . $\verb|^|$ is cancellative: $\verb|^|a = \verb|^|$ b implies $a = b$ . In particular, $\verb|^|$ is not an inverse operation, nor idempotent; in general, $\verb|^|(\verb|^|a) \neq a$ , and in particular, $\verb|^|1\neq1$ . My question is: Has this structure been studied, or at least been given a name, in abstract algebra? I'm not optimistic, because adding $\verb|^|$ doesn't appear to make the structure much more interesting than a monoid.  But if anyone can even point me towards similar structures, I'd be grateful.  (Although clearly groups are not a good fit because of the lack of invertibility.) ( Edited to include the cancellative property of $\verb|^|$ and to explicitly mention its non-idempotency.) A concrete example of the structure is: strings of symbols with brackets, with a semantics which gives the substrings inside brackets a different meaning. Joy is one such programming language based on this framework. The underlying monoid for Joy is well-described in that paper , but the bracketing operator (Joy's [...], my ^) is much less well-described algebraically, and I'm trying to find out more about where it might fit in (or be made to fit in.)","Apologies if this is is not very well-defined or exposes my ignorance; I know comparatively little about abstract algebra. The structure of certain programming languages can be described with the algebraic structure where is associative and unital, and i.e., a monoid with an extra unary operation.  Unfortunately, nothing much can be said about except that: distributes over , i.e., . is cancellative: b implies . In particular, is not an inverse operation, nor idempotent; in general, , and in particular, . My question is: Has this structure been studied, or at least been given a name, in abstract algebra? I'm not optimistic, because adding doesn't appear to make the structure much more interesting than a monoid.  But if anyone can even point me towards similar structures, I'd be grateful.  (Although clearly groups are not a good fit because of the lack of invertibility.) ( Edited to include the cancellative property of and to explicitly mention its non-idempotency.) A concrete example of the structure is: strings of symbols with brackets, with a semantics which gives the substrings inside brackets a different meaning. Joy is one such programming language based on this framework. The underlying monoid for Joy is well-described in that paper , but the bracketing operator (Joy's [...], my ^) is much less well-described algebraically, and I'm trying to find out more about where it might fit in (or be made to fit in.)","(S,\cdot,\verb|^|) \cdot:S\times S\rightarrow S \verb|^| : S \rightarrow S \verb|^| \verb|^| \cdot \verb|^|(a\cdot b) = \verb|^|a\cdot \verb|^|b \verb|^| \verb|^|a = \verb|^| a = b \verb|^| \verb|^|(\verb|^|a) \neq a \verb|^|1\neq1 \verb|^| \verb|^|","['abstract-algebra', 'terminology', 'formal-languages']"
44,How to tell if an element of a quotient ring is a zero divisor,How to tell if an element of a quotient ring is a zero divisor,,"I am looking at Hartshorne Example III.9.8.4., p260.  He says that $a$ is not a zero divisor in $k[a,x,y,z]/I$, where $$ I = (a^2(x+1) -z^2, ax(x+1)-yz, xz-ay,y^2-x^2(x+1)).  $$ Is there a good way to see this?","I am looking at Hartshorne Example III.9.8.4., p260.  He says that $a$ is not a zero divisor in $k[a,x,y,z]/I$, where $$ I = (a^2(x+1) -z^2, ax(x+1)-yz, xz-ay,y^2-x^2(x+1)).  $$ Is there a good way to see this?",,"['abstract-algebra', 'algebraic-geometry', 'commutative-algebra']"
45,"For local ring $R$, does funcotor $\operatorname{Hom( Spec}R, X)$ characterize scheme $X$?","For local ring , does funcotor  characterize scheme ?","R \operatorname{Hom( Spec}R, X) X","Let $\bf{Sch, Sets, Ring}$ be a category of schemes, sets, commutative rings. By Yoneda's lemma, scheme $X$ is characterized by contravariant functor $$\operatorname{Hom}(*, X): \bf{Sch}^{op}\to Sets$$ Now thinking  glueing of schemes by affine schemes, $X$ can be characterized by covariant functor $$\operatorname{Hom}({\operatorname{ Spec} }(*), X): \bf{Ring}\to Sets$$ I want to know that whether $X$ is characterized by only local rings? i.e. for schemes $X, Y$ , if for all local ring $R$ , $\operatorname{Hom}({\operatorname{ Spec} }(R), X)\cong \operatorname{Hom}({\operatorname{ Spec} }(R), Y)$ , then $X\cong Y$ ?","Let be a category of schemes, sets, commutative rings. By Yoneda's lemma, scheme is characterized by contravariant functor Now thinking  glueing of schemes by affine schemes, can be characterized by covariant functor I want to know that whether is characterized by only local rings? i.e. for schemes , if for all local ring , , then ?","\bf{Sch, Sets, Ring} X \operatorname{Hom}(*, X): \bf{Sch}^{op}\to Sets X \operatorname{Hom}({\operatorname{ Spec} }(*), X): \bf{Ring}\to Sets X X, Y R \operatorname{Hom}({\operatorname{ Spec} }(R), X)\cong \operatorname{Hom}({\operatorname{ Spec} }(R), Y) X\cong Y","['abstract-algebra', 'algebraic-geometry', 'ring-theory', 'local-rings']"
46,Rings with 'non-harmless' zero-divisors,Rings with 'non-harmless' zero-divisors,,"The following excerpt is from pp. 246–247 of Paolo Aluffi's Algebra: Chapter 0: 1.2. Prime and irreducible elements. Let $R$ be a (commutative) ring [with $1$ ], and let $a,b\in R$ . We say that $a$ divides $b$ , or that $a$ is a divisor of $b$ , or that $b$ is a multiple of $a$ , if $b\in(a)$ , that is $$ (\exists c\in R), \quad b = ac. $$ We use the notation $a \mid b$ . Two elements $a,b$ are associates if $(a) = (b)$ , that is, if $a\mid b$ and $b\mid a$ . Lemma 1.5. Let $a,b$ be nonzero elements of an integral domain $R$ . Then $a$ and $b$ are associates if and only if $a = ub$ , for $u$ a unit in $R$ . [Proof omitted.] Incidentally, here the reader sees why it is convenient to restrict our attention to integral domains. This argument really shows that if $(a) = (b) \ne (0)$ in an integral domain, and $b = ca$ , then $c$ is necessarily a unit. Away from the comfortable environment of integral domains, even such harmless-looking statements may fail: in $\Bbb Z/6\Bbb Z$ , the classes $[2]_6,[4]_6$ of $2$ and $4$ are associates according to our definition, and $[4]_6 = [2]_6\cdot[2]_6$ , yet $[2]_6$ is not a unit. However, $[4]_6 = [5]_6\cdot [2]_6$ and $[5]_6$ is a unit, so this is not a counterexample to Lemma 1.5. In fact, Lemma 1.5 may fail over rings with 'non-harmless' zero-divisors (yes, there is such a notion) [emphasis added]. Since at this point, Aluffi does not say what such rings are called, I was hoping someone might know what type of rings Aluffi is referring to. (And hopefully provide a little context as to why they are interesting!)","The following excerpt is from pp. 246–247 of Paolo Aluffi's Algebra: Chapter 0: 1.2. Prime and irreducible elements. Let be a (commutative) ring [with ], and let . We say that divides , or that is a divisor of , or that is a multiple of , if , that is We use the notation . Two elements are associates if , that is, if and . Lemma 1.5. Let be nonzero elements of an integral domain . Then and are associates if and only if , for a unit in . [Proof omitted.] Incidentally, here the reader sees why it is convenient to restrict our attention to integral domains. This argument really shows that if in an integral domain, and , then is necessarily a unit. Away from the comfortable environment of integral domains, even such harmless-looking statements may fail: in , the classes of and are associates according to our definition, and , yet is not a unit. However, and is a unit, so this is not a counterexample to Lemma 1.5. In fact, Lemma 1.5 may fail over rings with 'non-harmless' zero-divisors (yes, there is such a notion) [emphasis added]. Since at this point, Aluffi does not say what such rings are called, I was hoping someone might know what type of rings Aluffi is referring to. (And hopefully provide a little context as to why they are interesting!)","R 1 a,b\in R a b a b b a b\in(a) 
(\exists c\in R), \quad b = ac.
 a \mid b a,b (a) = (b) a\mid b b\mid a a,b R a b a = ub u R (a) = (b) \ne (0) b = ca c \Bbb Z/6\Bbb Z [2]_6,[4]_6 2 4 [4]_6 = [2]_6\cdot[2]_6 [2]_6 [4]_6 = [5]_6\cdot [2]_6 [5]_6","['abstract-algebra', 'ring-theory']"
47,Finding a chain map from an $F$-acyclic resolution to an injective resolution which is a monomorphism in each degree,Finding a chain map from an -acyclic resolution to an injective resolution which is a monomorphism in each degree,F,"Let $\mathcal{A}$ be an abelian category with enough injectives. Let $F:\mathcal{A}\to\mathcal{B}$ be a left-exact additive functor to $\mathcal{B}$ another abelian category If $M$ has an $F$-acyclic resolution:   $$0\longrightarrow M\longrightarrow X^0 \longrightarrow X^1 \longrightarrow X^2\longrightarrow\cdots$$ It is claimed ( In Lang (2002) on page 797 line 11 ) that I can find an injective resolution:   $$0\longrightarrow M\longrightarrow I^0 \longrightarrow I^1 \longrightarrow I^2\longrightarrow\cdots$$ and a cochain map from the first resolution to the second, such that each $i\geq 0$ the morphism $X^i\to I^i$ is a monomorphism. Question: How do I find such an injective resolution, with such a chain map? My attempts are below! 1) Using that we have enough injectives, I can find a monomorphism $X^0\to I^0$ for some $I^0$ injective. Then by composition I can find a morphism $M\to X^0\to I^0$. This gives me the monomorphism $M\hookrightarrow I^0$. Similarly using enough injectives, we can find a monomorphism $X^1\hookrightarrow I^1$ for $I^1$ injective. Then using the composite $X^0\to X^1\to I^1$ and that $I^1$ is injective, we induce a morphism from $I^0\to I^1$. We can do this inductively to obtain a cochain map between $0\to M\to X^\bullet$ and $0\to M\to I^\bullet$, with these monomorphisms for all $k\geq 0$ $X^k\hookrightarrow I^k$ for $I^k$ injective. But I can't show that the bottom row is exact, and hence I cannot show that this $I^\bullet$ an injective resolution. In fact part of me doubts that it even is (even trying to chase elements fails, since I have no epimorphisms appearing). 1b) I tried taking cokernels in each degree so that I could try to do something with the epimorphisms it gives me. Mainly I want to apply $F$ and use the long exact cohomology sequence, to make use of the $F$-acyclicity. But unless I can show exactness on the second row, I can't ensure $F$ will preserve the short exact sequence of complexes. 2) I tried using that we have enough injectives to induces a monomorphism from $M\to I_M^0$ and a monomorphism $X^0\to I_{X^0}^0$ and taking the direct sum, which I can show is also an injective object, and then repeating this for $I_{X^0}^0\oplus I_M^0$ and $X^1$, so on. But this seems highly non-exact (in fact all the morphisms in this bottom row are probably injective...) 3) I think the category of chain complexes has enough injectives, but I don't think an injective object in the chain category looks like what I want. 4) I couldn't think of any way to take the injective resolution first, and then choose compatible morphisms such that the result follows.","Let $\mathcal{A}$ be an abelian category with enough injectives. Let $F:\mathcal{A}\to\mathcal{B}$ be a left-exact additive functor to $\mathcal{B}$ another abelian category If $M$ has an $F$-acyclic resolution:   $$0\longrightarrow M\longrightarrow X^0 \longrightarrow X^1 \longrightarrow X^2\longrightarrow\cdots$$ It is claimed ( In Lang (2002) on page 797 line 11 ) that I can find an injective resolution:   $$0\longrightarrow M\longrightarrow I^0 \longrightarrow I^1 \longrightarrow I^2\longrightarrow\cdots$$ and a cochain map from the first resolution to the second, such that each $i\geq 0$ the morphism $X^i\to I^i$ is a monomorphism. Question: How do I find such an injective resolution, with such a chain map? My attempts are below! 1) Using that we have enough injectives, I can find a monomorphism $X^0\to I^0$ for some $I^0$ injective. Then by composition I can find a morphism $M\to X^0\to I^0$. This gives me the monomorphism $M\hookrightarrow I^0$. Similarly using enough injectives, we can find a monomorphism $X^1\hookrightarrow I^1$ for $I^1$ injective. Then using the composite $X^0\to X^1\to I^1$ and that $I^1$ is injective, we induce a morphism from $I^0\to I^1$. We can do this inductively to obtain a cochain map between $0\to M\to X^\bullet$ and $0\to M\to I^\bullet$, with these monomorphisms for all $k\geq 0$ $X^k\hookrightarrow I^k$ for $I^k$ injective. But I can't show that the bottom row is exact, and hence I cannot show that this $I^\bullet$ an injective resolution. In fact part of me doubts that it even is (even trying to chase elements fails, since I have no epimorphisms appearing). 1b) I tried taking cokernels in each degree so that I could try to do something with the epimorphisms it gives me. Mainly I want to apply $F$ and use the long exact cohomology sequence, to make use of the $F$-acyclicity. But unless I can show exactness on the second row, I can't ensure $F$ will preserve the short exact sequence of complexes. 2) I tried using that we have enough injectives to induces a monomorphism from $M\to I_M^0$ and a monomorphism $X^0\to I_{X^0}^0$ and taking the direct sum, which I can show is also an injective object, and then repeating this for $I_{X^0}^0\oplus I_M^0$ and $X^1$, so on. But this seems highly non-exact (in fact all the morphisms in this bottom row are probably injective...) 3) I think the category of chain complexes has enough injectives, but I don't think an injective object in the chain category looks like what I want. 4) I couldn't think of any way to take the injective resolution first, and then choose compatible morphisms such that the result follows.",,"['abstract-algebra', 'homology-cohomology', 'homological-algebra']"
48,If $D$ be a division ring and $D^*$ be finitely generated group then $D^*$ is abelian group?,If  be a division ring and  be finitely generated group then  is abelian group?,D D^* D^*,"Wedderburn's little theorem : every finite division ring $D$  is commutative, or $D^*$ is abelian group. Now  if $D^*$ be a finitely generated group  then  $D^*$ is an abelian group ?","Wedderburn's little theorem : every finite division ring $D$  is commutative, or $D^*$ is abelian group. Now  if $D^*$ be a finitely generated group  then  $D^*$ is an abelian group ?",,"['abstract-algebra', 'group-theory', 'ring-theory', 'examples-counterexamples', 'division-ring']"
49,Reference Request: Equivariant cup product in singular cohomology,Reference Request: Equivariant cup product in singular cohomology,,"I've been looking around for a standard treatment of what I think sould be called ""equivariant cup product in singular cohomology"", but couldn't find anything promissing. I did played around with what I expect this to be, so maybe you can tell me whether (a) this is known and written out somewhere (idealy in some axiomatic way), or (b) this is the right way to define the cup product in this setting. So  let's get started: Consider an oriented finite dimensional smooth Riemannian manifold $(M,g)$ together with a regular covering map $$\pi \colon \tilde{M} \to M,$$ where $\tilde{M}$ is equipped with the pullback metric $\tilde{g}:=\pi^*g$. Let us assume that the group of deck transformations $\Gamma$ is isomorphic to $\mathbb{Z}^k$, for some $k$ positive integer. Then we can define the cochain complex $$\left(S^{\bullet}(\tilde{M};\mathbb{Z}) \otimes_{\mathbb{Z}[\Gamma]}N, \delta \otimes \text{id}\right),$$ where $\mathbb{Z}[\Gamma]$ is the group ring w.r.t. $\Gamma$ and $N$ is some module over this group ring (note that we don't have to specify left and right actions, for $\mathbb{Z}[\Gamma]$ is commutative). $S^{\bullet}$ obviously denotes singular cohomology. My idea now was to define a ""cochain cup product"" on this chain complex as follows: $$\cup \colon \left(S^p(\tilde{M};\mathbb{Z})\otimes_{\mathbb{Z}[\Gamma]}N\right) \otimes_{\mathbb{Z}[\Gamma]}\left(S^q(\tilde{M};\mathbb{Z})\otimes_{\mathbb{Z}[\Gamma]}N\right) \longrightarrow S^{p+q}(\tilde{M};\mathbb{Z})\otimes_{\mathbb{Z}[\Gamma]}\left(N\otimes_{\mathbb{Z}[\Gamma]}N\right),$$ where $$(\varphi  \otimes n_1)\cup (\psi \otimes n_2):=(\varphi \cup \psi) \otimes (n_1\otimes n_2)$$ and the cup on the RHS is the usual cup product in singular cohomlogy. Note that lots of the assumption I made are superfluous or could be phrased more generally, but this is more or less the setting I'm working with (maybe it turns out that in this setting some complications become trivial and so on...). And $\mathbb{Z}_2$ coefficients are sufficient for what I need (maybe this also makes things easier as $\mathbb{Z}_2$ is a field). Every helpful comment is greatly appreciated, thank you! EDIT 1: It seems that my guess was incorrect. In Eilenberg and Steenrod's ""Foundations of Algebraic Topology"" on page 209 there is a definition of equivariant homology AND cohomology. Applying this general definition to our special case gives us the following equivariant cochain complex $$\left(\hom_{\mathbb{Z}[\Gamma]}\left(S_{\bullet}(\tilde{M};\mathbb{Z}),N\right),\delta\right).$$ Also, they claim in the book that equivariant homology/cohomology is a homology theory (where everything is equivariant, roughly speaking). So maybe understanding the corresponding Eilenberg-Zilber morphism and the induced map coming from the diagonal embedding (or more generally any diagonal approximation) migth lead to an answer... EDIT 2: I found some more papers by Eilenberg and by Steenrod. Eilenberg gives a definition of an equivariant cup product in equivariant singular cohomology (see ""Homology of Spaces with Operators""), but he uses an extra assumption that ensures that the cup product of two equivariant cochains is an equivariant cochain again. This assumption however, does not hold in my setting (if I understand correctly). The paper by Steenrod ""Homology with Local Coefficients"" gives a definition using a CW-decomposition and cellular homology, which seems too far away from what I possibly want.","I've been looking around for a standard treatment of what I think sould be called ""equivariant cup product in singular cohomology"", but couldn't find anything promissing. I did played around with what I expect this to be, so maybe you can tell me whether (a) this is known and written out somewhere (idealy in some axiomatic way), or (b) this is the right way to define the cup product in this setting. So  let's get started: Consider an oriented finite dimensional smooth Riemannian manifold $(M,g)$ together with a regular covering map $$\pi \colon \tilde{M} \to M,$$ where $\tilde{M}$ is equipped with the pullback metric $\tilde{g}:=\pi^*g$. Let us assume that the group of deck transformations $\Gamma$ is isomorphic to $\mathbb{Z}^k$, for some $k$ positive integer. Then we can define the cochain complex $$\left(S^{\bullet}(\tilde{M};\mathbb{Z}) \otimes_{\mathbb{Z}[\Gamma]}N, \delta \otimes \text{id}\right),$$ where $\mathbb{Z}[\Gamma]$ is the group ring w.r.t. $\Gamma$ and $N$ is some module over this group ring (note that we don't have to specify left and right actions, for $\mathbb{Z}[\Gamma]$ is commutative). $S^{\bullet}$ obviously denotes singular cohomology. My idea now was to define a ""cochain cup product"" on this chain complex as follows: $$\cup \colon \left(S^p(\tilde{M};\mathbb{Z})\otimes_{\mathbb{Z}[\Gamma]}N\right) \otimes_{\mathbb{Z}[\Gamma]}\left(S^q(\tilde{M};\mathbb{Z})\otimes_{\mathbb{Z}[\Gamma]}N\right) \longrightarrow S^{p+q}(\tilde{M};\mathbb{Z})\otimes_{\mathbb{Z}[\Gamma]}\left(N\otimes_{\mathbb{Z}[\Gamma]}N\right),$$ where $$(\varphi  \otimes n_1)\cup (\psi \otimes n_2):=(\varphi \cup \psi) \otimes (n_1\otimes n_2)$$ and the cup on the RHS is the usual cup product in singular cohomlogy. Note that lots of the assumption I made are superfluous or could be phrased more generally, but this is more or less the setting I'm working with (maybe it turns out that in this setting some complications become trivial and so on...). And $\mathbb{Z}_2$ coefficients are sufficient for what I need (maybe this also makes things easier as $\mathbb{Z}_2$ is a field). Every helpful comment is greatly appreciated, thank you! EDIT 1: It seems that my guess was incorrect. In Eilenberg and Steenrod's ""Foundations of Algebraic Topology"" on page 209 there is a definition of equivariant homology AND cohomology. Applying this general definition to our special case gives us the following equivariant cochain complex $$\left(\hom_{\mathbb{Z}[\Gamma]}\left(S_{\bullet}(\tilde{M};\mathbb{Z}),N\right),\delta\right).$$ Also, they claim in the book that equivariant homology/cohomology is a homology theory (where everything is equivariant, roughly speaking). So maybe understanding the corresponding Eilenberg-Zilber morphism and the induced map coming from the diagonal embedding (or more generally any diagonal approximation) migth lead to an answer... EDIT 2: I found some more papers by Eilenberg and by Steenrod. Eilenberg gives a definition of an equivariant cup product in equivariant singular cohomology (see ""Homology of Spaces with Operators""), but he uses an extra assumption that ensures that the cup product of two equivariant cochains is an equivariant cochain again. This assumption however, does not hold in my setting (if I understand correctly). The paper by Steenrod ""Homology with Local Coefficients"" gives a definition using a CW-decomposition and cellular homology, which seems too far away from what I possibly want.",,"['abstract-algebra', 'general-topology', 'differential-geometry', 'reference-request', 'algebraic-topology']"
50,Characterization of $A_5$ by the Centralizer of an Involution,Characterization of  by the Centralizer of an Involution,A_5,"In his thesis, Fowler had shown that $A_5$ is the only finite simple group $G$ affording an involution $u$ such that $C_G(u) \cong C_2 \times C_2$. Is there a proof of that result that relies on elementary methods of group theory and representation theory? For comparison: If a finite simple group $G$ affords an involution $u$ such that $C_G(u) \cong D_4$ is the dihedral group of order 8, then $|G| \in \{ 168, 360 \}$. This is proven in some standard textbooks such as the book of James and Liebeck. The proof relies on the Sylow theorems, the orthogonality relations for characters, and some standard facts about induced characters. Can the above statement be proven by similar methods? Here are some thoughts: Let $C = C_G(u)$ be the Klein four group, and let $P \leq G$ be a 2-Sylow subgroup containing $C$. Then $1 \lneq Z(P) \leq C$. Suppose $u \in Z(P)$. Then we have $P=C$. In particular, $P$ has a subgroup of index two containing a unique involution. By a lemma of Thompson [Isaacs - Character Theory of Finite Groups, Lemma 7.11], all involutions of $G$ must be conjugated, whence $P = C_G(x)$ for all $x \in P \setminus \{1\}$. So we are in the situation mentioned by Orat in the comments, and we know how to proceed. So it remains to consider the case, where $u \notin Z(P)$. Then $Z(P) = \langle z \rangle$ is of order two, and $C = \langle u,z \rangle$. There is a subgroup $D$ between $C$ and $N_P(C)$ isomorphic to the dihedral group of order 8. Of course this situation has to be impossible since it does not match with the $A_5$. Any ideas how to derive a contradiction from here? I don't know if it helps, but by Brauer-Fowler, $G$ can be embedded into the alternating group $A_{15}$. In particular, the order of $P$ is bounded by $2^{10}$. I think the remaining case can be handled by exactly the same methods as in the book of Collins: We have $\sum_i \chi_i(u)^2 = 4$, where $\chi_i$ runs over the irreducible characters of $G$, $\chi_1$ being the trivial character. Because of that equation, there are exactly four of those characters not vanishing on $u$, say $\chi_1, \dots, \chi_4$. Due to orthogonality relations, we may assume $\chi_2(u) = 1$, $\chi_3(u) = -1$ and $\chi_4(u) = \pm 1$. Now, in contrast to the case handled in the book, we have more than one conjugacy class of involutions in $G$. More precisely, $u$ and $z$ cannot be conjugated, since they have centralizers of different orders. Moreover, we also use (as in the book) that the product of two involutions is an involution if and only if they commute. So any two involutions $x,y \in G$ with $xy = u$ must be in $C_G(u)$. It follows that $u$ is not the product of any two elements of its conjugacy class $u^G$. The class algebra constants formula (applied to the conjugacy class $u^G$) yields $$ \frac{|G|}{16} \left( 1+ \frac{1}{\chi_2(1)} - \frac{1}{\chi_3(1)} \pm \frac{1}{\chi_4(1)} \right) = 0. $$ Now, since $G$ is perfect, we have $\chi_i(1) \geq 2$ for $i \geq 2$. But then the left hand side is strictly positive. So we arrived at a contradiction here. I would be happy if anyone can confirm my proof, or point out some mistake.","In his thesis, Fowler had shown that $A_5$ is the only finite simple group $G$ affording an involution $u$ such that $C_G(u) \cong C_2 \times C_2$. Is there a proof of that result that relies on elementary methods of group theory and representation theory? For comparison: If a finite simple group $G$ affords an involution $u$ such that $C_G(u) \cong D_4$ is the dihedral group of order 8, then $|G| \in \{ 168, 360 \}$. This is proven in some standard textbooks such as the book of James and Liebeck. The proof relies on the Sylow theorems, the orthogonality relations for characters, and some standard facts about induced characters. Can the above statement be proven by similar methods? Here are some thoughts: Let $C = C_G(u)$ be the Klein four group, and let $P \leq G$ be a 2-Sylow subgroup containing $C$. Then $1 \lneq Z(P) \leq C$. Suppose $u \in Z(P)$. Then we have $P=C$. In particular, $P$ has a subgroup of index two containing a unique involution. By a lemma of Thompson [Isaacs - Character Theory of Finite Groups, Lemma 7.11], all involutions of $G$ must be conjugated, whence $P = C_G(x)$ for all $x \in P \setminus \{1\}$. So we are in the situation mentioned by Orat in the comments, and we know how to proceed. So it remains to consider the case, where $u \notin Z(P)$. Then $Z(P) = \langle z \rangle$ is of order two, and $C = \langle u,z \rangle$. There is a subgroup $D$ between $C$ and $N_P(C)$ isomorphic to the dihedral group of order 8. Of course this situation has to be impossible since it does not match with the $A_5$. Any ideas how to derive a contradiction from here? I don't know if it helps, but by Brauer-Fowler, $G$ can be embedded into the alternating group $A_{15}$. In particular, the order of $P$ is bounded by $2^{10}$. I think the remaining case can be handled by exactly the same methods as in the book of Collins: We have $\sum_i \chi_i(u)^2 = 4$, where $\chi_i$ runs over the irreducible characters of $G$, $\chi_1$ being the trivial character. Because of that equation, there are exactly four of those characters not vanishing on $u$, say $\chi_1, \dots, \chi_4$. Due to orthogonality relations, we may assume $\chi_2(u) = 1$, $\chi_3(u) = -1$ and $\chi_4(u) = \pm 1$. Now, in contrast to the case handled in the book, we have more than one conjugacy class of involutions in $G$. More precisely, $u$ and $z$ cannot be conjugated, since they have centralizers of different orders. Moreover, we also use (as in the book) that the product of two involutions is an involution if and only if they commute. So any two involutions $x,y \in G$ with $xy = u$ must be in $C_G(u)$. It follows that $u$ is not the product of any two elements of its conjugacy class $u^G$. The class algebra constants formula (applied to the conjugacy class $u^G$) yields $$ \frac{|G|}{16} \left( 1+ \frac{1}{\chi_2(1)} - \frac{1}{\chi_3(1)} \pm \frac{1}{\chi_4(1)} \right) = 0. $$ Now, since $G$ is perfect, we have $\chi_i(1) \geq 2$ for $i \geq 2$. But then the left hand side is strictly positive. So we arrived at a contradiction here. I would be happy if anyone can confirm my proof, or point out some mistake.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'representation-theory', 'simple-groups']"
51,Prove that a free group of rank $\ge2$ is centerless and torsion-free,Prove that a free group of rank  is centerless and torsion-free,\ge2,"This exercise is from Rotman's Introduction to the Theory of Groups. It's just as the title states: prove a free group of rank $\ge2$ is centerless torsion-free. Here, the definition of a free group with basis $X$ is a group $F$ such that given any group $G$ and function $f:X\to G$, there is a unique $\varphi:F\to G$ extending $f$. The conclusion I'm supposed to make makes perfect sense when we consider what free groups really look like: they are just groups defined on a set of generators with no relations. However, I need to use this specific definition of free group for this problem and I'm having trouble. I've already proved it's centerless but I need some help on torsion-free. So here's the kind of thing I'm trying: Suppose we have a torsion element, call it $z$. Then $z^n=1$ for some $n$. Define a map $f:X\to F$ by $f(x)=x^n$. Because $F$ is free with basis $X$, this extends to a unique $\varphi:F\to F$. I want to somehow come up with a different map $\psi$ which also extends $f$, contradicting the uniqueness of $\varphi$. I'm not sure how to do this though. Does anybody have any suggestions, or alternative ideas?","This exercise is from Rotman's Introduction to the Theory of Groups. It's just as the title states: prove a free group of rank $\ge2$ is centerless torsion-free. Here, the definition of a free group with basis $X$ is a group $F$ such that given any group $G$ and function $f:X\to G$, there is a unique $\varphi:F\to G$ extending $f$. The conclusion I'm supposed to make makes perfect sense when we consider what free groups really look like: they are just groups defined on a set of generators with no relations. However, I need to use this specific definition of free group for this problem and I'm having trouble. I've already proved it's centerless but I need some help on torsion-free. So here's the kind of thing I'm trying: Suppose we have a torsion element, call it $z$. Then $z^n=1$ for some $n$. Define a map $f:X\to F$ by $f(x)=x^n$. Because $F$ is free with basis $X$, this extends to a unique $\varphi:F\to F$. I want to somehow come up with a different map $\psi$ which also extends $f$, contradicting the uniqueness of $\varphi$. I'm not sure how to do this though. Does anybody have any suggestions, or alternative ideas?",,"['abstract-algebra', 'group-theory', 'free-groups']"
52,Most general form of Cayley's theorem?,Most general form of Cayley's theorem?,,"For many classes of algebraic structures, there exists a family of structures such that any member of the class can be embedded in some member of the family (groups and symmetric groups, unital rings and rings of endomorphisms of abelian groups, distributive lattices and sets under union and intersection). Is there some construction from category theory or universal algebra that counts all of these examples as special cases?","For many classes of algebraic structures, there exists a family of structures such that any member of the class can be embedded in some member of the family (groups and symmetric groups, unital rings and rings of endomorphisms of abelian groups, distributive lattices and sets under union and intersection). Is there some construction from category theory or universal algebra that counts all of these examples as special cases?",,"['abstract-algebra', 'group-theory', 'category-theory', 'lattice-orders', 'universal-algebra']"
53,the localization of a ring is an integral domain iff the annihilators of zero divisors are comaximal ideals,the localization of a ring is an integral domain iff the annihilators of zero divisors are comaximal ideals,,"I would appreciate any help with the following problem. let $R$ be a commutative ring (with $1$). I need to show that the following are equivalent i) for every prime ideal $P$, the localization $R_P$ is an integral domain ii) for every maximal ideal $M$, $R_M$ is an integral domain iii) for $x,y \in R$ such that $xy = 0$ we have $Ann(x) + Ann(y) = R$ where $Ann$ denotes the annihilator ideal. Clearly, i) implies ii), and I showed that ii) implies iii). If iii) were false, then we can find $x,y \in R$ with $xy = 0$ and $Ann(x) + Ann(y) \subset M$ for some maximal ideal $M$. Then, in $R_M$ we have $\frac{xy}{1} = \frac{0}{1}$. Now, we are assuming that $R_M$ is an integral domain, hence either $\frac{x}{1} = \frac{0}{1}$ or $\frac{y}{1} = \frac{0}{1}$. Therefore, there exist $s,t \in R-M$ such that either $sx = 0$ or $ty = 0$. But $sx = 0$ means that $s \in Ann(x)$, hence $s \in M$, which contradicts the fact that $s \in R-M$. Similarly, $ty = 0$ leads to a contradiction. So ii) implies iii). I am having problems proving that iii) implies i). Let $P$ be a prime ideal of $R$ and suppose that $\frac{x}{s} \cdot \frac{y}{t} =\frac{0}{1}$. This means that there exists $z \in R-P$ with $zxy = 0$. Now, I want to show that either $\frac{x}{s}$ or $\frac{y}{t}$ is equal to $0$, so I attempted to show that either $zx = 0$ or $zy = 0$. From the given assumption we have $Ann(zx) + Ann(y) = R$, but I do not see how to conclude that $Ann(zx) = R$ (or maybe this approach is not even correct). I am studying this on my own, so this is not a HW problem. I would appreciate any suggestions. Thank you in advance.","I would appreciate any help with the following problem. let $R$ be a commutative ring (with $1$). I need to show that the following are equivalent i) for every prime ideal $P$, the localization $R_P$ is an integral domain ii) for every maximal ideal $M$, $R_M$ is an integral domain iii) for $x,y \in R$ such that $xy = 0$ we have $Ann(x) + Ann(y) = R$ where $Ann$ denotes the annihilator ideal. Clearly, i) implies ii), and I showed that ii) implies iii). If iii) were false, then we can find $x,y \in R$ with $xy = 0$ and $Ann(x) + Ann(y) \subset M$ for some maximal ideal $M$. Then, in $R_M$ we have $\frac{xy}{1} = \frac{0}{1}$. Now, we are assuming that $R_M$ is an integral domain, hence either $\frac{x}{1} = \frac{0}{1}$ or $\frac{y}{1} = \frac{0}{1}$. Therefore, there exist $s,t \in R-M$ such that either $sx = 0$ or $ty = 0$. But $sx = 0$ means that $s \in Ann(x)$, hence $s \in M$, which contradicts the fact that $s \in R-M$. Similarly, $ty = 0$ leads to a contradiction. So ii) implies iii). I am having problems proving that iii) implies i). Let $P$ be a prime ideal of $R$ and suppose that $\frac{x}{s} \cdot \frac{y}{t} =\frac{0}{1}$. This means that there exists $z \in R-P$ with $zxy = 0$. Now, I want to show that either $\frac{x}{s}$ or $\frac{y}{t}$ is equal to $0$, so I attempted to show that either $zx = 0$ or $zy = 0$. From the given assumption we have $Ann(zx) + Ann(y) = R$, but I do not see how to conclude that $Ann(zx) = R$ (or maybe this approach is not even correct). I am studying this on my own, so this is not a HW problem. I would appreciate any suggestions. Thank you in advance.",,['abstract-algebra']
54,"Infinite group has infinitely many subgroups, namely cyclic subgroups.","Infinite group has infinitely many subgroups, namely cyclic subgroups.",,"If $G$ is an infinite group then $G$ has infinitely many subgroups. Proof: Let's consider the following set: $C=\{\left \langle g \right \rangle: g\in G \}$ - collection of all cyclic subgroups in $G$ generated by elements of $G$. Two cases are possible: Exists infinitely many distinct cyclic subgroups $\Rightarrow$ We are done. Exists finitely many distinct cyclic subgroups for example $C=\{H_1, H_2,\dots, H_n\}$. Then $G=\bigcup \limits_{i=1}^{n}H_i$. Since $G$ is infinite then WLOG suppose that $H_1$ is also infinite, where $H_1=\left \langle g_1 \right \rangle$. Let's consider the following set $\{\left \langle g_1^n \right \rangle: n\in \mathbb{N}\}$ - the collection of all cyclic sugroups of $H_1\subset G.$  Let $K_1=\left \langle g_1 \right \rangle$, $K_2=\left \langle g_1^2 \right \rangle$, $K_3=\left \langle g_1^3 \right \rangle$, $\dots$. It's easy to show that $K_n$ and $K_m$ are distinct for $n\neq m$. Indeed, WLOG take $n<m$ and taking $g_1^n\in K_n$ but $g_1^n\notin K_m$ otherwise $g_1^n=g_1^{ml}$ where $l\in \mathbb{Z}$ $\Rightarrow$ $g_1^{n-ml}=e$ and since $H_1$ is infinite $\Rightarrow$ $n=ml$ which is contradiciton since $m>n$. Thus, the subgroups $K_n$ for any $n\in \mathbb{N}$ are cyclic subgroups of $H_1$ $\Rightarrow$ cyclic subgroups of $G$. Is this reasoning correct?","If $G$ is an infinite group then $G$ has infinitely many subgroups. Proof: Let's consider the following set: $C=\{\left \langle g \right \rangle: g\in G \}$ - collection of all cyclic subgroups in $G$ generated by elements of $G$. Two cases are possible: Exists infinitely many distinct cyclic subgroups $\Rightarrow$ We are done. Exists finitely many distinct cyclic subgroups for example $C=\{H_1, H_2,\dots, H_n\}$. Then $G=\bigcup \limits_{i=1}^{n}H_i$. Since $G$ is infinite then WLOG suppose that $H_1$ is also infinite, where $H_1=\left \langle g_1 \right \rangle$. Let's consider the following set $\{\left \langle g_1^n \right \rangle: n\in \mathbb{N}\}$ - the collection of all cyclic sugroups of $H_1\subset G.$  Let $K_1=\left \langle g_1 \right \rangle$, $K_2=\left \langle g_1^2 \right \rangle$, $K_3=\left \langle g_1^3 \right \rangle$, $\dots$. It's easy to show that $K_n$ and $K_m$ are distinct for $n\neq m$. Indeed, WLOG take $n<m$ and taking $g_1^n\in K_n$ but $g_1^n\notin K_m$ otherwise $g_1^n=g_1^{ml}$ where $l\in \mathbb{Z}$ $\Rightarrow$ $g_1^{n-ml}=e$ and since $H_1$ is infinite $\Rightarrow$ $n=ml$ which is contradiciton since $m>n$. Thus, the subgroups $K_n$ for any $n\in \mathbb{N}$ are cyclic subgroups of $H_1$ $\Rightarrow$ cyclic subgroups of $G$. Is this reasoning correct?",,"['abstract-algebra', 'group-theory', 'solution-verification', 'infinite-groups']"
55,A big list - Applications of the structure theorem of finitely generated modules over PIDs,A big list - Applications of the structure theorem of finitely generated modules over PIDs,,"I'm now a TA on an undergraduate course ""Algebra II"" and the main topics of the course are ""rings and modules"" and ""fields and the Galois theory"". We shall cover the fundamental result, namely the structure theorem of finitely generated module over PIDs . Two typical applications of the theorem are The structure theorem of finitely generated abelian groups (as $\mathbb{Z}$ -modules); The canonical forms (including Smith canonical forms and Jordan canonical forms) in linear algebra (by viewing the finite dimensional $k$ -vector space $V$ as a $k[\lambda]$ -module). In this post, I'm hoping to collect some interesting applications of the structure theorem besides the two examples above, since the two examples can be seen in almost all textbooks, and these may be boring for students. For example, this is an interesting application: Solutions of homogeneous linear differential equations are a special case of structure theorem for f.g. modules over a PID . Although this is somehow a special case of the ""canonical form"" example, this is rather surprising and can be regarded as a good example for me. And moreover, although I'm actually collecting examples for the course, any applications beyond the scope of the course are still great ! For example, as @KCd hinted in the comment, the theorem can be seen in basic algebraic number theory all the time, such as the Dirichlet unit theorem. Thank you all in advance! And thanks @KCd for his helpful comments!","I'm now a TA on an undergraduate course ""Algebra II"" and the main topics of the course are ""rings and modules"" and ""fields and the Galois theory"". We shall cover the fundamental result, namely the structure theorem of finitely generated module over PIDs . Two typical applications of the theorem are The structure theorem of finitely generated abelian groups (as -modules); The canonical forms (including Smith canonical forms and Jordan canonical forms) in linear algebra (by viewing the finite dimensional -vector space as a -module). In this post, I'm hoping to collect some interesting applications of the structure theorem besides the two examples above, since the two examples can be seen in almost all textbooks, and these may be boring for students. For example, this is an interesting application: Solutions of homogeneous linear differential equations are a special case of structure theorem for f.g. modules over a PID . Although this is somehow a special case of the ""canonical form"" example, this is rather surprising and can be regarded as a good example for me. And moreover, although I'm actually collecting examples for the course, any applications beyond the scope of the course are still great ! For example, as @KCd hinted in the comment, the theorem can be seen in basic algebraic number theory all the time, such as the Dirichlet unit theorem. Thank you all in advance! And thanks @KCd for his helpful comments!",\mathbb{Z} k V k[\lambda],"['abstract-algebra', 'commutative-algebra', 'modules', 'examples-counterexamples', 'big-list']"
56,Applications of Jordan-Holder theorem in an abelian category,Applications of Jordan-Holder theorem in an abelian category,,"The Jordan-Holder theorem says that any chain of subobjects of a finite lenght object can be refined to a composition series, and that any composition series has the same lenght. This theorem holds for any abelian category, and a notable example is the case of modules over some ring. While I do not need an example of the usefulness of J-H theorem in the context of modules, I would like to ask: What are applications of the J-H theorem in a general abelian category,   which is not (or not easily proven to be) a category of modules?","The Jordan-Holder theorem says that any chain of subobjects of a finite lenght object can be refined to a composition series, and that any composition series has the same lenght. This theorem holds for any abelian category, and a notable example is the case of modules over some ring. While I do not need an example of the usefulness of J-H theorem in the context of modules, I would like to ask: What are applications of the J-H theorem in a general abelian category,   which is not (or not easily proven to be) a category of modules?",,"['abstract-algebra', 'category-theory', 'modules', 'homological-algebra', 'abelian-categories']"
57,"Finite algebraic structures where all hyperoperations (addition, multiplication, exponentiation, tetration, etc.) are well-defined","Finite algebraic structures where all hyperoperations (addition, multiplication, exponentiation, tetration, etc.) are well-defined",,"Let $\langle R, +, \times, \uparrow, \uparrow\uparrow, \uparrow\uparrow\uparrow, \ldots; 0, 1\rangle$ be an algebraic structure with two constants $0, 1$ and where an infinite sequence of binary hyperoperations is defined, such that that the following holds ( $x\uparrow y$ will equivalently be denoted as $x^y$ ): The substructure $\langle R, +, \times, \uparrow, 0, 1\rangle$ satisfies all of Tarski's identities plus $x+0=x$ , $x\times 0=0$ , $x^0=1$ . In particular, $\langle R, +, \times, 0, 1\rangle$ is a commutative semiring. All higher-order hyperoperations satisfy the identities $x\uparrow^n 0 = 1$ , $x\uparrow^n (y+1) = x\uparrow^{n-1} (x\uparrow^n y)$ for all $n\ge  2$ . The paradigmatic example of such a structure is of course the infinite set of natural numbers $\mathbb{N}$ . However, I'm interested in exploring examples of finite structures of this sort. To simplify the problem, I am for the moment focusing on structures which are ""like $\mathbb{N}$ "" in the sense that all their elements can be obtained from $0$ by repeated incrementation. That is, we assume the following: $R$ is additively generated by $1$ (all its elements can be put as a finite sum of the form $1+1+\ldots+1$ , where the empty sum is taken to be $0$ ). $R$ has finite cardinality as a set. Alternatively, $R$ can be thought of as a finite model of Peano arithmetic, excluding the two axioms $\forall m,n, (S(m)=S(n)) \implies (m=n)$ and $\forall n, S(n)\neq 0$ , and including the corresponding inductive definitions for all the hyperoperations. Conditions 3 and 4 together imply that there must be an identity of the form $$\underbrace{1+1+\ldots+1}_{b\: \text{times}} = \underbrace{1+1+\ldots+1}_{b+a\: \text{times}}$$ where $a$ is a positive integer and $b$ a nonnegative integer. Since this identity must be invariant under the sucessor function $x \mapsto x+1$ and equality is transitive, this means that any such structure must be a finite quotient of $\mathbb{N}$ of the form $\mathbb{N}/(a\mathbb{N}+b)$ , where $a\mathbb{N}+b$ is the congruence relation where two distinct numbers are identified iff they are both greater than $b$ and they differ by a multiple of $a$ (which we will denote by $x \equiv y \mod_{\ge b} a$ ). Thanks to the distributive property, it's straightforward to check that multiplication can be uniquely defined in any quotient of this form, turning it into a semiring. I'm fairly sure this part of my reasoning is correct, as for example this answer from MathOverflow reaches the same conclusion. However, exponentiation cannot always be consistently defined for arbitrary $a$ and $b$ . For example, we have $2\equiv 5 \mod_{\ge 0} 3$ but $2^2 \not\equiv 2^5 \mod_{\ge 0} 3$ , so the ring $\mathbb{N}/(3\mathbb{N}+0) \cong \mathbb{Z}/\mathbb{3Z}$ is not compatible with exponentiation (in fact no nontrivial ring can be compatible with exponentiation, since if $1$ has an additive inverse we must have $1 = 0^0 = 0^{-1 + 1} = 0^{-1} \times 0 = 0$ ). Ordinary modular exponentiation has the property $$k^{\nu(n)} \equiv  k^{\nu(n)+\lambda(n)} \mod_{\ge 0} n,$$ for any integer $k$ , where $\lambda(n)$ is the Carmichael function of $n$ and $\nu(n) = \max_{p\mid n} \nu_p (n)$ is the maximum exponent appearing in the prime factorization of $n$ (with $\nu(1) = 0$ ). Since $k^b \ge b$ for any $k \ge 2$ and nonnegative $b$ , we have that the congruence $$k^b \equiv  k^{a+b} \mod_{\ge b} a$$ will hold for any $k$ if $\lambda(a) \mid a$ and $\nu(a) \le b$ . Call $\Lambda$ the set of pairs $(a,b)$ satisfying these two conditions (the set of possible $a$ form the sequence A124240 in OEIS). The property $m \mid n \implies (\lambda(m) \mid \lambda(n)) \land (\nu(m) \le \nu(n))$ implies that $(\lambda(a), \nu(a)) \in \Lambda$ whenever $(a, b) \in \Lambda$ , so that power towers of arbitrary height can be consistently defined in $\mathbb{N}/(a\mathbb{N}+b)$ . The corresponding congruence relation satisfies $$\forall k\in R, \quad (s\equiv t) \implies (s+k\equiv t+k) \land (s\times k\equiv t\times k) \land (s^k\equiv t^k) \land (k^s\equiv k^t)$$ (the first three congruences on the right side come from basic modular arithmetic), so all of Tarski's identities, which hold in $\mathbb{N}$ , will be preserved when taking the quotient. As for higher order hyperoperations, this is where I have the most doubts. I could only think of two further restrictions: First, base-zero tetration $0 \uparrow \uparrow k$ for $k\in \mathbb{N}$ defines a periodic sequence of period 2 alternating between $1$ and $0$ . If we are to have $0 \uparrow \uparrow b = 0 \uparrow \uparrow (b+a)$ , either $a$ is even or we get the trivial case $0=1$ . In this answer it is shown that modular tetration $x \uparrow \uparrow k \mod_{\ge 0} a$ for any nonzero $x$ is eventually constant as $k \to \infty$ ; call the limiting value $\hat{x}$ and $L(a)$ the least number such that $x \uparrow \uparrow L(a) \equiv \hat{x}$ for all $x$ . A sufficient condition for $$x\uparrow \uparrow b \equiv  x\uparrow \uparrow (b+a) \mod_{\ge b} a$$ to hold, then, is to simply choose $b \ge L(a)$ such that both numbers are congruent to $\hat{x}$ . I believe this condition is also necessary, since if we choose $b \le L(a) - 1$ then there is some $y$ for which $y\uparrow\uparrow (L(a)-1)$ is not congruent to $y\uparrow\uparrow (L(a)-1+a) \equiv \hat{y}$ . These two restrictions also apply for higher-order hyperoperations, as they eventually reduce to repeated tetration. Call $\Lambda^*$ the set of pairs $(a,b) \in \Lambda$ satisfying the two extra conditions $2 \mid a$ and $L(a) \le b$ . If I haven't made any mistake so far, the quotients $\mathbb{N}/(a\mathbb{N}+b)$ with $(a,b) \in \Lambda^*$ are then examples of finite structures compatible with the whole sequence of hyperoperations. The only other finite structure of this sort which is a quotient of $\mathbb{N}$ would be the trivial ring with $0=1$ . My question is: Is my whole reasoning so far essentially correct? Are there any other restrictions that I missed? EDIT (23-06-2021): I found some literature regarding finite semirings with exponentiation. They seem to be known as HSI-algebras in this context, since Tarski's identities are sometimes referred to as the High School Identities. In particular, Theorem 1.1 of this article confirms that the only finite quotients $\mathbb{N}/(a\mathbb{N}+b)$ admitting exponentiation are the ones such that for any prime $p$ and $e \in \mathbb{N}$ we have $$p^e|a \implies e \le b \qquad \text{and} \qquad p|a \implies (p-1)|a,$$ which is equivalent to $(a,b) \in \Lambda$ (the theorem actually considers quotients of $\mathbb{N}-\{0\}$ , but it can be easily extended to the case with $0$ ). I consider this part of the question settled.","Let be an algebraic structure with two constants and where an infinite sequence of binary hyperoperations is defined, such that that the following holds ( will equivalently be denoted as ): The substructure satisfies all of Tarski's identities plus , , . In particular, is a commutative semiring. All higher-order hyperoperations satisfy the identities , for all . The paradigmatic example of such a structure is of course the infinite set of natural numbers . However, I'm interested in exploring examples of finite structures of this sort. To simplify the problem, I am for the moment focusing on structures which are ""like "" in the sense that all their elements can be obtained from by repeated incrementation. That is, we assume the following: is additively generated by (all its elements can be put as a finite sum of the form , where the empty sum is taken to be ). has finite cardinality as a set. Alternatively, can be thought of as a finite model of Peano arithmetic, excluding the two axioms and , and including the corresponding inductive definitions for all the hyperoperations. Conditions 3 and 4 together imply that there must be an identity of the form where is a positive integer and a nonnegative integer. Since this identity must be invariant under the sucessor function and equality is transitive, this means that any such structure must be a finite quotient of of the form , where is the congruence relation where two distinct numbers are identified iff they are both greater than and they differ by a multiple of (which we will denote by ). Thanks to the distributive property, it's straightforward to check that multiplication can be uniquely defined in any quotient of this form, turning it into a semiring. I'm fairly sure this part of my reasoning is correct, as for example this answer from MathOverflow reaches the same conclusion. However, exponentiation cannot always be consistently defined for arbitrary and . For example, we have but , so the ring is not compatible with exponentiation (in fact no nontrivial ring can be compatible with exponentiation, since if has an additive inverse we must have ). Ordinary modular exponentiation has the property for any integer , where is the Carmichael function of and is the maximum exponent appearing in the prime factorization of (with ). Since for any and nonnegative , we have that the congruence will hold for any if and . Call the set of pairs satisfying these two conditions (the set of possible form the sequence A124240 in OEIS). The property implies that whenever , so that power towers of arbitrary height can be consistently defined in . The corresponding congruence relation satisfies (the first three congruences on the right side come from basic modular arithmetic), so all of Tarski's identities, which hold in , will be preserved when taking the quotient. As for higher order hyperoperations, this is where I have the most doubts. I could only think of two further restrictions: First, base-zero tetration for defines a periodic sequence of period 2 alternating between and . If we are to have , either is even or we get the trivial case . In this answer it is shown that modular tetration for any nonzero is eventually constant as ; call the limiting value and the least number such that for all . A sufficient condition for to hold, then, is to simply choose such that both numbers are congruent to . I believe this condition is also necessary, since if we choose then there is some for which is not congruent to . These two restrictions also apply for higher-order hyperoperations, as they eventually reduce to repeated tetration. Call the set of pairs satisfying the two extra conditions and . If I haven't made any mistake so far, the quotients with are then examples of finite structures compatible with the whole sequence of hyperoperations. The only other finite structure of this sort which is a quotient of would be the trivial ring with . My question is: Is my whole reasoning so far essentially correct? Are there any other restrictions that I missed? EDIT (23-06-2021): I found some literature regarding finite semirings with exponentiation. They seem to be known as HSI-algebras in this context, since Tarski's identities are sometimes referred to as the High School Identities. In particular, Theorem 1.1 of this article confirms that the only finite quotients admitting exponentiation are the ones such that for any prime and we have which is equivalent to (the theorem actually considers quotients of , but it can be easily extended to the case with ). I consider this part of the question settled.","\langle R, +, \times, \uparrow, \uparrow\uparrow, \uparrow\uparrow\uparrow, \ldots; 0, 1\rangle 0, 1 x\uparrow y x^y \langle R, +, \times, \uparrow, 0, 1\rangle x+0=x x\times 0=0 x^0=1 \langle R, +, \times, 0, 1\rangle x\uparrow^n 0 = 1 x\uparrow^n (y+1) = x\uparrow^{n-1} (x\uparrow^n y) n\ge  2 \mathbb{N} \mathbb{N} 0 R 1 1+1+\ldots+1 0 R R \forall m,n, (S(m)=S(n)) \implies (m=n) \forall n, S(n)\neq 0 \underbrace{1+1+\ldots+1}_{b\: \text{times}} = \underbrace{1+1+\ldots+1}_{b+a\: \text{times}} a b x \mapsto x+1 \mathbb{N} \mathbb{N}/(a\mathbb{N}+b) a\mathbb{N}+b b a x \equiv y \mod_{\ge b} a a b 2\equiv 5 \mod_{\ge 0} 3 2^2 \not\equiv 2^5 \mod_{\ge 0} 3 \mathbb{N}/(3\mathbb{N}+0) \cong \mathbb{Z}/\mathbb{3Z} 1 1 = 0^0 = 0^{-1 + 1} = 0^{-1} \times 0 = 0 k^{\nu(n)} \equiv  k^{\nu(n)+\lambda(n)} \mod_{\ge 0} n, k \lambda(n) n \nu(n) = \max_{p\mid n} \nu_p (n) n \nu(1) = 0 k^b \ge b k \ge 2 b k^b \equiv  k^{a+b} \mod_{\ge b} a k \lambda(a) \mid a \nu(a) \le b \Lambda (a,b) a m \mid n \implies (\lambda(m) \mid \lambda(n)) \land (\nu(m) \le \nu(n)) (\lambda(a), \nu(a)) \in \Lambda (a, b) \in \Lambda \mathbb{N}/(a\mathbb{N}+b) \forall k\in R, \quad (s\equiv t) \implies (s+k\equiv t+k) \land (s\times k\equiv t\times k) \land (s^k\equiv t^k) \land (k^s\equiv k^t) \mathbb{N} 0 \uparrow \uparrow k k\in \mathbb{N} 1 0 0 \uparrow \uparrow b = 0 \uparrow \uparrow (b+a) a 0=1 x \uparrow \uparrow k \mod_{\ge 0} a x k \to \infty \hat{x} L(a) x \uparrow \uparrow L(a) \equiv \hat{x} x x\uparrow \uparrow b \equiv  x\uparrow \uparrow (b+a) \mod_{\ge b} a b \ge L(a) \hat{x} b \le L(a) - 1 y y\uparrow\uparrow (L(a)-1) y\uparrow\uparrow (L(a)-1+a) \equiv \hat{y} \Lambda^* (a,b) \in \Lambda 2 \mid a L(a) \le b \mathbb{N}/(a\mathbb{N}+b) (a,b) \in \Lambda^* \mathbb{N} 0=1 \mathbb{N}/(a\mathbb{N}+b) p e \in \mathbb{N} p^e|a \implies e \le b \qquad \text{and} \qquad p|a \implies (p-1)|a, (a,b) \in \Lambda \mathbb{N}-\{0\} 0","['abstract-algebra', 'elementary-number-theory', 'solution-verification', 'modular-arithmetic', 'hyperoperation']"
58,"Affine group, $[L : \mathbb{Q}] = n\varphi(n)$ or ${1\over2}n\varphi(n)$","Affine group,  or",[L : \mathbb{Q}] = n\varphi(n) {1\over2}n\varphi(n),"Let $L$ be the Galois closure of $K = \mathbb{Q}(\sqrt[n]{a})$, where $a \in \mathbb{Q}$, $a > 0$ and suppose $[K : \mathbb{Q}] = n$. How do I see that $[L: \mathbb{Q}] = n\varphi(n)$ or ${1\over2}n\varphi(n)$? Remarks. Note that $\mathbb{Q}(\zeta_n) \cap K$ is a Galois extension of $\mathbb{Q}$. Is there a way to use the affine group here?","Let $L$ be the Galois closure of $K = \mathbb{Q}(\sqrt[n]{a})$, where $a \in \mathbb{Q}$, $a > 0$ and suppose $[K : \mathbb{Q}] = n$. How do I see that $[L: \mathbb{Q}] = n\varphi(n)$ or ${1\over2}n\varphi(n)$? Remarks. Note that $\mathbb{Q}(\zeta_n) \cap K$ is a Galois extension of $\mathbb{Q}$. Is there a way to use the affine group here?",,"['abstract-algebra', 'group-theory']"
59,Find the center of the symmetry group $S_n$.,Find the center of the symmetry group .,S_n,"Find the center of the symmetry group $S_n$. Attempt: By definition, the center is $Z(S_n) = \{ a \in S_n : ag = ga \forall\  g \in S_n\}$. Then we know the identity $e$ is in $S_n$ since there is always the trivial permutation. Suppose $a$ is in $S_n$, but not equal to identity. Now we can imagine the permutation as bijective function that maps from $\{1,2,\dotsc,n\}$ to $\{1,2,\dotsc,n\}$. So suppose $p$ is a permutation map. Then $p$ maps from a location $i$ to a location $j$. Take $p(i) = j$ where $i\neq j$. Let $k$ be in $\{1,2,\dotsc,n\}$, where $k$, $i$ and $j$ are all different elements. The cycle $r = (jk)$, then we will see if this commutes.  $rp(i) = rj$ Can someone please help me, I am stuck?  Thank you.","Find the center of the symmetry group $S_n$. Attempt: By definition, the center is $Z(S_n) = \{ a \in S_n : ag = ga \forall\  g \in S_n\}$. Then we know the identity $e$ is in $S_n$ since there is always the trivial permutation. Suppose $a$ is in $S_n$, but not equal to identity. Now we can imagine the permutation as bijective function that maps from $\{1,2,\dotsc,n\}$ to $\{1,2,\dotsc,n\}$. So suppose $p$ is a permutation map. Then $p$ maps from a location $i$ to a location $j$. Take $p(i) = j$ where $i\neq j$. Let $k$ be in $\{1,2,\dotsc,n\}$, where $k$, $i$ and $j$ are all different elements. The cycle $r = (jk)$, then we will see if this commutes.  $rp(i) = rj$ Can someone please help me, I am stuck?  Thank you.",,"['abstract-algebra', 'group-theory', 'permutations', 'symmetric-groups']"
60,"""Efficient version"" of Cayley's Theorem in Group Theory","""Efficient version"" of Cayley's Theorem in Group Theory",,"I'm considering finite groups only. Cayley's theorem says the a group $G$ is isomorphic to a subgroup of $S_{|G|}$. I think it's interesting to ask for smaller values of $n$ for which $G$ is a subgroup of $S_n$. Obviously, it's not always possible to do better than Cayley's theorem. But sometimes it is possible (for example, $\mathbb{Z}_6$ as a subgroup of $S_5$). So I'm asking: Given a finite group $G$, is there an algorithmic way to find or approximate the minimal $n$ for which $G$ is isomorphic to a subgroup of $S_n$? If the answer to $(1)$ is not known, is it known for specific classes of groups? In particular, for finite abelian groups, is it true that for a prime $p$, the minimal $n$ for $\mathbb{Z}_{p^{t_1}} \times \mathbb{Z}_{p^{t_2}}$ is $p^{t_1}+p^{t_2}$ (I can prove that is is true for different primes $p_1$ and $p_2$, but have problems when it's the same prime in both factors). Thanks!","I'm considering finite groups only. Cayley's theorem says the a group $G$ is isomorphic to a subgroup of $S_{|G|}$. I think it's interesting to ask for smaller values of $n$ for which $G$ is a subgroup of $S_n$. Obviously, it's not always possible to do better than Cayley's theorem. But sometimes it is possible (for example, $\mathbb{Z}_6$ as a subgroup of $S_5$). So I'm asking: Given a finite group $G$, is there an algorithmic way to find or approximate the minimal $n$ for which $G$ is isomorphic to a subgroup of $S_n$? If the answer to $(1)$ is not known, is it known for specific classes of groups? In particular, for finite abelian groups, is it true that for a prime $p$, the minimal $n$ for $\mathbb{Z}_{p^{t_1}} \times \mathbb{Z}_{p^{t_2}}$ is $p^{t_1}+p^{t_2}$ (I can prove that is is true for different primes $p_1$ and $p_2$, but have problems when it's the same prime in both factors). Thanks!",,"['group-theory', 'permutations']"
61,"Description of $\mathrm{Ext}^1(R/I,R/J)$",Description of,"\mathrm{Ext}^1(R/I,R/J)","Let $R$ be a commutative ring with unit and $I$ and $J$ are nonzero ideals of $R$. Do we have a nice description for $\mathrm{Ext}^1_R(R/I,R/J)$? What do I mean by a nice description? For example $$\mathrm{Tor}_1^R(R/I,R/J)=(I\cap J)/IJ,$$ so I would say that this is a nice description. So can we find something like this for $\mathrm{Ext}$, say for $J=0$? We do have a description for $\mathrm{Ext}$ when $J=I$, $$\mathrm{Ext}^1_R(R/I,R/I)=\mathrm{Hom}(\mathrm{Tor}^1_R(R/I,R/I), R/I)$$","Let $R$ be a commutative ring with unit and $I$ and $J$ are nonzero ideals of $R$. Do we have a nice description for $\mathrm{Ext}^1_R(R/I,R/J)$? What do I mean by a nice description? For example $$\mathrm{Tor}_1^R(R/I,R/J)=(I\cap J)/IJ,$$ so I would say that this is a nice description. So can we find something like this for $\mathrm{Ext}$, say for $J=0$? We do have a description for $\mathrm{Ext}$ when $J=I$, $$\mathrm{Ext}^1_R(R/I,R/I)=\mathrm{Hom}(\mathrm{Tor}^1_R(R/I,R/I), R/I)$$",,"['abstract-algebra', 'commutative-algebra', 'homological-algebra']"
62,cohomological proof of Maschke's theorem,cohomological proof of Maschke's theorem,,"I have been working on the following problem. . I have spent plenty of time trying to solve it myself. I am, however, unable to prove one small step in the argument. Beneath you can find my attempt. Step 2 (showing that $D$ is a principal derivation) is not so clear to me. I guess there must be some direct argument that says that $H^1(G, k^{m \times n}) = 0$, rather than doing explicit calculations. Any help or advice is appreciated. Thanks. 1) We first show that $D: G \to k^{m \times n}$ is a derivation. For this we check that $D(gh)=gD(h) + D(g)$. \begin{eqnarray*} gD(h) + D(g) &=& g \cdot [ C(h) B^{-1}(h) ] + C(g)B^{-1}(g) \\ &=& A(g)C(h)B^{-1}(h)B^{-1}(g) + C(g)B^{-1}(g) \\ &=& A(g)C(h)B^{-1}(gh) + C(g)B^{-1}(g) \\ &=& [A(g)C(h) + C(g)B^{-1}(g) B(gh) ] B^{-1}(gh) \\ &=& [A(g)C(h) + C(g)B^{-1}(g) B(g) B(h) ] B^{-1}(gh) \\ &=& [A(g)C(h) + C(g) B(h) ] B^{-1}(gh) \\ &=& C(gh)B^{-1}(gh) \\ &=& D(gh) \end{eqnarray*} The second last step can be understood by looking at the matrix representation of $\rho$. When one uses the fact that $\rho(gh)=\rho(g)\rho(h)$ and looks explicitly at the matrix multiplications one sees the desired identity in the right-top corner. 2) I don't see why $D$ is principal... 3) First bring $\begin{pmatrix} 1 & M \\ 0 & 1 \end{pmatrix}^{-1}$ to the right hand side. Next work out the left and right-hand matrix multiplications explicitly: \begin{eqnarray*} \begin{pmatrix} A(g) & C(g) + M B(g) \\ 0 & B(g) \end{pmatrix} &=& \begin{pmatrix} 1 & M \\ 0 & 1 \end{pmatrix} \cdot \begin{pmatrix} A(g) & C(g) \\ 0 & B(g) \end{pmatrix}  \\ &=& \begin{pmatrix} A(g) & 0 \\ 0 & B(g) \end{pmatrix} \cdot \begin{pmatrix} 1 & M \\ 0 & 1 \end{pmatrix} \\ &=& \begin{pmatrix} A(g) & A(g) M \\ 0 & B(g) \end{pmatrix} \end{eqnarray*} Taking the identity of the right-top corners, we get that $C(g) + M B(g) = A(g)M$. This is the condition that must hold such that we have $$\begin{pmatrix} 1 & M \\ 0 & 1 \end{pmatrix} \cdot \begin{pmatrix} A(g) & C(g) \\ 0 & B(g) \end{pmatrix} \cdot \begin{pmatrix} 1 & M \\ 0 & 1 \end{pmatrix}^{-1} = \begin{pmatrix} A(g) & 0 \\ 0 & B(g) \end{pmatrix}$$ But this condition is satisfied whenever $D$ is a principal derivation. When this is the case, there exists an $M$ such that $$C(g)B(g)^{-1} = D(g)= g \cdot M - M = A(g) M B(g)^{-1} - M$$ or $$C(g) = A(g) M - M B(g)$$ or $$C(g) + M B(g) = A(g)M$$ which was the desired result.","I have been working on the following problem. . I have spent plenty of time trying to solve it myself. I am, however, unable to prove one small step in the argument. Beneath you can find my attempt. Step 2 (showing that $D$ is a principal derivation) is not so clear to me. I guess there must be some direct argument that says that $H^1(G, k^{m \times n}) = 0$, rather than doing explicit calculations. Any help or advice is appreciated. Thanks. 1) We first show that $D: G \to k^{m \times n}$ is a derivation. For this we check that $D(gh)=gD(h) + D(g)$. \begin{eqnarray*} gD(h) + D(g) &=& g \cdot [ C(h) B^{-1}(h) ] + C(g)B^{-1}(g) \\ &=& A(g)C(h)B^{-1}(h)B^{-1}(g) + C(g)B^{-1}(g) \\ &=& A(g)C(h)B^{-1}(gh) + C(g)B^{-1}(g) \\ &=& [A(g)C(h) + C(g)B^{-1}(g) B(gh) ] B^{-1}(gh) \\ &=& [A(g)C(h) + C(g)B^{-1}(g) B(g) B(h) ] B^{-1}(gh) \\ &=& [A(g)C(h) + C(g) B(h) ] B^{-1}(gh) \\ &=& C(gh)B^{-1}(gh) \\ &=& D(gh) \end{eqnarray*} The second last step can be understood by looking at the matrix representation of $\rho$. When one uses the fact that $\rho(gh)=\rho(g)\rho(h)$ and looks explicitly at the matrix multiplications one sees the desired identity in the right-top corner. 2) I don't see why $D$ is principal... 3) First bring $\begin{pmatrix} 1 & M \\ 0 & 1 \end{pmatrix}^{-1}$ to the right hand side. Next work out the left and right-hand matrix multiplications explicitly: \begin{eqnarray*} \begin{pmatrix} A(g) & C(g) + M B(g) \\ 0 & B(g) \end{pmatrix} &=& \begin{pmatrix} 1 & M \\ 0 & 1 \end{pmatrix} \cdot \begin{pmatrix} A(g) & C(g) \\ 0 & B(g) \end{pmatrix}  \\ &=& \begin{pmatrix} A(g) & 0 \\ 0 & B(g) \end{pmatrix} \cdot \begin{pmatrix} 1 & M \\ 0 & 1 \end{pmatrix} \\ &=& \begin{pmatrix} A(g) & A(g) M \\ 0 & B(g) \end{pmatrix} \end{eqnarray*} Taking the identity of the right-top corners, we get that $C(g) + M B(g) = A(g)M$. This is the condition that must hold such that we have $$\begin{pmatrix} 1 & M \\ 0 & 1 \end{pmatrix} \cdot \begin{pmatrix} A(g) & C(g) \\ 0 & B(g) \end{pmatrix} \cdot \begin{pmatrix} 1 & M \\ 0 & 1 \end{pmatrix}^{-1} = \begin{pmatrix} A(g) & 0 \\ 0 & B(g) \end{pmatrix}$$ But this condition is satisfied whenever $D$ is a principal derivation. When this is the case, there exists an $M$ such that $$C(g)B(g)^{-1} = D(g)= g \cdot M - M = A(g) M B(g)^{-1} - M$$ or $$C(g) = A(g) M - M B(g)$$ or $$C(g) + M B(g) = A(g)M$$ which was the desired result.",,"['abstract-algebra', 'representation-theory', 'homological-algebra']"
63,Dual of a finite dimensional algebra is a coalgebra (ex. from Sweedler),Dual of a finite dimensional algebra is a coalgebra (ex. from Sweedler),,"Let $(A, M, u)$ be a finite dimensional algebra where $M: A\otimes A \rightarrow A$ denotes multiplication and $u: k \rightarrow A$ denotes unit. I want to prove that $(A^*, \Delta, \varepsilon) $ is a colagebra where $\Delta: A^*\rightarrow A^* \otimes A^*$ is a composition:  $$A^* \overset{M^*}{\rightarrow}(A\otimes A)^* \overset{\rho^{-1}}{\rightarrow}A^*\otimes A^*$$ And $\rho: V^*\otimes W^* \rightarrow (V\otimes W)^*$ is given by $<\rho(v^*, w^*), v\otimes w>=<v^*, v><w^*,w>$. I have proven that $\rho$ is injective and since $A$ is finite dimensional $\rho$ is also bijective and we can take the inverse $\rho^{-1}$. But I have problems understanding how does $\Delta$ work. By definition we have $<M^*(c^*), a\otimes b>=<c^*, M(a\otimes b)>=c^*(ab)$. But I can't understand what is $\rho^{-1}(M^*(c^*))$, or in other words which element of $A^*\otimes A^*$ can act like $M^*(c^*)$ via $\rho$? P.S. Please correct me if I have grammar mistakes. Thanks!","Let $(A, M, u)$ be a finite dimensional algebra where $M: A\otimes A \rightarrow A$ denotes multiplication and $u: k \rightarrow A$ denotes unit. I want to prove that $(A^*, \Delta, \varepsilon) $ is a colagebra where $\Delta: A^*\rightarrow A^* \otimes A^*$ is a composition:  $$A^* \overset{M^*}{\rightarrow}(A\otimes A)^* \overset{\rho^{-1}}{\rightarrow}A^*\otimes A^*$$ And $\rho: V^*\otimes W^* \rightarrow (V\otimes W)^*$ is given by $<\rho(v^*, w^*), v\otimes w>=<v^*, v><w^*,w>$. I have proven that $\rho$ is injective and since $A$ is finite dimensional $\rho$ is also bijective and we can take the inverse $\rho^{-1}$. But I have problems understanding how does $\Delta$ work. By definition we have $<M^*(c^*), a\otimes b>=<c^*, M(a\otimes b)>=c^*(ab)$. But I can't understand what is $\rho^{-1}(M^*(c^*))$, or in other words which element of $A^*\otimes A^*$ can act like $M^*(c^*)$ via $\rho$? P.S. Please correct me if I have grammar mistakes. Thanks!",,"['abstract-algebra', 'tensor-products']"
64,(Mathematical) Applications of Quantum Group,(Mathematical) Applications of Quantum Group,,What are some mathematical applications of Quantum Groups? I have tried researching and found that it is used to find solutions to Yang-Baxter equation. Any other applications? Thanks a lot.,What are some mathematical applications of Quantum Groups? I have tried researching and found that it is used to find solutions to Yang-Baxter equation. Any other applications? Thanks a lot.,,"['abstract-algebra', 'soft-question']"
65,Why can't we extend any field by simply adding a new symbol to it?,Why can't we extend any field by simply adding a new symbol to it?,,"After trying to recall some fundamental field theory, I got very confused at the notion of field extensions. For example, when we make $\mathbb{C}$ out of $\mathbb{R}$ , we can simply think of it as adding $i$ , which is a symbol for which $i^2 = -1$ . So: $\mathbb{C} = \mathbb{R}(i)$ . However, $\mathbb{C}$ is algebraically closed, since each polynomial with complex coefficients has a root in $\mathbb{C}$ . I suppose what confuses me is why it follows from here that there exists no further field extension of $\mathbb{C}$ . This is always stated as obvious, but I can not make a formal argument. Why can't we always simply add a new symbol $z$ to a given field $k$ , which squares to any element of $k(z) = \{ a + b z \; | \; a,b \in k \}$ . For simplicity, let's say that $z^2 = 1$ . I know that  we already have $1^2 = 1$ and $(-1)^2 = 1$ , but why is this an issue? Why can't we have two distinct elements square to the same element? By defining addition as $$ (a+bz) + (c+dz) = (a+c) + (b+d)z $$ and multiplication as $$ (a+bz)\cdot(c+dz) = ac + adz + bcz + bdz^2 = (ac + bd) + (ad + bc)z,$$ all the properties for being a field are easily verified. So, what's the issue here? Edit: Thank you everyone for very insightful comments. As a follow-up, since we can do extensions of any fields (although they are not themselves necessarily fields), how can we conclude that an algebraically closed field (say $\mathbb{C}$ ) has no finite field extension, i.e. none of such extension by some symbol is a field?","After trying to recall some fundamental field theory, I got very confused at the notion of field extensions. For example, when we make out of , we can simply think of it as adding , which is a symbol for which . So: . However, is algebraically closed, since each polynomial with complex coefficients has a root in . I suppose what confuses me is why it follows from here that there exists no further field extension of . This is always stated as obvious, but I can not make a formal argument. Why can't we always simply add a new symbol to a given field , which squares to any element of . For simplicity, let's say that . I know that  we already have and , but why is this an issue? Why can't we have two distinct elements square to the same element? By defining addition as and multiplication as all the properties for being a field are easily verified. So, what's the issue here? Edit: Thank you everyone for very insightful comments. As a follow-up, since we can do extensions of any fields (although they are not themselves necessarily fields), how can we conclude that an algebraically closed field (say ) has no finite field extension, i.e. none of such extension by some symbol is a field?","\mathbb{C} \mathbb{R} i i^2 = -1 \mathbb{C} = \mathbb{R}(i) \mathbb{C} \mathbb{C} \mathbb{C} z k k(z) = \{ a + b z \; | \; a,b \in k \} z^2 = 1 1^2 = 1 (-1)^2 = 1  (a+bz) + (c+dz) = (a+c) + (b+d)z   (a+bz)\cdot(c+dz) = ac + adz + bcz + bdz^2 = (ac + bd) + (ad + bc)z, \mathbb{C}","['abstract-algebra', 'field-theory', 'extension-field']"
66,Can a ring of positive characteristic have infinite number of elements?,Can a ring of positive characteristic have infinite number of elements?,,"For curiosity: can a ring of positive characteristic ever have infinite number of distinct elements? (For example, in $\mathbb{Z}/7\mathbb{Z}$ , there are really only seven elements.) We know that any field/ring of characteristic zero must have infinite elements, but I am not sure what happens above.","For curiosity: can a ring of positive characteristic ever have infinite number of distinct elements? (For example, in , there are really only seven elements.) We know that any field/ring of characteristic zero must have infinite elements, but I am not sure what happens above.",\mathbb{Z}/7\mathbb{Z},"['abstract-algebra', 'ring-theory', 'positive-characteristic']"
67,Has my understanding of a generating set been wrong this entire time?,Has my understanding of a generating set been wrong this entire time?,,"My understanding of the generating set (informally) was the following Pick any number of elements, say $g_1, g_2, \dotso, g_n$, in a finite group $G$. The set of all elements produced by a finite product of any combination of these elements is called the generating set of the elements $g_1, g_2, \dotso, g_n$. However, the course notes had the following definition of a generating set. If $S \subseteq$ G is a subset, then define $S^{-1} =  \left ( s^{-1} | s \in S  \right )$ and let $\langle S \rangle$ denote the set of all elements  of $G$ which can be written as finite products of elements of $S \bigcup S^{-1}.$ Am I missing something in my understanding of the generating set? Because in my understanding of it, there's absolutely nothing to do with any inverse elements whereas the course notes makes some mention of them for some reason.","My understanding of the generating set (informally) was the following Pick any number of elements, say $g_1, g_2, \dotso, g_n$, in a finite group $G$. The set of all elements produced by a finite product of any combination of these elements is called the generating set of the elements $g_1, g_2, \dotso, g_n$. However, the course notes had the following definition of a generating set. If $S \subseteq$ G is a subset, then define $S^{-1} =  \left ( s^{-1} | s \in S  \right )$ and let $\langle S \rangle$ denote the set of all elements  of $G$ which can be written as finite products of elements of $S \bigcup S^{-1}.$ Am I missing something in my understanding of the generating set? Because in my understanding of it, there's absolutely nothing to do with any inverse elements whereas the course notes makes some mention of them for some reason.",,"['abstract-algebra', 'definition']"
68,Do 'symmetric integers' have some other name?,Do 'symmetric integers' have some other name?,,"$-1 \cdot -1 = +1$, but there seems to me to be no reason we couldn't define a number system where negative number's and positive numbers were completely symmetric. Where: $$1 \cdot 1 = 1$$ $$-1 \cdot -1 = -1$$ I understand that in order to do this, multiplication could no longer be commutative and we'd have to decide what the result of $1 \cdot -1$ should be. I think we could choose that resulting sign of a multiplication could be the sign of the second term, so: $$1 \cdot -1 = -1$$ $$-1 \cdot 1 = 1$$ or more generally, the sign of any multiplication is determined by the sign of the second term. But where they otherwise behave roughly as expected, i.e. $1 - 2 = -1$. Some other consequences I'm aware of: $$\sqrt{-1} = -1$$ $$\sqrt{1} = 1$$ $f(x) = x^2$ would behave in a way that can only be described piecewise in the normal reals as $x^2$ when $x \geq 0$, and $-(x^2)$ when $x < 0$. Is there already research or another name for such a number system? Or perhaps is there a ring that matches this? After looking at the properties of a ring, on http://en.wikipedia.org/wiki/Ring_%28mathematics%29 what I've described cannot be a ring since it does not have a multiplicative identity. There is no element i_m such that a * $i_m = a$ and $i_m \cdot a = a$ since multiplying by $1$ in the system I've described may change the sign of $a$ to be positive.","$-1 \cdot -1 = +1$, but there seems to me to be no reason we couldn't define a number system where negative number's and positive numbers were completely symmetric. Where: $$1 \cdot 1 = 1$$ $$-1 \cdot -1 = -1$$ I understand that in order to do this, multiplication could no longer be commutative and we'd have to decide what the result of $1 \cdot -1$ should be. I think we could choose that resulting sign of a multiplication could be the sign of the second term, so: $$1 \cdot -1 = -1$$ $$-1 \cdot 1 = 1$$ or more generally, the sign of any multiplication is determined by the sign of the second term. But where they otherwise behave roughly as expected, i.e. $1 - 2 = -1$. Some other consequences I'm aware of: $$\sqrt{-1} = -1$$ $$\sqrt{1} = 1$$ $f(x) = x^2$ would behave in a way that can only be described piecewise in the normal reals as $x^2$ when $x \geq 0$, and $-(x^2)$ when $x < 0$. Is there already research or another name for such a number system? Or perhaps is there a ring that matches this? After looking at the properties of a ring, on http://en.wikipedia.org/wiki/Ring_%28mathematics%29 what I've described cannot be a ring since it does not have a multiplicative identity. There is no element i_m such that a * $i_m = a$ and $i_m \cdot a = a$ since multiplying by $1$ in the system I've described may change the sign of $a$ to be positive.",,['abstract-algebra']
69,"In a Commutative Ring, is Addition Necessarily Commutative?","In a Commutative Ring, is Addition Necessarily Commutative?",,"In A First Course in Abstract Algebra , Fraleigh writes on p. 172 that ""a ring in which multiplication is commutative is a commutative ring "". Of course, this raises the question: is addition necessarily commutative in a commutative ring? Is it commutative in any ring? Are there examples of rings in which addition is non-commutative? As far as I could tell, commutativity of the underlying additive operation is not one of the defining properties of a ring. Or is it? I don't think the book has been entirely clear on these issues --- I would be thankful if someone could shed some light on this.","In A First Course in Abstract Algebra , Fraleigh writes on p. 172 that ""a ring in which multiplication is commutative is a commutative ring "". Of course, this raises the question: is addition necessarily commutative in a commutative ring? Is it commutative in any ring? Are there examples of rings in which addition is non-commutative? As far as I could tell, commutativity of the underlying additive operation is not one of the defining properties of a ring. Or is it? I don't think the book has been entirely clear on these issues --- I would be thankful if someone could shed some light on this.",,"['abstract-algebra', 'ring-theory']"
70,Why don't we need to check for associativity for a subgroup?,Why don't we need to check for associativity for a subgroup?,,"The textbooks I read say that it's obvious that associativity follows in a subgroup, but it doesn't explain why (since it's not obvious to me). We still have to check for closure under the operation, the identity element, and the inverse, but why does associativity immediately follow for a subgroup just because the group itself has it?","The textbooks I read say that it's obvious that associativity follows in a subgroup, but it doesn't explain why (since it's not obvious to me). We still have to check for closure under the operation, the identity element, and the inverse, but why does associativity immediately follow for a subgroup just because the group itself has it?",,"['abstract-algebra', 'group-theory']"
71,Is there a short proof of $x^2=(-x)^2$ in an arbitrary ring?,Is there a short proof of  in an arbitrary ring?,x^2=(-x)^2,"Identity :  Let $R$ be a ring and $x \in R$.  Then $x^2=(-x)^2.$ It's exam marking time here, and one of the students used the above identity in a proof.  The identity is true, but I can't think of a straightforward proof of this. Question : Is there a short proof of this identity?  ( Note : $R$ might not have a multiplicative identity.) Here's a proof generated by Prover9 , which makes me think there might not be a shorter proof.  However, this might not necessarily be true, since Prover9 can only work with the ring theory axioms I input (and would have to prove any auxiliary lemmata we would take for granted). ============================== PROOF =================================  % Proof 1 at 0.01 (+ 0.00) seconds. % Length of proof is 26. % Level of proof is 10. % Maximum clause weight is 16. % Given clauses 30.  1 x * x = -x * -x # label(non_clause) # label(goal).  [goal]. 2 x + (y + z) = (x + y) + z.  [assumption]. 3 (x + y) + z = x + (y + z).  [copy(2),flip(a)]. 4 x + 0 = x.  [assumption]. 5 0 + x = x.  [assumption]. 6 x + -x = 0.  [assumption]. 8 x + y = y + x.  [assumption]. 10 x * (y + z) = (x * y) + (x * z).  [assumption]. 11 (x + y) * z = (x * z) + (y * z).  [assumption]. 12 -c1 * -c1 != c1 * c1.  [deny(1)]. 13 x + (-x + y) = y.  [para(6(a,1),3(a,1,1)),rewrite([5(2)]),flip(a)]. 18 (x * 0) + (x * y) = x * y.  [para(5(a,1),10(a,1,2)),flip(a)]. 19 (x * y) + (x * -y) = x * 0.  [para(6(a,1),10(a,1,2)),flip(a)]. 24 --x = x.  [para(6(a,1),13(a,1,2)),rewrite([4(2)]),flip(a)]. 25 x + (y + -x) = y.  [para(8(a,1),13(a,1,2))]. 27 (x * y) + ((-x * y) + (z * y)) = z * y.  [para(13(a,1),11(a,1,1)),rewrite([11(5)]),flip(a)]. 33 -x + (y + x) = y.  [para(24(a,1),25(a,1,2,2))]. 40 x + -(x + y) = -y.  [para(33(a,1),33(a,1,2)),rewrite([8(3)])]. 57 -(x + y) = -y + -x.  [para(33(a,1),40(a,1,2,1)),flip(a)]. 69 x * 0 = 0.  [para(18(a,1),33(a,1,2)),rewrite([8(4),6(4)]),flip(a)]. 70 (x * y) + (x * -y) = 0.  [back_rewrite(19),rewrite([69(6)])]. 78 -(x * -y) = x * y.  [para(70(a,1),33(a,1,2)),rewrite([8(5),5(5)])]. 87 x * -y = -(x * y).  [para(78(a,1),24(a,1,1)),flip(a)]. 88 -(-c1 * c1) != c1 * c1.  [back_rewrite(12),rewrite([87(5)])]. 101 -(-x * y) = x * y.  [para(27(a,1),33(a,1,2)),rewrite([57(5),8(8),13(8)])]. 102 $F.  [resolve(101,a,88,a)].  ============================== end of proof ==========================","Identity :  Let $R$ be a ring and $x \in R$.  Then $x^2=(-x)^2.$ It's exam marking time here, and one of the students used the above identity in a proof.  The identity is true, but I can't think of a straightforward proof of this. Question : Is there a short proof of this identity?  ( Note : $R$ might not have a multiplicative identity.) Here's a proof generated by Prover9 , which makes me think there might not be a shorter proof.  However, this might not necessarily be true, since Prover9 can only work with the ring theory axioms I input (and would have to prove any auxiliary lemmata we would take for granted). ============================== PROOF =================================  % Proof 1 at 0.01 (+ 0.00) seconds. % Length of proof is 26. % Level of proof is 10. % Maximum clause weight is 16. % Given clauses 30.  1 x * x = -x * -x # label(non_clause) # label(goal).  [goal]. 2 x + (y + z) = (x + y) + z.  [assumption]. 3 (x + y) + z = x + (y + z).  [copy(2),flip(a)]. 4 x + 0 = x.  [assumption]. 5 0 + x = x.  [assumption]. 6 x + -x = 0.  [assumption]. 8 x + y = y + x.  [assumption]. 10 x * (y + z) = (x * y) + (x * z).  [assumption]. 11 (x + y) * z = (x * z) + (y * z).  [assumption]. 12 -c1 * -c1 != c1 * c1.  [deny(1)]. 13 x + (-x + y) = y.  [para(6(a,1),3(a,1,1)),rewrite([5(2)]),flip(a)]. 18 (x * 0) + (x * y) = x * y.  [para(5(a,1),10(a,1,2)),flip(a)]. 19 (x * y) + (x * -y) = x * 0.  [para(6(a,1),10(a,1,2)),flip(a)]. 24 --x = x.  [para(6(a,1),13(a,1,2)),rewrite([4(2)]),flip(a)]. 25 x + (y + -x) = y.  [para(8(a,1),13(a,1,2))]. 27 (x * y) + ((-x * y) + (z * y)) = z * y.  [para(13(a,1),11(a,1,1)),rewrite([11(5)]),flip(a)]. 33 -x + (y + x) = y.  [para(24(a,1),25(a,1,2,2))]. 40 x + -(x + y) = -y.  [para(33(a,1),33(a,1,2)),rewrite([8(3)])]. 57 -(x + y) = -y + -x.  [para(33(a,1),40(a,1,2,1)),flip(a)]. 69 x * 0 = 0.  [para(18(a,1),33(a,1,2)),rewrite([8(4),6(4)]),flip(a)]. 70 (x * y) + (x * -y) = 0.  [back_rewrite(19),rewrite([69(6)])]. 78 -(x * -y) = x * y.  [para(70(a,1),33(a,1,2)),rewrite([8(5),5(5)])]. 87 x * -y = -(x * y).  [para(78(a,1),24(a,1,1)),flip(a)]. 88 -(-c1 * c1) != c1 * c1.  [back_rewrite(12),rewrite([87(5)])]. 101 -(-x * y) = x * y.  [para(27(a,1),33(a,1,2)),rewrite([57(5),8(8),13(8)])]. 102 $F.  [resolve(101,a,88,a)].  ============================== end of proof ==========================",,"['abstract-algebra', 'ring-theory']"
72,Does there exist a non-abelian group with roots?,Does there exist a non-abelian group with roots?,,"I just learned the definition of a division group : An abelian group $G$ is called divisible if for every $x\in G$ and every $k\in\mathbb{Z}^+$, there exists $y\in G$ such that $y^k=x$ (or $ky=x$, in additive notation). So I was wondering if the property of ""existence of roots"" implies that a group is abelian. I believe this is not the case, but I couldn't find a counterexample. I had some ideas of using free groups and/or semidirect products (involving $\mathbb{Q}$, which I believe is the ""simplest"" division group), but I didn't get anywhere.","I just learned the definition of a division group : An abelian group $G$ is called divisible if for every $x\in G$ and every $k\in\mathbb{Z}^+$, there exists $y\in G$ such that $y^k=x$ (or $ky=x$, in additive notation). So I was wondering if the property of ""existence of roots"" implies that a group is abelian. I believe this is not the case, but I couldn't find a counterexample. I had some ideas of using free groups and/or semidirect products (involving $\mathbb{Q}$, which I believe is the ""simplest"" division group), but I didn't get anywhere.",,['abstract-algebra']
73,"$\mathbb{Q}[x,y]/\langle x^2+y^2-1 \rangle$ is an integral domain, and its field of fractions is isomorphic to $\mathbb Q(t)$ [closed]","is an integral domain, and its field of fractions is isomorphic to  [closed]","\mathbb{Q}[x,y]/\langle x^2+y^2-1 \rangle \mathbb Q(t)","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question How can I prove that $\mathbb{Q}[x,y]/\langle x^2+y^2-1 \rangle$ is an integral domain? Also, I need to prove that its field of fractions is isomorphic to the field of rational functions $\mathbb{Q}(t)$ ? (The question is taken from UC Berkeley Preliminary Exam, Fall 1995. )","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question How can I prove that is an integral domain? Also, I need to prove that its field of fractions is isomorphic to the field of rational functions ? (The question is taken from UC Berkeley Preliminary Exam, Fall 1995. )","\mathbb{Q}[x,y]/\langle x^2+y^2-1 \rangle \mathbb{Q}(t)","['abstract-algebra', 'ring-theory', 'commutative-algebra', 'irreducible-polynomials']"
74,Prove that the group isomorphism $\mathbb{Z}^m \cong \mathbb{Z}^n$ implies that $m = n$,Prove that the group isomorphism  implies that,\mathbb{Z}^m \cong \mathbb{Z}^n m = n,"Let $m$ and $n$ be two nonnegative integers. Assume that there is a group isomorphism $\mathbb{Z}^m \cong \mathbb{Z}^n$ . Prove that $m = n$ . I tried using a contrapositive, ( $m \neq n$ implies $\mathbb{Z}^m \ncong \mathbb{Z}^n$ ), and I think the problem is that there won't be a homomorphism, but did not get anywhere. Is there a better approach to this problem?","Let and be two nonnegative integers. Assume that there is a group isomorphism . Prove that . I tried using a contrapositive, ( implies ), and I think the problem is that there won't be a homomorphism, but did not get anywhere. Is there a better approach to this problem?",m n \mathbb{Z}^m \cong \mathbb{Z}^n m = n m \neq n \mathbb{Z}^m \ncong \mathbb{Z}^n,"['abstract-algebra', 'group-theory', 'abelian-groups', 'group-isomorphism', 'free-abelian-group']"
75,Galois Group over Finite Field,Galois Group over Finite Field,,"I am having a bit of difficulty trying to answer the following question: What is the Galois group of $X^8-1$ over $\mathbb{F}_{11}$? So far I have factored $X^8-1$ as $$X^8-1=(X+10)(X+1)(X^2+1)(X^4+1).$$ I know $X^2+1$ is irreducible over $\mathbb{F}_{11}$ since $10$ is not a square modulo $11$. Also, $X^4+1$ is irreducible over $\mathbb{F}_{11}$. The roots of $X^2+1$ and $X^4+1$ over $\mathbb{Q}$ are $\pm i$ and $\pm \frac{\sqrt{2}}{2} \pm \frac{\sqrt{2}}{2} i$, respectively. We also see that $\sqrt{2} \not \in \mathbb{F}_{11}$ since no element squared is equal to $2$. I would then think that $\mathbb{F}_{11}(i, \sqrt{2})$ is a splitting field for $x^8-1$ over $\mathbb{F}_{11}$, which is clearly Galois. If all this were true, I would then venture that the Galois group is $V_4$. I have the feeling, however, that I have made many mistakes in my reasoning. How should one approach a problem like this?","I am having a bit of difficulty trying to answer the following question: What is the Galois group of $X^8-1$ over $\mathbb{F}_{11}$? So far I have factored $X^8-1$ as $$X^8-1=(X+10)(X+1)(X^2+1)(X^4+1).$$ I know $X^2+1$ is irreducible over $\mathbb{F}_{11}$ since $10$ is not a square modulo $11$. Also, $X^4+1$ is irreducible over $\mathbb{F}_{11}$. The roots of $X^2+1$ and $X^4+1$ over $\mathbb{Q}$ are $\pm i$ and $\pm \frac{\sqrt{2}}{2} \pm \frac{\sqrt{2}}{2} i$, respectively. We also see that $\sqrt{2} \not \in \mathbb{F}_{11}$ since no element squared is equal to $2$. I would then think that $\mathbb{F}_{11}(i, \sqrt{2})$ is a splitting field for $x^8-1$ over $\mathbb{F}_{11}$, which is clearly Galois. If all this were true, I would then venture that the Galois group is $V_4$. I have the feeling, however, that I have made many mistakes in my reasoning. How should one approach a problem like this?",,"['abstract-algebra', 'galois-theory', 'finite-fields']"
76,Puiseux series over an algebraically closed field,Puiseux series over an algebraically closed field,,"Using the construction $R_n = K[t^\frac1n]$, $L_n = \text{Quot}(R_n)$ and $P = \bigcup_{n\in \mathbb{N}}L_N$ one automatically gets that the Puiseux series are a field. Nevertheless they are also an algebraically closed field if $K$ is an algebraically closed field. Why is that?","Using the construction $R_n = K[t^\frac1n]$, $L_n = \text{Quot}(R_n)$ and $P = \bigcup_{n\in \mathbb{N}}L_N$ one automatically gets that the Puiseux series are a field. Nevertheless they are also an algebraically closed field if $K$ is an algebraically closed field. Why is that?",,"['abstract-algebra', 'field-theory', 'power-series']"
77,Group abelianization,Group abelianization,,"I was wondering if someone could give me an intuitive interpretation of what we have done after abelianizing a group. I know what formal definition is: once we have our group $G$ given, we take a quotient by the commutator subgroup $[G,G]$, where $[G,G]$ is the unique smallest normal subgroup $N$ such that $G/N$ is abelian. But who guarantees that this is abelian? Also I was wondering why is this so helpful if in fact we get weaker (poorer) group than the first one.","I was wondering if someone could give me an intuitive interpretation of what we have done after abelianizing a group. I know what formal definition is: once we have our group $G$ given, we take a quotient by the commutator subgroup $[G,G]$, where $[G,G]$ is the unique smallest normal subgroup $N$ such that $G/N$ is abelian. But who guarantees that this is abelian? Also I was wondering why is this so helpful if in fact we get weaker (poorer) group than the first one.",,"['abstract-algebra', 'group-theory', 'intuition']"
78,Show that quotient rings are not isomorphic,Show that quotient rings are not isomorphic,,"I've been given a homework problem that requires me to show that the rings $\mathbb{C}[x,y]/(y - x^2)$ and $\mathbb{C}[x,y]/(xy-1)$ are not isomorphic. This is my attempt at a solution: For $\mathbb{C}[x,y]/(y - x^2)$, we can parametrize in the following way: $x = t$ and $y = t^2$. Then this ring is isomorphic to $\mathbb{C}[t]$. For $\mathbb{C}[x,y]/(xy-1)$, we can parametrize $x = t$ and $ y = 1/t$. Then this is isomorphic to $\mathbb{C}[t, 1/t]$. But $\mathbb{C}[t, 1/t]$ is not isomorphic to $\mathbb{C}[t]$. Am I on the right track? If not, any helpful hints ?","I've been given a homework problem that requires me to show that the rings $\mathbb{C}[x,y]/(y - x^2)$ and $\mathbb{C}[x,y]/(xy-1)$ are not isomorphic. This is my attempt at a solution: For $\mathbb{C}[x,y]/(y - x^2)$, we can parametrize in the following way: $x = t$ and $y = t^2$. Then this ring is isomorphic to $\mathbb{C}[t]$. For $\mathbb{C}[x,y]/(xy-1)$, we can parametrize $x = t$ and $ y = 1/t$. Then this is isomorphic to $\mathbb{C}[t, 1/t]$. But $\mathbb{C}[t, 1/t]$ is not isomorphic to $\mathbb{C}[t]$. Am I on the right track? If not, any helpful hints ?",,"['abstract-algebra', 'ring-theory']"
79,"Isomorphisms: preserve structure, operation, or order?","Isomorphisms: preserve structure, operation, or order?",,"Everyone always says that isomorphisms preserve structure... but given the (multiple) definitions of isomorphism, I fail to see how the definitions equate with the intuitive meaning, which is that two sets are ""basically the same if you ignore naming and notation"". Here are the different definitions I've come across: Order Isomorphism Let $A$ be a (totally) ordered set with ordering $\le$ and $B$ be a (totally) ordered set with ordering $\preceq$ . An isomorphism of $A$ onto $B$ is a bijection $f:A\mapsto B$ that satisfies $$a \le b \iff f(a) \preceq f(b)$$ for all $a,b \in A$ . Group Isomorphism Let $A$ be a group with operation $\ast$ and $B$ be a group with operation $\#$ . An isomorphism of $A$ onto $B$ is a bijection $f:A\mapsto B$ that satisfies $$a \ast b = c \iff f(a) \# f(b) = f(c)$$ rather, put simply, $$f(a\ast b) = f(a) \# f(b)$$ for all $a,b,c \in A$ . Field Isomorphism Let $A$ be a field with operations $\#$ and $\ast$ and $B$ be a field with operations $+$ and $\times$ . An isomorphism of $A$ onto $B$ is a bijection $f:A\mapsto B$ that satisfies $$f(a \# b) = f(a) + F(b)$$ and $$f(a \ast b) = f(a) \times F(b)$$ for all $a,b,c \in A$ . Homomorphism The same as an isomorphism but not necessarily a bijection. An example of why this is confusing: It is technically not the case that $\mathbb{N}\subseteq\mathbb{Z}$ . For example, in the Natural Numbers, $0$ is the empty set $\varnothing$ , which has no elements, however in the integers, $0$ is the equivalence class $[(0,0)]$ of ordered pairs whose components are natural numbers that are equal, and it has infinitely many elements. These sets are not by any means equal , but for all intents and purposes, we consider $\mathbb{N}$ to be a subset of $\mathbb{Z}$ , because the set of non-negative integers is ""basically the same as"" the set of natural numbers. That is, the natural numbers have ""isomorphic copies"" in the integers. The question is, which type of isomorphism? Which of these definitions of isomorphism is ""correct"", and how does it equate to the intuitive meaning that the two sets are ""basically the same""? Furthermore, what's the point of homomorphism, why is it useful, and how is its intuitive meaning similar or different from the intuitive meaning of isomorphism?","Everyone always says that isomorphisms preserve structure... but given the (multiple) definitions of isomorphism, I fail to see how the definitions equate with the intuitive meaning, which is that two sets are ""basically the same if you ignore naming and notation"". Here are the different definitions I've come across: Order Isomorphism Let be a (totally) ordered set with ordering and be a (totally) ordered set with ordering . An isomorphism of onto is a bijection that satisfies for all . Group Isomorphism Let be a group with operation and be a group with operation . An isomorphism of onto is a bijection that satisfies rather, put simply, for all . Field Isomorphism Let be a field with operations and and be a field with operations and . An isomorphism of onto is a bijection that satisfies and for all . Homomorphism The same as an isomorphism but not necessarily a bijection. An example of why this is confusing: It is technically not the case that . For example, in the Natural Numbers, is the empty set , which has no elements, however in the integers, is the equivalence class of ordered pairs whose components are natural numbers that are equal, and it has infinitely many elements. These sets are not by any means equal , but for all intents and purposes, we consider to be a subset of , because the set of non-negative integers is ""basically the same as"" the set of natural numbers. That is, the natural numbers have ""isomorphic copies"" in the integers. The question is, which type of isomorphism? Which of these definitions of isomorphism is ""correct"", and how does it equate to the intuitive meaning that the two sets are ""basically the same""? Furthermore, what's the point of homomorphism, why is it useful, and how is its intuitive meaning similar or different from the intuitive meaning of isomorphism?","A \le B \preceq A B f:A\mapsto B a \le b \iff f(a) \preceq f(b) a,b \in A A \ast B \# A B f:A\mapsto B a \ast b = c \iff f(a) \# f(b) = f(c) f(a\ast b) = f(a) \# f(b) a,b,c \in A A \# \ast B + \times A B f:A\mapsto B f(a \# b) = f(a) + F(b) f(a \ast b) = f(a) \times F(b) a,b,c \in A \mathbb{N}\subseteq\mathbb{Z} 0 \varnothing 0 [(0,0)] \mathbb{N} \mathbb{Z}","['abstract-algebra', 'group-theory', 'field-theory', 'order-theory']"
80,Can an element other than the neutral element be its own inverse?,Can an element other than the neutral element be its own inverse?,,"Take the following operation $*$ on the set $\{a, b\}$: $a * b = a$ $b * a = a$ $a * a = b$ $b * b = b$ $b$ is the neutral element. Can $a$ also be its own inverse, even though it's not the neutral element? Or does the inverse property require that only the neutral element may be its own inverse but all other elements must have another element be the inverse.","Take the following operation $*$ on the set $\{a, b\}$: $a * b = a$ $b * a = a$ $a * a = b$ $b * b = b$ $b$ is the neutral element. Can $a$ also be its own inverse, even though it's not the neutral element? Or does the inverse property require that only the neutral element may be its own inverse but all other elements must have another element be the inverse.",,['abstract-algebra']
81,"If D is an Integral Domain and has finite characteristic p, prove p is prime.","If D is an Integral Domain and has finite characteristic p, prove p is prime.",,"So the question is simply. If $D$ is an integral domain and has finite characteristic prove that the characteristic of $D$ is a prime number. This is my proof. Assume $p$ is the characteristic of $D$. Let $a$ be a non zero element of $D$. Seeking a contradiction assume $p$ is not prime.  Then $p$ can be written as a factor: $rs=p$ for some $r$ and some $s$. By definition $pa=0$, so $(rs)a=0$. We know that $r,s$ are non-zero, so by definition of integral domain the only way this equation can equal zero is if $a=0$ however this is a contradiction as we chose a to be a non-zero element of $D$. Therefore $p$ is a prime. Is this proof correct? The answer I have for this problem is slightly longer and I thought I might have missed something in my proof.","So the question is simply. If $D$ is an integral domain and has finite characteristic prove that the characteristic of $D$ is a prime number. This is my proof. Assume $p$ is the characteristic of $D$. Let $a$ be a non zero element of $D$. Seeking a contradiction assume $p$ is not prime.  Then $p$ can be written as a factor: $rs=p$ for some $r$ and some $s$. By definition $pa=0$, so $(rs)a=0$. We know that $r,s$ are non-zero, so by definition of integral domain the only way this equation can equal zero is if $a=0$ however this is a contradiction as we chose a to be a non-zero element of $D$. Therefore $p$ is a prime. Is this proof correct? The answer I have for this problem is slightly longer and I thought I might have missed something in my proof.",,"['abstract-algebra', 'proof-verification', 'integral-domain']"
82,What is the difference between a Subgroup and a subset?,What is the difference between a Subgroup and a subset?,,"What is the difference between a Subgroup and a subset? I know hardly any Abstract algebra, just some things from youtube and wikipedia, but the notion of a subgroup being part of a larger group and a set being part of some group is indistinguishable to me. A nice simple answer would do fine. Thank you for your time","What is the difference between a Subgroup and a subset? I know hardly any Abstract algebra, just some things from youtube and wikipedia, but the notion of a subgroup being part of a larger group and a set being part of some group is indistinguishable to me. A nice simple answer would do fine. Thank you for your time",,"['abstract-algebra', 'group-theory', 'elementary-set-theory', 'terminology']"
83,Commutative rings without assuming identity,Commutative rings without assuming identity,,"I was going through Exercises in Dummit&Foote, which does not assume identity in the definition of a ring, and reached the following exercise: Prove that in a Boolean ring ($a^2 = a$ for all $a$) every prime ideal is a maximal ideal. This was relatively easy assuming identity, but isn't so clear without identity.  My questions,then, are: Can this be solved without assuming identity? The proof that maximal ideals are prime requires identity.  It states that if $R$ is a commutative ring with identity and $I$ is a maximal ideal, then $R/I$ is a field, and thus an integral domain.  Therefore, $I$ is a prime ideal.  Does something similar apply to commutative rings without identity?  Are maximal ideals necessarily prime in such rings?  If so, what is an example of such a ring?","I was going through Exercises in Dummit&Foote, which does not assume identity in the definition of a ring, and reached the following exercise: Prove that in a Boolean ring ($a^2 = a$ for all $a$) every prime ideal is a maximal ideal. This was relatively easy assuming identity, but isn't so clear without identity.  My questions,then, are: Can this be solved without assuming identity? The proof that maximal ideals are prime requires identity.  It states that if $R$ is a commutative ring with identity and $I$ is a maximal ideal, then $R/I$ is a field, and thus an integral domain.  Therefore, $I$ is a prime ideal.  Does something similar apply to commutative rings without identity?  Are maximal ideals necessarily prime in such rings?  If so, what is an example of such a ring?",,"['abstract-algebra', 'ring-theory', 'commutative-algebra', 'rngs']"
84,Intuition regarding Chevalley-Warning Theorem,Intuition regarding Chevalley-Warning Theorem,,Three versions of the theorem are stated on pages 1-2 in these notes by Pete L. Clark: http://alpha.math.uga.edu/~pete/4400ChevalleyWarning.pdf Could anyone offer some intuitive way to think about this theorem? Thanks!,Three versions of the theorem are stated on pages 1-2 in these notes by Pete L. Clark: http://alpha.math.uga.edu/~pete/4400ChevalleyWarning.pdf Could anyone offer some intuitive way to think about this theorem? Thanks!,,"['abstract-algebra', 'polynomials', 'algebraic-number-theory', 'intuition', 'finite-fields']"
85,Normal Subgroups that Intersect Trivially,Normal Subgroups that Intersect Trivially,,"Let $H$ and $K$ be normal subgroups of a group $G$ such that they intersect trivially. Why is it then the case that $$hk=kh;\;\;\;\; \forall h\in H,\;\forall k\in K?$$","Let $H$ and $K$ be normal subgroups of a group $G$ such that they intersect trivially. Why is it then the case that $$hk=kh;\;\;\;\; \forall h\in H,\;\forall k\in K?$$",,"['abstract-algebra', 'group-theory', 'normal-distribution']"
86,The Degree of Zero Polynomial.,The Degree of Zero Polynomial.,,"I wonder why the degree of the zero polynomial is $-\infty$ ? I heard that, it is $-\infty$ to make the formula $\deg(fg)=\deg(f)+\deg(g)$ hold when one of these polynomials is zero. However, if that was the only reason we could have said that it is $\infty$ instead.","I wonder why the degree of the zero polynomial is $-\infty$ ? I heard that, it is $-\infty$ to make the formula $\deg(fg)=\deg(f)+\deg(g)$ hold when one of these polynomials is zero. However, if that was the only reason we could have said that it is $\infty$ instead.",,"['abstract-algebra', 'polynomials', 'definition', 'intuition', 'convention']"
87,Why does the ideal $(a+bi)$ have index $a^2+b^2$ in $\mathbb{Z}[i]$? [duplicate],Why does the ideal  have index  in ? [duplicate],(a+bi) a^2+b^2 \mathbb{Z}[i],"This question already has answers here : Quotient ring of Gaussian integers (7 answers) Order of some quotient ring of Gaussian integers [duplicate] (3 answers) Closed 11 years ago . In the comments on the question Why does this module structure have $352512$ elements? , it is mentioned that the index of the ideal generated by $a+bi$ in $\mathbb{Z}[i]$ has order $a^2+b^2$. Is there a nice rigorous explanation of why this is so?","This question already has answers here : Quotient ring of Gaussian integers (7 answers) Order of some quotient ring of Gaussian integers [duplicate] (3 answers) Closed 11 years ago . In the comments on the question Why does this module structure have $352512$ elements? , it is mentioned that the index of the ideal generated by $a+bi$ in $\mathbb{Z}[i]$ has order $a^2+b^2$. Is there a nice rigorous explanation of why this is so?",,"['abstract-algebra', 'modules']"
88,Is it true that an element of a group whose order divides the order a subgroup is an element of the subgroup,Is it true that an element of a group whose order divides the order a subgroup is an element of the subgroup,,"Let $G$ be a group. Suppose that the order of $G$ is finite and that $H$ is a subgroup of $G$. Is it true that an element of $G$ whose order divides the order of $H$ is in $H$? Here is my attempt: Let $|G|=n$ and $|H|=m$. Then $m|n$. Moreover, the order of every subgroup of $G$ divides the order of $G$ and if $g \in G$, then$|g|=|\langle g \rangle|$. Since in itself $H$ is a group (under the same operation in $G$), then the order of its element divides $|H|=m$. Thus, if $|g||m$ then $\langle g\rangle\leq H$ which means that $g \in H$. Can you help me out with this one? I just can find a way put $g$ in $H$.","Let $G$ be a group. Suppose that the order of $G$ is finite and that $H$ is a subgroup of $G$. Is it true that an element of $G$ whose order divides the order of $H$ is in $H$? Here is my attempt: Let $|G|=n$ and $|H|=m$. Then $m|n$. Moreover, the order of every subgroup of $G$ divides the order of $G$ and if $g \in G$, then$|g|=|\langle g \rangle|$. Since in itself $H$ is a group (under the same operation in $G$), then the order of its element divides $|H|=m$. Thus, if $|g||m$ then $\langle g\rangle\leq H$ which means that $g \in H$. Can you help me out with this one? I just can find a way put $g$ in $H$.",,"['abstract-algebra', 'group-theory']"
89,Spectrum of a product of rings isomorphic to the product of the spectra,Spectrum of a product of rings isomorphic to the product of the spectra,,I've found in an exercise this statement: If $A$ is a commutative ring with unit and $A = A_{1} \times \dots \times A_{n}$ then $$\def\Spec{\operatorname{Spec}} \Spec(A) \cong  \Spec(A_{1})\times \dotsb \times \Spec(A_{n})$$ Why is this true ?,I've found in an exercise this statement: If $A$ is a commutative ring with unit and $A = A_{1} \times \dots \times A_{n}$ then $$\def\Spec{\operatorname{Spec}} \Spec(A) \cong  \Spec(A_{1})\times \dotsb \times \Spec(A_{n})$$ Why is this true ?,,"['abstract-algebra', 'ring-theory', 'commutative-algebra', 'ideals']"
90,$AB \neq 0$ but $BA=0$,but,AB \neq 0 BA=0,"Do there exists to matrices or objects such that $AB \neq 0$ but $BA=0$? Another way to ask this question is if there exists objects or matrices $A$ and $B$ such that... $[A,B]=AB$ where $[ \, , \, ]$ is the commutator $[A,B]=AB-BA.$ If such matrices do not exist, what does that imply about the algebra that the elements are in?","Do there exists to matrices or objects such that $AB \neq 0$ but $BA=0$? Another way to ask this question is if there exists objects or matrices $A$ and $B$ such that... $[A,B]=AB$ where $[ \, , \, ]$ is the commutator $[A,B]=AB-BA.$ If such matrices do not exist, what does that imply about the algebra that the elements are in?",,"['abstract-algebra', 'matrices', 'noncommutative-algebra']"
91,Quotient of free module,Quotient of free module,,Let $R$ be a commutative ring with $1$ and let $J$ be a proper ideal of $R$ such that $R/J \cong R^n$ as $R$-modules where $n$ is some natural number. Does this imply that $J$ is the trivial ideal? Basically I am trying to prove/disprove that if $J$ is a proper ideal of $R$ and $R/J$ is free then $J=0$ and above is my work.,Let $R$ be a commutative ring with $1$ and let $J$ be a proper ideal of $R$ such that $R/J \cong R^n$ as $R$-modules where $n$ is some natural number. Does this imply that $J$ is the trivial ideal? Basically I am trying to prove/disprove that if $J$ is a proper ideal of $R$ and $R/J$ is free then $J=0$ and above is my work.,,"['abstract-algebra', 'modules', 'ideals']"
92,"Cardinality of the quotient ring $\mathbb{Z}[x]/(x^2-3,2x+4)$ [duplicate]",Cardinality of the quotient ring  [duplicate],"\mathbb{Z}[x]/(x^2-3,2x+4)","This question already has answers here : Number of elements in the quotient ring $\mathbb{Z}[X]/(X^2-3, 2X+4)$ (2 answers) Closed 9 years ago . This problem is from a practice exam I was working on. What is the cardinality of the quotient $\mathbb{Z}[x]/(x^2-3,2x+4)$ ? Thoughts . If I find a ring that is easier to handle then this then I can go from there. So I think this is isomorphic to $\mathbb{Z}_2[x]/(x^2-3,x+2)$. And then I am stuck. Am I correct so far? What should I be looking to do from here? Thanks for your time and your answers.","This question already has answers here : Number of elements in the quotient ring $\mathbb{Z}[X]/(X^2-3, 2X+4)$ (2 answers) Closed 9 years ago . This problem is from a practice exam I was working on. What is the cardinality of the quotient $\mathbb{Z}[x]/(x^2-3,2x+4)$ ? Thoughts . If I find a ring that is easier to handle then this then I can go from there. So I think this is isomorphic to $\mathbb{Z}_2[x]/(x^2-3,x+2)$. And then I am stuck. Am I correct so far? What should I be looking to do from here? Thanks for your time and your answers.",,"['abstract-algebra', 'ring-theory', 'ideals']"
93,"If $[G:H]=n$, is it true that $x^n\in H$ for all $x\in G$?","If , is it true that  for all ?",[G:H]=n x^n\in H x\in G,"Let $G$ be a group and $H$ a subgroup with $[G:H]=n$. Is it true that $x^n\in H$ for all $x\in G$? Remarks. The answer is positive whenever $H$ is normal, e.g., for $n=2$. In general, by using the normal core of $H$, one can find an $m\ge 1$ such that $x^m\in H$ for all $x\in G$.","Let $G$ be a group and $H$ a subgroup with $[G:H]=n$. Is it true that $x^n\in H$ for all $x\in G$? Remarks. The answer is positive whenever $H$ is normal, e.g., for $n=2$. In general, by using the normal core of $H$, one can find an $m\ge 1$ such that $x^m\in H$ for all $x\in G$.",,['abstract-algebra']
94,"Is it possible that $(ab)^{-1}$ is defined although $a^{-1},b^{-1}$ are not?",Is it possible that  is defined although  are not?,"(ab)^{-1} a^{-1},b^{-1}","I wish to enquire about the properties of units in abstract algebra. In a ring $R$, a unit $u$ is an invertible element. Let $u=ab$. Is it possible that $a$ and $b$ are not units? Is it possible that they're prime? Motivation: If $u=ab$, then $(ab)^{-1}$ exists. We know that if the inverses of $a$ and $b$ exist, then $(ab)^{-1}=b^{-1}a^{-1}$. However, if the inverses of $a$ and $b$ don't exist, I feel $ab$ can still be a unit. However, I'm not sure of this. Thanks in advance!","I wish to enquire about the properties of units in abstract algebra. In a ring $R$, a unit $u$ is an invertible element. Let $u=ab$. Is it possible that $a$ and $b$ are not units? Is it possible that they're prime? Motivation: If $u=ab$, then $(ab)^{-1}$ exists. We know that if the inverses of $a$ and $b$ exist, then $(ab)^{-1}=b^{-1}a^{-1}$. However, if the inverses of $a$ and $b$ don't exist, I feel $ab$ can still be a unit. However, I'm not sure of this. Thanks in advance!",,"['abstract-algebra', 'ring-theory']"
95,Proving that a group homomorphism preserves the identity element,Proving that a group homomorphism preserves the identity element,,"Assume that $(G,*)$ and $(H,o)$ are groups and that $f:(G,*) \rightarrow (H,o)$ is a homomorphism. Let $e_G$ and $e_H$ denote the identity elements of $G$ and $H$, respectively. Show that $f(e_G)=e_H$. Approach: $f(e_G)=f(a*a^{-1})$ for $a,a^{-1} \in G$, so $f(a*a^{-1})=f(a)of(a^{-1})$. If that’s true, then how do we know that  $f(a^{-1})$ is the inverse of $f(a)$?","Assume that $(G,*)$ and $(H,o)$ are groups and that $f:(G,*) \rightarrow (H,o)$ is a homomorphism. Let $e_G$ and $e_H$ denote the identity elements of $G$ and $H$, respectively. Show that $f(e_G)=e_H$. Approach: $f(e_G)=f(a*a^{-1})$ for $a,a^{-1} \in G$, so $f(a*a^{-1})=f(a)of(a^{-1})$. If that’s true, then how do we know that  $f(a^{-1})$ is the inverse of $f(a)$?",,"['abstract-algebra', 'group-theory']"
96,$(\mathbb{Z}/2^n \mathbb{Z})^*$ is not a cyclic group for $n\geq 3$,is not a cyclic group for,(\mathbb{Z}/2^n \mathbb{Z})^* n\geq 3,"Question is to prove that $(\mathbb{Z}/2^n \mathbb{Z})^*$ is not a  cyclic group for $n\geq 3$ . Hint : Find two subgroups of order $2$ . I somehow feel that a cyclic group can not have two distinct groups of same order. but, I am not sure about the proof. I have no idea how to proceed for this. any hint would be appreciated. Thank You.","Question is to prove that is not a  cyclic group for . Hint : Find two subgroups of order . I somehow feel that a cyclic group can not have two distinct groups of same order. but, I am not sure about the proof. I have no idea how to proceed for this. any hint would be appreciated. Thank You.",(\mathbb{Z}/2^n \mathbb{Z})^* n\geq 3 2,['abstract-algebra']
97,Irreducibility of Multivariable Polynomials,Irreducibility of Multivariable Polynomials,,"Question: Let $k$ be a field and $p,q\in k[x]$ two relatively prime polynomials. Prove $p(x)y-q(x)$ is irreducible in $k[x,y]$. How does one show this? More generally, how does one show that multivariable polynomials are irreducible? In one variable we have access to tools like Gauss's lemma and Eisenstein's criteria, but I do not know any methods applicable to the multivariable case.","Question: Let $k$ be a field and $p,q\in k[x]$ two relatively prime polynomials. Prove $p(x)y-q(x)$ is irreducible in $k[x,y]$. How does one show this? More generally, how does one show that multivariable polynomials are irreducible? In one variable we have access to tools like Gauss's lemma and Eisenstein's criteria, but I do not know any methods applicable to the multivariable case.",,"['abstract-algebra', 'polynomials', 'irreducible-polynomials']"
98,"Show that, for every $n$, $A_{n+2}$ has a subgroup isomorphic to $S_n$","Show that, for every ,  has a subgroup isomorphic to",n A_{n+2} S_n,"Show that, for every $n$, $A_{n+2}$ has a subgroup isomorphic to $S_n$ Also, in general, when I construct an isomorphism, what's necessary to show that it's well-defined? Is showing it is a bijection and homomorphism enough? And are there any rules to follow when I try to construct a homomophism or isomorphism? I mean when I'm asked to show a certain group is isomorphic to another, it's always difficult for me to find the mapping.","Show that, for every $n$, $A_{n+2}$ has a subgroup isomorphic to $S_n$ Also, in general, when I construct an isomorphism, what's necessary to show that it's well-defined? Is showing it is a bijection and homomorphism enough? And are there any rules to follow when I try to construct a homomophism or isomorphism? I mean when I'm asked to show a certain group is isomorphic to another, it's always difficult for me to find the mapping.",,"['abstract-algebra', 'group-theory']"
99,Automorphism on integers,Automorphism on integers,,Is multiplying by a constant m (integer) on group of set of all integers on addition an automorphism? If so why does the 2nd example in http://en.wikipedia.org/wiki/Automorphism says that the unique non trivial automorphism is negation?,Is multiplying by a constant m (integer) on group of set of all integers on addition an automorphism? If so why does the 2nd example in http://en.wikipedia.org/wiki/Automorphism says that the unique non trivial automorphism is negation?,,"['abstract-algebra', 'group-theory']"
