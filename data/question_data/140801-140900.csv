,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Finding $\lim_{n \to \infty} n(\sqrt[n]{2} - 1)$,Finding,\lim_{n \to \infty} n(\sqrt[n]{2} - 1),I am trying to find the limit of: $$\lim_{n \to \infty} n(\sqrt[n]{2} - 1)$$ I know it should be very simple but I don't seem to get it. Thanks in advance for any help!,I am trying to find the limit of: $$\lim_{n \to \infty} n(\sqrt[n]{2} - 1)$$ I know it should be very simple but I don't seem to get it. Thanks in advance for any help!,,"['limits', 'radicals']"
1,Estimation: An integral from MIT Integration bee 2022 (QF),Estimation: An integral from MIT Integration bee 2022 (QF),,"We need to determine the value of: $$\lim_{n\to\infty}\sqrt n\cdot \int_{-1/2}^{1/2} (1-3x^2+x^4)^n\mathrm{d}x$$ Since a limit has been given, one suspects that, as usual, the integral is not easy to evaluate in general and some estimates have to be made. Some promising starts (and observations) include: The polynomial always lies between $5/16$ and $1$ in the given range. Perhaps we can ignore some higher order terms (even $x$ has modulus less than one)? But the problem is that exponent $n$ . Reduction formula and integration by parts can give a recursion. But all reasonable ways of splitting the integrand seem to complicate matters rather than simplify them. We can now take some hints from the limits. Perhaps some trigonometric substitution can aid. A reasonable one is $x^2=\frac 14\sin \theta$ or $x=\frac 12 \cos\theta$ but there is not much one can do due to the quadratic polynomial. Factorize the polynomial as $x^2-3x+1$ has all real roots? But I still get stuck. But I am unable to make any of these work. Does anyone have any suggestions?","We need to determine the value of: Since a limit has been given, one suspects that, as usual, the integral is not easy to evaluate in general and some estimates have to be made. Some promising starts (and observations) include: The polynomial always lies between and in the given range. Perhaps we can ignore some higher order terms (even has modulus less than one)? But the problem is that exponent . Reduction formula and integration by parts can give a recursion. But all reasonable ways of splitting the integrand seem to complicate matters rather than simplify them. We can now take some hints from the limits. Perhaps some trigonometric substitution can aid. A reasonable one is or but there is not much one can do due to the quadratic polynomial. Factorize the polynomial as has all real roots? But I still get stuck. But I am unable to make any of these work. Does anyone have any suggestions?",\lim_{n\to\infty}\sqrt n\cdot \int_{-1/2}^{1/2} (1-3x^2+x^4)^n\mathrm{d}x 5/16 1 x n x^2=\frac 14\sin \theta x=\frac 12 \cos\theta x^2-3x+1,"['limits', 'definite-integrals']"
2,Proof of power rule for limits?,Proof of power rule for limits?,,"I'm working through Spivak's Calculus book which proved the following: $$\lim_{x \to a}\ (f+g)(x) = \lim_{x \to a}\ f(x) + \lim_{x \to a}\ g(x)$$ $$\lim_{x \to a} \ (f \cdot g)(x) = \lim_{x \to a}\ f(x) \cdot  \lim_{x \to a}\ g(x)$$ $$\lim_{x \to a} \ \Bigg( \frac{1}{g} \Bigg) (x) = \frac{1}{\lim \limits_{x \to a} \ g(x)}$$ However, the proof that $$\lim_{x \to a}[f(x)^\alpha] = \left[\lim_{x \to a}f(x) \right]^\alpha$$ where $\alpha$ is a real number is missing. It's easy to prove from the above properties when $\alpha$ is an integer, but what about otherwise? I've looked online and only found proofs when it is an integer.","I'm working through Spivak's Calculus book which proved the following: $$\lim_{x \to a}\ (f+g)(x) = \lim_{x \to a}\ f(x) + \lim_{x \to a}\ g(x)$$ $$\lim_{x \to a} \ (f \cdot g)(x) = \lim_{x \to a}\ f(x) \cdot  \lim_{x \to a}\ g(x)$$ $$\lim_{x \to a} \ \Bigg( \frac{1}{g} \Bigg) (x) = \frac{1}{\lim \limits_{x \to a} \ g(x)}$$ However, the proof that $$\lim_{x \to a}[f(x)^\alpha] = \left[\lim_{x \to a}f(x) \right]^\alpha$$ where $\alpha$ is a real number is missing. It's easy to prove from the above properties when $\alpha$ is an integer, but what about otherwise? I've looked online and only found proofs when it is an integer.",,['limits']
3,Calculate $\lim_{n\to \infty} \frac{\frac{2}{1}+\frac{3^2}{2}+\frac{4^3}{3^2}+...+\frac{(n+1)^n}{n^{n-1}}}{n^2}$,Calculate,\lim_{n\to \infty} \frac{\frac{2}{1}+\frac{3^2}{2}+\frac{4^3}{3^2}+...+\frac{(n+1)^n}{n^{n-1}}}{n^2},"Calculate $$\lim_{n\to \infty} \frac{\displaystyle \frac{2}{1}+\frac{3^2}{2}+\frac{4^3}{3^2}+...+\frac{(n+1)^n}{n^{n-1}}}{n^2}$$ I have messed around with this task for quite a while now, but I haven't succeeded to find the solution yet. Help is appreciated!","Calculate $$\lim_{n\to \infty} \frac{\displaystyle \frac{2}{1}+\frac{3^2}{2}+\frac{4^3}{3^2}+...+\frac{(n+1)^n}{n^{n-1}}}{n^2}$$ I have messed around with this task for quite a while now, but I haven't succeeded to find the solution yet. Help is appreciated!",,['limits']
4,Limit of a product is the product of the limits - when?,Limit of a product is the product of the limits - when?,,"The limit of the product of two functions should be equal to the product of the limits: $$\lim_{x\to\infty}f(x)g(x) = \lim_{x\to\infty}f(x) \lim_{x\to\infty}g(x)$$ Now, the limit of $\frac{(x-1)3}{4x}$ = $\frac{3}{4}$ But the limit of $\frac{x-1}{4}$ = $\infty$ and the limit of $\frac{3}{x}$ = 0 How is this? Thanks!","The limit of the product of two functions should be equal to the product of the limits: $$\lim_{x\to\infty}f(x)g(x) = \lim_{x\to\infty}f(x) \lim_{x\to\infty}g(x)$$ Now, the limit of $\frac{(x-1)3}{4x}$ = $\frac{3}{4}$ But the limit of $\frac{x-1}{4}$ = $\infty$ and the limit of $\frac{3}{x}$ = 0 How is this? Thanks!",,['limits']
5,How to prove the following limit: $\lim_{n \to +\infty} 4^n\left[\sum_{k=0}^n (-1)^k{n\choose k}\ln (n+k)\right]=0$?,How to prove the following limit: ?,\lim_{n \to +\infty} 4^n\left[\sum_{k=0}^n (-1)^k{n\choose k}\ln (n+k)\right]=0,How to prove the following limit: $$\lim_{n \to +\infty} 4^n\left[\sum_{k=0}^n (-1)^k{n\choose k}\ln (n+k)\right]=0?$$,How to prove the following limit: $$\lim_{n \to +\infty} 4^n\left[\sum_{k=0}^n (-1)^k{n\choose k}\ln (n+k)\right]=0?$$,,"['analysis', 'limits', 'binomial-coefficients']"
6,Length of Interval,Length of Interval,,"I'm currently going through Terence Tao's book on measure theory and I'm having a little trouble with one of his exercises.  He tells us to observe that: $|I| = \lim_{n \to \infty} \frac{1}{N}\#\left(I \cap\frac{\mathbf{Z}}{N}\right)$ where $|I|$ is the length of any real interval, $\#|A|$ is the cardinality of a finite set, $N$ is an integer, and $\frac{\mathbf{Z}}{N}=\{\frac{n}{N}: n \in \mathbf{Z}\}$ I can convince myself that it's true for easy intervals like $(2,3)$.  But I'm having trouble proving it for the general case, or for ""hard"" intervals like $(2.113,2.114)$.  Does anyone have any tips to get started?","I'm currently going through Terence Tao's book on measure theory and I'm having a little trouble with one of his exercises.  He tells us to observe that: $|I| = \lim_{n \to \infty} \frac{1}{N}\#\left(I \cap\frac{\mathbf{Z}}{N}\right)$ where $|I|$ is the length of any real interval, $\#|A|$ is the cardinality of a finite set, $N$ is an integer, and $\frac{\mathbf{Z}}{N}=\{\frac{n}{N}: n \in \mathbf{Z}\}$ I can convince myself that it's true for easy intervals like $(2,3)$.  But I'm having trouble proving it for the general case, or for ""hard"" intervals like $(2.113,2.114)$.  Does anyone have any tips to get started?",,"['measure-theory', 'limits']"
7,l'Hopital rule on $\mathbb{C}$ [duplicate],l'Hopital rule on  [duplicate],\mathbb{C},This question already has answers here : Is L'Hopitals rule applicable to complex functions? (2 answers) Closed 8 years ago . Is the L'Hopital's rule true when using limits on $\mathbb{C}$ (the complex field)? I don't know if it is only valid using the real numbers $\mathbb{R}$... is that the case?,This question already has answers here : Is L'Hopitals rule applicable to complex functions? (2 answers) Closed 8 years ago . Is the L'Hopital's rule true when using limits on $\mathbb{C}$ (the complex field)? I don't know if it is only valid using the real numbers $\mathbb{R}$... is that the case?,,['limits']
8,Proof of the Central Limit Theorem using moment generating functions,Proof of the Central Limit Theorem using moment generating functions,,"Below is a method of proving the Central Limit Theorem using moment generating functions. Let $$X_{1},X_{2},...,X_{n}$$ be a sequence of i.i.d. random variables with expected value and variance $$E(X_{i}) = \mu < \infty,  Var(X_{i})=\sigma ^{2}< \infty.$$ Now let $$Z_{n}=\frac{\overline{X}-\mu }{\frac{\sigma }{\sqrt{n}}} = \frac{X_{1}+X_{2}+...+X_{n}-n\mu }{\sigma \sqrt{n}}.$$ We want to show that $$\lim_{n \to \infty} M_{Z_{n}}(t)=e^{\frac{t^{2}}{2}}$$ where $M_{X}(t)$ is the moment generating function over some finite interval.  In order to prove this, we can define a new random variable, $Y_{i}$, which is the normalized version of $X_{i}$.  Thus, $$Y_{i}=\frac{X_{i}-\mu }{\sigma }.$$ Then, we can say that $Y_{i}$ is i.i.d. with expected value and variance $$E(X_{i}) =0,  Var(X_{i})=1.$$ Using this information, we have $$Z_{n}=\frac{\overline{Y}-\mu }{\frac{\sigma }{\sqrt{n}}} = \frac{Y_{1}+Y_{2}+...+Y_{n} }{\sqrt{n}}.$$ Finding the moment generating function gives $$M_{Z_{n}}(t)=E[e^{t\frac{Y_{1}+Y_{2}+...+Y_{n} }{\sqrt{n}}}] =E[e^{t\frac{Y_{1}}{\sqrt{n}}}]\cdot E[e^{t\frac{Y_{2}}{\sqrt{n}}}]\cdot ...\cdot E[e^{t\frac{Y_{n}}{\sqrt{n}}}]= M_{Y_{1}}(\frac{t}{\sqrt{n}})^{n}.$$ Lastly, $$\lim_{n \to \infty} M_{Z_{n}}(t)=\lim_{n \to \infty} M_{Y_{1}}(\frac{t}{\sqrt{n}})^{n} = e^{\frac{t^{2}}{2}}.$$ This concludes the proof.  However, how does one show analytically that this final limit does indeed equal $$e^{\frac{t^{2}}{2}}?$$","Below is a method of proving the Central Limit Theorem using moment generating functions. Let $$X_{1},X_{2},...,X_{n}$$ be a sequence of i.i.d. random variables with expected value and variance $$E(X_{i}) = \mu < \infty,  Var(X_{i})=\sigma ^{2}< \infty.$$ Now let $$Z_{n}=\frac{\overline{X}-\mu }{\frac{\sigma }{\sqrt{n}}} = \frac{X_{1}+X_{2}+...+X_{n}-n\mu }{\sigma \sqrt{n}}.$$ We want to show that $$\lim_{n \to \infty} M_{Z_{n}}(t)=e^{\frac{t^{2}}{2}}$$ where $M_{X}(t)$ is the moment generating function over some finite interval.  In order to prove this, we can define a new random variable, $Y_{i}$, which is the normalized version of $X_{i}$.  Thus, $$Y_{i}=\frac{X_{i}-\mu }{\sigma }.$$ Then, we can say that $Y_{i}$ is i.i.d. with expected value and variance $$E(X_{i}) =0,  Var(X_{i})=1.$$ Using this information, we have $$Z_{n}=\frac{\overline{Y}-\mu }{\frac{\sigma }{\sqrt{n}}} = \frac{Y_{1}+Y_{2}+...+Y_{n} }{\sqrt{n}}.$$ Finding the moment generating function gives $$M_{Z_{n}}(t)=E[e^{t\frac{Y_{1}+Y_{2}+...+Y_{n} }{\sqrt{n}}}] =E[e^{t\frac{Y_{1}}{\sqrt{n}}}]\cdot E[e^{t\frac{Y_{2}}{\sqrt{n}}}]\cdot ...\cdot E[e^{t\frac{Y_{n}}{\sqrt{n}}}]= M_{Y_{1}}(\frac{t}{\sqrt{n}})^{n}.$$ Lastly, $$\lim_{n \to \infty} M_{Z_{n}}(t)=\lim_{n \to \infty} M_{Y_{1}}(\frac{t}{\sqrt{n}})^{n} = e^{\frac{t^{2}}{2}}.$$ This concludes the proof.  However, how does one show analytically that this final limit does indeed equal $$e^{\frac{t^{2}}{2}}?$$",,"['limits', 'statistics']"
9,How find this limit $\lim \limits_{x\to+\infty}e^{-x}\left(1+\frac{1}{x}\right)^{x^2}$ [duplicate],How find this limit  [duplicate],\lim \limits_{x\to+\infty}e^{-x}\left(1+\frac{1}{x}\right)^{x^2},This question already has answers here : finding the limit $\lim\limits_{x \to \infty }(\frac{1}{e}(1+\frac{1}{x})^x)^x$ (2 answers) Closed 6 years ago . find this limit $$\lim \limits_{x\to+\infty}e^{-x}\left(1+\dfrac{1}{x}\right)^{x^2}$$ my idea: $$\lim \limits_{x\to+\infty}e^{-x}\left(1+\dfrac{1}{x}\right)^{x^2}=\lim \limits_{x\to+\infty}e^{-x}\cdot e^x=1$$ But book is answer is not 1? and How about find it? Thank you,This question already has answers here : finding the limit $\lim\limits_{x \to \infty }(\frac{1}{e}(1+\frac{1}{x})^x)^x$ (2 answers) Closed 6 years ago . find this limit $$\lim \limits_{x\to+\infty}e^{-x}\left(1+\dfrac{1}{x}\right)^{x^2}$$ my idea: $$\lim \limits_{x\to+\infty}e^{-x}\left(1+\dfrac{1}{x}\right)^{x^2}=\lim \limits_{x\to+\infty}e^{-x}\cdot e^x=1$$ But book is answer is not 1? and How about find it? Thank you,,['limits']
10,Difficult limit question involving Euler's number and L'Hospital,Difficult limit question involving Euler's number and L'Hospital,,"The limit I'm trying to evaluate is $$ \lim_{x\to+\infty} e^x \left[e - \left(1 + \frac{1}{x}\right)^x\right] $$ After some hours trying, I've made almost no progress. I always end up in some indeterminate form and L'Hospital isn't getting me anywhere (though I think that, if used properly, it might solve the problem). Maybe it's not even that difficult and I'm just stuck for some stupid reason. Any ideas?","The limit I'm trying to evaluate is After some hours trying, I've made almost no progress. I always end up in some indeterminate form and L'Hospital isn't getting me anywhere (though I think that, if used properly, it might solve the problem). Maybe it's not even that difficult and I'm just stuck for some stupid reason. Any ideas?","
\lim_{x\to+\infty} e^x \left[e - \left(1 + \frac{1}{x}\right)^x\right]
",['limits']
11,Why would $1^{-\infty}$ not be 1?,Why would  not be 1?,1^{-\infty},"It would seem to me that $1^{-∞}=\lim_\limits{x→∞}1^{-x}=\lim_\limits{x→∞}\frac1{1^x}=\frac11=1$ no matter how we approach it. However, Wolfram Alpha answers with a mysteriously unqualified “ $\text{(undefined)}$ ”. Similarly, JavaScript also thinks that the result isn't a number. On the other hand, the very mathematically inclined APL languages NARS2000 and J both have it give $1$ . What reasons are there to reject $1^{-∞}=1$ ?","It would seem to me that no matter how we approach it. However, Wolfram Alpha answers with a mysteriously unqualified “ ”. Similarly, JavaScript also thinks that the result isn't a number. On the other hand, the very mathematically inclined APL languages NARS2000 and J both have it give . What reasons are there to reject ?",1^{-∞}=\lim_\limits{x→∞}1^{-x}=\lim_\limits{x→∞}\frac1{1^x}=\frac11=1 \text{(undefined)} 1 1^{-∞}=1,"['limits', 'power-series', 'infinite-product', 'mathematica']"
12,Does $\lim_{x\to 0^+} \sqrt{x}$ exist or not?,Does  exist or not?,\lim_{x\to 0^+} \sqrt{x},"Does this right-handed limit exist or not? $$\lim_{x\to 0^+} \sqrt{x}$$ Schaum's Easy Outline of Calculus (Second Edition) says it does. And doesn't. An example in the book states: The function $f(x)=\sqrt{x}$; then $f$ is defined only to the right of zero. Okay. So the right-handed limit does exist. Hence, $\lim_{x\to 0} \sqrt{x}=\lim_{x\to 0^+} \sqrt{x}=0$. Okay. I'm still with you. The limit is $0$. Of course, $\lim_{x\to 0^+} \sqrt{x}$ does not exist,... Qué, Mr. Fawlty? ... since $\sqrt{x}$ is not defined when $x<0$. Okay, so are they messing with me? Is my coffee too weak? Too strong? Is there some subtle truth about limits that escapes me? Or is that a typo? Did they mean in that last line to omit the '$+$' by the '$0$' and write ""Of course, $\lim_{x\to 0} \sqrt{x}$ does not exist,...""? EDIT: I think a minus sign is intended instead of a plus sign in that last limit.","Does this right-handed limit exist or not? $$\lim_{x\to 0^+} \sqrt{x}$$ Schaum's Easy Outline of Calculus (Second Edition) says it does. And doesn't. An example in the book states: The function $f(x)=\sqrt{x}$; then $f$ is defined only to the right of zero. Okay. So the right-handed limit does exist. Hence, $\lim_{x\to 0} \sqrt{x}=\lim_{x\to 0^+} \sqrt{x}=0$. Okay. I'm still with you. The limit is $0$. Of course, $\lim_{x\to 0^+} \sqrt{x}$ does not exist,... Qué, Mr. Fawlty? ... since $\sqrt{x}$ is not defined when $x<0$. Okay, so are they messing with me? Is my coffee too weak? Too strong? Is there some subtle truth about limits that escapes me? Or is that a typo? Did they mean in that last line to omit the '$+$' by the '$0$' and write ""Of course, $\lim_{x\to 0} \sqrt{x}$ does not exist,...""? EDIT: I think a minus sign is intended instead of a plus sign in that last limit.",,"['limits', 'radicals']"
13,Prove this limits with $\sin{(\tan{x})}-\tan{(\sin{x})}$,Prove this limits with,\sin{(\tan{x})}-\tan{(\sin{x})},How Find limit $$\lim_{x\to 0}\dfrac{\sin{(\tan{(\sin{(\tan{x})})})}-\tan{(\sin{(\tan{(\sin{x})})})}} {\sin{(\tan{x})}-\tan{(\sin{x})}}$$ My approach is the following: I use wolframalpha found this limits is $2$,How Find limit $$\lim_{x\to 0}\dfrac{\sin{(\tan{(\sin{(\tan{x})})})}-\tan{(\sin{(\tan{(\sin{x})})})}} {\sin{(\tan{x})}-\tan{(\sin{x})}}$$ My approach is the following: I use wolframalpha found this limits is $2$,,[]
14,Tricky limits problem,Tricky limits problem,,Find the limit of $$\lim_{x\to0}\left[1 + \left(\frac{\log \cos x}{\log \cos(x/2)}\right)^2 \right]^2$$ Any help would be thoroughly appreciated.,Find the limit of $$\lim_{x\to0}\left[1 + \left(\frac{\log \cos x}{\log \cos(x/2)}\right)^2 \right]^2$$ Any help would be thoroughly appreciated.,,['limits']
15,Finding asymptotes of exponential function and one-sided limit,Finding asymptotes of exponential function and one-sided limit,,Find the asymptotes of $$ \lim_{x \to \infty}x\cdot\exp\left(\dfrac{2}{x}\right)+1. $$ How is it done?,Find the asymptotes of $$ \lim_{x \to \infty}x\cdot\exp\left(\dfrac{2}{x}\right)+1. $$ How is it done?,,['limits']
16,Problem about solving infinity limit with square root,Problem about solving infinity limit with square root,,"(I) $$\lim_{x \to \infty } \, \left(\sqrt{x^2+x}-\sqrt{x^2-x}\right)=$$ $$\lim_{x \to \infty } \, \left(x\sqrt{1+1/x}-x\sqrt{1-1/x}\right)=$$ $$\lim_{x \to \infty } \, \left(x\sqrt{1}-x\sqrt{1}\right)=\lim_{x \to \infty } \, \left(x-x\right)=0$$ (II) $$\lim_{x \to \infty } \, \left(\sqrt{x^2+x}-\sqrt{x^2-x}\right)=$$ $$\lim_{x \to \infty } \, \left(\left(\sqrt{x^2+x}-\sqrt{x^2-x}\right)*\frac{\left(\sqrt{x^2+x}+\sqrt{x^2-x}\right)}{\left(\sqrt{x^2+x}+\sqrt{x^2-x}\right)}\right)=$$ $$\lim_{x \to \infty } \, \frac{2x}{\left(\sqrt{x^2+x}+\sqrt{x^2-x}\right)}=$$ $$\lim_{x \to \infty } \, \frac{2x}{\left(x\sqrt{1+1/x}+x\sqrt{1-1/x}\right)}=$$ $$\lim_{x \to \infty } \, \frac{2x}{\left(x\sqrt{1}+x\sqrt{1}\right)}=\lim_{x \to \infty } \, \frac{2x}{2x}=1$$ I found these two ways to evaluate this limit. I know the answer is 1. The first one is surely wrong. The question is: why? What is wrong there?","(I) $$\lim_{x \to \infty } \, \left(\sqrt{x^2+x}-\sqrt{x^2-x}\right)=$$ $$\lim_{x \to \infty } \, \left(x\sqrt{1+1/x}-x\sqrt{1-1/x}\right)=$$ $$\lim_{x \to \infty } \, \left(x\sqrt{1}-x\sqrt{1}\right)=\lim_{x \to \infty } \, \left(x-x\right)=0$$ (II) $$\lim_{x \to \infty } \, \left(\sqrt{x^2+x}-\sqrt{x^2-x}\right)=$$ $$\lim_{x \to \infty } \, \left(\left(\sqrt{x^2+x}-\sqrt{x^2-x}\right)*\frac{\left(\sqrt{x^2+x}+\sqrt{x^2-x}\right)}{\left(\sqrt{x^2+x}+\sqrt{x^2-x}\right)}\right)=$$ $$\lim_{x \to \infty } \, \frac{2x}{\left(\sqrt{x^2+x}+\sqrt{x^2-x}\right)}=$$ $$\lim_{x \to \infty } \, \frac{2x}{\left(x\sqrt{1+1/x}+x\sqrt{1-1/x}\right)}=$$ $$\lim_{x \to \infty } \, \frac{2x}{\left(x\sqrt{1}+x\sqrt{1}\right)}=\lim_{x \to \infty } \, \frac{2x}{2x}=1$$ I found these two ways to evaluate this limit. I know the answer is 1. The first one is surely wrong. The question is: why? What is wrong there?",,['limits']
17,"Continuity of $\frac{2xy}{x^2+y^2}$ at $(0,0)$",Continuity of  at,"\frac{2xy}{x^2+y^2} (0,0)","Given a Heaviside function $$f(x,y)=\begin{cases}\frac{2xy}{x^2+y^2}, &x^2+y^2 \neq 0\\0 ,&x^2+y^2=0 \end{cases}$$ Letting $a$ and $b$ be fixed constants, show that for all values of $a$ and $b$, including $0$, the one variable functions $g(x)=f(x,b)$ and $h(y)=f(a,y)$ are both continuous on the entire real line. And how to determine whether the function is continuous at $(0,0)$. What I did was: substitute $x=b$ and $y=a$ in the function $f(x,y)$ to get $f(x,b)=\frac{2xb}{x^2+b^2}$ $f(a,y)=\frac{2ay}{a^2+y^2}$ But I am unsure how to proceed from here. How should I take the limit of the function $f(x,y)$? How do I take care of the $x^2+y^2 \neq 0$?","Given a Heaviside function $$f(x,y)=\begin{cases}\frac{2xy}{x^2+y^2}, &x^2+y^2 \neq 0\\0 ,&x^2+y^2=0 \end{cases}$$ Letting $a$ and $b$ be fixed constants, show that for all values of $a$ and $b$, including $0$, the one variable functions $g(x)=f(x,b)$ and $h(y)=f(a,y)$ are both continuous on the entire real line. And how to determine whether the function is continuous at $(0,0)$. What I did was: substitute $x=b$ and $y=a$ in the function $f(x,y)$ to get $f(x,b)=\frac{2xb}{x^2+b^2}$ $f(a,y)=\frac{2ay}{a^2+y^2}$ But I am unsure how to proceed from here. How should I take the limit of the function $f(x,y)$? How do I take care of the $x^2+y^2 \neq 0$?",,"['limits', 'multivariable-calculus']"
18,How find this nice limit,How find this nice limit,,"It is well kown this following $$\lim_{x\to+\infty}\left(\dfrac{a^{\frac{1}{x}}+b^{\frac{1}{x}}}{2}\right)^x=\sqrt{ab}(a,b>0)$$ and also kown this general $$\lim_{x\to+\infty}\left(\dfrac{a_{1}^{\frac{1}{x}}+a_{2}^{\frac{1}{x}}+\cdots+a^{\frac{1}{x}}_{n}}{n}\right)^x=\sqrt[n]{a_{1}a_{2}\cdots a_{n}},(a_{i}>0,i=1,2,\cdots,n)$$ and some hours ago,I found this nice and Hard limit $$\lim_{x\to+\infty}x\left[\left(\dfrac{a^{\frac{1}{x}}+b^{\frac{1}{x}}}{2}\right)^x-\sqrt{ab}\right]=\dfrac{\sqrt{ab}}{8}\left(\ln{a}-\ln{b}\right)^2$$ my proof $$a^{\frac{1}{x}}+b^{\frac{1}{x}}=e^{\frac{1}{x}\ln{a}}+e^{\frac{1}{x}\ln{b}}=1+\dfrac{1}{x}\ln{a}+\dfrac{1}{2x^2}\ln^2{a}+1+\dfrac{1}{x}\ln{b}+\dfrac{1}{2x^2}\ln^2{b}+o(1/x^2)$$ then \begin{align*} \left(\dfrac{a^{\frac{1}{x}}+b^{\frac{1}{x}}}{2}\right)^x&=\left[1+\dfrac{1}{2x}\ln{ab}+\dfrac{1}{4x^2}\left(\ln^2{a}+\ln^2{b}\right)+o(\frac{1}{x^2})\right]^x\\ &\approx e^{x\ln{\left(1+\dfrac{1}{2x}\ln{ab}+\dfrac{1}{4x^2}\left(\ln^2{a}+\ln^2{b}\right)\right)}}\\ &\approx e^{x\left(\dfrac{1}{2x}\ln{ab}+\dfrac{1}{4x^2}\left(\ln^2{a}+\ln^2{b}\right)\right)}\\ &=\cdots\cdots\\ &\approx \sqrt{ab}\left(1+\dfrac{1}{8x}\left(2\ln^2{a}+2\ln^2{b}-\ln^2{(ab)}\right)\right) \end{align*} so $$\lim_{x\to+\infty}x\left[\left(\dfrac{a^{\frac{1}{x}}+b^{\frac{1}{x}}}{2}\right)^x-\sqrt{ab}\right]=\dfrac{\sqrt{ab}}{8}\left(\ln{a}-\ln{b}\right)^2$$ My question: $$\lim_{x\to+\infty}x\left[\left(\dfrac{a_{1}^{\frac{1}{x}}+a_{2}^{\frac{1}{x}}+\cdots+a^{\frac{1}{x}}_{n}}{n}\right)^x-\sqrt{a_{1}a_{2}\cdots a_{n}}\right]=?$$","It is well kown this following $$\lim_{x\to+\infty}\left(\dfrac{a^{\frac{1}{x}}+b^{\frac{1}{x}}}{2}\right)^x=\sqrt{ab}(a,b>0)$$ and also kown this general $$\lim_{x\to+\infty}\left(\dfrac{a_{1}^{\frac{1}{x}}+a_{2}^{\frac{1}{x}}+\cdots+a^{\frac{1}{x}}_{n}}{n}\right)^x=\sqrt[n]{a_{1}a_{2}\cdots a_{n}},(a_{i}>0,i=1,2,\cdots,n)$$ and some hours ago,I found this nice and Hard limit $$\lim_{x\to+\infty}x\left[\left(\dfrac{a^{\frac{1}{x}}+b^{\frac{1}{x}}}{2}\right)^x-\sqrt{ab}\right]=\dfrac{\sqrt{ab}}{8}\left(\ln{a}-\ln{b}\right)^2$$ my proof $$a^{\frac{1}{x}}+b^{\frac{1}{x}}=e^{\frac{1}{x}\ln{a}}+e^{\frac{1}{x}\ln{b}}=1+\dfrac{1}{x}\ln{a}+\dfrac{1}{2x^2}\ln^2{a}+1+\dfrac{1}{x}\ln{b}+\dfrac{1}{2x^2}\ln^2{b}+o(1/x^2)$$ then \begin{align*} \left(\dfrac{a^{\frac{1}{x}}+b^{\frac{1}{x}}}{2}\right)^x&=\left[1+\dfrac{1}{2x}\ln{ab}+\dfrac{1}{4x^2}\left(\ln^2{a}+\ln^2{b}\right)+o(\frac{1}{x^2})\right]^x\\ &\approx e^{x\ln{\left(1+\dfrac{1}{2x}\ln{ab}+\dfrac{1}{4x^2}\left(\ln^2{a}+\ln^2{b}\right)\right)}}\\ &\approx e^{x\left(\dfrac{1}{2x}\ln{ab}+\dfrac{1}{4x^2}\left(\ln^2{a}+\ln^2{b}\right)\right)}\\ &=\cdots\cdots\\ &\approx \sqrt{ab}\left(1+\dfrac{1}{8x}\left(2\ln^2{a}+2\ln^2{b}-\ln^2{(ab)}\right)\right) \end{align*} so $$\lim_{x\to+\infty}x\left[\left(\dfrac{a^{\frac{1}{x}}+b^{\frac{1}{x}}}{2}\right)^x-\sqrt{ab}\right]=\dfrac{\sqrt{ab}}{8}\left(\ln{a}-\ln{b}\right)^2$$ My question: $$\lim_{x\to+\infty}x\left[\left(\dfrac{a_{1}^{\frac{1}{x}}+a_{2}^{\frac{1}{x}}+\cdots+a^{\frac{1}{x}}_{n}}{n}\right)^x-\sqrt{a_{1}a_{2}\cdots a_{n}}\right]=?$$",,['limits']
19,Numerically estimate the limit of a function,Numerically estimate the limit of a function,,Is there an algorithm that will allow me to numerically compute the limit of a function f(x) in a principled way? The most naive algorithm would be to continue to compute the function for larger values of x. The first problem is how to figure out the 'large' values for x to compute the function for.  How do I know when to stop? Can we construct some error bars for this calculation perhaps based on some kind of statistical rationale? Any books for further reading will be much appreciated.,Is there an algorithm that will allow me to numerically compute the limit of a function f(x) in a principled way? The most naive algorithm would be to continue to compute the function for larger values of x. The first problem is how to figure out the 'large' values for x to compute the function for.  How do I know when to stop? Can we construct some error bars for this calculation perhaps based on some kind of statistical rationale? Any books for further reading will be much appreciated.,,"['analysis', 'limits', 'numerical-methods', 'asymptotics']"
20,Is the limit of $\left(\sum_{i=-n}^{i=n} \tanh(x - i)\right)-2x$ a sine wave?,Is the limit of  a sine wave?,\left(\sum_{i=-n}^{i=n} \tanh(x - i)\right)-2x,"I have been plotting functions of the form  $$\left(\sum_{i=-n}^{i=n} \tanh(x - i)\right)-2x$$ which, over the region around zero, look a lot like sine waves. For example, here's the function for $n=10$. Empirically, it seems to be something approximating $0.00065 \sin(2 \pi x)$. Beyond about $n=10$ it is very stable over the region $x\in[-5,5]$, as you might expect from $\tanh(x)$ being approximately $\pm 1$ for large $\pm x$. I have found plots of the series and $0.00065 \sin(2 \pi x)$ are indistinguishable. Here's the same plot zoomed out with this sine wave in orange: Is this actually tending towards a sine wave? and if so, what is the exact value   of the constant $k$ in $$\lim_{n \to \infty}\left(\sum_{i=-n}^{i=n} \tanh(x - i)\right)-2x = k \sin(2 \pi x)$$ Update I've calculated the specific values of this function symbolically with Mathematica, and evaluated it with $N[.,100]$ function at x=1/4, 5/4, 401/4 and 4001/4 which gives 1/4: 0.0006499727271926147575932558139650291715585570399687299381104388139877502424025845538796044013975974138    5/4: 0.0006499727271926147575932558139650291715585570399687299381104388139877502424025845538796044013975974138  401/4: 0.0006499727271926147575932558139650291715585570399687299381104388139877502424025845538796044013975974138 4001/4: 0.0006499727271926147575932558139650291715585570399687299381104388139877502424025845538796044013975974138 I'm not 100% sure this is not a numerical issue, but I also don't see any reason why it should be 0.00065 exactly either.","I have been plotting functions of the form  $$\left(\sum_{i=-n}^{i=n} \tanh(x - i)\right)-2x$$ which, over the region around zero, look a lot like sine waves. For example, here's the function for $n=10$. Empirically, it seems to be something approximating $0.00065 \sin(2 \pi x)$. Beyond about $n=10$ it is very stable over the region $x\in[-5,5]$, as you might expect from $\tanh(x)$ being approximately $\pm 1$ for large $\pm x$. I have found plots of the series and $0.00065 \sin(2 \pi x)$ are indistinguishable. Here's the same plot zoomed out with this sine wave in orange: Is this actually tending towards a sine wave? and if so, what is the exact value   of the constant $k$ in $$\lim_{n \to \infty}\left(\sum_{i=-n}^{i=n} \tanh(x - i)\right)-2x = k \sin(2 \pi x)$$ Update I've calculated the specific values of this function symbolically with Mathematica, and evaluated it with $N[.,100]$ function at x=1/4, 5/4, 401/4 and 4001/4 which gives 1/4: 0.0006499727271926147575932558139650291715585570399687299381104388139877502424025845538796044013975974138    5/4: 0.0006499727271926147575932558139650291715585570399687299381104388139877502424025845538796044013975974138  401/4: 0.0006499727271926147575932558139650291715585570399687299381104388139877502424025845538796044013975974138 4001/4: 0.0006499727271926147575932558139650291715585570399687299381104388139877502424025845538796044013975974138 I'm not 100% sure this is not a numerical issue, but I also don't see any reason why it should be 0.00065 exactly either.",,"['limits', 'trigonometry', 'hyperbolic-functions']"
21,$\lim_{x \to 0}\lfloor\frac{\tan^{98}x - \sin^{98} x}{x^{100}}\rfloor=?$,,\lim_{x \to 0}\lfloor\frac{\tan^{98}x - \sin^{98} x}{x^{100}}\rfloor=?,fine the limit : $$\lim_{x \to 0}\lfloor\frac{\tan^{98}x - \sin^{98} x}{x^{100}}\rfloor=?$$ We denote the floor funtion by $\lfloor x\rfloor$. My try: \begin{align} \lim_{x \to 0}\frac{\tan^{n}x - \sin^{n} x}{x^{n + 2}} &= \lim_{x \to 0}\frac{\tan x - \sin x}{x^{3}}\cdot \sum_{i = 0}^{n - 1}\frac{\tan^{n - 1 - i}x}{x^{n - 1 - i}}\cdot\frac{\sin ^{i}x}{x^{i}}\\ &= \frac{1}{2}\cdot\sum_{i = 0}^{n - 1}1\\ &= \frac{n}{2} \end{align} So: \begin{align} \lim_{x \to 0}\frac{\tan^{98}x - \sin^{98} x}{x^{100}}&=49\\ \lim_{x \to 0}\left\lfloor\frac{\tan^{98}x - \sin^{98} x}{x^{100}}\right\rfloor&=49  \end{align} is this correct?,fine the limit : $$\lim_{x \to 0}\lfloor\frac{\tan^{98}x - \sin^{98} x}{x^{100}}\rfloor=?$$ We denote the floor funtion by $\lfloor x\rfloor$. My try: \begin{align} \lim_{x \to 0}\frac{\tan^{n}x - \sin^{n} x}{x^{n + 2}} &= \lim_{x \to 0}\frac{\tan x - \sin x}{x^{3}}\cdot \sum_{i = 0}^{n - 1}\frac{\tan^{n - 1 - i}x}{x^{n - 1 - i}}\cdot\frac{\sin ^{i}x}{x^{i}}\\ &= \frac{1}{2}\cdot\sum_{i = 0}^{n - 1}1\\ &= \frac{n}{2} \end{align} So: \begin{align} \lim_{x \to 0}\frac{\tan^{98}x - \sin^{98} x}{x^{100}}&=49\\ \lim_{x \to 0}\left\lfloor\frac{\tan^{98}x - \sin^{98} x}{x^{100}}\right\rfloor&=49  \end{align} is this correct?,,['limits']
22,Pointwise convergence of Fourier series in two dimensions,Pointwise convergence of Fourier series in two dimensions,,"By Carleson's Theorem, we know that for every $f\in L^2(\mathbb{T})$ $$ f(x)=\lim_{N\rightarrow\infty}\sum_{k=-N}^N\hat{f}(k)e^{2\pi ikx}\;\text{ a.e.} $$ Suppose now that $f\in L^2(\mathbb{T}^2)$. Using Carleson's Theorem, which would be the easiest way to prove $$ f(x,y)=\lim_{N\rightarrow\infty}\sum_{k,l=-N}^N\hat{f}(k,l)e^{2\pi i(kx+ly)}\;\text{ a.e.}? $$ Is it also true that $$ f(x,y)=\lim_{M,N\rightarrow\infty}\sum_{|k|\leq M,\,|l|\leq N}\hat{f}(k,l)e^{2\pi i(kx+ly)}\;\text{ a.e.}? $$ (by $\lim_{M,N\rightarrow\infty}$ I mean the limit of a double sequence: given $\{a_{m,n}\}\subseteq\mathbb{C}$, we say that $\lim_{m,n\rightarrow\infty}a_{m,n}=L$ if for all $\epsilon>0$ there exists an $N_{\epsilon}\in\mathbb{N}$ such that $|L-a_{m,n}|<\epsilon$ for every $m,n\geq N_{\epsilon}$).","By Carleson's Theorem, we know that for every $f\in L^2(\mathbb{T})$ $$ f(x)=\lim_{N\rightarrow\infty}\sum_{k=-N}^N\hat{f}(k)e^{2\pi ikx}\;\text{ a.e.} $$ Suppose now that $f\in L^2(\mathbb{T}^2)$. Using Carleson's Theorem, which would be the easiest way to prove $$ f(x,y)=\lim_{N\rightarrow\infty}\sum_{k,l=-N}^N\hat{f}(k,l)e^{2\pi i(kx+ly)}\;\text{ a.e.}? $$ Is it also true that $$ f(x,y)=\lim_{M,N\rightarrow\infty}\sum_{|k|\leq M,\,|l|\leq N}\hat{f}(k,l)e^{2\pi i(kx+ly)}\;\text{ a.e.}? $$ (by $\lim_{M,N\rightarrow\infty}$ I mean the limit of a double sequence: given $\{a_{m,n}\}\subseteq\mathbb{C}$, we say that $\lim_{m,n\rightarrow\infty}a_{m,n}=L$ if for all $\epsilon>0$ there exists an $N_{\epsilon}\in\mathbb{N}$ such that $|L-a_{m,n}|<\epsilon$ for every $m,n\geq N_{\epsilon}$).",,"['limits', 'fourier-analysis', 'fourier-series', 'harmonic-analysis']"
23,Find the limit $\lim_\limits{x\to 0^+}{\left( e^{\frac{1}{\sin x}}-e^{\frac{1}{x}}\right)}$,Find the limit,\lim_\limits{x\to 0^+}{\left( e^{\frac{1}{\sin x}}-e^{\frac{1}{x}}\right)},"Find the limit:   $$\lim_\limits{x\to 0^+}{\left( e^{\frac{1}{\sin x}}-e^{\frac{1}{x}}\right)}$$ Using graph inspection, I have found the limit to be $+\infty$, but I cannot prove this in any way (I tried factorizing, using DLH)... Can anyone give a hint about that? The limit should be done without any approximations, because we haven't been taught those yet.","Find the limit:   $$\lim_\limits{x\to 0^+}{\left( e^{\frac{1}{\sin x}}-e^{\frac{1}{x}}\right)}$$ Using graph inspection, I have found the limit to be $+\infty$, but I cannot prove this in any way (I tried factorizing, using DLH)... Can anyone give a hint about that? The limit should be done without any approximations, because we haven't been taught those yet.",,[]
24,How does the chain rule for limits work?,How does the chain rule for limits work?,,"I have to evaluate the limit of this function, $$\lim_{x\to0^+} \arctan(\ln x)$$ I already know the answer, it's $-\dfrac{π}{2}$, but the only part I don't get it, how does it come to that? I did the following steps: $$\lim_{x\to0^+} \arctan(\ln x) = \arctan\left(\lim_{x\to0^+} \ln x\right)$$ The limit of $\ln(x)$ when $x$ approaces $0^+$ is negative infinity, wouldn't that mean the answer we're looking for is arctan of negative infinity, which is something we can't find? Still, it goes to: $$\lim_{x\to -\infty} \arctan (x) = -\dfrac{\pi}{2}$$, which is how the answer seem to work? How does this happen? And, how does the chain rule come in all this? Thank you in advance for your answer.","I have to evaluate the limit of this function, $$\lim_{x\to0^+} \arctan(\ln x)$$ I already know the answer, it's $-\dfrac{π}{2}$, but the only part I don't get it, how does it come to that? I did the following steps: $$\lim_{x\to0^+} \arctan(\ln x) = \arctan\left(\lim_{x\to0^+} \ln x\right)$$ The limit of $\ln(x)$ when $x$ approaces $0^+$ is negative infinity, wouldn't that mean the answer we're looking for is arctan of negative infinity, which is something we can't find? Still, it goes to: $$\lim_{x\to -\infty} \arctan (x) = -\dfrac{\pi}{2}$$, which is how the answer seem to work? How does this happen? And, how does the chain rule come in all this? Thank you in advance for your answer.",,"['limits', 'infinity']"
25,Prove $\lim \limits_{x \to\infty } \frac{f(x)}{x}=0$ and $f$ differentiable implies $ \lim \limits_{x \to\infty } \inf |f'(x)|=0 $,Prove  and  differentiable implies,\lim \limits_{x \to\infty } \frac{f(x)}{x}=0 f  \lim \limits_{x \to\infty } \inf |f'(x)|=0 ,"Given a differentiable function on $(a,+\infty)$ such as $\lim \limits_{x \to\infty } \frac{f(x)}{x}=0$ prove the following: $$ \lim \limits_{x \to\infty } \inf |f'(x)|=0  $$ I just can't see how to do it... (even after understanding How to show that $\lim\limits_{x \to \infty} f'(x) = 0$ implies $\lim\limits_{x \to \infty} \frac{f(x)}{x} = 0$? )","Given a differentiable function on $(a,+\infty)$ such as $\lim \limits_{x \to\infty } \frac{f(x)}{x}=0$ prove the following: $$ \lim \limits_{x \to\infty } \inf |f'(x)|=0  $$ I just can't see how to do it... (even after understanding How to show that $\lim\limits_{x \to \infty} f'(x) = 0$ implies $\lim\limits_{x \to \infty} \frac{f(x)}{x} = 0$? )",,"['limits', 'derivatives']"
26,What are some applications of projective Fraïssé limits?,What are some applications of projective Fraïssé limits?,,I am looking for some applications of projective Fraïssé limits . For example are they related to a theorem in set theory or topology? Also is there any modified version for them? (like the version of Hrushovski for Fraïssé limits which led to counterexample to Zilber's conjecture?) Would you please explain some about their importance?,I am looking for some applications of projective Fraïssé limits . For example are they related to a theorem in set theory or topology? Also is there any modified version for them? (like the version of Hrushovski for Fraïssé limits which led to counterexample to Zilber's conjecture?) Would you please explain some about their importance?,,"['limits', 'logic', 'model-theory', 'applications']"
27,Limit of $0^0$ (Indeterminate form),Limit of  (Indeterminate form),0^0,"Well this format of a limit $0^0$ is an indeterminate form. I claim that whatever this limit is (which depends on the exact question) should always be in between $[0,1]$ . Is my claim correct? I have no mathematical proof for it but just a basic idea, that any number base $0$ should try to pull towards $0$ , whereas any number power $0$ should pull towards $1$ .","Well this format of a limit is an indeterminate form. I claim that whatever this limit is (which depends on the exact question) should always be in between . Is my claim correct? I have no mathematical proof for it but just a basic idea, that any number base should try to pull towards , whereas any number power should pull towards .","0^0 [0,1] 0 0 0 1",['limits']
28,Scary looking limit with an elegant answer,Scary looking limit with an elegant answer,,"This problem was posted half a year ago by Pierre Mounir on a Facebook group and until now it received no answers. Since most of his problems that I saw were amazing I can bet this one it's worth the time. Wolfram returns the answer to be $2$ , which is quite elegant for it's look. I remembered about it yesterday and gave a try again taking for simplicity $k=1$ (I had no chance with a bigger number). Also my whole idea was to somehow get to a point where I can use $\lim\limits_{f\to 0}\frac{a^f-1}{f}=\ln a$ , thus I started as: $$\lim_{n\to \infty} {\frac{5^\frac{n!}{(2n)!}-4^\frac{1}{n!}}{3^\frac{n!}{(2n)!}-2^\frac{1}{n!}}}=\lim_{n\to \infty} \left(\frac{4}{2}\right)^{\frac{1}{n!}}\left(\frac{5^\frac{n!}{(2n)!}}{4^{\frac1{n!}}}-1\right)\left(\frac{3^\frac{n!}{(2n)!}}{2^{\frac1{n!}}}-1\right)^{-1}$$ $$=\lim_{n\to \infty} \underbrace{\sqrt[n!]{2}}_{\to 1}\left(\sqrt[n!]{\frac{5^\frac{1}{(2n)!}}{4}}-1\right)\left(\sqrt[n!]{\frac{3^\frac{1}{(2n)!}}{2}}-1\right)^{-1}$$ Well, yes $\frac{5^\frac{1}{(2n)!}}{4}$ and the other one in the denominator equals to $1$ , but still I don't see how to use that limit. Also I tried to take a logarithm on both sides or to use L'hospital, but looks like a dead end. I would love if someone can spot the trick for solving this limit and land some help.","This problem was posted half a year ago by Pierre Mounir on a Facebook group and until now it received no answers. Since most of his problems that I saw were amazing I can bet this one it's worth the time. Wolfram returns the answer to be , which is quite elegant for it's look. I remembered about it yesterday and gave a try again taking for simplicity (I had no chance with a bigger number). Also my whole idea was to somehow get to a point where I can use , thus I started as: Well, yes and the other one in the denominator equals to , but still I don't see how to use that limit. Also I tried to take a logarithm on both sides or to use L'hospital, but looks like a dead end. I would love if someone can spot the trick for solving this limit and land some help.",2 k=1 \lim\limits_{f\to 0}\frac{a^f-1}{f}=\ln a \lim_{n\to \infty} {\frac{5^\frac{n!}{(2n)!}-4^\frac{1}{n!}}{3^\frac{n!}{(2n)!}-2^\frac{1}{n!}}}=\lim_{n\to \infty} \left(\frac{4}{2}\right)^{\frac{1}{n!}}\left(\frac{5^\frac{n!}{(2n)!}}{4^{\frac1{n!}}}-1\right)\left(\frac{3^\frac{n!}{(2n)!}}{2^{\frac1{n!}}}-1\right)^{-1} =\lim_{n\to \infty} \underbrace{\sqrt[n!]{2}}_{\to 1}\left(\sqrt[n!]{\frac{5^\frac{1}{(2n)!}}{4}}-1\right)\left(\sqrt[n!]{\frac{3^\frac{1}{(2n)!}}{2}}-1\right)^{-1} \frac{5^\frac{1}{(2n)!}}{4} 1,"['limits', 'factorial']"
29,How to prove that limit doesn't exist using epsilon-delta definition?,How to prove that limit doesn't exist using epsilon-delta definition?,,"It is easy to prove the limit exists, all we have to show is there exists a relationship between $\delta$ and $\epsilon$. But how are we supposed to prove limit doesn't exists? The problem is when we are proving for a limit we already know what the limit is and with that, algebra is all that's needed. Please show through an example (you may show that $\lim_{x\rightarrow0} \frac{1}{x}$ doesn't exist) If possible please use the explanation scheme that is used by this answer https://math.stackexchange.com/a/66552/335742","It is easy to prove the limit exists, all we have to show is there exists a relationship between $\delta$ and $\epsilon$. But how are we supposed to prove limit doesn't exists? The problem is when we are proving for a limit we already know what the limit is and with that, algebra is all that's needed. Please show through an example (you may show that $\lim_{x\rightarrow0} \frac{1}{x}$ doesn't exist) If possible please use the explanation scheme that is used by this answer https://math.stackexchange.com/a/66552/335742",,['limits']
30,Why can $2^3$ be defined but $0^0$ cannot,Why can  be defined but  cannot,2^3 0^0,"From what I gather, we can't just define $0^0$ to be $0$ or $1$ or $69$ or whatever, because $\lim\limits_{x\mathop\to0}0^x=0$ and $\lim\limits_{x\mathop\to0}x^0=1$. So $0^0$ is called indeterminate But why can we define (say) $2^3$? How do I know that if $\lim\limits_{x\mathop\to c}f(x)=2$ and $\lim\limits_{x\mathop\to c}g(x)=3$ then $\lim\limits_{x\mathop\to c}f(x)^{g(x)}=8$ for all functions $f$ and $g$? Is this true and if so is there a proof?","From what I gather, we can't just define $0^0$ to be $0$ or $1$ or $69$ or whatever, because $\lim\limits_{x\mathop\to0}0^x=0$ and $\lim\limits_{x\mathop\to0}x^0=1$. So $0^0$ is called indeterminate But why can we define (say) $2^3$? How do I know that if $\lim\limits_{x\mathop\to c}f(x)=2$ and $\lim\limits_{x\mathop\to c}g(x)=3$ then $\lim\limits_{x\mathop\to c}f(x)^{g(x)}=8$ for all functions $f$ and $g$? Is this true and if so is there a proof?",,"['limits', 'indeterminate-forms']"
31,Why is $\lim\limits_{n\to\infty} \prod\limits_{k=1}^n \left(1 + \frac{k+1}{n^2}\right) = \sqrt e$?,Why is ?,\lim\limits_{n\to\infty} \prod\limits_{k=1}^n \left(1 + \frac{k+1}{n^2}\right) = \sqrt e,"Mathematica returns these somewhat striking (to me, at any rate) infinite product identities: $$\lim_{n\to\infty} \prod_{k=1}^n \left(1 + \frac{k+1}{n^2}\right) = \sqrt e$$ and $$\lim_{n\to\infty} \prod_{k=1}^n \left(1 + \frac{(k+1)^2}{n^3}\right) = \sqrt[3]{e}$$ With larger exponents, checking with software seems to take forever. I'm making a bit of a leap here, but these results suggest the following closed form for positive integer $p$ : $$\lim_{n\to\infty} \prod_{k=1}^n \left(1 + \frac{(k+1)^p}{n^{p+1}}\right) = e^{\frac1{p+1}}$$ How does one compute either or both of the first two limits? Does the conjecture hold? In the general case, we have $$\prod_{k=1}^n \left(1 + \frac{(k+1)^p}{n^{p+1}}\right) = \frac{n^{p+1}+(n+1)^p}{n^{p+1}+1} \prod_{k=1}^n \left(1 + \frac{k^p}{n^{p+1}}\right)$$ and the coefficient of the product converges to $1$ . From here, I rewrite the product as a sum of logarithms and attempt to rearrange terms to reveal a Riemann sum, but I have had no luck so far. For instance, in the case of $p=1$ , $$\begin{align*} \lim_{n\to\infty} \prod_{k=1}^n \left(1 + \frac k{n^2}\right) &= \exp\left(\lim_{n\to\infty}\sum_{k=1}^n \ln\left(1 + \frac k{n^2}\right)\right)\\[1ex] &= \exp\left(\lim_{n\to\infty}\sum_{k=1}^n \left(\ln\left(n+\frac kn\right) - \ln(n)\right)\right)\\[1ex] &= \exp\left(\lim_{n\to\infty}\left(\sum_{k=1}^n \ln\left(n+\frac kn\right) - n\ln(n)\right)\right)\\[1ex] &= \exp\left(\lim_{n\to\infty} n \left(\frac1n \sum_{k=1}^n \ln\left(n+\frac kn\right) - \ln(n)\right)\right)\\[1ex] \end{align*}$$","Mathematica returns these somewhat striking (to me, at any rate) infinite product identities: and With larger exponents, checking with software seems to take forever. I'm making a bit of a leap here, but these results suggest the following closed form for positive integer : How does one compute either or both of the first two limits? Does the conjecture hold? In the general case, we have and the coefficient of the product converges to . From here, I rewrite the product as a sum of logarithms and attempt to rearrange terms to reveal a Riemann sum, but I have had no luck so far. For instance, in the case of ,","\lim_{n\to\infty} \prod_{k=1}^n \left(1 + \frac{k+1}{n^2}\right) = \sqrt e \lim_{n\to\infty} \prod_{k=1}^n \left(1 + \frac{(k+1)^2}{n^3}\right) = \sqrt[3]{e} p \lim_{n\to\infty} \prod_{k=1}^n \left(1 + \frac{(k+1)^p}{n^{p+1}}\right) = e^{\frac1{p+1}} \prod_{k=1}^n \left(1 + \frac{(k+1)^p}{n^{p+1}}\right) = \frac{n^{p+1}+(n+1)^p}{n^{p+1}+1} \prod_{k=1}^n \left(1 + \frac{k^p}{n^{p+1}}\right) 1 p=1 \begin{align*}
\lim_{n\to\infty} \prod_{k=1}^n \left(1 + \frac k{n^2}\right) &= \exp\left(\lim_{n\to\infty}\sum_{k=1}^n \ln\left(1 + \frac k{n^2}\right)\right)\\[1ex]
&= \exp\left(\lim_{n\to\infty}\sum_{k=1}^n \left(\ln\left(n+\frac kn\right) - \ln(n)\right)\right)\\[1ex]
&= \exp\left(\lim_{n\to\infty}\left(\sum_{k=1}^n \ln\left(n+\frac kn\right) - n\ln(n)\right)\right)\\[1ex]
&= \exp\left(\lim_{n\to\infty} n \left(\frac1n \sum_{k=1}^n \ln\left(n+\frac kn\right) - \ln(n)\right)\right)\\[1ex]
\end{align*}","['limits', 'exponential-function', 'infinite-product']"
32,Babylonian Method Limit Question,Babylonian Method Limit Question,,"The Babylonian Method for finding square roots is a method that takes a guess, say $x$ , and averages $x$ and $\frac{a}{x}$ , where $a$ is the number you want to find the square root of. It then uses the average as a guess, and does the algorithm again. The value outputted will converge towards $\sqrt{a}$ . Translating this into algebraic terms, we get that $$\lim_{n\to\infty}f^n(2)=\sqrt{2},$$ where $f(x)=\frac{x+\frac{2}{x}}{2}$ . I wondered what would happen if we changed the initial input value. So, this is my question. Let $f(x)=\frac{x+\frac{2}{x}}{2}$ . What is $$\lim_{n\to\infty} f^n(2^n)?$$ Plugging this into my calculator, I got an answer that was about $1.591891656$ . I have no idea what is special about this number. If anyone could figure this out, I would appreciate it.","The Babylonian Method for finding square roots is a method that takes a guess, say , and averages and , where is the number you want to find the square root of. It then uses the average as a guess, and does the algorithm again. The value outputted will converge towards . Translating this into algebraic terms, we get that where . I wondered what would happen if we changed the initial input value. So, this is my question. Let . What is Plugging this into my calculator, I got an answer that was about . I have no idea what is special about this number. If anyone could figure this out, I would appreciate it.","x x \frac{a}{x} a \sqrt{a} \lim_{n\to\infty}f^n(2)=\sqrt{2}, f(x)=\frac{x+\frac{2}{x}}{2} f(x)=\frac{x+\frac{2}{x}}{2} \lim_{n\to\infty} f^n(2^n)? 1.591891656","['limits', 'radicals']"
33,"Given that $\lim\limits_{n \to \infty}\left(1+\frac{1}{n}\right)^n=e$, show that $1+\frac{1}{1!}+\frac{1}{2!}+\cdots=e$.","Given that , show that .",\lim\limits_{n \to \infty}\left(1+\frac{1}{n}\right)^n=e 1+\frac{1}{1!}+\frac{1}{2!}+\cdots=e,"Problem Given that $\lim\limits_{n \to \infty}\left(1+\dfrac{1}{n} \right)^n = e$, show that $1+\dfrac{1}{1!}+\dfrac{1}{2!}+\cdots=e$. Proof By binomial theorem, we have \begin{align*}\left(1+\dfrac{1}{n}\right)^n&=\sum_{k=0}^{k=n}\binom{n}{k}1^k \left(\frac{1}{n}\right)^{n-k}\\ &=\sum_{k=0}^{k=n}\frac{1}{k!}\left(1-\frac{1}{n}\right)\left(1-\frac{2}{n}\right)\cdots \left(1-\frac{k-1}{n}\right). \end{align*} Thus, on one hand, $$\left(1+\dfrac{1}{n}\right)^n \leq \sum_{k=0}^{k=n}\frac{1}{k!}.\tag1$$ Take the limits as $n \to \infty$ on the both sides of $(1)$. We have $$e=\lim_{n \to \infty}\left(1+\dfrac{1}{n}\right)^n\leq \varliminf_{n \to \infty} \sum_{k=0}^{k=n}\frac{1}{k!}.\tag2$$ On the other hand, take a positive integer $m$ such that $m<n$ and fix it. We have $$\left(1+\dfrac{1}{n}\right)^n \geq \sum_{k=0}^{k=m}\frac{1}{k!}\left(1-\frac{1}{n}\right)\left(1-\frac{2}{n}\right)\cdots \left(1-\frac{k-1}{n}\right).\tag3$$ Likewise,take the limits as $n \to \infty$ on the both sides of $(3)$. We have $$e=\lim_{n \to \infty}\left(1+\dfrac{1}{n}\right)^n\geq \sum_{k=0}^{k=m}\frac{1}{k!}.\tag4$$ Take the limits as $m \to \infty$ on the both sides of $(4)$. We have $$e \geq \varlimsup_{m \to \infty}\sum_{k=0}^{k=m}\frac{1}{k!}.\tag 5$$ Combine $(2)$ and $(5)$. It follows that $$\lim_{n \to \infty}\sum_{k=0}^{k=n}\frac{1}{k!}=e.$$ Please correct me If I'm faulty. Hope to see other solutions.","Problem Given that $\lim\limits_{n \to \infty}\left(1+\dfrac{1}{n} \right)^n = e$, show that $1+\dfrac{1}{1!}+\dfrac{1}{2!}+\cdots=e$. Proof By binomial theorem, we have \begin{align*}\left(1+\dfrac{1}{n}\right)^n&=\sum_{k=0}^{k=n}\binom{n}{k}1^k \left(\frac{1}{n}\right)^{n-k}\\ &=\sum_{k=0}^{k=n}\frac{1}{k!}\left(1-\frac{1}{n}\right)\left(1-\frac{2}{n}\right)\cdots \left(1-\frac{k-1}{n}\right). \end{align*} Thus, on one hand, $$\left(1+\dfrac{1}{n}\right)^n \leq \sum_{k=0}^{k=n}\frac{1}{k!}.\tag1$$ Take the limits as $n \to \infty$ on the both sides of $(1)$. We have $$e=\lim_{n \to \infty}\left(1+\dfrac{1}{n}\right)^n\leq \varliminf_{n \to \infty} \sum_{k=0}^{k=n}\frac{1}{k!}.\tag2$$ On the other hand, take a positive integer $m$ such that $m<n$ and fix it. We have $$\left(1+\dfrac{1}{n}\right)^n \geq \sum_{k=0}^{k=m}\frac{1}{k!}\left(1-\frac{1}{n}\right)\left(1-\frac{2}{n}\right)\cdots \left(1-\frac{k-1}{n}\right).\tag3$$ Likewise,take the limits as $n \to \infty$ on the both sides of $(3)$. We have $$e=\lim_{n \to \infty}\left(1+\dfrac{1}{n}\right)^n\geq \sum_{k=0}^{k=m}\frac{1}{k!}.\tag4$$ Take the limits as $m \to \infty$ on the both sides of $(4)$. We have $$e \geq \varlimsup_{m \to \infty}\sum_{k=0}^{k=m}\frac{1}{k!}.\tag 5$$ Combine $(2)$ and $(5)$. It follows that $$\lim_{n \to \infty}\sum_{k=0}^{k=n}\frac{1}{k!}=e.$$ Please correct me If I'm faulty. Hope to see other solutions.",,"['limits', 'analysis', 'proof-verification', 'limsup-and-liminf']"
34,Is $ \lim_{n \to \infty} a_n ^{b_n} = e^{\lim_{n \to \infty}(a_n - 1)b_n}$ always true?,Is  always true?, \lim_{n \to \infty} a_n ^{b_n} = e^{\lim_{n \to \infty}(a_n - 1)b_n},"Consider $a_n$ and $b_n$ are two sequences which $\lim _{n \to \infty} a_n  = 1$ and $\lim _{n \to \infty} b_n  = \infty$ . Can we always use this formula ? $$ \lim_{n \to \infty} a_n ^{b_n} = e^{\lim_{n \to \infty}(a_n - 1)b_n}$$ Also, when can we use this method for functions ? A famous case is $a_n = 1+ \frac{1}{n}$ and $b_n = n$ . So $\lim_{n \to \infty}(a_n - 1)b_n = 1$ and $a_n ^{b^n} = e^1 = e$","Consider $a_n$ and $b_n$ are two sequences which $\lim _{n \to \infty} a_n  = 1$ and $\lim _{n \to \infty} b_n  = \infty$ . Can we always use this formula ? $$ \lim_{n \to \infty} a_n ^{b_n} = e^{\lim_{n \to \infty}(a_n - 1)b_n}$$ Also, when can we use this method for functions ? A famous case is $a_n = 1+ \frac{1}{n}$ and $b_n = n$ . So $\lim_{n \to \infty}(a_n - 1)b_n = 1$ and $a_n ^{b^n} = e^1 = e$",,"['limits', 'limits-without-lhopital']"
35,Smallest constant in exponent so that limit of sum is $0$,Smallest constant in exponent so that limit of sum is,0,I am trying to work out the smallest constant $c>0$ so that $$\lim_{n \to \infty} \sum_{a=1}^n \sum_{b=0}^n {n \choose a} {n-a \choose b} \left({a+b \choose a} 2^{-a-b}\right)^{c n/\ln{n}} =0 .$$ If $c =3$ I can see that it certainly does and if $c=2$ it appears not to. How can you work out the constant exactly? My current guess from a crude use of Stirling's approximation is that $c> 6/e$ is necessary and sufficient.,I am trying to work out the smallest constant $c>0$ so that $$\lim_{n \to \infty} \sum_{a=1}^n \sum_{b=0}^n {n \choose a} {n-a \choose b} \left({a+b \choose a} 2^{-a-b}\right)^{c n/\ln{n}} =0 .$$ If $c =3$ I can see that it certainly does and if $c=2$ it appears not to. How can you work out the constant exactly? My current guess from a crude use of Stirling's approximation is that $c> 6/e$ is necessary and sufficient.,,"['limits', 'binomial-coefficients']"
36,"Let $f(x)=\frac{x\ln x}{1+x}$,evaluate $\lim _{m\to f\left(x_0\right)+}\frac{x_1+x_2-2x_0}{m+x_0}$.","Let ,evaluate .",f(x)=\frac{x\ln x}{1+x} \lim _{m\to f\left(x_0\right)+}\frac{x_1+x_2-2x_0}{m+x_0},"Let $f(x)=\frac{x\ln x}{1+x}$ ,suppose $x_0$ is the extreme point of $f$ .For $m\in \mathbb{R}	$ ,it's easy to see  that if $m>f(x_0)=-x_0$ ,then the equation $f(x)=m$ has exactly two real roots $x_1,x_2$ , $x_1<x_0<x_2$ .The question is how to evaluate the limit \begin{align*} \lim _{m\to f\left(x_0\right)+}\frac{x_1+x_2-2x_0}{m+x_0}？ \end{align*} To this problem,we can see that if $m\to f(x_0)+$ ,then $x_1\to x_0^-$ ,and $x_2\to x_0^+$ .By Taylor's formula we have $$f(x_1)-f(x_0)=f'(x_0)(x_1-x_0)+\frac12f''(\xi)(x_1-x_0)^2=\frac12f''(\xi)(x_1-x_0)^2,$$ where $\xi \in (x_1,x_0)$ .So $$m+x_0\sim \frac{f''(x_0)}{2}(x_1-x_0)^2.$$ But I can't handle $x_1+x_2-2x_0$ .If we can expand $x_1+x_2-2x_0$ as  terms of power of $(x_1-x_0)$ ,then it must be $$x_1+x_2-2x_0\sim k(x_1-x_0)^2.$$ But how can I get it?Maybe there are other ways.","Let ,suppose is the extreme point of .For ,it's easy to see  that if ,then the equation has exactly two real roots , .The question is how to evaluate the limit To this problem,we can see that if ,then ,and .By Taylor's formula we have where .So But I can't handle .If we can expand as  terms of power of ,then it must be But how can I get it?Maybe there are other ways.","f(x)=\frac{x\ln x}{1+x} x_0 f m\in \mathbb{R}	 m>f(x_0)=-x_0 f(x)=m x_1,x_2 x_1<x_0<x_2 \begin{align*}
\lim _{m\to f\left(x_0\right)+}\frac{x_1+x_2-2x_0}{m+x_0}？
\end{align*} m\to f(x_0)+ x_1\to x_0^- x_2\to x_0^+ f(x_1)-f(x_0)=f'(x_0)(x_1-x_0)+\frac12f''(\xi)(x_1-x_0)^2=\frac12f''(\xi)(x_1-x_0)^2, \xi \in (x_1,x_0) m+x_0\sim \frac{f''(x_0)}{2}(x_1-x_0)^2. x_1+x_2-2x_0 x_1+x_2-2x_0 (x_1-x_0) x_1+x_2-2x_0\sim k(x_1-x_0)^2.","['limits', 'mathematical-physics']"
37,Confusing Limit With Logarithms,Confusing Limit With Logarithms,,"So I am probably below the knowledge level of the average mathematician here- computer science student here studying algorithms at the graduate level and came across this peculiarity that I would appreciate some context to. Hopefully this is a softball question for you folks. Why is $$\lim_{x\to \infty} \frac{(\ln (\ln x))^{(\ln (\ln (x) )}}{x^2} = 0$$ But, $$\lim_{x\to \infty} \frac{(\ln (\ln x))^{\ln (x) }}{x^2} = \infty$$ I have tried to apply L'Hopital's rule to this case which may be correct, but gives some annoying derivatives to decipher. Would appreciate a bit more reasoning as to why $x^2$ dominates the first case but not the second. To my intuition, since $ln(ln(x))$ approaches infinity similar to $ln(x)$ (albeit incredibly slowly), $x^2$ should be dominated in both cases. Wolfram seems to disagree.","So I am probably below the knowledge level of the average mathematician here- computer science student here studying algorithms at the graduate level and came across this peculiarity that I would appreciate some context to. Hopefully this is a softball question for you folks. Why is But, I have tried to apply L'Hopital's rule to this case which may be correct, but gives some annoying derivatives to decipher. Would appreciate a bit more reasoning as to why dominates the first case but not the second. To my intuition, since approaches infinity similar to (albeit incredibly slowly), should be dominated in both cases. Wolfram seems to disagree.",\lim_{x\to \infty} \frac{(\ln (\ln x))^{(\ln (\ln (x) )}}{x^2} = 0 \lim_{x\to \infty} \frac{(\ln (\ln x))^{\ln (x) }}{x^2} = \infty x^2 ln(ln(x)) ln(x) x^2,"['limits', 'logarithms']"
38,Prove that $\prod_{i=1}^n (1+x_i/n) \sim \exp (\sum_{i=1}^n x_i/n)$ as $n\rightarrow\infty$?,Prove that  as ?,\prod_{i=1}^n (1+x_i/n) \sim \exp (\sum_{i=1}^n x_i/n) n\rightarrow\infty,"Let $x_1,x_2,\dots$ be an infinite sequence of real numbers. Assume that they are bounded, $|x_i| \le C < \infty$ for all $i$ for some $C$ . Is it true that, for any such sequence $$\lim_{n \rightarrow \infty} \prod_{i = 1}^n \left( 1 + \frac{x_i}{n} \right) = \lim_{n \rightarrow \infty}  \exp \left(\frac{1}{n} \sum_{i = 1}^n x_i \right)?\qquad\qquad (1)$$ So far I only have the heuristic argument $$\prod_{i = 1}^n \left[ \left( 1 + \frac{x_i}{n} \right)^n \right]^{1 / n} \rightarrow \prod_{i = 1}^n \mathrm e^{x_i / n} = \exp \left( \frac{1}{n} \sum_{i= 1}^n x_i \right)$$ but I can't be sure that it is correct. Update: The answer to my original question, that the lims in (1) are equal, is NO. In fact the limits might not exist. However their ratio goes to 1, so the two expressions are asymptotically equivalent. I've updated the title to reflect the actual true statement that was proved in the answers here to make this easier to find.","Let be an infinite sequence of real numbers. Assume that they are bounded, for all for some . Is it true that, for any such sequence So far I only have the heuristic argument but I can't be sure that it is correct. Update: The answer to my original question, that the lims in (1) are equal, is NO. In fact the limits might not exist. However their ratio goes to 1, so the two expressions are asymptotically equivalent. I've updated the title to reflect the actual true statement that was proved in the answers here to make this easier to find.","x_1,x_2,\dots |x_i| \le C < \infty i C \lim_{n \rightarrow \infty} \prod_{i = 1}^n \left( 1 + \frac{x_i}{n} \right)
= \lim_{n \rightarrow \infty}  \exp \left(\frac{1}{n} \sum_{i = 1}^n x_i
\right)?\qquad\qquad (1) \prod_{i = 1}^n \left[ \left( 1 + \frac{x_i}{n} \right)^n \right]^{1 / n}
\rightarrow \prod_{i = 1}^n \mathrm e^{x_i / n} = \exp \left( \frac{1}{n} \sum_{i= 1}^n x_i \right)","['limits', 'convergence-divergence', 'infinite-product']"
39,"Prove that $\lim_{(x,y)\rightarrow(0,0)} \frac{|x|^{a}|y|^{b}}{|x|^{c} + |y|^{d}}$ does not exist",Prove that  does not exist,"\lim_{(x,y)\rightarrow(0,0)} \frac{|x|^{a}|y|^{b}}{|x|^{c} + |y|^{d}}","Given $\frac{a}{c} + \frac{b}{d} = 1$ Prove that $$\lim_{(x,y)\rightarrow(0,0)} \frac{|x|^{a}|y|^{b}}{|x|^{c} + |y|^{d}}$$ does not exist. So I have done the proof for strict inequalities. And the limit only exists when the fractions add up to an integer greater than $1$. I'm not sure how to approach this other than counter examples.","Given $\frac{a}{c} + \frac{b}{d} = 1$ Prove that $$\lim_{(x,y)\rightarrow(0,0)} \frac{|x|^{a}|y|^{b}}{|x|^{c} + |y|^{d}}$$ does not exist. So I have done the proof for strict inequalities. And the limit only exists when the fractions add up to an integer greater than $1$. I'm not sure how to approach this other than counter examples.",,"['limits', 'multivariable-calculus']"
40,Limit involving Zeta and Gamma function,Limit involving Zeta and Gamma function,,Can someone help me evaluate this limit? $$\lim_{x\to +\infty}\frac {\zeta(1+\frac 1x)}{\Gamma(x)}$$ I never came across this kind of limit so I don't even know where to start.,Can someone help me evaluate this limit? $$\lim_{x\to +\infty}\frac {\zeta(1+\frac 1x)}{\Gamma(x)}$$ I never came across this kind of limit so I don't even know where to start.,,"['limits', 'gamma-function', 'riemann-zeta']"
41,"Multivariable limit $\lim_ {{(x,y)} \to {(0,0)}} \frac{xy^2\ln\frac{|x|}{|y|}}{{(x^2+y^2)}^{\frac 12}}$",Multivariable limit,"\lim_ {{(x,y)} \to {(0,0)}} \frac{xy^2\ln\frac{|x|}{|y|}}{{(x^2+y^2)}^{\frac 12}}","I have to prove that the limit $$\lim_ {{(x,y)} \to {(0,0)}} \frac{xy^2\ln\frac{|x|}{|y|}}{{(x^2+y^2)}^{\frac 12}}$$ does not exist. I've tried to find two different paths that show that the limit is divergent, but I couldn't find any. I've also tried to bound it, and it didn't work. Can somebody explain me how to do this? Thank you! PS: This is part of a much larger excercise, I didn't include it because it was irrelevant. I just need to prove that it does not converges to zero.","I have to prove that the limit $$\lim_ {{(x,y)} \to {(0,0)}} \frac{xy^2\ln\frac{|x|}{|y|}}{{(x^2+y^2)}^{\frac 12}}$$ does not exist. I've tried to find two different paths that show that the limit is divergent, but I couldn't find any. I've also tried to bound it, and it didn't work. Can somebody explain me how to do this? Thank you! PS: This is part of a much larger excercise, I didn't include it because it was irrelevant. I just need to prove that it does not converges to zero.",,"['limits', 'multivariable-calculus']"
42,Factorial limit from gamma function calculation,Factorial limit from gamma function calculation,,"I want to show that $$\lim_{n\rightarrow\infty}\dfrac{\Gamma\left(n+\frac12\right)}{\sqrt{n}\Gamma(n)}=1$$ Using the formula for $\Gamma\left(n+\frac12\right)$ here , it reduces to $$\lim_{n\rightarrow\infty}\dfrac{(2n)!\sqrt{\pi}}{2^{2n}\sqrt{n}(n-1)!n!}=1$$ How to prove that?","I want to show that $$\lim_{n\rightarrow\infty}\dfrac{\Gamma\left(n+\frac12\right)}{\sqrt{n}\Gamma(n)}=1$$ Using the formula for $\Gamma\left(n+\frac12\right)$ here , it reduces to $$\lim_{n\rightarrow\infty}\dfrac{(2n)!\sqrt{\pi}}{2^{2n}\sqrt{n}(n-1)!n!}=1$$ How to prove that?",,"['limits', 'factorial', 'gamma-function']"
43,"Check my workings: Prove the limit $\lim\limits_{x\to -2} (3x^2+4x-2)=2 $ using the $\epsilon,\delta$ definition.",Check my workings: Prove the limit  using the  definition.,"\lim\limits_{x\to -2} (3x^2+4x-2)=2  \epsilon,\delta","Prove the limit $\lim\limits_{x\to -2} (3x^2+4x-2)=2 $ using the $\epsilon,\delta$ definition. Precalculations My goal is to show that for all $\epsilon >0$, there exist a $\delta > 0$, such that   $$0<|x+2|<\delta\ \ \text{implies}\  |3x^2+4x-2-2|<\epsilon$$ $|3x^2+4x-2-2|=|3(x+2)^2-8x-16|$ $=|3(x+2)^2-4(x+2)|$ $\leq3|x+2|^2+4|x+2|$ by triangle inequality $<3\delta^2+4\delta$ Hence, it is sufficient to show that $3\delta^2+4\delta=\epsilon$ Proof For all $\epsilon>0$, choose $\delta=\min\left(\sqrt{\dfrac{\epsilon}{6}},\dfrac{\epsilon}{8}\right)$ $$\begin{align*}0<|x+2|<\delta\ \ \to\ \ &|3x^2+4x-2-2|<3\delta^2+4\delta\\&<3\left(\sqrt{\frac{\epsilon}{6}}\right)^2+4\delta\\&=\frac{\epsilon}{2}+4\delta\\&<\frac{\epsilon}{2}+4\frac{\epsilon}{8}\\&=\frac{\epsilon}{2}+\frac{\epsilon}{2}\\&=\epsilon\end{align*}$$ Therefore proven? Hehe. Not sure this will work or not.  My doubts lies in the steps. Hence, it is sufficient to show that $3\delta^2+4\delta=\epsilon$ choose $\delta=\min\left(\sqrt{\dfrac{\epsilon}{6}},\dfrac{\epsilon}{8}\right)$ And hey, I am looking out for other possible ways to do this question too.","Prove the limit $\lim\limits_{x\to -2} (3x^2+4x-2)=2 $ using the $\epsilon,\delta$ definition. Precalculations My goal is to show that for all $\epsilon >0$, there exist a $\delta > 0$, such that   $$0<|x+2|<\delta\ \ \text{implies}\  |3x^2+4x-2-2|<\epsilon$$ $|3x^2+4x-2-2|=|3(x+2)^2-8x-16|$ $=|3(x+2)^2-4(x+2)|$ $\leq3|x+2|^2+4|x+2|$ by triangle inequality $<3\delta^2+4\delta$ Hence, it is sufficient to show that $3\delta^2+4\delta=\epsilon$ Proof For all $\epsilon>0$, choose $\delta=\min\left(\sqrt{\dfrac{\epsilon}{6}},\dfrac{\epsilon}{8}\right)$ $$\begin{align*}0<|x+2|<\delta\ \ \to\ \ &|3x^2+4x-2-2|<3\delta^2+4\delta\\&<3\left(\sqrt{\frac{\epsilon}{6}}\right)^2+4\delta\\&=\frac{\epsilon}{2}+4\delta\\&<\frac{\epsilon}{2}+4\frac{\epsilon}{8}\\&=\frac{\epsilon}{2}+\frac{\epsilon}{2}\\&=\epsilon\end{align*}$$ Therefore proven? Hehe. Not sure this will work or not.  My doubts lies in the steps. Hence, it is sufficient to show that $3\delta^2+4\delta=\epsilon$ choose $\delta=\min\left(\sqrt{\dfrac{\epsilon}{6}},\dfrac{\epsilon}{8}\right)$ And hey, I am looking out for other possible ways to do this question too.",,"['limits', 'proof-writing', 'definition', 'solution-verification']"
44,$\lim\limits_{n\to\infty}f\left(\frac{x}{n}\right)=0$ for every $x > 0$. Prove $\lim\limits_{x \to 0}f(x)=0$,for every . Prove,\lim\limits_{n\to\infty}f\left(\frac{x}{n}\right)=0 x > 0 \lim\limits_{x \to 0}f(x)=0,"Function $f: (0, \infty) \to \mathbb{R}$ is continuous. For every positive $x$ we have $\lim\limits_{n\to\infty}f\left(\frac{x}{n}\right)=0$ . Prove that $\lim\limits_{x \to 0}f(x)=0$ . I have tried to deduce something from definition of continuity, but with no effect.","Function is continuous. For every positive we have . Prove that . I have tried to deduce something from definition of continuity, but with no effect.","f: (0, \infty) \to \mathbb{R} x \lim\limits_{n\to\infty}f\left(\frac{x}{n}\right)=0 \lim\limits_{x \to 0}f(x)=0","['limits', 'continuity']"
45,Evaluate $\lim\limits \frac{x-\sin\sin\cdots\sin x}{x^3}.$ [duplicate],Evaluate  [duplicate],\lim\limits \frac{x-\sin\sin\cdots\sin x}{x^3}.,"This question already has answers here : Evaluating limit (iterated sine function) (4 answers) Closed 5 years ago . Problem Evaluate $$\lim\limits_{x \to 0} \frac{x-\sin\sin\cdots\sin x}{x^3},$$where $\sin\sin\cdots\sin x$ denotes n-fold composite sine function. Solution Consider applying Taylor's Formula with 3-order at $x=0$. We may obtain $$f_n(x)=\sin\sin\cdots\sin x=x-\frac{n}{6}x^3+\mathcal{O}(x^3).\tag{*}$$ To prove this, we can apply the mathematical induction. Let $n=1,$ then $$f(x)=\sin x=x-\frac{1}{6}x^3+\mathcal{O}(x^3),$$  It's true and shows that $(*)$ holds for $n=1$. Assume that $(*)$ holds for $n=k$. Then $$\begin{align*}f_{k+1}(x)&=\sin(f_k(x))\\&=x-\frac{k}{6}x^3+\mathcal{O}(x^3)-\frac{1}{6}\left(x-\frac{k}{6}x^3+\mathcal{O}(x^3)\right)^3+\mathcal{O}(x^3)\\&=x-\frac{k+1}{6}x^3+\mathcal{O}(x^3)\end{align*}.$$ This shows that $(*)$ holds for $n=k+1$. As a result, $(*)$ holds for all $n=1,2,\cdots.$ Now,let's go back to deal with the problem. $$\lim\limits_{x \to 0} \frac{x-\sin\sin\cdots\sin x}{x^3}=\lim\limits_{x \to 0} \dfrac{x-\left(x-\dfrac{n}{6}x^3+\mathcal{O}(x^3)\right)}{x^3}=\frac{n}{6}.$$ Please correct me if I'm wrong. Hope to see other solutions. Thanks.","This question already has answers here : Evaluating limit (iterated sine function) (4 answers) Closed 5 years ago . Problem Evaluate $$\lim\limits_{x \to 0} \frac{x-\sin\sin\cdots\sin x}{x^3},$$where $\sin\sin\cdots\sin x$ denotes n-fold composite sine function. Solution Consider applying Taylor's Formula with 3-order at $x=0$. We may obtain $$f_n(x)=\sin\sin\cdots\sin x=x-\frac{n}{6}x^3+\mathcal{O}(x^3).\tag{*}$$ To prove this, we can apply the mathematical induction. Let $n=1,$ then $$f(x)=\sin x=x-\frac{1}{6}x^3+\mathcal{O}(x^3),$$  It's true and shows that $(*)$ holds for $n=1$. Assume that $(*)$ holds for $n=k$. Then $$\begin{align*}f_{k+1}(x)&=\sin(f_k(x))\\&=x-\frac{k}{6}x^3+\mathcal{O}(x^3)-\frac{1}{6}\left(x-\frac{k}{6}x^3+\mathcal{O}(x^3)\right)^3+\mathcal{O}(x^3)\\&=x-\frac{k+1}{6}x^3+\mathcal{O}(x^3)\end{align*}.$$ This shows that $(*)$ holds for $n=k+1$. As a result, $(*)$ holds for all $n=1,2,\cdots.$ Now,let's go back to deal with the problem. $$\lim\limits_{x \to 0} \frac{x-\sin\sin\cdots\sin x}{x^3}=\lim\limits_{x \to 0} \dfrac{x-\left(x-\dfrac{n}{6}x^3+\mathcal{O}(x^3)\right)}{x^3}=\frac{n}{6}.$$ Please correct me if I'm wrong. Hope to see other solutions. Thanks.",,"['limits', 'proof-verification', 'taylor-expansion']"
46,$\lim \limits_{x \rightarrow 0 }\lfloor \cos(x) \rfloor= ?$,,\lim \limits_{x \rightarrow 0 }\lfloor \cos(x) \rfloor= ?,"I was doing this question, i think the answer should be $0$ , My argument is as follows suppose $x \rightarrow 0$ from right side, that is $x $ approaches $0$ from positive side meaning $x$ is decreasing and therefore $\cos(x) $ should increase as $\cos(x)$ is a decreasing function , but $\cos(x) < 1$ hence their integral part will be $0$ , similarly for the negative case the $\cos(x)$ plays the same role as $\cos(x)$ is an even function. but at the same time i doubt what happens when $x = 0$,there $\cos(x) = 1$ and hence integer part will be $1$. Please correct me if i am wrong.","I was doing this question, i think the answer should be $0$ , My argument is as follows suppose $x \rightarrow 0$ from right side, that is $x $ approaches $0$ from positive side meaning $x$ is decreasing and therefore $\cos(x) $ should increase as $\cos(x)$ is a decreasing function , but $\cos(x) < 1$ hence their integral part will be $0$ , similarly for the negative case the $\cos(x)$ plays the same role as $\cos(x)$ is an even function. but at the same time i doubt what happens when $x = 0$,there $\cos(x) = 1$ and hence integer part will be $1$. Please correct me if i am wrong.",,['limits']
47,Algorithm for finding limits,Algorithm for finding limits,,"Programs like Mathematica or Wolfram Alpha are able to calculate very complex limits with apparent ease. I'm trying to make my own program to compute limits. I've searched all over, but haven't found anything about how they do it. What algorithm can I use to compute limits?","Programs like Mathematica or Wolfram Alpha are able to calculate very complex limits with apparent ease. I'm trying to make my own program to compute limits. I've searched all over, but haven't found anything about how they do it. What algorithm can I use to compute limits?",,['limits']
48,Interchanging two limits in a proof. Is it legitimate?,Interchanging two limits in a proof. Is it legitimate?,,"Landau defines $$\log x = \lim\limits_{k \to 0} {x^k-1 \over k}$$ I wanted to prove the elemental properties of the logaritm with this, namely: $\log xy = \log x +\log y $ $\log x^a = a\log x  $ $1-\dfrac 1 x\leq\log x \leq x-1  $ $\lim\limits_{x\to 0}\dfrac{\log(1+x)}{x}=1 $ $\dfrac{d}{dx}\log x = \dfrac 1 x$ I proved them all, however, in the last case I did this $$\eqalign{   & \frac{d}{dx}\log x = \lim \limits_{h \to 0} \frac{\log \left( x + h \right) - \log x}{h}  \cr    &  = \lim\limits_{h \to 0} \lim \limits_{k \to 0} \frac{\left( x + h \right)^k - x^k}{kh}  \cr    &  = \lim \limits_{k \to 0} \frac{1}{k}\lim \limits_{h \to 0} \frac{\left( x + h \right)^k - x^k}{h}  \cr    &  = \lim \limits_{k \to 0} \frac{1}{k}\lim \limits_{h \to 0} kx^{k - 1} = \lim \limits_{k \to 0} \lim \limits_{h \to 0} x^{k - 1} = x^{ - 1} } $$ Since I'm not familiar with multivariable calculus, I don't know how to justify this. What could work here?","Landau defines $$\log x = \lim\limits_{k \to 0} {x^k-1 \over k}$$ I wanted to prove the elemental properties of the logaritm with this, namely: $\log xy = \log x +\log y $ $\log x^a = a\log x  $ $1-\dfrac 1 x\leq\log x \leq x-1  $ $\lim\limits_{x\to 0}\dfrac{\log(1+x)}{x}=1 $ $\dfrac{d}{dx}\log x = \dfrac 1 x$ I proved them all, however, in the last case I did this $$\eqalign{   & \frac{d}{dx}\log x = \lim \limits_{h \to 0} \frac{\log \left( x + h \right) - \log x}{h}  \cr    &  = \lim\limits_{h \to 0} \lim \limits_{k \to 0} \frac{\left( x + h \right)^k - x^k}{kh}  \cr    &  = \lim \limits_{k \to 0} \frac{1}{k}\lim \limits_{h \to 0} \frac{\left( x + h \right)^k - x^k}{h}  \cr    &  = \lim \limits_{k \to 0} \frac{1}{k}\lim \limits_{h \to 0} kx^{k - 1} = \lim \limits_{k \to 0} \lim \limits_{h \to 0} x^{k - 1} = x^{ - 1} } $$ Since I'm not familiar with multivariable calculus, I don't know how to justify this. What could work here?",,"['limits', 'multivariable-calculus', 'proof-writing']"
49,Prove $n \rightarrow 2^+$ as $a \rightarrow 0$,Prove  as,n \rightarrow 2^+ a \rightarrow 0,"While studying solids of revolution at college, I came across a problem related to physics that seems to have an answer difficult to prove mathematically, which I have not been able to obtain. Motivation Consider the surface $z=|x|^n+|y|^n$ for $0 \le z \le h$ and $n \ge 1$ . If this surface were a physical object standing in a table, it is very clear that for values of $n \approx 1$ it will be in an unstable equilibrium, while at great values, such as $n=10$ it will be at an stable equilibrium, that is, the object may or may not tumble/fall under any force $F>0$ depending on the value of $n$ . The question is then at which value of $n$ does it transition from unstable to stable? $n=1.5$ and $n=10$ Problem statement From the motivating problem I was able to obtain the equation below. Solving for $n$ as $a \rightarrow 0$ gives the answer, for any constant $c>0$ in the physical problem, $c$ is the z-coordinate of the center of mass of the object and $a$ is associated with at which point it will tumble. Taking $a \rightarrow 0$ provides a situation in which it may fall if moved by any infinitesimal amount, which is an unstable equilibrium $$\left (c-a^n \right )na^{n-2}=1$$ However, there seems to be no feasible way to isolate $n$ in order to apply the limit $a \rightarrow 0$ . Using Wolfram Alpha, it seems the value of $n$ approaches $2$ by using $a \approx 0$ , but I see no way to prove it.","While studying solids of revolution at college, I came across a problem related to physics that seems to have an answer difficult to prove mathematically, which I have not been able to obtain. Motivation Consider the surface for and . If this surface were a physical object standing in a table, it is very clear that for values of it will be in an unstable equilibrium, while at great values, such as it will be at an stable equilibrium, that is, the object may or may not tumble/fall under any force depending on the value of . The question is then at which value of does it transition from unstable to stable? and Problem statement From the motivating problem I was able to obtain the equation below. Solving for as gives the answer, for any constant in the physical problem, is the z-coordinate of the center of mass of the object and is associated with at which point it will tumble. Taking provides a situation in which it may fall if moved by any infinitesimal amount, which is an unstable equilibrium However, there seems to be no feasible way to isolate in order to apply the limit . Using Wolfram Alpha, it seems the value of approaches by using , but I see no way to prove it.",z=|x|^n+|y|^n 0 \le z \le h n \ge 1 n \approx 1 n=10 F>0 n n n=1.5 n=10 n a \rightarrow 0 c>0 c a a \rightarrow 0 \left (c-a^n \right )na^{n-2}=1 n a \rightarrow 0 n 2 a \approx 0,"['limits', 'physics']"
50,Infinite sum of Wiener processes,Infinite sum of Wiener processes,,"Context There are two different Wiener processes $W_t$ and $V_t$ . It's known that they are independent. Additionally, we are given with third Wiener process $B_t$ that is given by the formula $$B_t = aW_t+bV_t, \quad \quad a^2 + b^2= 1.$$ Problem Find the limit in $L^2$ of $$S_n = \sum_{i=1}^n\left[B_{it/n} - B_{(i-1)t/n}\right]\left[V_{it/n} - V_{(i-1)t/n}\right]$$ as $n$ tends to infinity. My ideas I assume that this is the type of task where we need to calculate the expected value and the variance. As the latter tends to $0$ (it should), we can say that the desired limit is the expected value. The issue is that it's very overextended work to calculate the $E(S_n)$ . Let $W_{it/n} = X_i$ and $V_{it/n} = Y_i$ . We have $$\Bbb E(S_n) = \Bbb E\sum_{i=1}^n [aX_i + bY_i - aX_{i-1} - bY_{i-1}][Y_{i} - Y_{i-1}]$$ which can be written as $$\sum \Bbb E\bigg( aX_iY_i + bY_i^2 - aX_{i-1}Y_i - bY_{i-1}Y_i   - aX_iY_{i-1} - bY_iY_{i-1} + aX_{i-1}Y_{i-1} + bY_{i-1}^2\bigg).$$ Next calculations confuse me (what is $\Bbb E(X_i Y_{i-1})$ ?) Is it zero? And the main question how to calculate the variance ? If I'm not mistaken, $\Bbb E(S_n) = nb \to \infty$ , so we don't need variance. Am I right?","Context There are two different Wiener processes and . It's known that they are independent. Additionally, we are given with third Wiener process that is given by the formula Problem Find the limit in of as tends to infinity. My ideas I assume that this is the type of task where we need to calculate the expected value and the variance. As the latter tends to (it should), we can say that the desired limit is the expected value. The issue is that it's very overextended work to calculate the . Let and . We have which can be written as Next calculations confuse me (what is ?) Is it zero? And the main question how to calculate the variance ? If I'm not mistaken, , so we don't need variance. Am I right?","W_t V_t B_t B_t = aW_t+bV_t, \quad \quad a^2 + b^2= 1. L^2 S_n = \sum_{i=1}^n\left[B_{it/n} - B_{(i-1)t/n}\right]\left[V_{it/n} - V_{(i-1)t/n}\right] n 0 E(S_n) W_{it/n} = X_i V_{it/n} = Y_i \Bbb E(S_n) = \Bbb E\sum_{i=1}^n [aX_i + bY_i - aX_{i-1} - bY_{i-1}][Y_{i} - Y_{i-1}] \sum \Bbb E\bigg( aX_iY_i + bY_i^2 - aX_{i-1}Y_i - bY_{i-1}Y_i   - aX_iY_{i-1} - bY_iY_{i-1} + aX_{i-1}Y_{i-1} + bY_{i-1}^2\bigg). \Bbb E(X_i Y_{i-1}) \Bbb E(S_n) = nb \to \infty","['limits', 'stochastic-processes', 'stochastic-calculus']"
51,how to prove the integral converges,how to prove the integral converges,,"Suppose $$I=\lim_{x\rightarrow\infty}\lim_{b\searrow0}\int_{b}^{x}{g(y)dy}$$ exists and is finite, where $g$ is a continuous function from $\mathbb{R}^{+}$ to $\mathbb{R}$. Prove $$\lim_{x\rightarrow\infty,b\searrow 0}{\int_{b}^{x}{g(y)dy}}$$ exists and equals $I$. Can someone give me hints? I do not know where to start it.","Suppose $$I=\lim_{x\rightarrow\infty}\lim_{b\searrow0}\int_{b}^{x}{g(y)dy}$$ exists and is finite, where $g$ is a continuous function from $\mathbb{R}^{+}$ to $\mathbb{R}$. Prove $$\lim_{x\rightarrow\infty,b\searrow 0}{\int_{b}^{x}{g(y)dy}}$$ exists and equals $I$. Can someone give me hints? I do not know where to start it.",,['limits']
52,Need help with Limit solving,Need help with Limit solving,,How do I evaluate this limit? $$\lim_{x \to 0}\frac{\cos\sin\tan x - \cos\tan\sin x}{\cosh\arcsin x + \cosh\sin x - 2\cosh x}$$ I don't really have ideas on how to start. I think L'Hopital rule does not help in this case.,How do I evaluate this limit? $$\lim_{x \to 0}\frac{\cos\sin\tan x - \cos\tan\sin x}{\cosh\arcsin x + \cosh\sin x - 2\cosh x}$$ I don't really have ideas on how to start. I think L'Hopital rule does not help in this case.,,['limits']
53,How can I compute this limit? [duplicate],How can I compute this limit? [duplicate],,"This question already has answers here : Evaluating $\lim\limits_{n\to\infty} e^{-n} \sum\limits_{k=0}^{n} \frac{n^k}{k!}$ (9 answers) Closed 10 years ago . I have to compute $$ \lim_{n\to\infty} \exp(-n)\left(1+n+\frac{n^2}{2}+\ldots+\frac{n^n}{n!} \right)$$ I think the value is 1, but i don't know how to proof this. Do I have to estimate the remainder of the Taylor expansion of the exponential function? Does anyone have a hint for me? So do I have to show that $$ R_{n} (x) = \int_{-\infty}^{x} \frac{\exp(t)}{n!} \left(x-t\right)^{n}\mathrm{d}t $$ goes to 0? Or is there an easier way?","This question already has answers here : Evaluating $\lim\limits_{n\to\infty} e^{-n} \sum\limits_{k=0}^{n} \frac{n^k}{k!}$ (9 answers) Closed 10 years ago . I have to compute $$ \lim_{n\to\infty} \exp(-n)\left(1+n+\frac{n^2}{2}+\ldots+\frac{n^n}{n!} \right)$$ I think the value is 1, but i don't know how to proof this. Do I have to estimate the remainder of the Taylor expansion of the exponential function? Does anyone have a hint for me? So do I have to show that $$ R_{n} (x) = \int_{-\infty}^{x} \frac{\exp(t)}{n!} \left(x-t\right)^{n}\mathrm{d}t $$ goes to 0? Or is there an easier way?",,"['limits', 'taylor-expansion', 'exponential-function']"
54,Vector space define by limit,Vector space define by limit,,"Prove that the space of all $f \in \mathcal{C}(\mathbb{R},\mathbb{R})$ such that $$\lim_{n\rightarrow \infty }\frac{1}{n}\int_{-n}^{n}|f(x)|^2dx<+\infty$$ is a vector space over $\mathbb R$ . I have tried to show by Minkowski inequality that the sum of two elements of that space also belongs to that space but since the sequence is not monotone, it didn't work. I also tried to show by Cauchy condition but, again, it didn't work. So, I don't know what to do now.","Prove that the space of all such that is a vector space over . I have tried to show by Minkowski inequality that the sum of two elements of that space also belongs to that space but since the sequence is not monotone, it didn't work. I also tried to show by Cauchy condition but, again, it didn't work. So, I don't know what to do now.","f \in \mathcal{C}(\mathbb{R},\mathbb{R}) \lim_{n\rightarrow \infty }\frac{1}{n}\int_{-n}^{n}|f(x)|^2dx<+\infty \mathbb R","['limits', 'vector-spaces', 'normed-spaces']"
55,A Family of Limits Leading to an Interesting Function,A Family of Limits Leading to an Interesting Function,,"A while back I got very interested in limits of the form $$ \lim_{n\to\infty} (2A)^n \left (A-\underbrace{\sqrt{a+\sqrt{a+\ldots\sqrt{a+z}}}}_{n\textrm{ radicals}}  \right )=f_a^{-1}(z) $$ Where $A$ is the positive solution of $A^2=a+A$ . As notated, the limit can be used to define a function with some rather interesting properties. I explore some of them here . In particular, the function becomes trigonometric in the case $a=2$ , and related to the golden ratio in the case $a=1$ . These values also correspond to notable properties of the function. In particular, the roots of the function have some very interesting behavior that seems to become fractal. Does anyone know if this limit has been studied before? It seems to be related to the Mandelbrot set, inasmuch as the iterated radical is the inverse operation of the Mandelbrot iteration, though I haven't looked into this connection very much. Any information about other research on this subject or related fields of study would be greatly appreciated.","A while back I got very interested in limits of the form Where is the positive solution of . As notated, the limit can be used to define a function with some rather interesting properties. I explore some of them here . In particular, the function becomes trigonometric in the case , and related to the golden ratio in the case . These values also correspond to notable properties of the function. In particular, the roots of the function have some very interesting behavior that seems to become fractal. Does anyone know if this limit has been studied before? It seems to be related to the Mandelbrot set, inasmuch as the iterated radical is the inverse operation of the Mandelbrot iteration, though I haven't looked into this connection very much. Any information about other research on this subject or related fields of study would be greatly appreciated.","
\lim_{n\to\infty} (2A)^n \left (A-\underbrace{\sqrt{a+\sqrt{a+\ldots\sqrt{a+z}}}}_{n\textrm{ radicals}}  \right )=f_a^{-1}(z)
 A A^2=a+A a=2 a=1","['limits', 'reference-request', 'fractals', 'analytic-continuation']"
56,Solution verification: evaluate $\lim\limits_{n \to \infty}\frac{1^{\lambda n}+2^{\lambda n}+\cdots+n^{\lambda n}}{n^{\lambda n}}$ where $\lambda>0.$,Solution verification: evaluate  where,\lim\limits_{n \to \infty}\frac{1^{\lambda n}+2^{\lambda n}+\cdots+n^{\lambda n}}{n^{\lambda n}} \lambda>0.,"Problem Evaluate $$\lim_{n \to \infty}\frac{1^{\lambda n}+2^{\lambda n}+\cdots+n^{\lambda n}}{n^{\lambda n}}$$ where $\lambda>0.$ Solution Denote $$S_n:=\sum_{k=1}^{n}\left(\frac{k}{n}\right)^{\lambda n}=\sum_{k=0}^{n-1}\left(1-\frac{k}{n}\right)^{\lambda n}.\tag1$$ On one hand, we choose some $p\in \mathbb{N+}$ , fix it, and let $n>p+1$ . Then $$S_n \geq \sum_{k=0}^{p}\left(1-\frac{k}{n}\right)^{\lambda n}.\tag2$$ Let $n \to \infty$ within $(2)$ . We obtain $$\liminf_{n \to \infty}S_n\geq \liminf_{n \to \infty} \sum_{k=0}^{p}\left(1-\frac{k}{n}\right)^{\lambda n}=\sum_{k=0}^{p}\liminf_{n \to \infty}\left(1-\frac{k}{n}\right)^{\lambda n}=\sum_{k=0}^{p}e^{-\lambda k}.\tag3$$ Notice that $(3)$ holds for all $p$ .  We may let $p \to \infty$ and obtain $$\liminf_{n \to \infty}S_n\geq \sum_{k=0}^{\infty}e^{-\lambda k}=\frac{e^{\lambda}}{e^{\lambda}-1}.\tag4$$ On the other hand, from $1+x\leq e^x$ we may derive $\left(1-\dfrac{k}{n}\right)^{\lambda n}\leq e^{-\lambda k}.$ Thus $$S_n \leq \sum_{k=0}^{n-1}e^{-\lambda k}.\tag5$$ Let $n \to \infty$ within $(5)$ . We obtain $$\limsup_{n \to \infty}S_n \leq \limsup_{n \to \infty}\sum_{k=0}^{n-1}e^{-\lambda k}=\frac{e^{\lambda}}{e^{\lambda}-1}.\tag6$$ See $(4)$ and $(6)$ . We may conclude $$\frac{e^{\lambda}}{e^{\lambda}-1}\leq \liminf_{n \to \infty}S_n\leq \limsup_{n \to \infty}S_n\leq \frac{e^{\lambda}}{e^{\lambda}-1},\tag7$$ which implies $$\lim_{n \to \infty}S_n=\frac{e^{\lambda}}{e^{\lambda}-1}.\tag8$$ Please correct me if I'm wrong. Thanks.","Problem Evaluate where Solution Denote On one hand, we choose some , fix it, and let . Then Let within . We obtain Notice that holds for all .  We may let and obtain On the other hand, from we may derive Thus Let within . We obtain See and . We may conclude which implies Please correct me if I'm wrong. Thanks.","\lim_{n \to \infty}\frac{1^{\lambda n}+2^{\lambda n}+\cdots+n^{\lambda n}}{n^{\lambda n}} \lambda>0. S_n:=\sum_{k=1}^{n}\left(\frac{k}{n}\right)^{\lambda n}=\sum_{k=0}^{n-1}\left(1-\frac{k}{n}\right)^{\lambda n}.\tag1 p\in \mathbb{N+} n>p+1 S_n \geq \sum_{k=0}^{p}\left(1-\frac{k}{n}\right)^{\lambda n}.\tag2 n \to \infty (2) \liminf_{n \to \infty}S_n\geq \liminf_{n \to \infty} \sum_{k=0}^{p}\left(1-\frac{k}{n}\right)^{\lambda n}=\sum_{k=0}^{p}\liminf_{n \to \infty}\left(1-\frac{k}{n}\right)^{\lambda n}=\sum_{k=0}^{p}e^{-\lambda k}.\tag3 (3) p p \to \infty \liminf_{n \to \infty}S_n\geq \sum_{k=0}^{\infty}e^{-\lambda k}=\frac{e^{\lambda}}{e^{\lambda}-1}.\tag4 1+x\leq e^x \left(1-\dfrac{k}{n}\right)^{\lambda n}\leq e^{-\lambda k}. S_n \leq \sum_{k=0}^{n-1}e^{-\lambda k}.\tag5 n \to \infty (5) \limsup_{n \to \infty}S_n \leq \limsup_{n \to \infty}\sum_{k=0}^{n-1}e^{-\lambda k}=\frac{e^{\lambda}}{e^{\lambda}-1}.\tag6 (4) (6) \frac{e^{\lambda}}{e^{\lambda}-1}\leq \liminf_{n \to \infty}S_n\leq \limsup_{n \to \infty}S_n\leq \frac{e^{\lambda}}{e^{\lambda}-1},\tag7 \lim_{n \to \infty}S_n=\frac{e^{\lambda}}{e^{\lambda}-1}.\tag8","['limits', 'proof-verification', 'limsup-and-liminf']"
57,"Movement time of object with constant jerk, limited acceleration and velocity","Movement time of object with constant jerk, limited acceleration and velocity",,"A product is initially at rest on a conveyor belt: The initial conditions of the product can be described as follows:$$x_i=0$$ $$v_i=0$$ $$a_i=0$$$$j_i = j⋆ $$. The product will be moved forward distance of exactly Δx (known) to position it under the conveyor belt: When the movement is complete, the product will be sprayed for a few moments, so it must come to a complete rest at this point: The conveyor, which must move the product into position, will complete the movement in the minimum time possible. The conveyor's control system will apply a constant jerk of j⋆ (known), -j⋆ (known) or zero. However, the conveyor must obey these limits: It has a max velocity of v⋆ (known), a max acceleration of a⋆ (known), a max deceleration of d⋆ (known). Assume the product does not slide or slip on the conveyor. How long will it take the product to complete the movement of Δx ? Side Note: At first glance, it seems as though the problem can be solved by a simple application of the kinematic equation for one-dimensional motion with constant jerk (and its derivatives): $$x = x_0 + v_0 t + \frac{1}{2} a_0 t^2 + \frac{1}{6}jt^3$$ However, the limits v⋆ , a⋆ and d⋆ pose a major problem. For some values of Δx , the conveyor may not even reach max velocity, max acceleration, or max deceleration.  Therefore, there will be 8 possible scenarios in which different combinations of these limits are reached.  To assist with this part of the problem, I have created a decision chart that illustrates the different possible scenarios: Conveyor Limit Decision Chart As Δx gets smaller and smaller, the possibilities range from all three limits reached... ... to no limits reached... ... making it hard to describe the time passed during the movement with a single function.  I am stumped at this point- how do I predict which limits will be met and which will not?","A product is initially at rest on a conveyor belt: The initial conditions of the product can be described as follows:$$x_i=0$$ $$v_i=0$$ $$a_i=0$$$$j_i = j⋆ $$. The product will be moved forward distance of exactly Δx (known) to position it under the conveyor belt: When the movement is complete, the product will be sprayed for a few moments, so it must come to a complete rest at this point: The conveyor, which must move the product into position, will complete the movement in the minimum time possible. The conveyor's control system will apply a constant jerk of j⋆ (known), -j⋆ (known) or zero. However, the conveyor must obey these limits: It has a max velocity of v⋆ (known), a max acceleration of a⋆ (known), a max deceleration of d⋆ (known). Assume the product does not slide or slip on the conveyor. How long will it take the product to complete the movement of Δx ? Side Note: At first glance, it seems as though the problem can be solved by a simple application of the kinematic equation for one-dimensional motion with constant jerk (and its derivatives): $$x = x_0 + v_0 t + \frac{1}{2} a_0 t^2 + \frac{1}{6}jt^3$$ However, the limits v⋆ , a⋆ and d⋆ pose a major problem. For some values of Δx , the conveyor may not even reach max velocity, max acceleration, or max deceleration.  Therefore, there will be 8 possible scenarios in which different combinations of these limits are reached.  To assist with this part of the problem, I have created a decision chart that illustrates the different possible scenarios: Conveyor Limit Decision Chart As Δx gets smaller and smaller, the possibilities range from all three limits reached... ... to no limits reached... ... making it hard to describe the time passed during the movement with a single function.  I am stumped at this point- how do I predict which limits will be met and which will not?",,"['limits', 'physics', 'kinematics']"
58,Pythagoras tree bounding size,Pythagoras tree bounding size,,"The Pythagoras tree is a fractal generated by squares. For each square, two new smaller squares are constructed and connected by their corners to the original square. The angle of the triangle formed can be varied. The problem is finding the size of the bounding box (to a certain precision) for this fractal. The graph is essentially a rooted tree. Each node represents a square with a certain size and orientation. The tree is infinitely deep. For a certain precision, the problem is easily solved with a computer using branch and bound. I want to know if there are any estimates or hard limits on the size. Any bounds on the area of the fractal are also appreciated. The isosceles right triangle case gives a nice bounding box of $6 \times 4$ , which can be calculated easily with a geometric series. Other cases are more difficult for me (the maximal tree paths follow a zigzag pattern - for a while).","The Pythagoras tree is a fractal generated by squares. For each square, two new smaller squares are constructed and connected by their corners to the original square. The angle of the triangle formed can be varied. The problem is finding the size of the bounding box (to a certain precision) for this fractal. The graph is essentially a rooted tree. Each node represents a square with a certain size and orientation. The tree is infinitely deep. For a certain precision, the problem is easily solved with a computer using branch and bound. I want to know if there are any estimates or hard limits on the size. Any bounds on the area of the fractal are also appreciated. The isosceles right triangle case gives a nice bounding box of , which can be calculated easily with a geometric series. Other cases are more difficult for me (the maximal tree paths follow a zigzag pattern - for a while).",6 \times 4,"['limits', 'trees', 'fractals']"
59,Show $\lim\limits_{n\to\infty} n \ln\left(1-\frac{1}n\right) = -1$,Show,\lim\limits_{n\to\infty} n \ln\left(1-\frac{1}n\right) = -1,Could you help me show that $$\lim\limits_{n\to\infty} n \ln \left({1-\frac{1}n} \right) = -1 ?$$,Could you help me show that $$\lim\limits_{n\to\infty} n \ln \left({1-\frac{1}n} \right) = -1 ?$$,,['limits']
60,Is $\lim_{k\to\infty}\frac{\sum_{n=1}^{k} 2^{2\times3^{n}}}{2^{2\times3^{k}}}=1$?,Is ?,\lim_{k\to\infty}\frac{\sum_{n=1}^{k} 2^{2\times3^{n}}}{2^{2\times3^{k}}}=1,"Look at this limit. I think, this equality is true.But I'm not sure. $$\lim_{k\to\infty}\frac{\sum_{n=1}^{k} 2^{2\times3^{n}}}{2^{2\times3^{k}}}=1$$ For example, $k=3$, the ratio is $1.000000000014$ Is this limit mathematically correct ?","Look at this limit. I think, this equality is true.But I'm not sure. $$\lim_{k\to\infty}\frac{\sum_{n=1}^{k} 2^{2\times3^{n}}}{2^{2\times3^{k}}}=1$$ For example, $k=3$, the ratio is $1.000000000014$ Is this limit mathematically correct ?",,[]
61,Limit. $\lim_{n \to \infty}\frac{1^p+2^p+\ldots+n^{p}}{n^{p+1}}$. [duplicate],Limit. . [duplicate],\lim_{n \to \infty}\frac{1^p+2^p+\ldots+n^{p}}{n^{p+1}},This question already has answers here : Evaluating $\lim\limits_{n\to\infty} \left(\frac{1^p+2^p+3^p + \cdots + n^p}{n^p} - \frac{n}{p+1}\right)$ (7 answers) Closed 11 years ago . Have you any idea how to find the limit of the following sum:  $$\lim_{n \to \infty}\frac{1^p+2^p+\ldots+n^{p}}{n^{p+1}}.$$ Stolz-Cesaro? any more ideas?,This question already has answers here : Evaluating $\lim\limits_{n\to\infty} \left(\frac{1^p+2^p+3^p + \cdots + n^p}{n^p} - \frac{n}{p+1}\right)$ (7 answers) Closed 11 years ago . Have you any idea how to find the limit of the following sum:  $$\lim_{n \to \infty}\frac{1^p+2^p+\ldots+n^{p}}{n^{p+1}}.$$ Stolz-Cesaro? any more ideas?,,['limits']
62,I need help to advance in the resolution of that limit: $ \lim_{n \to \infty}{\sqrt[n]{\frac{n!}{n^n}}} $,I need help to advance in the resolution of that limit:, \lim_{n \to \infty}{\sqrt[n]{\frac{n!}{n^n}}} ,how I can continue this limit resolution? The limit is: $$ \lim_{n \to \infty}{\sqrt[n]{\frac{n!}{n^n}}} $$ This is that I have done: I apply this test: $ \lim_{n \to \infty}{\sqrt[n]{a_n}} = \frac{a_{n+1}}{a_n} $ Operating and simplifying I arrive to this point: $$ \lim_{n \to \infty}{\frac{n^n}{(n+1)^n}} $$ I've done something wrong? Thanks!,how I can continue this limit resolution? The limit is: $$ \lim_{n \to \infty}{\sqrt[n]{\frac{n!}{n^n}}} $$ This is that I have done: I apply this test: $ \lim_{n \to \infty}{\sqrt[n]{a_n}} = \frac{a_{n+1}}{a_n} $ Operating and simplifying I arrive to this point: $$ \lim_{n \to \infty}{\frac{n^n}{(n+1)^n}} $$ I've done something wrong? Thanks!,,"['limits', 'factorial', 'radicals']"
63,A question about limit $\lim_{n\to\infty} {n \choose n/2}/2^n$ [duplicate],A question about limit  [duplicate],\lim_{n\to\infty} {n \choose n/2}/2^n,"This question already has answers here : Determine $\lim\limits_{n \to \infty}{{n} \choose {\frac{n}{2}}}\frac{1}{2^n}$, where each $n$ is even (4 answers) Closed 4 years ago . My question is: What is the result of this limit: $\displaystyle \lim_{n \to +\infty} \frac{{n \choose n/2}}{2^n}=$ ?","This question already has answers here : Determine $\lim\limits_{n \to \infty}{{n} \choose {\frac{n}{2}}}\frac{1}{2^n}$, where each $n$ is even (4 answers) Closed 4 years ago . My question is: What is the result of this limit: $\displaystyle \lim_{n \to +\infty} \frac{{n \choose n/2}}{2^n}=$ ?",,"['limits', 'binomial-coefficients']"
64,Limit to infinity with natural logarithms $\lim_{x\to \infty } \left(\frac{\ln (2 x)}{\ln (x)}\right)^{\ln (x)} $,Limit to infinity with natural logarithms,\lim_{x\to \infty } \left(\frac{\ln (2 x)}{\ln (x)}\right)^{\ln (x)} ,"I found the following problem in my calculus book: Solve: $$\lim_{x\to \infty } \left(\frac{\ln (2 x)}{\ln (x)}\right)^{\ln (x)} $$ I tried to solve it using log rules and l'Hôpital's rule with no success, can someone give me any hints on how to go about this?","I found the following problem in my calculus book: Solve: $$\lim_{x\to \infty } \left(\frac{\ln (2 x)}{\ln (x)}\right)^{\ln (x)} $$ I tried to solve it using log rules and l'Hôpital's rule with no success, can someone give me any hints on how to go about this?",,"['limits', 'logarithms']"
65,Why is the following limit not 1?,Why is the following limit not 1?,,"Consider $$\lim_{n\to\infty} f(n)=\lim_{n\to\infty}\frac{(n!)!}{(n!-n)!}\tag{1}$$ For large $n$ , one can ignore $n$ wrt. $n!$ in the denominator. The limiting value should therefore approach $1$ . However, as the plot shows, the limit blows up. Why is that?","Consider For large , one can ignore wrt. in the denominator. The limiting value should therefore approach . However, as the plot shows, the limit blows up. Why is that?",\lim_{n\to\infty} f(n)=\lim_{n\to\infty}\frac{(n!)!}{(n!-n)!}\tag{1} n n n! 1,"['limits', 'approximation', 'factorial']"
66,What am I doing wrong in calculating the following limit?,What am I doing wrong in calculating the following limit?,,$$\lim_{x\to-2} \frac{x+2}{\sqrt{6+x}-2}=\lim_{x\to-2} \frac{1+2/x}{\sqrt{(6/x^2)+(1/x)}-2/x^2}$$ Dividing numerator and denominator by $x \neq0$ $$\frac{1+2/-2}{\sqrt{(6/4)+(1/-2)}-2/4}=\frac{0}{1/2}=0$$ but the limit is $4$ according to Wolfram Alpha?,$$\lim_{x\to-2} \frac{x+2}{\sqrt{6+x}-2}=\lim_{x\to-2} \frac{1+2/x}{\sqrt{(6/x^2)+(1/x)}-2/x^2}$$ Dividing numerator and denominator by $x \neq0$ $$\frac{1+2/-2}{\sqrt{(6/4)+(1/-2)}-2/4}=\frac{0}{1/2}=0$$ but the limit is $4$ according to Wolfram Alpha?,,"['limits', 'radicals']"
67,Evaluate $\lim_{x\to 1^{+}}(\sqrt{x}-1)^{x^2+2x-3}$ using L'Hôpital,Evaluate  using L'Hôpital,\lim_{x\to 1^{+}}(\sqrt{x}-1)^{x^2+2x-3},"I have solved before problems with L’Hopital’s Rule but this one is giving me a headache... Here it is: $$\lim_{x\to 1^{+}}(\sqrt{x}-1)^{x^2+2x-3}$$ I know that first you need to $ \log$ it so you can get the $x^2+2x-3$ upfront and then you find the derivative till it is not longer $0/0$ or $\infty/\infty$, but I am doing all that and still can't find solution. If someone can solved it I would really appreciate it.  Thank you","I have solved before problems with L’Hopital’s Rule but this one is giving me a headache... Here it is: $$\lim_{x\to 1^{+}}(\sqrt{x}-1)^{x^2+2x-3}$$ I know that first you need to $ \log$ it so you can get the $x^2+2x-3$ upfront and then you find the derivative till it is not longer $0/0$ or $\infty/\infty$, but I am doing all that and still can't find solution. If someone can solved it I would really appreciate it.  Thank you",,['limits']
68,Can someone explain this trigonometric limit?,Can someone explain this trigonometric limit?,,I have $$\lim \limits_{x\to 0}  \frac {\tan(2x)}{\sin(x)}$$ and in my case the result is $\frac{2}{1}$ =2 not whether it is right. This is my procedure. $$\lim \limits_{x\to 0}  \frac{\frac {\sin(2x)}{\cos(2x)}}{\frac{\sin(x)}{1}}= \lim \limits_{x\to 0}  {\dfrac {\sin(2x)}{(\cos(2x))(\sin(x))}}=\dfrac{2x\frac {\sin(2x)}{2x}}{\cos(2x)\frac{x\sin(x)}{x}}$$ I separate the limit. $$\frac{\left(\lim \limits_{x\to 0}2x\right) \cdot \left(\lim \limits_{x\to 0}\frac {\sin(2x)}{2x}\right)}{\lim \limits_{x\to 0}\left(\cos(2x)\right)\cdot\left(\lim \limits_{x\to 0}\frac{x\sin(x)}{x}\right)} = \lim \limits_{x\to 0} \dfrac{2x}{x}=\frac{2}{1} =2$$,I have $$\lim \limits_{x\to 0}  \frac {\tan(2x)}{\sin(x)}$$ and in my case the result is $\frac{2}{1}$ =2 not whether it is right. This is my procedure. $$\lim \limits_{x\to 0}  \frac{\frac {\sin(2x)}{\cos(2x)}}{\frac{\sin(x)}{1}}= \lim \limits_{x\to 0}  {\dfrac {\sin(2x)}{(\cos(2x))(\sin(x))}}=\dfrac{2x\frac {\sin(2x)}{2x}}{\cos(2x)\frac{x\sin(x)}{x}}$$ I separate the limit. $$\frac{\left(\lim \limits_{x\to 0}2x\right) \cdot \left(\lim \limits_{x\to 0}\frac {\sin(2x)}{2x}\right)}{\lim \limits_{x\to 0}\left(\cos(2x)\right)\cdot\left(\lim \limits_{x\to 0}\frac{x\sin(x)}{x}\right)} = \lim \limits_{x\to 0} \dfrac{2x}{x}=\frac{2}{1} =2$$,,['limits']
69,Logarithm as limit,Logarithm as limit,,Wolfram's website lists this as a limit representation of the natural log: $$\ln{z} = \lim_{\omega \to \infty} \omega(z^{1/\omega} - 1)$$ Is there a quick proof of this? Thanks,Wolfram's website lists this as a limit representation of the natural log: $$\ln{z} = \lim_{\omega \to \infty} \omega(z^{1/\omega} - 1)$$ Is there a quick proof of this? Thanks,,"['limits', 'logarithms']"
70,Circular reasoning in L'Hopital's rule,Circular reasoning in L'Hopital's rule,,"Suppose we have a function $f(x)$ that satisfies: $$\lim_{x\to\infty}f(x)=L$$ Where $L\in\mathbb{R}$ . Is this true? $$\lim_{x\to\infty}f'(x)=0$$ My approach was simply this: $$\lim_{x\to\infty}f(x)=\lim_{x\to\infty}\frac{xf(x)}{x}=L$$ And applying L'Hospital's rule we have: $$\lim_{x\to\infty}\frac{xf(x)}{x}=\lim_{x\to\infty}\frac{f(x)+xf'(x)}{1}=L$$ $$\lim_{x\to\infty}f(x)+xf'(x)=L+\lim_{x\to\infty}xf'(x)=L$$ And finally: $$\lim_{x\to\infty}xf'(x)=0$$ Now, the only way this is possible is if $\lim_{x\to\infty}f'(x)\neq\infty$ and $\lim_{x\to\infty}f'(x)\neq A\in\mathbb{R}$ , because otherways the $\lim_{x\to\infty}xf'(x)$ would go to infinity. In conclusion, $\lim_{x\to\infty}f'(x)=0$ Is this in any way circular reasoning? I'm especially worried about the part when we apply the L'Hospital's rule.","Suppose we have a function that satisfies: Where . Is this true? My approach was simply this: And applying L'Hospital's rule we have: And finally: Now, the only way this is possible is if and , because otherways the would go to infinity. In conclusion, Is this in any way circular reasoning? I'm especially worried about the part when we apply the L'Hospital's rule.",f(x) \lim_{x\to\infty}f(x)=L L\in\mathbb{R} \lim_{x\to\infty}f'(x)=0 \lim_{x\to\infty}f(x)=\lim_{x\to\infty}\frac{xf(x)}{x}=L \lim_{x\to\infty}\frac{xf(x)}{x}=\lim_{x\to\infty}\frac{f(x)+xf'(x)}{1}=L \lim_{x\to\infty}f(x)+xf'(x)=L+\lim_{x\to\infty}xf'(x)=L \lim_{x\to\infty}xf'(x)=0 \lim_{x\to\infty}f'(x)\neq\infty \lim_{x\to\infty}f'(x)\neq A\in\mathbb{R} \lim_{x\to\infty}xf'(x) \lim_{x\to\infty}f'(x)=0,"['limits', 'derivatives']"
71,Why did the author warn 'Don't do it!' on evaluating the limit of $\lim_{x\to 0} \frac{1-\cos(1-\cos x)}{\sin ^4 x}$ this way?,Why did the author warn 'Don't do it!' on evaluating the limit of  this way?,\lim_{x\to 0} \frac{1-\cos(1-\cos x)}{\sin ^4 x},"This is taken from Differential Calculus by Amit M Agarwal: Evaluate $$\lim_{x\to 0} \frac{1-\cos(1-\cos x)}{\sin ^4 x}$$ The question is quite easy using trigonometric identity viz. $1-\cos x = 2\sin^2\frac{x}{2}$ and then using $\lim_{x\to 0} \frac{\sin x}{x}= 1\,.$ The answer is $\frac{1}{8}\,.$ However, after evaluating the limit, the author cautioned as Don't do it! \begin{align}\lim_{x\to 0} \lim_{x\to 0} \frac{1-\cos(1-\cos x)}{\sin ^4 x} & =\lim_{x\to 0} \frac{1-\cos\left(\frac{1-\cos x}{x^2}\cdot x^2\right)}{x^4}\\ &= \lim_{x\to 0}\frac{1-\cos\left(\frac{x^2}{2}\right)}{x^4}\qquad \left(\textrm{As}\, \lim_{x\to 0}\frac{1-\cos x}{x^2}= \frac{1}{2} \right)\\&= \lim_{x\to 0}\frac{2\sin^2 \frac{x^2}{4}}{\frac{x^4}{16}\times 16}\\&= \frac{1}{8}\qquad \textrm{is wrong although the answer may be correct}\,.\end{align} Where is the 'wrong' in the evaluation? Edit: [...] the limit as $x\to 0$ is taken for a subexpression. That's generally invalid. We can't evaluate a limit inside a limit like that. While evaluating limit of a complicated expression one should not replace a sub-expression by its limit and continue with further calculations. Now, consider these limits: $$\bullet \lim_{x \to 4} \log(2x^{3/2}- 3x^{1/2}-1)$$ my book solves this as: $$\log\; [\lim_{x\to 4} 2 x^{3/2}- \lim_{x\to 4} 3x^{1/2} - \lim_{x\to 4} 1]= 2\log 3$$ Another one: $$\bullet \lim_{x\to 1} \sin(2x^2- x- 1)$$ This is solved as; $$\sin\;[\lim_{x\to 1} 2x^2 \lim_{x\to 1} x- \lim_{x\to 1} 1]= \sin 0= 0$$ The following limits are evaluated by first evaluating the limits of sub-expressions . Do these contradict the statement _ you can't take limit of a sub-expression while evaluating the limit of the whole function_?","This is taken from Differential Calculus by Amit M Agarwal: Evaluate The question is quite easy using trigonometric identity viz. and then using The answer is However, after evaluating the limit, the author cautioned as Don't do it! Where is the 'wrong' in the evaluation? Edit: [...] the limit as is taken for a subexpression. That's generally invalid. We can't evaluate a limit inside a limit like that. While evaluating limit of a complicated expression one should not replace a sub-expression by its limit and continue with further calculations. Now, consider these limits: my book solves this as: Another one: This is solved as; The following limits are evaluated by first evaluating the limits of sub-expressions . Do these contradict the statement _ you can't take limit of a sub-expression while evaluating the limit of the whole function_?","\lim_{x\to 0} \frac{1-\cos(1-\cos x)}{\sin ^4 x} 1-\cos x = 2\sin^2\frac{x}{2} \lim_{x\to 0} \frac{\sin x}{x}= 1\,. \frac{1}{8}\,. \begin{align}\lim_{x\to 0} \lim_{x\to 0} \frac{1-\cos(1-\cos x)}{\sin ^4 x} & =\lim_{x\to 0} \frac{1-\cos\left(\frac{1-\cos x}{x^2}\cdot x^2\right)}{x^4}\\ &= \lim_{x\to 0}\frac{1-\cos\left(\frac{x^2}{2}\right)}{x^4}\qquad \left(\textrm{As}\, \lim_{x\to 0}\frac{1-\cos x}{x^2}= \frac{1}{2} \right)\\&= \lim_{x\to 0}\frac{2\sin^2 \frac{x^2}{4}}{\frac{x^4}{16}\times 16}\\&= \frac{1}{8}\qquad \textrm{is wrong although the answer may be correct}\,.\end{align} x\to 0 \bullet \lim_{x \to 4} \log(2x^{3/2}- 3x^{1/2}-1) \log\; [\lim_{x\to 4} 2 x^{3/2}- \lim_{x\to 4} 3x^{1/2} - \lim_{x\to 4} 1]= 2\log 3 \bullet \lim_{x\to 1} \sin(2x^2- x- 1) \sin\;[\lim_{x\to 1} 2x^2 \lim_{x\to 1} x- \lim_{x\to 1} 1]= \sin 0= 0",[]
72,"How to evaluate $\lim_{ n\to \infty }\frac{a_n}{2^{n-1}}$, if $a_0=0$ and $a_{n+1}=a_n+\sqrt{a_n^2+1}$? [duplicate]","How to evaluate , if  and ? [duplicate]",\lim_{ n\to \infty }\frac{a_n}{2^{n-1}} a_0=0 a_{n+1}=a_n+\sqrt{a_n^2+1},"This question already has an answer here : Convergence of sequence given by $x_1=1$ and $x_{n+1}=x_n+\sqrt{x_n^2+1}$ (1 answer) Closed 7 years ago . Let $a_1,a_2,..,a_n$ be sequence of real numbers such that $a_{n+1}=a_{n}+\sqrt{1+a_n^2}$ and $a_0=0$. How to evaluate $\lim_{ n\to \infty  }\frac{a_n}{2^{n-1}}$ ?","This question already has an answer here : Convergence of sequence given by $x_1=1$ and $x_{n+1}=x_n+\sqrt{x_n^2+1}$ (1 answer) Closed 7 years ago . Let $a_1,a_2,..,a_n$ be sequence of real numbers such that $a_{n+1}=a_{n}+\sqrt{1+a_n^2}$ and $a_0=0$. How to evaluate $\lim_{ n\to \infty  }\frac{a_n}{2^{n-1}}$ ?",,['limits']
73,Find the limit of $\sum\limits_{k=1}^n\left(\sqrt{1+\frac{k}{n^2}}-1\right)$,Find the limit of,\sum\limits_{k=1}^n\left(\sqrt{1+\frac{k}{n^2}}-1\right),"$$\lim_{n\rightarrow\infty}\sum_{k=1}^n\left(\sqrt{1+\frac{k}{n^2}}-1\right)$$ Note that $\forall x\ge 0, \sqrt{x}-1\le\sqrt{1+x}-1\le x$ Then $$\sum_{k=1}^n\left(\sqrt{\frac{k}{n^2}}-1\right)\le S_n\le \sum_{k=1}^n\frac{k}{n^2}=\frac{1}{2}-\frac{1}{2n}$$ How do I evaluate the sum on the left?","$$\lim_{n\rightarrow\infty}\sum_{k=1}^n\left(\sqrt{1+\frac{k}{n^2}}-1\right)$$ Note that $\forall x\ge 0, \sqrt{x}-1\le\sqrt{1+x}-1\le x$ Then $$\sum_{k=1}^n\left(\sqrt{\frac{k}{n^2}}-1\right)\le S_n\le \sum_{k=1}^n\frac{k}{n^2}=\frac{1}{2}-\frac{1}{2n}$$ How do I evaluate the sum on the left?",,"['limits', 'inequality', 'summation', 'radicals']"
74,find this limit $\lim_{x\to0^{+}}\frac{\tan{(\tan{x})}-\tan{(\sin{x})}}{\tan{x}-\sin{x}}$,find this limit,\lim_{x\to0^{+}}\frac{\tan{(\tan{x})}-\tan{(\sin{x})}}{\tan{x}-\sin{x}},"find the limit. $$\lim_{x\to0^{+}}\dfrac{\tan{(\tan{x})}-\tan{(\sin{x})}}{\tan{x}-\sin{x}}$$ my try: $$\tan{x}=x+\dfrac{1}{3}x^3+o(x^3),\sin{x}=x-\dfrac{1}{6}x^3+o(x^3)$$ so $$\tan{(\tan{x})}=\tan{x}+\dfrac{1}{3}(\tan{x})^3+o(\tan^3{x})=x+\dfrac{1}{3}x^3+\dfrac{1}{3}(x+\dfrac{1}{3}x^3)^3+o(x^3)$$ $$\tan{(\sin{x})}=\sin{x}+\dfrac{1}{3}(\sin{x})^3+o(\sin^3{x})=x-\dfrac{1}{6}x^3+\dfrac{1}{3}(x-\dfrac{1}{6}x^3)^3+o(x^3)$$ so $$\tan{(\tan{x})}-\tan{(\sin{x})}=\dfrac{1}{2}x^3+o(x^3)$$ $$\tan{x}-\sin{x}=\dfrac{1}{2}x^3+o(x^3)$$ Have other metods? Thank you","find the limit. $$\lim_{x\to0^{+}}\dfrac{\tan{(\tan{x})}-\tan{(\sin{x})}}{\tan{x}-\sin{x}}$$ my try: $$\tan{x}=x+\dfrac{1}{3}x^3+o(x^3),\sin{x}=x-\dfrac{1}{6}x^3+o(x^3)$$ so $$\tan{(\tan{x})}=\tan{x}+\dfrac{1}{3}(\tan{x})^3+o(\tan^3{x})=x+\dfrac{1}{3}x^3+\dfrac{1}{3}(x+\dfrac{1}{3}x^3)^3+o(x^3)$$ $$\tan{(\sin{x})}=\sin{x}+\dfrac{1}{3}(\sin{x})^3+o(\sin^3{x})=x-\dfrac{1}{6}x^3+\dfrac{1}{3}(x-\dfrac{1}{6}x^3)^3+o(x^3)$$ so $$\tan{(\tan{x})}-\tan{(\sin{x})}=\dfrac{1}{2}x^3+o(x^3)$$ $$\tan{x}-\sin{x}=\dfrac{1}{2}x^3+o(x^3)$$ Have other metods? Thank you",,[]
75,How can I evaluate $\lim_{x\to1}\frac{\sqrt{5-x}-2}{\sqrt{2-x}-1}$ without invoking l'Hôpital's rule?,How can I evaluate  without invoking l'Hôpital's rule?,\lim_{x\to1}\frac{\sqrt{5-x}-2}{\sqrt{2-x}-1},"In the math clinic I work at, somebody in a Calculus 1 class asked for help with this limit problem. They have not covered basic differentiation techniques yet, let alone l'Hôpital's rule. $$\lim_{x\to1}\frac{\sqrt{5-x}-2}{\sqrt{2-x}-1}$$ We have tried various algebraic techniques, such as multiplying the top and bottom of the fraction by the conjugate of the denominator, but haven't had any success at getting rid of the indeterminate form. How can this limit be solved, using only techniques that would be available to a beginning Calculus 1 student?","In the math clinic I work at, somebody in a Calculus 1 class asked for help with this limit problem. They have not covered basic differentiation techniques yet, let alone l'Hôpital's rule. $$\lim_{x\to1}\frac{\sqrt{5-x}-2}{\sqrt{2-x}-1}$$ We have tried various algebraic techniques, such as multiplying the top and bottom of the fraction by the conjugate of the denominator, but haven't had any success at getting rid of the indeterminate form. How can this limit be solved, using only techniques that would be available to a beginning Calculus 1 student?",,['limits']
76,Infinite product limit,Infinite product limit,,"Suppose we have the infinite product $2(1/2)(2^4)(1/2^8)(2^{16})\dots$ I have a hunch that the infinite product is $0$ despite partial product being strictly positive. Am I correct? If so, then how? Thank you ahead of time!","Suppose we have the infinite product $2(1/2)(2^4)(1/2^8)(2^{16})\dots$ I have a hunch that the infinite product is $0$ despite partial product being strictly positive. Am I correct? If so, then how? Thank you ahead of time!",,['limits']
77,"If $(1+\sqrt{2})^n=a_{n}+b_{n}\sqrt{2}\;,(\forall n\in \mathbb{N}).$Then $\lim_{n\rightarrow \infty}\frac{a_{n}}{b_{n}} = $",If Then,"(1+\sqrt{2})^n=a_{n}+b_{n}\sqrt{2}\;,(\forall n\in \mathbb{N}). \lim_{n\rightarrow \infty}\frac{a_{n}}{b_{n}} = ","Let $a_{n},b_{n}\in \mathbb{Q}$ such that $(1+\sqrt{2})^n=a_{n}+b_{n}\sqrt{2}\;,(\forall n\in \mathbb{N}).$Then $\displaystyle \lim_{n\rightarrow \infty}\frac{a_{n}}{b_{n}} = $ $\bf{My\; Try::}$ Using $$(1+\sqrt{2})^n = 1+\binom{n}{1}(\sqrt{2})+\binom{n}{2}(\sqrt{2})^2+\cdots \cdots +\binom{n}{n}(\sqrt{2})^n$$ So $$a_{n} = \binom{n}{0}+\binom{n}{2}\cdot 2+\binom{n}{4}\cdot 2^2+\cdots \cdots $$ So $$b_{n} = \binom{n}{1}+\binom{n}{3}\cdot 2+\binom{n}{5}\cdot (2)^2+\cdots$$ Now How can i solve $$\lim_{n\rightarrow \infty}\frac{a_{n}}{b_{n}} = $$ Help required, Thanks","Let $a_{n},b_{n}\in \mathbb{Q}$ such that $(1+\sqrt{2})^n=a_{n}+b_{n}\sqrt{2}\;,(\forall n\in \mathbb{N}).$Then $\displaystyle \lim_{n\rightarrow \infty}\frac{a_{n}}{b_{n}} = $ $\bf{My\; Try::}$ Using $$(1+\sqrt{2})^n = 1+\binom{n}{1}(\sqrt{2})+\binom{n}{2}(\sqrt{2})^2+\cdots \cdots +\binom{n}{n}(\sqrt{2})^n$$ So $$a_{n} = \binom{n}{0}+\binom{n}{2}\cdot 2+\binom{n}{4}\cdot 2^2+\cdots \cdots $$ So $$b_{n} = \binom{n}{1}+\binom{n}{3}\cdot 2+\binom{n}{5}\cdot (2)^2+\cdots$$ Now How can i solve $$\lim_{n\rightarrow \infty}\frac{a_{n}}{b_{n}} = $$ Help required, Thanks",,['limits']
78,Can someone explain log?,Can someone explain log?,,"I have done some higher math, being an engineer. They usually focus on getting the correct answer as supposed to actually understanding math. Glorified calculators, basically. I know how to do stuff with $\log x$, even calculate it with different bases, yay me. So my next question might come as a bit of surprise. Can someone explain log from a real life point of view. Bonus points for explain $x \log x$. Please and thank you.","I have done some higher math, being an engineer. They usually focus on getting the correct answer as supposed to actually understanding math. Glorified calculators, basically. I know how to do stuff with $\log x$, even calculate it with different bases, yay me. So my next question might come as a bit of surprise. Can someone explain log from a real life point of view. Bonus points for explain $x \log x$. Please and thank you.",,"['limits', 'logarithms']"
79,Intuition for $\lim_{x\to\infty}\sqrt{x^6 - 9x^3}-x^3$,Intuition for,\lim_{x\to\infty}\sqrt{x^6 - 9x^3}-x^3,"Trying to get some intuition behind why: $$ \lim_{x\to\infty}\sqrt{x^6-9x^3}-x^3=-\frac{9}{2}. $$ First off, how would one calculate this? I tried maybe factoring out an $x^3$ from the inside of the square root, but the remainder is not factorable to make anything simpler. Also tried expressing $x^3$ as $\sqrt{x^6}$, but that doesn't really help either. One would think that, as $x^6$ grows more quickly than $x^3$ by a factor of $x^3$, the contribution of the $x^3$ term to the term in the square root would be dwarfed by the contribution of the the $x^6$ term, so the overall behavior of the first term in the limit would ""behave"" like $x^3$, as x gets bigger and bigger, so I would think intuitively that the limit would evaluate to 0.","Trying to get some intuition behind why: $$ \lim_{x\to\infty}\sqrt{x^6-9x^3}-x^3=-\frac{9}{2}. $$ First off, how would one calculate this? I tried maybe factoring out an $x^3$ from the inside of the square root, but the remainder is not factorable to make anything simpler. Also tried expressing $x^3$ as $\sqrt{x^6}$, but that doesn't really help either. One would think that, as $x^6$ grows more quickly than $x^3$ by a factor of $x^3$, the contribution of the $x^3$ term to the term in the square root would be dwarfed by the contribution of the the $x^6$ term, so the overall behavior of the first term in the limit would ""behave"" like $x^3$, as x gets bigger and bigger, so I would think intuitively that the limit would evaluate to 0.",,['limits']
80,Formal proof that Schwartz space is space of rapidly decreasing functions,Formal proof that Schwartz space is space of rapidly decreasing functions,,"Everybody says that the Schwartz space is a space of rapidly decreasing functions , or of functions that rapidly vanish , but I am baffled with proving it formally. I can come up with nice reasons why it is true, but not with a solid formal proof. What I want to prove is   $$f\in S(\mathbb{R}^n)\implies\lim_{|x|\to\infty}f(x)=0$$   or more precisely    $\forall\varepsilon>0,\exists K\subset\mathbb{R}^n$ compact, that $x\in(\mathbb{R}^n\setminus K)\implies|f(x)|<\varepsilon$. The Schwartz space is  $$S(\mathbb{R}^n)=\{f:\mathbb{R}^n\to\mathbb{R}^n\mid f\in C^\infty(\mathbb{R}^n),\; \|f\|_{\alpha,\beta}<\infty\}$$ where $\alpha,\beta\in\mathbb{N}_0^n$ are multi-indexes and  $$\|f\|_{\alpha,\beta}=\sup_{x\in\mathbb{R}^n}|x^\alpha \partial_\beta f(x)|.$$","Everybody says that the Schwartz space is a space of rapidly decreasing functions , or of functions that rapidly vanish , but I am baffled with proving it formally. I can come up with nice reasons why it is true, but not with a solid formal proof. What I want to prove is   $$f\in S(\mathbb{R}^n)\implies\lim_{|x|\to\infty}f(x)=0$$   or more precisely    $\forall\varepsilon>0,\exists K\subset\mathbb{R}^n$ compact, that $x\in(\mathbb{R}^n\setminus K)\implies|f(x)|<\varepsilon$. The Schwartz space is  $$S(\mathbb{R}^n)=\{f:\mathbb{R}^n\to\mathbb{R}^n\mid f\in C^\infty(\mathbb{R}^n),\; \|f\|_{\alpha,\beta}<\infty\}$$ where $\alpha,\beta\in\mathbb{N}_0^n$ are multi-indexes and  $$\|f\|_{\alpha,\beta}=\sup_{x\in\mathbb{R}^n}|x^\alpha \partial_\beta f(x)|.$$",,"['limits', 'schwartz-space']"
81,Distance to Cross a City Diagonally,Distance to Cross a City Diagonally,,"If I had to cross from the southwest corner of a city to the northeast corner of a rectangular city and I could do so by helicopter, the distance would be $\sqrt{x^2 + y^2}$, which is less than $x + y$. If I chose to cross that same city by foot, and I chose to travel diagonally, I would have to go by city blocks and the distance would be $x + y$. As I make the blocks smaller, the distance is still $x + y$. It seems like this must be able to be expressed as a limit that evaluates to $\sqrt{x^2 + y^2}$ as the block sizes approach $0$.","If I had to cross from the southwest corner of a city to the northeast corner of a rectangular city and I could do so by helicopter, the distance would be $\sqrt{x^2 + y^2}$, which is less than $x + y$. If I chose to cross that same city by foot, and I chose to travel diagonally, I would have to go by city blocks and the distance would be $x + y$. As I make the blocks smaller, the distance is still $x + y$. It seems like this must be able to be expressed as a limit that evaluates to $\sqrt{x^2 + y^2}$ as the block sizes approach $0$.",,['limits']
82,Help find the mistake in this problem of finding limit (using L'Hopital),Help find the mistake in this problem of finding limit (using L'Hopital),,"Evaluate $$\lim_{x \to 0} \left(\frac{1}{x^2}-\cot^2x\right).$$ Attempt \begin{align*} &\lim_{x \to 0} \left(\frac{1}{x^2}-\cot^2x\right)\\ = &\lim_{x \to 0} \left(\frac{1}{x}-\cot{x}\right)\left(\frac{1}{x}+\cot{x}\right)\\ = &\lim_{x \to 0} \left(\frac{\sin{x}+x\cos{x}}{x\sin{x}}\right)\left(\frac{\sin{x}-x\cos{x}}{x\sin{x}}\right)\\ = &\lim_{x \to 0} \left(\frac{\sin{x}+x\cos{x}}{x\sin{x}}\right) \times \lim_{x \to 0}\left(\frac{\sin{x}-x\cos{x}}{x\sin{x}}\right). \end{align*} Both the terms are in $\frac00$ form. So applying L'Hopital on both the limits we have, $$= \lim_{x \to 0} \left(\frac{2\cos{x}-x\sin{x}}{x\cos{x}+\sin{x}}\right) \times \lim_{x \to 0}\left(\frac{x\sin{x}}{x\cos{x}+\sin{x}}\right).$$ The second term is in $\frac00$ form. So applying L'Hopital on the second limit we have, \begin{align*} = &\lim_{x \to 0} \left(\frac{2\cos{x}-x\sin{x}}{x\cos{x}+\sin{x}}\right) \times \lim_{x \to 0}\left(\frac{x\cos{x}+\sin{x}}{2\cos{x}-x\sin{x}}\right)\\ =& \lim_{x \to 0} \left(\frac{2\cos{x}-x\sin{x}}{x\cos{x}+\sin{x}}\right) \times \left(\frac{x\cos{x}+\sin{x}}{2\cos{x}-x\sin{x}}\right)\\ =& 1 \end{align*} The correct answer is $\dfrac23$ which can be found using series expansion. But I think I'm making a conceptual mistake in one of the above steps. Could you please point out to the specific step where I've committed a mistake in above solution?","Evaluate Attempt Both the terms are in form. So applying L'Hopital on both the limits we have, The second term is in form. So applying L'Hopital on the second limit we have, The correct answer is which can be found using series expansion. But I think I'm making a conceptual mistake in one of the above steps. Could you please point out to the specific step where I've committed a mistake in above solution?","\lim_{x \to 0} \left(\frac{1}{x^2}-\cot^2x\right). \begin{align*}
&\lim_{x \to 0} \left(\frac{1}{x^2}-\cot^2x\right)\\
= &\lim_{x \to 0} \left(\frac{1}{x}-\cot{x}\right)\left(\frac{1}{x}+\cot{x}\right)\\
= &\lim_{x \to 0} \left(\frac{\sin{x}+x\cos{x}}{x\sin{x}}\right)\left(\frac{\sin{x}-x\cos{x}}{x\sin{x}}\right)\\
= &\lim_{x \to 0} \left(\frac{\sin{x}+x\cos{x}}{x\sin{x}}\right) \times \lim_{x \to 0}\left(\frac{\sin{x}-x\cos{x}}{x\sin{x}}\right).
\end{align*} \frac00 = \lim_{x \to 0} \left(\frac{2\cos{x}-x\sin{x}}{x\cos{x}+\sin{x}}\right) \times \lim_{x \to 0}\left(\frac{x\sin{x}}{x\cos{x}+\sin{x}}\right). \frac00 \begin{align*}
= &\lim_{x \to 0} \left(\frac{2\cos{x}-x\sin{x}}{x\cos{x}+\sin{x}}\right) \times \lim_{x \to 0}\left(\frac{x\cos{x}+\sin{x}}{2\cos{x}-x\sin{x}}\right)\\
=& \lim_{x \to 0} \left(\frac{2\cos{x}-x\sin{x}}{x\cos{x}+\sin{x}}\right) \times \left(\frac{x\cos{x}+\sin{x}}{2\cos{x}-x\sin{x}}\right)\\
=& 1
\end{align*} \dfrac23",['limits']
83,Limit involving binomial coefficient: $\lim_{n\to\infty} \left(\frac n{n-1}\right)^2 \left(\frac12\right)^n \sum_{i=1}^n \binom ni \cdot \frac{i-1}i$,Limit involving binomial coefficient:,\lim_{n\to\infty} \left(\frac n{n-1}\right)^2 \left(\frac12\right)^n \sum_{i=1}^n \binom ni \cdot \frac{i-1}i,I was trying to find the below limit. The sum can be written in a hypergeometric function but it doesn't seem to help me to find the limit. Any help will be appreciated. $$ \lim_{n \rightarrow \infty} \left. \left(  \frac{n}{n-1} \right)^2 \left(  \frac{1}{2} \right)^n   \sum_{i=1}^n {\binom{n}{i} \cdot \frac{i-1}{i}} \right. $$ Thanks.,I was trying to find the below limit. The sum can be written in a hypergeometric function but it doesn't seem to help me to find the limit. Any help will be appreciated. $$ \lim_{n \rightarrow \infty} \left. \left(  \frac{n}{n-1} \right)^2 \left(  \frac{1}{2} \right)^n   \sum_{i=1}^n {\binom{n}{i} \cdot \frac{i-1}{i}} \right. $$ Thanks.,,"['limits', 'summation', 'binomial-coefficients', 'hypergeometric-function']"
84,Evaluating the given limit without using L'H rule.,Evaluating the given limit without using L'H rule.,,"Evaluate $\;\lim\limits_{x\to 1} \cfrac{x^{P+1} - x(P+1) + P}{(x-1)^2} $ . The reason why I've mentioned for help in evaluating the given limit without using L'H rule is that I've already solved it using L'H rule twice (shown below) but I'm actually looking for an alter method for this question, that is without using L'H rule. Just as a curiosity to solve it in multiple ways. Using L'H rule, this is what I've done: $$\begin{align} & \lim_{x\to 1} \cfrac{(P+1)x^P - (P+1)}{2(x-1)} \tag{1}\\ &\lim_{x\to 1} \cfrac{P(P+1)x^{P-1}}{2} \tag{2} \\ \implies &\lim_{x\to 1} \cfrac{x^{P+1} - x(P+1) + P}{(x-1)^2}= \cfrac{P(P+1)}{2}\end{align}$$ I've used L'H rule twice here, in steps (1) and (2). The above work was just to show what I've tried yet. I'm not sure how to move forward without using L'H rule. Any help will be greatly appreciated.","Evaluate . The reason why I've mentioned for help in evaluating the given limit without using L'H rule is that I've already solved it using L'H rule twice (shown below) but I'm actually looking for an alter method for this question, that is without using L'H rule. Just as a curiosity to solve it in multiple ways. Using L'H rule, this is what I've done: I've used L'H rule twice here, in steps (1) and (2). The above work was just to show what I've tried yet. I'm not sure how to move forward without using L'H rule. Any help will be greatly appreciated.","\;\lim\limits_{x\to 1} \cfrac{x^{P+1} - x(P+1) + P}{(x-1)^2}  \begin{align} & \lim_{x\to 1} \cfrac{(P+1)x^P - (P+1)}{2(x-1)} \tag{1}\\
&\lim_{x\to 1} \cfrac{P(P+1)x^{P-1}}{2} \tag{2} \\
\implies &\lim_{x\to 1} \cfrac{x^{P+1} - x(P+1) + P}{(x-1)^2}= \cfrac{P(P+1)}{2}\end{align}","['limits', 'limits-without-lhopital']"
85,Calculate trig limit of type $\frac{0}{0}$ without L'Hopital,Calculate trig limit of type  without L'Hopital,\frac{0}{0},I am trying to figure out the solution to this Limit without using L'Hopital. $$ \lim \limits_{x \to \pi} \frac {(\tan (4x))^2 } {(x - \pi )^2} $$ Any help would be greatly appreciated.,I am trying to figure out the solution to this Limit without using L'Hopital. $$ \lim \limits_{x \to \pi} \frac {(\tan (4x))^2 } {(x - \pi )^2} $$ Any help would be greatly appreciated.,,"['limits', 'trigonometry', 'limits-without-lhopital']"
86,Is the Epsilon-Delta definition of a limit not precise enough?,Is the Epsilon-Delta definition of a limit not precise enough?,,"Consider the function $f: \{-1, +1\}\to \mathbb R$ defined by $f(x)= \arcsin (\frac{1+x^2}{2x})$ . Due to the following two inequalities : (i) $1+x^2 \geq 2x$ (ii) $1+x^2 \geq -2x$ , the function can only be defined at $x=1$ and $x=-1$ . I have learnt that the epsilon delta definition only includes those values of $x$ which are in the domain of $f(x)$ . But in this case, the function isn't defined on either side of x=1. So this is my question: Is it correct to say that the limit as $x$ approaches $1$ of $f(x$ ) is $\frac{\pi}{2}$ ? Can the above question be given a definitive ""yes"" or ""no"" answer, or must it(unfortunately) vary from person to person? If the latter, is the ""precise"" definition of a limit not precise enough? How can the answer be proved or disproved using the epsilon delta definition? I have also read that functions are by default continuous at isolated points. Can I conclude from the definition of continuity (the limit equals the value of the function evaluated at the point) that the limit must exist? Note : Forgive my ignorance but I do not know a thing about topology. I'm looking for a detailed answer but in simple terms, preferably written in the language of calculus. Thanks for the help.","Consider the function defined by . Due to the following two inequalities : (i) (ii) , the function can only be defined at and . I have learnt that the epsilon delta definition only includes those values of which are in the domain of . But in this case, the function isn't defined on either side of x=1. So this is my question: Is it correct to say that the limit as approaches of ) is ? Can the above question be given a definitive ""yes"" or ""no"" answer, or must it(unfortunately) vary from person to person? If the latter, is the ""precise"" definition of a limit not precise enough? How can the answer be proved or disproved using the epsilon delta definition? I have also read that functions are by default continuous at isolated points. Can I conclude from the definition of continuity (the limit equals the value of the function evaluated at the point) that the limit must exist? Note : Forgive my ignorance but I do not know a thing about topology. I'm looking for a detailed answer but in simple terms, preferably written in the language of calculus. Thanks for the help.","f: \{-1, +1\}\to \mathbb R f(x)= \arcsin (\frac{1+x^2}{2x}) 1+x^2 \geq 2x 1+x^2 \geq -2x x=1 x=-1 x f(x) x 1 f(x \frac{\pi}{2}",['limits']
87,Prove $\lim_{x\rightarrow 0}\cos (x)=1$ with the epsilon-delta definition of limits,Prove  with the epsilon-delta definition of limits,\lim_{x\rightarrow 0}\cos (x)=1,Prove $$     \lim_{x\rightarrow 0}\cos (x)=1 $$ with the epsilon-delta definition of limits,Prove $$     \lim_{x\rightarrow 0}\cos (x)=1 $$ with the epsilon-delta definition of limits,,"['limits', 'trigonometry']"
88,Behavior of function $\sum_{j = n}^\infty \frac{\sin^2((2j-1) \pi x)}{(2j-1)^2}$,Behavior of function,\sum_{j = n}^\infty \frac{\sin^2((2j-1) \pi x)}{(2j-1)^2},"For a positive integer $n$ , define the function $$ F_n(x) = n^2 \sum_{j = n}^\infty \frac{\sin^2((2j-1) \pi x)}{(2j-1)^2}. $$ I am trying to understand the behavior of $F_n(x)$ in the following sense. For a positive exponent $\alpha$ , I would like to compute the limit $$ L_\alpha = \lim_{n \to \infty} F_n(1/n^\alpha).  $$ Based on plotting in Mathematica, it appears that $L_\alpha$ has the following behavior: it seems to be $0$ for $\alpha > 2$ , infinite for $\alpha < 2$ and some finite number for $\alpha = 2$ . I tried to simplify the summation but I am having difficulty passing to the limit.","For a positive integer , define the function I am trying to understand the behavior of in the following sense. For a positive exponent , I would like to compute the limit Based on plotting in Mathematica, it appears that has the following behavior: it seems to be for , infinite for and some finite number for . I tried to simplify the summation but I am having difficulty passing to the limit.","n 
F_n(x) = n^2 \sum_{j = n}^\infty \frac{\sin^2((2j-1) \pi x)}{(2j-1)^2}.
 F_n(x) \alpha 
L_\alpha = \lim_{n \to \infty} F_n(1/n^\alpha). 
 L_\alpha 0 \alpha > 2 \alpha < 2 \alpha = 2","['limits', 'inequality', 'asymptotics', 'trigonometric-series']"
89,Evaluate $\lim\limits_{x \to 0}\frac{e^{(1+x)^{\frac{1}{x}}}-(1+x)^{\frac{e}{x}}}{x^2}$ [duplicate],Evaluate  [duplicate],\lim\limits_{x \to 0}\frac{e^{(1+x)^{\frac{1}{x}}}-(1+x)^{\frac{e}{x}}}{x^2},"This question already has answers here : Evaluate $\lim_{x\to 0}\frac{e^{(x+1)^{1/x}}-(x+1)^{e/x}}{x^2}$ (3 answers) Closed 5 years ago . Solution Expanding $(1+x)^{\frac{1}{x}}$ at $x=0$ by Taylor's Formula，we obtain \begin{align*} (1+x)^{\frac{1}{x}}&=\exp\left[\frac{\ln(1+x)}{x}\right]=\exp \left(\frac{x-\frac{x^2}{2}+\frac{x^3}{3}+\cdots }{x}\right)\\ &=\exp \left(1-\frac{x}{2}+\frac{x^2}{3}+\cdots \right)=e\cdot\exp \left(-\frac{x}{2}+\frac{x^2}{3}+\cdots \right) \\ &=e\left[1+\left(-\dfrac{x}{2}+\dfrac{x^2}{3}+\cdots \right)+\frac{1}{2!}\left(-\dfrac{x}{2}+\dfrac{x^2}{3}+\cdots \right)^2+\cdots\right]\\ &=e\left(1-\frac{x}{2}+\frac{11}{24}x^2+\cdots\right)\\ &=e-\frac{ex}{2}+\frac{11}{24}ex^2+\cdots \end{align*} Likewise, expanding $e^{(1+x)^{\frac{1}{x}}}$ at $x=0$ , we obtain \begin{align*} e^{(1+x)^{\frac{1}{x}}}&=(e^e)^{1-\frac{x}{2}+\frac{11}{24}x^2-\cdots}=e^e\cdot (e^e)^{-\frac{x}{2}+\frac{11}{24}x^2+\cdots}\\ &=e^e\cdot\left[1+\left(-\frac{x}{2}+\frac{11}{24}x^2+\cdots\right)\ln e^e+\frac{1}{2!}\left(-\frac{x}{2}+\frac{11}{24}x^2+\cdots\right)^2\ln^2 e^e+\cdots\right]\\ &=e^e \cdot\left[1-\frac{ex}{2}+\frac{1}{24}(11e+3e^2)x^2+\cdots\right] \end{align*} Expanding $(1+x)^{\frac{e}{x}}$ at $x=0$ , it follows that \begin{align*} (1+x)^{\frac{e}{x}}&=\exp\left[\frac{e\ln(1+x)}{x}\right]=\exp \left(e\cdot\frac{x-\frac{x^2}{2}+\frac{x^3}{3}+\cdots }{x}\right)\\ &=\exp \left(e-\frac{ex}{2}+\frac{ex^2}{3}+\cdots \right)=e^e\cdot\exp \left(-\frac{ex}{2}+\frac{ex^2}{3}+\cdots \right) \\ &=e^e\left[1+\left(-\frac{ex}{2}+\frac{ex^2}{3}+\cdots \right)+\frac{1}{2!}\left(-\frac{ex}{2}+\frac{ex^2}{3}+\cdots \right)^2+\cdots\right]\\ &=e^e\left[1-\frac{ex}{2}+\frac{1}{24}e(8+3e)x^2+\cdots\right] \end{align*} Therefore \begin{align*} &\lim_{x \to 0}\frac{e^{(1+x)^{\frac{1}{x}}}-(1+x)^{\frac{e}{x}}}{x^2}\\ =&\lim_{x \to 0}\frac{e^e\cdot\left[1-\dfrac{ex}{2}+\dfrac{1}{24}e(11+3e)x^2+\cdots\right]-e^e\cdot\left[1-\dfrac{ex}{2}+\dfrac{1}{24}e(8+3e)x^2+\cdots\right]}{x^2}\\ =&e^e\cdot\frac{1}{8}e\\ =&\frac{1}{8}e^{e+1} \end{align*} Please check. Is there any more simpler solution?","This question already has answers here : Evaluate $\lim_{x\to 0}\frac{e^{(x+1)^{1/x}}-(x+1)^{e/x}}{x^2}$ (3 answers) Closed 5 years ago . Solution Expanding at by Taylor's Formula，we obtain Likewise, expanding at , we obtain Expanding at , it follows that Therefore Please check. Is there any more simpler solution?","(1+x)^{\frac{1}{x}} x=0 \begin{align*}
(1+x)^{\frac{1}{x}}&=\exp\left[\frac{\ln(1+x)}{x}\right]=\exp \left(\frac{x-\frac{x^2}{2}+\frac{x^3}{3}+\cdots }{x}\right)\\
&=\exp \left(1-\frac{x}{2}+\frac{x^2}{3}+\cdots \right)=e\cdot\exp \left(-\frac{x}{2}+\frac{x^2}{3}+\cdots \right) \\
&=e\left[1+\left(-\dfrac{x}{2}+\dfrac{x^2}{3}+\cdots \right)+\frac{1}{2!}\left(-\dfrac{x}{2}+\dfrac{x^2}{3}+\cdots \right)^2+\cdots\right]\\
&=e\left(1-\frac{x}{2}+\frac{11}{24}x^2+\cdots\right)\\
&=e-\frac{ex}{2}+\frac{11}{24}ex^2+\cdots
\end{align*} e^{(1+x)^{\frac{1}{x}}} x=0 \begin{align*}
e^{(1+x)^{\frac{1}{x}}}&=(e^e)^{1-\frac{x}{2}+\frac{11}{24}x^2-\cdots}=e^e\cdot (e^e)^{-\frac{x}{2}+\frac{11}{24}x^2+\cdots}\\
&=e^e\cdot\left[1+\left(-\frac{x}{2}+\frac{11}{24}x^2+\cdots\right)\ln e^e+\frac{1}{2!}\left(-\frac{x}{2}+\frac{11}{24}x^2+\cdots\right)^2\ln^2 e^e+\cdots\right]\\
&=e^e \cdot\left[1-\frac{ex}{2}+\frac{1}{24}(11e+3e^2)x^2+\cdots\right]
\end{align*} (1+x)^{\frac{e}{x}} x=0 \begin{align*}
(1+x)^{\frac{e}{x}}&=\exp\left[\frac{e\ln(1+x)}{x}\right]=\exp \left(e\cdot\frac{x-\frac{x^2}{2}+\frac{x^3}{3}+\cdots }{x}\right)\\
&=\exp \left(e-\frac{ex}{2}+\frac{ex^2}{3}+\cdots \right)=e^e\cdot\exp \left(-\frac{ex}{2}+\frac{ex^2}{3}+\cdots \right) \\
&=e^e\left[1+\left(-\frac{ex}{2}+\frac{ex^2}{3}+\cdots \right)+\frac{1}{2!}\left(-\frac{ex}{2}+\frac{ex^2}{3}+\cdots \right)^2+\cdots\right]\\
&=e^e\left[1-\frac{ex}{2}+\frac{1}{24}e(8+3e)x^2+\cdots\right]
\end{align*} \begin{align*}
&\lim_{x \to 0}\frac{e^{(1+x)^{\frac{1}{x}}}-(1+x)^{\frac{e}{x}}}{x^2}\\
=&\lim_{x \to 0}\frac{e^e\cdot\left[1-\dfrac{ex}{2}+\dfrac{1}{24}e(11+3e)x^2+\cdots\right]-e^e\cdot\left[1-\dfrac{ex}{2}+\dfrac{1}{24}e(8+3e)x^2+\cdots\right]}{x^2}\\
=&e^e\cdot\frac{1}{8}e\\
=&\frac{1}{8}e^{e+1}
\end{align*}","['limits', 'proof-verification']"
90,Why does $\epsilon$ come first in the $\epsilon-\delta$ definition of limit? [duplicate],Why does  come first in the  definition of limit? [duplicate],\epsilon \epsilon-\delta,"This question already has answers here : What's wrong with this ""backwards"" definition of limit? (5 answers) Closed 4 years ago . As we know, $\underset{x\rightarrow c}{\lim}f(x)=L\Leftrightarrow$ for every $\epsilon>0$ there exists $\delta>0$ such that if $0<|x-c|<\delta$, then $|f(x)-L|<\epsilon$. My question is: why do we say that for every $\epsilon>0$ there exists $\delta>0$ and not vice versa , why not for every $\delta>0$ there exists $\epsilon>0$?","This question already has answers here : What's wrong with this ""backwards"" definition of limit? (5 answers) Closed 4 years ago . As we know, $\underset{x\rightarrow c}{\lim}f(x)=L\Leftrightarrow$ for every $\epsilon>0$ there exists $\delta>0$ such that if $0<|x-c|<\delta$, then $|f(x)-L|<\epsilon$. My question is: why do we say that for every $\epsilon>0$ there exists $\delta>0$ and not vice versa , why not for every $\delta>0$ there exists $\epsilon>0$?",,"['limits', 'epsilon-delta']"
91,"Understanding limits and how to interpret the meaning of ""arbitrarily close""","Understanding limits and how to interpret the meaning of ""arbitrarily close""",,"I have read several introductory notes on limits of functions, and in all of them they introduce the notion of a limit of a function $f(x)$ by discussing what happens to the value of $f$ as $x$ approaches a given value, say $x=a$. In doing so they use phrases of the form ""if $\lim_{x\rightarrow a}f(x)=L$ exists, this means that given a value of $x$ sufficiently close to $a$ (but not equal to $a$), we can make $f(x)$ arbitrarily close to $L$"" . What confuses me about this is, if one has the result $\lim_{x\rightarrow a}f(x)=L$, does this mean that one should take ""arbitrarily close"" as ""equal to"" ? Is it that since arbitrarily close values of $x$ to $x=a$ lead to the value of $f$ being arbitrarily close to its value at $x=a$, we can imply that the limiting value of $f$ is exactly equal to $L$!? The primary reason I ask is because the derivative is defined as the limit of a diffence quotient that itself is undefined at the point we are approaching, so how is one to interpret the limiting value of this difference quotient $$\lim_{\Delta x\rightarrow 0}\frac{f(x+\Delta x)-f(x)}{\Delta x}=f'(x)$$ how can one state that its limiting value is exactly equal to the slope of the tangent line to the point $x$, equivalently the instantaneous rate of change in the value of the function $f$ with respect $x$ at the point $x$. How can we be certain that this is true? I feel like I might have missing something important here. If anyone can enlighten me it would be much appreciated!","I have read several introductory notes on limits of functions, and in all of them they introduce the notion of a limit of a function $f(x)$ by discussing what happens to the value of $f$ as $x$ approaches a given value, say $x=a$. In doing so they use phrases of the form ""if $\lim_{x\rightarrow a}f(x)=L$ exists, this means that given a value of $x$ sufficiently close to $a$ (but not equal to $a$), we can make $f(x)$ arbitrarily close to $L$"" . What confuses me about this is, if one has the result $\lim_{x\rightarrow a}f(x)=L$, does this mean that one should take ""arbitrarily close"" as ""equal to"" ? Is it that since arbitrarily close values of $x$ to $x=a$ lead to the value of $f$ being arbitrarily close to its value at $x=a$, we can imply that the limiting value of $f$ is exactly equal to $L$!? The primary reason I ask is because the derivative is defined as the limit of a diffence quotient that itself is undefined at the point we are approaching, so how is one to interpret the limiting value of this difference quotient $$\lim_{\Delta x\rightarrow 0}\frac{f(x+\Delta x)-f(x)}{\Delta x}=f'(x)$$ how can one state that its limiting value is exactly equal to the slope of the tangent line to the point $x$, equivalently the instantaneous rate of change in the value of the function $f$ with respect $x$ at the point $x$. How can we be certain that this is true? I feel like I might have missing something important here. If anyone can enlighten me it would be much appreciated!",,"['limits', 'derivatives', 'intuition']"
92,Epsilon-Delta proof of $\lim_{x\to 2} x^2=4$,Epsilon-Delta proof of,\lim_{x\to 2} x^2=4,"I have seen and understand the delta-epsilon proof of the limit of $x^2$ for $x\to2$, such as explained here: https://www.youtube.com/watch?v=gLpQgWWXgMM Now I am wondering, is there also another way? How about this: Verify that $\lim x^2=4$ (for $x\to2$) STEP A: Express epsilon in terms of $x$: \begin{align} |x^2-4| &< \varepsilon\\ -\varepsilon &< x^2-4 < \varepsilon\\ 4-\varepsilon &< x^2 < 4+\varepsilon\\ \sqrt{4-\varepsilon} &< x < \sqrt{4+\varepsilon} \end{align} STEP B: Express delta in terms of $x$ \begin{align} |x-2| &< \delta\\ -\delta &< x-2 < \delta\\ 2-\delta &< x < 2+\delta \end{align} STEP C: Now we can express $\delta$ in terms of $\varepsilon$ hence proving the limit. If we take $\delta=\min\{-2+\sqrt{4+\varepsilon},2-\sqrt{4-\varepsilon}\}$ then the limit is proven Did I make any mistake? Thanks! Cheers!","I have seen and understand the delta-epsilon proof of the limit of $x^2$ for $x\to2$, such as explained here: https://www.youtube.com/watch?v=gLpQgWWXgMM Now I am wondering, is there also another way? How about this: Verify that $\lim x^2=4$ (for $x\to2$) STEP A: Express epsilon in terms of $x$: \begin{align} |x^2-4| &< \varepsilon\\ -\varepsilon &< x^2-4 < \varepsilon\\ 4-\varepsilon &< x^2 < 4+\varepsilon\\ \sqrt{4-\varepsilon} &< x < \sqrt{4+\varepsilon} \end{align} STEP B: Express delta in terms of $x$ \begin{align} |x-2| &< \delta\\ -\delta &< x-2 < \delta\\ 2-\delta &< x < 2+\delta \end{align} STEP C: Now we can express $\delta$ in terms of $\varepsilon$ hence proving the limit. If we take $\delta=\min\{-2+\sqrt{4+\varepsilon},2-\sqrt{4-\varepsilon}\}$ then the limit is proven Did I make any mistake? Thanks! Cheers!",,"['limits', 'quadratics', 'epsilon-delta']"
93,finding the limit $\lim\limits_{x \to \infty }(\frac{1}{e}(1+\frac{1}{x})^x)^x$,finding the limit,\lim\limits_{x \to \infty }(\frac{1}{e}(1+\frac{1}{x})^x)^x,Can someone show me how to calculate the limit: $$\lim_{x \to \infty }\left(\frac{1}{e}\left(1+\frac{1}{x}\right)^x\right)^x $$ I tried to use taylor series but failed. Thanks,Can someone show me how to calculate the limit: $$\lim_{x \to \infty }\left(\frac{1}{e}\left(1+\frac{1}{x}\right)^x\right)^x $$ I tried to use taylor series but failed. Thanks,,['limits']
94,"Show $\lim_{n \to \infty} \min\{a_{n},b_{n}\} = \min\{a,b\}$",Show,"\lim_{n \to \infty} \min\{a_{n},b_{n}\} = \min\{a,b\}","If $\lim_{n \to \infty} a_{n} = a$ and $\lim_{n \to \infty} b_{n} = b$, how can we show that $\lim_{n \to \infty} \min\{a_{n},b_{n}\} = \min\{a,b\}$? I say $\min\{a_{n},b_{n}\} $ has two cases: $a_{n}$ and $b_{n}$. So (1) $\lim_{n \to \infty} a_{n} = a$ and (2) $\lim_{n \to \infty} b_{n} = b$. Now I don't know how to imply the $\min\{a,b\}$.","If $\lim_{n \to \infty} a_{n} = a$ and $\lim_{n \to \infty} b_{n} = b$, how can we show that $\lim_{n \to \infty} \min\{a_{n},b_{n}\} = \min\{a,b\}$? I say $\min\{a_{n},b_{n}\} $ has two cases: $a_{n}$ and $b_{n}$. So (1) $\lim_{n \to \infty} a_{n} = a$ and (2) $\lim_{n \to \infty} b_{n} = b$. Now I don't know how to imply the $\min\{a,b\}$.",,"['limits', 'proof-writing', 'problem-solving']"
95,Improper integrals question: why does this work?,Improper integrals question: why does this work?,,"So, I'm solving $$ \int_0^3{  \frac{dx}{ \sqrt{ 9 - x^2 } }  } $$ The catch here is $f(x)$ is defined on ( -3, 3 ) only. And I know the way to solve this is to integrate (yielding $ \arcsin{ \frac{x}{3} } + C $), then evaluate $ \lim_{c \to 3^-}{ \arcsin{\frac{c}{3}} - \arcsin{ 0 } } $, which is simply $ \frac{ \pi }{2} $ My question is why does this work ?  I don't intuitively understand how, although $\sqrt{ 9 - x^2 }$ blows up very slowly to infinity as $ x \to 3 $, it still goes there, so how is the sum of the area under that curve not infinity?","So, I'm solving $$ \int_0^3{  \frac{dx}{ \sqrt{ 9 - x^2 } }  } $$ The catch here is $f(x)$ is defined on ( -3, 3 ) only. And I know the way to solve this is to integrate (yielding $ \arcsin{ \frac{x}{3} } + C $), then evaluate $ \lim_{c \to 3^-}{ \arcsin{\frac{c}{3}} - \arcsin{ 0 } } $, which is simply $ \frac{ \pi }{2} $ My question is why does this work ?  I don't intuitively understand how, although $\sqrt{ 9 - x^2 }$ blows up very slowly to infinity as $ x \to 3 $, it still goes there, so how is the sum of the area under that curve not infinity?",,"['intuition', 'limits', 'improper-integrals']"
96,Can we apply L'Hopital's rule where the derivative is not continuous?,Can we apply L'Hopital's rule where the derivative is not continuous?,,"My doubt arises due to the following : We know that the definition of the derivative of a function at a point $x=a$ , if it is differentiable at $a$ , is: $$f'(a) = \lim_{h \rightarrow 0} \frac {f(a+h) - f(a)}{h}$$ Suppose that the function $f(x)$ is differentiable in a finite interval $[c,d]$ and $a \in (c,d) $ So, we can apply L'Hopital's rule. On differentiating numerator and denominator with respect to $h$ , we get: $$f'(a) = \lim_{h \rightarrow 0} \frac {f(a+h) - f(a)}{h} = \lim_{h \rightarrow 0} \frac {f'(a+h)}{1}$$ Which implies that $$f'(a) = \lim_{h \rightarrow 0} f'(a+h)$$ Which means that the function $f'(x)$ is continuous at $x=a$ But this not necessarily true. A function may have a derivative everywhere but its derivative may not be continuous at some point. One of many counterexamples is: $$f(x) = \begin{cases} 0 \text{ ;    if x=0} \\ x^2 \sin \frac{1}{x} \text{;       if x $\neq$ 0 } \end{cases}$$ Whose derivative isn't continuous at $0$ So, is something wrong with what I have done ? Or is it necessary that for applying L'Hopital's rule, the function's derivative must be a continuous function? If the latter is true, why does that condition appear in the proof for L'Hopital's rule ?","My doubt arises due to the following : We know that the definition of the derivative of a function at a point , if it is differentiable at , is: Suppose that the function is differentiable in a finite interval and So, we can apply L'Hopital's rule. On differentiating numerator and denominator with respect to , we get: Which implies that Which means that the function is continuous at But this not necessarily true. A function may have a derivative everywhere but its derivative may not be continuous at some point. One of many counterexamples is: Whose derivative isn't continuous at So, is something wrong with what I have done ? Or is it necessary that for applying L'Hopital's rule, the function's derivative must be a continuous function? If the latter is true, why does that condition appear in the proof for L'Hopital's rule ?","x=a a f'(a) = \lim_{h \rightarrow 0} \frac {f(a+h) - f(a)}{h} f(x) [c,d] a \in (c,d)  h f'(a) = \lim_{h \rightarrow 0} \frac {f(a+h) - f(a)}{h} = \lim_{h \rightarrow 0} \frac {f'(a+h)}{1} f'(a) = \lim_{h \rightarrow 0} f'(a+h) f'(x) x=a f(x) = \begin{cases} 0 \text{ ;    if x=0} \\ x^2 \sin \frac{1}{x} \text{;       if x \neq 0 } \end{cases} 0","['limits', 'derivatives', 'continuity']"
97,Limit of the solutions of a trigonometric equation,Limit of the solutions of a trigonometric equation,,"Consider the equation \begin{equation} 2\cos(\sqrt{\lambda} \pi) \sin(\sqrt{\lambda} \frac{\pi}{n}) + \sin(\sqrt{\lambda} \pi) \cos(\sqrt{\lambda} \frac{\pi}{n})=0 \end{equation} for $n \in \mathbb{N}$. This came up when solving an eigenvalue equation. I want to show the existence of a unique solution $\lambda (n) \in (0,1)$ which satisfies $\lambda (n) \rightarrow 1$ as $n \rightarrow \infty$. Intuitively the solution, if existent, should have this property since the first part of the sum approaches $\sin(\sqrt{\lambda} \pi)$ and the second part approaches $0$ as $n \rightarrow \infty$. However, I don't see how to prove this. Bonus question: What if 2 is replaced by an arbitrary $k \in \mathbb{N}$. Do the solutions, if existent, still converge to $1$? Edit: I'm sorry to break the symmetry, but I forgot a factor in the first part of the sum.","Consider the equation \begin{equation} 2\cos(\sqrt{\lambda} \pi) \sin(\sqrt{\lambda} \frac{\pi}{n}) + \sin(\sqrt{\lambda} \pi) \cos(\sqrt{\lambda} \frac{\pi}{n})=0 \end{equation} for $n \in \mathbb{N}$. This came up when solving an eigenvalue equation. I want to show the existence of a unique solution $\lambda (n) \in (0,1)$ which satisfies $\lambda (n) \rightarrow 1$ as $n \rightarrow \infty$. Intuitively the solution, if existent, should have this property since the first part of the sum approaches $\sin(\sqrt{\lambda} \pi)$ and the second part approaches $0$ as $n \rightarrow \infty$. However, I don't see how to prove this. Bonus question: What if 2 is replaced by an arbitrary $k \in \mathbb{N}$. Do the solutions, if existent, still converge to $1$? Edit: I'm sorry to break the symmetry, but I forgot a factor in the first part of the sum.",,"['limits', 'trigonometry']"
98,evaluate lim: $\lim _{x\to \infty }\left(2x\left(e-\left(1+\frac{1}{x}\right)^x\right)\right)$,evaluate lim:,\lim _{x\to \infty }\left(2x\left(e-\left(1+\frac{1}{x}\right)^x\right)\right),I was trying to solve the above limit and it seems like I'm getting mixed results. I used the fact that: $\lim _{x\to \infty }\left(1+\frac{1}{x}\right)^x = e$ And after that trying with L'Hospital's rule but it didn't get me much further.,I was trying to solve the above limit and it seems like I'm getting mixed results. I used the fact that: $\lim _{x\to \infty }\left(1+\frac{1}{x}\right)^x = e$ And after that trying with L'Hospital's rule but it didn't get me much further.,,['limits']
99,Every function from discrete metric space to another metric space is uniformly continuous,Every function from discrete metric space to another metric space is uniformly continuous,,"My solution:It is fairly straightforward graphically but I just want to ensure if it is rigorous enough. Suppose $X$ is a discrete metric space and $f$ be any function from $X$ to $Y$ where $Y$ is any other metric space. Let $d_{x}$ and $d_{y}$ denote the metrics in metric spaces $X$ and $Y$ respectively. If $p,q\in X, p=q$, then $d_{x}(p,q)=0<\delta$ for any $\delta>0$. If $p,q\in X, p\neq q$, then $d_{x}(p,q)=1$. Choosing $\delta=2$, we have $d_{y}(f(p),f(q))<\epsilon$, $\forall \epsilon>0$. So $\forall \epsilon>0,$ we have $\delta>2$, such that $d_{x}(p,q)<\delta$ implies $d_{y}(f(p),f(q))<\epsilon$. So, any function from a discrete metric space to any other metric space is uniformly continuous. Please suggest if it is correct and rigorous enough.","My solution:It is fairly straightforward graphically but I just want to ensure if it is rigorous enough. Suppose $X$ is a discrete metric space and $f$ be any function from $X$ to $Y$ where $Y$ is any other metric space. Let $d_{x}$ and $d_{y}$ denote the metrics in metric spaces $X$ and $Y$ respectively. If $p,q\in X, p=q$, then $d_{x}(p,q)=0<\delta$ for any $\delta>0$. If $p,q\in X, p\neq q$, then $d_{x}(p,q)=1$. Choosing $\delta=2$, we have $d_{y}(f(p),f(q))<\epsilon$, $\forall \epsilon>0$. So $\forall \epsilon>0,$ we have $\delta>2$, such that $d_{x}(p,q)<\delta$ implies $d_{y}(f(p),f(q))<\epsilon$. So, any function from a discrete metric space to any other metric space is uniformly continuous. Please suggest if it is correct and rigorous enough.",,"['limits', 'continuity', 'uniform-continuity']"
