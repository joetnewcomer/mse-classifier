,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Probability of product of $5$ digits being divisible by $5$ or $7$.,Probability of product of  digits being divisible by  or .,5 5 7,"Seven digit numbers are formed using digits $1,2,3,4,5,6,7,8,9$ without repetition. The probability of selecting a number such that product of any $5$ consecutive digits is divisible by either $5$ or $7$ is $P$ . Then $12P$ is equal to: PS: I know this question has been asked and answered over here as well as on this site.Their answer is $7$ but I've gotten $8$ . I am unable to identify the error in my solution. Any corrections or full solutions would be appreciated First I tried to define $P'$ $P'= 1 - P$ $P'$ = probability of selecting a number such that atleast one of the consecutive $5$ digits aren't divisible by $5$ or $7$ or both. We try to find $P'-$ Counting of Favorable Cases: Case $1$ : Number has no $5$ or $7$ is $7!$ Case $2$ : Number has either $5$ or $7$ . In this case the number $5$ / $7$ can occupy positions $1,2,6$ and $7$ so that atleast one set of consecutive digits will not be divisible by $5$ or $7$ . Hence $4\choose1$ $\times$ $7\choose6$ $\times$ $6!$ $\times$ $2$ Case $3$ :Number has both $5$ and $7$ . In this case $5$ and $7$ occupy either positions $[1,2],[6,7]$ or $[1,7]$ . So $3\choose1$ $\times$ $2!$ $\times$ $7\choose 5$ $\times$ $5!$ Favorable Cases = $7!$ $\times$ $(1 + 8 + 3)$ Total Cases = $9 \choose 7$ $\times$ $7!$ $P'= \frac{1}{3}$ $\therefore P = \frac{2}{3}$ , Hence $\boxed{12P = 8}$","Seven digit numbers are formed using digits without repetition. The probability of selecting a number such that product of any consecutive digits is divisible by either or is . Then is equal to: PS: I know this question has been asked and answered over here as well as on this site.Their answer is but I've gotten . I am unable to identify the error in my solution. Any corrections or full solutions would be appreciated First I tried to define = probability of selecting a number such that atleast one of the consecutive digits aren't divisible by or or both. We try to find Counting of Favorable Cases: Case : Number has no or is Case : Number has either or . In this case the number / can occupy positions and so that atleast one set of consecutive digits will not be divisible by or . Hence Case :Number has both and . In this case and occupy either positions or . So Favorable Cases = Total Cases = , Hence","1,2,3,4,5,6,7,8,9 5 5 7 P 12P 7 8 P' P'= 1 - P P' 5 5 7 P'- 1 5 7 7! 2 5 7 5 7 1,2,6 7 5 7 4\choose1 \times 7\choose6 \times 6! \times 2 3 5 7 5 7 [1,2],[6,7] [1,7] 3\choose1 \times 2! \times 7\choose 5 \times 5! 7! \times (1 + 8 + 3) 9 \choose 7 \times 7! P'= \frac{1}{3} \therefore P = \frac{2}{3} \boxed{12P = 8}","['probability', 'contest-math']"
1,What percent of lighted grids are walkable: a trick-or-treating problem,What percent of lighted grids are walkable: a trick-or-treating problem,,"I am a math teacher that likes to invent fun math problems to explore. Here is one I have been investigating for a little while and have made little progress on because the number of possible $n \times n$ ""grids"" here is always $2^{n^2}$ . I imagine a kid trick-or-treating over a grid of houses on Halloween whose parents tell him he can only keep going so long as one house nearby has their lights on. Each house is either a lit or unlit square on a grid, which can be thought of as $n \times n$ for simplicity's sake, but $n \times m$ explorations are welcome also. The kid can start at any point in the grid, he does not have to begin on the edge. My question(s) are as follows: Given an $n \times n$ grid, is there a closed form for the number of ""walkable"" grids? These are grids where the kid can get to every single house, meaning no house with its lights on is surrounded by houses with their lights off. As $n \to \infty$ , does the percentage of lighted grids that are walkable approach a non-zero value? (My gut feeling here is no, but it would be super cool if it did!) Here is some information I do know: For $2 \times 2$ grids, every one is walkable. For $3 \times 3$ grids, >50% are walkable, because if the middle house has its lights on, the grid is walkable no matter what. For $n \times n$ grids where $n \geq 4$ , a lower bound on the percentage of walkable grids is given by $\frac{100}{(n-2)^2}$ for similar reasons to $3 \times 3$ grids; if all the ""middle"" houses are lighted, the grid is automatically walkable. If this is similar to another problem, please let me know, and if you find out anything cool here, awesome! This is my first post here, so hopefully this will at least spark some curiosity and exploration. I cannot write code very well, but imagine someone could brute force this for small integers for a little bit. Edit: The kid can walk by the same house multiple times as long as it has its lights on. Only interested in visiting lit houses. ""Walkable"" if every lit house is within a king move of another lit house. Daniel Mathias had a worthwhile note simplifying the problem in the comments below, quoted, ""...you have a grid of cells that are either 0 or 1 and you are asking what percentage of these grids have all of the 1's connected.""","I am a math teacher that likes to invent fun math problems to explore. Here is one I have been investigating for a little while and have made little progress on because the number of possible ""grids"" here is always . I imagine a kid trick-or-treating over a grid of houses on Halloween whose parents tell him he can only keep going so long as one house nearby has their lights on. Each house is either a lit or unlit square on a grid, which can be thought of as for simplicity's sake, but explorations are welcome also. The kid can start at any point in the grid, he does not have to begin on the edge. My question(s) are as follows: Given an grid, is there a closed form for the number of ""walkable"" grids? These are grids where the kid can get to every single house, meaning no house with its lights on is surrounded by houses with their lights off. As , does the percentage of lighted grids that are walkable approach a non-zero value? (My gut feeling here is no, but it would be super cool if it did!) Here is some information I do know: For grids, every one is walkable. For grids, >50% are walkable, because if the middle house has its lights on, the grid is walkable no matter what. For grids where , a lower bound on the percentage of walkable grids is given by for similar reasons to grids; if all the ""middle"" houses are lighted, the grid is automatically walkable. If this is similar to another problem, please let me know, and if you find out anything cool here, awesome! This is my first post here, so hopefully this will at least spark some curiosity and exploration. I cannot write code very well, but imagine someone could brute force this for small integers for a little bit. Edit: The kid can walk by the same house multiple times as long as it has its lights on. Only interested in visiting lit houses. ""Walkable"" if every lit house is within a king move of another lit house. Daniel Mathias had a worthwhile note simplifying the problem in the comments below, quoted, ""...you have a grid of cells that are either 0 or 1 and you are asking what percentage of these grids have all of the 1's connected.""",n \times n 2^{n^2} n \times n n \times m n \times n n \to \infty 2 \times 2 3 \times 3 n \times n n \geq 4 \frac{100}{(n-2)^2} 3 \times 3,"['probability', 'combinatorics', 'recreational-mathematics', 'puzzle', 'percolation']"
2,Calculating the probability of placing $N$th in a point-based tournament?,Calculating the probability of placing th in a point-based tournament?,N,"Consider a tournament with $N$ players. The tournament consists of $M$ games. Each game involves all $N$ players. The result of a game is a ranked list of players. Tournament points are awarded based on the per-game rank. At the end of $M$ games, the players are then ranked by the sum total of their tournament points. How can I calculate the probability of given player placing in a given rank at the end of tournament? Clearly the probability is $1 / N$ when all tournament point sums are equal, like at the start of the tournament. The particular case I'm interested in is given an intermediate state of tournament points what is the probability? Example $N = 3$ , $M = 3$ . Tournament points are awarded 10 points, 5 points, and 0 points for 1st, 2nd, and 3rd respectively. Team 1 2 3 A 10 B 5 C 0 What is the probability Team A will place 2nd at the end of the tournament? My Shortcomings Note: I realize now the following doesn't correctly account for ties, my apologies. Manually Counting My naive approach was to manually count all the possibilities. The total possible tournament outcomes $N!^M$ . In this case, M is 2 because there are only 2 remaining games. This gives 36 possible outcomes. By tallying the results, where A places second, I get 8, thus 8 / 36 is the probability that Team A places second. This approach is quickly intractable as N or M increases. Therein lies my problem. I need to be able to calculate the probability without manually counting the frequency in all possible permutations. Conditional Probability I then tried to calculate the probability of one round using conditional probability: Let $S_x$ = the score of team $x$ Then P(A placing 2nd) = $P(S_b > S_a + 5)$ + $P(S_c > S_a + 10)$ - $P(S_b > S_a + 5 \bigcap S_c > S_a + 10)$ $P(S_c > S_a + 10) = 0$ $P(S_b > S_a + 5) = P(S_b = 10) P(S_a = 0 | S_b = 10)$ $P(S_b > S_a + 5) = \frac{1}{3} P(S_a = 0 | S_b = 10)$ $P(S_b > S_a + 5) = \frac{1}{3} \frac{1}{2}$ $P(S_b > S_a + 5) = \frac{1}{6}$ But this approach doesn't easily lend itself to multiple rounds and it is still computationally complex for larger $N$ and $M$ . My intuition is that there's some deeper combinatorics at work here but I couldn't make headway thinking of a solution. I tried to think of a sub-problem that might be composable here. Using the example, Team B outplacing Team C (in any permutation), but I feel like it's not useful because it is the probability of a dependent event.","Consider a tournament with players. The tournament consists of games. Each game involves all players. The result of a game is a ranked list of players. Tournament points are awarded based on the per-game rank. At the end of games, the players are then ranked by the sum total of their tournament points. How can I calculate the probability of given player placing in a given rank at the end of tournament? Clearly the probability is when all tournament point sums are equal, like at the start of the tournament. The particular case I'm interested in is given an intermediate state of tournament points what is the probability? Example , . Tournament points are awarded 10 points, 5 points, and 0 points for 1st, 2nd, and 3rd respectively. Team 1 2 3 A 10 B 5 C 0 What is the probability Team A will place 2nd at the end of the tournament? My Shortcomings Note: I realize now the following doesn't correctly account for ties, my apologies. Manually Counting My naive approach was to manually count all the possibilities. The total possible tournament outcomes . In this case, M is 2 because there are only 2 remaining games. This gives 36 possible outcomes. By tallying the results, where A places second, I get 8, thus 8 / 36 is the probability that Team A places second. This approach is quickly intractable as N or M increases. Therein lies my problem. I need to be able to calculate the probability without manually counting the frequency in all possible permutations. Conditional Probability I then tried to calculate the probability of one round using conditional probability: Let = the score of team Then P(A placing 2nd) = + - But this approach doesn't easily lend itself to multiple rounds and it is still computationally complex for larger and . My intuition is that there's some deeper combinatorics at work here but I couldn't make headway thinking of a solution. I tried to think of a sub-problem that might be composable here. Using the example, Team B outplacing Team C (in any permutation), but I feel like it's not useful because it is the probability of a dependent event.",N M N M 1 / N N = 3 M = 3 N!^M S_x x P(S_b > S_a + 5) P(S_c > S_a + 10) P(S_b > S_a + 5 \bigcap S_c > S_a + 10) P(S_c > S_a + 10) = 0 P(S_b > S_a + 5) = P(S_b = 10) P(S_a = 0 | S_b = 10) P(S_b > S_a + 5) = \frac{1}{3} P(S_a = 0 | S_b = 10) P(S_b > S_a + 5) = \frac{1}{3} \frac{1}{2} P(S_b > S_a + 5) = \frac{1}{6} N M,"['probability', 'combinatorics']"
3,Relationship between the weak law of large numbers and characteristic functions,Relationship between the weak law of large numbers and characteristic functions,,"I'm trying to learn some probability theory atm and got stuck with the following exercise in Durrett's Probability: Theory and Examples : Exercise 3.3.17. Let $X_1, X_2, \ldots $ be i.i.d. with characteristic function $\varphi$. If $\varphi'(0) = ia$ and $S_n = X_1 + \dots + X_n$, then $S_n/n\to a $ in probability. If $S_n/n\to a$ in probability then $\varphi(t/n)^n\to e^{iat}$ as $n\to \infty$. Use 2. and the uniform continuity of $\varphi$ to show that $(\varphi(h)-1)/h \to -ia$ as $h\to 0$. Thus the weak law holds if and only if $\varphi'(0)$ exists. I would really appreciate some help with the third part of this exercise (I don't quite see the connection between $\varphi(t/n)^n$ and $(\varphi(h)-1)/h$, yet). Thanks for your help! =) My thoughts on 1, 2 : I managed to prove 1. using the inequality $\mu\{x\, : \, |x|>u/2\} \le u^{-1} \int_{-u}^u (1-\varphi(t)) \, dt$, where $\mu$ is the pushforward measure of a random variable $X$ and $\varphi$ is its characteristic function. Using the fact that the ch.f. of $S_n/n - a$ is given by $e^{-iat}\varphi(t/n)^n$, this leads to $$P\left[\left|\frac{S_n}n - a\right| > 2/u \right]\le u^{-1} \int_{-u}^u (1-e^{-iat}\varphi(t/n)^n) \, dt$$ Now 1. implies $\varphi(t/n)^n \to e^{iat}$, so the RHS goes to zero as $n\to \infty$ for every fixed $u$. 2.: Using $|e^{i\epsilon t} - 1| \le 2\epsilon |t|$ for small enough $\epsilon>0$: \begin{align} \left|\varphi (t/n)^n - e^{iat}\right|  &= \left| E\left[e^{iS_n/nt} - e^{iat}\right]\right| \\ &\le E\left|e^{i(S_n/n - a)t} - 1\right| \\ &\le 2 \epsilon |t| + 2P[|S_n/n - a|> \epsilon] \end{align} So $\limsup_{n\to\infty}\, \left|\varphi (t/n)^n - e^{iat}\right| \le 2\epsilon |t|$ and since $\epsilon>0$ was arbitrary (apart from being small) this proves $\varphi(t/n)^n\to e^{iat}$.","I'm trying to learn some probability theory atm and got stuck with the following exercise in Durrett's Probability: Theory and Examples : Exercise 3.3.17. Let $X_1, X_2, \ldots $ be i.i.d. with characteristic function $\varphi$. If $\varphi'(0) = ia$ and $S_n = X_1 + \dots + X_n$, then $S_n/n\to a $ in probability. If $S_n/n\to a$ in probability then $\varphi(t/n)^n\to e^{iat}$ as $n\to \infty$. Use 2. and the uniform continuity of $\varphi$ to show that $(\varphi(h)-1)/h \to -ia$ as $h\to 0$. Thus the weak law holds if and only if $\varphi'(0)$ exists. I would really appreciate some help with the third part of this exercise (I don't quite see the connection between $\varphi(t/n)^n$ and $(\varphi(h)-1)/h$, yet). Thanks for your help! =) My thoughts on 1, 2 : I managed to prove 1. using the inequality $\mu\{x\, : \, |x|>u/2\} \le u^{-1} \int_{-u}^u (1-\varphi(t)) \, dt$, where $\mu$ is the pushforward measure of a random variable $X$ and $\varphi$ is its characteristic function. Using the fact that the ch.f. of $S_n/n - a$ is given by $e^{-iat}\varphi(t/n)^n$, this leads to $$P\left[\left|\frac{S_n}n - a\right| > 2/u \right]\le u^{-1} \int_{-u}^u (1-e^{-iat}\varphi(t/n)^n) \, dt$$ Now 1. implies $\varphi(t/n)^n \to e^{iat}$, so the RHS goes to zero as $n\to \infty$ for every fixed $u$. 2.: Using $|e^{i\epsilon t} - 1| \le 2\epsilon |t|$ for small enough $\epsilon>0$: \begin{align} \left|\varphi (t/n)^n - e^{iat}\right|  &= \left| E\left[e^{iS_n/nt} - e^{iat}\right]\right| \\ &\le E\left|e^{i(S_n/n - a)t} - 1\right| \\ &\le 2 \epsilon |t| + 2P[|S_n/n - a|> \epsilon] \end{align} So $\limsup_{n\to\infty}\, \left|\varphi (t/n)^n - e^{iat}\right| \le 2\epsilon |t|$ and since $\epsilon>0$ was arbitrary (apart from being small) this proves $\varphi(t/n)^n\to e^{iat}$.",,['probability-theory']
4,Calculate probability of flipping coins with probabilities dependent on previous results,Calculate probability of flipping coins with probabilities dependent on previous results,,"Let's say I have a biased coin that has a probability of 0.2 for heads and 0.8 for tails. If you flip it 6 times without getting a heads however, the probability of heads increases by 0.2 per non-heads flip afterwards, so the 7th flip without heads will have a probability of 0.4 for heads and 0.6 for tails, the 8th flip will have a probability of 0.6 for heads, and the 10th flip will have a guaranteed chance for heads. If you get heads at any point, the probability resets back to 0.2, and you would have to roll 6 tails again to raise the probability. In this scenario, what is the best way to calculate the probability of getting X amount of heads with an arbitrary number of coin flips? For example, how might I calculate the probability of getting 2 heads from 10 flips with this coin? So far, I've tried brute-force simulation and dynamic programming, which certainly work, but I just wondered if there was a more direct and efficient approach. I've looked into using discrete Fourier transforms on rolling multiple dice, but I've found that the problem doesn't exactly translate one-to-one with the one here. I'm not too knowledgeable about probability and math in general, so I hope someone more knowledgeable could offer some suggestions or insights. Thanks for the help!","Let's say I have a biased coin that has a probability of 0.2 for heads and 0.8 for tails. If you flip it 6 times without getting a heads however, the probability of heads increases by 0.2 per non-heads flip afterwards, so the 7th flip without heads will have a probability of 0.4 for heads and 0.6 for tails, the 8th flip will have a probability of 0.6 for heads, and the 10th flip will have a guaranteed chance for heads. If you get heads at any point, the probability resets back to 0.2, and you would have to roll 6 tails again to raise the probability. In this scenario, what is the best way to calculate the probability of getting X amount of heads with an arbitrary number of coin flips? For example, how might I calculate the probability of getting 2 heads from 10 flips with this coin? So far, I've tried brute-force simulation and dynamic programming, which certainly work, but I just wondered if there was a more direct and efficient approach. I've looked into using discrete Fourier transforms on rolling multiple dice, but I've found that the problem doesn't exactly translate one-to-one with the one here. I'm not too knowledgeable about probability and math in general, so I hope someone more knowledgeable could offer some suggestions or insights. Thanks for the help!",,"['probability', 'dice', 'programming', 'dynamic-programming']"
5,Applications of the Triangle Inequality Proof,Applications of the Triangle Inequality Proof,,"I am trying to understand the proof that the forecast error for an ARIMA(p,d,q) model becomes larger when the forecast period grows larger. Suppose I take the ARIMA(p, q, d) : $$\nabla^d y_t = \sum_{i=1}^p \phi_i \nabla^d y_{t-i} + \sum_{j=1}^q \theta_j e_{t-j} + e_t$$ $$\nabla^d y_t = \phi_1 \nabla^d y_{t-1} + \dots + \phi_p \nabla^d y_{t-p} + e_t - \theta_1 e_{t-1} - \dots - \theta_q e_{t-q}$$ $$y_t - y_{t-d} = \phi_1 (y_{t-1} - y_{t-d-1}) + \dots + \phi_p (y_{t-p} - y_{t-d-p}) + e_t - \theta_1 e_{t-1} - \dots - \theta_q e_{t-q}$$ $$y_t = c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p} + e_t - \theta_1 e_{t-1} - \dots - \theta_q e_{t-q} + y_{t-d}$$ $$y_t = c + \sum_{i=1}^p \phi_i y_{t-i} + \sum_{j=1}^q \theta_j e_{t-j} + e_t + y_{t-d}$$ where: $y_t$ : the value of the time series at time t. $\nabla^d$ : the differencing operator, which takes the difference between the value of the time series at time t and the value at time t-d. The parameter d determines the order of differencing. $\phi_1, \dots, \phi_p$ : the autoregressive coefficients, which specify the dependence of the current value of the time series on its past values. The parameter p determines the order of the autoregressive model. $e_t$ : the error term at time t, which represents the deviation of the actual value of the time series from the predicted value based on past values. $\theta_1, \dots, \theta_q$ : the moving average coefficients, which specify the dependence of the current value of the time series on the past error terms. The parameter q determines the order of the moving average model. $y_{t-d}$ : the value of the time series at time t-d, which is used when performing differencing with parameter d. $c$ : a constant term, which represents the mean value of the time series when all other terms are zero. This term is optional and can be omitted from the model. $c = y_d - \phi_1 y_{d-1} - \dots - \phi_p y_{d-p}$ is a constant. The forecast at time $t+h$ , where $h$ is the number of time steps ahead, can be written as: $$\hat{y}{t+h} = c + \phi_1 y{t+h-1} + \cdots + \phi_p y_{t+h-p} - \theta_1 e_{t+h-1} - \cdots - \theta_q e_{t+h-q}$$ where: $y_t$ is the observed value at time $t$ $c$ is the constant term $\phi_1,\ldots,\phi_p$ are the autoregressive parameters $\theta_1,\ldots,\theta_q$ are the moving average parameters $e_t$ is the error term at time $t$ . The prediction error at time $t+h$ is then given by: $$e_{t+h} = y_{t+h} - \hat{y}_{t+h}$$ Here is where I get confused: Suppose we take the absolute value of this error and applying the triangle inequality -  we get: $$|e_{t+h}| = |y_{t+h} - \hat{y}{t+h}| \leq |y{t+h} - y_{t+h-1}| + |y_{t+h-1} - \hat{y}_{t+h-1}|$$ The first term on the right-hand side is simply the size of the change in the observed values between time $t+h$ and time $t+h-1$ . The second term is the prediction error at time $t+h-1$ . Substituting in the expression for $\hat{y}_{t+h-1}$ and taking the absolute value, we get: $$|y_{t+h-1} - \hat{y}{t+h-1}| = |e{t+h-1}| \leq |y_{t+h-1} - y_{t+h-2}| + |y_{t+h-2} - \hat{y}_{t+h-2}|$$ Repeating this process for $h-1$ steps, we get: $$|e_{t+h}| \leq |y_{t+h} - y_{t+h-1}| + \cdots + |y_{t+1} - y_t| + |e_{t+1}|$$ Now, assuming that the errors are uncorrelated and have constant variance, the expected value of the square of the prediction error at time $t+h$ is given by: $$\operatorname{E}(|e_{t+h}|^2) = h \sigma^2$$ where $\sigma^2$ is the variance of the error term. Thus, as the forecast horizon $h$ increases, the expected value of the prediction error grows linearly with $h$ . I am not sure that I fully understand how the triangle identity can be used in this proof - can someone please help me understand?","I am trying to understand the proof that the forecast error for an ARIMA(p,d,q) model becomes larger when the forecast period grows larger. Suppose I take the ARIMA(p, q, d) : where: : the value of the time series at time t. : the differencing operator, which takes the difference between the value of the time series at time t and the value at time t-d. The parameter d determines the order of differencing. : the autoregressive coefficients, which specify the dependence of the current value of the time series on its past values. The parameter p determines the order of the autoregressive model. : the error term at time t, which represents the deviation of the actual value of the time series from the predicted value based on past values. : the moving average coefficients, which specify the dependence of the current value of the time series on the past error terms. The parameter q determines the order of the moving average model. : the value of the time series at time t-d, which is used when performing differencing with parameter d. : a constant term, which represents the mean value of the time series when all other terms are zero. This term is optional and can be omitted from the model. is a constant. The forecast at time , where is the number of time steps ahead, can be written as: where: is the observed value at time is the constant term are the autoregressive parameters are the moving average parameters is the error term at time . The prediction error at time is then given by: Here is where I get confused: Suppose we take the absolute value of this error and applying the triangle inequality -  we get: The first term on the right-hand side is simply the size of the change in the observed values between time and time . The second term is the prediction error at time . Substituting in the expression for and taking the absolute value, we get: Repeating this process for steps, we get: Now, assuming that the errors are uncorrelated and have constant variance, the expected value of the square of the prediction error at time is given by: where is the variance of the error term. Thus, as the forecast horizon increases, the expected value of the prediction error grows linearly with . I am not sure that I fully understand how the triangle identity can be used in this proof - can someone please help me understand?","\nabla^d y_t = \sum_{i=1}^p \phi_i \nabla^d y_{t-i} + \sum_{j=1}^q \theta_j e_{t-j} + e_t \nabla^d y_t = \phi_1 \nabla^d y_{t-1} + \dots + \phi_p \nabla^d y_{t-p} + e_t - \theta_1 e_{t-1} - \dots - \theta_q e_{t-q} y_t - y_{t-d} = \phi_1 (y_{t-1} - y_{t-d-1}) + \dots + \phi_p (y_{t-p} - y_{t-d-p}) + e_t - \theta_1 e_{t-1} - \dots - \theta_q e_{t-q} y_t = c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p} + e_t - \theta_1 e_{t-1} - \dots - \theta_q e_{t-q} + y_{t-d} y_t = c + \sum_{i=1}^p \phi_i y_{t-i} + \sum_{j=1}^q \theta_j e_{t-j} + e_t + y_{t-d} y_t \nabla^d \phi_1, \dots, \phi_p e_t \theta_1, \dots, \theta_q y_{t-d} c c = y_d - \phi_1 y_{d-1} - \dots - \phi_p y_{d-p} t+h h \hat{y}{t+h} = c + \phi_1 y{t+h-1} + \cdots + \phi_p y_{t+h-p} - \theta_1 e_{t+h-1} - \cdots - \theta_q e_{t+h-q} y_t t c \phi_1,\ldots,\phi_p \theta_1,\ldots,\theta_q e_t t t+h e_{t+h} = y_{t+h} - \hat{y}_{t+h} |e_{t+h}| = |y_{t+h} - \hat{y}{t+h}| \leq |y{t+h} - y_{t+h-1}| + |y_{t+h-1} - \hat{y}_{t+h-1}| t+h t+h-1 t+h-1 \hat{y}_{t+h-1} |y_{t+h-1} - \hat{y}{t+h-1}| = |e{t+h-1}| \leq |y_{t+h-1} - y_{t+h-2}| + |y_{t+h-2} - \hat{y}_{t+h-2}| h-1 |e_{t+h}| \leq |y_{t+h} - y_{t+h-1}| + \cdots + |y_{t+1} - y_t| + |e_{t+1}| t+h \operatorname{E}(|e_{t+h}|^2) = h \sigma^2 \sigma^2 h h","['probability', 'geometry']"
6,Distribution of Difference of Independent Inverse Gaussians,Distribution of Difference of Independent Inverse Gaussians,,"Suppose, $X_1$ and $X_2$ are independent inverse Gaussian random variables with parameters $(\mu, \lambda_1)$ and $(\mu, \lambda_2)$ , respectively.  (See Wikipedia for the notation). I am interested in the following probability (and whether or not it has a closed form solution): $$ P(X_1 < X_2) = \int_0^\infty \int_0^y \exp \left(- \frac{\lambda_1(x-\mu)^2}{2\mu^2x} - \frac{\lambda_2(y-\mu)^2}{2\mu^2y} \right) \sqrt{\frac{\lambda_1 \lambda_2}{(2\pi)^2 x^3 y^3}} dx dy$$ $$= \int_0^\infty e^{-\frac{\lambda_2(y-\mu)^2}{2\mu^2y}}\left(\Phi \left(\sqrt{\frac{\lambda_1}{y}}\left(\frac{y}{\mu} -1\right) \right) + e^{2\lambda_1/\mu}\Phi \left(-\sqrt{\frac{\lambda_1}{y}}\left(\frac{y}{\mu} +1\right) \right) \right) \sqrt{\frac{\lambda_2}{2\pi y^3}}dy$$ where $\Phi$ is the cdf (Cumulative distribution function), of a standard normal random variable. I have no idea how to deal with this integral.  The reason I am interested in its closed form is due to the fact that for the case $\mu = 0$ , there is a very nice closed form solution relating to the hitting times of two independent Brownian motions ( see here ). I have tried consulting this table for hints of how to deal with the integral of the erf function , against some other complicated term, to no avail. Any help would be massively appreciated!","Suppose, and are independent inverse Gaussian random variables with parameters and , respectively.  (See Wikipedia for the notation). I am interested in the following probability (and whether or not it has a closed form solution): where is the cdf (Cumulative distribution function), of a standard normal random variable. I have no idea how to deal with this integral.  The reason I am interested in its closed form is due to the fact that for the case , there is a very nice closed form solution relating to the hitting times of two independent Brownian motions ( see here ). I have tried consulting this table for hints of how to deal with the integral of the erf function , against some other complicated term, to no avail. Any help would be massively appreciated!","X_1 X_2 (\mu, \lambda_1) (\mu, \lambda_2)  P(X_1 < X_2) = \int_0^\infty \int_0^y \exp \left(- \frac{\lambda_1(x-\mu)^2}{2\mu^2x} - \frac{\lambda_2(y-\mu)^2}{2\mu^2y} \right) \sqrt{\frac{\lambda_1 \lambda_2}{(2\pi)^2 x^3 y^3}} dx dy = \int_0^\infty e^{-\frac{\lambda_2(y-\mu)^2}{2\mu^2y}}\left(\Phi \left(\sqrt{\frac{\lambda_1}{y}}\left(\frac{y}{\mu} -1\right) \right) + e^{2\lambda_1/\mu}\Phi \left(-\sqrt{\frac{\lambda_1}{y}}\left(\frac{y}{\mu} +1\right) \right) \right) \sqrt{\frac{\lambda_2}{2\pi y^3}}dy \Phi \mu = 0","['probability', 'probability-distributions', 'definite-integrals', 'normal-distribution']"
7,Counting paths to winning distribution,Counting paths to winning distribution,,"In a tournament, there are $2n$ teams, $n \in \mathbb{N}$ . Each round, the teams pair up. This means that after each round, $n$ teams win and $n$ teams lose. If we are given a scorecard containing each team’s number of wins after $r$ rounds, we would like to determine all possible ways that this win outcome could have occurred. In essence, we would like to find all possible ways of representing a vector $(w_0, w_1, … w_{2n - 1}), w_i \in \mathbb{N_0}$ as a sum of vectors of the form $(r_0, r_1, … r_{2n-1}), r \in \{0, 1\}$ where $\sum_{i=0}^{2n-1} r_i = n$ . How might I go about achieving this? We tried representing each component of the vector as a node on a graph having a certain value. We were also interested in barycentric coordinates or other coordinate systems. I also tried more of a pure linear algebra approach and represented it as a system of linear diophantine equations, yet we still haven't been able to solve this one.","In a tournament, there are teams, . Each round, the teams pair up. This means that after each round, teams win and teams lose. If we are given a scorecard containing each team’s number of wins after rounds, we would like to determine all possible ways that this win outcome could have occurred. In essence, we would like to find all possible ways of representing a vector as a sum of vectors of the form where . How might I go about achieving this? We tried representing each component of the vector as a node on a graph having a certain value. We were also interested in barycentric coordinates or other coordinate systems. I also tried more of a pure linear algebra approach and represented it as a system of linear diophantine equations, yet we still haven't been able to solve this one.","2n n \in \mathbb{N} n n r (w_0, w_1, … w_{2n - 1}), w_i \in \mathbb{N_0} (r_0, r_1, … r_{2n-1}), r \in \{0, 1\} \sum_{i=0}^{2n-1} r_i = n","['linear-algebra', 'probability', 'combinatorics', 'graph-theory', 'vector-spaces']"
8,Consistent or inconsistent estimator,Consistent or inconsistent estimator,,"If $\hat{\theta}_n$ is an estimator for the parameter $\theta$ , then the two sufficient conditions to ensure consistency of $\hat{\theta}_n$ are: Bias( $\hat{\theta}_n)\to 0$ and Var $(\hat{\theta}_n)\to 0$ , then we will have $\lim_{n\to\infty}Pr(|\hat{\theta}_n-\theta|>\varepsilon)=0, \forall\varepsilon>0$ . Now suppose $X_1,\ldots X_n$ be iid samples drawn from the Bernoulli distribution with parameter $p$ . Let $\hat{p}=\bar{X}$ be the estimator for $p$ , where $\bar{X}$ is the sample mean. It is clear that $Bias(\hat p)=E(\hat{p})-p=0$ and $Var(\hat{p})=\frac{Var(X_1)}{n}\to 0$ , as $n\to\infty$ . We should expect that $\hat{p}$ is a consistent estimator. However, from the definition of consistency, if I choose $\varepsilon=0.25, p=0.5$ and $X_i=1$ for all i. Then $\hat{p}= 1$ and $$Pr(|\hat{p}-p|>\varepsilon) = Pr(1-0.5>0.25) = 1.$$ This looks like a contradiction to the sufficient conditions above. Can anyone let me know about the mistakes in my argument? Thank you!","If is an estimator for the parameter , then the two sufficient conditions to ensure consistency of are: Bias( and Var , then we will have . Now suppose be iid samples drawn from the Bernoulli distribution with parameter . Let be the estimator for , where is the sample mean. It is clear that and , as . We should expect that is a consistent estimator. However, from the definition of consistency, if I choose and for all i. Then and This looks like a contradiction to the sufficient conditions above. Can anyone let me know about the mistakes in my argument? Thank you!","\hat{\theta}_n \theta \hat{\theta}_n \hat{\theta}_n)\to 0 (\hat{\theta}_n)\to 0 \lim_{n\to\infty}Pr(|\hat{\theta}_n-\theta|>\varepsilon)=0, \forall\varepsilon>0 X_1,\ldots X_n p \hat{p}=\bar{X} p \bar{X} Bias(\hat p)=E(\hat{p})-p=0 Var(\hat{p})=\frac{Var(X_1)}{n}\to 0 n\to\infty \hat{p} \varepsilon=0.25, p=0.5 X_i=1 \hat{p}= 1 Pr(|\hat{p}-p|>\varepsilon) = Pr(1-0.5>0.25) = 1.","['probability', 'statistics', 'parameter-estimation', 'probability-limit-theorems']"
9,Probability of even number of dice result [closed],Probability of even number of dice result [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question Suppose we roll a dice (with 6 sides) 21 times. What are the odds of getting either 1 or 2 an even amount of times? I tried to calculate it by representing the number of times we get 1 or two as a binomial variable $B(n=21, p=\frac{1}{3})$ , And summing the probability of getting $0, 2,\ldots, 20$ and I get the probability is $0.5$ , which from what I saw in a different way of calculation is wrong. Where did my method go wrong?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question Suppose we roll a dice (with 6 sides) 21 times. What are the odds of getting either 1 or 2 an even amount of times? I tried to calculate it by representing the number of times we get 1 or two as a binomial variable , And summing the probability of getting and I get the probability is , which from what I saw in a different way of calculation is wrong. Where did my method go wrong?","B(n=21, p=\frac{1}{3}) 0, 2,\ldots, 20 0.5",['probability']
10,Asymptotic density of an infinite union of subgroups,Asymptotic density of an infinite union of subgroups,,"Let $1 < a_1 < a_2 < a_3 <{} ...$ be a sequence of integers. For a subset $A \subset \Bbb Z$ , denote by $d(A)$ its natural density (if it exists). Is it true that $$  \lim_{N \to +\infty}  d\Big( \bigcup_{i \leq N} a_i \Bbb Z \Big)  =  d\Big( \bigcup_{i = 1}^{\infty} a_i \Bbb Z \Big)$$ ? The density of the finite union $\bigcup_{i \leq N} a_i \Bbb Z$ can be computed via the inclusion-exclusion principle, but it is not clear how it would relate to the infinite union.","Let be a sequence of integers. For a subset , denote by its natural density (if it exists). Is it true that ? The density of the finite union can be computed via the inclusion-exclusion principle, but it is not clear how it would relate to the infinite union.","1 < a_1 < a_2 < a_3 <{} ... A \subset \Bbb Z d(A)  
\lim_{N \to +\infty} 
d\Big( \bigcup_{i \leq N} a_i \Bbb Z \Big) 
= 
d\Big( \bigcup_{i = 1}^{\infty} a_i \Bbb Z \Big) \bigcup_{i \leq N} a_i \Bbb Z","['probability', 'number-theory', 'elementary-number-theory', 'arithmetic']"
11,Relationship between measure theory and quantification,Relationship between measure theory and quantification,,"In a 1978 paper published by David P. Ellerman and Gian-Carlo Rota, the duo discuss the relationship(s) exhibited by measure theory, probability theory, and logic paying special attention to how these concepts apply to describing the nature of quantification via an algebraic perspective. I was hoping that someone who is familiar with the paper (or for someone whose research area would permit them an understanding) to elucidate its underlying concepts. In particular, in paragraph two on page 3 (of 21) in the attached pdf , the authors state ""It has long been known (e.g., Wright [12]) that there is an analogy between averaging operators such as the conditional expectation operators of probability theory and the algebraic (existential) quantifiers in Halmos"" theory of polyadic algebras (Halmos [5]) or the cylindrifications used by Tarski and his co-workers in the theory of cylindric algebras (Henkin, Monk, and Tarski [7]). All these operators satisfy an averaging condition which has the general form: $A(f) \cdot A(g) = A \cdot (f \cdot A(g))$ . We will provide some theoretical underpinning for this analogy by constructing the logical quantifiers using an abstract rendition of conditional expectation operators on a ring of simple random variables."" Can someone please elaborate upon how the logical quantifiers are related to conditional expectation operators as the term is used here. Additionally, can anyone point me towards some good resources for pursuing this line of inquiry further (lectures, videos, books, etc.)?","In a 1978 paper published by David P. Ellerman and Gian-Carlo Rota, the duo discuss the relationship(s) exhibited by measure theory, probability theory, and logic paying special attention to how these concepts apply to describing the nature of quantification via an algebraic perspective. I was hoping that someone who is familiar with the paper (or for someone whose research area would permit them an understanding) to elucidate its underlying concepts. In particular, in paragraph two on page 3 (of 21) in the attached pdf , the authors state ""It has long been known (e.g., Wright [12]) that there is an analogy between averaging operators such as the conditional expectation operators of probability theory and the algebraic (existential) quantifiers in Halmos"" theory of polyadic algebras (Halmos [5]) or the cylindrifications used by Tarski and his co-workers in the theory of cylindric algebras (Henkin, Monk, and Tarski [7]). All these operators satisfy an averaging condition which has the general form: . We will provide some theoretical underpinning for this analogy by constructing the logical quantifiers using an abstract rendition of conditional expectation operators on a ring of simple random variables."" Can someone please elaborate upon how the logical quantifiers are related to conditional expectation operators as the term is used here. Additionally, can anyone point me towards some good resources for pursuing this line of inquiry further (lectures, videos, books, etc.)?",A(f) \cdot A(g) = A \cdot (f \cdot A(g)),"['probability', 'abstract-algebra', 'measure-theory', 'logic', 'algebraic-logic']"
12,"How can I show that $\bigcap_{n\geq 0} \sigma(X_n,X_{n+1}...)=\sigma(X)$?",How can I show that ?,"\bigcap_{n\geq 0} \sigma(X_n,X_{n+1}...)=\sigma(X)","Let $X$ be a uniformly distributed random variable in $[0,1]$ and define $X_n:=\lfloor 2^nX\rfloor 2^{-n}$ for all $n$ . I want to show that $\bigcap_{n\geq 0} \sigma(X_n,X_{n+1}...)=\sigma(X)$ . My idea was the following: Proof : $\subseteq$ Let me remark that for all $n$ , $X_n$ is $\sigma(X)$ measurable since $X_n=f(X)$ where $f(x)=\lfloor 2^nx\rfloor 2^{-n}$ is measurable. Therefore $\sigma(X_n,X_{n+1},...)\subset \sigma(X)$ for all $n$ and hence $\bigcap_{n\geq 0} \sigma(X_n,X_{n+1}...)\subseteq\sigma(X)$ . $\supseteq$ Here let me remark that $$\frac{2^nX}{2^n}\leq X_n\leq \frac{2^nX+1}{2^n}$$ where $\frac{2^nX}{2^n}\rightarrow X$ and $\frac{2^nX+1}{2^n}\rightarrow X$ , so in particular $X_n\rightarrow X$ . But since the limit exists we also know that $\limsup_{n\rightarrow \infty}X_n=X$ but we know that $\limsup_n$ is $\sigma(X_n,X_{n+1},...)$ measurable. But then this implies that $X$ is $\sigma(X_n,X_{n+1},...)$ measurable for all $n$ and hence $\bigcap_{n\geq 0} \sigma(X_n,X_{n+1}...)\supseteq\sigma(X)$ . Does this work?","Let be a uniformly distributed random variable in and define for all . I want to show that . My idea was the following: Proof : Let me remark that for all , is measurable since where is measurable. Therefore for all and hence . Here let me remark that where and , so in particular . But since the limit exists we also know that but we know that is measurable. But then this implies that is measurable for all and hence . Does this work?","X [0,1] X_n:=\lfloor 2^nX\rfloor 2^{-n} n \bigcap_{n\geq 0} \sigma(X_n,X_{n+1}...)=\sigma(X) \subseteq n X_n \sigma(X) X_n=f(X) f(x)=\lfloor 2^nx\rfloor 2^{-n} \sigma(X_n,X_{n+1},...)\subset \sigma(X) n \bigcap_{n\geq 0} \sigma(X_n,X_{n+1}...)\subseteq\sigma(X) \supseteq \frac{2^nX}{2^n}\leq X_n\leq \frac{2^nX+1}{2^n} \frac{2^nX}{2^n}\rightarrow X \frac{2^nX+1}{2^n}\rightarrow X X_n\rightarrow X \limsup_{n\rightarrow \infty}X_n=X \limsup_n \sigma(X_n,X_{n+1},...) X \sigma(X_n,X_{n+1},...) n \bigcap_{n\geq 0} \sigma(X_n,X_{n+1}...)\supseteq\sigma(X)","['probability', 'probability-theory', 'solution-verification', 'stochastic-calculus']"
13,Optimal Strategies for Coin-Flipping Games,Optimal Strategies for Coin-Flipping Games,,"My nephew thought of the following problem over the holidays: I have 100 coins, half these coins are currently face up heads, the other half are currently face up tails. Currently, my score is 0. Each coin has a 0.5 probability of landing on heads and a 0.5 probability of landing on tails Round 1: I randomly generate an integer ""n"" between 0 and 100, select ""n"" coins and flip them - naturally, some change sides, some don't. the coins that weren't selected stay as they are. After Round 1 - each coins that is now facing the opposite side gives me 1 point. And each coin that is facing the same way takes away 1 point. Record the total number of points. Round 2: I now again generate another integer ""n""  and flip these ""n"" coins. For coins that were not previously selected but were now selected if they land on the other side, I get 1 point but if they land on the same side, I lose 1 point. And all coins that were not selected in Round 2 automatically deduct 1 point each. Record total points from Round 2 and add to total points from Round 1. Repeat for many rounds Now, my nephew's game lead me to the following questions: After Round 2, what is my expected score? How many Rounds do I need to play to get some expected score of ""x""? On average, what is the optimal number of rounds I should play to give me the highest possible score (before my score starts to decrease and decrease)? My feeling is that initially it it makes sense to play this game for a few rounds regardless - but then, based on your score, there is probably an probabilistic optimal number of rounds to stop playing this game. I think that maybe you can use a Markov Chain in where states are either the ""score of the game"" (e.g. probability of transitioning to -5 points given you are at -1 point) and then estimate the ""time to absorption"" for different questions based on simulations or solving this analytically - but I am not sure. Any ideas?","My nephew thought of the following problem over the holidays: I have 100 coins, half these coins are currently face up heads, the other half are currently face up tails. Currently, my score is 0. Each coin has a 0.5 probability of landing on heads and a 0.5 probability of landing on tails Round 1: I randomly generate an integer ""n"" between 0 and 100, select ""n"" coins and flip them - naturally, some change sides, some don't. the coins that weren't selected stay as they are. After Round 1 - each coins that is now facing the opposite side gives me 1 point. And each coin that is facing the same way takes away 1 point. Record the total number of points. Round 2: I now again generate another integer ""n""  and flip these ""n"" coins. For coins that were not previously selected but were now selected if they land on the other side, I get 1 point but if they land on the same side, I lose 1 point. And all coins that were not selected in Round 2 automatically deduct 1 point each. Record total points from Round 2 and add to total points from Round 1. Repeat for many rounds Now, my nephew's game lead me to the following questions: After Round 2, what is my expected score? How many Rounds do I need to play to get some expected score of ""x""? On average, what is the optimal number of rounds I should play to give me the highest possible score (before my score starts to decrease and decrease)? My feeling is that initially it it makes sense to play this game for a few rounds regardless - but then, based on your score, there is probably an probabilistic optimal number of rounds to stop playing this game. I think that maybe you can use a Markov Chain in where states are either the ""score of the game"" (e.g. probability of transitioning to -5 points given you are at -1 point) and then estimate the ""time to absorption"" for different questions based on simulations or solving this analytically - but I am not sure. Any ideas?",,['probability']
14,Is this probability always $1$?,Is this probability always ?,1,"Let $X$ be a random variable, uniformly distributed in the interval $(0,1)$ . Compute $P(X(1-X) < \frac14)$ . Attempt: $$P(X(1-X)<\tfrac14)=P(X-X^2<\tfrac14)=P(X^2-X+\tfrac14>0)=P((X-\tfrac12)^2>0)$$ Isn't this always $1$ though? I guess it's a trick question? (Because the rest of the problems basically steer you towards working with the density functions)","Let be a random variable, uniformly distributed in the interval . Compute . Attempt: Isn't this always though? I guess it's a trick question? (Because the rest of the problems basically steer you towards working with the density functions)","X (0,1) P(X(1-X) < \frac14) P(X(1-X)<\tfrac14)=P(X-X^2<\tfrac14)=P(X^2-X+\tfrac14>0)=P((X-\tfrac12)^2>0) 1","['probability', 'statistics', 'solution-verification']"
15,Question about the proof of compactness of events in Bernoulli measure,Question about the proof of compactness of events in Bernoulli measure,,"The problem is presented in Example 1.63 (page 29) of ""Probability Theorey"" 3rd edition by Prof. Achim Klenke. We construct a measure for an infinitely often repeated random experiment with finitely many possible outcomes. Notations: Let $E$ be the set of possible outcomes. For a fixed realization of the repeated experiment, let $\omega_{1},\omega_{2},\ldots \in E$ be the observed outcomes. Hence the space of all possible outcomes of the repeated experiment is $\Omega = E^{\mathbb{N}}$ . Define the set of all sequences whose first $n$ values are $\omega_{1},\ldots,\omega_{n}$ : \begin{equation*} [\omega_1, \ldots, \omega_n]:=\{\omega^{\prime} \in \Omega: \omega_i^{\prime}=\omega_i \text { for any } i=1, \ldots, n\} \end{equation*} Let $\mathcal{A}_0=\{\emptyset\}$ . For $n \in N$ , define the class of cylinder sets that depend only on the first $n$ coordinates \begin{equation*} \mathcal{A}_n:=\{[\omega_1, \ldots, \omega_n]: \omega_1, \ldots, \omega_n \in E\}, \end{equation*} and let $\mathcal{A} :=\bigcup_{n=0}^{\infty} \mathcal{A}_n$ . Purpose : Let $A, A_1, A_2, \ldots \in \mathcal{A}$ and $A \subset \bigcup_{n=1}^{\infty} A_n$ . We want to show that there exists an $N \in \mathbb{N}$ such that \begin{equation*}     A \subset \bigcup_{n=1}^N A_n . \end{equation*} Proof Let $B_n:=A \backslash \bigcup_{i=1}^n A_i$ . We assume $B_n \neq \emptyset$ for all $n \in N$ in order to get a contradiction. By Dirichlet's pigeonhole principle (recall that $E$ is finite), we can choose $\omega_1 \in E$ such that $\left[\omega_1\right] \cap B_n \neq \emptyset$ for infinitely many $n \in N$ . Since $B_1 \supset B_2 \supset \cdots$ , we obtain \begin{equation*}     [\omega_1] \cap B_n \neq \emptyset \quad \text { for all } n \in N . \end{equation*} Successively choose $\omega_2, \omega_3, \ldots \in E$ in such a way that \begin{equation*}     [\omega_1, \ldots, \omega_k] \cap B_n \neq \emptyset \quad \text { for all } k, n \in N \end{equation*} Question: Can we now directly conclude that \begin{equation*}     \omega= (\omega_1, \omega_2, \ldots) \in B_{n} \text{ for all } n \in \mathbb{N}  \end{equation*} Is there any danger of it?","The problem is presented in Example 1.63 (page 29) of ""Probability Theorey"" 3rd edition by Prof. Achim Klenke. We construct a measure for an infinitely often repeated random experiment with finitely many possible outcomes. Notations: Let be the set of possible outcomes. For a fixed realization of the repeated experiment, let be the observed outcomes. Hence the space of all possible outcomes of the repeated experiment is . Define the set of all sequences whose first values are : Let . For , define the class of cylinder sets that depend only on the first coordinates and let . Purpose : Let and . We want to show that there exists an such that Proof Let . We assume for all in order to get a contradiction. By Dirichlet's pigeonhole principle (recall that is finite), we can choose such that for infinitely many . Since , we obtain Successively choose in such a way that Question: Can we now directly conclude that Is there any danger of it?","E \omega_{1},\omega_{2},\ldots \in E \Omega = E^{\mathbb{N}} n \omega_{1},\ldots,\omega_{n} \begin{equation*}
[\omega_1, \ldots, \omega_n]:=\{\omega^{\prime} \in \Omega: \omega_i^{\prime}=\omega_i \text { for any } i=1, \ldots, n\}
\end{equation*} \mathcal{A}_0=\{\emptyset\} n \in N n \begin{equation*}
\mathcal{A}_n:=\{[\omega_1, \ldots, \omega_n]: \omega_1, \ldots, \omega_n \in E\},
\end{equation*} \mathcal{A} :=\bigcup_{n=0}^{\infty} \mathcal{A}_n A, A_1, A_2, \ldots \in \mathcal{A} A \subset \bigcup_{n=1}^{\infty} A_n N \in \mathbb{N} \begin{equation*}
    A \subset \bigcup_{n=1}^N A_n .
\end{equation*} B_n:=A \backslash \bigcup_{i=1}^n A_i B_n \neq \emptyset n \in N E \omega_1 \in E \left[\omega_1\right] \cap B_n \neq \emptyset n \in N B_1 \supset B_2 \supset \cdots \begin{equation*}
    [\omega_1] \cap B_n \neq \emptyset \quad \text { for all } n \in N .
\end{equation*} \omega_2, \omega_3, \ldots \in E \begin{equation*}
    [\omega_1, \ldots, \omega_k] \cap B_n \neq \emptyset \quad \text { for all } k, n \in N
\end{equation*} \begin{equation*}
    \omega= (\omega_1, \omega_2, \ldots) \in B_{n} \text{ for all } n \in \mathbb{N} 
\end{equation*}","['probability', 'probability-theory', 'measure-theory', 'induction', 'transfinite-induction']"
16,Probability Theory by Klenke theorem 15.9. That a finite measure is characterized by its characteristic function. [closed],Probability Theory by Klenke theorem 15.9. That a finite measure is characterized by its characteristic function. [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 1 year ago . Improve this question The theorem 15.9 (Characteristic function) says: A finite measure $\mu \in \mathcal{M}_{f}(\mathbb{R}^{d})$ (here this symbol just means finite measures on $\mathbb{R}^{d}$ ) is characterized by its characteristic function. Proof: Let $\mu_{1}, \mu_{2} \in \mathcal{M}_{f}(\mathbb{R}^{d})$ with $\psi_{\mu_{1}}(t) = \psi_{\mu_{2}}(t)$ for all $t \in \mathbb{R}^{d}$ . By Theorem 13.11(ii), $C_{c}(\mathbb{R}^{d})$ is a separating class for $\mathcal{M}_{f}(\mathbb{R}^{d})$ . Hence it is enough to show that $\int f d\mu_{1} = \int f d\mu_{2}$ for all $f \in C_{c}(\mathbb{R}^{d})$ . Let $f: \mathbb{R}^{d} \xrightarrow{} \mathbb{R}$ be continuous with compact support and let $\epsilon > 0$ . Assume that $K > 0$ is large enough such that $f(x) = 0$ for $x \notin (-K/2, K/2)^{d}$ and such that $\mu_{i}(\mathbb{R}^{d} - (-K, K)^{d}) < \epsilon, i = 1, 2$ . Consider the torus $E := \mathbb{R}^{d} / (2K\mathbb{Z}^{d})$ and define $\tilde{f} : E \xrightarrow{} \mathbb{R}$ by $\tilde{f}(x + 2K\mathbb{Z}^{d}) = f(x)$ for $x \in [-K, K)^{d}$ . Since the support of $f$ is contained in $(-K, K)^{d}$ , $\tilde{f}$ is continuous. For $m \in \mathbb{Z}^{d}$ define $g_{m}: \mathbb{R}^{d} \xrightarrow{} \mathbb{C}, x \mapsto \text{exp}(i \langle \pi m / K, x \rangle)$ . Let $\mathcal{C}$ be the algebra of finite linear combinations of the $g_{m}$ . For $g \in \mathcal{C}$ , we have $g(x) = g(x + 2Kn)$ for all $x \in \mathbb{R}^{d}$ and $n \in \mathbb{Z}^{d}$ . Hence, the map $\tilde{g} : E \xrightarrow{} \mathbb{C}, \tilde{g}(x + 2K \mathbb{Z}^{d}) = g(x)$ is well-defined, continuous and bounded. Furthermore, $\tilde{\mathcal{C}} := \{\tilde{g} : g \in \mathcal{C}\} \subset C_{b}(E; \mathbb{C})$ is an algebra that separates points and is closed under complex conjugation. As $E$ is compact, by the Stone-Weierstrass theorem, there is a $g \in \mathcal{C}$ such that $||\tilde{g} - \tilde{f} ||_{\infty} < \epsilon$ . We infer $||(f-g) \chi_{[-K, K]^{d}} ||_{\infty} < \epsilon$ and $||(f-g)\chi_{\mathbb{R}^{d} - [-K, K]^{d}} ||_{\infty} \leq ||g||_{\infty} = ||\tilde{g}||_{\infty} \leq ||\tilde{f}||_{\infty} + \epsilon = ||f||_{\infty} + \epsilon$ . By assumption of the theorem, $\int g d \mu_{1} = \int g d \mu_{2}$ . Hence, using the triangle inequality, we conclude $|\int f d \mu_{1} - \int f d \mu_{2}| \leq \int |f - g| d \mu_{1} + \int |f - g| d \mu_{2} \leq \epsilon(2 ||f ||_{\infty} + 2\epsilon + \mu_{1}(\mathbb{R}^{d}) + \mu_{2}(\mathbb{R}^{d}))$ . As $\epsilon$ is arbitrary, the integrals coincide. Why is this $\tilde{f}$ well defined? That is, if $x, y$ are from the same left coset, i.e., $x + 2K\mathbb{Z}^{d} = y + 2K\mathbb{Z}^{d}$ , how to see $f(x) = f(y)$ ? I found here https://proofwiki.org/wiki/Elements_in_Same_Left_Coset_iff_Product_with_Inverse_in_Subgroup#:~:text=Let%20x%2Cy%E2%88%88G,x%E2%88%921y%E2%88%88H . that $x, y$ are in the same left coset of $H$ iff $x^{-1}y$ is in the $H$ . For any $z$ outside $[-K, K)^{d}$ , $z$ is equivalent to some $x \in [-K, K)^{d}$ , with $z + 2K\mathbb{Z}^{d} = x + 2K\mathbb{Z}^{d}$ and so by definition $\tilde{f}(z + 2K\mathbb{Z}^{d}) = \tilde{f}(x + 2K\mathbb{Z}^{d}) = f(x)$ . So to see it is well-defined it's enough to consider two points $x, y \in [-K, K)^{d}$ . If $x + 2K\mathbb{Z}^{d} = y + 2K\mathbb{Z}^{d}$ , then $y - x = 2Km$ , for some $m \in \mathbb{Z}^{d}$ . And recall that $\tilde{f}(x + 2K\mathbb{Z}^{d})$ is defined only for $x \in [-K, K)^{d}$ , and for any two vectors from $[-K, K)^{d}$ , their difference in each coordinate is less than $2K$ , so such $m$ must be zero, meaning $x = y$ . My question: What does it mean for $\tilde{f}$ to be continuous? Why does $f$ 's being supported within $(-K, K)^{d}$ imply $\tilde{f}$ is continuous? Here is the definition of the quotient space: https://en.wikipedia.org/wiki/Quotient_space_(topology) . Given a topological space $X$ , the quotient space under ~ is the quotient set $Y$ equipped with the quotient topology, which is the topology whose open sets are the subsets $U \subset Y = X / \text{~}$ such that $\{x \in X: [x] \in U\}$ is an open subset of $X$ ; that is, $U \subset X / \text{~}$ iff $\{x \in X: [x] \in U\} \in \tau_{X}$ . Now $Y = \{x + 2K\mathbb{Z}^{d}: x \in [-K, K)^{d}\}$ so any subset $U$ of $Y$ has the form $U = \{x_{i} + 2K\mathbb{Z}^{d}\}_{i \in I}$ . If such $U$ is open in the quotient topology, this means $\{x_{i}\}_{i \in I}$ is open in $X$ . For any open interval of the form $A = (a, b) \in \mathbb{R}$ , we want to show $(\tilde{f})^{-1}(A)$ is open in the quotient topology. Since $f$ is continuous, $f^{-1}(A)$ is open in $\mathbb{R}^{d}$ . What next? (I don't know if this works: Denote $A = \{x_{j}\}_{j \in J}$ . Then $\{[x_{j}]\}_{j \in J}$ is open in the quotient topology. Since for any $z \in \mathbb{R}^{d}$ , $z$ is in the same equivalence class as $z'$ for some $z' \in [-K, K)^{d}$ , we can rewrite $\{[x_{j}]\}_{j \in J} = \{[x_{t}]\}_{t \in T}$ where each $x_{t} \in [-K, K)^{d}$ . By the definition of quotient topology, that $\{[x_{j}]\}_{j \in J} = \{[x_{t}]\}_{t \in T}$ is open implies $\{x_{t}\}_{t \in T}$ is open. ) This theorem is really scary to me (I learned measure theory mainly from the first five chapters from Donald Cohn's book, and I really like his style where every step is explained clearly. So when the author just omits something when they could have used some more words to explain a bit further it causes difficulty to my understanding. Maybe the details missing here are obvious to you but I really can't understand)... Appreciate any help!","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 1 year ago . Improve this question The theorem 15.9 (Characteristic function) says: A finite measure (here this symbol just means finite measures on ) is characterized by its characteristic function. Proof: Let with for all . By Theorem 13.11(ii), is a separating class for . Hence it is enough to show that for all . Let be continuous with compact support and let . Assume that is large enough such that for and such that . Consider the torus and define by for . Since the support of is contained in , is continuous. For define . Let be the algebra of finite linear combinations of the . For , we have for all and . Hence, the map is well-defined, continuous and bounded. Furthermore, is an algebra that separates points and is closed under complex conjugation. As is compact, by the Stone-Weierstrass theorem, there is a such that . We infer and . By assumption of the theorem, . Hence, using the triangle inequality, we conclude . As is arbitrary, the integrals coincide. Why is this well defined? That is, if are from the same left coset, i.e., , how to see ? I found here https://proofwiki.org/wiki/Elements_in_Same_Left_Coset_iff_Product_with_Inverse_in_Subgroup#:~:text=Let%20x%2Cy%E2%88%88G,x%E2%88%921y%E2%88%88H . that are in the same left coset of iff is in the . For any outside , is equivalent to some , with and so by definition . So to see it is well-defined it's enough to consider two points . If , then , for some . And recall that is defined only for , and for any two vectors from , their difference in each coordinate is less than , so such must be zero, meaning . My question: What does it mean for to be continuous? Why does 's being supported within imply is continuous? Here is the definition of the quotient space: https://en.wikipedia.org/wiki/Quotient_space_(topology) . Given a topological space , the quotient space under ~ is the quotient set equipped with the quotient topology, which is the topology whose open sets are the subsets such that is an open subset of ; that is, iff . Now so any subset of has the form . If such is open in the quotient topology, this means is open in . For any open interval of the form , we want to show is open in the quotient topology. Since is continuous, is open in . What next? (I don't know if this works: Denote . Then is open in the quotient topology. Since for any , is in the same equivalence class as for some , we can rewrite where each . By the definition of quotient topology, that is open implies is open. ) This theorem is really scary to me (I learned measure theory mainly from the first five chapters from Donald Cohn's book, and I really like his style where every step is explained clearly. So when the author just omits something when they could have used some more words to explain a bit further it causes difficulty to my understanding. Maybe the details missing here are obvious to you but I really can't understand)... Appreciate any help!","\mu \in \mathcal{M}_{f}(\mathbb{R}^{d}) \mathbb{R}^{d} \mu_{1}, \mu_{2} \in \mathcal{M}_{f}(\mathbb{R}^{d}) \psi_{\mu_{1}}(t) = \psi_{\mu_{2}}(t) t \in \mathbb{R}^{d} C_{c}(\mathbb{R}^{d}) \mathcal{M}_{f}(\mathbb{R}^{d}) \int f d\mu_{1} = \int f d\mu_{2} f \in C_{c}(\mathbb{R}^{d}) f: \mathbb{R}^{d} \xrightarrow{} \mathbb{R} \epsilon > 0 K > 0 f(x) = 0 x \notin (-K/2, K/2)^{d} \mu_{i}(\mathbb{R}^{d} - (-K, K)^{d}) < \epsilon, i = 1, 2 E := \mathbb{R}^{d} / (2K\mathbb{Z}^{d}) \tilde{f} : E \xrightarrow{} \mathbb{R} \tilde{f}(x + 2K\mathbb{Z}^{d}) = f(x) x \in [-K, K)^{d} f (-K, K)^{d} \tilde{f} m \in \mathbb{Z}^{d} g_{m}: \mathbb{R}^{d} \xrightarrow{} \mathbb{C}, x \mapsto \text{exp}(i \langle \pi m / K, x \rangle) \mathcal{C} g_{m} g \in \mathcal{C} g(x) = g(x + 2Kn) x \in \mathbb{R}^{d} n \in \mathbb{Z}^{d} \tilde{g} : E \xrightarrow{} \mathbb{C}, \tilde{g}(x + 2K \mathbb{Z}^{d}) = g(x) \tilde{\mathcal{C}} := \{\tilde{g} : g \in \mathcal{C}\} \subset C_{b}(E; \mathbb{C}) E g \in \mathcal{C} ||\tilde{g} - \tilde{f} ||_{\infty} < \epsilon ||(f-g) \chi_{[-K, K]^{d}} ||_{\infty} < \epsilon ||(f-g)\chi_{\mathbb{R}^{d} - [-K, K]^{d}} ||_{\infty} \leq ||g||_{\infty} = ||\tilde{g}||_{\infty} \leq ||\tilde{f}||_{\infty} + \epsilon = ||f||_{\infty} + \epsilon \int g d \mu_{1} = \int g d \mu_{2} |\int f d \mu_{1} - \int f d \mu_{2}| \leq \int |f - g| d \mu_{1} + \int |f - g| d \mu_{2} \leq \epsilon(2 ||f ||_{\infty} + 2\epsilon + \mu_{1}(\mathbb{R}^{d}) + \mu_{2}(\mathbb{R}^{d})) \epsilon \tilde{f} x, y x + 2K\mathbb{Z}^{d} = y + 2K\mathbb{Z}^{d} f(x) = f(y) x, y H x^{-1}y H z [-K, K)^{d} z x \in [-K, K)^{d} z + 2K\mathbb{Z}^{d} = x + 2K\mathbb{Z}^{d} \tilde{f}(z + 2K\mathbb{Z}^{d}) = \tilde{f}(x + 2K\mathbb{Z}^{d}) = f(x) x, y \in [-K, K)^{d} x + 2K\mathbb{Z}^{d} = y + 2K\mathbb{Z}^{d} y - x = 2Km m \in \mathbb{Z}^{d} \tilde{f}(x + 2K\mathbb{Z}^{d}) x \in [-K, K)^{d} [-K, K)^{d} 2K m x = y \tilde{f} f (-K, K)^{d} \tilde{f} X Y U \subset Y = X / \text{~} \{x \in X: [x] \in U\} X U \subset X / \text{~} \{x \in X: [x] \in U\} \in \tau_{X} Y = \{x + 2K\mathbb{Z}^{d}: x \in [-K, K)^{d}\} U Y U = \{x_{i} + 2K\mathbb{Z}^{d}\}_{i \in I} U \{x_{i}\}_{i \in I} X A = (a, b) \in \mathbb{R} (\tilde{f})^{-1}(A) f f^{-1}(A) \mathbb{R}^{d} A = \{x_{j}\}_{j \in J} \{[x_{j}]\}_{j \in J} z \in \mathbb{R}^{d} z z' z' \in [-K, K)^{d} \{[x_{j}]\}_{j \in J} = \{[x_{t}]\}_{t \in T} x_{t} \in [-K, K)^{d} \{[x_{j}]\}_{j \in J} = \{[x_{t}]\}_{t \in T} \{x_{t}\}_{t \in T}","['probability', 'probability-theory', 'quotient-spaces', 'characteristic-functions']"
17,Convergence in distribution of $S_n/\sqrt{n} - S_{2n}/\sqrt{2n}$,Convergence in distribution of,S_n/\sqrt{n} - S_{2n}/\sqrt{2n},"Suppose $X_1,X_2,...$ are i.i.d with $\mathcal{N}(0,1)$ . Denote $S_n=X_1+...+X_n$ . I am trying to determine if the sequence of random variables defined as: $$\frac{S_n}{\sqrt{n}} -\frac{S_{2n}}{\sqrt{2n}}$$ converges in distribution and if so, what is the limiting distribution. Here is my thinking so far of which I tend to be doubtful. Rewriting the above I get: $$\frac{X_1+...+X_n}{\sqrt{n}}-\frac{X_1+...+X_n}{\sqrt{2n}}-\frac{X_{n+1}+...+X_{2n}}{\sqrt{2n}}=$$ $$\frac{(1-\frac{1}{\sqrt{2}})(X_1+...+X_n)}{\sqrt{n}} -\frac{(\frac{1}{\sqrt{2}})(X_{n+1}+...+X_{2n})}{\sqrt{n}}$$ If I start taking large $n$ , then by the central limit theorem the distributions of both rv's $\frac{X_1+...+X_n}{\sqrt{n}}$ and $\frac{X_{n+1}+...+X_{2n}}{\sqrt{n}}$ will be closer and closer to $\mathcal{N}(0,1)$ . Furthermore, for any fixed $n$ they are independent rv's. So for large $n$ , I am effectively dealing with a linear combination of two independent $\mathcal{N}(0,1)$ rv's. Hence, I get $$\frac{(1-\frac{1}{\sqrt{2}})(X_1+...+X_n)}{\sqrt{n}} -\frac{(\frac{1}{\sqrt{2}})(X_{n+1}+...+X_{2n})}{\sqrt{n}}\sim\mathcal{N}(0,(1-\frac{1}{\sqrt{2}})^2+(\frac{1}{\sqrt{2}})^2)$$ and consequently: $$\frac{S_n}{\sqrt{n}} -\frac{S_{2n}}{\sqrt{2n}}\xrightarrow{d} \mathcal{N}(0,(1-\frac{1}{\sqrt{2}})^2+(\frac{1}{\sqrt{2}})^2) $$ I am aware that this is not a rigorous proof, but is my intuition correct or is it misleading me completely? Will highly appreciate any help with this problem.","Suppose are i.i.d with . Denote . I am trying to determine if the sequence of random variables defined as: converges in distribution and if so, what is the limiting distribution. Here is my thinking so far of which I tend to be doubtful. Rewriting the above I get: If I start taking large , then by the central limit theorem the distributions of both rv's and will be closer and closer to . Furthermore, for any fixed they are independent rv's. So for large , I am effectively dealing with a linear combination of two independent rv's. Hence, I get and consequently: I am aware that this is not a rigorous proof, but is my intuition correct or is it misleading me completely? Will highly appreciate any help with this problem.","X_1,X_2,... \mathcal{N}(0,1) S_n=X_1+...+X_n \frac{S_n}{\sqrt{n}} -\frac{S_{2n}}{\sqrt{2n}} \frac{X_1+...+X_n}{\sqrt{n}}-\frac{X_1+...+X_n}{\sqrt{2n}}-\frac{X_{n+1}+...+X_{2n}}{\sqrt{2n}}= \frac{(1-\frac{1}{\sqrt{2}})(X_1+...+X_n)}{\sqrt{n}} -\frac{(\frac{1}{\sqrt{2}})(X_{n+1}+...+X_{2n})}{\sqrt{n}} n \frac{X_1+...+X_n}{\sqrt{n}} \frac{X_{n+1}+...+X_{2n}}{\sqrt{n}} \mathcal{N}(0,1) n n \mathcal{N}(0,1) \frac{(1-\frac{1}{\sqrt{2}})(X_1+...+X_n)}{\sqrt{n}} -\frac{(\frac{1}{\sqrt{2}})(X_{n+1}+...+X_{2n})}{\sqrt{n}}\sim\mathcal{N}(0,(1-\frac{1}{\sqrt{2}})^2+(\frac{1}{\sqrt{2}})^2) \frac{S_n}{\sqrt{n}} -\frac{S_{2n}}{\sqrt{2n}}\xrightarrow{d} \mathcal{N}(0,(1-\frac{1}{\sqrt{2}})^2+(\frac{1}{\sqrt{2}})^2) ","['probability', 'convergence-divergence', 'random-variables', 'normal-distribution', 'central-limit-theorem']"
18,If I roll 2 dice and then add those 2 numbers together. What is the probability that the sum is $\le 5$ or that the sum is $\ge 11$,If I roll 2 dice and then add those 2 numbers together. What is the probability that the sum is  or that the sum is,\le 5 \ge 11,I want to solve this dice problem but I'm not sure if my solution is the most efficient way. If I roll one dice and then one more and then add those 2 numbers together. What is the probability that the sum is $\le 5$ or that the sum is $\ge 11$ ? I basically wrote out all possible occasions and checked how many of the possibilities were less than or equal to 5 or greater than or equal to 11. 10 possibiliteis has a sum of 5 or less and 3 had a sum of 11 or more. And this is out of $6 \times 6 = 36$ total possibilities. So $\frac {10}{36} + \frac {3}{36} = \frac {13}{36}$ Is there a better way of solving this?,I want to solve this dice problem but I'm not sure if my solution is the most efficient way. If I roll one dice and then one more and then add those 2 numbers together. What is the probability that the sum is or that the sum is ? I basically wrote out all possible occasions and checked how many of the possibilities were less than or equal to 5 or greater than or equal to 11. 10 possibiliteis has a sum of 5 or less and 3 had a sum of 11 or more. And this is out of total possibilities. So Is there a better way of solving this?,\le 5 \ge 11 6 \times 6 = 36 \frac {10}{36} + \frac {3}{36} = \frac {13}{36},['probability']
19,Sums of independent random variables cannot accumulate at a point,Sums of independent random variables cannot accumulate at a point,,"Suppose that we have independent random variables $X_1, X_2, \ldots$ , each of which has finite mean. I am interested in the condition $$\underset{n\to\infty}{\lim} \sup_{x\in \mathbb{R}} \mathbb{P}\Bigl(\sum_{i=1}^n X_i=x\Bigr)=0. \tag{A}$$ What are some necessary conditions which guarantee that condition $(A)$ holds? What are some sufficient conditions which guarantee that condition $(A)$ holds? What are some necessary conditions which guarantee that condition $(A)$ holds, under the additional hypothesis of identical distributions? My hope is that iid and non-constant would give $(A)$ . What are some sufficient conditions that guarantee that condition $(A)$ holds, under the additional hypothesis of identical distributions? The low hanging fruit here is the assumption of finite variance, since then CLT gives the conclusion. I'm hoping for something more general which could apply under the assumption of finite first moments, without necessarily assuming finite $1+\delta$ moments for any $\delta>0$ .","Suppose that we have independent random variables , each of which has finite mean. I am interested in the condition What are some necessary conditions which guarantee that condition holds? What are some sufficient conditions which guarantee that condition holds? What are some necessary conditions which guarantee that condition holds, under the additional hypothesis of identical distributions? My hope is that iid and non-constant would give . What are some sufficient conditions that guarantee that condition holds, under the additional hypothesis of identical distributions? The low hanging fruit here is the assumption of finite variance, since then CLT gives the conclusion. I'm hoping for something more general which could apply under the assumption of finite first moments, without necessarily assuming finite moments for any .","X_1, X_2, \ldots \underset{n\to\infty}{\lim} \sup_{x\in \mathbb{R}} \mathbb{P}\Bigl(\sum_{i=1}^n X_i=x\Bigr)=0. \tag{A} (A) (A) (A) (A) (A) 1+\delta \delta>0","['probability', 'probability-theory', 'statistics']"
20,Expectation of the exponential of the amount of time a Brownian motion spent in half space,Expectation of the exponential of the amount of time a Brownian motion spent in half space,,"Let $(B)_{t\geq 0}$ be a standard Brownian motion and let $c\in\mathbb{R}$ . Can we calculate the expectation of the exponential of the time $B$ will be spent above $c$ , $$\mathbb{E}\left[\exp\left\{\int_0^t\mathbb{1}_{\{B_u>c\}}du\right\}\right]?$$ My thought is: Let $L_t:=\exp\left\{\int_0^t\mathbb{1}_{\{B_u>c\}}du\right\}$ , it is a non-decreasing process, hence, a sub-martingale. Next step is to identify its Doob-Meyer decomposition, and this is where I cannot proceed.","Let be a standard Brownian motion and let . Can we calculate the expectation of the exponential of the time will be spent above , My thought is: Let , it is a non-decreasing process, hence, a sub-martingale. Next step is to identify its Doob-Meyer decomposition, and this is where I cannot proceed.",(B)_{t\geq 0} c\in\mathbb{R} B c \mathbb{E}\left[\exp\left\{\int_0^t\mathbb{1}_{\{B_u>c\}}du\right\}\right]? L_t:=\exp\left\{\int_0^t\mathbb{1}_{\{B_u>c\}}du\right\},"['probability', 'stochastic-processes', 'stochastic-calculus', 'brownian-motion', 'stochastic-analysis']"
21,Probability of a painted cube with at least one white face,Probability of a painted cube with at least one white face,,"A large solid cube with edge length of $4$ cm. is made from white dice with edge length of $1$ cm., and and the whole surface of the cube is painted black. These smaller cubes are shuffled randomly. A blind man (who also cannot feel the paint) reassembles the small cubes into a large one. What is the probability that there is a at least one white face outside this large cube? I find it very difficult to solve but perhaps I am missing something. This is what I got to: There are $24$ dice with 1 painted face, $24$ dice with 2 painted faces, $8$ dice with 3 painted faces, and $8$ dice with no painted faces. Calling them type 1, 2, 3 and 4 respectively, the probabilities of choosing a dice of each type are: $P(T_1) = 3/8$ , $P(T_2) = 3/8$ , $P(T_3) = 1/8$ , $P(T_4) = 1/8$ . By Total probability theorem the probability of choosing a white face in a dice is: $P(B_d)=P(B_d/T_1)P(T_1)+P(B_d/T_2)P(T_2)+P(B_d/T_3)P(T_3)+P(B_d/T_4)P(T_4)=(5/6)(3/8)+(4/6)(3/8)+(3/6)(1/8)+(6/6)(1/8)=3/4$ I think it is not easy to find the probability of having at least a white face in the cube from this. The probability of having a white face in a dice outside the cube is dependent on other faces. For instance, a dice with $3$ painted faces could make that upto $3$ faces on the cube are not white. I tried an approach similar to Paul Sinclair's but I am not convinced it is a good one. The probability for a face to be white would be: $P(B)= (P(B_d))^{16}=(3/4)^{16}=(3^{16}/4^{16})$ . Consequently, the probability that it is not completely white is $P(B^c)=1-P(B)=1-3^{16}/4^{16}=(4^{16}-3^{16})/4^{16}$ . If we consider for $i \in \{1, \ldots, 6 \}$ the events B_i = ""the face i is white"" and $B_i^c$ its complement, then: $P(B_i) = 3^{16}/4^{16}$ and $P(B_i^c)=(4^{16}-3^{16})/4^{16}$ . Thus: $P(\text{At least 1 face is white})=1-P(\text{0 white faces})=1-P(B_1^c \cap B_2^c \cap B_3^c \cap B_4^c \cap B_5^c \cap B_6^c)=1-P(B_1^c)P(B_2^c | B_1^c)P(B_3^c | B_1^c \cap B_2^c) P(B_4^c | B_1^c \cap B_2^c \cap B_3^c) P(B_5^c | B_1^c \cap B_2^c \cap B_3^c \cap B_4^c) P(B_6^c | B_1^c \cap B_2^c \cap B_3^c \cap B_4^c \cap B_5^c) \approx 1-(99/100)^6$ Consequently: $P(\text{At least 1 face is white})=1-P(\text{0 white faces}) \approx 1- (P(B_i))^6 = 1-((4^{16}-3^{16})/4^{16})^6 \approx 1-(99/100)^6 \approx (6/100)=0.06$","A large solid cube with edge length of cm. is made from white dice with edge length of cm., and and the whole surface of the cube is painted black. These smaller cubes are shuffled randomly. A blind man (who also cannot feel the paint) reassembles the small cubes into a large one. What is the probability that there is a at least one white face outside this large cube? I find it very difficult to solve but perhaps I am missing something. This is what I got to: There are dice with 1 painted face, dice with 2 painted faces, dice with 3 painted faces, and dice with no painted faces. Calling them type 1, 2, 3 and 4 respectively, the probabilities of choosing a dice of each type are: , , , . By Total probability theorem the probability of choosing a white face in a dice is: I think it is not easy to find the probability of having at least a white face in the cube from this. The probability of having a white face in a dice outside the cube is dependent on other faces. For instance, a dice with painted faces could make that upto faces on the cube are not white. I tried an approach similar to Paul Sinclair's but I am not convinced it is a good one. The probability for a face to be white would be: . Consequently, the probability that it is not completely white is . If we consider for the events B_i = ""the face i is white"" and its complement, then: and . Thus: Consequently:","4 1 24 24 8 8 P(T_1) = 3/8 P(T_2) = 3/8 P(T_3) = 1/8 P(T_4) = 1/8 P(B_d)=P(B_d/T_1)P(T_1)+P(B_d/T_2)P(T_2)+P(B_d/T_3)P(T_3)+P(B_d/T_4)P(T_4)=(5/6)(3/8)+(4/6)(3/8)+(3/6)(1/8)+(6/6)(1/8)=3/4 3 3 P(B)= (P(B_d))^{16}=(3/4)^{16}=(3^{16}/4^{16}) P(B^c)=1-P(B)=1-3^{16}/4^{16}=(4^{16}-3^{16})/4^{16} i \in \{1, \ldots, 6 \} B_i^c P(B_i) = 3^{16}/4^{16} P(B_i^c)=(4^{16}-3^{16})/4^{16} P(\text{At least 1 face is white})=1-P(\text{0 white faces})=1-P(B_1^c \cap B_2^c \cap B_3^c \cap B_4^c \cap B_5^c \cap B_6^c)=1-P(B_1^c)P(B_2^c | B_1^c)P(B_3^c | B_1^c \cap B_2^c) P(B_4^c | B_1^c \cap B_2^c \cap B_3^c) P(B_5^c | B_1^c \cap B_2^c \cap B_3^c \cap B_4^c) P(B_6^c | B_1^c \cap B_2^c \cap B_3^c \cap B_4^c \cap B_5^c) \approx 1-(99/100)^6 P(\text{At least 1 face is white})=1-P(\text{0 white faces}) \approx 1- (P(B_i))^6 = 1-((4^{16}-3^{16})/4^{16})^6 \approx 1-(99/100)^6 \approx (6/100)=0.06","['probability', 'combinatorics', 'geometry']"
22,A random walk on $\mathbb{Z}$ is transient iff $\sum_{i=0}^{\infty}\mathbb{P}\left(S_{i}=S_{0}\right)<\infty$,A random walk on  is transient iff,\mathbb{Z} \sum_{i=0}^{\infty}\mathbb{P}\left(S_{i}=S_{0}\right)<\infty,"I was asked to prove the said equivalence for a simple random walk on the integers (which may be biased): $X_{i}\in\left\{ \pm1\right\} $ , $S_{n}=S_{0}+\sum_{i=1}^{n}X_{i}$ . The first direction is pretty direct: assume that $\sum_{i=0}^{\infty}\mathbb{P}\left(S_{i}=S_{0}\right)<\infty$ , then according to the Borel–Cantelli lemma, we have $\mathbb{P}\left(S_{i}=S_{0}\quad i.o.\right)=0$ . I am having trouble with the other direction. Ultimately, I would like to use the counterpart of B-C, but $S_i=S_0$ and $S_j=S_0$ might be dependnet (in the case $\mathbb{P}\left(X_{i}=\pm1\right)\neq0.5$ ). If we define $Z=\#\left\{ i\mid S_{i}=S_{0}\right\} $ we know that $\mathbb{E}\left[Z\right]=\sum_{i=0}^{n}\mathbb{P}\left(S_{i}=S_{0}\right)=\infty$ , but I don't know how to proceed from here. I have seen this question which seems very relevant, but it is rather not detailed.","I was asked to prove the said equivalence for a simple random walk on the integers (which may be biased): , . The first direction is pretty direct: assume that , then according to the Borel–Cantelli lemma, we have . I am having trouble with the other direction. Ultimately, I would like to use the counterpart of B-C, but and might be dependnet (in the case ). If we define we know that , but I don't know how to proceed from here. I have seen this question which seems very relevant, but it is rather not detailed.",X_{i}\in\left\{ \pm1\right\}  S_{n}=S_{0}+\sum_{i=1}^{n}X_{i} \sum_{i=0}^{\infty}\mathbb{P}\left(S_{i}=S_{0}\right)<\infty \mathbb{P}\left(S_{i}=S_{0}\quad i.o.\right)=0 S_i=S_0 S_j=S_0 \mathbb{P}\left(X_{i}=\pm1\right)\neq0.5 Z=\#\left\{ i\mid S_{i}=S_{0}\right\}  \mathbb{E}\left[Z\right]=\sum_{i=0}^{n}\mathbb{P}\left(S_{i}=S_{0}\right)=\infty,"['probability', 'random-walk']"
23,How different can posterior mean be for two mean zero unit variance priors?,How different can posterior mean be for two mean zero unit variance priors?,,"Suppose $Y \mid \theta \sim N(\theta, \sigma^2)$ . Let $\mu_G(Y)$ be the posterior mean for $\theta$ when $\theta \sim G$ is the prior. How large can the average discrepancy in posterior mean $$ \sup_{G, G_0, \sigma} E_{Y \sim G_0 \star N(0, \sigma^2)} [(\mu_G(Y) - \mu_{G_0}(Y))^2] $$ be, where $G, G_0$ are constrained to have mean zero and variance one? My guess is that it's bounded by the squared Wasserstein-2 distance between $G, G_0$ , which is in turn bounded by 2, but I'm having trouble proving it.","Suppose . Let be the posterior mean for when is the prior. How large can the average discrepancy in posterior mean be, where are constrained to have mean zero and variance one? My guess is that it's bounded by the squared Wasserstein-2 distance between , which is in turn bounded by 2, but I'm having trouble proving it.","Y \mid \theta \sim N(\theta, \sigma^2) \mu_G(Y) \theta \theta \sim G 
\sup_{G, G_0, \sigma} E_{Y \sim G_0 \star N(0, \sigma^2)} [(\mu_G(Y) - \mu_{G_0}(Y))^2]
 G, G_0 G, G_0","['probability', 'statistics', 'bayesian']"
24,Conditional probability $\mathbb{P}(B|(A\cup\overline{B})$,Conditional probability,\mathbb{P}(B|(A\cup\overline{B}),"I have one probability question, which I got an answer different from the one given in the textbook. Here it is: Suppose $A,B$ are two events in the same sample space $\Omega$ , such that $\mathbb{P}(\overline{A})=0.3, \mathbb{P}(B)=0.4$ and $\mathbb{P}(A\cap\overline{B})=0.5$ , calculate the conditional probability $\mathbb{P}(B|(A\cup\overline{B}))$ . Here $\overline{A}$ denotes the opposite event of $A$ , and similar for $\overline{B}$ . My way to solve the problem: By definition of conditional probability, we have $$\mathbb{P}(B|(A\cup\overline{B}))=\frac{\mathbb{P}(B\cap(A\cup\overline{B}))}{\mathbb{P}(A\cup\overline{B})}.$$ Note that $$B\cap(A\cup\overline{B})=(B\cap A)\cup(B\cap\overline{B})=B\cap A, $$ we have $$\mathbb{P}(B\cap(A\cup\overline{B}))=\mathbb{P}(B\cap A)=\mathbb{P}(A)-\mathbb{P}(A\cap\overline{B})=(1-0.3)-0.5=0.2.$$ On the other hand, $$\mathbb{P}(A\cup\overline{B})=\mathbb{P}(A)+\mathbb{P}(\overline{B})-\mathbb{P}(A\cap\overline{B})=(1-0.3)+(1-0.4)-0.5=0.8.$$ Hence $$\mathbb{P}(B|(A\cup\overline{B}))=\frac{0.2}{0.8}=0.25.$$ However, the answer given by the textbook is $\dfrac{1}{1960}$ , without any justification and explanation. I wonder whether I made any mistakes or the textbook's answer is wrong. Any comments?","I have one probability question, which I got an answer different from the one given in the textbook. Here it is: Suppose are two events in the same sample space , such that and , calculate the conditional probability . Here denotes the opposite event of , and similar for . My way to solve the problem: By definition of conditional probability, we have Note that we have On the other hand, Hence However, the answer given by the textbook is , without any justification and explanation. I wonder whether I made any mistakes or the textbook's answer is wrong. Any comments?","A,B \Omega \mathbb{P}(\overline{A})=0.3, \mathbb{P}(B)=0.4 \mathbb{P}(A\cap\overline{B})=0.5 \mathbb{P}(B|(A\cup\overline{B})) \overline{A} A \overline{B} \mathbb{P}(B|(A\cup\overline{B}))=\frac{\mathbb{P}(B\cap(A\cup\overline{B}))}{\mathbb{P}(A\cup\overline{B})}. B\cap(A\cup\overline{B})=(B\cap A)\cup(B\cap\overline{B})=B\cap A,  \mathbb{P}(B\cap(A\cup\overline{B}))=\mathbb{P}(B\cap A)=\mathbb{P}(A)-\mathbb{P}(A\cap\overline{B})=(1-0.3)-0.5=0.2. \mathbb{P}(A\cup\overline{B})=\mathbb{P}(A)+\mathbb{P}(\overline{B})-\mathbb{P}(A\cap\overline{B})=(1-0.3)+(1-0.4)-0.5=0.8. \mathbb{P}(B|(A\cup\overline{B}))=\frac{0.2}{0.8}=0.25. \dfrac{1}{1960}","['probability', 'probability-theory', 'conditional-probability']"
25,Growth Rate of Supremum of Random Walk (with negative drift),Growth Rate of Supremum of Random Walk (with negative drift),,"Let $(X_k)_{k=1}^\infty$ be a sequence of i.i.d. random variables, with $\mathbb{E}(X_i)$ finite and negative. Define $S_n := X_1 + ... + X_n$ , and $M_n := \max(S_1,S_2,...,S_n)$ . It follows from the final bullet point of Sangchul Lee's answer to this question Is the expectation of the supremum of a random walk with negative drift finite? that if $X_i$ have infinite variance, then $\mathbb{E}(M_n) \rightarrow +\infty$ as $n \rightarrow +\infty$ . However I am curious how quickly this growth to infinity occurs? I am not sure if there is a general answer in terms of the distribution of $X_i$ , so I started by trying to use a simple test example. I defined the $X_i$ variables to be absolutely continuous with density function $f(x) := \frac{2}{(x+3)^3}$ for $x \in [-2,\infty)$ . This is the simpliest example I could think of which would have a finite negative mean (in this case the mean is $-1$ ), yet infinite variance. However, with this example, I am struggling to work out how quickly $\mathbb{E}(M_n)$ diverges. By a very rough numerical calculation, it seems as though the density function of $M_2$ decays like $O(\frac{1}{x^{2.8}})$ (maybe the true exponent is $e$ ?). I'm guessing that as $n \rightarrow +\infty$ the density of $M_n$ tends to a decay rate of $O(\frac{1}{x^2})$ , but again I have no idea how to attack this. Does anyone know what kind of analytic tools I can use to tackle this problem? Cheers.","Let be a sequence of i.i.d. random variables, with finite and negative. Define , and . It follows from the final bullet point of Sangchul Lee's answer to this question Is the expectation of the supremum of a random walk with negative drift finite? that if have infinite variance, then as . However I am curious how quickly this growth to infinity occurs? I am not sure if there is a general answer in terms of the distribution of , so I started by trying to use a simple test example. I defined the variables to be absolutely continuous with density function for . This is the simpliest example I could think of which would have a finite negative mean (in this case the mean is ), yet infinite variance. However, with this example, I am struggling to work out how quickly diverges. By a very rough numerical calculation, it seems as though the density function of decays like (maybe the true exponent is ?). I'm guessing that as the density of tends to a decay rate of , but again I have no idea how to attack this. Does anyone know what kind of analytic tools I can use to tackle this problem? Cheers.","(X_k)_{k=1}^\infty \mathbb{E}(X_i) S_n := X_1 + ... + X_n M_n := \max(S_1,S_2,...,S_n) X_i \mathbb{E}(M_n) \rightarrow +\infty n \rightarrow +\infty X_i X_i f(x) := \frac{2}{(x+3)^3} x \in [-2,\infty) -1 \mathbb{E}(M_n) M_2 O(\frac{1}{x^{2.8}}) e n \rightarrow +\infty M_n O(\frac{1}{x^2})","['probability', 'probability-theory', 'analysis', 'random-walk']"
26,"chapter 1 ex 4.22 from Brownian motion, martingales, and stochastic calculus by Jean-Francois Le Gall","chapter 1 ex 4.22 from Brownian motion, martingales, and stochastic calculus by Jean-Francois Le Gall",,"This is ex 4.22 from chapter 1 of''Brownian motion, martingales, and stochastic calculus by Jean-Francois Le Gall''. exercise 4.22: Processes on defined on a probability space $(\Omega,\mathcal{F},P)$ equipped with a complete filtration $(\mathcal{F}_t)_{t \in [0,\infty]}$ . Let U be an $\mathcal{F}_0$ -measurable real random variable, and let M be a continuous local martingale. Show the process $N_t=UM_t$ is a continuous local martingale. Definition of continuous local martingale:an adapted process $M=(M_t)_{t \geq 0}$ with continuous sample paths and such that $M_0=0$ a.s. is called a continuous local martingale if there exists a nondecreasing sequence $(T_n)_{n \geq 0}$ of stopping times such that $T_n \uparrow \infty$ and, for every n, the stopped process $M^{T_n}$ is a uniformly integrable martingale. Property of continuous local martingale: A martingale with continuous sample paths is a continuous local martingale and the sequence $T_n=n$ reduces M. I am not sure [https-//math.stackexchange.com/questions/4289186/how-to-prove-that-um-t-t-geq-0-is-a-continuous-local-martingale?rq=1][1] this is a feasible way, since ex 4.22 has a stronger condition with a complete filtration. Bounty: I haven't made any further progress, so an answer is appreciated. update: I will follow neijimban's hint and try the techniques used in prop 4.7 of the book: We write $M_t=M_0+N_t.$ there exists a sequence $(T_n)$ of stopping times that reduces N. Then if $s \leq t$ , we have for every n, $N_{ s \wedge T_n}=E[N_{ t \wedge T_n} | \mathcal{F}_s]$ . Adding both sides the random variable $M_0$ (which is $\mathcal{F}_0$ -measurable and in $L^1$ by assumption), we get $M_{s \wedge T_n}=E[M_{t \wedge T_n} | \mathcal{F}_s]$ . Then I am not sure if we can assume there is a random variable Z such that $|M_{t \wedge T_n}| \leq Z$ , and then proceed by dominated convergence to get that $M_{ t \wedge T_n}$ converges to $M_t$ in $L^1$ . Then as $n \rightarrow \infty$ , we get $M_s=E[M_t | \mathcal{F}_s]$ to get uniformly integrable. Let $\tau_n= \inf \{t \geq 0: |UM_t| \geq n\}$ or $\tau \wedge T_n$ where $(T_n)_{n \geq 0}$ reduces $M$ . Since $T_n$ are stopping times and $M^{T_n}$ is a continuous local martingale and $|M^{T_n}| \leq n$ . If we assume $M_0 \in L^1$ , $M^{T_n}$ is dominated by $n+|M_0|$ . So $\lim_{n \rightarrow \infty} \int_{|M^{T_n}|>n} |M^{T_n}| dP(\omega)=0$ and this is the continuous local martingale. I am not sure if this works, so please provide an alternative if this is not correct, thanks!","This is ex 4.22 from chapter 1 of''Brownian motion, martingales, and stochastic calculus by Jean-Francois Le Gall''. exercise 4.22: Processes on defined on a probability space equipped with a complete filtration . Let U be an -measurable real random variable, and let M be a continuous local martingale. Show the process is a continuous local martingale. Definition of continuous local martingale:an adapted process with continuous sample paths and such that a.s. is called a continuous local martingale if there exists a nondecreasing sequence of stopping times such that and, for every n, the stopped process is a uniformly integrable martingale. Property of continuous local martingale: A martingale with continuous sample paths is a continuous local martingale and the sequence reduces M. I am not sure [https-//math.stackexchange.com/questions/4289186/how-to-prove-that-um-t-t-geq-0-is-a-continuous-local-martingale?rq=1][1] this is a feasible way, since ex 4.22 has a stronger condition with a complete filtration. Bounty: I haven't made any further progress, so an answer is appreciated. update: I will follow neijimban's hint and try the techniques used in prop 4.7 of the book: We write there exists a sequence of stopping times that reduces N. Then if , we have for every n, . Adding both sides the random variable (which is -measurable and in by assumption), we get . Then I am not sure if we can assume there is a random variable Z such that , and then proceed by dominated convergence to get that converges to in . Then as , we get to get uniformly integrable. Let or where reduces . Since are stopping times and is a continuous local martingale and . If we assume , is dominated by . So and this is the continuous local martingale. I am not sure if this works, so please provide an alternative if this is not correct, thanks!","(\Omega,\mathcal{F},P) (\mathcal{F}_t)_{t \in [0,\infty]} \mathcal{F}_0 N_t=UM_t M=(M_t)_{t \geq 0} M_0=0 (T_n)_{n \geq 0} T_n \uparrow \infty M^{T_n} T_n=n M_t=M_0+N_t. (T_n) s \leq t N_{ s \wedge T_n}=E[N_{ t \wedge T_n} | \mathcal{F}_s] M_0 \mathcal{F}_0 L^1 M_{s \wedge T_n}=E[M_{t \wedge T_n} | \mathcal{F}_s] |M_{t \wedge T_n}| \leq Z M_{ t \wedge T_n} M_t L^1 n \rightarrow \infty M_s=E[M_t | \mathcal{F}_s] \tau_n= \inf \{t \geq 0: |UM_t| \geq n\} \tau \wedge T_n (T_n)_{n \geq 0} M T_n M^{T_n} |M^{T_n}| \leq n M_0 \in L^1 M^{T_n} n+|M_0| \lim_{n \rightarrow \infty} \int_{|M^{T_n}|>n} |M^{T_n}| dP(\omega)=0","['probability', 'stochastic-calculus', 'self-learning', 'quadratic-variation', 'local-martingales']"
27,Trading Cards & Packs - compute the hypothetical number of packs to purchase to achieve a certain probability to complete a certain Cards' set,Trading Cards & Packs - compute the hypothetical number of packs to purchase to achieve a certain probability to complete a certain Cards' set,,"I'm trying to solve a probability problem within the context of a Trading Cards game, but I'm not sure if using Hypergeometric Distribution is the right way. Given the following conditions: Given a population size of N=1500 cards: 5 different Team Cards x 10 copies x 30 Teams To win the game, a user needs to collect all m=5 Cards of a chosen Team (e.g. Team A) User can buy only packs of 5 Cards per time ( n=5 permitted draws) Packs have no duplicated cards. Cards are equally distributed across Packs. By using the Excel Formula: =HYPGEOMDIST(k,n,m,N) the probability to pick 1 Team card by purchasing k=1 Pack should be 14.59% By making the product of the probabilities, the probability to complete a set of 5 team cards by purchasing 5 Packs should be 14.59%^5 = 0.01% If all above is correct, I'm still wondering how to find the number of packs I would need to buy, given a certain target p probability to complete the set . E.g. If I wanted a target p = 50% or p = 99% or p = 100% probability of completing the m = 5 Cards Team set, how many packs shall I buy? I'm adding here the excel with the data, hope it helps! https://docs.google.com/spreadsheets/d/1B-sRhDB_DGmPbGpL-4BD0fzVPJ01dUMlD950seoGMk4/edit?usp=sharing","I'm trying to solve a probability problem within the context of a Trading Cards game, but I'm not sure if using Hypergeometric Distribution is the right way. Given the following conditions: Given a population size of N=1500 cards: 5 different Team Cards x 10 copies x 30 Teams To win the game, a user needs to collect all m=5 Cards of a chosen Team (e.g. Team A) User can buy only packs of 5 Cards per time ( n=5 permitted draws) Packs have no duplicated cards. Cards are equally distributed across Packs. By using the Excel Formula: =HYPGEOMDIST(k,n,m,N) the probability to pick 1 Team card by purchasing k=1 Pack should be 14.59% By making the product of the probabilities, the probability to complete a set of 5 team cards by purchasing 5 Packs should be 14.59%^5 = 0.01% If all above is correct, I'm still wondering how to find the number of packs I would need to buy, given a certain target p probability to complete the set . E.g. If I wanted a target p = 50% or p = 99% or p = 100% probability of completing the m = 5 Cards Team set, how many packs shall I buy? I'm adding here the excel with the data, hope it helps! https://docs.google.com/spreadsheets/d/1B-sRhDB_DGmPbGpL-4BD0fzVPJ01dUMlD950seoGMk4/edit?usp=sharing",,"['probability', 'hypergeometric-function', 'coupon-collector']"
28,Generalizations of Stein's identity to product of functions of gaussian vector,Generalizations of Stein's identity to product of functions of gaussian vector,,"Given a $d$ -dimensional Gaussian $X \sim N(\mu, \Sigma)$ and two real-valued differentiable functions $f,g$ with bounded first derivatives, I am wondering if there is a simple expression for the covariance: $$ \text{Cov} (f(X),g(X)). $$ For example, when $f(X) = X_k$ for some $k \in \{1,\dots, d\}$ , Stein's lemma gives $$ \text{Cov} (X_1,g(X)) = \sum_{i=1} \Sigma_{1i} \mathbb{E} \left( \frac{\partial g(x)}{\partial x_i} \bigg |_{x=X}\right). $$ Are there similar expressions for more general $f$ though? reference: Lemma 1 here https://reader.elsevier.com/reader/sd/pii/016771529490121X?token=62B2E8B7BA323BC1AF5B699E9A3115C156EC43A959F764C79D0C044730A2C8F605B5CC1F9E5807764BCC2D37793AC619&originRegion=us-east-1&originCreation=20220630190410","Given a -dimensional Gaussian and two real-valued differentiable functions with bounded first derivatives, I am wondering if there is a simple expression for the covariance: For example, when for some , Stein's lemma gives Are there similar expressions for more general though? reference: Lemma 1 here https://reader.elsevier.com/reader/sd/pii/016771529490121X?token=62B2E8B7BA323BC1AF5B699E9A3115C156EC43A959F764C79D0C044730A2C8F605B5CC1F9E5807764BCC2D37793AC619&originRegion=us-east-1&originCreation=20220630190410","d X \sim N(\mu, \Sigma) f,g 
\text{Cov} (f(X),g(X)).
 f(X) = X_k k \in \{1,\dots, d\} 
\text{Cov} (X_1,g(X)) = \sum_{i=1} \Sigma_{1i} \mathbb{E} \left( \frac{\partial g(x)}{\partial x_i} \bigg |_{x=X}\right).
 f","['probability', 'statistics', 'statistical-inference', 'gaussian']"
29,"Is ""Popping Popcorn"" a Stochastic Process?","Is ""Popping Popcorn"" a Stochastic Process?",,"While placing a bag of popcorn in the microwave today, I noticed the following thing: In the 30 seconds, the time difference between (e.g. treat ""time"" as a random variable) which any two kernel of popcorn can pop has some probability distribution function (e.g. the n-th kernel pops at time T1 and the n_1-th kernel pops at time T2 : T2 - T1 has some probability distribution In the next 30 seconds, the time difference between which any two kernels of popcorn can pop has some other probability distribution - naturally, the time difference between successive kernel pops is obviously shorter In the last 30 seconds, the time difference between which any two kernels of popcorn can pop has another probability distribution - naturally, the time difference between successive kernel pops is longer than this same time difference in the previous 30 seconds. This brings me to my question - could ""popping popcorn"" be modelled as some Stochastic Process (such as a Inhomogeneous Poisson Arrival Process, Markov Switching Process or a Brownian Motion ) ? For instance, suppose we knew that 43 kernels of popcorn have already popped - we also know that time at which each kernel popped (e.g. kernel_1 = 12 seconds, kernel_2 = 19 seconds, kernel_3 = 21 seconds .... kernel_42 = 81 seconds, kernel_43 = 85 seconds). This means we also know the time differences between each successive pair of popcorn kernels (e.g. Kernel_2_1 = 19 - 12 = 7 seconds, Kernel_3_2 = 21 - 19 seconds = 2 seconds, etc.). With this information (and historical data we collected about popcorn kernels popping) , could we make some inference as to when the 44th kernel of popcorn is expected to pop? Thanks! Note 1: I feel a very naive way to approach this problem would be to just fit some regression model through historical data - here I have shown this using the R programming language. library(dplyr)     #first 30 seconds     popping_times = abs(rnorm(40,10,3))     index = 1:40     data_1 = data.frame(index, popping_times)  #next 30 seconds popping_times = abs(rnorm(80,5, 2)) index = 1:80 data_2 = data.frame(index, popping_times)  #next 30 seconds popping_times = abs(rnorm(30,7,3)) index = 1:30 data_3 = data.frame(index, popping_times)  total_data = rbind(data_1, data_2, data_3) total_data$index = 1:nrow(total_data)  total_data = data.frame(total_data %>%  mutate(popping_times_diff = abs(popping_times - lag(popping_times))))  total_data[is.na(total_data)] <- 0 The data looks something like this: index popping_times popping_times_diff 1     1     10.380158         0.00000000 2     2      8.874496         1.50566244 3     3      8.966483         0.09198782 4     4      9.312913         0.34642958 5     5     11.094466         1.78155287 6     6      9.707844         1.38662218 And the plots look something like this: plot(total_data $index, total_data$ popping_times, main=""Popping Times of Popcorn Kernels"", xlab=""Kernel Index"", ylab=""Popping Time (Seconds)"", type = ""b"")  plot(total_data $index, total_data$ popping_times_diff, main=""Popping Time Difference of Popcorn Kernels"", xlab=""Kernel Index"", ylab=""Popping Time Difference (Seconds)"", type = ""b"") Note 2: If we aggregated the data into ""number of kernels that popped within each 10 second interval"" - we might be able to make a time series model (e.g. ARIMA) that predicts how many kernels will pop in the next 10 seconds ... but this is a slightly different problem from the one I am interested in","While placing a bag of popcorn in the microwave today, I noticed the following thing: In the 30 seconds, the time difference between (e.g. treat ""time"" as a random variable) which any two kernel of popcorn can pop has some probability distribution function (e.g. the n-th kernel pops at time T1 and the n_1-th kernel pops at time T2 : T2 - T1 has some probability distribution In the next 30 seconds, the time difference between which any two kernels of popcorn can pop has some other probability distribution - naturally, the time difference between successive kernel pops is obviously shorter In the last 30 seconds, the time difference between which any two kernels of popcorn can pop has another probability distribution - naturally, the time difference between successive kernel pops is longer than this same time difference in the previous 30 seconds. This brings me to my question - could ""popping popcorn"" be modelled as some Stochastic Process (such as a Inhomogeneous Poisson Arrival Process, Markov Switching Process or a Brownian Motion ) ? For instance, suppose we knew that 43 kernels of popcorn have already popped - we also know that time at which each kernel popped (e.g. kernel_1 = 12 seconds, kernel_2 = 19 seconds, kernel_3 = 21 seconds .... kernel_42 = 81 seconds, kernel_43 = 85 seconds). This means we also know the time differences between each successive pair of popcorn kernels (e.g. Kernel_2_1 = 19 - 12 = 7 seconds, Kernel_3_2 = 21 - 19 seconds = 2 seconds, etc.). With this information (and historical data we collected about popcorn kernels popping) , could we make some inference as to when the 44th kernel of popcorn is expected to pop? Thanks! Note 1: I feel a very naive way to approach this problem would be to just fit some regression model through historical data - here I have shown this using the R programming language. library(dplyr)     #first 30 seconds     popping_times = abs(rnorm(40,10,3))     index = 1:40     data_1 = data.frame(index, popping_times)  #next 30 seconds popping_times = abs(rnorm(80,5, 2)) index = 1:80 data_2 = data.frame(index, popping_times)  #next 30 seconds popping_times = abs(rnorm(30,7,3)) index = 1:30 data_3 = data.frame(index, popping_times)  total_data = rbind(data_1, data_2, data_3) total_data$index = 1:nrow(total_data)  total_data = data.frame(total_data %>%  mutate(popping_times_diff = abs(popping_times - lag(popping_times))))  total_data[is.na(total_data)] <- 0 The data looks something like this: index popping_times popping_times_diff 1     1     10.380158         0.00000000 2     2      8.874496         1.50566244 3     3      8.966483         0.09198782 4     4      9.312913         0.34642958 5     5     11.094466         1.78155287 6     6      9.707844         1.38662218 And the plots look something like this: plot(total_data popping_times, main=""Popping Times of Popcorn Kernels"", xlab=""Kernel Index"", ylab=""Popping Time (Seconds)"", type = ""b"")  plot(total_data popping_times_diff, main=""Popping Time Difference of Popcorn Kernels"", xlab=""Kernel Index"", ylab=""Popping Time Difference (Seconds)"", type = ""b"") Note 2: If we aggregated the data into ""number of kernels that popped within each 10 second interval"" - we might be able to make a time series model (e.g. ARIMA) that predicts how many kernels will pop in the next 10 seconds ... but this is a slightly different problem from the one I am interested in","index, total_data index, total_data","['probability', 'stochastic-processes']"
30,2D Random Walk or just simple math?,2D Random Walk or just simple math?,,"I'm looking into a physical problem that involves the following sum: $$r(t)=\sum_{\substack{z_l=0,1\\1\leq l \leq N}}|b_{z_1z_2\dots z_N}|^2e^{-2i\epsilon_{z_1z_2\cdots z_N}t}$$ where $\epsilon_{z_1z_2\cdots z_N}$ are random phases and $\sum |b_{z_1z_2\dots z_N}|^2=1$ , where the $b$ 's are normally distributed. By trying to say something about it, one could look at it as a 2D random walk problem with fixed total lenth (sum of all step lengths). Apparently that yields $$\langle|r(t)|^2|\rangle\propto \langle |b_{z_1z_2\cdots z_N}|^2\rangle=2^{-N}\tag{1}$$ Is it really necessary to look at it this way? Isn't it just a simple calculation involving some calculation rules of $\langle \cdot\rangle$ ? For further information on the physics behind this problem, see the book from Schlosshauer on Decoherence: 978-3-540-35775-9 (p. 91). I myself tried to do it with ""simple math"" although couldn't make it: I calculated $$\mathbb{E}(|r(t)|^2)=\mathbb{E}\left(\left(\sum_{\substack{z_l=0,1\\1\leq l \leq N}}|b_{z_1z_2\dots z_N}|^2e^{-2i\epsilon_{z_1z_2\cdots z_N}t}\right)\left(\sum_{\substack{z_l=0,1\\1\leq l \leq N}}|b_{z_1z_2\dots z_N}|^2e^{2i\epsilon_{z_1z_2\cdots z_N}t}\right)\right)=\left(\left(\sum_{\substack{z_l=0,1\\1\leq l \leq N}}\mathbb{E}(|b_{z_1z_2\dots z_N}|^2)\mathbb{E}(e^{-2i\epsilon_{z_1z_2\cdots z_N}t})\right)\left(\sum_{\substack{z_l=0,1\\1\leq l \leq N}}\mathbb{E}(|b_{z_1z_2\dots z_N}|^2)\mathbb{E}(e^{2i\epsilon_{z_1z_2\cdots z_N}t})\right)\right)=2^{-N}\cdot 2^N \mathbb{E}(e^{-2i\epsilon_{z_1z_2\cdots z_N}t}) \cdot 2^{-N}\cdot 2^N \mathbb{E}(e^{2i\epsilon_{z_1z_2\cdots z_N}t})$$ with $$\mathbb{E}(e^{-2i\epsilon_{z_1z_2\cdots z_N}t})$$ the characteristic function of a random variable $\epsilon$ with (continuous) unform distribution on $[0,2\pi]$ (where no $N$ occurs). As you can see, the factor $N$ is not in the final formula, although $(1)$ says so. Am I making a mistake? This question has been cross-posted on PSE .","I'm looking into a physical problem that involves the following sum: where are random phases and , where the 's are normally distributed. By trying to say something about it, one could look at it as a 2D random walk problem with fixed total lenth (sum of all step lengths). Apparently that yields Is it really necessary to look at it this way? Isn't it just a simple calculation involving some calculation rules of ? For further information on the physics behind this problem, see the book from Schlosshauer on Decoherence: 978-3-540-35775-9 (p. 91). I myself tried to do it with ""simple math"" although couldn't make it: I calculated with the characteristic function of a random variable with (continuous) unform distribution on (where no occurs). As you can see, the factor is not in the final formula, although says so. Am I making a mistake? This question has been cross-posted on PSE .","r(t)=\sum_{\substack{z_l=0,1\\1\leq l \leq N}}|b_{z_1z_2\dots z_N}|^2e^{-2i\epsilon_{z_1z_2\cdots z_N}t} \epsilon_{z_1z_2\cdots z_N} \sum |b_{z_1z_2\dots z_N}|^2=1 b \langle|r(t)|^2|\rangle\propto \langle |b_{z_1z_2\cdots z_N}|^2\rangle=2^{-N}\tag{1} \langle \cdot\rangle \mathbb{E}(|r(t)|^2)=\mathbb{E}\left(\left(\sum_{\substack{z_l=0,1\\1\leq l \leq N}}|b_{z_1z_2\dots z_N}|^2e^{-2i\epsilon_{z_1z_2\cdots z_N}t}\right)\left(\sum_{\substack{z_l=0,1\\1\leq l \leq N}}|b_{z_1z_2\dots z_N}|^2e^{2i\epsilon_{z_1z_2\cdots z_N}t}\right)\right)=\left(\left(\sum_{\substack{z_l=0,1\\1\leq l \leq N}}\mathbb{E}(|b_{z_1z_2\dots z_N}|^2)\mathbb{E}(e^{-2i\epsilon_{z_1z_2\cdots z_N}t})\right)\left(\sum_{\substack{z_l=0,1\\1\leq l \leq N}}\mathbb{E}(|b_{z_1z_2\dots z_N}|^2)\mathbb{E}(e^{2i\epsilon_{z_1z_2\cdots z_N}t})\right)\right)=2^{-N}\cdot 2^N \mathbb{E}(e^{-2i\epsilon_{z_1z_2\cdots z_N}t}) \cdot 2^{-N}\cdot 2^N \mathbb{E}(e^{2i\epsilon_{z_1z_2\cdots z_N}t}) \mathbb{E}(e^{-2i\epsilon_{z_1z_2\cdots z_N}t}) \epsilon [0,2\pi] N N (1)","['probability', 'stochastic-processes', 'physics', 'expected-value', 'random-walk']"
31,"How many dice rolls would it take, for an N-sided die, to guess with 99% probability that the number of sides is N?","How many dice rolls would it take, for an N-sided die, to guess with 99% probability that the number of sides is N?",,Thought of this question today when I was looking at the dungeons and dragons dice.,Thought of this question today when I was looking at the dungeons and dragons dice.,,"['probability', 'statistics', 'discrete-mathematics', 'computer-science']"
32,What is the probability of two people of the same race randomly sitting next to each other in a room?,What is the probability of two people of the same race randomly sitting next to each other in a room?,,"Imagine there are four people, two of them are white, and two of them are asian. There are 2 chairs on one side of a room and 2 on the other side. Everyone is randomly assigned a seat. What is the probability that two people of the same race are seated on the same side of the room? My intuition leads me to believe the probability is 1/3, since it can be thought of in terms of pulling colored balls from a bucket. There are two red balls and two blue balls. The color of the first ball doesn't matter, whichever color is chosen, there will be one left of the same color and two left of opposite color, hence 1/3 chance of choosing a ball of same color. Is my reasoning correct?","Imagine there are four people, two of them are white, and two of them are asian. There are 2 chairs on one side of a room and 2 on the other side. Everyone is randomly assigned a seat. What is the probability that two people of the same race are seated on the same side of the room? My intuition leads me to believe the probability is 1/3, since it can be thought of in terms of pulling colored balls from a bucket. There are two red balls and two blue balls. The color of the first ball doesn't matter, whichever color is chosen, there will be one left of the same color and two left of opposite color, hence 1/3 chance of choosing a ball of same color. Is my reasoning correct?",,"['probability', 'discrete-mathematics']"
33,Convergence of non iid observations on the empirical distribution,Convergence of non iid observations on the empirical distribution,,"Let $f$ be a function on domain $X$ with binary output $f: X\to \{0,1\}$ . We define an arbiatry distribution $\mathcal{Q}$ over $X$ and the empircal distribution of $n$ samples from $\mathcal{Q}$ -- $\mathbf{Q}^n$ By the Glivenko–Cantelli theorem the expected average value of $f$ on the empircal distribution converges to the probability that $f(x)=1$ : \begin{equation} \mathbb{E}_{\mathbf{Q}^n}\left[\frac{1}{n}\sum_{x\in \mathbf{Q}^n} f(x)\right] \longrightarrow \mathbb{P}_{x\sim \mathcal{Q}}[f(x)=1] \end{equation} Now to makes things slightly more complicated we consider $F$ to be some class of binary functions. We can consider a simple game where we draw $n$ samples from $\mathcal{Q}$ (i.e sample an empircal distribution $\mathbf{Q}^n$ ) then find the function $f\in F$ that minimizes the sum $\sum_{x\in \mathbf{Q}^n} f(x)$ . I would like to show that this the mean of $f$ on $\mathbf{Q}^n$ converges (at least weakly): \begin{equation} \mathbb{E}_{\mathbf{Q}^n}\left[\min_{f\in F}\frac{1}{n}\sum_{x\in \mathbf{Q}^n} f(x)\right] \longrightarrow \min_{f\in F} \mathbb{P}_{x\sim \mathcal{Q}}[f(x)=1] \end{equation} This seems intuitively true to me, but the problem is that $f(x)$ is not iid for all $x\in \mathbf{Q}^n$ (a simple example is to let $F$ be the class of functions that output $1$ only in some fixed $L_p$ ball then if you observe $f(x_i)=1$ you know that $f(x_j)=0$ for any $x_j$ that cannot be contained in a ball with $x_i$ ). I'm a bit stuck on how to reason about this dependence structure, but I'm still convinced that Equation 2 should converge, at least for some minimal set of assumptions on the behaviour of $F$ . Thanks","Let be a function on domain with binary output . We define an arbiatry distribution over and the empircal distribution of samples from -- By the Glivenko–Cantelli theorem the expected average value of on the empircal distribution converges to the probability that : Now to makes things slightly more complicated we consider to be some class of binary functions. We can consider a simple game where we draw samples from (i.e sample an empircal distribution ) then find the function that minimizes the sum . I would like to show that this the mean of on converges (at least weakly): This seems intuitively true to me, but the problem is that is not iid for all (a simple example is to let be the class of functions that output only in some fixed ball then if you observe you know that for any that cannot be contained in a ball with ). I'm a bit stuck on how to reason about this dependence structure, but I'm still convinced that Equation 2 should converge, at least for some minimal set of assumptions on the behaviour of . Thanks","f X f: X\to \{0,1\} \mathcal{Q} X n \mathcal{Q} \mathbf{Q}^n f f(x)=1 \begin{equation}
\mathbb{E}_{\mathbf{Q}^n}\left[\frac{1}{n}\sum_{x\in \mathbf{Q}^n} f(x)\right] \longrightarrow \mathbb{P}_{x\sim \mathcal{Q}}[f(x)=1]
\end{equation} F n \mathcal{Q} \mathbf{Q}^n f\in F \sum_{x\in \mathbf{Q}^n} f(x) f \mathbf{Q}^n \begin{equation}
\mathbb{E}_{\mathbf{Q}^n}\left[\min_{f\in F}\frac{1}{n}\sum_{x\in \mathbf{Q}^n} f(x)\right] \longrightarrow \min_{f\in F} \mathbb{P}_{x\sim \mathcal{Q}}[f(x)=1]
\end{equation} f(x) x\in \mathbf{Q}^n F 1 L_p f(x_i)=1 f(x_j)=0 x_j x_i F","['probability', 'expected-value', 'weak-convergence', 'empirical-processes']"
34,Expected number of isolated pairs,Expected number of isolated pairs,,"An urn contains $a$ white balls and $b$ black balls. We draw balls from the urn uniformly at random without replacement, and line the balls up as we draw them out. We stop once we draw three white balls in a row. When lined up, what is the expected number of isolated pairs of white balls? Assume $a\geq 2b+3$ , so the game is guaranteed to end with drawing three consecutive white balls. I found this problem here , which gives a straightforward inductive approach. However, the answer of $\frac{b}{a+1}$ is surprisingly clean. I was wondering if there's a clean solution with a nice probablistic interpretation, rather than a purely algebraic one based on induction. Any ideas are appreciated!","An urn contains white balls and black balls. We draw balls from the urn uniformly at random without replacement, and line the balls up as we draw them out. We stop once we draw three white balls in a row. When lined up, what is the expected number of isolated pairs of white balls? Assume , so the game is guaranteed to end with drawing three consecutive white balls. I found this problem here , which gives a straightforward inductive approach. However, the answer of is surprisingly clean. I was wondering if there's a clean solution with a nice probablistic interpretation, rather than a purely algebraic one based on induction. Any ideas are appreciated!",a b a\geq 2b+3 \frac{b}{a+1},"['probability', 'random-variables', 'expected-value']"
35,Discrepancy between binomial and nCr methods of calculating probability?,Discrepancy between binomial and nCr methods of calculating probability?,,"A bag has 1000 balls, some colored red, and the rest green. The fraction of red-colored balls in the bag is 0.2. A blindfolded observer picks out 5 balls from this bag. What is the probability that 3 reds are picked? Method 1: The probability p of picking a red ball is 0.2. So, this can be treated as a binomial problem: $ Pr$ (3 red balls picked) $= {5 \choose 3} (0.2)^3 (0.8)^2 = 0.0512$ Method 2: We choose 3 red balls from a group of 200 and 2 green ones from the group of 800. So, the probability becomes: $ \frac{{200 \choose 3} {800 \choose 2}} {{1000 \choose 5}} \approx 0.0508785...$ Why is there a slight discrepancy in the probabilities? Is it an artifact of my calculator's limitations or am I missing something deeper?","A bag has 1000 balls, some colored red, and the rest green. The fraction of red-colored balls in the bag is 0.2. A blindfolded observer picks out 5 balls from this bag. What is the probability that 3 reds are picked? Method 1: The probability p of picking a red ball is 0.2. So, this can be treated as a binomial problem: (3 red balls picked) Method 2: We choose 3 red balls from a group of 200 and 2 green ones from the group of 800. So, the probability becomes: Why is there a slight discrepancy in the probabilities? Is it an artifact of my calculator's limitations or am I missing something deeper?", Pr = {5 \choose 3} (0.2)^3 (0.8)^2 = 0.0512  \frac{{200 \choose 3} {800 \choose 2}} {{1000 \choose 5}} \approx 0.0508785...,"['probability', 'combinatorics', 'binomial-distribution']"
36,What distribution over X yields uniformly distributed distances between elements of X?,What distribution over X yields uniformly distributed distances between elements of X?,,"I'm curious if you can find a probability distribution over $\mathcal{X} \subseteq \mathbb{R}^d$ , such that the random variable $D = d(X,Y)$ is uniformly distributed over the interval $[0, diam(\mathcal{X})]$ , where $X,Y$ are i.i.d. If this is even possible, what would that distribution look like? Are there constraints on $\mathcal{X}$ that make this possible/impossible?","I'm curious if you can find a probability distribution over , such that the random variable is uniformly distributed over the interval , where are i.i.d. If this is even possible, what would that distribution look like? Are there constraints on that make this possible/impossible?","\mathcal{X} \subseteq \mathbb{R}^d D = d(X,Y) [0, diam(\mathcal{X})] X,Y \mathcal{X}","['probability', 'probability-theory', 'random-variables']"
37,Find a compund Poisson variable with characterist function as centered compound Poisson,Find a compund Poisson variable with characterist function as centered compound Poisson,,"I know that if $(X_j , j \geq 1)$ is a sequence of i.i.d. process with $\sigma$ being the probability distribution of $X_j's$ and $N \sim \text{Poisson}(\lambda)$ independent of the $X_j's$ , then the compund Poisson random variable $$Z = \sum_j^N X_j \sim CP(\lambda, \sigma)$$ is such that its characteristic function is given by: $$\varphi_X(t)= e^{\lambda(\,\phi_X(t)- 1 \,)}, \varphi_X(t) = \int_{\mathbb{R}}e^{itx}d\sigma(x)$$ Moreover, for the centered compound Poisson variable $\tilde{Z} = Z - \mathbb{E}Z = Z - \lambda \mathbb{E}X$ , its ch. f. is given by \begin{equation}\label{eq1} \varphi_{\tilde{Z}}(t)= e^{\lambda\phi(t)},\quad \phi(t)= \int_{\mathbb{R}}\Big(e^itx - 1 - itx\Big)d \sigma(x)\tag{1} \end{equation} But I would like to know if there is any $\bar{X}_j$ so that the characteristic function of $$\bar{Z}= \sum_{j=1}^N \bar{X}_j$$ is given, directly (without centering), by equation (\ref{eq1}). I tried putting $\bar{X}_j = X_j - \mathbb{E}X_j$ , but it didn't work. some help!","I know that if is a sequence of i.i.d. process with being the probability distribution of and independent of the , then the compund Poisson random variable is such that its characteristic function is given by: Moreover, for the centered compound Poisson variable , its ch. f. is given by But I would like to know if there is any so that the characteristic function of is given, directly (without centering), by equation (\ref{eq1}). I tried putting , but it didn't work. some help!","(X_j , j \geq 1) \sigma X_j's N \sim \text{Poisson}(\lambda) X_j's Z = \sum_j^N X_j \sim CP(\lambda, \sigma) \varphi_X(t)= e^{\lambda(\,\phi_X(t)- 1 \,)}, \varphi_X(t) = \int_{\mathbb{R}}e^{itx}d\sigma(x) \tilde{Z} = Z - \mathbb{E}Z = Z - \lambda \mathbb{E}X \begin{equation}\label{eq1}
\varphi_{\tilde{Z}}(t)= e^{\lambda\phi(t)},\quad \phi(t)= \int_{\mathbb{R}}\Big(e^itx - 1 - itx\Big)d \sigma(x)\tag{1}
\end{equation} \bar{X}_j \bar{Z}= \sum_{j=1}^N \bar{X}_j \bar{X}_j = X_j - \mathbb{E}X_j","['probability', 'measure-theory', 'poisson-distribution']"
38,Symmetrization and the CLT,Symmetrization and the CLT,,"Let $(X_n)$ be an independent sequence of real standard normal random variables and let $(\boldsymbol{\Sigma}_n)$ be a sequence of $n \times n$ (growing size) real positive definite matrices. Define the triangular array $(Y_{i,n})_{1\leq i\leq n}$ as follows: $$\boldsymbol{Y}_n:=\boldsymbol\Sigma_n^{1/2}\boldsymbol{X}_n$$ where $\boldsymbol{Y}_n=(Y_{1,n},\dots,Y_{n,n})^\top$ and $\boldsymbol{X}_n=(X_{1},\dots,X_{n})^\top$ . Let $\boldsymbol \Sigma_n=\boldsymbol Q_n^\top\boldsymbol \Lambda_n \boldsymbol Q_n$ be an eigendecomposition of $\boldsymbol \Sigma_n$ . Then $$\boldsymbol{Y}_n^\top \boldsymbol{Y}_n=\boldsymbol{Z}_n^\top \boldsymbol \Lambda_n \boldsymbol{Z}_n$$ where $\boldsymbol{Z}_n:=\boldsymbol Q_n \boldsymbol{X}_n$ is a standard normal vector of uncorrelated, hence independent, random variables. I want to apply the CLT for triangular arrays to the  normalized sum $$\xi_n:=\frac{1}{s_n}\sum_{i=1}^n (Y^2_{i,n}-\sigma_{ii,n})=\frac{1}{s_n}\sum_{i=1}^n \lambda_{i,n}(Z_{i,n}^2-1)$$ where $s^2_n=Var(\sum_{i=1}^n Y^2_{i,n})=Var(\sum_{i=1}^n \lambda_{i,n}Z_{i,n}^2)=2\sum_{i=1}^n \lambda^2_{i,n}$ . The Lyapounov's condition for $\delta=2$ is $$\frac{E[(Z_{i,n}^2-1)^4]}{4} \frac{\sum_{i=1}^n \lambda^4_{i,n}}{(\sum_{i=1}^n \lambda^2_{i,n})^2}=15 \frac{Tr(\boldsymbol\Sigma_n^4)}{Tr(\boldsymbol\Sigma_n^2)^2} \to 0 \quad \text{as} \quad n\to \infty $$ The specific example am considering is with the tridiagonal matrix $$\boldsymbol\Sigma_n = \begin{bmatrix} 1 & 1/2 \\ 1/2 & 1 & 1/2 \\ & 1/2 & 1 & \ddots \\ & & \ddots & \ddots & \ddots \\ & & & \ddots  & 1 & 1/2 \\ & & & & 1/2 & 1 & 1/2 \\ & & & & & 1/2 & 1 \end{bmatrix}$$ whose eigenvalues are given by $\lambda_{i,n}=1+\cos(\frac{i\pi}{n+1})$ , $i=1,\dots,n$ . One checks that the Lyapounov's condition above is satisfied in this case. Simulations with $n=200$ reveal that the tails of the distribution of $\xi_n$ are not particularly well approximated by the normal distribution. On the other hand the tails of the symmetrized statistic $\xi_n-\xi'_n$ , where $\xi'_n$ is an independent copy of $\xi_n$ , seem to be much better fitted by the normal distribution. Are there any theoretical reasons for this? Thanks a lot for your help.","Let be an independent sequence of real standard normal random variables and let be a sequence of (growing size) real positive definite matrices. Define the triangular array as follows: where and . Let be an eigendecomposition of . Then where is a standard normal vector of uncorrelated, hence independent, random variables. I want to apply the CLT for triangular arrays to the  normalized sum where . The Lyapounov's condition for is The specific example am considering is with the tridiagonal matrix whose eigenvalues are given by , . One checks that the Lyapounov's condition above is satisfied in this case. Simulations with reveal that the tails of the distribution of are not particularly well approximated by the normal distribution. On the other hand the tails of the symmetrized statistic , where is an independent copy of , seem to be much better fitted by the normal distribution. Are there any theoretical reasons for this? Thanks a lot for your help.","(X_n) (\boldsymbol{\Sigma}_n) n \times n (Y_{i,n})_{1\leq i\leq n} \boldsymbol{Y}_n:=\boldsymbol\Sigma_n^{1/2}\boldsymbol{X}_n \boldsymbol{Y}_n=(Y_{1,n},\dots,Y_{n,n})^\top \boldsymbol{X}_n=(X_{1},\dots,X_{n})^\top \boldsymbol \Sigma_n=\boldsymbol Q_n^\top\boldsymbol \Lambda_n \boldsymbol Q_n \boldsymbol \Sigma_n \boldsymbol{Y}_n^\top \boldsymbol{Y}_n=\boldsymbol{Z}_n^\top \boldsymbol \Lambda_n \boldsymbol{Z}_n \boldsymbol{Z}_n:=\boldsymbol Q_n \boldsymbol{X}_n \xi_n:=\frac{1}{s_n}\sum_{i=1}^n (Y^2_{i,n}-\sigma_{ii,n})=\frac{1}{s_n}\sum_{i=1}^n \lambda_{i,n}(Z_{i,n}^2-1) s^2_n=Var(\sum_{i=1}^n Y^2_{i,n})=Var(\sum_{i=1}^n \lambda_{i,n}Z_{i,n}^2)=2\sum_{i=1}^n \lambda^2_{i,n} \delta=2 \frac{E[(Z_{i,n}^2-1)^4]}{4} \frac{\sum_{i=1}^n \lambda^4_{i,n}}{(\sum_{i=1}^n \lambda^2_{i,n})^2}=15 \frac{Tr(\boldsymbol\Sigma_n^4)}{Tr(\boldsymbol\Sigma_n^2)^2} \to 0 \quad \text{as} \quad n\to \infty  \boldsymbol\Sigma_n = \begin{bmatrix}
1 & 1/2 \\
1/2 & 1 & 1/2 \\
& 1/2 & 1 & \ddots \\
& & \ddots & \ddots & \ddots \\
& & & \ddots  & 1 & 1/2 \\
& & & & 1/2 & 1 & 1/2 \\
& & & & & 1/2 & 1
\end{bmatrix} \lambda_{i,n}=1+\cos(\frac{i\pi}{n+1}) i=1,\dots,n n=200 \xi_n \xi_n-\xi'_n \xi'_n \xi_n","['probability', 'probability-theory', 'statistics', 'eigenvalues-eigenvectors', 'central-limit-theorem']"
39,Understanding Game Theory Terminology in Probability Based Games,Understanding Game Theory Terminology in Probability Based Games,,"I have no background and Economics and am trying to teach myself about some basic things in Game Theory. For example, I am trying to understand the following terms: Nash Equilibrium Optimal Strategy Saddle Point To illustrate these concepts, suppose we have the following game (I think the game I have created is called a ""Stackelberg Game""): There are 2 players: Player 1 and Player 2 There are 2 Coins : Coin A and Coin B Coin A has a 0.5 Probability of landing on Heads and a 0.5 Probability of landing on Tails Coin B has a 0.7 Probability of landing on Heads and a 0.3 Probability of landing on Tails If Coin A lands on Heads, a score of +1 is obtained - if Coin A lands on Tails, a score of -1 is obtained. If coin B lands on Heads, a score of -2 is obtained - if Coin A lands on Tails, a score of +3 is obtained. In this game: Player 1 selects a coin and then flips this coin and records his score Next, Player 2 selects a coin and then flips this coin and records his score The player with the highest score wins In this game, Player 2 always has an advantage. He see what coin Player 1 picked and select the more favorable coin based on the choice of Player 1. If Player 1 picked Coin B and got ""unlucky"", Player 2 automatically wins if he picks Coin A If Player 1 picked Coin B and got ""lucky"", Player 2 can only win if he also picks Coin A If Player 1 picked Coin A, regardless of Player 1's result - Player 2 should also pick Coin A if he wants to minimize his chances of loosing My Question: In this game that I created, I am trying to identify the Nash Equilibrium, Optimal Strategy and Saddle Point : I am confused between the concepts of Nash Equilibrium and Optimal Strategies. Based on the analysis I provided above, it seems like the Optimal Strategy in this game is for both players to always select Coin A - Would this be the Nash Equilibrium? I do not understand the concept of a Saddle Point in Game Theory. From Calculus and Optimization, I understand that a Saddle Point is a point on a function in which the first derivatives of the function at that point are 0 but the function does not have a maximum or a minimum of any sort at that point - in Machine Learning, Saddle Points are considered to be obstacles when trying to ""fine tune"" Machine Learning Models. However, I read a bit about Saddle Points in Game Theory, but I don't quite understand how to identify them or why they are important. Does the game I created have a Saddle Point? If so, what does the Saddle Point in this game ""mean"" (e.g. Does the Saddle Point simultaneously identify the ""best case action for Player 1 and the worst case action for Player 2"" )? If this game does not have a Saddle Point - can we ""modify this game"" (e.g. add more coins, e.g. Coin A, Coin B, Coin C, etc.) such that a Saddle Point can exist? Thanks!","I have no background and Economics and am trying to teach myself about some basic things in Game Theory. For example, I am trying to understand the following terms: Nash Equilibrium Optimal Strategy Saddle Point To illustrate these concepts, suppose we have the following game (I think the game I have created is called a ""Stackelberg Game""): There are 2 players: Player 1 and Player 2 There are 2 Coins : Coin A and Coin B Coin A has a 0.5 Probability of landing on Heads and a 0.5 Probability of landing on Tails Coin B has a 0.7 Probability of landing on Heads and a 0.3 Probability of landing on Tails If Coin A lands on Heads, a score of +1 is obtained - if Coin A lands on Tails, a score of -1 is obtained. If coin B lands on Heads, a score of -2 is obtained - if Coin A lands on Tails, a score of +3 is obtained. In this game: Player 1 selects a coin and then flips this coin and records his score Next, Player 2 selects a coin and then flips this coin and records his score The player with the highest score wins In this game, Player 2 always has an advantage. He see what coin Player 1 picked and select the more favorable coin based on the choice of Player 1. If Player 1 picked Coin B and got ""unlucky"", Player 2 automatically wins if he picks Coin A If Player 1 picked Coin B and got ""lucky"", Player 2 can only win if he also picks Coin A If Player 1 picked Coin A, regardless of Player 1's result - Player 2 should also pick Coin A if he wants to minimize his chances of loosing My Question: In this game that I created, I am trying to identify the Nash Equilibrium, Optimal Strategy and Saddle Point : I am confused between the concepts of Nash Equilibrium and Optimal Strategies. Based on the analysis I provided above, it seems like the Optimal Strategy in this game is for both players to always select Coin A - Would this be the Nash Equilibrium? I do not understand the concept of a Saddle Point in Game Theory. From Calculus and Optimization, I understand that a Saddle Point is a point on a function in which the first derivatives of the function at that point are 0 but the function does not have a maximum or a minimum of any sort at that point - in Machine Learning, Saddle Points are considered to be obstacles when trying to ""fine tune"" Machine Learning Models. However, I read a bit about Saddle Points in Game Theory, but I don't quite understand how to identify them or why they are important. Does the game I created have a Saddle Point? If so, what does the Saddle Point in this game ""mean"" (e.g. Does the Saddle Point simultaneously identify the ""best case action for Player 1 and the worst case action for Player 2"" )? If this game does not have a Saddle Point - can we ""modify this game"" (e.g. add more coins, e.g. Coin A, Coin B, Coin C, etc.) such that a Saddle Point can exist? Thanks!",,"['probability', 'optimization', 'game-theory']"
40,"Is it possible to partition $[a,b]$ into countably many ""identical"" sets which are translation from each other?","Is it possible to partition  into countably many ""identical"" sets which are translation from each other?","[a,b]","Given $[0,1]$ , is it possible to find one set $A_1\subset[0,1]$ and $A_i = A_1 + a_i, i\in\mathbb{N}, a_i\in\mathbb{R}$ such that $\{A_i\}$ is a countable partition of $[0,1]$ . By $A_1 + a_i$ , it means translating $A_1$ by the value of $a_i$ , e.g. $[0, 0.5] + 0.25\triangleq [0.25, 0.75]$ . Edit: It turns out with constraint of modulo one, this $A_1$ could be the non-measurable Vitali set. And $[0,1] = \cup_{q\in Q\cap[0,1]}(V_1 + q)$ .","Given , is it possible to find one set and such that is a countable partition of . By , it means translating by the value of , e.g. . Edit: It turns out with constraint of modulo one, this could be the non-measurable Vitali set. And .","[0,1] A_1\subset[0,1] A_i = A_1 + a_i, i\in\mathbb{N}, a_i\in\mathbb{R} \{A_i\} [0,1] A_1 + a_i A_1 a_i [0, 0.5] + 0.25\triangleq [0.25, 0.75] A_1 [0,1] = \cup_{q\in Q\cap[0,1]}(V_1 + q)",['probability']
41,Fourier transform of uniform distribution on irreducible representation,Fourier transform of uniform distribution on irreducible representation,,"I'm reading through Diaconis' notes Group Representations in Probability and Statistics , and working on the following exercise, where $\hat{P}$ denotes the Fourier transform of $P$ : Recall that the uniform distribution is defined by $U(s) = \frac{1}{|G|}$ where $|G|$ is the order of a group $G$ . Then at the trivial representation $\hat{U}(\rho) = 1$ and at any other non-trivial irreducible representation $\hat{U}(\rho) = 0$ . I think I have proof by contradiction, but I'm a little unsure about it and would love some feedback. Also, if there is a direct proof I would love to hear about it. Proof: First, let $\rho$ be the trivial representation. Then $$\hat{U}(\rho) = \sum_s U(s) \rho(s) = \sum_s \frac{1}{|G|} = 1.$$ Next, let $\rho$ be a nontrivial irreducible representation carried by $V$ . First, note that $\operatorname{im}\hat{U}(\rho)$ is an invariant subspace of $V$ : $$\rho(t)\hat{U}(\rho)(v) = \rho(t)\left( \frac{1}{|G|} \sum_s \rho(s) \right) v = \left( \frac{1}{|G|} \sum_s \rho(t) \rho(s) \right) v = \hat{U}(\rho)(v)$$ for all $v \in V$ . Thus, $\operatorname{im}\hat{U}(\rho) = 0$ or $V$ . Suppose the latter for the sake of contradiction. Then $\hat{U}(\rho)$ is an isomorphism. Let $v \in V$ and $s \in G$ such that $\rho(s)v \neq v$ . Such a pair of $v, s$ exists because otherwise, the span of $v$ would be an invariant subspace corresponding to the trivial representation. Then $$ \hat{U}(\rho)(\rho(s) v) = \hat{U}(\rho) v$$ by translation, contradicting the injectivity of $\hat{U}(\rho)$ . Thus, $\operatorname{im}\hat{U}(\rho) = 0$ , so $\hat{U}(\rho) = 0$ . $\square$ Thanks!","I'm reading through Diaconis' notes Group Representations in Probability and Statistics , and working on the following exercise, where denotes the Fourier transform of : Recall that the uniform distribution is defined by where is the order of a group . Then at the trivial representation and at any other non-trivial irreducible representation . I think I have proof by contradiction, but I'm a little unsure about it and would love some feedback. Also, if there is a direct proof I would love to hear about it. Proof: First, let be the trivial representation. Then Next, let be a nontrivial irreducible representation carried by . First, note that is an invariant subspace of : for all . Thus, or . Suppose the latter for the sake of contradiction. Then is an isomorphism. Let and such that . Such a pair of exists because otherwise, the span of would be an invariant subspace corresponding to the trivial representation. Then by translation, contradicting the injectivity of . Thus, , so . Thanks!","\hat{P} P U(s) = \frac{1}{|G|} |G| G \hat{U}(\rho) = 1 \hat{U}(\rho) = 0 \rho \hat{U}(\rho) = \sum_s U(s) \rho(s) = \sum_s \frac{1}{|G|} = 1. \rho V \operatorname{im}\hat{U}(\rho) V \rho(t)\hat{U}(\rho)(v) = \rho(t)\left( \frac{1}{|G|} \sum_s \rho(s) \right) v = \left( \frac{1}{|G|} \sum_s \rho(t) \rho(s) \right) v = \hat{U}(\rho)(v) v \in V \operatorname{im}\hat{U}(\rho) = 0 V \hat{U}(\rho) v \in V s \in G \rho(s)v \neq v v, s v  \hat{U}(\rho)(\rho(s) v) = \hat{U}(\rho) v \hat{U}(\rho) \operatorname{im}\hat{U}(\rho) = 0 \hat{U}(\rho) = 0 \square","['probability', 'solution-verification', 'representation-theory', 'fourier-transform']"
42,How close is the median of a uniformly chosen subsample to the median of the full set?,How close is the median of a uniformly chosen subsample to the median of the full set?,,"Take $N$ a set of n numbers, sample s numbers from $N$ uniformly and with replacement giving us the set $S$ . What is the relationship between the median of $N$ and the median of $S$ ? I want a result that looks like: With probability ... the median of $S$ has a rank in $N$ in $\left[\left(\frac12 - d\right)n, \left(\frac12 + d\right) n\right]$ . I am actually interested in a proof of Lemma 13 of https://dl.acm.org/doi/pdf/10.1145/3409964.3461790 (which comes without a proof) Thanks :)","Take a set of n numbers, sample s numbers from uniformly and with replacement giving us the set . What is the relationship between the median of and the median of ? I want a result that looks like: With probability ... the median of has a rank in in . I am actually interested in a proof of Lemma 13 of https://dl.acm.org/doi/pdf/10.1145/3409964.3461790 (which comes without a proof) Thanks :)","N N S N S S N \left[\left(\frac12 - d\right)n, \left(\frac12 + d\right) n\right]","['probability', 'discrete-mathematics', 'algorithms', 'sampling', 'median']"
43,Limit of expectation of a function of sufficient statistics,Limit of expectation of a function of sufficient statistics,,"Let $h$ be abounded differentiable function on $[0, \infty)$ , vanishing at zero, $h(0)=0$ . If $Z$ has a standard normal distribution, $Z\sim\mathcal{N}(0, 1)$ , find $$\lim_{n\rightarrow \infty}n \mathbb{E}[h(1/(n^2Z^2))].$$ Noticing that $h(1/x^2)$ is absolutely integrable, and that $Z^2$ is the sufficient statistics for exponential family $Z\sim \mathcal{N}(0, \sigma^2)$ , I was trying to relate this problem to calculating the cumulants of $Z^2$ by differential identities of the exponential family. However, I was stuck and not sure whether this is the intended way to solve this problem. Any hints would be greatly appreciated. Solutions using properties of exponential family are preferable, as this is an exercise on this topic.","Let be abounded differentiable function on , vanishing at zero, . If has a standard normal distribution, , find Noticing that is absolutely integrable, and that is the sufficient statistics for exponential family , I was trying to relate this problem to calculating the cumulants of by differential identities of the exponential family. However, I was stuck and not sure whether this is the intended way to solve this problem. Any hints would be greatly appreciated. Solutions using properties of exponential family are preferable, as this is an exercise on this topic.","h [0, \infty) h(0)=0 Z Z\sim\mathcal{N}(0, 1) \lim_{n\rightarrow \infty}n \mathbb{E}[h(1/(n^2Z^2))]. h(1/x^2) Z^2 Z\sim \mathcal{N}(0, \sigma^2) Z^2","['probability', 'statistics']"
44,$\pi$ estimation with a coin,estimation with a coin,\pi,"I have a topic in which I have to estimate the value of $\pi$ by throwing coins and write the calculations in excel. I found this article but I can't figure out how $N_0$ was determined. Here in the example where $M=100$ and $N=1000$ , $N_0$ is $80$ but why? And also in the second picture there is $M=100$ , $N=1000000$ , and $N_0= 78929$ . Thank you !","I have a topic in which I have to estimate the value of by throwing coins and write the calculations in excel. I found this article but I can't figure out how was determined. Here in the example where and , is but why? And also in the second picture there is , , and . Thank you !",\pi N_0 M=100 N=1000 N_0 80 M=100 N=1000000 N_0= 78929,"['probability', 'statistics']"
45,Odds of testing the good light bulb,Odds of testing the good light bulb,,"We have three light bulbs on a shelf. We randomly pick one for testing. We know that among the bulbs there are two relatively non-optimal ('bad') and one relatively optimal ('good') regarding luminosity. Let $\alpha, \beta$ and $\gamma$ denote the three bulbs, where $\alpha$ is the appropriate bulb. The corresponding probabilities that a given bulb will produce maximal luminosity per trial are $P_{\alpha}$ , $P_{\beta}$ , and $P_{\gamma}$ , respectively. We carry out $N$ independent tests for which we observe that the chosen bulb worked well $k$ times. What's the probability that we picked up the the good one for testing? My solution: Let $A$ denote the event that we picked out the appropriate bulb, and let $B$ denote the event that $k$ successes occurred in $N$ trials. Then the desired conditional probability is the following: \begin{align*} P(A \mid B) &= \frac{P(A \cap B)}{P(B)}\\ &=  \frac{\binom{N}{k}\cdot P_{\alpha}^{k} \cdot\left( 1 - P_{\alpha}\right)^{N -k}}{\binom{N}{k}\cdot P_{\alpha}^{k} \cdot\left( 1 - P_{\alpha}\right)^{N -k} + \binom{N}{k}\cdot P_{\beta}^{k} \cdot\left( 1 - P_{\beta}\right)^{N -k} + \binom{N}{k}\cdot P_{\gamma}^{k} \cdot\left( 1 - P_{\gamma}\right)^{N -k}}\\  &= \frac{P_{\alpha}^{k} \cdot\left( 1 - P_{\alpha}\right)^{N -k}}{P_{\alpha}^{k} \cdot\left( 1 - P_{\alpha}\right)^{N -k} + P_{\beta}^{k} \cdot\left( 1 - P_{\beta}\right)^{N -k} + P_{\gamma}^{k} \cdot\left( 1 - P_{\gamma}\right)^{N -k}}. \end{align*}","We have three light bulbs on a shelf. We randomly pick one for testing. We know that among the bulbs there are two relatively non-optimal ('bad') and one relatively optimal ('good') regarding luminosity. Let and denote the three bulbs, where is the appropriate bulb. The corresponding probabilities that a given bulb will produce maximal luminosity per trial are , , and , respectively. We carry out independent tests for which we observe that the chosen bulb worked well times. What's the probability that we picked up the the good one for testing? My solution: Let denote the event that we picked out the appropriate bulb, and let denote the event that successes occurred in trials. Then the desired conditional probability is the following:","\alpha, \beta \gamma \alpha P_{\alpha} P_{\beta} P_{\gamma} N k A B k N \begin{align*}
P(A \mid B) &= \frac{P(A \cap B)}{P(B)}\\ &= 
\frac{\binom{N}{k}\cdot P_{\alpha}^{k} \cdot\left( 1 - P_{\alpha}\right)^{N -k}}{\binom{N}{k}\cdot P_{\alpha}^{k} \cdot\left( 1 - P_{\alpha}\right)^{N -k} + \binom{N}{k}\cdot P_{\beta}^{k} \cdot\left( 1 - P_{\beta}\right)^{N -k} + \binom{N}{k}\cdot P_{\gamma}^{k} \cdot\left( 1 - P_{\gamma}\right)^{N -k}}\\
 &= \frac{P_{\alpha}^{k} \cdot\left( 1 - P_{\alpha}\right)^{N -k}}{P_{\alpha}^{k} \cdot\left( 1 - P_{\alpha}\right)^{N -k} + P_{\beta}^{k} \cdot\left( 1 - P_{\beta}\right)^{N -k} + P_{\gamma}^{k} \cdot\left( 1 - P_{\gamma}\right)^{N -k}}.
\end{align*}","['probability', 'combinatorics', 'discrete-mathematics', 'conditional-probability']"
46,"$\max\{ aX_n, b Y_n\} \to \max\{aX,bY\}$",,"\max\{ aX_n, b Y_n\} \to \max\{aX,bY\}","Suppose $(X_n,Y_n)$ is a sequence of bivariate random variables. Suppose there exists a bivariate random variable $(X,Y)$ such that $X_n\to X$ and $Y_n\to Y$ in distribution. Suppose further that for all $a,b\in \mathbb{R}$ we have $$\max\{aX_n, bY_n\} \stackrel{d}{\to} \max\{aX,bY\}.$$ Then does $(X_n,Y_n)\stackrel{d}{\to} (X,Y)$ ? I am not sure if it is at all true. The question is pretty similar to the Cramer Wold device but with the max operator. Note that we may write $$aX_n+bY_n= \max\{aX_n, bY_n\}+\min\{aX_n, bY_n\}=\max\{aX_n, bY_n\}-\max\{-aX_n, -bY_n\}.$$ However, this does not quite help. Any thought or help is appreciated.","Suppose is a sequence of bivariate random variables. Suppose there exists a bivariate random variable such that and in distribution. Suppose further that for all we have Then does ? I am not sure if it is at all true. The question is pretty similar to the Cramer Wold device but with the max operator. Note that we may write However, this does not quite help. Any thought or help is appreciated.","(X_n,Y_n) (X,Y) X_n\to X Y_n\to Y a,b\in \mathbb{R} \max\{aX_n, bY_n\} \stackrel{d}{\to} \max\{aX,bY\}. (X_n,Y_n)\stackrel{d}{\to} (X,Y) aX_n+bY_n= \max\{aX_n, bY_n\}+\min\{aX_n, bY_n\}=\max\{aX_n, bY_n\}-\max\{-aX_n, -bY_n\}.","['probability', 'weak-convergence', 'probability-limit-theorems']"
47,Coin Flipping Riddle,Coin Flipping Riddle,,"2 criminals A and B, were recently captured and brought to prison. They were then locked in two separate rooms. Known for being exceedingly smart, the prison warden set a test for them. They flip a fair coin an infinite number of times and told outcomes of odd trials to A and even trials to B. Now A and B are separately told to pick a trial whose outcome they don't know, i.e., A is supposed to pick an even number, and B is supposed to pick an odd number. If the outcomes of the trials A and B picked are the same, the prison warden will release them. If they are different, they will spend life in jail. The Warden told them what they were going to do and let them agree upon a common strategy in advance but after that they can't communicate. Find a strategy such that the chance of winning is higher than $0.5$ . I recently tried this problem in university and I am interested to see other people's solutions, and perhaps the optimal solution. Edit: My strategy is that both players agree that as soon as they see 2 tails in a row for themselves, they pick the number in the middle, i.e. A sees a tail on coin flip 2 and 4 so he picks 3, B does the same. After running this on a computer simulation I get a 60% winrate. Although I don't fully understand why. Edit: a 70% chance of winning has been found on my Puzzling StackExchange duplicate post! Strategies so Far 70% chance of winning by @Jaap Scherphuis 2/3 probability of winning by @Mike Earnest 62.5% chance of winning by @Teo Miklethun","2 criminals A and B, were recently captured and brought to prison. They were then locked in two separate rooms. Known for being exceedingly smart, the prison warden set a test for them. They flip a fair coin an infinite number of times and told outcomes of odd trials to A and even trials to B. Now A and B are separately told to pick a trial whose outcome they don't know, i.e., A is supposed to pick an even number, and B is supposed to pick an odd number. If the outcomes of the trials A and B picked are the same, the prison warden will release them. If they are different, they will spend life in jail. The Warden told them what they were going to do and let them agree upon a common strategy in advance but after that they can't communicate. Find a strategy such that the chance of winning is higher than . I recently tried this problem in university and I am interested to see other people's solutions, and perhaps the optimal solution. Edit: My strategy is that both players agree that as soon as they see 2 tails in a row for themselves, they pick the number in the middle, i.e. A sees a tail on coin flip 2 and 4 so he picks 3, B does the same. After running this on a computer simulation I get a 60% winrate. Although I don't fully understand why. Edit: a 70% chance of winning has been found on my Puzzling StackExchange duplicate post! Strategies so Far 70% chance of winning by @Jaap Scherphuis 2/3 probability of winning by @Mike Earnest 62.5% chance of winning by @Teo Miklethun",0.5,"['probability', 'puzzle']"
48,Likelihood the wordle of the day is sampled as a uniform distribution from the entire wordle word list,Likelihood the wordle of the day is sampled as a uniform distribution from the entire wordle word list,,"I have been playing the wordle a lot recently. I noticed early on that misspellings on my part were 'accepted' as valid words, but that the answer was always a well known word. Recently I saw the list of all the 5 letter strings considered 'valid' by the wordle. Indeed, the majority of these are unknown to me. However, over the last 20 days only one wordle word was unknown to me. I am wondering what the likelihood is that the 'wordle of the day' set is equal to the 'accepted words' list? Is the wordle of the day generated from a uniform sampling of the accepted words list or a subset? For example, these are the first 10 wordle words in the wordle dictionary: ""aahed"",""aalii"",""aargh"",""aarti"",""abaca"",""abaci"",""abacs"",""abaft"",""abaka"",""abamp"" There are two sets, A and W . A contains all valid 5 letter words accepted by wordle. W contains the set of words chosen to be wordle of the day. We know W is a subset of A . I either know a word or I don't know it. K + U = 1 For the sake of argument, let's say that I recognize 10% of the words from set A . Thus Pr(K | A) = 0.1 . Of the last 20 wordles of the day, I was familiar with all but one. Thus Pr(K | W) = 0.95 with an N of 20. My question is - what is the likelihood W = A ?","I have been playing the wordle a lot recently. I noticed early on that misspellings on my part were 'accepted' as valid words, but that the answer was always a well known word. Recently I saw the list of all the 5 letter strings considered 'valid' by the wordle. Indeed, the majority of these are unknown to me. However, over the last 20 days only one wordle word was unknown to me. I am wondering what the likelihood is that the 'wordle of the day' set is equal to the 'accepted words' list? Is the wordle of the day generated from a uniform sampling of the accepted words list or a subset? For example, these are the first 10 wordle words in the wordle dictionary: ""aahed"",""aalii"",""aargh"",""aarti"",""abaca"",""abaci"",""abacs"",""abaft"",""abaka"",""abamp"" There are two sets, A and W . A contains all valid 5 letter words accepted by wordle. W contains the set of words chosen to be wordle of the day. We know W is a subset of A . I either know a word or I don't know it. K + U = 1 For the sake of argument, let's say that I recognize 10% of the words from set A . Thus Pr(K | A) = 0.1 . Of the last 20 wordles of the day, I was familiar with all but one. Thus Pr(K | W) = 0.95 with an N of 20. My question is - what is the likelihood W = A ?",,"['probability', 'statistics']"
49,Joint distribution of diagonal of Wishart matrix,Joint distribution of diagonal of Wishart matrix,,"If $S\sim \mathcal{W}_p(\Sigma, n)$ , what is the distribution of the diagonal part of $S$ , $\text{diag}(S)$ ? Principal submatrices of Wishart distributed matrices are also Wishart distributed, implying that each diagonal element is marginally gamma distributed. But what is the joint distribution of these diagonal terms? A refresher on the Wishart distribution: Let $X_i \sim \mathcal{N}_p(0, \Sigma)$ be a vector independently drawn from p-variate normal distribution with mean $0$ and covariance $\Sigma$ for $i\in\{1,\ldots,p\}$ . Then $$ S \equiv \sum_{i=1}^{n}X_iX_i^T \sim \mathcal{W}_p(\Sigma, n)\,. $$ I have been able to determine the joint moment generating function (MFG) of diag( $\Sigma$ ), and I will include the derivation here. I am a bit stuck at this point however, so feel free to skip to the bottom or ignore this work entirely if you think there is a better approach . The meat of my work so far: Let's denote the diagonal elements of $S$ as $\sigma \equiv \text{diag}(\Sigma)$ , with $\sigma_i = S_{ii}$ . Then we have that $$ \sigma_i = \left(\sum_{j=1}^{n}X_jX_j^T\right)_{ii} = \sum_{j=1}^{n}(X_j)_i^2\,. $$ In vector notation, we have $$ \sigma = \sum_{i=1}^{n} X_i\odot X_i\,, $$ where $\odot$ is the Hadamard (elementwise) product. Let's consider the MGF of $\sigma$ : $$ \phi_\sigma^n(t) \equiv \left\langle e^{t^T\sigma} \right\rangle_{p(\sigma)} = \left\langle e^{t^T\left(\sum_{i=1}^nX_i\odot X_i\right)} \right\rangle_{p(X_1, \ldots, X_n)} = \left\langle e^{t^T\left(X\odot X\right)} \right\rangle_{p(X)}^n\,, $$ where the first equality is given by the law of the unconscious statistician and the second is given by the independence of $X_i$ and $X_j$ for $i \neq j$ . Considering just $$ \phi(t) \equiv \phi_\sigma^1(t) = \left\langle e^{t^T\left(X\odot X\right)} \right\rangle_{p(X)}\,, $$ we have $$ \begin{align} \phi(t) &= \int_{-\infty}^{\infty} dX \ p(X) e^{t^T\left(X\odot X\right)} \\ &= \int_{-\infty}^{\infty} dX \ \frac{1}{\sqrt{2\pi}} |\Sigma|^{-1/2} \exp\left(-\frac{1}{2} X^T\Sigma^{-1}X\right) \exp\left(t^T\left(X\odot X\right)\right)\,. \end{align} $$ If we let $D \equiv \begin{bmatrix}t_1 & &0\\ &\ddots& \\ 0&&t_p \end{bmatrix}$ , then we can write $t^T\left(X\odot X\right) = X^T D X$ : $$ \begin{align} \phi(t) &= \int_{-\infty}^{\infty} dX \ \frac{1}{\sqrt{2\pi}} |\Sigma|^{-1/2} \exp\left(-\frac{1}{2} X^T\Sigma^{-1}X\right) \exp\left(X^T D X\right) \\ &= \int_{-\infty}^{\infty} dX \ \frac{1}{\sqrt{2\pi}} |\Sigma|^{-1/2} \exp\left(-\frac{1}{2} X^T(\Sigma^{-1} - 2D)X\right)\,. \end{align} $$ If $\Sigma$ is positive definite, we have a finite radius $\epsilon > 0$ such that $\Sigma^{-1} - 2D$ is positive definite for $|t_i| < \epsilon \ \forall i$ , and we can find a full-rank $p\times p$ matrix $B$ such that $B^TB = \Sigma^{-1} - 2D$ in this radius. Let's change variables, letting $Y \equiv BX$ . Including the Jacobian, we have $$ \begin{align} \phi(t) &= \int_{-\infty}^{\infty} dX \ \frac{1}{\sqrt{2\pi}} |\Sigma|^{-1/2} \exp\left(-\frac{1}{2} X^T(\Sigma^{-1} - 2D)X\right)  \\ &= |\Sigma|^{-1/2} |B|^{-1} \int_{-\infty}^{\infty} dY \ \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} Y^TY\right) \\ &= |\Sigma|^{-1/2} |B|^{-1} \\ &= |\Sigma|^{-1/2} |\Sigma^{-1} - 2D|^{-1/2} \\ &= |I_p - 2\Sigma D|^{-1/2}\,. \end{align} $$ Finally, we have ( TLDR ): $$ \phi_\sigma^n(t) = |I_p - 2\Sigma D|^{-\frac{n}{2}}\,. $$ This feels like a neat little result in of itself. I am fairly confident that this is correct, since it reduces to the MGF of the gamma distribution for $p=1$ , and it nicely reduces to the product of several gamma MFGs if $\Sigma$ is diagonal. However, I don't really know where to go from here. I can't really compute the inverse Laplace transform by inspection here, and I quickly get very lost when trying to do it explicitly. Is there a known result anybody can point me to? Is there a nice method of retrieving a joint density for $\sigma$ from this expression? Thank you very much for the help!","If , what is the distribution of the diagonal part of , ? Principal submatrices of Wishart distributed matrices are also Wishart distributed, implying that each diagonal element is marginally gamma distributed. But what is the joint distribution of these diagonal terms? A refresher on the Wishart distribution: Let be a vector independently drawn from p-variate normal distribution with mean and covariance for . Then I have been able to determine the joint moment generating function (MFG) of diag( ), and I will include the derivation here. I am a bit stuck at this point however, so feel free to skip to the bottom or ignore this work entirely if you think there is a better approach . The meat of my work so far: Let's denote the diagonal elements of as , with . Then we have that In vector notation, we have where is the Hadamard (elementwise) product. Let's consider the MGF of : where the first equality is given by the law of the unconscious statistician and the second is given by the independence of and for . Considering just we have If we let , then we can write : If is positive definite, we have a finite radius such that is positive definite for , and we can find a full-rank matrix such that in this radius. Let's change variables, letting . Including the Jacobian, we have Finally, we have ( TLDR ): This feels like a neat little result in of itself. I am fairly confident that this is correct, since it reduces to the MGF of the gamma distribution for , and it nicely reduces to the product of several gamma MFGs if is diagonal. However, I don't really know where to go from here. I can't really compute the inverse Laplace transform by inspection here, and I quickly get very lost when trying to do it explicitly. Is there a known result anybody can point me to? Is there a nice method of retrieving a joint density for from this expression? Thank you very much for the help!","S\sim \mathcal{W}_p(\Sigma, n) S \text{diag}(S) X_i \sim \mathcal{N}_p(0, \Sigma) 0 \Sigma i\in\{1,\ldots,p\} 
S \equiv \sum_{i=1}^{n}X_iX_i^T \sim \mathcal{W}_p(\Sigma, n)\,.
 \Sigma S \sigma \equiv \text{diag}(\Sigma) \sigma_i = S_{ii} 
\sigma_i = \left(\sum_{j=1}^{n}X_jX_j^T\right)_{ii} = \sum_{j=1}^{n}(X_j)_i^2\,.
 
\sigma = \sum_{i=1}^{n} X_i\odot X_i\,,
 \odot \sigma 
\phi_\sigma^n(t) \equiv \left\langle e^{t^T\sigma} \right\rangle_{p(\sigma)}
= \left\langle e^{t^T\left(\sum_{i=1}^nX_i\odot X_i\right)} \right\rangle_{p(X_1, \ldots, X_n)}
= \left\langle e^{t^T\left(X\odot X\right)} \right\rangle_{p(X)}^n\,,
 X_i X_j i \neq j 
\phi(t) \equiv \phi_\sigma^1(t) = \left\langle e^{t^T\left(X\odot X\right)} \right\rangle_{p(X)}\,,
 
\begin{align}
\phi(t) &= \int_{-\infty}^{\infty} dX \ p(X) e^{t^T\left(X\odot X\right)}
\\
&= \int_{-\infty}^{\infty} dX \ \frac{1}{\sqrt{2\pi}} |\Sigma|^{-1/2} \exp\left(-\frac{1}{2} X^T\Sigma^{-1}X\right) \exp\left(t^T\left(X\odot X\right)\right)\,.
\end{align}
 D \equiv \begin{bmatrix}t_1 & &0\\ &\ddots& \\ 0&&t_p \end{bmatrix} t^T\left(X\odot X\right) = X^T D X 
\begin{align}
\phi(t) &= \int_{-\infty}^{\infty} dX \ \frac{1}{\sqrt{2\pi}} |\Sigma|^{-1/2} \exp\left(-\frac{1}{2} X^T\Sigma^{-1}X\right) \exp\left(X^T D X\right)
\\
&= \int_{-\infty}^{\infty} dX \ \frac{1}{\sqrt{2\pi}} |\Sigma|^{-1/2} \exp\left(-\frac{1}{2} X^T(\Sigma^{-1} - 2D)X\right)\,.
\end{align}
 \Sigma \epsilon > 0 \Sigma^{-1} - 2D |t_i| < \epsilon \ \forall i p\times p B B^TB = \Sigma^{-1} - 2D Y \equiv BX 
\begin{align}
\phi(t) &= \int_{-\infty}^{\infty} dX \ \frac{1}{\sqrt{2\pi}} |\Sigma|^{-1/2} \exp\left(-\frac{1}{2} X^T(\Sigma^{-1} - 2D)X\right) 
\\
&= |\Sigma|^{-1/2} |B|^{-1} \int_{-\infty}^{\infty} dY \ \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{1}{2} Y^TY\right)
\\
&= |\Sigma|^{-1/2} |B|^{-1}
\\
&= |\Sigma|^{-1/2} |\Sigma^{-1} - 2D|^{-1/2}
\\
&= |I_p - 2\Sigma D|^{-1/2}\,.
\end{align}
 
\phi_\sigma^n(t) = |I_p - 2\Sigma D|^{-\frac{n}{2}}\,.
 p=1 \Sigma \sigma","['probability', 'probability-theory', 'probability-distributions', 'moment-generating-functions', 'random-matrices']"
50,The asymptotic formula on Poisson Branching Model,The asymptotic formula on Poisson Branching Model,,"Consider the Poisson branching model with mean $c = 1$ and root Eve. For $n ≥ 3$ , let $A_n$ be the event where Eve has precisely two children, Dana and Fan, and that the total tree size $T = n$ . Let X be the size of the subtree with root Dana. For each $j \geq 1$ , find $\lim_{n \to \infty} \mathbb{P}(X=j|A_n)$ . Find the asymptotic formula for $\mathbb{P}(\frac{n}{3}<X<\frac{2n}{3})$ . $\mathbf{Hint:}$ Set $x=n\alpha$ and $y=n-1-x \sim n\beta$ with $\alpha+\beta = 1$ . Use the asymptotic equation $\mathbb{P}(T=k) = \frac{e^{-k}k^{k-1}}{k!} \sim \frac{1}{\sqrt{2\pi}}k^{-\frac{3}{2}}$ to estimate $\mathbb{P}(X=x)\mathbb{P}(Y=y)$ . The denominator $\mathbb{P}(X+Y = n-1)$ is dominated by $min(X,Y)$ being small. Perhaps surprisingly, the conditional distribution of $X$ is highly skewed to the corners. $\mathbf{Idea:}$ That's what I managed to do. $$\mathbb{P}(X=j|A_n)= \frac{\mathbb{P}(X=j \cap A_n)}{\mathbb{P}(A_n)}=\frac{\mathbb{P}(X=j \cap Z_1=2 \cap T=n)}{\mathbb{P}(Z_1=2 \cap T=n)}$$ Then $$\mathbb{P}(A_n)= \mathbb{P}(Z_1=2 \cap T=n)= \mathbb{P}(T=n|Z_1=2)\mathbb{P}(Z_1=2)$$ $$\mathbb{P}(X=j \cap Z_1=2 \cap T=n)= \mathbb{P}(X=j \cap T=n|Z_1=2)\mathbb{P}(Z_1=2)$$ We denote by $Y$ the size of the subtree with root Fan, then $$\mathbb{P}(X=j \cap T=n|Z_1=2) = \mathbb{P}(X=j \cap Y=n-1-j|Z_1=2)= \\  \mathbb{P}(X=j \cap Y=n-1-j)=\mathbb{P}(X=j)\mathbb{P}(Y=n-1-j).$$ As the hint suggest we use the asymptotic equation to estimate $\mathbb{P}(X=j)\mathbb{P}(Y=n-1-j)$ . Then $$\mathbb{P}(X=j) \sim \frac{1}{\sqrt{2\pi}}j^{-\frac{3}{2}} = \frac{1}{\sqrt{2\pi}}(n\alpha)^{-\frac{3}{2}}$$ and $$\mathbb{P}(Y=n-1-j) \sim \frac{1}{\sqrt{2\pi}}(n-1-j)^{-\frac{3}{2}} = \frac{1}{\sqrt{2\pi}}(n\beta)^{-\frac{3}{2}}$$ Combining them both we get: $$\mathbb{P}(X=j)\mathbb{P}(Y=n-1-j) \sim \frac{1}{\sqrt{2\pi}}(n\alpha)^{-\frac{3}{2}}(n\beta)^{-\frac{3}{2}}=\frac{1}{\sqrt{2\pi}}n^{-3}(\alpha\beta)^{-\frac{3}{2}}$$ Finally we have that $$\mathbb{P}(A_n) =\mathbb{P}(T=n|Z_1=2)\mathbb{P}(Z_1=2)= \mathbb{P}(X+Y=n-1)\mathbb{P}(Z_1=2)$$ And we can approximate $\mathbb{P}(X+Y=n-1) \leq min(X,Y)$ . Then we can conclude that $$\mathbb{P}(X=j|A_n)=\frac{\mathbb{P}(X=j \cap T=n|Z_1=2)}{\mathbb{P}(T=n|Z_1=2)} \geq \frac{\mathbb{P}(X=j)\mathbb{P}(Y=n-1-j)}{min(X,Y)} \sim \frac{\frac{1}{\sqrt{2\pi}}n^{-3}(\alpha\beta)^{-\frac{3}{2}}}{min(X,Y)}$$ $\mathbf{Questions}$ I'm confused about this $min(X,Y)$ . I don't understand if I can conclude like this the first part of the exercise. And I actually have no idea how to answer the second part (Find the asymptotic formula for $\mathbb{P}(\frac{n}{3}<X<\frac{2n}{3})$ ). Thank you all for the help!","Consider the Poisson branching model with mean and root Eve. For , let be the event where Eve has precisely two children, Dana and Fan, and that the total tree size . Let X be the size of the subtree with root Dana. For each , find . Find the asymptotic formula for . Set and with . Use the asymptotic equation to estimate . The denominator is dominated by being small. Perhaps surprisingly, the conditional distribution of is highly skewed to the corners. That's what I managed to do. Then We denote by the size of the subtree with root Fan, then As the hint suggest we use the asymptotic equation to estimate . Then and Combining them both we get: Finally we have that And we can approximate . Then we can conclude that I'm confused about this . I don't understand if I can conclude like this the first part of the exercise. And I actually have no idea how to answer the second part (Find the asymptotic formula for ). Thank you all for the help!","c = 1 n ≥ 3 A_n T = n j \geq 1 \lim_{n \to \infty} \mathbb{P}(X=j|A_n) \mathbb{P}(\frac{n}{3}<X<\frac{2n}{3}) \mathbf{Hint:} x=n\alpha y=n-1-x \sim n\beta \alpha+\beta = 1 \mathbb{P}(T=k) = \frac{e^{-k}k^{k-1}}{k!} \sim \frac{1}{\sqrt{2\pi}}k^{-\frac{3}{2}} \mathbb{P}(X=x)\mathbb{P}(Y=y) \mathbb{P}(X+Y = n-1) min(X,Y) X \mathbf{Idea:} \mathbb{P}(X=j|A_n)= \frac{\mathbb{P}(X=j \cap A_n)}{\mathbb{P}(A_n)}=\frac{\mathbb{P}(X=j \cap Z_1=2 \cap T=n)}{\mathbb{P}(Z_1=2 \cap T=n)} \mathbb{P}(A_n)= \mathbb{P}(Z_1=2 \cap T=n)= \mathbb{P}(T=n|Z_1=2)\mathbb{P}(Z_1=2) \mathbb{P}(X=j \cap Z_1=2 \cap T=n)= \mathbb{P}(X=j \cap T=n|Z_1=2)\mathbb{P}(Z_1=2) Y \mathbb{P}(X=j \cap T=n|Z_1=2) = \mathbb{P}(X=j \cap Y=n-1-j|Z_1=2)= \\ 
\mathbb{P}(X=j \cap Y=n-1-j)=\mathbb{P}(X=j)\mathbb{P}(Y=n-1-j). \mathbb{P}(X=j)\mathbb{P}(Y=n-1-j) \mathbb{P}(X=j) \sim \frac{1}{\sqrt{2\pi}}j^{-\frac{3}{2}} = \frac{1}{\sqrt{2\pi}}(n\alpha)^{-\frac{3}{2}} \mathbb{P}(Y=n-1-j) \sim \frac{1}{\sqrt{2\pi}}(n-1-j)^{-\frac{3}{2}} = \frac{1}{\sqrt{2\pi}}(n\beta)^{-\frac{3}{2}} \mathbb{P}(X=j)\mathbb{P}(Y=n-1-j) \sim \frac{1}{\sqrt{2\pi}}(n\alpha)^{-\frac{3}{2}}(n\beta)^{-\frac{3}{2}}=\frac{1}{\sqrt{2\pi}}n^{-3}(\alpha\beta)^{-\frac{3}{2}} \mathbb{P}(A_n) =\mathbb{P}(T=n|Z_1=2)\mathbb{P}(Z_1=2)= \mathbb{P}(X+Y=n-1)\mathbb{P}(Z_1=2) \mathbb{P}(X+Y=n-1) \leq min(X,Y) \mathbb{P}(X=j|A_n)=\frac{\mathbb{P}(X=j \cap T=n|Z_1=2)}{\mathbb{P}(T=n|Z_1=2)} \geq \frac{\mathbb{P}(X=j)\mathbb{P}(Y=n-1-j)}{min(X,Y)} \sim \frac{\frac{1}{\sqrt{2\pi}}n^{-3}(\alpha\beta)^{-\frac{3}{2}}}{min(X,Y)} \mathbf{Questions} min(X,Y) \mathbb{P}(\frac{n}{3}<X<\frac{2n}{3})","['probability', 'asymptotics', 'poisson-process', 'random-graphs']"
51,Verification of Proof of Law of Total Covariance,Verification of Proof of Law of Total Covariance,,"If $X,Y,Z$ are 3 random variables, then prove that the following holds: $$\mathrm{Cov}(X,Y)=\mathbb{E}(\mathrm{Cov}(X,Y|Z))+\mathrm{Cov}(\mathbb{E}(X|Z),\mathbb{E}(Y|Z))$$ Would the following be a good proof? I know that $\mathrm{Cov}(X,Y|Z)=\mathbb{E}(XY|Z)-\mathbb{E}(X|Z)\mathbb{E}(Y|Z)$ . So taking expectations over $Z$ , and using linearity, yields, $$\mathbb{E}(\mathrm{Cov}(X,Y|Z))=\mathbb{E}[\mathbb{E}(XY|Z)]-\mathbb{E}[\mathbb{E}(X|Z)\mathbb{E}(Y|Z)].$$ Also, using the definition of covariance gives that $$\mathrm{Cov}(\mathbb{E}(X|Z),\mathbb{E}(Y|Z))=\mathbb{E}[\mathbb{E}(X|Z)\mathbb{E}(Y|Z)]-\mathbb{E}[\mathbb{E}(X|Z)]\mathbb{E}[\mathbb{E}(Y|Z)].$$ So, adding the two equations gives $$\mathbb{E}(\mathrm{Cov}(X,Y|Z))+\mathrm{Cov}(\mathbb{E}(X|Z),\mathbb{E}(Y|Z))=\mathbb{E}[\mathbb{E}(XY|Z)]-\mathbb{E}[\mathbb{E}(X|Z)]\mathbb{E}[\mathbb{E}(Y|Z)].$$ Now by the Tower law, the RHS is $\mathrm{Cov}(X,Y)=\mathbb{E}(XY)-\mathbb{E}(X)\mathbb{E}(Y)$ . Thus, we have that $$\mathrm{Cov}(X,Y)=\mathbb{E}(\mathrm{Cov}(X,Y|Z))+\mathrm{Cov}(\mathbb{E}(X|Z),\mathbb{E}(Y|Z)),$$ as required.","If are 3 random variables, then prove that the following holds: Would the following be a good proof? I know that . So taking expectations over , and using linearity, yields, Also, using the definition of covariance gives that So, adding the two equations gives Now by the Tower law, the RHS is . Thus, we have that as required.","X,Y,Z \mathrm{Cov}(X,Y)=\mathbb{E}(\mathrm{Cov}(X,Y|Z))+\mathrm{Cov}(\mathbb{E}(X|Z),\mathbb{E}(Y|Z)) \mathrm{Cov}(X,Y|Z)=\mathbb{E}(XY|Z)-\mathbb{E}(X|Z)\mathbb{E}(Y|Z) Z \mathbb{E}(\mathrm{Cov}(X,Y|Z))=\mathbb{E}[\mathbb{E}(XY|Z)]-\mathbb{E}[\mathbb{E}(X|Z)\mathbb{E}(Y|Z)]. \mathrm{Cov}(\mathbb{E}(X|Z),\mathbb{E}(Y|Z))=\mathbb{E}[\mathbb{E}(X|Z)\mathbb{E}(Y|Z)]-\mathbb{E}[\mathbb{E}(X|Z)]\mathbb{E}[\mathbb{E}(Y|Z)]. \mathbb{E}(\mathrm{Cov}(X,Y|Z))+\mathrm{Cov}(\mathbb{E}(X|Z),\mathbb{E}(Y|Z))=\mathbb{E}[\mathbb{E}(XY|Z)]-\mathbb{E}[\mathbb{E}(X|Z)]\mathbb{E}[\mathbb{E}(Y|Z)]. \mathrm{Cov}(X,Y)=\mathbb{E}(XY)-\mathbb{E}(X)\mathbb{E}(Y) \mathrm{Cov}(X,Y)=\mathbb{E}(\mathrm{Cov}(X,Y|Z))+\mathrm{Cov}(\mathbb{E}(X|Z),\mathbb{E}(Y|Z)),","['probability', 'solution-verification', 'conditional-probability', 'covariance']"
52,Conditional expectations with special structure. Counterexample?,Conditional expectations with special structure. Counterexample?,,"We have: three random variables $X_1,X_2,X_3$ , three $\sigma$ -fields $\mathcal{G}_1, \mathcal{G}_2, \mathcal{G}_3$ , three random variables $Y_{1,2}$ , $Y_{2,3}$ , $Y_{3,1}$ , such that: $X_1=\mathbb{E}(Y_{1,2}|\mathcal{G_1})$ and $X_2=\mathbb{E}(Y_{1,2}|\mathcal{G_2})$ , $X_2=\mathbb{E}(Y_{2,3}|\mathcal{G_2})$ and $X_3=\mathbb{E}(Y_{2,3}|\mathcal{G_3})$ , $X_3=\mathbb{E}(Y_{3,1}|\mathcal{G_3})$ and $X_1=\mathbb{E}(Y_{3,1}|\mathcal{G_1})$ . (all the above hold a.s.) My question is: does this imply, that there exists a random variable $Z$ , such that $$X_i=\mathbb{E}(Z|\mathcal{G}_i),$$ for $i=1,2,3$ ? Let us  assume that there exists a random variable $U\sim \mathcal{U}(0,1)$ which is independent from all $X_i, Y_{i,j}$ . I have been thinking about this a bit, and I am rather convinced, that this is not true. Even though,  I do not have a good idea on how to construct a counterexample. I will be glad for any help.","We have: three random variables , three -fields , three random variables , , , such that: and , and , and . (all the above hold a.s.) My question is: does this imply, that there exists a random variable , such that for ? Let us  assume that there exists a random variable which is independent from all . I have been thinking about this a bit, and I am rather convinced, that this is not true. Even though,  I do not have a good idea on how to construct a counterexample. I will be glad for any help.","X_1,X_2,X_3 \sigma \mathcal{G}_1, \mathcal{G}_2, \mathcal{G}_3 Y_{1,2} Y_{2,3} Y_{3,1} X_1=\mathbb{E}(Y_{1,2}|\mathcal{G_1}) X_2=\mathbb{E}(Y_{1,2}|\mathcal{G_2}) X_2=\mathbb{E}(Y_{2,3}|\mathcal{G_2}) X_3=\mathbb{E}(Y_{2,3}|\mathcal{G_3}) X_3=\mathbb{E}(Y_{3,1}|\mathcal{G_3}) X_1=\mathbb{E}(Y_{3,1}|\mathcal{G_1}) Z X_i=\mathbb{E}(Z|\mathcal{G}_i), i=1,2,3 U\sim \mathcal{U}(0,1) X_i, Y_{i,j}","['probability', 'probability-theory']"
53,How to show that an upper bound is $\frac{2-b}{1-b}\mathbb E [Y^{b}]$ exists for the following integral,How to show that an upper bound is  exists for the following integral,\frac{2-b}{1-b}\mathbb E [Y^{b}],"Consider for $b \in (0,1)$ and two continuous positive random variables $X$ and $Y$ , that we have the following inequalities already given: $$1.\; \mathbb E [X^{b}] \leq \int\limits_{0}^{\infty}b x ^{b-1}\left(\mathbb P(X\geq  x, Y\leq x)+\mathbb P(Y>x)\right)dx\\ 2. \; \mathbb P(X\geq  x, Y\leq x)\leq \frac{1}{x}\mathbb E[Y\land x]$$ I want to show that: $$ \mathbb E [X^{b}] \leq \frac{2-b}{1-b}\mathbb E [Y^{b}]$$ My ideas so far: $$ \int\limits_{0}^{\infty}b x ^{b-1}\left(\mathbb P(X\geq  x, Y\leq x)+\mathbb P(Y>x)\right)dx\leq \int\limits_{0}^{\infty}b x ^{b-1}\left(\frac{1}{x}\mathbb E[Y\land x]+\mathbb P(Y>x)\right)dx\\=\int\limits_{0}^{\infty}b x ^{b-2}\mathbb E[Y\land x]dx+\int\limits_{0}^{\infty}b x ^{b-1}\mathbb P(Y>x)dx$$ Note that the second term can be written as $\int\limits_{0}^{\infty}b x ^{b-1}\mathbb P(Y>x)dx=\mathbb E[Y^{b}]$ But I am having trouble computing the first term $\int\limits_{0}^{\infty}b x ^{b-2}\mathbb E[Y\land x]dx$ . Any ideas?","Consider for and two continuous positive random variables and , that we have the following inequalities already given: I want to show that: My ideas so far: Note that the second term can be written as But I am having trouble computing the first term . Any ideas?","b \in (0,1) X Y 1.\; \mathbb E [X^{b}] \leq \int\limits_{0}^{\infty}b x ^{b-1}\left(\mathbb P(X\geq  x, Y\leq x)+\mathbb P(Y>x)\right)dx\\ 2. \; \mathbb P(X\geq  x, Y\leq x)\leq \frac{1}{x}\mathbb E[Y\land x]  \mathbb E [X^{b}] \leq \frac{2-b}{1-b}\mathbb E [Y^{b}]  \int\limits_{0}^{\infty}b x ^{b-1}\left(\mathbb P(X\geq  x, Y\leq x)+\mathbb P(Y>x)\right)dx\leq \int\limits_{0}^{\infty}b x ^{b-1}\left(\frac{1}{x}\mathbb E[Y\land x]+\mathbb P(Y>x)\right)dx\\=\int\limits_{0}^{\infty}b x ^{b-2}\mathbb E[Y\land x]dx+\int\limits_{0}^{\infty}b x ^{b-1}\mathbb P(Y>x)dx \int\limits_{0}^{\infty}b x ^{b-1}\mathbb P(Y>x)dx=\mathbb E[Y^{b}] \int\limits_{0}^{\infty}b x ^{b-2}\mathbb E[Y\land x]dx","['probability', 'inequality', 'expected-value']"
54,Expected number of iterations until all the cars can no longer move,Expected number of iterations until all the cars can no longer move,,"I am studying probability questions and I would like to revive this question for more attention. Assume we have an array of length 2n. The first n slots are cars. Each round, we flip n coins where each coin $X_i$ corresponds to car i. $X_i$ is a fair coin and if it is heads, we will move car i to the right if the right space is free. We are interested in the number of expected rounds until all the cars have moved to their final position at the end. The previous question states that this is asymptotically O(nlogn) but I am unable to reason why. Thank you!","I am studying probability questions and I would like to revive this question for more attention. Assume we have an array of length 2n. The first n slots are cars. Each round, we flip n coins where each coin corresponds to car i. is a fair coin and if it is heads, we will move car i to the right if the right space is free. We are interested in the number of expected rounds until all the cars have moved to their final position at the end. The previous question states that this is asymptotically O(nlogn) but I am unable to reason why. Thank you!",X_i X_i,"['probability', 'statistics', 'expected-value']"
55,entropy of skew product,entropy of skew product,,"Let $Y$ be a compact abelian group and let $m$ be the haar measure.  Suppose that $f:X \to Y$ is measurable and let $T:X \to X$ be an invertible measure preserving transformation.  Show that the entropy of $\tau(x,y)=(T(x),y+f(x))$ is equal to $h(T)$ . I know that for skew products $(T(x),S_xy)$ the entropy is equal to $h(T)+h_T(S)$ where $h_T(S)$ is the fiber entropy.  Here $S_xy=y+f(x)$ .  My initial thought is to estimate the sizes of the partitions $\beta_1^n(x)=S_{x}^{-1} \beta \vee S_{x}^{-1}S_{Tx}^{-1} \vee \dots \vee S_{x}^{-1} \dots S_{T^{n-1}x}^{-1} \beta$ and show that it must be bounded by $Kn$ .  Then $h_T(\beta,S)=\lim_{n \to \infty} \frac{1}{n}\int_X H( \beta_0^n(x)) \to 0$ .",Let be a compact abelian group and let be the haar measure.  Suppose that is measurable and let be an invertible measure preserving transformation.  Show that the entropy of is equal to . I know that for skew products the entropy is equal to where is the fiber entropy.  Here .  My initial thought is to estimate the sizes of the partitions and show that it must be bounded by .  Then .,"Y m f:X \to Y T:X \to X \tau(x,y)=(T(x),y+f(x)) h(T) (T(x),S_xy) h(T)+h_T(S) h_T(S) S_xy=y+f(x) \beta_1^n(x)=S_{x}^{-1} \beta \vee S_{x}^{-1}S_{Tx}^{-1} \vee \dots \vee S_{x}^{-1} \dots S_{T^{n-1}x}^{-1} \beta Kn h_T(\beta,S)=\lim_{n \to \infty} \frac{1}{n}\int_X H( \beta_0^n(x)) \to 0","['real-analysis', 'probability', 'measure-theory', 'dynamical-systems', 'ergodic-theory']"
56,Conditional expectation given an event and a $\sigma$-algebra,Conditional expectation given an event and a -algebra,\sigma,"Let $X$ be an integrable real random variable on the probability space $(\Omega,\mathcal A,P)$ . If $A\in \mathcal A$ is an event with probability $0<P[A]<1$ then we have that $$E[X|\sigma(A)]=1_A \frac{E[1_AX]}{P[A]}+1_{A^c} \frac{E[1_{A^c}X]}{P[A^c]} \quad P\text{-a.s.} $$ Now suppose $\mathcal F$ a sub- $\sigma$ -algebra of $\mathcal A$ . Am wondering if the following holds: $$E[X|\sigma(A,\mathcal F)]=1_A \frac{E[1_AX|\mathcal F]}{P[A|\mathcal F]}+1_{A^c} \frac{E[1_{A^c}X|\mathcal F]}{P[A^c|\mathcal F]} \quad P\text{-a.s.} \quad \quad (1)$$ assuming that $0<P[A|\mathcal F]<1$ $P$ -a.s. (I don't see how to dispense with this assumption). It seems to me that the answer is  yes based on the following argument: Let $Y$ denote the RHS of $(1)$ .Then $Y$ is $\sigma(A,\mathcal F)$ measurable. We check that $Y$ is integrable using the conditional Jensen's inequality,the law of total expectation, and the pull-out property: $$ E\Bigg[\Bigg|1_A \frac{E[1_AX|\mathcal F]}{P[A|\mathcal F]}\Bigg|\Bigg]\leq E\bigg[1_A \frac{E[1_A |X|\mid\mathcal F]}{P[A|\mathcal F]}\bigg]= E\bigg[E[1_A|\mathcal F] \frac{E[1_A |X|\mid\mathcal F]}{P[A|\mathcal F]}\bigg]=E[1_A|X|]< \infty$$ and similarly $E\Bigg[\Bigg|1_{A^c} \frac{E[1_{A^c}X|\mathcal F]}{P[A^c|\mathcal F]}\Bigg|\Bigg]<\infty$ . Now, we verify that $\sigma(A,\mathcal F)=\Big\{(A\cap B_1) \cup (A^c\cap B_2) : B_1,B_2\in\mathcal F \Big\}$ , and therefore by additivity it is is sufficient to check that $E[1_{A\cap B}Y]=E[1_{A\cap B}X]$ and $E[1_{A^c\cap B}Y]=E[1_{A^c\cap B}X]$ for all $B\in\mathcal F$ . This we verify by direct computation, using again the law of total expectation and the pull-out property: $$E[1_{A\cap B}Y]=E\bigg[1_A\frac{E[1_{A\cap B}X|\mathcal F]}{P[A|\mathcal F]}\bigg]=E\bigg[E[1_A|\mathcal F]\frac{E[1_{A\cap B}X|\mathcal F]}{P[A|\mathcal F]}\bigg]=E[1_{A\cap B}X]$$ and similarly $E[1_{A^c\cap B}Y]=E[1_{A^c\cap B}X]$ . Am I missing something? Is there a way to ensure that $0<P[A|\mathcal F]<1$ $P$ -a.s.? Thanks a lot for your help. Proposition. A necessary and sufficient condition for $0<P[A|\mathcal F]<1$ a.s. is that $P[A\cap B]>0$ and $P[A^c\cap B]>0$ for all $B\in \mathcal F$ with $P[B]>0$ . In other words, the partition $\{A,A^c\}$ of $\Omega$ must split each non-null set of $\mathcal F$ into two non-null parts. Necessity. Suppose $0<P[A|\mathcal F]<1$ a.s.. Let $B\in \mathcal F$ with $P[B]>0$ . Then $$P[A\cap B]=E[1_{A\cap B}]=E[P[A|\mathcal F] 1_B]>0$$ for otherwise $P[A|\mathcal F] 1_B=0$ a.s. and so $P[A|\mathcal F]=0$ on an $\mathcal F$ -measurable set of positive measure, contradiction. Similarly, $P[A^c\cap B]>0$ , for otherwise $P[A|\mathcal F]=1$ on an $\mathcal F$ -measurable set of positive measure. Sufficiency. Suppose $P[A\cap B]>0$ and $P[A^c\cap B]>0$ for all $B\in \mathcal F$ with $P[B]>0$ . Suppose first by contradiction that $P[A|\mathcal F]=0$ on a set $B\in \mathcal F$ with $P[B]>0$ . Note that $P[B]<1$ , for otherwise $P[A|\mathcal F]=0$ a.s. which implies $P[A]=0$ , contradiction. Then $$E[1_A|\mathcal F]=1_BE[1_A|\mathcal F]=E[1_{A\cap B}|\mathcal F]$$ and so $E[1_{A\cap B^c}|\mathcal F]=E[1_A|\mathcal F]-E[1_{A\cap B}|\mathcal F]=0$ a.s.. This implies $P[A\cap B^c]=0$ , contrary to hypothesis. Next suppose by contradiction that $P[A|\mathcal F]=1$ on a set $B\in \mathcal F$ with $P[B]>0$ . Note that $P[B]<1$ , for otherwise $P[A|\mathcal F]=1$ a.s. which implies $P[A]=1$ , contradiction. Then $E[1_A|\mathcal F]\geq 1_B$ a.s. and so $$E[1_{A\cap B}|\mathcal F]=1_B E[1_A|\mathcal F]\geq 1_B=E[1_B|\mathcal F]$$ which implies $0\geq E[1_{A\cap B}-1_B|\mathcal F]\geq 0$ or $E[1_{A^c\cap B}|\mathcal F]=0$ a.s.. Hence $P[A^c\cap B]=0$ , contrary to hypothesis.","Let be an integrable real random variable on the probability space . If is an event with probability then we have that Now suppose a sub- -algebra of . Am wondering if the following holds: assuming that -a.s. (I don't see how to dispense with this assumption). It seems to me that the answer is  yes based on the following argument: Let denote the RHS of .Then is measurable. We check that is integrable using the conditional Jensen's inequality,the law of total expectation, and the pull-out property: and similarly . Now, we verify that , and therefore by additivity it is is sufficient to check that and for all . This we verify by direct computation, using again the law of total expectation and the pull-out property: and similarly . Am I missing something? Is there a way to ensure that -a.s.? Thanks a lot for your help. Proposition. A necessary and sufficient condition for a.s. is that and for all with . In other words, the partition of must split each non-null set of into two non-null parts. Necessity. Suppose a.s.. Let with . Then for otherwise a.s. and so on an -measurable set of positive measure, contradiction. Similarly, , for otherwise on an -measurable set of positive measure. Sufficiency. Suppose and for all with . Suppose first by contradiction that on a set with . Note that , for otherwise a.s. which implies , contradiction. Then and so a.s.. This implies , contrary to hypothesis. Next suppose by contradiction that on a set with . Note that , for otherwise a.s. which implies , contradiction. Then a.s. and so which implies or a.s.. Hence , contrary to hypothesis.","X (\Omega,\mathcal A,P) A\in \mathcal A 0<P[A]<1 E[X|\sigma(A)]=1_A \frac{E[1_AX]}{P[A]}+1_{A^c} \frac{E[1_{A^c}X]}{P[A^c]} \quad P\text{-a.s.}  \mathcal F \sigma \mathcal A E[X|\sigma(A,\mathcal F)]=1_A \frac{E[1_AX|\mathcal F]}{P[A|\mathcal F]}+1_{A^c} \frac{E[1_{A^c}X|\mathcal F]}{P[A^c|\mathcal F]} \quad P\text{-a.s.} \quad \quad (1) 0<P[A|\mathcal F]<1 P Y (1) Y \sigma(A,\mathcal F) Y  E\Bigg[\Bigg|1_A \frac{E[1_AX|\mathcal F]}{P[A|\mathcal F]}\Bigg|\Bigg]\leq E\bigg[1_A \frac{E[1_A |X|\mid\mathcal F]}{P[A|\mathcal F]}\bigg]= E\bigg[E[1_A|\mathcal F] \frac{E[1_A |X|\mid\mathcal F]}{P[A|\mathcal F]}\bigg]=E[1_A|X|]< \infty E\Bigg[\Bigg|1_{A^c} \frac{E[1_{A^c}X|\mathcal F]}{P[A^c|\mathcal F]}\Bigg|\Bigg]<\infty \sigma(A,\mathcal F)=\Big\{(A\cap B_1) \cup (A^c\cap B_2) : B_1,B_2\in\mathcal F \Big\} E[1_{A\cap B}Y]=E[1_{A\cap B}X] E[1_{A^c\cap B}Y]=E[1_{A^c\cap B}X] B\in\mathcal F E[1_{A\cap B}Y]=E\bigg[1_A\frac{E[1_{A\cap B}X|\mathcal F]}{P[A|\mathcal F]}\bigg]=E\bigg[E[1_A|\mathcal F]\frac{E[1_{A\cap B}X|\mathcal F]}{P[A|\mathcal F]}\bigg]=E[1_{A\cap B}X] E[1_{A^c\cap B}Y]=E[1_{A^c\cap B}X] 0<P[A|\mathcal F]<1 P 0<P[A|\mathcal F]<1 P[A\cap B]>0 P[A^c\cap B]>0 B\in \mathcal F P[B]>0 \{A,A^c\} \Omega \mathcal F 0<P[A|\mathcal F]<1 B\in \mathcal F P[B]>0 P[A\cap B]=E[1_{A\cap B}]=E[P[A|\mathcal F] 1_B]>0 P[A|\mathcal F] 1_B=0 P[A|\mathcal F]=0 \mathcal F P[A^c\cap B]>0 P[A|\mathcal F]=1 \mathcal F P[A\cap B]>0 P[A^c\cap B]>0 B\in \mathcal F P[B]>0 P[A|\mathcal F]=0 B\in \mathcal F P[B]>0 P[B]<1 P[A|\mathcal F]=0 P[A]=0 E[1_A|\mathcal F]=1_BE[1_A|\mathcal F]=E[1_{A\cap B}|\mathcal F] E[1_{A\cap B^c}|\mathcal F]=E[1_A|\mathcal F]-E[1_{A\cap B}|\mathcal F]=0 P[A\cap B^c]=0 P[A|\mathcal F]=1 B\in \mathcal F P[B]>0 P[B]<1 P[A|\mathcal F]=1 P[A]=1 E[1_A|\mathcal F]\geq 1_B E[1_{A\cap B}|\mathcal F]=1_B E[1_A|\mathcal F]\geq 1_B=E[1_B|\mathcal F] 0\geq E[1_{A\cap B}-1_B|\mathcal F]\geq 0 E[1_{A^c\cap B}|\mathcal F]=0 P[A^c\cap B]=0","['probability', 'probability-theory', 'measure-theory', 'solution-verification', 'conditional-expectation']"
57,How to generate Gaussian random variables with given covariance and autocorrelation structure,How to generate Gaussian random variables with given covariance and autocorrelation structure,,"Given a pos. def. covariance matrix $\Sigma\in \mathbb{R}^{d\times d}$ and first order autocorrelations $a\in (-1,1)^d$ for each dimension, I would like to generate Gaussian random variables in time $X_t \sim N(0,\Sigma)$ such that in each dimension $i$ the lag-1-autocorrelation $a_i$ is met. Is there a simple characterization which combinations of $\Sigma$ and $a$ are valid? How could I generate such a process efficiently? I'd prefer a solution using the following VAR(1) model: $$ X_t = A \cdot X_{t-1} + \varepsilon_t, $$ where $\varepsilon_t \sim N(0,\Sigma_\varepsilon)$ iid and $A= \mathrm{diag}(a)$ . Then by $X_t = \sum_{k=0}^\infty A^k \varepsilon_{t-k}$ , it would hold that $$X_t\sim N\left(0, \sum_{k=0}^\infty A^k \Sigma_\varepsilon A^k \right)= N\left(0,\Bigl(\frac{(\Sigma_{\varepsilon})_{ij}}{1-a_i a_j} \Bigr)_{ij}\right).~$$ Now if the next step is well-defined, it holds that $(\Sigma_\varepsilon)_{ij} = \Sigma_{ij} (1-a_i a_j)$ . Unfortunately $\Sigma$ and $a$ may be incompatible such that the resulting $\Sigma_\varepsilon$ is not pos. semidefinite. So can anyone characterize the conditions on $\Sigma$ and $A$ (or better on $\Sigma$ and $a$ ) such that there exists a positive definite covariance matrix $\Sigma_\varepsilon$ that allows me to generate the corresponding VAR-process? For example if $A$ and $\Sigma$ share the same eigenvectors then there exists a pos. semidefinite matrix $\Sigma_\varepsilon$ for the VAR(1)-process above. But my particular setting is the covariance matrix of $d$ points on the sphere $S^2$ of an isotropic random field, hence the eigenvector basis will not be the standard basis and $A$ not diagonal (as preferred). Another idea, instead of VAR(1), is to use a spatio-temporal covariance function for $d$ grid points and time such that the autocorrelation depends on the location. Which combinations of $\Sigma$ and $a$ are allowed now and how do I find a plausible covariance function?","Given a pos. def. covariance matrix and first order autocorrelations for each dimension, I would like to generate Gaussian random variables in time such that in each dimension the lag-1-autocorrelation is met. Is there a simple characterization which combinations of and are valid? How could I generate such a process efficiently? I'd prefer a solution using the following VAR(1) model: where iid and . Then by , it would hold that Now if the next step is well-defined, it holds that . Unfortunately and may be incompatible such that the resulting is not pos. semidefinite. So can anyone characterize the conditions on and (or better on and ) such that there exists a positive definite covariance matrix that allows me to generate the corresponding VAR-process? For example if and share the same eigenvectors then there exists a pos. semidefinite matrix for the VAR(1)-process above. But my particular setting is the covariance matrix of points on the sphere of an isotropic random field, hence the eigenvector basis will not be the standard basis and not diagonal (as preferred). Another idea, instead of VAR(1), is to use a spatio-temporal covariance function for grid points and time such that the autocorrelation depends on the location. Which combinations of and are allowed now and how do I find a plausible covariance function?","\Sigma\in \mathbb{R}^{d\times d} a\in (-1,1)^d X_t \sim N(0,\Sigma) i a_i \Sigma a  X_t = A \cdot X_{t-1} + \varepsilon_t,  \varepsilon_t \sim N(0,\Sigma_\varepsilon) A= \mathrm{diag}(a) X_t = \sum_{k=0}^\infty A^k \varepsilon_{t-k} X_t\sim N\left(0, \sum_{k=0}^\infty A^k \Sigma_\varepsilon A^k \right)= N\left(0,\Bigl(\frac{(\Sigma_{\varepsilon})_{ij}}{1-a_i a_j} \Bigr)_{ij}\right).~ (\Sigma_\varepsilon)_{ij} = \Sigma_{ij} (1-a_i a_j) \Sigma a \Sigma_\varepsilon \Sigma A \Sigma a \Sigma_\varepsilon A \Sigma \Sigma_\varepsilon d S^2 A d \Sigma a","['linear-algebra', 'probability', 'statistics', 'probability-distributions', 'time-series']"
58,Random area covered by disks: what do I need to prove normal covergence?,Random area covered by disks: what do I need to prove normal covergence?,,"Consider a square $\mathcal{L}$ of side $L$ , a Poisson point process $\mathcal{P} \subset \mathcal{L}$ of expected $n$ points, and unit disks with their centres at the points of $\mathcal{P}$ . The disks can overlap with the boundary $\partial \mathcal{L}$ . Let the total area of the square $\mathcal{L}$ covered by the union of the disks be the random variable $\mathcal{A}$ . In the `dense' limit where $n \to \infty$ at fixed $L$ , consider the random uncovered area $L^2 - A$ . Does this converge somehow to a Gaussian distribution? The area will be small, but random, and so a sort of central limit theorem may well apply. I imagine I could bound the deviation of $f_{L^2-A}(x)$ from that of the corresponding Gaussian distribution with the same mean and variance as $L^2 - A$ . What would I need to prove this? Just bounds on some of the moments of $L^2 - A$ ? Below is an image of the square. The area $L^2 - A$ is shown in white.","Consider a square of side , a Poisson point process of expected points, and unit disks with their centres at the points of . The disks can overlap with the boundary . Let the total area of the square covered by the union of the disks be the random variable . In the `dense' limit where at fixed , consider the random uncovered area . Does this converge somehow to a Gaussian distribution? The area will be small, but random, and so a sort of central limit theorem may well apply. I imagine I could bound the deviation of from that of the corresponding Gaussian distribution with the same mean and variance as . What would I need to prove this? Just bounds on some of the moments of ? Below is an image of the square. The area is shown in white.",\mathcal{L} L \mathcal{P} \subset \mathcal{L} n \mathcal{P} \partial \mathcal{L} \mathcal{L} \mathcal{A} n \to \infty L L^2 - A f_{L^2-A}(x) L^2 - A L^2 - A L^2 - A,"['probability', 'geometry', 'normal-distribution']"
59,Probability of draw with random play on $N\times N$ Tic Tac Toe,Probability of draw with random play on  Tic Tac Toe,N\times N,"I was coding a tic tac toe game where $2$ players play tic tac toe randomly on a given $N$ board size $(N\times N)$ . $X$ starts first. If one side gets $N$ consecutive (horizontal/vertical/diagonal) of their symbols, they win. If no one won and there is no more space on the board, it is a draw. I implemented it and give it a go a couple of times. I realized when $N$ gets bigger the draw rate increase a lot even with relatively small $N$ 's. For example I couldn't get a single non-draw game with $10\times 10$ board in $20$ games. My question is, what is the probability of a draw on $N\times N$ tic tac toe board with random play in terms of $N$ ? or if it is too complicated to express, what is the general relation between $N$ and the draw rate? P.S: I don't know the answer.","I was coding a tic tac toe game where players play tic tac toe randomly on a given board size . starts first. If one side gets consecutive (horizontal/vertical/diagonal) of their symbols, they win. If no one won and there is no more space on the board, it is a draw. I implemented it and give it a go a couple of times. I realized when gets bigger the draw rate increase a lot even with relatively small 's. For example I couldn't get a single non-draw game with board in games. My question is, what is the probability of a draw on tic tac toe board with random play in terms of ? or if it is too complicated to express, what is the general relation between and the draw rate? P.S: I don't know the answer.",2 N (N\times N) X N N N 10\times 10 20 N\times N N N,"['probability', 'tic-tac-toe']"
60,"Measurability of maximum likelihood estimator. Is there a mistake in Lehmann's ""Theory of point estimation""?","Measurability of maximum likelihood estimator. Is there a mistake in Lehmann's ""Theory of point estimation""?",,"I'm trying to prove that MLE from the proof of one theorem in Lehmann's ""Theory of point estimation"" (the theorem is below) is a measurable function. I know that under some regularity conditions (e.g. stats.stackexchange.com/questions/430954/example-of-a-non-measurable-maximum-likelihood-estimator or when is the maximum likelihood estimator measurable )  MLE is measurable, but it didn't help me to prove that the closest root from the proof is a measurable function. The problem is as follows. By definition, an estimate is a measurable function. Even the existence of a measurable version of MLE is not so obvious, but here an extremum arises over the set of MLE. It is unlikely that a theorem from a classical book can be wrong, but how to prove it? The theorem is here: I think that in $(3.14)$ the set $|\hat{\theta}_n - \theta_0|$ may be not measurable (but I don't know counterexample).","I'm trying to prove that MLE from the proof of one theorem in Lehmann's ""Theory of point estimation"" (the theorem is below) is a measurable function. I know that under some regularity conditions (e.g. stats.stackexchange.com/questions/430954/example-of-a-non-measurable-maximum-likelihood-estimator or when is the maximum likelihood estimator measurable )  MLE is measurable, but it didn't help me to prove that the closest root from the proof is a measurable function. The problem is as follows. By definition, an estimate is a measurable function. Even the existence of a measurable version of MLE is not so obvious, but here an extremum arises over the set of MLE. It is unlikely that a theorem from a classical book can be wrong, but how to prove it? The theorem is here: I think that in the set may be not measurable (but I don't know counterexample).",(3.14) |\hat{\theta}_n - \theta_0|,"['probability', 'probability-theory', 'statistics', 'statistical-inference', 'measurable-functions']"
61,limsup of a sequence of subadditive random variables with subgaussian tail.,limsup of a sequence of subadditive random variables with subgaussian tail.,,"Suppose that $X_n$ is a sequence of random variables satisfying $\bullet P(X_n > t\sqrt{n}) < e^{-t^2}$ , and $\bullet X_{m+n} < X_m+X_n$ for all $n,m$ . We want to try and show that $$\limsup_{n\rightarrow \infty} \frac{X_n}{\sqrt{n\log\log(n)}} \leq 1$$ holds almost surely. My initial attempt was to look at the subsequence $X_{n_k}$ where $n_k = (1+\delta)^k$ for a small $\delta$ . Since $$ P(X_{n_k} > (1+\epsilon)\sqrt{n_k\log\log(n_k)}) < e^{-(1+\epsilon)\log\log(n_k)} = \frac{1}{\log(n_k)^{1+\epsilon}} = \left(\frac{1}{k\log(1+\delta)}\right)^{1+\epsilon} $$ Borel-Cantelli implies that $$ \limsup_{k\rightarrow \infty} \frac{X_{n_k}}{\sqrt{n_k\log\log(n_k)}}\leq 1 $$ holds almost surely. I now want to use the subadditivity of the sequence to help me bound the terms $X_n$ for $n_k\leq n < n_{k+1}$ . In particular, we have that $$ \frac{X_n}{\sqrt{n\log\log n}} \leq \frac{X_{n_k}}{\sqrt{n_k\log\log n_k}} + \frac{X_{n-n_k}}{\sqrt{n\log\log n}} $$ so that I need to show that $$ \frac{X_{n-n_k}}{\sqrt{n\log\log n}} \rightarrow 0 $$ almost surely. Heuristically, this should be true since $n-n_k \approx \delta (1+\delta)^k$ will be much smaller than $n$ , but I'm stuck here. Does anyone have any ideas on how I can bound this $X_{n-n_k}$ term? Is this even a good approach to this problem?","Suppose that is a sequence of random variables satisfying , and for all . We want to try and show that holds almost surely. My initial attempt was to look at the subsequence where for a small . Since Borel-Cantelli implies that holds almost surely. I now want to use the subadditivity of the sequence to help me bound the terms for . In particular, we have that so that I need to show that almost surely. Heuristically, this should be true since will be much smaller than , but I'm stuck here. Does anyone have any ideas on how I can bound this term? Is this even a good approach to this problem?","X_n \bullet P(X_n > t\sqrt{n}) < e^{-t^2} \bullet X_{m+n} < X_m+X_n n,m \limsup_{n\rightarrow \infty} \frac{X_n}{\sqrt{n\log\log(n)}} \leq 1 X_{n_k} n_k = (1+\delta)^k \delta 
P(X_{n_k} > (1+\epsilon)\sqrt{n_k\log\log(n_k)}) < e^{-(1+\epsilon)\log\log(n_k)} = \frac{1}{\log(n_k)^{1+\epsilon}} = \left(\frac{1}{k\log(1+\delta)}\right)^{1+\epsilon}
 
\limsup_{k\rightarrow \infty} \frac{X_{n_k}}{\sqrt{n_k\log\log(n_k)}}\leq 1
 X_n n_k\leq n < n_{k+1} 
\frac{X_n}{\sqrt{n\log\log n}} \leq \frac{X_{n_k}}{\sqrt{n_k\log\log n_k}} + \frac{X_{n-n_k}}{\sqrt{n\log\log n}}
 
\frac{X_{n-n_k}}{\sqrt{n\log\log n}} \rightarrow 0
 n-n_k \approx \delta (1+\delta)^k n X_{n-n_k}","['probability', 'probability-theory', 'borel-cantelli-lemmas']"
62,$\left(S_{n}\right)_{n \geq 0}$ be a simple symmetric random walk. Prove $ P_{0}(S_{m}=x \mid S_{n}=y)=P_{0}(S_{n-m}=y-x \mid S_{n}=y) $,be a simple symmetric random walk. Prove,\left(S_{n}\right)_{n \geq 0}  P_{0}(S_{m}=x \mid S_{n}=y)=P_{0}(S_{n-m}=y-x \mid S_{n}=y) ,"I am stuck on the question below and I am not sure if I am heading in the right direction. Will be grateful for any direction. Let $\left(S_{n}\right)_{n \geq 0}$ be a simple symmetric random walk. (a) Suppose that $m, n, x, y$ are integers such that $n>m>0$ , and $n, y$ have the same parity. Prove that $$ P_{0}\left(S_{m}=x \mid S_{n}=y\right)=P_{0}\left(S_{n-m}=y-x \mid S_{n}=y\right) $$ (b) Hence compute $E_{0}\left(S_{m}+S_{n-m} \mid S_{n}=y\right)$ and $E_{0}\left(S_{m} \mid S_{2 m}=2 x\right)$ . My attempt I am stuck on how to fine $P_{0}\left(S_{m}=x \mid S_{n}=y\right)$ but this is what I did: Le $\varepsilon_{1}, \ldots, \varepsilon_{n}$ be elements of $\{-1,+1\}$ such that $\varepsilon_{1}+\cdots+\varepsilon_{n}=x$ . Then I can write $P_{0}\left(S_{m}=x \mid S_{n}=y\right)$ as: $$P_{0}\left(\varepsilon_{1} +, \ldots, + \varepsilon_{m}=x \mid S_{n}=y\right)$$ $$=\frac{P_{0}\left(\varepsilon_{1} +, \ldots, + \varepsilon_{m}=x\right)}{P_{0}\left(S_{n}=y\right)}$$ From the notes of Markov-chain and random walks by Takis , using Lemma 16 p.83, I know that $$P_{0}\left(S_{n}=y\right)=\left(\begin{array}{c}n \\ (n+y) / 2\end{array}\right) 2^{-n}$$ And since each $\varepsilon_{i}$ should be uniformly distributed then $$P_{0}(\varepsilon_{1} +, \ldots, + \varepsilon_{m}=x ) = \frac{1}{2^m}$$ Therefore: $$P_{0}\left(S_{m}=x \mid S_{n}=y\right) = \frac{2^{-m}}{\left(\begin{array}{c}n \\ (n+y) / 2\end{array}\right)2^{-n}}$$ Similarly: $$P_{0}\left(S_{n-m}=y-x \mid S_{n}=y\right) =\frac{P_{0}\left(\varepsilon_{m} +, \ldots, + \varepsilon_{n}=y-x\right)}{P_{0}\left(S_{n}=y\right)} = \frac{2^{-(n-m)}}{\left(\begin{array}{c}n \\ (n+y) / 2\end{array}\right)2^{-n}}$$ I am not sure if what I am doing is correct so any insight would help me a lot. For the second part where I need to find $E_{0}\left(S_{m}+S_{n-m} \mid S_{n}=y\right)$ , I believe I can split this into $$E_{0}\left(S_{m} \mid S_{n}=y\right) + E_{0}\left(S_{n-m} \mid S_{n}=y\right)$$ Then I just take the expectation of the binomial distributed part and uniformly distributed part, am I right? Thank you","I am stuck on the question below and I am not sure if I am heading in the right direction. Will be grateful for any direction. Let be a simple symmetric random walk. (a) Suppose that are integers such that , and have the same parity. Prove that (b) Hence compute and . My attempt I am stuck on how to fine but this is what I did: Le be elements of such that . Then I can write as: From the notes of Markov-chain and random walks by Takis , using Lemma 16 p.83, I know that And since each should be uniformly distributed then Therefore: Similarly: I am not sure if what I am doing is correct so any insight would help me a lot. For the second part where I need to find , I believe I can split this into Then I just take the expectation of the binomial distributed part and uniformly distributed part, am I right? Thank you","\left(S_{n}\right)_{n \geq 0} m, n, x, y n>m>0 n, y 
P_{0}\left(S_{m}=x \mid S_{n}=y\right)=P_{0}\left(S_{n-m}=y-x \mid S_{n}=y\right)
 E_{0}\left(S_{m}+S_{n-m} \mid S_{n}=y\right) E_{0}\left(S_{m} \mid S_{2 m}=2 x\right) P_{0}\left(S_{m}=x \mid S_{n}=y\right) \varepsilon_{1}, \ldots, \varepsilon_{n} \{-1,+1\} \varepsilon_{1}+\cdots+\varepsilon_{n}=x P_{0}\left(S_{m}=x \mid S_{n}=y\right) P_{0}\left(\varepsilon_{1} +, \ldots, + \varepsilon_{m}=x \mid S_{n}=y\right) =\frac{P_{0}\left(\varepsilon_{1} +, \ldots, + \varepsilon_{m}=x\right)}{P_{0}\left(S_{n}=y\right)} P_{0}\left(S_{n}=y\right)=\left(\begin{array}{c}n \\ (n+y) / 2\end{array}\right) 2^{-n} \varepsilon_{i} P_{0}(\varepsilon_{1} +, \ldots, + \varepsilon_{m}=x ) = \frac{1}{2^m} P_{0}\left(S_{m}=x \mid S_{n}=y\right) = \frac{2^{-m}}{\left(\begin{array}{c}n \\ (n+y) / 2\end{array}\right)2^{-n}} P_{0}\left(S_{n-m}=y-x \mid S_{n}=y\right) =\frac{P_{0}\left(\varepsilon_{m} +, \ldots, + \varepsilon_{n}=y-x\right)}{P_{0}\left(S_{n}=y\right)} = \frac{2^{-(n-m)}}{\left(\begin{array}{c}n \\ (n+y) / 2\end{array}\right)2^{-n}} E_{0}\left(S_{m}+S_{n-m} \mid S_{n}=y\right) E_{0}\left(S_{m} \mid S_{n}=y\right) + E_{0}\left(S_{n-m} \mid S_{n}=y\right)","['probability', 'markov-chains', 'uniform-distribution', 'random-walk', 'binomial-distribution']"
63,Expected distance/ time for a bouncing ball to stop.,Expected distance/ time for a bouncing ball to stop.,,"I saw someone on YouTube glue half of a bouncy ball to a golf ball. I thought of an interesting expected value problem using it. Consider when you drop the ball it lands on the bouncy or non bouncy side with equal probability. When it lands on the bouncy side it returns to $a\%$ of its height before the bounce and if it lands on the golf side it bounces and returns to $b\%$ of its height before the bounce. With $0<b<a<100$ Lets say the ball stops bouncing when it first bounces up less than $m$ centimetres. For example, let $a = 50$ , $b=10$ and $m=1$ you drop the ball from $1$ metre and it lands on the bouncy side so returns to $50$ cm. It then bounces on the golf side so returns to $5$ cm. It then bounces on the bouncy side three times so goes $2.5$ , $1.25$ , $0.625$ and stops bouncing. I had a few problems that I am sure are very related. If you drop the ball from $1$ metre what is the expected number of bounces? If you drop the ball from $1$ metre what is the expected distance it travels before it stops bouncing. I am happy to consider only the second question and allow for infinite bouncing, I feel like the condition on stopping bouncing will make calculations very hard. Can we relate these two problems nicely? My ideas: Let $H$ denote the height of the ball after $n$ bounces. (Starting from $1$ metre) $H = a^X \cdot b^{n-X}$ Where $X\sim$ Bin( $n,\frac{1}{2}$ ) denotes the number of ""bouncy bounces"" We could probably uses logarithms to find out all different combinations that yield $H< m$ and binomial co-efficents then manually calculate EV We could say on average each bounce reduces our ball's height by $\frac{b+a}{2}$ and then think use logs to work out how many bounces it would take reducing at the rate of $\frac{b+a}{2}$ to stop. But I don't think this will give a very accurate answer because of the RV.s in the exponent. We can just run some code. Please no full solutions I want to tackle this myself! Just hints for now","I saw someone on YouTube glue half of a bouncy ball to a golf ball. I thought of an interesting expected value problem using it. Consider when you drop the ball it lands on the bouncy or non bouncy side with equal probability. When it lands on the bouncy side it returns to of its height before the bounce and if it lands on the golf side it bounces and returns to of its height before the bounce. With Lets say the ball stops bouncing when it first bounces up less than centimetres. For example, let , and you drop the ball from metre and it lands on the bouncy side so returns to cm. It then bounces on the golf side so returns to cm. It then bounces on the bouncy side three times so goes , , and stops bouncing. I had a few problems that I am sure are very related. If you drop the ball from metre what is the expected number of bounces? If you drop the ball from metre what is the expected distance it travels before it stops bouncing. I am happy to consider only the second question and allow for infinite bouncing, I feel like the condition on stopping bouncing will make calculations very hard. Can we relate these two problems nicely? My ideas: Let denote the height of the ball after bounces. (Starting from metre) Where Bin( ) denotes the number of ""bouncy bounces"" We could probably uses logarithms to find out all different combinations that yield and binomial co-efficents then manually calculate EV We could say on average each bounce reduces our ball's height by and then think use logs to work out how many bounces it would take reducing at the rate of to stop. But I don't think this will give a very accurate answer because of the RV.s in the exponent. We can just run some code. Please no full solutions I want to tackle this myself! Just hints for now","a\% b\% 0<b<a<100 m a = 50 b=10 m=1 1 50 5 2.5 1.25 0.625 1 1 H n 1 H = a^X \cdot b^{n-X} X\sim n,\frac{1}{2} H< m \frac{b+a}{2} \frac{b+a}{2}","['probability', 'statistics', 'expected-value']"
64,"Is there something like ""stochastic induction""?","Is there something like ""stochastic induction""?",,"I'm trying to prove convergence of a stochastic approximation-like algorithm. I have two questions about prove-techniques when working with randomness. 1. For a non-random sequence $(a_t)_t$ one could prove $a_t \rightarrow a$ by induction as follows: $$\text{Assume } |a_t - a| \leq \epsilon \text{ for some } t \text{ and show that } |a_{t+1} - a| \leq \delta \cdot \epsilon \text{ with } \delta \in (0, 1)$$ Now consider the random case. If one would knew that the induction hypothesis holds with probability $p_t$ and the induction step holds with probability $q_t$ , e.g.: $$\mathbb{P}(|a_t - a| \leq \epsilon) \geq p_t, \text{ and if } |a_t - a| \leq \epsilon \text{ is true, then}$$ $$\mathbb{P}(|a_{t+1} - a| \leq \delta \cdot \epsilon \ \big| \ |a_t - a| \leq \epsilon) \geq q_t$$ Could one show that $a_t \rightarrow a$ almost surely? Furthermore, how would the conditions of $p_t$ and $q_t$ look like? I would assume that $\prod_t p_t \cdot q_t > C$ must hold. 2. Alternatively, for a non-random sequence $(a_t)_t$ one could prove $a_t \rightarrow a$ simply by showing that: $$|a_{t+1} - a| \leq \dots \leq \delta |a_t - a| \text{ with } \delta \in (0, 1) \ \forall t$$ Now consider again the random case. If the statement holds with probability $p_t$ . Does $a_t \rightarrow a$ follow with probability $\prod_t p_t$ ? Thanks in advance!","I'm trying to prove convergence of a stochastic approximation-like algorithm. I have two questions about prove-techniques when working with randomness. 1. For a non-random sequence one could prove by induction as follows: Now consider the random case. If one would knew that the induction hypothesis holds with probability and the induction step holds with probability , e.g.: Could one show that almost surely? Furthermore, how would the conditions of and look like? I would assume that must hold. 2. Alternatively, for a non-random sequence one could prove simply by showing that: Now consider again the random case. If the statement holds with probability . Does follow with probability ? Thanks in advance!","(a_t)_t a_t \rightarrow a \text{Assume } |a_t - a| \leq \epsilon \text{ for some } t \text{ and show that } |a_{t+1} - a| \leq \delta \cdot \epsilon \text{ with } \delta \in (0, 1) p_t q_t \mathbb{P}(|a_t - a| \leq \epsilon) \geq p_t, \text{ and if } |a_t - a| \leq \epsilon \text{ is true, then} \mathbb{P}(|a_{t+1} - a| \leq \delta \cdot \epsilon \ \big| \ |a_t - a| \leq \epsilon) \geq q_t a_t \rightarrow a p_t q_t \prod_t p_t \cdot q_t > C (a_t)_t a_t \rightarrow a |a_{t+1} - a| \leq \dots \leq \delta |a_t - a| \text{ with } \delta \in (0, 1) \ \forall t p_t a_t \rightarrow a \prod_t p_t","['probability', 'probability-theory', 'convergence-divergence', 'stochastic-processes', 'stochastic-approximation']"
65,Are $\mathfrak{Re}\varphi$ and $\mathfrak{Im}\varphi$ characteristic functions?,Are  and  characteristic functions?,\mathfrak{Re}\varphi \mathfrak{Im}\varphi,"Let $\varphi$ be the characteristic function of a random variable X. Are $\mathfrak{Re}\varphi$ and $\mathfrak{Im}\varphi$ characteristic functions? I'm very stuck on this question. I'm not sure what would be required to prove that something is a characteristic function. It seems like these function does not violate any of the basic properties of characteristic functions https://en.wikipedia.org/wiki/Characteristic_function_(probability_theory)#Properties . So I cannot say that these are not characteristic functions. However, I do not know what is required to show that these definitely are characteristic functions. So far my only thoughts have been to say $\varphi(t)=\mathbb{E}(e^{itX})=f(t)+ig(t)$ for some functions $f:\mathbb{R}\to\mathbb{R}, g:\mathbb{R}\to\mathbb{R}$ , and then the question is whether $f$ and $g$ are characteristic functions. I thought this would be a simple question, but I am quite stuck, so any help would be much appreciated!","Let be the characteristic function of a random variable X. Are and characteristic functions? I'm very stuck on this question. I'm not sure what would be required to prove that something is a characteristic function. It seems like these function does not violate any of the basic properties of characteristic functions https://en.wikipedia.org/wiki/Characteristic_function_(probability_theory)#Properties . So I cannot say that these are not characteristic functions. However, I do not know what is required to show that these definitely are characteristic functions. So far my only thoughts have been to say for some functions , and then the question is whether and are characteristic functions. I thought this would be a simple question, but I am quite stuck, so any help would be much appreciated!","\varphi \mathfrak{Re}\varphi \mathfrak{Im}\varphi \varphi(t)=\mathbb{E}(e^{itX})=f(t)+ig(t) f:\mathbb{R}\to\mathbb{R}, g:\mathbb{R}\to\mathbb{R} f g",['probability']
66,"almost sure convergency of $W_{n}=\frac{Z_{n}}{m^{n}}\to W$ implies $\frac{Z_{n}}{\mu^{n}}\to0$, where $\mu>m$.","almost sure convergency of  implies , where .",W_{n}=\frac{Z_{n}}{m^{n}}\to W \frac{Z_{n}}{\mu^{n}}\to0 \mu>m,"I though to apply  Borel-Cantelli, however it does not seem to work. m and $\mu$ are constants, with $0<m<\mu$ , with random identically distributed variables $Z_{n}$ , which take values in $\mathbb{N}_{0}$ . What I got was (all of them are nonnegativ so I leave the abs away):\ $\sum^{\infty}_{n=0}\mathbb{P}(\frac{Z_{n}}{\mu^{n}}>\epsilon) =\sum^{\infty}_{n=0}\mathbb{P}(Z_{n}>\mu^{n}\epsilon)=\sum^{\infty}_{n=0}\mathbb{E}(\mathbb{1}_{\{Z_{n}>\mu^{n}\epsilon\}})=\mathbb{E}(\sum^{\infty}_{n=0}\mathbb{1}_{\{Z_{n}>\mu^{n}\epsilon\}})$ \ I though when I get expected value it would be easier to estimate because of linearity and at this point I tried to estimate the last part with $\sum^{\infty}_{n=0}\mathbb{P}(|\frac{Z_{n}}{m^{n}}-W|>\epsilon)$ turning it into expected value. However it doesn´t seem to help a lot. Well now because of the comment I notice, that is sufficient to use , that $\frac{m}{\mu}<1$ then because W is some finite value $\lim_{n\to \infty}\frac{Z_{n}}{\mu^{n}}=\lim_{n\to \infty}\frac{Z_{n}}{\mu^{n}}\frac{m^{n}}{m^{n}}=W\lim_{n\to \infty}(\frac{m}{\mu})^{n}=0$","I though to apply  Borel-Cantelli, however it does not seem to work. m and are constants, with , with random identically distributed variables , which take values in . What I got was (all of them are nonnegativ so I leave the abs away):\ \ I though when I get expected value it would be easier to estimate because of linearity and at this point I tried to estimate the last part with turning it into expected value. However it doesn´t seem to help a lot. Well now because of the comment I notice, that is sufficient to use , that then because W is some finite value",\mu 0<m<\mu Z_{n} \mathbb{N}_{0} \sum^{\infty}_{n=0}\mathbb{P}(\frac{Z_{n}}{\mu^{n}}>\epsilon) =\sum^{\infty}_{n=0}\mathbb{P}(Z_{n}>\mu^{n}\epsilon)=\sum^{\infty}_{n=0}\mathbb{E}(\mathbb{1}_{\{Z_{n}>\mu^{n}\epsilon\}})=\mathbb{E}(\sum^{\infty}_{n=0}\mathbb{1}_{\{Z_{n}>\mu^{n}\epsilon\}}) \sum^{\infty}_{n=0}\mathbb{P}(|\frac{Z_{n}}{m^{n}}-W|>\epsilon) \frac{m}{\mu}<1 \lim_{n\to \infty}\frac{Z_{n}}{\mu^{n}}=\lim_{n\to \infty}\frac{Z_{n}}{\mu^{n}}\frac{m^{n}}{m^{n}}=W\lim_{n\to \infty}(\frac{m}{\mu})^{n}=0,"['probability', 'stochastic-processes', 'borel-cantelli-lemmas']"
67,"Do we have $E[X_T|X_S]=X_{T\land S}$ for stopping times T,S?","Do we have  for stopping times T,S?",E[X_T|X_S]=X_{T\land S},"Let $X_t$ be a Right continuous Martingale. ( $t\in (0,\infty)$ ). Let $T,S$ be two arbitrary stopping time. Do we have \begin{align*} E[X_T|\mathcal{F}_S] = X_{T\land S} \end{align*} By optional sampling theorem, we have $E[X_T|\mathcal{F}_{T\land S}]=X_{T\land S}$ . And $\mathcal{F}_S\cap \mathcal{F}_T= \mathcal{F}_{S\land T}$ . However, still cannot connect these with the statement above. Does anyone have any idea or comments? Thanks! Kind of figure out. Assume the U.I. condition (optional sampling also assumes this). So $X_{T\land t}$ is a R-Martingale, and the limit $X_T$ exists. Apply optional sampling to $X_{T\land \infty}=X_T$ we get \begin{align*} E[X_{T}|\mathcal{F}_S]=X_{T\land S} \end{align*} Guess this is the right answer.","Let be a Right continuous Martingale. ( ). Let be two arbitrary stopping time. Do we have By optional sampling theorem, we have . And . However, still cannot connect these with the statement above. Does anyone have any idea or comments? Thanks! Kind of figure out. Assume the U.I. condition (optional sampling also assumes this). So is a R-Martingale, and the limit exists. Apply optional sampling to we get Guess this is the right answer.","X_t t\in (0,\infty) T,S \begin{align*}
E[X_T|\mathcal{F}_S] = X_{T\land S}
\end{align*} E[X_T|\mathcal{F}_{T\land S}]=X_{T\land S} \mathcal{F}_S\cap \mathcal{F}_T= \mathcal{F}_{S\land T} X_{T\land t} X_T X_{T\land \infty}=X_T \begin{align*}
E[X_{T}|\mathcal{F}_S]=X_{T\land S}
\end{align*}","['probability', 'stochastic-processes', 'conditional-probability']"
68,conditional expectation of the product,conditional expectation of the product,,"Let $\Omega$ be a compact Hausdorff space in $\mathbb{C}^n$ . Let $\sigma_\Omega$ be the Borel sigma algebra on $\Omega$ . Let $\zeta: \Omega\longrightarrow\partial \mathbb{D}$ be a non constant continuous  function. Let $\sigma_{\partial \mathbb{D}}$ be the Borel sigma algebra on $\partial \mathbb{D}$ (Unit circle on the complex plane). Now consider the sigma algebra $\sigma_\zeta=\{{\zeta}^{-1}(A): \;A\in \sigma_{\partial \mathbb{D}}\}\subset \sigma_\Omega$ . Now let $f\in L^1(\Omega, \sigma_\Omega, \mu)$ and lets define a new measure $f_\mu$ on $(\Omega,\sigma_\zeta)$ as $f_{\mu}(A)=\int_A f d\mu$ . It is easy to see that for $A\in \sigma_\zeta $ , ${\mu}(A)=0$ implies $f_{\mu}(A)=0$ , i.e $f_{\mu}(A)$ is absolutely continuous with the restriction of $\mu$ to $\sigma_\zeta$ , so by the Radon Nikodym theorem there exists a $g\in L^1 (\Omega, \sigma_\zeta, \mu)$ such that $\int_A f d\mu =\int_A g d\mu$ for every $A\in \sigma_\zeta$ . Lets call this $g$ as the conditional expectation of $f$ and denote it as $E(f|\sigma_\zeta)$ . Now suppose $h,k\in L^1(\Omega, \sigma_\Omega, \mu)$ are bounded. Then will it be true that $E(hk|\sigma_\zeta)=E(h|\sigma_\zeta)E(k|\sigma_\zeta).$ If not, then is there a condition (preferably if and only if) under which the same would hold? An answer in Measure Theory terms would be really appreciated.","Let be a compact Hausdorff space in . Let be the Borel sigma algebra on . Let be a non constant continuous  function. Let be the Borel sigma algebra on (Unit circle on the complex plane). Now consider the sigma algebra . Now let and lets define a new measure on as . It is easy to see that for , implies , i.e is absolutely continuous with the restriction of to , so by the Radon Nikodym theorem there exists a such that for every . Lets call this as the conditional expectation of and denote it as . Now suppose are bounded. Then will it be true that If not, then is there a condition (preferably if and only if) under which the same would hold? An answer in Measure Theory terms would be really appreciated.","\Omega \mathbb{C}^n \sigma_\Omega \Omega \zeta: \Omega\longrightarrow\partial \mathbb{D} \sigma_{\partial \mathbb{D}} \partial \mathbb{D} \sigma_\zeta=\{{\zeta}^{-1}(A): \;A\in \sigma_{\partial \mathbb{D}}\}\subset \sigma_\Omega f\in L^1(\Omega, \sigma_\Omega, \mu) f_\mu (\Omega,\sigma_\zeta) f_{\mu}(A)=\int_A f d\mu A\in \sigma_\zeta  {\mu}(A)=0 f_{\mu}(A)=0 f_{\mu}(A) \mu \sigma_\zeta g\in L^1 (\Omega, \sigma_\zeta, \mu) \int_A f d\mu =\int_A g d\mu A\in \sigma_\zeta g f E(f|\sigma_\zeta) h,k\in L^1(\Omega, \sigma_\Omega, \mu) E(hk|\sigma_\zeta)=E(h|\sigma_\zeta)E(k|\sigma_\zeta).","['probability', 'probability-theory', 'measure-theory', 'conditional-expectation']"
69,"The Birth-Death Process, the Umbrella Problem and the running shoes problem, or why is it so difficult to assign Markov Chain states.","The Birth-Death Process, the Umbrella Problem and the running shoes problem, or why is it so difficult to assign Markov Chain states.",,"Disclaimer: I have been thinking about this problem for 3 days and my previous thoughts, as well as the details of the problem, are in this question , so please take a look there to understand the context. Thanks. Long story short, I had thought of a state assignment that turned out to be wrong because there is some dependency that is not being accounted for. Now, I was solving the MIT OCW problem set on Markov Chains , and I noticed that the first problem is so similar to the umbrella problem. However, the state assignment used in the solution is different. Applying the same logic to the umbrella problem, I will draw the Markov chain for a single place (assume it's home, there is no loss of generality since the other place will have the remaining umbrellas anyway). State $i$ means that this place has exactly $i$ umbrellas. This should be correct since (1) half of the time I'm going from home to office, and the other half I'm going from office to home, (2) For state $0$ , the probability that home stays at $0$ is that [it doesn't rain, or it rains but I'm going from home to office], so $p_{00} = 1-p + \frac{1}{2}p = 1-\frac{1}{2}p$ , and so on. Therefore, the steady state probabilities are each $\frac{1}{5}$ , and the probability that I get wet is that [home is at state $0$ and I am going from home to office and it rains] or [home is at state 4 and I am going from office to home and it rains]. That is, the probability that I get wet should be $\frac{1}{5}\times \frac{1}{2} \times p + \frac{1}{5} \times \frac{1}{2} \times p = \frac{1}{5}p$ . This does not agree with the famous solution for the umbrella problem, which is $\frac{p(1-p)}{5-p}$ . Could somebody please explain to me whether the two problems are actually essentially different, or are there multiple solutions to this problem depending on the initial set of assumptions? And in the latter case, what assumptions have been made here to make the solutions so drastically different?","Disclaimer: I have been thinking about this problem for 3 days and my previous thoughts, as well as the details of the problem, are in this question , so please take a look there to understand the context. Thanks. Long story short, I had thought of a state assignment that turned out to be wrong because there is some dependency that is not being accounted for. Now, I was solving the MIT OCW problem set on Markov Chains , and I noticed that the first problem is so similar to the umbrella problem. However, the state assignment used in the solution is different. Applying the same logic to the umbrella problem, I will draw the Markov chain for a single place (assume it's home, there is no loss of generality since the other place will have the remaining umbrellas anyway). State means that this place has exactly umbrellas. This should be correct since (1) half of the time I'm going from home to office, and the other half I'm going from office to home, (2) For state , the probability that home stays at is that [it doesn't rain, or it rains but I'm going from home to office], so , and so on. Therefore, the steady state probabilities are each , and the probability that I get wet is that [home is at state and I am going from home to office and it rains] or [home is at state 4 and I am going from office to home and it rains]. That is, the probability that I get wet should be . This does not agree with the famous solution for the umbrella problem, which is . Could somebody please explain to me whether the two problems are actually essentially different, or are there multiple solutions to this problem depending on the initial set of assumptions? And in the latter case, what assumptions have been made here to make the solutions so drastically different?",i i 0 0 p_{00} = 1-p + \frac{1}{2}p = 1-\frac{1}{2}p \frac{1}{5} 0 \frac{1}{5}\times \frac{1}{2} \times p + \frac{1}{5} \times \frac{1}{2} \times p = \frac{1}{5}p \frac{p(1-p)}{5-p},"['probability', 'probability-theory', 'markov-chains', 'conditional-probability', 'markov-process']"
70,Numerical Computation of Chapman-Kolmogorov Condition for a Dataset,Numerical Computation of Chapman-Kolmogorov Condition for a Dataset,,"Good day every one, I am trying to solve a particular problem relating to probabilities: I have a time series dataset, where the price at time $t_i$ can be given as $x(t_i)$ We can then define a returns time series where $r_i = \ln[x(t_{i+1})/x(t_i)]$ I now want to find some timescale $t_M$ for which the returns could be viewed as a Markov process. For this, we require the Chapman-Kolmogorov (CM) equation $$p(r_2, t_2|r_1, t_1) = \int dr' p(r_2, t_2|r', t')p(r', t'|r_1, t_1) $$ to hold where p represents the usual conditional probability which can be given as $$p(r_i, t_i|r_j, t_j) = \frac{p(r_i, t_i;r_j, t_j) }{p(r_j, t_j) } $$ We can  reformulate the CM equation as  the value $$S= |(p(r_2, t_2|r_1, t_1)- \int dr' p(r_2, t_2|r', t')p(r', t'|r_1, t_1) |$$ For some given $r_1$ and $r_2$ in terms of $t'-t_1$ for example. We then expect $t_M = t'-t_1$ for a value of $t'-t_1$ where S vanishes or reaches its minimum. From what I have read, this value S, the CM conditions and subsequently the conditional probabilities can be computed numerically. So much so, that all the papers I have read make it seem like it a trivial matter. But I cannot find a way to do it. Does anyone know how to compute such values numerically from a given dataset? Thank you for your help .","Good day every one, I am trying to solve a particular problem relating to probabilities: I have a time series dataset, where the price at time can be given as We can then define a returns time series where I now want to find some timescale for which the returns could be viewed as a Markov process. For this, we require the Chapman-Kolmogorov (CM) equation to hold where p represents the usual conditional probability which can be given as We can  reformulate the CM equation as  the value For some given and in terms of for example. We then expect for a value of where S vanishes or reaches its minimum. From what I have read, this value S, the CM conditions and subsequently the conditional probabilities can be computed numerically. So much so, that all the papers I have read make it seem like it a trivial matter. But I cannot find a way to do it. Does anyone know how to compute such values numerically from a given dataset? Thank you for your help .","t_i x(t_i) r_i = \ln[x(t_{i+1})/x(t_i)] t_M p(r_2, t_2|r_1, t_1) = \int dr' p(r_2, t_2|r', t')p(r', t'|r_1, t_1)  p(r_i, t_i|r_j, t_j) = \frac{p(r_i, t_i;r_j, t_j) }{p(r_j, t_j) }  S= |(p(r_2, t_2|r_1, t_1)- \int dr' p(r_2, t_2|r', t')p(r', t'|r_1, t_1) | r_1 r_2 t'-t_1 t_M = t'-t_1 t'-t_1","['probability', 'statistics', 'numerical-methods', 'markov-chains', 'time-series']"
71,"A coin is tossed several times and the outcomes are being recorded in a string of H and T. How long - on average - will you have to wait for an ""TTH?"" [closed]","A coin is tossed several times and the outcomes are being recorded in a string of H and T. How long - on average - will you have to wait for an ""TTH?"" [closed]",,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 2 years ago . Improve this question Problem. A coin is tossed several times and the outcomes are being recorded in a string of H (heads) and T (tails). For example, that is recorded as ""HHTTTHTH,"" so, how long - on average - will you have to wait for an ""TTH ?"" I need to a plan of solving by simulation via python. Here is the algorithm: First make a while loop for flipping coins. Then make booleans for each of the states $s_{0}, s_{1}, s_{2}, s_{3}:$ $s_{i}$ means that we have made $i$ progress on getting ""TTH."" For example, $s_{2}$ means that our last 2 flips were ""TT."" If $E_{i}$ is the expected number of flips until we get to $s_{3}$ given that we are on $s_{i},$ then we are looking for the value of $E_{0}.$ We have the following system using states $$E_{0}= 1+ \frac{1}{2}E_{1}+ \frac{1}{2}E_{0}$$ $$E_{1}= 1+ \frac{1}{2}E_{0}+\frac{1}{2}E_{2}$$ $$E_{2}= 1+ \frac{1}{2}E_{2}+ 0$$ This means $$E_{2}= 2$$ $$E_{0}- E_{1}= 2$$ $$2E_{1}- E_0= 4$$ $$E_{1}= 6$$ $$\boxed{E_{0}= 8}$$ Now just simulate the progress the flip makes toward the final state by changing the values of each of the booleans until it gets to the final state. Also you would have to repeat this process a sufficient number of trials and take the average number of flips of all trials. Edit . What is the probability of it takes 8 trials to wait for an ""TTH ?"" Hope can you help... thanks a real lot !","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 2 years ago . Improve this question Problem. A coin is tossed several times and the outcomes are being recorded in a string of H (heads) and T (tails). For example, that is recorded as ""HHTTTHTH,"" so, how long - on average - will you have to wait for an ""TTH ?"" I need to a plan of solving by simulation via python. Here is the algorithm: First make a while loop for flipping coins. Then make booleans for each of the states means that we have made progress on getting ""TTH."" For example, means that our last 2 flips were ""TT."" If is the expected number of flips until we get to given that we are on then we are looking for the value of We have the following system using states This means Now just simulate the progress the flip makes toward the final state by changing the values of each of the booleans until it gets to the final state. Also you would have to repeat this process a sufficient number of trials and take the average number of flips of all trials. Edit . What is the probability of it takes 8 trials to wait for an ""TTH ?"" Hope can you help... thanks a real lot !","s_{0}, s_{1}, s_{2}, s_{3}: s_{i} i s_{2} E_{i} s_{3} s_{i}, E_{0}. E_{0}= 1+ \frac{1}{2}E_{1}+ \frac{1}{2}E_{0} E_{1}= 1+ \frac{1}{2}E_{0}+\frac{1}{2}E_{2} E_{2}= 1+ \frac{1}{2}E_{2}+ 0 E_{2}= 2 E_{0}- E_{1}= 2 2E_{1}- E_0= 4 E_{1}= 6 \boxed{E_{0}= 8}","['probability', 'algorithms']"
72,Convergence in probability of conditional second cross-moment for bivariate stationary process,Convergence in probability of conditional second cross-moment for bivariate stationary process,,"Let $(X_t,Y_t)_{t\in\mathbb N}$ be a bivariate real stationary process, and let $\mathcal F_t:=\sigma(Y_s :s\leq t)$ be the filtration generated by $Y_t$ . Assuming the following convergence result $$\frac{1}{T}\sum_{t=1}^T X^2_tY_t^2 \overset{p}{\to} E[X_t^2Y_t^2]<\infty \text{ as } T\to \infty,$$ holds, can I show then that $$\frac{1}{T}\sum_{t=1}^T E[X^2_t|\mathcal F_t]Y^2_t \overset{p}{\to} E[X_t^2Y_t^2] \text{ as } T\to \infty $$ ? (Assume $E[X^2_t]<\infty $ and $Y_t$ bounded). EDIT: This lemma can be found in Bauer's book measure and integration. The lemma implies that we have $\frac{1}{T}\sum_{t=1}^T X^2_tY_t^2 \overset{L^1}{\to} E[X_t^2Y_t^2]\text{ as } T\to \infty$ . We can then almost prove the claim: if we replace $\mathcal F_t$ by $\mathcal F_T$ then for any $\epsilon>0$ we have $$P\bigg[\bigg|\frac{1}{T}\sum_{t=1}^T E[X^2_t|\mathcal F_T]Y_t^2 - E[X_t^2Y_t^2]\bigg|\geq \epsilon\bigg ]$$ $$=P\bigg[\bigg|E\bigg[\frac{1}{T}\sum_{t=1}^T X_t^2Y_t^2 - E[X_t^2Y_t^2] \big|\mathcal F_T\bigg]\bigg|\geq \epsilon\bigg ]$$ $$\leq P\bigg[E\bigg[ \bigg|\frac{1}{T}\sum_{t=1}^T X_t^2Y_t^2 - E[X_t^2Y_t^2] \bigg| \big|\mathcal F_T\bigg]\geq \epsilon\bigg ]$$ $$\leq \frac{1}{\epsilon} E\bigg[ \bigg|\frac{1}{T}\sum_{t=1}^T X_t^2Y_t^2 - E[X_t^2Y_t^2] \bigg|\bigg ] \to 0  \text{ as } T\to \infty,$$ where the first inequality is Jensen's inequality for conditional expectations and the second inequality is because $\epsilon1_A\leq 1_AE\bigg[ \bigg|\frac{1}{T}\sum_{t=1}^T X_t^2Y_t^2 - E[X_t^2Y_t^2] \bigg| \big|\mathcal F_T\bigg]$ with $A=\bigg\{E\bigg[ \bigg|\frac{1}{T}\sum_{t=1}^T X_t^2Y_t^2 - E[X_t^2Y_t^2] \bigg| \big|\mathcal F_T\bigg]\geq \epsilon\bigg\}$ .","Let be a bivariate real stationary process, and let be the filtration generated by . Assuming the following convergence result holds, can I show then that ? (Assume and bounded). EDIT: This lemma can be found in Bauer's book measure and integration. The lemma implies that we have . We can then almost prove the claim: if we replace by then for any we have where the first inequality is Jensen's inequality for conditional expectations and the second inequality is because with .","(X_t,Y_t)_{t\in\mathbb N} \mathcal F_t:=\sigma(Y_s :s\leq t) Y_t \frac{1}{T}\sum_{t=1}^T X^2_tY_t^2 \overset{p}{\to} E[X_t^2Y_t^2]<\infty \text{ as } T\to \infty, \frac{1}{T}\sum_{t=1}^T E[X^2_t|\mathcal F_t]Y^2_t \overset{p}{\to} E[X_t^2Y_t^2] \text{ as } T\to \infty  E[X^2_t]<\infty  Y_t \frac{1}{T}\sum_{t=1}^T X^2_tY_t^2 \overset{L^1}{\to} E[X_t^2Y_t^2]\text{ as } T\to \infty \mathcal F_t \mathcal F_T \epsilon>0 P\bigg[\bigg|\frac{1}{T}\sum_{t=1}^T E[X^2_t|\mathcal F_T]Y_t^2 - E[X_t^2Y_t^2]\bigg|\geq \epsilon\bigg ] =P\bigg[\bigg|E\bigg[\frac{1}{T}\sum_{t=1}^T X_t^2Y_t^2 - E[X_t^2Y_t^2] \big|\mathcal F_T\bigg]\bigg|\geq \epsilon\bigg ] \leq P\bigg[E\bigg[ \bigg|\frac{1}{T}\sum_{t=1}^T X_t^2Y_t^2 - E[X_t^2Y_t^2] \bigg| \big|\mathcal F_T\bigg]\geq \epsilon\bigg ] \leq \frac{1}{\epsilon} E\bigg[ \bigg|\frac{1}{T}\sum_{t=1}^T X_t^2Y_t^2 - E[X_t^2Y_t^2] \bigg|\bigg ] \to 0  \text{ as } T\to \infty, \epsilon1_A\leq 1_AE\bigg[ \bigg|\frac{1}{T}\sum_{t=1}^T X_t^2Y_t^2 - E[X_t^2Y_t^2] \bigg| \big|\mathcal F_T\bigg] A=\bigg\{E\bigg[ \bigg|\frac{1}{T}\sum_{t=1}^T X_t^2Y_t^2 - E[X_t^2Y_t^2] \bigg| \big|\mathcal F_T\bigg]\geq \epsilon\bigg\}","['probability', 'probability-theory', 'conditional-expectation', 'law-of-large-numbers', 'stationary-processes']"
73,Expectation and Variance of Branching Process,Expectation and Variance of Branching Process,,"Let $X_n, n \in \mathbb Z_+$ , be the branching process generated by $\xi$ , $\mathbb E \xi = a$ $\mathbb Var \xi = \sigma^2$ . Find the $E X_{n} $ and $Var X_n$ My Attempt: By the Galton-Walton process, $$X_n = \sum_{i=1}^{X_n -1} \xi_{i}^{n}$$ The Expectation is defined as: $$ EX_n = E(E \left( X_n|X_{n-1} \right) )$$ $$E \left( X_n|X_{n-1} =k \right) = E \left( \sum_{i=1}^{X_n -1} \xi_{i}^{n}|X_{n-1} =k\right)=ka $$ $$\implies EX_n = E(E \left( X_n|X_{n-1} \right) ) = a EX_{n-1}$$ Since, $EX_1 =E\xi =a \implies EX_n =a^n$ Please I am stuck here. Is my approach correct? Also any help or hint on how to find the Variances is appreciated. Thanks!","Let , be the branching process generated by , . Find the and My Attempt: By the Galton-Walton process, The Expectation is defined as: Since, Please I am stuck here. Is my approach correct? Also any help or hint on how to find the Variances is appreciated. Thanks!","X_n, n \in \mathbb Z_+ \xi \mathbb E \xi = a \mathbb Var \xi = \sigma^2 E X_{n}  Var X_n X_n = \sum_{i=1}^{X_n -1} \xi_{i}^{n}  EX_n = E(E \left( X_n|X_{n-1} \right) ) E \left( X_n|X_{n-1} =k \right) = E \left( \sum_{i=1}^{X_n -1} \xi_{i}^{n}|X_{n-1} =k\right)=ka  \implies EX_n = E(E \left( X_n|X_{n-1} \right) ) = a EX_{n-1} EX_1 =E\xi =a \implies EX_n =a^n","['probability', 'stochastic-processes', 'conditional-expectation']"
74,Time spent in an interval of Brownian motion,Time spent in an interval of Brownian motion,,"Consider the standard one-dimensional Brownian motion $B_t$ . Set $$X_t = \int_0^t\mathbf{1}_{[0, 1]}(B_s)ds.$$ Here $\mathbf{1}_{[0, 1]}(\cdot)$ is the indicator function of the interval $[0, 1]$ . $X_t$ denotes the total time spent by Brownian motion in $[0, 1]$ before time $t$ , which can also be written using the Brownian local time as $$X_t = \int_0^1L^a(t)da.$$ Question: Can we compute the distribution of $X_t$ ? If we can't get the specific expression of its density, is there any method to get the decay estimate (lower and upper bound) of its density?","Consider the standard one-dimensional Brownian motion . Set Here is the indicator function of the interval . denotes the total time spent by Brownian motion in before time , which can also be written using the Brownian local time as Question: Can we compute the distribution of ? If we can't get the specific expression of its density, is there any method to get the decay estimate (lower and upper bound) of its density?","B_t X_t = \int_0^t\mathbf{1}_{[0, 1]}(B_s)ds. \mathbf{1}_{[0, 1]}(\cdot) [0, 1] X_t [0, 1] t X_t = \int_0^1L^a(t)da. X_t","['probability', 'functional-analysis', 'stochastic-processes', 'stochastic-calculus', 'brownian-motion']"
75,Estimating the number of infected people,Estimating the number of infected people,,"Given a set $U$ of people, there is a subset $S\subset U$ of infected people. You only know the size of $U$ but want to estimate $\dfrac{|S|}{|U|}$ . However, you don't have enough time to test every single person $p\in U$ on the virus so you decide to only test a fraction $K\subset U$ of all the people and thus try to approximate the ratio $\dfrac{|S|}{|U|}\approx \dfrac{|\{p\in K\mid p \text{ is infected}\}|}{|K|}$ . Given that you obtain a ratio $r\in [0.4\cdot |U|, 0.6\cdot |U|]$ , you want calculate the probability that the actual ratio $\dfrac{|S|}{|U|}$ lies in the interval $[0.4\cdot |U|, 0.6 \cdot |U|].$ I read some procedures about this often involving the normal distribution, however, I'm not allowed to use the normal distribution for this task. How could I attempt to solve this problem?","Given a set of people, there is a subset of infected people. You only know the size of but want to estimate . However, you don't have enough time to test every single person on the virus so you decide to only test a fraction of all the people and thus try to approximate the ratio . Given that you obtain a ratio , you want calculate the probability that the actual ratio lies in the interval I read some procedures about this often involving the normal distribution, however, I'm not allowed to use the normal distribution for this task. How could I attempt to solve this problem?","U S\subset U U \dfrac{|S|}{|U|} p\in U K\subset U \dfrac{|S|}{|U|}\approx \dfrac{|\{p\in K\mid p \text{ is infected}\}|}{|K|} r\in [0.4\cdot |U|, 0.6\cdot |U|] \dfrac{|S|}{|U|} [0.4\cdot |U|, 0.6 \cdot |U|].","['probability', 'approximation']"
76,Optimal strategy to maximize cumulative sum of dice rolls but the sum cannot be a square number [duplicate],Optimal strategy to maximize cumulative sum of dice rolls but the sum cannot be a square number [duplicate],,"This question already has an answer here : Toss a fair die until the cumulative sum is a perfect square-Expected Value (1 answer) Closed 3 years ago . The rule of the game is as follows: The player rolls a fair six-sided dice repetitively until the game is over. After each roll, if the cumulative sum of all the rolls so far is a square number (1, 4, 9...), then the game is over, and the player gets nothing. If the cumulative sum of all the rolls is NOT a square number, the player can choose to either stop rolling and get paid the current cumulative sum amount of dollars; or she can choose to continue the game and roll again. For example, let's say Alice plays this game and rolls a ""2"". It's not a square number. Alice chooses to continue the game. She rolls a ""6"" this time. Now the cumulative sum is 8. She can choose to keep rolling, or she can end the game and keep 8$ as her winning. Let's say she chooses to roll again and rolls a ""1"", then the cumulative sum becomes 9, a square number. The game ends and she gets no money. My question is: what is the optimal strategy for this game, and what is the expectation of winnings if you play the optimal strategy? Or in other words, if you charge a ticket price to play this game, it should be at least how much? My thoughts: let $E_k$ denote your expected payoff if the current cumulative sum is $k$ . $E_k=0  $ if $k=n^2$ where $n=1,2,...$ The ticket price would be $E_0=\frac{1}{6}(E_1+E_2+E_3+E_4+E_5+E_6)=\frac{1}{6}(E_2+E_3+E_5+E_6)$ And we have $E_2=max(2,\frac{1}{6}\sum^{i=6}_{i=1}E_{2+i}), E_3=max(3,\frac{1}{6}\sum^{i=6}_{i=1}E_{3+i}),...$ etc. To me this looks like a dynamic programming problem, but I cannot find a termination point. At what cumulative sum should I stop rolling? Let's say $N$ is a large integer, then for any cumulative sum $s$ where $(N-1)^2<s<N^2-6$ , we would absolutely choose to roll again. But if we get cumulative sum $N^2-6\leq s<N^2$ , do we also choose to roll gain? For example, $s=N^2-1$ , how do I know if $E_s=max(s,\frac{1}{6}\sum^{i=6}_{i=1}E_{s+i})=s>\frac{1}{6}\sum^{i=6}_{i=1}E_{s+i}?$","This question already has an answer here : Toss a fair die until the cumulative sum is a perfect square-Expected Value (1 answer) Closed 3 years ago . The rule of the game is as follows: The player rolls a fair six-sided dice repetitively until the game is over. After each roll, if the cumulative sum of all the rolls so far is a square number (1, 4, 9...), then the game is over, and the player gets nothing. If the cumulative sum of all the rolls is NOT a square number, the player can choose to either stop rolling and get paid the current cumulative sum amount of dollars; or she can choose to continue the game and roll again. For example, let's say Alice plays this game and rolls a ""2"". It's not a square number. Alice chooses to continue the game. She rolls a ""6"" this time. Now the cumulative sum is 8. She can choose to keep rolling, or she can end the game and keep 8$ as her winning. Let's say she chooses to roll again and rolls a ""1"", then the cumulative sum becomes 9, a square number. The game ends and she gets no money. My question is: what is the optimal strategy for this game, and what is the expectation of winnings if you play the optimal strategy? Or in other words, if you charge a ticket price to play this game, it should be at least how much? My thoughts: let denote your expected payoff if the current cumulative sum is . if where The ticket price would be And we have etc. To me this looks like a dynamic programming problem, but I cannot find a termination point. At what cumulative sum should I stop rolling? Let's say is a large integer, then for any cumulative sum where , we would absolutely choose to roll again. But if we get cumulative sum , do we also choose to roll gain? For example, , how do I know if","E_k k E_k=0   k=n^2 n=1,2,... E_0=\frac{1}{6}(E_1+E_2+E_3+E_4+E_5+E_6)=\frac{1}{6}(E_2+E_3+E_5+E_6) E_2=max(2,\frac{1}{6}\sum^{i=6}_{i=1}E_{2+i}), E_3=max(3,\frac{1}{6}\sum^{i=6}_{i=1}E_{3+i}),... N s (N-1)^2<s<N^2-6 N^2-6\leq s<N^2 s=N^2-1 E_s=max(s,\frac{1}{6}\sum^{i=6}_{i=1}E_{s+i})=s>\frac{1}{6}\sum^{i=6}_{i=1}E_{s+i}?","['probability', 'expected-value', 'game-theory', 'dice', 'dynamic-programming']"
77,Size of the largest disk not intersecting N random points on the 2D torus,Size of the largest disk not intersecting N random points on the 2D torus,,"Consider $N$ points generated uniformly at random on the 2d torus (ie. unit square after identifying right/left and up/down). I am interested in estimating the size (eg. diameter) of the largest disk that does not intersect any of these N points. Even estimating the rate of decay of the expected diameter would be a good start! In 1D (ie. a circle), this is quite well-known and has been discussed several times on this form. The 2D case seems more complicated. PS: the torus is the simplest 2D object I could think of (ie. no boundary effects). If this makes things easier to work on the unit square, I would also be very interested!","Consider points generated uniformly at random on the 2d torus (ie. unit square after identifying right/left and up/down). I am interested in estimating the size (eg. diameter) of the largest disk that does not intersect any of these N points. Even estimating the rate of decay of the expected diameter would be a good start! In 1D (ie. a circle), this is quite well-known and has been discussed several times on this form. The 2D case seems more complicated. PS: the torus is the simplest 2D object I could think of (ie. no boundary effects). If this makes things easier to work on the unit square, I would also be very interested!",N,"['probability', 'geometry', 'geometric-probability']"
78,Supremum of the integral over the compact and convex set belongs the extreme set,Supremum of the integral over the compact and convex set belongs the extreme set,,"Let $T:X \to X$ be a continuous function on a compact metric space. We say that $\mu$ is $T-$ invariant if $\mu(T^{-1}(A))=\mu(A)$ for all $A \in \mathbb{B}_{X}$ . We denote by $M(X,T)$ the space of all $T-$ invariant measure which is a nonempty, compact and convex in weak* topology. Let $f:X \to \mathbb{R}$ be a continuous function. Denote $G(f)=\sup_{\mu \in M(X,T)} \int f d\mu$ . $\textbf{Question}:$ Why the supremum is always attained by a measure that belongs to the extreme set $M(X,T)$ ? My attempt: I think that is related to the fact $M(X,T)$ is compact and convex in weak* topology, but I don't know why the measure must belong the extreme set $M(X,T)$ ?","Let be a continuous function on a compact metric space. We say that is invariant if for all . We denote by the space of all invariant measure which is a nonempty, compact and convex in weak* topology. Let be a continuous function. Denote . Why the supremum is always attained by a measure that belongs to the extreme set ? My attempt: I think that is related to the fact is compact and convex in weak* topology, but I don't know why the measure must belong the extreme set ?","T:X \to X \mu T- \mu(T^{-1}(A))=\mu(A) A \in \mathbb{B}_{X} M(X,T) T- f:X \to \mathbb{R} G(f)=\sup_{\mu \in M(X,T)} \int f d\mu \textbf{Question}: M(X,T) M(X,T) M(X,T)","['probability', 'functional-analysis', 'measure-theory', 'dynamical-systems', 'ergodic-theory']"
79,"""3 faces coin"" with biased interpretation","""3 faces coin"" with biased interpretation",,"Say we have a ""3 faces coin"" that will be flipped infinitely many times. Face one is Heads (H) that comes up with probability $(1-m) p$ , Face two is Tails (T) that comes up with probability $(1-m)(1-p)$ , and Face three is Empty (E) that comes up with probability $m$ . Both $m$ and $p$ are between 0 and 1. Say the coin flipper never misreads H and T. However, whenever they face E, they will interpret it, with probability $g \geq 0.5$ , as evidence in favor of the face that came up more often until that point. (i.e. it will randomly transform the observed E into H or T, depending on the observed relative frequency of T and H). Particular example: if, at any point in time, the agent has observed 5 realizations in favor of H and 3 in favor of T, then if they face E, they will interpret as H with probability $g$ or as T with probability $1-g$ . Once the interpretation is stored, it will also influence the way the future E realizations will be interpreted. QUESTION IS: what is the probability that the number of H realizations/interpretations is greater than (less than) the number of number of T realizations/interpretations after infinitely many flips? (i.e., I am looking for an analytical/closed-form solution for such probability and that does depend on the parameters $m,p,g$ . Obs. 1: For simplicity, if the number of H's and T's are the same, one could use a tie break that favors H. I conjecture this is not wlog, but it makes things easier. Obs. 2: This problem seems to be similar to the Gambler's ruin, Polya's urn or some other branching process. But I haven't been able to solve it properly (not a mathematician here). thx!","Say we have a ""3 faces coin"" that will be flipped infinitely many times. Face one is Heads (H) that comes up with probability , Face two is Tails (T) that comes up with probability , and Face three is Empty (E) that comes up with probability . Both and are between 0 and 1. Say the coin flipper never misreads H and T. However, whenever they face E, they will interpret it, with probability , as evidence in favor of the face that came up more often until that point. (i.e. it will randomly transform the observed E into H or T, depending on the observed relative frequency of T and H). Particular example: if, at any point in time, the agent has observed 5 realizations in favor of H and 3 in favor of T, then if they face E, they will interpret as H with probability or as T with probability . Once the interpretation is stored, it will also influence the way the future E realizations will be interpreted. QUESTION IS: what is the probability that the number of H realizations/interpretations is greater than (less than) the number of number of T realizations/interpretations after infinitely many flips? (i.e., I am looking for an analytical/closed-form solution for such probability and that does depend on the parameters . Obs. 1: For simplicity, if the number of H's and T's are the same, one could use a tie break that favors H. I conjecture this is not wlog, but it makes things easier. Obs. 2: This problem seems to be similar to the Gambler's ruin, Polya's urn or some other branching process. But I haven't been able to solve it properly (not a mathematician here). thx!","(1-m) p (1-m)(1-p) m m p g \geq 0.5 g 1-g m,p,g","['probability', 'combinatorics', 'asymptotics', 'martingales', 'branching-rules']"
80,Intuition: expected value formula of a continuous random variable and Riemann sums,Intuition: expected value formula of a continuous random variable and Riemann sums,,"In my introductory probability class the expected value was given just by definition as $$E[X] = \int_{-\infty}^{+\infty} xf(x) dx$$ with $f(x)$ representing the probability density function of the distribution. But I was thinking about a way to correlate the idea in the discrete case with the one in the continuous case as follows (consider that I haven't studied measure theory yet) : Form a partition of a interval $[a,b]$ so that $a=x_0 < x_1 < ... < x_n = b$ then we write the Riemann sum $$\sum_{i=0}^{n-1} (x_{i+1}-x_{i}) x^* f(x^*)$$ with every $x^* \in [x_{i},x_{i+1}]$ . But now since $(x_{i+1}-x_{i}) f(x^*) \approx P(x_i<x<x_{i+1})$ and this approximation will improve more the points are close, we have something like the discrete case because $$\sum_{i=0}^{n-1} (x_{i+1}-x_{i}) x^* f(x^*) \approx \sum_{i=0}^{n-1} x^*P(x_i<x<x_{i+1}) $$ More the points of the partition increase more a single point $x^*$ becomes the ''right'' value to multiply with $P(x_i<x<x_{i+1}) $ to obtain the expected value , and it's known that the left hand side becomes $\int_{a}^{b} xf(x) dx$ , then letting $a \rightarrow -\infty$ and $b \rightarrow +\infty$ we are done. I think the main problem with this reasoning is the interpretation of $x^*$ to become eventually the ''right'' value. Do you consider this reasoning a valid intuition for the expected value?Thanks in advance","In my introductory probability class the expected value was given just by definition as with representing the probability density function of the distribution. But I was thinking about a way to correlate the idea in the discrete case with the one in the continuous case as follows (consider that I haven't studied measure theory yet) : Form a partition of a interval so that then we write the Riemann sum with every . But now since and this approximation will improve more the points are close, we have something like the discrete case because More the points of the partition increase more a single point becomes the ''right'' value to multiply with to obtain the expected value , and it's known that the left hand side becomes , then letting and we are done. I think the main problem with this reasoning is the interpretation of to become eventually the ''right'' value. Do you consider this reasoning a valid intuition for the expected value?Thanks in advance","E[X] = \int_{-\infty}^{+\infty} xf(x) dx f(x) [a,b] a=x_0 < x_1 < ... < x_n = b \sum_{i=0}^{n-1} (x_{i+1}-x_{i}) x^* f(x^*) x^* \in [x_{i},x_{i+1}] (x_{i+1}-x_{i}) f(x^*) \approx P(x_i<x<x_{i+1}) \sum_{i=0}^{n-1} (x_{i+1}-x_{i}) x^* f(x^*) \approx \sum_{i=0}^{n-1} x^*P(x_i<x<x_{i+1})  x^* P(x_i<x<x_{i+1})  \int_{a}^{b} xf(x) dx a \rightarrow -\infty b \rightarrow +\infty x^*","['probability', 'probability-theory', 'expected-value', 'intuition']"
81,What happens with the convex hull of $6$ random points on a sphere?,What happens with the convex hull of  random points on a sphere?,6,"Given a collection of points on the sphere, we can consider their spherical convex hull: add all points on the shortest path between two points in the set, repeat until the resulting set does not expand. (For specificity, say that the shortest path between two diametrically opposite points is the whole sphere, but it doesn't matter for our purposes.) I am interested in the distribution of outcomes from taking the convex hull of $n$ random points on the sphere. When $n=3$ , the result is a triangle with probability 1. When $n=4$ , the result is a triangle with probability $\frac12$ , a quadrilateral with probability $\frac38$ , and the whole sphere with probability $\frac18$ . When $n=5$ , the result is a triangle with probability $\frac5{16}$ , a quadrilateral with probability $\frac5{16}$ , a pentagon with probability $\frac1{16}$ , and the whole sphere with probability $\frac5{16}$ . The above results can be derived given the moments of a random triangle's area and some cute counting arguments; see this question and answer for an instance of the sort of reasoning that produces these conclusions. Happy to elaborate on the proofs of these results in the comments if desired. However, the cute geometric arguments for $n\le 5$ start falling apart at $n=6$ . The probability that the convex hull of $6$ points will be a triangle is $\frac{15}{32}-\frac{15\ln(2)}{4\pi^2}\approx0.2054$ ; again, see the linked thread above for an explanation of why this is (basically, the issue is that the third moment of the area of a random spherical triangle gets messy in a way that earlier moments do not). On the other hand, the odds that the convex hull of $6$ random points is the entire sphere is exactly $\frac12$ ; see this answer for a citation and general formula. What are the chances that the convex hull is a quadrilateral, pentagon, or hexagon? Given the above results, it seems possible that one of these three probabilities may yet be rational (and thus possibly amenable to a natural geometric argument), even though of course not all three can be. Empirically, in a Monte Carlo simulation with $100000$ trials, there were $20369$ triangles, $22127$ quadrilaterals, $6880$ pentagons, $591$ hexagons, and $50033$ spheres.","Given a collection of points on the sphere, we can consider their spherical convex hull: add all points on the shortest path between two points in the set, repeat until the resulting set does not expand. (For specificity, say that the shortest path between two diametrically opposite points is the whole sphere, but it doesn't matter for our purposes.) I am interested in the distribution of outcomes from taking the convex hull of random points on the sphere. When , the result is a triangle with probability 1. When , the result is a triangle with probability , a quadrilateral with probability , and the whole sphere with probability . When , the result is a triangle with probability , a quadrilateral with probability , a pentagon with probability , and the whole sphere with probability . The above results can be derived given the moments of a random triangle's area and some cute counting arguments; see this question and answer for an instance of the sort of reasoning that produces these conclusions. Happy to elaborate on the proofs of these results in the comments if desired. However, the cute geometric arguments for start falling apart at . The probability that the convex hull of points will be a triangle is ; again, see the linked thread above for an explanation of why this is (basically, the issue is that the third moment of the area of a random spherical triangle gets messy in a way that earlier moments do not). On the other hand, the odds that the convex hull of random points is the entire sphere is exactly ; see this answer for a citation and general formula. What are the chances that the convex hull is a quadrilateral, pentagon, or hexagon? Given the above results, it seems possible that one of these three probabilities may yet be rational (and thus possibly amenable to a natural geometric argument), even though of course not all three can be. Empirically, in a Monte Carlo simulation with trials, there were triangles, quadrilaterals, pentagons, hexagons, and spheres.",n n=3 n=4 \frac12 \frac38 \frac18 n=5 \frac5{16} \frac5{16} \frac1{16} \frac5{16} n\le 5 n=6 6 \frac{15}{32}-\frac{15\ln(2)}{4\pi^2}\approx0.2054 6 \frac12 100000 20369 22127 6880 591 50033,"['probability', 'geometry', 'spherical-geometry', 'spherical-trigonometry']"
82,"Conditional probability, expectation and the Borel-Kolmogorov paradox","Conditional probability, expectation and the Borel-Kolmogorov paradox",,"I've been studying a course in probability theory, and I'm getting tied up with the formal treatment of conditional probability. Breiman explains quite well that the usual definition, $$ \mathbb{P}(A \;|\; B) := \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} $$ only makes sense if $\mathbb{P}(B)>0$ , and sometimes we want to condition on sets of measure zero, eg. $\mathbb{P}(A \;|\; X=x)$ over some continuous state space. So far, so good. In order to deal with this problem, we instead use an (in my opinion) long-winded definition that comes from Radon-Nikodym derivatives, and defines the conditional probability in terms of the conditional expectation , and leads to the problem of regular conditional probabilities which may or may not exist and are only almost surely unique. In my opinion it's a total mess, but it's worth it if it means you can get a rigorous treatment. But then I discovered the Borel-Kolmogorov paradox , which shows us that conditioning on a set of measure zero isn't even well-defined , and to the best of my knowledge, Breiman's treatment (which I understand was devised by Kolmogorov and is considered the standard) doesn't help here. So my question is twofold: Does the formal approach actually solve the paradox? I expect the answer here is no, but I'd be happy to be proven wrong. If not, what is the point of the formal treatment if it doesn't actually allow us to do the one thing it's set up to do -- namely, to condition on events with probability zero?","I've been studying a course in probability theory, and I'm getting tied up with the formal treatment of conditional probability. Breiman explains quite well that the usual definition, only makes sense if , and sometimes we want to condition on sets of measure zero, eg. over some continuous state space. So far, so good. In order to deal with this problem, we instead use an (in my opinion) long-winded definition that comes from Radon-Nikodym derivatives, and defines the conditional probability in terms of the conditional expectation , and leads to the problem of regular conditional probabilities which may or may not exist and are only almost surely unique. In my opinion it's a total mess, but it's worth it if it means you can get a rigorous treatment. But then I discovered the Borel-Kolmogorov paradox , which shows us that conditioning on a set of measure zero isn't even well-defined , and to the best of my knowledge, Breiman's treatment (which I understand was devised by Kolmogorov and is considered the standard) doesn't help here. So my question is twofold: Does the formal approach actually solve the paradox? I expect the answer here is no, but I'd be happy to be proven wrong. If not, what is the point of the formal treatment if it doesn't actually allow us to do the one thing it's set up to do -- namely, to condition on events with probability zero?", \mathbb{P}(A \;|\; B) := \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}  \mathbb{P}(B)>0 \mathbb{P}(A \;|\; X=x),"['probability', 'probability-theory', 'stochastic-processes', 'conditional-probability', 'conditional-expectation']"
83,What is the analogy to logic when denoting independence of random variables as $p\models X\perp Y$?,What is the analogy to logic when denoting independence of random variables as ?,p\models X\perp Y,"I'm reading Nir Friedman and Daphne Koller's ""Probabilistic Graphical Models: Principles and Techniques"". The authors occasionally use the notation $$p\models X\perp Y \mid Z$$ to indicate that the set of random variables $X$ is independent of $Y$ given $Z$ , under $p$ (which is a joint distribution over the values of all of these variables). I've never studied mathematical logic, but I read a little about models, semantic and syntactic consequences and am still unclear as to the precise meaning of the symbol $\models$ in the expression above. How is this analogous to formal logic systems? Are the conditional independence assertions analogous to formulae? Axioms?","I'm reading Nir Friedman and Daphne Koller's ""Probabilistic Graphical Models: Principles and Techniques"". The authors occasionally use the notation to indicate that the set of random variables is independent of given , under (which is a joint distribution over the values of all of these variables). I've never studied mathematical logic, but I read a little about models, semantic and syntactic consequences and am still unclear as to the precise meaning of the symbol in the expression above. How is this analogous to formal logic systems? Are the conditional independence assertions analogous to formulae? Axioms?",p\models X\perp Y \mid Z X Y Z p \models,"['probability', 'logic', 'independence', 'bayesian-network']"
84,"Understanding the difference between ""different iid random variables"" and ""different instance of same random variable""","Understanding the difference between ""different iid random variables"" and ""different instance of same random variable""",,"In the derivation of unbiased sample variance , it is considered that $X_i$ are iid random variables while $X_i$ actually represents a sample from a population. So my question is that shouldn't we consider $X_i$ to be an instance of same random variable whose probability distribution is probability of selecting $X_i$ ? For example, consider I have population of people with different heights and I select a sample of people from this population. This sampling of people can be thought of a repetitive procedure of sampling a value from a random variable i.e uniform r.v. Isn't it so? Also I have generally seen it that when we consider samples of a stochastic process, we model each sample as i.i.d instead of repetitive sampling (taking multiple values) from the stochastic process. For example in the definition of strict sense stochastic process we say that the joint distribution of different samples of $X_t$ will be same whether we sample it at any time. Here my question will be rephrased as how can we have joint distribution of constant numbers? Like if I have a series of dice-face-numbers then there is no meaning of considering joint probability distribution of these numbers?","In the derivation of unbiased sample variance , it is considered that are iid random variables while actually represents a sample from a population. So my question is that shouldn't we consider to be an instance of same random variable whose probability distribution is probability of selecting ? For example, consider I have population of people with different heights and I select a sample of people from this population. This sampling of people can be thought of a repetitive procedure of sampling a value from a random variable i.e uniform r.v. Isn't it so? Also I have generally seen it that when we consider samples of a stochastic process, we model each sample as i.i.d instead of repetitive sampling (taking multiple values) from the stochastic process. For example in the definition of strict sense stochastic process we say that the joint distribution of different samples of will be same whether we sample it at any time. Here my question will be rephrased as how can we have joint distribution of constant numbers? Like if I have a series of dice-face-numbers then there is no meaning of considering joint probability distribution of these numbers?",X_i X_i X_i X_i X_t,"['probability', 'probability-theory', 'random-variables', 'conditional-probability', 'independence']"
85,Random signs inequality,Random signs inequality,,"Let $X_1, X_2, \dots, X_n, \dots$ be Rademacher random variables (random signs), i.e. with the distribution $$X_i \sim  \left\{   \begin{array}{@{}ll@{}}     \ \ \ 1 & \text{with probability} \ \ \ \frac{1}{2},\\     -1 & \text{with probability} \ \ \ \frac{1}{2}.   \end{array}\right.$$ Now, let us fix $y\in\ell^2$ - meaning that $y=(y_i)_{i=1}^{\infty}$ and $\sum_{i=1}^{\infty}y_i^2<\infty$ . How to prove that $$\Bigg(\mathbb{E}\Bigg|\sum_{i=1}^{\infty} y_iX_i\Bigg|^{p}\Bigg)^{\frac{1}{p}} \ \le \ c\cdot\sqrt{p}\cdot\Bigg(\sum_{i=1}^{\infty}y_i^2\Bigg)^{\frac{1}{2}}?$$ Here $p>1$ , and $c$ is a universal constant. I have found it in a paper being called a ""standard inequality"", but I am stuck on how to obtain this $\sqrt{p}$ factor. I will be glad for any help or insight.","Let be Rademacher random variables (random signs), i.e. with the distribution Now, let us fix - meaning that and . How to prove that Here , and is a universal constant. I have found it in a paper being called a ""standard inequality"", but I am stuck on how to obtain this factor. I will be glad for any help or insight.","X_1, X_2, \dots, X_n, \dots X_i \sim  \left\{
  \begin{array}{@{}ll@{}}
    \ \ \ 1 & \text{with probability} \ \ \ \frac{1}{2},\\
    -1 & \text{with probability} \ \ \ \frac{1}{2}.
  \end{array}\right. y\in\ell^2 y=(y_i)_{i=1}^{\infty} \sum_{i=1}^{\infty}y_i^2<\infty \Bigg(\mathbb{E}\Bigg|\sum_{i=1}^{\infty} y_iX_i\Bigg|^{p}\Bigg)^{\frac{1}{p}} \ \le \ c\cdot\sqrt{p}\cdot\Bigg(\sum_{i=1}^{\infty}y_i^2\Bigg)^{\frac{1}{2}}? p>1 c \sqrt{p}","['probability', 'functional-analysis']"
86,Binomial distribution with weights but constant probability,Binomial distribution with weights but constant probability,,"What is the name of the distribution and/or distribution function of random variable $X$ ? $X=a_1 x_1+a_2 x_2+...+a_n x_n$ $x_1, ..., x_n$ are independent binomial variables (Bernoulli). $x_i=1$ with probability $p$ and $0$ else. $a_1, \dots, a_n$ are given deterministic integers (weights) $\geq 1$ . How do I show that $P(X>\frac{1}{2}\sum a_i)+\frac{1}{2}P(X=\frac{1}{2}\sum a_i) \geq p$ for $p \geq \frac{1}{2}$ ? This is not a Poisson binomial distribution. It is a generalized Poisson binomial but with the nice property that $p$ is constant. This question gives an almost identical problem. In the spirit of the answer to this question I can calculate probabilities as $P(s_k)=(1-p)^n \sum_{i_1,...,i_m |\sum a_{i_k}=s_k} \Bigl( \frac{p}{1-p}\Bigr)^{\sum i_k}$ . However, this does not seem to get me closer to the proof. It might also be possible to cleverly get the desired proof without a complete distribution function.","What is the name of the distribution and/or distribution function of random variable ? are independent binomial variables (Bernoulli). with probability and else. are given deterministic integers (weights) . How do I show that for ? This is not a Poisson binomial distribution. It is a generalized Poisson binomial but with the nice property that is constant. This question gives an almost identical problem. In the spirit of the answer to this question I can calculate probabilities as . However, this does not seem to get me closer to the proof. It might also be possible to cleverly get the desired proof without a complete distribution function.","X X=a_1 x_1+a_2 x_2+...+a_n x_n x_1, ..., x_n x_i=1 p 0 a_1, \dots, a_n \geq 1 P(X>\frac{1}{2}\sum a_i)+\frac{1}{2}P(X=\frac{1}{2}\sum a_i) \geq p p \geq \frac{1}{2} p P(s_k)=(1-p)^n \sum_{i_1,...,i_m |\sum a_{i_k}=s_k} \Bigl( \frac{p}{1-p}\Bigr)^{\sum i_k}","['probability', 'probability-distributions', 'binomial-distribution']"
87,Is there a continuous transition between deterministic and stochastic?,Is there a continuous transition between deterministic and stochastic?,,"This is perhaps a more philosophical question. Hence, I'm looking for philosophical answers but more concrete ones or maybe directions to books or articles about it would also be of help. During my different studies in mathematics it seems like the field has always been separated into two branches: 'stochastic mathematics' and 'deterministic mathematics' (and after this, more branches). 'Stochastic' involves some sort of probability and we always deal with distributions. Deterministic however involves no randomness. Even though it logically seems right to completely separate these, I've wondered if there exists a continuous transition between the two. Just think of pseudo-random systems. Most computer programs dealing with stochastic values is actually not stochastic. Is it theoretically possible to make a pseudo-random process that ultimately becomes random? I don't know much about how to create pseudo-random processes - maybe that would be a point of departure towards an answer? What do you think? Are those two fields meant to be separated or does it make sense to ask my question?","This is perhaps a more philosophical question. Hence, I'm looking for philosophical answers but more concrete ones or maybe directions to books or articles about it would also be of help. During my different studies in mathematics it seems like the field has always been separated into two branches: 'stochastic mathematics' and 'deterministic mathematics' (and after this, more branches). 'Stochastic' involves some sort of probability and we always deal with distributions. Deterministic however involves no randomness. Even though it logically seems right to completely separate these, I've wondered if there exists a continuous transition between the two. Just think of pseudo-random systems. Most computer programs dealing with stochastic values is actually not stochastic. Is it theoretically possible to make a pseudo-random process that ultimately becomes random? I don't know much about how to create pseudo-random processes - maybe that would be a point of departure towards an answer? What do you think? Are those two fields meant to be separated or does it make sense to ask my question?",,"['probability', 'mathematical-modeling']"
88,CDF's of Two Binomial Distributions with the Same Mean and Different Probabilities,CDF's of Two Binomial Distributions with the Same Mean and Different Probabilities,,"Suppose we have two binomial random variables $X_i \sim B(\frac{a}{p_i},p_i)$ and $X_j \sim B(\frac{a}{p_j},p_j)$ , where $a$ is a positive integer, and both $\frac{a}{p_i}$ and $\frac{a}{p_j}$ are integers. Assume $p_i > p_j$ . In particular, we have $E[X_i] = E[X_j] = a$ . Let $F_{X_i} (x)$ and $F_{X_j} (x)$ denote the CDF's of $X_i$ and $X_j$ , respectively. It seems true (with an example in the image below) that $F_{X_i} (x) < F_{X_j} (x)$ for $x < a$ and $F_{X_i} (x) > F_{X_j} (x)$ for $x > a$ . Is there a simple way to prove it (or, is this result somewhat established in literature)?","Suppose we have two binomial random variables and , where is a positive integer, and both and are integers. Assume . In particular, we have . Let and denote the CDF's of and , respectively. It seems true (with an example in the image below) that for and for . Is there a simple way to prove it (or, is this result somewhat established in literature)?","X_i \sim B(\frac{a}{p_i},p_i) X_j \sim B(\frac{a}{p_j},p_j) a \frac{a}{p_i} \frac{a}{p_j} p_i > p_j E[X_i] = E[X_j] = a F_{X_i} (x) F_{X_j} (x) X_i X_j F_{X_i} (x) < F_{X_j} (x) x < a F_{X_i} (x) > F_{X_j} (x) x > a","['probability', 'binomial-distribution']"
89,Entropy of the difference of two random variables,Entropy of the difference of two random variables,,"Let $X$ and $Y$ be two i.i.d (independent and identically distributed) discrete random variables with distribution $P=(p_0, p_1, \ldots, p_{q-1})$ and support $\{0,1,...,q-1\}$ with $q \geq 2$ . Take $$ P_M = \arg\max_{P} H(X-Y) $$ where $H$ is the Shannon entropy function, i.e. $H(X) = \sum_{i=0}^{q-1} -p_i \log p_i$ . For which $P_M$ we achieve the maximum? For $q=2$ it is not difficult to show that the uniform distribution achieve the maximum. Is it true also for $q>2$ ?","Let and be two i.i.d (independent and identically distributed) discrete random variables with distribution and support with . Take where is the Shannon entropy function, i.e. . For which we achieve the maximum? For it is not difficult to show that the uniform distribution achieve the maximum. Is it true also for ?","X Y P=(p_0, p_1, \ldots, p_{q-1}) \{0,1,...,q-1\} q \geq 2 
P_M = \arg\max_{P} H(X-Y)
 H H(X) = \sum_{i=0}^{q-1} -p_i \log p_i P_M q=2 q>2","['probability', 'optimization', 'lagrange-multiplier', 'entropy']"
90,Find $\lim_{m \to \infty} m E \left[ \log\left( \frac{N+\frac{1}{2}}{m+1} \right) \right]$ where $N$ is Poisson random variable with mean $m$,Find  where  is Poisson random variable with mean,\lim_{m \to \infty} m E \left[ \log\left( \frac{N+\frac{1}{2}}{m+1} \right) \right] N m,"Let $N$ be a Poisson random variable with the mean parameter $m$ . We are interested in finding the following limit \begin{align} \lim_{m \to \infty} m E \left[ \log\left( \frac{N+\frac{1}{2}}{m+1} \right) \right].  \end{align} Things that I tried: First, I found an upper bound by using Jensen's inequality \begin{align} \lim_{m \to \infty} m E \left[ \log\left( \frac{N+\frac{1}{2}}{m+1} \right) \right]\le  \lim_{m \to \infty} m  \log\left( \frac{ E \left[ N  \right]+\frac{1}{2}}{m+1} \right)= \lim_{m \to \infty} m  \log\left( \frac{ m+\frac{1}{2}}{m+1} \right)=-\frac{1}{2}.  \end{align} For the lower bound I tried to use the CLT argument from a related question in here : \begin{align} m E \left[ \log\left( \frac{N+\frac{1}{2}}{m+1} \right) \right]= m E \left[ \log\left( \frac{N+\frac{1}{2}}{m+1} \right) | N \ge \frac{m}{k} \right] P[ N \ge \frac{m}{k} ]+ m E \left[ \log\left( \frac{N+\frac{1}{2}}{m+1} \right) | N < \frac{m}{k} \right] P[ N < \frac{m}{k} ] \end{align} for some $k>0$ .  Via the CLT it can be shown that $P[ N < \frac{m}{k} ] \to 0$ for all $0<k <1$ , and we have that \begin{align} \lim_{m\to \infty}m E \left[ \log\left( \frac{N+\frac{1}{2}}{m+1} \right) \right]= \lim_{m\to \infty}m E \left[ \log\left( \frac{N+\frac{1}{2}}{m+1} \right) | N \ge \frac{m}{k} \right] . \end{align} Now if we use the lower bound at this point, we get \begin{align*}  \lim_{m\to \infty}m E \left[ \log\left( \frac{N+\frac{1}{2}}{m+1} \right) | N \ge \frac{m}{k} \right]  \ge   \lim_{m\to \infty}m  \log\left( \frac{ \frac{m}{k}+\frac{1}{2}}{m+1} \right) =-\infty, \end{align*} for all $k \in (0,1)$ .","Let be a Poisson random variable with the mean parameter . We are interested in finding the following limit Things that I tried: First, I found an upper bound by using Jensen's inequality For the lower bound I tried to use the CLT argument from a related question in here : for some .  Via the CLT it can be shown that for all , and we have that Now if we use the lower bound at this point, we get for all .","N m \begin{align}
\lim_{m \to \infty} m E \left[ \log\left( \frac{N+\frac{1}{2}}{m+1} \right) \right]. 
\end{align} \begin{align}
\lim_{m \to \infty} m E \left[ \log\left( \frac{N+\frac{1}{2}}{m+1} \right) \right]\le  \lim_{m \to \infty} m  \log\left( \frac{ E \left[ N  \right]+\frac{1}{2}}{m+1} \right)= \lim_{m \to \infty} m  \log\left( \frac{ m+\frac{1}{2}}{m+1} \right)=-\frac{1}{2}. 
\end{align} \begin{align}
m E \left[ \log\left( \frac{N+\frac{1}{2}}{m+1} \right) \right]= m E \left[ \log\left( \frac{N+\frac{1}{2}}{m+1} \right) | N \ge \frac{m}{k} \right] P[ N \ge \frac{m}{k} ]+ m E \left[ \log\left( \frac{N+\frac{1}{2}}{m+1} \right) | N < \frac{m}{k} \right] P[ N < \frac{m}{k} ]
\end{align} k>0 P[ N < \frac{m}{k} ] \to 0 0<k <1 \begin{align}
\lim_{m\to \infty}m E \left[ \log\left( \frac{N+\frac{1}{2}}{m+1} \right) \right]= \lim_{m\to \infty}m E \left[ \log\left( \frac{N+\frac{1}{2}}{m+1} \right) | N \ge \frac{m}{k} \right] .
\end{align} \begin{align*}
 \lim_{m\to \infty}m E \left[ \log\left( \frac{N+\frac{1}{2}}{m+1} \right) | N \ge \frac{m}{k} \right]  \ge   \lim_{m\to \infty}m  \log\left( \frac{ \frac{m}{k}+\frac{1}{2}}{m+1} \right) =-\infty,
\end{align*} k \in (0,1)","['probability', 'limits', 'probability-theory', 'poisson-distribution']"
91,"If X and Y are continuous random variables, does this imply that they are jointly continuous?","If X and Y are continuous random variables, does this imply that they are jointly continuous?",,"As far as I am aware, if $X$ and $Y$ were jointly continuous, then this implies that $X$ and $Y$ are individually continuous random variables, with densities which can derived from the joint density of $X$ & $Y$ (by computing the marginals). Additionally, I am aware that the converse is not always true. However, I have seen proofs in probability textbooks that state that $X$ and $Y$ are individually continuous, and then proceed to talk about the joint density of $X$ and $Y$ , which implies that they are jointly continuous which I thought wasn't always necessarily true? Is it fair to conclude that if $X$ and $Y$ are individually continuous, then they are always jointly continuous? If so, why?","As far as I am aware, if and were jointly continuous, then this implies that and are individually continuous random variables, with densities which can derived from the joint density of & (by computing the marginals). Additionally, I am aware that the converse is not always true. However, I have seen proofs in probability textbooks that state that and are individually continuous, and then proceed to talk about the joint density of and , which implies that they are jointly continuous which I thought wasn't always necessarily true? Is it fair to conclude that if and are individually continuous, then they are always jointly continuous? If so, why?",X Y X Y X Y X Y X Y X Y,"['probability', 'probability-theory', 'probability-distributions', 'random-variables', 'density-function']"
92,Cholesky decomposition to price asymmetric derivatives - Thank you all for your attention,Cholesky decomposition to price asymmetric derivatives - Thank you all for your attention,,"I am new here. My first post concerns the evaluation of multiasset options in Robust Programming like you see from papers below Robust Option Pricing (page 848, 5.1) Robust Option Pricing (page 31, 6.1) Option Pricing with LP (page 76, 6.2). I write this post because I am trying to understand how the authors of these works arrive to the condition $ \left \| C(\widetilde{R}_1-\check{R}_1) \right \| \leq \Gamma $ where $\Gamma$ is a predefined parameter whose value reflects the degree of risk-tolerance of the investor, but for purposes of question it doesn't matter knowing what it means. $\widetilde{R}_1=\begin{bmatrix} \frac{\tilde{S}_1^1}{S_0^1} & \frac{\tilde{S}_1^2}{S_0^2} & \cdots  & \frac{\tilde{S}_1^M}{S_0^M} \end{bmatrix}^T$ represents the vector of random returns for the first period (i.e. $t=1$ ) given the current price of $m$ -th stock included in the underlying basket of multiasset option, for $m=1,...,M$ . $\check{R}_1=\begin{bmatrix} \frac{\check{S}_1^1}{S_0^1} & \frac{\check{S}_1^2}{S_0^2} & \cdots  & \frac{\check{S}_1^M}{S_0^M} \end{bmatrix}^T$ represents the vector of expected returns for the first period (i.e. $t=1$ ) given the current price of $m$ -th stock included in the underlying basket of multiasset option, for $m=1,...,M$ . $C=\sum^{-1/2}$ is a matrix obtained with Cholesky decomposition. Text say that $C^{T}C=\sum^{-1}$ (see page 76 third paper) where $\sum$ is the variance-covariance matrix of returns. $||x||$ is (I quote from page 31 second link that I inserted) a general norm of a factor, depending on the modeler's preference, it can be $L_1,L_2,L_{\infty}$ or $D$ -norm . Paper below Robust Linear Optimization under General Norms elaborates on the meaning of $||x||$ but I don’t quite understand what it says. At page 513, authors say to consider an uncertainty set for $\widetilde{A}$ with $U=\begin{Bmatrix} \widetilde{A}|\left \| M(vec(\widetilde{A})-vec(\check{A}))\right \|\leq \Delta \end{Bmatrix},\Delta \geq 0$ , and for Proposition 1. of page 512 they say that we have to apply the definition of $||x||_p \doteq (\sum_{j=1}^{n}|x_j|^p)^{\frac{1}{p}}$ , with $M$ a diagonal matrix containing the inverses of the ranges of coefficient variation . This way to model the uncertainty results from the fact that "" Let S be a closed, bounded convex set and consider an uncertainty set in which the uncertain coefficients are allowed to vary in such a way that the deviations from their nominal values fall in a convex set. [...] We consider uncertainty sets that arise from the requirement that the distance (as measured by an arbitrary norm) between uncertain coefficients and their nominal values is bounded. "" ** Fog. ** This is what I've understood, I hope you can help me to make clear my reasoning. Since with a multiasset option we need to consider the correlation between asset prices included in the underlying basket of option, authors sugget (for independence and identically distribution, i.e. to say second order stationarity, that characterize log-returns, which implies the normality for CLT theorem: we are in a Black-Scholes environment) to study correlation between asset returns given the condition $R_t=\frac{S_t}{S_0}$ that binds prices and returns. Clearly correlation should be evaluated between each $m$ -th asset included in the basket (i.e. for each couple of asset on a set of $M$ elements) and for each $t$ -th instant of time. Obviously it's not the case to have such tremendous amount of information, moreover subject to uncertainty, so instead to constructing a random variance-covariance matrix for each $t$ ... [i.e. to say considering, for each $t$ , one by one, all the possible couples of assets and for any $j$ -th possible realization of assets returns (for $j=1,...,n$ and $n$ the sample size) calculating the sum of cross products of deviations of these possibile realizations from their estimations] ...we use the properties of $\sum$ . I know that $\sum$ is square, symmetric and positive definite, i.e. to say not-singular, i.e. to say invertible, so exists a $\sum^{-1}$ . Since $\sum^{-1}$ is also symmetric and positive definite I can factor it with Cholesky, that is an LU decomposition for symmetric and positive definite matrices. So, I should have an upper triangular matrix $C$ with positive elements above the diagonal and a lower triangular normed matrix $C^T$ (i.e. with all ones under under the diagonal). And because the intersection between $t$ lower triangular matrices and $t$ upper triangular matrices gives us $t$ diagonal matrices, we should get $M$ matrix. Right? Now I am confused. A) How do we know that $M$ contains the inverses of the ranges of coefficient variation ? And what does it mean this proposition? B) Authors say that modelling the uncertainty set of returns in that way ensure , regardless of future realization of returns, that the difference with their expectations will be always lower than a not-negative Delta. But why multiply $M$ with that diff? What does it mean, basically, that matrix product? C) In the second paper (page 31), author gives us an example with the $L_1$ -norm. I quote: If $L_1$ is used, $ \left \| C(\widetilde{R}_1-\check{R}_1) \right \| \leq \Gamma $ is equivalent to: $\begin{matrix} \sum_{i=1}^{M}C_{m,i}\cdot \widetilde{R}_1^i-s^m \leq \sum_{i=1}^{M}C_{m,i}\cdot \check{R}_1^i, \forall m=1...,M\\ -\sum_{i=1}^{M}C_{m,i}\cdot \widetilde{R}_1^i-s^m \leq -\sum_{i=1}^{M}C_{m,i}\cdot \check{R}_1^i, \forall m=1...,M \\ \sum_{m=1}^{M}s^m \leq \Gamma \end{matrix}$ Does he only apply the definition of norm? D) Authors say that the choice of a norm is discretional, but what does it involves choice a norm instead another one? What are the implications on the number of constraints of linear programming problem? Thanks in advance just for reading.","I am new here. My first post concerns the evaluation of multiasset options in Robust Programming like you see from papers below Robust Option Pricing (page 848, 5.1) Robust Option Pricing (page 31, 6.1) Option Pricing with LP (page 76, 6.2). I write this post because I am trying to understand how the authors of these works arrive to the condition where is a predefined parameter whose value reflects the degree of risk-tolerance of the investor, but for purposes of question it doesn't matter knowing what it means. represents the vector of random returns for the first period (i.e. ) given the current price of -th stock included in the underlying basket of multiasset option, for . represents the vector of expected returns for the first period (i.e. ) given the current price of -th stock included in the underlying basket of multiasset option, for . is a matrix obtained with Cholesky decomposition. Text say that (see page 76 third paper) where is the variance-covariance matrix of returns. is (I quote from page 31 second link that I inserted) a general norm of a factor, depending on the modeler's preference, it can be or -norm . Paper below Robust Linear Optimization under General Norms elaborates on the meaning of but I don’t quite understand what it says. At page 513, authors say to consider an uncertainty set for with , and for Proposition 1. of page 512 they say that we have to apply the definition of , with a diagonal matrix containing the inverses of the ranges of coefficient variation . This way to model the uncertainty results from the fact that "" Let S be a closed, bounded convex set and consider an uncertainty set in which the uncertain coefficients are allowed to vary in such a way that the deviations from their nominal values fall in a convex set. [...] We consider uncertainty sets that arise from the requirement that the distance (as measured by an arbitrary norm) between uncertain coefficients and their nominal values is bounded. "" ** Fog. ** This is what I've understood, I hope you can help me to make clear my reasoning. Since with a multiasset option we need to consider the correlation between asset prices included in the underlying basket of option, authors sugget (for independence and identically distribution, i.e. to say second order stationarity, that characterize log-returns, which implies the normality for CLT theorem: we are in a Black-Scholes environment) to study correlation between asset returns given the condition that binds prices and returns. Clearly correlation should be evaluated between each -th asset included in the basket (i.e. for each couple of asset on a set of elements) and for each -th instant of time. Obviously it's not the case to have such tremendous amount of information, moreover subject to uncertainty, so instead to constructing a random variance-covariance matrix for each ... [i.e. to say considering, for each , one by one, all the possible couples of assets and for any -th possible realization of assets returns (for and the sample size) calculating the sum of cross products of deviations of these possibile realizations from their estimations] ...we use the properties of . I know that is square, symmetric and positive definite, i.e. to say not-singular, i.e. to say invertible, so exists a . Since is also symmetric and positive definite I can factor it with Cholesky, that is an LU decomposition for symmetric and positive definite matrices. So, I should have an upper triangular matrix with positive elements above the diagonal and a lower triangular normed matrix (i.e. with all ones under under the diagonal). And because the intersection between lower triangular matrices and upper triangular matrices gives us diagonal matrices, we should get matrix. Right? Now I am confused. A) How do we know that contains the inverses of the ranges of coefficient variation ? And what does it mean this proposition? B) Authors say that modelling the uncertainty set of returns in that way ensure , regardless of future realization of returns, that the difference with their expectations will be always lower than a not-negative Delta. But why multiply with that diff? What does it mean, basically, that matrix product? C) In the second paper (page 31), author gives us an example with the -norm. I quote: If is used, is equivalent to: Does he only apply the definition of norm? D) Authors say that the choice of a norm is discretional, but what does it involves choice a norm instead another one? What are the implications on the number of constraints of linear programming problem? Thanks in advance just for reading."," \left \| C(\widetilde{R}_1-\check{R}_1) \right \| \leq \Gamma  \Gamma \widetilde{R}_1=\begin{bmatrix}
\frac{\tilde{S}_1^1}{S_0^1} & \frac{\tilde{S}_1^2}{S_0^2} & \cdots  & \frac{\tilde{S}_1^M}{S_0^M}
\end{bmatrix}^T t=1 m m=1,...,M \check{R}_1=\begin{bmatrix}
\frac{\check{S}_1^1}{S_0^1} & \frac{\check{S}_1^2}{S_0^2} & \cdots  & \frac{\check{S}_1^M}{S_0^M}
\end{bmatrix}^T t=1 m m=1,...,M C=\sum^{-1/2} C^{T}C=\sum^{-1} \sum ||x|| L_1,L_2,L_{\infty} D ||x|| \widetilde{A} U=\begin{Bmatrix}
\widetilde{A}|\left \| M(vec(\widetilde{A})-vec(\check{A}))\right \|\leq \Delta
\end{Bmatrix},\Delta \geq 0 ||x||_p \doteq (\sum_{j=1}^{n}|x_j|^p)^{\frac{1}{p}} M R_t=\frac{S_t}{S_0} m M t t t j j=1,...,n n \sum \sum \sum^{-1} \sum^{-1} C C^T t t t M M M L_1 L_1  \left \| C(\widetilde{R}_1-\check{R}_1) \right \| \leq \Gamma  \begin{matrix}
\sum_{i=1}^{M}C_{m,i}\cdot \widetilde{R}_1^i-s^m \leq \sum_{i=1}^{M}C_{m,i}\cdot \check{R}_1^i, \forall m=1...,M\\ -\sum_{i=1}^{M}C_{m,i}\cdot \widetilde{R}_1^i-s^m \leq -\sum_{i=1}^{M}C_{m,i}\cdot \check{R}_1^i, \forall m=1...,M
\\ \sum_{m=1}^{M}s^m \leq \Gamma
\end{matrix}","['linear-algebra', 'probability', 'matrices', 'optimization', 'normed-spaces']"
93,"If $\lim_{n\to\infty}P(A_n)=1$, there exists a subsequence $\{A_{n_k}\}_{k=1}^\infty$ with $P\left(\bigcap_{k=1}^\infty A_{n_k}\right)>0$","If , there exists a subsequence  with",\lim_{n\to\infty}P(A_n)=1 \{A_{n_k}\}_{k=1}^\infty P\left(\bigcap_{k=1}^\infty A_{n_k}\right)>0,"Problem: Let $(\Omega,\mathcal F,P)$ be a probability space. Given a sequence of events $\{A_n\}_{n=1}^\infty\subset\mathcal F$ with $\lim\limits_{n\to\infty}P(A_n)=1$ , there exists a subsequence $\{A_{n_k}\}_{k=1}^\infty$ such that $$P\left(\bigcap_{k=1}^\infty A_{n_k}\right)>0.$$ My Proof: Choose a sequence $\{\varepsilon_j\}_{j=1}^\infty$ with $\varepsilon_j=4^{-j-1}>0$ for all $j\in\mathbb N$ . Then $\sum_{j=1}^\infty\varepsilon_j=12^{-1}<1$ by the geometric series formula. Next, since $P(A_n)\to1$ as $n\to\infty$ , we can choose a subsequence $\{A_{n_j}\}_{j=1}^\infty$ such that $P(A_{n_j})>1-\varepsilon_j$ for all $j\in\mathbb N$ . Then $$P\left(\bigcap_{j=1}^\infty A_{n_j}\right)=1-P\left(\bigcup_{j=1}^\infty A_{n_j}^\complement\right)\geq1-\sum_{j=1}^\infty P\left(A_{n_j}^\complement\right)>1-\sum_{j=1}^\infty \varepsilon_j=\frac{11}{12}>0.$$ Do you agree with my proof above? Thank you for your time.","Problem: Let be a probability space. Given a sequence of events with , there exists a subsequence such that My Proof: Choose a sequence with for all . Then by the geometric series formula. Next, since as , we can choose a subsequence such that for all . Then Do you agree with my proof above? Thank you for your time.","(\Omega,\mathcal F,P) \{A_n\}_{n=1}^\infty\subset\mathcal F \lim\limits_{n\to\infty}P(A_n)=1 \{A_{n_k}\}_{k=1}^\infty P\left(\bigcap_{k=1}^\infty A_{n_k}\right)>0. \{\varepsilon_j\}_{j=1}^\infty \varepsilon_j=4^{-j-1}>0 j\in\mathbb N \sum_{j=1}^\infty\varepsilon_j=12^{-1}<1 P(A_n)\to1 n\to\infty \{A_{n_j}\}_{j=1}^\infty P(A_{n_j})>1-\varepsilon_j j\in\mathbb N P\left(\bigcap_{j=1}^\infty A_{n_j}\right)=1-P\left(\bigcup_{j=1}^\infty A_{n_j}^\complement\right)\geq1-\sum_{j=1}^\infty P\left(A_{n_j}^\complement\right)>1-\sum_{j=1}^\infty \varepsilon_j=\frac{11}{12}>0.","['probability', 'probability-theory', 'solution-verification']"
94,LOTUS for non-continuous funtion,LOTUS for non-continuous funtion,,"Let $f_n:[a,b]\to\mathbb{R}$ be a sequence of continuous and uniformly bounded functions. Suppose, $f_n(x)\to f(x)$ pointwise for all $x\in [a,b]$ . Now, let us take $X\sim \text{Unif}[a,b]$ . Then, as the $f_n$ 's are continuous, so by the Law of the Unconscious Statistician (LOTUS) , we can say that $$E[f_n(X)]=\frac{1}{b-a}\int_a^b f_n(u)~du$$ Also, due to the uniformly bounded criterion, by DCT , it can be easily shown that $E[f_n(X)]\longrightarrow E[f(X)]$ . But the problem is that $f$ is not necessarily a continuous function. So I'm not allowed to use LOTUS on $f$ and write $E[f(X)]$ as a Riemann Integral . Can someone please help to resolve this issue without using any measure theory arguments ? You may assume that $f$ is Riemann integrable. Thanks in advance. P.S. : What I want to show is : $$\int_a^b f_n(t)~dt ~\longrightarrow~\int_a^b f(t)~dt$$ where the integrals are Riemann integrals.","Let be a sequence of continuous and uniformly bounded functions. Suppose, pointwise for all . Now, let us take . Then, as the 's are continuous, so by the Law of the Unconscious Statistician (LOTUS) , we can say that Also, due to the uniformly bounded criterion, by DCT , it can be easily shown that . But the problem is that is not necessarily a continuous function. So I'm not allowed to use LOTUS on and write as a Riemann Integral . Can someone please help to resolve this issue without using any measure theory arguments ? You may assume that is Riemann integrable. Thanks in advance. P.S. : What I want to show is : where the integrals are Riemann integrals.","f_n:[a,b]\to\mathbb{R} f_n(x)\to f(x) x\in [a,b] X\sim \text{Unif}[a,b] f_n E[f_n(X)]=\frac{1}{b-a}\int_a^b f_n(u)~du E[f_n(X)]\longrightarrow E[f(X)] f f E[f(X)] f \int_a^b f_n(t)~dt ~\longrightarrow~\int_a^b f(t)~dt","['real-analysis', 'probability', 'probability-theory', 'expected-value', 'pointwise-convergence']"
95,Treasure box with a bomb and $100 probability game,Treasure box with a bomb and $100 probability game,,"There's a treasure box with \$100. There's a 0.5 probability that the box contains a bomb. The bomb has a probability of exploding on the i-th (for $i \leq 100$ ) day given by a uniform distribution. The bomb will eventually explode if there is a bomb. If it does not explode in the end, you can take all the money inside the treasure box. On the $i$ -th day, how much are you willing to pay to play the game? I am a little confused by the question, but I proceeded with finding the conditional probability that the treasure box doesn't have a bomb given that it doesn't explode the first $i$ days. Let $A$ denote the event that it doesn't have a bomb. Let $D_i$ denote the event that it went the first $i$ days without exploding. We want to derive $P(A | D_i)$ . \begin{align}     P(A|D_i) = \frac{P(D_i|A)P(A)}{P(D_i)} \\     P(A) = 0.5 \\     P(D_i | A) = 1 \\     P(D_i) = P(D_i | A) P(A) + P(D_i|A^c) P(A^c) \\     = 1 * 0.5 + P(D_i|A^c) * 0.5  \end{align} For $P(D_i|A^c)$ , which is given that there's a bomb in the box, what is the probability that the bomb doesn't explode in the first $i$ days. Finding the complement is easier. The complement is the probability that it explodes on any given day in the first i days, which is $i/100$ . So then the complement of this is $1 - i/100$ . So we have \begin{align}     P(D_i) = (2 - i/100) * 0.5 \\     P(A|D_i) = \frac{0.5}{(2 - i/100) * 0.5} \\     = \frac{1}{(2-i/100)} \end{align} This probability gets smaller and smaller as the number of days increases without the box exploding. After finding this, I am not sure how to answer the definitively answer the question of how much I'd pay to play the game on a given day. Because the conditional probability that the box has a bomb decreases with the number of days where the bomb doesn't explode, I would be inclined to say that I'd pay less and less, i.e., the amount I'd pay to play the game decreases as days go by. Is there an actual amount that is optimal to offer to play such a game?","There's a treasure box with \$100. There's a 0.5 probability that the box contains a bomb. The bomb has a probability of exploding on the i-th (for ) day given by a uniform distribution. The bomb will eventually explode if there is a bomb. If it does not explode in the end, you can take all the money inside the treasure box. On the -th day, how much are you willing to pay to play the game? I am a little confused by the question, but I proceeded with finding the conditional probability that the treasure box doesn't have a bomb given that it doesn't explode the first days. Let denote the event that it doesn't have a bomb. Let denote the event that it went the first days without exploding. We want to derive . For , which is given that there's a bomb in the box, what is the probability that the bomb doesn't explode in the first days. Finding the complement is easier. The complement is the probability that it explodes on any given day in the first i days, which is . So then the complement of this is . So we have This probability gets smaller and smaller as the number of days increases without the box exploding. After finding this, I am not sure how to answer the definitively answer the question of how much I'd pay to play the game on a given day. Because the conditional probability that the box has a bomb decreases with the number of days where the bomb doesn't explode, I would be inclined to say that I'd pay less and less, i.e., the amount I'd pay to play the game decreases as days go by. Is there an actual amount that is optimal to offer to play such a game?","i \leq 100 i i A D_i i P(A | D_i) \begin{align}
    P(A|D_i) = \frac{P(D_i|A)P(A)}{P(D_i)} \\
    P(A) = 0.5 \\
    P(D_i | A) = 1 \\
    P(D_i) = P(D_i | A) P(A) + P(D_i|A^c) P(A^c) \\
    = 1 * 0.5 + P(D_i|A^c) * 0.5 
\end{align} P(D_i|A^c) i i/100 1 - i/100 \begin{align}
    P(D_i) = (2 - i/100) * 0.5 \\
    P(A|D_i) = \frac{0.5}{(2 - i/100) * 0.5} \\
    = \frac{1}{(2-i/100)}
\end{align}",['probability']
96,Variance of $\log ( \exp(A) + \exp(B) )$,Variance of,\log ( \exp(A) + \exp(B) ),"If $A, B$ are real valued (not necessarily independent) random variables with finite means and variances, what do we know about the variance of $\log ( \exp(A) + \exp(B) )$ , in terms of the variances of $A, B$ ? In particular, if the latter are small, will the former be small? Here's my best upper bound so far: \begin{eqnarray*} \operatorname{VAR}[ \log ( \exp(A) + \exp(B) ) ] &\leq& \operatorname{VAR}[ A ] + \operatorname{VAR}[ B ]  +  (2 \log 2) \sqrt{\operatorname{VAR}[A] + \operatorname{VAR}[B] } + (\log 2)^2 \end{eqnarray*} With the bound above, it is not enough for $\operatorname{VAR}[A], \operatorname{VAR}[B]$ to be small for $\operatorname{VAR}\left[ \log\left( \exp(A) + \exp(B) \right) \right]$ to be small because of the $(\log 2)^2$ term. If $A$ , $B$ are concentrated each on a single point, then clearly $\operatorname{VAR}[ \log ( \exp(A) + \exp(B) ) ] = 0$ . So the bound can be improved, but how? Also, can the bound be improved if we know that $A$ and $B$ are in fact dependent? Here's how to derive the current bound: \begin{eqnarray} \operatorname{VAR}[ \log ( \exp(A) + \exp(B) ) ] &=& \operatorname{VAR}[ \log( \exp( \max\{A,B\} ) + \exp(\min\{A,B\} )) ] \\ &=& \operatorname{VAR}[ \log( \exp( \max\{A,B\} ) ( 1 + \exp(\min\{A,B\} - \max\{A,B\})]\\ &=& \operatorname{VAR}[ \max\{A,B\} ] \\ & & + \operatorname{VAR}[ \log( 1 + \exp(\min\{A,B\} - \max\{A,B\}))] \\ & & + 2 \operatorname{COV}[ \max\{A,B\} , \log( 1 + \exp(\min\{A,B\} - \max\{A,B\})) ] \end{eqnarray} For the first term, we will use \begin{eqnarray} \operatorname{VAR}[ \max\{A,B\} ] \leq \operatorname{VAR}[ A ] + \operatorname{VAR}[ B ] \end{eqnarray} (see https://stats.stackexchange.com/q/48093 ). Define \begin{eqnarray} C \stackrel{\Delta}{=} \log(1+ \exp(\min\{A,B\} - \max\{A,B\}) ) \end{eqnarray} and note that $0 \leq C \leq \log 2$ . The second term is bounded this way: \begin{eqnarray} \operatorname{VAR}[ C ] &=& E C^2 - (E C)^2 \leq E C^2 \leq (\log 2)^2 \end{eqnarray} Finally, for the third term \begin{eqnarray*} \operatorname{COV}[ \max\{A,B\} , C ] &\leq & \sqrt{ \operatorname{VAR}[\max\{A,B\}] \operatorname{VAR}[C]} \\ &\leq & \sqrt{ (\operatorname{VAR}[A] + \operatorname{VAR}[B] ) (\log 2)^2} \\ &=&(\log 2) \sqrt{\operatorname{VAR}[A] + \operatorname{VAR}[B] }  \end{eqnarray*} Putting everything together, this currently reads: \begin{eqnarray*} \operatorname{VAR}[ \log ( \exp(A) + \exp(B) ) ] &\leq& \operatorname{VAR}[ A ] + \operatorname{VAR}[ B ]  +  (2 \log 2) \sqrt{\operatorname{VAR}[A] + \operatorname{VAR}[B] } + (\log 2)^2 \end{eqnarray*} P.S. The idea works more generally: \begin{eqnarray*} \operatorname{VAR}\left[ \log\left( \sum_{i=1}^n \exp(X_i) \right) \right] \leq \sum_{i=1}^n \operatorname{VAR}[ X_i ] + (2 \log n) \sqrt{\sum_{i=1}^n \operatorname{VAR}[X_i]} + (\log n)^2 \end{eqnarray*} by observing that if $Y_1, \cdots, Y_n$ are ordered versions of $X_1,\cdots,X_n$ , (largest to smallest) \begin{eqnarray*} \sum_{i=1}^n \exp(X_i) &=& \sum_{i=1}^n \exp(Y_i) \\ &=& \exp(Y_1) \left( 1 + \sum_{i=2}^n \exp(Y_i - Y_1) \right) \end{eqnarray*}","If are real valued (not necessarily independent) random variables with finite means and variances, what do we know about the variance of , in terms of the variances of ? In particular, if the latter are small, will the former be small? Here's my best upper bound so far: With the bound above, it is not enough for to be small for to be small because of the term. If , are concentrated each on a single point, then clearly . So the bound can be improved, but how? Also, can the bound be improved if we know that and are in fact dependent? Here's how to derive the current bound: For the first term, we will use (see https://stats.stackexchange.com/q/48093 ). Define and note that . The second term is bounded this way: Finally, for the third term Putting everything together, this currently reads: P.S. The idea works more generally: by observing that if are ordered versions of , (largest to smallest)","A, B \log ( \exp(A) + \exp(B) ) A, B \begin{eqnarray*}
\operatorname{VAR}[ \log ( \exp(A) + \exp(B) ) ] &\leq& \operatorname{VAR}[ A ] + \operatorname{VAR}[ B ]  +  (2 \log 2) \sqrt{\operatorname{VAR}[A] + \operatorname{VAR}[B] } + (\log 2)^2
\end{eqnarray*} \operatorname{VAR}[A], \operatorname{VAR}[B] \operatorname{VAR}\left[ \log\left( \exp(A) + \exp(B) \right) \right] (\log 2)^2 A B \operatorname{VAR}[ \log ( \exp(A) + \exp(B) ) ] = 0 A B \begin{eqnarray}
\operatorname{VAR}[ \log ( \exp(A) + \exp(B) ) ] &=& \operatorname{VAR}[ \log( \exp( \max\{A,B\} ) + \exp(\min\{A,B\} )) ] \\
&=& \operatorname{VAR}[ \log( \exp( \max\{A,B\} ) ( 1 + \exp(\min\{A,B\} - \max\{A,B\})]\\
&=& \operatorname{VAR}[ \max\{A,B\} ] \\
& & + \operatorname{VAR}[ \log( 1 + \exp(\min\{A,B\} - \max\{A,B\}))] \\
& & + 2 \operatorname{COV}[ \max\{A,B\} , \log( 1 + \exp(\min\{A,B\} - \max\{A,B\})) ]
\end{eqnarray} \begin{eqnarray}
\operatorname{VAR}[ \max\{A,B\} ] \leq \operatorname{VAR}[ A ] + \operatorname{VAR}[ B ]
\end{eqnarray} \begin{eqnarray}
C \stackrel{\Delta}{=} \log(1+ \exp(\min\{A,B\} - \max\{A,B\}) )
\end{eqnarray} 0 \leq C \leq \log 2 \begin{eqnarray}
\operatorname{VAR}[ C ] &=& E C^2 - (E C)^2 \leq E C^2 \leq (\log 2)^2
\end{eqnarray} \begin{eqnarray*}
\operatorname{COV}[ \max\{A,B\} , C ] &\leq & \sqrt{ \operatorname{VAR}[\max\{A,B\}] \operatorname{VAR}[C]} \\
&\leq & \sqrt{ (\operatorname{VAR}[A] + \operatorname{VAR}[B] ) (\log 2)^2} \\
&=&(\log 2) \sqrt{\operatorname{VAR}[A] + \operatorname{VAR}[B] } 
\end{eqnarray*} \begin{eqnarray*}
\operatorname{VAR}[ \log ( \exp(A) + \exp(B) ) ] &\leq& \operatorname{VAR}[ A ] + \operatorname{VAR}[ B ]  +  (2 \log 2) \sqrt{\operatorname{VAR}[A] + \operatorname{VAR}[B] } + (\log 2)^2
\end{eqnarray*} \begin{eqnarray*}
\operatorname{VAR}\left[ \log\left( \sum_{i=1}^n \exp(X_i) \right) \right] \leq \sum_{i=1}^n \operatorname{VAR}[ X_i ] + (2 \log n) \sqrt{\sum_{i=1}^n \operatorname{VAR}[X_i]} + (\log n)^2
\end{eqnarray*} Y_1, \cdots, Y_n X_1,\cdots,X_n \begin{eqnarray*}
\sum_{i=1}^n \exp(X_i) &=& \sum_{i=1}^n \exp(Y_i) \\
&=& \exp(Y_1) \left( 1 + \sum_{i=2}^n \exp(Y_i - Y_1) \right)
\end{eqnarray*}","['probability', 'logarithms', 'variance']"
97,A roll-unter-Game with cashback-rate on losses.,A roll-unter-Game with cashback-rate on losses.,,"Im scretching my head about the following roll-under-game: Choose a bet-amount $B$ and a number $N$ between $2$ and $96$ . Then the bank picks a Number $R$ between $1$ and $100$ discretly uniformly distributed. If $N< R$ you won the game and get $w(N)\cdot B$ back, where ... $$w(N)=\frac{\beta}{P(R<N)}=\frac{\beta}{\frac{N-1}{100}}=\frac{100\cdot\beta}{N-1}$$ ... with $P(R<N)=\frac{N-1}{100}$ and $0\leq\beta\leq1$ . On the other hand, if the result is $N\geq R$ you loose a portion $\gamma\cdot B$ of your bet-amount. In the case considered here the following applies: $\beta=0.985$ and $\gamma\in[0,1]$ (loss-ratio) with $\gamma:=1-\delta$ , where $\delta\in[0,1]$ (cashback-rate). So for $\delta=0$ or else $\gamma=1$ we get the classical roll-under-game with no cashback-bonus. I'll give two quick example to illustrate the game: i) [ $\beta=0.985$ , $\gamma=1$ (no chashback)]: If you pick $N=51$ you will win with a probability of $50\%$ and get $1.97\cdot B$ back. In the event of a loss the entire bet-amount is lost. ii) [ $\beta=0.985$ , $\gamma=0.7$ (30% chashback)]: If you pick $N=51$ you will win with a probability of $50\%$ and get $1.97\cdot B$ back. In the event of a loss you will receive $0.3\cdot B$ back. We choose a fixed number $N\in\{2,3,\ldots,96\}$ , a bet-amount $B>0$ and play the game with a seed-capital of $M_0\in\mathbb{R}_+$ multiple times in a row. Set $\omega:=W(N)-1$ . Lets define be a sequence of r.v.'s with $X_n\in\{1,0\}$ such that $X_n=1$ describes a win and $X_n=0$ represents a loss of the $n$ -th game played. Obviously $P(X_n=1)=:p=P(R<N)=\frac{N-1}{100}$ and $P(X_n=0)=:q=1-p$ holds for all $n\in\mathbb{N}$ , therefore $X\sim Ber(p)$ . Because $X_n$ is bernoulli distributed we know that $E[X_n]=p$ . Now we take a look at the earnings in each round. Obviosly they can be described by the sequence of r.v. $Y_n=X_n\cdot\omega B+(X_n-1)\cdot\gamma B$ . The expected value is: $$\begin{align}E[Y_n]&=E[X_n\cdot\omega B+(X_n-1)\cdot\gamma B]\\&=E[X_n]\cdot\omega B+(E[X_n]-1)\cdot\gamma B\\&=p\cdot\omega B+(p-1)\cdot\gamma B=B(p\cdot\omega-(1-p)\cdot\gamma)\\&=B(p\cdot\omega-q\cdot\gamma)\end{align}$$ So it is a fair game with $E[Y_n]=0$ if $\omega=1$ , $\gamma=1$ and $p=0.5$ . We continue to simplify the expression from above with $\delta:=1-\gamma$ (Cashbackrate), $\Delta:=\beta-1$ and $\omega=W(N)-1=\frac{\beta}{p}-1$ . Leading to: $$\begin{align}E[Y_n]&=B\cdot(\omega p-\gamma q)=B\cdot((\frac{\beta}{p}-1)p-\gamma q)\\&=B\cdot(\beta-p-\gamma(1-p))=B\cdot(\beta-p-\gamma+\gamma p)\\&=B\cdot(\beta-p(1-\gamma)-\gamma)=B\cdot(\beta-p\delta-(1-\delta))\\&=B\cdot((\beta-1)+\delta(1-p)=B\cdot((\beta-1)+\delta q)\\&=B\cdot(\Delta+\delta q) \end{align}$$ In the case described at the beginning with $\beta=0.985$ and no cashback (meaning $\gamma=1$ ) and $N=51$ (which gives $p=0.5$ ) we get $E[Y_n]=B\cdot(\Delta+\delta q)=B(-0.015+0\cdot 0.5)=-0.015\cdot B$ . So in a roll-under-game without any cashback on losses we expect a loss of 1.5% of the bet-amount each game. Now let's check how the cashback rate has to be changed in order to expect a profit: $$E[Y_n]\geq 0\Leftrightarrow B\cdot(\Delta+\delta q)\geq0 \Rightarrow (\Delta+\delta q)\geq0 \Leftrightarrow q\geq\frac{-\Delta}{\delta} $$ So we expect a profitable game when $\frac{-\Delta}{\delta}<q$ . For $N=51$ , $\beta=0.985$ and with a cashback-rate of $\delta=5\%,7\%,9\%,11\%,13\%,15\%$ we achieve a expected profit of $1\%,2\%,3\%,4\%,5\%,6\%$ of the bet-amount $B$ each game. On the other hand if we choose a cashback-rate of $\delta=3\%$ we achieve a fair game. Is this calculation correct so far? For the sake of completeness we look at the capital stocks r.v.'s $M_1,M_2,\ldots$ after each round. $M_0$ describes the overall capital at the beginning of the game. It is clear that $M_n=M_0+\sum_{k=1}^n X_n$ applies. For the expected value we get: $$\begin{align}E[M_n]&=E[M_0+\sum_{k=1}^n Y_n]=M_0+\sum_{k=1}^n E[X_n]=M_0+B\sum_{k=1}^n(\Delta+\delta q)\\&=M_0+n\cdot B(\Delta+\delta q)\end{align}$$ Now i try to find the probability that i am runied after $n$ games. For this we define $W_n=\sum_{k=1}X_k$ the number of wins in $n$ trails, which we know is binomial distributed $W_n\sim Bin(n,p)$ . Further we define $L_n=n-W_n$ the number of lost games in $n$ trails. So we are bankrupt if: $$\begin{align}M_0+W_n\omega B-L_n\cdot\gamma B\leq 0  &\Leftrightarrow M_0+W_n\omega B \leq L_n\cdot\gamma B \\ &\Leftrightarrow M_0+W_n\omega B \leq (n-W_n)\cdot\gamma B  \\ &\Leftrightarrow M_0+W_n\omega B \leq n\gamma B-W_n\gamma B \\ &\Leftrightarrow M_0+W_n B(\omega-\gamma) \leq n\gamma B \\&\Leftrightarrow W_n\leq\frac{n\gamma B-M_0}{B(\omega-\gamma)} \end{align}$$ So the probability to get ruined after $n$ games is given by $P(W_n\leq K)$ with $K:=\frac{n\gamma B-M_0}{B(\omega-\gamma)}$ . Because $W_n\sim Bin(n,p)$ we know the cumulativ distribution function $F_{W_n}(x)$ . This gives us: $$P(W_n\leq K)=F_{W_n}(K)=\sum_{k=0}^{\lfloor K \rfloor}\binom{n}{k}p^kq^{n-k}$$ Is this correct? Are there any flaws? Especially the last result seems somehow strange to me. How can I determine the probability that I have a higher capital $M_n$ after $n$ games than at the beginning $M_0$ ? Which means nothing else than, how do I calculate the probability $P(M_n\geq M_0)$ ? Is a approach through random walks better? Basically it is a random walk with different step-sizes. It walks $\gamma B$ to the left side with probability $q=1-p$ and walks $\omega(N) B$ to the right with the probability $p=\frac{N-1}{100}$ . Is there any literature for this special case of a random-walk? Any assistance, thoughts or comments would be much appreciated.","Im scretching my head about the following roll-under-game: Choose a bet-amount and a number between and . Then the bank picks a Number between and discretly uniformly distributed. If you won the game and get back, where ... ... with and . On the other hand, if the result is you loose a portion of your bet-amount. In the case considered here the following applies: and (loss-ratio) with , where (cashback-rate). So for or else we get the classical roll-under-game with no cashback-bonus. I'll give two quick example to illustrate the game: i) [ , (no chashback)]: If you pick you will win with a probability of and get back. In the event of a loss the entire bet-amount is lost. ii) [ , (30% chashback)]: If you pick you will win with a probability of and get back. In the event of a loss you will receive back. We choose a fixed number , a bet-amount and play the game with a seed-capital of multiple times in a row. Set . Lets define be a sequence of r.v.'s with such that describes a win and represents a loss of the -th game played. Obviously and holds for all , therefore . Because is bernoulli distributed we know that . Now we take a look at the earnings in each round. Obviosly they can be described by the sequence of r.v. . The expected value is: So it is a fair game with if , and . We continue to simplify the expression from above with (Cashbackrate), and . Leading to: In the case described at the beginning with and no cashback (meaning ) and (which gives ) we get . So in a roll-under-game without any cashback on losses we expect a loss of 1.5% of the bet-amount each game. Now let's check how the cashback rate has to be changed in order to expect a profit: So we expect a profitable game when . For , and with a cashback-rate of we achieve a expected profit of of the bet-amount each game. On the other hand if we choose a cashback-rate of we achieve a fair game. Is this calculation correct so far? For the sake of completeness we look at the capital stocks r.v.'s after each round. describes the overall capital at the beginning of the game. It is clear that applies. For the expected value we get: Now i try to find the probability that i am runied after games. For this we define the number of wins in trails, which we know is binomial distributed . Further we define the number of lost games in trails. So we are bankrupt if: So the probability to get ruined after games is given by with . Because we know the cumulativ distribution function . This gives us: Is this correct? Are there any flaws? Especially the last result seems somehow strange to me. How can I determine the probability that I have a higher capital after games than at the beginning ? Which means nothing else than, how do I calculate the probability ? Is a approach through random walks better? Basically it is a random walk with different step-sizes. It walks to the left side with probability and walks to the right with the probability . Is there any literature for this special case of a random-walk? Any assistance, thoughts or comments would be much appreciated.","B N 2 96 R 1 100 N< R w(N)\cdot B w(N)=\frac{\beta}{P(R<N)}=\frac{\beta}{\frac{N-1}{100}}=\frac{100\cdot\beta}{N-1} P(R<N)=\frac{N-1}{100} 0\leq\beta\leq1 N\geq R \gamma\cdot B \beta=0.985 \gamma\in[0,1] \gamma:=1-\delta \delta\in[0,1] \delta=0 \gamma=1 \beta=0.985 \gamma=1 N=51 50\% 1.97\cdot B \beta=0.985 \gamma=0.7 N=51 50\% 1.97\cdot B 0.3\cdot B N\in\{2,3,\ldots,96\} B>0 M_0\in\mathbb{R}_+ \omega:=W(N)-1 X_n\in\{1,0\} X_n=1 X_n=0 n P(X_n=1)=:p=P(R<N)=\frac{N-1}{100} P(X_n=0)=:q=1-p n\in\mathbb{N} X\sim Ber(p) X_n E[X_n]=p Y_n=X_n\cdot\omega B+(X_n-1)\cdot\gamma B \begin{align}E[Y_n]&=E[X_n\cdot\omega B+(X_n-1)\cdot\gamma B]\\&=E[X_n]\cdot\omega B+(E[X_n]-1)\cdot\gamma B\\&=p\cdot\omega B+(p-1)\cdot\gamma B=B(p\cdot\omega-(1-p)\cdot\gamma)\\&=B(p\cdot\omega-q\cdot\gamma)\end{align} E[Y_n]=0 \omega=1 \gamma=1 p=0.5 \delta:=1-\gamma \Delta:=\beta-1 \omega=W(N)-1=\frac{\beta}{p}-1 \begin{align}E[Y_n]&=B\cdot(\omega p-\gamma q)=B\cdot((\frac{\beta}{p}-1)p-\gamma q)\\&=B\cdot(\beta-p-\gamma(1-p))=B\cdot(\beta-p-\gamma+\gamma p)\\&=B\cdot(\beta-p(1-\gamma)-\gamma)=B\cdot(\beta-p\delta-(1-\delta))\\&=B\cdot((\beta-1)+\delta(1-p)=B\cdot((\beta-1)+\delta q)\\&=B\cdot(\Delta+\delta q) \end{align} \beta=0.985 \gamma=1 N=51 p=0.5 E[Y_n]=B\cdot(\Delta+\delta q)=B(-0.015+0\cdot 0.5)=-0.015\cdot B E[Y_n]\geq 0\Leftrightarrow B\cdot(\Delta+\delta q)\geq0 \Rightarrow (\Delta+\delta q)\geq0 \Leftrightarrow q\geq\frac{-\Delta}{\delta}  \frac{-\Delta}{\delta}<q N=51 \beta=0.985 \delta=5\%,7\%,9\%,11\%,13\%,15\% 1\%,2\%,3\%,4\%,5\%,6\% B \delta=3\% M_1,M_2,\ldots M_0 M_n=M_0+\sum_{k=1}^n X_n \begin{align}E[M_n]&=E[M_0+\sum_{k=1}^n Y_n]=M_0+\sum_{k=1}^n E[X_n]=M_0+B\sum_{k=1}^n(\Delta+\delta q)\\&=M_0+n\cdot B(\Delta+\delta q)\end{align} n W_n=\sum_{k=1}X_k n W_n\sim Bin(n,p) L_n=n-W_n n \begin{align}M_0+W_n\omega B-L_n\cdot\gamma B\leq 0 
&\Leftrightarrow M_0+W_n\omega B \leq L_n\cdot\gamma B
\\ &\Leftrightarrow M_0+W_n\omega B \leq (n-W_n)\cdot\gamma B 
\\ &\Leftrightarrow M_0+W_n\omega B \leq n\gamma B-W_n\gamma B
\\ &\Leftrightarrow M_0+W_n B(\omega-\gamma) \leq n\gamma B
\\&\Leftrightarrow W_n\leq\frac{n\gamma B-M_0}{B(\omega-\gamma)} \end{align} n P(W_n\leq K) K:=\frac{n\gamma B-M_0}{B(\omega-\gamma)} W_n\sim Bin(n,p) F_{W_n}(x) P(W_n\leq K)=F_{W_n}(K)=\sum_{k=0}^{\lfloor K \rfloor}\binom{n}{k}p^kq^{n-k} M_n n M_0 P(M_n\geq M_0) \gamma B q=1-p \omega(N) B p=\frac{N-1}{100}","['probability', 'probability-theory', 'random-variables', 'expected-value', 'random-walk']"
98,PDF and CDF of the ratio of the max to min of an iid random sample: a quick check of the calculation!,PDF and CDF of the ratio of the max to min of an iid random sample: a quick check of the calculation!,,"My question can be thought as a direct continuation of this important question and its first answer (but see this question also), which gives us the joint distribution of the max and min of an iid random sample. In my question, I just wanted to make sure if this formula for the ratio  of maximum to minimum of an iid random sample is correct, given that first answer to the abovementioned question, which is indeed correct. Proposition: Let $\{N_1 \dots N_n\}$ be iid random sample generated by a positive random variable $N > 0$ with CDF $F_N$ and PDF $f_N.$ Let $U_n, L_n$ denote the maximum and minimum of the random sample. Let us denote the PDF of their joint distribution by $f_{U_n, L_n}$ and  that of the ratio by $f_{\frac{U_n}{L_n}}. $ Then we have the following expression of the PDF of the ratio: $$  f_{\frac{U_n}{L_n}}(z) =  n(n-1)\int_{s=0}^{\infty}sf_N(s)f_N(sz)(F_N(sz)-F_N(s))^{n-2}, z \in [1, \infty) $$ Note that: $\frac{U_n}{L_n} \ge 1.$ So let $ z \ge  1.$ Then: \begin{align*} 	\\& f_{\frac{U_n}{L_n}}(z) 	\\& = \frac{d}{dz} P[\frac{U_n}{L_n} \le z] 	\\&= \frac{d}{dz} P[{U_n} \le z L_n] 	\\&= \frac{d}{dz} \int_{ \{ t \le z s\} } f_{L_n, U_n}(s,t) ds dt 	\\&= \frac{d}{dz} \int_{s=0}^{\infty} \left[  \int_{t=0}^{zs} f_{L_n, U_n}(s,t) ds  \right]  dt 	\\&=  \int_{s=0}^{\infty} \frac{d}{dz} \left[  \int_{t=0}^{zs} f_{L_n, U_n}(s,t)  ds \right]  dt 	\\&=  \int_{s=0}^{\infty} s  f_{L_n, U_n}(s,sz) ds 	\\&= n(n-1) \int_{s=0}^{\infty} s f_N(s)f_N(sz)(F_N(sz)-F_N(s))^{n-2} ds     \\& \text{ (The next step is only for Uniform distributions) }     \\&= n(n-1) \int_{s=0}^{1/z} s f_N(s)f_N(sz)(F_N(sz)-F_N(s))^{n-2} ds (\text{ because when } sz > 1, f_N(sz)=0. ) 	\end{align*} Note that in the second to last line of the above, proof, I indeed used the general ( not for uniform distribution) expression for $f_{L_n, U_n}$ provided by the first answer to the question I mentioned in the very first line. In the last line, we specialized for uniform distributions. EDIT: After a few first comments, I'm including a check for $n=2, N \sim \mathcal{U}(0,1)$ below: Plug in $n=2, $ and assume $N \sim U(0,1),$ then the last line of my calculation becomes $2 \int_{0}^{1/z}sds= 1/z^2,$ which is the PDF of the inverse uniform distribution . Now note that, in this question , they treat the distribution of $L_2/U_2$ and obtain it as a uniform distribution $\mathcal{U}(0,1),$ implying that at least in this simple case, my calculations are correct. Indeed a similar calculation also shows that when $X_i \sim_{i.i.d.} \mathcal{U}(0,1), U_n/L_n$ has PDF $f_{U_n/L_n}(z)= \frac{n-1}{z^n}, z \geq 1.$","My question can be thought as a direct continuation of this important question and its first answer (but see this question also), which gives us the joint distribution of the max and min of an iid random sample. In my question, I just wanted to make sure if this formula for the ratio  of maximum to minimum of an iid random sample is correct, given that first answer to the abovementioned question, which is indeed correct. Proposition: Let be iid random sample generated by a positive random variable with CDF and PDF Let denote the maximum and minimum of the random sample. Let us denote the PDF of their joint distribution by and  that of the ratio by Then we have the following expression of the PDF of the ratio: Note that: So let Then: Note that in the second to last line of the above, proof, I indeed used the general ( not for uniform distribution) expression for provided by the first answer to the question I mentioned in the very first line. In the last line, we specialized for uniform distributions. EDIT: After a few first comments, I'm including a check for below: Plug in and assume then the last line of my calculation becomes which is the PDF of the inverse uniform distribution . Now note that, in this question , they treat the distribution of and obtain it as a uniform distribution implying that at least in this simple case, my calculations are correct. Indeed a similar calculation also shows that when has PDF","\{N_1 \dots N_n\} N > 0 F_N f_N. U_n, L_n f_{U_n, L_n} f_{\frac{U_n}{L_n}}.    f_{\frac{U_n}{L_n}}(z) =  n(n-1)\int_{s=0}^{\infty}sf_N(s)f_N(sz)(F_N(sz)-F_N(s))^{n-2}, z \in [1, \infty)  \frac{U_n}{L_n} \ge 1.  z \ge  1. \begin{align*}
	\\& f_{\frac{U_n}{L_n}}(z)
	\\& = \frac{d}{dz} P[\frac{U_n}{L_n} \le z]
	\\&= \frac{d}{dz} P[{U_n} \le z L_n]
	\\&= \frac{d}{dz} \int_{ \{ t \le z s\} } f_{L_n, U_n}(s,t) ds dt
	\\&= \frac{d}{dz} \int_{s=0}^{\infty} \left[  \int_{t=0}^{zs} f_{L_n, U_n}(s,t) ds  \right]  dt
	\\&=  \int_{s=0}^{\infty} \frac{d}{dz} \left[  \int_{t=0}^{zs} f_{L_n, U_n}(s,t)  ds \right]  dt
	\\&=  \int_{s=0}^{\infty} s  f_{L_n, U_n}(s,sz) ds
	\\&= n(n-1) \int_{s=0}^{\infty} s f_N(s)f_N(sz)(F_N(sz)-F_N(s))^{n-2} ds
    \\& \text{ (The next step is only for Uniform distributions) }
    \\&= n(n-1) \int_{s=0}^{1/z} s f_N(s)f_N(sz)(F_N(sz)-F_N(s))^{n-2} ds (\text{ because when } sz > 1, f_N(sz)=0. )
	\end{align*} f_{L_n, U_n} n=2, N \sim \mathcal{U}(0,1) n=2,  N \sim U(0,1), 2 \int_{0}^{1/z}sds= 1/z^2, L_2/U_2 \mathcal{U}(0,1), X_i \sim_{i.i.d.} \mathcal{U}(0,1), U_n/L_n f_{U_n/L_n}(z)= \frac{n-1}{z^n}, z \geq 1.","['probability', 'probability-distributions', 'density-function']"
99,Concentration Inequality with Random Coefficients,Concentration Inequality with Random Coefficients,,"Suppose $X_1,X_2,\ldots$ are independent and take values in $[0,1]$ . For any coefficients $C_1,C_2, \ldots \in [0,1]$ we have the concentration inequality $$P \left( \Big| \,\sum_{i=1}^N C_iX_i - \sum_{i=1}^N C_i \mathbb E[X_i]\,\Big|> t\right) \le \exp \left(-\frac{t^2}{2\sum_{i=1}^N C_i^2}\right) \le \exp \left(-\frac{t^2}{2N}\right).$$ In particular take $t = \frac{1}{2}\sum_{i=1}^N C_i \mathbb E[X_i]$ to get for example $$P \left(  \sum_{i=1}^N C_iX_i \ge \frac{1}{2}\sum_{i=1}^N C_i \mathbb E[X_i]\right) \le \exp \left(-\frac{\big(\sum_{i=1}^N C_i \mathbb E[X_i]\big)^2}{8N}\right).$$ Provided $C_i,\mathbb E[X_i]$ do not go to zero too quickly the RHS has order $e^{-cN}$ for some $c >0$ . Thus we get a  high probability bound for the sum being too far from its expectation. Now suppose instead of $C_1,C_2,\ldots$ we use random variables $Y_1,Y_2,\ldots$ as coefficients. Under some independence or martingale assumptions it is possible to bound the chance $\sum_{i=1}^N Y_i X_i $ is far from it's expectation. If $X_i,Y_i$ are independent the expectation is $\sum_{i=1}^N \mathbb E[Y_i] \mathbb E[X_i]$ . What I am interested in is subtler. I want to bound the chance $\sum_{i=1}^N Y_i X_i $ is far from $\sum_{i=1}^N   \mathbb E[Y_i] X_i$ . I don't know how to go about this, or what kind of assumptions are needed. Does anyone have any hints? Some things I am happy to assume are that $X_1,X_2,\ldots$ are independent and each $X_i,Y_i$ are independent. I am not happy to assume $Y_1,Y_2,\ldots$ are independent or that $Y_i$ is independent of $X_1,\ldots, X_{i-1}$ . Context: This comes up in the analysis of an algorithm. Each $X_i$ is a prediction for the state of the world on day $i$ and each $Y_i$ is part of the action taken on day $i$ . Hence $Y_i$ is dependent on $X_i$ directly. Since the actions change slowly $Y_i$ is not independent of the past actions. Hence it is also dependent on $X_{i-1},\ldots, X_1$ . The $X_i$ in $X_i Y_i$ measures the performance of the algorithm.","Suppose are independent and take values in . For any coefficients we have the concentration inequality In particular take to get for example Provided do not go to zero too quickly the RHS has order for some . Thus we get a  high probability bound for the sum being too far from its expectation. Now suppose instead of we use random variables as coefficients. Under some independence or martingale assumptions it is possible to bound the chance is far from it's expectation. If are independent the expectation is . What I am interested in is subtler. I want to bound the chance is far from . I don't know how to go about this, or what kind of assumptions are needed. Does anyone have any hints? Some things I am happy to assume are that are independent and each are independent. I am not happy to assume are independent or that is independent of . Context: This comes up in the analysis of an algorithm. Each is a prediction for the state of the world on day and each is part of the action taken on day . Hence is dependent on directly. Since the actions change slowly is not independent of the past actions. Hence it is also dependent on . The in measures the performance of the algorithm.","X_1,X_2,\ldots [0,1] C_1,C_2, \ldots \in [0,1] P \left( \Big| \,\sum_{i=1}^N C_iX_i - \sum_{i=1}^N C_i \mathbb E[X_i]\,\Big|> t\right) \le \exp \left(-\frac{t^2}{2\sum_{i=1}^N C_i^2}\right) \le \exp \left(-\frac{t^2}{2N}\right). t = \frac{1}{2}\sum_{i=1}^N C_i \mathbb E[X_i] P \left(  \sum_{i=1}^N C_iX_i \ge \frac{1}{2}\sum_{i=1}^N C_i \mathbb E[X_i]\right) \le \exp \left(-\frac{\big(\sum_{i=1}^N C_i \mathbb E[X_i]\big)^2}{8N}\right). C_i,\mathbb E[X_i] e^{-cN} c >0 C_1,C_2,\ldots Y_1,Y_2,\ldots \sum_{i=1}^N Y_i X_i  X_i,Y_i \sum_{i=1}^N \mathbb E[Y_i] \mathbb E[X_i] \sum_{i=1}^N Y_i X_i  \sum_{i=1}^N   \mathbb E[Y_i] X_i X_1,X_2,\ldots X_i,Y_i Y_1,Y_2,\ldots Y_i X_1,\ldots, X_{i-1} X_i i Y_i i Y_i X_i Y_i X_{i-1},\ldots, X_1 X_i X_i Y_i","['probability', 'martingales', 'concentration-of-measure']"
