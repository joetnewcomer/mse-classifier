,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Evaluating the limit of a sum using integration,Evaluating the limit of a sum using integration,,"One of the first results we learn in definite integral is that if $f(x)$ is Riemann integrable in $(0,1)$ then we have $\lim_{n \to \infty}\dfrac{1}{n}\sum_{i=1}^{n}f\Big(\dfrac{i}{n}\Big) = \int_{0}^{1}f(x)dx$ . I was playing around with this to see if this can be generalized and I found the following. We can rewrite the above result as $$ \lim_{n \to \infty}\frac{1}{1+1+\ldots\text{$n$-times}}\sum_{i=1}^{n}1\times f\Big(\frac{1+1+\ldots\text{$i$-times}}{1+1+\ldots\text{$n$-times}}\Big) = \int_{0}^{1}f(x)dx. $$ The LHS can be written in the general form given below and we ask ourselves for which sequence $a_i$ does the following hold $$ \lim_{n \to \infty}\frac{1}{a_1 + a_2 + \ldots + a_n}\sum_{i=1}^{n}a_i f\Big(\frac{a_1 + a_2 + \ldots + a_i}{a_1 + a_2 + \ldots + a_n}\Big) =\int_{0}^{1}f(x)dx. $$ Trivially this holds for $a_i = c$ where $c$ is a non-zero constant and the above result is the case when $c=1$ . I also observed that this holds for sequence of natural numbers $a_i = i$ since $$ \lim_{n \to \infty}\frac{2}{n^2+n}\sum_{i=1}^{n}i f\Big(\frac{i^2+i}{n^2+n}\Big) =\int_{0}^{1}f(x)dx. $$ Experimentally, this also holds for the sequence of prime numbers $a_i = p_n$ and also for the sequence of composite numbers $c_n$ . Question : What are the necessary and sufficient conditions on $a_i$ for the above relation to hold? Related question","One of the first results we learn in definite integral is that if is Riemann integrable in then we have . I was playing around with this to see if this can be generalized and I found the following. We can rewrite the above result as The LHS can be written in the general form given below and we ask ourselves for which sequence does the following hold Trivially this holds for where is a non-zero constant and the above result is the case when . I also observed that this holds for sequence of natural numbers since Experimentally, this also holds for the sequence of prime numbers and also for the sequence of composite numbers . Question : What are the necessary and sufficient conditions on for the above relation to hold? Related question","f(x) (0,1) \lim_{n \to \infty}\dfrac{1}{n}\sum_{i=1}^{n}f\Big(\dfrac{i}{n}\Big) = \int_{0}^{1}f(x)dx 
\lim_{n \to \infty}\frac{1}{1+1+\ldots\text{n-times}}\sum_{i=1}^{n}1\times f\Big(\frac{1+1+\ldots\text{i-times}}{1+1+\ldots\text{n-times}}\Big) = \int_{0}^{1}f(x)dx.
 a_i 
\lim_{n \to \infty}\frac{1}{a_1 + a_2 + \ldots + a_n}\sum_{i=1}^{n}a_i f\Big(\frac{a_1 + a_2 + \ldots + a_i}{a_1 + a_2 + \ldots + a_n}\Big) =\int_{0}^{1}f(x)dx.
 a_i = c c c=1 a_i = i 
\lim_{n \to \infty}\frac{2}{n^2+n}\sum_{i=1}^{n}i f\Big(\frac{i^2+i}{n^2+n}\Big) =\int_{0}^{1}f(x)dx.
 a_i = p_n c_n a_i","['real-analysis', 'integration', 'number-theory', 'limits', 'summation']"
1,A proof for $\nabla |u|^p = p\ \text{sgn}(u)|u|^{p-1}\nabla u$.,A proof for .,\nabla |u|^p = p\ \text{sgn}(u)|u|^{p-1}\nabla u,"I believe the formula $$\nabla |u|^p = p\ \text{sgn}(u)|u|^{p-1}\nabla u $$ to be true for $u\in W^{1,p}(\Omega)$ , where $\Omega$ is an open domain with nice boundary, but failed to find a good reference for this.  I am sorry if this has been asked already. The usual assumption that I usually see regarding the chain rule for Sobolev functions, i.e. $$ \nabla F(u) = F'(u)\nabla u, $$ is that $F\in C^1$ with bounded derivative (or perhaps a Lipschitz function). I want $F(t)=|t|^p$ , which doesn't satisfy that assumption. My idea of the proof goes as follow: I'd like to mimic the proof of the case $p=1$ so first we show a similar result for $$ F_\varepsilon(t) = (\varepsilon^2 + t^2)^{p/2} $$ by approximation with $F_\varepsilon(u_n)$ , where $u_n\to u$ in $W^{1,p}$ and $u_n\in C_c^\infty(\Omega)$ . Then we take $\varepsilon \to 0$ to approximate $|\cdot|^p$ . While the usual argument using the Dominated Convergence Theorem wouldn't work, I think Vitali Convergence Theorem might do the trick. Does anyone know a reference to this result? Also, if the formula doesn't hold in general spaces (i.e. $u\notin W^{1,p}$ ) then does anyone have a counterexample? I have seen the proofs of the cases $p=1,2$ . Finding a reference for a general $p\in [1,\infty)$ turns out to be a lot harder than I anticipated.","I believe the formula to be true for , where is an open domain with nice boundary, but failed to find a good reference for this.  I am sorry if this has been asked already. The usual assumption that I usually see regarding the chain rule for Sobolev functions, i.e. is that with bounded derivative (or perhaps a Lipschitz function). I want , which doesn't satisfy that assumption. My idea of the proof goes as follow: I'd like to mimic the proof of the case so first we show a similar result for by approximation with , where in and . Then we take to approximate . While the usual argument using the Dominated Convergence Theorem wouldn't work, I think Vitali Convergence Theorem might do the trick. Does anyone know a reference to this result? Also, if the formula doesn't hold in general spaces (i.e. ) then does anyone have a counterexample? I have seen the proofs of the cases . Finding a reference for a general turns out to be a lot harder than I anticipated.","\nabla |u|^p = p\ \text{sgn}(u)|u|^{p-1}\nabla u
 u\in W^{1,p}(\Omega) \Omega 
\nabla F(u) = F'(u)\nabla u,
 F\in C^1 F(t)=|t|^p p=1 
F_\varepsilon(t) = (\varepsilon^2 + t^2)^{p/2}
 F_\varepsilon(u_n) u_n\to u W^{1,p} u_n\in C_c^\infty(\Omega) \varepsilon \to 0 |\cdot|^p u\notin W^{1,p} p=1,2 p\in [1,\infty)","['real-analysis', 'functional-analysis', 'reference-request', 'partial-differential-equations', 'sobolev-spaces']"
2,Evaluate the limit without using Stirling Formula,Evaluate the limit without using Stirling Formula,,I want to prove that $$\lim_{n \to \infty} p \frac{n^{(p+1)/2} (n!)^p p^{np+1}}{(np+p)!} = p^{1/2} (2\pi)^{(p-1)/2}$$ So I am working the book The Gamma Function by Emile Artin . In the book this limit is involved for proving the Gauss multiplication formula of the Gamma Function. The book uses the Stirling Formula for $n!$ to show the limit is that. However I was wondering if there is another way of prooving this limit that doesn't use Stirling Formula. Does anyone sees any way of approaching this?? Edit The limit making some manipulations is the same as $$\lim_{n \to \infty} p \frac{(n!)^p p^{np}}{(np)!n^{(p-1)/2}}$$ maybe this helps someone attempmting this task,I want to prove that So I am working the book The Gamma Function by Emile Artin . In the book this limit is involved for proving the Gauss multiplication formula of the Gamma Function. The book uses the Stirling Formula for to show the limit is that. However I was wondering if there is another way of prooving this limit that doesn't use Stirling Formula. Does anyone sees any way of approaching this?? Edit The limit making some manipulations is the same as maybe this helps someone attempmting this task,"\lim_{n \to \infty} p \frac{n^{(p+1)/2} (n!)^p p^{np+1}}{(np+p)!}
= p^{1/2} (2\pi)^{(p-1)/2} n! \lim_{n \to \infty} p \frac{(n!)^p p^{np}}{(np)!n^{(p-1)/2}}","['real-analysis', 'limits', 'gamma-function']"
3,Convergence rate of $a_{n+1}=\ln(a_n+1)$,Convergence rate of,a_{n+1}=\ln(a_n+1),"Let $a_0=1$ and $a_{n+1}=\ln(a_n+1)$ . The goal is to find $$ (\star)\quad \lim_{n\to \infty}\frac{n(na_n-2)}{\ln n} $$ It is easy to see that $a_n\to 0$ .  In addition, we can prove that $$ \lim_{n\to \infty}na_n=2. $$ Indeed, one can obtain by  Stolz's theorem that \begin{align} \lim_{n\to\infty}\frac{n}{\frac{1}{a_n}} &= \lim_{n\to\infty}\frac{(n+1)-n}{\frac{1}{a_{n+1}}-\frac{1}{a_n}} \\ &=\lim_{n\to\infty} \frac{1}{\frac{1}{\ln(1+a_n)}-\frac{1}{a_n}} \\ &= \lim_{n\to\infty}\frac{a_n\ln(1+a_n)}{a_n-\ln(1+a_n)}  \\ &=2 , \end{align} How to estimate the further limit $(\star)$ ?","Let and . The goal is to find It is easy to see that .  In addition, we can prove that Indeed, one can obtain by  Stolz's theorem that How to estimate the further limit ?","a_0=1 a_{n+1}=\ln(a_n+1) 
(\star)\quad \lim_{n\to \infty}\frac{n(na_n-2)}{\ln n}
 a_n\to 0 
\lim_{n\to \infty}na_n=2.
 \begin{align} \lim_{n\to\infty}\frac{n}{\frac{1}{a_n}} &= \lim_{n\to\infty}\frac{(n+1)-n}{\frac{1}{a_{n+1}}-\frac{1}{a_n}} \\ &=\lim_{n\to\infty} \frac{1}{\frac{1}{\ln(1+a_n)}-\frac{1}{a_n}} \\ &= \lim_{n\to\infty}\frac{a_n\ln(1+a_n)}{a_n-\ln(1+a_n)}  \\ &=2 , \end{align} (\star)","['calculus', 'real-analysis', 'limits']"
4,Evaluate $\lim\limits_{n \to \infty}\frac{n}{\sqrt[n]{n!}}$.,Evaluate .,\lim\limits_{n \to \infty}\frac{n}{\sqrt[n]{n!}},"Solution Notice that $$(\forall x \in \mathbb{R})~~e^x=1+x+\frac{x^2}{2!}+\cdots+\frac{x^n}{n!}+\cdots.$$ Let $x=n$ where $n\in \mathbb{N_+}$ . Then we obtain $$e^n=1+n+\frac{n^2}{2!}+\cdots+\frac{n^n}{n!}+\cdots>\frac{n^n}{n!}.$$ Thus, we obtain $$e>\frac{n}{\sqrt[n]{n!}}.\tag1$$ Moreover, we can find that, for $k=0,1,\cdots,n-1.$ $$\frac{n^k}{k!}< \frac{n^n}{n!}.$$ Thus \begin{align*} e^n&=1+n+\frac{n^2}{2!}+\cdots+\frac{n^n}{n!}+\frac{n^n}{n!}\cdot\left[\frac{n}{n+1}+\frac{n^2}{(n+1)(n+2)}+\cdots\right]\\ &< (n+1)\cdot\frac{n^n}{n!}+\frac{n^n}{n!}\cdot\left[\frac{n}{n+1}+\frac{n^2}{(n+1)^2}+\cdots\right]\\ &=(n+1)\cdot\frac{n^n}{n!}+\frac{n^n}{n!}\cdot n\\ &=(2n+1)\cdot \frac{n^n}{n!}, \end{align*} which shows that $$\frac{n}{\sqrt[n]{n!}}>\frac{e}{\sqrt[n]{2n+1}}.\tag2$$ Combining $(1)$ and $(2)$ , we have $$e>\frac{n}{\sqrt[n]{n!}}>\frac{e}{\sqrt[n]{2n+1}}\to e(n \to \infty).$$ By the squeeze theorem, $$\lim_{n \to \infty}\frac{n}{\sqrt[n]{n!}}=e.$$ Please correct me if I'm wrong. Many thanks.","Solution Notice that Let where . Then we obtain Thus, we obtain Moreover, we can find that, for Thus which shows that Combining and , we have By the squeeze theorem, Please correct me if I'm wrong. Many thanks.","(\forall x \in \mathbb{R})~~e^x=1+x+\frac{x^2}{2!}+\cdots+\frac{x^n}{n!}+\cdots. x=n n\in \mathbb{N_+} e^n=1+n+\frac{n^2}{2!}+\cdots+\frac{n^n}{n!}+\cdots>\frac{n^n}{n!}. e>\frac{n}{\sqrt[n]{n!}}.\tag1 k=0,1,\cdots,n-1. \frac{n^k}{k!}< \frac{n^n}{n!}. \begin{align*}
e^n&=1+n+\frac{n^2}{2!}+\cdots+\frac{n^n}{n!}+\frac{n^n}{n!}\cdot\left[\frac{n}{n+1}+\frac{n^2}{(n+1)(n+2)}+\cdots\right]\\
&< (n+1)\cdot\frac{n^n}{n!}+\frac{n^n}{n!}\cdot\left[\frac{n}{n+1}+\frac{n^2}{(n+1)^2}+\cdots\right]\\
&=(n+1)\cdot\frac{n^n}{n!}+\frac{n^n}{n!}\cdot n\\
&=(2n+1)\cdot \frac{n^n}{n!},
\end{align*} \frac{n}{\sqrt[n]{n!}}>\frac{e}{\sqrt[n]{2n+1}}.\tag2 (1) (2) e>\frac{n}{\sqrt[n]{n!}}>\frac{e}{\sqrt[n]{2n+1}}\to e(n \to \infty). \lim_{n \to \infty}\frac{n}{\sqrt[n]{n!}}=e.","['real-analysis', 'sequences-and-series', 'limits', 'proof-verification']"
5,Cauchy definite integral vs Riemann once again,Cauchy definite integral vs Riemann once again,,"The problem of showing the equivalence between the Cauchy (definite) integral defined below and the Riemann integral is an exercise with hint in C.R.Rosentrater, Varieties of Integration , 2015. I am interested in one direction, precisely the one contained in exercise 32 of chapter 3. Its converse, easy to prove, is contained in exercise 38 chapter 2. In what follows $\mathcal P_L$ indicates the partition $\mathcal P$ tagged by the left endpoints of its subintervals. The symbol $S_R$ indicates a Riemann sum. A Cauchy sum is a Riemann sum referred to a partition tagged by the left endpoints. It is clear that $f$ is Cauchy integrable over $[a,b]\,$ only if, $\,$ for any $\varepsilon>0$ , we can find a $\delta>0$ so that, whenever $\mathcal P$ and $\mathcal Q$ are partitions of $[a,b]$ with $\|\mathcal P\|<\delta$ and $\|\mathcal Q\|<\delta$ , one has $$|S_R(f,\mathcal P_L)-S_R(f,\mathcal Q_L)|<\varepsilon$$ Author suggests a proof by contraposition using a characterization of Riemann integrability, called there ""height-width bounds theorem"". The statement to be proved is If $f$ is bounded and Cauchy integrable over $[a,b]$ , then $f$ is Riemann integrable over $[a,b]$ I restated what is written in exercise 32 because Cauchy integrability doesn't imply boundedness. This problem appears here and there in the literature but, as far as I know, proofs are not at all elementary. Maybe Rosentrater's hint refers to an elementary one. I don't know how to use the hint. I am waiting for a further hint at least. Many thanks in advance.","The problem of showing the equivalence between the Cauchy (definite) integral defined below and the Riemann integral is an exercise with hint in C.R.Rosentrater, Varieties of Integration , 2015. I am interested in one direction, precisely the one contained in exercise 32 of chapter 3. Its converse, easy to prove, is contained in exercise 38 chapter 2. In what follows indicates the partition tagged by the left endpoints of its subintervals. The symbol indicates a Riemann sum. A Cauchy sum is a Riemann sum referred to a partition tagged by the left endpoints. It is clear that is Cauchy integrable over only if, for any , we can find a so that, whenever and are partitions of with and , one has Author suggests a proof by contraposition using a characterization of Riemann integrability, called there ""height-width bounds theorem"". The statement to be proved is If is bounded and Cauchy integrable over , then is Riemann integrable over I restated what is written in exercise 32 because Cauchy integrability doesn't imply boundedness. This problem appears here and there in the literature but, as far as I know, proofs are not at all elementary. Maybe Rosentrater's hint refers to an elementary one. I don't know how to use the hint. I am waiting for a further hint at least. Many thanks in advance.","\mathcal P_L \mathcal P S_R f [a,b]\, \, \varepsilon>0 \delta>0 \mathcal P \mathcal Q [a,b] \|\mathcal P\|<\delta \|\mathcal Q\|<\delta |S_R(f,\mathcal P_L)-S_R(f,\mathcal Q_L)|<\varepsilon f [a,b] f [a,b]","['real-analysis', 'integration', 'analysis']"
6,Infinite limit of a differentiable function,Infinite limit of a differentiable function,,"Let $f:[a,+\infty) \to \Bbb{R}$ be a differentiable function with the property: $$\inf\{f'(x)|x>a\}>0$$Prove that $\lim_{x \to +\infty}f(x)=+\infty$ Here is my solution: Let $c=\inf\{f'(x)|x>a\}>0$ and $x \geq a+1$ From Mean Value Theorem exists $x_0 \in [a+1,x]$ such that $$f(x)=f(a+1)+f'(x_0)(x-a+1)$$ But $f'(x) \geq c,\forall x \geq a+1$ Thus $$f(x) \geq f(a+1)+c(x-a+1),\forall x \geq a+1 \Rightarrow \liminf_{x \to +\infty}f(x)=+\infty$$ Thus $\lim_{x \to +\infty}f(x)=+\infty$ Is my solution correct or am i missing something? Thank you in advance.","Let $f:[a,+\infty) \to \Bbb{R}$ be a differentiable function with the property: $$\inf\{f'(x)|x>a\}>0$$Prove that $\lim_{x \to +\infty}f(x)=+\infty$ Here is my solution: Let $c=\inf\{f'(x)|x>a\}>0$ and $x \geq a+1$ From Mean Value Theorem exists $x_0 \in [a+1,x]$ such that $$f(x)=f(a+1)+f'(x_0)(x-a+1)$$ But $f'(x) \geq c,\forall x \geq a+1$ Thus $$f(x) \geq f(a+1)+c(x-a+1),\forall x \geq a+1 \Rightarrow \liminf_{x \to +\infty}f(x)=+\infty$$ Thus $\lim_{x \to +\infty}f(x)=+\infty$ Is my solution correct or am i missing something? Thank you in advance.",,"['calculus', 'real-analysis', 'limits', 'derivatives', 'proof-verification']"
7,Integral of the function $ (1+|x|^2)^{-k}$,Integral of the function, (1+|x|^2)^{-k},"I want to prove that the $$\int _{ { R }^{ n } }{ \frac { 1 }{ { (1+{ |x| }^{ 2 }) }^{ k } }  } <\infty $$ if $k>\frac n 2$; where |x| is the usual norm in ${R}^{n}$. I tried this: $$\int _{ { R }^{ n } }{ \frac { 1 }{ { (1+{ |x| }^{ 2 }) }^{ k } }  }=\int _{ { R }^{ n }-{ B }_{ 1 }(0) }{ \frac { 1 }{ { (1+{ |x| }^{ 2 }) }^{ k } }  } + \int _{ { B }_{ 1 }(0) }{ \frac { 1 }{ { (1+{ |x| }^{ 2 }) }^{ k } }  } $$ where ${ B }_{ 1 }(0)$ is the unit ball, the second term of the sum is finite since $$\int _{ { B }_{ 1 }(0) }{ \frac { 1 }{ { (1+{ |x| }^{ 2 }) }^{ k } }  } \le \int _{ { B }_{ 1 }(0) }{ \frac { 1 }{ { ({ |x| }^{ 2 }) }^{ k } }  }<\infty $$ if $k>\frac n 2$ but i do not know how to estimate the first term of the sum, any idea?","I want to prove that the $$\int _{ { R }^{ n } }{ \frac { 1 }{ { (1+{ |x| }^{ 2 }) }^{ k } }  } <\infty $$ if $k>\frac n 2$; where |x| is the usual norm in ${R}^{n}$. I tried this: $$\int _{ { R }^{ n } }{ \frac { 1 }{ { (1+{ |x| }^{ 2 }) }^{ k } }  }=\int _{ { R }^{ n }-{ B }_{ 1 }(0) }{ \frac { 1 }{ { (1+{ |x| }^{ 2 }) }^{ k } }  } + \int _{ { B }_{ 1 }(0) }{ \frac { 1 }{ { (1+{ |x| }^{ 2 }) }^{ k } }  } $$ where ${ B }_{ 1 }(0)$ is the unit ball, the second term of the sum is finite since $$\int _{ { B }_{ 1 }(0) }{ \frac { 1 }{ { (1+{ |x| }^{ 2 }) }^{ k } }  } \le \int _{ { B }_{ 1 }(0) }{ \frac { 1 }{ { ({ |x| }^{ 2 }) }^{ k } }  }<\infty $$ if $k>\frac n 2$ but i do not know how to estimate the first term of the sum, any idea?",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus', 'polar-coordinates']"
8,If all proper subsequences converge to same limit then the sequence converges.,If all proper subsequences converge to same limit then the sequence converges.,,Let $\{X_n\}_n$ be a bounded sequence. Its convergent proper subsequences converge to the same limit $\ell$ . I want to prove that $\{X_n\}_n$ converges to $\ell$ . Notice that proper subsequences are all the sequences except for the sequence itself. Is it enough to say that $\{X_{2n}\}$ and $\{X_{2n+1}\}$ are convergent to $l$ then $\{X_n\}$ is convergent to $\ell$ ?,Let be a bounded sequence. Its convergent proper subsequences converge to the same limit . I want to prove that converges to . Notice that proper subsequences are all the sequences except for the sequence itself. Is it enough to say that and are convergent to then is convergent to ?,\{X_n\}_n \ell \{X_n\}_n \ell \{X_{2n}\} \{X_{2n+1}\} l \{X_n\} \ell,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
9,A measure and the Fourier inverse transform of its Fourier transform,A measure and the Fourier inverse transform of its Fourier transform,,"Given a finite Borel measure $\mu$ on $\mathbb{R}^d$ , consider the Fourier inverse transform of its Fourier transform: $\mathcal{F}^{-1}(\mathcal{F}(\mu))$ , where $$ \mathcal{F}(\mu)(\xi) = \int \exp(-2\pi i x \cdot \xi) d\mu(x). $$ and $$ \mathcal{F}^{-1}(g)(\xi) = \int \exp(2\pi i x \cdot \xi) g(x) dx. $$ How is $\mu$ related to $\mathcal{F}^{-1}(\mathcal{F}(\mu))$ ? In particular, how are their supports related? For context, I know that if instead of a measure $\mu$ we had a function $f$ , we would have $$\mathcal{F}^{-1}(\mathcal{F}(f)) = f$$ (under suitable hypotheses, such as $f$ and $\mathcal{F}(f)$ in $L^1$ ). I don't expect anything like this to hold for measures. But I thought perhaps the supports of $\mathcal{F}^{-1}(\mathcal{F}(\mu))$ and $\mu$ might be related. For context, I know that if $\mathcal{\mu} \in L^1(\mathbb{R}^d)$ , then $\mu$ has a continuous density.","Given a finite Borel measure on , consider the Fourier inverse transform of its Fourier transform: , where and How is related to ? In particular, how are their supports related? For context, I know that if instead of a measure we had a function , we would have (under suitable hypotheses, such as and in ). I don't expect anything like this to hold for measures. But I thought perhaps the supports of and might be related. For context, I know that if , then has a continuous density.","\mu \mathbb{R}^d \mathcal{F}^{-1}(\mathcal{F}(\mu)) 
\mathcal{F}(\mu)(\xi) = \int \exp(-2\pi i x \cdot \xi) d\mu(x).
 
\mathcal{F}^{-1}(g)(\xi) = \int \exp(2\pi i x \cdot \xi) g(x) dx.
 \mu \mathcal{F}^{-1}(\mathcal{F}(\mu)) \mu f \mathcal{F}^{-1}(\mathcal{F}(f)) = f f \mathcal{F}(f) L^1 \mathcal{F}^{-1}(\mathcal{F}(\mu)) \mu \mathcal{\mu} \in L^1(\mathbb{R}^d) \mu","['real-analysis', 'measure-theory', 'fourier-analysis', 'harmonic-analysis', 'fourier-transform']"
10,A smooth function $f(x)$ has a unique local and global minimum. What happens to its location as $f(x)$ varies smoothly in time?,A smooth function  has a unique local and global minimum. What happens to its location as  varies smoothly in time?,f(x) f(x),"Let $f(x,t)$ be a smooth function $\mathbb R^2\to\mathbb R$ such that   $F_t(x):=f(x,t)$ has a unique local minimum in $x$ for every fixed   $t\in[0,1]$. Further assume that this local minimum of $F_t(x)$ is also the unique global minimum of $F_t(x)$. How regularly does the location of this unique minimum vary with   respect to $t$? In other words, if $x=\chi(t)$ is the $x$-value where   $F_t(x)$ attains its unique minimum, can we say that $\chi(t)$ is a   smooth function of $t$? If not, is $\chi(t)$ differentiable or   continuous? I asked a similar version of this question here . The answer was correct and very clever, but I was wondering what happens if we insist that the unique global minimum was also a unique local minimum.","Let $f(x,t)$ be a smooth function $\mathbb R^2\to\mathbb R$ such that   $F_t(x):=f(x,t)$ has a unique local minimum in $x$ for every fixed   $t\in[0,1]$. Further assume that this local minimum of $F_t(x)$ is also the unique global minimum of $F_t(x)$. How regularly does the location of this unique minimum vary with   respect to $t$? In other words, if $x=\chi(t)$ is the $x$-value where   $F_t(x)$ attains its unique minimum, can we say that $\chi(t)$ is a   smooth function of $t$? If not, is $\chi(t)$ differentiable or   continuous? I asked a similar version of this question here . The answer was correct and very clever, but I was wondering what happens if we insist that the unique global minimum was also a unique local minimum.",,"['real-analysis', 'maxima-minima']"
11,"Does there exist a continuous f ,integration of $\int_{0}^{1} x^n f(x)= 1$ for all $n$","Does there exist a continuous f ,integration of  for all",\int_{0}^{1} x^n f(x)= 1 n,"Does there exist  a continuous function $f: [0,1]\to[0, \infty )$ such that $$\int_{0}^{1} x^n f(x)= 1$$ for all $n$. Here is what I tried. As $f$ is a continuous function on the compact set $[0,1]$ , there exists $m,M \in \mathbb{R}$, $m \leq f(x) \leq M$. Now applying integration to both side we have $\frac{m}{n+1} \leq 1 \leq \frac{M}{n+1}$ i.e. $m \leq n+1 \leq M$ for all $n\geq 1$. This contradicts the fact that the set $\mathbb{N}$ is unbounded. It would be great help if you would verify my solution. If the solution is wrong a hint will be greatly appreciated.","Does there exist  a continuous function $f: [0,1]\to[0, \infty )$ such that $$\int_{0}^{1} x^n f(x)= 1$$ for all $n$. Here is what I tried. As $f$ is a continuous function on the compact set $[0,1]$ , there exists $m,M \in \mathbb{R}$, $m \leq f(x) \leq M$. Now applying integration to both side we have $\frac{m}{n+1} \leq 1 \leq \frac{M}{n+1}$ i.e. $m \leq n+1 \leq M$ for all $n\geq 1$. This contradicts the fact that the set $\mathbb{N}$ is unbounded. It would be great help if you would verify my solution. If the solution is wrong a hint will be greatly appreciated.",,"['real-analysis', 'integration', 'proof-verification', 'continuity']"
12,"Show that $\mathbb{R}$, when equipped with the standard topology, is not compact.","Show that , when equipped with the standard topology, is not compact.",\mathbb{R},"Proof: Let $\mathscr{U}$ := {$U_n$}$_{n \geq 1}$ = {$(n-1,n+1)$}$_{n \geq 1}$ be an open cover of $\mathbb{R}$. Consider finitely many of them, say: $U_{n_1} \ldots U_{n_k}$. Setting $N := \max(n_1 \ldots n_k)$, we get that: $U_{n_i} \subset U_N \implies \bigcup_{i=1}^{k} U_{n_i} = U_N = (N-1,N+1) \subsetneq \mathbb{R}$. Hence, $\mathscr{U}$ cannot be reduced to a finite subcover of $\mathbb{R}$ so that it is not compact. QED . How did I do? Thanks.","Proof: Let $\mathscr{U}$ := {$U_n$}$_{n \geq 1}$ = {$(n-1,n+1)$}$_{n \geq 1}$ be an open cover of $\mathbb{R}$. Consider finitely many of them, say: $U_{n_1} \ldots U_{n_k}$. Setting $N := \max(n_1 \ldots n_k)$, we get that: $U_{n_i} \subset U_N \implies \bigcup_{i=1}^{k} U_{n_i} = U_N = (N-1,N+1) \subsetneq \mathbb{R}$. Hence, $\mathscr{U}$ cannot be reduced to a finite subcover of $\mathbb{R}$ so that it is not compact. QED . How did I do? Thanks.",,"['real-analysis', 'general-topology', 'proof-verification', 'compactness']"
13,Existence of $\xi$ and $\eta$ such that $f'(\xi)+f'(\eta)=\xi+\eta$,Existence of  and  such that,\xi \eta f'(\xi)+f'(\eta)=\xi+\eta,"Let $f$ be continuous on $[0,1]$ , differentiable in $(0,1)$ . Assume futher that $f(0)=0$ , $f(1)=1/2$ . Show that there exist $\xi,\eta\in (0,1)$ such that $f(\xi)+f'(\eta)=\xi+\eta$ . I saw this problem in a draft. I do not know whether it is true. Up to now, I have not find a counterexample. My idea is as follows: $f(\xi)-\xi=\eta-f'(\eta)$ . Let $F(x)=f(x)-x$ , then $F(0)=0, F(1)=-1/2$ , so $f(\xi)-\xi$ can be chosen to be arbitrary $a\in (-1/2,0)$ . The strategy is then to find $\eta$ such $a=\eta-f'(\eta)$ . Roll's theorem may be applied. However, letting $G(x)=ax-x^2/2+f(x)$ , then $G(0)=0$ , $G(1)=a$ . This fact does not verify that of Roll. So how can I get across the difficulties? If we change $f(\xi)+f'(\eta)=\xi+\eta$ to be $f'(\xi)+f'(\eta)=\xi+\eta$ for some $\xi,\eta\in (0,1), \xi\neq \eta$ . Can we prove it?","Let be continuous on , differentiable in . Assume futher that , . Show that there exist such that . I saw this problem in a draft. I do not know whether it is true. Up to now, I have not find a counterexample. My idea is as follows: . Let , then , so can be chosen to be arbitrary . The strategy is then to find such . Roll's theorem may be applied. However, letting , then , . This fact does not verify that of Roll. So how can I get across the difficulties? If we change to be for some . Can we prove it?","f [0,1] (0,1) f(0)=0 f(1)=1/2 \xi,\eta\in (0,1) f(\xi)+f'(\eta)=\xi+\eta f(\xi)-\xi=\eta-f'(\eta) F(x)=f(x)-x F(0)=0, F(1)=-1/2 f(\xi)-\xi a\in (-1/2,0) \eta a=\eta-f'(\eta) G(x)=ax-x^2/2+f(x) G(0)=0 G(1)=a f(\xi)+f'(\eta)=\xi+\eta f'(\xi)+f'(\eta)=\xi+\eta \xi,\eta\in (0,1), \xi\neq \eta","['real-analysis', 'calculus']"
14,"Let $f(z)$ be analytic in the unit disc,","Let  be analytic in the unit disc,",f(z),"Let $f(z)$ be analytic in the unit disc, $Rf(z) \ge 1$, and $f(0)>0$. Show that $$f(0) \cdot \frac{1-|z|}{1+|z|} \le |f(z)| \le f(0) \cdot \frac{1+|z|}{1-|z|} \quad z \in D $$ This looks like something I would have to prove using cases. I tired to throw some numbers in there to see what was going on but I am still not seeing how to prove this. I think I am missing or forgetting a concept to use. I have attempted the Schwartz lemma as suggested below and assumed $f(0)=1$ I am not sure if this is the right path to take?","Let $f(z)$ be analytic in the unit disc, $Rf(z) \ge 1$, and $f(0)>0$. Show that $$f(0) \cdot \frac{1-|z|}{1+|z|} \le |f(z)| \le f(0) \cdot \frac{1+|z|}{1-|z|} \quad z \in D $$ This looks like something I would have to prove using cases. I tired to throw some numbers in there to see what was going on but I am still not seeing how to prove this. I think I am missing or forgetting a concept to use. I have attempted the Schwartz lemma as suggested below and assumed $f(0)=1$ I am not sure if this is the right path to take?",,"['real-analysis', 'complex-analysis']"
15,Prove that if a set is nowhere dense iff the complement of the closure of the set is dense.,Prove that if a set is nowhere dense iff the complement of the closure of the set is dense.,,"I am able to prove the iff in the forward direction. But, I am having trouble proving the statement in other direction. I am trying to use the definition of dense, but I am not getting anywhere with it.","I am able to prove the iff in the forward direction. But, I am having trouble proving the statement in other direction. I am trying to use the definition of dense, but I am not getting anywhere with it.",,"['real-analysis', 'general-topology']"
16,Understanding Rudin's proof that a Riemann integrable function is measurable,Understanding Rudin's proof that a Riemann integrable function is measurable,,"In the book ""Principles of Mathematical Analysis"" by Walter Rudin, he proves the following theorem (slightly reworded), Theorem. If $f$ is Riemann integrable on $[a,b],$ then $f$ is Lebesgue integrable on $[a,b]$ with respect to the Lebesgue measure $m$ and   $$ \int_a^b f \ dx = \mathscr{R} \int_a^b f \ dx $$   Where $\mathscr{R} \int$ denotes the Riemann integral, while $\int$ denotes the Lebesgue integral. Proof Suupose $f$ is bounded. Then there exists a sequence $\{P_k\}$ of partitions of $[a,b]$ such that $P_{k+1}$ is a refinement of $P_k$ for each $k$ and   $$ \lim_{k\rightarrow\infty} L(P_k,f) = \mathscr{R}\underline{\int_a^b} f \ dx, \quad \lim_{k\rightarrow\infty} U(P_k,f) = \mathscr{R}\overline{\int_a^b} f \ dx. $$   Where $L(P_k), U(P_k)$ are the upper and lower sums respectively. If $P_k=\{a=x_0<x_1<\dots<x_n=b\},$ these are defined as,   $$ L(P_k,f) = \sum_{i=1}^n (x_i-x_{i-1})m_i, \quad U(P_k,f) = \sum_{i=1}^n (x_i-x_{i-1})M_i,$$   where $M_i = \sup_{x\in[x_{i-1},x_i]} f(x)$ and $m_i = \inf_{x\in[x_{i-1},x_i]} f(x).$ We then define functions $U,L$ as $U_k(a)=L_k(a)=f(a)$ and for each $x \in(x_{i-1},x_i],$ $1\leq i \leq n,$ $U_k(x)=M_i$ and $L_k(x)=m_i.$ Then for all $x\in [a,b],$   $$ L(P_k,f) = \int_a^b L_k \ dx, \quad U(P_k,f) = \int_a^b U_k \ dx, $$   and $$L_1(x) \leq L_2(x) \leq \dots \leq f(x) \leq \dots \leq U_2(x) \leq U_1(x). $$   There the sequence of functions $L_k, U_k$ converge point-wise on $[a,b],$ so let $L, U$ be the limit functions respectively. Then $L$ and $U$ are bounded measurable functions on $[a,b]$ and for any $x \in [a,b],$   $$ L(x) \leq f(x) \leq U(x), $$   and by the monotone convergence theorem,   $$ \int_a^b L(x) \ dx = \mathscr{R} \underline{\int_a^b} f \ dx, \quad \int_a^b U(x) \ dx = \mathscr{R} \overline{\int_a^b} f \ dx. $$   Since $f$ is Riemann integrable, the upper and lower Riemann integrals are equal. Since $L(x) \leq U(x),$ it follows that $L(x) = U(x)$ almost everywhere on [a,b]. Then $L(x) = f(x) = U(x)$ almost everywhere on $[a,b],$ so $f$ is measurable and the result follows. I've understood the proof up until the last sentence, but I don't understand how you can conclude that $f$ is measurable (the part I bolded). I know it is a standard result that working with the Lebesgue measure on $[a,b],$ if $f$ is measurable and $f(x)=g(x)$ almost everywhere on, then $g$ is also measurable (and that this result also holds more generally). However when trying to prove this, I needed to show that a subset of a measure zero set is measurable.  While I know this is true for the Lebesgue measure, I'm struggling to prove it using the definition of measurable sets provided by Rudin. To be brief, Rudin defines a set to be finitely $\mu$-measurable if there exists a sequence of elementary sets which converges to it, in the sense that the outer measure of the symmetric difference converges to $0.$ A set if *$\mu$-measurable if there exists a sequence of finitely $\mu$-measurable sets that converge to it. I'm also wondering if there's an easier way to prove this assertion, rather than referring to the more general result. Edit in response to hardmath's response: To prove that $f$ is measurable using the fact that $L$ is and $f=L$ almost everywhere, I initially thought to do the following, Let $E = \{ x \mid f(x) \neq L(x) \},$ then $E$ is measurable and $m(E) = 0,$ where $m$ is the Lebesgue measure. We wish to show that for every $c \in \Bbb R,$ the set $\{ x \mid f(x)>c\}$ is measurable (this is how Rudin defines measurable functions). So given an arbitrary $c,$ we have, $$ f^{-1}((c,\infty]) = \left(f^{-1}((c,\infty]) \cap E \right) \ \cup \ \left(L^{-1}((c,\infty]) \cap E^C \right), $$ since $f$ and $L$ agree on $E.$ The second term is measurable, but I'm not sure how to one can conclude that the first is.","In the book ""Principles of Mathematical Analysis"" by Walter Rudin, he proves the following theorem (slightly reworded), Theorem. If $f$ is Riemann integrable on $[a,b],$ then $f$ is Lebesgue integrable on $[a,b]$ with respect to the Lebesgue measure $m$ and   $$ \int_a^b f \ dx = \mathscr{R} \int_a^b f \ dx $$   Where $\mathscr{R} \int$ denotes the Riemann integral, while $\int$ denotes the Lebesgue integral. Proof Suupose $f$ is bounded. Then there exists a sequence $\{P_k\}$ of partitions of $[a,b]$ such that $P_{k+1}$ is a refinement of $P_k$ for each $k$ and   $$ \lim_{k\rightarrow\infty} L(P_k,f) = \mathscr{R}\underline{\int_a^b} f \ dx, \quad \lim_{k\rightarrow\infty} U(P_k,f) = \mathscr{R}\overline{\int_a^b} f \ dx. $$   Where $L(P_k), U(P_k)$ are the upper and lower sums respectively. If $P_k=\{a=x_0<x_1<\dots<x_n=b\},$ these are defined as,   $$ L(P_k,f) = \sum_{i=1}^n (x_i-x_{i-1})m_i, \quad U(P_k,f) = \sum_{i=1}^n (x_i-x_{i-1})M_i,$$   where $M_i = \sup_{x\in[x_{i-1},x_i]} f(x)$ and $m_i = \inf_{x\in[x_{i-1},x_i]} f(x).$ We then define functions $U,L$ as $U_k(a)=L_k(a)=f(a)$ and for each $x \in(x_{i-1},x_i],$ $1\leq i \leq n,$ $U_k(x)=M_i$ and $L_k(x)=m_i.$ Then for all $x\in [a,b],$   $$ L(P_k,f) = \int_a^b L_k \ dx, \quad U(P_k,f) = \int_a^b U_k \ dx, $$   and $$L_1(x) \leq L_2(x) \leq \dots \leq f(x) \leq \dots \leq U_2(x) \leq U_1(x). $$   There the sequence of functions $L_k, U_k$ converge point-wise on $[a,b],$ so let $L, U$ be the limit functions respectively. Then $L$ and $U$ are bounded measurable functions on $[a,b]$ and for any $x \in [a,b],$   $$ L(x) \leq f(x) \leq U(x), $$   and by the monotone convergence theorem,   $$ \int_a^b L(x) \ dx = \mathscr{R} \underline{\int_a^b} f \ dx, \quad \int_a^b U(x) \ dx = \mathscr{R} \overline{\int_a^b} f \ dx. $$   Since $f$ is Riemann integrable, the upper and lower Riemann integrals are equal. Since $L(x) \leq U(x),$ it follows that $L(x) = U(x)$ almost everywhere on [a,b]. Then $L(x) = f(x) = U(x)$ almost everywhere on $[a,b],$ so $f$ is measurable and the result follows. I've understood the proof up until the last sentence, but I don't understand how you can conclude that $f$ is measurable (the part I bolded). I know it is a standard result that working with the Lebesgue measure on $[a,b],$ if $f$ is measurable and $f(x)=g(x)$ almost everywhere on, then $g$ is also measurable (and that this result also holds more generally). However when trying to prove this, I needed to show that a subset of a measure zero set is measurable.  While I know this is true for the Lebesgue measure, I'm struggling to prove it using the definition of measurable sets provided by Rudin. To be brief, Rudin defines a set to be finitely $\mu$-measurable if there exists a sequence of elementary sets which converges to it, in the sense that the outer measure of the symmetric difference converges to $0.$ A set if *$\mu$-measurable if there exists a sequence of finitely $\mu$-measurable sets that converge to it. I'm also wondering if there's an easier way to prove this assertion, rather than referring to the more general result. Edit in response to hardmath's response: To prove that $f$ is measurable using the fact that $L$ is and $f=L$ almost everywhere, I initially thought to do the following, Let $E = \{ x \mid f(x) \neq L(x) \},$ then $E$ is measurable and $m(E) = 0,$ where $m$ is the Lebesgue measure. We wish to show that for every $c \in \Bbb R,$ the set $\{ x \mid f(x)>c\}$ is measurable (this is how Rudin defines measurable functions). So given an arbitrary $c,$ we have, $$ f^{-1}((c,\infty]) = \left(f^{-1}((c,\infty]) \cap E \right) \ \cup \ \left(L^{-1}((c,\infty]) \cap E^C \right), $$ since $f$ and $L$ agree on $E.$ The second term is measurable, but I'm not sure how to one can conclude that the first is.",,"['real-analysis', 'integration', 'measure-theory']"
17,Does $\sum\limits_{n=1}^\infty\sin(n)\sin\left(\frac{\pi}{2n}\right)$ converge?,Does  converge?,\sum\limits_{n=1}^\infty\sin(n)\sin\left(\frac{\pi}{2n}\right),"I must determine whether if the following series converges, converges absolutely, or diverges: $$\sum_{n=1}^\infty\sin(n)\sin\left(\frac{\pi}{2n}\right)$$ By the comparison test, I have already found that $\sum\limits_{n=1}^\infty \left(\sin\left(\frac{\pi}{2n}\right)\right)^p$ converges for $p>1$ and diverges for $p \leq 1$. Thus, $ \sum\limits_{n=1}^\infty\sin\left(\frac{\pi}{2n}\right)$ diverges by this criterion. I suspect the entire series will also diverge, and that I have to use the comparison test, but I encountered an issue: Since $-1 \leq \sin n \leq 1$, we have that $\sin(n)\sin\left(\frac{\pi}{2n}\right) \leq \sin\left(\frac{\pi}{2n}\right)$. This would be useful if the series represented by the term on the right converged; in its current state, this cannot be used to prove divergence. Is my reasoning wrong? Should I be using another test for this series? Thank you.","I must determine whether if the following series converges, converges absolutely, or diverges: $$\sum_{n=1}^\infty\sin(n)\sin\left(\frac{\pi}{2n}\right)$$ By the comparison test, I have already found that $\sum\limits_{n=1}^\infty \left(\sin\left(\frac{\pi}{2n}\right)\right)^p$ converges for $p>1$ and diverges for $p \leq 1$. Thus, $ \sum\limits_{n=1}^\infty\sin\left(\frac{\pi}{2n}\right)$ diverges by this criterion. I suspect the entire series will also diverge, and that I have to use the comparison test, but I encountered an issue: Since $-1 \leq \sin n \leq 1$, we have that $\sin(n)\sin\left(\frac{\pi}{2n}\right) \leq \sin\left(\frac{\pi}{2n}\right)$. This would be useful if the series represented by the term on the right converged; in its current state, this cannot be used to prove divergence. Is my reasoning wrong? Should I be using another test for this series? Thank you.",,"['calculus', 'real-analysis']"
18,"If $f:[a,b]\to\mathbb{R}$ is increasing, does it maps Borel sets to measurable sets?","If  is increasing, does it maps Borel sets to measurable sets?","f:[a,b]\to\mathbb{R}","Suppose $f:[a,b]\to\mathbb{R}$ is strictly increasing and left-continuous. Does it follow that $f$ maps Borel subsets of $[a,b]$ to Lebesgue measurable subsets of $\mathbb{R}$? My intuition tells me that this is the case because the fact that $f$ is non-decreasing on a closed interval $[a,b]$ implies that $f$ has at most countably many discontinuities. But I cannot continue from there.","Suppose $f:[a,b]\to\mathbb{R}$ is strictly increasing and left-continuous. Does it follow that $f$ maps Borel subsets of $[a,b]$ to Lebesgue measurable subsets of $\mathbb{R}$? My intuition tells me that this is the case because the fact that $f$ is non-decreasing on a closed interval $[a,b]$ implies that $f$ has at most countably many discontinuities. But I cannot continue from there.",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
19,Topology for Divergent Sequence,Topology for Divergent Sequence,,"Let $a_n$ be a sequence of real numbers and define the ""Cesàro limit"" of $a_n$ to be $C \lim\limits_{n\rightarrow \infty}{a_n} = \lim\limits_{N\rightarrow\infty}{\frac{1}{N}} \sum\limits_{n=1}^{N}{a_n}$. I'd like to give $\mathbb{R}$ a topology that gives rise to this mode of convergence. However, if we take the closure of a set as the set of all limit points of that set (i.e. $Cl(S)$ is set of real numbers which are Cesàro limits of sequences of numbers in $S$), then it's clear to me that $Cl(S)$ is the (closure of) the convex hull of $S$. This ensures that this closure operator cannot come from a topology, since $Cl(S\cup T) \neq Cl(S)\cup Cl(T)$ whenever $Cl(S)\cup Cl(T)$ is not connected. My question is then this. Is there a generalization of ""topology"" that would allow such a thing? Is there anything useful that can be gained from this definition?","Let $a_n$ be a sequence of real numbers and define the ""Cesàro limit"" of $a_n$ to be $C \lim\limits_{n\rightarrow \infty}{a_n} = \lim\limits_{N\rightarrow\infty}{\frac{1}{N}} \sum\limits_{n=1}^{N}{a_n}$. I'd like to give $\mathbb{R}$ a topology that gives rise to this mode of convergence. However, if we take the closure of a set as the set of all limit points of that set (i.e. $Cl(S)$ is set of real numbers which are Cesàro limits of sequences of numbers in $S$), then it's clear to me that $Cl(S)$ is the (closure of) the convex hull of $S$. This ensures that this closure operator cannot come from a topology, since $Cl(S\cup T) \neq Cl(S)\cup Cl(T)$ whenever $Cl(S)\cup Cl(T)$ is not connected. My question is then this. Is there a generalization of ""topology"" that would allow such a thing? Is there anything useful that can be gained from this definition?",,"['real-analysis', 'general-topology', 'divergent-series']"
20,Mean Value Theorem Confusion.,Mean Value Theorem Confusion.,,"In H. Cartan 's Differential Calculus , Theorem 3.1.1 is called the Mean Value Theorem and is stated as: Theorem: Let $f:[a,b]\to\mathbf R^n$ and $g:[a,b]\to\mathbf R$ be two functions which are continuous on $[a,b]$  and differentiable on $(a,b)$.   Assume that $\|f'(x)\|\leq g'(x)$ for all $x\in(a,b)$.   Then $\|f(b)-f(a)\|\leq g(b)-g(a)$. On the other hand, the universally known (Lagrange's) Mean Value Theorem states that Theorem: Let $f:[a,b]\to\mathbf R$ be a function which is continuous on $[a,b]$ and differentiable on $(a,b)$.   Then there exists $c\in(a,b)$ such that $(f(b)-f(a))/(b-a)=f'(c)$. Since they are both being called Mean Value Theorems, I think may be one follows easily from the other. But I fail to see the connection. Can somebody please shed some light on this. Thanks.","In H. Cartan 's Differential Calculus , Theorem 3.1.1 is called the Mean Value Theorem and is stated as: Theorem: Let $f:[a,b]\to\mathbf R^n$ and $g:[a,b]\to\mathbf R$ be two functions which are continuous on $[a,b]$  and differentiable on $(a,b)$.   Assume that $\|f'(x)\|\leq g'(x)$ for all $x\in(a,b)$.   Then $\|f(b)-f(a)\|\leq g(b)-g(a)$. On the other hand, the universally known (Lagrange's) Mean Value Theorem states that Theorem: Let $f:[a,b]\to\mathbf R$ be a function which is continuous on $[a,b]$ and differentiable on $(a,b)$.   Then there exists $c\in(a,b)$ such that $(f(b)-f(a))/(b-a)=f'(c)$. Since they are both being called Mean Value Theorems, I think may be one follows easily from the other. But I fail to see the connection. Can somebody please shed some light on this. Thanks.",,"['real-analysis', 'derivatives']"
21,A measurable function equal to a countable sum of characteristic functions?,A measurable function equal to a countable sum of characteristic functions?,,"A theorem in measure theory says that if $\mu$ is a measure on $X$ and $f : X \rightarrow [ 0, \infty]$ is $\mu$-measurable, then there exists a sequence $( A_k)_{k \in \mathbb{N}}$ of $\mu$-measurable sets in $X$ such that $$f = \sum_{k = 1}^{\infty} \frac{1}{k} 1_{A_k}$$ where $1_{A_k} ( x) = 1$ if $x \in A_k$ and 0 otherwise. I am trying to understand the intuition of this result. If $X =\mathbb{R}$ and $f$ is continuous, then $f$ takes an uncountable number of values in its range. How could that be given a countable sum and values?","A theorem in measure theory says that if $\mu$ is a measure on $X$ and $f : X \rightarrow [ 0, \infty]$ is $\mu$-measurable, then there exists a sequence $( A_k)_{k \in \mathbb{N}}$ of $\mu$-measurable sets in $X$ such that $$f = \sum_{k = 1}^{\infty} \frac{1}{k} 1_{A_k}$$ where $1_{A_k} ( x) = 1$ if $x \in A_k$ and 0 otherwise. I am trying to understand the intuition of this result. If $X =\mathbb{R}$ and $f$ is continuous, then $f$ takes an uncountable number of values in its range. How could that be given a countable sum and values?",,"['real-analysis', 'measure-theory']"
22,Complete metric subspaces of $\mathbb{Q}$,Complete metric subspaces of,\mathbb{Q},"Is there a nice characterization for the complete metric subspaces of $\mathbb{Q}$ (with the usual metric)? It seems like a such a subspace must have empty interior; if it contained an open interval then it would clearly contain a non-converging Cauchy sequence. But this isn't enough, because consider the subset $\left\{\frac{1}{n} : n \in \mathbb{N} \right\} \subseteq \mathbb{Q}$. This has empty interior but is not complete because it doesn't contain $0$. In other words, a complete subspace of $\mathbb{Q}$ must be closed (which I think is true about metric spaces in general). So a complete subspace of $\mathbb{Q}$ must be closed and have empty interior, but are these two sufficient to imply completeness? If so, how do I prove this, and if not, what am I missing? I'm not necessarily looking for a complete answer, just some pointers in the right direction.","Is there a nice characterization for the complete metric subspaces of $\mathbb{Q}$ (with the usual metric)? It seems like a such a subspace must have empty interior; if it contained an open interval then it would clearly contain a non-converging Cauchy sequence. But this isn't enough, because consider the subset $\left\{\frac{1}{n} : n \in \mathbb{N} \right\} \subseteq \mathbb{Q}$. This has empty interior but is not complete because it doesn't contain $0$. In other words, a complete subspace of $\mathbb{Q}$ must be closed (which I think is true about metric spaces in general). So a complete subspace of $\mathbb{Q}$ must be closed and have empty interior, but are these two sufficient to imply completeness? If so, how do I prove this, and if not, what am I missing? I'm not necessarily looking for a complete answer, just some pointers in the right direction.",,"['real-analysis', 'metric-spaces']"
23,Convex function almost surely differentiable.,Convex function almost surely differentiable.,,"If f: $\mathbb{R}^n \rightarrow \mathbb{R}$ is a convex function, i heard that f is almost everywhere differentiable. Is it true? I can't find a proof (n-dimentional). Thank you for any help","If f: $\mathbb{R}^n \rightarrow \mathbb{R}$ is a convex function, i heard that f is almost everywhere differentiable. Is it true? I can't find a proof (n-dimentional). Thank you for any help",,"['real-analysis', 'derivatives', 'convex-analysis']"
24,Prove that the dimension of the tangent space $T_x(X)$ of a k-dimensional manifold is k,Prove that the dimension of the tangent space  of a k-dimensional manifold is k,T_x(X),"In section 2, page 9 of Guillemin and Pollack's book $\textit{Differential Topology}$, he gave a proof that the dimension of the tangent space $T_x(X)$ is equal to the dimension of the manifold $X$. However I read the proof at least 5 times and had no idea what they were talking about. ""The dimension of the vector space $T_x(X)$ is, as you expect, the dimension $k$ of $X$. To prove this, we use the smoothness of he inverse $\phi^{-1}$. Choose an open set $W$ in $\mathbf{R}^N$ and a smooth map $\Phi': \mathbf{R}^N \rightarrow \mathbf{R}^k$ that extends $\phi^{-1}$. Then $\Phi'\circ\phi$ is the identity map of U, so the chain rule implies that the sequence of linear transformations $\mathbf{R}^k\xrightarrow{d\phi_0}T_x(X)\xrightarrow{d\Phi_x'}\mathbf{R}^k$ is the identity map of $\mathbf{R}^k$. It follows that $d\phi_0 :\mathbf{R}^k \rightarrow T_x(X)$ is an isomorphism, so $dim T_x(X)=k$."" Just for everyone's information, in GP's book, the convention is that $\phi$ is a diffeomorphism from $\mathbf{R}^k$ to a k-dimensional manifold $X\subset \mathbf{R}^N$ ($\phi :\mathbf{R}^k\rightarrow X$). I have two questions here: why do we need to choose an open set $W$ in $\mathbf{R}^N$ and define $\Phi'$ which extends $\phi^{-1}$? Naively I would just take $\phi^{-1}$. Then $d\phi^{-1}$ can also map $T_x(X)$ to $\mathbf{R}^k$ without problems. The map $\Phi'$ seems a bit artificial and redundant here. In fact it seems really redundant to me as when you extend $\phi^{-1}$, you actually don't care the rest of the function except for the $\phi^{-1}$ part. The proof seems only to serve as a proof that dim $T_x(X)=k$. However we know the tangent space itself is a manifold(actually a hyperplane). Why don't we find a diffeomorphism between $T_x(X)$ and $\mathbf{R}^k$? I haven't proved but I think we can use $d\phi_0$ as the diffeomorphism. Thanks a lot for everyone's help! Regards, Evariste","In section 2, page 9 of Guillemin and Pollack's book $\textit{Differential Topology}$, he gave a proof that the dimension of the tangent space $T_x(X)$ is equal to the dimension of the manifold $X$. However I read the proof at least 5 times and had no idea what they were talking about. ""The dimension of the vector space $T_x(X)$ is, as you expect, the dimension $k$ of $X$. To prove this, we use the smoothness of he inverse $\phi^{-1}$. Choose an open set $W$ in $\mathbf{R}^N$ and a smooth map $\Phi': \mathbf{R}^N \rightarrow \mathbf{R}^k$ that extends $\phi^{-1}$. Then $\Phi'\circ\phi$ is the identity map of U, so the chain rule implies that the sequence of linear transformations $\mathbf{R}^k\xrightarrow{d\phi_0}T_x(X)\xrightarrow{d\Phi_x'}\mathbf{R}^k$ is the identity map of $\mathbf{R}^k$. It follows that $d\phi_0 :\mathbf{R}^k \rightarrow T_x(X)$ is an isomorphism, so $dim T_x(X)=k$."" Just for everyone's information, in GP's book, the convention is that $\phi$ is a diffeomorphism from $\mathbf{R}^k$ to a k-dimensional manifold $X\subset \mathbf{R}^N$ ($\phi :\mathbf{R}^k\rightarrow X$). I have two questions here: why do we need to choose an open set $W$ in $\mathbf{R}^N$ and define $\Phi'$ which extends $\phi^{-1}$? Naively I would just take $\phi^{-1}$. Then $d\phi^{-1}$ can also map $T_x(X)$ to $\mathbf{R}^k$ without problems. The map $\Phi'$ seems a bit artificial and redundant here. In fact it seems really redundant to me as when you extend $\phi^{-1}$, you actually don't care the rest of the function except for the $\phi^{-1}$ part. The proof seems only to serve as a proof that dim $T_x(X)=k$. However we know the tangent space itself is a manifold(actually a hyperplane). Why don't we find a diffeomorphism between $T_x(X)$ and $\mathbf{R}^k$? I haven't proved but I think we can use $d\phi_0$ as the diffeomorphism. Thanks a lot for everyone's help! Regards, Evariste",,"['real-analysis', 'general-topology', 'analysis', 'differential-geometry', 'differential-topology']"
25,"$C[0,1]$ is complete w.r.t. which norm(s)",is complete w.r.t. which norm(s),"C[0,1]","$C[0,1]$ is complete w.r.t. which norm(s) $\displaystyle\|f\|_\infty=\sup_{t\in[0,1]}|f(t)|$ $\displaystyle\|f\|_1=\int_0^1|f(t)| \, dt$ $\displaystyle\|f\|_\infty^{0,1}=\|f\|_\infty+|f(0)|+|f(1)|$ $\displaystyle\|f\|=\sqrt{\int_0^1|f(t)| \, dt}$ I know 1 is true. But I don't know how to check completeness of norm base on know complete norm. How can I solve it?","$C[0,1]$ is complete w.r.t. which norm(s) $\displaystyle\|f\|_\infty=\sup_{t\in[0,1]}|f(t)|$ $\displaystyle\|f\|_1=\int_0^1|f(t)| \, dt$ $\displaystyle\|f\|_\infty^{0,1}=\|f\|_\infty+|f(0)|+|f(1)|$ $\displaystyle\|f\|=\sqrt{\int_0^1|f(t)| \, dt}$ I know 1 is true. But I don't know how to check completeness of norm base on know complete norm. How can I solve it?",,"['real-analysis', 'normed-spaces']"
26,Does $f\colon \Omega \to \mathbb R$ differentiable imply $f$ locally Lipschitz?,Does  differentiable imply  locally Lipschitz?,f\colon \Omega \to \mathbb R f,"Let $f\colon \Omega \subseteq \mathbb R^n \to \mathbb R$ be a differentiable function. Is it true that $f$ is locally Lipschitz, i.e. Lipschitz on every compact $K \subset \Omega$? If $f$ were continuously differentiable, the answer would clearly be affirmative, by the mean value theorem and Weierstrass. What if we ask only for $f$ to be differentiable? I think I've found a counterexample with $n=1$: $$ f \colon \mathbb R \ni x \mapsto \begin{cases}x^2\sin\left(\frac{1}{x^2}\right) & x\ne 0 \\ 0 & x=0 \end{cases} $$ The function $f$ is differentiable for every $x \in \mathbb R$, but I think that $$ f'(x) = \begin{cases} 2x\sin\left(\frac{1}{x^2}\right) - \frac{2}{x}\cos\left(\frac{1}{x^2}\right) & x\ne 0 \\ 0 & x=0 \end{cases} $$ is not bounded in every neighbourhood of $0$. What do you think? Is my counterexample correct? Thanks.","Let $f\colon \Omega \subseteq \mathbb R^n \to \mathbb R$ be a differentiable function. Is it true that $f$ is locally Lipschitz, i.e. Lipschitz on every compact $K \subset \Omega$? If $f$ were continuously differentiable, the answer would clearly be affirmative, by the mean value theorem and Weierstrass. What if we ask only for $f$ to be differentiable? I think I've found a counterexample with $n=1$: $$ f \colon \mathbb R \ni x \mapsto \begin{cases}x^2\sin\left(\frac{1}{x^2}\right) & x\ne 0 \\ 0 & x=0 \end{cases} $$ The function $f$ is differentiable for every $x \in \mathbb R$, but I think that $$ f'(x) = \begin{cases} 2x\sin\left(\frac{1}{x^2}\right) - \frac{2}{x}\cos\left(\frac{1}{x^2}\right) & x\ne 0 \\ 0 & x=0 \end{cases} $$ is not bounded in every neighbourhood of $0$. What do you think? Is my counterexample correct? Thanks.",,"['real-analysis', 'analysis', 'derivatives']"
27,Continuous functions with a certain condition,Continuous functions with a certain condition,,What is the class of continuous functions $f\colon \mathbb{R}\to\mathbb{R}$ which satisfy $f(x)-f(y)\in\mathbb{Q}$ if and only if $x-y\in \mathbb{Q}$?,What is the class of continuous functions $f\colon \mathbb{R}\to\mathbb{R}$ which satisfy $f(x)-f(y)\in\mathbb{Q}$ if and only if $x-y\in \mathbb{Q}$?,,"['real-analysis', 'continuity']"
28,Limit comparison test proof,Limit comparison test proof,,"We had the following theorem in class: Let $(a_n)$ and $(b_n)$ be sequences and $b_n>0$ and $\lim_{n\rightarrow\infty}\frac{a_n}{b_n}=L$ with $L\in\mathbb R\backslash\{0\}$. Then $\sum a_n$ converges if and only if $\sum b_n$ converges. So by recapitulating the lecture I've tried to prove it but I didn't get it. It's obvious that you have to use the comparison test, but how? Can anybody help? Thanks a lot!","We had the following theorem in class: Let $(a_n)$ and $(b_n)$ be sequences and $b_n>0$ and $\lim_{n\rightarrow\infty}\frac{a_n}{b_n}=L$ with $L\in\mathbb R\backslash\{0\}$. Then $\sum a_n$ converges if and only if $\sum b_n$ converges. So by recapitulating the lecture I've tried to prove it but I didn't get it. It's obvious that you have to use the comparison test, but how? Can anybody help? Thanks a lot!",,"['real-analysis', 'sequences-and-series']"
29,Why uniform closure $\mathscr{B}$ of an algebra $\mathscr{A}$ of bounded complex functions is uniformly closed?,Why uniform closure  of an algebra  of bounded complex functions is uniformly closed?,\mathscr{B} \mathscr{A},"Let $\mathscr{A}$ be an algebra of bounded complex functions. (Or if necessary, continuous and domain of functions is compact) Definition: $\mathscr{B}$ is uniformly closed iff $f\in\mathscr{B}$ whenever $f_n\in \mathscr{B} (n=1,2,\cdot)$ and $f_n\rightarrow f$ uniformly. $\mathscr{B}$ is the uniform closure of $\mathscr{A}$ iff $\mathscr{B}$ is the set of all functions which are limits of uniformly convergent sequences of members of $\mathscr{A}$. ============ Let $\mathscr{B}$ be a uniform closure of $\mathscr{A}$. How do i prove that $\mathscr{B}$ is uniformly closed in ZF? Does Stone-Weierstrass theorem require choice since it is critical to prove Stone-Weierstrass Theorem?","Let $\mathscr{A}$ be an algebra of bounded complex functions. (Or if necessary, continuous and domain of functions is compact) Definition: $\mathscr{B}$ is uniformly closed iff $f\in\mathscr{B}$ whenever $f_n\in \mathscr{B} (n=1,2,\cdot)$ and $f_n\rightarrow f$ uniformly. $\mathscr{B}$ is the uniform closure of $\mathscr{A}$ iff $\mathscr{B}$ is the set of all functions which are limits of uniformly convergent sequences of members of $\mathscr{A}$. ============ Let $\mathscr{B}$ be a uniform closure of $\mathscr{A}$. How do i prove that $\mathscr{B}$ is uniformly closed in ZF? Does Stone-Weierstrass theorem require choice since it is critical to prove Stone-Weierstrass Theorem?",,"['real-analysis', 'axiom-of-choice']"
30,"Iterated Integrals - ""Counterexample"" to Fubini's Theorem","Iterated Integrals - ""Counterexample"" to Fubini's Theorem",,"Showing the iterated integrals $$\int_{[0,1]}\left[\int_{[0,1]}\frac{x^2-y^2}{(x^2+y^2)^2}dx\right]dy\quad\text{and}\quad\int_{[0,1]}\left[\int_{[0,1]}\frac{x^2-y^2}{(x^2+y^2)^2}dy\right]dx$$ are different isn't too hard, so this certainly implies $f(x,y)\notin L^1([0,1]^2)$ (otherwise it would be a counterexample to Fubini's Theorem), so how do we show that $$\iint_{[0,1]^2}|f|\;dx\:dy=\infty?$$ I've been thinking about this, but my intuition always leads me back to iterated integrals. Any help would be appreciated in guiding me in which direction I should take.","Showing the iterated integrals $$\int_{[0,1]}\left[\int_{[0,1]}\frac{x^2-y^2}{(x^2+y^2)^2}dx\right]dy\quad\text{and}\quad\int_{[0,1]}\left[\int_{[0,1]}\frac{x^2-y^2}{(x^2+y^2)^2}dy\right]dx$$ are different isn't too hard, so this certainly implies $f(x,y)\notin L^1([0,1]^2)$ (otherwise it would be a counterexample to Fubini's Theorem), so how do we show that $$\iint_{[0,1]^2}|f|\;dx\:dy=\infty?$$ I've been thinking about this, but my intuition always leads me back to iterated integrals. Any help would be appreciated in guiding me in which direction I should take.",,"['calculus', 'real-analysis', 'measure-theory']"
31,Discontinuous functions that are continuous on every line in ${\bf R}^2$,Discontinuous functions that are continuous on every line in,{\bf R}^2,"This is exercise 7 from chapter 4 of Walter Rudin Principles of Mathematical Analysis , 3rd edition. (Page 99) Define $f$ and $g$ on ${\bf R}^2$ by: $$f(x,y) = \cases {0,&if $(x,y)=(0,0)$\\ xy^2/(x^2+y^4) &otherwise}$$ $$g(x,y) = \cases {0,&if $(x,y)=(0,0)$\\ xy^2/(x^2+y^6) &otherwise}$$ Prove that: $f$ is bounded on ${\bf R}^2$ $g$ is unbounded in every neighborhood of $(0,0)$ $f$ is not continuous at $(0,0)$ Nevertheless, the restrictions of both $f$ and $g$ to every straight line in ${\bf R}^2$ are continuous! This is one of the few specific problems I remember from my university career, which ended some time ago. I remember it because I toiled over it for so long and when I finally found the answer it seemed so simple.","This is exercise 7 from chapter 4 of Walter Rudin Principles of Mathematical Analysis , 3rd edition. (Page 99) Define $f$ and $g$ on ${\bf R}^2$ by: $$f(x,y) = \cases {0,&if $(x,y)=(0,0)$\\ xy^2/(x^2+y^4) &otherwise}$$ $$g(x,y) = \cases {0,&if $(x,y)=(0,0)$\\ xy^2/(x^2+y^6) &otherwise}$$ Prove that: $f$ is bounded on ${\bf R}^2$ $g$ is unbounded in every neighborhood of $(0,0)$ $f$ is not continuous at $(0,0)$ Nevertheless, the restrictions of both $f$ and $g$ to every straight line in ${\bf R}^2$ are continuous! This is one of the few specific problems I remember from my university career, which ended some time ago. I remember it because I toiled over it for so long and when I finally found the answer it seemed so simple.",,"['real-analysis', 'continuity']"
32,Why convolution of integrable function $f$ with some sequence tends to $f$ a.e.,Why convolution of integrable function  with some sequence tends to  a.e.,f f,"Let $g: \mathbb{R} \rightarrow \mathbb{R}$ be integrable with$\int_{\mathbb{R}}g(x)dx=1$ and $|g(x)| \leq \frac{C}{(1+|x|)^{1+h}}$ for $x \in \mathbb{R} $, where $C, h>0$ are constants. Let $g_t(x)=\frac{1}{t} g(\frac{x}{t})$ for $x \in \mathbb{R}$, $t>0$. I want  to show that: If $f\in L^p$, where $1\leq p\leq \infty$, then $f*g_t(x) \rightarrow f(x)$ a.e. I have tried in this way: Let $x\in \mathbb{R}$ be the Lebesgue point of $f$, that is $lim_{r\rightarrow 0} \frac{1}{r} \int_{B(x,r)} |f(y)-f(x)|dx=0$, then $$ |f*g_t(x)-f(x)|\leq \int_{\mathbb{R}} g_t(x-y)|f(y)-f(x)|dy =I_1+I_2, $$ where $I_1=\int_{B(x,t)} g_t(x-y)|f(y)-f(x)|dy \leq\frac{1}{t} \int_{B(x,t)}  \frac{C}{(1+\|\frac{x-y}{t}\|)^{1+h}} |f(y)-f(x)|dy $ $ \leq C\frac{1}{t}\int_{B(x,t)} |f(y)-f(x)|dy \rightarrow 0 \ as \ t \rightarrow 0;$ $I_2=\int_{\mathbb{R}\setminus B(x,t)} g_t(x-y)|f(y)-f(x)|dy .$ I don't know how to estimate the integral $I_2$.","Let $g: \mathbb{R} \rightarrow \mathbb{R}$ be integrable with$\int_{\mathbb{R}}g(x)dx=1$ and $|g(x)| \leq \frac{C}{(1+|x|)^{1+h}}$ for $x \in \mathbb{R} $, where $C, h>0$ are constants. Let $g_t(x)=\frac{1}{t} g(\frac{x}{t})$ for $x \in \mathbb{R}$, $t>0$. I want  to show that: If $f\in L^p$, where $1\leq p\leq \infty$, then $f*g_t(x) \rightarrow f(x)$ a.e. I have tried in this way: Let $x\in \mathbb{R}$ be the Lebesgue point of $f$, that is $lim_{r\rightarrow 0} \frac{1}{r} \int_{B(x,r)} |f(y)-f(x)|dx=0$, then $$ |f*g_t(x)-f(x)|\leq \int_{\mathbb{R}} g_t(x-y)|f(y)-f(x)|dy =I_1+I_2, $$ where $I_1=\int_{B(x,t)} g_t(x-y)|f(y)-f(x)|dy \leq\frac{1}{t} \int_{B(x,t)}  \frac{C}{(1+\|\frac{x-y}{t}\|)^{1+h}} |f(y)-f(x)|dy $ $ \leq C\frac{1}{t}\int_{B(x,t)} |f(y)-f(x)|dy \rightarrow 0 \ as \ t \rightarrow 0;$ $I_2=\int_{\mathbb{R}\setminus B(x,t)} g_t(x-y)|f(y)-f(x)|dy .$ I don't know how to estimate the integral $I_2$.",,['real-analysis']
33,Prove that $f_n$ converges to $f$ in $L_1$ norm given $\int f_n \to \int f$,Prove that  converges to  in  norm given,f_n f L_1 \int f_n \to \int f,"a homework question from measure and integraiton theory course. Suppose $f_n \in L_1(\mathbb R^d)$ for each $n\in\mathbb N$ ， $f_n\geq 0$ ， $f_n\to f$ a.e  and $\int f_n\to\int f<\infty$ . Prove that $\int|f_n-f| \to 0$ (Hint: ( $f_n - f)_-\leq f$ . Use dominated convergence theorem ) I am thinking that since $|f_n -f | \to 0$ a.e and $|f_n-f|=(f_n - f)_+ + (fn-f)_-$ . If I can show $(f_n-f)_+ ≤ f$ and $(f_n-f)_-≤f$ . Then since $f_n$ and $f$ are integrable , by DCT, $\int|f_n-f| \to \int0 =0$ .  I think my approach might be wrong ..","a homework question from measure and integraiton theory course. Suppose for each ， ， a.e  and . Prove that (Hint: ( . Use dominated convergence theorem ) I am thinking that since a.e and . If I can show and . Then since and are integrable , by DCT, .  I think my approach might be wrong ..",f_n \in L_1(\mathbb R^d) n\in\mathbb N f_n\geq 0 f_n\to f \int f_n\to\int f<\infty \int|f_n-f| \to 0 f_n - f)_-\leq f |f_n -f | \to 0 |f_n-f|=(f_n - f)_+ + (fn-f)_- (f_n-f)_+ ≤ f (f_n-f)_-≤f f_n f \int|f_n-f| \to \int0 =0,"['real-analysis', 'measure-theory', 'integration']"
34,Extension of partial derivatives and the definition of $C^k(\overline{\Omega})$,Extension of partial derivatives and the definition of,C^k(\overline{\Omega}),"The following is an excerpt from Folland's Introduction to Partial Differential Equations : Let the open set $\Omega\subset{\mathbb R}^n$, and $k$ be a positive integer. $C^k(\Omega)$ will denote the space of functions possessing continuous derivatives up to order $k$ on $\Omega$, and $C^k(\overline{\Omega})$ will denote the space of all $u\in C^k(\Omega)$ such that $\partial^{\alpha}u$ extends continuously to the closure $\overline{\Omega}$ for $0\leq|\alpha|\leq k$. As I understand, an extension means that there exists $\widetilde{\partial^{\alpha}u}$ which is defined on $\overline{\Omega}$ and $\widetilde{\partial^{\alpha}u}|_{\Omega}=\partial^{\alpha}u$. And ""extends continuously"" means $\widetilde{\partial^{\alpha}u}$ is continuous with respect to the relative topology on $\overline{\Omega}$. Here are my questions : How do people usually do such extension? When does such extension exist and when does not? Let $\Omega$ be an open subset of ${\mathbb R}$. Is there any   non-trivial counterexample such that   this kind of extension does not exist?","The following is an excerpt from Folland's Introduction to Partial Differential Equations : Let the open set $\Omega\subset{\mathbb R}^n$, and $k$ be a positive integer. $C^k(\Omega)$ will denote the space of functions possessing continuous derivatives up to order $k$ on $\Omega$, and $C^k(\overline{\Omega})$ will denote the space of all $u\in C^k(\Omega)$ such that $\partial^{\alpha}u$ extends continuously to the closure $\overline{\Omega}$ for $0\leq|\alpha|\leq k$. As I understand, an extension means that there exists $\widetilde{\partial^{\alpha}u}$ which is defined on $\overline{\Omega}$ and $\widetilde{\partial^{\alpha}u}|_{\Omega}=\partial^{\alpha}u$. And ""extends continuously"" means $\widetilde{\partial^{\alpha}u}$ is continuous with respect to the relative topology on $\overline{\Omega}$. Here are my questions : How do people usually do such extension? When does such extension exist and when does not? Let $\Omega$ be an open subset of ${\mathbb R}$. Is there any   non-trivial counterexample such that   this kind of extension does not exist?",,['real-analysis']
35,Positive definite Hessians from strictly convex functions,Positive definite Hessians from strictly convex functions,,"Let $f: D \to \mathbb{R}\ $ be a function on non-singular, convex domain $D \subseteq \mathbb{R}^d$ and let us assume the second-order derivatives of $f$ exist. It is well known that $f$ is convex if and only if its Hessian $\nabla^2 f(x)$ is positive semi-definite for all $x \in D$. It is also known that if $\nabla^2 f(x)$ is positive definite for all $x \in D$, we may conclude that $f$ is strictly convex (for a reference, see Boyd and Vandenberghe, 2004 ). On the other hand, if $f$ is strictly convex, we still merely know that $\nabla^2 f(x)$ is positive semi-definite for all $x \in D$. That is, there may be $x \in D$ such that $y^T \nabla^2 f(x) y = 0\ $ for some $y\not=0$. As an example, consider $f(x)=x^4$. In this case, $f$ is strictly convex, but $f''(x)=12x^2$ and, hence, $yf''(x)y=0$ for $x=0$ and $yf''(x)y>0$ for all $x\not=0$. Yet, these points that ruin the complete positive definiteness seem to be very sparsely distributed within $D$. So, my question is as follows: If $f$ is strictly convex, how can we characterize the set of points $X$ for which   $\nabla^2f(x)$ is not positive definite for $x \in X$, and    $\nabla^2f(x)$ is positive definite for $x \in D \setminus X$? That is, has such a set $X$ been investigated before, what properties are known, and where can I learn more about it? Any reference is welcome. In particular, my guess is that on can state the following: Conjecture 1: The set $X$ is merely a discrete subset of $D$. EDIT: Since Conjecture 1 has obviously been disproved by George Lowther below, allow me to restate my guess as the following (less bold) statement: Conjecture 2: The set $X$ is does not contain a non-empty open ball. or, even more cautious: Conjecture 3: The set $D \setminus X$ does contain a non-empty open ball.","Let $f: D \to \mathbb{R}\ $ be a function on non-singular, convex domain $D \subseteq \mathbb{R}^d$ and let us assume the second-order derivatives of $f$ exist. It is well known that $f$ is convex if and only if its Hessian $\nabla^2 f(x)$ is positive semi-definite for all $x \in D$. It is also known that if $\nabla^2 f(x)$ is positive definite for all $x \in D$, we may conclude that $f$ is strictly convex (for a reference, see Boyd and Vandenberghe, 2004 ). On the other hand, if $f$ is strictly convex, we still merely know that $\nabla^2 f(x)$ is positive semi-definite for all $x \in D$. That is, there may be $x \in D$ such that $y^T \nabla^2 f(x) y = 0\ $ for some $y\not=0$. As an example, consider $f(x)=x^4$. In this case, $f$ is strictly convex, but $f''(x)=12x^2$ and, hence, $yf''(x)y=0$ for $x=0$ and $yf''(x)y>0$ for all $x\not=0$. Yet, these points that ruin the complete positive definiteness seem to be very sparsely distributed within $D$. So, my question is as follows: If $f$ is strictly convex, how can we characterize the set of points $X$ for which   $\nabla^2f(x)$ is not positive definite for $x \in X$, and    $\nabla^2f(x)$ is positive definite for $x \in D \setminus X$? That is, has such a set $X$ been investigated before, what properties are known, and where can I learn more about it? Any reference is welcome. In particular, my guess is that on can state the following: Conjecture 1: The set $X$ is merely a discrete subset of $D$. EDIT: Since Conjecture 1 has obviously been disproved by George Lowther below, allow me to restate my guess as the following (less bold) statement: Conjecture 2: The set $X$ is does not contain a non-empty open ball. or, even more cautious: Conjecture 3: The set $D \setminus X$ does contain a non-empty open ball.",,"['real-analysis', 'convex-analysis']"
36,"What does an ""exotic"" derivation at a point $x_{0}\in \mathbb{R}^{n}$ look like?","What does an ""exotic"" derivation at a point  look like?",x_{0}\in \mathbb{R}^{n},"for $x_{0}\in \mathbb{R}^{n}$ I denote by $G^{k}(x_{0})\: (1\le k\le \infty )$ the $\mathbb{R}$ -algebra of germs of real-valued $C^{k}$ -functions at $x_{0}$ . Recall that a derivation at $x_{0}$ is a linear form $D:G^{k}(x_{0})\to\mathbb{R}$ which satisfies the product rule $D(f\cdot g)=f(x_{0})\cdot Dg+g(x_{0})\cdot Df$ . I call a differentiable function $c:\mathbb{R\to \mathbb{R}^{n}}$ with $c(0)=x_{0}$ a curve at $x_{0}$ . Recall that each such curve defines  a derivation $D_{c}$ at $x_{0}$ by $D_{c}(f)=\frac{d }{dt}(f\circ c)_{|t=0}$ (where $f$ is any representative of the respective germ), called the derivative in the direction of the tangent to $c$ at $x_{0}$ .The well-known (and not hard to prove) fact that in the case $k=\infty $ every derivation at $x_{0}$ is actually a directional derivative is sometimes used to define the tangent space at a point of a $C^{\infty }$ -manifold. But already as a student I was baffled  to learn that this does not work for $k\lt \infty $ : It has been proved that the vector space of all derivations is infinite-dimensional in this case. Just recently I have studied a rather short proof of this but have not been able to deduce a definite example for such an ""exotic"" derivation which is not a directional derivative. Can anybody present such an example here? And I would like to know: are there derivations $\neq 0$ on $G^{0}(x_{0})$ ?","for I denote by the -algebra of germs of real-valued -functions at . Recall that a derivation at is a linear form which satisfies the product rule . I call a differentiable function with a curve at . Recall that each such curve defines  a derivation at by (where is any representative of the respective germ), called the derivative in the direction of the tangent to at .The well-known (and not hard to prove) fact that in the case every derivation at is actually a directional derivative is sometimes used to define the tangent space at a point of a -manifold. But already as a student I was baffled  to learn that this does not work for : It has been proved that the vector space of all derivations is infinite-dimensional in this case. Just recently I have studied a rather short proof of this but have not been able to deduce a definite example for such an ""exotic"" derivation which is not a directional derivative. Can anybody present such an example here? And I would like to know: are there derivations on ?",x_{0}\in \mathbb{R}^{n} G^{k}(x_{0})\: (1\le k\le \infty ) \mathbb{R} C^{k} x_{0} x_{0} D:G^{k}(x_{0})\to\mathbb{R} D(f\cdot g)=f(x_{0})\cdot Dg+g(x_{0})\cdot Df c:\mathbb{R\to \mathbb{R}^{n}} c(0)=x_{0} x_{0} D_{c} x_{0} D_{c}(f)=\frac{d }{dt}(f\circ c)_{|t=0} f c x_{0} k=\infty  x_{0} C^{\infty } k\lt \infty  \neq 0 G^{0}(x_{0}),"['real-analysis', 'manifolds']"
37,"Suppose $f(t) = f(-t)$ and $f \in C^\infty$, show that there's a $h(t) \in C^\infty$ with $h(t^2) = f(t)$","Suppose  and , show that there's a  with",f(t) = f(-t) f \in C^\infty h(t) \in C^\infty h(t^2) = f(t),"So the hint of the exercise is that I should use Taylor's theorem (I'm not assuming $f$ is analytic). Then it's clear that all odd derivatives of $f$ at $0$ is $0$ so in some sense $h$ can be defined by the Taylor coefficients $h_k = f_{2k}$ . More generally, can I show that if $f \in C^{2k}$ and is even, then there exists $h \in C^k$ with $h(t^2) = f(t)$ ?","So the hint of the exercise is that I should use Taylor's theorem (I'm not assuming is analytic). Then it's clear that all odd derivatives of at is so in some sense can be defined by the Taylor coefficients . More generally, can I show that if and is even, then there exists with ?",f f 0 0 h h_k = f_{2k} f \in C^{2k} h \in C^k h(t^2) = f(t),"['real-analysis', 'calculus']"
38,How might one make this Gaussian integral derivation of Stirling's approximation more airtight?,How might one make this Gaussian integral derivation of Stirling's approximation more airtight?,,"I will be following an argument presented by the YouTuber Flammable Maths - which is in fairness not their own, they're just giving simplified content to their audience! Their derivation is appealingly intuitive, but I definitely feel there is some rigour lacking - however I am not able to follow the only other proof I have seen, so I will be trying to improve upon the argument they make. We want to show: $$z!\sim z^ze^{-z}\cdot\sqrt{2\pi z}$$ We begin with: $$\begin{align}z!=\Gamma(z+1)&=\int_0^\infty e^{-t}t^z\,dt=\int_0^\infty\exp(z\ln(t)-t)\,dt\\&=\int_{-1}^\infty\exp(z\ln(z(1+x))-z(1+x))\cdot z\,dx\\&=z^{z+1}e^{-z}\int_{-1}^\infty\exp(z\cdot(\ln(1+x)-x))\,dz\end{align}$$ So far, no problems at all. However, these next lines of argument (paraphrased by me) feel... wishy-washy: As the expression $\ln(1+x)-x\lt0$ for all non-zero $x$ , the integrand is $\exp(-s)$ for some large quantity $s$ , as $z\to\infty$ . On the interval $(-1,0)$ , $\ln(1+x)-x$ diverges rapidly to $-\infty$ , even more so in multiplication by large $z$ , and hence the exponential decays quickly to $0$ . Likewise, on the interval $(0,\infty)$ the integrand goes sharply to $0$ . We may then consider the integrand as non-negligible for $x$ in a small neighbourhood of $0$ , and as such the Maclaurin expansion of $\ln(1+x)$ may be used to get the integrand as $\exp(-z\cdot\frac{x^2}{2})\cdot\exp(-z\cdot O(x^3))$ as $x\to0$ . As $z$ gets larger, the neighbourhood in which the integrand is non-negligible gets smaller, and thus the $O(x^3)$ term vanishes. We now obtain: $$z!\approx z^{z+1}e^{-z}\int_{-\infty}^\infty\exp\left(-z\cdot\frac{x^2}{2}\right)\,dx$$ For large $z$ , extending the lower bound to $-\infty$ as that extra area is again negligible. The Gaussian integral now returns $z!\approx z^{z+1}e^{-z}\sqrt{2\pi/z}=z^ze^{-z}\sqrt{2\pi z}$ There are many problems here, in my view. We first are discarding the majority of the range of integration, invoking a Maclaurin approximation but discarding the $O(x^3)$ term without proof that this does not affect things (for example, could I not simply include the $x^3$ term as well, losing the beloved Gaussian integral, and then truncate at $x^4$ instead), and then re-instating a much larger range of integration - the entire real line - of a different function . We also have not shown that the negligible portions of the integral remain negligible even under multiplication by $z^{z+1}e^{-z}$ . We are also missing the important aspect of asymptotic equivalence: we have only (heuristically) shown that $z!\in\Theta(z^ze^{-z}\sqrt{2\pi z})$ . The curious thing is, all the steps I balk at are in fact, as visually corroborated by Desmos, fairly good approximations anyway - but I have no idea how to show that, as there are too many moving parts and shortcuts in this argument for me to handle. My question: is the argument rigorously salvageable, or should I give up and look to another approach? Many thanks.","I will be following an argument presented by the YouTuber Flammable Maths - which is in fairness not their own, they're just giving simplified content to their audience! Their derivation is appealingly intuitive, but I definitely feel there is some rigour lacking - however I am not able to follow the only other proof I have seen, so I will be trying to improve upon the argument they make. We want to show: We begin with: So far, no problems at all. However, these next lines of argument (paraphrased by me) feel... wishy-washy: As the expression for all non-zero , the integrand is for some large quantity , as . On the interval , diverges rapidly to , even more so in multiplication by large , and hence the exponential decays quickly to . Likewise, on the interval the integrand goes sharply to . We may then consider the integrand as non-negligible for in a small neighbourhood of , and as such the Maclaurin expansion of may be used to get the integrand as as . As gets larger, the neighbourhood in which the integrand is non-negligible gets smaller, and thus the term vanishes. We now obtain: For large , extending the lower bound to as that extra area is again negligible. The Gaussian integral now returns There are many problems here, in my view. We first are discarding the majority of the range of integration, invoking a Maclaurin approximation but discarding the term without proof that this does not affect things (for example, could I not simply include the term as well, losing the beloved Gaussian integral, and then truncate at instead), and then re-instating a much larger range of integration - the entire real line - of a different function . We also have not shown that the negligible portions of the integral remain negligible even under multiplication by . We are also missing the important aspect of asymptotic equivalence: we have only (heuristically) shown that . The curious thing is, all the steps I balk at are in fact, as visually corroborated by Desmos, fairly good approximations anyway - but I have no idea how to show that, as there are too many moving parts and shortcuts in this argument for me to handle. My question: is the argument rigorously salvageable, or should I give up and look to another approach? Many thanks.","z!\sim z^ze^{-z}\cdot\sqrt{2\pi z} \begin{align}z!=\Gamma(z+1)&=\int_0^\infty e^{-t}t^z\,dt=\int_0^\infty\exp(z\ln(t)-t)\,dt\\&=\int_{-1}^\infty\exp(z\ln(z(1+x))-z(1+x))\cdot z\,dx\\&=z^{z+1}e^{-z}\int_{-1}^\infty\exp(z\cdot(\ln(1+x)-x))\,dz\end{align} \ln(1+x)-x\lt0 x \exp(-s) s z\to\infty (-1,0) \ln(1+x)-x -\infty z 0 (0,\infty) 0 x 0 \ln(1+x) \exp(-z\cdot\frac{x^2}{2})\cdot\exp(-z\cdot O(x^3)) x\to0 z O(x^3) z!\approx z^{z+1}e^{-z}\int_{-\infty}^\infty\exp\left(-z\cdot\frac{x^2}{2}\right)\,dx z -\infty z!\approx z^{z+1}e^{-z}\sqrt{2\pi/z}=z^ze^{-z}\sqrt{2\pi z} O(x^3) x^3 x^4 z^{z+1}e^{-z} z!\in\Theta(z^ze^{-z}\sqrt{2\pi z})","['real-analysis', 'asymptotics', 'approximation', 'gamma-function']"
39,How to show $\lim_{n\to\infty}n\cdot \sum_{m=1}^{\infty}\Big(1-\frac{1}{m}\Big)^n\cdot \frac{1}{m^2}=1.$,How to show,\lim_{n\to\infty}n\cdot \sum_{m=1}^{\infty}\Big(1-\frac{1}{m}\Big)^n\cdot \frac{1}{m^2}=1.,"I am wondering if the following limit is correct and how to show it. $$\lim_{n\to\infty}n\cdot \sum_{m=1}^{\infty}\Big(1-\frac{1}{m}\Big)^n\cdot \frac{1}{m^2}=1.$$ Edit: I apologize for the lacking context. My initial motivation was considering the probability that given $n+1$ randomly chosen positive integers, a prime $p$ divides at least two of them. I understand that “randomly choosing $n+1$ integers” isn’t rigorous, and this is simply introducing how I got to the infinite series above. Assuming that a prime $p$ divides an integer with probability $1/p$ and the process is independent, the probability $f(n,p)$ is $$f(n,p)=1–\Big(1-\frac{1}{p}\Big)^n\Big(1+\frac{n}{p}\Big).$$ Interestingly, summing $f(n,p)$ over all primes and observing the plot on mathematica, it seems to bound the prime-counting function $\pi(n)$ from below and I conjectured $\sum_{p}f(n,p) \sim \pi(n).$ Naturally, I also considered $f(n+1,p)-f(n,p),$ which is $$n\Big(1-\frac{1}{p}\Big)^n\cdot \frac{1}{p^2}.$$ Noting that the logarithmic integral approximates the prime-counting function and again comparing plots on mathematica, I conjectured the above formula summed over all primes asymptotically equals $1/\ln(n).$ Finally, to consider a simpler version, I summed the above formula over all positive integers $m$ , which is the infinite series in my original question. Again based on mathematica and “feeling” based off the prime number theorem, I guessed the limit to be 1 as $n$ grows to infinity.","I am wondering if the following limit is correct and how to show it. Edit: I apologize for the lacking context. My initial motivation was considering the probability that given randomly chosen positive integers, a prime divides at least two of them. I understand that “randomly choosing integers” isn’t rigorous, and this is simply introducing how I got to the infinite series above. Assuming that a prime divides an integer with probability and the process is independent, the probability is Interestingly, summing over all primes and observing the plot on mathematica, it seems to bound the prime-counting function from below and I conjectured Naturally, I also considered which is Noting that the logarithmic integral approximates the prime-counting function and again comparing plots on mathematica, I conjectured the above formula summed over all primes asymptotically equals Finally, to consider a simpler version, I summed the above formula over all positive integers , which is the infinite series in my original question. Again based on mathematica and “feeling” based off the prime number theorem, I guessed the limit to be 1 as grows to infinity.","\lim_{n\to\infty}n\cdot \sum_{m=1}^{\infty}\Big(1-\frac{1}{m}\Big)^n\cdot \frac{1}{m^2}=1. n+1 p n+1 p 1/p f(n,p) f(n,p)=1–\Big(1-\frac{1}{p}\Big)^n\Big(1+\frac{n}{p}\Big). f(n,p) \pi(n) \sum_{p}f(n,p) \sim \pi(n). f(n+1,p)-f(n,p), n\Big(1-\frac{1}{p}\Big)^n\cdot \frac{1}{p^2}. 1/\ln(n). m n","['real-analysis', 'limits']"
40,Sum of the series $\sum_{n=0}^{\infty} \lfloor n\sqrt{2} \rfloor x^n$?,Sum of the series ?,\sum_{n=0}^{\infty} \lfloor n\sqrt{2} \rfloor x^n,Can we find the sum of the series $\sum_{n=0}^{\infty} \lfloor n\sqrt{2} \rfloor x^n$ explicitly? The question is related to this one where sum is computed if $\sqrt{2}$ is replaced by some rational number $r$ .,Can we find the sum of the series explicitly? The question is related to this one where sum is computed if is replaced by some rational number .,\sum_{n=0}^{\infty} \lfloor n\sqrt{2} \rfloor x^n \sqrt{2} r,"['real-analysis', 'analysis', 'functions', 'irrational-numbers']"
41,"If $h$ is twice differentiable, then $|h|$ is twice differentiable except on a countable set","If  is twice differentiable, then  is twice differentiable except on a countable set",h |h|,"Let $h:\mathbb R\to\mathbb R$ be differentiable. It can be shown that $$N:=\left\{a\in\mathbb R:h(a)=0\text{ and }h'(a)\ne0\right\}$$ is countable and $|h|$ is differentiable on $\mathbb R\setminus N$ with $$|h|'(a)=\begin{cases}\displaystyle\frac{h(a)}{\left|h(a)\right|}h'(a)&\text{, if }h(a)\ne0\\0&\text{, if }h'(a)=0\end{cases}\tag1$$ for all $a\in\mathbb R$ . Assuming $h$ is twice differentiable, can we show a similar statement for the second derivative of $|h|$ , i.e. that there is a countable $N'\subseteq\mathbb R$ such that $|h|$ is twice differentiable on $\mathbb R\setminus N'$ ? EDIT : It would be enough for me, if $N'$ can be shown to have Lebesgue measure $0$ (as opposed to being even countable). Moreover, if necessary, feel free to assume that $h''$ is continuous. EDIT 2 : We already know that $|h|$ is differentiable at $a$ with $$|h|'(a)=\operatorname{sgn}(h(a))h'(a)\tag5$$ for all $a\in\left\{h\ne0\right\}\cup\left\{h'=0\right\}$ . Now, since $h$ is continuous, $\operatorname{sgn}h$ is differentiable at $a$ with $$(\operatorname{sgn}h)'(a)=0\tag6$$ for all $a\in\left\{h\ne0\right\}\cup\left\{h=0\right\}^\circ$ (see: Can we show differentiability of $\operatorname{sgn}h$ on a larger set than $\left\{h\ne0\right\}$? ). Thus, by the chain rule, $|h|$ is twice differentiable at $a$ with $$|h|''(a)=\operatorname{sgn}(h(a))h''(a)\tag7$$ for all $a\in\left\{h\ne0\right\}\cup\left\{h=0\right\}^\circ\cap\left\{h'=0\right\}$ . The complement of the latter set is $$N_0:=\left\{h=0\right\}\cap\left(\mathbb R\setminus\left\{h=0\right\}^\circ\cup\left\{h'\ne0\right\}\right)=\partial\left\{h=0\right\}\cup N.$$ However, since $\partial\left\{h=0\right\}$ doesn't need to have Lebesgue measure $0$ (please correct me if I'm wrong), we cannot conclude. (Please take note of my related question: if $h$ is twice differentiable, what is the largest set on which $|h|$ is twice differentiable? .)","Let be differentiable. It can be shown that is countable and is differentiable on with for all . Assuming is twice differentiable, can we show a similar statement for the second derivative of , i.e. that there is a countable such that is twice differentiable on ? EDIT : It would be enough for me, if can be shown to have Lebesgue measure (as opposed to being even countable). Moreover, if necessary, feel free to assume that is continuous. EDIT 2 : We already know that is differentiable at with for all . Now, since is continuous, is differentiable at with for all (see: Can we show differentiability of $\operatorname{sgn}h$ on a larger set than $\left\{h\ne0\right\}$? ). Thus, by the chain rule, is twice differentiable at with for all . The complement of the latter set is However, since doesn't need to have Lebesgue measure (please correct me if I'm wrong), we cannot conclude. (Please take note of my related question: if $h$ is twice differentiable, what is the largest set on which $|h|$ is twice differentiable? .)","h:\mathbb R\to\mathbb R N:=\left\{a\in\mathbb R:h(a)=0\text{ and }h'(a)\ne0\right\} |h| \mathbb R\setminus N |h|'(a)=\begin{cases}\displaystyle\frac{h(a)}{\left|h(a)\right|}h'(a)&\text{, if }h(a)\ne0\\0&\text{, if }h'(a)=0\end{cases}\tag1 a\in\mathbb R h |h| N'\subseteq\mathbb R |h| \mathbb R\setminus N' N' 0 h'' |h| a |h|'(a)=\operatorname{sgn}(h(a))h'(a)\tag5 a\in\left\{h\ne0\right\}\cup\left\{h'=0\right\} h \operatorname{sgn}h a (\operatorname{sgn}h)'(a)=0\tag6 a\in\left\{h\ne0\right\}\cup\left\{h=0\right\}^\circ |h| a |h|''(a)=\operatorname{sgn}(h(a))h''(a)\tag7 a\in\left\{h\ne0\right\}\cup\left\{h=0\right\}^\circ\cap\left\{h'=0\right\} N_0:=\left\{h=0\right\}\cap\left(\mathbb R\setminus\left\{h=0\right\}^\circ\cup\left\{h'\ne0\right\}\right)=\partial\left\{h=0\right\}\cup N. \partial\left\{h=0\right\} 0","['real-analysis', 'calculus', 'derivatives', 'absolute-value']"
42,Show that the following sequence converges. Please Critique my proof.,Show that the following sequence converges. Please Critique my proof.,,"The problem is as follows: Let $\{a_n\}$ be a sequence of nonnegative numbers such that $$ a_{n+1}\leq a_n+\frac{(-1)^n}{n}. $$ Show that $a_n$ converges. My (wrong) proof: Notice that $$ |a_{n+1}-a_n|\leq \left|\frac{(-1)^n}{n}\right|\leq\frac{1}{n} $$ and since it is known that $\frac{1}{n}\rightarrow 0$ as $n\rightarrow \infty$ . We see that we can arbitarily bound, $|a_{n+1}-a_n|$ . Thus, $a_n$ converges. My question: This is a question from a comprehensive exam I found and am using to review. Should I argue that we should select $N$ so that $n>N$ implies $\left|\frac{1}{n}\right|<\epsilon$ as well? Notes: Currently working on the proof.","The problem is as follows: Let be a sequence of nonnegative numbers such that Show that converges. My (wrong) proof: Notice that and since it is known that as . We see that we can arbitarily bound, . Thus, converges. My question: This is a question from a comprehensive exam I found and am using to review. Should I argue that we should select so that implies as well? Notes: Currently working on the proof.","\{a_n\} 
a_{n+1}\leq a_n+\frac{(-1)^n}{n}.
 a_n 
|a_{n+1}-a_n|\leq \left|\frac{(-1)^n}{n}\right|\leq\frac{1}{n}
 \frac{1}{n}\rightarrow 0 n\rightarrow \infty |a_{n+1}-a_n| a_n N n>N \left|\frac{1}{n}\right|<\epsilon","['real-analysis', 'sequences-and-series']"
43,Functional equation involving $f(x^4)+f(x^2)+f(x)$,Functional equation involving,f(x^4)+f(x^2)+f(x),"Find all increasing functions $f$ from positive reals to positive reals satisfying $f(x^4) + f(x^2) + f(x) = x^4 + x^2 + x$ . It's easy to show that $f(1)=1$ , and I was also able to show that $$f(x)-x = f(x^{(8^k)}) - x^{(8^k)}$$ for all integers $k$ . But where to go from here? Any hints would be much appreciated.","Find all increasing functions from positive reals to positive reals satisfying . It's easy to show that , and I was also able to show that for all integers . But where to go from here? Any hints would be much appreciated.",f f(x^4) + f(x^2) + f(x) = x^4 + x^2 + x f(1)=1 f(x)-x = f(x^{(8^k)}) - x^{(8^k)} k,"['real-analysis', 'functional-equations']"
44,Why is the plot of $f(t)=\frac{\partial}{\partial t}\left\{\sin(\sin(\pi t))\right\}$ so similar to a triangle wave?,Why is the plot of  so similar to a triangle wave?,f(t)=\frac{\partial}{\partial t}\left\{\sin(\sin(\pi t))\right\},"I was playing around the other day and I found that the function $$t\to f(t), f(t)=\frac{\partial}{\partial t}\left\{\sin(\sin(\pi t))\right\}$$ seemed to be very close to the triangle wave. Is there some intuitive explanation for this?",I was playing around the other day and I found that the function seemed to be very close to the triangle wave. Is there some intuitive explanation for this?,"t\to f(t), f(t)=\frac{\partial}{\partial t}\left\{\sin(\sin(\pi t))\right\}","['real-analysis', 'soft-question', 'fourier-analysis', 'signal-processing']"
45,How evaluate $ \int_0^\infty \frac{\ln^2\;\left|\;\tan\left(\frac{ax}{2}-\frac{\pi}{4}\right)\;\right|}{1+x^2} \;dx$,How evaluate, \int_0^\infty \frac{\ln^2\;\left|\;\tan\left(\frac{ax}{2}-\frac{\pi}{4}\right)\;\right|}{1+x^2} \;dx,"How evaluate (without using Complex analysis) $$ \int_0^\infty \frac{\ln^2\;\left|\;\tan\left(\frac{ax}{2}-\frac{\pi}{4}\right)\;\right|}{1+x^2}\; dx\quad (a\gt0)$$ My Attempt: I used the expansion of the following functions: $$ \ln\left|2\sin(x)\right| \text{ and }\ln\left|2\cos(x)\right| $$ To get the following expansion: $$ \ln\;\left|\;\tan\left(\frac{ax}{2}-\frac{\pi}{4}\right)\;\right|=2\sum_{n=1}^\infty (-1)^n \frac{\sin[(2n-1)ax]}{2n-1} $$ Then I expressed the square of the logarithmic function as follows: $$ \ln^2\;\left|\;\tan\left(\frac{ax}{2}-\frac{\pi}{4}\right)\;\right|=4\sum_{m=1}^\infty\sum_{n=1}^\infty (-1)^{m+n} \frac{\sin[(2m-1)ax]\sin[(2n-1)ax]}{(2m-1)(2n-1)} $$ And used the formula of the product of two sines, then integrated the following well known integral under the summation sign: $$ \int_0^\infty \frac{\cos(bx)}{1+x^2} \;dx$$ And expressed the final result in terms of $\; \arctan\;\left(\;e^{-a}\;\right)\; $ but this was not the right answer according to the one evaluated by Complex analysis, which is $$ \frac{{\pi}^3}{8}-2\pi\; \arctan^2\;\left(\;e^{-a}\;\right) $$ Any hint for another method or idea?","How evaluate (without using Complex analysis) My Attempt: I used the expansion of the following functions: To get the following expansion: Then I expressed the square of the logarithmic function as follows: And used the formula of the product of two sines, then integrated the following well known integral under the summation sign: And expressed the final result in terms of but this was not the right answer according to the one evaluated by Complex analysis, which is Any hint for another method or idea?", \int_0^\infty \frac{\ln^2\;\left|\;\tan\left(\frac{ax}{2}-\frac{\pi}{4}\right)\;\right|}{1+x^2}\; dx\quad (a\gt0)  \ln\left|2\sin(x)\right| \text{ and }\ln\left|2\cos(x)\right|   \ln\;\left|\;\tan\left(\frac{ax}{2}-\frac{\pi}{4}\right)\;\right|=2\sum_{n=1}^\infty (-1)^n \frac{\sin[(2n-1)ax]}{2n-1}   \ln^2\;\left|\;\tan\left(\frac{ax}{2}-\frac{\pi}{4}\right)\;\right|=4\sum_{m=1}^\infty\sum_{n=1}^\infty (-1)^{m+n} \frac{\sin[(2m-1)ax]\sin[(2n-1)ax]}{(2m-1)(2n-1)}   \int_0^\infty \frac{\cos(bx)}{1+x^2} \;dx \; \arctan\;\left(\;e^{-a}\;\right)\;   \frac{{\pi}^3}{8}-2\pi\; \arctan^2\;\left(\;e^{-a}\;\right) ,"['real-analysis', 'improper-integrals']"
46,Writing a Fake Proof for Real Analysis,Writing a Fake Proof for Real Analysis,,"mathematics community! I'm teaching a course in Real Analysis soon, and one thing I wanted to include were a few ""fake proofs"" for my students to evaluate. The research I've done hasn't turned up any fake-proofs regarding continuity. So I'll put it out to you: do any of you have a favorite ""proof"" involving continuity or a sequential criterion with a flaw in it that could potentially stump some people?","mathematics community! I'm teaching a course in Real Analysis soon, and one thing I wanted to include were a few ""fake proofs"" for my students to evaluate. The research I've done hasn't turned up any fake-proofs regarding continuity. So I'll put it out to you: do any of you have a favorite ""proof"" involving continuity or a sequential criterion with a flaw in it that could potentially stump some people?",,"['real-analysis', 'continuity', 'fake-proofs']"
47,Does $\sum_{n=1}^\infty \frac{T(n)}{2^{2^n}}$ converge?,Does  converge?,\sum_{n=1}^\infty \frac{T(n)}{2^{2^n}},"Let $T(n)$ be the number of distinct topologies on a set with $n$ elements. Does $\displaystyle\sum_{n=1}^\infty \displaystyle\frac{T(n)}{2^{2^n}}$ converge? There is not much context to this unfortunately. It's a problem I came up with myself, when counting the number of topologies on an $n$-element set for $n=2,3$ (I am a beginner in topology). I am not sure of the difficulty of this problem but any progress toward a solution would be appreciated.","Let $T(n)$ be the number of distinct topologies on a set with $n$ elements. Does $\displaystyle\sum_{n=1}^\infty \displaystyle\frac{T(n)}{2^{2^n}}$ converge? There is not much context to this unfortunately. It's a problem I came up with myself, when counting the number of topologies on an $n$-element set for $n=2,3$ (I am a beginner in topology). I am not sure of the difficulty of this problem but any progress toward a solution would be appreciated.",,"['real-analysis', 'sequences-and-series', 'combinatorics', 'general-topology']"
48,Convergence of $\sum\limits_{n=1}^{\infty}a_n$ implies convergence of $\sum\limits_{n=1}^{\infty}a_n^{\sigma_n}$ where $\sigma_n=\frac{n}{n+1}$?,Convergence of  implies convergence of  where ?,\sum\limits_{n=1}^{\infty}a_n \sum\limits_{n=1}^{\infty}a_n^{\sigma_n} \sigma_n=\frac{n}{n+1},"I really don't want spoilers for this problem, but I am wondering if my approach is correct. (I am of course looking for a solution based on ""standard"" stuff regarding series, not relying on some exotic theorem from which it may follow immediately.) Let $ a_n > 0 $ for all $ n = 1, 2, 3, \ldots$ and suppose $ \sum\limits_{n=1}^{\infty} a_n $ converges. Let $ \sigma_n = \frac{n}{n+1} $. To show that $ \sum\limits_{n=1}^{\infty} a_n^{\sigma_n} $ converges, first observe that $$ \limsup\limits_{n \to \infty} ([a_n^{\sigma_n}]^{1/n}) = \limsup\limits_{n \to \infty} (a_n^{1/(n+1)}) = \limsup\limits_{n \to \infty} (a_n^{1/n}) $$ so since $ \sum a_n $ converges, $ \limsup \sqrt[n]{a_n^{\sigma_n}} \leq 1 $. If $ \limsup (\sqrt[n]{a_n^{\sigma_n}}) < 1 $, the result follows (by the Root test). Suppose $ \limsup (\sqrt[n]{a_n^{\sigma_n}}) = 1 $ intsead. Then ( this part may be totally wrong ) since $ a_n > 0 $ and $ a_n \to 0 $, $ \lim\limits_{n \to \infty} \sqrt[n]{a_n^{\sigma_n}} = 1 $, which means $$ \lim\limits_{n \to \infty} \dfrac{1}{\sqrt[n]{a_n}} = \liminf\limits_{n \to \infty} \dfrac{1}{\sqrt[n]{a_n}} = 1. $$ Hence one can find $ N \in \mathbf{N} $ such that for all $ n \geq N $, (i) $ 0 < a_n < 1 $ and (ii) $ 1 < \dfrac{1}{a_n^{1/(n+1)}} < 2 $. Then $$ |a_n^{\sigma_n}| = \left|a_n^{1 - \tfrac{1}{n+1}}\right| = |a_n| \cdot \left|\dfrac{1}{a_n^{1/(n+1)}}\right| \leq 2|a_n| $$ so $ \sum\limits_{n=1}^{\infty} a_n^{\sigma_n} $ must converge by the Comparison test.","I really don't want spoilers for this problem, but I am wondering if my approach is correct. (I am of course looking for a solution based on ""standard"" stuff regarding series, not relying on some exotic theorem from which it may follow immediately.) Let $ a_n > 0 $ for all $ n = 1, 2, 3, \ldots$ and suppose $ \sum\limits_{n=1}^{\infty} a_n $ converges. Let $ \sigma_n = \frac{n}{n+1} $. To show that $ \sum\limits_{n=1}^{\infty} a_n^{\sigma_n} $ converges, first observe that $$ \limsup\limits_{n \to \infty} ([a_n^{\sigma_n}]^{1/n}) = \limsup\limits_{n \to \infty} (a_n^{1/(n+1)}) = \limsup\limits_{n \to \infty} (a_n^{1/n}) $$ so since $ \sum a_n $ converges, $ \limsup \sqrt[n]{a_n^{\sigma_n}} \leq 1 $. If $ \limsup (\sqrt[n]{a_n^{\sigma_n}}) < 1 $, the result follows (by the Root test). Suppose $ \limsup (\sqrt[n]{a_n^{\sigma_n}}) = 1 $ intsead. Then ( this part may be totally wrong ) since $ a_n > 0 $ and $ a_n \to 0 $, $ \lim\limits_{n \to \infty} \sqrt[n]{a_n^{\sigma_n}} = 1 $, which means $$ \lim\limits_{n \to \infty} \dfrac{1}{\sqrt[n]{a_n}} = \liminf\limits_{n \to \infty} \dfrac{1}{\sqrt[n]{a_n}} = 1. $$ Hence one can find $ N \in \mathbf{N} $ such that for all $ n \geq N $, (i) $ 0 < a_n < 1 $ and (ii) $ 1 < \dfrac{1}{a_n^{1/(n+1)}} < 2 $. Then $$ |a_n^{\sigma_n}| = \left|a_n^{1 - \tfrac{1}{n+1}}\right| = |a_n| \cdot \left|\dfrac{1}{a_n^{1/(n+1)}}\right| \leq 2|a_n| $$ so $ \sum\limits_{n=1}^{\infty} a_n^{\sigma_n} $ must converge by the Comparison test.",,"['real-analysis', 'sequences-and-series']"
49,What IS the successor function without saying $S(n) = n + 1$?,What IS the successor function without saying ?,S(n) = n + 1,"I get really frustrated that all these real analysis books and online webpages say $S(n) = n + 1$ but then say addition is defined in terms off the relationships $a + 0 = a$ and $a + S(b) = S(a+b)$. I feel like this is a bit of a circular definition because we haven't really defined $S(n)$ and I don't like labeling it ""$S(n) = n + 1$"" because that feels like a cheap way of appealing to intuition when the whole point of analysis is to rigorously define the very things we normally take for granted and find obvious so that we more accurately understand what we are and are not permitted to do with these numbers. Can we just treat it as a mapping to some other distinct element? If I look at it this way then I would end up defining axioms like this: Zero is a number. If $a$ is a number, then the successor of $a$, denoted $S(a)$, is a number. $a$ and $S(a)$ are considered distinct numbers. If two numbers have the same successors, then they themselves are equal numbers. Zero is not the successor of any other number. So I am envisioning a sort of linked-list relationship: $$0 \rightarrow \alpha \rightarrow \beta \rightarrow \gamma \rightarrow \delta \rightarrow \epsilon \rightarrow \zeta \rightarrow \eta...$$ The labels are arbitrary but I am seeing natural numbers as just nodes linked together where successor just means ""whatever this node points to"". So you wouldn't see something like $\alpha$ (or anything else) point to two different nodes, or anything pointing to $0$, or anything pointing to itself. And then if we want to compute $\beta + \gamma$ using our definition of addition we see that: $$\beta + \gamma = \beta + S(\beta) = S(\beta + \beta)$$ $$\beta + \beta = \beta + S(\alpha) = S(\beta + \alpha)$$ $$\beta + \alpha = \beta + S(0) = S(\beta + 0)$$ $$\beta + 0 = \beta$$ Combining: $$\beta + \gamma = S(S(S(\beta)))$$ Which makes sense intuitively, taking the third successor to $\beta$. And if we want to define $\beta$ relative to $0$ or its eventual node in the list we could replace it with successors and then show that $$\beta + \gamma = S(S(S(S(\alpha)))) = S(S(S(S(S(0))))) = \epsilon$$ I mean is this the right way to think of it? Am I right to find $S(n) = n + 1$ problematic or am I missing some point as to why it's always defined this way?","I get really frustrated that all these real analysis books and online webpages say $S(n) = n + 1$ but then say addition is defined in terms off the relationships $a + 0 = a$ and $a + S(b) = S(a+b)$. I feel like this is a bit of a circular definition because we haven't really defined $S(n)$ and I don't like labeling it ""$S(n) = n + 1$"" because that feels like a cheap way of appealing to intuition when the whole point of analysis is to rigorously define the very things we normally take for granted and find obvious so that we more accurately understand what we are and are not permitted to do with these numbers. Can we just treat it as a mapping to some other distinct element? If I look at it this way then I would end up defining axioms like this: Zero is a number. If $a$ is a number, then the successor of $a$, denoted $S(a)$, is a number. $a$ and $S(a)$ are considered distinct numbers. If two numbers have the same successors, then they themselves are equal numbers. Zero is not the successor of any other number. So I am envisioning a sort of linked-list relationship: $$0 \rightarrow \alpha \rightarrow \beta \rightarrow \gamma \rightarrow \delta \rightarrow \epsilon \rightarrow \zeta \rightarrow \eta...$$ The labels are arbitrary but I am seeing natural numbers as just nodes linked together where successor just means ""whatever this node points to"". So you wouldn't see something like $\alpha$ (or anything else) point to two different nodes, or anything pointing to $0$, or anything pointing to itself. And then if we want to compute $\beta + \gamma$ using our definition of addition we see that: $$\beta + \gamma = \beta + S(\beta) = S(\beta + \beta)$$ $$\beta + \beta = \beta + S(\alpha) = S(\beta + \alpha)$$ $$\beta + \alpha = \beta + S(0) = S(\beta + 0)$$ $$\beta + 0 = \beta$$ Combining: $$\beta + \gamma = S(S(S(\beta)))$$ Which makes sense intuitively, taking the third successor to $\beta$. And if we want to define $\beta$ relative to $0$ or its eventual node in the list we could replace it with successors and then show that $$\beta + \gamma = S(S(S(S(\alpha)))) = S(S(S(S(S(0))))) = \epsilon$$ I mean is this the right way to think of it? Am I right to find $S(n) = n + 1$ problematic or am I missing some point as to why it's always defined this way?",,"['real-analysis', 'notation', 'definition', 'natural-numbers']"
50,Is $x\log\bigl(\cos(x)\bigr)$ an even or odd function?,Is  an even or odd function?,x\log\bigl(\cos(x)\bigr),Is $x\log\bigl(\cos(x)\bigr)$ an even or odd function? $$f(-x)=-x\log\bigl(\cos(-x)\bigr)=-x\log\bigl(\cos(x)\bigr)=-f(x)$$ So it seems an odd function and i've tried to draw the graph too. But the suggested solution in my book says that it is an even function.,Is $x\log\bigl(\cos(x)\bigr)$ an even or odd function? $$f(-x)=-x\log\bigl(\cos(-x)\bigr)=-x\log\bigl(\cos(x)\bigr)=-f(x)$$ So it seems an odd function and i've tried to draw the graph too. But the suggested solution in my book says that it is an even function.,,"['real-analysis', 'functions', 'even-and-odd-functions']"
51,Show that the supremum of a collection of lower semicontinuous function is lower semicontinuous,Show that the supremum of a collection of lower semicontinuous function is lower semicontinuous,,"I know there's already a question with a title very similar to this, unfortunately as I understand the OP skips over the part of the proof that is not clear to me. Let $I$ be a set and $f_\alpha$, $\alpha \in I$ be a collection of lower semicontinuous functions. Show that $g=\sup\limits_{\alpha\in I}\,f_\alpha$ is lower semicontinuous. My attempt: let $S_f(t)=\{x\in\mathbb{R}^n: f(x)>t\}$ for $t\in\mathbb{R}$. It is easy to see that $f$ is lower semicontinuous if and only if $S_f(t)$ is an open set, now define $$S(t)=\bigcup_\limits{\alpha\in I}S_{f_\alpha}(t).$$ Obviously $S(t)$ is open. I should now show that $S_{g}(t)=S(t)$, which ends the proof. Now, obviously $S(t)\subset S_g(t)$ because if $f_\alpha(x)>t$ for some $\alpha\in I$ then $g(x)>t$ because $g(x)>f_\alpha(x)$ for all $\alpha\in I$, but I fail to see why $S_g(t)\subset S(t)$, I think I should use the definition of supremum but I don't see how. I'm thinking I should prove it for a particular $t\in \mathbb{R}$ because that seems very untrue for any $t$. Thank you.","I know there's already a question with a title very similar to this, unfortunately as I understand the OP skips over the part of the proof that is not clear to me. Let $I$ be a set and $f_\alpha$, $\alpha \in I$ be a collection of lower semicontinuous functions. Show that $g=\sup\limits_{\alpha\in I}\,f_\alpha$ is lower semicontinuous. My attempt: let $S_f(t)=\{x\in\mathbb{R}^n: f(x)>t\}$ for $t\in\mathbb{R}$. It is easy to see that $f$ is lower semicontinuous if and only if $S_f(t)$ is an open set, now define $$S(t)=\bigcup_\limits{\alpha\in I}S_{f_\alpha}(t).$$ Obviously $S(t)$ is open. I should now show that $S_{g}(t)=S(t)$, which ends the proof. Now, obviously $S(t)\subset S_g(t)$ because if $f_\alpha(x)>t$ for some $\alpha\in I$ then $g(x)>t$ because $g(x)>f_\alpha(x)$ for all $\alpha\in I$, but I fail to see why $S_g(t)\subset S(t)$, I think I should use the definition of supremum but I don't see how. I'm thinking I should prove it for a particular $t\in \mathbb{R}$ because that seems very untrue for any $t$. Thank you.",,"['real-analysis', 'continuity', 'semicontinuous-functions']"
52,A limit like a Riemann sum,A limit like a Riemann sum,,"Let $f:[0,1] \to \Bbb R_+$ be a non-negative continuous function. For all integer $n \geq 1$, we define : $$U_n=\left(\frac 1n\sum\limits_{k=1}^n \sqrt[n]{f\left(\frac kn\right)}\right)^n$$ What can be said about : $$\lim_{n \to +\infty} U_n ?$$ What I have done till now: Using concavity of $x \mapsto \sqrt[n]{x}$  I proved that $$\lim_{n \to +\infty} U_n \leq  \int_0^1 f(t) dt $$","Let $f:[0,1] \to \Bbb R_+$ be a non-negative continuous function. For all integer $n \geq 1$, we define : $$U_n=\left(\frac 1n\sum\limits_{k=1}^n \sqrt[n]{f\left(\frac kn\right)}\right)^n$$ What can be said about : $$\lim_{n \to +\infty} U_n ?$$ What I have done till now: Using concavity of $x \mapsto \sqrt[n]{x}$  I proved that $$\lim_{n \to +\infty} U_n \leq  \int_0^1 f(t) dt $$",,"['real-analysis', 'sequences-and-series', 'limits']"
53,What is the right way to approximate $e^{-1/x^2}$ by polynomials?,What is the right way to approximate  by polynomials?,e^{-1/x^2},"It's well known that $f(x) = e^{-1/x^2}$ (with zero added) is a smooth function that's not analytic at $x=0$, because every derivative at zero is zero, and so all of its Taylor polynomials are zero. For the sake of simplicity fix the center at zero for the rest of the question. This function isn't all that pathological and it seems like there should still be a principled way to approximate it by polynomials by using some other natural data about $f$. More concretely, what is a method for approximating a smooth function $f(x)$ by polynomials that has the following properties: Optimal by some natural criterion (analogous to how the degree-$k$ Taylor polynomial is optimal among degree $k$ polynomials on a sufficiently small interval) Graded, i.e. there is a parameter $n$ so that larger values of $n$ use higher-degree polynomials and improve the approximation. Efficiently computable, i.e., there is a $\text{poly}(n)$-time algorithm which constructs the polynomial representation from the input parameter of $n$ Nontriviality for $e^{-1/x^2}$","It's well known that $f(x) = e^{-1/x^2}$ (with zero added) is a smooth function that's not analytic at $x=0$, because every derivative at zero is zero, and so all of its Taylor polynomials are zero. For the sake of simplicity fix the center at zero for the rest of the question. This function isn't all that pathological and it seems like there should still be a principled way to approximate it by polynomials by using some other natural data about $f$. More concretely, what is a method for approximating a smooth function $f(x)$ by polynomials that has the following properties: Optimal by some natural criterion (analogous to how the degree-$k$ Taylor polynomial is optimal among degree $k$ polynomials on a sufficiently small interval) Graded, i.e. there is a parameter $n$ so that larger values of $n$ use higher-degree polynomials and improve the approximation. Efficiently computable, i.e., there is a $\text{poly}(n)$-time algorithm which constructs the polynomial representation from the input parameter of $n$ Nontriviality for $e^{-1/x^2}$",,"['calculus', 'real-analysis', 'polynomials', 'algorithms', 'approximation']"
54,Limit of $\sum\limits_{k=0}^n \frac{{n\choose k}}{n^k(k+3)}$ when $n\to\infty$ [duplicate],Limit of  when  [duplicate],\sum\limits_{k=0}^n \frac{{n\choose k}}{n^k(k+3)} n\to\infty,This question already has answers here : Evalute $ \lim_{n\rightarrow \infty}\sum^{n}_{k=0}\frac{\binom{n}{k}}{n^k(k+3)} $ (5 answers) Closed 4 years ago . What is $$ \lim_{n \to  \infty} \sum_{k=0}^n \frac{{n\choose k}}{n^k(k+3)}\ ?$$ I know the way by integration and that the answer is $e-2$ but I am more interested in use of sandwich theorem which provides a maxima or a closed form to it. Expansions may also be useful.,This question already has answers here : Evalute $ \lim_{n\rightarrow \infty}\sum^{n}_{k=0}\frac{\binom{n}{k}}{n^k(k+3)} $ (5 answers) Closed 4 years ago . What is $$ \lim_{n \to  \infty} \sum_{k=0}^n \frac{{n\choose k}}{n^k(k+3)}\ ?$$ I know the way by integration and that the answer is $e-2$ but I am more interested in use of sandwich theorem which provides a maxima or a closed form to it. Expansions may also be useful.,,"['real-analysis', 'integration', 'limits']"
55,Monotone Convergence theorem for decreasing sequence,Monotone Convergence theorem for decreasing sequence,,"Suppose $f_n: X\to [0, \infty]$ is measurable for $n = 1, 2, 3, \dots$, $f_1 \geqslant f_2 \geqslant f_3 \geqslant \dots \geqslant 0,$ $f_n(x) \to f(x)$ as $n\to \infty$, for every $x\in X$, and $f_1 \in L^1(\mu)$. Prove that then $$\lim \limits_{n\to \infty}\int \limits_{X}f_nd\mu= \int \limits_{X}fd\mu$$ and show that this conclusion does not follow if the condition ""$f_1 \in L_1 (\mu)$"" is omitted. Proof: $$f_1 \geqslant f_2 \geqslant f_3 \geqslant \dots \geqslant 0 \implies -f_1 \leqslant -f_2 \leqslant -f_3 \leqslant \dots \leqslant 0 \implies 0\leqslant f_1-f_2\leqslant f_1-f_3\leqslant \dots\leqslant f_1.$$ In other words, sequence $g_n=f_1-f_n$ is increasing & measurable and $g_n(x)\to f_1(x)-f(x)$ for each $x\in X$ and we can use Monotone convergence theorem: $$\lim \limits_{n\to \infty}\int \limits_{X}g_nd\mu=\lim \limits_{n\to \infty}\int \limits_{X}(f_1-f_n)d\mu=\int \limits_{X}(f_1-f)d\mu.$$ If $f_1\in L^1(\mu)$ then $f_n\in  L^1(\mu)$ for each $n\in \mathbb{N}$ and $f\in L^1(\mu)$ then: $$\int \limits_{X}f_1d\mu-\lim \limits_{n\to \infty}\int \limits_{X}f_nd\mu=\int \limits_{X}f_1d\mu-\int \limits_{X}fd\mu$$ since $\int \limits_{X}f_1d\mu$ is finite we can subtract it and we get what we need! $\color{red}{Wrong \quad Counterexample:}$ Condition $f_1\in L^1(\mu)$ is crucial! Suppose the we omit this condition. Let $X=\mathbb{N}, \mathfrak{M}=2^{\mathbb{N}}$ and $\mu=|\cdot|$ is counting measure on $\mathfrak{M}$. Suppose $f_n(x)=\dfrac{1_{A_n}(x)}{n}$ where $A_n=\{1,2,\dots, n\}$. It's easy to check that $f_n(x)\to 0$ as $n\to \infty$ for $x\in X$. But $\int \limits_{X}f_nd\mu=\frac{1}{n}\mu(A_n)=1.$ So $$1=\lim \limits_{n\to \infty}\int \limits_{X}f_nd\mu\neq \int \limits_{X}fd\mu=0$$ Is my proof and its counterexample correct? Would be very grateful for any suggestions & comments. EDIT: Let's consider triple $(X,\mathfrak{M},\mu):=(\mathbb{N},2^{\mathbb{N}},|\cdot|)$, where $|\cdot |$ - counting measure on $2^\mathbb{N}$. Let $A_n=\{n, n+1,\dots\}$. Suppose that $f_n(x)=\dfrac{1_{A_n}}{n}$. It's easy to see that $f_1\geqslant f_2\geqslant \dots \geqslant f_n\geqslant \dots \geqslant 0$. Also note that $f_n(x)\to f(x)=0$ as $n\to \infty$ for $x\in X$. Also $\int \limits_{X}f_nd\mu=\dfrac{1}{n}\mu(A_n)=\infty.$ Hence $$\infty=\lim \limits_{n\to \infty}\int \limits_{X}f_nd\mu\neq\int \limits_{X}fd\mu=0.$$ Is it true?","Suppose $f_n: X\to [0, \infty]$ is measurable for $n = 1, 2, 3, \dots$, $f_1 \geqslant f_2 \geqslant f_3 \geqslant \dots \geqslant 0,$ $f_n(x) \to f(x)$ as $n\to \infty$, for every $x\in X$, and $f_1 \in L^1(\mu)$. Prove that then $$\lim \limits_{n\to \infty}\int \limits_{X}f_nd\mu= \int \limits_{X}fd\mu$$ and show that this conclusion does not follow if the condition ""$f_1 \in L_1 (\mu)$"" is omitted. Proof: $$f_1 \geqslant f_2 \geqslant f_3 \geqslant \dots \geqslant 0 \implies -f_1 \leqslant -f_2 \leqslant -f_3 \leqslant \dots \leqslant 0 \implies 0\leqslant f_1-f_2\leqslant f_1-f_3\leqslant \dots\leqslant f_1.$$ In other words, sequence $g_n=f_1-f_n$ is increasing & measurable and $g_n(x)\to f_1(x)-f(x)$ for each $x\in X$ and we can use Monotone convergence theorem: $$\lim \limits_{n\to \infty}\int \limits_{X}g_nd\mu=\lim \limits_{n\to \infty}\int \limits_{X}(f_1-f_n)d\mu=\int \limits_{X}(f_1-f)d\mu.$$ If $f_1\in L^1(\mu)$ then $f_n\in  L^1(\mu)$ for each $n\in \mathbb{N}$ and $f\in L^1(\mu)$ then: $$\int \limits_{X}f_1d\mu-\lim \limits_{n\to \infty}\int \limits_{X}f_nd\mu=\int \limits_{X}f_1d\mu-\int \limits_{X}fd\mu$$ since $\int \limits_{X}f_1d\mu$ is finite we can subtract it and we get what we need! $\color{red}{Wrong \quad Counterexample:}$ Condition $f_1\in L^1(\mu)$ is crucial! Suppose the we omit this condition. Let $X=\mathbb{N}, \mathfrak{M}=2^{\mathbb{N}}$ and $\mu=|\cdot|$ is counting measure on $\mathfrak{M}$. Suppose $f_n(x)=\dfrac{1_{A_n}(x)}{n}$ where $A_n=\{1,2,\dots, n\}$. It's easy to check that $f_n(x)\to 0$ as $n\to \infty$ for $x\in X$. But $\int \limits_{X}f_nd\mu=\frac{1}{n}\mu(A_n)=1.$ So $$1=\lim \limits_{n\to \infty}\int \limits_{X}f_nd\mu\neq \int \limits_{X}fd\mu=0$$ Is my proof and its counterexample correct? Would be very grateful for any suggestions & comments. EDIT: Let's consider triple $(X,\mathfrak{M},\mu):=(\mathbb{N},2^{\mathbb{N}},|\cdot|)$, where $|\cdot |$ - counting measure on $2^\mathbb{N}$. Let $A_n=\{n, n+1,\dots\}$. Suppose that $f_n(x)=\dfrac{1_{A_n}}{n}$. It's easy to see that $f_1\geqslant f_2\geqslant \dots \geqslant f_n\geqslant \dots \geqslant 0$. Also note that $f_n(x)\to f(x)=0$ as $n\to \infty$ for $x\in X$. Also $\int \limits_{X}f_nd\mu=\dfrac{1}{n}\mu(A_n)=\infty.$ Hence $$\infty=\lim \limits_{n\to \infty}\int \limits_{X}f_nd\mu\neq\int \limits_{X}fd\mu=0.$$ Is it true?",,"['real-analysis', 'measure-theory', 'proof-verification', 'monotone-functions']"
56,Show that the set $\{x \in \mathbb{R}| \lim_{n \to \infty} \sin(a_n x) \mbox{ exists}\}$ has zero measure,Show that the set  has zero measure,\{x \in \mathbb{R}| \lim_{n \to \infty} \sin(a_n x) \mbox{ exists}\},"$a_n$ is a sequence of real numbers such that $a_n \to +\infty$. Show that the set $E = \{x \in \mathbb{R}| \lim_{n \to \infty} \sin(a_n x) \mbox{ exists}\}$ has zero (Lebesgue) measure. The hint for the exercise says ""Riemann-Lebesgue lemma and dominated convergence theorem"". But for a start, I have no idea of how to show that $E$ is measurable (or of finite measure).","$a_n$ is a sequence of real numbers such that $a_n \to +\infty$. Show that the set $E = \{x \in \mathbb{R}| \lim_{n \to \infty} \sin(a_n x) \mbox{ exists}\}$ has zero (Lebesgue) measure. The hint for the exercise says ""Riemann-Lebesgue lemma and dominated convergence theorem"". But for a start, I have no idea of how to show that $E$ is measurable (or of finite measure).",,"['real-analysis', 'lebesgue-integral']"
57,The domain of $x^x$?,The domain of ?,x^x,"This one looks simple, but apparently there is something more to it.  $$f{(x)=x^x}$$ I read somewhere that the domain is $\Bbb R_+$, a friend said that $x\lt-1, x\gt0$... I'm really confused, because i don't understand why the domain isn't just all the real numbers. According to any grapher online the domain is $\Bbb R_+$. Any Thoughts on the matter? Can someone explain what am I missing?","This one looks simple, but apparently there is something more to it.  $$f{(x)=x^x}$$ I read somewhere that the domain is $\Bbb R_+$, a friend said that $x\lt-1, x\gt0$... I'm really confused, because i don't understand why the domain isn't just all the real numbers. According to any grapher online the domain is $\Bbb R_+$. Any Thoughts on the matter? Can someone explain what am I missing?",,['real-analysis']
58,Why is the Cauchy product of two convergent (but not absolutely) series either convergent or indeterminate (but does not converge to infinity)?,Why is the Cauchy product of two convergent (but not absolutely) series either convergent or indeterminate (but does not converge to infinity)?,,"It is well-known that the Cauchy product of two absolutely convergent series is absolutely convergent. However, my professor added (without giving a proof) that if the series are convergent (but not absolutely convergent), then the Cauchy product is either convergent or irregular (i.e. indeterminate ), but not divergent to infinity . How do you prove this statement?","It is well-known that the Cauchy product of two absolutely convergent series is absolutely convergent. However, my professor added (without giving a proof) that if the series are convergent (but not absolutely convergent), then the Cauchy product is either convergent or irregular (i.e. indeterminate ), but not divergent to infinity . How do you prove this statement?",,"['calculus', 'real-analysis', 'sequences-and-series']"
59,Does the proof of Bolzano-Weierstrass theorem require axiom of choice?,Does the proof of Bolzano-Weierstrass theorem require axiom of choice?,,"When selecting the terms of subsequence from each bisections, I thought axiom of choice might be required. But I'm not so sure whether or not, so please tell me. [edited] I'm sorry for the lack of explanation. I want to prove this statement: Let $a_1, a_2, \ldots \in \mathbf{R}$, and $(a_n)_{n\in\mathbf{N}}$ is bounded, then $(a_n)$ has some convergent subsequence. The proof is as follows. Since $(a_n)$ is bounded, for all $n\in\mathbf{N}$,   $a_n \in I = [b, c]$. Now, let $I_0 = I$ and if $I_n = [b_n, c_n]$,  we define $d_n = (b_n+c_n)/2$ and  if infinite terms of $(a_n)$ is included in $[b_n, d_n]$(resp. $[d_n, c_n]$), we will define  $I_{n+1} = [b_n, d_n]$(resp. $[d_n, c_n]$).If both intervals contain infinite terms, let $I_{n+1}$ be $[d_n, c_n]$. For all $n\in \mathbf{N}$, infinite numbers of $m \in \mathbf{N}$ exist such that $a_m \in I_n$ suffices. We take the sequence of natural numbers $(n(k))_{k\in\mathbf{N}}$which suffices $n(0) < n(1) < \cdots < n(k) < \cdots$ following this procedure: Now we have already selected $a_{n(1)}, \ldots, a_{n(k)}$, there are infinite numbers of $m\in \mathbf{N}$ which suffices $n(k)<m, a_m \in I_{k+1}$, so let's take the minimum m out of it. Applying this process recursively, we obtain a infinite convergent subsequence(?). I think intuitively, by only repeating this process we can't obtain countable infinite terms of subsequence because we have to repeat infinite times.","When selecting the terms of subsequence from each bisections, I thought axiom of choice might be required. But I'm not so sure whether or not, so please tell me. [edited] I'm sorry for the lack of explanation. I want to prove this statement: Let $a_1, a_2, \ldots \in \mathbf{R}$, and $(a_n)_{n\in\mathbf{N}}$ is bounded, then $(a_n)$ has some convergent subsequence. The proof is as follows. Since $(a_n)$ is bounded, for all $n\in\mathbf{N}$,   $a_n \in I = [b, c]$. Now, let $I_0 = I$ and if $I_n = [b_n, c_n]$,  we define $d_n = (b_n+c_n)/2$ and  if infinite terms of $(a_n)$ is included in $[b_n, d_n]$(resp. $[d_n, c_n]$), we will define  $I_{n+1} = [b_n, d_n]$(resp. $[d_n, c_n]$).If both intervals contain infinite terms, let $I_{n+1}$ be $[d_n, c_n]$. For all $n\in \mathbf{N}$, infinite numbers of $m \in \mathbf{N}$ exist such that $a_m \in I_n$ suffices. We take the sequence of natural numbers $(n(k))_{k\in\mathbf{N}}$which suffices $n(0) < n(1) < \cdots < n(k) < \cdots$ following this procedure: Now we have already selected $a_{n(1)}, \ldots, a_{n(k)}$, there are infinite numbers of $m\in \mathbf{N}$ which suffices $n(k)<m, a_m \in I_{k+1}$, so let's take the minimum m out of it. Applying this process recursively, we obtain a infinite convergent subsequence(?). I think intuitively, by only repeating this process we can't obtain countable infinite terms of subsequence because we have to repeat infinite times.",,"['real-analysis', 'analysis', 'axiom-of-choice']"
60,Convex set of derivatives implies mean value theorem,Convex set of derivatives implies mean value theorem,,"Let  U$ \subset$ $R^{^{n}}\ $be open, $f:U\rightarrow R^{m}$  differentiable on  U, and segment $[a,b]\subset U$. Assume  that the set of derivatives $\{ f'(x)\in L(R^{^{n}},R^{^{m}}):x\in [a,b] \}$ is convex. Prove that there exist a $\theta$ in $ [a,b]$ such that $f(b)-f(a)=f'(\theta )(b-a)$. I have started working on ""Real mathematical analysis"" of Charles Chapman Pugh. I tried to solve this problem which is number 17 in chapter 5,but I was totally stucked even how to start.Any help is appreciated.","Let  U$ \subset$ $R^{^{n}}\ $be open, $f:U\rightarrow R^{m}$  differentiable on  U, and segment $[a,b]\subset U$. Assume  that the set of derivatives $\{ f'(x)\in L(R^{^{n}},R^{^{m}}):x\in [a,b] \}$ is convex. Prove that there exist a $\theta$ in $ [a,b]$ such that $f(b)-f(a)=f'(\theta )(b-a)$. I have started working on ""Real mathematical analysis"" of Charles Chapman Pugh. I tried to solve this problem which is number 17 in chapter 5,but I was totally stucked even how to start.Any help is appreciated.",,"['real-analysis', 'multivariable-calculus']"
61,Equivalence of $\pi$ is the first positive zero of the taylor series for $\sin(x)$ and $\pi/4 = 1 - \frac{1}{3} + \frac{1}{5} - \frac{1}{7} + \cdots$,Equivalence of  is the first positive zero of the taylor series for  and,\pi \sin(x) \pi/4 = 1 - \frac{1}{3} + \frac{1}{5} - \frac{1}{7} + \cdots,"For $x\in\mathbb{R}$, define $\sin (x) = x - x^3/3!+x^5/5!-\cdots$ and $\pi = 4(1-\frac{1}{3}+\frac{1}{5} -\frac{1}{7}+\cdots)$. Then show that $\sin(\pi/2) = 1$ In the prologue of Real and Complex Analysis by Walter Rudin, pi is defined as the first positive zero of the series defined as $\sin(x)$; I want to check that pi is same as above defined Pi.","For $x\in\mathbb{R}$, define $\sin (x) = x - x^3/3!+x^5/5!-\cdots$ and $\pi = 4(1-\frac{1}{3}+\frac{1}{5} -\frac{1}{7}+\cdots)$. Then show that $\sin(\pi/2) = 1$ In the prologue of Real and Complex Analysis by Walter Rudin, pi is defined as the first positive zero of the series defined as $\sin(x)$; I want to check that pi is same as above defined Pi.",,"['real-analysis', 'sequences-and-series', 'trigonometry', 'taylor-expansion']"
62,A question on cosine integral,A question on cosine integral,,"So I've read a book and found myself stumped in this integral: $$\int_{0}^{\pi} \frac{\cos(n\theta)}{b^2-a^2\cos(2\theta)}\, d\theta=\begin{cases} \,\,0 &,\quad\mbox{if}\,\, n\,\,\mbox{is odd}\\[20pt] \,\,\dfrac{\pi}{\sqrt{b^4-a^4}}\left(\dfrac{\sqrt{b^2-\sqrt{b^4-a^4}}}{a}\right)^n&,\quad\mbox{if}\,\, n\,\,\mbox{is even}\\ \end{cases}$$   where $b>a$. Anyone knows how to evaluate it? Or knows a reference for helping me to prove formula above?","So I've read a book and found myself stumped in this integral: $$\int_{0}^{\pi} \frac{\cos(n\theta)}{b^2-a^2\cos(2\theta)}\, d\theta=\begin{cases} \,\,0 &,\quad\mbox{if}\,\, n\,\,\mbox{is odd}\\[20pt] \,\,\dfrac{\pi}{\sqrt{b^4-a^4}}\left(\dfrac{\sqrt{b^2-\sqrt{b^4-a^4}}}{a}\right)^n&,\quad\mbox{if}\,\, n\,\,\mbox{is even}\\ \end{cases}$$   where $b>a$. Anyone knows how to evaluate it? Or knows a reference for helping me to prove formula above?",,"['calculus', 'real-analysis', 'integration', 'trigonometry', 'definite-integrals']"
63,"Proof of Conway's ""Simplicity Rule"" for Surreal Numbers","Proof of Conway's ""Simplicity Rule"" for Surreal Numbers",,"A ""number"" in the sense of Combinatorial Game Theory is a game  $G = \{ a,b,c,\dots | \; d,e,f,\dots \}$ such that $a,b,c < d,e,f$.  Then our game is between the left and right options: $$  a,b,c <  G < d,e,f $$ However, we still don't know which number we are dealing with until we invoke his Simplicity Rule If there's any number that fits, it's the simplest number that fits. At this point, we don't necessarily know that all finite games should have denominators which are power of $2$, but it's true.  Instead we get 4 rules and  a fuzzy notion of ""simplicity"": $0 = \{ | \}$ $n+1 = \{ n| \}$ $-n-1 = \{ | -n \}$ $\tfrac{2p+1}{2^{q+1}} = \left\{\tfrac{p}{2^q}  | \tfrac{p+1}{2^q} \right\} $ The numbers we construct kind of resemble the notches of a ruler It is known these games generalize Dedekind cut construction of the real numbers from the Rationals.  However $\mathbb{Q}$ hasn't been constructed yet... not even $\mathbb{Z}[\tfrac{1}{2}]$. How do we prove these simplicity rules?","A ""number"" in the sense of Combinatorial Game Theory is a game  $G = \{ a,b,c,\dots | \; d,e,f,\dots \}$ such that $a,b,c < d,e,f$.  Then our game is between the left and right options: $$  a,b,c <  G < d,e,f $$ However, we still don't know which number we are dealing with until we invoke his Simplicity Rule If there's any number that fits, it's the simplest number that fits. At this point, we don't necessarily know that all finite games should have denominators which are power of $2$, but it's true.  Instead we get 4 rules and  a fuzzy notion of ""simplicity"": $0 = \{ | \}$ $n+1 = \{ n| \}$ $-n-1 = \{ | -n \}$ $\tfrac{2p+1}{2^{q+1}} = \left\{\tfrac{p}{2^q}  | \tfrac{p+1}{2^q} \right\} $ The numbers we construct kind of resemble the notches of a ruler It is known these games generalize Dedekind cut construction of the real numbers from the Rationals.  However $\mathbb{Q}$ hasn't been constructed yet... not even $\mathbb{Z}[\tfrac{1}{2}]$. How do we prove these simplicity rules?",,"['real-analysis', 'combinatorial-game-theory', 'surreal-numbers']"
64,"Closed form of $\sum_{n=1}^\infty(-1)^{n+1} \frac xn \ln\left(1+\frac xn\right), \quad x \in (0,1)$",Closed form of,"\sum_{n=1}^\infty(-1)^{n+1} \frac xn \ln\left(1+\frac xn\right), \quad x \in (0,1)","Is there a known closed form of the series below? $$\sum_{n=1}^\infty(-1)^{n+1} \frac xn \ln\left(1+\frac xn\right), \quad x \in (0,1]$$","Is there a known closed form of the series below? $$\sum_{n=1}^\infty(-1)^{n+1} \frac xn \ln\left(1+\frac xn\right), \quad x \in (0,1]$$",,"['calculus', 'real-analysis', 'sequences-and-series', 'complex-analysis', 'elliptic-integrals']"
65,"Showing the existence of a continuous, strictly increasing function $f$ on $\mathbb{R}$ such that $f'(x) = 0$ almost everywhere","Showing the existence of a continuous, strictly increasing function  on  such that  almost everywhere",f \mathbb{R} f'(x) = 0,"Problem: Show that there exists a continuous strictly increasing function $f$ on $\mathbb{R}$ such that $f'(x) = 0$ almost everywhere. Attempt: Perhaps we could modify the Cantor Function which is non-decreasing, continuous, and constant on each interval in the complement of the Cantor Set in $[0,1]$. Of course, we'd need to do the following: Make the Cantor function strictly increasing (since the Cantor Function is constant on certain intervals, it's certainly not strictly increasing as is). Extend our modified Cantor Function from $[0,1]$ to all of $\mathbb{R}$. Verify that $f'(x) = 0$ for all irrational $x$ (or for all but a countable subset of $\mathbb{R}$).","Problem: Show that there exists a continuous strictly increasing function $f$ on $\mathbb{R}$ such that $f'(x) = 0$ almost everywhere. Attempt: Perhaps we could modify the Cantor Function which is non-decreasing, continuous, and constant on each interval in the complement of the Cantor Set in $[0,1]$. Of course, we'd need to do the following: Make the Cantor function strictly increasing (since the Cantor Function is constant on certain intervals, it's certainly not strictly increasing as is). Extend our modified Cantor Function from $[0,1]$ to all of $\mathbb{R}$. Verify that $f'(x) = 0$ for all irrational $x$ (or for all but a countable subset of $\mathbb{R}$).",,"['real-analysis', 'analysis']"
66,How to learn inequalities and become good at proving them?,How to learn inequalities and become good at proving them?,,"I am taking a real analysis course next year and I want to start slowly preparing for that class now, so I hope you can help me. The class is quite challenging and the fail rate is relatively high. The class uses Rudin's Principles of Mathematical Analysis. To prepare for the class, I decided to go through Abbott's Understanding Analysis, Book of Proof by Hammack and work on inequalities. I read that it is important to be good with proving inequalities so can you give me advice on how I can do that? Are there books that can help me with that? What else do you think I should work on, what books do you recommend? Is it necessary to go through a more rigorous calculus textbook? The courses I did in calculus used Thomas' Calculus, which I think is much less rigorous than something like Spivak's. Would very much appreciate your advice.","I am taking a real analysis course next year and I want to start slowly preparing for that class now, so I hope you can help me. The class is quite challenging and the fail rate is relatively high. The class uses Rudin's Principles of Mathematical Analysis. To prepare for the class, I decided to go through Abbott's Understanding Analysis, Book of Proof by Hammack and work on inequalities. I read that it is important to be good with proving inequalities so can you give me advice on how I can do that? Are there books that can help me with that? What else do you think I should work on, what books do you recommend? Is it necessary to go through a more rigorous calculus textbook? The courses I did in calculus used Thomas' Calculus, which I think is much less rigorous than something like Spivak's. Would very much appreciate your advice.",,"['real-analysis', 'reference-request', 'inequality']"
67,$f$ is convex function iff Hessian matrix is nonnegative-definite.,is convex function iff Hessian matrix is nonnegative-definite.,f,"Let $f: \mathbb{R}^2 \rightarrow \mathbb{R}$, $f \in C^2$. Show that $f$ is convex function iff Hessian matrix is nonnegative-definite. $f(x,y)$ is convex if $f( \lambda x + (1-\lambda )y) \le \lambda f(x) + (1- \lambda)f(y)$ for any $x,y \in \mathbb{R}^2$. Hessian matrix is nonnegative-definite if $f_{xx}'' x^2 + f_{x,y}(x+y) + f_{yy}''y^2 \ge 0$ I know the definition but I have no idea how prove the If and only if condition or first and second implication?","Let $f: \mathbb{R}^2 \rightarrow \mathbb{R}$, $f \in C^2$. Show that $f$ is convex function iff Hessian matrix is nonnegative-definite. $f(x,y)$ is convex if $f( \lambda x + (1-\lambda )y) \le \lambda f(x) + (1- \lambda)f(y)$ for any $x,y \in \mathbb{R}^2$. Hessian matrix is nonnegative-definite if $f_{xx}'' x^2 + f_{x,y}(x+y) + f_{yy}''y^2 \ge 0$ I know the definition but I have no idea how prove the If and only if condition or first and second implication?",,"['real-analysis', 'optimization', 'convex-analysis', 'hessian-matrix', 'positive-semidefinite']"
68,Questions about weak derivatives,Questions about weak derivatives,,"There are two definitions of generalized differentiation that seem relevant to the context of PDEs. (That is we generalize what objects can be differentiated but we stay in Euclidean space.  There are also other types of generalizations that change the space like Frechet or Gateaux derivatives in Banach spaces.) One is that we take any distribution (continuous linear functional on $C^\infty_{com}$ with the topology of convergence of all derivatives in sup norm) and we precompose it with differentiation times a negative sign.  There is no question of existence here. The other is called the ""weak derivative"" and it is supposed to be applied only to locally integrable Borel measurable functions, but again on some open subset of Euclidean space.  Then, the weak derivative may or may not exist, and when it does the definition is that it is locally integrable and must satisfy the integration by parts relation against smooth compactly supported test functions. I gather that the first generalization is a generalization even of the second one, as long as one identifies locally integrable functions with their integration distributions.  So first extends second extends ordinary differentiation.  My questions are as follows, if that is correct: Which distributions have antiderivatives?  Which locally integrable functions have weak antiderivatives?  In the weak case, it seems like this has to do with absolute continuity? It is easy for me to formalize a statement that says that derivatives depend on only local information for weak derivatives.  But is there something like this for distributional derivatives?  What would it mean to look locally at a distribution? For weak derivatives only, which may not exist, what are some examples of when an $\alpha$th derivative exists, but there exists a $\beta\le \alpha$ for which the $\beta$th weak derivative does not exist?  What if instead $\beta\ge \alpha$, which may be nontrivial since I don't think antiderivatives come for free? Is there a heuristic, much like there was for ordinary derivatives, that allows me to tell if the weak derivative will exist, and perhaps even to quickly compute the answer?  Right now, all I can see is that if the function is piecewise smooth, then if a weak derivative exists, then you know all the behavior except at the joining points, and those don't matter since the weak derivative is only defined up to a.e.  So then if you can come up with a good reason why this guess doesn't actually work, then no other candidate could possibly work either. (I am reading out of Knapp's book on Advanced Analysis)","There are two definitions of generalized differentiation that seem relevant to the context of PDEs. (That is we generalize what objects can be differentiated but we stay in Euclidean space.  There are also other types of generalizations that change the space like Frechet or Gateaux derivatives in Banach spaces.) One is that we take any distribution (continuous linear functional on $C^\infty_{com}$ with the topology of convergence of all derivatives in sup norm) and we precompose it with differentiation times a negative sign.  There is no question of existence here. The other is called the ""weak derivative"" and it is supposed to be applied only to locally integrable Borel measurable functions, but again on some open subset of Euclidean space.  Then, the weak derivative may or may not exist, and when it does the definition is that it is locally integrable and must satisfy the integration by parts relation against smooth compactly supported test functions. I gather that the first generalization is a generalization even of the second one, as long as one identifies locally integrable functions with their integration distributions.  So first extends second extends ordinary differentiation.  My questions are as follows, if that is correct: Which distributions have antiderivatives?  Which locally integrable functions have weak antiderivatives?  In the weak case, it seems like this has to do with absolute continuity? It is easy for me to formalize a statement that says that derivatives depend on only local information for weak derivatives.  But is there something like this for distributional derivatives?  What would it mean to look locally at a distribution? For weak derivatives only, which may not exist, what are some examples of when an $\alpha$th derivative exists, but there exists a $\beta\le \alpha$ for which the $\beta$th weak derivative does not exist?  What if instead $\beta\ge \alpha$, which may be nontrivial since I don't think antiderivatives come for free? Is there a heuristic, much like there was for ordinary derivatives, that allows me to tell if the weak derivative will exist, and perhaps even to quickly compute the answer?  Right now, all I can see is that if the function is piecewise smooth, then if a weak derivative exists, then you know all the behavior except at the joining points, and those don't matter since the weak derivative is only defined up to a.e.  So then if you can come up with a good reason why this guess doesn't actually work, then no other candidate could possibly work either. (I am reading out of Knapp's book on Advanced Analysis)",,"['real-analysis', 'functional-analysis', 'partial-differential-equations', 'weak-derivatives']"
69,Additional ways of defining real powers.,Additional ways of defining real powers.,,"I am familiar with the following 3 way of defining real powers: Given $x,y\in\mathbb{R}$, such that $x\gt0$, we can define $$  x^y =    \begin{cases}      \sup_{\,q\in\mathbb{Q}}\{x^q :q\le y\} &;\ 1\le x\\ \\      \inf_{\,q\in\mathbb{Q}}\{x^q :q\le y\} &;\ 0\lt x\lt 1\\    \end{cases} $$ Using the theory of integration, we define for $x\gt0$: $$ \ln x:=\int_1^x{\frac{dt}{t}} $$ We can then show, using this definition, that $\ln x$ has all the familiar identities of logarithms. In addition we can show that $\ln x$ is a bijection, and has an inverse which we define as $e^x$. We then define for any $x,y\in\mathbb{R}$, such that $x\gt0$: $$ x^y :=e^{y\ln x} $$ And show that this definition coincides with the definition of rational powers (using the identities of logarithms). Using the theory of series' we can show that the series: $$ e^x:=\sum_{k=0}^{\infty}{\frac{x^k}{k!}} $$ converges for all $x\in\mathbb{R}$. Then, similarily to definition 2, we show that $e^x$ is a bijection and define it's inverse as $\ln x$. Finally we define for any $x,y\in\mathbb{R}$, such that $x\gt0$: $$ x^y :=e^{y\ln x} $$ We can then proceed to show as before that the familiar identities of the exponent hold, and thus the definition coincides with the rational powers. My question has two parts: First off, while these definitions coincide (meaning that for a given $x,y\in\mathbb{R}$ all definitions give the same value for $x^y$), can the definitions themselves be derived from a simpler basis. Or, in other words, are all three definitions different descriptions of the same underlying definition, deriving from the same axioms? Are there additional descriptions for real powers?","I am familiar with the following 3 way of defining real powers: Given $x,y\in\mathbb{R}$, such that $x\gt0$, we can define $$  x^y =    \begin{cases}      \sup_{\,q\in\mathbb{Q}}\{x^q :q\le y\} &;\ 1\le x\\ \\      \inf_{\,q\in\mathbb{Q}}\{x^q :q\le y\} &;\ 0\lt x\lt 1\\    \end{cases} $$ Using the theory of integration, we define for $x\gt0$: $$ \ln x:=\int_1^x{\frac{dt}{t}} $$ We can then show, using this definition, that $\ln x$ has all the familiar identities of logarithms. In addition we can show that $\ln x$ is a bijection, and has an inverse which we define as $e^x$. We then define for any $x,y\in\mathbb{R}$, such that $x\gt0$: $$ x^y :=e^{y\ln x} $$ And show that this definition coincides with the definition of rational powers (using the identities of logarithms). Using the theory of series' we can show that the series: $$ e^x:=\sum_{k=0}^{\infty}{\frac{x^k}{k!}} $$ converges for all $x\in\mathbb{R}$. Then, similarily to definition 2, we show that $e^x$ is a bijection and define it's inverse as $\ln x$. Finally we define for any $x,y\in\mathbb{R}$, such that $x\gt0$: $$ x^y :=e^{y\ln x} $$ We can then proceed to show as before that the familiar identities of the exponent hold, and thus the definition coincides with the rational powers. My question has two parts: First off, while these definitions coincide (meaning that for a given $x,y\in\mathbb{R}$ all definitions give the same value for $x^y$), can the definitions themselves be derived from a simpler basis. Or, in other words, are all three definitions different descriptions of the same underlying definition, deriving from the same axioms? Are there additional descriptions for real powers?",,"['real-analysis', 'integration', 'sequences-and-series', 'definition']"
70,Showing $\sin(x) < x$ for all $x>0$ using the mean value theorem,Showing  for all  using the mean value theorem,\sin(x) < x x>0,"I want to show that $\sin(x) < x$ for all $x>0$, using the mean value theorem. Since the sine is bounded above by $1$, it's obviously true for $x > 1$. Consider $x \in ]0,1]$. Let $f(x)=\sin(x)$. Choose $a=0$ and $x>0$, then there is, according to the mean value theorem, an $x_0$ between $a$ and $x$ with $$f'(x_0)=\frac{f(x)-f(a)}{x-a} \Leftrightarrow (\sin(x))'(x_0)= \frac{\sin(x)-\sin(a)}{x} \Leftrightarrow \cos(x_0)=\frac{\sin(x)}{x}$$ Since $1\geq x_0>0 \Rightarrow \cos(x_0) < 1$, $$\Rightarrow 1 > \cos(x_0)=\frac{\sin(x)}{x} \Rightarrow x  > \sin(x)$$ Is my proof correct?","I want to show that $\sin(x) < x$ for all $x>0$, using the mean value theorem. Since the sine is bounded above by $1$, it's obviously true for $x > 1$. Consider $x \in ]0,1]$. Let $f(x)=\sin(x)$. Choose $a=0$ and $x>0$, then there is, according to the mean value theorem, an $x_0$ between $a$ and $x$ with $$f'(x_0)=\frac{f(x)-f(a)}{x-a} \Leftrightarrow (\sin(x))'(x_0)= \frac{\sin(x)-\sin(a)}{x} \Leftrightarrow \cos(x_0)=\frac{\sin(x)}{x}$$ Since $1\geq x_0>0 \Rightarrow \cos(x_0) < 1$, $$\Rightarrow 1 > \cos(x_0)=\frac{\sin(x)}{x} \Rightarrow x  > \sin(x)$$ Is my proof correct?",,"['real-analysis', 'trigonometry', 'inequality', 'proof-verification']"
71,Proving the Exterior (Outer) Measure of Rectangle is Equal to Volume,Proving the Exterior (Outer) Measure of Rectangle is Equal to Volume,,"I'm having trouble understanding one step of Stein and Shakarchi's proof that the exterior measure of a rectangle is equal to its volume. The proof I reference is part of Example 4 in section 1.2 of the Real Analysis book. To summarize the proof, first they show $|R| \leq m_*(E)$. To show the reverse inequality, the book divides $\mathbb{R}^d$ into cubes of side length $1/k$. Then, it considers the sets $\mathcal{Q}$ and $\mathcal{Q}'$. The claim is that if $\mathcal{Q}$ consists of the finite collection of all cubes entirely contained in rectangle $R$ and $\mathcal{Q}'$ the finite collection of all cubes that intersect the complement of $R$, then there are $O(k^{d-1})$ cubes in $\mathcal{Q}'$, where $O$ denotes standard Big-Oh notation. From here, using the volumes of the cubes, the text uses: $$ \sum_{Q \in (\mathcal{Q} \cup \mathcal{Q}')} |Q| \leq |R| + O(1/k) $$ to complete the proof. Question: Why are there $O(k^{d-1})$ cubes in $\mathcal{Q}'$? I interpret $\mathcal{Q}'$ to be the set of all cubes in the complement, i.e. $\mathcal{Q} \cup \mathcal{Q}' = \mathbb{R}^d$. If this is the case, then I believe $\mathcal{Q}'$ is not a finite collection of cubes and a Big-Oh approximation of its size doesn't make sense.","I'm having trouble understanding one step of Stein and Shakarchi's proof that the exterior measure of a rectangle is equal to its volume. The proof I reference is part of Example 4 in section 1.2 of the Real Analysis book. To summarize the proof, first they show $|R| \leq m_*(E)$. To show the reverse inequality, the book divides $\mathbb{R}^d$ into cubes of side length $1/k$. Then, it considers the sets $\mathcal{Q}$ and $\mathcal{Q}'$. The claim is that if $\mathcal{Q}$ consists of the finite collection of all cubes entirely contained in rectangle $R$ and $\mathcal{Q}'$ the finite collection of all cubes that intersect the complement of $R$, then there are $O(k^{d-1})$ cubes in $\mathcal{Q}'$, where $O$ denotes standard Big-Oh notation. From here, using the volumes of the cubes, the text uses: $$ \sum_{Q \in (\mathcal{Q} \cup \mathcal{Q}')} |Q| \leq |R| + O(1/k) $$ to complete the proof. Question: Why are there $O(k^{d-1})$ cubes in $\mathcal{Q}'$? I interpret $\mathcal{Q}'$ to be the set of all cubes in the complement, i.e. $\mathcal{Q} \cup \mathcal{Q}' = \mathbb{R}^d$. If this is the case, then I believe $\mathcal{Q}'$ is not a finite collection of cubes and a Big-Oh approximation of its size doesn't make sense.",,['real-analysis']
72,"Proof verification for limit point, lim sup and lim inf","Proof verification for limit point, lim sup and lim inf",,"Proposition : Let $(a_n)_{n=m}^{\infty}$ be a sequence of real numbers, let $L^+$ be the limit superior of the sequence, and let $L^-$ be the limit inferior of this sequence  (thus both $L^+, L^-$ are extened real numbers). (a) For every $x> L^+$ , there exists an $N\ge m $ such that $\,a_n<x$ for all $n\ge N$ (the elements of the sequence are eventually less than $x$ ). Similarly, for every $y<L^-$ there exists an $N\ge m $ such that $\,a_n>y$ for all $n\ge N$ . (b) For every $x< L^+$ , and every $N\ge m$ , there exists an $n\ge N$ such that $a_n> x$ (the elements of the sequence exceed $x$ infinitely often). Similarly for $y>L^-$ , and every $N\ge m$ , there exists an $n\ge N$ such that $a_n< y$ . (c) We have $\,\text{inf}_{n\ge m}(a_n)\le L^- \le L^+\le \text{sup}_{n\ge m}(a_n)$ (d) If $c$ is any limit point of $(a_n)$ then we have $L^- \le c \le  L^+$ (e) If $L^+$ is finite then it is a limit point of the sequence. Similarly if $L^-$ is finite. (f) Let $c$ be a real number. If $(a_n)\rightarrow c$ , then we must have $L^+=c=L^-$ . Conversely if $L^+=c=L^-$ , then $(a_n)\rightarrow c$ . I'd like to know if my proof is sound. I'd appreciate any suggestion. Proof: Let $a_N^{+}=\text{sup}_{n\ge N} (a_n)$ and $a_N^{-}=\text{inf}_{n\ge N} (a_n)$ . (a) Suppose $x> L^+$ . Let $\varepsilon = x-L^+$ . Thus by definition this means, $\text{inf}_{N\ge m} (a_N^{+}) <x$ ,  so $L+\varepsilon$ cannot be the infimum of the sequence; and thus there is at least one $N\ge K$ ( $K\ge m$ ) such that $a_N^{+}<L+\varepsilon = x$ . Since $\,a_n \le a_N^{+}$ . Thus $a_n \le x $ for each $n\ge N$ . The second part can be proved using a similar argument. (b) Now if $x < L^+=\text{inf}_{N\ge m} (a_N^{+})\le a_N^{+}$ . Let us fix some $N\ge m$ . Thus, $x <a_N^{+}$ , and by definition there must be some $n\ge N$ such that $x< a_n$ (otherwise $x$ would be an upper bound which is less than the least upper bound, which clearly is a contradiction  ), as desired. The other part is proven similarly. (c) Since $\text{sup}_{n\ge m} (a_n) = a_m^+$ and $\text{inf}_{n\ge m} (a_n) = a_m^-$ . Clearly, $a_m^-\le\text{sup}_{N\ge m}(a_N^{-})= L^-$ and $L^+=\text{inf}_{N\ge m} (a_N^{+})\le a_m^+$ . So, we need to show $L^-\le L^+$ . Since we have $a_N^{-}\le a_N^{+}$ for every $N$ . Thus $L^-\le L^+$ (because the limit conserve the non-strict inequality), as desired. (d) Suppose both $L^{+}, L^{-}$ are finite and we let $\varepsilon>0$ be given. By (a) there is some $N$ such that $a_n \le L^+ +\varepsilon$ for each $n\ge N$ . Now, using the definition of limit point, there is some $n_0\ge N$ such that $|a_{n_0}-c| \le \varepsilon$ . Thus $\,c-L^{+}= (c-a_{n_0})+(a_{n_0}-L^{+})\le 2 \varepsilon$ , since $\varepsilon$ was arbitrary, this implies $c\le L^{+}$ . Similarly, there is some $M\ge m$ , so that $L^{-}-\varepsilon < a_n$ for each $n\ge M$ . By hypothesis $c$ is a limit point and then, for this $M$ there is an $n'\ge M$ for which $|a_{n'}-c|\le \varepsilon$ . Thus $L^{-}-c = L^{-} - a_{n'}+a_{n'}-c\le 2 \varepsilon$ , i.e., $L^{-}-c\le 2 \varepsilon$ for any $\varepsilon>0$ and hence $L^{-}\le c$ . If $L^{+} = +\infty$ , then $c\le L^{+}$ . Now either $L^{-}=+\infty$ or $L^{-}=-\infty$ . The second case is trivial. But in the first case this would imply that the limit point is infinite and we are not defined yet infinite limit points but follows in the exact same way. (e) Suppose $L^{+}$ is finite and let $\varepsilon>0$ be given. Thus, as a consecuence of (a) there is some $N>m$ so that $a_n<L^{+}+\varepsilon$ for any $n\ge N$ . Using (b) we know that there at least one $n'\ge N$ such that $L^{+}-\varepsilon<a_{n'}$ . So, $L^{+}-\varepsilon< a_{n'}<L^{+}+\varepsilon$ , i.e., $|a_{n'}-L^{+}|\le \varepsilon$ . Since $\varepsilon$ was arbitrary and we can choose any $N\ge m$ , thus $L^{+}$ is a limit point. The other case is symmetric. (f) Suppose $(a_n)\rightarrow c$ ; we wish to show $L^{+}= L^{-}$ , by (c) we already know that $L^{-}\le c\le L^{+}$ so, it will suffice to prove $L^{+}\le c \le  L^{-}$ . Let $\varepsilon>0$ be given. Since $(a_n)$ converges to a real number, then is bounded. So, the limit superior and inferior are finite and limit points. There is a $N\ge m$ such that $|a_n-c|\le \varepsilon\,$ for each $n$ , and there is $n'\ge N$ so that $L^{+}-\varepsilon< a_n'$ . Thus $L^{+}-c = (L^{+}-a_n')+(a_n'-c)\le 2\varepsilon$ . And thus $L^{+}\le c$ . A symmetric argument prove that $L^{-}\ge c$ and we're done. To the other direction we assume that $c=L^{+}=L^{-}$ . So, there is some $N_1$ such that $a_n < L^{+} + \varepsilon$ and similarly there is some $N_2$ so that $L^{-} - \varepsilon< a_n$ . We pick the greater between $N_1$ and $N_2$ so, the two inequalities occurs simultaneously. Thus $L^{-} - \varepsilon<a_n < L^{+} + \varepsilon$ and since both are the same, this would imply $|a_n - c|\le \varepsilon$ for every $n\ge N$ , which shows that the sequence converges to $c$ . $\square$","Proposition : Let be a sequence of real numbers, let be the limit superior of the sequence, and let be the limit inferior of this sequence  (thus both are extened real numbers). (a) For every , there exists an such that for all (the elements of the sequence are eventually less than ). Similarly, for every there exists an such that for all . (b) For every , and every , there exists an such that (the elements of the sequence exceed infinitely often). Similarly for , and every , there exists an such that . (c) We have (d) If is any limit point of then we have (e) If is finite then it is a limit point of the sequence. Similarly if is finite. (f) Let be a real number. If , then we must have . Conversely if , then . I'd like to know if my proof is sound. I'd appreciate any suggestion. Proof: Let and . (a) Suppose . Let . Thus by definition this means, ,  so cannot be the infimum of the sequence; and thus there is at least one ( ) such that . Since . Thus for each . The second part can be proved using a similar argument. (b) Now if . Let us fix some . Thus, , and by definition there must be some such that (otherwise would be an upper bound which is less than the least upper bound, which clearly is a contradiction  ), as desired. The other part is proven similarly. (c) Since and . Clearly, and . So, we need to show . Since we have for every . Thus (because the limit conserve the non-strict inequality), as desired. (d) Suppose both are finite and we let be given. By (a) there is some such that for each . Now, using the definition of limit point, there is some such that . Thus , since was arbitrary, this implies . Similarly, there is some , so that for each . By hypothesis is a limit point and then, for this there is an for which . Thus , i.e., for any and hence . If , then . Now either or . The second case is trivial. But in the first case this would imply that the limit point is infinite and we are not defined yet infinite limit points but follows in the exact same way. (e) Suppose is finite and let be given. Thus, as a consecuence of (a) there is some so that for any . Using (b) we know that there at least one such that . So, , i.e., . Since was arbitrary and we can choose any , thus is a limit point. The other case is symmetric. (f) Suppose ; we wish to show , by (c) we already know that so, it will suffice to prove . Let be given. Since converges to a real number, then is bounded. So, the limit superior and inferior are finite and limit points. There is a such that for each , and there is so that . Thus . And thus . A symmetric argument prove that and we're done. To the other direction we assume that . So, there is some such that and similarly there is some so that . We pick the greater between and so, the two inequalities occurs simultaneously. Thus and since both are the same, this would imply for every , which shows that the sequence converges to .","(a_n)_{n=m}^{\infty} L^+ L^- L^+, L^- x> L^+ N\ge m  \,a_n<x n\ge N x y<L^- N\ge m  \,a_n>y n\ge N x< L^+ N\ge m n\ge N a_n> x x y>L^- N\ge m n\ge N a_n< y \,\text{inf}_{n\ge m}(a_n)\le L^- \le L^+\le \text{sup}_{n\ge m}(a_n) c (a_n) L^- \le c \le  L^+ L^+ L^- c (a_n)\rightarrow c L^+=c=L^- L^+=c=L^- (a_n)\rightarrow c a_N^{+}=\text{sup}_{n\ge N} (a_n) a_N^{-}=\text{inf}_{n\ge N} (a_n) x> L^+ \varepsilon = x-L^+ \text{inf}_{N\ge m} (a_N^{+}) <x L+\varepsilon N\ge K K\ge m a_N^{+}<L+\varepsilon = x \,a_n \le a_N^{+} a_n \le x  n\ge N x < L^+=\text{inf}_{N\ge m} (a_N^{+})\le a_N^{+} N\ge m x <a_N^{+} n\ge N x< a_n x \text{sup}_{n\ge m} (a_n) = a_m^+ \text{inf}_{n\ge m} (a_n) = a_m^- a_m^-\le\text{sup}_{N\ge m}(a_N^{-})= L^- L^+=\text{inf}_{N\ge m} (a_N^{+})\le a_m^+ L^-\le L^+ a_N^{-}\le a_N^{+} N L^-\le L^+ L^{+}, L^{-} \varepsilon>0 N a_n \le L^+ +\varepsilon n\ge N n_0\ge N |a_{n_0}-c| \le \varepsilon \,c-L^{+}= (c-a_{n_0})+(a_{n_0}-L^{+})\le 2 \varepsilon \varepsilon c\le L^{+} M\ge m L^{-}-\varepsilon < a_n n\ge M c M n'\ge M |a_{n'}-c|\le \varepsilon L^{-}-c = L^{-} - a_{n'}+a_{n'}-c\le 2 \varepsilon L^{-}-c\le 2 \varepsilon \varepsilon>0 L^{-}\le c L^{+} = +\infty c\le L^{+} L^{-}=+\infty L^{-}=-\infty L^{+} \varepsilon>0 N>m a_n<L^{+}+\varepsilon n\ge N n'\ge N L^{+}-\varepsilon<a_{n'} L^{+}-\varepsilon< a_{n'}<L^{+}+\varepsilon |a_{n'}-L^{+}|\le \varepsilon \varepsilon N\ge m L^{+} (a_n)\rightarrow c L^{+}= L^{-} L^{-}\le c\le L^{+} L^{+}\le c \le  L^{-} \varepsilon>0 (a_n) N\ge m |a_n-c|\le \varepsilon\, n n'\ge N L^{+}-\varepsilon< a_n' L^{+}-c = (L^{+}-a_n')+(a_n'-c)\le 2\varepsilon L^{+}\le c L^{-}\ge c c=L^{+}=L^{-} N_1 a_n < L^{+} + \varepsilon N_2 L^{-} - \varepsilon< a_n N_1 N_2 L^{-} - \varepsilon<a_n < L^{+} + \varepsilon |a_n - c|\le \varepsilon n\ge N c \square","['real-analysis', 'proof-verification']"
73,"If $\displaystyle \lim _{x\to +\infty}y(x)\in \mathbb R$, then $\lim _{x\to +\infty}y'(x)=0$ [duplicate]","If , then  [duplicate]",\displaystyle \lim _{x\to +\infty}y(x)\in \mathbb R \lim _{x\to +\infty}y'(x)=0,"This question already has answers here : If a function has a finite limit at infinity, does that imply its derivative goes to zero? (6 answers) Closed 10 years ago . Not homework. I need this (or something similar) to solve 4. in this question . Let $y:(a ,+\infty)\to \mathbb R$ be $C^1$. Prove that $$\lim_{x\to +\infty}y(x)=\eta\text{ for some }\eta\in \mathbb R\implies\text{the following limit exists and } \lim_{x\to +\infty}y'(x)=0$$ Intuitively this is true because $\displaystyle \lim_{x\to +\infty}y(x)=\eta$ means that $y$ almost stops increasing or decreasing, so $\displaystyle \lim_{x\to +\infty}y'(x)=0$. But how to prove it? I tried $$\lim_{x\to +\infty}y'(x)=0=\lim_{x\to +\infty} \lim_{h\to 0}\dfrac{y(x+h)-y(x)}h= \lim_{h\to 0}\lim_{x\to +\infty}\dfrac{y(x+h)-y(x)}h= \lim_{h\to 0}\dfrac{a-a}h=0,$$ but why I can change the order of the limits? If I can't even do that, how can I prove this? After reading the threads Tyler provided, I now just need to prove that $\displaystyle \lim_{x\to +\infty}y'(x)$ exists? Please consider a suitable adaption to the  linked question.","This question already has answers here : If a function has a finite limit at infinity, does that imply its derivative goes to zero? (6 answers) Closed 10 years ago . Not homework. I need this (or something similar) to solve 4. in this question . Let $y:(a ,+\infty)\to \mathbb R$ be $C^1$. Prove that $$\lim_{x\to +\infty}y(x)=\eta\text{ for some }\eta\in \mathbb R\implies\text{the following limit exists and } \lim_{x\to +\infty}y'(x)=0$$ Intuitively this is true because $\displaystyle \lim_{x\to +\infty}y(x)=\eta$ means that $y$ almost stops increasing or decreasing, so $\displaystyle \lim_{x\to +\infty}y'(x)=0$. But how to prove it? I tried $$\lim_{x\to +\infty}y'(x)=0=\lim_{x\to +\infty} \lim_{h\to 0}\dfrac{y(x+h)-y(x)}h= \lim_{h\to 0}\lim_{x\to +\infty}\dfrac{y(x+h)-y(x)}h= \lim_{h\to 0}\dfrac{a-a}h=0,$$ but why I can change the order of the limits? If I can't even do that, how can I prove this? After reading the threads Tyler provided, I now just need to prove that $\displaystyle \lim_{x\to +\infty}y'(x)$ exists? Please consider a suitable adaption to the  linked question.",,"['calculus', 'real-analysis', 'limits', 'derivatives']"
74,Learning Proofs (for Computer Science),Learning Proofs (for Computer Science),,"Harvard's math curriculum, for freshmen, is divided into 4 classes beyond the BC Calculus level, Math 21, 23, 25 and 55. Math 21 is your classic plug-and-chug multivariable calculus and linear algebra course. The rest of the courses teach multivariable calculus and linear algebra in the context of proofs, along with some real analysis. I decided to take Math 21 this semester (don't ask me why, it was a horrible decision and it is why I am writing this post). Anyways, I now realize that I need to learn proofs in order to use them in higher-level computer science classes. Plus, I might have an interest in higher mathematics in concepts such as real analysis, probability theory, optimization, etc. How should I go about learning proofs, specifically in the context of discrete math, linear algebra, and real analysis, so that I can apply my knowledge to computer science and the higher-level math in which I might be interested? Thanks for your advice in advance. This is my first question on Math Stack Exchange, and I'm curious to see what the community is like!","Harvard's math curriculum, for freshmen, is divided into 4 classes beyond the BC Calculus level, Math 21, 23, 25 and 55. Math 21 is your classic plug-and-chug multivariable calculus and linear algebra course. The rest of the courses teach multivariable calculus and linear algebra in the context of proofs, along with some real analysis. I decided to take Math 21 this semester (don't ask me why, it was a horrible decision and it is why I am writing this post). Anyways, I now realize that I need to learn proofs in order to use them in higher-level computer science classes. Plus, I might have an interest in higher mathematics in concepts such as real analysis, probability theory, optimization, etc. How should I go about learning proofs, specifically in the context of discrete math, linear algebra, and real analysis, so that I can apply my knowledge to computer science and the higher-level math in which I might be interested? Thanks for your advice in advance. This is my first question on Math Stack Exchange, and I'm curious to see what the community is like!",,"['real-analysis', 'linear-algebra', 'proof-writing', 'computer-science']"
75,Is this theorem in Rudin's Real and Complex Analysis wrong as stated?,Is this theorem in Rudin's Real and Complex Analysis wrong as stated?,,"I have forgotten all of my measure theory and will now ask a very dumb question. Consider the following theorem, which I have produced word for word from Rudin's Real and Complex Analysis , third edition. It is theorem $7.13$ in chapter $7$ and can be found on page $142$. Associate to each $x\in \mathbb R^k$ a sequence $\{E_i(x))\}$ that   shrinks to x nicely. If $\mu$ is a complex Borel measure and $\mu  \perp m$, then $$\lim _{i\rightarrow \infty} \frac{\mu(E_i(x))}{m(E_i(x))} =0\  \text{a.e.} \ [m].$$ Consider the measure $\mu$ defined by giving a point mass to each point in $\mathbb Q$. This is a singular measure with respect to the Lebesgue measure. In other words, the measure of a set is the number of rational points it contains. Let $x=0$ in $\mathbb R$, and consider the intervals $(-1/n, 1/n)$ that shrink nicely to $0$. The $\mu$ measure of these is infinite. So each term in the limit is infinite, and the limit itself must be infinite. This seems to contradict the theorem. Where have I gone wrong in this reasoning? ( This question is closely related.)","I have forgotten all of my measure theory and will now ask a very dumb question. Consider the following theorem, which I have produced word for word from Rudin's Real and Complex Analysis , third edition. It is theorem $7.13$ in chapter $7$ and can be found on page $142$. Associate to each $x\in \mathbb R^k$ a sequence $\{E_i(x))\}$ that   shrinks to x nicely. If $\mu$ is a complex Borel measure and $\mu  \perp m$, then $$\lim _{i\rightarrow \infty} \frac{\mu(E_i(x))}{m(E_i(x))} =0\  \text{a.e.} \ [m].$$ Consider the measure $\mu$ defined by giving a point mass to each point in $\mathbb Q$. This is a singular measure with respect to the Lebesgue measure. In other words, the measure of a set is the number of rational points it contains. Let $x=0$ in $\mathbb R$, and consider the intervals $(-1/n, 1/n)$ that shrink nicely to $0$. The $\mu$ measure of these is infinite. So each term in the limit is infinite, and the limit itself must be infinite. This seems to contradict the theorem. Where have I gone wrong in this reasoning? ( This question is closely related.)",,"['real-analysis', 'measure-theory']"
76,Need to prove $f$ continuous at $x_0$ iff for every monotonic sequence $(x_n)$ converging to $x_0$ we have $\lim f(x_n)=f(x_0)$,Need to prove  continuous at  iff for every monotonic sequence  converging to  we have,f x_0 (x_n) x_0 \lim f(x_n)=f(x_0),"This was a problem that the Professor went over in class, but I am having trouble understanding and finishing the proof. The full question is: $f:I \rightarrow \mathbb R$ is continuous at $x_0 \in I$ if and only if for any monotonic sequence $x_n$, with $x_n \rightarrow x_0$ we have $f(x_n) \rightarrow f(x_0)$ This is his solution (I'll mention where I am confused): We know that for every monotonic sequence   $$(x_n) \rightarrow x_0, f(x_n) \rightarrow f(x_0)$$   Want to show that    $$x_n \rightarrow x_0 \implies f(x_n) \rightarrow f(x_0) $$   for any $x_n$.   With $x_n \rightarrow x_0$ we know there exists a monotonic convergent subsequence,   $$x_{n_k} \rightarrow x_0 .$$ Now we want to show $\{f(x_n)\}_n$ is Cauchy. (Where did this come from? Why does this need to be shown? From what I understand, $\{f(x_n)\}_n$ is a subsequence  of the function $f$? Or would this technically be called a subfunction? I don't know if there is such a thing) For any $\epsilon > 0, \exists N_{\epsilon}$ such that    $$|f(x_m)-f(x_n)|<\epsilon \quad for \quad m,n>N_{\epsilon} $$   If not, $\exists \epsilon_0, \forall N\in \mathbb R$ such that    $$|f(x_m)-f(x_n)|\geq \epsilon \quad for \quad some \quad m,n>N$$ I am supposed to finish the proof by showing that $\{f(x_n)\}_n$ is Cauchy, but I am not sure how to do this and don't know where to begin. Sorry if the question involves a lot of explaining, it isn't a homework problem to be turned in but I need to understand what is going on (if I can show it is Cauchy, however, I do get a bit of extra credit, so please don't give me the answer off the bat). Thanks for any help! This function\continuity chapter really has me scratching my head.","This was a problem that the Professor went over in class, but I am having trouble understanding and finishing the proof. The full question is: $f:I \rightarrow \mathbb R$ is continuous at $x_0 \in I$ if and only if for any monotonic sequence $x_n$, with $x_n \rightarrow x_0$ we have $f(x_n) \rightarrow f(x_0)$ This is his solution (I'll mention where I am confused): We know that for every monotonic sequence   $$(x_n) \rightarrow x_0, f(x_n) \rightarrow f(x_0)$$   Want to show that    $$x_n \rightarrow x_0 \implies f(x_n) \rightarrow f(x_0) $$   for any $x_n$.   With $x_n \rightarrow x_0$ we know there exists a monotonic convergent subsequence,   $$x_{n_k} \rightarrow x_0 .$$ Now we want to show $\{f(x_n)\}_n$ is Cauchy. (Where did this come from? Why does this need to be shown? From what I understand, $\{f(x_n)\}_n$ is a subsequence  of the function $f$? Or would this technically be called a subfunction? I don't know if there is such a thing) For any $\epsilon > 0, \exists N_{\epsilon}$ such that    $$|f(x_m)-f(x_n)|<\epsilon \quad for \quad m,n>N_{\epsilon} $$   If not, $\exists \epsilon_0, \forall N\in \mathbb R$ such that    $$|f(x_m)-f(x_n)|\geq \epsilon \quad for \quad some \quad m,n>N$$ I am supposed to finish the proof by showing that $\{f(x_n)\}_n$ is Cauchy, but I am not sure how to do this and don't know where to begin. Sorry if the question involves a lot of explaining, it isn't a homework problem to be turned in but I need to understand what is going on (if I can show it is Cauchy, however, I do get a bit of extra credit, so please don't give me the answer off the bat). Thanks for any help! This function\continuity chapter really has me scratching my head.",,"['real-analysis', 'limits', 'convergence-divergence', 'continuity', 'cauchy-sequences']"
77,"A theorem about Cesàro mean, related to Stolz-Cesàro theorem","A theorem about Cesàro mean, related to Stolz-Cesàro theorem",,"Original Title: Tauberian theorems and Cesàro sum Theorem (Landau-Hardy, From Rudin's Principle of Mathematical Analysis Exercise 3.14) $\newcommand\abs[1]{\left\lvert#1\right\rvert}$ If $\{s_n\}$ is a complex sequence, define its arithmetic means $\sigma_n$ by $$\sigma_n=\frac{s_0+s_1+\dotsb+s_n}{n+1}\qquad(n=0,1,2,\dotsc)$$ Put $a_n=s_n-s_{n-1}$ for $n\ge1$. Assume $M<+\infty$ and $\abs{na_n}\le M$ for all $n$, and $\lim_{n\to\infty}\sigma_n=\sigma$, then $\lim_{n\to\infty}s_n=\sigma$. The outline of the proof If $m<n$, then $$s_n-\sigma_n=\frac{m+1}{n-m}(\sigma_n-\sigma_m)+\frac1{n-m}\sum_{k=m+1}^n(s_n-s_k)\tag{*}$$ Notice that $\abs{s_n-s_k}\le(n-m-1)M\,/\,(m+2)$, fix $\epsilon>0$ and associate with each $n$ the integer $m$ that satisfies $$m\le\frac{n-\epsilon}{1+\epsilon}<m+1$$ Then $(m+1)\,/\,(n-m)\le1/\epsilon$ and $\abs{s_n-s_k}<M\epsilon$. Hence $$\limsup_{n\to\infty}\,\abs{s_n-\sigma}\le M\epsilon$$ Questions and thoughts It seems that the equation (*) comes out strangely. I wonder how to discover such kind of strange identities. So is there any observation, even deeper, to look through that equation? Thanks!","Original Title: Tauberian theorems and Cesàro sum Theorem (Landau-Hardy, From Rudin's Principle of Mathematical Analysis Exercise 3.14) $\newcommand\abs[1]{\left\lvert#1\right\rvert}$ If $\{s_n\}$ is a complex sequence, define its arithmetic means $\sigma_n$ by $$\sigma_n=\frac{s_0+s_1+\dotsb+s_n}{n+1}\qquad(n=0,1,2,\dotsc)$$ Put $a_n=s_n-s_{n-1}$ for $n\ge1$. Assume $M<+\infty$ and $\abs{na_n}\le M$ for all $n$, and $\lim_{n\to\infty}\sigma_n=\sigma$, then $\lim_{n\to\infty}s_n=\sigma$. The outline of the proof If $m<n$, then $$s_n-\sigma_n=\frac{m+1}{n-m}(\sigma_n-\sigma_m)+\frac1{n-m}\sum_{k=m+1}^n(s_n-s_k)\tag{*}$$ Notice that $\abs{s_n-s_k}\le(n-m-1)M\,/\,(m+2)$, fix $\epsilon>0$ and associate with each $n$ the integer $m$ that satisfies $$m\le\frac{n-\epsilon}{1+\epsilon}<m+1$$ Then $(m+1)\,/\,(n-m)\le1/\epsilon$ and $\abs{s_n-s_k}<M\epsilon$. Hence $$\limsup_{n\to\infty}\,\abs{s_n-\sigma}\le M\epsilon$$ Questions and thoughts It seems that the equation (*) comes out strangely. I wonder how to discover such kind of strange identities. So is there any observation, even deeper, to look through that equation? Thanks!",,"['calculus', 'real-analysis', 'sequences-and-series', 'reference-request']"
78,Finding upper and lower derivatives,Finding upper and lower derivatives,,"Let $$D^+f(x)=\lim\limits_{h \to 0}\left[\sup\limits_{0<|t|\leq h}\frac{f(x+t)-f(x)}{t}\right] \text{  and  } D^-f(x)=\lim\limits_{h \to 0}\left[\inf\limits_{0<|t|\leq h}\frac{f(x+t)-f(x)}{t}\right]$$ represent the upper and lower derivative respectively. Let $f(x)=\begin{cases} x\sin(\frac{1}{x}) \text{ if } x \neq 0 \\ 0 \text{ if } x =0\end{cases}$ and $g(x)=\chi_{\mathbb{Q}}$. My attempt at a solution: If $x=0$, $D^+f(x)=0$ and $D^-f(x)=-\infty$.  If $x \neq 0$, then $D^+f(x)=\infty$ and $D^-f(x)=0$. I believe the answer is similar to $g(x)$.","Let $$D^+f(x)=\lim\limits_{h \to 0}\left[\sup\limits_{0<|t|\leq h}\frac{f(x+t)-f(x)}{t}\right] \text{  and  } D^-f(x)=\lim\limits_{h \to 0}\left[\inf\limits_{0<|t|\leq h}\frac{f(x+t)-f(x)}{t}\right]$$ represent the upper and lower derivative respectively. Let $f(x)=\begin{cases} x\sin(\frac{1}{x}) \text{ if } x \neq 0 \\ 0 \text{ if } x =0\end{cases}$ and $g(x)=\chi_{\mathbb{Q}}$. My attempt at a solution: If $x=0$, $D^+f(x)=0$ and $D^-f(x)=-\infty$.  If $x \neq 0$, then $D^+f(x)=\infty$ and $D^-f(x)=0$. I believe the answer is similar to $g(x)$.",,['real-analysis']
79,Distributional derivative coincides with classical derivative?,Distributional derivative coincides with classical derivative?,,"I am having some trouble understanding the precise meaning of the following statement: ""if $f \in C^1 (\Omega)$ for some $\Omega \subset \mathbb R^n$, then the distributional derivative of $f$ coincides with its classical derivative"". I know that $f$ induces the following distribution: $$ T_f (\phi) = \int f \phi , $$ where the integral is taken over $\Omega$ $\phi \in \mathscr D (\Omega)$ is a test function (I'm assuming $\mathbb R^n = \mathbb R$ for simplicity), and then the distributional derivative of $f$ can be represented as  $$ \frac{d}{dx} T_f (\phi) = - \int f \frac{d \phi}{dx} . $$ What is then to be understood by saying that the classical derivative of $f$ is equal to its distributional derivative? Integration by parts shows that $$ \frac{d}{dx} T_f (\phi) = - \int f \frac{d \phi}{dx} = - \left( - \int \phi f' \right) = T_{\frac{df}{dx} } (\phi) , $$ therefore $$ \frac{d}{dx} T_f (\phi) = T_{\frac{df}{dx} } (\phi) . $$ Since this last equation is valid for every test function $\phi$, does it then follow that both concepts of derivative are equivalent? Any comments would be much appreciated.","I am having some trouble understanding the precise meaning of the following statement: ""if $f \in C^1 (\Omega)$ for some $\Omega \subset \mathbb R^n$, then the distributional derivative of $f$ coincides with its classical derivative"". I know that $f$ induces the following distribution: $$ T_f (\phi) = \int f \phi , $$ where the integral is taken over $\Omega$ $\phi \in \mathscr D (\Omega)$ is a test function (I'm assuming $\mathbb R^n = \mathbb R$ for simplicity), and then the distributional derivative of $f$ can be represented as  $$ \frac{d}{dx} T_f (\phi) = - \int f \frac{d \phi}{dx} . $$ What is then to be understood by saying that the classical derivative of $f$ is equal to its distributional derivative? Integration by parts shows that $$ \frac{d}{dx} T_f (\phi) = - \int f \frac{d \phi}{dx} = - \left( - \int \phi f' \right) = T_{\frac{df}{dx} } (\phi) , $$ therefore $$ \frac{d}{dx} T_f (\phi) = T_{\frac{df}{dx} } (\phi) . $$ Since this last equation is valid for every test function $\phi$, does it then follow that both concepts of derivative are equivalent? Any comments would be much appreciated.",,['real-analysis']
80,Does the integral test work on higher dimensions?,Does the integral test work on higher dimensions?,,"The integral test of convergence states that, if $f:[1,+\infty)\to[0,+\infty)$ is a monotonically decreasing nonnegative function, then the series $\sum_1^\infty f(n)$ converges iff $\int_1^\infty f(n) dn$ is finite. Is the high-dimensional generalization also true? That is, given $f:[1,+\infty)^N \to[0,+\infty)$, and $f(\dotsc,n_i,\dotsc) \ge f(\dotsc,n_i+\epsilon,\dotsc)$ for all $1\le i\le N$ and $n_i\in[1,+\infty)$ and $\epsilon>0$, then the sum $$ \sum_{n_1=1}^\infty  \cdots \sum_{n_N=1}^\infty f(n_1,\dotsc,n_N) $$ converges iff the multiple integral $$ \int_1^\infty \cdots \int_1^\infty f(n_1,\dotsc,n_N) dn_1 \dotsm dn_N $$ is finite. (This is just for checking if my answer over physics.SE is reasonable.)","The integral test of convergence states that, if $f:[1,+\infty)\to[0,+\infty)$ is a monotonically decreasing nonnegative function, then the series $\sum_1^\infty f(n)$ converges iff $\int_1^\infty f(n) dn$ is finite. Is the high-dimensional generalization also true? That is, given $f:[1,+\infty)^N \to[0,+\infty)$, and $f(\dotsc,n_i,\dotsc) \ge f(\dotsc,n_i+\epsilon,\dotsc)$ for all $1\le i\le N$ and $n_i\in[1,+\infty)$ and $\epsilon>0$, then the sum $$ \sum_{n_1=1}^\infty  \cdots \sum_{n_N=1}^\infty f(n_1,\dotsc,n_N) $$ converges iff the multiple integral $$ \int_1^\infty \cdots \int_1^\infty f(n_1,\dotsc,n_N) dn_1 \dotsm dn_N $$ is finite. (This is just for checking if my answer over physics.SE is reasonable.)",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
81,"Let f be a continuous real valued function on the compact interval [a,b]. Given ϵ>0, show that there is a polynomial p such that: |p(x)−f(x)|<ϵ","Let f be a continuous real valued function on the compact interval [a,b]. Given ϵ>0, show that there is a polynomial p such that: |p(x)−f(x)|<ϵ",,"Let $f$ be a continuous real valued function on the compact interval $[a,b]$ . Given $\epsilon > 0$ , show that there is a polynomial $p$ such that: $p(a)=f(a)$ , $p'(a)=0$ and $|p(x) - f(x)| < \epsilon$ This is a question I came across in Pugh's Real Mathematical Analysis. Is this not a simple application of the Weierstrass Approximation Theorem and the fact that the set of polynomials is dense in $C^0([a, b], \mathbb{R})$ . Density means that for each $f \in C^0$ and each $\epsilon > 0$ there is a polynomial function $p(x)$ such that for all $x \in [a, b]$ , $|f(x) − p(x)| < \epsilon$ . So the third property being asked is immediately satisfied. Is this correct? But how does this imply that $p(a)=f(a)$ and $p'(a)=0$ ? Am I misreading the question or not understanding it at all? Any tips or clarifications would be greatly appreciated. $\mathbf{Edit:} \mathbf{ Attempted} \mathbf{ Solution}$ $f(x)$ is a continuous function on the compact interval $[a,b]$ . By the Weierstrass Approximation Theorem we can find a polynomial $q(x)$ such that: $|f(x)-q(x)|<\epsilon/2$ So, let $p(x)=q(x)-q(a) +f(a)$ , then $p(a)=f(a)$ Given $\epsilon > 0$ , define $q(x) = (\frac{b-a}{n}$ ) $(\frac{x-a}{b-a})^n$ for large $n$ Then, $q'(x)=(\frac{x-a}{b-a})^{n-1}$ so $q'(a)= 0$ and thus $p'(a)=0$ Finally, $|p(x) - f(x)| < |q(x) - f(x)| + |q(a)-f(a)|$ $|p(x) - f(x)| < \epsilon/2 + \epsilon/2$ $|p(x) - f(x)| < \epsilon$ Is this a reasonable solution? Thank you kindly.","Let be a continuous real valued function on the compact interval . Given , show that there is a polynomial such that: , and This is a question I came across in Pugh's Real Mathematical Analysis. Is this not a simple application of the Weierstrass Approximation Theorem and the fact that the set of polynomials is dense in . Density means that for each and each there is a polynomial function such that for all , . So the third property being asked is immediately satisfied. Is this correct? But how does this imply that and ? Am I misreading the question or not understanding it at all? Any tips or clarifications would be greatly appreciated. is a continuous function on the compact interval . By the Weierstrass Approximation Theorem we can find a polynomial such that: So, let , then Given , define ) for large Then, so and thus Finally, Is this a reasonable solution? Thank you kindly.","f [a,b] \epsilon > 0 p p(a)=f(a) p'(a)=0 |p(x) - f(x)| < \epsilon C^0([a, b], \mathbb{R}) f \in C^0 \epsilon > 0 p(x) x \in [a, b] |f(x) − p(x)| < \epsilon p(a)=f(a) p'(a)=0 \mathbf{Edit:} \mathbf{ Attempted} \mathbf{ Solution} f(x) [a,b] q(x) |f(x)-q(x)|<\epsilon/2 p(x)=q(x)-q(a) +f(a) p(a)=f(a) \epsilon > 0 q(x) = (\frac{b-a}{n} (\frac{x-a}{b-a})^n n q'(x)=(\frac{x-a}{b-a})^{n-1} q'(a)= 0 p'(a)=0 |p(x) - f(x)| < |q(x) - f(x)| + |q(a)-f(a)| |p(x) - f(x)| < \epsilon/2 + \epsilon/2 |p(x) - f(x)| < \epsilon","['real-analysis', 'general-topology', 'solution-verification', 'metric-spaces', 'weierstrass-approximation']"
82,Approximating step functions with polynomials,Approximating step functions with polynomials,,"Let $t_1 < t_2 < \cdots <t_m$ be real, and $X = \cup_{i=1}^{m-1} (t_i, t_{i+1})$ be a union of real open intervals.  Let $f:X \rightarrow \{-1, 1\}$ be any piecewise constant function of form $$ f(x) =      \begin{cases}        a_1 & \text{ if } t_1 < x < t_2 \\       a_2 & \text{ if }t_2 < x < t_3 \\       \vdots \\ a_{m-2} & \text{ if } t_{m-2} < x < t_{m-1} \\        a_{m-1} & \text{ if } t_{m-1} < x < t_m     \end{cases} $$ Where $a_i \in \{-1, 1\}$ , and $a_{i} = -a_{i+1}$ for $i = 1, ..., m-1$ . I have a number of questions regarding polynomial approximations of such a function $f$ : Can we always find a sequence of polynomials $(p_n)$ so that $(p_n)$ converge pointwise to $f$ , AND we have some fixed (not arbitrary) global error bound, say $1$ , such that $|p_n(x) - f(x)| \leq 1$ for all $x \in X$ and $n \in \mathbb{N}$ ? If so, are such polynomials easy to find? How quickly do we get convergence? I am aware that, upon picking a suitable inner product, we can use any collection of orthonormal polynomials to make approximations of functions.  For example I know the Chebyshev, Bernstein, Jacobi etc. polynomials can be used to approximate continuous functions on bounded intervals, but I have found no theorem that says we can use these to construct approximations for arbitrary piecewise constant functions like the one given above. Indeed, it is easy to find a polynomial approximation for the Heaviside Step function for example, however it is unclear how, or if this an be done for more complicated step functions.","Let be real, and be a union of real open intervals.  Let be any piecewise constant function of form Where , and for . I have a number of questions regarding polynomial approximations of such a function : Can we always find a sequence of polynomials so that converge pointwise to , AND we have some fixed (not arbitrary) global error bound, say , such that for all and ? If so, are such polynomials easy to find? How quickly do we get convergence? I am aware that, upon picking a suitable inner product, we can use any collection of orthonormal polynomials to make approximations of functions.  For example I know the Chebyshev, Bernstein, Jacobi etc. polynomials can be used to approximate continuous functions on bounded intervals, but I have found no theorem that says we can use these to construct approximations for arbitrary piecewise constant functions like the one given above. Indeed, it is easy to find a polynomial approximation for the Heaviside Step function for example, however it is unclear how, or if this an be done for more complicated step functions.","t_1 < t_2 < \cdots <t_m X = \cup_{i=1}^{m-1} (t_i, t_{i+1}) f:X \rightarrow \{-1, 1\} 
f(x) = 
    \begin{cases} 
      a_1 & \text{ if } t_1 < x < t_2 \\
      a_2 & \text{ if }t_2 < x < t_3 \\
      \vdots \\
a_{m-2} & \text{ if } t_{m-2} < x < t_{m-1} \\ 
      a_{m-1} & \text{ if } t_{m-1} < x < t_m 
   \end{cases}
 a_i \in \{-1, 1\} a_{i} = -a_{i+1} i = 1, ..., m-1 f (p_n) (p_n) f 1 |p_n(x) - f(x)| \leq 1 x \in X n \in \mathbb{N}","['real-analysis', 'polynomials', 'numerical-methods', 'approximation']"
83,Proving $\int_0^\infty\frac{\mathrm dw}{(n+w)(\pi^2+(\log w)^2)}=\frac1{\log n}-\frac1{n-1}$ for any positive integer $n\geq 2$,Proving  for any positive integer,\int_0^\infty\frac{\mathrm dw}{(n+w)(\pi^2+(\log w)^2)}=\frac1{\log n}-\frac1{n-1} n\geq 2,"For any positive integer $n\geq 2$ prove that $$\int_0^\infty\frac{\mathrm dw}{(n+w)(\pi^2+(\log w)^2)}=\frac1{\log n}-\frac1{n-1}.$$ Wolfram Alpha unfortunately cannot give a step-by-step solution (it does not even give me the form on the above RHS even if I set $n=10$ , say, but comparing first few decimal digits shows that the result is almost surely correct). The only idea I had was $w=\mathrm e^{\pi\tan \theta}$ in order to kill the second multiplier of the denominator, but then I cannot deal with $$\frac{1}{\pi}\int_{-\pi/2}^{\pi/2}\frac{\exp(\pi\tan \theta)}{(n+\exp(\pi\tan \theta))}\,\mathrm d\theta.$$ Any help appreciated! UPDATE: Letting $w=\mathrm e^z$ and then using the residue theorem on $\displaystyle \int_{-\infty}^{\infty} \frac{\mathrm dz}{(n\mathrm e^{-z}+1)(\pi^2 + z^2)}$ with the semicircular contour centered at $0$ and radius $R$ (in the upper-half plane) seems to be a very promising idea, but unfortunately the contribution from the circular part does not tend to $0$ as $R\to \infty$ . :(","For any positive integer prove that Wolfram Alpha unfortunately cannot give a step-by-step solution (it does not even give me the form on the above RHS even if I set , say, but comparing first few decimal digits shows that the result is almost surely correct). The only idea I had was in order to kill the second multiplier of the denominator, but then I cannot deal with Any help appreciated! UPDATE: Letting and then using the residue theorem on with the semicircular contour centered at and radius (in the upper-half plane) seems to be a very promising idea, but unfortunately the contribution from the circular part does not tend to as . :(","n\geq 2 \int_0^\infty\frac{\mathrm dw}{(n+w)(\pi^2+(\log w)^2)}=\frac1{\log n}-\frac1{n-1}. n=10 w=\mathrm e^{\pi\tan \theta} \frac{1}{\pi}\int_{-\pi/2}^{\pi/2}\frac{\exp(\pi\tan \theta)}{(n+\exp(\pi\tan \theta))}\,\mathrm d\theta. w=\mathrm e^z \displaystyle \int_{-\infty}^{\infty} \frac{\mathrm dz}{(n\mathrm e^{-z}+1)(\pi^2 + z^2)} 0 R 0 R\to \infty","['real-analysis', 'integration', 'complex-analysis']"
84,Showing a sequence is not Schauder basis,Showing a sequence is not Schauder basis,,"I'm tring to show that in $L^2[0,1]$ , there are at least two distinct choices of coefficients $\left(c_{n}\right)_{n \in \mathbb{Z}}$ such that for $0<b<1$ $$1=\sum_{n \in \mathbb{Z}} c_{n} e^{2 \pi i b n x}.$$ One seems obvious that $c_0=1$ and other $c_n$ are all $0$ . How to determine other choices of $c_n$ ? Also I want to show this series converges in $L^2$ -norm so $ \left\{e^{2 \pi i b n x}\right\}_{n \in \mathbb{Z}} $ is not a Schauder basis.","I'm tring to show that in , there are at least two distinct choices of coefficients such that for One seems obvious that and other are all . How to determine other choices of ? Also I want to show this series converges in -norm so is not a Schauder basis.","L^2[0,1] \left(c_{n}\right)_{n \in \mathbb{Z}} 0<b<1 1=\sum_{n \in \mathbb{Z}} c_{n} e^{2 \pi i b n x}. c_0=1 c_n 0 c_n L^2 
\left\{e^{2 \pi i b n x}\right\}_{n \in \mathbb{Z}}
","['real-analysis', 'functional-analysis']"
85,What do you call this property involving a function between two complete metric spaces?,What do you call this property involving a function between two complete metric spaces?,,"I have a notion, for which I am not able to find any reference name, as I am not that familiar with these concepts. Please help me by pointing to a definition for the below scenario. Is there a name for the following property of the setup? There is a a continuous and onto function $e : A \to B$, $A$ and $B$ being two different complete metric spaces. For any element $b\in B$, and for any element $a \in e^{-1}(\{b\})$, (where $e^{-1}(\{b\})$ is the pre-image of the element $b$ in the function $e$), For every punctured neighbourhood of $b$ denoted as $P_{\epsilon}(b)$, the pre-image $e^{-1}(P_{\epsilon}(b))$  contains a sequence $\{a_n\}$, such that $\{a_n\} \to a$","I have a notion, for which I am not able to find any reference name, as I am not that familiar with these concepts. Please help me by pointing to a definition for the below scenario. Is there a name for the following property of the setup? There is a a continuous and onto function $e : A \to B$, $A$ and $B$ being two different complete metric spaces. For any element $b\in B$, and for any element $a \in e^{-1}(\{b\})$, (where $e^{-1}(\{b\})$ is the pre-image of the element $b$ in the function $e$), For every punctured neighbourhood of $b$ denoted as $P_{\epsilon}(b)$, the pre-image $e^{-1}(P_{\epsilon}(b))$  contains a sequence $\{a_n\}$, such that $\{a_n\} \to a$",,"['real-analysis', 'general-topology', 'analysis', 'functions', 'metric-spaces']"
86,"Intermediate inequalities; is there a way to know if you're getting a ""bad deal""?","Intermediate inequalities; is there a way to know if you're getting a ""bad deal""?",,"Let's say I want to prove a contest-style inequality $f(a, b, c) + g(a, b, c) \ge h(a, b, c)$ in some $S \subseteq \mathbb{R}^3$ . Suppose I want to make the RHS simpler by applying the AM-GM inequality, so that now it remains to prove $2\sqrt {f(a, b, c) g(a, b, c)} \ge h(a, b, c)$ . However, this new inequality may or may not be true. So my questions are: 1) Is it known which inequalities are ""better than others""? For example, when you solve an inequality do you think ""Let me try to avoid using QM-AM, Am-HM is better"" 2) If there are no general rules for question 1), is there a way for me to tell if I'm getting a ""bad deal"" by using some inequality? 3) Does the answer to $1)$ depend on the internal structure of the expressions you're applying the inequalities to? For example, is it possible that applying AM-GM to $a^2+2b+c$ and $b^2+2a+c$ gives a very bad inequality, but applying something like $\dfrac {ab}{c}$ and $\dfrac {bc}{a}$ gives a very good inequality? If you have a book recomendation that deals with the topic I would appreciate it as well.","Let's say I want to prove a contest-style inequality in some . Suppose I want to make the RHS simpler by applying the AM-GM inequality, so that now it remains to prove . However, this new inequality may or may not be true. So my questions are: 1) Is it known which inequalities are ""better than others""? For example, when you solve an inequality do you think ""Let me try to avoid using QM-AM, Am-HM is better"" 2) If there are no general rules for question 1), is there a way for me to tell if I'm getting a ""bad deal"" by using some inequality? 3) Does the answer to depend on the internal structure of the expressions you're applying the inequalities to? For example, is it possible that applying AM-GM to and gives a very bad inequality, but applying something like and gives a very good inequality? If you have a book recomendation that deals with the topic I would appreciate it as well.","f(a, b, c) + g(a, b, c) \ge h(a, b, c) S \subseteq \mathbb{R}^3 2\sqrt {f(a, b, c) g(a, b, c)} \ge h(a, b, c) 1) a^2+2b+c b^2+2a+c \dfrac {ab}{c} \dfrac {bc}{a}","['real-analysis', 'inequality', 'reference-request', 'contest-math', 'a.m.-g.m.-inequality']"
87,Can one determine where $\vert z^2-x \vert \le \vert z\vert$?,Can one determine where ?,\vert z^2-x \vert \le \vert z\vert,"Please look at the nice attempt in the answer below, too before you answer: Let $z \in \mathbb C$ have positive real part and $x>0$ a positive number. I am trying to find the smallest real part of $z$ such that $$\vert z^2-x \vert \le \vert z\vert.$$ Does anyone know how to solve this? The problem is that over $\mathbb C$ such inequalities become multi-valued since $z=a+ib$ . I should add that wolframalpha also gets some expressions but it is not completely transparent what the minimum possible real part is after all. click me Any comments are highly appreciated.","Please look at the nice attempt in the answer below, too before you answer: Let have positive real part and a positive number. I am trying to find the smallest real part of such that Does anyone know how to solve this? The problem is that over such inequalities become multi-valued since . I should add that wolframalpha also gets some expressions but it is not completely transparent what the minimum possible real part is after all. click me Any comments are highly appreciated.",z \in \mathbb C x>0 z \vert z^2-x \vert \le \vert z\vert. \mathbb C z=a+ib,"['real-analysis', 'calculus']"
88,"Is there an example of $F_{\sigma}$ set in $[0,1]$ with empty interior and with measure $1$? [closed]",Is there an example of  set in  with empty interior and with measure ? [closed],"F_{\sigma} [0,1] 1","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Is there an example of  $F_{\sigma}$ set in $[0,1]$ with empty interior and with measure $1$? I wonder if it exists.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Is there an example of  $F_{\sigma}$ set in $[0,1]$ with empty interior and with measure $1$? I wonder if it exists.",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
89,Use of Lagrange multipliers in pure math problems,Use of Lagrange multipliers in pure math problems,,"I realize that Lagrange multipliers are extremely useful for applied optimization problems. However, I know that the standard analytic proof of the spectral theorem relies on them. I've also seen a few other uses/mentions of them in some pure math textbooks. (For example, Wade's An Introduction to Analysis uses an exercise on Lagrange multipliers to later prove a result due to Bernstein on the convergence of Fourier series.) My question, then, is if Lagrange multipliers are generally a useful   technique for extremal problems that arise in pure math. If so, are   there some well-known proofs in this area that use them (other than   those I mentioned)? I'm simply curious as to their use outside applied optimization, since the derivation of the existence of the so-called Lagrange multiplier is really just a corollary of a very geometric fact--namely that the gradient is perpendicular to the level sets. EDIT: To be a bit more specific, by ""useful"" I mean that it is in fact applicable to certain pure math problems with somewhat regular frequency.","I realize that Lagrange multipliers are extremely useful for applied optimization problems. However, I know that the standard analytic proof of the spectral theorem relies on them. I've also seen a few other uses/mentions of them in some pure math textbooks. (For example, Wade's An Introduction to Analysis uses an exercise on Lagrange multipliers to later prove a result due to Bernstein on the convergence of Fourier series.) My question, then, is if Lagrange multipliers are generally a useful   technique for extremal problems that arise in pure math. If so, are   there some well-known proofs in this area that use them (other than   those I mentioned)? I'm simply curious as to their use outside applied optimization, since the derivation of the existence of the so-called Lagrange multiplier is really just a corollary of a very geometric fact--namely that the gradient is perpendicular to the level sets. EDIT: To be a bit more specific, by ""useful"" I mean that it is in fact applicable to certain pure math problems with somewhat regular frequency.",,"['real-analysis', 'multivariable-calculus', 'derivatives', 'soft-question', 'lagrange-multiplier']"
90,Hypothesis of Stone-Weierstrass Theorem,Hypothesis of Stone-Weierstrass Theorem,,"Why do we need compactness in the space as hypothesis in Stone-Weierstrass Theorem? Theorem: Let $A \subset C(K)$ such that $A$ is a subalgebra that vanishes nowhere. For each $x, y \in K $ with $x \neq  y $, there exists $f \in A$ such that $f(x)\neq f(y)$. Then $ \overline A = C(K)$, where $C(K)$ is the space of continuous functions over the compact space $K$.","Why do we need compactness in the space as hypothesis in Stone-Weierstrass Theorem? Theorem: Let $A \subset C(K)$ such that $A$ is a subalgebra that vanishes nowhere. For each $x, y \in K $ with $x \neq  y $, there exists $f \in A$ such that $f(x)\neq f(y)$. Then $ \overline A = C(K)$, where $C(K)$ is the space of continuous functions over the compact space $K$.",,"['real-analysis', 'general-topology', 'functional-analysis', 'metric-spaces', 'compactness']"
91,Calculate $\int_0^{2\pi} \frac{1}{2+\sin(x)} \ dx$,Calculate,\int_0^{2\pi} \frac{1}{2+\sin(x)} \ dx,$$\int_0^{2\pi} \frac{1}{2+\sin(x)} \ dx$$ $$\sin(x)=\frac{e^{ix}-e^{-ix}}{2i}$$ $$z=\alpha(x)=e^{ix}$$ $$\sin(x)=\frac{z^2-1}{2zi}$$ $$2+\sin(x)=\frac{4zi+z^2-1}{2zi}$$ $$\frac{1}{2+\sin(x)}=\frac{2zi}{4zi+z^2-1}$$ $$\int_\alpha \frac{2zi}{4zi+z^2-1} \ \ \frac{1}{zi}$$ $$4zi+z^2-1=0$$ $$z_1=\frac{-4i+2\sqrt{3}i}{2}$$ $$z_2=\frac{-4i-2\sqrt{3}i}{2}$$ $$\rvert z_1 \rvert <1$$ $$\rvert z_2 \rvert >1$$ Using Residue theorem: $$\int_\alpha \frac{2}{4zi+z^2-1}=2 \pi i \ \lim_{z\rightarrow-2i+\sqrt{3}i} \ \frac{2}{z+2i+\sqrt{3}i}=\frac{2 \pi}{\sqrt{3}}$$ Is it correct? Thanks!,$$\int_0^{2\pi} \frac{1}{2+\sin(x)} \ dx$$ $$\sin(x)=\frac{e^{ix}-e^{-ix}}{2i}$$ $$z=\alpha(x)=e^{ix}$$ $$\sin(x)=\frac{z^2-1}{2zi}$$ $$2+\sin(x)=\frac{4zi+z^2-1}{2zi}$$ $$\frac{1}{2+\sin(x)}=\frac{2zi}{4zi+z^2-1}$$ $$\int_\alpha \frac{2zi}{4zi+z^2-1} \ \ \frac{1}{zi}$$ $$4zi+z^2-1=0$$ $$z_1=\frac{-4i+2\sqrt{3}i}{2}$$ $$z_2=\frac{-4i-2\sqrt{3}i}{2}$$ $$\rvert z_1 \rvert <1$$ $$\rvert z_2 \rvert >1$$ Using Residue theorem: $$\int_\alpha \frac{2}{4zi+z^2-1}=2 \pi i \ \lim_{z\rightarrow-2i+\sqrt{3}i} \ \frac{2}{z+2i+\sqrt{3}i}=\frac{2 \pi}{\sqrt{3}}$$ Is it correct? Thanks!,,"['real-analysis', 'integration', 'analysis']"
92,Show that a convex compact set in $R^2$ can be cut into 4 sets of equal area by 2 perpendicular lines,Show that a convex compact set in  can be cut into 4 sets of equal area by 2 perpendicular lines,R^2,Okay I need to show this using calculus and mean value theorem. My try : Let $D$ be a convex and compact set in $R^2$ Now let $R$ be a compact closed rectangle such that $D \subset R$. Draw two lines parallel to the axis such that $D$ is now composed of four subsets name them $R_1 \cup R_2 \cup R_3 \cup R_4 = D $ Okay now create these functions : $$F(p)= 1 \ if\ p\epsilon R_1 \\ =0 \ else\\\\G(p)= 1 \ if\ p\epsilon R_2 \\ =0 \ else \\\\ H(p)= 1 \ if\ p\epsilon R_3 \\ =0 \ else \\\\ T(p)= 1 \ if\ p\epsilon R_4 \\ =0 \ else$$ Then we have  $$\int\int_{R}F(p)+\int\int_{R}G(p)+\int\int_{R}H(p)+\int\int_{R}T(p)=A(D)$$ $A(D)$ denotes the area of this compact convex set $D$ Now since $$\int\int_{R}F(p)= A(R_1)=\sum_{R_{ij}: \bigcup R_{ij}=R_1} A(R_{ij})$$ We can say that $\exists R_{ij}: \bigcup R_{ij}=R_1$ such that $$\sum_{R_{ij}: \bigcup R_{ij}=R_1} A(R_{ij})> \frac{A(D)}{4}$$$ Also  $\exists R_{ij}: \bigcup R_{ij}=R_1$ such that $$\sum_{R_{ij}: \bigcup R_{ij}=R_1} A(R_{ij}) < \frac{A(D)}{4}$$$ Then by the intermediate value theorem $\exists R_{ij}: \bigcup R_{ij}=R_1$ such that $$\sum_{R_{ij}: \bigcup R_{ij}=R_1} A(R_{ij}) = \frac{A(D)}{4}$$$ Then we can find $R_{ij}$ which form the $R_1$ such so that above is the case. I can apply the same procedure to other functions. What do you think about this approach? Is it correct/incorrect what kind of correction does it need?,Okay I need to show this using calculus and mean value theorem. My try : Let $D$ be a convex and compact set in $R^2$ Now let $R$ be a compact closed rectangle such that $D \subset R$. Draw two lines parallel to the axis such that $D$ is now composed of four subsets name them $R_1 \cup R_2 \cup R_3 \cup R_4 = D $ Okay now create these functions : $$F(p)= 1 \ if\ p\epsilon R_1 \\ =0 \ else\\\\G(p)= 1 \ if\ p\epsilon R_2 \\ =0 \ else \\\\ H(p)= 1 \ if\ p\epsilon R_3 \\ =0 \ else \\\\ T(p)= 1 \ if\ p\epsilon R_4 \\ =0 \ else$$ Then we have  $$\int\int_{R}F(p)+\int\int_{R}G(p)+\int\int_{R}H(p)+\int\int_{R}T(p)=A(D)$$ $A(D)$ denotes the area of this compact convex set $D$ Now since $$\int\int_{R}F(p)= A(R_1)=\sum_{R_{ij}: \bigcup R_{ij}=R_1} A(R_{ij})$$ We can say that $\exists R_{ij}: \bigcup R_{ij}=R_1$ such that $$\sum_{R_{ij}: \bigcup R_{ij}=R_1} A(R_{ij})> \frac{A(D)}{4}$$$ Also  $\exists R_{ij}: \bigcup R_{ij}=R_1$ such that $$\sum_{R_{ij}: \bigcup R_{ij}=R_1} A(R_{ij}) < \frac{A(D)}{4}$$$ Then by the intermediate value theorem $\exists R_{ij}: \bigcup R_{ij}=R_1$ such that $$\sum_{R_{ij}: \bigcup R_{ij}=R_1} A(R_{ij}) = \frac{A(D)}{4}$$$ Then we can find $R_{ij}$ which form the $R_1$ such so that above is the case. I can apply the same procedure to other functions. What do you think about this approach? Is it correct/incorrect what kind of correction does it need?,,[]
93,"Coefficients of a formal power series satisfying $\exp(f(z)) = 1 + f(q\,z)/q$",Coefficients of a formal power series satisfying,"\exp(f(z)) = 1 + f(q\,z)/q","Let $(q;\,q)_n$ denote the $q$-Pochhammer symbol : $$(q;\,q)_n = \prod_{k=1}^n (1 - q^k), \quad(q;\,q)_0 = 1.\tag1$$ Consider a formal power series in $z$: $$f(z) = \sum_{n=1}^\infty \frac{(-1)^{n+1}P_n(q)}{n!\,(q;\,q)_{n-1}}z^n,\tag2$$ where $P_n(q)$ are some (yet unknown) polynomials in $q$:  $$P_n(q) = \sum_{k=0}^{m} c_{n,k} \, q^k,\tag3$$ where $m=\binom{n-1}2 = \frac{(n-1)(n-2)}2$ and $c_{n,k}$ are some integer coefficients. Suppose the formal power series $f(z)$ satisfies the functional equation $$\exp(f(z)) = 1 + f(q\,z)/q.\tag4$$ Expanding the left-hand side of $(4)$ in powers of $z$ using the exponential partial Bell polynomials , and comparing coefficients at corresponding powers of $z$ at both sides, we can obtain a system of equations, by solving which we can find the coefficients of the polynomials $P_n(q)$: $$ \begin{align} P_1(q) &= 1\\ P_2(q) &= 1\\ P_3(q) &= 2 + q\\ P_4(q) &= 6 + 6 q + 5 q^2 + q^3\\ P_5(q) &= 24 + 36 q + 46 q^2 + 40 q^3 + 24 q^4 + 9 q^5 + q^6\\ \dots \end{align}\tag5 $$ This is a quite slow process, even when done on a computer. I computed the polynomials up to $n=27$ (they can be found here ) using a Mathematica program that can be found here . There are some patterns in the coefficients I computed (so far they are just conjectures): $$ \begin{align} c_{n,0} &= (n-1)!&\vphantom{\Huge|}\\ c_{n,1} &= \frac{(n-2)(n-1)!}2, &n\ge2\\ c_{n,2} &= \frac{(3n+8)(n-3)(n-1)!}{24}, &n\ge3\\ c_{n,3} &= \frac{(n^2 + 5 n - 34)\,n!}{48}, & n\ge4 \end{align} \tag6 $$ and $$ \begin{align} c_{n,m} &= 1&\vphantom{\Huge|}\\ c_{n,m-1} &= \frac{(n+1)(n-2)}2, &n\ge2\\ c_{n,m-2} &= \frac{(3 n^3 - 5 n^2 + 6 n + 8)(n-3)}{24}, &n\ge3\\ c_{n,m-3} &= \frac{(n^4 - 10 n^3 + 43 n^2 - 74 n + 16) (n - 1) \, n}{48}, &n\ge4 \end{align} \tag7 $$ where $m=\binom{n-1}2$. Other coefficients seem to follow more complicated patterns. We can also observe that $$ \begin{align} P_n(1) &= \frac{(n-1)!\,n!}{2^{n-1}}\\ P_{2n}(-1) &= \frac{(2n-1)!\,n!}{3^{n-1}}\\ P_{2n-1}(-1) &= \frac{(2n-1)!!\,(2n-2)!}{6^{n-1}}, \end{align}\tag8 $$ where $n!!$ denotes the double factorial . I am trying to find a more direct formula for the polynomials $P_n(q)$ or their coefficients $c_{n,k}$ (possibly, containing finite products and sums, but not requiring to solve equations).","Let $(q;\,q)_n$ denote the $q$-Pochhammer symbol : $$(q;\,q)_n = \prod_{k=1}^n (1 - q^k), \quad(q;\,q)_0 = 1.\tag1$$ Consider a formal power series in $z$: $$f(z) = \sum_{n=1}^\infty \frac{(-1)^{n+1}P_n(q)}{n!\,(q;\,q)_{n-1}}z^n,\tag2$$ where $P_n(q)$ are some (yet unknown) polynomials in $q$:  $$P_n(q) = \sum_{k=0}^{m} c_{n,k} \, q^k,\tag3$$ where $m=\binom{n-1}2 = \frac{(n-1)(n-2)}2$ and $c_{n,k}$ are some integer coefficients. Suppose the formal power series $f(z)$ satisfies the functional equation $$\exp(f(z)) = 1 + f(q\,z)/q.\tag4$$ Expanding the left-hand side of $(4)$ in powers of $z$ using the exponential partial Bell polynomials , and comparing coefficients at corresponding powers of $z$ at both sides, we can obtain a system of equations, by solving which we can find the coefficients of the polynomials $P_n(q)$: $$ \begin{align} P_1(q) &= 1\\ P_2(q) &= 1\\ P_3(q) &= 2 + q\\ P_4(q) &= 6 + 6 q + 5 q^2 + q^3\\ P_5(q) &= 24 + 36 q + 46 q^2 + 40 q^3 + 24 q^4 + 9 q^5 + q^6\\ \dots \end{align}\tag5 $$ This is a quite slow process, even when done on a computer. I computed the polynomials up to $n=27$ (they can be found here ) using a Mathematica program that can be found here . There are some patterns in the coefficients I computed (so far they are just conjectures): $$ \begin{align} c_{n,0} &= (n-1)!&\vphantom{\Huge|}\\ c_{n,1} &= \frac{(n-2)(n-1)!}2, &n\ge2\\ c_{n,2} &= \frac{(3n+8)(n-3)(n-1)!}{24}, &n\ge3\\ c_{n,3} &= \frac{(n^2 + 5 n - 34)\,n!}{48}, & n\ge4 \end{align} \tag6 $$ and $$ \begin{align} c_{n,m} &= 1&\vphantom{\Huge|}\\ c_{n,m-1} &= \frac{(n+1)(n-2)}2, &n\ge2\\ c_{n,m-2} &= \frac{(3 n^3 - 5 n^2 + 6 n + 8)(n-3)}{24}, &n\ge3\\ c_{n,m-3} &= \frac{(n^4 - 10 n^3 + 43 n^2 - 74 n + 16) (n - 1) \, n}{48}, &n\ge4 \end{align} \tag7 $$ where $m=\binom{n-1}2$. Other coefficients seem to follow more complicated patterns. We can also observe that $$ \begin{align} P_n(1) &= \frac{(n-1)!\,n!}{2^{n-1}}\\ P_{2n}(-1) &= \frac{(2n-1)!\,n!}{3^{n-1}}\\ P_{2n-1}(-1) &= \frac{(2n-1)!!\,(2n-2)!}{6^{n-1}}, \end{align}\tag8 $$ where $n!!$ denotes the double factorial . I am trying to find a more direct formula for the polynomials $P_n(q)$ or their coefficients $c_{n,k}$ (possibly, containing finite products and sums, but not requiring to solve equations).",,"['real-analysis', 'sequences-and-series', 'combinatorics', 'power-series', 'functional-equations']"
94,Comparing Two Statements of the Rank Theorem,Comparing Two Statements of the Rank Theorem,,"I don't think this a duplicate, even though a similar question appears here . Let $m\geq n$ and let $F:\mathbb R^n\to \mathbb R^m$ be a $\mathcal C'$ mapping s.t. rank $F'(x)=r\leq n$ for all $x\in E\subseteq \mathbb R^n.\ $Fix $a\in E$ and set $A=F'(a)$ and let $P$ be a projection in $\mathbb R^m$ onto the $Y_1=$range of $A$. Set $Y_2=kerP.$ Then the claim is that there are open sets $U\subseteq E$ and $V$ in $\mathbb R^n$ s.t $a\in U$; and there is a  bijective $\mathcal C'$ map $H:V\to U$ s.t $\tag1 F(Hx)=Ax+\varphi (Ax)$ where $\varphi$ is a $\mathcal C'$ function mapping $A(V)$ into $U$. This is,of course, a somewhat abbreviated version of the statement that appears in blue Rudin. The proof is not hard, but it seems rather abstruse and uniformative, compared to the version I first learned, which follows my comments, and question here. $H$ seems to be simply a change of coordinates (diffeomorphism),but I can't find an easy geometric interpretation of $\varphi$. It is easy to show that $P$ restricted to $F(U)$ is a bijection onto $A(V)$ but this is immediate from the other version below. In fact, it seems a lot easier to understand the idea in the following, different (?) version of the Rank Theorem: Suppose $U, V$ are open in $\mathbb R^n,\mathbb R^m$,resp. and let $F:U\to V$ be a $\mathcal C'$ map s.t. $F'(x)$ has rank $r$ for all $x\in U$. Then, there exist open sets $U_1,V_1\in \mathbb R^n,\mathbb R^m$, resp. and  diffeomorphsims $\varphi:U_1\to U_0$ and $\psi:V_1\to V_0$ s.t. for fixed $a\in U$, $\varphi (a)=0;\ \psi (f(p))=0$ and $\tag2 \psi\circ f\circ \varphi ^{-1} (x_1,x_2,\cdots ,x_r,x_{r+1},\cdots x_n)=(x_1,\cdots ,x_r,0,0,\cdots ,0)$. This formulation is simpler and more intuitive. Are the two versions equivalent? If not, how is Rudin's ""better"" than the other?","I don't think this a duplicate, even though a similar question appears here . Let $m\geq n$ and let $F:\mathbb R^n\to \mathbb R^m$ be a $\mathcal C'$ mapping s.t. rank $F'(x)=r\leq n$ for all $x\in E\subseteq \mathbb R^n.\ $Fix $a\in E$ and set $A=F'(a)$ and let $P$ be a projection in $\mathbb R^m$ onto the $Y_1=$range of $A$. Set $Y_2=kerP.$ Then the claim is that there are open sets $U\subseteq E$ and $V$ in $\mathbb R^n$ s.t $a\in U$; and there is a  bijective $\mathcal C'$ map $H:V\to U$ s.t $\tag1 F(Hx)=Ax+\varphi (Ax)$ where $\varphi$ is a $\mathcal C'$ function mapping $A(V)$ into $U$. This is,of course, a somewhat abbreviated version of the statement that appears in blue Rudin. The proof is not hard, but it seems rather abstruse and uniformative, compared to the version I first learned, which follows my comments, and question here. $H$ seems to be simply a change of coordinates (diffeomorphism),but I can't find an easy geometric interpretation of $\varphi$. It is easy to show that $P$ restricted to $F(U)$ is a bijection onto $A(V)$ but this is immediate from the other version below. In fact, it seems a lot easier to understand the idea in the following, different (?) version of the Rank Theorem: Suppose $U, V$ are open in $\mathbb R^n,\mathbb R^m$,resp. and let $F:U\to V$ be a $\mathcal C'$ map s.t. $F'(x)$ has rank $r$ for all $x\in U$. Then, there exist open sets $U_1,V_1\in \mathbb R^n,\mathbb R^m$, resp. and  diffeomorphsims $\varphi:U_1\to U_0$ and $\psi:V_1\to V_0$ s.t. for fixed $a\in U$, $\varphi (a)=0;\ \psi (f(p))=0$ and $\tag2 \psi\circ f\circ \varphi ^{-1} (x_1,x_2,\cdots ,x_r,x_{r+1},\cdots x_n)=(x_1,\cdots ,x_r,0,0,\cdots ,0)$. This formulation is simpler and more intuitive. Are the two versions equivalent? If not, how is Rudin's ""better"" than the other?",,"['real-analysis', 'general-topology', 'differential-geometry']"
95,Must atoms of a Borel measure space be singletons?,Must atoms of a Borel measure space be singletons?,,"It's been a while since I've done any real analysis, so I'd appreciate some guidance. Suppose we're working on the real line, with some Borel measure induced by a non-decreasing, right-continuous function $F$. Clearly all the points of discontinuity of $F$ are atoms (of which there may only be countably many). So if we had a non-singleton atom $A$, it would have to be uncountable. I wanted to conclude the argument by considering the set $A \setminus \{x\}$ for any $x \in  A$, but since the Borel $\sigma$-algebra isn't complete, there's no reason why I should expect that to be a measurable set. Is there a better way to see why this result might be true, or is it false? Edit: I think I figured it out. Suppose $A$ is an atomic with positive measure $\epsilon$. Then if we partition the real line into half-open intervals of measure less than $\epsilon$, then the intersection of $A$ with one of these intervals should be a proper subset of $A$, with positive measure. Edit: I think that might not work in general? Can we even partition the real line into countably many intervals of measure $< \epsilon$ for any $\epsilon > 0$? I suppose it must work for finite measure spaces?","It's been a while since I've done any real analysis, so I'd appreciate some guidance. Suppose we're working on the real line, with some Borel measure induced by a non-decreasing, right-continuous function $F$. Clearly all the points of discontinuity of $F$ are atoms (of which there may only be countably many). So if we had a non-singleton atom $A$, it would have to be uncountable. I wanted to conclude the argument by considering the set $A \setminus \{x\}$ for any $x \in  A$, but since the Borel $\sigma$-algebra isn't complete, there's no reason why I should expect that to be a measurable set. Is there a better way to see why this result might be true, or is it false? Edit: I think I figured it out. Suppose $A$ is an atomic with positive measure $\epsilon$. Then if we partition the real line into half-open intervals of measure less than $\epsilon$, then the intersection of $A$ with one of these intervals should be a proper subset of $A$, with positive measure. Edit: I think that might not work in general? Can we even partition the real line into countably many intervals of measure $< \epsilon$ for any $\epsilon > 0$? I suppose it must work for finite measure spaces?",,"['real-analysis', 'measure-theory']"
96,What's happening at $a=-1$ in $\int x^a dx$? [duplicate],What's happening at  in ? [duplicate],a=-1 \int x^a dx,"This question already has answers here : What is so special about $\alpha=-1$ in the integral of $x^\alpha$? (7 answers) Closed 8 years ago . If we take the right limit $$\lim_{a\to-1}\int x^a dx=\lim_{a\to-1}\frac{x^{a+1}}{a+1}=+\infty$$ but on the other hand $$\int\lim_{a\to-1} x^a dx=\ln x$$ I'm aware you can't just commute the limit and the integral, but I'd still like an explanation here. To me this is analogous to someone saying ""The right limit of $1/x$ is $+\infty$ and the left one is $-\infty$ but $1/0$ is $7$ (or something)"" Is there an intuitive explanation to this break in continuity?","This question already has answers here : What is so special about $\alpha=-1$ in the integral of $x^\alpha$? (7 answers) Closed 8 years ago . If we take the right limit $$\lim_{a\to-1}\int x^a dx=\lim_{a\to-1}\frac{x^{a+1}}{a+1}=+\infty$$ but on the other hand $$\int\lim_{a\to-1} x^a dx=\ln x$$ I'm aware you can't just commute the limit and the integral, but I'd still like an explanation here. To me this is analogous to someone saying ""The right limit of $1/x$ is $+\infty$ and the left one is $-\infty$ but $1/0$ is $7$ (or something)"" Is there an intuitive explanation to this break in continuity?",,"['real-analysis', 'integration']"
97,Is $\sum_{n\ge 1} \sin(n^2)/n$ convergent?,Is  convergent?,\sum_{n\ge 1} \sin(n^2)/n,"Is the series $$\sum_{n\ge 1} \frac{\sin(n^2)}{n}$$ convergent? My thoughts so far: 1) This is an alternating series so the integration test does not work here. 2) The Weyl inequality roughly says $$\sum_{n\le N} \sin(n^2)$$ is $O(N^{1/2+\epsilon})$ , so the Dirichlet test does not work directly, but one can take $$a_n=n^{-1},b_n=\sum_{k\le n} \sin(k^2)$$ and follow the idea of Dirichlet test. The problem now is that the Weyl bound does not hold for all $N$ .","Is the series convergent? My thoughts so far: 1) This is an alternating series so the integration test does not work here. 2) The Weyl inequality roughly says is , so the Dirichlet test does not work directly, but one can take and follow the idea of Dirichlet test. The problem now is that the Weyl bound does not hold for all .","\sum_{n\ge 1} \frac{\sin(n^2)}{n} \sum_{n\le N} \sin(n^2) O(N^{1/2+\epsilon}) a_n=n^{-1},b_n=\sum_{k\le n} \sin(k^2) N","['real-analysis', 'sequences-and-series', 'convergence-divergence']"
98,"Find $E\subseteq\mathbb{R}$ such that $\liminf_{\delta\to 0}\frac{m(E\cap(-\delta,\delta))}{2\delta}=\alpha$",Find  such that,"E\subseteq\mathbb{R} \liminf_{\delta\to 0}\frac{m(E\cap(-\delta,\delta))}{2\delta}=\alpha","Problem: Let $\alpha$ and $\beta$ be such that $0\leq\alpha\leq\beta\leq 1$. Find a measurable set $E\subseteq\mathbb{R}$ such that   $$\liminf_{\delta\to 0}\frac{m(E\cap(-\delta,\delta))}{2\delta}=\alpha\quad\text{and}\quad\limsup_{\delta\to 0}\frac{m(E\cap(-\delta,\delta))}{2\delta}=\beta,$$   where $m$ is the Lebesgue measure. (Taken from Rudin's Real and Complex Analysis , Chapter 7, Exercise 2.) I tried many things, but every attempt fails. All I can get is a set $E$ corresponding to $\alpha=0$ and $\beta=1$. But when $0<\alpha\leq\beta<1$, I don't know how to get $E$.","Problem: Let $\alpha$ and $\beta$ be such that $0\leq\alpha\leq\beta\leq 1$. Find a measurable set $E\subseteq\mathbb{R}$ such that   $$\liminf_{\delta\to 0}\frac{m(E\cap(-\delta,\delta))}{2\delta}=\alpha\quad\text{and}\quad\limsup_{\delta\to 0}\frac{m(E\cap(-\delta,\delta))}{2\delta}=\beta,$$   where $m$ is the Lebesgue measure. (Taken from Rudin's Real and Complex Analysis , Chapter 7, Exercise 2.) I tried many things, but every attempt fails. All I can get is a set $E$ corresponding to $\alpha=0$ and $\beta=1$. But when $0<\alpha\leq\beta<1$, I don't know how to get $E$.",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
99,If a derivative $f^\prime$ is of bounded variation then prove that $f^\prime$ is continuous.,If a derivative  is of bounded variation then prove that  is continuous.,f^\prime f^\prime,"I am having trouble with the following problem: Let $f:[a,b]\to \mathbb R$ be  differentiable on $[a,b]$ and $f^\prime$ is of bounded variation on $[a,b]$. Prove that $f^\prime$ is continuous on $[a,b]$. My guess is affirmative. I know that $f^\prime$ can be expressed as difference of two monotonic functions $f^\prime=g-h$ on $[a,b]$. Since $f^\prime$  satisfies intermediate value property (i.e. for any $\lambda$ such that $f^\prime(x)<\lambda<f^\prime(y)$ then there exists a $t$ in $[a,b]$ between $x$ and $y$ such that $f^\prime(t)=\lambda$). From this I am attempting to show that since $f^\prime=g-h$ and both of $g$ and $h$ are monotone, then $f^\prime$ will be continuous. But I do not make it. I have seen a nearly similar type of questions in the MSE but I find hard to address it. Let me know whether my guess is right and if so how to proceed further. Thanks for your attention.","I am having trouble with the following problem: Let $f:[a,b]\to \mathbb R$ be  differentiable on $[a,b]$ and $f^\prime$ is of bounded variation on $[a,b]$. Prove that $f^\prime$ is continuous on $[a,b]$. My guess is affirmative. I know that $f^\prime$ can be expressed as difference of two monotonic functions $f^\prime=g-h$ on $[a,b]$. Since $f^\prime$  satisfies intermediate value property (i.e. for any $\lambda$ such that $f^\prime(x)<\lambda<f^\prime(y)$ then there exists a $t$ in $[a,b]$ between $x$ and $y$ such that $f^\prime(t)=\lambda$). From this I am attempting to show that since $f^\prime=g-h$ and both of $g$ and $h$ are monotone, then $f^\prime$ will be continuous. But I do not make it. I have seen a nearly similar type of questions in the MSE but I find hard to address it. Let me know whether my guess is right and if so how to proceed further. Thanks for your attention.",,"['real-analysis', 'bounded-variation']"
