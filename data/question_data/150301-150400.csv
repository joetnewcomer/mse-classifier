,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Proving an integral using a series,Proving an integral using a series,,"If $f:(0,1]\rightarrow \textbf {R}$ is defined by $f(x)=2nx$ for $\frac{1}{n+1}\leq x \leq \frac 1n$ and $n$ is a natural number, assuming that $\sum_{k=1}^{k=\infty}1/k^2=\pi^2/6$,  show that $\int_0^1 f(x)dx=\pi^2/6$. So I wrote the summation of the integral from $x=\frac{1}{n+1}$ to $x=\frac{1}{n}$ of $f(x)dx$. After I integrate I'm left with the infinite series of $\frac{1}{k}-\frac{k}{(k+1)^2}$ from $k=1$ to $k=\infty$? Not sure where to go from here.","If $f:(0,1]\rightarrow \textbf {R}$ is defined by $f(x)=2nx$ for $\frac{1}{n+1}\leq x \leq \frac 1n$ and $n$ is a natural number, assuming that $\sum_{k=1}^{k=\infty}1/k^2=\pi^2/6$,  show that $\int_0^1 f(x)dx=\pi^2/6$. So I wrote the summation of the integral from $x=\frac{1}{n+1}$ to $x=\frac{1}{n}$ of $f(x)dx$. After I integrate I'm left with the infinite series of $\frac{1}{k}-\frac{k}{(k+1)^2}$ from $k=1$ to $k=\infty$? Not sure where to go from here.",,"['real-analysis', 'integration', 'sequences-and-series', 'analysis']"
1,Shift Operator not continuous,Shift Operator not continuous,,"Let $X=L^1(\mathbb{R}^n)$ and $T:\mathbb{R} \rightarrow L(X)$, such that $(T(\tau)f)(t):=f(t+\tau)$. The question is: Is $T$ continuous? Well, my idea was the following: $|\tau_1-\tau_2| \le \delta \rightarrow ||T(\tau_1)-T(\tau_2)|| = sup_{f,||f||=1} \int |f(t+\tau_1) - f(t+\tau_2)|$ But the problem is: We could take any function with compact support(for example $[0,1]$), such that on intervals of length $|I|=\frac{\delta}{2}$, $f$ would be +1 on $[0,\frac{\delta}{2}]$ and negative one on $[\frac{\delta}{2},\delta]$ and so on. So the integral could become arbitrarily large. So I guess the shift-operator $T$ is not continuous, is this right?","Let $X=L^1(\mathbb{R}^n)$ and $T:\mathbb{R} \rightarrow L(X)$, such that $(T(\tau)f)(t):=f(t+\tau)$. The question is: Is $T$ continuous? Well, my idea was the following: $|\tau_1-\tau_2| \le \delta \rightarrow ||T(\tau_1)-T(\tau_2)|| = sup_{f,||f||=1} \int |f(t+\tau_1) - f(t+\tau_2)|$ But the problem is: We could take any function with compact support(for example $[0,1]$), such that on intervals of length $|I|=\frac{\delta}{2}$, $f$ would be +1 on $[0,\frac{\delta}{2}]$ and negative one on $[\frac{\delta}{2},\delta]$ and so on. So the integral could become arbitrarily large. So I guess the shift-operator $T$ is not continuous, is this right?",,"['calculus', 'real-analysis']"
2,"Example where $\|\,f_n\|_\infty\to \infty$, but $\|\,f_n\|_1\to 0$","Example where , but","\|\,f_n\|_\infty\to \infty \|\,f_n\|_1\to 0","Please help me to solve the following problem that is in the Lebesgue integral discussion Give an example of a sequence $\,\,f_n : [0, 1] \to \Bbb R$ of continuous functions such that $\,\,\|f_n\|_\infty \to \infty$ but $\int_0^1\lvert\, f_n\rvert\,d\lambda \to 0$ as $n\to\infty$. Original screenshot","Please help me to solve the following problem that is in the Lebesgue integral discussion Give an example of a sequence $\,\,f_n : [0, 1] \to \Bbb R$ of continuous functions such that $\,\,\|f_n\|_\infty \to \infty$ but $\int_0^1\lvert\, f_n\rvert\,d\lambda \to 0$ as $n\to\infty$. Original screenshot",,"['real-analysis', 'integration', 'analysis', 'lebesgue-integral', 'lebesgue-measure']"
3,"Does the union of all these neighborhood cover $[0,1]$",Does the union of all these neighborhood cover,"[0,1]","Consider the rationals in $[0,1]$. Around each I take a neighborhood (possibly of different radii). Is the union of all these neighborhood sure to cover $[0,1]$? What if I had used irrationals instead of rationals?","Consider the rationals in $[0,1]$. Around each I take a neighborhood (possibly of different radii). Is the union of all these neighborhood sure to cover $[0,1]$? What if I had used irrationals instead of rationals?",,"['general-topology', 'analysis', 'compactness']"
4,"Let $f,g$ be differentiable with $f(0)=g(0)$ and $f'(x)<g'(x)$. Prove that $f(x)<g(x)$.",Let  be differentiable with  and . Prove that .,"f,g f(0)=g(0) f'(x)<g'(x) f(x)<g(x)","Let $f,g:\mathbb{R} \rightarrow \mathbb{R}$ be differentiable with $f(0)=g(0)$ and $f'(x) < g'(x)$ for all $x$ belonging to the set of real numbers. Prove that $f(x)<g(x)$ for all $x>0$. Any help? Im so confused :P","Let $f,g:\mathbb{R} \rightarrow \mathbb{R}$ be differentiable with $f(0)=g(0)$ and $f'(x) < g'(x)$ for all $x$ belonging to the set of real numbers. Prove that $f(x)<g(x)$ for all $x>0$. Any help? Im so confused :P",,"['real-analysis', 'analysis', 'inequality', 'derivatives']"
5,Integral in hyperbolic coordinates,Integral in hyperbolic coordinates,,"all. My homework problem is the following: Define $D=\{(x,y)\mid x,y>0, 1\leq x^2-y^2\leq 9, 2\leq xy\leq4 \}$.. For a continuous function $f:D\rightarrow\mathbb{R}$, use the hyperbolic coordinates from Exercise 7 to show that $$ \int_D[x^2+y^2]dxdy=8 $$ In exercise 7, the function we defined was, for $x,y>0$, $$ \Phi(x,y)=(x^2-y^2,xy) $$ And we proved that this is a smooth change of variables with $$ \det{D\Phi}=\begin{vmatrix}2x&y\\-2y&x\end{vmatrix}=2x^2+2y^2 $$ So what I've tried is, by the change of variables theorem, $$ \int_D[x^2+y^2]dxdy=\int_1^9\int_2^4\left(\left((x^2-y^2)^2+(xy)^2\right)(2x^2+2y^2) \right)dxdy=\frac{19391968}{7}\neq 8 $$ I know why this is wrong, but I'm not sure how to do it. I should have some $u=x^2-y^2$, $v=xy$ and integrate from $u=1$ to $u=9$ and $v=2$ to $v=4$, but I'm not quite sure how to fill in all the details, including how to get my integral and $\det(D\Phi$ in terms of $u,v$. Thanks","all. My homework problem is the following: Define $D=\{(x,y)\mid x,y>0, 1\leq x^2-y^2\leq 9, 2\leq xy\leq4 \}$.. For a continuous function $f:D\rightarrow\mathbb{R}$, use the hyperbolic coordinates from Exercise 7 to show that $$ \int_D[x^2+y^2]dxdy=8 $$ In exercise 7, the function we defined was, for $x,y>0$, $$ \Phi(x,y)=(x^2-y^2,xy) $$ And we proved that this is a smooth change of variables with $$ \det{D\Phi}=\begin{vmatrix}2x&y\\-2y&x\end{vmatrix}=2x^2+2y^2 $$ So what I've tried is, by the change of variables theorem, $$ \int_D[x^2+y^2]dxdy=\int_1^9\int_2^4\left(\left((x^2-y^2)^2+(xy)^2\right)(2x^2+2y^2) \right)dxdy=\frac{19391968}{7}\neq 8 $$ I know why this is wrong, but I'm not sure how to do it. I should have some $u=x^2-y^2$, $v=xy$ and integrate from $u=1$ to $u=9$ and $v=2$ to $v=4$, but I'm not quite sure how to fill in all the details, including how to get my integral and $\det(D\Phi$ in terms of $u,v$. Thanks",,"['calculus', 'real-analysis', 'integration', 'analysis']"
6,Is indicator function integrable?,Is indicator function integrable?,,"f is discontinuous on [0,1] so it's not integrable? but I think the answer is yes, it is integrable. But i dont know how to prove that. any hint would be great. thanks","f is discontinuous on [0,1] so it's not integrable? but I think the answer is yes, it is integrable. But i dont know how to prove that. any hint would be great. thanks",,['analysis']
7,"How take example such this three conditions.is continuous at all irrationals, discontinuous at all rationals","How take example such this three conditions.is continuous at all irrationals, discontinuous at all rationals",,"Example: The function $f(x)$ such this follow three conditions: (1): $x\in [0,1]$ (2): such $f(x)$ is continuous at all irrationals, discontinuous at all rationals; (3):and $f$   have many  infinite number discontinuity point of the second kind in $[0,1]$ my try: if only such condition $(1)$ and $(2)$,I can take this example: Riemann function: $f(x)=\begin{cases} \dfrac{1}{p}, & x=\dfrac{q}{p}, ~p,q\in \mathbb{N}, ~(p,q)=1\\ 0, & x=0,~1~,~\text{or}~x\in \mathbb{R} \setminus \mathbb{Q} \end{cases}$ But we have must such condition $(3)$,so we easy see this Riemann function is not such it.so Now what's function such it? Thank you  very much!","Example: The function $f(x)$ such this follow three conditions: (1): $x\in [0,1]$ (2): such $f(x)$ is continuous at all irrationals, discontinuous at all rationals; (3):and $f$   have many  infinite number discontinuity point of the second kind in $[0,1]$ my try: if only such condition $(1)$ and $(2)$,I can take this example: Riemann function: $f(x)=\begin{cases} \dfrac{1}{p}, & x=\dfrac{q}{p}, ~p,q\in \mathbb{N}, ~(p,q)=1\\ 0, & x=0,~1~,~\text{or}~x\in \mathbb{R} \setminus \mathbb{Q} \end{cases}$ But we have must such condition $(3)$,so we easy see this Riemann function is not such it.so Now what's function such it? Thank you  very much!",,"['real-analysis', 'analysis']"
8,Integral from zero to infinity of $\int_0^{\infty}\frac{(1-e^{-\lambda z})}{\lambda^{a+1}} d \lambda$,Integral from zero to infinity of,\int_0^{\infty}\frac{(1-e^{-\lambda z})}{\lambda^{a+1}} d \lambda,"I know that the value of the integral is as follows $$\int_0^{\infty}\frac{(1-e^{-\lambda z})}{\lambda^{a+1}} d \lambda =z^a \frac{\Gamma(1-a)}{a}$$ However, how exactly the integral is calculated? How one proves that the integral even exists? Why does the integral only exist for $0<a<1$?","I know that the value of the integral is as follows $$\int_0^{\infty}\frac{(1-e^{-\lambda z})}{\lambda^{a+1}} d \lambda =z^a \frac{\Gamma(1-a)}{a}$$ However, how exactly the integral is calculated? How one proves that the integral even exists? Why does the integral only exist for $0<a<1$?",,"['analysis', 'improper-integrals']"
9,"Accumulation points of the set $S=\{(\frac {1} {n}, \frac {1} {m}) \space m, n \in \mathbb N\}$",Accumulation points of the set,"S=\{(\frac {1} {n}, \frac {1} {m}) \space m, n \in \mathbb N\}","The exercise is to find the accumulation points of the set  $S=\{(\frac {1} {n}, \frac {1} {m}) \space m, n \in \mathbb N\}$ I'm trying to prove that if $A$={accumulation points of the set $S$}, then $A=\{(0,0), (\frac {1} {n},0),(0,\frac {1} {m}) \space n,m \in \mathbb N\}$. I could prove that the set of the right side of the equality is included in $A$. I don't know how to prove the other inclusion, which means that if $x$ is an accumulation point of $S$, then $x$ has to be $(0,0)$, or of the form $(\frac {1} {n},0)$ or $(0,\frac {1} {m})$.","The exercise is to find the accumulation points of the set  $S=\{(\frac {1} {n}, \frac {1} {m}) \space m, n \in \mathbb N\}$ I'm trying to prove that if $A$={accumulation points of the set $S$}, then $A=\{(0,0), (\frac {1} {n},0),(0,\frac {1} {m}) \space n,m \in \mathbb N\}$. I could prove that the set of the right side of the equality is included in $A$. I don't know how to prove the other inclusion, which means that if $x$ is an accumulation point of $S$, then $x$ has to be $(0,0)$, or of the form $(\frac {1} {n},0)$ or $(0,\frac {1} {m})$.",,"['general-topology', 'analysis']"
10,When to rationalize numerator and/or denominator? [duplicate],When to rationalize numerator and/or denominator? [duplicate],,"This question already has answers here : Why rationalize the denominator? (15 answers) Closed 9 years ago . Sometimes, we have to rationalize either the numerator or the denominator, and sometimes we can still work the problem without rationalizing. So, in some cases, rationalizing can be done, although it is not necessary, but if it is done, it will be equivalent to the original function, correct? Will it be equivalent to the original function at all points? This is a very general question: when do we have to rationalize either the numerator or denominator, and when can we still work the problem without doing so? By ""general question"" , I'm interested in various fields of mathematics, not necessarily examples from calculus or algebra. Can you provide some examples/explanations? Thank you! EDIT : please don't close the question. I'm interested in various explanations to understand better the concept of rationalizing.","This question already has answers here : Why rationalize the denominator? (15 answers) Closed 9 years ago . Sometimes, we have to rationalize either the numerator or the denominator, and sometimes we can still work the problem without rationalizing. So, in some cases, rationalizing can be done, although it is not necessary, but if it is done, it will be equivalent to the original function, correct? Will it be equivalent to the original function at all points? This is a very general question: when do we have to rationalize either the numerator or denominator, and when can we still work the problem without doing so? By ""general question"" , I'm interested in various fields of mathematics, not necessarily examples from calculus or algebra. Can you provide some examples/explanations? Thank you! EDIT : please don't close the question. I'm interested in various explanations to understand better the concept of rationalizing.",,"['calculus', 'abstract-algebra', 'analysis', 'functions', 'irrational-numbers']"
11,Generalization of absolute continuity with $f(x) = x^a \sin(1/x^b)$,Generalization of absolute continuity with,f(x) = x^a \sin(1/x^b),"As a generalization of Prove that $x^\alpha \cdot\sin(1/x)$ is absolutely continuous on $(0,1)$ : Let $f : (0, 1] \to \mathbb{R}$ be the function denoted by $f(x) = x^a \sin(1/x^b)$. Determine for which $a,b$ the function $f$ is absolutely continuous. So at least we know what happens when $b=1$ according to the link. In addition, for what $a,b$ is $f$ uniformly continuous but not absolutely continuous? (cf Showing that $f(x)=x\sin (1/x)$ is not absolutely continuous on $[0,1]$ )","As a generalization of Prove that $x^\alpha \cdot\sin(1/x)$ is absolutely continuous on $(0,1)$ : Let $f : (0, 1] \to \mathbb{R}$ be the function denoted by $f(x) = x^a \sin(1/x^b)$. Determine for which $a,b$ the function $f$ is absolutely continuous. So at least we know what happens when $b=1$ according to the link. In addition, for what $a,b$ is $f$ uniformly continuous but not absolutely continuous? (cf Showing that $f(x)=x\sin (1/x)$ is not absolutely continuous on $[0,1]$ )",,"['real-analysis', 'analysis', 'measure-theory', 'continuity', 'lebesgue-integral']"
12,"If $D$ is dense in $X$, and $Y\subset X$, what conditions on $Y$ ensure that $D\cap Y$ is dense in $Y$?","If  is dense in , and , what conditions on  ensure that  is dense in ?",D X Y\subset X Y D\cap Y Y,"Suppose that $(X,\tau)$ is a topological space, $D\subset X$ is dense in $X$ and $Y\subset X$. It can be shown that if $Y$ is open, then $D\cap Y$ is dense in $Y$ (using the subset topology). However, $Y$ being open is not necessary, since, for example, the set of non-negative rationals is dense in the set of non-negative reals. My question is: Are there necessary and sufficient conditions on $Y$ so that $D\cap Y$ is dense in $Y$? If not, what other sufficient conditions are there? If it allows us to make stronger statements, please substitute $(X,\tau)$ with some normed space.","Suppose that $(X,\tau)$ is a topological space, $D\subset X$ is dense in $X$ and $Y\subset X$. It can be shown that if $Y$ is open, then $D\cap Y$ is dense in $Y$ (using the subset topology). However, $Y$ being open is not necessary, since, for example, the set of non-negative rationals is dense in the set of non-negative reals. My question is: Are there necessary and sufficient conditions on $Y$ so that $D\cap Y$ is dense in $Y$? If not, what other sufficient conditions are there? If it allows us to make stronger statements, please substitute $(X,\tau)$ with some normed space.",,"['real-analysis', 'general-topology', 'analysis']"
13,Interspersing of integers by rationals,Interspersing of integers by rationals,,"I'm wondering if the next argument is sound or maybe need some adjustments; Proposition (Interspersing of integers by rationals): Let $x\in \mathbb{Q}$. Then there exists an integer such that $n\le x< n+1$. In fact this integer is unique. In particular, there exists a natural number $N$ such that $N>x$ (i.e. there is no such thing as a rational number which is larger than all the natural numbers). Proof: By the trichotomy property of the rational numbers, either $x$ is positive, negative or zero. If $x=0$, it's trivial since $\,0 \le x<1$. Now if $x$ is positive number, by definition $x= \frac{p}{q}$ where $p,q\in \mathbb{N}- \left\{0 \right\}$. Then using the Euclidean algorithm we have that: $$p = mq+r, \text{  where   } \:\: 0 \le r < q $$ We shall show that the natural number $m$ has the desired property $\,m \le x < m+1$. It is sufficient to show that $\,\,0 \le x-m<1$. $x -m= \frac{p}{q}-m =\frac{mq+r}{q}-{m} = \frac{r}{q}$. Since by the Euclidean algorithm we know that $\,0 \le r < q$  then $\,0 \le \frac{r}{q} <1$ as desired. Now suppose there is a $n \not= m $ such that $p = nq+r$. So, $\,mq+r = nq+r$ and we conclude that $m=n$ which show the uniqueness of $m$. If $x$ is a negative rational number, we have that $x = \frac{-a\,}{\,\,\,b\,}$ where $a,b\in \mathbb{N}- \left\{0\right\}$, so $-x = \frac{a}{b}$ and we may  use the same argument that in the positive case. To conclude we need to show that the integer is unique and is follows of the above argument. I feel that the argument is a bit flawed, I would appreciate any suggestion. Thanks in advance","I'm wondering if the next argument is sound or maybe need some adjustments; Proposition (Interspersing of integers by rationals): Let $x\in \mathbb{Q}$. Then there exists an integer such that $n\le x< n+1$. In fact this integer is unique. In particular, there exists a natural number $N$ such that $N>x$ (i.e. there is no such thing as a rational number which is larger than all the natural numbers). Proof: By the trichotomy property of the rational numbers, either $x$ is positive, negative or zero. If $x=0$, it's trivial since $\,0 \le x<1$. Now if $x$ is positive number, by definition $x= \frac{p}{q}$ where $p,q\in \mathbb{N}- \left\{0 \right\}$. Then using the Euclidean algorithm we have that: $$p = mq+r, \text{  where   } \:\: 0 \le r < q $$ We shall show that the natural number $m$ has the desired property $\,m \le x < m+1$. It is sufficient to show that $\,\,0 \le x-m<1$. $x -m= \frac{p}{q}-m =\frac{mq+r}{q}-{m} = \frac{r}{q}$. Since by the Euclidean algorithm we know that $\,0 \le r < q$  then $\,0 \le \frac{r}{q} <1$ as desired. Now suppose there is a $n \not= m $ such that $p = nq+r$. So, $\,mq+r = nq+r$ and we conclude that $m=n$ which show the uniqueness of $m$. If $x$ is a negative rational number, we have that $x = \frac{-a\,}{\,\,\,b\,}$ where $a,b\in \mathbb{N}- \left\{0\right\}$, so $-x = \frac{a}{b}$ and we may  use the same argument that in the positive case. To conclude we need to show that the integer is unique and is follows of the above argument. I feel that the argument is a bit flawed, I would appreciate any suggestion. Thanks in advance",,"['analysis', 'self-learning']"
14,Help with a simple problem involving a functional inequality (trying to prove Gronwall's inequality),Help with a simple problem involving a functional inequality (trying to prove Gronwall's inequality),,"So while trying to prove Grownwall's inequality, my proof led me to the following statement: $h'(x) \le h(x)g(x)$. Now when $h'(x)=h(x)g(x)$ the following holds: $h(x)=k \exp G(x)$, where $k$ is a constant number. Can I conclude from this that $h(x) \le k \exp G(x)$ (since $h'(x)<h(x)g(x)$)?","So while trying to prove Grownwall's inequality, my proof led me to the following statement: $h'(x) \le h(x)g(x)$. Now when $h'(x)=h(x)g(x)$ the following holds: $h(x)=k \exp G(x)$, where $k$ is a constant number. Can I conclude from this that $h(x) \le k \exp G(x)$ (since $h'(x)<h(x)g(x)$)?",,"['calculus', 'real-analysis', 'analysis', 'integration', 'inequality']"
15,Proving that the nested intervals theorem implies that every upper bounded non-empty set has a supremum,Proving that the nested intervals theorem implies that every upper bounded non-empty set has a supremum,,"I've been trying to solve a kinda long problem for a few days and can't quite finish it. Prove the equivalence between (1) Every bounded sequence has a convergent subsequence. (2) All cauchy sequences are convergent. (3) If $ (I_n)_{n\geq 1} $ is a family of closed nested intervals so that $|I_n| \rightarrow 0 $ as $n \rightarrow \infty$ there exist a unique $x$ so that $x \in  \bigcap_{n=1}^\infty I_n$. (4) Every upper bounded non-empty set has a supremum. (5) Every monotone, upper bounded sequence has a supremum. I managed to get $(1)\rightarrow(2)\rightarrow(3)$ and $(4)\rightarrow(5)\rightarrow(1)$ so I obviously would love to prove $(3)\rightarrow(4)$. I manage to get a decreasing sequence since if there's no supremum then for every upper bound $s$ there exist at least one $\varepsilon > 0$ so that $s - \varepsilon$ is also an upper bound. So I can choose one upper bound $s$ and consider the sequence $(s - \varepsilon_n)_{n \in \mathbb{N}}$ that gets closer to the set as $n$ grows, where $s - \varepsilon_n$ is an upper bound for all $n$. Now I'd like to construct a monotonically increasing sequence of elements of the set so that $|A_n - (s - \varepsilon_n)| \rightarrow 0$ but I'm not sure how to do this, or if my decreasing sequence is convenient for this, it does seem quite ugly to work with, but couldn't really come up with an improvement. I've consider taking another direction and proving that (3) implies something else and that in turn that implies (4), but I couldn't find a good way of doing it. Maybe I missed something? Any help would be greatly appreciated.","I've been trying to solve a kinda long problem for a few days and can't quite finish it. Prove the equivalence between (1) Every bounded sequence has a convergent subsequence. (2) All cauchy sequences are convergent. (3) If $ (I_n)_{n\geq 1} $ is a family of closed nested intervals so that $|I_n| \rightarrow 0 $ as $n \rightarrow \infty$ there exist a unique $x$ so that $x \in  \bigcap_{n=1}^\infty I_n$. (4) Every upper bounded non-empty set has a supremum. (5) Every monotone, upper bounded sequence has a supremum. I managed to get $(1)\rightarrow(2)\rightarrow(3)$ and $(4)\rightarrow(5)\rightarrow(1)$ so I obviously would love to prove $(3)\rightarrow(4)$. I manage to get a decreasing sequence since if there's no supremum then for every upper bound $s$ there exist at least one $\varepsilon > 0$ so that $s - \varepsilon$ is also an upper bound. So I can choose one upper bound $s$ and consider the sequence $(s - \varepsilon_n)_{n \in \mathbb{N}}$ that gets closer to the set as $n$ grows, where $s - \varepsilon_n$ is an upper bound for all $n$. Now I'd like to construct a monotonically increasing sequence of elements of the set so that $|A_n - (s - \varepsilon_n)| \rightarrow 0$ but I'm not sure how to do this, or if my decreasing sequence is convenient for this, it does seem quite ugly to work with, but couldn't really come up with an improvement. I've consider taking another direction and proving that (3) implies something else and that in turn that implies (4), but I couldn't find a good way of doing it. Maybe I missed something? Any help would be greatly appreciated.",,"['real-analysis', 'sequences-and-series', 'analysis']"
16,"The set $A=\{(x, y)\in \mathbb{R}^2:|x|=|y|\}$ is connected",The set  is connected,"A=\{(x, y)\in \mathbb{R}^2:|x|=|y|\}","Two disjoint sets $A$ and $B$, neither empty, are said to be mutually separated if neither contains a boundary point of the other. A set is disconnected if it is the union of separated subsets, and is called connected if it is not disconnected. With the above definition of connected set, how to prove that the set $A=\{(x, y)\in \mathbb{R}^2:|x|=|y|\}$ is connected. Exercise should try using the definition of previous related and for this I guess I should do the proof by contradiction but do not know how to proceed.","Two disjoint sets $A$ and $B$, neither empty, are said to be mutually separated if neither contains a boundary point of the other. A set is disconnected if it is the union of separated subsets, and is called connected if it is not disconnected. With the above definition of connected set, how to prove that the set $A=\{(x, y)\in \mathbb{R}^2:|x|=|y|\}$ is connected. Exercise should try using the definition of previous related and for this I guess I should do the proof by contradiction but do not know how to proceed.",,['analysis']
17,Clarifying a step in proving uniqueness of Jordan Decomp of signed measures,Clarifying a step in proving uniqueness of Jordan Decomp of signed measures,,"This should be real simple but I have been struggling to see this in an easily intuitive manner. Basically, my confusion comes down to showing that if $A,B$ and $A',B'$ are two arbitrary Hahn decompositions and that if $v = v^+ - v^-$ is a signed measure that: $v^+(E\cap A) = v^+(E\cap A')$ where E is an arbitrary set in the $\sigma$-algebra. I keep only being able to think about it by saying $E \cap A = (E \cap A')\cup N$ where $N$ is some null set since Hahn-Decompositions are unique except for null sets. Then I want to say that $v^+(E\cap A) = v^+(E\cap A') + v^+(N)$. But it isn't clear to me why $v^+(N) = 0$ for any null set $N$. ie $N$ would satisfy the criteria of a null set if $v^+(n) = v^-(n) \forall $ measurable $n \subset N$. What am I missing? What is the simplest way to show $v^+(E\cap A) = v^+(E\cap A')$?","This should be real simple but I have been struggling to see this in an easily intuitive manner. Basically, my confusion comes down to showing that if $A,B$ and $A',B'$ are two arbitrary Hahn decompositions and that if $v = v^+ - v^-$ is a signed measure that: $v^+(E\cap A) = v^+(E\cap A')$ where E is an arbitrary set in the $\sigma$-algebra. I keep only being able to think about it by saying $E \cap A = (E \cap A')\cup N$ where $N$ is some null set since Hahn-Decompositions are unique except for null sets. Then I want to say that $v^+(E\cap A) = v^+(E\cap A') + v^+(N)$. But it isn't clear to me why $v^+(N) = 0$ for any null set $N$. ie $N$ would satisfy the criteria of a null set if $v^+(n) = v^-(n) \forall $ measurable $n \subset N$. What am I missing? What is the simplest way to show $v^+(E\cap A) = v^+(E\cap A')$?",,"['analysis', 'measure-theory']"
18,continuity of the derivative under certain conditions,continuity of the derivative under certain conditions,,"I am working on this exercise in a book which asks to prove that $f$ is differentiable if $f$ is continuous and that $\lim \limits_{x\rightarrow x_0} f'(x)$ exists. I know that this is easy to show using the Mean Value Theorem. Now the problem asks: what can be said about the continuity of $f'$? Well since $f$ is differentiable, I only have to show that $\lim \limits_{x\rightarrow x_0} f'(x)=f'(x_0)$. But ideas how to proceed elude me a bit. Any suggestions? EDIT: Oh shoot...didn't realize how easy this was. The previous problem in this book showed how to prove half of it and I didn't realize till I read the comments below. Thanks guys!","I am working on this exercise in a book which asks to prove that $f$ is differentiable if $f$ is continuous and that $\lim \limits_{x\rightarrow x_0} f'(x)$ exists. I know that this is easy to show using the Mean Value Theorem. Now the problem asks: what can be said about the continuity of $f'$? Well since $f$ is differentiable, I only have to show that $\lim \limits_{x\rightarrow x_0} f'(x)=f'(x_0)$. But ideas how to proceed elude me a bit. Any suggestions? EDIT: Oh shoot...didn't realize how easy this was. The previous problem in this book showed how to prove half of it and I didn't realize till I read the comments below. Thanks guys!",,"['real-analysis', 'analysis', 'derivatives', 'continuity']"
19,Definition of accumulation point,Definition of accumulation point,,"I have here a definition of accumulation point : A point $x$ in a metric space $M$ is called an accumulation point of $A \subset M$ if every neighbourhood of $x$ contains some point of $A$ distinct from $x$. The definition seems vague to me; how to translate the condition in it? I) For each neighbourhood $N$ of $x$, there exists some $y\in A$ such that $y\in N$ and $y\neq x.$ or II) There exists some $y\in A$ such that for each neighbourhood $N$ of $x$, $y\in N$ and $y\neq x.$","I have here a definition of accumulation point : A point $x$ in a metric space $M$ is called an accumulation point of $A \subset M$ if every neighbourhood of $x$ contains some point of $A$ distinct from $x$. The definition seems vague to me; how to translate the condition in it? I) For each neighbourhood $N$ of $x$, there exists some $y\in A$ such that $y\in N$ and $y\neq x.$ or II) There exists some $y\in A$ such that for each neighbourhood $N$ of $x$, $y\in N$ and $y\neq x.$",,"['general-topology', 'analysis', 'metric-spaces', 'terminology']"
20,Harmonic analysis in number theory,Harmonic analysis in number theory,,"When I was reading Folland's A course in abstract harmonic analysis , I was told these materials have wonderful applications to number theory. However, I do not see really a lot of examples there. Can someone give some references on the applications of harmonic analysis to number theory? I am asking this question because I am trying to figure out what to do in my coming graduate years. I have learnt a lot analysis but I do not really like doing hard analysis all day, so maybe some application of analysis to other parts of mathematics suits me better. Thanks so much!","When I was reading Folland's A course in abstract harmonic analysis , I was told these materials have wonderful applications to number theory. However, I do not see really a lot of examples there. Can someone give some references on the applications of harmonic analysis to number theory? I am asking this question because I am trying to figure out what to do in my coming graduate years. I have learnt a lot analysis but I do not really like doing hard analysis all day, so maybe some application of analysis to other parts of mathematics suits me better. Thanks so much!",,"['analysis', 'number-theory', 'harmonic-analysis', 'career-development']"
21,About Regulated and Bounded Variations in Banach Spaces,About Regulated and Bounded Variations in Banach Spaces,,"In the following definitions, we assumed that $(X,\left\|\cdot\right\|)$ is a Banach space. Definition 1. $f:[a,b]\to X$ is of bounded variation on $[a,b]$ if  $$\operatorname{Var}(f;[a,b])=\operatorname{sup}(D)\sum \left\|f(v)-f(u)\right\|<+\infty$$ where the supremum is taken over all divisions $D=\{[u,v]\}$ of $[a,b]$. Definition 2. $f:[a,b]\to X$ is regulated if it has one-sided limits at every point of $[a,b]$, i.e. for every $c\in [a,b)$ there is a value $f(c+)\in X$ such that $$\lim_{t\to c^+}\left\|f(t)-f(c+)\right\|=0$$ and if for every $c\in (a,b]$ there is a value $f(c-)\in X$ such that $$\lim_{t\to c^-}\left\|f(t)-f(c-)\right\|=0.$$ Question. Using the preceding definitions, how do we show that every function of bounded variation is regulated. I need some help on this and many thanks in advance...","In the following definitions, we assumed that $(X,\left\|\cdot\right\|)$ is a Banach space. Definition 1. $f:[a,b]\to X$ is of bounded variation on $[a,b]$ if  $$\operatorname{Var}(f;[a,b])=\operatorname{sup}(D)\sum \left\|f(v)-f(u)\right\|<+\infty$$ where the supremum is taken over all divisions $D=\{[u,v]\}$ of $[a,b]$. Definition 2. $f:[a,b]\to X$ is regulated if it has one-sided limits at every point of $[a,b]$, i.e. for every $c\in [a,b)$ there is a value $f(c+)\in X$ such that $$\lim_{t\to c^+}\left\|f(t)-f(c+)\right\|=0$$ and if for every $c\in (a,b]$ there is a value $f(c-)\in X$ such that $$\lim_{t\to c^-}\left\|f(t)-f(c-)\right\|=0.$$ Question. Using the preceding definitions, how do we show that every function of bounded variation is regulated. I need some help on this and many thanks in advance...",,"['real-analysis', 'analysis', 'bounded-variation']"
22,Fourier series for $[x]-x+\frac{1}{2}$,Fourier series for,[x]-x+\frac{1}{2},"$[x]-x+\frac{1}{2}$ has the Fourier series $$\sum_{n=1}^{\infty} \frac{\sin{2n\pi x}}{n\pi}.$$ By evaluating the series directly, which requires some work, it can be shown that the series is convergent to $[x]-x+\frac{1}{2}$ if $x$ is not an integer. My question is, do we have any criteria by which we can easily see that $$[x]-x+\frac{1}{2}=\sum_{n=1}^{\infty} \frac{\sin{2n\pi x}}{n\pi}$$ when $x$ is not an integer?","$[x]-x+\frac{1}{2}$ has the Fourier series $$\sum_{n=1}^{\infty} \frac{\sin{2n\pi x}}{n\pi}.$$ By evaluating the series directly, which requires some work, it can be shown that the series is convergent to $[x]-x+\frac{1}{2}$ if $x$ is not an integer. My question is, do we have any criteria by which we can easily see that $$[x]-x+\frac{1}{2}=\sum_{n=1}^{\infty} \frac{\sin{2n\pi x}}{n\pi}$$ when $x$ is not an integer?",,"['analysis', 'fourier-series']"
23,Set of all n-tuples is countable,Set of all n-tuples is countable,,"I'm having trouble understanding the last part of the proof of this theorem (2.13) in the Rudin (blue) book: Let $A$ be a countable set, and let $B_n$ be the set of all $n$-tuples $(a_1,\ldots, a_n)$, where $n\in \mathbb{N}$ and $a_k\in A$ ($k=1,\ldots, n$), and the elements $a_1,\ldots, a_n$ need not be distinct. Then $B_n$ is countable. I'm not understanding how $B_n$ is the union of a countable set of countable sets. If someone could clarify this or provide a more in depth proof, I would appreciate it! Thank you.","I'm having trouble understanding the last part of the proof of this theorem (2.13) in the Rudin (blue) book: Let $A$ be a countable set, and let $B_n$ be the set of all $n$-tuples $(a_1,\ldots, a_n)$, where $n\in \mathbb{N}$ and $a_k\in A$ ($k=1,\ldots, n$), and the elements $a_1,\ldots, a_n$ need not be distinct. Then $B_n$ is countable. I'm not understanding how $B_n$ is the union of a countable set of countable sets. If someone could clarify this or provide a more in depth proof, I would appreciate it! Thank you.",,"['real-analysis', 'analysis']"
24,Lebesgue Measure of a k-cell,Lebesgue Measure of a k-cell,,"Working through Rudin's RCA construction (Theorem 2.20, p. 53) of the Lebesgue measure using the Riesz Representation Theorem. Rudin constructs a linear functional $\Lambda$ on $\operatorname{C}_c(\mathbb{R}^k)$ such that $$\Lambda f := \lim\limits_{n \to \infty} 2^{-nk} \sum\limits_{x \in P_n} f(x)$$ where $P_n$ is the set of all vectors of the form $x = (a_1/2^n,...,a_k/2^n)$ for $a_1,...,a_k \in \mathbb{Z}$. Now let $W$ be an open $k$-cell. Rudin considers the set $S_r = \{ Q \in \Omega_r: \overline{Q} \subset W\}$, where $\Omega_r$ is the set of all boxes of the form $Q = \{ x: a_i \leq x_i < a_i + 2^{-r}, a_i \in P_r, 1 \leq i \leq k\}$. He then defines $$E_r = \bigcup\limits_{Q \in S_r} Q$$ and applies Urysohn's Lemma to obtain a function $0 \leq f_r \leq 1$ such that $f[\overline{E}_r] = 1$, $\operatorname{supp}(f)\subseteq W$, and $\overline{E}_r \subseteq \operatorname{supp}(f)$. Note that $\overline{E}_r $ is compact. He then asserts without proof that $$\operatorname{vol}(E_r) \leq \Lambda f_r \leq \Lambda g_r \leq \operatorname{vol}(W)$$ where $g_r := \max\{f_i: 1 \leq i \leq r\}$. How do we establish this inequality? It would seem a priori that it would arise out of demonstrating $\operatorname{vol}(E_r) \leq \Lambda_n f$ for all $n$, but I have so far been unsuccessful in this matter. Using Urysohn's Lemma we obtained that $\chi_{\overline{E}_r} \leq f \leq \chi_W$, which may yield another route. As a secondary question, are there any other good sources for a construction of the Lebesgue measure on $\mathbb{R}^k$ using the Riesz Representation Theorem? All of the other standard texts, e.g. Royden, on the subject construct an outer Lebesgue measure and extend using the results of Caratheodory.","Working through Rudin's RCA construction (Theorem 2.20, p. 53) of the Lebesgue measure using the Riesz Representation Theorem. Rudin constructs a linear functional $\Lambda$ on $\operatorname{C}_c(\mathbb{R}^k)$ such that $$\Lambda f := \lim\limits_{n \to \infty} 2^{-nk} \sum\limits_{x \in P_n} f(x)$$ where $P_n$ is the set of all vectors of the form $x = (a_1/2^n,...,a_k/2^n)$ for $a_1,...,a_k \in \mathbb{Z}$. Now let $W$ be an open $k$-cell. Rudin considers the set $S_r = \{ Q \in \Omega_r: \overline{Q} \subset W\}$, where $\Omega_r$ is the set of all boxes of the form $Q = \{ x: a_i \leq x_i < a_i + 2^{-r}, a_i \in P_r, 1 \leq i \leq k\}$. He then defines $$E_r = \bigcup\limits_{Q \in S_r} Q$$ and applies Urysohn's Lemma to obtain a function $0 \leq f_r \leq 1$ such that $f[\overline{E}_r] = 1$, $\operatorname{supp}(f)\subseteq W$, and $\overline{E}_r \subseteq \operatorname{supp}(f)$. Note that $\overline{E}_r $ is compact. He then asserts without proof that $$\operatorname{vol}(E_r) \leq \Lambda f_r \leq \Lambda g_r \leq \operatorname{vol}(W)$$ where $g_r := \max\{f_i: 1 \leq i \leq r\}$. How do we establish this inequality? It would seem a priori that it would arise out of demonstrating $\operatorname{vol}(E_r) \leq \Lambda_n f$ for all $n$, but I have so far been unsuccessful in this matter. Using Urysohn's Lemma we obtained that $\chi_{\overline{E}_r} \leq f \leq \chi_W$, which may yield another route. As a secondary question, are there any other good sources for a construction of the Lebesgue measure on $\mathbb{R}^k$ using the Riesz Representation Theorem? All of the other standard texts, e.g. Royden, on the subject construct an outer Lebesgue measure and extend using the results of Caratheodory.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral']"
25,mathematical analysis books for self study [duplicate],mathematical analysis books for self study [duplicate],,"This question already has answers here : Good First Course in real analysis book for self study (7 answers) Closed 11 years ago . I am looking for mathematical analysis books whose explanation is polite. If they has many familiar example, I will be happier. I am familiar with set theory, group theory, elementary theory, but am not familiar with analysis at all.","This question already has answers here : Good First Course in real analysis book for self study (7 answers) Closed 11 years ago . I am looking for mathematical analysis books whose explanation is polite. If they has many familiar example, I will be happier. I am familiar with set theory, group theory, elementary theory, but am not familiar with analysis at all.",,"['analysis', 'reference-request']"
26,Trying to prove a theorem about convergence of a sequence.,Trying to prove a theorem about convergence of a sequence.,,"The following is the problem that I am working on. Prove that if $\{s_n\}$ converges, then $\{|s_n|\}$ converges. The following is the idea of the proof I'm trying to make, so it's a little loose. However, I'm not 100% sure if I'm doing it right. proof: If $\{s_n\}$ converges, then $\{s_n\}$ must be bounded and Cauchy, i.e, $\delta(s_n,s_m) < \epsilon$ for a large enough $n,m$. Consider $\delta(|s_n|,s_m) $. If $s_n > 0$ then $\delta(|s_n|,s_m) =\delta(s_n,s_m) < \epsilon$ so we are done. And I'm stuck here. I cannot make a good argument when $s_n<o$. I can see that $\{|s_n|\}$ is bounded above by either $\sup (s_n)$ or $|\inf (s_n)|$, but I remember that boundedness doesn't guarantee convergence. $\{s_n\}$ is not necessarily monotonic either, so I was wondering what I could do. Can someone help me out ?","The following is the problem that I am working on. Prove that if $\{s_n\}$ converges, then $\{|s_n|\}$ converges. The following is the idea of the proof I'm trying to make, so it's a little loose. However, I'm not 100% sure if I'm doing it right. proof: If $\{s_n\}$ converges, then $\{s_n\}$ must be bounded and Cauchy, i.e, $\delta(s_n,s_m) < \epsilon$ for a large enough $n,m$. Consider $\delta(|s_n|,s_m) $. If $s_n > 0$ then $\delta(|s_n|,s_m) =\delta(s_n,s_m) < \epsilon$ so we are done. And I'm stuck here. I cannot make a good argument when $s_n<o$. I can see that $\{|s_n|\}$ is bounded above by either $\sup (s_n)$ or $|\inf (s_n)|$, but I remember that boundedness doesn't guarantee convergence. $\{s_n\}$ is not necessarily monotonic either, so I was wondering what I could do. Can someone help me out ?",,"['sequences-and-series', 'analysis']"
27,"Explanation to statement ""If $f,g$ are uniformly continuous on $S$,then $fg$ may not be uniformly continuous on $S$""","Explanation to statement ""If  are uniformly continuous on ,then  may not be uniformly continuous on ""","f,g S fg S","If $f,g$ is uniformly continuous on some domain $S$,then $fg$ still uniformly continuous. I think i could find a counter example but when i try to explain why the statement fail by considering some property the uniform continuous functions have, it seems that i don't quite see(or don't quite get the big picture) why they fail. So my question is can it be explained by considering the property of the uniform continuity to explain the statement above The definition of uniform continuity I am studying: $ \forall \epsilon>0, \exists \delta>0 s.t.\forall x,y \in S,d(x,y)<\delta \implies d(f(x),f(y))<\epsilon$","If $f,g$ is uniformly continuous on some domain $S$,then $fg$ still uniformly continuous. I think i could find a counter example but when i try to explain why the statement fail by considering some property the uniform continuous functions have, it seems that i don't quite see(or don't quite get the big picture) why they fail. So my question is can it be explained by considering the property of the uniform continuity to explain the statement above The definition of uniform continuity I am studying: $ \forall \epsilon>0, \exists \delta>0 s.t.\forall x,y \in S,d(x,y)<\delta \implies d(f(x),f(y))<\epsilon$",,"['real-analysis', 'analysis']"
28,lagrange multipliers fails,lagrange multipliers fails,,"I am looking for a certain counter example. Assume a $C^1$ function $f$ is to be optimized with respect to a $C^1$ constraint $g=0$, and we have at a point $(x,y)$, the existence of a lagrange multiplier $\lambda$ with $$ \begin{align} &\nabla f(x,y)=\lambda\nabla g(x,y)\\ &\nabla g(x,y)\neq0\\ \end{align} $$ But $f$ fails to have an extremum at this point with respect to $g=0$ thanks","I am looking for a certain counter example. Assume a $C^1$ function $f$ is to be optimized with respect to a $C^1$ constraint $g=0$, and we have at a point $(x,y)$, the existence of a lagrange multiplier $\lambda$ with $$ \begin{align} &\nabla f(x,y)=\lambda\nabla g(x,y)\\ &\nabla g(x,y)\neq0\\ \end{align} $$ But $f$ fails to have an extremum at this point with respect to $g=0$ thanks",,"['real-analysis', 'analysis', 'optimization', 'lagrange-multiplier']"
29,Fourier series $\sum_{m=0}^\infty \frac{\cos (2m+1)x}{2m+1}$,Fourier series,\sum_{m=0}^\infty \frac{\cos (2m+1)x}{2m+1},Does anyone know the sum of Fourier series $$\sum_{m=0}^\infty \frac{\cos (2m+1)x}{2m+1}?$$ I tried WA; it does not return a function.,Does anyone know the sum of Fourier series $$\sum_{m=0}^\infty \frac{\cos (2m+1)x}{2m+1}?$$ I tried WA; it does not return a function.,,"['sequences-and-series', 'analysis', 'fourier-series']"
30,questions about integration by substitution,questions about integration by substitution,,"I've calculated the following integral by substitution $z=x^2$ $$\int x\cdot\cos(x^2)\,\mathrm d x=\frac12\int\cos(z)\,\mathrm dz=\frac12\sin(z)+c=\frac12\cos(x^2)+c$$ with $c\in\mathbb R$. My first question is about the constant $c$. Is it the same constant when you resubstitute? And second, I've used the trick $z=x^2\leadsto \mathrm dz=2x\,\mathrm dx\leadsto \mathrm dx=\frac{\mathrm dz}{2x}$. Can you place $\Leftrightarrow$ between these 'transformations'? Or just $\Rightarrow$ or none of them?","I've calculated the following integral by substitution $z=x^2$ $$\int x\cdot\cos(x^2)\,\mathrm d x=\frac12\int\cos(z)\,\mathrm dz=\frac12\sin(z)+c=\frac12\cos(x^2)+c$$ with $c\in\mathbb R$. My first question is about the constant $c$. Is it the same constant when you resubstitute? And second, I've used the trick $z=x^2\leadsto \mathrm dz=2x\,\mathrm dx\leadsto \mathrm dx=\frac{\mathrm dz}{2x}$. Can you place $\Leftrightarrow$ between these 'transformations'? Or just $\Rightarrow$ or none of them?",,['calculus']
31,How to prove these,How to prove these,,"How to prove these (1) $\displaystyle\sum_{n=4}^\infty\frac{(-1)^n\ln n}n$ if it is absolutely  or conditionally convergent? and (2) $\displaystyle\sum_{n=1}^\infty\frac n{(-2)^n}$ if it is absolutely or conditionally convergent? What I am thinking about the first one is that the series will be conditionally convergent as the sequence $\frac{\ln n}n$ is eventually decreasing to 0 and by Leibniz theorem the series will be convergent, but I don't know how show it not abosultely. And also I am thinking in the same thing about the second series. Can any one solve these for  me so that I can get the complete idea and I'll try to solve another problems by myself. Thanks.","How to prove these (1) $\displaystyle\sum_{n=4}^\infty\frac{(-1)^n\ln n}n$ if it is absolutely  or conditionally convergent? and (2) $\displaystyle\sum_{n=1}^\infty\frac n{(-2)^n}$ if it is absolutely or conditionally convergent? What I am thinking about the first one is that the series will be conditionally convergent as the sequence $\frac{\ln n}n$ is eventually decreasing to 0 and by Leibniz theorem the series will be convergent, but I don't know how show it not abosultely. And also I am thinking in the same thing about the second series. Can any one solve these for  me so that I can get the complete idea and I'll try to solve another problems by myself. Thanks.",,['analysis']
32,L'Hôpital's rule for vector-valued functions,L'Hôpital's rule for vector-valued functions,,"What are the traps, when using L'Hôpital's rule in multivariable calculus to determine the lim of an vector function?  I heard there some more cases, where L'Hôpital's rule in multivariable calculus is not applicable. Update: I meant multivariable calculus. Its a bit difficult to translate math problems from german to english.","What are the traps, when using L'Hôpital's rule in multivariable calculus to determine the lim of an vector function?  I heard there some more cases, where L'Hôpital's rule in multivariable calculus is not applicable. Update: I meant multivariable calculus. Its a bit difficult to translate math problems from german to english.",,['analysis']
33,Proof on showing whether the complement of the set $\mathbb Q\times\mathbb Q$ is open or close set,Proof on showing whether the complement of the set  is open or close set,\mathbb Q\times\mathbb Q,"The question is as follows: Given set A = $\mathbb Q\times\mathbb Q$ Determine whether the complement set $ A^c$ is open or close. Here are my thoughts : 1/ $\ A^c = \{\mbox{all points }(x,y)\mbox{ where at least one of }x, y \mbox{ is irrational}\}$ . 2/ Known that $\mathbb Q$ is neither open or close, its Cartesian product $\mathbb Q\times \mathbb Q$ therefore is neither open or close. 3/ Thus the complement of that Cartesian product is neither open or close Are my ideas ok ? Would it be safe to say the following ? Since $\mathbb Q$ and its complement have density, then for a sequence of points $(x,y)$ in $\ A^c$ , we can always find a point $(a,b)$ , which is an element of $A$ , such that the sequence converges to $(a,b)$ .   Thus $\ A^c$ is not closed On the other hand, $\ A^c$ is not open since we can always find an element in this set, such that either the $1$ st or $2$ nd coordinate is a rational number. Please help me on this question Thanks in advance ^_^","The question is as follows: Given set A = Determine whether the complement set is open or close. Here are my thoughts : 1/ . 2/ Known that is neither open or close, its Cartesian product therefore is neither open or close. 3/ Thus the complement of that Cartesian product is neither open or close Are my ideas ok ? Would it be safe to say the following ? Since and its complement have density, then for a sequence of points in , we can always find a point , which is an element of , such that the sequence converges to .   Thus is not closed On the other hand, is not open since we can always find an element in this set, such that either the st or nd coordinate is a rational number. Please help me on this question Thanks in advance ^_^","\mathbb Q\times\mathbb Q  A^c \ A^c = \{\mbox{all points }(x,y)\mbox{ where at least one of }x, y \mbox{ is irrational}\} \mathbb Q \mathbb Q\times \mathbb Q \mathbb Q (x,y) \ A^c (a,b) A (a,b) \ A^c \ A^c 1 2","['general-topology', 'analysis']"
34,"Intuition behind $(\{a, b, p, q\} \subset \mathbb{R}^{+} \;\wedge\;\; 1/p +1/q = 1) \Rightarrow a^p/p + b^q/q \geq ab$",Intuition behind,"(\{a, b, p, q\} \subset \mathbb{R}^{+} \;\wedge\;\; 1/p +1/q = 1) \Rightarrow a^p/p + b^q/q \geq ab","If $p$ and $q$ are positive real numbers with 1 $$ \frac{1}{p} + \frac{1}{q} = 1,$$ then, for any non-negative real numbers $a, b$, $$ \frac{a^p}{p} + \frac{b^q}{q} \geq ab$$ My textbook offers a totally unenlightening (albeit fairly clear) proof of this fact. 2 What's the intuition behind it? 1 Is there a name for pairs of positive numbers $p, q$ in that satisfy $\frac{1}{p} + \frac{1}{q} = 1\;$ ? 2 Said textbook does not give any name for this theorem, or for the inequality.  I'd love to know what they are.","If $p$ and $q$ are positive real numbers with 1 $$ \frac{1}{p} + \frac{1}{q} = 1,$$ then, for any non-negative real numbers $a, b$, $$ \frac{a^p}{p} + \frac{b^q}{q} \geq ab$$ My textbook offers a totally unenlightening (albeit fairly clear) proof of this fact. 2 What's the intuition behind it? 1 Is there a name for pairs of positive numbers $p, q$ in that satisfy $\frac{1}{p} + \frac{1}{q} = 1\;$ ? 2 Said textbook does not give any name for this theorem, or for the inequality.  I'd love to know what they are.",,"['analysis', 'inequality']"
35,Why $ g(p) = 0.5 p^{-0.2} + 0.5 p^{-0.5} $ has a well-defined inverse that is continuous and strictly decreasing.,Why  has a well-defined inverse that is continuous and strictly decreasing., g(p) = 0.5 p^{-0.2} + 0.5 p^{-0.5} ,"A book that I am reading claims the following about the function $ g(p) = 0.5 p^{-0.2} + 0.5 p^{-0.5} $ (which is a demand function): Formal arguments based on the Intermediate Value Theorem and the Implicit Function     Theorem would establish that the inverse demand function is well-defined, continuous     and strictly decreasing. I have done some reading on the Intermediate Value Theorem and the Implicit Function Theorem . I understand what the Intermediate Value Theorem is about, but I am not so sure if I have understood the Implicit Function Theorem. My question is: What might be those formal arguments based on these two theorems, and how do they show that the inverse function is well-defined, continuous and strictly decreasing?","A book that I am reading claims the following about the function $ g(p) = 0.5 p^{-0.2} + 0.5 p^{-0.5} $ (which is a demand function): Formal arguments based on the Intermediate Value Theorem and the Implicit Function     Theorem would establish that the inverse demand function is well-defined, continuous     and strictly decreasing. I have done some reading on the Intermediate Value Theorem and the Implicit Function Theorem . I understand what the Intermediate Value Theorem is about, but I am not so sure if I have understood the Implicit Function Theorem. My question is: What might be those formal arguments based on these two theorems, and how do they show that the inverse function is well-defined, continuous and strictly decreasing?",,"['calculus', 'analysis', 'inverse']"
36,"for $|f (x_1) + ... + f (x_n)| \le M$ how to prove that $S=\{ x\in [0,1]:f(x)\ne 0\}$ is countable?",for  how to prove that  is countable?,"|f (x_1) + ... + f (x_n)| \le M S=\{ x\in [0,1]:f(x)\ne 0\}","Let f be a real-valued function defined for every $x$ in the interval   $0\le x \le 1$. Suppose there is a positive number M having the following   property: for every choice of a finite number of points $x_1, x_2, ..., x_n$ in the   interval $0 \le x \le  1$, the sum   $$|f (x_1) + ... + f (x_n)|  \le M$$   Let $S$ be the set of those $x$ in $0 \le x \le 1$ for which $f(x) \neq 0$. Prove that $S$   is countable. I am having hard time to understand it's solution. The solution is as follows. Proof : Let $S_n = \{x \in [0, 1] : |f (x)| \ge 1/n\}$ , then $S_n$ is a finite set by   hypothesis. In addition, $S = \cup_{n=1}^\infty S_n$. So, S is countable. There are infinite irrationals between $0$ and $1$, how does defining taking $x$ such that $|f(x)| \ge 1/n$ prove that the set is countable. Aren't we counting irrationals as well as rationals between $0$ and $1$? I think it would be more intuitive to begin by assuming that the set $S$ in uncountable and arrive at contradiction that $|f (x_1) + ... + f (x_n)|$ is bounded. But I don't know how. Can we do this way? Also can anyone elaborate that proof from manual so that I can understand?","Let f be a real-valued function defined for every $x$ in the interval   $0\le x \le 1$. Suppose there is a positive number M having the following   property: for every choice of a finite number of points $x_1, x_2, ..., x_n$ in the   interval $0 \le x \le  1$, the sum   $$|f (x_1) + ... + f (x_n)|  \le M$$   Let $S$ be the set of those $x$ in $0 \le x \le 1$ for which $f(x) \neq 0$. Prove that $S$   is countable. I am having hard time to understand it's solution. The solution is as follows. Proof : Let $S_n = \{x \in [0, 1] : |f (x)| \ge 1/n\}$ , then $S_n$ is a finite set by   hypothesis. In addition, $S = \cup_{n=1}^\infty S_n$. So, S is countable. There are infinite irrationals between $0$ and $1$, how does defining taking $x$ such that $|f(x)| \ge 1/n$ prove that the set is countable. Aren't we counting irrationals as well as rationals between $0$ and $1$? I think it would be more intuitive to begin by assuming that the set $S$ in uncountable and arrive at contradiction that $|f (x_1) + ... + f (x_n)|$ is bounded. But I don't know how. Can we do this way? Also can anyone elaborate that proof from manual so that I can understand?",,['analysis']
37,Is Exercise 8.3 in Rudin's Principles of Analysis as easy as it seems?,Is Exercise 8.3 in Rudin's Principles of Analysis as easy as it seems?,,"Rudin Theorem 8.3 says that if $$\sum_{j=1}^\infty |a_{ij}| = b_i$$ and $\sum b_i$ converges, then $$\sum_i \sum_j a_{ij} = \sum_j \sum_i a_{ij}$$ Rudin 8.3 asks us to show that if $a_{ij} \geq 0$ for all $i,j$, then $$\sum_i \sum_j a_{ij} = \sum_j \sum_i a_{ij}$$ including the case $\infty = \infty$. It seems to me that the conditions of the Theorem follow pretty simply from the fact that $a_{ij} = |a_{ij}|$, and so we can say that if the LHS converges then the RHS converges, and to the same number, and that if the RHS converges, then the LHS converges, and to the same number. So the LHS converges if and only if the RHS converges to the same number. If one side diverges, then the other also diverges, and they both must diverge to $+\infty$. Hence we have equality. But that seems too easy. Is there something I am missing?","Rudin Theorem 8.3 says that if $$\sum_{j=1}^\infty |a_{ij}| = b_i$$ and $\sum b_i$ converges, then $$\sum_i \sum_j a_{ij} = \sum_j \sum_i a_{ij}$$ Rudin 8.3 asks us to show that if $a_{ij} \geq 0$ for all $i,j$, then $$\sum_i \sum_j a_{ij} = \sum_j \sum_i a_{ij}$$ including the case $\infty = \infty$. It seems to me that the conditions of the Theorem follow pretty simply from the fact that $a_{ij} = |a_{ij}|$, and so we can say that if the LHS converges then the RHS converges, and to the same number, and that if the RHS converges, then the LHS converges, and to the same number. So the LHS converges if and only if the RHS converges to the same number. If one side diverges, then the other also diverges, and they both must diverge to $+\infty$. Hence we have equality. But that seems too easy. Is there something I am missing?",,"['real-analysis', 'sequences-and-series', 'analysis']"
38,The radius of convergence of the power series $\sum_{0}^{\infty}P(n)x^n$,The radius of convergence of the power series,\sum_{0}^{\infty}P(n)x^n,"I came across the following problem: Let $P(x)$ be a non-zero polynomial of degree $N.$ The radius of convergence of the power series $\sum_{0}^{\infty}P(n)x^n$ (a)depends on $N,$ (b)is $1$ for all $N,$ (c)is $0$ for all $N,$ (d)is $\infty$ for all $N.$ My attempts: I see that  $lim_{n\to\infty}P(n+1)/P(n)=1$,because if I take $N=2$,then we can take $P(n)=a_0+a_1n+a_2n^2$ and $P(n+1)=a_0+a_1(n+1)+a_2(n+1)^2$ and hence $\frac {P(n+1)}{P(n)}=1+\frac {a_1+(2n+1)a_2}{P(n)}$ which tends to $1$ as $n$ tends to $\infty$. From here,Can i say that (b)is the right choice? Am i going in the right direction? Please comment. Can someone point me in the right direction? Thanks in advance for your time.","I came across the following problem: Let $P(x)$ be a non-zero polynomial of degree $N.$ The radius of convergence of the power series $\sum_{0}^{\infty}P(n)x^n$ (a)depends on $N,$ (b)is $1$ for all $N,$ (c)is $0$ for all $N,$ (d)is $\infty$ for all $N.$ My attempts: I see that  $lim_{n\to\infty}P(n+1)/P(n)=1$,because if I take $N=2$,then we can take $P(n)=a_0+a_1n+a_2n^2$ and $P(n+1)=a_0+a_1(n+1)+a_2(n+1)^2$ and hence $\frac {P(n+1)}{P(n)}=1+\frac {a_1+(2n+1)a_2}{P(n)}$ which tends to $1$ as $n$ tends to $\infty$. From here,Can i say that (b)is the right choice? Am i going in the right direction? Please comment. Can someone point me in the right direction? Thanks in advance for your time.",,['real-analysis']
39,Infinite series question from analysis,Infinite series question from analysis,,"Let $a_n > 0$ and for all $n$ let $$\sum\limits_{j=n}^{2n} a_j \le \dfrac 1n $$  Prove or give a counterexample to the statement $$\sum\limits_{j=1}^{\infty} a_j < \infty$$ Not sure where to start, a push in the right direction would be great.  Thanks!","Let $a_n > 0$ and for all $n$ let $$\sum\limits_{j=n}^{2n} a_j \le \dfrac 1n $$  Prove or give a counterexample to the statement $$\sum\limits_{j=1}^{\infty} a_j < \infty$$ Not sure where to start, a push in the right direction would be great.  Thanks!",,"['real-analysis', 'analysis']"
40,Prove that there is no $1-1$ continuously differentiable map $f : \mathbb{R}^2 \to \mathbb{R}.$ [duplicate],Prove that there is no  continuously differentiable map  [duplicate],1-1 f : \mathbb{R}^2 \to \mathbb{R}.,This question already has answers here : Closed 11 years ago . Possible Duplicate: existence of a map between $\mathbb R^2$ and $\mathbb R$ Prove that there is no $1-1$ continuously differentiable map $f : \mathbb{R}^2 \to \mathbb{R}$. I know it's something to do with the implicit function theorem but I don't even know  where to start?,This question already has answers here : Closed 11 years ago . Possible Duplicate: existence of a map between $\mathbb R^2$ and $\mathbb R$ Prove that there is no $1-1$ continuously differentiable map $f : \mathbb{R}^2 \to \mathbb{R}$. I know it's something to do with the implicit function theorem but I don't even know  where to start?,,['analysis']
41,Density of space in a Sobolev space,Density of space in a Sobolev space,,"An exercise from Gilbarg-Trudinger Elliptic Partial Differential Equations states the following : ""Using Lemma 9.12, show that for a $C^{1,1}$ domain $\Omega$ the subspace $$\{u \in C^2{(\bar{\Omega})} | u = 0 \ \text{on} \ \partial \Omega\}$$   is dense in $W^{2,p}(\Omega)\cap W^{1,p}_0(\Omega)$ for $1<p<\infty$."" I don't see how the lemma quoted helps in the proof. Here is the lemma Lemma 9.12 Let $u \in W^{1,1}_0(\Omega^{+}), f\in L^p(\Omega^{+}), 1<p<\infty$ satisfy $\Delta u= f$ weakly in $\Omega^{+}$ with $u=0$ near $(\partial \Omega)^{+}$.  Then $u \in W^{2,p}(\Omega)\cap W^{1,p}_0(\Omega^{+})$ and    $$||D^2u||_{p;\Omega^{+}} \leq C||f||_{p;\Omega^{+}}.$$ Here $\Omega^{+}$ means $\{x \in \partial \Omega | x_n >0\}$. Could you provide some help please?  Thank you.","An exercise from Gilbarg-Trudinger Elliptic Partial Differential Equations states the following : ""Using Lemma 9.12, show that for a $C^{1,1}$ domain $\Omega$ the subspace $$\{u \in C^2{(\bar{\Omega})} | u = 0 \ \text{on} \ \partial \Omega\}$$   is dense in $W^{2,p}(\Omega)\cap W^{1,p}_0(\Omega)$ for $1<p<\infty$."" I don't see how the lemma quoted helps in the proof. Here is the lemma Lemma 9.12 Let $u \in W^{1,1}_0(\Omega^{+}), f\in L^p(\Omega^{+}), 1<p<\infty$ satisfy $\Delta u= f$ weakly in $\Omega^{+}$ with $u=0$ near $(\partial \Omega)^{+}$.  Then $u \in W^{2,p}(\Omega)\cap W^{1,p}_0(\Omega^{+})$ and    $$||D^2u||_{p;\Omega^{+}} \leq C||f||_{p;\Omega^{+}}.$$ Here $\Omega^{+}$ means $\{x \in \partial \Omega | x_n >0\}$. Could you provide some help please?  Thank you.",,"['analysis', 'partial-differential-equations', 'sobolev-spaces']"
42,Analysis problem with volume,Analysis problem with volume,,"I'm looking for a complete answer to this problem. Let $U,V\subset\mathbb{R}^d$ be open sets and $\Phi:U\rightarrow V$ be a homeomorphism. Suppose $\Phi$ is differentiable in $x_0$ and that $\det D\Phi(x_0)=0$. Let $\{C_n\}$ be a sequence of open(or closed) cubes in $U$ such that $x_0$ is inside the cubes and with its sides going to $0$ when $n\rightarrow\infty$. Denoting the $d$-dimension volume of a set by $\operatorname{Vol}(.)$, show that $$\lim_{n\rightarrow\infty}\frac{\operatorname{Vol}(\Phi(C_n))}{\operatorname{Vol}(C_n)}=0$$ I know that $\Phi$ cant be a diffeomorphism in $x_0$, but a have know idea how to use this, or how to do anything different. Thanks for helping.","I'm looking for a complete answer to this problem. Let $U,V\subset\mathbb{R}^d$ be open sets and $\Phi:U\rightarrow V$ be a homeomorphism. Suppose $\Phi$ is differentiable in $x_0$ and that $\det D\Phi(x_0)=0$. Let $\{C_n\}$ be a sequence of open(or closed) cubes in $U$ such that $x_0$ is inside the cubes and with its sides going to $0$ when $n\rightarrow\infty$. Denoting the $d$-dimension volume of a set by $\operatorname{Vol}(.)$, show that $$\lim_{n\rightarrow\infty}\frac{\operatorname{Vol}(\Phi(C_n))}{\operatorname{Vol}(C_n)}=0$$ I know that $\Phi$ cant be a diffeomorphism in $x_0$, but a have know idea how to use this, or how to do anything different. Thanks for helping.",,"['real-analysis', 'analysis']"
43,Is $f^{(0)}:=f$ a valid assumption for a proof involving derivatives on induction?,Is  a valid assumption for a proof involving derivatives on induction?,f^{(0)}:=f,"My motivation for this was proving the general Cauchy-Integral formula (in complex analysis) for an arbitrary derivative.  Every book I read shows at least the first derivative using a $\delta-\epsilon$ argument, but we already did this technique/style when showing the proof for $f(z_0)=\int_\Gamma \frac{f(z)}{(z-z_0)}dz$. So my question is a general one (i.e. I mention the C-Int formula only as my motivation), and I'm curious if (in the general case), we may (when convenient) consider the ""zeroth derivative"" (the function) as a base case for induction on a derivative.... obviously, the general formula for any such problem wasn't probably ""seen"" by looking at only that base case (I'm certain that the historical derivation for the general formula required the first three derivatives since the third derivative is the first time $n!≠n:n≠0$.  None the less, I'm wondering if ""zero derivative"" is a valid assumption for such a proof or just a ""definition"".","My motivation for this was proving the general Cauchy-Integral formula (in complex analysis) for an arbitrary derivative.  Every book I read shows at least the first derivative using a $\delta-\epsilon$ argument, but we already did this technique/style when showing the proof for $f(z_0)=\int_\Gamma \frac{f(z)}{(z-z_0)}dz$. So my question is a general one (i.e. I mention the C-Int formula only as my motivation), and I'm curious if (in the general case), we may (when convenient) consider the ""zeroth derivative"" (the function) as a base case for induction on a derivative.... obviously, the general formula for any such problem wasn't probably ""seen"" by looking at only that base case (I'm certain that the historical derivation for the general formula required the first three derivatives since the third derivative is the first time $n!≠n:n≠0$.  None the less, I'm wondering if ""zero derivative"" is a valid assumption for such a proof or just a ""definition"".",,['analysis']
44,Function is pointwise limit of integrals,Function is pointwise limit of integrals,,"This is a question from some old masters exams. Let $\phi_{n}$ be a sequence of continuous, real functions on $\mathbb{R}$ such that $\phi_{n}(x) = 0$ for all $|x|\geq 1/n$ and $\phi_{n}(x)\geq 0$ for all $x\in\mathbb{R}$, and $$\int\limits_{-1}^{1}{\phi_{n}(x)dx}=1$$ For each continuous function $f:\mathbb{R}\rightarrow\mathbb{R}$, let $$f_{n}(x)=\int\limits_{-\infty}^{\infty}{\phi_{n}(x-y)f(y)dy}$$ Prove that $f_{n}(x)$ converges pointwise to $f(x)$ and prove that if $f(x)=0$ for $|x|\geq 10$, then $f_{n}$ converges uniformly to $f$.","This is a question from some old masters exams. Let $\phi_{n}$ be a sequence of continuous, real functions on $\mathbb{R}$ such that $\phi_{n}(x) = 0$ for all $|x|\geq 1/n$ and $\phi_{n}(x)\geq 0$ for all $x\in\mathbb{R}$, and $$\int\limits_{-1}^{1}{\phi_{n}(x)dx}=1$$ For each continuous function $f:\mathbb{R}\rightarrow\mathbb{R}$, let $$f_{n}(x)=\int\limits_{-\infty}^{\infty}{\phi_{n}(x-y)f(y)dy}$$ Prove that $f_{n}(x)$ converges pointwise to $f(x)$ and prove that if $f(x)=0$ for $|x|\geq 10$, then $f_{n}$ converges uniformly to $f$.",,"['real-analysis', 'analysis', 'integration']"
45,"Showing $\sum_{n=1}^\infty n^{-x} $ *doesn't* converge uniformly for $x \in (1,\infty)$?",Showing  *doesn't* converge uniformly for ?,"\sum_{n=1}^\infty n^{-x}  x \in (1,\infty)","A question says, show that $\sum_{n=1}^\infty n^{-x} $ converges pointwise but not uniformly for $x \in (1,\infty)$. I can show it converges pointwise by taking $x\in (1+\delta, \infty)$ for any $x$ and $\delta>0$ and then using the Weierstrass-M test on $1+\delta$. But I'm struggeling to show that it doesn't converge uniformly? Thanks!","A question says, show that $\sum_{n=1}^\infty n^{-x} $ converges pointwise but not uniformly for $x \in (1,\infty)$. I can show it converges pointwise by taking $x\in (1+\delta, \infty)$ for any $x$ and $\delta>0$ and then using the Weierstrass-M test on $1+\delta$. But I'm struggeling to show that it doesn't converge uniformly? Thanks!",,['real-analysis']
46,Show the following  series converges.,Show the following  series converges.,,"Let the real sequence ${x_n}$ be given by, $$\sum_{j=1}^{2n} \frac {1}{j} - \sum_{j=1}^{n} \frac {1}{j}. $$ Show  that $0<x_{n}<x_{n+1}$ and that $x_{n}<1$ for all $n$. Deduce that $x_{n}$ converges, giving your reason. I seem to think this has something to do with $\sum_{j=1}^{2n} \frac {1}{j} - \sum_{j=1}^{n} \frac {1}{j} $ = $\sum_{j=1}^{n} \frac {1}{j+n}$? Many thanks in advance.","Let the real sequence ${x_n}$ be given by, $$\sum_{j=1}^{2n} \frac {1}{j} - \sum_{j=1}^{n} \frac {1}{j}. $$ Show  that $0<x_{n}<x_{n+1}$ and that $x_{n}<1$ for all $n$. Deduce that $x_{n}$ converges, giving your reason. I seem to think this has something to do with $\sum_{j=1}^{2n} \frac {1}{j} - \sum_{j=1}^{n} \frac {1}{j} $ = $\sum_{j=1}^{n} \frac {1}{j+n}$? Many thanks in advance.",,"['real-analysis', 'sequences-and-series', 'analysis']"
47,Proof for an Inequality,Proof for an Inequality,,"Let $e^{e^x}=\sum\limits_{n\geq0}a_nx^n$, prove that $$a_n\geq e(\gamma\log n)^{-n}$$ for  $n\geq2$, where $\gamma$ is some constant great than $e$.","Let $e^{e^x}=\sum\limits_{n\geq0}a_nx^n$, prove that $$a_n\geq e(\gamma\log n)^{-n}$$ for  $n\geq2$, where $\gamma$ is some constant great than $e$.",,"['analysis', 'inequality']"
48,A condition for a function to be constant,A condition for a function to be constant,,"I need to proof this result: Let $\alpha >1$ and $c\in\mathbb{R}$. If $f:U\subset\mathbb{R}^m\rightarrow\mathbb{R}^n$, U open, satisfies $|f(x)-f(y)|\leq c|x-y|^\alpha$ for every $x$, $y$ $\in U$, then $f$ is constant in every component of $U$. I just didn't have any idea on how to start it, I'm doing my first multivariable analysis course now!","I need to proof this result: Let $\alpha >1$ and $c\in\mathbb{R}$. If $f:U\subset\mathbb{R}^m\rightarrow\mathbb{R}^n$, U open, satisfies $|f(x)-f(y)|\leq c|x-y|^\alpha$ for every $x$, $y$ $\in U$, then $f$ is constant in every component of $U$. I just didn't have any idea on how to start it, I'm doing my first multivariable analysis course now!",,"['analysis', 'multivariable-calculus']"
49,Find a continuous differential function,Find a continuous differential function,,Find a continuous differential function $f$ on $\mathbb{R}$ sastisfies the following conditions $f(\mathbb{Q}) \subset \mathbb{Q}$ $f(\mathbb{R} \backslash \mathbb{Q}) \subset      \mathbb{R} \backslash \mathbb{Q}$; $f'$ isn't constant.,Find a continuous differential function $f$ on $\mathbb{R}$ sastisfies the following conditions $f(\mathbb{Q}) \subset \mathbb{Q}$ $f(\mathbb{R} \backslash \mathbb{Q}) \subset      \mathbb{R} \backslash \mathbb{Q}$; $f'$ isn't constant.,,"['calculus', 'analysis']"
50,Supremums of measurable functions,Supremums of measurable functions,,"According to my textbook, supremums of measurable functions exist and are measurable.  But what about the sequence of functions $f_n: [0, 1] \to \mathbb{R}$ given by $f_n = n$?  I don't think this sequence has a supremum but I do think all those functions are measurable.  How can I reconcile this difference? Thank you!","According to my textbook, supremums of measurable functions exist and are measurable.  But what about the sequence of functions $f_n: [0, 1] \to \mathbb{R}$ given by $f_n = n$?  I don't think this sequence has a supremum but I do think all those functions are measurable.  How can I reconcile this difference? Thank you!",,['analysis']
51,"How to prove $(1-2x)^2=1/3+4/\pi^2\sum_1^\infty \cos(2n x \pi)/n^2$ for $x \in [0,1)$?",How to prove  for ?,"(1-2x)^2=1/3+4/\pi^2\sum_1^\infty \cos(2n x \pi)/n^2 x \in [0,1)","I tried to prove that $$(1-2x)^2=1/3+4/\pi^2\sum_1^\infty \cos(2n x \pi)/n^2$$ for $x \in [0,1)$ with Fourier analysis, but I just found a Fourier series which defines the function. I also found the fourier series of $\cos(2n x \pi)$. I don't think these results are helpful. Any suggestions on how to prove this equation?","I tried to prove that $$(1-2x)^2=1/3+4/\pi^2\sum_1^\infty \cos(2n x \pi)/n^2$$ for $x \in [0,1)$ with Fourier analysis, but I just found a Fourier series which defines the function. I also found the fourier series of $\cos(2n x \pi)$. I don't think these results are helpful. Any suggestions on how to prove this equation?",,"['analysis', 'fourier-analysis', 'fourier-series']"
52,L'Hospital's Rule for $\infty/\infty$,L'Hospital's Rule for,\infty/\infty,"Arthur Mattuck in his Introduction to Analysis book, pg. 220 says, in order to prove L'Hospital's Rule for $\infty/\infty$ case, Let $L=\lim_{x \to \infty} \frac{f'(x)}{g'(x)}$ and choose $a$ so that $\frac{f'(x)}{g'(x)} \stackrel{\approx}{\epsilon} L$  for $x>a$. And prove two approximations below (valid for $x\gg 1$) $$ \frac{f(x)}{g(x)} \stackrel{\approx}{\epsilon} \frac{f(x)-f(a)}{g(x)-g(a)} \stackrel{\approx}{\epsilon} L $$ His hint is, for the first approximation, that we write $$ f(x) - f(a) = f(x) [ 1 - f(a)/f(x)] $$ and use limit theorem, for the second, use the Cauchy Mean-value Theorem. Anyone know how to pursue this proof? Thanks,","Arthur Mattuck in his Introduction to Analysis book, pg. 220 says, in order to prove L'Hospital's Rule for $\infty/\infty$ case, Let $L=\lim_{x \to \infty} \frac{f'(x)}{g'(x)}$ and choose $a$ so that $\frac{f'(x)}{g'(x)} \stackrel{\approx}{\epsilon} L$  for $x>a$. And prove two approximations below (valid for $x\gg 1$) $$ \frac{f(x)}{g(x)} \stackrel{\approx}{\epsilon} \frac{f(x)-f(a)}{g(x)-g(a)} \stackrel{\approx}{\epsilon} L $$ His hint is, for the first approximation, that we write $$ f(x) - f(a) = f(x) [ 1 - f(a)/f(x)] $$ and use limit theorem, for the second, use the Cauchy Mean-value Theorem. Anyone know how to pursue this proof? Thanks,",,"['calculus', 'analysis']"
53,finding maximum of a function on a closed set,finding maximum of a function on a closed set,,"I need to find the maximum of the function $\ f(x,y,z) = y $ on the following closed set : $\  y^2+x^2 + z^2 = 3 $ $\  y+ x + z=1$ But I don't have a clue on how to do it ... Trivially  i can get that $\ 0 < \max(f) <  \sqrt{3} $. Can you give me some pieces of advice on how  to solve this kind of problem .","I need to find the maximum of the function $\ f(x,y,z) = y $ on the following closed set : $\  y^2+x^2 + z^2 = 3 $ $\  y+ x + z=1$ But I don't have a clue on how to do it ... Trivially  i can get that $\ 0 < \max(f) <  \sqrt{3} $. Can you give me some pieces of advice on how  to solve this kind of problem .",,"['analysis', 'multivariable-calculus', 'optimization']"
54,"A function $f\in C([0,1])$ with certain properties",A function  with certain properties,"f\in C([0,1])","Let $f\in C([0,1])$ be differentiable on $(0,1)$. Suppose that $f(0)=f(1)=0$ and that there is an $x_0\in(0,1)$ where $f(x_0)=1$. I have to prove that there is a $c\in(0,1)$ satisfying $|f^\prime(c)|>2$. This is what I have done: $f(x)=\int^x_0f^\prime(t)dt=-\int^1_xf^\prime(t)dt$. Now suppose in the seek of a contradiction that $|f^\prime(x)|\leq2$ for every $x\in(0,1)$. Then we have $|f(x)|\leq\int^x_0|f^\prime(t)|dt\leq 2x$ and $|f(x)|\leq\int^1_x|f^\prime(t)|dt\leq 2(1-x)$. And so $|f(x)|\leq\mathrm{min}\{2x,2(1-x)\}$. Evaluating this inequality in $x_0$ we obtain $1\leq2\mathrm{min}\{x_0,1-x_0\}$ and this is a contradiction if $x_0\neq1/2$. Now, if $x_0=1/2$ we can easily reduce to the case $|f(x)|<1$ if $x\neq1/2$. But I can't continue the proof in this case. Could you help me ,please?","Let $f\in C([0,1])$ be differentiable on $(0,1)$. Suppose that $f(0)=f(1)=0$ and that there is an $x_0\in(0,1)$ where $f(x_0)=1$. I have to prove that there is a $c\in(0,1)$ satisfying $|f^\prime(c)|>2$. This is what I have done: $f(x)=\int^x_0f^\prime(t)dt=-\int^1_xf^\prime(t)dt$. Now suppose in the seek of a contradiction that $|f^\prime(x)|\leq2$ for every $x\in(0,1)$. Then we have $|f(x)|\leq\int^x_0|f^\prime(t)|dt\leq 2x$ and $|f(x)|\leq\int^1_x|f^\prime(t)|dt\leq 2(1-x)$. And so $|f(x)|\leq\mathrm{min}\{2x,2(1-x)\}$. Evaluating this inequality in $x_0$ we obtain $1\leq2\mathrm{min}\{x_0,1-x_0\}$ and this is a contradiction if $x_0\neq1/2$. Now, if $x_0=1/2$ we can easily reduce to the case $|f(x)|<1$ if $x\neq1/2$. But I can't continue the proof in this case. Could you help me ,please?",,"['real-analysis', 'analysis', 'functions']"
55,Orthonormal basis error diagonally dominant?,Orthonormal basis error diagonally dominant?,,"I'm working on an error estimate for a numerical method, and in the process I've stumbled across the following abstract inequality which I think is true, but am having a hard time proving. Suppose $\{\phi_i\}_{i=1}^N$ and $\{\psi_i\}_{i=1}^N$ are orthonormal bases of $\mathbb{R}^N$ with $(\phi_i,\psi_i) \ge 0$, and call the ""error"" between these basis vectors $e_i=\phi_i-\psi_i$. Is there a constant $C$ independent of N such that $$ \sum_{i \neq j} (e_i,e_j) \le C \sum_i ||e_i||^2? $$ I've written a matlab script to test it for thousands of random sets of orthonormal vectors and found no counterexamples so far for C=1.","I'm working on an error estimate for a numerical method, and in the process I've stumbled across the following abstract inequality which I think is true, but am having a hard time proving. Suppose $\{\phi_i\}_{i=1}^N$ and $\{\psi_i\}_{i=1}^N$ are orthonormal bases of $\mathbb{R}^N$ with $(\phi_i,\psi_i) \ge 0$, and call the ""error"" between these basis vectors $e_i=\phi_i-\psi_i$. Is there a constant $C$ independent of N such that $$ \sum_{i \neq j} (e_i,e_j) \le C \sum_i ||e_i||^2? $$ I've written a matlab script to test it for thousands of random sets of orthonormal vectors and found no counterexamples so far for C=1.",,"['linear-algebra', 'analysis', 'inequality']"
56,Simple proof of the Binomial Theorem for $\mathbb{R}$ [closed],Simple proof of the Binomial Theorem for  [closed],\mathbb{R},"As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references, or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question can be improved and possibly reopened, visit the help center for guidance. Closed 13 years ago . I have noticed that two of the math books used in high school goes about the binomial theorem in this way: Prove it on integers using induction Generalize it and use it in proofs needed to develop calculus Use calculus to prove the binomial theorem for $\mathbb{R}$ In college level text books (Rudin...) this approach is of cause not taken, but these are often to advanced to show in high school. There is a rather simple outline here that develops most of the foundation needed for proving the binomial theorem for $\mathbb{R}$. But I was wondering if anyone knows of a simple proof suitable to showing in a high school class for interested although not advanced students ?.","As it currently stands, this question is not a good fit for our Q&A format. We expect answers to be supported by facts, references, or expertise, but this question will likely solicit debate, arguments, polling, or extended discussion. If you feel that this question can be improved and possibly reopened, visit the help center for guidance. Closed 13 years ago . I have noticed that two of the math books used in high school goes about the binomial theorem in this way: Prove it on integers using induction Generalize it and use it in proofs needed to develop calculus Use calculus to prove the binomial theorem for $\mathbb{R}$ In college level text books (Rudin...) this approach is of cause not taken, but these are often to advanced to show in high school. There is a rather simple outline here that develops most of the foundation needed for proving the binomial theorem for $\mathbb{R}$. But I was wondering if anyone knows of a simple proof suitable to showing in a high school class for interested although not advanced students ?.",,"['calculus', 'analysis']"
57,"$f(x)=x-x^2$ if $x$ is rational, $x+x^2$ if $x$ is irrational","if  is rational,  if  is irrational",f(x)=x-x^2 x x+x^2 x,"$$f(x)= \left\{\begin{array}{ll}   x-x^2 &\mbox{if $x$ is rational,}\\   x+x^2 &\mbox{if $x$ is irrational.}   \end{array}\right.$$   Show that $f'(0)=1$ and yet there is no neighborhood $I$ of the point $0$ on which this function is monotonically increasing.","$$f(x)= \left\{\begin{array}{ll}   x-x^2 &\mbox{if $x$ is rational,}\\   x+x^2 &\mbox{if $x$ is irrational.}   \end{array}\right.$$   Show that $f'(0)=1$ and yet there is no neighborhood $I$ of the point $0$ on which this function is monotonically increasing.",,['analysis']
58,Ring germs of $C^{\infty}$ functions on the real line,Ring germs of  functions on the real line,C^{\infty},"Is the ring $\mathcal{O}$ of germs of $C^{\infty}$ functions defined on the neighborhoods of $0\in\mathbb{R}$ the localization of the ring of $C^{\infty}$ functions on $\mathbb{R}$ at the maximal ideal $\mathfrak{m}$ of those functions which vanish at the origin? I can construct an injective map from $C^{\infty}_\mathfrak{m}$ to $\mathcal{O}$, but I'm having trouble showing surjectivity. I know that this is true for regular functions (those which are locally quotient of polynomials) over an algebraically closed field. One big difference I see between regular functions and $C^{\infty}$ functions is that $\mathcal{O}$ is not integral domain.","Is the ring $\mathcal{O}$ of germs of $C^{\infty}$ functions defined on the neighborhoods of $0\in\mathbb{R}$ the localization of the ring of $C^{\infty}$ functions on $\mathbb{R}$ at the maximal ideal $\mathfrak{m}$ of those functions which vanish at the origin? I can construct an injective map from $C^{\infty}_\mathfrak{m}$ to $\mathcal{O}$, but I'm having trouble showing surjectivity. I know that this is true for regular functions (those which are locally quotient of polynomials) over an algebraically closed field. One big difference I see between regular functions and $C^{\infty}$ functions is that $\mathcal{O}$ is not integral domain.",,"['real-analysis', 'abstract-algebra', 'analysis', 'algebraic-geometry']"
59,"How to prove that convergence is equivalent to pointwise convergence in $C[0,1]$ with the integral norm?",How to prove that convergence is equivalent to pointwise convergence in  with the integral norm?,"C[0,1]","I'm trying to prove (or disprove) that in the set $C[0,1]$ of continuous (bounded) functions on the real interval [0,1] with the integral norm $\|f(x)\|_1 = \int_0^1|f(x)|dx$ that a sequence of functions $f_n$ is convergent to $f$ if and only if $f_n$ is pointwise-convergent to $f$. My intuition tells me that this is true, but I'm having trouble formulating an attack (particularly for the convergence => pointwise convergence case). Given $\forall \epsilon > 0 \; \exists N \in \mathbb{N} \;$ $\forall n \geq N \; \int_0^1 |f_n(x) - f(x)|dx < \epsilon $, I'm trying to prove that for an arbitrary $x \in [0,1]$, $\forall \epsilon_0 > 0$ there is an $N_0$ such that for $n_0 \geq N_0 |f_{n_0}(x) - f(x)| < \epsilon_0$. I think I want to make $N_0$ somehow depend on the that integral (or a 'slice' of it), but I don't see it yet. Any pointers/advice/hints are welcome.","I'm trying to prove (or disprove) that in the set $C[0,1]$ of continuous (bounded) functions on the real interval [0,1] with the integral norm $\|f(x)\|_1 = \int_0^1|f(x)|dx$ that a sequence of functions $f_n$ is convergent to $f$ if and only if $f_n$ is pointwise-convergent to $f$. My intuition tells me that this is true, but I'm having trouble formulating an attack (particularly for the convergence => pointwise convergence case). Given $\forall \epsilon > 0 \; \exists N \in \mathbb{N} \;$ $\forall n \geq N \; \int_0^1 |f_n(x) - f(x)|dx < \epsilon $, I'm trying to prove that for an arbitrary $x \in [0,1]$, $\forall \epsilon_0 > 0$ there is an $N_0$ such that for $n_0 \geq N_0 |f_{n_0}(x) - f(x)| < \epsilon_0$. I think I want to make $N_0$ somehow depend on the that integral (or a 'slice' of it), but I don't see it yet. Any pointers/advice/hints are welcome.",,"['analysis', 'convergence-divergence', 'metric-spaces']"
60,Exercise 9.2 from Apostol's Mathematical Analysis book. Uniform convergence of product,Exercise 9.2 from Apostol's Mathematical Analysis book. Uniform convergence of product,,"This is a problem (Exercise 9.2) from Apostol's Mathematical Analysis (second edition) which I couldn't solve. $\bullet$ Define two sequences $\{f_{n}\}$ and $\{g_{n}\}$ as follows: $f_{n}(x) = x \Bigl(1 + \frac{1}{n}\Bigr)$ for $x \in \mathbb{R}$, $n \in \mathbb{N}$ $g_{n}(x)= \begin{cases}  \frac{1}{n} & x=0 \  \text{or}\  x \in \mathbb{R} - \mathbb{Q} \\  b+\frac{1}{n} & x \in \mathbb{Q},\ \text{say}\  x=\frac{a}{b}, b>0 \end{cases}$ $\mathbb{R}$ is the set of reals, $\mathbb{Q}$ is the set of rational and $\mathbb{R} - \mathbb{Q}$ is the set of irrationals. I have to show that if $h_{n}(x)=f_{n}(x)g_{n}(x)$, then $h_{n}(x)$ does not converge uniformly on any bounded interval. How can I show this?","This is a problem (Exercise 9.2) from Apostol's Mathematical Analysis (second edition) which I couldn't solve. $\bullet$ Define two sequences $\{f_{n}\}$ and $\{g_{n}\}$ as follows: $f_{n}(x) = x \Bigl(1 + \frac{1}{n}\Bigr)$ for $x \in \mathbb{R}$, $n \in \mathbb{N}$ $g_{n}(x)= \begin{cases}  \frac{1}{n} & x=0 \  \text{or}\  x \in \mathbb{R} - \mathbb{Q} \\  b+\frac{1}{n} & x \in \mathbb{Q},\ \text{say}\  x=\frac{a}{b}, b>0 \end{cases}$ $\mathbb{R}$ is the set of reals, $\mathbb{Q}$ is the set of rational and $\mathbb{R} - \mathbb{Q}$ is the set of irrationals. I have to show that if $h_{n}(x)=f_{n}(x)g_{n}(x)$, then $h_{n}(x)$ does not converge uniformly on any bounded interval. How can I show this?",,['real-analysis']
61,When is an infinite sum of polynomials analytic?,When is an infinite sum of polynomials analytic?,,"Consider the following function: $ f(x) = \sum_{i,j>=0} a_{ij} x^i (1-x)^j$ , where the coefficients satisfy $a_{ij} \in [0,1]$. Observe that $f(x)$ converges for $x\in [0,1]$. Is $f$ analytic over $[0,1]$? If not, how about over $[c,d]$ for $c,d \in (0,1)$? I think I can show that the answer is yes for the $[c,d]$ case by showing uniform convergence (EDIT: I was wrong... Not sure I can anymore). Is there a simpler way? More importantly, are there nice general conditions that guarantee that the infinite sum of polynomials is analytic over the interval (or a proper sub-interval) of convergence? This motivates the following questions: Question 1: Let $f(x) = \sum_{i,j} a_{ij} x^i (1-x)^j$, where there are no guarantees on $a_{ij}$. Assume $f$ converges on interval $[a,b]$.   Is $f(x)$ analytic over $[a,b]$? What about over $[c,d]$ where $c,d \in (a,b)$? Question 2: Let $f(x) = \sum_k p_k(x)$, where $p_k$ is a polynomial. Assume $f$ converges on interval $[a,b]$, and that $p_k(x) \geq 0$ for $x \in [a,b]$.   Is $f(x)$ analytic over $[a,b]$? What about over $[c,d]$ where $c,d \in (a,b)$? Question 3: Same as question 2, but there are no guarantees on the sign of $p_k(x)$ anymore. (EDIT: As Jonas Meyer pointed out, this is false by the Weierstrass approximation theorem) I guess what I'm trying to get at is understanding which natural conditions, short of manually showing uniform convergence, guarantee that a convergent sum of polynomials is analytic over its region of convergence (or a sub-interval of that). Question 1 starts with a very specific sum of polynomials that I somewhat understand, and Questions 2 and 3 are grasping at a generalization. These questions are my best guesses as to what generalizations may look like, though please feel free to suggest others if I'm off-base here.","Consider the following function: $ f(x) = \sum_{i,j>=0} a_{ij} x^i (1-x)^j$ , where the coefficients satisfy $a_{ij} \in [0,1]$. Observe that $f(x)$ converges for $x\in [0,1]$. Is $f$ analytic over $[0,1]$? If not, how about over $[c,d]$ for $c,d \in (0,1)$? I think I can show that the answer is yes for the $[c,d]$ case by showing uniform convergence (EDIT: I was wrong... Not sure I can anymore). Is there a simpler way? More importantly, are there nice general conditions that guarantee that the infinite sum of polynomials is analytic over the interval (or a proper sub-interval) of convergence? This motivates the following questions: Question 1: Let $f(x) = \sum_{i,j} a_{ij} x^i (1-x)^j$, where there are no guarantees on $a_{ij}$. Assume $f$ converges on interval $[a,b]$.   Is $f(x)$ analytic over $[a,b]$? What about over $[c,d]$ where $c,d \in (a,b)$? Question 2: Let $f(x) = \sum_k p_k(x)$, where $p_k$ is a polynomial. Assume $f$ converges on interval $[a,b]$, and that $p_k(x) \geq 0$ for $x \in [a,b]$.   Is $f(x)$ analytic over $[a,b]$? What about over $[c,d]$ where $c,d \in (a,b)$? Question 3: Same as question 2, but there are no guarantees on the sign of $p_k(x)$ anymore. (EDIT: As Jonas Meyer pointed out, this is false by the Weierstrass approximation theorem) I guess what I'm trying to get at is understanding which natural conditions, short of manually showing uniform convergence, guarantee that a convergent sum of polynomials is analytic over its region of convergence (or a sub-interval of that). Question 1 starts with a very specific sum of polynomials that I somewhat understand, and Questions 2 and 3 are grasping at a generalization. These questions are my best guesses as to what generalizations may look like, though please feel free to suggest others if I'm off-base here.",,"['real-analysis', 'analysis']"
62,The complex numbers inequality $(|a+tb|^p-|a|^p)/t \leq |a+b|^p-|a|^p$,The complex numbers inequality,(|a+tb|^p-|a|^p)/t \leq |a+b|^p-|a|^p,"The following inequality is from the proof that the $L^p$ norm is Gâteaux differentiable for $ 1 < p<\infty$ (from ""Analysis"" by Lieb and Loss). Let $a$, $b\in\mathbb{C}$ and $-1\leq t\leq 1$, $t\not=0.$ Then  $$|a|^p-|a-b|^p\leq\frac{1}{t}(|a+tb|^p-|a|^p) \leq |a+b|^p-|a|^p.$$ I managed to prove the second inequality for positive $t$ by writing $a+tb=(1-t)a+t(a+b)$and using the convexity of $\cdot^p$. From this the first inequality follows for negative $t$ by substituting $-b$ for $b$. The same trick would finish the proof, if I could prove either the second inequality for negative $t$ or the first inequality for positive $t$.","The following inequality is from the proof that the $L^p$ norm is Gâteaux differentiable for $ 1 < p<\infty$ (from ""Analysis"" by Lieb and Loss). Let $a$, $b\in\mathbb{C}$ and $-1\leq t\leq 1$, $t\not=0.$ Then  $$|a|^p-|a-b|^p\leq\frac{1}{t}(|a+tb|^p-|a|^p) \leq |a+b|^p-|a|^p.$$ I managed to prove the second inequality for positive $t$ by writing $a+tb=(1-t)a+t(a+b)$and using the convexity of $\cdot^p$. From this the first inequality follows for negative $t$ by substituting $-b$ for $b$. The same trick would finish the proof, if I could prove either the second inequality for negative $t$ or the first inequality for positive $t$.",,"['analysis', 'inequality']"
63,"Suppose $1\leq p_1<p_2<+\infty$ and $\mu$ a finite measure, then $\mathscr{L}^{p_2}(X,\mathscr{A},\mu)\subseteq\mathscr{L}^{p_1}(X,\mathscr{A},\mu)$","Suppose  and  a finite measure, then","1\leq p_1<p_2<+\infty \mu \mathscr{L}^{p_2}(X,\mathscr{A},\mu)\subseteq\mathscr{L}^{p_1}(X,\mathscr{A},\mu)","I need to prove the following result: Suppose $1\leq p_1<p_2<+\infty$ and $\mu$ a finite measure, then $\mathscr{L}^{p_2}(X,\mathscr{A},\mu)\subseteq\mathscr{L}^{p_1}(X,\mathscr{A},\mu)$ . Here is my attempt: Let $f\in\mathscr{L}^{p_2}(X,\mathscr{A},\mu)$ . Then $f$ is an $\mathscr{A}$ -measurable function such that $|f|^{p_2}$ is integrable. So $\int(|f|^{p_2})^+d\mu = \int|f|^{p_2} < +\infty$ . For $x$ in $X$ that $|f(x)|\geq1$ , we have $|f(x)|^{p_1}\leq|f(x)|^{p_2}$ . For $x$ in $X$ such that $0\leq|f(x)|<1$ , we have $|f(x)|^{p_1}<1$ . Define a function $g$ by letting \begin{align*} g(x) =  \begin{cases} f(x)\quad &\text{if $|f(x)|\geq1$},\\ \\ 1\quad &\text{if $0\leq|f(x)|<1$}. \end{cases} \end{align*} Then $|f(x)|^{p_1}\leq|g(x)|^{p_2}$ for all $x$ in $X$ . Define a function $h$ by letting \begin{align*} h(x) =  \begin{cases} f(x)\quad &\text{if $|f(x)|\geq1$},\\ \\ 0\quad &\text{if $0\leq|f(x)|<1$}. \end{cases} \end{align*} Then $g(x)=h(x)+\chi_{A}$ , where $A=\{x\in X:0\leq|f(x)|<1\}$ , and $|g(x)|^{p_2} = |h(x)|^{p_2} + \chi_{A}$ for all $x$ in $X$ . We prove that $|h|^{p_2}$ is $\mathscr{A}$ -measurable. For each $t\geq1$ , the set $\{x\in X:|h(x)|<t\} = \{x\in X:|f(x)|<t\}\in\mathscr{A}$ , because $|f|$ is integrable and so $\mathscr{A}$ -measurable. For each $t<1$ , the set $\{x\in X:|h(x)|<t\} = \{x\in X:|f(x)|<s\} \in \mathscr{A}$ for some $x\in\mathbb{R}$ . Thus, $|h|$ is $\mathscr{A}$ -measurable (and real-valued), and so $|h|^{p_2}$ is $\mathscr{A}$ -measurable. Since $|h|^{p_2}$ and $|f|^{p_2}$ are both nonnegative real-valued $\mathscr{A}$ -measurable functions such that $|h(x)|^{p_2}\leq|f(x)|^{p_2}$ holds at each $x$ in $X$ , it follows that $\int|h|^{p_2}d\mu \leq \int|f|^{p_2}d\mu < +\infty$ . So, $|h|^{P_2}$ is integrable. Since $\mu$ is a finite measure on the measurable space $(X,\mathscr{A})$ , it follows that $\int\chi_{A}d\mu = \mu(A) < +\infty$ , and so $\chi_{A}$ is integrable. Therefore, $int|g|^{p_2}d\mu = \int(|h|^{p_2}+\chi_{A})d\mu = \int|h|^{p_2}d\mu + \int\chi_{A}d\mu < +\infty$ . Since $|f|^{p_1}$ and $|g|^{p_2}$ are nonnegative real-valued $\mathscr{A}$ -measurable functions on $X$ such that $|f(x)|^{p_1}\leq|g(x)|^{p_2}$ holds at each $x$ in $X$ , we have \begin{align*} \int|f|^{p_1}d\mu \leq \int|g|^{p_2}d\mu < +\infty. \end{align*} Hence, $|f|^{p_1}$ is integrable, $f\in\mathscr{L}^{p_1}(X,\mathscr{A},\mu)$ . Is my attempt correct? I am unconfident about the part proving $|h|^{p_2}$ is $\mathscr{A}$ -measurable (e.g., ""for each $t<1$ , the set $\{x\in X:|h(x)|<t\} = \{x\in X:|f(x)|<s\} \in \mathscr{A}$ for some $x\in\mathbb{R}$ ""). Also I'm not sure if there are mistakes elsewhere. I would really appreciate it if someone could help me check my proof! Thank you very much! Definition $\quad$ Let $(X,\mathscr{A},\mu)$ be a measure space, and let $p$ satisfy $1\leq p<+\infty$ . Then $\mathscr{L}^p(X,\mathscr{A},\mu)$ is the set of all $\mathscr{A}$ -measurable functions $f$ such that $|f|^p$ is integrable. Reference: Example 3.3.5 from Measure Theory by Donald Cohn.","I need to prove the following result: Suppose and a finite measure, then . Here is my attempt: Let . Then is an -measurable function such that is integrable. So . For in that , we have . For in such that , we have . Define a function by letting Then for all in . Define a function by letting Then , where , and for all in . We prove that is -measurable. For each , the set , because is integrable and so -measurable. For each , the set for some . Thus, is -measurable (and real-valued), and so is -measurable. Since and are both nonnegative real-valued -measurable functions such that holds at each in , it follows that . So, is integrable. Since is a finite measure on the measurable space , it follows that , and so is integrable. Therefore, . Since and are nonnegative real-valued -measurable functions on such that holds at each in , we have Hence, is integrable, . Is my attempt correct? I am unconfident about the part proving is -measurable (e.g., ""for each , the set for some ""). Also I'm not sure if there are mistakes elsewhere. I would really appreciate it if someone could help me check my proof! Thank you very much! Definition Let be a measure space, and let satisfy . Then is the set of all -measurable functions such that is integrable. Reference: Example 3.3.5 from Measure Theory by Donald Cohn.","1\leq p_1<p_2<+\infty \mu \mathscr{L}^{p_2}(X,\mathscr{A},\mu)\subseteq\mathscr{L}^{p_1}(X,\mathscr{A},\mu) f\in\mathscr{L}^{p_2}(X,\mathscr{A},\mu) f \mathscr{A} |f|^{p_2} \int(|f|^{p_2})^+d\mu = \int|f|^{p_2} < +\infty x X |f(x)|\geq1 |f(x)|^{p_1}\leq|f(x)|^{p_2} x X 0\leq|f(x)|<1 |f(x)|^{p_1}<1 g \begin{align*}
g(x) = 
\begin{cases}
f(x)\quad &\text{if |f(x)|\geq1},\\
\\
1\quad &\text{if 0\leq|f(x)|<1}.
\end{cases}
\end{align*} |f(x)|^{p_1}\leq|g(x)|^{p_2} x X h \begin{align*}
h(x) = 
\begin{cases}
f(x)\quad &\text{if |f(x)|\geq1},\\
\\
0\quad &\text{if 0\leq|f(x)|<1}.
\end{cases}
\end{align*} g(x)=h(x)+\chi_{A} A=\{x\in X:0\leq|f(x)|<1\} |g(x)|^{p_2} = |h(x)|^{p_2} + \chi_{A} x X |h|^{p_2} \mathscr{A} t\geq1 \{x\in X:|h(x)|<t\} = \{x\in X:|f(x)|<t\}\in\mathscr{A} |f| \mathscr{A} t<1 \{x\in X:|h(x)|<t\} = \{x\in X:|f(x)|<s\} \in \mathscr{A} x\in\mathbb{R} |h| \mathscr{A} |h|^{p_2} \mathscr{A} |h|^{p_2} |f|^{p_2} \mathscr{A} |h(x)|^{p_2}\leq|f(x)|^{p_2} x X \int|h|^{p_2}d\mu \leq \int|f|^{p_2}d\mu < +\infty |h|^{P_2} \mu (X,\mathscr{A}) \int\chi_{A}d\mu = \mu(A) < +\infty \chi_{A} int|g|^{p_2}d\mu = \int(|h|^{p_2}+\chi_{A})d\mu = \int|h|^{p_2}d\mu + \int\chi_{A}d\mu < +\infty |f|^{p_1} |g|^{p_2} \mathscr{A} X |f(x)|^{p_1}\leq|g(x)|^{p_2} x X \begin{align*}
\int|f|^{p_1}d\mu \leq \int|g|^{p_2}d\mu < +\infty.
\end{align*} |f|^{p_1} f\in\mathscr{L}^{p_1}(X,\mathscr{A},\mu) |h|^{p_2} \mathscr{A} t<1 \{x\in X:|h(x)|<t\} = \{x\in X:|f(x)|<s\} \in \mathscr{A} x\in\mathbb{R} \quad (X,\mathscr{A},\mu) p 1\leq p<+\infty \mathscr{L}^p(X,\mathscr{A},\mu) \mathscr{A} f |f|^p","['real-analysis', 'analysis', 'measure-theory', 'solution-verification', 'proof-writing']"
64,Jensen's inequality: are there missing details from the hypothesis?,Jensen's inequality: are there missing details from the hypothesis?,,"I'm trying to prove Jensen's inequality for an exercise, that is: having $f:[0, 1] \to \mathbb{R}$ , continuous, I want to prove $e^{\int_0^1 f(x) dx} \leq \int_0^1 e^{f(x)} dx$ . I think that I got stuck in the proof because I tried to use the fact that $ax + b \leq e^x$ where the equality only holds at a tangent point $x^*$ , and due to this, applying the integral to both: $$\int_0^1 ax + b\ dx \leq \int_0^1 e^x dx$$ I then would replace $x \mapsto f(x)$ . But then I don't know how to proceed to prove that $\int_0^1 e^{f(x)}\ dx \geq e^{\int_0^1 f(x)\ dx}$ Also, those doubts arose: Is this true only for functions $f$ defined as $f:[0, 1] \to \mathbb{R}$ or even for a generic $f:[a, b]\to \mathbb{R}$ ? Does the continuity hypothesis of $f$ suffice? Wouldn't I need something more, like positivity of $f$ ? Or maybe even that $f$ should be an increasing function? Also I know that somehow, somewhere, I should take into account the convexity of $e^x$ , but it doesn't sound stable to me: $e^x$ is convex, ok. But only knowing $f$ is continuous, how can I say $e^{f(x)}$ is convex too?","I'm trying to prove Jensen's inequality for an exercise, that is: having , continuous, I want to prove . I think that I got stuck in the proof because I tried to use the fact that where the equality only holds at a tangent point , and due to this, applying the integral to both: I then would replace . But then I don't know how to proceed to prove that Also, those doubts arose: Is this true only for functions defined as or even for a generic ? Does the continuity hypothesis of suffice? Wouldn't I need something more, like positivity of ? Or maybe even that should be an increasing function? Also I know that somehow, somewhere, I should take into account the convexity of , but it doesn't sound stable to me: is convex, ok. But only knowing is continuous, how can I say is convex too?","f:[0, 1] \to \mathbb{R} e^{\int_0^1 f(x) dx} \leq \int_0^1 e^{f(x)} dx ax + b \leq e^x x^* \int_0^1 ax + b\ dx \leq \int_0^1 e^x dx x \mapsto f(x) \int_0^1 e^{f(x)}\ dx \geq e^{\int_0^1 f(x)\ dx} f f:[0, 1] \to \mathbb{R} f:[a, b]\to \mathbb{R} f f f e^x e^x f e^{f(x)}","['real-analysis', 'integration', 'analysis', 'solution-verification', 'proof-explanation']"
65,"Proof: If $f\in\mathscr{L}^{\infty}(X,\mathscr{A},\mu)$, then $\{x\in X:|f(x)|>\|f\|_{\infty}\}$ is locally $\mu$-null.","Proof: If , then  is locally -null.","f\in\mathscr{L}^{\infty}(X,\mathscr{A},\mu) \{x\in X:|f(x)|>\|f\|_{\infty}\} \mu","I am self-studying Measure Theory by Donald Cohn. When proving this statement: If $f\in\mathscr{L}^{\infty}(X,\mathscr{A},\mu)$ , then $\{x\in X:|f(x)|>\|f\|_{\infty}\}$ is locally $\mu$ -null. the book wrote: If $\{M_n\}$ is a nonincreasing sequence of real numbers such that $\|f\|_{\infty}=\lim_{n\to\infty}M_n$ and such that for each $n$ the set $\{x\in X:|f(x)|>M_n\}$ is locally $\mu$ -null, then the set $\{x\in X:|f(x)|>\|f\|_{\infty}\}$ is the union of the sets $\{x\in X:|f(x)|>M_n\}$ and so is locally $\mu$ -null. I can see that the union of a sequence of locally $\mu$ -null sets is locally $\mu$ -null. But my question is: How can we make sure that such a sequence $\mathbf{\{M_n\}}$ exists? Could someone please explain this for me? Thank you very much! Some related definitions: Definition $\quad$ A subset $B$ of $X$ is $\mu$ -negligible (or $\mu$ -null ) if there is a subset $A$ of $X$ such that $A\in\mathscr{A}$ , $B \subseteq A$ , and $\mu(A)=0$ . Definition $\quad$ Let $\mathscr{L}^{\infty}(X,\mathscr{A},\mu,\mathbb{R})$ be the set of all bounded real-valued $\mathscr{A}$ -measurable functions on $X$ , and let $\mathscr{L}^{\infty}(X,\mathscr{A},\mu,\mathbb{C})$ be the set of all bounded complex-valued $\mathscr{A}$ -measurable functions on $X$ . In discussions that are valid for both real- and complex-valued functions we will often use $\mathscr{L}^p(X,\mathscr{A},\mu)$ to represent either $\mathscr{L}^p(X,\mathscr{A},\mu,\mathbb{R})$ or $\mathscr{L}^p(X,\mathscr{A},\mu,\mathbb{C})$ . Definition $\quad$ A subset $N$ of $X$ is locally $\mu$ -null if for each set $A$ that belongs to $\mathscr{A}$ and satisfies $\mu(A)<+\infty$ the set $A\bigcap N$ is $\mu$ -null. A property of points of $X$ is said to hold \textit{locally almost everywhere} if the set of points at which it fails to hold is locally null. Definition $\quad$ We can define $\|\cdot\|_p$ in the case where $p=+\infty$ by letting $\|f\|_{\infty}$ be the infimum of those nonnegative numbers $M$ such that $\{x\in X:|f(x)|>M\}$ is locally $\mu$ -null. An Update Thanks for @kobe's answer! I would like to add one more thing for my own record: that the set $A$ is nonempty, where $A=\Big\{M\in\mathbb{R}_+: \{x\in X:|f(x)|>M\}\ \text{is locally $\mu$-null}\Big\}$ . This is because of the fact that $f\in\mathscr{L}^{\infty}$ , which means $f$ is bounded, which implies there is a real number $M$ such that $|f(x)|\leq M$ for all $x$ , which means the set $\{x\in X:|f(x)|>M\}=\emptyset$ and thus is $\mu$ -null and so is locally $\mu$ -null.","I am self-studying Measure Theory by Donald Cohn. When proving this statement: If , then is locally -null. the book wrote: If is a nonincreasing sequence of real numbers such that and such that for each the set is locally -null, then the set is the union of the sets and so is locally -null. I can see that the union of a sequence of locally -null sets is locally -null. But my question is: How can we make sure that such a sequence exists? Could someone please explain this for me? Thank you very much! Some related definitions: Definition A subset of is -negligible (or -null ) if there is a subset of such that , , and . Definition Let be the set of all bounded real-valued -measurable functions on , and let be the set of all bounded complex-valued -measurable functions on . In discussions that are valid for both real- and complex-valued functions we will often use to represent either or . Definition A subset of is locally -null if for each set that belongs to and satisfies the set is -null. A property of points of is said to hold \textit{locally almost everywhere} if the set of points at which it fails to hold is locally null. Definition We can define in the case where by letting be the infimum of those nonnegative numbers such that is locally -null. An Update Thanks for @kobe's answer! I would like to add one more thing for my own record: that the set is nonempty, where . This is because of the fact that , which means is bounded, which implies there is a real number such that for all , which means the set and thus is -null and so is locally -null.","f\in\mathscr{L}^{\infty}(X,\mathscr{A},\mu) \{x\in X:|f(x)|>\|f\|_{\infty}\} \mu \{M_n\} \|f\|_{\infty}=\lim_{n\to\infty}M_n n \{x\in X:|f(x)|>M_n\} \mu \{x\in X:|f(x)|>\|f\|_{\infty}\} \{x\in X:|f(x)|>M_n\} \mu \mu \mu \mathbf{\{M_n\}} \quad B X \mu \mu A X A\in\mathscr{A} B \subseteq A \mu(A)=0 \quad \mathscr{L}^{\infty}(X,\mathscr{A},\mu,\mathbb{R}) \mathscr{A} X \mathscr{L}^{\infty}(X,\mathscr{A},\mu,\mathbb{C}) \mathscr{A} X \mathscr{L}^p(X,\mathscr{A},\mu) \mathscr{L}^p(X,\mathscr{A},\mu,\mathbb{R}) \mathscr{L}^p(X,\mathscr{A},\mu,\mathbb{C}) \quad N X \mu A \mathscr{A} \mu(A)<+\infty A\bigcap N \mu X \quad \|\cdot\|_p p=+\infty \|f\|_{\infty} M \{x\in X:|f(x)|>M\} \mu A A=\Big\{M\in\mathbb{R}_+: \{x\in X:|f(x)|>M\}\ \text{is locally \mu-null}\Big\} f\in\mathscr{L}^{\infty} f M |f(x)|\leq M x \{x\in X:|f(x)|>M\}=\emptyset \mu \mu","['real-analysis', 'analysis', 'measure-theory', 'proof-explanation']"
66,Question About Function Integrability - Proposition 2.3.10 from Measury Theory by Donald Cohn,Question About Function Integrability - Proposition 2.3.10 from Measury Theory by Donald Cohn,,"I am self-studying measure theory using Measure Theory by Donald Cohn. The book makes the following definition: Definition $\quad$ Suppose that $f:X\to[-\infty,+\infty]$ is $\mathscr{A}$ -measurable and that $A\in\mathscr{A}$ . Then $f$ is integrable over $A$ if the function $f\chi_A$ is integrable, and in this case $\int_Afd\mu$ , the integral of $f$ over $A$ , is defined to be $\int f\chi_Ad\mu$ . I am confused by the proof the following proposition: Proposition $\quad$ 2.3.10 $\quad$ Let $(X,\mathscr{A},\mu)$ be a measure space, and let $f$ be a $[0,+\infty]$ -valued $\mathscr{A}$ -measurable function on $X$ . If $t$ is a positive real number and if $A_t$ is defined by $A_t=\{x\in X:f(x)\geq t\}$ , then \begin{align*}     \mu(A_t)\leq\frac{1}{t}\int_{A_t}fd\mu\leq\frac{1}{t}\int fd\mu. \end{align*} Proof $\quad$ The relation $0\leq t\chi_{A_t}\leq f\chi_{A_t}\leq f$ and part (c) of Proposition 2.3.4 imply that \begin{align*}     \int t\chi_{A_t}d\mu \leq \int_{A_t}fd\mu\leq\int fd\mu. \end{align*} Since $\int t\chi_{A_t} = t\mu(A_t)$ , the proposition follows. My question with this proof is what if $f\chi_{A_t}$ is not integrable? According to the definition, $\int f\chi_{A_t}d\mu = \int_{A_t}fd\mu$ is only defined when $f\chi_{A_t}$ integrable. However, in this proposition, $f$ is a nonnegative extended real-valued function, which means it can have value $+\infty$ at some point in $X$ . Then \begin{align*} \begin{split} &\int(f\chi_{A_t})^+d\mu\\ =\ &\int(f\chi_{A_t})d\mu\\ =\ &\sup\left\{\int gd\mu:g\ \text{is a nonnegative simple real-valued $\mathscr{A}$-measurable function on $X$}\\ \text{and $g\leq f\chi_{A_t}$}\right\}\\ =\ &\sup\left\{\sum_{i=1}^mb_i\mu(B_i):\text{$g$ is $\mathscr{A}$-measurable; $g=\sum_{i=1}^mb_i\chi_{B_i}\leq f\chi_{A_t}$;}\\ \text{and for all $i$ $b_i\in[0,+\infty)$, $B_i$'s are disjoint subsets of $X$ that belong to $\mathscr{A}$}\right\} \end{split} \end{align*} may have value $+\infty$ , and so that $f\chi_{A_t}$ is not integrable, and so $\int_{A_t}fd\mu$ is not defined. Am I missing anything? Or is the book wrong? I really appreciate any help! Background Information: Construction of the Integral Stage 1 $\quad$ We begin with the simple function. Let $(X,\mathscr{A})$ be a measurable space. We will denote by $\mathscr{S}$ the collection of all simple real-valued $\mathscr{A}$ -measurable functions on $X$ and by $\mathscr{S}_+$ the collection of nonnegative functions in $\mathscr{S}$ . Let $\mu$ be a measure on $(X,\mathscr{A})$ . If $f$ belongs to $\mathscr{S}_+$ and is given by $f = \sum_{i=1}^ma_i\chi_{A_i}$ where $a_1,\dots,a_m$ are nonnegative real numbers and $A_1,\dots,A_m$ are disjoint subsets of $X$ that belong to $\mathscr{A}$ , then $\int f d\mu$ , the integral of $f$ with respect to $\mu$ , is defined to be $\sum_{i=1}^ma_i\mu(A_i)$ (note that this sum is either a nonnegative real number or $+\infty$ ). Stage 2 $\quad$ As our next step, we define the integral of an arbitrary $[0,+\infty]$ -valued $\mathscr{A}$ -measurable function on $X$ . For such a function $f$ , let \begin{align*} \int fd\mu = \sup\left\{\int gd\mu:g\in\mathscr{S}_+\ \text{and}\ g\leq f\right\}. \end{align*} Stage 3 $\quad$ Finally, let $f$ be an arbitrary $[-\infty,+\infty]$ -valued $\mathscr{A}$ -measurable function on $X$ . If $\int f^+d\mu$ and $\int f^-d\mu$ are both finite, then $f$ is called integrable (or $\mu$ -integrable or summable ), and its integral $\int fd\mu$ is defined by \begin{align*} \int fd\mu = \int f^+d\mu - \int f^-d\mu. \end{align*} The integral of $f$ is said to exist if at least one of $\int f^+d\mu$ and $\int f^-d\mu$ is finite, and again in this case, $\int fd\mu$ is defined to be $\int f^+d\mu - \int f^-d\mu$ . In either case one sometimes writes $\int f(x)\mu(dx)$ or $\int f(x)d\mu(x)$ in place of $\int fd\mu$ .","I am self-studying measure theory using Measure Theory by Donald Cohn. The book makes the following definition: Definition Suppose that is -measurable and that . Then is integrable over if the function is integrable, and in this case , the integral of over , is defined to be . I am confused by the proof the following proposition: Proposition 2.3.10 Let be a measure space, and let be a -valued -measurable function on . If is a positive real number and if is defined by , then Proof The relation and part (c) of Proposition 2.3.4 imply that Since , the proposition follows. My question with this proof is what if is not integrable? According to the definition, is only defined when integrable. However, in this proposition, is a nonnegative extended real-valued function, which means it can have value at some point in . Then may have value , and so that is not integrable, and so is not defined. Am I missing anything? Or is the book wrong? I really appreciate any help! Background Information: Construction of the Integral Stage 1 We begin with the simple function. Let be a measurable space. We will denote by the collection of all simple real-valued -measurable functions on and by the collection of nonnegative functions in . Let be a measure on . If belongs to and is given by where are nonnegative real numbers and are disjoint subsets of that belong to , then , the integral of with respect to , is defined to be (note that this sum is either a nonnegative real number or ). Stage 2 As our next step, we define the integral of an arbitrary -valued -measurable function on . For such a function , let Stage 3 Finally, let be an arbitrary -valued -measurable function on . If and are both finite, then is called integrable (or -integrable or summable ), and its integral is defined by The integral of is said to exist if at least one of and is finite, and again in this case, is defined to be . In either case one sometimes writes or in place of .","\quad f:X\to[-\infty,+\infty] \mathscr{A} A\in\mathscr{A} f A f\chi_A \int_Afd\mu f A \int f\chi_Ad\mu \quad \quad (X,\mathscr{A},\mu) f [0,+\infty] \mathscr{A} X t A_t A_t=\{x\in X:f(x)\geq t\} \begin{align*}
    \mu(A_t)\leq\frac{1}{t}\int_{A_t}fd\mu\leq\frac{1}{t}\int fd\mu.
\end{align*} \quad 0\leq t\chi_{A_t}\leq f\chi_{A_t}\leq f \begin{align*}
    \int t\chi_{A_t}d\mu \leq \int_{A_t}fd\mu\leq\int fd\mu.
\end{align*} \int t\chi_{A_t} = t\mu(A_t) f\chi_{A_t} \int f\chi_{A_t}d\mu = \int_{A_t}fd\mu f\chi_{A_t} f +\infty X \begin{align*}
\begin{split}
&\int(f\chi_{A_t})^+d\mu\\
=\ &\int(f\chi_{A_t})d\mu\\
=\ &\sup\left\{\int gd\mu:g\ \text{is a nonnegative simple real-valued \mathscr{A}-measurable function on X}\\
\text{and g\leq f\chi_{A_t}}\right\}\\
=\ &\sup\left\{\sum_{i=1}^mb_i\mu(B_i):\text{g is \mathscr{A}-measurable; g=\sum_{i=1}^mb_i\chi_{B_i}\leq f\chi_{A_t};}\\
\text{and for all i b_i\in[0,+\infty), B_i's are disjoint subsets of X that belong to \mathscr{A}}\right\}
\end{split}
\end{align*} +\infty f\chi_{A_t} \int_{A_t}fd\mu \quad (X,\mathscr{A}) \mathscr{S} \mathscr{A} X \mathscr{S}_+ \mathscr{S} \mu (X,\mathscr{A}) f \mathscr{S}_+ f = \sum_{i=1}^ma_i\chi_{A_i} a_1,\dots,a_m A_1,\dots,A_m X \mathscr{A} \int f d\mu f \mu \sum_{i=1}^ma_i\mu(A_i) +\infty \quad [0,+\infty] \mathscr{A} X f \begin{align*}
\int fd\mu = \sup\left\{\int gd\mu:g\in\mathscr{S}_+\ \text{and}\ g\leq f\right\}.
\end{align*} \quad f [-\infty,+\infty] \mathscr{A} X \int f^+d\mu \int f^-d\mu f \mu \int fd\mu \begin{align*}
\int fd\mu = \int f^+d\mu - \int f^-d\mu.
\end{align*} f \int f^+d\mu \int f^-d\mu \int fd\mu \int f^+d\mu - \int f^-d\mu \int f(x)\mu(dx) \int f(x)d\mu(x) \int fd\mu","['real-analysis', 'integration', 'analysis', 'measure-theory', 'measurable-functions']"
67,The restriction of Lebesgue measure to the $\sigma$-algebra of Borel subsets of $\mathbb{R}$ is not complete.,The restriction of Lebesgue measure to the -algebra of Borel subsets of  is not complete.,\sigma \mathbb{R},"I am new to measure theory. I am confused by the following claim from Measure Theory by Donald Cohn (Section 1.5 Completeness and Regularity): Claim $\quad$ The restriction of Lebesgue measure to the $\sigma$ -algebra of Borel subsets of $\mathbb{R}$ is not complete. In this post, I will denote the Lebesgue outer measure by $\lambda^*$ . According to the textbook, Definition $\quad$ The restriction of Lebesgue outer measure on $\mathbb{R}$ (or on $\mathbb{R}^d$ ) to $\mathcal{B}(\mathbb{R})$ or to $\mathcal{B}(\mathbb{R}^d)$ is called Lebesgue measure and will be denoted by $\lambda$ . The restriction of Lebesgue outer measure on $\mathbb{R}$ (or on $\mathbb{R}^d$ ) to the collection of Lebesgue measurable subsets of $\mathbb{R}$ (or of $\mathbb{R}^d$ ) is also called Lebesgue measure and will be denoted by $\lambda$ as well. Definition $\quad$ Let $(X,\mathcal{A},\mu)$ be a measure space. The measure $\mu$ (or the measure space $(X,\mathcal{A},\mu)$ ) is complete if the relations $A\in\mathcal{A}$ , $\mu(A)=0$ , and $B \subseteq A$ together imply that $B\in\mathcal{A}$ . So, my understanding of this claim is the following: Let $(\mathbb{R},\mathcal{B}(\mathbb{R}),\lambda)$ be a measure space. Let $A$ be a Borel subset of $\mathbb{R}$ with $\lambda(A)=0$ . There exists a subset $B$ of $A$ such that $B \notin \mathcal{B}(\mathbb{R})$ . I saw that this post used a Vitali set as an example to show that such a set $B$ does exist. However, I am very confused by its explanation. Let $V$ be a Vitali set in $[0,1]$ . I know that, since $V$ is not measurable with respect to the Lebesgue outer measure $\lambda^*$ , it follows that $V \notin \mathcal{B}(\mathbb{R})$ . I think we would want to find a set $A$ such that $A \in \mathcal{B}(\mathbb{R})$ , $\lambda(A)=0$ , and $V \subseteq A$ . But what exactly is this $A$ in the example? In that post I mentioned above, it seems to me that it wanted to say that the Vitali set $V$ is of Lebesgue measure zero in $\mathbb{R}^2$ and itself is not a Borel subset of $\mathbb{R}^2$ . However, if $V$ is not even Lebesgue measurable (i.e., $V$ is not measurable with respect to the Lebesgue outer measure), wouldn't $\lambda(V)$ be not defined? In addition, I couldn't see how his example showed the claim is true, because there was no such set $A \subseteq \mathcal{B}(\mathbb{R})$ . Basically, I am completely lost. I would really appreciate it if someone could help me clarify it or present another example! As @LeeMosher pointed out. The claim should have been understood in the following way: Let $(\mathbb{R},\mathcal{B}(\mathbb{R}),\lambda)$ be a measure space. For some Borel subset $A$ of $\mathbb{R}$ with $\lambda(A) = 0$ , there exists a subset $B$ of $A$ where $B \notin \mathcal{B}(\mathbb{R})$ .","I am new to measure theory. I am confused by the following claim from Measure Theory by Donald Cohn (Section 1.5 Completeness and Regularity): Claim The restriction of Lebesgue measure to the -algebra of Borel subsets of is not complete. In this post, I will denote the Lebesgue outer measure by . According to the textbook, Definition The restriction of Lebesgue outer measure on (or on ) to or to is called Lebesgue measure and will be denoted by . The restriction of Lebesgue outer measure on (or on ) to the collection of Lebesgue measurable subsets of (or of ) is also called Lebesgue measure and will be denoted by as well. Definition Let be a measure space. The measure (or the measure space ) is complete if the relations , , and together imply that . So, my understanding of this claim is the following: Let be a measure space. Let be a Borel subset of with . There exists a subset of such that . I saw that this post used a Vitali set as an example to show that such a set does exist. However, I am very confused by its explanation. Let be a Vitali set in . I know that, since is not measurable with respect to the Lebesgue outer measure , it follows that . I think we would want to find a set such that , , and . But what exactly is this in the example? In that post I mentioned above, it seems to me that it wanted to say that the Vitali set is of Lebesgue measure zero in and itself is not a Borel subset of . However, if is not even Lebesgue measurable (i.e., is not measurable with respect to the Lebesgue outer measure), wouldn't be not defined? In addition, I couldn't see how his example showed the claim is true, because there was no such set . Basically, I am completely lost. I would really appreciate it if someone could help me clarify it or present another example! As @LeeMosher pointed out. The claim should have been understood in the following way: Let be a measure space. For some Borel subset of with , there exists a subset of where .","\quad \sigma \mathbb{R} \lambda^* \quad \mathbb{R} \mathbb{R}^d \mathcal{B}(\mathbb{R}) \mathcal{B}(\mathbb{R}^d) \lambda \mathbb{R} \mathbb{R}^d \mathbb{R} \mathbb{R}^d \lambda \quad (X,\mathcal{A},\mu) \mu (X,\mathcal{A},\mu) A\in\mathcal{A} \mu(A)=0 B \subseteq A B\in\mathcal{A} (\mathbb{R},\mathcal{B}(\mathbb{R}),\lambda) A \mathbb{R} \lambda(A)=0 B A B \notin \mathcal{B}(\mathbb{R}) B V [0,1] V \lambda^* V \notin \mathcal{B}(\mathbb{R}) A A \in \mathcal{B}(\mathbb{R}) \lambda(A)=0 V \subseteq A A V \mathbb{R}^2 \mathbb{R}^2 V V \lambda(V) A \subseteq \mathcal{B}(\mathbb{R}) (\mathbb{R},\mathcal{B}(\mathbb{R}),\lambda) A \mathbb{R} \lambda(A) = 0 B A B \notin \mathcal{B}(\mathbb{R})","['real-analysis', 'analysis', 'measure-theory', 'proof-explanation', 'lebesgue-measure']"
68,Question About The Remark after Proposition 1.4.11 from Measure Theory by Donold Cohn,Question About The Remark after Proposition 1.4.11 from Measure Theory by Donold Cohn,,"My Question Define subsets $G$ , $G_0$ , and $G_1$ of $\mathbb{R}$ by \begin{align*}     G &= \{x:x=r+n\sqrt{2}\ \text{for some $r$ in $\mathbb{Q}$ and $n$ in $\mathbb{Z}$}\},\\     G_0 &= \{x:x=r+2n\sqrt{2}\ \text{for some $r$ in $\mathbb{Q}$ and $n$ in $\mathbb{Z}$}\},\ \text{and}\\     G_1 &= \{x:x=r+(2n+1)\sqrt{2}\ \text{for some $r$ in $\mathbb{Q}$ and $n$ in $\mathbb{Z}$}\}. \end{align*} Define a relation $\sim$ on $\mathbb{R}$ by letting $x \sim y$ hold when $x-y\in G$ ; the relation $\sim$ is then an equivalence relation on $\mathbb{R}$ . Use the axiom of choice to form a subset $E$ of $\mathbb{R}$ that contains exactly one representative of each equivalence class of $\sim$ . Let $A = E + G_0$ (that is, let $A$ consist of the points that have the form $e+g_0$ for some $e$ in $E$ and some $g_0$ in $G_0$ ). I got confused by the book's remark that ""the set $A$ defined above is not Lebesgue measurable: if it were, then both $A$ and $A^c$ would include (in fact, would be) Lebesgue measurable sets of positive Lebesgue measure"". Could someone please help me explain why this is true? Background Information The above remark is made after the follow proposition: Proposition 1.4.11 $\quad$ There is a subset $A$ of $\mathbb{R}$ such that each Lebesgue measurable set that is included in $A$ or in $A^c$ has Lebesgue measure zero. Proof $\quad$ Define subsets $G$ , $G_0$ , and $G_1$ of $\mathbb{R}$ by \begin{align*}     G &= \{x:x=r+n\sqrt{2}\ \text{for some $r$ in $\mathbb{Q}$ and $n$ in $\mathbb{Z}$}\},\\     G_0 &= \{x:x=r+2n\sqrt{2}\ \text{for some $r$ in $\mathbb{Q}$ and $n$ in $\mathbb{Z}$}\},\ \text{and}\\     G_1 &= \{x:x=r+(2n+1)\sqrt{2}\ \text{for some $r$ in $\mathbb{Q}$ and $n$ in $\mathbb{Z}$}\}. \end{align*} One can prove that $G$ and $G_0$ are subgroups of $\mathbb{R}$ (under addition), and $G_0$ and $G_1$ are disjoint, that $G_1 = G_0 + \sqrt{2}$ , and that $G = G_0 \bigcup G_1$ . Define a relation $\sim$ on $\mathbb{R}$ by letting $x \sim y$ hold when $x-y\in G$ ; the relation $\sim$ is then an equivalence relation on $\mathbb{R}$ . Use the axiom of choice to form a subset $E$ of $\mathbb{R}$ that contains exactly one representative of each equivalence class of $\sim$ . Let $A = E + G_0$ (that is, let $A$ consist of the points that have the form $e+g_0$ for some $e$ in $E$ and some $g_0$ in $G_0$ ). We now show that there does note exist a Lebesgue measurable subset $B$ of $A$ such that $\lambda(B)>0$ . For this let us assume that such a set exists; we will derive a contradiction. Proposition 1.4.10 implies that there is an interval $(-\epsilon,\epsilon)$ that is included in $\text{diff}(B)$ and hence in $\text{diff}(A)$ . Since $G_1$ is dense in $\mathbb{R}$ , it meets the interval $(-\epsilon,\epsilon)$ and hence meets $\text{diff}(A)$ . This, however, is impossible, since each element of $\text{diff}(A)$ is of the form $e_1-e_2+g_0$ (where $e_1$ and $e_2$ belong to $E$ and $g_0$ belongs to $G_0$ ) and so cannot belong to $G_1$ (the relation $e_1-e_2+g_0=g_1$ would imply that $e_1=e_2$ and $g_0=g_1$ , contradicting the disjointness of $G_0$ and $G_1$ ). This completes our proof that every Lebesgue measurable subset of $A$ must have Lebesgue measure zero. One can check that $A^c = E + G_1$ and hence that $A^c = A + \sqrt{2}$ . It follows that each Lebesgue measurable subset of $A^c$ is of the form $B+\sqrt{2}$ for some Lebesgue measurable subset $B$ of $A$ . Since $A$ has no Lebesgue measurable subsets of positive measure, it follows that $A^c$ also has no such subsets, and with this the proof is complete. The definition of $\text{diff}$ is the following: Definition $\quad$ Let $A$ be a subset of $\mathbb{R}$ . Then $\text{diff}(A)$ is the subset of $\mathbb{R}$ defined by \begin{align*} \text{diff}(A) = \{x-y:x \in A\ \text{and}\ y \in A\}. \end{align*} Proposition 1.4.10 is the following: Proposition 1.4.10 $\quad$ Let $A$ be a Lebesgue measurable subset of $\mathbb{R}$ such that $\lambda(A) > 0$ . Then $\text{diff}(A)$ includes an open interval that contains 0. Any help will be really appreciated!","My Question Define subsets , , and of by Define a relation on by letting hold when ; the relation is then an equivalence relation on . Use the axiom of choice to form a subset of that contains exactly one representative of each equivalence class of . Let (that is, let consist of the points that have the form for some in and some in ). I got confused by the book's remark that ""the set defined above is not Lebesgue measurable: if it were, then both and would include (in fact, would be) Lebesgue measurable sets of positive Lebesgue measure"". Could someone please help me explain why this is true? Background Information The above remark is made after the follow proposition: Proposition 1.4.11 There is a subset of such that each Lebesgue measurable set that is included in or in has Lebesgue measure zero. Proof Define subsets , , and of by One can prove that and are subgroups of (under addition), and and are disjoint, that , and that . Define a relation on by letting hold when ; the relation is then an equivalence relation on . Use the axiom of choice to form a subset of that contains exactly one representative of each equivalence class of . Let (that is, let consist of the points that have the form for some in and some in ). We now show that there does note exist a Lebesgue measurable subset of such that . For this let us assume that such a set exists; we will derive a contradiction. Proposition 1.4.10 implies that there is an interval that is included in and hence in . Since is dense in , it meets the interval and hence meets . This, however, is impossible, since each element of is of the form (where and belong to and belongs to ) and so cannot belong to (the relation would imply that and , contradicting the disjointness of and ). This completes our proof that every Lebesgue measurable subset of must have Lebesgue measure zero. One can check that and hence that . It follows that each Lebesgue measurable subset of is of the form for some Lebesgue measurable subset of . Since has no Lebesgue measurable subsets of positive measure, it follows that also has no such subsets, and with this the proof is complete. The definition of is the following: Definition Let be a subset of . Then is the subset of defined by Proposition 1.4.10 is the following: Proposition 1.4.10 Let be a Lebesgue measurable subset of such that . Then includes an open interval that contains 0. Any help will be really appreciated!","G G_0 G_1 \mathbb{R} \begin{align*}
    G &= \{x:x=r+n\sqrt{2}\ \text{for some r in \mathbb{Q} and n in \mathbb{Z}}\},\\
    G_0 &= \{x:x=r+2n\sqrt{2}\ \text{for some r in \mathbb{Q} and n in \mathbb{Z}}\},\ \text{and}\\
    G_1 &= \{x:x=r+(2n+1)\sqrt{2}\ \text{for some r in \mathbb{Q} and n in \mathbb{Z}}\}.
\end{align*} \sim \mathbb{R} x \sim y x-y\in G \sim \mathbb{R} E \mathbb{R} \sim A = E + G_0 A e+g_0 e E g_0 G_0 A A A^c \quad A \mathbb{R} A A^c \quad G G_0 G_1 \mathbb{R} \begin{align*}
    G &= \{x:x=r+n\sqrt{2}\ \text{for some r in \mathbb{Q} and n in \mathbb{Z}}\},\\
    G_0 &= \{x:x=r+2n\sqrt{2}\ \text{for some r in \mathbb{Q} and n in \mathbb{Z}}\},\ \text{and}\\
    G_1 &= \{x:x=r+(2n+1)\sqrt{2}\ \text{for some r in \mathbb{Q} and n in \mathbb{Z}}\}.
\end{align*} G G_0 \mathbb{R} G_0 G_1 G_1 = G_0 + \sqrt{2} G = G_0 \bigcup G_1 \sim \mathbb{R} x \sim y x-y\in G \sim \mathbb{R} E \mathbb{R} \sim A = E + G_0 A e+g_0 e E g_0 G_0 B A \lambda(B)>0 (-\epsilon,\epsilon) \text{diff}(B) \text{diff}(A) G_1 \mathbb{R} (-\epsilon,\epsilon) \text{diff}(A) \text{diff}(A) e_1-e_2+g_0 e_1 e_2 E g_0 G_0 G_1 e_1-e_2+g_0=g_1 e_1=e_2 g_0=g_1 G_0 G_1 A A^c = E + G_1 A^c = A + \sqrt{2} A^c B+\sqrt{2} B A A A^c \text{diff} \quad A \mathbb{R} \text{diff}(A) \mathbb{R} \begin{align*}
\text{diff}(A) = \{x-y:x \in A\ \text{and}\ y \in A\}.
\end{align*} \quad A \mathbb{R} \lambda(A) > 0 \text{diff}(A)","['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure', 'outer-measure']"
69,The positive Laplacian is indeed the negative Laplacian,The positive Laplacian is indeed the negative Laplacian,,"I know this question sounds like a joke. And it probably is:). I found it kind of annoying, but also interesting, to call $-\Delta=-\sum_{j=1}^n\partial^2_{jj}$ ""the positive Laplacian"" as it is the positive operator with respect to standard $L^2$ -pairing. But in the mean time it is also regarded as the ""negative of Laplacian"" since we have a minus sign. I think there are two different questions related to this topic. What is a more clear way to explain the difference on what positive and negative is for a Laplacian? And also how to use such terminologies appropriately in different circumstances. I believe such clarification is important, as the Hodge-Laplacian $\Box=dd^*+d^*d$ (let me not using $\Delta$ here) can be called a ""positive Laplacian"" with slightly less ambiguity. Also the word ""subharmonic functions"" refers to those $f$ such that $-\Delta f\le0$ , but not $\Delta f\le0$ . Are other similar (but also interesting) confusions happen in other fields of mathematics? If I remember correctly, the meaning of covariant tensors and contravariant tensors in commutative algebra and in differential geometry are flip. They seem less ""annoying"" than Laplacian though.","I know this question sounds like a joke. And it probably is:). I found it kind of annoying, but also interesting, to call ""the positive Laplacian"" as it is the positive operator with respect to standard -pairing. But in the mean time it is also regarded as the ""negative of Laplacian"" since we have a minus sign. I think there are two different questions related to this topic. What is a more clear way to explain the difference on what positive and negative is for a Laplacian? And also how to use such terminologies appropriately in different circumstances. I believe such clarification is important, as the Hodge-Laplacian (let me not using here) can be called a ""positive Laplacian"" with slightly less ambiguity. Also the word ""subharmonic functions"" refers to those such that , but not . Are other similar (but also interesting) confusions happen in other fields of mathematics? If I remember correctly, the meaning of covariant tensors and contravariant tensors in commutative algebra and in differential geometry are flip. They seem less ""annoying"" than Laplacian though.",-\Delta=-\sum_{j=1}^n\partial^2_{jj} L^2 \Box=dd^*+d^*d \Delta f -\Delta f\le0 \Delta f\le0,"['calculus', 'analysis', 'education', 'laplacian']"
70,Proving $e^{-\mu}\left(\left(\frac{e}{1+\delta}\right)^{(1+\delta)\mu}+\left(\frac{e}{1-\delta}\right)^{(1-\delta)\mu}\right) \le 2e^{-C\mu\delta^2}$ [duplicate],Proving  [duplicate],e^{-\mu}\left(\left(\frac{e}{1+\delta}\right)^{(1+\delta)\mu}+\left(\frac{e}{1-\delta}\right)^{(1-\delta)\mu}\right) \le 2e^{-C\mu\delta^2},"This question already has an answer here : Solving exercise 2.3.5 in Vershynin's HDP book with the best choice of c (1 answer) Closed 4 months ago . I'm trying to show that $$e^{-\mu}\left(\left(\frac{e}{1+\delta}\right)^{(1+\delta)\mu} + \left(\frac{e}{1-\delta}\right)^{(1-\delta)\mu}\right) \le 2e^{-C\mu\delta^2},$$ for an absolute constant $C > 0$ , where $\delta \in (0,1]$ and $\mu > 0$ . Using the idea in this answer , I wrote $$\log(1+\delta) = 0 + \delta \frac{1}{1+0} - \frac{\delta^2}{(1+\delta')^2},$$ for some $\delta' \in (0, \delta).$ This gives $(1+\delta')^2 \le 4$ , i.e., $- \frac{\delta^2}{(1+\delta')^2} \le -\frac{\delta^2}{4}$ . Plugging back in, we have the upper bound $$\log(1+\delta) = \delta - \frac{\delta^2}{(1+\delta')^2} \le \delta - \frac{\delta^2}{4}.$$ This doesn't immediately seem helpful, since we are looking for a bound of the form $$\log(1+\delta) \ge \frac{\delta + C\delta^2}{1+ \delta},$$ i.e., a lower bound. Could I get a hint? Thanks!","This question already has an answer here : Solving exercise 2.3.5 in Vershynin's HDP book with the best choice of c (1 answer) Closed 4 months ago . I'm trying to show that for an absolute constant , where and . Using the idea in this answer , I wrote for some This gives , i.e., . Plugging back in, we have the upper bound This doesn't immediately seem helpful, since we are looking for a bound of the form i.e., a lower bound. Could I get a hint? Thanks!","e^{-\mu}\left(\left(\frac{e}{1+\delta}\right)^{(1+\delta)\mu} + \left(\frac{e}{1-\delta}\right)^{(1-\delta)\mu}\right) \le 2e^{-C\mu\delta^2}, C > 0 \delta \in (0,1] \mu > 0 \log(1+\delta) = 0 + \delta \frac{1}{1+0} - \frac{\delta^2}{(1+\delta')^2}, \delta' \in (0, \delta). (1+\delta')^2 \le 4 - \frac{\delta^2}{(1+\delta')^2} \le -\frac{\delta^2}{4} \log(1+\delta) = \delta - \frac{\delta^2}{(1+\delta')^2} \le \delta - \frac{\delta^2}{4}. \log(1+\delta) \ge \frac{\delta + C\delta^2}{1+ \delta},","['analysis', 'proof-explanation']"
71,Limit as $x\to \infty$ of a function given in integral form,Limit as  of a function given in integral form,x\to \infty,"Consider $F(x) = \int_0^{x^2} \frac{t^2+1}{t^4+2t^2+4}dt$ . I want to study $$ \lim_{x\to\infty} F(x), \quad \lim_{x\to\infty} \frac{F(x)}{x} $$ Ideas: First, we can observe than $f(t) = \frac{t^2+1}{t^4+2t^2+4}$ is a continuous function over $\mathbb{R}$ , so $G(x) = \int_0^{x} \frac{t^2+1}{t^4+2t^2+4}dt $ is well defined, is continuous, differentiable and $F'=f$ . Is clear that $F = G \circ g$ , being $g(x) = x^2$ , so $F'$ is also differentiable and we can easily compute its derivative using the Chain Rule. If I find that $\lim_{x\to\infty} F(x) = \infty$ , then I could apply L'Hôpital Rule and study the limit of $F'(x)$ , which is $0$ . Can you help me with $\lim_{x\to\infty} F(x)$ ? I'm stuck with this part.","Consider . I want to study Ideas: First, we can observe than is a continuous function over , so is well defined, is continuous, differentiable and . Is clear that , being , so is also differentiable and we can easily compute its derivative using the Chain Rule. If I find that , then I could apply L'Hôpital Rule and study the limit of , which is . Can you help me with ? I'm stuck with this part.","F(x) = \int_0^{x^2} \frac{t^2+1}{t^4+2t^2+4}dt 
\lim_{x\to\infty} F(x), \quad \lim_{x\to\infty} \frac{F(x)}{x}
 f(t) = \frac{t^2+1}{t^4+2t^2+4} \mathbb{R} G(x) = \int_0^{x} \frac{t^2+1}{t^4+2t^2+4}dt  F'=f F = G \circ g g(x) = x^2 F' \lim_{x\to\infty} F(x) = \infty F'(x) 0 \lim_{x\to\infty} F(x)","['real-analysis', 'calculus', 'analysis', 'definite-integrals']"
72,Quantitative bound on Wasserstein distances by $L^p$ distances?,Quantitative bound on Wasserstein distances by  distances?,L^p,"Given two smooth probability densities $f$ and $g$ on $\mathbb{R}$ (or $\mathbb{R}_+$ ) with finite $p$ -th moments. I am wondering if anyone is aware of some explicit upper bound on $W_p(f,g)$ in terms of $\|f-g\|_{L^p}$ (especially for $p=1$ and $p=2$ ) ? As one typically view Wasserstein distances as ""weak"" metrics and $L^p$ metrics as ""strong"" metrics. It is very natural to search for some explicit bound on $W_p(f,g)$ in terms of $\|f-g\|_{L^p}$ . However, I did not find any useful references in this regard. Any help or pointers to literatures are greatly appreciated! Remark: It is well-known that $W_1(f,g)$ has a nice formulation in terms of cumulative distribution functions (while it seems that $W_2$ has no such nice representation). Indeed, let $F$ and $G$ represent the cumulative distribution functions corresponding to the densities $f$ and $g$ , respectively. Then we have $$W_1(f,g) = \int |F(x)-G(x)| \mathrm{d}x$$ So in some sense, $W_1(f,g)$ looks like some $L^1$ norm (except that we are using the cdf's instead the pdf's). I am very curious as to why $$\int |F(x)-G(x)| \mathrm{d}x$$ can be somehow controlled by $$\int |f(x)-g(x)| \mathrm{d}x,$$ and if possible, I would like to see an explicit bound.","Given two smooth probability densities and on (or ) with finite -th moments. I am wondering if anyone is aware of some explicit upper bound on in terms of (especially for and ) ? As one typically view Wasserstein distances as ""weak"" metrics and metrics as ""strong"" metrics. It is very natural to search for some explicit bound on in terms of . However, I did not find any useful references in this regard. Any help or pointers to literatures are greatly appreciated! Remark: It is well-known that has a nice formulation in terms of cumulative distribution functions (while it seems that has no such nice representation). Indeed, let and represent the cumulative distribution functions corresponding to the densities and , respectively. Then we have So in some sense, looks like some norm (except that we are using the cdf's instead the pdf's). I am very curious as to why can be somehow controlled by and if possible, I would like to see an explicit bound.","f g \mathbb{R} \mathbb{R}_+ p W_p(f,g) \|f-g\|_{L^p} p=1 p=2 L^p W_p(f,g) \|f-g\|_{L^p} W_1(f,g) W_2 F G f g W_1(f,g) = \int |F(x)-G(x)| \mathrm{d}x W_1(f,g) L^1 \int |F(x)-G(x)| \mathrm{d}x \int |f(x)-g(x)| \mathrm{d}x,","['probability', 'analysis', 'metric-spaces', 'normed-spaces', 'wasserstein']"
73,What is wrong with this proof of the Cauchy mean value theorem? [duplicate],What is wrong with this proof of the Cauchy mean value theorem? [duplicate],,"This question already has an answer here : Why is this a fake proof? Cauchy's Mean Value Theorem (1 answer) Closed 7 months ago . What is wrong with this proof of the Cauchy mean value theorem? False Proof. The mean value theorem applies to $f$ on $[a, b]$ , so there exists $c \in (a, b)$ with $$ f'(c) = \frac{f(b)−f(a)}{b−a}. $$ The mean value theorem also applies to $g$ on $[a, b]$ , so there exists $c \in (a, b)$ with $$ g'(c) = \frac{g(b)−g(a)}{b−a}. $$ Thus, $$ \frac{f'(c)}{g'(c)}  = \frac{f(b) − f(a)}{b−a}  \cdot \frac{b-a}{g(b) − g(a)}  = \frac{f(b) − f(a)}{g(b) − g(a)}. $$ I think this may be false as it is assumed that both functions $f$ and $g$ have a common point $c$ such that this works, but I'm not sure. Any help? Thanks.","This question already has an answer here : Why is this a fake proof? Cauchy's Mean Value Theorem (1 answer) Closed 7 months ago . What is wrong with this proof of the Cauchy mean value theorem? False Proof. The mean value theorem applies to on , so there exists with The mean value theorem also applies to on , so there exists with Thus, I think this may be false as it is assumed that both functions and have a common point such that this works, but I'm not sure. Any help? Thanks.","f [a, b] c \in (a, b) 
f'(c) = \frac{f(b)−f(a)}{b−a}.
 g [a, b] c \in (a, b) 
g'(c) = \frac{g(b)−g(a)}{b−a}.
 
\frac{f'(c)}{g'(c)} 
= \frac{f(b) − f(a)}{b−a} 
\cdot \frac{b-a}{g(b) − g(a)} 
= \frac{f(b) − f(a)}{g(b) − g(a)}.
 f g c","['real-analysis', 'calculus', 'analysis', 'solution-verification', 'proof-explanation']"
74,"Suppose $f,g$ are polynomials with degree $\leq d$. If there exist $C$ and $c$ such that $|f-g|\leq C|x|^{d+1}$ for any $|x|<c$. Do we have $f=g$?",Suppose  are polynomials with degree . If there exist  and  such that  for any . Do we have ?,"f,g \leq d C c |f-g|\leq C|x|^{d+1} |x|<c f=g","I guess the answer is yes, and here's my sketch of proof, but I'm not sure my argument is correct or not. (Since I didn't use any information about $c$ .) proof: Let $f=\sum_{i=0}^{d} a_{i}x^{i}$ and $g=\sum_{i=0}^{d} b_{i}x^{i}$ . Suppose that $f\neq g$ , i.e., there is some $a_{i}\neq b_{i}$ . By assumption, we have $$C\geq \dfrac{|f(x)-g(x)|}{|x|^{d+1}}\geq\sum_{i=0}^{d}|a_{i}-b_{i}|\dfrac{1}{|x|^{d-i+1}}\geq \dfrac{|a_{i}-b_{i}|}{|x|^{d-i+1}}$$ However, as $x\to 0$ , RHS $\to\infty$ and LHS $=C$ , which contradicts. Therefore $a_{i}=b_{i}$ for all $i$ , and so $f=g$ .","I guess the answer is yes, and here's my sketch of proof, but I'm not sure my argument is correct or not. (Since I didn't use any information about .) proof: Let and . Suppose that , i.e., there is some . By assumption, we have However, as , RHS and LHS , which contradicts. Therefore for all , and so .",c f=\sum_{i=0}^{d} a_{i}x^{i} g=\sum_{i=0}^{d} b_{i}x^{i} f\neq g a_{i}\neq b_{i} C\geq \dfrac{|f(x)-g(x)|}{|x|^{d+1}}\geq\sum_{i=0}^{d}|a_{i}-b_{i}|\dfrac{1}{|x|^{d-i+1}}\geq \dfrac{|a_{i}-b_{i}|}{|x|^{d-i+1}} x\to 0 \to\infty =C a_{i}=b_{i} i f=g,"['real-analysis', 'analysis', 'polynomials']"
75,"Is the set of polynomials of odd degree dense in $\mathcal{C}^{0}([a,b])$?",Is the set of polynomials of odd degree dense in ?,"\mathcal{C}^{0}([a,b])","Let $$E=\{\sum\limits_{k=0}^{n}a_{k}\,x^{2k+1}\, | \, n\in\mathbb{N}\cup\{0\}\}$$ be the set of polynomials of odd degree in each term defined on $[1,2]$ . (a) Show that $E$ is not closed in $\mathcal{C}^{0}([1,2])$ , where $\mathcal{C}^{0}([1,2])$ is the space of continuous function from $[1,2]$ to $\mathbb{R}$ with sup norm. (b) Is $E$ dense in $\mathcal{C}^{0}([1,2])$ ? Here's my proof: (a) Consider the partial sum of the taylor series of $\sin(x)$ , then every partial sum are in $E$ obviously, i.e., there is a sequence $\{f_{i}(x)\}$ of $E$ , However, the limit of the sequence $f(x)=\sin(x)$ is not in $E$ . This shows that $E$ is not closed. (b) I've tried to prove it using Stone-Weierstrass Theorem, but the theorem only states the polynomial space $\mathbb{R}[x]$ dense in $\mathcal{C}^{0}([a,b])$ , i.e., $$\forall \epsilon >0 \mbox{ and } \forall f(x)\in\mathcal{C}^{0}([a,b]), \exists \, p(x)\in\mathbb{R}[x] \mbox{ s.t. } \|f(x)-p(x)\|_{\infty}<\epsilon.$$ Thus I want to use triangle inequality to find a polynomial with odd degree $p'(x)$ such that $\|p(x)-p'(x)\|<\epsilon$ , but it is impossible since not every polynomial function is the odd function. If you control the distance of two polynomials when $x>0$ , another side must have large distance. So I'm stuck, and I guess the answer is NOT. Can anyone give me some hints? Thanks!","Let be the set of polynomials of odd degree in each term defined on . (a) Show that is not closed in , where is the space of continuous function from to with sup norm. (b) Is dense in ? Here's my proof: (a) Consider the partial sum of the taylor series of , then every partial sum are in obviously, i.e., there is a sequence of , However, the limit of the sequence is not in . This shows that is not closed. (b) I've tried to prove it using Stone-Weierstrass Theorem, but the theorem only states the polynomial space dense in , i.e., Thus I want to use triangle inequality to find a polynomial with odd degree such that , but it is impossible since not every polynomial function is the odd function. If you control the distance of two polynomials when , another side must have large distance. So I'm stuck, and I guess the answer is NOT. Can anyone give me some hints? Thanks!","E=\{\sum\limits_{k=0}^{n}a_{k}\,x^{2k+1}\, | \, n\in\mathbb{N}\cup\{0\}\} [1,2] E \mathcal{C}^{0}([1,2]) \mathcal{C}^{0}([1,2]) [1,2] \mathbb{R} E \mathcal{C}^{0}([1,2]) \sin(x) E \{f_{i}(x)\} E f(x)=\sin(x) E E \mathbb{R}[x] \mathcal{C}^{0}([a,b]) \forall \epsilon >0 \mbox{ and } \forall f(x)\in\mathcal{C}^{0}([a,b]), \exists \, p(x)\in\mathbb{R}[x] \mbox{ s.t. } \|f(x)-p(x)\|_{\infty}<\epsilon. p'(x) \|p(x)-p'(x)\|<\epsilon x>0","['analysis', 'polynomials', 'dense-subspaces']"
76,"When picking a delta to show a function is continuous, does that delta need to rely on epsilon?","When picking a delta to show a function is continuous, does that delta need to rely on epsilon?",,"Generally, when we want to show continuity, we are given a small $\varepsilon$ and we pick a $\delta$ according to this $\varepsilon.$ Heuristically, we can consider this $\delta$ to be a function of $\varepsilon,$ which must decrease as $\varepsilon$ decreases, for example with the function $x \mapsto x$ , we have $\delta(\varepsilon) = \varepsilon.$ I also see that with a constant function, we can pick $\delta$ to be any number we like, so in a way our function $\delta$ does not depend on $\varepsilon.$ My question is: Let $(X,d_x),(Y,d_Y)$ be metric spaces. If $f: X \to Y$ is a non-constant continuous function, must $\delta$ depend on $\varepsilon$ ? I can see that if $X$ is infinite the answer is yes. Say we choose a $\delta>0$ , and pick $x,x' \in X : d_X(x,x') < \delta,$ we can pick $\varepsilon  = d_Y(f(x),f(x')),$ which means that this $\delta$ does not work for this $\varepsilon.$ I think the infinite set means that we can always pick $x,x'$ such that $f(x)\ne f(x')$ , and $d_x(x,x')<\delta.$ I see that this argument does not work does not work if $X$ is finite though, as we can pick $\delta = \min_{x,x' \in X}\{d_X(x,x')\},$ so I worry that there is some weird finite set where we can pick a $\delta$ that works for all $\varepsilon.$ I know that thinking of $\delta$ as a function of $\varepsilon$ in this way isn't necessarily accurate, as it may imply that only one $\delta$ works for a $\varepsilon$ , which is not true, but this isn't the point of my question. EDIT: I think I have thought of a counter-example. Let $X = Y = \{0,1\}$ with the discrete metric. Let $f(x) = x.$ Then choosing $\delta = 1/2$ implies continuity, but $f$ is not constant.","Generally, when we want to show continuity, we are given a small and we pick a according to this Heuristically, we can consider this to be a function of which must decrease as decreases, for example with the function , we have I also see that with a constant function, we can pick to be any number we like, so in a way our function does not depend on My question is: Let be metric spaces. If is a non-constant continuous function, must depend on ? I can see that if is infinite the answer is yes. Say we choose a , and pick we can pick which means that this does not work for this I think the infinite set means that we can always pick such that , and I see that this argument does not work does not work if is finite though, as we can pick so I worry that there is some weird finite set where we can pick a that works for all I know that thinking of as a function of in this way isn't necessarily accurate, as it may imply that only one works for a , which is not true, but this isn't the point of my question. EDIT: I think I have thought of a counter-example. Let with the discrete metric. Let Then choosing implies continuity, but is not constant.","\varepsilon \delta \varepsilon. \delta \varepsilon, \varepsilon x \mapsto x \delta(\varepsilon) = \varepsilon. \delta \delta \varepsilon. (X,d_x),(Y,d_Y) f: X \to Y \delta \varepsilon X \delta>0 x,x' \in X : d_X(x,x') < \delta, \varepsilon  = d_Y(f(x),f(x')), \delta \varepsilon. x,x' f(x)\ne f(x') d_x(x,x')<\delta. X \delta = \min_{x,x' \in X}\{d_X(x,x')\}, \delta \varepsilon. \delta \varepsilon \delta \varepsilon X = Y = \{0,1\} f(x) = x. \delta = 1/2 f","['analysis', 'continuity', 'epsilon-delta']"
77,Convergence of recursive sequence similar to harmonic series,Convergence of recursive sequence similar to harmonic series,,"Given $k>1$ and sequence $\{a_n\}$ satisfies $0<a_1<1$ , and $\forall n\ge1$ : $$a_{n+1}=a_n+\frac{a_n^k}{n^k} $$ Prove: $\{a_n\}$ is convergent. I think there is a limit to $\{a_n\}$ , but I cannot find a function to fit it and use induction, such as: $$a_n<C(1-n^{1-k})$$","Given and sequence satisfies , and : Prove: is convergent. I think there is a limit to , but I cannot find a function to fit it and use induction, such as:",k>1 \{a_n\} 0<a_1<1 \forall n\ge1 a_{n+1}=a_n+\frac{a_n^k}{n^k}  \{a_n\} \{a_n\} a_n<C(1-n^{1-k}),"['calculus', 'sequences-and-series', 'analysis']"
78,What is the best bound for $f'$ knowing bounds on $f$ and $f'''$?,What is the best bound for  knowing bounds on  and ?,f' f f''',"Let $f:\mathbb R\to \mathbb R$ be a three-times differentiable function. Suppose $|f(x)|\le 1$ and $|f'''(x)|\le 3$ for any $x\in \mathbb R$ . Show that $|f'(x)|\le 1$ for any $x\in \mathbb R$ . The hint to this question is to apply Taylor's theorem on $[x-h,x]$ and $[x,x+h]$ for any $x\in \mathbb R$ and $h>0$ . I have $$ \begin{align} f(x-h)&=f(x)-f'(x)h+{f''(x)\over 2}h^2-{f'''(w_1)\over 6}h^3, \\ f(x+h)&=f(x)+f'(x)h+{f''(x)\over 2}h^2+{f'''(w_2)\over 6}h^3 \end{align} $$ Hence $$f(x+h)-f(x-h)=2hf'(x)+{f'''(w_1)+f'''(w_2)\over 6}h^3.$$ However, I don't know how to proceed from here. I only know that $|f(x+h)-f(x-h)|\le 2$ . Can anyone help?","Let be a three-times differentiable function. Suppose and for any . Show that for any . The hint to this question is to apply Taylor's theorem on and for any and . I have Hence However, I don't know how to proceed from here. I only know that . Can anyone help?","f:\mathbb R\to \mathbb R |f(x)|\le 1 |f'''(x)|\le 3 x\in \mathbb R |f'(x)|\le 1 x\in \mathbb R [x-h,x] [x,x+h] x\in \mathbb R h>0 
\begin{align}
f(x-h)&=f(x)-f'(x)h+{f''(x)\over 2}h^2-{f'''(w_1)\over 6}h^3, \\
f(x+h)&=f(x)+f'(x)h+{f''(x)\over 2}h^2+{f'''(w_2)\over 6}h^3
\end{align}
 f(x+h)-f(x-h)=2hf'(x)+{f'''(w_1)+f'''(w_2)\over 6}h^3. |f(x+h)-f(x-h)|\le 2","['analysis', 'derivatives', 'taylor-expansion', 'interpolation-theory']"
79,Solving the following ratio of two series,Solving the following ratio of two series,,"I'm currently trying to solve the following expression for $0< s \leq 1$ : $$\frac{\sum\limits _{n=1}^{\lfloor l/3\rfloor}s^{l-3n}(1-s)^{3n}{l-1 \choose 3n-1}}{\sum\limits _{n=0}^{\lfloor l/3\rfloor}s^{l-3n}(1-s)^{3n}{l \choose 3n}}$$ I've only made very modest progress, transforming it into the following ratio: $$\frac{3}{l}\frac{\sum\limits _{n=1}^{\lfloor l/3\rfloor}n\cdot x^{3n}\cdot{l-1 \choose 3n-1}}{1+\sum\limits _{n=1}^{\lfloor l/3\rfloor}x^{3n}\cdot{l \choose 3n}},$$ where $x=\frac{1-s}{s}$ . I'm trying to find an analytic expression for the whole ratio, for all $l\in\mathbb{N}_{>0}$ . More modestly, I'm curious to find a way to prove that/how the ratio converges for $l\rightarrow\infty$ . Numerical simulation suggest that it converges, for $l\rightarrow \infty$ , to $1-s$ . Any help/tips/pointers would be immensely appreciated!","I'm currently trying to solve the following expression for : I've only made very modest progress, transforming it into the following ratio: where . I'm trying to find an analytic expression for the whole ratio, for all . More modestly, I'm curious to find a way to prove that/how the ratio converges for . Numerical simulation suggest that it converges, for , to . Any help/tips/pointers would be immensely appreciated!","0< s \leq 1 \frac{\sum\limits _{n=1}^{\lfloor l/3\rfloor}s^{l-3n}(1-s)^{3n}{l-1 \choose 3n-1}}{\sum\limits _{n=0}^{\lfloor l/3\rfloor}s^{l-3n}(1-s)^{3n}{l \choose 3n}} \frac{3}{l}\frac{\sum\limits _{n=1}^{\lfloor l/3\rfloor}n\cdot x^{3n}\cdot{l-1 \choose 3n-1}}{1+\sum\limits _{n=1}^{\lfloor l/3\rfloor}x^{3n}\cdot{l \choose 3n}}, x=\frac{1-s}{s} l\in\mathbb{N}_{>0} l\rightarrow\infty l\rightarrow \infty 1-s","['sequences-and-series', 'analysis', 'convergence-divergence', 'ceiling-and-floor-functions']"
80,$f(x_0) + f''(x_0) = 2f'(x_0)$,,f(x_0) + f''(x_0) = 2f'(x_0),"f $\in$ C $^2$ [a,b], and f has at least three distinct roots in [a,b]. I'm required to show that there's a point x $_0$ in [a,b], such that f(x $_0$ ) + f''(x $_0$ ) = 2f'(x $_0$ ). I concluded that there are points y $_1$ and y $_2$ such that f'(y $_1$ ) and f'(y $_2$ ) are zero. and a z $_1$ such that f''(z $_1$ ) = 0. I used the taylor formula at these points, I also considered to define a function g(x) = f(x) - f'(x) so that I have to show that g(x $_0$ ) = g'(x $_0$ ), but it didn't help. I also tried to find some counter examples, but didn't succeed. What should I do?","f C [a,b], and f has at least three distinct roots in [a,b]. I'm required to show that there's a point x in [a,b], such that f(x ) + f''(x ) = 2f'(x ). I concluded that there are points y and y such that f'(y ) and f'(y ) are zero. and a z such that f''(z ) = 0. I used the taylor formula at these points, I also considered to define a function g(x) = f(x) - f'(x) so that I have to show that g(x ) = g'(x ), but it didn't help. I also tried to find some counter examples, but didn't succeed. What should I do?",\in ^2 _0 _0 _0 _0 _1 _2 _1 _2 _1 _1 _0 _0,"['analysis', 'taylor-expansion', 'rolles-theorem', 'mean-value-theorem']"
81,"Uniform convergence of $\sum_{n=1}^\infty x \sin\frac{1}{x^2n^2}$, $x \in (0,+\infty)$","Uniform convergence of ,","\sum_{n=1}^\infty x \sin\frac{1}{x^2n^2} x \in (0,+\infty)","Consider a series $\sum_{n=1}^\infty f_n(x)$ , where $f_n(x) = x \sin\frac{1}{x^2n^2}$ and $x \in (0, +\infty)$ . If one fix an arbitrary $x \in (0, +\infty)$ , then for sufficiently large $n \in \mathbb{N}$ one would have $$ |f_n(x)| = \left|x \sin\frac{1}{x^2 n^2}\right| < x\frac{1}{x^2 n^2}. $$ The series $\sum_{n=1}^\infty \frac{1}{xn^2}$ converges for any fixed $x \in (0, +\infty)$ , so $\sum_{n=1}^\infty f_n(x)$ converges pointwise for $x \in (0, +\infty)$ . But does $\sum_{n=1}^\infty f_n(x)$ converge uniformly on $(0, +\infty)$ ? It is clear, that if $x \in [1, +\infty)$ , then $0 < \sin\frac{1}{x^2n^2} < \frac{1}{x^2n^2}, \; \forall n \in \mathbb{N}, $ $$ \left|x \sin\frac{1}{x^2 n^2}\right| < \frac{1}{xn^2} < \frac{1}{n^2}, $$ so $\sum_{n=1}^\infty x \sin\frac{1}{x^2 n^2}$ converges uniformly on $[1, +\infty)$ . But I don't understand what we can do with the series when $x \in (0,1)$ . Any help would be appreciated.","Consider a series , where and . If one fix an arbitrary , then for sufficiently large one would have The series converges for any fixed , so converges pointwise for . But does converge uniformly on ? It is clear, that if , then so converges uniformly on . But I don't understand what we can do with the series when . Any help would be appreciated.","\sum_{n=1}^\infty f_n(x) f_n(x) = x \sin\frac{1}{x^2n^2} x \in (0, +\infty) x \in (0, +\infty) n \in \mathbb{N} 
|f_n(x)| = \left|x \sin\frac{1}{x^2 n^2}\right| < x\frac{1}{x^2 n^2}.
 \sum_{n=1}^\infty \frac{1}{xn^2} x \in (0, +\infty) \sum_{n=1}^\infty f_n(x) x \in (0, +\infty) \sum_{n=1}^\infty f_n(x) (0, +\infty) x \in [1, +\infty) 0 < \sin\frac{1}{x^2n^2} < \frac{1}{x^2n^2}, \; \forall n \in \mathbb{N},  
\left|x \sin\frac{1}{x^2 n^2}\right| < \frac{1}{xn^2} < \frac{1}{n^2},
 \sum_{n=1}^\infty x \sin\frac{1}{x^2 n^2} [1, +\infty) x \in (0,1)","['real-analysis', 'analysis', 'uniform-convergence']"
82,Show that $f(x) = \sum_{k=0}^{\infty}{\frac{\sin^2(kx)}{1+k^2 x^2}}$ uniformly converges.,Show that  uniformly converges.,f(x) = \sum_{k=0}^{\infty}{\frac{\sin^2(kx)}{1+k^2 x^2}},"I want to show that $f(x) = \sum_{k=0}^{\infty}{\frac{\sin^2(kx)}{1+k^2 x^2}}$ uniformly converges for $|x| \geq \delta$ for any given $\delta > 0$ . I don't know how to use the M-test here, since it seems each term is constantly upper bounded by 0.359, regardless of the value of $k$ . I tried performing the Cauchy Criterion, but I am having a hard time showing that an $N$ exists for all $|x| \geq \delta$ . Any advice or hints would be great! For context, I'm a first year undergraduate maths student.","I want to show that uniformly converges for for any given . I don't know how to use the M-test here, since it seems each term is constantly upper bounded by 0.359, regardless of the value of . I tried performing the Cauchy Criterion, but I am having a hard time showing that an exists for all . Any advice or hints would be great! For context, I'm a first year undergraduate maths student.",f(x) = \sum_{k=0}^{\infty}{\frac{\sin^2(kx)}{1+k^2 x^2}} |x| \geq \delta \delta > 0 k N |x| \geq \delta,"['real-analysis', 'analysis', 'uniform-convergence', 'real-numbers']"
83,Counter example about convolution of symmetric functions on locally compact groups,Counter example about convolution of symmetric functions on locally compact groups,,"This question is motivated on what I think is an error in a result (Lemma 1.6.5) given in Dietmar, A. and Echterhoff, S., Principles of Harmonic Analysis, 2nd edition, Springer, pp.24 Suppose $G$ is a locally compact topological group and let $\lambda$ be a (left) Haar measure. One statement of the  aforementioned Lemma is that if $\phi$ and $\psi$ are $L_1(G,\lambda)$ symmetric functions ( $\phi(x^{-1})=\phi(x)$ and $\psi(x^{-1})=\psi(x)$ for all $x\in G$ ), then $\phi*\psi$ is also symmetric. I think this is not necessarily true, unless $G$ is also Abelian, or $\phi=\psi$ . A simple deduction one can make about $\phi*\psi$ is that \begin{align} \phi*\psi(x)&:=\int_G \phi(y)\psi(y^{-1}x)\,\lambda(dy)\\ &=\int_G\phi(xy)\psi(y^{-1})\,\lambda(dy)\\ &=\int_G\phi(y^{-1})\psi(yx)\Delta(y^{-1})\,\lambda(dy)\\ &=\int_G\phi(xy^{-1})\psi(y)\Delta(y^{-1})\,\lambda(dy) \end{align} where $\Delta$ is the modular function of $G$ . From this, and the symmetry of $\phi$ and $\psi$ one can establish that $$\phi*\psi(x^{-1})=\int_G\phi(x^{-1}y)\psi(y^{-1})\,\lambda(dy)=\int_G\phi(y^{-1}x)\psi(y)\,\lambda(dy)=\psi*\phi(x)$$ I seems to me that this is the best one can say about the between relation $\phi*\psi(x)$ and $\phi*\psi(x^{-1})$ under the general assumptions above. Of course, if $G$ were in addition commutative, then $\phi*\psi=\psi*\phi$ and thus, $\phi*\psi$ would be symmetric. The problem is to prove or disprove (via a counter example) whether it is indeed the case that $\phi*\psi$ is symmetric under the general assumptions on $G$ .","This question is motivated on what I think is an error in a result (Lemma 1.6.5) given in Dietmar, A. and Echterhoff, S., Principles of Harmonic Analysis, 2nd edition, Springer, pp.24 Suppose is a locally compact topological group and let be a (left) Haar measure. One statement of the  aforementioned Lemma is that if and are symmetric functions ( and for all ), then is also symmetric. I think this is not necessarily true, unless is also Abelian, or . A simple deduction one can make about is that where is the modular function of . From this, and the symmetry of and one can establish that I seems to me that this is the best one can say about the between relation and under the general assumptions above. Of course, if were in addition commutative, then and thus, would be symmetric. The problem is to prove or disprove (via a counter example) whether it is indeed the case that is symmetric under the general assumptions on .","G \lambda \phi \psi L_1(G,\lambda) \phi(x^{-1})=\phi(x) \psi(x^{-1})=\psi(x) x\in G \phi*\psi G \phi=\psi \phi*\psi \begin{align}
\phi*\psi(x)&:=\int_G \phi(y)\psi(y^{-1}x)\,\lambda(dy)\\
&=\int_G\phi(xy)\psi(y^{-1})\,\lambda(dy)\\
&=\int_G\phi(y^{-1})\psi(yx)\Delta(y^{-1})\,\lambda(dy)\\
&=\int_G\phi(xy^{-1})\psi(y)\Delta(y^{-1})\,\lambda(dy)
\end{align} \Delta G \phi \psi \phi*\psi(x^{-1})=\int_G\phi(x^{-1}y)\psi(y^{-1})\,\lambda(dy)=\int_G\phi(y^{-1}x)\psi(y)\,\lambda(dy)=\psi*\phi(x) \phi*\psi(x) \phi*\psi(x^{-1}) G \phi*\psi=\psi*\phi \phi*\psi \phi*\psi G","['analysis', 'harmonic-analysis']"
84,Comparison between $a^{b!}$ and $(a^b)!$,Comparison between  and,a^{b!} (a^b)!,"I've recently discovered two conclusions that may be true. However I can't prove it. The statement is as follows : Suppose $a$ , $b$ are positive integers and $a\geq b \geq 2$ , then I found this conclusion $a^{b!}<(a^b)!$ might be true. On the other hand, for any $a\geq 2$ , there must exist an integer $b_0>a$ ( $b_0$ is related to $a$ ) such that for any $b\geq b_0$ we have $a^{b!}>(a^b)!$ . I have verified the above conclusion in some cases such as $2^{100!}>2^{100}!$ , $99^{280!}>99^{280}!$ , $100^{100!}<100^{100}!$ , $200^{100!}<200^{100}!$ . For $a=2$ , we can verify that $2^{5!}>2^{5}!$ is true and $b_0=5$ is the smallest one.","I've recently discovered two conclusions that may be true. However I can't prove it. The statement is as follows : Suppose , are positive integers and , then I found this conclusion might be true. On the other hand, for any , there must exist an integer ( is related to ) such that for any we have . I have verified the above conclusion in some cases such as , , , . For , we can verify that is true and is the smallest one.",a b a\geq b \geq 2 a^{b!}<(a^b)! a\geq 2 b_0>a b_0 a b\geq b_0 a^{b!}>(a^b)! 2^{100!}>2^{100}! 99^{280!}>99^{280}! 100^{100!}<100^{100}! 200^{100!}<200^{100}! a=2 2^{5!}>2^{5}! b_0=5,"['analysis', 'inequality', 'factorial']"
85,To prove an identity related to Gamma function,To prove an identity related to Gamma function,,"Question: How to prove the following identity for all positive integers $k$ and $n$ : \begin{align}\tag{1} (k+1)(2k+1) \cdots (nk+1) = \sum_{i=1}^n & \binom{n}{i} \left( \frac{i}{n}(k+2)-1 \right) \left[ (k+1)(2k+1) \cdots ((i-1)k+1) \right] \\ &\ \times \left[ (k+1)(2k+1) \cdots ((n-i)k+1) \right]. \end{align} I could check by enumeration in $n$ : \begin{align} n&=1, & k+1 &= \binom{1}{1} (k+2-1), \\ n&=2, & (k+1)(2k+1) &= \binom{2}{1} \left( \frac{1}{2}(k+2)-1 \right) (k+1) + \binom{2}{2} \left( (k+2)-1 \right) (k+1) \\ & & & = k(k+1) + (k+1)^2, \\ n&=3, & (k+1)(2k+1)(3k+1) &= \binom{3}{1} \left( \frac{1}{3}(k+2)-1 \right) (k+1)(2k+1) \\ & & &\quad + \binom{3}{2} \left( \frac{2}{3}(k+2)-1 \right) (k+1)^2 \\ & & &\quad + \binom{3}{3} \left( (k+2)-1 \right) (k+1)(2k+1) \\ & & & = (k-1)(k+1)(2k+1) + (2k+1)(k+1)^2 + (k+1)^2(2k+1), \\ & \cdots & & \end{align} But I failed to prove it by induction in $n$ : Set \begin{equation} I(n;k) := \sum_{i=1}^n \binom{n}{i} \left( \frac{i}{n}(k+2)-1 \right) \left[ (k+1) \cdots ((i-1)k+1) \right] \left[ (k+1) \cdots ((n-i)k+1) \right]. \end{equation} Then \begin{equation} \begin{split} I(n+1;k) - I(n;k) =&\ (k+1) \left[ (k+1) \cdots (nk+1) \right] \\ &\ + \sum_{i=1}^n \binom{n}{i} \left[ (k+1) \cdots ((i-1)k+1) \right] \left[ (k+1) \cdots ((n-i)k+1) \right] \\ &\ \qquad\quad \times \underbrace{\left[ \frac{n+1}{n+1-i} \left( \frac{i}{n+1}(k+2)-1 \right) ((n+1-i)k+1) - \left( \frac{i}{n}(k+2)-1 \right) \right]}_{=:J(n;k,i)} \end{split} \end{equation} If the followin equation holds, \begin{equation}\tag{2} J(n;k,i) = \left( \frac{i}{n}(k+2)-1 \right)(nk-1), \end{equation} then using the inductive hypothesis $I(n;k) = (k+1) \cdots (nk+1)$ , we have \begin{equation} \begin{split} I(n+1;k) - I(n;k) =&\ (k+1) \left[ (k+1) \cdots (nk+1) \right] + (nk-1) I(n;k) = (n+1)k I(n;k) \end{split} \end{equation} which yields $I(n+1;k) = ((n+1)k+1) I(n;k) = (k+1) \cdots (nk+1) ((n+1)k+1)$ as desired. But, unfortunately, equation $(2)$ does not hold... I was also thinking that the question may be related to Gamma function , because $$ (k+1)(2k+1) \cdots (nk+1) = k^n \frac{\Gamma(n+1+\frac{1}{k})}{\Gamma(1+\frac{1}{k})} $$ so that identity $(1)$ turns to \begin{equation}\tag{1'} k \Gamma\left(n+1+\frac{1}{k}\right) \Gamma\left(1+\frac{1}{k}\right) = \sum_{i=1}^n \binom{n}{i} \left( \frac{i}{n}(k+2)-1 \right) \Gamma\left(i+\frac{1}{k}\right) \Gamma\left(n-i+1+\frac{1}{k}\right). \end{equation} But still, I have no clue to prove $(1')$ . Could anyone help on it? Any hint or comment will be appreciated. TIA...","Question: How to prove the following identity for all positive integers and : I could check by enumeration in : But I failed to prove it by induction in : Set Then If the followin equation holds, then using the inductive hypothesis , we have which yields as desired. But, unfortunately, equation does not hold... I was also thinking that the question may be related to Gamma function , because so that identity turns to But still, I have no clue to prove . Could anyone help on it? Any hint or comment will be appreciated. TIA...","k n \begin{align}\tag{1}
(k+1)(2k+1) \cdots (nk+1) = \sum_{i=1}^n & \binom{n}{i} \left( \frac{i}{n}(k+2)-1 \right) \left[ (k+1)(2k+1) \cdots ((i-1)k+1) \right] \\
&\ \times \left[ (k+1)(2k+1) \cdots ((n-i)k+1) \right].
\end{align} n \begin{align}
n&=1, & k+1 &= \binom{1}{1} (k+2-1), \\
n&=2, & (k+1)(2k+1) &= \binom{2}{1} \left( \frac{1}{2}(k+2)-1 \right) (k+1) + \binom{2}{2} \left( (k+2)-1 \right) (k+1) \\
& & & = k(k+1) + (k+1)^2, \\
n&=3, & (k+1)(2k+1)(3k+1) &= \binom{3}{1} \left( \frac{1}{3}(k+2)-1 \right) (k+1)(2k+1) \\
& & &\quad + \binom{3}{2} \left( \frac{2}{3}(k+2)-1 \right) (k+1)^2 \\
& & &\quad + \binom{3}{3} \left( (k+2)-1 \right) (k+1)(2k+1) \\
& & & = (k-1)(k+1)(2k+1) + (2k+1)(k+1)^2 + (k+1)^2(2k+1), \\
& \cdots & &
\end{align} n \begin{equation}
I(n;k) := \sum_{i=1}^n \binom{n}{i} \left( \frac{i}{n}(k+2)-1 \right) \left[ (k+1) \cdots ((i-1)k+1) \right] \left[ (k+1) \cdots ((n-i)k+1) \right].
\end{equation} \begin{equation}
\begin{split}
I(n+1;k) - I(n;k) =&\ (k+1) \left[ (k+1) \cdots (nk+1) \right] \\
&\ + \sum_{i=1}^n \binom{n}{i} \left[ (k+1) \cdots ((i-1)k+1) \right] \left[ (k+1) \cdots ((n-i)k+1) \right] \\
&\ \qquad\quad \times \underbrace{\left[ \frac{n+1}{n+1-i} \left( \frac{i}{n+1}(k+2)-1 \right) ((n+1-i)k+1) - \left( \frac{i}{n}(k+2)-1 \right) \right]}_{=:J(n;k,i)}
\end{split}
\end{equation} \begin{equation}\tag{2}
J(n;k,i) = \left( \frac{i}{n}(k+2)-1 \right)(nk-1),
\end{equation} I(n;k) = (k+1) \cdots (nk+1) \begin{equation}
\begin{split}
I(n+1;k) - I(n;k) =&\ (k+1) \left[ (k+1) \cdots (nk+1) \right] + (nk-1) I(n;k) = (n+1)k I(n;k)
\end{split}
\end{equation} I(n+1;k) = ((n+1)k+1) I(n;k) = (k+1) \cdots (nk+1) ((n+1)k+1) (2) 
(k+1)(2k+1) \cdots (nk+1) = k^n \frac{\Gamma(n+1+\frac{1}{k})}{\Gamma(1+\frac{1}{k})}
 (1) \begin{equation}\tag{1'}
k \Gamma\left(n+1+\frac{1}{k}\right) \Gamma\left(1+\frac{1}{k}\right) = \sum_{i=1}^n \binom{n}{i} \left( \frac{i}{n}(k+2)-1 \right) \Gamma\left(i+\frac{1}{k}\right) \Gamma\left(n-i+1+\frac{1}{k}\right).
\end{equation} (1')","['combinatorics', 'analysis', 'induction', 'gamma-function', 'combinatorial-proofs']"
86,The derivative of a multivariable function evaluated at $0$,The derivative of a multivariable function evaluated at,0,"I am currently looking at the problem: For a function $f \in C^1( \mathbb{B}, \mathbb{R}^k)$ where there exists some positive value $\beta$ such that $\lvert Df(0)h \rvert \ge \beta \lvert h \rvert $ for all $h \in \mathbb{R}^n$ Now define $P: \mathbb{B} \rightarrow \mathbb{R}^k$ where $P(x) = f(x) - Df(0)x$ and calculate $DP(0)$ . I believe that the best way to find $DP(0)$ is to consider $P(x+h) - P(x)$ and look for terms that are linear in $h$ and evaluate this at $x=0$ . This gives us $$P(x+h) - P(x) = \big{(} f(x+h) - Df(0)(x+h) - f(x) + Df(0)x \big{)} = \big{(} f(x+h) - Df(0)h - f(x) \big{)}$$ Here we, see that $Df(0)h$ is linear in $h$ , however, it is unclear to me whether or not $f(x+h)$ will also have linear terms and so this approach seems to be indeterminate to me. I would be grateful for any guidance.","I am currently looking at the problem: For a function where there exists some positive value such that for all Now define where and calculate . I believe that the best way to find is to consider and look for terms that are linear in and evaluate this at . This gives us Here we, see that is linear in , however, it is unclear to me whether or not will also have linear terms and so this approach seems to be indeterminate to me. I would be grateful for any guidance.","f \in C^1( \mathbb{B}, \mathbb{R}^k) \beta \lvert Df(0)h \rvert \ge \beta \lvert h \rvert  h \in \mathbb{R}^n P: \mathbb{B} \rightarrow \mathbb{R}^k P(x) = f(x) - Df(0)x DP(0) DP(0) P(x+h) - P(x) h x=0 P(x+h) - P(x) = \big{(} f(x+h) - Df(0)(x+h) - f(x) + Df(0)x \big{)} = \big{(} f(x+h) - Df(0)h - f(x) \big{)} Df(0)h h f(x+h)","['real-analysis', 'calculus', 'analysis', 'multivariable-calculus', 'derivatives']"
87,"Showing $\big\{(x,y)\in\mathbb{R}^n$ $\times$ $\mathbb{R}$ $\mid $ $\min ( f,g ) < y$ $< \max ( f, g)\big\}$ is open in $\mathbb{R}^{n+1}$",Showing       is open in,"\big\{(x,y)\in\mathbb{R}^n \times \mathbb{R} \mid  \min ( f,g ) < y < \max ( f, g)\big\} \mathbb{R}^{n+1}","In a course on Multivariable Calculus, I came across the following problem, and am looking for some guidance on how to approach it. Question Consider continuous functions $f,g$ in $\mathbb{R}^n$ and define the set $$A =\big\{(x,y)\in\mathbb{R}^n\times\mathbb{R}\mid \min \big( f(x),g(x) \big) < y < \max \big( f(x), g(x)\big)\big\}$$ Prove that $A$ is an open subset of $\mathbb{R}^{n+1}.$ Thoughts I believe that a good first step here would be to try to prove the continuity of $\min\{f,g\}$ and $\max\{f,g\}$ separately. However, I haven’t made progress on this simplified exercise. Perhaps this suggested simplification doesn't end up being an avenue that is worth pursuing, although at the moment this is my only idea on how to make a start with this. I would be grateful for any help in solving either the problem itself or with the simplified problems (which should allow me to try to progress with the main problem).","In a course on Multivariable Calculus, I came across the following problem, and am looking for some guidance on how to approach it. Question Consider continuous functions in and define the set Prove that is an open subset of Thoughts I believe that a good first step here would be to try to prove the continuity of and separately. However, I haven’t made progress on this simplified exercise. Perhaps this suggested simplification doesn't end up being an avenue that is worth pursuing, although at the moment this is my only idea on how to make a start with this. I would be grateful for any help in solving either the problem itself or with the simplified problems (which should allow me to try to progress with the main problem).","f,g \mathbb{R}^n A =\big\{(x,y)\in\mathbb{R}^n\times\mathbb{R}\mid \min \big( f(x),g(x) \big) < y < \max \big( f(x), g(x)\big)\big\} A \mathbb{R}^{n+1}. \min\{f,g\} \max\{f,g\}","['real-analysis', 'analysis', 'multivariable-calculus', 'continuity', 'proof-writing']"
88,"Prove that there exists $\xi \in (a,b)$ such that $f(a)-2f(\frac{a+b}{2})+f(b)=\frac{1}{4}(b-a)^2f''(\xi) .$ [duplicate]",Prove that there exists  such that  [duplicate],"\xi \in (a,b) f(a)-2f(\frac{a+b}{2})+f(b)=\frac{1}{4}(b-a)^2f''(\xi) .","This question already has answers here : Proving a 2nd order Mean-Value theorem [closed] (2 answers) Closed 1 year ago . Given that f is twice differentiable on $[a,b]$ , prove that there exists $\xi \in (a,b)$ such that $$f(a)-2f\left(\frac{a+b}{2}\right)+f(b)=\frac{1}{4}(b-a)^2f''(\xi) .$$ This problem was given in a book and the hint was to consider the following auxiliary function, and apply Rolle's Theorem: $$F(t) = f(t) +f(a) -2f\left(\frac{t+a}{2}\right) - \lambda\cdot\frac{(t-a)^2}{4}$$ Where $$\lambda = \frac{f(a)+f(b)-2f\left(\displaystyle\frac{a+b}{2}\right)}{\displaystyle\frac{(b-a)^2}{4}}.$$ I tried and got stuck. My attempt: Notice that $F(a) =  F(b) = 0$ , by Rolle's Theorem, there exists $\xi_1 \in (a,b)$ such that $F'(\xi_1)=0$ . After differentiation, we obtain $$F'(t) = f'(t)-f'\left(\displaystyle\frac{t+a}{2}\right)-\lambda\cdot\displaystyle\frac{t-a}{2}$$ We find that $F'(a) = 0$ , thus applying Rolle's Theorem again, there exists $\xi_2 \in (a,\xi_1)$ such that $F''(\xi_2) = 0$ Therefore $$0=f''(\xi_2)-\frac{1}{2}f''\left(\frac{\xi_2+a}{2}\right) - \frac{\lambda}{2}$$ and $$\lambda = 2f''(\xi_2) - f''\left(\frac{\xi_2+a}{2}\right).$$ I got stuck here and was unable to proceed. I did consider using Darboux's Theorem but I don't think that it is applicable here.","This question already has answers here : Proving a 2nd order Mean-Value theorem [closed] (2 answers) Closed 1 year ago . Given that f is twice differentiable on , prove that there exists such that This problem was given in a book and the hint was to consider the following auxiliary function, and apply Rolle's Theorem: Where I tried and got stuck. My attempt: Notice that , by Rolle's Theorem, there exists such that . After differentiation, we obtain We find that , thus applying Rolle's Theorem again, there exists such that Therefore and I got stuck here and was unable to proceed. I did consider using Darboux's Theorem but I don't think that it is applicable here.","[a,b] \xi \in (a,b) f(a)-2f\left(\frac{a+b}{2}\right)+f(b)=\frac{1}{4}(b-a)^2f''(\xi) . F(t) = f(t) +f(a) -2f\left(\frac{t+a}{2}\right) - \lambda\cdot\frac{(t-a)^2}{4} \lambda = \frac{f(a)+f(b)-2f\left(\displaystyle\frac{a+b}{2}\right)}{\displaystyle\frac{(b-a)^2}{4}}. F(a) =  F(b) = 0 \xi_1 \in (a,b) F'(\xi_1)=0 F'(t) = f'(t)-f'\left(\displaystyle\frac{t+a}{2}\right)-\lambda\cdot\displaystyle\frac{t-a}{2} F'(a) = 0 \xi_2 \in (a,\xi_1) F''(\xi_2) = 0 0=f''(\xi_2)-\frac{1}{2}f''\left(\frac{\xi_2+a}{2}\right) - \frac{\lambda}{2} \lambda = 2f''(\xi_2) - f''\left(\frac{\xi_2+a}{2}\right).","['real-analysis', 'analysis', 'rolles-theorem']"
89,$U_{n} + \lfloor \sqrt{U_{n}} \rfloor = k^{2}$ [closed],[closed],U_{n} + \lfloor \sqrt{U_{n}} \rfloor = k^{2},"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . This post was edited and submitted for review last year and failed to reopen the post: Original close reason(s) were not resolved Improve this question I've been stuck, I tried to prove it by extracting the explicit formula but i got nowhere.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . This post was edited and submitted for review last year and failed to reopen the post: Original close reason(s) were not resolved Improve this question I've been stuck, I tried to prove it by extracting the explicit formula but i got nowhere.",,['sequences-and-series']
90,Better estimate on binomial coefficient $\binom{n}{k}$ when $k=(1-o(1))n$?,Better estimate on binomial coefficient  when ?,\binom{n}{k} k=(1-o(1))n,"For binomial coefficient $\binom{n}{k}$ , I know one inequality saying that $$\binom{n}{k}\le (en/k)^k.$$ While for $k=(1-\epsilon)n$ if $\epsilon$ is an $o(1)$ -term, where we assume $n$ tends to infinity, the above inequality gives an exponential bound. I am wondering is there a better estimate on $\binom{n}{k}$ in this case?","For binomial coefficient , I know one inequality saying that While for if is an -term, where we assume tends to infinity, the above inequality gives an exponential bound. I am wondering is there a better estimate on in this case?",\binom{n}{k} \binom{n}{k}\le (en/k)^k. k=(1-\epsilon)n \epsilon o(1) n \binom{n}{k},"['combinatorics', 'analysis', 'inequality']"
91,How to use induction in this sense?,How to use induction in this sense?,,"For context, I've learned induction before, in a different way than this author mentions, but I believe I understand it's a rephrasing of it. However, I have no idea how to apply it to these two problems in particular, here is the definition of induction, I attempted this problem first, but my technique is not correct Proof : Let $E=\{n\in\mathbb{N}\mid (x_1+x_2+\cdots +x_{n-1})+x_n=x_1+x_2+\cdots+x_n\}$ then we want to show by induction that $E=\mathbb{N}$ . Consider that $E\subset \mathbb{N}$ and that $x_1=x_1$ so $1\in E$ . Now, assume that $k\in E$ then \begin{align*}         (x_1+\cdots +x_{k-1})+x_k=x_1+\cdots+x_k     \end{align*} furthermore consider the sum \begin{align*}         (x_1+\cdots + x_k)+x_{k+1}     \end{align*} then by associativity of the reals, we can achieve \begin{align*}         (x_1+\cdots+x_{k-1})+(x_{k}+x_{k+1})         =x_1+\cdots+x_k+x_{k+1}     \end{align*} by the inductive hypothesis since $x_k+x_{k+1}$ is real therefore $E=\mathbb{N}$ and the sum $x_1+\cdots +x_n$ is defined independently of the insertion of parentheses. However, this seems circular in some way; any suggestions on using this definition correctly and/or proving this statement by induction? Also, I think I missed a couple of cases regarding the bracket movements.","For context, I've learned induction before, in a different way than this author mentions, but I believe I understand it's a rephrasing of it. However, I have no idea how to apply it to these two problems in particular, here is the definition of induction, I attempted this problem first, but my technique is not correct Proof : Let then we want to show by induction that . Consider that and that so . Now, assume that then furthermore consider the sum then by associativity of the reals, we can achieve by the inductive hypothesis since is real therefore and the sum is defined independently of the insertion of parentheses. However, this seems circular in some way; any suggestions on using this definition correctly and/or proving this statement by induction? Also, I think I missed a couple of cases regarding the bracket movements.","E=\{n\in\mathbb{N}\mid (x_1+x_2+\cdots +x_{n-1})+x_n=x_1+x_2+\cdots+x_n\} E=\mathbb{N} E\subset \mathbb{N} x_1=x_1 1\in E k\in E \begin{align*}
        (x_1+\cdots +x_{k-1})+x_k=x_1+\cdots+x_k
    \end{align*} \begin{align*}
        (x_1+\cdots + x_k)+x_{k+1}
    \end{align*} \begin{align*}
        (x_1+\cdots+x_{k-1})+(x_{k}+x_{k+1})
        =x_1+\cdots+x_k+x_{k+1}
    \end{align*} x_k+x_{k+1} E=\mathbb{N} x_1+\cdots +x_n","['real-analysis', 'analysis', 'induction']"
92,How does the Axiom of Choice plays role in this proof?,How does the Axiom of Choice plays role in this proof?,,"I was reading a proof of the following theorem: Let $(X,d)$ be a metric space, if $(X,d)$ is complete and totally bounded then is compact (in the sequencial sense). Proof: Let $(x^{(n)})_{n=1}^{\infty}$ be a sequence in $X$ , the metric space is totally boundend then we can find $y_1,\dots, y_n\in X$ such that $$X= \bigcup_{i=1}^n B(y_i, 1)$$ notice that there must exists $1\leq k\leq n$ , such that for all $N\geq 1$ , there exist $n\geq N$ , and $x^{(n)}\in B(y_k,1)$ and we set: $$(1;1):= \min\{m\in\mathbb{N} : x^{(m)}\in B(y_k, 1)\}$$ $$(n; 1):= \min\{m\in\mathbb{N} : m > (n-1, 1), \ x^{(m)}\in B(y_k, 1)\} \quad\forall n\geq2$$ then $(x^{(n;1)})_{n=1}^{\infty}$ is a subsequence of $(x^{(n)})_{n=1}^{\infty}$ and $(x^{(n;1)})_{n=1}^{\infty}$ is contained in a $1$ -ball; we repeat this procedure with the sequence $(x^{(n;1)})_{n=1}^{\infty}$ using $1/2$ -balls to cover $X$ , then we obtain a subsequence $(x^{(n;2)})_{n=1}^{\infty}$ of $(x^{(n;1)})_{n=1}^{\infty}$ , such that $(x^{(n;2)})_{n=1}^{\infty}$ is contained in a single $1/2$ -ball; we repeat this procedure infinte times, then for all $j\geq 1$ we get a subsequence $(x^{(n;j)})_{n=1}^{\infty}$ of $(x^{(n)})_{n=1}^{\infty}$ , such that for each $j$ , the elements of $(x^{(n;j)})_{n=1}^{\infty}$ are contained in a single ball of radius $1/j$ , and also that each sequence $(x^{(n;j+1)})_{n=1}^{\infty}$ is a subsequence of the previous one $(x^{(n;j)})_{n=1}^{\infty}$ , finally we take the sequence $(x^{(n;n)})_{n=1}^{\infty}$ , this sequence is a subsequence of $(x^{(n)})_{n=1}^{\infty}$ , also is a Cauchy sequence and is convergent, since, the space is complete, then every sequence in $X$ has a convergent subsequence. My doubt is in the way we construct the sequence $(x^{(n,n)})_{n=1}^{\infty}$ , How we can use more rigorous arguments to construct that sequence? I suspect that we need Axiom of choice, so my attempt was: Let $\mathcal{C}$ be set of all subsequence $(y^{(n)})_{n=1}^{\infty}$ of $(x^{(n)})_{n=1}^{\infty}$ and define $\Omega:= \mathcal{C}\times \mathbb{N}$ with the relation $\sim$ defined as follows: $\big((y^{(n)})_{n=1}^{\infty}, \ k\big) \sim \big((z^{(n)})_{n=1}^{\infty}, \ j\big)$ iff $j=k+1$ and $(z^{(n)})_{n=1}^{\infty}$ is a subsequence of $(y^{(n)})_{n=1}^{\infty}$ such that $d(z^{(m)}, z^{(n)}) < 2/j$ for all $n,m\geq 1$ , with a similar prodecure to we used at the beginning of the proof we can show that for all $a\in \Omega$ there exist $b\in\Omega$ such that $a\sim b$ , for each $a\in\Omega$ we define $R(a):=\{b\in\Omega : a\sim b\}$ , with the axiom of choice we can find a function: $$f : \Omega\rightarrow \bigcup_{a\in\Omega} R(a)$$ such that $f(a)\in R(a)$ for all $a\in\Omega$ , so consider the sequence $\big[\big(f^k((x^{(n)})_{n=0}^{\infty},0\big)\big]_{k=1}^{\infty}$ , for each $j\geq 1$ we define the sequence $(x^{(n:j)})_{n=1}^{\infty}$ as the sequence of the first entry of $f^j((x^{(n)})_{n=0}^{\infty},0\big)$","I was reading a proof of the following theorem: Let be a metric space, if is complete and totally bounded then is compact (in the sequencial sense). Proof: Let be a sequence in , the metric space is totally boundend then we can find such that notice that there must exists , such that for all , there exist , and and we set: then is a subsequence of and is contained in a -ball; we repeat this procedure with the sequence using -balls to cover , then we obtain a subsequence of , such that is contained in a single -ball; we repeat this procedure infinte times, then for all we get a subsequence of , such that for each , the elements of are contained in a single ball of radius , and also that each sequence is a subsequence of the previous one , finally we take the sequence , this sequence is a subsequence of , also is a Cauchy sequence and is convergent, since, the space is complete, then every sequence in has a convergent subsequence. My doubt is in the way we construct the sequence , How we can use more rigorous arguments to construct that sequence? I suspect that we need Axiom of choice, so my attempt was: Let be set of all subsequence of and define with the relation defined as follows: iff and is a subsequence of such that for all , with a similar prodecure to we used at the beginning of the proof we can show that for all there exist such that , for each we define , with the axiom of choice we can find a function: such that for all , so consider the sequence , for each we define the sequence as the sequence of the first entry of","(X,d) (X,d) (x^{(n)})_{n=1}^{\infty} X y_1,\dots, y_n\in X X= \bigcup_{i=1}^n B(y_i, 1) 1\leq k\leq n N\geq 1 n\geq N x^{(n)}\in B(y_k,1) (1;1):= \min\{m\in\mathbb{N} : x^{(m)}\in B(y_k, 1)\} (n; 1):= \min\{m\in\mathbb{N} : m > (n-1, 1), \ x^{(m)}\in B(y_k, 1)\} \quad\forall n\geq2 (x^{(n;1)})_{n=1}^{\infty} (x^{(n)})_{n=1}^{\infty} (x^{(n;1)})_{n=1}^{\infty} 1 (x^{(n;1)})_{n=1}^{\infty} 1/2 X (x^{(n;2)})_{n=1}^{\infty} (x^{(n;1)})_{n=1}^{\infty} (x^{(n;2)})_{n=1}^{\infty} 1/2 j\geq 1 (x^{(n;j)})_{n=1}^{\infty} (x^{(n)})_{n=1}^{\infty} j (x^{(n;j)})_{n=1}^{\infty} 1/j (x^{(n;j+1)})_{n=1}^{\infty} (x^{(n;j)})_{n=1}^{\infty} (x^{(n;n)})_{n=1}^{\infty} (x^{(n)})_{n=1}^{\infty} X (x^{(n,n)})_{n=1}^{\infty} \mathcal{C} (y^{(n)})_{n=1}^{\infty} (x^{(n)})_{n=1}^{\infty} \Omega:= \mathcal{C}\times \mathbb{N} \sim \big((y^{(n)})_{n=1}^{\infty}, \ k\big) \sim \big((z^{(n)})_{n=1}^{\infty}, \ j\big) j=k+1 (z^{(n)})_{n=1}^{\infty} (y^{(n)})_{n=1}^{\infty} d(z^{(m)}, z^{(n)}) < 2/j n,m\geq 1 a\in \Omega b\in\Omega a\sim b a\in\Omega R(a):=\{b\in\Omega : a\sim b\} f : \Omega\rightarrow \bigcup_{a\in\Omega} R(a) f(a)\in R(a) a\in\Omega \big[\big(f^k((x^{(n)})_{n=0}^{\infty},0\big)\big]_{k=1}^{\infty} j\geq 1 (x^{(n:j)})_{n=1}^{\infty} f^j((x^{(n)})_{n=0}^{\infty},0\big)","['general-topology', 'analysis', 'set-theory', 'axiom-of-choice']"
93,"smooth maps between submanifolds, the image of tangent space under differential is contained in a tangent space","smooth maps between submanifolds, the image of tangent space under differential is contained in a tangent space",,"I want to show : This is from ""Mathematical analysis"" by Andrew Browder. This is not a manifolds text so we have only defined submanifolds on $R^n$ using the local immersion definition. I know there are at least 4 more equivalent definitions. The definition of tangent space is defined by smooth curves. Also smooth mapping on a set is defined to be a smooth extension $F$ to an open set such that $F$ and $f$ agrees on $M$ and a previous theorem showed that the differential (jacobian matrix) applied to tangent spaces are well defined. So by definition, an element of $h\in T_p(M)$ is a smooth curve $\gamma$ that maps to $M$ such that $\gamma(0)=p$ and $\gamma'(0)=h$ . So I want to show $dF_p(h)=h' \in T_{f(p)}(N)$ . Now this is where I have been stuck for a while, because we want a curve $\gamma_N$ that maps to $N$ such that $\gamma_N(0)=f(p)$ and $\gamma_N'(0)=h'$ , but I can't think of any good reason why such a curve should exist.","I want to show : This is from ""Mathematical analysis"" by Andrew Browder. This is not a manifolds text so we have only defined submanifolds on using the local immersion definition. I know there are at least 4 more equivalent definitions. The definition of tangent space is defined by smooth curves. Also smooth mapping on a set is defined to be a smooth extension to an open set such that and agrees on and a previous theorem showed that the differential (jacobian matrix) applied to tangent spaces are well defined. So by definition, an element of is a smooth curve that maps to such that and . So I want to show . Now this is where I have been stuck for a while, because we want a curve that maps to such that and , but I can't think of any good reason why such a curve should exist.",R^n F F f M h\in T_p(M) \gamma M \gamma(0)=p \gamma'(0)=h dF_p(h)=h' \in T_{f(p)}(N) \gamma_N N \gamma_N(0)=f(p) \gamma_N'(0)=h',"['analysis', 'differential-geometry', 'manifolds', 'smooth-manifolds', 'submanifold']"
94,Big-$O$ confusion,Big- confusion,O,"$f(x) = O(g(x))$ means that $\frac{f(x)}{g(x)}$ is bounded. But $x=O(x^2 + 1),\ x\in  \mathbb R$ while $x\ne O(x^2)$ . Is there a human friendly explanation of what $O$ is? the definitions I saw are as follows: f, g: E → R, a is a limit point of E. If there are $\dot{U}_a$ and a function ϕ: E → R, f(x)=ϕ(x)⋅g(x) and ϕ(x) is bounded in $\dot{U}_a$ ∩E , then f = O(g) f, g: E→R. ∃ C> $0$ : |f(x)| ≤ C ⋅ |g(x)| ⇒ f=O(g) in E and examples: sin x = O(x) $x \in R$ cos x ≠ O(x) $x \in R$ x ≠ O(sin x) $x \in R$ x = O( $x^2+1$ ) $x \in R$ x ≠O( $x^2$ ) $x \in R$","means that is bounded. But while . Is there a human friendly explanation of what is? the definitions I saw are as follows: f, g: E → R, a is a limit point of E. If there are and a function ϕ: E → R, f(x)=ϕ(x)⋅g(x) and ϕ(x) is bounded in ∩E , then f = O(g) f, g: E→R. ∃ C> : |f(x)| ≤ C ⋅ |g(x)| ⇒ f=O(g) in E and examples: sin x = O(x) cos x ≠ O(x) x ≠ O(sin x) x = O( ) x ≠O( )","f(x) = O(g(x)) \frac{f(x)}{g(x)} x=O(x^2 + 1),\ x\in  \mathbb R x\ne O(x^2) O \dot{U}_a \dot{U}_a 0 x \in R x \in R x \in R x^2+1 x \in R x^2 x \in R",['analysis']
95,Proving that T is the Taylor Polynomial of f of degree n.,Proving that T is the Taylor Polynomial of f of degree n.,,"Problem: Let $I$ be an interval, $f \in C^n(I,\mathbb R), x_0 \in I,$ and $T$ , a polynomial of degree $n$ with $$\lim_{x\to a}\frac{f(x)-T(x)}{(x-x_0)^n}=0.$$ Prove that T is the Taylor polynomial of $f$ of degree $n$ in $x_0 $ . I have come across several questions proving that if T is a Taylor polynomial then we have the above equation. However, I can't seem to figure out how to make my way towards this proof as it is basically the asking for the other direction of the Taylor polynomial theorem.  I see a pattern between how the above limit is similar to what is the derivative of $f(x_0)$ , but I need some help seeing the overall picture. I know we are done once I prove that: $$T(x) = \sum_{k=0}^{n}\frac{f^{k}(x)}{k!}(x-x_0)^k$$ Any help is appreciated.","Problem: Let be an interval, and , a polynomial of degree with Prove that T is the Taylor polynomial of of degree in . I have come across several questions proving that if T is a Taylor polynomial then we have the above equation. However, I can't seem to figure out how to make my way towards this proof as it is basically the asking for the other direction of the Taylor polynomial theorem.  I see a pattern between how the above limit is similar to what is the derivative of , but I need some help seeing the overall picture. I know we are done once I prove that: Any help is appreciated.","I f \in C^n(I,\mathbb R), x_0 \in I, T n \lim_{x\to a}\frac{f(x)-T(x)}{(x-x_0)^n}=0. f n x_0  f(x_0) T(x) = \sum_{k=0}^{n}\frac{f^{k}(x)}{k!}(x-x_0)^k","['analysis', 'taylor-expansion']"
96,show that $h$ is a differentiable and $h(-1)=h(0)=h'(0)=0$ and $h(1)=1$ then $h^{(3)}\geq 3$,show that  is a differentiable and  and  then,h h(-1)=h(0)=h'(0)=0 h(1)=1 h^{(3)}\geq 3,"If $h$ is an real function, differentiable three times on $[-1,1]$ , such that $h(-1)=h(0)=h'(0)=0$ and $h(1)=1$ . Prove that exist an real number $r\in(-1,1)$ such that $h^{(3)}(r)\geq 3$ For this I used Taylor's theorem so \begin{eqnarray} h(x)= h(a)+h'(a)(x-a)+\frac{h''(a)}{2!}(x-a)^{2}+\frac{h^{(3)}(a)}{3!}(x-a)^{3} \end{eqnarray} for $a\in[-1,1]$ , then, for $a=0$ \begin{eqnarray} h(x)= \frac{h''(0)}{2!}x^{2}+\frac{h^{(3)}(0)}{3!}x^{3} \end{eqnarray} and for $a=1$ , and $a=-1$ \begin{eqnarray} h(x)= 1+h'(1)(x-1)+\frac{h''(1)}{2!}(x-1)^{2}+\frac{h^{(3)}(1)}{3!}(x-1)^{3}\\ h(x)=h'(-1)(x+1)+\frac{h''(-1)}{2!}(x+1)^{2}+\frac{h^{(3)}(-1)}{3!}(x+1)^{3} \end{eqnarray} but I don't the way to continue. Do you know some hint to continue?","If is an real function, differentiable three times on , such that and . Prove that exist an real number such that For this I used Taylor's theorem so for , then, for and for , and but I don't the way to continue. Do you know some hint to continue?","h [-1,1] h(-1)=h(0)=h'(0)=0 h(1)=1 r\in(-1,1) h^{(3)}(r)\geq 3 \begin{eqnarray}
h(x)= h(a)+h'(a)(x-a)+\frac{h''(a)}{2!}(x-a)^{2}+\frac{h^{(3)}(a)}{3!}(x-a)^{3}
\end{eqnarray} a\in[-1,1] a=0 \begin{eqnarray}
h(x)= \frac{h''(0)}{2!}x^{2}+\frac{h^{(3)}(0)}{3!}x^{3}
\end{eqnarray} a=1 a=-1 \begin{eqnarray}
h(x)= 1+h'(1)(x-1)+\frac{h''(1)}{2!}(x-1)^{2}+\frac{h^{(3)}(1)}{3!}(x-1)^{3}\\
h(x)=h'(-1)(x+1)+\frac{h''(-1)}{2!}(x+1)^{2}+\frac{h^{(3)}(-1)}{3!}(x+1)^{3}
\end{eqnarray}","['real-analysis', 'calculus', 'analysis', 'taylor-expansion']"
97,Proving Cauchy Theorem,Proving Cauchy Theorem,,"This theorem says that a function $f$ is defined in the interval (a, + $\infty$ ) and it is bounded in every finite interval (a,b). Then it holds that: (assume that the limit of the RHS exists) $$\lim_{x\to \infty} \frac{f(x)}{x} = \lim_{x\to \infty}[f(x+1) - f(x)];$$ $$\lim_{x\to \infty} [f(x)]^{1/x} = \lim_{x\to \infty}\frac{f(x+1)}{f(x)}.$$ My question is how to prove the above equations. Any help will be appreciated.","This theorem says that a function is defined in the interval (a, + ) and it is bounded in every finite interval (a,b). Then it holds that: (assume that the limit of the RHS exists) My question is how to prove the above equations. Any help will be appreciated.",f \infty \lim_{x\to \infty} \frac{f(x)}{x} = \lim_{x\to \infty}[f(x+1) - f(x)]; \lim_{x\to \infty} [f(x)]^{1/x} = \lim_{x\to \infty}\frac{f(x+1)}{f(x)}.,['analysis']
98,Sum of the square harmonic series,Sum of the square harmonic series,,"I stumbled across the following series reviewing some HW from a few years ago $\sum_{i=1}^{n}\left(\sum_{j=i}^{n}\frac{1}{j}\right)^2$ i.e. $(\frac{1}{1}+\frac{1}{2}+\ldots+\frac{1}{n})^2+(\frac{1}{2}+\ldots+\frac{1}{n})^2+\ldots+(\frac{1}{n})^2$ This series equals $2n-\sum_{i=1}^{n}\frac{1}{i}$ , which I have confirmed with some code.  I am curious if anyone can give a hand in trying to show this relation.  So far, writing $\sum_{i=1}^{n}\frac{1}{i}$ as $S_n$ , I have rewritten the sum as $S_n^2+(S_n-S_1)^2+(S_n-S_2)^2+\ldots +(S_n-S_{n-1})^2$ But have been stuck at dead ends using this approach.  Any thoughts or hints would be greatly appreciated.","I stumbled across the following series reviewing some HW from a few years ago i.e. This series equals , which I have confirmed with some code.  I am curious if anyone can give a hand in trying to show this relation.  So far, writing as , I have rewritten the sum as But have been stuck at dead ends using this approach.  Any thoughts or hints would be greatly appreciated.",\sum_{i=1}^{n}\left(\sum_{j=i}^{n}\frac{1}{j}\right)^2 (\frac{1}{1}+\frac{1}{2}+\ldots+\frac{1}{n})^2+(\frac{1}{2}+\ldots+\frac{1}{n})^2+\ldots+(\frac{1}{n})^2 2n-\sum_{i=1}^{n}\frac{1}{i} \sum_{i=1}^{n}\frac{1}{i} S_n S_n^2+(S_n-S_1)^2+(S_n-S_2)^2+\ldots +(S_n-S_{n-1})^2,['analysis']
99,$\max_{k=1}^{n}|x_k| \xrightarrow[\text{}]{\text{$n \rightarrow \infty$}} \sup_{k=1}^{\infty}|x_k|$,n \rightarrow \infty,\max_{k=1}^{n}|x_k| \xrightarrow[\text{}]{\text{ }} \sup_{k=1}^{\infty}|x_k|,"I want to prove the following: Let $(x_n)$ be a bounded sequence, then $\max_{k=1}^{n}|x_k| \xrightarrow[\text{}]{\text{$n \rightarrow \infty$}} \sup_{k=1}^{\infty}|x_k|$ . My Calculations: Let $c:=\sup_{k=1}^{\infty}|x_k|$ . If $c$ is the supremum of $(x_n)$ , then (1) $\forall n \in \mathbb{N}: |x_n| \leq c$ (2) $\forall \epsilon>0 :\exists |x_j|$ such that $|x_j|>c-\epsilon$ Just to clarify it, I am looking at the sequence $(|x_n|)_{n \in \mathbb{N}}$ and NOT at the sequence $(x_n)_{n \in \mathbb{N}}$ . This is the reason why I get absolute values in (1) and (2). Now considering the inequality in (2): $|x_j|>c-\epsilon$ , we get $\epsilon > c- |x_j|$ I noticed $\epsilon$ and $c -|x_j|$ are both positive numbers. This means I can ""take the absolute value"" of the inequality and get $|c- |x_j||<\epsilon $ The inequality I am looking for is $|c-\max_{k=1}^n|x_k||  <\epsilon$ . Now there can be two cases: Case 1: $|x_j|$ is the maximum, i.e $|x_j|=\max_{k=1}^n|x_k|$ and $|x_{n+1}|>|x_j|$ But this just means the inequality still holds. Since $|x_{n+1}|>|x_j|$ it follows that $|c-|x_j||<|c-|x_{n+1}||<\epsilon$ Case 2: $|x_j|$ is the maximum, i.e $|x_j|=\max_{k=1}^n|x_k|$ and $|x_{n+1}|<|x_j|$ In this case nothing changes, since $|x_j|=\max_{k=1}^{n+1}|x_k|$ and the equality $|c- \max_{k=1}^{n+1}|x_k||=|c-|x_j||<\epsilon $ still holds. By this reasoning it follows that, for every $n \in \mathbb{N}$ , there exists a $\epsilon>0$ such that: $|\underbrace{sup_{k=1}^{\infty}|x_k|}_{=c} - \max_{k=1}^n|x_k||<\epsilon$ . The question I would like to ask is: Is this proof right? __________________________________________________________ Edit: One user pointed out that I did not show the definiton of convergence. In fact, I messed up a little bit in the last part. So now the second try: So I looked up the definition. Let $y_n$ be a sequence and $y \in \mathbb{R}$ . The sequence is convergent if for every $\epsilon>0$ , there exists a $N \in \mathbb{N}$ such that $|y_n-y|<\epsilon$ for all $n>N$ So let $\epsilon>0$ . The property of the supremum says that there exists is a $x_j$ such that $|sup_{k=1}^\infty{|x_k|}-\underbrace{max_{k=1}^{N}{|x_k|}}_{=x_j}|<\epsilon_1$ . This means, there exists a $N \in \mathbb{N}$ such that $x_j=max_{k=1}^{N}|x_k|$ . Choosing N such that the $\epsilon_1$ I get from the supremum is $\epsilon_1 \leq \epsilon$ , then the inequality $|sup_{k=1}^\infty{|x_k|}-max_{k=1}^{N}{|x_k|}|<\epsilon$ holds. Now if $|x_{N+1}|>|x_j|$ , the inequality still holds (Case 1). If $|x_N+1|<|x_j|$ (Case 2), $ x_j=max_{k=1}^{N+1}{|x_k|}$ and the equality still holds. And by induction it is true for all $n \geq N$ . Meaning, $\forall \epsilon >0 :\exists N \in \mathbb{N}$ such that $|sup_{k=1}^\infty{|x_k|}-max_{k=1}^{N}{|x_k|}|<\epsilon$ for all $n \geq N$","I want to prove the following: Let be a bounded sequence, then . My Calculations: Let . If is the supremum of , then (1) (2) such that Just to clarify it, I am looking at the sequence and NOT at the sequence . This is the reason why I get absolute values in (1) and (2). Now considering the inequality in (2): , we get I noticed and are both positive numbers. This means I can ""take the absolute value"" of the inequality and get The inequality I am looking for is . Now there can be two cases: Case 1: is the maximum, i.e and But this just means the inequality still holds. Since it follows that Case 2: is the maximum, i.e and In this case nothing changes, since and the equality still holds. By this reasoning it follows that, for every , there exists a such that: . The question I would like to ask is: Is this proof right? __________________________________________________________ Edit: One user pointed out that I did not show the definiton of convergence. In fact, I messed up a little bit in the last part. So now the second try: So I looked up the definition. Let be a sequence and . The sequence is convergent if for every , there exists a such that for all So let . The property of the supremum says that there exists is a such that . This means, there exists a such that . Choosing N such that the I get from the supremum is , then the inequality holds. Now if , the inequality still holds (Case 1). If (Case 2), and the equality still holds. And by induction it is true for all . Meaning, such that for all",(x_n) \max_{k=1}^{n}|x_k| \xrightarrow[\text{}]{\text{n \rightarrow \infty}} \sup_{k=1}^{\infty}|x_k| c:=\sup_{k=1}^{\infty}|x_k| c (x_n) \forall n \in \mathbb{N}: |x_n| \leq c \forall \epsilon>0 :\exists |x_j| |x_j|>c-\epsilon (|x_n|)_{n \in \mathbb{N}} (x_n)_{n \in \mathbb{N}} |x_j|>c-\epsilon \epsilon > c- |x_j| \epsilon c -|x_j| |c- |x_j||<\epsilon  |c-\max_{k=1}^n|x_k||  <\epsilon |x_j| |x_j|=\max_{k=1}^n|x_k| |x_{n+1}|>|x_j| |x_{n+1}|>|x_j| |c-|x_j||<|c-|x_{n+1}||<\epsilon |x_j| |x_j|=\max_{k=1}^n|x_k| |x_{n+1}|<|x_j| |x_j|=\max_{k=1}^{n+1}|x_k| |c- \max_{k=1}^{n+1}|x_k||=|c-|x_j||<\epsilon  n \in \mathbb{N} \epsilon>0 |\underbrace{sup_{k=1}^{\infty}|x_k|}_{=c} - \max_{k=1}^n|x_k||<\epsilon y_n y \in \mathbb{R} \epsilon>0 N \in \mathbb{N} |y_n-y|<\epsilon n>N \epsilon>0 x_j |sup_{k=1}^\infty{|x_k|}-\underbrace{max_{k=1}^{N}{|x_k|}}_{=x_j}|<\epsilon_1 N \in \mathbb{N} x_j=max_{k=1}^{N}|x_k| \epsilon_1 \epsilon_1 \leq \epsilon |sup_{k=1}^\infty{|x_k|}-max_{k=1}^{N}{|x_k|}|<\epsilon |x_{N+1}|>|x_j| |x_N+1|<|x_j|  x_j=max_{k=1}^{N+1}{|x_k|} n \geq N \forall \epsilon >0 :\exists N \in \mathbb{N} |sup_{k=1}^\infty{|x_k|}-max_{k=1}^{N}{|x_k|}|<\epsilon n \geq N,"['real-analysis', 'analysis', 'supremum-and-infimum']"
