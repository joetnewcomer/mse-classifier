,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"If $f(x)=\int_{x-1}^x f(s)ds$, is $f$ constant? Periodic?","If , is  constant? Periodic?",f(x)=\int_{x-1}^x f(s)ds f,"I was thinking of periodic functions, and in particular the following type of condition: If a function $f:\mathbb{R}\to\mathbb{R}$ always ""tends to its average"", then it should be periodic. To make things more formal, by ""tending to the average"" we could say something like $f(x)=\int_{x-1}^x f(s)ds$ . This is only the average depending on a previous time interval of length $1$ , but it seems an interesting enough property. However, the only type of functions which I could find that satisfies this property are the constant ones! Question: If $f:\mathbb{R}\to\mathbb{R}$ is continuous (or more generaly measurable) and $f(x)=\int_{x-1}^x f(s)ds$ for (almost) every $x\in\mathbb{R}$ , then is $f$ constant (a.e.)? Periodic (a.e.)? Here is a first try for $C^1$ functions (see edit below!): If $f$ is $C^1$ and $x$ is fixed, we can use Taylor expansion $f(s)=f(x)+O(s-x)$ (and similarly for $x-1$ ) to obtain \begin{align*} f(x+t)-f(x)&=\int_x^{x+t}f(s)ds-\int_{x-1}^{x-1+t}f(s)ds\\ &=\int_x^{x+t}f(x)+O(s-x)ds-\int_{x-1}^{x-1+t}f(x-1)+O(s-x+1)ds\\ &=t(f(x)-f(x-1))+O(t^2), \end{align*} so $f'(x)=f(x)-f(x-1)$ . This is obviously true if $f$ is constant, but the converse is not clear to me at the moment. Edit : From a comment and answer below, the equation $f'(x)=f(x)-f(x-1)$ has non-periodic solutions on $\mathbb{R}\setminus\mathbb{Z}$ , so this should not be the way to go for $C^1$ functions. However, even in this case it is not clear that any solution of this equation will satisfy $f(x)=\int_{x-1}^x f(s)ds$ , which is the question: All I can obtain, in principle, is $f(x)-f(x-1)=\int_x^{x-1}f(s)ds-\int_{x-2}^{x-1}f(s)ds$ .","I was thinking of periodic functions, and in particular the following type of condition: If a function always ""tends to its average"", then it should be periodic. To make things more formal, by ""tending to the average"" we could say something like . This is only the average depending on a previous time interval of length , but it seems an interesting enough property. However, the only type of functions which I could find that satisfies this property are the constant ones! Question: If is continuous (or more generaly measurable) and for (almost) every , then is constant (a.e.)? Periodic (a.e.)? Here is a first try for functions (see edit below!): If is and is fixed, we can use Taylor expansion (and similarly for ) to obtain so . This is obviously true if is constant, but the converse is not clear to me at the moment. Edit : From a comment and answer below, the equation has non-periodic solutions on , so this should not be the way to go for functions. However, even in this case it is not clear that any solution of this equation will satisfy , which is the question: All I can obtain, in principle, is .","f:\mathbb{R}\to\mathbb{R} f(x)=\int_{x-1}^x f(s)ds 1 f:\mathbb{R}\to\mathbb{R} f(x)=\int_{x-1}^x f(s)ds x\in\mathbb{R} f C^1 f C^1 x f(s)=f(x)+O(s-x) x-1 \begin{align*}
f(x+t)-f(x)&=\int_x^{x+t}f(s)ds-\int_{x-1}^{x-1+t}f(s)ds\\
&=\int_x^{x+t}f(x)+O(s-x)ds-\int_{x-1}^{x-1+t}f(x-1)+O(s-x+1)ds\\
&=t(f(x)-f(x-1))+O(t^2),
\end{align*} f'(x)=f(x)-f(x-1) f f'(x)=f(x)-f(x-1) \mathbb{R}\setminus\mathbb{Z} C^1 f(x)=\int_{x-1}^x f(s)ds f(x)-f(x-1)=\int_x^{x-1}f(s)ds-\int_{x-2}^{x-1}f(s)ds","['calculus', 'integration']"
1,Seeing that $\lim_{x \to \infty} \sum (-x)^n/n! = 0$,Seeing that,\lim_{x \to \infty} \sum (-x)^n/n! = 0,Is there any way to see directly from the power series that $$\lim_{x \to \infty} \sum_{n=0}^\infty \frac{(-x)^n}{n!} = 0$$ ?  I realize that $\displaystyle{\lim_{x \to \infty} e^{-x} = 0}$ .  That's not what I'm asking.,Is there any way to see directly from the power series that ?  I realize that .  That's not what I'm asking.,\lim_{x \to \infty} \sum_{n=0}^\infty \frac{(-x)^n}{n!} = 0 \displaystyle{\lim_{x \to \infty} e^{-x} = 0},"['calculus', 'sequences-and-series', 'limits', 'power-series']"
2,Trouble understanding how the Transfer Principle is applied for the Extreme Value theorem.,Trouble understanding how the Transfer Principle is applied for the Extreme Value theorem.,,"I am reading Keisler's Elementary Calculus (which can be downloaded here ). I am having trouble understanding his proof sketch of Extreme Value Theorem and how he is applying the Transfer Principle. For reference, he defines the ""Transfer Principle"" as: Every real statement that holds for one or more particular functions holds for the hyperreal natural extension of these functions. On page 164 (using left corner numbering) of the book he provides the following ""sketch"": I understand the counter examples and I am able to understand the issues with them using standard tools. I don't understand, however, how one can immediately utilize the Transfer Principle. It is not immediately obvious to me that ""there is a partition point $a + K\delta$ at which $f(a + K\delta)$ has the largest value."" To elaborate, the proof seems circular. In trying to to ""expand"" the sketch to be more precise. I ended up writing instead of: By the Transfer Principle, there is a partition point $a + K\delta$ at which $f(a + K\delta)$ has the largest value. To: Applying the Transfer Principle to the Extreme Value Theorem we see that the Extreme Value holds for hyperreals as well. Hence, there is a partition point $a + K\delta$ at which $f(a + K\delta)$ has the largest value. But this relies on a proof of the Extreme Value Theorem for reals. Hopefully what I am saying makes sense, please ask for any clarification.","I am reading Keisler's Elementary Calculus (which can be downloaded here ). I am having trouble understanding his proof sketch of Extreme Value Theorem and how he is applying the Transfer Principle. For reference, he defines the ""Transfer Principle"" as: Every real statement that holds for one or more particular functions holds for the hyperreal natural extension of these functions. On page 164 (using left corner numbering) of the book he provides the following ""sketch"": I understand the counter examples and I am able to understand the issues with them using standard tools. I don't understand, however, how one can immediately utilize the Transfer Principle. It is not immediately obvious to me that ""there is a partition point at which has the largest value."" To elaborate, the proof seems circular. In trying to to ""expand"" the sketch to be more precise. I ended up writing instead of: By the Transfer Principle, there is a partition point at which has the largest value. To: Applying the Transfer Principle to the Extreme Value Theorem we see that the Extreme Value holds for hyperreals as well. Hence, there is a partition point at which has the largest value. But this relies on a proof of the Extreme Value Theorem for reals. Hopefully what I am saying makes sense, please ask for any clarification.",a + K\delta f(a + K\delta) a + K\delta f(a + K\delta) a + K\delta f(a + K\delta),"['calculus', 'nonstandard-analysis', 'extreme-value-theorem']"
3,A spherical ball of salt is dissolving in water,A spherical ball of salt is dissolving in water,,A spherical ball of salt is dissolving in water in such a way that the rate of decrease in volume at any instant is proportional to the surface. Prove that the radius is decreasing at a constant rate. My Approach: $$\dfrac {dV}{dt}\propto Surface (S)$$ $$\dfrac {dV}{dt}=k.S$$ where $k$ is a proportionality constant.  $$\dfrac {dV}{dt}=k.4\pi r^2$$ How do I proceed?,A spherical ball of salt is dissolving in water in such a way that the rate of decrease in volume at any instant is proportional to the surface. Prove that the radius is decreasing at a constant rate. My Approach: $$\dfrac {dV}{dt}\propto Surface (S)$$ $$\dfrac {dV}{dt}=k.S$$ where $k$ is a proportionality constant.  $$\dfrac {dV}{dt}=k.4\pi r^2$$ How do I proceed?,,"['calculus', 'derivatives']"
4,"How to calculate $\int{\frac {x^2}{ ( x\cos x -\sin x ) ^2}}\,{\rm d}x$?",How to calculate ?,"\int{\frac {x^2}{ ( x\cos x -\sin x ) ^2}}\,{\rm d}x","When I want to calculate $$\int{\frac {x^2}{ ( x\cos x -\sin x ) ^2}}\,{\rm d}x$$ I have tested with software and get $${\frac {x\sin \left( x \right) +\cos \left( x \right) }{x\cos \left( x  \right) -\sin \left( x \right) }}$$ But I can not come to this conclusion, neither using integration by parts, nor using trigonometric identities, nor multiplying by their conjugate, Even by rational trigonometric substitution. I do not know what else to try. Could you give me any suggestions?","When I want to calculate $$\int{\frac {x^2}{ ( x\cos x -\sin x ) ^2}}\,{\rm d}x$$ I have tested with software and get $${\frac {x\sin \left( x \right) +\cos \left( x \right) }{x\cos \left( x  \right) -\sin \left( x \right) }}$$ But I can not come to this conclusion, neither using integration by parts, nor using trigonometric identities, nor multiplying by their conjugate, Even by rational trigonometric substitution. I do not know what else to try. Could you give me any suggestions?",,"['calculus', 'indefinite-integrals', 'trigonometric-integrals']"
5,Is there a pattern to expression for the nested sums of the first $n$ terms of an expression? [duplicate],Is there a pattern to expression for the nested sums of the first  terms of an expression? [duplicate],n,"This question already has answers here : Finite Sum of Power? (2 answers) Proof of the hockey stick/Zhu Shijie identity $\sum\limits_{t=0}^n \binom tk = \binom{n+1}{k+1}$ (20 answers) Closed 7 years ago . Apologies for the confusing title but I couldn't think of a better way to phrase it. What I'm talking about is this: $$ \sum_{i=1}^n \;i = \frac{1}{2}n \left(n+1\right)$$ $$ \sum_{i=1}^n \; \frac{1}{2}i\left(i+1\right) = \frac{1}{6}n\left(n+1\right)\left(n+2\right) $$ $$ \sum_{i=1}^n \; \frac{1}{6}i\left(i+1\right)\left(i+2\right) = \frac{1}{24}n\left(n+1\right)\left(n+2\right)\left(n+3\right) $$ We see that this seems to indicate: $$ \sum_{n_m=1}^{n}\sum_{n_{m-1}=1}^{n_m}\ldots \sum_{n_1=1}^{n_2} \; n_1 = \frac{1}{m!}\prod_{k = 0}^{m}(n+k) $$ Is this a known result? If so how would you go about proving it? I have tried a few inductive arguments but because I couldn't express the intermediate expressions nicely, I didn't really get anywhere.","This question already has answers here : Finite Sum of Power? (2 answers) Proof of the hockey stick/Zhu Shijie identity $\sum\limits_{t=0}^n \binom tk = \binom{n+1}{k+1}$ (20 answers) Closed 7 years ago . Apologies for the confusing title but I couldn't think of a better way to phrase it. What I'm talking about is this: $$ \sum_{i=1}^n \;i = \frac{1}{2}n \left(n+1\right)$$ $$ \sum_{i=1}^n \; \frac{1}{2}i\left(i+1\right) = \frac{1}{6}n\left(n+1\right)\left(n+2\right) $$ $$ \sum_{i=1}^n \; \frac{1}{6}i\left(i+1\right)\left(i+2\right) = \frac{1}{24}n\left(n+1\right)\left(n+2\right)\left(n+3\right) $$ We see that this seems to indicate: $$ \sum_{n_m=1}^{n}\sum_{n_{m-1}=1}^{n_m}\ldots \sum_{n_1=1}^{n_2} \; n_1 = \frac{1}{m!}\prod_{k = 0}^{m}(n+k) $$ Is this a known result? If so how would you go about proving it? I have tried a few inductive arguments but because I couldn't express the intermediate expressions nicely, I didn't really get anywhere.",,"['calculus', 'summation']"
6,Find the limit $\lim _{ x \to 0} \frac{\sqrt[3]{1+6x+3x^2+3x^3+3x^4}-\sqrt[4]{1+8x+4x^2+4x^3-2x^4}}{6x^2}$,Find the limit,\lim _{ x \to 0} \frac{\sqrt[3]{1+6x+3x^2+3x^3+3x^4}-\sqrt[4]{1+8x+4x^2+4x^3-2x^4}}{6x^2},"I have been trying to find the limit,  $$\lim _{ x \to 0} \frac{\sqrt[3]{1+6x+3x^2+3x^3+3x^4}-\sqrt[4]{1+8x+4x^2+4x^3-2x^4}}{6x^2}$$ and sort of succeeded. But my $0$ answer doesn't converge with what Wolfram says which is $1/3$. Therefore, I would really appreciate it, if you could give me the right answer and drop a hint on the solution.","I have been trying to find the limit,  $$\lim _{ x \to 0} \frac{\sqrt[3]{1+6x+3x^2+3x^3+3x^4}-\sqrt[4]{1+8x+4x^2+4x^3-2x^4}}{6x^2}$$ and sort of succeeded. But my $0$ answer doesn't converge with what Wolfram says which is $1/3$. Therefore, I would really appreciate it, if you could give me the right answer and drop a hint on the solution.",,"['calculus', 'limits']"
7,"What does $\frac{x^3}{9}\bigg|_0^1$ mean, and how should it be spoken?","What does  mean, and how should it be spoken?",\frac{x^3}{9}\bigg|_0^1,"$$\frac{x^3}{9}\Bigg|_0^1$$ The vertical line above: what does it mean, and how would I state this whole structure in spoken words, so that a screen reader would be able to read it aloud correctly?","$$\frac{x^3}{9}\Bigg|_0^1$$ The vertical line above: what does it mean, and how would I state this whole structure in spoken words, so that a screen reader would be able to read it aloud correctly?",,"['calculus', 'integration', 'algebra-precalculus', 'notation']"
8,Find $f'(1)$ if $f$ is continuous and such that $f (f (x))=1+x$ for every $x$,Find  if  is continuous and such that  for every,f'(1) f f (f (x))=1+x x,If $ f $ is a continuous function satisfying $f (f (x))=1+x$ find $f'(1)$. I just guessed $f (x)=x+1/2$.Any formal method for this sum ?,If $ f $ is a continuous function satisfying $f (f (x))=1+x$ find $f'(1)$. I just guessed $f (x)=x+1/2$.Any formal method for this sum ?,,['calculus']
9,"Find $\lim\limits_{n\to\infty}{\left(1+\frac{1}{n^k}\right)\left(1+\frac{2}{n^k}\right)\cdots\left(1+\frac{n}{n^k}\right) }$ for $k=1, 2, 3, \cdots$",Find  for,"\lim\limits_{n\to\infty}{\left(1+\frac{1}{n^k}\right)\left(1+\frac{2}{n^k}\right)\cdots\left(1+\frac{n}{n^k}\right) } k=1, 2, 3, \cdots","First of all, I already searched Google, math.stackexchange.com... I know  $$ \lim_{n\rightarrow\infty} \left( 1+ \frac{1}{n}      \right)      ^n=e$$ That is $$ \lim_{n\rightarrow\infty} \underbrace{\left(1+\frac{1}{n}\right)\left(1+\frac{1}{n}\right)\cdots\left(1+\frac{1}{n}\right) }_{\text{n times}}     =e$$ $$$$ At this time, I made some problems modifying above. $$ \lim_{n\rightarrow\infty} {\left(1+\frac{1}{n}\right)\left(1+\frac{2}{n}\right)\cdots\left(1+\frac{n}{n}\right) } =f(1)   $$ $$ \lim_{n\rightarrow\infty} {\left(1+\frac{1}{n^2}\right)\left(1+\frac{2}{n^2}\right)\cdots\left(1+\frac{n}{n^2}\right) }    =f(2)$$ $$ \lim_{n\rightarrow\infty} {\left(1+\frac{1}{n^3}\right)\left(1+\frac{2}{n^3}\right)\cdots\left(1+\frac{n}{n^3}\right) }    =f(3)$$ $$ \lim_{n\rightarrow\infty} {\left(1+\frac{1}{n^k}\right)\left(1+\frac{2}{n^k}\right)\cdots\left(1+\frac{n}{n^k}\right) }    =f(k)$$ $$$$ After thinking above, I feel I'm spinning my wheels with these limit problems. Eventually, I searched wolframalpha. And the next images are results of wolfram. (I take a LOG, because I don't know COMMAND of n-times product.) $$$$ These result (if we trust wolframalpha) say $$f(1)=\infty$$ $$f(2)=\sqrt{e}$$ $$f(3)=1$$ $$f(30)=1$$ NOW, I'm asking you for help. I'd like to know how can I find $f(k)$ (for $k=1,2,3,4, \cdots$ ). I already used Riemann sum, taking Log...  but I didn't get anyhing. ;-( Thank you for your attention to this matter. ----------- EDIT --------------------------------- The result for $f(1), f(2), f(3), f(30)$ is an achievement of Wolframalpha, not me. I'm still spinning my wheel, $f(1), f(2), f(3)$, and so on...","First of all, I already searched Google, math.stackexchange.com... I know  $$ \lim_{n\rightarrow\infty} \left( 1+ \frac{1}{n}      \right)      ^n=e$$ That is $$ \lim_{n\rightarrow\infty} \underbrace{\left(1+\frac{1}{n}\right)\left(1+\frac{1}{n}\right)\cdots\left(1+\frac{1}{n}\right) }_{\text{n times}}     =e$$ $$$$ At this time, I made some problems modifying above. $$ \lim_{n\rightarrow\infty} {\left(1+\frac{1}{n}\right)\left(1+\frac{2}{n}\right)\cdots\left(1+\frac{n}{n}\right) } =f(1)   $$ $$ \lim_{n\rightarrow\infty} {\left(1+\frac{1}{n^2}\right)\left(1+\frac{2}{n^2}\right)\cdots\left(1+\frac{n}{n^2}\right) }    =f(2)$$ $$ \lim_{n\rightarrow\infty} {\left(1+\frac{1}{n^3}\right)\left(1+\frac{2}{n^3}\right)\cdots\left(1+\frac{n}{n^3}\right) }    =f(3)$$ $$ \lim_{n\rightarrow\infty} {\left(1+\frac{1}{n^k}\right)\left(1+\frac{2}{n^k}\right)\cdots\left(1+\frac{n}{n^k}\right) }    =f(k)$$ $$$$ After thinking above, I feel I'm spinning my wheels with these limit problems. Eventually, I searched wolframalpha. And the next images are results of wolfram. (I take a LOG, because I don't know COMMAND of n-times product.) $$$$ These result (if we trust wolframalpha) say $$f(1)=\infty$$ $$f(2)=\sqrt{e}$$ $$f(3)=1$$ $$f(30)=1$$ NOW, I'm asking you for help. I'd like to know how can I find $f(k)$ (for $k=1,2,3,4, \cdots$ ). I already used Riemann sum, taking Log...  but I didn't get anyhing. ;-( Thank you for your attention to this matter. ----------- EDIT --------------------------------- The result for $f(1), f(2), f(3), f(30)$ is an achievement of Wolframalpha, not me. I'm still spinning my wheel, $f(1), f(2), f(3)$, and so on...",,"['calculus', 'sequences-and-series', 'limits', 'infinite-product', 'riemann-sum']"
10,How Differential got into calculus,How Differential got into calculus,,"I am confused what differentials are and how they become related to derivatives and integrals, as neither my textbook explains them nor my teacher. What we do to solve differential equations is to convert derivative to a ""differential form"" like this: $$\frac{dy}{dx}= 2x$$ $$dy = 2xdx$$ When we substitute to solve integrals, differentials also come up there. I'm asking: How did we get differentials, what is their physical meaning (like derivative means slope, Integration results in area under curve). I would like someone gave me a little historical aspect. When did differentials come into the scene of calculus?","I am confused what differentials are and how they become related to derivatives and integrals, as neither my textbook explains them nor my teacher. What we do to solve differential equations is to convert derivative to a ""differential form"" like this: $$\frac{dy}{dx}= 2x$$ $$dy = 2xdx$$ When we substitute to solve integrals, differentials also come up there. I'm asking: How did we get differentials, what is their physical meaning (like derivative means slope, Integration results in area under curve). I would like someone gave me a little historical aspect. When did differentials come into the scene of calculus?",,['calculus']
11,Sum the infinite series,Sum the infinite series,,"How to solve this: \begin{equation*} \sum_{n=1}^{\infty }\left[ \frac{1\cdot 3\cdot 5\cdots \left( 2n-1\right) }{ 2\cdot 4\cdot 6\cdots 2n}\right] ^{3} \end{equation*} I can make the bracket thing, $\left[ C(2n,n)/4^{n}\right] ^{3}$, but how to proceed now.","How to solve this: \begin{equation*} \sum_{n=1}^{\infty }\left[ \frac{1\cdot 3\cdot 5\cdots \left( 2n-1\right) }{ 2\cdot 4\cdot 6\cdots 2n}\right] ^{3} \end{equation*} I can make the bracket thing, $\left[ C(2n,n)/4^{n}\right] ^{3}$, but how to proceed now.",,"['calculus', 'sequences-and-series']"
12,How can I check if my derivative for an implicit function is correct?,How can I check if my derivative for an implicit function is correct?,,For explicit functions I can calculate the derivative at a certian point using the original function: $$\frac{f(1+0.1) - f(1)}{0.1}$$ And then use $\frac{d}{dx}f(1)$ to check if the function is correct. But what can I do for implicit funcions? how can I calculate the change then compare it with my derivative function Edit: for example if I was asked to differentiate $f(x)=x^2+\tanh(x)$ and if I am unsure about my answer I could type in on the calculator: $$\frac{f(5+0.000001) - f(5)}{0.000001}$$ and then check my $\frac{d}{dx}f(5)$ they should be approximately equal. My question is if I have an implicit function like $$xy^3=\tan(x+2y)-(x^2-1)$$ and after I differentiate it how can I check if it is correct,For explicit functions I can calculate the derivative at a certian point using the original function: $$\frac{f(1+0.1) - f(1)}{0.1}$$ And then use $\frac{d}{dx}f(1)$ to check if the function is correct. But what can I do for implicit funcions? how can I calculate the change then compare it with my derivative function Edit: for example if I was asked to differentiate $f(x)=x^2+\tanh(x)$ and if I am unsure about my answer I could type in on the calculator: $$\frac{f(5+0.000001) - f(5)}{0.000001}$$ and then check my $\frac{d}{dx}f(5)$ they should be approximately equal. My question is if I have an implicit function like $$xy^3=\tan(x+2y)-(x^2-1)$$ and after I differentiate it how can I check if it is correct,,"['calculus', 'derivatives']"
13,Asymptotic behaviour of a sequence with finite sum,Asymptotic behaviour of a sequence with finite sum,,I'm working on$$ u_n=\sum_{k=0}^{n}\frac{1}{k^2+(n-k)^2} $$ to find out an asymptotic behaviour as $n \rightarrow +\infty.$ I've already seen that $u_n$ tends to $0$. Thanks for your help.,I'm working on$$ u_n=\sum_{k=0}^{n}\frac{1}{k^2+(n-k)^2} $$ to find out an asymptotic behaviour as $n \rightarrow +\infty.$ I've already seen that $u_n$ tends to $0$. Thanks for your help.,,"['calculus', 'sequences-and-series']"
14,Proving that a definition of e is unique,Proving that a definition of e is unique,,"We can define $e$ as the number such that $\lim_{h \to 0} \frac{e^h-1}{h}=1$.  However, of course we can only define $e$ this way if it is unique, i.e., there is no other value $c$ for which that is a true statement. Could someone prove this uniqueness for me? I am tutoring a calculus student and he asked me something to this effect and I couldn't see why. Thanks!","We can define $e$ as the number such that $\lim_{h \to 0} \frac{e^h-1}{h}=1$.  However, of course we can only define $e$ this way if it is unique, i.e., there is no other value $c$ for which that is a true statement. Could someone prove this uniqueness for me? I am tutoring a calculus student and he asked me something to this effect and I couldn't see why. Thanks!",,"['calculus', 'exponential-function']"
15,The Geodesics of a Sphere.,The Geodesics of a Sphere.,,"I need to find the geodesics of a sphere. Then in polar coordinates $$x=a \sin\theta \cos\phi \\y=a \sin\theta \sin\phi\\   z=a\cos\theta$$ Then $ds^2=dx^2+dy^2+dz^2$. Can someone please tell me how is $ds^2=a^2(d\theta^2+\sin^2\theta \, d\phi ^2)$ obtained. I don't know much about spherical coordinates.","I need to find the geodesics of a sphere. Then in polar coordinates $$x=a \sin\theta \cos\phi \\y=a \sin\theta \sin\phi\\   z=a\cos\theta$$ Then $ds^2=dx^2+dy^2+dz^2$. Can someone please tell me how is $ds^2=a^2(d\theta^2+\sin^2\theta \, d\phi ^2)$ obtained. I don't know much about spherical coordinates.",,['calculus']
16,Why does finding the $x$ that maximizes $\ln(f(x))$ is the same as finding the $x$ that maximizes $f(x)$?,Why does finding the  that maximizes  is the same as finding the  that maximizes ?,x \ln(f(x)) x f(x),"I'm reading about maximum likelihood here . In the last paragraph of the first page, it says: Why does the value of $p$ that maximizes $\log L(p;3)$ is the same $p$ that maximizes $L(p;3)$. The fact that it mentions that log is an increasing function does not help me understand it at all. Does this method work for any increasing function? I've covered single and two variable optimization but have not come across this fact until now. I'd like to understand why it must be true and read more about its explanation. So, I would also appreciate it if someone can provide some links to it (I don't know what term to search).","I'm reading about maximum likelihood here . In the last paragraph of the first page, it says: Why does the value of $p$ that maximizes $\log L(p;3)$ is the same $p$ that maximizes $L(p;3)$. The fact that it mentions that log is an increasing function does not help me understand it at all. Does this method work for any increasing function? I've covered single and two variable optimization but have not come across this fact until now. I'd like to understand why it must be true and read more about its explanation. So, I would also appreciate it if someone can provide some links to it (I don't know what term to search).",,"['calculus', 'optimization', 'estimation']"
17,"If $f(a) = g(a)$ and $f'(x) < g'(x)$ for all $x \in (a,b)$, then $f(b) < g(b)$","If  and  for all , then","f(a) = g(a) f'(x) < g'(x) x \in (a,b) f(b) < g(b)","Assume that $f$ and $g$ are continuous on $[a, b]$ and differentiable on $(a, b)$. Prove that if $f(a) = g(a)$ and $f'(x) < g'(x)$ for all $x \in (a,b)$, then $f(b) < g(b)$. I understand that if $f$ and $g$ start at the same point, and $g$ increases at a faster rate than $f$, it will have a bigger end value. I'm just not sure how to prove it. Consider $h(x)=  g(x) - f (x)$ (this is the space between the two functions) $$\begin{align*} h(a) &= g(a) - f(a)\\ h(a) &= 0	&(\text{they are in the same spot in the beginning}) \end{align*}$$ $$\begin{align*} h'(x) &= \frac{g'(x) - f'(x)}{g(x) - f(x)}\\ h'(x) &> 0	&(\text{since } g'(x)> f'(x)) \end{align*}$$ so $g(x) - f(x) > 0 \implies g(x) > f(x)$? Is this correct?","Assume that $f$ and $g$ are continuous on $[a, b]$ and differentiable on $(a, b)$. Prove that if $f(a) = g(a)$ and $f'(x) < g'(x)$ for all $x \in (a,b)$, then $f(b) < g(b)$. I understand that if $f$ and $g$ start at the same point, and $g$ increases at a faster rate than $f$, it will have a bigger end value. I'm just not sure how to prove it. Consider $h(x)=  g(x) - f (x)$ (this is the space between the two functions) $$\begin{align*} h(a) &= g(a) - f(a)\\ h(a) &= 0	&(\text{they are in the same spot in the beginning}) \end{align*}$$ $$\begin{align*} h'(x) &= \frac{g'(x) - f'(x)}{g(x) - f(x)}\\ h'(x) &> 0	&(\text{since } g'(x)> f'(x)) \end{align*}$$ so $g(x) - f(x) > 0 \implies g(x) > f(x)$? Is this correct?",,"['calculus', 'derivatives']"
18,Prove that a given recursion sequence converges,Prove that a given recursion sequence converges,,I'm given: $$\begin{align*} x_1&=\frac32\\\\ x_{n+1}&=\frac3{4-x_n} \end{align*}$$ How do I go about to formally prove the sequence converges and show it? Thanks in advance.,I'm given: $$\begin{align*} x_1&=\frac32\\\\ x_{n+1}&=\frac3{4-x_n} \end{align*}$$ How do I go about to formally prove the sequence converges and show it? Thanks in advance.,,"['calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
19,How find that $\left(\frac{x}{1-x^2}+\frac{3x^3}{1-x^6}+\frac{5x^5}{1-x^{10}}+\frac{7x^7}{1-x^{14}}+\cdots\right)^2=\sum_{i=0}^{\infty}a_{i}x^i$,How find that,\left(\frac{x}{1-x^2}+\frac{3x^3}{1-x^6}+\frac{5x^5}{1-x^{10}}+\frac{7x^7}{1-x^{14}}+\cdots\right)^2=\sum_{i=0}^{\infty}a_{i}x^i,let $$\left(\dfrac{x}{1-x^2}+\dfrac{3x^3}{1-x^6}+\dfrac{5x^5}{1-x^{10}}+\dfrac{7x^7}{1-x^{14}}+\cdots\right)^2=\sum_{i=0}^{\infty}a_{i}x^i$$ How find the $a_{2^n}=?$ my idea:let $$\dfrac{nx^n}{1-x^{2n}}=nx^n(1+x^{2n}+x^{4n}+\cdots+x^{2kn}+\cdots)=n\sum_{i=0}^{\infty}x^{(2k+1)n}$$ Thank you everyone,let $$\left(\dfrac{x}{1-x^2}+\dfrac{3x^3}{1-x^6}+\dfrac{5x^5}{1-x^{10}}+\dfrac{7x^7}{1-x^{14}}+\cdots\right)^2=\sum_{i=0}^{\infty}a_{i}x^i$$ How find the $a_{2^n}=?$ my idea:let $$\dfrac{nx^n}{1-x^{2n}}=nx^n(1+x^{2n}+x^{4n}+\cdots+x^{2kn}+\cdots)=n\sum_{i=0}^{\infty}x^{(2k+1)n}$$ Thank you everyone,,"['calculus', 'combinatorics', 'discrete-mathematics', 'special-functions']"
20,Use of parenthesis in lambda calculus,Use of parenthesis in lambda calculus,,"As a summer project I am trying to learn lambda calculus. I am not that good with math but I have learned myself several programming languages and somehow got the idea that learning lambda calculus would be somewhat like learning another programming language (you are welcome to correct me on that one). I have used lambda expressions in languages like F# and C#. But I am learning lambda calculus with the book ""Lambda calculus and combinators: An introduction"". In this book we are shown several examples of lambda expressions that look really familiar, but they continue the book with parenthesis omitted. The first excersise is to fill in parenthesis and lambda-symbols in lambda expressions that have been omitted. My question is: What are the ""rules"" for use of parentheses in lambda expressions? Example: $xyz(yx)$ becomes $(((xy)z)(yx))$. I can't seem to figure out what determines where the parenthesis goes...","As a summer project I am trying to learn lambda calculus. I am not that good with math but I have learned myself several programming languages and somehow got the idea that learning lambda calculus would be somewhat like learning another programming language (you are welcome to correct me on that one). I have used lambda expressions in languages like F# and C#. But I am learning lambda calculus with the book ""Lambda calculus and combinators: An introduction"". In this book we are shown several examples of lambda expressions that look really familiar, but they continue the book with parenthesis omitted. The first excersise is to fill in parenthesis and lambda-symbols in lambda expressions that have been omitted. My question is: What are the ""rules"" for use of parentheses in lambda expressions? Example: $xyz(yx)$ becomes $(((xy)z)(yx))$. I can't seem to figure out what determines where the parenthesis goes...",,"['calculus', 'lambda-calculus']"
21,Show that $\displaystyle \sum_{k \ge 0}\frac{1}{k^2+k-\alpha} = \frac{\pi}{\sqrt{4\alpha+1}}\tan\left(\frac{1}{2}\pi\sqrt{4\alpha+1}\right)$,Show that,\displaystyle \sum_{k \ge 0}\frac{1}{k^2+k-\alpha} = \frac{\pi}{\sqrt{4\alpha+1}}\tan\left(\frac{1}{2}\pi\sqrt{4\alpha+1}\right),"For $\alpha\in\mathbb{R}$, it seems that we have $\displaystyle \sum_{k \ge 0}\frac{1}{k^2+k-\alpha} = \frac{\pi}{\sqrt{4\alpha+1}}\tan\left(\frac{1}{2}\pi\sqrt{4\alpha+1}\right).$ I've tried many techniques and ways -- partial fractions, turning it into an integral, finding the partial sum and so on -- to show it, but all ended in failure. Many thanks for the help in advance.","For $\alpha\in\mathbb{R}$, it seems that we have $\displaystyle \sum_{k \ge 0}\frac{1}{k^2+k-\alpha} = \frac{\pi}{\sqrt{4\alpha+1}}\tan\left(\frac{1}{2}\pi\sqrt{4\alpha+1}\right).$ I've tried many techniques and ways -- partial fractions, turning it into an integral, finding the partial sum and so on -- to show it, but all ended in failure. Many thanks for the help in advance.",,"['calculus', 'complex-analysis', 'sequences-and-series']"
22,Estimates for Stirling's formula remainder,Estimates for Stirling's formula remainder,,"It is proved in Advanced Calculus by Angus Taylor, § 20.8, that $$\log n!=\log \left( \left( \frac{n}{e}\right) ^{n}\sqrt{2\pi n}\right) +r_{n},$$ where $$r_{n}=\sum_{k=1}^{\infty }S_{k}$$ with $$S_{k}=\sum_{p=n+1}^{\infty }\frac{k}{2(k+1)(k+2)p^{k+1}}.$$ This formula for $r_{n}$ provides a method for finding the estimate $$\frac{1}{12\left( n+1\right) }<r_{n}<\frac{1}{12\left( n-1\right) }.$$ Indeed $$\frac{1}{k\left( n+1\right) ^{k}}=\sum_{p=n+1}^{\infty }\int_{p}^{p+1}\frac{% 1}{x^{k+1}}\mathrm dx<\sum_{p=n+1}^{\infty }\frac{1}{p^{k+1}}$$ $$\sum_{p=n+1}^{\infty }\frac{1}{p^{k+1}}<\sum_{p=n+1}^{\infty }\int_{p-1}^{p}% \frac{1}{x^{k+1}}\mathrm dx=\frac{1}{kn^{k}},$$ and so $$\frac{1}{2(k+1)(k+2)\left( n+1\right) ^{k}}<S_{k}<\frac{1}{2(k+1)(k+2)n^{k}} $$ $$\frac{1}{12\left( n+1\right) }<\sum_{k=1}^{\infty }\frac{1}{% 2(k+1)(k+2)\left( n+1\right) ^{k}}<r_{n}<\sum_{k=1}^{\infty }\frac{1}{% 2(k+1)(k+2)n^{k}}<\frac{1}{12\left( n-1\right) }.$$ Added :The Wikipedia article pointed to in the comment gives an approximation based on the Euler-MacLaurin formula in terms of the Bernoulli numbers and states that it can also be obtained by repeated integration by parts. The Stirling series that appears in the same article is different from the one above. Question (edited): Are there better estimates of $r_{n}$ based on different methods?","It is proved in Advanced Calculus by Angus Taylor, § 20.8, that $$\log n!=\log \left( \left( \frac{n}{e}\right) ^{n}\sqrt{2\pi n}\right) +r_{n},$$ where $$r_{n}=\sum_{k=1}^{\infty }S_{k}$$ with $$S_{k}=\sum_{p=n+1}^{\infty }\frac{k}{2(k+1)(k+2)p^{k+1}}.$$ This formula for $r_{n}$ provides a method for finding the estimate $$\frac{1}{12\left( n+1\right) }<r_{n}<\frac{1}{12\left( n-1\right) }.$$ Indeed $$\frac{1}{k\left( n+1\right) ^{k}}=\sum_{p=n+1}^{\infty }\int_{p}^{p+1}\frac{% 1}{x^{k+1}}\mathrm dx<\sum_{p=n+1}^{\infty }\frac{1}{p^{k+1}}$$ $$\sum_{p=n+1}^{\infty }\frac{1}{p^{k+1}}<\sum_{p=n+1}^{\infty }\int_{p-1}^{p}% \frac{1}{x^{k+1}}\mathrm dx=\frac{1}{kn^{k}},$$ and so $$\frac{1}{2(k+1)(k+2)\left( n+1\right) ^{k}}<S_{k}<\frac{1}{2(k+1)(k+2)n^{k}} $$ $$\frac{1}{12\left( n+1\right) }<\sum_{k=1}^{\infty }\frac{1}{% 2(k+1)(k+2)\left( n+1\right) ^{k}}<r_{n}<\sum_{k=1}^{\infty }\frac{1}{% 2(k+1)(k+2)n^{k}}<\frac{1}{12\left( n-1\right) }.$$ Added :The Wikipedia article pointed to in the comment gives an approximation based on the Euler-MacLaurin formula in terms of the Bernoulli numbers and states that it can also be obtained by repeated integration by parts. The Stirling series that appears in the same article is different from the one above. Question (edited): Are there better estimates of $r_{n}$ based on different methods?",,"['calculus', 'approximation']"
23,Circular definition of continuity,Circular definition of continuity,,"When evaluating limits, it's tempting to just plug in the approach value into the function to get the answer. For example, if $f(x) = x^2$ and we need to solve $\lim_{x \to 2}(f(x))$ then we want to just take $f(2)$ which gets us $2^2$ which gets us $4$ . However, we first need to prove that $f$ is continuous at $2$ before we can do that. And in order to do that, we need to show that $\lim_{x \to 2}(f(x)) = f(2)$ . And in order to evaluate that limit, we need to show that $f$ is continuous at $2$ . So we end up with a circular definition of continuity. In precalculus (or ""Advanced Integrated Math 3"" in my case), we did learn about the concept of continuity but without any rigorous definition. We would just kind of look at the graph and use our intuition to determine where the function is continuous and where it is discontinuous. Though that's not really a reliable way to determine continuity, since we can only plot finitely many points of a function on a graph, even with our best technology. How am I supposed to know that, in the tiny space between two of those points, the function isn't discontinuous?","When evaluating limits, it's tempting to just plug in the approach value into the function to get the answer. For example, if and we need to solve then we want to just take which gets us which gets us . However, we first need to prove that is continuous at before we can do that. And in order to do that, we need to show that . And in order to evaluate that limit, we need to show that is continuous at . So we end up with a circular definition of continuity. In precalculus (or ""Advanced Integrated Math 3"" in my case), we did learn about the concept of continuity but without any rigorous definition. We would just kind of look at the graph and use our intuition to determine where the function is continuous and where it is discontinuous. Though that's not really a reliable way to determine continuity, since we can only plot finitely many points of a function on a graph, even with our best technology. How am I supposed to know that, in the tiny space between two of those points, the function isn't discontinuous?",f(x) = x^2 \lim_{x \to 2}(f(x)) f(2) 2^2 4 f 2 \lim_{x \to 2}(f(x)) = f(2) f 2,"['calculus', 'limits', 'continuity', 'education']"
24,"Show that $\int_{0}^{\frac{\pi}{2}} \log(1 + \tan^4 x) \cos^2 x \, dx = \frac{\pi \log(6 + 4\sqrt{2})}{4} - \frac{\pi}{2}$",Show that,"\int_{0}^{\frac{\pi}{2}} \log(1 + \tan^4 x) \cos^2 x \, dx = \frac{\pi \log(6 + 4\sqrt{2})}{4} - \frac{\pi}{2}","Show that $$\int_{0}^{\frac{\pi}{2}} \log(1 + \tan^4 x) \cos^2 x \, dx = \frac{\pi \log(6 + 4\sqrt{2})}{4} - \frac{\pi}{2}$$ My try : $$\Omega = \int_{0}^{\frac{\pi}{2}} \log(1 + \tan^4 x) \cos^2 x \, dx = \int_{0}^{\infty} \frac{\log(1 + x^4)}{(1 + x^2)^2} \, dx$$ $$\int_{0}^{\infty} \frac{x^2 \left\{ \log(1 + x^4) - 4 \log(x) \right\}}{(1 + x^2)^2} \, dx$$ $$2\Omega = \int_{0}^{\infty} \frac{\log(1 + x^4)}{1 + x^2} \, dx - 4 \int_{0}^{\infty} \frac{x^2 \log(x)}{(1 + x^2)^2} \, dx$$ $$\Omega = \frac{1}{2} \int_{0}^{\frac{\pi}{2}} \log(1 + \tan^4 x) \, dx + 2 \int_{0}^{\infty} \frac{\log(x)}{(1 + x^2)^2} \, dx$$",Show that My try :,"\int_{0}^{\frac{\pi}{2}} \log(1 + \tan^4 x) \cos^2 x \, dx = \frac{\pi \log(6 + 4\sqrt{2})}{4} - \frac{\pi}{2} \Omega = \int_{0}^{\frac{\pi}{2}} \log(1 + \tan^4 x) \cos^2 x \, dx = \int_{0}^{\infty} \frac{\log(1 + x^4)}{(1 + x^2)^2} \, dx \int_{0}^{\infty} \frac{x^2 \left\{ \log(1 + x^4) - 4 \log(x) \right\}}{(1 + x^2)^2} \, dx 2\Omega = \int_{0}^{\infty} \frac{\log(1 + x^4)}{1 + x^2} \, dx - 4 \int_{0}^{\infty} \frac{x^2 \log(x)}{(1 + x^2)^2} \, dx \Omega = \frac{1}{2} \int_{0}^{\frac{\pi}{2}} \log(1 + \tan^4 x) \, dx + 2 \int_{0}^{\infty} \frac{\log(x)}{(1 + x^2)^2} \, dx","['calculus', 'integration', 'definite-integrals', 'closed-form']"
25,Weird system of PDEs defined on a sphere,Weird system of PDEs defined on a sphere,,"Let $x$ and $y$ be functions defined on a simply connected (open or closed) portion of the surface of a (unit) sphere, and consider the following system of PDEs: \begin{align} \|\nabla x\|^2 =  \|\nabla y\|^2 = \| \nabla x \times \nabla y \|^2 \end{align} (Some suitable conditions may be provided.) Are there non-constant solutions for such a system ? Any ideas about how to reduce it to single equations for $x$ and $y$ ?","Let and be functions defined on a simply connected (open or closed) portion of the surface of a (unit) sphere, and consider the following system of PDEs: (Some suitable conditions may be provided.) Are there non-constant solutions for such a system ? Any ideas about how to reduce it to single equations for and ?","x y \begin{align}
\|\nabla x\|^2 =
 \|\nabla y\|^2 = \| \nabla x \times \nabla y \|^2
\end{align} x y","['calculus', 'analysis', 'differential-geometry', 'partial-differential-equations', 'vector-fields']"
26,$\int x^{dx}-1$,,\int x^{dx}-1,"If you go to Flammable Maths 's YouTube channel and scroll through some of his videos you see him solving the following integral: $$\int x^{dx}-1$$ he explains that this is a Product integral. My questions are the following: 1 - What is the geometric meaning of a product integral? 2 - does it make sense to have: $$\int f(x,dx)$$ and if $f(x,dx) = g(x)dx$ then it's just a regular integrals and if $f(x,dx) = g(x)^{dx}$ it's just a product integral? I'll leave the link to the video here .",If you go to Flammable Maths 's YouTube channel and scroll through some of his videos you see him solving the following integral: he explains that this is a Product integral. My questions are the following: 1 - What is the geometric meaning of a product integral? 2 - does it make sense to have: and if then it's just a regular integrals and if it's just a product integral? I'll leave the link to the video here .,"\int x^{dx}-1 \int f(x,dx) f(x,dx) = g(x)dx f(x,dx) = g(x)^{dx}","['calculus', 'integration', 'soft-question']"
27,Is the $n^{th}$ derivative of $\sin(x)$ just a translation of $\sin(x)$?,Is the  derivative of  just a translation of ?,n^{th} \sin(x) \sin(x),"I noticed that $$\frac d{dx}\sin x=\cos x=\sin\left(x+\frac\pi2\right)$$ $$\frac{d^n}{dx^n}\sin x=\sin\left(x+\frac{\pi n}2\right)$$ Does this hold for any positive real value of $n$? If so, does anybody have any reasoning behind why it's just a simple translation?","I noticed that $$\frac d{dx}\sin x=\cos x=\sin\left(x+\frac\pi2\right)$$ $$\frac{d^n}{dx^n}\sin x=\sin\left(x+\frac{\pi n}2\right)$$ Does this hold for any positive real value of $n$? If so, does anybody have any reasoning behind why it's just a simple translation?",,['calculus']
28,Find the limit $\lim\limits_{n \to +\infty}\sum\limits_{k=n}^{3n} \binom{k-1}{n-1} \left(\frac{1}{3}\right)^n \left(\frac{2}{3}\right)^{k-n}$,Find the limit,\lim\limits_{n \to +\infty}\sum\limits_{k=n}^{3n} \binom{k-1}{n-1} \left(\frac{1}{3}\right)^n \left(\frac{2}{3}\right)^{k-n},"The problem is to find the following limit: $$\lim_{n \to +\infty}\sum\limits_{k=n}^{3n} \binom{k-1}{n-1} \left(\dfrac{1}{3}\right)^n \left(\dfrac{2}{3}\right)^{k-n}$$ I see that it looks similarly to the formula from the binomial theorem, but don't get how we can make use of it. Any ideas would be greatly appreciated.","The problem is to find the following limit: $$\lim_{n \to +\infty}\sum\limits_{k=n}^{3n} \binom{k-1}{n-1} \left(\dfrac{1}{3}\right)^n \left(\dfrac{2}{3}\right)^{k-n}$$ I see that it looks similarly to the formula from the binomial theorem, but don't get how we can make use of it. Any ideas would be greatly appreciated.",,"['calculus', 'probability', 'limits', 'contest-math', 'binomial-coefficients']"
29,Proving $\int_{0}^{\infty}\sin\left({1\over 4x^2}\right)\ln x\cdot{\mathrm dx\over x^2}=-\sqrt{\pi\over 2}\cdot\left({\pi-2\gamma \over 4}\right)$,Proving,\int_{0}^{\infty}\sin\left({1\over 4x^2}\right)\ln x\cdot{\mathrm dx\over x^2}=-\sqrt{\pi\over 2}\cdot\left({\pi-2\gamma \over 4}\right),"Consider these two similar integrals $$\int_{0}^{\infty}\sin\left({1\over 4x^2}\right)\ln x\cdot{\mathrm dx\over x^2}=-\sqrt{\pi\over 2}\cdot\left({\pi-2\gamma \over 4}\right)\tag1$$ $$\int_{0}^{\infty}\sin\left({1\over 4x^2}\right)\ln^2x\cdot{\mathrm dx\over x^2}=\sqrt{\pi\over 2}\cdot\left({\pi-2\gamma \over 4}\right)^2\tag2$$ How does one prove $(1)$ and $(2)$ ? An attempt: Using the $\sin x$ series, and let $u=\ln x$ , then $(1)$ becomes $$\sum_{n=0}^{\infty}{(-1)^n\over 2^{2n+1}(2n+1)!}\int_{-\infty}^{\infty}ue^{-2(n+1)u}\mathrm du\tag3$$ Integral $(3)$ diverges. Another attempt: $u={1\over 4x^2}$ then $(1)$ becomes $$-{1\over 8}\int_{0}^{\infty}\sin u\ln{(4u)}\cdot{\mathrm du\over u^{3/2}}\tag4$$ Using $\ln u$ series, then $(4)$ becomes $$-{1\over 4}\sum_{n=0}^{\infty}{1\over 2n+1}\int_{0}^{\infty}\left({4u-1\over 4u+1}\right)^{2n+1}\cdot{\sin u\over u^{3/2}}\mathrm du\tag5$$ Using $\coth^{-1} x={1\over 2}\ln{x-1\over x+1}$ $$-{1\over 4}\sum_{n=0}^{\infty}{1\over 2n+1}\int_{0}^{\infty}e^{2(2n+1)\coth^{-1}4x}\cdot{\sin u\over u^{3/2}}\mathrm du\tag6$$ How else can we proceed?","Consider these two similar integrals How does one prove and ? An attempt: Using the series, and let , then becomes Integral diverges. Another attempt: then becomes Using series, then becomes Using How else can we proceed?",\int_{0}^{\infty}\sin\left({1\over 4x^2}\right)\ln x\cdot{\mathrm dx\over x^2}=-\sqrt{\pi\over 2}\cdot\left({\pi-2\gamma \over 4}\right)\tag1 \int_{0}^{\infty}\sin\left({1\over 4x^2}\right)\ln^2x\cdot{\mathrm dx\over x^2}=\sqrt{\pi\over 2}\cdot\left({\pi-2\gamma \over 4}\right)^2\tag2 (1) (2) \sin x u=\ln x (1) \sum_{n=0}^{\infty}{(-1)^n\over 2^{2n+1}(2n+1)!}\int_{-\infty}^{\infty}ue^{-2(n+1)u}\mathrm du\tag3 (3) u={1\over 4x^2} (1) -{1\over 8}\int_{0}^{\infty}\sin u\ln{(4u)}\cdot{\mathrm du\over u^{3/2}}\tag4 \ln u (4) -{1\over 4}\sum_{n=0}^{\infty}{1\over 2n+1}\int_{0}^{\infty}\left({4u-1\over 4u+1}\right)^{2n+1}\cdot{\sin u\over u^{3/2}}\mathrm du\tag5 \coth^{-1} x={1\over 2}\ln{x-1\over x+1} -{1\over 4}\sum_{n=0}^{\infty}{1\over 2n+1}\int_{0}^{\infty}e^{2(2n+1)\coth^{-1}4x}\cdot{\sin u\over u^{3/2}}\mathrm du\tag6,"['calculus', 'integration', 'definite-integrals']"
30,Is there another way to solve this differential equation?,Is there another way to solve this differential equation?,,"To put this into some context, I was considering the general equation for damped harmonic motion: $\ddot z +2\gamma\dot z +\omega_0^2z=0$ and specifically there limiting cases where the the damping force (to which the second term corresponds) is zero, and where the restoring force (to which the last term corresponds) is zero. In the second case, I yield the differential equation $\ddot z +2\gamma\dot z=0$ and I solved this in the following way: I split $\ddot z$ into $\frac{d}{dt}\dot z$ and then separated the vairables: $\int\frac{1}{\dot z}d\dot z =\int-2\gamma dt$ which gives the exponential solution expected. Now I am considering the case where there is simple harmonic motion with no damping, i.e. the case that gives rise to the differential equation: $\ddot x +\omega_0^2 x=0$ I can't figre out- is there a way to solve this without just assuming an exponential solution and finding the constants in the solution? For example, in the equation above I was able to solve it by separating the variables. I'm not so sure this would work here where we have a second derivative of x and x itself, but i'm not sure. Also, I would be grateful if anyone could recommend an online resource or book that explains how to solve different types of integrals and differential equations well. I have been trying to study this for some time but haven't come across anything that seems very complete, and perhaps something that has some worked examples?","To put this into some context, I was considering the general equation for damped harmonic motion: $\ddot z +2\gamma\dot z +\omega_0^2z=0$ and specifically there limiting cases where the the damping force (to which the second term corresponds) is zero, and where the restoring force (to which the last term corresponds) is zero. In the second case, I yield the differential equation $\ddot z +2\gamma\dot z=0$ and I solved this in the following way: I split $\ddot z$ into $\frac{d}{dt}\dot z$ and then separated the vairables: $\int\frac{1}{\dot z}d\dot z =\int-2\gamma dt$ which gives the exponential solution expected. Now I am considering the case where there is simple harmonic motion with no damping, i.e. the case that gives rise to the differential equation: $\ddot x +\omega_0^2 x=0$ I can't figre out- is there a way to solve this without just assuming an exponential solution and finding the constants in the solution? For example, in the equation above I was able to solve it by separating the variables. I'm not so sure this would work here where we have a second derivative of x and x itself, but i'm not sure. Also, I would be grateful if anyone could recommend an online resource or book that explains how to solve different types of integrals and differential equations well. I have been trying to study this for some time but haven't come across anything that seems very complete, and perhaps something that has some worked examples?",,"['calculus', 'integration', 'ordinary-differential-equations', 'derivatives']"
31,Limit $\lim_{n \to \infty} \frac {a_n}{2^{n-1}}=\frac 4{\pi}$ for $a_{n+1}=a_n+\sqrt{1+a_n^2}$ and $a_0=0$,Limit  for  and,\lim_{n \to \infty} \frac {a_n}{2^{n-1}}=\frac 4{\pi} a_{n+1}=a_n+\sqrt{1+a_n^2} a_0=0,"I found the following question in a book:- $Q:$Let $a_1, a_2, ... , a_n$ be a sequence of real numbers with $a_{n+1}=a_n+\sqrt{1+a_n^2}$ and $a_0=0$. Prove that $$\lim_{n \to \infty} \frac {a_n}{2^{n-1}}=\frac 4{\pi}$$ I tried many things none of which seemed fruitful. First thing I did was to define $a_n=2^{n-1}b_n$. Substituting this into the condition, we get $$2^{n}b_n=2^{n-1}b_n+\sqrt{1+2^{2n-2}b_n^2}\implies b_{n+1}=\frac {b_n}2+\sqrt{\frac1{2^{2n}}+\left(\frac {b_n}2\right)^2}$$ Simplifying this gives $$b_{n+1}^2-b_{n+1}b_n=\frac1{2^{2n}}\implies b_{n+1}(b_{n+1}-b_n)=\frac1{2^{2n}}$$ This doesn't lead anywhere. One lead that I got was by substituting $a_n=\tan \theta_n$. This gives $$\begin{align}a_{n+1}&=\tan \theta_n+\sec \theta_n\\&=\frac{\sin\theta_n+1}{\cos\theta_n}\\&=\frac{1+\tan\frac{\theta_n}2}{1-\tan\frac{\theta_n}2}\\&=\tan\left(\frac {\theta_n}2+\frac{\pi}4\right)\end{align}$$ Hence $$\tan\theta_{n+1}=\tan\left(\frac {\theta_n}2+\frac{\pi}4\right)\implies \theta_{n+1}=\frac {\theta_n}2+\frac{\pi}4\implies \theta_n=\frac {\pi}2+c\left(\frac 12\right)^n$$From initial conditions we get $\theta_0=0\implies c=-\frac{\pi}2$. Therefore $$\lim_{n \to \infty} \frac{a_n}{2^{n-1}}=\lim_{n\to\infty}\frac{\tan\left(\frac {\pi}2-\frac{\pi}2 \left( \frac 12 \right)^n\right)}{2^{n-1}}=\lim_{n\to\infty}\frac{2\left(\frac12\right)^n}{\tan\left(\frac {\pi}2\left(\frac12\right)^n\right)}=\lim_{n\to\infty}\frac{2\left(\frac12\right)^n}{\frac{\pi}2\left(\frac12\right)^n}=\frac4{\pi}$$ Is this proof valid and are there any other ways of doing it?","I found the following question in a book:- $Q:$Let $a_1, a_2, ... , a_n$ be a sequence of real numbers with $a_{n+1}=a_n+\sqrt{1+a_n^2}$ and $a_0=0$. Prove that $$\lim_{n \to \infty} \frac {a_n}{2^{n-1}}=\frac 4{\pi}$$ I tried many things none of which seemed fruitful. First thing I did was to define $a_n=2^{n-1}b_n$. Substituting this into the condition, we get $$2^{n}b_n=2^{n-1}b_n+\sqrt{1+2^{2n-2}b_n^2}\implies b_{n+1}=\frac {b_n}2+\sqrt{\frac1{2^{2n}}+\left(\frac {b_n}2\right)^2}$$ Simplifying this gives $$b_{n+1}^2-b_{n+1}b_n=\frac1{2^{2n}}\implies b_{n+1}(b_{n+1}-b_n)=\frac1{2^{2n}}$$ This doesn't lead anywhere. One lead that I got was by substituting $a_n=\tan \theta_n$. This gives $$\begin{align}a_{n+1}&=\tan \theta_n+\sec \theta_n\\&=\frac{\sin\theta_n+1}{\cos\theta_n}\\&=\frac{1+\tan\frac{\theta_n}2}{1-\tan\frac{\theta_n}2}\\&=\tan\left(\frac {\theta_n}2+\frac{\pi}4\right)\end{align}$$ Hence $$\tan\theta_{n+1}=\tan\left(\frac {\theta_n}2+\frac{\pi}4\right)\implies \theta_{n+1}=\frac {\theta_n}2+\frac{\pi}4\implies \theta_n=\frac {\pi}2+c\left(\frac 12\right)^n$$From initial conditions we get $\theta_0=0\implies c=-\frac{\pi}2$. Therefore $$\lim_{n \to \infty} \frac{a_n}{2^{n-1}}=\lim_{n\to\infty}\frac{\tan\left(\frac {\pi}2-\frac{\pi}2 \left( \frac 12 \right)^n\right)}{2^{n-1}}=\lim_{n\to\infty}\frac{2\left(\frac12\right)^n}{\tan\left(\frac {\pi}2\left(\frac12\right)^n\right)}=\lim_{n\to\infty}\frac{2\left(\frac12\right)^n}{\frac{\pi}2\left(\frac12\right)^n}=\frac4{\pi}$$ Is this proof valid and are there any other ways of doing it?",,"['calculus', 'limits', 'trigonometry', 'proof-verification', 'substitution']"
32,$\sum_{n=1}^\infty \frac{1}{(n^2-1)!} - \sum_{n=1}^\infty \frac{1}{(7n+1)!}$ is almost $1+1/6$,is almost,\sum_{n=1}^\infty \frac{1}{(n^2-1)!} - \sum_{n=1}^\infty \frac{1}{(7n+1)!} 1+1/6,"I've recognized, that $$\mathcal{S} = \sum_{n=1}^\infty \frac{1}{(n^2-1)!} - \sum_{n=1}^\infty \frac{1}{(7n+1)!} \approx 1.1666666666666666666657785992648796$$ which is almost $1+1/6$. I think it is not a mathematical coincidence , but I'm not sure what are behind the scences. Why is this value almost $1+1/6$? In the first terms of the sequences $n^2-1$ [$A005563$ ] and $7n+1$ [$A016993$ ] the terms $8$ and $15$ coincide. Furthermore $$\sum_{n=1}^\infty \frac{1}{(n^2-1)!} \approx 1.166691468254732970341437561639 $$ and $$\sum_{n=1}^\infty \frac{1}{(7n+1)!} \approx 0.000024801588066303675658962374$$ Are there a family of this type of identities? Also would be nice to see the exact value of $1+1/6-\mathcal{S}$ which is approximately $$ 8.880674017870608390962304909410175868736976 \cdot 10^{-22} $$","I've recognized, that $$\mathcal{S} = \sum_{n=1}^\infty \frac{1}{(n^2-1)!} - \sum_{n=1}^\infty \frac{1}{(7n+1)!} \approx 1.1666666666666666666657785992648796$$ which is almost $1+1/6$. I think it is not a mathematical coincidence , but I'm not sure what are behind the scences. Why is this value almost $1+1/6$? In the first terms of the sequences $n^2-1$ [$A005563$ ] and $7n+1$ [$A016993$ ] the terms $8$ and $15$ coincide. Furthermore $$\sum_{n=1}^\infty \frac{1}{(n^2-1)!} \approx 1.166691468254732970341437561639 $$ and $$\sum_{n=1}^\infty \frac{1}{(7n+1)!} \approx 0.000024801588066303675658962374$$ Are there a family of this type of identities? Also would be nice to see the exact value of $1+1/6-\mathcal{S}$ which is approximately $$ 8.880674017870608390962304909410175868736976 \cdot 10^{-22} $$",,"['calculus', 'sequences-and-series', 'recreational-mathematics', 'closed-form']"
33,How prove this inequality $\left|\int_{a}^{b}f(x)dx\right|\leq \frac{1}{8}(b-a)^{2}\int_{a}^{b}\left|f''(x)\right|dx$,How prove this inequality,\left|\int_{a}^{b}f(x)dx\right|\leq \frac{1}{8}(b-a)^{2}\int_{a}^{b}\left|f''(x)\right|dx,"Let $f(x)\in \mathcal C^2([a,b]),f(\frac{a+b}{2})=0$. Show that$$\left|\int_{a}^{b}f(x)dx\right|\leq \frac{1}{8}(b-a)^{2}\int_{a}^{b}\left|f''(x)\right|dx.$$ I try to use Taylor's Theorem with Integral form  of the Remainder ,then I have $$f(x)=f(\frac{a+b}{2})+f'(\frac{a+b}{2})(x-\frac{a+b}{2})+\int_{\frac{a+b}{2}}^{x}(x-t)f''(t)dt.$$ Since $f(\frac{a+b}{2})=\int_{a}^{b}f'(\frac{a+b}{2})(x-\frac{a+b}{2})dx=0, $ we get$\int_{a}^{b}f(x)dx=\int_{a}^{b}\left(\int_{\frac{a+b}{2}}^{x}(x-t)f''(t)dt\right)dx.$ I think the point of this problem is to prove: $$\left|\int_{a}^{b}\left(\int_{\frac{a+b}{2}}^{x}(x-t)f''(t)dt\right)dx\right|\leq\frac{1}{8}(b-a)^{2}\int_{a}^{b}\left|f''(x)\right|dx.$$The coeffient of $\frac{1}{8}$ is strange ,and how find it ?","Let $f(x)\in \mathcal C^2([a,b]),f(\frac{a+b}{2})=0$. Show that$$\left|\int_{a}^{b}f(x)dx\right|\leq \frac{1}{8}(b-a)^{2}\int_{a}^{b}\left|f''(x)\right|dx.$$ I try to use Taylor's Theorem with Integral form  of the Remainder ,then I have $$f(x)=f(\frac{a+b}{2})+f'(\frac{a+b}{2})(x-\frac{a+b}{2})+\int_{\frac{a+b}{2}}^{x}(x-t)f''(t)dt.$$ Since $f(\frac{a+b}{2})=\int_{a}^{b}f'(\frac{a+b}{2})(x-\frac{a+b}{2})dx=0, $ we get$\int_{a}^{b}f(x)dx=\int_{a}^{b}\left(\int_{\frac{a+b}{2}}^{x}(x-t)f''(t)dt\right)dx.$ I think the point of this problem is to prove: $$\left|\int_{a}^{b}\left(\int_{\frac{a+b}{2}}^{x}(x-t)f''(t)dt\right)dx\right|\leq\frac{1}{8}(b-a)^{2}\int_{a}^{b}\left|f''(x)\right|dx.$$The coeffient of $\frac{1}{8}$ is strange ,and how find it ?",,"['calculus', 'integration']"
34,"How does this ""integration by differentiation"" method work","How does this ""integration by differentiation"" method work",,"Apparently, the integral of a function f(x) from a to b can be done through differentiation through this method: $$   \int_a^b f(x)dx = \lim_{x \rightarrow  \ 0 }  f(\frac{d}{d x} )\frac{e^{bx}-e^{ax}}{x} $$ Can anyone explain why this works? We can assume that f has a MacLauren representation with an infinite radius of convergence. An example with f(x) = c, c constant, the integral of f from a to b is: $$ \lim_{x \rightarrow  \ 0 }  c\frac{e^{bx}-e^{ax}}{x} =  \lim_{x \rightarrow  \ 0 }  c\frac{1+bx+0.5(bx)^2...-1-ax-0.5(ax)^2...}{x} = c(b-a) $$ as expected. However I don't know how to show this is true for all (most?) functions with a MacLauren series. Help? The f(d/dx) and limit is what's being difficult to deal with. If f is something like x^2 then f(d/dx) is d^2/dx^2 but with a general function f I can't establish a pattern.","Apparently, the integral of a function f(x) from a to b can be done through differentiation through this method: $$   \int_a^b f(x)dx = \lim_{x \rightarrow  \ 0 }  f(\frac{d}{d x} )\frac{e^{bx}-e^{ax}}{x} $$ Can anyone explain why this works? We can assume that f has a MacLauren representation with an infinite radius of convergence. An example with f(x) = c, c constant, the integral of f from a to b is: $$ \lim_{x \rightarrow  \ 0 }  c\frac{e^{bx}-e^{ax}}{x} =  \lim_{x \rightarrow  \ 0 }  c\frac{1+bx+0.5(bx)^2...-1-ax-0.5(ax)^2...}{x} = c(b-a) $$ as expected. However I don't know how to show this is true for all (most?) functions with a MacLauren series. Help? The f(d/dx) and limit is what's being difficult to deal with. If f is something like x^2 then f(d/dx) is d^2/dx^2 but with a general function f I can't establish a pattern.",,"['calculus', 'integration', 'sequences-and-series', 'derivatives']"
35,"Is this closed-form of $\int_0^1 \operatorname{Li}_3^2(x)\,dx$ correct?",Is this closed-form of  correct?,"\int_0^1 \operatorname{Li}_3^2(x)\,dx","According to Freitas' paper at page $11$. $$\int_0^1 \operatorname{Li}_3^2(x)\,dx = 20-8\zeta(2)-10\zeta(3)-\frac{15}{2}\zeta(4)-2\zeta(2)\zeta(3)+\zeta^2(3).$$ I evaluated the LHS and it is $0.427714784290824$ to me, but the RHS is $-15.8071337213762487846272$. Where am I wrong? Is this closed-form correct? If not, what is the correct closed-form? Edit. The correct closed-form is $$\int_0^1 \operatorname{Li}_3^2(x)\,dx = 20-8\zeta(2)-10\zeta(3)+\frac{15}{2}\zeta(4)-2\zeta(2)\zeta(3)+\zeta^2(3).$$","According to Freitas' paper at page $11$. $$\int_0^1 \operatorname{Li}_3^2(x)\,dx = 20-8\zeta(2)-10\zeta(3)-\frac{15}{2}\zeta(4)-2\zeta(2)\zeta(3)+\zeta^2(3).$$ I evaluated the LHS and it is $0.427714784290824$ to me, but the RHS is $-15.8071337213762487846272$. Where am I wrong? Is this closed-form correct? If not, what is the correct closed-form? Edit. The correct closed-form is $$\int_0^1 \operatorname{Li}_3^2(x)\,dx = 20-8\zeta(2)-10\zeta(3)+\frac{15}{2}\zeta(4)-2\zeta(2)\zeta(3)+\zeta^2(3).$$",,"['calculus', 'integration', 'polynomials', 'definite-integrals', 'closed-form']"
36,"Closed-form of the sequence ${_2F_1}\left(\begin{array}c\tfrac12,-n\\\tfrac32\end{array}\middle|\,\frac{1}{2}\right)$",Closed-form of the sequence,"{_2F_1}\left(\begin{array}c\tfrac12,-n\\\tfrac32\end{array}\middle|\,\frac{1}{2}\right)","Is there a closed-form of the following sequence? $$a_n={_2F_1}\left(\begin{array}c\tfrac12,-n\\\tfrac32\end{array}\middle|\,\frac{1}{2}\right),$$ where $_2F_1$ is the hypergeometric function and $n \in \mathbb{N}$. Maple could evaluate $a_n$ for arbitrary $n$. The exact values for $a_n$ from $n=0$ to $10$.  $$1,\frac 56,{\frac {43}{60}},{\frac {177}{280}},{\frac {2867}{5040}},{ \frac {11531}{22176}},{\frac {92479}{192192}},{\frac {74069}{164736}}, {\frac {2371495}{5601024}},{\frac {9488411}{23648768}},{\frac { 126527543}{331082752}},\dots$$ It is interesting, that the first $7$ term of the numerator sequence matches with $\text{A126963}$ on OEIS, but after that it breaks.","Is there a closed-form of the following sequence? $$a_n={_2F_1}\left(\begin{array}c\tfrac12,-n\\\tfrac32\end{array}\middle|\,\frac{1}{2}\right),$$ where $_2F_1$ is the hypergeometric function and $n \in \mathbb{N}$. Maple could evaluate $a_n$ for arbitrary $n$. The exact values for $a_n$ from $n=0$ to $10$.  $$1,\frac 56,{\frac {43}{60}},{\frac {177}{280}},{\frac {2867}{5040}},{ \frac {11531}{22176}},{\frac {92479}{192192}},{\frac {74069}{164736}}, {\frac {2371495}{5601024}},{\frac {9488411}{23648768}},{\frac { 126527543}{331082752}},\dots$$ It is interesting, that the first $7$ term of the numerator sequence matches with $\text{A126963}$ on OEIS, but after that it breaks.",,"['calculus', 'sequences-and-series', 'special-functions', 'closed-form', 'hypergeometric-function']"
37,finding the closest distance between a point a curve,finding the closest distance between a point a curve,,"consider the curve $y=x^2$ what are the points on the curve that are the closest to the point $(1,0)$ using calculus I got the two points but what is the connection between normals and the closest distance to a curve from a point is it that drawing normals from 1,0 and seeing where it meets the curve gives the points closest to it  - although a normal from 1,0 does go through the origin even though the origin is not the closest to the curve please explain these observations","consider the curve $y=x^2$ what are the points on the curve that are the closest to the point $(1,0)$ using calculus I got the two points but what is the connection between normals and the closest distance to a curve from a point is it that drawing normals from 1,0 and seeing where it meets the curve gives the points closest to it  - although a normal from 1,0 does go through the origin even though the origin is not the closest to the curve please explain these observations",,"['calculus', 'plane-curves']"
38,Prove $\sin a=\int_{-\infty}^{\infty}\cos(ax^2)\frac{\sinh(2ax)}{\sinh(\pi x)} \operatorname dx$,Prove,\sin a=\int_{-\infty}^{\infty}\cos(ax^2)\frac{\sinh(2ax)}{\sinh(\pi x)} \operatorname dx,Derive the integral representation $$\sin a=\int_{-\infty}^{\infty}\cos(ax^2)\frac{\sinh(2ax)}{\sinh(\pi x)}dx$$ for $|a|\le \pi/2$ .,Derive the integral representation for .,\sin a=\int_{-\infty}^{\infty}\cos(ax^2)\frac{\sinh(2ax)}{\sinh(\pi x)}dx |a|\le \pi/2,"['calculus', 'integration', 'improper-integrals', 'contour-integration']"
39,How to solve the following equation $\sqrt[3]{x+3}+\sqrt[3]{x}=\sqrt[3]{3+8x}$,How to solve the following equation,\sqrt[3]{x+3}+\sqrt[3]{x}=\sqrt[3]{3+8x},"I am trying to solve this equation: $$\sqrt[3]{x+3}+\sqrt[3]{x}=\sqrt[3]{3+8x}$$ I would like to get some advice, how to solve it. Thanks.","I am trying to solve this equation: $$\sqrt[3]{x+3}+\sqrt[3]{x}=\sqrt[3]{3+8x}$$ I would like to get some advice, how to solve it. Thanks.",,"['calculus', 'algebra-precalculus', 'radicals']"
40,Evaluate the integral $\int_{0}^{+\infty}\frac{dx}{1 + x^{1000}} $,Evaluate the integral,\int_{0}^{+\infty}\frac{dx}{1 + x^{1000}} ,"Evaluate the integral   \begin{equation} \int\limits_{0}^{+\infty}\frac{dx}{1 + x^{1000}} \end{equation} I tried using the change of variable, integration by parts, even wolframalpha... Nothing helped. Theoretically speaking, it can be solved by using residue calculus, but we have 500 residues in the upper half-plane. I would be grateful for just a hint.","Evaluate the integral   \begin{equation} \int\limits_{0}^{+\infty}\frac{dx}{1 + x^{1000}} \end{equation} I tried using the change of variable, integration by parts, even wolframalpha... Nothing helped. Theoretically speaking, it can be solved by using residue calculus, but we have 500 residues in the upper half-plane. I would be grateful for just a hint.",,['calculus']
41,How to calculate the integral of $x^x$ between $0$ and $1$ using series? [duplicate],How to calculate the integral of  between  and  using series? [duplicate],x^x 0 1,"This question already has answers here : Finding $\int x^xdx$ (8 answers) Closed 11 years ago . How to calculate $\int_0^1 x^x\,dx$ using series? I read from a book that  $$\int_0^1 x^x\,dx = 1-\frac{1}{2^2}+\frac{1}{3^3}+\dots+(-1)^n\frac{1}{(n+1)^{n+1}}+\cdots$$ but I can't prove it. Thanks in advance. P.S: I found some useful materials here and here .","This question already has answers here : Finding $\int x^xdx$ (8 answers) Closed 11 years ago . How to calculate $\int_0^1 x^x\,dx$ using series? I read from a book that  $$\int_0^1 x^x\,dx = 1-\frac{1}{2^2}+\frac{1}{3^3}+\dots+(-1)^n\frac{1}{(n+1)^{n+1}}+\cdots$$ but I can't prove it. Thanks in advance. P.S: I found some useful materials here and here .",,"['calculus', 'sequences-and-series', 'power-series']"
42,Strengthening My Foundation in Mathematics,Strengthening My Foundation in Mathematics,,"Someone told me that each equation I included in the book would halve the sales. From Stephen Hawking's A Brief History of Time (1988), this quote summarizes why I believe that I have a weak foundation in mathematics. I've always been fascinated by equations; their applications, implications, and histories. I would always wonder where equations came from and how they were developed, meanwhile losing track of the information that teachers were throwing at me. Unfortunately, this is still the case even now :P As an electrical engineering major/math minor, I feel obligated to understand the concepts that I learn in math at a fundamental level. Unfortunately, I didn't really respect math back in middle school and high school, so my understanding of certain topics in math are fuzzy at best. For example, I've passed Calculus I/II/III and Differential Equations, but I never really understood what I was doing; the calculations I'd make were by rote and not by intuition. So I'll ask, where can I start? What do you all recommend I do? Any specific books/sources? Thanks","Someone told me that each equation I included in the book would halve the sales. From Stephen Hawking's A Brief History of Time (1988), this quote summarizes why I believe that I have a weak foundation in mathematics. I've always been fascinated by equations; their applications, implications, and histories. I would always wonder where equations came from and how they were developed, meanwhile losing track of the information that teachers were throwing at me. Unfortunately, this is still the case even now :P As an electrical engineering major/math minor, I feel obligated to understand the concepts that I learn in math at a fundamental level. Unfortunately, I didn't really respect math back in middle school and high school, so my understanding of certain topics in math are fuzzy at best. For example, I've passed Calculus I/II/III and Differential Equations, but I never really understood what I was doing; the calculations I'd make were by rote and not by intuition. So I'll ask, where can I start? What do you all recommend I do? Any specific books/sources? Thanks",,"['calculus', 'reference-request', 'education', 'self-learning']"
43,How to evalutate this exponential integral,How to evalutate this exponential integral,,Is there an easy way to compute $$\int_{-\infty}^\infty\exp(-x^2+2x)\mathrm{d}x$$ without using a computer package?,Is there an easy way to compute $$\int_{-\infty}^\infty\exp(-x^2+2x)\mathrm{d}x$$ without using a computer package?,,"['calculus', 'integration', 'definite-integrals', 'improper-integrals']"
44,Prove $f(x)=\int\frac{e^x}{x}\mathrm dx$ is not an elementary function,Prove  is not an elementary function,f(x)=\int\frac{e^x}{x}\mathrm dx,"How do I prove that the exponential integral $$f(x)=\int \frac{e^x}{x}\mathrm dx$$ is not an elementary function ? Also, what are the general methods and tricks to prove that an integral or solution to an equation is not an elementary function?","How do I prove that the exponential integral $$f(x)=\int \frac{e^x}{x}\mathrm dx$$ is not an elementary function ? Also, what are the general methods and tricks to prove that an integral or solution to an equation is not an elementary function?",,"['calculus', 'integration', 'special-functions']"
45,How many real solutions does $\lambda_1 e^{y - \lambda_1 e^y} (1 - \lambda_1 e^y ) + \lambda_2 e^{y - \lambda_2 e^y} (1 - \lambda_2 e^y ) = 0$ have?,How many real solutions does  have?,\lambda_1 e^{y - \lambda_1 e^y} (1 - \lambda_1 e^y ) + \lambda_2 e^{y - \lambda_2 e^y} (1 - \lambda_2 e^y ) = 0,"I am trying to work out for what $\lambda_1, \lambda_2 > 0$ is it true that $f(y) = \lambda_1 e^{y-\lambda_1 e^y} + \lambda_2 e^{y-\lambda_2 e^y}$ is unimodal? Experimentally it seems it is unimodal when $\lambda_1 < \lambda_2$ and $\frac{\lambda_2}{\lambda{1}} <  7.5$ . To work this out I started with: $$\frac{d}{dy} \left(\lambda_1 e^{y-\lambda_1 e^y} + \lambda_2 e^{y-\lambda_2 e^y} \right) = \lambda_1 e^{y - \lambda_1 e^y}  (1 - \lambda_1 e^y ) + \lambda_2 e^{y - \lambda_2 e^y}  (1 - \lambda_2 e^y )$$ It seems we then need to check when $$\lambda_1 e^{y - \lambda_1 e^y}  (1 - \lambda_1 e^y ) + \lambda_2 e^{y - \lambda_2 e^y}  (1 - \lambda_2 e^y ) = 0$$ has more than one solution when solved for $y \in \mathbb{R}$ .  How can we determine the conditions under which it has different numbers of solutions? Added: Substituting $z = e^y$ and dividing by $e^{y-1}$ we are trying to determine how many solutions $$  \lambda_1 e^{1-\lambda_1 z}(1-\lambda_1 z) +\lambda_2e^{1-\lambda_2 z}(1-\lambda_2 z) = 0 $$ has with $z > 0$ . Examples: Example $\lambda_1 = 1, \lambda_2 = 7$ with only one mode (code in python): import matplotlib.pyplot as plt import numpy as np def pdf_func(y, params):     return sum([lambd*np.exp(y - lambd * np.exp(y)) for lambd in params]) params = [1, 7] xs = np.linspace(-10,10,1000) plt.plot(xs, [pdf_func(y, params) for y in xs]) Example $\lambda_1 = 1, \lambda_2 = 50$ with two modes: Questions How can one prove (assuming it is true) that that the number of local maxima that $f(y)$ has is either 1 or 2 and there are no other possibilities? Is it true that for $\lambda_2 > \lambda_1 > 0$ , there exists a threshold $c$ so that if $\frac{\lambda_2}{\lambda_1} < c$ then $f(y)$ is unimodal and if not it has two local maxima? (My guess is that the answer is yes and this threshold is around $7.5$ .)","I am trying to work out for what is it true that is unimodal? Experimentally it seems it is unimodal when and . To work this out I started with: It seems we then need to check when has more than one solution when solved for .  How can we determine the conditions under which it has different numbers of solutions? Added: Substituting and dividing by we are trying to determine how many solutions has with . Examples: Example with only one mode (code in python): import matplotlib.pyplot as plt import numpy as np def pdf_func(y, params):     return sum([lambd*np.exp(y - lambd * np.exp(y)) for lambd in params]) params = [1, 7] xs = np.linspace(-10,10,1000) plt.plot(xs, [pdf_func(y, params) for y in xs]) Example with two modes: Questions How can one prove (assuming it is true) that that the number of local maxima that has is either 1 or 2 and there are no other possibilities? Is it true that for , there exists a threshold so that if then is unimodal and if not it has two local maxima? (My guess is that the answer is yes and this threshold is around .)","\lambda_1, \lambda_2 > 0 f(y) = \lambda_1 e^{y-\lambda_1 e^y} + \lambda_2 e^{y-\lambda_2 e^y} \lambda_1 < \lambda_2 \frac{\lambda_2}{\lambda{1}} <  7.5 \frac{d}{dy} \left(\lambda_1 e^{y-\lambda_1 e^y} + \lambda_2 e^{y-\lambda_2 e^y} \right) = \lambda_1 e^{y - \lambda_1 e^y}  (1 - \lambda_1 e^y ) + \lambda_2 e^{y - \lambda_2 e^y}  (1 - \lambda_2 e^y ) \lambda_1 e^{y - \lambda_1 e^y}  (1 - \lambda_1 e^y ) + \lambda_2 e^{y - \lambda_2 e^y}  (1 - \lambda_2 e^y ) = 0 y \in \mathbb{R} z = e^y e^{y-1} 
 \lambda_1 e^{1-\lambda_1 z}(1-\lambda_1 z) +\lambda_2e^{1-\lambda_2 z}(1-\lambda_2 z) = 0
 z > 0 \lambda_1 = 1, \lambda_2 = 7 \lambda_1 = 1, \lambda_2 = 50 f(y) \lambda_2 > \lambda_1 > 0 c \frac{\lambda_2}{\lambda_1} < c f(y) 7.5",['calculus']
46,"Why does $\int_a^b f(x)h'(x) \, \mathrm{d}x=0$ imply that $f$ is constant?",Why does  imply that  is constant?,"\int_a^b f(x)h'(x) \, \mathrm{d}x=0 f","As an assignment, I have to prove the following: If $f(x)$ is a piecewise continuous function and $\int_a^b f(x)h'(x) \, \mathrm{d}x=0$ for all piecewise continuously differentiable $h(x)$ that satisfy $h(a)=h(b)=0$ , then $f$ is constant on $[a,b]$ . The assignment also provides some hints: Define $$c:=\frac{1}{b-a} \int_a^b f(x) \, \mathrm{d}x=\frac{1}{b-a} \sum_{i=1}^{m} \left ( \int_{x_{I-1}}^{x_i} f(x) \, \mathrm{d}x \right )$$ and use $$h(x)=\int_a^x f(s)-c \, \mathrm{d}s$$ Show that $$\int_a^b \left ( f(x)-c \right )h'(x) \, \mathrm{d}x=0$$ as well as $$\int_a^b \left ( f(x)-c \right )h'(x) \, \mathrm{d}x = \int_a^b \left ( f(x)-c \right )^2 \, \mathrm{d}x$$ and use this to conclude that $f(x)-c=0$ for all $x \in [a,b]$ . Now, from what I understand, $h(b)=0$ : $$h(x)=\int_a^b f(s)-c \, \mathrm{d}s=\int_a^b f(s) \, \mathrm{d}s-\left . cs \right |_{s=a}^b=(b-a)c-(cb-ca)=0$$ Obviously, $h(a)=0$ too. What I don't understand, is how I'm supposed to manipulate $\int_a^b \left ( f(x)-c \right )h'(x) \, \mathrm{d}x$ to obtain $\int_a^b \left ( f(x)-c \right )^2 \, \mathrm{d}x$ . I've tried numerous things (including integration by parts, which looks promising), but to no avail.","As an assignment, I have to prove the following: If is a piecewise continuous function and for all piecewise continuously differentiable that satisfy , then is constant on . The assignment also provides some hints: Define and use Show that as well as and use this to conclude that for all . Now, from what I understand, : Obviously, too. What I don't understand, is how I'm supposed to manipulate to obtain . I've tried numerous things (including integration by parts, which looks promising), but to no avail.","f(x) \int_a^b f(x)h'(x) \, \mathrm{d}x=0 h(x) h(a)=h(b)=0 f [a,b] c:=\frac{1}{b-a} \int_a^b f(x) \, \mathrm{d}x=\frac{1}{b-a} \sum_{i=1}^{m} \left ( \int_{x_{I-1}}^{x_i} f(x) \, \mathrm{d}x \right ) h(x)=\int_a^x f(s)-c \, \mathrm{d}s \int_a^b \left ( f(x)-c \right )h'(x) \, \mathrm{d}x=0 \int_a^b \left ( f(x)-c \right )h'(x) \, \mathrm{d}x = \int_a^b \left ( f(x)-c \right )^2 \, \mathrm{d}x f(x)-c=0 x \in [a,b] h(b)=0 h(x)=\int_a^b f(s)-c \, \mathrm{d}s=\int_a^b f(s) \, \mathrm{d}s-\left . cs \right |_{s=a}^b=(b-a)c-(cb-ca)=0 h(a)=0 \int_a^b \left ( f(x)-c \right )h'(x) \, \mathrm{d}x \int_a^b \left ( f(x)-c \right )^2 \, \mathrm{d}x","['calculus', 'integration', 'derivatives']"
47,Evaluating $\int_{-\infty}^0 \log(\frac{1}{2}\operatorname{erfc}(x))\mathrm dx$,Evaluating,\int_{-\infty}^0 \log(\frac{1}{2}\operatorname{erfc}(x))\mathrm dx,I am looking to evaluate $$\int_{-\infty}^0 \log\left(\frac{1}{2}\operatorname{erfc}(x)\right)\mathrm dx = -0.337~668~477...$$ Both Maple and Mathematica have failed to give a closed-form expression but indicate the value is around $−0.337~668~477...$ which doesn't appear to be close to some well known quantity.,I am looking to evaluate Both Maple and Mathematica have failed to give a closed-form expression but indicate the value is around which doesn't appear to be close to some well known quantity.,\int_{-\infty}^0 \log\left(\frac{1}{2}\operatorname{erfc}(x)\right)\mathrm dx = -0.337~668~477... −0.337~668~477...,"['calculus', 'integration', 'definite-integrals', 'error-function']"
48,"Find limit $a_{n + 1} = \int_{0}^{a_n}(1 + \frac{1}{4} \cos^{2n + 1} t)dt,$",Find limit,"a_{n + 1} = \int_{0}^{a_n}(1 + \frac{1}{4} \cos^{2n + 1} t)dt,","Find the limit of the sequence: $$a_{n + 1} = \int_{0}^{a_n}(1 + \frac{1}{4} \cos^{2n + 1} t)dt,$$ such that $a_0 \in (0, 2 \pi)$ That was one of the tasks in the Olympiad . Here is my approach. First, I wanted to simplify the integral: $\int_{0}^{a_n}(1 + \frac{1}{4} \cos^{2n + 1} t)dt = \int_{0}^{a_n}(dt) +  \frac{1}{4} \int_{0}^{a_n} \cos^{2n + 1} (t) dt$ That leads to the following relation: $$a_{n + 1} = a_n + \frac{1}{4} \int_{0}^{a_n} \cos^{2n + 1} (t) dt$$ Now, there is a $\cos t$ with some power which reminded me of the standart integral $\int \cos^n(x) dx$. We can find a recursive formula for it in the following way: $I_n = \int \cos^n(x) dx = \int \cos(x) \cos^{n - 1}(x) dx = \sin x \cos^{n - 1}x - (n - 1)\int \sin(x) \cos^{n - 2}(x) (- \sin (x)) dx.$ This leads to $I_n = \sin x \cos^{n - 1}x + (n - 1) I_{n - 2} - (n - 1) I_n$ And final recurrence relation is $$I_n = \frac{1}{n} \sin x \cos^{n - 1}x + \frac{n - 1}{n} I_{n - 2}$$ For a long time I am trying to make a connection between the original integral $\int_{0}^{a_n} \cos^{2n + 1} (t) dt$ and this recurrence relation, but I have failed to come up with anything meaningful at the moment. Well, I guess we can just plug in $2n + 1$ instead of $n$ and we get $$I_{2n + 1} = \frac{1}{2n + 1} \sin x \cos^{2n}x + \frac{2n}{2n + 1} I_{2n - 1}$$ Ok, now if we try to evaluate this as definite integral we should get $I_{2n + 1}(a_n) - I_{2n + 1}(0) = (\frac{1}{2n + 1} \sin a_n \cos^{2n}a_n + \frac{2n}{2n + 1} I_{2n - 1}(a_n)) - (0 + \frac{2n}{2n + 1} I_{2n - 1}(0))$ $I_{2n + 1}(a_n) - I_{2n + 1}(0) = \frac{1}{2n + 1} \sin a_n \cos^{2n}a_n + \frac{2n}{2n + 1} I_{2n - 1}(a_n) - \frac{2n}{2n + 1} I_{2n - 1}(0).$ So,  $$\frac{1}{4} \int_{0}^{a_n} \cos^{2n + 1} (t) dt = \frac{1}{4(2n + 1)} \sin a_n \cos^{2n}a_n + \frac{2n}{4(2n + 1)} \big[ I_{2n - 1}(a_n) - I_{2n - 1}(0) \big] $$ $$\frac{1}{4} \int_{0}^{a_n} \cos^{2n + 1} (t) dt = \frac{1}{4(2n + 1)} \sin a_n \cos^{2n}a_n + \frac{2n}{4(2n + 1)} \big[ I_{2n - 1}(a_n) - \cos a_0 \big] $$ I would appreciate any help if you provide me with some insights or clues on how to proceed.","Find the limit of the sequence: $$a_{n + 1} = \int_{0}^{a_n}(1 + \frac{1}{4} \cos^{2n + 1} t)dt,$$ such that $a_0 \in (0, 2 \pi)$ That was one of the tasks in the Olympiad . Here is my approach. First, I wanted to simplify the integral: $\int_{0}^{a_n}(1 + \frac{1}{4} \cos^{2n + 1} t)dt = \int_{0}^{a_n}(dt) +  \frac{1}{4} \int_{0}^{a_n} \cos^{2n + 1} (t) dt$ That leads to the following relation: $$a_{n + 1} = a_n + \frac{1}{4} \int_{0}^{a_n} \cos^{2n + 1} (t) dt$$ Now, there is a $\cos t$ with some power which reminded me of the standart integral $\int \cos^n(x) dx$. We can find a recursive formula for it in the following way: $I_n = \int \cos^n(x) dx = \int \cos(x) \cos^{n - 1}(x) dx = \sin x \cos^{n - 1}x - (n - 1)\int \sin(x) \cos^{n - 2}(x) (- \sin (x)) dx.$ This leads to $I_n = \sin x \cos^{n - 1}x + (n - 1) I_{n - 2} - (n - 1) I_n$ And final recurrence relation is $$I_n = \frac{1}{n} \sin x \cos^{n - 1}x + \frac{n - 1}{n} I_{n - 2}$$ For a long time I am trying to make a connection between the original integral $\int_{0}^{a_n} \cos^{2n + 1} (t) dt$ and this recurrence relation, but I have failed to come up with anything meaningful at the moment. Well, I guess we can just plug in $2n + 1$ instead of $n$ and we get $$I_{2n + 1} = \frac{1}{2n + 1} \sin x \cos^{2n}x + \frac{2n}{2n + 1} I_{2n - 1}$$ Ok, now if we try to evaluate this as definite integral we should get $I_{2n + 1}(a_n) - I_{2n + 1}(0) = (\frac{1}{2n + 1} \sin a_n \cos^{2n}a_n + \frac{2n}{2n + 1} I_{2n - 1}(a_n)) - (0 + \frac{2n}{2n + 1} I_{2n - 1}(0))$ $I_{2n + 1}(a_n) - I_{2n + 1}(0) = \frac{1}{2n + 1} \sin a_n \cos^{2n}a_n + \frac{2n}{2n + 1} I_{2n - 1}(a_n) - \frac{2n}{2n + 1} I_{2n - 1}(0).$ So,  $$\frac{1}{4} \int_{0}^{a_n} \cos^{2n + 1} (t) dt = \frac{1}{4(2n + 1)} \sin a_n \cos^{2n}a_n + \frac{2n}{4(2n + 1)} \big[ I_{2n - 1}(a_n) - I_{2n - 1}(0) \big] $$ $$\frac{1}{4} \int_{0}^{a_n} \cos^{2n + 1} (t) dt = \frac{1}{4(2n + 1)} \sin a_n \cos^{2n}a_n + \frac{2n}{4(2n + 1)} \big[ I_{2n - 1}(a_n) - \cos a_0 \big] $$ I would appreciate any help if you provide me with some insights or clues on how to proceed.",,"['calculus', 'integration', 'sequences-and-series', 'limits', 'recurrence-relations']"
49,"Where to start mathematics for Artificial Intelligence (Machine Learning, Probability, Robotics) [closed]","Where to start mathematics for Artificial Intelligence (Machine Learning, Probability, Robotics) [closed]",,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 7 years ago . Improve this question A bit about myself: I am Dan. I am from the rural parts of Africa. I have no formal education what so ever (not even schooling) so please mind my English. I have moved to the great United States seven years ago. Currently I work as Software Engineer, I have self taught programming. My work is related to machine learning and making predictions on top of it. Having said that, I know the syntactical things, I heavily use scikit learn in python. But how algorithms work - I have no idea. I searched and asked around. People suggest a courses on Coursera / EDX. Those courses actually have a prerequisite. I do not understand their explanations. Somehow, those are not intuitive to me. I manage to complete the assignments as well with the help from forums and/or other co-students. At the end of it, I am still empty. One of my friend from (he is from India) told I need Mathematics. This is something I have heard a lot of times. While learning C programming, this came up. While learning numpy, scipy it came up. But if you ask me, it never stopped me from going ahead. As I understand correctly, for the mastery that I imagine, Maths is must. Hence, I started my research of where to start learning. And in short - I am way more confused than before beginning. Following are the paths I explored. People say Linear Algebra is must for Machine Learning. Whenever I pick up some highly recommended book, they have Calculus in it (most say it as a prerequisite). When I pick up Calculus books, they say Algebra is must. When I pick up algebra, they say basic familiarity to Calculus is must. I have no idea where to start. Since more than three months, I am moving from one source to another, and it does not seem to be going anywhere. I visited the a private college in my area. I visited the public libraries. I am talking to the so called experts in the field. People give me various suggestions: Start from Calculus Made Easy, by F.R.S. (1910 edition). As I started reading, the references given to 'officially called' are completely unknown to me. I google those -> keep on wandering Start from Mathematics for a Practicle Man, by George Howe. This is the only book I could keep up with, but only partially. I am having hard time what is going on once Geometry starts. My friend from India (He is an expert - PhD Machine Learning) suggested that I should start from Discrete Mathematics. He suggested two books Discrete Mathematics Proof, Structures and Applications, by Rowan Garnier and John Taylor and another one - Discrete Mathematics by Norman Biggs. I started reading the latter, but very difficult to follow the proofs. The math teacher at the private college then again suggested me that I must learn the proof systems. He suggested another book - How to prove it, by Danial Velleman. The fact that I never saw inside of any classroom is proving too costly for me. People on the internet (especially quora) are more vague. Someone says learn ""Metamathematics"" someone would say ""precalculus"", someone will say ""Predicate Logic is must"" and others outright deny me that I can learn now. My goal is very very clear. I want to learn Artificial Intelligence systems. In that, create a robot, put some intelligence on top of it. Other streams are quite easy to follow - especially Electronics, Operating Systems, Arduino. Can you guide me to learn Linear Algebra, Probability (may be statistics if this is different than Probability) and Calculus AFTER KNOWING THAT I KNOW ABSOLUTELY ZERO MATHEMATICS. Off course, I know the additions and multiplications, but the formal introduction to the things adds a lot of different perspectives to my brain. One, may be off topic from the main question, why almost every book focuses so much on the proofs? I initially think I would be interested in knowing the Whats' than Hows'. Just like a function in programming - input and output. Is this helpful to have this mindset? Why is pro's and cos's of knowing and not knowing the proofs? My friend pointed me to this site. He said mostly your question will be closed, as opinion based questions are not valid. My take is - Isn't there simple sequence of books? Why such a straight forward question should have different opinions? And if this question is not entertained, where should I asked? I am already out of resources as already told above. Edit I forgot to add. I cannot follow the formal path of schooling - undergrad - grad. I have people to feed. My job just makes us all survive. I like the idea of getting hold of their syllabus and doing it at night at home. But, the whole intention is to do specific study rather than generic. Further to this, if you think and believe it is best bet to start with school subjects, may I further ask - Are there public syllabus available? Or I need to travel to school? Again, I just hope there are no differences from one school to the other school. Otherwise I will be facing the same issue - which school syllabus to follow? Note: My friend has been kind enough to help me ask question correctly here.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 7 years ago . Improve this question A bit about myself: I am Dan. I am from the rural parts of Africa. I have no formal education what so ever (not even schooling) so please mind my English. I have moved to the great United States seven years ago. Currently I work as Software Engineer, I have self taught programming. My work is related to machine learning and making predictions on top of it. Having said that, I know the syntactical things, I heavily use scikit learn in python. But how algorithms work - I have no idea. I searched and asked around. People suggest a courses on Coursera / EDX. Those courses actually have a prerequisite. I do not understand their explanations. Somehow, those are not intuitive to me. I manage to complete the assignments as well with the help from forums and/or other co-students. At the end of it, I am still empty. One of my friend from (he is from India) told I need Mathematics. This is something I have heard a lot of times. While learning C programming, this came up. While learning numpy, scipy it came up. But if you ask me, it never stopped me from going ahead. As I understand correctly, for the mastery that I imagine, Maths is must. Hence, I started my research of where to start learning. And in short - I am way more confused than before beginning. Following are the paths I explored. People say Linear Algebra is must for Machine Learning. Whenever I pick up some highly recommended book, they have Calculus in it (most say it as a prerequisite). When I pick up Calculus books, they say Algebra is must. When I pick up algebra, they say basic familiarity to Calculus is must. I have no idea where to start. Since more than three months, I am moving from one source to another, and it does not seem to be going anywhere. I visited the a private college in my area. I visited the public libraries. I am talking to the so called experts in the field. People give me various suggestions: Start from Calculus Made Easy, by F.R.S. (1910 edition). As I started reading, the references given to 'officially called' are completely unknown to me. I google those -> keep on wandering Start from Mathematics for a Practicle Man, by George Howe. This is the only book I could keep up with, but only partially. I am having hard time what is going on once Geometry starts. My friend from India (He is an expert - PhD Machine Learning) suggested that I should start from Discrete Mathematics. He suggested two books Discrete Mathematics Proof, Structures and Applications, by Rowan Garnier and John Taylor and another one - Discrete Mathematics by Norman Biggs. I started reading the latter, but very difficult to follow the proofs. The math teacher at the private college then again suggested me that I must learn the proof systems. He suggested another book - How to prove it, by Danial Velleman. The fact that I never saw inside of any classroom is proving too costly for me. People on the internet (especially quora) are more vague. Someone says learn ""Metamathematics"" someone would say ""precalculus"", someone will say ""Predicate Logic is must"" and others outright deny me that I can learn now. My goal is very very clear. I want to learn Artificial Intelligence systems. In that, create a robot, put some intelligence on top of it. Other streams are quite easy to follow - especially Electronics, Operating Systems, Arduino. Can you guide me to learn Linear Algebra, Probability (may be statistics if this is different than Probability) and Calculus AFTER KNOWING THAT I KNOW ABSOLUTELY ZERO MATHEMATICS. Off course, I know the additions and multiplications, but the formal introduction to the things adds a lot of different perspectives to my brain. One, may be off topic from the main question, why almost every book focuses so much on the proofs? I initially think I would be interested in knowing the Whats' than Hows'. Just like a function in programming - input and output. Is this helpful to have this mindset? Why is pro's and cos's of knowing and not knowing the proofs? My friend pointed me to this site. He said mostly your question will be closed, as opinion based questions are not valid. My take is - Isn't there simple sequence of books? Why such a straight forward question should have different opinions? And if this question is not entertained, where should I asked? I am already out of resources as already told above. Edit I forgot to add. I cannot follow the formal path of schooling - undergrad - grad. I have people to feed. My job just makes us all survive. I like the idea of getting hold of their syllabus and doing it at night at home. But, the whole intention is to do specific study rather than generic. Further to this, if you think and believe it is best bet to start with school subjects, may I further ask - Are there public syllabus available? Or I need to travel to school? Again, I just hope there are no differences from one school to the other school. Otherwise I will be facing the same issue - which school syllabus to follow? Note: My friend has been kind enough to help me ask question correctly here.",,"['calculus', 'linear-algebra', 'probability', 'algebra-precalculus', 'proof-verification']"
50,Absolute value in trigonometric substitutions,Absolute value in trigonometric substitutions,,"In general, when we are trying to remove radicals from integrals, we perform a trigonometric substitution (either a circular or hyperbolic trig function), but often this results in a radical of the form $\sqrt{(f(x))^2}$, with $f$ being an arbitrary trigonometric function. What most texts tend to do is simply take $\sqrt{(f(x))^2} = f(x)$, without the absolute value of |f(x)|, and the texts do not offer any motivation as to why $\sqrt{(f(x))^2} = f(x) \neq |f(x)|$. I would have assumed the correct way to proceed would be $\sqrt{(f(x))^2} = |f(x)|$. Why is this the case? I'll give an example to show further explain what I'm trying to ask : $$\text{Integrate} \ \ \  \int{\frac{1}{\sqrt{x^2 + 16}}}\ dx$$ We let $\ x = 4\tan\theta \implies dx = 4\sec^2\theta \ d\theta$ \begin{equation} \label{eq1} \begin{split} \implies\int{\frac{1}{\sqrt{x^2 + 16}}}\ dx & = \int{\frac{4\sec^2\theta \ }{\sqrt{(4\tan\theta)^2 + 4^2}}}\ d\theta \\  & = \int{\frac{4\sec^2\theta \ }{\sqrt{(4^2\sec^2\theta)}}}\ d\theta \\ &= \int{\frac{4\sec^2\theta \ }{4\cdot|\sec\theta \ |}}\ d\theta \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \text{(*)}\\ \end{split} \end{equation} What most texts do is omit the absolute value in the last starred step. Thus the denomitor of the integral becomes $\ 4\sec\theta \ $ instead of $4\cdot|\sec\theta \ |$ and there is no need to break the integral up into cases. Why is that so? We have not assumed $\sec\theta > 0$, so how can $|\sec\theta \ | = \sec\theta$?","In general, when we are trying to remove radicals from integrals, we perform a trigonometric substitution (either a circular or hyperbolic trig function), but often this results in a radical of the form $\sqrt{(f(x))^2}$, with $f$ being an arbitrary trigonometric function. What most texts tend to do is simply take $\sqrt{(f(x))^2} = f(x)$, without the absolute value of |f(x)|, and the texts do not offer any motivation as to why $\sqrt{(f(x))^2} = f(x) \neq |f(x)|$. I would have assumed the correct way to proceed would be $\sqrt{(f(x))^2} = |f(x)|$. Why is this the case? I'll give an example to show further explain what I'm trying to ask : $$\text{Integrate} \ \ \  \int{\frac{1}{\sqrt{x^2 + 16}}}\ dx$$ We let $\ x = 4\tan\theta \implies dx = 4\sec^2\theta \ d\theta$ \begin{equation} \label{eq1} \begin{split} \implies\int{\frac{1}{\sqrt{x^2 + 16}}}\ dx & = \int{\frac{4\sec^2\theta \ }{\sqrt{(4\tan\theta)^2 + 4^2}}}\ d\theta \\  & = \int{\frac{4\sec^2\theta \ }{\sqrt{(4^2\sec^2\theta)}}}\ d\theta \\ &= \int{\frac{4\sec^2\theta \ }{4\cdot|\sec\theta \ |}}\ d\theta \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \text{(*)}\\ \end{split} \end{equation} What most texts do is omit the absolute value in the last starred step. Thus the denomitor of the integral becomes $\ 4\sec\theta \ $ instead of $4\cdot|\sec\theta \ |$ and there is no need to break the integral up into cases. Why is that so? We have not assumed $\sec\theta > 0$, so how can $|\sec\theta \ | = \sec\theta$?",,"['calculus', 'integration', 'trigonometry', 'absolute-value']"
51,How to prove this inequality about the arc-length of convex functions?,How to prove this inequality about the arc-length of convex functions?,,"Let be $f,F:[0,1] \to \mathbb{R}$ with $f,F \in C^2([0,1])$ two convex functions such that $f \le F$ in all points  and $f(0)=F(0)$ , $f(1)=F(1)$ . Considering the plane-curves $\gamma(t)=(t,f(t))$ and $\Gamma(t)=(t,F(t)), t\in[0,1]$ prove that the arc length of $\gamma$ is equal or greater than $\Gamma$ 's one. Using length formula for a cartesian curve, this question became the following: demonstrate \begin{equation} \int_0^1 \sqrt{1+\{f'(t)\}^{2}}dt \ge \int_0^1 \sqrt{1+\{F'(t)\}^{2}}dt  \end{equation} Thanks for any help.","Let be with two convex functions such that in all points  and , . Considering the plane-curves and prove that the arc length of is equal or greater than 's one. Using length formula for a cartesian curve, this question became the following: demonstrate Thanks for any help.","f,F:[0,1] \to \mathbb{R} f,F \in C^2([0,1]) f \le F f(0)=F(0) f(1)=F(1) \gamma(t)=(t,f(t)) \Gamma(t)=(t,F(t)), t\in[0,1] \gamma \Gamma \begin{equation}
\int_0^1 \sqrt{1+\{f'(t)\}^{2}}dt \ge \int_0^1 \sqrt{1+\{F'(t)\}^{2}}dt 
\end{equation}","['calculus', 'definite-integrals', 'convex-analysis', 'plane-curves']"
52,How to find the zeros of this function?,How to find the zeros of this function?,,"There is a function, called $f(x)$, where: $$ f(x) = 2(x-a) + 2\cos x (\sin x - b) $$ $a$ and $b$ are constants. I would like to find all the possible values of $x$ where $ f(x) = 0 $ I've tried to solve it this way: First I simplified the equation: $$ 2x - 2a + 2\cos x\sin x - 2b\cos x = 0 $$ Then I replaced the $2\cos x\sin x$ to $\sin 2x$, and moved it to the other side: $$ 2a - 2x + 2b\cos x = \sin 2x$$ After that I used the arcsine function: $$ x_1 = \frac{1}{2} \arcsin(2a - 2x + 2b\cos x) + 2n\pi$$ $$ x_2 = \pi - \frac{1}{2} \arcsin(2a - 2x + 2b\cos x) + 2n\pi$$ I don't know how to continue it. It is probably a dead end. Could you please give me hints about how should I solve it? I would like to express $x$ without using $x$.","There is a function, called $f(x)$, where: $$ f(x) = 2(x-a) + 2\cos x (\sin x - b) $$ $a$ and $b$ are constants. I would like to find all the possible values of $x$ where $ f(x) = 0 $ I've tried to solve it this way: First I simplified the equation: $$ 2x - 2a + 2\cos x\sin x - 2b\cos x = 0 $$ Then I replaced the $2\cos x\sin x$ to $\sin 2x$, and moved it to the other side: $$ 2a - 2x + 2b\cos x = \sin 2x$$ After that I used the arcsine function: $$ x_1 = \frac{1}{2} \arcsin(2a - 2x + 2b\cos x) + 2n\pi$$ $$ x_2 = \pi - \frac{1}{2} \arcsin(2a - 2x + 2b\cos x) + 2n\pi$$ I don't know how to continue it. It is probably a dead end. Could you please give me hints about how should I solve it? I would like to express $x$ without using $x$.",,"['calculus', 'trigonometry']"
53,"Understanding implicit differentiation with concepts like ""function"" and ""lambda abstraction.""","Understanding implicit differentiation with concepts like ""function"" and ""lambda abstraction.""",,"In high school, we learned to reason like so: $$(*) \qquad \frac{d}{dx}(x^2+x) = \frac{d}{dx}(x^2)+\frac{d}{dx}(x) = 2x+1$$ Now that I know more, I can ""reanalyze"" this chain of reasoning using ideas that I have more faith in, like ""function"" and ""lambda abstraction."" We begin by defining $\nabla (f)$ as the derivative of $f$. Then the above chain of reasoning becomes: $$\nabla\mathop{\lambda}_{x:\mathbb{R}}(x^2+x) = \nabla\mathop{\lambda}_{x:\mathbb{R}}(x^2)+\nabla\mathop{\lambda}_{x:\mathbb{R}}(x) = \left(\mathop{\lambda}_{x:\mathbb{R}}2x\right) +\left(\mathop{\lambda}_{x:\mathbb{R}}1\right) = \mathop{\lambda}_{x:\mathbb{R}}(2x+1)$$ So I can confidently say that I understand $(*)$, because I can reanalyze it in terms of ideas that I have a lot of faith in, like ""function"" and ""lambda abstraction."" Onwards. In high school, we also learned a pattern of reasoning that was referred to as ""implicit differentiation."" It looks a bit like so: Suppose $y^2+x = x^2+y.$ Then: $$\frac{d}{dx}(y^2+x) = \frac{d}{dx}(x^2+y).$$ $$\therefore 2y \frac{dy}{dx}+1 = 2x+ \frac{dy}{dx}$$ $$\therefore (2y-1)\frac{dy}{dx} = 2x-1$$ $$\therefore \left(\frac{dy}{dx} = \frac{2x-1}{2y-1}\right) \vee (y = 1/2)$$ Unfortunately, I still have absolute no idea what any of this means. Question. How can we reanalyze implicit differentiation using respectable concepts like ""function"" and ""lambda abstraction"" and ""limit"", and without using concepts such as as ""dependent variable"" and ""independent variable"" and ""differential."" Edit. It seems to be unclear what I'm looking for. I'm not looking for handwaiving and intuition. I want something as formal as possible, with the minimum of handwaiving. For example, if you're going to ""switch"" semantics so that $y^2+x=x+y^2$ is no longer a condition on pairs of points $(x,y) \in \mathbb{R}^2$ but instead becomes a condition on smooth functions $(a,b) \rightarrow \mathbb{R}^2,$ you should make that completely clear, introduce notation for the set of paths, etc., and the rest of your answer should be phrased in terms of this notation. Write definitions. State theorems if relevant. Tell me the domains and codomains of all your functions. I want the absolute technical logical and/or set-theoretic nitty-gritty here.","In high school, we learned to reason like so: $$(*) \qquad \frac{d}{dx}(x^2+x) = \frac{d}{dx}(x^2)+\frac{d}{dx}(x) = 2x+1$$ Now that I know more, I can ""reanalyze"" this chain of reasoning using ideas that I have more faith in, like ""function"" and ""lambda abstraction."" We begin by defining $\nabla (f)$ as the derivative of $f$. Then the above chain of reasoning becomes: $$\nabla\mathop{\lambda}_{x:\mathbb{R}}(x^2+x) = \nabla\mathop{\lambda}_{x:\mathbb{R}}(x^2)+\nabla\mathop{\lambda}_{x:\mathbb{R}}(x) = \left(\mathop{\lambda}_{x:\mathbb{R}}2x\right) +\left(\mathop{\lambda}_{x:\mathbb{R}}1\right) = \mathop{\lambda}_{x:\mathbb{R}}(2x+1)$$ So I can confidently say that I understand $(*)$, because I can reanalyze it in terms of ideas that I have a lot of faith in, like ""function"" and ""lambda abstraction."" Onwards. In high school, we also learned a pattern of reasoning that was referred to as ""implicit differentiation."" It looks a bit like so: Suppose $y^2+x = x^2+y.$ Then: $$\frac{d}{dx}(y^2+x) = \frac{d}{dx}(x^2+y).$$ $$\therefore 2y \frac{dy}{dx}+1 = 2x+ \frac{dy}{dx}$$ $$\therefore (2y-1)\frac{dy}{dx} = 2x-1$$ $$\therefore \left(\frac{dy}{dx} = \frac{2x-1}{2y-1}\right) \vee (y = 1/2)$$ Unfortunately, I still have absolute no idea what any of this means. Question. How can we reanalyze implicit differentiation using respectable concepts like ""function"" and ""lambda abstraction"" and ""limit"", and without using concepts such as as ""dependent variable"" and ""independent variable"" and ""differential."" Edit. It seems to be unclear what I'm looking for. I'm not looking for handwaiving and intuition. I want something as formal as possible, with the minimum of handwaiving. For example, if you're going to ""switch"" semantics so that $y^2+x=x+y^2$ is no longer a condition on pairs of points $(x,y) \in \mathbb{R}^2$ but instead becomes a condition on smooth functions $(a,b) \rightarrow \mathbb{R}^2,$ you should make that completely clear, introduce notation for the set of paths, etc., and the rest of your answer should be phrased in terms of this notation. Write definitions. State theorems if relevant. Tell me the domains and codomains of all your functions. I want the absolute technical logical and/or set-theoretic nitty-gritty here.",,"['calculus', 'derivatives', 'implicit-differentiation']"
54,"Hands of the clock, Revisited.","Hands of the clock, Revisited.",,"It has already been answered ( here ) that it is impossible for the (continuously moving) hands of a clock to trisect the face of said clock. Even ideally the hour, minute, and second hand can never pairwise form 120 degree angles with each other. Can we find the time of day where they most nearly do? To put a metric on this one could take the area of the smallest sector divided by the largest sector. (One being the unatainable optimum.) Or add up the amount by which the three angles are off. $$\left|\frac{2\pi}{3} - \measuredangle H M\right| + \left|\frac{2\pi}{3} - \measuredangle H S\right| + \left|\frac{2\pi}{3} - \measuredangle M S\right|$$ where zero is now the unatainable optimum.","It has already been answered ( here ) that it is impossible for the (continuously moving) hands of a clock to trisect the face of said clock. Even ideally the hour, minute, and second hand can never pairwise form 120 degree angles with each other. Can we find the time of day where they most nearly do? To put a metric on this one could take the area of the smallest sector divided by the largest sector. (One being the unatainable optimum.) Or add up the amount by which the three angles are off. $$\left|\frac{2\pi}{3} - \measuredangle H M\right| + \left|\frac{2\pi}{3} - \measuredangle H S\right| + \left|\frac{2\pi}{3} - \measuredangle M S\right|$$ where zero is now the unatainable optimum.",,"['calculus', 'geometry']"
55,Recommend books for learning math from elementary school?,Recommend books for learning math from elementary school?,,"I've learned math long time ago, but I hardly remember anything. I really want to relearn by reading  good books, from the ground up. Workbooks didn't really help. Other posts recommend different books. So a list of best books feels helpful from Pre-algebra to calculus and beyond. Would you be so kind to add which others you recommend. English is not my native language; please excuse typing errors. Children The Number Devil: A Mathematical Adventure by Hans Magnus Enzensberger Alice in Puzzle-Land by Raymond M. Smullyan The Phantom Tollbooth by Norton Juster ... Beginner What Is Mathematics? An Elementary Approach to Ideas and Methods by Richard Courant Mathematics: Its Content, Methods and Meaning (Dover Books on Mathematics) by A. D. Aleksandrov Mathematics: A very short introduction by Timothy Gowers ... Basic Math Basic Mathematics by Serge Lang 1998  (high school or college students.) ... Pre-Algebra Pre-Algebra DeMYSTiFieD by Allan Bluman ... Algebra Algebra by Israel M. Gelfand 2013 A book of Abstract Algebra by Charles Pinter? ... Algebra II ... Trigonometry Trigonometry by I.M. Gelfand 2013 ... Pre-Calculus ... Calculus Calculus by Michael Spivak ... Problem Solving https://en.wikipedia.org/wiki/Yakov_Perelman#Books How to Solve it by Polya's Techniques in Problem Solving by Steven George Krantz The Art and Craft of Problem Solving by Paul Zeitz How to Prove It: A Structured Approach by Daniel J. Velleman 2006 Problem-Solving Strategies (Problem Books in Mathematics) by Arthur Engel Number Theory (Dover Books on Mathematics) by George E. Andrews Some of the books in the Art of Problem Solving series at www.ArtofProblemSolving.com","I've learned math long time ago, but I hardly remember anything. I really want to relearn by reading  good books, from the ground up. Workbooks didn't really help. Other posts recommend different books. So a list of best books feels helpful from Pre-algebra to calculus and beyond. Would you be so kind to add which others you recommend. English is not my native language; please excuse typing errors. Children The Number Devil: A Mathematical Adventure by Hans Magnus Enzensberger Alice in Puzzle-Land by Raymond M. Smullyan The Phantom Tollbooth by Norton Juster ... Beginner What Is Mathematics? An Elementary Approach to Ideas and Methods by Richard Courant Mathematics: Its Content, Methods and Meaning (Dover Books on Mathematics) by A. D. Aleksandrov Mathematics: A very short introduction by Timothy Gowers ... Basic Math Basic Mathematics by Serge Lang 1998  (high school or college students.) ... Pre-Algebra Pre-Algebra DeMYSTiFieD by Allan Bluman ... Algebra Algebra by Israel M. Gelfand 2013 A book of Abstract Algebra by Charles Pinter? ... Algebra II ... Trigonometry Trigonometry by I.M. Gelfand 2013 ... Pre-Calculus ... Calculus Calculus by Michael Spivak ... Problem Solving https://en.wikipedia.org/wiki/Yakov_Perelman#Books How to Solve it by Polya's Techniques in Problem Solving by Steven George Krantz The Art and Craft of Problem Solving by Paul Zeitz How to Prove It: A Structured Approach by Daniel J. Velleman 2006 Problem-Solving Strategies (Problem Books in Mathematics) by Arthur Engel Number Theory (Dover Books on Mathematics) by George E. Andrews Some of the books in the Art of Problem Solving series at www.ArtofProblemSolving.com",,"['calculus', 'soft-question', 'arithmetic']"
56,"Two functions discontinuous, but sum continuous","Two functions discontinuous, but sum continuous",,"I have some exercise that asks me this: Find $f$ and $g$ discontinuos such that $f+g$ is continuous. This is what I tought: $$f = \mbox{sign}(x)$$ $$g = -\mbox{sign}(x)$$ Where $\mbox{sign}(x)$ is the function that maps to $1$ if $x\ge0$ and $-1$ if $x<0$. Then, the two are discontinuous in $0$, but the sum, is continuous, because it's equal to $0$ in every point. First of all, is this example correct? The exercise also asks me to find examples such that: $f,g$ discontinuous, but $f$ composed with $g$ is continuous. (need help) $f$ continuous, $g$ discontinuous but the composite $f$ with $g$ is continuous. (for the second one, if I take $g=\mbox{sign}(x)$ and $f=|x|$, then $fog = 1$. Is this right? Could you guys give at least a hint, or a less poor example?","I have some exercise that asks me this: Find $f$ and $g$ discontinuos such that $f+g$ is continuous. This is what I tought: $$f = \mbox{sign}(x)$$ $$g = -\mbox{sign}(x)$$ Where $\mbox{sign}(x)$ is the function that maps to $1$ if $x\ge0$ and $-1$ if $x<0$. Then, the two are discontinuous in $0$, but the sum, is continuous, because it's equal to $0$ in every point. First of all, is this example correct? The exercise also asks me to find examples such that: $f,g$ discontinuous, but $f$ composed with $g$ is continuous. (need help) $f$ continuous, $g$ discontinuous but the composite $f$ with $g$ is continuous. (for the second one, if I take $g=\mbox{sign}(x)$ and $f=|x|$, then $fog = 1$. Is this right? Could you guys give at least a hint, or a less poor example?",,['calculus']
57,Stokes' Theorem Explanation,Stokes' Theorem Explanation,,"Can someone explain what Stokes' Theorem is measuring? What would taking the integral of a vector on a surface give you? When would you use it? This is the only definition I have and I don't really understand what it's saying. Let $S$ be an oriented smooth surface that is bounded by a simple, closed, smooth boundary curve $C$ with positive orientation. Also let $\vec{F}$ be a vector field then, $$\int\limits_C \vec{F} \cdot d\vec{r} = \iint\limits_S \mathrm{curl}\ \vec{F} \cdot d\vec{S}$$","Can someone explain what Stokes' Theorem is measuring? What would taking the integral of a vector on a surface give you? When would you use it? This is the only definition I have and I don't really understand what it's saying. Let $S$ be an oriented smooth surface that is bounded by a simple, closed, smooth boundary curve $C$ with positive orientation. Also let $\vec{F}$ be a vector field then, $$\int\limits_C \vec{F} \cdot d\vec{r} = \iint\limits_S \mathrm{curl}\ \vec{F} \cdot d\vec{S}$$",,"['calculus', 'multivariable-calculus', 'definition', 'intuition']"
58,"Mathematically, why does $[ma]\mathrm{d}x = [mv]\mathrm{d}v$?","Mathematically, why does ?",[ma]\mathrm{d}x = [mv]\mathrm{d}v,"I am taking an introductory level class, Physics with Calculus, using Priscilla Laws' Workshop Physics. The activity guide has asked me to prove that: $$ma\,\mathrm{d}x = mv\,\mathrm{d}v$$ My physics instructor informed me that the correct method to prove this is: Start with: $$ma\,\mathrm{d}x$$ Knowing that: $$a = \frac{\mathrm{d}v}{\mathrm{d}t}$$ Therefore: $$ma\,\mathrm{d}x = m\frac{\mathrm{d}v}{\mathrm{d}t}\mathrm{d}x$$ Rearrange using laws of multiplication: $$m\frac{\mathrm{d}v}{\mathrm{d}t}\mathrm{d}x = m\frac{\mathrm{d}x}{\mathrm{d}t}\mathrm{d}v$$ Knowing that: $$v = \frac{\mathrm{d}x}{\mathrm{d}t}$$ Therefore: $$m\frac{\mathrm{d}x}{\mathrm{d}t}\mathrm{d}v = mv\,\mathrm{d}v$$ Therefore: $$ma\,\mathrm{d}x = mv\,\mathrm{d}v$$ But every single mathematician I have talked to agrees; this is magic math. It is not good practice to, quite frankly, abuse infinitesimals in this way. And yet, the book suggesting that we do this has been under the careful eye of physicists nationally for years -- Surely if doing the above were wrong, someone would have pointed it out by now, and insisted that it be removed. By this reasoning, clearly the above is not necessarily wrong per se, but it is incredibly clear that there is a lot more mathematical complication going on under the hood which makes the above possible. It is my goal here to understand why in the name of goodness the above works, and what rules had to be maintained in order to ensure its validity. This brings me to the first part of my question -- Why does the above work? What could go wrong using the methods used above? How can I avoid making mistakes when manipulating infinitesimals in that way? Unfortunately, being an entry level calculus student, I am frankly not equipped to deal with such topics as partial derivatives, differential equations, or infinitesimals/hyperreals. Even so, I have been determined to understand, at least informally, the underlying math behind the above. And what little ""knowledge"" I have amassed thus far implies something which I find disturbing. Before I explain what I, personally, see wrong with the above, please allow me to informally define what I understand differentials to be, as well as a more explicit notation through which to write them. I am sure that an official notation exists for this, but I do not know enough in mathematics to use such a notation. When we have $y = f(x)$, and say $f'(x) = \frac{\mathrm{d}y}{\mathrm{d}x}$, what we are ""really"" saying is this: The ratio between the infinitely small change in $y$ at some solution set $x$ resulting from a corresponding infinitely small change in $x$, and the infinitely small change in $x$ at some location $x$ from a corresponding infinitely small change in $x$. The above sentence translates into mathematical notation, literally as: $$\lim_{h\to 0} \biggl[\frac{f(y + h) - f(h)}{x + h - x}\biggr]$$ where $\mathrm{d}y = \lim_{h\to 0} [f(x + h) - f(x)]$, and $lim_{h\to 0} [dx = (x + h - x)]$. Now, this is all fine and dandy, until we have a relationship such as: $$y = xz = f(x, y)$$ Suddenly $\mathrm{d}y$ could be either: $\mathrm{d}y = (f(x + h, z) - f(x, z))$ or: $\mathrm{d}y = (f(x, z + h) - f(x, z))$ (In fact, there exists a third situation where $\mathrm{d}y = y + h - y$ if $y$ is what we are taking the derivative/integral with respect to.) With these rather tedious ambiguities, it seems prudent to not only state the variable whose change we are observing, but to also state which variable we are changing to create this change. Furthermore, it would be nice if we could also keep track of what values all of our variables start with. Therefore, I shall define a new syntax to make this simpler: if $y = f(x)$ then: $$f'(A.x) = \frac{\mathrm{d}y_{A_x}}{\mathrm{d}x_{A_x}}$$ Where $A$ is the solution set vector (containing values of $x$, and $y$, and for instance $A.x$ is the $x$ value contained by $A$) where all of our variables start, and where $\mathrm{d}y_{A_x}$ means: Starting with the values contained in $A$, and changing $x$ by some infinitesimal $h$, what is observed change in $y$? and when $\mathrm{d}x_{A_x}$ means: Starting with the values contained in $A$, and changing $x$ by some infinitesimal $h$, what is observed change in $x$?"" By these definitions, we see that our notion of $f'(x)$ hasn't changed -- we are merely being more specific about what's changing, and what changes we are observing. Now, using this new notation, and trying to use the same proof that my physics instructor suggested, we run into some issues. Executing the first four steps: Start with: $$ma\,\mathrm{d}x_{A_x}$$ ($\mathrm{d}x_{A_x}$ was chosen (as opposed to, for instance, $\mathrm{d}x_{A_t}$) because it was originally the trailing $\mathrm{d}x$ of an integral taken with respect to $x$) Knowing that: $$a = \frac{\mathrm{d}v_{A_t}}{\mathrm{d}t_{A_t}}$$ Therefore: $$ma\,\mathrm{d}x_{A_x} = m\frac{\mathrm{d}v_{A_t}}{\mathrm{d}t_{A_t}}\mathrm{d}x$$ Rearrange using laws of multiplication: $$m\frac{\mathrm{d}v_{A_t}}{\mathrm{d}t_{A_t}}\mathrm{d}x_{A_x} = m\frac{\mathrm{d}x_{A_x}}{\mathrm{d}t_{A_t}}\mathrm{d}v_{A_t}$$ We end up with the expression $m\frac{\mathrm{d}x_{A_x}}{\mathrm{d}t_{A_t}}\mathrm{d}v_{A_t}$. Now, in the method suggested by my physics instructor, we wold normally try to replace $\frac{\mathrm{d}x}{\mathrm{d}t}$ (or in this case, $\frac{\mathrm{d}x_{A_x}}{\mathrm{d}t_{A_t}}$) with $v$ -- but there's a problem: $$v = \frac{\mathrm{d}x_{A_t}}{\mathrm{d}t_{A_t}}\text{ NOT }\frac{\mathrm{d}x_{A_x}}{\mathrm{d}t_{A_t}}$$ In order for us to replace $\frac{\mathrm{d}x_{A_x}}{\mathrm{d}t_{A_t}}$ with $v$, $\mathrm{d}x_{A_x}$ would have to equal $\mathrm{d}x_{A_t}$! That's an implication that I am not certain is true, and this is what bothers me about the method prescribed by my instructor. This brings me to the second half of my question: Am I correct in thinking that the method prescribed by my instructor is wrong for this reason? Are the concepts I am conveying here valid, or is there another way of viewing this? Is there an alternative method which I could use to prove the same thing, but does not fall prey to these problems? Does $\mathrm{d}x_{A_x} = \mathrm{d}x_{A_t}$? Will $\mathrm{d}x_{A_x}$ ALWAYS equal $\mathrm{d}x_{A_t}$ by definition, or do they only equal in this case, because of the nature of kinematic motion?","I am taking an introductory level class, Physics with Calculus, using Priscilla Laws' Workshop Physics. The activity guide has asked me to prove that: $$ma\,\mathrm{d}x = mv\,\mathrm{d}v$$ My physics instructor informed me that the correct method to prove this is: Start with: $$ma\,\mathrm{d}x$$ Knowing that: $$a = \frac{\mathrm{d}v}{\mathrm{d}t}$$ Therefore: $$ma\,\mathrm{d}x = m\frac{\mathrm{d}v}{\mathrm{d}t}\mathrm{d}x$$ Rearrange using laws of multiplication: $$m\frac{\mathrm{d}v}{\mathrm{d}t}\mathrm{d}x = m\frac{\mathrm{d}x}{\mathrm{d}t}\mathrm{d}v$$ Knowing that: $$v = \frac{\mathrm{d}x}{\mathrm{d}t}$$ Therefore: $$m\frac{\mathrm{d}x}{\mathrm{d}t}\mathrm{d}v = mv\,\mathrm{d}v$$ Therefore: $$ma\,\mathrm{d}x = mv\,\mathrm{d}v$$ But every single mathematician I have talked to agrees; this is magic math. It is not good practice to, quite frankly, abuse infinitesimals in this way. And yet, the book suggesting that we do this has been under the careful eye of physicists nationally for years -- Surely if doing the above were wrong, someone would have pointed it out by now, and insisted that it be removed. By this reasoning, clearly the above is not necessarily wrong per se, but it is incredibly clear that there is a lot more mathematical complication going on under the hood which makes the above possible. It is my goal here to understand why in the name of goodness the above works, and what rules had to be maintained in order to ensure its validity. This brings me to the first part of my question -- Why does the above work? What could go wrong using the methods used above? How can I avoid making mistakes when manipulating infinitesimals in that way? Unfortunately, being an entry level calculus student, I am frankly not equipped to deal with such topics as partial derivatives, differential equations, or infinitesimals/hyperreals. Even so, I have been determined to understand, at least informally, the underlying math behind the above. And what little ""knowledge"" I have amassed thus far implies something which I find disturbing. Before I explain what I, personally, see wrong with the above, please allow me to informally define what I understand differentials to be, as well as a more explicit notation through which to write them. I am sure that an official notation exists for this, but I do not know enough in mathematics to use such a notation. When we have $y = f(x)$, and say $f'(x) = \frac{\mathrm{d}y}{\mathrm{d}x}$, what we are ""really"" saying is this: The ratio between the infinitely small change in $y$ at some solution set $x$ resulting from a corresponding infinitely small change in $x$, and the infinitely small change in $x$ at some location $x$ from a corresponding infinitely small change in $x$. The above sentence translates into mathematical notation, literally as: $$\lim_{h\to 0} \biggl[\frac{f(y + h) - f(h)}{x + h - x}\biggr]$$ where $\mathrm{d}y = \lim_{h\to 0} [f(x + h) - f(x)]$, and $lim_{h\to 0} [dx = (x + h - x)]$. Now, this is all fine and dandy, until we have a relationship such as: $$y = xz = f(x, y)$$ Suddenly $\mathrm{d}y$ could be either: $\mathrm{d}y = (f(x + h, z) - f(x, z))$ or: $\mathrm{d}y = (f(x, z + h) - f(x, z))$ (In fact, there exists a third situation where $\mathrm{d}y = y + h - y$ if $y$ is what we are taking the derivative/integral with respect to.) With these rather tedious ambiguities, it seems prudent to not only state the variable whose change we are observing, but to also state which variable we are changing to create this change. Furthermore, it would be nice if we could also keep track of what values all of our variables start with. Therefore, I shall define a new syntax to make this simpler: if $y = f(x)$ then: $$f'(A.x) = \frac{\mathrm{d}y_{A_x}}{\mathrm{d}x_{A_x}}$$ Where $A$ is the solution set vector (containing values of $x$, and $y$, and for instance $A.x$ is the $x$ value contained by $A$) where all of our variables start, and where $\mathrm{d}y_{A_x}$ means: Starting with the values contained in $A$, and changing $x$ by some infinitesimal $h$, what is observed change in $y$? and when $\mathrm{d}x_{A_x}$ means: Starting with the values contained in $A$, and changing $x$ by some infinitesimal $h$, what is observed change in $x$?"" By these definitions, we see that our notion of $f'(x)$ hasn't changed -- we are merely being more specific about what's changing, and what changes we are observing. Now, using this new notation, and trying to use the same proof that my physics instructor suggested, we run into some issues. Executing the first four steps: Start with: $$ma\,\mathrm{d}x_{A_x}$$ ($\mathrm{d}x_{A_x}$ was chosen (as opposed to, for instance, $\mathrm{d}x_{A_t}$) because it was originally the trailing $\mathrm{d}x$ of an integral taken with respect to $x$) Knowing that: $$a = \frac{\mathrm{d}v_{A_t}}{\mathrm{d}t_{A_t}}$$ Therefore: $$ma\,\mathrm{d}x_{A_x} = m\frac{\mathrm{d}v_{A_t}}{\mathrm{d}t_{A_t}}\mathrm{d}x$$ Rearrange using laws of multiplication: $$m\frac{\mathrm{d}v_{A_t}}{\mathrm{d}t_{A_t}}\mathrm{d}x_{A_x} = m\frac{\mathrm{d}x_{A_x}}{\mathrm{d}t_{A_t}}\mathrm{d}v_{A_t}$$ We end up with the expression $m\frac{\mathrm{d}x_{A_x}}{\mathrm{d}t_{A_t}}\mathrm{d}v_{A_t}$. Now, in the method suggested by my physics instructor, we wold normally try to replace $\frac{\mathrm{d}x}{\mathrm{d}t}$ (or in this case, $\frac{\mathrm{d}x_{A_x}}{\mathrm{d}t_{A_t}}$) with $v$ -- but there's a problem: $$v = \frac{\mathrm{d}x_{A_t}}{\mathrm{d}t_{A_t}}\text{ NOT }\frac{\mathrm{d}x_{A_x}}{\mathrm{d}t_{A_t}}$$ In order for us to replace $\frac{\mathrm{d}x_{A_x}}{\mathrm{d}t_{A_t}}$ with $v$, $\mathrm{d}x_{A_x}$ would have to equal $\mathrm{d}x_{A_t}$! That's an implication that I am not certain is true, and this is what bothers me about the method prescribed by my instructor. This brings me to the second half of my question: Am I correct in thinking that the method prescribed by my instructor is wrong for this reason? Are the concepts I am conveying here valid, or is there another way of viewing this? Is there an alternative method which I could use to prove the same thing, but does not fall prey to these problems? Does $\mathrm{d}x_{A_x} = \mathrm{d}x_{A_t}$? Will $\mathrm{d}x_{A_x}$ ALWAYS equal $\mathrm{d}x_{A_t}$ by definition, or do they only equal in this case, because of the nature of kinematic motion?",,"['calculus', 'multivariable-calculus', 'differential']"
59,Maximum and Minimum Value of $f(x)$,Maximum and Minimum Value of,f(x),"$$f(x)=\sin(x)+\int_{-\pi/2}^{\pi/2}\left(\sin(x)+t\cos(x)\right)f(t)\,\mathrm dt$$ Find maximum and minimum values of $f(x)$ . I tried to simplify this expression by checking even or odd property of $f(x)$ . We can write the above expression as $$f(x)=(1+I_1)\sin(x)+I_2\cos(x)$$ where $$I_1=\int_{-\pi/2}^{\pi/2}f(t)\,\mathrm dt$$ and $$I_2=\int_{-\pi/2}^{\pi/2}tf(t)\,\mathrm dt$$ if $f$ is even $I_2=0$ and if $f$ is odd $I_1=0$ , but if $f$ is even $$f(x)=(1+I_1)\sin(x)$$ which is odd. Similarly if $f$ is odd $$f(x)=\sin(x)+I_2\cos(x)$$ which is neither even nor odd. So $f(x)$ is neither even nor odd. Now to find maxima or minima $f'(x)=0$ i.e., $$f'(x)=(1+I_1)\cos(x)-I_2\sin(x)=0$$ $\implies$ $$\tan(x)=\frac{1+I_1}{I_2}$$ Unable to proceed further...","Find maximum and minimum values of . I tried to simplify this expression by checking even or odd property of . We can write the above expression as where and if is even and if is odd , but if is even which is odd. Similarly if is odd which is neither even nor odd. So is neither even nor odd. Now to find maxima or minima i.e., Unable to proceed further...","f(x)=\sin(x)+\int_{-\pi/2}^{\pi/2}\left(\sin(x)+t\cos(x)\right)f(t)\,\mathrm dt f(x) f(x) f(x)=(1+I_1)\sin(x)+I_2\cos(x) I_1=\int_{-\pi/2}^{\pi/2}f(t)\,\mathrm dt I_2=\int_{-\pi/2}^{\pi/2}tf(t)\,\mathrm dt f I_2=0 f I_1=0 f f(x)=(1+I_1)\sin(x) f f(x)=\sin(x)+I_2\cos(x) f(x) f'(x)=0 f'(x)=(1+I_1)\cos(x)-I_2\sin(x)=0 \implies \tan(x)=\frac{1+I_1}{I_2}","['calculus', 'integration', 'trigonometry', 'optimization']"
60,"Solve the functional equation $f(x)=f\left({x\over 3}\right)+f\left({2x\over 3}\right)$ with $f : [0,\infty) \to \mathbb R$ continuous",Solve the functional equation  with  continuous,"f(x)=f\left({x\over 3}\right)+f\left({2x\over 3}\right) f : [0,\infty) \to \mathbb R","Solve the functional equation $$f(x)=f\left({x\over 3}\right)+f\left({2x\over 3}\right)\qquad \forall x\geq 0$$    with $f : [0,\infty) \to \mathbb R$ continuous. I can't manage to get this one to the form of Cauchy's functional equation , but I imagine that's how it's maybe done.","Solve the functional equation $$f(x)=f\left({x\over 3}\right)+f\left({2x\over 3}\right)\qquad \forall x\geq 0$$    with $f : [0,\infty) \to \mathbb R$ continuous. I can't manage to get this one to the form of Cauchy's functional equation , but I imagine that's how it's maybe done.",,"['calculus', 'functions', 'continuity', 'problem-solving']"
61,"What's the difference between direction, sense, and orientation?","What's the difference between direction, sense, and orientation?",,"I'm trying to understand the difference between the sense, orientation, and direction of a vector. According to this , sense is specified by two points on a line parallel to a vector. Orientation is specified by the relationship between the vector and given reference lines (which I'm interpreting to be some basis). However, these two definitions seem to be synonymous with direction. How do these 3 terms differ?","I'm trying to understand the difference between the sense, orientation, and direction of a vector. According to this , sense is specified by two points on a line parallel to a vector. Orientation is specified by the relationship between the vector and given reference lines (which I'm interpreting to be some basis). However, these two definitions seem to be synonymous with direction. How do these 3 terms differ?",,"['calculus', 'definition']"
62,How to prove $\zeta'(0)/\zeta(0)=\log(2\pi)$?,How to prove ?,\zeta'(0)/\zeta(0)=\log(2\pi),"How do I prove that $\zeta'(0)/\zeta(0)=\log(2\pi)$ ? I can get $\zeta(0)=-\frac{1}{2}$, but I don't know how to calculate $\zeta'(0)=-\frac{1}{2}\log(2\pi)$ ? Can you help me ? Here $\zeta(s)$ is Riemann zeta function: $$\zeta(s):=\sum_{n=1}^{\infty}\frac{1}{n^s}. $$","How do I prove that $\zeta'(0)/\zeta(0)=\log(2\pi)$ ? I can get $\zeta(0)=-\frac{1}{2}$, but I don't know how to calculate $\zeta'(0)=-\frac{1}{2}\log(2\pi)$ ? Can you help me ? Here $\zeta(s)$ is Riemann zeta function: $$\zeta(s):=\sum_{n=1}^{\infty}\frac{1}{n^s}. $$",,"['calculus', 'riemann-zeta']"
63,Proving $\sin(1/x)$ has an antiderivative,Proving  has an antiderivative,\sin(1/x),"I would want to prove that the function defined as follows: $f(x)=\sin(1/x)$ and $f(0)=0$ has an antiderivative on the entire $\mathbb{R}$ (well, I'm not sure if I haven't worded this awkwardly, but essentially I am trying to show that $f$ is a derivative of some function that is differentiable in every $x\in\mathbb{R}$). First off, $f$ is continuous on $\mathbb{R}\setminus0$, hence it has an antiderivative for every $x\in\mathbb{R}\setminus0$. However, $f$ is not continuous in $x=0$. Is there a way we can deal with that? Furthermore, I'm curious about the significance of $f(0)=0$. Suppose we redefine our function, and take $f(0)=c$, for some $c\in\mathbb{R}$, $c\ne{0}$. Would then $f$ have an antiderivative on $\mathbb{R}$?","I would want to prove that the function defined as follows: $f(x)=\sin(1/x)$ and $f(0)=0$ has an antiderivative on the entire $\mathbb{R}$ (well, I'm not sure if I haven't worded this awkwardly, but essentially I am trying to show that $f$ is a derivative of some function that is differentiable in every $x\in\mathbb{R}$). First off, $f$ is continuous on $\mathbb{R}\setminus0$, hence it has an antiderivative for every $x\in\mathbb{R}\setminus0$. However, $f$ is not continuous in $x=0$. Is there a way we can deal with that? Furthermore, I'm curious about the significance of $f(0)=0$. Suppose we redefine our function, and take $f(0)=c$, for some $c\in\mathbb{R}$, $c\ne{0}$. Would then $f$ have an antiderivative on $\mathbb{R}$?",,"['calculus', 'integration']"
64,Deriving the addition formula of $\sin u$ from a total differential equation,Deriving the addition formula of  from a total differential equation,\sin u,How do we derive the addition formula of $\sin u$ from the following equation? $$\frac{dx}{\sqrt{1 - x^2}} + \frac{dy}{\sqrt{1 - y^2}} = 0$$ Motivation Let $u = \int_{0}^{x}\frac{dt}{\sqrt{1 - t^2}}$ Then $x = \sin u$ Let $v = \int_{0}^{y}\frac{dt}{\sqrt{1 - t^2}}$ Then $y = \sin v$ Let $u + v = const.$ Then $d(u + v) = \frac{dx}{\sqrt{1 - x^2}} + \frac{dy}{\sqrt{1 - y^2}} = 0$,How do we derive the addition formula of $\sin u$ from the following equation? $$\frac{dx}{\sqrt{1 - x^2}} + \frac{dy}{\sqrt{1 - y^2}} = 0$$ Motivation Let $u = \int_{0}^{x}\frac{dt}{\sqrt{1 - t^2}}$ Then $x = \sin u$ Let $v = \int_{0}^{y}\frac{dt}{\sqrt{1 - t^2}}$ Then $y = \sin v$ Let $u + v = const.$ Then $d(u + v) = \frac{dx}{\sqrt{1 - x^2}} + \frac{dy}{\sqrt{1 - y^2}} = 0$,,"['calculus', 'ordinary-differential-equations']"
65,Differentiating under integral sign -- trig counterexample,Differentiating under integral sign -- trig counterexample,,"I really hate integration by parts, so when faced with $\int_{-\pi}^\pi x^2 \cos n x \, dx$ I tried writing it as $$\int_{-\pi}^\pi x^2 \cos n x \, dx = \frac{d}{dn} \int_{-\pi}^\pi x \sin n x \, dx =  \frac{d^2}{dn^2} \int_{-\pi}^\pi  \cos n x \, dx $$1 I have done something wrong.  The integrand is continuously differentiable with respect to n and I thought that was enough.  How can I get differentiating under the integral sign to work?","I really hate integration by parts, so when faced with $\int_{-\pi}^\pi x^2 \cos n x \, dx$ I tried writing it as $$\int_{-\pi}^\pi x^2 \cos n x \, dx = \frac{d}{dn} \int_{-\pi}^\pi x \sin n x \, dx =  \frac{d^2}{dn^2} \int_{-\pi}^\pi  \cos n x \, dx $$1 I have done something wrong.  The integrand is continuously differentiable with respect to n and I thought that was enough.  How can I get differentiating under the integral sign to work?",,['calculus']
66,Bounding ${(2d-1)n-1\choose n-1}$,Bounding,{(2d-1)n-1\choose n-1},"Claim: ${3n-1\choose n-1}\le 6.25^n$. Why? Can the proof be extended to obtain a bound on ${(2d-1)n-1\choose     n-1}$, with the bound being $f(d)^n$ for some function $f$? (These numbers describe the number of some $d$-dimensional combinatorial objects; claim 1 is the case $d=2$, and is not my claim).","Claim: ${3n-1\choose n-1}\le 6.25^n$. Why? Can the proof be extended to obtain a bound on ${(2d-1)n-1\choose     n-1}$, with the bound being $f(d)^n$ for some function $f$? (These numbers describe the number of some $d$-dimensional combinatorial objects; claim 1 is the case $d=2$, and is not my claim).",,"['calculus', 'combinatorics', 'inequality', 'binomial-coefficients']"
67,Prove that $ \sum \frac{2^k}{k}$ is divisible by $2^M$,Prove that  is divisible by, \sum \frac{2^k}{k} 2^M,"For each integer $M > 0$ , there ${\bf exists}$ an $n$ such that $$ \sum_{k=1}^n \dfrac{ 2^k}{k} $$ is divisible by $2^M$ ${\bf try}$ Im struggling a bit to visualize this exercise. So, I tried to see for concrete number, for instance take $M=1$ , then $n=2$ works: as $$ 2 + \dfrac{2^1}{2} = 2^1 (1 + 2 )$$ Now, take $M=2$ and factor $$ 2^{2} \underbrace{ \left( \dfrac{1}{2} + \dfrac{1}{2} + \dfrac{2}{3} + \dfrac{2^4}{4} ... + \dfrac{2^{n-2} }{n} \right) }_{(*)}$$ now, we need to choose $n$ so that $(*)$ is an integer. Im unable to do so. Any help?","For each integer , there an such that is divisible by Im struggling a bit to visualize this exercise. So, I tried to see for concrete number, for instance take , then works: as Now, take and factor now, we need to choose so that is an integer. Im unable to do so. Any help?",M > 0 {\bf exists} n  \sum_{k=1}^n \dfrac{ 2^k}{k}  2^M {\bf try} M=1 n=2  2 + \dfrac{2^1}{2} = 2^1 (1 + 2 ) M=2  2^{2} \underbrace{ \left( \dfrac{1}{2} + \dfrac{1}{2} + \dfrac{2}{3} + \dfrac{2^4}{4} ... + \dfrac{2^{n-2} }{n} \right) }_{(*)} n (*),"['calculus', 'arithmetic']"
68,"Maximize $\int_{0}^{1} f(x)^5 dx$ over all $f\colon[0,1]\to[-1,1]$ with $\int_{0}^{1} f(x)^3 dx=\int_{0}^{1} f(x) dx= 0$.",Maximize  over all  with .,"\int_{0}^{1} f(x)^5 dx f\colon[0,1]\to[-1,1] \int_{0}^{1} f(x)^3 dx=\int_{0}^{1} f(x) dx= 0","Maximize $\int_{0}^{1} f(x)^5 dx$ over all $f\colon[0,1]\to[-1,1]$ with $\int_{0}^{1} f(x)^3 dx=\int_{0}^{1} f(x) dx= 0$ . I'm not even sure where to start with this problem. Any hints would be appreciated.",Maximize over all with . I'm not even sure where to start with this problem. Any hints would be appreciated.,"\int_{0}^{1} f(x)^5 dx f\colon[0,1]\to[-1,1] \int_{0}^{1} f(x)^3 dx=\int_{0}^{1} f(x) dx= 0","['calculus', 'integration', 'calculus-of-variations']"
69,"If $f(x+y)=f(x)f(y)-g(x)g(y)$ and $g(x+y)=f(x)g(y)+f(y)g(x)$, with $f'(0)=0$, determine $[f(x)]^2+[g(x)]^2$","If  and , with , determine",f(x+y)=f(x)f(y)-g(x)g(y) g(x+y)=f(x)g(y)+f(y)g(x) f'(0)=0 [f(x)]^2+[g(x)]^2,"Given the expressions: $f(x+y)=f(x)f(y)-g(x)g(y)$ $g(x+y)=f(x)g(y)+f(y)g(x)$ the exercise is to show that $[f(x)]^2+[g(x)]^2$ is constant for all real $x$ and determine its value, knowing that $f$ and $g$ are real differentiable non-constant functions, and that $f'(0)=0$ . I realized it looks like the $\sin$ and $\cos$ functions, so the answer must be $1$ . To prove something that way, I tried showing that $f$ and $g$ were always on the interval $[-1,1]$ . I have also tried to derivate each expression and plug in $x=y=0$ or only $y=0$ , but was unable to develop the solution.","Given the expressions: the exercise is to show that is constant for all real and determine its value, knowing that and are real differentiable non-constant functions, and that . I realized it looks like the and functions, so the answer must be . To prove something that way, I tried showing that and were always on the interval . I have also tried to derivate each expression and plug in or only , but was unable to develop the solution.","f(x+y)=f(x)f(y)-g(x)g(y) g(x+y)=f(x)g(y)+f(y)g(x) [f(x)]^2+[g(x)]^2 x f g f'(0)=0 \sin \cos 1 f g [-1,1] x=y=0 y=0","['calculus', 'functions', 'derivatives']"
70,Evaluating $S(n)=\int_0^{\pi/2} \log(\sin x)^n\mathrm dx$,Evaluating,S(n)=\int_0^{\pi/2} \log(\sin x)^n\mathrm dx,"I would like to see how you evaluate $$S(n)=\int_0^{\pi/2} \log(\sin x)^n\mathrm dx,\qquad n\in\Bbb N_0$$ Here's how I do it. Start with $$\int_0^{\pi/2}\sin(x)^a\cos(x)^b\mathrm dx$$ Use $t=\sin(x)^2$ to see that $$\int_0^{\pi/2}\sin(x)^a\cos(x)^b\mathrm dx=\frac12\int_0^1t^{\frac{a-1}2}(1-t)^{\frac{b-1}2}\mathrm dt=\frac{\Gamma(\frac{a+1}2)\Gamma(\frac{b+1}2)}{2\Gamma(\frac{a+b}2+1)}$$ So $$\int_0^{\pi/2}\sin(x)^{2a}\mathrm dx=\frac{\sqrt\pi}2\frac{\Gamma(a+\frac12)}{\Gamma(a+1)}$$ Taking $\left(\frac{d}{da}\right)^n$ on both sides $$2^{n}\int_0^{\pi/2}\sin(x)^{2a}\log(\sin x)^n\mathrm dx=\frac{\sqrt\pi}2\left(\frac{d}{da}\right)^n\,\frac{\Gamma(a+\frac12)}{\Gamma(a+1)}$$ And evaluating at $a=0$ , $$S(n)=\frac{\sqrt{\pi}}{2^{n+1}}\left[\left(\frac{d}{da}\right)^n\,\frac{\Gamma(a+\frac12)}{\Gamma(a+1)}\right]_{a=0}$$ Which is a closed form. Enjoy!","I would like to see how you evaluate Here's how I do it. Start with Use to see that So Taking on both sides And evaluating at , Which is a closed form. Enjoy!","S(n)=\int_0^{\pi/2} \log(\sin x)^n\mathrm dx,\qquad n\in\Bbb N_0 \int_0^{\pi/2}\sin(x)^a\cos(x)^b\mathrm dx t=\sin(x)^2 \int_0^{\pi/2}\sin(x)^a\cos(x)^b\mathrm dx=\frac12\int_0^1t^{\frac{a-1}2}(1-t)^{\frac{b-1}2}\mathrm dt=\frac{\Gamma(\frac{a+1}2)\Gamma(\frac{b+1}2)}{2\Gamma(\frac{a+b}2+1)} \int_0^{\pi/2}\sin(x)^{2a}\mathrm dx=\frac{\sqrt\pi}2\frac{\Gamma(a+\frac12)}{\Gamma(a+1)} \left(\frac{d}{da}\right)^n 2^{n}\int_0^{\pi/2}\sin(x)^{2a}\log(\sin x)^n\mathrm dx=\frac{\sqrt\pi}2\left(\frac{d}{da}\right)^n\,\frac{\Gamma(a+\frac12)}{\Gamma(a+1)} a=0 S(n)=\frac{\sqrt{\pi}}{2^{n+1}}\left[\left(\frac{d}{da}\right)^n\,\frac{\Gamma(a+\frac12)}{\Gamma(a+1)}\right]_{a=0}","['calculus', 'integration']"
71,The Gravity Potential of an Ellipsoid,The Gravity Potential of an Ellipsoid,,"The Original Problem Recently, I have encountered the following volume integral $$\mathcal{I}=\int_{\Omega}\frac{1}{|\mathbf{x}-\mathbf{x}'|}dV(\mathbf{x}') \tag{1}$$ where $\Omega$ is an ellipsoid defined by $$\Omega=\bigg\{(x'_1,x'_2,x'_3):\frac{{x'}_1^2}{a_1^2}+\frac{{x'}_2^2}{a_2^2}+\frac{{x'}_3^2}{a_3^2}\le1\bigg\}$$ and $a_1,\,a_2,\,a_3$ are the semi-major axes of ellipsoid. This representation of $\Omega$ also implies that the origin of our coordinate system is at the center of the ellipsoid. Also, $\mathbf{x}$ is some arbitrary point in $\mathbb{R}^3$ which may belong to $\Omega$ or not. This integral arises in different problems of physics like the gravitation of a solid ellipsoidal mass or the electrostatic field of a solid ellipsoidal distribution of charges. I want to simplify this integral as far as possible. I have been told that it can be represented in terms a simple integral over the real line and the answer will depend on wheather the point $\mathbf{x}$ is inside or outside $\Omega$ . I first do a translation by using the change of coordinates $\mathbf{x}'-\mathbf{x}=\mathbf{y}$ \begin{align*} \mathcal{I} &= \int_{\Omega}\frac{1}{|\mathbf{x}-\mathbf{x}'|}dV(\mathbf{x}') \\ &= \int_{\Omega}\frac{1}{|\mathbf{x}'-\mathbf{x}|}dV(\mathbf{x}') \\ &= \int_{\Omega}\frac{1}{|\mathbf{y}|}dV(\mathbf{y})  \end{align*} consequently, $\Omega$ can be represented as below $$\Omega=\bigg\{(y_1,y_2,y_3):\frac{(y_1+x_1)^2}{a_1^2}+\frac{(y_2+x_2)^2}{a_2^2}+\frac{(y_3+x_3)^2}{a_3^2}\le1\bigg\}$$ and then I use the following transformation of coordinates \begin{align*} y_1&=r\sin\theta\cos\phi \\ y_2&=r\sin\theta\sin\phi \\ y_3&=r\cos\theta \end{align*} so I can get \begin{align*} |\mathbf{y}|&=r \\ dV(\mathbf{y})&=r^2\sin\theta dr\,d\theta\,d\phi \\ \Omega&=\bigg\{(r,\theta,\phi):\frac{(r\sin\theta\cos\phi+x_1)^2}{a_1^2}+\frac{(r\sin\theta\sin\phi+x_2)^2}{a_2^2}+\frac{(r\cos\theta+x_3)^2}{a_3^2}\le1\bigg\} \end{align*} so the integral becomes \begin{align*}  \mathcal{I}=\int_{\Omega}r\sin\theta\,dr\,d\theta\,d\phi \tag{2} \end{align*} I am stuck right here. Can someone help to determine the limits of integration in $(2)$ and proceed to do the integration? I don't need every detail. An outline will also suffice but not just the final answer! Any help is appreciated. :) Extension of the Original Problem Indeed Eq. (1) happens when the density of the distribution of mass or charges is uniform. A more interesting problem would be to evaluate the following integral. $$\mathcal{I}=\int_{\Omega}\frac{\rho(\mathbf{x}')}{|\mathbf{x}-\mathbf{x}'|}dV(\mathbf{x}') \tag{3}$$ as a special case of the above problem, I am interested in the evaluation of the following integral $$\mathcal{I}_{IJ\dots K}=\int_{\Omega}\frac{x'_Ix'_J \dots x'_K}{|\mathbf{x}-\mathbf{x}'|}dV(\mathbf{x}') \tag{4}$$ where $I,\,J,\dots,K$ are positive integers which range from $1$ to $3$ and $x'_I$ is the $I$ th component of $\mathbf{x}'$ with respect to the standard Cartesian basis.","The Original Problem Recently, I have encountered the following volume integral where is an ellipsoid defined by and are the semi-major axes of ellipsoid. This representation of also implies that the origin of our coordinate system is at the center of the ellipsoid. Also, is some arbitrary point in which may belong to or not. This integral arises in different problems of physics like the gravitation of a solid ellipsoidal mass or the electrostatic field of a solid ellipsoidal distribution of charges. I want to simplify this integral as far as possible. I have been told that it can be represented in terms a simple integral over the real line and the answer will depend on wheather the point is inside or outside . I first do a translation by using the change of coordinates consequently, can be represented as below and then I use the following transformation of coordinates so I can get so the integral becomes I am stuck right here. Can someone help to determine the limits of integration in and proceed to do the integration? I don't need every detail. An outline will also suffice but not just the final answer! Any help is appreciated. :) Extension of the Original Problem Indeed Eq. (1) happens when the density of the distribution of mass or charges is uniform. A more interesting problem would be to evaluate the following integral. as a special case of the above problem, I am interested in the evaluation of the following integral where are positive integers which range from to and is the th component of with respect to the standard Cartesian basis.","\mathcal{I}=\int_{\Omega}\frac{1}{|\mathbf{x}-\mathbf{x}'|}dV(\mathbf{x}') \tag{1} \Omega \Omega=\bigg\{(x'_1,x'_2,x'_3):\frac{{x'}_1^2}{a_1^2}+\frac{{x'}_2^2}{a_2^2}+\frac{{x'}_3^2}{a_3^2}\le1\bigg\} a_1,\,a_2,\,a_3 \Omega \mathbf{x} \mathbb{R}^3 \Omega \mathbf{x} \Omega \mathbf{x}'-\mathbf{x}=\mathbf{y} \begin{align*}
\mathcal{I} &= \int_{\Omega}\frac{1}{|\mathbf{x}-\mathbf{x}'|}dV(\mathbf{x}') \\
&= \int_{\Omega}\frac{1}{|\mathbf{x}'-\mathbf{x}|}dV(\mathbf{x}') \\
&= \int_{\Omega}\frac{1}{|\mathbf{y}|}dV(\mathbf{y}) 
\end{align*} \Omega \Omega=\bigg\{(y_1,y_2,y_3):\frac{(y_1+x_1)^2}{a_1^2}+\frac{(y_2+x_2)^2}{a_2^2}+\frac{(y_3+x_3)^2}{a_3^2}\le1\bigg\} \begin{align*}
y_1&=r\sin\theta\cos\phi \\
y_2&=r\sin\theta\sin\phi \\
y_3&=r\cos\theta
\end{align*} \begin{align*}
|\mathbf{y}|&=r \\
dV(\mathbf{y})&=r^2\sin\theta dr\,d\theta\,d\phi \\
\Omega&=\bigg\{(r,\theta,\phi):\frac{(r\sin\theta\cos\phi+x_1)^2}{a_1^2}+\frac{(r\sin\theta\sin\phi+x_2)^2}{a_2^2}+\frac{(r\cos\theta+x_3)^2}{a_3^2}\le1\bigg\}
\end{align*} \begin{align*} 
\mathcal{I}=\int_{\Omega}r\sin\theta\,dr\,d\theta\,d\phi
\tag{2}
\end{align*} (2) \mathcal{I}=\int_{\Omega}\frac{\rho(\mathbf{x}')}{|\mathbf{x}-\mathbf{x}'|}dV(\mathbf{x}') \tag{3} \mathcal{I}_{IJ\dots K}=\int_{\Omega}\frac{x'_Ix'_J \dots x'_K}{|\mathbf{x}-\mathbf{x}'|}dV(\mathbf{x}') \tag{4} I,\,J,\dots,K 1 3 x'_I I \mathbf{x}'","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
72,Constructing a convex function with prescribed Hessians at two given points,Constructing a convex function with prescribed Hessians at two given points,,"Given two positive definite matrices $A, B \in \Bbb R^{n \times n}$, is there a way to construct a convex function $f: \Bbb R^n \to \Bbb R$ such that $$\nabla^2f(x)=A \qquad \text{and} \qquad \nabla^2f(y)=B$$ for two distinct $x,y \in \Bbb R^n$ (say $x=0_n$, $y=1_n$)? I've tried the usual suspects (stuff with quadratics, exponentials, etc) without going anywhere.","Given two positive definite matrices $A, B \in \Bbb R^{n \times n}$, is there a way to construct a convex function $f: \Bbb R^n \to \Bbb R$ such that $$\nabla^2f(x)=A \qquad \text{and} \qquad \nabla^2f(y)=B$$ for two distinct $x,y \in \Bbb R^n$ (say $x=0_n$, $y=1_n$)? I've tried the usual suspects (stuff with quadratics, exponentials, etc) without going anywhere.",,"['calculus', 'convex-analysis', 'hessian-matrix']"
73,Analyze if the function is uniformly continuous: $f(x) = \frac{x}{1+x^{2}}$,Analyze if the function is uniformly continuous:,f(x) = \frac{x}{1+x^{2}},"Analyze if the function is uniformly continuous: $$f(x) = \frac{x}{1+x^{2}}, x\in\mathbb{R}$$ We are free to use anything but I'd like to solve it with $\varepsilon$-$\delta$. And I also think this is the best way doing, too. I found out that the function isn't uniformly continuous but I'm not sure at all (I just claim this because I couldn't find a solution..): $$\left|\frac{x}{1+x^{2}}-\frac{x_{0}}{1+x_{0}^{2}}\right | = \left|\frac{x(1+x_{0}^{2})-\left(x_{0}(1+x^{2})\right)}{(1+x^{2})(1+x_{0}^{2})}\right |= \left|\frac{x+x\cdot x_{0}^{2}-x_{0}-x_{0}\cdot x^{2}}{(1+x^{2})(1+x_{0}^{2})}\right|$$ $$= \left| \frac{x-x_{0}+x \cdot x_{0}^{2}-x_{0} \cdot x^{2}}{(1+x^{2})(1+x_{0}^{2})}\right| < \left|\frac{\delta + x \cdot x_{0}^{2}-x_{0} \cdot x^{2}}{(1+x^{2})(1+x_{0}^{2})}\right | < \left|\frac{\delta + x \cdot x_{0}^{2}}{(1+x^{2}) (1+x_{0}^{2})}\right| < \delta +x \cdot x_{0}^{2}$$ Omg, first I must say it was a pain typing all that as a LATEX beginner... Anyway, what makes me feel bad is I cannot see if it's uniformly continuous or not, the way I solved it. Cannot even see if the function is continuous at all (but if I look at it, it seems like it's continuous at least). Did I do anything correctly at all?","Analyze if the function is uniformly continuous: $$f(x) = \frac{x}{1+x^{2}}, x\in\mathbb{R}$$ We are free to use anything but I'd like to solve it with $\varepsilon$-$\delta$. And I also think this is the best way doing, too. I found out that the function isn't uniformly continuous but I'm not sure at all (I just claim this because I couldn't find a solution..): $$\left|\frac{x}{1+x^{2}}-\frac{x_{0}}{1+x_{0}^{2}}\right | = \left|\frac{x(1+x_{0}^{2})-\left(x_{0}(1+x^{2})\right)}{(1+x^{2})(1+x_{0}^{2})}\right |= \left|\frac{x+x\cdot x_{0}^{2}-x_{0}-x_{0}\cdot x^{2}}{(1+x^{2})(1+x_{0}^{2})}\right|$$ $$= \left| \frac{x-x_{0}+x \cdot x_{0}^{2}-x_{0} \cdot x^{2}}{(1+x^{2})(1+x_{0}^{2})}\right| < \left|\frac{\delta + x \cdot x_{0}^{2}-x_{0} \cdot x^{2}}{(1+x^{2})(1+x_{0}^{2})}\right | < \left|\frac{\delta + x \cdot x_{0}^{2}}{(1+x^{2}) (1+x_{0}^{2})}\right| < \delta +x \cdot x_{0}^{2}$$ Omg, first I must say it was a pain typing all that as a LATEX beginner... Anyway, what makes me feel bad is I cannot see if it's uniformly continuous or not, the way I solved it. Cannot even see if the function is continuous at all (but if I look at it, it seems like it's continuous at least). Did I do anything correctly at all?",,"['calculus', 'analysis', 'continuity', 'epsilon-delta']"
74,Tensor Calculus,Tensor Calculus,,"I am currently a 3rd year undergraduate electronic engineering student. I have completed a course in dynamics, calculus I, calculus II and calculus III. I've started self studying tensor calculus, my sources are the video lecture series on the YouTube channel; ""MathTheBeautiful"" and the freeware textbook/notes; ""Introduction to Tensor Calculus"" by Kees Dullemond & Kasper Peeters. Other textbooks go much more in depth in advanced math topics. I have been through the first 3 chapters and watched the first 5 videos, but I don't seem to understand the content. I don't know what I should take from these lectures and notes and what part of the work to focus on in order to start practicing as soon as possible. I want to learn tensor calculus in order to study more advanced mathematics and physics such as; General Relativity, Differential Geometry, Continuum Mechanics etc. I've also seen many other textbooks on continuum mechanics and tensor analysis for mathematicians/physicists. All of these sources seem quite different and seem like I require much more advanced topics in mathematics in order to understand. How should I approach tensor calculus? through a physics or through a mathematics perspective? From what I've seen, tensor calculus seems very abstract and more towards the proving side of the spectrum (like a pure mathematics subject), it doesn't look ""practicable"" as appose to other calculus courses where I could go to any chapter in the textbook and find many problems to practice and become familiar with the concept. Is my current knowledge in calculus and physics + dynamics enough, or do I need to first learn a few more concepts in mathematics in order to begin attacking tensor calculus problems?","I am currently a 3rd year undergraduate electronic engineering student. I have completed a course in dynamics, calculus I, calculus II and calculus III. I've started self studying tensor calculus, my sources are the video lecture series on the YouTube channel; ""MathTheBeautiful"" and the freeware textbook/notes; ""Introduction to Tensor Calculus"" by Kees Dullemond & Kasper Peeters. Other textbooks go much more in depth in advanced math topics. I have been through the first 3 chapters and watched the first 5 videos, but I don't seem to understand the content. I don't know what I should take from these lectures and notes and what part of the work to focus on in order to start practicing as soon as possible. I want to learn tensor calculus in order to study more advanced mathematics and physics such as; General Relativity, Differential Geometry, Continuum Mechanics etc. I've also seen many other textbooks on continuum mechanics and tensor analysis for mathematicians/physicists. All of these sources seem quite different and seem like I require much more advanced topics in mathematics in order to understand. How should I approach tensor calculus? through a physics or through a mathematics perspective? From what I've seen, tensor calculus seems very abstract and more towards the proving side of the spectrum (like a pure mathematics subject), it doesn't look ""practicable"" as appose to other calculus courses where I could go to any chapter in the textbook and find many problems to practice and become familiar with the concept. Is my current knowledge in calculus and physics + dynamics enough, or do I need to first learn a few more concepts in mathematics in order to begin attacking tensor calculus problems?",,"['calculus', 'differential-geometry', 'physics', 'tensors', 'general-relativity']"
75,Prove that $\ln$ and $\exp$ are inverses,Prove that  and  are inverses,\ln \exp,If we take the definitions of $\exp$ and $\ln$ as follows: $\exp(x) = {\large\sum\limits_{i=0}^\infty} \dfrac{x^i}{i!}$ $\ln(x) = {\large\int_1^x} \dfrac1t\ dt$ how could we prove that these functions are inverses? Neither $$\exp(\ln(x)) = \sum^\infty_{i=0}\frac{\left(\int_1^x\frac 1t\ dt\right)^i}{i!}$$ nor $$\ln(\exp(x)) = \int_1^{\sum\limits_{i=0}^\infty \frac{x^i}{i!}} \frac 1t\ dt$$ look at all feasible to me.  Is there some theorem(s) that'd help make this a bit easier?  Hints are welcome. :),If we take the definitions of $\exp$ and $\ln$ as follows: $\exp(x) = {\large\sum\limits_{i=0}^\infty} \dfrac{x^i}{i!}$ $\ln(x) = {\large\int_1^x} \dfrac1t\ dt$ how could we prove that these functions are inverses? Neither $$\exp(\ln(x)) = \sum^\infty_{i=0}\frac{\left(\int_1^x\frac 1t\ dt\right)^i}{i!}$$ nor $$\ln(\exp(x)) = \int_1^{\sum\limits_{i=0}^\infty \frac{x^i}{i!}} \frac 1t\ dt$$ look at all feasible to me.  Is there some theorem(s) that'd help make this a bit easier?  Hints are welcome. :),,"['calculus', 'sequences-and-series', 'exponential-function', 'inverse']"
76,Evaluating $\int e^{x\sin x+\cos x}\left(\frac{x^4\cos^3 x-x\sin x+\cos x}{x^2\cos^2 x}\right)dx$,Evaluating,\int e^{x\sin x+\cos x}\left(\frac{x^4\cos^3 x-x\sin x+\cos x}{x^2\cos^2 x}\right)dx,"Evaluate $$\displaystyle \int e^{x\sin x+\cos x}\left(\frac{x^4\cos^3 x-x\sin x+\cos x}{x^2\cos^2 x}\right)dx$$ $\bf{My\; Try::}$ Let $$\begin{align}I &= \int e^{x\sin x+\cos x}\left(\frac{x^4\cos^3 x-x\sin x+\cos x}{x^2\cos^2 x}\right)dx\\ &=\int e^{x\sin x+\cos x}\left(x^2\cos x+\frac{\cos x-x\sin x}{x^2\cos^2 x}\right)dx\\ &= \int x\cdot e^{x\sin x+\cos x}\left(x\cos x\right)dx+\int e^{x\sin x+\cos x}\left(\frac{\cos x-x\sin x}{x^2\cos^2 x}\right)dx\\ \end{align}$$ Now Let $x\sin x+\cos x = t\;,$ Then $x\cos x\,dx = dt$ and Integration by parts for $\bf{1^{st}}$ Integral So $$\displaystyle I = x\cdot e^{x\sin x+\cos x}-\int e^{x\sin x+\cos x}dx+\int e^{x\sin x+\cos x}\left(\frac{\cos x-x\sin x}{x^2\cos^2 x}\right)dx$$ Now I do not understand how to solve after that.","Evaluate $$\displaystyle \int e^{x\sin x+\cos x}\left(\frac{x^4\cos^3 x-x\sin x+\cos x}{x^2\cos^2 x}\right)dx$$ $\bf{My\; Try::}$ Let $$\begin{align}I &= \int e^{x\sin x+\cos x}\left(\frac{x^4\cos^3 x-x\sin x+\cos x}{x^2\cos^2 x}\right)dx\\ &=\int e^{x\sin x+\cos x}\left(x^2\cos x+\frac{\cos x-x\sin x}{x^2\cos^2 x}\right)dx\\ &= \int x\cdot e^{x\sin x+\cos x}\left(x\cos x\right)dx+\int e^{x\sin x+\cos x}\left(\frac{\cos x-x\sin x}{x^2\cos^2 x}\right)dx\\ \end{align}$$ Now Let $x\sin x+\cos x = t\;,$ Then $x\cos x\,dx = dt$ and Integration by parts for $\bf{1^{st}}$ Integral So $$\displaystyle I = x\cdot e^{x\sin x+\cos x}-\int e^{x\sin x+\cos x}dx+\int e^{x\sin x+\cos x}\left(\frac{\cos x-x\sin x}{x^2\cos^2 x}\right)dx$$ Now I do not understand how to solve after that.",,"['calculus', 'integration', 'indefinite-integrals']"
77,Prove this closed-form of sum of ${_4F_3}$ hypergeometric functions,Prove this closed-form of sum of  hypergeometric functions,{_4F_3},"I think the following identity is true. How could we prove it? $${_4F_3}\left(\begin{array}c 1,1,1,1 \\\tfrac54,2,2\end{array}\middle|\,1\right) + 3\,{_4F_3}\left(\begin{array}c\tfrac12,\tfrac12,1,1\\\tfrac32,\tfrac32,\tfrac32\end{array}\middle|\,1\right) = \frac{\pi^3}{8} + \frac{ 3\pi^2}{4}\ln 2 - \frac 72 \zeta (3).$$ Here ${_pF_q}$ is the generalized hypergeometric function and $\zeta(3)$ is Apéry's constant . A numerical approximation of the identity: $$4.7994017718717349316710651835631714860755433336848275119882157\dots$$","I think the following identity is true. How could we prove it? $${_4F_3}\left(\begin{array}c 1,1,1,1 \\\tfrac54,2,2\end{array}\middle|\,1\right) + 3\,{_4F_3}\left(\begin{array}c\tfrac12,\tfrac12,1,1\\\tfrac32,\tfrac32,\tfrac32\end{array}\middle|\,1\right) = \frac{\pi^3}{8} + \frac{ 3\pi^2}{4}\ln 2 - \frac 72 \zeta (3).$$ Here ${_pF_q}$ is the generalized hypergeometric function and $\zeta(3)$ is Apéry's constant . A numerical approximation of the identity: $$4.7994017718717349316710651835631714860755433336848275119882157\dots$$",,"['calculus', 'special-functions', 'closed-form', 'hypergeometric-function']"
78,A proviso in l'Hospital's rule,A proviso in l'Hospital's rule,,"L'Hospital's Rule states that $$\lim_{x\to a}\frac{f(x)}{g(x)} = \lim_{x\to a}\frac{f'(x)}{g'(x)}$$ can be applied when: (1) $f$, $g$ are differentiable; (2) $g'(x) \neq 0$ for $x$ near $a$ (except possibly at $a$); (3) $\displaystyle\lim_{x\to a}f(x) = 0 = \displaystyle\lim_{x\to a}g(x)$, or $\displaystyle\lim_{x\to a}f(x) = \pm\infty = \displaystyle\lim_{x\to a}g(x)$; and (4) the limit on the RHS exists or it equals $\pm\infty$. $\ $ Why is proviso (2) necessary? Edit: This question was asked in the context of a first undergraduate calculus course. Thus, the domains of $f$ and $g$ can be assumed to include a subset of $\mathbb R$ for which $a$ is an accumulation point, that has at most two connected components. I apologise to Paramanand if this changes the question - I intended to communicate this by tagging the question calculus . I think that this setup means that the quotient $f'/g'$ will be defined on an interval near $a$ when the criteria (1)-(4) hold.","L'Hospital's Rule states that $$\lim_{x\to a}\frac{f(x)}{g(x)} = \lim_{x\to a}\frac{f'(x)}{g'(x)}$$ can be applied when: (1) $f$, $g$ are differentiable; (2) $g'(x) \neq 0$ for $x$ near $a$ (except possibly at $a$); (3) $\displaystyle\lim_{x\to a}f(x) = 0 = \displaystyle\lim_{x\to a}g(x)$, or $\displaystyle\lim_{x\to a}f(x) = \pm\infty = \displaystyle\lim_{x\to a}g(x)$; and (4) the limit on the RHS exists or it equals $\pm\infty$. $\ $ Why is proviso (2) necessary? Edit: This question was asked in the context of a first undergraduate calculus course. Thus, the domains of $f$ and $g$ can be assumed to include a subset of $\mathbb R$ for which $a$ is an accumulation point, that has at most two connected components. I apologise to Paramanand if this changes the question - I intended to communicate this by tagging the question calculus . I think that this setup means that the quotient $f'/g'$ will be defined on an interval near $a$ when the criteria (1)-(4) hold.",,"['calculus', 'limits']"
79,Speeding up the convergence of a series,Speeding up the convergence of a series,,I want to speed up the convergence of a series involving rational expressions the expression is $$\sum _{x=1}^{\infty }\left( -1\right) ^{x}\dfrac {-x^{2}-2x+1} {x^{4}+2x^{2}+1}$$ If I have not misunderstood anything the error in the infinite sum is at most the absolute value of the last neglected term. The formula for the $n$th term is $\dfrac {-x^{2}-2x+1} {x^{4}+2x^{2}+1}$ from the definition of the series. To get the series I used Maxima the computer algebra system. I have noticed that to get 13 decimal places of the series one must wade through $312958$ terms of the series. I had to kill the computer GUI and some other system processes and run Maxima to compute the sum. I took about 5 minutes. The final sum I obtained was $0.3106137076850$. Is there any way to speed up the convergence of the sum? In general is there any way to speed up the convergence of the sum of $$\sum _{x=1}^{\infty }\left( -1\right) ^{x}\dfrac {p(x)} {q(x)}$$ where both ${p(x)}$ and ${q(x)}$ are rational functions?,I want to speed up the convergence of a series involving rational expressions the expression is $$\sum _{x=1}^{\infty }\left( -1\right) ^{x}\dfrac {-x^{2}-2x+1} {x^{4}+2x^{2}+1}$$ If I have not misunderstood anything the error in the infinite sum is at most the absolute value of the last neglected term. The formula for the $n$th term is $\dfrac {-x^{2}-2x+1} {x^{4}+2x^{2}+1}$ from the definition of the series. To get the series I used Maxima the computer algebra system. I have noticed that to get 13 decimal places of the series one must wade through $312958$ terms of the series. I had to kill the computer GUI and some other system processes and run Maxima to compute the sum. I took about 5 minutes. The final sum I obtained was $0.3106137076850$. Is there any way to speed up the convergence of the sum? In general is there any way to speed up the convergence of the sum of $$\sum _{x=1}^{\infty }\left( -1\right) ^{x}\dfrac {p(x)} {q(x)}$$ where both ${p(x)}$ and ${q(x)}$ are rational functions?,,"['calculus', 'sequences-and-series', 'maxima-software']"
80,"What is meant by ""global smooth coordinates""?","What is meant by ""global smooth coordinates""?",,"On p. 65 of John M. Lee's book Introduction to Smooth Manifolds , we find the following as part of Exercise 3.17: Verify that $(\tilde{x}, \tilde{y})$ are smooth coordinates on $\mathbb{R}^2$ , where $$\tilde{x}=x; \;\;\tilde{y} = x^3+y.$$ What is meant by showing that these are global smooth coordinates? Are we just showing that $(x,y) \mapsto (x,x^3+y):\mathbb{R}^2\to \mathbb{R}^2$ is a diffeomorphism? Thanks folks","On p. 65 of John M. Lee's book Introduction to Smooth Manifolds , we find the following as part of Exercise 3.17: Verify that are smooth coordinates on , where What is meant by showing that these are global smooth coordinates? Are we just showing that is a diffeomorphism? Thanks folks","(\tilde{x}, \tilde{y}) \mathbb{R}^2 \tilde{x}=x; \;\;\tilde{y} = x^3+y. (x,y) \mapsto (x,x^3+y):\mathbb{R}^2\to \mathbb{R}^2","['calculus', 'differential-geometry', 'differential-topology']"
81,Compute the integral $\int\frac{dx}{(x^2-x+1)\sqrt{x^2+x+1}}$,Compute the integral,\int\frac{dx}{(x^2-x+1)\sqrt{x^2+x+1}},"Compute the indefinite integral   $$ \int\frac{dx}{(x^2-x+1)\sqrt{x^2+x+1}} $$ My Attempt: $$ \int\frac{dx}{(x^2-x+1)\sqrt{x^2+x+1}}=\int\frac{1}{(x^2-x+1)^{3/2}.\sqrt{\dfrac{x^2+x+1}{x^2-x+1}}}\,dx $$ Now define $t$ such that $t^2=\dfrac{x^2+x+1}{x^2-x+1}$ to get $$ \begin{align} 2t\,dt &= \frac{(x^2-x+1)(2x+1)-(x^2+x+1)\cdot (2x-1)}{(x^2-x+1)^2}\,dx\\ 2tdt &= \frac{-4x^2+2x+2}{(x^2-x+1)^2}dx \end{align} $$ I don't know how to proceed from here.","Compute the indefinite integral   $$ \int\frac{dx}{(x^2-x+1)\sqrt{x^2+x+1}} $$ My Attempt: $$ \int\frac{dx}{(x^2-x+1)\sqrt{x^2+x+1}}=\int\frac{1}{(x^2-x+1)^{3/2}.\sqrt{\dfrac{x^2+x+1}{x^2-x+1}}}\,dx $$ Now define $t$ such that $t^2=\dfrac{x^2+x+1}{x^2-x+1}$ to get $$ \begin{align} 2t\,dt &= \frac{(x^2-x+1)(2x+1)-(x^2+x+1)\cdot (2x-1)}{(x^2-x+1)^2}\,dx\\ 2tdt &= \frac{-4x^2+2x+2}{(x^2-x+1)^2}dx \end{align} $$ I don't know how to proceed from here.",,"['calculus', 'integration', 'indefinite-integrals']"
82,How prove this $\sum_{n=1}^{\infty}\frac{\zeta_{2}}{n^4}=\zeta^2(3)-\frac{1}{3}\zeta(6)$,How prove this,\sum_{n=1}^{\infty}\frac{\zeta_{2}}{n^4}=\zeta^2(3)-\frac{1}{3}\zeta(6),"show that $$\sum_{n=1}^{\infty}\dfrac{\zeta_{2}}{n^4}=\zeta^2(3)-\dfrac{1}{3}\zeta(6)$$ where $$\zeta_{m}=\sum_{k=1}^{n}\dfrac{1}{k^m},\zeta(m)=\sum_{k=1}^{\infty}\dfrac{1}{k^m}$$ is true？ because This result is my frend tell me. This problem have someone research it?Thank you my some idea: $$\zeta^3(3)=\left(\sum_{n=0}^{\infty}\dfrac{1}{(n+1)^3}\right)^2=\sum_{n=0}^{\infty}\sum_{k=0}^{n}\dfrac{1}{(k+1)^3(n-k+1)^3}$$ and use $$\dfrac{1}{(k+1)(n-k+1)}=\dfrac{1}{n+2}\left(\dfrac{1}{k+1}+\dfrac{1}{n-k+1}\right)$$ and $$(a+b)^3=a^3+3a^2b+3ab^2+b^3$$ and $$\sum_{n=1}^{\infty}\dfrac{H_{n}}{(n+1)^5}=\dfrac{1}{2}\left(5\zeta(6)-2\zeta(2)\zeta(4)-\zeta^2(3)\right)$$ But is very ugly, someone  have other nice methods? Thank you .","show that $$\sum_{n=1}^{\infty}\dfrac{\zeta_{2}}{n^4}=\zeta^2(3)-\dfrac{1}{3}\zeta(6)$$ where $$\zeta_{m}=\sum_{k=1}^{n}\dfrac{1}{k^m},\zeta(m)=\sum_{k=1}^{\infty}\dfrac{1}{k^m}$$ is true？ because This result is my frend tell me. This problem have someone research it?Thank you my some idea: $$\zeta^3(3)=\left(\sum_{n=0}^{\infty}\dfrac{1}{(n+1)^3}\right)^2=\sum_{n=0}^{\infty}\sum_{k=0}^{n}\dfrac{1}{(k+1)^3(n-k+1)^3}$$ and use $$\dfrac{1}{(k+1)(n-k+1)}=\dfrac{1}{n+2}\left(\dfrac{1}{k+1}+\dfrac{1}{n-k+1}\right)$$ and $$(a+b)^3=a^3+3a^2b+3ab^2+b^3$$ and $$\sum_{n=1}^{\infty}\dfrac{H_{n}}{(n+1)^5}=\dfrac{1}{2}\left(5\zeta(6)-2\zeta(2)\zeta(4)-\zeta^2(3)\right)$$ But is very ugly, someone  have other nice methods? Thank you .",,"['calculus', 'sequences-and-series']"
83,Solving for unknown functions,Solving for unknown functions,,"I am not a mathematician, so excuse if my question is silly or badly stated. I have the following problem. I have 2 conditions on two unknown continuously differentiable functions: $$A(t)=\frac{1}{B(t)}+C \\ B(t)=D-A(t)-\int_0^t A(\tau) d\tau.$$ C and D are constants. I also know $A(0)$ and $B(0)$. I am looking for a way to get the value of $A(t)$ and $B(t)$ for small $t>0$. So far I have a numerical solution, but that involves a lot of interpolation and I don't think it is very good. I was wondering if there is some way to get an analytic solution for this problem. I don't expect you to solve the problem for me, I'm willing to learn and I'd be very grateful if you could point me towards possible readings. Thanks in advance.","I am not a mathematician, so excuse if my question is silly or badly stated. I have the following problem. I have 2 conditions on two unknown continuously differentiable functions: $$A(t)=\frac{1}{B(t)}+C \\ B(t)=D-A(t)-\int_0^t A(\tau) d\tau.$$ C and D are constants. I also know $A(0)$ and $B(0)$. I am looking for a way to get the value of $A(t)$ and $B(t)$ for small $t>0$. So far I have a numerical solution, but that involves a lot of interpolation and I don't think it is very good. I was wondering if there is some way to get an analytic solution for this problem. I don't expect you to solve the problem for me, I'm willing to learn and I'd be very grateful if you could point me towards possible readings. Thanks in advance.",,"['calculus', 'systems-of-equations', 'integral-equations']"
84,Help understanding proof of Schwarz Inequality,Help understanding proof of Schwarz Inequality,,"I'm working through Spivak's Calculus over the summer, and I'm currently on problem 19 of Chapter 1, which involves proving the Schwarz inequality. The first two parts of the proof are fairly straightforward, but I don't understand the last part. Spivak gives the inequality: $0<\lambda^2(y_1^2+y_2^2)-2\lambda(x_1y_1+x_2y_2)+(x_1^2+x_2^2)$ and suggests using the quadratic formula (I'm not sure I understand this part, since the equation is greater than zero, how can there be any solutions?) to arrive at the Schwarz inequality. In the answer key, the following is given. $\displaystyle\left[\frac{2(x_1y_1+x_2y_2)}{(y_1^2+y_2^2)}\right]^2-\frac{4(x_1^2+y_1^2)}{(y_1^2+y_2^2)}<0$ I'm not really sure how this follows since I can't get it just by trying to find the roots of the equations vis a vis the quadratic formula.","I'm working through Spivak's Calculus over the summer, and I'm currently on problem 19 of Chapter 1, which involves proving the Schwarz inequality. The first two parts of the proof are fairly straightforward, but I don't understand the last part. Spivak gives the inequality: $0<\lambda^2(y_1^2+y_2^2)-2\lambda(x_1y_1+x_2y_2)+(x_1^2+x_2^2)$ and suggests using the quadratic formula (I'm not sure I understand this part, since the equation is greater than zero, how can there be any solutions?) to arrive at the Schwarz inequality. In the answer key, the following is given. $\displaystyle\left[\frac{2(x_1y_1+x_2y_2)}{(y_1^2+y_2^2)}\right]^2-\frac{4(x_1^2+y_1^2)}{(y_1^2+y_2^2)}<0$ I'm not really sure how this follows since I can't get it just by trying to find the roots of the equations vis a vis the quadratic formula.",,['calculus']
85,Why does the sum of these trigonometric expressions give such a simple result?,Why does the sum of these trigonometric expressions give such a simple result?,,"During calculations I came across the following identity: $$M+2(1-m) = \sum_{l=1}^{M-1} \frac{\cos\left(2\pi\frac{(2-m)\,l}{M}\right) - \cos\left(2\pi\frac{m\,l}{M}\right)}{2(1-\cos\left(2\pi\frac{l}{M}\right)}, \quad \forall m\in \{2,\dots, M\}, M\in \mathbb{N}$$ I cannot see why this rather complicated sum should give such a simple expression. Does anyone know trig-tricks to simplify the sum? Can one already see intuitively that the result is linear in $m$ ?","During calculations I came across the following identity: $$M+2(1-m) = \sum_{l=1}^{M-1} \frac{\cos\left(2\pi\frac{(2-m)\,l}{M}\right) - \cos\left(2\pi\frac{m\,l}{M}\right)}{2(1-\cos\left(2\pi\frac{l}{M}\right)}, \quad \forall m\in \{2,\dots, M\}, M\in \mathbb{N}$$ I cannot see why this rather complicated sum should give such a simple expression. Does anyone know trig-tricks to simplify the sum? Can one already see intuitively that the result is linear in $m$ ?",,"['calculus', 'trigonometry']"
86,"Limit of $1/x^2$ - Apostol 3.2, Example 4","Limit of  - Apostol 3.2, Example 4",1/x^2,"In Apostol, One Variable Calculus Volume 1, section 3.2, page 130, he gives the following example (roughly paraphrased): Let $f(x) = \frac{1}{x^2}$ if $x \neq 0$ , and let $f(0) = 0$ . To prove rigorously that there is no real number $A$ such that $\lim_{x\to0^+} f(x)=A$ , we may argue as follows: Suppose there were such an $A$ , say $A>0$ . Choose a neighborhood $N(A)$ of length $1$ . In the interval $0 < x < \frac{1}{A + 2}$ , we have $f(x) = \frac{1}{x^2} > (A + 2)^2 > (A + 2)$ , so $f(x)$ cannot lie in the neighborhood $N(A)$ . Thus, every neighborhood $N(0)$ contains points $x > 0$ for which $f(x)$ is outside $N(A)$ , so (3.3) is violated for this choice of $N(A)$ . Hence $f$ has no right-hand limit at $0$ . While I intuitively understand why the function has no limit, I'm completely lost on his proof. What justifies him from moving from $f(x)$ lies outside of $N(A)$ for the neighborhood $0 < x < \frac{1}{A+2}$ to $f(x)$ lies outside every neighborhood? The way I've been thinking about it is to translate the proof into terms from the $ε-δ$ definition. Thus, when he says ""in the interval $0 < x < \frac{1}{A+2}$ "", he's setting $ε = \frac{1}{A+2}$ , and then showing that $f(x)$ lies outside of $|f(x) - A| < ε$ for $|x-0| < δ$ . But, if we were to prove that there is no limit, we have to show that for some $ε$ , no $δ$ works. He says this in, ""Thus, every neighborhood $N(0)$ contains points..."", but I don't see how he can move from ""this $δ$ doesn't work"" to ""no $δ$ works"". The best I can come up with is that either I'm making a mistake in thinking he's choosing $δ = \frac{1}{A+2}$ , or that particular $δ$ is supposed to be a catch all. That is, if any $δ$ will work, this one should. But if that is the case, I don't see why this $δ$ has to be the one that works. Thanks in advance.","In Apostol, One Variable Calculus Volume 1, section 3.2, page 130, he gives the following example (roughly paraphrased): Let if , and let . To prove rigorously that there is no real number such that , we may argue as follows: Suppose there were such an , say . Choose a neighborhood of length . In the interval , we have , so cannot lie in the neighborhood . Thus, every neighborhood contains points for which is outside , so (3.3) is violated for this choice of . Hence has no right-hand limit at . While I intuitively understand why the function has no limit, I'm completely lost on his proof. What justifies him from moving from lies outside of for the neighborhood to lies outside every neighborhood? The way I've been thinking about it is to translate the proof into terms from the definition. Thus, when he says ""in the interval "", he's setting , and then showing that lies outside of for . But, if we were to prove that there is no limit, we have to show that for some , no works. He says this in, ""Thus, every neighborhood contains points..."", but I don't see how he can move from ""this doesn't work"" to ""no works"". The best I can come up with is that either I'm making a mistake in thinking he's choosing , or that particular is supposed to be a catch all. That is, if any will work, this one should. But if that is the case, I don't see why this has to be the one that works. Thanks in advance.",f(x) = \frac{1}{x^2} x \neq 0 f(0) = 0 A \lim_{x\to0^+} f(x)=A A A>0 N(A) 1 0 < x < \frac{1}{A + 2} f(x) = \frac{1}{x^2} > (A + 2)^2 > (A + 2) f(x) N(A) N(0) x > 0 f(x) N(A) N(A) f 0 f(x) N(A) 0 < x < \frac{1}{A+2} f(x) ε-δ 0 < x < \frac{1}{A+2} ε = \frac{1}{A+2} f(x) |f(x) - A| < ε |x-0| < δ ε δ N(0) δ δ δ = \frac{1}{A+2} δ δ δ,"['calculus', 'limits']"
87,Is the use of $\lfloor x\rfloor$ legitimate to correct discontinuities?,Is the use of  legitimate to correct discontinuities?,\lfloor x\rfloor,"Is the use of $\lfloor x\rfloor$ legitimate to correct discontinuities? In functions like $\tan^{-1}(a \tan(x))$, the angle wraps and the result is discontinuous. Is it legitimate to redefine the equation as in $\tan(a \tan^{-1}(x)) + \pi\lfloor \pi x + \frac{1}{2} \rfloor \mathop{\rm sgn}(a)$ to keep it continuous?  Or is it better to write it $\tan(a \tan^{-1}(x)) + (x - \tan^{-1}(\tan(x))) \mathop{\rm sgn}(a)$, or some other way–or should this never be corrected in the first place? I realize this example is rather trivial due to the multivalued nature of the arctangent.  Here is one that is not so trivial: the arc length of the cycloid. Unless I'm missing something, the problem with simply calculating integral for the arc length $\int \sqrt{2 - 2\cos(t)} dt = 2 \int |\sin(\frac{t}{2})| dt = 4-4\cos(\frac{t}{2})\mathop{\rm sgn}(\sin(\frac{t}{2}))$ is that it jumps back down to zero every $2\pi$.  This could be corrected in one of the above ways.  If left alone it is simply incorrect except in $0 < t < 2\pi$. Now Wolfram|Alpha gives a terribly convoluted function for it.  How did W|A redefine the process to come out correct for all $t$ (not to mention so convoluted)?  Is this more natural or otherwise more legitimate than simply adding a floor function to it? In other words: is there a mathematical reason to prefer one method over another?","Is the use of $\lfloor x\rfloor$ legitimate to correct discontinuities? In functions like $\tan^{-1}(a \tan(x))$, the angle wraps and the result is discontinuous. Is it legitimate to redefine the equation as in $\tan(a \tan^{-1}(x)) + \pi\lfloor \pi x + \frac{1}{2} \rfloor \mathop{\rm sgn}(a)$ to keep it continuous?  Or is it better to write it $\tan(a \tan^{-1}(x)) + (x - \tan^{-1}(\tan(x))) \mathop{\rm sgn}(a)$, or some other way–or should this never be corrected in the first place? I realize this example is rather trivial due to the multivalued nature of the arctangent.  Here is one that is not so trivial: the arc length of the cycloid. Unless I'm missing something, the problem with simply calculating integral for the arc length $\int \sqrt{2 - 2\cos(t)} dt = 2 \int |\sin(\frac{t}{2})| dt = 4-4\cos(\frac{t}{2})\mathop{\rm sgn}(\sin(\frac{t}{2}))$ is that it jumps back down to zero every $2\pi$.  This could be corrected in one of the above ways.  If left alone it is simply incorrect except in $0 < t < 2\pi$. Now Wolfram|Alpha gives a terribly convoluted function for it.  How did W|A redefine the process to come out correct for all $t$ (not to mention so convoluted)?  Is this more natural or otherwise more legitimate than simply adding a floor function to it? In other words: is there a mathematical reason to prefer one method over another?",,"['calculus', 'trigonometry', 'functions']"
88,Is there a gamma-like function for the q-factorial?,Is there a gamma-like function for the q-factorial?,,"I'm looking at quantum calculus and just trying to understand what is going with this subject. Looking at the q-factorial made me wonder if this function could take all real or even complex numbers in the same way that $\Gamma (z)$ works as an extension of $f(n) =n!$. Since, I need practice with both $\Gamma $ and q-analogs, would it be a good project to try to recreate $\Gamma (z)$ in this new setting or is the whole project lacking in sanity, mathematical soundness? Also, a minor question, why is there sometimes a coefficient in q-analog expansions as in this expression: $(a;q)_n = \prod_{k=0}^{n-1} (1-aq^k)=(1-a)(1-aq)(1-aq^2)\cdots(1-aq^{n-1}).$ I'm a bit embarrassed not to know, but non of the lit. I have explains it, I'll just randomly see it tossed in there from time to time... and it really throws me off. Thank you.","I'm looking at quantum calculus and just trying to understand what is going with this subject. Looking at the q-factorial made me wonder if this function could take all real or even complex numbers in the same way that $\Gamma (z)$ works as an extension of $f(n) =n!$. Since, I need practice with both $\Gamma $ and q-analogs, would it be a good project to try to recreate $\Gamma (z)$ in this new setting or is the whole project lacking in sanity, mathematical soundness? Also, a minor question, why is there sometimes a coefficient in q-analog expansions as in this expression: $(a;q)_n = \prod_{k=0}^{n-1} (1-aq^k)=(1-a)(1-aq)(1-aq^2)\cdots(1-aq^{n-1}).$ I'm a bit embarrassed not to know, but non of the lit. I have explains it, I'll just randomly see it tossed in there from time to time... and it really throws me off. Thank you.",,"['calculus', 'special-functions', 'gamma-function', 'q-analogs', 'quantum-calculus']"
89,Determining the limit of the sequence $a_{n+1}=-ta_n^2+(t+1)a_n$,Determining the limit of the sequence,a_{n+1}=-ta_n^2+(t+1)a_n,"Let $t\in(0,1]$ , $a_0\in (0,\frac{1}{t}+1)$ and $a_{n+1}=-ta_n^2+(t+1)a_n$ . Does $\lim\limits_{n\to\infty}a_n$ exist for all $\,t\in(0,1]\,$ and $\,a_0\in\big(0,\frac{1}{t}+1\big)\,$ ? If yes, determine the limit. I already showed that $a_n$ is bounded and I think the limit exists and is one. Now I have problems to show that $a_n$ is monotone increasing/decreasing, because it is either increasing or decreasing or nothing (for $t=0.8$ and $a_0=2.24$ ) it is not decreasing and not increasing. I also tried the Banach-fixed-point theorem but I think I can't use it here.","Let , and . Does exist for all and ? If yes, determine the limit. I already showed that is bounded and I think the limit exists and is one. Now I have problems to show that is monotone increasing/decreasing, because it is either increasing or decreasing or nothing (for and ) it is not decreasing and not increasing. I also tried the Banach-fixed-point theorem but I think I can't use it here.","t\in(0,1] a_0\in (0,\frac{1}{t}+1) a_{n+1}=-ta_n^2+(t+1)a_n \lim\limits_{n\to\infty}a_n \,t\in(0,1]\, \,a_0\in\big(0,\frac{1}{t}+1\big)\, a_n a_n t=0.8 a_0=2.24","['calculus', 'sequences-and-series', 'convergence-divergence', 'recurrence-relations']"
90,Find $\lim_{n \to \infty}a_n$ where $a_1=1$ and $a_{n+1}=a_n+\frac{1}{2^na_n}$.,Find  where  and .,\lim_{n \to \infty}a_n a_1=1 a_{n+1}=a_n+\frac{1}{2^na_n},"There are some attempts as follows. Obviously, $\{a_n\}$ is increasing. Thus $a_n\ge a_1=1$ . Therefore $$a_{n+1}-a_n=\frac{1}{2^na_n}\le \frac{1}{2^n},$$ which gives that $$a_n=a_1+\sum_{k=1}^{n-1}(a_{k+1}-a_k)\le a_1+\sum_{k=1}^{n-1}\frac{1}{2^k}=2-\frac{1}{2^{n-1}}.$$ Hence, $a_n\le 2$ . Similarily, $$a_{n+1}-a_n=\frac{1}{2^na_n}\ge \frac{1}{2^{n+1}},$$ which gives that $$a_n=a_1+\sum_{k=1}^{n-1}(a_{k+1}-a_k)\ge a_1+\sum_{k=1}^{n-1}\frac{1}{2^{k+1}}=\frac{3}{2}-\frac{1}{2^{n}}.$$ Put the both aspects together, we have $$\frac{3}{2}-\frac{1}{2^n}\le a_n\le 2-\frac{1}{2^{n-1}}.$$ But this does not satisfy the applying conditions of the squeeze theorem. Any other solutions?","There are some attempts as follows. Obviously, is increasing. Thus . Therefore which gives that Hence, . Similarily, which gives that Put the both aspects together, we have But this does not satisfy the applying conditions of the squeeze theorem. Any other solutions?","\{a_n\} a_n\ge a_1=1 a_{n+1}-a_n=\frac{1}{2^na_n}\le \frac{1}{2^n}, a_n=a_1+\sum_{k=1}^{n-1}(a_{k+1}-a_k)\le a_1+\sum_{k=1}^{n-1}\frac{1}{2^k}=2-\frac{1}{2^{n-1}}. a_n\le 2 a_{n+1}-a_n=\frac{1}{2^na_n}\ge \frac{1}{2^{n+1}}, a_n=a_1+\sum_{k=1}^{n-1}(a_{k+1}-a_k)\ge a_1+\sum_{k=1}^{n-1}\frac{1}{2^{k+1}}=\frac{3}{2}-\frac{1}{2^{n}}. \frac{3}{2}-\frac{1}{2^n}\le a_n\le 2-\frac{1}{2^{n-1}}.","['calculus', 'sequences-and-series', 'limits']"
91,Solving $\lim_{x\to0}\frac{\cos\left(\frac{\pi}{2\cos(x)}\right)}{\sin(\sin(x^2))}$,Solving,\lim_{x\to0}\frac{\cos\left(\frac{\pi}{2\cos(x)}\right)}{\sin(\sin(x^2))},"I've been asked to solve the limit. $$\lim_{x\to0}\frac{\cos\left(\frac{\pi}{2\cos(x)}\right)}{\sin(\sin(x^2))}$$ Here's my approach: $$\lim_{x\to0}\frac{\cos\left(\frac{\pi}{2\cos(x)}\right)}{\sin(\sin(x^2))}$$ Using the identity, $\cos(x) =\sin(90^{\circ} - x)$ \begin{aligned}\implies \lim_{x\to0}\frac{\cos\left(\frac{\pi}{2\cos(x)}\right)}{\sin(\sin(x^2))}  & = \lim_{x\to0}\frac{\sin\left(\frac{\pi}{2} - \frac{\pi}{2\cos(x)}\right)}{\sin(\sin(x^2))} \\& = \lim_{x\to0}\dfrac{\left(\dfrac{\pi}{2} - \dfrac{\pi}{2\cos(x)}\right)\cdot\dfrac{\sin\left(\frac{\pi}{2} - \frac{\pi}{2\cos(x)}\right)}{\left(\frac{\pi}{2} - \frac{\pi}{2\cos(x)}\right)}}{\sin(\sin(x^2))} \\ & = \lim_{x\to0}\dfrac{\left(\dfrac{\pi}{2} - \dfrac{\pi}{2\cos(x)}\right)}{\sin(\sin(x^2))}\cdot \underbrace{\lim_{x\to0}\dfrac{\sin\left(\frac{\pi}{2} - \frac{\pi}{2\cos(x)}\right)}{\left(\frac{\pi}{2} - \frac{\pi}{2\cos(x)}\right)}}_{1} \\ & = \dfrac{\lim\limits_{x\to0}\dfrac{\pi}{2}\left(\dfrac{\cos(x) - 1}{\cos(x)}\right)}{\underbrace{\lim\limits_{x\to0}\dfrac{\sin(\sin(x^2))}{\sin(x^2)}}_1\cdot\sin(x^2)} \\ & =\dfrac{\lim\limits_{x\to0}\dfrac{\pi}{2}\left(\dfrac{\cos(x) - 1}{\cos(x)}\right)}{\underbrace{\lim\limits_{x\to0}\dfrac{\sin(x^2)}{x^2}}_1\cdot x^2} \\ & =  \color{blue}{\boxed{\lim\limits_{x\to0}\dfrac{\pi}{2x^2}\left(\dfrac{\cos(x) - 1}{\cos(x)}\right)}} \end{aligned} Now, I'm unable to think of anything to do with this boxed part. Can anyone check my above method and tell me what to do further with this question? Any other shorter method is also most welcomed!","I've been asked to solve the limit. Here's my approach: Using the identity, Now, I'm unable to think of anything to do with this boxed part. Can anyone check my above method and tell me what to do further with this question? Any other shorter method is also most welcomed!","\lim_{x\to0}\frac{\cos\left(\frac{\pi}{2\cos(x)}\right)}{\sin(\sin(x^2))} \lim_{x\to0}\frac{\cos\left(\frac{\pi}{2\cos(x)}\right)}{\sin(\sin(x^2))} \cos(x) =\sin(90^{\circ} - x) \begin{aligned}\implies \lim_{x\to0}\frac{\cos\left(\frac{\pi}{2\cos(x)}\right)}{\sin(\sin(x^2))} 
& = \lim_{x\to0}\frac{\sin\left(\frac{\pi}{2} - \frac{\pi}{2\cos(x)}\right)}{\sin(\sin(x^2))}
\\& = \lim_{x\to0}\dfrac{\left(\dfrac{\pi}{2} - \dfrac{\pi}{2\cos(x)}\right)\cdot\dfrac{\sin\left(\frac{\pi}{2} - \frac{\pi}{2\cos(x)}\right)}{\left(\frac{\pi}{2} - \frac{\pi}{2\cos(x)}\right)}}{\sin(\sin(x^2))}
\\ & = \lim_{x\to0}\dfrac{\left(\dfrac{\pi}{2} - \dfrac{\pi}{2\cos(x)}\right)}{\sin(\sin(x^2))}\cdot \underbrace{\lim_{x\to0}\dfrac{\sin\left(\frac{\pi}{2} - \frac{\pi}{2\cos(x)}\right)}{\left(\frac{\pi}{2} - \frac{\pi}{2\cos(x)}\right)}}_{1}
\\ & = \dfrac{\lim\limits_{x\to0}\dfrac{\pi}{2}\left(\dfrac{\cos(x) - 1}{\cos(x)}\right)}{\underbrace{\lim\limits_{x\to0}\dfrac{\sin(\sin(x^2))}{\sin(x^2)}}_1\cdot\sin(x^2)}
\\ & =\dfrac{\lim\limits_{x\to0}\dfrac{\pi}{2}\left(\dfrac{\cos(x) - 1}{\cos(x)}\right)}{\underbrace{\lim\limits_{x\to0}\dfrac{\sin(x^2)}{x^2}}_1\cdot x^2}
\\ & =  \color{blue}{\boxed{\lim\limits_{x\to0}\dfrac{\pi}{2x^2}\left(\dfrac{\cos(x) - 1}{\cos(x)}\right)}}
\end{aligned}",['calculus']
92,Probability all angles of triangle formed within semircircle less than $120^\circ$,Probability all angles of triangle formed within semircircle less than,120^\circ,"$3$ points $A$ , $B$ , $C$ are randomly chosen on the circumference of a circle. If $A$ , $B$ , $C$ all lie on a semicircle, then what is the probability that all of the angles of triangle $ABC$ are less than $120^\circ$ ? Okay, let's fix a semicircle and make an interval $[0, 1]$ along it. It's clear the condition that the condition that all angles are less than $120^\circ$ means that the two points of the triangle furthest apart on the semicircle are further than $2/3$ apart. So we want to calculate the percentage of the volume of the unit cube already satisfying the following: $$0 < x < y < z < 1$$ subject to the additional inequality $$z - x > {2\over3}$$ However, I'm not sure what to do from here. Any help would be well-appreciated. UPDATE: With the helpful hint of Daniel Mathias in the comments, I was able to get the answer of ${5\over9}$ . But I am wondering if there is a way to solve it in the more complicated way I proposed, where there's $3$ variables and maybe we can do some multivariable integration.","points , , are randomly chosen on the circumference of a circle. If , , all lie on a semicircle, then what is the probability that all of the angles of triangle are less than ? Okay, let's fix a semicircle and make an interval along it. It's clear the condition that the condition that all angles are less than means that the two points of the triangle furthest apart on the semicircle are further than apart. So we want to calculate the percentage of the volume of the unit cube already satisfying the following: subject to the additional inequality However, I'm not sure what to do from here. Any help would be well-appreciated. UPDATE: With the helpful hint of Daniel Mathias in the comments, I was able to get the answer of . But I am wondering if there is a way to solve it in the more complicated way I proposed, where there's variables and maybe we can do some multivariable integration.","3 A B C A B C ABC 120^\circ [0, 1] 120^\circ 2/3 0 < x < y < z < 1 z - x > {2\over3} {5\over9} 3","['calculus', 'probability', 'integration', 'multivariable-calculus', 'geometric-probability']"
93,How to bound the error for the Taylor expansion of the inverse of a mean of exponentials?,How to bound the error for the Taylor expansion of the inverse of a mean of exponentials?,,"If $|x| \leq R / 10$ for some $R\in \mathbb{N}$ , then it is easily shown that $$\left|e^{-x} - \sum_{k=0}^R \frac{(-1)^k x^k}{k!}\right| \leq e^{-R}.$$ I would like to have a similar result (i.e. with an error bound of the same form) for the multivariate function $\frac{1}{\frac{1}{M} \sum_{i=1}^M e^{x_i}}$ , where the necessary control on $M\in \mathbb{N}$ and on each $x_i$ 's can be imposed. Any ideas ? After playing with formal Taylor series, here's a partial answer. We can write $$\sum_{i=1}^M e^{x_i} = \sum_{\alpha\in \mathbb{N}_0^M} c_{\alpha} x^{\alpha}, \quad \text{with } c_{\alpha} = \begin{cases} \frac{1}{k!}, &\mbox{if } \alpha = k e_k, \\ 0, &\mbox{otherwise}, \end{cases}$$ where $e_k$ is the $k$ -th standard basis vector, and $x^{\alpha} = \prod_{i=1}^M x_i^{\alpha_i}$ . Now the goal is to find coefficients $d_{\beta}$ such that $$\left(\sum_{\alpha\in \mathbb{N}_0^M} c_{\alpha} x^{\alpha}\right) \cdot \left(\sum_{\beta\in \mathbb{N}_0^M} d_{\beta} x^{\beta}\right) = \sum_{\gamma\in \mathbb{N}_0^M} x^{\gamma} \left(\sum_{\alpha + \beta = \gamma} c_{\alpha} d_{\beta}\right) = 1.$$","If for some , then it is easily shown that I would like to have a similar result (i.e. with an error bound of the same form) for the multivariate function , where the necessary control on and on each 's can be imposed. Any ideas ? After playing with formal Taylor series, here's a partial answer. We can write where is the -th standard basis vector, and . Now the goal is to find coefficients such that","|x| \leq R / 10 R\in \mathbb{N} \left|e^{-x} - \sum_{k=0}^R \frac{(-1)^k x^k}{k!}\right| \leq e^{-R}. \frac{1}{\frac{1}{M} \sum_{i=1}^M e^{x_i}} M\in \mathbb{N} x_i \sum_{i=1}^M e^{x_i} = \sum_{\alpha\in \mathbb{N}_0^M} c_{\alpha} x^{\alpha}, \quad \text{with } c_{\alpha} =
\begin{cases}
\frac{1}{k!}, &\mbox{if } \alpha = k e_k, \\
0, &\mbox{otherwise},
\end{cases} e_k k x^{\alpha} = \prod_{i=1}^M x_i^{\alpha_i} d_{\beta} \left(\sum_{\alpha\in \mathbb{N}_0^M} c_{\alpha} x^{\alpha}\right) \cdot \left(\sum_{\beta\in \mathbb{N}_0^M} d_{\beta} x^{\beta}\right) = \sum_{\gamma\in \mathbb{N}_0^M} x^{\gamma} \left(\sum_{\alpha + \beta = \gamma} c_{\alpha} d_{\beta}\right) = 1.","['calculus', 'taylor-expansion']"
94,Derivative of $\operatorname{arctan2}$,Derivative of,\operatorname{arctan2},"I'm currently working on some navigation equations and I would like to write down the derivative with respect to $x$ of something like $$f(x) = \operatorname{arctan2}(c(x), d(x))$$ I've searched wherever I've could and the only thing I've come across are the partial derivatives of $\operatorname{arctan2}(y,x)$ with respect to $x$ and $y$ . To be more specific, my equation looks like this: $$\psi = \operatorname{arctan2} \left ( -m_y \cos(\phi + \delta \phi) + m_z \sin(\phi + \delta \phi) \ , \ m_x \cos(\theta + \delta \theta) + m_y \sin(\psi + \delta \psi) \sin(\theta + \delta \theta) + m_x \cos(\phi + \delta \phi) \sin(\theta + \delta \theta)  \right )$$ and I want to know $$\frac{\partial \ \psi}{\partial \ \delta  \phi} \text{ and } \frac{\partial \ \psi}{\partial \ \delta  \theta}$$ I've tried to rewrite the expression to some conditional that checks if $d(x) < 0$ , and if so it sums $\pi$ to $\tan(c(x),d(x))$ . The thing is that the condition depends on both $\delta \theta$ and $\delta \phi$ and I don't know how to derive it. Both $\delta \theta$ and $\delta \phi$ may be assumed to be very small angles. I don't know for sure if the expressions I want exist. I've got no problem in using conditionals as long as the expressions I want are in closed-form. All suggestions are appreciated. Thank you in advance.","I'm currently working on some navigation equations and I would like to write down the derivative with respect to of something like I've searched wherever I've could and the only thing I've come across are the partial derivatives of with respect to and . To be more specific, my equation looks like this: and I want to know I've tried to rewrite the expression to some conditional that checks if , and if so it sums to . The thing is that the condition depends on both and and I don't know how to derive it. Both and may be assumed to be very small angles. I don't know for sure if the expressions I want exist. I've got no problem in using conditionals as long as the expressions I want are in closed-form. All suggestions are appreciated. Thank you in advance.","x f(x) = \operatorname{arctan2}(c(x), d(x)) \operatorname{arctan2}(y,x) x y \psi = \operatorname{arctan2} \left ( -m_y \cos(\phi + \delta \phi) + m_z \sin(\phi + \delta \phi) \ , \ m_x \cos(\theta + \delta \theta) + m_y \sin(\psi + \delta \psi) \sin(\theta + \delta \theta) + m_x \cos(\phi + \delta \phi) \sin(\theta + \delta \theta)  \right ) \frac{\partial \ \psi}{\partial \ \delta  \phi} \text{ and } \frac{\partial \ \psi}{\partial \ \delta  \theta} d(x) < 0 \pi \tan(c(x),d(x)) \delta \theta \delta \phi \delta \theta \delta \phi","['calculus', 'derivatives', 'trigonometry', 'partial-derivative', 'closed-form']"
95,Symmetry of certain improper integral,Symmetry of certain improper integral,,"Let $a < b < c < d$ be real numbers. Is it true that the following identity holds?  $$\int_a^b \frac{{\rm d}x}{\sqrt{|x-a||x-b||x-c||x-d|}}= \int_c^d \frac{{\rm d}x}{\sqrt{|x-a||x-b||x-c||x-d|}}$$ A friend of mine told me a friend of his said that it was true. But alas, we don't have a clue of how to attack this. We made a couple of cases in Mathematica and it seems to be true, unless we missed something simple.","Let $a < b < c < d$ be real numbers. Is it true that the following identity holds?  $$\int_a^b \frac{{\rm d}x}{\sqrt{|x-a||x-b||x-c||x-d|}}= \int_c^d \frac{{\rm d}x}{\sqrt{|x-a||x-b||x-c||x-d|}}$$ A friend of mine told me a friend of his said that it was true. But alas, we don't have a clue of how to attack this. We made a couple of cases in Mathematica and it seems to be true, unless we missed something simple.",,"['calculus', 'integration', 'improper-integrals']"
96,How to find the value of this integral? $\int_0^1x^2\sqrt{1-x^2}\log(1-x)dx=-\frac{G}{4}+\frac{1}{24}+\frac{\pi}{64}-\frac{\pi}{16}\ln2$,How to find the value of this integral?,\int_0^1x^2\sqrt{1-x^2}\log(1-x)dx=-\frac{G}{4}+\frac{1}{24}+\frac{\pi}{64}-\frac{\pi}{16}\ln2,"$$I=\int_0^1x^2\sqrt{1-x^2}\log(1-x)dx=-\frac{G}{4}+\frac{1}{24}+\frac{\pi}{64}-\frac{\pi}{16}\ln2$$  Where G is the Catalan's contant My try: Integrating by parts, we have:  $$3I=\int_0^1x^4\ln(1-x)\frac{dx}{\sqrt{1-x^2}}+\int_0^1x^3\sqrt{1-x^2}\frac{dx}{1-x}=S+T$$  $$\frac{x^3}{1-x}=-x^2-x-1+\frac{1}{1-x}$$  It is easy to deduce:  $$\int_0^1x^2\sqrt{1-x^2}dx=\frac{\pi}{16},\int_0^1x\sqrt{1-x^2}dx=\frac{1}{3},\int_0^1\sqrt{1-x^2}dx=\frac{\pi}{4},\int_0^1\sqrt{1-x^2}\frac{dx}{1-x}=\frac{\pi}{2}+1$$  $$3I=S+\frac{3\pi}{16}+\frac{2}{3}$$  But how to deduct S?","$$I=\int_0^1x^2\sqrt{1-x^2}\log(1-x)dx=-\frac{G}{4}+\frac{1}{24}+\frac{\pi}{64}-\frac{\pi}{16}\ln2$$  Where G is the Catalan's contant My try: Integrating by parts, we have:  $$3I=\int_0^1x^4\ln(1-x)\frac{dx}{\sqrt{1-x^2}}+\int_0^1x^3\sqrt{1-x^2}\frac{dx}{1-x}=S+T$$  $$\frac{x^3}{1-x}=-x^2-x-1+\frac{1}{1-x}$$  It is easy to deduce:  $$\int_0^1x^2\sqrt{1-x^2}dx=\frac{\pi}{16},\int_0^1x\sqrt{1-x^2}dx=\frac{1}{3},\int_0^1\sqrt{1-x^2}dx=\frac{\pi}{4},\int_0^1\sqrt{1-x^2}\frac{dx}{1-x}=\frac{\pi}{2}+1$$  $$3I=S+\frac{3\pi}{16}+\frac{2}{3}$$  But how to deduct S?",,"['calculus', 'integration', 'definite-integrals']"
97,How to compute $\int\limits_{\mathbb{R}^3}\det D^2 u(\mathbf{r}) d\mathbf{r}$?,How to compute ?,\int\limits_{\mathbb{R}^3}\det D^2 u(\mathbf{r}) d\mathbf{r},"Let $u\in C^3(\mathbb{R}^3)$ and $u$ is zero outside some bounded domain. I need to find the value of  $$ \int\limits_{ \large\mathbb{R}^3}\det \mathrm D^2 u(\mathbf{r}) \, \mathrm d\mathbf{r} $$ I suspect it equals zero. I can show this for some specific cases. Here $\mathrm D u$ is a gradient of $u$, while $\mathrm D^2u$ is a Hessian.","Let $u\in C^3(\mathbb{R}^3)$ and $u$ is zero outside some bounded domain. I need to find the value of  $$ \int\limits_{ \large\mathbb{R}^3}\det \mathrm D^2 u(\mathbf{r}) \, \mathrm d\mathbf{r} $$ I suspect it equals zero. I can show this for some specific cases. Here $\mathrm D u$ is a gradient of $u$, while $\mathrm D^2u$ is a Hessian.",,"['calculus', 'integration', 'vector-analysis']"
98,Other integrals for the tribonacci constant?,Other integrals for the tribonacci constant?,,"The post, Is there an integral for the golden ratio? gives numerous beautiful integrals for $\phi$. Some were just specializations of trigonometric evaluations such as, $$F(k)=\int_0^\infty \frac{x^{\pi/k-1}}{1+x^{2\pi}}dx =\frac{1}{2}\csc\Big(\frac{\pi}{2k}\Big)=\phi,\quad\text{at}\;k=5$$ However, integrals for phi's cousin the tribonacci constant $T$ seem to be harder to find. Phi appears in the pentagon, dodecahedron, etc, but $T$ also has a geometric context, the snub cube, $\hskip2.8in$ One integral I know for $T$ is, $$\beta\times \Bigl(\frac{T+1}{T}\Bigr)^2=\int_0^1 \frac{1}{\sqrt{(1-t^2)(1-k^2t^2)}}dt = 1.570983\dots$$ where, $$\beta=\frac{\Gamma\bigl(\tfrac{1}{11}\bigr)\, \Gamma\bigl(\tfrac{3}{11}\bigr)\, \Gamma\bigl(\tfrac{4}{11}\bigr)\, \Gamma\bigl(\tfrac{5}{11}\bigr)\, \Gamma\bigl(\tfrac{9}{11}\bigr)}{11^{1/4}(4\pi)^2}$$ $$k = \frac{1}{2}\sqrt{2-\sqrt{\frac{2T+15}{2T+1}}}$$ though it is a bit unsatisfying as $T$ appears in the integrand. Q: Are there other nice integrals for the tribonacci constant $T$?","The post, Is there an integral for the golden ratio? gives numerous beautiful integrals for $\phi$. Some were just specializations of trigonometric evaluations such as, $$F(k)=\int_0^\infty \frac{x^{\pi/k-1}}{1+x^{2\pi}}dx =\frac{1}{2}\csc\Big(\frac{\pi}{2k}\Big)=\phi,\quad\text{at}\;k=5$$ However, integrals for phi's cousin the tribonacci constant $T$ seem to be harder to find. Phi appears in the pentagon, dodecahedron, etc, but $T$ also has a geometric context, the snub cube, $\hskip2.8in$ One integral I know for $T$ is, $$\beta\times \Bigl(\frac{T+1}{T}\Bigr)^2=\int_0^1 \frac{1}{\sqrt{(1-t^2)(1-k^2t^2)}}dt = 1.570983\dots$$ where, $$\beta=\frac{\Gamma\bigl(\tfrac{1}{11}\bigr)\, \Gamma\bigl(\tfrac{3}{11}\bigr)\, \Gamma\bigl(\tfrac{4}{11}\bigr)\, \Gamma\bigl(\tfrac{5}{11}\bigr)\, \Gamma\bigl(\tfrac{9}{11}\bigr)}{11^{1/4}(4\pi)^2}$$ $$k = \frac{1}{2}\sqrt{2-\sqrt{\frac{2T+15}{2T+1}}}$$ though it is a bit unsatisfying as $T$ appears in the integrand. Q: Are there other nice integrals for the tribonacci constant $T$?",,"['calculus', 'sequences-and-series', 'definite-integrals', 'constants']"
99,Evaluating a certain integral without the fundamental theorem,Evaluating a certain integral without the fundamental theorem,,"I'm a TA for a calculus course. And they recently began calculating definite integrals using a definition equivalent to Riemann's criterion. Of course, the type of things they were calculating were fairly elementary such as $$\int_0^1x\;dx\qquad\text{and}\qquad\int_0^1x^2\;dx$$ Knowing full well that the fundamental theorem of calculus was on the itinerary, I decided to give them an appreciation for the result by proving a much more general result (still using rectangles). Namely I showed: $$\int_a^b x^n\;dx=\frac{b^{n+1}}{n+1}-\frac{a^{n+1}}{n+1}$$ This can be calculated in a way that parallels the calculation of the above examples. As long as one knows that $$\lim_{m\rightarrow\infty}\frac{1^n+2^n+\cdots+m^n}{\frac{m^{n+1}}{n+1}}=1$$ one is able to proceed. Granted, I had to give a loose argument for why this is true, but knowing that $$1+2+\cdots+n=\frac{1}{2}n^2+\cdots \qquad\text{ and } 1^2+2^2+ \cdots +n^2 = \frac{1}{3}n^3 + \cdots$$ The pattern seems plausible. I thought this was cute, so I also gave them the proof that $$\int_0^x\cos t\;dt=\sin x$$ which can be derived with rectangles using Dirichlet's identity: $$1+2\sum_{k=1}^n\cos(kx)=\frac{\sin\left([n+1/2]x\right)}{\sin(x/2)}$$ To be sure, many students found this un-amusing, but they all greatly affirmed that they were glad to have the fundamental theorem after it was delivered to them. So goal achieved. But I was intrigued by how many other integrals could I evaluate using the naive method? $$\int_0^x e^t\;dt$$ isn't too bad as it's a geometric sum. The next thing in line was, of course, $$\int_1^x\ln t\; dt$$ This is where I ran into trouble. I had been using the fact that $$\int_a^b f(x)\;dx=\lim_{n\rightarrow\infty}\sum_{k=1}^n f\left(a+k\frac{b-a}{n}\right)\frac{b-a}{n}$$ for integrable $f$ to do the fore-going facts. But this approach seems intractable for $$\int_1^x\ln t\; dt$$ At least, I don't have the requisite limit knowledge or 'algebraic trick' needed to proceed. I was able to calculate this with the fact that $$\int_0^{\ln x}e^t\;dt+\int_1^x\ln t\;dt=x\ln x$$ which is a relationship that can be proven naively. But I was hoping someone here knew the 'trick' needed to calculate $$\int_1^x \ln t\;dt$$ without the fundamental theorem or relying on the insight to reflect the area in question. Any help is appreciated.","I'm a TA for a calculus course. And they recently began calculating definite integrals using a definition equivalent to Riemann's criterion. Of course, the type of things they were calculating were fairly elementary such as $$\int_0^1x\;dx\qquad\text{and}\qquad\int_0^1x^2\;dx$$ Knowing full well that the fundamental theorem of calculus was on the itinerary, I decided to give them an appreciation for the result by proving a much more general result (still using rectangles). Namely I showed: $$\int_a^b x^n\;dx=\frac{b^{n+1}}{n+1}-\frac{a^{n+1}}{n+1}$$ This can be calculated in a way that parallels the calculation of the above examples. As long as one knows that $$\lim_{m\rightarrow\infty}\frac{1^n+2^n+\cdots+m^n}{\frac{m^{n+1}}{n+1}}=1$$ one is able to proceed. Granted, I had to give a loose argument for why this is true, but knowing that $$1+2+\cdots+n=\frac{1}{2}n^2+\cdots \qquad\text{ and } 1^2+2^2+ \cdots +n^2 = \frac{1}{3}n^3 + \cdots$$ The pattern seems plausible. I thought this was cute, so I also gave them the proof that $$\int_0^x\cos t\;dt=\sin x$$ which can be derived with rectangles using Dirichlet's identity: $$1+2\sum_{k=1}^n\cos(kx)=\frac{\sin\left([n+1/2]x\right)}{\sin(x/2)}$$ To be sure, many students found this un-amusing, but they all greatly affirmed that they were glad to have the fundamental theorem after it was delivered to them. So goal achieved. But I was intrigued by how many other integrals could I evaluate using the naive method? $$\int_0^x e^t\;dt$$ isn't too bad as it's a geometric sum. The next thing in line was, of course, $$\int_1^x\ln t\; dt$$ This is where I ran into trouble. I had been using the fact that $$\int_a^b f(x)\;dx=\lim_{n\rightarrow\infty}\sum_{k=1}^n f\left(a+k\frac{b-a}{n}\right)\frac{b-a}{n}$$ for integrable $f$ to do the fore-going facts. But this approach seems intractable for $$\int_1^x\ln t\; dt$$ At least, I don't have the requisite limit knowledge or 'algebraic trick' needed to proceed. I was able to calculate this with the fact that $$\int_0^{\ln x}e^t\;dt+\int_1^x\ln t\;dt=x\ln x$$ which is a relationship that can be proven naively. But I was hoping someone here knew the 'trick' needed to calculate $$\int_1^x \ln t\;dt$$ without the fundamental theorem or relying on the insight to reflect the area in question. Any help is appreciated.",,['calculus']
