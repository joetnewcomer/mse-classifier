,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How to prove $\sum_{k=1}^n (-1)^{(k+1)} k \binom nk=0$ [closed],How to prove  [closed],\sum_{k=1}^n (-1)^{(k+1)} k \binom nk=0,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question How to prove this identity? $$\sum_{k=1}^n (-1)^{(k+1)} k \binom nk=0$$ This is George Casella statistical inference textbook exercise 1.27 (c). I have no idea to prove.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question How to prove this identity? This is George Casella statistical inference textbook exercise 1.27 (c). I have no idea to prove.",\sum_{k=1}^n (-1)^{(k+1)} k \binom nk=0,"['combinatorics', 'statistics', 'summation', 'binomial-coefficients']"
1,find density function from moment generating function,find density function from moment generating function,,"I'm suppose to obtain the density function of this mgf for a discrete random variable $Y$ taking values in range $[0, \infty)$ , where $E(e^{ty}) = e^{-l}[1-e^t(1-p)]^{-ld},c\in \mathbb{R}^+,p \in [0,1]$ . My intuition was to find the taylor polynomial of this function but it quickly blew into a incomprehensible mess.","I'm suppose to obtain the density function of this mgf for a discrete random variable taking values in range , where . My intuition was to find the taylor polynomial of this function but it quickly blew into a incomprehensible mess.","Y [0, \infty) E(e^{ty}) = e^{-l}[1-e^t(1-p)]^{-ld},c\in \mathbb{R}^+,p \in [0,1]","['probability', 'statistics', 'moment-generating-functions']"
2,What is the difference between a two sample t-test and a paired t-test?,What is the difference between a two sample t-test and a paired t-test?,,"I have the following question from AP statistics: Two friends were curious if it was faster to use the drive-thru or order at the counter at their favorite fast food restaurant. For 555 different visits, one of them ordered at the counter while the other used the drive-thru (determined by a coin toss). Each person ordered the same meal at every visit. They want to test if these results suggest a significant difference in the average time between ordering at the counter and ordering in the drive-thru. Assume that the necessary conditions for inference were met. Which of these is the most appropriate test and alternative hypothesis? A) Paired t-test with alternative hypothesis: $\mu_{counter} - \mu_{drive-thru} > 0$ B) Paired t-test with alternative hypothesis: $\mu_{counter} - \mu_{drive-thru} \neq 0$ C) Two-sample t-test with alternative hypothesis: $\mu_{counter} > \mu_{drive-thru}$ D) Two-sample t-test with alternative hypothesis: $\mu_{counter} \neq \mu_{drive-thru}$ E) Two-sample t-test with alternative hypothesis: $\mu_{counter} < \mu_{drive-thru}$ It is easy to exclude A, C, E and I am just struggling with whether it should be a two-sample t-test or a paired t-test. I know the difference between them is that a two-sample t-test gets data from two different group and paired t-test gets data from one group. The correct answer for this question is B which means this is a paired t-test, but this question states that 'one of them ordered at the counter while the other used the drive-thru."" Isn't that considered as two groups and this should be considered as two-sample t-test? Also, what is the more clear difference between a two-sample t test and a paired t-test? I am very confused with that. Thanks for any responses! *Sorry for the bad format, I don't really know how to type those symbols.","I have the following question from AP statistics: Two friends were curious if it was faster to use the drive-thru or order at the counter at their favorite fast food restaurant. For 555 different visits, one of them ordered at the counter while the other used the drive-thru (determined by a coin toss). Each person ordered the same meal at every visit. They want to test if these results suggest a significant difference in the average time between ordering at the counter and ordering in the drive-thru. Assume that the necessary conditions for inference were met. Which of these is the most appropriate test and alternative hypothesis? A) Paired t-test with alternative hypothesis: B) Paired t-test with alternative hypothesis: C) Two-sample t-test with alternative hypothesis: D) Two-sample t-test with alternative hypothesis: E) Two-sample t-test with alternative hypothesis: It is easy to exclude A, C, E and I am just struggling with whether it should be a two-sample t-test or a paired t-test. I know the difference between them is that a two-sample t-test gets data from two different group and paired t-test gets data from one group. The correct answer for this question is B which means this is a paired t-test, but this question states that 'one of them ordered at the counter while the other used the drive-thru."" Isn't that considered as two groups and this should be considered as two-sample t-test? Also, what is the more clear difference between a two-sample t test and a paired t-test? I am very confused with that. Thanks for any responses! *Sorry for the bad format, I don't really know how to type those symbols.",\mu_{counter} - \mu_{drive-thru} > 0 \mu_{counter} - \mu_{drive-thru} \neq 0 \mu_{counter} > \mu_{drive-thru} \mu_{counter} \neq \mu_{drive-thru} \mu_{counter} < \mu_{drive-thru},['statistics']
3,Expected value of the smaller of two random variables.,Expected value of the smaller of two random variables.,,"I am studying machine learning by Kevin Murphy and a very simple problem got me stuck. Note for the jaintors - I know a solution is probably available, but I want to know why my solution is bad. I will rewrite the problem here so I can explain what is wrong with my solution. Suppose X, Y are two points sampled independently at random from the interval $[0,1]$ . What is the expected location of the leftmost point? Here is a picture of my approach: I have split a $[0,1]\times [0,1]$ plane into two triangles, one of which satisfies the condition $Y<X$ . This happens with probability of $0.5$ , but I can, without loss of generality, just calculate the result for this triangle, since for the other one the result will be the same, just with flipped variables. This way, I already have $Y<X$ . Now, I need to derive a pdf for $Y$ , given $X=x$ . If $X=x$ , than $Y$ must be smaller than $x$ . This means, that $y\in(0,x)$ . Going further, if $y \in (0,x)$ , and $y$ is uniformly distributed, than the pdf of $Y$ given $X$ must be: $P(Y|X=x)={1\over x}$ - since the pdf must integrate to one. Now what I need is the pdf of $Y$ itself, so I can calculate it's expected value. For this reason, I sum over the whole available $X$ space: $p(y)=\int_{y}^{1}{1 \over x}dx$ - i integrate from $y$ , because $x > y$ . This way, I get $p(y) = -\log(y)$ . Then, the expected value is: $\mathbb{E}[y] = -\int_{0}^{1}y\log(y) = {1 \over 4}$ . However, using another approach, I can do: $p(\min(X,Y) > x) = p(X > x, Y > x) = (1 - x)(1 - x) = (1-x)^2$ Let $Z = \min(X, Y)$ , then: $p(Z > x) = (1-x)^2$ , and: $p(Z < x) = 1 - (1-x)^2$ , giving: $P_Z(z) = 1-(1-z)^2$ , which is the cdf of $Z$ - so I can obtain the $pdf$ by differentiation. $p(z) = {d \over {dz}}P_Z(z) = 2(1-z)$ Now, I can go straight into the expected value: $\mathbb{E}[z] = \int_{0}^{1}2(z-z^2) = {1 \over 3}$ After checking on the internet, the second result i got (1/3), is the valid one. But I'm not really that concerned with why this is the right result, but rather why my first approach was wrong.","I am studying machine learning by Kevin Murphy and a very simple problem got me stuck. Note for the jaintors - I know a solution is probably available, but I want to know why my solution is bad. I will rewrite the problem here so I can explain what is wrong with my solution. Suppose X, Y are two points sampled independently at random from the interval . What is the expected location of the leftmost point? Here is a picture of my approach: I have split a plane into two triangles, one of which satisfies the condition . This happens with probability of , but I can, without loss of generality, just calculate the result for this triangle, since for the other one the result will be the same, just with flipped variables. This way, I already have . Now, I need to derive a pdf for , given . If , than must be smaller than . This means, that . Going further, if , and is uniformly distributed, than the pdf of given must be: - since the pdf must integrate to one. Now what I need is the pdf of itself, so I can calculate it's expected value. For this reason, I sum over the whole available space: - i integrate from , because . This way, I get . Then, the expected value is: . However, using another approach, I can do: Let , then: , and: , giving: , which is the cdf of - so I can obtain the by differentiation. Now, I can go straight into the expected value: After checking on the internet, the second result i got (1/3), is the valid one. But I'm not really that concerned with why this is the right result, but rather why my first approach was wrong.","[0,1] [0,1]\times [0,1] Y<X 0.5 Y<X Y X=x X=x Y x y\in(0,x) y \in (0,x) y Y X P(Y|X=x)={1\over x} Y X p(y)=\int_{y}^{1}{1 \over x}dx y x > y p(y) = -\log(y) \mathbb{E}[y] = -\int_{0}^{1}y\log(y) = {1 \over 4} p(\min(X,Y) > x) = p(X > x, Y > x) = (1 - x)(1 - x) = (1-x)^2 Z = \min(X, Y) p(Z > x) = (1-x)^2 p(Z < x) = 1 - (1-x)^2 P_Z(z) = 1-(1-z)^2 Z pdf p(z) = {d \over {dz}}P_Z(z) = 2(1-z) \mathbb{E}[z] = \int_{0}^{1}2(z-z^2) = {1 \over 3}","['probability', 'statistics', 'density-function', 'order-statistics']"
4,How do you find $E(X^3)$ of a Poisson Distribution? [closed],How do you find  of a Poisson Distribution? [closed],E(X^3),"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I know the proof to find the variance of a Poisson Distribution, and I tried to use that to find $E(X^3)$ , but I can't get it to work. Any help would be great!!","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I know the proof to find the variance of a Poisson Distribution, and I tried to use that to find , but I can't get it to work. Any help would be great!!",E(X^3),"['probability', 'statistics', 'probability-distributions', 'poisson-distribution', 'poisson-summation-formula']"
5,How to calculate the correlation between the number of heads of 100 toss of coin and the number of heads of the first 10 toss of those 100 tosses?,How to calculate the correlation between the number of heads of 100 toss of coin and the number of heads of the first 10 toss of those 100 tosses?,,"It is a bit cumbersome to explain: Toss a coin is a Bernoulli distribution, with the probability of seeing a head is p if we toss this coin 100 times, we should expect $X_{1}$ times of head. Within that 100 toss (this is important, we are NOT tossing another 10 times), we should see $X_{2}$ heads from the first 10 toss. How to calculate $corr(X_{1}, X_{2})$ ? The only thing I can think of is, $X_{1} > X_{2}$ , practially, we are doing two sets: toss a coin 10 times, we see $X_{2}$ heads independently toss a coin 90 times, we see $X_{3}$ heads we want to calculate $corr(X_{2}, X_{2} + X_{3})$","It is a bit cumbersome to explain: Toss a coin is a Bernoulli distribution, with the probability of seeing a head is p if we toss this coin 100 times, we should expect times of head. Within that 100 toss (this is important, we are NOT tossing another 10 times), we should see heads from the first 10 toss. How to calculate ? The only thing I can think of is, , practially, we are doing two sets: toss a coin 10 times, we see heads independently toss a coin 90 times, we see heads we want to calculate","X_{1} X_{2} corr(X_{1}, X_{2}) X_{1} > X_{2} X_{2} X_{3} corr(X_{2}, X_{2} + X_{3})","['probability', 'statistics', 'correlation']"
6,"Application of chain rule, and some recursion","Application of chain rule, and some recursion",,"Consider the differentiable functions $L^1(x,\theta^1),L^2(x^2,\theta^2),...,L^l(x^l,\theta^l)$ , where every $x_k,\theta^k$ are real vectors, for $k=1,...,l$ . Also define $\theta=(\theta^1,...,\theta^l)$ . Define the composite function $f(x,\theta)=x^{l+1}$ recursively by doing $x^k= L^{k-1}(x^{k-1},\theta^{k-1})$ , $x^1=x$ . Compute $J_\theta f$ , the jacobian of $f$ with respect to $\theta$ For some context, I'm trying to implement gradient descent for optimizing the loss function of a neural network, and if my computations are correct I don't understand why we do back-propagation, instead of, say, forward-propagation... Here is my attempt, is there any mistake? Compute $J f$ : using the chain rule: $$ Jf=JL^l(x^l,\theta^l)= \left ( J_{x^l}L^l\cdot J_{x,\theta^1,...,\theta^{l-1}}x^l \middle| J_{\theta^l}L^l\right )= \left ( J_{x^l}L^l\cdot J_{x,\theta^1,...,\theta^{l-1}}L^{l-1} \middle| J_{\theta^l}L^l\right )$$ Hence we can write $Jf=J^l$ , where $J^l$ is given by the following recursive rule: $$J^k=\left ( J_{x^k}L^k\cdot J^{k-1}\middle| J_{\theta^k}L^k\right ), \quad J^1=J_{x,\theta^1}L^1$$ Obtain $J_\theta f$ : we want to obtain the last columns of $Jf$ , corresponding to the derivatives with respect to $\theta^1,...,\theta^l$ . Clearly $$J_\theta f=\left ( J_{x^l}L^l\cdot J_{\theta^1,...,\theta^{l-1}}L^{l-1} \middle| J_{\theta^l}L^l\right )$$ Hence $J_\theta f=G^l$ , where: $$G^k=\left ( J_{x^k}L^k\cdot G^{k-1}\middle| J_{\theta^k}L^k\right ), \quad G^1=J_{\theta^1}L^1$$","Consider the differentiable functions , where every are real vectors, for . Also define . Define the composite function recursively by doing , . Compute , the jacobian of with respect to For some context, I'm trying to implement gradient descent for optimizing the loss function of a neural network, and if my computations are correct I don't understand why we do back-propagation, instead of, say, forward-propagation... Here is my attempt, is there any mistake? Compute : using the chain rule: Hence we can write , where is given by the following recursive rule: Obtain : we want to obtain the last columns of , corresponding to the derivatives with respect to . Clearly Hence , where:","L^1(x,\theta^1),L^2(x^2,\theta^2),...,L^l(x^l,\theta^l) x_k,\theta^k k=1,...,l \theta=(\theta^1,...,\theta^l) f(x,\theta)=x^{l+1} x^k= L^{k-1}(x^{k-1},\theta^{k-1}) x^1=x J_\theta f f \theta J f  Jf=JL^l(x^l,\theta^l)= \left ( J_{x^l}L^l\cdot J_{x,\theta^1,...,\theta^{l-1}}x^l \middle| J_{\theta^l}L^l\right )= \left ( J_{x^l}L^l\cdot J_{x,\theta^1,...,\theta^{l-1}}L^{l-1} \middle| J_{\theta^l}L^l\right ) Jf=J^l J^l J^k=\left ( J_{x^k}L^k\cdot J^{k-1}\middle| J_{\theta^k}L^k\right ), \quad J^1=J_{x,\theta^1}L^1 J_\theta f Jf \theta^1,...,\theta^l J_\theta f=\left ( J_{x^l}L^l\cdot J_{\theta^1,...,\theta^{l-1}}L^{l-1} \middle| J_{\theta^l}L^l\right ) J_\theta f=G^l G^k=\left ( J_{x^k}L^k\cdot G^{k-1}\middle| J_{\theta^k}L^k\right ), \quad G^1=J_{\theta^1}L^1","['real-analysis', 'calculus', 'linear-algebra', 'statistics', 'machine-learning']"
7,Is there a non-mathematical definition of the geometric mean?,Is there a non-mathematical definition of the geometric mean?,,"I am aware that the geometric mean is often used with the lognormal distribution, because then it directly relates to the arithmetic mean with the normal distribution. But I was trying to think of an intuitive defintion of the geometric mean. For example, the median can be explained as ""the data point such that half of the data points have higher values and the other half have lower values."" Is there a similar definition for the geometric mean?","I am aware that the geometric mean is often used with the lognormal distribution, because then it directly relates to the arithmetic mean with the normal distribution. But I was trying to think of an intuitive defintion of the geometric mean. For example, the median can be explained as ""the data point such that half of the data points have higher values and the other half have lower values."" Is there a similar definition for the geometric mean?",,"['statistics', 'means', 'central-tendency']"
8,"If $\operatorname{Corr}(X,Y)=1$, then $ \operatorname{Corr}(X,Z)=\operatorname{Corr}(Y,Z)$","If , then","\operatorname{Corr}(X,Y)=1  \operatorname{Corr}(X,Z)=\operatorname{Corr}(Y,Z)","$\DeclareMathOperator{\Corr}{Corr}$ Given that I have three r.v. $X,Y,Z$ and $\Corr(X,Y)=1$ , can I then conclude that $\Corr(X,Z)=\Corr(Y,Z)$ ? I've tested on some data and found that it was true in my tests, but I've had no luck in regards to the math.","Given that I have three r.v. and , can I then conclude that ? I've tested on some data and found that it was true in my tests, but I've had no luck in regards to the math.","\DeclareMathOperator{\Corr}{Corr} X,Y,Z \Corr(X,Y)=1 \Corr(X,Z)=\Corr(Y,Z)","['statistics', 'covariance', 'correlation']"
9,MLE of Negative Binomial distributions of different sizes,MLE of Negative Binomial distributions of different sizes,,"There are two teams that are competing in a series of matches. These matches are a best-of- $x$ format, so after either team wins $\lfloor{\dfrac{x}{2}}\rfloor+1$ the series is over. This is, in essence, a negative binomial distribution. These two teams play a few different matches, with different $x$ values, and I want to know what the MLE is of $p$ , where $p$ is the probability that Team $1$ defeats Team $2$ . Here is what I attempted: Our likelyhood function is as follows: $$ L(k_i, x_i; p) = \prod_{i=1}^n c_ip^{k_i}(1-p_i)^{x_i-k_i} $$ where $x_i$ notes that match $i$ was a Best-of- $x$ and $k_i$ is the number of matches won by Team $1$ . If Team $1$ won match $i$ , then $c_i = \binom{x-1}{k-1}$ , else $c_i = \binom{x-1}{k}$ . Take the $\log$ of the likelyhood, we find: $$ \log{L} = \sum_{i=1}^n c_i+\log(p)\sum_{i=1}^nk_i + \log(1-p)\sum_{i=1}^n(x_i-k_i) $$ differentiating this with respect to $p$ , and setting that to $0$ to find the maximum, we get that: $$ 0 = \dfrac{\sum_{i=1}^nk_i}{p} - \dfrac{\sum_{i=1}^n(x_i-k_i)}{1-p} $$ $$ 0 = (1-p)\sum_{i=1}^nk_i - p\sum_{i=1}^n(x_i-k_i) $$ $$ \sum_{i=1}^nk_i = p\left(\sum_{i=1}^nk_i + \sum_{i=1}^n(x_i-k_i) \right) = p\sum_{i=1}^nx_i $$ which leads us to the final result $$ p = \dfrac{\sum_{i=1}^nk_i}{\sum_{i=1}^nx_i} = \dfrac{wins}{games} $$ From here I was a little skeptical of this result, so I simulated sets of matches for various probabilities of victory between teams to find the numerical MLE. For example, I did the scenario in which there is are $2$ Best-of- $1$ s, and one Best-of- $5$ . Team $1$ loses the best of $1$ s, but wins the Best-of- $5$ $3$ - $1$ . Numerically, I found the MLE to be $p\approx .65$ , but using the MLE I just found I would get that $$ p = \dfrac{0+0+3}{1+1+4} = \dfrac{1}{2} $$","There are two teams that are competing in a series of matches. These matches are a best-of- format, so after either team wins the series is over. This is, in essence, a negative binomial distribution. These two teams play a few different matches, with different values, and I want to know what the MLE is of , where is the probability that Team defeats Team . Here is what I attempted: Our likelyhood function is as follows: where notes that match was a Best-of- and is the number of matches won by Team . If Team won match , then , else . Take the of the likelyhood, we find: differentiating this with respect to , and setting that to to find the maximum, we get that: which leads us to the final result From here I was a little skeptical of this result, so I simulated sets of matches for various probabilities of victory between teams to find the numerical MLE. For example, I did the scenario in which there is are Best-of- s, and one Best-of- . Team loses the best of s, but wins the Best-of- - . Numerically, I found the MLE to be , but using the MLE I just found I would get that","x \lfloor{\dfrac{x}{2}}\rfloor+1 x p p 1 2 
L(k_i, x_i; p) = \prod_{i=1}^n c_ip^{k_i}(1-p_i)^{x_i-k_i}
 x_i i x k_i 1 1 i c_i = \binom{x-1}{k-1} c_i = \binom{x-1}{k} \log 
\log{L} = \sum_{i=1}^n c_i+\log(p)\sum_{i=1}^nk_i + \log(1-p)\sum_{i=1}^n(x_i-k_i)
 p 0 
0 = \dfrac{\sum_{i=1}^nk_i}{p} - \dfrac{\sum_{i=1}^n(x_i-k_i)}{1-p}
 
0 = (1-p)\sum_{i=1}^nk_i - p\sum_{i=1}^n(x_i-k_i)
 
\sum_{i=1}^nk_i = p\left(\sum_{i=1}^nk_i + \sum_{i=1}^n(x_i-k_i) \right) = p\sum_{i=1}^nx_i
 
p = \dfrac{\sum_{i=1}^nk_i}{\sum_{i=1}^nx_i} = \dfrac{wins}{games}
 2 1 5 1 1 5 3 1 p\approx .65 
p = \dfrac{0+0+3}{1+1+4} = \dfrac{1}{2}
","['probability', 'statistics', 'optimization', 'statistical-inference', 'maximum-likelihood']"
10,Correlation coefficient between two binominals,Correlation coefficient between two binominals,,"My problem is the following: Roll a dice 30 times, X is the number of 1's and Y is the number of 6's. Find the correlation coefficient $R(X,Y)=\frac{\operatorname{Cov}(X,Y)}{\sigma_x \sigma_y}$ .  The covariance is $\operatorname{Cov}(X,Y)=E(XY)-E(X)E(Y)=0-5*5=-25$ and the product of the ${\sigma}$ -s is $30\cdot\frac{1}{6}\cdot\frac{5}{6}=\frac{25}{6}$ , that gives $R=-6$ which is a bad solution for $R$ .  Where did I mistake? Can anybody fix it? Thank you for your answers","My problem is the following: Roll a dice 30 times, X is the number of 1's and Y is the number of 6's. Find the correlation coefficient .  The covariance is and the product of the -s is , that gives which is a bad solution for .  Where did I mistake? Can anybody fix it? Thank you for your answers","R(X,Y)=\frac{\operatorname{Cov}(X,Y)}{\sigma_x \sigma_y} \operatorname{Cov}(X,Y)=E(XY)-E(X)E(Y)=0-5*5=-25 {\sigma} 30\cdot\frac{1}{6}\cdot\frac{5}{6}=\frac{25}{6} R=-6 R","['probability', 'statistics']"
11,Maximum Likelihood Estimate with different parameters,Maximum Likelihood Estimate with different parameters,,"Suppose that X and Y are independent Poisson distributed values with means $\theta$ and $2\theta$ , respectively. Consider the combined estimator of $\theta$ $$ \tilde{\theta} = k_1 X + k_2 Y $$ where $k_1$ and $k_2$ are arbitrary constants. Find the condition on $k_1$ and $k_2$ such that $\tilde{\theta}$ is an unbiased estimator of $\theta$ . For $\tilde{\theta}$ unbiased, show that the variance of the estimator is minimized by taking $k_1 = 1/3$ and $k_2 = 1/3$ . Given observations $x$ and $y$ find the maximum likelihood estimate of $\theta$ and hence show that $\tilde{\theta}$ is also the maximum likelihood estimator. I have gotten (1) and (2) okay, but it's (3) I am having trouble with, I'd be okay if $X$ and $Y$ had the same parameter but I'm having trouble with $X$ and $Y$ having different parameters, any help would be appreciated. NOTE For (1) I got $k_1 = 1 - 2k_2$ . For (2) I found the variance of $\tilde{\theta}$ , then differentiated and let equal to zero to minimize - therefore we get (after subbing in $k_2 = 1 - k_1/2 $ ) $$3k_1-1=0,$$ which when subbing in $1/3$ , we see it is minimised. Thank you.","Suppose that X and Y are independent Poisson distributed values with means and , respectively. Consider the combined estimator of where and are arbitrary constants. Find the condition on and such that is an unbiased estimator of . For unbiased, show that the variance of the estimator is minimized by taking and . Given observations and find the maximum likelihood estimate of and hence show that is also the maximum likelihood estimator. I have gotten (1) and (2) okay, but it's (3) I am having trouble with, I'd be okay if and had the same parameter but I'm having trouble with and having different parameters, any help would be appreciated. NOTE For (1) I got . For (2) I found the variance of , then differentiated and let equal to zero to minimize - therefore we get (after subbing in ) which when subbing in , we see it is minimised. Thank you.","\theta 2\theta \theta 
\tilde{\theta} = k_1 X + k_2 Y
 k_1 k_2 k_1 k_2 \tilde{\theta} \theta \tilde{\theta} k_1 = 1/3 k_2 = 1/3 x y \theta \tilde{\theta} X Y X Y k_1 = 1 - 2k_2 \tilde{\theta} k_2 = 1 - k_1/2  3k_1-1=0, 1/3","['statistics', 'statistical-inference', 'poisson-distribution', 'estimation', 'maximum-likelihood']"
12,Variance of the sum of uncorrelated variables,Variance of the sum of uncorrelated variables,,"In using the Bienaymé formula to find the variance of means, I do not understand why $$\operatorname{Var}\!\left(\frac{1}{n}\sum_{i=1}^{n}{X_i}\right)=\frac{1}{n^2}\sum_{i=1}^n{\operatorname{Var}(X_i)}$$ I assume it is a matter of simple algebraic manipulation, but I do not understand it. Could someone please explain this property?","In using the Bienaymé formula to find the variance of means, I do not understand why I assume it is a matter of simple algebraic manipulation, but I do not understand it. Could someone please explain this property?",\operatorname{Var}\!\left(\frac{1}{n}\sum_{i=1}^{n}{X_i}\right)=\frac{1}{n^2}\sum_{i=1}^n{\operatorname{Var}(X_i)},"['probability-theory', 'statistics', 'variance']"
13,"Conditional expectation, what is my mistake","Conditional expectation, what is my mistake",,"From SOA sample #238: In a large population of patients, $.20$ have early stage cancer, $.10$ have advanced stage   cancer, and the other $.70$ do not have cancer. Six patients from this population are   randomly selected.   Calculate the expected number of selected patients with advanced stage cancer, given that   at least one of the selected patients has early stage cancer. What is wrong with my solution? $${1\cdot{5\choose 1}\cdot.1^1\cdot.9^4+2\cdot{5\choose 2}\cdot.1^2\cdot.9^3+3\cdot{5\choose 3}\cdot.1^3\cdot.9^2+4\cdot{5\choose 4}\cdot.1^4\cdot.9^1+5\cdot{5\choose 5}\cdot.1^5\cdot.9^0}\over{1-.8^6}$$ where the numerator is assuming there are only $5$ spots for a patient to have advanced stage cancer, since at least once has early stage cancer, and the denominator is the probability that at least one has early stage cancer. EDIT: It has been made clear to me from David Diaz's answer that at least part of my mistake was trying to apply methods that can only be used in the hyper-geometric distribution to the binomial distribution. That is, I was trying to say, let there be one person with early stage cancer and consider him independently, and consider the other five independently where they can be either early, advanced, or no cancer. That would work if the question was hyper-geometric, for example, if the question was ""If there are $N$ patients $.20$ have early stage cancer, $.10$ have advanced stage cancer, and the other $.70$ do not have cancer. Six patients from this population are randomly selected etc..."".  Then I would be able to do the following $${1\cdot {.2N \choose 1}{{.1N}\choose 1 }{{.2N-1+.7N}\choose 4 } + 2\cdot {.2N \choose 1}{{.1N}\choose 2 }{{.2N-1+.7N}\choose 3 }+ 3\cdot {.2N \choose 1}{{.1N}\choose 3 }{{.2N-1+.7N}\choose 2 }+ 4\cdot {.2N \choose 1}{{.1N}\choose 4 }{{.2N-1+.7N}\choose 1 }+ 5\cdot {.2N \choose 1}{{.1N}\choose 5 }{{.2N-1+.7N}\choose 0 }\over {.2N \choose 1}{{.2N-1+.8N}\choose 5}}$$ However, the binomial distribution is fundamentally different, and every trial that selects an early stage patient must be accounted for.","From SOA sample #238: In a large population of patients, have early stage cancer, have advanced stage   cancer, and the other do not have cancer. Six patients from this population are   randomly selected.   Calculate the expected number of selected patients with advanced stage cancer, given that   at least one of the selected patients has early stage cancer. What is wrong with my solution? where the numerator is assuming there are only spots for a patient to have advanced stage cancer, since at least once has early stage cancer, and the denominator is the probability that at least one has early stage cancer. EDIT: It has been made clear to me from David Diaz's answer that at least part of my mistake was trying to apply methods that can only be used in the hyper-geometric distribution to the binomial distribution. That is, I was trying to say, let there be one person with early stage cancer and consider him independently, and consider the other five independently where they can be either early, advanced, or no cancer. That would work if the question was hyper-geometric, for example, if the question was ""If there are patients have early stage cancer, have advanced stage cancer, and the other do not have cancer. Six patients from this population are randomly selected etc..."".  Then I would be able to do the following However, the binomial distribution is fundamentally different, and every trial that selects an early stage patient must be accounted for.",.20 .10 .70 {1\cdot{5\choose 1}\cdot.1^1\cdot.9^4+2\cdot{5\choose 2}\cdot.1^2\cdot.9^3+3\cdot{5\choose 3}\cdot.1^3\cdot.9^2+4\cdot{5\choose 4}\cdot.1^4\cdot.9^1+5\cdot{5\choose 5}\cdot.1^5\cdot.9^0}\over{1-.8^6} 5 N .20 .10 .70 {1\cdot {.2N \choose 1}{{.1N}\choose 1 }{{.2N-1+.7N}\choose 4 } + 2\cdot {.2N \choose 1}{{.1N}\choose 2 }{{.2N-1+.7N}\choose 3 }+ 3\cdot {.2N \choose 1}{{.1N}\choose 3 }{{.2N-1+.7N}\choose 2 }+ 4\cdot {.2N \choose 1}{{.1N}\choose 4 }{{.2N-1+.7N}\choose 1 }+ 5\cdot {.2N \choose 1}{{.1N}\choose 5 }{{.2N-1+.7N}\choose 0 }\over {.2N \choose 1}{{.2N-1+.8N}\choose 5}},"['probability', 'statistics']"
14,Are the sample space for one and multiple coin tosses the same?,Are the sample space for one and multiple coin tosses the same?,,"I am reading 'All of Statistics by Larry Wasserman'. I am at the first chapter and reading about sample space, sample outcome and events. I am a bit confused with one of the examples the author provided when explaining sample space. Definition of sample space from the book: The sample space $\Omega$ is the set of possible outcomes of an experiment. Here is what I understood: If our experiment is a coin toss, then the outcome of the experiment is either head or tail. So our sample space is $\Omega = \{H, T\}$ Also since sample space is a set it can not contain duplicate values. Then the book goes on and gives us an example: If we toss a coin forever , then the sample space is the infinite set, $\Omega = \{\omega = (\omega_1, \omega_2, \omega_3, . . . , ) : \omega_i ∈ {H, T}\}$ But this contradict with my understanding. Because in the example, both $\omega_1$ and $\omega_2$ can be H, and duplicate values are not allowed in a set . My understanding is that, regardless of how many times we toss the coin(once or forever), the sample space will always contain two values( $\Omega = \{H, T\}$ ) and not an infinite number of values, as the book says - because of the definition of set. Can someone help me clear our my confusion.","I am reading 'All of Statistics by Larry Wasserman'. I am at the first chapter and reading about sample space, sample outcome and events. I am a bit confused with one of the examples the author provided when explaining sample space. Definition of sample space from the book: The sample space is the set of possible outcomes of an experiment. Here is what I understood: If our experiment is a coin toss, then the outcome of the experiment is either head or tail. So our sample space is Also since sample space is a set it can not contain duplicate values. Then the book goes on and gives us an example: If we toss a coin forever , then the sample space is the infinite set, But this contradict with my understanding. Because in the example, both and can be H, and duplicate values are not allowed in a set . My understanding is that, regardless of how many times we toss the coin(once or forever), the sample space will always contain two values( ) and not an infinite number of values, as the book says - because of the definition of set. Can someone help me clear our my confusion.","\Omega \Omega = \{H, T\} \Omega = \{\omega = (\omega_1, \omega_2, \omega_3, . . . , ) : \omega_i ∈ {H, T}\} \omega_1 \omega_2 \Omega = \{H, T\}",['statistics']
15,Assume I choose $n$ random integers such that the last digit is uniformly distributed. What is the distribution of the last digit of the sum?,Assume I choose  random integers such that the last digit is uniformly distributed. What is the distribution of the last digit of the sum?,n,"Say that I sample $n$ random integers from some random variable $X$. The distribution has the last digit of the integer uniformly distributed. I then take the samples and add them $$ Y = x_1+x_2+x_3 + ... + x_n $$ What is the distribution of the last digit of $Y$? I want to also say uniform, but I'm not sure","Say that I sample $n$ random integers from some random variable $X$. The distribution has the last digit of the integer uniformly distributed. I then take the samples and add them $$ Y = x_1+x_2+x_3 + ... + x_n $$ What is the distribution of the last digit of $Y$? I want to also say uniform, but I'm not sure",,"['probability', 'combinatorics', 'statistics', 'probability-distributions', 'uniform-distribution']"
16,How do I get the expected value of this random value knowing its c.d.f.,How do I get the expected value of this random value knowing its c.d.f.,,"Can anyone answer with steps how to get the expected value of this random variable? Let $X$ be a random variable with following c.d.f, $$F(x) = \begin{cases}0  &x < -1\\ \dfrac{1-x^2}4 & -1 \le x < \dfrac{-1}{\sqrt 2}\\ \dfrac12 - x^4 & \dfrac{-1}{\sqrt 2} \le x < 0 \\ \dfrac34 + x & 0 \le x \lt\dfrac14 \\ 1 &x \ge \dfrac14 \end{cases}$$ Find $\mathbb E(X)$ Source","Can anyone answer with steps how to get the expected value of this random variable? Let $X$ be a random variable with following c.d.f, $$F(x) = \begin{cases}0  &x < -1\\ \dfrac{1-x^2}4 & -1 \le x < \dfrac{-1}{\sqrt 2}\\ \dfrac12 - x^4 & \dfrac{-1}{\sqrt 2} \le x < 0 \\ \dfrac34 + x & 0 \le x \lt\dfrac14 \\ 1 &x \ge \dfrac14 \end{cases}$$ Find $\mathbb E(X)$ Source",,"['probability', 'statistics', 'expectation', 'education']"
17,expected value greater than probability,expected value greater than probability,,"I'm supposed to prove that for any Random Variable X, $E[X^4] \ge \frac 14 P(X^2\ge \frac 12)$ I tried substituting the definitions of expected value and of the probability into the inequality, but that gets me no where. Any tips on where to go with this proof? Would a moment generating function lead me in the right direction? Thank you","I'm supposed to prove that for any Random Variable X, $E[X^4] \ge \frac 14 P(X^2\ge \frac 12)$ I tried substituting the definitions of expected value and of the probability into the inequality, but that gets me no where. Any tips on where to go with this proof? Would a moment generating function lead me in the right direction? Thank you",,"['probability', 'statistics', 'random-variables']"
18,"What does the notation $1_{[0,\infty)}(x)$ mean?",What does the notation  mean?,"1_{[0,\infty)}(x)","I came across this problem with the Rayleigh distribution where this notation was used: $f(x|\theta) = \displaystyle\frac{x}{\theta^2}\exp(-\displaystyle\frac{x^2}{2\theta^2})1_{[0,\infty)}(x)$ What does the notation $1_{[0,\infty)}(x)$ mean?","I came across this problem with the Rayleigh distribution where this notation was used: $f(x|\theta) = \displaystyle\frac{x}{\theta^2}\exp(-\displaystyle\frac{x^2}{2\theta^2})1_{[0,\infty)}(x)$ What does the notation $1_{[0,\infty)}(x)$ mean?",,"['probability', 'statistics', 'notation']"
19,Difference between theoretic moment and sample moment and understanding,Difference between theoretic moment and sample moment and understanding,,"OK, So I'm reading the following definitions $E(X_k)$ is the kth (theoretical) moment of the distribution (about the origin), for k = 1,2,... $M_k=\frac{1}{n}\sum_{i=1}^nX^k_i$ is the kth sample moment, for k = 1,2,... My questions are: How is the sample moment different from the theoretical moment? Isn't $E(X^k)=\frac{1}{n}\sum_{i=1}^nX^k_i$ ? What is the connection between sample moment and theoretical moment?","OK, So I'm reading the following definitions $E(X_k)$ is the kth (theoretical) moment of the distribution (about the origin), for k = 1,2,... $M_k=\frac{1}{n}\sum_{i=1}^nX^k_i$ is the kth sample moment, for k = 1,2,... My questions are: How is the sample moment different from the theoretical moment? Isn't $E(X^k)=\frac{1}{n}\sum_{i=1}^nX^k_i$ ? What is the connection between sample moment and theoretical moment?",,"['statistics', 'probability-distributions']"
20,what is smallest possible value of range of $7$ values given that their mean is $12$ and their median is $9$,what is smallest possible value of range of  values given that their mean is  and their median is,7 12 9,"what is smallest  possible  value  of range of $7$ values given that their mean is $12$ and their median is $9$ :- a) $3$ B) $6$ C) $7$ D) $8$ What is the proper approach to solve this problem , is there any relation or inquality that help ? What if we need the largest possible value? Thank you for your help","what is smallest  possible  value  of range of $7$ values given that their mean is $12$ and their median is $9$ :- a) $3$ B) $6$ C) $7$ D) $8$ What is the proper approach to solve this problem , is there any relation or inquality that help ? What if we need the largest possible value? Thank you for your help",,"['probability', 'statistics']"
21,Is the maximum likelihood estimator an unbiased estimator?,Is the maximum likelihood estimator an unbiased estimator?,,"Given is a random sample $X_1, .., X_n$ drawn from a distribution with the pdf $$ f(x; \theta) = \left\{ \begin{array}{ll}       \dfrac{1}{4} e^{-\dfrac{1}{4}(x-\theta)} & \theta < x \\       0 & \text{otherwise} \\ \end{array}  \right.  $$ with an unknown paramter $\theta$ $(0 < \theta)$. Find the maximum likelihood estimator (MLE) and determine whether or not it's unbiased. So for the MLE I found $X_{1:N}$, since the derivative of the likelihood function is $\dfrac{n}{4}$ with $n>0$, which is always positive so the likelihood function is increasing, which means we should choose the maximum value of $\theta$ possible to maximise the function. Our restriction is $\theta < x$, so the maximum we can choose is $X_{1:n}$. However, for determining its unbiasedness I'm not completely sure. We want to show I guess that $E[X_{1:n}] = \theta$, but I don't really know what to do with the expression $E[X_{1:n}]$ to be able to write it in an appropriate form including $\theta$. How should this be done?","Given is a random sample $X_1, .., X_n$ drawn from a distribution with the pdf $$ f(x; \theta) = \left\{ \begin{array}{ll}       \dfrac{1}{4} e^{-\dfrac{1}{4}(x-\theta)} & \theta < x \\       0 & \text{otherwise} \\ \end{array}  \right.  $$ with an unknown paramter $\theta$ $(0 < \theta)$. Find the maximum likelihood estimator (MLE) and determine whether or not it's unbiased. So for the MLE I found $X_{1:N}$, since the derivative of the likelihood function is $\dfrac{n}{4}$ with $n>0$, which is always positive so the likelihood function is increasing, which means we should choose the maximum value of $\theta$ possible to maximise the function. Our restriction is $\theta < x$, so the maximum we can choose is $X_{1:n}$. However, for determining its unbiasedness I'm not completely sure. We want to show I guess that $E[X_{1:n}] = \theta$, but I don't really know what to do with the expression $E[X_{1:n}]$ to be able to write it in an appropriate form including $\theta$. How should this be done?",,"['statistics', 'maximum-likelihood']"
22,Proving that SSE and SSR are independent [duplicate],Proving that SSE and SSR are independent [duplicate],,"This question already has answers here : How to prove SSE and SSR are independent (2 answers) Closed 2 years ago . I'm trying to show that SSE and SSR are independent (conditionally on X) but I have to use the following steps/hint. [Hint: Notice you have to consider SSE and SSR as random variables, so be careful how you define them. You may want to use the result that two linear forms U = AX and V = BX, with A and B being constant matrices and X is Normal, are independent iff Cov(U, V) = 0]. I know that the question was posted before but I'm not finding how to prove it using this hint. Any help is much appreciated!","This question already has answers here : How to prove SSE and SSR are independent (2 answers) Closed 2 years ago . I'm trying to show that SSE and SSR are independent (conditionally on X) but I have to use the following steps/hint. [Hint: Notice you have to consider SSE and SSR as random variables, so be careful how you define them. You may want to use the result that two linear forms U = AX and V = BX, with A and B being constant matrices and X is Normal, are independent iff Cov(U, V) = 0]. I know that the question was posted before but I'm not finding how to prove it using this hint. Any help is much appreciated!",,"['statistics', 'discrete-mathematics', 'least-squares', 'linear-regression']"
23,Probability Distribution (solve for k),Probability Distribution (solve for k),,"I don't know how to get this question started so a push in the right direction would be a great help. Here is the question; Let $X$ be a discrete random variable with probability distribution:   $$\begin{array}{c|c|c|c|c} x & 1 & 2 &3 & 4\\\hline P(X=x) & 1k & 2k & 3k & 4k \end{array}$$ (a) Solve for k ... so how would I use the formula to input this information in to find $k$? You guys do not have to solve for all the $k$'s. Just show me how to solve for one of the $k$'s, and then I think I can do the rest. Thank you!","I don't know how to get this question started so a push in the right direction would be a great help. Here is the question; Let $X$ be a discrete random variable with probability distribution:   $$\begin{array}{c|c|c|c|c} x & 1 & 2 &3 & 4\\\hline P(X=x) & 1k & 2k & 3k & 4k \end{array}$$ (a) Solve for k ... so how would I use the formula to input this information in to find $k$? You guys do not have to solve for all the $k$'s. Just show me how to solve for one of the $k$'s, and then I think I can do the rest. Thank you!",,"['probability', 'statistics', 'probability-distributions']"
24,"If $X \sim N(0,1)$ and $Y \sim N(0,1)$ are two random variables that may or may not be independent, what is $E(XY)$?","If  and  are two random variables that may or may not be independent, what is ?","X \sim N(0,1) Y \sim N(0,1) E(XY)","If $X \sim N(0,1)$ and $Y \sim N(0,1)$ are two random variables that may or may not be independent, I am wondering what $\operatorname{E}(XY)$ might be. It appears they have the same distribution, but does that imply anything about independence?","If $X \sim N(0,1)$ and $Y \sim N(0,1)$ are two random variables that may or may not be independent, I am wondering what $\operatorname{E}(XY)$ might be. It appears they have the same distribution, but does that imply anything about independence?",,"['probability', 'probability-theory', 'statistics']"
25,"How many ways can a slate of 4 (distinct) officers from 20 people, etc.","How many ways can a slate of 4 (distinct) officers from 20 people, etc.",,"In how many ways can a slate of 4 officers (president, vice-president, secretary, treasurer) be selected from among 20 people? $$4! {20 \choose 4}$$ In how many ways can a committee of three be selected from among 10 people? $$3! {10 \choose 3}$$ I wanna make sure I'm doing the questions above correctly, any feedback is greatly appreciated","In how many ways can a slate of 4 officers (president, vice-president, secretary, treasurer) be selected from among 20 people? $$4! {20 \choose 4}$$ In how many ways can a committee of three be selected from among 10 people? $$3! {10 \choose 3}$$ I wanna make sure I'm doing the questions above correctly, any feedback is greatly appreciated",,"['probability', 'statistics']"
26,Continuity vs Differentiability Intuitive Differences [duplicate],Continuity vs Differentiability Intuitive Differences [duplicate],,"This question already has answers here : Continuous versus differentiable (8 answers) Closed 7 years ago . I am a developer and am beginning to dabble with these mathematical concepts. I got to know that continuity means that making small changes in the input means that the output will also change by small amount only. This looks fine to me, as this means there will not be breaks in the curve leading to abrupt jumps i.e. discontinuity. Now, similarly, differentiability, I heard it means that at a point you can do linear approximation of the curve or something like a tangent to the curve exists at that point...but I don't get it. What does differentiability mean intuitively? What special property does differentiability confer to the curve? Is differentiability related to continuity? EDIT: I am trying to understand the input vs output relation under differentiability the same way in which I explained continuity above. The other question does not focus on this (I read it, it was defining it in terms of derivatives rather than an input-output perspective). Thanks for your help.","This question already has answers here : Continuous versus differentiable (8 answers) Closed 7 years ago . I am a developer and am beginning to dabble with these mathematical concepts. I got to know that continuity means that making small changes in the input means that the output will also change by small amount only. This looks fine to me, as this means there will not be breaks in the curve leading to abrupt jumps i.e. discontinuity. Now, similarly, differentiability, I heard it means that at a point you can do linear approximation of the curve or something like a tangent to the curve exists at that point...but I don't get it. What does differentiability mean intuitively? What special property does differentiability confer to the curve? Is differentiability related to continuity? EDIT: I am trying to understand the input vs output relation under differentiability the same way in which I explained continuity above. The other question does not focus on this (I read it, it was defining it in terms of derivatives rather than an input-output perspective). Thanks for your help.",,"['calculus', 'linear-algebra', 'statistics', 'differential-geometry', 'continuity']"
27,How many passwords can you form [closed],How many passwords can you form [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question How many passwords can you form using 4 A's, 5 B's and 6 C's , such that all A's are before C's. I found that the answer is 3003 but I didnt understand it","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question How many passwords can you form using 4 A's, 5 B's and 6 C's , such that all A's are before C's. I found that the answer is 3003 but I didnt understand it",,"['probability', 'combinatorics', 'statistics']"
28,Expected value calculation problem,Expected value calculation problem,,"The problem is such that how to find expected value if variable y has the following distributn( kind of discrete) $$p(y=0)=0.2$$ $$p(y=1)=0.3$$ $$p(y=2)=0.3$$ $$p(y>2)=0.2 $$ And for $y$ is $\textrm{Poisson}$ distributed, if $y>2$ . So i was confused by last term , i think we wouldnt consider cases when $y=3,4,5, \ldots$ so i am aware that it is possible to rewrite $p(y>2)$ as $1-p(y=1)-p(y=0)$ but then probabilities do not coincide. So how to find EY?","The problem is such that how to find expected value if variable y has the following distributn( kind of discrete) And for is distributed, if . So i was confused by last term , i think we wouldnt consider cases when so i am aware that it is possible to rewrite as but then probabilities do not coincide. So how to find EY?","p(y=0)=0.2 p(y=1)=0.3 p(y=2)=0.3 p(y>2)=0.2
 y \textrm{Poisson} y>2 y=3,4,5, \ldots p(y>2) 1-p(y=1)-p(y=0)","['probability', 'statistics', 'poisson-distribution']"
29,Finding the joint Probability distribution of $X$ and $Y$?,Finding the joint Probability distribution of  and ?,X Y,"If the joint probability distribution of X and Y is given by $$f(x,y)= \frac{(x-y)^2}{7}, \text{for }x=1,2,3;y=1,2 $$ $(1)$ Find the probability distribution of $U = X + Y; $ $(2)$ Find the conditional probability distribution of $X$ given $U =4.$ In order to solve this problem one must draw a chart. $$\begin{array}{|c|c|c|c|} \hline (x,y)& (1,1) & (1,2) & (2,1) & (2,2) & (3,1) & (3,2) \\ \hline  f(x,y)& 0 & \frac{1}{7} & \frac{1}{7} & 0 & \frac{4}{7} & \frac{1}{7}\\ \hline   U=x+y& 2 & 3 &3 & 4 & 4 & 5\\ \hline x &  & &\\ \hline \end {array}$$ How does one fill up the rest of the table and answer questions one and two. EDIT In order to find solve $(1)$ one must add all the related $f(x,y)$ relations. Thus $(1)$ $$ \quad P(U=2) =0, \\ P(U=3) = \frac{1}{7} + \frac{1}{7} = \frac27, \\ P(U=4)= 0+\frac{4}{7} = \frac47, \\ P(U=5)=\frac{1}{7}$$ One must use this notation to solve. $$P(x=1|U=4)= \frac{P(x=1,U=4)}{P(U=4)} = ? \\P(x=2|U=4) = ? \\ P(x=3|U=4) = ? $$ Knowing this does anyone know how to solve $(2)$ using this notation? What does one substitute for this question to derive the answer?","If the joint probability distribution of X and Y is given by $$f(x,y)= \frac{(x-y)^2}{7}, \text{for }x=1,2,3;y=1,2 $$ $(1)$ Find the probability distribution of $U = X + Y; $ $(2)$ Find the conditional probability distribution of $X$ given $U =4.$ In order to solve this problem one must draw a chart. $$\begin{array}{|c|c|c|c|} \hline (x,y)& (1,1) & (1,2) & (2,1) & (2,2) & (3,1) & (3,2) \\ \hline  f(x,y)& 0 & \frac{1}{7} & \frac{1}{7} & 0 & \frac{4}{7} & \frac{1}{7}\\ \hline   U=x+y& 2 & 3 &3 & 4 & 4 & 5\\ \hline x &  & &\\ \hline \end {array}$$ How does one fill up the rest of the table and answer questions one and two. EDIT In order to find solve $(1)$ one must add all the related $f(x,y)$ relations. Thus $(1)$ $$ \quad P(U=2) =0, \\ P(U=3) = \frac{1}{7} + \frac{1}{7} = \frac27, \\ P(U=4)= 0+\frac{4}{7} = \frac47, \\ P(U=5)=\frac{1}{7}$$ One must use this notation to solve. $$P(x=1|U=4)= \frac{P(x=1,U=4)}{P(U=4)} = ? \\P(x=2|U=4) = ? \\ P(x=3|U=4) = ? $$ Knowing this does anyone know how to solve $(2)$ using this notation? What does one substitute for this question to derive the answer?",,"['probability', 'statistics', 'probability-distributions']"
30,How to find the variance of $U= X-2Y+4Z$? & The Co-variance of $U=X-2Y+4Z$ and $V = 3X-Y-Z$,How to find the variance of ? & The Co-variance of  and,U= X-2Y+4Z U=X-2Y+4Z V = 3X-Y-Z,"EDIT If the random variables $X,Y, Z$ have the expected, $$\text{ means: }\mu_{x}=2 \qquad \qquad \mu_{y}=-3 \qquad \qquad \mu_{z} = 4$$ $$ \text{variances: }\sigma_{x}^{2}=3 \qquad \qquad \sigma_{y}^{2}=2 \qquad \qquad \sigma^{2}_{z}=8$$ $$\text{covariances: }\text{cov}(X,Y) =1 \quad \quad \text{cov}(X,Z) = -2 \quad \quad \text{Cov}(Y,Z) = 3$$ find the variance of $U = X-2Y+4Z$ . The co-variance of $U$ and $V = 3X-Y-Z$ One must use these formulas in order to solve this problem. From what I have deduced from the formulas above in order to find the variance one must use this formula $v(U)= \text{var}(a_x+b_y+c_z) =a^2\cdot \text{var}(x) +b^2\cdot \text{var}(Y)+c^2 \cdot \text{var}(z) + 2ab \cdot cov(x,y) +2ac\cdot \text{cov}(x,z)+2abc \cdot \text{cov}(Y,Z)$ To find the co-variance one must use this formula $\text{cov}(u,v) = \text{cov}(a_1+b_1+c_1,a_2+b_2+c_2)=(a_{1})(a_{2})\text{var}(x)+(b_{1})(b_{2})\text{var}(Y)+(c_{1})(c_{2})\text{var}(Z)+\left[ (a_{1})(b_{2})+(b_{1})(a_{2}) \right] \cdot \text{cov}(X,Y)+\left[ (a_{1})(c_{2})+(c_{1})(a_{2}) \right] \cdot \text{cov}(X,Z)+ \left[ (b_{1})(c_{2})+(c_{1})(b_{2}) \right] \cdot \text{cov}(Y,Z)$ Is the formula that I used above a correct interpretation of what is alluded by in the formulas above? Lastly, I do not want to make duplicates so the questions I have asked above is different from what I asked before in the previous questions, mainly because am asking  about the interpretations of the formulas...... not just the answer.I hope this is enough information so that this question can be its own independent entity.","EDIT If the random variables have the expected, find the variance of . The co-variance of and One must use these formulas in order to solve this problem. From what I have deduced from the formulas above in order to find the variance one must use this formula To find the co-variance one must use this formula Is the formula that I used above a correct interpretation of what is alluded by in the formulas above? Lastly, I do not want to make duplicates so the questions I have asked above is different from what I asked before in the previous questions, mainly because am asking  about the interpretations of the formulas...... not just the answer.I hope this is enough information so that this question can be its own independent entity.","X,Y, Z \text{ means: }\mu_{x}=2 \qquad \qquad \mu_{y}=-3 \qquad \qquad \mu_{z} = 4  \text{variances: }\sigma_{x}^{2}=3 \qquad \qquad \sigma_{y}^{2}=2 \qquad \qquad \sigma^{2}_{z}=8 \text{covariances: }\text{cov}(X,Y) =1 \quad \quad \text{cov}(X,Z) = -2 \quad \quad \text{Cov}(Y,Z) = 3 U = X-2Y+4Z U V = 3X-Y-Z v(U)= \text{var}(a_x+b_y+c_z) =a^2\cdot \text{var}(x) +b^2\cdot \text{var}(Y)+c^2 \cdot \text{var}(z) + 2ab \cdot cov(x,y) +2ac\cdot \text{cov}(x,z)+2abc \cdot \text{cov}(Y,Z) \text{cov}(u,v) = \text{cov}(a_1+b_1+c_1,a_2+b_2+c_2)=(a_{1})(a_{2})\text{var}(x)+(b_{1})(b_{2})\text{var}(Y)+(c_{1})(c_{2})\text{var}(Z)+\left[ (a_{1})(b_{2})+(b_{1})(a_{2}) \right] \cdot \text{cov}(X,Y)+\left[ (a_{1})(c_{2})+(c_{1})(a_{2}) \right] \cdot \text{cov}(X,Z)+ \left[ (b_{1})(c_{2})+(c_{1})(b_{2}) \right] \cdot \text{cov}(Y,Z)","['statistics', 'covariance', 'variance']"
31,How to find normal distribution that has a quadratic?,How to find normal distribution that has a quadratic?,,Let $X$ be a normal random variable with mean 1 and variance 4. Find $P(X^2 − 2X ≤ 8)$. (Answer key .86) My attempt  $$P(X^2-2X\le 8)=P((X+2)(X-4)\le 0)$$ and this is where I am lost.  I did the following$$P(X\le -2)+P(X\le 4)=1$$ and noticed that answer is just $1$   by looking at the mean.  So I am doing something wrong.,Let $X$ be a normal random variable with mean 1 and variance 4. Find $P(X^2 − 2X ≤ 8)$. (Answer key .86) My attempt  $$P(X^2-2X\le 8)=P((X+2)(X-4)\le 0)$$ and this is where I am lost.  I did the following$$P(X\le -2)+P(X\le 4)=1$$ and noticed that answer is just $1$   by looking at the mean.  So I am doing something wrong.,,"['probability', 'statistics', 'normal-distribution']"
32,Picking cards sequentially vs consecutively,Picking cards sequentially vs consecutively,,"We have a pack of 6 cards over the table. Cards are: {A, A}, {B, B}, {C}, {D}. There are 3 players ( Papa , Pepe , Popo ) sat around the table. Cards are all upside down, so the players cannot see which card they are picking. Now we are presented with two scenarios: Case 1: each player picks one card at a time. That is, first Papa picks a card, then Pepe, then Popo, until there are no cards over the table (i.e. cards are being picked sequentially ). Case 2: each player picks their 2 cards directly, that is: Papa picks 2 cards, then Pepe, then Popo. (i.e. cards are being picked consecutively ). I am being asked if the probability that Papa has the two A's is the same (or not) for both cases. My knowledge on statistics and probability is not very advanced, but I'd say that the chance for Papa picking the two A's will not be the same, but I don't have a convincing argument in favour. Maybe someone could point me towards the right direction in solving this problem.","We have a pack of 6 cards over the table. Cards are: {A, A}, {B, B}, {C}, {D}. There are 3 players ( Papa , Pepe , Popo ) sat around the table. Cards are all upside down, so the players cannot see which card they are picking. Now we are presented with two scenarios: Case 1: each player picks one card at a time. That is, first Papa picks a card, then Pepe, then Popo, until there are no cards over the table (i.e. cards are being picked sequentially ). Case 2: each player picks their 2 cards directly, that is: Papa picks 2 cards, then Pepe, then Popo. (i.e. cards are being picked consecutively ). I am being asked if the probability that Papa has the two A's is the same (or not) for both cases. My knowledge on statistics and probability is not very advanced, but I'd say that the chance for Papa picking the two A's will not be the same, but I don't have a convincing argument in favour. Maybe someone could point me towards the right direction in solving this problem.",,"['probability', 'probability-theory', 'statistics', 'card-games']"
33,"let $f(x)=(3(x+x^2))/14$ and $x$ between $0$ and $2$ , zero otherwise be the pdf for a random variable $X$ ,Find the median and the mode?","let  and  between  and  , zero otherwise be the pdf for a random variable  ,Find the median and the mode?",f(x)=(3(x+x^2))/14 x 0 2 X,"let f(x)=(3(x+x^2))/14 and x between 0 and 2  , zero otherwise be the pdf for a random variable X Find the median and the mode ` Could you please help me Is it correct or not?","let f(x)=(3(x+x^2))/14 and x between 0 and 2  , zero otherwise be the pdf for a random variable X Find the median and the mode ` Could you please help me Is it correct or not?",,"['probability', 'statistics']"
34,Papoulis 4th Ed. 2-16 - A box contains n identical balls numbered 1 through n. Suppose k balls are drawn in succession.,Papoulis 4th Ed. 2-16 - A box contains n identical balls numbered 1 through n. Suppose k balls are drawn in succession.,,"I'm having trouble with the following problem: A box contains $n$ identical balls numbered $1$ through $n$ . Suppose $k$ balls are drawn in succession. (a) What is the probability  that $m$ is the largest number drawn? (b) What is the probability that the largest number drawn is less than or equal to $m$ ? The results are (according to the solutions manual), respectively: $$\frac{m-1 \choose k-1 }{n \choose k }$$ $$\frac{m \choose k }{n \choose k }$$ My understanding of the problem is as follows: There are $n$ balls in total, $k$ are drawn, so there are $k$ random numbers from $1$ to $n$ . I'm asked to find if the greatest number between all the ones that were drawn ( $k$ ), equals $m$ (for part a) and $\le m$ (for part b). I get that the binomial coefficient in the denominator are all the different ways to arrange in a group of $k$ the $n$ balls. But I have no clue on how to find the numerator. Thanks in advance.","I'm having trouble with the following problem: A box contains identical balls numbered through . Suppose balls are drawn in succession. (a) What is the probability  that is the largest number drawn? (b) What is the probability that the largest number drawn is less than or equal to ? The results are (according to the solutions manual), respectively: My understanding of the problem is as follows: There are balls in total, are drawn, so there are random numbers from to . I'm asked to find if the greatest number between all the ones that were drawn ( ), equals (for part a) and (for part b). I get that the binomial coefficient in the denominator are all the different ways to arrange in a group of the balls. But I have no clue on how to find the numerator. Thanks in advance.",n 1 n k m m \frac{m-1 \choose k-1 }{n \choose k } \frac{m \choose k }{n \choose k } n k k 1 n k m \le m k n,"['probability', 'combinatorics', 'statistics']"
35,Statistics-relationships between gamma and exponential distribution,Statistics-relationships between gamma and exponential distribution,,"Just want to clarify whether the following is correct: If gamma(a,b) ,then exp(a/b)? where a,b are parameters for gamma  and a/b is the parameter for exp for example, gamma(1,2)=exp(1/2) Is this true for every a,b>0? Thank you!","Just want to clarify whether the following is correct: If gamma(a,b) ,then exp(a/b)? where a,b are parameters for gamma  and a/b is the parameter for exp for example, gamma(1,2)=exp(1/2) Is this true for every a,b>0? Thank you!",,"['statistics', 'probability-distributions']"
36,Moment generating function of a constant,Moment generating function of a constant,,"This might be trivial, but can you elaborate why moment generating function for a constant $c$ is $e^{cX}$, where $X$ is a random variable.","This might be trivial, but can you elaborate why moment generating function for a constant $c$ is $e^{cX}$, where $X$ is a random variable.",,"['probability-theory', 'statistics', 'moment-generating-functions', 'constants']"
37,"In a Sample of 10 telephones, 4 are defective. If 2 are selected at random and tested, what is the probability that all will be non-defective?","In a Sample of 10 telephones, 4 are defective. If 2 are selected at random and tested, what is the probability that all will be non-defective?",,"This problem is dependent because it matters which one you choose, So i don't think we can do the multiplication thing in this one. Probability of ( non defective ) = 6/10 What does the question mean when it says all will be non-defective? is ""all"" the 2 randomly chosen telephone? How would i do this problem? 2 is chosen randomly and 6 is non defective, I just thought of doing 2/6 cause 2 was the chosen and 6 total are non-defective. But if i wanted to find the number of non defective i would just do 6/10? I feel like I don't understand this question","This problem is dependent because it matters which one you choose, So i don't think we can do the multiplication thing in this one. Probability of ( non defective ) = 6/10 What does the question mean when it says all will be non-defective? is ""all"" the 2 randomly chosen telephone? How would i do this problem? 2 is chosen randomly and 6 is non defective, I just thought of doing 2/6 cause 2 was the chosen and 6 total are non-defective. But if i wanted to find the number of non defective i would just do 6/10? I feel like I don't understand this question",,"['probability', 'statistics']"
38,Prove that the average of iid Gaussian random variables is Gaussian,Prove that the average of iid Gaussian random variables is Gaussian,,"Given $x_1, \ldots, x_N$, independent and all distributed as a   Gaussian with mean $\mu$ and variance $\sigma^2$. Then, the average   $$\bar{x} = \frac{1}{N}\sum_{i=1}^Nx_i$$ is distributed as a Gaussian   with mean $\mu$ and variance $\frac{\sigma^2}{N}.$ This is a very well-known result. Anyway, I'm looking around to find a proof for this and I'm not having luck.","Given $x_1, \ldots, x_N$, independent and all distributed as a   Gaussian with mean $\mu$ and variance $\sigma^2$. Then, the average   $$\bar{x} = \frac{1}{N}\sum_{i=1}^Nx_i$$ is distributed as a Gaussian   with mean $\mu$ and variance $\frac{\sigma^2}{N}.$ This is a very well-known result. Anyway, I'm looking around to find a proof for this and I'm not having luck.",,"['probability', 'statistics']"
39,Probability of at least one event,Probability of at least one event,,"There are 2 independent events, probability that Exam A is a success is 0.4. Probability that Exam B is a success is 0.7. What is the probability that at least one of these is a success. So I thought the way 'at-least- one of these is a success is 'a is a success' or 'b is a success' or 'a and b is a success'. But the answer is  'a is a success' or 'b is a success' - 'a and b is a success'. I am confused as to why this is.","There are 2 independent events, probability that Exam A is a success is 0.4. Probability that Exam B is a success is 0.7. What is the probability that at least one of these is a success. So I thought the way 'at-least- one of these is a success is 'a is a success' or 'b is a success' or 'a and b is a success'. But the answer is  'a is a success' or 'b is a success' - 'a and b is a success'. I am confused as to why this is.",,"['probability', 'statistics']"
40,Probability question involving infinite number of vertical chords in a 1 inch circle. [closed],Probability question involving infinite number of vertical chords in a 1 inch circle. [closed],,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question Infinite number of vertical chords drawn on a circle with a 1 inch radius. What is the probability that a randomly picked chord is shorter than the radius? The answer should be $1 - .5√ 3$ or $.134$ but I'm not sure how to approach this problem.,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question Infinite number of vertical chords drawn on a circle with a 1 inch radius. What is the probability that a randomly picked chord is shorter than the radius? The answer should be $1 - .5√ 3$ or $.134$ but I'm not sure how to approach this problem.,,"['probability', 'geometry', 'statistics']"
41,"Variance $= 0$, show that $X=\mu$ with probability one","Variance , show that  with probability one",= 0 X=\mu,"If the variance of $X$ is zero, show that $X=\mu$ with probability one. Using Chebychev's inequality that is, \begin{equation*} P(|X-\mu|\geq k\sigma)\leq\frac{1}{k^2}, \end{equation*} I just let $\sigma=0$,thus, $P(|X-\mu|\geq 0)$, as our absolute value is always greater than or equal to one, this probability equals one.. Does this look correct?","If the variance of $X$ is zero, show that $X=\mu$ with probability one. Using Chebychev's inequality that is, \begin{equation*} P(|X-\mu|\geq k\sigma)\leq\frac{1}{k^2}, \end{equation*} I just let $\sigma=0$,thus, $P(|X-\mu|\geq 0)$, as our absolute value is always greater than or equal to one, this probability equals one.. Does this look correct?",,"['probability', 'statistics', 'probability-theory', 'probability-distributions']"
42,statistics dice problem,statistics dice problem,,"If $5$ fair dice are thrown at the same time, how do you find the probability that there are three $1$'s and two $2$'s? The answer says its $5C2 \cdot (1/6)^5$ but I don't understand why.","If $5$ fair dice are thrown at the same time, how do you find the probability that there are three $1$'s and two $2$'s? The answer says its $5C2 \cdot (1/6)^5$ but I don't understand why.",,"['combinatorics', 'statistics']"
43,Gamblers ruin formula,Gamblers ruin formula,,"Hello , I have been reading about gamblers ruin and I found this formula can anyone confirm its accuracy ?   I assume they only bet one chip a time","Hello , I have been reading about gamblers ruin and I found this formula can anyone confirm its accuracy ?   I assume they only bet one chip a time",,"['probability', 'statistics', 'game-theory', 'gambling']"
44,"Show that the variance, $\mathbb E((x-\mathbb E(x))^2)$, can be written as $\mathbb E(x^2)-(\mathbb E(x))^2$","Show that the variance, , can be written as",\mathbb E((x-\mathbb E(x))^2) \mathbb E(x^2)-(\mathbb E(x))^2,"This question has been set in the Christmas work for the chemists at oxford uni and the hint that was given in the problem sheet was ""does $\mathbb E(x)$ depend on $x$?"". There is a derivation on Wikipedia: $$\mathbb E((x-\mathbb E(x))^2)$$ Expand the brackets (I get this bit) $$\mathbb E(x^2-2x\mathbb E(x)+\mathbb E(x)^2)$$ Use the fact the expectation value of the sum of the terms is the same as the sum of the expectation values of the terms: $$\mathbb E(x^2)-2\mathbb E(x\mathbb E(x))+\mathbb E(\mathbb E(x)^2)$$ This step isn't written explicitly in the derivation on Wikipedia but it must be the case. Now the step that I don't understand: $$\mathbb E(x^2)-2\mathbb E(x)\mathbb E(x)+\mathbb E(\mathbb E(x)^2)$$ Why does the term in the middle not change to $2\mathbb E(x)\mathbb E(\mathbb E(x))$? Then collect the terms:$$\mathbb E(x^2)-\mathbb E(x)^2$$ Also, this doesn't seem to utilise the hint given by my lecturer. Is there a better way of doing it?","This question has been set in the Christmas work for the chemists at oxford uni and the hint that was given in the problem sheet was ""does $\mathbb E(x)$ depend on $x$?"". There is a derivation on Wikipedia: $$\mathbb E((x-\mathbb E(x))^2)$$ Expand the brackets (I get this bit) $$\mathbb E(x^2-2x\mathbb E(x)+\mathbb E(x)^2)$$ Use the fact the expectation value of the sum of the terms is the same as the sum of the expectation values of the terms: $$\mathbb E(x^2)-2\mathbb E(x\mathbb E(x))+\mathbb E(\mathbb E(x)^2)$$ This step isn't written explicitly in the derivation on Wikipedia but it must be the case. Now the step that I don't understand: $$\mathbb E(x^2)-2\mathbb E(x)\mathbb E(x)+\mathbb E(\mathbb E(x)^2)$$ Why does the term in the middle not change to $2\mathbb E(x)\mathbb E(\mathbb E(x))$? Then collect the terms:$$\mathbb E(x^2)-\mathbb E(x)^2$$ Also, this doesn't seem to utilise the hint given by my lecturer. Is there a better way of doing it?",,"['statistics', 'expectation']"
45,Conditional probability: At least 3 kings given there are at least 2 kings in the hand of 13.,Conditional probability: At least 3 kings given there are at least 2 kings in the hand of 13.,,"My first ""conditional probability"" problem. Sorry for all the questions. My instructor doesn't make sense to the class. A hand of 13 cards is to be dealt at random and without any replacement from an ordinary deck of playing cards. Find the conditional probability that there are at least three kings in the hand given that the hand contains at least two kings. A deep, good, and thorough explanation would be tremendously appreciated. Thanks","My first ""conditional probability"" problem. Sorry for all the questions. My instructor doesn't make sense to the class. A hand of 13 cards is to be dealt at random and without any replacement from an ordinary deck of playing cards. Find the conditional probability that there are at least three kings in the hand given that the hand contains at least two kings. A deep, good, and thorough explanation would be tremendously appreciated. Thanks",,"['probability', 'combinatorics', 'statistics']"
46,Expected value of n trials where probability of an event occuring is 1/n,Expected value of n trials where probability of an event occuring is 1/n,,"Say the probability of an event occurring is 1/1000, and there are 1000 trials. What's the expected number of events that occur? I got to an answer in a quick script by doing the above 100,000 times and averaging the results. I got 0.99895, which seems like it makes sense. How would I use math to get right to this answer? The only thing I can think of to calculate is the probability that an event never occurs, which would be 0.999^1000, but I am stuck there.","Say the probability of an event occurring is 1/1000, and there are 1000 trials. What's the expected number of events that occur? I got to an answer in a quick script by doing the above 100,000 times and averaging the results. I got 0.99895, which seems like it makes sense. How would I use math to get right to this answer? The only thing I can think of to calculate is the probability that an event never occurs, which would be 0.999^1000, but I am stuck there.",,"['probability', 'statistics']"
47,Uniformly Distributed Random Variables Minimum,Uniformly Distributed Random Variables Minimum,,"Let A,B,C,D,E be independent random variables, each of which is uniformly distributed in the interval [0,18].  Let X=min{A,B,C,D,E}. What is the expected value of X? Enter your answer as a decimal. I know that I need to be attempting some work,but any direction would be greatly appreciated!","Let A,B,C,D,E be independent random variables, each of which is uniformly distributed in the interval [0,18].  Let X=min{A,B,C,D,E}. What is the expected value of X? Enter your answer as a decimal. I know that I need to be attempting some work,but any direction would be greatly appreciated!",,['statistics']
48,Odds of two dice roll,Odds of two dice roll,,"If we roll two dice, what are the odds that we roll one five OR the sum of the rolled dice equals an odd number? The odds of rolling one five from two dice rolls is $\frac{1}{36}$.  The odds of rolling an odd number from the sum of two rolls requires that we roll one even number from one die and an odd number from another die.  The odds of this happening are $\frac{1}{2}$. Therefore, the odds of either even occuring should be: $\frac{1}{36} + \frac{1}{2} = \frac{19}{36}$ However, this is incorrect.  Apparently the answer is $\frac{23}{36}$.  I do not understand why.  I wrote a python script to evaluate my odds: import random  def rolldice(count):     return [random.randint(1, 6) for i in range(count)]  def compute_odd(dice):     return sum(dice) % 2 == 1  def has_num_only(dice, num):     return dice.count(num) == 1  g_iEvents = 0 g_iSimulations = 8888  for i in range(g_iSimulations):     dice = rolldice(2)     if compute_odd(dice) or has_num_only(dice, 5):         g_iEvents += 1  print( float(g_iEvents) / float(g_iSimulations) ) print( g_iEvents ) print( g_iSimulations ) After several runs, I keep getting the answer of 61% - Therefore this is showing that both answers are incorrect.","If we roll two dice, what are the odds that we roll one five OR the sum of the rolled dice equals an odd number? The odds of rolling one five from two dice rolls is $\frac{1}{36}$.  The odds of rolling an odd number from the sum of two rolls requires that we roll one even number from one die and an odd number from another die.  The odds of this happening are $\frac{1}{2}$. Therefore, the odds of either even occuring should be: $\frac{1}{36} + \frac{1}{2} = \frac{19}{36}$ However, this is incorrect.  Apparently the answer is $\frac{23}{36}$.  I do not understand why.  I wrote a python script to evaluate my odds: import random  def rolldice(count):     return [random.randint(1, 6) for i in range(count)]  def compute_odd(dice):     return sum(dice) % 2 == 1  def has_num_only(dice, num):     return dice.count(num) == 1  g_iEvents = 0 g_iSimulations = 8888  for i in range(g_iSimulations):     dice = rolldice(2)     if compute_odd(dice) or has_num_only(dice, 5):         g_iEvents += 1  print( float(g_iEvents) / float(g_iSimulations) ) print( g_iEvents ) print( g_iSimulations ) After several runs, I keep getting the answer of 61% - Therefore this is showing that both answers are incorrect.",,"['probability', 'statistics', 'dice']"
49,Wrong result from LLR using Dunning Entropy method,Wrong result from LLR using Dunning Entropy method,,"I'm trying to use Dunning's method of calculating LLR to compare word instances between two fulltext indexes.  His method uses entropy as part of the calculation. Dunning's blog post: http://tdunning.blogspot.com/2008/03/surprise-and-coincidence.html But, although I've implemented it in both Excel and Java and they give the same answers, I believe the answers are wrong. Two reasons I believe my results are wrong: 1: They don't agree with this online calculator (which uses a different formula): http://ucrel.lancs.ac.uk/llwizard.html 2: They are always negative; that is more disturbing. Link to my faulty XLS sheet: (hope this is OK) https://www.dropbox.com/s/bnzmk7ttf4mv23k/entropy-and-LLR-suspect-gist.xlsx Some theories I have: 1: Maybe my contingency table is setup wrong?  Dunning talks about an abstract CT, but he doesn't specifically say how to fill it out with term frequencies counts.  For example, in my table in cell 1,1 I put the number of times the word ""spam"" occurs in corpus A, whereas Dunning says ""Event A and B together"".  So, when you have term counts, maybe there's some step to convert those counts into a proper CT? 2: Maybe some misunderstanding about the denominators that I'm using when calculating probability.  In Steps 2, 3 and 4 I'm always dividing by the CT grandTotal, maybe that's wrong? 3: Maybe my entropy calculation is not the form that Dunning had in mind, perhaps there's some scaling I'm adding or not adding.  I found this page http://mail-archives.apache.org/mod_mbox/mahout-dev/201009.mbox/% [email protected] %3E where Dunning replied to a question and mentions ""un-normalized entropy"".  But I didn't follow the syntax and conversation well enough to related it back to what I was doing.","I'm trying to use Dunning's method of calculating LLR to compare word instances between two fulltext indexes.  His method uses entropy as part of the calculation. Dunning's blog post: http://tdunning.blogspot.com/2008/03/surprise-and-coincidence.html But, although I've implemented it in both Excel and Java and they give the same answers, I believe the answers are wrong. Two reasons I believe my results are wrong: 1: They don't agree with this online calculator (which uses a different formula): http://ucrel.lancs.ac.uk/llwizard.html 2: They are always negative; that is more disturbing. Link to my faulty XLS sheet: (hope this is OK) https://www.dropbox.com/s/bnzmk7ttf4mv23k/entropy-and-LLR-suspect-gist.xlsx Some theories I have: 1: Maybe my contingency table is setup wrong?  Dunning talks about an abstract CT, but he doesn't specifically say how to fill it out with term frequencies counts.  For example, in my table in cell 1,1 I put the number of times the word ""spam"" occurs in corpus A, whereas Dunning says ""Event A and B together"".  So, when you have term counts, maybe there's some step to convert those counts into a proper CT? 2: Maybe some misunderstanding about the denominators that I'm using when calculating probability.  In Steps 2, 3 and 4 I'm always dividing by the CT grandTotal, maybe that's wrong? 3: Maybe my entropy calculation is not the form that Dunning had in mind, perhaps there's some scaling I'm adding or not adding.  I found this page http://mail-archives.apache.org/mod_mbox/mahout-dev/201009.mbox/% [email protected] %3E where Dunning replied to a question and mentions ""un-normalized entropy"".  But I didn't follow the syntax and conversation well enough to related it back to what I was doing.",,"['probability', 'statistics', 'entropy']"
50,Letter-Sending probability problem,Letter-Sending probability problem,,"Here is another question from the book of V. Rohatgi and A. Saleh. I would like to ask help again. Here it goes: Consider a town with $N$ people. A person sends two letters to two separate people, each of whom is asked to repeat the procedure. Thus for each letter received, two letters are sent out to separate persons chosen at random (irrespective of what happened in the past). What is the probability that in the first $n$ stages the person who started the chain letter game will not receive a letter? I am thinking of solving this through complementation, i.e. what I have so far is that for $n=1$, $$P\{\text{the person who started the chain letter game will receive a letter}\}=0$$ for $n=2$,  $$P\{\text{the person who started the chain letter game will receive a letter from the 1st sender}\}$$  $$=P\{\text{the person who started the chain letter game will receive a letter from the 2nd sender}\}$$ $$=\frac{_{N-2}C_1}{_{N-1}C_2}$$ $$P\{\text{the person who started the chain letter game will receive letters from both 2nd senders}\}$$ $$=\left(\frac{_{N-2}C_1}{_{N-1}C_2}\right)^2$$ Then I would simply calculate the probability of the union of the elementary events, then continue for each $n$. Now, I find it too tedious and I am not so sure if this correct. Can anyone help please? Thanks.","Here is another question from the book of V. Rohatgi and A. Saleh. I would like to ask help again. Here it goes: Consider a town with $N$ people. A person sends two letters to two separate people, each of whom is asked to repeat the procedure. Thus for each letter received, two letters are sent out to separate persons chosen at random (irrespective of what happened in the past). What is the probability that in the first $n$ stages the person who started the chain letter game will not receive a letter? I am thinking of solving this through complementation, i.e. what I have so far is that for $n=1$, $$P\{\text{the person who started the chain letter game will receive a letter}\}=0$$ for $n=2$,  $$P\{\text{the person who started the chain letter game will receive a letter from the 1st sender}\}$$  $$=P\{\text{the person who started the chain letter game will receive a letter from the 2nd sender}\}$$ $$=\frac{_{N-2}C_1}{_{N-1}C_2}$$ $$P\{\text{the person who started the chain letter game will receive letters from both 2nd senders}\}$$ $$=\left(\frac{_{N-2}C_1}{_{N-1}C_2}\right)^2$$ Then I would simply calculate the probability of the union of the elementary events, then continue for each $n$. Now, I find it too tedious and I am not so sure if this correct. Can anyone help please? Thanks.",,"['probability', 'combinatorics', 'statistics']"
51,Flip a fair coin repeatedly. What is the probability that the first sequence of heads is exactly two heads long?,Flip a fair coin repeatedly. What is the probability that the first sequence of heads is exactly two heads long?,,Flip a fair coin repeatedly. What is the probability that the first sequence of heads is exactly two heads long? P(1st sequence = HH) = ?  I am assuming that this is a conditional probability question. Where we know that P(A|B)=[P(B|A)P(A)]\P(B). How can I use this tool to solve this question... that is if I even need to!,Flip a fair coin repeatedly. What is the probability that the first sequence of heads is exactly two heads long? P(1st sequence = HH) = ?  I am assuming that this is a conditional probability question. Where we know that P(A|B)=[P(B|A)P(A)]\P(B). How can I use this tool to solve this question... that is if I even need to!,,"['probability', 'statistics']"
52,Probability and sets,Probability and sets,,You are given two events $A$ and $B$ with $P(A \text { or } B) = 0.4$ and $P(A \text{ or } B’) = 0.8$. Determine $P(A)$. Note: $B’ = \text{ not }B = $ the complement of $B$. Answer: $0.2$ Not sure how to go about solving this problem. I tried $P(A \cup B) = P(A) + P(B) - P(A \text{ and } B) = 0.8$ and $P(A\cup B’) = P(A) + P(B’) - P(A \text{ and } B’) = 0.8$. But I am stuck here.,You are given two events $A$ and $B$ with $P(A \text { or } B) = 0.4$ and $P(A \text{ or } B’) = 0.8$. Determine $P(A)$. Note: $B’ = \text{ not }B = $ the complement of $B$. Answer: $0.2$ Not sure how to go about solving this problem. I tried $P(A \cup B) = P(A) + P(B) - P(A \text{ and } B) = 0.8$ and $P(A\cup B’) = P(A) + P(B’) - P(A \text{ and } B’) = 0.8$. But I am stuck here.,,['probability']
53,The national lottery,The national lottery,,"When playing the lottery you have to pick 6 numbers out of 45 possibilities. Since the order of the numbers don't matter, the number of possible combinations for the jackpot (and hence the 6 correct numbers) is given by: $ \dbinom{45}{6} = C^6_{45} = \frac{45!}{(45-6)!6!} = 8145060 $ Assuming the numbers are picked at random out of a uniform distribution the chance of winning the jackpot is given by: $P(win) = \frac{1}{8145060}\approx 0.0000123 \%$ Now I can figure two possible scenarios: You play every week with the same numbers hoping that one day you'll get the jackpot. You play every week with a different set of numbers hoping that one day you'll get the jackpot. Now I was wondering if there is a difference in the chance of winning between the two methods. I was thinking that in the first case the chances of winning are larger because if you stick to your number the alternatives might run out. While if you switch you don't have the other options that become eliminated. But then I started wondering, because in the Monty Hall paradox the chances become larger if you switch. For the first case I believe the chance of winning in the $n^{th}$ drawing is simply calculated as: $1-(1-P(win))^n$ But in the second case I wouldn't know how to succeed.","When playing the lottery you have to pick 6 numbers out of 45 possibilities. Since the order of the numbers don't matter, the number of possible combinations for the jackpot (and hence the 6 correct numbers) is given by: $ \dbinom{45}{6} = C^6_{45} = \frac{45!}{(45-6)!6!} = 8145060 $ Assuming the numbers are picked at random out of a uniform distribution the chance of winning the jackpot is given by: $P(win) = \frac{1}{8145060}\approx 0.0000123 \%$ Now I can figure two possible scenarios: You play every week with the same numbers hoping that one day you'll get the jackpot. You play every week with a different set of numbers hoping that one day you'll get the jackpot. Now I was wondering if there is a difference in the chance of winning between the two methods. I was thinking that in the first case the chances of winning are larger because if you stick to your number the alternatives might run out. While if you switch you don't have the other options that become eliminated. But then I started wondering, because in the Monty Hall paradox the chances become larger if you switch. For the first case I believe the chance of winning in the $n^{th}$ drawing is simply calculated as: $1-(1-P(win))^n$ But in the second case I wouldn't know how to succeed.",,"['probability', 'statistics', 'probability-theory', 'order-statistics']"
54,Basic statistics - Calculate distribution of winning,Basic statistics - Calculate distribution of winning,,I have a 100 sided fair dice with each side labelled 1 thru 100.  I win if the number rolled is 49 or higher (1% advantage). 1.  What is the probability of me winning exactly 500 rolls if the dice is rolled 1000 times? What is the general formula for calculating the probability of winning exactly W rolls if: P=probability of winning (52% if the above example) N=total number of rolls,I have a 100 sided fair dice with each side labelled 1 thru 100.  I win if the number rolled is 49 or higher (1% advantage). 1.  What is the probability of me winning exactly 500 rolls if the dice is rolled 1000 times? What is the general formula for calculating the probability of winning exactly W rolls if: P=probability of winning (52% if the above example) N=total number of rolls,,"['probability', 'statistics', 'probability-distributions']"
55,Number of length-five words,Number of length-five words,,"How many length-five words can be written using two A's, two T's and one E? Why is it not $\binom5 2 \times\binom 5 2 \times \binom 5 1$? Is it $ \binom 5 3 = 10$?","How many length-five words can be written using two A's, two T's and one E? Why is it not $\binom5 2 \times\binom 5 2 \times \binom 5 1$? Is it $ \binom 5 3 = 10$?",,"['probability', 'statistics', 'permutations', 'combinations']"
56,How to break apart this sum?,How to break apart this sum?,,"I have a summation I need to break apart but I can't figure it out http://www.collectionscanada.gc.ca/obj/s4/f2/dsk1/tape10/PQDD_0027/MQ50799.pdf $p.15$, right after line $(3.8)$ Going from the first $ns^{2}$ identity to the next line, where the author breaks apart the sum. I understand how he's trying to take out the $nth$ (jth) in this case) term, and use the equality identity above (for the distance between the sample mean and ""reduced sample"" mean) to get to the solution, but I can't figure out how that's a legal move, to break out the sum of two things squared into its components or the sequence of steps that was omitted... If you guys could help I would appreciate it, thanks","I have a summation I need to break apart but I can't figure it out http://www.collectionscanada.gc.ca/obj/s4/f2/dsk1/tape10/PQDD_0027/MQ50799.pdf $p.15$, right after line $(3.8)$ Going from the first $ns^{2}$ identity to the next line, where the author breaks apart the sum. I understand how he's trying to take out the $nth$ (jth) in this case) term, and use the equality identity above (for the distance between the sample mean and ""reduced sample"" mean) to get to the solution, but I can't figure out how that's a legal move, to break out the sum of two things squared into its components or the sequence of steps that was omitted... If you guys could help I would appreciate it, thanks",,['statistics']
57,How many chess games of a suspect does one have to analyze to have a reliable answer to the question whether the suspect cheats?,How many chess games of a suspect does one have to analyze to have a reliable answer to the question whether the suspect cheats?,,"This is a practical question that was asked on a correspondence chess site. The site struggles with chess engine users, who are considered cheaters. The method people use to determine whether someone is a cheater is this. Take a large enough number $x$ of games the person has played; analyze them with an engine (starting from the point where the game leaves online chess databases, whose use is allowed) and find the average match-up rates, for example: 1st choice - 50% match-up rate, 2nd choice - 65% match-up rate, 3rd choice - 80% match-up rate, where the first number is the ratio $$\frac{\text{the number of analyzed moves that are considered best by the engine}}{\text{the number of analyzed moves}},$$ the second number is the ratio $$\frac{\text{the number of analyzed moves that are considered best or second best by the engine}}{\text{the number of analyzed moves}},$$ and the third number is found in a similar way. The question is how large $x$ must be in order for these numbers to approximate the overall match-up rates well. I think the right way to ask this question more precisely is to fix a small error $\varepsilon$ and a small probability $p$, and ask how large $x$ must be for the probability of the actual error exceeding $\varepsilon$ to be less than $p$. I did do a course in statistics, but I remember almost nothing of it. Could you help me with this?","This is a practical question that was asked on a correspondence chess site. The site struggles with chess engine users, who are considered cheaters. The method people use to determine whether someone is a cheater is this. Take a large enough number $x$ of games the person has played; analyze them with an engine (starting from the point where the game leaves online chess databases, whose use is allowed) and find the average match-up rates, for example: 1st choice - 50% match-up rate, 2nd choice - 65% match-up rate, 3rd choice - 80% match-up rate, where the first number is the ratio $$\frac{\text{the number of analyzed moves that are considered best by the engine}}{\text{the number of analyzed moves}},$$ the second number is the ratio $$\frac{\text{the number of analyzed moves that are considered best or second best by the engine}}{\text{the number of analyzed moves}},$$ and the third number is found in a similar way. The question is how large $x$ must be in order for these numbers to approximate the overall match-up rates well. I think the right way to ask this question more precisely is to fix a small error $\varepsilon$ and a small probability $p$, and ask how large $x$ must be for the probability of the actual error exceeding $\varepsilon$ to be less than $p$. I did do a course in statistics, but I remember almost nothing of it. Could you help me with this?",,['statistics']
58,Does a Chi-Square random variable $\chi^2_1$ mean that only one normal random variable was taken?,Does a Chi-Square random variable  mean that only one normal random variable was taken?,\chi^2_1,"I'm trying to understand how Chi-Square variables work. So far, I know that a Chi-Square random variable, $\chi^2$, means that one random value has been taken from a normally distributed graph. Let's say it was the standard normal distribution. This means, $\chi^2$ has a high probability of being zero or near zero. Here's what I don't understand; How many degrees of freedom does $\chi^2$ have? If it only represents one random variable, then it has zero degrees of freedom, doesn't it? For example, if I take 5 random variables from a normal distribution, is it $\chi^2_1+\chi^2_1+\chi^2_1+\chi^2_1+\chi^2_1= \chi^2_5$ or is one somehow not counted?","I'm trying to understand how Chi-Square variables work. So far, I know that a Chi-Square random variable, $\chi^2$, means that one random value has been taken from a normally distributed graph. Let's say it was the standard normal distribution. This means, $\chi^2$ has a high probability of being zero or near zero. Here's what I don't understand; How many degrees of freedom does $\chi^2$ have? If it only represents one random variable, then it has zero degrees of freedom, doesn't it? For example, if I take 5 random variables from a normal distribution, is it $\chi^2_1+\chi^2_1+\chi^2_1+\chi^2_1+\chi^2_1= \chi^2_5$ or is one somehow not counted?",,"['statistics', 'normal-distribution']"
59,Average proportion for proportions with different denominators,Average proportion for proportions with different denominators,,"Say I have an experiment in which subjects are asked to respond to some stimulus. Their responses are transcribed and coded, first as ""Valid/Invalid"", and then for the valid responses, ""Correct/Incorrect"". I would like to compute the average of the proportion, $\frac{\text{number correct}}{\text{number valid}}$. There seem to be two different approaches to this. The first is I could  use the total proportion correct, $\frac{\sum{Correct_n}}{\sum{Valid_n}}$. The second would be to find the actual average of the proportions. I cannot straightforwardly compute the average proportion by adding the subject proportions and dividing by the number of subjects, as each subject had a different number of valid responses, and therefore each proportion has a different denominator. So, I end up the the following expression: $\frac{n*[(Valid_2 \times Valid_3 \times...Valid_n)Correct_1 +  (Valid_1 \times Valid_3 \times ... Valid_n)Correct_2... ]}{Valid_1 \times Valid_2 \times...Valid_n}$ While the two approaches seem similar, I can't figure out if they are going to end up being the same thing, and further I can't decided which one makes more sense, or even if either of them do.","Say I have an experiment in which subjects are asked to respond to some stimulus. Their responses are transcribed and coded, first as ""Valid/Invalid"", and then for the valid responses, ""Correct/Incorrect"". I would like to compute the average of the proportion, $\frac{\text{number correct}}{\text{number valid}}$. There seem to be two different approaches to this. The first is I could  use the total proportion correct, $\frac{\sum{Correct_n}}{\sum{Valid_n}}$. The second would be to find the actual average of the proportions. I cannot straightforwardly compute the average proportion by adding the subject proportions and dividing by the number of subjects, as each subject had a different number of valid responses, and therefore each proportion has a different denominator. So, I end up the the following expression: $\frac{n*[(Valid_2 \times Valid_3 \times...Valid_n)Correct_1 +  (Valid_1 \times Valid_3 \times ... Valid_n)Correct_2... ]}{Valid_1 \times Valid_2 \times...Valid_n}$ While the two approaches seem similar, I can't figure out if they are going to end up being the same thing, and further I can't decided which one makes more sense, or even if either of them do.",,"['statistics', 'average']"
60,Basic probability question,Basic probability question,,"Out of $120$ people, $5$ need to be selected for a team. What is the probability that you are selected for this team? $$P(A) = \frac{\binom51}{\binom{120}4}$$ Is this correct?","Out of $120$ people, $5$ need to be selected for a team. What is the probability that you are selected for this team? $$P(A) = \frac{\binom51}{\binom{120}4}$$ Is this correct?",,"['probability', 'statistics']"
61,Derivation of pmf from convolution,Derivation of pmf from convolution,,"Suppose that a discrete random variable (with finite support) $Y$ is given by $Y = X_1 - X_2$, where $X_1$ and $X_2$ are both discrete random variables with finite support and with the same probability mass function. Is it possible to determine the pmf of $X_1$ from the pmf of $Y$?","Suppose that a discrete random variable (with finite support) $Y$ is given by $Y = X_1 - X_2$, where $X_1$ and $X_2$ are both discrete random variables with finite support and with the same probability mass function. Is it possible to determine the pmf of $X_1$ from the pmf of $Y$?",,"['probability', 'statistics', 'convolution']"
62,Difference of sums or sum of differences this is the question,Difference of sums or sum of differences this is the question,,"If you have a bunch of paired measurements, let's say you measured peoples weights before and after holidays (AND you are assuming these are samples from an underlaying Gaussian distribution) what is the correct thing to do, to calculate the difference in weight between before and after holidays?! Difference of averages or average of differences?","If you have a bunch of paired measurements, let's say you measured peoples weights before and after holidays (AND you are assuming these are samples from an underlaying Gaussian distribution) what is the correct thing to do, to calculate the difference in weight between before and after holidays?! Difference of averages or average of differences?",,['statistics']
63,Simple Least-Squares Regression Question,Simple Least-Squares Regression Question,,"Given a set of 5 points (i.e. (1, 3), (2, 8) etc...), how can I get just the slope of the best fit line? I've been looking up least squares regression, but I'm rather statistics ignorant and don't understand most of the terminology and math behind it. Can anyone explain it a bit more simply?","Given a set of 5 points (i.e. (1, 3), (2, 8) etc...), how can I get just the slope of the best fit line? I've been looking up least squares regression, but I'm rather statistics ignorant and don't understand most of the terminology and math behind it. Can anyone explain it a bit more simply?",,"['statistics', 'regression']"
64,Calculating mu and sigma (μ and σ) of a normal random variable,Calculating mu and sigma (μ and σ) of a normal random variable,,"Let X be a normally distributed variable with unknown parameters  μ and σ (sigma). If we know that P (X ≥ 75) = 0.7291 and P (X ≥ 83) = 0.7764. With the information given Is it possible to determine the values for μ and σ ?. It is possible these odds?, Personally I see no sense to this probabilities? because I believe that the resulting values ​​of these probabilities are simply not possible. Am I right? P (X ≥ 75) = 0.7291 and P (X ≥ 83) = 0.7764","Let X be a normally distributed variable with unknown parameters  μ and σ (sigma). If we know that P (X ≥ 75) = 0.7291 and P (X ≥ 83) = 0.7764. With the information given Is it possible to determine the values for μ and σ ?. It is possible these odds?, Personally I see no sense to this probabilities? because I believe that the resulting values ​​of these probabilities are simply not possible. Am I right? P (X ≥ 75) = 0.7291 and P (X ≥ 83) = 0.7764",,"['probability', 'statistics', 'normal-distribution']"
65,Estimating sample size,Estimating sample size,,"I have $5$ different options: ($a,b,c,d,e$) out of which one is correct ($c$ in  this case). What should be the sample size (the number of people I should ask to answer) so that I can get $80\%$ confidence that the correct answer is chosen? Thanks","I have $5$ different options: ($a,b,c,d,e$) out of which one is correct ($c$ in  this case). What should be the sample size (the number of people I should ask to answer) so that I can get $80\%$ confidence that the correct answer is chosen? Thanks",,['statistics']
66,$\mathbb{E}(|X-Y|^3)$ Absolute expected value,Absolute expected value,\mathbb{E}(|X-Y|^3),"I need to find $\mathbb{E}(|X-Y|^3)$ where $X$ and $Y$ are independent distributions and are continuously uniform distributed on interval $[0,1]$.","I need to find $\mathbb{E}(|X-Y|^3)$ where $X$ and $Y$ are independent distributions and are continuously uniform distributed on interval $[0,1]$.",,"['probability', 'statistics']"
67,"can I get a bound on the probability of deviation, similar to Markov inequality?","can I get a bound on the probability of deviation, similar to Markov inequality?",,"I have two random variables $X$ and $Y$, both receiving values between 0 and 1. I know that $E[X - Y] \ge 0$. Can I get any inequality of the form: $P(X - Y \ge \delta) \le F(\delta,X,Y)$ where $F(\delta,X,Y)$ is a (reasonable) function of $\delta$ and $E[X-Y]$? Markov inequality would be good here, for example, by setting $F(\delta,X,Y) = E[X-Y]/\delta$. However, Markov inequality would require $X-Y \ge 0$, and I do not necessarily have that.","I have two random variables $X$ and $Y$, both receiving values between 0 and 1. I know that $E[X - Y] \ge 0$. Can I get any inequality of the form: $P(X - Y \ge \delta) \le F(\delta,X,Y)$ where $F(\delta,X,Y)$ is a (reasonable) function of $\delta$ and $E[X-Y]$? Markov inequality would be good here, for example, by setting $F(\delta,X,Y) = E[X-Y]/\delta$. However, Markov inequality would require $X-Y \ge 0$, and I do not necessarily have that.",,"['probability', 'statistics', 'inequality']"
68,What Is the Probability The Second Kid Is a Boy? [duplicate],What Is the Probability The Second Kid Is a Boy? [duplicate],,"This question already has answers here : In a family with two children, what are the chances, if one of the children is a girl, that both children are girls? (21 answers) Closed 4 months ago . Okay, so I was asked this question in an interview on a machine learning expert position. To be honest, the question itself (and the hint by the interviewer) seemed quite ill-phrased, which probably is the reason I ended up failing the interview, and he thought I must be super dumb. Here is the original question. You know your colleague has two kids, and also know one of them is a boy. What is the probability that the other one is a boy too? I was a bit puzzled, then he gave me a hint, by asking me to use Bayes' theorem, which I knew from high school $$\mathbb{P}(A\cap B)=\mathbb{P}(A|B)*\mathbb{P}(B)$$ I could see that given one kid is a boy corresponds to event $B$ , but could not really figure out the other quantities. To confuse matters, he gave me hints like when you see people with two kids, most of the times it is a boy and a girl, right? I could not argue with him, obviously, but I cannot reach any such conclusion based on my personal observation either. I tried to tell things like to calculate it we need empirical data like survey of all couples having two kids in the city/country etc. absent other information, the second child has the same probability of being a boy as the percentage of males in the country, assuming each kid's gender is independent But seems she had some assumption about the scenario (that meant the problem can be solved purely mathematically) that I failed to clarify. Upon further thought, there may be some biological concepts on how chromosomes interact to decide the gender of the second kid (and whether it is biased one way or another), but that is hardly fair to expect from an ML engineer. Is that where the answer lies? But the reason for this post is not to complain, but I am giving the context, just to ask what exactly am I missing in the question assuming it is meant to be a probability (and not biology) question.","This question already has answers here : In a family with two children, what are the chances, if one of the children is a girl, that both children are girls? (21 answers) Closed 4 months ago . Okay, so I was asked this question in an interview on a machine learning expert position. To be honest, the question itself (and the hint by the interviewer) seemed quite ill-phrased, which probably is the reason I ended up failing the interview, and he thought I must be super dumb. Here is the original question. You know your colleague has two kids, and also know one of them is a boy. What is the probability that the other one is a boy too? I was a bit puzzled, then he gave me a hint, by asking me to use Bayes' theorem, which I knew from high school I could see that given one kid is a boy corresponds to event , but could not really figure out the other quantities. To confuse matters, he gave me hints like when you see people with two kids, most of the times it is a boy and a girl, right? I could not argue with him, obviously, but I cannot reach any such conclusion based on my personal observation either. I tried to tell things like to calculate it we need empirical data like survey of all couples having two kids in the city/country etc. absent other information, the second child has the same probability of being a boy as the percentage of males in the country, assuming each kid's gender is independent But seems she had some assumption about the scenario (that meant the problem can be solved purely mathematically) that I failed to clarify. Upon further thought, there may be some biological concepts on how chromosomes interact to decide the gender of the second kid (and whether it is biased one way or another), but that is hardly fair to expect from an ML engineer. Is that where the answer lies? But the reason for this post is not to complain, but I am giving the context, just to ask what exactly am I missing in the question assuming it is meant to be a probability (and not biology) question.",\mathbb{P}(A\cap B)=\mathbb{P}(A|B)*\mathbb{P}(B) B,"['probability', 'probability-theory', 'statistics', 'bayesian']"
69,How to prove/check a random generator is a unifrom random distribution?,How to prove/check a random generator is a unifrom random distribution?,,"If I have access to a random integer generator that produces values in the range from $0$ to $N-1$ , how can I verify whether this generator truly produces a uniform random distribution? Simply analyzing the frequencies of generated numbers might not be sufficient, as the sequence could be cyclic, repeating in the order of $0, 1, \cdots, N-1$ , and then starting over from $0$ . I believe that examining only $M$ samples ( $M >> N$ is some fixed large number) for this purpose might be inadequate. Perhaps considering the time series of the generated numbers is crucial. Are there any advanced techniques available for accurately assessing this random number generator?","If I have access to a random integer generator that produces values in the range from to , how can I verify whether this generator truly produces a uniform random distribution? Simply analyzing the frequencies of generated numbers might not be sufficient, as the sequence could be cyclic, repeating in the order of , and then starting over from . I believe that examining only samples ( is some fixed large number) for this purpose might be inadequate. Perhaps considering the time series of the generated numbers is crucial. Are there any advanced techniques available for accurately assessing this random number generator?","0 N-1 0, 1, \cdots, N-1 0 M M >> N","['probability', 'statistics', 'algorithms']"
70,"Why is it ""obvious"" that the expected value of a continuous uniform distribution is (a+b)/2?","Why is it ""obvious"" that the expected value of a continuous uniform distribution is (a+b)/2?",,"For a continuous uniform random variable X with support on an interval [a,b], where a<b, one can always calculate the expected value, by integrating, to arrive at the value of (a+b)/2. However, since, I have been, for better or worse, ""corrupted"" by formal mathematics, I seem to no longer trust my intuition; even worse, I seem to question even the most ""obvious"" mathematical conclusions, given that I have been humbled one too many times whenever I have thought things to be ""obvious"". However, I keep running into the fact that it is ""obviously obvious"" that the expected value of a continuous uniform distribution is (a+b)/2 time and time again. Everyone I talk to, literally everyone, no matter their backgrounds, their occupations, can see this to be true. Even middle school children can see this to be true (when uniform distribution is explained in an informal way). I, however, can't. I seem to be completely oblivious to its obviousness. So, could someone please explain why it is ""obvious"" that the expected value of a continuous random variable on an interval [a,b], where a<b, is (a+b)/2? I understand the 'why' of it all when one formally defines a uniform distribution and integrates it. But I would be grateful if someone could provide an intuitive explanation for the same, preferably through an example.","For a continuous uniform random variable X with support on an interval [a,b], where a<b, one can always calculate the expected value, by integrating, to arrive at the value of (a+b)/2. However, since, I have been, for better or worse, ""corrupted"" by formal mathematics, I seem to no longer trust my intuition; even worse, I seem to question even the most ""obvious"" mathematical conclusions, given that I have been humbled one too many times whenever I have thought things to be ""obvious"". However, I keep running into the fact that it is ""obviously obvious"" that the expected value of a continuous uniform distribution is (a+b)/2 time and time again. Everyone I talk to, literally everyone, no matter their backgrounds, their occupations, can see this to be true. Even middle school children can see this to be true (when uniform distribution is explained in an informal way). I, however, can't. I seem to be completely oblivious to its obviousness. So, could someone please explain why it is ""obvious"" that the expected value of a continuous random variable on an interval [a,b], where a<b, is (a+b)/2? I understand the 'why' of it all when one formally defines a uniform distribution and integrates it. But I would be grateful if someone could provide an intuitive explanation for the same, preferably through an example.",,"['probability', 'statistics', 'expected-value', 'intuition', 'uniform-distribution']"
71,"Does uniform distribution on every square $[0,a]^2$ along diagonal imply uniform CDF on the entire $ [0,1]^2$",Does uniform distribution on every square  along diagonal imply uniform CDF on the entire,"[0,a]^2  [0,1]^2","Let $F: [0,1]^2\to R $ be a continuous cdf with uniform marginals, i.e., $F(x,1)=x$ and $F(1,y)=y$ . Suppose $F$ is symmetric, i.e., $F(x,y)=F(y,x)$ . Suppose we also know that $F(a,a)=a^2$ for all $a\in [0,1]$ . Can we conclude that $F$ is uniform distribution on $[0,1]^2$ ? Thanks.","Let be a continuous cdf with uniform marginals, i.e., and . Suppose is symmetric, i.e., . Suppose we also know that for all . Can we conclude that is uniform distribution on ? Thanks.","F: [0,1]^2\to R  F(x,1)=x F(1,y)=y F F(x,y)=F(y,x) F(a,a)=a^2 a\in [0,1] F [0,1]^2","['probability', 'statistics', 'probability-distributions', 'correlation', 'copula']"
72,Sum of a set of numbers squared is larger than $N$ times the mean squared [closed],Sum of a set of numbers squared is larger than  times the mean squared [closed],N,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I require the proof for the following $$\sum_{i = 1}^N x_i^2 \ge N \mu ^2 $$ where $x_i \in \mathbb R$ and $$\mu = \frac{1}{N} \sum_{i = 1}^N x_i$$ I can visually see how this is true (I imagine rectangles and squares), and would like to know if there's a common name for the result. If not, what's an easy way to show a proof for this? Thanks!","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I require the proof for the following where and I can visually see how this is true (I imagine rectangles and squares), and would like to know if there's a common name for the result. If not, what's an easy way to show a proof for this? Thanks!",\sum_{i = 1}^N x_i^2 \ge N \mu ^2  x_i \in \mathbb R \mu = \frac{1}{N} \sum_{i = 1}^N x_i,['statistics']
73,True or False: $P(A \cup B \cup C) \geq P(A | B \cup C) P(B) P(B \cup C)$,True or False:,P(A \cup B \cup C) \geq P(A | B \cup C) P(B) P(B \cup C),I am trying to determine if $$P(A \cup B \cup C) \geq P(A | B \cup C) P(B) P(B \cup C)$$ I tried $$\begin{align} P(A \cup B \cup C) \geq P(A | B \cup C) P(B) P(B \cup C) = P(A \cap (B \cup C) ) \cdot P(B) \end{align}$$ which is not leading to anywhere. I feel like I am missing something really obvious. Thank you.,I am trying to determine if I tried which is not leading to anywhere. I feel like I am missing something really obvious. Thank you.,"P(A \cup B \cup C) \geq P(A | B \cup C) P(B) P(B \cup C) \begin{align}
P(A \cup B \cup C) \geq P(A | B \cup C) P(B) P(B \cup C) = P(A \cap (B \cup C) ) \cdot P(B)
\end{align}","['probability', 'statistics']"
74,Is $(X_1+X_2+...+X_n)/n$ a random variable?,Is  a random variable?,(X_1+X_2+...+X_n)/n,"If $X_1,X_2,...,X_n$ are i.i.d random variables and are all discrete/continuous, then is $(X_1+X_2+...X_n)/n$ also a random variable? My attempt: For continuous type, I guess it is a random variable. Since $Z=X+Y$ is a random variable, we can view $X_1+X_2+...+X_n=nZ$ , then we can get the CDF of it. After we got the CDF, we can get the PDF. But I stuck on the CDF Step, because at here it is in higher dimensions, so we can not use the classical method in two dimensions to get the pdf. And I am also not sure about the discrete  case. Could someone explain more to me? Please give me the answer about two cases. ( $X_1,X_2,...,X_n$ are all discrete and $X_1,X_2,...,X_n$ are all continuous) Moreover, if we just apply the definition of random variables(transfer the event to a real number), then maybe in both cases. They are random variables. But here we are trying to transfer multiple events? Will the sample space(collection of all outcomes) change to higher dimensions?(like the case in joint pmf/pdf)","If are i.i.d random variables and are all discrete/continuous, then is also a random variable? My attempt: For continuous type, I guess it is a random variable. Since is a random variable, we can view , then we can get the CDF of it. After we got the CDF, we can get the PDF. But I stuck on the CDF Step, because at here it is in higher dimensions, so we can not use the classical method in two dimensions to get the pdf. And I am also not sure about the discrete  case. Could someone explain more to me? Please give me the answer about two cases. ( are all discrete and are all continuous) Moreover, if we just apply the definition of random variables(transfer the event to a real number), then maybe in both cases. They are random variables. But here we are trying to transfer multiple events? Will the sample space(collection of all outcomes) change to higher dimensions?(like the case in joint pmf/pdf)","X_1,X_2,...,X_n (X_1+X_2+...X_n)/n Z=X+Y X_1+X_2+...+X_n=nZ X_1,X_2,...,X_n X_1,X_2,...,X_n","['probability', 'probability-theory', 'statistics', 'random-variables', 'statistical-inference']"
75,Find the value $\hat{\beta}$ which minimizes $\sum_{i=1}^{4}|i||i- \beta|$ for $\beta \in \mathbb{R}$,Find the value  which minimizes  for,\hat{\beta} \sum_{i=1}^{4}|i||i- \beta| \beta \in \mathbb{R},"I am looking for the value $\hat{\beta}$ which satisfies the following condition: for each $\beta \in \mathbb{R}$ , $$\sum_{i=1}^{4}|i||i - \hat{\beta}| \leq \sum_{i=1}^{4}|i||i - \beta|\text{.}$$ Simulation confirms that $\hat{\beta} = 3$ works as a solution: f <- function(beta) {   i <- seq(1, 4, 1)   return(sum(abs(i)*abs(i- beta))) } optimize(f, interval = c(-10, 10)) $minimum [1] 3.000031 However, I'm not sure how to arrive at the answer of $3$ through theoretical machinery. I had attempted to handwave this by saying that it is sufficient, by term-by-term comparison, to compare $|i - \hat{\beta}|$ to $|i - \beta|$ , for which $\hat{\beta}$ should be the median of $\{1, 2, 3, 4\}$ , yielding an answer of $2.5$ (which is not supported by simulation).","I am looking for the value which satisfies the following condition: for each , Simulation confirms that works as a solution: f <- function(beta) {   i <- seq(1, 4, 1)   return(sum(abs(i)*abs(i- beta))) } optimize(f, interval = c(-10, 10)) $minimum [1] 3.000031 However, I'm not sure how to arrive at the answer of through theoretical machinery. I had attempted to handwave this by saying that it is sufficient, by term-by-term comparison, to compare to , for which should be the median of , yielding an answer of (which is not supported by simulation).","\hat{\beta} \beta \in \mathbb{R} \sum_{i=1}^{4}|i||i - \hat{\beta}| \leq \sum_{i=1}^{4}|i||i - \beta|\text{.} \hat{\beta} = 3 3 |i - \hat{\beta}| |i - \beta| \hat{\beta} \{1, 2, 3, 4\} 2.5","['algebra-precalculus', 'statistics', 'inequality', 'optimization']"
76,How to show that $P(X_{n1}+X_{n2}+\dots+X_{nn} \neq 0) \leq 1/n$?,How to show that ?,P(X_{n1}+X_{n2}+\dots+X_{nn} \neq 0) \leq 1/n,"The question is : Suppose that for each positive integer $n$ , $X_{n1},X_{n2},\dots,X_{nn}$ are independent, identically distributed random variables taking only the values $-\sqrt{n}$ , $0$ , $\sqrt{n}$ , such that \begin{equation*} P(X_{n1}= -\sqrt{n})= P(X_{n1}= \sqrt{n})=\frac{1}{2n^2}, P(X_{n1}=0)= 1- \frac{1}{n^2}. \end{equation*} i) Show that $P(X_{n1}+X_{n2}+\dots+X_{nn} \neq 0) \leq 1/n$ ? and use this to show that $X_{n1}+X_{n2}+\dots+X_{nn} \Rightarrow \delta_0 $ as $n \rightarrow \infty$ , where $\delta_0$ is degenerate probability measure defined by $\delta_0(\{0\})=1$ and $\delta_0(\{R-0\})=0$ . I tried to solve this by using the Lindeberg central limit theorem and I have shown that $E(X_{nk})=0$ , $\operatorname{Var}(X_{nk})=1/n$ and $\operatorname{Var}\left(X_{n1}+X_{n2}+\dots+X_{nn}\right)=1$ . Can anyone give me some hints to solve this question?","The question is : Suppose that for each positive integer , are independent, identically distributed random variables taking only the values , , , such that i) Show that ? and use this to show that as , where is degenerate probability measure defined by and . I tried to solve this by using the Lindeberg central limit theorem and I have shown that , and . Can anyone give me some hints to solve this question?","n X_{n1},X_{n2},\dots,X_{nn} -\sqrt{n} 0 \sqrt{n} \begin{equation*}
P(X_{n1}= -\sqrt{n})= P(X_{n1}= \sqrt{n})=\frac{1}{2n^2}, P(X_{n1}=0)= 1- \frac{1}{n^2}.
\end{equation*} P(X_{n1}+X_{n2}+\dots+X_{nn} \neq 0) \leq 1/n X_{n1}+X_{n2}+\dots+X_{nn} \Rightarrow \delta_0  n \rightarrow \infty \delta_0 \delta_0(\{0\})=1 \delta_0(\{R-0\})=0 E(X_{nk})=0 \operatorname{Var}(X_{nk})=1/n \operatorname{Var}\left(X_{n1}+X_{n2}+\dots+X_{nn}\right)=1","['probability', 'probability-theory', 'statistics', 'central-limit-theorem']"
77,Does this summation $ \sum_{x=a}^b \frac{p(1-p)^x}{(1-p)^a-(1-p)^b} $ equal 1?,Does this summation  equal 1?, \sum_{x=a}^b \frac{p(1-p)^x}{(1-p)^a-(1-p)^b} ,"I believe that I have found the PMF of a truncated geometric distribution, however I want to verify that this is a valid PMF by showing its sum is equal to 1. In the following PMF, the random variable x is bounded between a and b: $ P(X = x) = \frac{p(1-p)^x}{(1-p)^a-(1-p)^b}$ Given a, b, and p are all constants (a and b are both positive integers, and 0 < p < 1), is it possible to show that the summation is equal to 1: $\sum_{x=a}^b P(X=x) = 1$ ?","I believe that I have found the PMF of a truncated geometric distribution, however I want to verify that this is a valid PMF by showing its sum is equal to 1. In the following PMF, the random variable x is bounded between a and b: Given a, b, and p are all constants (a and b are both positive integers, and 0 < p < 1), is it possible to show that the summation is equal to 1: ?", P(X = x) = \frac{p(1-p)^x}{(1-p)^a-(1-p)^b} \sum_{x=a}^b P(X=x) = 1,"['probability', 'statistics', 'summation', 'random-variables']"
78,An Easy Looking Positive Semidefinite Matrices Implication,An Easy Looking Positive Semidefinite Matrices Implication,,"I'm reading the article ""Controlling the false discovery rate via knockoffs"" by Candes and Barber ( https://arxiv.org/abs/1404.5609 ) and faced the following problem that I couldn't handle. We have $X\in\mathbb{R}^{n\times p}$ with rank $p$ . We define $\Sigma:=X^TX$ We have a vector $s\in \mathbb{R}^p$ with non-negative entries and $diag(s)$ is defined as the $p\times p$ matrix with diagonal entries $s$ , zero otherwise. As I've understood from the beginning of the page 9 in the article, the authors claim that: \begin{equation} 2\Sigma \succeq \text{diag}(s) \iff 2\text{diag}(s)-\text{diag}(s)\Sigma^{-1}\text{diag}(s)\succeq 0 \end{equation} (From $X \succeq Y$ , I understand that $X-Y$ is positive semidefinite) I have shown the $2\text{diag}(s)-\text{diag}(s)\Sigma^{-1}\text{diag}(s)\succeq 0 \implies 2\Sigma \succeq \text{diag}(s)$ as follows: $\Sigma$ is symmetric positive definite. So, we can say, for any coordinate $i\in\{1,2,\dots,p\}$ , $\Sigma_{ii}(\Sigma^{-1})_{ii}\geq 1$ . What we have is equivalent to having $e_i^T(2\text{diag}(s)-\text{diag}(s)\Sigma^{-1}\text{diag}(s))e_i\geq0$ for every $i$ . So, we have $2s_i-s_i^2(\Sigma^{-1})_{ii}\geq 0$ . Using $\Sigma_{ii}(\Sigma^{-1})_{ii}\geq 1$ , we obtain $2\Sigma_{ii}-s_i\geq 0$ for all $i$ . So we can conclude $2\Sigma \succeq \text{diag}(s)$ . But I cannot do the same for the converse, and couldn't figure it out. I would be glad for any idea. It looks like there could be a counterexample. I feel like I may misunderstood the article.","I'm reading the article ""Controlling the false discovery rate via knockoffs"" by Candes and Barber ( https://arxiv.org/abs/1404.5609 ) and faced the following problem that I couldn't handle. We have with rank . We define We have a vector with non-negative entries and is defined as the matrix with diagonal entries , zero otherwise. As I've understood from the beginning of the page 9 in the article, the authors claim that: (From , I understand that is positive semidefinite) I have shown the as follows: is symmetric positive definite. So, we can say, for any coordinate , . What we have is equivalent to having for every . So, we have . Using , we obtain for all . So we can conclude . But I cannot do the same for the converse, and couldn't figure it out. I would be glad for any idea. It looks like there could be a counterexample. I feel like I may misunderstood the article.","X\in\mathbb{R}^{n\times p} p \Sigma:=X^TX s\in \mathbb{R}^p diag(s) p\times p s \begin{equation}
2\Sigma \succeq \text{diag}(s) \iff 2\text{diag}(s)-\text{diag}(s)\Sigma^{-1}\text{diag}(s)\succeq 0
\end{equation} X \succeq Y X-Y 2\text{diag}(s)-\text{diag}(s)\Sigma^{-1}\text{diag}(s)\succeq 0 \implies 2\Sigma \succeq \text{diag}(s) \Sigma i\in\{1,2,\dots,p\} \Sigma_{ii}(\Sigma^{-1})_{ii}\geq 1 e_i^T(2\text{diag}(s)-\text{diag}(s)\Sigma^{-1}\text{diag}(s))e_i\geq0 i 2s_i-s_i^2(\Sigma^{-1})_{ii}\geq 0 \Sigma_{ii}(\Sigma^{-1})_{ii}\geq 1 2\Sigma_{ii}-s_i\geq 0 i 2\Sigma \succeq \text{diag}(s)","['linear-algebra', 'matrices', 'statistics', 'matrix-decomposition', 'positive-definite']"
79,"X and Y are two independent standard gaussian variables, what is the Probability that X>100*Y?","X and Y are two independent standard gaussian variables, what is the Probability that X>100*Y?",,"We want to find $P(X> 100*Y)$ . My strategy was to first express $X>100*Y$ as $X-100*Y>0$ and express $Z=X-100*Y$ . $Z$ should be another normal random variable, yet not standard. My approach further was to standardise it so I can use 68–95–99.7 rule. However this approach is leading nowhere. Can anyone give any hints how I should approach it?","We want to find . My strategy was to first express as and express . should be another normal random variable, yet not standard. My approach further was to standardise it so I can use 68–95–99.7 rule. However this approach is leading nowhere. Can anyone give any hints how I should approach it?",P(X> 100*Y) X>100*Y X-100*Y>0 Z=X-100*Y Z,"['probability', 'statistics', 'random-variables']"
80,Why $E[X] = \sum_{i=1}^{\infty}(i\times Pr\{X=i\})$ is equal to $E[X] = \sum_{i=1}^{\infty}( Pr\{X \ge i \}))$ [duplicate],Why  is equal to  [duplicate],E[X] = \sum_{i=1}^{\infty}(i\times Pr\{X=i\}) E[X] = \sum_{i=1}^{\infty}( Pr\{X \ge i \})),"This question already has an answer here : Proof of expectation of discrete random variable (1 answer) Closed 2 years ago . If we have expectation $E[X] = \sum_{i=1}^{\infty}(i\times Pr\{X=i\})$ , where $i \in \mathbb{N}$ . Can you please explain how is it equal to $E[X] = \sum_{i=1}^{\infty}( Pr\{X \ge i \}))$ ? I see that $$ Pr\left[ X\ge i \right] =1-Pr\left[ X<i \right]  $$ $$ Pr\left[ X<i \right] =\frac{1}{N}\times \frac{2}{N}\cdots \frac{i-1}{N} $$ $$ Pr\left[ X\ge i \right] =1-\frac{1}{N}\times \frac{2}{N}\cdots \frac{i-1}{N} $$ $$ =1-\frac{\left( i-1 \right) !}{N} $$ $$ =\frac{N-\left( i-1 \right) !}{N} $$ $$ \sum_{i=1}^{\infty}{\left( i\times Pr\{X=i\} \right)}=i\times \frac{i}{N}\ne \sum_{i=1}^{\infty}{\left( Pr\{X\ge i\} \right)}=\frac{N-\left( i-1 \right) !}{N} $$ So, can you please correct if I am wrong? I am not able to get to formula that $E[X] = \sum_{i=1}^{\infty}(i\times Pr\{X=i\})$ , where $i \in \mathbb{N}$ is equal to $E[X] = \sum_{i=1}^{\infty}( Pr\{X \ge i \}))$ ?.","This question already has an answer here : Proof of expectation of discrete random variable (1 answer) Closed 2 years ago . If we have expectation , where . Can you please explain how is it equal to ? I see that So, can you please correct if I am wrong? I am not able to get to formula that , where is equal to ?.","E[X] = \sum_{i=1}^{\infty}(i\times Pr\{X=i\}) i \in \mathbb{N} E[X] = \sum_{i=1}^{\infty}( Pr\{X \ge i \})) 
Pr\left[ X\ge i \right] =1-Pr\left[ X<i \right] 
 
Pr\left[ X<i \right] =\frac{1}{N}\times \frac{2}{N}\cdots \frac{i-1}{N}
 
Pr\left[ X\ge i \right] =1-\frac{1}{N}\times \frac{2}{N}\cdots \frac{i-1}{N}
 
=1-\frac{\left( i-1 \right) !}{N}
 
=\frac{N-\left( i-1 \right) !}{N}
 
\sum_{i=1}^{\infty}{\left( i\times Pr\{X=i\} \right)}=i\times \frac{i}{N}\ne \sum_{i=1}^{\infty}{\left( Pr\{X\ge i\} \right)}=\frac{N-\left( i-1 \right) !}{N}
 E[X] = \sum_{i=1}^{\infty}(i\times Pr\{X=i\}) i \in \mathbb{N} E[X] = \sum_{i=1}^{\infty}( Pr\{X \ge i \}))","['probability', 'statistics']"
81,Calculating the distribution functions from two random variables X and Y,Calculating the distribution functions from two random variables X and Y,,"I'm currently trying to catch up on Stochastics for university and I'm really stuck on this one although I feel like its not as difficult as I may think: Let X, Y be random variables. The random variable X takes the values 1, 2 and 3 with probabilities P(X = 1) = 0.5, P(X = 2) = 0.3 and P(X = 3) = 0.2 and Y takes the values 1 and 2 with probabilities P(Y = 1) = 0.7 and P(Y = 2) = 0.3. Moreover, it is known that P(X = 1, Y = 1) = 0.35 and P(X = 3, Y = 1) = 0.2. a) Calculate the distribution functions of X and Y . b) Compute the remaining probabilities P(X = i, Y = j) for i ∈ {1, 2, 3} and j ∈ {1, 2}. Regarding a: I think I just need to see how its done once and than I'll probably get it but my University is not publishing the solutions or the script so my only options are googling and hoping to get it but I really dont :/ Regarding b: I get why P(X = 1, Y = 1) = 0.35 ( because 0.5*0.7 ) but why is P(X = 3, Y = 1) = 0.2  ( 0.2 * 0.7 ? )? I know its alot and I'm not expecting full on solutions but maybe someone can explain the basic approach to solving the exercise? I would greatly appreciate it :)","I'm currently trying to catch up on Stochastics for university and I'm really stuck on this one although I feel like its not as difficult as I may think: Let X, Y be random variables. The random variable X takes the values 1, 2 and 3 with probabilities P(X = 1) = 0.5, P(X = 2) = 0.3 and P(X = 3) = 0.2 and Y takes the values 1 and 2 with probabilities P(Y = 1) = 0.7 and P(Y = 2) = 0.3. Moreover, it is known that P(X = 1, Y = 1) = 0.35 and P(X = 3, Y = 1) = 0.2. a) Calculate the distribution functions of X and Y . b) Compute the remaining probabilities P(X = i, Y = j) for i ∈ {1, 2, 3} and j ∈ {1, 2}. Regarding a: I think I just need to see how its done once and than I'll probably get it but my University is not publishing the solutions or the script so my only options are googling and hoping to get it but I really dont :/ Regarding b: I get why P(X = 1, Y = 1) = 0.35 ( because 0.5*0.7 ) but why is P(X = 3, Y = 1) = 0.2  ( 0.2 * 0.7 ? )? I know its alot and I'm not expecting full on solutions but maybe someone can explain the basic approach to solving the exercise? I would greatly appreciate it :)",,"['probability', 'probability-theory', 'statistics', 'probability-distributions', 'stochastic-calculus']"
82,Finding $\mathbb{E}\left[\hat{\sigma}^2\right] = \frac{\sum_{i=1}^{n}{\left(\ln(x)-\mu\right)^2}}{n}$ where $X \sim \mathrm{LogNorm}$,Finding  where,\mathbb{E}\left[\hat{\sigma}^2\right] = \frac{\sum_{i=1}^{n}{\left(\ln(x)-\mu\right)^2}}{n} X \sim \mathrm{LogNorm},"I'm tasked with finding $\mathbb{E}\left(\hat{\sigma}^2\right)$ where $\hat{\sigma}^2 = \frac{\sum_{i=1}^{n}{\left(\ln(x)-\mu\right)^2}}{n}$ , $x \sim \mathrm{Lognorm}(\mu, \sigma^{2}$ ) . So far I get, $$\frac{1}{n}\sum_{i=1}^{n}{ \mathbb{E}\left((\ln(x)-\mu)^2\right)}$$ and can simplify to $$\frac{1}{n}\sum_{i=1}^{n}{\sigma^2} \\ \frac{n\sigma^2}{n} = \sigma^2$$ but I know this isn't right because the MLE estimate of $\sigma^2$ shouldn't be unbiased. Can anyone assist on what I did wrong?","I'm tasked with finding where , ) . So far I get, and can simplify to but I know this isn't right because the MLE estimate of shouldn't be unbiased. Can anyone assist on what I did wrong?","\mathbb{E}\left(\hat{\sigma}^2\right) \hat{\sigma}^2 = \frac{\sum_{i=1}^{n}{\left(\ln(x)-\mu\right)^2}}{n} x \sim \mathrm{Lognorm}(\mu, \sigma^{2} \frac{1}{n}\sum_{i=1}^{n}{ \mathbb{E}\left((\ln(x)-\mu)^2\right)} \frac{1}{n}\sum_{i=1}^{n}{\sigma^2} \\ \frac{n\sigma^2}{n} = \sigma^2 \sigma^2","['probability', 'probability-theory', 'statistics', 'probability-distributions', 'statistical-inference']"
83,When is $E(X-\mu)^2 \ne E(X^2)-\mu^2$?,When is ?,E(X-\mu)^2 \ne E(X^2)-\mu^2,"Consider the probability density function $$f(x)=\frac{x}{18}, \quad x \in [0,18]$$ Find $E(x)$ and $Var(x)$ . Solution : a.) \begin{align*} E(x) &=\int_{-\infty}^{\infty} x f(x) dx \\ &=\int_{0}^{18} x \left(\frac{x}{18}\right) dx \\ &=\frac{1}{18} \int_0^{18} x^2 dx \\ &=108 \end{align*} b.) Method 1: \begin{align*} Var(x) &=E(x-\mu)^2 \\ &=\int_{-\infty}^{\infty} (x-\mu) f(x) dx \\ &=\int_{0}^{18} (x-108)^2 \left(\frac{x}{18}\right) dx \\ &=83106 \end{align*} Method 2: \begin{align*} Var(x) &=E(x^2)-\mu^2 \\ &=\int_{-\infty}^{\infty} x^2 f(x) dx - (108)^2 \\ &=\int_{0}^{18} x^2 \left(\frac{x}{18}\right) dx - (108)^2\\ &=\frac{1}{18}\int_{0}^{18} x^3 dx - (108)^2\\ &=-10206 \end{align*} Clearly the second method is wrong, since a variance cannot be negative. But I cannot find the error. Can someone explain why the two methods yield different values? Is it not always the case that $E(X-\mu)^2 \ne E(X^2)-\mu^2$ ?","Consider the probability density function Find and . Solution : a.) b.) Method 1: Method 2: Clearly the second method is wrong, since a variance cannot be negative. But I cannot find the error. Can someone explain why the two methods yield different values? Is it not always the case that ?","f(x)=\frac{x}{18}, \quad x \in [0,18] E(x) Var(x) \begin{align*}
E(x)
&=\int_{-\infty}^{\infty} x f(x) dx \\
&=\int_{0}^{18} x \left(\frac{x}{18}\right) dx \\
&=\frac{1}{18} \int_0^{18} x^2 dx \\
&=108
\end{align*} \begin{align*}
Var(x)
&=E(x-\mu)^2 \\
&=\int_{-\infty}^{\infty} (x-\mu) f(x) dx \\
&=\int_{0}^{18} (x-108)^2 \left(\frac{x}{18}\right) dx \\
&=83106
\end{align*} \begin{align*}
Var(x)
&=E(x^2)-\mu^2 \\
&=\int_{-\infty}^{\infty} x^2 f(x) dx - (108)^2 \\
&=\int_{0}^{18} x^2 \left(\frac{x}{18}\right) dx - (108)^2\\
&=\frac{1}{18}\int_{0}^{18} x^3 dx - (108)^2\\
&=-10206
\end{align*} E(X-\mu)^2 \ne E(X^2)-\mu^2","['calculus', 'probability', 'integration', 'statistics', 'random-variables']"
84,Poisson distribution for arriving vehicles,Poisson distribution for arriving vehicles,,"I'm interested to model the number of Electric Vehicles (EVs) which arrive to a charging station during one day and their Time-of-Arrivals (ToA). I read that the number of EVs arriving at a charging station during a time interval is considered to follow a Poisson distribution, which uses a parameter which is called $\lambda$ , which is determined by $λ=r \cdot t$ , where: $r$ is the ""arrival rate""; $t$ is the ""time duration"". Example: $r = 1 \space EV / hour$ $t = 24 \space hours$ I know that the probability with which n = 20 EVs arrive at the charging station during 24 hours is: $$ P(n=20) = \frac{e^{-r t} \cdot (r t)^n}{n!} = 0.0623 = 6.23 \% $$ but it's not what I'm looking for, because I'd like to obtain: number of Electric Vehicles (EVs) which arrive to a charging station during one day; and their Time-of-Arrivals (ToA); or, alternatively (if what I request above is not possible), could also be sufficient to obtain: the number of EVs which arrives at each hour of the day. Which could be a way to reach my goals?","I'm interested to model the number of Electric Vehicles (EVs) which arrive to a charging station during one day and their Time-of-Arrivals (ToA). I read that the number of EVs arriving at a charging station during a time interval is considered to follow a Poisson distribution, which uses a parameter which is called , which is determined by , where: is the ""arrival rate""; is the ""time duration"". Example: I know that the probability with which n = 20 EVs arrive at the charging station during 24 hours is: but it's not what I'm looking for, because I'd like to obtain: number of Electric Vehicles (EVs) which arrive to a charging station during one day; and their Time-of-Arrivals (ToA); or, alternatively (if what I request above is not possible), could also be sufficient to obtain: the number of EVs which arrives at each hour of the day. Which could be a way to reach my goals?",\lambda λ=r \cdot t r t r = 1 \space EV / hour t = 24 \space hours  P(n=20) = \frac{e^{-r t} \cdot (r t)^n}{n!} = 0.0623 = 6.23 \% ,"['statistics', 'probability-distributions', 'random-variables', 'poisson-distribution', 'poisson-process']"
85,Calculating method of moments estimators for exponential random variables,Calculating method of moments estimators for exponential random variables,,"I'm trying to find the method of moment estimators for $\sigma$ and $\tau$ . I have the i.i.d. exponential random variables $X_1, \dots, X_n$ with the density functions $$f(x; \sigma, \tau)= \begin{cases}  \dfrac{1}{\sigma} e^{-(x - \tau)/\sigma} &\text{if}\, x\geq \tau\\       0 &\text{otherwise} \end{cases}$$ I know that $E(X)=\sigma + \tau$ and $E(X^2)=2\sigma^2+2\sigma \tau + \tau^2$ . Then, for method of moments, we set $\frac{1}{n}\sum_{i=1}^{n}X_i=\hat{\sigma}+\hat{\tau}$ and $\frac{1}{n}\sum_{i=1}^nX_i^2=2\hat{\sigma}^2+2\hat{\sigma}\hat{\tau}+\hat{\tau}^2$ . But I don't understand how to proceed from here to get the estimators for $\sigma$ and $\tau$ , and I don't really understand how $E(X)=\sigma + \tau$ and $E(X^2)=2\sigma^2+2\sigma \tau + \tau^2$ were calculated. For $E(X)=\sigma + \tau$ , I tried $$E(X) = \int_\tau^\infty \dfrac{1}{\sigma} e^{-(x - \tau)/\sigma} \ dx,$$ but this results in $1$ , so I don't think it's correct. So how do proceed from here to get the estimators for $\sigma$ and $\tau$ ? And how were $E(X)$ and $E(X^2)$ calculated?","I'm trying to find the method of moment estimators for and . I have the i.i.d. exponential random variables with the density functions I know that and . Then, for method of moments, we set and . But I don't understand how to proceed from here to get the estimators for and , and I don't really understand how and were calculated. For , I tried but this results in , so I don't think it's correct. So how do proceed from here to get the estimators for and ? And how were and calculated?","\sigma \tau X_1, \dots, X_n f(x; \sigma, \tau)=
\begin{cases}
 \dfrac{1}{\sigma} e^{-(x - \tau)/\sigma} &\text{if}\, x\geq \tau\\
      0 &\text{otherwise}
\end{cases} E(X)=\sigma + \tau E(X^2)=2\sigma^2+2\sigma \tau + \tau^2 \frac{1}{n}\sum_{i=1}^{n}X_i=\hat{\sigma}+\hat{\tau} \frac{1}{n}\sum_{i=1}^nX_i^2=2\hat{\sigma}^2+2\hat{\sigma}\hat{\tau}+\hat{\tau}^2 \sigma \tau E(X)=\sigma + \tau E(X^2)=2\sigma^2+2\sigma \tau + \tau^2 E(X)=\sigma + \tau E(X) = \int_\tau^\infty \dfrac{1}{\sigma} e^{-(x - \tau)/\sigma} \ dx, 1 \sigma \tau E(X) E(X^2)","['statistics', 'exponential-distribution']"
86,Expected value and variance from random variable,Expected value and variance from random variable,,"Given the following $Y_1 \sim \mathcal{N}(μ, σ^2 )$ and $Y_2=α+βY_1+U \;where \; Y_1 \;and \;U\;is\;independent\;and\;U∼\mathcal{N}(0,v^2)$ Let $μ=350$ and $σ^2 =12365$ How would i calculate the expected value and variance from $Y_2$ ? And how can i find the distribution of it?",Given the following and Let and How would i calculate the expected value and variance from ? And how can i find the distribution of it?,"Y_1 \sim \mathcal{N}(μ, σ^2 ) Y_2=α+βY_1+U \;where \; Y_1 \;and \;U\;is\;independent\;and\;U∼\mathcal{N}(0,v^2) μ=350 σ^2 =12365 Y_2","['probability', 'statistics', 'probability-distributions', 'random-variables', 'expected-value']"
87,"How to understand $\Gamma(\alpha,1)$ distribution when $\alpha$ is large?",How to understand  distribution when  is large?,"\Gamma(\alpha,1) \alpha","It is said for large $\alpha$ , $\operatorname{Gamma}(\alpha,1) \approx \operatorname{Normal}(\alpha,\alpha)$ . Can anyone tell me why? We know the mgf of a $\operatorname{Gamma}(\alpha, \beta)$ distribution is given by $$M_X(t)=\left(\frac{1}{1-\beta t}\right)^\alpha, \quad t<\frac{1}{\beta}.$$ Hence, given $\beta=1$ , we have mgf $\left(\frac{1}{1- t}\right)^\alpha$ , $t<1$ . Then $$M_X{\left(\frac{t-\alpha}{\sqrt{a}}\right)}=\left(\frac{1}{1- \frac{t-\alpha}{\sqrt{a}}}\right)^\alpha, \quad t<1.$$ How to get the limit when $\alpha\to\infty$ ?","It is said for large , . Can anyone tell me why? We know the mgf of a distribution is given by Hence, given , we have mgf , . Then How to get the limit when ?","\alpha \operatorname{Gamma}(\alpha,1) \approx \operatorname{Normal}(\alpha,\alpha) \operatorname{Gamma}(\alpha, \beta) M_X(t)=\left(\frac{1}{1-\beta t}\right)^\alpha, \quad t<\frac{1}{\beta}. \beta=1 \left(\frac{1}{1- t}\right)^\alpha t<1 M_X{\left(\frac{t-\alpha}{\sqrt{a}}\right)}=\left(\frac{1}{1- \frac{t-\alpha}{\sqrt{a}}}\right)^\alpha, \quad t<1. \alpha\to\infty","['probability-theory', 'statistics', 'probability-distributions']"
88,Using generalized logistic curve to create a mathematical model from data.,Using generalized logistic curve to create a mathematical model from data.,,"The first row is time and the second row is height of a plant. We need to use generalized logistic curve to model the behavior of the plant. The equation of the logistic curve is : $$N = \frac{N_*}{1+(N_*/N_0-1)e^{-a_0t}}$$ where $N_*$ is the maximum height of the plant or in other words, the supremum of the logistic function. $a_0$ is how fast the function increases. My question is the following: I know how to determine $N_*$ , we just make up a number that is above $251$ and less than $251 + (251 - 247) $ . But how to exactly determine $a_0$ . I am quite lost here. How I would do it is to calculate every $a_0$ for every time and height, with equation $a_0 = \frac{1}{t}\ln(\frac{N}{N_0})$ . In this equation $N$ would be the difference between heights, so let's say between $18$ and $33$ the $N$ for time equals $1$ would be $33 - 18$ . Is the process here of creating a table of $\sum_{k = 1}^{10}(10 - i)$ column elements of differences, so that we take multiple $N_0$ until we run out of data from the primary table in the picture (so until $N_0 = 9$ ). Then we can use a formula for $a_0 = \frac{\sum a_{0l}*t_l}{\sum t_l}$ where $l$ is the number of elements. Is my procedure for creating a logistic out of the data provided correct?","The first row is time and the second row is height of a plant. We need to use generalized logistic curve to model the behavior of the plant. The equation of the logistic curve is : where is the maximum height of the plant or in other words, the supremum of the logistic function. is how fast the function increases. My question is the following: I know how to determine , we just make up a number that is above and less than . But how to exactly determine . I am quite lost here. How I would do it is to calculate every for every time and height, with equation . In this equation would be the difference between heights, so let's say between and the for time equals would be . Is the process here of creating a table of column elements of differences, so that we take multiple until we run out of data from the primary table in the picture (so until ). Then we can use a formula for where is the number of elements. Is my procedure for creating a logistic out of the data provided correct?",N = \frac{N_*}{1+(N_*/N_0-1)e^{-a_0t}} N_* a_0 N_* 251 251 + (251 - 247)  a_0 a_0 a_0 = \frac{1}{t}\ln(\frac{N}{N_0}) N 18 33 N 1 33 - 18 \sum_{k = 1}^{10}(10 - i) N_0 N_0 = 9 a_0 = \frac{\sum a_{0l}*t_l}{\sum t_l} l,"['statistics', 'functions', 'mathematical-modeling', 'logistic-regression']"
89,A Negative binomial problem $P(X \ge 5)$ equals?,A Negative binomial problem  equals?,P(X \ge 5),"Consider a sequence of independent Bernoulli trials with the probability of success in each trial being $\dfrac{1}{3}$ . Let $X$ denote the number of trials required to get the second success. Then $P(X \ge 5)$ equals. $A=\dfrac{3}{7}$ $B=\dfrac{16}{27}$ $C=\dfrac{2}{3}$ $D=\dfrac{9}{13}$ This is a problem of negative binomial $r=2$ $P(X=x)= {x+r-1 \choose r-1}p^rq^x ;\ \ x=0,1,2..$ $P(X\ge5)=1-P(X < 5) \implies 1-(P(X = 0)+P(X = 1)+P(X = 2)+P(X = 3)+P(X = 4))$ This is very time consuming how do I save time on this problem. How do I utilize CDF of negative binomial distribution in this problem? ${\displaystyle k\mapsto 1-I_{p}(k+1,\,r),}$ the regularized incomplete beta function",Consider a sequence of independent Bernoulli trials with the probability of success in each trial being . Let denote the number of trials required to get the second success. Then equals. This is a problem of negative binomial This is very time consuming how do I save time on this problem. How do I utilize CDF of negative binomial distribution in this problem? the regularized incomplete beta function,"\dfrac{1}{3} X P(X \ge 5) A=\dfrac{3}{7} B=\dfrac{16}{27} C=\dfrac{2}{3} D=\dfrac{9}{13} r=2 P(X=x)= {x+r-1 \choose r-1}p^rq^x ;\ \ x=0,1,2.. P(X\ge5)=1-P(X < 5) \implies 1-(P(X = 0)+P(X = 1)+P(X = 2)+P(X = 3)+P(X = 4)) {\displaystyle k\mapsto 1-I_{p}(k+1,\,r),}","['probability', 'statistics', 'probability-distributions']"
90,"If $X$ and $Y$ are independent random variables, are $X^2$ and $Y^2$ also independent?","If  and  are independent random variables, are  and  also independent?",X Y X^2 Y^2,"If $X$ and $Y$ are independent random variables, are $X^2$ and $Y^2$ also independent? Can this be proven?","If and are independent random variables, are and also independent? Can this be proven?",X Y X^2 Y^2,['statistics']
91,$θ_2$ is better than $θ_1$ to estimate $μ$?,is better than  to estimate ?,θ_2 θ_1 μ,"We offer two estimators for the average concentration $μ$ of lead in the atmosphere of a region of Quebec where factories manufacturing dyes are located. The first estimator $θ_1$ has a bias equal to $0.2$ and a variance of $0.02$ . The second estimator $θ_2$ is unbiased and has a variance equal to $0.06$ . Which one is the best estimator? I think $θ_2$ is better than $θ_1$ to estimate $μ$ , but I am not sure. EDIT A PhD student in statistics explained to me that if $MSE(\theta_1) = MSE(\theta_2)$ , then we cannot conclude. In other words, $\theta_2$ is not preferred over $\theta_1$ or inversely. I am not sure about that.","We offer two estimators for the average concentration of lead in the atmosphere of a region of Quebec where factories manufacturing dyes are located. The first estimator has a bias equal to and a variance of . The second estimator is unbiased and has a variance equal to . Which one is the best estimator? I think is better than to estimate , but I am not sure. EDIT A PhD student in statistics explained to me that if , then we cannot conclude. In other words, is not preferred over or inversely. I am not sure about that.",μ θ_1 0.2 0.02 θ_2 0.06 θ_2 θ_1 μ MSE(\theta_1) = MSE(\theta_2) \theta_2 \theta_1,"['statistics', 'estimation', 'parameter-estimation']"
92,Multivariate optimal transport,Multivariate optimal transport,,"Optimal transport offers a way to find the optimal transport plan between 1 source distribution $P_r$ and 1 target distribution $P_\theta$ , where $\boldsymbol\Gamma$ is the optimal transport plan and $\boldsymbol D$ is the distance matrix. How or can the model instead be extended to the case where there are multiple source distributions , $P_r^1, P_r^2, \dots, P_r^k$ (or perhaps a weighted combination of them so that they collapse into a single distribution), from which we would like to move towards the target distribution $P_\theta$ ?","Optimal transport offers a way to find the optimal transport plan between 1 source distribution and 1 target distribution , where is the optimal transport plan and is the distance matrix. How or can the model instead be extended to the case where there are multiple source distributions , (or perhaps a weighted combination of them so that they collapse into a single distribution), from which we would like to move towards the target distribution ?","P_r P_\theta \boldsymbol\Gamma \boldsymbol D P_r^1, P_r^2, \dots, P_r^k P_\theta","['statistics', 'probability-distributions', 'optimization', 'linear-programming', 'optimal-transport']"
93,How to prove $\int_Xexp(-\frac12X'AX)dX=(2\pi)^\frac{n}2{\lvert{A}\rvert}^{-\frac12}$ [duplicate],How to prove  [duplicate],\int_Xexp(-\frac12X'AX)dX=(2\pi)^\frac{n}2{\lvert{A}\rvert}^{-\frac12},"This question already has answers here : If $A$ is positive definite, then $\int_{\mathbb{R}^n}\mathrm{e}^{-\langle Ax,x\rangle}\text{d}x=\left|\det\left({\pi}^{-1}A\right)\right|^{-1/2}$ (3 answers) Closed 3 years ago . I was studying a solution to a problem and this result was given without any explanation: If $X\sim{N}(0,I_n)$ & A is a symmetric positive definite matrix, $$ \int_X\exp\left(-\frac12X'AX\right)dX=(2\pi)^\frac{n}2{\lvert{A}\rvert}^{-\frac12} $$ How can I prove this proposition?","This question already has answers here : If $A$ is positive definite, then $\int_{\mathbb{R}^n}\mathrm{e}^{-\langle Ax,x\rangle}\text{d}x=\left|\det\left({\pi}^{-1}A\right)\right|^{-1/2}$ (3 answers) Closed 3 years ago . I was studying a solution to a problem and this result was given without any explanation: If & A is a symmetric positive definite matrix, How can I prove this proposition?","X\sim{N}(0,I_n) 
\int_X\exp\left(-\frac12X'AX\right)dX=(2\pi)^\frac{n}2{\lvert{A}\rvert}^{-\frac12}
","['linear-algebra', 'statistics']"
94,"MLE of $(\theta_1,\theta_2)$ in a piecewise PDF",MLE of  in a piecewise PDF,"(\theta_1,\theta_2)","I am trying to find the MLE of $\theta=(\theta_1,\theta_2)$ in a random sample $\{X\}_{i=1}^n$ with the following pdf $$f(x\mid\theta)= \begin{cases} (\theta_1+\theta_2)^{-1}\exp\left(\frac{-x}{\theta_1}\right) &,  x>0\\  (\theta_1+\theta_2)^{-1}\exp\left(\frac{x}{\theta_2}\right) &,  x\le0\\ \end{cases} $$ If I let $\bar{X}_1$ be the average of the $n_1$ values where $X_1>0$ and $\bar{X}_2$ the average of $n_2$ values where $X_i\le 0$ and $n_1+n_2=n$ Then the likelihood function is: $$L(\theta\mid  X)=\left(\frac 1 {\theta_1+\theta_2}\right)^n\exp\left(\frac{-n_1\bar{X}_1}{\theta_1}+\frac{n_2\bar{X}_2}{\theta_2}\right)$$ but I am having trouble maximizing this function.",I am trying to find the MLE of in a random sample with the following pdf If I let be the average of the values where and the average of values where and Then the likelihood function is: but I am having trouble maximizing this function.,"\theta=(\theta_1,\theta_2) \{X\}_{i=1}^n f(x\mid\theta)= \begin{cases}
(\theta_1+\theta_2)^{-1}\exp\left(\frac{-x}{\theta_1}\right) &,  x>0\\ 
(\theta_1+\theta_2)^{-1}\exp\left(\frac{x}{\theta_2}\right) &,  x\le0\\
\end{cases}
 \bar{X}_1 n_1 X_1>0 \bar{X}_2 n_2 X_i\le 0 n_1+n_2=n L(\theta\mid  X)=\left(\frac 1 {\theta_1+\theta_2}\right)^n\exp\left(\frac{-n_1\bar{X}_1}{\theta_1}+\frac{n_2\bar{X}_2}{\theta_2}\right)","['statistics', 'statistical-inference', 'maximum-likelihood', 'parameter-estimation']"
95,What sample size is needed to ensure a majority?,What sample size is needed to ensure a majority?,,"The results of a sample of voters showed that $55\%$ voted for a given candidate. It was determined that at a confidence level of $0.95$ that candidate would be the winner (i.e. would receive the majority of the votes). What sample size is needed to ensure the accuracy of that statement? Given the mean, standard deviation, and sample size, I can find a confidence interval using $Z_{1-\alpha/2}$ and without the standard deviation with $t_\alpha(n-1)$ . This question, however, doesn't seem to give the necessary information to use either approach. Perhaps the possible $5\%$ difference between $55\%$ and ""majority"" should be used in the calculation, but I don't know how to approach making the correct formulas.","The results of a sample of voters showed that voted for a given candidate. It was determined that at a confidence level of that candidate would be the winner (i.e. would receive the majority of the votes). What sample size is needed to ensure the accuracy of that statement? Given the mean, standard deviation, and sample size, I can find a confidence interval using and without the standard deviation with . This question, however, doesn't seem to give the necessary information to use either approach. Perhaps the possible difference between and ""majority"" should be used in the calculation, but I don't know how to approach making the correct formulas.",55\% 0.95 Z_{1-\alpha/2} t_\alpha(n-1) 5\% 55\%,"['statistics', 'sampling', 'confidence-interval']"
96,How can I mathematically compare two measurements to improve accuracy?,How can I mathematically compare two measurements to improve accuracy?,,I have two different techniques for measuring geolocation accuracy. I am asked to combine two different measurements in any way that might improve accuracy. Say I have an error of n1 using measurements A and an error of n2 using measurements B. Can I do better than both n1 and n2 by using both sets of information?  Any suggestions or techniques that could be implemented in Python or MATLAB are greatly appreciated.,I have two different techniques for measuring geolocation accuracy. I am asked to combine two different measurements in any way that might improve accuracy. Say I have an error of n1 using measurements A and an error of n2 using measurements B. Can I do better than both n1 and n2 by using both sets of information?  Any suggestions or techniques that could be implemented in Python or MATLAB are greatly appreciated.,,"['probability', 'statistics', 'random-variables', 'mathematical-modeling', 'data-analysis']"
97,"Let $X_1, X_2, X_3$ be i.i.d exponential random variables with mean $1$. What is $\operatorname{Pr}(X_1 < X_2 < X_3)$?",Let  be i.i.d exponential random variables with mean . What is ?,"X_1, X_2, X_3 1 \operatorname{Pr}(X_1 < X_2 < X_3)","Ive been working on this question and just want to know if i'm on the right track of if im completely off. The join pdf for the order statistic is $f_{x_{(1)}x_{(2)}x_{(3)}}(y_1,y_2,y_3)$ = $3!e^{-(y_1 +y_2 + y_3)} $ by integrating, i get the densities for $x_{(1)}$ and $x_{(2)}$ , and for $x_{(2)}$ and $x_{(3)}$ $f_{x_{(1)}x_{(2)}}(y_1,y_2)$ = $6(e^{-(y_1 + y_2)} - e^{-(y_1 + 2y_2)})$ $f_{x_{(2)}x_{(3)}}(y_1,y_2)$ = $6(e^{-(2y_2 + y_3)} - e^{-(y_2 + y_3)})$ We need to fine $Pr(X_1 < X_2 < X_3)$ $Pr(X_1 < X_2 < X_3)$ = $Pr(X_2 - X_3) - Pr(X_1 < X_2)$ where $Pr(X_2 < X_3)$ = $\int_{0}^\infty \int_{0}^{x_{3}} 6(e^{-(2y_2 + y_3)} - e^{-(y_2 + y_3)}) $ $Pr(X_1 < X_2)$ = $\int_{0}^\infty \int_{0}^{x_{2}} 6(e^{-(y_1 + y_2)} - e^{-(y_1 + 2y_2)}) $ Thus $Pr(X_1 < X_2 < X_3)$ = $\int_{0}^\infty \int_{0}^{x_{3}} 6(e^{-(2y_2 + y_3)} - e^{-(y_2 + y_3)}) $ - $\int_{0}^\infty \int_{0}^{x_{2}} 6(e^{-(y_1 + y_2)} - e^{-(y_1 + 2y_2)}) $ This is where im at so far. Is this correct?","Ive been working on this question and just want to know if i'm on the right track of if im completely off. The join pdf for the order statistic is = by integrating, i get the densities for and , and for and = = We need to fine = where = = Thus = - This is where im at so far. Is this correct?","f_{x_{(1)}x_{(2)}x_{(3)}}(y_1,y_2,y_3) 3!e^{-(y_1 +y_2 + y_3)}  x_{(1)} x_{(2)} x_{(2)} x_{(3)} f_{x_{(1)}x_{(2)}}(y_1,y_2) 6(e^{-(y_1 + y_2)} - e^{-(y_1 + 2y_2)}) f_{x_{(2)}x_{(3)}}(y_1,y_2) 6(e^{-(2y_2 + y_3)} - e^{-(y_2 + y_3)}) Pr(X_1 < X_2 < X_3) Pr(X_1 < X_2 < X_3) Pr(X_2 - X_3) - Pr(X_1 < X_2) Pr(X_2 < X_3) \int_{0}^\infty \int_{0}^{x_{3}} 6(e^{-(2y_2 + y_3)} - e^{-(y_2 + y_3)})  Pr(X_1 < X_2) \int_{0}^\infty \int_{0}^{x_{2}} 6(e^{-(y_1 + y_2)} - e^{-(y_1 + 2y_2)})  Pr(X_1 < X_2 < X_3) \int_{0}^\infty \int_{0}^{x_{3}} 6(e^{-(2y_2 + y_3)} - e^{-(y_2 + y_3)})  \int_{0}^\infty \int_{0}^{x_{2}} 6(e^{-(y_1 + y_2)} - e^{-(y_1 + 2y_2)}) ","['probability', 'statistics', 'probability-distributions', 'order-statistics']"
98,"If $Y = \sum_{i=1}^nX_i$, how do I express $Var(\frac{1}{n}Y)$ as a function of $Var(X)$?","If , how do I express  as a function of ?",Y = \sum_{i=1}^nX_i Var(\frac{1}{n}Y) Var(X),"To give more context, we have $X_i = 1$ if true, $0$ otherwise. The probability that the outcome will be true is 0.6. The sample is independent. This is my work so far, but I feel like it's wrong. $\DeclareMathOperator{\Var}{Var}$ $\Var(\frac{1}{n}Y) = E[(\frac{1}{n}\sum_{i = 1}^nX_i)^2] - E[\frac{1}{n}\sum_{i = 1}^nX_i]^2$ $ = E[\frac{1}{n}\sum_{i = 1}^nX_i\cdot\frac{1}{n}\sum_{i = 1}^nX_i] - (E[X])^2 = E[X^2] - E[X]^2 = \Var(X).$ Where am I going wrong? Thanks. EDIT: To get these equivilances, I'm using the linearity of the expected value and the fact that $E[AB] = E[A]E[B]$ .","To give more context, we have if true, otherwise. The probability that the outcome will be true is 0.6. The sample is independent. This is my work so far, but I feel like it's wrong. Where am I going wrong? Thanks. EDIT: To get these equivilances, I'm using the linearity of the expected value and the fact that .",X_i = 1 0 \DeclareMathOperator{\Var}{Var} \Var(\frac{1}{n}Y) = E[(\frac{1}{n}\sum_{i = 1}^nX_i)^2] - E[\frac{1}{n}\sum_{i = 1}^nX_i]^2  = E[\frac{1}{n}\sum_{i = 1}^nX_i\cdot\frac{1}{n}\sum_{i = 1}^nX_i] - (E[X])^2 = E[X^2] - E[X]^2 = \Var(X). E[AB] = E[A]E[B],"['probability', 'statistics', 'expected-value', 'variance']"
99,What is the expectation of number of words 'ab',What is the expectation of number of words 'ab',,"What is the expectation of number of words 'ab'  in random 20 length phrase that use letters from $\left \{ a,b \right \}$ ? There are $2^{20}$ words with length 20 over an alphabet with 2 letters...",What is the expectation of number of words 'ab'  in random 20 length phrase that use letters from ? There are words with length 20 over an alphabet with 2 letters...,"\left \{ a,b \right \} 2^{20}","['probability', 'statistics']"
