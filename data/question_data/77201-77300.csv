,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Calculating (the orbits under the action of) the group generated by two 1-parameter subgroups acting on a Euclidean space: is there a method?,Calculating (the orbits under the action of) the group generated by two 1-parameter subgroups acting on a Euclidean space: is there a method?,,"I have a 5-dimensional Euclidean space, $E$ , with an orthonormal basis ${\bf x}, {\bf y}_{1}, {\bf w}_{1}, {\bf y}_{2}, {\bf w}_{2}$ and two transformations: $$ A_{1} : -{\bf x} + {\bf y}_{1} \mapsto {\bf w}_{1}, \quad {\bf w}_{1} \mapsto {\bf x} - {\bf y}_{1}, \quad  {\bf y}_{2} \mapsto {\bf 0}, \quad {\bf w}_{2} \mapsto {\bf 0}  $$ and $$ A_{2} : -{\bf x} + {\bf y}_{2} \mapsto {\bf w}_{2}, \quad {\bf w}_{2} \mapsto {\bf x} - {\bf y}_{2}, \quad  {\bf y}_{1} \mapsto {\bf 0},\quad {\bf w}_{1} \mapsto {\bf 0}. $$ I am trying to calculate the orbit of a generic point in $E$ under the action of the group, $G_{A_{1}, A_{2}}$ , generated by the 1-parameter groups $t \mapsto e^{t A_{1}}, t \mapsto e^{t A_{2}}$ . Here are approaches I have tried and challenges encountered (which are almost surely from my lack of familiarity with the needed theory, so an answer like ""read X"" is a great answer). Calculate the Lie algebra generated by $A_{1}, A_{2}$ with the Lie bracket defined as the commutation: $[A_{1}, A_{2}] = A_{2} A_{1} - A_{1} A_{2}$ .  I am getting that the dimension is no lower than 6.  But even having a basis for the Lie algebra, is there a theory for determining the group $G_{A_{1}, A_{2}}$ ?  (The underlying field is the reals, not algebraically closed, so Dynkin diagrams would not apply.) Trying to consider, first, the two 1-parameter groups as if acting on two disjoint 3-D Euclidean spaces (then the orbit of a point is the torus $S^{1} \times S^{1}$ ), and then trying to calculate the topological factor space $(S^{1} \times S^{1}) / \sim$ by a suitable relation $\sim$ (which identifies two 1-dimensional subspaces from each 3-D space), but have had a hard time ""seeing"" the factor space. Anyway, if there is a theory, I'd appreciate some sources.  If I am missing something straightforward, being pointed toward it would be great.","I have a 5-dimensional Euclidean space, , with an orthonormal basis and two transformations: and I am trying to calculate the orbit of a generic point in under the action of the group, , generated by the 1-parameter groups . Here are approaches I have tried and challenges encountered (which are almost surely from my lack of familiarity with the needed theory, so an answer like ""read X"" is a great answer). Calculate the Lie algebra generated by with the Lie bracket defined as the commutation: .  I am getting that the dimension is no lower than 6.  But even having a basis for the Lie algebra, is there a theory for determining the group ?  (The underlying field is the reals, not algebraically closed, so Dynkin diagrams would not apply.) Trying to consider, first, the two 1-parameter groups as if acting on two disjoint 3-D Euclidean spaces (then the orbit of a point is the torus ), and then trying to calculate the topological factor space by a suitable relation (which identifies two 1-dimensional subspaces from each 3-D space), but have had a hard time ""seeing"" the factor space. Anyway, if there is a theory, I'd appreciate some sources.  If I am missing something straightforward, being pointed toward it would be great.","E {\bf x}, {\bf y}_{1}, {\bf w}_{1}, {\bf y}_{2}, {\bf w}_{2} 
A_{1} : -{\bf x} + {\bf y}_{1} \mapsto {\bf w}_{1}, \quad {\bf w}_{1} \mapsto {\bf x} - {\bf y}_{1}, \quad 
{\bf y}_{2} \mapsto {\bf 0}, \quad {\bf w}_{2} \mapsto {\bf 0} 
 
A_{2} : -{\bf x} + {\bf y}_{2} \mapsto {\bf w}_{2}, \quad {\bf w}_{2} \mapsto {\bf x} - {\bf y}_{2}, \quad 
{\bf y}_{1} \mapsto {\bf 0},\quad {\bf w}_{1} \mapsto {\bf 0}.
 E G_{A_{1}, A_{2}} t \mapsto e^{t A_{1}}, t \mapsto e^{t A_{2}} A_{1}, A_{2} [A_{1}, A_{2}] = A_{2} A_{1} - A_{1} A_{2} G_{A_{1}, A_{2}} S^{1} \times S^{1} (S^{1} \times S^{1}) / \sim \sim","['general-topology', 'differential-geometry', 'lie-groups']"
1,Is a matrix multiplied with its transpose something special?,Is a matrix multiplied with its transpose something special?,,"In my math lectures, we talked about the Gram-Determinant where a matrix times its transpose are multiplied together. Is $A A^\mathrm T$ something special for any matrix $A$?","In my math lectures, we talked about the Gram-Determinant where a matrix times its transpose are multiplied together. Is $A A^\mathrm T$ something special for any matrix $A$?",,['matrices']
2,What are the Differences Between a Matrix and a Tensor?,What are the Differences Between a Matrix and a Tensor?,,"What is the difference between a matrix and a tensor? Or, what makes a tensor, a tensor? I know that a matrix is a table of values, right? But, a tensor?","What is the difference between a matrix and a tensor? Or, what makes a tensor, a tensor? I know that a matrix is a table of values, right? But, a tensor?",,"['matrices', 'tensors']"
3,Transpose of inverse vs inverse of transpose,Transpose of inverse vs inverse of transpose,,"Given a square matrix, is the transpose of the inverse equal to the inverse of the transpose? $$ (A^{-1})^T = (A^T)^{-1} $$","Given a square matrix, is the transpose of the inverse equal to the inverse of the transpose?","
(A^{-1})^T = (A^T)^{-1}
","['matrices', 'inverse', 'transpose']"
4,Intuition behind Matrix Multiplication,Intuition behind Matrix Multiplication,,"If I multiply two numbers, say $3$ and $5$, I know it means add $3$ to itself $5$ times or add $5$ to itself $3$ times. But If I multiply two matrices, what does it mean ?  I mean I can't think it in terms of repetitive addition. What is the intuitive way of thinking about multiplication of matrices?","If I multiply two numbers, say $3$ and $5$, I know it means add $3$ to itself $5$ times or add $5$ to itself $3$ times. But If I multiply two matrices, what does it mean ?  I mean I can't think it in terms of repetitive addition. What is the intuitive way of thinking about multiplication of matrices?",,"['matrices', 'intuition', 'faq']"
5,"Given this transformation matrix, how do I decompose it into translation, rotation and scale matrices?","Given this transformation matrix, how do I decompose it into translation, rotation and scale matrices?",,"I have this problem from my Graphics course. Given this transformation matrix: $$\begin{pmatrix} -2 &-1&  2\\ -2  &1& -1\\  0  &0&  1\\ \end{pmatrix}$$ I need to extract translation, rotation and scale matrices. I've also have the answer (which is $TRS$): $$T=\begin{pmatrix} 1&0&2\\ 0&1&-1\\ 0&0&1\end{pmatrix}\\ R=\begin{pmatrix} 1/\sqrt2 & -1/\sqrt2 &0 \\ 1/\sqrt2 & 1/\sqrt2 &0 \\ 0&0&1 \end{pmatrix}\\ S=\begin{pmatrix} -2/\sqrt2 & 0 & 0 \\ 0 & \sqrt2 & 0 \\ 0& 0& 1 \end{pmatrix} %    1 0  2        1/sqrt(2) -1/sqrt(2) 0         -2/sqrt(2) 0     0 %T = 0 1 -1    R = /1/sqrt(2) 1/sqrt(2) 0     S = 0        sqrt(2) 0 %    0 0  1        0          0         1         0          0     1 $$ I just have no idea (except for the Translation matrix) how I would get to this solution.","I have this problem from my Graphics course. Given this transformation matrix: $$\begin{pmatrix} -2 &-1&  2\\ -2  &1& -1\\  0  &0&  1\\ \end{pmatrix}$$ I need to extract translation, rotation and scale matrices. I've also have the answer (which is $TRS$): $$T=\begin{pmatrix} 1&0&2\\ 0&1&-1\\ 0&0&1\end{pmatrix}\\ R=\begin{pmatrix} 1/\sqrt2 & -1/\sqrt2 &0 \\ 1/\sqrt2 & 1/\sqrt2 &0 \\ 0&0&1 \end{pmatrix}\\ S=\begin{pmatrix} -2/\sqrt2 & 0 & 0 \\ 0 & \sqrt2 & 0 \\ 0& 0& 1 \end{pmatrix} %    1 0  2        1/sqrt(2) -1/sqrt(2) 0         -2/sqrt(2) 0     0 %T = 0 1 -1    R = /1/sqrt(2) 1/sqrt(2) 0     S = 0        sqrt(2) 0 %    0 0  1        0          0         1         0          0     1 $$ I just have no idea (except for the Translation matrix) how I would get to this solution.",,['matrices']
6,Derivative of the inverse of a matrix,Derivative of the inverse of a matrix,,"In a scientific paper, I've seen the following $$\frac{\delta K^{-1}}{\delta p} = -K^{-1}\frac{\delta K}{\delta p}K^{-1}$$ where $K$ is a $n \times n$ matrix that depends on $p$ . In my calculations I would have done the following $$\frac{\delta K^{-1}}{\delta p} = -K^{-2}\frac{\delta K}{\delta p}=-K^{-T}K^{-1}\frac{\delta K}{\delta p}$$ Is my calculation wrong? Note: I think $K$ is symmetric.","In a scientific paper, I've seen the following where is a matrix that depends on . In my calculations I would have done the following Is my calculation wrong? Note: I think is symmetric.",\frac{\delta K^{-1}}{\delta p} = -K^{-1}\frac{\delta K}{\delta p}K^{-1} K n \times n p \frac{\delta K^{-1}}{\delta p} = -K^{-2}\frac{\delta K}{\delta p}=-K^{-T}K^{-1}\frac{\delta K}{\delta p} K,"['matrices', 'derivatives', 'inverse', 'matrix-calculus']"
7,Why is the determinant the volume of a parallelepiped in any dimensions?,Why is the determinant the volume of a parallelepiped in any dimensions?,,"For $n = 2$ , I can visualize that the determinant $n \times n$ matrix is the area of the parallelograms by actually calculating the area by coordinates.  But how can one easily realize that it is true for any dimensions?","For , I can visualize that the determinant matrix is the area of the parallelograms by actually calculating the area by coordinates.  But how can one easily realize that it is true for any dimensions?",n = 2 n \times n,"['matrices', 'geometry', 'determinant', 'volume']"
8,What is the difference between the Frobenius norm and the 2-norm of a matrix?,What is the difference between the Frobenius norm and the 2-norm of a matrix?,,"Given a matrix, is the Frobenius norm of that matrix always equal to the 2-norm of it, or are there certain matrices where these two norm methods would produce different results? If they are identical, then I suppose the only difference between them is the method of calculation, eh?","Given a matrix, is the Frobenius norm of that matrix always equal to the 2-norm of it, or are there certain matrices where these two norm methods would produce different results? If they are identical, then I suppose the only difference between them is the method of calculation, eh?",,"['matrices', 'normed-spaces', 'matrix-norms']"
9,Factorial of a matrix: what could be the use of it?,Factorial of a matrix: what could be the use of it?,,"Recently on this site, the question was raised how we might define the factorial operation $\mathsf{A}!$ on a square matrix $\mathsf{A}$. The answer , perhaps unsurprisingly, involves the Gamma function . What use might it be to take the factorial of a matrix?  Do any applications come to mind, or does this – for now* – seem to be restricted to the domain of recreational mathematics? (*Until e.g. theoretical physics turns out to have a use for this, as happened with Calabi–Yau manifolds and superstring theory ...)","Recently on this site, the question was raised how we might define the factorial operation $\mathsf{A}!$ on a square matrix $\mathsf{A}$. The answer , perhaps unsurprisingly, involves the Gamma function . What use might it be to take the factorial of a matrix?  Do any applications come to mind, or does this – for now* – seem to be restricted to the domain of recreational mathematics? (*Until e.g. theoretical physics turns out to have a use for this, as happened with Calabi–Yau manifolds and superstring theory ...)",,"['matrices', 'operator-theory', 'factorial', 'matrix-calculus', 'applications']"
10,How to prove that a $3 \times 3$ Magic Square must have $5$ in its middle cell?,How to prove that a  Magic Square must have  in its middle cell?,3 \times 3 5,"A Magic Square of order $n$ is an arrangement of $n^2$ numbers, usually distinct integers, in a square, such that the $n$ numbers in all rows, all columns, and both diagonals sum to the same constant. How to prove that a normal $3\times 3$ magic square where the integers from 1 to 9 are arranged in some way, must have $5$ in its middle cell? I have tried taking $a,b,c,d,e,f,g,h,i$ and solving equations to calculate $e$ but there are so many equations that I could not manage to solve them.","A Magic Square of order is an arrangement of numbers, usually distinct integers, in a square, such that the numbers in all rows, all columns, and both diagonals sum to the same constant. How to prove that a normal magic square where the integers from 1 to 9 are arranged in some way, must have in its middle cell? I have tried taking and solving equations to calculate but there are so many equations that I could not manage to solve them.","n n^2 n 3\times 3 5 a,b,c,d,e,f,g,h,i e","['matrices', 'magic-square']"
11,Matrices: left inverse is also right inverse? [duplicate],Matrices: left inverse is also right inverse? [duplicate],,"This question already has answers here : If $AB = I$ then $BA = I$ (34 answers) Closed 10 years ago . If $A$ and $B$ are square matrices, and $AB=I$ , then I think it is also true that $BA=I$ . In fact, this Wikipedia page says that this ""follows from the associativity of matrix multiplication"". I assume there's a nice simple one-line proof, but can't seem to find it. Nothing exotic, here -- assume that the matrices have finite size and their elements are real numbers. This isn't homework (if that matters to you). My last homework assignment was about 50 years ago.","This question already has answers here : If $AB = I$ then $BA = I$ (34 answers) Closed 10 years ago . If and are square matrices, and , then I think it is also true that . In fact, this Wikipedia page says that this ""follows from the associativity of matrix multiplication"". I assume there's a nice simple one-line proof, but can't seem to find it. Nothing exotic, here -- assume that the matrices have finite size and their elements are real numbers. This isn't homework (if that matters to you). My last homework assignment was about 50 years ago.",A B AB=I BA=I,['matrices']
12,What is the usefulness of matrices?,What is the usefulness of matrices?,,I have matrices for my syllabus but I don't know where they find their use. I even asked my teacher but she also has no answer. Can anyone please tell me where they are used? And please also give me an example of how they are used?,I have matrices for my syllabus but I don't know where they find their use. I even asked my teacher but she also has no answer. Can anyone please tell me where they are used? And please also give me an example of how they are used?,,['matrices']
13,How many rows and columns are in an m x n matrix?,How many rows and columns are in an m x n matrix?,,"A simple question: By definition, does an m x n matrix have m rows and n columns, or is it vice versa?","A simple question: By definition, does an m x n matrix have m rows and n columns, or is it vice versa?",,['matrices']
14,Rotation Matrix of rotation around a point other than the origin,Rotation Matrix of rotation around a point other than the origin,,"In homogeneous coordinates, a rotation matrix around the origin can be described as $R = \begin{bmatrix}\cos(\theta) & -\sin(\theta) & 0\\\sin(\theta) & \cos(\theta) & 0 \\ 0&0&1\end{bmatrix}$ with the angle $\theta$ and the rotation being counter-clockwise. A translation amongst $x$ and $y$ can be defined as: $T(x,y) = \begin{bmatrix}1&0&x\\ 0& 1&y\\0&0&1\end{bmatrix}$ As I understand, the rotation matrix around an arbitrary point, can be expressed as moving the rotation point to the origin, rotating around the origin and moving back to the original position. The formula of this operations can be described in a simple multiplication of $T(x,y) * R * T(-x,-y) \qquad (I)$ I find this to be counter-intuitive. In my understanding, it should be $T(-x,-y) * R * T(x,y) \qquad (II)$ The two formulations are definitely not equal. The first equation yields $E1 = \begin{bmatrix}\cos(\theta) & -\sin(\theta) & -x\cdot\cos(\theta)+y\cdot\sin(\theta)+x\\\sin(\theta) & \cos(\theta) & -x\cdot\sin(\theta)-y\cdot\cos(\theta)+y \\ 0&0&1\end{bmatrix}$ The second one: $E2 = \begin{bmatrix}\cos(\theta) & -\sin(\theta) & x\cdot\cos(\theta)-y\cdot\sin(\theta)-x\\\sin(\theta) & \cos(\theta) & x\cdot\sin(\theta)+y\cdot\cos(\theta)-y \\ 0&0&1\end{bmatrix}$ So, which one is correct?","In homogeneous coordinates, a rotation matrix around the origin can be described as with the angle and the rotation being counter-clockwise. A translation amongst and can be defined as: As I understand, the rotation matrix around an arbitrary point, can be expressed as moving the rotation point to the origin, rotating around the origin and moving back to the original position. The formula of this operations can be described in a simple multiplication of I find this to be counter-intuitive. In my understanding, it should be The two formulations are definitely not equal. The first equation yields The second one: So, which one is correct?","R = \begin{bmatrix}\cos(\theta) & -\sin(\theta) & 0\\\sin(\theta) & \cos(\theta) & 0 \\ 0&0&1\end{bmatrix} \theta x y T(x,y) = \begin{bmatrix}1&0&x\\ 0& 1&y\\0&0&1\end{bmatrix} T(x,y) * R * T(-x,-y) \qquad (I) T(-x,-y) * R * T(x,y) \qquad (II) E1 = \begin{bmatrix}\cos(\theta) & -\sin(\theta) & -x\cdot\cos(\theta)+y\cdot\sin(\theta)+x\\\sin(\theta) & \cos(\theta) & -x\cdot\sin(\theta)-y\cdot\cos(\theta)+y \\ 0&0&1\end{bmatrix} E2 = \begin{bmatrix}\cos(\theta) & -\sin(\theta) & x\cdot\cos(\theta)-y\cdot\sin(\theta)-x\\\sin(\theta) & \cos(\theta) & x\cdot\sin(\theta)+y\cdot\cos(\theta)-y \\ 0&0&1\end{bmatrix}","['matrices', 'rotations', 'affine-geometry', 'homogeneous-spaces']"
15,Sum of all elements in a matrix,Sum of all elements in a matrix,,The trace is the sum of the elements on the diagonal of a matrix. Is there a similar operation for the sum of all the elements in a matrix?,The trace is the sum of the elements on the diagonal of a matrix. Is there a similar operation for the sum of all the elements in a matrix?,,['matrices']
16,'Linux' math program with interactive terminal?,'Linux' math program with interactive terminal?,,Are there any open source math programs out there that have an interactive terminal and that work on linux? So for example you could enter two matrices and specify an operation such as multiply and it would then return the answer or a error message specifying why an answer can't be computed? I am just looking for something that can perform basic matrix operations and modular arithmetic.,Are there any open source math programs out there that have an interactive terminal and that work on linux? So for example you could enter two matrices and specify an operation such as multiply and it would then return the answer or a error message specifying why an answer can't be computed? I am just looking for something that can perform basic matrix operations and modular arithmetic.,,"['matrices', 'modular-arithmetic', 'math-software', 'computer-algebra-systems']"
17,Sum of squares of dependent Gaussian random variables,Sum of squares of dependent Gaussian random variables,,"Ok, so the Chi-Squared distribution with $n$ degrees of freedom is the sum of the squares of $n$ independent Gaussian random variables. The trouble is, my Gaussian random variables are not independent. They do however all have zero mean and the same variance. Supposing I have a covariance matrix---which again is not a diagonal matrix because they aren't independent, but all the elements along the diagonal are equal to each other because they have the same variance, and in fact the covariance matrix is a symmetric Toeplitz matrix (and I'm not saying that this is important to the solution if there is one, but if it's a necessary property to get anywhere, by all means use that fact)---is there some way to decompose this sum of squares of these Gaussian random variables into perhaps a sum of chi-squared random variables and possibly Gaussian random variables? In other words, I can't directly just square them all and add them together and call it a chi squared distribution because a chi squared distribution is a sum of independent Gaussian squares, and they aren't independent. I know how to find a linear transformation of the Gaussian random variables which are $n$ independent Gaussians, but that's no help because they aren't the things being squared, you see.","Ok, so the Chi-Squared distribution with degrees of freedom is the sum of the squares of independent Gaussian random variables. The trouble is, my Gaussian random variables are not independent. They do however all have zero mean and the same variance. Supposing I have a covariance matrix---which again is not a diagonal matrix because they aren't independent, but all the elements along the diagonal are equal to each other because they have the same variance, and in fact the covariance matrix is a symmetric Toeplitz matrix (and I'm not saying that this is important to the solution if there is one, but if it's a necessary property to get anywhere, by all means use that fact)---is there some way to decompose this sum of squares of these Gaussian random variables into perhaps a sum of chi-squared random variables and possibly Gaussian random variables? In other words, I can't directly just square them all and add them together and call it a chi squared distribution because a chi squared distribution is a sum of independent Gaussian squares, and they aren't independent. I know how to find a linear transformation of the Gaussian random variables which are independent Gaussians, but that's no help because they aren't the things being squared, you see.",n n n,"['matrices', 'statistics', 'probability-distributions', 'random-variables', 'quadratic-forms']"
18,Prove that the eigenvalues of a block matrix are the combined eigenvalues of its blocks,Prove that the eigenvalues of a block matrix are the combined eigenvalues of its blocks,,"Let $A$ be a block upper triangular matrix: $$A = \begin{pmatrix} A_{1,1}&A_{1,2}\\ 0&A_{2,2} \end{pmatrix}$$ where $A_{1,1} ∈ C^{p \times p}$ , $A_{2,2} ∈ C^{(n-p) \times (n-p)}$ . Show that the eigenvalues of $A$ are the combined eigenvalues of $A_{1,1}$ and $A_{2,2}$ I've been pretty much stuck looking at this for a good hour and a half, so any help would be much appreciated. Thanks.","Let be a block upper triangular matrix: where , . Show that the eigenvalues of are the combined eigenvalues of and I've been pretty much stuck looking at this for a good hour and a half, so any help would be much appreciated. Thanks.","A A = \begin{pmatrix} A_{1,1}&A_{1,2}\\ 0&A_{2,2} \end{pmatrix} A_{1,1} ∈ C^{p \times p} A_{2,2} ∈ C^{(n-p) \times (n-p)} A A_{1,1} A_{2,2}","['matrices', 'eigenvalues-eigenvectors', 'block-matrices']"
19,Derivative of Quadratic Form,Derivative of Quadratic Form,,"For the Quadratic Form $X^TAX; X\in\mathbb{R}^n, A\in\mathbb{R}^{n \times n}$ (which simplifies to $\Sigma_{i=0}^n\Sigma_{j=0}^nA_{ij}x_ix_j$), I tried to take the derivative wrt. X ($\Delta_X X^TAX$) and ended up with the following: The $k^{th}$ element of the derivative represented as $\Delta_{X_k}X^TAX=[\Sigma_{i=1}^n(A_{ik}x_k+A_{ki})x_i] + A_{kk}x_k(1-x_k)$ Does this result look right? Is there an alternative form? I'm trying to get to the $\mu_0$ of Gaussian Discriminant Analysis by maximizing the log likelihood and I need to take the derivative of a Quadratic form. Either the result I mentioned above is wrong (shouldn't be because I went over my arithmetic several times) or the form I arrived at above is not the terribly useful to my problem (because I'm unable to proceed). I can give more details about the problem or the steps I put down to arrive at the above result, but I didn't want to clutter to start off. Please let me know if more details are necessary. Any link to related material is also much appreciated.","For the Quadratic Form $X^TAX; X\in\mathbb{R}^n, A\in\mathbb{R}^{n \times n}$ (which simplifies to $\Sigma_{i=0}^n\Sigma_{j=0}^nA_{ij}x_ix_j$), I tried to take the derivative wrt. X ($\Delta_X X^TAX$) and ended up with the following: The $k^{th}$ element of the derivative represented as $\Delta_{X_k}X^TAX=[\Sigma_{i=1}^n(A_{ik}x_k+A_{ki})x_i] + A_{kk}x_k(1-x_k)$ Does this result look right? Is there an alternative form? I'm trying to get to the $\mu_0$ of Gaussian Discriminant Analysis by maximizing the log likelihood and I need to take the derivative of a Quadratic form. Either the result I mentioned above is wrong (shouldn't be because I went over my arithmetic several times) or the form I arrived at above is not the terribly useful to my problem (because I'm unable to proceed). I can give more details about the problem or the steps I put down to arrive at the above result, but I didn't want to clutter to start off. Please let me know if more details are necessary. Any link to related material is also much appreciated.",,"['matrices', 'multivariable-calculus', 'derivatives', 'matrix-calculus', 'quadratic-forms']"
20,Why are nonsquare matrices not invertible?,Why are nonsquare matrices not invertible?,,I have a theoretical question. Why are non-square matrices not invertible? I am running into a lot of doubts like this in my introductory study of linear algebra.,I have a theoretical question. Why are non-square matrices not invertible? I am running into a lot of doubts like this in my introductory study of linear algebra.,,['matrices']
21,What does Determinant of Covariance Matrix give?,What does Determinant of Covariance Matrix give?,,"I am representing my 3d data using its sample covariance matrix. I want to know what the determinant of covariance Matrix represents. If the determinant is positive, zero, negative, high positive, high negative, what does it mean or represent? Thanks EDIT: Covariance is being used to represent variance for 3d coordinates that I have. If my covariance matrix A determinant is +100, and the other covariance matrix B determinant is +5. Which of these values show if the variance is more or not. Which value tells that data points are more dispersed. Which value shows that readings are further away from mean.","I am representing my 3d data using its sample covariance matrix. I want to know what the determinant of covariance Matrix represents. If the determinant is positive, zero, negative, high positive, high negative, what does it mean or represent? Thanks EDIT: Covariance is being used to represent variance for 3d coordinates that I have. If my covariance matrix A determinant is +100, and the other covariance matrix B determinant is +5. Which of these values show if the variance is more or not. Which value tells that data points are more dispersed. Which value shows that readings are further away from mean.",,"['matrices', 'covariance']"
22,How to compute homography matrix H from corresponding points (2d-2d planar Homography),How to compute homography matrix H from corresponding points (2d-2d planar Homography),,"I went through this thread Mapping Irregular Quadrilateral to a Rectangle If i know the 4 corresponding points in image say p1->p1' p2->p2' p3->p3' p4->p4' then how to compute pi(x,y) from pi'(x,y) i don't know how to compute elements in Homography matrix H from those 8 known points [x']= [h11 h12 h13]  [x] [y']= [h21 h22 h23]  [y] [(1)]=[h31 h32 (1)] [(1)] [Excuse me. I am not sure if I should extend this question, or create a new one, since I can't post comments on threads] I want to ask the same question, but using absolute values so I can visualize it. Lets say my points on the image plane are: p[0] = x:407 y:253 p[1] = x:386 y:253 p[2] = x:406 y:232 p[3] = x:385 y:232 these points are in a 500px width x 333px height image plane with 0,0 at top left corner. These points represents a picture of a real plane where a 30mm side square are located. Assuming this picture was taken by a fixed camera at origin heading Z axis. So, I know the physical distance between p0,p1 ; p0,p2 ; p1,p3; p2,p3 are 30mm. But is it possible to get the X,Y,Z from each of these points using only this information above?","I went through this thread Mapping Irregular Quadrilateral to a Rectangle If i know the 4 corresponding points in image say p1->p1' p2->p2' p3->p3' p4->p4' then how to compute pi(x,y) from pi'(x,y) i don't know how to compute elements in Homography matrix H from those 8 known points [x']= [h11 h12 h13]  [x] [y']= [h21 h22 h23]  [y] [(1)]=[h31 h32 (1)] [(1)] [Excuse me. I am not sure if I should extend this question, or create a new one, since I can't post comments on threads] I want to ask the same question, but using absolute values so I can visualize it. Lets say my points on the image plane are: p[0] = x:407 y:253 p[1] = x:386 y:253 p[2] = x:406 y:232 p[3] = x:385 y:232 these points are in a 500px width x 333px height image plane with 0,0 at top left corner. These points represents a picture of a real plane where a 30mm side square are located. Assuming this picture was taken by a fixed camera at origin heading Z axis. So, I know the physical distance between p0,p1 ; p0,p2 ; p1,p3; p2,p3 are 30mm. But is it possible to get the X,Y,Z from each of these points using only this information above?",,"['matrices', 'geometry', 'projective-geometry']"
23,Proof for why a matrix multiplied by its transpose is positive semidefinite,Proof for why a matrix multiplied by its transpose is positive semidefinite,,"The top answer to this question says Moreover if $A$ is regular, then $AA^T$ is also positive definite,   since $$x^TAA^Tx=(A^Tx)^T(A^Tx)> 0$$ Suppose $A$ is not regular. It holds that $$x^TAA^Tx=(A^Tx)^T(A^Tx)= \|A^Tx\|^2_2  \ge 0$$ Therefore $AA^T$ is positive semidefinite. Is this argument enough, or am I missing something?","The top answer to this question says Moreover if $A$ is regular, then $AA^T$ is also positive definite,   since $$x^TAA^Tx=(A^Tx)^T(A^Tx)> 0$$ Suppose $A$ is not regular. It holds that $$x^TAA^Tx=(A^Tx)^T(A^Tx)= \|A^Tx\|^2_2  \ge 0$$ Therefore $AA^T$ is positive semidefinite. Is this argument enough, or am I missing something?",,"['matrices', 'normed-spaces']"
24,Norm of a symmetric matrix equals spectral radius,Norm of a symmetric matrix equals spectral radius,,"How do I prove that the norm of a matrix equals the absolutely largest eigenvalue of the matrix? This is the precise question: Let $A$ be a symmetric $n \times n$ matrix. Consider $A$ as an operator in $\mathbb{R}^n$ given by $x \mapsto Ax$. Prove that $\|A\| = \max_j |\lambda_j|$, where $\lambda_j$ are the eigenvalues of $A$. I've read the relevant sections in my literature over and over but can't find any clue on how to begin. A solution is suggested here but the notion of diagonal operator is not in my literature so it doesn't tell me very much. So, any other hints on how to solve the question? Thanks.","How do I prove that the norm of a matrix equals the absolutely largest eigenvalue of the matrix? This is the precise question: Let $A$ be a symmetric $n \times n$ matrix. Consider $A$ as an operator in $\mathbb{R}^n$ given by $x \mapsto Ax$. Prove that $\|A\| = \max_j |\lambda_j|$, where $\lambda_j$ are the eigenvalues of $A$. I've read the relevant sections in my literature over and over but can't find any clue on how to begin. A solution is suggested here but the notion of diagonal operator is not in my literature so it doesn't tell me very much. So, any other hints on how to solve the question? Thanks.",,"['matrices', 'functional-analysis', 'operator-theory', 'symmetric-matrices', 'spectral-radius']"
25,Meaning of the spectral norm of a matrix,Meaning of the spectral norm of a matrix,,Is there an intuitive meaning for the spectral norm of a matrix? Why would an algorithm calculate the relative recovery in spectral norm between two images (i.e. one before the algorithm and the other after)? Thanks,Is there an intuitive meaning for the spectral norm of a matrix? Why would an algorithm calculate the relative recovery in spectral norm between two images (i.e. one before the algorithm and the other after)? Thanks,,"['matrices', 'matrix-norms', 'spectral-norm']"
26,$AB-BA=I$ having no solutions,having no solutions,AB-BA=I,"The following question is from Artin's Algebra . If $A$ and $B$ are two square matrices with real entries, show that $AB-BA=I$ has no solutions. I have no idea on how to tackle this question. I tried block multiplication, but it didn't appear to work.","The following question is from Artin's Algebra . If and are two square matrices with real entries, show that has no solutions. I have no idea on how to tackle this question. I tried block multiplication, but it didn't appear to work.",A B AB-BA=I,"['matrices', 'matrix-equations']"
27,Are most matrices invertible? [duplicate],Are most matrices invertible? [duplicate],,"This question already has answers here : Probability of having zero determinant (5 answers) Closed 10 years ago . I asked myself this question, to which I think the answer is ""yes"". One reason would be that an invertible matrix has infinitely many options for its determinant (except $0$), whereas a non-invertible must have $0$ as the determinant. Do you have another approach to this question, in terms of probability for example?","This question already has answers here : Probability of having zero determinant (5 answers) Closed 10 years ago . I asked myself this question, to which I think the answer is ""yes"". One reason would be that an invertible matrix has infinitely many options for its determinant (except $0$), whereas a non-invertible must have $0$ as the determinant. Do you have another approach to this question, in terms of probability for example?",,"['matrices', 'inverse']"
28,"A System of Matrix Equations (2 Riccati, 1 Lyapunov)","A System of Matrix Equations (2 Riccati, 1 Lyapunov)",,"Setup: Let $\gamma \in(0,1)$, ${\bf F},{\bf Q} \in \mathbb R^{n\times n}$, ${\bf H}\in \mathbb R^{n\times r}$, and ${\bf R}\in \mathbb R^{r\times r}$ be given and suppose that  ${\bf P}$,${\bf W}$,${\bf X}\in \mathbb R^{n\times n}$, and ${\bf K}$,${\bf L}\in \mathbb R^{n\times r}$ satisfy \begin{align} {\bf P} &={\bf F}({\bf I}_{n}-{\bf K} {\bf H}^\top){\bf P}{\bf F}^\top+{\bf Q}, \;\;\;\;\;\;\;\;\text{where}\;\;\;\;{\bf K}\equiv {\bf P} {\bf H}\left({\bf H}^\top{\bf P} {\bf H}+ \frac{1}{\gamma}{\bf R} \right)^{-1} \tag1\\[4ex] {\bf W} &={\bf F}({\bf I}_{n}-{\bf L} {\bf H}^\top){\bf W}{\bf F}^\top+{\bf Q}, \;\;\;\;\;\;\;\;\text{where}\;\;\;\;{\bf L}\equiv {\bf W} {\bf H}({\bf H}^\top {\bf W} {\bf H}+ {\bf R})^{-1} \tag2\\[4ex] {\bf X} &={\bf K}{\bf H}^\top {\bf W}+({\bf I}_n-{\bf K}{\bf H}^\top){\bf F}{\bf X}({\bf I}_n-{\bf H} {\bf L}^\top){\bf F}^\top \tag3\\ {\color{white}X} \end{align} Moreover, assume that ${\bf P}$, ${\bf W}$, ${\bf R}$, and ${\bf Q}$ are symmetric, and ${\bf P}$, ${\bf W}$ and ${\bf X}$ are invertible. Want to prove: $${\bf K}=(\gamma{\bf I}_{n}+(1-\gamma) {\bf X} {\bf W}^{-1}){\bf L} \tag4$$ Some ideas and comments: In the scalar case (with $n=r=1$) what works is to subtract $(2)$ from $(1)$ which eliminates ${\bf Q}$, then use that to solve for ${\bf F}^2$, i.e. (suppressing the $^\top$ notation) $$ {\bf F}^2 = \frac{(\gamma{\bf H}^2{\bf P}+{\bf R})({\bf H}^2{\bf W}+{\bf R})({\bf P}-{\bf W})}{{\bf R}^2({\bf P}-{\bf W})+(1-\gamma){\bf H}^2{\bf R}{\bf P}{\bf W})},$$ substitute this into $(3)$ and solve for ${\bf X}$ which yields $${\bf X} = \frac{(\gamma {\bf R}({\bf P}-{\bf W})+\gamma(1-\gamma){\bf H}^2{\bf P}{\bf W}}{(1-\gamma)(\gamma {\bf H}^2{\bf P}+{\bf R}))}. $$ Finally, solving for ${\bf X}$ using $(4)$ yields the same thing. Notice that the equations $(1)$ and $(2)$ are Riccati equations on ${\bf P}$ and ${\bf W}$ respectively, which means that there are known methods (described here ) to solve for ${\bf P}$ and ${\bf W}$. Though I was not able to make use of those solutions. Equation $(3)$ is similar to a Lyapunov equation on ${\bf X}$, which means that a vectorization method (described here ) allows one to obtain an equation for ${\bf X}$. I think the proof will come from a procedure similar to the one that works for the scalar case. That is, subtracting $(2)$ from $(1)$, using this equation to eliminate ${\bf F}$ from $(3)$, then showing that the resulting equation implies $(4)$. If you can prove it under additional assumptions that could also be helpful. Here is a simple Matlab code that allows you to test the result: gamma = 0.5;      F = [2, 2, 3; 4, 5, 6; 7, 8, 9];  % n x n     H = [1, 2; 3, 1; 2, 1];           % n x r     R = [3 , 1; 1, 3];                % r x r, symmetric     Q = [2, 0, 0; 0, 5, 0; 0, 0, 9];  % n x n, symmetric      n = length(F);      r = length(R);       I = eye(n);      P = I;     W = I;     X = I;     for i=1:1000         K = (P*H)/(H'*P*H+(R/gamma));         P = F*(P-K*H'*P)*F'+Q;         L = (W*H)/(H'*W*H+R);             W = F*(W-L*H'*W)*F'+Q;         X = K*H'*W+(I-K*H')*F*X*(I-H*L')*F';     end      disp(['K - (gamma*I+(1-gamma)*X*W^(-1))*L = ',...           num2str(sum(sum(abs(K-(gamma*I+(1-gamma)*X*(W^(-1)))*L))))]) This question is a boiled down version of another question: Forecast equivalence between two steady-state Kalman filters (with ${\bf L} \equiv {\bf L}_1$, ${\bf W} \equiv {\bf W}_{11}$, and ${\bf X} \equiv {\bf W}_{12}$).","Setup: Let $\gamma \in(0,1)$, ${\bf F},{\bf Q} \in \mathbb R^{n\times n}$, ${\bf H}\in \mathbb R^{n\times r}$, and ${\bf R}\in \mathbb R^{r\times r}$ be given and suppose that  ${\bf P}$,${\bf W}$,${\bf X}\in \mathbb R^{n\times n}$, and ${\bf K}$,${\bf L}\in \mathbb R^{n\times r}$ satisfy \begin{align} {\bf P} &={\bf F}({\bf I}_{n}-{\bf K} {\bf H}^\top){\bf P}{\bf F}^\top+{\bf Q}, \;\;\;\;\;\;\;\;\text{where}\;\;\;\;{\bf K}\equiv {\bf P} {\bf H}\left({\bf H}^\top{\bf P} {\bf H}+ \frac{1}{\gamma}{\bf R} \right)^{-1} \tag1\\[4ex] {\bf W} &={\bf F}({\bf I}_{n}-{\bf L} {\bf H}^\top){\bf W}{\bf F}^\top+{\bf Q}, \;\;\;\;\;\;\;\;\text{where}\;\;\;\;{\bf L}\equiv {\bf W} {\bf H}({\bf H}^\top {\bf W} {\bf H}+ {\bf R})^{-1} \tag2\\[4ex] {\bf X} &={\bf K}{\bf H}^\top {\bf W}+({\bf I}_n-{\bf K}{\bf H}^\top){\bf F}{\bf X}({\bf I}_n-{\bf H} {\bf L}^\top){\bf F}^\top \tag3\\ {\color{white}X} \end{align} Moreover, assume that ${\bf P}$, ${\bf W}$, ${\bf R}$, and ${\bf Q}$ are symmetric, and ${\bf P}$, ${\bf W}$ and ${\bf X}$ are invertible. Want to prove: $${\bf K}=(\gamma{\bf I}_{n}+(1-\gamma) {\bf X} {\bf W}^{-1}){\bf L} \tag4$$ Some ideas and comments: In the scalar case (with $n=r=1$) what works is to subtract $(2)$ from $(1)$ which eliminates ${\bf Q}$, then use that to solve for ${\bf F}^2$, i.e. (suppressing the $^\top$ notation) $$ {\bf F}^2 = \frac{(\gamma{\bf H}^2{\bf P}+{\bf R})({\bf H}^2{\bf W}+{\bf R})({\bf P}-{\bf W})}{{\bf R}^2({\bf P}-{\bf W})+(1-\gamma){\bf H}^2{\bf R}{\bf P}{\bf W})},$$ substitute this into $(3)$ and solve for ${\bf X}$ which yields $${\bf X} = \frac{(\gamma {\bf R}({\bf P}-{\bf W})+\gamma(1-\gamma){\bf H}^2{\bf P}{\bf W}}{(1-\gamma)(\gamma {\bf H}^2{\bf P}+{\bf R}))}. $$ Finally, solving for ${\bf X}$ using $(4)$ yields the same thing. Notice that the equations $(1)$ and $(2)$ are Riccati equations on ${\bf P}$ and ${\bf W}$ respectively, which means that there are known methods (described here ) to solve for ${\bf P}$ and ${\bf W}$. Though I was not able to make use of those solutions. Equation $(3)$ is similar to a Lyapunov equation on ${\bf X}$, which means that a vectorization method (described here ) allows one to obtain an equation for ${\bf X}$. I think the proof will come from a procedure similar to the one that works for the scalar case. That is, subtracting $(2)$ from $(1)$, using this equation to eliminate ${\bf F}$ from $(3)$, then showing that the resulting equation implies $(4)$. If you can prove it under additional assumptions that could also be helpful. Here is a simple Matlab code that allows you to test the result: gamma = 0.5;      F = [2, 2, 3; 4, 5, 6; 7, 8, 9];  % n x n     H = [1, 2; 3, 1; 2, 1];           % n x r     R = [3 , 1; 1, 3];                % r x r, symmetric     Q = [2, 0, 0; 0, 5, 0; 0, 0, 9];  % n x n, symmetric      n = length(F);      r = length(R);       I = eye(n);      P = I;     W = I;     X = I;     for i=1:1000         K = (P*H)/(H'*P*H+(R/gamma));         P = F*(P-K*H'*P)*F'+Q;         L = (W*H)/(H'*W*H+R);             W = F*(W-L*H'*W)*F'+Q;         X = K*H'*W+(I-K*H')*F*X*(I-H*L')*F';     end      disp(['K - (gamma*I+(1-gamma)*X*W^(-1))*L = ',...           num2str(sum(sum(abs(K-(gamma*I+(1-gamma)*X*(W^(-1)))*L))))]) This question is a boiled down version of another question: Forecast equivalence between two steady-state Kalman filters (with ${\bf L} \equiv {\bf L}_1$, ${\bf W} \equiv {\bf W}_{11}$, and ${\bf X} \equiv {\bf W}_{12}$).",,"['matrices', 'optimization', 'matrix-equations', 'optimal-control']"
29,Why is the Frobenius norm of a matrix greater than or equal to the spectral norm?,Why is the Frobenius norm of a matrix greater than or equal to the spectral norm?,,How can one prove that $ \|A\|_2 \le \|A\|_F $ without using $ \|A\|_2^2 := \lambda_{\max}(A^TA) $ ? It makes sense that the $2$ -norm would be less than or equal to the Frobenius norm but I don't know how to prove it.  I do know: $$\|A\|_2 = \max_{\|x\|_2 = 1} {\|Ax\|_2}$$ and I know I can define the Frobenius norm to be: $$\|A\|_F^2 = \sum_{j=1}^n {\|Ae_j\|_2^2}$$ but I don't see how this could help.  I don't know how else to compare the two norms though.,How can one prove that without using ? It makes sense that the -norm would be less than or equal to the Frobenius norm but I don't know how to prove it.  I do know: and I know I can define the Frobenius norm to be: but I don't see how this could help.  I don't know how else to compare the two norms though., \|A\|_2 \le \|A\|_F   \|A\|_2^2 := \lambda_{\max}(A^TA)  2 \|A\|_2 = \max_{\|x\|_2 = 1} {\|Ax\|_2} \|A\|_F^2 = \sum_{j=1}^n {\|Ae_j\|_2^2},"['matrices', 'normed-spaces', 'equivalence-relations', 'matrix-norms', 'spectral-norm']"
30,Is there a fast way to prove a symmetric tridiagonal matrix is positive definite?,Is there a fast way to prove a symmetric tridiagonal matrix is positive definite?,,"I' m trying to prove that $$A=\begin{pmatrix}  4 & 2 & 0 & 0 & 0 \\  2 & 5 & 2 & 0 & 0 \\  0 & 2 & 5 & 2 & 0 \\  0 & 0 & 2 & 5 & 2 \\  0 & 0 & 0 & 2 & 5 \\    \end{pmatrix}$$ admits a Cholesky decomposition . $A$ is symmetric, so it admits a Cholesky decomposition iff it is positive definite . The only methods I know for checking this are: $X^tAX > 0, \quad \forall X \in \mathbb{K}^n- \{0\}$. If $\lambda$ is an eigenvalue of $A$, then $\lambda>0.$ I have failed to prove it using 1 and 2 is taking me so much time. Is there any easier way to do this, given that $A$ is tridiagonal ?","I' m trying to prove that $$A=\begin{pmatrix}  4 & 2 & 0 & 0 & 0 \\  2 & 5 & 2 & 0 & 0 \\  0 & 2 & 5 & 2 & 0 \\  0 & 0 & 2 & 5 & 2 \\  0 & 0 & 0 & 2 & 5 \\    \end{pmatrix}$$ admits a Cholesky decomposition . $A$ is symmetric, so it admits a Cholesky decomposition iff it is positive definite . The only methods I know for checking this are: $X^tAX > 0, \quad \forall X \in \mathbb{K}^n- \{0\}$. If $\lambda$ is an eigenvalue of $A$, then $\lambda>0.$ I have failed to prove it using 1 and 2 is taking me so much time. Is there any easier way to do this, given that $A$ is tridiagonal ?",,"['matrices', 'symmetric-matrices', 'positive-definite', 'tridiagonal-matrices']"
31,How to calculate the gradient of log det matrix inverse?,How to calculate the gradient of log det matrix inverse?,,"How to calculate the gradient with respect to $X$ of: $$ \log \mathrm{det}\, X^{-1} $$ here $X$ is a positive definite matrix, and det is the determinant of a matrix. How to calculate this? Or what's the result? Thanks!","How to calculate the gradient with respect to $X$ of: $$ \log \mathrm{det}\, X^{-1} $$ here $X$ is a positive definite matrix, and det is the determinant of a matrix. How to calculate this? Or what's the result? Thanks!",,"['matrices', 'derivatives', 'matrix-calculus', 'scalar-fields']"
32,Convert vector into diagonal matrix,Convert vector into diagonal matrix,,"Given a vector $[x_1,x_2,x_3, \dots, x_n]^T$ , is it possible to obtain a diagonal matrix, $ \left[\begin{array}{c c c c c} x_1 & 0 & 0 & \dots & 0\\ 0 & x_2 & 0 & \dots & 0\\ 0 & 0 & x_3 & \dots & 0\\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \dots & x_n\\ \end{array} \right] $ using matrix operations (like multiplication and/or addition with identity matrix etc)? This seems trivial, but I am unable to work it out! I need to do this for automation of process in Maxima, so that I don't have to manually type in the elements diagonally. Thanks. EDIT: I recently found a direct function diag_matrix(x1,x2,x3,...) in Maxima. Which means that if we have a list [x1, x2, x3] , we can use apply(diag_matrix, [x1, x2, x3]) . I am not sure if it is introduced in a recent version or it existed before I posted this question.","Given a vector , is it possible to obtain a diagonal matrix, using matrix operations (like multiplication and/or addition with identity matrix etc)? This seems trivial, but I am unable to work it out! I need to do this for automation of process in Maxima, so that I don't have to manually type in the elements diagonally. Thanks. EDIT: I recently found a direct function diag_matrix(x1,x2,x3,...) in Maxima. Which means that if we have a list [x1, x2, x3] , we can use apply(diag_matrix, [x1, x2, x3]) . I am not sure if it is introduced in a recent version or it existed before I posted this question.","[x_1,x_2,x_3, \dots, x_n]^T 
\left[\begin{array}{c c c c c}
x_1 & 0 & 0 & \dots & 0\\
0 & x_2 & 0 & \dots & 0\\
0 & 0 & x_3 & \dots & 0\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \dots & x_n\\
\end{array}
\right]
",['matrices']
33,Finding path-lengths by the power of Adjacency matrix of an undirected graph,Finding path-lengths by the power of Adjacency matrix of an undirected graph,,"I knew from Mark Newman's book - Networks: An Introduction (Page 137, Eq: 6.31) that, if $A$ is the adjacency matrix of a graph, then $ij$'th entry of $A^k$ will give me the number of $k$-length paths connecting the vertices $i$ and $j$. This works very well for directed graphs. But does it work for undirected graphs too? For instance, for the undireceted network below: if i want to calculate how many $3$-length paths are there from vertex-$2$ to vertex-$1$, then I should find $[A^3]_{12}$. Proceeding in this way, I get,  \begin{eqnarray} A^3 =  \left( \begin{matrix} 4 && 6 && 1 && 5 && 5 \\ 6 && 2 && 3 && 6 && 2  \\ 1 && 3 && 0 && 1 && 2 \\ 5 && 6 && 1 && 4 && 5 \\ 5 && 2 && 2 && 5 && 2 \end{matrix} \right) \end{eqnarray} And, I find, the entry of the 1st row and 2nd column = 6 = entry of the 2nd row and 1st column . Does it mean that there are 6 paths of length 3 from vertex-2 to vertex-1? Cearly it is not true because I have only $1$ path of length $3$ from 2 to 1, namely the sequence: $(2,4,5,1)$. What am I missing? UPDATE: I am attaching a snapshot of Newman's book. He only talks about ""paths"", but never about a ""walk"". Is it a mistake? Newman's book snapshot","I knew from Mark Newman's book - Networks: An Introduction (Page 137, Eq: 6.31) that, if $A$ is the adjacency matrix of a graph, then $ij$'th entry of $A^k$ will give me the number of $k$-length paths connecting the vertices $i$ and $j$. This works very well for directed graphs. But does it work for undirected graphs too? For instance, for the undireceted network below: if i want to calculate how many $3$-length paths are there from vertex-$2$ to vertex-$1$, then I should find $[A^3]_{12}$. Proceeding in this way, I get,  \begin{eqnarray} A^3 =  \left( \begin{matrix} 4 && 6 && 1 && 5 && 5 \\ 6 && 2 && 3 && 6 && 2  \\ 1 && 3 && 0 && 1 && 2 \\ 5 && 6 && 1 && 4 && 5 \\ 5 && 2 && 2 && 5 && 2 \end{matrix} \right) \end{eqnarray} And, I find, the entry of the 1st row and 2nd column = 6 = entry of the 2nd row and 1st column . Does it mean that there are 6 paths of length 3 from vertex-2 to vertex-1? Cearly it is not true because I have only $1$ path of length $3$ from 2 to 1, namely the sequence: $(2,4,5,1)$. What am I missing? UPDATE: I am attaching a snapshot of Newman's book. He only talks about ""paths"", but never about a ""walk"". Is it a mistake? Newman's book snapshot",,"['matrices', 'graph-theory', 'adjacency-matrix', 'nonnegative-matrices']"
34,Elegant proofs that similar matrices have the same characteristic polynomial?,Elegant proofs that similar matrices have the same characteristic polynomial?,,"It's a simple exercise to show that two similar matrices has the same eigenvalues and eigenvectors (my favorite way is noting that they represent the same linear transformation in different bases). However, to show that two matrices has the same characteristic polynomial it does not suffice to show that they have the same eigenvalues and eigenvectors - one needs to say something smart about the algebraic multiplicities of the eigenvalues. Moreover, we might be working over a field which is not algebraically closed and hence simply ""don't have"" all the eigenvalues. This can be overcome, of course, by working in the algebraic closure of the field, but it complicates the explanation. I'm looking for a proof that is simple and stand-alone as much as possible (the goal is writing an expository article about the subject, so clarity is the most important thing, not efficiency).","It's a simple exercise to show that two similar matrices has the same eigenvalues and eigenvectors (my favorite way is noting that they represent the same linear transformation in different bases). However, to show that two matrices has the same characteristic polynomial it does not suffice to show that they have the same eigenvalues and eigenvectors - one needs to say something smart about the algebraic multiplicities of the eigenvalues. Moreover, we might be working over a field which is not algebraically closed and hence simply ""don't have"" all the eigenvalues. This can be overcome, of course, by working in the algebraic closure of the field, but it complicates the explanation. I'm looking for a proof that is simple and stand-alone as much as possible (the goal is writing an expository article about the subject, so clarity is the most important thing, not efficiency).",,"['matrices', 'similar-matrices', 'characteristic-polynomial']"
35,Computing the Smith Normal Form,Computing the Smith Normal Form,,"Let $A_R$ be the finitely generated abelian group, determined by the relation-matrix $$R := \begin{bmatrix} -6 &  111  & -36 & 6\\ 5  &  -672 & 210 & 74\\ 0 & -255 & 81 & 24\\ -7  &   255       &-81 & -10 \end{bmatrix}$$ Reduce this matrix using Smith Normal Form and determine the isomorphism type of $A_R$. I know that the Smith Normal Form of this matrix is: $$\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 3 & 0 & 0 \\ 0 & 0 & 21 & 0 \\ 0 & 0 & 0 & 0  \end{bmatrix} $$ However, this was computed using Maple and I need to understand the method of computing this manually which I am struggling to grasp. Can anyone help?","Let $A_R$ be the finitely generated abelian group, determined by the relation-matrix $$R := \begin{bmatrix} -6 &  111  & -36 & 6\\ 5  &  -672 & 210 & 74\\ 0 & -255 & 81 & 24\\ -7  &   255       &-81 & -10 \end{bmatrix}$$ Reduce this matrix using Smith Normal Form and determine the isomorphism type of $A_R$. I know that the Smith Normal Form of this matrix is: $$\begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 3 & 0 & 0 \\ 0 & 0 & 21 & 0 \\ 0 & 0 & 0 & 0  \end{bmatrix} $$ However, this was computed using Maple and I need to understand the method of computing this manually which I am struggling to grasp. Can anyone help?",,"['matrices', 'finite-groups', 'abelian-groups', 'matrix-decomposition', 'smith-normal-form']"
36,Parenthesis vs brackets for matrices,Parenthesis vs brackets for matrices,,"When I first learned linear algebra, both the professor and the book used brackets like [ and ] to enclose matrices. However, in my current differential equations textbook, matrices are enclosed by parenthesis, and I suddenly realize everybody else are using them too. So are brackets/parenthesis for enclosing matrices always totally interchangeable?","When I first learned linear algebra, both the professor and the book used brackets like [ and ] to enclose matrices. However, in my current differential equations textbook, matrices are enclosed by parenthesis, and I suddenly realize everybody else are using them too. So are brackets/parenthesis for enclosing matrices always totally interchangeable?",,"['matrices', 'notation']"
37,How do you prove that $tr(B^{T} A )$ is a inner product?,How do you prove that  is a inner product?,tr(B^{T} A ),"Consider the vectorspace of all real $m \times n$ vectors and define an    inner product $\langle A,B\rangle = \operatorname{tr}(B^T   A)$.    ""tr"" stands for ""trace""        which is the sum of the diagonal entries of a matrix. How do you prove that $\operatorname{tr}(B^T   A)$ is indeed a inner product? Kind regards","Consider the vectorspace of all real $m \times n$ vectors and define an    inner product $\langle A,B\rangle = \operatorname{tr}(B^T   A)$.    ""tr"" stands for ""trace""        which is the sum of the diagonal entries of a matrix. How do you prove that $\operatorname{tr}(B^T   A)$ is indeed a inner product? Kind regards",,"['matrices', 'vector-spaces', 'inner-products', 'trace']"
38,What happens if I repeatedly alternately normalize the rows and columns of a matrix?,What happens if I repeatedly alternately normalize the rows and columns of a matrix?,,Here is an algorithm: input matrix M (in-place) divide each row of M by its norm divide each column of M by its norm repeat What will M look like after this has been repeated many times? Can we say anything about what this process does?,Here is an algorithm: input matrix M (in-place) divide each row of M by its norm divide each column of M by its norm repeat What will M look like after this has been repeated many times? Can we say anything about what this process does?,,"['matrices', 'convergence-divergence']"
39,How to find camera position and rotation from a 4x4 matrix?,How to find camera position and rotation from a 4x4 matrix?,,"To find the intrinsic and extrinsic parameters, I calibrated it and the software gave me the extrinsic parameters as a 4 x 4 matrix. This seems to be a 4x4 homogeneous transformation matrix. The values are as follows: $$ \left( \begin{array}     0.211 & -.306 & -.928 & .789 \\     .662 & .742 & -.0947 & .147 \\     .718 & -.595 & .360 & 3.26 \\     0 & 0 &0 & 1 \\ \end{array} \right) $$ I also have the intrinsic parameters of the camera like focal length, principal point, skew, distortion coefficients, etc. How do I extract the camera position and rotation in world coordinates using this matrix? EDIT: On the left, I have shown a cam and its viewing a 3d object, and I take a photo of this 3D object from the cam. The right is what I want. I want to get the world position/rotation of the cam and the world position/rotation and actual size of the image in 3d space.","To find the intrinsic and extrinsic parameters, I calibrated it and the software gave me the extrinsic parameters as a 4 x 4 matrix. This seems to be a 4x4 homogeneous transformation matrix. The values are as follows: I also have the intrinsic parameters of the camera like focal length, principal point, skew, distortion coefficients, etc. How do I extract the camera position and rotation in world coordinates using this matrix? EDIT: On the left, I have shown a cam and its viewing a 3d object, and I take a photo of this 3D object from the cam. The right is what I want. I want to get the world position/rotation of the cam and the world position/rotation and actual size of the image in 3d space."," \left(
\begin{array}
    0.211 & -.306 & -.928 & .789 \\
    .662 & .742 & -.0947 & .147 \\
    .718 & -.595 & .360 & 3.26 \\
    0 & 0 &0 & 1 \\
\end{array}
\right)
","['matrices', 'computer-vision']"
40,The Determinant of a Sum of Matrices,The Determinant of a Sum of Matrices,,"Given $N$ $n \times n$ matrices $\mathsf{A}^{1}, \dots, \mathsf{A}^{N}$,  \begin{align} \det \left( \sum_{i = 1}^{N} \mathsf{A}^{i} \right) = \sum_{\sigma \in S} \det \mathsf{A}^{\sigma}, \end{align} where $S = \{ \sigma \colon \{ 1, \dots, n \} \mapsto \{ 1, \dots, N \} \}$ and $(\mathsf{A}^{\sigma})_{ij} = \mathsf{A}_{ij}^{\sigma(i)}$. Where is this beautiful result published?","Given $N$ $n \times n$ matrices $\mathsf{A}^{1}, \dots, \mathsf{A}^{N}$,  \begin{align} \det \left( \sum_{i = 1}^{N} \mathsf{A}^{i} \right) = \sum_{\sigma \in S} \det \mathsf{A}^{\sigma}, \end{align} where $S = \{ \sigma \colon \{ 1, \dots, n \} \mapsto \{ 1, \dots, N \} \}$ and $(\mathsf{A}^{\sigma})_{ij} = \mathsf{A}_{ij}^{\sigma(i)}$. Where is this beautiful result published?",,"['matrices', 'reference-request']"
41,Is there such a thing as a matrix of functions?,Is there such a thing as a matrix of functions?,,"Do we ever put functions as entries of a matrix? If so, are these matrices used in linear algebra or do they have some other special use? There have been minor not neccessarily conflicts per se, but disagreements on the nature of this question and so I am adding a little statement below to clear this up. I have noticed that ""function"" has been interpreted two ways within the answers. An actual raw function such as merely writing ""f"". Such a concept is beyond my current understanding (unless I am being stupid somehow), but it is interesting nonetheless. A function call returning a value. This is primarily what I meant in my post. Either one of these is valid. In fact, I think the broadness of this question dictates the fact that people will interpret it differently. In essence, your mileage will vary, and both are so similar from my standpoint that they are all good answers (or good examples if not standalone answers).","Do we ever put functions as entries of a matrix? If so, are these matrices used in linear algebra or do they have some other special use? There have been minor not neccessarily conflicts per se, but disagreements on the nature of this question and so I am adding a little statement below to clear this up. I have noticed that ""function"" has been interpreted two ways within the answers. An actual raw function such as merely writing ""f"". Such a concept is beyond my current understanding (unless I am being stupid somehow), but it is interesting nonetheless. A function call returning a value. This is primarily what I meant in my post. Either one of these is valid. In fact, I think the broadness of this question dictates the fact that people will interpret it differently. In essence, your mileage will vary, and both are so similar from my standpoint that they are all good answers (or good examples if not standalone answers).",,"['matrices', 'terminology']"
42,Why do we call complex numbers “numbers” but we don’t consider 2-vectors numbers?,Why do we call complex numbers “numbers” but we don’t consider 2-vectors numbers?,,We refer to complex numbers as numbers. However we refer to vectors as arrays of numbers. There doesn’t seem to be anything that makes one more numeric than the other. Is this just a quirk of history and naming or is there something more fundamental?,We refer to complex numbers as numbers. However we refer to vectors as arrays of numbers. There doesn’t seem to be anything that makes one more numeric than the other. Is this just a quirk of history and naming or is there something more fundamental?,,"['matrices', 'complex-numbers', 'terminology', 'philosophy']"
43,Why do the $n \times n$ non-singular matrices form an “open” set?,Why do the  non-singular matrices form an “open” set?,n \times n,"Why is the set of $n\times n$ real, non-singular matrices an  open subset of the set of all $n\times n$ real matrices? I don’t quite understand what ”open” means in this context. Thank you.","Why is the set of real, non-singular matrices an  open subset of the set of all real matrices? I don’t quite understand what ”open” means in this context. Thank you.",n\times n n\times n,['matrices']
44,Determine a matrix knowing its eigenvalues and eigenvectors,Determine a matrix knowing its eigenvalues and eigenvectors,,"I read through similar questions, but I couldn't find an answer to this: How do you determine the symmetric matrix A if you know: $\lambda_1 = 1, \  eigenvector_1 = \pmatrix{1& 0&-1}^T;$ $\lambda_2 = -2, \ eigenvector_2 = \pmatrix{1& 1& 1}^T;$ $\lambda_3 = 2,  \ eigenvector_3 = \pmatrix{-1& 2& -1}^T;$ I tried to solve it as an equation system for each line, but it didn't work somehow. I tried to find the inverse of the eigenvectors, but it brought a wrong matrix. Do you know how to solve it? Thanks!","I read through similar questions, but I couldn't find an answer to this: How do you determine the symmetric matrix A if you know: $\lambda_1 = 1, \  eigenvector_1 = \pmatrix{1& 0&-1}^T;$ $\lambda_2 = -2, \ eigenvector_2 = \pmatrix{1& 1& 1}^T;$ $\lambda_3 = 2,  \ eigenvector_3 = \pmatrix{-1& 2& -1}^T;$ I tried to solve it as an equation system for each line, but it didn't work somehow. I tried to find the inverse of the eigenvectors, but it brought a wrong matrix. Do you know how to solve it? Thanks!",,"['matrices', 'eigenvalues-eigenvectors']"
45,Is division of matrices possible?,Is division of matrices possible?,,"Is it possible to divide a matrix by another? If yes, What will be the result of $\dfrac AB$ if $$         A = \begin{pmatrix}         a & b \\         c & d \\         \end{pmatrix},         B = \begin{pmatrix}         w & x \\         y & z \\         \end{pmatrix}? $$","Is it possible to divide a matrix by another? If yes, What will be the result of $\dfrac AB$ if $$         A = \begin{pmatrix}         a & b \\         c & d \\         \end{pmatrix},         B = \begin{pmatrix}         w & x \\         y & z \\         \end{pmatrix}? $$",,"['matrices', 'divisibility']"
46,How to calculate the gradient of $x^T A x$? [duplicate],How to calculate the gradient of ? [duplicate],x^T A x,"This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 4 years ago . I am watching the following video lecture: https://www.youtube.com/watch?v=G_p4QJrjdOw In there, he talks about calculating gradient of $ x^{T}Ax $ and he does that using the concept of exterior derivative. The proof goes as follows: $ y = x^{T}Ax$ $ dy = dx^{T}Ax + x^{T}Adx = x^{T}(A+A^{T})dx$ (using trace property of matrices) $ dy = (\nabla y)^{T} dx $ and because the rule is true for all $dx$ $ \nabla y = x^{T}(A+A^{T})$ It seems that in step 2, some form of product rule for differentials is applied. I am familiar with product rule for single variable calculus, but I am not understanding how product rule was applied to a multi-variate function expressed in matrix form. It would be great if somebody could point me to a mathematical theorem that allows Step 2 in the above proof. Thanks! Ajay","This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 4 years ago . I am watching the following video lecture: https://www.youtube.com/watch?v=G_p4QJrjdOw In there, he talks about calculating gradient of $ x^{T}Ax $ and he does that using the concept of exterior derivative. The proof goes as follows: $ y = x^{T}Ax$ $ dy = dx^{T}Ax + x^{T}Adx = x^{T}(A+A^{T})dx$ (using trace property of matrices) $ dy = (\nabla y)^{T} dx $ and because the rule is true for all $dx$ $ \nabla y = x^{T}(A+A^{T})$ It seems that in step 2, some form of product rule for differentials is applied. I am familiar with product rule for single variable calculus, but I am not understanding how product rule was applied to a multi-variate function expressed in matrix form. It would be great if somebody could point me to a mathematical theorem that allows Step 2 in the above proof. Thanks! Ajay",,"['matrices', 'multivariable-calculus', 'derivatives', 'differential-forms', 'quadratic-forms']"
47,When does a Square Matrix have an LU Decomposition?,When does a Square Matrix have an LU Decomposition?,,"When can we split a square matrix (rows = columns) into it’s LU decomposition?  The LUP (LU Decomposition with pivoting) always exists; however, a true LU decomposition does not always exist.  How do we tell if it does/doesn't exist?  (Note: decomposition and factorization are equivalent in this article) From the Wikipedia article on LU decompositions : Any square matrix $A$ admits an LUP factorization. If $A$ is invertible, then it admits an LU (or LDU) factorization if and only if all its leading principal minors are non-zero. If $A$ is a singular matrix of rank $k$, then it admits an LU factorization if the first $k$ leading principal minors are non-zero, although the converse is not true. This implies that for a square matrix: LUP always exists (We can use this to quickly figure out the determinant). If the matrix is invertible (the determinant is not 0), then a pure LU decomposition exists only if the leading principal minors are not 0. If the matrix is not invertible (the determinant is 0), then we can't know if there is a pure LU decomposition. The problem is this third statement here. “If $A$ is a singular matrix of rank $k$, then it admits an LU factorization if the first $k$ leading principal minors are non-zero”, gives us a way to find out if LU decomposition exists for a singular (non-invertible) matrix.  However, it then says, “although the converse is not true”, implying that even if a leading principal minor is 0, that we could still have a valid LU decomposition that we can't detect. This leads us back to the question: is there a way of truly knowing whether a matrix has an LU decomposition?","When can we split a square matrix (rows = columns) into it’s LU decomposition?  The LUP (LU Decomposition with pivoting) always exists; however, a true LU decomposition does not always exist.  How do we tell if it does/doesn't exist?  (Note: decomposition and factorization are equivalent in this article) From the Wikipedia article on LU decompositions : Any square matrix $A$ admits an LUP factorization. If $A$ is invertible, then it admits an LU (or LDU) factorization if and only if all its leading principal minors are non-zero. If $A$ is a singular matrix of rank $k$, then it admits an LU factorization if the first $k$ leading principal minors are non-zero, although the converse is not true. This implies that for a square matrix: LUP always exists (We can use this to quickly figure out the determinant). If the matrix is invertible (the determinant is not 0), then a pure LU decomposition exists only if the leading principal minors are not 0. If the matrix is not invertible (the determinant is 0), then we can't know if there is a pure LU decomposition. The problem is this third statement here. “If $A$ is a singular matrix of rank $k$, then it admits an LU factorization if the first $k$ leading principal minors are non-zero”, gives us a way to find out if LU decomposition exists for a singular (non-invertible) matrix.  However, it then says, “although the converse is not true”, implying that even if a leading principal minor is 0, that we could still have a valid LU decomposition that we can't detect. This leads us back to the question: is there a way of truly knowing whether a matrix has an LU decomposition?",,"['matrices', 'algorithms', 'matrix-decomposition']"
48,'Trace trick' for expectations of quadratic forms,'Trace trick' for expectations of quadratic forms,,"I am trying to understand the proof for the Kullback-Leibler divergence between two multivariate normal distributions . On the way, a sort of trace trick is applied for the expectation of the quadratic form $$E[ (x-\mu)^T \Sigma^{-1} (x-\mu) ]= \operatorname{trace}(E[(x-\mu)(x-\mu)^T)] \Sigma^{-1}),$$ where $x$ is MV-normal with mean $\mu$ and covariance matrix $\Sigma$ . The expectation is taken over $x$ . I would like to understand why this identity holds. I think more than one step is taken at once. I believe, $\operatorname{trace}(E[(x-\mu)(x-\mu)^T] \Sigma^{-1})$ = $\operatorname{trace}(E[(x-\mu) \Sigma^{-1} (x-\mu)^T])$ , but where does the trace come from?","I am trying to understand the proof for the Kullback-Leibler divergence between two multivariate normal distributions . On the way, a sort of trace trick is applied for the expectation of the quadratic form where is MV-normal with mean and covariance matrix . The expectation is taken over . I would like to understand why this identity holds. I think more than one step is taken at once. I believe, = , but where does the trace come from?","E[ (x-\mu)^T \Sigma^{-1} (x-\mu) ]= \operatorname{trace}(E[(x-\mu)(x-\mu)^T)] \Sigma^{-1}), x \mu \Sigma x \operatorname{trace}(E[(x-\mu)(x-\mu)^T] \Sigma^{-1}) \operatorname{trace}(E[(x-\mu) \Sigma^{-1} (x-\mu)^T])","['matrices', 'statistics', 'expected-value', 'trace']"
49,Outer Product of Two Matrices?,Outer Product of Two Matrices?,,"How would I go about calculating the outer product of two matrices of 2 dimensions each?  From what I can find, outer product seems to be the product of two vectors, $u$ and the transpose of another vector, $v^T$. As an example, how would I calculate the outer product of $A$ and $B$, where  $$A = \begin{pmatrix}1 & 2 \\ 3 & 4\end{pmatrix} \qquad B = \begin{pmatrix}5 & 6 & 7 \\ 8 & 9 & 10\end{pmatrix}$$","How would I go about calculating the outer product of two matrices of 2 dimensions each?  From what I can find, outer product seems to be the product of two vectors, $u$ and the transpose of another vector, $v^T$. As an example, how would I calculate the outer product of $A$ and $B$, where  $$A = \begin{pmatrix}1 & 2 \\ 3 & 4\end{pmatrix} \qquad B = \begin{pmatrix}5 & 6 & 7 \\ 8 & 9 & 10\end{pmatrix}$$",,"['matrices', 'tensor-products']"
50,Is trace invariant under cyclic permutation with rectangular matrices?,Is trace invariant under cyclic permutation with rectangular matrices?,,"I'm working with trace of matrices. Trace is defined for square matrix and there are some useful rule, i.e. $\text{tr}(AB) = \text{tr}(BA)$ , with $A$ and $B$ square, and more in general trace is invariant under cyclic permutation. I was wondering if the formula $\text{tr}(AB) = \text{tr}(BA)$ holds even if $A$ and $B$ are rectangular, namely $A$ is $n$ -by- $m$ and $B$ is $m$ -by- $n$ . I figured out that if one completes the involved matrices to be square by adding zero entries in the right places, then the formula still works... but I want to be sure about this!","I'm working with trace of matrices. Trace is defined for square matrix and there are some useful rule, i.e. , with and square, and more in general trace is invariant under cyclic permutation. I was wondering if the formula holds even if and are rectangular, namely is -by- and is -by- . I figured out that if one completes the involved matrices to be square by adding zero entries in the right places, then the formula still works... but I want to be sure about this!",\text{tr}(AB) = \text{tr}(BA) A B \text{tr}(AB) = \text{tr}(BA) A B A n m B m n,"['matrices', 'trace']"
51,Trace of an Inverse Matrix,Trace of an Inverse Matrix,,"I want to know if there is a way to simplify, or a closed form solution of $tr(\Sigma^{-1})$ where $\Sigma$ is a symmetric positive definite matrix.","I want to know if there is a way to simplify, or a closed form solution of $tr(\Sigma^{-1})$ where $\Sigma$ is a symmetric positive definite matrix.",,"['matrices', 'trace']"
52,Can you multiply 3 matrices simultaneouly?,Can you multiply 3 matrices simultaneouly?,,"I know that the algorithm for multiplying 2 matrices is defined as: $$(AB)_{ij} = (\text{row }i\text{ of matrix }A) ⋅ (\text{column }j\text{ of matrix }B)$$ And I know that matrix multiplication is associative. So in the case of 3 matrices: $$(A ⋅ B) ⋅ C = A ⋅ (B ⋅ C)$$ Is there an algorithm for matrix multiplication that can multiply 3 matrices simultaneously? For example, could one evaluate the expression $ABC$ without first evaluating either of: $(AB) ⋅ C$ $A ⋅ (BC)$ Additional questions I am in high school, so I am not sure if this is the correct terminology, but I read somewhere about binary operations, where for instance, an operation such as normal multiplication takes 2 elements (such as 2 real numbers) and produces an output, and you can't multiply 3 numbers together simultaneously. Is matrix multiplication a binary operation, where you can't multiply 3 matrices simultaneously? Is $A ⋅ B ⋅ C$ defined as $(A⋅ B)⋅ C$ or $A⋅ (B⋅ C)$ ? Is the omission of the parentheses in $A⋅ B⋅ C$ (matrices) just a form of notation? What is the most precise way to interpret expressions of multiplication or addition (associative operations) with more than 3 variables - where there are no brackets, such as $A⋅B⋅C⋅D$ ?","I know that the algorithm for multiplying 2 matrices is defined as: And I know that matrix multiplication is associative. So in the case of 3 matrices: Is there an algorithm for matrix multiplication that can multiply 3 matrices simultaneously? For example, could one evaluate the expression without first evaluating either of: Additional questions I am in high school, so I am not sure if this is the correct terminology, but I read somewhere about binary operations, where for instance, an operation such as normal multiplication takes 2 elements (such as 2 real numbers) and produces an output, and you can't multiply 3 numbers together simultaneously. Is matrix multiplication a binary operation, where you can't multiply 3 matrices simultaneously? Is defined as or ? Is the omission of the parentheses in (matrices) just a form of notation? What is the most precise way to interpret expressions of multiplication or addition (associative operations) with more than 3 variables - where there are no brackets, such as ?",(AB)_{ij} = (\text{row }i\text{ of matrix }A) ⋅ (\text{column }j\text{ of matrix }B) (A ⋅ B) ⋅ C = A ⋅ (B ⋅ C) ABC (AB) ⋅ C A ⋅ (BC) A ⋅ B ⋅ C (A⋅ B)⋅ C A⋅ (B⋅ C) A⋅ B⋅ C A⋅B⋅C⋅D,"['matrices', 'binary-operations']"
53,Matrix determinant contradicts corresponding box volume – how is it possible?,Matrix determinant contradicts corresponding box volume – how is it possible?,,"I'm taking an online linear algebra course and got stuck with a problem (it's not for credit)... Since I don't know anyone qualified in person, this is last resort. Pretty sure I've made some trivial error, but can't find it... If you can spot it, please let me know. Thank you in advance. As we know, absolute value of determinant of a square matrix is equal to the volume of the parallelepiped with edges corresponding to the matrix's columns (or rows). Consider matrix $A$ with columns $(1,1,0)^T,(0,1,1)^T,(1,0,1)^T$. Its determinant is 2. But it seems to me that the volume of the cube with edges corresponding to A's columns, is not 2… I mean, all three column vectors clearly have the same length, and it is $\sqrt2$, therefore volume of the cube with edges corresponding to these vectors must be $\sqrt2^3=2\sqrt2$. What am I doing wrong?","I'm taking an online linear algebra course and got stuck with a problem (it's not for credit)... Since I don't know anyone qualified in person, this is last resort. Pretty sure I've made some trivial error, but can't find it... If you can spot it, please let me know. Thank you in advance. As we know, absolute value of determinant of a square matrix is equal to the volume of the parallelepiped with edges corresponding to the matrix's columns (or rows). Consider matrix $A$ with columns $(1,1,0)^T,(0,1,1)^T,(1,0,1)^T$. Its determinant is 2. But it seems to me that the volume of the cube with edges corresponding to A's columns, is not 2… I mean, all three column vectors clearly have the same length, and it is $\sqrt2$, therefore volume of the cube with edges corresponding to these vectors must be $\sqrt2^3=2\sqrt2$. What am I doing wrong?",,"['matrices', 'determinant', 'volume']"
54,"extracting rotation, scale values from 2d transformation matrix","extracting rotation, scale values from 2d transformation matrix",,"How can I extract rotation and scale values from a 2D transformation matrix? matrix = [1, 0, 0, 1, 0, 0]  matrix.rotate(45 / 180 * PI) matrix.scale(3, 4) matrix.translate(50, 100) matrix.rotate(30 / 180 * PI) matrix.scale(-2, 4) Now my matrix have values [a, b, c, d, tx, ty] . Lets forget about the processes above and imagine that we have only the values a, b, c, d, tx, and ty. How can I find final rotation and scale values?","How can I extract rotation and scale values from a 2D transformation matrix? matrix = [1, 0, 0, 1, 0, 0]  matrix.rotate(45 / 180 * PI) matrix.scale(3, 4) matrix.translate(50, 100) matrix.rotate(30 / 180 * PI) matrix.scale(-2, 4) Now my matrix have values [a, b, c, d, tx, ty] . Lets forget about the processes above and imagine that we have only the values a, b, c, d, tx, and ty. How can I find final rotation and scale values?",,['matrices']
55,Gradient of squared Frobenius norm of a matrix,Gradient of squared Frobenius norm of a matrix,,"In linear regression, the loss function is expressed as $$ W \mapsto \frac1N \left\| X W - Y \right\|_{\text{F}}^2 $$ where the matrices $X$ and $Y$ are given. Taking the gradient yields $$ W \mapsto \frac 2N \, X^T( X W - Y ) $$ Why is this so?","In linear regression, the loss function is expressed as where the matrices and are given. Taking the gradient yields Why is this so?"," W \mapsto \frac1N \left\| X W - Y \right\|_{\text{F}}^2  X Y  W \mapsto \frac 2N \, X^T( X W - Y ) ","['matrices', 'derivatives', 'matrix-calculus', 'least-squares', 'scalar-fields']"
56,Showing $\prod\limits_{i<j} \frac{x_i-x_j}{i-j}$ is an integer,Showing  is an integer,\prod\limits_{i<j} \frac{x_i-x_j}{i-j},"Let $x_1,...,x_n$ be distinct integers. Prove that $$\prod_{i<j} \frac{x_i-x_j}{i-j}\in \mathbb Z$$ I know there is a solution using determinant of a matrix, but I can't remember it now. Any help will be appreciated.","Let $x_1,...,x_n$ be distinct integers. Prove that $$\prod_{i<j} \frac{x_i-x_j}{i-j}\in \mathbb Z$$ I know there is a solution using determinant of a matrix, but I can't remember it now. Any help will be appreciated.",,"['number-theory', 'matrices', 'determinant']"
57,Why is the Operator Norm so hard to calculate?,Why is the Operator Norm so hard to calculate?,,"I recently took a better look at the operator norm defined on a matrix $\mathbf A \in \Bbb{K}^{n\times n}$ as follows: $$ \|\mathbf A\|_p=\sup\{\|\mathbf Ax\|_p \mid x\in\Bbb{K}^n\land\|x\|=1\} $$ The first time I looked at this I thought ""ok, lets calculate it for a few example matrices"". I started with $n = 3$ and $p = 2$, just to start ""simple"". Let $$ \mathbf A = \left[\begin{matrix}a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33}\end{matrix}\right]\quad a_{ij}\in\Bbb{K}^n $$ Now if we're going to minimize $\|\mathbf Ax\|_2$ ($ = \|\mathbf Ax\|$), we might as well make it easy on ourselves and only minimize $\|\mathbf Ax\|^2$ so as to not worry about that annoying radical. We get $$ \begin{align} \|\mathbf Ax\|^2 & = (a_{11}x_1 + a_{12}x_2 + a_{13}x_3)^2 + (a_{21}x_1 + a_{22}x_2 + a_{23}x_3)^2 + (a_{31}x_1 + a_{32}x_2 + a_{33}x_3)^2 \\ & = Ax_1^2 + Bx_2^2 + Cx_3^2 + Dx_1x_2 + Ex_1x_3 + Fx_2x_3 \end{align} $$ where $$ \begin{align} A & = a_{11}^2+a_{21}^2+a_{31}^2 \\ B & = a_{12}^2+a_{22}^2+a_{32}^2 \\ C & = a_{13}^2+a_{23}^2+a_{33}^2 \\ D & = 2(a_{11}a_{12} + a_{21}a_{22} + a_{31}a_{33}) \\ E & = 2(a_{11}a_{13} + a_{21}a_{23} + a_{31}a_{33}) \\ F & = 2(a_{12}a_{13} + a_{22}a_{23} + a_{32}a_{33}) \end{align} $$ Now lets define  $$ G(x_1,\ x_2,\ x_3) = Ax_1^2+Bx_2^2+Cx_3^2+Dx_1x_2+Ex_1x_3+Fx_2x_3 $$ So if we want to minimize $||\mathbf Ax||^2$, we're either going to have to minimize $$ N(x_1,\ x_2,\ x_3) = \frac{G(x_1,\ x_2,\ x_3)}{x_1^2+x_2^2+x_3^2} $$ or simply minimize $G$ with the constraint $g(x_1,\ x_2,\ x_3) = x_1^2 + x_2^2 + x_3^2 = 1$. The latter seemed easier to me, so I gave it a shot using Lagrange multipliers. As usual, I defined  $$ \mathcal{L}(x_1,\ x_2,\ x_3,\ \lambda) = G(x_1,\ x_2,\ x_3)-\lambda g(x_1,\ x_2,\ x_3) $$ setting it's gradient to zero gives $$ \nabla \mathcal L = 0 \implies \begin{cases} 2(A - \lambda)x_1 + Dx_2 + Ex_3 & = 0 \\ Dx_1 + 2(B - \lambda)x_2 + Fx_3 & = 0 \\ Ex_1 + Fx_2 + 2(C - \lambda)x_3 & = 0 \\ x_1^2 + x_2^2 + x_3^2 - 1 & = 0 \end{cases} $$ Now this is where I really started to get stuck. I tried solving the first three equations for $x_1,\ x_2,$ and $x_3$ but didn't end up with anything I could use. I tried solving for $x_1$ in terms of $x_2,\ x_3,$ and $\lambda$, then $x_2$ in terms of $x_3$ and $\lambda$, and then subbing that all into the third equation, but ended up with either $x_3 = 0$ or  $$ 4\lambda^3 - 4\lambda^2(A+B+C) + \lambda(4AB+4AC+4BC-D2+E^2+F^2)-4ABC-AF^2-BE^2+CD^2+DEF = 0 $$ which, although technically solvable for $\lambda$ via the cubic equation, would be incredibly messy. Now, I probably created my own roadblock for this problem, because I didn't want to think about the system of equations logically and just wanted to bash it out. Regardless of my approach, it seems like the operator norm is a very difficult thing to calculate, and I only analyzed the case where $n = 3$ and $p = 2$. What about the general case? What if $n = 75$ and $p = 9/4$? How on earth would you calculate it then? The questions above are rhetorical, however, and my actual question is as follows: Why define such an ordinary norm for matrices which is so difficult to calculate in general? I see the operator norm everywhere, and it seems like the standard norm for a lot of theorems (unless I'm mistaken and ||A|| just means any matrix norm). So why would we define such a standard norm in a way that is so difficult to calculate? What's the point? Is it easy to work with in theorems? I get that it intuitively makes sense as a norm, but it can't possibly be that easy to work with, especially in comparison to things like the Frobenius norm. So why do we care about this definition?","I recently took a better look at the operator norm defined on a matrix $\mathbf A \in \Bbb{K}^{n\times n}$ as follows: $$ \|\mathbf A\|_p=\sup\{\|\mathbf Ax\|_p \mid x\in\Bbb{K}^n\land\|x\|=1\} $$ The first time I looked at this I thought ""ok, lets calculate it for a few example matrices"". I started with $n = 3$ and $p = 2$, just to start ""simple"". Let $$ \mathbf A = \left[\begin{matrix}a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33}\end{matrix}\right]\quad a_{ij}\in\Bbb{K}^n $$ Now if we're going to minimize $\|\mathbf Ax\|_2$ ($ = \|\mathbf Ax\|$), we might as well make it easy on ourselves and only minimize $\|\mathbf Ax\|^2$ so as to not worry about that annoying radical. We get $$ \begin{align} \|\mathbf Ax\|^2 & = (a_{11}x_1 + a_{12}x_2 + a_{13}x_3)^2 + (a_{21}x_1 + a_{22}x_2 + a_{23}x_3)^2 + (a_{31}x_1 + a_{32}x_2 + a_{33}x_3)^2 \\ & = Ax_1^2 + Bx_2^2 + Cx_3^2 + Dx_1x_2 + Ex_1x_3 + Fx_2x_3 \end{align} $$ where $$ \begin{align} A & = a_{11}^2+a_{21}^2+a_{31}^2 \\ B & = a_{12}^2+a_{22}^2+a_{32}^2 \\ C & = a_{13}^2+a_{23}^2+a_{33}^2 \\ D & = 2(a_{11}a_{12} + a_{21}a_{22} + a_{31}a_{33}) \\ E & = 2(a_{11}a_{13} + a_{21}a_{23} + a_{31}a_{33}) \\ F & = 2(a_{12}a_{13} + a_{22}a_{23} + a_{32}a_{33}) \end{align} $$ Now lets define  $$ G(x_1,\ x_2,\ x_3) = Ax_1^2+Bx_2^2+Cx_3^2+Dx_1x_2+Ex_1x_3+Fx_2x_3 $$ So if we want to minimize $||\mathbf Ax||^2$, we're either going to have to minimize $$ N(x_1,\ x_2,\ x_3) = \frac{G(x_1,\ x_2,\ x_3)}{x_1^2+x_2^2+x_3^2} $$ or simply minimize $G$ with the constraint $g(x_1,\ x_2,\ x_3) = x_1^2 + x_2^2 + x_3^2 = 1$. The latter seemed easier to me, so I gave it a shot using Lagrange multipliers. As usual, I defined  $$ \mathcal{L}(x_1,\ x_2,\ x_3,\ \lambda) = G(x_1,\ x_2,\ x_3)-\lambda g(x_1,\ x_2,\ x_3) $$ setting it's gradient to zero gives $$ \nabla \mathcal L = 0 \implies \begin{cases} 2(A - \lambda)x_1 + Dx_2 + Ex_3 & = 0 \\ Dx_1 + 2(B - \lambda)x_2 + Fx_3 & = 0 \\ Ex_1 + Fx_2 + 2(C - \lambda)x_3 & = 0 \\ x_1^2 + x_2^2 + x_3^2 - 1 & = 0 \end{cases} $$ Now this is where I really started to get stuck. I tried solving the first three equations for $x_1,\ x_2,$ and $x_3$ but didn't end up with anything I could use. I tried solving for $x_1$ in terms of $x_2,\ x_3,$ and $\lambda$, then $x_2$ in terms of $x_3$ and $\lambda$, and then subbing that all into the third equation, but ended up with either $x_3 = 0$ or  $$ 4\lambda^3 - 4\lambda^2(A+B+C) + \lambda(4AB+4AC+4BC-D2+E^2+F^2)-4ABC-AF^2-BE^2+CD^2+DEF = 0 $$ which, although technically solvable for $\lambda$ via the cubic equation, would be incredibly messy. Now, I probably created my own roadblock for this problem, because I didn't want to think about the system of equations logically and just wanted to bash it out. Regardless of my approach, it seems like the operator norm is a very difficult thing to calculate, and I only analyzed the case where $n = 3$ and $p = 2$. What about the general case? What if $n = 75$ and $p = 9/4$? How on earth would you calculate it then? The questions above are rhetorical, however, and my actual question is as follows: Why define such an ordinary norm for matrices which is so difficult to calculate in general? I see the operator norm everywhere, and it seems like the standard norm for a lot of theorems (unless I'm mistaken and ||A|| just means any matrix norm). So why would we define such a standard norm in a way that is so difficult to calculate? What's the point? Is it easy to work with in theorems? I get that it intuitively makes sense as a norm, but it can't possibly be that easy to work with, especially in comparison to things like the Frobenius norm. So why do we care about this definition?",,"['matrices', 'definition', 'normed-spaces', 'matrix-norms']"
58,Relation between Cholesky and SVD,Relation between Cholesky and SVD,,"When we have a symmetric matrix $A = LL^*$, we can obtain L using Cholesky decomposition of $A$ ($L^*$ is $L$ transposed). Can anyone tell me how we can get this same $L$ using SVD or Eigen decomposition? Thank you.","When we have a symmetric matrix $A = LL^*$, we can obtain L using Cholesky decomposition of $A$ ($L^*$ is $L$ transposed). Can anyone tell me how we can get this same $L$ using SVD or Eigen decomposition? Thank you.",,"['matrices', 'eigenvalues-eigenvectors', 'matrix-decomposition', 'svd', 'cholesky-decomposition']"
59,Derivative of matrix exponential w.r.t. to each element of the matrix,Derivative of matrix exponential w.r.t. to each element of the matrix,,I have $x= \exp(At)$ where $A$ is a matrix. I would like to find derivative of $x$ with respect to each element of $A$ . Could anyone help with this problem?,I have where is a matrix. I would like to find derivative of with respect to each element of . Could anyone help with this problem?,x= \exp(At) A x A,"['matrices', 'derivatives', 'matrix-calculus', 'matrix-exponential']"
60,Differentiate $f(x)=x^TAx$,Differentiate,f(x)=x^TAx,"Calculate the differential of the function $f: \Bbb R^n \to \Bbb R$ given by $$f(x) = x^T A x$$ with $A$ symmetric. Also, differentiate this function with respect to $x^T$ . How exactly does this work in the case of vectors and matrices? Could anyone please help me out?","Calculate the differential of the function given by with symmetric. Also, differentiate this function with respect to . How exactly does this work in the case of vectors and matrices? Could anyone please help me out?",f: \Bbb R^n \to \Bbb R f(x) = x^T A x A x^T,"['matrices', 'multivariable-calculus', 'derivatives', 'quadratic-forms', 'scalar-fields']"
61,Properties of the cone of positive semidefinite matrices,Properties of the cone of positive semidefinite matrices,,"The set of positive semidefinite symmetric real matrices forms a cone. We can define an order over the set of matrices by saying $X \geq Y$ if and only if $X - Y$ is positive semidefinite. I suspect that this order does not have the lattice property, but I would still like to know which matrices are candidates for the meet and join of two matrices. In other words, let $P$ be the cone of positive semidefinite matrices. Is there a nice characterization of the set $(X+P)\cap (Y+P)$ , for two given matrices? What are the minimal points in this intersection?","The set of positive semidefinite symmetric real matrices forms a cone. We can define an order over the set of matrices by saying if and only if is positive semidefinite. I suspect that this order does not have the lattice property, but I would still like to know which matrices are candidates for the meet and join of two matrices. In other words, let be the cone of positive semidefinite matrices. Is there a nice characterization of the set , for two given matrices? What are the minimal points in this intersection?",X \geq Y X - Y P (X+P)\cap (Y+P),"['matrices', 'order-theory', 'lattice-orders', 'positive-semidefinite', 'vector-lattices']"
62,What kind of matrix is this and why does this happen?,What kind of matrix is this and why does this happen?,,"So I was studying Markov chains and I came across this matrix  \begin{align*}P=\left( \begin{array}{ccccc}         0 & \frac{1}{4} & \frac{3}{4} & 0 & 0\\         \frac{1}{4} & 0 & 0 & \frac{1}{4} & \frac{1}{2}\\         \frac{1}{2} & 0 & 0 & \frac{1}{4}& \frac{1}{4}\\         0 & \frac{1}{4} & \frac{3}{4} & 0& 0\\         0 & \frac{1}{4} & \frac{3}{4} & 0& 0\\     \end{array} \right).\end{align*} I noticed (by brute force) that \begin{align*}P^2=\left( \begin{array}{ccccc}         \frac{7}{16} & 0 & 0 & \frac{1}{4} & \frac{5}{16}\\         0 & \frac{1}{4} & \frac{3}{4} & 0& 0\\         0 & \frac{1}{4} & \frac{3}{4} & 0& 0\\         \frac{3}{8} & 0 & 0 & \frac{1}{4}& \frac{1}{2}\\         \frac{7}{16} & 0 & 0 & \frac{1}{4} & \frac{5}{16}\\     \end{array} \right),\end{align*} and \begin{align*}P^3=\left( \begin{array}{ccccc}         0 & \frac{1}{4} & \frac{3}{4} & 0& 0\\         \frac{7}{16} & 0 & 0 & \frac{1}{4} & \frac{5}{16}\\         \frac{7}{16} & 0 & 0 & \frac{1}{4} & \frac{5}{16}\\         0 & \frac{1}{4} & \frac{3}{4} & 0& 0\\         0 & \frac{1}{4} & \frac{3}{4} & 0& 0\\     \end{array} \right).\end{align*} In fact; using a computer I found that every even power takes the form of the $P^2$ matrix and every odd power takes the form of the $P^3$ matrix. I just wanted to know why that oscillation occurs? Is there a special name for the kind of matrix that $P$ is for it to exhibit that kind of behaviour?","So I was studying Markov chains and I came across this matrix  \begin{align*}P=\left( \begin{array}{ccccc}         0 & \frac{1}{4} & \frac{3}{4} & 0 & 0\\         \frac{1}{4} & 0 & 0 & \frac{1}{4} & \frac{1}{2}\\         \frac{1}{2} & 0 & 0 & \frac{1}{4}& \frac{1}{4}\\         0 & \frac{1}{4} & \frac{3}{4} & 0& 0\\         0 & \frac{1}{4} & \frac{3}{4} & 0& 0\\     \end{array} \right).\end{align*} I noticed (by brute force) that \begin{align*}P^2=\left( \begin{array}{ccccc}         \frac{7}{16} & 0 & 0 & \frac{1}{4} & \frac{5}{16}\\         0 & \frac{1}{4} & \frac{3}{4} & 0& 0\\         0 & \frac{1}{4} & \frac{3}{4} & 0& 0\\         \frac{3}{8} & 0 & 0 & \frac{1}{4}& \frac{1}{2}\\         \frac{7}{16} & 0 & 0 & \frac{1}{4} & \frac{5}{16}\\     \end{array} \right),\end{align*} and \begin{align*}P^3=\left( \begin{array}{ccccc}         0 & \frac{1}{4} & \frac{3}{4} & 0& 0\\         \frac{7}{16} & 0 & 0 & \frac{1}{4} & \frac{5}{16}\\         \frac{7}{16} & 0 & 0 & \frac{1}{4} & \frac{5}{16}\\         0 & \frac{1}{4} & \frac{3}{4} & 0& 0\\         0 & \frac{1}{4} & \frac{3}{4} & 0& 0\\     \end{array} \right).\end{align*} In fact; using a computer I found that every even power takes the form of the $P^2$ matrix and every odd power takes the form of the $P^3$ matrix. I just wanted to know why that oscillation occurs? Is there a special name for the kind of matrix that $P$ is for it to exhibit that kind of behaviour?",,['matrices']
63,What is a principal minor of a matrix?,What is a principal minor of a matrix?,,I was going through the book on operation research by Hamdy A.Taha. It referred to principal minor of a hessian matrix. Can someone explain what is meant by a principal minor? Is it different from 'minor of a matrix'?,I was going through the book on operation research by Hamdy A.Taha. It referred to principal minor of a hessian matrix. Can someone explain what is meant by a principal minor? Is it different from 'minor of a matrix'?,,"['matrices', 'optimization', 'operations-research']"
64,How does the SVD solve the least squares problem?,How does the SVD solve the least squares problem?,,"How do I prove that the least-squares solution for $$\text{minimize} \quad \|Ax-b\|_2$$ is $A^{+} b$, where $A^{+}$ is the pseudoinverse of $A$?","How do I prove that the least-squares solution for $$\text{minimize} \quad \|Ax-b\|_2$$ is $A^{+} b$, where $A^{+}$ is the pseudoinverse of $A$?",,"['matrices', 'optimization', 'least-squares', 'svd', 'quadratic-programming']"
65,Derivatives of eigenvalues,Derivatives of eigenvalues,,"Say I have a hermitian matrix A with elements $A_{ij}$, given the eigenvalues $e_p$, (and optional eigenvectors $v_p$) Is there an easy(ish) way to calculate $\frac{\partial e_p}{\partial A_{ij}}$? The elements represent potentials between particles and so any deviations would also leave the matrix hermitian. [Post some answers edit: Including some degenerate eigenvalues]","Say I have a hermitian matrix A with elements $A_{ij}$, given the eigenvalues $e_p$, (and optional eigenvectors $v_p$) Is there an easy(ish) way to calculate $\frac{\partial e_p}{\partial A_{ij}}$? The elements represent potentials between particles and so any deviations would also leave the matrix hermitian. [Post some answers edit: Including some degenerate eigenvalues]",,"['matrices', 'eigenvalues-eigenvectors', 'matrix-calculus']"
66,"Prove that if the sum of each row of $A$ equals $s$, then $s$ is an eigenvalue of $A$.","Prove that if the sum of each row of  equals , then  is an eigenvalue of .",A s s A,"Let $A$ be an $n \times n$ matrix. $i)$Prove that if the sum of each row of $A$ equals $s$, then $s$ is an eigenvalue of $A$. $ii)$Prove that if the sum of each column of $A$ equals $s$, then $s$ is an eigenvalue of $A$. I think that being an eigenvalue of $A$ implies that $sv=Av$ for some vector $v$. Furthermore, I know that $[a_i] = s$ if we let $a_i$ denote the i-th row of $A$. However, I do not seem to be able to find a link between these two facts. Could anyone please help me out?","Let $A$ be an $n \times n$ matrix. $i)$Prove that if the sum of each row of $A$ equals $s$, then $s$ is an eigenvalue of $A$. $ii)$Prove that if the sum of each column of $A$ equals $s$, then $s$ is an eigenvalue of $A$. I think that being an eigenvalue of $A$ implies that $sv=Av$ for some vector $v$. Furthermore, I know that $[a_i] = s$ if we let $a_i$ denote the i-th row of $A$. However, I do not seem to be able to find a link between these two facts. Could anyone please help me out?",,"['matrices', 'eigenvalues-eigenvectors']"
67,Gradient of linear scalar field $X \mapsto \operatorname{tr}(AXB)$,Gradient of linear scalar field,X \mapsto \operatorname{tr}(AXB),Could someone explain the following? $$ \nabla_X \operatorname{tr}(AXB) = BA $$ I understand that $$ {\rm d} \operatorname{tr}(AXB) = \operatorname{tr}(BA \; {\rm d} X) $$ but I don't quite understand how to move ${\rm d} X$ out of the trace.,Could someone explain the following? I understand that but I don't quite understand how to move out of the trace., \nabla_X \operatorname{tr}(AXB) = BA   {\rm d} \operatorname{tr}(AXB) = \operatorname{tr}(BA \; {\rm d} X)  {\rm d} X,"['matrices', 'derivatives', 'matrix-calculus', 'scalar-fields']"
68,Positive semi-definite vs positive definite,Positive semi-definite vs positive definite,,"I am confused about the difference between positive semi-definite and positive definite. May I understand that positive semi-definite means symmetric and $x'Ax \ge 0$, while positive definite means symmetric and $x'Ax \gt 0$?","I am confused about the difference between positive semi-definite and positive definite. May I understand that positive semi-definite means symmetric and $x'Ax \ge 0$, while positive definite means symmetric and $x'Ax \gt 0$?",,"['matrices', 'positive-definite']"
69,Sudoku with special properties,Sudoku with special properties,,"Sudoku is a puzzle, with the objective is to fill a 9×9 grid with digits so that each column, each row, and each of the nine 3×3 sub-grids that compose the grid (also ""sudoku-blocks"") contains all of the digits from 1 to 9. Let's define block as a 3x3 sub-grid (not necessarily forming one of the sudoku-blocks, but including them) containing all the digits 1 to 9. Let's define N as a number of all valid blocks on the grid. For the usual sudoku puzzle $N=9$. The maximum theoretically possible $N=49$ (7 blocks per row*7 blocks per column). I found sudoku puzzle with $N=10$, to prove that puzzles with $N>9$ exist. Here's one: +-------+-------+-------+ | 3 9 6 | 4 1 5 | 2 7 8 | | 1 2 5 | 7 3 8 | 4 6 9 | | 4 7 8 | 2 6 9 | 3 1 5 | +-------+-------+-------+ | 7 5 9 | 6 4 2 | 8 3 1 | | 8 4 3 | 5 9 1 | 7 2 6 | | 2 6 1 | 3 8 7 | 5 9 4 | +-------+-------+-------+ | 5 3 4 | 9 2 6 | 1 8 7 | | 6 8 7 | 1 5 3 | 9 4 2 | | 9 1 2 | 8 7 4 | 6 5 3 | +-------+-------+-------+ The 10th block is in the top right corner: 5 2 7 8 4 6 9 3 1 And here's another with $N=33$ ($N = 3*7 + 3*7 - 9$) +-------+-------+-------+ | 1 2 3 | 4 5 6 | 7 8 9 | | 4 5 6 | 7 8 9 | 1 2 3 | | 7 8 9 | 1 2 3 | 4 5 6 | +-------+-------+-------+ | 2 3 1 | 5 6 4 | 8 9 7 | | 5 6 4 | 8 9 7 | 2 3 1 | | 8 9 7 | 2 3 1 | 5 6 4 | +-------+-------+-------+ | 3 1 2 | 6 4 5 | 9 7 8 | | 6 4 5 | 9 7 8 | 3 1 2 | | 9 7 8 | 3 1 2 | 6 4 5 | +-------+-------+-------+ Questions: Does sudoku puzzle with N=49 exist? No If yes, then what is it? If no, then what's the maximum possible N? Why? Update . This update is fully based on @Emisor answer and proof. Assume $N=49$ possible, let's try generating a puzzle: +-------+---  | 1 2 3 | X  | 4 5 6 | Y  | 7 8 9 | Z +-------+--- | A B C | D The block on the first figure is to be taken as our ""starting"" one. Since there are no other numbers on the board, having them ordered makes no difference, for now. Now, for $N=49$ several conditions must be met: X, Y, Z must be filled with 1, 4, 7 A, B, C must be filled with 1, 2, 3 Since, X cannot be 1 and A cannot be 1, this statements are also true: Y or Z must be 1 B or C must be 1 That makes block 56Y,89Z,BCD invalid as it must contain two 1, therefore $N=49$ is impossible. That makes only one question left: What's the maximum possible $N$? Why?","Sudoku is a puzzle, with the objective is to fill a 9×9 grid with digits so that each column, each row, and each of the nine 3×3 sub-grids that compose the grid (also ""sudoku-blocks"") contains all of the digits from 1 to 9. Let's define block as a 3x3 sub-grid (not necessarily forming one of the sudoku-blocks, but including them) containing all the digits 1 to 9. Let's define N as a number of all valid blocks on the grid. For the usual sudoku puzzle $N=9$. The maximum theoretically possible $N=49$ (7 blocks per row*7 blocks per column). I found sudoku puzzle with $N=10$, to prove that puzzles with $N>9$ exist. Here's one: +-------+-------+-------+ | 3 9 6 | 4 1 5 | 2 7 8 | | 1 2 5 | 7 3 8 | 4 6 9 | | 4 7 8 | 2 6 9 | 3 1 5 | +-------+-------+-------+ | 7 5 9 | 6 4 2 | 8 3 1 | | 8 4 3 | 5 9 1 | 7 2 6 | | 2 6 1 | 3 8 7 | 5 9 4 | +-------+-------+-------+ | 5 3 4 | 9 2 6 | 1 8 7 | | 6 8 7 | 1 5 3 | 9 4 2 | | 9 1 2 | 8 7 4 | 6 5 3 | +-------+-------+-------+ The 10th block is in the top right corner: 5 2 7 8 4 6 9 3 1 And here's another with $N=33$ ($N = 3*7 + 3*7 - 9$) +-------+-------+-------+ | 1 2 3 | 4 5 6 | 7 8 9 | | 4 5 6 | 7 8 9 | 1 2 3 | | 7 8 9 | 1 2 3 | 4 5 6 | +-------+-------+-------+ | 2 3 1 | 5 6 4 | 8 9 7 | | 5 6 4 | 8 9 7 | 2 3 1 | | 8 9 7 | 2 3 1 | 5 6 4 | +-------+-------+-------+ | 3 1 2 | 6 4 5 | 9 7 8 | | 6 4 5 | 9 7 8 | 3 1 2 | | 9 7 8 | 3 1 2 | 6 4 5 | +-------+-------+-------+ Questions: Does sudoku puzzle with N=49 exist? No If yes, then what is it? If no, then what's the maximum possible N? Why? Update . This update is fully based on @Emisor answer and proof. Assume $N=49$ possible, let's try generating a puzzle: +-------+---  | 1 2 3 | X  | 4 5 6 | Y  | 7 8 9 | Z +-------+--- | A B C | D The block on the first figure is to be taken as our ""starting"" one. Since there are no other numbers on the board, having them ordered makes no difference, for now. Now, for $N=49$ several conditions must be met: X, Y, Z must be filled with 1, 4, 7 A, B, C must be filled with 1, 2, 3 Since, X cannot be 1 and A cannot be 1, this statements are also true: Y or Z must be 1 B or C must be 1 That makes block 56Y,89Z,BCD invalid as it must contain two 1, therefore $N=49$ is impossible. That makes only one question left: What's the maximum possible $N$? Why?",,"['matrices', 'algorithms', 'recreational-mathematics', 'puzzle', 'sudoku']"
70,Are all Lie groups Matrix Lie groups?,Are all Lie groups Matrix Lie groups?,,"I have beard a bit about so-called matrix Lie groups. From what I understand (and I don't understand it well) a matrix Lie group is a closed subgroup of $GL_n(\mathbb{C})$. There is also the notion of a Lie group. It is something about a smooth manifold of the manifold $M_n(\mathbb{C})$. I have also hear something saying that all Lie groups are in fact isomorphic to a matrix Lie group. Is this correct? Could someone give me a bit more detail about this? What, for example, is the isomorphism? Is it of abstract groups, manifolds, or ...?","I have beard a bit about so-called matrix Lie groups. From what I understand (and I don't understand it well) a matrix Lie group is a closed subgroup of $GL_n(\mathbb{C})$. There is also the notion of a Lie group. It is something about a smooth manifold of the manifold $M_n(\mathbb{C})$. I have also hear something saying that all Lie groups are in fact isomorphic to a matrix Lie group. Is this correct? Could someone give me a bit more detail about this? What, for example, is the isomorphism? Is it of abstract groups, manifolds, or ...?",,"['matrices', 'group-theory', 'manifolds', 'lie-groups']"
71,What is the theory of Matrices?,What is the theory of Matrices?,,"I've just passed high school and studying matrices. I've learned about determinant, transpose and adjoint etc. I've learned the method of finding these things but what's the purpose of finding these things? What actually Matrices do which makes it solving equations easier. Why determinant is equal to $(ab)-(cd)$ not $(cd)-(ab)$ for the matrix, $\left[\begin{matrix}a&c\\d&b\end{matrix}\right] $ ? Why inverse of $\rm A$ is equal to $\dfrac{\operatorname{adj}A} {\det A}$ not $\dfrac{\det A} {\operatorname{adj}A}$ ? I've read many answers like, What is the usefulness of matrices? , they say that matrices do this and that ,but they does not explain how or why ? Also what is the relation between vectors and matrices?","I've just passed high school and studying matrices. I've learned about determinant, transpose and adjoint etc. I've learned the method of finding these things but what's the purpose of finding these things? What actually Matrices do which makes it solving equations easier. Why determinant is equal to $(ab)-(cd)$ not $(cd)-(ab)$ for the matrix, $\left[\begin{matrix}a&c\\d&b\end{matrix}\right] $ ? Why inverse of $\rm A$ is equal to $\dfrac{\operatorname{adj}A} {\det A}$ not $\dfrac{\det A} {\operatorname{adj}A}$ ? I've read many answers like, What is the usefulness of matrices? , they say that matrices do this and that ,but they does not explain how or why ? Also what is the relation between vectors and matrices?",,"['matrices', 'determinant']"
72,How to construct a covariance matrix from a 2x2 data set,How to construct a covariance matrix from a 2x2 data set,,so if given a covariance matrix I can find the eigenvalues and move forward from there... but I seem to have trouble with the step before if I am given a data set and am told to create the covariance matrix. Looking at the notes I see the formula: $$cov(x) = \frac{1}{n-1}\sum(x_i - \bar{x})(y_i -\bar{y})   $$ I'm not too sure what to do with this formula and was hoping you can tell me how. Data Set: X1 | X2     ---|---     3  | 7     2  | 4,so if given a covariance matrix I can find the eigenvalues and move forward from there... but I seem to have trouble with the step before if I am given a data set and am told to create the covariance matrix. Looking at the notes I see the formula: $$cov(x) = \frac{1}{n-1}\sum(x_i - \bar{x})(y_i -\bar{y})   $$ I'm not too sure what to do with this formula and was hoping you can tell me how. Data Set: X1 | X2     ---|---     3  | 7     2  | 4,,"['matrices', 'correlation']"
73,Eigenvalues of matrix with entries that are continuous functions,Eigenvalues of matrix with entries that are continuous functions,,"For each $t \in [0,b]$, let $M(t)$ be an $n \times n$ matrix with entries $m_{ij}(t).$ The matrix $M(t)$ is invertible and positive-definite, so the eigenvalues of $M(t)$ exist and are positive for every $t$. The entries are continuous functions: $m_{ij} \in C^0[0,b]$. Does this mean the eigenvalues $\lambda_i(t)$ of $M(t)$ are also continuous functions? It is true for $n=1$ and $n=2$. I can't do it for the general case.","For each $t \in [0,b]$, let $M(t)$ be an $n \times n$ matrix with entries $m_{ij}(t).$ The matrix $M(t)$ is invertible and positive-definite, so the eigenvalues of $M(t)$ exist and are positive for every $t$. The entries are continuous functions: $m_{ij} \in C^0[0,b]$. Does this mean the eigenvalues $\lambda_i(t)$ of $M(t)$ are also continuous functions? It is true for $n=1$ and $n=2$. I can't do it for the general case.",,"['matrices', 'functional-analysis']"
74,Multiplying by a $1\times 1$ matrix?,Multiplying by a  matrix?,1\times 1,"For matrix multiplication to work, you have to multiply an $m \times n$ matrix by an $n \times p$ matrix, so we have $$\bigg(m \times n\bigg)\bigg(   n\times p \bigg).$$ But what about a $1 \times 1$ matrix? Is this just a scalar? But every matrix can be mulitplied by a scalar; so do $1 \times 1$ matrices break the rule?","For matrix multiplication to work, you have to multiply an $m \times n$ matrix by an $n \times p$ matrix, so we have $$\bigg(m \times n\bigg)\bigg(   n\times p \bigg).$$ But what about a $1 \times 1$ matrix? Is this just a scalar? But every matrix can be mulitplied by a scalar; so do $1 \times 1$ matrices break the rule?",,[]
75,"Checkboard matrix, brand new or old?","Checkboard matrix, brand new or old?",,"Ok so what I found was a square matrix of order $n×n$ where $n$ follows $2m+1$ and $m$ is a natural number the pattern these matrices follow is as follows: for a $3×3$ matrix: $$ A = \left( \begin{array}{ccc} b & a & c \\ a & a+b+c & a \\ c & a & b \end{array} \right)$$ and the cool thing is that $$ A^x = \left( \begin{array}{ccc} q & p & r \\ p & p+q+r & p \\ r & p & q \end{array} \right)$$ where $ x $ is any natural number Now for $5×5$ it goes like $$ A = \left( \begin{array}{ccc} b & b & a & c & c\\ b & b & a & c & c\\ a & a & 2b+a+2c & a & a\\ c & c & a & b & b\\ c & c & a & b & b\end{array} \right)$$ and again $$ A^x = \left( \begin{array}{ccc} q & q & p & r & r\\ q & q & p & r & r\\ p & p & 2q+p+2r & p & p\\ r & r & p & q & q\\ r & r & p & q & q\end{array} \right)$$ Once more for a $7×7$ matrix we have $$ A = \left( \begin{array}{ccc} b & b & b & a & c & c & c\\ b & b & b & a & c & c & c\\ b & b & b & a & c & c & c\\ a & a & a & 3b+a+3c & a & a & a\\ c & c & c & a & b & b & b\\ c & c & c & a & b & b & b\\ c & c & c & a & b & b & b\\ \end{array} \right)$$ and then  $$ A^x = \left( \begin{array}{ccc} q & q & q & p & r & r & r\\ q & q & q & p & r & r & r\\ q & q & q & p & r & r & r\\ p & p & p & 3q+p+3r & p & p & p\\ r & r & r & p & q & q & q\\ r & r & r & p & q & q & q\\ r & r & r & p & q & q & q\\ \end{array} \right)$$ and so on and so forth, I didn't find whether this has already been observed and neither a name for this so I am calling this checkboard matrix, because it looks like that. I just have one question, has this been found already, if yes please give details. Addendum: Thanks to Robert Israel for pointing out the error,  I have made some changes,  please check into it.  The pattern for the middle element is $ m (b+c)+a $","Ok so what I found was a square matrix of order $n×n$ where $n$ follows $2m+1$ and $m$ is a natural number the pattern these matrices follow is as follows: for a $3×3$ matrix: $$ A = \left( \begin{array}{ccc} b & a & c \\ a & a+b+c & a \\ c & a & b \end{array} \right)$$ and the cool thing is that $$ A^x = \left( \begin{array}{ccc} q & p & r \\ p & p+q+r & p \\ r & p & q \end{array} \right)$$ where $ x $ is any natural number Now for $5×5$ it goes like $$ A = \left( \begin{array}{ccc} b & b & a & c & c\\ b & b & a & c & c\\ a & a & 2b+a+2c & a & a\\ c & c & a & b & b\\ c & c & a & b & b\end{array} \right)$$ and again $$ A^x = \left( \begin{array}{ccc} q & q & p & r & r\\ q & q & p & r & r\\ p & p & 2q+p+2r & p & p\\ r & r & p & q & q\\ r & r & p & q & q\end{array} \right)$$ Once more for a $7×7$ matrix we have $$ A = \left( \begin{array}{ccc} b & b & b & a & c & c & c\\ b & b & b & a & c & c & c\\ b & b & b & a & c & c & c\\ a & a & a & 3b+a+3c & a & a & a\\ c & c & c & a & b & b & b\\ c & c & c & a & b & b & b\\ c & c & c & a & b & b & b\\ \end{array} \right)$$ and then  $$ A^x = \left( \begin{array}{ccc} q & q & q & p & r & r & r\\ q & q & q & p & r & r & r\\ q & q & q & p & r & r & r\\ p & p & p & 3q+p+3r & p & p & p\\ r & r & r & p & q & q & q\\ r & r & r & p & q & q & q\\ r & r & r & p & q & q & q\\ \end{array} \right)$$ and so on and so forth, I didn't find whether this has already been observed and neither a name for this so I am calling this checkboard matrix, because it looks like that. I just have one question, has this been found already, if yes please give details. Addendum: Thanks to Robert Israel for pointing out the error,  I have made some changes,  please check into it.  The pattern for the middle element is $ m (b+c)+a $",,['matrices']
76,A matrix to the power of zero gives identity matrix even if it doesn't have an inverse?,A matrix to the power of zero gives identity matrix even if it doesn't have an inverse?,,"If one matrix whose determinant is equal to 0 which means it doesn't have an inverse. Then how is possible to find the value of the matrix to the power of 0 equal to identity matrix when multiplying the original matrix with something undefined? Is it a math fluke, or I am missing some important information?","If one matrix whose determinant is equal to 0 which means it doesn't have an inverse. Then how is possible to find the value of the matrix to the power of 0 equal to identity matrix when multiplying the original matrix with something undefined? Is it a math fluke, or I am missing some important information?",,['matrices']
77,Insightful proofs for Sherman-Morrison Formula and Matrix Determinant Lemma,Insightful proofs for Sherman-Morrison Formula and Matrix Determinant Lemma,,"There are two statement about a matrix under rank-one updates that I would be grateful if you give me some insightful proofs. Suppose $A$ be a nonsingular $n \times n$ matrix and $\mathbf{u},\mathbf{v}$ be vectors. First, the Sherman-Morrison formula states that: $$ \left( A + \mathbf{u} \mathbf{v}^T \right)^{-1} = A^{-1} - \frac{A^{-1}\mathbf{u} \mathbf{v}^TA^{-1}}{1 + \mathbf{v}^TA^{-1}\mathbf{u}}.$$ (We can prove this by verifying that the RHS multiplied by $A + \mathbf{u} \mathbf{v}^T$ is $I$ .) Second, Matrix Determinant Lemma states that: $$\det\left(A+\mathbf{u}\mathbf{v}^T\right) = \det(A) \left( 1 + \mathbf{v}^T A^{-1}\mathbf{u} \right).$$ (In the proof from wikipedia , we just have to verify some identity again.) It's easy to verify these proofs but it's not clear to me how to come up with the identity. Are there any other proofs which are not just by multiplication of matrices, and give us some insight ? Or even some informal explanation ?","There are two statement about a matrix under rank-one updates that I would be grateful if you give me some insightful proofs. Suppose be a nonsingular matrix and be vectors. First, the Sherman-Morrison formula states that: (We can prove this by verifying that the RHS multiplied by is .) Second, Matrix Determinant Lemma states that: (In the proof from wikipedia , we just have to verify some identity again.) It's easy to verify these proofs but it's not clear to me how to come up with the identity. Are there any other proofs which are not just by multiplication of matrices, and give us some insight ? Or even some informal explanation ?","A n \times n \mathbf{u},\mathbf{v}  \left( A + \mathbf{u} \mathbf{v}^T \right)^{-1} = A^{-1} - \frac{A^{-1}\mathbf{u} \mathbf{v}^TA^{-1}}{1 + \mathbf{v}^TA^{-1}\mathbf{u}}. A + \mathbf{u} \mathbf{v}^T I \det\left(A+\mathbf{u}\mathbf{v}^T\right) = \det(A) \left( 1 + \mathbf{v}^T A^{-1}\mathbf{u} \right).","['matrices', 'determinant', 'inverse', 'alternative-proof']"
78,How to prove Fibonacci sequence with matrices? [duplicate],How to prove Fibonacci sequence with matrices? [duplicate],,This question already has answers here : Proof of this result related to Fibonacci numbers: $\begin{pmatrix}1&1\\1&0\end{pmatrix}^n=\begin{pmatrix}F_{n+1}&F_n\\F_n&F_{n-1}\end{pmatrix}$? (4 answers) Closed 8 years ago . How do you prove that: $$ \begin{pmatrix} 1 & 1\\ 1 & 0 \end{pmatrix}^n =  \begin{pmatrix} F_{n+1} & F_n\\ F_{n} & F_{n-1} \end{pmatrix}$$,This question already has answers here : Proof of this result related to Fibonacci numbers: $\begin{pmatrix}1&1\\1&0\end{pmatrix}^n=\begin{pmatrix}F_{n+1}&F_n\\F_n&F_{n-1}\end{pmatrix}$? (4 answers) Closed 8 years ago . How do you prove that: $$ \begin{pmatrix} 1 & 1\\ 1 & 0 \end{pmatrix}^n =  \begin{pmatrix} F_{n+1} & F_n\\ F_{n} & F_{n-1} \end{pmatrix}$$,,"['matrices', 'fibonacci-numbers']"
79,Why do positive definite matrices have to be symmetric? [duplicate],Why do positive definite matrices have to be symmetric? [duplicate],,This question already has answers here : Do positive semidefinite matrices have to be symmetric? (3 answers) Closed 7 years ago . Definitions of positive definiteness usually look like this: A symmetric matrix $M$ is positive definite if $x^T M x > 0$ for all vectors $x \neq 0$. Why must $M$ be symmetric? The definition seems to make sense for general square matrices.,This question already has answers here : Do positive semidefinite matrices have to be symmetric? (3 answers) Closed 7 years ago . Definitions of positive definiteness usually look like this: A symmetric matrix $M$ is positive definite if $x^T M x > 0$ for all vectors $x \neq 0$. Why must $M$ be symmetric? The definition seems to make sense for general square matrices.,,"['matrices', 'quadratic-forms', 'positive-definite', 'symmetric-matrices']"
80,Variety of Nilpotent Matrices,Variety of Nilpotent Matrices,,"Let $k$ be an algebraically closed field and view $M_n(k)$ as $\mathbb{A}^{n^2}$.  $A\in M_n(k)$ is nilpotent if and only if $A^n=0$.  Since the equation $A^n=0$ is given by $n^2$ polynomial equations, it defines a variety in $\mathbb{A}^{n^2}$.  What is the dimension of this variety?  Is it irreducible?  If not, what are its irreducible components?","Let $k$ be an algebraically closed field and view $M_n(k)$ as $\mathbb{A}^{n^2}$.  $A\in M_n(k)$ is nilpotent if and only if $A^n=0$.  Since the equation $A^n=0$ is given by $n^2$ polynomial equations, it defines a variety in $\mathbb{A}^{n^2}$.  What is the dimension of this variety?  Is it irreducible?  If not, what are its irreducible components?",,"['matrices', 'algebraic-geometry']"
81,How to denote matrix concatenation?,How to denote matrix concatenation?,,"Trivial question: Is there any standard notation for the concatenation of two or more matrices? Example: $$A = \left(\begin{array}[c c]  - a_1 & a_2\\ a_3 & a_4  \end{array}\right),$$ $$B = \left(\begin{array}[c c]  - b_1 & b_2\\ b_3 & b_4  \end{array}\right),$$ Then the concatenation (by rows) of $A$ and $B$ is: $$C = \left(\begin{array}[c c]  - a_1 & a_2\\ a_3 & a_4 \\  b_1 & b_2\\ b_3 & b_4  \end{array}\right).$$ I just want to know if there is a standard notation for this operation.","Trivial question: Is there any standard notation for the concatenation of two or more matrices? Example: $$A = \left(\begin{array}[c c]  - a_1 & a_2\\ a_3 & a_4  \end{array}\right),$$ $$B = \left(\begin{array}[c c]  - b_1 & b_2\\ b_3 & b_4  \end{array}\right),$$ Then the concatenation (by rows) of $A$ and $B$ is: $$C = \left(\begin{array}[c c]  - a_1 & a_2\\ a_3 & a_4 \\  b_1 & b_2\\ b_3 & b_4  \end{array}\right).$$ I just want to know if there is a standard notation for this operation.",,"['matrices', 'notation', 'block-matrices']"
82,What does the ideal norm of matrix elements really mean?,What does the ideal norm of matrix elements really mean?,,"Say we have a number field $K$ (specifically, an imaginary quadratic field) and a $2\times2$ matrix $\sigma=\pmatrix{a&c\\b&d}$ with elements $a,b,c,d\in\mathcal O_k$, the ring of integers of $K$. Generally, what is the significance of the norm of the ideal in $\mathcal O_k$ generated by the matrix elements, written $N(a,b,c,d)$, to the matrix $\sigma$? For example, does the norm give us information about the eigenvalues of $\sigma$? If we write the norm as $N(\sigma)$, is the norm of a product $N(\sigma\tau)$ related to $N(\sigma)$ and $N(\tau)$ in a useful way, maybe involving determinants as well? Is $N$ or its square root a bona fide matrix norm ? I ask because in Vulakh (1994) ""Reflections in extended Bianchi groups"" , the extended Bianchi group is  represented by matrices for which $\det(\sigma)=\epsilon N(\sigma)$ where $\epsilon$ is a unit in $\mathcal O_k$. I don't really follow how one is led to write down that equation.","Say we have a number field $K$ (specifically, an imaginary quadratic field) and a $2\times2$ matrix $\sigma=\pmatrix{a&c\\b&d}$ with elements $a,b,c,d\in\mathcal O_k$, the ring of integers of $K$. Generally, what is the significance of the norm of the ideal in $\mathcal O_k$ generated by the matrix elements, written $N(a,b,c,d)$, to the matrix $\sigma$? For example, does the norm give us information about the eigenvalues of $\sigma$? If we write the norm as $N(\sigma)$, is the norm of a product $N(\sigma\tau)$ related to $N(\sigma)$ and $N(\tau)$ in a useful way, maybe involving determinants as well? Is $N$ or its square root a bona fide matrix norm ? I ask because in Vulakh (1994) ""Reflections in extended Bianchi groups"" , the extended Bianchi group is  represented by matrices for which $\det(\sigma)=\epsilon N(\sigma)$ where $\epsilon$ is a unit in $\mathcal O_k$. I don't really follow how one is led to write down that equation.",,"['matrices', 'algebraic-number-theory', 'ideals', 'normed-spaces']"
83,Does a positive semidefinite matrix always have a non-negative trace?,Does a positive semidefinite matrix always have a non-negative trace?,,"If $A$ is a positive semidefinite matrix ( $A\succeq 0)$ , does it imply that $\mbox{Tr}(A)\geq 0$ , where the $\mbox{Tr}(\cdot)$ denotes the trace. If not, any counter-example? Thanks.","If is a positive semidefinite matrix ( , does it imply that , where the denotes the trace. If not, any counter-example? Thanks.",A A\succeq 0) \mbox{Tr}(A)\geq 0 \mbox{Tr}(\cdot),"['matrices', 'trace', 'positive-semidefinite']"
84,What is an interpretation of the matrix exponential?,What is an interpretation of the matrix exponential?,,"I just read about the existence of the ""matrix exponential"" $$e^X := \sum_{k = 0}^\infty\frac1{k!}X^k$$ Is there a simple way to interpret this? I understand the analog between  real number exponentials as infinite Taylor expansions. However, I have no easy way of interpreting in the case of a matrix. I've read that it relates to linear ODE's","I just read about the existence of the ""matrix exponential"" $$e^X := \sum_{k = 0}^\infty\frac1{k!}X^k$$ Is there a simple way to interpret this? I understand the analog between  real number exponentials as infinite Taylor expansions. However, I have no easy way of interpreting in the case of a matrix. I've read that it relates to linear ODE's",,"['matrices', 'exponentiation', 'matrix-exponential']"
85,Recovering the two $SU(2)$ matrices from $SO(4)$ matrix,Recovering the two  matrices from  matrix,SU(2) SO(4),"Since there is a $2$-$1$ homomorphism from $SU(2)\times SU(2)$ to $SO(4)$ there should be a way to recover the two $SU(2)$ matrices given an $SO(4)$ matrix. I believe I could set this up as a system of equations using the map from above and solve for the coefficients of the $SU(2)$ matrices.  However, I wonder if anyone has already done this or can point me to the formulas?","Since there is a $2$-$1$ homomorphism from $SU(2)\times SU(2)$ to $SO(4)$ there should be a way to recover the two $SU(2)$ matrices given an $SO(4)$ matrix. I believe I could set this up as a system of equations using the map from above and solve for the coefficients of the $SU(2)$ matrices.  However, I wonder if anyone has already done this or can point me to the formulas?",,"['matrices', 'lie-groups']"
86,Can I compare real and complex eigenvalues?,Can I compare real and complex eigenvalues?,,"I'm calculating the eigenvalues of the matrix    $\begin{pmatrix} 2 &0 &0& 1\\      0 &1& 0& 1\\      0 &0& 3& 1\\     -1 &0 &0 &1\end{pmatrix}$, which are $1$,$3$, $\frac{3}{2}+\sqrt{3}i$ and $\frac{3}{2}-\sqrt{3}i$. I wish to recognize the biggest and smallest of these. But how can I compare real and complex numbers?","I'm calculating the eigenvalues of the matrix    $\begin{pmatrix} 2 &0 &0& 1\\      0 &1& 0& 1\\      0 &0& 3& 1\\     -1 &0 &0 &1\end{pmatrix}$, which are $1$,$3$, $\frac{3}{2}+\sqrt{3}i$ and $\frac{3}{2}-\sqrt{3}i$. I wish to recognize the biggest and smallest of these. But how can I compare real and complex numbers?",,"['matrices', 'complex-numbers', 'eigenvalues-eigenvectors']"
87,What is the codimension of matrices of rank $r$ as a manifold?,What is the codimension of matrices of rank  as a manifold?,r,"I'm reading through G&P's Differential Topology book, but I hit a wall at the end of section 4. There is a result stating The set $X=\{A\in M_{m\times n}(\mathbb{R}):\mathrm{rk}(A)=r\}$ is a submanifold of $\mathbb{R}^{m\times n}$ with codimension $(m-r)(n-r)$. There is a suggestion: Let $A\in M_{m\times n}(\mathbb{R})$ have form $$ A=\begin{pmatrix} B & C \\ D & E\end{pmatrix} $$ where $B$ is an invertible $r\times r$ matrix. Then right multiply by $$ \begin{pmatrix} I & -BC^{-1} \\ 0 & I \end{pmatrix} $$ and show $\mathrm{rk}(A)=r$ iff $E-DB^{-1}C=0$. I multiplied out and got the matrix $$ M:=\begin{pmatrix} B & 0 \\ D & E-DB^{-1}C\end{pmatrix}. $$ Since I multiplied by a nonsingular matrix, I know that $\mathrm{rk}(A)=\mathrm{rk}(M)$. If $E-DB^{-1}C=0$, then  $$ M=\begin{pmatrix} B & 0 \\ D & 0 \end{pmatrix} $$ has rank $r$, so $A$ has rank $r$. For the converse, if $A$ has rank $r$, then $M$ has rank $r$, so by performing row operations, $M$ is row equivalent to a matrix of the form $$ \begin{pmatrix} I_r & 0 \\ 0 & 0 \end{pmatrix}. $$ This would imply $E-DB^{-1}C$ is row equivalent to $0$, and I think this implies $E-DB^{-1}C=0$. My main concern is then, how does this approach imply $\mathrm{codim}(X)=(m-r)(n-r)$? Is there some special map I can apply the Preimage Theorem to? Thank you.","I'm reading through G&P's Differential Topology book, but I hit a wall at the end of section 4. There is a result stating The set $X=\{A\in M_{m\times n}(\mathbb{R}):\mathrm{rk}(A)=r\}$ is a submanifold of $\mathbb{R}^{m\times n}$ with codimension $(m-r)(n-r)$. There is a suggestion: Let $A\in M_{m\times n}(\mathbb{R})$ have form $$ A=\begin{pmatrix} B & C \\ D & E\end{pmatrix} $$ where $B$ is an invertible $r\times r$ matrix. Then right multiply by $$ \begin{pmatrix} I & -BC^{-1} \\ 0 & I \end{pmatrix} $$ and show $\mathrm{rk}(A)=r$ iff $E-DB^{-1}C=0$. I multiplied out and got the matrix $$ M:=\begin{pmatrix} B & 0 \\ D & E-DB^{-1}C\end{pmatrix}. $$ Since I multiplied by a nonsingular matrix, I know that $\mathrm{rk}(A)=\mathrm{rk}(M)$. If $E-DB^{-1}C=0$, then  $$ M=\begin{pmatrix} B & 0 \\ D & 0 \end{pmatrix} $$ has rank $r$, so $A$ has rank $r$. For the converse, if $A$ has rank $r$, then $M$ has rank $r$, so by performing row operations, $M$ is row equivalent to a matrix of the form $$ \begin{pmatrix} I_r & 0 \\ 0 & 0 \end{pmatrix}. $$ This would imply $E-DB^{-1}C$ is row equivalent to $0$, and I think this implies $E-DB^{-1}C=0$. My main concern is then, how does this approach imply $\mathrm{codim}(X)=(m-r)(n-r)$? Is there some special map I can apply the Preimage Theorem to? Thank you.",,"['matrices', 'differential-geometry', 'manifolds', 'differential-topology', 'block-matrices']"
88,How to calculate square matrix to power n?,How to calculate square matrix to power n?,,"I have a matrix of non-negative numbers, say $A$. (1) How do we calculate $A^n$? (2) How can we calculate $A^n$ using usual matrix exponential trick to do it fast ? Edit 1 Also theres another property of matrix A that its diagonals consists always of 0 & other elements either 0 or 1. Can we do this just by involving matrix multiplication ?","I have a matrix of non-negative numbers, say $A$. (1) How do we calculate $A^n$? (2) How can we calculate $A^n$ using usual matrix exponential trick to do it fast ? Edit 1 Also theres another property of matrix A that its diagonals consists always of 0 & other elements either 0 or 1. Can we do this just by involving matrix multiplication ?",,['matrices']
89,Are matrices rank 2 tensors?,Are matrices rank 2 tensors?,,"I know that this is sometimes the case, but that some matrices are not tensors. So what is the intuitive and specific demands of a matrix to also be a tensor? Does it need to be quadratic, singular or something else? Some sources I read seem to suggest that all rank 2 matrices are tensors while other just claims that ""some"" matrices are rank 2 tensors. What's the connection between tensors and matrices?","I know that this is sometimes the case, but that some matrices are not tensors. So what is the intuitive and specific demands of a matrix to also be a tensor? Does it need to be quadratic, singular or something else? Some sources I read seem to suggest that all rank 2 matrices are tensors while other just claims that ""some"" matrices are rank 2 tensors. What's the connection between tensors and matrices?",,"['matrices', 'tensors', 'tensor-rank']"
90,On the invertibility of the adjacency matrix of a graph,On the invertibility of the adjacency matrix of a graph,,"Which are the sufficient and necessary conditions for an undirected graph with no self edges (i.e. no loop of length $1$) to have an invertible adjacency matrix? In this case, the adjacency matrix is symmetric (i.e. $A = A^\top$). Moreover, all the diagonal elements are $0$ and there is a $1$ in both the entries $(i,j)$ and $(j,i)$, with $i\neq j$, if and only if vertices $i$ and $j$ are connected. A first necessary condition is the following: all vertices must have at least one connection, otherwise the relative row of $A$ is null. But what else?","Which are the sufficient and necessary conditions for an undirected graph with no self edges (i.e. no loop of length $1$) to have an invertible adjacency matrix? In this case, the adjacency matrix is symmetric (i.e. $A = A^\top$). Moreover, all the diagonal elements are $0$ and there is a $1$ in both the entries $(i,j)$ and $(j,i)$, with $i\neq j$, if and only if vertices $i$ and $j$ are connected. A first necessary condition is the following: all vertices must have at least one connection, otherwise the relative row of $A$ is null. But what else?",,"['matrices', 'graph-theory', 'spectral-graph-theory', 'adjacency-matrix']"
91,"Does this expression have a (""better"") determinant form?","Does this expression have a (""better"") determinant form?",,"[ Edit I've found a determinant that satisfies the letter of the previous version of this question, but not its ""Cayley-Menger"" spirit.] A tetrahedron with face areas $w$, $x$, $y$, $z$ and ""pseudo-face"" areas $h$, $j$, $k$ has volume $V$ given by $$\begin{eqnarray} 81 V^4 &=& 2 w^2 x^2 y^2 + 2 w^2 y^2 z^2 + 2 w^2 z^2 x^2 + 2 x^2 y^2 z^2 + h^2 j^2 k^2 \\[4pt] &-&h^2(w^2 x^2+y^2 z^2)-j^2(w^2 y^2+z^2 x^2)-k^2(w^2 z^2+x^2 y^2) \end{eqnarray}$$ All the cool kids in the world of tetrahedral formulas are the determinants or minors of some matrix or another, such as the Cayley-Menger or the Gram. I seek to express the above volume formula in such a form. Note that, with $A$, $B$, $C$ the dihedral angles between respective face-pairs $(y,z)$, $(z,x)$, $(x,y)$, we have this formula for volume: $$81 V^4 = 4 x^2 y^2 z^2 \left( 1 - 2 \cos{A} \cos{B} \cos{C} - \cos^2 A - \cos^2 B - \cos^2 C \right) = 4 x^2 y^2 z^2 \left|\begin{array}{ccc} 1 & -\cos C & -\cos B \\ -\cos C & 1 & -\cos A \\ -\cos B & -\cos A & 1 \end{array}\right|$$ We also have what I call the Second Law of Cosines ... $$h^2 = y^2 + z^2 - 2 y z \cos A \qquad j^2 = z^2 + x^2 - 2 z x \cos B \qquad k^2 = x^2 + y^2 - 2 x y \cos C$$ ... which gives rise to this determinant form of the volume formula: $$81V^4 = \frac{1}{2} \left|\begin{array}{ccc}  2 x^2 & k^2-x^2-y^2 & j^2-z^2-x^2 \\ k^2-x^2-y^2 & 2 y^2 & h^2-y^2-z^2 \\ j^2-z^2-x^2 & h^2-y^2-z^2 & 2 z^2 \end{array}\right| $$ Verifying equality with my target volume formula requires the tetrahedral ""Sum of Squares"" identity: $$w^2+x^2+y^2+z^2=h^2+j^2+k^2$$ Now, while I'm pleased to have a reasonably-straightforward determinant equation (thus answering my original question myself), that particular determinant doesn't quite seem a worthy peer of the Cayley-Menger ... perhaps because it doesn't treat all face areas equally. Can we do better? (Yes, ""better"" is subjective, but I think we know what I mean: it should have the Cayley-Menger spirit . I suppose my primary criterion is that the determinant be symmetric in the four face areas and the three pseudo-face areas; ideally, the individual entries are (subjectively) ""uncomplicated"".) Here's a possibly-helpful discussion from the previous version of this question: If $a$, $b$, $c$, $d$, $e$, $f$ are edges of the tetrahedron with $w = \triangle def$, $x = \triangle dbc$, $y = \triangle aec$, and $z = \triangle abf$, then  $$ 9 V^2 a^2 = [h,y,z] \qquad 9 V^2 \; b^2 = [j,z,x] \qquad 9 V^2 \; c^2 = [k,x,y]\\ 9 V^2 d^2 = [h,w,x] \qquad 9 V^2 \; e^2 = [j,w,y] \qquad 9 V^2 \; f^2 = [k,w,z] $$ where ""$[\bullet]$"" is the ""Heronic product"" $$\begin{eqnarray} [p,q,r] &:=& (p+q+r)(-p+q+r)(p-q+r)(p+q-r)\\ &=&-p^4-q^4-r^4+2p^2q^2+2q^2r^2+2r^2p^2 \end{eqnarray}$$ Consequently, we can substitute into the Cayley-Menger determinant formula to write $$\begin{eqnarray} 288 V^2 &=& \left| \begin{array}{ccccc} 0 & a^2 & b^2 & c^2 & 1 \\ a^2 & 0 & f^2 & e^2 & 1 \\ b^2 & f^2 & 0 & d^2 & 1 \\ c^2 & e^2 & d^2 & 0 & 1 \\ 1 & 1 & 1 & 1 & 0 \end{array}\right| \\ &=& \frac{1}{(9V^2)^3}\left| \begin{array}{ccccc} 0 & [h,y,z] & [j,z,x] & [k,x,y] & 1 \\ \; [h,y,z] & 0 & [k,w,z] & [j,w,y] & 1 \\ \; [j,z,x] & [k,w,z] & 0 & [h,w,x] & 1 \\ \; [k,x,y] & [j,w,y] & [h,w,x] & 0 & 1 \\ 1 & 1 & 1 & 1 & 0 \end{array}\right|\\ &=:&\frac{1}{729V^6} \mathrm{det} M \end{eqnarray} $$ Since $\mathrm{det}M$ is proportional to the 8th-power of $V$ ---and therefore, the square of the target formula--- perhaps I should attempt to construct $\sqrt{M}$. My investigations of matrix $M$ have been fruitless. (Note, though, that despite its complexity ---and the fact that it generates the square of what I want--- $M$ has the kind of symmetry I would expect from a properly Cayley-Menger-esque result.) I think it's time to add a bounty to this question. [Bounty expired.] Edit Here's a tantalizing alternative ""(negative-)square-of-what-I-want"" matrix: $$N := \left|\begin{array}{cccccc} 0 & z^2 & y^2 & h^2 & y^2 & z^2 \\ z^2 & 0 & x^2 & x^2 & j^2 & z^2 \\ y^2 & x^2 & 0 & x^2 & y^2 & k^2 \\ h^2 & x^2 & x^2 & 0 & w^2 & w^2 \\ y^2 & j^2 & y^2 & w^2 & 0 & w^2 \\ z^2 & z^2 & k^2 & w^2 & w^2 & 0 \end{array}\right| \qquad \mathrm{det}N = -\left(81 V^4\right)^2$$ The pattern in the entries can be described thusly: Index the rows and columns with edges $(a,b,c,d,e,f)$. Then the $pq$-th element ($p\ne q$) corresponds to the face determined by edges $p$ and $q$. (Opposite edges $p$ and $q$ determine a pseudo-face.) The minors of $N$ are very interesting. Writing $N_p$ for the matrix obtained by removing row and column $p$ from $N$: $$\begin{array}{ccc} \mathrm{det} N_a = 2 w^2 x^2 \cdot 81 V^4 & \mathrm{det} N_b = 2 w^2 y^2 \cdot 81 V^4 & \mathrm{det} N_c = 2 w^2 z^2 \cdot 81 V^4 \\ \mathrm{det} N_d = 2 y^2 z^2 \cdot 81 V^4 & \mathrm{det} N_e = 2 z^2 x^2 \cdot 81 V^4 & \mathrm{det} N_f = 2 x^2 y^2 \cdot 81 V^4 \\ \end{array}$$ Note, for instance in the case of $N_a$, that faces $w$ and $x$ meet along edge $d$, which is opposite edge $a$. Moreover, with $N_{pq}$ the matrix obtained by removing row $p$ and column $q$ (with $p\ne q$): $$\mathrm{det} N_{ab} = - w^2 \left( k^2 - x^2 - y^2 \right) \cdot 81 V^4 = 2 w^2 x y \cos C \cdot 81 V^4 \qquad \text{etc}$$ I get the sense that this gets me closer to my goal. Edit. Getting even closer. Augmenting matrix $N$ into more Menger-like form gives $$\begin{align} P &:= \left|\begin{array}{ccccccc} 0 & z^2 & y^2 & h^2 & y^2 & z^2 & 1 \\ z^2 & 0 & x^2 & x^2 & j^2 & z^2 & 1 \\ y^2 & x^2 & 0 & x^2 & y^2 & k^2 & 1 \\ h^2 & x^2 & x^2 & 0 & w^2 & w^2 & 1 \\ y^2 & j^2 & y^2 & w^2 & 0 & w^2 & 1 \\ z^2 & z^2 & k^2 & w^2 & w^2 & 0 & 1 \\ 1 & 1 & 1 & 1 & 1 & 1 & 0 \end{array}\right| \\[6pt] \mathrm{det} P &= 2\left(w^4+x^4+y^4+z^4-h^2j^2-j^2k^2-k^2h^2\right) \cdot 81 V^4 \end{align}$$ This may be about as good as I can expect. I'd prefer, though, that the multiplied polynomial --if there must be one-- have an unambiguous sign (and vanish only trivially).","[ Edit I've found a determinant that satisfies the letter of the previous version of this question, but not its ""Cayley-Menger"" spirit.] A tetrahedron with face areas $w$, $x$, $y$, $z$ and ""pseudo-face"" areas $h$, $j$, $k$ has volume $V$ given by $$\begin{eqnarray} 81 V^4 &=& 2 w^2 x^2 y^2 + 2 w^2 y^2 z^2 + 2 w^2 z^2 x^2 + 2 x^2 y^2 z^2 + h^2 j^2 k^2 \\[4pt] &-&h^2(w^2 x^2+y^2 z^2)-j^2(w^2 y^2+z^2 x^2)-k^2(w^2 z^2+x^2 y^2) \end{eqnarray}$$ All the cool kids in the world of tetrahedral formulas are the determinants or minors of some matrix or another, such as the Cayley-Menger or the Gram. I seek to express the above volume formula in such a form. Note that, with $A$, $B$, $C$ the dihedral angles between respective face-pairs $(y,z)$, $(z,x)$, $(x,y)$, we have this formula for volume: $$81 V^4 = 4 x^2 y^2 z^2 \left( 1 - 2 \cos{A} \cos{B} \cos{C} - \cos^2 A - \cos^2 B - \cos^2 C \right) = 4 x^2 y^2 z^2 \left|\begin{array}{ccc} 1 & -\cos C & -\cos B \\ -\cos C & 1 & -\cos A \\ -\cos B & -\cos A & 1 \end{array}\right|$$ We also have what I call the Second Law of Cosines ... $$h^2 = y^2 + z^2 - 2 y z \cos A \qquad j^2 = z^2 + x^2 - 2 z x \cos B \qquad k^2 = x^2 + y^2 - 2 x y \cos C$$ ... which gives rise to this determinant form of the volume formula: $$81V^4 = \frac{1}{2} \left|\begin{array}{ccc}  2 x^2 & k^2-x^2-y^2 & j^2-z^2-x^2 \\ k^2-x^2-y^2 & 2 y^2 & h^2-y^2-z^2 \\ j^2-z^2-x^2 & h^2-y^2-z^2 & 2 z^2 \end{array}\right| $$ Verifying equality with my target volume formula requires the tetrahedral ""Sum of Squares"" identity: $$w^2+x^2+y^2+z^2=h^2+j^2+k^2$$ Now, while I'm pleased to have a reasonably-straightforward determinant equation (thus answering my original question myself), that particular determinant doesn't quite seem a worthy peer of the Cayley-Menger ... perhaps because it doesn't treat all face areas equally. Can we do better? (Yes, ""better"" is subjective, but I think we know what I mean: it should have the Cayley-Menger spirit . I suppose my primary criterion is that the determinant be symmetric in the four face areas and the three pseudo-face areas; ideally, the individual entries are (subjectively) ""uncomplicated"".) Here's a possibly-helpful discussion from the previous version of this question: If $a$, $b$, $c$, $d$, $e$, $f$ are edges of the tetrahedron with $w = \triangle def$, $x = \triangle dbc$, $y = \triangle aec$, and $z = \triangle abf$, then  $$ 9 V^2 a^2 = [h,y,z] \qquad 9 V^2 \; b^2 = [j,z,x] \qquad 9 V^2 \; c^2 = [k,x,y]\\ 9 V^2 d^2 = [h,w,x] \qquad 9 V^2 \; e^2 = [j,w,y] \qquad 9 V^2 \; f^2 = [k,w,z] $$ where ""$[\bullet]$"" is the ""Heronic product"" $$\begin{eqnarray} [p,q,r] &:=& (p+q+r)(-p+q+r)(p-q+r)(p+q-r)\\ &=&-p^4-q^4-r^4+2p^2q^2+2q^2r^2+2r^2p^2 \end{eqnarray}$$ Consequently, we can substitute into the Cayley-Menger determinant formula to write $$\begin{eqnarray} 288 V^2 &=& \left| \begin{array}{ccccc} 0 & a^2 & b^2 & c^2 & 1 \\ a^2 & 0 & f^2 & e^2 & 1 \\ b^2 & f^2 & 0 & d^2 & 1 \\ c^2 & e^2 & d^2 & 0 & 1 \\ 1 & 1 & 1 & 1 & 0 \end{array}\right| \\ &=& \frac{1}{(9V^2)^3}\left| \begin{array}{ccccc} 0 & [h,y,z] & [j,z,x] & [k,x,y] & 1 \\ \; [h,y,z] & 0 & [k,w,z] & [j,w,y] & 1 \\ \; [j,z,x] & [k,w,z] & 0 & [h,w,x] & 1 \\ \; [k,x,y] & [j,w,y] & [h,w,x] & 0 & 1 \\ 1 & 1 & 1 & 1 & 0 \end{array}\right|\\ &=:&\frac{1}{729V^6} \mathrm{det} M \end{eqnarray} $$ Since $\mathrm{det}M$ is proportional to the 8th-power of $V$ ---and therefore, the square of the target formula--- perhaps I should attempt to construct $\sqrt{M}$. My investigations of matrix $M$ have been fruitless. (Note, though, that despite its complexity ---and the fact that it generates the square of what I want--- $M$ has the kind of symmetry I would expect from a properly Cayley-Menger-esque result.) I think it's time to add a bounty to this question. [Bounty expired.] Edit Here's a tantalizing alternative ""(negative-)square-of-what-I-want"" matrix: $$N := \left|\begin{array}{cccccc} 0 & z^2 & y^2 & h^2 & y^2 & z^2 \\ z^2 & 0 & x^2 & x^2 & j^2 & z^2 \\ y^2 & x^2 & 0 & x^2 & y^2 & k^2 \\ h^2 & x^2 & x^2 & 0 & w^2 & w^2 \\ y^2 & j^2 & y^2 & w^2 & 0 & w^2 \\ z^2 & z^2 & k^2 & w^2 & w^2 & 0 \end{array}\right| \qquad \mathrm{det}N = -\left(81 V^4\right)^2$$ The pattern in the entries can be described thusly: Index the rows and columns with edges $(a,b,c,d,e,f)$. Then the $pq$-th element ($p\ne q$) corresponds to the face determined by edges $p$ and $q$. (Opposite edges $p$ and $q$ determine a pseudo-face.) The minors of $N$ are very interesting. Writing $N_p$ for the matrix obtained by removing row and column $p$ from $N$: $$\begin{array}{ccc} \mathrm{det} N_a = 2 w^2 x^2 \cdot 81 V^4 & \mathrm{det} N_b = 2 w^2 y^2 \cdot 81 V^4 & \mathrm{det} N_c = 2 w^2 z^2 \cdot 81 V^4 \\ \mathrm{det} N_d = 2 y^2 z^2 \cdot 81 V^4 & \mathrm{det} N_e = 2 z^2 x^2 \cdot 81 V^4 & \mathrm{det} N_f = 2 x^2 y^2 \cdot 81 V^4 \\ \end{array}$$ Note, for instance in the case of $N_a$, that faces $w$ and $x$ meet along edge $d$, which is opposite edge $a$. Moreover, with $N_{pq}$ the matrix obtained by removing row $p$ and column $q$ (with $p\ne q$): $$\mathrm{det} N_{ab} = - w^2 \left( k^2 - x^2 - y^2 \right) \cdot 81 V^4 = 2 w^2 x y \cos C \cdot 81 V^4 \qquad \text{etc}$$ I get the sense that this gets me closer to my goal. Edit. Getting even closer. Augmenting matrix $N$ into more Menger-like form gives $$\begin{align} P &:= \left|\begin{array}{ccccccc} 0 & z^2 & y^2 & h^2 & y^2 & z^2 & 1 \\ z^2 & 0 & x^2 & x^2 & j^2 & z^2 & 1 \\ y^2 & x^2 & 0 & x^2 & y^2 & k^2 & 1 \\ h^2 & x^2 & x^2 & 0 & w^2 & w^2 & 1 \\ y^2 & j^2 & y^2 & w^2 & 0 & w^2 & 1 \\ z^2 & z^2 & k^2 & w^2 & w^2 & 0 & 1 \\ 1 & 1 & 1 & 1 & 1 & 1 & 0 \end{array}\right| \\[6pt] \mathrm{det} P &= 2\left(w^4+x^4+y^4+z^4-h^2j^2-j^2k^2-k^2h^2\right) \cdot 81 V^4 \end{align}$$ This may be about as good as I can expect. I'd prefer, though, that the multiplied polynomial --if there must be one-- have an unambiguous sign (and vanish only trivially).",,"['geometry', 'matrices']"
92,How to prove that if the determinant of the matrix is zero then at least one eigenvalue must be zero? [duplicate],How to prove that if the determinant of the matrix is zero then at least one eigenvalue must be zero? [duplicate],,"This question already has answers here : Show that a matrix $A$ is singular if and only if $0$ is an eigenvalue. (8 answers) Closed 5 years ago . For a matrix $A$, if $\det(A)=0,$ prove and provide an example that at least one eigenvalue must be zero. At first, I tried using the identity that the product of eigenvalues is the determinant of the matrix, so it follows that at least one must be zero for the determinant to be zero. Is this correct? Could I also prove it by using $(A-\lambda I)X=0$, for some $X\neq 0?$ If $\lambda=0,$ then we have $AX=0$, but I can't say $\det(A)\cdot \det(X)=0$ because $X$ is not a square matrix and doesn't have a determinant. How would I continue?","This question already has answers here : Show that a matrix $A$ is singular if and only if $0$ is an eigenvalue. (8 answers) Closed 5 years ago . For a matrix $A$, if $\det(A)=0,$ prove and provide an example that at least one eigenvalue must be zero. At first, I tried using the identity that the product of eigenvalues is the determinant of the matrix, so it follows that at least one must be zero for the determinant to be zero. Is this correct? Could I also prove it by using $(A-\lambda I)X=0$, for some $X\neq 0?$ If $\lambda=0,$ then we have $AX=0$, but I can't say $\det(A)\cdot \det(X)=0$ because $X$ is not a square matrix and doesn't have a determinant. How would I continue?",,"['matrices', 'eigenvalues-eigenvectors', 'determinant']"
93,Orthogonal matrix norm,Orthogonal matrix norm,,"If $H$ is an orthogonal matrix, then $||H||=1$ and $||HA||=||A||, \forall A$-matrix (such that we can writ $H \cdot A$). What norm is this about?","If $H$ is an orthogonal matrix, then $||H||=1$ and $||HA||=||A||, \forall A$-matrix (such that we can writ $H \cdot A$). What norm is this about?",,"['matrices', 'orthogonality']"
94,"Set, n-Tuple, Vector and Matrix — links and differences","Set, n-Tuple, Vector and Matrix — links and differences",,"I know this question has been asked like 1000 times, however all supplied answers were not really satisfying to me. My question concerns the similarities and differences between these mathematical objects. First, the Set. A set is defined to be the entity of distinct objects (not necessarily numbers). The arrangement of objects is not relevant. We use curly braces to denote sets; commata are used to seperate the objects within the set. Second, the $n$-Tuple. A $n$-tuple is very similar to a set, however the objects need not to be the same and the ordering of objects within the $n$-tuple is important. $n$-Tuples are usually denoted with parentheses and the objects within are seperated with commata as in sets. Also, it is common to build the set of even numbers for instance like this: $\{2n\mid n\in \mathbb{N}\}$. However, I have never seen something like this with regard to n-tuples. Third, the Vector. A vector is an element of a vector space. However, if I calculate the Cartesian product of, for instance, $\mathbb{R}×\mathbb{R}$ then the objects of $\mathbb{R}^2$ are (column-)vectors which are denoted as tuples. Furthermore, I often see box brackets to denote such vectors and the elements are written in one column (opposed to tuples or sets). Also, commata are not used to separate the objects (however, sometimes I see the elements of row vectors separated by commata). However, I have never seen such notation when for instance describing elements of $\mathbb{N}\times\mathbb{R}$. Finally, matrices. Matrices are arrays of numbers and clearly linked to vectors as each column/row is a vector. However, I have never seen commata used in combination with matrices. Furthermore, the space of matrices is written as $A^{(m×n)}$. I know what the idea behind this notation is, however, as matrices are linked to vectors I have problems to really understand it. Those concepts are obviously linked, however at certain points there arise crucial differences between them (which also come, I believe, from notational differences between authors and fields of mathematics). I hope my problem is comprehensible and someone can help me and shed light on my issues. Thanks!","I know this question has been asked like 1000 times, however all supplied answers were not really satisfying to me. My question concerns the similarities and differences between these mathematical objects. First, the Set. A set is defined to be the entity of distinct objects (not necessarily numbers). The arrangement of objects is not relevant. We use curly braces to denote sets; commata are used to seperate the objects within the set. Second, the $n$-Tuple. A $n$-tuple is very similar to a set, however the objects need not to be the same and the ordering of objects within the $n$-tuple is important. $n$-Tuples are usually denoted with parentheses and the objects within are seperated with commata as in sets. Also, it is common to build the set of even numbers for instance like this: $\{2n\mid n\in \mathbb{N}\}$. However, I have never seen something like this with regard to n-tuples. Third, the Vector. A vector is an element of a vector space. However, if I calculate the Cartesian product of, for instance, $\mathbb{R}×\mathbb{R}$ then the objects of $\mathbb{R}^2$ are (column-)vectors which are denoted as tuples. Furthermore, I often see box brackets to denote such vectors and the elements are written in one column (opposed to tuples or sets). Also, commata are not used to separate the objects (however, sometimes I see the elements of row vectors separated by commata). However, I have never seen such notation when for instance describing elements of $\mathbb{N}\times\mathbb{R}$. Finally, matrices. Matrices are arrays of numbers and clearly linked to vectors as each column/row is a vector. However, I have never seen commata used in combination with matrices. Furthermore, the space of matrices is written as $A^{(m×n)}$. I know what the idea behind this notation is, however, as matrices are linked to vectors I have problems to really understand it. Those concepts are obviously linked, however at certain points there arise crucial differences between them (which also come, I believe, from notational differences between authors and fields of mathematics). I hope my problem is comprehensible and someone can help me and shed light on my issues. Thanks!",,"['matrices', 'elementary-set-theory', 'notation', 'vectors']"
95,How is the column space of a matrix A orthogonal to its nullspace?,How is the column space of a matrix A orthogonal to its nullspace?,,How do you show that the column space of a matrix A is orthogonal to its nullspace?,How do you show that the column space of a matrix A is orthogonal to its nullspace?,,[]
96,Why is the determinant of the 0x0 matrix equal 1? [closed],Why is the determinant of the 0x0 matrix equal 1? [closed],,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question I don't understand why the determinant of the 0x0 matrix equals 1.,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question I don't understand why the determinant of the 0x0 matrix equals 1.,,"['matrices', 'determinant']"
97,Why multiply a matrix with its transpose?,Why multiply a matrix with its transpose?,,"This might be a very stupid question, but I do not seem to understand why I would multiple a matrix with its transpose. I am not a mathematician, but I am very interested in understanding the practical usage of equations: Imagine I have three products sales Apple, Orange and Pear for the last 3 days in a matrix form called A:  $$ A=  \begin{bmatrix}         Apple & Orange & Pear \\         10 & 2 & 5 \\         5 & 3 & 10 \\         4 & 3 & 2 \\         5 & 10 & 5 \\         \end{bmatrix}$$ What will $AA^{\rm T}$ tell me? I have seen this long answer link: Is a matrix multiplied with its transpose something special? , but I did not get it at all. I see that a lot of equations use the product $AA^{\rm T}$ and I really hope that someone will give a very simple answer.","This might be a very stupid question, but I do not seem to understand why I would multiple a matrix with its transpose. I am not a mathematician, but I am very interested in understanding the practical usage of equations: Imagine I have three products sales Apple, Orange and Pear for the last 3 days in a matrix form called A:  $$ A=  \begin{bmatrix}         Apple & Orange & Pear \\         10 & 2 & 5 \\         5 & 3 & 10 \\         4 & 3 & 2 \\         5 & 10 & 5 \\         \end{bmatrix}$$ What will $AA^{\rm T}$ tell me? I have seen this long answer link: Is a matrix multiplied with its transpose something special? , but I did not get it at all. I see that a lot of equations use the product $AA^{\rm T}$ and I really hope that someone will give a very simple answer.",,"['matrices', 'transpose']"
98,Lipschitz-constant gradient implies bounded eigenvalues on Hessian,Lipschitz-constant gradient implies bounded eigenvalues on Hessian,,"I've read in a few places that if we have a Lipschitz gradient $$\|\nabla f(x) - \nabla f(y)\|\leq L\|x-y\|,\, \forall x,y, $$ we can equivalently say $\nabla^2f\preceq LI.$ But I'm having a hard time showing this. (Equivalently, I want to show $z^T \nabla^2f(x)z\leq z^TLIz=Lz^Tz,\forall\, x,z $.)","I've read in a few places that if we have a Lipschitz gradient $$\|\nabla f(x) - \nabla f(y)\|\leq L\|x-y\|,\, \forall x,y, $$ we can equivalently say $\nabla^2f\preceq LI.$ But I'm having a hard time showing this. (Equivalently, I want to show $z^T \nabla^2f(x)z\leq z^TLIz=Lz^Tz,\forall\, x,z $.)",,"['matrices', 'eigenvalues-eigenvectors', 'convex-analysis', 'lipschitz-functions', 'hessian-matrix']"
99,Inverse of $A^{-1}+B^{-1}$,Inverse of,A^{-1}+B^{-1},"If $A, B$ and $A + B$ are all $n × n$ invertible matrices. Prove that $A^{−1} + B^{−1}$ is invertible and the inverse is $A(A + B) ^{−1}B$. I am afraid I am really stuck on this one, and I haven't really tried much because I don't know what to try. Thanks for all the help guys, I understand now.","If $A, B$ and $A + B$ are all $n × n$ invertible matrices. Prove that $A^{−1} + B^{−1}$ is invertible and the inverse is $A(A + B) ^{−1}B$. I am afraid I am really stuck on this one, and I haven't really tried much because I don't know what to try. Thanks for all the help guys, I understand now.",,['matrices']
