,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Why conjugate when switching order of inner product?,Why conjugate when switching order of inner product?,,"There is an axiom of inner product spaces that states: $\overline{\langle x,y\rangle } = \langle y,x\rangle$ Basically (without any conceptual understanding) it seems like all you have to do when you swap the order of the arguments in an inner product space is take their conjugate. How does this make any sense? I know if we are dealing with an inner product space over $\mathbb{R}$ then the conjugate of a real number is just the real number itself so there is no change. But how does this make sense over the field $\mathbb{C}$?","There is an axiom of inner product spaces that states: $\overline{\langle x,y\rangle } = \langle y,x\rangle$ Basically (without any conceptual understanding) it seems like all you have to do when you swap the order of the arguments in an inner product space is take their conjugate. How does this make any sense? I know if we are dealing with an inner product space over $\mathbb{R}$ then the conjugate of a real number is just the real number itself so there is no change. But how does this make sense over the field $\mathbb{C}$?",,"['linear-algebra', 'inner-products']"
1,Lower bound on norm of product of two matrices,Lower bound on norm of product of two matrices,,"Let $\vert \vert . \vert \vert$ be the 2-norm. Since this norm is submultiplicative, we know that for any two square matrices $A, B \in \mathbb{R}^{n \times n}$, $$ \vert \vert A B \vert \vert \leq \vert\vert A \vert \vert \vert \vert B \vert \vert \leq \sigma_{\textrm{max}}(A) \vert \vert B \vert \vert.$$ What I am looking for is an inequality of the form $$ \sigma_{\textrm{min}}(A) \vert \vert B \vert \vert \leq \vert \vert A B \vert \vert. $$ The first inequality is true because this norm simply satisfies the submultiplicative property. But what about the second inequality? Is it true? And if not, is it only true for special type of matrices?","Let $\vert \vert . \vert \vert$ be the 2-norm. Since this norm is submultiplicative, we know that for any two square matrices $A, B \in \mathbb{R}^{n \times n}$, $$ \vert \vert A B \vert \vert \leq \vert\vert A \vert \vert \vert \vert B \vert \vert \leq \sigma_{\textrm{max}}(A) \vert \vert B \vert \vert.$$ What I am looking for is an inequality of the form $$ \sigma_{\textrm{min}}(A) \vert \vert B \vert \vert \leq \vert \vert A B \vert \vert. $$ The first inequality is true because this norm simply satisfies the submultiplicative property. But what about the second inequality? Is it true? And if not, is it only true for special type of matrices?",,"['linear-algebra', 'matrices']"
2,Vector spaces of the same finite dimension are isomorphic,Vector spaces of the same finite dimension are isomorphic,,"Suppose that V and W are vector spaces with the same dimension. We wish to show that V is isomorphic to W , i.e. show that there exists a bijective linear function, mapping from V to W . I understand that it will suffice to find a linear function that maps a basis of V to a basis of W . This is because any element of a vector space can be written as a unique linear combination of its basis elements. However I'm not sure how to show that such a map exists.","Suppose that V and W are vector spaces with the same dimension. We wish to show that V is isomorphic to W , i.e. show that there exists a bijective linear function, mapping from V to W . I understand that it will suffice to find a linear function that maps a basis of V to a basis of W . This is because any element of a vector space can be written as a unique linear combination of its basis elements. However I'm not sure how to show that such a map exists.",,['linear-algebra']
3,"If every eigenvalue of $A$ is zero, does this mean $A$ is a zero matrix?","If every eigenvalue of  is zero, does this mean  is a zero matrix?",A A,"If every eigenvalue of $A$ is zero, show that $A$ is nilpotent. I got this question as my homework. I am just wondering if every eigenvalue of $A$ is zero, then $A$ is zero, why bother to prove $A$ is nilpotent.","If every eigenvalue of $A$ is zero, show that $A$ is nilpotent. I got this question as my homework. I am just wondering if every eigenvalue of $A$ is zero, then $A$ is zero, why bother to prove $A$ is nilpotent.",,['linear-algebra']
4,Why are linear functions linear?,Why are linear functions linear?,,"I always thought linear functions need to satisfy $$f(x+y)=f(x)+f(y).$$ I am a tad confused now, consider $f(x)=2x+3$. $f(1)=5$, $f(2)=7$, $f(1+2)=f(3)=9 \neq f(1)+f(2)$ which was what I thought linear functions should satisfy. Could someone clarify?","I always thought linear functions need to satisfy $$f(x+y)=f(x)+f(y).$$ I am a tad confused now, consider $f(x)=2x+3$. $f(1)=5$, $f(2)=7$, $f(1+2)=f(3)=9 \neq f(1)+f(2)$ which was what I thought linear functions should satisfy. Could someone clarify?",,"['linear-algebra', 'functions', 'arithmetic']"
5,Finding the vector perpendicular to the plane,Finding the vector perpendicular to the plane,,"Why is the perpendicular vector on a plane always the vector of coefficients of the variables in the plane equation? e.g., for the plane $2x-y+3z=8$, the perpendicular vector is $(2,-1,3)$. Thanks.","Why is the perpendicular vector on a plane always the vector of coefficients of the variables in the plane equation? e.g., for the plane $2x-y+3z=8$, the perpendicular vector is $(2,-1,3)$. Thanks.",,"['linear-algebra', 'multivariable-calculus']"
6,Principal submatrices of a positive definite matrix,Principal submatrices of a positive definite matrix,,"Let $\mathbf{A}\in\mathbb{R}^{N \times N}$ be symmetric positive definite. For some $1\leq k<N$ , partition $$\mathbf{A}=\begin{pmatrix}\mathbf{A}_{11} & \mathbf{A}_{12} \\ \mathbf{A}_{12}^T & \mathbf{A}_{22}\end{pmatrix},$$ where $\mathbf{A}_{11}$ is $k\times k$ and $\mathbf{A}_{22}$ is $(N-k)\times (N-k)$ . I'm trying to show that the principal submatrices $\mathbf{A}_{11}$ and $\mathbf{A}_{22}$ are also symmetric positive definite. I've been able to show that $$\left [ \begin{array}{cc} \mathbf{A}_{11} & \mathbf{A}_{12}\\ \mathbf{A}_{12}^T & \mathbf{A}_{22} \end{array} \right ] = \mathbf{A} = \mathbf{A}^T = \left [ \begin{array}{cc} \mathbf{A}_{11}^T & \mathbf{A}_{12}\\ \mathbf{A}_{12}^T & \mathbf{A}_{22}^T \end{array} \right ]$$ This implies that $\mathbf{A}_{11} = \mathbf{A}_{11}^T$ , and $\mathbf{A}_{22} = \mathbf{A}_{22}^T$ . Thus, $\mathbf{A}_{11}$ and $\mathbf{A}_{22}$ are symmetric. I'm struggling to show that $\mathbf{A}$ is positive definite. Does anyone have any ideas?","Let be symmetric positive definite. For some , partition where is and is . I'm trying to show that the principal submatrices and are also symmetric positive definite. I've been able to show that This implies that , and . Thus, and are symmetric. I'm struggling to show that is positive definite. Does anyone have any ideas?","\mathbf{A}\in\mathbb{R}^{N \times N} 1\leq k<N \mathbf{A}=\begin{pmatrix}\mathbf{A}_{11} & \mathbf{A}_{12} \\ \mathbf{A}_{12}^T & \mathbf{A}_{22}\end{pmatrix}, \mathbf{A}_{11} k\times k \mathbf{A}_{22} (N-k)\times (N-k) \mathbf{A}_{11} \mathbf{A}_{22} \left [ \begin{array}{cc}
\mathbf{A}_{11} & \mathbf{A}_{12}\\
\mathbf{A}_{12}^T & \mathbf{A}_{22} \end{array} \right ] = \mathbf{A} = \mathbf{A}^T = \left [ \begin{array}{cc}
\mathbf{A}_{11}^T & \mathbf{A}_{12}\\
\mathbf{A}_{12}^T & \mathbf{A}_{22}^T \end{array} \right ] \mathbf{A}_{11} = \mathbf{A}_{11}^T \mathbf{A}_{22} = \mathbf{A}_{22}^T \mathbf{A}_{11} \mathbf{A}_{22} \mathbf{A}","['linear-algebra', 'matrices', 'positive-definite', 'symmetric-matrices', 'block-matrices']"
7,Realification and Complexification of Vector Spaces,Realification and Complexification of Vector Spaces,,"I am interested in a good comprehensive resource on realification and complexification of vector spaces over the reals or complexes (and the interplay of these structures on the 'same' space in general). In particular, understanding of the basic theory is necessary and useful for a more intuitive approach towards functional analysis. Can you give me a tip? For example, Serge Lang's classical book does not explicitly work this part out. I am aware of a few pages in Arnold's book on ODE, but there should be something more comprehensive and neat somewhere out there.","I am interested in a good comprehensive resource on realification and complexification of vector spaces over the reals or complexes (and the interplay of these structures on the 'same' space in general). In particular, understanding of the basic theory is necessary and useful for a more intuitive approach towards functional analysis. Can you give me a tip? For example, Serge Lang's classical book does not explicitly work this part out. I am aware of a few pages in Arnold's book on ODE, but there should be something more comprehensive and neat somewhere out there.",,"['linear-algebra', 'abstract-algebra', 'reference-request', 'vector-spaces']"
8,Can vectors be inverted?,Can vectors be inverted?,,I wish to enquire if it is possible to solve the below for $C$. $$B^{-1}(x-\mu) = xc $$ Here obviously $B$ is an invertible matrix and both $c$ and $\mu$ are column vectors. Would the solution be $$x^{-1}B^{-1}(x-\mu) = c $$ is it possible to invert vectors ? How about if it was the other way $$B^{-1}(x-\mu) = cx $$ Is there any other way to do this ? Thanks in advance.,I wish to enquire if it is possible to solve the below for $C$. $$B^{-1}(x-\mu) = xc $$ Here obviously $B$ is an invertible matrix and both $c$ and $\mu$ are column vectors. Would the solution be $$x^{-1}B^{-1}(x-\mu) = c $$ is it possible to invert vectors ? How about if it was the other way $$B^{-1}(x-\mu) = cx $$ Is there any other way to do this ? Thanks in advance.,,['linear-algebra']
9,Is a characteristic polynomial we consider in Linear Algebra a polynomial or a polynomial function?,Is a characteristic polynomial we consider in Linear Algebra a polynomial or a polynomial function?,,"In Linear Algebra, we consider characteristic polynomials. Is a characteristic polynomial we consider in Linear Algebra a polynomial or a polynomial function? I think it is a polynomial function. I am reading ""Introduction to Linear Algebra"" (in Japanese) by Kazuo Matsuzaka. In this book, the characteristic polynomial of a linear map $F$ is defined by $\det(A - \lambda I)$ , where $A$ is a matrix which represents $F$ . And in this book, the author defines a determinant only for a matrix whose elements belong to a some field $K$ . If $\det(A - \lambda I)$ is a polynomial, then the elements of $A - \lambda I$ are polynomials too. But the author didn't define a determinant for a matrix whose elements are polynomials.","In Linear Algebra, we consider characteristic polynomials. Is a characteristic polynomial we consider in Linear Algebra a polynomial or a polynomial function? I think it is a polynomial function. I am reading ""Introduction to Linear Algebra"" (in Japanese) by Kazuo Matsuzaka. In this book, the characteristic polynomial of a linear map is defined by , where is a matrix which represents . And in this book, the author defines a determinant only for a matrix whose elements belong to a some field . If is a polynomial, then the elements of are polynomials too. But the author didn't define a determinant for a matrix whose elements are polynomials.",F \det(A - \lambda I) A F K \det(A - \lambda I) A - \lambda I,"['linear-algebra', 'polynomials', 'definition', 'characteristic-polynomial']"
10,Tips for writing proofs,Tips for writing proofs,,"When writing formal proofs in abstract and linear algebra, what kind of jargon is useful for conveying solutions effectively to others? In general, how should one go about structuring a formal proof so that it is both clear and succinct? What are some strategies for approaching problems that you will need to write formal proofs for?","When writing formal proofs in abstract and linear algebra, what kind of jargon is useful for conveying solutions effectively to others? In general, how should one go about structuring a formal proof so that it is both clear and succinct? What are some strategies for approaching problems that you will need to write formal proofs for?",,"['linear-algebra', 'abstract-algebra', 'soft-question', 'proof-writing']"
11,Is there anything special about this matrix?,Is there anything special about this matrix?,,"I've just encountered a matrix which seems to display nothing special to me: $$B=\begin{pmatrix}1&4&2\\0 &-3 &-2\\ 0 &4 &3 \end{pmatrix}$$ But further observation reveals something stunning: $$B^n=\cases{{I}&n is even\\{B}&n is odd}$$ So it leads me to wonder if there is indeed some special properties of this matrix $B$, or more probably, $B$ belongs to a whole special class of matrices whose name I don't know? Could you drop me a hint? Thanks in advance. EDIT I think I was being a bit stupid... It suddenly hit me that any matrix $B$ such that $B^2=I$ will have this property.","I've just encountered a matrix which seems to display nothing special to me: $$B=\begin{pmatrix}1&4&2\\0 &-3 &-2\\ 0 &4 &3 \end{pmatrix}$$ But further observation reveals something stunning: $$B^n=\cases{{I}&n is even\\{B}&n is odd}$$ So it leads me to wonder if there is indeed some special properties of this matrix $B$, or more probably, $B$ belongs to a whole special class of matrices whose name I don't know? Could you drop me a hint? Thanks in advance. EDIT I think I was being a bit stupid... It suddenly hit me that any matrix $B$ such that $B^2=I$ will have this property.",,"['linear-algebra', 'matrices', 'terminology']"
12,Conversion of rotation matrix to quaternion,Conversion of rotation matrix to quaternion,,We use unit length Quaternion to represent rotations. Following is a general rotation matrix obtained ${\begin{bmatrix}m_{00} & m_{01}&m_{02} \\ m_{10} & m_{11}&m_{12}\\  m_{20} & m_{21}&m_{22}\end{bmatrix}}_{3\times 3}\tag 1 $. How do I accurately calculate quaternion $q = q_1i+q_2j+q_3k+q_4$for this matrix?Means   how can we write $q_i$s interms of given $m_{ij}$  accurately?,We use unit length Quaternion to represent rotations. Following is a general rotation matrix obtained ${\begin{bmatrix}m_{00} & m_{01}&m_{02} \\ m_{10} & m_{11}&m_{12}\\  m_{20} & m_{21}&m_{22}\end{bmatrix}}_{3\times 3}\tag 1 $. How do I accurately calculate quaternion $q = q_1i+q_2j+q_3k+q_4$for this matrix?Means   how can we write $q_i$s interms of given $m_{ij}$  accurately?,,"['linear-algebra', 'matrices', 'ordinary-differential-equations', 'matrix-equations', 'quaternions']"
13,Lang's Linear Algebra: what's next?,Lang's Linear Algebra: what's next?,,"I've completed the study of Lang's Linear Algebra ($3^\text{rd}$ edition). To put it simply, I have enjoyed the subject and I would like to know ""what's next"". In other words, I would like to know what are the "" more advanced "" topics in linear algebra that are not covered by Lang's ( update: or Roman's ) book and where to study them; what are the ""modern research topics"" in "" pure "" linear algebra.","I've completed the study of Lang's Linear Algebra ($3^\text{rd}$ edition). To put it simply, I have enjoyed the subject and I would like to know ""what's next"". In other words, I would like to know what are the "" more advanced "" topics in linear algebra that are not covered by Lang's ( update: or Roman's ) book and where to study them; what are the ""modern research topics"" in "" pure "" linear algebra.",,"['linear-algebra', 'reference-request', 'soft-question', 'book-recommendation']"
14,Proof relation between Levi-Civita symbol and Kronecker deltas in Group Theory,Proof relation between Levi-Civita symbol and Kronecker deltas in Group Theory,,"In order to prove the following identity: $$\sum_{k}\epsilon_{ijk}\epsilon_{lmk}=\delta_{il}\delta_{jm}-\delta_{im}\delta_{jl}$$ Instead of checking this by brute force, Landau writes thr product of Levi-Civita symbols as: $$\epsilon_{ijk}\epsilon_{lmn}=\det\left|    \begin{array}{cccc}       \delta_{il}   & \delta_{im}  & \delta_{in}   \\       \delta_{jl}   & \delta_{jm}  & \delta_{jn}   \\       \delta_{kl}   & \delta_{km}  & \delta_{kn}       \end{array} \right| $$ The proof that the equalty holds is quite straightforward if you consider what values the indices can take. But I've been told that there's a much more profound and elegant demonstration based on the representation of the symmetric group. Does anybody know this approach based on group theory?","In order to prove the following identity: $$\sum_{k}\epsilon_{ijk}\epsilon_{lmk}=\delta_{il}\delta_{jm}-\delta_{im}\delta_{jl}$$ Instead of checking this by brute force, Landau writes thr product of Levi-Civita symbols as: $$\epsilon_{ijk}\epsilon_{lmn}=\det\left|    \begin{array}{cccc}       \delta_{il}   & \delta_{im}  & \delta_{in}   \\       \delta_{jl}   & \delta_{jm}  & \delta_{jn}   \\       \delta_{kl}   & \delta_{km}  & \delta_{kn}       \end{array} \right| $$ The proof that the equalty holds is quite straightforward if you consider what values the indices can take. But I've been told that there's a much more profound and elegant demonstration based on the representation of the symmetric group. Does anybody know this approach based on group theory?",,"['linear-algebra', 'group-theory', 'determinant', 'vector-analysis']"
15,Symmetric Matrices with trace zero,Symmetric Matrices with trace zero,,"Let $M_n$ denote the set of complex matrices of order $n$. It is well known that if $A\in M_n$ has trace zero then $A$ can be written as $A=BC-CB$, where $B,C\in M_n$. Is it true that every symmetric matrix $S\in M_n$ with trace zero can be written as $S=RR^t-R^tR$, for some $R\in M_n$? This theorem is true for real matrices. If $S$ is a real symmetric matrix with trace zero and order $n$ then exists a real matrix $R$ of order $n$ such that $S=RR^t-R^tR$. Is it true for complex matrices?","Let $M_n$ denote the set of complex matrices of order $n$. It is well known that if $A\in M_n$ has trace zero then $A$ can be written as $A=BC-CB$, where $B,C\in M_n$. Is it true that every symmetric matrix $S\in M_n$ with trace zero can be written as $S=RR^t-R^tR$, for some $R\in M_n$? This theorem is true for real matrices. If $S$ is a real symmetric matrix with trace zero and order $n$ then exists a real matrix $R$ of order $n$ such that $S=RR^t-R^tR$. Is it true for complex matrices?",,"['linear-algebra', 'matrices', 'trace', 'symmetric-matrices']"
16,Decomposition of a representation into a direct sum of irreducible ones,Decomposition of a representation into a direct sum of irreducible ones,,"I'm studying representation theory and in the book (Fulton and Harris) the author makes the following proposition with the following proof: Proposition : For any representation $V$ of a finite group $G$ , there is a decomposition $$V = V_1^{\oplus a_1}\oplus\cdots \oplus V_k^{\oplus a_k},$$ where the $V_i$ are distinct irreducible representations. The decomposition of $V$ into a direct sum of the $k$ factors is unique, as are the $V_i$ that occur and their multiplicities. Proof : It follows from Schur's lemma that if $W$ is another representation of $G$ , with a decomposition $W = \bigoplus W_j^{\oplus b_j}$ , and $\varphi : V\to W$ is a map of representations, then $\varphi$ must map the factor $V_i^{\oplus a_i}$ into the factor $W_j^{\oplus b_j}$ for which $W_j\simeq V_i$ ; when applied to the identity map of $V$ to $V$ , the stated uniqueness follows. I must confess I didn't understand. The fact that we can decompose $V$ like this I do understand that follows from the fact that if $V$ has a proper nonzero subrepresentation $W$ then there is another subrepresentation $W'$ such that $V = W\oplus W'$ . In that case, if either $W$ or $W'$ are not irreducible we can apply the same idea to them, until we have the desired decomposition. Now, this proof of uniqueness I really can't understand. I mean, uniqueness means that if we have $$V = V_1^{\oplus a_1}\oplus\cdots \oplus V_k^{\oplus a_k}\simeq W_1^{\oplus b_1}\oplus\cdots \oplus W_{r}^{\oplus b_r},$$ then we have $k = r$ , $a_i = b_i$ and $W_i\simeq V_i$ . I can't understand how this argument the author presents shows all of this. Indeed the whole point is that $V$ has these two decompositions then they are isomorphic, so that there exists one isomorphism $$\varphi : V_1^{\oplus a_1}\oplus\cdots \oplus V_k^{\oplus a_k}\to W_1^{\oplus b_1}\oplus\cdots \oplus W_r^{\oplus b_r}.$$ If we restrict it to $V_i$ we get one isomorphism $\varphi : V_i\to \varphi(V_i)$ . But why $\varphi(V_i)=W_j$ for some $j$ ? I mean, couldn't $\varphi(V_i)$ be some other subspace of the direct sum of the $W_i$ which is not one of the $W_i$ themselves? So how to understand this proof about the decomposition of a representation? What really is the argument used in this proof?","I'm studying representation theory and in the book (Fulton and Harris) the author makes the following proposition with the following proof: Proposition : For any representation of a finite group , there is a decomposition where the are distinct irreducible representations. The decomposition of into a direct sum of the factors is unique, as are the that occur and their multiplicities. Proof : It follows from Schur's lemma that if is another representation of , with a decomposition , and is a map of representations, then must map the factor into the factor for which ; when applied to the identity map of to , the stated uniqueness follows. I must confess I didn't understand. The fact that we can decompose like this I do understand that follows from the fact that if has a proper nonzero subrepresentation then there is another subrepresentation such that . In that case, if either or are not irreducible we can apply the same idea to them, until we have the desired decomposition. Now, this proof of uniqueness I really can't understand. I mean, uniqueness means that if we have then we have , and . I can't understand how this argument the author presents shows all of this. Indeed the whole point is that has these two decompositions then they are isomorphic, so that there exists one isomorphism If we restrict it to we get one isomorphism . But why for some ? I mean, couldn't be some other subspace of the direct sum of the which is not one of the themselves? So how to understand this proof about the decomposition of a representation? What really is the argument used in this proof?","V G V = V_1^{\oplus a_1}\oplus\cdots \oplus V_k^{\oplus a_k}, V_i V k V_i W G W = \bigoplus W_j^{\oplus b_j} \varphi : V\to W \varphi V_i^{\oplus a_i} W_j^{\oplus b_j} W_j\simeq V_i V V V V W W' V = W\oplus W' W W' V = V_1^{\oplus a_1}\oplus\cdots \oplus V_k^{\oplus a_k}\simeq W_1^{\oplus b_1}\oplus\cdots \oplus W_{r}^{\oplus b_r}, k = r a_i = b_i W_i\simeq V_i V \varphi : V_1^{\oplus a_1}\oplus\cdots \oplus V_k^{\oplus a_k}\to W_1^{\oplus b_1}\oplus\cdots \oplus W_r^{\oplus b_r}. V_i \varphi : V_i\to \varphi(V_i) \varphi(V_i)=W_j j \varphi(V_i) W_i W_i","['linear-algebra', 'group-theory', 'finite-groups', 'representation-theory', 'proof-explanation']"
17,Old AMM problem,Old AMM problem,,"I am working on an old AMM problem: Suppose $A,B$ are $n\times n$ real symmetric matrices with $\operatorname{tr} ((A+B)^k)= \operatorname{tr}(A^k) + \operatorname{tr}(B^k) $ for every positive integer $k>0$. Prove that $AB=0$. I've done the following: denote $(\lambda_i),(\mu_i),(\eta_i)$ the eigenvalues of $A,B,A+B$, respectively. They are real since $A,B$ are symmetric. Moreover, since $A,B$ are diagonalizable, if all $\lambda_i$ or all $\mu_j$ are zero, then $A$ or $B$ is zero and $AB=0$. suppose there exist some non-zero eigenvalues; then the identity given translates in  $$ \sum_{i=1}^n \lambda_i^k +\sum_{i=1}^n \mu_i^k=\sum_{i=1}^n \eta_i^k \qquad \forall k >0 $$ from here I can prove that any non-zero eigenvalue $\eta_i$ can be found in the LHS also with the same multiplicity (divide by the one with the greatest absolute value and take the limits $k \to \infty$, $k$ odd and $k \to \infty$, $k$ even). And therefore in the LHS there are at least $n$ eigenvalues equal to zero. so $A$ has $p$ zero eigenvalues and $B$ has $n-p$ zero eigenvalues. I feel like it is not long until the end, but I can't get any further ideas. How to proceed from here?","I am working on an old AMM problem: Suppose $A,B$ are $n\times n$ real symmetric matrices with $\operatorname{tr} ((A+B)^k)= \operatorname{tr}(A^k) + \operatorname{tr}(B^k) $ for every positive integer $k>0$. Prove that $AB=0$. I've done the following: denote $(\lambda_i),(\mu_i),(\eta_i)$ the eigenvalues of $A,B,A+B$, respectively. They are real since $A,B$ are symmetric. Moreover, since $A,B$ are diagonalizable, if all $\lambda_i$ or all $\mu_j$ are zero, then $A$ or $B$ is zero and $AB=0$. suppose there exist some non-zero eigenvalues; then the identity given translates in  $$ \sum_{i=1}^n \lambda_i^k +\sum_{i=1}^n \mu_i^k=\sum_{i=1}^n \eta_i^k \qquad \forall k >0 $$ from here I can prove that any non-zero eigenvalue $\eta_i$ can be found in the LHS also with the same multiplicity (divide by the one with the greatest absolute value and take the limits $k \to \infty$, $k$ odd and $k \to \infty$, $k$ even). And therefore in the LHS there are at least $n$ eigenvalues equal to zero. so $A$ has $p$ zero eigenvalues and $B$ has $n-p$ zero eigenvalues. I feel like it is not long until the end, but I can't get any further ideas. How to proceed from here?",,"['linear-algebra', 'matrices']"
18,Computational complexity of computing the determinant,Computational complexity of computing the determinant,,"The formula for the determinant of an $n$ by $ n$ matrix given by expansion of minors involves $n!$ terms. As such, computing the determinant of a given matrix of with integer entries via expansion by minors takes a number of steps is bounded below by $n!$ . (In practice the number of steps required depends on the size  of the matrix entries). However, the determinant of such a matrix can also be computed by Gaussian Elimination; we know how each elementary row operation affects the determinant of a matrix and if we keep track of the row reduction steps which we perform in the process of performing Gaussian elimination we can almost immediately reconstruct the determinant of the original matrix from this data. The number of steps that it takes to row reduce a matrix with integer entries is $O(n^3)$ and so this method of computing the determinant of a matrix takes $O(n^3)$ steps. Juxtaposing the two methods above, one sees that the computational problem under discussion looks to be very time consuming from one perspective but is much faster from another perspective. In this respect the problem is analogous to that of computing a partial sum of a telescoping series. And yet it's not at all clear to me how see the ""implicit cancellation"" from the formula that comes from expansion by minors. Indeed, the formula bears a strong superficial similarity with that of the formula for the permanent of a matrix, the computation of which is #P-complete . The fact that half of the terms of the determinant have negative signs in front of them makes the formula for the determinant look more like a telescoping sum than the formula for the permanent, but not by very much. Is there a way to see the fact that the determinant of a matrix is   bounded by a polynomial in the dimension of the matrix and size of the   entries directly from the formula given by expansion by minors?","The formula for the determinant of an $n$ by $ n$ matrix given by expansion of minors involves $n!$ terms. As such, computing the determinant of a given matrix of with integer entries via expansion by minors takes a number of steps is bounded below by $n!$ . (In practice the number of steps required depends on the size  of the matrix entries). However, the determinant of such a matrix can also be computed by Gaussian Elimination; we know how each elementary row operation affects the determinant of a matrix and if we keep track of the row reduction steps which we perform in the process of performing Gaussian elimination we can almost immediately reconstruct the determinant of the original matrix from this data. The number of steps that it takes to row reduce a matrix with integer entries is $O(n^3)$ and so this method of computing the determinant of a matrix takes $O(n^3)$ steps. Juxtaposing the two methods above, one sees that the computational problem under discussion looks to be very time consuming from one perspective but is much faster from another perspective. In this respect the problem is analogous to that of computing a partial sum of a telescoping series. And yet it's not at all clear to me how see the ""implicit cancellation"" from the formula that comes from expansion by minors. Indeed, the formula bears a strong superficial similarity with that of the formula for the permanent of a matrix, the computation of which is #P-complete . The fact that half of the terms of the determinant have negative signs in front of them makes the formula for the determinant look more like a telescoping sum than the formula for the permanent, but not by very much. Is there a way to see the fact that the determinant of a matrix is   bounded by a polynomial in the dimension of the matrix and size of the   entries directly from the formula given by expansion by minors?",,"['linear-algebra', 'soft-question', 'computer-science']"
19,What is the intuition behind / How can we interpret the eigenvalues and eigenvectors of Euclidean Distance Matrices?,What is the intuition behind / How can we interpret the eigenvalues and eigenvectors of Euclidean Distance Matrices?,,"Given a set of points $x_1,x_2,\dots,x_m$ in the euclidean space $\mathbb{R}^n$, we can form a $m\times m$ Euclidean Distance Matrix $D$ where $D_{ij}={\|x_i-x_j\|}^2$. We know a little bit about these matrices like: It is symmetric. Its Trace is $0$. It has (at most) $n+2$ non-zero eigenvalues; It has exactly $n+2$ non-zero eigenvalues whenever $m > n$; Source: Relationship between eigenvalues of two related, Euclidean distance matrices What is the intuition behind the eigenvalues and eigenvectors of such matrices? In the case of a covariance matrix formed from data points, we can say that the eigenvectors are the directions in the the spread of data is maximum and these are called as principal components. In the case of adjacency matrices of graphs also, there seems to be an interpretation for the eigenvectors as given here: http://daylateanddollarshort.com/math/pdfs/spectral.pdf Is there a similar interpretation for these Euclidean Distance Matrices (EDM's)? Thank You. Even partial answers and ideas are welcome.","Given a set of points $x_1,x_2,\dots,x_m$ in the euclidean space $\mathbb{R}^n$, we can form a $m\times m$ Euclidean Distance Matrix $D$ where $D_{ij}={\|x_i-x_j\|}^2$. We know a little bit about these matrices like: It is symmetric. Its Trace is $0$. It has (at most) $n+2$ non-zero eigenvalues; It has exactly $n+2$ non-zero eigenvalues whenever $m > n$; Source: Relationship between eigenvalues of two related, Euclidean distance matrices What is the intuition behind the eigenvalues and eigenvectors of such matrices? In the case of a covariance matrix formed from data points, we can say that the eigenvectors are the directions in the the spread of data is maximum and these are called as principal components. In the case of adjacency matrices of graphs also, there seems to be an interpretation for the eigenvectors as given here: http://daylateanddollarshort.com/math/pdfs/spectral.pdf Is there a similar interpretation for these Euclidean Distance Matrices (EDM's)? Thank You. Even partial answers and ideas are welcome.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'intuition', 'spectral-graph-theory']"
20,"Oh Times, $\otimes$ in linear algebra and tensors","Oh Times,  in linear algebra and tensors",\otimes,"Can I have some clarification of the different meanings of $\otimes$ as in the unifying and separating implications in basic linear algebra and tensors? Here is some of the overloading of this symbol... 1.1. Kronecker matrix product : If $A$ is an $m \times n$ matrix and $B$ is a $p \times q$ matrix, then the Kronecker product A ⊗ B is the $mp \times nq$ block matrix: $$A\color{red}{\otimes}B=\begin{bmatrix}a_{11}\mathbf B&\cdots&a_{1n}\mathbf B\\\vdots&\ddots&\vdots\\a_{m1}\mathbf B&\cdots&a_{mn}\mathbf B\end{bmatrix}$$ 1.2. Outer product : $\mathbf u \otimes \mathbf v = \mathbf{uv}^\top = \begin{bmatrix}u_1\\u_2\\u_3\\u_4\end{bmatrix}\begin{bmatrix}v_1&v_2&v_3&v_4\end{bmatrix}=\begin{bmatrix}u_1v_1&u_1v_2&u_1v_3\\u_2v_1&u_2v_2&u_2v_3\\u_3v_1&u_3v_2&u_3v_3\end{bmatrix}$ Definition of the tensor space : $$\begin{align}T^p_q\,V &= \underset{p}{\underbrace{V\color{darkorange}{\otimes}\cdots\color{darkorange}{\otimes} V}} \color{darkorange}{\otimes} \underset{q}{\underbrace{V^*\color{darkorange}{\otimes}\cdots\color{darkorange}{\otimes} V^*}}:=\{T\, |\, T\, \text{ is a (p,q) tensor}\}\\[3ex]&=\{T: \underset{p}{\underbrace{V^*\times \cdots \times V^*}}\times \underset{q}{\underbrace{V\times \cdots \times V}} \overset{\sim}\rightarrow K\}\end{align}$$ Definition of the tensor product : It takes $T\in T_q^p V$ and $S\in T^r_s V$ so that: $$T\color{blue}{\otimes}S\in T_{q+s}^{p+r}V$$ defined as: $$\begin{align}&(T\color{blue}{\otimes}S)(\underbrace{ \omega_1,\cdots,\omega_q,\cdots,\omega_{q+s}, v_1,\cdots,v_p,\cdots,v_{p+r}}_\text{'eats'})\\&:= T(\underbrace{\omega_1,\cdots,\omega_q, v_1,\cdots,v_p}_{\text{'eats up' p vec's + q covec's}\rightarrow \text{no.}})\underbrace{\cdot}_{\text{in the field}}S(\underbrace{\omega_{q+1},\cdots,\omega_{q+s}, v_{p+1},\cdots,v_{p+r}}_{\text{'eats up' p vec's and q covec's} \rightarrow\text{no.}})\end{align}$$ An example of, for instance, some operation like $\underbrace{e_{a_1}\color{blue}{\otimes}\cdots\color{blue}{\otimes}e_{a_p}\color{blue}{\otimes} \epsilon^{b_1}\color{blue}{\otimes}\cdots\color{blue}{\otimes}\epsilon^{b_q}}_{(p,q)\text{ tensor}}$ after settling for some basis could be helpful. For clarity this is a fragment of the more daunting expression: $$ T=\underbrace{\sum_{a_1=1}^{\text{dim v sp.}}\cdots\sum_{b_1=1}^{\text{dim v sp.}}}_{\text{p + q sums (usually omitted)}}\underbrace{\color{green}{T^{\overbrace{a_1,\cdots,a_p}^{\text{numbers}}}_{\quad\quad\quad\quad\underbrace{b_1,\cdots,b_q}_{\text{numbers}}}}}_{\text{a number}}\underbrace{\cdot}_{\text{S-multiplication}}\underbrace{e_{a_1}\color{blue}{\otimes}\cdots\color{blue}{\otimes}e_{a_p}\color{blue}{\otimes} \epsilon^{b_1}\color{blue}{\otimes}\cdots\color{blue}{\otimes}\epsilon^{b_q}}_{(p,q)\text{ tensor}}$$ showing how to recuperate a tensor from its components. I realize that there is a connection as stated here : The Kronecker product of matrices corresponds to the abstract tensor product of linear maps. Specifically, if the vector spaces $V, W, X$, and $Y$ have bases $\{v_1, \cdots, v_m\}, \{w_1,\cdots, w_n\}, \{x_1,\cdots, x_d\},$ and $\{y_1, \cdots, y_e\}$, respectively, and if the matrices $A$ and $B$ represent the linear transformations $S : V \rightarrow X$ and $T : W \rightarrow Y$, respectively in the appropriate bases, then the matrix $A ⊗ B$ represents the tensor product of the two maps, $S ⊗ T : V ⊗ W → X ⊗ Y$ with respect to the basis $\{v_1 ⊗ w_1, v_1 ⊗ w_2, \cdots, v_2 ⊗ w_1, \cdots, v_m ⊗ w_n\}$ of $V ⊗ W$ and the similarly defined basis of $X ⊗ Y$ with the property that $A ⊗ B(v_i ⊗ w_j) = (Av_i) ⊗ (Bw_j)$, where $i$ and $j$ are integers in the proper range. But it is still elusive...","Can I have some clarification of the different meanings of $\otimes$ as in the unifying and separating implications in basic linear algebra and tensors? Here is some of the overloading of this symbol... 1.1. Kronecker matrix product : If $A$ is an $m \times n$ matrix and $B$ is a $p \times q$ matrix, then the Kronecker product A ⊗ B is the $mp \times nq$ block matrix: $$A\color{red}{\otimes}B=\begin{bmatrix}a_{11}\mathbf B&\cdots&a_{1n}\mathbf B\\\vdots&\ddots&\vdots\\a_{m1}\mathbf B&\cdots&a_{mn}\mathbf B\end{bmatrix}$$ 1.2. Outer product : $\mathbf u \otimes \mathbf v = \mathbf{uv}^\top = \begin{bmatrix}u_1\\u_2\\u_3\\u_4\end{bmatrix}\begin{bmatrix}v_1&v_2&v_3&v_4\end{bmatrix}=\begin{bmatrix}u_1v_1&u_1v_2&u_1v_3\\u_2v_1&u_2v_2&u_2v_3\\u_3v_1&u_3v_2&u_3v_3\end{bmatrix}$ Definition of the tensor space : $$\begin{align}T^p_q\,V &= \underset{p}{\underbrace{V\color{darkorange}{\otimes}\cdots\color{darkorange}{\otimes} V}} \color{darkorange}{\otimes} \underset{q}{\underbrace{V^*\color{darkorange}{\otimes}\cdots\color{darkorange}{\otimes} V^*}}:=\{T\, |\, T\, \text{ is a (p,q) tensor}\}\\[3ex]&=\{T: \underset{p}{\underbrace{V^*\times \cdots \times V^*}}\times \underset{q}{\underbrace{V\times \cdots \times V}} \overset{\sim}\rightarrow K\}\end{align}$$ Definition of the tensor product : It takes $T\in T_q^p V$ and $S\in T^r_s V$ so that: $$T\color{blue}{\otimes}S\in T_{q+s}^{p+r}V$$ defined as: $$\begin{align}&(T\color{blue}{\otimes}S)(\underbrace{ \omega_1,\cdots,\omega_q,\cdots,\omega_{q+s}, v_1,\cdots,v_p,\cdots,v_{p+r}}_\text{'eats'})\\&:= T(\underbrace{\omega_1,\cdots,\omega_q, v_1,\cdots,v_p}_{\text{'eats up' p vec's + q covec's}\rightarrow \text{no.}})\underbrace{\cdot}_{\text{in the field}}S(\underbrace{\omega_{q+1},\cdots,\omega_{q+s}, v_{p+1},\cdots,v_{p+r}}_{\text{'eats up' p vec's and q covec's} \rightarrow\text{no.}})\end{align}$$ An example of, for instance, some operation like $\underbrace{e_{a_1}\color{blue}{\otimes}\cdots\color{blue}{\otimes}e_{a_p}\color{blue}{\otimes} \epsilon^{b_1}\color{blue}{\otimes}\cdots\color{blue}{\otimes}\epsilon^{b_q}}_{(p,q)\text{ tensor}}$ after settling for some basis could be helpful. For clarity this is a fragment of the more daunting expression: $$ T=\underbrace{\sum_{a_1=1}^{\text{dim v sp.}}\cdots\sum_{b_1=1}^{\text{dim v sp.}}}_{\text{p + q sums (usually omitted)}}\underbrace{\color{green}{T^{\overbrace{a_1,\cdots,a_p}^{\text{numbers}}}_{\quad\quad\quad\quad\underbrace{b_1,\cdots,b_q}_{\text{numbers}}}}}_{\text{a number}}\underbrace{\cdot}_{\text{S-multiplication}}\underbrace{e_{a_1}\color{blue}{\otimes}\cdots\color{blue}{\otimes}e_{a_p}\color{blue}{\otimes} \epsilon^{b_1}\color{blue}{\otimes}\cdots\color{blue}{\otimes}\epsilon^{b_q}}_{(p,q)\text{ tensor}}$$ showing how to recuperate a tensor from its components. I realize that there is a connection as stated here : The Kronecker product of matrices corresponds to the abstract tensor product of linear maps. Specifically, if the vector spaces $V, W, X$, and $Y$ have bases $\{v_1, \cdots, v_m\}, \{w_1,\cdots, w_n\}, \{x_1,\cdots, x_d\},$ and $\{y_1, \cdots, y_e\}$, respectively, and if the matrices $A$ and $B$ represent the linear transformations $S : V \rightarrow X$ and $T : W \rightarrow Y$, respectively in the appropriate bases, then the matrix $A ⊗ B$ represents the tensor product of the two maps, $S ⊗ T : V ⊗ W → X ⊗ Y$ with respect to the basis $\{v_1 ⊗ w_1, v_1 ⊗ w_2, \cdots, v_2 ⊗ w_1, \cdots, v_m ⊗ w_n\}$ of $V ⊗ W$ and the similarly defined basis of $X ⊗ Y$ with the property that $A ⊗ B(v_i ⊗ w_j) = (Av_i) ⊗ (Bw_j)$, where $i$ and $j$ are integers in the proper range. But it is still elusive...",,"['linear-algebra', 'abstract-algebra', 'tensor-products', 'tensors']"
21,Tensor products of infinite-dimensional spaces and other objects,Tensor products of infinite-dimensional spaces and other objects,,"It has just occurred to me that most of my intuition for tensor products is derived from the special case of finite-dimensional vector spaces, so I'm wondering which properties I've taken for granted are true in general, and which are not. Let $U$ and $V$ be $k$-vector spaces, possibly infinite-dimensional. Does it remain true that $U^* \otimes V \cong \textrm{Hom}(U, V)$ naturally in $U$ and $V$? Let $A, B, C$ be objects in an abelian category, or better, a monoidal closed category. Is it true that $\textrm{Hom}(A, B \otimes C) \cong \textrm{Hom}(A, B) \otimes \textrm{Hom}(A, C)$ naturally in $A, B, C$? (Motivation: $\textrm{Hom}(A, -)$ preserves (cartesian) products.) In the same context as above, is there a bifunctor $\mathscr{F}(-, -)$ such that $\textrm{Hom}(A, C) \otimes \textrm{Hom}(B, C) \cong \textrm{Hom}(\mathscr{F}(A, B), C)$ naturally in $A, B, C$? (Motivation: $\textrm{Hom}(-, C)$ maps coproducts to products.)","It has just occurred to me that most of my intuition for tensor products is derived from the special case of finite-dimensional vector spaces, so I'm wondering which properties I've taken for granted are true in general, and which are not. Let $U$ and $V$ be $k$-vector spaces, possibly infinite-dimensional. Does it remain true that $U^* \otimes V \cong \textrm{Hom}(U, V)$ naturally in $U$ and $V$? Let $A, B, C$ be objects in an abelian category, or better, a monoidal closed category. Is it true that $\textrm{Hom}(A, B \otimes C) \cong \textrm{Hom}(A, B) \otimes \textrm{Hom}(A, C)$ naturally in $A, B, C$? (Motivation: $\textrm{Hom}(A, -)$ preserves (cartesian) products.) In the same context as above, is there a bifunctor $\mathscr{F}(-, -)$ such that $\textrm{Hom}(A, C) \otimes \textrm{Hom}(B, C) \cong \textrm{Hom}(\mathscr{F}(A, B), C)$ naturally in $A, B, C$? (Motivation: $\textrm{Hom}(-, C)$ maps coproducts to products.)",,"['linear-algebra', 'commutative-algebra', 'tensor-products']"
22,"Is there anything ""nice"" about the set of normal matrices (over $\Bbb R$ and $\Bbb C$?)","Is there anything ""nice"" about the set of normal matrices (over  and ?)",\Bbb R \Bbb C,"Normal matrices are of course useful to any linear algebra buff, not least because of the spectral theorem .  However, taken as a whole, they tend to have some not-so-nice properties.  For example: unlike the unitary matrices, they are not closed under multiplication.  That is, $A$ and $B$ being normal does not guarantee that $AB$ is normal unlike the Hermitian matrices, they don't form a linear space.  That is, $A$ and $B$ being normal does not guarantee that $A+B$ is normal I'm wondering, then, if there are any nice properties of this set, or any good ways of thinking of how the set of normal matrices sits within $\Bbb C^{n\times n}$, or perhaps $\Bbb R^{n \times n}$. Here's the way I see it: the normal matrices are the zero-set of the quadratic map $\phi:\Bbb F^{n \times n} \to \Bbb F^{n \times n}$ ($\Bbb F \in \Bbb{\{R,C\}}$) given by $$ \phi(A) = A^*A - AA^* $$ The set is closed.  It is (I think) a smooth manifold in $\Bbb R^{n \times n}$, and it is connected in $\Bbb C^{n \times n}$. Is it also connected in $\Bbb R^{n \times n}$?  What is its dimension as a manifold in $\Bbb R^{n \times n}$?  Do we know anything about its topological structure (Cohomology, for instance)?  Is there some underlying algebraic structure to the ""space"" of normal matrices?  Is there some interesting connection to draw between the unitary matrices and the Hermitian matrices that comes out of this analysis (the exponential map comes to mind)? Any input here is appreciated.  Thanks for reading :)","Normal matrices are of course useful to any linear algebra buff, not least because of the spectral theorem .  However, taken as a whole, they tend to have some not-so-nice properties.  For example: unlike the unitary matrices, they are not closed under multiplication.  That is, $A$ and $B$ being normal does not guarantee that $AB$ is normal unlike the Hermitian matrices, they don't form a linear space.  That is, $A$ and $B$ being normal does not guarantee that $A+B$ is normal I'm wondering, then, if there are any nice properties of this set, or any good ways of thinking of how the set of normal matrices sits within $\Bbb C^{n\times n}$, or perhaps $\Bbb R^{n \times n}$. Here's the way I see it: the normal matrices are the zero-set of the quadratic map $\phi:\Bbb F^{n \times n} \to \Bbb F^{n \times n}$ ($\Bbb F \in \Bbb{\{R,C\}}$) given by $$ \phi(A) = A^*A - AA^* $$ The set is closed.  It is (I think) a smooth manifold in $\Bbb R^{n \times n}$, and it is connected in $\Bbb C^{n \times n}$. Is it also connected in $\Bbb R^{n \times n}$?  What is its dimension as a manifold in $\Bbb R^{n \times n}$?  Do we know anything about its topological structure (Cohomology, for instance)?  Is there some underlying algebraic structure to the ""space"" of normal matrices?  Is there some interesting connection to draw between the unitary matrices and the Hermitian matrices that comes out of this analysis (the exponential map comes to mind)? Any input here is appreciated.  Thanks for reading :)",,"['linear-algebra', 'matrices', 'reference-request', 'algebraic-topology', 'differential-topology']"
23,The benefit of LU decomposition over explicitly computing the inverse,The benefit of LU decomposition over explicitly computing the inverse,,"I'm going to teach a linear algebra course in the fall, and I want to motivate the topic of matrix factorizations such as the LU decomposition. A natural question one can ask is, why care about this when one already knows how to compute $A^{-1}$ and can use it to solve $Ax=b$ just by matrix-vector multiplication? The standard answer is that is that in practice one should (almost) never actually invert a matrix , because you will incur less roundoff error by backsubstituting a factorization like $LUx = b$ than by performing the multiplication $x=A^{-1}b$. However, I recently came across the paper "" How accurate is inv(A)*b ? "" by Druinsky and Toledo (2012) where they argue that the received wisdom is misleading, and a solution $x_{\text{inv}}=Vx$ obtained using an approximate inverse $V\approx A^{-1}$ is typically just as close to the true $x$ as a solution $x_{\text{LU}}$ obtained by backsubstitution. So I am no longer as sure about numerical accuracy as a motivation for the LU decomposition as I used to be. I did a few numerical experiments with random ill-conditioned matrices in Matlab, based on Druinsky and Toledo's code in Sec. 6 of their paper. It seems like the forward errors $\|x_{\text{inv}}-x\|$ and $\|x_{\text{LU}}-x\|$ were indeed usually quite similar, but the backward error $\|Ax_{\text{inv}}-b\|$ could be much bigger than $\|Ax_{\text{LU}}-b\|$. I also worked out a tiny example by hand: $$\begin{align} A &= \begin{bmatrix}1.00&2.01 \\ 1.01 & 2.03\end{bmatrix} \\ &= \underbrace{\begin{bmatrix}1.00 & 2.01 \\ 0 & -1.00\times10^{-4}\end{bmatrix}}_{L}\underbrace{\begin{bmatrix}1 & 0 \\ 1.01 & 1\end{bmatrix}}_{U} \\ &= {\underbrace{\begin{bmatrix}-2.03\times10^4 & 2.01\times10^4 \\ 1.01\times10^4 & -1.00\times10^4 \end{bmatrix}}_{A^{-1}}}^{-1}, \\ b &= \begin{bmatrix}1.01 \\ 1.02\end{bmatrix}. \end{align}$$ The exact solution is $x=[-1, 1]^T$. Computing $A^{-1}b$ with three decimal digits of precision in the intermediate calculations yields the result $x_{\text{inv}} = [0, 0]^T$ due to catastrophic cancellation. The same precision in LU backsubstitution gives $x_{\text{LU}} = [1.01, 0]^T$. Both solutions clearly differ from the true solution $x$ by an error on the order of $10^0$, but $Ax_{\text{LU}}$ matches $b$ to numerical precision while $Ax_{\text{inv}}$ is totally off. My questions are: Is the explicit $2\times2$ example above representative of what happens in practice, or is it a meaningless example computed with too little precision that just coincidentally happens to match the other numerical experiments? Are the numerical observations generally true? To make this precise, let $A=LU$ and define $V = U^{-1}*L^{-1}$, $x_{\text{inv}} = V*b$, and $x_{\text{LU}}= U^{-1}*(L^{-1}*b)$, where $*$ denotes multiplication with numerical roundoff error. Is it usually the case that $\|Ax_{\text{inv}}-b\|>\|Ax_{\text{LU}}-b\|$? What are some examples of practical applications where forward error is more important than backward error, and vice versa?","I'm going to teach a linear algebra course in the fall, and I want to motivate the topic of matrix factorizations such as the LU decomposition. A natural question one can ask is, why care about this when one already knows how to compute $A^{-1}$ and can use it to solve $Ax=b$ just by matrix-vector multiplication? The standard answer is that is that in practice one should (almost) never actually invert a matrix , because you will incur less roundoff error by backsubstituting a factorization like $LUx = b$ than by performing the multiplication $x=A^{-1}b$. However, I recently came across the paper "" How accurate is inv(A)*b ? "" by Druinsky and Toledo (2012) where they argue that the received wisdom is misleading, and a solution $x_{\text{inv}}=Vx$ obtained using an approximate inverse $V\approx A^{-1}$ is typically just as close to the true $x$ as a solution $x_{\text{LU}}$ obtained by backsubstitution. So I am no longer as sure about numerical accuracy as a motivation for the LU decomposition as I used to be. I did a few numerical experiments with random ill-conditioned matrices in Matlab, based on Druinsky and Toledo's code in Sec. 6 of their paper. It seems like the forward errors $\|x_{\text{inv}}-x\|$ and $\|x_{\text{LU}}-x\|$ were indeed usually quite similar, but the backward error $\|Ax_{\text{inv}}-b\|$ could be much bigger than $\|Ax_{\text{LU}}-b\|$. I also worked out a tiny example by hand: $$\begin{align} A &= \begin{bmatrix}1.00&2.01 \\ 1.01 & 2.03\end{bmatrix} \\ &= \underbrace{\begin{bmatrix}1.00 & 2.01 \\ 0 & -1.00\times10^{-4}\end{bmatrix}}_{L}\underbrace{\begin{bmatrix}1 & 0 \\ 1.01 & 1\end{bmatrix}}_{U} \\ &= {\underbrace{\begin{bmatrix}-2.03\times10^4 & 2.01\times10^4 \\ 1.01\times10^4 & -1.00\times10^4 \end{bmatrix}}_{A^{-1}}}^{-1}, \\ b &= \begin{bmatrix}1.01 \\ 1.02\end{bmatrix}. \end{align}$$ The exact solution is $x=[-1, 1]^T$. Computing $A^{-1}b$ with three decimal digits of precision in the intermediate calculations yields the result $x_{\text{inv}} = [0, 0]^T$ due to catastrophic cancellation. The same precision in LU backsubstitution gives $x_{\text{LU}} = [1.01, 0]^T$. Both solutions clearly differ from the true solution $x$ by an error on the order of $10^0$, but $Ax_{\text{LU}}$ matches $b$ to numerical precision while $Ax_{\text{inv}}$ is totally off. My questions are: Is the explicit $2\times2$ example above representative of what happens in practice, or is it a meaningless example computed with too little precision that just coincidentally happens to match the other numerical experiments? Are the numerical observations generally true? To make this precise, let $A=LU$ and define $V = U^{-1}*L^{-1}$, $x_{\text{inv}} = V*b$, and $x_{\text{LU}}= U^{-1}*(L^{-1}*b)$, where $*$ denotes multiplication with numerical roundoff error. Is it usually the case that $\|Ax_{\text{inv}}-b\|>\|Ax_{\text{LU}}-b\|$? What are some examples of practical applications where forward error is more important than backward error, and vice versa?",,"['linear-algebra', 'matrices']"
24,How to find eigenvalues and eigenvectors of this matrix,How to find eigenvalues and eigenvectors of this matrix,,"Can you help to find eigenvalues and eigenvectors of the following matrix? Here is the matrix: $$C = \small \begin{pmatrix} -\sin(\theta_{2} - \theta_{M}) & \sin(\theta_{1} - \theta_{M}) & 0 & \ldots & 0 & \sin(\theta_{2} - \theta_{1}) \\  \sin(\theta_{3} - \theta_{2}) & -\sin(\theta_{3} - \theta_{1}) & \sin(\theta_{2} - \theta_{1}) & \ldots & 0 & 0 \\  0 & \sin(\theta_{4} - \theta_{3}) & -\sin(\theta_{4} - \theta_{2}) & \ldots & 0 & 0 \\ 0 & \ddots & \ddots & \ddots & \ddots & \vdots\\ \sin(\theta_{M} - \theta_{M - 1}) & 0 & 0 & \ldots & \sin(\theta_{1} - \theta_{M}) & -\sin(\theta_{1} - \theta_{M - 1}) \\ \end{pmatrix} $$ where $0 = \theta_{1} < \ldots < \theta_{M} < 2 \pi$. I have found 2 vectors for eigenvalue 0: $$ v_{1} = (1, \cos(\theta_{1}), \ldots, \cos(\theta_{M - 1}))^{T} $$ $$ v_{2} = (0, \sin(\theta_{1}), \ldots, \sin(\theta_{M - 1}))^{T} $$ In case $\theta_{j} = \frac{2 \pi (j - 1)}{M}, \; j = 1, \ldots, M$ the matrix becomes circulant matrix , and it's obvious to find all its eigenvectors and eigenvalues. If it's essential for somebody, I can explain the application where this matrix appears. In article by Lele, Kulkarni, Willsky - ""Convex-polygon estimation from support-line measurements"" this matrix represents the conditions of consistent support function measurements. Each $i$-th row of matrix represent conditions that $i$-th descrete radius of curvature is positive. These are the criteria of consistency. Here is the detailed description of geometrical interpretation of this matrix. Defintion 1. The support function of convex body $K$ in $\mathbb{R}^{n}$ is defined as follows: $$ h_{K}(u) = \sup \limits_{x \in K} (x, u) $$ Each value of support function corresponds to 1 support hyperplane of convex body. Suppose that $n = 2$ (we work in plane). Maybe this picture will help you to understand what we are speaking about: We consider $M$ measurements $h_{1}, \ldots, h_{M}$ of support function at directions $u_{1}, u_{2}, \ldots, u_{M}$ (all $||u_{j}|| = 1$, and $u_{j} = (\cos \theta_{j}, \sin \theta_{j}$)), and want to reconstruct the body $K$ from these measurements. In ideal, we can just intersect half-spaces corresponding to support hyperspaces: $$ K = \bigcap \limits_{i = 1}^{M} \{x \in \mathbb{R}^{n} : \; (x, u_{i}) < (h_{i}, u_{i})\} $$ But measurements are noisy, so there could be no convex body that has such support function values at these directions. So we need to correct them. To do this we need some conditions representing the consistency of support function measurements. Here are these conditions: Theorem 1. A vector $h \in \mathbb{R}^{M}$ is the vector of support function measurements of some convex body if an only if $$ C h \geq 0 $$ The value $h_{i - 1} \sin(\theta_{i + 1} - \theta_{i}) - h_{i} \sin(\theta_{i + 1} - \theta_{i - 1}) + h_{i + 1} \sin(\theta_{i} - \theta_{i - 1})$ is the value of $i$-th descrete radius of curvature. If all these values are positive, then the measurements are consistent. The vectors $$ v_{1} = (1, \cos(\theta_{1}), \ldots, \cos(\theta_{M - 1}))^{T} $$ $$ v_{2} = (0, \sin(\theta_{1}), \ldots, \sin(\theta_{M - 1}))^{T} $$ are just vectors of support function values of convex bodies consisting only from one point: $(1, 0)$ for $v_{1}$ and $(0, 1)$ for $v_{2}$. Adding these vectors and their combinations to other support function measurements does not change descrete radio of curvature and correspond to translation of the convex body in $\mathbb{R}^{n}$.","Can you help to find eigenvalues and eigenvectors of the following matrix? Here is the matrix: $$C = \small \begin{pmatrix} -\sin(\theta_{2} - \theta_{M}) & \sin(\theta_{1} - \theta_{M}) & 0 & \ldots & 0 & \sin(\theta_{2} - \theta_{1}) \\  \sin(\theta_{3} - \theta_{2}) & -\sin(\theta_{3} - \theta_{1}) & \sin(\theta_{2} - \theta_{1}) & \ldots & 0 & 0 \\  0 & \sin(\theta_{4} - \theta_{3}) & -\sin(\theta_{4} - \theta_{2}) & \ldots & 0 & 0 \\ 0 & \ddots & \ddots & \ddots & \ddots & \vdots\\ \sin(\theta_{M} - \theta_{M - 1}) & 0 & 0 & \ldots & \sin(\theta_{1} - \theta_{M}) & -\sin(\theta_{1} - \theta_{M - 1}) \\ \end{pmatrix} $$ where $0 = \theta_{1} < \ldots < \theta_{M} < 2 \pi$. I have found 2 vectors for eigenvalue 0: $$ v_{1} = (1, \cos(\theta_{1}), \ldots, \cos(\theta_{M - 1}))^{T} $$ $$ v_{2} = (0, \sin(\theta_{1}), \ldots, \sin(\theta_{M - 1}))^{T} $$ In case $\theta_{j} = \frac{2 \pi (j - 1)}{M}, \; j = 1, \ldots, M$ the matrix becomes circulant matrix , and it's obvious to find all its eigenvectors and eigenvalues. If it's essential for somebody, I can explain the application where this matrix appears. In article by Lele, Kulkarni, Willsky - ""Convex-polygon estimation from support-line measurements"" this matrix represents the conditions of consistent support function measurements. Each $i$-th row of matrix represent conditions that $i$-th descrete radius of curvature is positive. These are the criteria of consistency. Here is the detailed description of geometrical interpretation of this matrix. Defintion 1. The support function of convex body $K$ in $\mathbb{R}^{n}$ is defined as follows: $$ h_{K}(u) = \sup \limits_{x \in K} (x, u) $$ Each value of support function corresponds to 1 support hyperplane of convex body. Suppose that $n = 2$ (we work in plane). Maybe this picture will help you to understand what we are speaking about: We consider $M$ measurements $h_{1}, \ldots, h_{M}$ of support function at directions $u_{1}, u_{2}, \ldots, u_{M}$ (all $||u_{j}|| = 1$, and $u_{j} = (\cos \theta_{j}, \sin \theta_{j}$)), and want to reconstruct the body $K$ from these measurements. In ideal, we can just intersect half-spaces corresponding to support hyperspaces: $$ K = \bigcap \limits_{i = 1}^{M} \{x \in \mathbb{R}^{n} : \; (x, u_{i}) < (h_{i}, u_{i})\} $$ But measurements are noisy, so there could be no convex body that has such support function values at these directions. So we need to correct them. To do this we need some conditions representing the consistency of support function measurements. Here are these conditions: Theorem 1. A vector $h \in \mathbb{R}^{M}$ is the vector of support function measurements of some convex body if an only if $$ C h \geq 0 $$ The value $h_{i - 1} \sin(\theta_{i + 1} - \theta_{i}) - h_{i} \sin(\theta_{i + 1} - \theta_{i - 1}) + h_{i + 1} \sin(\theta_{i} - \theta_{i - 1})$ is the value of $i$-th descrete radius of curvature. If all these values are positive, then the measurements are consistent. The vectors $$ v_{1} = (1, \cos(\theta_{1}), \ldots, \cos(\theta_{M - 1}))^{T} $$ $$ v_{2} = (0, \sin(\theta_{1}), \ldots, \sin(\theta_{M - 1}))^{T} $$ are just vectors of support function values of convex bodies consisting only from one point: $(1, 0)$ for $v_{1}$ and $(0, 1)$ for $v_{2}$. Adding these vectors and their combinations to other support function measurements does not change descrete radio of curvature and correspond to translation of the convex body in $\mathbb{R}^{n}$.",,"['linear-algebra', 'matrices', 'geometry', 'differential-geometry', 'eigenvalues-eigenvectors']"
25,A possible converse to the Cayley-Hamilton theorem?,A possible converse to the Cayley-Hamilton theorem?,,"Happy new year MSE! During my holiday vacation I had an interesting idea! The Cayley-Hamilton theorem states that if $f:\mathbb C^n\to\mathbb C^n$ is a linear function, then it is a root of its own characteristic polynomial $\chi_f(f) = 0$. So I wondered: what if we had a function $f:\mathbb C^n \to \mathbb C^n$ satisfying a polynomial functionial equation (PFE), i.e. there is some polynomial $p=\sum a_k x^k$ such that $p(f)=0$, where we interpret $$ f^k  = \underbrace{f\circ\ldots\circ f}_{k \text{ -times}}$$ and $f^0 = \text{id}$. (replace product of variables with composition of functions) E.g. if $p=x^2+ax+b$ then $p(f)=0$ iff $f(f(x)) + af(x) +bx=0$ for all $x$. This is motivated by the fact that after all matrix multiplication is nothing but the composition of linear functions. Quesiton: Under which conditions would we be able to conclude that $f$ must be a linear function? Here a few things are important to keep in mind If $f$ solves the PFE $p(f)=0$, then so does $\phi^{-1} \circ f\circ \phi$ for any bijective function $\phi$. The way we write down $p$ matters, e.g. although $x(x-1) = x^2 -x$, the resulting PFE $f(f(x)-x) = 0$ and $f(f(x)) - f(x)= 0$ are  different. (This raises an interesting side question about under which conditions their solutions must coincide) General solutions to functional equations can be messy if no additional regularity assumptions are made (cf. Cauchy's equation ). With these caveats in mind I would question the validity of the following Conjecture: Let $f\colon\mathbb C^n\to\mathbb C^n$ be an entire function satisfying a PFE $p(f) =  0$. Then $f$ is conjugate linear, i.e. there exists a holomorphic bijection $\phi\in\text{Aut}(\mathbb C^n)$ such that $\phi^{-1}\circ f\circ\phi$ is linear. I did some digging in the literature and found this wonderful paper by Ahern and Rudin . They consider holomorphic $f$ that are functional roots of unity $f^m =\text{id}$ (also known as Babbage's equation), which is equivalent to the PFE given by $p=x^m-1$. Among other things they prove: If $f^m = \text{id}$ and $f$ is affine, i.e. $f(z)= Lz+c$, then $f$ is conjugate linear. If $f^m = \text{id}$ and $f$ has a fixed point, then $f$ is conjugate linear locally around it. If $f^m = \text{id}$ and $f$ is $\mathbb C^2\to\mathbb C^2$ and a finite composition of overshears , then $f$ is conjugate linear. Here and overshear is a map of the form $$\begin{pmatrix}x\\ y\end{pmatrix} \longrightarrow \begin{pmatrix}g(y)x+h(y)\\ y\end{pmatrix}$$ with $g,h$ entire and $g(y)\neq 0$ for all $y$; or more generally $f(x_i) = x_i$ for $i\neq j$ and $f(x_j) = g(x) x_j + h(x)$ where $g,h$ are entire, independent of $x_j$ and $g(x)\neq 0$ for all $x$. It is known that the set of all finite compositions of overshears forms a dense subgroup of $\text{Aut}(\mathbb C^n)$. There are also some known negative results of non-linearizable holomorphic automorphisms (e.g. Derksen 1997 ) but I don't understand enough of the advanced algebra to really fathom this paper and its possible implications on the question at hand. There are some simpler sub-problems that might be easier to track: Problem 1: Let $f(z)=Lz+c$ be affine and satisfy a PFE $p(f)=0$. Does $f$ admit a fixed point? In this case $f$ is conjugate linear by choosing $\phi$ to be the translation onto the fixed point. If false, this might be the easiest route towards a counter example. If true the next logical step should be to try Problem 2: If $f:\mathbb C^n \to \mathbb C^n$ is entire and solves the PFE $p(f)=0$, then $f$ admits a fixed point. Finally, a neat little observation I made is the following: if $f$ solves the PFE $p(f)=0$, and there exists a non-zero vector $v$ and entire function $g$ such that $f(\lambda v) = g(\lambda) v$ for all $\lambda \in \mathbb C$, then $f^k(\lambda v) = g^k(\lambda) v$, hence $g$ is a scalar function solution to the PFE $p(g)=0$. Maybe this indicates that some sort of Eigenvalue theory is possible? Anyway, it seems like some of this stuff is still untapped terrain so it might be worth some further investigation. Thanks for reading!","Happy new year MSE! During my holiday vacation I had an interesting idea! The Cayley-Hamilton theorem states that if $f:\mathbb C^n\to\mathbb C^n$ is a linear function, then it is a root of its own characteristic polynomial $\chi_f(f) = 0$. So I wondered: what if we had a function $f:\mathbb C^n \to \mathbb C^n$ satisfying a polynomial functionial equation (PFE), i.e. there is some polynomial $p=\sum a_k x^k$ such that $p(f)=0$, where we interpret $$ f^k  = \underbrace{f\circ\ldots\circ f}_{k \text{ -times}}$$ and $f^0 = \text{id}$. (replace product of variables with composition of functions) E.g. if $p=x^2+ax+b$ then $p(f)=0$ iff $f(f(x)) + af(x) +bx=0$ for all $x$. This is motivated by the fact that after all matrix multiplication is nothing but the composition of linear functions. Quesiton: Under which conditions would we be able to conclude that $f$ must be a linear function? Here a few things are important to keep in mind If $f$ solves the PFE $p(f)=0$, then so does $\phi^{-1} \circ f\circ \phi$ for any bijective function $\phi$. The way we write down $p$ matters, e.g. although $x(x-1) = x^2 -x$, the resulting PFE $f(f(x)-x) = 0$ and $f(f(x)) - f(x)= 0$ are  different. (This raises an interesting side question about under which conditions their solutions must coincide) General solutions to functional equations can be messy if no additional regularity assumptions are made (cf. Cauchy's equation ). With these caveats in mind I would question the validity of the following Conjecture: Let $f\colon\mathbb C^n\to\mathbb C^n$ be an entire function satisfying a PFE $p(f) =  0$. Then $f$ is conjugate linear, i.e. there exists a holomorphic bijection $\phi\in\text{Aut}(\mathbb C^n)$ such that $\phi^{-1}\circ f\circ\phi$ is linear. I did some digging in the literature and found this wonderful paper by Ahern and Rudin . They consider holomorphic $f$ that are functional roots of unity $f^m =\text{id}$ (also known as Babbage's equation), which is equivalent to the PFE given by $p=x^m-1$. Among other things they prove: If $f^m = \text{id}$ and $f$ is affine, i.e. $f(z)= Lz+c$, then $f$ is conjugate linear. If $f^m = \text{id}$ and $f$ has a fixed point, then $f$ is conjugate linear locally around it. If $f^m = \text{id}$ and $f$ is $\mathbb C^2\to\mathbb C^2$ and a finite composition of overshears , then $f$ is conjugate linear. Here and overshear is a map of the form $$\begin{pmatrix}x\\ y\end{pmatrix} \longrightarrow \begin{pmatrix}g(y)x+h(y)\\ y\end{pmatrix}$$ with $g,h$ entire and $g(y)\neq 0$ for all $y$; or more generally $f(x_i) = x_i$ for $i\neq j$ and $f(x_j) = g(x) x_j + h(x)$ where $g,h$ are entire, independent of $x_j$ and $g(x)\neq 0$ for all $x$. It is known that the set of all finite compositions of overshears forms a dense subgroup of $\text{Aut}(\mathbb C^n)$. There are also some known negative results of non-linearizable holomorphic automorphisms (e.g. Derksen 1997 ) but I don't understand enough of the advanced algebra to really fathom this paper and its possible implications on the question at hand. There are some simpler sub-problems that might be easier to track: Problem 1: Let $f(z)=Lz+c$ be affine and satisfy a PFE $p(f)=0$. Does $f$ admit a fixed point? In this case $f$ is conjugate linear by choosing $\phi$ to be the translation onto the fixed point. If false, this might be the easiest route towards a counter example. If true the next logical step should be to try Problem 2: If $f:\mathbb C^n \to \mathbb C^n$ is entire and solves the PFE $p(f)=0$, then $f$ admits a fixed point. Finally, a neat little observation I made is the following: if $f$ solves the PFE $p(f)=0$, and there exists a non-zero vector $v$ and entire function $g$ such that $f(\lambda v) = g(\lambda) v$ for all $\lambda \in \mathbb C$, then $f^k(\lambda v) = g^k(\lambda) v$, hence $g$ is a scalar function solution to the PFE $p(g)=0$. Maybe this indicates that some sort of Eigenvalue theory is possible? Anyway, it seems like some of this stuff is still untapped terrain so it might be worth some further investigation. Thanks for reading!",,"['linear-algebra', 'complex-analysis', 'functional-equations']"
26,Is there an elegant way to prove a function is linear?,Is there an elegant way to prove a function is linear?,,"I'm reading Hoffman and Kunze's linear algebra book and on page 73 in the exercise 7, they ask to verify this function $$T(x_1,x_2,x_3)=(x_1-x_2+2x_3,2x_1+x_2,-x_1-2x_2+2x_3)$$   is a linear transformation. This exercise is really simple, but a little bit tedious. We have to define arbitrary $u=(x_u,y_u,z_u)$ and $v=(x_v,y_v,z_v)$ elements of $F^3$ and show $T(u+v)=T(u)+T(v)$ and $T(ku)=kT(u)$ for $k\in F$. (we can see $F$ as $\mathbb R$ or $\mathbb C$) Is there a way more elegant to prove this function is linear?","I'm reading Hoffman and Kunze's linear algebra book and on page 73 in the exercise 7, they ask to verify this function $$T(x_1,x_2,x_3)=(x_1-x_2+2x_3,2x_1+x_2,-x_1-2x_2+2x_3)$$   is a linear transformation. This exercise is really simple, but a little bit tedious. We have to define arbitrary $u=(x_u,y_u,z_u)$ and $v=(x_v,y_v,z_v)$ elements of $F^3$ and show $T(u+v)=T(u)+T(v)$ and $T(ku)=kT(u)$ for $k\in F$. (we can see $F$ as $\mathbb R$ or $\mathbb C$) Is there a way more elegant to prove this function is linear?",,['linear-algebra']
27,Why are eigenvalues of nilpotent matrices equal to zero? [duplicate],Why are eigenvalues of nilpotent matrices equal to zero? [duplicate],,"This question already has answers here : Prove that the only eigenvalue of a nilpotent operator is 0? (2 answers) Closed 7 years ago . If $A$ is a $ \displaystyle  10 \times 10 $ matrix such that $A^{3} = 0$ but $A^{2}  \neq 0$ (so A is nilpotent) then I know that $A$ is not invertible, but why does at least one eigenvalue of $A$ have to be equal to zero? How would one show that all eigenvalues of $A$ are equal to zero?","This question already has answers here : Prove that the only eigenvalue of a nilpotent operator is 0? (2 answers) Closed 7 years ago . If $A$ is a $ \displaystyle  10 \times 10 $ matrix such that $A^{3} = 0$ but $A^{2}  \neq 0$ (so A is nilpotent) then I know that $A$ is not invertible, but why does at least one eigenvalue of $A$ have to be equal to zero? How would one show that all eigenvalues of $A$ are equal to zero?",,"['linear-algebra', 'matrices', 'nilpotence']"
28,Zero vector of a vector space,Zero vector of a vector space,,"I know that every vector space needs to contain a zero vector. But all the vector spaces I've seen have the zero vector actually being zero (e.g. $\mathbf{0}=\langle0,0,\ldots,0\rangle$). Can't the ""zero vector"" not involve zero, as long as it acts as the additive identity? If that's the case then are there any graphical representations of a vector space that does not contain the origin?","I know that every vector space needs to contain a zero vector. But all the vector spaces I've seen have the zero vector actually being zero (e.g. $\mathbf{0}=\langle0,0,\ldots,0\rangle$). Can't the ""zero vector"" not involve zero, as long as it acts as the additive identity? If that's the case then are there any graphical representations of a vector space that does not contain the origin?",,"['linear-algebra', 'vector-spaces', 'axioms']"
29,What is wrong with this proof that symmetric matrices commute?,What is wrong with this proof that symmetric matrices commute?,,"Symmetric matrices represent real self-adjoint maps, i.e. linear maps that have the following property: $$\langle\vec{v},f(\vec{w})\rangle=\langle f(\vec{v}),\vec{w}\rangle$$ where $\langle,\rangle$ donates the scalar (dot) product. Using this logic: $$\langle\vec{v},AB\vec{v}\rangle=\langle A\vec{v},B\vec{v}\rangle=\langle BA\vec{v},\vec{v}\rangle$$ Where $A$ and $B$ are symmetric matrices. Using the fact that the real scalar dot product is commutative: $$\langle BA\vec{v},\vec{v}\rangle=\langle\vec{v},BA\vec{v}\rangle$$ We therefore have the result: $$\langle\vec{v},AB\vec{v}\rangle=\langle\vec{v},BA\vec{v}\rangle$$ This holds true for any real vector $\vec{v}$ so therefore $AB=BA$ . However, symmetric matrices do not always commute so something is wrong with this proof.","Symmetric matrices represent real self-adjoint maps, i.e. linear maps that have the following property: where donates the scalar (dot) product. Using this logic: Where and are symmetric matrices. Using the fact that the real scalar dot product is commutative: We therefore have the result: This holds true for any real vector so therefore . However, symmetric matrices do not always commute so something is wrong with this proof.","\langle\vec{v},f(\vec{w})\rangle=\langle f(\vec{v}),\vec{w}\rangle \langle,\rangle \langle\vec{v},AB\vec{v}\rangle=\langle A\vec{v},B\vec{v}\rangle=\langle BA\vec{v},\vec{v}\rangle A B \langle BA\vec{v},\vec{v}\rangle=\langle\vec{v},BA\vec{v}\rangle \langle\vec{v},AB\vec{v}\rangle=\langle\vec{v},BA\vec{v}\rangle \vec{v} AB=BA","['linear-algebra', 'proof-verification', 'fake-proofs', 'symmetric-matrices', 'self-adjoint-operators']"
30,Why is this translation not a linear transformation? [closed],Why is this translation not a linear transformation? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question From Linear map , the sixth example : The translation $x \rightarrow x+1$ is not a linear transformation. Why? What about $x \rightarrow x +dx$ ? Is this translation a linear transformation? Does it matter if the transformation is not linear?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question From Linear map , the sixth example : The translation is not a linear transformation. Why? What about ? Is this translation a linear transformation? Does it matter if the transformation is not linear?",x \rightarrow x+1 x \rightarrow x +dx,"['terminology', 'linear-algebra']"
31,Characterization of the trace function,Characterization of the trace function,,"We know that the trace of a matrix is a linear map for all square matrices and that $\operatorname{trace}(AB)=\operatorname{trace}(BA)$ when the multiplication makes sense. On the Wikipedia page for trace , under properties, it says that these properties characterize the trace completely in the following sense: If $f$ is a linear function on the space of square matrices satisfying $f(xy)=f(yx)$ , then $f$ and $\operatorname{tr}$ are proportional. A note on the bottom of the page gives the justification, but I do not understand the logic of it. Thanks","We know that the trace of a matrix is a linear map for all square matrices and that when the multiplication makes sense. On the Wikipedia page for trace , under properties, it says that these properties characterize the trace completely in the following sense: If is a linear function on the space of square matrices satisfying , then and are proportional. A note on the bottom of the page gives the justification, but I do not understand the logic of it. Thanks",\operatorname{trace}(AB)=\operatorname{trace}(BA) f f(xy)=f(yx) f \operatorname{tr},"['linear-algebra', 'matrices']"
32,How do you determine which representation of a function to use for Newton's method?,How do you determine which representation of a function to use for Newton's method?,,"Take the equation: $$x^2 + 2x = \frac{1}{1+x^2}$$ I subtracted the right term over to form $~f_1(x)~$ : $$x^2 + 2x - \frac{1}{1+x^2} = 0$$ I wanted to take the derivative, so I rearranged things to make it a bit easier, call it $~f_2(x)~$ : $$x^4 + 2x^3 + x^2 + 2x - 1 = 0$$ I noticed when I graphed $~f_1(x)~$ and $~f_2(x)~$ that their plots were different $($ although they shared the same solution for $~x)~$ . Newton's method iterates down the graph line, so I'd imagine that Newton's method for these two equations are not equivalent. They'd find the same solution, but they would get there different ways. In that case, is there a way to decide which equation to use for Newton's to obtain the best/quickest result?","Take the equation: I subtracted the right term over to form : I wanted to take the derivative, so I rearranged things to make it a bit easier, call it : I noticed when I graphed and that their plots were different although they shared the same solution for . Newton's method iterates down the graph line, so I'd imagine that Newton's method for these two equations are not equivalent. They'd find the same solution, but they would get there different ways. In that case, is there a way to decide which equation to use for Newton's to obtain the best/quickest result?",x^2 + 2x = \frac{1}{1+x^2} ~f_1(x)~ x^2 + 2x - \frac{1}{1+x^2} = 0 ~f_2(x)~ x^4 + 2x^3 + x^2 + 2x - 1 = 0 ~f_1(x)~ ~f_2(x)~ ( ~x)~,"['linear-algebra', 'numerical-methods']"
33,Proof that any linear system cannot have exactly 2 solutions.,Proof that any linear system cannot have exactly 2 solutions.,,"How would you go about proving that for any system of linear equations (whether all are homogenous or not) can only have either (if this is true): One solution Infinitely many solutions No solutions I found this a bit difficult to prove (even though its a very fundamental thing about any linear equation). The intuitive geometric explanation is that a line can only intersect at one point, and if it intersects at a later point, it can't be a linear equation, but I don't think this is a convincing proof. I thought of if you assume that there are two (or more, but I picked two) solutions for some linear system, then for the points in between Solution Set 1 : X 1 , X 2 ....., X n Solution Set 2 : X 1 , X 2 ....., X n Then (I think), the points between S 1 and S 2 , must be infinitely many points (and thus infinitely many solutions) such that these points can also satisfy the linear system, which would mean the system has 2 infinite solutions. However, I don't think this is rigorous enough and nor do I understand completely why its true. Can anyone help in explaining (correcting) and elaborating on the intuition and proof of this?","How would you go about proving that for any system of linear equations (whether all are homogenous or not) can only have either (if this is true): One solution Infinitely many solutions No solutions I found this a bit difficult to prove (even though its a very fundamental thing about any linear equation). The intuitive geometric explanation is that a line can only intersect at one point, and if it intersects at a later point, it can't be a linear equation, but I don't think this is a convincing proof. I thought of if you assume that there are two (or more, but I picked two) solutions for some linear system, then for the points in between Solution Set 1 : X 1 , X 2 ....., X n Solution Set 2 : X 1 , X 2 ....., X n Then (I think), the points between S 1 and S 2 , must be infinitely many points (and thus infinitely many solutions) such that these points can also satisfy the linear system, which would mean the system has 2 infinite solutions. However, I don't think this is rigorous enough and nor do I understand completely why its true. Can anyone help in explaining (correcting) and elaborating on the intuition and proof of this?",,"['linear-algebra', 'systems-of-equations']"
34,Equation of a plane passing through 3 points,Equation of a plane passing through 3 points,,"It should be simple, but I'm having trouble. The three points are $$A(1,-2,1)\qquad B(4,-2,-2)\qquad C(4,1,4)$$ The plane I get is $$x+2y+z+6=0$$ but it obviously does not pass through the three points $A,B,C$.","It should be simple, but I'm having trouble. The three points are $$A(1,-2,1)\qquad B(4,-2,-2)\qquad C(4,1,4)$$ The plane I get is $$x+2y+z+6=0$$ but it obviously does not pass through the three points $A,B,C$.",,"['linear-algebra', 'vectors', 'analytic-geometry', '3d']"
35,How to compute the smallest eigenvalue using the power iteration algorithm?,How to compute the smallest eigenvalue using the power iteration algorithm?,,"I need to write a program which computes the largest and the smallest (in terms of absolute value) eigenvalues using both power iteration and inverse iteration . I can find them using the inverse iteration , and I can also find the largest one using the power method . But I have no idea how to find the smallest one using the power method . I tried applying some kind of shift such as $A - \lambda_{max}I$, but to no avail. How can I modify the power method so that it computes the smallest eigenvalue?","I need to write a program which computes the largest and the smallest (in terms of absolute value) eigenvalues using both power iteration and inverse iteration . I can find them using the inverse iteration , and I can also find the largest one using the power method . But I have no idea how to find the smallest one using the power method . I tried applying some kind of shift such as $A - \lambda_{max}I$, but to no avail. How can I modify the power method so that it computes the smallest eigenvalue?",,"['linear-algebra', 'numerical-methods', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
36,"Why does $A^TA=I, \det A=1$ mean $A$ is a rotation matrix?",Why does  mean  is a rotation matrix?,"A^TA=I, \det A=1 A","I know if $A^TA=I$, $A$ is an orthogonal matrix. Orthogonal matrices also contain two different types: if $\det A=1$, $A$ is a rotation matrix; if $\det A=-1$, $A$ is a reflection matrix. My question is : what is the relationship between the determinant of $A$ and rotation/reflection. Can you explain why $\det A=\pm 1$ means $A$ is a rotation/reflection from the geometric perspective? EDIT : I think the following questions need be figured out first. Do rotation matrices only exist in 2D and 3D space? That is: for any dimensional matrix, as long as it is orthogonal and with determinant as 1, the matrix represents a rotation transformation, right? Note the orthogonality and determinant are applicable to arbitrary dimensional matrices. What is the most fundamental definition of a rotation transformation? Since an orthogonal matrix preserves length and angle, can we say an orthogonal matrix represents a ""rigid body"" transformation? ""Rigid body"" transformation contains two basic types: rotation and reflection?","I know if $A^TA=I$, $A$ is an orthogonal matrix. Orthogonal matrices also contain two different types: if $\det A=1$, $A$ is a rotation matrix; if $\det A=-1$, $A$ is a reflection matrix. My question is : what is the relationship between the determinant of $A$ and rotation/reflection. Can you explain why $\det A=\pm 1$ means $A$ is a rotation/reflection from the geometric perspective? EDIT : I think the following questions need be figured out first. Do rotation matrices only exist in 2D and 3D space? That is: for any dimensional matrix, as long as it is orthogonal and with determinant as 1, the matrix represents a rotation transformation, right? Note the orthogonality and determinant are applicable to arbitrary dimensional matrices. What is the most fundamental definition of a rotation transformation? Since an orthogonal matrix preserves length and angle, can we say an orthogonal matrix represents a ""rigid body"" transformation? ""Rigid body"" transformation contains two basic types: rotation and reflection?",,"['linear-algebra', 'matrices', 'geometry', 'rotations', 'orthogonal-matrices']"
37,Rank product of matrix compared to individual matrices. [duplicate],Rank product of matrix compared to individual matrices. [duplicate],,"This question already has answers here : Closed 11 years ago . Possible Duplicate: How to prove $\text{Rank}(AB)\leq \min(\text{Rank}(A), \text{Rank}(B))$? If $A$ is an $m\times n$ matrix and $B$ is a $n \times r$ matrix,    prove that the rank of matrix $AB$ is at most $\mathrm{rank}(A)$. I asked a similar question earlier phrased incorrectly. The above is closer to the actual question generalised.","This question already has answers here : Closed 11 years ago . Possible Duplicate: How to prove $\text{Rank}(AB)\leq \min(\text{Rank}(A), \text{Rank}(B))$? If $A$ is an $m\times n$ matrix and $B$ is a $n \times r$ matrix,    prove that the rank of matrix $AB$ is at most $\mathrm{rank}(A)$. I asked a similar question earlier phrased incorrectly. The above is closer to the actual question generalised.",,"['linear-algebra', 'matrices', 'matrix-rank']"
38,How does linear algebra help with computer science?,How does linear algebra help with computer science?,,"I'm a Computer Science student. I've just completed a linear algebra course. I got 75 points out of 100 points on the final exam. I know linear algebra well. As a programmer, I'm having a difficult time understanding how linear algebra helps with computer science? Can someone please clear me up on this topic?","I'm a Computer Science student. I've just completed a linear algebra course. I got 75 points out of 100 points on the final exam. I know linear algebra well. As a programmer, I'm having a difficult time understanding how linear algebra helps with computer science? Can someone please clear me up on this topic?",,"['linear-algebra', 'soft-question', 'computer-science', 'big-list', 'applications']"
39,How to divide by a matrix,How to divide by a matrix,,"I found a question in an old exam, where the function $\phi(z) := \frac{\exp(z) - 1}{z}$ is given. Now we evaluate $\phi(\mathbf{A})$. But how do I divide by a matrix? I already thought about taking the inverse, but how do I now on which side I have to multiply the inverse?","I found a question in an old exam, where the function $\phi(z) := \frac{\exp(z) - 1}{z}$ is given. Now we evaluate $\phi(\mathbf{A})$. But how do I divide by a matrix? I already thought about taking the inverse, but how do I now on which side I have to multiply the inverse?",,"['linear-algebra', 'matrices', 'functions']"
40,How to check if a symmetric $4\times4$ matrix is positive semidefinite?,How to check if a symmetric  matrix is positive semidefinite?,4\times4,How does one  check whether symmetric $4\times4$ matrix is positive semidefinite? What if this matrix has also rank deficiency: is it rank $3$ ?,How does one  check whether symmetric matrix is positive semidefinite? What if this matrix has also rank deficiency: is it rank ?,4\times4 3,"['linear-algebra', 'matrices', 'symmetric-matrices', 'positive-semidefinite']"
41,Correlation matrix from Covariance matrix,Correlation matrix from Covariance matrix,,"This is for a project which I've been trying to find some information for Covariance matrix and correlation matrix. I understand that for a $n \times n$ matrix $A, AA^T$ will give me the covariance matrix. Is there any relationship between the covariance and correlation matrix? Sorry maybe I wasn't clear. I wanted to use Cholesky decomposition to generate correlated variables from random variables. I do know how to do it using matlab. And I understand how it works for 2 variables. But when I scale up the matrix to $n \times n$ instead of $2 \times 2$, I am not sure how it will work out. would appreciate if someone could provide more hint on the mathematics.","This is for a project which I've been trying to find some information for Covariance matrix and correlation matrix. I understand that for a $n \times n$ matrix $A, AA^T$ will give me the covariance matrix. Is there any relationship between the covariance and correlation matrix? Sorry maybe I wasn't clear. I wanted to use Cholesky decomposition to generate correlated variables from random variables. I do know how to do it using matlab. And I understand how it works for 2 variables. But when I scale up the matrix to $n \times n$ instead of $2 \times 2$, I am not sure how it will work out. would appreciate if someone could provide more hint on the mathematics.",,"['linear-algebra', 'statistics']"
42,Is Hoffman-Kunze a good book to read next?,Is Hoffman-Kunze a good book to read next?,,"I'm planning on self-studying linear algebra, and trying to decide on a book.  I'm thinking of using Hoffman and Kunze. What sort of experience is required to handle Hoffman and Kunze? So far, I've read most of Axler's Linear Algebra Done Right. (It was for a class in high school, so we just worked through it and got as far as we got.) I feel like I understand it pretty well, and I really liked it, but I've read that it has a rather unusual approach and I would like to try something different. I've read that Hoffman and Kunze is good, but that it is heavy on the algebra.  I'm not sure how do calibrate that, though.  Does it mean ""Don't use it for linear algebra for engineers"" or ""You should have a year of algebra, but if you have that, it's not a big deal"".  (I guess it's somewhere in between.) I specifically like that it includes a strong emphasis on matrices, which are pointedly ignored in Axler, without devolving into being just a manual for computation. This is my impression of the book from having read around (mostly here), but if something of it is wrong, please correct me.  I have very little experience to provide comparison and normalize the different recommendations I've read.","I'm planning on self-studying linear algebra, and trying to decide on a book.  I'm thinking of using Hoffman and Kunze. What sort of experience is required to handle Hoffman and Kunze? So far, I've read most of Axler's Linear Algebra Done Right. (It was for a class in high school, so we just worked through it and got as far as we got.) I feel like I understand it pretty well, and I really liked it, but I've read that it has a rather unusual approach and I would like to try something different. I've read that Hoffman and Kunze is good, but that it is heavy on the algebra.  I'm not sure how do calibrate that, though.  Does it mean ""Don't use it for linear algebra for engineers"" or ""You should have a year of algebra, but if you have that, it's not a big deal"".  (I guess it's somewhere in between.) I specifically like that it includes a strong emphasis on matrices, which are pointedly ignored in Axler, without devolving into being just a manual for computation. This is my impression of the book from having read around (mostly here), but if something of it is wrong, please correct me.  I have very little experience to provide comparison and normalize the different recommendations I've read.",,"['linear-algebra', 'reference-request', 'soft-question']"
43,Proof: Sum of dimension of orthogonal complement and vector subspace,Proof: Sum of dimension of orthogonal complement and vector subspace,,"Let $V$ be a finite dimensional real vector space with inner product $\langle \, , \rangle$ and let $W$ be a subspace of $V$. The orthogonal complement of $W$ is defined as   $$ W^\perp= \left\{ v \in V \,:\, \langle v,w \rangle = 0 \text{ for all } w \in W \right\}. $$   Prove the following: $\dim W + \dim W^\perp= \dim V$. I'm not sure how to find the relationship between number of basis vectors in $W$ and $W^\perp$.","Let $V$ be a finite dimensional real vector space with inner product $\langle \, , \rangle$ and let $W$ be a subspace of $V$. The orthogonal complement of $W$ is defined as   $$ W^\perp= \left\{ v \in V \,:\, \langle v,w \rangle = 0 \text{ for all } w \in W \right\}. $$   Prove the following: $\dim W + \dim W^\perp= \dim V$. I'm not sure how to find the relationship between number of basis vectors in $W$ and $W^\perp$.",,"['linear-algebra', 'vector-spaces', 'inner-products']"
44,Are there simple methods for calculating the determinant of symmetric matrices?,Are there simple methods for calculating the determinant of symmetric matrices?,,"I've seen that there are lots of exercises about determinants of symmetric matrices in my algebra books. Some are easy and others are a bit more twisted, but the basic problem is almost always the same. I have been trying to come up with a method to calculate these a bit more quickly, since—at least for me—they invariably end with a very ugly stream of numbers and letters. For example I started with a $3\times 3$ matrix like this: $$A= \begin{pmatrix} a & b & c \\ b & a & b \\ c & b & a \end{pmatrix}$$ which looks fairly simple, but the best I could come up with for the determinant was: $$2b^2(c-a)+a(a^2-c^2) \quad \text{ or } \quad a(a^2-2b^2-c^2)+2b^2c$$ These look horrific and absolutely not what anyone in his right mind would use. It goes without saying that I haven't even tried this with matrices bigger than $3\times 3$ . Is there something I have been missing, or is there nothing to do about it?","I've seen that there are lots of exercises about determinants of symmetric matrices in my algebra books. Some are easy and others are a bit more twisted, but the basic problem is almost always the same. I have been trying to come up with a method to calculate these a bit more quickly, since—at least for me—they invariably end with a very ugly stream of numbers and letters. For example I started with a matrix like this: which looks fairly simple, but the best I could come up with for the determinant was: These look horrific and absolutely not what anyone in his right mind would use. It goes without saying that I haven't even tried this with matrices bigger than . Is there something I have been missing, or is there nothing to do about it?","3\times 3 A= \begin{pmatrix}
a & b & c \\
b & a & b \\
c & b & a \end{pmatrix} 2b^2(c-a)+a(a^2-c^2)
\quad
\text{ or }
\quad
a(a^2-2b^2-c^2)+2b^2c 3\times 3","['linear-algebra', 'matrices']"
45,Product of a vector and its transpose (Projections),Product of a vector and its transpose (Projections),,"I am doing a basic course on linear algebra, where the guy says $a^Ta$ is a number and $aa^T$ is a matrix not.m Why? Background: Say we are projecting a vector $b$ onto a vector $a$. By the condition of orthogonality, the dot product is zero $$a^T(b-xa)=0$$  then $$x =\frac{a^Tb} {a^Ta}$$. The projection vector $p$ since it lies on $a$ is: $$p=ax$$ $$p=a\frac{a^Tb} {a^Ta}$$ $$p=\frac{aa^T} {a^Ta}b$$ To me both $aa^T$ and $a^Ta$ are dot products and the order shouldn't matter. Then  $p=b$. But it is not. Why?","I am doing a basic course on linear algebra, where the guy says $a^Ta$ is a number and $aa^T$ is a matrix not.m Why? Background: Say we are projecting a vector $b$ onto a vector $a$. By the condition of orthogonality, the dot product is zero $$a^T(b-xa)=0$$  then $$x =\frac{a^Tb} {a^Ta}$$. The projection vector $p$ since it lies on $a$ is: $$p=ax$$ $$p=a\frac{a^Tb} {a^Ta}$$ $$p=\frac{aa^T} {a^Ta}b$$ To me both $aa^T$ and $a^Ta$ are dot products and the order shouldn't matter. Then  $p=b$. But it is not. Why?",,"['linear-algebra', 'vectors', 'transpose', 'projection-matrices']"
46,"Why use Gauss Jordan Elimination instead of Gaussian Elimination, Differences","Why use Gauss Jordan Elimination instead of Gaussian Elimination, Differences",,"Why use Gaussian Elimination instead of Gauss Jordan Elimination and vice versa for solving systems of linear equations? What are the differences, benefits of each, etc.? I've just been solving linear equation systems, of the form Ax = B, by reducing matrix A to a diagonal matrix where every nonzero value equals 1. I'm not exactly sure if this would be considered Gaussian Elimination or Gauss-Jordan Elimination.","Why use Gaussian Elimination instead of Gauss Jordan Elimination and vice versa for solving systems of linear equations? What are the differences, benefits of each, etc.? I've just been solving linear equation systems, of the form Ax = B, by reducing matrix A to a diagonal matrix where every nonzero value equals 1. I'm not exactly sure if this would be considered Gaussian Elimination or Gauss-Jordan Elimination.",,"['linear-algebra', 'matrix-equations', 'gaussian-elimination']"
47,Determining whether a symmetric matrix is positive-definite (algorithm),Determining whether a symmetric matrix is positive-definite (algorithm),,"I'm trying to create a program, that will decompose a matrix using the Cholesky decomposition. The decomposition itself isn't a difficult algorithm, but a matrix, to be eligible for Cholesky decomposition, must be symmetric and positive-definite. Checking whether a matrix is symmetric is easy, but the positive part proves to be more complex. I've read about the Sylvester's criterion, but that leads to determinants, and based on what I found on the web, those are quite extensive and hard on computers. In a nutshell - is there something I might be missing? Due to the fact the the matrix is square or something like that, is there possibly a simpler way to determine whether it's positive? Regards, Paul","I'm trying to create a program, that will decompose a matrix using the Cholesky decomposition. The decomposition itself isn't a difficult algorithm, but a matrix, to be eligible for Cholesky decomposition, must be symmetric and positive-definite. Checking whether a matrix is symmetric is easy, but the positive part proves to be more complex. I've read about the Sylvester's criterion, but that leads to determinants, and based on what I found on the web, those are quite extensive and hard on computers. In a nutshell - is there something I might be missing? Due to the fact the the matrix is square or something like that, is there possibly a simpler way to determine whether it's positive? Regards, Paul",,"['linear-algebra', 'matrices', 'numerical-methods', 'numerical-linear-algebra', 'positive-definite']"
48,The determinant of adjugate matrix,The determinant of adjugate matrix,,"I have the following proof that I would like to be walked through because I'm not intuitively seeing what to do: If $A$ is $n\times n$, prove $\det\left(\operatorname{adj}(A)\right) = \det(A)^{n-1}$. I know the property of $A\operatorname{adj}(A) = \det(A)I$ is important but I don't know how to apply it to get an answer. Any help is much appreciated. Thanks,","I have the following proof that I would like to be walked through because I'm not intuitively seeing what to do: If $A$ is $n\times n$, prove $\det\left(\operatorname{adj}(A)\right) = \det(A)^{n-1}$. I know the property of $A\operatorname{adj}(A) = \det(A)I$ is important but I don't know how to apply it to get an answer. Any help is much appreciated. Thanks,",,"['linear-algebra', 'matrices', 'determinant']"
49,Why is the operator $2$-norm of a diagonal matrix its largest value?,Why is the operator -norm of a diagonal matrix its largest value?,2,"I read this in my textbook have tried working through it - I keep getting max 2-norm(Ax), which is just the magnitude of Ax. How should I do this proof? (note, this is not for homework, I'm just trying to understand why as no proof is provided).","I read this in my textbook have tried working through it - I keep getting max 2-norm(Ax), which is just the magnitude of Ax. How should I do this proof? (note, this is not for homework, I'm just trying to understand why as no proof is provided).",,"['linear-algebra', 'matrices', 'normed-spaces', 'matrix-norms', 'spectral-norm']"
50,Prove that Every Vector Space Has a Basis,Prove that Every Vector Space Has a Basis,,"My textbook extended the following proof to show that every vector space, including the infinite-dimensional case, has a basis. Condition: $S$ is a linearly independent subset of a vector space $V$. Theorem:  There is a maximal linearly independent subset of $V$ that contains $S$. Proof. Let $F$ be the family of all linearly independent subsets of $V$ that contains $S$.  If $C$ is a chain in $F$ and there exists a member $U \in F$ that contains each member of $C$, by the maximal principle, $U$ is the maximal element of $F$, the family of all linearly independent subsets of $V$.  As a result, $U$ is the maximal linearly independent subset of $V$.  So $U$ is the basis of $V$. Let $U$ be the union of the elements of $C$.  Clearly $U$ contains each element of $C$.  To show that $U$ is a linearly independent subset of $V$, first note that $S \subset U$.  Let $u_1, u_2, \ldots, u_n$ be vectors in $U$ and $c_1, c_2 ... c_n$ be scalars such that $0 = \sum_{i=1}^{n} {c_i}{u_i}$.  Because $u_i \in U$ for all $i$, there exist sets $A_i$ in $C$ such that $u_i \in A_i$.  Since $C$ is a chain, there is one set, say $A_k$, that contains all the others.  So $u_1, u_2,\ldots, u_n \in A_k$.  But since $A_k$ is linearly independent, $c_i = 0$ for all $i$.  Therefore, $U$ is linearly independent.  By the maximal principle, $U$ is the maximal element of $F$. $\square$ My questions are as follows: Is the author arguing that since each vector space has a basis, the infinite-dimensional vector space also has a basis?  This is similar to saying that $\lim_{n \rightarrow \infty} a_n = 0$ if $a_n = 0$ for all $n$. How come the author is checking one chain $C \in F$ only?  I thought that the maximal principle requires that the maximal element contains all members of each chains. I am still not sure of how $u_1, u_2, \ldots, u_n$ are picked out.  The greatest number of vectors in a linearly independent subset cannot exceed $\dim(V)$, but that is assuming that $V$ has a basis.  So, how does the author know what the finite number $n$ is? When the author is assigning $u_i$ to $A_i$, the set $\{A_i\}$ is not yet a chain.  But the $A_i$ can be rearranged to form a chain.  For example, $u_1 \in B_1$, $u_1, u_2 \in B_2,\ldots,$ and $u_1, u_2,\ldots, u_n \in B_n$.","My textbook extended the following proof to show that every vector space, including the infinite-dimensional case, has a basis. Condition: $S$ is a linearly independent subset of a vector space $V$. Theorem:  There is a maximal linearly independent subset of $V$ that contains $S$. Proof. Let $F$ be the family of all linearly independent subsets of $V$ that contains $S$.  If $C$ is a chain in $F$ and there exists a member $U \in F$ that contains each member of $C$, by the maximal principle, $U$ is the maximal element of $F$, the family of all linearly independent subsets of $V$.  As a result, $U$ is the maximal linearly independent subset of $V$.  So $U$ is the basis of $V$. Let $U$ be the union of the elements of $C$.  Clearly $U$ contains each element of $C$.  To show that $U$ is a linearly independent subset of $V$, first note that $S \subset U$.  Let $u_1, u_2, \ldots, u_n$ be vectors in $U$ and $c_1, c_2 ... c_n$ be scalars such that $0 = \sum_{i=1}^{n} {c_i}{u_i}$.  Because $u_i \in U$ for all $i$, there exist sets $A_i$ in $C$ such that $u_i \in A_i$.  Since $C$ is a chain, there is one set, say $A_k$, that contains all the others.  So $u_1, u_2,\ldots, u_n \in A_k$.  But since $A_k$ is linearly independent, $c_i = 0$ for all $i$.  Therefore, $U$ is linearly independent.  By the maximal principle, $U$ is the maximal element of $F$. $\square$ My questions are as follows: Is the author arguing that since each vector space has a basis, the infinite-dimensional vector space also has a basis?  This is similar to saying that $\lim_{n \rightarrow \infty} a_n = 0$ if $a_n = 0$ for all $n$. How come the author is checking one chain $C \in F$ only?  I thought that the maximal principle requires that the maximal element contains all members of each chains. I am still not sure of how $u_1, u_2, \ldots, u_n$ are picked out.  The greatest number of vectors in a linearly independent subset cannot exceed $\dim(V)$, but that is assuming that $V$ has a basis.  So, how does the author know what the finite number $n$ is? When the author is assigning $u_i$ to $A_i$, the set $\{A_i\}$ is not yet a chain.  But the $A_i$ can be rearranged to form a chain.  For example, $u_1 \in B_1$, $u_1, u_2 \in B_2,\ldots,$ and $u_1, u_2,\ldots, u_n \in B_n$.",,"['linear-algebra', 'vector-spaces', 'axiom-of-choice']"
51,Applying Graph Theory to Linear Algebra (not the other way around),Applying Graph Theory to Linear Algebra (not the other way around),,"I know about applications of Linear Algebra to Graph Theory, I find them boring. What interests me is whether one can draw graph-like pictures of linear functions to understand them better. Do you know of any results like that? I have one particular question I would like to know the answer to: Let $f : V \rightarrow V$ be a linear function and $b_1,...,b_n \in V$ a basis of $V$ . Also for every $v \in V$ define $v_1,...,v_n$ so that $v_1 b_1 + ... + v_n b_n = v$ . Finally let $G = (B,E)$ be the graph with $B = \{b_1,...,b_n\}$ and $E = \{ (b_i, b_j) \text{ with weight } f(b_i)_j \mid i,j \in \{1,...,n\} \}$ . In words: draw a circle for every basis element and connect them so that you can see how $f$ maps the basis elements to each other. Now delete all weights that are zero and assume the other weights are positive. Can we say something like: There is a cycle in $G$ if and only if $f$ has an eigenvector? To me that sounds like the Perron–Frobenius theorem . I'm also wondering if one could prove the existence of Jordan-Normal-Forms using graphs like this. (generalized eigenvectors are then maybe cycles connected by a tree) In general I feel like there should be a graph-theoretic perspective on the (basic) concepts I've seen in linear algebra. What do you think?","I know about applications of Linear Algebra to Graph Theory, I find them boring. What interests me is whether one can draw graph-like pictures of linear functions to understand them better. Do you know of any results like that? I have one particular question I would like to know the answer to: Let be a linear function and a basis of . Also for every define so that . Finally let be the graph with and . In words: draw a circle for every basis element and connect them so that you can see how maps the basis elements to each other. Now delete all weights that are zero and assume the other weights are positive. Can we say something like: There is a cycle in if and only if has an eigenvector? To me that sounds like the Perron–Frobenius theorem . I'm also wondering if one could prove the existence of Jordan-Normal-Forms using graphs like this. (generalized eigenvectors are then maybe cycles connected by a tree) In general I feel like there should be a graph-theoretic perspective on the (basic) concepts I've seen in linear algebra. What do you think?","f : V \rightarrow V b_1,...,b_n \in V V v \in V v_1,...,v_n v_1 b_1 + ... + v_n b_n = v G = (B,E) B = \{b_1,...,b_n\} E = \{ (b_i, b_j) \text{ with weight } f(b_i)_j \mid i,j \in \{1,...,n\} \} f G f","['linear-algebra', 'graph-theory']"
52,Geometric interpretations of matrix inverses,Geometric interpretations of matrix inverses,,"Let $A$ be an invertible $n \times n$ matrix.  Suppose we interpret each row of $A$ as a point in $\mathbb{R}^n$; then these $n$ points define a unique hyperplane in $\mathbb{R}^n$ that passes through each point (this hyperplane does not intersect the origin). Under this geometric interpretation, $A^{-1}$ has an interesting property: the normal vector to the hyperplane is given by the row sums of $A^{-1}$ (i.e. $A^{-1} \cdot 1$, where $1 = \langle 1, \dots, 1 \rangle^T$). Within this geometric interpretation of $A$, what other interesting properties does $A^{-1}$ have?  Do the individual entries of $A^{-1}$ have geometric meaning?  How about the column sums?","Let $A$ be an invertible $n \times n$ matrix.  Suppose we interpret each row of $A$ as a point in $\mathbb{R}^n$; then these $n$ points define a unique hyperplane in $\mathbb{R}^n$ that passes through each point (this hyperplane does not intersect the origin). Under this geometric interpretation, $A^{-1}$ has an interesting property: the normal vector to the hyperplane is given by the row sums of $A^{-1}$ (i.e. $A^{-1} \cdot 1$, where $1 = \langle 1, \dots, 1 \rangle^T$). Within this geometric interpretation of $A$, what other interesting properties does $A^{-1}$ have?  Do the individual entries of $A^{-1}$ have geometric meaning?  How about the column sums?",,"['linear-algebra', 'matrices', 'geometry', 'inverse', 'geometric-interpretation']"
53,How to understand the spectral decomposition geometrically?,How to understand the spectral decomposition geometrically?,,"Let $A$ be a $k\times k$ positive definite symmetric matrix. By spectral decomposition, we have $$A = \lambda_1e_1e_1'+ ... + \lambda_ke_ke_k'$$ and $$A^{-1} = \sum_{i=1}^k\frac{1}{\lambda_i}e_ie_i'$$ How to understand spectral decomposition and the relationship between $A$ and $A^{-1}$ geometrically?","Let be a positive definite symmetric matrix. By spectral decomposition, we have and How to understand spectral decomposition and the relationship between and geometrically?",A k\times k A = \lambda_1e_1e_1'+ ... + \lambda_ke_ke_k' A^{-1} = \sum_{i=1}^k\frac{1}{\lambda_i}e_ie_i' A A^{-1},"['linear-algebra', 'matrices', 'spectral-theory', 'matrix-decomposition']"
54,For every matrix $A\in M_{2}( \mathbb{C}) $ there's  $X\in M_{2}( \mathbb{C})$ such that $X^2=A$?,For every matrix  there's   such that ?,A\in M_{2}( \mathbb{C})  X\in M_{2}( \mathbb{C}) X^2=A,"True\False? For every matrix $A\in M_{2}( \mathbb{C}) $ there's  $X\in M_{2}( \mathbb{C})$ such that $X^2=A$. I know that every complexed matrix has a Jordan form matrix $J$ such that $P^{-1}CP=J$, But it's not diagonalizable for sure. Thanks","True\False? For every matrix $A\in M_{2}( \mathbb{C}) $ there's  $X\in M_{2}( \mathbb{C})$ such that $X^2=A$. I know that every complexed matrix has a Jordan form matrix $J$ such that $P^{-1}CP=J$, But it's not diagonalizable for sure. Thanks",,"['linear-algebra', 'matrices']"
55,A real function which is additive but not homogenous,A real function which is additive but not homogenous,,"From the theory of linear mappings, we know linear maps over a vector space  satisfy two properties: Additivity : $$f(v+w)=f(v)+f(w)$$ Homogeneity : $$f(\alpha v)=\alpha f(v)$$ which $\alpha\in \mathbb{F}$ is a scalar in the field which the vector space is defined on, and neither of these conditions implies the other one. If $f$ is defined over the complex numbers, $f:\mathbb{C}\longrightarrow \mathbb{C}$, then finding a mapping which is additive but not homogenous is simple; for example, $f(c)=c^*$. But can any one present an example on the reals, $f:\mathbb{R}\longrightarrow \mathbb{R}$, which is additive but not homogenous?","From the theory of linear mappings, we know linear maps over a vector space  satisfy two properties: Additivity : $$f(v+w)=f(v)+f(w)$$ Homogeneity : $$f(\alpha v)=\alpha f(v)$$ which $\alpha\in \mathbb{F}$ is a scalar in the field which the vector space is defined on, and neither of these conditions implies the other one. If $f$ is defined over the complex numbers, $f:\mathbb{C}\longrightarrow \mathbb{C}$, then finding a mapping which is additive but not homogenous is simple; for example, $f(c)=c^*$. But can any one present an example on the reals, $f:\mathbb{R}\longrightarrow \mathbb{R}$, which is additive but not homogenous?",,"['linear-algebra', 'functions', 'linear-transformations', 'functional-equations']"
56,Formula to project a vector onto a plane,Formula to project a vector onto a plane,,"I have a reference plane formed by $3$ points in $\mathbb{R}^3$ – $A, B$ and $C$ . I have a $4$ th point, $D$ . I would like to project the vector $\vec{BD}$ onto the reference plane as well as project vector $\vec{BD}$ onto the plane orthogonal to the reference plane at vector $\vec{AB}$ . Ultimately, I need the angle between $\vec{AB}$ and $\vec{BD}$ both when the vectors are projected on to the reference plane as well as the orthogonal plane. I have completed tutorials on projecting a vector onto a line in $\mathbb{R}^2$ but haven't figured out how to translate that to $\mathbb{R}^3$ ... Please note the diagram only shows the reference plane as parallel to the $xy$ plane for the sake of convenience. In my examples, the reference plane could be at any orientation. I am using $3$ D coordinate data from an electromagnetic motion tracker and the reference plane will be constantly moving. I understand the cross product of the two vectors $\vec{AB} \times \vec{BC}$ results in the normal vector to their plane. I have $2$ different methods to calculate that but am a little lost once I get to this point. I have seen both unit vector notation and column vector notation but am confused by moving between the different styles. It would be most helpful if you could tell me the formal name of the notation/equations you use. I know the scalar equation of a plane through point $(a,b,c)$ with normal $\hat{n} = [n_1, n_2, n_3]$ is: $$ n_1(x-a) + n_2(y-b) +n_3(z-c) = 0  $$ and the standard linear equation definition is: $$ Ax + By + Cz = D $$ but I could use some tips on when the equation is $=D$ and when it is $=0$ as well as any additional equations for a plane and in which circumstances the different forms are appropriate. I hope I've made sense here. Thanks for any help you can provide.","I have a reference plane formed by points in – and . I have a th point, . I would like to project the vector onto the reference plane as well as project vector onto the plane orthogonal to the reference plane at vector . Ultimately, I need the angle between and both when the vectors are projected on to the reference plane as well as the orthogonal plane. I have completed tutorials on projecting a vector onto a line in but haven't figured out how to translate that to ... Please note the diagram only shows the reference plane as parallel to the plane for the sake of convenience. In my examples, the reference plane could be at any orientation. I am using D coordinate data from an electromagnetic motion tracker and the reference plane will be constantly moving. I understand the cross product of the two vectors results in the normal vector to their plane. I have different methods to calculate that but am a little lost once I get to this point. I have seen both unit vector notation and column vector notation but am confused by moving between the different styles. It would be most helpful if you could tell me the formal name of the notation/equations you use. I know the scalar equation of a plane through point with normal is: and the standard linear equation definition is: but I could use some tips on when the equation is and when it is as well as any additional equations for a plane and in which circumstances the different forms are appropriate. I hope I've made sense here. Thanks for any help you can provide.","3 \mathbb{R}^3 A, B C 4 D \vec{BD} \vec{BD} \vec{AB} \vec{AB} \vec{BD} \mathbb{R}^2 \mathbb{R}^3 xy 3 \vec{AB} \times \vec{BC} 2 (a,b,c) \hat{n} = [n_1, n_2, n_3] 
n_1(x-a) + n_2(y-b) +n_3(z-c) = 0 
 
Ax + By + Cz = D
 =D =0","['linear-algebra', '3d', 'vectors']"
57,"What do physicists mean when they say something is ""not a vector""?","What do physicists mean when they say something is ""not a vector""?",,"It's common for physicists to say that not every 3-tuple of real numbers is a vector: “Well, isn’t torque just a vector?” It does turn out to be a vector, but we do not know that right away without making an analysis.... because force is a vector it transforms into the new system in the same way as do $x$ , $y$ , and $z$ , since a thing is a vector if and only if the various components transform in the same way as $x$ , $y$ , and $z$ . Richard Feynman You might be inclined to say that a vector is anything that has three components that combine properly under addition. Well, how about this: We have a barrel of fruit that contains $N_x$ pears, $N_y$ apples, and $N_z$ bananas. Is $N = N_x\hat{x} + N_y\hat{y} + N_z\hat{z}$ a vector? It has three components, and when you add another barrel with $M_x$ pears, $M_y$ apples, and $M_z$ bananas the result is $(N_x + M_x)$ pears, $(N_y + M_y)$ apples, $(N_z + M_z)$ bananas. So it does add like a vector. Yet it’s obviously not a vector, in the physicist’s sense of the word, because it doesn’t really have a direction. What exactly is wrong with it? David J. Griffiths, Introduction to Electricity and Magnetism, 1.15 Interestingly, Griffiths' example of apples and bananas not being a vector is almost identical to Strang's example of what a vector is: ""You can't add apples and oranges."" In a strange way, this is the reason for vectors. We have two separate numbers $V_I$ and $V_2$ . That pair produces a two-dimensional vector $v$ . Gilbert Strang, Introduction to Linear Algebra, 1.1 What do physicists mean then when they say a certain $(x,y,z)$ isn't a vector? Certainly any element of $\mathbb R^3$ is a vector. The sources talk about being the same under coordinate transforms: but of course any transform of $(x,y,z)$ yields a particular $(x', y', z')$ value.  What does it mean for a 3-tuple to be invariant under transform? Is there a way of expressing the physicists' statement clearly and precisely? How can I test if a certain tuple (or function) ""is"" a vector? Can this statement be put into mathematical language: $f: \mathbb R^3 \to \mathbb R^3$ is a physicists' vector if...","It's common for physicists to say that not every 3-tuple of real numbers is a vector: “Well, isn’t torque just a vector?” It does turn out to be a vector, but we do not know that right away without making an analysis.... because force is a vector it transforms into the new system in the same way as do , , and , since a thing is a vector if and only if the various components transform in the same way as , , and . Richard Feynman You might be inclined to say that a vector is anything that has three components that combine properly under addition. Well, how about this: We have a barrel of fruit that contains pears, apples, and bananas. Is a vector? It has three components, and when you add another barrel with pears, apples, and bananas the result is pears, apples, bananas. So it does add like a vector. Yet it’s obviously not a vector, in the physicist’s sense of the word, because it doesn’t really have a direction. What exactly is wrong with it? David J. Griffiths, Introduction to Electricity and Magnetism, 1.15 Interestingly, Griffiths' example of apples and bananas not being a vector is almost identical to Strang's example of what a vector is: ""You can't add apples and oranges."" In a strange way, this is the reason for vectors. We have two separate numbers and . That pair produces a two-dimensional vector . Gilbert Strang, Introduction to Linear Algebra, 1.1 What do physicists mean then when they say a certain isn't a vector? Certainly any element of is a vector. The sources talk about being the same under coordinate transforms: but of course any transform of yields a particular value.  What does it mean for a 3-tuple to be invariant under transform? Is there a way of expressing the physicists' statement clearly and precisely? How can I test if a certain tuple (or function) ""is"" a vector? Can this statement be put into mathematical language: is a physicists' vector if...","x y z x y z N_x N_y N_z N = N_x\hat{x} + N_y\hat{y} + N_z\hat{z} M_x M_y M_z (N_x + M_x) (N_y + M_y) (N_z + M_z) V_I V_2 v (x,y,z) \mathbb R^3 (x,y,z) (x', y', z') f: \mathbb R^3 \to \mathbb R^3","['linear-algebra', 'vectors', 'physics', 'applications']"
58,Eigenvalues of a rank-one update of a matrix,Eigenvalues of a rank-one update of a matrix,,"Let $\lambda_1,\dots,\lambda_n$ be the eigenvalues of the symmetric matrix $A \in {\Bbb R}^{n \times n}$ . Consider a rank-one perturbation $$B=A+\rho\,uu^T$$ where $u \in {\Bbb R}^n$ . Is there an analytical expression for the eigenvalues of $B$ , exploiting the known eigenvalues of $A$ ? I'm thinking something similar to the Sherman-Woodbury formula for the update of the inverse could work, but I have not succeeded. I found some related results: Bunch–Nielsen–Sorensen formula A stable and efficient algorithm for the rank-one modification of the symmetric eigenproblem (1994) But both seem to require that $u$ is an eigenvector of $A$ . Here, I am considering a more general statement, where $u$ need not be an eigenvector of $A$ .","Let be the eigenvalues of the symmetric matrix . Consider a rank-one perturbation where . Is there an analytical expression for the eigenvalues of , exploiting the known eigenvalues of ? I'm thinking something similar to the Sherman-Woodbury formula for the update of the inverse could work, but I have not succeeded. I found some related results: Bunch–Nielsen–Sorensen formula A stable and efficient algorithm for the rank-one modification of the symmetric eigenproblem (1994) But both seem to require that is an eigenvector of . Here, I am considering a more general statement, where need not be an eigenvector of .","\lambda_1,\dots,\lambda_n A \in {\Bbb R}^{n \times n} B=A+\rho\,uu^T u \in {\Bbb R}^n B A u A u A","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'spectral-theory']"
59,Coordinate-Free Definition of Trace.,Coordinate-Free Definition of Trace.,,"$\DeclareMathOperator{\tr}{trace}$ I am reading the Wikipedia article on the trace operator. The section titled Coordinate-Free Definition defines the trace as follows. Let $V$ be a finite dimensional vector space over a field $F$ and define a bilinear map $f:V\times V^*\to F$ as $f(v, \omega)=\omega(v)$ for all $(v, \omega)\in V\times V^*$ . This maps induces a unique linear map $\tr:V\otimes V^*\to F$ . Since $\text{End}(V)$ has a canonical isomorphism with $V\otimes V^*$ , we have now a notion of trace of a linear operator on $V$ . The Question: The second paragraph of the section in the article says that This also clarifies why $\tr(AB)=\tr(BA)$ . I can't see how $\tr(AB)=\tr(BA)$ follows from this definition at all. Can somebody give me a hint?","I am reading the Wikipedia article on the trace operator. The section titled Coordinate-Free Definition defines the trace as follows. Let be a finite dimensional vector space over a field and define a bilinear map as for all . This maps induces a unique linear map . Since has a canonical isomorphism with , we have now a notion of trace of a linear operator on . The Question: The second paragraph of the section in the article says that This also clarifies why . I can't see how follows from this definition at all. Can somebody give me a hint?","\DeclareMathOperator{\tr}{trace} V F f:V\times V^*\to F f(v, \omega)=\omega(v) (v, \omega)\in V\times V^* \tr:V\otimes V^*\to F \text{End}(V) V\otimes V^* V \tr(AB)=\tr(BA) \tr(AB)=\tr(BA)","['linear-algebra', 'tensors', 'trace']"
60,What is the importance of definite and semidefinite matrices?,What is the importance of definite and semidefinite matrices?,,I would like to know some of the most important definitions and theorems of definite and semidefinite matrices and their importance in linear algebra. Thank you for your help.,I would like to know some of the most important definitions and theorems of definite and semidefinite matrices and their importance in linear algebra. Thank you for your help.,,"['linear-algebra', 'matrices', 'symmetric-matrices', 'positive-definite', 'positive-semidefinite']"
61,Eigenvalues for $3\times 3$ stochastic matrices,Eigenvalues for  stochastic matrices,3\times 3,"This is a plot of the non-real eigenvalues of $10^4$ randomly  generated $3\times3$ stochastic matrices. It's pretty clear that they lie in the convex hull of the three cube roots of unity. The boundary on the left hand side is easy to explain. If the  stochastic matrix $P$ has a non-real eigenvalue $\lambda$, then  $\text{trace}(P)=\lambda+\bar\lambda+1=2\text{Re}(\lambda)+1$. On the other hand, the trace of $P$ is  also the sum of its diagonal entries, and hence  is non-negative real number. Therefore, $\text{Re}(\lambda)\geq -{1\over 2}$. I hope that there is an easy explanation for the other 2 sides of  the triangle, perhaps in terms of some other matrix invariant.  So far, I can't think of any. Any ideas? By the way, it is not hard to show that every point in  the triangle can be achieved as the eigenvalue of  a stochastic matrix of the form $$P=\begin{bmatrix}1-s-t&s&t\\ t&1-s-t&s\\ s&t&1-s-t \end{bmatrix}$$ for some $s\geq 0, t\geq 0, s+t\leq 1$.","This is a plot of the non-real eigenvalues of $10^4$ randomly  generated $3\times3$ stochastic matrices. It's pretty clear that they lie in the convex hull of the three cube roots of unity. The boundary on the left hand side is easy to explain. If the  stochastic matrix $P$ has a non-real eigenvalue $\lambda$, then  $\text{trace}(P)=\lambda+\bar\lambda+1=2\text{Re}(\lambda)+1$. On the other hand, the trace of $P$ is  also the sum of its diagonal entries, and hence  is non-negative real number. Therefore, $\text{Re}(\lambda)\geq -{1\over 2}$. I hope that there is an easy explanation for the other 2 sides of  the triangle, perhaps in terms of some other matrix invariant.  So far, I can't think of any. Any ideas? By the way, it is not hard to show that every point in  the triangle can be achieved as the eigenvalue of  a stochastic matrix of the form $$P=\begin{bmatrix}1-s-t&s&t\\ t&1-s-t&s\\ s&t&1-s-t \end{bmatrix}$$ for some $s\geq 0, t\geq 0, s+t\leq 1$.",,"['linear-algebra', 'matrices']"
62,Determinant of a generalized Pascal matrix,Determinant of a generalized Pascal matrix,,"Let $M$ denote the infinite matrix defined recursively by $$ M_{ij} =  \begin{cases}   1, &  \text{if } i=1 \text{ and } j=1; \\   aM_{i-1,j}+bM_{i,j-1}+cM_{i-1,j-1}, & \mbox{otherwise}.\\ \end{cases} $$ ($M_{i,0}$ and $M_{0,j}$ are both defined to be $0$.) ( Added : I just discovered that the numbers in the $M$ matrix are called weighted Delannoy numbers .) Let $M_n$ denote the $n \times n$ upper-left submatrix of $M$. For example, with $a = b = c = 1$, $M_1 = \begin{bmatrix} 1 \end{bmatrix}$, $M_2= \begin{bmatrix} 1 & 1 \\ 1 & 3 \end{bmatrix}$, and $M_3 = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 3 & 5 \\ 1 & 5 & 13 \end{bmatrix}$, and $M_4 = \begin{bmatrix}   		1 & 1 & 1 & 1 \\   		1 & 3 & 5 & 7 \\   		1 & 5 & 13 & 25 \\   		1 & 7 & 25 & 63   	\end{bmatrix}.$ A few years ago one of my students proved, by induction, that $$\det M_n = (ab+c)^{n(n-1)/2}.$$ My question is Is there a noninductive proof that $\det M_n = (ab+c)^{n(n-1)/2}$ that gives more insight into why the determinant works out so nicely? For example, when $a = b = 1$, $c = 0$, $M$ is the symmetric Pascal matrix .  I've seen more than one way to prove that $\det M_n = 1$ in this case.  For example, Edelman and Strang give four proofs of an LU-decomposition that does it.  I also once saw, at a conference, a combinatorial proof using the interpretation of the determinant in terms of nonintersecting paths in a directed graph.  (I think the talk was given by Art Benjamin, but it was several years ago, and I may be misremembering.)  So I know that there are some nice proofs in the special case of the Pascal matrix.  But what about the general case?","Let $M$ denote the infinite matrix defined recursively by $$ M_{ij} =  \begin{cases}   1, &  \text{if } i=1 \text{ and } j=1; \\   aM_{i-1,j}+bM_{i,j-1}+cM_{i-1,j-1}, & \mbox{otherwise}.\\ \end{cases} $$ ($M_{i,0}$ and $M_{0,j}$ are both defined to be $0$.) ( Added : I just discovered that the numbers in the $M$ matrix are called weighted Delannoy numbers .) Let $M_n$ denote the $n \times n$ upper-left submatrix of $M$. For example, with $a = b = c = 1$, $M_1 = \begin{bmatrix} 1 \end{bmatrix}$, $M_2= \begin{bmatrix} 1 & 1 \\ 1 & 3 \end{bmatrix}$, and $M_3 = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 3 & 5 \\ 1 & 5 & 13 \end{bmatrix}$, and $M_4 = \begin{bmatrix}   		1 & 1 & 1 & 1 \\   		1 & 3 & 5 & 7 \\   		1 & 5 & 13 & 25 \\   		1 & 7 & 25 & 63   	\end{bmatrix}.$ A few years ago one of my students proved, by induction, that $$\det M_n = (ab+c)^{n(n-1)/2}.$$ My question is Is there a noninductive proof that $\det M_n = (ab+c)^{n(n-1)/2}$ that gives more insight into why the determinant works out so nicely? For example, when $a = b = 1$, $c = 0$, $M$ is the symmetric Pascal matrix .  I've seen more than one way to prove that $\det M_n = 1$ in this case.  For example, Edelman and Strang give four proofs of an LU-decomposition that does it.  I also once saw, at a conference, a combinatorial proof using the interpretation of the determinant in terms of nonintersecting paths in a directed graph.  (I think the talk was given by Art Benjamin, but it was several years ago, and I may be misremembering.)  So I know that there are some nice proofs in the special case of the Pascal matrix.  But what about the general case?",,"['linear-algebra', 'combinatorics', 'matrices', 'determinant']"
63,"best strategies for 'Squid Game' episode 6 game 4 ""marble game"" players","best strategies for 'Squid Game' episode 6 game 4 ""marble game"" players",,"Two players each get $n=10$ marbles. Every alternating turn, one player (say, the player whose turn it is) hides an amount of own marbles in fist, and, the other player must guess if hidden amount is odd or even, and, that other player (i.e., the player whose turn it is not) also must bet an amount of own marbles. If guess is right, the player whose turn it is gives (as much as possible) the amount of bet marbles to opponent. If guess is wrong, the player whose turn it is takes the amount of bet marbles from opponent. Next, the turn now alternates to the other player. The game stops when one player has all $2n=20$ marbles. The losing player gets killed (in the series, that is). Which strategies for both players (perhaps one strategy for the one who gets first turn, and one strategy for the other player) give maximal probabilities to win (and to not get killed). We must assume both players know the starting conditions and are perfect mathematicians and logicians. On a side note: is this an old or new game? If it is a known old game, can anyone tell where its 'official' rules (and, maybe, solution(s)) are documented? remark (series details coming so warning : spoiler ahead) There is a YT video where rules are explained to be: if guesser guesses wrong, guesser must give amount 'h' that was hidden by hider, not amount 'b' bet by guesser. https://www.youtube.com/watch?v=GX4AkD_vdhw The video also gives the simple solution for that variant. And presenter also mentions rules are not clear, but, most people, he says, believe 'h'. Some comments claim otherwise. Fair enough. From examples in series it is not entirely clear to me either what the rule is. There is only one example where guesser guesses wrong. In that example, guesser bets b=2, and hider hides h=3, and guesser guesses odd. One can see the guesser give 2 (amount b bet), and, next it is shown guesser has only one more left. The guesser stops playing. If the rule were to give 3 (amount h hidden), guesser would immediately loose, and if the guards were paying attention, guesser would have been killed. If the rule were to give 2 (amount b bet) then guesser, upon becoming hider, would also loose. But, current guesser is a cheater so, in both cases (having to give b=2 or h=3), it fits him to hold that last marble. Note that the cheater is portrayed to be clever and his opponent to be dumb. And the opponent did not know the game. However, if rule were to have to give h(=3), then guesser would cheat hard in not holding on to the rules, and, be lucky to still be alive. After all, the outcome of each games in the series is said to be fatal, but fair. But, if rule were to have to give b(=2), then guesser would still cheat hard by stopping the game, and, even manipulating opponents marbles. A moderately clever guard paying attention would notice game was not played fair, and, a somewhat more clever guard paying attention would even know guesser lost in any case.","Two players each get marbles. Every alternating turn, one player (say, the player whose turn it is) hides an amount of own marbles in fist, and, the other player must guess if hidden amount is odd or even, and, that other player (i.e., the player whose turn it is not) also must bet an amount of own marbles. If guess is right, the player whose turn it is gives (as much as possible) the amount of bet marbles to opponent. If guess is wrong, the player whose turn it is takes the amount of bet marbles from opponent. Next, the turn now alternates to the other player. The game stops when one player has all marbles. The losing player gets killed (in the series, that is). Which strategies for both players (perhaps one strategy for the one who gets first turn, and one strategy for the other player) give maximal probabilities to win (and to not get killed). We must assume both players know the starting conditions and are perfect mathematicians and logicians. On a side note: is this an old or new game? If it is a known old game, can anyone tell where its 'official' rules (and, maybe, solution(s)) are documented? remark (series details coming so warning : spoiler ahead) There is a YT video where rules are explained to be: if guesser guesses wrong, guesser must give amount 'h' that was hidden by hider, not amount 'b' bet by guesser. https://www.youtube.com/watch?v=GX4AkD_vdhw The video also gives the simple solution for that variant. And presenter also mentions rules are not clear, but, most people, he says, believe 'h'. Some comments claim otherwise. Fair enough. From examples in series it is not entirely clear to me either what the rule is. There is only one example where guesser guesses wrong. In that example, guesser bets b=2, and hider hides h=3, and guesser guesses odd. One can see the guesser give 2 (amount b bet), and, next it is shown guesser has only one more left. The guesser stops playing. If the rule were to give 3 (amount h hidden), guesser would immediately loose, and if the guards were paying attention, guesser would have been killed. If the rule were to give 2 (amount b bet) then guesser, upon becoming hider, would also loose. But, current guesser is a cheater so, in both cases (having to give b=2 or h=3), it fits him to hold that last marble. Note that the cheater is portrayed to be clever and his opponent to be dumb. And the opponent did not know the game. However, if rule were to have to give h(=3), then guesser would cheat hard in not holding on to the rules, and, be lucky to still be alive. After all, the outcome of each games in the series is said to be fatal, but fair. But, if rule were to have to give b(=2), then guesser would still cheat hard by stopping the game, and, even manipulating opponents marbles. A moderately clever guard paying attention would notice game was not played fair, and, a somewhat more clever guard paying attention would even know guesser lost in any case.",n=10 2n=20,"['linear-algebra', 'probability', 'probability-theory', 'game-theory', 'nash-equilibrium']"
64,How to visualize a rank-2 tensor?,How to visualize a rank-2 tensor?,,"The notion (rank-2) ""tensor"" appears in many different parts of physics, e.g. stress tensor, moment of inertia tensor, etc. I know mathematically a tensor can be represented by a $3 \times 3$ matrix. But I can't grasp its geometrical picture — unlike scalar (a number) and vector (an arrow with direction and magnitude) which I can easily see what's going on. How to visualize a tensor?","The notion (rank-2) ""tensor"" appears in many different parts of physics, e.g. stress tensor, moment of inertia tensor, etc. I know mathematically a tensor can be represented by a $3 \times 3$ matrix. But I can't grasp its geometrical picture — unlike scalar (a number) and vector (an arrow with direction and magnitude) which I can easily see what's going on. How to visualize a tensor?",,"['geometry', 'linear-algebra', 'intuition', 'tensors']"
65,What is the difference between a tensor product and an outer product?,What is the difference between a tensor product and an outer product?,,I have seen the tensor product written as $$  \left( \begin{array}{c} a \\ b \\  \end{array} \right) \otimes  \left( \begin{array}{c} c \\ d \\  \end{array} \right) = \left( \begin{array}{c} ac \\ ad \\ bc \\ bd \\  \end{array} \right)$$ However I have also seen it written as  $$  \left( \begin{array}{c} a \\ b \\  \end{array} \right) \otimes  \left( \begin{array}{c} c \\ d \\  \end{array} \right) =   \left( \begin{array}{c} a \\ b \\  \end{array} \right)   \left( \begin{array}{c} c & d \\ \end{array} \right) =    \left( \begin{array}{c} ac & ad\\ bc & bd \\\end{array} \right) $$ Which I have seen in the context of outer products.  Why are there two ways to do a tensor product?,I have seen the tensor product written as $$  \left( \begin{array}{c} a \\ b \\  \end{array} \right) \otimes  \left( \begin{array}{c} c \\ d \\  \end{array} \right) = \left( \begin{array}{c} ac \\ ad \\ bc \\ bd \\  \end{array} \right)$$ However I have also seen it written as  $$  \left( \begin{array}{c} a \\ b \\  \end{array} \right) \otimes  \left( \begin{array}{c} c \\ d \\  \end{array} \right) =   \left( \begin{array}{c} a \\ b \\  \end{array} \right)   \left( \begin{array}{c} c & d \\ \end{array} \right) =    \left( \begin{array}{c} ac & ad\\ bc & bd \\\end{array} \right) $$ Which I have seen in the context of outer products.  Why are there two ways to do a tensor product?,,"['linear-algebra', 'tensor-products']"
66,Prove that vector space and dual space have same dimension,Prove that vector space and dual space have same dimension,,"As an exercise in my textbook, I need to prove that if $V$ is a finite dimensional vector space with dual space $V^*$ over $\mathbb{R}$, then dim$(V)$=dim$(V^*)$. Let $\omega\in V^*$ and let $\{e_1,...,e_n\}$ be a basis for $V$. Define $e^i\in V^*$ by $e^i(e_j)=\delta_{ij}$. We show that $\{e^1,...,e^n\}$ spans $V^*$. $\omega(v)=\omega(v_1e_1+...+v_ne_n)=v_1\omega(e_1)+...+v_n\omega(e_n).$ If $\omega(e_1)=\lambda_1,...,\omega(e_n)=\lambda_n$, then $\omega(v)=v_1\lambda_1e^1(e_1)+...+v_n\lambda_ne^n(e_n)$=$\lambda_1e^1(v)+...+\lambda_ne^n(v)$. To show $\{e^1,...,e^n\}$ is linearly independent, suppose that $0=c_1e^1+...+c_ne^n$ is the zero mapping to $\mathbb{R}$. Consider the image of $e_1:$  $0(e_1)=c_1*1+...+c_n*0=c_1$ Hence, $c_1=0$. Repeating the procedure for $e_j$, $2\leq j\leq n$, we see that $c_1=c_2=...=c_n=0$. Does this proof look correct? If the proof is correct, I have an additional small question. It seems like this proof is dependent on the fact that $V$ is a real (or complex) vector space, since we define the covectors to have the image set $\{0,1\}$. Is there a proof that works for vectors spaces over general fields?","As an exercise in my textbook, I need to prove that if $V$ is a finite dimensional vector space with dual space $V^*$ over $\mathbb{R}$, then dim$(V)$=dim$(V^*)$. Let $\omega\in V^*$ and let $\{e_1,...,e_n\}$ be a basis for $V$. Define $e^i\in V^*$ by $e^i(e_j)=\delta_{ij}$. We show that $\{e^1,...,e^n\}$ spans $V^*$. $\omega(v)=\omega(v_1e_1+...+v_ne_n)=v_1\omega(e_1)+...+v_n\omega(e_n).$ If $\omega(e_1)=\lambda_1,...,\omega(e_n)=\lambda_n$, then $\omega(v)=v_1\lambda_1e^1(e_1)+...+v_n\lambda_ne^n(e_n)$=$\lambda_1e^1(v)+...+\lambda_ne^n(v)$. To show $\{e^1,...,e^n\}$ is linearly independent, suppose that $0=c_1e^1+...+c_ne^n$ is the zero mapping to $\mathbb{R}$. Consider the image of $e_1:$  $0(e_1)=c_1*1+...+c_n*0=c_1$ Hence, $c_1=0$. Repeating the procedure for $e_j$, $2\leq j\leq n$, we see that $c_1=c_2=...=c_n=0$. Does this proof look correct? If the proof is correct, I have an additional small question. It seems like this proof is dependent on the fact that $V$ is a real (or complex) vector space, since we define the covectors to have the image set $\{0,1\}$. Is there a proof that works for vectors spaces over general fields?",,['linear-algebra']
67,"Matrix algebra: The ""magical inverse"" trick","Matrix algebra: The ""magical inverse"" trick",,"In relation to my Master' Thesis, I made an observation that has bothered me for some time now, and I come here hoping that some of you guys can shed some light on it. In my project I work with a complex block matrix on the form $$\begin{equation} B = \begin{bmatrix} u & v \\ w & x \end{bmatrix} \end{equation},$$ where the four submatrices $u$, $v$, $w$ and $x$ are complex square matrices of the same size (and thus $B$ itself is also a square matrix). These submatrices fulfill the following three relations: $$\begin{align} & uu^\dagger - vv^\dagger = I \tag{1}\\ & xx^\dagger-ww^\dagger = I \tag{2}\\ & uw^\dagger = vx^\dagger \tag{3} \end{align}$$ Here, I use dagger to represent the conjugate transpose. Now using just the three equations above, we can construct the following block matrix equation: $$\begin{equation} \begin{bmatrix} u & v\\ w & x \end{bmatrix} \begin{bmatrix} u^\dagger & -w^\dagger\\ -v^\dagger & x^\dagger \end{bmatrix} = \begin{bmatrix} I & 0\\ 0 & I \end{bmatrix} \tag{a} \end{equation}$$ The only thing here slightly non-trivial is the lower left equation, which reads $$wu^\dagger = xv^\dagger \tag{4},$$ but this is quickly seen to be the conjugate transpose of equation (3) above. Equation (a) implies that our $B$ has an inverse, namely $$B^{-1} = \begin{bmatrix} u^\dagger & -w^\dagger\\ -v^\dagger & x^\dagger \end{bmatrix}.$$ And because square matrices have equal right and left inverses, we then get the following additional block matrix equation: $$\begin{equation} \begin{bmatrix} u^\dagger & -w^\dagger\\ -v^\dagger & x^\dagger \end{bmatrix} \begin{bmatrix} u & v\\ w & x \end{bmatrix} = \begin{bmatrix} I & 0\\ 0 & I \end{bmatrix} \tag{b} \end{equation}$$ Deconstructing this, we suddenly end up with four new equations: $$\begin{align} & u^\dagger u - w^\dagger w = I \tag{5}\\ & x^\dagger x - v^\dagger v = I \tag{6}\\ & u^\dagger v = w^\dagger x \tag{7}\\ & v^\dagger u = x^\dagger w \tag{8} \end{align}$$ The equations (5) to (8) followed directly from the equations (1) to (4), through the trick with the inverse of $B$. So far, so good. My problem here is that I can't find any way through normal operations to get the equations (5) to (8) from the equations (1) to (4)! It seems very odd to me (and to people I've spoken to) that we need to introduce these large block matrices $B$ and $B^{-1}$ to transfer between the two equivalent sets of matrix equations. Shouldn't I also be able to do this just by performing ""normal"" algebraic operations such as addition, subtraction and matrix multiplication, as well as maybe complex conjugation? Or should this ""magical inverse trick"" be considered a natural part of your toolbox when handling matrix equations? I understand that matrix algebra behaves differently than normal scalar algebra, so it might very well be the case – I have just never heard of anything like it before. If anyone could give me some insight on this, it would save the day for both me and many of my fellow students! Thanks. EDIT: Thanks for your replies! It is now clear to me that the inverse trick $AB = I \Longleftrightarrow BA = I$ is a natural part of the matrix algebra toolbox, because as Omnomnomnom points out under, there is no way of proving this equivalence through normal matrix algebraic means. Still, to me, there is something else going on here as well. In order to get from equations (1)–(4) to equations (5)–(8) we actually construct the larger block matrices $B$ and $B^{-1}$ and do the inverse trick on them – so in some sense, we use the inverse trick ""in a higher dimension"" than the dimension of the original equations. I find it very strange that the equations cannot be rewritten just through matrix equations in the same dimension. Or do you think there is a way of getting from equations (1)–(4) to equations (5)–(8) without constructing $B$ and $B^{-1}$, but just by somehow using the inverse trick on the original equations?","In relation to my Master' Thesis, I made an observation that has bothered me for some time now, and I come here hoping that some of you guys can shed some light on it. In my project I work with a complex block matrix on the form $$\begin{equation} B = \begin{bmatrix} u & v \\ w & x \end{bmatrix} \end{equation},$$ where the four submatrices $u$, $v$, $w$ and $x$ are complex square matrices of the same size (and thus $B$ itself is also a square matrix). These submatrices fulfill the following three relations: $$\begin{align} & uu^\dagger - vv^\dagger = I \tag{1}\\ & xx^\dagger-ww^\dagger = I \tag{2}\\ & uw^\dagger = vx^\dagger \tag{3} \end{align}$$ Here, I use dagger to represent the conjugate transpose. Now using just the three equations above, we can construct the following block matrix equation: $$\begin{equation} \begin{bmatrix} u & v\\ w & x \end{bmatrix} \begin{bmatrix} u^\dagger & -w^\dagger\\ -v^\dagger & x^\dagger \end{bmatrix} = \begin{bmatrix} I & 0\\ 0 & I \end{bmatrix} \tag{a} \end{equation}$$ The only thing here slightly non-trivial is the lower left equation, which reads $$wu^\dagger = xv^\dagger \tag{4},$$ but this is quickly seen to be the conjugate transpose of equation (3) above. Equation (a) implies that our $B$ has an inverse, namely $$B^{-1} = \begin{bmatrix} u^\dagger & -w^\dagger\\ -v^\dagger & x^\dagger \end{bmatrix}.$$ And because square matrices have equal right and left inverses, we then get the following additional block matrix equation: $$\begin{equation} \begin{bmatrix} u^\dagger & -w^\dagger\\ -v^\dagger & x^\dagger \end{bmatrix} \begin{bmatrix} u & v\\ w & x \end{bmatrix} = \begin{bmatrix} I & 0\\ 0 & I \end{bmatrix} \tag{b} \end{equation}$$ Deconstructing this, we suddenly end up with four new equations: $$\begin{align} & u^\dagger u - w^\dagger w = I \tag{5}\\ & x^\dagger x - v^\dagger v = I \tag{6}\\ & u^\dagger v = w^\dagger x \tag{7}\\ & v^\dagger u = x^\dagger w \tag{8} \end{align}$$ The equations (5) to (8) followed directly from the equations (1) to (4), through the trick with the inverse of $B$. So far, so good. My problem here is that I can't find any way through normal operations to get the equations (5) to (8) from the equations (1) to (4)! It seems very odd to me (and to people I've spoken to) that we need to introduce these large block matrices $B$ and $B^{-1}$ to transfer between the two equivalent sets of matrix equations. Shouldn't I also be able to do this just by performing ""normal"" algebraic operations such as addition, subtraction and matrix multiplication, as well as maybe complex conjugation? Or should this ""magical inverse trick"" be considered a natural part of your toolbox when handling matrix equations? I understand that matrix algebra behaves differently than normal scalar algebra, so it might very well be the case – I have just never heard of anything like it before. If anyone could give me some insight on this, it would save the day for both me and many of my fellow students! Thanks. EDIT: Thanks for your replies! It is now clear to me that the inverse trick $AB = I \Longleftrightarrow BA = I$ is a natural part of the matrix algebra toolbox, because as Omnomnomnom points out under, there is no way of proving this equivalence through normal matrix algebraic means. Still, to me, there is something else going on here as well. In order to get from equations (1)–(4) to equations (5)–(8) we actually construct the larger block matrices $B$ and $B^{-1}$ and do the inverse trick on them – so in some sense, we use the inverse trick ""in a higher dimension"" than the dimension of the original equations. I find it very strange that the equations cannot be rewritten just through matrix equations in the same dimension. Or do you think there is a way of getting from equations (1)–(4) to equations (5)–(8) without constructing $B$ and $B^{-1}$, but just by somehow using the inverse trick on the original equations?",,"['linear-algebra', 'matrices', 'inverse', 'matrix-equations']"
68,What do trivial and non-trivial solution of homogeneous equations mean in matrices? [closed],What do trivial and non-trivial solution of homogeneous equations mean in matrices? [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question Suppose I have system of 3 equations $$a_1x+b_1y+c_1z=0$$ $$a_2x+b_2y+c_2z=0$$ $$a_3x+b_3y+c_3z=0$$ and cofficient matrix $A=\begin{equation} \begin{pmatrix} a_1 & b_1 & c_1 \\ a_2 & b_2 & c_2 \\ a_3 & b_3 & c_3 \end{pmatrix} \end{equation}$ So I have been told that solution of this matrix will be non-trivial if $|A|=0$ and trivial in any other case. As far as I know non trivial solution means solutions is not equal to zero but in any case $x,y,z=0$ will satisfy given equations regardless of it's value of determinant. So, why do we call it ""non-trivial"" solution?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question Suppose I have system of 3 equations $$a_1x+b_1y+c_1z=0$$ $$a_2x+b_2y+c_2z=0$$ $$a_3x+b_3y+c_3z=0$$ and cofficient matrix $A=\begin{equation} \begin{pmatrix} a_1 & b_1 & c_1 \\ a_2 & b_2 & c_2 \\ a_3 & b_3 & c_3 \end{pmatrix} \end{equation}$ So I have been told that solution of this matrix will be non-trivial if $|A|=0$ and trivial in any other case. As far as I know non trivial solution means solutions is not equal to zero but in any case $x,y,z=0$ will satisfy given equations regardless of it's value of determinant. So, why do we call it ""non-trivial"" solution?",,"['linear-algebra', 'matrices', 'systems-of-equations']"
69,Proving an implication of two dimensional matrix.,Proving an implication of two dimensional matrix.,,"If $A = \begin{bmatrix} x & 1\\ y & 0\end{bmatrix}, B = \begin{bmatrix} z & 1\\ w & 0\end{bmatrix}$ , for $x,y,z,w \in \Bbb{R}$ . I have observed by considering many  examples of $x,y,z,w$ that: If all the eigen values of $A^2B$ and $AB^2$ are less than one in absolute value $\implies$ $\det(AB+A+I)<0$ and $\det(BA+B+I)<0$ is not possible. Any way how to prove it actually? I am thinking if $\det(AB+A+I)<0$ and $\det(BA+B+I)<0$ ,  then perhaps it would violate certain assumptions on the eigenvalues of $A^2B, AB^2$ ? Explicit forms of matrices: $A^2B = \begin{bmatrix} z(x^2+y)+xw & x^2+y\\ xyz+wy & xy\end{bmatrix}$ $AB^2 = \begin{bmatrix} x(w+z^2)+wz & xz+w\\ y(z^2+w) & yz\end{bmatrix}$ $AB +A+I = \begin{bmatrix}  xz+w+x+1& x+1\\ yz+y & y+1\end{bmatrix}$ $BA+B+I = \begin{bmatrix} xz+y+z+1 & z+1\\ xw+w & w+1\end{bmatrix}$","If , for . I have observed by considering many  examples of that: If all the eigen values of and are less than one in absolute value and is not possible. Any way how to prove it actually? I am thinking if and ,  then perhaps it would violate certain assumptions on the eigenvalues of ? Explicit forms of matrices:","A = \begin{bmatrix} x & 1\\ y & 0\end{bmatrix}, B = \begin{bmatrix} z & 1\\ w & 0\end{bmatrix} x,y,z,w \in \Bbb{R} x,y,z,w A^2B AB^2 \implies \det(AB+A+I)<0 \det(BA+B+I)<0 \det(AB+A+I)<0 \det(BA+B+I)<0 A^2B, AB^2 A^2B = \begin{bmatrix} z(x^2+y)+xw & x^2+y\\ xyz+wy & xy\end{bmatrix} AB^2 = \begin{bmatrix} x(w+z^2)+wz & xz+w\\ y(z^2+w) & yz\end{bmatrix} AB +A+I = \begin{bmatrix}  xz+w+x+1& x+1\\ yz+y & y+1\end{bmatrix} BA+B+I = \begin{bmatrix} xz+y+z+1 & z+1\\ xw+w & w+1\end{bmatrix}","['linear-algebra', 'matrices', 'optimization', 'determinant']"
70,"$\sum_{k=0}^{100} a_k x^{k}=0, a_i \in \mathbb{Z}$ How many equations?",How many equations?,"\sum_{k=0}^{100} a_k x^{k}=0, a_i \in \mathbb{Z}","Let $x^{100}+a_{99} x^{99}+a_{98} x^{98}+ \dots +a_{1}x +a_{0}=0, \ \ a_{100}=1, a_i \in \mathbb{Z}$, is an algebraic equation with integer coefficients. Assume that all (100 with multiplicity) roots are positive real numbers: $0 < x_1\leq x_2\leq x_3 \leq x_4 \leq \dots \leq x_{99} \leq x_{100} < m, \ \ m \in \mathbb{N} $. The question is: How many such different equations are there for the interval (0, m)? Update. Maybe I found solution for the case when $x^2+a_1 x+a_2=0, a_1,a_2 \in \mathbb{Z}$. If $ 0\leq x_1\leq x_2<m$, from $(x-x_1)(x-x_2) = x^2+a_1 x+a_2$ we have a system $x_1 x_2 =a_2; x_1+x_2=-a_1,$  $2m<a_1<0, \ \ 0<a_2<m^2$. So we have $\binom{m+2}{3}=\frac{m(m+1)(m+2)}{6}$ different equations. Update: Complete answer in case m=3 will be also helpfull. And will be awarded.","Let $x^{100}+a_{99} x^{99}+a_{98} x^{98}+ \dots +a_{1}x +a_{0}=0, \ \ a_{100}=1, a_i \in \mathbb{Z}$, is an algebraic equation with integer coefficients. Assume that all (100 with multiplicity) roots are positive real numbers: $0 < x_1\leq x_2\leq x_3 \leq x_4 \leq \dots \leq x_{99} \leq x_{100} < m, \ \ m \in \mathbb{N} $. The question is: How many such different equations are there for the interval (0, m)? Update. Maybe I found solution for the case when $x^2+a_1 x+a_2=0, a_1,a_2 \in \mathbb{Z}$. If $ 0\leq x_1\leq x_2<m$, from $(x-x_1)(x-x_2) = x^2+a_1 x+a_2$ we have a system $x_1 x_2 =a_2; x_1+x_2=-a_1,$  $2m<a_1<0, \ \ 0<a_2<m^2$. So we have $\binom{m+2}{3}=\frac{m(m+1)(m+2)}{6}$ different equations. Update: Complete answer in case m=3 will be also helpfull. And will be awarded.",,[]
71,Show that B is a nonsingular matrix (not that obvious).,Show that B is a nonsingular matrix (not that obvious).,,"How would you proceed if you were asked in an interview to show that B is a nonsingular matrix (in an elegant way)? $$B= \begin{pmatrix} 1& 1.25& −0.50& 0.15\\ 0.15& 2& 1.25& −1.50\\ −0.45& 0.25& 3& 1.25\\ 0.25& −0.15& 0.25& 4\\ \end{pmatrix}$$ In my opinion, taking the time to compute the determinant of this $4\times4$ matrix during the interview would not be appreciated by the interviewer.","How would you proceed if you were asked in an interview to show that B is a nonsingular matrix (in an elegant way)? $$B= \begin{pmatrix} 1& 1.25& −0.50& 0.15\\ 0.15& 2& 1.25& −1.50\\ −0.45& 0.25& 3& 1.25\\ 0.25& −0.15& 0.25& 4\\ \end{pmatrix}$$ In my opinion, taking the time to compute the determinant of this $4\times4$ matrix during the interview would not be appreciated by the interviewer.",,"['linear-algebra', 'matrices', 'matrix-rank']"
72,To Find Eigenvalues,To Find Eigenvalues,,"Find the eigenvalues of the $6\times 6$ matrix $$\left[\begin{matrix}  0 & 0 & 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 & 0 & 1 \\ 1 & 0 & 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 & 0 & 0 \\ \end{matrix}\right]$$ The options  are $1, -1, i, -i$ It is a real symmetric matrix and the eigenvalues of a real symmetric matrix are real. Hence $i$ and $-i$ can't be its  eigenvalues. Then what else we can say? Is there any easy way to find it?","Find the eigenvalues of the $6\times 6$ matrix $$\left[\begin{matrix}  0 & 0 & 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 0 & 0 & 1 \\ 1 & 0 & 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 & 0 & 0 \\ \end{matrix}\right]$$ The options  are $1, -1, i, -i$ It is a real symmetric matrix and the eigenvalues of a real symmetric matrix are real. Hence $i$ and $-i$ can't be its  eigenvalues. Then what else we can say? Is there any easy way to find it?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
73,What is the difference between codomain and range?,What is the difference between codomain and range?,,"My book says that if there is a linear transformation $T: V \to V'$ , then $V'$ is the codomain of $T$ but it also says that $T[V]$ is the range of $T$ . $T[V]$ the same as $V'$ ?","My book says that if there is a linear transformation , then is the codomain of but it also says that is the range of . the same as ?",T: V \to V' V' T T[V] T T[V] V',['linear-algebra']
74,Is it always true that $\det(A^2+B^2)\geq0$?,Is it always true that ?,\det(A^2+B^2)\geq0,"Let $A$ and $B$ be real square matrices of the same size. Is it true that $$\det(A^2+B^2)\geq0\,?$$ If $AB=BA$ then the answer is positive: $$\det(A^2+B^2)=\det(A+iB)\det(A-iB)=\det(A+iB)\overline{\det(A+iB)}\geq0.$$","Let $A$ and $B$ be real square matrices of the same size. Is it true that $$\det(A^2+B^2)\geq0\,?$$ If $AB=BA$ then the answer is positive: $$\det(A^2+B^2)=\det(A+iB)\det(A-iB)=\det(A+iB)\overline{\det(A+iB)}\geq0.$$",,"['linear-algebra', 'matrices', 'determinant']"
75,Proof that the rank of a skew-symmetric matrix is at least $2$,Proof that the rank of a skew-symmetric matrix is at least,2,"Is there a succinct proof for the fact that the rank of a non-zero skew-symmetric matrix ($A = -A^T$)  is at least 2? I can think of a proof by contradiction: Assume rank is 1. Then you express all other rows as multiple of the first row. Using skew-symmetric property, this matrix has to be a zero matrix. Why does such a matrix have at least 2 non-zero eigenvalues?","Is there a succinct proof for the fact that the rank of a non-zero skew-symmetric matrix ($A = -A^T$)  is at least 2? I can think of a proof by contradiction: Assume rank is 1. Then you express all other rows as multiple of the first row. Using skew-symmetric property, this matrix has to be a zero matrix. Why does such a matrix have at least 2 non-zero eigenvalues?",,"['linear-algebra', 'matrices', 'matrix-rank', 'skew-symmetric-matrices']"
76,Find the standard matrix for a linear transformation,Find the standard matrix for a linear transformation,,If $T: \Bbb R^3→ \Bbb R^3$ is a linear transformation such that: $$ T \Bigg (\begin{bmatrix}-2 \\ 3 \\ -4 \\ \end{bmatrix} \Bigg) = \begin{bmatrix} 5\\ 3 \\ 14 \\ \end{bmatrix}$$ $$T \Bigg (\begin{bmatrix} 3 \\ -2 \\ 3 \\ \end{bmatrix} \Bigg) = \begin{bmatrix}-4 \\ 6 \\ -14 \\ \end{bmatrix}$$ $$  T\Bigg (\begin{bmatrix}-4 \\ -5 \\ 5 \\ \end{bmatrix} \Bigg) = \begin{bmatrix} -6\\ -40 \\ -2 \\ \end{bmatrix} $$ Then the standard matrix for T is... I'm not exactly sure how to approach this problem. Could anyone explain how to solve this problem?,If is a linear transformation such that: Then the standard matrix for T is... I'm not exactly sure how to approach this problem. Could anyone explain how to solve this problem?,"T: \Bbb R^3→ \Bbb R^3 
T \Bigg (\begin{bmatrix}-2 \\ 3 \\ -4 \\ \end{bmatrix} \Bigg) = \begin{bmatrix} 5\\ 3 \\ 14 \\ \end{bmatrix} T \Bigg (\begin{bmatrix} 3 \\ -2 \\ 3 \\ \end{bmatrix} \Bigg) = \begin{bmatrix}-4 \\ 6 \\ -14 \\ \end{bmatrix}   T\Bigg (\begin{bmatrix}-4 \\ -5 \\ 5 \\ \end{bmatrix} \Bigg) = \begin{bmatrix} -6\\ -40 \\ -2 \\ \end{bmatrix}
","['linear-algebra', 'matrices', 'linear-transformations']"
77,Do real matrices always have real eigenvalues?,Do real matrices always have real eigenvalues?,,"I was trying to show that orthogonal matrices have eigenvalues $1$ or $-1$. Let $u$ be an eigenvector of $A$ (orthogonal) corresponding to eigenvalue $\lambda$. Since orthogonal matrices preserve length, $ \|Au\|=|\lambda|\cdot\|u\|=\|u\|$. Since $\|u\|\ne0$, $|\lambda|=1$. Now I am stuck to show that lambda is only a real number. Can any one help with this?","I was trying to show that orthogonal matrices have eigenvalues $1$ or $-1$. Let $u$ be an eigenvector of $A$ (orthogonal) corresponding to eigenvalue $\lambda$. Since orthogonal matrices preserve length, $ \|Au\|=|\lambda|\cdot\|u\|=\|u\|$. Since $\|u\|\ne0$, $|\lambda|=1$. Now I am stuck to show that lambda is only a real number. Can any one help with this?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
78,A symmetric matrix whose square is zero,A symmetric matrix whose square is zero,,"I was once asked in an oral exam whether there can be a symmetric non zero matrix whose square is zero. After some thought I replied that there couldn't be because the minimal polynomial of such a matrix is guaranteed to be $x^2$ which shows that it isn't diagonalizable. I had to further clarify that a matrix is diagonalizable iff its minimal polynomial is a product of distinct linear factors, and that every symmetric matrix is diagonalizable. While all this is correct, the examiner mentioned that there is a simpler argument possible but he didn't elaborate on it. I have since been wondering what that simpler argument could be. Can someone give a simpler proof? Thanks","I was once asked in an oral exam whether there can be a symmetric non zero matrix whose square is zero. After some thought I replied that there couldn't be because the minimal polynomial of such a matrix is guaranteed to be $x^2$ which shows that it isn't diagonalizable. I had to further clarify that a matrix is diagonalizable iff its minimal polynomial is a product of distinct linear factors, and that every symmetric matrix is diagonalizable. While all this is correct, the examiner mentioned that there is a simpler argument possible but he didn't elaborate on it. I have since been wondering what that simpler argument could be. Can someone give a simpler proof? Thanks",,['linear-algebra']
79,"What about $GL(n,\mathbb C)$? Is it open, dense in $M(n,\mathbb C)$?","What about ? Is it open, dense in ?","GL(n,\mathbb C) M(n,\mathbb C)","What about $GL(n,\mathbb C)$? Is it open, dense in $M(n,\mathbb C)$?","What about $GL(n,\mathbb C)$? Is it open, dense in $M(n,\mathbb C)$?",,['linear-algebra']
80,"If $A$ and $B$ are positive definite, then is $B^{-1} - A^{-1}$ positive semidefinite?","If  and  are positive definite, then is  positive semidefinite?",A B B^{-1} - A^{-1},"I've found this while googling some properties of positive semidefinite matrices. (Unfortunately, I cannot remember where I've discovered it.) If this is true, it'll greatly save my time in my work. Is it true? How can you prove it? Let's say I have two real symmetric and positive definite matrices $\mathbf A$ and $\mathbf B$ of the same size. Also, assume that $\mathbf A - \mathbf B$ is positive semidefinite too. Then, $\mathbf B^{-1} - \mathbf A^{-1}$ is also positive semidefinite.","I've found this while googling some properties of positive semidefinite matrices. (Unfortunately, I cannot remember where I've discovered it.) If this is true, it'll greatly save my time in my work. Is it true? How can you prove it? Let's say I have two real symmetric and positive definite matrices $\mathbf A$ and $\mathbf B$ of the same size. Also, assume that $\mathbf A - \mathbf B$ is positive semidefinite too. Then, $\mathbf B^{-1} - \mathbf A^{-1}$ is also positive semidefinite.",,['linear-algebra']
81,How to find the basis for a vector space?,How to find the basis for a vector space?,,"I've been given the following as a homework problem: Find a basis for the following subspace of $F^5$:    $$W = \{(a, b, c, d, e) \in F^5 \mid a - c - d = 0\}$$ At the moment, I've been just guessing at potential solutions.  There must be a better method than guess and check. How do I solve this and similar problems?","I've been given the following as a homework problem: Find a basis for the following subspace of $F^5$:    $$W = \{(a, b, c, d, e) \in F^5 \mid a - c - d = 0\}$$ At the moment, I've been just guessing at potential solutions.  There must be a better method than guess and check. How do I solve this and similar problems?",,['linear-algebra']
82,Uncountable Basis?,Uncountable Basis?,,"I was reading up on the difference between countable and uncountable sets, and was wondering if there was a basis of uncountable size.  I now know there are, however they all seem to be covering rather high level areas in math.  So I was wondering; what is the simplest example of a basis of uncountable size for a vector space over a field?","I was reading up on the difference between countable and uncountable sets, and was wondering if there was a basis of uncountable size.  I now know there are, however they all seem to be covering rather high level areas in math.  So I was wondering; what is the simplest example of a basis of uncountable size for a vector space over a field?",,"['linear-algebra', 'vector-spaces']"
83,Does the set of matrix commutators form a subspace?,Does the set of matrix commutators form a subspace?,,"The following is an interesting problem from Linear Algebra 2nd Ed - Hoffman & Kunze (3.5 Q17). Let $W$ be the subspace spanned by the commutators of $M_{n\times n}\left(F\right)$: $$C=\left[A, B\right] = AB-BA$$  Prove that $W$ is exactly the subspace of matrices with zero trace. Assuming this is true, one can construct $n^2 - 1$ linearly independent matrices, in particular $$[e_{i,n}, e_{n,i}]\ \text{for $1\le i\le n-1$}$$ $$[e_{i,n}, e_{j,n}]\ \text{for $i\neq j$}$$ where $e_{i,j}$ are the standard basis with $0$ entry everywhere except row $i$ column $j$ which span the space of traceless matrices. However, I have trouble showing (or rather, believing, since this fact seems to be given) that the set of commutators form a subspace. In particular, I am having difficulty showing that the set is closed under addition. Can anyone shed some light?","The following is an interesting problem from Linear Algebra 2nd Ed - Hoffman & Kunze (3.5 Q17). Let $W$ be the subspace spanned by the commutators of $M_{n\times n}\left(F\right)$: $$C=\left[A, B\right] = AB-BA$$  Prove that $W$ is exactly the subspace of matrices with zero trace. Assuming this is true, one can construct $n^2 - 1$ linearly independent matrices, in particular $$[e_{i,n}, e_{n,i}]\ \text{for $1\le i\le n-1$}$$ $$[e_{i,n}, e_{j,n}]\ \text{for $i\neq j$}$$ where $e_{i,j}$ are the standard basis with $0$ entry everywhere except row $i$ column $j$ which span the space of traceless matrices. However, I have trouble showing (or rather, believing, since this fact seems to be given) that the set of commutators form a subspace. In particular, I am having difficulty showing that the set is closed under addition. Can anyone shed some light?",,['linear-algebra']
84,Can we deduce the characteristic polynomial for this matrix?,Can we deduce the characteristic polynomial for this matrix?,,"Given a square $n \times n$ matrix $A$ that satisfies $$\sum\limits_{k=0}^n a_k A^k = 0$$ for some coefficients $a_0, a_1, \dots, a_n,$ can we deduce that its characteristic polynomial is $\sum\limits_{k=0}^n a_k x^k$ ?",Given a square matrix that satisfies for some coefficients can we deduce that its characteristic polynomial is ?,"n \times n A \sum\limits_{k=0}^n a_k A^k = 0 a_0, a_1, \dots, a_n, \sum\limits_{k=0}^n a_k x^k","['linear-algebra', 'matrices', 'characteristic-polynomial']"
85,What is the most efficient way to determine if a matrix is invertible?,What is the most efficient way to determine if a matrix is invertible?,,"I'm learning Linear Algebra using MIT's Open Courseware Course 18.06 Quite often, the professor says ""... assuming that the matrix is invertible ..."". Somewhere in the lecture he says that using a determinant on an $n \times n$ matrix is on the order of $O(n!)$ operations, where an operation is a multiplication and a subtraction. Is there a more efficient way? If the aim is to get the inverse, rather than just determine the invertibility, what is the most effecient way to do this?","I'm learning Linear Algebra using MIT's Open Courseware Course 18.06 Quite often, the professor says ""... assuming that the matrix is invertible ..."". Somewhere in the lecture he says that using a determinant on an $n \times n$ matrix is on the order of $O(n!)$ operations, where an operation is a multiplication and a subtraction. Is there a more efficient way? If the aim is to get the inverse, rather than just determine the invertibility, what is the most effecient way to do this?",,"['linear-algebra', 'matrices']"
86,Detecting whether a point is above or below a slope,Detecting whether a point is above or below a slope,,Is there a simple test to know if a point is above or below a line in 2 dimensional vector domain?,Is there a simple test to know if a point is above or below a line in 2 dimensional vector domain?,,['linear-algebra']
87,"How to prove ""eigenvalues of polynomial of matrix $A$ = polynomial of eigenvalues of matrix $A$ ""","How to prove ""eigenvalues of polynomial of matrix  = polynomial of eigenvalues of matrix  """,A A,"Title looks a little bit twisted. What I want to say is the following: $A\in\mathbb{R}^{n\times n}$, polynomial of matrix $A$: $P(A)=\displaystyle \sum_{k=0}^{n} c_k A^k$. $\lambda(A)$ is the set of eigenvalues of $A$. So, we want to prove $\lambda(P(A))=P(\lambda(A))$ So far, I can prove it when $A$ is diagonalisable, but when $A$ is not diagonalisable, it seems we can't just simply use Jordan normal form to prove it. (Or this statement won't hold when it's not diagonalisable?) Can anyone suggest me something and help me out? Thanks!","Title looks a little bit twisted. What I want to say is the following: $A\in\mathbb{R}^{n\times n}$, polynomial of matrix $A$: $P(A)=\displaystyle \sum_{k=0}^{n} c_k A^k$. $\lambda(A)$ is the set of eigenvalues of $A$. So, we want to prove $\lambda(P(A))=P(\lambda(A))$ So far, I can prove it when $A$ is diagonalisable, but when $A$ is not diagonalisable, it seems we can't just simply use Jordan normal form to prove it. (Or this statement won't hold when it's not diagonalisable?) Can anyone suggest me something and help me out? Thanks!",,"['linear-algebra', 'polynomials', 'eigenvalues-eigenvectors']"
88,Proving that the dual of the $\mathcal{l}_p$ norm is the $\mathcal{l}_q$ norm.,Proving that the dual of the  norm is the  norm.,\mathcal{l}_p \mathcal{l}_q,"Let $\| \cdot \|$ be a norm on $\mathbb{R}^n$ . The associated dual norm, denoted $\| \cdot \|_*$ is defined as $\| z \|_* = \sup\{ z^{t} x : \| x \| < 1 \}$ . Does someone know how to prove that the dual norm of the $\mathcal l_{p}$ norm is the $\mathcal l_{q}$ norm? I read about norms and it was stated without proof in a book.","Let be a norm on . The associated dual norm, denoted is defined as . Does someone know how to prove that the dual norm of the norm is the norm? I read about norms and it was stated without proof in a book.",\| \cdot \| \mathbb{R}^n \| \cdot \|_* \| z \|_* = \sup\{ z^{t} x : \| x \| < 1 \} \mathcal l_{p} \mathcal l_{q},"['linear-algebra', 'functional-analysis']"
89,Proof of the Inverse of a Scalar times a Matrix,Proof of the Inverse of a Scalar times a Matrix,,How would I prove that given a square matrix $A$ and non-zero scalar $c$ that $$(cA)^{-1}=c^{-1}A^{-1}$$,How would I prove that given a square matrix $A$ and non-zero scalar $c$ that $$(cA)^{-1}=c^{-1}A^{-1}$$,,"['linear-algebra', 'matrices', 'inverse']"
90,What is the number of invertible $n\times n$ matrices in $\operatorname{GL}_n(F)$?,What is the number of invertible  matrices in ?,n\times n \operatorname{GL}_n(F),"$F$ is a finite field of order $q$. What is the size of $\operatorname{GL}_n(F)$ ? I am reading Dummit and Foote ""Abstract Algebra"".  The following formula is given:  $(q^n - 1)(q^n - q)\cdots(q^n - q^{n-1})$.  The case for $n = 1$ is trivial.  I understand that for $n = 2$  the first row of the matrix can be any ordered pair of field elements except for $0,0$. and the second row can be any ordered pair of field elements that is not a multiple of the first row.  So for $n = 2$  there are $(q^n - 1)(q^n - q)$ invertible matrices.  For $n\geq 3$, I cannot seem to understand why  the formula works.  I have looked at Sloane's OEIS A002884.  I have also constructed and stared at a list of all $168$ $3\times 3$ invertible matrices over $GF(2)$.  I would most appreciate a concrete and detailed explanation of how say $(2^3 - 1)(2^3 - 2)(2^3 - 2^2)$ counts these $168$ matrices.","$F$ is a finite field of order $q$. What is the size of $\operatorname{GL}_n(F)$ ? I am reading Dummit and Foote ""Abstract Algebra"".  The following formula is given:  $(q^n - 1)(q^n - q)\cdots(q^n - q^{n-1})$.  The case for $n = 1$ is trivial.  I understand that for $n = 2$  the first row of the matrix can be any ordered pair of field elements except for $0,0$. and the second row can be any ordered pair of field elements that is not a multiple of the first row.  So for $n = 2$  there are $(q^n - 1)(q^n - q)$ invertible matrices.  For $n\geq 3$, I cannot seem to understand why  the formula works.  I have looked at Sloane's OEIS A002884.  I have also constructed and stared at a list of all $168$ $3\times 3$ invertible matrices over $GF(2)$.  I would most appreciate a concrete and detailed explanation of how say $(2^3 - 1)(2^3 - 2)(2^3 - 2^2)$ counts these $168$ matrices.",,"['linear-algebra', 'combinatorics', 'group-theory']"
91,Invertible Matrices are dense,Invertible Matrices are dense,,"While reading about linear algebra for math olympiads in these notes , I came across the following assertion: Remark. The set of invertible matrices form a Zariski (dense) open subset, and hence to verify a polynomial identity, it suffices to verify it on this dense subset. Could someone provide an explanation of what it means to be a ""Zariski (dense) open subset""? A proof of this result is sketched in the notes, but I feel there is some deeper theory going on underneath. In case anyone is interested, the author has a similar set of notes here .","While reading about linear algebra for math olympiads in these notes , I came across the following assertion: Remark. The set of invertible matrices form a Zariski (dense) open subset, and hence to verify a polynomial identity, it suffices to verify it on this dense subset. Could someone provide an explanation of what it means to be a ""Zariski (dense) open subset""? A proof of this result is sketched in the notes, but I feel there is some deeper theory going on underneath. In case anyone is interested, the author has a similar set of notes here .",,"['linear-algebra', 'general-topology', 'matrices', 'problem-solving']"
92,Are the singular values of the transpose equal to those of the original matrix?,Are the singular values of the transpose equal to those of the original matrix?,,"It is well known that eigenvalues for real symmetric matrices are the same for matrices $A$ and its transpose $A^\dagger$. This made me wonder: Can I say the same about the singular values of a rectangular matrix? So basically, are the eigenvalues of $B B^\dagger$ the same as those of $B^\dagger B$?","It is well known that eigenvalues for real symmetric matrices are the same for matrices $A$ and its transpose $A^\dagger$. This made me wonder: Can I say the same about the singular values of a rectangular matrix? So basically, are the eigenvalues of $B B^\dagger$ the same as those of $B^\dagger B$?",,"['linear-algebra', 'matrices', 'singular-values']"
93,Why is $\mathbb{R}^2$ not a subspace of $\mathbb{R}^3$?,Why is  not a subspace of ?,\mathbb{R}^2 \mathbb{R}^3,"I cannot understand why $\mathbb{R}^2$ is not a subspace of $\mathbb{R}^3$. My reasoning is as follows: Choose any elements $v_1$ and $v_2$ from $\mathbb{R}^2$, add them together you get an element of $\mathbb{R}^2$ and same for scalar multiplication and $0$ vector is an element of $\mathbb{R}^2$. So where did I go wrong?","I cannot understand why $\mathbb{R}^2$ is not a subspace of $\mathbb{R}^3$. My reasoning is as follows: Choose any elements $v_1$ and $v_2$ from $\mathbb{R}^2$, add them together you get an element of $\mathbb{R}^2$ and same for scalar multiplication and $0$ vector is an element of $\mathbb{R}^2$. So where did I go wrong?",,"['linear-algebra', 'abstract-algebra', 'vectors']"
94,Adjoint of derivative operator on polynomial space,Adjoint of derivative operator on polynomial space,,"I was working on a problem when I made the following reasoning. I know that every linear operator $T:V \longrightarrow V$ on a Hilbert space $(V,\langle.,.\rangle)$ such that $\dim(V)<\infty$ has one (unique) adjoint operator $T^*:V \longrightarrow V$ (that is, $\langle T u,v\rangle = \langle u, T^* v \rangle$ $\forall u,v \in V$). So if $V:=P_n$ is the space of all polynomials with degree less than or equal to $n \in \mathbb{N}$ (which gives $\dim(V)=n+1<\infty$) and $\langle f,g \rangle := \int_0^1f(t)g(t) \,  dt$, what is the adjoint of the derivative operator $T=\dfrac{d}{dt}$? I've tried to solve that, but still to no avail. I wonder if that is a silly question, but I haven't had any success searching for the answer either, so I apologize in advance if that's the case.","I was working on a problem when I made the following reasoning. I know that every linear operator $T:V \longrightarrow V$ on a Hilbert space $(V,\langle.,.\rangle)$ such that $\dim(V)<\infty$ has one (unique) adjoint operator $T^*:V \longrightarrow V$ (that is, $\langle T u,v\rangle = \langle u, T^* v \rangle$ $\forall u,v \in V$). So if $V:=P_n$ is the space of all polynomials with degree less than or equal to $n \in \mathbb{N}$ (which gives $\dim(V)=n+1<\infty$) and $\langle f,g \rangle := \int_0^1f(t)g(t) \,  dt$, what is the adjoint of the derivative operator $T=\dfrac{d}{dt}$? I've tried to solve that, but still to no avail. I wonder if that is a silly question, but I haven't had any success searching for the answer either, so I apologize in advance if that's the case.",,['linear-algebra']
95,Probability of having zero determinant,Probability of having zero determinant,,"Given a matrix $A_{n \times n}$, which has elements $a_{i,j} \sim \mathrm{unif} \left[a,b\right]$, what is the probablity of $\det(A)$ being zero?  What if $a_{i,j}$ have any other distribution? Added: Let's assume an extension of the about problem; What is the probability of, $\mathbb{P}(|\det(A)| < \epsilon ), \; s.t. \; \epsilon \in \mathbb{R} $ ? Thanks!","Given a matrix $A_{n \times n}$, which has elements $a_{i,j} \sim \mathrm{unif} \left[a,b\right]$, what is the probablity of $\det(A)$ being zero?  What if $a_{i,j}$ have any other distribution? Added: Let's assume an extension of the about problem; What is the probability of, $\mathbb{P}(|\det(A)| < \epsilon ), \; s.t. \; \epsilon \in \mathbb{R} $ ? Thanks!",,"['linear-algebra', 'probability-theory']"
96,Real life applications of general vector spaces,Real life applications of general vector spaces,,Students familiar with Euclidean space find the introduction of general vectors spaces pretty boring and abstract particularly when describing vector spaces such as set of polynomials or set of continuous functions. Is there a tangible way to introduce this? Are there examples which will have a real impact? I would like to introduce this in an engaging manner to introductory students. Are there any real life applications of general vector spaces?,Students familiar with Euclidean space find the introduction of general vectors spaces pretty boring and abstract particularly when describing vector spaces such as set of polynomials or set of continuous functions. Is there a tangible way to introduce this? Are there examples which will have a real impact? I would like to introduce this in an engaging manner to introductory students. Are there any real life applications of general vector spaces?,,"['linear-algebra', 'vector-spaces', 'education', 'applications']"
97,Every matrix is a product of two symmetric matrices,Every matrix is a product of two symmetric matrices,,"Let $\mathbb{F}$ be a field with char $\mathbb{F} \neq 2$ . Let $A \in M_n(\mathbb{F})$ . Does there exist symmetric matrices $B,C \in M_n(\mathbb{F})$ such that $A=BC$ ? The answer is yes when $\mathbb{F} = \mathbb{C}$ or $\mathbb{R}$ . But how about other fields?",Let be a field with char . Let . Does there exist symmetric matrices such that ? The answer is yes when or . But how about other fields?,"\mathbb{F} \mathbb{F} \neq 2 A \in M_n(\mathbb{F}) B,C \in M_n(\mathbb{F}) A=BC \mathbb{F} = \mathbb{C} \mathbb{R}","['linear-algebra', 'matrices', 'products', 'symmetric-matrices']"
98,Properties of zero-diagonal symmetric matrices,Properties of zero-diagonal symmetric matrices,,"I'm interested in the properties of zero-diagonal symmetric (or hermitian) matrices, also known as hollow symmetric (or hermitian) matrices. The only thing I can come up with is that it cannot be positive definite (if it's not the zero matrix): The Cholesky decomposition provides for positive definite matrices $A$ the existence of a lower triangular $L$ with $A=LL^*$. However the diagonal of $LL^*$ is the inner product of each of the rows of $L$ with itself. Since the diagonal of $A$ consists of zeros, so $L$ (and thus $A$) must be the zero matrix. The sorts of questions that interest me are: which symmetric matrices can be transformed orthogonally into a zero-diagonal matrix? what can we say about the eigen-values of a zero-diagonal symmetric matrix?. any other interesting known properties??","I'm interested in the properties of zero-diagonal symmetric (or hermitian) matrices, also known as hollow symmetric (or hermitian) matrices. The only thing I can come up with is that it cannot be positive definite (if it's not the zero matrix): The Cholesky decomposition provides for positive definite matrices $A$ the existence of a lower triangular $L$ with $A=LL^*$. However the diagonal of $LL^*$ is the inner product of each of the rows of $L$ with itself. Since the diagonal of $A$ consists of zeros, so $L$ (and thus $A$) must be the zero matrix. The sorts of questions that interest me are: which symmetric matrices can be transformed orthogonally into a zero-diagonal matrix? what can we say about the eigen-values of a zero-diagonal symmetric matrix?. any other interesting known properties??",,"['linear-algebra', 'matrices', 'symmetric-matrices', 'hermitian-matrices']"
99,Inverse of a block matrix with singular diagonal blocks,Inverse of a block matrix with singular diagonal blocks,,"I have a special case where $$X=\left(\begin{array}{cc} A & B\\ C & 0 \end{array}\right)$$ and: $X$ is non-singular $A \in \Bbb R^{n \times n}$ is singular $B \in \Bbb R^{n \times m}$ is full column rank $C\in \Bbb R^{m \times n}$ is full row rank $D \ = 0_{m\times m}$ How do you calculate $X^{-1}$ in this case? For example, $$X=\begin{pmatrix}0&1\\1&0\end{pmatrix}$$","I have a special case where and: is non-singular is singular is full column rank is full row rank How do you calculate in this case? For example,","X=\left(\begin{array}{cc}
A & B\\
C & 0
\end{array}\right) X A \in \Bbb R^{n \times n} B \in \Bbb R^{n \times m} C\in \Bbb R^{m \times n} D \ = 0_{m\times m} X^{-1} X=\begin{pmatrix}0&1\\1&0\end{pmatrix}","['linear-algebra', 'matrices', 'inverse', 'block-matrices']"
