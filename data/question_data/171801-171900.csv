,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Statistical tests for checking that the mean is less than some value and for comparing two population means,Statistical tests for checking that the mean is less than some value and for comparing two population means,,"How do I do a statistics test that my population mean is less than some value (rather than equal to some value)? I want to do this in two flavors: sample 30 oranges and test whether the average weight of an orange from my population is less than 1 pound. sample 30 oranges and 30 apples test whether the the average weight of an apple from my population is more than 2x the the average weight of an orange from my population. For the first case, I see a Student's t -test for one sample which looks related: could I verify that the sample is different from 1 pound with that test, then actually compute the sample mean and compare to 1 pound to check if I'm below or above. For the second case, could I do a one-way anova ? I would double the weight of each sampled apple and use these new values as a sample to compare vs the sampled oranges. Again, I would use the test to verify that they were different, then actually compute the respective sample means and compare to see if I'm below or above. Is this the proper way to do such tests that a population mean is less/greater than some value, or is there some better way?","How do I do a statistics test that my population mean is less than some value (rather than equal to some value)? I want to do this in two flavors: sample 30 oranges and test whether the average weight of an orange from my population is less than 1 pound. sample 30 oranges and 30 apples test whether the the average weight of an apple from my population is more than 2x the the average weight of an orange from my population. For the first case, I see a Student's t -test for one sample which looks related: could I verify that the sample is different from 1 pound with that test, then actually compute the sample mean and compare to 1 pound to check if I'm below or above. For the second case, could I do a one-way anova ? I would double the weight of each sampled apple and use these new values as a sample to compare vs the sampled oranges. Again, I would use the test to verify that they were different, then actually compute the respective sample means and compare to see if I'm below or above. Is this the proper way to do such tests that a population mean is less/greater than some value, or is there some better way?",,['statistics']
1,Deriving a statistic of the t distribution with 2 degrees of freedom,Deriving a statistic of the t distribution with 2 degrees of freedom,,"Assume I have an independent $X_{1}, X_{2}, \ldots, X_{n}$ with $X_{i} \sim N(i,i^{2})$ and I want to find a statistic that has a t distribution with 2 degrees of freedom. How would I go about showing that?  I don't think t distribution is one that is related to the normal or the F, but I would like someone to help clarify the steps and methodology towards tackling problems like these.","Assume I have an independent $X_{1}, X_{2}, \ldots, X_{n}$ with $X_{i} \sim N(i,i^{2})$ and I want to find a statistic that has a t distribution with 2 degrees of freedom. How would I go about showing that?  I don't think t distribution is one that is related to the normal or the F, but I would like someone to help clarify the steps and methodology towards tackling problems like these.",,"['probability', 'probability-theory', 'statistics', 'probability-distributions', 'order-statistics']"
2,"Mean of values ${1 \over m} {\sum_{i=1}^m {s_i \over t_i}}$ vs. Value of sums ${\sum_{i=1}^m {s_i} \over \sum_{i=1}^m {t_i}}$, What is the relation?","Mean of values  vs. Value of sums , What is the relation?",{1 \over m} {\sum_{i=1}^m {s_i \over t_i}} {\sum_{i=1}^m {s_i} \over \sum_{i=1}^m {t_i}},"Similar to: Average of products VS. product of averages I have $$ R = \{(s_1, t_1), (s_2, t_2), ..., (s_n, t_n)\} $$ With the property that, for all pairs $(s_i, t_i)$, $0 \le s_i < t_i$. $$ \forall (s_i, t_i) : 0 \le s_i < t_i $$ The value of one pair, $(s_i, t_i)$ is $s_i \over t_i$ I want to select an $m$ sized subset where $m < n$ that maximizes value. One way to compute the value of a subset $P \subseteq R$ is to take the mean of the values of the individual pairs: $$p(P) = {1 \over m} {\sum_{i=1}^m {s_i \over t_i}}$$ Another way is to compute the value of the summed individual elements from the pairs: $$q(P) = {\sum_{i=1}^m {s_i} \over \sum_{i=1}^m {t_i}}$$ Let $P'$ be the subset $P$ of size $m$ such that $p(P)$ is maximized.  (This is easy.  Select the $m$ elements of $R$ with highest value.) Let $Q'$ be the subset $Q$ of size $m$ such that $q(Q)$ is maximized. (I think this requires combinatorial search.) What is the relationship between $p(P')$ and $q(Q')$? Will $p(P') \le q(Q')$ always? Which is the truer maximum value?","Similar to: Average of products VS. product of averages I have $$ R = \{(s_1, t_1), (s_2, t_2), ..., (s_n, t_n)\} $$ With the property that, for all pairs $(s_i, t_i)$, $0 \le s_i < t_i$. $$ \forall (s_i, t_i) : 0 \le s_i < t_i $$ The value of one pair, $(s_i, t_i)$ is $s_i \over t_i$ I want to select an $m$ sized subset where $m < n$ that maximizes value. One way to compute the value of a subset $P \subseteq R$ is to take the mean of the values of the individual pairs: $$p(P) = {1 \over m} {\sum_{i=1}^m {s_i \over t_i}}$$ Another way is to compute the value of the summed individual elements from the pairs: $$q(P) = {\sum_{i=1}^m {s_i} \over \sum_{i=1}^m {t_i}}$$ Let $P'$ be the subset $P$ of size $m$ such that $p(P)$ is maximized.  (This is easy.  Select the $m$ elements of $R$ with highest value.) Let $Q'$ be the subset $Q$ of size $m$ such that $q(Q)$ is maximized. (I think this requires combinatorial search.) What is the relationship between $p(P')$ and $q(Q')$? Will $p(P') \le q(Q')$ always? Which is the truer maximum value?",,"['sequences-and-series', 'statistics', 'data-analysis']"
3,"Calculate distribution of median statistic of a random sample of size 11 from uniform(0,1)","Calculate distribution of median statistic of a random sample of size 11 from uniform(0,1)",,"Okay, so firstly is there a difference between ""median distribution"" and the median? Here's my idea of tackling this problem.  I know that the median of a distribution is a value m s.t. $P(X\leq m) \geq 1/2$ and $P(X\geq m) \geq 1/2$.  Since we are looking at the uniform distribution which is continuous, we just need to make sure we have an m that satisfied the following: $$\int_{-\infty}^{m} 1 dx=\int_{m}^{\infty} 1 dx=1/2$$, right?  And to do this would we start off by taking the derivative?  I guess if you do that, you get the 1/2 becoming 0 and 1dx equalling 1dx which doesn't make any sense...  Am I thinking in the right approach and if not, what needs to be changed?","Okay, so firstly is there a difference between ""median distribution"" and the median? Here's my idea of tackling this problem.  I know that the median of a distribution is a value m s.t. $P(X\leq m) \geq 1/2$ and $P(X\geq m) \geq 1/2$.  Since we are looking at the uniform distribution which is continuous, we just need to make sure we have an m that satisfied the following: $$\int_{-\infty}^{m} 1 dx=\int_{m}^{\infty} 1 dx=1/2$$, right?  And to do this would we start off by taking the derivative?  I guess if you do that, you get the 1/2 becoming 0 and 1dx equalling 1dx which doesn't make any sense...  Am I thinking in the right approach and if not, what needs to be changed?",,"['probability', 'probability-theory', 'statistics']"
4,Bivariate probability distribution problem,Bivariate probability distribution problem,,"Problem : Let $X$ be a random variable such that $X \sim N(0, 1)$. Let $W$ be a random variable independent of X such that $\Pr [W = 1] = \Pr [W = −1] = \frac12$. Define $Y = WX$ Show that $Y$ has the same distribution as $X$. I don't really know where to begin with this problem. I have an idea in mind for bivariate transformations but I'm not sure that's even what I need to do. I think it's intuitively clear that $X$ and $Y$ have the same distributions but...I don't know how to show it? My attempt is as follows: $$Y = WX \implies P(Y)=P(WX)$$ since $\Pr [W = 1] = \Pr [W = −1] = \frac12 \implies$ $W$ is a discrete random variable. \begin{align} \implies & \Pr(Y \le y) = \Pr(W=w)\Pr(X \le x) \\[10pt] & = \sum_w \Pr(W=w) \cdot \frac1{\sqrt {2 \pi}}e^{-x^2/2} = 1\cdot\frac1{\sqrt {2 \pi}} e^{-x^2/2} = \frac1{\sqrt {2 \pi}} e^{-x^2/2} \end{align} If this is correct please let me know because I just kind of guessed this is what I was supposed to do. If not, any help would be great.","Problem : Let $X$ be a random variable such that $X \sim N(0, 1)$. Let $W$ be a random variable independent of X such that $\Pr [W = 1] = \Pr [W = −1] = \frac12$. Define $Y = WX$ Show that $Y$ has the same distribution as $X$. I don't really know where to begin with this problem. I have an idea in mind for bivariate transformations but I'm not sure that's even what I need to do. I think it's intuitively clear that $X$ and $Y$ have the same distributions but...I don't know how to show it? My attempt is as follows: $$Y = WX \implies P(Y)=P(WX)$$ since $\Pr [W = 1] = \Pr [W = −1] = \frac12 \implies$ $W$ is a discrete random variable. \begin{align} \implies & \Pr(Y \le y) = \Pr(W=w)\Pr(X \le x) \\[10pt] & = \sum_w \Pr(W=w) \cdot \frac1{\sqrt {2 \pi}}e^{-x^2/2} = 1\cdot\frac1{\sqrt {2 \pi}} e^{-x^2/2} = \frac1{\sqrt {2 \pi}} e^{-x^2/2} \end{align} If this is correct please let me know because I just kind of guessed this is what I was supposed to do. If not, any help would be great.",,"['probability', 'probability-theory', 'statistics', 'probability-distributions']"
5,"Find $P(X/Y \leq t)$, $P(XY \leq t)$, and use it to find $P(XY/Z \leq t)$","Find , , and use it to find",P(X/Y \leq t) P(XY \leq t) P(XY/Z \leq t),"Let X, Y, Z be independent uniform (0,1) random variables. We know that the uniform (0, 1) pdf will be f(x|a,b)=1/(b-a).  I'm not really seeing how you would deal with this.  For instance, how would you be able to bring Z into the equation if you're given two different $P(X/Y \leq t)$ and $P(XY \leq t)$?  It doesn't seem like it makes a whole lot of sense.","Let X, Y, Z be independent uniform (0,1) random variables. We know that the uniform (0, 1) pdf will be f(x|a,b)=1/(b-a).  I'm not really seeing how you would deal with this.  For instance, how would you be able to bring Z into the equation if you're given two different $P(X/Y \leq t)$ and $P(XY \leq t)$?  It doesn't seem like it makes a whole lot of sense.",,"['probability', 'probability-theory', 'statistics']"
6,Find A and B for a continuous random variable with the following density,Find A and B for a continuous random variable with the following density,,"Suppose $X$ is a (continuous) random variable with density   $$f_X(x) = \begin{cases} 0 & \mbox{if }x ≤ 0 \\[0.25ex] (A/x^2) & \mbox{if }0 < x < 1 \\[0.25ex] (B/x^2) &\mbox{if }x ≥ 1 \end{cases}$$ for some real numbers $A$ and $B$. What are $A$ and $B$? As a probability distribution, the density should be equal to 1 I tried integrating in terms of the intervals but it is not possible to integrate the $(A/x^2)$ as it doesn't converge in the interval $0$ to $1$. Any help would be appreciated!","Suppose $X$ is a (continuous) random variable with density   $$f_X(x) = \begin{cases} 0 & \mbox{if }x ≤ 0 \\[0.25ex] (A/x^2) & \mbox{if }0 < x < 1 \\[0.25ex] (B/x^2) &\mbox{if }x ≥ 1 \end{cases}$$ for some real numbers $A$ and $B$. What are $A$ and $B$? As a probability distribution, the density should be equal to 1 I tried integrating in terms of the intervals but it is not possible to integrate the $(A/x^2)$ as it doesn't converge in the interval $0$ to $1$. Any help would be appreciated!",,"['probability', 'statistics', 'random-variables']"
7,What is the significance of operating on random variables. Like finding E[x^2 + 3],What is the significance of operating on random variables. Like finding E[x^2 + 3],,I had read the wiki page about moments. But still unable to get the real life significance. https://en.wikipedia.org/wiki/Moment_(mathematics) It also says the second moment of the distribution is the variance. Does it mean E[x^2] is the variance ?? I also know variance is E[x^2]-(E[x])^2. I am lost. Any input will help. Thanks.,I had read the wiki page about moments. But still unable to get the real life significance. https://en.wikipedia.org/wiki/Moment_(mathematics) It also says the second moment of the distribution is the variance. Does it mean E[x^2] is the variance ?? I also know variance is E[x^2]-(E[x])^2. I am lost. Any input will help. Thanks.,,"['probability', 'statistics', 'probability-distributions', 'moment-generating-functions']"
8,Question about creating $2\times 2$ covariance matrix with call option?,Question about creating  covariance matrix with call option?,2\times 2,"I'm completely stuck on how to do this problem. How can you go about calculating the variance of $Y$ and the covariance between $X$ and $Y$? I'm not sure how to use the information given to solve this problem. Let X be normal with mean zero and variance $\sigma^2$. Let $Y$ be a call option on $X$ struck at $K = 0$. Calculate the $2 \times 2$ covariance matrix of $(X, Y)$.","I'm completely stuck on how to do this problem. How can you go about calculating the variance of $Y$ and the covariance between $X$ and $Y$? I'm not sure how to use the information given to solve this problem. Let X be normal with mean zero and variance $\sigma^2$. Let $Y$ be a call option on $X$ struck at $K = 0$. Calculate the $2 \times 2$ covariance matrix of $(X, Y)$.",,"['matrices', 'statistics', 'finance', 'covariance']"
9,How can I solve the probability of a sample mean exceeds population mean if I'm not provided means?,How can I solve the probability of a sample mean exceeds population mean if I'm not provided means?,,"The question states ""Times spent studying by students the week before exams follows a normal distribution with standard deviation of 8 hours. A random sample of four students was taken in order to estimate the man study time for the population of all students. What is the probability the sample mean exceeds the population mean by more than 2 hours?"" I'm not given any means to work with, so I can't use Z-Score as far as I can tell. I'm clearly not seeing something. Is there a way to back solve for the answer? Is it something along the lines of just 2/standard error and finding Z greater than that?","The question states ""Times spent studying by students the week before exams follows a normal distribution with standard deviation of 8 hours. A random sample of four students was taken in order to estimate the man study time for the population of all students. What is the probability the sample mean exceeds the population mean by more than 2 hours?"" I'm not given any means to work with, so I can't use Z-Score as far as I can tell. I'm clearly not seeing something. Is there a way to back solve for the answer? Is it something along the lines of just 2/standard error and finding Z greater than that?",,"['statistics', 'normal-distribution', 'central-limit-theorem']"
10,Determine Marginal Probability density function of X,Determine Marginal Probability density function of X,,"Random Variables X and Y have the joint probability density function f(x,y)= 8xy/81 for 0 < x < 3 and 0 < y < x Determine the marginal probability density function of X. I am confused if I should be using integration of 0 to X or 0 to 3? fx(X)=$$\frac{8}{81}$$  $$\int_{0}^{3} xy dy$$ : $$\frac{8X}{81}$$  $$\int_{0}^{3} [y^2] dy$$ $$\frac{8X}{81} (3^2) - (0^2)$$ =$$\frac{8X}{27}$$","Random Variables X and Y have the joint probability density function f(x,y)= 8xy/81 for 0 < x < 3 and 0 < y < x Determine the marginal probability density function of X. I am confused if I should be using integration of 0 to X or 0 to 3? fx(X)=$$\frac{8}{81}$$  $$\int_{0}^{3} xy dy$$ : $$\frac{8X}{81}$$  $$\int_{0}^{3} [y^2] dy$$ $$\frac{8X}{81} (3^2) - (0^2)$$ =$$\frac{8X}{27}$$",,['statistics']
11,stat probability,stat probability,,"Okay so $A=0.2, B=0.5$ and the probability that both $A$ and $B$ occur is equal to $0.12$. What is $P((A \cap B) \cup A^c)$? What I basically did was $0.12 \times 0.5+0.5+0.2-0.12 = 1.2$. Am I doing it right?","Okay so $A=0.2, B=0.5$ and the probability that both $A$ and $B$ occur is equal to $0.12$. What is $P((A \cap B) \cup A^c)$? What I basically did was $0.12 \times 0.5+0.5+0.2-0.12 = 1.2$. Am I doing it right?",,"['probability', 'statistics']"
12,Expected mean squared error and MSR,Expected mean squared error and MSR,,"In a small-scale regression study, five observations on $Y$ were   obtained corresponding to $X = 1,4,10, 11$, and $14$. Assume that   $\sigma=0.6,B_0=5,B_1=3$ a. What are the expected values off MSR and MSE here? b. For derermining whether or not a regression relation exists, would   it have been better or worse to have made the five observations at $X  = 6,7, 8, 9$, and $10$? Why? Would the same answer apply if the principal purpose were to estimate the mean response for $X = 8$?   Discuss. $$Y_i=B_0+B_1X_i+\epsilon_i$$ $$\hat{Y}_i=\hat{B}_0+\hat{B}_1X_i$$ $$MSR=\sum(\hat{Y}_i-\overline{Y})^2$$ $$MSE=\frac{\sum (Y_i-\hat{Y}_i)^2}{n-2}=\frac{\sum(B_0+B_1X_i+\epsilon_i-\hat{B}_0-\hat{B}_1 X_i)^2}{n-2}$$ I'm still doesn't understand what they want, they want $$E(MSE);E(MSR) \text{ ?}$$ What they mean by expected values?","In a small-scale regression study, five observations on $Y$ were   obtained corresponding to $X = 1,4,10, 11$, and $14$. Assume that   $\sigma=0.6,B_0=5,B_1=3$ a. What are the expected values off MSR and MSE here? b. For derermining whether or not a regression relation exists, would   it have been better or worse to have made the five observations at $X  = 6,7, 8, 9$, and $10$? Why? Would the same answer apply if the principal purpose were to estimate the mean response for $X = 8$?   Discuss. $$Y_i=B_0+B_1X_i+\epsilon_i$$ $$\hat{Y}_i=\hat{B}_0+\hat{B}_1X_i$$ $$MSR=\sum(\hat{Y}_i-\overline{Y})^2$$ $$MSE=\frac{\sum (Y_i-\hat{Y}_i)^2}{n-2}=\frac{\sum(B_0+B_1X_i+\epsilon_i-\hat{B}_0-\hat{B}_1 X_i)^2}{n-2}$$ I'm still doesn't understand what they want, they want $$E(MSE);E(MSR) \text{ ?}$$ What they mean by expected values?",,"['statistics', 'self-learning', 'regression', 'mean-square-error']"
13,Can you calculate confidence intervals for the population mean if the obtained sample is not normally distributed?,Can you calculate confidence intervals for the population mean if the obtained sample is not normally distributed?,,"From my knowledge, if the obtained sample is approximately normally distributed, we can use t-tables to calculate the population mean confidence interval without knowing the population standard deviation. However is there a way to calculate the population mean confidence interval if the obtained sample is not normally distributed??","From my knowledge, if the obtained sample is approximately normally distributed, we can use t-tables to calculate the population mean confidence interval without knowing the population standard deviation. However is there a way to calculate the population mean confidence interval if the obtained sample is not normally distributed??",,"['statistics', 'statistical-inference']"
14,Hipergeometric. Kids and candy.,Hipergeometric. Kids and candy.,,"Problem There are $15$ identical bags of candy each containing $20$ yellow, $15$ red, $5$ blue and $10$ green candies. $15$ children are each given their own candy bag and each randomly picks $12$ candies from their own bags. What is the probability that at least two of the kids will have at least one green candy? This what I get so far: Each bag contains $N=50$ candies out of which $k=10$ are green. Each child draws $n=12$ times without replacement. Considering the number of ""successes"", drawing a green candy, this is a Hypergeometric distribution with parameters $N,k,n$. Therefore, the probability that a child draws at least one candy is $$1-\frac{40\choose12}{50\choose 12}.$$ Now, I need to calculate the probabilty of two of this 15 kids  will have at least one green candy. I'm stuck here. I show my progress, after the help: I calculate P(X>=)=1-P(X=0) = 0.9539 Then Y=# of kids with green candy. P(y>=2)= 1- 15Cn0 p^0 (1-p0)^15 - 15Cn1 p^1 (1-p)^14       = 1-[(1)(1)(0.0461)^15 - (15 (0.9539) (0.0461)^14]       = aprox 1 After all this I'm thinking: Is that true? The probability of at leat two kids have at least one green candy could be 100%? Thanks, for your help community.","Problem There are $15$ identical bags of candy each containing $20$ yellow, $15$ red, $5$ blue and $10$ green candies. $15$ children are each given their own candy bag and each randomly picks $12$ candies from their own bags. What is the probability that at least two of the kids will have at least one green candy? This what I get so far: Each bag contains $N=50$ candies out of which $k=10$ are green. Each child draws $n=12$ times without replacement. Considering the number of ""successes"", drawing a green candy, this is a Hypergeometric distribution with parameters $N,k,n$. Therefore, the probability that a child draws at least one candy is $$1-\frac{40\choose12}{50\choose 12}.$$ Now, I need to calculate the probabilty of two of this 15 kids  will have at least one green candy. I'm stuck here. I show my progress, after the help: I calculate P(X>=)=1-P(X=0) = 0.9539 Then Y=# of kids with green candy. P(y>=2)= 1- 15Cn0 p^0 (1-p0)^15 - 15Cn1 p^1 (1-p)^14       = 1-[(1)(1)(0.0461)^15 - (15 (0.9539) (0.0461)^14]       = aprox 1 After all this I'm thinking: Is that true? The probability of at leat two kids have at least one green candy could be 100%? Thanks, for your help community.",,"['probability', 'statistics']"
15,"find the p.d.f. first by determining their d.f.’s, and secondly directly..","find the p.d.f. first by determining their d.f.’s, and secondly directly..",,"If the r.v. $X$ is distributed as Negative Exponential with parameter $λ$, find the p.d.f. of each one of the r.v.’s $Y, Z$, where $Y = e^X,\, Z = \log{X}$, first by determining their d.f.’s, and secondly directly.. Could any one help me with this question where $f(x)=λ e ^{− λ x}$ in Negative Exponential I got $fY(y)=\dfrac{1}{y\ fx(\log{y})}$,  $fZ(z)=e^z fx(e^z)$ in the direct way but I am not sure of my answer","If the r.v. $X$ is distributed as Negative Exponential with parameter $λ$, find the p.d.f. of each one of the r.v.’s $Y, Z$, where $Y = e^X,\, Z = \log{X}$, first by determining their d.f.’s, and secondly directly.. Could any one help me with this question where $f(x)=λ e ^{− λ x}$ in Negative Exponential I got $fY(y)=\dfrac{1}{y\ fx(\log{y})}$,  $fZ(z)=e^z fx(e^z)$ in the direct way but I am not sure of my answer",,"['statistics', 'proof-verification']"
16,Compute the distance between two points with standard normal distribution.,Compute the distance between two points with standard normal distribution.,,"Suppose the position of points $x_1$, $x_2$, $x_3,\ldots, x_{10}$ on real line satisfies standard normal distribution. Then for a new point $x$, set $d_i(x):=|x-x_i|$, $i=1,\ldots, 10$. Compute the density function $$p(x):=\min_{i}\{d_i(x)\}$$ I, apparently, get confused on how to write done $d_i$ and how to have $min$ function. Please advise!","Suppose the position of points $x_1$, $x_2$, $x_3,\ldots, x_{10}$ on real line satisfies standard normal distribution. Then for a new point $x$, set $d_i(x):=|x-x_i|$, $i=1,\ldots, 10$. Compute the density function $$p(x):=\min_{i}\{d_i(x)\}$$ I, apparently, get confused on how to write done $d_i$ and how to have $min$ function. Please advise!",,"['probability', 'statistics']"
17,"Covariance and correlation, and how are they related?","Covariance and correlation, and how are they related?",,"I get that corellation is the covariance divided by the multiplie variance of the two, uh, things. What i don't get is why they are divided by the multiplied variance, and why that limits the value to the range -1 : 1. I suppose i'm really looking for a logical explanation, although a mathematical one wold welcome nontheless.","I get that corellation is the covariance divided by the multiplie variance of the two, uh, things. What i don't get is why they are divided by the multiplied variance, and why that limits the value to the range -1 : 1. I suppose i'm really looking for a logical explanation, although a mathematical one wold welcome nontheless.",,"['statistics', 'correlation', 'covariance']"
18,Probability of second player winning once,Probability of second player winning once,,"Team A and Team B are both composed of three members, respectively. Each member is assigned with the order to play (e.g. first, second, and the last) and should obey the following rules.  (a) The first players of Team A and Team B match against each other  (b) The player who beat the opponent continues to play a game with the next player of the other team  (c) If all members of any team are defeated, the game is over  Find the probability that the second player of Team A wins only once. (The probability of each player winning is 0.5 and all matches end with a winner and a loser (there is no draw)) I think there are 5 cases of the second player winning. I just tried to count all the possible outcomes and got the answer to be 9/32. However i think i am wrong... Can anyone help me with this problem?","Team A and Team B are both composed of three members, respectively. Each member is assigned with the order to play (e.g. first, second, and the last) and should obey the following rules.  (a) The first players of Team A and Team B match against each other  (b) The player who beat the opponent continues to play a game with the next player of the other team  (c) If all members of any team are defeated, the game is over  Find the probability that the second player of Team A wins only once. (The probability of each player winning is 0.5 and all matches end with a winner and a loser (there is no draw)) I think there are 5 cases of the second player winning. I just tried to count all the possible outcomes and got the answer to be 9/32. However i think i am wrong... Can anyone help me with this problem?",,"['probability', 'statistics']"
19,"Probability that weighted selection will appear in result set when choosing 3, without replacement","Probability that weighted selection will appear in result set when choosing 3, without replacement",,"I'm trying to solve a combinatorics problem for a program I am writing but I'm having some trouble. Here's an abstracted version... I have 5 people with weights assigned to them A: 500 B: 300 C: 200 D: 150 E: 100 What is the probability of Person B being selected if I choose 3 at a time given these weights (without replacement)? So a result set might look like [A,D,E], [C,B,A], etc. If I set all the weights to 100 I can calculate the probability as (1/5) + (4/5)(1/4) + (4/5)(3/4)(1/3) = 3/5 I'm thinking about it as the probability B is chosen the first time plus  the probability it wasn't chosen the first time multiplied by the probability it is chosen the second time, and so on for the third time, but I'm getting confused and stuck as soon as I try to expand with different values. I'm looking for some sort of formula that would help me quickly calculate probabilities for hundred of different weightings. Thanks in advance for the help! EDIT: Seems like this is definitely know as a WRS-N-W problem or Weighted  Random  Sampling  without  Replacement, with defined Weights","I'm trying to solve a combinatorics problem for a program I am writing but I'm having some trouble. Here's an abstracted version... I have 5 people with weights assigned to them A: 500 B: 300 C: 200 D: 150 E: 100 What is the probability of Person B being selected if I choose 3 at a time given these weights (without replacement)? So a result set might look like [A,D,E], [C,B,A], etc. If I set all the weights to 100 I can calculate the probability as (1/5) + (4/5)(1/4) + (4/5)(3/4)(1/3) = 3/5 I'm thinking about it as the probability B is chosen the first time plus  the probability it wasn't chosen the first time multiplied by the probability it is chosen the second time, and so on for the third time, but I'm getting confused and stuck as soon as I try to expand with different values. I'm looking for some sort of formula that would help me quickly calculate probabilities for hundred of different weightings. Thanks in advance for the help! EDIT: Seems like this is definitely know as a WRS-N-W problem or Weighted  Random  Sampling  without  Replacement, with defined Weights",,"['combinatorics', 'statistics']"
20,"Computing $E(XY)$ for finding $Cov(X,Y)$",Computing  for finding,"E(XY) Cov(X,Y)","Consider tossing a cubic die once and let $n$ be the smallest number of dots that appear on top. Define two random variables $X$ and $Y$ such that: $X=1$ if $n \in \left \{1,2  \right \}$, $X=2$ if $n \in \left \{3,4  \right \}$ and $X=3$ if $n \in \left \{5,6  \right \}$ and $Y=0$ if $n \in \left \{3,6  \right \}$, $Y=1$ if $n \in \left \{1,4  \right \}$, and $Y=2$ if $n \in \left \{2,5  \right \}$ Find the $cov(X,Y)$. The support of $X$ is $\left \{1,2,3  \right \}$ and the support of $Y$ is $\left \{0,1,2  \right \}$. The marginal probabilities are $f_{x}(X=1)=\frac{1}{3}$, $f_{x}(X=2)=\frac{1}{3}$, and $f_{x}(X=3)=\frac{1}{3}$ and the same for $Y=\left \{ 0,1,2 \right \}$ I have found $E(Y)=\frac{1}{3} (0) + \frac{1}{3} (1) + \frac{1}{3} (2) = 1$ and $E(X)=2$. The formula for $Cov(X,Y) = E[(X-E(X))(Y-E(Y))]$, so I have $$Cov(X,Y) = E[(X-2)(Y-1)]=E[XY - X - 2Y +2]=E[XY]-E[X]-2E[Y]+2= E[XY] -2-2+2= E[XY] - 2.$$ EDIT: I was stuck on the $E[XY]$ part but have an answer now.","Consider tossing a cubic die once and let $n$ be the smallest number of dots that appear on top. Define two random variables $X$ and $Y$ such that: $X=1$ if $n \in \left \{1,2  \right \}$, $X=2$ if $n \in \left \{3,4  \right \}$ and $X=3$ if $n \in \left \{5,6  \right \}$ and $Y=0$ if $n \in \left \{3,6  \right \}$, $Y=1$ if $n \in \left \{1,4  \right \}$, and $Y=2$ if $n \in \left \{2,5  \right \}$ Find the $cov(X,Y)$. The support of $X$ is $\left \{1,2,3  \right \}$ and the support of $Y$ is $\left \{0,1,2  \right \}$. The marginal probabilities are $f_{x}(X=1)=\frac{1}{3}$, $f_{x}(X=2)=\frac{1}{3}$, and $f_{x}(X=3)=\frac{1}{3}$ and the same for $Y=\left \{ 0,1,2 \right \}$ I have found $E(Y)=\frac{1}{3} (0) + \frac{1}{3} (1) + \frac{1}{3} (2) = 1$ and $E(X)=2$. The formula for $Cov(X,Y) = E[(X-E(X))(Y-E(Y))]$, so I have $$Cov(X,Y) = E[(X-2)(Y-1)]=E[XY - X - 2Y +2]=E[XY]-E[X]-2E[Y]+2= E[XY] -2-2+2= E[XY] - 2.$$ EDIT: I was stuck on the $E[XY]$ part but have an answer now.",,"['probability', 'statistics']"
21,How to find the degrees of freedom for a chi-square variable,How to find the degrees of freedom for a chi-square variable,,How does one find the degrees of freedom for a Chi-square random variable when trying to fit a distribution to a sample? I read an explanation regarding this in this source . I don't understand how to find it yet. Can someone explain this to me?,How does one find the degrees of freedom for a Chi-square random variable when trying to fit a distribution to a sample? I read an explanation regarding this in this source . I don't understand how to find it yet. Can someone explain this to me?,,"['statistics', 'statistical-inference']"
22,Negative binomial maximum likelihood,Negative binomial maximum likelihood,,"The pdf of a negative binomial is $$θ(X=x)= \left( \begin{array}{c} x+j-1  \\ x  \end{array} \right)(1-θ)^xθ^j,$$ How would I create the likelihood of this function in order to maximize θ?And how does the likelihood change if there is n observations vs. 1 observation? So far, I have that the likelihood is ∏ (j + x − 1 C x) θ^j (1-θ)^x which is simplified to (through the derivative of the log-likelihood) =jn lnθ+∑X ln(1-θ) Meaning that jn/θ=∑X/1-θ I solved for theta and have: θ-hat= jn/(jn+∑x) Is there something wrong? If so, where? Something extra: (The alternative pdf is $$P(X=x|j,θ)= \binom{x-1}{j-1}θ^{j}(1-θ)^{x-j}$$ would it yield the same likelihood?)","The pdf of a negative binomial is $$θ(X=x)= \left( \begin{array}{c} x+j-1  \\ x  \end{array} \right)(1-θ)^xθ^j,$$ How would I create the likelihood of this function in order to maximize θ?And how does the likelihood change if there is n observations vs. 1 observation? So far, I have that the likelihood is ∏ (j + x − 1 C x) θ^j (1-θ)^x which is simplified to (through the derivative of the log-likelihood) =jn lnθ+∑X ln(1-θ) Meaning that jn/θ=∑X/1-θ I solved for theta and have: θ-hat= jn/(jn+∑x) Is there something wrong? If so, where? Something extra: (The alternative pdf is $$P(X=x|j,θ)= \binom{x-1}{j-1}θ^{j}(1-θ)^{x-j}$$ would it yield the same likelihood?)",,"['probability', 'statistics']"
23,Regression of Irregular Exponential,Regression of Irregular Exponential,,I am trying to model the population growth of countries with the following logistic equation: $$p(t) = \frac{P_oK}{P_0+(K-P_0)e^{(-rt)}}\tag{displayed}$$ Where $p$ = population; $P_0$ = initial population; $K$ = carrying capacity; $r$ = constant; and $t$ = time I have the data of the population over time and a set carrying capacity. But I would like to know how to go about performing a regression with this function to best fit the all the data I have available. Preface: I think I may be out of my depth here in the math required but I am very willing to try... and if nothing else it would at least satisfy my curiosity. Thanks.,I am trying to model the population growth of countries with the following logistic equation: $$p(t) = \frac{P_oK}{P_0+(K-P_0)e^{(-rt)}}\tag{displayed}$$ Where $p$ = population; $P_0$ = initial population; $K$ = carrying capacity; $r$ = constant; and $t$ = time I have the data of the population over time and a set carrying capacity. But I would like to know how to go about performing a regression with this function to best fit the all the data I have available. Preface: I think I may be out of my depth here in the math required but I am very willing to try... and if nothing else it would at least satisfy my curiosity. Thanks.,,['statistics']
24,Questions on finding expected value and variance on a Poisson distribution,Questions on finding expected value and variance on a Poisson distribution,,"FProblem: A student walks along a real line and tries to get to the origin. Each step he makes is random ; the larger the intended step, the greater the variance is of that step. When the student is at location $x$, the next step has a mean of $0$ and variance of $\alpha$. Let $X_n ={}$ position of the student after $n$ steps. Let $N\sim\mathrm{Poisson}(\lambda)$ Find (a) $E(X_N \mid X_0 = x_0)$ , (b) $\operatorname{Var}(X_N \mid X_0 = x_0)$ (Express $X_N$ as a sum of $N$ random variables) Approach: I first expressed $X_N$ as sum of $N$ RVs : $X_N = X_1+ X_2 + \cdots+ X_N$. For (a) I am thinking $E(X_N \mid X_0 = x_0) = E(X_1+ X_2 + \cdots + X_N \mid X_0 = x_0) = n(0) = 0$ and don't know how to do (b). Am I on the right track?","FProblem: A student walks along a real line and tries to get to the origin. Each step he makes is random ; the larger the intended step, the greater the variance is of that step. When the student is at location $x$, the next step has a mean of $0$ and variance of $\alpha$. Let $X_n ={}$ position of the student after $n$ steps. Let $N\sim\mathrm{Poisson}(\lambda)$ Find (a) $E(X_N \mid X_0 = x_0)$ , (b) $\operatorname{Var}(X_N \mid X_0 = x_0)$ (Express $X_N$ as a sum of $N$ random variables) Approach: I first expressed $X_N$ as sum of $N$ RVs : $X_N = X_1+ X_2 + \cdots+ X_N$. For (a) I am thinking $E(X_N \mid X_0 = x_0) = E(X_1+ X_2 + \cdots + X_N \mid X_0 = x_0) = n(0) = 0$ and don't know how to do (b). Am I on the right track?",,"['probability', 'statistics', 'random-variables', 'conditional-expectation', 'poisson-distribution']"
25,What is the relationship or difference between MLE and EM algorithm?,What is the relationship or difference between MLE and EM algorithm?,,"I am trying to study EM algorithm and Maximum Likelihood Estimation. Somehow, they both sound the same to me but can't really say the difference. Maybe I don't really understand any of them. I have just started. Can somebody tell me what they do and the relationship or difference between them?","I am trying to study EM algorithm and Maximum Likelihood Estimation. Somehow, they both sound the same to me but can't really say the difference. Maybe I don't really understand any of them. I have just started. Can somebody tell me what they do and the relationship or difference between them?",,"['statistics', 'data-analysis', 'statistical-mechanics', 'data-mining']"
26,How to prove the convergence in probability?,How to prove the convergence in probability?,,"$Y_n$ , $n = 1, 2, ...$ is a sequence of nonnegative random variables with their means converging to $0$ . Can we show $Y_n \rightarrow 0$ in probability? Thanks!",", is a sequence of nonnegative random variables with their means converging to . Can we show in probability? Thanks!","Y_n n = 1, 2, ... 0 Y_n \rightarrow 0","['probability-theory', 'statistics']"
27,Degrees of Freedom-the intution behind the concept.,Degrees of Freedom-the intution behind the concept.,,"What are degrees of freedom? I have some general information about this concept, but i would like to know how this concept originated theoretically and why is this concept so imptortant? Why is there a need for this concept? (e.g: in Student's distribution or in χ2 distribution) Thanks in advance! :)","What are degrees of freedom? I have some general information about this concept, but i would like to know how this concept originated theoretically and why is this concept so imptortant? Why is there a need for this concept? (e.g: in Student's distribution or in χ2 distribution) Thanks in advance! :)",,"['statistics', 'statistical-inference']"
28,Bounding $L^1$ norm of multinomial data,Bounding  norm of multinomial data,L^1,"Let $(X_1, X_2, \cdots, X_d) \sim Multinomial(n,(p_1,p_2, \cdots, p_d) )$.  I would like to have a high probability bound on  $$ \sum_{i=1}^d |X_i - np_i|. $$ I know that the marginal of each $X_i$ is binomial, so can I use this term by term in the summation to get a high probability bound of like $\sum_{i=1}^d\sqrt{np_i(1-p_i)}$?","Let $(X_1, X_2, \cdots, X_d) \sim Multinomial(n,(p_1,p_2, \cdots, p_d) )$.  I would like to have a high probability bound on  $$ \sum_{i=1}^d |X_i - np_i|. $$ I know that the marginal of each $X_i$ is binomial, so can I use this term by term in the summation to get a high probability bound of like $\sum_{i=1}^d\sqrt{np_i(1-p_i)}$?",,"['probability', 'statistics']"
29,Statistics question on basil bush random variable,Statistics question on basil bush random variable,,"The height, $H$ , in meters of a basil bush is a random variable with the probability density function $f_{_H}(t)=e^t,\;0\leq t\leq H_0$ such that $H_0$ is the maximal height. $\color{blue}{(1)}$ I need to find $H_0$ $\color{blue}{(2)}$ to find the average height of the basil bush $\color{blue}{(3)}$ to find the probability that the height of the basil bush is at least $0.3$ meters, exactly $0.3$ meters, at the most $0.3$ meters $\color{blue}{(4)}$ A basil bush is considered adult if its height is at least $0.3$ meters. What is the probability that the height of the adult basil bush is bigger then $0.5$ meters? Bigger then $(0.7)$ meters? My attempt: $\color{blue}{(1)}$ $$1=\int_{-\infty}^{\infty}f_{_X}(t)dt=\int_{0}^{H_0}e^t(t)dt=e^{H_0}-1$$ $$e^{H_0}=2, \;H_0=\ln(2)\approx\boxed{0.693}$$ $\color{blue}{(2)}$ $$E(X)=\int_{-\infty}^{\infty}t\cdot f_{_X}(t)dt=\int_{0}^{\ln(2)}t\cdot e^t(t)dt$$ $\color{gray}{\text{By parts z=t dz=dt  , dg=$e^tdt$, $g=e^t$}}$ $$$$ $$te^t\bigg|_{0}^{\ln(2)}-\int_{0}^{\ln(2)} e^tdt=\ln(2)e^{\ln(2)}-2+1\approx\boxed{0.386}$$ $\Longrightarrow$ The average height is 0.386 meters $\color{blue}{(3)}$ At least $0.3:$ $P(X\leq 0.3)=\displaystyle\int_{0}^{0.3}e^tdt \approx \boxed{0.349}$ Exactly $0.3:$ $P(X=0.3)=\boxed{0}$ At the most $0.3:$ $P(X \geq 0.3)=1-0.349\approx \boxed{0.651}$ $\color{blue}{(4)}$ $P(X> 0.5)=\displaystyle\int_{0.5}^{\ln(2)}=0.351\;,P(X\geq 0.3)=0.651,\;P(0.3\times 0.5)=P(0.15)=0.161$ $P(X\geq 0.5\big|X\geq 0.3)=\boxed{\frac{0.161}{0.351}}$ $P(X>0.7)=\displaystyle\int_{0.7}^{\ln(2)}e^tdt=2-e^{0.7}\approx 0.0138$ $P(X> 0.7\big|X\geq 0.3)=\frac{}{0.351}$ Is this correct?","The height, , in meters of a basil bush is a random variable with the probability density function such that is the maximal height. I need to find to find the average height of the basil bush to find the probability that the height of the basil bush is at least meters, exactly meters, at the most meters A basil bush is considered adult if its height is at least meters. What is the probability that the height of the adult basil bush is bigger then meters? Bigger then meters? My attempt: The average height is 0.386 meters At least Exactly At the most Is this correct?","H f_{_H}(t)=e^t,\;0\leq t\leq H_0 H_0 \color{blue}{(1)} H_0 \color{blue}{(2)} \color{blue}{(3)} 0.3 0.3 0.3 \color{blue}{(4)} 0.3 0.5 (0.7) \color{blue}{(1)} 1=\int_{-\infty}^{\infty}f_{_X}(t)dt=\int_{0}^{H_0}e^t(t)dt=e^{H_0}-1 e^{H_0}=2, \;H_0=\ln(2)\approx\boxed{0.693} \color{blue}{(2)} E(X)=\int_{-\infty}^{\infty}t\cdot f_{_X}(t)dt=\int_{0}^{\ln(2)}t\cdot e^t(t)dt \color{gray}{\text{By parts z=t dz=dt  , dg=e^tdt, g=e^t}}  te^t\bigg|_{0}^{\ln(2)}-\int_{0}^{\ln(2)} e^tdt=\ln(2)e^{\ln(2)}-2+1\approx\boxed{0.386} \Longrightarrow \color{blue}{(3)} 0.3: P(X\leq 0.3)=\displaystyle\int_{0}^{0.3}e^tdt \approx \boxed{0.349} 0.3: P(X=0.3)=\boxed{0} 0.3: P(X \geq 0.3)=1-0.349\approx \boxed{0.651} \color{blue}{(4)} P(X> 0.5)=\displaystyle\int_{0.5}^{\ln(2)}=0.351\;,P(X\geq 0.3)=0.651,\;P(0.3\times 0.5)=P(0.15)=0.161 P(X\geq 0.5\big|X\geq 0.3)=\boxed{\frac{0.161}{0.351}} P(X>0.7)=\displaystyle\int_{0.7}^{\ln(2)}e^tdt=2-e^{0.7}\approx 0.0138 P(X> 0.7\big|X\geq 0.3)=\frac{}{0.351}","['probability', 'statistics', 'proof-verification']"
30,Probability of histogram bars,Probability of histogram bars,,"Say I collect data that follows a Normal distribution $f(z)$ in a histogram with bins of width $w$. I want to calculate the probability that the number of hits $N_i > N_j$. My naive approach would be to first calculate the probability of finding a hit in bin $i$ by integrating $f(z)$ over the bin: $$ p_i = \int_{z_i-w/2}^{z_i+w/2}f(z)dz $$ The number of hits in a bin then follows a binomial distribution such that $$ P(N_i = x) = p_i^x\:(1-p_i)^{N-x}\cdot \left(\begin{matrix}N\\x\end{matrix}\right) $$ where $N=\sum_i{N_i}$ is the total number of hits collected. To find $P(N_i>N_j)$, I would then sum over all combinations for which this condition holds: $$ P(N_i>N_j) = \sum_{x=1}^{N/2} \sum_{y=0}^{x-1}P(N_i=x)\cdot P(N_j=y) $$ I was hoping for there is a more elegant, perhaps analytically solvable method of finding these probabilities.","Say I collect data that follows a Normal distribution $f(z)$ in a histogram with bins of width $w$. I want to calculate the probability that the number of hits $N_i > N_j$. My naive approach would be to first calculate the probability of finding a hit in bin $i$ by integrating $f(z)$ over the bin: $$ p_i = \int_{z_i-w/2}^{z_i+w/2}f(z)dz $$ The number of hits in a bin then follows a binomial distribution such that $$ P(N_i = x) = p_i^x\:(1-p_i)^{N-x}\cdot \left(\begin{matrix}N\\x\end{matrix}\right) $$ where $N=\sum_i{N_i}$ is the total number of hits collected. To find $P(N_i>N_j)$, I would then sum over all combinations for which this condition holds: $$ P(N_i>N_j) = \sum_{x=1}^{N/2} \sum_{y=0}^{x-1}P(N_i=x)\cdot P(N_j=y) $$ I was hoping for there is a more elegant, perhaps analytically solvable method of finding these probabilities.",,['statistics']
31,Approximation to a compounded Binomial distribution,Approximation to a compounded Binomial distribution,,"I need to find an approximation, from which I can easily sample, to the following compounded Binomial distribution: $X \sim \mathrm{Binomial}(e^{-\epsilon}, \ n)$ where $\epsilon \sim \mathrm{Gamma}(\alpha, \beta)$ I know that if $e^{-\epsilon}$ were a Beta then $X$ would be a Beta-Binomial. Moreover, I know that $N\ \mathrm{Beta(\alpha, N)} \rightarrow \mathrm{Gamma}(\alpha, 1)$ in distribution. However $e^{-\epsilon}$ is not a Gamma and I cannot use this nice approximation. $\epsilon$ has parameters such that its mode is around 0.1 so I thought I could use a Taylor expansion of $e^{-\epsilon}$, but it does not help in making my compound Binomial tractable. Since I want to sample from this in a particle filter, I don't want to use any importance sampling (or related) because I need something quite fast, so I'm looking for a known distribution which could be a good approximation. EDIT 1: I think I've found an answer to my problem. Notice that $e^{-\epsilon} \in ]0, 1]$. So it's the support of a Beta. Natural idea is to minimize the KL divergence between the distribution of $e^{-\epsilon}$ (we know its density its just an easy change of variable) and a Beta.  If we denote $P=e^{-\epsilon}$ and $q_P(p)=\frac{\beta^\alpha}{\Gamma(\alpha)}(-\log p)^{\alpha-1}p^{\beta-1}$ its density and if we denote $q(\ \cdot \ ; a, b)$ the density of a $\mathrm{Beta}(a,b)$ we have: \begin{equation*} \begin{split} D_{KL}(a,b) & = \int_{0}^{1}q_P(p)\frac{q_P(p)}{q(p; a, b)}\mathrm{d}p \\ & \propto - \int_{0}^{1}q_P(p)\log(q(p; a, b))\mathrm{d}p \\ & \propto - \int_{0}^{1}q_P(p)\log(\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}p^{a-1}(1-p)^{b-1})\mathrm{d}p \\ & \propto -\log\Gamma(a+b) + \log\Gamma(a) + \log\Gamma(b) \\ & \qquad  + (a-1)\int_{0}^{1}(-\log p)\frac{\beta^\alpha}{\Gamma(\alpha)}(-\log p)^{\alpha-1}p^{\beta-1}\mathrm{d}p \\ & \qquad - (b-1)\int_{0}^{1}\log (1-p)\frac{\beta^\alpha}{\Gamma(\alpha)}(-\log p)^{\alpha-1}p^{\beta-1}\mathrm{d}p \end{split} \end{equation*} Now notice that $-\log p(-\log p)^{\alpha-1}p^{\beta-1}= (-\log p)^{\alpha}p^{\beta-1}$ is proportional to the density of $e^{-X}$ where $X \sim \mathrm{Gamma}(\alpha+1, \beta)$ and if we denote $K=\int_{0}^{1}\log(1-p)\frac{\beta^\alpha}{\Gamma(\alpha)}(-\log p)^{\alpha-1}p^{\beta-1}\mathrm{d}p$ we have: \begin{equation*} \begin{split} D_{KL}(a,b) & = \propto -\log\Gamma(a+b) + \log\Gamma(a) + \log\Gamma(b) \\  & \qquad  + (a-1)\frac{\beta^\alpha}{\Gamma(\alpha)}\frac{\Gamma(\alpha+1)}{\beta^{\alpha+1}} - (b-1)K \\ & \propto -\log\Gamma(a+b) + \log\Gamma(a) + \log\Gamma(b) + (a-1)\frac{\alpha}{\beta} - (b-1)K \end{split} \end{equation*} Now we minimize by finding a critical point: \begin{equation} \frac{\partial }{\partial a}D_{KL} = -\psi^{(0)}(a+b) + \psi^{(0)}(a) + \frac{\alpha}{\beta} = 0 \end{equation} \begin{equation} \frac{\partial }{\partial b}D_{KL} = -\psi^{(0)}(a+b) + \psi^{(0)}(b) - K = 0 \end{equation} where $\psi^{(0)}$ is the digamma function. $K$ is easily calculated numerically and so is the system of two equations above I did a numerical example with  $\alpha = \beta = 10$ and found $a = 6.739281$ and $b=10.736471$. I sampled 10000 times from $e^{-\epsilon}$ and the Beta with coefficient $a$ and $b$ and drew the following QQ-plot which seems to indicate that the approximation is accurate and I can use a Beta-Binomial to sample from instead of from $\mathrm{Binomial}(e^{-\epsilon}, n)$:","I need to find an approximation, from which I can easily sample, to the following compounded Binomial distribution: $X \sim \mathrm{Binomial}(e^{-\epsilon}, \ n)$ where $\epsilon \sim \mathrm{Gamma}(\alpha, \beta)$ I know that if $e^{-\epsilon}$ were a Beta then $X$ would be a Beta-Binomial. Moreover, I know that $N\ \mathrm{Beta(\alpha, N)} \rightarrow \mathrm{Gamma}(\alpha, 1)$ in distribution. However $e^{-\epsilon}$ is not a Gamma and I cannot use this nice approximation. $\epsilon$ has parameters such that its mode is around 0.1 so I thought I could use a Taylor expansion of $e^{-\epsilon}$, but it does not help in making my compound Binomial tractable. Since I want to sample from this in a particle filter, I don't want to use any importance sampling (or related) because I need something quite fast, so I'm looking for a known distribution which could be a good approximation. EDIT 1: I think I've found an answer to my problem. Notice that $e^{-\epsilon} \in ]0, 1]$. So it's the support of a Beta. Natural idea is to minimize the KL divergence between the distribution of $e^{-\epsilon}$ (we know its density its just an easy change of variable) and a Beta.  If we denote $P=e^{-\epsilon}$ and $q_P(p)=\frac{\beta^\alpha}{\Gamma(\alpha)}(-\log p)^{\alpha-1}p^{\beta-1}$ its density and if we denote $q(\ \cdot \ ; a, b)$ the density of a $\mathrm{Beta}(a,b)$ we have: \begin{equation*} \begin{split} D_{KL}(a,b) & = \int_{0}^{1}q_P(p)\frac{q_P(p)}{q(p; a, b)}\mathrm{d}p \\ & \propto - \int_{0}^{1}q_P(p)\log(q(p; a, b))\mathrm{d}p \\ & \propto - \int_{0}^{1}q_P(p)\log(\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}p^{a-1}(1-p)^{b-1})\mathrm{d}p \\ & \propto -\log\Gamma(a+b) + \log\Gamma(a) + \log\Gamma(b) \\ & \qquad  + (a-1)\int_{0}^{1}(-\log p)\frac{\beta^\alpha}{\Gamma(\alpha)}(-\log p)^{\alpha-1}p^{\beta-1}\mathrm{d}p \\ & \qquad - (b-1)\int_{0}^{1}\log (1-p)\frac{\beta^\alpha}{\Gamma(\alpha)}(-\log p)^{\alpha-1}p^{\beta-1}\mathrm{d}p \end{split} \end{equation*} Now notice that $-\log p(-\log p)^{\alpha-1}p^{\beta-1}= (-\log p)^{\alpha}p^{\beta-1}$ is proportional to the density of $e^{-X}$ where $X \sim \mathrm{Gamma}(\alpha+1, \beta)$ and if we denote $K=\int_{0}^{1}\log(1-p)\frac{\beta^\alpha}{\Gamma(\alpha)}(-\log p)^{\alpha-1}p^{\beta-1}\mathrm{d}p$ we have: \begin{equation*} \begin{split} D_{KL}(a,b) & = \propto -\log\Gamma(a+b) + \log\Gamma(a) + \log\Gamma(b) \\  & \qquad  + (a-1)\frac{\beta^\alpha}{\Gamma(\alpha)}\frac{\Gamma(\alpha+1)}{\beta^{\alpha+1}} - (b-1)K \\ & \propto -\log\Gamma(a+b) + \log\Gamma(a) + \log\Gamma(b) + (a-1)\frac{\alpha}{\beta} - (b-1)K \end{split} \end{equation*} Now we minimize by finding a critical point: \begin{equation} \frac{\partial }{\partial a}D_{KL} = -\psi^{(0)}(a+b) + \psi^{(0)}(a) + \frac{\alpha}{\beta} = 0 \end{equation} \begin{equation} \frac{\partial }{\partial b}D_{KL} = -\psi^{(0)}(a+b) + \psi^{(0)}(b) - K = 0 \end{equation} where $\psi^{(0)}$ is the digamma function. $K$ is easily calculated numerically and so is the system of two equations above I did a numerical example with  $\alpha = \beta = 10$ and found $a = 6.739281$ and $b=10.736471$. I sampled 10000 times from $e^{-\epsilon}$ and the Beta with coefficient $a$ and $b$ and drew the following QQ-plot which seems to indicate that the approximation is accurate and I can use a Beta-Binomial to sample from instead of from $\mathrm{Binomial}(e^{-\epsilon}, n)$:",,"['probability', 'statistics', 'monte-carlo', 'binomial-distribution', 'gamma-distribution']"
32,How do I use interpolation with the Z table?,How do I use interpolation with the Z table?,,"My textbook has an example of interpolation, but I am not sure how the book did it since it doesn't explain it. It says if we want $P(Z < 1.246)$ we must use interpolation and the steps given are: $$P(Z < 1.24) + (6/10)[P(Z < 1.25) - P(Z < 1.24)].$$ Can someone explain to me where the $(6/10)$ came from? Shouldn't it be $(6/1000)$ since the $1.24+0.006 = 1.246?$ I am very very confused about how they got the 6 out of 1.246.","My textbook has an example of interpolation, but I am not sure how the book did it since it doesn't explain it. It says if we want $P(Z < 1.246)$ we must use interpolation and the steps given are: $$P(Z < 1.24) + (6/10)[P(Z < 1.25) - P(Z < 1.24)].$$ Can someone explain to me where the $(6/10)$ came from? Shouldn't it be $(6/1000)$ since the $1.24+0.006 = 1.246?$ I am very very confused about how they got the 6 out of 1.246.",,"['probability', 'statistics', 'interpolation']"
33,Measuring correlation of a truncated sample,Measuring correlation of a truncated sample,,"Suppose samples $(X_i,Y_i)$ were drawn from a multinomial distribution $N(\mu_X, \mu_Y, \sigma_X, \sigma_Y)$. The correlation between $X$ and $Y$ can be then estimated as $$\hat{\rho}_{XY}=\frac{1}{N}\sum_{i,j}\frac{(X_i - \mu_X)(Y_i - \mu_Y)}{\sigma_X\sigma_Y}$$ But now suppose tha all samples with $X_i>c$ are discarded, where $c$ is some constant. How does that change the correlation estimate? The way I see it, the sum can be decomposed as $$\hat{\rho}_{XY}=\frac{1}{\sigma_X\sigma_Y}\sum_{j}(Y_i - \mu_Y)\left( \frac{1}{N_C}\sum_{i\in C}(X_i - \mu_X) + \frac{1}{N_{C'}}\sum_{i\in C'}(X_i - \mu_X)\right)$$ where $C=\lbrace i;X_i\leq c\rbrace$, $C=\lbrace i;X_i > c\rbrace$ and $N$, $N_{C'}$ is the number of elements in $C$ and $C'$, respectively. Now now the correlation changes depending on the sign of the term $\frac{1}{N_{C'}}\sum_{i\in C'}(X_i - \mu_X)$ which is discarded: if $c>\mu_X$ then all terms in that sum will be positive so throwing these away will reduce the correlation, and vice versa if $c<\mu_X$. Is there a more intuitive way to see this? Also, what would be some unbiased estimator for the underlying distribution (i.e. the distribution without the discarded samples), if $c$ is known?","Suppose samples $(X_i,Y_i)$ were drawn from a multinomial distribution $N(\mu_X, \mu_Y, \sigma_X, \sigma_Y)$. The correlation between $X$ and $Y$ can be then estimated as $$\hat{\rho}_{XY}=\frac{1}{N}\sum_{i,j}\frac{(X_i - \mu_X)(Y_i - \mu_Y)}{\sigma_X\sigma_Y}$$ But now suppose tha all samples with $X_i>c$ are discarded, where $c$ is some constant. How does that change the correlation estimate? The way I see it, the sum can be decomposed as $$\hat{\rho}_{XY}=\frac{1}{\sigma_X\sigma_Y}\sum_{j}(Y_i - \mu_Y)\left( \frac{1}{N_C}\sum_{i\in C}(X_i - \mu_X) + \frac{1}{N_{C'}}\sum_{i\in C'}(X_i - \mu_X)\right)$$ where $C=\lbrace i;X_i\leq c\rbrace$, $C=\lbrace i;X_i > c\rbrace$ and $N$, $N_{C'}$ is the number of elements in $C$ and $C'$, respectively. Now now the correlation changes depending on the sign of the term $\frac{1}{N_{C'}}\sum_{i\in C'}(X_i - \mu_X)$ which is discarded: if $c>\mu_X$ then all terms in that sum will be positive so throwing these away will reduce the correlation, and vice versa if $c<\mu_X$. Is there a more intuitive way to see this? Also, what would be some unbiased estimator for the underlying distribution (i.e. the distribution without the discarded samples), if $c$ is known?",,['statistics']
34,sample variance of regular polygon upon superimposition of vertices,sample variance of regular polygon upon superimposition of vertices,,"Given, the vertices of a regular polygon, the centroid here would be the sample mean of the vertices and we assume it to be at the origin. The distance from each vertex to centroid is $\frac{s}{2\sin(\frac{180}{n})}$ where $n$ is number of sides and $s$ is the length of side. My definition of variance of vertices (loose notation, as I havent defined any probability law  or sampling law) is sum of squares of distance from each vertex to the center divided by the total number of vertices. Question: What would be the 'above defined' variance if one vertex was made to be superimposed on another adjacent vertex, i.e an edge was dropped? (Edge is dropped from a visual perspective only, and note that the number of vertices is still the same in the denominator for computing the variance.) Similarly, what would be the variance if two edges were dropped by superimposing two vertices on two corresponding adjacent vertices? and so on, what would be the variances after this process is done with three vertices? Can this sample variance be parametrized either as a formula or as an approximation, given as input the number of edges that are dropped (as in via superimposition), the number of sides originally before the dropping process, and the length of each side? Do note again, that the total number of points is not reduced and remains the same, and the edge is only visually seeming like dropped although in reality the edge exists with zero distance. On dimension, let's stick to 2D coordinates being used for describing each vertex of the regular polygon. Just in case you need some more structure or standardization of notation, you may assume that all the regular polygons that we start with prior to the point-superposition/edge dropping process are inscribed in a unit-circle. Also, I ask this question because I'd like to do this: Let V be the variance of the initial regular polygon. Then say the edge dropping was done to get a polygon with a given configuration of say k out of the total t points superimposing at P_1. Then I compute the variance of the new set of points, which will be lesser than V. Then I'd like to scale by \alpha the  rest of the vertices t-k so that the variance of all the points equals V. Then I'd like to compare the sum of squares of distances of all pairs of points in initial polygon vs the same in the configuration after the k, t-k configuration was reached and see how much they differ by.","Given, the vertices of a regular polygon, the centroid here would be the sample mean of the vertices and we assume it to be at the origin. The distance from each vertex to centroid is $\frac{s}{2\sin(\frac{180}{n})}$ where $n$ is number of sides and $s$ is the length of side. My definition of variance of vertices (loose notation, as I havent defined any probability law  or sampling law) is sum of squares of distance from each vertex to the center divided by the total number of vertices. Question: What would be the 'above defined' variance if one vertex was made to be superimposed on another adjacent vertex, i.e an edge was dropped? (Edge is dropped from a visual perspective only, and note that the number of vertices is still the same in the denominator for computing the variance.) Similarly, what would be the variance if two edges were dropped by superimposing two vertices on two corresponding adjacent vertices? and so on, what would be the variances after this process is done with three vertices? Can this sample variance be parametrized either as a formula or as an approximation, given as input the number of edges that are dropped (as in via superimposition), the number of sides originally before the dropping process, and the length of each side? Do note again, that the total number of points is not reduced and remains the same, and the edge is only visually seeming like dropped although in reality the edge exists with zero distance. On dimension, let's stick to 2D coordinates being used for describing each vertex of the regular polygon. Just in case you need some more structure or standardization of notation, you may assume that all the regular polygons that we start with prior to the point-superposition/edge dropping process are inscribed in a unit-circle. Also, I ask this question because I'd like to do this: Let V be the variance of the initial regular polygon. Then say the edge dropping was done to get a polygon with a given configuration of say k out of the total t points superimposing at P_1. Then I compute the variance of the new set of points, which will be lesser than V. Then I'd like to scale by \alpha the  rest of the vertices t-k so that the variance of all the points equals V. Then I'd like to compare the sum of squares of distances of all pairs of points in initial polygon vs the same in the configuration after the k, t-k configuration was reached and see how much they differ by.",,"['combinatorics', 'geometry', 'statistics', 'computational-geometry', 'combinatorial-geometry']"
35,About 'Marcinkiewicz–Zygmund inequality',About 'Marcinkiewicz–Zygmund inequality',,"Marcinkiewicz–Zygmund inequality gives gives relations between moments of a collection of independent random variables . The statement of this inequality can be seen in Wiki https://en.wikipedia.org/wiki/Marcinkiewicz%E2%80%93Zygmund_inequality However, another better form can be  seen among the top six lines on the left column in Page 5 of http://arxiv.org/pdf/1312.4626v1.pdf . However, the related citation Burkholder, D. L. Sharp inequalities for martingales and stochastic integrals can not be easily accessed on Internet. Could anyone rigorously restate the details of this form of Marcinkiewicz–Zygmund inequality?","Marcinkiewicz–Zygmund inequality gives gives relations between moments of a collection of independent random variables . The statement of this inequality can be seen in Wiki https://en.wikipedia.org/wiki/Marcinkiewicz%E2%80%93Zygmund_inequality However, another better form can be  seen among the top six lines on the left column in Page 5 of http://arxiv.org/pdf/1312.4626v1.pdf . However, the related citation Burkholder, D. L. Sharp inequalities for martingales and stochastic integrals can not be easily accessed on Internet. Could anyone rigorously restate the details of this form of Marcinkiewicz–Zygmund inequality?",,"['calculus', 'probability', 'probability-theory', 'statistics', 'random-variables']"
36,Statistical Significance of a Simple Test,Statistical Significance of a Simple Test,,"Please help me with this basic question on statistics:  If a standard brick is dropped on a standard raw chicken egg from 1 meter; the egg breaks. How many times does this dropped-brick-onto-egg need to be repeated to establish statistical significance? At what point can it be stated that: ""It has been proven that a brick dropped onto a raw egg will cause the egg to break"" or ""It has been proven to a 99% percent certainty."" (This is not a riddle or a joke. I'm trying to understand this basic concept.)","Please help me with this basic question on statistics:  If a standard brick is dropped on a standard raw chicken egg from 1 meter; the egg breaks. How many times does this dropped-brick-onto-egg need to be repeated to establish statistical significance? At what point can it be stated that: ""It has been proven that a brick dropped onto a raw egg will cause the egg to break"" or ""It has been proven to a 99% percent certainty."" (This is not a riddle or a joke. I'm trying to understand this basic concept.)",,['statistics']
37,Log-Likelihood Ratio of signals with multivariate normal distribution.,Log-Likelihood Ratio of signals with multivariate normal distribution.,,"I am on a project and I had to study about signal detection, Bayes decision theory etc. The basic paper I am reading, gives an equation for LLR, referring to these equations: $H_0 : Y\sim N(\mu_0,\Sigma_0)$ $H_1 : Y\sim N(\mu_1,\Sigma_1)$ The log-likelihood ratio is given then by: $$\log(L(y))=\frac{1}{2}\left[\log\frac{| \Sigma_0 |}{| \Sigma_1 |} + y^T(\Sigma_0^{-1} - \Sigma_1^{-1})y+2 (\mu_1^T\Sigma_1^{-1} -\mu_0^T \Sigma_0^{-1} )y+\mu_0^T \Sigma_0^{-1}\mu_0-\mu_1^T\Sigma_1^{-1}\mu_1\right]$$ But as I wanted to reproduce the final equation, I ended in this: $$\log(L(y))=\frac{1}{2}\left[\log\frac{| \Sigma_0 |}{| \Sigma_1 |}+y^{T}(\Sigma _0^{-1}-\Sigma_1^{-1})y+(\mu_1^T\Sigma_1^{-1}-\mu_0^T \Sigma_0^{-1}) y + \mu_0^T \Sigma_0^{-1} \mu_0-\mu_1^T\Sigma_1^{-1}\mu_1+y^T(\Sigma_1^{-1}\mu_1-\Sigma_0^{-1} \mu_0)\right]$$ The above are equal if only: $$y^T(\Sigma_1^{-1}\mu_1-\Sigma_0^{-1}\mu_0) = (\mu_1^T\Sigma_1^{-1}-\mu_0^T\Sigma_0^{-1})y$$ which I find obviously wrong. Can someone help me with this?","I am on a project and I had to study about signal detection, Bayes decision theory etc. The basic paper I am reading, gives an equation for LLR, referring to these equations: $H_0 : Y\sim N(\mu_0,\Sigma_0)$ $H_1 : Y\sim N(\mu_1,\Sigma_1)$ The log-likelihood ratio is given then by: $$\log(L(y))=\frac{1}{2}\left[\log\frac{| \Sigma_0 |}{| \Sigma_1 |} + y^T(\Sigma_0^{-1} - \Sigma_1^{-1})y+2 (\mu_1^T\Sigma_1^{-1} -\mu_0^T \Sigma_0^{-1} )y+\mu_0^T \Sigma_0^{-1}\mu_0-\mu_1^T\Sigma_1^{-1}\mu_1\right]$$ But as I wanted to reproduce the final equation, I ended in this: $$\log(L(y))=\frac{1}{2}\left[\log\frac{| \Sigma_0 |}{| \Sigma_1 |}+y^{T}(\Sigma _0^{-1}-\Sigma_1^{-1})y+(\mu_1^T\Sigma_1^{-1}-\mu_0^T \Sigma_0^{-1}) y + \mu_0^T \Sigma_0^{-1} \mu_0-\mu_1^T\Sigma_1^{-1}\mu_1+y^T(\Sigma_1^{-1}\mu_1-\Sigma_0^{-1} \mu_0)\right]$$ The above are equal if only: $$y^T(\Sigma_1^{-1}\mu_1-\Sigma_0^{-1}\mu_0) = (\mu_1^T\Sigma_1^{-1}-\mu_0^T\Sigma_0^{-1})y$$ which I find obviously wrong. Can someone help me with this?",,"['linear-algebra', 'statistics']"
38,"""At least"" type probability question.","""At least"" type probability question.",,"Recently, I asked a question: Team A has more Points than team B Though I ultimately got the right answer, it took extreme casework, and long computations. My question is: suppose the question was restated: Team A plays $5$ matches with Team $B, C, D, E, F$. Every team has probability $\frac{1}{2}$ of any match it plays. What is the probability that Team A wins at least two of the matches it plays? It would take casework like: $$\frac{\binom{5}{2} + \binom{5}{3} + \binom{5}{4} + \binom{5}{5}}{1024}$$ Is there another, easier/efficient way to do this?","Recently, I asked a question: Team A has more Points than team B Though I ultimately got the right answer, it took extreme casework, and long computations. My question is: suppose the question was restated: Team A plays $5$ matches with Team $B, C, D, E, F$. Every team has probability $\frac{1}{2}$ of any match it plays. What is the probability that Team A wins at least two of the matches it plays? It would take casework like: $$\frac{\binom{5}{2} + \binom{5}{3} + \binom{5}{4} + \binom{5}{5}}{1024}$$ Is there another, easier/efficient way to do this?",,"['probability', 'combinatorics', 'algebra-precalculus', 'statistics', 'contest-math']"
39,What is meant by statisticians when they talk about between population differences vs within population differences?,What is meant by statisticians when they talk about between population differences vs within population differences?,,"Suppose we have two populations of people in different parts of the world and we want to talk about the variation in heights between the two populations.  As I understand it, statisticians are talking about the variation of height in people randomly selected from each population. Suppose that everyone in one population is 5 feet tall and everyone in the other population is 6 feet tall.  Would it be correct to say that the within population variation is 0 but the between population variation is 1 foot?   Alternatively, if the mean and variance of the two populations were identical, would it be right to say that the between population variation is 0 and the within population variation is just the usual variance?  Is there a simple general formula for determining the two values?","Suppose we have two populations of people in different parts of the world and we want to talk about the variation in heights between the two populations.  As I understand it, statisticians are talking about the variation of height in people randomly selected from each population. Suppose that everyone in one population is 5 feet tall and everyone in the other population is 6 feet tall.  Would it be correct to say that the within population variation is 0 but the between population variation is 1 foot?   Alternatively, if the mean and variance of the two populations were identical, would it be right to say that the between population variation is 0 and the within population variation is just the usual variance?  Is there a simple general formula for determining the two values?",,"['statistics', 'standard-deviation', 'covariance']"
40,Prove that $E[U(X)] \ge E[U(Z)]$,Prove that,E[U(X)] \ge E[U(Z)],"Let U: $\mathbb R$ -> $\mathbb R$ be a concave function, let X be a random variable with a finite expected value, and let Y be a random variable that is independent of X and has an expected value 0. Define Z=X+Y. Prove that $E[U(X)] \ge E[U(Z)]$ I know that $E(X)=E(Z)$, and by Jensen's inequality $U[E(X)] \ge E[U(X)]$ but it gives me nothing so far. Please help. Thanks a lot.","Let U: $\mathbb R$ -> $\mathbb R$ be a concave function, let X be a random variable with a finite expected value, and let Y be a random variable that is independent of X and has an expected value 0. Define Z=X+Y. Prove that $E[U(X)] \ge E[U(Z)]$ I know that $E(X)=E(Z)$, and by Jensen's inequality $U[E(X)] \ge E[U(X)]$ but it gives me nothing so far. Please help. Thanks a lot.",,"['probability', 'statistics', 'statistical-inference', 'economics']"
41,Sample median of Cauchy distribution is consistent. How?,Sample median of Cauchy distribution is consistent. How?,,"When we use chebyshev's inequality to show whether an estimator is consistent or not, we require the mean square error of the estimator and I do not know sample median's probability distribution. So please advice how this can be shown.","When we use chebyshev's inequality to show whether an estimator is consistent or not, we require the mean square error of the estimator and I do not know sample median's probability distribution. So please advice how this can be shown.",,"['statistics', 'statistical-inference', 'quantile']"
42,probability application,probability application,,"1) A disease has hit a city. The percentage of the population infected $t$ days     after the disease arrives is approximated by $$p(t) = 12te^{\frac{-t}{7}} \qquad \mbox{for} \qquad0\leq t \leq 35.$$ After how many days is the percentage of infected people a maximum? What is the maximum percent of the population infected? The maximum percent of  the population  infected    is  ______  % 2) A container contains 12 diesel engines.  The company chooses 5 engines at random and will not ship the container if any of the engines chosen are defective.  Find the probability that a container will be shipped even though it contains 2 defectives if the sample size is 5. For the first problem, the number of days at which the percentage is at maximum is 7. Clearly, if I substitute this to $p(t)$ I will get the maximum percentage. My problem is how did they get the answer of 7 days? How do I deal with this kind of problem? Is there a specific formula? I'm trying to figure it out but can't. Also for the second problem I made use of the hypergeometric formula that is $$p(x) = \frac{\left[C(k,x) \cdot C(N-k, n-x)\right]}{C(N,n)}$$ where $N$ is the size of population, $k$ is the number of successes in the population, $x$ is the number of successes in the sample and $n$ is the sample size. I used this and I got a different answer. The answer should have been 0.318 but I got a different one. Please help.","1) A disease has hit a city. The percentage of the population infected $t$ days     after the disease arrives is approximated by $$p(t) = 12te^{\frac{-t}{7}} \qquad \mbox{for} \qquad0\leq t \leq 35.$$ After how many days is the percentage of infected people a maximum? What is the maximum percent of the population infected? The maximum percent of  the population  infected    is  ______  % 2) A container contains 12 diesel engines.  The company chooses 5 engines at random and will not ship the container if any of the engines chosen are defective.  Find the probability that a container will be shipped even though it contains 2 defectives if the sample size is 5. For the first problem, the number of days at which the percentage is at maximum is 7. Clearly, if I substitute this to $p(t)$ I will get the maximum percentage. My problem is how did they get the answer of 7 days? How do I deal with this kind of problem? Is there a specific formula? I'm trying to figure it out but can't. Also for the second problem I made use of the hypergeometric formula that is $$p(x) = \frac{\left[C(k,x) \cdot C(N-k, n-x)\right]}{C(N,n)}$$ where $N$ is the size of population, $k$ is the number of successes in the population, $x$ is the number of successes in the sample and $n$ is the sample size. I used this and I got a different answer. The answer should have been 0.318 but I got a different one. Please help.",,"['probability', 'statistics']"
43,Estimating grader bias/variance and MLE test scores given multiple graders assigned to grade each test,Estimating grader bias/variance and MLE test scores given multiple graders assigned to grade each test,,"Suppose we have $m$ graders and $n$ students, and we want to grade a test so that $k$ graders are assigned to grade to each test, and all graders grade the same number of tests. (I realize $m,n,k$ have to satisfy certain properties to make this ""perfect assignment"" possible, but I'd rather just skip this point and assume it's true). Also, to make things interesting, let's assume the assignment of graders to tests is random (so it's not like a group of graders all have the same set of tests to grade). Furthermore, let's assume that each grader $i$ has a mean bias $\mu_i$ and a variance $\sigma_i^2$ for that bias associated with their grading, and the bias they apply to each test they grade is sampled independently from a normal distribution with these parameters. And each test $j$ has a ""true grade"" $c_j$. So then if grader $i$ is assigned to grade test $j$, then the grade they assign will be $c_j + x_{ij}$ where $x_{ij}$ is the sampled bias from the normal distribution with parameters $\mu_i$ and $\sigma_i^2$. If the $\mu_i$ and $\sigma_i^2$ are unknown, how do we find the maximum likelihood values for the true grade scores $c_j$? If using a prior for graders' parameters is required I guess I'm ok with that. I would also like to know the MLE (or MAP if we go Bayesian) values for the grader parameters $\mu_i$ and $\sigma_i^2$. The idea being that graders with lower estimated variance should be preferred to those with higher variance, if we want as accurate of assigned grades as possible in the future. I've phrased this in terms of test grading for clairty, but it's actually for an ""active learning"" problem in machine learning that we are very interested in in our lab, hence insights on this problem could really help.","Suppose we have $m$ graders and $n$ students, and we want to grade a test so that $k$ graders are assigned to grade to each test, and all graders grade the same number of tests. (I realize $m,n,k$ have to satisfy certain properties to make this ""perfect assignment"" possible, but I'd rather just skip this point and assume it's true). Also, to make things interesting, let's assume the assignment of graders to tests is random (so it's not like a group of graders all have the same set of tests to grade). Furthermore, let's assume that each grader $i$ has a mean bias $\mu_i$ and a variance $\sigma_i^2$ for that bias associated with their grading, and the bias they apply to each test they grade is sampled independently from a normal distribution with these parameters. And each test $j$ has a ""true grade"" $c_j$. So then if grader $i$ is assigned to grade test $j$, then the grade they assign will be $c_j + x_{ij}$ where $x_{ij}$ is the sampled bias from the normal distribution with parameters $\mu_i$ and $\sigma_i^2$. If the $\mu_i$ and $\sigma_i^2$ are unknown, how do we find the maximum likelihood values for the true grade scores $c_j$? If using a prior for graders' parameters is required I guess I'm ok with that. I would also like to know the MLE (or MAP if we go Bayesian) values for the grader parameters $\mu_i$ and $\sigma_i^2$. The idea being that graders with lower estimated variance should be preferred to those with higher variance, if we want as accurate of assigned grades as possible in the future. I've phrased this in terms of test grading for clairty, but it's actually for an ""active learning"" problem in machine learning that we are very interested in in our lab, hence insights on this problem could really help.",,"['statistics', 'statistical-inference']"
44,"If a 17%-efficient system becomes ""10 times more efficient"", what is the absolute efficiency? Or is this not possible?","If a 17%-efficient system becomes ""10 times more efficient"", what is the absolute efficiency? Or is this not possible?",,"Sometimes in reading around the net, I see things like ""This car could be ten times more efficient if the drivetrain and engine were replaced by batteries and electric-motor wheels."" If I'm not mistaken, the usual tank-to-wheel efficiency of the average car is 17%. Ten times this would be 170%, which is just not possible. So what does ""ten times more efficient"" really mean? I suspect it's exponential or logarithmic in some way, but I can't even guess the formula. Or maybe most tech blogs are dumb and don't know what they're really saying mathematically? Maybe it's just not possible? In general, if something is x % efficient, and you want it to be p times more efficient, what would be the result, y %?","Sometimes in reading around the net, I see things like ""This car could be ten times more efficient if the drivetrain and engine were replaced by batteries and electric-motor wheels."" If I'm not mistaken, the usual tank-to-wheel efficiency of the average car is 17%. Ten times this would be 170%, which is just not possible. So what does ""ten times more efficient"" really mean? I suspect it's exponential or logarithmic in some way, but I can't even guess the formula. Or maybe most tech blogs are dumb and don't know what they're really saying mathematically? Maybe it's just not possible? In general, if something is x % efficient, and you want it to be p times more efficient, what would be the result, y %?",,"['statistics', 'percentages']"
45,Outlier detection with robust multiple regression model,Outlier detection with robust multiple regression model,,"I have a set of features (eg, location, income, budget, education) that I use to predict a continuous variable (say, amount spent per day on the internet). I am interested in detecting outliers. I want my model to be very strict and not to be swayed by outliers. I want my outlier detection to be done on the fly. My method is to use all the data I have so far to create a regression and then see if any point are above 3 SD from the residual mean (0). I then re-train the regression using all of the data EXCEPT the points I had just determined to be 3 SD from the residual mean. I continue this for some preset number of iterations, at each turn removing outliers and re-training. Each day I iteratively retrain the model using the new data and all of the old data. I was wondering if there is a name for this technique-- since it's the first thing I thought of, someone else must have thought of it already?","I have a set of features (eg, location, income, budget, education) that I use to predict a continuous variable (say, amount spent per day on the internet). I am interested in detecting outliers. I want my model to be very strict and not to be swayed by outliers. I want my outlier detection to be done on the fly. My method is to use all the data I have so far to create a regression and then see if any point are above 3 SD from the residual mean (0). I then re-train the regression using all of the data EXCEPT the points I had just determined to be 3 SD from the residual mean. I continue this for some preset number of iterations, at each turn removing outliers and re-training. Each day I iteratively retrain the model using the new data and all of the old data. I was wondering if there is a name for this technique-- since it's the first thing I thought of, someone else must have thought of it already?",,"['statistics', 'regression', 'data-analysis']"
46,What is the best way to interpolate over the 25th and 75th percentile of SAT scores?,What is the best way to interpolate over the 25th and 75th percentile of SAT scores?,,"The problem: I know the 25th and 75th percentiles of SAT scores for students admitted to a given university, and I want to interpolate over those two points in order to estimate all the percentiles (i.e. 1st-100th) of scores for students admitted to the university. What I know about SAT score distributions: SAT scores must be in the interval [600, 2400] and are approximately normally distributed on a nationwide basis: Some extremely competitive universities, such as MIT and Harvard, may have the highest possible SAT score (i.e. a 2400) at their 75th percentile, so I'm guessing their distribution might be truncated on the right side (not sure if this would still be a normal distribution?). I have a histogram of all 1,547,990 SAT scores taken in 2010 including the mean and standard deviation: http://professionals.collegeboard.com/profdownload/sat-percentile-ranks-composite-cr-m-w-2010.pdf .","The problem: I know the 25th and 75th percentiles of SAT scores for students admitted to a given university, and I want to interpolate over those two points in order to estimate all the percentiles (i.e. 1st-100th) of scores for students admitted to the university. What I know about SAT score distributions: SAT scores must be in the interval [600, 2400] and are approximately normally distributed on a nationwide basis: Some extremely competitive universities, such as MIT and Harvard, may have the highest possible SAT score (i.e. a 2400) at their 75th percentile, so I'm guessing their distribution might be truncated on the right side (not sure if this would still be a normal distribution?). I have a histogram of all 1,547,990 SAT scores taken in 2010 including the mean and standard deviation: http://professionals.collegeboard.com/profdownload/sat-percentile-ranks-composite-cr-m-w-2010.pdf .",,"['probability', 'statistics', 'probability-distributions', 'normal-distribution', 'interpolation']"
47,Do we only use T distribution for confidence interval of Beta for linear regression or we can also normal distribution?,Do we only use T distribution for confidence interval of Beta for linear regression or we can also normal distribution?,,Do we only use T distribution for confidence interval of Beta for linear regression or we can also normal distribution? Is it that when sample size is less than 30 then we use T distribution else Normal distribution.,Do we only use T distribution for confidence interval of Beta for linear regression or we can also normal distribution? Is it that when sample size is less than 30 then we use T distribution else Normal distribution.,,"['statistics', 'regression']"
48,What is the statistical equilibrium for this simulation of happy bubbles?,What is the statistical equilibrium for this simulation of happy bubbles?,,"Happy Bubbles I hope this is not too specific or practical, but I just made a simulation of sorts and seem to have hit quite close to an equilibrium (by accident). Now I am wondering if and how you would calculate the exact function or exact value for this situations statistical ""sweetspot"". The Problem I was playing around with D3.js and created a canvas with randomly appearing bubbles with a 5% chance every iteration. Also every iteration bubbles touching any border will disappear. This can happen because the bubbles move around every iteration. The Parameters Starting with a single bubble Every bubble has a x- and a y-coordinate which represents the center of the bubble Bubble Size: 20px radius Iterationlength: 100 ms Canvas Size: 600px by 600px Bubble Speed: -5px to 5px on X-Axis and Y-Axis ( Math.random() * 10) - 5 (the random number is a floating point number between 0 (including) and 1 (excluding)) Starting position per bubble: Math.random() * 600 for x and y coordinate (seperately) Bubble pops when it gets within 20px of any outer edge (again using the center as reference) The Equilibrium With these Parameters new bubbles and disappearing bubbles seem to cancel each other out. The number of bubbles ranges from 100 to 130 but has not breached those limits in more then an hour. The Question Do these Parameters ensure such an equilibirum or is it nothing special, that the number seems to be rather stable? How would one calculate this? Caveats One I can think of is that the bubble could move from 20.000001 to 15.000001 in one iteration. But I guess that is a small detail. Maybe this isn't even relevant. Edit: Here is the jsFiddle! (Be carefull with less then 100 ms iteration time, it might stress the browser)","Happy Bubbles I hope this is not too specific or practical, but I just made a simulation of sorts and seem to have hit quite close to an equilibrium (by accident). Now I am wondering if and how you would calculate the exact function or exact value for this situations statistical ""sweetspot"". The Problem I was playing around with D3.js and created a canvas with randomly appearing bubbles with a 5% chance every iteration. Also every iteration bubbles touching any border will disappear. This can happen because the bubbles move around every iteration. The Parameters Starting with a single bubble Every bubble has a x- and a y-coordinate which represents the center of the bubble Bubble Size: 20px radius Iterationlength: 100 ms Canvas Size: 600px by 600px Bubble Speed: -5px to 5px on X-Axis and Y-Axis ( Math.random() * 10) - 5 (the random number is a floating point number between 0 (including) and 1 (excluding)) Starting position per bubble: Math.random() * 600 for x and y coordinate (seperately) Bubble pops when it gets within 20px of any outer edge (again using the center as reference) The Equilibrium With these Parameters new bubbles and disappearing bubbles seem to cancel each other out. The number of bubbles ranges from 100 to 130 but has not breached those limits in more then an hour. The Question Do these Parameters ensure such an equilibirum or is it nothing special, that the number seems to be rather stable? How would one calculate this? Caveats One I can think of is that the bubble could move from 20.000001 to 15.000001 in one iteration. But I guess that is a small detail. Maybe this isn't even relevant. Edit: Here is the jsFiddle! (Be carefull with less then 100 ms iteration time, it might stress the browser)",,"['statistics', 'stochastic-processes', 'random-variables', 'random', 'simulation']"
49,How to write R program to solve the confidence interval?,How to write R program to solve the confidence interval?,,"The problem: let $X_1,\ldots,X_n$ be random variable from $\mathrm{Poisson}(\theta)$. Under $H_0: \theta=\theta_0$, we want to find the $(1-\alpha)100\%$ confidence interval for $\theta$ by using the likelihood ratio interval, $-2\ln\lambda(x)$.  Now, I have $$1-\alpha = P\{2n[(\theta-\bar{X})-\bar{X} \ln(\theta/\bar{X})] \leq \chi^2_{0.05,df.=1}\}$$ where $2n[(\theta-\bar{X})-\bar{X}\ln(\theta/\bar{X})]$ has chi-square distribution with df. one for large sample size. This is not a formula for the upper limit of $\theta$. Thus I must use the R program to solve it but I have no idea. Please suggest me how to do and for improve my work. Thanks.","The problem: let $X_1,\ldots,X_n$ be random variable from $\mathrm{Poisson}(\theta)$. Under $H_0: \theta=\theta_0$, we want to find the $(1-\alpha)100\%$ confidence interval for $\theta$ by using the likelihood ratio interval, $-2\ln\lambda(x)$.  Now, I have $$1-\alpha = P\{2n[(\theta-\bar{X})-\bar{X} \ln(\theta/\bar{X})] \leq \chi^2_{0.05,df.=1}\}$$ where $2n[(\theta-\bar{X})-\bar{X}\ln(\theta/\bar{X})]$ has chi-square distribution with df. one for large sample size. This is not a formula for the upper limit of $\theta$. Thus I must use the R program to solve it but I have no idea. Please suggest me how to do and for improve my work. Thanks.",,"['statistics', 'statistical-inference']"
50,Finding variance,Finding variance,,"Given a sapmle $(X_1, X_2 , \ldots , X_n)$ from  normal distribution with parameters $(a , \sigma ^ 2)$, find $$ \operatorname{Var}\left( \frac{1}{n} \sum\limits_{n=1}^n(X_i - \overline X)^2\right)$$ where $\overline X$ is the sample mean. I can calculate it when $\overline X$  is replaced with expectation using independence , but in this case I cannot use independence. I want to reduce it to fourth moment of normal distribution but I fail to.","Given a sapmle $(X_1, X_2 , \ldots , X_n)$ from  normal distribution with parameters $(a , \sigma ^ 2)$, find $$ \operatorname{Var}\left( \frac{1}{n} \sum\limits_{n=1}^n(X_i - \overline X)^2\right)$$ where $\overline X$ is the sample mean. I can calculate it when $\overline X$  is replaced with expectation using independence , but in this case I cannot use independence. I want to reduce it to fourth moment of normal distribution but I fail to.",,"['probability', 'statistics', 'probability-theory', 'probability-distributions']"
51,Probability of Independent Events individual vs in series,Probability of Independent Events individual vs in series,,"I understand that independent events (such as a fair coin flip) should not be viewed in succession. For example, if you flip heads 10 times in a row, the odds of flipping the next coin heads is still 50%. However, there is another way of looking at the coin flips. What is the probability of flipping 11 heads in a row And I already know that to be 0.5^11 ~= 0.1% So if you were making a bet on coin flips and 10 heads came up, would you still base a bet on the 50% fact, or on the odds of getting heads 11 times in a row (.1%) fact? If you consider the 50% fact or 0.1% fact, please explain why?","I understand that independent events (such as a fair coin flip) should not be viewed in succession. For example, if you flip heads 10 times in a row, the odds of flipping the next coin heads is still 50%. However, there is another way of looking at the coin flips. What is the probability of flipping 11 heads in a row And I already know that to be 0.5^11 ~= 0.1% So if you were making a bet on coin flips and 10 heads came up, would you still base a bet on the 50% fact, or on the odds of getting heads 11 times in a row (.1%) fact? If you consider the 50% fact or 0.1% fact, please explain why?",,"['probability', 'probability-theory', 'statistics', 'gambling']"
52,Unbiased estimators for binomial distributed variable,Unbiased estimators for binomial distributed variable,,"I'm having trouble determining whether the estimator beneath is unbiased or not. First some info: $x_i \sim binom(n_i,p)$ I have a sample of $x_1,x_2,x_3,x_4$ so $n=4$ I want to find out if the following $\tilde{p}=\frac{1}{4}\left(\frac{x_{1}}{n_{1}}+\frac{x_{2}}{n_{2}}+\frac{x_{3}}{n_{3}}+\frac{x_{4}}{n_{4}}\right)$ I know that the criteria for an unbiased estimator is that $E(\tilde{p})=p$ This is what I came up with: $E(\tilde{p})=E\left(\frac{1}{4}\left(\frac{x_{1}}{n_{1}}+\frac{x_{2}}{n_{2}}+\frac{x_{3}}{n_{3}}+\frac{x_{4}}{n_{4}}\right)\right)=\frac{1}{4}E\left(\frac{x_{1}}{n_{1}}+\frac{x_{2}}{n_{2}}+\frac{x_{3}}{n_{3}}+\frac{x_{4}}{n_{4}}\right)$ Now, I'm not sure if my next step is valid and how to proceed if it is: $E(\tilde{p})=\frac{1}{4}E\left(\frac{x_{1}}{n_{1}}+\frac{x_{2}}{n_{2}}+\frac{x_{3}}{n_{3}}+\frac{x_{4}}{n_{4}}\right)=\frac{1}{4}\left(\frac{E(x_{1})}{n_{1}}+\frac{E(x_{2})}{n_{2}}+\frac{E(x_{3})}{n_{3}}+\frac{E(x_{4})}{n_{4}}\right)$ I tried to look in other threads but I wasn't able to find any similar calculation.","I'm having trouble determining whether the estimator beneath is unbiased or not. First some info: $x_i \sim binom(n_i,p)$ I have a sample of $x_1,x_2,x_3,x_4$ so $n=4$ I want to find out if the following $\tilde{p}=\frac{1}{4}\left(\frac{x_{1}}{n_{1}}+\frac{x_{2}}{n_{2}}+\frac{x_{3}}{n_{3}}+\frac{x_{4}}{n_{4}}\right)$ I know that the criteria for an unbiased estimator is that $E(\tilde{p})=p$ This is what I came up with: $E(\tilde{p})=E\left(\frac{1}{4}\left(\frac{x_{1}}{n_{1}}+\frac{x_{2}}{n_{2}}+\frac{x_{3}}{n_{3}}+\frac{x_{4}}{n_{4}}\right)\right)=\frac{1}{4}E\left(\frac{x_{1}}{n_{1}}+\frac{x_{2}}{n_{2}}+\frac{x_{3}}{n_{3}}+\frac{x_{4}}{n_{4}}\right)$ Now, I'm not sure if my next step is valid and how to proceed if it is: $E(\tilde{p})=\frac{1}{4}E\left(\frac{x_{1}}{n_{1}}+\frac{x_{2}}{n_{2}}+\frac{x_{3}}{n_{3}}+\frac{x_{4}}{n_{4}}\right)=\frac{1}{4}\left(\frac{E(x_{1})}{n_{1}}+\frac{E(x_{2})}{n_{2}}+\frac{E(x_{3})}{n_{3}}+\frac{E(x_{4})}{n_{4}}\right)$ I tried to look in other threads but I wasn't able to find any similar calculation.",,['statistics']
53,Confidence Interval Question - Confused on Approaching the Problem,Confidence Interval Question - Confused on Approaching the Problem,,"My question is almost exactly the same as another one here on math stack exchange, but it isn't as explanatory as I'd like it to be with some parts. I was unsure of whether or not it'd be ""okay"" to repost the question, especially since the other one was asked about three years ago. Question is: An electric scale gives a reading equal to the true weight plus a random error that is normally distributed with mean 0 mg and standard deviation = 0.1 mg. Suppose that the results of five successive weighings (in mg) of the same object are as follows:     3.142, 3.163, 3.155, 3.150, 3.141. a) Compute a 95 percent confidence interval estimate of the true weight.  b) Compute a 99 percent confidence interval estimate of the true weight. Based on the answer that was given, we first find the sample mean, which is just: $(3.142+3.163+3.155+3.150+3.141)/5.$ Then, we find the standard deviation, which is where I get some confusion. We're supposed to use the ""other"" standard deviation, but I don't know what the differences would be. The apparent ""new"" standard deviation would be: std deviation = $(1/\sqrt5) * .1$ Then, we multiply that by plus or minus 1.96. However, I am completely confused as to how 1.96 is obtained. There is a table we can consult, but it seems that there are different tables depending on whether or not the confidence interval is two-sided - in which case, how would we know if the CI is two-sided? I hope I'm being clear enough here, a slow explanation would be very helpful.","My question is almost exactly the same as another one here on math stack exchange, but it isn't as explanatory as I'd like it to be with some parts. I was unsure of whether or not it'd be ""okay"" to repost the question, especially since the other one was asked about three years ago. Question is: An electric scale gives a reading equal to the true weight plus a random error that is normally distributed with mean 0 mg and standard deviation = 0.1 mg. Suppose that the results of five successive weighings (in mg) of the same object are as follows:     3.142, 3.163, 3.155, 3.150, 3.141. a) Compute a 95 percent confidence interval estimate of the true weight.  b) Compute a 99 percent confidence interval estimate of the true weight. Based on the answer that was given, we first find the sample mean, which is just: $(3.142+3.163+3.155+3.150+3.141)/5.$ Then, we find the standard deviation, which is where I get some confusion. We're supposed to use the ""other"" standard deviation, but I don't know what the differences would be. The apparent ""new"" standard deviation would be: std deviation = $(1/\sqrt5) * .1$ Then, we multiply that by plus or minus 1.96. However, I am completely confused as to how 1.96 is obtained. There is a table we can consult, but it seems that there are different tables depending on whether or not the confidence interval is two-sided - in which case, how would we know if the CI is two-sided? I hope I'm being clear enough here, a slow explanation would be very helpful.",,['statistics']
54,Chocolatier sampler boxes problem: applying goal programming and mixed-integer programing to optimally compromise goals.,Chocolatier sampler boxes problem: applying goal programming and mixed-integer programing to optimally compromise goals.,,"QUESTION: A boutique chocolatier is planning to make a number of sampler boxes,   each containing $36$ chocolates.  (Therefore the total number of   chocolates should be divisible by $36$.)  The chocolatier would like   to achieve the following goals: include at least $300$ Hazelnut Glories include at least as many Vanilla Surrenders as Hazelnut Glories, include at least $240$ Caramel Desires, include at least $400$ Dark Mysteries, produce no more than $27$ sampler boxes of chocolates overall. Assume the goals are equally important. (a) Apply goal programming methods to formulate a mixed-integer   program to find the optimal compromise between these goals. (b) Solve your mixed-integer program in Excel and interpret the   solution.  Your answer in your main report should include the IP   formulation, the optimal solution, and an interpretation of the   optimal solution for the boutique chocolatier.  You should also submit   the Excel file you used to solve the IP. Can someone help me figuring out how to calculate this question? In this question, I have 4 variables (I think it would be more easily to calculate), and 5 goals. SO for the 27 boxes, I multiply the number of 36 = 972. Then, it subject to 5 constraints, and 4 variables are integers and all decision values is large than or equal to 0. Then when I enter the question into Excel Solver, I got two different answers and both are quite reasonable. The first answer is equal to: 300, 32, 240, 400. And for the second answer is equal to: 32, 300, 240, 400. The only different is that for the first answer, I have click the box of all variables are non-negative. SO, I don't know which answer is the best or actually i do it wrong?? Thanks for your help!!!","QUESTION: A boutique chocolatier is planning to make a number of sampler boxes,   each containing $36$ chocolates.  (Therefore the total number of   chocolates should be divisible by $36$.)  The chocolatier would like   to achieve the following goals: include at least $300$ Hazelnut Glories include at least as many Vanilla Surrenders as Hazelnut Glories, include at least $240$ Caramel Desires, include at least $400$ Dark Mysteries, produce no more than $27$ sampler boxes of chocolates overall. Assume the goals are equally important. (a) Apply goal programming methods to formulate a mixed-integer   program to find the optimal compromise between these goals. (b) Solve your mixed-integer program in Excel and interpret the   solution.  Your answer in your main report should include the IP   formulation, the optimal solution, and an interpretation of the   optimal solution for the boutique chocolatier.  You should also submit   the Excel file you used to solve the IP. Can someone help me figuring out how to calculate this question? In this question, I have 4 variables (I think it would be more easily to calculate), and 5 goals. SO for the 27 boxes, I multiply the number of 36 = 972. Then, it subject to 5 constraints, and 4 variables are integers and all decision values is large than or equal to 0. Then when I enter the question into Excel Solver, I got two different answers and both are quite reasonable. The first answer is equal to: 300, 32, 240, 400. And for the second answer is equal to: 32, 300, 240, 400. The only different is that for the first answer, I have click the box of all variables are non-negative. SO, I don't know which answer is the best or actually i do it wrong?? Thanks for your help!!!",,"['statistics', 'linear-programming', 'integer-programming']"
55,Finding yearly weather statistics from tomorrow's weather probability,Finding yearly weather statistics from tomorrow's weather probability,,"I'm trying to solve this problem from a book, but so far I haven't found how to approach it... I made a graph, and tried to calculate some probabilities..  but nothing What should I do? Thanks!","I'm trying to solve this problem from a book, but so far I haven't found how to approach it... I made a graph, and tried to calculate some probabilities..  but nothing What should I do? Thanks!",,"['probability', 'statistics']"
56,On random functions taking values in the space of continous functions.,On random functions taking values in the space of continous functions.,,"Here is the passage that is unclear to me ( Theoretical statistics by Keener): In this section we develop a weak law of large numbers for averages of random functions. This is used in the rest of the chapter to establish consistency and asymptotic normality of maximum likelihood and other estimators. Let $X_1, X_2, \dots$ be i.i.d., let $K$ be a compact set in $\mathbb{R}^p$ , and define $$ W_i(t) = h(t, X_i) \,, \quad t \in K$$ where $h(t,x)$ is a continuous function of $t$ for all $x$ . Then $W_1, W_2, \dots$ are i.i.d. random functions taking values in $C(K)$ , the space of continuous functions on $K$ . I do not understand how $W_1, W_2, \dots$ take values in $C(K)$ . For every different realization of the random variable they take as value a continuous  function? So $W_i(t)$ does not only depend on $t$ and $i$ but also on the realised event on the sample space? Probably I realise I have never fully understood what the definition $W_i(t) = h(t,X_i)$ really means, could somebody guide me through it?","Here is the passage that is unclear to me ( Theoretical statistics by Keener): In this section we develop a weak law of large numbers for averages of random functions. This is used in the rest of the chapter to establish consistency and asymptotic normality of maximum likelihood and other estimators. Let be i.i.d., let be a compact set in , and define where is a continuous function of for all . Then are i.i.d. random functions taking values in , the space of continuous functions on . I do not understand how take values in . For every different realization of the random variable they take as value a continuous  function? So does not only depend on and but also on the realised event on the sample space? Probably I realise I have never fully understood what the definition really means, could somebody guide me through it?","X_1, X_2, \dots K \mathbb{R}^p  W_i(t) = h(t, X_i) \,, \quad t \in K h(t,x) t x W_1, W_2, \dots C(K) K W_1, W_2, \dots C(K) W_i(t) t i W_i(t) = h(t,X_i)","['probability-theory', 'statistics', 'stochastic-processes', 'random-variables', 'definition']"
57,Linear or Non-Linear Model,Linear or Non-Linear Model,,"I have the following regression equation \begin{align*} y_i = \alpha + \gamma\cdot\beta\cdot x_i+ \varepsilon_i, \end{align*} where $y_i$, $x_i$ and $\varepsilon_i$ are $n\times 1$ vectors, $\varepsilon_i\stackrel{iid}{\sim}\text{N}(0,\sigma^2)$ for all $i=1,\ldots,n$ and $\alpha$, $\beta$ and $\gamma$ are unkown constants. Is this a linear regression model in the unkown parameters $\alpha$, $\beta$ and $\gamma$ ?","I have the following regression equation \begin{align*} y_i = \alpha + \gamma\cdot\beta\cdot x_i+ \varepsilon_i, \end{align*} where $y_i$, $x_i$ and $\varepsilon_i$ are $n\times 1$ vectors, $\varepsilon_i\stackrel{iid}{\sim}\text{N}(0,\sigma^2)$ for all $i=1,\ldots,n$ and $\alpha$, $\beta$ and $\gamma$ are unkown constants. Is this a linear regression model in the unkown parameters $\alpha$, $\beta$ and $\gamma$ ?",,"['statistics', 'regression']"
58,Conditional expectation and rao-blacwell,Conditional expectation and rao-blacwell,,"I am studying on UMVUE, and I'm struggling to find that conditional expectation Let $X_1,\ldots,X_n$ random sample of $X\sim U[0,\theta]$. i) Show that $2X_1$ is a unbiased estimator for $\theta$ and use the Rao-BlackWell Theorem for found the UMVUE for $\theta$. ii)Calculate $E[X_{(n)}]$ and explicitly find UMVUE for $\theta$ I already show that $2X_1$ is unbiased and also found that $X_{(n)}=\max(X_1,\ldots,X_n)$ is a complete and sufficient statistic for $\theta$, but I am having trouble finding the conditional, how I can calulate $$E[2X_1\mid X_{(n)}]=2E[X_1\mid X_{(n)}]$$ How do I calculate the conditional distribution and the expectation?","I am studying on UMVUE, and I'm struggling to find that conditional expectation Let $X_1,\ldots,X_n$ random sample of $X\sim U[0,\theta]$. i) Show that $2X_1$ is a unbiased estimator for $\theta$ and use the Rao-BlackWell Theorem for found the UMVUE for $\theta$. ii)Calculate $E[X_{(n)}]$ and explicitly find UMVUE for $\theta$ I already show that $2X_1$ is unbiased and also found that $X_{(n)}=\max(X_1,\ldots,X_n)$ is a complete and sufficient statistic for $\theta$, but I am having trouble finding the conditional, how I can calulate $$E[2X_1\mid X_{(n)}]=2E[X_1\mid X_{(n)}]$$ How do I calculate the conditional distribution and the expectation?",,"['statistics', 'statistical-inference', 'estimation']"
59,Piece-wise probability density and cumulative distribution function exercise,Piece-wise probability density and cumulative distribution function exercise,,"Given a random variable $X$ with the density function: $f(x) = a$ if $0 \leq x \leq b$      and $f(x) = b$ if $b < x < a + b$ I want to solve the following exercises regarding this distribution: a) How large is $a$ if $b = 3a$? b) $a = 0.5, b = 1$, then: i) What is $P(X>1)$? ii) What is $P(0.8<X<1.2)$ iii) What is $x_1$ s.t. $ P(X<x_1) = 0.05$? c) Calculate $F(x)$ My ideas: a) The area under the function has to sum up to $1$, hence I assume that $6a^2 = 1$ and hence $a = \sqrt{1/6}$ — is that correct? b) i) $[\frac{1}{2}x^2]_1^{1.5}$ ii) $ 0.5 * [\frac{1}{2}x^2]_{0.8}^{1} + [\frac{1}{2}x^2]_1^{1.2}$ iii) My idea would to solve for b in $\int_{0}^b f(x) x dx = 0.5 $ — but how exactly would I formulate $f(x)$? c) I would assume we end up with two $F(x)$, each one being the integral of the respective $f(x)$ and hence $0.5x$ and $x$ respectively, is that correct? Thanks","Given a random variable $X$ with the density function: $f(x) = a$ if $0 \leq x \leq b$      and $f(x) = b$ if $b < x < a + b$ I want to solve the following exercises regarding this distribution: a) How large is $a$ if $b = 3a$? b) $a = 0.5, b = 1$, then: i) What is $P(X>1)$? ii) What is $P(0.8<X<1.2)$ iii) What is $x_1$ s.t. $ P(X<x_1) = 0.05$? c) Calculate $F(x)$ My ideas: a) The area under the function has to sum up to $1$, hence I assume that $6a^2 = 1$ and hence $a = \sqrt{1/6}$ — is that correct? b) i) $[\frac{1}{2}x^2]_1^{1.5}$ ii) $ 0.5 * [\frac{1}{2}x^2]_{0.8}^{1} + [\frac{1}{2}x^2]_1^{1.2}$ iii) My idea would to solve for b in $\int_{0}^b f(x) x dx = 0.5 $ — but how exactly would I formulate $f(x)$? c) I would assume we end up with two $F(x)$, each one being the integral of the respective $f(x)$ and hence $0.5x$ and $x$ respectively, is that correct? Thanks",,"['probability', 'statistics']"
60,Prove a result in multiple linear regression,Prove a result in multiple linear regression,,"This arises in multiple linear regression. Given $m, n \in \mathbb{N}$ and matrices $X \in \mathbb{R}^{m \times (n+1)} (m > n + 1), H = X(X'X)^{-1}X' \in \mathbb{R}^{m\times m}, I = I_m$ and $J \in \mathbb{R}^{m\times m}$, a matrix of $1$'s, how does one show that $$J(I-H) = 0\text{ ?}$$ I am actually trying to show $$(I-H)(H-\frac{1}{n}J) = 0$$ which reduces to $J(I-H) = 0$ by noting that $HH =H$. Apparently, this might have something to do with $J(y - X \hat{\beta}) = 0 \ \forall y \in \mathbb{R}^{1\times(m+1)}$ where  $\hat{\beta} = (X'X)^{-1}X'y $? Is the statement true? If so why? If not, why not, and how then to prove $J(I-H) = 0$ or $(I-H)(H-\frac{1}{n}J) = 0$? I think there's some property of MLR needed. On the other hand, the equation $J(I-H) = 0$ itself looks like it is independent of statistics. Must one really use some properties of MLR? What are some possible counterexamples?","This arises in multiple linear regression. Given $m, n \in \mathbb{N}$ and matrices $X \in \mathbb{R}^{m \times (n+1)} (m > n + 1), H = X(X'X)^{-1}X' \in \mathbb{R}^{m\times m}, I = I_m$ and $J \in \mathbb{R}^{m\times m}$, a matrix of $1$'s, how does one show that $$J(I-H) = 0\text{ ?}$$ I am actually trying to show $$(I-H)(H-\frac{1}{n}J) = 0$$ which reduces to $J(I-H) = 0$ by noting that $HH =H$. Apparently, this might have something to do with $J(y - X \hat{\beta}) = 0 \ \forall y \in \mathbb{R}^{1\times(m+1)}$ where  $\hat{\beta} = (X'X)^{-1}X'y $? Is the statement true? If so why? If not, why not, and how then to prove $J(I-H) = 0$ or $(I-H)(H-\frac{1}{n}J) = 0$? I think there's some property of MLR needed. On the other hand, the equation $J(I-H) = 0$ itself looks like it is independent of statistics. Must one really use some properties of MLR? What are some possible counterexamples?",,"['linear-algebra', 'statistics', 'regression']"
61,Sampling distribution of $\frac{\bar{X}}{S}$,Sampling distribution of,\frac{\bar{X}}{S},"Suppose that I have a random sample $X_1, … ,X_n$ from a $N(0,\sigma^2)$ distribution. What is the distribution of $$\frac{\bar{X}}{S}$$ and what is it's standard deviation? Here $\bar{X}$ is the sample mean and $S$ is the sample standard deviation.","Suppose that I have a random sample $X_1, … ,X_n$ from a $N(0,\sigma^2)$ distribution. What is the distribution of $$\frac{\bar{X}}{S}$$ and what is it's standard deviation? Here $\bar{X}$ is the sample mean and $S$ is the sample standard deviation.",,"['statistics', 'probability-distributions', 'sampling']"
62,Probability of Bit Errors Poisson Question,Probability of Bit Errors Poisson Question,,"I'm not quite sure how to get the correct probability for this question. Q: The probability of error in the transmission of a binary digit over a communication channel is 1/10^3. Write an expression for the exact probability of more than 3 errors when transmitting a block of 10^3 bits. What is its approximate value? Assume independence. Since it's approximate, I'm using Poisson's distribution formula. The expected value (for λ) is 1, I believe. K would be 3. However, by plugging these values in and obtaining the result, I'm only getting the answer for exactly 3 errors. I need to get the probability of receiving more than three errors. Initially, I thought it would be: 1 - P(0 errors) - P(1 error) - P(2 errors) - P(3 errors) However, I'm not sure if this is correct or if there's a possibility of overlap. Can anyone help in what the probability should be or if I'm going about this problem in the correct way?","I'm not quite sure how to get the correct probability for this question. Q: The probability of error in the transmission of a binary digit over a communication channel is 1/10^3. Write an expression for the exact probability of more than 3 errors when transmitting a block of 10^3 bits. What is its approximate value? Assume independence. Since it's approximate, I'm using Poisson's distribution formula. The expected value (for λ) is 1, I believe. K would be 3. However, by plugging these values in and obtaining the result, I'm only getting the answer for exactly 3 errors. I need to get the probability of receiving more than three errors. Initially, I thought it would be: 1 - P(0 errors) - P(1 error) - P(2 errors) - P(3 errors) However, I'm not sure if this is correct or if there's a possibility of overlap. Can anyone help in what the probability should be or if I'm going about this problem in the correct way?",,"['probability', 'statistics']"
63,Completion with respect a collection of measures.,Completion with respect a collection of measures.,,"Given a set of measures $\mathcal{M}$ on a measurable space $(\Omega,\mathscr{F})$, the $\mathcal{M}$-completion of $\mathscr{F}$ is defined as $$ \overline{\mathscr{F}}^\mathcal{M}:=\bigcap_{\mu\in\mathcal{M}}\mathscr{F}^\mu,$$ where $\mathscr{F}^\mu$ is the completion of $\mathscr{F}$ with respect to the measure $\mu$. Let $\mathscr{N}_\mathcal{M}$ denote the set of all $\mathcal{M}$-null sets, that is, the collection of all sets $A\subset\Omega$ such that $\mu^*(A)=0$ for all $\mu\in\mathcal{M}$. It is clear that $$\sigma(\mathscr{F},\mathscr{N}_\mathcal{M})\subset\overline{\mathscr{F}}^\mathcal{M}.$$  The question is whether both are actually the same. I think not, but can't think of a counter example at the moment. The relevance of the question has to do with the notion of sufficient $\sigma$--algebra and minimal sufficient $\sigma$--algebras in statistics.","Given a set of measures $\mathcal{M}$ on a measurable space $(\Omega,\mathscr{F})$, the $\mathcal{M}$-completion of $\mathscr{F}$ is defined as $$ \overline{\mathscr{F}}^\mathcal{M}:=\bigcap_{\mu\in\mathcal{M}}\mathscr{F}^\mu,$$ where $\mathscr{F}^\mu$ is the completion of $\mathscr{F}$ with respect to the measure $\mu$. Let $\mathscr{N}_\mathcal{M}$ denote the set of all $\mathcal{M}$-null sets, that is, the collection of all sets $A\subset\Omega$ such that $\mu^*(A)=0$ for all $\mu\in\mathcal{M}$. It is clear that $$\sigma(\mathscr{F},\mathscr{N}_\mathcal{M})\subset\overline{\mathscr{F}}^\mathcal{M}.$$  The question is whether both are actually the same. I think not, but can't think of a counter example at the moment. The relevance of the question has to do with the notion of sufficient $\sigma$--algebra and minimal sufficient $\sigma$--algebras in statistics.",,"['measure-theory', 'statistics']"
64,Probability of particular subset of balls occurring in a larger set chosen from a total?,Probability of particular subset of balls occurring in a larger set chosen from a total?,,"If I have 3 balls chosen out of 6 (2 blue, 2 red, and 2 green) with replacement, what is the chance a chosen ball will be blue? How about two blue balls? I think I can reason this out, but I'm curious about the approach using permutations and combinations. So far, I'm thinking I have 6 balls in total, so all possible permutations would be 6! = 720, and then all permutations of 3 would be 6!/3! = 120, but this is where I'm stuck (and end up thinking in circles too hard, likely making an easy problem into something way more difficult). I'm thinking for one blue ball, we have a probability of 1/3 of choosing one each time, so for three choices this is still just 1/3...but it doesn't feel right. Any suggestions on how to tackle this?","If I have 3 balls chosen out of 6 (2 blue, 2 red, and 2 green) with replacement, what is the chance a chosen ball will be blue? How about two blue balls? I think I can reason this out, but I'm curious about the approach using permutations and combinations. So far, I'm thinking I have 6 balls in total, so all possible permutations would be 6! = 720, and then all permutations of 3 would be 6!/3! = 120, but this is where I'm stuck (and end up thinking in circles too hard, likely making an easy problem into something way more difficult). I'm thinking for one blue ball, we have a probability of 1/3 of choosing one each time, so for three choices this is still just 1/3...but it doesn't feel right. Any suggestions on how to tackle this?",,"['probability', 'statistics', 'probability-theory']"
65,General probability equation,General probability equation,,"I am really struggling with statistics. I am trying to come up with the basic equation/concept of calculating the probability for something. for example:  I know that the probability of coming up with exactly 3 heads in 4 flips of a coin is 1/4, but I don't know why (other than writing out all possibilities- but I can't apply that to large numbers). heres what I know so far:  Each flip has a 1/2 Chance of coming up heads, meaning you have two possibilities with each flip.  I know then that the number of total possible outcomes for this particular example is 2^4 (possibilities ^ # of tries). I just don't know how to apply that so I can come up with the general equation for the probability of something. another example: rolling a die 2x and having it come up with even numbers both times.","I am really struggling with statistics. I am trying to come up with the basic equation/concept of calculating the probability for something. for example:  I know that the probability of coming up with exactly 3 heads in 4 flips of a coin is 1/4, but I don't know why (other than writing out all possibilities- but I can't apply that to large numbers). heres what I know so far:  Each flip has a 1/2 Chance of coming up heads, meaning you have two possibilities with each flip.  I know then that the number of total possible outcomes for this particular example is 2^4 (possibilities ^ # of tries). I just don't know how to apply that so I can come up with the general equation for the probability of something. another example: rolling a die 2x and having it come up with even numbers both times.",,['statistics']
66,Order statistics when variables have different distributions,Order statistics when variables have different distributions,,"Let a,b and c be random variables, where a~U[0,8] and b~U[3,8]. Let c=max{a,b}, What is the mean of c? In general, let $(x_1, ..., x_n)$ be independently distributed in different supports. What is the distribution of its maximum, i.e. of its highest order statistic? I have the solution for my first question (from simulating it in Matlab) but I am looking for the general expression. I encountered the Bapat-Beg Theorem but I am convinced there is something simpler I can use. Intuitive explanations are very welcomed.","Let a,b and c be random variables, where a~U[0,8] and b~U[3,8]. Let c=max{a,b}, What is the mean of c? In general, let $(x_1, ..., x_n)$ be independently distributed in different supports. What is the distribution of its maximum, i.e. of its highest order statistic? I have the solution for my first question (from simulating it in Matlab) but I am looking for the general expression. I encountered the Bapat-Beg Theorem but I am convinced there is something simpler I can use. Intuitive explanations are very welcomed.",,"['statistics', 'probability-theory', 'order-statistics']"
67,Set of pairs of options that could be wrong/right,Set of pairs of options that could be wrong/right,,"One has a list of n options out of which 2 are incorrect, and guesses can be made by picking a pair of options. After picking a pair as a guess, it is either valid, in which case both of the pair's options are incorrect, or not, in which case one or both are a correct option. How would making a invalid guess change the probability of other pairs being valid?","One has a list of n options out of which 2 are incorrect, and guesses can be made by picking a pair of options. After picking a pair as a guess, it is either valid, in which case both of the pair's options are incorrect, or not, in which case one or both are a correct option. How would making a invalid guess change the probability of other pairs being valid?",,"['combinatorics', 'statistics', 'combinations']"
68,expectation calculation problem,expectation calculation problem,,"I got the answers for this and i know its 1.05 but the way it explains is very difficult to understand so im seeking for some help here. A system made up of 7 components with independent, identically distributed lifetimes will operate until any of 1 of the system's components fails. If the life time X of each component has density function $f(x) = \begin{cases} 3/x^4,  & \text{for 1<x}\\ 0, & \text{otherwise} \end{cases}$ what is the expected lifetime until failure of the system? I tried to find the intersect of 7 components by integrating and power it by 7 but it doesnt give me anything useful...","I got the answers for this and i know its 1.05 but the way it explains is very difficult to understand so im seeking for some help here. A system made up of 7 components with independent, identically distributed lifetimes will operate until any of 1 of the system's components fails. If the life time X of each component has density function $f(x) = \begin{cases} 3/x^4,  & \text{for 1<x}\\ 0, & \text{otherwise} \end{cases}$ what is the expected lifetime until failure of the system? I tried to find the intersect of 7 components by integrating and power it by 7 but it doesnt give me anything useful...",,"['probability', 'statistics', 'probability-theory']"
69,Book Suggestion for Statistics,Book Suggestion for Statistics,,"I recently finished my Masters in Mathematics, more incline to analysis and algebra. I do not know if I should blame my school for laying a poor foundation in statistics or I should blame myself for avoiding it completely all the while till date (I have not done any course work on it). I am absolutely zero in Statistics, and presently for a competitive exam I am suppose to write in two months (CSIR-UGC NET Examination), $1/4^{th}$ of the questions are from Stats (Other 3/4th portion includes analysis, algebra and differential equations). Can someone suggest a good book which like ""Schaum's Outline for Linear Algebra"" (I found it a good self contained book, with bare minimum facts required for survival for newbies, and I use it to revise what I studied in past) for statistics? I came to know about ""Schaum's Outline of Statistics"" and it is priced very high here in India, and I don't know if it is worth the money. There are other Schaum's Outline books as well for statistics but I do not know which one to buy. I did go through similar posts and checked out the suggestions there, but they are proper complete text books. Can someone suggest some good book(s) for statistics, starting from zero, concise with material up to Masters (or at least undergraduate) syllabus? Thanks in advance.","I recently finished my Masters in Mathematics, more incline to analysis and algebra. I do not know if I should blame my school for laying a poor foundation in statistics or I should blame myself for avoiding it completely all the while till date (I have not done any course work on it). I am absolutely zero in Statistics, and presently for a competitive exam I am suppose to write in two months (CSIR-UGC NET Examination), $1/4^{th}$ of the questions are from Stats (Other 3/4th portion includes analysis, algebra and differential equations). Can someone suggest a good book which like ""Schaum's Outline for Linear Algebra"" (I found it a good self contained book, with bare minimum facts required for survival for newbies, and I use it to revise what I studied in past) for statistics? I came to know about ""Schaum's Outline of Statistics"" and it is priced very high here in India, and I don't know if it is worth the money. There are other Schaum's Outline books as well for statistics but I do not know which one to buy. I did go through similar posts and checked out the suggestions there, but they are proper complete text books. Can someone suggest some good book(s) for statistics, starting from zero, concise with material up to Masters (or at least undergraduate) syllabus? Thanks in advance.",,"['statistics', 'book-recommendation']"
70,Degree of freedom,Degree of freedom,,"I have learnt from solving different statistical problems that if we have a sample of size $n$ picked from the population with normal distribution, the degree of freedom for, say, Student distribution or Chi-square distribution distribution will be always $n-1$. Recently i have discovered the following problem: Let ${X_1},{X_2}, \cdots ,{X_{10}}$ be independent   $N(0,1)$-distributed (standard normal distribution) random variables.   Define $$X = X_1^2 + X_2^2 +  \cdots  + X_{10}^2$$ What is the   distribution, expectation and variance of $X$? I understand that this is a Chi-square distribution. What i don't get is why the degree of freedom is $10$ instead of $9$? If $X$ is a sum of $10$ squared $RV$ shouldn't we treat it as a sample with size $n=10$ and hence $X \sim {\chi}^2(n - 1)$?","I have learnt from solving different statistical problems that if we have a sample of size $n$ picked from the population with normal distribution, the degree of freedom for, say, Student distribution or Chi-square distribution distribution will be always $n-1$. Recently i have discovered the following problem: Let ${X_1},{X_2}, \cdots ,{X_{10}}$ be independent   $N(0,1)$-distributed (standard normal distribution) random variables.   Define $$X = X_1^2 + X_2^2 +  \cdots  + X_{10}^2$$ What is the   distribution, expectation and variance of $X$? I understand that this is a Chi-square distribution. What i don't get is why the degree of freedom is $10$ instead of $9$? If $X$ is a sum of $10$ squared $RV$ shouldn't we treat it as a sample with size $n=10$ and hence $X \sim {\chi}^2(n - 1)$?",,['statistics']
71,Showing $s^2=\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})^2=\frac{1}{n-1}\left [\sum_{i=1}^n x_i^2-\frac{1}{n}\left ( \sum_1^nx_i\right )^2 \right ]$ [duplicate],Showing  [duplicate],s^2=\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})^2=\frac{1}{n-1}\left [\sum_{i=1}^n x_i^2-\frac{1}{n}\left ( \sum_1^nx_i\right )^2 \right ],This question already has answers here : Sample variance derivation (2 answers) Closed 9 years ago . I've got as far as this  $$s^2=\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})^2=\frac{1}{n-1}\sum_{i=1}^n\left ( x_i^2-2x_i\bar{x}+\bar{x}^2\right )$$ $$=\frac{1}{n-1}\left [\sum_{i=1}^nx_i^2+\bar{x}^2-2\bar{x}\sum_{i=1}^n x_i \right ]$$ $$\frac{1}{n-1}\left [\sum_{i=1}^nx_i^2 +\left ( \bar{x}-\sum_{i=1}^n x_i\right )^2 -\left (\sum_{i=1}^n x_i \right  )^2\right ]$$ And now I'm stuck. I can use $$\sum_{i=1}^n\frac{x_i}{n}=\bar{x}$$ However can't get the result. If someone could point me in the right direction I'd be grateful.,This question already has answers here : Sample variance derivation (2 answers) Closed 9 years ago . I've got as far as this  $$s^2=\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})^2=\frac{1}{n-1}\sum_{i=1}^n\left ( x_i^2-2x_i\bar{x}+\bar{x}^2\right )$$ $$=\frac{1}{n-1}\left [\sum_{i=1}^nx_i^2+\bar{x}^2-2\bar{x}\sum_{i=1}^n x_i \right ]$$ $$\frac{1}{n-1}\left [\sum_{i=1}^nx_i^2 +\left ( \bar{x}-\sum_{i=1}^n x_i\right )^2 -\left (\sum_{i=1}^n x_i \right  )^2\right ]$$ And now I'm stuck. I can use $$\sum_{i=1}^n\frac{x_i}{n}=\bar{x}$$ However can't get the result. If someone could point me in the right direction I'd be grateful.,,"['statistics', 'standard-deviation']"
72,"if $w$ is a normal distribution where $n(0,1)$, then find the mgf of $w^2$.","if  is a normal distribution where , then find the mgf of .","w n(0,1) w^2","if $w$ is a normal distribution where $n(0,1)$, then find the mgf of $w^2$. I have looked it up and the answer is chi squared but i cannot seem to find a way to integrate this correctly. I start the problem by generalizing it as  $e^{(tx)}* f(x^2)= e^{(tx)}*[1/\sqrt{2\pi } * e^{-(x^2/2)}]^2$ Am I starting this problem of incorrectly ?","if $w$ is a normal distribution where $n(0,1)$, then find the mgf of $w^2$. I have looked it up and the answer is chi squared but i cannot seem to find a way to integrate this correctly. I start the problem by generalizing it as  $e^{(tx)}* f(x^2)= e^{(tx)}*[1/\sqrt{2\pi } * e^{-(x^2/2)}]^2$ Am I starting this problem of incorrectly ?",,"['calculus', 'statistics']"
73,"Simple calculations of mean, standard deviation, and probability","Simple calculations of mean, standard deviation, and probability",,"You are a successful entrepreneur that has developed a new sustainable product that is manufactured through a standard production process. As part of this process, the product goes through quality assurance testing. The quality assurance inspector selects 10 units of your product at random from the production batch; if at least 3 units are defective, then the batch is rejected and will not be sold in stores. Previous records show that 10% of the products in each batch have issues. What is the mean number of defective units in the sample? What is the standard deviation of defective units in the sample? What is the probability that the batch will be rejected? I'm having a little trouble calculating the mean and standard deviation. However, what I have got so far was that the mean is 1, standard deviation is 3, then the probability would be 0.2514. I'm very unsure on my answer and would like to see if anyone can point out anything I'm doing wrong. Thanks a lot! :)","You are a successful entrepreneur that has developed a new sustainable product that is manufactured through a standard production process. As part of this process, the product goes through quality assurance testing. The quality assurance inspector selects 10 units of your product at random from the production batch; if at least 3 units are defective, then the batch is rejected and will not be sold in stores. Previous records show that 10% of the products in each batch have issues. What is the mean number of defective units in the sample? What is the standard deviation of defective units in the sample? What is the probability that the batch will be rejected? I'm having a little trouble calculating the mean and standard deviation. However, what I have got so far was that the mean is 1, standard deviation is 3, then the probability would be 0.2514. I'm very unsure on my answer and would like to see if anyone can point out anything I'm doing wrong. Thanks a lot! :)",,"['probability', 'statistics', 'normal-distribution', 'standard-deviation']"
74,Clarifying the importance of the quantile function in probability theory,Clarifying the importance of the quantile function in probability theory,,"I want to cement my understanding of the quantile function in probability theory and here is the way I understand it. (1) We start off with some probability space $(\mathbb R, B = \sigma(\mathbb R),\mathbb P)$. (2)Define the CDF of $\mathbb P$ as $F_{\mathbb P}=\mathbb P((-\infty,c])~\forall c \in \mathbb R$ (3) Let $X(u) = inf\{c \in \mathbb R : F_{\mathbb P}(c) \ge u    \} ~~\forall u \in (0,1)$ Then, by the following lemma: Every probability meausure on $(\mathbb R, B)$ is already uniquely determined by some CDF function, X is a random variable (4) $X:(0,1) \to \mathbb R$ (5) New Probability space $( (0,1), B (0,1)=\sigma((0,1)), \mathscr U_{(0,1)})$ where $\mathscr U$ is the uniform probability measure. (6)By definition of a random variable, $A \ \in B(\mathbb R)~\implies X^{-1}(A) \in B((0,1) )$. I hope this logic makes sense, now the importance of this is, as I understand it, is that for any probability measusre, we are able to find a random variable from the uniform probability space back to the original space. And this is very useful, for example in monte carlo simulation if we are trying to simulate from some distribution, and we are not sure how, we can simulate a uniform and plug it into the quantile? Any intuition is appreciated!","I want to cement my understanding of the quantile function in probability theory and here is the way I understand it. (1) We start off with some probability space $(\mathbb R, B = \sigma(\mathbb R),\mathbb P)$. (2)Define the CDF of $\mathbb P$ as $F_{\mathbb P}=\mathbb P((-\infty,c])~\forall c \in \mathbb R$ (3) Let $X(u) = inf\{c \in \mathbb R : F_{\mathbb P}(c) \ge u    \} ~~\forall u \in (0,1)$ Then, by the following lemma: Every probability meausure on $(\mathbb R, B)$ is already uniquely determined by some CDF function, X is a random variable (4) $X:(0,1) \to \mathbb R$ (5) New Probability space $( (0,1), B (0,1)=\sigma((0,1)), \mathscr U_{(0,1)})$ where $\mathscr U$ is the uniform probability measure. (6)By definition of a random variable, $A \ \in B(\mathbb R)~\implies X^{-1}(A) \in B((0,1) )$. I hope this logic makes sense, now the importance of this is, as I understand it, is that for any probability measusre, we are able to find a random variable from the uniform probability space back to the original space. And this is very useful, for example in monte carlo simulation if we are trying to simulate from some distribution, and we are not sure how, we can simulate a uniform and plug it into the quantile? Any intuition is appreciated!",,"['statistics', 'measure-theory', 'probability-theory', 'probability-distributions']"
75,Need help interpreting weird result for seemingly simple problem.,Need help interpreting weird result for seemingly simple problem.,,I am solving a problem and just can't wrap my had around the result I'm getting... Here it is: So my next step would be setting derivative equal to zero and solving for Theta... Are my calculations wrong?... I would appreciate any constructive advice.,I am solving a problem and just can't wrap my had around the result I'm getting... Here it is: So my next step would be setting derivative equal to zero and solving for Theta... Are my calculations wrong?... I would appreciate any constructive advice.,,['statistics']
76,2n points with arrangement on circle?,2n points with arrangement on circle?,,"I Read a Short Questions on Mathematics and get stuck in one challenging problem. any idea from my friends? Suppose 2n points with arrangement $P_1,Q_1,P_2,Q_2,...,P_n,Q_n$  with equidistance is distributed on the circumference of one circle. We want to connect these points with $n$ Non-crossover Hypotenuse such that: 1) each Hypotenuse joins one point $P_i$ to one point $Q_i$ and 2) each of these $2n$ points, exactly place on one Hypotenuse. if the number of ways to doing this equal to $a_n$, what is the value   of $a_5$?","I Read a Short Questions on Mathematics and get stuck in one challenging problem. any idea from my friends? Suppose 2n points with arrangement $P_1,Q_1,P_2,Q_2,...,P_n,Q_n$  with equidistance is distributed on the circumference of one circle. We want to connect these points with $n$ Non-crossover Hypotenuse such that: 1) each Hypotenuse joins one point $P_i$ to one point $Q_i$ and 2) each of these $2n$ points, exactly place on one Hypotenuse. if the number of ways to doing this equal to $a_n$, what is the value   of $a_5$?",,"['combinatorics', 'statistics', 'discrete-mathematics', 'circles', 'combinations']"
77,13 cards are drawn from a deck one after another... What the expected number of...?,13 cards are drawn from a deck one after another... What the expected number of...?,,"Suppose we have a normal deck of $52$ cards. We shuffle them well and then turn over the first $13$ cards one-by-one. If the first card is one of the four aces we say that a match has occurred; similarly, if the second card is one of the twos; the third card is one of the threes, etc.; until the $13$ th (one of the kings). What is the expected number of matches? What is the variance?","Suppose we have a normal deck of cards. We shuffle them well and then turn over the first cards one-by-one. If the first card is one of the four aces we say that a match has occurred; similarly, if the second card is one of the twos; the third card is one of the threes, etc.; until the th (one of the kings). What is the expected number of matches? What is the variance?",52 13 13,"['probability', 'combinatorics', 'statistics', 'expected-value', 'card-games']"
78,Statistics Binomial Formula,Statistics Binomial Formula,,"Jack tosses a fair coin 6 times. Then Jill tosses the same coin 9 times. Write out an expression for the chance that ""Jack gets 2 heads and Jill gets 4 heads."" I know that you use the binomial formula to get the chances for each but do you then multiply them together since it's asking for the chance of both or do you just add them?","Jack tosses a fair coin 6 times. Then Jill tosses the same coin 9 times. Write out an expression for the chance that ""Jack gets 2 heads and Jill gets 4 heads."" I know that you use the binomial formula to get the chances for each but do you then multiply them together since it's asking for the chance of both or do you just add them?",,['statistics']
79,Covariance and Independence problem,Covariance and Independence problem,,"The Random Variables $X$ and $Y$ can each take on only two values. Show that if $Cov(X,Y)=0$, then $X$ and $Y$ are independent. One can see that the distributions take the form: $P(X=x_1)=p$ and $P(X=x_2)=1-p$ $P(Y=y_1)=q$ and $P(Y=y_1)=1-q$ To show independence one must show that, $P(X=x_i,Y=y_j)=P(X=x_i)P(Y=y_j)$ $\forall i,j$ in {$1,2$}. As the $Cov(X,Y)=0$, then $E[XY]=E[X]E[Y]$, $(1)$ so obviously this can be used to demonstrate the independence. Do I just need to slot in for $(1)$ and investigate? And the above distributions are clearly binomial in nature where $n=1$. Any ideas on how I would continue?","The Random Variables $X$ and $Y$ can each take on only two values. Show that if $Cov(X,Y)=0$, then $X$ and $Y$ are independent. One can see that the distributions take the form: $P(X=x_1)=p$ and $P(X=x_2)=1-p$ $P(Y=y_1)=q$ and $P(Y=y_1)=1-q$ To show independence one must show that, $P(X=x_i,Y=y_j)=P(X=x_i)P(Y=y_j)$ $\forall i,j$ in {$1,2$}. As the $Cov(X,Y)=0$, then $E[XY]=E[X]E[Y]$, $(1)$ so obviously this can be used to demonstrate the independence. Do I just need to slot in for $(1)$ and investigate? And the above distributions are clearly binomial in nature where $n=1$. Any ideas on how I would continue?",,"['statistics', 'probability-theory', 'covariance', 'descriptive-statistics']"
80,Urn containg tickets problem,Urn containg tickets problem,,"From an urn containing n tickets numbered $1, 2, \dots, n$, $r$ tickets are drawn simultaneously and arranged in increasing order of their numbers $x_1<x_2<\dots<x_r$. Show that the probability that $x_i=s$ is $$\frac{{s-1 \choose i-1}{n-s \choose r-i}}{{n \choose r}}$$ My attempt From n tickets, r tickets can be drawn in ${n \choose r}$ ways. From the question, I understand that after drawing the tickets they should be arrangers in increasing order and so their  positions cannot be interchanged. Now I am unable to understand the meaning probability that $x_i=s$ . Please explain the meaning of the question and help me to solve. I hope someone can help. Thanks.","From an urn containing n tickets numbered $1, 2, \dots, n$, $r$ tickets are drawn simultaneously and arranged in increasing order of their numbers $x_1<x_2<\dots<x_r$. Show that the probability that $x_i=s$ is $$\frac{{s-1 \choose i-1}{n-s \choose r-i}}{{n \choose r}}$$ My attempt From n tickets, r tickets can be drawn in ${n \choose r}$ ways. From the question, I understand that after drawing the tickets they should be arrangers in increasing order and so their  positions cannot be interchanged. Now I am unable to understand the meaning probability that $x_i=s$ . Please explain the meaning of the question and help me to solve. I hope someone can help. Thanks.",,"['probability', 'combinatorics', 'statistics', 'probability-theory']"
81,Help show a statistic converges in probability given another statistic that converges in probability,Help show a statistic converges in probability given another statistic that converges in probability,,"Let $Y = (Y_1,\dots,Y_n)$ be a random sample from $N(\mu,1)$ and $\bar{Y}=\sum\limits_{i=1}^nY_i/n$ I am given that $\bar{Y}^2$ converges in probability to $\mu^2$ and now need to show that $\bar{Y}^2-\frac{1}{n}$ also converges in probability to $\mu^2$. Since  $\bar{Y}^2$ converges in probability to $\mu^2$ means: $$ \forall \delta>0 ,\forall \epsilon>0, \exists N, \text{ such that }\\\,n>N \Rightarrow P\left(\left|\bar{Y}-\mu^2\right| \ge \delta\right)<\epsilon$$ and $$\lim\limits_{n \rightarrow \infty} \frac{1}{n} = 0$$ It looks to me that the solution involves selecting the right $n$ so that $P\left(\left|\bar{Y}-\frac{1}{n}-\mu^2\right| \ge \delta \right) \le P\left(\left|\bar{Y}-\mu^2\right| \ge \delta \right)<\epsilon$ and I tried to show this can be done by selecting letting $0<\frac{1}{n}<\frac{\delta}{2}$, which was a common thing to do in the exercises involving $\epsilon-\delta$ proofs. But I am unable to prove this is true. In fact, I think this is not true because the interval in the LHS is smaller than the interval on the RHS, hence the probability should be $>$ instead of $\le$ Am I on the right track and is there an alternative method that is quicker?","Let $Y = (Y_1,\dots,Y_n)$ be a random sample from $N(\mu,1)$ and $\bar{Y}=\sum\limits_{i=1}^nY_i/n$ I am given that $\bar{Y}^2$ converges in probability to $\mu^2$ and now need to show that $\bar{Y}^2-\frac{1}{n}$ also converges in probability to $\mu^2$. Since  $\bar{Y}^2$ converges in probability to $\mu^2$ means: $$ \forall \delta>0 ,\forall \epsilon>0, \exists N, \text{ such that }\\\,n>N \Rightarrow P\left(\left|\bar{Y}-\mu^2\right| \ge \delta\right)<\epsilon$$ and $$\lim\limits_{n \rightarrow \infty} \frac{1}{n} = 0$$ It looks to me that the solution involves selecting the right $n$ so that $P\left(\left|\bar{Y}-\frac{1}{n}-\mu^2\right| \ge \delta \right) \le P\left(\left|\bar{Y}-\mu^2\right| \ge \delta \right)<\epsilon$ and I tried to show this can be done by selecting letting $0<\frac{1}{n}<\frac{\delta}{2}$, which was a common thing to do in the exercises involving $\epsilon-\delta$ proofs. But I am unable to prove this is true. In fact, I think this is not true because the interval in the LHS is smaller than the interval on the RHS, hence the probability should be $>$ instead of $\le$ Am I on the right track and is there an alternative method that is quicker?",,"['statistics', 'proof-writing', 'estimation', 'epsilon-delta']"
82,Can we predict the past?,Can we predict the past?,,"Can we use probability rules to predict the occurrence of an event which has already happened in the past or already formed? For example, hemoglobin is a protein formed of $141$ amino acids connected like a chain with specific order, the first amino acid is Leucine (we have only $20$ types of amino acids forming any protein), is it valid to say that probability of leucine being first in this chain is $\frac1{20}$ so the probability to get the $141$ a.a hemoglobin with such order by chance is $\left(\frac{1}{20}\right)^{141}?$ or this prediction makes no sense as we already have hemoglobin formed with such order in the nature?","Can we use probability rules to predict the occurrence of an event which has already happened in the past or already formed? For example, hemoglobin is a protein formed of $141$ amino acids connected like a chain with specific order, the first amino acid is Leucine (we have only $20$ types of amino acids forming any protein), is it valid to say that probability of leucine being first in this chain is $\frac1{20}$ so the probability to get the $141$ a.a hemoglobin with such order by chance is $\left(\frac{1}{20}\right)^{141}?$ or this prediction makes no sense as we already have hemoglobin formed with such order in the nature?",,"['probability', 'statistics']"
83,CDF of middle of $3$ random variables,CDF of middle of  random variables,3,"Let the independent random variables $X_1$, $X_2$, $X_3$ have the same cdf $F(X).$ Let $Y$ be the middle value (second largest) of $X_1$, $X_2$, $X_3$. Determine the cdf of  $Y$. (Hint: use a Binomial distribution). My approach is that this is a binomial distribution where I am trying to calculate the probability that $2$ of that random variables are less than or equal to some number $x$ and one is of the random variables is larger. Is this the right approach?","Let the independent random variables $X_1$, $X_2$, $X_3$ have the same cdf $F(X).$ Let $Y$ be the middle value (second largest) of $X_1$, $X_2$, $X_3$. Determine the cdf of  $Y$. (Hint: use a Binomial distribution). My approach is that this is a binomial distribution where I am trying to calculate the probability that $2$ of that random variables are less than or equal to some number $x$ and one is of the random variables is larger. Is this the right approach?",,"['statistics', 'order-statistics']"
84,Proof for Standard Deviation Formula for a Binomial Distribution,Proof for Standard Deviation Formula for a Binomial Distribution,,"I understand the concept of standard deviation as the square root of the square of the mean of each sample value - the mean of the sample values . Here is the mathematical representation (I've solved out the proof independently) : 1.) $\sigma = \sqrt{\{x^2\} - \{x\}^2}$ where $\{\,\}$ is the average and $x$ is a sample value. 2.) There is an alternate mathematical representation using summation sigma (for discrete random variable also) that more people are probably acquainted with. Or does this one have a slightly different meaning, I'm not sure? My question is, can someone explicitly show me the derivation for the standard deviation of a binomial distribution. Here is  the information I know: 1.) Final formula:  $\sigma = \sqrt{pqN}$ 2.) $p =$ probability of event A occurring AKA $p = n(A)/N$ where $A$ is an event OR the first binomially distributed random variable , $n(A)$ is the amount of times event $A$ happens, and $N$ is the total number of events 3.) $q =$ probability of event $B$ occurring AKA $p = n(B)/N$ where $B$ is an event OR the second binomially distributed random variable , $n(B)$ is the amount of times event $B$ happens, and $N$ is the total number of events. Also, $q = 1-p$ because there are only two events, $A$ and $B$.","I understand the concept of standard deviation as the square root of the square of the mean of each sample value - the mean of the sample values . Here is the mathematical representation (I've solved out the proof independently) : 1.) $\sigma = \sqrt{\{x^2\} - \{x\}^2}$ where $\{\,\}$ is the average and $x$ is a sample value. 2.) There is an alternate mathematical representation using summation sigma (for discrete random variable also) that more people are probably acquainted with. Or does this one have a slightly different meaning, I'm not sure? My question is, can someone explicitly show me the derivation for the standard deviation of a binomial distribution. Here is  the information I know: 1.) Final formula:  $\sigma = \sqrt{pqN}$ 2.) $p =$ probability of event A occurring AKA $p = n(A)/N$ where $A$ is an event OR the first binomially distributed random variable , $n(A)$ is the amount of times event $A$ happens, and $N$ is the total number of events 3.) $q =$ probability of event $B$ occurring AKA $p = n(B)/N$ where $B$ is an event OR the second binomially distributed random variable , $n(B)$ is the amount of times event $B$ happens, and $N$ is the total number of events. Also, $q = 1-p$ because there are only two events, $A$ and $B$.",,"['probability', 'algebra-precalculus', 'statistics', 'probability-distributions', 'standard-deviation']"
85,Transformation of Extreme Value Distribution,Transformation of Extreme Value Distribution,,"Let $X$ be a random variable following distribution function (i.e., generalized Pareto distribution) $$ F_{\gamma, \sigma}(x) = 1-\left( 1+\frac{\gamma x}{\sigma} \right)^{-\frac{1}{\gamma}}, $$ where $\sigma>0, \gamma\in\mathbb R$. In addition,$x\in[0, \infty)$ if $\gamma\geq 0$ and $x\in\left[0, -\frac{\sigma}{\gamma}\right]$ if $\gamma<0$. It is claimed that $U:=\left( 1+\frac{\gamma X}{\sigma} \right)^{-\frac{1}{\gamma}}$ is uniformly distributed on $(0,1)$. To check this claim it is sufficient to show that the distribution function for $U$ is $F_U(u)=u$. $$ F_U(u) = \mathbb P(U\leq u) = \mathbb P \left[\left( 1+\frac{\gamma X}{\sigma} \right)^{-\frac{1}{\gamma}}\leq u\right] = F_{\gamma, \sigma}\left[ \frac{\sigma}{\gamma}\left( u^{-\gamma}-1 \right) \right] = 1-u. $$ From the above calculation, it seems that the claim does NOT check out. Is there any explanation for the claim, please? Thank you!","Let $X$ be a random variable following distribution function (i.e., generalized Pareto distribution) $$ F_{\gamma, \sigma}(x) = 1-\left( 1+\frac{\gamma x}{\sigma} \right)^{-\frac{1}{\gamma}}, $$ where $\sigma>0, \gamma\in\mathbb R$. In addition,$x\in[0, \infty)$ if $\gamma\geq 0$ and $x\in\left[0, -\frac{\sigma}{\gamma}\right]$ if $\gamma<0$. It is claimed that $U:=\left( 1+\frac{\gamma X}{\sigma} \right)^{-\frac{1}{\gamma}}$ is uniformly distributed on $(0,1)$. To check this claim it is sufficient to show that the distribution function for $U$ is $F_U(u)=u$. $$ F_U(u) = \mathbb P(U\leq u) = \mathbb P \left[\left( 1+\frac{\gamma X}{\sigma} \right)^{-\frac{1}{\gamma}}\leq u\right] = F_{\gamma, \sigma}\left[ \frac{\sigma}{\gamma}\left( u^{-\gamma}-1 \right) \right] = 1-u. $$ From the above calculation, it seems that the claim does NOT check out. Is there any explanation for the claim, please? Thank you!",,"['probability', 'statistics', 'self-learning']"
86,"Find marginal distribution of $Y$ where $Y\mid X$ is $N(a_1+a_2X,\sigma_1^2)$ and $X$ is $N(\mu,\sigma^2)$?",Find marginal distribution of  where  is  and  is ?,"Y Y\mid X N(a_1+a_2X,\sigma_1^2) X N(\mu,\sigma^2)","Let a random variable $X$ be normal $N(\mu,\sigma^2)$ and let the conditional distribution of $Y$ given $X$ be normal $N(a_1+a_2X,\sigma_1^2)$. a)Find the joint probability density function of $X$ and $Y$. b)Find the marginal distribution of $Y$ and the correlation coefficient of $X$ and $Y$. For (a), I just multiplied the conditional density of $Y$ given $X$ and density of $X$; and I think it's ok. For (b), I tried to write their joint density in the form of bivariate normal but couldn't do that. On the other hand, we know that if the random variables $X$ and $Y$ are bivariate normal then, the conditional distribution of $X$ given $Y$ is normal with mean $E[X\mid Y]$ and variance $(1-\rho^2)\sigma_X^2$. But is that true that if $X$ is normal and $Y$ given $X$ is normal, then they are bivariate normal? So that I can do (b) easily, or is there another way to solve this question? Thanks!","Let a random variable $X$ be normal $N(\mu,\sigma^2)$ and let the conditional distribution of $Y$ given $X$ be normal $N(a_1+a_2X,\sigma_1^2)$. a)Find the joint probability density function of $X$ and $Y$. b)Find the marginal distribution of $Y$ and the correlation coefficient of $X$ and $Y$. For (a), I just multiplied the conditional density of $Y$ given $X$ and density of $X$; and I think it's ok. For (b), I tried to write their joint density in the form of bivariate normal but couldn't do that. On the other hand, we know that if the random variables $X$ and $Y$ are bivariate normal then, the conditional distribution of $X$ given $Y$ is normal with mean $E[X\mid Y]$ and variance $(1-\rho^2)\sigma_X^2$. But is that true that if $X$ is normal and $Y$ given $X$ is normal, then they are bivariate normal? So that I can do (b) easily, or is there another way to solve this question? Thanks!",,"['probability', 'statistics', 'normal-distribution', 'covariance']"
87,Poisson Distribution: P(exceeds certain number),Poisson Distribution: P(exceeds certain number),,"A professor plans to schedule an open lab in order to provide answers and additional help to students in the hour before homework is due. The number of students who will come to open lab will vary from week to week, and the professor assumes the count in a particular week will follow a Poisson(15) distribution. The professor is offered a room for the open lab, but is concerned that the room capacity of 23 won’t be sufficient. Compute the probability that the number of students who come to open lab in a particular week will exceed the room capacity. I get that λ=15, but how do you find P(X > 23)? Would it be 1-P(0 < X ≤ 23)? That would take ages to calculate...","A professor plans to schedule an open lab in order to provide answers and additional help to students in the hour before homework is due. The number of students who will come to open lab will vary from week to week, and the professor assumes the count in a particular week will follow a Poisson(15) distribution. The professor is offered a room for the open lab, but is concerned that the room capacity of 23 won’t be sufficient. Compute the probability that the number of students who come to open lab in a particular week will exceed the room capacity. I get that λ=15, but how do you find P(X > 23)? Would it be 1-P(0 < X ≤ 23)? That would take ages to calculate...",,"['probability', 'statistics', 'probability-distributions']"
88,Variance measure for categorical data?,Variance measure for categorical data?,,"I have four discrete categories, each category has a sample count, and I'd like some measure of variance, where minimum variance is counts are evenly divided among all four categories and max variance is all counts are in one category and the other three have zero. Is there any standard measure or calculation that does something like this?","I have four discrete categories, each category has a sample count, and I'd like some measure of variance, where minimum variance is counts are evenly divided among all four categories and max variance is all counts are in one category and the other three have zero. Is there any standard measure or calculation that does something like this?",,"['statistics', 'discrete-mathematics', 'order-statistics']"
89,Show that the average of n observations is equal to the expected value,Show that the average of n observations is equal to the expected value,,"Show that the average of n observations is equal to the expected value   with the density function with index k is equal to   the number of observations equal to k divided by the total number of   observation, that is, $$\frac{y_1 +...+y_n}{n} = \sum_{k=1}^{n}k*p_k$$   there $p_k=\#(y_i: y_i = k)/n$ Any hints? I really don't know how to show the statement above.","Show that the average of n observations is equal to the expected value   with the density function with index k is equal to   the number of observations equal to k divided by the total number of   observation, that is, $$\frac{y_1 +...+y_n}{n} = \sum_{k=1}^{n}k*p_k$$   there $p_k=\#(y_i: y_i = k)/n$ Any hints? I really don't know how to show the statement above.",,"['probability', 'statistics', 'stochastic-analysis']"
90,Maximum Likelihood Estimator of scaled beta,Maximum Likelihood Estimator of scaled beta,,"I want to find the MLE of $X = \theta{Y}$, where $\theta > 0$ and $Y \sim \mathrm{Beta}(2,1)$. The density for $X$ is given by $$f_{\theta}(x) = \frac{2x}{\theta^{2}}$$ on $[0,\theta]$. It has been a bit problematic finding the MLE here (for fixed $x \in [0, \theta]$, doesn't seem to be any value $\hat{\theta}$ that maximizes this function). I'd appreciate any help with this.","I want to find the MLE of $X = \theta{Y}$, where $\theta > 0$ and $Y \sim \mathrm{Beta}(2,1)$. The density for $X$ is given by $$f_{\theta}(x) = \frac{2x}{\theta^{2}}$$ on $[0,\theta]$. It has been a bit problematic finding the MLE here (for fixed $x \in [0, \theta]$, doesn't seem to be any value $\hat{\theta}$ that maximizes this function). I'd appreciate any help with this.",,['statistics']
91,Question about asymmetry of chi-square distribution,Question about asymmetry of chi-square distribution,,"Let $X_1,\dots,X_n$ be a set of i.i.d. chi-square random variables with $k$ degrees of freedom. Consider the statistic $\arg\max_i\{|X_i/k - 1|\}=X_{\alpha}$. I wonder about the probability that $X_{\alpha} = X_{\text{max}}$, rather than $X_{\text{min}}$? Does it change if $k$ gets large? I suspect that it may be very, very difficult to get a precise answer to the above questions. So I would like to ask the simpler question : can we at least say that it is more likely that $X_{\alpha} = X_{\text{max}}$? Simulations seem to indicate yes. Is there a heuristic reason why?","Let $X_1,\dots,X_n$ be a set of i.i.d. chi-square random variables with $k$ degrees of freedom. Consider the statistic $\arg\max_i\{|X_i/k - 1|\}=X_{\alpha}$. I wonder about the probability that $X_{\alpha} = X_{\text{max}}$, rather than $X_{\text{min}}$? Does it change if $k$ gets large? I suspect that it may be very, very difficult to get a precise answer to the above questions. So I would like to ask the simpler question : can we at least say that it is more likely that $X_{\alpha} = X_{\text{max}}$? Simulations seem to indicate yes. Is there a heuristic reason why?",,"['probability', 'statistics', 'random-variables']"
92,Problem solving the standard deviation for a stochastic variable,Problem solving the standard deviation for a stochastic variable,,"Information: In a laboratory we have a vial of water that's being kept on 50 degrees Celsius. This is measured with the best thermometer in the world. The standard deviation on this thermometer is sigma = 2.0 degrees Celsius. X = measured temperature in degrees Celsius. Then we have E(X) = 50.0 and Sigma = 2.0 when we measure the temperature in the water we know is 50 degrees celsius. Y = temperature in degrees fahrenheit. Previous solved equations: We now arrive at the problem with the stochastic variable and its standard deviation. We have bought in 5 thermometers and we use them all to find the temperature in the vial of water. We use the average from these to determine the temperature in the water. You can use the rule for independent variables (which we assume our Xs are): e) Calculate the standard deviation for the stochastic variable W. So what I thought at first could be the answer was this: The 4s I got from the solution in a) where I calculated for X. It is short for , which is equal to 8 on each X, because I am assuming that X = 50 and thus by using the sigma standard deviation for the thermometers I have 48, 50 and 52. EDIT: After being corrected (keeping the error for future references): ^This was also wrong, it should be the one which is blockquoted down below. But on another forum I got a suggestion that it might be the following: Seems like you want the standard deviation of the mean, I'm not sure   where the 8 plays in at all, but since the variables are independent,   $$S_W^2 = \text{Var}(W) = \text{Var}\left(\dfrac{1}{5}(X_1+\cdots > X_5)\right) = \dfrac{1}{5^2}\text{Var}\left(X_1+\cdots X_5\right) $$ $$=\dfrac{1}{25}\sum_{i=1}^5 \text{Var} \left(X_i\right) > =\dfrac{1}{25}\sum_{i=1}^5 (2)^2 = \dfrac{20}{25}=\dfrac{4}{5} $$ So if I interpreted this right, $S_W=2/\sqrt{5}$. Maybe wait for   another opinion. BUT, I got this suggestion before I added in the rest information with the temperatures and the previous solved equations. Thanks in advance for tips to what I can read or do to solve this.","Information: In a laboratory we have a vial of water that's being kept on 50 degrees Celsius. This is measured with the best thermometer in the world. The standard deviation on this thermometer is sigma = 2.0 degrees Celsius. X = measured temperature in degrees Celsius. Then we have E(X) = 50.0 and Sigma = 2.0 when we measure the temperature in the water we know is 50 degrees celsius. Y = temperature in degrees fahrenheit. Previous solved equations: We now arrive at the problem with the stochastic variable and its standard deviation. We have bought in 5 thermometers and we use them all to find the temperature in the vial of water. We use the average from these to determine the temperature in the water. You can use the rule for independent variables (which we assume our Xs are): e) Calculate the standard deviation for the stochastic variable W. So what I thought at first could be the answer was this: The 4s I got from the solution in a) where I calculated for X. It is short for , which is equal to 8 on each X, because I am assuming that X = 50 and thus by using the sigma standard deviation for the thermometers I have 48, 50 and 52. EDIT: After being corrected (keeping the error for future references): ^This was also wrong, it should be the one which is blockquoted down below. But on another forum I got a suggestion that it might be the following: Seems like you want the standard deviation of the mean, I'm not sure   where the 8 plays in at all, but since the variables are independent,   $$S_W^2 = \text{Var}(W) = \text{Var}\left(\dfrac{1}{5}(X_1+\cdots > X_5)\right) = \dfrac{1}{5^2}\text{Var}\left(X_1+\cdots X_5\right) $$ $$=\dfrac{1}{25}\sum_{i=1}^5 \text{Var} \left(X_i\right) > =\dfrac{1}{25}\sum_{i=1}^5 (2)^2 = \dfrac{20}{25}=\dfrac{4}{5} $$ So if I interpreted this right, $S_W=2/\sqrt{5}$. Maybe wait for   another opinion. BUT, I got this suggestion before I added in the rest information with the temperatures and the previous solved equations. Thanks in advance for tips to what I can read or do to solve this.",,"['statistics', 'random-variables', 'standard-deviation']"
93,Find the covariance of $Y_1$ and $Y_2$,Find the covariance of  and,Y_1 Y_2,"I had a statistics question I was hoping for help on: Let $Y_1$ and $Y_2$ be discrete random variables with join probability function: $$f(x,y) = \begin{cases} \dfrac{y_1 + 2y_2}{18} & \text{if $y_1 = 1,2 ; y_2 = 1,2$} \\ 0 & \text{elsewhere} \end{cases}$$ Find the covariance of $Y_1$ and $Y_2$. Below is my work, would anyone be able to let me know if I'm right or if there are any errors? Thank you so much in advance, I really appreciate it! Probabilities derived from the above function: $(Y_1,Y_2)$ $(1,1) = \dfrac{1}{6}$, $(2,1) = \dfrac{2}{9}$ $(1,2) = \dfrac{5}{18}$, $(2,2) = \dfrac{1}{3}$ Marginal Probabilities from the above probabilities: $p_1(1)$ = $\dfrac{4}{9}$ $p_1(2)$ = $\dfrac{5}{9}$ $p_2(1)$ = $\dfrac{7}{18}$ $p_2(2)$ = $\dfrac{11}{18}$ $E[Y_1 Y_2] = (1)(1)(1/6)+(1)(2)(2/9)+(2)(1)(5/18)+(2)(2)(1/3) = \dfrac{5}{2}$ $E[Y_1]$ = (1)(4/9)+(2)(5/9) = $\dfrac{14}{9}$ $E[Y_2] = (1)(7/18) + 2(11/8) = \dfrac{29}{18}$ $$\operatorname{Cov}(Y_1,Y_2)=E[Y_1 Y_2]-E[Y_1] E[Y_2] = \frac{5}{2} - \frac{14}{9} \cdot \frac{29}{18} = -\frac{1}{162}$$","I had a statistics question I was hoping for help on: Let $Y_1$ and $Y_2$ be discrete random variables with join probability function: $$f(x,y) = \begin{cases} \dfrac{y_1 + 2y_2}{18} & \text{if $y_1 = 1,2 ; y_2 = 1,2$} \\ 0 & \text{elsewhere} \end{cases}$$ Find the covariance of $Y_1$ and $Y_2$. Below is my work, would anyone be able to let me know if I'm right or if there are any errors? Thank you so much in advance, I really appreciate it! Probabilities derived from the above function: $(Y_1,Y_2)$ $(1,1) = \dfrac{1}{6}$, $(2,1) = \dfrac{2}{9}$ $(1,2) = \dfrac{5}{18}$, $(2,2) = \dfrac{1}{3}$ Marginal Probabilities from the above probabilities: $p_1(1)$ = $\dfrac{4}{9}$ $p_1(2)$ = $\dfrac{5}{9}$ $p_2(1)$ = $\dfrac{7}{18}$ $p_2(2)$ = $\dfrac{11}{18}$ $E[Y_1 Y_2] = (1)(1)(1/6)+(1)(2)(2/9)+(2)(1)(5/18)+(2)(2)(1/3) = \dfrac{5}{2}$ $E[Y_1]$ = (1)(4/9)+(2)(5/9) = $\dfrac{14}{9}$ $E[Y_2] = (1)(7/18) + 2(11/8) = \dfrac{29}{18}$ $$\operatorname{Cov}(Y_1,Y_2)=E[Y_1 Y_2]-E[Y_1] E[Y_2] = \frac{5}{2} - \frac{14}{9} \cdot \frac{29}{18} = -\frac{1}{162}$$",,"['probability', 'statistics', 'discrete-mathematics', 'random-variables', 'covariance']"
94,Help reading a scatterplot,Help reading a scatterplot,,"In reading the following scatterplot Would it be correct to say there is a strong positive linear relationship between the two axis? If not, could you please explain why? Also is it correct to consider the ringed point as outlier?","In reading the following scatterplot Would it be correct to say there is a strong positive linear relationship between the two axis? If not, could you please explain why? Also is it correct to consider the ringed point as outlier?",,['statistics']
95,Do we need to check that maximum likelihood estimator is a maximum?,Do we need to check that maximum likelihood estimator is a maximum?,,"For maximum likelihood estimation, do we theoretically need to check that the critical point is a maximum (rather than a minimum or saddle point) or is this automatic? I believe that it is automatic and it has to do with the fisher information matrix, which comes from the second partial derivatives matrix, which we know is the the inverse of a variance matrix, which we know is semi-positive definite.","For maximum likelihood estimation, do we theoretically need to check that the critical point is a maximum (rather than a minimum or saddle point) or is this automatic? I believe that it is automatic and it has to do with the fisher information matrix, which comes from the second partial derivatives matrix, which we know is the the inverse of a variance matrix, which we know is semi-positive definite.",,"['statistics', 'statistical-inference']"
96,"How do I compare student pre-test scores with post-test scores to evaluate whether or not they ""learned""?","How do I compare student pre-test scores with post-test scores to evaluate whether or not they ""learned""?",,"We need to track student advancement in a topic based on pre and post test scores.  That is, we give a pre-test on day 1 of class, then on the last day we give the exact same test, renamed as a post-test.  Somehow we have to take the first score and assign a target number for the second number to show improvement.  This second number has to be higher than the first number.  Students who hit this number, or above, on the post test are considered a success.  Students who do not are counted as a failure.  Based on the percent of students who are ""success"" we get evaluated as how well the students learned.  My problem is trying to figure out how to calculate how well the students did, or did not, improve. My first thought was simply take the difference in scores and calculate a percentage change.  But how do I then measure student success?  A change in 50% of their score?  This seems somewhat easy for people who did rather poor on their pre-test, but those who did rather well it seems to penalize.  That is, jumping from 0% to 50% is great, but jumping from 75% to 85% is does not seem as good if we just measure %change.  The first student would be a ""success"" even failing the course, but the second student would be considered a ""failure"" even passing the course rather well. If it helps, here are the pre-test scores for one class: 20 40 44 12 48 32 24 44 28 0 36 40 and pre-test scores from another... 76 40 40 32 60 64 68 48 36 72 56 20 24 36 52 the exact same method must be used in each class to show ""success"" versus ""failure"".","We need to track student advancement in a topic based on pre and post test scores.  That is, we give a pre-test on day 1 of class, then on the last day we give the exact same test, renamed as a post-test.  Somehow we have to take the first score and assign a target number for the second number to show improvement.  This second number has to be higher than the first number.  Students who hit this number, or above, on the post test are considered a success.  Students who do not are counted as a failure.  Based on the percent of students who are ""success"" we get evaluated as how well the students learned.  My problem is trying to figure out how to calculate how well the students did, or did not, improve. My first thought was simply take the difference in scores and calculate a percentage change.  But how do I then measure student success?  A change in 50% of their score?  This seems somewhat easy for people who did rather poor on their pre-test, but those who did rather well it seems to penalize.  That is, jumping from 0% to 50% is great, but jumping from 75% to 85% is does not seem as good if we just measure %change.  The first student would be a ""success"" even failing the course, but the second student would be considered a ""failure"" even passing the course rather well. If it helps, here are the pre-test scores for one class: 20 40 44 12 48 32 24 44 28 0 36 40 and pre-test scores from another... 76 40 40 32 60 64 68 48 36 72 56 20 24 36 52 the exact same method must be used in each class to show ""success"" versus ""failure"".",,['statistics']
97,Sequence of Gamma r.v.s converges in probability to 1,Sequence of Gamma r.v.s converges in probability to 1,,"Let $\{X_n\}$ be a squence of Gamma distributed random variables with pdf $$  f(x;\alpha,\beta) =   \begin{cases}        \hfill \dfrac{x^{\alpha - 1}e^{-x/\beta}}{\beta^{\alpha}\Gamma(\alpha)}    \hfill & \text{ for $x>0$} \\       \hfill 0 \hfill & \text{elsewhere} \\   \end{cases} $$ In our case, we have $\alpha = n$ and $\beta = 1/n$. Show that $$X_n \overset{\mathbb{P}}{\to} 1$$ My try: We have to show that for every $\varepsilon>0$: $$\mathbb{P}(|X_n - 1|<\varepsilon)=\mathbb{P}(-1+\varepsilon < X_n < 1-\varepsilon)=1$$ Thus: \begin{align*} \mathbb{P}(-1+\varepsilon < X_n < 1-\varepsilon) &= \int_{-1+\varepsilon}^{1-\varepsilon} f(x;n,1/n)\mathrm{d}x\\ &= \int_{0}^{1-\varepsilon}\dfrac{x^{n - 1}e^{-nx}}{\dfrac{1}{n^n}\Gamma(n)}\mathrm{d}x \\ &=\dfrac{n^n}{\Gamma(n)}\int_{0}^{1-\varepsilon} x^{n - 1}e^{-nx}\mathrm{d}x \end{align*} Now make the substitution $x \mapsto nx$, this yields \begin{align*} \mathbb{P}(-1+\varepsilon < X_n < 1-\varepsilon) &= \dfrac{n^n}{\Gamma(n)}\int_{0}^{n(1-\varepsilon)} \dfrac{1}{n}\left(\dfrac{x}{n}\right)^{n - 1}e^{-x}\mathrm{d}x \\  &= \dfrac{1}{\Gamma(n)}\int_{0}^{n(1-\varepsilon)} x^{n - 1}e^{-x}\mathrm{d}x \end{align*} Now, taking the limit as $n \to \infty$ we yield \begin{align*} \mathbb{P}(-1+\varepsilon < X_n < 1-\varepsilon) &=\lim_{n \to \infty}\dfrac{1}{\Gamma(n)} \int_0^{n(1-\varepsilon)} x^{n - 1}e^{-x}\mathrm{d}x \\ &= \lim_{n \to \infty}\dfrac{\Gamma(n)}{\Gamma(n)} \\ &=1 \end{align*} Since $\varepsilon>0$ was given arbitrarily, we conclude that $$X_n \overset{\mathbb{P}}{\to} 1$$ And I think I solved my question... For the sake of future reference or if someone has the same (kind of) problem, I will post it anyways, even tough I am pretty sure I solved it.","Let $\{X_n\}$ be a squence of Gamma distributed random variables with pdf $$  f(x;\alpha,\beta) =   \begin{cases}        \hfill \dfrac{x^{\alpha - 1}e^{-x/\beta}}{\beta^{\alpha}\Gamma(\alpha)}    \hfill & \text{ for $x>0$} \\       \hfill 0 \hfill & \text{elsewhere} \\   \end{cases} $$ In our case, we have $\alpha = n$ and $\beta = 1/n$. Show that $$X_n \overset{\mathbb{P}}{\to} 1$$ My try: We have to show that for every $\varepsilon>0$: $$\mathbb{P}(|X_n - 1|<\varepsilon)=\mathbb{P}(-1+\varepsilon < X_n < 1-\varepsilon)=1$$ Thus: \begin{align*} \mathbb{P}(-1+\varepsilon < X_n < 1-\varepsilon) &= \int_{-1+\varepsilon}^{1-\varepsilon} f(x;n,1/n)\mathrm{d}x\\ &= \int_{0}^{1-\varepsilon}\dfrac{x^{n - 1}e^{-nx}}{\dfrac{1}{n^n}\Gamma(n)}\mathrm{d}x \\ &=\dfrac{n^n}{\Gamma(n)}\int_{0}^{1-\varepsilon} x^{n - 1}e^{-nx}\mathrm{d}x \end{align*} Now make the substitution $x \mapsto nx$, this yields \begin{align*} \mathbb{P}(-1+\varepsilon < X_n < 1-\varepsilon) &= \dfrac{n^n}{\Gamma(n)}\int_{0}^{n(1-\varepsilon)} \dfrac{1}{n}\left(\dfrac{x}{n}\right)^{n - 1}e^{-x}\mathrm{d}x \\  &= \dfrac{1}{\Gamma(n)}\int_{0}^{n(1-\varepsilon)} x^{n - 1}e^{-x}\mathrm{d}x \end{align*} Now, taking the limit as $n \to \infty$ we yield \begin{align*} \mathbb{P}(-1+\varepsilon < X_n < 1-\varepsilon) &=\lim_{n \to \infty}\dfrac{1}{\Gamma(n)} \int_0^{n(1-\varepsilon)} x^{n - 1}e^{-x}\mathrm{d}x \\ &= \lim_{n \to \infty}\dfrac{\Gamma(n)}{\Gamma(n)} \\ &=1 \end{align*} Since $\varepsilon>0$ was given arbitrarily, we conclude that $$X_n \overset{\mathbb{P}}{\to} 1$$ And I think I solved my question... For the sake of future reference or if someone has the same (kind of) problem, I will post it anyways, even tough I am pretty sure I solved it.",,"['probability', 'statistics', 'probability-distributions']"
98,How to determine a conditional distribution,How to determine a conditional distribution,,"Consider: $$X\stackrel{d}{=}N(\mu, \sigma^2)\qquad Y\mid X \stackrel{d}{=}N(\alpha+\beta X, \tau^2) \qquad U\mid Y,X \stackrel{d}{=} N(0,\nu^2)$$ Determine the distribution of $W=X+U$ given $Y$ and $X$ . How should handle this problem? Could someone give me some pointers? Should I start with $\mathcal{P}(W<w\mid X,Y) = \mathcal{P}(U<w-X\mid X,Y)$ ? (what then?) Dus $U\mid Y,X \stackrel{d}{=} N(0,\nu^2)$ imply $U$ independent from $Y,X$ since the distribution does not contain $X,Y$ ?",Consider: Determine the distribution of given and . How should handle this problem? Could someone give me some pointers? Should I start with ? (what then?) Dus imply independent from since the distribution does not contain ?,"X\stackrel{d}{=}N(\mu, \sigma^2)\qquad Y\mid X \stackrel{d}{=}N(\alpha+\beta X, \tau^2) \qquad U\mid Y,X \stackrel{d}{=} N(0,\nu^2) W=X+U Y X \mathcal{P}(W<w\mid X,Y) = \mathcal{P}(U<w-X\mid X,Y) U\mid Y,X \stackrel{d}{=} N(0,\nu^2) U Y,X X,Y","['probability', 'statistics', 'probability-distributions']"
99,General sufficient condition for independence of these two random Variables.,General sufficient condition for independence of these two random Variables.,,"I need to state and prove a general sufficient condition on(a,b,c) for independence of two random Variables. We have that $a,b$ and $c$ are real numbers and the random variables are below: $$ Y_1=aZ_1+bZ_2+cZ_3 $$ $$ Y_2=aZ_2+bZ_3+cZ_4 $$ Where $Z_i$ are iid from $Z$. Here, I need to prove this result regardless of the probability distribution $Z$. I am also given that $$ Cov[Y_1,Y_2]=(ab+bc)Var[Z] $$ Thanks for the help in advance,","I need to state and prove a general sufficient condition on(a,b,c) for independence of two random Variables. We have that $a,b$ and $c$ are real numbers and the random variables are below: $$ Y_1=aZ_1+bZ_2+cZ_3 $$ $$ Y_2=aZ_2+bZ_3+cZ_4 $$ Where $Z_i$ are iid from $Z$. Here, I need to prove this result regardless of the probability distribution $Z$. I am also given that $$ Cov[Y_1,Y_2]=(ab+bc)Var[Z] $$ Thanks for the help in advance,",,"['probability', 'statistics', 'random-variables', 'covariance']"
