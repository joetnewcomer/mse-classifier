,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Extension, restriction, and coextension of scalars adjunctions in the case of noncommutative rings?","Extension, restriction, and coextension of scalars adjunctions in the case of noncommutative rings?",,"If we have a ring homomorphism $f: R \to S$, then we can put an $(R,S)$ or $(S, R)$ bimodule structure on $S$ and define the extension $f_!:= S \otimes_R (-) : R \text{Mod} \to S \text{Mod}$, restriction $f_*: S \text{Mod} \to R \text{Mod}$ and coextension $f_*:= \text{Hom}_{R}(S, -) : R \text{Mod} \to S \text{Mod}$ of scalars functors. These constructions do not depend at all on the commutativity of the rings $R$ and $S$, but in every source I've seen that mentions extension and coextension of scalars, it is assumed that the rings are commutative, such as in the nLab article . Is there any reason that the rings are assumed to be commutative? Does the adjunction still hold in the noncommutative case? Are there any properties of these constructions that are ""less nice"" in the noncommutative case? Or is it simply for convenience? Most of my algebra knowledge is self-taught, so I apologize if I'm missing something obvious.","If we have a ring homomorphism $f: R \to S$, then we can put an $(R,S)$ or $(S, R)$ bimodule structure on $S$ and define the extension $f_!:= S \otimes_R (-) : R \text{Mod} \to S \text{Mod}$, restriction $f_*: S \text{Mod} \to R \text{Mod}$ and coextension $f_*:= \text{Hom}_{R}(S, -) : R \text{Mod} \to S \text{Mod}$ of scalars functors. These constructions do not depend at all on the commutativity of the rings $R$ and $S$, but in every source I've seen that mentions extension and coextension of scalars, it is assumed that the rings are commutative, such as in the nLab article . Is there any reason that the rings are assumed to be commutative? Does the adjunction still hold in the noncommutative case? Are there any properties of these constructions that are ""less nice"" in the noncommutative case? Or is it simply for convenience? Most of my algebra knowledge is self-taught, so I apologize if I'm missing something obvious.",,"['linear-algebra', 'abstract-algebra', 'ring-theory', 'category-theory', 'modules']"
1,Graded Vector Spaces (definition),Graded Vector Spaces (definition),,"I am studying Algebraic Operads with the book Algebraic Operads, by Jean-Louis Loday and Bruno Vallette and I'm having a little problem with the definition of graded vector space. My advisor and I disagree on the definition. The book defines it this way: my advisor thinks it is equivalent to saying that $V = \bigoplus_{n\in \mathbb{Z}} V_n$. I believe that they are different. In nLab ( Graded Vector Spaces ) I found the following definition which strengthens my argument. Finally, who is correct? Can anyone suggest me any other reference about graded vector spaces?","I am studying Algebraic Operads with the book Algebraic Operads, by Jean-Louis Loday and Bruno Vallette and I'm having a little problem with the definition of graded vector space. My advisor and I disagree on the definition. The book defines it this way: my advisor thinks it is equivalent to saying that $V = \bigoplus_{n\in \mathbb{Z}} V_n$. I believe that they are different. In nLab ( Graded Vector Spaces ) I found the following definition which strengthens my argument. Finally, who is correct? Can anyone suggest me any other reference about graded vector spaces?",,"['linear-algebra', 'category-theory', 'operads']"
2,Derivative of $l_1$ norm,Derivative of  norm,l_1,I want to compute the following derivative with respect to $n\times1$ vector $\mathbf x$.  $$g = \left\lVert \mathbf x - A \mathbf x  \right\rVert_1 $$ My work: $$g = \left\lVert \mathbf x - A \mathbf x  \right\rVert_1  = \sum_{i=1}^{n} \lvert x_i - (A\mathbf x)_i\rvert = \sum_{i=1}^{n} \lvert x_i - A_i \cdot \mathbf x \rvert = \sum_{i=1}^{n} \lvert x_i - \sum_{j=1}^n a_{ij} x_j\rvert$$  So the $k$th element of derivative is: $$\frac{\partial g}{\partial x_k} = \frac{\partial }{\partial x_k}\sum_{i=1}^n \lvert x_i - \sum_{j=1}^n a_{ij} x_j\rvert $$ $$= \frac{\partial }{\partial x_k}\bigg(\lvert x_1 - \sum_{j=1}^n a_{1j} x_j\rvert +\cdots+ \lvert x_k - \sum_{j=1}^n a_{kj} x_j\rvert + \cdots\lvert x_n - \sum_{j=1}^n a_{nj} x_j\rvert  \bigg)$$ $$ =-a_{1k}sign(x_1 - \sum_{j=1}^n a_{1j} x_j)-\cdots+(1-a_{kk})sign(x_k - \sum_{j=1}^n a_{kj} x_j)-\cdots -a_{nk}sign(x_n - \sum_{j=1}^n a_{nj} x_j)$$ And my questions: Is this derivation correct? How I can represent the answer compactly? Can you introduce me a source to master this material? Thanks.,I want to compute the following derivative with respect to $n\times1$ vector $\mathbf x$.  $$g = \left\lVert \mathbf x - A \mathbf x  \right\rVert_1 $$ My work: $$g = \left\lVert \mathbf x - A \mathbf x  \right\rVert_1  = \sum_{i=1}^{n} \lvert x_i - (A\mathbf x)_i\rvert = \sum_{i=1}^{n} \lvert x_i - A_i \cdot \mathbf x \rvert = \sum_{i=1}^{n} \lvert x_i - \sum_{j=1}^n a_{ij} x_j\rvert$$  So the $k$th element of derivative is: $$\frac{\partial g}{\partial x_k} = \frac{\partial }{\partial x_k}\sum_{i=1}^n \lvert x_i - \sum_{j=1}^n a_{ij} x_j\rvert $$ $$= \frac{\partial }{\partial x_k}\bigg(\lvert x_1 - \sum_{j=1}^n a_{1j} x_j\rvert +\cdots+ \lvert x_k - \sum_{j=1}^n a_{kj} x_j\rvert + \cdots\lvert x_n - \sum_{j=1}^n a_{nj} x_j\rvert  \bigg)$$ $$ =-a_{1k}sign(x_1 - \sum_{j=1}^n a_{1j} x_j)-\cdots+(1-a_{kk})sign(x_k - \sum_{j=1}^n a_{kj} x_j)-\cdots -a_{nk}sign(x_n - \sum_{j=1}^n a_{nj} x_j)$$ And my questions: Is this derivation correct? How I can represent the answer compactly? Can you introduce me a source to master this material? Thanks.,,"['linear-algebra', 'normed-spaces', 'partial-derivative']"
3,"Prove that if $P(P(x)) = Q(Q(x))$, then the polynomials $P$ and $Q$ are equal.","Prove that if , then the polynomials  and  are equal.",P(P(x)) = Q(Q(x)) P Q,Let $P$ and $Q$ be polynomials with complex coefficients such that $P(P(x)) = Q(Q(x))$. Prove that $P = Q$. It is obvious that degree of both will be equal. But I don't have any idea how to solve this question.,Let $P$ and $Q$ be polynomials with complex coefficients such that $P(P(x)) = Q(Q(x))$. Prove that $P = Q$. It is obvious that degree of both will be equal. But I don't have any idea how to solve this question.,,['linear-algebra']
4,Recurrence relation for the determinant of a tridiagonal matrix,Recurrence relation for the determinant of a tridiagonal matrix,,"Let $$f_n := \begin{vmatrix} a_1 & b_1 \\ c_1 & a_2 & b_2 \\ & c_2 & \ddots & \ddots \\ & & \ddots & \ddots & b_{n-1} \\ & & & c_{n-1} & a_n \end{vmatrix}$$ Apparently, the determinant of the tridiagional matrix above is given by the recurrence relation $$f_n = a_n f_{n-1} - c_{n-1} b_{n-1}f_{n-2}$$ with initial values $f_0 = 1$ and $f_{-1} = 0$ (according to Wikipedia). Can anyone please explain to me how they came to this recurrence relation? I don’t really understand how to derive it.","Let Apparently, the determinant of the tridiagional matrix above is given by the recurrence relation with initial values and (according to Wikipedia). Can anyone please explain to me how they came to this recurrence relation? I don’t really understand how to derive it.","f_n := \begin{vmatrix}
a_1 & b_1 \\
c_1 & a_2 & b_2 \\
& c_2 & \ddots & \ddots \\
& & \ddots & \ddots & b_{n-1} \\
& & & c_{n-1} & a_n
\end{vmatrix} f_n = a_n f_{n-1} - c_{n-1} b_{n-1}f_{n-2} f_0 = 1 f_{-1} = 0","['linear-algebra', 'matrices', 'recurrence-relations', 'determinant', 'tridiagonal-matrices']"
5,I'm trying to calculate $e^{At}$. Where do I go wrong?,I'm trying to calculate . Where do I go wrong?,e^{At},"Let $$A=\begin{bmatrix}0 & 1 & 0 & 0 \\ -1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & -4 & 0\end{bmatrix}$$ I want to determine $e^{At}$. I tried it using two methods. First via the Jordan form: The characteristic polynomial of $A$ is given by $$\det(I\xi-A)=\begin{vmatrix} \xi & -1 & 0 & 0 \\ 1 & \xi & 0 & 0 \\ 0 & 0 & \xi & -1 \\ 0 & 0 & 4 & \xi \end{vmatrix}=1\cdot\begin{vmatrix} 1 & 0 & 0 \\ 0 & \xi & -1 \\ 0 & 4 & \xi \end{vmatrix}+\xi\cdot\begin{vmatrix} \xi & 0 & 0 \\ 0 & \xi & -1 \\ 0 & 4 & \xi \end{vmatrix}$$ $$\begin{vmatrix} 1 & 0 & 0 \\ 0 & \xi & -1 \\ 0 & 4 & \xi \end{vmatrix}=1\cdot\begin{vmatrix} \xi & -1 \\ 4 & \xi \end{vmatrix}=\xi^{2}+4\qquad\text{and}\qquad \begin{vmatrix} \xi & 0 & 0 \\ 0 & \xi & -1 \\ 0 & 4 & \xi \end{vmatrix}=\xi\cdot\begin{vmatrix} \xi & -1 \\ 4 & \xi \end{vmatrix}=\xi(\xi^{2}+4)$$ $$\implies\det(I\xi-A)=\xi^{2}+4+\xi(\xi^{3}+4\xi)=\xi^{4}+5\xi^{2}+4=(\xi^{2}+4)(\xi^{2}+1)$$ Then the eigenvalues are given by $\lambda_{1,2}=\pm i$ and $\lambda_{3,4}=\pm 2i$. Now, computing the corresponding eigenvectors: For $\lambda=i$, we have $$\begin{cases} -ix+y=0 \\ -x-iy=0 \\ iz+w=0 \\ -4z-iw=0 \end{cases}\implies\begin{cases} x=-y \\ 4z=-w \end{cases}$$ Then setting $x=a$ and $z=b$, for $a,b\in\mathbb{C}$, we obtain: $$v_{1}=\begin{pmatrix} a \\ -a \\ b \\ -4b \end{pmatrix}=a\begin{pmatrix} 1 \\ -1 \\ 0 \\ 0 \end{pmatrix}+b\begin{pmatrix} 0 \\ 0 \\ 1 \\ -4 \end{pmatrix}$$ Similarly, for $\lambda=-i$, we compute, for complex $c,d$: $$v_{2}=c\begin{pmatrix} -1 \\ 1 \\ 0 \\ 0 \end{pmatrix}+d\begin{pmatrix} 0 \\ 0 \\ -1 \\ 4 \end{pmatrix}$$ For $\lambda=2i$, we get: $$v_{3}=j\begin{pmatrix} 1 \\ -1 \\ 0 \\ 0 \end{pmatrix}+k\begin{pmatrix} 0 \\ 0 \\ 1 \\ -2 \end{pmatrix}$$ And finally, for $\lambda=-2i$: $$v_{4}=m\begin{pmatrix} -1 \\ 1 \\ 0 \\ 0 \end{pmatrix}+n\begin{pmatrix} 0 \\ 0 \\ -1 \\ 2 \end{pmatrix}$$ Define $S=\begin{bmatrix} v_{1} & v_{2} & v_{3} & v_{4} \end{bmatrix}$. Then $$S=\begin{bmatrix} a & -c & j & -m \\ -a & c & -j & m \\ b & -d & k & -n \\ -4b & 4d & -2k & 2n \end{bmatrix}$$ But I have to calculate $S^{-1}$ via this method (which seems a bit too complicated), since $e^{At}=Se^{S^{-1}ASt}S^{-1}$. Then I tried to compute $e^{At}$ using the theory of autonomous behaviours: So I said that every strong solution of $\frac{d}{dt}x=Ax$ is of the form $$x(t)=B_{10}e^{-it}+B_{20}e^{it}+B_{30}e^{-2it}+B_{40}e^{2it}$$ The vectors $B_{ij}$ should satisfy the relations $$(-i\cdot I-A)B_{10}=0$$ $$(i\cdot I-A)B_{20}=0$$ $$(-2i\cdot I-A)B_{30}=0$$ $$(2i\cdot I-A)B_{40}=0$$ Solving these equations yield $B_{10}=v_{1}$, $B_{20}=v_{2}$, $B_{30}=v_{3}$, $B_{40}=v_{4}$, as calculated previously. Hence every solution of $x$ can be written as $$x(t)=a\begin{bmatrix}-1 \\ 1 \\ 0 \\ 0\end{bmatrix}e^{-it}+b\begin{bmatrix}0 \\ 0 \\ -1 \\ 4\end{bmatrix}e^{-it}+c\begin{bmatrix}-1 \\ 1 \\ 0 \\ 0 \end{bmatrix}e^{it}+d\begin{bmatrix} 0 \\ 0 \\ 1 \\ 4 \end{bmatrix}e^{it} +j\begin{bmatrix} -1 \\ 1 \\ 0 \\ 0 \end{bmatrix}e^{-2it}+k\begin{bmatrix} 0 \\ 0 \\ -1 \\ 2 \end{bmatrix}e^{-2it}+m\begin{bmatrix} 1 \\ -1 \\ 0 \\ 0 \end{bmatrix}e^{2it}+n\begin{bmatrix}0 \\ 0 \\ 1 \\ -2 \end{bmatrix}e^{2it}$$ From this we can obtain four linearly independent solutions: $$x_{1}(t)=\begin{bmatrix}-e^{it} \\ e^{it} \\ 0 \\ 0\end{bmatrix}\qquad x_{2}(t)=\begin{bmatrix}e^{2it} \\ -e^{2it} \\ 0 \\ 0\end{bmatrix}\qquad x_{3}(t)=\begin{bmatrix}0 \\ 0 \\ e^{it} \\ 4e^{it}\end{bmatrix} \qquad x_{4}(t)=\begin{bmatrix} 0 \\ 0 \\ e^{2it} \\ -2e^{2it}\end{bmatrix}$$ The matrix $X$ is defined as $X=\begin{bmatrix}x_{1} & x_{2} & x_{3} & x_{4}\end{bmatrix}$ Then $e^{At}=\Phi(t):=X(t)X^{-1}(0)=\text{something really complicated, so I think I went wrong somewhere}$","Let $$A=\begin{bmatrix}0 & 1 & 0 & 0 \\ -1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & -4 & 0\end{bmatrix}$$ I want to determine $e^{At}$. I tried it using two methods. First via the Jordan form: The characteristic polynomial of $A$ is given by $$\det(I\xi-A)=\begin{vmatrix} \xi & -1 & 0 & 0 \\ 1 & \xi & 0 & 0 \\ 0 & 0 & \xi & -1 \\ 0 & 0 & 4 & \xi \end{vmatrix}=1\cdot\begin{vmatrix} 1 & 0 & 0 \\ 0 & \xi & -1 \\ 0 & 4 & \xi \end{vmatrix}+\xi\cdot\begin{vmatrix} \xi & 0 & 0 \\ 0 & \xi & -1 \\ 0 & 4 & \xi \end{vmatrix}$$ $$\begin{vmatrix} 1 & 0 & 0 \\ 0 & \xi & -1 \\ 0 & 4 & \xi \end{vmatrix}=1\cdot\begin{vmatrix} \xi & -1 \\ 4 & \xi \end{vmatrix}=\xi^{2}+4\qquad\text{and}\qquad \begin{vmatrix} \xi & 0 & 0 \\ 0 & \xi & -1 \\ 0 & 4 & \xi \end{vmatrix}=\xi\cdot\begin{vmatrix} \xi & -1 \\ 4 & \xi \end{vmatrix}=\xi(\xi^{2}+4)$$ $$\implies\det(I\xi-A)=\xi^{2}+4+\xi(\xi^{3}+4\xi)=\xi^{4}+5\xi^{2}+4=(\xi^{2}+4)(\xi^{2}+1)$$ Then the eigenvalues are given by $\lambda_{1,2}=\pm i$ and $\lambda_{3,4}=\pm 2i$. Now, computing the corresponding eigenvectors: For $\lambda=i$, we have $$\begin{cases} -ix+y=0 \\ -x-iy=0 \\ iz+w=0 \\ -4z-iw=0 \end{cases}\implies\begin{cases} x=-y \\ 4z=-w \end{cases}$$ Then setting $x=a$ and $z=b$, for $a,b\in\mathbb{C}$, we obtain: $$v_{1}=\begin{pmatrix} a \\ -a \\ b \\ -4b \end{pmatrix}=a\begin{pmatrix} 1 \\ -1 \\ 0 \\ 0 \end{pmatrix}+b\begin{pmatrix} 0 \\ 0 \\ 1 \\ -4 \end{pmatrix}$$ Similarly, for $\lambda=-i$, we compute, for complex $c,d$: $$v_{2}=c\begin{pmatrix} -1 \\ 1 \\ 0 \\ 0 \end{pmatrix}+d\begin{pmatrix} 0 \\ 0 \\ -1 \\ 4 \end{pmatrix}$$ For $\lambda=2i$, we get: $$v_{3}=j\begin{pmatrix} 1 \\ -1 \\ 0 \\ 0 \end{pmatrix}+k\begin{pmatrix} 0 \\ 0 \\ 1 \\ -2 \end{pmatrix}$$ And finally, for $\lambda=-2i$: $$v_{4}=m\begin{pmatrix} -1 \\ 1 \\ 0 \\ 0 \end{pmatrix}+n\begin{pmatrix} 0 \\ 0 \\ -1 \\ 2 \end{pmatrix}$$ Define $S=\begin{bmatrix} v_{1} & v_{2} & v_{3} & v_{4} \end{bmatrix}$. Then $$S=\begin{bmatrix} a & -c & j & -m \\ -a & c & -j & m \\ b & -d & k & -n \\ -4b & 4d & -2k & 2n \end{bmatrix}$$ But I have to calculate $S^{-1}$ via this method (which seems a bit too complicated), since $e^{At}=Se^{S^{-1}ASt}S^{-1}$. Then I tried to compute $e^{At}$ using the theory of autonomous behaviours: So I said that every strong solution of $\frac{d}{dt}x=Ax$ is of the form $$x(t)=B_{10}e^{-it}+B_{20}e^{it}+B_{30}e^{-2it}+B_{40}e^{2it}$$ The vectors $B_{ij}$ should satisfy the relations $$(-i\cdot I-A)B_{10}=0$$ $$(i\cdot I-A)B_{20}=0$$ $$(-2i\cdot I-A)B_{30}=0$$ $$(2i\cdot I-A)B_{40}=0$$ Solving these equations yield $B_{10}=v_{1}$, $B_{20}=v_{2}$, $B_{30}=v_{3}$, $B_{40}=v_{4}$, as calculated previously. Hence every solution of $x$ can be written as $$x(t)=a\begin{bmatrix}-1 \\ 1 \\ 0 \\ 0\end{bmatrix}e^{-it}+b\begin{bmatrix}0 \\ 0 \\ -1 \\ 4\end{bmatrix}e^{-it}+c\begin{bmatrix}-1 \\ 1 \\ 0 \\ 0 \end{bmatrix}e^{it}+d\begin{bmatrix} 0 \\ 0 \\ 1 \\ 4 \end{bmatrix}e^{it} +j\begin{bmatrix} -1 \\ 1 \\ 0 \\ 0 \end{bmatrix}e^{-2it}+k\begin{bmatrix} 0 \\ 0 \\ -1 \\ 2 \end{bmatrix}e^{-2it}+m\begin{bmatrix} 1 \\ -1 \\ 0 \\ 0 \end{bmatrix}e^{2it}+n\begin{bmatrix}0 \\ 0 \\ 1 \\ -2 \end{bmatrix}e^{2it}$$ From this we can obtain four linearly independent solutions: $$x_{1}(t)=\begin{bmatrix}-e^{it} \\ e^{it} \\ 0 \\ 0\end{bmatrix}\qquad x_{2}(t)=\begin{bmatrix}e^{2it} \\ -e^{2it} \\ 0 \\ 0\end{bmatrix}\qquad x_{3}(t)=\begin{bmatrix}0 \\ 0 \\ e^{it} \\ 4e^{it}\end{bmatrix} \qquad x_{4}(t)=\begin{bmatrix} 0 \\ 0 \\ e^{2it} \\ -2e^{2it}\end{bmatrix}$$ The matrix $X$ is defined as $X=\begin{bmatrix}x_{1} & x_{2} & x_{3} & x_{4}\end{bmatrix}$ Then $e^{At}=\Phi(t):=X(t)X^{-1}(0)=\text{something really complicated, so I think I went wrong somewhere}$",,"['linear-algebra', 'matrices', 'exponential-function', 'control-theory']"
6,How can I tell that my matrix is nilpotent?,How can I tell that my matrix is nilpotent?,,"I just computed a 15x15 matrix by hand :( It is not upper triangular as I hoped it would be.  But my computations agree with what's offered in the student solution. My question is:  the solution then says ""this matrix is nilpotent, so all the eigenvalues are zero."" I get the part where the spectrum = {0} i.f.f. the operator is nilpotent, but how can I tell it actually is nilpotent, just by observing the matrix that I got? EDIT: Here is my computation of the matrix: The vector space is the space of polynomials in two variables x,y, of degree less than or equal to 4. So, I let the basis for this space be the set $${ 1,x,x^2,x^3,x^4, y, y^2, y^3, y^4, xy, xy^2, x^2y, xy^3, x^2y^2, x^3y }$$ The operator is a Laplacian operator on polynomials: f(x,y) --> f(x+1,y) + f(x-1,y) + f(x,y-1) + f(x,y+1) - 4f(x,y), for f(x,y) in V. With the above (ordered) basis, I applied this Laplacian operator to each basis element.  I wrote each result as a linear combination of the ordered basis.  Now, taking the transpose of the coefficients, the matrix w.r.t. the ordered basis is: $$         \begin{bmatrix}          0 & 0 & 2 & 0 & 2 & 0 & 2 & 0 & 2   & 0 & 0 & 0 & 0 & 0 & 0 \\          0 & 0 & 0 & 6 & 0 & 0 & 0 & 0 & 0   & 0 & 2 & 0 & 0 & 0 & 0 \\          0 & 0 & 0 & 0 & 12& 0 & 0 & 0 & 0   & 0 & 0 & 0 & 0 & 2 & 0 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0   & 0 & 0 & 0 & 0 & 0 & 0 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0   & 0 & 0 & 0 & 0 & 0 & 0 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 6 & 0   & 0 & 0 & 2 & 0 & 0 & 0 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 12   & 0 & 0 & 0 & 0 & 2 & 0 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0   & 0 & 0 & 0 & 0 & 0 & 0 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0   & 0 & 0 & 0 & 0 & 0 & 0 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0   & 0 & 0 & 0 & 6 & 0 & 6 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0   & 0 & 0 & 0 & 0 & 0 & 0 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0   & 0 & 0 & 0 & 0 & 0 & 0 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0   & 0 & 0 & 0 & 0 & 0 & 0 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0   & 0 & 0 & 0 & 0 & 0 & 0 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0   & 0 & 0 & 0 & 0 & 0 & 0 \\ \end{bmatrix} $$ And thanks to the commenters (below), I now see that my matrix is indeed nilpotent, since it is upper triangular, with zeros on the main diagonal; in my initial sketch of the matrix - I was way off. :-) (I had thought that the 6's were below the diagonal.) Thanks,","I just computed a 15x15 matrix by hand :( It is not upper triangular as I hoped it would be.  But my computations agree with what's offered in the student solution. My question is:  the solution then says ""this matrix is nilpotent, so all the eigenvalues are zero."" I get the part where the spectrum = {0} i.f.f. the operator is nilpotent, but how can I tell it actually is nilpotent, just by observing the matrix that I got? EDIT: Here is my computation of the matrix: The vector space is the space of polynomials in two variables x,y, of degree less than or equal to 4. So, I let the basis for this space be the set $${ 1,x,x^2,x^3,x^4, y, y^2, y^3, y^4, xy, xy^2, x^2y, xy^3, x^2y^2, x^3y }$$ The operator is a Laplacian operator on polynomials: f(x,y) --> f(x+1,y) + f(x-1,y) + f(x,y-1) + f(x,y+1) - 4f(x,y), for f(x,y) in V. With the above (ordered) basis, I applied this Laplacian operator to each basis element.  I wrote each result as a linear combination of the ordered basis.  Now, taking the transpose of the coefficients, the matrix w.r.t. the ordered basis is: $$         \begin{bmatrix}          0 & 0 & 2 & 0 & 2 & 0 & 2 & 0 & 2   & 0 & 0 & 0 & 0 & 0 & 0 \\          0 & 0 & 0 & 6 & 0 & 0 & 0 & 0 & 0   & 0 & 2 & 0 & 0 & 0 & 0 \\          0 & 0 & 0 & 0 & 12& 0 & 0 & 0 & 0   & 0 & 0 & 0 & 0 & 2 & 0 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0   & 0 & 0 & 0 & 0 & 0 & 0 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0   & 0 & 0 & 0 & 0 & 0 & 0 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 6 & 0   & 0 & 0 & 2 & 0 & 0 & 0 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 12   & 0 & 0 & 0 & 0 & 2 & 0 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0   & 0 & 0 & 0 & 0 & 0 & 0 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0   & 0 & 0 & 0 & 0 & 0 & 0 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0   & 0 & 0 & 0 & 6 & 0 & 6 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0   & 0 & 0 & 0 & 0 & 0 & 0 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0   & 0 & 0 & 0 & 0 & 0 & 0 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0   & 0 & 0 & 0 & 0 & 0 & 0 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0   & 0 & 0 & 0 & 0 & 0 & 0 \\          0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0   & 0 & 0 & 0 & 0 & 0 & 0 \\ \end{bmatrix} $$ And thanks to the commenters (below), I now see that my matrix is indeed nilpotent, since it is upper triangular, with zeros on the main diagonal; in my initial sketch of the matrix - I was way off. :-) (I had thought that the 6's were below the diagonal.) Thanks,",,"['linear-algebra', 'eigenvalues-eigenvectors', 'spectral-theory', 'nilpotence']"
7,"Tensor, exterior, symmetric powers over fields of nonzero characteristic","Tensor, exterior, symmetric powers over fields of nonzero characteristic",,"I was reading Fulton and Harris' discussion of exterior and symmetric products as quotient spaces of tensor products in their rep theory book when I noticed that they made this claim (the emphasis is mine): The exterior powers $ \bigwedge^n V $ and symmetric powers $ \operatorname{Sym}^n V $ can also be realized as subspaces of $ V^{\otimes n} $, assuming, as we have throughout, that the ground field has characteristic 0. Why must the ground field be characteristic zero for this to be true? In particular, I don't see any problem with the quotient construction of the exterior and symmetric products for a vector space with nonzero characteristic; if there is no problem with this construction, then the inclusion of these products into the tensor product ought to be well-defined as well. Am I missing something obvious here?","I was reading Fulton and Harris' discussion of exterior and symmetric products as quotient spaces of tensor products in their rep theory book when I noticed that they made this claim (the emphasis is mine): The exterior powers $ \bigwedge^n V $ and symmetric powers $ \operatorname{Sym}^n V $ can also be realized as subspaces of $ V^{\otimes n} $, assuming, as we have throughout, that the ground field has characteristic 0. Why must the ground field be characteristic zero for this to be true? In particular, I don't see any problem with the quotient construction of the exterior and symmetric products for a vector space with nonzero characteristic; if there is no problem with this construction, then the inclusion of these products into the tensor product ought to be well-defined as well. Am I missing something obvious here?",,"['linear-algebra', 'abstract-algebra', 'multilinear-algebra']"
8,Commutative property of matrix multiplication (or lack thereof),Commutative property of matrix multiplication (or lack thereof),,"Assuming $A$ and $B$ are invertible matrices and are of proper dimensions to be multiplied (say, $2\times2$), is the following expression correct for all examples of matrices $A$ and $B$? $$(A^{-1}B)(AB^{-1}) = A^{-1}BAB^{-1} = A^{-1}AB^{-1}B = I^2 = I$$ My understanding is that for matrices $A$ and $B$, $AB$ doesn't necessarily equal $BA$ as matrix multiplication is not commutative. I'm trying to simplify the below expression: $$(AB)^{-1}(AC^{-1})(D^{-1}C^{-1})^{-1}D^{-1}$$ Nothing is given about the matrices $A$, $B$, $C$, or $D$ beyond that they are invertible and of correct dimensions such that any matrix multiplication is possible. My process is as below: $$\begin{align}(AB)^{-1}(AC^{-1})(D^{-1}C^{-1})^{-1}D^{-1} &= (A^{-1}B^{-1})(AC^{-1})(DC)D^{-1}\\ &= A^{-1}B^{-1}AC^{-1}DCD^{-1}\\ &= B^{-1}A^{-1}AC^{-1}CDD^{-1}\\ &= B^{-1}I^3\\ &= B^{-1}\end{align}$$ You'll notice the error I've made here: $(AB)^{-1} = (B^{-1}A^{-1})$, not $(A^{-1}B^{-1})$, but in the end it doesn't change the answer. The answer in the textbook is indeed $B^{-1}$. According to the above: $$(B^{-1}A^{-1}) = (A^{-1}B^{-1})$$ But matrices are not commutative? Why does the algebra suggest they are?","Assuming $A$ and $B$ are invertible matrices and are of proper dimensions to be multiplied (say, $2\times2$), is the following expression correct for all examples of matrices $A$ and $B$? $$(A^{-1}B)(AB^{-1}) = A^{-1}BAB^{-1} = A^{-1}AB^{-1}B = I^2 = I$$ My understanding is that for matrices $A$ and $B$, $AB$ doesn't necessarily equal $BA$ as matrix multiplication is not commutative. I'm trying to simplify the below expression: $$(AB)^{-1}(AC^{-1})(D^{-1}C^{-1})^{-1}D^{-1}$$ Nothing is given about the matrices $A$, $B$, $C$, or $D$ beyond that they are invertible and of correct dimensions such that any matrix multiplication is possible. My process is as below: $$\begin{align}(AB)^{-1}(AC^{-1})(D^{-1}C^{-1})^{-1}D^{-1} &= (A^{-1}B^{-1})(AC^{-1})(DC)D^{-1}\\ &= A^{-1}B^{-1}AC^{-1}DCD^{-1}\\ &= B^{-1}A^{-1}AC^{-1}CDD^{-1}\\ &= B^{-1}I^3\\ &= B^{-1}\end{align}$$ You'll notice the error I've made here: $(AB)^{-1} = (B^{-1}A^{-1})$, not $(A^{-1}B^{-1})$, but in the end it doesn't change the answer. The answer in the textbook is indeed $B^{-1}$. According to the above: $$(B^{-1}A^{-1}) = (A^{-1}B^{-1})$$ But matrices are not commutative? Why does the algebra suggest they are?",,"['linear-algebra', 'matrices']"
9,$A$ is diagonalizable if $A^8+A^2=I$,is diagonalizable if,A A^8+A^2=I,"Given a matrix $A\in M_{n}(\mathbb{C})$ such that $A^8+A^2=I$, prove that $A$ is diagonalizable. So let $p(x)=x^8+x^2-1$ and we know that $p(A)=0$. The next step would be to show that the algebric and geometric multipliciteis of all the eigenvalues are equal. But this polynomial is reducible in a very unpleasent way, so even checking for the minimal polynomial is not an option. What can I do differently.","Given a matrix $A\in M_{n}(\mathbb{C})$ such that $A^8+A^2=I$, prove that $A$ is diagonalizable. So let $p(x)=x^8+x^2-1$ and we know that $p(A)=0$. The next step would be to show that the algebric and geometric multipliciteis of all the eigenvalues are equal. But this polynomial is reducible in a very unpleasent way, so even checking for the minimal polynomial is not an option. What can I do differently.",,['linear-algebra']
10,"Prove you can choose orthonormal bases of any two subspaces of Euclidean space such that $(e_i, f_j)=0$ if $ i\neq j $",Prove you can choose orthonormal bases of any two subspaces of Euclidean space such that  if,"(e_i, f_j)=0  i\neq j ","Prove you can choose orthonormal bases $(e_1,...,e_k)$ and $(f_1,...,f_j)$ of any two subspaces of Euclidean space such that $(e_i, f_j)=0$ if $i\neq j$ and $(e_i, f_j) \geq 0$ This is a question from a question bank my teacher linked to us. The way I want to approach it is to let $(e_1,...,e_k)$ be the standard basis and then define some sort of map that will ensure $(e_i, f_j)=0$ $\forall$ $i\neq j$ I'm struggling with being able to come up with this map on my own. Any help would be appreciated.","Prove you can choose orthonormal bases $(e_1,...,e_k)$ and $(f_1,...,f_j)$ of any two subspaces of Euclidean space such that $(e_i, f_j)=0$ if $i\neq j$ and $(e_i, f_j) \geq 0$ This is a question from a question bank my teacher linked to us. The way I want to approach it is to let $(e_1,...,e_k)$ be the standard basis and then define some sort of map that will ensure $(e_i, f_j)=0$ $\forall$ $i\neq j$ I'm struggling with being able to come up with this map on my own. Any help would be appreciated.",,"['linear-algebra', 'abstract-algebra']"
11,"Orthogonal complement of subspace of continuous functions and odd in the interval $[-1,1]$.",Orthogonal complement of subspace of continuous functions and odd in the interval .,"[-1,1]","The follow question was found on the Hoffman's book. Let $V$ be the real inner product space consisting  of the space of real-valued continuous functions on the interval, $-1\leq t \leq 1$, with the inner product $(f|g)=\displaystyle \int_{-1}^{1} {f(t)g(t)}dt$ Let $W$ be the subspace odd functions, ie, functions satisfying $f(-t)=-f(t)$. Find the orthogonal complement of $W$. I suppose that orthogonal complement of $W$ is the subspace of functions that satisfy $f(t)=f(-t)$. Someone have ideas to proof that?","The follow question was found on the Hoffman's book. Let $V$ be the real inner product space consisting  of the space of real-valued continuous functions on the interval, $-1\leq t \leq 1$, with the inner product $(f|g)=\displaystyle \int_{-1}^{1} {f(t)g(t)}dt$ Let $W$ be the subspace odd functions, ie, functions satisfying $f(-t)=-f(t)$. Find the orthogonal complement of $W$. I suppose that orthogonal complement of $W$ is the subspace of functions that satisfy $f(t)=f(-t)$. Someone have ideas to proof that?",,['linear-algebra']
12,"Calculate a matrix to the power of ""n"" given an eigenvector","Calculate a matrix to the power of ""n"" given an eigenvector",,"I have a question that I simply cannot solve. I do not want a direct answer to the question but simply an explanation as to the steps one would take to go about solving it, that way I can try it myself. To avoid getting an answer to the problem I wish to solve, I will use variables in place of actual matrices. Calculate $A^n$x where both A and x are known , and x is an eigenvector of A . A is a 2-by-2 matrix. For this question, I'm supposed to use the idea of linear combinations, which is what confuses me, I assume that means I'm supposed to solve it using the following format: (This is my take on it, it is most likely wrong) First find $\lambda$ -->  Ax = $\lambda$x Then --> x = a $\lambda$x Solve for a and then do something like --> $A^n$ = a$\lambda^n$x I'm really unsure as to how to solve this, can someone point me in the right direction. Perhaps show me step by step what I need to do.","I have a question that I simply cannot solve. I do not want a direct answer to the question but simply an explanation as to the steps one would take to go about solving it, that way I can try it myself. To avoid getting an answer to the problem I wish to solve, I will use variables in place of actual matrices. Calculate $A^n$x where both A and x are known , and x is an eigenvector of A . A is a 2-by-2 matrix. For this question, I'm supposed to use the idea of linear combinations, which is what confuses me, I assume that means I'm supposed to solve it using the following format: (This is my take on it, it is most likely wrong) First find $\lambda$ -->  Ax = $\lambda$x Then --> x = a $\lambda$x Solve for a and then do something like --> $A^n$ = a$\lambda^n$x I'm really unsure as to how to solve this, can someone point me in the right direction. Perhaps show me step by step what I need to do.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
13,What is the limit of the rank of the power of a matrix?,What is the limit of the rank of the power of a matrix?,,"I am interested in $$\lim_{k \to \infty} \operatorname{rank} \left( \mathbf{A}^k \right)$$ for a $n \times n$ matrix $\mathbf{A}$ . I know that for a nilpotent matrix, $\mathbf{A}^k=0$ when $k$ is big enough, which means $\operatorname{rank} (\mathbf{A}^k) \to 0$ when $k \to \infty$ . for a nonsingular matrix, $\mathbf{A}^k$ is also nonsingular and $\operatorname{rank} \left( \mathbf{A}^k \right)$ is the number of the columns/rows. Is there any general theorem that tells us the result for a general square matrix?","I am interested in for a matrix . I know that for a nilpotent matrix, when is big enough, which means when . for a nonsingular matrix, is also nonsingular and is the number of the columns/rows. Is there any general theorem that tells us the result for a general square matrix?",\lim_{k \to \infty} \operatorname{rank} \left( \mathbf{A}^k \right) n \times n \mathbf{A} \mathbf{A}^k=0 k \operatorname{rank} (\mathbf{A}^k) \to 0 k \to \infty \mathbf{A}^k \operatorname{rank} \left( \mathbf{A}^k \right),"['linear-algebra', 'matrices', 'matrix-rank']"
14,Embedding of finite groups into general linear group,Embedding of finite groups into general linear group,,It's clear that for any field $\mathbb{F}$ any finite group $G$ can be embedded into $GL_{n}(\mathbb{F})$ for some $n$. My question is about one modification of this result. Let's fix positive integer $N$. Is it true that for any finite group $G$ there exist a field $\mathbb{F}$ such that $G$ is embeddable into $GL_N(\mathbb{F})$? UPD: For $N=1$ it's false. Let's consider $N>1$.,It's clear that for any field $\mathbb{F}$ any finite group $G$ can be embedded into $GL_{n}(\mathbb{F})$ for some $n$. My question is about one modification of this result. Let's fix positive integer $N$. Is it true that for any finite group $G$ there exist a field $\mathbb{F}$ such that $G$ is embeddable into $GL_N(\mathbb{F})$? UPD: For $N=1$ it's false. Let's consider $N>1$.,,['linear-algebra']
15,Average value of a bilinear map on a Euclidean sphere,Average value of a bilinear map on a Euclidean sphere,,"Let $(V, g = \langle \cdot, \cdot \rangle)$ be a Euclidean vector space and let $B \colon V \times V \to \mathbb{R}$ be a symmetric bilinear form. Denote by $S_r = S(0,r)$ the sphere centered at the origin in $V$ with radius $r > 0$, $\sigma_r$ the volume element on $S_r$ induced from the metric $g$ and $\operatorname{vol}(S_r) = \int_{S_r} \sigma_r$ its volume. How can one prove that: \begin{equation}  \frac{1}{\operatorname{vol}(S_r)}\int_{S_r} B(x, x) \, \sigma_r = \frac{r^2}{\dim V}\operatorname{tr}_g(B)~. \end{equation} Here we have denoted by $\operatorname{tr}_g(B)$ the $g$-trace of $B$, i.e. the trace of the $g$-self adjoint endomorphism of $V$ associated to $B$. It is also equal to the  trace of the matrix representing $B$ in a $g$-orthonormal basis.","Let $(V, g = \langle \cdot, \cdot \rangle)$ be a Euclidean vector space and let $B \colon V \times V \to \mathbb{R}$ be a symmetric bilinear form. Denote by $S_r = S(0,r)$ the sphere centered at the origin in $V$ with radius $r > 0$, $\sigma_r$ the volume element on $S_r$ induced from the metric $g$ and $\operatorname{vol}(S_r) = \int_{S_r} \sigma_r$ its volume. How can one prove that: \begin{equation}  \frac{1}{\operatorname{vol}(S_r)}\int_{S_r} B(x, x) \, \sigma_r = \frac{r^2}{\dim V}\operatorname{tr}_g(B)~. \end{equation} Here we have denoted by $\operatorname{tr}_g(B)$ the $g$-trace of $B$, i.e. the trace of the $g$-self adjoint endomorphism of $V$ associated to $B$. It is also equal to the  trace of the matrix representing $B$ in a $g$-orthonormal basis.",,"['linear-algebra', 'integration', 'eigenvalues-eigenvectors', 'harmonic-functions', 'bilinear-form']"
16,Equality case in the Frobenius rank inequality,Equality case in the Frobenius rank inequality,,"In many linear algebra books, the following rank inequalities are found: Frobenius inequality Let $A$ , $B$ and $C$ be three matrices such that the product $ABC$ is defined.   Then $$\operatorname{rk}(ABC) + \operatorname{rk}(B) \geq \operatorname{rk}(AB) + \operatorname{rk}(BC).$$ In the special case case $B = I$ , the Frobenius inequality reduces to the Sylvester inequality Let $A$ and $B$ be two matrices such that the product $AB$ is defined. Then $$\operatorname{rk}(A) + \operatorname{rk}(B) - n \leq \operatorname{rk}(AB).$$ Now I wonder about the equality cases in those inequalities. It is common knowledge that In the Sylvester inequality, equality holds if and only if $$\ker(A) \subseteq  \operatorname{Im}(B).$$ But I didn't find anything on the Frobenius inequality. So my question is: How can the equality case in the Frobenius inequality be characterized?","In many linear algebra books, the following rank inequalities are found: Frobenius inequality Let , and be three matrices such that the product is defined.   Then In the special case case , the Frobenius inequality reduces to the Sylvester inequality Let and be two matrices such that the product is defined. Then Now I wonder about the equality cases in those inequalities. It is common knowledge that In the Sylvester inequality, equality holds if and only if But I didn't find anything on the Frobenius inequality. So my question is: How can the equality case in the Frobenius inequality be characterized?",A B C ABC \operatorname{rk}(ABC) + \operatorname{rk}(B) \geq \operatorname{rk}(AB) + \operatorname{rk}(BC). B = I A B AB \operatorname{rk}(A) + \operatorname{rk}(B) - n \leq \operatorname{rk}(AB). \ker(A) \subseteq  \operatorname{Im}(B).,"['linear-algebra', 'matrices', 'inequality', 'matrix-rank']"
17,Composition of two orthogonal projections,Composition of two orthogonal projections,,"Let $V$ be a finite dimensional Euclidean space and let $W_1,W_2$ be two subspaces of $V$. Let $P_1,P_2$ denote the projections onto $W_1,W_2$ respectively. Is it true that the composition $P_1\circ P_2$ is always diagonalizable?","Let $V$ be a finite dimensional Euclidean space and let $W_1,W_2$ be two subspaces of $V$. Let $P_1,P_2$ denote the projections onto $W_1,W_2$ respectively. Is it true that the composition $P_1\circ P_2$ is always diagonalizable?",,"['linear-algebra', 'hilbert-spaces']"
18,Why diagonal matrix SVD sorted from largest to smallest value?,Why diagonal matrix SVD sorted from largest to smallest value?,,"Why diagonal matrix SVD sorted from largest to smallest value? D is diagonal matrix, $D=(d_1 \ge ,d_2 \ge ,..., \ge d_L)$. Whether there is a journal that could explain this?","Why diagonal matrix SVD sorted from largest to smallest value? D is diagonal matrix, $D=(d_1 \ge ,d_2 \ge ,..., \ge d_L)$. Whether there is a journal that could explain this?",,"['linear-algebra', 'matrices', 'svd']"
19,Linear algebra notation involving $\preceq$,Linear algebra notation involving,\preceq,"I am trying to read a paper which involves some linear algebra. I would appreciate if anyone could clarify what the below statements exactly mean. 1-) $0 \preceq B^TB \preceq A^TA$ where $A \in \mathbb{R}^{n \times m}$ and $B \in \mathbb{R}^{l \times m}$ are matrices. 2-) $\langle A_i,x \rangle^2$ where $A_i$ and $x$ are vectors","I am trying to read a paper which involves some linear algebra. I would appreciate if anyone could clarify what the below statements exactly mean. 1-) $0 \preceq B^TB \preceq A^TA$ where $A \in \mathbb{R}^{n \times m}$ and $B \in \mathbb{R}^{l \times m}$ are matrices. 2-) $\langle A_i,x \rangle^2$ where $A_i$ and $x$ are vectors",,"['linear-algebra', 'notation']"
20,Prove that the square sum of eigenvalues is no more than the Frobenius norm for a square complex matrix,Prove that the square sum of eigenvalues is no more than the Frobenius norm for a square complex matrix,,"Let $ \boldsymbol{A}=(a_{ij})_{n\times n} $ be a complex square matrix with eigenvalues: $\lambda_1 ,\lambda_2, \dots , \lambda_n $ .  Prove that $$ \sum_{r=1}^{n} |\lambda_r|^2 \le \sum_{i,j=1}^{n} |a_{ij}|^2\,.$$ Furthermore, show that the equality holds if and only if $$\boldsymbol{A^H A=AA^H}\,.$$ Here, $\boldsymbol{A^H}$ is the conjugate transpose of $\boldsymbol{A}$ . We can write $$\sum_{i,j=1}^n\,|a_{ij}|^2=\operatorname{Tr}(\boldsymbol{AA^H})=\|\boldsymbol{A}\|_F^2,$$ where $\|\_\|_F$ is the Frobenius norm.  So this inequality gives a lower bound on $\|\boldsymbol{A}\|_F$ , namely $$\|\boldsymbol{A}\|_F\geq \sqrt{\sum_{r=1}^n\,|\lambda_r|^2}\,.$$ Edit. Another proof of the inequality is found here .  However, there is no discussion about the equality case.","Let be a complex square matrix with eigenvalues: .  Prove that Furthermore, show that the equality holds if and only if Here, is the conjugate transpose of . We can write where is the Frobenius norm.  So this inequality gives a lower bound on , namely Edit. Another proof of the inequality is found here .  However, there is no discussion about the equality case."," \boldsymbol{A}=(a_{ij})_{n\times n}  \lambda_1 ,\lambda_2, \dots , \lambda_n   \sum_{r=1}^{n} |\lambda_r|^2 \le \sum_{i,j=1}^{n} |a_{ij}|^2\,. \boldsymbol{A^H A=AA^H}\,. \boldsymbol{A^H} \boldsymbol{A} \sum_{i,j=1}^n\,|a_{ij}|^2=\operatorname{Tr}(\boldsymbol{AA^H})=\|\boldsymbol{A}\|_F^2, \|\_\|_F \|\boldsymbol{A}\|_F \|\boldsymbol{A}\|_F\geq \sqrt{\sum_{r=1}^n\,|\lambda_r|^2}\,.","['linear-algebra', 'matrices', 'inequality', 'eigenvalues-eigenvectors', 'normed-spaces']"
21,What's a non-zero (column) vector?,What's a non-zero (column) vector?,,"This is a basic question, but I can't find the definition on wikipedia, google, or math.stackexchange, because I only find examples of it being used in problems. Therefore, I want to clarify: Does a non-zero vector have at least one non-zero entry or must all the entries be non-zero? Example from wikipedia: ""M is said to be positive definite if z'Mz is positive for any non-zero column vector z of n real numbers""","This is a basic question, but I can't find the definition on wikipedia, google, or math.stackexchange, because I only find examples of it being used in problems. Therefore, I want to clarify: Does a non-zero vector have at least one non-zero entry or must all the entries be non-zero? Example from wikipedia: ""M is said to be positive definite if z'Mz is positive for any non-zero column vector z of n real numbers""",,['linear-algebra']
22,If three corners of a parallelogram are known solve for the 3 possible 4th corners.,If three corners of a parallelogram are known solve for the 3 possible 4th corners.,,"An example would be three corners being the points: (1,1), (4,2) and (1,3). I understand the specific solution for this example: (4,4), (4,0) or (-2,2). Which I reasoned when i drew it out. The example came from a linear algebra textbook and i'm curious if it can be done some other way.","An example would be three corners being the points: (1,1), (4,2) and (1,3). I understand the specific solution for this example: (4,4), (4,0) or (-2,2). Which I reasoned when i drew it out. The example came from a linear algebra textbook and i'm curious if it can be done some other way.",,['linear-algebra']
23,The annihilator of an intersection is the sum of annihilators,The annihilator of an intersection is the sum of annihilators,,"Given a subset $X$ of a vector space $V$, let $X^\circ$ be the annihilator of $X$, that is $X^\circ = \{y\in V^* | \; y(x)=0, \;\forall\; x\in X\}$, where $V^*$ is the dual space of $V$. Question: If $M$ and $N$ are subspaces of an $n$-dimensional vector space $V$, then $(M\cap N)^\circ =M^\circ +N^\circ $. It's easy to prove this result by using that $(M+N)^\circ =M^\circ \cap N^\circ $, which is easy to prove too. My doubt is: Is there a ""direct way"" to prove that $(M\cap N)^\circ =M^\circ +N^\circ $ (without make use of other equality)? Thanks!","Given a subset $X$ of a vector space $V$, let $X^\circ$ be the annihilator of $X$, that is $X^\circ = \{y\in V^* | \; y(x)=0, \;\forall\; x\in X\}$, where $V^*$ is the dual space of $V$. Question: If $M$ and $N$ are subspaces of an $n$-dimensional vector space $V$, then $(M\cap N)^\circ =M^\circ +N^\circ $. It's easy to prove this result by using that $(M+N)^\circ =M^\circ \cap N^\circ $, which is easy to prove too. My doubt is: Is there a ""direct way"" to prove that $(M\cap N)^\circ =M^\circ +N^\circ $ (without make use of other equality)? Thanks!",,['linear-algebra']
24,Find the eigenvalues and eigenvectors of the matrix with all diagonal elements as $d$ and rest $1$,Find the eigenvalues and eigenvectors of the matrix with all diagonal elements as  and rest,d 1,A matrix has all elements 1 except the diagonal elements. It is an $n\times n$ matrix. What are the eigenvectors and eigenvalues ? Solving book problems in Strang book and stuck on this one and I have no idea where to begin ?,A matrix has all elements 1 except the diagonal elements. It is an $n\times n$ matrix. What are the eigenvectors and eigenvalues ? Solving book problems in Strang book and stuck on this one and I have no idea where to begin ?,,['linear-algebra']
25,Are there other Identity Matrices?,Are there other Identity Matrices?,,"Is there only one identity matrix $$\begin{pmatrix} 1&0&...&...&0\\0&1&0&...&0\\...&0&1&...&0\\...&...&0&1&0\\...&...&...&0&1\end{pmatrix}$$ etc.. Or are there different identity matrices for other bases? A textbook example asks if $[T]_{\beta} = I$ (the $n\times n$ identity matrix) for some basis $\beta$, is $T$ the identity operator?","Is there only one identity matrix $$\begin{pmatrix} 1&0&...&...&0\\0&1&0&...&0\\...&0&1&...&0\\...&...&0&1&0\\...&...&...&0&1\end{pmatrix}$$ etc.. Or are there different identity matrices for other bases? A textbook example asks if $[T]_{\beta} = I$ (the $n\times n$ identity matrix) for some basis $\beta$, is $T$ the identity operator?",,['linear-algebra']
26,Understanding a proof of corollary of Farkas lemma,Understanding a proof of corollary of Farkas lemma,,"I'm trying to understand a proof of a corollary to the Farkas lemma in some lecture notes. For completeness sake I'll first state the Farkas lemma and then the corollary + proof, as stated in these lecture notes. (Farkas lemma) Given $A \in \mathbb{R}^{m\times n}$, $b \in \mathbb{R}^m$. Then $Ax = b$ has a nonnegative solution if and only if there is no vector $y$ satisfying $y^{T}A \geq 0$ and $y^{T}b < 0$. (Corollary) The system $Ax \leq b$ has a solution if and only if there is no vector $y$ satisfying $y \geq 0$, $y^T A = 0$ and $y^T b < 0$. (Proof of corollary) Let $A'$ be the matrix $A' :=[A\quad -A\quad  I]$, where $I$ is the identity matrix. Then $Ax \leq b$ has a solution $x$ iff the system $A'x' = b$ has a nonnegative solution $x'$. Applying the Farkas lemma to the latter system gives the corollary. Here, I parse $[A\quad -A\quad  I]$ as being the matrix created by appending $A$, $-A$ and $I$. What I don't understand is how I should calculate $A'x'$ and of what dimension $x'$ should be. It seems to me that $A'$ has dimensions $m \times 3n$ so that $x'$ has dimensions $3n \times 1$, but this isn't right since $b$ is of dimension $m \times 1$. I'm guessing I'm missing something really obvious but alas I can't see it.","I'm trying to understand a proof of a corollary to the Farkas lemma in some lecture notes. For completeness sake I'll first state the Farkas lemma and then the corollary + proof, as stated in these lecture notes. (Farkas lemma) Given $A \in \mathbb{R}^{m\times n}$, $b \in \mathbb{R}^m$. Then $Ax = b$ has a nonnegative solution if and only if there is no vector $y$ satisfying $y^{T}A \geq 0$ and $y^{T}b < 0$. (Corollary) The system $Ax \leq b$ has a solution if and only if there is no vector $y$ satisfying $y \geq 0$, $y^T A = 0$ and $y^T b < 0$. (Proof of corollary) Let $A'$ be the matrix $A' :=[A\quad -A\quad  I]$, where $I$ is the identity matrix. Then $Ax \leq b$ has a solution $x$ iff the system $A'x' = b$ has a nonnegative solution $x'$. Applying the Farkas lemma to the latter system gives the corollary. Here, I parse $[A\quad -A\quad  I]$ as being the matrix created by appending $A$, $-A$ and $I$. What I don't understand is how I should calculate $A'x'$ and of what dimension $x'$ should be. It seems to me that $A'$ has dimensions $m \times 3n$ so that $x'$ has dimensions $3n \times 1$, but this isn't right since $b$ is of dimension $m \times 1$. I'm guessing I'm missing something really obvious but alas I can't see it.",,['linear-algebra']
27,Unitary equivalence and eigenvalues,Unitary equivalence and eigenvalues,,"After a search I did find threads with similar topics, but none with exactly what I want to know. A matrix $A$ is complex and normal [real and symmetric] if and only if it is unitarily [orthogonally] equivalent to a [real] diagonal matrix. Write $A$ as $A=P^*DP$.  Then are the entries of $D$ the eigenvalues of $A$? Let's say $A-tI = Q^*D'Q$.  I believe this is still complex and normal [real and symmetric].  Of course we have $\det(A-tI)=\det(Q^*DQ)=\det(Q^*)\det(D')\det(Q)=\det(D')$, and this does the trick if the entries of $D'$ are $A_{ii}-t$, but I guess it's not clear to me that this is the case. I suspect this is true, because if it is then some nice things that I'm trying to prove follow from it (wishful thinking, I know).  So how do we prove it?","After a search I did find threads with similar topics, but none with exactly what I want to know. A matrix $A$ is complex and normal [real and symmetric] if and only if it is unitarily [orthogonally] equivalent to a [real] diagonal matrix. Write $A$ as $A=P^*DP$.  Then are the entries of $D$ the eigenvalues of $A$? Let's say $A-tI = Q^*D'Q$.  I believe this is still complex and normal [real and symmetric].  Of course we have $\det(A-tI)=\det(Q^*DQ)=\det(Q^*)\det(D')\det(Q)=\det(D')$, and this does the trick if the entries of $D'$ are $A_{ii}-t$, but I guess it's not clear to me that this is the case. I suspect this is true, because if it is then some nice things that I'm trying to prove follow from it (wishful thinking, I know).  So how do we prove it?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
28,Help with Cramer's rule and barycentric coordinates,Help with Cramer's rule and barycentric coordinates,,"I'm trying to learn about barycentric coordinates so after a Google search found this PDF file which didn't look too scary. I'm only on page 3 and getting confused so hope I can get help here... A triangle has vertices $p_1$, $p_2$, $p_3$ and a barycentric combination of the three points takes the form $$p = up_1 + vp_2 + wp_3$$ where $$u + v + w = 1$$ It says p can be rewritten $$p=up_1 + vp_2 + (1 - u - v)p_3$$ Then it asks ""How can we find the barycentric coordinates of a given point p?"" The claim is that there are 3 equations and the following linear system is setup: $$\begin{bmatrix}p_1&p_2&p_3\\1&1&1\end{bmatrix}\begin{bmatrix}u\\v\\w\end{bmatrix} = \begin{bmatrix}p\\1\end{bmatrix}$$ for the unknown $u,v,w$. The system is then solved using Cramer's rule: $$A = \begin{vmatrix}p_1&p_2&p_3\\1&1&1\end{vmatrix}$$ $$A_1 = \begin{vmatrix}p&p_2&p_3\\1&1&1\end{vmatrix}$$ $$A_2 = \begin{vmatrix}p_1&p&p_3\\1&1&1\end{vmatrix}$$ $$A_3 = \begin{vmatrix}p_1&p_2&p\\1&1&1\end{vmatrix}$$ My 2 points of confusion: 1) Is there 3 equations as claimed? I can see $p = \dots$ and $u + v + w = 1$ which is 2 equations. 2) Cramer's rule uses determinants, i.e. a value linked to a square matrix. The coefficient matrix here is 2-by-3, not square. Can someone help me see what the author is doing here please?","I'm trying to learn about barycentric coordinates so after a Google search found this PDF file which didn't look too scary. I'm only on page 3 and getting confused so hope I can get help here... A triangle has vertices $p_1$, $p_2$, $p_3$ and a barycentric combination of the three points takes the form $$p = up_1 + vp_2 + wp_3$$ where $$u + v + w = 1$$ It says p can be rewritten $$p=up_1 + vp_2 + (1 - u - v)p_3$$ Then it asks ""How can we find the barycentric coordinates of a given point p?"" The claim is that there are 3 equations and the following linear system is setup: $$\begin{bmatrix}p_1&p_2&p_3\\1&1&1\end{bmatrix}\begin{bmatrix}u\\v\\w\end{bmatrix} = \begin{bmatrix}p\\1\end{bmatrix}$$ for the unknown $u,v,w$. The system is then solved using Cramer's rule: $$A = \begin{vmatrix}p_1&p_2&p_3\\1&1&1\end{vmatrix}$$ $$A_1 = \begin{vmatrix}p&p_2&p_3\\1&1&1\end{vmatrix}$$ $$A_2 = \begin{vmatrix}p_1&p&p_3\\1&1&1\end{vmatrix}$$ $$A_3 = \begin{vmatrix}p_1&p_2&p\\1&1&1\end{vmatrix}$$ My 2 points of confusion: 1) Is there 3 equations as claimed? I can see $p = \dots$ and $u + v + w = 1$ which is 2 equations. 2) Cramer's rule uses determinants, i.e. a value linked to a square matrix. The coefficient matrix here is 2-by-3, not square. Can someone help me see what the author is doing here please?",,"['linear-algebra', 'geometry']"
29,Majorization relation between the absolute values squared of the entries of a matrix and the singular values squared,Majorization relation between the absolute values squared of the entries of a matrix and the singular values squared,,"Let $A = [A_{ij}]$ be an $n\times n$ square matrix with complex entries, and let $\sigma_k$, $k=1,\ldots, n$ be its singular values. Suppose that the squared Frobenius norm satisfies  $$ \mathrm{Tr}(A^\dagger A) = \sum_{i,j=1}^{n}|A_{ij}|^2 = \sum_{k=1}^n\sigma^2_k=1 \>, $$ where $A^\dagger$ is the conjugate transpose of $A$. Is the vector given by the absolute values squared of the entries, $(|A_{ij}|^2)_{ij}$, majorized by the vector $(\sigma_k^2)_k$? (As usual when discussing majorization, with proper padding of 0's, so that both vectors have $n^2$ elements.) Consider vectors $(a_i)$ and $(b_i)$ of length $m$ such that $\sum_{i=1}^m a_i = \sum_{i=1}^m b_i$. Then, we say that $a$ majorizes $b$ if $\sum_{i=1}^k a_{(i)} \geq \sum_{i=1}^k b_{(i)}$ for each $1 \leq k \leq m$, where $a_{(i)}$ denotes the $i$th largest element of $(a_i)$ and likewise for $b_{(i)}$ with respect to $(b_i)$. For a more detailed definition of majorization, please see http://en.wikipedia.org/wiki/Majorization . I looked numerically for a counterexample, and found none. If it is true, I would suppose it is well-known, and in case I would appreciate a reference as precise as possible. Thank you!","Let $A = [A_{ij}]$ be an $n\times n$ square matrix with complex entries, and let $\sigma_k$, $k=1,\ldots, n$ be its singular values. Suppose that the squared Frobenius norm satisfies  $$ \mathrm{Tr}(A^\dagger A) = \sum_{i,j=1}^{n}|A_{ij}|^2 = \sum_{k=1}^n\sigma^2_k=1 \>, $$ where $A^\dagger$ is the conjugate transpose of $A$. Is the vector given by the absolute values squared of the entries, $(|A_{ij}|^2)_{ij}$, majorized by the vector $(\sigma_k^2)_k$? (As usual when discussing majorization, with proper padding of 0's, so that both vectors have $n^2$ elements.) Consider vectors $(a_i)$ and $(b_i)$ of length $m$ such that $\sum_{i=1}^m a_i = \sum_{i=1}^m b_i$. Then, we say that $a$ majorizes $b$ if $\sum_{i=1}^k a_{(i)} \geq \sum_{i=1}^k b_{(i)}$ for each $1 \leq k \leq m$, where $a_{(i)}$ denotes the $i$th largest element of $(a_i)$ and likewise for $b_{(i)}$ with respect to $(b_i)$. For a more detailed definition of majorization, please see http://en.wikipedia.org/wiki/Majorization . I looked numerically for a counterexample, and found none. If it is true, I would suppose it is well-known, and in case I would appreciate a reference as precise as possible. Thank you!",,"['linear-algebra', 'matrices', 'majorization']"
30,Can this circulant determinant be zero?,Can this circulant determinant be zero?,,"The question is: If $a,b,c$ are negative distinct real numbers,then the determinant $$ \begin{vmatrix} a & b & c \\  b & c & a\\  c & a & b \end{vmatrix} $$ is $$(a) \le 0 \quad (b) \gt 0 \quad (c) \lt 0 \quad (d) \ge 0 $$ My strategy: I identifed that the matrix is a circulant hence the determinant can be expressed in the form of $-(a^3 + b^3 + c^3 - 3abc)$ which implies that $-(a+b+c)\frac{1}{2}[(a-b)^2 + (b-c)^2 + (c-a)^2]$ hence $(-)(-)(+) \gt 0$ but the answers says it is $\ge 0$, so can we have three $a,b,c$ such that the answer is $0$ ?","The question is: If $a,b,c$ are negative distinct real numbers,then the determinant $$ \begin{vmatrix} a & b & c \\  b & c & a\\  c & a & b \end{vmatrix} $$ is $$(a) \le 0 \quad (b) \gt 0 \quad (c) \lt 0 \quad (d) \ge 0 $$ My strategy: I identifed that the matrix is a circulant hence the determinant can be expressed in the form of $-(a^3 + b^3 + c^3 - 3abc)$ which implies that $-(a+b+c)\frac{1}{2}[(a-b)^2 + (b-c)^2 + (c-a)^2]$ hence $(-)(-)(+) \gt 0$ but the answers says it is $\ge 0$, so can we have three $a,b,c$ such that the answer is $0$ ?",,"['linear-algebra', 'determinant']"
31,How to diagonalize a large sparse symmetric matrix to get the eigenvalues and eigenvectors,How to diagonalize a large sparse symmetric matrix to get the eigenvalues and eigenvectors,,"How does one diagonalize a large sparse symmetric matrix to get the eigenvalues and the eigenvectors? The problem is the matrix could be very large (though it is sparse), at most $2500\times 2500$. Is there a good algorithm to do that and most importantly, one that I can implement it into my own code? Thanks a lot!","How does one diagonalize a large sparse symmetric matrix to get the eigenvalues and the eigenvectors? The problem is the matrix could be very large (though it is sparse), at most $2500\times 2500$. Is there a good algorithm to do that and most importantly, one that I can implement it into my own code? Thanks a lot!",,"['linear-algebra', 'algorithms', 'matrices', 'numerical-methods']"
32,"If $f(X)=AX-XA$ is diagonalizable, show that $A$ is diagonalizable","If  is diagonalizable, show that  is diagonalizable",f(X)=AX-XA A,"Let $f:M_n(F)\rightarrow M_n(F), X\mapsto AX-XA$ . If $f$ is diagonalizable, I want to show that $A$ is diagonalizable.  I'd prefer to avoid Jordan Blocks. I know that $f$ is diagonalizable if and only if: its minimal polynomial is square-free, or there exist $d$ linearly independent eigenvectors where $d = \dim M_n(F)$ , or the characteristic polynomial of $f$ factors into linear terms and each geometric multiplicity equals the corresponding algebraic multiplicity.","Let . If is diagonalizable, I want to show that is diagonalizable.  I'd prefer to avoid Jordan Blocks. I know that is diagonalizable if and only if: its minimal polynomial is square-free, or there exist linearly independent eigenvectors where , or the characteristic polynomial of factors into linear terms and each geometric multiplicity equals the corresponding algebraic multiplicity.","f:M_n(F)\rightarrow M_n(F), X\mapsto AX-XA f A f d d = \dim M_n(F) f","['linear-algebra', 'matrices', 'diagonalization']"
33,Determining the ellipse tangent to the sides of a given convex pentagon [duplicate],Determining the ellipse tangent to the sides of a given convex pentagon [duplicate],,"This question already has answers here : Inscribing an ellipse in an irregular convex pentagon (2 answers) Closed 7 months ago . Question: Given the five vertices of a convex pentagon, determine the ellipse that is tangent to the sides of this pentagon. For example, if the five vertices are: $P_1 (11,0) \ , P_2(15,0) \ , P_3 (15,10) \ , P_4  (8, 10) \ , P_5  (5, 3) $ , determine the center of the ellipse, the lengths of the semi-major and semi-minor axes, and the angle that the major axis makes with the positive $x$ -axis. My approach: is detailed in my answer below. Your comments, hints, and alternative solutions are highly appreciated.","This question already has answers here : Inscribing an ellipse in an irregular convex pentagon (2 answers) Closed 7 months ago . Question: Given the five vertices of a convex pentagon, determine the ellipse that is tangent to the sides of this pentagon. For example, if the five vertices are: , determine the center of the ellipse, the lengths of the semi-major and semi-minor axes, and the angle that the major axis makes with the positive -axis. My approach: is detailed in my answer below. Your comments, hints, and alternative solutions are highly appreciated.","P_1 (11,0) \ , P_2(15,0) \ , P_3 (15,10) \ , P_4  (8, 10) \ , P_5  (5, 3)  x","['linear-algebra', 'geometry', 'analytic-geometry', 'conic-sections']"
34,How to verify this matrix identity?,How to verify this matrix identity?,,"Let $m\geq 3$ be a positive odd number and let $M$ be the $m\times m$ matrix defined by $$M=\begin{bmatrix}0&1&0&0&\cdots&0\\ 0&0&1&0&\cdots &0\\ 0&0&0&1&\cdots &0\\ \vdots&\vdots&\vdots&\vdots&\ddots&\vdots\\ 0&0&0&0&\cdots &1\\ 1&-2&2&-2&\cdots&2\end{bmatrix}.$$ Is it true that $M^{2m}=I_m$ , the identity matrix of size $m$ ? I verified the case $m=3,5,7,9$ but am not sure how to prove this in general. Any help/hint will be appreciated.","Let be a positive odd number and let be the matrix defined by Is it true that , the identity matrix of size ? I verified the case but am not sure how to prove this in general. Any help/hint will be appreciated.","m\geq 3 M m\times m M=\begin{bmatrix}0&1&0&0&\cdots&0\\
0&0&1&0&\cdots &0\\
0&0&0&1&\cdots &0\\
\vdots&\vdots&\vdots&\vdots&\ddots&\vdots\\
0&0&0&0&\cdots &1\\
1&-2&2&-2&\cdots&2\end{bmatrix}. M^{2m}=I_m m m=3,5,7,9","['linear-algebra', 'matrices', 'companion-matrices']"
35,Find a set of vectors with special property,Find a set of vectors with special property,,"Given vectors $\alpha_1, \dots , \alpha_{m}$ in an $n$ -dimensional Euclidean space, such that $(\alpha_i, \alpha_j) \leq 0$ for $i\neq j$ and $(a_i,a_i)\neq 0$ . Find the maximum value of $m$ . The answer is $2n$ . It's easy to know $m\geq 2n$ . Let $e_1,\dots,e_n$ be the orthonormal basis of $V$ . And take $e_{n+i}=-e_i.$ Then $(e_i,e_j)\leq 0,i\neq j.$ But I don't know how to show $m\leq 2n.$","Given vectors in an -dimensional Euclidean space, such that for and . Find the maximum value of . The answer is . It's easy to know . Let be the orthonormal basis of . And take Then But I don't know how to show","\alpha_1, \dots , \alpha_{m} n (\alpha_i, \alpha_j) \leq 0 i\neq j (a_i,a_i)\neq 0 m 2n m\geq 2n e_1,\dots,e_n V e_{n+i}=-e_i. (e_i,e_j)\leq 0,i\neq j. m\leq 2n.","['linear-algebra', 'vector-spaces']"
36,"Symplectic version of ""Gram-Schmidt""","Symplectic version of ""Gram-Schmidt""",,"Let $w$ be a symplectic form on a vector space $V$ of dimension $2g$ . Suppose we already have a free family $(a_1, \dots, a_g)$ such that $w(a_i, a_j) = 0$ . I also have a family $(b_1, \dots, b_g)$ which verify that $(a_1, \dots, a_g, b_1, \dots, b_g)$ is a basis of $V$ . I've read somewhere that there is a symplectic version of Gram-Schmidt to make the basis $(a_1, \dots, a_g, b_1, \dots, b_g)$ a symplectic one, but concretely, how does it work ? Thank you !","Let be a symplectic form on a vector space of dimension . Suppose we already have a free family such that . I also have a family which verify that is a basis of . I've read somewhere that there is a symplectic version of Gram-Schmidt to make the basis a symplectic one, but concretely, how does it work ? Thank you !","w V 2g (a_1, \dots, a_g) w(a_i, a_j) = 0 (b_1, \dots, b_g) (a_1, \dots, a_g, b_1, \dots, b_g) V (a_1, \dots, a_g, b_1, \dots, b_g)","['linear-algebra', 'symplectic-linear-algebra', 'gram-schmidt']"
37,Example of copositive matrix but neither positive semidefinite nor elementwise nonnegative?,Example of copositive matrix but neither positive semidefinite nor elementwise nonnegative?,,An $n \times n$ real symmetric matrix $A$ is said to be copositive if $\mathbf{x}^{\top}A\mathbf{x} \geq 0$ for all $\mathbf{x}\in\mathbb{R}^{n}$ such that $\mathbf{x} \geq 0$ . I wonder whether the copositive matrix is also positive semidefinite or elementwise nonnegative. I think it is not true since positive semidefinite requires $\mathbf{x}^{\top}A\mathbf{x} \geq 0$ for all $\mathbf{x}\neq 0$ . Could anyone give me an example that is a copositive matrix but neither positive semidefinite nor elementwise nonnegative? Thanks.,An real symmetric matrix is said to be copositive if for all such that . I wonder whether the copositive matrix is also positive semidefinite or elementwise nonnegative. I think it is not true since positive semidefinite requires for all . Could anyone give me an example that is a copositive matrix but neither positive semidefinite nor elementwise nonnegative? Thanks.,n \times n A \mathbf{x}^{\top}A\mathbf{x} \geq 0 \mathbf{x}\in\mathbb{R}^{n} \mathbf{x} \geq 0 \mathbf{x}^{\top}A\mathbf{x} \geq 0 \mathbf{x}\neq 0,"['linear-algebra', 'matrices', 'convex-optimization', 'copositivity']"
38,The Jordan Canonical Form of linear operator in two variables polynomial,The Jordan Canonical Form of linear operator in two variables polynomial,,"I've been trying to solve the following exercise, In the space of bivariate polynomials of the form $f(x,y)=\sum_{n,m=0}^2a_{n,m}x^ny^m$ , the lineal operator $T$ is defined by $Tf(x,y)=f(x+1,y+1)$ . Obtain a Jordan Canonical Form of T I think i've made considerable progress, but I'm concerned i am taking the longest path, and i would like to know if there exist an easy way. Let $V$ be the space of bivariate polynomials of degree at most 4, I've done the following, First, I expressed the operator as $$Tf(x,y)=\sum_{m=0}^2(y+1)^m\left[(a_{0,m}+a_{1,m}+a_{2,m})+ x(a_{1,m}+2a_{2,m})+x^2a_{2,m}\right]$$ Then I observed that whenever $T$ is applied to a function $f$ in $V$ , then the non-zero coefficients of the same degree as the polynomial (non-zero coefficients with maximum $n+m$ ) remain the same. This implies that the only possible eigenvalue is $\lambda=1$ . To find the eigenvector i tried to found the set of functions $f\in V$ such that $Tf-\lambda f=Tf-f=0$ , using the expressions above it's possible to write the following $$\array{ f(x+1,y+1)-f(x,y)=&\sum_{m=0}^2[(y+1)^m(a_{0,m}+a_{1,m}+a_{2,m})-y^ma_{0,m}]+\\  &x\sum_{m=0}^2[(y+1)^m(a_{1,m}+2a_{2,m})-y^ma_{1,m}]+\\ &x^2\sum_{m=0}^2[(y+1)^ma_{2,m}-y^ma_{2,m}] } $$ Then if this polynomial is $0$ everywhere then if we treat $y$ as a constant the univariate polynomial is $0$ everywhere and then each coeficient is $0$ , i.e every sum in the expression above is $0$ and because this is true for every $y$ then this is also a $0$ everywhere polynomial for $y$ where every coefficient should be $0$ , after doing this I got a equation system that has a solution only when the following is true $$       \begin{cases}        a_{2,2}=a_{2,1}=a_{1,2}=0\\        a_{1,1}+2a_{2_0}=0 \\        a_{0,1}+a_{1,0}=0\\        a_{0,2}=a_{0,2}\\       \end{cases}$$ We can refer to $f\in V$ by it's coefficents, this means $f$ can be expressed as a vector in $\mathbb{R}^9$ , in the cannonical basis $f$ can be represented as $f=(a_{0,0},a_{0,1},a_{1,0},a_{0,2},a_{2,0},a_{1,1},a_{2,1},a_{1,2},a_{2,2})$ , then a base of the eigenspace  can be given by $$\array{ v_1=(1,0,0,0,0,0,0,0,0)\\ v_2=(0,1,-1,0,0,0,0,0,0)\\ v_3=(0,0,0,1,1,-2,0,0,0)\\ }$$ I consider that analyzing the powers of $(T-\lambda I)$ , like $(T-\lambda I)^2$ or $(T-\lambda I)^3$ is going to be a struggle, so, how should I construct the Jordan canonical form here?","I've been trying to solve the following exercise, In the space of bivariate polynomials of the form , the lineal operator is defined by . Obtain a Jordan Canonical Form of T I think i've made considerable progress, but I'm concerned i am taking the longest path, and i would like to know if there exist an easy way. Let be the space of bivariate polynomials of degree at most 4, I've done the following, First, I expressed the operator as Then I observed that whenever is applied to a function in , then the non-zero coefficients of the same degree as the polynomial (non-zero coefficients with maximum ) remain the same. This implies that the only possible eigenvalue is . To find the eigenvector i tried to found the set of functions such that , using the expressions above it's possible to write the following Then if this polynomial is everywhere then if we treat as a constant the univariate polynomial is everywhere and then each coeficient is , i.e every sum in the expression above is and because this is true for every then this is also a everywhere polynomial for where every coefficient should be , after doing this I got a equation system that has a solution only when the following is true We can refer to by it's coefficents, this means can be expressed as a vector in , in the cannonical basis can be represented as , then a base of the eigenspace  can be given by I consider that analyzing the powers of , like or is going to be a struggle, so, how should I construct the Jordan canonical form here?","f(x,y)=\sum_{n,m=0}^2a_{n,m}x^ny^m T Tf(x,y)=f(x+1,y+1) V Tf(x,y)=\sum_{m=0}^2(y+1)^m\left[(a_{0,m}+a_{1,m}+a_{2,m})+ x(a_{1,m}+2a_{2,m})+x^2a_{2,m}\right] T f V n+m \lambda=1 f\in V Tf-\lambda f=Tf-f=0 \array{
f(x+1,y+1)-f(x,y)=&\sum_{m=0}^2[(y+1)^m(a_{0,m}+a_{1,m}+a_{2,m})-y^ma_{0,m}]+\\ 
&x\sum_{m=0}^2[(y+1)^m(a_{1,m}+2a_{2,m})-y^ma_{1,m}]+\\
&x^2\sum_{m=0}^2[(y+1)^ma_{2,m}-y^ma_{2,m}]
}
 0 y 0 0 0 y 0 y 0  
     \begin{cases}
       a_{2,2}=a_{2,1}=a_{1,2}=0\\
       a_{1,1}+2a_{2_0}=0 \\
       a_{0,1}+a_{1,0}=0\\
       a_{0,2}=a_{0,2}\\ 
     \end{cases} f\in V f \mathbb{R}^9 f f=(a_{0,0},a_{0,1},a_{1,0},a_{0,2},a_{2,0},a_{1,1},a_{2,1},a_{1,2},a_{2,2}) \array{
v_1=(1,0,0,0,0,0,0,0,0)\\
v_2=(0,1,-1,0,0,0,0,0,0)\\
v_3=(0,0,0,1,1,-2,0,0,0)\\
} (T-\lambda I) (T-\lambda I)^2 (T-\lambda I)^3","['linear-algebra', 'polynomials', 'jordan-normal-form', 'generalized-eigenvector']"
39,Is there a way to find the value of the largest entry of a hidden vector of integers?,Is there a way to find the value of the largest entry of a hidden vector of integers?,,"Consider the following set-up: A person has a vector of with integer entries, which is not known to you. The person refuses to reveal the vector to you; however, you may supply another vector with integer entries, and they will tell you the result of taking the dot product with the hidden vector. A straightforward way to extract the value at the $i$ -th index is to supply the vector $(0, 0, \ldots, 1, \ldots, 0)$ - where the number $1$ occurs at the $i$ -th position. Doing this for all values of $i$ lets you recover the hidden vector in number of steps equal to the length of the vector. Is there a shorter way to just extract the absolute value of the largest entry (or an upper bound for the same) in this set-up? For example, if it is known that all the entries happen to be positive integers, one can supply the vector $(1, 1, \ldots, 1)$ and obtain an upper bound for the largest entry in a single step. Also, if it happens that it is not possible to do this in fewer than $n$ steps (where $n$ is the length of the vector), could you please refer me to a proof for why this is the case? Another variant of the question - is it possible to determine the positions of the negative numbers in the vector in fewer than $n$ steps? Edit: I came across a variant of this question a while ago. The set-up was almost identical, except the vector had positive integer entries instead of just integer entries. The solution involved finding an upper bound $M$ for the largest term (exactly as I have outlined above, and then taking the dot product with the vector $(1, M, M^2, \ldots, M^{n-1})$ to get a number whose base- $M$ expansion was the entries of the vector. I found this really interesting, and I was wondering if this could be generalized to either the full set of integers or the rationals. Since it seems unlikely that one will be able to determine the full vector in fewer than $n$ steps, I am interested in knowing whether it will be possible to determine a simpler property - such as the positions of the negative entries - in fewer than $n$ steps.","Consider the following set-up: A person has a vector of with integer entries, which is not known to you. The person refuses to reveal the vector to you; however, you may supply another vector with integer entries, and they will tell you the result of taking the dot product with the hidden vector. A straightforward way to extract the value at the -th index is to supply the vector - where the number occurs at the -th position. Doing this for all values of lets you recover the hidden vector in number of steps equal to the length of the vector. Is there a shorter way to just extract the absolute value of the largest entry (or an upper bound for the same) in this set-up? For example, if it is known that all the entries happen to be positive integers, one can supply the vector and obtain an upper bound for the largest entry in a single step. Also, if it happens that it is not possible to do this in fewer than steps (where is the length of the vector), could you please refer me to a proof for why this is the case? Another variant of the question - is it possible to determine the positions of the negative numbers in the vector in fewer than steps? Edit: I came across a variant of this question a while ago. The set-up was almost identical, except the vector had positive integer entries instead of just integer entries. The solution involved finding an upper bound for the largest term (exactly as I have outlined above, and then taking the dot product with the vector to get a number whose base- expansion was the entries of the vector. I found this really interesting, and I was wondering if this could be generalized to either the full set of integers or the rationals. Since it seems unlikely that one will be able to determine the full vector in fewer than steps, I am interested in knowing whether it will be possible to determine a simpler property - such as the positions of the negative entries - in fewer than steps.","i (0, 0, \ldots, 1, \ldots, 0) 1 i i (1, 1, \ldots, 1) n n n M (1, M, M^2, \ldots, M^{n-1}) M n n","['linear-algebra', 'vectors', 'information-theory', 'coding-theory']"
40,A problem about a rank's inequality of a complex matrix,A problem about a rank's inequality of a complex matrix,,"Let $X,Y \in M_{n\times n}(\mathbb{C})$ such that $X^{2}=Y^{2}=I_{n}$ . Prove that $$\operatorname{rank}((X+I_n)(Y-I_n))+\operatorname{rank}((X-I_n)(Y+I_n))=\operatorname{rank}(X-Y)$$ My approach: Using the well-known inequality: $\operatorname{rank}(X)+\operatorname{rank}(Y)\geq \operatorname{rank}(X+Y)$ , we can see that $$\operatorname{rank}(X-Y)=\operatorname{rank}(X-XY+XY-Y)\leq \operatorname{rank}(X-XY)+\operatorname{rank}(XY-Y)$$ but, how can I continue from here?","Let such that . Prove that My approach: Using the well-known inequality: , we can see that but, how can I continue from here?","X,Y \in M_{n\times n}(\mathbb{C}) X^{2}=Y^{2}=I_{n} \operatorname{rank}((X+I_n)(Y-I_n))+\operatorname{rank}((X-I_n)(Y+I_n))=\operatorname{rank}(X-Y) \operatorname{rank}(X)+\operatorname{rank}(Y)\geq \operatorname{rank}(X+Y) \operatorname{rank}(X-Y)=\operatorname{rank}(X-XY+XY-Y)\leq \operatorname{rank}(X-XY)+\operatorname{rank}(XY-Y)",[]
41,Is every matrix norm compatible with a vector norm?,Is every matrix norm compatible with a vector norm?,,"in $\mathbb{C}^n$ , I know every vector norm induces a matrix norm, and an induced matrix norm is compatible to its dedicated vector norm. So for every vector norm there exists a matrix norm, where the matrix norm is compatible with the vector norm. But is the other implication true too? Is there for every matrix norm a vector norm it is compatible with?","in , I know every vector norm induces a matrix norm, and an induced matrix norm is compatible to its dedicated vector norm. So for every vector norm there exists a matrix norm, where the matrix norm is compatible with the vector norm. But is the other implication true too? Is there for every matrix norm a vector norm it is compatible with?",\mathbb{C}^n,"['linear-algebra', 'matrices', 'normed-spaces', 'matrix-norms']"
42,Vector space for which finite-dimensionality is open,Vector space for which finite-dimensionality is open,,"Is there a vector space for which it is unknown whether it is finite-dimensional or infinite-dimensional? This vector space should be somehow "" naturally occuring "" in the sense that it is interesting as a vector space itself. Otherwise one could take e.g. the set of all twin primes (or any set of which finiteness is unknown) and consider it as a free vector space over some field to get a trivial example.","Is there a vector space for which it is unknown whether it is finite-dimensional or infinite-dimensional? This vector space should be somehow "" naturally occuring "" in the sense that it is interesting as a vector space itself. Otherwise one could take e.g. the set of all twin primes (or any set of which finiteness is unknown) and consider it as a free vector space over some field to get a trivial example.",,"['linear-algebra', 'functional-analysis', 'vector-spaces']"
43,Are Banach norms Fréchet differentiable?,Are Banach norms Fréchet differentiable?,,"Suppose $(V, \|\cdot\|_V)$ and $(W, \|\cdot\|_W)$ are two Banach spaces and $f: V \to W$ is some function. We call a bounded linear operator $A \in B(V, W)$ Fréchet derivative of $f$ in $x \in V$ iff $$\lim_{h \to 0} \frac{\|f(x + h) - f(x) - Ah\|_W}{\|h\|_V} = 0$$ We call a $f$ Fréchet differentiable in $x$ iff there exists a Fréchet derivative of $f$ in $x$ . My question is: Suppose $(V, \|\cdot\|_V)$ is a Banach space. $f: V \to \mathbb{R}, v \mapsto \|v\|_V$ . Is it true, that $f$ is Fréchet differentiable $\forall x \in V \setminus \{0\}$ ? This statement is indeed true in the specific case, when $V$ is a Hilbert space. Proof: One can manually check, that $h \mapsto \frac{h}{2\sqrt{x_0}}$ is a Fréchet derivative for $x \mapsto \sqrt{|x|}$ in $x_0 \neq 0$ . One can also manually check, that $h \mapsto 2\langle v, h \rangle_V$ is a Fréchet derivative for $x \mapsto \langle x, x \rangle_V$ in all $v \in V$ . And it is a well known fact, that the composition of Fréchet derivatives of two functions is a Fréchet derivative of their composition. Thus, as $\|v\|_V = \sqrt{\langle v, v \rangle_V}$ , we have, that $h \mapsto \ \frac{\langle v, h \rangle_V}{\|v\|_V}$ is a Fréchet derivative of $\|v\|_V$ in all $v \in V \setminus \{0\}$ .","Suppose and are two Banach spaces and is some function. We call a bounded linear operator Fréchet derivative of in iff We call a Fréchet differentiable in iff there exists a Fréchet derivative of in . My question is: Suppose is a Banach space. . Is it true, that is Fréchet differentiable ? This statement is indeed true in the specific case, when is a Hilbert space. Proof: One can manually check, that is a Fréchet derivative for in . One can also manually check, that is a Fréchet derivative for in all . And it is a well known fact, that the composition of Fréchet derivatives of two functions is a Fréchet derivative of their composition. Thus, as , we have, that is a Fréchet derivative of in all .","(V, \|\cdot\|_V) (W, \|\cdot\|_W) f: V \to W A \in B(V, W) f x \in V \lim_{h \to 0} \frac{\|f(x + h) - f(x) - Ah\|_W}{\|h\|_V} = 0 f x f x (V, \|\cdot\|_V) f: V \to \mathbb{R}, v \mapsto \|v\|_V f \forall x \in V \setminus \{0\} V h \mapsto \frac{h}{2\sqrt{x_0}} x \mapsto \sqrt{|x|} x_0 \neq 0 h \mapsto 2\langle v, h \rangle_V x \mapsto \langle x, x \rangle_V v \in V \|v\|_V = \sqrt{\langle v, v \rangle_V} h \mapsto \ \frac{\langle v, h \rangle_V}{\|v\|_V} \|v\|_V v \in V \setminus \{0\}","['linear-algebra', 'functional-analysis', 'banach-spaces', 'normed-spaces', 'frechet-derivative']"
44,Spectral theorem/ exchanging limit of series and operator,Spectral theorem/ exchanging limit of series and operator,,"I am currently learning quantum mechanics and there is one typical scenario i encounter in my physics books: Suppose $\mathcal{H}$ is a Hilbert space and $A: \operatorname{Dom}(A)\to \mathcal{H}$ is a (linear) operator, $Dom(A)\subseteq\mathcal{H}$ . In addition, let $\mathcal{B}:=\{v_n: n \in\mathbb{N}\}$ be a set of eigenvectors of $A$ : $A(v_n)=\lambda_nv_n$ . If there is a $v \in \operatorname{Dom}(A)$ and a sequence of coefficients $(c_n)_{n \in\mathbb{N}}$ with $v=\sum\limits_{n=1}^{\infty}c_nv_n$ , then the authors write $A(v)=A(\sum\limits_{n=1}^{\infty}c_nv_n)=\sum\limits_{n=1}^{\infty}c_nA(v_n)=\sum\limits_{n=1}^{\infty}(c_n\lambda_n)v_n$ . From what i know, A is not continuous in general, but this is a special case of the spectral theorem. I think that a physicist would write $A=\sum\limits_{n=1}^{\infty}|v_n\rangle\langle v_n|$ and call this the spectral theorem for the discrete case . The problem is that when looking into a math book about the spectral theorem, i see a lot of integrals and things get very complicated. So it would be nice if someone could explain this special case of the spectral theorem (its requirements and statements), or suggest a good source.","I am currently learning quantum mechanics and there is one typical scenario i encounter in my physics books: Suppose is a Hilbert space and is a (linear) operator, . In addition, let be a set of eigenvectors of : . If there is a and a sequence of coefficients with , then the authors write . From what i know, A is not continuous in general, but this is a special case of the spectral theorem. I think that a physicist would write and call this the spectral theorem for the discrete case . The problem is that when looking into a math book about the spectral theorem, i see a lot of integrals and things get very complicated. So it would be nice if someone could explain this special case of the spectral theorem (its requirements and statements), or suggest a good source.",\mathcal{H} A: \operatorname{Dom}(A)\to \mathcal{H} Dom(A)\subseteq\mathcal{H} \mathcal{B}:=\{v_n: n \in\mathbb{N}\} A A(v_n)=\lambda_nv_n v \in \operatorname{Dom}(A) (c_n)_{n \in\mathbb{N}} v=\sum\limits_{n=1}^{\infty}c_nv_n A(v)=A(\sum\limits_{n=1}^{\infty}c_nv_n)=\sum\limits_{n=1}^{\infty}c_nA(v_n)=\sum\limits_{n=1}^{\infty}(c_n\lambda_n)v_n A=\sum\limits_{n=1}^{\infty}|v_n\rangle\langle v_n|,"['linear-algebra', 'operator-theory', 'spectral-theory']"
45,The difference between Orthonormal Basis and the Standard Basis,The difference between Orthonormal Basis and the Standard Basis,,"I am pretty confused about the relation and the difference between these two concepts. At first glance, when I hear: Orthonormal basis I instantly relates it to the standard basis - but I'm not sure about this relation. Firstly, if these two concepts were the same - why have they got different names? Secondly, I have a sneaking suspicion that these are fundamentally different concepts - where one is mentioned when we concern about an inner product space and the other one (standard basis) is relevant when we represent column vectors over a field. Nevertheless, I have no satisfying explanation to myself about what is the intrinsic difference between the both concepts. So, can someone clarify this intrinsic difference between the concepts?","I am pretty confused about the relation and the difference between these two concepts. At first glance, when I hear: Orthonormal basis I instantly relates it to the standard basis - but I'm not sure about this relation. Firstly, if these two concepts were the same - why have they got different names? Secondly, I have a sneaking suspicion that these are fundamentally different concepts - where one is mentioned when we concern about an inner product space and the other one (standard basis) is relevant when we represent column vectors over a field. Nevertheless, I have no satisfying explanation to myself about what is the intrinsic difference between the both concepts. So, can someone clarify this intrinsic difference between the concepts?",,"['linear-algebra', 'inner-products', 'orthogonality']"
46,Principal angles between subspaces,Principal angles between subspaces,,"Let $A$ , $B$ be $k$ -dimensional subspaces of an Euclidean space $V$ of dimension $\geq 2k$ . How to find an orthonormal system $x_1,...,x_{2k}$ in $V$ and numbers $\phi_1,...,\phi_k \in [0,\frac{\pi}{2}]$ such that $\{x_1,...,x_k \} $ is a basis (obviously orthonormal) in $A$ and $\{ x_1\cos \phi_1 +x_{k+1} \sin\phi_1,...,x_k\cos \phi_k +x_{2k}\sin \phi_k \} $ is a basis (orthonormal) of $B$ ? I wish a proof or references. I'm not interested in the original Jordan proof from 1875. Thanks.","Let , be -dimensional subspaces of an Euclidean space of dimension . How to find an orthonormal system in and numbers such that is a basis (obviously orthonormal) in and is a basis (orthonormal) of ? I wish a proof or references. I'm not interested in the original Jordan proof from 1875. Thanks.","A B k V \geq 2k x_1,...,x_{2k} V \phi_1,...,\phi_k \in [0,\frac{\pi}{2}] \{x_1,...,x_k \}  A \{ x_1\cos \phi_1 +x_{k+1} \sin\phi_1,...,x_k\cos \phi_k +x_{2k}\sin \phi_k \}  B",['linear-algebra']
47,derivative of logarithm of determinant,derivative of logarithm of determinant,,"I am trying to read ""pattern recognition and machine learning"" and in the appendix there is a forumla with no proof. The author suggest to solve the following formula using the given four formulas given below. $\lambda_i$ and $u_i$ are eigen values and eigen vectors respectively. Any hint as to how to tackle this proof. Most proof on the internet use Jacobi's formula. The book is available online: http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf Trying to prove C.22. $$\frac { \partial } { \partial x } \ln | \mathbf { A } | = \operatorname { Tr } \left( \mathbf { A } ^ { - 1 } \frac { \partial \mathbf { A } } { \partial x } \right)$$ $\mathbf { u } _ { i } ^ { \mathrm { T } } \mathbf { u } _ { j } = I _ { i j }$ $ \mathbf { A } = \sum _ { i = 1 } ^ { M } \lambda _ { i } \mathbf { u } _ { i } \mathbf { u } _ { i } ^ { \mathrm { T } } $ $ \mathbf { A } ^ { - 1 } = \sum _ { i = 1 } ^ { M } \frac { 1 } { \lambda _ { i } } \mathbf { u } _ { i } \mathbf { u } _ { i } ^ { \mathrm { T } } $ $ | \mathbf { A } | = \prod _ { i = 1 } ^ { M } \lambda _ { i } $","I am trying to read ""pattern recognition and machine learning"" and in the appendix there is a forumla with no proof. The author suggest to solve the following formula using the given four formulas given below. and are eigen values and eigen vectors respectively. Any hint as to how to tackle this proof. Most proof on the internet use Jacobi's formula. The book is available online: http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf Trying to prove C.22.",\lambda_i u_i \frac { \partial } { \partial x } \ln | \mathbf { A } | = \operatorname { Tr } \left( \mathbf { A } ^ { - 1 } \frac { \partial \mathbf { A } } { \partial x } \right) \mathbf { u } _ { i } ^ { \mathrm { T } } \mathbf { u } _ { j } = I _ { i j }  \mathbf { A } = \sum _ { i = 1 } ^ { M } \lambda _ { i } \mathbf { u } _ { i } \mathbf { u } _ { i } ^ { \mathrm { T } }   \mathbf { A } ^ { - 1 } = \sum _ { i = 1 } ^ { M } \frac { 1 } { \lambda _ { i } } \mathbf { u } _ { i } \mathbf { u } _ { i } ^ { \mathrm { T } }   | \mathbf { A } | = \prod _ { i = 1 } ^ { M } \lambda _ { i } ,"['linear-algebra', 'multivariable-calculus', 'matrix-calculus']"
48,Prove that $tr(A)^p = tr(A^p)\bmod p$ where $A$ is a square integer matrix and $p$ is a prime number.,Prove that  where  is a square integer matrix and  is a prime number.,tr(A)^p = tr(A^p)\bmod p A p,"I'm looking for an elementary proof (one which does not use Galois theory). For the case $p = 3$ , we have that $tr(A^3) = tr(A)^3 - 3e_1e_2 + 3e_3$ where the $e_i$ are coefficients of the characterstic polynomial of $A$ , and are thus integers, so the result follows. I cannot see a way to generalize this for arbitrary $p$ . Any help would be great!","I'm looking for an elementary proof (one which does not use Galois theory). For the case , we have that where the are coefficients of the characterstic polynomial of , and are thus integers, so the result follows. I cannot see a way to generalize this for arbitrary . Any help would be great!",p = 3 tr(A^3) = tr(A)^3 - 3e_1e_2 + 3e_3 e_i A p,"['linear-algebra', 'matrices', 'modular-arithmetic', 'trace']"
49,A Matrices Problem - Cayley-Hamilton or Bash?,A Matrices Problem - Cayley-Hamilton or Bash?,,"Here's a cool problem I came across sometime back, and I haven't been able to solve it yet (let's hope that people at Math SE come up with interesting solutions for it!) $A$ is a square matrix of order 2, with $|A| ≠ 0$ such that $|A + |A|adj(A)| = 0$, where $|A|$ and $adj(A)$ denote the determinant and adjoint/adjugate of matrix A, respectively. Find $|A - |A|adj(A)| = ?$ Is there any way to avoid a bash , by assuming elements of the matrix and then trying to compute the desired determinant? Would it be possible to proceed using Cayley Hamilton theorem? Also, I think this result could possibly be generalised to a nxn square matrix. So, please help me find the value of the desired determinant, and also let's collectively investigate this problem for square matrices of higher order - could probably be an interesting generalisation , who knows? Update: I just solved the problem for a 2x2 square matrix - the required determinant is 4, and the determinant of matrix A is 1. Now, the question that remains is, how do I solve it for an nxn square matrix ?","Here's a cool problem I came across sometime back, and I haven't been able to solve it yet (let's hope that people at Math SE come up with interesting solutions for it!) $A$ is a square matrix of order 2, with $|A| ≠ 0$ such that $|A + |A|adj(A)| = 0$, where $|A|$ and $adj(A)$ denote the determinant and adjoint/adjugate of matrix A, respectively. Find $|A - |A|adj(A)| = ?$ Is there any way to avoid a bash , by assuming elements of the matrix and then trying to compute the desired determinant? Would it be possible to proceed using Cayley Hamilton theorem? Also, I think this result could possibly be generalised to a nxn square matrix. So, please help me find the value of the desired determinant, and also let's collectively investigate this problem for square matrices of higher order - could probably be an interesting generalisation , who knows? Update: I just solved the problem for a 2x2 square matrix - the required determinant is 4, and the determinant of matrix A is 1. Now, the question that remains is, how do I solve it for an nxn square matrix ?",,"['linear-algebra', 'matrices', 'determinant']"
50,The linear operator $T_A(B)=AB-BA$ defined on $\mathbb M_n(\mathbb C)$ has a null determinant,The linear operator  defined on  has a null determinant,T_A(B)=AB-BA \mathbb M_n(\mathbb C),"Let $T_A:\mathbb{M_n(C)}\rightarrow\mathbb{M_n(C)}$ be a linear operator such that $T(B)=AB-BA$, were $\mathbb{M_n(C)}$ denote the space of matrix with order $n$ and complex inputs. Then $T_A$ has null determinant. Proof: Is sufficient to show that $T_A$ has not inverse, i.e., $T_A$ is not one-to-one. But this is inmediate because $T_A(I)=0$ so, $\ker(T_A)\neq\{0\}$. I have doubts about my proof. It is right?","Let $T_A:\mathbb{M_n(C)}\rightarrow\mathbb{M_n(C)}$ be a linear operator such that $T(B)=AB-BA$, were $\mathbb{M_n(C)}$ denote the space of matrix with order $n$ and complex inputs. Then $T_A$ has null determinant. Proof: Is sufficient to show that $T_A$ has not inverse, i.e., $T_A$ is not one-to-one. But this is inmediate because $T_A(I)=0$ so, $\ker(T_A)\neq\{0\}$. I have doubts about my proof. It is right?",,"['linear-algebra', 'matrices', 'proof-verification', 'determinant']"
51,Suppose $V$ is finite-dimensional and $E$ is a subspace of $\mathscr L(V)$,Suppose  is finite-dimensional and  is a subspace of,V E \mathscr L(V),"Suppose $V$ is finite-dimensional and $E$ is a subspace of $\mathscr L(V)$ such that $ST\in E$ and $TS \in E$ for all $S \in \mathscr L(V)$ and all $T\in E$. Prove that $E = \{0\}$ or $E=\mathscr L(V)$. I have started the proof, but I get lost and am not sure how to finish out what I have: Suppose $v_1,\ldots,v_n$ is a basis of $V$. If $E=\{0\}$, we are done. Suppose $E\neq\{0\}$, then there exists a nonzero $T\in E$, which means there exists some $v_k\in\{v_1,\ldots,v_n\} $ such that $T(v_k)\neq0$. Let $a_1,\ldots,a_n\in \Bbb F$ such that $T(v_k)=a_1v_1+\cdots+a_nv_n\neq0$ meaning there exists some $a_l\in \{a_1,\ldots,a_n\}$ such that $a_l\neq0$. Clearly, I'll need to incorporate the fact that $ST$ and $TS$ are in $E$, and hopefully get to the point that $I\in E$.","Suppose $V$ is finite-dimensional and $E$ is a subspace of $\mathscr L(V)$ such that $ST\in E$ and $TS \in E$ for all $S \in \mathscr L(V)$ and all $T\in E$. Prove that $E = \{0\}$ or $E=\mathscr L(V)$. I have started the proof, but I get lost and am not sure how to finish out what I have: Suppose $v_1,\ldots,v_n$ is a basis of $V$. If $E=\{0\}$, we are done. Suppose $E\neq\{0\}$, then there exists a nonzero $T\in E$, which means there exists some $v_k\in\{v_1,\ldots,v_n\} $ such that $T(v_k)\neq0$. Let $a_1,\ldots,a_n\in \Bbb F$ such that $T(v_k)=a_1v_1+\cdots+a_nv_n\neq0$ meaning there exists some $a_l\in \{a_1,\ldots,a_n\}$ such that $a_l\neq0$. Clearly, I'll need to incorporate the fact that $ST$ and $TS$ are in $E$, and hopefully get to the point that $I\in E$.",,['linear-algebra']
52,Properties of generalized eigenvalue problem when hermitian,Properties of generalized eigenvalue problem when hermitian,,"This Wikipedia page says that, for the generalized eigenvalue problem  $$\boldsymbol{A}\boldsymbol{v}=\lambda\boldsymbol{B}\boldsymbol{v},$$ if $\boldsymbol{A}$ and $\boldsymbol{B}$ are hermitian and $\boldsymbol{B}$ is positive-definite, then (1) eigenvalues $\lambda$ are real; (2) eigenvectors $\boldsymbol{v}_1$ and $\boldsymbol{v}_2$ with distinct eigenvalues are $\boldsymbol{B}$-orthogonal ($\boldsymbol{v}_1^*\boldsymbol{B}\boldsymbol{v}_2=0$). How to prove (2)? I found the proof of (1) like this , but I can't find the proof of (2). The reference of this property on the Wikipedia page doesn't give the proof either.","This Wikipedia page says that, for the generalized eigenvalue problem  $$\boldsymbol{A}\boldsymbol{v}=\lambda\boldsymbol{B}\boldsymbol{v},$$ if $\boldsymbol{A}$ and $\boldsymbol{B}$ are hermitian and $\boldsymbol{B}$ is positive-definite, then (1) eigenvalues $\lambda$ are real; (2) eigenvectors $\boldsymbol{v}_1$ and $\boldsymbol{v}_2$ with distinct eigenvalues are $\boldsymbol{B}$-orthogonal ($\boldsymbol{v}_1^*\boldsymbol{B}\boldsymbol{v}_2=0$). How to prove (2)? I found the proof of (1) like this , but I can't find the proof of (2). The reference of this property on the Wikipedia page doesn't give the proof either.",,['linear-algebra']
53,Probability that leading Eigenvalue is real,Probability that leading Eigenvalue is real,,"What is the probability that the leading Eigenvalue (largest real part) of a large i.i.d Gaussian (real) random matrix is real? To what will this probability converge in the limit of large size? Update: my numerical experiments find that the fraction of leading eigenvalues that are real drops with size but appears to saturate somewhere around 0.3. I calculated this for matrices up to N=8000, albeit for only 80 realizations each.","What is the probability that the leading Eigenvalue (largest real part) of a large i.i.d Gaussian (real) random matrix is real? To what will this probability converge in the limit of large size? Update: my numerical experiments find that the fraction of leading eigenvalues that are real drops with size but appears to saturate somewhere around 0.3. I calculated this for matrices up to N=8000, albeit for only 80 realizations each.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'random-matrices']"
54,how to find out a matrix for a given minimal polynomial,how to find out a matrix for a given minimal polynomial,,"I know how to find out the the minimal polynomial for a given matrix. But I am stuck to do the reverse process. For example how to find out a  $3\times3$ matrix, whose minimal polynomial is $x^2$.","I know how to find out the the minimal polynomial for a given matrix. But I am stuck to do the reverse process. For example how to find out a  $3\times3$ matrix, whose minimal polynomial is $x^2$.",,['linear-algebra']
55,Least Squares with Euclidean ($ {L}_{2} $) Norm Constraint,Least Squares with Euclidean () Norm Constraint, {L}_{2} ,"Suppose I have set of samples $(x_i,y_i), 1 \leq i \leq n$. I am interested in solving the following optimization problem: $$ \min \sum_{i=1}^n (y_i-a^\top x_i)^2, \quad \text{s.t } \|a\|_{2} = 1. $$ If we assume that $\sum_i x_i x_i^\top$ is invertible, I am wondering if one can prove that the solution to the above optimization problem is  $$ a^\ast=\frac{\left(\sum_i x_i x_i^\top \right)^{-1} (\sum_i x_i y_i)}{\|\left(\sum_i x_i x_i^\top \right)^{-1} (\sum_i x_i y_i)\|} $$ Does the above solution still hold if we relax the constraint to be $\|a\|_{2} \leq 1$? Assuming that the above solution does not hold, in this inequality constrained case, does running a projected gradient descent guaranteed to find the true minimum since the problem is convex?","Suppose I have set of samples $(x_i,y_i), 1 \leq i \leq n$. I am interested in solving the following optimization problem: $$ \min \sum_{i=1}^n (y_i-a^\top x_i)^2, \quad \text{s.t } \|a\|_{2} = 1. $$ If we assume that $\sum_i x_i x_i^\top$ is invertible, I am wondering if one can prove that the solution to the above optimization problem is  $$ a^\ast=\frac{\left(\sum_i x_i x_i^\top \right)^{-1} (\sum_i x_i y_i)}{\|\left(\sum_i x_i x_i^\top \right)^{-1} (\sum_i x_i y_i)\|} $$ Does the above solution still hold if we relax the constraint to be $\|a\|_{2} \leq 1$? Assuming that the above solution does not hold, in this inequality constrained case, does running a projected gradient descent guaranteed to find the true minimum since the problem is convex?",,"['linear-algebra', 'optimization', 'convex-optimization', 'nonlinear-optimization', 'least-squares']"
56,"If the sum of eigenvectors is an eigenvector, then they all correspond to the same eigenvalue","If the sum of eigenvectors is an eigenvector, then they all correspond to the same eigenvalue",,"I believe I am very close to finishing this proof, but I cannot figure out the last part. If anybody could check my work and maybe give me a little hint, it would be greatly appreciated! Let $V$ be a finite-dimensional vector space and $T \in \mathcal{L}(V)$, and let $\mathbf{u,v} \in V$ be eigenvectors of $T$. Claim :  If $\mathbf{u} + \mathbf{v}$ is an eigenvector of $T$, then $\mathbf{u}, \mathbf{v}$, and $\mathbf{u+v}$ all correspond to the same eigenvalue. Proof (So far!) : Suppose $T(\mathbf{u}) = \lambda_1 \mathbf{u}$ and $T(\mathbf{v}) = \lambda_2 \mathbf{v}$ with $\mathbf{u}, \mathbf{v} \neq \mathbf{0}$. Now suppose $T(\mathbf{u+v}) = \lambda_3(\mathbf{u+v})$ with $\mathbf{u+v}\neq \mathbf{0}$. Then, $$ T(\mathbf{u}) + T(\mathbf{v}) = \lambda_3 \mathbf{u} + \lambda_3 \mathbf{v}\\ \lambda_1\mathbf{u} + \lambda_2\mathbf{v} =  \lambda_3 \mathbf{u} + \lambda_3\mathbf{v}\\ \lambda_1\mathbf{u} + \lambda_2\mathbf{v} - \lambda_3 \mathbf{u} -\lambda_3\mathbf{v} = \mathbf{0}\\ (\lambda_1 - \lambda_3) \mathbf{u} + (\lambda_2 - \lambda_3)\mathbf{v} = \mathbf{0} $$ Now I know in order to show that $\lambda_1 = \lambda_2 = \lambda_3$, I must show that the only solution to the last line is the trivial one. This would imply that $\mathbf{u}$ and $\mathbf{v}$ are linearly independent which I am unconvinced of! The only information I have to my advantage I haven't used yet is the fact that $\mathbf{u}, \mathbf{v}, \mathbf{u+v} \neq \mathbf{0}$. I really cannot see how this information can help me though. Perhaps I am going about this wrong, but that's why I want to ask! Thanks for your help.","I believe I am very close to finishing this proof, but I cannot figure out the last part. If anybody could check my work and maybe give me a little hint, it would be greatly appreciated! Let $V$ be a finite-dimensional vector space and $T \in \mathcal{L}(V)$, and let $\mathbf{u,v} \in V$ be eigenvectors of $T$. Claim :  If $\mathbf{u} + \mathbf{v}$ is an eigenvector of $T$, then $\mathbf{u}, \mathbf{v}$, and $\mathbf{u+v}$ all correspond to the same eigenvalue. Proof (So far!) : Suppose $T(\mathbf{u}) = \lambda_1 \mathbf{u}$ and $T(\mathbf{v}) = \lambda_2 \mathbf{v}$ with $\mathbf{u}, \mathbf{v} \neq \mathbf{0}$. Now suppose $T(\mathbf{u+v}) = \lambda_3(\mathbf{u+v})$ with $\mathbf{u+v}\neq \mathbf{0}$. Then, $$ T(\mathbf{u}) + T(\mathbf{v}) = \lambda_3 \mathbf{u} + \lambda_3 \mathbf{v}\\ \lambda_1\mathbf{u} + \lambda_2\mathbf{v} =  \lambda_3 \mathbf{u} + \lambda_3\mathbf{v}\\ \lambda_1\mathbf{u} + \lambda_2\mathbf{v} - \lambda_3 \mathbf{u} -\lambda_3\mathbf{v} = \mathbf{0}\\ (\lambda_1 - \lambda_3) \mathbf{u} + (\lambda_2 - \lambda_3)\mathbf{v} = \mathbf{0} $$ Now I know in order to show that $\lambda_1 = \lambda_2 = \lambda_3$, I must show that the only solution to the last line is the trivial one. This would imply that $\mathbf{u}$ and $\mathbf{v}$ are linearly independent which I am unconvinced of! The only information I have to my advantage I haven't used yet is the fact that $\mathbf{u}, \mathbf{v}, \mathbf{u+v} \neq \mathbf{0}$. I really cannot see how this information can help me though. Perhaps I am going about this wrong, but that's why I want to ask! Thanks for your help.",,"['linear-algebra', 'proof-verification', 'proof-writing', 'eigenvalues-eigenvectors']"
57,Number of non singular matrices over a finite field of order 2,Number of non singular matrices over a finite field of order 2,,"I have to find out the number of $3×3$ non singular matrices over a field of order $2$ . I tried in the following way. First to find out a non singular matrix $A,$ clearly any row of $A$ can't be full of $0$ s. So the first row (say) can be filled up by $(8-1)$ ways. Once the row is filled up,the next row can't be the same and also can't be full of zeros,so we can fill the next row by $ (8-2)$ ways. And at last the third row also can't be full of zeros,same as the first row,and same as the second row also.So we have $(8-3)$ choices. Hence the number of non singular matrices seems to be $7×6×5=210$ . Am I right? Or there are more non singular matrices ? May be less also. Please correct me if I am wrong. Thank you.","I have to find out the number of non singular matrices over a field of order . I tried in the following way. First to find out a non singular matrix clearly any row of can't be full of s. So the first row (say) can be filled up by ways. Once the row is filled up,the next row can't be the same and also can't be full of zeros,so we can fill the next row by ways. And at last the third row also can't be full of zeros,same as the first row,and same as the second row also.So we have choices. Hence the number of non singular matrices seems to be . Am I right? Or there are more non singular matrices ? May be less also. Please correct me if I am wrong. Thank you.","3×3 2 A, A 0 (8-1) 
(8-2) (8-3) 7×6×5=210",['linear-algebra']
58,What are the eigenvalues of matrix $uu^T$ where $u$ is a column vector?,What are the eigenvalues of matrix  where  is a column vector?,uu^T u,"Please let me know if this problem is duplicated and I will remove it ASAP. I see this problem on an interview book. The vector is defined as $u=(u_1,u_2,...,u_n)^T$. Then the eigenvalues of $uu^T$ are given as $\Sigma_{i=1}^n u_i^2$ with multiplicity 1 and 0 with multiplicity $n-1$. I try to start with $det(uu^T-\lambda I)$ and try to show this is exactly ($\lambda-\Sigma_{i=1}^n u_i^2)\lambda^{n-1}=0$ Any help will be appreciated!","Please let me know if this problem is duplicated and I will remove it ASAP. I see this problem on an interview book. The vector is defined as $u=(u_1,u_2,...,u_n)^T$. Then the eigenvalues of $uu^T$ are given as $\Sigma_{i=1}^n u_i^2$ with multiplicity 1 and 0 with multiplicity $n-1$. I try to start with $det(uu^T-\lambda I)$ and try to show this is exactly ($\lambda-\Sigma_{i=1}^n u_i^2)\lambda^{n-1}=0$ Any help will be appreciated!",,"['linear-algebra', 'symmetric-matrices']"
59,The Projection Matrix is Equal to its Transpose [duplicate],The Projection Matrix is Equal to its Transpose [duplicate],,"This question already has answers here : Why is a projection matrix symmetric? (4 answers) Closed 7 years ago . By inspecting its formula one can see easily that the matrix for projection onto a subspace is equal to its transpose. But what is the underlying ""geometric"" reason for this equality?  I have hard time figuring out why it has to be so? EDIT: After reviewing the suggested links, it became more clear, but the reasoning was still escaping my intuition.    However, I think that I can (based on the formal arguments in the links) put forward a rough argument.  Basically, for any operator $A$ and two vectors $x$, $y$ $$<Ax,y>=<x,A^Ty>$$ For a vector $y$ in the orthogonal complement to the subspace S we're projecting onto using the $P^S$ projection operator, $$<P^Sx,y>=0=<x,(P^S)^Ty>$$ Because we took any vector $x$, it means that $(P^S)^Ty=0$.  As $y$ is in the orthogonal complement, $P^Sy=0$ this means that $$(P^S)^Ty=0=P^Sy$$ (in plain English, for any vector in the orthogonal complement to $S$ the action of $P^S$ makes it 0, which is a symmetric action). For $y$ in $S$ and any $x$, $<P^Sx,y>=<x,y>$ and $P^Sy=y$ (by the properties of the projection operation), but also $<P^Sx,y>=<x,(P^S)^Ty>$ (true for any operator). Putting these together that yields that:$$(P^S)^Ty=y=P^Sy$$. (in plain English, for any vector in $S$ the action of $P^S$ is the identity, which is a symmetric action). To sum it up, this shows that the operator $P^S$ acts symmetrically on both elements of S and on elements in its orthogonal complement.  Being that it is a linear operator, then it acts symmetrically on all vectors $x$. A big thing why this works (I think) is that the concept of projection implies (finite) dimension inner product space and that is much more structured than a simple (finite) dimensional vector space. LAST EDIT: Even more clear after digesting the answer of Trial and Error. Given a subspace M to project onto, any vector $z_1$ can be written as the sum of two orthogonal components: $$z_1= (z_1-P_{\mathcal{M}}z_1) + P_{\mathcal{M}}z_1,$$ But the key geometric thing is that these two components belong to orthogonal subspaces , M and $\perp M$ (any vector from one subspace is perpendicular on any vector from the other subspace). The projection operator will leave the  component in M unchanged and annihilate the component in $\perp M$. Because of the fact that the subspaces themselves are orthogonal the product of ANY two arbitrary vectors $z_1, z_2$ $$<z_1,z_2>=<(z_1-P_{\mathcal{M}}z_1) + P_{\mathcal{M}}z_1,(z_2-P_{\mathcal{M}}z_2) + P_{\mathcal{M}}z_2>$$ becomes (by the orthogonality of M and $\perp M$: $$<z_1,z_2>=<(z_1-P_{\mathcal{M}}z_1),(z_2-P_{\mathcal{M}}z_2)>+<P_{\mathcal{M}}z_1,P_{\mathcal{M}}z_2>$$ But applying $P_M$ annihilates/zeroes the first bracket and leaves unchanged the second bracket.  This holds whether we apply it to $z_1$ or to $z_2$ . Ergo the effect of $P_M$ is the same whether applied to the first of second term of the scalar product.  So its matrix should be equal to its transpose. The bolded text indicates key issues that I was not fully appreciating before. Thank you for your patience!","This question already has answers here : Why is a projection matrix symmetric? (4 answers) Closed 7 years ago . By inspecting its formula one can see easily that the matrix for projection onto a subspace is equal to its transpose. But what is the underlying ""geometric"" reason for this equality?  I have hard time figuring out why it has to be so? EDIT: After reviewing the suggested links, it became more clear, but the reasoning was still escaping my intuition.    However, I think that I can (based on the formal arguments in the links) put forward a rough argument.  Basically, for any operator $A$ and two vectors $x$, $y$ $$<Ax,y>=<x,A^Ty>$$ For a vector $y$ in the orthogonal complement to the subspace S we're projecting onto using the $P^S$ projection operator, $$<P^Sx,y>=0=<x,(P^S)^Ty>$$ Because we took any vector $x$, it means that $(P^S)^Ty=0$.  As $y$ is in the orthogonal complement, $P^Sy=0$ this means that $$(P^S)^Ty=0=P^Sy$$ (in plain English, for any vector in the orthogonal complement to $S$ the action of $P^S$ makes it 0, which is a symmetric action). For $y$ in $S$ and any $x$, $<P^Sx,y>=<x,y>$ and $P^Sy=y$ (by the properties of the projection operation), but also $<P^Sx,y>=<x,(P^S)^Ty>$ (true for any operator). Putting these together that yields that:$$(P^S)^Ty=y=P^Sy$$. (in plain English, for any vector in $S$ the action of $P^S$ is the identity, which is a symmetric action). To sum it up, this shows that the operator $P^S$ acts symmetrically on both elements of S and on elements in its orthogonal complement.  Being that it is a linear operator, then it acts symmetrically on all vectors $x$. A big thing why this works (I think) is that the concept of projection implies (finite) dimension inner product space and that is much more structured than a simple (finite) dimensional vector space. LAST EDIT: Even more clear after digesting the answer of Trial and Error. Given a subspace M to project onto, any vector $z_1$ can be written as the sum of two orthogonal components: $$z_1= (z_1-P_{\mathcal{M}}z_1) + P_{\mathcal{M}}z_1,$$ But the key geometric thing is that these two components belong to orthogonal subspaces , M and $\perp M$ (any vector from one subspace is perpendicular on any vector from the other subspace). The projection operator will leave the  component in M unchanged and annihilate the component in $\perp M$. Because of the fact that the subspaces themselves are orthogonal the product of ANY two arbitrary vectors $z_1, z_2$ $$<z_1,z_2>=<(z_1-P_{\mathcal{M}}z_1) + P_{\mathcal{M}}z_1,(z_2-P_{\mathcal{M}}z_2) + P_{\mathcal{M}}z_2>$$ becomes (by the orthogonality of M and $\perp M$: $$<z_1,z_2>=<(z_1-P_{\mathcal{M}}z_1),(z_2-P_{\mathcal{M}}z_2)>+<P_{\mathcal{M}}z_1,P_{\mathcal{M}}z_2>$$ But applying $P_M$ annihilates/zeroes the first bracket and leaves unchanged the second bracket.  This holds whether we apply it to $z_1$ or to $z_2$ . Ergo the effect of $P_M$ is the same whether applied to the first of second term of the scalar product.  So its matrix should be equal to its transpose. The bolded text indicates key issues that I was not fully appreciating before. Thank you for your patience!",,"['linear-algebra', 'linear-transformations']"
60,Relationship between null-space and determinant,Relationship between null-space and determinant,,"Assume a real-valued square $n\times n$ matrix $A$ has a non-trivial nullspace. Why is the following true? Since A has a non-trivial nullspace, it is not invertible and thus   $\det A = 0$ I understand that a matrix is not invertible if $\det A = 0$, but I don't understand how having a non-trivial nullspace implies that a matrix has no inverse.","Assume a real-valued square $n\times n$ matrix $A$ has a non-trivial nullspace. Why is the following true? Since A has a non-trivial nullspace, it is not invertible and thus   $\det A = 0$ I understand that a matrix is not invertible if $\det A = 0$, but I don't understand how having a non-trivial nullspace implies that a matrix has no inverse.",,"['linear-algebra', 'vector-spaces', 'determinant']"
61,Dual spaces for dummies?,Dual spaces for dummies?,,"I'm at a complete loss for dual space right now, and linear functionals by association. My notes from lecture and my book are completely unhelpful, and I'm finding myself making up solutions to homework from patterns I'm gleaning from answers I find online or the examples in the book. The current problem I need to do reads: Define f (some special non-italicised notation for a linear functional, whatever that is -- I have no idea how to denote that in text and will just toss it into the \$'d limits) $\in(\Bbb R^2)^*$ by f$(x,y) = 2x + y$ and $T:\Bbb R^2 \rightarrow \Bbb R^2$ by $T(x,y) = (3x + 2y, x)$. (a) Compute $T^t(f)$. (b) Compute $[T^t]_{\beta^*}(f)$, where $\beta$ is the standard ordered basis for $\Bbb R^2$ and $\beta^* = \{f_1, f_2\}$ is the dual basis, by finding scalars $a, b, c$, and $d$ such that $T^t(f_1) = af_1 + cf_2$ and $T^t(f_2) = bf_1 + df_2$. (c) Compute $[T]_\beta$ and $([T]_\beta)^t$, and compare your results with (b). I have no clue where to even start. I thought maybe I could consider f as some sort of thing I could plug into the transformation (like $f = \{2, 1\}$ then $T(2,1) = (8,2)$) but that doesn't get me anywhere, let alone a matrix to transpose. I don't think I understand anything regarding this topic; I can't find any examples in the book that make any sense, my notes are equally cryptic (and most of my lecture time I'm frantically scribbling what the professor writes on the board with no concept of what's going on), and I'm not seeing anything online that's of help either. Usually there are a decent amount of pdf's to search... but not this time.","I'm at a complete loss for dual space right now, and linear functionals by association. My notes from lecture and my book are completely unhelpful, and I'm finding myself making up solutions to homework from patterns I'm gleaning from answers I find online or the examples in the book. The current problem I need to do reads: Define f (some special non-italicised notation for a linear functional, whatever that is -- I have no idea how to denote that in text and will just toss it into the \$'d limits) $\in(\Bbb R^2)^*$ by f$(x,y) = 2x + y$ and $T:\Bbb R^2 \rightarrow \Bbb R^2$ by $T(x,y) = (3x + 2y, x)$. (a) Compute $T^t(f)$. (b) Compute $[T^t]_{\beta^*}(f)$, where $\beta$ is the standard ordered basis for $\Bbb R^2$ and $\beta^* = \{f_1, f_2\}$ is the dual basis, by finding scalars $a, b, c$, and $d$ such that $T^t(f_1) = af_1 + cf_2$ and $T^t(f_2) = bf_1 + df_2$. (c) Compute $[T]_\beta$ and $([T]_\beta)^t$, and compare your results with (b). I have no clue where to even start. I thought maybe I could consider f as some sort of thing I could plug into the transformation (like $f = \{2, 1\}$ then $T(2,1) = (8,2)$) but that doesn't get me anywhere, let alone a matrix to transpose. I don't think I understand anything regarding this topic; I can't find any examples in the book that make any sense, my notes are equally cryptic (and most of my lecture time I'm frantically scribbling what the professor writes on the board with no concept of what's going on), and I'm not seeing anything online that's of help either. Usually there are a decent amount of pdf's to search... but not this time.",,"['linear-algebra', 'change-of-basis']"
62,Why do we call this transformation non-singular?,Why do we call this transformation non-singular?,,"In linear algebra books, the authors call the linear transformation $T$ with the property $$T(\alpha)=0\implies \alpha=0$$ non-singular. What's the motivation behind the term ""non-singular""?","In linear algebra books, the authors call the linear transformation $T$ with the property $$T(\alpha)=0\implies \alpha=0$$ non-singular. What's the motivation behind the term ""non-singular""?",,"['linear-algebra', 'definition', 'linear-transformations']"
63,When is the restriction of a normal operator not normal?,When is the restriction of a normal operator not normal?,,"I was proving the spectral theorem for normal operators on finite-dimensional complex vector spaces today during a test, when I arrived at the point in which If $T\in\operatorname{End}(V)$ is normal, then if $W$ is a subspace of $V$ is $T$-invariant ""surely"" the restriction $T|_W\colon W\to W$ must be normal too. But the professor said he could find a counterexample. Now, the claim above was true in that case, but I'm still curious about that example. Practically, he said that it can happen that the restriction of the adjoint $T^*$ may not coincide with the adjoint of $T$ in $W$, in other words $$ (T|_W)^*\ne (T^*)|_W. $$ When is it true (in finite-dimensional vector spaces)?","I was proving the spectral theorem for normal operators on finite-dimensional complex vector spaces today during a test, when I arrived at the point in which If $T\in\operatorname{End}(V)$ is normal, then if $W$ is a subspace of $V$ is $T$-invariant ""surely"" the restriction $T|_W\colon W\to W$ must be normal too. But the professor said he could find a counterexample. Now, the claim above was true in that case, but I'm still curious about that example. Practically, he said that it can happen that the restriction of the adjoint $T^*$ may not coincide with the adjoint of $T$ in $W$, in other words $$ (T|_W)^*\ne (T^*)|_W. $$ When is it true (in finite-dimensional vector spaces)?",,"['linear-algebra', 'adjoint-operators']"
64,The rows of an orthogonal matrix form an orthonormal basis,The rows of an orthogonal matrix form an orthonormal basis,,"A matrix $A \in \operatorname{Mat}(n \times n, \Bbb R)$ is said to be orthogonal if its columns are orthonormal relative to the dot product on $\Bbb R^n$. By considering $A^TA$, show that $A$ is an orthogonal matrix if and only if $A^T = A^{−1}$. Deduce that the rows of any $n × n$ orthogonal matrix $A$ form an orthonormal basis for the space of $n$-component row vectors over $\Bbb R$. I am trying to do part 2. What I tried is that since we figured out that $A^T = A^{-1}$, and the inverse of $A$ is the left product of elementary matrices to $A$, the row space of $A^TA =$ row space of $A$. Also, since $A^TA = I$, a basis of the row space of $A$ is a basis of the row space of $I$. Since the columns of $I$ are the standard basis of $\Bbb R^n$ $(e_1, ..., e_n)$, and are orthonormal to each other, they form an orthonormal basis of $\Bbb R^n$. Something tells me this proof is wrong. Could someone give me some guidance?","A matrix $A \in \operatorname{Mat}(n \times n, \Bbb R)$ is said to be orthogonal if its columns are orthonormal relative to the dot product on $\Bbb R^n$. By considering $A^TA$, show that $A$ is an orthogonal matrix if and only if $A^T = A^{−1}$. Deduce that the rows of any $n × n$ orthogonal matrix $A$ form an orthonormal basis for the space of $n$-component row vectors over $\Bbb R$. I am trying to do part 2. What I tried is that since we figured out that $A^T = A^{-1}$, and the inverse of $A$ is the left product of elementary matrices to $A$, the row space of $A^TA =$ row space of $A$. Also, since $A^TA = I$, a basis of the row space of $A$ is a basis of the row space of $I$. Since the columns of $I$ are the standard basis of $\Bbb R^n$ $(e_1, ..., e_n)$, and are orthonormal to each other, they form an orthonormal basis of $\Bbb R^n$. Something tells me this proof is wrong. Could someone give me some guidance?",,"['linear-algebra', 'matrices', 'orthogonality']"
65,Is there an easy way to see that $E(X^2) \geq E^2(X)$?,Is there an easy way to see that ?,E(X^2) \geq E^2(X),"I'm trying to memorize the Steiner translation theorem/König–Huygens formula. The English name seems to be ""Algebraic formula for the variance"" $$Var(X) = E[(X-E(X))^2] = E(X^2) -E^2(X)\;\;\;[1]$$ I assume that since $Var(X) \geq 0$ , $E(X^2) \geq E^2(X)$ holds. Correct? There is a not too complicated proof for [1] on the wikipedia page linked that I understand. But it takes some time to reproduce it. However, I can perfectly remember the outcome of $E(X^2), -, E^2(X)$ just not the order. So (since the difference has to be $\geq 0$ ) is there an easy way to see which of $E(X^2)$ and $E^2(X)$ is bigger? Best shot so far from @wiskundeliefhebber: Remembering that one is at least as big as the other. Then with $P(X=1) = P(X=-1) = 0.5$ follows $E(X)=0$ and $E(X^2)=1$ Ergo $E^2(X) \leq E(X^2)$ footnote: $E^2(X)$ means $(E(X))^2$","I'm trying to memorize the Steiner translation theorem/König–Huygens formula. The English name seems to be ""Algebraic formula for the variance"" I assume that since , holds. Correct? There is a not too complicated proof for [1] on the wikipedia page linked that I understand. But it takes some time to reproduce it. However, I can perfectly remember the outcome of just not the order. So (since the difference has to be ) is there an easy way to see which of and is bigger? Best shot so far from @wiskundeliefhebber: Remembering that one is at least as big as the other. Then with follows and Ergo footnote: means","Var(X) = E[(X-E(X))^2] = E(X^2) -E^2(X)\;\;\;[1] Var(X) \geq 0 E(X^2) \geq E^2(X) E(X^2), -, E^2(X) \geq 0 E(X^2) E^2(X) P(X=1) = P(X=-1) = 0.5 E(X)=0 E(X^2)=1 E^2(X) \leq E(X^2) E^2(X) (E(X))^2","['linear-algebra', 'probability', 'inequality']"
66,Dimension of set of all homogeneous polynomial of degree $d$ in $n$-variables over a field $F$,Dimension of set of all homogeneous polynomial of degree  in -variables over a field,d n F,"Let, $V$ be a set of all homogeneous polynomial of degree $d$ in $n$ -variables over a field $F$ . Then dimension of $V_F$ is (A) $\left(\begin{matrix}n\\d\end{matrix}\right)$ (B) $\left(\begin{matrix}d\\n\end{matrix}\right)$ (C) $\left(\begin{matrix}n+d-1\\d-1\end{matrix}\right)$ (D) $\left(\begin{matrix}n+d\\d\end{matrix}\right)$ . I tried through example for $n=2,3,4$ and $d=2,3,4$ and try to generalize the formula. For $n=3$ and $d=3$ we get the basis of $V$ is $\{x^3,y^3,z^3,xy^2,yx^2,yz^2,zy^2,xz^2,zx^2,xyz\}$ . So $dim(V)=10$ . Similarly for $n=3$ and $d=2$ we get $dim(V)=6$ . Also for $n=2$ and $d=2$ $dim(V)=3$ . But I could not generalize these for arbitrary $n$ and $d$ . Suggest to find the general formula or any other way to determine it directly..","Let, be a set of all homogeneous polynomial of degree in -variables over a field . Then dimension of is (A) (B) (C) (D) . I tried through example for and and try to generalize the formula. For and we get the basis of is . So . Similarly for and we get . Also for and . But I could not generalize these for arbitrary and . Suggest to find the general formula or any other way to determine it directly..","V d n F V_F \left(\begin{matrix}n\\d\end{matrix}\right) \left(\begin{matrix}d\\n\end{matrix}\right) \left(\begin{matrix}n+d-1\\d-1\end{matrix}\right) \left(\begin{matrix}n+d\\d\end{matrix}\right) n=2,3,4 d=2,3,4 n=3 d=3 V \{x^3,y^3,z^3,xy^2,yx^2,yz^2,zy^2,xz^2,zx^2,xyz\} dim(V)=10 n=3 d=2 dim(V)=6 n=2 d=2 dim(V)=3 n d","['linear-algebra', 'polynomials', 'vector-spaces']"
67,Are a row vector and a column vector the same thing?,Are a row vector and a column vector the same thing?,,"Suppose I have  $$ A =  \begin{bmatrix} a\\b\\c\\d\\ \end{bmatrix}$$ $$ B =  \begin{bmatrix} a& b& c & d\\ \end{bmatrix}$$ Now, I know $A = B^T$. But in what sense are these different mathematical objects? $$ \begin{bmatrix} 1\\ 2\\ 3\\ 4\\ \end{bmatrix} + \begin{bmatrix} 1\\ 2\\ 3\\ 4\\ \end{bmatrix}= \begin{bmatrix} 2\\ 4\\ 6\\ 8\\ \end{bmatrix} $$ $$ \begin{bmatrix} 1&2&3&4\\ \end{bmatrix} + \begin{bmatrix} 1&2&3&4\\ \end{bmatrix}= \begin{bmatrix} 2&4&6&8\\ \end{bmatrix} $$ To me, these seem to behave the same way. Is there any difference between a row vector and a column vector? How are $A$ and $B$ different?","Suppose I have  $$ A =  \begin{bmatrix} a\\b\\c\\d\\ \end{bmatrix}$$ $$ B =  \begin{bmatrix} a& b& c & d\\ \end{bmatrix}$$ Now, I know $A = B^T$. But in what sense are these different mathematical objects? $$ \begin{bmatrix} 1\\ 2\\ 3\\ 4\\ \end{bmatrix} + \begin{bmatrix} 1\\ 2\\ 3\\ 4\\ \end{bmatrix}= \begin{bmatrix} 2\\ 4\\ 6\\ 8\\ \end{bmatrix} $$ $$ \begin{bmatrix} 1&2&3&4\\ \end{bmatrix} + \begin{bmatrix} 1&2&3&4\\ \end{bmatrix}= \begin{bmatrix} 2&4&6&8\\ \end{bmatrix} $$ To me, these seem to behave the same way. Is there any difference between a row vector and a column vector? How are $A$ and $B$ different?",,['linear-algebra']
68,To be a scalar matrix or not to be?,To be a scalar matrix or not to be?,,"What follows is a pretty (but not so easy) exercise. Is fun off-topic? Let $A, B \in M_2(\mathbb{C})$. Show that for every $m, n \in \mathbb{N}$ $$((AB)^m-(BA)^m)((AB)^n-(BA)^n)$$ is a scalar matrix (that is, of the form $\lambda I_2$).","What follows is a pretty (but not so easy) exercise. Is fun off-topic? Let $A, B \in M_2(\mathbb{C})$. Show that for every $m, n \in \mathbb{N}$ $$((AB)^m-(BA)^m)((AB)^n-(BA)^n)$$ is a scalar matrix (that is, of the form $\lambda I_2$).",,['linear-algebra']
69,"The product of two symmetric, positive semidefinite matrices has non-negative eigenvalues","The product of two symmetric, positive semidefinite matrices has non-negative eigenvalues",,"How can I prove the following? If $A$ and $B$ are two symmetric, positive semidefinite matrices then all eigenvalues of $AB$ are non-negative.","How can I prove the following? If $A$ and $B$ are two symmetric, positive semidefinite matrices then all eigenvalues of $AB$ are non-negative.",,"['linear-algebra', 'matrices', 'positive-semidefinite']"
70,Prove that $T^n$ is diagonalizable.,Prove that  is diagonalizable.,T^n,"Prove or give a counterexample: If $V$ is a complex vector space and $\text{dim V} = n$ and $T \in L(V)$, then $T^n$ is diagonalizable. In order to show that $T$ is diagonalizable I need to show that I have $n$ distinct eigenvalues. If I use the theorem that states that if $V$ is a complex vector space and $T \in L(V)$ then there is a basis consisting of the generalized eigenvectors of $T$, then is this sufficient, because I feel like this is more complicated than that? Any tips or help? Thank you!","Prove or give a counterexample: If $V$ is a complex vector space and $\text{dim V} = n$ and $T \in L(V)$, then $T^n$ is diagonalizable. In order to show that $T$ is diagonalizable I need to show that I have $n$ distinct eigenvalues. If I use the theorem that states that if $V$ is a complex vector space and $T \in L(V)$ then there is a basis consisting of the generalized eigenvectors of $T$, then is this sufficient, because I feel like this is more complicated than that? Any tips or help? Thank you!",,"['linear-algebra', 'eigenvalues-eigenvectors', 'diagonalization']"
71,Matrix solution to $X-AXA=K$,Matrix solution to,X-AXA=K,"This came up in a practical problem involving a state change in a digital filter system. Find $X$ given $A$ and $K$, where $A,X,K$ are all $n$ x $n$ square matrices: $$    X - A X A = K $$ I couldn't find a direct way to do this, eventually I realized that it's just $n^2$ linear equations in $n^2$ unknowns, and it can be solved as follows: Restate so $X$ and $K$ are column vectors $\vec x$ and $\vec k$, each with $n^2$ elements The term $A X A$ becomes $A_L A_R \vec x$, where $A_L, A_R$ are $n^2$ x $n^2$ matrices which perform the equivalent of multiplying by $A$ on the left and right in the original form Find $\vec x = \left(I-A_L A_R \right)^{-1} \vec k$ and reorder to get $X$ For $n=3$, for instance, if $$  A = \begin{bmatrix}      a & b & c \\ d & e & f \\ g & h & i \end{bmatrix} $$ then $$ A_L = \begin{bmatrix}   a & 0 & 0 & b & 0 & 0 & c & 0 & 0 \\   0 & a & 0 & 0 & b & 0 & 0 & c & 0  \\   0 & 0 & a & 0 & 0 & b & 0 & 0 & c  \\   d & 0 & 0 & e & 0 & 0 & f & 0 & 0 \\   0 & d & 0 & 0 & e & 0 & 0 & f & 0 \\   0 & 0 & d & 0 & 0 & e & 0 & 0 & f \\   g & 0 & 0 & h & 0 & 0 & i & 0 & 0 \\   0 & g & 0 & 0 & h & 0 & 0 & i & 0 \\   0 & 0 & g & 0 & 0 & h & 0 & 0 & i \end{bmatrix} $$ and (in block form): $$ A_R = \begin{bmatrix}   A^T & 0 & 0 \\    0 & A^T & 0 \\    0 &  0 & A^T \end{bmatrix} $$ Question is: is there any easier way to solve this? I'm now thinking that the relationship between the $n^2$ variables is such that you can't solve it without this kind of rewriting, but I'd be happy to be proven wrong. Is there a name for the procedure of restating the problem as above? Or a name for the type of the original equation? Perhaps it would be more natural with tensor notation - I'm not that familiar with that area. One other note: the original equation can be be rewritten as the following two forms: $$  X = A X A + K $$ and (assuming $A$ is not singular) $$ X = A^{-1} \left( X -  K \right) A^{-1} $$ ... and it seems to me that one of these (depending on the properties of $A$) may be usable as a iterator that will converge to the correct value of $X$. In some applications this may easier to work with than the full solution.","This came up in a practical problem involving a state change in a digital filter system. Find $X$ given $A$ and $K$, where $A,X,K$ are all $n$ x $n$ square matrices: $$    X - A X A = K $$ I couldn't find a direct way to do this, eventually I realized that it's just $n^2$ linear equations in $n^2$ unknowns, and it can be solved as follows: Restate so $X$ and $K$ are column vectors $\vec x$ and $\vec k$, each with $n^2$ elements The term $A X A$ becomes $A_L A_R \vec x$, where $A_L, A_R$ are $n^2$ x $n^2$ matrices which perform the equivalent of multiplying by $A$ on the left and right in the original form Find $\vec x = \left(I-A_L A_R \right)^{-1} \vec k$ and reorder to get $X$ For $n=3$, for instance, if $$  A = \begin{bmatrix}      a & b & c \\ d & e & f \\ g & h & i \end{bmatrix} $$ then $$ A_L = \begin{bmatrix}   a & 0 & 0 & b & 0 & 0 & c & 0 & 0 \\   0 & a & 0 & 0 & b & 0 & 0 & c & 0  \\   0 & 0 & a & 0 & 0 & b & 0 & 0 & c  \\   d & 0 & 0 & e & 0 & 0 & f & 0 & 0 \\   0 & d & 0 & 0 & e & 0 & 0 & f & 0 \\   0 & 0 & d & 0 & 0 & e & 0 & 0 & f \\   g & 0 & 0 & h & 0 & 0 & i & 0 & 0 \\   0 & g & 0 & 0 & h & 0 & 0 & i & 0 \\   0 & 0 & g & 0 & 0 & h & 0 & 0 & i \end{bmatrix} $$ and (in block form): $$ A_R = \begin{bmatrix}   A^T & 0 & 0 \\    0 & A^T & 0 \\    0 &  0 & A^T \end{bmatrix} $$ Question is: is there any easier way to solve this? I'm now thinking that the relationship between the $n^2$ variables is such that you can't solve it without this kind of rewriting, but I'd be happy to be proven wrong. Is there a name for the procedure of restating the problem as above? Or a name for the type of the original equation? Perhaps it would be more natural with tensor notation - I'm not that familiar with that area. One other note: the original equation can be be rewritten as the following two forms: $$  X = A X A + K $$ and (assuming $A$ is not singular) $$ X = A^{-1} \left( X -  K \right) A^{-1} $$ ... and it seems to me that one of these (depending on the properties of $A$) may be usable as a iterator that will converge to the correct value of $X$. In some applications this may easier to work with than the full solution.",,['linear-algebra']
72,Pairwise commuting nilpotent matrices: alternative solution needed,Pairwise commuting nilpotent matrices: alternative solution needed,,"I have a problem: Let $A_1,A_2,...,A_n$ be $n\times n$ nilpotent matrices which are commute in each pair ($A_iA_j=A_jA_i$). Prove that: $$A_1A_2...A_n=0$$ I have got a solution by proving that $Im(A_n)$ is an invariant supspace under $A_1...A_{n-1}$, therefore we can use the induction method by considering the $n-1$ restrictions $A_1|_{Im(A_n)}$, $A_2|_{Im(A_n)}$, ... $A_{n-1}|_{Im(A_n)}$. However I really want to find a direct proof (maybe without using the restriction of linear transformations on an invariant supspace) since I think it would be a more intuitive way to see the problem (compared to that of induction method).","I have a problem: Let $A_1,A_2,...,A_n$ be $n\times n$ nilpotent matrices which are commute in each pair ($A_iA_j=A_jA_i$). Prove that: $$A_1A_2...A_n=0$$ I have got a solution by proving that $Im(A_n)$ is an invariant supspace under $A_1...A_{n-1}$, therefore we can use the induction method by considering the $n-1$ restrictions $A_1|_{Im(A_n)}$, $A_2|_{Im(A_n)}$, ... $A_{n-1}|_{Im(A_n)}$. However I really want to find a direct proof (maybe without using the restriction of linear transformations on an invariant supspace) since I think it would be a more intuitive way to see the problem (compared to that of induction method).",,['linear-algebra']
73,Proof of the inequality $\sqrt{\det X} \leq \frac{\operatorname{tr}X}{2}$,Proof of the inequality,\sqrt{\det X} \leq \frac{\operatorname{tr}X}{2},"Let $A, B \in M_2(\mathbb{R})$ be symmetric and positive definite. Put $X:=AB$.  then, we have the following inequality: $$\sqrt{\det X}\leq \dfrac{1}{2}\operatorname{trace}X.$$ and the equality holds iff $\exists \lambda>0\ s.t.\ X=\lambda E$. I cannot prove this though I have put  $A=\left(\begin{array}{ccc} a &s\\ s &b\\ \end{array}  \right), $ $B=\left(\begin{array}{ccc} x &t\\ t &y\\ \end{array}  \right), $  and have extended each part of the inequality by $a,s,b,x,t,y$. Please help me. (I encountered this problem when I tried to solve [3.10] of Nishikawa ""Variational Problems in Geometry"". )","Let $A, B \in M_2(\mathbb{R})$ be symmetric and positive definite. Put $X:=AB$.  then, we have the following inequality: $$\sqrt{\det X}\leq \dfrac{1}{2}\operatorname{trace}X.$$ and the equality holds iff $\exists \lambda>0\ s.t.\ X=\lambda E$. I cannot prove this though I have put  $A=\left(\begin{array}{ccc} a &s\\ s &b\\ \end{array}  \right), $ $B=\left(\begin{array}{ccc} x &t\\ t &y\\ \end{array}  \right), $  and have extended each part of the inequality by $a,s,b,x,t,y$. Please help me. (I encountered this problem when I tried to solve [3.10] of Nishikawa ""Variational Problems in Geometry"". )",,"['linear-algebra', 'matrices']"
74,Linear map from zero vector to zero vector.,Linear map from zero vector to zero vector.,,"I am reading an introduction on linear maps in my text book on linear algebra. The following statements are made: Suppose $G_1 (\vec{u}) = (x_1 + 2x_2 + 3x_3 + 1, 4x_1, 9x_3)$ Then we can use the following property of linear maps. Let $\lambda = 0$ and $\vec{u} = \vec{0}$ $$G(\lambda\vec{u}) = \lambda G(\vec{u})$$ And specifically: $$G(\vec{0}) = 0 \cdot G(\vec{0}) = \vec{0}$$ This means that a linear map maps the zero vector to the zero vector. It also means that $G_1$ cannot be a linear map, this is because $G_1(0,0,0) = (1,0,0) \neq (0,0,0)$. The constant term $1$ is breaking the linearity. My analysis I don't understands the above statements completely. For example this statement: $G(\vec{0}) = 0 \cdot G(\vec{0}) = 0$ should be true for any function $G(\vec{u})$, since whatever result of the  map $G(\vec{u})$ will be it will be multiplied by $0$ and result in $\vec{0}$. In the case above it would be $0 \cdot (1,0,0) = \vec{0}$. This would map the zero vector to the zero vector and hence be a correct linear map? Can anyone please explain this to me?","I am reading an introduction on linear maps in my text book on linear algebra. The following statements are made: Suppose $G_1 (\vec{u}) = (x_1 + 2x_2 + 3x_3 + 1, 4x_1, 9x_3)$ Then we can use the following property of linear maps. Let $\lambda = 0$ and $\vec{u} = \vec{0}$ $$G(\lambda\vec{u}) = \lambda G(\vec{u})$$ And specifically: $$G(\vec{0}) = 0 \cdot G(\vec{0}) = \vec{0}$$ This means that a linear map maps the zero vector to the zero vector. It also means that $G_1$ cannot be a linear map, this is because $G_1(0,0,0) = (1,0,0) \neq (0,0,0)$. The constant term $1$ is breaking the linearity. My analysis I don't understands the above statements completely. For example this statement: $G(\vec{0}) = 0 \cdot G(\vec{0}) = 0$ should be true for any function $G(\vec{u})$, since whatever result of the  map $G(\vec{u})$ will be it will be multiplied by $0$ and result in $\vec{0}$. In the case above it would be $0 \cdot (1,0,0) = \vec{0}$. This would map the zero vector to the zero vector and hence be a correct linear map? Can anyone please explain this to me?",,['linear-algebra']
75,How is the Chapman-Kolmogorov Equation not a fancy name for matrix multiplication?,How is the Chapman-Kolmogorov Equation not a fancy name for matrix multiplication?,,"The Chapman-Kolmogorov Equation : $$p^{m+n}(i,j)=\sum_kp^m(i,k)p^n(k,j)$$ Matrix Multiplication (with $[A]_{i,j}=a_{i,j}$ where $A$ is a linear map """" for B) $$[AB]_{i,j}=\sum_ka_{i,k}b_{k,j}$$ In this $p(i,j)$ denotes the transition probability from $i$ to $j$, $p$ is the matrix of these. So really is is just saying $A^{m+n}=A^mA^n$ surely. I will be asked to prove these on exams (it is on every past paper) and I want to do it this way, but I've checked MANY sources and searched through many lectures notes from all corners of the globe, they all do it the conditional probability way - which I don't mind, but it is just matrix multiplication, with induction right? (I'm not sure how I'd do induction over this case, but it is still countable, so induction ought to work!) Please help me create a proof using matrix multiplication, or at least explain why it isn't a proof (or why it is avoided)","The Chapman-Kolmogorov Equation : $$p^{m+n}(i,j)=\sum_kp^m(i,k)p^n(k,j)$$ Matrix Multiplication (with $[A]_{i,j}=a_{i,j}$ where $A$ is a linear map """" for B) $$[AB]_{i,j}=\sum_ka_{i,k}b_{k,j}$$ In this $p(i,j)$ denotes the transition probability from $i$ to $j$, $p$ is the matrix of these. So really is is just saying $A^{m+n}=A^mA^n$ surely. I will be asked to prove these on exams (it is on every past paper) and I want to do it this way, but I've checked MANY sources and searched through many lectures notes from all corners of the globe, they all do it the conditional probability way - which I don't mind, but it is just matrix multiplication, with induction right? (I'm not sure how I'd do induction over this case, but it is still countable, so induction ought to work!) Please help me create a proof using matrix multiplication, or at least explain why it isn't a proof (or why it is avoided)",,"['linear-algebra', 'probability', 'matrices', 'markov-chains', 'markov-process']"
76,Finding the dimension of subspace span(S),Finding the dimension of subspace span(S),,"Problem: Consider the set of vectors $S= \{a_1,a_2,a_3,a_4\}$ where $a_1= (6,4,1,-1,2)$ $a_2 = (1,0,2,3,-4)$ $a_3= (1,4,-9,-16,22)$ $a_4= (7,1,0,-1,3)$ Find the dimension of the subspace $span(S)$? I know that dimension is the maximum number of linearly independent vectors in a subspace. So is the dimension in this case 4? Since there are 4 vectors? Find a set of vectors in $S$ that forms basis of $span(S)$? How do I solve this?","Problem: Consider the set of vectors $S= \{a_1,a_2,a_3,a_4\}$ where $a_1= (6,4,1,-1,2)$ $a_2 = (1,0,2,3,-4)$ $a_3= (1,4,-9,-16,22)$ $a_4= (7,1,0,-1,3)$ Find the dimension of the subspace $span(S)$? I know that dimension is the maximum number of linearly independent vectors in a subspace. So is the dimension in this case 4? Since there are 4 vectors? Find a set of vectors in $S$ that forms basis of $span(S)$? How do I solve this?",,"['linear-algebra', 'vector-spaces']"
77,How to tell if two matrices are equal up to a permutation?,How to tell if two matrices are equal up to a permutation?,,"Given two real rectangular matrices, $A$ and $B$ , how can I tell if they are equal up to a permutation of their rows/column without trying all possible permutations? (This is closely related to the question I asked yesterday, Algorithm to determine matrix equivalence , but which seems too specific/complicated to receive a response. Maybe I'll be luckier with this simpler one!","Given two real rectangular matrices, and , how can I tell if they are equal up to a permutation of their rows/column without trying all possible permutations? (This is closely related to the question I asked yesterday, Algorithm to determine matrix equivalence , but which seems too specific/complicated to receive a response. Maybe I'll be luckier with this simpler one!",A B,"['linear-algebra', 'combinatorics', 'matrices', 'permutations', 'decision-problems']"
78,Does first isomorphism theorem hold in the category of normed linear spaces?,Does first isomorphism theorem hold in the category of normed linear spaces?,,"Consider the category of normed linear spaces over $\mathbb{C}$ with bounded linear maps as morphisms. If $M\subset X$ is a subspace, then the quotient space $X/M$ has a map $\|x+M\|: = \inf_{y\in M}\|x-y\|_X$, which is a norm iff $M$ is closed in $X$ under the topology induced by the norm of $X$. Let $f:L\to M$ be bounded linear map between two normed linear spaces, it is easy to check that $\ker f$ is indeed closed, thus $L/\ker f$ is a normed linear space itself. Is it true that $$L/\ker f\cong f(L)$$ i.e. there exist invertible bounded linear map whose inverse is also bounded? I checked that the canonical vector space isomorphism $L/\ker f \to f(L)$ is indeed bounded, but I have trouble showing that the inverse is bounded, or equivalently, continuous. In other words, I want to show that  $$\forall \epsilon>0. \exists \delta>0. \|f(l)-f(m)\|<\delta \Longrightarrow \|(l+\ker f)-(m+\ker f)\|<\epsilon$$ Is it true or is there a counterexample? If the statement is true, then what about the stronger statement: If $f:L\to M$ is an ismorphism of linear spaces that is bounded, then $f^{-1}$ is also bounded.","Consider the category of normed linear spaces over $\mathbb{C}$ with bounded linear maps as morphisms. If $M\subset X$ is a subspace, then the quotient space $X/M$ has a map $\|x+M\|: = \inf_{y\in M}\|x-y\|_X$, which is a norm iff $M$ is closed in $X$ under the topology induced by the norm of $X$. Let $f:L\to M$ be bounded linear map between two normed linear spaces, it is easy to check that $\ker f$ is indeed closed, thus $L/\ker f$ is a normed linear space itself. Is it true that $$L/\ker f\cong f(L)$$ i.e. there exist invertible bounded linear map whose inverse is also bounded? I checked that the canonical vector space isomorphism $L/\ker f \to f(L)$ is indeed bounded, but I have trouble showing that the inverse is bounded, or equivalently, continuous. In other words, I want to show that  $$\forall \epsilon>0. \exists \delta>0. \|f(l)-f(m)\|<\delta \Longrightarrow \|(l+\ker f)-(m+\ker f)\|<\epsilon$$ Is it true or is there a counterexample? If the statement is true, then what about the stronger statement: If $f:L\to M$ is an ismorphism of linear spaces that is bounded, then $f^{-1}$ is also bounded.",,"['linear-algebra', 'abstract-algebra', 'category-theory']"
79,"If $M$ is positive definite, then $\operatorname{det}{(M)}\leq \prod_i m_{ii}$","If  is positive definite, then",M \operatorname{det}{(M)}\leq \prod_i m_{ii},"In the Wikipedia article on positive definite matrices they claim that if $M$ is positive definite, then the determinant of $M$ is bounded by the product of its diagonal entries. How might we show this? Ideas: The sum of the diagonal entries of $M$ is the trace, which is also the sum of the eigenvalues. The product of the eigenvalues is the determinant. All the diagonal entries of $A$ must be real and strictly positive. Also we have $|m_{ij}|\leq \sqrt{m_{ii}m_{jj}}\leq \frac{m_{ii}+m_{jj}}{2}$. So far nothing is working... We might try to write the Gram matrix $m_{ij} = \langle v_i, v_j \rangle$, and then $\prod_i m_{ii} = \prod_i \|v_i\|^2$. Now we'd need to show that the determinant, which is a polynomial in the entries of $v_i$, is less than $\prod_i \|v_i\|^2$ somehow.","In the Wikipedia article on positive definite matrices they claim that if $M$ is positive definite, then the determinant of $M$ is bounded by the product of its diagonal entries. How might we show this? Ideas: The sum of the diagonal entries of $M$ is the trace, which is also the sum of the eigenvalues. The product of the eigenvalues is the determinant. All the diagonal entries of $A$ must be real and strictly positive. Also we have $|m_{ij}|\leq \sqrt{m_{ii}m_{jj}}\leq \frac{m_{ii}+m_{jj}}{2}$. So far nothing is working... We might try to write the Gram matrix $m_{ij} = \langle v_i, v_j \rangle$, and then $\prod_i m_{ii} = \prod_i \|v_i\|^2$. Now we'd need to show that the determinant, which is a polynomial in the entries of $v_i$, is less than $\prod_i \|v_i\|^2$ somehow.",,"['linear-algebra', 'matrices', 'inequality']"
80,Show that a set of projectors summing to the identity implies mutually orthogonal projectors,Show that a set of projectors summing to the identity implies mutually orthogonal projectors,,"The general setting is the study of positive operator measures in quantum mechanics , instead of the projector operator measures. Going from the PVM to the POVM is just saying that our bunch of operators are not pairwise orthogonal. SO the question is: In a matrix algebra or the bounded operators on a Hilbert space, how to prove that if a bunch of projectors sums to the identity [or their sum is less than or equal to the identity], they must be mutually orthogonal ? Can we prove this when we only know that a projector is an operator $p$ such that $ p = p^* = p^2 $ a projector is always positive, i.e., $$ \forall\,\,{ \lvert \psi \rangle} \qquad 0 \leq  \langle\psi \lvert P \lvert  \psi \rangle. $$ I cannot find a proof that if $p, q$ are (positive) projectors then $pq$ is again positive. We know that in general, if two operators commute and are positive, then their product is again positive. But how the constraint by the identity forces them to be orthogonal in our setting ? My book on quantum mechanics says, $ \sum_j p_j \leq \text{Id} \iff \forall{ i \neq j} \quad p_i \leq \text{Id} - p_j $ without proof unfortunately.","The general setting is the study of positive operator measures in quantum mechanics , instead of the projector operator measures. Going from the PVM to the POVM is just saying that our bunch of operators are not pairwise orthogonal. SO the question is: In a matrix algebra or the bounded operators on a Hilbert space, how to prove that if a bunch of projectors sums to the identity [or their sum is less than or equal to the identity], they must be mutually orthogonal ? Can we prove this when we only know that a projector is an operator such that a projector is always positive, i.e., I cannot find a proof that if are (positive) projectors then is again positive. We know that in general, if two operators commute and are positive, then their product is again positive. But how the constraint by the identity forces them to be orthogonal in our setting ? My book on quantum mechanics says, without proof unfortunately.","p 
p = p^* = p^2
 
\forall\,\,{ \lvert \psi \rangle} \qquad 0 \leq  \langle\psi \lvert P \lvert  \psi \rangle.
 p, q pq 
\sum_j p_j \leq \text{Id} \iff \forall{ i \neq j} \quad p_i \leq \text{Id} - p_j
","['linear-algebra', 'matrices', 'c-star-algebras', 'quantum-mechanics']"
81,approximating diagonal of inverse sum of low rank and diagonal matrices,approximating diagonal of inverse sum of low rank and diagonal matrices,,"I was wondering if there is any theorem or algorithm to approximate the diagonal elements of the inverse of sum of low rank symmetric positive semi-definite and non-negative diagonal matrix. Let me be more specific. Let say I have: $\mathbf{M} = \Lambda + diag(\mathbf{q})$, where $\Lambda$ is a low rank symmetric positive semi-definite and $diag(\mathbf{q})$ is a diagonal matrix whose diagonal elements are specified by the non-negative vector $\mathbf{q}$. I would like to approximate $(\mathbf{M}^{-1})_{ii}$, diagonal elements of the inverse of $\mathbf{M}$. Q: Is there any way to approximate it efficiently? I can come up with a linear that is very large and if I solve it, I can find the diagonal elements of $\mathbf{M}^{-1}$, but is a trivial one! Isn't it a common problem for people who find to find pre-conditioning for solving large linear system. I could not find anything. My problem is not pre-conditioning. I am actually interested to find diagonal elements of $\mathbf{M}^{-1}$. Thanks,","I was wondering if there is any theorem or algorithm to approximate the diagonal elements of the inverse of sum of low rank symmetric positive semi-definite and non-negative diagonal matrix. Let me be more specific. Let say I have: $\mathbf{M} = \Lambda + diag(\mathbf{q})$, where $\Lambda$ is a low rank symmetric positive semi-definite and $diag(\mathbf{q})$ is a diagonal matrix whose diagonal elements are specified by the non-negative vector $\mathbf{q}$. I would like to approximate $(\mathbf{M}^{-1})_{ii}$, diagonal elements of the inverse of $\mathbf{M}$. Q: Is there any way to approximate it efficiently? I can come up with a linear that is very large and if I solve it, I can find the diagonal elements of $\mathbf{M}^{-1}$, but is a trivial one! Isn't it a common problem for people who find to find pre-conditioning for solving large linear system. I could not find anything. My problem is not pre-conditioning. I am actually interested to find diagonal elements of $\mathbf{M}^{-1}$. Thanks,",,"['linear-algebra', 'matrices', 'numerical-methods', 'approximation', 'numerical-linear-algebra']"
82,"Proving $\text{rank}(A+B) \leq \text{rank}(A) + \text{rank}(B) - \text{rank}(AB)$ when $A,B$ commute",Proving  when  commute,"\text{rank}(A+B) \leq \text{rank}(A) + \text{rank}(B) - \text{rank}(AB) A,B","Let $A,B$ be $n \times n$ matrices and $AB=BA$. Show that $$\text{rank}(A+B) \leq \text{rank}(A) + \text{rank}(B) - \text{rank}(AB)$$ Attempt at the solution: I first proved that $$R(A+B) \subseteq R(A) + R(B)$$ where R(A) is Range(A).  Then I used the dimension formula that states that $$\dim(U+V) = \dim(U) + \dim(V) - \dim(U \cap V)$$ so that I get $$r(A+B) \leq r(A) +r(B) - \dim(R(A)\cap R(B))$$ The last part would be to show that $$\dim(R(A)\cap R(B)) = \dim(R(AB))$$ using the fact that $AB=BA$. I need help with this last part. Also please critique me if the above solution is acceptable or if there is something wrong with it. Thank you!!","Let $A,B$ be $n \times n$ matrices and $AB=BA$. Show that $$\text{rank}(A+B) \leq \text{rank}(A) + \text{rank}(B) - \text{rank}(AB)$$ Attempt at the solution: I first proved that $$R(A+B) \subseteq R(A) + R(B)$$ where R(A) is Range(A).  Then I used the dimension formula that states that $$\dim(U+V) = \dim(U) + \dim(V) - \dim(U \cap V)$$ so that I get $$r(A+B) \leq r(A) +r(B) - \dim(R(A)\cap R(B))$$ The last part would be to show that $$\dim(R(A)\cap R(B)) = \dim(R(AB))$$ using the fact that $AB=BA$. I need help with this last part. Also please critique me if the above solution is acceptable or if there is something wrong with it. Thank you!!",,"['linear-algebra', 'matrices', 'inequality', 'matrix-rank']"
83,Combine transformation matrices,Combine transformation matrices,,"Question: Find the transformation matrix that combines the following transformation matrices, in order: $$\begin{bmatrix}  &3  &0  &0  &0 \\   &0  &-1  &0  &0 \\   &0  &0  &2  &0 \\   &0  &0  &0  &1 \\   \end{bmatrix}$$ $$\begin{bmatrix}  &\frac{\sqrt3}{2}  &0  &\frac{-1}{2}  &0 \\   &0  &1  &0  &0 \\   &\frac{1}{2}  &0  &\frac{\sqrt3}{2}  &0 \\   &0  &0  &0  &1 \\   \end{bmatrix}$$ $$\begin{bmatrix}  &1  &0  &0  &3 \\   &0  &1  &0  &-1 \\   &0  &0  &1  &2 \\   &0  &0  &0  &1 \\   \end{bmatrix}$$ I know you are meant to multiply them together, and I get the same result online calculators get, but the answer in my textbook is: $$\begin{bmatrix}  &\frac{3\sqrt3}{2}  &0  &-1  &3 \\   &0  &-1  &0  &-1 \\   &\frac{3}{2}  &0  &\sqrt3  &2 \\   &0  &0  &0  &1 \\   \end{bmatrix}$$ Am I missing something, how is this result achieved?","Question: Find the transformation matrix that combines the following transformation matrices, in order: $$\begin{bmatrix}  &3  &0  &0  &0 \\   &0  &-1  &0  &0 \\   &0  &0  &2  &0 \\   &0  &0  &0  &1 \\   \end{bmatrix}$$ $$\begin{bmatrix}  &\frac{\sqrt3}{2}  &0  &\frac{-1}{2}  &0 \\   &0  &1  &0  &0 \\   &\frac{1}{2}  &0  &\frac{\sqrt3}{2}  &0 \\   &0  &0  &0  &1 \\   \end{bmatrix}$$ $$\begin{bmatrix}  &1  &0  &0  &3 \\   &0  &1  &0  &-1 \\   &0  &0  &1  &2 \\   &0  &0  &0  &1 \\   \end{bmatrix}$$ I know you are meant to multiply them together, and I get the same result online calculators get, but the answer in my textbook is: $$\begin{bmatrix}  &\frac{3\sqrt3}{2}  &0  &-1  &3 \\   &0  &-1  &0  &-1 \\   &\frac{3}{2}  &0  &\sqrt3  &2 \\   &0  &0  &0  &1 \\   \end{bmatrix}$$ Am I missing something, how is this result achieved?",,"['linear-algebra', 'matrices']"
84,Finding squared norm of vector,Finding squared norm of vector,,"Suppose you have a $2\times 1$ column vector $x=[7,2]^{T}$. How would you find $||x||^{2}$? Would it be $7^{2} + 2^{2}$? Is this equivalent to the distance from the origin?","Suppose you have a $2\times 1$ column vector $x=[7,2]^{T}$. How would you find $||x||^{2}$? Would it be $7^{2} + 2^{2}$? Is this equivalent to the distance from the origin?",,['linear-algebra']
85,What is the sense of bottom row of affine transform matrix?,What is the sense of bottom row of affine transform matrix?,,"Usually affine transform matrix (in 2D) is represented like where block A is responsible for linear transformation (no translation) and block B is responsible for translation. Block D is always zero and block C is always one. What if I put some values into blocks D and C I will affect only third (bottom) component of 2D vector, which should be always 1 and usually plays no role. But can it? Are there some generalizations, where third component plays some role and consequently, bottom row of transform matrix also does? And one more question: does this decomposition is actual only for 2D? Is it the same for 2D and xD?","Usually affine transform matrix (in 2D) is represented like where block A is responsible for linear transformation (no translation) and block B is responsible for translation. Block D is always zero and block C is always one. What if I put some values into blocks D and C I will affect only third (bottom) component of 2D vector, which should be always 1 and usually plays no role. But can it? Are there some generalizations, where third component plays some role and consequently, bottom row of transform matrix also does? And one more question: does this decomposition is actual only for 2D? Is it the same for 2D and xD?",,"['linear-algebra', 'geometric-transformation']"
86,Vandermonde matrix rank,Vandermonde matrix rank,,Let ${\bf A} \in \mathbb{C}^{M\times N}$ be a Vandermonde matrix \begin{equation} \bf A = \begin{bmatrix}1&1&\cdots&1 \\ z_1&z_2&\cdots&z_N\\ \vdots&\vdots&\ddots&\vdots\\ z_1^{M-1}&z_2^{M-1}&\cdots&z_N^{M-1} \end{bmatrix} \end{equation} where $z_n=e^{i\omega_n}$. It is known that the rank of $\bf A $ is $N$ if $M\geq N$ and $z_m\neq z_n$ when $m\neq n$. Is there any formal proof? Thanks.,Let ${\bf A} \in \mathbb{C}^{M\times N}$ be a Vandermonde matrix \begin{equation} \bf A = \begin{bmatrix}1&1&\cdots&1 \\ z_1&z_2&\cdots&z_N\\ \vdots&\vdots&\ddots&\vdots\\ z_1^{M-1}&z_2^{M-1}&\cdots&z_N^{M-1} \end{bmatrix} \end{equation} where $z_n=e^{i\omega_n}$. It is known that the rank of $\bf A $ is $N$ if $M\geq N$ and $z_m\neq z_n$ when $m\neq n$. Is there any formal proof? Thanks.,,"['linear-algebra', 'matrices', 'matrix-rank']"
87,Is every skew-adjoint matrix a commutator of two self-adjoint matrices,Is every skew-adjoint matrix a commutator of two self-adjoint matrices,,"I'm looking to solve some matrix equations. One of the equations involves a commutator, so my question is as follows: let $A$ be a skew-self-adjoint, traceless matrix, does the equation $[X,Y] = A$ always have a self-adjoint solution? For every size of matrices. I hope that this is a well-known fact. Perhaps it is related to the fact that the traceless skew-adjoint matrices are $\mathfrak{s}\mathfrak{u}_n$.","I'm looking to solve some matrix equations. One of the equations involves a commutator, so my question is as follows: let $A$ be a skew-self-adjoint, traceless matrix, does the equation $[X,Y] = A$ always have a self-adjoint solution? For every size of matrices. I hope that this is a well-known fact. Perhaps it is related to the fact that the traceless skew-adjoint matrices are $\mathfrak{s}\mathfrak{u}_n$.",,"['linear-algebra', 'matrices', 'lie-algebras']"
88,Matrix commuting with commutator,Matrix commuting with commutator,,"Suppose $A$ and $B$ are real or complex $n \times n$ matrices and $C = [A,B]$ is their commutator. If $C$ commutes with $A$, show that $C$ is nilpotent.","Suppose $A$ and $B$ are real or complex $n \times n$ matrices and $C = [A,B]$ is their commutator. If $C$ commutes with $A$, show that $C$ is nilpotent.",,"['linear-algebra', 'matrices']"
89,"A ""Matrix Trigonometry""","A ""Matrix Trigonometry""",,"$e^X$ for matrix $X$ is defined as an always-converging taylor series (provided that $X$ is a $n \times n $ complex matrix): $$e^X:=\sum_{k=0}^{\infty}\frac{X^k}{k!} $$ A thought occurred to me that we might as well define $\cos(X):=\frac12(e^{iX}+e^{-iX})$ and $\sin(X):=\frac1{2i}(e^{iX}-e^{-iX})$. Now some obvious questions arise: Is there a generalization for $2\pi$, the period of sine and cosine? Perhaps the best way to do so is to generalize the Euler's Identity $e^{2i\pi}=1$; Is there matrix $T$ such that $e^T=1$? This implies that $\cos (X+T)=\cos (X), \sin(X+T)=\sin (X)$. A simple calculation shows that $\cos^2(X)+\sin^2(X)=I$. Can we generalize other trigonometric identities any further? Can this concept be used further to derive some useful results? My senses tell me this should find its place in applied mathematics. If there's any previous reference (which I think is likely) please inform me.","$e^X$ for matrix $X$ is defined as an always-converging taylor series (provided that $X$ is a $n \times n $ complex matrix): $$e^X:=\sum_{k=0}^{\infty}\frac{X^k}{k!} $$ A thought occurred to me that we might as well define $\cos(X):=\frac12(e^{iX}+e^{-iX})$ and $\sin(X):=\frac1{2i}(e^{iX}-e^{-iX})$. Now some obvious questions arise: Is there a generalization for $2\pi$, the period of sine and cosine? Perhaps the best way to do so is to generalize the Euler's Identity $e^{2i\pi}=1$; Is there matrix $T$ such that $e^T=1$? This implies that $\cos (X+T)=\cos (X), \sin(X+T)=\sin (X)$. A simple calculation shows that $\cos^2(X)+\sin^2(X)=I$. Can we generalize other trigonometric identities any further? Can this concept be used further to derive some useful results? My senses tell me this should find its place in applied mathematics. If there's any previous reference (which I think is likely) please inform me.",,"['linear-algebra', 'trigonometry', 'exponentiation']"
90,"Endomorphisms, their matrix representations, and relation to similarity","Endomorphisms, their matrix representations, and relation to similarity",,"This question is really several short general questions to clear up some confusions. We'll start with where I'm at: An endomorphism $\phi$ is a map from a vector space $V$ to itself. After choosing a basis $\mathcal{B}$ of $V$, one may determine a matrix to represent such an endomorphism with respect to that basis, say $[\phi]_{\mathcal{B}}$. Question 1) If given a matrix without a basis specified, could you deduce a unique endomorphism it corresponds to? (my lean is no) Question 2) In a similarity transform, say $A=SDS^{-1}$ where $D$ is diagonal, $S$ is the change of basis matrix from one basis to another. My question is, since $D$ is diagonal does that mean the matrix $D$ is the same endomorphism as $A$ with respect to the standard basis in $\mathbb{R^{n}}$. Or are we unable to determine which bases are involved if only given the matrices. Question 3) Given a matrix related to an endomorphism, is it possible to determine the basis used to represent the endomorphism. (Lean yes) Overarching Question) I am trying to understand what happens under similarity transforms. I understand we input a vector, a change of basis is applied to it, the endomorphism is applied with respect to the new basis, and then it is changed back to the old basis, but my confusion relates to the construction of the similarity matrix. If a matrix is diagonalizable, then $S$ turns out to be the eigenvectors arranged in a prescribed order. Why is this! Why do the eigenvectors for the endomorphism $\phi$ with respect to one basis act as a change of basis matrix, and what basis to they go to? This is really part of question 2. Does this send the vectors to the standard basis? Or some other basis that just happens to diagonalize the endomorphism. Thanks!","This question is really several short general questions to clear up some confusions. We'll start with where I'm at: An endomorphism $\phi$ is a map from a vector space $V$ to itself. After choosing a basis $\mathcal{B}$ of $V$, one may determine a matrix to represent such an endomorphism with respect to that basis, say $[\phi]_{\mathcal{B}}$. Question 1) If given a matrix without a basis specified, could you deduce a unique endomorphism it corresponds to? (my lean is no) Question 2) In a similarity transform, say $A=SDS^{-1}$ where $D$ is diagonal, $S$ is the change of basis matrix from one basis to another. My question is, since $D$ is diagonal does that mean the matrix $D$ is the same endomorphism as $A$ with respect to the standard basis in $\mathbb{R^{n}}$. Or are we unable to determine which bases are involved if only given the matrices. Question 3) Given a matrix related to an endomorphism, is it possible to determine the basis used to represent the endomorphism. (Lean yes) Overarching Question) I am trying to understand what happens under similarity transforms. I understand we input a vector, a change of basis is applied to it, the endomorphism is applied with respect to the new basis, and then it is changed back to the old basis, but my confusion relates to the construction of the similarity matrix. If a matrix is diagonalizable, then $S$ turns out to be the eigenvectors arranged in a prescribed order. Why is this! Why do the eigenvectors for the endomorphism $\phi$ with respect to one basis act as a change of basis matrix, and what basis to they go to? This is really part of question 2. Does this send the vectors to the standard basis? Or some other basis that just happens to diagonalize the endomorphism. Thanks!",,"['linear-algebra', 'abstract-algebra', 'matrices']"
91,One-parameter subgroup of SL(n) is diagonalizable,One-parameter subgroup of SL(n) is diagonalizable,,"Given a group homomorphism $\rho\colon \mathbb{C}^\times \rightarrow SL(n,\mathbb{C})$. Why is it true that the subgroup $\rho(\mathbb{C}^\times)$ is diagonalizable, i.e. that there is a basis of $\mathbb{C}^n$ such that $\rho(\mathbb{C}^\times)$ is a subgroup of the group of diagonal matrices in this basis? I found this in a proof and suppose that it is a special case of a more general fact in representation theory, but unfortunately I do not have enough background in this area. It probably can be derived somehow from the Jordan Chevalley decomposition. Thanks!","Given a group homomorphism $\rho\colon \mathbb{C}^\times \rightarrow SL(n,\mathbb{C})$. Why is it true that the subgroup $\rho(\mathbb{C}^\times)$ is diagonalizable, i.e. that there is a basis of $\mathbb{C}^n$ such that $\rho(\mathbb{C}^\times)$ is a subgroup of the group of diagonal matrices in this basis? I found this in a proof and suppose that it is a special case of a more general fact in representation theory, but unfortunately I do not have enough background in this area. It probably can be derived somehow from the Jordan Chevalley decomposition. Thanks!",,['linear-algebra']
92,Can axis/angle notation match all possible orientations of a rotation matrix?,Can axis/angle notation match all possible orientations of a rotation matrix?,,"The rotation group is isomorphic to the orthogonal group $SO(3)$. So a rotation matrix can represent all the possible rotation transformations on the euclidean space $R3$ obtainable by the operation of composition. The axis/angle notation describes any rotation that can be obtained by rotating a solid object around an axis passing on the reference origin by the given angle. Given a pair $(\mathbf{u}, \theta)$, where $\mathbf{u}$ is a vector and corresponds to the axis rotation and $\theta$ is the roation angle, can this single pair represents all the possible rotations on the euclidean space R3 obtainable by composition?","The rotation group is isomorphic to the orthogonal group $SO(3)$. So a rotation matrix can represent all the possible rotation transformations on the euclidean space $R3$ obtainable by the operation of composition. The axis/angle notation describes any rotation that can be obtained by rotating a solid object around an axis passing on the reference origin by the given angle. Given a pair $(\mathbf{u}, \theta)$, where $\mathbf{u}$ is a vector and corresponds to the axis rotation and $\theta$ is the roation angle, can this single pair represents all the possible rotations on the euclidean space R3 obtainable by composition?",,"['linear-algebra', 'matrices', 'rotations']"
93,Showing that $\mathbf{X}^{2} + \mathbf{X} = \mathbf{A}$ has a solution,Showing that  has a solution,\mathbf{X}^{2} + \mathbf{X} = \mathbf{A},"Show that there exists some $\epsilon >0$ s.t. for all $\mathbf{A}\in \mathbb{R}^{2\times 2} $ with $|( \mathbf{A})_{i, j}| < \epsilon $ for all $i, j$ (let the space of all such matrices be $E$ ) the equation $$ \begin{align*} \mathbf{X}^{2} + \mathbf{X} = \mathbf{A} \end{align*} $$ has some solution $\mathbf{X}\in \mathbb{R}^{2\times 2} $ . My approach would be to use the Banach fixed point theorem: Consider the mapping $$ \begin{align*}   \Phi \colon E \to \mathbb{R}^{2, 2}, \quad \Phi ( \mathbf{X})   = \mathbf{A} - \mathbf{X}^{2} .\end{align*} $$ So first we need to find an $\epsilon > 0$ s.t. $\Phi [ E]\subseteq E$ . Since $$ \begin{align*} \begin{bmatrix}   a & b \\ c & d   \end{bmatrix} ^{2} = \begin{bmatrix} a^{2} + cb & ab + db \\ ac + c d &  b c + d ^{2}  \end{bmatrix}  \implies \forall i, j \in \{ 1, 2\}\colon ( \mathbf{X}^{2})_{i, j} < 2 \epsilon^{2} \end{align*} $$ choosing any $\epsilon < 1 / 2$ will assure that $\Phi ( \mathbf{X}^{2}) \in E$ for $\mathbf{X} \in E$ . Next we have to determine when $\Phi $ becomes a contraction. One has $$ \begin{align*} \left\| \Phi ( \mathbf{X}) - \Phi ( \mathbf{Y})\right\|_{\infty }   &=  \left\| \mathbf{X}^{2} - \mathbf{Y}^{2}\right\|_{\infty}  \\  &=  \max_{i \in \{ 1, 2\}} \sum_{j = 1}^{2} \left|   ( \mathbf{X}^{2})_{i, j}- ( \mathbf{Y}^{2})_{i, j} \right| \\ &\leqslant \max_{i \in \{ 1, 2\}} \sum_{j = 1}^{2}\!\left(  \left|   ( \mathbf{X}^{2})_{i, j}\right|+ \left|  ( \mathbf{Y}^{2})_{i, j}  \right| \right) \\ & < 2 \!\left( 2 \epsilon ^{2} + 2 \epsilon ^{2}\right) = 8 \epsilon ^{2} \end{align*} $$ and $\left\| \mathbf{X} - \mathbf{Y}\right\|_{\infty} < 4\epsilon $ meaning we simply have to choose some $\epsilon $ satisfying $$ \begin{align*} 2 \epsilon ^{2} < \epsilon  \iff \epsilon  < \frac{1}{2} .\end{align*} $$ First of all, is my approach correct? Secondly, are there more elegant solutions (this question was in the context of an analysis course, meaning I didn't think about any elegant linear algebra solutions).","Show that there exists some s.t. for all with for all (let the space of all such matrices be ) the equation has some solution . My approach would be to use the Banach fixed point theorem: Consider the mapping So first we need to find an s.t. . Since choosing any will assure that for . Next we have to determine when becomes a contraction. One has and meaning we simply have to choose some satisfying First of all, is my approach correct? Secondly, are there more elegant solutions (this question was in the context of an analysis course, meaning I didn't think about any elegant linear algebra solutions).","\epsilon >0 \mathbf{A}\in \mathbb{R}^{2\times 2}  |( \mathbf{A})_{i, j}| < \epsilon  i, j E 
\begin{align*}
\mathbf{X}^{2} + \mathbf{X} = \mathbf{A}
\end{align*}
 \mathbf{X}\in \mathbb{R}^{2\times 2}  
\begin{align*}
  \Phi \colon E \to \mathbb{R}^{2, 2}, \quad \Phi ( \mathbf{X})
  = \mathbf{A} - \mathbf{X}^{2}
.\end{align*}
 \epsilon > 0 \Phi [ E]\subseteq E 
\begin{align*}
\begin{bmatrix}
  a & b \\ c & d  
\end{bmatrix} ^{2} = \begin{bmatrix}
a^{2} + cb & ab + db \\ ac + c d &  b c + d ^{2} 
\end{bmatrix} 
\implies \forall i, j \in \{ 1, 2\}\colon ( \mathbf{X}^{2})_{i, j} < 2 \epsilon^{2}
\end{align*}
 \epsilon < 1 / 2 \Phi ( \mathbf{X}^{2}) \in E \mathbf{X} \in E \Phi  
\begin{align*}
\left\| \Phi ( \mathbf{X}) - \Phi ( \mathbf{Y})\right\|_{\infty }  
&= 
\left\| \mathbf{X}^{2} - \mathbf{Y}^{2}\right\|_{\infty} 
\\ 
&=  \max_{i \in \{ 1, 2\}} \sum_{j = 1}^{2} \left|   ( \mathbf{X}^{2})_{i, j}- ( \mathbf{Y}^{2})_{i, j} \right|
\\
&\leqslant \max_{i \in \{ 1, 2\}} \sum_{j = 1}^{2}\!\left(  \left|   ( \mathbf{X}^{2})_{i, j}\right|+ \left|  ( \mathbf{Y}^{2})_{i, j}  \right|
\right)
\\
& < 2 \!\left( 2 \epsilon ^{2} + 2 \epsilon ^{2}\right) = 8 \epsilon ^{2}
\end{align*}
 \left\| \mathbf{X} - \mathbf{Y}\right\|_{\infty} < 4\epsilon  \epsilon  
\begin{align*}
2 \epsilon ^{2} < \epsilon  \iff \epsilon  < \frac{1}{2}
.\end{align*}
","['linear-algebra', 'matrices', 'analysis', 'matrix-equations', 'fixed-point-theorems']"
94,Can 1 matrix represent 2 different linear maps?,Can 1 matrix represent 2 different linear maps?,,"I understand that matrices represent linear maps and all of the exercises I have done have been representing a specific example linear map with a matrix. It got me wondering: can a unique matrix represent 2 different linear maps (i.e., can 2 different linear maps be represented by the same matrix)? If it is possible, could someone give me an example? Thank you!","I understand that matrices represent linear maps and all of the exercises I have done have been representing a specific example linear map with a matrix. It got me wondering: can a unique matrix represent 2 different linear maps (i.e., can 2 different linear maps be represented by the same matrix)? If it is possible, could someone give me an example? Thank you!",,"['linear-algebra', 'soft-question']"
95,Is $\text{GL}_2(\mathbb{R})/\mathbb{R}^{\times}$ isomorphic to $\text{SL}_2(\mathbb{R})$?,Is  isomorphic to ?,\text{GL}_2(\mathbb{R})/\mathbb{R}^{\times} \text{SL}_2(\mathbb{R}),"Let $\text{GL}_2(\mathbb{R})$ be the set of real $2\times 2$ invertible matrices (where the operation is matrix multiplication). It has $\text{SL}_2(\mathbb{R})=\{ A \in \text{GL}_2(\mathbb{R}| \det A = 1)$ as a normal subgroup. Similarly, let $\text{gl}_2(\mathbb{R})$ be the set of real $2\times 2$ matrices (where the operation is matrix addition). It has $\text{sl}_2(\mathbb{R})=\{ A \in \text{gl}_2(\mathbb{R}| \text{tr} A = 0)$ as a normal subgroup. It is standard to show that $\text{GL}_2(\mathbb{R})/\text{SL}_2(\mathbb{R})\cong\mathbb{R}^{\times}$ and $\text{gl}_2(\mathbb{R})/\text{sl}_2(\mathbb{R})\cong\mathbb{R}^+$ . Recently I saw that the surjective homomorphism $\text{gl}_2(\mathbb{R})\to\text{sl}_2(\mathbb{R})$ given by $$\begin{pmatrix} a & b \\ c & d \end{pmatrix}\mapsto  \begin{pmatrix} a - d & b \\ c & d - a \end{pmatrix}$$ has a kernel isomorphic to $\mathbb{R}^+$ , allowing us to write $\text{gl}_2(\mathbb{R})/\mathbb{R}^+\cong\text{sl}_2(\mathbb{R})$ . All the above begs the question: is there a surjective homomorphism $\text{GL}_2(\mathbb{R})\to\text{SL}_2(\mathbb{R})$ with kernel isomorphic to $\mathbb{R}^{\times}$ ? Note : the map $\begin{pmatrix} a & b \\ c & d \end{pmatrix}\mapsto \frac{1}{ad-bc}\begin{pmatrix} a & b \\ c & d \end{pmatrix}$ does not work.","Let be the set of real invertible matrices (where the operation is matrix multiplication). It has as a normal subgroup. Similarly, let be the set of real matrices (where the operation is matrix addition). It has as a normal subgroup. It is standard to show that and . Recently I saw that the surjective homomorphism given by has a kernel isomorphic to , allowing us to write . All the above begs the question: is there a surjective homomorphism with kernel isomorphic to ? Note : the map does not work.","\text{GL}_2(\mathbb{R}) 2\times 2 \text{SL}_2(\mathbb{R})=\{ A \in \text{GL}_2(\mathbb{R}| \det A = 1) \text{gl}_2(\mathbb{R}) 2\times 2 \text{sl}_2(\mathbb{R})=\{ A \in \text{gl}_2(\mathbb{R}| \text{tr} A = 0) \text{GL}_2(\mathbb{R})/\text{SL}_2(\mathbb{R})\cong\mathbb{R}^{\times} \text{gl}_2(\mathbb{R})/\text{sl}_2(\mathbb{R})\cong\mathbb{R}^+ \text{gl}_2(\mathbb{R})\to\text{sl}_2(\mathbb{R}) \begin{pmatrix} a & b \\ c & d \end{pmatrix}\mapsto 
\begin{pmatrix} a - d & b \\ c & d - a \end{pmatrix} \mathbb{R}^+ \text{gl}_2(\mathbb{R})/\mathbb{R}^+\cong\text{sl}_2(\mathbb{R}) \text{GL}_2(\mathbb{R})\to\text{SL}_2(\mathbb{R}) \mathbb{R}^{\times} \begin{pmatrix} a & b \\ c & d \end{pmatrix}\mapsto \frac{1}{ad-bc}\begin{pmatrix} a & b \\ c & d \end{pmatrix}","['linear-algebra', 'matrices', 'group-theory', 'group-homomorphism']"
96,Let $T: \mathbb{R}^3 \rightarrow \mathbb{R}^3$ be a linear map such that $T^3 = T$. Show that $T$ is diagonalizable (elementarily),Let  be a linear map such that . Show that  is diagonalizable (elementarily),T: \mathbb{R}^3 \rightarrow \mathbb{R}^3 T^3 = T T,"While revising for my linear algebra exam I came across this exam problem: Let $T: \mathbb{R}^3 \rightarrow \mathbb{R}^3$ be a linear mapping such that $T^3 = T$ and $T^2 \neq T,T^2 \neq \text{id}_V$ . If $ \text{dim(ker(} T )) = 2$ , show that the matrix of $T$ is equal to diag $(0,0,-1)$ with respect to a suitable basis. My idea Assume for the moment that $T$ is diagonalizable. If $\lambda$ is an eigenvalue of $T$ then $\lambda^3 = \lambda$ since $T^3 = T$ , so $\lambda \in \{ 0,-1,1 \}$ . Since $ \text{dim(ker(} T )) = 2$ , clearly $0$ is an eigenvalue. If all three values above would be eigenvalues then the geometric multiplicity of $0$ would be $1$ and not $2$ , so $-1$ and $1$ can't both be eigenvalues. If $1$ is the other eigenvalue then since $T$ is diagonalizable we'd have that the matrix of $T$ is $M = $ diag $(0,0,1)$ with respect to some basis. But $M^2 = M$ so we'd have $T^2 = T$ which is not the case. So $-1$ must be the other eigenvalue and the result follows. Question How can I prove that $T$ is diagonalizable? For some context, the only definition of diagonalizability we have is that $T: V \rightarrow V$ diagonalizable if there is a basis of $V$ made up of eigenvectors of $V$ and that's how we usually prove diagonalizability (by explicitely providing a basis). We also have the spectral theorem for symmetric matrices but I doubt it helps here. (I also know that the minimal polynomial of $T$ must divide $x(x-1)(x+1)$ via the relation given, so the minimal polynomial is a product of distinct linear factors, so $T$ is definitely diagonalizable. But we haven't done this in the course and this method wouldn't be accepted in the exam without a full proof, so I'm looking for a more elementary way.)","While revising for my linear algebra exam I came across this exam problem: Let be a linear mapping such that and . If , show that the matrix of is equal to diag with respect to a suitable basis. My idea Assume for the moment that is diagonalizable. If is an eigenvalue of then since , so . Since , clearly is an eigenvalue. If all three values above would be eigenvalues then the geometric multiplicity of would be and not , so and can't both be eigenvalues. If is the other eigenvalue then since is diagonalizable we'd have that the matrix of is diag with respect to some basis. But so we'd have which is not the case. So must be the other eigenvalue and the result follows. Question How can I prove that is diagonalizable? For some context, the only definition of diagonalizability we have is that diagonalizable if there is a basis of made up of eigenvectors of and that's how we usually prove diagonalizability (by explicitely providing a basis). We also have the spectral theorem for symmetric matrices but I doubt it helps here. (I also know that the minimal polynomial of must divide via the relation given, so the minimal polynomial is a product of distinct linear factors, so is definitely diagonalizable. But we haven't done this in the course and this method wouldn't be accepted in the exam without a full proof, so I'm looking for a more elementary way.)","T: \mathbb{R}^3 \rightarrow \mathbb{R}^3 T^3 = T T^2 \neq T,T^2 \neq \text{id}_V  \text{dim(ker(} T )) = 2 T (0,0,-1) T \lambda T \lambda^3 = \lambda T^3 = T \lambda \in \{ 0,-1,1 \}  \text{dim(ker(} T )) = 2 0 0 1 2 -1 1 1 T T M =  (0,0,1) M^2 = M T^2 = T -1 T T: V \rightarrow V V V T x(x-1)(x+1) T","['linear-algebra', 'diagonalization']"
97,A trick in Linear algebra,A trick in Linear algebra,,"Let $V$ denote a finite dimensional vector space with inner product $\langle\cdot, \cdot\rangle$ , and $\alpha_1,\alpha_2,\cdots,\alpha_r,\beta_1,\beta_2,\cdots,\beta_r\in V$ , Suppose there exists a nonzero $\alpha\in V$ , such that: $$ \sum_{i=1}^r \langle\alpha,\alpha_i\rangle \beta_i=0 $$ Then prove a very symmetric result: There exists a nonzero $\beta\in V$ ,such that: $$ \sum_{i=1}^r \langle\beta,\beta_i\rangle\alpha_i=0 $$ It’s obviously true from my instinct since in this problem $\alpha$ and $\beta$ has an equal position, and i think it may be use contradiction to prove this, but i have no ideal how to do exactly Thanks in advance for any help!","Let denote a finite dimensional vector space with inner product , and , Suppose there exists a nonzero , such that: Then prove a very symmetric result: There exists a nonzero ,such that: It’s obviously true from my instinct since in this problem and has an equal position, and i think it may be use contradiction to prove this, but i have no ideal how to do exactly Thanks in advance for any help!","V \langle\cdot, \cdot\rangle \alpha_1,\alpha_2,\cdots,\alpha_r,\beta_1,\beta_2,\cdots,\beta_r\in V \alpha\in V 
\sum_{i=1}^r \langle\alpha,\alpha_i\rangle \beta_i=0
 \beta\in V 
\sum_{i=1}^r \langle\beta,\beta_i\rangle\alpha_i=0
 \alpha \beta",['linear-algebra']
98,Orthonormal columns implies orthonormal rows,Orthonormal columns implies orthonormal rows,,"I find it non-intuitive if I impose that all of a square matrix's columns are normalized and mutually orthogonal, then all its rows are also normalized and mutually orthogonal. Any intuitive explanation for this? Also if I relax the conditions to be only mutually orthogonal without being normalized, is this still true? And why so?","I find it non-intuitive if I impose that all of a square matrix's columns are normalized and mutually orthogonal, then all its rows are also normalized and mutually orthogonal. Any intuitive explanation for this? Also if I relax the conditions to be only mutually orthogonal without being normalized, is this still true? And why so?",,['linear-algebra']
99,How to impose orthonormality constraints by method of Lagrange multipliers,How to impose orthonormality constraints by method of Lagrange multipliers,,"I want to find the matrix $\Phi: \mathbb{R}^n \to \mathbb{R}^m$ , $m<n$ that minimizes $$V={\rm tr}(\Phi R \Phi^T)$$ subject to the orthonormality constraint $$\Phi\Phi^T=I$$ where $R: \mathbb{R}^n \to \mathbb{R}^n$ is a given symmetric positive definite matrix. How do I apply the Lagrange multiplier method to this constrained optimization problem? I tried: $$\tilde{V}={\rm tr}(\Phi R \Phi^T) - {\rm tr}\left(\Lambda(\Phi\Phi^T-I)\right)$$ where $\Lambda:\mathbb{R}^m \to \mathbb{R}^m$ is a general matrix of Lagrange multipliers, and this yields the necessary conditions for minimality: $$\frac{\partial \tilde{V}}{\partial \Lambda}=\Phi\Phi^T-I=0$$ $$\frac{\partial \tilde{V}}{\partial \Phi}=R\Phi^T-\Phi^T\Lambda=0$$ The first equation is the orthonormality condition, so far so good. But how is the second equation supposed to find minimum candidates/critical points? Intuitively, I think I know that the global minimum argument $\Phi$ must be the basis of the eigenspace of the lowest $m$ eigenvalues of $R$ . But $\Lambda$ is a general $m\times m$ matrix and nothing seems to constrain it to a diagonal matrix with $m$ eigenvalues (let alone the lowest) on the diagonal . Or am I applying the Lagrange multiplier method the wrong way? By the way: it seems easy for $m=1$ . Then $\Phi^T$ simply becomes a vector and $\Lambda=\lambda$ becomes a scalar. This yields the eigenvalue problem for one eigenvector, which is 'diagonal' in the trivial sense: $$R\Phi^T-\lambda\Phi^T=0$$","I want to find the matrix , that minimizes subject to the orthonormality constraint where is a given symmetric positive definite matrix. How do I apply the Lagrange multiplier method to this constrained optimization problem? I tried: where is a general matrix of Lagrange multipliers, and this yields the necessary conditions for minimality: The first equation is the orthonormality condition, so far so good. But how is the second equation supposed to find minimum candidates/critical points? Intuitively, I think I know that the global minimum argument must be the basis of the eigenspace of the lowest eigenvalues of . But is a general matrix and nothing seems to constrain it to a diagonal matrix with eigenvalues (let alone the lowest) on the diagonal . Or am I applying the Lagrange multiplier method the wrong way? By the way: it seems easy for . Then simply becomes a vector and becomes a scalar. This yields the eigenvalue problem for one eigenvector, which is 'diagonal' in the trivial sense:",\Phi: \mathbb{R}^n \to \mathbb{R}^m m<n V={\rm tr}(\Phi R \Phi^T) \Phi\Phi^T=I R: \mathbb{R}^n \to \mathbb{R}^n \tilde{V}={\rm tr}(\Phi R \Phi^T) - {\rm tr}\left(\Lambda(\Phi\Phi^T-I)\right) \Lambda:\mathbb{R}^m \to \mathbb{R}^m \frac{\partial \tilde{V}}{\partial \Lambda}=\Phi\Phi^T-I=0 \frac{\partial \tilde{V}}{\partial \Phi}=R\Phi^T-\Phi^T\Lambda=0 \Phi m R \Lambda m\times m m m=1 \Phi^T \Lambda=\lambda R\Phi^T-\lambda\Phi^T=0,"['linear-algebra', 'eigenvalues-eigenvectors', 'lagrange-multiplier', 'orthonormal']"
