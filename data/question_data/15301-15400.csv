,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,some question about weyl's law,some question about weyl's law,,"Let $\Omega\subset\mathbb{R}^n$ be a bounded domain with smooth boundary, consider the following Dirichlet eigenvalue problem. \begin{equation}\label{1}   \left\{          \begin{array}{ll}            -\Delta u=\lambda u, & \hbox{in $\Omega$;} \\[3mm]            u=0, & \hbox{on $\partial\Omega$.}          \end{array}        \right. \end{equation} The Weyl's law shows that  $$ N(\lambda)=\#\{k\mid 0<\lambda_k \leq \lambda\}\sim \frac{\omega_n}{(2\pi)^n} |\Omega| \lambda^{\frac{n}{2}},\qquad \lambda\to +\infty. $$ and it means that  $$ \lambda_k\sim \frac{(2\pi)^2}{(\omega_n|\Omega|)^{\frac{2}{n}}} k^{\frac{2}{n}},\qquad k\to +\infty.$$ I feel confused about how can we prove the second conclusion from the first conclusion. It seems we only know that $N(\lambda_k)\geq k$. I found in some books that use the fact $N(\lambda_k)=k$, I don't know how to get this since $\lambda_{k}$ may has multiplicty. For example, if $0<\lambda_{1}<\lambda_2=\lambda_3=\lambda_4<\lambda_5<\cdots$, then we have $N(\lambda_2)=N(\lambda_4)=4$. Can some one help me with this question? Thanks a lot!","Let $\Omega\subset\mathbb{R}^n$ be a bounded domain with smooth boundary, consider the following Dirichlet eigenvalue problem. \begin{equation}\label{1}   \left\{          \begin{array}{ll}            -\Delta u=\lambda u, & \hbox{in $\Omega$;} \\[3mm]            u=0, & \hbox{on $\partial\Omega$.}          \end{array}        \right. \end{equation} The Weyl's law shows that  $$ N(\lambda)=\#\{k\mid 0<\lambda_k \leq \lambda\}\sim \frac{\omega_n}{(2\pi)^n} |\Omega| \lambda^{\frac{n}{2}},\qquad \lambda\to +\infty. $$ and it means that  $$ \lambda_k\sim \frac{(2\pi)^2}{(\omega_n|\Omega|)^{\frac{2}{n}}} k^{\frac{2}{n}},\qquad k\to +\infty.$$ I feel confused about how can we prove the second conclusion from the first conclusion. It seems we only know that $N(\lambda_k)\geq k$. I found in some books that use the fact $N(\lambda_k)=k$, I don't know how to get this since $\lambda_{k}$ may has multiplicty. For example, if $0<\lambda_{1}<\lambda_2=\lambda_3=\lambda_4<\lambda_5<\cdots$, then we have $N(\lambda_2)=N(\lambda_4)=4$. Can some one help me with this question? Thanks a lot!",,"['calculus', 'partial-differential-equations', 'spectral-theory']"
1,Evaluate $\int_{0}^{\infty} \frac{x \exp\left\{-\beta^2 x^2\right\}}{\sinh \left(\frac{\pi x}{2}\right)} \mathrm{d} x$,Evaluate,\int_{0}^{\infty} \frac{x \exp\left\{-\beta^2 x^2\right\}}{\sinh \left(\frac{\pi x}{2}\right)} \mathrm{d} x,"I am trying to evaluate the integral $$ \int_{0}^{\infty} \frac{x \exp\left\{-\beta^2 x^2\right\}}{\sinh \left(\frac{\pi x}{2}\right)} \mathrm{d} x, $$ where $\beta \in \mathbb{R}$ is some constant. After many unfruitful attempts and checking up several integral handbooks, I am still not sure how to handle it. Does there exist an analytic solution? Thanks in advance! Edit: Inspired by the discussions in this question , I tried to make the following manipulation (assume we can interchange the order of the infinite sum and the definite integral): $$ \begin{aligned} \int_{0}^{\infty} \frac{x \exp(-\beta^2 x^2)}{\sinh(\pi x/2)}\mathrm{d}x =& \frac{16}{\pi^3} \int_{0}^{\infty}\sum_{n=0}^{\infty} x \exp(-\beta^2x^2) \exp(-(2n+1)x) \mathrm{d} x \\ =&\frac{16}{\pi^3} \sum_{n=0}^{\infty} \exp\left(\frac{(2n+1)^2}{4\beta^2} \right) \int_{0}^{\infty} x \exp\left\{ -\frac{1}{2\frac{1}{2\beta^2}} \left(x + \frac{2n+1}{2\beta^2} \right)^2 \right\} \mathrm{d}x\\ =& \frac{16}{\pi^3}\sum_{n=0}^{\infty} \exp\left(\frac{(2n+1)^2}{4\beta^2}\right) \left\{ \frac{\sqrt{2\pi}}{2\beta^2}\exp\left(-(2n+1)^2\beta^2\right) \right. \\ &\left.- \frac{\sqrt{\pi}(2n+1)}{2\beta^3} \left(1-\Phi\left(\frac{\sqrt{2}}{2}(2n+1)\beta\right)\right) \right\} \end{aligned}, $$ where $\Phi(\cdot)$ is the standard normal cdf. This still seems not very promising: I have little idea how to deal with this infinite series.","I am trying to evaluate the integral $$ \int_{0}^{\infty} \frac{x \exp\left\{-\beta^2 x^2\right\}}{\sinh \left(\frac{\pi x}{2}\right)} \mathrm{d} x, $$ where $\beta \in \mathbb{R}$ is some constant. After many unfruitful attempts and checking up several integral handbooks, I am still not sure how to handle it. Does there exist an analytic solution? Thanks in advance! Edit: Inspired by the discussions in this question , I tried to make the following manipulation (assume we can interchange the order of the infinite sum and the definite integral): $$ \begin{aligned} \int_{0}^{\infty} \frac{x \exp(-\beta^2 x^2)}{\sinh(\pi x/2)}\mathrm{d}x =& \frac{16}{\pi^3} \int_{0}^{\infty}\sum_{n=0}^{\infty} x \exp(-\beta^2x^2) \exp(-(2n+1)x) \mathrm{d} x \\ =&\frac{16}{\pi^3} \sum_{n=0}^{\infty} \exp\left(\frac{(2n+1)^2}{4\beta^2} \right) \int_{0}^{\infty} x \exp\left\{ -\frac{1}{2\frac{1}{2\beta^2}} \left(x + \frac{2n+1}{2\beta^2} \right)^2 \right\} \mathrm{d}x\\ =& \frac{16}{\pi^3}\sum_{n=0}^{\infty} \exp\left(\frac{(2n+1)^2}{4\beta^2}\right) \left\{ \frac{\sqrt{2\pi}}{2\beta^2}\exp\left(-(2n+1)^2\beta^2\right) \right. \\ &\left.- \frac{\sqrt{\pi}(2n+1)}{2\beta^3} \left(1-\Phi\left(\frac{\sqrt{2}}{2}(2n+1)\beta\right)\right) \right\} \end{aligned}, $$ where $\Phi(\cdot)$ is the standard normal cdf. This still seems not very promising: I have little idea how to deal with this infinite series.",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'hyperbolic-functions']"
2,What constitutes the classification of functions into **elementary** and **non-elementary**?,What constitutes the classification of functions into **elementary** and **non-elementary**?,,Stemming from a comment thread in another question I got curious about why exponential and trig functions are considered elementary but there are so very many other non-algebraic functions which are not. Are there any particular motivations or is it something that becomes obvious when one has studied enough analysis? Is it the exponential functions relation to being eigenfunction to differentiation that is central to this choice or something else?,Stemming from a comment thread in another question I got curious about why exponential and trig functions are considered elementary but there are so very many other non-algebraic functions which are not. Are there any particular motivations or is it something that becomes obvious when one has studied enough analysis? Is it the exponential functions relation to being eigenfunction to differentiation that is central to this choice or something else?,,"['calculus', 'analysis', 'math-history', 'elementary-functions']"
3,$\int\text{e}^{-ax^2 } \text{erf}\left(bx + c\right) dx$,,\int\text{e}^{-ax^2 } \text{erf}\left(bx + c\right) dx,"I'm hoping to find a closed expression for the following integral. $$ \int\text{e}^{-ax^2 } \text{erf}\left(bx + c\right) dx $$ One can find a solution for a family of products between exponentials and error functions. None of which apparently have the offset term in the error function. I have tried tackling the problem with two approaches. Approach #1: Expanding the error function hoping to find nice cancelations leading to the maclerin series of some known elementary function. Following a similar approach by Alex : $$ \begin{aligned} \int\text{e}^{-ax^2 } \text{erf}\left(bx + c\right) dx &= \frac{2}{\sqrt{\pi}}  \sum_{n=0}^\infty \frac{(-1)^n }{n!(2n+1)} \int(bx+c)^{2n +1} \text{e}^{-ax^2} dx \\ & = \frac{2}{\sqrt{\pi}}  \sum_{n=0}^\infty \frac{(-1)^n }{n!(2n+1)}  \int   \sum_{k=0}^{2n+1} {{2n+1}\choose{k}} (bx)^{k} c^{2n+1-k} \text{e}^{-ax^2} dx \\ & = \frac{2}{\sqrt{\pi}}  \sum_{n=0}^\infty \frac{(-1)^n }{n!(2n+1)}     \sum_{k=0}^{2n+1} {{2n+1}\choose{k}} c^{2n+1-k} b^k\int x^{k}  \text{e}^{-ax^2} dx  = \\ &\frac{2}{\sqrt{\pi}}  \sum_{n=0}^\infty \frac{(-1)^n }{n!(2n+1)}     \sum_{k=0}^{2n+1} {{2n+1}\choose{k}} c^{2n+1-k} b^k \left(-\frac{1}{2}a^{-\frac{k+1}{2}} \Gamma\left(\frac{k+1}{2},ax^2\right)\right)  \\   &= -\frac{1}{\sqrt{a}\sqrt{\pi}}  \sum_{n=0}^\infty \frac{(-1)^n }{n!(2n+1)}     \sum_{k=0}^{2n+1} {{2n+1}\choose{k}} c^{2n+1-k} \left(\frac{b}{\sqrt{a}}\right)^k  \Gamma\left(\frac{k+1}{2},ax^2\right)  \end{aligned} $$ I have used the binomial expansion for $(bx + c)^{2n+1}$ and that $\int x^k \text{e}^{-ax^2}dx = -\frac{1}{2}a^{-\frac{k+1}{2}} \Gamma\left(\frac{k+1}{2} ,ax^2\right)$ where $\Gamma(,)$ is the incomplete gamma function. Too bad, the last term can not be combined again in the form of a binomial expansion. Approach #2: Instead of expanding the error function, I tried writing it in terms of the cumulative CDF function (Q-Function) as $\text{erf}(x) = 2\Phi(\sqrt{2} x) - 1$. However, the following can be shown to be true using integration under the integral sign with respect to $\mu$. [Section 2.4 , and ref] $$ \frac{1}{\sqrt{2 \pi} \sigma}\int_{-\infty}^{\infty}\Phi(\lambda x) \text{e}^{-\frac{(x - \mu)^2}{2 \sigma^2}}dx =\Phi\left(\frac{\lambda \mu}{\sqrt{1+\lambda^2\sigma^2}}\right) $$ Now with some change of variables and rescaling we are instead interested in the following integral: $$ \int\text{e}^{a_1 x^2 + a_2 x} \text{erf}\left(x\right) dx = \underbrace{2\int \text{e}^{a_1 x^2 + a_2 x} \Phi(\sqrt{2} x) dx}_{I} - \underbrace{\int \text{e}^{a_1 x^2 + a_2 x} dx}_{easy} $$ However, what I'm not certain of if I can use the trick of integration under integral sign for the indefinite integral labeled I. Can I, with some change of variables, use the result deduced for the definite integral case as $\Phi\left(\frac{\lambda \mu}{\sqrt{1+\lambda^2\sigma^2}}\right) + C(x)$? EDIT: It seems that the problem in had has no closed form solution as pointed out by user90369 . Also, user90369 has pointed out that the following more general case have no closed form solution.   $$ \int x^{2n} \text{e}^{-ax^2} \text{erf}(bx+c) dx $$   I was wondering, if there are any good approximations that I can use here. By good, I mean refer to an error that is $|e(x)| \leq 10^{-5} \forall x$.   For starter, I was looking at the high accuracy approximations in here for the erf function. Unfortunately, none of these approximations result into an integral that inherits a closed form solution. I, however, have the following suggested approach with the use of the following identity.   $$ \text{erf}(bx+c) = 2 \Phi\left(\sqrt{2} (bx+c)\right) - 1 $$   This results into the following:   $$ \begin{aligned} \int\text{e}^{-ax^2 } \text{erf}\left(bx + c\right) dx = 2\int \text{e}^{-ax^2 } \Phi\left(\sqrt{2} (bx+c)\right) dx - \int \text{e}^{-ax^2 } dx \end{aligned} $$   Now, one can use the approximation of the $\Phi$-function that results from applying Chernof's bound. Link $$ \Phi(x) \approx \frac{1}{12} \text{e}^{-\frac{x^2}{2}} + \frac{1}{4} \text{e}^{-\frac{2}{3} x^2} $$   I'd like to take a suggestion of how good is this approximation after computing the integral. Or maybe if there are other better approximations/recommendations that result into a manageable integral afterwards.","I'm hoping to find a closed expression for the following integral. $$ \int\text{e}^{-ax^2 } \text{erf}\left(bx + c\right) dx $$ One can find a solution for a family of products between exponentials and error functions. None of which apparently have the offset term in the error function. I have tried tackling the problem with two approaches. Approach #1: Expanding the error function hoping to find nice cancelations leading to the maclerin series of some known elementary function. Following a similar approach by Alex : $$ \begin{aligned} \int\text{e}^{-ax^2 } \text{erf}\left(bx + c\right) dx &= \frac{2}{\sqrt{\pi}}  \sum_{n=0}^\infty \frac{(-1)^n }{n!(2n+1)} \int(bx+c)^{2n +1} \text{e}^{-ax^2} dx \\ & = \frac{2}{\sqrt{\pi}}  \sum_{n=0}^\infty \frac{(-1)^n }{n!(2n+1)}  \int   \sum_{k=0}^{2n+1} {{2n+1}\choose{k}} (bx)^{k} c^{2n+1-k} \text{e}^{-ax^2} dx \\ & = \frac{2}{\sqrt{\pi}}  \sum_{n=0}^\infty \frac{(-1)^n }{n!(2n+1)}     \sum_{k=0}^{2n+1} {{2n+1}\choose{k}} c^{2n+1-k} b^k\int x^{k}  \text{e}^{-ax^2} dx  = \\ &\frac{2}{\sqrt{\pi}}  \sum_{n=0}^\infty \frac{(-1)^n }{n!(2n+1)}     \sum_{k=0}^{2n+1} {{2n+1}\choose{k}} c^{2n+1-k} b^k \left(-\frac{1}{2}a^{-\frac{k+1}{2}} \Gamma\left(\frac{k+1}{2},ax^2\right)\right)  \\   &= -\frac{1}{\sqrt{a}\sqrt{\pi}}  \sum_{n=0}^\infty \frac{(-1)^n }{n!(2n+1)}     \sum_{k=0}^{2n+1} {{2n+1}\choose{k}} c^{2n+1-k} \left(\frac{b}{\sqrt{a}}\right)^k  \Gamma\left(\frac{k+1}{2},ax^2\right)  \end{aligned} $$ I have used the binomial expansion for $(bx + c)^{2n+1}$ and that $\int x^k \text{e}^{-ax^2}dx = -\frac{1}{2}a^{-\frac{k+1}{2}} \Gamma\left(\frac{k+1}{2} ,ax^2\right)$ where $\Gamma(,)$ is the incomplete gamma function. Too bad, the last term can not be combined again in the form of a binomial expansion. Approach #2: Instead of expanding the error function, I tried writing it in terms of the cumulative CDF function (Q-Function) as $\text{erf}(x) = 2\Phi(\sqrt{2} x) - 1$. However, the following can be shown to be true using integration under the integral sign with respect to $\mu$. [Section 2.4 , and ref] $$ \frac{1}{\sqrt{2 \pi} \sigma}\int_{-\infty}^{\infty}\Phi(\lambda x) \text{e}^{-\frac{(x - \mu)^2}{2 \sigma^2}}dx =\Phi\left(\frac{\lambda \mu}{\sqrt{1+\lambda^2\sigma^2}}\right) $$ Now with some change of variables and rescaling we are instead interested in the following integral: $$ \int\text{e}^{a_1 x^2 + a_2 x} \text{erf}\left(x\right) dx = \underbrace{2\int \text{e}^{a_1 x^2 + a_2 x} \Phi(\sqrt{2} x) dx}_{I} - \underbrace{\int \text{e}^{a_1 x^2 + a_2 x} dx}_{easy} $$ However, what I'm not certain of if I can use the trick of integration under integral sign for the indefinite integral labeled I. Can I, with some change of variables, use the result deduced for the definite integral case as $\Phi\left(\frac{\lambda \mu}{\sqrt{1+\lambda^2\sigma^2}}\right) + C(x)$? EDIT: It seems that the problem in had has no closed form solution as pointed out by user90369 . Also, user90369 has pointed out that the following more general case have no closed form solution.   $$ \int x^{2n} \text{e}^{-ax^2} \text{erf}(bx+c) dx $$   I was wondering, if there are any good approximations that I can use here. By good, I mean refer to an error that is $|e(x)| \leq 10^{-5} \forall x$.   For starter, I was looking at the high accuracy approximations in here for the erf function. Unfortunately, none of these approximations result into an integral that inherits a closed form solution. I, however, have the following suggested approach with the use of the following identity.   $$ \text{erf}(bx+c) = 2 \Phi\left(\sqrt{2} (bx+c)\right) - 1 $$   This results into the following:   $$ \begin{aligned} \int\text{e}^{-ax^2 } \text{erf}\left(bx + c\right) dx = 2\int \text{e}^{-ax^2 } \Phi\left(\sqrt{2} (bx+c)\right) dx - \int \text{e}^{-ax^2 } dx \end{aligned} $$   Now, one can use the approximation of the $\Phi$-function that results from applying Chernof's bound. Link $$ \Phi(x) \approx \frac{1}{12} \text{e}^{-\frac{x^2}{2}} + \frac{1}{4} \text{e}^{-\frac{2}{3} x^2} $$   I'd like to take a suggestion of how good is this approximation after computing the integral. Or maybe if there are other better approximations/recommendations that result into a manageable integral afterwards.",,"['calculus', 'integration', 'definite-integrals', 'indefinite-integrals', 'error-function']"
4,Help an geographer define an equation!,Help an geographer define an equation!,,"I'm a physical geographer and I'm working on a piece of research where I have some broad physical relationships which I want to try and put into an equation - i.e. I want to define an empirical equation which describes the phenomena I've observed. I'm way out of my depth in formulating equations so I'm hoping someone here can help. I have a suspicion this is not a hugely complex problem for anyone who knows maths! It's probably not relevant to the question, but it's all about how the physical properties of peat bogs alter how their water content changes over time, and then how that water content, coupled with their density, affects wildfire risk. For a given point in time, and a given location I'll have a volumetric water content $V_x$. (So I I'll have a lots of these values for different times and locations) For each location I'll have a normally distributed bulk density $pb$, for which I know the mean and standard deviation. If the volumetric water content is multiplied by bulk density it gives gravimetic water content $U_x$. (So for any given $V_x$ and I'd have a bunch of values for $U_x$, which I think would be a probability distribution?) Gravimetric water content determines wildfire susceptibility (drier is more susceptible). It makes sense to define 2 thresholds: $U^*$, below which burning is almost certain, and $U_w$ above which burning is unlikely. I'd like to define this as probability of burning with 1 (or near 1) at $U^*$ and a probability of $~0.1$ to $0.2$ at $U_w$. Between these values the relationship would not be linear but would be a curve which drops off with increasing U. Ideally (as we're still trying to get a handle on this) there would be an adjustable factor for exactly how quickly this drops off with increasing $U$ approaching $U_w$. So what I'd like to do is take a probability distribution of $U$ (based on the $V_x$ and the distribution of pb values). And then multiple this by the probability of burning relationship so I can say; given a volumetric water content, and given the bulk density distribution, $K$ is the probability of wildfire burning. As I said, I'm just not good enough with maths to put all these things together into an equation! The ultimate idea is to propose this empirical equation which then I and other researcher can test. Thanks in advance for any help you can give in helping me define this.","I'm a physical geographer and I'm working on a piece of research where I have some broad physical relationships which I want to try and put into an equation - i.e. I want to define an empirical equation which describes the phenomena I've observed. I'm way out of my depth in formulating equations so I'm hoping someone here can help. I have a suspicion this is not a hugely complex problem for anyone who knows maths! It's probably not relevant to the question, but it's all about how the physical properties of peat bogs alter how their water content changes over time, and then how that water content, coupled with their density, affects wildfire risk. For a given point in time, and a given location I'll have a volumetric water content $V_x$. (So I I'll have a lots of these values for different times and locations) For each location I'll have a normally distributed bulk density $pb$, for which I know the mean and standard deviation. If the volumetric water content is multiplied by bulk density it gives gravimetic water content $U_x$. (So for any given $V_x$ and I'd have a bunch of values for $U_x$, which I think would be a probability distribution?) Gravimetric water content determines wildfire susceptibility (drier is more susceptible). It makes sense to define 2 thresholds: $U^*$, below which burning is almost certain, and $U_w$ above which burning is unlikely. I'd like to define this as probability of burning with 1 (or near 1) at $U^*$ and a probability of $~0.1$ to $0.2$ at $U_w$. Between these values the relationship would not be linear but would be a curve which drops off with increasing U. Ideally (as we're still trying to get a handle on this) there would be an adjustable factor for exactly how quickly this drops off with increasing $U$ approaching $U_w$. So what I'd like to do is take a probability distribution of $U$ (based on the $V_x$ and the distribution of pb values). And then multiple this by the probability of burning relationship so I can say; given a volumetric water content, and given the bulk density distribution, $K$ is the probability of wildfire burning. As I said, I'm just not good enough with maths to put all these things together into an equation! The ultimate idea is to propose this empirical equation which then I and other researcher can test. Thanks in advance for any help you can give in helping me define this.",,"['calculus', 'integration', 'statistics']"
5,How to truly understand integration and differentiation?,How to truly understand integration and differentiation?,,"I took Calc I course and currently studying Calc II. I'm pretty good at integration and differentiation in terms of mathematically solving equations, but I don't truly understand the concepts. When I apply integration rules to a solve a problem it just feels like magic to me, so I don't really get what's going on. For instance, if try to a solve an unfamiliar application problem I won't really be able to solve it. I feel like I'm just a useless calculator. At the beginning, When I started to learn about differentiation I had a pretty good understanding like finding the instantaneous rate of change, and understood the relation distance => velocity => acceleration (and why it works that way). Now I lost track. I want to be able to use these concepts creatively in real-world applications. Can you suggest a book or a give me an advice on how to truly understand these concepts?","I took Calc I course and currently studying Calc II. I'm pretty good at integration and differentiation in terms of mathematically solving equations, but I don't truly understand the concepts. When I apply integration rules to a solve a problem it just feels like magic to me, so I don't really get what's going on. For instance, if try to a solve an unfamiliar application problem I won't really be able to solve it. I feel like I'm just a useless calculator. At the beginning, When I started to learn about differentiation I had a pretty good understanding like finding the instantaneous rate of change, and understood the relation distance => velocity => acceleration (and why it works that way). Now I lost track. I want to be able to use these concepts creatively in real-world applications. Can you suggest a book or a give me an advice on how to truly understand these concepts?",,"['calculus', 'integration', 'derivatives']"
6,Absolute maximum of $f(x) = \sqrt{x^4-3x^2-6x+13}-\sqrt{x^4-x^2+1}$,Absolute maximum of,f(x) = \sqrt{x^4-3x^2-6x+13}-\sqrt{x^4-x^2+1},"My maths tutor solved  this by using $f(x) = \sqrt{(x^2-2)^2+(x-3)^2}-\sqrt{(x^2-1)^2+(x-0)^2}$ and treating $A(2,3)$ and $B(1,0)$ as fixed points and $P(x^2,x)$ as a moving point and using the difference between the sides PA and PB of the triangle PAB and relating that to the side AB to find the answer. i.e. $PA-PB \leq AB = \sqrt{(2-1)^2-(3-0)^2} = \sqrt{10}$ What are other methods to solve this and is there a specific way to turn any such equation into the form of 2 squares as demonstrated above or is that just a special instance wherein that is possible.","My maths tutor solved  this by using $f(x) = \sqrt{(x^2-2)^2+(x-3)^2}-\sqrt{(x^2-1)^2+(x-0)^2}$ and treating $A(2,3)$ and $B(1,0)$ as fixed points and $P(x^2,x)$ as a moving point and using the difference between the sides PA and PB of the triangle PAB and relating that to the side AB to find the answer. i.e. $PA-PB \leq AB = \sqrt{(2-1)^2-(3-0)^2} = \sqrt{10}$ What are other methods to solve this and is there a specific way to turn any such equation into the form of 2 squares as demonstrated above or is that just a special instance wherein that is possible.",,"['calculus', 'radicals', 'maxima-minima']"
7,An Inequality in Calculus,An Inequality in Calculus,,"Let $f(0)>0$ and $f(x)$ increases on $[0,1]$. There exists a positive number $s$ such that   $$\int_0^1xf(x)\mathrm{d}x=s\int_0^1f(x)\mathrm{d}x$$   Prove that   $$\int_0^sf(x)\mathrm{d}x\le\int_s^1f(x)\mathrm{d}x$$ This is a problem from my homework. Let $\displaystyle{g(x)=\int_0^xf(t)\mathrm{d}t},x\in[0,1]$. If $f$ IS CONTINUOUS, we have $g(0)=0, g(1)=1$(WLOG, we can presume that) and $s=\displaystyle{\int_0^1xg'(x)\mathrm{d}x=1-\int_0^1g(x)\mathrm{d}x}$, which is the same to $\displaystyle{\int_0^1g(x)\mathrm{d}x=1-s}$. If $g(s)>\frac{1}{2}$ and $f$ IS CONTINUOUS, we have $g(x)$ is convex on $[0,1]$. I used some inequalities to show that  $$\int_0^1g(x)\ge\frac{1-s}{2(1-g(s))}$$ and we get $g(s)\le\frac{1}{2}$, follws the contradiction. But the question is that I want to find a solution WITHOUT continuity (or this solution can be transformed) and I'd like this progress to be more ""analytical"" because I established a coordinate system to prove the inequality. Thanks for your help!","Let $f(0)>0$ and $f(x)$ increases on $[0,1]$. There exists a positive number $s$ such that   $$\int_0^1xf(x)\mathrm{d}x=s\int_0^1f(x)\mathrm{d}x$$   Prove that   $$\int_0^sf(x)\mathrm{d}x\le\int_s^1f(x)\mathrm{d}x$$ This is a problem from my homework. Let $\displaystyle{g(x)=\int_0^xf(t)\mathrm{d}t},x\in[0,1]$. If $f$ IS CONTINUOUS, we have $g(0)=0, g(1)=1$(WLOG, we can presume that) and $s=\displaystyle{\int_0^1xg'(x)\mathrm{d}x=1-\int_0^1g(x)\mathrm{d}x}$, which is the same to $\displaystyle{\int_0^1g(x)\mathrm{d}x=1-s}$. If $g(s)>\frac{1}{2}$ and $f$ IS CONTINUOUS, we have $g(x)$ is convex on $[0,1]$. I used some inequalities to show that  $$\int_0^1g(x)\ge\frac{1-s}{2(1-g(s))}$$ and we get $g(s)\le\frac{1}{2}$, follws the contradiction. But the question is that I want to find a solution WITHOUT continuity (or this solution can be transformed) and I'd like this progress to be more ""analytical"" because I established a coordinate system to prove the inequality. Thanks for your help!",,"['calculus', 'integration']"
8,Definite integral problem of $\frac{x^n}{n!}$,Definite integral problem of,\frac{x^n}{n!},"I want to evaluate the following definite integral. $$\int_0^\infty\frac{x^n}{n!}dn$$ Where we have $n!=\Gamma(n+1)=\int_0^\infty t^ne^{-t}dt$ so that we can have $n\in\mathbb{R}$. I don't think there is a solution to this, but I do note that $$\int_0^\infty\frac{x^n}{n!}dn\sim\sum_{n=0}^\infty\frac{x^n}{n!}=e^x$$ Also, we define $0^0=1$. The context to this problem is that I want to see if $$\int_0^\infty\frac{x^nf^{(n)}(a)}{n!}dn=\sum_{n=0}^\infty\frac{x^nf^{(n)}(a)}{n!}$$ A modification to Taylor's theorem for fractional calculus. Under my conditions, it is obvious that the two are equal for $x=0$, but even for the seemingly trivial case of $x=1$, I don't know how to solve the integral. WolframAlpha provides a series representation for the indefinite integral ($x=1$), but I am unsure if I can even use it. Feel free to tackle just the $x=1$ case.  From graphing, it seems very possible that $\int_0^\infty\frac1{n!}dn=e$, but I'm not fully sure. From wolframalpha , I have found that $\int_0^\infty\frac1{n!}d\approx2.2665$. EDIT Numerical calculations say $\int_{-\gamma}^\infty\frac1{n!}dn\approx2.70907$ where $\gamma$ is the Euler-Mascheroni constant.  It seems very close to $e$...","I want to evaluate the following definite integral. $$\int_0^\infty\frac{x^n}{n!}dn$$ Where we have $n!=\Gamma(n+1)=\int_0^\infty t^ne^{-t}dt$ so that we can have $n\in\mathbb{R}$. I don't think there is a solution to this, but I do note that $$\int_0^\infty\frac{x^n}{n!}dn\sim\sum_{n=0}^\infty\frac{x^n}{n!}=e^x$$ Also, we define $0^0=1$. The context to this problem is that I want to see if $$\int_0^\infty\frac{x^nf^{(n)}(a)}{n!}dn=\sum_{n=0}^\infty\frac{x^nf^{(n)}(a)}{n!}$$ A modification to Taylor's theorem for fractional calculus. Under my conditions, it is obvious that the two are equal for $x=0$, but even for the seemingly trivial case of $x=1$, I don't know how to solve the integral. WolframAlpha provides a series representation for the indefinite integral ($x=1$), but I am unsure if I can even use it. Feel free to tackle just the $x=1$ case.  From graphing, it seems very possible that $\int_0^\infty\frac1{n!}dn=e$, but I'm not fully sure. From wolframalpha , I have found that $\int_0^\infty\frac1{n!}d\approx2.2665$. EDIT Numerical calculations say $\int_{-\gamma}^\infty\frac1{n!}dn\approx2.70907$ where $\gamma$ is the Euler-Mascheroni constant.  It seems very close to $e$...",,"['calculus', 'definite-integrals', 'recreational-mathematics', 'gamma-function', 'fractional-calculus']"
9,$\pm$ sign in $y=\arcsin\frac{x}{\sqrt{1+x^2}}$,sign in,\pm y=\arcsin\frac{x}{\sqrt{1+x^2}},"If:  $$y=\arcsin\frac{x}{\sqrt{1+x^2}}$$ Then:  $$\sin(y)=\frac{x}{\sqrt{1+x^2}}$$ $$\cos^2(y)=1-\sin^2(y)=\frac{1}{1+x^2}$$ $$ \tan^2(y)=\sec^2(y)-1=1+x^2-1=x^2$$ Therefore I would say: $$\tan(y)=\pm x$$ However, my calculus book says (without the $\pm$): $$\tan(y)=x$$ Question: Why can we remove the $\pm$?","If:  $$y=\arcsin\frac{x}{\sqrt{1+x^2}}$$ Then:  $$\sin(y)=\frac{x}{\sqrt{1+x^2}}$$ $$\cos^2(y)=1-\sin^2(y)=\frac{1}{1+x^2}$$ $$ \tan^2(y)=\sec^2(y)-1=1+x^2-1=x^2$$ Therefore I would say: $$\tan(y)=\pm x$$ However, my calculus book says (without the $\pm$): $$\tan(y)=x$$ Question: Why can we remove the $\pm$?",,"['calculus', 'trigonometry']"
10,The magic of existential transfer,The magic of existential transfer,,"Yesterday I finished grading the final exam in a course on infinitesimal calculus taught to 130 freshmen. One of the problems on the exam was to show that if a function $f$ is differentiable at $c\in\mathbb{R}$ and satisfies $f'(c)>0$ then there is a point $x>c$ such that $f(x)>f(c)$. One of the students provided an original solution. If $\alpha>0$ is infinitesimal then $st(\frac{f(c+\alpha)-f(c)}{\alpha})>0$ and it easily follows that $f(c+\alpha)-f(c)>0$. Hence the hyperreal $x=c+\alpha$ satisfies $f(x)>f(c)$ and therefore the following formula is satisfied over the hyperreals: $$(\exists x)\;f(x)>f(c).$$ Now by existential transfer the same formula holds over the reals. Therefore there exists $x\in\mathbb{R}$ such that $f(x)>f(c)$, QED. Are there additional examples of existential transfer at this level?","Yesterday I finished grading the final exam in a course on infinitesimal calculus taught to 130 freshmen. One of the problems on the exam was to show that if a function $f$ is differentiable at $c\in\mathbb{R}$ and satisfies $f'(c)>0$ then there is a point $x>c$ such that $f(x)>f(c)$. One of the students provided an original solution. If $\alpha>0$ is infinitesimal then $st(\frac{f(c+\alpha)-f(c)}{\alpha})>0$ and it easily follows that $f(c+\alpha)-f(c)>0$. Hence the hyperreal $x=c+\alpha$ satisfies $f(x)>f(c)$ and therefore the following formula is satisfied over the hyperreals: $$(\exists x)\;f(x)>f(c).$$ Now by existential transfer the same formula holds over the reals. Therefore there exists $x\in\mathbb{R}$ such that $f(x)>f(c)$, QED. Are there additional examples of existential transfer at this level?",,"['calculus', 'logic', 'nonstandard-analysis']"
11,Freshman calculus - Stokes's theorem proof,Freshman calculus - Stokes's theorem proof,,"Many calculus text books and courses do not introduce  full proof of Stokes's theorem because of differential forms  and topological concepts. There are only restrict proofs (for example, simple region , $C^2$-parametrization of boundaries of surfaces ). Is there any complete proof for Stokes's theorem in $\rm R^3$ without using differential forms and topological concepts? If it is possible, I want to teach the complete proof in TA-course.","Many calculus text books and courses do not introduce  full proof of Stokes's theorem because of differential forms  and topological concepts. There are only restrict proofs (for example, simple region , $C^2$-parametrization of boundaries of surfaces ). Is there any complete proof for Stokes's theorem in $\rm R^3$ without using differential forms and topological concepts? If it is possible, I want to teach the complete proof in TA-course.",,"['calculus', 'multivariable-calculus', 'education', 'stokes-theorem']"
12,The value of the integral [closed],The value of the integral [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question $$I = \int\limits_0^{\frac{\pi }{2}} {\frac{{x\sin 2x}}{{\left( {1 + \pi {{\sin }^2}x} \right)\left( {1 + 2\pi {{\sin }^2}x} \right)}}} dx$$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question $$I = \int\limits_0^{\frac{\pi }{2}} {\frac{{x\sin 2x}}{{\left( {1 + \pi {{\sin }^2}x} \right)\left( {1 + 2\pi {{\sin }^2}x} \right)}}} dx$$",,"['calculus', 'integration', 'definite-integrals']"
13,What's the differences between multi variable and vector calculus,What's the differences between multi variable and vector calculus,,"This is a conceptual question. If we use vector calculus and multi variable calculus as synonym, will it be completely wrong? If so what topics does multi variable calculus have but vector calculus doesn't? What is the difference?","This is a conceptual question. If we use vector calculus and multi variable calculus as synonym, will it be completely wrong? If so what topics does multi variable calculus have but vector calculus doesn't? What is the difference?",,"['calculus', 'multivariable-calculus']"
14,Solution to $y'=y^2-4$,Solution to,y'=y^2-4,"I recognize this as a separable differential equation and receive the expression: $\frac{dy}{y^2-4}=dx$ The issue comes about when evaluating the left hand side integral: $\frac{dy}{y^2-4}$ I attempt to do this integral through partial fraction decomposition using the following logic: $\frac{1}{(y+2)(y-2)} = \frac{A}{y+2}+\frac{B}{y-2}$ Therefore, $1=Ay-2A+By+2B$. Since the coefficients must be the same on both sides of the equation it follows that: $0=A+B$    and $1=-2A+2B$. Hence, $A=-B$, $B=\frac14$, $A=-\frac14$. Thus the differential equation should be transformed into: $-\frac{1}{4} \frac{dy}{y+2} + \frac14 \frac{dy}{y-2} = x+C$ Solving this should yield: $-\frac14 \ln|y+2| + \frac14 \ln|y-2| = x+C$ which simplifies as: $\ln(y-2)-\ln(y+2)=4(x+c)$ $\ln[(y-2)/(y+2)]=4(x+c)$ $(y-2)/(y+2)=\exp(4(x+c))$ $y-2=y*\exp(4(x+c)+2\exp(4(x+c))$ $y-y\exp(4(x+c))=2+2\exp(4(x+c))$ $y(1-\exp(4(x+c)))=2(1+\exp(4(x+c)))$ $y= 2(1+\exp(4(x+c)))/(1-\exp(4(x+c)))$ However, when done in Mathematica/Wolfram Alpha the result is given as (proof in the attached image) $\frac14 \ln(2-y) -\frac14 \ln(2+y) = x + C$ and returns an answer of: $y= 2(1-\exp(4(x+c)))/(1+\exp(4(x+c)))$. Can anyone figure out where I have made an error? The only thing I can think of is something with evaluating the absolute values of the natural logarithms.","I recognize this as a separable differential equation and receive the expression: $\frac{dy}{y^2-4}=dx$ The issue comes about when evaluating the left hand side integral: $\frac{dy}{y^2-4}$ I attempt to do this integral through partial fraction decomposition using the following logic: $\frac{1}{(y+2)(y-2)} = \frac{A}{y+2}+\frac{B}{y-2}$ Therefore, $1=Ay-2A+By+2B$. Since the coefficients must be the same on both sides of the equation it follows that: $0=A+B$    and $1=-2A+2B$. Hence, $A=-B$, $B=\frac14$, $A=-\frac14$. Thus the differential equation should be transformed into: $-\frac{1}{4} \frac{dy}{y+2} + \frac14 \frac{dy}{y-2} = x+C$ Solving this should yield: $-\frac14 \ln|y+2| + \frac14 \ln|y-2| = x+C$ which simplifies as: $\ln(y-2)-\ln(y+2)=4(x+c)$ $\ln[(y-2)/(y+2)]=4(x+c)$ $(y-2)/(y+2)=\exp(4(x+c))$ $y-2=y*\exp(4(x+c)+2\exp(4(x+c))$ $y-y\exp(4(x+c))=2+2\exp(4(x+c))$ $y(1-\exp(4(x+c)))=2(1+\exp(4(x+c)))$ $y= 2(1+\exp(4(x+c)))/(1-\exp(4(x+c)))$ However, when done in Mathematica/Wolfram Alpha the result is given as (proof in the attached image) $\frac14 \ln(2-y) -\frac14 \ln(2+y) = x + C$ and returns an answer of: $y= 2(1-\exp(4(x+c)))/(1+\exp(4(x+c)))$. Can anyone figure out where I have made an error? The only thing I can think of is something with evaluating the absolute values of the natural logarithms.",,"['calculus', 'differential']"
15,How do I solve the following differential equation,How do I solve the following differential equation,,$$\frac{d^2y}{dx^2}=x^2y$$ Solving it by writing out a characteristic equation is not helping me find the solution to the above equation. Any help would be appreciated thanks.,$$\frac{d^2y}{dx^2}=x^2y$$ Solving it by writing out a characteristic equation is not helping me find the solution to the above equation. Any help would be appreciated thanks.,,"['calculus', 'ordinary-differential-equations']"
16,"Integration of $\frac{e^{\cos^2x}+\ln(1+x)}{10^{x^3}\arctan(\sqrt{x})}$, possibly numerical","Integration of , possibly numerical",\frac{e^{\cos^2x}+\ln(1+x)}{10^{x^3}\arctan(\sqrt{x})},"A couple of days ago I came across the following integral: $$\int_{0.02}^{0.08} \frac{e^{\cos^2x}+\ln(1+x)}{10^{x^3}\arctan(\sqrt{x})}\,{\rm d}x$$ The funny thing is, I found this integral written in a door of a bathroom stall in my university. I did what any reasonable person would do: I took a picture of the integral and tried to solve it later. Needless to say, I couldn't do it. Sure, there is a high chance of the person who wrote this being just a troll. However, I put this in Wolfram Alpha and it gave $0.791523$ as the answer. Such a neat rational number. And I wonder: is there a straightforward way to attack this? If you want to use numerical methods, fine, as long it is possible to do it by hand.","A couple of days ago I came across the following integral: $$\int_{0.02}^{0.08} \frac{e^{\cos^2x}+\ln(1+x)}{10^{x^3}\arctan(\sqrt{x})}\,{\rm d}x$$ The funny thing is, I found this integral written in a door of a bathroom stall in my university. I did what any reasonable person would do: I took a picture of the integral and tried to solve it later. Needless to say, I couldn't do it. Sure, there is a high chance of the person who wrote this being just a troll. However, I put this in Wolfram Alpha and it gave $0.791523$ as the answer. Such a neat rational number. And I wonder: is there a straightforward way to attack this? If you want to use numerical methods, fine, as long it is possible to do it by hand.",,"['calculus', 'integration']"
17,Solving an integral for a characteristic function,Solving an integral for a characteristic function,,"For $L>0, H>0,\alpha>0,\sigma>0,$ $$f(t)=\int_L^H \frac{ e^{i t x} \alpha  H \left(\frac{\sigma -H \log \left(\frac{H-x}{H-L}\right)}{\sigma }\right)^{-\alpha -1}}{\sigma  (H-x)} \, \mathrm{d}x$$ $\textbf{Background}$ This is the characteristic function of a nonstandard probability distribution.","For $L>0, H>0,\alpha>0,\sigma>0,$ $$f(t)=\int_L^H \frac{ e^{i t x} \alpha  H \left(\frac{\sigma -H \log \left(\frac{H-x}{H-L}\right)}{\sigma }\right)^{-\alpha -1}}{\sigma  (H-x)} \, \mathrm{d}x$$ $\textbf{Background}$ This is the characteristic function of a nonstandard probability distribution.",,"['calculus', 'integration', 'probability-theory', 'probability-distributions', 'characteristic-functions']"
18,"Closed-form of $\int_0^{\pi/2} \arctan(x)\cot(x)\,dx$",Closed-form of,"\int_0^{\pi/2} \arctan(x)\cot(x)\,dx","I'm looking for a closed-form of the following integral problem. $$I = \int_0^{\pi/2} \arctan(x)\cot(x)\,dx.$$ The numerical approximation of $I$ is $$I \approx 0.96644524676637380447182915131032699868606574138656587245691342\dots$$ I've found nothing with Maple or Mathematica .","I'm looking for a closed-form of the following integral problem. $$I = \int_0^{\pi/2} \arctan(x)\cot(x)\,dx.$$ The numerical approximation of $I$ is $$I \approx 0.96644524676637380447182915131032699868606574138656587245691342\dots$$ I've found nothing with Maple or Mathematica .",,"['calculus', 'definite-integrals', 'closed-form']"
19,How would you answer this integral using integration by parts?,How would you answer this integral using integration by parts?,,$$\int \frac {xe^{2x}}{(1+2x)^2}dx$$ I've tried setting $u=e^2$ and $dv=\frac {x}{(1+2x)^2}$ but I'm getting a really long partial answer like: $$\int \frac {xe^2}{(1+2x)^2}dx = e^{2x}\left[\frac {\ln (1+2x)}{4}+ \frac {1}{4(1+2x)^2}\right] - \ldots$$ I integrated $\frac {x}{(1+2x)^2}$ by decomposing the fraction into $\frac {1}{2(1+2x)}- \frac {1}{2(1+2x)^2}$ which would then give the integral as $$\int \frac {x}{(1+2x)^2}dx = \int \left[ \frac {1}{2(1+2x)} - \frac {1}{2(1+2x)^2}\right]dx$$ But yes I'm still stuck after doing all of this things. Am I on the right path or should I change the values of $u$ and $dv$?,$$\int \frac {xe^{2x}}{(1+2x)^2}dx$$ I've tried setting $u=e^2$ and $dv=\frac {x}{(1+2x)^2}$ but I'm getting a really long partial answer like: $$\int \frac {xe^2}{(1+2x)^2}dx = e^{2x}\left[\frac {\ln (1+2x)}{4}+ \frac {1}{4(1+2x)^2}\right] - \ldots$$ I integrated $\frac {x}{(1+2x)^2}$ by decomposing the fraction into $\frac {1}{2(1+2x)}- \frac {1}{2(1+2x)^2}$ which would then give the integral as $$\int \frac {x}{(1+2x)^2}dx = \int \left[ \frac {1}{2(1+2x)} - \frac {1}{2(1+2x)^2}\right]dx$$ But yes I'm still stuck after doing all of this things. Am I on the right path or should I change the values of $u$ and $dv$?,,['calculus']
20,Geometric Interpretation of Liouville's Theorem?,Geometric Interpretation of Liouville's Theorem?,,"The only bounded entire functions in $\mathbb{C}$ are constants. Could someone please give me a geometric interpretation of the theorem above? I don't intuitively understand why it's true. Also, aren't periodic functions counterexamples? i.e. $\sin(z)$?","The only bounded entire functions in $\mathbb{C}$ are constants. Could someone please give me a geometric interpretation of the theorem above? I don't intuitively understand why it's true. Also, aren't periodic functions counterexamples? i.e. $\sin(z)$?",,"['calculus', 'complex-analysis', 'multivariable-calculus']"
21,"Length of parametric curve $\phi(t)=(f(t)\cos(t),f(t)\sin(t))$",Length of parametric curve,"\phi(t)=(f(t)\cos(t),f(t)\sin(t))","Define the curve $\phi$ by $\phi(t):=(f(t)\cos(t),f(t)\sin(t))$, where $f$ be a strictly increasing infinitly many differentiable function . Find an explicit formula for the length of $\phi$ between $\phi(a)$ and $\phi(b)$. By using the definition of the length of a curve, you could reduce the problem to compute the integral $$\int_a^b \sqrt{f'(x)^2 + f(x)^2} dx $$ But now tricks by change of variables and integration by parts seems not to be helpful. Probably this integral cannot be computed explicitly in terms of antiderivatives.","Define the curve $\phi$ by $\phi(t):=(f(t)\cos(t),f(t)\sin(t))$, where $f$ be a strictly increasing infinitly many differentiable function . Find an explicit formula for the length of $\phi$ between $\phi(a)$ and $\phi(b)$. By using the definition of the length of a curve, you could reduce the problem to compute the integral $$\int_a^b \sqrt{f'(x)^2 + f(x)^2} dx $$ But now tricks by change of variables and integration by parts seems not to be helpful. Probably this integral cannot be computed explicitly in terms of antiderivatives.",,"['calculus', 'integration', 'definite-integrals', 'parametric', 'curves']"
22,Motivation behind parameters,Motivation behind parameters,,"This article shows a technique of evaluating a definite integral by introducing a suitable parameter. This however doesn't throw light on motivation for introducing that particular parameter . For inctance: $1)$ $\int_0^\infty \dfrac{\sin x}{x}dx $ can be evaluated by introducing the function $f(b)=\int_0^\infty \dfrac{\sin x}{x}e^{bx}dx$ $2)$ $\int_0^{\pi/2} x\cot x dx$ can be evaluated by introducing $f(b)=\int_0^{\pi/2}\dfrac{\arctan(b\tan(x))}{\tan (x)} dx$ What is the motivation behind these parameters, in general how would I  find a parameter to evaluate a particular definite integral without trail and error?","This article shows a technique of evaluating a definite integral by introducing a suitable parameter. This however doesn't throw light on motivation for introducing that particular parameter . For inctance: $1)$ $\int_0^\infty \dfrac{\sin x}{x}dx $ can be evaluated by introducing the function $f(b)=\int_0^\infty \dfrac{\sin x}{x}e^{bx}dx$ $2)$ $\int_0^{\pi/2} x\cot x dx$ can be evaluated by introducing $f(b)=\int_0^{\pi/2}\dfrac{\arctan(b\tan(x))}{\tan (x)} dx$ What is the motivation behind these parameters, in general how would I  find a parameter to evaluate a particular definite integral without trail and error?",,"['calculus', 'definite-integrals', 'motivation']"
23,"What is the integral $\int x^t/\Gamma(1+t) \, dt$? (In general: relation between series and integrals)",What is the integral ? (In general: relation between series and integrals),"\int x^t/\Gamma(1+t) \, dt","(The question arises from playing with translating series into integrals) I wanted to see, what it means to have a ""continuous"" relative for powerseries and other series; the most simple one perhaps $$ \begin{array} {} f_1(x) = \sum _{k=0}^\infty x^k = {1 \over 1-x} &\to & g_1(x)= \int_0^\infty x^t \,dt = - { 1\over \log(x) } \end{array}$$ I couldn't get this  $$ \begin{array} {} f_2(x)=\sum _{k=0}^\infty {x^k \over k!} = \exp(x) &\to& g_2(x)=\int_0^\infty { x^t\over \Gamma(1+t) } \,dt =\text{ ???} \end{array}$$ by, for instance, Wolfram alpha... and I'd like to proceed to some more general $$ \begin{array} {} f_\varphi(x)=\sum _{k=0}^\infty \varphi(k) x^k   &\to& g_\varphi(x)=\int_0^\infty \varphi(t) x^t \,dt =\text{ ???} \end{array}$$ where $\varphi(k)$ is some meaningful function producing common sets of coefficients for the power series. Playing a bit with numerical evaluations for $f_2(x),g_2(x)$ so far did not uncover anything obvious, but I am interested, whether there are some relations known; for instance whether there are relations between $g_2(x) \cdot g_2(y)$ perhaps  analoguously to $ f_2(x) \cdot f_2(y) = f_2(x+y)$ or the like. Is there something known about it? Is there a possibly a list of sums/integrals-relations done elsewhere? In general, I'd like to get more intuition about this; it reminds me that I should possibly re-read in the explanations for the Euler/MacLaurin-summation formula where the ""dance between discrete and continuous"" (as it is a title of a nice article about the work of Delabaere on Euler's divergent series) has a similar relevance. (But this is possibly too much for this Q&A-site ...)","(The question arises from playing with translating series into integrals) I wanted to see, what it means to have a ""continuous"" relative for powerseries and other series; the most simple one perhaps $$ \begin{array} {} f_1(x) = \sum _{k=0}^\infty x^k = {1 \over 1-x} &\to & g_1(x)= \int_0^\infty x^t \,dt = - { 1\over \log(x) } \end{array}$$ I couldn't get this  $$ \begin{array} {} f_2(x)=\sum _{k=0}^\infty {x^k \over k!} = \exp(x) &\to& g_2(x)=\int_0^\infty { x^t\over \Gamma(1+t) } \,dt =\text{ ???} \end{array}$$ by, for instance, Wolfram alpha... and I'd like to proceed to some more general $$ \begin{array} {} f_\varphi(x)=\sum _{k=0}^\infty \varphi(k) x^k   &\to& g_\varphi(x)=\int_0^\infty \varphi(t) x^t \,dt =\text{ ???} \end{array}$$ where $\varphi(k)$ is some meaningful function producing common sets of coefficients for the power series. Playing a bit with numerical evaluations for $f_2(x),g_2(x)$ so far did not uncover anything obvious, but I am interested, whether there are some relations known; for instance whether there are relations between $g_2(x) \cdot g_2(y)$ perhaps  analoguously to $ f_2(x) \cdot f_2(y) = f_2(x+y)$ or the like. Is there something known about it? Is there a possibly a list of sums/integrals-relations done elsewhere? In general, I'd like to get more intuition about this; it reminds me that I should possibly re-read in the explanations for the Euler/MacLaurin-summation formula where the ""dance between discrete and continuous"" (as it is a title of a nice article about the work of Delabaere on Euler's divergent series) has a similar relevance. (But this is possibly too much for this Q&A-site ...)",,"['calculus', 'elementary-number-theory', 'recreational-mathematics']"
24,Taylor Series of $\frac{1}{1-\cos x}$,Taylor Series of,\frac{1}{1-\cos x},"The problem is, as the title suggests, to find the Power Series Expansion of $\frac{1}{1- \cos x}$ around $x=c$. What I've tried: Direct Computation: Derivatives get very ugly quickly, and don't yield a nice formula that I can recognize as a ""series."" Tried finding the integral of $\frac{1}{1- \cos x}$, finding it's series and then differentiating it to get the new series. Tried reverse of the above, differentiating and finding it's series, then integrating (very messy). Then I tried some substitution ""tricks"", like using the series of $\frac{1}{1-x}$ and then plugging in the series expansion for $\cos x$, but that's a double sum that I struggled to produce anything useful from :$\displaystyle \sum_{k=0}^\infty\left(\sum_{n=0}^\infty\frac{(-1)^nx^{2n}}{(2n)!}\right)^k$ I am literally at my witts end with this problem. I have spent perhaps a day or two trying to figure it out, because I feel that I am so close - but just barely missing something. I do not want the solution posted - now it's personal and I have to figure it out, but I would greatly appreciate a hint in the right direction, or to point out a mistake that I may be overlooking.","The problem is, as the title suggests, to find the Power Series Expansion of $\frac{1}{1- \cos x}$ around $x=c$. What I've tried: Direct Computation: Derivatives get very ugly quickly, and don't yield a nice formula that I can recognize as a ""series."" Tried finding the integral of $\frac{1}{1- \cos x}$, finding it's series and then differentiating it to get the new series. Tried reverse of the above, differentiating and finding it's series, then integrating (very messy). Then I tried some substitution ""tricks"", like using the series of $\frac{1}{1-x}$ and then plugging in the series expansion for $\cos x$, but that's a double sum that I struggled to produce anything useful from :$\displaystyle \sum_{k=0}^\infty\left(\sum_{n=0}^\infty\frac{(-1)^nx^{2n}}{(2n)!}\right)^k$ I am literally at my witts end with this problem. I have spent perhaps a day or two trying to figure it out, because I feel that I am so close - but just barely missing something. I do not want the solution posted - now it's personal and I have to figure it out, but I would greatly appreciate a hint in the right direction, or to point out a mistake that I may be overlooking.",,"['calculus', 'power-series', 'taylor-expansion']"
25,Calculus 2 Trigonometric Integrals with odd exponents,Calculus 2 Trigonometric Integrals with odd exponents,,I do not know how to go about taking the integral of this. I have tried to break it up so I can take the integral of sin^2(x) sin(x) cos^8(x) cos(x). But then this would require me use the reduction formula for the integral of cos^8(x) I believe. If anyone would spare some time to help me with hints or steps to take I would be very thankful.,I do not know how to go about taking the integral of this. I have tried to break it up so I can take the integral of sin^2(x) sin(x) cos^8(x) cos(x). But then this would require me use the reduction formula for the integral of cos^8(x) I believe. If anyone would spare some time to help me with hints or steps to take I would be very thankful.,,"['calculus', 'integration', 'trigonometry']"
26,Calculus 2 Trigonometric Integrals,Calculus 2 Trigonometric Integrals,,"I have used the reduction formula for the integral of cos^n(x)dx but was unable to produce a correct answer. I think what throws me off is the (9x). Any helpful hints, tips, or other methods for this would be greatly appreciated.  This was the answer I came up with through use of the formula.","I have used the reduction formula for the integral of cos^n(x)dx but was unable to produce a correct answer. I think what throws me off is the (9x). Any helpful hints, tips, or other methods for this would be greatly appreciated.  This was the answer I came up with through use of the formula.",,"['calculus', 'integration', 'trigonometry']"
27,Riccati differential equation,Riccati differential equation,,"I have to solve the following equation :    $$ \frac{dx}{dt}(t)=-q x^2(t) +1  $$ with $x(0)=1$ and $q>0$. At first I consider the two cases: $q=1$, then I take the change of variable  $ x= \frac{u^{\prime}}{u}$ then with small calculations I got the second ord er linear homogeneous differential equation $ u^{\prime\prime} -u =0$, a solution of this equation is $u=c_1+c_2e^t$ and again to our x we get $x=\frac{c_2e^t}{c_1+c_2e^t}$ and with the initial condition  $x(0)=1$ we get $x=1$ if $q$ is not $1$, directly I assume that $-q x^2(t) +1 $ is non zero and solve the equation by simple integration of $$ \int \frac{dx}{1-qx^2}= \int 1 dt$$ then I got the following solution $$ x(t)= \frac{1}{\sqrt{q} \tanh(t\sqrt{q} +c)}$$ where c is the constant determined from the initial condition $c=\operatorname{atanh}(\sqrt{q})$ Are these discussions and solution steps correct?  May I assume in the second case that $-q x^2(t) +1 $ is non zero directly?   and in the first case, considering the change of variable  $ x= \frac{u^{\prime}}{u}$  with no assumptions on $u$ is correct? Thanks for any ideas.","I have to solve the following equation :    $$ \frac{dx}{dt}(t)=-q x^2(t) +1  $$ with $x(0)=1$ and $q>0$. At first I consider the two cases: $q=1$, then I take the change of variable  $ x= \frac{u^{\prime}}{u}$ then with small calculations I got the second ord er linear homogeneous differential equation $ u^{\prime\prime} -u =0$, a solution of this equation is $u=c_1+c_2e^t$ and again to our x we get $x=\frac{c_2e^t}{c_1+c_2e^t}$ and with the initial condition  $x(0)=1$ we get $x=1$ if $q$ is not $1$, directly I assume that $-q x^2(t) +1 $ is non zero and solve the equation by simple integration of $$ \int \frac{dx}{1-qx^2}= \int 1 dt$$ then I got the following solution $$ x(t)= \frac{1}{\sqrt{q} \tanh(t\sqrt{q} +c)}$$ where c is the constant determined from the initial condition $c=\operatorname{atanh}(\sqrt{q})$ Are these discussions and solution steps correct?  May I assume in the second case that $-q x^2(t) +1 $ is non zero directly?   and in the first case, considering the change of variable  $ x= \frac{u^{\prime}}{u}$  with no assumptions on $u$ is correct? Thanks for any ideas.",,"['calculus', 'ordinary-differential-equations']"
28,A problem about limit,A problem about limit,,"Problem: Suppose a sequence $\{x_n\}_{n\in\mathbb{N}}$ satisfies that  $$\lim_{n\to\infty}\bigg(x_n\cdot\sum_{k=0}^nx_k^2\bigg)=1.$$ Prove that $\lim_{n\to\infty}\sqrt[3]{3n}x_n=1$. I do not have much idea about how to construct precisely this limit since it is hard to imagine how the ""$3$"" under the root symbol comes. Can anyone give me some hints? Many thanks!","Problem: Suppose a sequence $\{x_n\}_{n\in\mathbb{N}}$ satisfies that  $$\lim_{n\to\infty}\bigg(x_n\cdot\sum_{k=0}^nx_k^2\bigg)=1.$$ Prove that $\lim_{n\to\infty}\sqrt[3]{3n}x_n=1$. I do not have much idea about how to construct precisely this limit since it is hard to imagine how the ""$3$"" under the root symbol comes. Can anyone give me some hints? Many thanks!",,"['calculus', 'analysis']"
29,Recognizing that a function has no elementary antiderivative [duplicate],Recognizing that a function has no elementary antiderivative [duplicate],,"This question already has answers here : How can you prove that a function has no closed form integral? (7 answers) Closed 9 years ago . Is there a method to check whether a function is integrable? Of-course trying to solve it is one but some questions in integration may be so tricky that I don't get the correct method to start off with those problems. So, is there a method to find correctly whether a function is integrable? Clarification: I am asking about indefinite integrals which have no elementary anti derivative.","This question already has answers here : How can you prove that a function has no closed form integral? (7 answers) Closed 9 years ago . Is there a method to check whether a function is integrable? Of-course trying to solve it is one but some questions in integration may be so tricky that I don't get the correct method to start off with those problems. So, is there a method to find correctly whether a function is integrable? Clarification: I am asking about indefinite integrals which have no elementary anti derivative.",,"['calculus', 'integration']"
30,Equation $\displaystyle\sum_{k=0}^{n-1}(-1)^{n-k-1}\dfrac{(n+k)!}{(k!)^2(n-k-1)!}=n^2$,Equation,\displaystyle\sum_{k=0}^{n-1}(-1)^{n-k-1}\dfrac{(n+k)!}{(k!)^2(n-k-1)!}=n^2,I think  this equality is very inters prove that: $\displaystyle\sum_{k=0}^{n-1}(-1)^{n-k-1}\dfrac{(n+k)!}{(k!)^2(n-k-1)!}=n^2$,I think  this equality is very inters prove that: $\displaystyle\sum_{k=0}^{n-1}(-1)^{n-k-1}\dfrac{(n+k)!}{(k!)^2(n-k-1)!}=n^2$,,"['calculus', 'sequences-and-series', 'discrete-mathematics']"
31,Help make Wonder Woman's box big.,Help make Wonder Woman's box big.,,"Wonder Woman wants a box for her lasso. It is to be built from a rectangular piece of steel measuring 25 cm by 40 cm by cutting out a square from each corner and then bending up the sides. Find the size of the corner square which will produce a container that will hold the most. Per suggestion by user Mark S, rewrote question to accept answer.","Wonder Woman wants a box for her lasso. It is to be built from a rectangular piece of steel measuring 25 cm by 40 cm by cutting out a square from each corner and then bending up the sides. Find the size of the corner square which will produce a container that will hold the most. Per suggestion by user Mark S, rewrote question to accept answer.",,['calculus']
32,How to prove $\nabla\cdot \vec{B}=0 \Rightarrow \exists \vec{A}:\vec{B}=\nabla \times \vec{A}$,How to prove,\nabla\cdot \vec{B}=0 \Rightarrow \exists \vec{A}:\vec{B}=\nabla \times \vec{A},"Suppose $\vec{B}$ is a differentiable vector field defined everywhere such that $\nabla\cdot \vec{B}=0$. Define $\vec{A}$ by the integral $$A_1=\int_0^1 \lambda(zB_2(\lambda x,\lambda y,\lambda z)- yB_3(\lambda x,\lambda y,\lambda z)) d\lambda$$ Together with its two cyclic permutations for $A_2,A_3$ I'm trying to work out two things here: $1.$ What is $\frac{d}{d\lambda}B_i(\lambda x,\lambda y,\lambda z)$ $2.$ How we can use $1.$ to determine $\frac{\partial A_2}{\partial x}-\frac{\partial A_1}{\partial y}=B_3$ From this we can deduce the existance of the magnetic potential by extending $2.$, this is what I have so far: Is $\frac{d}{d\lambda}B_i(\lambda x,\lambda y,\lambda z)=(x,y,x) \cdot \nabla B_i$? And can we bring the partial derivative on $A_i$ inside the integral? I have proceeded along these lines but have not found a way to substitute. Any help would be greatly appreciated!","Suppose $\vec{B}$ is a differentiable vector field defined everywhere such that $\nabla\cdot \vec{B}=0$. Define $\vec{A}$ by the integral $$A_1=\int_0^1 \lambda(zB_2(\lambda x,\lambda y,\lambda z)- yB_3(\lambda x,\lambda y,\lambda z)) d\lambda$$ Together with its two cyclic permutations for $A_2,A_3$ I'm trying to work out two things here: $1.$ What is $\frac{d}{d\lambda}B_i(\lambda x,\lambda y,\lambda z)$ $2.$ How we can use $1.$ to determine $\frac{\partial A_2}{\partial x}-\frac{\partial A_1}{\partial y}=B_3$ From this we can deduce the existance of the magnetic potential by extending $2.$, this is what I have so far: Is $\frac{d}{d\lambda}B_i(\lambda x,\lambda y,\lambda z)=(x,y,x) \cdot \nabla B_i$? And can we bring the partial derivative on $A_i$ inside the integral? I have proceeded along these lines but have not found a way to substitute. Any help would be greatly appreciated!",,['calculus']
33,Implicit use of the Implicit Function Theorem when finding tangent lines to polar curves.,Implicit use of the Implicit Function Theorem when finding tangent lines to polar curves.,,"Recently I found myself having to teach students how to find the slope of a tangent line to a curve in $\mathbb R^2$ given in polar coordinates by the equation $r = f(\theta)$. The students' calculus book instructs them that surely the slope of the tangent line must be given by $\frac{dy}{dx}$ and uses the chain rule to calculate $$\frac{dy}{dx} = \frac{dy/d\theta}{dx/d\theta}$$ Then, using the fact that $x = r\cos\theta$ and $y = r\sin\theta$ they are therefore able to find a formula for $\frac{dy}{dx}$ in terms of $r$ and $\theta$. There is however a problem with this line of reasoning, namely that there is no compelling reason for $y$ to be defined even locally as a function of $x$. Knowledge of the implicit function theorem allows one to formulate the hypothesis necessary to make the above use of the chain rule correct. However it would be nice to justify the use of the chain rule here using only methods available to a first year calculus student. My question is therefore Is there a nice way (intuitive or rigorous) of explaining when and why $y$ is a differentiable function of $x$ in this case, using only methods available to a first year calculus student? Yes, it is possible to look through the proof of the implicit function theorem and simplify it in this case, but I am especially interested in hearing the thoughts of experienced teachers, so I hope I am justified in asking the question.","Recently I found myself having to teach students how to find the slope of a tangent line to a curve in $\mathbb R^2$ given in polar coordinates by the equation $r = f(\theta)$. The students' calculus book instructs them that surely the slope of the tangent line must be given by $\frac{dy}{dx}$ and uses the chain rule to calculate $$\frac{dy}{dx} = \frac{dy/d\theta}{dx/d\theta}$$ Then, using the fact that $x = r\cos\theta$ and $y = r\sin\theta$ they are therefore able to find a formula for $\frac{dy}{dx}$ in terms of $r$ and $\theta$. There is however a problem with this line of reasoning, namely that there is no compelling reason for $y$ to be defined even locally as a function of $x$. Knowledge of the implicit function theorem allows one to formulate the hypothesis necessary to make the above use of the chain rule correct. However it would be nice to justify the use of the chain rule here using only methods available to a first year calculus student. My question is therefore Is there a nice way (intuitive or rigorous) of explaining when and why $y$ is a differentiable function of $x$ in this case, using only methods available to a first year calculus student? Yes, it is possible to look through the proof of the implicit function theorem and simplify it in this case, but I am especially interested in hearing the thoughts of experienced teachers, so I hope I am justified in asking the question.",,"['calculus', 'education']"
34,Indefinite integral of $1/(1+x^2)$,Indefinite integral of,1/(1+x^2),"I was studying calculus 1 other day, in the subject of integral of partial fractions, when I perceived that the well know indefinite integral: $$ \int \dfrac{dx}{1+x^2} $$ could be done another way, just by factoring it in $\mathbb{C}[x]$. So I did $1+x^2=(x+i)(x-i)$. Hence  $$ \dfrac{1}{1+x^2}=\dfrac{A(x-i)+B(x+i)}{1+x^2} $$ for some A, B constants. It is easy to see that $A=i/2$ and $B=-i/2$.  Then $$ \int \dfrac{dx}{1+x^2}=\int \left(\dfrac{i/2}{x+i}+\dfrac{-i/2}{x-i} \right) dx = \dfrac{i}{2}\ln{|x+i|}-\dfrac{i}{2}\ln{|x-i|}+c=$$ $$=\dfrac{i}{2}\ln{\left|\dfrac{x+i}{x-i}\right|}+c$$ Finally, my question is: Am I allowed to factorize a polynomial in $\mathbb{C}[x]$ to integrate it, and if I am, what is the relation between the real indefinite integral $\tan^{-1}$ and $\dfrac{i}{2}\ln{\left|\dfrac{x+i}{x-i}\right|}$. Thank you in advance.","I was studying calculus 1 other day, in the subject of integral of partial fractions, when I perceived that the well know indefinite integral: $$ \int \dfrac{dx}{1+x^2} $$ could be done another way, just by factoring it in $\mathbb{C}[x]$. So I did $1+x^2=(x+i)(x-i)$. Hence  $$ \dfrac{1}{1+x^2}=\dfrac{A(x-i)+B(x+i)}{1+x^2} $$ for some A, B constants. It is easy to see that $A=i/2$ and $B=-i/2$.  Then $$ \int \dfrac{dx}{1+x^2}=\int \left(\dfrac{i/2}{x+i}+\dfrac{-i/2}{x-i} \right) dx = \dfrac{i}{2}\ln{|x+i|}-\dfrac{i}{2}\ln{|x-i|}+c=$$ $$=\dfrac{i}{2}\ln{\left|\dfrac{x+i}{x-i}\right|}+c$$ Finally, my question is: Am I allowed to factorize a polynomial in $\mathbb{C}[x]$ to integrate it, and if I am, what is the relation between the real indefinite integral $\tan^{-1}$ and $\dfrac{i}{2}\ln{\left|\dfrac{x+i}{x-i}\right|}$. Thank you in advance.",,"['calculus', 'integration']"
35,Finding maxima and minima of a function,Finding maxima and minima of a function,,"A couple problems are giving me trouble in finding the relative maxima/minima of the function.  I think the problem stems from me possibly not finding all of the critical numbers of the function, but I don't see what I missed. Given $f(x)= 5x + 10 \sin x$, I calculated the derivative as $5 + 10 \cos x$, and found the first critical number by this work: $$5+ 10 \cos x=0$$ $$\frac{5}{5}+10 \cos x= 0-5 \Rightarrow 10 \cos x= -5$$ $$\frac{10 \cos x}{10}= \frac{-5}{10}\Rightarrow \cos x= -\frac{1}{2}$$ $$x= \arccos(-\frac{1}{2}) = \text{First critical number is }\frac{2\pi}{3}$$ That gave me the maxima of the formula, since $$f(\frac{2\pi}{3})= 5(\frac{2\pi}{3})+10 \sin(\frac{2\pi}{3})= \frac{10\pi}{3}+5\sqrt3$$ However, I need the other critical number to calculate the minima.  Should I look for the value of $\arccos(\frac{1}{2})$?","A couple problems are giving me trouble in finding the relative maxima/minima of the function.  I think the problem stems from me possibly not finding all of the critical numbers of the function, but I don't see what I missed. Given $f(x)= 5x + 10 \sin x$, I calculated the derivative as $5 + 10 \cos x$, and found the first critical number by this work: $$5+ 10 \cos x=0$$ $$\frac{5}{5}+10 \cos x= 0-5 \Rightarrow 10 \cos x= -5$$ $$\frac{10 \cos x}{10}= \frac{-5}{10}\Rightarrow \cos x= -\frac{1}{2}$$ $$x= \arccos(-\frac{1}{2}) = \text{First critical number is }\frac{2\pi}{3}$$ That gave me the maxima of the formula, since $$f(\frac{2\pi}{3})= 5(\frac{2\pi}{3})+10 \sin(\frac{2\pi}{3})= \frac{10\pi}{3}+5\sqrt3$$ However, I need the other critical number to calculate the minima.  Should I look for the value of $\arccos(\frac{1}{2})$?",,['calculus']
36,how to evaluate $\int_0^{\infty} \frac{x \ln ^2\left(1-e^{-2 \pi x}\right)}{e^{\frac{\pi x}{2}}+1} d x$,how to evaluate,\int_0^{\infty} \frac{x \ln ^2\left(1-e^{-2 \pi x}\right)}{e^{\frac{\pi x}{2}}+1} d x,Question: how to evaluate $$\int_0^{\infty} \frac{x \ln ^2\left(1-e^{-2 \pi x}\right)}{e^{\frac{\pi x}{2}}+1} d x$$ MY try to evaluate the integral $$ \begin{aligned} & I=\int_0^{\infty} \frac{x \ln ^2\left(1-e^{-2 \pi x}\right)}{e^{\frac{\pi x}{2}}+1} d x \\ & e^{-\frac{\pi x}{2}} \stackrel{\rightharpoonup x}{=}-\frac{4}{\pi^2} \int_0^1 \frac{\ln (x) \ln ^2\left(1-x^4\right)}{1+x} d x \\ & =-\frac{4}{\pi^2} \int_0^1 \frac{(1-x)\left(1+x^2\right) \ln (x) \ln ^2\left(1-x^4\right)}{1-x^4} d x \\ & =-\frac{4}{\pi^2} \int_0^1 \frac{\left(1-x+x^2-x^3\right) \ln (x) \ln ^2\left(1-x^4\right)}{1-x^4} d x \\ & =-\frac{4}{\pi^2}\left(\int_0^1 \frac{\ln (x) \ln ^2\left(1-x^4\right)}{1-x^4} d x-\int_0^1 \frac{x \ln (x) \ln ^2\left(1-x^4\right)}{1-x^4} d x\right. \\ & \left.+\int_0^1 \frac{x^2 \ln (x) \ln ^2\left(1-x^4\right)}{1-x^4} d x-\int_0^1 \frac{x^3 \ln (x) \ln ^2\left(1-x^4\right)}{1-x^4} d x\right) \\ &  \stackrel{x^4\rightharpoonup  x}{=}\frac{1}{4 \pi^2}(\underbrace{\int_0^1 \frac{x^{-\frac{3}{4}} \ln (x) \ln ^2(1-x)}{1-x} d x}_{I_1}-\underbrace{\int_0^1 \frac{x^{-\frac{1}{2}} \ln (x) \ln ^2(1-x)}{1-x} d x}_{I_2} \\ & +\underbrace{\int_0^1 \frac{x^{-\frac{1}{4}} \ln (x) \ln ^2(1-x)}{1-x} d x}_{I_3}-\underbrace{\int_0^1 \frac{\ln (x) \ln ^2(1-x)}{1-x} d x}_{I_4}) \\ & \end{aligned} $$ I don't know how to evaluate last $4$ integrals,Question: how to evaluate MY try to evaluate the integral I don't know how to evaluate last integrals,"\int_0^{\infty} \frac{x \ln ^2\left(1-e^{-2 \pi x}\right)}{e^{\frac{\pi x}{2}}+1} d x 
\begin{aligned}
& I=\int_0^{\infty} \frac{x \ln ^2\left(1-e^{-2 \pi x}\right)}{e^{\frac{\pi x}{2}}+1} d x \\
& e^{-\frac{\pi x}{2}} \stackrel{\rightharpoonup x}{=}-\frac{4}{\pi^2} \int_0^1 \frac{\ln (x) \ln ^2\left(1-x^4\right)}{1+x} d x \\
& =-\frac{4}{\pi^2} \int_0^1 \frac{(1-x)\left(1+x^2\right) \ln (x) \ln ^2\left(1-x^4\right)}{1-x^4} d x \\
& =-\frac{4}{\pi^2} \int_0^1 \frac{\left(1-x+x^2-x^3\right) \ln (x) \ln ^2\left(1-x^4\right)}{1-x^4} d x \\
& =-\frac{4}{\pi^2}\left(\int_0^1 \frac{\ln (x) \ln ^2\left(1-x^4\right)}{1-x^4} d x-\int_0^1 \frac{x \ln (x) \ln ^2\left(1-x^4\right)}{1-x^4} d x\right. \\
& \left.+\int_0^1 \frac{x^2 \ln (x) \ln ^2\left(1-x^4\right)}{1-x^4} d x-\int_0^1 \frac{x^3 \ln (x) \ln ^2\left(1-x^4\right)}{1-x^4} d x\right) \\
&  \stackrel{x^4\rightharpoonup  x}{=}\frac{1}{4 \pi^2}(\underbrace{\int_0^1 \frac{x^{-\frac{3}{4}} \ln (x) \ln ^2(1-x)}{1-x} d x}_{I_1}-\underbrace{\int_0^1 \frac{x^{-\frac{1}{2}} \ln (x) \ln ^2(1-x)}{1-x} d x}_{I_2} \\
& +\underbrace{\int_0^1 \frac{x^{-\frac{1}{4}} \ln (x) \ln ^2(1-x)}{1-x} d x}_{I_3}-\underbrace{\int_0^1 \frac{\ln (x) \ln ^2(1-x)}{1-x} d x}_{I_4}) \\
&
\end{aligned}
 4","['calculus', 'integration']"
37,"How to evaluate $\int_0^1 \dfrac{\operatorname{Li}_2\left(\frac{x}{4}\right)}{4-x}\,\log\left(\dfrac{1+\sqrt{1-x}}{1-\sqrt{1-x}}\right)\,dx$",How to evaluate,"\int_0^1 \dfrac{\operatorname{Li}_2\left(\frac{x}{4}\right)}{4-x}\,\log\left(\dfrac{1+\sqrt{1-x}}{1-\sqrt{1-x}}\right)\,dx","crossposted: https://mathoverflow.net/q/464839 How to evaluate $$\int_0^1 \dfrac{\operatorname{Li}_2\left(\frac{x}{4}\right)}{4-x}\,\log\left(\dfrac{1+\sqrt{1-x}}{1-\sqrt{1-x}}\right)\,dx=\dfrac{\pi^4}{1944}.$$ I am going to have to look back through my papers to find how it was evaluated. It's been a while and I forget without doing it all over again. Unless, someone wants to jump on it before I get back...please feel free. I used a sub ( $t=x/4$ ), parts, and the identity $\sum_{n=1}^{\infty}H_{n}^{(2)}x^{n}=\frac{Li_{2}(x)}{1-x}$ Maybe break it up using: $$H_{n+1}^{(2)}-\frac{1}{(n+1)^{2}}=H_{n}^{(2)}$$ and/or $$\sum_{n=1}^{\infty}\frac{x^{2n}}{(n+1)(2n+1)\binom{2n}{n}}=\frac{4(\sin^{-1}(\frac{x}{2}))^{2}}{x^{2}}$$ I get it now. This is related to the identity: $$ \left( \sin^{-1}(z)\right)^4=\frac{3}{2}\sum_{k=1}^\infty\frac{H_{k-1}^{(2)}(2z)^{2k}}{k^2 \binom{2k}{k}} \quad |z|<1 $$","crossposted: https://mathoverflow.net/q/464839 How to evaluate I am going to have to look back through my papers to find how it was evaluated. It's been a while and I forget without doing it all over again. Unless, someone wants to jump on it before I get back...please feel free. I used a sub ( ), parts, and the identity Maybe break it up using: and/or I get it now. This is related to the identity:","\int_0^1 \dfrac{\operatorname{Li}_2\left(\frac{x}{4}\right)}{4-x}\,\log\left(\dfrac{1+\sqrt{1-x}}{1-\sqrt{1-x}}\right)\,dx=\dfrac{\pi^4}{1944}. t=x/4 \sum_{n=1}^{\infty}H_{n}^{(2)}x^{n}=\frac{Li_{2}(x)}{1-x} H_{n+1}^{(2)}-\frac{1}{(n+1)^{2}}=H_{n}^{(2)} \sum_{n=1}^{\infty}\frac{x^{2n}}{(n+1)(2n+1)\binom{2n}{n}}=\frac{4(\sin^{-1}(\frac{x}{2}))^{2}}{x^{2}} 
\left( \sin^{-1}(z)\right)^4=\frac{3}{2}\sum_{k=1}^\infty\frac{H_{k-1}^{(2)}(2z)^{2k}}{k^2 \binom{2k}{k}} \quad |z|<1
","['calculus', 'integration', 'definite-integrals', 'closed-form']"
38,Convergence of series where $\alpha$ gets close to 1 periodically?,Convergence of series where  gets close to 1 periodically?,\alpha,Its known that $$ \sum_{n=1}^{\infty}\frac{1}{n^\alpha}$$ converges with $\alpha > 1$ . But how does something like $$\sum_{k=1}^\infty\frac{1}{k^{2-sin(k)}}$$ behave that gets close to 1 periodically? The usual tests for convergence failed. Interestlingly I tried this logarithmic convergence criteria I found on german wikipedia. I states that if the series $$b_k=\frac{ln(k^{sin(k)-1})}{ln(ln(k))}$$ converges to something stricly smaller that $-1$ the series converges. Now i pluged this into wolframalpha and it outputs $-\infty$ . But looking at the step-by-step soltion it seems to make a mistake. It applies the product rule for limits wrong IMO.,Its known that converges with . But how does something like behave that gets close to 1 periodically? The usual tests for convergence failed. Interestlingly I tried this logarithmic convergence criteria I found on german wikipedia. I states that if the series converges to something stricly smaller that the series converges. Now i pluged this into wolframalpha and it outputs . But looking at the step-by-step soltion it seems to make a mistake. It applies the product rule for limits wrong IMO., \sum_{n=1}^{\infty}\frac{1}{n^\alpha} \alpha > 1 \sum_{k=1}^\infty\frac{1}{k^{2-sin(k)}} b_k=\frac{ln(k^{sin(k)-1})}{ln(ln(k))} -1 -\infty,"['calculus', 'sequences-and-series', 'analysis']"
39,Comprehension problem regarding exercise in chapter on Fundamental Theorem of Calculus,Comprehension problem regarding exercise in chapter on Fundamental Theorem of Calculus,,"I am retired and working my way through a Stewart Calculus book for personal interest.  I am having problems understanding part of an exercise at the end of the chapter on the Fundamental Theorem of Calculus.  This is basic, introductory material so please bear with me. The last exercise in this chapter has 4 parts (a, b, c and d).  I am having problems with part b.  I am hoping a reader here will help me understand what exactly is being requested as the solution.  The exercise states the following: A high-tech company purchases a new computing system whose initial value is $V$ .  The system will depreciate at the rate $f=f(t)$ and will accumulate maintenance costs at the rate of $g=g(t)$ , where $t$ is the time measured in months.  The company wants to determine the optimal time to replace the system. Part b) Suppose that $$ f(t)=  \begin{cases} \frac{V}{15}-\frac{V}{450}t, & if \ 0 \lt t \le30 \\[2ex] 0, & if \ t>30 \end{cases} $$ and $$ g(t)=\frac{Vt^2}{12,900},\ t>0 $$ Determine the length of time $T$ for the total depreciation $D(t)= \int_{0}^{t}f(s)ds$ to equal the initial value $V$ . Here are my questions: This part asks us to determine the total depreciation time $D(t)$ .  I understand this is exclusive of the maintenance rate.  In other words, the depreciation rate $f(t)$ is factored into the solution BUT the maintenance rate $g(t)$ is not (i.e. I can ignore the maintenance rate).  Is this correct?  If this is not correct then can you please explain why? Again, we are asked to determine the total depreciation time.  The formula for $f(t)$ shows the time interval of depreciation is $(0,30]$ .  I understand this to mean the computer system has fully depreciated when $t=30$ . So the total depreciation $D(t)=V$ is when $t=30$ . Is this correct? If so then this seems to me to the answer to part $b$ .  This however seems too simplistic an answer and therefore believe I have failed to comprehend exactly what is being asked, and what is required as a solution to part b.  Can someone help me understand where/what the mistake is that I am making? Thank you, Ian","I am retired and working my way through a Stewart Calculus book for personal interest.  I am having problems understanding part of an exercise at the end of the chapter on the Fundamental Theorem of Calculus.  This is basic, introductory material so please bear with me. The last exercise in this chapter has 4 parts (a, b, c and d).  I am having problems with part b.  I am hoping a reader here will help me understand what exactly is being requested as the solution.  The exercise states the following: A high-tech company purchases a new computing system whose initial value is .  The system will depreciate at the rate and will accumulate maintenance costs at the rate of , where is the time measured in months.  The company wants to determine the optimal time to replace the system. Part b) Suppose that and Determine the length of time for the total depreciation to equal the initial value . Here are my questions: This part asks us to determine the total depreciation time .  I understand this is exclusive of the maintenance rate.  In other words, the depreciation rate is factored into the solution BUT the maintenance rate is not (i.e. I can ignore the maintenance rate).  Is this correct?  If this is not correct then can you please explain why? Again, we are asked to determine the total depreciation time.  The formula for shows the time interval of depreciation is .  I understand this to mean the computer system has fully depreciated when . So the total depreciation is when . Is this correct? If so then this seems to me to the answer to part .  This however seems too simplistic an answer and therefore believe I have failed to comprehend exactly what is being asked, and what is required as a solution to part b.  Can someone help me understand where/what the mistake is that I am making? Thank you, Ian","V f=f(t) g=g(t) t 
f(t)= 
\begin{cases}
\frac{V}{15}-\frac{V}{450}t, & if \ 0 \lt t \le30 \\[2ex]
0, & if \ t>30
\end{cases}
 
g(t)=\frac{Vt^2}{12,900},\ t>0
 T D(t)= \int_{0}^{t}f(s)ds V D(t) f(t) g(t) f(t) (0,30] t=30 D(t)=V t=30 b","['calculus', 'integration']"
40,A person's quickest path between any two points on perimeter of elliptical lake never involves both swimming and running. Find maximum eccentricity.,A person's quickest path between any two points on perimeter of elliptical lake never involves both swimming and running. Find maximum eccentricity.,,"I made up this question. A police officer's job is to patrol the perimeter of an elliptical lake. They have a constant (unknown) swimming speed and a constant (unknown) running speed. Their quickest path between any two points on the perimeter never involves both swimming and running. What is the maximum value of $e$ , the eccentricity of the ellipse? Proof that $e$ has an upper bound If $e\approx 1$ then the lake looks almost like a straight river. Consider a point on one side, and another point on the other side further ""downstream"". Clearly, the quickest path between the points could involve swimming and running. We can prove that if $e=0$ (circle) then the quickest path never involves swimming and running. Suppose $e=0$ and the quickest path between two points involves swimming and running, i.e. the path includes a chord and arc that share an endpoint. Let $\alpha=$ angle subtended by the chord ( $0<\alpha<\pi$ ), and $\theta=$ angle subtended by the chord-arc pair. Assuming the radius of the circle is $1$ , the time for this path is $T=\dfrac{2\sin (\alpha/2)}{v_{\text{swim}}}+\dfrac{\theta-\alpha}{v_{\text{run}}}$ $\dfrac{d^2 T}{d\alpha^2}=-\dfrac{\sin (\alpha/2)}{2v_{\text{swim}}}<0$ So the minimum value of $T$ occurs at one of the endpoints, i.e. $\alpha=0$ or $\alpha=\theta$ , which implies that the path involves either only running or only swimming, contradiction. Therefore if $e=0$ then the quickest path never involves swimming and running. So $e$ must have an upper bound.","I made up this question. A police officer's job is to patrol the perimeter of an elliptical lake. They have a constant (unknown) swimming speed and a constant (unknown) running speed. Their quickest path between any two points on the perimeter never involves both swimming and running. What is the maximum value of , the eccentricity of the ellipse? Proof that has an upper bound If then the lake looks almost like a straight river. Consider a point on one side, and another point on the other side further ""downstream"". Clearly, the quickest path between the points could involve swimming and running. We can prove that if (circle) then the quickest path never involves swimming and running. Suppose and the quickest path between two points involves swimming and running, i.e. the path includes a chord and arc that share an endpoint. Let angle subtended by the chord ( ), and angle subtended by the chord-arc pair. Assuming the radius of the circle is , the time for this path is So the minimum value of occurs at one of the endpoints, i.e. or , which implies that the path involves either only running or only swimming, contradiction. Therefore if then the quickest path never involves swimming and running. So must have an upper bound.",e e e\approx 1 e=0 e=0 \alpha= 0<\alpha<\pi \theta= 1 T=\dfrac{2\sin (\alpha/2)}{v_{\text{swim}}}+\dfrac{\theta-\alpha}{v_{\text{run}}} \dfrac{d^2 T}{d\alpha^2}=-\dfrac{\sin (\alpha/2)}{2v_{\text{swim}}}<0 T \alpha=0 \alpha=\theta e=0 e,"['calculus', 'geometry', 'optimization', 'conic-sections', 'arc-length']"
41,Summation $\sum_{n=0}^{\infty}\left(\frac{x^{2n}\left(n+1\right)^{x}\ }{n!}\right)$,Summation,\sum_{n=0}^{\infty}\left(\frac{x^{2n}\left(n+1\right)^{x}\ }{n!}\right),"$$f\left(x\right)=\sum_{n=0}^{\infty}\left(\frac{x^{2n}\left(n+1\right)^{x}\ }{n!}\right)$$ This is a self made problem I came across while researching something, so I do not have high hopes for it having a closed form. First of all, Wolfram Does Not give a Closed Form for the Function. But it does give the function value for values of $x$ as follows: $$f(-2)=\frac{\text{Ei}(4)-\gamma-\ln4}{4}$$ $$f(-1)=e-1$$ $$f(0)=1$$ $$f(1)=2e$$ $$f(2)=29e^4$$ $$f(3)=1279e^9$$ $$f(4)=113137e^{16}$$ Now, for $f(n)$ , $n\in \mathrm N$ : $$f(n)=a_ne^{n^2}$$ The Sequence "" $2, 29, 1279, 113137$ "" led to no results on OEIS. EDIT: Using Binomial Expansion: $$f(x)=\sum_{r=0}^{x}\frac{x!}{\left(x-r\right)!r!}\sum_{n=0}^{\infty}\frac{x^{2n}n^{r}}{n!}$$ Now you could say the Problem Comes Down to Expanding $\frac{n^r}{n!}$ : Example: $$\frac{n^4}{n!}=\frac{1}{\left(n-4\right)!}+\frac{6}{\left(n-3\right)!}+\frac{7}{\left(n-2\right)!}+\frac{1}{\left(n-1\right)!}$$ But then, although I can do this by hand, how do I write it as a function of $r$ . Second Edit: Using @SangchulLee's help, I was able to deduce this form for natural numbers: $$f(n)=\left(\sum_{r=0}^{n}\binom{n}{r}\text{T}_r\left(n^2\right)\right)e^{n^{2}}$$ where, $\text{T}_r(x)$ are Touchard Polynomials . Then after browsing through I found a wonderful property: $$\frac{T_{n+1}\left(x\right)}{x}=\sum_{k=0}^{n}\binom{n}{k}T_{k}\left(x\right)$$ This leads to the closed form in terms of Touchard Polynomial: $$f(n)=\frac{\text{T}_{n+1}\left(n^2\right)}{n^{2}}e^{n^2}$$ $$\color{green}{f(x)=\frac{\left(x+1\right)!}{\pi x^{2}}\int_{0}^{\pi}\left(e^{x^{2}\left(e^{\cos\left(t\right)}\cos\left(\sin\left(t\right)\right)\right)}\cos\left(x^{2}e^{\cos\left(t\right)}\sin\left(\sin t\right)-\left(x+1\right)t\right)\right)dt}$$ Well this seems to be the closed form. This needs verification though.","This is a self made problem I came across while researching something, so I do not have high hopes for it having a closed form. First of all, Wolfram Does Not give a Closed Form for the Function. But it does give the function value for values of as follows: Now, for , : The Sequence "" "" led to no results on OEIS. EDIT: Using Binomial Expansion: Now you could say the Problem Comes Down to Expanding : Example: But then, although I can do this by hand, how do I write it as a function of . Second Edit: Using @SangchulLee's help, I was able to deduce this form for natural numbers: where, are Touchard Polynomials . Then after browsing through I found a wonderful property: This leads to the closed form in terms of Touchard Polynomial: Well this seems to be the closed form. This needs verification though.","f\left(x\right)=\sum_{n=0}^{\infty}\left(\frac{x^{2n}\left(n+1\right)^{x}\ }{n!}\right) x f(-2)=\frac{\text{Ei}(4)-\gamma-\ln4}{4} f(-1)=e-1 f(0)=1 f(1)=2e f(2)=29e^4 f(3)=1279e^9 f(4)=113137e^{16} f(n) n\in \mathrm N f(n)=a_ne^{n^2} 2, 29, 1279, 113137 f(x)=\sum_{r=0}^{x}\frac{x!}{\left(x-r\right)!r!}\sum_{n=0}^{\infty}\frac{x^{2n}n^{r}}{n!} \frac{n^r}{n!} \frac{n^4}{n!}=\frac{1}{\left(n-4\right)!}+\frac{6}{\left(n-3\right)!}+\frac{7}{\left(n-2\right)!}+\frac{1}{\left(n-1\right)!} r f(n)=\left(\sum_{r=0}^{n}\binom{n}{r}\text{T}_r\left(n^2\right)\right)e^{n^{2}} \text{T}_r(x) \frac{T_{n+1}\left(x\right)}{x}=\sum_{k=0}^{n}\binom{n}{k}T_{k}\left(x\right) f(n)=\frac{\text{T}_{n+1}\left(n^2\right)}{n^{2}}e^{n^2} \color{green}{f(x)=\frac{\left(x+1\right)!}{\pi x^{2}}\int_{0}^{\pi}\left(e^{x^{2}\left(e^{\cos\left(t\right)}\cos\left(\sin\left(t\right)\right)\right)}\cos\left(x^{2}e^{\cos\left(t\right)}\sin\left(\sin t\right)-\left(x+1\right)t\right)\right)dt}","['calculus', 'summation']"
42,Strange Absurdities in a Calculus Problem!,Strange Absurdities in a Calculus Problem!,,"If $f$ is a differentiable function in $[0,1]$ such that $f(f(x))=x$ and $f(0)=1$ . Find the value of $\int_{0}^{1} (x-f(x))^{2016} dx$ . This is a very popular problem(maybe). I used two attempts to solve the problem. It appears my Attempt $1$ didn't quite work, as I intended. My Attempt $2$ surprisingly worked. I will demonstrate both the attempts. Attempt 1: We first consider,if $f(x)=y$ then, $f(f(x))=x=f(y).$ We consider, the integral $I=\int (x-f(x))^{2016} dx$ . We try changing, all the independent variable of integration, from $x$ to $y$ ( or, in other words we are trying to apply method of substitution i.e substituting, $y=f(x)$ in $I$ ). For that, we have to replace $dx$ as well. We notice that, $$f(y)=x\implies \frac{df(y)}{dy}\frac{dy}{dx}=\frac{d}{dx}x\implies f'(y)\frac{dy}{dx}=1\implies f'(y)dy=dx.$$ We are now in a place to completely do the change of variables in $I$ . We have, $$I=\int (x-f(x))^{2016} dx=\int (f(y)-y)^{2016}f'(y)dy$$ . We can again, change the variable representation in $I$ to $x$ as, $$I=\int(f(x)-x)^{2016}f'(x)dx\tag {1}.$$ Now, we again observe $\int (x-f(x))^{2016} dx$ and adding it with $( 1),$ we have $$2I=\int\left( (x-f(x))^{2016} +(f(x)-x)^{2016}f'(x)\right)dx.$$ This can be readily simplified to, $$2I=\int\left( (x-f(x))^{2016}(1 +f'(x))\right)dx.$$ We now, have, $$I=\frac{\int\left( (x-f(x))^{2016}(1 +f'(x)\right)dx}{2}.$$ We hereby, put the upper limit and lower limit as $1$ and $0$ respectively. This means, $$I=\frac{\int_0^1\left( (x-f(x))^{2016}(1 +f'(x)\right)dx}{2}.$$ Here comes a mystery. I was stuck at this particular step. Attempt 2: (In this method, I proceeded to work with $I=\int_0^1(x-f(x))^{2016}dx$ instead of, $I=\int(x-f(x))^{2016}dx$ ) We first consider,if $f(x)=y$ then, $f(f(x))=x=f(y).$ We consider, the integral $I=\int (x-f(x))^{2016} dx$ . We try changing, all the independent variable of integration, from $x$ to $y$ ( or, in other words we are trying to apply method of substitution i.e substituting, $y=f(x)$ in $I$ ). For that, we have to replace $dx$ as well. We notice that, $$f(y)=x\implies \frac{df(y)}{dy}\frac{dy}{dx}=\frac{d}{dx}x\implies f'(y)\frac{dy}{dx}=1\implies f'(y)dy=dx.$$ We are now in a place to completely do the change of variables in $I$ . We have, $$I=\int_0^1 (x-f(x))^{2016} dx=\int_1^0 (f(y)-y)^{2016}f'(y)dy$$ . (We have, $y=f(x)$ and $x=0\implies y=f(0)=1$ and $x=1\implies y=f(1)=f(f(0))=0$ and since the variable of integration is $y$ here, so the corresponding upper limit and lower limit in terms $x$ (initially) $(1,0)$ will change to $(0,1)$ due to change in variable to $y$ ) We can again, change the variable representation in $I$ to $x$ as, $$I=\int_1^0(f(x)-x)^{2016}f'(x)dx$$ $$\implies I=-\int_0^1(f(x)-x)^{2016}f'(x)dx \tag 1.$$ Now, we again observe $\int_0^1 (x-f(x))^{2016} dx$ and adding it with $( 1),$ we have $$2I=\int_0^1\left( (x-f(x))^{2016} -(f(x)-x)^{2016}f'(x)\right)dx.$$ This can be readily simplified to, $$2I=\int_0^1\left( (x-f(x))^{2016}(1 -f'(x))\right)dx.$$ We now, have, $$I=\frac{\int_0^1(x-f(x))^{2016}(1 -f'(x))dx}{2}.$$ We notice, $$I=\frac{\int_0^1 (x-f(x))^{2016}(1 -f'(x))dx}{2},$$ can again be readily simplified by substituting $t=x-f(x)$ and so, $$\frac{dt}{dx}=(1-f'(x))\implies \frac{dt}{(1-f'(x))}=dx.$$ Hence, $$I=\frac{\int_0^1 (x-f(x))^{2016}(1 -f'(x))dx}{2}=$$ $$\begin{align}\frac{\int_{-1}^1 (x-f(x))^{2016}(1 -f'(x))\frac{dt}{(1-f'(x))}}{2}\end{align}$$ $$=\frac 1{2}\int_{-1}^1t^{2016}dt=\frac{1}{2.2017}[t^{2017}]^1_{-1}=\frac{1}{2017}.$$ So, I could solve, the problem in my 2nd Attempt prestty much easily. I analyzed both of the attempt $1$ and $2.$ I found that the problem in Attempt $1$ started because in the last step of Attempt $1$ we hit at the expression $$I=\frac{\int_0^1\left( (x-f(x))^{2016}(1 +f'(x)\right)dx}{2}.$$ In attempt $2$ , inspite of hitting at $$I=\frac{\int_0^1\left( (x-f(x))^{2016}(1 +f'(x)\right)dx}{2},$$ we hit at $$I=\frac{\int_0^1(x-f(x))^{2016}(1 -f'(x))dx}{2}.$$ This expression $$I=\frac{\int_0^1(x-f(x))^{2016}(1 -f'(x))dx}{2},$$ was easy to manipulate further, because, we had $(1-f'(x))$ here, instead of the ""troublesome"" $(1+f'(x))$ there. But this is strange, why do I get $$I=\frac{\int_0^1(x-f(x))^{2016}(1 -f'(x))dx}{2},$$ in attempt $2$ , instead of $$I=\frac{\int_0^1\left( (x-f(x))^{2016}(1 +f'(x)\right)dx}{2},$$ as in Attempt $1$ ? If one notice, then it might be observed that, there is no difference in the procedure applied in both of the attempts, only thing that was not alike, is that in attempt one we ignored the limits (upper and lower limit) and at the final step we put them to convert $I$ into a definite integral. On the other, hand, in attempt $2$ we exactly imitated what we did in attempt $1$ just that, here, at each step we were writing the upper and lower limits. My question, is, why this mysterious thing is occuring ? Both of the simplication in the two attempts should have been same. Where does the difference occur , in either of the two cases ? Another (Unexplained) Absurdity : If in Attempt $2$ we took the substitution $t=(x-f(x))^{2016}$ then things would have been different and we would have got, $I=0.$ The corresponding calculation, for $t=(x-f(x))^{2016}$ : $$I=\frac{\int_0^1 (x-f(x))^{2016}(1 -f'(x))dx}{2}=$$ $$\begin{align}\frac{\int_1^1 (x-f(x))^{2016}(1 -f'(x))\frac{dt}{2016(x-f(x))^{2015}(1-f'(x))}}{2}\end{align}$$ $$=0.$$ Precisely, I am looking for a convincing explanation of these apparent absurdities, using elementary real analysis (basic) elementary calculus(/basic real analysis).","If is a differentiable function in such that and . Find the value of . This is a very popular problem(maybe). I used two attempts to solve the problem. It appears my Attempt didn't quite work, as I intended. My Attempt surprisingly worked. I will demonstrate both the attempts. Attempt 1: We first consider,if then, We consider, the integral . We try changing, all the independent variable of integration, from to ( or, in other words we are trying to apply method of substitution i.e substituting, in ). For that, we have to replace as well. We notice that, We are now in a place to completely do the change of variables in . We have, . We can again, change the variable representation in to as, Now, we again observe and adding it with we have This can be readily simplified to, We now, have, We hereby, put the upper limit and lower limit as and respectively. This means, Here comes a mystery. I was stuck at this particular step. Attempt 2: (In this method, I proceeded to work with instead of, ) We first consider,if then, We consider, the integral . We try changing, all the independent variable of integration, from to ( or, in other words we are trying to apply method of substitution i.e substituting, in ). For that, we have to replace as well. We notice that, We are now in a place to completely do the change of variables in . We have, . (We have, and and and since the variable of integration is here, so the corresponding upper limit and lower limit in terms (initially) will change to due to change in variable to ) We can again, change the variable representation in to as, Now, we again observe and adding it with we have This can be readily simplified to, We now, have, We notice, can again be readily simplified by substituting and so, Hence, So, I could solve, the problem in my 2nd Attempt prestty much easily. I analyzed both of the attempt and I found that the problem in Attempt started because in the last step of Attempt we hit at the expression In attempt , inspite of hitting at we hit at This expression was easy to manipulate further, because, we had here, instead of the ""troublesome"" there. But this is strange, why do I get in attempt , instead of as in Attempt ? If one notice, then it might be observed that, there is no difference in the procedure applied in both of the attempts, only thing that was not alike, is that in attempt one we ignored the limits (upper and lower limit) and at the final step we put them to convert into a definite integral. On the other, hand, in attempt we exactly imitated what we did in attempt just that, here, at each step we were writing the upper and lower limits. My question, is, why this mysterious thing is occuring ? Both of the simplication in the two attempts should have been same. Where does the difference occur , in either of the two cases ? Another (Unexplained) Absurdity : If in Attempt we took the substitution then things would have been different and we would have got, The corresponding calculation, for : Precisely, I am looking for a convincing explanation of these apparent absurdities, using elementary real analysis (basic) elementary calculus(/basic real analysis).","f [0,1] f(f(x))=x f(0)=1 \int_{0}^{1} (x-f(x))^{2016} dx 1 2 f(x)=y f(f(x))=x=f(y). I=\int (x-f(x))^{2016} dx x y y=f(x) I dx f(y)=x\implies \frac{df(y)}{dy}\frac{dy}{dx}=\frac{d}{dx}x\implies f'(y)\frac{dy}{dx}=1\implies f'(y)dy=dx. I I=\int (x-f(x))^{2016} dx=\int (f(y)-y)^{2016}f'(y)dy I x I=\int(f(x)-x)^{2016}f'(x)dx\tag {1}. \int (x-f(x))^{2016} dx ( 1), 2I=\int\left( (x-f(x))^{2016} +(f(x)-x)^{2016}f'(x)\right)dx. 2I=\int\left( (x-f(x))^{2016}(1 +f'(x))\right)dx. I=\frac{\int\left( (x-f(x))^{2016}(1 +f'(x)\right)dx}{2}. 1 0 I=\frac{\int_0^1\left( (x-f(x))^{2016}(1 +f'(x)\right)dx}{2}. I=\int_0^1(x-f(x))^{2016}dx I=\int(x-f(x))^{2016}dx f(x)=y f(f(x))=x=f(y). I=\int (x-f(x))^{2016} dx x y y=f(x) I dx f(y)=x\implies \frac{df(y)}{dy}\frac{dy}{dx}=\frac{d}{dx}x\implies f'(y)\frac{dy}{dx}=1\implies f'(y)dy=dx. I I=\int_0^1 (x-f(x))^{2016} dx=\int_1^0 (f(y)-y)^{2016}f'(y)dy y=f(x) x=0\implies y=f(0)=1 x=1\implies y=f(1)=f(f(0))=0 y x (1,0) (0,1) y I x I=\int_1^0(f(x)-x)^{2016}f'(x)dx \implies I=-\int_0^1(f(x)-x)^{2016}f'(x)dx \tag 1. \int_0^1 (x-f(x))^{2016} dx ( 1), 2I=\int_0^1\left( (x-f(x))^{2016} -(f(x)-x)^{2016}f'(x)\right)dx. 2I=\int_0^1\left( (x-f(x))^{2016}(1 -f'(x))\right)dx. I=\frac{\int_0^1(x-f(x))^{2016}(1 -f'(x))dx}{2}. I=\frac{\int_0^1 (x-f(x))^{2016}(1 -f'(x))dx}{2}, t=x-f(x) \frac{dt}{dx}=(1-f'(x))\implies \frac{dt}{(1-f'(x))}=dx. I=\frac{\int_0^1 (x-f(x))^{2016}(1 -f'(x))dx}{2}= \begin{align}\frac{\int_{-1}^1 (x-f(x))^{2016}(1 -f'(x))\frac{dt}{(1-f'(x))}}{2}\end{align} =\frac 1{2}\int_{-1}^1t^{2016}dt=\frac{1}{2.2017}[t^{2017}]^1_{-1}=\frac{1}{2017}. 1 2. 1 1 I=\frac{\int_0^1\left( (x-f(x))^{2016}(1 +f'(x)\right)dx}{2}. 2 I=\frac{\int_0^1\left( (x-f(x))^{2016}(1 +f'(x)\right)dx}{2}, I=\frac{\int_0^1(x-f(x))^{2016}(1 -f'(x))dx}{2}. I=\frac{\int_0^1(x-f(x))^{2016}(1 -f'(x))dx}{2}, (1-f'(x)) (1+f'(x)) I=\frac{\int_0^1(x-f(x))^{2016}(1 -f'(x))dx}{2}, 2 I=\frac{\int_0^1\left( (x-f(x))^{2016}(1 +f'(x)\right)dx}{2}, 1 I 2 1 2 t=(x-f(x))^{2016} I=0. t=(x-f(x))^{2016} I=\frac{\int_0^1 (x-f(x))^{2016}(1 -f'(x))dx}{2}= \begin{align}\frac{\int_1^1 (x-f(x))^{2016}(1 -f'(x))\frac{dt}{2016(x-f(x))^{2015}(1-f'(x))}}{2}\end{align} =0.","['calculus', 'integration', 'definite-integrals']"
43,Evaluate $\int_{0}^{1} \frac{K(k)E(k)^2-\frac{\pi^3}{8} }{k} \text{d}k$ and $\int_{0}^{1} \frac{E(k)^3-\frac{\pi^3}{8} }{k} \text{d}k$,Evaluate  and,\int_{0}^{1} \frac{K(k)E(k)^2-\frac{\pi^3}{8} }{k} \text{d}k \int_{0}^{1} \frac{E(k)^3-\frac{\pi^3}{8} }{k} \text{d}k,"Let $K(k),E(k)$ be the complete elliptic integral of the first kind and second kind respectively, where $k$ is the elliptic modulus. Consider four integrals, $$\begin{aligned} &I_1=\int_{0}^{1} \frac{K(k)^3-\frac{\pi^3}{8} }{k} \text{d}k,\\ &I_2=\int_{0}^{1} \frac{K(k)^2E(k)-\frac{\pi^3}{8} }{k} \text{d}k,\\ &I_3=\int_{0}^{1} \frac{K(k)E(k)^2-\frac{\pi^3}{8} }{k} \text{d}k,\\ &I_4=\int_{0}^{1} \frac{E(k)^3-\frac{\pi^3}{8} }{k} \text{d}k. \end{aligned}$$ $I_1$ is computed to be $$ \int_{0}^{1} \frac{K(k)^3-\frac{\pi^3}{8} }{k} \text{d}k=\frac{\Gamma\left ( \frac{1}{4}  \right )^8}{3200\pi^2}  -\frac{12}{5}\beta(4)+\frac{\pi^3}{4}\ln(2), $$ where we utilize $$ \int_{0}^{1}\left ( \frac{K^\prime}{K}  \right )^{s-1} \left[ \frac{K(k)\left ( K(k)^2-\frac{\pi^2}{4}  \right ) }{k}  -\frac{k}{5}K(k)^3  \right]\text{d}k =\frac{\pi^{4-s}}{20}\Gamma(s)\zeta(s)\left [ \beta(s-4)+5\beta(s-2) \right ]. $$ By differentiatng $(1-k^2)K(k)^3$ , we have $$ \int_{0}^{1} \left ( kK(k)^3-\frac{3K(k)^2\left ( K(k)-E(k) \right ) }{k}  \right ) \text{d} k=\left [ (1-k^2)K(k)^3 \right ] \Big|^{1}_0=-\frac{\pi^3}{8}. $$ Hence the evaluation, $I_2$ is $$ \int_{0}^{1} \frac{K(k)^2E(k)-\frac{\pi^3}{8} }{k} \text{d}k=-\frac{\Gamma\left ( \frac{1}{4}  \right )^8}{4800\pi^2}  -\frac{12}{5}\beta(4)+\frac{\pi^3}{4}\ln(2)-\frac{\pi^3}{24}. $$ While I have met the tedious part, differentiating $k^2(1-k^2)K(k)^3,E(k)^3,(1-k^2)K(k)^2E(k)$ and $(1-k^2)K(k)E(k)$ . They generate the following relations: $$\begin{aligned} &(1)3kK^2E-k^3K^3\equiv0,\\ &(2)E^3/k-KE^2/k\equiv0,\\ &(3)E^3/k+KE^2/k+2kK^2E-3kKE^2\equiv0,\\ &(4)2KE^2/k-kK^2E\equiv0, \end{aligned}$$ in which we say $f(k)$ is ""equal"" to $0$ if the regularized value of $\int_{0}^{1}f(k)\text{d}k$ has known explicit closed-forms. These four equations have five unknowns and it seems that the fifth can be constructed normally, though, I was stuck here. Question. Whether we can find the fifth relation or the closed-forms of integrals the title comprised?","Let be the complete elliptic integral of the first kind and second kind respectively, where is the elliptic modulus. Consider four integrals, is computed to be where we utilize By differentiatng , we have Hence the evaluation, is While I have met the tedious part, differentiating and . They generate the following relations: in which we say is ""equal"" to if the regularized value of has known explicit closed-forms. These four equations have five unknowns and it seems that the fifth can be constructed normally, though, I was stuck here. Question. Whether we can find the fifth relation or the closed-forms of integrals the title comprised?","K(k),E(k) k \begin{aligned}
&I_1=\int_{0}^{1} \frac{K(k)^3-\frac{\pi^3}{8} }{k}
\text{d}k,\\
&I_2=\int_{0}^{1} \frac{K(k)^2E(k)-\frac{\pi^3}{8} }{k}
\text{d}k,\\
&I_3=\int_{0}^{1} \frac{K(k)E(k)^2-\frac{\pi^3}{8} }{k}
\text{d}k,\\
&I_4=\int_{0}^{1} \frac{E(k)^3-\frac{\pi^3}{8} }{k}
\text{d}k.
\end{aligned} I_1 
\int_{0}^{1} \frac{K(k)^3-\frac{\pi^3}{8} }{k}
\text{d}k=\frac{\Gamma\left ( \frac{1}{4}  \right )^8}{3200\pi^2} 
-\frac{12}{5}\beta(4)+\frac{\pi^3}{4}\ln(2),
 
\int_{0}^{1}\left ( \frac{K^\prime}{K}  \right )^{s-1}
\left[ \frac{K(k)\left ( K(k)^2-\frac{\pi^2}{4}  \right ) }{k} 
-\frac{k}{5}K(k)^3  \right]\text{d}k
=\frac{\pi^{4-s}}{20}\Gamma(s)\zeta(s)\left [ \beta(s-4)+5\beta(s-2) \right ].
 (1-k^2)K(k)^3 
\int_{0}^{1} \left ( kK(k)^3-\frac{3K(k)^2\left ( K(k)-E(k) \right ) }{k}  \right )
\text{d} k=\left [ (1-k^2)K(k)^3 \right ] \Big|^{1}_0=-\frac{\pi^3}{8}.
 I_2 
\int_{0}^{1} \frac{K(k)^2E(k)-\frac{\pi^3}{8} }{k}
\text{d}k=-\frac{\Gamma\left ( \frac{1}{4}  \right )^8}{4800\pi^2} 
-\frac{12}{5}\beta(4)+\frac{\pi^3}{4}\ln(2)-\frac{\pi^3}{24}.
 k^2(1-k^2)K(k)^3,E(k)^3,(1-k^2)K(k)^2E(k) (1-k^2)K(k)E(k) \begin{aligned}
&(1)3kK^2E-k^3K^3\equiv0,\\
&(2)E^3/k-KE^2/k\equiv0,\\
&(3)E^3/k+KE^2/k+2kK^2E-3kKE^2\equiv0,\\
&(4)2KE^2/k-kK^2E\equiv0,
\end{aligned} f(k) 0 \int_{0}^{1}f(k)\text{d}k","['calculus', 'integration', 'definite-integrals', 'closed-form', 'elliptic-integrals']"
44,The taxonomy of real functions of one real variable,The taxonomy of real functions of one real variable,,"In treatments like Spivak's Calculus , and in Pete Clark's notes here , the motivation is to describe a set of real functions of one real variable with ""sufficiently nice properties"" -- that is to say, a taxonomy of such functions. It seems to me that we can think of this taxonomy as analytic $\subset$ infintely differentiable $\subset$ n-times differentiable $\subset$ continuous $\subset$ general functions. I am trying (I think) to understand the jump from infinitely differentiable to analytic. I am at the end of Spivak's Chapter 24 where the statement is made that ""a convergent power series is always the Taylor series of the function which it defines"". That is (WLOG considering Maclaurin rather than Taylor series), if in some interval $J=(-R,R)$ we have that the function $f(x) = \sum_0^\infty a_n x^n$ converges, then it is in fact equal to its Taylor series $$a_n = \frac{f^{(n)}(0)}{n!}.$$ But as noted in Prof. Clark's notes, we can ask the more general question: Question 12.5. Let $f : I  R$ be an infinitely differentiable function and $T(x) = \sum_0^\infty \frac{f^{(n)}(0)}{n!} x^n$ be its Taylor series. We may then ask: a) For which values of $x$ does $T(x)$ converge? b) If for $x \in I$ , $T(x)$ converges, do we have $T(x) = f(x)$ ? I want to ask whether the following are the only possibilities other than $f$ being analytic (I think this means equal to it's Taylor series?) everywhere it's defined: The interval of convergence $J$ of the Taylor series of $f$ is a subset of $I$ (the interval on which $f$ is defined), but $f$ is given by its Taylor series on $J$ . I believe this is given e.g. by $f(x) = \textrm{ln}(1+x)$ . I have not yet studied later chapters in Spivak but I suspect this case arises when the complex version of the function encounters a singularity at that radius. The interval of convergence $J$ of the Taylor series of $f$ equals $I$ (the interval on which $f$ is defined), but $f$ is NOT given by its Taylor series on all of $J$ . I believe this is given e.g. by $f(x) = e^{-1/x^2} \ \textrm{for} \ x \neq 0, f(0) = 0$ . Further, with respect to question 2, I'm wondering if it's possible to comment on ""what goes wrong"". Prof. Clark provides sufficient conditions for analyticity in his Theorem 12.6, but I can't quite understand why something that has the limit of its Taylor remainder converge (so that the limit of Taylor polynomials converges) would equal anything other than the function from which the Taylor polynomials were constructed? I suspect this has something to do with uniform convergence but I'm not sure. Edit: With respect to the question in the last paragraph, I suppose a possibility is that the Taylor polynomials $P_{n,f}$ of $f$ converge to some function which is not $f$ , so that the remainder $R_{n,f}$ converges too (unlike in possibility 1 above, where I think the reason the sequence of Taylor polynomial does not converge is because the remainder blows up), but it does not converge to 0?","In treatments like Spivak's Calculus , and in Pete Clark's notes here , the motivation is to describe a set of real functions of one real variable with ""sufficiently nice properties"" -- that is to say, a taxonomy of such functions. It seems to me that we can think of this taxonomy as analytic infintely differentiable n-times differentiable continuous general functions. I am trying (I think) to understand the jump from infinitely differentiable to analytic. I am at the end of Spivak's Chapter 24 where the statement is made that ""a convergent power series is always the Taylor series of the function which it defines"". That is (WLOG considering Maclaurin rather than Taylor series), if in some interval we have that the function converges, then it is in fact equal to its Taylor series But as noted in Prof. Clark's notes, we can ask the more general question: Question 12.5. Let be an infinitely differentiable function and be its Taylor series. We may then ask: a) For which values of does converge? b) If for , converges, do we have ? I want to ask whether the following are the only possibilities other than being analytic (I think this means equal to it's Taylor series?) everywhere it's defined: The interval of convergence of the Taylor series of is a subset of (the interval on which is defined), but is given by its Taylor series on . I believe this is given e.g. by . I have not yet studied later chapters in Spivak but I suspect this case arises when the complex version of the function encounters a singularity at that radius. The interval of convergence of the Taylor series of equals (the interval on which is defined), but is NOT given by its Taylor series on all of . I believe this is given e.g. by . Further, with respect to question 2, I'm wondering if it's possible to comment on ""what goes wrong"". Prof. Clark provides sufficient conditions for analyticity in his Theorem 12.6, but I can't quite understand why something that has the limit of its Taylor remainder converge (so that the limit of Taylor polynomials converges) would equal anything other than the function from which the Taylor polynomials were constructed? I suspect this has something to do with uniform convergence but I'm not sure. Edit: With respect to the question in the last paragraph, I suppose a possibility is that the Taylor polynomials of converge to some function which is not , so that the remainder converges too (unlike in possibility 1 above, where I think the reason the sequence of Taylor polynomial does not converge is because the remainder blows up), but it does not converge to 0?","\subset \subset \subset \subset J=(-R,R) f(x) = \sum_0^\infty a_n x^n a_n = \frac{f^{(n)}(0)}{n!}. f : I  R T(x) = \sum_0^\infty \frac{f^{(n)}(0)}{n!} x^n x T(x) x \in I T(x) T(x) = f(x) f J f I f f J f(x) = \textrm{ln}(1+x) J f I f f J f(x) = e^{-1/x^2} \ \textrm{for} \ x \neq 0, f(0) = 0 P_{n,f} f f R_{n,f}",['calculus']
45,How would one go about integrating $\int x^2y'dx$ ?? - Implicit Integration??,How would one go about integrating  ?? - Implicit Integration??,\int x^2y'dx,"My friend and I are attempting to do some crazy integral stuff which he came up with but I am still having trouble getting my head around, but here's the general gist of what he is trying to get at: Propose we have a function: $f(x)={\sqrt{x}}$ . This of course is easy enough to integrate in of itself but what we're trying to figure out is if we make it so ${y^2}=x$ then integrate both sides of the function, can we still get the same answer. Here's how he computed it: $y^2=x$ $y^2y'=xy'$ $\int y^2y'dx=\int xy'dx$ The $\int y^2y'dx$ can be solved using u-substitution because recall that y is a function with respect to x rather than a constant. Additionally, the $\int xy' dx$ can be solved using integration by parts where one can arrive at $\int xy'dx=xy-\int y dx$ note how that $\int ydx = Y$ so now we have: $\frac{1}{3}y^3=xy-Y$ Now we solve for $Y$ : $Y=xy-\frac{1}{3}y^3$ $Y=\frac{2}{3}y^3$ This is the correct answer, but it's just in terms of y, substituting $y=\sqrt{x}$ into the equation, we arrive at: $Y=\frac{2}{3}x^{\frac{3}{2}}$ which is the correct answer if we'd just used the power rule of integrals. Now that was just an example to see if it worked the true problem arrises from when we take a new line, notably: $x^2+y^2=1$ and ""implicitly integrate"", note the quotations, just a made up name. If we do what we normally do: $x^2y'+y^2y'=y'$ $\int x^2y'dx + \int y^2y'dx = \int y'dx$ $\int x^2y'dx + \frac{1}{3}y^{3} = y$ Our problem occurs when we attempt to integrate that $x^2$ guy. After preforming 2 integrations by parts the integral ends up containing $Y$ which is just something that we don't want to happen. So if anyone knows what to do and we're just not seeing something painfully obvious please do tell. Addendum This is work we've done on $\int x^2y'dx$ and some other stuff as per the request of StinkingBishop: If $u=x$ , $du=dx$ and $dv=xy'$ , $v=\int xy'dx$ and then by subbing into the IBP formula we arrived at: $x(xy-Y)-\int (xy-Y)dx$ which is where problems arise because now we're integrating $Y$ . Additionally, we tried taking derivatives and manipulating the equation but the furtherest we got was: (Differentiation) $2x+2yy'=0$ then $y'=\frac{-x}{y}$ $x^2+y^2=1$ $\frac{x^2}{y}+y=\frac{1}{y}$ (Recall: $y'=\frac{-x}{y}$ ) $-xy'+y=\frac{1}{y}$ (Integrating) $-(xy-Y)+Y=\int \frac{dx}{y}$ and this is where we got stuck again, if anyone can help that would be great.","My friend and I are attempting to do some crazy integral stuff which he came up with but I am still having trouble getting my head around, but here's the general gist of what he is trying to get at: Propose we have a function: . This of course is easy enough to integrate in of itself but what we're trying to figure out is if we make it so then integrate both sides of the function, can we still get the same answer. Here's how he computed it: The can be solved using u-substitution because recall that y is a function with respect to x rather than a constant. Additionally, the can be solved using integration by parts where one can arrive at note how that so now we have: Now we solve for : This is the correct answer, but it's just in terms of y, substituting into the equation, we arrive at: which is the correct answer if we'd just used the power rule of integrals. Now that was just an example to see if it worked the true problem arrises from when we take a new line, notably: and ""implicitly integrate"", note the quotations, just a made up name. If we do what we normally do: Our problem occurs when we attempt to integrate that guy. After preforming 2 integrations by parts the integral ends up containing which is just something that we don't want to happen. So if anyone knows what to do and we're just not seeing something painfully obvious please do tell. Addendum This is work we've done on and some other stuff as per the request of StinkingBishop: If , and , and then by subbing into the IBP formula we arrived at: which is where problems arise because now we're integrating . Additionally, we tried taking derivatives and manipulating the equation but the furtherest we got was: (Differentiation) then (Recall: ) (Integrating) and this is where we got stuck again, if anyone can help that would be great.",f(x)={\sqrt{x}} {y^2}=x y^2=x y^2y'=xy' \int y^2y'dx=\int xy'dx \int y^2y'dx \int xy' dx \int xy'dx=xy-\int y dx \int ydx = Y \frac{1}{3}y^3=xy-Y Y Y=xy-\frac{1}{3}y^3 Y=\frac{2}{3}y^3 y=\sqrt{x} Y=\frac{2}{3}x^{\frac{3}{2}} x^2+y^2=1 x^2y'+y^2y'=y' \int x^2y'dx + \int y^2y'dx = \int y'dx \int x^2y'dx + \frac{1}{3}y^{3} = y x^2 Y \int x^2y'dx u=x du=dx dv=xy' v=\int xy'dx x(xy-Y)-\int (xy-Y)dx Y 2x+2yy'=0 y'=\frac{-x}{y} x^2+y^2=1 \frac{x^2}{y}+y=\frac{1}{y} y'=\frac{-x}{y} -xy'+y=\frac{1}{y} -(xy-Y)+Y=\int \frac{dx}{y},"['calculus', 'integration']"
46,What happens when an even natural number $n$ meets the integral $\int_{0}^{\frac{\pi}{2}} x\tan 2 x \ln^n (\tan x) d x$?,What happens when an even natural number  meets the integral ?,n \int_{0}^{\frac{\pi}{2}} x\tan 2 x \ln^n (\tan x) d x,"In my post , I found that $$ \int_{0}^{\frac{\pi}{2}} x\tan 2 x \ln (\tan x) d x = -\frac{\pi^{3}}{32},$$ then I  want to generalise it to $$ I_n=\int_{0}^{\frac{\pi}{2}} x\tan 2 x \ln^n (\tan x) d x $$ Again, letting $x\mapsto \frac{\pi}{2}-x$ transform the integral $$ \begin{aligned} I_n=&\frac{(-1)^{n+1} \pi}{2} \int_0^{\frac{\pi}{2}} \tan (2 x) \ln ^n(\tan x) d x +(-1)^n \int_0^{\frac{\pi}{2}} x \tan (2 x) \ln ^n(\tan x) d x \end{aligned} $$ Fortunately, when $n$ is odd , $$ I_n=-\frac{\pi}{4} \underbrace{\int_0^{\frac{\pi}{2}} \tan (2 x) \ln ^n(\tan x) d x}_{J_n} $$ Helpfully, the substitution $y=\tan^2 x$ simplifies the integral $$ \begin{aligned} J_n &=\int_0^{\frac\pi2} \frac{2 \tan x}{1-\tan ^2 x} \ln ^n(\tan x) dx =\frac{1}{2^n} \int_0^{\infty} \frac{\ln ^n y}{1-y^2} d y \\ &=\frac{1}{2^{n-1}} \int_0^1 \frac{\ln ^n y}{1-y^2} d y=\frac{n !}{2^{n-1}}\left(1-\frac{1}{2^{n+1}}\right)\zeta(n+1) \end{aligned} $$ (For the last integral please refer to the footnote) Now we can conclude that $$\boxed{\int_{0}^{\frac{\pi}{2}} x\tan 2 x \ln^n (\tan x) d x =-\frac{n ! \pi }{2^{n+1}}\left(1-\frac{1}{2^{n+1}}\right) \zeta(n+1)} $$ where $n$ is a odd natural numbers. For examples, $$ \begin{aligned} I_1 &=-\frac{\pi \cdot 1 !}{2^2}\left(1-\frac{1}{2^2}\right) \zeta(2)=-\frac{\pi^3}{32} \\ I_3 &=-\frac{\pi \cdot 3 !}{2^4}\left(1-\frac{1}{2^4}\right) \zeta(4)=-\frac{\pi^5}{256} \\ I_5 &=-\frac{\pi \cdot 5 !}{2^6}\left(1-\frac{1}{2^6}\right) \zeta(6)=-\frac{\pi^7}{512} \\ I_{23} &=-\frac{\pi \cdot 23 !}{2^{24}}\left(1-\frac{1}{2^{24}}\right) \zeta(24) =-\frac{968383680827 \pi^{25}}{536870912} \end{aligned} $$ Succeeding in finding a formula for $I_{2n+1}$ with odd powers, I am eager to find one for the $I_{2n}$ with even powers as the technique just used fails when $n$ is even. Could you help? Footnote: $$\int_0^1 \frac{\ln ^n y}{1-y^2} d y =\sum_{k=0}^{\infty} \int_0^1 y^{2 k} \ln ^n y d y =\left.\frac{\partial^n}{\partial a^n} \int_0^1 y^a d y\right|_{a=2 k} \\= \sum_{k=0}^{\infty}\frac{(-1)^n n !}{(2 k+1)^{n+1}} =n!\left(1-\frac{1}{2^{n+1}}\right) \zeta(n+1) $$","In my post , I found that then I  want to generalise it to Again, letting transform the integral Fortunately, when is odd , Helpfully, the substitution simplifies the integral (For the last integral please refer to the footnote) Now we can conclude that where is a odd natural numbers. For examples, Succeeding in finding a formula for with odd powers, I am eager to find one for the with even powers as the technique just used fails when is even. Could you help? Footnote:"," \int_{0}^{\frac{\pi}{2}} x\tan 2 x \ln (\tan x) d x = -\frac{\pi^{3}}{32},  I_n=\int_{0}^{\frac{\pi}{2}} x\tan 2 x \ln^n (\tan x) d x  x\mapsto \frac{\pi}{2}-x 
\begin{aligned}
I_n=&\frac{(-1)^{n+1} \pi}{2} \int_0^{\frac{\pi}{2}} \tan (2 x) \ln ^n(\tan x) d x +(-1)^n \int_0^{\frac{\pi}{2}} x \tan (2 x) \ln ^n(\tan x) d x
\end{aligned}
 n 
I_n=-\frac{\pi}{4} \underbrace{\int_0^{\frac{\pi}{2}} \tan (2 x) \ln ^n(\tan x) d x}_{J_n}
 y=\tan^2 x 
\begin{aligned}
J_n &=\int_0^{\frac\pi2} \frac{2 \tan x}{1-\tan ^2 x} \ln ^n(\tan x) dx =\frac{1}{2^n} \int_0^{\infty} \frac{\ln ^n y}{1-y^2} d y \\
&=\frac{1}{2^{n-1}} \int_0^1 \frac{\ln ^n y}{1-y^2} d y=\frac{n !}{2^{n-1}}\left(1-\frac{1}{2^{n+1}}\right)\zeta(n+1)
\end{aligned}  \boxed{\int_{0}^{\frac{\pi}{2}} x\tan 2 x \ln^n (\tan x) d x =-\frac{n ! \pi }{2^{n+1}}\left(1-\frac{1}{2^{n+1}}\right) \zeta(n+1)}  n 
\begin{aligned}
I_1 &=-\frac{\pi \cdot 1 !}{2^2}\left(1-\frac{1}{2^2}\right) \zeta(2)=-\frac{\pi^3}{32} \\
I_3 &=-\frac{\pi \cdot 3 !}{2^4}\left(1-\frac{1}{2^4}\right) \zeta(4)=-\frac{\pi^5}{256} \\
I_5 &=-\frac{\pi \cdot 5 !}{2^6}\left(1-\frac{1}{2^6}\right) \zeta(6)=-\frac{\pi^7}{512} \\
I_{23} &=-\frac{\pi \cdot 23 !}{2^{24}}\left(1-\frac{1}{2^{24}}\right) \zeta(24) =-\frac{968383680827 \pi^{25}}{536870912}
\end{aligned}
 I_{2n+1} I_{2n} n \int_0^1 \frac{\ln ^n y}{1-y^2} d y =\sum_{k=0}^{\infty} \int_0^1 y^{2 k} \ln ^n y d y =\left.\frac{\partial^n}{\partial a^n} \int_0^1 y^a d y\right|_{a=2 k} \\= \sum_{k=0}^{\infty}\frac{(-1)^n n !}{(2 k+1)^{n+1}} =n!\left(1-\frac{1}{2^{n+1}}\right) \zeta(n+1) ","['calculus', 'integration', 'definite-integrals', 'trigonometric-integrals', 'zeta-functions']"
47,Find the value of $\int_{0}^{1}f(x)dx$ given the following conditions without using CS inequality,Find the value of  given the following conditions without using CS inequality,\int_{0}^{1}f(x)dx,"We are asked to solve for the value of $\int_{0}^{1}f(x)dx$ given that $$ f(1) = 0 \tag{1}  $$ $$ \\ \int_{0}^{1}[f'(x)]^2dx = 7 \tag{2} \\ $$ $$ \int_{0}^{1}x^2f(x)dx = \frac{1}{3} \tag{3} $$ We can solve this using Cauchy-Schwartz Inequality: From $(3)$ we can use integration by parts to obtain $$ \int_{0}^{1}x^3f'(x)dx = -1 \tag{4}$$ With $(2)$ and $(3)$ , and from Cauchy-Schwartz inequality $$ \Big[ \int_{0}^{1}x^3f'(x)dx \Big]^2 \leq \underbrace{\Big( \int_{0}^{1}[x^3]^2dx \Big)}_{\frac{1}{7}} \underbrace{\Big( \int_{0}^{1}[f'(x)]^2dx\Big)}_{7 \text{ from } (2)}$$ Therefore we have, $$ 1 \leq 1 $$ Thus an equality, which in turn implies $f'(x) = ax^3$ where $a$ is just a scalar. From $(1)$ we have, $$f(x) = \frac{a}{4}x^4 + C \rightarrow f(1) = \frac{a}{4} + C  = 0 $$ and from $(2)$ we have, $$ \int_{0}^{1}[f'(x)]^2dx = \int_{0}^{1}(ax^3)^2dx = a^2\underbrace{\int_{0}^{1}x^6dx}_{\frac{1}{7}} = 7 \rightarrow a = \pm 7$$ $$f(x) = \pm \frac{7}{4}x^4 \mp \frac{7}{4}$$ and finally from $(3)$ we can see $$\int_{0}^{1}x^2\big(-\frac{7}{4}x^4 +\frac{7}{4} \big) dx = \frac{1}{3} $$ but $$\int_{0}^{1}x^2\big(\frac{7}{4}x^4 - \frac{7}{4} \big) dx = - \frac{1}{3} $$ Therefore we can conclude $$ \boxed{f(x) = -\frac{7}{4}x^4 +\frac{7}{4}} $$ and $$\boxed{\int_{0}^{1}f(x)dx = \int_{0}^{1}-\frac{7}{4}x^4 +\frac{7}{4}dx = \frac{7}{5}}$$ This concludes the solution using CS inequality. But are there other methods to solve this, preferably only using high-school/AP level calculus?","We are asked to solve for the value of given that We can solve this using Cauchy-Schwartz Inequality: From we can use integration by parts to obtain With and , and from Cauchy-Schwartz inequality Therefore we have, Thus an equality, which in turn implies where is just a scalar. From we have, and from we have, and finally from we can see but Therefore we can conclude and This concludes the solution using CS inequality. But are there other methods to solve this, preferably only using high-school/AP level calculus?",\int_{0}^{1}f(x)dx  f(1) = 0 \tag{1}    \\ \int_{0}^{1}[f'(x)]^2dx = 7 \tag{2} \\   \int_{0}^{1}x^2f(x)dx = \frac{1}{3} \tag{3}  (3)  \int_{0}^{1}x^3f'(x)dx = -1 \tag{4} (2) (3)  \Big[ \int_{0}^{1}x^3f'(x)dx \Big]^2 \leq \underbrace{\Big( \int_{0}^{1}[x^3]^2dx \Big)}_{\frac{1}{7}} \underbrace{\Big( \int_{0}^{1}[f'(x)]^2dx\Big)}_{7 \text{ from } (2)}  1 \leq 1  f'(x) = ax^3 a (1) f(x) = \frac{a}{4}x^4 + C \rightarrow f(1) = \frac{a}{4} + C  = 0  (2)  \int_{0}^{1}[f'(x)]^2dx = \int_{0}^{1}(ax^3)^2dx = a^2\underbrace{\int_{0}^{1}x^6dx}_{\frac{1}{7}} = 7 \rightarrow a = \pm 7 f(x) = \pm \frac{7}{4}x^4 \mp \frac{7}{4} (3) \int_{0}^{1}x^2\big(-\frac{7}{4}x^4 +\frac{7}{4} \big) dx = \frac{1}{3}  \int_{0}^{1}x^2\big(\frac{7}{4}x^4 - \frac{7}{4} \big) dx = - \frac{1}{3}   \boxed{f(x) = -\frac{7}{4}x^4 +\frac{7}{4}}  \boxed{\int_{0}^{1}f(x)dx = \int_{0}^{1}-\frac{7}{4}x^4 +\frac{7}{4}dx = \frac{7}{5}},"['calculus', 'analysis', 'cauchy-schwarz-inequality']"
48,"Solution verification: Usage of L'Hpital's rule, derivatives of trigonometric functions","Solution verification: Usage of L'Hpital's rule, derivatives of trigonometric functions",,"I did the following problem: Find $\displaystyle\lim_{x\to 0} \frac{\cos^2x-1}{x^2}$ The following solution was given: $$\lim_{x\to 0} \frac{\cos^2x-1}{x^2} = \lim_{x\to 0} \frac{2 \sin x \cdot \cos x}{2x} = 1$$ My questions regarding this solution are these: In the denominator of the second limit in the solution above we have $2x$ and since we have $x$ aproaching $0$ this would mean division by $0$ which is not allowed. So, using L'Hpital's rule, we should find the next higher derivative. And regarding the numerator of this second limit I think, it should be $2 \cos x \cdot (- \sin x)$ by usage of the chain rule. Thus I came the following solution: $$\lim_{x\to 0} \frac{\cos^2x-1}{x^2} = \lim_{x\to 0} \frac{2 \cos x \cdot (- \sin x)}{2x} = \lim_{x\to 0} \frac{2\sin^2 x - 2 \cos^2 x}{2} = \frac{-2}{2} = -1$$ So, since these are different solutions, which is correct and where did I make a mistake? I would be thankful for explanations.","I did the following problem: Find The following solution was given: My questions regarding this solution are these: In the denominator of the second limit in the solution above we have and since we have aproaching this would mean division by which is not allowed. So, using L'Hpital's rule, we should find the next higher derivative. And regarding the numerator of this second limit I think, it should be by usage of the chain rule. Thus I came the following solution: So, since these are different solutions, which is correct and where did I make a mistake? I would be thankful for explanations.",\displaystyle\lim_{x\to 0} \frac{\cos^2x-1}{x^2} \lim_{x\to 0} \frac{\cos^2x-1}{x^2} = \lim_{x\to 0} \frac{2 \sin x \cdot \cos x}{2x} = 1 2x x 0 0 2 \cos x \cdot (- \sin x) \lim_{x\to 0} \frac{\cos^2x-1}{x^2} = \lim_{x\to 0} \frac{2 \cos x \cdot (- \sin x)}{2x} = \lim_{x\to 0} \frac{2\sin^2 x - 2 \cos^2 x}{2} = \frac{-2}{2} = -1,"['calculus', 'limits', 'trigonometry', 'indeterminate-forms']"
49,Integers $n$ such that $i(i+1)(i+2) \cdots (i+n)$ is real or pure imaginary,Integers  such that  is real or pure imaginary,n i(i+1)(i+2) \cdots (i+n),"A couple of days ago I happened to come across [1], where the curious fact that $i(i-1)(i-2)(i-3)=-10$ appears ($i$ is the imaginary unit). This led me to the following question: Problem 1: Is $3$ the only positive integer value of $n$ such that $i(i-1)(i-2) \cdots (i-n)$ is a real number or a pure imaginary number? If not, can we describe all such integer values of $n$? Initial Analysis: We can view $i(i-1)(i-2) \cdots (i-n)$ as the result of applying a finite sequence of operations to $i,$ each of which involves a radial stretch from the origin and a rotation about the origin. Specifically, $i$ is moved radially outward by a factor of $\sqrt{1^2 + 1^2}\sqrt{1^2 + 2^2} \cdots \sqrt{1^2 + n^2}$ and rotated counterclockwise by the angle $\arctan 1 + \arctan 2 + \cdots + \arctan n.$ Since we don't care about the magnitude of the result, only whether we land on the real axis or the imaginary axis, Problem 1 reduces to asking whether $3$ is the only positive integer value of $n$ such that $\arctan 1 + \arctan 2 + \cdots \arctan n$ is an integer multiple of $\frac{\pi}{4}.$ One can also check that $i(i+1)(i+2)(i+3) = -10.$ This is not a coincidence. Since $\arctan 1 + \arctan 2 + \arctan 3$ is an integer multiple of $\frac{\pi}{4},$ it follows that $\arctan (-1) + \arctan (-2) + \arctan (-3) = -\left(\arctan 1 + \arctan 2 + \arctan 3 \right)$ must also be an integer multiple of $\frac{\pi}{4}.$ Other than the products $(i-3)(i-2)(i-1)(i)$ and $(i)(i+1)(i+2)(i+3)$ (and two other products obtained by omitting the $i$ factor from these two), and products of the form $(i-k)(i-k+1) \cdots (i+k-1)(i+k)$ where $k$ can be any positive integer, I don't know any product of the form $(i+m)(i+m+1) \cdots (i+n)$ for integers $m$ and $n$ with $m < n$ that equals a real number or a pure imaginary number. Problem 2: Are $(i-3)(i-2)(i-1)(i)$ and $(i)(i+1)(i+2)(i+3)$ (and two other products obtained by omitting the the $i$ factor from these two), and products of the form $(i-k)(i-k+1) \cdots (i+k-1)(i+k)$ where $k$ can be any positive integer, the only pairs of integers $(m,n)$ with $m < n$ such that $(i+m)(i+m+1) \cdots (i+n)$ is a real number or a pure imaginary number? If not, can we describe all such pairs of integers $(m,n)$? Of course, it is easy to come up problems having a broader scope, such as replacing ""$i+m$"" with an arbitrary complex number whose real and imaginary parts are integers and/or using factors that increment the imaginary part by $1$ (or increment both the real and imaginary parts by $1$) and/or using factors that increment the real part (or the imaginary part, or both the real part and the imaginary part) by a constant integer amount, etc. I suspect the answers to these problems can be obtained from a careful analysis of Carl Strmer's 1890s results involving Machin-like formulas , but my knowledge of French and of this field of mathematics is rather poor. For those interested, I suggest looking at Strmer [2]. I also suspect there is a more direct way to solve Problem 1, and perhaps also Problem 2. Personally, I am only moderately interested in this issue, but I am posting it because I thought others in this group might find this something interesting to pursue. In particular, if there is not a fairly trivial way to solve Problem 1 and someone manages to find a solution that isn't extremely difficult, I suspect that such a solution would make for an interesting Math-Monthly type paper. [1] Charles-Ange Laisant (18411920), Remarque sur une quation diffrentielle linaire [Remark on a linear differential equation], Bulletin de la Socit mathmatique de France 23 (1895), 62-63. [2] Fredrik Carl Mlertz Strmer [Strmer] (1874-1957), Sur l'application de la thorie des nombres entiers complexes  la solution en nombres rationnels $x_{1} \; x_{2} \; \dots \; x_{n} \; c_{1} \; c_{2} \; \dots \; c_{n}, \; k$ de l'quation : $c_{1} \text{arc tg}\, x_{1} + c_{2} \text{arc tg}\, x_{2} + \dots  . + c_{n} \text{arc tg}\, x_{n} = k\frac{\pi}{4},$ [On an application of the theory of complex integers to the solution in rational numbers $x_{1} \; x_{2} \; \dots \; x_{n} \; c_{1} \; c_{2} \; \dots \; c_{n}, \; k$ of the equation: $c_{1} \arctan x_{1} + c_{2} \arctan x_{2} + \dots . + c_{n} \arctan x_{n} = k\frac{\pi}{4}$], Archiv for Mathematik og Naturvidenskab 19 #3 (1896), 95 + 1 (errata) pages. UPDATE (30 December 2013): I have incorporated the comment benh made and I have made slight corrections to my Strmer paper citation, but otherwise I have left my original wording intact. I'm impressed with the variety of mathematical techniques brought up in the comments and solutions, especially the probabilistic analysis that Hagen von Eitzen gave. I'm choosing KenWSmith's answer because, more than anyone else, he is responsible for bringing to light a solution (to Problem 1): We observe that all products of the form $i(i+1)(i+2) \cdots (i+n)$ have the form $a+bi$ where both $a$ and $b$ are integers. Thus, if $a+bi$ is real or pure imaginary, we have $a=0$ or $b=0,$ and hence the modulus of $a+bi$ equals $b$ or $a,$ and hence the modulus of $a+bi$ will be an integer. On the other hand, the modulus of $a+bi$ also equals $\sqrt{1^2 + 1^2}\sqrt{1^2 + 2^2} \cdots \sqrt{1^2 + n^2}.$ Thus, Problem 1 is equivalent to finding all positive integer values of $n$ such that $\sqrt{1^2 + 1^2}\sqrt{1^2 + 2^2} \cdots \sqrt{1^2 + n^2}$ is an integer. Equivalently, find all positive integer values of $n$ such that $(1^2 + 1^2)(1^2 + 2^2) \cdots (1^2 + n^2)$ is the square of an integer. KenWSmith then posted this last version in mathoverflow: When is the product (1+1)(1+4)(1+n^2) a perfect square? On the same day Lucia supplied an answer by linking to a preprint version of a 2008 paper by Javier Cilleruelo [Journal of Number Theory 128 #8 (August 2008), 2488-2491], which was written solely to answer the question of whether $n=3$ is the only positive integer such that $(1^2 + 1^2)(1^2 + 2^2) \cdots (1^2 + n^2)$ is the square of an integer, which was conjectured and ""partially verified"" in the 2-month earlier paper by Amdeberhan/Medina/Moll [Journal of Number Theory 128 #6 (June 2008), 1807-1846].","A couple of days ago I happened to come across [1], where the curious fact that $i(i-1)(i-2)(i-3)=-10$ appears ($i$ is the imaginary unit). This led me to the following question: Problem 1: Is $3$ the only positive integer value of $n$ such that $i(i-1)(i-2) \cdots (i-n)$ is a real number or a pure imaginary number? If not, can we describe all such integer values of $n$? Initial Analysis: We can view $i(i-1)(i-2) \cdots (i-n)$ as the result of applying a finite sequence of operations to $i,$ each of which involves a radial stretch from the origin and a rotation about the origin. Specifically, $i$ is moved radially outward by a factor of $\sqrt{1^2 + 1^2}\sqrt{1^2 + 2^2} \cdots \sqrt{1^2 + n^2}$ and rotated counterclockwise by the angle $\arctan 1 + \arctan 2 + \cdots + \arctan n.$ Since we don't care about the magnitude of the result, only whether we land on the real axis or the imaginary axis, Problem 1 reduces to asking whether $3$ is the only positive integer value of $n$ such that $\arctan 1 + \arctan 2 + \cdots \arctan n$ is an integer multiple of $\frac{\pi}{4}.$ One can also check that $i(i+1)(i+2)(i+3) = -10.$ This is not a coincidence. Since $\arctan 1 + \arctan 2 + \arctan 3$ is an integer multiple of $\frac{\pi}{4},$ it follows that $\arctan (-1) + \arctan (-2) + \arctan (-3) = -\left(\arctan 1 + \arctan 2 + \arctan 3 \right)$ must also be an integer multiple of $\frac{\pi}{4}.$ Other than the products $(i-3)(i-2)(i-1)(i)$ and $(i)(i+1)(i+2)(i+3)$ (and two other products obtained by omitting the $i$ factor from these two), and products of the form $(i-k)(i-k+1) \cdots (i+k-1)(i+k)$ where $k$ can be any positive integer, I don't know any product of the form $(i+m)(i+m+1) \cdots (i+n)$ for integers $m$ and $n$ with $m < n$ that equals a real number or a pure imaginary number. Problem 2: Are $(i-3)(i-2)(i-1)(i)$ and $(i)(i+1)(i+2)(i+3)$ (and two other products obtained by omitting the the $i$ factor from these two), and products of the form $(i-k)(i-k+1) \cdots (i+k-1)(i+k)$ where $k$ can be any positive integer, the only pairs of integers $(m,n)$ with $m < n$ such that $(i+m)(i+m+1) \cdots (i+n)$ is a real number or a pure imaginary number? If not, can we describe all such pairs of integers $(m,n)$? Of course, it is easy to come up problems having a broader scope, such as replacing ""$i+m$"" with an arbitrary complex number whose real and imaginary parts are integers and/or using factors that increment the imaginary part by $1$ (or increment both the real and imaginary parts by $1$) and/or using factors that increment the real part (or the imaginary part, or both the real part and the imaginary part) by a constant integer amount, etc. I suspect the answers to these problems can be obtained from a careful analysis of Carl Strmer's 1890s results involving Machin-like formulas , but my knowledge of French and of this field of mathematics is rather poor. For those interested, I suggest looking at Strmer [2]. I also suspect there is a more direct way to solve Problem 1, and perhaps also Problem 2. Personally, I am only moderately interested in this issue, but I am posting it because I thought others in this group might find this something interesting to pursue. In particular, if there is not a fairly trivial way to solve Problem 1 and someone manages to find a solution that isn't extremely difficult, I suspect that such a solution would make for an interesting Math-Monthly type paper. [1] Charles-Ange Laisant (18411920), Remarque sur une quation diffrentielle linaire [Remark on a linear differential equation], Bulletin de la Socit mathmatique de France 23 (1895), 62-63. [2] Fredrik Carl Mlertz Strmer [Strmer] (1874-1957), Sur l'application de la thorie des nombres entiers complexes  la solution en nombres rationnels $x_{1} \; x_{2} \; \dots \; x_{n} \; c_{1} \; c_{2} \; \dots \; c_{n}, \; k$ de l'quation : $c_{1} \text{arc tg}\, x_{1} + c_{2} \text{arc tg}\, x_{2} + \dots  . + c_{n} \text{arc tg}\, x_{n} = k\frac{\pi}{4},$ [On an application of the theory of complex integers to the solution in rational numbers $x_{1} \; x_{2} \; \dots \; x_{n} \; c_{1} \; c_{2} \; \dots \; c_{n}, \; k$ of the equation: $c_{1} \arctan x_{1} + c_{2} \arctan x_{2} + \dots . + c_{n} \arctan x_{n} = k\frac{\pi}{4}$], Archiv for Mathematik og Naturvidenskab 19 #3 (1896), 95 + 1 (errata) pages. UPDATE (30 December 2013): I have incorporated the comment benh made and I have made slight corrections to my Strmer paper citation, but otherwise I have left my original wording intact. I'm impressed with the variety of mathematical techniques brought up in the comments and solutions, especially the probabilistic analysis that Hagen von Eitzen gave. I'm choosing KenWSmith's answer because, more than anyone else, he is responsible for bringing to light a solution (to Problem 1): We observe that all products of the form $i(i+1)(i+2) \cdots (i+n)$ have the form $a+bi$ where both $a$ and $b$ are integers. Thus, if $a+bi$ is real or pure imaginary, we have $a=0$ or $b=0,$ and hence the modulus of $a+bi$ equals $b$ or $a,$ and hence the modulus of $a+bi$ will be an integer. On the other hand, the modulus of $a+bi$ also equals $\sqrt{1^2 + 1^2}\sqrt{1^2 + 2^2} \cdots \sqrt{1^2 + n^2}.$ Thus, Problem 1 is equivalent to finding all positive integer values of $n$ such that $\sqrt{1^2 + 1^2}\sqrt{1^2 + 2^2} \cdots \sqrt{1^2 + n^2}$ is an integer. Equivalently, find all positive integer values of $n$ such that $(1^2 + 1^2)(1^2 + 2^2) \cdots (1^2 + n^2)$ is the square of an integer. KenWSmith then posted this last version in mathoverflow: When is the product (1+1)(1+4)(1+n^2) a perfect square? On the same day Lucia supplied an answer by linking to a preprint version of a 2008 paper by Javier Cilleruelo [Journal of Number Theory 128 #8 (August 2008), 2488-2491], which was written solely to answer the question of whether $n=3$ is the only positive integer such that $(1^2 + 1^2)(1^2 + 2^2) \cdots (1^2 + n^2)$ is the square of an integer, which was conjectured and ""partially verified"" in the 2-month earlier paper by Amdeberhan/Medina/Moll [Journal of Number Theory 128 #6 (June 2008), 1807-1846].",,"['number-theory', 'complex-numbers']"
50,"$\int_0^1 tf(2x-t)dt=\frac{1}{2}\arctan(x^2), f(1)=1$, find $\int_0^1 f(x)dx$. here $f$ is continuous.",", find . here  is continuous.","\int_0^1 tf(2x-t)dt=\frac{1}{2}\arctan(x^2), f(1)=1 \int_0^1 f(x)dx f","$\int_0^1 tf(2x-t)dt=\frac{1}{2}\arctan(x^2),\ f(1)=1$ , find $\int_0^1 f(x)dx$ . here $f$ is continuous. By substituting $2x-t=s$ , and taking derivative, we have $2\int_{2x-1}^{2x}f(s)ds=2f(2x-1)+\frac{x}{1+x^4}$ . Let $x=1/2$ , we have $2\int_0^1 f(s)ds=2f(0)+x/(1+x^4)|_{x=1/2}$ . But we know just $f(1)=1$ , not $f(0)$ . How to pass across this obstacle? I strongly suspect it is a wrong problem, but could not find some counterexample.",", find . here is continuous. By substituting , and taking derivative, we have . Let , we have . But we know just , not . How to pass across this obstacle? I strongly suspect it is a wrong problem, but could not find some counterexample.","\int_0^1 tf(2x-t)dt=\frac{1}{2}\arctan(x^2),\ f(1)=1 \int_0^1 f(x)dx f 2x-t=s 2\int_{2x-1}^{2x}f(s)ds=2f(2x-1)+\frac{x}{1+x^4} x=1/2 2\int_0^1 f(s)ds=2f(0)+x/(1+x^4)|_{x=1/2} f(1)=1 f(0)","['calculus', 'integration', 'definite-integrals']"
51,Can you write $1-x-\frac{x^2}{2!}+\frac{x^3}{3!}-\dots$ with elementary functions,Can you write  with elementary functions,1-x-\frac{x^2}{2!}+\frac{x^3}{3!}-\dots,"Can you write $$1-x-\frac{x^2}{2!}+\frac{x^3}{3!}-\frac{x^4}{4!}+\frac{x^5}{5!}+\frac{x^6}{6!}-\frac{x^7}{7!}-\dots$$ with elementary functions, where the function is related to the Thue-Morse sequence stated here ? I was told it converges with all inputs. But I don't know how to write it in elementary functions. Thanks","Can you write with elementary functions, where the function is related to the Thue-Morse sequence stated here ? I was told it converges with all inputs. But I don't know how to write it in elementary functions. Thanks",1-x-\frac{x^2}{2!}+\frac{x^3}{3!}-\frac{x^4}{4!}+\frac{x^5}{5!}+\frac{x^6}{6!}-\frac{x^7}{7!}-\dots,"['calculus', 'taylor-expansion', 'elementary-functions']"
52,integral with function Gamma [closed],integral with function Gamma [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question In the article The Annals of Statistics Vol 18 March 1990, Natural exponential families with cubic variance functions , he is shown with a probabilistic method that $$\int_0^{\infty}\frac{x^{x+p-1}e^{-x}}{\Gamma(x+p+1)}dx=\frac{1}{p}.$$ I am looking for a proof of analysis","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question In the article The Annals of Statistics Vol 18 March 1990, Natural exponential families with cubic variance functions , he is shown with a probabilistic method that I am looking for a proof of analysis",\int_0^{\infty}\frac{x^{x+p-1}e^{-x}}{\Gamma(x+p+1)}dx=\frac{1}{p}.,"['calculus', 'integration']"
53,Does this limit exist or is undefined?,Does this limit exist or is undefined?,,"$$\lim_{x\to -\infty}\ln\left(\frac{x^2+1}{x-3}\right)=\infty$$ This is the answer I get from wolfram alpha, but shouldn't the answer be the limit doesn't exist? For large negative values of x, we can ignore the +1 and -3 so we can change the limit to $$\lim_{x\to -\infty}\ln\left(\frac{x^2}{x}\right)$$ As x approaches - $\infty$ , $\left(\frac{x^2}{x}\right)$ also approaches - $\infty$ so we get $\ln\left(-\infty\right)$ . However, $\ln\left(-\infty\right)$ doesn't make sense because ln(x) isn't even defined for negative numbers. So, the limit doesn't exist and is therefore undefined. Am I wrong?","This is the answer I get from wolfram alpha, but shouldn't the answer be the limit doesn't exist? For large negative values of x, we can ignore the +1 and -3 so we can change the limit to As x approaches - , also approaches - so we get . However, doesn't make sense because ln(x) isn't even defined for negative numbers. So, the limit doesn't exist and is therefore undefined. Am I wrong?",\lim_{x\to -\infty}\ln\left(\frac{x^2+1}{x-3}\right)=\infty \lim_{x\to -\infty}\ln\left(\frac{x^2}{x}\right) \infty \left(\frac{x^2}{x}\right) \infty \ln\left(-\infty\right) \ln\left(-\infty\right),['calculus']
54,How to distribute $n$ red balls between two bins to maximize the chance of sampling only red balls?,How to distribute  red balls between two bins to maximize the chance of sampling only red balls?,n,"Consider two bins that contain an unknown number of black balls. We wish to split $n$ red balls (i.e., choose a number $x\in\{0,\ldots,n-1\}$ to place in a random bin) so that if we pick one ball at random from each bin we maximize our chances of sampling only red balls. It seems that regardless of the number of black balls in each, we should split the red balls evenly. Mathematically, show that for any positive numbers $b_1,b_2$ (representing the black balls) and $n$ : $$ \frac{x}{x+b_1}\cdot\frac{(n-x)}{(n-x)+b_2} + \frac{x}{x+b_2}\cdot\frac{(n-x)}{(n-x)+b_1} \le \frac{2\cdot (n/2)^2}{(n/2+b_1)\cdot (n/2+b_2)}. $$ One way to do that is to consider the function $f(x)$ $$ f(x) = \frac{x}{x+b_1}\cdot\frac{(n-x)}{(n-x)+b_2} + \frac{x}{x+b_2}\cdot\frac{(n-x)}{(n-x)+b_1}, $$ derive it, and find its maximum. I'm wondering if there's some averaging argument that we can use to simplify this proof. Clarification: Our goal is to choose the split $(x,n-x)$ . We cannot determine whether $x$ will be in the bin with $b_1$ balls or the one with $b_2$ balls since we do not know $b_1$ and $b_2$ . Therefore, I assume that $x$ will be placed with $b_1$ with prob. 1/2 and with $b_2$ with prob. 1/2.","Consider two bins that contain an unknown number of black balls. We wish to split red balls (i.e., choose a number to place in a random bin) so that if we pick one ball at random from each bin we maximize our chances of sampling only red balls. It seems that regardless of the number of black balls in each, we should split the red balls evenly. Mathematically, show that for any positive numbers (representing the black balls) and : One way to do that is to consider the function derive it, and find its maximum. I'm wondering if there's some averaging argument that we can use to simplify this proof. Clarification: Our goal is to choose the split . We cannot determine whether will be in the bin with balls or the one with balls since we do not know and . Therefore, I assume that will be placed with with prob. 1/2 and with with prob. 1/2.","n x\in\{0,\ldots,n-1\} b_1,b_2 n 
\frac{x}{x+b_1}\cdot\frac{(n-x)}{(n-x)+b_2} + \frac{x}{x+b_2}\cdot\frac{(n-x)}{(n-x)+b_1} \le \frac{2\cdot (n/2)^2}{(n/2+b_1)\cdot (n/2+b_2)}.
 f(x) 
f(x) = \frac{x}{x+b_1}\cdot\frac{(n-x)}{(n-x)+b_2} + \frac{x}{x+b_2}\cdot\frac{(n-x)}{(n-x)+b_1},
 (x,n-x) x b_1 b_2 b_1 b_2 x b_1 b_2","['calculus', 'probability', 'average', 'balls-in-bins']"
55,Compute $ \xi_p= \prod_{n=1}^{\infty} (1+\frac {1}{n^p})$,Compute, \xi_p= \prod_{n=1}^{\infty} (1+\frac {1}{n^p}),"The main question I want to ask is inspired from this question Find the value of $$\prod_{n=1}^{\infty} \left(1+\frac {1}{n^2}\right)$$ Now, I have solved this question easily using product representation of $\displaystyle \frac {\sin x}{x}$ and then substituting $x=i\pi$ in that representation to get $$\prod_{n=1}^{\infty} \left(1+\frac {1}{n^2}\right)=\frac {\sinh \pi}{\pi}$$ But this led me to investigate a more general infinite product systems as $$\displaystyle\xi_p= \prod_{n=1}^{\infty} \left(1+\frac {1}{n^p}\right)$$ where $p\in N$ and $p\ge 2$ . But I don't know of any special function like that of $\displaystyle \frac {\sin x}{x}$ that could be used to calculate $\xi_p$ , so I just started with finding values of $\xi_2,\xi_3,\xi_4,\cdots$ , and what is quite surprising is that I could conjecture beautiful closed forms for  odd and even $p$ (which I still think are the same). Here are the closed forms I could conjecture $\star$ For odd $p$ $$\displaystyle\xi_p=\frac {1}{\displaystyle \prod_{r=1}^{s-1} \Gamma\left(1+(-1)^r (-1)^{\frac rs}\right)}$$ $ \star$ While for even $p=2k$ , $k\ge 1$ and $k\in N$ I got $$\displaystyle \xi_p=\alpha \frac {\displaystyle \prod_{r=1}^k \sin\left((-1)^{\frac {2r-1}{s}}\pi\right)}{\pi^k}$$ where $\alpha$ can take only the values $+1,-1,i,-i$ depending on some condition (which I was not able to find) It did irk me that the closed form for even $p$ had product of sines instead of gamma function and noticed that using the properties of gamma function I can convert the RHS of the case of even $p$ to a similar form of RHS of odd $p$ but got stuck between at some point. I would like to know whether or not my closed forms are correct and also some rigorous methods to find the closed forms for $\xi_p$ . I would also appreciate if someone could give a proof for the closed forms that he/she would have got for $\xi_p$","The main question I want to ask is inspired from this question Find the value of Now, I have solved this question easily using product representation of and then substituting in that representation to get But this led me to investigate a more general infinite product systems as where and . But I don't know of any special function like that of that could be used to calculate , so I just started with finding values of , and what is quite surprising is that I could conjecture beautiful closed forms for  odd and even (which I still think are the same). Here are the closed forms I could conjecture For odd While for even , and I got where can take only the values depending on some condition (which I was not able to find) It did irk me that the closed form for even had product of sines instead of gamma function and noticed that using the properties of gamma function I can convert the RHS of the case of even to a similar form of RHS of odd but got stuck between at some point. I would like to know whether or not my closed forms are correct and also some rigorous methods to find the closed forms for . I would also appreciate if someone could give a proof for the closed forms that he/she would have got for","\prod_{n=1}^{\infty} \left(1+\frac {1}{n^2}\right) \displaystyle \frac {\sin x}{x} x=i\pi \prod_{n=1}^{\infty} \left(1+\frac {1}{n^2}\right)=\frac {\sinh \pi}{\pi} \displaystyle\xi_p= \prod_{n=1}^{\infty} \left(1+\frac {1}{n^p}\right) p\in N p\ge 2 \displaystyle \frac {\sin x}{x} \xi_p \xi_2,\xi_3,\xi_4,\cdots p \star p \displaystyle\xi_p=\frac {1}{\displaystyle \prod_{r=1}^{s-1} \Gamma\left(1+(-1)^r (-1)^{\frac rs}\right)}  \star p=2k k\ge 1 k\in N \displaystyle \xi_p=\alpha \frac {\displaystyle \prod_{r=1}^k \sin\left((-1)^{\frac {2r-1}{s}}\pi\right)}{\pi^k} \alpha +1,-1,i,-i p p p \xi_p \xi_p","['calculus', 'sequences-and-series', 'gamma-function', 'infinite-product']"
56,closed-form solution to $\int_0^\infty x^a\exp(-bx)\left(\frac{1}{\text{erfc}(c\sqrt{x})}\right)^{2a}$,closed-form solution to,\int_0^\infty x^a\exp(-bx)\left(\frac{1}{\text{erfc}(c\sqrt{x})}\right)^{2a},"This integral comes up in a problem in Statistics involving power laws. Here are some notes if anyone is interested. The integral in question would be related to equation (7) therein. I would like to compute $$ \int_0^\infty x^a\exp(-bx)\left(\frac{1}{\text{erfc}(c\sqrt{x})}\right)^{2a} dx, $$ where $b > 0$ , $a >0$ and $c \in \mathbb{R}$ and erfc is the complementary error function . This looks hopeless. Anything I could try to squeeze out a closed-form solution?","This integral comes up in a problem in Statistics involving power laws. Here are some notes if anyone is interested. The integral in question would be related to equation (7) therein. I would like to compute where , and and erfc is the complementary error function . This looks hopeless. Anything I could try to squeeze out a closed-form solution?","
\int_0^\infty x^a\exp(-bx)\left(\frac{1}{\text{erfc}(c\sqrt{x})}\right)^{2a} dx,
 b > 0 a >0 c \in \mathbb{R}","['calculus', 'integration', 'error-function']"
57,"Prove that there exists $\xi \in (-1,1)$ such that $f'''(\xi)=3.$",Prove that there exists  such that,"\xi \in (-1,1) f'''(\xi)=3.","Problem Assume that $f(x)$ has the $3$ -order continuous derivative over $[-1,1]$ , and $f(-1)=0,f(1)=1,f'(0)=0$ . Prove that there exists $\xi \in (-1,1)$ such that $f'''(\xi)=3.$ Proof According to Taylor's formula of $f(x)$ expanding at $x=0$ $$f(x)=f(0)+\frac{x^2f''(0)}{2}+\frac{x^3f'''(\xi)}{6},~~~0\lessgtr \xi \lessgtr x,$$ we obtain $$0=f(-1)=f(0)+\frac{f''(0)}{2}-\frac{f'''(\xi_1)}{6},~~~-1<\xi_1<0,$$ $$1=f(1)=f(0)+\frac{f''(0)}{2}+\frac{f'''(\xi_2)}{6},~~~0<\xi_2<1.$$ By substraction, we have $$f'''(\xi_1)+f'''(\xi_2)=6,$$ which implies, $f'''(\xi_1),f'''(\xi_2)$ are both equal to $3$ orone of them is greater than $3$ and the other less than $3$ . For the former case,the conclusion holds triviallyas for the latter one, according to the continuity of $f'''(x)$ , applying the intermediate value theorem, there exists $\xi_3 \in (\xi_1,\xi_2)\subset (-1,1)$ such that $f'''(\xi_3)=3$ the conclusion holds as well. Comment But I suggest that, we only need the existence of $f'''(x)$ , and the continuity of it is not necessary. That's because, according to Darboux's theorem, the derivative function itself has the intermediate value property.","Problem Assume that has the -order continuous derivative over , and . Prove that there exists such that Proof According to Taylor's formula of expanding at we obtain By substraction, we have which implies, are both equal to orone of them is greater than and the other less than . For the former case,the conclusion holds triviallyas for the latter one, according to the continuity of , applying the intermediate value theorem, there exists such that the conclusion holds as well. Comment But I suggest that, we only need the existence of , and the continuity of it is not necessary. That's because, according to Darboux's theorem, the derivative function itself has the intermediate value property.","f(x) 3 [-1,1] f(-1)=0,f(1)=1,f'(0)=0 \xi \in (-1,1) f'''(\xi)=3. f(x) x=0 f(x)=f(0)+\frac{x^2f''(0)}{2}+\frac{x^3f'''(\xi)}{6},~~~0\lessgtr \xi \lessgtr x, 0=f(-1)=f(0)+\frac{f''(0)}{2}-\frac{f'''(\xi_1)}{6},~~~-1<\xi_1<0, 1=f(1)=f(0)+\frac{f''(0)}{2}+\frac{f'''(\xi_2)}{6},~~~0<\xi_2<1. f'''(\xi_1)+f'''(\xi_2)=6, f'''(\xi_1),f'''(\xi_2) 3 3 3 f'''(x) \xi_3 \in (\xi_1,\xi_2)\subset (-1,1) f'''(\xi_3)=3 f'''(x)","['calculus', 'proof-verification']"
58,"Show the sequence $\{{\sqrt{5}}~,{\sqrt{5+{\sqrt5}}}~,\sqrt{5+\sqrt{5+\sqrt{5}}}~,...\}$ converges and find its limit.",Show the sequence  converges and find its limit.,"\{{\sqrt{5}}~,{\sqrt{5+{\sqrt5}}}~,\sqrt{5+\sqrt{5+\sqrt{5}}}~,...\}","Show the sequence $\bigg\{{\sqrt{5}}~,{\sqrt{5+{\sqrt5}}}~,\sqrt{5+\sqrt{5+\sqrt{5}}}~,...\bigg\}$ converges and find its limit. Attempt : Let $a_{1}=\sqrt{5}$ and $a_{n+1}={\sqrt{5+a_{n}}}$ for $n=2,3,...$ Now we apply the induction to prove the sequence$(a_{n})$ is increasing , bounded above by $3.$ $(a_{n})$ is increasing $:$ Note first that $a_{n}\ge0$ for each $n\in{\bf N}$. As $n=1$, one has $a_{1}^{2}=5<5+\sqrt 5=a_{2}^{2}\Longrightarrow |a_{1}|<|a_{2}|\Longrightarrow a_{1}<a_{2}$ . Now assume $a_{n-1}<a_{n}$ for some $n\in\bf N$ . Then $a_{n}^{2}=5+a_{n-1}<5+a_{n}=a_{n+1}^{2}\Longrightarrow |a_{n}|<|a_{n+1}|\Longrightarrow a_{n}<a_{n+1}$ . $(a_{n})$ is bounded above by 3 $:$ As the same manner , we have $a_{1}=\sqrt 5<\sqrt 9=3$ for $n=1$. Suppose the process holds for some integer $n>0,$ that is , $a_{n}<3$ for some $n\in\bf N$ . Whence , $a_{n+1}=\sqrt{5+a_{n}}<\sqrt{5+3}<\sqrt9=3 .$ Therefore, we see on account of the monotonic sequence Theorem that $(a_{n})$ is convergent . Limit of $(a_{n}):$ Take $\displaystyle\lim_{n\rightarrow\infty}a_{n}=L$ for some $L\ge0$ since $a_{n}$ is non-negative for all $n\in\bf N$. Thus , $\sqrt{5+L}=\sqrt{5+\displaystyle\lim_{n\rightarrow\infty}a_{n-1}}\color{red}=\displaystyle\lim_{n\rightarrow\infty}\sqrt{5+a_{n-1}}=\displaystyle\lim_{n\rightarrow\infty}a_{n}=L$ , keep in mind that the red equality holds by $\sqrt{x}$ is continuous . Then , one has $L^{2}-L-5=0\Longrightarrow L=\displaystyle\frac{1+\sqrt{21}}{2}$ since $L$ is non-negative . Can anyone check my proof for validity if you have the time , otherwise ignore this, that is okay . Any comment or valuable suggestion I will be grateful .","Show the sequence $\bigg\{{\sqrt{5}}~,{\sqrt{5+{\sqrt5}}}~,\sqrt{5+\sqrt{5+\sqrt{5}}}~,...\bigg\}$ converges and find its limit. Attempt : Let $a_{1}=\sqrt{5}$ and $a_{n+1}={\sqrt{5+a_{n}}}$ for $n=2,3,...$ Now we apply the induction to prove the sequence$(a_{n})$ is increasing , bounded above by $3.$ $(a_{n})$ is increasing $:$ Note first that $a_{n}\ge0$ for each $n\in{\bf N}$. As $n=1$, one has $a_{1}^{2}=5<5+\sqrt 5=a_{2}^{2}\Longrightarrow |a_{1}|<|a_{2}|\Longrightarrow a_{1}<a_{2}$ . Now assume $a_{n-1}<a_{n}$ for some $n\in\bf N$ . Then $a_{n}^{2}=5+a_{n-1}<5+a_{n}=a_{n+1}^{2}\Longrightarrow |a_{n}|<|a_{n+1}|\Longrightarrow a_{n}<a_{n+1}$ . $(a_{n})$ is bounded above by 3 $:$ As the same manner , we have $a_{1}=\sqrt 5<\sqrt 9=3$ for $n=1$. Suppose the process holds for some integer $n>0,$ that is , $a_{n}<3$ for some $n\in\bf N$ . Whence , $a_{n+1}=\sqrt{5+a_{n}}<\sqrt{5+3}<\sqrt9=3 .$ Therefore, we see on account of the monotonic sequence Theorem that $(a_{n})$ is convergent . Limit of $(a_{n}):$ Take $\displaystyle\lim_{n\rightarrow\infty}a_{n}=L$ for some $L\ge0$ since $a_{n}$ is non-negative for all $n\in\bf N$. Thus , $\sqrt{5+L}=\sqrt{5+\displaystyle\lim_{n\rightarrow\infty}a_{n-1}}\color{red}=\displaystyle\lim_{n\rightarrow\infty}\sqrt{5+a_{n-1}}=\displaystyle\lim_{n\rightarrow\infty}a_{n}=L$ , keep in mind that the red equality holds by $\sqrt{x}$ is continuous . Then , one has $L^{2}-L-5=0\Longrightarrow L=\displaystyle\frac{1+\sqrt{21}}{2}$ since $L$ is non-negative . Can anyone check my proof for validity if you have the time , otherwise ignore this, that is okay . Any comment or valuable suggestion I will be grateful .",,"['calculus', 'analysis', 'proof-verification', 'nested-radicals']"
59,Are there discussion sites online for university level mathematics?,Are there discussion sites online for university level mathematics?,,"I have been trying to learn maths on my own for a while, I've realised most of the problems that stump me would be solved in 5 seconds, if I had a mentor. Of course, learning on your own teaches you problem solving, but too often I'm banging my head against the wall on something I was not ready for. Teaching yourself math is like fumbling around in the dark. It is clear that my highschool math curriculum has not prepared me for the advanced math I am trying to learn. Most books I've come across seem to assume a classroom setting, there are solutions missing, the language too terse. I know StackExchange is a question and answer site, not for discussions. So where could a math autodidact go online to discuss math? Preferably with others who are also self learning higher level mathematics or those who are knowledgeable about mathematics and can act as mentor. I don't mind paying a subscription, if there is such a service.","I have been trying to learn maths on my own for a while, I've realised most of the problems that stump me would be solved in 5 seconds, if I had a mentor. Of course, learning on your own teaches you problem solving, but too often I'm banging my head against the wall on something I was not ready for. Teaching yourself math is like fumbling around in the dark. It is clear that my highschool math curriculum has not prepared me for the advanced math I am trying to learn. Most books I've come across seem to assume a classroom setting, there are solutions missing, the language too terse. I know StackExchange is a question and answer site, not for discussions. So where could a math autodidact go online to discuss math? Preferably with others who are also self learning higher level mathematics or those who are knowledgeable about mathematics and can act as mentor. I don't mind paying a subscription, if there is such a service.",,"['calculus', 'probability', 'abstract-algebra', 'combinatorics', 'self-learning']"
60,"How to determine $n$, such that $x\uparrow \uparrow n>10^{100}$?","How to determine , such that ?",n x\uparrow \uparrow n>10^{100},"If $x$ is a real number greater than $e^{e^{-1}}$ , then $x\uparrow \uparrow n$ (A power tower of $n$ $x's$) tends to $\infty$, if $n$ tends to $\infty$. Therefore, there must be a number $n$, such that $x\uparrow\uparrow n>10^{100}$ Can I determine the smallest number $n$ satisfying this inequality without applying the iteration $x_1=x$ , $x_{n+1}=x^{x_n}$ ? For example, for $\color\red {x=e^{e^{-1}}+10^{-10}}$, we have $\color\green {n=323\ 892}$","If $x$ is a real number greater than $e^{e^{-1}}$ , then $x\uparrow \uparrow n$ (A power tower of $n$ $x's$) tends to $\infty$, if $n$ tends to $\infty$. Therefore, there must be a number $n$, such that $x\uparrow\uparrow n>10^{100}$ Can I determine the smallest number $n$ satisfying this inequality without applying the iteration $x_1=x$ , $x_{n+1}=x^{x_n}$ ? For example, for $\color\red {x=e^{e^{-1}}+10^{-10}}$, we have $\color\green {n=323\ 892}$",,"['calculus', 'tetration', 'big-numbers']"
61,"Derivative of a f(x,y) with respect to g(x,y)","Derivative of a f(x,y) with respect to g(x,y)",,"I've seen other posts about finding a derivative with respect to another function, but I didn't understand how it would work when the functions have more than one variable. I would like a general explanation about how to find $\frac{df(x,y)}{dg(x,y)}$, but take the following functions to illustrate it: Given the functions $$f(x,y) = 3x + 5y$$ $$g(x,y) = 2x + y$$ is it possible to find $\frac{df}{dg}$?","I've seen other posts about finding a derivative with respect to another function, but I didn't understand how it would work when the functions have more than one variable. I would like a general explanation about how to find $\frac{df(x,y)}{dg(x,y)}$, but take the following functions to illustrate it: Given the functions $$f(x,y) = 3x + 5y$$ $$g(x,y) = 2x + y$$ is it possible to find $\frac{df}{dg}$?",,"['calculus', 'derivatives']"
62,Limit points of a sequence constructed from pi (if pi is normal),Limit points of a sequence constructed from pi (if pi is normal),,"I'm interested in this question apparently posed by John Nash, which I found in the book A Beautiful Mind. If you make up a bunch of fractions of pi $3.141592\ldots$. If you start   from the decimal point, take the first digit, and place decimal point to   the left, you get $.1$ Then take the next 2 digits $.41$ Then take the next 3 digits $.592$ You get a sequence of fractions between $0$ and $1$. What are the limit points of this set of numbers? I would like to know what can be deduced by assuming $\pi$ is normal? It seems to me that, if $\pi$ is normal, the sequence defined above must be non-convergent and therefore have at least two limit points (see the discussion here ). But I think by choosing carefully which subsequences to look for, you should be able to construct as many limit points as you like (again, if $\pi$ is normal). For example, you could look for subsequences bounded by $[0,1/3)$, $[1/3,2/3)$ and $[2/3,1]$ to find three distinct limit points. Could anyone tell me if this reasoning is correct?","I'm interested in this question apparently posed by John Nash, which I found in the book A Beautiful Mind. If you make up a bunch of fractions of pi $3.141592\ldots$. If you start   from the decimal point, take the first digit, and place decimal point to   the left, you get $.1$ Then take the next 2 digits $.41$ Then take the next 3 digits $.592$ You get a sequence of fractions between $0$ and $1$. What are the limit points of this set of numbers? I would like to know what can be deduced by assuming $\pi$ is normal? It seems to me that, if $\pi$ is normal, the sequence defined above must be non-convergent and therefore have at least two limit points (see the discussion here ). But I think by choosing carefully which subsequences to look for, you should be able to construct as many limit points as you like (again, if $\pi$ is normal). For example, you could look for subsequences bounded by $[0,1/3)$, $[1/3,2/3)$ and $[2/3,1]$ to find three distinct limit points. Could anyone tell me if this reasoning is correct?",,"['calculus', 'sequences-and-series', 'limits', 'pi']"
63,Help Calculation of $\sum_{k=1}^{\infty} \frac{k^{2n}}{e^k -1}$,Help Calculation of,\sum_{k=1}^{\infty} \frac{k^{2n}}{e^k -1},"Recently, I read a book : Euler, Riemann, Ramanujan - Contact mathematician beyond the space-time by Nobushige Kurokaw. It says that Ramanujan had found the following formula $$\sum_{k=1}^{\infty} \frac{k}{e^{2k \pi}-1}=\frac{1}{24}-\frac{1}{8\pi}$$ After few month, I succeeded in finding similar formula using Euler-Maclaurin Formula: $$\sum_{k=1}^{\infty}\frac{k}{e^k -1}=\frac{{\pi}^2}{6}-\frac{11}{24}$$ $$\sum_{k=1}^{\infty}\frac{k^{2n-1}}{e^k -1}=\frac{\left| B_{2n}\right|}{4n}((2\pi)^{2n}+(-1)^{n+1}) \quad when \quad n>1$$ I wonder if we can generalize te following formula : $$\sum_{k=1}^{\infty}\frac{k^{2n}}{e^k -1}$$ I tried with various ways, but I failed. Please answer back users~~~ PS. It's first time that I answer a question on this site. So I could have made some mistakes while writing....","Recently, I read a book : Euler, Riemann, Ramanujan - Contact mathematician beyond the space-time by Nobushige Kurokaw. It says that Ramanujan had found the following formula $$\sum_{k=1}^{\infty} \frac{k}{e^{2k \pi}-1}=\frac{1}{24}-\frac{1}{8\pi}$$ After few month, I succeeded in finding similar formula using Euler-Maclaurin Formula: $$\sum_{k=1}^{\infty}\frac{k}{e^k -1}=\frac{{\pi}^2}{6}-\frac{11}{24}$$ $$\sum_{k=1}^{\infty}\frac{k^{2n-1}}{e^k -1}=\frac{\left| B_{2n}\right|}{4n}((2\pi)^{2n}+(-1)^{n+1}) \quad when \quad n>1$$ I wonder if we can generalize te following formula : $$\sum_{k=1}^{\infty}\frac{k^{2n}}{e^k -1}$$ I tried with various ways, but I failed. Please answer back users~~~ PS. It's first time that I answer a question on this site. So I could have made some mistakes while writing....",,"['calculus', 'sequences-and-series', 'summation']"
64,Is there any educational value into reading the original work of the authors who discovered certain theorems or concepts?,Is there any educational value into reading the original work of the authors who discovered certain theorems or concepts?,,"I am interested in reading original work of some authors of theorems or concepts in mathematics because I believe that there is also an educational value to this and it might help me understand better those concepts because I can see where and how they came from from the author's minds. For example I would be really curious into reading the work of Leibnitz and Newton regarding calculus, differentials, and integrals. Do you think it could help me better understand those concepts? If yes, can anyone help me to find those original works?","I am interested in reading original work of some authors of theorems or concepts in mathematics because I believe that there is also an educational value to this and it might help me understand better those concepts because I can see where and how they came from from the author's minds. For example I would be really curious into reading the work of Leibnitz and Newton regarding calculus, differentials, and integrals. Do you think it could help me better understand those concepts? If yes, can anyone help me to find those original works?",,"['calculus', 'analysis', 'derivatives', 'math-history']"
65,A wrong proof for an (evident) lemma,A wrong proof for an (evident) lemma,,"( Eliashberg, Y.; Mishachev, N.M. , Wrinkling of smooth mappings and its applications. I , Invent. Math. 130, No.2, 345-369 (1997). ZBL0896.58010 . \cite{EM}) Let $ \alpha  : [a, b] \to \mathbb{R}$  is a 1-dimensional wrinkle. Then for   any positive numbers $s_1, \dots , s_l $ such that  $ s:= \sum\nolimits_{1}^{l} s_i \ge  {\text{span} \left(\alpha \right)} $, there exists a wrinkled map   $\beta : [a, b] \to \mathbb{R}$ which is $\left( s - \text{span}  \left(\alpha \right) \right)$-close to $\alpha$ in $C^0$-norm,   coincides with $\alpha$ near the end-points of the interval $[a,b],$    and which has exactly $l$ wrinkles whose spans are equal to $s_1,  \dots , s_l$. I prove for the case $l=2$. By hypothesis, $\alpha $ is a cubic function with two critical points $c, d \in  \left[ a, b \right] , a< c<d<b$, such that $c$ is the maximum and $d$ is the minimum.  For convenience, we assume $\alpha (d)=0$. It implies that $\alpha (c) =\text{span}(\alpha)$. Let $\varepsilon := s_1+s_2 -  \text{span} \left(\alpha \right)$.  We have to prove that there is a wrinkled map $\beta : [a, b] \to \mathbb{R}$ such that $\beta$ is $\varepsilon$-close to $\alpha$ in $C^0$-norm, coincides with $\alpha$ near $a, b$, and has $2$ wrinkles $\alpha_1$ and $\alpha_2$ whose spans are equal to $s_1, s_2.$ The required function $\beta$ is a wrinkled map, i.e., by definition, there are disjoint subsets $[a_1, e_1]$ and $[e_2, b_1]$, where $a \le a_1 \le e_1  \le e_2 \le b_1 \le b$, of the interval $[a,b]$ such that over them, $\beta$ are wrinkles, which we denoted $\alpha_1, \alpha_2$; and outside them, $\beta$ is a submersion. For the points $a_1 \le e_1  \le e_2 \le b_1 $ in $[a,b]$ will be chosen later, we can define $\beta$ by $$ \beta := \begin{cases}   \text{a smooth path connected } a \text{ and } a_1 \text{ which is a  submersion};\\  \alpha_1 \text{ is a one-dimensional wrinkle on } [a_1, e_1];\\  \text{a smooth path connected } e_1 \text{ and } e_2 \text{ which is a  submersion};\\   \alpha_2 \text{ is a one-dimensional wrinkle on } [e_2, b_1]; \\  \text{a smooth path connected } b_1 \text{ and } b \text{ which is a  submersion}. \end{cases}$$ The required functions $\alpha_1, \alpha_2$ are cubic, hence it suffices to indicate four independent equations for each $\alpha_i, i=1, 2$. If we suppose $\alpha_1$ reaches maximum at $c$ and $\alpha_2$ reaches minimum at $d$, we would get two equations for each $\alpha_i, i=1,2$.  $$ \begin{cases}   \alpha_1 (c) := t_1;\\  \alpha_1^\prime (c) = 0; \end{cases}  \text{ and }  \begin{cases}  \alpha_2 (d) := t_2;\\  \alpha_2^\prime (d) = 0; \end{cases}  $$ where the values $t_1$ and $ t_2$ will be chosen later, such that the condition that $\beta$ is $\varepsilon$-close to $\alpha$ in $C^0$-norm holds, i.e.,  \begin{equation}\label{chopeq232} \begin{cases} \Vert \alpha - \alpha_1 \Vert_{C^0[a_1, e_1]} = \mathop{\text{max}}\limits_{x \in [a_1, e_1]} \vert \left( \alpha - \alpha_1 \right)(x)  \vert < \varepsilon; \\ \Vert \alpha - \alpha_2 \Vert_{C^0[e_2, b_1]} = \mathop{\text{max}}\limits_{x \in [e_2, b_1]} \vert \left( \alpha - \alpha_2 \right)(x)  \vert < \varepsilon.  \end{cases}  \end{equation}  The point $c$ is the critical point of both functions $\alpha $ and $ \alpha_1$ on $[a_1, e_1]$, so the derivative $$ \left( \alpha - \alpha_1 \right)^\prime (c) = \alpha^\prime (c)  - \alpha_1^\prime (c) = 0.$$  Therefore $c$ is one of two critical points of $\alpha - \alpha_1 $  on $[a_1, e_1]$. We could choose, for example,  $$t_1:= \alpha(c)+ \varepsilon = \text{span}(\alpha)+\varepsilon,$$ then $ \vert \left( \alpha - \alpha_1 \right)(c)  \vert = \varepsilon.$  Similarly $d$ is one of two critical points of $\alpha - \alpha_2 $  on  $[e_2,b_1]$. We could choose,  $$t_2:= \alpha(d) - \varepsilon =  -\varepsilon,$$  then $ \vert \left( \alpha - \alpha_2 \right)(d)  \vert = \varepsilon.$ Now we want to find the other extremum of $\alpha - \alpha_1 $  on $[a_1, e_1]$ and of $\alpha - \alpha_2 $  on  $[e_2,b_2]$. Because the spans of $\alpha_1$ and $\alpha_2$ are equal to $s_1, s_2$, we have one more equation for each wrinkle $\alpha_1$ and $\alpha_2$. $$  \begin{cases} \mathop{\text{min}}\limits_{x \in [a_1, e_1]} \alpha_1 (x) = \mathop{\text{max}}\limits_{x \in [a_1, e_1]} \alpha_1 (x)  - s_1 =  \text{span}(\alpha)+\varepsilon -s_1 =s_2;\\ % \text{span}(\alpha)+ s_1 + s_2  -\text{span}(\alpha)  -s_1 =   \mathop{\text{max}}\limits_{x \in [e_2, b_1]} \alpha_2 (x) = \mathop{\text{min}}\limits_{x \in [e_2, b_1]} \alpha_2 (x) + s_2 = s_2 - \varepsilon.  \end{cases}  $$ We define the minimum value of $\alpha$ on $[a_1, e_1]$ and the maximum of $\alpha$ on $[e_2, b_1]$ so that the condition $\varepsilon$-close to $\alpha$ in $C^0$-norm holds: $$   \mathop{\text{min}}\limits_{x \in [a_1, e_1]} \alpha (x) := s_2 + \varepsilon, \text{ and } \mathop{\text{max}}\limits_{x \in [e_2, b_1]} \alpha (x)  := s_2 - 2 \varepsilon.   $$ Let  $$  \begin{cases} e_0 := \alpha^{-1} \left(s_2 + \varepsilon\right);\\  e_3 := \alpha^{-1} \left(s_2 - 2 \varepsilon\right). \end{cases}  $$ Thus we defined two cubic functions, $f_1$ reaches maximum at $c$ with value $ \text{span}(\alpha) + \varepsilon$ and minimum at $e_0$ with value $s_2$; and $f_2$  reaches maximum at $e_3$ with value $ s_2 - 2 \varepsilon $ and minimum at $d$ with value $-\varepsilon$. The intersections of $f_1$ and $\alpha$ give us two points, which we denoted $(a_1,\alpha_1(a_1))$, $(e_1,\alpha_1(e_1))$; and the intersections of $f_2$ and $\alpha$ give us two other points, which we denoted $(e_2, \alpha_2(e_2))$, $(b_1,\alpha_2(b_1))$. Finally we get the wrinkles $\alpha_1$ and $\alpha_2$. In conclusion, for the given conditions, we could construct a wrinkled map $\beta$ which is $\varepsilon$-closed to $\alpha$ in $C^0$-norm, coincides with $\alpha$ near $a, b$, and has two wrinkles $\alpha_1, \alpha_2$ whose spans are $s_1, s_2$. Unfortunately it is wrong. In fact as the construction, the $C^0$-norm of $\alpha_1-\alpha$ would be greater than $\epsilon$. The point is, I constructed $ e_0 $ to be the point which $\alpha_1$ reaches its local minimum $s_2$ and $\alpha(e_0)=s_2+\epsilon$. But $\alpha_1-\alpha$ does not reach its local minimum at $ e_0$. Vague idea: the equations and inequalities for $\alpha_1$ are: 1)     Maximality at $c$; 2)     Span$(\alpha_1)$ equals $s_1$; 3)     Value at $a$ (or $a_1$); 4)     [Inequality] range of $\alpha_1-\alpha$ is smaller than $\epsilon$. I am really need your help. I only got so far. To specify everything, like the existence of the solution, I don't know... Thanks.","( Eliashberg, Y.; Mishachev, N.M. , Wrinkling of smooth mappings and its applications. I , Invent. Math. 130, No.2, 345-369 (1997). ZBL0896.58010 . \cite{EM}) Let $ \alpha  : [a, b] \to \mathbb{R}$  is a 1-dimensional wrinkle. Then for   any positive numbers $s_1, \dots , s_l $ such that  $ s:= \sum\nolimits_{1}^{l} s_i \ge  {\text{span} \left(\alpha \right)} $, there exists a wrinkled map   $\beta : [a, b] \to \mathbb{R}$ which is $\left( s - \text{span}  \left(\alpha \right) \right)$-close to $\alpha$ in $C^0$-norm,   coincides with $\alpha$ near the end-points of the interval $[a,b],$    and which has exactly $l$ wrinkles whose spans are equal to $s_1,  \dots , s_l$. I prove for the case $l=2$. By hypothesis, $\alpha $ is a cubic function with two critical points $c, d \in  \left[ a, b \right] , a< c<d<b$, such that $c$ is the maximum and $d$ is the minimum.  For convenience, we assume $\alpha (d)=0$. It implies that $\alpha (c) =\text{span}(\alpha)$. Let $\varepsilon := s_1+s_2 -  \text{span} \left(\alpha \right)$.  We have to prove that there is a wrinkled map $\beta : [a, b] \to \mathbb{R}$ such that $\beta$ is $\varepsilon$-close to $\alpha$ in $C^0$-norm, coincides with $\alpha$ near $a, b$, and has $2$ wrinkles $\alpha_1$ and $\alpha_2$ whose spans are equal to $s_1, s_2.$ The required function $\beta$ is a wrinkled map, i.e., by definition, there are disjoint subsets $[a_1, e_1]$ and $[e_2, b_1]$, where $a \le a_1 \le e_1  \le e_2 \le b_1 \le b$, of the interval $[a,b]$ such that over them, $\beta$ are wrinkles, which we denoted $\alpha_1, \alpha_2$; and outside them, $\beta$ is a submersion. For the points $a_1 \le e_1  \le e_2 \le b_1 $ in $[a,b]$ will be chosen later, we can define $\beta$ by $$ \beta := \begin{cases}   \text{a smooth path connected } a \text{ and } a_1 \text{ which is a  submersion};\\  \alpha_1 \text{ is a one-dimensional wrinkle on } [a_1, e_1];\\  \text{a smooth path connected } e_1 \text{ and } e_2 \text{ which is a  submersion};\\   \alpha_2 \text{ is a one-dimensional wrinkle on } [e_2, b_1]; \\  \text{a smooth path connected } b_1 \text{ and } b \text{ which is a  submersion}. \end{cases}$$ The required functions $\alpha_1, \alpha_2$ are cubic, hence it suffices to indicate four independent equations for each $\alpha_i, i=1, 2$. If we suppose $\alpha_1$ reaches maximum at $c$ and $\alpha_2$ reaches minimum at $d$, we would get two equations for each $\alpha_i, i=1,2$.  $$ \begin{cases}   \alpha_1 (c) := t_1;\\  \alpha_1^\prime (c) = 0; \end{cases}  \text{ and }  \begin{cases}  \alpha_2 (d) := t_2;\\  \alpha_2^\prime (d) = 0; \end{cases}  $$ where the values $t_1$ and $ t_2$ will be chosen later, such that the condition that $\beta$ is $\varepsilon$-close to $\alpha$ in $C^0$-norm holds, i.e.,  \begin{equation}\label{chopeq232} \begin{cases} \Vert \alpha - \alpha_1 \Vert_{C^0[a_1, e_1]} = \mathop{\text{max}}\limits_{x \in [a_1, e_1]} \vert \left( \alpha - \alpha_1 \right)(x)  \vert < \varepsilon; \\ \Vert \alpha - \alpha_2 \Vert_{C^0[e_2, b_1]} = \mathop{\text{max}}\limits_{x \in [e_2, b_1]} \vert \left( \alpha - \alpha_2 \right)(x)  \vert < \varepsilon.  \end{cases}  \end{equation}  The point $c$ is the critical point of both functions $\alpha $ and $ \alpha_1$ on $[a_1, e_1]$, so the derivative $$ \left( \alpha - \alpha_1 \right)^\prime (c) = \alpha^\prime (c)  - \alpha_1^\prime (c) = 0.$$  Therefore $c$ is one of two critical points of $\alpha - \alpha_1 $  on $[a_1, e_1]$. We could choose, for example,  $$t_1:= \alpha(c)+ \varepsilon = \text{span}(\alpha)+\varepsilon,$$ then $ \vert \left( \alpha - \alpha_1 \right)(c)  \vert = \varepsilon.$  Similarly $d$ is one of two critical points of $\alpha - \alpha_2 $  on  $[e_2,b_1]$. We could choose,  $$t_2:= \alpha(d) - \varepsilon =  -\varepsilon,$$  then $ \vert \left( \alpha - \alpha_2 \right)(d)  \vert = \varepsilon.$ Now we want to find the other extremum of $\alpha - \alpha_1 $  on $[a_1, e_1]$ and of $\alpha - \alpha_2 $  on  $[e_2,b_2]$. Because the spans of $\alpha_1$ and $\alpha_2$ are equal to $s_1, s_2$, we have one more equation for each wrinkle $\alpha_1$ and $\alpha_2$. $$  \begin{cases} \mathop{\text{min}}\limits_{x \in [a_1, e_1]} \alpha_1 (x) = \mathop{\text{max}}\limits_{x \in [a_1, e_1]} \alpha_1 (x)  - s_1 =  \text{span}(\alpha)+\varepsilon -s_1 =s_2;\\ % \text{span}(\alpha)+ s_1 + s_2  -\text{span}(\alpha)  -s_1 =   \mathop{\text{max}}\limits_{x \in [e_2, b_1]} \alpha_2 (x) = \mathop{\text{min}}\limits_{x \in [e_2, b_1]} \alpha_2 (x) + s_2 = s_2 - \varepsilon.  \end{cases}  $$ We define the minimum value of $\alpha$ on $[a_1, e_1]$ and the maximum of $\alpha$ on $[e_2, b_1]$ so that the condition $\varepsilon$-close to $\alpha$ in $C^0$-norm holds: $$   \mathop{\text{min}}\limits_{x \in [a_1, e_1]} \alpha (x) := s_2 + \varepsilon, \text{ and } \mathop{\text{max}}\limits_{x \in [e_2, b_1]} \alpha (x)  := s_2 - 2 \varepsilon.   $$ Let  $$  \begin{cases} e_0 := \alpha^{-1} \left(s_2 + \varepsilon\right);\\  e_3 := \alpha^{-1} \left(s_2 - 2 \varepsilon\right). \end{cases}  $$ Thus we defined two cubic functions, $f_1$ reaches maximum at $c$ with value $ \text{span}(\alpha) + \varepsilon$ and minimum at $e_0$ with value $s_2$; and $f_2$  reaches maximum at $e_3$ with value $ s_2 - 2 \varepsilon $ and minimum at $d$ with value $-\varepsilon$. The intersections of $f_1$ and $\alpha$ give us two points, which we denoted $(a_1,\alpha_1(a_1))$, $(e_1,\alpha_1(e_1))$; and the intersections of $f_2$ and $\alpha$ give us two other points, which we denoted $(e_2, \alpha_2(e_2))$, $(b_1,\alpha_2(b_1))$. Finally we get the wrinkles $\alpha_1$ and $\alpha_2$. In conclusion, for the given conditions, we could construct a wrinkled map $\beta$ which is $\varepsilon$-closed to $\alpha$ in $C^0$-norm, coincides with $\alpha$ near $a, b$, and has two wrinkles $\alpha_1, \alpha_2$ whose spans are $s_1, s_2$. Unfortunately it is wrong. In fact as the construction, the $C^0$-norm of $\alpha_1-\alpha$ would be greater than $\epsilon$. The point is, I constructed $ e_0 $ to be the point which $\alpha_1$ reaches its local minimum $s_2$ and $\alpha(e_0)=s_2+\epsilon$. But $\alpha_1-\alpha$ does not reach its local minimum at $ e_0$. Vague idea: the equations and inequalities for $\alpha_1$ are: 1)     Maximality at $c$; 2)     Span$(\alpha_1)$ equals $s_1$; 3)     Value at $a$ (or $a_1$); 4)     [Inequality] range of $\alpha_1-\alpha$ is smaller than $\epsilon$. I am really need your help. I only got so far. To specify everything, like the existence of the solution, I don't know... Thanks.",,['calculus']
66,A generalization of the Euler-Mascheroni constant,A generalization of the Euler-Mascheroni constant,,"Let $f:[1,+\infty)\rightarrow \mathbb{R}$ be a differentiable function. We are dealing with the limit of the sequence  $$ f(n)-\sum_{k=1}^nf'(k). $$ If $f=\log$, then it is convergent to $-\gamma$ (where $\gamma$ is the Euler-Mascheroni constant). Now, (a) Are there some criteria for its convergence (by putting some conditions on $f$)? (b) Does anyone know some references (paper, book, etc.) about it?","Let $f:[1,+\infty)\rightarrow \mathbb{R}$ be a differentiable function. We are dealing with the limit of the sequence  $$ f(n)-\sum_{k=1}^nf'(k). $$ If $f=\log$, then it is convergent to $-\gamma$ (where $\gamma$ is the Euler-Mascheroni constant). Now, (a) Are there some criteria for its convergence (by putting some conditions on $f$)? (b) Does anyone know some references (paper, book, etc.) about it?",,"['calculus', 'sequences-and-series', 'limits', 'euler-mascheroni-constant']"
67,How to prove or falsify this inequality?,How to prove or falsify this inequality?,,"In STEP 2014 Paper II Question 2, an inequality is assumed for candidates to attempting the question about the approximation of $\pi $ $$\int_{0}^{\pi } (f(x))^2 dx \le \int_{0}^{\pi } (f'(x))^2 dx $$ Where, $$f(0)=f(\pi )=0$$ It then asked for the construction of functions in the use of approximate $\pi $. The question itself is not difficult at all, but I'm pretty interested in the reason why the inequality works. However, it seems like a fresh high school student is not eligible for it XD and I even got something non-sense. So could anyone help me? Thanks a lot for any hint, guide, or most precisely, proof.","In STEP 2014 Paper II Question 2, an inequality is assumed for candidates to attempting the question about the approximation of $\pi $ $$\int_{0}^{\pi } (f(x))^2 dx \le \int_{0}^{\pi } (f'(x))^2 dx $$ Where, $$f(0)=f(\pi )=0$$ It then asked for the construction of functions in the use of approximate $\pi $. The question itself is not difficult at all, but I'm pretty interested in the reason why the inequality works. However, it seems like a fresh high school student is not eligible for it XD and I even got something non-sense. So could anyone help me? Thanks a lot for any hint, guide, or most precisely, proof.",,"['calculus', 'integration', 'derivatives', 'inequality', 'integral-inequality']"
68,What's infinte sum of the reciprocal of the primorial?,What's infinte sum of the reciprocal of the primorial?,,"$$\sum_{n=1}^\infty \frac{1}{p_n\#} = \frac{1}{2}+\frac{1}{2\times3}+\frac{1}{2\times3\times5}+\dots$$ where $p_n\#$ is the nth Primorial. Does this sum approaches some known value or constant and do they have a name for it? I'm also interested in the value for the alternating series which is $$\sum_{n=1}^\infty \frac{(-1)^{n+1}}{p_n\#} = \frac{1}{2}-\frac{1}{2\times3}+\frac{1}{2\times3\times5}-\dots$$ I have tried finding it in google but nothing seems to pop up. If so I would like to see this calculated to a few decimal places , because I can't find a program to find the an infinite sum base on primorial. Edit: Is there any literature,papers or study of these 2 series and similar to these series ?","$$\sum_{n=1}^\infty \frac{1}{p_n\#} = \frac{1}{2}+\frac{1}{2\times3}+\frac{1}{2\times3\times5}+\dots$$ where $p_n\#$ is the nth Primorial. Does this sum approaches some known value or constant and do they have a name for it? I'm also interested in the value for the alternating series which is $$\sum_{n=1}^\infty \frac{(-1)^{n+1}}{p_n\#} = \frac{1}{2}-\frac{1}{2\times3}+\frac{1}{2\times3\times5}-\dots$$ I have tried finding it in google but nothing seems to pop up. If so I would like to see this calculated to a few decimal places , because I can't find a program to find the an infinite sum base on primorial. Edit: Is there any literature,papers or study of these 2 series and similar to these series ?",,"['calculus', 'sequences-and-series', 'prime-numbers', 'primorial']"
69,definite integral of elliptic integral of first kind,definite integral of elliptic integral of first kind,,"The signal-to-noise ratio of a Hall-effect magnetic sensor is proportional to $$ H(f,p)=\frac{I_1 (f,p)}{\sqrt{KK'(\frac{1-f}{1+f})} \sqrt{KK'(\frac{1-p}{1+p})}} $$ with $KK'(x)=K(x)K'(x)$ and $K'(x)=K(\sqrt{1-x^2}) $ and $$ I_1(f,p)=\int_{\alpha=0}^{\pi/2} \frac{F(\alpha,\frac{1-p}{1+p})d\alpha}{\sqrt{\sin^2\alpha+(\frac{1-f}{1+f})^2\cos^2\alpha}} $$ with the incomplete elliptic integral of first type $F(\alpha,k)=\int_{t=0}^{\alpha}\frac{dt}{\sqrt{1-k^2\sin^2t}}$ and $K(k)=F(\pi/2,k)$. The two parameters $0\le f,p\le 1$ are related to input and output resistances $0\le\lambda_f,\lambda_p\le\infty $ via $$\lambda_f=\frac{2K(f)}{K'(f)}=\frac{K'(\frac{1-f}{1+f})}{K(\frac{1-f}{1+f})} \quad \lambda_p=\frac{K'(p)}{2K(p)}=\frac{K(\frac{1-p}{1+p})}{K'(\frac{1-p}{1+p})}=\frac{2K((\frac{1-\sqrt p}{1+\sqrt p})^2)}{K'((\frac{1-\sqrt p}{1+\sqrt p})^2)}$$ Questions: Q1:  Is it possible to compute $I_1$ in closed form, at least for specific values of $f,p$ (except 0 or 1)? Q2: Is it possible to simplify $I_1$ for the special case of a symmetric Hall-plate with equal input and output resistances $\lambda_f=\lambda_p$, which means $\sqrt p=\frac{1-\sqrt f}{1+\sqrt f}$? Q3: Numerical inspection shows that $H(f,p)$ remains constant if one transforms $(\lambda_f,\lambda_p)\to(2/\lambda_f,2/\lambda_p)$, which means $(f,p)\to(\frac{1-f}{1+f},(\frac{\sqrt{1+p}-\sqrt{2\sqrt{p}}}{\sqrt{1+p}+\sqrt{2\sqrt{p}}})^2)$ I have wrecked my brain for months about these points: fruitlessly. Any properties of $H(f,p)$ or even better $H(\lambda_f,\lambda_p)$, which can be seen in the integral above would be interesting. $H(\lambda_f,\lambda_p)$ in 3D-plot and contour-plot The plot shows that $H(\lambda_f,\lambda_p)$ has a maximum at $\lambda_f=\lambda_p=\sqrt 2$ and you can also see the symmetry in the contour plot (red circles). It becomes also apparent that there are infinitely many points on the iso-lines, for which $H(\lambda_f,\lambda_p)$ is constant. It would be great to have a closed formula that relates $\lambda_f$ versus $\lambda_p$ along an iso-line. For the symmetric case $\lambda_f=\lambda_p$ I have found a fairly accurate and astonishingly simple approximation (error < 2%) $$ H(\lambda_f=\lambda_p) \approx \frac{\lambda_f}{\sqrt{\lambda_f^4+\lambda_f^2/2+4}} $$ The accurate expression for this case can be cast in the form $$ H(\lambda_f=\lambda_p) = \frac{I_2(f)}{K(f)K(\frac{1-f}{1+f})} \quad I_2(f)=\frac{1}{1+f}\int_{\alpha=0}^{\pi/2}\frac{F(\alpha,\frac{2\sqrt f}{1+f})d\alpha}{\sqrt{1-\frac{4 f}{(1+f)^2}\cos^2\alpha}}$$  where $I_2$ is only a weak function of $f$ and $I_2(f)=I_2(\frac{1-f}{1+f})$ (by numerical inspection). $I_2(0)=I_2(1)=\pi^2/8$ and the maximum is $I_2(\sqrt 2-1)=(\sqrt 2/3) K^2(\sqrt{2}-1)$. Using the integral definition of $F(\alpha,k)$ one can rewrite  $$ I_2(f)=\int_{x=0}^{1}\int_{y=0}^{\sqrt{1-x^2}}\frac{(1+f)dy dx}{\sqrt{1-x^2}\sqrt{1-y^2}\sqrt{(1+f)^2-4 f x^2}\sqrt{(1+f)^2-4 f y^2}} $$ Transforming into cylindrical coordinates and computing over the 90 angle gives  $$ I_2(f)=\int_{x=0}^{1} \frac{K(\frac{(1-f)x\sqrt{1+6f+f^2-4fx}}{(1+f)(2-x)\sqrt{1+2f+f^2-4fx}})}{(2-x)\sqrt{1+2f+f^2-4fx}}dx = \int_{z=0}^{1} \frac{K(\frac{1-f}{1+f}\frac{1-z}{1+z}\sqrt{\frac{(1+f)^2+4fz}{(1-f)^2+4fz}})}{(1+z)\sqrt{(1-f)^2+4fz}}dz  $$ where we have only a complete instead of an incomplete elliptic integral - however, I still do not see the symmetry:- The last integral has some similarity to (131) in ""Elliptic integral Evaluations of Bessel moments"", by Bailey,Borwein,Broadhurst,Glasser (free download https://arxiv.org/pdf/0801.0891.pdf ). If we set $\hat f = 2f/(1+f^2)$ then there is a one-to-one relation between $f$ and $\hat f$ in the interval [0,1]. Then the transformation $f\to (1-f)/(1+f)$ means $\hat f\to\sqrt{1-\hat f^2}$ and the conjecture is $I_2(\hat f)=I_2(\sqrt{1-\hat f^2})$. In the general case $\lambda_f \ne \lambda_p$ we set $\hat p = (1-p)^2/(1+6p+p^2)$. Again there is a one-to-one relation between $p$ and $\hat p$ in the interval [0,1]. Then the transformation $p\to (\sqrt{1+p}-\sqrt{2\sqrt{p}})^{2} (\sqrt{1+p}+\sqrt{2\sqrt{p}})^{-2}$ means $\hat p\to\sqrt{1-\hat p^2}$ and the conjecture is $H(\hat f,\hat p)=H(\sqrt{1-\hat f^2}, \sqrt{1-\hat p^2})$. By partial integration one can show that $H$ and $I_1$ remain the same, if $\lambda_f$ and $\lambda_p$ are swapped.","The signal-to-noise ratio of a Hall-effect magnetic sensor is proportional to $$ H(f,p)=\frac{I_1 (f,p)}{\sqrt{KK'(\frac{1-f}{1+f})} \sqrt{KK'(\frac{1-p}{1+p})}} $$ with $KK'(x)=K(x)K'(x)$ and $K'(x)=K(\sqrt{1-x^2}) $ and $$ I_1(f,p)=\int_{\alpha=0}^{\pi/2} \frac{F(\alpha,\frac{1-p}{1+p})d\alpha}{\sqrt{\sin^2\alpha+(\frac{1-f}{1+f})^2\cos^2\alpha}} $$ with the incomplete elliptic integral of first type $F(\alpha,k)=\int_{t=0}^{\alpha}\frac{dt}{\sqrt{1-k^2\sin^2t}}$ and $K(k)=F(\pi/2,k)$. The two parameters $0\le f,p\le 1$ are related to input and output resistances $0\le\lambda_f,\lambda_p\le\infty $ via $$\lambda_f=\frac{2K(f)}{K'(f)}=\frac{K'(\frac{1-f}{1+f})}{K(\frac{1-f}{1+f})} \quad \lambda_p=\frac{K'(p)}{2K(p)}=\frac{K(\frac{1-p}{1+p})}{K'(\frac{1-p}{1+p})}=\frac{2K((\frac{1-\sqrt p}{1+\sqrt p})^2)}{K'((\frac{1-\sqrt p}{1+\sqrt p})^2)}$$ Questions: Q1:  Is it possible to compute $I_1$ in closed form, at least for specific values of $f,p$ (except 0 or 1)? Q2: Is it possible to simplify $I_1$ for the special case of a symmetric Hall-plate with equal input and output resistances $\lambda_f=\lambda_p$, which means $\sqrt p=\frac{1-\sqrt f}{1+\sqrt f}$? Q3: Numerical inspection shows that $H(f,p)$ remains constant if one transforms $(\lambda_f,\lambda_p)\to(2/\lambda_f,2/\lambda_p)$, which means $(f,p)\to(\frac{1-f}{1+f},(\frac{\sqrt{1+p}-\sqrt{2\sqrt{p}}}{\sqrt{1+p}+\sqrt{2\sqrt{p}}})^2)$ I have wrecked my brain for months about these points: fruitlessly. Any properties of $H(f,p)$ or even better $H(\lambda_f,\lambda_p)$, which can be seen in the integral above would be interesting. $H(\lambda_f,\lambda_p)$ in 3D-plot and contour-plot The plot shows that $H(\lambda_f,\lambda_p)$ has a maximum at $\lambda_f=\lambda_p=\sqrt 2$ and you can also see the symmetry in the contour plot (red circles). It becomes also apparent that there are infinitely many points on the iso-lines, for which $H(\lambda_f,\lambda_p)$ is constant. It would be great to have a closed formula that relates $\lambda_f$ versus $\lambda_p$ along an iso-line. For the symmetric case $\lambda_f=\lambda_p$ I have found a fairly accurate and astonishingly simple approximation (error < 2%) $$ H(\lambda_f=\lambda_p) \approx \frac{\lambda_f}{\sqrt{\lambda_f^4+\lambda_f^2/2+4}} $$ The accurate expression for this case can be cast in the form $$ H(\lambda_f=\lambda_p) = \frac{I_2(f)}{K(f)K(\frac{1-f}{1+f})} \quad I_2(f)=\frac{1}{1+f}\int_{\alpha=0}^{\pi/2}\frac{F(\alpha,\frac{2\sqrt f}{1+f})d\alpha}{\sqrt{1-\frac{4 f}{(1+f)^2}\cos^2\alpha}}$$  where $I_2$ is only a weak function of $f$ and $I_2(f)=I_2(\frac{1-f}{1+f})$ (by numerical inspection). $I_2(0)=I_2(1)=\pi^2/8$ and the maximum is $I_2(\sqrt 2-1)=(\sqrt 2/3) K^2(\sqrt{2}-1)$. Using the integral definition of $F(\alpha,k)$ one can rewrite  $$ I_2(f)=\int_{x=0}^{1}\int_{y=0}^{\sqrt{1-x^2}}\frac{(1+f)dy dx}{\sqrt{1-x^2}\sqrt{1-y^2}\sqrt{(1+f)^2-4 f x^2}\sqrt{(1+f)^2-4 f y^2}} $$ Transforming into cylindrical coordinates and computing over the 90 angle gives  $$ I_2(f)=\int_{x=0}^{1} \frac{K(\frac{(1-f)x\sqrt{1+6f+f^2-4fx}}{(1+f)(2-x)\sqrt{1+2f+f^2-4fx}})}{(2-x)\sqrt{1+2f+f^2-4fx}}dx = \int_{z=0}^{1} \frac{K(\frac{1-f}{1+f}\frac{1-z}{1+z}\sqrt{\frac{(1+f)^2+4fz}{(1-f)^2+4fz}})}{(1+z)\sqrt{(1-f)^2+4fz}}dz  $$ where we have only a complete instead of an incomplete elliptic integral - however, I still do not see the symmetry:- The last integral has some similarity to (131) in ""Elliptic integral Evaluations of Bessel moments"", by Bailey,Borwein,Broadhurst,Glasser (free download https://arxiv.org/pdf/0801.0891.pdf ). If we set $\hat f = 2f/(1+f^2)$ then there is a one-to-one relation between $f$ and $\hat f$ in the interval [0,1]. Then the transformation $f\to (1-f)/(1+f)$ means $\hat f\to\sqrt{1-\hat f^2}$ and the conjecture is $I_2(\hat f)=I_2(\sqrt{1-\hat f^2})$. In the general case $\lambda_f \ne \lambda_p$ we set $\hat p = (1-p)^2/(1+6p+p^2)$. Again there is a one-to-one relation between $p$ and $\hat p$ in the interval [0,1]. Then the transformation $p\to (\sqrt{1+p}-\sqrt{2\sqrt{p}})^{2} (\sqrt{1+p}+\sqrt{2\sqrt{p}})^{-2}$ means $\hat p\to\sqrt{1-\hat p^2}$ and the conjecture is $H(\hat f,\hat p)=H(\sqrt{1-\hat f^2}, \sqrt{1-\hat p^2})$. By partial integration one can show that $H$ and $I_1$ remain the same, if $\lambda_f$ and $\lambda_p$ are swapped.",,"['calculus', 'integration', 'definite-integrals', 'special-functions', 'elliptic-integrals']"
70,Numerical integration of $\int_0^{2\pi}\frac{dx}{3+\sin x}$,Numerical integration of,\int_0^{2\pi}\frac{dx}{3+\sin x},"We know that numerical integration using end points is first order, trapezoid and mid-piont rules second order, and Simpson's rule fourth order. However, with $\int_0^{2\pi}\frac{dx}{3+\sin x}$, we have the following errors: n   Left point              Right point             Trapezoid               Mid-point               Simpson 1   2.02200572599407E-2     2.02200572599406E-2     2.02200572599406E-2     2.02200572599407E-2     2.02200572599407E-2 2   2.02200572599407E-2     2.02200572599406E-2     2.02200572599406E-2     -2.14466094067260E-2    -7.55772051783715E-3 3   1.80370579205280E-5     1.80370579205280E-5     1.80370579205280E-5     1.80370579205280E-5     1.80370579204725E-5 4   -6.13276073392621E-4    -6.13276073392621E-4    -6.13276073392621E-4    6.12214122685750E-4     2.03717390659608E-4 5   1.56304432175070E-8     1.56304432175070E-8     1.56304432175070E-8     1.56304432175070E-8     1.56304432175070E-8 6   1.80370579205280E-5     1.80370579205280E-5     1.80370579205280E-5     -1.80379781545281E-5    -6.01296612956492E-6 7   1.35448319227294E-11    1.35448319227294E-11    1.35448319227294E-11    1.35449429450318E-11    1.35449429450318E-11 8   -5.30975353407737E-7    -5.30975353463248E-7    -5.30975353407737E-7    5.30974556545161E-7     1.76991253264536E-7 9   1.20459198171829E-14    1.19904086659517E-14    1.20459198171829E-14    1.19904086659517E-14    1.20459198171829E-14 10  1.56304431619958E-8     1.56304431619958E-8     1.56304431619958E-8     -1.56304434395516E-8    -5.21014786869500E-9 Simpson is surprised. Why are the end points over-performing? Another interesting fact is that the errors are not converging steadily but rather in an oscillating manner. Is there any explanation for this?","We know that numerical integration using end points is first order, trapezoid and mid-piont rules second order, and Simpson's rule fourth order. However, with $\int_0^{2\pi}\frac{dx}{3+\sin x}$, we have the following errors: n   Left point              Right point             Trapezoid               Mid-point               Simpson 1   2.02200572599407E-2     2.02200572599406E-2     2.02200572599406E-2     2.02200572599407E-2     2.02200572599407E-2 2   2.02200572599407E-2     2.02200572599406E-2     2.02200572599406E-2     -2.14466094067260E-2    -7.55772051783715E-3 3   1.80370579205280E-5     1.80370579205280E-5     1.80370579205280E-5     1.80370579205280E-5     1.80370579204725E-5 4   -6.13276073392621E-4    -6.13276073392621E-4    -6.13276073392621E-4    6.12214122685750E-4     2.03717390659608E-4 5   1.56304432175070E-8     1.56304432175070E-8     1.56304432175070E-8     1.56304432175070E-8     1.56304432175070E-8 6   1.80370579205280E-5     1.80370579205280E-5     1.80370579205280E-5     -1.80379781545281E-5    -6.01296612956492E-6 7   1.35448319227294E-11    1.35448319227294E-11    1.35448319227294E-11    1.35449429450318E-11    1.35449429450318E-11 8   -5.30975353407737E-7    -5.30975353463248E-7    -5.30975353407737E-7    5.30974556545161E-7     1.76991253264536E-7 9   1.20459198171829E-14    1.19904086659517E-14    1.20459198171829E-14    1.19904086659517E-14    1.20459198171829E-14 10  1.56304431619958E-8     1.56304431619958E-8     1.56304431619958E-8     -1.56304434395516E-8    -5.21014786869500E-9 Simpson is surprised. Why are the end points over-performing? Another interesting fact is that the errors are not converging steadily but rather in an oscillating manner. Is there any explanation for this?",,"['calculus', 'integration', 'numerical-methods']"
71,A calculus limit problem,A calculus limit problem,,"The problem is: $\lim_{x\to0}$ $\frac{sin(\frac{1}{x})}{sin(\frac{1}{sin(x)})}$ my intuition tells that the answer equal to $1$ by the limit equality : $\lim_{x\to0}$ $\frac{sinx}{x}=1$. But we can easily see that the limit of numerator and denominator both don't exist, and they are both between -1 and 1. I cannot seem to find a rigorous argument for this problem. If any one can help or give some hint would be very much appreciated!","The problem is: $\lim_{x\to0}$ $\frac{sin(\frac{1}{x})}{sin(\frac{1}{sin(x)})}$ my intuition tells that the answer equal to $1$ by the limit equality : $\lim_{x\to0}$ $\frac{sinx}{x}=1$. But we can easily see that the limit of numerator and denominator both don't exist, and they are both between -1 and 1. I cannot seem to find a rigorous argument for this problem. If any one can help or give some hint would be very much appreciated!",,['calculus']
72,How to compute this integral $\int_0^1\frac{\ln x}{x^2-x-1}dx$?,How to compute this integral ?,\int_0^1\frac{\ln x}{x^2-x-1}dx,"How to compute  this integral $\int_0^1\frac{\ln x}{x^2-x-1}dx$? One method I have thought before is to solve the equation $x^2-x-1=0$and thus get its roots $x_{1,2}=\frac{\sqrt{5}\pm 1}{2}$,let $\phi=\frac{\sqrt{5}+ 1}{2}$. Using the formula $$\int \frac{\ln x}{x-a}=\ln x\ln(1-\frac{x}{a})+{L_i}_2(\frac{x}{a}),$$ we get $$\int_0^1\frac{\ln x}{x^2-x-1}dx =\frac1{\sqrt{5}}({L_i}_2(\frac1\phi)-{L_i}_2(-\phi)).$$ By reference to the following results  $${L_i}_2(\frac1\phi)=\frac1{10}\pi^2-\ln ^2\phi,{L_i}_2(-\phi)=-\frac1{10}\pi^2-\ln ^2\phi,$$ we can get $$\int_0^1\frac{\ln x}{x^2-x-1}dx=\frac{\pi^2}{5\sqrt{5}}.$$ This method relies on the property of Polylogarithm(see https://en.wikipedia.org/wiki/Polylogarithm ),  and it looks dull.Can someone give any ""cleverer""method which is not ralated to Polylogarithm?Many thanks in advance.","How to compute  this integral $\int_0^1\frac{\ln x}{x^2-x-1}dx$? One method I have thought before is to solve the equation $x^2-x-1=0$and thus get its roots $x_{1,2}=\frac{\sqrt{5}\pm 1}{2}$,let $\phi=\frac{\sqrt{5}+ 1}{2}$. Using the formula $$\int \frac{\ln x}{x-a}=\ln x\ln(1-\frac{x}{a})+{L_i}_2(\frac{x}{a}),$$ we get $$\int_0^1\frac{\ln x}{x^2-x-1}dx =\frac1{\sqrt{5}}({L_i}_2(\frac1\phi)-{L_i}_2(-\phi)).$$ By reference to the following results  $${L_i}_2(\frac1\phi)=\frac1{10}\pi^2-\ln ^2\phi,{L_i}_2(-\phi)=-\frac1{10}\pi^2-\ln ^2\phi,$$ we can get $$\int_0^1\frac{\ln x}{x^2-x-1}dx=\frac{\pi^2}{5\sqrt{5}}.$$ This method relies on the property of Polylogarithm(see https://en.wikipedia.org/wiki/Polylogarithm ),  and it looks dull.Can someone give any ""cleverer""method which is not ralated to Polylogarithm?Many thanks in advance.",,"['calculus', 'integration']"
73,"Prove that, $f'(0) \ge -\sqrt{2}$ for a function $f$ satisfying some conditions on $(-1,1)$. [duplicate]","Prove that,  for a function  satisfying some conditions on . [duplicate]","f'(0) \ge -\sqrt{2} f (-1,1)","This question already has answers here : If $f$ is twice differentiable and satisfies the following constraints, prove that $f'(0)\geq-\sqrt 2$. (2 answers) Closed 1 year ago . Let $f:(-1,1)\to \mathbb{R}$ be a twice differentiable function such that, $f(0)=1$, $f'(x)0$, $f(x)0$ and $f''(x)f(x)$ for all $x0$. Prove that, $f'(0)-\sqrt2$ Progress: I was able to prove $f'(0)-2$. For this I have applied Cauchy MVT for $f'$ and $g$ on $[0,x]$ for any $x\in (0,1)$ where $g: [0,x]\to\mathbb{R}$ is defined by, $g(x)=\sqrt{x+1}$. But, couldn't find another approach to get $-\sqrt{2}$.","This question already has answers here : If $f$ is twice differentiable and satisfies the following constraints, prove that $f'(0)\geq-\sqrt 2$. (2 answers) Closed 1 year ago . Let $f:(-1,1)\to \mathbb{R}$ be a twice differentiable function such that, $f(0)=1$, $f'(x)0$, $f(x)0$ and $f''(x)f(x)$ for all $x0$. Prove that, $f'(0)-\sqrt2$ Progress: I was able to prove $f'(0)-2$. For this I have applied Cauchy MVT for $f'$ and $g$ on $[0,x]$ for any $x\in (0,1)$ where $g: [0,x]\to\mathbb{R}$ is defined by, $g(x)=\sqrt{x+1}$. But, couldn't find another approach to get $-\sqrt{2}$.",,"['calculus', 'inequality', 'derivatives']"
74,Separable Differential Equation (Check Answer),Separable Differential Equation (Check Answer),,"Question: Determine all differentiable functions in the form $y$ = $f(x)$ which have the properties: $f'(x)$ = $(f(x))^3$ and $f(0)=2$ What I have done I set up the differential equation as such $$ \frac{dy}{dx} = y^3 $$ $$ \int y^{-3} dy = \int dx$$ $$ {-1\over 2y^2} = x + c $$ $$ {1\over 2y^2} = -x-c $$ At $y = 2 , x = 0 , c = -\frac{1}{8}$ $$ {1\over 2y^2} = {1\over 8} - x$$ $$ 2y^2 = {8\over {1-8x}} $$ $$ y^2  = {4\over {1-8x}} $$ $$ y = \sqrt{4\over {1-8x}} $$ Is this correct? Are there any limits of x or y to consider?",Question: Determine all differentiable functions in the form = which have the properties: = and What I have done I set up the differential equation as such At Is this correct? Are there any limits of x or y to consider?,"y f(x) f'(x) (f(x))^3 f(0)=2  \frac{dy}{dx} = y^3   \int y^{-3} dy = \int dx  {-1\over 2y^2} = x + c   {1\over 2y^2} = -x-c  y = 2 , x = 0 , c = -\frac{1}{8}  {1\over 2y^2} = {1\over 8} - x  2y^2 = {8\over {1-8x}}   y^2  = {4\over {1-8x}}   y = \sqrt{4\over {1-8x}} ","['calculus', 'integration', 'ordinary-differential-equations']"
75,How do I proceed Calculating. $\int_0^{\frac{\pi}{2}} \arccos ( \frac{\cos x}{1+2 \cos x}) dx$ [closed],How do I proceed Calculating.  [closed],\int_0^{\frac{\pi}{2}} \arccos ( \frac{\cos x}{1+2 \cos x}) dx,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question How do I proceed to integrate  $$ \int_0^{\frac{\pi}{2}} \arccos \left( \frac{\cos x}{1+2 \cos x}\right) dx $$ The question is complete..looking forward for any help.....","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question How do I proceed to integrate  $$ \int_0^{\frac{\pi}{2}} \arccos \left( \frac{\cos x}{1+2 \cos x}\right) dx $$ The question is complete..looking forward for any help.....",,"['calculus', 'integration', 'definite-integrals']"
76,Questions concerning the Integration of Integer Tetration,Questions concerning the Integration of Integer Tetration,,"I've been interested in finding the antiderivative of integer tetration, a function defined as iterative exponentiation. Integer tetration is written as $^n$$x$ where $^1$$x =x$, $^2$$x =x^x$, $^3$$x =$ $x^{\scriptscriptstyle x^{x}}$ and so forth where $n=1,2,3\ldots$ (Further info can be found on Wikipedia . One solution to the problem is found on the MathWorld page (see equation #10), but the given solution is difficult to evaluate; accordingly, I am searching for an simpler representation than the result given. (Note that my questions are located at the bottom of this post, and number theorists are encouraged to skip to that point... everything leading up to the questions is background and context.) Using Wolfram Alpha I found the Puiseux series of $x^x$ to be the following:$$1+ x\log(x)+\frac{1}{2}x^2\log^2(x) + \frac{1}{6}x^3\log^3(x)\ldots$$ $$ = \sum_{n=0}^{\infty} \frac{x^n \log^n(x)}{n!} \qquad 1.$$  As such, the antiderivative can be found simply by integrating the series: $$\int x^xdx = \int \bigg(\sum_{n=0}^{\infty} \frac{x^n \log^n(x)}{n!}\bigg)dx = \sum_{n=0}^{\infty}\bigg(\frac{1}{n!}\int x^n \log^n(x)dx\bigg)$$ We then note the following: (again, pulled from Wolfram Alpha) $$\int x^n \log^n(x)dx = \frac{\Gamma(n+1,-(n+1)\log(x))(-n-1)^{-n} }{(n+1)} \qquad 2. $$ All that is left is to substitute in Equation 2 and convert the denominator the gamma function. $$\sum_{n=0}^{\infty}\bigg(\frac{1}{n!}\int x^n \log^n(x)dx\bigg) = \sum_{n=0}^{\infty}\bigg(\frac{\Gamma(n+1,-(n+1)\log(x))(-n-1)^{-n} }{\Gamma(n+2)}\bigg)$$ This solution can be checked by substituting in x=1, yielding the Sophomore's Dream : $$ \sum_{n=0}^{\infty}\bigg(\frac{\Gamma(n+1,-(n+1)\log(1))(-n-1)^{-n} }{\Gamma(n+2)}\bigg) = \sum_{n=0}^{\infty}\bigg(\frac{\Gamma(n+1,0)(-n-1)^{-n} }{\Gamma(n+2)}\bigg)$$ $$= \sum_{n=0}^{\infty}\bigg(\frac{\Gamma(n+1)(-n-1)^{-n} }{\Gamma(n+2)}\bigg) = \sum_{n=0}^{\infty}\bigg((-1)^n(n+1)^{n+1}\bigg)$$ $$= 1-\frac{1}{4}+\frac{1}{27}-\frac{1}{64}\,\ldots \, = -\sum_{n=1}^{\infty}(-n)^{-n} = \int_{0}^{1}x^xdx$$ Further, any other definite integral of $x^x$ can be calculated the same way. (Note that $F'(x)$ is $x^x$) $$\int_{0}^{r}x^xdx = F(r) - \lim_{a\to 0}F(a)$$ $$= F(r) - \sum_{n=0}^{\infty}\lim_{a\to 0}\bigg(\frac{\Gamma(n+1,-(n+1)\log(a))(-n-1)^{-n} }{\Gamma(n+2)}\bigg) = F(r)$$ No standard Puiseux series can be found for $^n$$x$ when $n>2$, although Wolfram Alpha tells me that ""Generalized Puiseux series"" exists for these functions. For $^3$$x$ the series is as such: $$x + x^2\log^2(x) + \frac{1}{2}x^3\log^3(x)[1+\log(x)] + \frac{1}{6}x^4\log^4(x)[1+3\log(x)+\log^2(x)]\ldots$$ Notice that this series closely resembles Equation 1, except for the additional powers of $\log(x)$. If we temporarily exclude the coefficients on these additional terms we find that the $n^{th}$ term of the series is multiplied by $\sum_{n=0}^{n-2}\log^n(x)$ (except for the first term). As a result, we can represent the series as follow, where the additional coefficients are represented as $C_{kn}$ $$\sum_{n=2}^{\infty}\bigg[\frac{x^n\log^n(x)}{\Gamma(n)}\sum_{k=0}^{n-2}\bigg(C_{kn}\log^k(x)\bigg)\bigg] = \sum_{n=2}^{\infty}\sum_{k=0}^{n-2}\bigg(\frac{C_{kn}\log^{k+n}(x)x^n}{\Gamma(n)}\bigg)  \qquad 3. $$ Listing the coefficients generated, we get $[1],[1,1],[1,3,1],[1,7,6,1]\ldots$ A quick search on the OEIS revealed that these are in fact the Stirling numbers of the second kind, written as $S_n^k$ or $S(n,k)$. Substituting this into the series representation above, we get the following series for $^3$$x$: $$x + \sum_{n=2}^{\infty}\sum_{k=0}^{n-2}\bigg(\frac{S_{n-1}^{k+1}\log^{k+n}(x)x^n}{\Gamma(n)}\bigg) = x + \sum_{n=0}^{\infty}\sum_{k=0}^{n}\bigg(\frac{S_{n+1}^{k+1}\log^{k+n+2}(x)x^{n+2}}{\Gamma(n+2)}\bigg)$$ Going through a similar process to the one outlined for $x^x$, $\int$ $^3$$xdx$ can be shown to be the following equation: $$\frac{x^2}{2} + \sum_{n=0}^{\infty}\sum_{k=0}^{n}\bigg(\frac{\Gamma[n+3+k,-(n+3)\log(x)]S_{n+1}^{k+1}}{\Gamma(n+2)(-(n+3))^{n+3+k}}\bigg)$$ Likewise, we find that $$\int_{0}^{r} {^3x}dx = \frac{r^2}{2} + \sum_{n=0}^{\infty}\sum_{k=0}^{n}\bigg(\frac{\Gamma[n+3+k,-(n+3)\log(r)]S_{n+1}^{k+1}}{\Gamma(n+2)(-(n+3))^{n+3+k}}\bigg)$$ The hard part is when we get to the generalized Puiseux series for $^4x$, which is: $$1+x\log(x)+\frac{1}{2}x^2\log^2(x)[1+2\log(x)]+\frac{1}{6}x^3 \log^3(x) [1+9 \log(x)+3\log^2(x)]+\ldots$$ $$***$$ Here is where the real questions start. This series has the first few powers of $x\log(x)$ as $x^x$, namely $1+x\log(x)$. I wonder if this is related to the fact that the function $y = (x^x)^y$ is the same as $y = (^{2n}x)^y$? Equivalently, $y=x^y$ is the same as $y = (^{2n-1}x)^y$. We can see this same symmetry in that for all $^{2n-1}x$ I checked (up to $^7x$) the first two terms are $x+x^2\log^2(x)$, while for all $^{2n}x$ I checked the first two terms are $1+x\log(x)$. Thus, my first question is somewhat broad... Is there any simple explanation for this symmetry ? My next question concerns the series for $^4x$. We can represent this in a very similar form to Equation 1 , except for the additional powers of $\log(x)$, reminiscent of the additions to Equation 1 . Thus, we get the form $$\sum_{n=0}^{\infty} \bigg[\frac{x^n \log^n(x)}{n!}\sum_{k=0}^{n-1}\bigg(C_{kn}\log^k(x)\bigg)\bigg] = 1 + \sum_{n=1}^{\infty} \bigg[\frac{x^n \log^n(x)}{n!}\sum_{k=0}^{n-1}\bigg(C_{kn}\log^k(x)\bigg)\bigg]$$ $$= 1 + \sum_{n=1}^{\infty}\sum_{k=0}^{n-1}\bigg(\frac{x^n \log^{n+k}(x)C_{kn}}{n!}\bigg) = 1 + \sum_{n=0}^{\infty}\sum_{k=0}^{n}\bigg(\frac{x^{n+1} \log^{n+1+k}(x)C_{kn}}{\Gamma(n+2)}\bigg)$$ Having already worked with $^2x$ and $^3x$ integrating this series would not be difficult, but I can't figure out what the constants are. Thus, my main question is: What is the pattern for the numbers $[1,2],[1,9,3],[1,28,36,4],[1,75,245,110,5],[1,186,1290,1410,300,6]...$ where each number is some $C_{kn}$? I have noticed that the first member of each set is 1, and the second term in each set is $nS_n^2$ (where the first set is n=2, the second is n=3, and so forth and $S_n^k$ are the Stirling numbers of the second kind). The nth set also contains n members (using the definition of n above), and the final member of each set is n. Other than these observations, I can't find a way to represent these numbers. My final question is ""How do we calculate a generalized Puiseux series, specifically for $^nx$ where $n>2$? I feel that if I had more knowledge on how these series are calculated I could gain insight into my other questions, but I cannot find much literature on finding generalized Puiseux series, let alone literature elementary enough for me to understand and apply to tetration. (Note: If I should split this post into three seperate posts for each question I can, but I included them all here to avoid posting excessively. If I should edit my tags to reach a wider audience I will do so, although I am not sure what specific fields of mathematics my questions fall under)","I've been interested in finding the antiderivative of integer tetration, a function defined as iterative exponentiation. Integer tetration is written as $^n$$x$ where $^1$$x =x$, $^2$$x =x^x$, $^3$$x =$ $x^{\scriptscriptstyle x^{x}}$ and so forth where $n=1,2,3\ldots$ (Further info can be found on Wikipedia . One solution to the problem is found on the MathWorld page (see equation #10), but the given solution is difficult to evaluate; accordingly, I am searching for an simpler representation than the result given. (Note that my questions are located at the bottom of this post, and number theorists are encouraged to skip to that point... everything leading up to the questions is background and context.) Using Wolfram Alpha I found the Puiseux series of $x^x$ to be the following:$$1+ x\log(x)+\frac{1}{2}x^2\log^2(x) + \frac{1}{6}x^3\log^3(x)\ldots$$ $$ = \sum_{n=0}^{\infty} \frac{x^n \log^n(x)}{n!} \qquad 1.$$  As such, the antiderivative can be found simply by integrating the series: $$\int x^xdx = \int \bigg(\sum_{n=0}^{\infty} \frac{x^n \log^n(x)}{n!}\bigg)dx = \sum_{n=0}^{\infty}\bigg(\frac{1}{n!}\int x^n \log^n(x)dx\bigg)$$ We then note the following: (again, pulled from Wolfram Alpha) $$\int x^n \log^n(x)dx = \frac{\Gamma(n+1,-(n+1)\log(x))(-n-1)^{-n} }{(n+1)} \qquad 2. $$ All that is left is to substitute in Equation 2 and convert the denominator the gamma function. $$\sum_{n=0}^{\infty}\bigg(\frac{1}{n!}\int x^n \log^n(x)dx\bigg) = \sum_{n=0}^{\infty}\bigg(\frac{\Gamma(n+1,-(n+1)\log(x))(-n-1)^{-n} }{\Gamma(n+2)}\bigg)$$ This solution can be checked by substituting in x=1, yielding the Sophomore's Dream : $$ \sum_{n=0}^{\infty}\bigg(\frac{\Gamma(n+1,-(n+1)\log(1))(-n-1)^{-n} }{\Gamma(n+2)}\bigg) = \sum_{n=0}^{\infty}\bigg(\frac{\Gamma(n+1,0)(-n-1)^{-n} }{\Gamma(n+2)}\bigg)$$ $$= \sum_{n=0}^{\infty}\bigg(\frac{\Gamma(n+1)(-n-1)^{-n} }{\Gamma(n+2)}\bigg) = \sum_{n=0}^{\infty}\bigg((-1)^n(n+1)^{n+1}\bigg)$$ $$= 1-\frac{1}{4}+\frac{1}{27}-\frac{1}{64}\,\ldots \, = -\sum_{n=1}^{\infty}(-n)^{-n} = \int_{0}^{1}x^xdx$$ Further, any other definite integral of $x^x$ can be calculated the same way. (Note that $F'(x)$ is $x^x$) $$\int_{0}^{r}x^xdx = F(r) - \lim_{a\to 0}F(a)$$ $$= F(r) - \sum_{n=0}^{\infty}\lim_{a\to 0}\bigg(\frac{\Gamma(n+1,-(n+1)\log(a))(-n-1)^{-n} }{\Gamma(n+2)}\bigg) = F(r)$$ No standard Puiseux series can be found for $^n$$x$ when $n>2$, although Wolfram Alpha tells me that ""Generalized Puiseux series"" exists for these functions. For $^3$$x$ the series is as such: $$x + x^2\log^2(x) + \frac{1}{2}x^3\log^3(x)[1+\log(x)] + \frac{1}{6}x^4\log^4(x)[1+3\log(x)+\log^2(x)]\ldots$$ Notice that this series closely resembles Equation 1, except for the additional powers of $\log(x)$. If we temporarily exclude the coefficients on these additional terms we find that the $n^{th}$ term of the series is multiplied by $\sum_{n=0}^{n-2}\log^n(x)$ (except for the first term). As a result, we can represent the series as follow, where the additional coefficients are represented as $C_{kn}$ $$\sum_{n=2}^{\infty}\bigg[\frac{x^n\log^n(x)}{\Gamma(n)}\sum_{k=0}^{n-2}\bigg(C_{kn}\log^k(x)\bigg)\bigg] = \sum_{n=2}^{\infty}\sum_{k=0}^{n-2}\bigg(\frac{C_{kn}\log^{k+n}(x)x^n}{\Gamma(n)}\bigg)  \qquad 3. $$ Listing the coefficients generated, we get $[1],[1,1],[1,3,1],[1,7,6,1]\ldots$ A quick search on the OEIS revealed that these are in fact the Stirling numbers of the second kind, written as $S_n^k$ or $S(n,k)$. Substituting this into the series representation above, we get the following series for $^3$$x$: $$x + \sum_{n=2}^{\infty}\sum_{k=0}^{n-2}\bigg(\frac{S_{n-1}^{k+1}\log^{k+n}(x)x^n}{\Gamma(n)}\bigg) = x + \sum_{n=0}^{\infty}\sum_{k=0}^{n}\bigg(\frac{S_{n+1}^{k+1}\log^{k+n+2}(x)x^{n+2}}{\Gamma(n+2)}\bigg)$$ Going through a similar process to the one outlined for $x^x$, $\int$ $^3$$xdx$ can be shown to be the following equation: $$\frac{x^2}{2} + \sum_{n=0}^{\infty}\sum_{k=0}^{n}\bigg(\frac{\Gamma[n+3+k,-(n+3)\log(x)]S_{n+1}^{k+1}}{\Gamma(n+2)(-(n+3))^{n+3+k}}\bigg)$$ Likewise, we find that $$\int_{0}^{r} {^3x}dx = \frac{r^2}{2} + \sum_{n=0}^{\infty}\sum_{k=0}^{n}\bigg(\frac{\Gamma[n+3+k,-(n+3)\log(r)]S_{n+1}^{k+1}}{\Gamma(n+2)(-(n+3))^{n+3+k}}\bigg)$$ The hard part is when we get to the generalized Puiseux series for $^4x$, which is: $$1+x\log(x)+\frac{1}{2}x^2\log^2(x)[1+2\log(x)]+\frac{1}{6}x^3 \log^3(x) [1+9 \log(x)+3\log^2(x)]+\ldots$$ $$***$$ Here is where the real questions start. This series has the first few powers of $x\log(x)$ as $x^x$, namely $1+x\log(x)$. I wonder if this is related to the fact that the function $y = (x^x)^y$ is the same as $y = (^{2n}x)^y$? Equivalently, $y=x^y$ is the same as $y = (^{2n-1}x)^y$. We can see this same symmetry in that for all $^{2n-1}x$ I checked (up to $^7x$) the first two terms are $x+x^2\log^2(x)$, while for all $^{2n}x$ I checked the first two terms are $1+x\log(x)$. Thus, my first question is somewhat broad... Is there any simple explanation for this symmetry ? My next question concerns the series for $^4x$. We can represent this in a very similar form to Equation 1 , except for the additional powers of $\log(x)$, reminiscent of the additions to Equation 1 . Thus, we get the form $$\sum_{n=0}^{\infty} \bigg[\frac{x^n \log^n(x)}{n!}\sum_{k=0}^{n-1}\bigg(C_{kn}\log^k(x)\bigg)\bigg] = 1 + \sum_{n=1}^{\infty} \bigg[\frac{x^n \log^n(x)}{n!}\sum_{k=0}^{n-1}\bigg(C_{kn}\log^k(x)\bigg)\bigg]$$ $$= 1 + \sum_{n=1}^{\infty}\sum_{k=0}^{n-1}\bigg(\frac{x^n \log^{n+k}(x)C_{kn}}{n!}\bigg) = 1 + \sum_{n=0}^{\infty}\sum_{k=0}^{n}\bigg(\frac{x^{n+1} \log^{n+1+k}(x)C_{kn}}{\Gamma(n+2)}\bigg)$$ Having already worked with $^2x$ and $^3x$ integrating this series would not be difficult, but I can't figure out what the constants are. Thus, my main question is: What is the pattern for the numbers $[1,2],[1,9,3],[1,28,36,4],[1,75,245,110,5],[1,186,1290,1410,300,6]...$ where each number is some $C_{kn}$? I have noticed that the first member of each set is 1, and the second term in each set is $nS_n^2$ (where the first set is n=2, the second is n=3, and so forth and $S_n^k$ are the Stirling numbers of the second kind). The nth set also contains n members (using the definition of n above), and the final member of each set is n. Other than these observations, I can't find a way to represent these numbers. My final question is ""How do we calculate a generalized Puiseux series, specifically for $^nx$ where $n>2$? I feel that if I had more knowledge on how these series are calculated I could gain insight into my other questions, but I cannot find much literature on finding generalized Puiseux series, let alone literature elementary enough for me to understand and apply to tetration. (Note: If I should split this post into three seperate posts for each question I can, but I included them all here to avoid posting excessively. If I should edit my tags to reach a wider audience I will do so, although I am not sure what specific fields of mathematics my questions fall under)",,"['calculus', 'sequences-and-series', 'number-theory', 'tetration']"
77,Infinite Integration in Limits of Integration,Infinite Integration in Limits of Integration,,"Given the following: $$ u_0 = \int \limits_{ 0 } ^{ 1 } x \, dx , \:\:\: u_1 = \int \limits^{ \int \limits_{ 1/2 } ^{ 1 } x \, dx } _{ \int \limits_{ 0 } ^{ 1/2 } x \, dx } x \,dx , \:\:\: u_2 = \int \limits_{ { \int \limits_{{\int \limits _{0} ^{1/4} x \, dx}} ^{{\int \limits_{1/4} ^{2/4} x \, dx}} x \, dx}} ^{{\int \limits_{{\int \limits_{2/4} ^{3/4} x \, dx}} ^{{\int \limits_{3/4} ^{1} x \, dx}} x \, dx}} x \,dx$$ So as we can see, $ \{ u_{ i } \} _{ i = 0 } ^{ \infty } $ is in fact a sequence of real numbers, in which only three of the first few terms are shown. So now, if $P = \displaystyle \prod _{ n = 0 } ^{ 2015 } \frac{ 1 } { 4 u_{ n } }$, how can we find $  \log_{2}P$?","Given the following: $$ u_0 = \int \limits_{ 0 } ^{ 1 } x \, dx , \:\:\: u_1 = \int \limits^{ \int \limits_{ 1/2 } ^{ 1 } x \, dx } _{ \int \limits_{ 0 } ^{ 1/2 } x \, dx } x \,dx , \:\:\: u_2 = \int \limits_{ { \int \limits_{{\int \limits _{0} ^{1/4} x \, dx}} ^{{\int \limits_{1/4} ^{2/4} x \, dx}} x \, dx}} ^{{\int \limits_{{\int \limits_{2/4} ^{3/4} x \, dx}} ^{{\int \limits_{3/4} ^{1} x \, dx}} x \, dx}} x \,dx$$ So as we can see, $ \{ u_{ i } \} _{ i = 0 } ^{ \infty } $ is in fact a sequence of real numbers, in which only three of the first few terms are shown. So now, if $P = \displaystyle \prod _{ n = 0 } ^{ 2015 } \frac{ 1 } { 4 u_{ n } }$, how can we find $  \log_{2}P$?",,"['calculus', 'integration', 'sequences-and-series']"
78,Is Risch algorithm learnable by a human being?,Is Risch algorithm learnable by a human being?,,"I've discovered a few months ago about the Risch's algorithm. It seems to be the best thing we have to find antiderivatives. Some days ago, I've found lectures on the integration of elementary functions. And in the abstract of the lectures, the author argued that there is a misconception among calculus professors: That they say that derivation is just an algorithm, but integration is not and he wants to show that it's not completely true. In his lectures, he gives: Hermite algorithm. Lazard-Rioboo-Trager algorithm. Abel's theorem and the non-integrability of algebraic functions in terms of elementary functions. So, it seems that these algoritms are learnable and also that learn it would not be so overkill. I got curious if Risch algorithm could also be learned by a human being.","I've discovered a few months ago about the Risch's algorithm. It seems to be the best thing we have to find antiderivatives. Some days ago, I've found lectures on the integration of elementary functions. And in the abstract of the lectures, the author argued that there is a misconception among calculus professors: That they say that derivation is just an algorithm, but integration is not and he wants to show that it's not completely true. In his lectures, he gives: Hermite algorithm. Lazard-Rioboo-Trager algorithm. Abel's theorem and the non-integrability of algebraic functions in terms of elementary functions. So, it seems that these algoritms are learnable and also that learn it would not be so overkill. I got curious if Risch algorithm could also be learned by a human being.",,"['calculus', 'algorithms']"
79,A derivation of the Euler-Maclaurin formula?,A derivation of the Euler-Maclaurin formula?,,"The generating function for the Bernoulli numbers $B_n$ is $$\frac{x}{e^x-1}=\sum_{n=0}^\infty\frac{B_n}{n!}x^n$$ The sum of an infinite geometric series is $$\frac{1}{1-x}=\sum_{k=0}^\infty x^k$$ Replacing $x$ with $e^x$ yields $$\frac{1}{1-e^x}=\sum_{k=0}^\infty(e^x)^k$$ Multiplying both sides by $-x$ yields $$\frac{x}{e^x-1}=-\sum_{k=0}^\infty(e^x)^kx$$ Hence $$\sum_{k=0}^\infty(e^x)^kx+\sum_{n=0}^\infty\frac{B_n}{n!}x^n=0$$ Replacing $x$ with the derivative operator $D$ yields $$\sum_{k=0}^\infty(e^D)^kD+\sum_{n=0}^\infty\frac{B_n}{n!}D^n=0$$ The exponential of the derivative operator is the shift operator $T$: $$e^D=T$$ Hence $$\sum_{k=0}^\infty T^kD+\sum_{n=0}^\infty\frac{B_n}{n!}D^n=0$$ Applying this to a function $f$ yields $$\sum_{k=0}^\infty (T^kDf)(x)+\sum_{n=0}^\infty\frac{B_n}{n!}(D^nf)(x)=0$$ which is equivalent by $(Tf)(x)=f(x+1)$ to $$\sum_{k=0}^\infty (Df)(x+k)+\sum_{n=0}^\infty\frac{B_n}{n!}(D^nf)(x)=0$$ If $Df=g$, this means $$\sum_{k=0}^\infty g(x+k)+\int_a^xg(t)\,\mathrm{d}t+\sum_{n=0}^\infty\frac{B_{n+1}}{(n+1)!}(D^ng)(x)=0$$ Since the values of the Riemann zeta function at the negative integers are $$\zeta(-n)=-\frac{B_{n+1}}{(n+1)}$$ This can be expressed as $$\sum_{k=0}^\infty g(x+k)+\int_a^xg(t)\,\mathrm{d}t-\sum_{n=0}^\infty\frac{\zeta(-n)}{n!}(D^ng)(x)=0$$ or $$\sum_{k=0}^\infty g(x+k)=\sum_{n=0}^\infty\frac{\zeta(-n)}{n!}(D^ng)(x)-\int_a^xg(t)\,\mathrm{d}t$$ Have I derived a version of the Euler-Maclaurin formula correctly? Is an additional error term needed somewhere? Why does the expression on the right side of the equation seem to depend on $a$, while the one on the left does not? How can I manipulate this procedure to obtain the more traditional version of the formula, e.g. with finite sums? I know, for example, that one can express a finite sum of powers using $$\frac{1-x^{n+1}}{1-x}=1+x+x^2+x^3+\ldots+x^n=\sum_{k=0}^n x^k$$","The generating function for the Bernoulli numbers $B_n$ is $$\frac{x}{e^x-1}=\sum_{n=0}^\infty\frac{B_n}{n!}x^n$$ The sum of an infinite geometric series is $$\frac{1}{1-x}=\sum_{k=0}^\infty x^k$$ Replacing $x$ with $e^x$ yields $$\frac{1}{1-e^x}=\sum_{k=0}^\infty(e^x)^k$$ Multiplying both sides by $-x$ yields $$\frac{x}{e^x-1}=-\sum_{k=0}^\infty(e^x)^kx$$ Hence $$\sum_{k=0}^\infty(e^x)^kx+\sum_{n=0}^\infty\frac{B_n}{n!}x^n=0$$ Replacing $x$ with the derivative operator $D$ yields $$\sum_{k=0}^\infty(e^D)^kD+\sum_{n=0}^\infty\frac{B_n}{n!}D^n=0$$ The exponential of the derivative operator is the shift operator $T$: $$e^D=T$$ Hence $$\sum_{k=0}^\infty T^kD+\sum_{n=0}^\infty\frac{B_n}{n!}D^n=0$$ Applying this to a function $f$ yields $$\sum_{k=0}^\infty (T^kDf)(x)+\sum_{n=0}^\infty\frac{B_n}{n!}(D^nf)(x)=0$$ which is equivalent by $(Tf)(x)=f(x+1)$ to $$\sum_{k=0}^\infty (Df)(x+k)+\sum_{n=0}^\infty\frac{B_n}{n!}(D^nf)(x)=0$$ If $Df=g$, this means $$\sum_{k=0}^\infty g(x+k)+\int_a^xg(t)\,\mathrm{d}t+\sum_{n=0}^\infty\frac{B_{n+1}}{(n+1)!}(D^ng)(x)=0$$ Since the values of the Riemann zeta function at the negative integers are $$\zeta(-n)=-\frac{B_{n+1}}{(n+1)}$$ This can be expressed as $$\sum_{k=0}^\infty g(x+k)+\int_a^xg(t)\,\mathrm{d}t-\sum_{n=0}^\infty\frac{\zeta(-n)}{n!}(D^ng)(x)=0$$ or $$\sum_{k=0}^\infty g(x+k)=\sum_{n=0}^\infty\frac{\zeta(-n)}{n!}(D^ng)(x)-\int_a^xg(t)\,\mathrm{d}t$$ Have I derived a version of the Euler-Maclaurin formula correctly? Is an additional error term needed somewhere? Why does the expression on the right side of the equation seem to depend on $a$, while the one on the left does not? How can I manipulate this procedure to obtain the more traditional version of the formula, e.g. with finite sums? I know, for example, that one can express a finite sum of powers using $$\frac{1-x^{n+1}}{1-x}=1+x+x^2+x^3+\ldots+x^n=\sum_{k=0}^n x^k$$",,"['calculus', 'sequences-and-series', 'analysis', 'taylor-expansion', 'euler-maclaurin']"
80,Can we interchange the Integral and Summation when a limit is $\infty$?,Can we interchange the Integral and Summation when a limit is ?,\infty,"I was trying to Evaluate the Integral: $$\Large{I=\int_1^{\infty} \frac{\ln x}{x^2+1} dx}$$ $$\color{#66f}{{\frac{1}{x^2+1} = \frac{1}{x^2\left(1+\frac{1}{x^2}\right)}=\frac{1}{x^2}\cdot \frac{1}{1+x^{-2}}=\frac{1}{x^2} \sum_{n=0}^{\infty} \left(\frac{1}{-x^{2}}\right)^{n}}}$$ $${I=\int_1^{\infty} \left(\frac{\ln x}{x^2}-\frac{\ln x}{x^4}+\frac{\ln x}{x^6}+\cdots\right)dx}=-\int_1^{\infty} \sum_{k=1}^{\infty} \frac{\ln x}{(-1)^kx^{2k}} dx$$ Now I would like to interchange the integral and the summation, like : $$-\int_1^{\infty} \sum_{k=1}^{\infty} \frac{\ln x}{(-1)^kx^{2k}} dx=-\sum_{k=1}^{\infty} \int_1^{\infty}  \frac{\ln x}{(-1)^kx^{2k}} dx$$ But I'm not sure  if I can do that when $\infty$ is present (not real)... $$\text{I know that:}$$ $$\bbox[8pt, border: solid 2pt crimson]{-\int_1^{b} \sum_{k=1}^{a} \frac{\ln x}{(-1)^kx^{2k}} dx=-\sum_{k=1}^{a} \int_1^{b}  \frac{\ln x}{(-1)^kx^{2k}} dx}$$ $$\color{crimson}{\text{Where a, b is real}}$$ But is it the same when $a,b=\infty$?","I was trying to Evaluate the Integral: $$\Large{I=\int_1^{\infty} \frac{\ln x}{x^2+1} dx}$$ $$\color{#66f}{{\frac{1}{x^2+1} = \frac{1}{x^2\left(1+\frac{1}{x^2}\right)}=\frac{1}{x^2}\cdot \frac{1}{1+x^{-2}}=\frac{1}{x^2} \sum_{n=0}^{\infty} \left(\frac{1}{-x^{2}}\right)^{n}}}$$ $${I=\int_1^{\infty} \left(\frac{\ln x}{x^2}-\frac{\ln x}{x^4}+\frac{\ln x}{x^6}+\cdots\right)dx}=-\int_1^{\infty} \sum_{k=1}^{\infty} \frac{\ln x}{(-1)^kx^{2k}} dx$$ Now I would like to interchange the integral and the summation, like : $$-\int_1^{\infty} \sum_{k=1}^{\infty} \frac{\ln x}{(-1)^kx^{2k}} dx=-\sum_{k=1}^{\infty} \int_1^{\infty}  \frac{\ln x}{(-1)^kx^{2k}} dx$$ But I'm not sure  if I can do that when $\infty$ is present (not real)... $$\text{I know that:}$$ $$\bbox[8pt, border: solid 2pt crimson]{-\int_1^{b} \sum_{k=1}^{a} \frac{\ln x}{(-1)^kx^{2k}} dx=-\sum_{k=1}^{a} \int_1^{b}  \frac{\ln x}{(-1)^kx^{2k}} dx}$$ $$\color{crimson}{\text{Where a, b is real}}$$ But is it the same when $a,b=\infty$?",,"['calculus', 'definite-integrals', 'summation']"
81,Approximation of the exponential,Approximation of the exponential,,"Let $c>1,k\in\mathbb{N}$. Let's consider two approximations of the exponential function : The first one is the most common one $f_k(x)=\left(1+\frac{x}{c^k}\right)^{c^k}$ and the second one is $q_k(x)=\displaystyle\sum _{p=0}^k\frac{c^{\frac{p^2+p}{2}}}{\displaystyle\prod _{n=1}^p(c^n-1)\prod_{n=1}^{k-p}(1-c^n)}f_p\left(x\right)$. It has been shown here that $q_k(x)$ goes to $e^x$ as $c\to\infty$ or $k\to\infty$. In the following question, we'll stay with with fixed $c,k$ However, after experimenting a bit on desmos , I have conjectured some results I am not able to prove mathematically, and therefore have some question : One can see (on the desmos graph) that there exists a non empty interval $I$ around $0$ on which $q_k(x)$ is a better approximation of $e^x$ than $f_k(x)$ (by that I mean that $\forall x\in I,|q_k(x)-e^x|\le|f_k(x)-e^x|$). As $c$ or $k$ increase, this interval gets larger and larger. How can one prove that such an interval exists ? What are the bounds of this interval ? It seems to me that on this interval, $q_k(x)$ 'sticks' to $e^x$ way closer than $f_k(x)$ does. I conjectured that $\forall\epsilon,c>0,\exists k\in\mathbb{N},\forall x\in[-c,c],\left|\dfrac{q_k(x)-e^x}{f_k(x)-e^x}\right|<\epsilon$. How can one show that ?","Let $c>1,k\in\mathbb{N}$. Let's consider two approximations of the exponential function : The first one is the most common one $f_k(x)=\left(1+\frac{x}{c^k}\right)^{c^k}$ and the second one is $q_k(x)=\displaystyle\sum _{p=0}^k\frac{c^{\frac{p^2+p}{2}}}{\displaystyle\prod _{n=1}^p(c^n-1)\prod_{n=1}^{k-p}(1-c^n)}f_p\left(x\right)$. It has been shown here that $q_k(x)$ goes to $e^x$ as $c\to\infty$ or $k\to\infty$. In the following question, we'll stay with with fixed $c,k$ However, after experimenting a bit on desmos , I have conjectured some results I am not able to prove mathematically, and therefore have some question : One can see (on the desmos graph) that there exists a non empty interval $I$ around $0$ on which $q_k(x)$ is a better approximation of $e^x$ than $f_k(x)$ (by that I mean that $\forall x\in I,|q_k(x)-e^x|\le|f_k(x)-e^x|$). As $c$ or $k$ increase, this interval gets larger and larger. How can one prove that such an interval exists ? What are the bounds of this interval ? It seems to me that on this interval, $q_k(x)$ 'sticks' to $e^x$ way closer than $f_k(x)$ does. I conjectured that $\forall\epsilon,c>0,\exists k\in\mathbb{N},\forall x\in[-c,c],\left|\dfrac{q_k(x)-e^x}{f_k(x)-e^x}\right|<\epsilon$. How can one show that ?",,"['calculus', 'functions', 'convergence-divergence', 'exponential-function', 'approximation']"
82,What is the difference between tensor calculus and exterior derivative type concepts?,What is the difference between tensor calculus and exterior derivative type concepts?,,"I am trying to clarify terms in order to help me figure out what I'd like to study. I understand that $p$-forms and $p$-vectors are used with things like wedge products, exterior algebras, and a bunch of other concepts I don't understand that seem to fall under the label of ""exterior"" or ""geometric"". For instance, the exterior derivative. Yet I also understand there is something called ""tensor calculus"" which is supposed to be the tensor analog of vector calculus. Are these two genres of mathematical objects different? They both seem to involve tensors and calculus so I am confused since a concept like exterior derivative seems to involve both tensors and calculus and yet I don't see it called ""tensor calculus."" Thanks!","I am trying to clarify terms in order to help me figure out what I'd like to study. I understand that $p$-forms and $p$-vectors are used with things like wedge products, exterior algebras, and a bunch of other concepts I don't understand that seem to fall under the label of ""exterior"" or ""geometric"". For instance, the exterior derivative. Yet I also understand there is something called ""tensor calculus"" which is supposed to be the tensor analog of vector calculus. Are these two genres of mathematical objects different? They both seem to involve tensors and calculus so I am confused since a concept like exterior derivative seems to involve both tensors and calculus and yet I don't see it called ""tensor calculus."" Thanks!",,"['calculus', 'vectors', 'tensors', 'exterior-algebra']"
83,Can these two indefinite integrals be evaluated in closed form?,Can these two indefinite integrals be evaluated in closed form?,,"I'm wondering whether any of these two indefinite integrals $$\int \frac{1}{\sqrt{1+\alpha \sinh(x)^{-4/3}}}dx$$ $$\int \frac{\sinh(x)^{-4/3}}{\sqrt{1+\alpha\sinh(x)^{-4/3}}}dx$$ can be evaluated in closed form. I've tried a couple of substitutions, looked at integral tables, and asked Mathematica, to no avail so far.","I'm wondering whether any of these two indefinite integrals $$\int \frac{1}{\sqrt{1+\alpha \sinh(x)^{-4/3}}}dx$$ $$\int \frac{\sinh(x)^{-4/3}}{\sqrt{1+\alpha\sinh(x)^{-4/3}}}dx$$ can be evaluated in closed form. I've tried a couple of substitutions, looked at integral tables, and asked Mathematica, to no avail so far.",,"['calculus', 'integration', 'indefinite-integrals', 'hyperbolic-functions']"
84,Any weird 'modular like mathematical space' that behaves like as if it is infinite until a threshold is reached?,Any weird 'modular like mathematical space' that behaves like as if it is infinite until a threshold is reached?,,"(Using the advice from Mathoverflow, I have rephrased and splitted up the question) (Might be a bit layman because I don't have rigorous math term to describe the concept) Generalize it to mathematical spaces, are there spaces which are sort of like a . Having multiple $\mathbb{R}^2$ spaces that has a finite size k but acts like as if extending to infinity (see point b) sewed together b . Has a weird property that for each trip wrt a parameter (e.g. time) if each distance of travel $D_i$ exceed a certain value k, you will hop to the corresponding location of the neighboring patch of $\mathbb{R}^2$ plus moving in this new patch by a distance $D \hspace{1mm}mod\hspace{1mm}k$. If your distance of travel for any of the trips does not exceed k, then no matter how far you travel, you will still be travelling within the same patch and never reach the edge of it Using an example, (letting the parameter be time) property b is basically saying that you only jump to neighbouring patches (and travelling the remainder of it after dividing k) if your velocity exceed the threshold k (the size of the patch), otherwise you will only be moving within a single patch forever, even if the total distance traveled exceed k An attempt to formulate the above descriptions mathematically: Question: Is there a class of mathematical spaces that has similar properties as described below or a more generalized version of it? a. For the case of $\mathbb{R}^2$, each patch is a $\mathbb{R}^2$ space and is described by ordered pair of integers $(p_{1},p_{2})$. Let $(0,0)$ be the central patch. Therefore every point $P_{x}$ in the space can be described by $(p_{x1},p_{x2},x_1,x_2)$ and the separation $S_{xy}$ between points is described by $(\Delta p_{x1y1},\Delta p_{x2y2},\Delta x_1y_1,\Delta x_2y_2)$ where $$\Delta x_jy_j=y_j-x_j,j=1,2$$ $$\Delta p_{x_jy_j}=p_{y_j}-p_{x_j},j=1,2$$ Let a path $\mathit{\lambda}$ connecting from $P_x$ to $P_z$ (both points are located on the same patch). $P_x'$ be the actual destination. $D$ is the total distance of the path $\mathit{\lambda}$ which is the sum of the distances of i trips (shorter partitions of the path i.e. $\lambda_i$), $$D=\sum_{i=0}^nD_i,D \geq D_i$$ where $D_i$ is defined as the usual Euclidean distance between two neighbouring points in each partition $$D_i=\sqrt{\sum_{j=1}^2\left(x_{(i+1)j}-x_{ij}\right)^2}$$ Let $d$ be the Euclidean separation between $P_x$ and $P_z$ defined as $$d=\sqrt{\sum_{j=1}^2\left(z_j-x_j\right)^2}$$ b. Each patch has a threshold $k$ where $k \in \mathbb{Z}$ such that If $\exists D_i>k$ then $P_x'$ has coordinates $$\left\{\begin{matrix} (p_{x1}+1,p_{x2},x_1+D \hspace{1mm}mod\hspace{1mm}k,x_2),z_1>x_1,z_2=x_2 \\ (p_{x1},p_{x2}+1,x_1,x_2+D \hspace{1mm}mod\hspace{1mm}k),z_1=x_1,z_2>x_2 \\  (p_{x1}+1,p_{x2},x_1-D \hspace{1mm}mod\hspace{1mm}k,x_2),z_1<x_1,z_2=x_2 \\ (p_{x1},p_{x2}+1,x_1,x_2-D \hspace{1mm}mod\hspace{1mm}k),z_1=x_1,z_2<x_2 \\ \\ (p_{x1}+1,p_{x2}+1,x_1\left(1+\frac{D \hspace{1mm}mod\hspace{1mm}k}{d}\right),x_2\left(1+\frac{D \hspace{1mm}mod\hspace{1mm}k}{d}\right)),z_1>x_1,z_2>x_2 \\ (p_{x1}+1,p_{x2}-1,x_1\left(1+\frac{D \hspace{1mm}mod\hspace{1mm}k}{d}\right),x_2\left(1-\frac{D \hspace{1mm}mod\hspace{1mm}k}{d}\right)),z_1>x_1,z_2<x_2 \\  (p_{x1}-1,p_{x2}+1,x_1\left(1-\frac{D \hspace{1mm}mod\hspace{1mm}k}{d}\right),x_2\left(1+\frac{D \hspace{1mm}mod\hspace{1mm}k}{d}\right)),z_1<x_1,z_2>x_2 \\ (p_{x1}-1,p_{x2}-1,x_1\left(1-\frac{D \hspace{1mm}mod\hspace{1mm}k}{d}\right),x_2\left(1-\frac{D \hspace{1mm}mod\hspace{1mm}k}{d}\right)),z_1<x_1,z_2<x_2 \\  \end{matrix}\right.$$ If there is no $D_i>k$ then regardless of whether $D>k$ $$P_x'=P_z$$ . For the case of $\mathbb{R}^n$ a. Each patch is a $\mathbb{R}^n$ space and is described by a vector $\mathbf{p}$ with integer components. Therefore $\mathbf{p=0}$ is the central patch. Therefore every point $P_x$ in the space is described by ($\mathbf{p_x}$,$\mathbf{x}$) and the separation $\mathbf{S_{xy}}$ is described in a similar fashion to the $\mathbb{R}^2$ case as ($\mathbf{\Delta p_{xy}}, \mathbf{\Delta xy}$). The path $\lambda$, the points $P_z$, $P_x'$, the distances $D$, $D_i$ and the threshold $k$ are generaliation of the $\mathbb{R}^2$ case. The vector $\mathbf{d}$ which go from $P_x$ to $P_z$ is defined as follows $$\mathbf{d}=\mathbf{z}-\mathbf{x}$$ such that $d=|\mathbf{d}|$ and $\hat{\mathbf{d}}=\frac{\mathbf{d}}{d}$ b. Each patch has a threshold $k$ where $k \in \mathbb{Z}$ such that If $\exists D_i>k$ then the coordinates of $P_x'$ is $$(\mathbf{p_x+}\sum_{j=1}^n \epsilon_j \mathbf{e_j},\mathbf{x+}(D \hspace{1mm}mod\hspace{1mm}k)\hat{\mathbf{d}})$$ where $$\epsilon_j=\left\{\begin{matrix} 1,\mathbf{d \cdot e_j}>0\\ -1,\mathbf{d \cdot e_j}<0\\ 0,\mathbf{d \cdot e_j}=0 \end{matrix}\right.$$ where $\mathbf{e_j}$ are unit vectors along the $j,m\in [1,n]$ cartesian directions and $\mathbf{e_j}\cdot\mathbf{e_m}=\delta_{jm}$ If there is no $D_i>k$ then regardless of whether $D>k$ $$P_x'=P_z$$ I would like some suggestion or sources for further reading on such objects if they exist in order to learn more about their properties","(Using the advice from Mathoverflow, I have rephrased and splitted up the question) (Might be a bit layman because I don't have rigorous math term to describe the concept) Generalize it to mathematical spaces, are there spaces which are sort of like a . Having multiple $\mathbb{R}^2$ spaces that has a finite size k but acts like as if extending to infinity (see point b) sewed together b . Has a weird property that for each trip wrt a parameter (e.g. time) if each distance of travel $D_i$ exceed a certain value k, you will hop to the corresponding location of the neighboring patch of $\mathbb{R}^2$ plus moving in this new patch by a distance $D \hspace{1mm}mod\hspace{1mm}k$. If your distance of travel for any of the trips does not exceed k, then no matter how far you travel, you will still be travelling within the same patch and never reach the edge of it Using an example, (letting the parameter be time) property b is basically saying that you only jump to neighbouring patches (and travelling the remainder of it after dividing k) if your velocity exceed the threshold k (the size of the patch), otherwise you will only be moving within a single patch forever, even if the total distance traveled exceed k An attempt to formulate the above descriptions mathematically: Question: Is there a class of mathematical spaces that has similar properties as described below or a more generalized version of it? a. For the case of $\mathbb{R}^2$, each patch is a $\mathbb{R}^2$ space and is described by ordered pair of integers $(p_{1},p_{2})$. Let $(0,0)$ be the central patch. Therefore every point $P_{x}$ in the space can be described by $(p_{x1},p_{x2},x_1,x_2)$ and the separation $S_{xy}$ between points is described by $(\Delta p_{x1y1},\Delta p_{x2y2},\Delta x_1y_1,\Delta x_2y_2)$ where $$\Delta x_jy_j=y_j-x_j,j=1,2$$ $$\Delta p_{x_jy_j}=p_{y_j}-p_{x_j},j=1,2$$ Let a path $\mathit{\lambda}$ connecting from $P_x$ to $P_z$ (both points are located on the same patch). $P_x'$ be the actual destination. $D$ is the total distance of the path $\mathit{\lambda}$ which is the sum of the distances of i trips (shorter partitions of the path i.e. $\lambda_i$), $$D=\sum_{i=0}^nD_i,D \geq D_i$$ where $D_i$ is defined as the usual Euclidean distance between two neighbouring points in each partition $$D_i=\sqrt{\sum_{j=1}^2\left(x_{(i+1)j}-x_{ij}\right)^2}$$ Let $d$ be the Euclidean separation between $P_x$ and $P_z$ defined as $$d=\sqrt{\sum_{j=1}^2\left(z_j-x_j\right)^2}$$ b. Each patch has a threshold $k$ where $k \in \mathbb{Z}$ such that If $\exists D_i>k$ then $P_x'$ has coordinates $$\left\{\begin{matrix} (p_{x1}+1,p_{x2},x_1+D \hspace{1mm}mod\hspace{1mm}k,x_2),z_1>x_1,z_2=x_2 \\ (p_{x1},p_{x2}+1,x_1,x_2+D \hspace{1mm}mod\hspace{1mm}k),z_1=x_1,z_2>x_2 \\  (p_{x1}+1,p_{x2},x_1-D \hspace{1mm}mod\hspace{1mm}k,x_2),z_1<x_1,z_2=x_2 \\ (p_{x1},p_{x2}+1,x_1,x_2-D \hspace{1mm}mod\hspace{1mm}k),z_1=x_1,z_2<x_2 \\ \\ (p_{x1}+1,p_{x2}+1,x_1\left(1+\frac{D \hspace{1mm}mod\hspace{1mm}k}{d}\right),x_2\left(1+\frac{D \hspace{1mm}mod\hspace{1mm}k}{d}\right)),z_1>x_1,z_2>x_2 \\ (p_{x1}+1,p_{x2}-1,x_1\left(1+\frac{D \hspace{1mm}mod\hspace{1mm}k}{d}\right),x_2\left(1-\frac{D \hspace{1mm}mod\hspace{1mm}k}{d}\right)),z_1>x_1,z_2<x_2 \\  (p_{x1}-1,p_{x2}+1,x_1\left(1-\frac{D \hspace{1mm}mod\hspace{1mm}k}{d}\right),x_2\left(1+\frac{D \hspace{1mm}mod\hspace{1mm}k}{d}\right)),z_1<x_1,z_2>x_2 \\ (p_{x1}-1,p_{x2}-1,x_1\left(1-\frac{D \hspace{1mm}mod\hspace{1mm}k}{d}\right),x_2\left(1-\frac{D \hspace{1mm}mod\hspace{1mm}k}{d}\right)),z_1<x_1,z_2<x_2 \\  \end{matrix}\right.$$ If there is no $D_i>k$ then regardless of whether $D>k$ $$P_x'=P_z$$ . For the case of $\mathbb{R}^n$ a. Each patch is a $\mathbb{R}^n$ space and is described by a vector $\mathbf{p}$ with integer components. Therefore $\mathbf{p=0}$ is the central patch. Therefore every point $P_x$ in the space is described by ($\mathbf{p_x}$,$\mathbf{x}$) and the separation $\mathbf{S_{xy}}$ is described in a similar fashion to the $\mathbb{R}^2$ case as ($\mathbf{\Delta p_{xy}}, \mathbf{\Delta xy}$). The path $\lambda$, the points $P_z$, $P_x'$, the distances $D$, $D_i$ and the threshold $k$ are generaliation of the $\mathbb{R}^2$ case. The vector $\mathbf{d}$ which go from $P_x$ to $P_z$ is defined as follows $$\mathbf{d}=\mathbf{z}-\mathbf{x}$$ such that $d=|\mathbf{d}|$ and $\hat{\mathbf{d}}=\frac{\mathbf{d}}{d}$ b. Each patch has a threshold $k$ where $k \in \mathbb{Z}$ such that If $\exists D_i>k$ then the coordinates of $P_x'$ is $$(\mathbf{p_x+}\sum_{j=1}^n \epsilon_j \mathbf{e_j},\mathbf{x+}(D \hspace{1mm}mod\hspace{1mm}k)\hat{\mathbf{d}})$$ where $$\epsilon_j=\left\{\begin{matrix} 1,\mathbf{d \cdot e_j}>0\\ -1,\mathbf{d \cdot e_j}<0\\ 0,\mathbf{d \cdot e_j}=0 \end{matrix}\right.$$ where $\mathbf{e_j}$ are unit vectors along the $j,m\in [1,n]$ cartesian directions and $\mathbf{e_j}\cdot\mathbf{e_m}=\delta_{jm}$ If there is no $D_i>k$ then regardless of whether $D>k$ $$P_x'=P_z$$ I would like some suggestion or sources for further reading on such objects if they exist in order to learn more about their properties",,"['calculus', 'geometry']"
85,Closed-form of integrals containing double exponentials,Closed-form of integrals containing double exponentials,,"Are there closed forms for the following integrals? $$\begin{align} I_1(w) & = \int_{-\infty}^{\infty} \frac{\exp(-we^y)}{y^2+\pi^2} dy, \\ I_2(w) & = \int_{-\infty}^{\infty} \frac{\exp(-we^y+y)}{y^2+\pi^2} dy, \\ I_3(w,a) & = \int_{-\infty}^{\infty} \frac{\exp(-we^y) \cdot (a+e^y)}{y^2 + \pi^2} \, dy, \end{align}$$ where $w$ and $a$ are nonzero real parameters. I'm interested in special case of $w=1$, and $a=1$ or $a=2$. The motivation of the problem is the FransnRobinson constant , and in this answer I've shown that $I_3(1,1) = \int_0^1 \frac{1}{\Gamma(x)} dx.$ But maybe there is also a closed form for $I_3(1,1)$ perhaps in term of exponential integrals . Some related integrals. $$\begin{align} & \int \exp\left(-e^{y}\right) dy = \operatorname{Ei}(-e^{y})+C, \\ & \int \exp\left(-we^{y}\right) dy = \operatorname{Ei}(-we^{y})+C, \\ & \int \exp\left(-e^{y}+y\right) dy = -\exp \left(-e^y \right)+C, \\ & \int \exp\left(-we^{y}+y\right) dy = \frac{-\exp \left(-we^y \right)}{w}+C, \end{align}$$ where $\operatorname{Ei}$ is the exponential integral .","Are there closed forms for the following integrals? $$\begin{align} I_1(w) & = \int_{-\infty}^{\infty} \frac{\exp(-we^y)}{y^2+\pi^2} dy, \\ I_2(w) & = \int_{-\infty}^{\infty} \frac{\exp(-we^y+y)}{y^2+\pi^2} dy, \\ I_3(w,a) & = \int_{-\infty}^{\infty} \frac{\exp(-we^y) \cdot (a+e^y)}{y^2 + \pi^2} \, dy, \end{align}$$ where $w$ and $a$ are nonzero real parameters. I'm interested in special case of $w=1$, and $a=1$ or $a=2$. The motivation of the problem is the FransnRobinson constant , and in this answer I've shown that $I_3(1,1) = \int_0^1 \frac{1}{\Gamma(x)} dx.$ But maybe there is also a closed form for $I_3(1,1)$ perhaps in term of exponential integrals . Some related integrals. $$\begin{align} & \int \exp\left(-e^{y}\right) dy = \operatorname{Ei}(-e^{y})+C, \\ & \int \exp\left(-we^{y}\right) dy = \operatorname{Ei}(-we^{y})+C, \\ & \int \exp\left(-e^{y}+y\right) dy = -\exp \left(-e^y \right)+C, \\ & \int \exp\left(-we^{y}+y\right) dy = \frac{-\exp \left(-we^y \right)}{w}+C, \end{align}$$ where $\operatorname{Ei}$ is the exponential integral .",,"['calculus', 'integration', 'definite-integrals', 'exponential-function', 'closed-form']"
86,Differentiating $\arcsin(x)$ using first principle method.,Differentiating  using first principle method.,\arcsin(x),"Q. Differentiate $\sin^{-1}(x)$ using first principle (delta) method. I did this the following way: $$y=\sin^{-1}(x)$$ $$\therefore\frac{dy}{dx}=\lim_{h\to \:0\:}\left(\frac{\sin^{-1}\left(x+h\right)-\sin^{-1}x}{h}\right)$$ Now let $$\sin^{-1}(x+h)=A\text{ and }\sin^{-1}(x)=B$$ or $$x+h=\sin A\text{ and }x=\sin B$$ $$\therefore h=\sin A-\sin B$$  When $h\to \:0\:$ , $$\sin A\to \sin B\text{ or }A\to B\text{ or }A-B\to 0\tag 1$$ $$\therefore\frac{dy}{dx}=\lim_{A-B\to \:\:0\:}\left(\frac{A-B}{\sin A-\sin B}\right)$$ $=\frac{dy}{dx}=\lim_{A-B\to 0}\left(\frac{A-B}{2\cos\left(\frac{A+B}{2}\right)\sin\left(\frac{A-B}{2}\right)}\right)$ $\:\frac{dy}{dx}=\lim_{A-B\to \:\:0\:}\left(\frac{2}{2\cos\left(\frac{A+B}{2}\right)\frac{\sin\left(\frac{A-B}{2}\right)}{\frac{A-B}{2}}}\right)$ $\:\frac{dy}{dx}=\lim_{A-B\to \:\:\:0\:}\left(\frac{2}{2\cos\left(\frac{A+B}{2}\right)}\right)$ as ($\:\frac{dy}{dx}=\lim_{A-B\to 0\:}\frac{\sin\left(\frac{A-B}{2}\right)}{\frac{A-B}{2}}=1$) $\frac{dy}{dx}=\left(\frac{2}{2\cos\left(\frac{2B}{2}\right)}\right)=\left(\frac{2}{2\cos\left(\sin^{-1}x\right)}\right)=\frac{1}{\sqrt{1-x^2\:}}$ The final answer is valid. But is this method correct? Is EQ.1 correct? Or is it just a fluke. (It seems to work for all other inverse functions too).","Q. Differentiate $\sin^{-1}(x)$ using first principle (delta) method. I did this the following way: $$y=\sin^{-1}(x)$$ $$\therefore\frac{dy}{dx}=\lim_{h\to \:0\:}\left(\frac{\sin^{-1}\left(x+h\right)-\sin^{-1}x}{h}\right)$$ Now let $$\sin^{-1}(x+h)=A\text{ and }\sin^{-1}(x)=B$$ or $$x+h=\sin A\text{ and }x=\sin B$$ $$\therefore h=\sin A-\sin B$$  When $h\to \:0\:$ , $$\sin A\to \sin B\text{ or }A\to B\text{ or }A-B\to 0\tag 1$$ $$\therefore\frac{dy}{dx}=\lim_{A-B\to \:\:0\:}\left(\frac{A-B}{\sin A-\sin B}\right)$$ $=\frac{dy}{dx}=\lim_{A-B\to 0}\left(\frac{A-B}{2\cos\left(\frac{A+B}{2}\right)\sin\left(\frac{A-B}{2}\right)}\right)$ $\:\frac{dy}{dx}=\lim_{A-B\to \:\:0\:}\left(\frac{2}{2\cos\left(\frac{A+B}{2}\right)\frac{\sin\left(\frac{A-B}{2}\right)}{\frac{A-B}{2}}}\right)$ $\:\frac{dy}{dx}=\lim_{A-B\to \:\:\:0\:}\left(\frac{2}{2\cos\left(\frac{A+B}{2}\right)}\right)$ as ($\:\frac{dy}{dx}=\lim_{A-B\to 0\:}\frac{\sin\left(\frac{A-B}{2}\right)}{\frac{A-B}{2}}=1$) $\frac{dy}{dx}=\left(\frac{2}{2\cos\left(\frac{2B}{2}\right)}\right)=\left(\frac{2}{2\cos\left(\sin^{-1}x\right)}\right)=\frac{1}{\sqrt{1-x^2\:}}$ The final answer is valid. But is this method correct? Is EQ.1 correct? Or is it just a fluke. (It seems to work for all other inverse functions too).",,"['calculus', 'limits', 'derivatives']"
87,Composing a smooth even function and square root,Composing a smooth even function and square root,,"Let $f:\mathbb{R}\to\mathbb{R}$ be smooth and satisfy $f(-x)=f(x)$ for all x. Define $g:[0,\infty)\to\mathbb{R}$ by $g(x)=f(\sqrt{x})$. Is $g$ necessarily smooth at $0$? I guess the answer is positive. $f$ being an even function implies that all its derivatives of odd order vanish at $0$, hence when substituting $\sqrt{x}$ in the Taylor series of $f$ around $0$, one obtains a power series in $x$. This is obviously not a proof, as most of the smooth functions are not analytic. Showing that $f$ is differentiable is easy, and working a little harder one also shows that it is twice differentiable. It seems like it should work the same for derivatives of larger orders, and yet, I haven't found a satisfactory proof.","Let $f:\mathbb{R}\to\mathbb{R}$ be smooth and satisfy $f(-x)=f(x)$ for all x. Define $g:[0,\infty)\to\mathbb{R}$ by $g(x)=f(\sqrt{x})$. Is $g$ necessarily smooth at $0$? I guess the answer is positive. $f$ being an even function implies that all its derivatives of odd order vanish at $0$, hence when substituting $\sqrt{x}$ in the Taylor series of $f$ around $0$, one obtains a power series in $x$. This is obviously not a proof, as most of the smooth functions are not analytic. Showing that $f$ is differentiable is easy, and working a little harder one also shows that it is twice differentiable. It seems like it should work the same for derivatives of larger orders, and yet, I haven't found a satisfactory proof.",,['calculus']
88,"$\lim_{x\to+\infty}\frac{f(x)}{e^x}=l$, then $\lim_{x\to+\infty}\frac{f'(x)}{e^x}=l$?",", then ?",\lim_{x\to+\infty}\frac{f(x)}{e^x}=l \lim_{x\to+\infty}\frac{f'(x)}{e^x}=l,"Let $f(x)$ be a  function,  the second derivative $f^{\prime\prime}(x)$ exists, and $\lim\limits_{x\to +\infty}\dfrac{f(x)}{e^x}=l$ . $c\gt0$ such that  for sufficiently large $x$ , $|f''(x)|<c|f'(x)|$ . then we have $$\lim_{x\to + \infty}\frac{f'(x)}{e^x}=l $$ converse of L'Hpital's rule ? Thank you very much for your help","Let be a  function,  the second derivative exists, and . such that  for sufficiently large , . then we have converse of L'Hpital's rule ? Thank you very much for your help",f(x) f^{\prime\prime}(x) \lim\limits_{x\to +\infty}\dfrac{f(x)}{e^x}=l c\gt0 x |f''(x)|<c|f'(x)| \lim_{x\to + \infty}\frac{f'(x)}{e^x}=l ,"['calculus', 'analysis', 'limits']"
89,Doubt about integration over a curve,Doubt about integration over a curve,,"I'm reading a paper and I have a doubt: Suppose we have a compact Riemannian manifold and an eigenfunction of the Laplace operator $\Delta u=-\lambda u$. Suppose in addition that we have the following gradient estimate $$ \frac{|\nabla u|}{\sqrt{2-u^2}}\leq \sqrt{\lambda}. $$ The paper says: Take now two points $x_1$ and $x_2$ such that $u(x_1)=0$ and $u(x_2)=1$ and connect them by the shortest geodesic $\gamma(t)$ with $|\dot\gamma(t)|=1$. Then, integrating over $\gamma$, we have \begin{align} \int_0^1\frac{du}{\sqrt{2-u^2}} & \leq \int_{\gamma}\frac{1}{\sqrt{2-u^2}}|\frac{du}{dt}|dt\\ & \leq \int_{\gamma}\sqrt{\lambda}dt\\ & \leq \sqrt{\lambda}d. \end{align} Could someone please, explain me the reason why are those the limits of integration? I think my doubt is because I'm confuse about the definition of ""integration over a curve"". My attempt was the following: I think that we should take a parametrization of $\gamma:[0,l]\to M$ and then take the composition $\varphi(t):=\arcsin(\frac{u(\gamma(t))}{\sqrt 2})$. We would get \begin{align} |\dot \varphi(t)| & =\Big|\frac{1}{\sqrt{2-u(\gamma(t))^2}}\Big\langle\nabla u_{\gamma(t)},\dot\gamma\rangle\Big|\\ & \leq \frac{|\nabla u_{\gamma(t)}|}{\sqrt{2-u(\gamma(t))^2}} \end{align} Then we use the Fundamental Theorem of Calculus and previous inequality to estimate the result \begin{align} |\varphi(l)-\varphi(0)| & =|\int_0^l\dot \varphi(s)ds|\\ & \leq \int_0^l |\dot \varphi(s)|ds\\ & \leq \int_0^l|\dot \varphi(s)|ds\\ & \leq \int_0^l\frac{|\nabla u|}{\sqrt{2-u^2}}dt\\ & \leq \int_0^l\sqrt \lambda dt. \end{align} Which would give a wrong answer. Could someone please, point out to me what I am doing wrong?","I'm reading a paper and I have a doubt: Suppose we have a compact Riemannian manifold and an eigenfunction of the Laplace operator $\Delta u=-\lambda u$. Suppose in addition that we have the following gradient estimate $$ \frac{|\nabla u|}{\sqrt{2-u^2}}\leq \sqrt{\lambda}. $$ The paper says: Take now two points $x_1$ and $x_2$ such that $u(x_1)=0$ and $u(x_2)=1$ and connect them by the shortest geodesic $\gamma(t)$ with $|\dot\gamma(t)|=1$. Then, integrating over $\gamma$, we have \begin{align} \int_0^1\frac{du}{\sqrt{2-u^2}} & \leq \int_{\gamma}\frac{1}{\sqrt{2-u^2}}|\frac{du}{dt}|dt\\ & \leq \int_{\gamma}\sqrt{\lambda}dt\\ & \leq \sqrt{\lambda}d. \end{align} Could someone please, explain me the reason why are those the limits of integration? I think my doubt is because I'm confuse about the definition of ""integration over a curve"". My attempt was the following: I think that we should take a parametrization of $\gamma:[0,l]\to M$ and then take the composition $\varphi(t):=\arcsin(\frac{u(\gamma(t))}{\sqrt 2})$. We would get \begin{align} |\dot \varphi(t)| & =\Big|\frac{1}{\sqrt{2-u(\gamma(t))^2}}\Big\langle\nabla u_{\gamma(t)},\dot\gamma\rangle\Big|\\ & \leq \frac{|\nabla u_{\gamma(t)}|}{\sqrt{2-u(\gamma(t))^2}} \end{align} Then we use the Fundamental Theorem of Calculus and previous inequality to estimate the result \begin{align} |\varphi(l)-\varphi(0)| & =|\int_0^l\dot \varphi(s)ds|\\ & \leq \int_0^l |\dot \varphi(s)|ds\\ & \leq \int_0^l|\dot \varphi(s)|ds\\ & \leq \int_0^l\frac{|\nabla u|}{\sqrt{2-u^2}}dt\\ & \leq \int_0^l\sqrt \lambda dt. \end{align} Which would give a wrong answer. Could someone please, point out to me what I am doing wrong?",,"['calculus', 'spectral-theory']"
90,Why does this proof of the chain rule not work?,Why does this proof of the chain rule not work?,,"Why is this proof not valid? Here is my ""rigorized"" version: We write $$\frac{d}{dx}f\big(g(x)\big)=\lim_{b\to a}\frac{f\big(g(b)\big)-f\big(g(a)\big)}{b-a}=\lim_{b\to a}\left[\frac{f\big(g(b)\big)-f\big(g(a)\big)}{g(b)-g(a)}\cdot\frac{g(b)-g(a)}{b-a}\right]$$ Now we split the limit up to get $$\lim_{b\to a}\left[\frac{f\big(g(b)\big)-f\big(g(a)\big)}{g(b)-g(a)}\right]\cdot\lim_{b\to a}\left[\frac{g(b)-g(a)}{b-a}\right]$$ In the first limit, we can set $y=g(x)$. Then by differentiability, and hence continuity, of $g$, we have $y\to g(a)$ as $x\to a$. Therefore the first limit can be expressed as $$\lim_{y\to g(a)}\left[\frac{f\big(y\big)-f\big(g(a)\big)}{y-g(a)}\right]$$ So we get, by definition of the derivative, $$\frac{d}{dx}f\big(g(x)\big)=f'\big(g(x)\big)\,g'(x)$$ There are two objections given to this proof. The first is that one cannot multiply by the quantity $\big(g(b)-g(a)\big)/\big(g(b)-g(a)\big)$ as $g$ may be constant around $x=a$ and the expression would then be undefined. However, it seems obvious that we can simply consider the case of $g$ constant separately (and the case of $g'$ not defined due to 'infinite oscillations', the difficulty cited in the wiki article), where it is easily seen that the formula is valid. (Indeed, this exact point is made in the comments under the answer to this question .) The second is that the limit substitution is not justified. I don't understand this either. That limit rule (in which such substitutions are allowed for continuous functions) could easily be proven, but that has nothing to do with the chain rule. This is the objection given in an answer to this question. I found three other relevant sources. The first proof in the Wikipedia article explicitly avoids the proof above on the basis that the expression noted may be undefined. In the last page of this PDF the author offers students $3$ points if they can explain why the argument I gave above is a flawed proof. In this PDF the author similarly showcases the flawed proof, and then moves on to the 'real' proof. The 'real' proofs are, shall we say, 'not pretty'. I am wondering if this salvaged version of the intuitive proof really does not work, and why? EDIT: THEORETICAL ADDENDUM FOR THE CASE OF INFINITE OSCILLATIONS Suppose that $g(a)=g(b)$ for infinitely many $b$ in all neighborhoods of $a$. Then I claim: if $g'(a)$ is defined, it must equal $0$. Proof. The quantity $$\frac{g(x)-g(a)}{x-a}$$ can be made equal to $0$ by picking an appropriate $b$ in the interval $(a-\delta,a)\cup (a,a+\delta)$ regardless of how small $\delta>0$ is. Therefore there does not exist, for every $\epsilon>0$, a $\delta>0$ such that the difference quotient is always within $\epsilon$ of any limiting value not equal to $0$. For the limit $L\ne 0$, take $\epsilon=|L|/2$. As the limit must exist by hypothesis, it exists and equals $0$. Next I claim: the chain rule is valid in this case. Proof. The chain rule formula returns $f'(g(a))g'(a)=f'(g(a))\cdot 0=0$ in this case. We prove that, in fact, the derivative is equal to zero. Because $g'(a)=0$, we can make $$|g(x)-g(a)|<\epsilon |x-a|$$ true for any $\epsilon>0$ by picking an appropriate $\delta>0$. Then suppose that $f'(g(a))=c$. We have $$|f(x)-f(g(a))|<\max\{|c+\epsilon|,|c-\epsilon|\}|x-g(a)|$$ for any $\epsilon>0$ when $x$ is sufficiently close to $g(a)$. So make $|x-a|$ sufficiently small for all conditions. Then we have $$|f(g(x))-f(g(a))|<\max\{|c+\epsilon|,|c-\epsilon|\}|g(x)-g(a)|$$ $$\frac{|f(g(x))-f(g(a))|}{|x-a|}<\max\{|c+\epsilon|,|c-\epsilon|\}\frac{|g(x)-g(a)|}{|x-a|}$$ But $$\frac{|g(x)-g(a)|}{|x-a|}<\epsilon^*$$ ($\epsilon^*$ the first epsilon) so we have $$\left|\frac{f(g(x))-f(g(a))}{x-a}\right|<\epsilon^*\max\{|c+\epsilon|,|c-\epsilon|\}$$ Therefore the difference quotient can be made arbitrarily small, and hence $$\frac{d}{dx}\left(f\big(g(x)\big)\right)=0$$ as was to be shown.","Why is this proof not valid? Here is my ""rigorized"" version: We write $$\frac{d}{dx}f\big(g(x)\big)=\lim_{b\to a}\frac{f\big(g(b)\big)-f\big(g(a)\big)}{b-a}=\lim_{b\to a}\left[\frac{f\big(g(b)\big)-f\big(g(a)\big)}{g(b)-g(a)}\cdot\frac{g(b)-g(a)}{b-a}\right]$$ Now we split the limit up to get $$\lim_{b\to a}\left[\frac{f\big(g(b)\big)-f\big(g(a)\big)}{g(b)-g(a)}\right]\cdot\lim_{b\to a}\left[\frac{g(b)-g(a)}{b-a}\right]$$ In the first limit, we can set $y=g(x)$. Then by differentiability, and hence continuity, of $g$, we have $y\to g(a)$ as $x\to a$. Therefore the first limit can be expressed as $$\lim_{y\to g(a)}\left[\frac{f\big(y\big)-f\big(g(a)\big)}{y-g(a)}\right]$$ So we get, by definition of the derivative, $$\frac{d}{dx}f\big(g(x)\big)=f'\big(g(x)\big)\,g'(x)$$ There are two objections given to this proof. The first is that one cannot multiply by the quantity $\big(g(b)-g(a)\big)/\big(g(b)-g(a)\big)$ as $g$ may be constant around $x=a$ and the expression would then be undefined. However, it seems obvious that we can simply consider the case of $g$ constant separately (and the case of $g'$ not defined due to 'infinite oscillations', the difficulty cited in the wiki article), where it is easily seen that the formula is valid. (Indeed, this exact point is made in the comments under the answer to this question .) The second is that the limit substitution is not justified. I don't understand this either. That limit rule (in which such substitutions are allowed for continuous functions) could easily be proven, but that has nothing to do with the chain rule. This is the objection given in an answer to this question. I found three other relevant sources. The first proof in the Wikipedia article explicitly avoids the proof above on the basis that the expression noted may be undefined. In the last page of this PDF the author offers students $3$ points if they can explain why the argument I gave above is a flawed proof. In this PDF the author similarly showcases the flawed proof, and then moves on to the 'real' proof. The 'real' proofs are, shall we say, 'not pretty'. I am wondering if this salvaged version of the intuitive proof really does not work, and why? EDIT: THEORETICAL ADDENDUM FOR THE CASE OF INFINITE OSCILLATIONS Suppose that $g(a)=g(b)$ for infinitely many $b$ in all neighborhoods of $a$. Then I claim: if $g'(a)$ is defined, it must equal $0$. Proof. The quantity $$\frac{g(x)-g(a)}{x-a}$$ can be made equal to $0$ by picking an appropriate $b$ in the interval $(a-\delta,a)\cup (a,a+\delta)$ regardless of how small $\delta>0$ is. Therefore there does not exist, for every $\epsilon>0$, a $\delta>0$ such that the difference quotient is always within $\epsilon$ of any limiting value not equal to $0$. For the limit $L\ne 0$, take $\epsilon=|L|/2$. As the limit must exist by hypothesis, it exists and equals $0$. Next I claim: the chain rule is valid in this case. Proof. The chain rule formula returns $f'(g(a))g'(a)=f'(g(a))\cdot 0=0$ in this case. We prove that, in fact, the derivative is equal to zero. Because $g'(a)=0$, we can make $$|g(x)-g(a)|<\epsilon |x-a|$$ true for any $\epsilon>0$ by picking an appropriate $\delta>0$. Then suppose that $f'(g(a))=c$. We have $$|f(x)-f(g(a))|<\max\{|c+\epsilon|,|c-\epsilon|\}|x-g(a)|$$ for any $\epsilon>0$ when $x$ is sufficiently close to $g(a)$. So make $|x-a|$ sufficiently small for all conditions. Then we have $$|f(g(x))-f(g(a))|<\max\{|c+\epsilon|,|c-\epsilon|\}|g(x)-g(a)|$$ $$\frac{|f(g(x))-f(g(a))|}{|x-a|}<\max\{|c+\epsilon|,|c-\epsilon|\}\frac{|g(x)-g(a)|}{|x-a|}$$ But $$\frac{|g(x)-g(a)|}{|x-a|}<\epsilon^*$$ ($\epsilon^*$ the first epsilon) so we have $$\left|\frac{f(g(x))-f(g(a))}{x-a}\right|<\epsilon^*\max\{|c+\epsilon|,|c-\epsilon|\}$$ Therefore the difference quotient can be made arbitrarily small, and hence $$\frac{d}{dx}\left(f\big(g(x)\big)\right)=0$$ as was to be shown.",,['calculus']
91,Most famous competition problems? [closed],Most famous competition problems? [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 10 years ago . Improve this question When I've attended math competition discussions, I've often heard people remark ""oh, this is a famous problem"" or say that it's similar to one. Most of them I've actually never heard of before. Competition books tend to have a vast list of past problems, and I haven't been able to sift through and tell which ones are supposedly well known and which ones aren't. Contest math isn't especially a priority for me, but it would be nice to at least know of certain problems I should be familiar with. Could someone give a list of some problems and technique that are part of the ""folklore"", so to speak? I'm interested in any competition topic, but to narrow the discussion, let's try to emphasize competition problems in calculus/analysis (sequences, series, integrals, etc.)","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 10 years ago . Improve this question When I've attended math competition discussions, I've often heard people remark ""oh, this is a famous problem"" or say that it's similar to one. Most of them I've actually never heard of before. Competition books tend to have a vast list of past problems, and I haven't been able to sift through and tell which ones are supposedly well known and which ones aren't. Contest math isn't especially a priority for me, but it would be nice to at least know of certain problems I should be familiar with. Could someone give a list of some problems and technique that are part of the ""folklore"", so to speak? I'm interested in any competition topic, but to narrow the discussion, let's try to emphasize competition problems in calculus/analysis (sequences, series, integrals, etc.)",,"['calculus', 'contest-math', 'big-list']"
92,Qualitative dependence of solution to second-order matrix differential equation on eigenvalues,Qualitative dependence of solution to second-order matrix differential equation on eigenvalues,,"Suppose we have a matrix differential equation in $\vec{x}(t)=\left(\begin{smallmatrix}x_{1}(t) \\ \vdots \\ x_{n}(t)\end{smallmatrix}\right)$ , such that: $$\frac{\mathrm{d}^{2}\vec{x}}{\mathrm{d}t^{2}}=-\mathbf{M}\vec{x}$$ And $\mathbf{M}$ is a real, symmetric matrix, which therefore diagonalizes to $\mathbf{M}=\mathbf{P}\mathbf{\Lambda}\mathbf{P}^{-1}$ , with $\mathbf{\Lambda}=\operatorname{diag}(\lambda_{1},\dots,\lambda_{n})$ . We can perform a change of basis such that $\vec{x}(t)=\mathbf{P}\vec{\eta}(t)$ , and therefore we have the following: $$\frac{\mathrm{d}^{2}\vec{\eta}}{\mathrm{d}t^{2}}=-\mathbf{\Lambda}\vec{\eta} \implies \frac{\mathrm{d}^{2}\eta_{i}}{\mathrm{d}t^{2}}=-\lambda_{i}\eta_{i}$$ This is easy to solve and gives us: $$\eta_{i}=\begin{cases}A_{i}\cos(\sqrt{\lambda_{i}}t)+B_{i}\sin(\sqrt{\lambda_{i}}t) & \lambda_{i} > 0 \\ A_{i}+B_{i}t & \lambda_{i} = 0 \\ A_{i}e^{\sqrt{-\lambda_{i}}t}+B_{i}e^{-\sqrt{-\lambda_{i}}t} & \lambda_{i} < 0\end{cases}$$ We can therefore solve for $\vec{x}$ by computing $\mathbf{P}\vec{\eta}$ . However, I am then asked how the qualitative behaviour of $\vec{x}$ depends upon the eigenvalues of $\mathbf{M}$ . I am having trouble understanding what is required of me; we have the qualitative behaviour of $\eta_{i}$ and it's dependence upon $\lambda_{i}$ , but without more explicit knowledge of the change of basis matrix $\mathbf{P}$ I'm not sure what we can say about $\vec{x}$ ? The question then requires us to focus on the $2$ -dimensional coupled system of differential equations: \begin{align} \frac{\mathrm{d}^{2}x_{1}}{\mathrm{d}t^{2}}&=-kx_{1}-lx_{2} \\ \frac{\mathrm{d}^{2}x_{2}}{\mathrm{d}t^{2}}&= -lx_{1} - kx_{2} \end{align} We can therefore construct $\mathbf{M}=\left(\begin{smallmatrix}k & l \\ l & k\end{smallmatrix}\right)$ , which diagonalizes to give $\mathbf{\Lambda}=\operatorname{diag}(k-l,k+l)$ and $\mathbf{P}=\frac{1}{\sqrt{2}}\left(\begin{smallmatrix}-1 & 1 \\ 1 & 1\end{smallmatrix}\right)$ . Now clearly the solution can be worked out explicitly using the method above and then the qualitative behaviour can be explored, but is there anything we can say without explicit calculation?","Suppose we have a matrix differential equation in , such that: And is a real, symmetric matrix, which therefore diagonalizes to , with . We can perform a change of basis such that , and therefore we have the following: This is easy to solve and gives us: We can therefore solve for by computing . However, I am then asked how the qualitative behaviour of depends upon the eigenvalues of . I am having trouble understanding what is required of me; we have the qualitative behaviour of and it's dependence upon , but without more explicit knowledge of the change of basis matrix I'm not sure what we can say about ? The question then requires us to focus on the -dimensional coupled system of differential equations: We can therefore construct , which diagonalizes to give and . Now clearly the solution can be worked out explicitly using the method above and then the qualitative behaviour can be explored, but is there anything we can say without explicit calculation?","\vec{x}(t)=\left(\begin{smallmatrix}x_{1}(t) \\ \vdots \\ x_{n}(t)\end{smallmatrix}\right) \frac{\mathrm{d}^{2}\vec{x}}{\mathrm{d}t^{2}}=-\mathbf{M}\vec{x} \mathbf{M} \mathbf{M}=\mathbf{P}\mathbf{\Lambda}\mathbf{P}^{-1} \mathbf{\Lambda}=\operatorname{diag}(\lambda_{1},\dots,\lambda_{n}) \vec{x}(t)=\mathbf{P}\vec{\eta}(t) \frac{\mathrm{d}^{2}\vec{\eta}}{\mathrm{d}t^{2}}=-\mathbf{\Lambda}\vec{\eta} \implies \frac{\mathrm{d}^{2}\eta_{i}}{\mathrm{d}t^{2}}=-\lambda_{i}\eta_{i} \eta_{i}=\begin{cases}A_{i}\cos(\sqrt{\lambda_{i}}t)+B_{i}\sin(\sqrt{\lambda_{i}}t) & \lambda_{i} > 0 \\
A_{i}+B_{i}t & \lambda_{i} = 0 \\
A_{i}e^{\sqrt{-\lambda_{i}}t}+B_{i}e^{-\sqrt{-\lambda_{i}}t} & \lambda_{i} < 0\end{cases} \vec{x} \mathbf{P}\vec{\eta} \vec{x} \mathbf{M} \eta_{i} \lambda_{i} \mathbf{P} \vec{x} 2 \begin{align}
\frac{\mathrm{d}^{2}x_{1}}{\mathrm{d}t^{2}}&=-kx_{1}-lx_{2} \\
\frac{\mathrm{d}^{2}x_{2}}{\mathrm{d}t^{2}}&= -lx_{1} - kx_{2}
\end{align} \mathbf{M}=\left(\begin{smallmatrix}k & l \\ l & k\end{smallmatrix}\right) \mathbf{\Lambda}=\operatorname{diag}(k-l,k+l) \mathbf{P}=\frac{1}{\sqrt{2}}\left(\begin{smallmatrix}-1 & 1 \\ 1 & 1\end{smallmatrix}\right)","['calculus', 'linear-algebra', 'ordinary-differential-equations', 'systems-of-equations']"
93,How to solve this integral equation?,How to solve this integral equation?,,"Solve this integral equation: $$ {{\rm e}^{{\rm i}k\,\sqrt{\vphantom{\Large A}\,r^{2} + z^{2}\,}\,} \over \sqrt{\vphantom{\large A}r^{2} + z^{2}\,}} = \int_0^{\infty}{\rm K}_{0}\left(\lambda r\right) \cos\left(\,\sqrt{\vphantom{\Large A}\,\lambda^{2} + k^{2}\,}\,z\right) {\rm f}\left(\lambda\right)\,{\rm d}\lambda $$ where ${\rm f}\left(\lambda\right)$ is the unknown function. ${\rm K}_{0}\left(\lambda r\right)$ is the modified Bessel function of the second kind of order zero. $r > 0$. Could anyone give some hints ?. Thanks !.","Solve this integral equation: $$ {{\rm e}^{{\rm i}k\,\sqrt{\vphantom{\Large A}\,r^{2} + z^{2}\,}\,} \over \sqrt{\vphantom{\large A}r^{2} + z^{2}\,}} = \int_0^{\infty}{\rm K}_{0}\left(\lambda r\right) \cos\left(\,\sqrt{\vphantom{\Large A}\,\lambda^{2} + k^{2}\,}\,z\right) {\rm f}\left(\lambda\right)\,{\rm d}\lambda $$ where ${\rm f}\left(\lambda\right)$ is the unknown function. ${\rm K}_{0}\left(\lambda r\right)$ is the modified Bessel function of the second kind of order zero. $r > 0$. Could anyone give some hints ?. Thanks !.",,"['calculus', 'integral-equations']"
94,Open Problem in Fixed Point Theory [Prize],Open Problem in Fixed Point Theory [Prize],,"This open problem appeared on the bulletins of Evans Hall at Berkeley this week. I hope this doesn't violate StackExchange policy ( the solution carries a $500 prize ), but I thought why not re-post it here! Definition 1 Let $x$ and $y$ be i.i.d. random variables distributed according to $F$ with continuous support over $R^{+}$.  Let continuously differentiable function $g(x, y)$ exhibit: 1. Symmetry:  $\forall x, y \in [0, \infty), g(x,y) = g(y, x).$ 2 Synergy: $\forall x, y \in [0, \infty), g(x, y) > x+y.$ 3 Increasing Differences: $\forall x, y \in [0, \infty), \frac{d}{dx}g(x,y) > 1.$ 4 Initial Coincidence: $\forall x \in [0, \infty), g(x,0) = x.$ Conjecture 2 (Fixed Point) $\exists \gamma \in (\frac{1}{2} , 1) : \gamma = E[\frac{x}{x+y} : g(x, y)\gamma > x$ AND $g(x,y)(1-\gamma) > y]. $ Remark 3 Conditions 1 and 2 on $g$ are required.  Optional conditions 3 and 4 reduce difficulty but a solution that does not require them would be superior. Remark 4 Contest ends on 5/31/13.  If more than one correct solution is submitted, the first will be selected.  If a counterexample is discovered, a prize will also be awarded. Contact 5 Justin Tumlinson (Ifo Institute for Econonmic Research at Munich) [emailprotected] John Morgan (U.C. Berkeley) [emailprotected]","This open problem appeared on the bulletins of Evans Hall at Berkeley this week. I hope this doesn't violate StackExchange policy ( the solution carries a $500 prize ), but I thought why not re-post it here! Definition 1 Let $x$ and $y$ be i.i.d. random variables distributed according to $F$ with continuous support over $R^{+}$.  Let continuously differentiable function $g(x, y)$ exhibit: 1. Symmetry:  $\forall x, y \in [0, \infty), g(x,y) = g(y, x).$ 2 Synergy: $\forall x, y \in [0, \infty), g(x, y) > x+y.$ 3 Increasing Differences: $\forall x, y \in [0, \infty), \frac{d}{dx}g(x,y) > 1.$ 4 Initial Coincidence: $\forall x \in [0, \infty), g(x,0) = x.$ Conjecture 2 (Fixed Point) $\exists \gamma \in (\frac{1}{2} , 1) : \gamma = E[\frac{x}{x+y} : g(x, y)\gamma > x$ AND $g(x,y)(1-\gamma) > y]. $ Remark 3 Conditions 1 and 2 on $g$ are required.  Optional conditions 3 and 4 reduce difficulty but a solution that does not require them would be superior. Remark 4 Contest ends on 5/31/13.  If more than one correct solution is submitted, the first will be selected.  If a counterexample is discovered, a prize will also be awarded. Contact 5 Justin Tumlinson (Ifo Institute for Econonmic Research at Munich) [emailprotected] John Morgan (U.C. Berkeley) [emailprotected]",,"['calculus', 'analysis', 'economics']"
95,What is the name of the vertical bar in $(x^2+1)\vert_{x = 4}$ or $\left.\left(\frac{x^3}{3}+x+c\right) \right\vert_0^4$?,What is the name of the vertical bar in  or ?,(x^2+1)\vert_{x = 4} \left.\left(\frac{x^3}{3}+x+c\right) \right\vert_0^4,"I've always wanted to know what the name of the vertical bar in these examples was: $f(x)=(x^2+1)\vert_{x = 4}$ (I know this means evaluate $x$ at $4$ ) $\int_0^4 (x^2+1) \,dx = \left.\left(\frac{x^3}{3}+x+c\right) \right\vert_0^4$ (and I know this means that you would then evaluate at $x=0$ and $x=4$ , then subtract $F(4)-F(0)$ if finding the net signed area) I know it seems trivial, but it's something I can't really seem to find when I go googling and the question came up in my calc class last night and no one seemed to know. Also, for bonus internets; What is the name of the horizontal bar in $\frac{x^3}{3}$ ? Is that called an obelus?","I've always wanted to know what the name of the vertical bar in these examples was: (I know this means evaluate at ) (and I know this means that you would then evaluate at and , then subtract if finding the net signed area) I know it seems trivial, but it's something I can't really seem to find when I go googling and the question came up in my calc class last night and no one seemed to know. Also, for bonus internets; What is the name of the horizontal bar in ? Is that called an obelus?","f(x)=(x^2+1)\vert_{x = 4} x 4 \int_0^4 (x^2+1) \,dx = \left.\left(\frac{x^3}{3}+x+c\right) \right\vert_0^4 x=0 x=4 F(4)-F(0) \frac{x^3}{3}","['soft-question', 'terminology']"
96,Evaluating $\int \limits_{a}^{\infty} \frac{\exp\left(-ax\right)}{\log(x)\left(c+x\right)^2}dx$,Evaluating,\int \limits_{a}^{\infty} \frac{\exp\left(-ax\right)}{\log(x)\left(c+x\right)^2}dx,I have the following integral $$\int \limits_{a}^{\infty} \frac{\exp\left(-ax\right)}{\log(x)\left(c+x\right)^2} dx$$ that I do not know how to evaluate. Could you please give me a hint? Thanks in advance.,I have the following integral $$\int \limits_{a}^{\infty} \frac{\exp\left(-ax\right)}{\log(x)\left(c+x\right)^2} dx$$ that I do not know how to evaluate. Could you please give me a hint? Thanks in advance.,,"['calculus', 'integration']"
97,"How many method to evaluate the integral $\int_{0}^{1} \frac{\ln ^{n}(1-x)}{x} d x , \textrm{ where }n\in N?$",How many method to evaluate the integral,"\int_{0}^{1} \frac{\ln ^{n}(1-x)}{x} d x , \textrm{ where }n\in N?","$$ \begin{aligned} \int_{0}^{1} \frac{\ln (1-x)}{x} d x &\stackrel{x \rightarrow 1-x}{=} \int_{0}^{1} \frac{\ln x}{1-x} d x \\ &=\sum_{k=0}^{\infty} \int_{0}^{1} x^{k} \ln x d x \\ &=\sum_{k=0}^{\infty} \int_{0}^{1} \ln x d\left(\frac{x^{k+1}}{k+1}\right) \\ & \stackrel{I B P}{=} \sum_{k=0}^{\infty}\left(\left[\frac{x^{k+1} \ln x}{k+1}\right]_{0}^{1}-\int_{0}^1{\frac{x^{k}}{k+1}} d x\right) \\ &=-\sum_{k=0}^{\infty} \frac{1}{(k+1)^{2}} \\ &=-\zeta(2) \end{aligned} $$ Similarly, $$ \begin{aligned} \int_{0}^{1} \frac{\ln ^{2}(1-x)}{x} d x  \stackrel{x\mapsto 1-x}{=}  & \int_{0}^{1} \frac{\ln ^{2} x}{1-x} d x \\ =& \sum_{k=0}^{\infty} \int_{0}^{1} x^{k} \ln ^{2} x d x \\ \stackrel{IBP}{=} & \sum_{k=0}^{\infty} \frac{1}{k+1}\left(\left[x^{k+1} \ln ^{2} x\right]_{0}^{1}-\int_{0}^{1} 2 x^{k} \ln x d x\right) \\ \stackrel{IBP}{=}&-2 \sum_{k=0}^{\infty} \frac{1}{k+1}\left(\left[\frac{x^{k+1}\ln x}{k+1}\right]_{0}^{1}-\int_{0}^{1} \frac{x^{k}}{k+1} d x\right) \\ =& 2 \sum_{k=0}^{\infty} \frac{1}{(k+1)^{3}} \\ =& 2 \zeta(3) \end{aligned} $$ Replacing the power of $\ln x$ by $n$ and performing integration by parts by $n$ times yields $$ \begin{aligned}\int_{0}^{1} \frac{\ln ^{n}(1-x)}{x} d x &\stackrel{x\mapsto 1-x}{=} \int_{0}^{1} \frac{\ln ^{n} x}{1-x} d x\\ &\qquad\qquad \vdots \\&= (-1)(-2)(-3) \cdots(-n) \sum_{k=0}^{\infty} \frac{1}{(k+1)^{n+1}}\\&= (-1)^{n} n ! \zeta (n+1)\end{aligned} $$ My Question Is there an alternative method to evaluate the integral?","Similarly, Replacing the power of by and performing integration by parts by times yields My Question Is there an alternative method to evaluate the integral?","
\begin{aligned}
\int_{0}^{1} \frac{\ln (1-x)}{x} d x &\stackrel{x \rightarrow 1-x}{=} \int_{0}^{1} \frac{\ln x}{1-x} d x \\
&=\sum_{k=0}^{\infty} \int_{0}^{1} x^{k} \ln x d x \\
&=\sum_{k=0}^{\infty} \int_{0}^{1} \ln x d\left(\frac{x^{k+1}}{k+1}\right) \\
& \stackrel{I B P}{=} \sum_{k=0}^{\infty}\left(\left[\frac{x^{k+1} \ln x}{k+1}\right]_{0}^{1}-\int_{0}^1{\frac{x^{k}}{k+1}} d x\right) \\
&=-\sum_{k=0}^{\infty} \frac{1}{(k+1)^{2}} \\
&=-\zeta(2)
\end{aligned}
 
\begin{aligned}
\int_{0}^{1} \frac{\ln ^{2}(1-x)}{x} d x 
\stackrel{x\mapsto 1-x}{=}  & \int_{0}^{1} \frac{\ln ^{2} x}{1-x} d x \\
=& \sum_{k=0}^{\infty} \int_{0}^{1} x^{k} \ln ^{2} x d x \\
\stackrel{IBP}{=} & \sum_{k=0}^{\infty} \frac{1}{k+1}\left(\left[x^{k+1} \ln ^{2} x\right]_{0}^{1}-\int_{0}^{1} 2 x^{k} \ln x d x\right) \\
\stackrel{IBP}{=}&-2 \sum_{k=0}^{\infty} \frac{1}{k+1}\left(\left[\frac{x^{k+1}\ln x}{k+1}\right]_{0}^{1}-\int_{0}^{1} \frac{x^{k}}{k+1} d x\right) \\
=& 2 \sum_{k=0}^{\infty} \frac{1}{(k+1)^{3}} \\
=& 2 \zeta(3)
\end{aligned}
 \ln x n n 
\begin{aligned}\int_{0}^{1} \frac{\ln ^{n}(1-x)}{x} d x &\stackrel{x\mapsto 1-x}{=} \int_{0}^{1} \frac{\ln ^{n} x}{1-x} d x\\ &\qquad\qquad \vdots \\&= (-1)(-2)(-3) \cdots(-n) \sum_{k=0}^{\infty} \frac{1}{(k+1)^{n+1}}\\&= (-1)^{n} n ! \zeta (n+1)\end{aligned}
","['calculus', 'integration', 'definite-integrals', 'improper-integrals']"
98,"How to Evaluate $\int\frac1{x \ln x+ 7 \ln x} \,\mathrm dx$",How to Evaluate,"\int\frac1{x \ln x+ 7 \ln x} \,\mathrm dx",I have tried many methods but do not know how to integrate this: $$ \int \frac{1}{x\ln x + 7\ln x} dx $$ with respect to x.,I have tried many methods but do not know how to integrate this: $$ \int \frac{1}{x\ln x + 7\ln x} dx $$ with respect to x.,,"['calculus', 'integration', 'indefinite-integrals']"
99,How to find the following limit algebraically?,How to find the following limit algebraically?,,"I've been trying to answer this for a while and I know it's a simple question relative to most questions that are posted here. $$ \lim_{x\rightarrow -2}\: \frac{x^4+5x^3+6x^2}{x^2(x+1)-4(x+1)} $$ If we substitute -2 for $ x $ we get $ 0/0 $, an indeterminate form. I figured that the denominator can be rewritten as $ (x^2-4)(x+1) $. And then I tried to factor something in the numerator but couldn't see anything interesting. How do I find the limit algebraically? I know that the answer is supposed to be 1, but I don't know how they got there. Thanks in advance!","I've been trying to answer this for a while and I know it's a simple question relative to most questions that are posted here. $$ \lim_{x\rightarrow -2}\: \frac{x^4+5x^3+6x^2}{x^2(x+1)-4(x+1)} $$ If we substitute -2 for $ x $ we get $ 0/0 $, an indeterminate form. I figured that the denominator can be rewritten as $ (x^2-4)(x+1) $. And then I tried to factor something in the numerator but couldn't see anything interesting. How do I find the limit algebraically? I know that the answer is supposed to be 1, but I don't know how they got there. Thanks in advance!",,"['calculus', 'limits']"
