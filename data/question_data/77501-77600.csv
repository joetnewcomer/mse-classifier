,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Show that a matrix has positive determinant,Show that a matrix has positive determinant,,"For a natural number $i>0$, let $p_i$ be the $i$th prime number, that is, $p_1=2, p_2=3, p_3=5,...$. Show that for all $n$, the following matrix has positive determinant $$ \begin{pmatrix} 1^{p_1} & 2^{p_1} & \cdots & (n-1)^{p_1} & n^{p_1} \\ 1^{p_2} & 2^{p_2} & \cdots & (n-1)^{p_2} & n^{p_2} \\ \vdots  & \vdots  & \vdots  & \vdots  & \vdots  \\ 1^{p_n} & 2^{p_n} & \cdots & (n-1)^{p_n} & n^{p_n}\\ \end{pmatrix} $$ The hint provided is to use the fact that the polynomial $P(x)=a_nx^{p_n}+a_{n-1}x^{p_{n-1}} + \cdots + a_1x^{p_1}+a_0x^{p_1}$ has at most $n-1$ positive roots for all real constants. But I don't know how to use the fact to prove the question. Can anyone help me? The source of the problem is here , question $2.5$.","For a natural number $i>0$, let $p_i$ be the $i$th prime number, that is, $p_1=2, p_2=3, p_3=5,...$. Show that for all $n$, the following matrix has positive determinant $$ \begin{pmatrix} 1^{p_1} & 2^{p_1} & \cdots & (n-1)^{p_1} & n^{p_1} \\ 1^{p_2} & 2^{p_2} & \cdots & (n-1)^{p_2} & n^{p_2} \\ \vdots  & \vdots  & \vdots  & \vdots  & \vdots  \\ 1^{p_n} & 2^{p_n} & \cdots & (n-1)^{p_n} & n^{p_n}\\ \end{pmatrix} $$ The hint provided is to use the fact that the polynomial $P(x)=a_nx^{p_n}+a_{n-1}x^{p_{n-1}} + \cdots + a_1x^{p_1}+a_0x^{p_1}$ has at most $n-1$ positive roots for all real constants. But I don't know how to use the fact to prove the question. Can anyone help me? The source of the problem is here , question $2.5$.",,"['matrices', 'contest-math', 'determinant']"
1,How can you verify that a 3 by 3 unimodular matrix generates an infinite number of Fermat near misses?,How can you verify that a 3 by 3 unimodular matrix generates an infinite number of Fermat near misses?,,"I am interested in finding 3 by 3 Ramanujan-Hirschhorn matrices. By definition, a Ramanujan-Hirschhorn matrix is a 3 by 3 matrix which produces an infinite number of Fermat near misses. Ramanujan in his ""lost notebook"" makes the amazing claim that if the integers a_n, b_n, c_n  are defined by: $$\sum_{x\ge0}{a_n x^n}=\frac{1 + 53x + 9x^2}{1 − 82x − 82x^2 + x^3}$$ $$\sum_{x\ge0}{b_n x^n}=\frac{2 - 26x - 12x^2}{1 − 82x − 82x^2 + x^3}$$ $$\sum_{x\ge0}{c_n x^n}=\frac{2 + 8x - 10x^2}{1 − 82x − 82x^2 + x^3}$$ then $a_n^3 + b_n^3 = c_n^3 + (-1)^n$ Two proofs of this claim and a plausible explanation of how Ramanujan may have been led to it have been given by Michael Hirschhorn (1993 -94 ). Indeed, Hirschhorn showed that the sequences  {a_n} , {b_n} and  {c_n}  are given by $$ \begin{array}{}  \begin{bmatrix}     a_n \\\     b_n \\\     c_n      \end{bmatrix} & = {\begin{bmatrix}                    63 & 104 & −68 \\\                    64 & 104 & −67 \\\                    80 & 131 & −85                     \end{bmatrix}}^n & \cdot &  \begin{bmatrix}                                          1 \\\                                          2 \\\                                          2                                          \end{bmatrix} \end{array} $$ Notice that the matrix above is unimodular and it produces an infinite number of triples of Fermat near misses when multiplied on the right by the column vector (1 2 2 ).  In 2013 Tito Piezas , with the help of Mathematica's GeneratingFunction command was able to find: $$ \begin{aligned} \sum_{n=0}^\infty a_n x^n &= \frac{-9(417-5602x+x^2)}{R_2}\\\ \sum_{n=0}^\infty b_n x^n &= \frac{8(-566-11315x+x^2)}{R_2}\\\ \sum_{n=0}^\infty c_n x^n &= \frac{-6(877+6898x+x^2)}{R_2} \end{aligned} $$ where $R_2 = -1+184899x-184899x^2+x^3$ and, $a_n^3+b_n^3 = c_n^3 + 1$ which is a sum of cubes identity analogous to Ramanujan's. I used a method very similar to Michael Hirschhorn's method to derive a 3 by 3 unimodular matrix associated with the above Tito Piezas sum of cubes identity.  The unimodular matrix that i have found is: $$ \begin{array} {} \begin{bmatrix}     a_n \\\     b_n \\\     c_n      \end{bmatrix} &=& {\begin{bmatrix}                    156625 & 115992 & −79656 \\\                    189000 & 139969 & −96120 \\\                    219624 & 162648 & −111695                     \end{bmatrix}}^n \cdot \begin{bmatrix}                                          3753 \\\                                          4528 \\\                                          5262                                          \end{bmatrix}  \end{array} $$ As you can see the above matrix is also unimodular and according to Tito Piezas generates an infinite number of Fermat near miss triples when multiplied on the right by the column vector (3753 4528 5262 ). But how can i verify the claim that the above matrix produces an infinite number of Fermat near miss triples?  Any suggestions or ideas ? More importantly, how can I derive or compute another Ramanujan-Hirschhorn matrix different from the one given above ? Is there a systematic way of deriving these matrices ? There is probably an infinite number of them !! Does anyone know any other Ramanujan-Hirschhorn matrices ?","I am interested in finding 3 by 3 Ramanujan-Hirschhorn matrices. By definition, a Ramanujan-Hirschhorn matrix is a 3 by 3 matrix which produces an infinite number of Fermat near misses. Ramanujan in his ""lost notebook"" makes the amazing claim that if the integers a_n, b_n, c_n  are defined by: then Two proofs of this claim and a plausible explanation of how Ramanujan may have been led to it have been given by Michael Hirschhorn (1993 -94 ). Indeed, Hirschhorn showed that the sequences  {a_n} , {b_n} and  {c_n}  are given by Notice that the matrix above is unimodular and it produces an infinite number of triples of Fermat near misses when multiplied on the right by the column vector (1 2 2 ).  In 2013 Tito Piezas , with the help of Mathematica's GeneratingFunction command was able to find: where and, which is a sum of cubes identity analogous to Ramanujan's. I used a method very similar to Michael Hirschhorn's method to derive a 3 by 3 unimodular matrix associated with the above Tito Piezas sum of cubes identity.  The unimodular matrix that i have found is: As you can see the above matrix is also unimodular and according to Tito Piezas generates an infinite number of Fermat near miss triples when multiplied on the right by the column vector (3753 4528 5262 ). But how can i verify the claim that the above matrix produces an infinite number of Fermat near miss triples?  Any suggestions or ideas ? More importantly, how can I derive or compute another Ramanujan-Hirschhorn matrix different from the one given above ? Is there a systematic way of deriving these matrices ? There is probably an infinite number of them !! Does anyone know any other Ramanujan-Hirschhorn matrices ?","\sum_{x\ge0}{a_n x^n}=\frac{1 + 53x + 9x^2}{1 − 82x − 82x^2 + x^3} \sum_{x\ge0}{b_n x^n}=\frac{2 - 26x - 12x^2}{1 − 82x − 82x^2 + x^3} \sum_{x\ge0}{c_n x^n}=\frac{2 + 8x - 10x^2}{1 − 82x − 82x^2 + x^3} a_n^3 + b_n^3 = c_n^3 + (-1)^n  \begin{array}{} 
\begin{bmatrix}
    a_n \\\
    b_n \\\
    c_n 
    \end{bmatrix} & = {\begin{bmatrix}
                   63 & 104 & −68 \\\
                   64 & 104 & −67 \\\
                   80 & 131 & −85 
                   \end{bmatrix}}^n & \cdot &  \begin{bmatrix}
                                         1 \\\
                                         2 \\\
                                         2 
                                        \end{bmatrix} \end{array}
 
\begin{aligned}
\sum_{n=0}^\infty a_n x^n &= \frac{-9(417-5602x+x^2)}{R_2}\\\
\sum_{n=0}^\infty b_n x^n &= \frac{8(-566-11315x+x^2)}{R_2}\\\
\sum_{n=0}^\infty c_n x^n &= \frac{-6(877+6898x+x^2)}{R_2}
\end{aligned}
 R_2 = -1+184899x-184899x^2+x^3 a_n^3+b_n^3 = c_n^3 + 1  \begin{array} {}
\begin{bmatrix}
    a_n \\\
    b_n \\\
    c_n 
    \end{bmatrix} &=& {\begin{bmatrix}
                   156625 & 115992 & −79656 \\\
                   189000 & 139969 & −96120 \\\
                   219624 & 162648 & −111695 
                   \end{bmatrix}}^n \cdot \begin{bmatrix}
                                         3753 \\\
                                         4528 \\\
                                         5262 
                                        \end{bmatrix} 
\end{array} ","['matrices', 'number-theory', 'computational-mathematics', 'unimodular-matrices']"
2,Characterization of Volumes of Lattice Cubes,Characterization of Volumes of Lattice Cubes,,"Here is a problem that came up in a conversation with a professor. I do not know if he knew the answer (and told me none of it) and has since passed so I can no longer ask him about it. Let $C$ be a lattice cube in $\mathbb{R}^n$. Characterize all possible volumes for $C$. A cube is called a lattice cube if and only if every vertex has integer coordinates. I broke this proof into three cases, the last of which I am having trouble with in one direction. We will let $V(n)$ be the set of all numbers $V$ for which there exists a lattice cube of volume $V$ in dimension $n$. We will break into three cases based on the value mod 4. \begin{align*} V(2k+1)&=\{a^n:a\in\mathbb{N}\} \\ V(4k)&=\{a^\frac{n}{2}:a\in\mathbb{N}\} \\ V(4k+2)&\supseteq\{(a^2+b^2)^\frac{n}{2}:a,b\in\mathbb{N}\} \end{align*} These statements I have proven, and conjecture that the last one is an equality. I've been trying to use a collapsing dimension argument to show if I can make a cube of side length $s$ in $\mathbb{R}^{4k+2}$ then I can in $\mathbb{R}^{4k-2}$, at which point the theorem follows since I have proven the special case of $n=2$ (which is quite trivial - there is no way to write down a square whose volume is not of the specified form in $2$D. The above assertions (sans my conjecture) are proven here","Here is a problem that came up in a conversation with a professor. I do not know if he knew the answer (and told me none of it) and has since passed so I can no longer ask him about it. Let $C$ be a lattice cube in $\mathbb{R}^n$. Characterize all possible volumes for $C$. A cube is called a lattice cube if and only if every vertex has integer coordinates. I broke this proof into three cases, the last of which I am having trouble with in one direction. We will let $V(n)$ be the set of all numbers $V$ for which there exists a lattice cube of volume $V$ in dimension $n$. We will break into three cases based on the value mod 4. \begin{align*} V(2k+1)&=\{a^n:a\in\mathbb{N}\} \\ V(4k)&=\{a^\frac{n}{2}:a\in\mathbb{N}\} \\ V(4k+2)&\supseteq\{(a^2+b^2)^\frac{n}{2}:a,b\in\mathbb{N}\} \end{align*} These statements I have proven, and conjecture that the last one is an equality. I've been trying to use a collapsing dimension argument to show if I can make a cube of side length $s$ in $\mathbb{R}^{4k+2}$ then I can in $\mathbb{R}^{4k-2}$, at which point the theorem follows since I have proven the special case of $n=2$ (which is quite trivial - there is no way to write down a square whose volume is not of the specified form in $2$D. The above assertions (sans my conjecture) are proven here",,"['geometry', 'matrices', 'number-theory']"
3,"Are complex determinants for matrices possible and if so, how can they be interpreted?","Are complex determinants for matrices possible and if so, how can they be interpreted?",,"I've been asked to compute the determinant of a $3 \times 3$ matrix with complex entries. I have done so using the normal expansion along a row or column method that I would use were the entries real. My questions are: 1) is this what I should be doing? 2) I obtained a complex result - how do I interpret what this means? I've been given to understand that the absolute of the determinant of a $3 \times 3$ matrix would represent it's volume, but can a volume be complex?","I've been asked to compute the determinant of a $3 \times 3$ matrix with complex entries. I have done so using the normal expansion along a row or column method that I would use were the entries real. My questions are: 1) is this what I should be doing? 2) I obtained a complex result - how do I interpret what this means? I've been given to understand that the absolute of the determinant of a $3 \times 3$ matrix would represent it's volume, but can a volume be complex?",,"['matrices', 'complex-numbers', 'determinant']"
4,Inequality between induced matrix norms implies equality,Inequality between induced matrix norms implies equality,,"Let us consider $N_1$ and $N_2$ two induced norms on the space of the square matrices with $n\in\mathbb N^*$ rows and colums. Assume that $N_1 \leq N_2$. Is that true that $N_1 = N_2$ ? [edit] The answer is yes (see Horn and Johnson, Matrix Analysis, 5.6). The proof is based on inequalities with induced norms and the equality is obtained by considering particular rank-one matrices.","Let us consider $N_1$ and $N_2$ two induced norms on the space of the square matrices with $n\in\mathbb N^*$ rows and colums. Assume that $N_1 \leq N_2$. Is that true that $N_1 = N_2$ ? [edit] The answer is yes (see Horn and Johnson, Matrix Analysis, 5.6). The proof is based on inequalities with induced norms and the equality is obtained by considering particular rank-one matrices.",,"['matrices', 'normed-spaces']"
5,Minecraft water spreading initial arrangements,Minecraft water spreading initial arrangements,,"So, as most Minecraft players quickly learn, the best way to flood an area is to place water along a diagonal. There are of course other ways to flood the same area with the same number of buckets placed. For those not familiar with the game, we can think of this as a different sort of game. We start with a $0, 1$ matrix of size $n$ . At each step, every $0$ with at least two $1$ 's directly adjacent to it is replaced with a $1$ . Its not hard to see that this process always terminates. The objective is to end with a matrix of all $1$ 's with a minimal number of $1$ 's to start. Its not hard to see that an optimal initial arrangement is the identity matrix, and that the minimum number of starting $1$ ’s is $n$ . Of course, there are other matrices that work, such as $$\begin{pmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0\\ 0 & 0 & 0 & 1\\ 0 & 0 & 1 & 0\end{pmatrix}.$$ We quickly see that we are looking for permutation matrices, but not every permutation matrix works. For example $$\begin{pmatrix} 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ 1 & 0 & 0 & 0\\ 0 & 0 & 1 & 0\end{pmatrix}$$ does not work. The question then is this: which permutation matrices do work? Is there a nice characterization? We can think of forming ""blocks"" and then growing blocks together if they share a corner, but is there something else we can say? It appears the counter example given is some kind of forbidden configuration. Edit: it appears that there are non-permutation matrices that work, such as $$\begin{pmatrix} 1 & 0 & 1 \\ 0 & 0 & 0 \\ 1 & 0 & 0 \end{pmatrix}.$$ Regardless, we can add the assumption that we are working only with permutation matrices. Thus we only consider expansion across corners as the other type of growth cannot happen. Now if we consider starting with a permuation matrix, and grow it to its final state, we see that the final states are a bunch of disjoint squares of filled in area. We can shrink these squares to a single cell, so equivalently, we ask ourselves which permutation matrices exhibit no growth at all?","So, as most Minecraft players quickly learn, the best way to flood an area is to place water along a diagonal. There are of course other ways to flood the same area with the same number of buckets placed. For those not familiar with the game, we can think of this as a different sort of game. We start with a matrix of size . At each step, every with at least two 's directly adjacent to it is replaced with a . Its not hard to see that this process always terminates. The objective is to end with a matrix of all 's with a minimal number of 's to start. Its not hard to see that an optimal initial arrangement is the identity matrix, and that the minimum number of starting ’s is . Of course, there are other matrices that work, such as We quickly see that we are looking for permutation matrices, but not every permutation matrix works. For example does not work. The question then is this: which permutation matrices do work? Is there a nice characterization? We can think of forming ""blocks"" and then growing blocks together if they share a corner, but is there something else we can say? It appears the counter example given is some kind of forbidden configuration. Edit: it appears that there are non-permutation matrices that work, such as Regardless, we can add the assumption that we are working only with permutation matrices. Thus we only consider expansion across corners as the other type of growth cannot happen. Now if we consider starting with a permuation matrix, and grow it to its final state, we see that the final states are a bunch of disjoint squares of filled in area. We can shrink these squares to a single cell, so equivalently, we ask ourselves which permutation matrices exhibit no growth at all?","0, 1 n 0 1 1 1 1 1 n \begin{pmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0\\ 0 & 0 & 0 & 1\\ 0 & 0 & 1 & 0\end{pmatrix}. \begin{pmatrix} 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ 1 & 0 & 0 & 0\\ 0 & 0 & 1 & 0\end{pmatrix} \begin{pmatrix} 1 & 0 & 1 \\ 0 & 0 & 0 \\ 1 & 0 & 0 \end{pmatrix}.","['matrices', 'forbidden-subgraphs']"
6,Expressing diagonal matrix using elementary matrices as generators in $\operatorname{SL}(\mathcal O_K \oplus \mathfrak a)$,Expressing diagonal matrix using elementary matrices as generators in,\operatorname{SL}(\mathcal O_K \oplus \mathfrak a),"Let $K$ be a real quadratic number field, $\mathcal O_K$ its ring of integers and an $\mathfrak a \subset K$ a fractional ideal. I've read in van der Geer that the group $$\operatorname{SL}(\mathcal O_K \oplus \mathfrak a) := \left\{ \begin{pmatrix}a & b \\ c & d\end{pmatrix} \in \operatorname{SL}_2(K) : a,d \in  \mathcal O_K, b \in \mathfrak a^{-1}, c \in \mathfrak a \right\}$$ is generated by matricies of the form $$\pm \begin{pmatrix}1 & b \\ 0 & 1\end{pmatrix},\quad \pm \begin{pmatrix}1 & 0 \\ c & 1\end{pmatrix}.$$ So I tried to express \begin{pmatrix}\varepsilon & 0 \\ 0 & \varepsilon^{-1}\end{pmatrix} for $\varepsilon \in \mathcal O_K^\times$ with those matrices and was successful with an approach I developed using four matrices. This approach relied on a condition on the number field and I was not sure if it's always satisfied so I wrote a program to check that. It turned out that in $K=\mathbb Q(\sqrt{146})$ my approach doesn't work (for all squarefree $d<146$ it works in $K=\mathbb Q(\sqrt{d})$ ). Can anyone find a decomposition for $\varepsilon = 145 + 12\sqrt{146}$ being the fundamental unit and $\mathfrak a$ being any ideal which is not principal (for example $\mathfrak a:=(5,1+\sqrt{146})$ which is a prime ideal over $5$ ). By the way, the class number of $K=\mathbb Q(\sqrt{146})$ is $2$ . If you have a general approach this would help me even more. Edit: A few days after posting this question I could show that there is no way for $d \in \{ 146, 170, 194, 221, 226, 254, 290, 291, 323, 326, 365, 386, 399, 410, 434, 439, 442, 445, 485, 499, 506, 514, 530, 533, 574, 579, 582, 646, 674, 706, 723, 730, 731, 785, 786, 791, 799, 839, 842, 866, 870, 890, 898, 899, 901, 910, 914, 959, 962, 965, 970, 982, 986 \}$ to do it for all units with only four matrices. This is due to how $(\varepsilon \pm 1)$ decomposes into prime ideals (and to which ideal classes they belong). So if van der Geer is right we need here at least five matrices and I'm very curious to see how this works.","Let be a real quadratic number field, its ring of integers and an a fractional ideal. I've read in van der Geer that the group is generated by matricies of the form So I tried to express for with those matrices and was successful with an approach I developed using four matrices. This approach relied on a condition on the number field and I was not sure if it's always satisfied so I wrote a program to check that. It turned out that in my approach doesn't work (for all squarefree it works in ). Can anyone find a decomposition for being the fundamental unit and being any ideal which is not principal (for example which is a prime ideal over ). By the way, the class number of is . If you have a general approach this would help me even more. Edit: A few days after posting this question I could show that there is no way for to do it for all units with only four matrices. This is due to how decomposes into prime ideals (and to which ideal classes they belong). So if van der Geer is right we need here at least five matrices and I'm very curious to see how this works.","K \mathcal O_K \mathfrak a \subset K \operatorname{SL}(\mathcal O_K \oplus \mathfrak a) := \left\{ \begin{pmatrix}a & b \\ c & d\end{pmatrix} \in \operatorname{SL}_2(K) : a,d \in  \mathcal O_K, b \in \mathfrak a^{-1}, c \in \mathfrak a \right\} \pm \begin{pmatrix}1 & b \\ 0 & 1\end{pmatrix},\quad \pm \begin{pmatrix}1 & 0 \\ c & 1\end{pmatrix}. \begin{pmatrix}\varepsilon & 0 \\ 0 & \varepsilon^{-1}\end{pmatrix} \varepsilon \in \mathcal O_K^\times K=\mathbb Q(\sqrt{146}) d<146 K=\mathbb Q(\sqrt{d}) \varepsilon = 145 + 12\sqrt{146} \mathfrak a \mathfrak a:=(5,1+\sqrt{146}) 5 K=\mathbb Q(\sqrt{146}) 2 d \in \{ 146, 170, 194, 221, 226, 254, 290, 291, 323, 326, 365, 386, 399, 410, 434, 439, 442, 445, 485, 499, 506, 514, 530, 533, 574, 579, 582, 646, 674, 706, 723, 730, 731, 785, 786, 791, 799, 839, 842, 866, 870, 890, 898, 899, 901, 910, 914, 959, 962, 965, 970, 982, 986 \} (\varepsilon \pm 1)","['matrices', 'number-theory', 'algebraic-number-theory', 'matrix-decomposition']"
7,Analytical solution for a neat semidefinite program (SDP),Analytical solution for a neat semidefinite program (SDP),,"Let $A \in S^{n}_{+}$ be a positive semi-definite matrix with all entries being non-negative. I wonder if there is an analytical solution to the following SDP in correlation matrix $X \in S^{n}_{+}$ $$\begin{array}{ll} \underset{X \in S^{n}_{+}}{\text{minimize}} & \mbox{Tr} (A X)\\ \text{subject to} & X_{ii} = 1, \quad \forall i \in [n]\end{array}$$ Does this optimization problem have an analytical solution? To share some idea on the objective, consider the spectral decomposition of matrix $A$ $$A = \sum_{k} \lambda_k y_k y^{T}_k$$ with $\lambda_k > 0$ being the positive eigenvalues of matrix $A$ and $y_k \in \mathbb{R}^{n}$ the corresponding eigenvectors. The objective is to minimize the weighted sum of variances, i.e., $$\mbox{Tr} (A X) = \sum_{k} \lambda_k y^{T}_k X y_k$$ The dual problem is $$\begin{array}{ll} \underset{D \text{ is diagonal}}{\text{maximize}} & \mbox{Tr}(D)\\ \text{subject to} & A \succeq D\end{array}$$ The problem is so neat, so I wonder if there is any hope to have an analytical solution.","Let be a positive semi-definite matrix with all entries being non-negative. I wonder if there is an analytical solution to the following SDP in correlation matrix Does this optimization problem have an analytical solution? To share some idea on the objective, consider the spectral decomposition of matrix with being the positive eigenvalues of matrix and the corresponding eigenvectors. The objective is to minimize the weighted sum of variances, i.e., The dual problem is The problem is so neat, so I wonder if there is any hope to have an analytical solution.","A \in S^{n}_{+} X \in S^{n}_{+} \begin{array}{ll} \underset{X \in S^{n}_{+}}{\text{minimize}} & \mbox{Tr} (A X)\\ \text{subject to} & X_{ii} = 1, \quad \forall i \in [n]\end{array} A A = \sum_{k} \lambda_k y_k y^{T}_k \lambda_k > 0 A y_k \in \mathbb{R}^{n} \mbox{Tr} (A X) = \sum_{k} \lambda_k y^{T}_k X y_k \begin{array}{ll} \underset{D \text{ is diagonal}}{\text{maximize}} & \mbox{Tr}(D)\\ \text{subject to} & A \succeq D\end{array}","['matrices', 'optimization', 'convex-optimization', 'positive-semidefinite', 'semidefinite-programming']"
8,Uniqueness of an infinite system of linear ODEs,Uniqueness of an infinite system of linear ODEs,,"How to prove that $\dot{x}=ax,\space x(0)=1$ has a unique solution if $a,x$ are infinite dimensional matrices? More specifically, let $Q$ be a bounded infinitesimal generator, i.e. $Q=(q_{i,j})_{i,j\in\mathbb{N}_0}$, its entries comprise a bounded set of real numbers, all of its entries not on the main diagonal are non-negative and each of its rows sums to $0$. A family $(M_t)_{t\in[0,\infty)}$, $M_t=(m_{i,j}^{(t)})_{i,j\in\mathbb{N}_0}$ of real-valued, infinite-dimensional matrices shall be called a $Q$-system iff it is a Markov semigroup [i.e. $M_0=I$ (the identity matrix), for each $s\in[0,\infty)$, $M_s$ is stochastic and $\forall s,t\in[0,\infty),\space M_{s+t}=M_sM_t$)], it is entry-wise differentiable on $[0,\infty)$ (right differentiable at $0$) and $M_t'=QM_t$ for all $t\in[0,\infty)$. How can it be shown that if $(P_t), (\tilde{P}_t)$ are $Q$-systems, $P_t=\tilde{P}_t$ for all $0\leq t$? Here's what i've done so far in an effort to solve this problem. I have actually found a solution in a probability textbook, but could not make sense of the proof . I have investigated some properties of infinite dimensional matrices with an emphasis on infinite generators and Markov semigroups. While some useful results were obtained in the process, it hasn't brought me any closer to solving the present problem. Edit I have found the following article that answers my question. Unfortunately, it is not very clear to me, since it relies on external results.","How to prove that $\dot{x}=ax,\space x(0)=1$ has a unique solution if $a,x$ are infinite dimensional matrices? More specifically, let $Q$ be a bounded infinitesimal generator, i.e. $Q=(q_{i,j})_{i,j\in\mathbb{N}_0}$, its entries comprise a bounded set of real numbers, all of its entries not on the main diagonal are non-negative and each of its rows sums to $0$. A family $(M_t)_{t\in[0,\infty)}$, $M_t=(m_{i,j}^{(t)})_{i,j\in\mathbb{N}_0}$ of real-valued, infinite-dimensional matrices shall be called a $Q$-system iff it is a Markov semigroup [i.e. $M_0=I$ (the identity matrix), for each $s\in[0,\infty)$, $M_s$ is stochastic and $\forall s,t\in[0,\infty),\space M_{s+t}=M_sM_t$)], it is entry-wise differentiable on $[0,\infty)$ (right differentiable at $0$) and $M_t'=QM_t$ for all $t\in[0,\infty)$. How can it be shown that if $(P_t), (\tilde{P}_t)$ are $Q$-systems, $P_t=\tilde{P}_t$ for all $0\leq t$? Here's what i've done so far in an effort to solve this problem. I have actually found a solution in a probability textbook, but could not make sense of the proof . I have investigated some properties of infinite dimensional matrices with an emphasis on infinite generators and Markov semigroups. While some useful results were obtained in the process, it hasn't brought me any closer to solving the present problem. Edit I have found the following article that answers my question. Unfortunately, it is not very clear to me, since it relies on external results.",,"['matrices', 'ordinary-differential-equations', 'markov-process']"
9,Write $(x^2 + y^2 + z^2)^2 - 3 ( x^3 y + y^3 z + z^3 x)$ as a sum of (three) squares of quadratic forms,Write  as a sum of (three) squares of quadratic forms,(x^2 + y^2 + z^2)^2 - 3 ( x^3 y + y^3 z + z^3 x),"The quartic form $$(x^2 + y^2 + z^2)^2 - 3 ( x^3 y + y^3 z + z^3 x)$$ is non-negative for all real $x$, $y$, $z$, as one can check (with some effort). A theorem of Hilbert implies that there exist quadratic forms $Q_1$, $Q_2$, $Q_3$ so that $$(x^2 + y^2 + z^2)^2 - 3( x^3 y + y^3 z + z^3 x) = Q_1^2 + Q_2^2 + Q_3^2$$ I would like to find an explicit writing of the quartic forms, with rational quadratic forms $Q_i$. Maybe more than $3$ terms are necessary.","The quartic form $$(x^2 + y^2 + z^2)^2 - 3 ( x^3 y + y^3 z + z^3 x)$$ is non-negative for all real $x$, $y$, $z$, as one can check (with some effort). A theorem of Hilbert implies that there exist quadratic forms $Q_1$, $Q_2$, $Q_3$ so that $$(x^2 + y^2 + z^2)^2 - 3( x^3 y + y^3 z + z^3 x) = Q_1^2 + Q_2^2 + Q_3^2$$ I would like to find an explicit writing of the quartic forms, with rational quadratic forms $Q_i$. Maybe more than $3$ terms are necessary.",,"['matrices', 'polynomials', 'optimization', 'quadratic-forms', 'sum-of-squares-method']"
10,Prove that the nuclear norm is convex,Prove that the nuclear norm is convex,,"For an $m \times n$ matrix, $A$, the nuclear norm of $A$ is defined as $\sum_{i}\sigma_{i}(A)$ where $\sigma_{i}(A)$ is the $i^{th}$ singular value of $A$.   I've read that the nuclear norm is convex on the set of $m \times n$ matrices.   I don't see how this true and can't find a proof online.","For an $m \times n$ matrix, $A$, the nuclear norm of $A$ is defined as $\sum_{i}\sigma_{i}(A)$ where $\sigma_{i}(A)$ is the $i^{th}$ singular value of $A$.   I've read that the nuclear norm is convex on the set of $m \times n$ matrices.   I don't see how this true and can't find a proof online.",,"['matrices', 'convex-analysis', 'normed-spaces', 'matrix-norms', 'nuclear-norm']"
11,Calculate effective rank of matrix,Calculate effective rank of matrix,,How can I calculate the effective rank of a matrix? I know how to calculate the rank but not how to calculate the effective rank.,How can I calculate the effective rank of a matrix? I know how to calculate the rank but not how to calculate the effective rank.,,['matrices']
12,Let $M$ be a non-zero $3\times 3$ matrix satisfying $M^3=O$,Let  be a non-zero  matrix satisfying,M 3\times 3 M^3=O,"Let $M$ be a non-zero $3\times 3$ matrix satisfying $M^3=0$, where $0$ is the $3\times 3$ zero matrix. Then $(A)\det(\frac{1}{2}M^2+M+I)\neq0$ $(B)\det(\frac{1}{2}M^2-M+I)=0$ $(C)\det(\frac{1}{2}M^2+M+I)=0$ $(D)\det(\frac{1}{2}M^2-M+I)\neq0$ This is a multiple correct choice type question.More than one may be correct answers. My attempt: Let $M$ be a non-singular matrix,therefore its inverse exists. $M^3=0$ Multiply both sides by $M^{-1}$ to get,$M^2=0$. Again multiply both sides by $M^{-1}$ to get,$M=0$ But this is a contradiction.So $M$ is a singular matrix.Its inverse does not exist. But i dont know how to solve further.Please help me.Thanks.","Let $M$ be a non-zero $3\times 3$ matrix satisfying $M^3=0$, where $0$ is the $3\times 3$ zero matrix. Then $(A)\det(\frac{1}{2}M^2+M+I)\neq0$ $(B)\det(\frac{1}{2}M^2-M+I)=0$ $(C)\det(\frac{1}{2}M^2+M+I)=0$ $(D)\det(\frac{1}{2}M^2-M+I)\neq0$ This is a multiple correct choice type question.More than one may be correct answers. My attempt: Let $M$ be a non-singular matrix,therefore its inverse exists. $M^3=0$ Multiply both sides by $M^{-1}$ to get,$M^2=0$. Again multiply both sides by $M^{-1}$ to get,$M=0$ But this is a contradiction.So $M$ is a singular matrix.Its inverse does not exist. But i dont know how to solve further.Please help me.Thanks.",,['matrices']
13,"""Orthogonal"" Rectangular Matrix","""Orthogonal"" Rectangular Matrix",,"Is it possible to have a matrix $\mathbf B \in \mathbb R^{m\times n}$ such that it satisfies: $$\mathbf B^T\cdot\mathbf B = \mathbf I_n$$ Where $\mathbf I_n$ is the $n\times n$ identity matrix. Or in other words, is it possible to have a rectangular matrix such that it's transpose is it's left inverse? If so, what kind of matrix is this? How could you go about constructing such a matrix?","Is it possible to have a matrix $\mathbf B \in \mathbb R^{m\times n}$ such that it satisfies: $$\mathbf B^T\cdot\mathbf B = \mathbf I_n$$ Where $\mathbf I_n$ is the $n\times n$ identity matrix. Or in other words, is it possible to have a rectangular matrix such that it's transpose is it's left inverse? If so, what kind of matrix is this? How could you go about constructing such a matrix?",,"['matrices', 'inverse']"
14,Prove that $AB=BA=0$ for two idempotent matrices.,Prove that  for two idempotent matrices.,AB=BA=0,"Suppose that $A, B$ are idempotent matrices ($A^2=A$), such that $A + B$ is idempotent, prove that $AB = BA = 0$","Suppose that $A, B$ are idempotent matrices ($A^2=A$), such that $A + B$ is idempotent, prove that $AB = BA = 0$",,['matrices']
15,Something's not right about my understanding about identity matrices.,Something's not right about my understanding about identity matrices.,,"I tried the following problem. Let $A = \begin{bmatrix} \alpha & 0 \\ 0 & \beta \end{bmatrix}$ and $B = \begin{bmatrix} 0 & \gamma \\ \delta & 0 \end{bmatrix}$ . There are 2 statements: $AB - BA$ is always an invertible matrix, and $AB - BA$ is never an identity matrix. Now I'm being asked to find out whether these statements are true or false. I'm not too familiar with $\LaTeX$ and I don't have enough rep to upload images, I had to put my problem as a linked image: On calculation: I first got the product of the two matrices as: $AB = \begin{bmatrix} 0 & \alpha\gamma \\ \beta\delta & 0 \end{bmatrix}$ $BA = \begin{bmatrix} 0 & \beta\gamma \\ \alpha\delta & 0 \end{bmatrix}$ $AB - BA = \begin{bmatrix} 0 & (\alpha-\beta)\gamma \\ (\beta-\alpha)\delta & 0 \end{bmatrix}$ From the above I got that $AB-BA$ cannot be an identity matrix since the main diagonal elements are all zero. So statement 2 is correct, I believe. And I also felt Statement 1 too was correct, since $|AB-BA|=\gamma\delta(\alpha-\beta)^2$ . (Ouchie, it was quite careless of me!) But the answer tells me a different story. I deduced that there are two possibilities: either the answer given is incorrect, or I have made an error somewhere and I am not able to identify.","I tried the following problem. Let and . There are 2 statements: is always an invertible matrix, and is never an identity matrix. Now I'm being asked to find out whether these statements are true or false. I'm not too familiar with and I don't have enough rep to upload images, I had to put my problem as a linked image: On calculation: I first got the product of the two matrices as: From the above I got that cannot be an identity matrix since the main diagonal elements are all zero. So statement 2 is correct, I believe. And I also felt Statement 1 too was correct, since . (Ouchie, it was quite careless of me!) But the answer tells me a different story. I deduced that there are two possibilities: either the answer given is incorrect, or I have made an error somewhere and I am not able to identify.",A = \begin{bmatrix} \alpha & 0 \\ 0 & \beta \end{bmatrix} B = \begin{bmatrix} 0 & \gamma \\ \delta & 0 \end{bmatrix} AB - BA AB - BA \LaTeX AB = \begin{bmatrix} 0 & \alpha\gamma \\ \beta\delta & 0 \end{bmatrix} BA = \begin{bmatrix} 0 & \beta\gamma \\ \alpha\delta & 0 \end{bmatrix} AB - BA = \begin{bmatrix} 0 & (\alpha-\beta)\gamma \\ (\beta-\alpha)\delta & 0 \end{bmatrix} AB-BA |AB-BA|=\gamma\delta(\alpha-\beta)^2,"['matrices', 'solution-verification', 'determinant', 'question-verification']"
16,Does a matrix $A$ of $\pm1$'s of order $11$ exist with $\det A >4000$?,Does a matrix  of 's of order  exist with ?,A \pm1 11 \det A >4000,How to prove or disprove that statement: there exists a square matrix of size 11 having all entries $ \pm 1$ and its determinant greater than  4000?,How to prove or disprove that statement: there exists a square matrix of size 11 having all entries $ \pm 1$ and its determinant greater than  4000?,,"['matrices', 'examples-counterexamples']"
17,Showing the complex conjugate of an eigenvector to a given eigenvalue is an eigenvector for the conjugate eigenvalue.,Showing the complex conjugate of an eigenvector to a given eigenvalue is an eigenvector for the conjugate eigenvalue.,,"I'm taking an introductory differential equations course, and we learned the matrix method for systems. We're currently covering complex eigenvalues, and I was asked to prove the following problem. However, I have no idea of how to start, and I was hoping you could provide hints as to what the solution is. What I know :  If $\mathbf{x_1}+i\mathbf{x_2}$ is a solution for the system  $\mathbf{\dot x}=A\mathbf{x}$, then its real and imaginary parts $\mathbf{x_{1}},\mathbf{x_{2}}$ are solutions to the system. Suppose the matrix $\mathbf A$ with real entries has eigenvalues $\lambda=\alpha+i\beta$ and $\bar\lambda=\alpha-i\beta$. Suppose also that $\mathbf Y_{0}=\binom{x_{1}+iy_{1}}{x_{2}+iy_{2}}$ is an eigenvector for the eigenvalue $\lambda$. Show that $\mathbf{\bar Y_{0}}=\binom{x_{1}-iy_{1}}{x_{2}-iy_{2}}$ is an eigenvector for the eigenvalue $\bar\lambda$. In other words the complex conjugate of an eigenvector for $\lambda$ is an eigenvector for $\bar\lambda$.","I'm taking an introductory differential equations course, and we learned the matrix method for systems. We're currently covering complex eigenvalues, and I was asked to prove the following problem. However, I have no idea of how to start, and I was hoping you could provide hints as to what the solution is. What I know :  If $\mathbf{x_1}+i\mathbf{x_2}$ is a solution for the system  $\mathbf{\dot x}=A\mathbf{x}$, then its real and imaginary parts $\mathbf{x_{1}},\mathbf{x_{2}}$ are solutions to the system. Suppose the matrix $\mathbf A$ with real entries has eigenvalues $\lambda=\alpha+i\beta$ and $\bar\lambda=\alpha-i\beta$. Suppose also that $\mathbf Y_{0}=\binom{x_{1}+iy_{1}}{x_{2}+iy_{2}}$ is an eigenvector for the eigenvalue $\lambda$. Show that $\mathbf{\bar Y_{0}}=\binom{x_{1}-iy_{1}}{x_{2}-iy_{2}}$ is an eigenvector for the eigenvalue $\bar\lambda$. In other words the complex conjugate of an eigenvector for $\lambda$ is an eigenvector for $\bar\lambda$.",,"['matrices', 'ordinary-differential-equations']"
18,"What is the largest determinant you can get by filling in 0,1 or 2 into a 4-by-4 matrix?","What is the largest determinant you can get by filling in 0,1 or 2 into a 4-by-4 matrix?",,For example $$\left| \begin{array}{ccc} 2 & 0 & 0 & 2 \\ 2 & 0 & 2 & 0 \\ 0 & 2 & 1 & 2 \\ 2 & 2 & 0 & 0 \end{array} \right|=40$$ Can it get bigger than that? And what's your approach?,For example $$\left| \begin{array}{ccc} 2 & 0 & 0 & 2 \\ 2 & 0 & 2 & 0 \\ 0 & 2 & 1 & 2 \\ 2 & 2 & 0 & 0 \end{array} \right|=40$$ Can it get bigger than that? And what's your approach?,,['matrices']
19,Zeros diagonal element of a semidefinite matrix leads to zeros row/column. Why?,Zeros diagonal element of a semidefinite matrix leads to zeros row/column. Why?,,"I have a similar problem as in this question. In short words: Assume a square, positive semidefinite matrix $A\in\mathbb R^{n\times n}$. Show that if a diagonal element of $A$ is zeros then the corresponding row and column are all zero. The answer in the other question suggests to look at the definiteness of the matrix $$ B = \begin{pmatrix} a_{i,i} & a_{i,k} \\ a_{k,i} & a_{k,k} \end{pmatrix}$$ where $a_{k,k}=0$ and both $a_{i,k}$ and $a_{k,i}$ are non-zero. I tried to proof this fact but I am unsure if I did it correctly. Also there might be a better/quicker/clearer way to do it. Would you mind going through it and give me your oppinion? If one calculates the eigenvalues we get  $$\lambda_{1,2} = \frac{a_{i,i}\pm\sqrt{a_{i,i}^2+4a_{i,k}^2}}{2}.$$ Thus we know that $B$ is indefinite as $\sqrt{a_{i,i}^2 + 4a_{i,k}^2} > \lvert a_{i,i} \rvert$. If $A$ is positive semidefinite then $x^T A x \geq 0$ for all $x\in\mathbb R$ must hold. Assume a matrix $C\in\mathbb R^{n \times 2}$ that is zeros except for the elemets $c_{i,1}=c_{k,2}=1$. This must also hold true for the special case that $x=Cy$ with $y\in\mathbb R^2$. We obtain $y^T C^T A C y = y^T B y$ means that $C^T A C$ is indefinite. This is a contradiction to the assumption that $A$ is positive semidefinite. Is there an (easy) way to extend this to block matrices?","I have a similar problem as in this question. In short words: Assume a square, positive semidefinite matrix $A\in\mathbb R^{n\times n}$. Show that if a diagonal element of $A$ is zeros then the corresponding row and column are all zero. The answer in the other question suggests to look at the definiteness of the matrix $$ B = \begin{pmatrix} a_{i,i} & a_{i,k} \\ a_{k,i} & a_{k,k} \end{pmatrix}$$ where $a_{k,k}=0$ and both $a_{i,k}$ and $a_{k,i}$ are non-zero. I tried to proof this fact but I am unsure if I did it correctly. Also there might be a better/quicker/clearer way to do it. Would you mind going through it and give me your oppinion? If one calculates the eigenvalues we get  $$\lambda_{1,2} = \frac{a_{i,i}\pm\sqrt{a_{i,i}^2+4a_{i,k}^2}}{2}.$$ Thus we know that $B$ is indefinite as $\sqrt{a_{i,i}^2 + 4a_{i,k}^2} > \lvert a_{i,i} \rvert$. If $A$ is positive semidefinite then $x^T A x \geq 0$ for all $x\in\mathbb R$ must hold. Assume a matrix $C\in\mathbb R^{n \times 2}$ that is zeros except for the elemets $c_{i,1}=c_{k,2}=1$. This must also hold true for the special case that $x=Cy$ with $y\in\mathbb R^2$. We obtain $y^T C^T A C y = y^T B y$ means that $C^T A C$ is indefinite. This is a contradiction to the assumption that $A$ is positive semidefinite. Is there an (easy) way to extend this to block matrices?",,"['analysis', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices']"
20,Formal definition of $n$ by $0$ and $0$ by $n$ matrices,Formal definition of  by  and  by  matrices,n 0 0 n,"A matrix is usually informally defined as a rectangular array of numbers. To make this definition formal, we can define a matrix as a map from $\{1,...,m\} \times \{1,...,n\}$ to the underlying field of scalars, where $\times$ denotes cartesian product. However, a subtle complication arises when $m=0$ or $n=0$ . In that case, the matrix would be an empty function. The problem, however, is that there is then no way to distinguish between $m \times 0$ matrices from $0 \times n$ matrices. In fact, under the cartesian product definition, for all natural numbers $m$ , $m'$ , $n$ , and $n'$ , the $m \times 0$ , $m' \times 0$ , $0 \times n$ , and $0 \times n'$ matrices are all the same entity, namely the empty function. This is, to me, an undesirable state of affairs. I want to be able to distinguish, for example, $2 \times 0$ , $3 \times 0$ , $0 \times 2$ , and $0 \times 3$ matrices. Is there a better definition of matrix that some mathematician has written about in some paper or book that avoids that problem?","A matrix is usually informally defined as a rectangular array of numbers. To make this definition formal, we can define a matrix as a map from to the underlying field of scalars, where denotes cartesian product. However, a subtle complication arises when or . In that case, the matrix would be an empty function. The problem, however, is that there is then no way to distinguish between matrices from matrices. In fact, under the cartesian product definition, for all natural numbers , , , and , the , , , and matrices are all the same entity, namely the empty function. This is, to me, an undesirable state of affairs. I want to be able to distinguish, for example, , , , and matrices. Is there a better definition of matrix that some mathematician has written about in some paper or book that avoids that problem?","\{1,...,m\} \times \{1,...,n\} \times m=0 n=0 m \times 0 0 \times n m m' n n' m \times 0 m' \times 0 0 \times n 0 \times n' 2 \times 0 3 \times 0 0 \times 2 0 \times 3","['matrices', 'definition']"
21,Trace of Matrix Exponential closed form expression,Trace of Matrix Exponential closed form expression,,"I would like to compute the $\mathbf{tr}(e^A)$ where $A$ is some square matrix with entries that have only values of either $0$ or $1$, and $\mathbf{tr}$ is the trace operator.  Are there closed form expressions for this or some way to compute it easily without having to compute the actual matrix exponential for every element in A? I have gotten this far: $$ \mathbf{tr}(e^A) = \sum_{k=0}^\infty \frac{1}{k!} \mathbf{tr}(A^k) $$ due to the property that $$ \mathbf{tr}(A+B) = \mathbf{tr}(A) + \mathbf{tr}(B) $$ but not sure how to go further and perhaps get a simpler expression?  I would like some expression form of this because I am going to use this as a constraint in an optimization problem... otherwise I wouldn't worry about the computational cost of computing all the elements of $e^A$.","I would like to compute the $\mathbf{tr}(e^A)$ where $A$ is some square matrix with entries that have only values of either $0$ or $1$, and $\mathbf{tr}$ is the trace operator.  Are there closed form expressions for this or some way to compute it easily without having to compute the actual matrix exponential for every element in A? I have gotten this far: $$ \mathbf{tr}(e^A) = \sum_{k=0}^\infty \frac{1}{k!} \mathbf{tr}(A^k) $$ due to the property that $$ \mathbf{tr}(A+B) = \mathbf{tr}(A) + \mathbf{tr}(B) $$ but not sure how to go further and perhaps get a simpler expression?  I would like some expression form of this because I am going to use this as a constraint in an optimization problem... otherwise I wouldn't worry about the computational cost of computing all the elements of $e^A$.",,"['matrices', 'matrix-exponential']"
22,What are the $\succ$ and $\prec$ operators for when used with matrices?,What are the  and  operators for when used with matrices?,\succ \prec,"I understand that $A\succ0$ means that ""A is a positive definite matrix"" (i.e.; all of the eigenvalues of A are positive). But what does it mean when the right hand side is a different value than zero? For example, what does the expression below imply? $$ A \succ 7.3 $$ Also, what is the name of this operator?","I understand that $A\succ0$ means that ""A is a positive definite matrix"" (i.e.; all of the eigenvalues of A are positive). But what does it mean when the right hand side is a different value than zero? For example, what does the expression below imply? $$ A \succ 7.3 $$ Also, what is the name of this operator?",,"['matrices', 'notation']"
23,Connection between PCA and linear regression,Connection between PCA and linear regression,,"Is there a formal link between linear regression and PCA? The goal of PCA is to decompose a matrix into a linear combination of variables that contain most of the information in the matrix. Suppose for sake of argument that we're doing PCA on an input matrix rather than its covariance matrix, and the columns $X_1, X2, ..., X_n$ of the matrix are variables of interest. Then intuitively it seems that the PCA procedure is similar to a linear regression where one uses a linear combination of the variables to predict the entries in the matrix. Is this correct thinking? How can it be made mathematically precise? Imagine enumerating the (infinite) space of all linear combinations of the variables $X_1, X_2, ...,X_n$ of a matrix of data and doing linear regression on each such combination to measure how much of the rows of the matrix the combination can 'explain'. Is there an interpretation of what PCA doing in terms of this operation? I.e. how in this procedure PCA would select the 'best' linear combinations? I realize this procedure is obviously not computationally feasible, I only present it to try to make the link between PCA and linear regression. This procedure works directly with linear combinations of columns of a matrix so it does not require them to be orthogonal.","Is there a formal link between linear regression and PCA? The goal of PCA is to decompose a matrix into a linear combination of variables that contain most of the information in the matrix. Suppose for sake of argument that we're doing PCA on an input matrix rather than its covariance matrix, and the columns $X_1, X2, ..., X_n$ of the matrix are variables of interest. Then intuitively it seems that the PCA procedure is similar to a linear regression where one uses a linear combination of the variables to predict the entries in the matrix. Is this correct thinking? How can it be made mathematically precise? Imagine enumerating the (infinite) space of all linear combinations of the variables $X_1, X_2, ...,X_n$ of a matrix of data and doing linear regression on each such combination to measure how much of the rows of the matrix the combination can 'explain'. Is there an interpretation of what PCA doing in terms of this operation? I.e. how in this procedure PCA would select the 'best' linear combinations? I realize this procedure is obviously not computationally feasible, I only present it to try to make the link between PCA and linear regression. This procedure works directly with linear combinations of columns of a matrix so it does not require them to be orthogonal.",,"['matrices', 'statistics', 'statistical-inference', 'regression', 'principal-component-analysis']"
24,Algorithm to find conjugacy classes of subgroups/elements (in matrix groups)?,Algorithm to find conjugacy classes of subgroups/elements (in matrix groups)?,,"I'm looking for a simple (=doable to implement by myself) algorithm to compute the conjugacy classes of elements and subgroups of a given subgroup of $\text{P}{\Gamma}\text{L}(n,q)$. So given a group $G\le \text{P}{\Gamma}\text{L}(n,q)$, find its conjugacy classes of elements, and/or its conjugacy classes of subgroups. I know that GAP/Magma can do this, but I would like to do my own implementation (in Java) for various reasons. I store groups by their generators, Schreier-Sims chain and action on projective subspaces of $\text{PG}(n-1,q)$, and already have the code to compute set-wise stabilizers, isomorphisms and orbits (all on projective subspaces), so the use of these in any algorithm is free for me. Given that a doable implementation is worth losing a $\mathcal O(\log|G|)$ factor (or similar) compared to GAP/Magma, what algorithm is most suited to use here? A reference to an algorithm in pseudo- code would be most welcome: Google failed to find me anything (even a paper), and trying to understand what GAP does internally is.. well.. horrible.","I'm looking for a simple (=doable to implement by myself) algorithm to compute the conjugacy classes of elements and subgroups of a given subgroup of $\text{P}{\Gamma}\text{L}(n,q)$. So given a group $G\le \text{P}{\Gamma}\text{L}(n,q)$, find its conjugacy classes of elements, and/or its conjugacy classes of subgroups. I know that GAP/Magma can do this, but I would like to do my own implementation (in Java) for various reasons. I store groups by their generators, Schreier-Sims chain and action on projective subspaces of $\text{PG}(n-1,q)$, and already have the code to compute set-wise stabilizers, isomorphisms and orbits (all on projective subspaces), so the use of these in any algorithm is free for me. Given that a doable implementation is worth losing a $\mathcal O(\log|G|)$ factor (or similar) compared to GAP/Magma, what algorithm is most suited to use here? A reference to an algorithm in pseudo- code would be most welcome: Google failed to find me anything (even a paper), and trying to understand what GAP does internally is.. well.. horrible.",,"['group-theory', 'matrices', 'algorithms', 'finite-groups', 'computational-algebra']"
25,If $AB=0$ prove that $\mathrm{rank}(A)+\mathrm{rank}(B)\leq n$,If  prove that,AB=0 \mathrm{rank}(A)+\mathrm{rank}(B)\leq n,"Let $A,B\in M_n(\mathbb{R})$ such that $AB=0$ . Prove that $$\mathrm{rank}(A)+\mathrm{rank}(B)\leq n.$$ From the given information, I only know that $\mathrm{rank}(AB)=0$ .","Let such that . Prove that From the given information, I only know that .","A,B\in M_n(\mathbb{R}) AB=0 \mathrm{rank}(A)+\mathrm{rank}(B)\leq n. \mathrm{rank}(AB)=0","['matrices', 'inequality', 'matrix-rank']"
26,Finding Transformation matrix between two $2D$ coordinate frames [Pixel Plane to World Coordinate Plane],Finding Transformation matrix between two  coordinate frames [Pixel Plane to World Coordinate Plane],2D,"The question I'm trying to figure out states that I have $N$ points $$(P_{a1x},P_{a1y}) , (P_{a2x},P_{a2y}),\dots,(P_{aNx},P_{aNx})$$ which correspond to a Pixel plane $xy$ of a camera, and other $N$ points $$(P_{b1w},P_{b1z}), (P_{b2w},P_{b2z}),\dots,(P_{bNw},P_{bNz})$$ which correspond to my $2D$ World Coordinate Frame $wz$. I've to find the transformation (Rotation + Translation) between these two sets of points so that I can translate the point from the camera space to the world space. I've made a lot of measures and I've got the two set of points, but how should I proceed now ?","The question I'm trying to figure out states that I have $N$ points $$(P_{a1x},P_{a1y}) , (P_{a2x},P_{a2y}),\dots,(P_{aNx},P_{aNx})$$ which correspond to a Pixel plane $xy$ of a camera, and other $N$ points $$(P_{b1w},P_{b1z}), (P_{b2w},P_{b2z}),\dots,(P_{bNw},P_{bNz})$$ which correspond to my $2D$ World Coordinate Frame $wz$. I've to find the transformation (Rotation + Translation) between these two sets of points so that I can translate the point from the camera space to the world space. I've made a lot of measures and I've got the two set of points, but how should I proceed now ?",,"['matrices', 'transformational-geometry', 'computer-vision']"
27,Is there a mathematical operator that will turn a matrix into its absolute values?,Is there a mathematical operator that will turn a matrix into its absolute values?,,"For a given matrix $X=\left(x_{i,\,j}\right)_{(i,\,j)}$ , I am searching for a mathematical operator that will give me another matrix $Y$ with the absolute values of $X$ , i.e.: $Y=\left(|x_{i,\,j}|\right)_{(i,\,j)}$ . It would be nice, if I could write something like $Y=|X|$ but I dont think, that this is a valid notation. Is there any convention how to do so for matrices (or vectors)? Or do I have to define my own operator like $\tilde{X}:=\left(|x_{i,\,j}|\right)_{(i,\,j)}$ ? Thank you! :)","For a given matrix , I am searching for a mathematical operator that will give me another matrix with the absolute values of , i.e.: . It would be nice, if I could write something like but I dont think, that this is a valid notation. Is there any convention how to do so for matrices (or vectors)? Or do I have to define my own operator like ? Thank you! :)","X=\left(x_{i,\,j}\right)_{(i,\,j)} Y X Y=\left(|x_{i,\,j}|\right)_{(i,\,j)} Y=|X| \tilde{X}:=\left(|x_{i,\,j}|\right)_{(i,\,j)}","['matrices', 'notation', 'absolute-value']"
28,"Is $[X,Y] \neq 0$ the sufficient condition of $e^{X+Y} \neq e^Xe^Y$?",Is  the sufficient condition of ?,"[X,Y] \neq 0 e^{X+Y} \neq e^Xe^Y","We know that if X commutes with Y, where X and Y are $n\times n$ matrices,  then we have $$e^{X+Y}=e^Xe^Y$$ However, can we conclude that $e^{X+Y} \neq e^Xe^Y$ if X doesn't commute with Y ? Is there any counterexample ? Or prove if it is right.","We know that if X commutes with Y, where X and Y are $n\times n$ matrices,  then we have $$e^{X+Y}=e^Xe^Y$$ However, can we conclude that $e^{X+Y} \neq e^Xe^Y$ if X doesn't commute with Y ? Is there any counterexample ? Or prove if it is right.",,['matrices']
29,A question on Gram matrix,A question on Gram matrix,,"The entries of Gram matrix is defined by $ \langle x_i,x_j\rangle$ in the $(i,j)^{\text{th}}$ position. It is known that Gram matrix is positive semidefinite. Is it still positive semidefinite if $ \langle x_i,x_j\rangle$  is replaced by  $ |\langle x_i,x_j\rangle|$?","The entries of Gram matrix is defined by $ \langle x_i,x_j\rangle$ in the $(i,j)^{\text{th}}$ position. It is known that Gram matrix is positive semidefinite. Is it still positive semidefinite if $ \langle x_i,x_j\rangle$  is replaced by  $ |\langle x_i,x_j\rangle|$?",,['matrices']
30,Prove that $MN-NM$ is singular. [duplicate],Prove that  is singular. [duplicate],MN-NM,"This question already has answers here : $A^2+B^2=AB$ and $BA-AB$ is non-singular [duplicate] (2 answers) Closed 5 years ago . Let $M$ and $N$ be square matrices such that $M^2+N^2=MN$ . Then prove that $MN-NM$ is singular. So basically I have to prove: $\det(MN-NM)=0$ . I tried to prove this by multiplying the given condition by the inverse of matrices $M$ and $N$ , but could not come to the answer. Could anyone please give a hint?","This question already has answers here : $A^2+B^2=AB$ and $BA-AB$ is non-singular [duplicate] (2 answers) Closed 5 years ago . Let and be square matrices such that . Then prove that is singular. So basically I have to prove: . I tried to prove this by multiplying the given condition by the inverse of matrices and , but could not come to the answer. Could anyone please give a hint?",M N M^2+N^2=MN MN-NM \det(MN-NM)=0 M N,['matrices']
31,The Inverse of a Fourth Order Tensor,The Inverse of a Fourth Order Tensor,,"Suppose that we have a fourth order tensor ${\bf{A}}$ $${\bf{A}}=A_{ijkl} {\bf{e}}_i \otimes {\bf{e}}_j \otimes {\bf{e}}_k \otimes {\bf{e}}_l$$ in the orthonormal basis $\{{\bf{e}}_1,{\bf{e}}_2,{\bf{e}}_3\}$ for $\mathbb{R}^3$ . Here, we have used Einstein summation convention which assume sum over any repeated index. Then we define the inverse of ${\bf{A}}$ denoted by ${\bf{B}}$ as follows $${\bf{A}} : {\bf{B}} = {\bf{B}} : {\bf{A}} = {\bf{I}}$$ where ${\bf{I}}$ is the fourth order identity tensor $$\begin{align} {\bf{I}} &=I_{ijkl} {\bf{e}}_i \otimes {\bf{e}}_j \otimes {\bf{e}}_k \otimes {\bf{e}}_l\\ &=\delta_{ik} \delta_{jl} {\bf{e}}_i \otimes {\bf{e}}_j \otimes {\bf{e}}_k \otimes {\bf{e}}_l \end{align}$$ where $\delta_{ij}$ is the Kronecker's Delta $$\delta_{ij}= \begin{cases} 1 & i=j \\ 0 & i \ne j \end{cases}$$ and $:$ is the double contraction defined by $${\bf{A}} : {\bf{B}}=A_{ijmn}B_{mnkl} {\bf{e}}_i \otimes {\bf{e}}_j \otimes {\bf{e}}_k \otimes {\bf{e}}_l$$ I want to write a code to compute ${\bf{B}}$ . However, I could not find any good resource on the net that gives the elements of ${\bf{B}}$ in terms of the elements of ${\bf{A}}$ . How should I compute ${\bf{B}} = {\bf{A}}^{-1}$ ? An application of this can be found in the theory of elasticity, where the fourth order tensor have the following symmetries $$A_{ijkl}=A_{jikl}=A_{ijlk}=A_{klij}.$$ That would be great if you can also touch upon this.","Suppose that we have a fourth order tensor in the orthonormal basis for . Here, we have used Einstein summation convention which assume sum over any repeated index. Then we define the inverse of denoted by as follows where is the fourth order identity tensor where is the Kronecker's Delta and is the double contraction defined by I want to write a code to compute . However, I could not find any good resource on the net that gives the elements of in terms of the elements of . How should I compute ? An application of this can be found in the theory of elasticity, where the fourth order tensor have the following symmetries That would be great if you can also touch upon this.","{\bf{A}} {\bf{A}}=A_{ijkl} {\bf{e}}_i \otimes {\bf{e}}_j \otimes {\bf{e}}_k \otimes {\bf{e}}_l \{{\bf{e}}_1,{\bf{e}}_2,{\bf{e}}_3\} \mathbb{R}^3 {\bf{A}} {\bf{B}} {\bf{A}} : {\bf{B}} = {\bf{B}} : {\bf{A}} = {\bf{I}} {\bf{I}} \begin{align}
{\bf{I}} &=I_{ijkl} {\bf{e}}_i \otimes {\bf{e}}_j \otimes {\bf{e}}_k \otimes {\bf{e}}_l\\
&=\delta_{ik} \delta_{jl} {\bf{e}}_i \otimes {\bf{e}}_j \otimes {\bf{e}}_k \otimes {\bf{e}}_l
\end{align} \delta_{ij} \delta_{ij}=
\begin{cases}
1 & i=j \\
0 & i \ne j
\end{cases} : {\bf{A}} : {\bf{B}}=A_{ijmn}B_{mnkl} {\bf{e}}_i \otimes {\bf{e}}_j \otimes {\bf{e}}_k \otimes {\bf{e}}_l {\bf{B}} {\bf{B}} {\bf{A}} {\bf{B}} = {\bf{A}}^{-1} A_{ijkl}=A_{jikl}=A_{ijlk}=A_{klij}.","['matrices', 'multivariable-calculus', 'mathematical-physics', 'tensors']"
32,How to decompose a matrix into the outer product of two vectors?,How to decompose a matrix into the outer product of two vectors?,,"I have a matrix $M$ and I would like to find two vectors $u$ and $v$, that minimize $$ \sum_{i,j} (M_{i,j}-u_iv_j)^2  $$ How can I do this (numerically)? Actually this is very simplified compared to my actual problem. For instance, I dont really have a full matrix, but only values for some range of $(i,j)$. However, I think once I know how to solve it in the general case, it should be straightforward to skip certain entries.","I have a matrix $M$ and I would like to find two vectors $u$ and $v$, that minimize $$ \sum_{i,j} (M_{i,j}-u_iv_j)^2  $$ How can I do this (numerically)? Actually this is very simplified compared to my actual problem. For instance, I dont really have a full matrix, but only values for some range of $(i,j)$. However, I think once I know how to solve it in the general case, it should be straightforward to skip certain entries.",,"['matrices', 'optimization', 'numerical-methods', 'numerical-optimization']"
33,Matrix representations of tensors,Matrix representations of tensors,,"I've been trying to teach myself general relativity, and I always get stuck at the same point: I don't really understand what the metric tensor is. Unless I'm incorrect, and please correct me if I'm wrong, I realize that it defines a geodesic distance in some coordinate system over some manifold. However, what ultimately confuses me is the calculation. For example, the metric tensor in Euclidean space is defined to be $g_{ij}= \frac{\partial x^k}{ \partial x^i}\frac{\partial x^l}{ \partial x^j}$, according to Wikipedia anyway ( https://en.wikipedia.org/wiki/Metric_tensor ). But doesn't this have a matrix representation? If so, I have absolutely no idea what it would look like. Can someone give an example. Furthermore, I know that the metric tensor can be used to raise or lower the index of an arbitrary tensor. For example, considering the Riemannian curvature tensor, $R_{ijkl} = g_{kp}R^p_{ijl}$. Is there a matrix equation (using matrix representation) that explains why this happened. I have read ( Using metric to raise and lower indices ) and understand that it has to do with a change from covariant to covariant basis. But if $g_{kp}$ is a matrix with a certain number of rows and columns, mustn't $R^p_{ijl}$ also be a matrix of rows and columns? Did any of them disappear upon taking the transformation? This does not seem possible. So what is the difference between a matrix of covariant elements versus contravariant elements? Please use matrices to explain and not Einstein summation notation. Secondly, what does the inverse of the metric tensor look like and how does it cancel out indeces of another matrix? For example, the Ricci tensor is given by $R_{ik} = g^{jl}R_{ijkl}$. What happened to those elements in the matrix $R_{ijkl}$ that look like $a_{ijkl}$? Lastly, can anyone give a matrix representation of a Christoffel symbol $\Gamma^k_{ij} = \frac{1}{2} g^{kl} \frac{ \partial g_{jl}}{ \partial x^i}+ \frac{ \partial g_{il}}{ \partial x^j}− \frac{ \partial g_{ij}}{ \partial x^l}$? What does the partial derivative of the matrix $g_{jl}$ in terms of $x^i$ look like? How does the matrix $g_{jl}$ differ from the matrix $g_{il}$ and from the matrix $g_{ij}$? Are they somehow the same matrix? What I am ultimately trying to figure out is how to do actual calculations using tensors. Ultimately, there must be a list of numbers representing coordinates in a given reference frame. But how do these numbers interact with one another when indeces are changed and canceled out? If anyone can shed light on this, using examples with numbers if possible, I would greatly appreciate it.","I've been trying to teach myself general relativity, and I always get stuck at the same point: I don't really understand what the metric tensor is. Unless I'm incorrect, and please correct me if I'm wrong, I realize that it defines a geodesic distance in some coordinate system over some manifold. However, what ultimately confuses me is the calculation. For example, the metric tensor in Euclidean space is defined to be $g_{ij}= \frac{\partial x^k}{ \partial x^i}\frac{\partial x^l}{ \partial x^j}$, according to Wikipedia anyway ( https://en.wikipedia.org/wiki/Metric_tensor ). But doesn't this have a matrix representation? If so, I have absolutely no idea what it would look like. Can someone give an example. Furthermore, I know that the metric tensor can be used to raise or lower the index of an arbitrary tensor. For example, considering the Riemannian curvature tensor, $R_{ijkl} = g_{kp}R^p_{ijl}$. Is there a matrix equation (using matrix representation) that explains why this happened. I have read ( Using metric to raise and lower indices ) and understand that it has to do with a change from covariant to covariant basis. But if $g_{kp}$ is a matrix with a certain number of rows and columns, mustn't $R^p_{ijl}$ also be a matrix of rows and columns? Did any of them disappear upon taking the transformation? This does not seem possible. So what is the difference between a matrix of covariant elements versus contravariant elements? Please use matrices to explain and not Einstein summation notation. Secondly, what does the inverse of the metric tensor look like and how does it cancel out indeces of another matrix? For example, the Ricci tensor is given by $R_{ik} = g^{jl}R_{ijkl}$. What happened to those elements in the matrix $R_{ijkl}$ that look like $a_{ijkl}$? Lastly, can anyone give a matrix representation of a Christoffel symbol $\Gamma^k_{ij} = \frac{1}{2} g^{kl} \frac{ \partial g_{jl}}{ \partial x^i}+ \frac{ \partial g_{il}}{ \partial x^j}− \frac{ \partial g_{ij}}{ \partial x^l}$? What does the partial derivative of the matrix $g_{jl}$ in terms of $x^i$ look like? How does the matrix $g_{jl}$ differ from the matrix $g_{il}$ and from the matrix $g_{ij}$? Are they somehow the same matrix? What I am ultimately trying to figure out is how to do actual calculations using tensors. Ultimately, there must be a list of numbers representing coordinates in a given reference frame. But how do these numbers interact with one another when indeces are changed and canceled out? If anyone can shed light on this, using examples with numbers if possible, I would greatly appreciate it.",,"['matrices', 'tensors', 'general-relativity']"
34,Expectation Operator on a Matrix,Expectation Operator on a Matrix,,"Kind of embarrassing, but I'm completely blanking on what applying the expectation operator to a matrix means, and I can't find a simple explanation anywhere, or an example of how to carry out the computation. I'm learning about Wiener filtering, which can be used for a number of different purposes. Basically, the Wiener filter works as as mean-square-error minimization between the input signal and the ""target"" signal. The signal is assumed to be composed of our target and noise : $$ x[n] = y[n] + w[n] $$ Where $x$ is the input signal, $y$ is the target and $w$ is noise. All are assumed to be wide-sense stationary random processes, meaning that the mean and covariance do not change in time. Where I'm getting hung up is that the covariance of $x$ with respect to itself (the variance matrix) is calculated as $$ C_{x} = E[xx^{T}] $$ but I don't remember what it means to apply the expectation operator to a matrix (in this case, $xx^{T}$ ). EDIT: Perhaps, I should be more clear in that what I don't understand is how to apply the expectation operator to the individual values within the matrix. For instance, the entry in the first row, first column of $$C_{x}$$ will be $$x[0] * x[0] = x[0]^{2}$$ . How does one calculate $$E[x[0]^{2}]$$ , and what does that mean?","Kind of embarrassing, but I'm completely blanking on what applying the expectation operator to a matrix means, and I can't find a simple explanation anywhere, or an example of how to carry out the computation. I'm learning about Wiener filtering, which can be used for a number of different purposes. Basically, the Wiener filter works as as mean-square-error minimization between the input signal and the ""target"" signal. The signal is assumed to be composed of our target and noise : Where is the input signal, is the target and is noise. All are assumed to be wide-sense stationary random processes, meaning that the mean and covariance do not change in time. Where I'm getting hung up is that the covariance of with respect to itself (the variance matrix) is calculated as but I don't remember what it means to apply the expectation operator to a matrix (in this case, ). EDIT: Perhaps, I should be more clear in that what I don't understand is how to apply the expectation operator to the individual values within the matrix. For instance, the entry in the first row, first column of will be . How does one calculate , and what does that mean?", x[n] = y[n] + w[n]  x y w x  C_{x} = E[xx^{T}]  xx^{T} C_{x} x[0] * x[0] = x[0]^{2} E[x[0]^{2}],"['matrices', 'statistics', 'operator-theory']"
35,Determinant of a nilpotent matrix,Determinant of a nilpotent matrix,,Let $A$ be a nilpotent matrix. Prove that $\det(I+A)=1$ Could someone at least give me a clue ?,Let $A$ be a nilpotent matrix. Prove that $\det(I+A)=1$ Could someone at least give me a clue ?,,"['matrices', 'determinant']"
36,direct products and direct sums  for matrices and for vector spaces,direct products and direct sums  for matrices and for vector spaces,,"I was wondering what relations and similarities are between direct product for matrices and direct product for vector spaces? Or do they just unfortunately and somehow misleadingly happen to have the same name? Note that the direct product for matrices is also called Kronecker product or tensor product of matrices. I was wondering if there is a similar thing for matrices just as direct product for vector spaces? Direct sum for matrices seems to correspond to direct sum for vector spaces, instead of direct product for vector spaces. So I guess direct sum for matrices is not the answer? Thanks to Arturo for his comment : You can connect direct sums of matrices with direct sums of vector   spaces in the following sense: if $A$ is an $n×m$ matrix and $B$ is a   $p×q$   matrix, then $A⊕B$ is the block diagonal matrix that has upper left   block $A$ and bottom right block $B$. Interpreting $A$ as a map $F_m→F_n$   and $B$   as a map $F_q→F_p$, then $A⊕B$ is the corresponding map $F_m⊕F_q→F_n⊕F_p$. Since direct sum and direct product of vector spaces share so much similarity, why is it direct sum instead of direct product of vector spaces that the direct sum of matrices correspond to? Thanks!","I was wondering what relations and similarities are between direct product for matrices and direct product for vector spaces? Or do they just unfortunately and somehow misleadingly happen to have the same name? Note that the direct product for matrices is also called Kronecker product or tensor product of matrices. I was wondering if there is a similar thing for matrices just as direct product for vector spaces? Direct sum for matrices seems to correspond to direct sum for vector spaces, instead of direct product for vector spaces. So I guess direct sum for matrices is not the answer? Thanks to Arturo for his comment : You can connect direct sums of matrices with direct sums of vector   spaces in the following sense: if $A$ is an $n×m$ matrix and $B$ is a   $p×q$   matrix, then $A⊕B$ is the block diagonal matrix that has upper left   block $A$ and bottom right block $B$. Interpreting $A$ as a map $F_m→F_n$   and $B$   as a map $F_q→F_p$, then $A⊕B$ is the corresponding map $F_m⊕F_q→F_n⊕F_p$. Since direct sum and direct product of vector spaces share so much similarity, why is it direct sum instead of direct product of vector spaces that the direct sum of matrices correspond to? Thanks!",,"['matrices', 'vector-spaces']"
37,Find the roots of a polynomial using its companion matrix,Find the roots of a polynomial using its companion matrix,,I would like to find the roots of a polynomial using its companion matrix. The polynomial is ${p(x) = x^4-10x^2+9}$ The companion matrix $M$ is $M={\left[ \begin{array}{cccc} 0 & 0 & 0 & -9 \\ 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 10       \\ 0 & 0 & 1 & 0 \end{array} \right]}$ A theorem says that the eigenvalues of $M$ are the roots of $p(x)$. I tried to find the characteristic polynomial of $M$ but it turned out to be $p(x)$. What should I do to obtain the eigenvalues of $M$?,I would like to find the roots of a polynomial using its companion matrix. The polynomial is ${p(x) = x^4-10x^2+9}$ The companion matrix $M$ is $M={\left[ \begin{array}{cccc} 0 & 0 & 0 & -9 \\ 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 10       \\ 0 & 0 & 1 & 0 \end{array} \right]}$ A theorem says that the eigenvalues of $M$ are the roots of $p(x)$. I tried to find the characteristic polynomial of $M$ but it turned out to be $p(x)$. What should I do to obtain the eigenvalues of $M$?,,"['matrices', 'polynomials']"
38,Is the rank of a Tensor different from the rank of a Matrix?,Is the rank of a Tensor different from the rank of a Matrix?,,"As far as I know, the rank of a matrix is the dimension of the vector space generated by columns. In NumPy notation, x = np.array([[1, 2], [2, 4]]) has a rank of one. np.linalg.matrix_rank(x) confirms that it is one. While studying the TensorFlow page shown below, https://www.tensorflow.org/get_started/get_started I saw the following remarks: A tensor's rank is its number of dimensions. Here are some examples of tensors:  3 # a rank 0 tensor; this is a scalar with shape [] [1. ,2., 3.] # a rank 1 tensor; this is a vector with shape [3] [[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3] [[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3] I'm totally confused. Q1. What is the relationship between the rank of a Matrix and the rank of a Tensor? Is it a completely different thing? Q2. In the case of the above matrix x = np.array([[1, 2], [2, 4]]) , is the rank two if we assume that it is a tensor not a matrix? Thank you!","As far as I know, the rank of a matrix is the dimension of the vector space generated by columns. In NumPy notation, x = np.array([[1, 2], [2, 4]]) has a rank of one. np.linalg.matrix_rank(x) confirms that it is one. While studying the TensorFlow page shown below, https://www.tensorflow.org/get_started/get_started I saw the following remarks: A tensor's rank is its number of dimensions. Here are some examples of tensors:  3 # a rank 0 tensor; this is a scalar with shape [] [1. ,2., 3.] # a rank 1 tensor; this is a vector with shape [3] [[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3] [[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3] I'm totally confused. Q1. What is the relationship between the rank of a Matrix and the rank of a Tensor? Is it a completely different thing? Q2. In the case of the above matrix x = np.array([[1, 2], [2, 4]]) , is the rank two if we assume that it is a tensor not a matrix? Thank you!",,"['matrices', 'tensors', 'tensor-rank']"
39,Historical meaning and usage of determinant,Historical meaning and usage of determinant,,"Can anyone please explain how, why, and where determinants were developed/formalized? What was their historical usage? Why were they initially formulated and what were they used for (and later generalized for)? I'm asking because I'm trying to truly understand the meaning of the determinant. I somewhat understand that it represents the distortion of volume of a region represented by a matrix after its transformation, but I find it hard to believe that that's the historical representation for it.","Can anyone please explain how, why, and where determinants were developed/formalized? What was their historical usage? Why were they initially formulated and what were they used for (and later generalized for)? I'm asking because I'm trying to truly understand the meaning of the determinant. I somewhat understand that it represents the distortion of volume of a region represented by a matrix after its transformation, but I find it hard to believe that that's the historical representation for it.",,"['matrices', 'math-history', 'determinant']"
40,Up-to-date Matrix Cookbook,Up-to-date Matrix Cookbook,,"My copy of the Matrix cookbook is dated November 15, 2012, and is the newest copy I've been able to find. Identities may not change overtime, but the approach to an error-free presentation can be asymptotic, and some topics may be missing. Where can I find an up-to-date copy? The address ""matrixcookbook.com"" listed in the 2012 book is defunct, the email in the book doesn't work, and the 2302.dk website seems to be out of commission, and I haven't found personal sites for the authors.","My copy of the Matrix cookbook is dated November 15, 2012, and is the newest copy I've been able to find. Identities may not change overtime, but the approach to an error-free presentation can be asymptotic, and some topics may be missing. Where can I find an up-to-date copy? The address ""matrixcookbook.com"" listed in the 2012 book is defunct, the email in the book doesn't work, and the 2302.dk website seems to be out of commission, and I haven't found personal sites for the authors.",,"['matrices', 'reference-request', 'online-resources', 'reference-works']"
41,How to calculate the derivative of log det matrix?,How to calculate the derivative of log det matrix?,,"How to calculate the derivative with respect to $X$ of: $$ \log \mathrm{det}\, X $$ here $X$ is a positive definite matrix, and det is the determinant of a matrix. How to calculate this? Thanks! I know it's a classical problem, but I can't find some clear material from the Internet. So some good reference is also very helpful! The hardness for me to understand is that the domain of $X$ is confined to be $S^n$. Therefore, for each symmetric matrix $X$, a specific $n(n+1)/2$-dimension vector would represent it. But the result is $X^{-1}$ (if I remember it right), a matrix form with $n^2$ elements. How to interpret the matrix form result?","How to calculate the derivative with respect to $X$ of: $$ \log \mathrm{det}\, X $$ here $X$ is a positive definite matrix, and det is the determinant of a matrix. How to calculate this? Thanks! I know it's a classical problem, but I can't find some clear material from the Internet. So some good reference is also very helpful! The hardness for me to understand is that the domain of $X$ is confined to be $S^n$. Therefore, for each symmetric matrix $X$, a specific $n(n+1)/2$-dimension vector would represent it. But the result is $X^{-1}$ (if I remember it right), a matrix form with $n^2$ elements. How to interpret the matrix form result?",,"['matrices', 'matrix-calculus']"
42,Is $ SU_2 \otimes SU_2 $ conjugate to $ SO_4(\mathbb{R}) $ in $ SU_4 $?,Is  conjugate to  in ?, SU_2 \otimes SU_2   SO_4(\mathbb{R})   SU_4 ,"Let $ A,B $ be matrix groups (with entries in the same field). Then the tensor/Kronecker product $ A \otimes B $ is a matrix group and $$ \pi: A \times B \to A \otimes B  $$ is a group homomorphism. Taking $ A=B=SU_2 $ we have a map $ \pi: SU_2 \times SU_2 \to SU_4 $ given by $$ (A,B) \mapsto A \otimes B  $$ The only nontrivial element of the kernel is $ (-1,-1) $ . So the image $ SU_2 \otimes SU_2 $ of $ \pi $ is a subgroup of $ SU_4 $ isomorphic to $$  SU_2 \times SU_2/ (-1,-1) \cong SO_4(\mathbb{R})  $$ Is $ SU_2 \otimes SU_2 $ conjugate to $ SO_4(\mathbb{R}) $ in $ SU_4 $ ? If so what is a matrix conjugating one to the other? (this part was unanswered for a while but is now answered in the update) Since matrices in $ SU_2 $ have real trace then all matrices in $ SU_2 \otimes SU_2 $ have real trace. So it is at least plausible that $ SU_2 \otimes SU_2 $ and $ SO_4(\mathbb{R}) $ are conjugate. Also what is the normalizer in $ SU_4 $ of $ SU_2 \otimes SU_2 $ / the normalizer in $ SU_4 $ of $ SO_4(\mathbb{R}) $ ? Certainly $ iI $ normalizes $ SO_4(\mathbb{R}) $ and $$ T= \zeta_8 \begin{bmatrix} 1 & 0 & 0 & 0\\ 0 & 0 & 1 & 0\\ 0& 1 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ \end{bmatrix} $$ normalizes $ SU_2 \otimes SU_2 $ . EDIT: Just to reiterate what Jason DeVito both $ SO_4(\mathbb{R}) $ and $ SU_2 \otimes SU_2 $ are normalized by $$ T= \zeta_8 \begin{bmatrix} 1 & 0 & 0 & 0\\ 0 & 0 & 1 & 0\\ 0& 1 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ \end{bmatrix} $$ where $$ \begin{bmatrix} 1 & 0 & 0 & 0\\ 0 & 0 & 1 & 0\\ 0& 1 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ \end{bmatrix}=SWAP \in O_4(\mathbb{R}) $$ is the $ SWAP $ operator and $ \zeta_8 $ just normalizes the determinant. Both $ SO_4(\mathbb{R}) $ and $ SU_2 \otimes SU_2 $ contain $ -I $ and both do not contain $ iI $ . Since \begin{align*} (\zeta_8 SWAP)^2&=iI\\  (\zeta_8 SWAP)^4&=-I \end{align*} we can combine this with the results from https://arxiv.org/pdf/math/0605784.pdf to conclude that both $ SO_4(\mathbb{R}) $ and $ SU_2 \otimes SU_2 $ have cyclic $ 4 $ component group generated by $ \zeta_8 SWAP $ . UPDATE: A specific unitary matrix conjugating $ SU_2 \otimes SU_2 $ to $ SO_4(\mathbb{R}) $ is $$ Q=\frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 0 & 0 & i \\ 0 & i & 1 & 0 \\ 0 & i & -1 & 0 \\ 1 & 0 & 0 & -i \end{bmatrix} $$ in other words there is an isomorphism $ SU_2 \otimes SU_2 \to SO_4(\mathbb{R}) $ given by $$ M \mapsto Q^{-1} M Q $$ This is Theorem 1 from https://arxiv.org/pdf/quant-ph/0002045.pdf the matrix $ Q $ is the change of basis from standard basis to the Bell basis well known in quantum computing. This theorem is an interesting and short read for anyone interested in quantum information or just Lie groups. The argument proceeds by concocting a definite form that is preserved by local unitaries $ SU_2 \otimes SU_2 $ , instantiating the claims in the answers below that such a form must exist.","Let be matrix groups (with entries in the same field). Then the tensor/Kronecker product is a matrix group and is a group homomorphism. Taking we have a map given by The only nontrivial element of the kernel is . So the image of is a subgroup of isomorphic to Is conjugate to in ? If so what is a matrix conjugating one to the other? (this part was unanswered for a while but is now answered in the update) Since matrices in have real trace then all matrices in have real trace. So it is at least plausible that and are conjugate. Also what is the normalizer in of / the normalizer in of ? Certainly normalizes and normalizes . EDIT: Just to reiterate what Jason DeVito both and are normalized by where is the operator and just normalizes the determinant. Both and contain and both do not contain . Since we can combine this with the results from https://arxiv.org/pdf/math/0605784.pdf to conclude that both and have cyclic component group generated by . UPDATE: A specific unitary matrix conjugating to is in other words there is an isomorphism given by This is Theorem 1 from https://arxiv.org/pdf/quant-ph/0002045.pdf the matrix is the change of basis from standard basis to the Bell basis well known in quantum computing. This theorem is an interesting and short read for anyone interested in quantum information or just Lie groups. The argument proceeds by concocting a definite form that is preserved by local unitaries , instantiating the claims in the answers below that such a form must exist."," A,B   A \otimes B  
\pi: A \times B \to A \otimes B 
  A=B=SU_2   \pi: SU_2 \times SU_2 \to SU_4  
(A,B) \mapsto A \otimes B 
  (-1,-1)   SU_2 \otimes SU_2   \pi   SU_4  
 SU_2 \times SU_2/ (-1,-1) \cong SO_4(\mathbb{R}) 
  SU_2 \otimes SU_2   SO_4(\mathbb{R})   SU_4   SU_2   SU_2 \otimes SU_2   SU_2 \otimes SU_2   SO_4(\mathbb{R})   SU_4   SU_2 \otimes SU_2   SU_4   SO_4(\mathbb{R})   iI   SO_4(\mathbb{R})  
T=
\zeta_8
\begin{bmatrix}
1 & 0 & 0 & 0\\
0 & 0 & 1 & 0\\
0& 1 & 0 & 0 \\
0 & 0 & 0 & 1 \\
\end{bmatrix}
  SU_2 \otimes SU_2   SO_4(\mathbb{R})   SU_2 \otimes SU_2  
T=
\zeta_8
\begin{bmatrix}
1 & 0 & 0 & 0\\
0 & 0 & 1 & 0\\
0& 1 & 0 & 0 \\
0 & 0 & 0 & 1 \\
\end{bmatrix}
 
\begin{bmatrix}
1 & 0 & 0 & 0\\
0 & 0 & 1 & 0\\
0& 1 & 0 & 0 \\
0 & 0 & 0 & 1 \\
\end{bmatrix}=SWAP \in O_4(\mathbb{R})
  SWAP   \zeta_8   SO_4(\mathbb{R})   SU_2 \otimes SU_2   -I   iI  \begin{align*}
(\zeta_8 SWAP)^2&=iI\\
 (\zeta_8 SWAP)^4&=-I
\end{align*}  SO_4(\mathbb{R})   SU_2 \otimes SU_2   4   \zeta_8 SWAP   SU_2 \otimes SU_2   SO_4(\mathbb{R})  
Q=\frac{1}{\sqrt{2}}
\begin{bmatrix}
1 & 0 & 0 & i \\
0 & i & 1 & 0 \\
0 & i & -1 & 0 \\
1 & 0 & 0 & -i
\end{bmatrix}
  SU_2 \otimes SU_2 \to SO_4(\mathbb{R})  
M \mapsto Q^{-1} M Q
  Q   SU_2 \otimes SU_2 ","['matrices', 'lie-groups', 'tensor-products', 'unitary-matrices', 'exceptional-isomorphisms']"
43,Notation for the ith row and column of a matrix,Notation for the ith row and column of a matrix,,"When noting the $i^{th}$ scalar of a vector $\mathbf{x}$ one usually does it as $x_i$, since it is a scalar When doing this for matrices that are being denoted in bold, let's say $\mathbf{A}$, how should I write the $i^{th}$ row or $j^{th}$ column? This question provides some insight as to how to distinguish  from rows and columns but does not address any possible standards for doing it as $\mathbf{A}_{i*}$, $A_{i*}$ or maybe even $\mathbf{a}_{i*}$ Is there a preferable form?","When noting the $i^{th}$ scalar of a vector $\mathbf{x}$ one usually does it as $x_i$, since it is a scalar When doing this for matrices that are being denoted in bold, let's say $\mathbf{A}$, how should I write the $i^{th}$ row or $j^{th}$ column? This question provides some insight as to how to distinguish  from rows and columns but does not address any possible standards for doing it as $\mathbf{A}_{i*}$, $A_{i*}$ or maybe even $\mathbf{a}_{i*}$ Is there a preferable form?",,"['matrices', 'notation', 'vectors']"
44,"$A, B$ are matrices from $M_4(\mathbb R)$ so that $AB=BA$ and $\det(A^2-AB+B^2)=0$.",are matrices from  so that  and .,"A, B M_4(\mathbb R) AB=BA \det(A^2-AB+B^2)=0","Prove that, if $A, B$ are matrices from $M_4(\Bbb R)$ so that $AB=BA$ and $\det(A^2 −AB + B^2) = 0$ then: $$ \det(A + B) + 3\det(A − B) = 6 (\det(A) + \det(B)). \tag 1 $$ What I tried: Because of $AB=BA$ we can use, let's say, the Newton's binomial expansion for $A$ and $B$ , but it didn't take me to the solution. Also it's easy to show that $\det (A^3 + B^3) = 0$ .","Prove that, if are matrices from so that and then: What I tried: Because of we can use, let's say, the Newton's binomial expansion for and , but it didn't take me to the solution. Also it's easy to show that .","A, B M_4(\Bbb R) AB=BA \det(A^2 −AB + B^2) = 0 
\det(A + B) + 3\det(A − B) = 6 (\det(A) + \det(B)). \tag 1
 AB=BA A B \det (A^3 + B^3) = 0",['matrices']
45,How can I combine affine transformations into one matrix?,How can I combine affine transformations into one matrix?,,"So from what I understand from this picture, the box is stretched to twice its width. And it is then flipped from the x-axis. And then it is rotated 30 degrees anticlockwise. So these three transformations are Scaling , then Reflection then Rotation . How am I supposed to go about answering this problem. Also, it says which homogenous 2d matrix, this is confusing for me because when you convert cartesian coordinates to homogenous coordinates, you are going from 2d - 3d. This question is saying homogenous but it is a purely 2d transformation, I find this confusing. Any help would be appreciated. EDIT: Final matrices: The scaling matrix is... $$\begin{bmatrix}2& 0& 0\\ 0& 1 & 0\\0& 0 & 1\end{bmatrix}$$ The translation matrix is... $$\begin{bmatrix}1& 0& 3\\ 0& 1 & -1\\0& 0 & 1\end{bmatrix}$$ And the Rotation matrix is...$$\begin{bmatrix}\cos(30) & -\sin(30) & 0\\ \sin(30) & \cos(30) & 0\\ 0 & 0 & 1 \end{bmatrix}$$ Is that right? Now you simply matrix multiply?","So from what I understand from this picture, the box is stretched to twice its width. And it is then flipped from the x-axis. And then it is rotated 30 degrees anticlockwise. So these three transformations are Scaling , then Reflection then Rotation . How am I supposed to go about answering this problem. Also, it says which homogenous 2d matrix, this is confusing for me because when you convert cartesian coordinates to homogenous coordinates, you are going from 2d - 3d. This question is saying homogenous but it is a purely 2d transformation, I find this confusing. Any help would be appreciated. EDIT: Final matrices: The scaling matrix is... $$\begin{bmatrix}2& 0& 0\\ 0& 1 & 0\\0& 0 & 1\end{bmatrix}$$ The translation matrix is... $$\begin{bmatrix}1& 0& 3\\ 0& 1 & -1\\0& 0 & 1\end{bmatrix}$$ And the Rotation matrix is...$$\begin{bmatrix}\cos(30) & -\sin(30) & 0\\ \sin(30) & \cos(30) & 0\\ 0 & 0 & 1 \end{bmatrix}$$ Is that right? Now you simply matrix multiply?",,"['matrices', 'transformation', 'image-processing']"
46,Gradient of squared Frobenius norm,Gradient of squared Frobenius norm,,"I would like to find the gradient of $\frac{1}{2} \big \| X A^T \big \|_F^2$ with respect to $X_{ij}$ . Going by the chain rule in the Matrix Cookbook (eqn 126), it's something like $$\frac{\partial}{\partial X_{ij}} \Big[\frac{1}{2} \big\| X A \big\|_F^2 \Big] = \text{Tr} \Big[(XA^T)^T (J^{jk} A^T) \Big]$$ where $J$ has same dimensions as $X$ and has zeros everywhere except for entry $(j,k)$ . I m not so sure about the $J^{jk} A^T$ bit (Cookbook eqn 66 applies here?).","I would like to find the gradient of with respect to . Going by the chain rule in the Matrix Cookbook (eqn 126), it's something like where has same dimensions as and has zeros everywhere except for entry . I m not so sure about the bit (Cookbook eqn 66 applies here?).","\frac{1}{2} \big \| X A^T \big \|_F^2 X_{ij} \frac{\partial}{\partial X_{ij}} \Big[\frac{1}{2} \big\| X A \big\|_F^2 \Big] = \text{Tr} \Big[(XA^T)^T (J^{jk} A^T) \Big] J X (j,k) J^{jk} A^T","['matrices', 'derivatives', 'normed-spaces', 'matrix-norms', 'scalar-fields']"
47,Is there a simple way of proving that $\text{GL}_n(R) \not\cong \text{GL}_m(R)$?,Is there a simple way of proving that ?,\text{GL}_n(R) \not\cong \text{GL}_m(R),"Letting $\mathbb{F}_{1}$ and $\mathbb{F}_{2}$ be fields, and letting $n \geq 3$ and $m$ be natural numbers, it is known that $\text{GL}_{m}(\mathbb{F}_{1})$ and $\text{GL}_{n}(\mathbb{F}_{2})$ are elementarily equivalent if and only if $m=n$ and $\mathbb{F}_{1} \equiv \mathbb{F}_{2}$ (as proven in ""Elementary Properties of Linear Groups"" in the collection ""The Metamathematics of Algebraic Systems — Collected Papers: 1936–1967""). So, given a field $\mathbb{F}$, if $n \neq m$, then $\text{GL}_{m}(\mathbb{F}) \not\equiv \text{GL}_{n}(\mathbb{F})$, and thus $\text{GL}_{m}(\mathbb{F}) \not\cong \text{GL}_{n}(\mathbb{F})$. Letting $R$ be a commutative ring (with unity), and letting $n, m \in \mathbb{N}$ be such that $n \neq m$, is there a simple ""algebraic"" way of proving that $\text{GL}_{m}(R)$ and $\text{GL}_{n}(R)$ are not isomorphic (as groups)? Is there a simple group-theoretic way of showing that $\text{GL}_{m}(\mathbb{F}) \not\cong \text{GL}_{n}(\mathbb{F})$ for a field $\mathbb{F}$? Certain special cases of this problem trivially hold, for example in the case whereby $\mathbb{F}$ is finite, in which case $|\text{GL}_{m}(\mathbb{F})| \neq |\text{GL}_{n}(\mathbb{F})|$.","Letting $\mathbb{F}_{1}$ and $\mathbb{F}_{2}$ be fields, and letting $n \geq 3$ and $m$ be natural numbers, it is known that $\text{GL}_{m}(\mathbb{F}_{1})$ and $\text{GL}_{n}(\mathbb{F}_{2})$ are elementarily equivalent if and only if $m=n$ and $\mathbb{F}_{1} \equiv \mathbb{F}_{2}$ (as proven in ""Elementary Properties of Linear Groups"" in the collection ""The Metamathematics of Algebraic Systems — Collected Papers: 1936–1967""). So, given a field $\mathbb{F}$, if $n \neq m$, then $\text{GL}_{m}(\mathbb{F}) \not\equiv \text{GL}_{n}(\mathbb{F})$, and thus $\text{GL}_{m}(\mathbb{F}) \not\cong \text{GL}_{n}(\mathbb{F})$. Letting $R$ be a commutative ring (with unity), and letting $n, m \in \mathbb{N}$ be such that $n \neq m$, is there a simple ""algebraic"" way of proving that $\text{GL}_{m}(R)$ and $\text{GL}_{n}(R)$ are not isomorphic (as groups)? Is there a simple group-theoretic way of showing that $\text{GL}_{m}(\mathbb{F}) \not\cong \text{GL}_{n}(\mathbb{F})$ for a field $\mathbb{F}$? Certain special cases of this problem trivially hold, for example in the case whereby $\mathbb{F}$ is finite, in which case $|\text{GL}_{m}(\mathbb{F})| \neq |\text{GL}_{n}(\mathbb{F})|$.",,"['matrices', 'group-theory', 'model-theory']"
48,A matrix w/integer eigenvalues and trigonometric identity,A matrix w/integer eigenvalues and trigonometric identity,,"Any intuition and/or rigorous arguments on the proofs of the following statements would be appreciated: Let $n$ be a natural number.  (a) Consider the following Toeplitz/circulant symmetric matrix: $$ \Lambda_{kl} = \begin{cases}\frac{2n(n+1)}{3}, & k = l, \\[6pt] \frac{-1}{1-\cos\frac{2\pi(k-l)}{2n+1}}, & k \ne l,\end{cases} $$ where $k,l = 1,2, \ldots, 2n+1$. Prove that its eigenvalues are natural numbers(!) $$2n, 4n-2, 6n-6, \ldots, n(n+1)$$ with multiplicity $2$ and $0$ w/multiplicity $1$. (b) Find a formula for the right signs in the following trigonometric identity $$ \tan\left(\frac{l\pi}{2n+1}\right) = 2\sum_{k=1}^n (\pm)\sin\left(\frac{k\pi}{2n+1}\right),\qquad l = 1,2,\ldots, n. $$ and show that the signs are uniquely determined in the case of prime $2n+1$. The problems (that are formulated to be self-contained) arise from discrete approximation of continuous inverse boundary problem (see http://en.wikibooks.org/wiki/On_2D_Inverse_Problems for background) and the statements were formulated w/computer help.","Any intuition and/or rigorous arguments on the proofs of the following statements would be appreciated: Let $n$ be a natural number.  (a) Consider the following Toeplitz/circulant symmetric matrix: $$ \Lambda_{kl} = \begin{cases}\frac{2n(n+1)}{3}, & k = l, \\[6pt] \frac{-1}{1-\cos\frac{2\pi(k-l)}{2n+1}}, & k \ne l,\end{cases} $$ where $k,l = 1,2, \ldots, 2n+1$. Prove that its eigenvalues are natural numbers(!) $$2n, 4n-2, 6n-6, \ldots, n(n+1)$$ with multiplicity $2$ and $0$ w/multiplicity $1$. (b) Find a formula for the right signs in the following trigonometric identity $$ \tan\left(\frac{l\pi}{2n+1}\right) = 2\sum_{k=1}^n (\pm)\sin\left(\frac{k\pi}{2n+1}\right),\qquad l = 1,2,\ldots, n. $$ and show that the signs are uniquely determined in the case of prime $2n+1$. The problems (that are formulated to be self-contained) arise from discrete approximation of continuous inverse boundary problem (see http://en.wikibooks.org/wiki/On_2D_Inverse_Problems for background) and the statements were formulated w/computer help.",,"['matrices', 'trigonometry', 'eigenvalues-eigenvectors', 'boolean-algebra', 'natural-numbers']"
49,Möbius function from random number sequence,Möbius function from random number sequence,,"Consider some arbitrary number sequence like the decimal expansion of $\pi$ = {3, 1, 4, 1, 5, 9, 2}. Prepend the sequence with the number $1$ so that you get {1, 3, 1, 4, 1, 5, 9, 2}. Then plug it into the first column in a matrix that has the following recurrence definition: $$\begin{align} T(n,1) &= (n-1)\text{th digit of }\pi, \\ T(n,2) &= T(n,1) - T(n-1,2), \\ \text{for } k>2, T(n,k) &= \sum\limits_{i=1}^{k-1} T(n-i,k-1)-\sum\limits_{i=1}^{k-1} T(n-i,k) \end{align} $$ That table looks like this: $$\displaystyle \left( \begin{array}{cccccccc}  1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  3 & 3 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & -2 & 3 & 0 & 0 & 0 & 0 & 0 \\  4 & 6 & -2 & 3 & 0 & 0 & 0 & 0 \\  1 & -5 & 3 & -2 & 3 & 0 & 0 & 0 \\  5 & 10 & 0 & 3 & -2 & 3 & 0 & 0 \\  9 & -1 & 2 & -3 & 3 & -2 & 3 & 0 \\  2 & 3 & 7 & 7 & -3 & 3 & -2 & 3 \end{array} \right)$$ Then calculate the matrix inverse of the matrix above: $$\displaystyle \left( \begin{array}{cccccccc}  1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  -1 & \frac{1}{3} & 0 & 0 & 0 & 0 & 0 & 0 \\  -1 & \frac{2}{9} & \frac{1}{3} & 0 & 0 & 0 & 0 & 0 \\  0 & -\frac{14}{27} & \frac{2}{9} & \frac{1}{3} & 0 & 0 & 0 & 0 \\  -1 & -\frac{1}{81} & -\frac{5}{27} & \frac{2}{9} & \frac{1}{3} & 0 & 0 & 0 \\  1 & -\frac{146}{243} & -\frac{28}{81} & -\frac{5}{27} & \frac{2}{9} & \frac{1}{3} & 0 & 0 \\  -1 & -\frac{688}{729} & -\frac{11}{243} & -\frac{1}{81} & -\frac{5}{27} & \frac{2}{9} & \frac{1}{3} & 0 \\  0 & \frac{694}{2187} & -\frac{850}{729} & -\frac{92}{243} & -\frac{1}{81} & -\frac{5}{27} & \frac{2}{9} & \frac{1}{3} \end{array} \right)$$ Why then is there the Möbius function sequence in the first column? I have checked this for random sequences in programs like this: (*Mathematica*) Clear[t, n, k, a, b]; nn = 8; a = Flatten[{1, RealDigits[N[Pi, nn - 1]][[1]]}] Length[a] t[n_, 1] := t[n, 1] = a[[n]]; t[n_, k_] :=    t[n, k] =     If[And[n > 1, k > 1],      If[k == 2, t[n, k - 1] - t[n - 1, k],       Sum[t[n - i, k - 1], {i, 1, k - 1}] -        Sum[t[n - i, k], {i, 1, k - 1}]], 0]; A = Table[Table[t[n, k], {k, 1, nn}], {n, 1, nn}]; MatrixForm[A] MatrixForm[Inverse[A]] for up to 100 times 100 matrices and I always get the Möbius function in the first column. I believe it has something do with the fact that the divisibility matrix equal to 1 if k divides n and 0 otherwise: $$\displaystyle \left( \begin{array}{cccccccccccc}  1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & 1 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\  1 & 1 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\  1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\  1 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\  1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\  1 & 1 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 \end{array} \right)$$ satisfies the recurrence, that (edit 21.7.2014) Jeffrey Shallit formulated for me: $$\begin{align} T(n,1) &= 1, \\ \text{for } k>1, T(n,k) &= \sum\limits_{i=1}^{k-1} T(n-i,k-1)-\sum\limits_{i=1}^{k-1} T(n-i,k) \end{align}$$ and also the recurrence: $$\begin{align} T(n,1) &=1, \\  T(n,2) &= T(n,1)-T(n-1,2),\\ \text{for } k>2, T(n,k) &= \sum\limits_{i=1}^{k-1} T(n-i,k-1) -\sum\limits_{i=1}^{k-1} T(n-i,k) \end{align}$$ which is the same as the first recurrence in the beginning of the question except that the first column here is equal to 1,1,1... Edit 28.3.2014: (*Mathematica Mats Granvik 28.3.2014*) Clear[A, t, n, k, a, nn]; nn = 8; a = Table[StringJoin[{""x"", ToString[n]}], {n, 1, nn}] Length[a] t[n_, 1] := t[n, 1] = a[[n]]; t[n_, k_] :=    t[n, k] =     If[And[n > 1, k > 1],      If[k == 2, t[n, k - 1] - t[n - 1, k],       Sum[t[n - i, k - 1], {i, 1, k - 1}] -        Sum[t[n - i, k], {i, 1, k - 1}]], 0]; A = Table[Table[t[n, k], {k, 1, nn}], {n, 1, nn}]; MatrixForm[A] MatrixForm[a[[1]]*Inverse[A]] Outputs: 1,-1,-1,0,-1,1,-1,0,... Edit 7.4.2013: Input: {""x0"", ""x1"", ""x2"", ""x3"", ""x4"", ""x5"", ""x6"", ""x7"", ""x8"", ""x9"", ""x10""} Mathematica program: (*Mathematica Mats Granvik 7.4.2014*) Clear[A, t, n, k, a, nn]; nn = 11; a = Table[StringJoin[{""x"", ToString[n - 1]}], {n, 1, nn}] Length[a] t[n_, 1] := t[n, 1] = If[n <= 3, 1, a[[n]]]; t[n_, k_] :=    t[n, k] =     If[n >= k,      If[n <= 3, 1,       If[And[k > 1],        If[Or[k == 2, k == 3],         t[n, k - 1] - Sum[t[n - i, k], {i, 1, k - 1}],         If[k >= 4,          Sum[t[n - i, k - 1], {i, 1, k - 2}] -           Sum[t[n - i, k], {i, 1, k - 1}], 0], 0], 0, 0], 0], 0]; A = Table[Table[t[n, k], {k, 1, nn}], {n, 1, nn}]; MatrixForm[A] Inverse[A][[2 ;; nn, 1]] Output; {-1, 0, -1, -1, -2, -1, -2, -2, -2, -1} Which is the Mertens function with the first term negated. Edit 21.7.2014: The matrix inverse of this triangle: $$\left( \begin{array}{cccccc}  x_1 & 0 & 0 & 0 & 0 & 0 \\  x_2 & x_2 & 0 & 0 & 0 & 0 \\  x_3 & x_3-x_2 & x_2 & 0 & 0 & 0 \\  x_4 & x_2-x_3+x_4 & x_3-x_2 & x_2 & 0 & 0 \\  x_5 & -x_2+x_3-x_4+x_5 & x_4-x_3 & x_3-x_2 & x_2 & 0 \\  x_6 & x_2-x_3+x_4-x_5+x_6 & x_2-x_4+x_5 & x_4-x_3 & x_3-x_2 & x_2 \end{array} \right)$$ gives the möbius function function divided by $x_1$. Edit 24.7.2014: (*Program start*) (*Mathematica Mats Granvik 24.7.2014*) Clear[A, t, n, k, a, nn]; nn = 32; Print[""Random numbers as input:""] a = RandomReal[7, nn] Length[a]; t[n_, 1] := t[n, 1] = a[[n]]; t[n_, k_] :=    t[n, k] =     If[And[n > 1, k > 1],      If[k == 2, t[n, k - 1] - t[n - 1, k],       Sum[t[n - i, k - 1], {i, 1, k - 1}] -        Sum[t[n - i, k], {i, 1, k - 1}]], 0]; A = Table[Table[t[n, k], {k, 1, nn}], {n, 1, nn}]; MatrixForm[A]; B = a[[1]]*Inverse[A]; Print[""Möbius function as output:""] MatrixForm[B]; Chop[B[[All, 1]]] (*program end*)","Consider some arbitrary number sequence like the decimal expansion of $\pi$ = {3, 1, 4, 1, 5, 9, 2}. Prepend the sequence with the number $1$ so that you get {1, 3, 1, 4, 1, 5, 9, 2}. Then plug it into the first column in a matrix that has the following recurrence definition: $$\begin{align} T(n,1) &= (n-1)\text{th digit of }\pi, \\ T(n,2) &= T(n,1) - T(n-1,2), \\ \text{for } k>2, T(n,k) &= \sum\limits_{i=1}^{k-1} T(n-i,k-1)-\sum\limits_{i=1}^{k-1} T(n-i,k) \end{align} $$ That table looks like this: $$\displaystyle \left( \begin{array}{cccccccc}  1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  3 & 3 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & -2 & 3 & 0 & 0 & 0 & 0 & 0 \\  4 & 6 & -2 & 3 & 0 & 0 & 0 & 0 \\  1 & -5 & 3 & -2 & 3 & 0 & 0 & 0 \\  5 & 10 & 0 & 3 & -2 & 3 & 0 & 0 \\  9 & -1 & 2 & -3 & 3 & -2 & 3 & 0 \\  2 & 3 & 7 & 7 & -3 & 3 & -2 & 3 \end{array} \right)$$ Then calculate the matrix inverse of the matrix above: $$\displaystyle \left( \begin{array}{cccccccc}  1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  -1 & \frac{1}{3} & 0 & 0 & 0 & 0 & 0 & 0 \\  -1 & \frac{2}{9} & \frac{1}{3} & 0 & 0 & 0 & 0 & 0 \\  0 & -\frac{14}{27} & \frac{2}{9} & \frac{1}{3} & 0 & 0 & 0 & 0 \\  -1 & -\frac{1}{81} & -\frac{5}{27} & \frac{2}{9} & \frac{1}{3} & 0 & 0 & 0 \\  1 & -\frac{146}{243} & -\frac{28}{81} & -\frac{5}{27} & \frac{2}{9} & \frac{1}{3} & 0 & 0 \\  -1 & -\frac{688}{729} & -\frac{11}{243} & -\frac{1}{81} & -\frac{5}{27} & \frac{2}{9} & \frac{1}{3} & 0 \\  0 & \frac{694}{2187} & -\frac{850}{729} & -\frac{92}{243} & -\frac{1}{81} & -\frac{5}{27} & \frac{2}{9} & \frac{1}{3} \end{array} \right)$$ Why then is there the Möbius function sequence in the first column? I have checked this for random sequences in programs like this: (*Mathematica*) Clear[t, n, k, a, b]; nn = 8; a = Flatten[{1, RealDigits[N[Pi, nn - 1]][[1]]}] Length[a] t[n_, 1] := t[n, 1] = a[[n]]; t[n_, k_] :=    t[n, k] =     If[And[n > 1, k > 1],      If[k == 2, t[n, k - 1] - t[n - 1, k],       Sum[t[n - i, k - 1], {i, 1, k - 1}] -        Sum[t[n - i, k], {i, 1, k - 1}]], 0]; A = Table[Table[t[n, k], {k, 1, nn}], {n, 1, nn}]; MatrixForm[A] MatrixForm[Inverse[A]] for up to 100 times 100 matrices and I always get the Möbius function in the first column. I believe it has something do with the fact that the divisibility matrix equal to 1 if k divides n and 0 otherwise: $$\displaystyle \left( \begin{array}{cccccccccccc}  1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & 1 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\  1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\  1 & 1 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\  1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\  1 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\  1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\  1 & 1 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 \end{array} \right)$$ satisfies the recurrence, that (edit 21.7.2014) Jeffrey Shallit formulated for me: $$\begin{align} T(n,1) &= 1, \\ \text{for } k>1, T(n,k) &= \sum\limits_{i=1}^{k-1} T(n-i,k-1)-\sum\limits_{i=1}^{k-1} T(n-i,k) \end{align}$$ and also the recurrence: $$\begin{align} T(n,1) &=1, \\  T(n,2) &= T(n,1)-T(n-1,2),\\ \text{for } k>2, T(n,k) &= \sum\limits_{i=1}^{k-1} T(n-i,k-1) -\sum\limits_{i=1}^{k-1} T(n-i,k) \end{align}$$ which is the same as the first recurrence in the beginning of the question except that the first column here is equal to 1,1,1... Edit 28.3.2014: (*Mathematica Mats Granvik 28.3.2014*) Clear[A, t, n, k, a, nn]; nn = 8; a = Table[StringJoin[{""x"", ToString[n]}], {n, 1, nn}] Length[a] t[n_, 1] := t[n, 1] = a[[n]]; t[n_, k_] :=    t[n, k] =     If[And[n > 1, k > 1],      If[k == 2, t[n, k - 1] - t[n - 1, k],       Sum[t[n - i, k - 1], {i, 1, k - 1}] -        Sum[t[n - i, k], {i, 1, k - 1}]], 0]; A = Table[Table[t[n, k], {k, 1, nn}], {n, 1, nn}]; MatrixForm[A] MatrixForm[a[[1]]*Inverse[A]] Outputs: 1,-1,-1,0,-1,1,-1,0,... Edit 7.4.2013: Input: {""x0"", ""x1"", ""x2"", ""x3"", ""x4"", ""x5"", ""x6"", ""x7"", ""x8"", ""x9"", ""x10""} Mathematica program: (*Mathematica Mats Granvik 7.4.2014*) Clear[A, t, n, k, a, nn]; nn = 11; a = Table[StringJoin[{""x"", ToString[n - 1]}], {n, 1, nn}] Length[a] t[n_, 1] := t[n, 1] = If[n <= 3, 1, a[[n]]]; t[n_, k_] :=    t[n, k] =     If[n >= k,      If[n <= 3, 1,       If[And[k > 1],        If[Or[k == 2, k == 3],         t[n, k - 1] - Sum[t[n - i, k], {i, 1, k - 1}],         If[k >= 4,          Sum[t[n - i, k - 1], {i, 1, k - 2}] -           Sum[t[n - i, k], {i, 1, k - 1}], 0], 0], 0, 0], 0], 0]; A = Table[Table[t[n, k], {k, 1, nn}], {n, 1, nn}]; MatrixForm[A] Inverse[A][[2 ;; nn, 1]] Output; {-1, 0, -1, -1, -2, -1, -2, -2, -2, -1} Which is the Mertens function with the first term negated. Edit 21.7.2014: The matrix inverse of this triangle: $$\left( \begin{array}{cccccc}  x_1 & 0 & 0 & 0 & 0 & 0 \\  x_2 & x_2 & 0 & 0 & 0 & 0 \\  x_3 & x_3-x_2 & x_2 & 0 & 0 & 0 \\  x_4 & x_2-x_3+x_4 & x_3-x_2 & x_2 & 0 & 0 \\  x_5 & -x_2+x_3-x_4+x_5 & x_4-x_3 & x_3-x_2 & x_2 & 0 \\  x_6 & x_2-x_3+x_4-x_5+x_6 & x_2-x_4+x_5 & x_4-x_3 & x_3-x_2 & x_2 \end{array} \right)$$ gives the möbius function function divided by $x_1$. Edit 24.7.2014: (*Program start*) (*Mathematica Mats Granvik 24.7.2014*) Clear[A, t, n, k, a, nn]; nn = 32; Print[""Random numbers as input:""] a = RandomReal[7, nn] Length[a]; t[n_, 1] := t[n, 1] = a[[n]]; t[n_, k_] :=    t[n, k] =     If[And[n > 1, k > 1],      If[k == 2, t[n, k - 1] - t[n - 1, k],       Sum[t[n - i, k - 1], {i, 1, k - 1}] -        Sum[t[n - i, k], {i, 1, k - 1}]], 0]; A = Table[Table[t[n, k], {k, 1, nn}], {n, 1, nn}]; MatrixForm[A]; B = a[[1]]*Inverse[A]; Print[""Möbius function as output:""] MatrixForm[B]; Chop[B[[All, 1]]] (*program end*)",,"['matrices', 'elementary-number-theory', 'recurrence-relations', 'mobius-function']"
50,Going back from a correlation matrix to the original matrix,Going back from a correlation matrix to the original matrix,,"I have N sensors which are been sampled M times, so I have an N by M readout matrix. If I want to know the relation and dependencies of these sensors simplest thing is to do a Pearson's correlation which gives me an N by N correlation matrix.  Now let's say if I have the correlation matrix but I want to explore tho possible readout space that can lead to such correlation matrix what can I do?  So the question is: given an N by N correlation matrix how you can get to a matrix that would have such correlation matrix? Any comment is appreciated.","I have N sensors which are been sampled M times, so I have an N by M readout matrix. If I want to know the relation and dependencies of these sensors simplest thing is to do a Pearson's correlation which gives me an N by N correlation matrix.  Now let's say if I have the correlation matrix but I want to explore tho possible readout space that can lead to such correlation matrix what can I do?  So the question is: given an N by N correlation matrix how you can get to a matrix that would have such correlation matrix? Any comment is appreciated.",,"['matrices', 'correlation']"
51,Gradient of ${\mathrm X} \mapsto \mbox{tr} \left(\mathrm A \mathrm X^{-1} \mathrm B \right)$,Gradient of,{\mathrm X} \mapsto \mbox{tr} \left(\mathrm A \mathrm X^{-1} \mathrm B \right),"Prove that $$\nabla_{\mathrm X} \mbox{tr} (\mathrm A \mathrm X^{-1} \mathrm B) = - \mathrm X^{-\top} \mathrm A^\top \mathrm B^\top \mathrm X^{-\top}$$ My proof is below. I am interested in other proofs. My proof Let $$f (\mathrm X) := \mbox{tr} (\mathrm A \mathrm X^{-1} \mathrm B)$$ Hence, $$\begin{array}{rl} f (\mathrm X + h \mathrm V) &= \mbox{tr} (\mathrm A (\mathrm X + h \mathrm V)^{-1} \mathrm B)\\ &= \mbox{tr} (\mathrm A (\mathrm X ( \mathrm I + h \mathrm X^{-1} \mathrm V))^{-1} \mathrm B)\\ &= \mbox{tr} (\mathrm A ( \mathrm I + h \mathrm X^{-1} \mathrm V)^{-1} \mathrm X^{-1} \mathrm B)\\ &= \mbox{tr} (\mathrm A ( \mathrm I - h \mathrm X^{-1} \mathrm V + O (h^2)) \mathrm X^{-1} \mathrm B)\\ &= \mbox{tr} (\mathrm A \mathrm X^{-1} \mathrm B) - h \, \mbox{tr} ( \mathrm A \mathrm X^{-1} \mathrm V \mathrm X^{-1} \mathrm B) + O (h^2)\\ &= f (\mathrm X) - h \, \mbox{tr} ( \mathrm X^{-1} \mathrm B \mathrm A \mathrm X^{-1} \mathrm V ) + O (h^2)\end{array}$$ Thus, the directional derivative of $f$ in the direction of $\mathrm V$ at $\mathrm X$ is $$D_{\mathrm V} f (\mathrm X) = - \mbox{tr} ( \mathrm X^{-1} \mathrm B \mathrm A \mathrm X^{-1} \mathrm V ) = - \mbox{tr} ( (\mathrm X^{-\top} \mathrm A^\top \mathrm B^\top \mathrm X^{-\top})^\top \mathrm V ) = - \langle \mathrm X^{-\top} \mathrm A^\top \mathrm B^\top \mathrm X^{-\top}, \mathrm V \rangle$$ and, lastly, $$\nabla_{\mathrm X} \mbox{tr} (\mathrm A \mathrm X^{-1} \mathrm B) = - \mathrm X^{-\top} \mathrm A^\top \mathrm B^\top \mathrm X^{-\top}$$","Prove that $$\nabla_{\mathrm X} \mbox{tr} (\mathrm A \mathrm X^{-1} \mathrm B) = - \mathrm X^{-\top} \mathrm A^\top \mathrm B^\top \mathrm X^{-\top}$$ My proof is below. I am interested in other proofs. My proof Let $$f (\mathrm X) := \mbox{tr} (\mathrm A \mathrm X^{-1} \mathrm B)$$ Hence, $$\begin{array}{rl} f (\mathrm X + h \mathrm V) &= \mbox{tr} (\mathrm A (\mathrm X + h \mathrm V)^{-1} \mathrm B)\\ &= \mbox{tr} (\mathrm A (\mathrm X ( \mathrm I + h \mathrm X^{-1} \mathrm V))^{-1} \mathrm B)\\ &= \mbox{tr} (\mathrm A ( \mathrm I + h \mathrm X^{-1} \mathrm V)^{-1} \mathrm X^{-1} \mathrm B)\\ &= \mbox{tr} (\mathrm A ( \mathrm I - h \mathrm X^{-1} \mathrm V + O (h^2)) \mathrm X^{-1} \mathrm B)\\ &= \mbox{tr} (\mathrm A \mathrm X^{-1} \mathrm B) - h \, \mbox{tr} ( \mathrm A \mathrm X^{-1} \mathrm V \mathrm X^{-1} \mathrm B) + O (h^2)\\ &= f (\mathrm X) - h \, \mbox{tr} ( \mathrm X^{-1} \mathrm B \mathrm A \mathrm X^{-1} \mathrm V ) + O (h^2)\end{array}$$ Thus, the directional derivative of $f$ in the direction of $\mathrm V$ at $\mathrm X$ is $$D_{\mathrm V} f (\mathrm X) = - \mbox{tr} ( \mathrm X^{-1} \mathrm B \mathrm A \mathrm X^{-1} \mathrm V ) = - \mbox{tr} ( (\mathrm X^{-\top} \mathrm A^\top \mathrm B^\top \mathrm X^{-\top})^\top \mathrm V ) = - \langle \mathrm X^{-\top} \mathrm A^\top \mathrm B^\top \mathrm X^{-\top}, \mathrm V \rangle$$ and, lastly, $$\nabla_{\mathrm X} \mbox{tr} (\mathrm A \mathrm X^{-1} \mathrm B) = - \mathrm X^{-\top} \mathrm A^\top \mathrm B^\top \mathrm X^{-\top}$$",,"['matrices', 'derivatives', 'matrix-calculus', 'trace', 'scalar-fields']"
52,Prove the Hadamard product representation,Prove the Hadamard product representation,,"Let $A$ and $B$ be $m \times n$ matrices with low-rank structures: $$ A = U_{A}\Sigma_{A}V_{A}^{T},\quad B= U_{B}\Sigma_{B}V_{B}^{T}, $$ Prove that Hadamard product $A\circ B$ admits the following representation $$ A\circ B = (U_{A}^T\odot U_{B}^T)^T (\Sigma_{A}\otimes\Sigma_{B})(V_{A}^{T}\odot V_{B}^{T}), $$ where $\odot$ represents the Khatri-Rao product, and $\otimes$ the Kronecker product.","Let and be matrices with low-rank structures: Prove that Hadamard product admits the following representation where represents the Khatri-Rao product, and the Kronecker product.","A B m \times n 
A = U_{A}\Sigma_{A}V_{A}^{T},\quad B= U_{B}\Sigma_{B}V_{B}^{T},
 A\circ B 
A\circ B = (U_{A}^T\odot U_{B}^T)^T (\Sigma_{A}\otimes\Sigma_{B})(V_{A}^{T}\odot V_{B}^{T}),
 \odot \otimes","['matrices', 'matrix-decomposition', 'svd', 'kronecker-product', 'hadamard-product']"
53,Could one make a ring of matrices of uncountable size?,Could one make a ring of matrices of uncountable size?,,"I've seen several kinds of matrices. You could see a real square matrix as a mapping: $$ A \quad : \quad \{1, 2,\cdots, n \}^2 \ \longrightarrow \ \mathbb{R} $$ I've seen that there were also infinite matrices like these: $$ A \quad : \quad \mathbb{Z} \times \mathbb{Z} \ \longrightarrow \ \mathbb{R} $$ So I wondered if we could make an uncountable matrix like this one: $$ A \quad : \quad[0,1] \times [0,1] \ \longrightarrow \ \mathbb{R} $$ Equipped with the following matrixproduct: $$ f \circ g \quad : \quad (x_0,y_0) \ \longmapsto \ \int_0^1f(t,y_0)g(x_0,1-t) dt $$ The function that maps everything to $0$ could be seen as the $0$-element.  I wondered if this could become a ring with with some neutral matrix. I'd say that the following map described should somehow be this map, but the required features of a unitary element don't hold.  $$ f(x,y) = 0  \ \text{ if } \ x \neq 1-y  \qquad \text{ and } \qquad f(x,1-x) = 1 \  $$ Do you think that we can find another unitary element to make this a ring?","I've seen several kinds of matrices. You could see a real square matrix as a mapping: $$ A \quad : \quad \{1, 2,\cdots, n \}^2 \ \longrightarrow \ \mathbb{R} $$ I've seen that there were also infinite matrices like these: $$ A \quad : \quad \mathbb{Z} \times \mathbb{Z} \ \longrightarrow \ \mathbb{R} $$ So I wondered if we could make an uncountable matrix like this one: $$ A \quad : \quad[0,1] \times [0,1] \ \longrightarrow \ \mathbb{R} $$ Equipped with the following matrixproduct: $$ f \circ g \quad : \quad (x_0,y_0) \ \longmapsto \ \int_0^1f(t,y_0)g(x_0,1-t) dt $$ The function that maps everything to $0$ could be seen as the $0$-element.  I wondered if this could become a ring with with some neutral matrix. I'd say that the following map described should somehow be this map, but the required features of a unitary element don't hold.  $$ f(x,y) = 0  \ \text{ if } \ x \neq 1-y  \qquad \text{ and } \qquad f(x,1-x) = 1 \  $$ Do you think that we can find another unitary element to make this a ring?",,"['matrices', 'ring-theory']"
54,Maurer-Cartan 1-form,Maurer-Cartan 1-form,,"Can anyone help me with the following? Let $\rho$ be the right-invariant Maurer-Cartan 1-form $$\rho = dg\  g^{-1}$$ I want to show that the MC equation $$d\rho - \rho \wedge\rho = 0$$ holds. So I compute $$d\rho = -dg \wedge d(g^{-1}) = dg\wedge g^{-1}dg\ g^{-1}$$ Why am I allowed to take the $g^{-1}$ through the wedge to get the right result? Naively it seems this should be wrong, because the wedge is essentially a commutator of matrices. Or is my notation too simplistic. I'm aware that I can get this result more generally by considering the structure equation for right-invariant forms, but ideally I'd like to make this direct computation rigorous, if possible! Many thanks in advance!","Can anyone help me with the following? Let $\rho$ be the right-invariant Maurer-Cartan 1-form $$\rho = dg\  g^{-1}$$ I want to show that the MC equation $$d\rho - \rho \wedge\rho = 0$$ holds. So I compute $$d\rho = -dg \wedge d(g^{-1}) = dg\wedge g^{-1}dg\ g^{-1}$$ Why am I allowed to take the $g^{-1}$ through the wedge to get the right result? Naively it seems this should be wrong, because the wedge is essentially a commutator of matrices. Or is my notation too simplistic. I'm aware that I can get this result more generally by considering the structure equation for right-invariant forms, but ideally I'd like to make this direct computation rigorous, if possible! Many thanks in advance!",,"['matrices', 'differential-geometry', 'lie-groups', 'differential-forms']"
55,what does it mean for a matrix to be greater than another?,what does it mean for a matrix to be greater than another?,,"I am reading these notes on viscosity solutions, here is a theorem: Let us assume $u\in C^2$ is a classical solution of $F(x,u,Du,D^2u)=0$, $x\in \Omega$ then $u$ is a viscosity solution whenever one of the following two is satsified: 1 The PDE does not depend on $D^2u$ 2 $F(x,z,p,M)\leq F(x,z,p,N)$ when $M\geq N$ I believe here $M$ and $N$ are Hessian matrices. My question is what does it mean that a matrix is greater or equal to another? I remember $M\geq 0$ means it is semi-positive definite, does it mean $M-N$ needs to be semi-positive definite then?","I am reading these notes on viscosity solutions, here is a theorem: Let us assume $u\in C^2$ is a classical solution of $F(x,u,Du,D^2u)=0$, $x\in \Omega$ then $u$ is a viscosity solution whenever one of the following two is satsified: 1 The PDE does not depend on $D^2u$ 2 $F(x,z,p,M)\leq F(x,z,p,N)$ when $M\geq N$ I believe here $M$ and $N$ are Hessian matrices. My question is what does it mean that a matrix is greater or equal to another? I remember $M\geq 0$ means it is semi-positive definite, does it mean $M-N$ needs to be semi-positive definite then?",,"['matrices', 'notation']"
56,Can we show that the determinant of this matrix is non-zero?,Can we show that the determinant of this matrix is non-zero?,,"Consider the following symmetric matrix $M=   \begin{bmatrix}     f(x) & f(2x) & \dots & f(nx)\\     f(2x) & f(4x) & \dots & f(2nx)\\      \vdots & \vdots & \dots & \vdots\\      f(nx) & f(2nx) & \dots & f(n^2x)   \end{bmatrix}$ , where $f(x): \mathbb{R} \rightarrow \mathbb{R}$ is a continuous, nonlinear, and strictly increasing that satisfies the following properties: $f(0) = 0$ and $f(x \neq 0) \neq 0$ If $a,b,c,d \neq 0$ and $ab = cd$ and $a+b > c+d \Rightarrow f(a)f(b) < f(c)f(d)$ . Can we show that there exists an $x \in \mathbb{R}$ such that the determinant of $M$ is non-zero? Proof for $n=1$ is trivial. For $n=2$ we have $det(M) = f(x)f(4x) - f(2x)f(2x)$ , which is less than 0 for $x > 0$ and greater than 0 for $x < 0$ based on assumption 2. Can we prove this for a general $n$ ? EDIT: I simulated matrix $M$ for different values of $n$ and the determinant is non-zero for almost every $x$ . Is it possible to perhaps prove this by contradiction? EDIT2: Another way to look at this problem is to show that $f(mx)$ are linearly independent functions. In other words, if $k_1 f(x) + k_2 f(2x) + \dots k_n f(nx) = 0$ , for all $x \in \mathbb{R}$ , then $k_1 = k_2 = \dots = k_n = 0$ . Under what conditions on $f$ , $f(mx)$ are linearly independent?","Consider the following symmetric matrix , where is a continuous, nonlinear, and strictly increasing that satisfies the following properties: and If and and . Can we show that there exists an such that the determinant of is non-zero? Proof for is trivial. For we have , which is less than 0 for and greater than 0 for based on assumption 2. Can we prove this for a general ? EDIT: I simulated matrix for different values of and the determinant is non-zero for almost every . Is it possible to perhaps prove this by contradiction? EDIT2: Another way to look at this problem is to show that are linearly independent functions. In other words, if , for all , then . Under what conditions on , are linearly independent?","M=
  \begin{bmatrix}
    f(x) & f(2x) & \dots & f(nx)\\
    f(2x) & f(4x) & \dots & f(2nx)\\ 
    \vdots & \vdots & \dots & \vdots\\ 
    f(nx) & f(2nx) & \dots & f(n^2x)
  \end{bmatrix} f(x): \mathbb{R} \rightarrow \mathbb{R} f(0) = 0 f(x \neq 0) \neq 0 a,b,c,d \neq 0 ab = cd a+b > c+d \Rightarrow f(a)f(b) < f(c)f(d) x \in \mathbb{R} M n=1 n=2 det(M) = f(x)f(4x) - f(2x)f(2x) x > 0 x < 0 n M n x f(mx) k_1 f(x) + k_2 f(2x) + \dots k_n f(nx) = 0 x \in \mathbb{R} k_1 = k_2 = \dots = k_n = 0 f f(mx)","['matrices', 'vector-spaces', 'vectors', 'determinant', 'functional-inequalities']"
57,A determinant involving a polynomial is $0$,A determinant involving a polynomial is,0,"Let $n \geq 2$ and $f:\mathbb{R} \to \mathbb{R}, \: f(x)=(x-x_1)(x-x_2)\dots(x-x_n)$ where $x_1,\dots, x_n$ are distinct real numbers. The matrix $A=(a_{ij})_{1 \leq i,j \leq n}$ is defined as follows:   $$a_{ij}=\begin{cases} \dfrac{f'(x_i)-f'(x_j)}{x_i-x_j}, & \text{if } i \neq j \\[6px] f''(x_i), & \text{if } i = j \end{cases}$$   Prove that $\det A = 0$ This is related to this problem, so I know that $$\frac{1}{f(x)}=\sum_{k=1}^n \frac{1}{f'(x_k)(x-x_k)}, \quad \forall x \neq x_i$$ I tried to write the terms of $A$, but getting the second derivative of $f$ in terms of those $x_i$ is pretty complicated. So I tried to simplify it by writing $$f(x)=a_nx^n+a_{n-1}x^{n-1}+\dots+a_0$$ and so $f''(x)=n(n-1)x^{n-2}+(n-1)(n-2)x^{n-3}+\dots+2\cdot 1\cdot x^0$. However, I am not sure this will lead to something helpful.","Let $n \geq 2$ and $f:\mathbb{R} \to \mathbb{R}, \: f(x)=(x-x_1)(x-x_2)\dots(x-x_n)$ where $x_1,\dots, x_n$ are distinct real numbers. The matrix $A=(a_{ij})_{1 \leq i,j \leq n}$ is defined as follows:   $$a_{ij}=\begin{cases} \dfrac{f'(x_i)-f'(x_j)}{x_i-x_j}, & \text{if } i \neq j \\[6px] f''(x_i), & \text{if } i = j \end{cases}$$   Prove that $\det A = 0$ This is related to this problem, so I know that $$\frac{1}{f(x)}=\sum_{k=1}^n \frac{1}{f'(x_k)(x-x_k)}, \quad \forall x \neq x_i$$ I tried to write the terms of $A$, but getting the second derivative of $f$ in terms of those $x_i$ is pretty complicated. So I tried to simplify it by writing $$f(x)=a_nx^n+a_{n-1}x^{n-1}+\dots+a_0$$ and so $f''(x)=n(n-1)x^{n-2}+(n-1)(n-2)x^{n-3}+\dots+2\cdot 1\cdot x^0$. However, I am not sure this will lead to something helpful.",,"['matrices', 'derivatives', 'polynomials', 'determinant', 'partial-fractions']"
58,What is known about the eigenvectors of random matrices?,What is known about the eigenvectors of random matrices?,,"Let $A$ be a real asymmetric $n \times n$ matrix with i.i.d. random, zero-mean elements.  What results, if any, are there for the eigenvectors of $A$?  In particular: How are the elements of the eigenvectors distributed? If $u_i$ and $u_j$ are eigenvectors of $A$, what is the distribution of $|u_i^*u_j|$? Numerically, I've found that every eigenvector corresponding to a complex eigenvalue has a single real element.  (Naturally, real eigenvalues have corresponding real eigenvectors.)  Has this been proven? What is the expected number of real eigenvalues of $A$? (Note: I'm really interested in constructing random matrices $A = VDV^{-1}$ where $D$ is a diagonal matrix of eigenvalues drawn from a distribution that differs from the one given by the various circular laws, and $V$ is the matrix of eigenvectors drawn from the distribution of eigenvectors of random matrices.  So this question can be summarized: how do I draw $V$?)","Let $A$ be a real asymmetric $n \times n$ matrix with i.i.d. random, zero-mean elements.  What results, if any, are there for the eigenvectors of $A$?  In particular: How are the elements of the eigenvectors distributed? If $u_i$ and $u_j$ are eigenvectors of $A$, what is the distribution of $|u_i^*u_j|$? Numerically, I've found that every eigenvector corresponding to a complex eigenvalue has a single real element.  (Naturally, real eigenvalues have corresponding real eigenvectors.)  Has this been proven? What is the expected number of real eigenvalues of $A$? (Note: I'm really interested in constructing random matrices $A = VDV^{-1}$ where $D$ is a diagonal matrix of eigenvalues drawn from a distribution that differs from the one given by the various circular laws, and $V$ is the matrix of eigenvectors drawn from the distribution of eigenvectors of random matrices.  So this question can be summarized: how do I draw $V$?)",,"['matrices', 'reference-request', 'eigenvalues-eigenvectors', 'random-matrices']"
59,$AB=z \mathrm{Id}_n$ implies $z^m BA = z^{m+1} \mathrm{Id}_n$ for what $m$?,implies  for what ?,AB=z \mathrm{Id}_n z^m BA = z^{m+1} \mathrm{Id}_n m,"This question builds on a series of questions looking for elementary proofs that $AB=\mathrm{Id}$ implies $BA=\mathrm{Id}$, for $A$ and $B$ both $n \times n$ matrices over a commutative ring. First the question, then motivation, then a small bit of progress. Let $R$ be a commutative ring. Let $A$ and $B$ be $n \times n$ matrices with entries in $R$ satisfying $AB=z \mathrm{Id}_n$ for some $z \in R$. We cannot conclude that $BA = z \mathrm{Id}_n$: Take $R=\mathbb{Z}$, $z=0$, $A = \left( \begin{smallmatrix} 0 & 1 \\ 0 & 0 \end{smallmatrix} \right)$ and $B = \left( \begin{smallmatrix} 1 & 0 \\ 0 & 0 \end{smallmatrix} \right)$. However, we can conclude $z^n BA = z^{n+1} \mathrm{Id}$ (proof below). What is the smallest $m$ (as a function of $n$) for which we can conclude $z^m BA = z^{m+1} \mathrm{Id}$? Motivation Suppose that we can do this for some particular $m$ and $n$. Let $k[a,b,z]$ be the polynomial ring in $2n^2+1$ variables, $a_{ij}$, $b_{ij}$ and $z$ (over some field $k$). Let $S$ be the quotient of $k[a,b,z]$ by the $n^2$ relations gotten by expanding the matrix product $AB=z \mathrm{Id}_n$. Since this question is framed for an arbitrary commutative ring, it applies in particular to $S$. So, if the answer is ""yes"" for some $(m,n)$, then the entries of $z^m BA - z^{m+1} \mathrm{Id}_n$ are zero in $S$. In other words, they lie in the ideal of $k[a,b,z]$ generated by the entries of $AB-z \mathrm{Id}_n$. I.e. we must have polynomial relations $$z^m \left( \sum_r B_{ir} A_{rj} - z \delta_{ij} \right) = \sum_{k, \ell} P_{ij}^{k \ell}(a, b, z) \left( \sum_s A_{ks} A_{s\ell} - z \delta_{k \ell} \right) \quad (\ast)$$ for some polynomials $P_{ij}^{k \ell}(a, b, z)$. Put a grading on $k[a,b,z]$, where $\deg a_{ij} = \deg b_{ij}=1$ and $\deg z=2$. Then we may assume that the $P_{ij}^{k \ell}$ have degree $m$. Plugging in $z=1$ to $(\ast)$, we get a high school algebra proof that $AB=\mathrm{Id}_n$ implies $BA = \mathrm{Id}_n$ involving polynomials of degree $\leq m$. Conversely, if we have a proof that $AB=\mathrm{Id}_n$ implies $BA = \mathrm{Id}_n$  by pure high school algebra (adding, multiplying and dividing polynomials and substituting in the equations $AB = \mathrm{Id}_n$) then we can take every formula in that proof in replace each $1$ by an appropriate power of $z$ to make the equations homogenous. The result will be a proof of $(\ast)$ for some $m$ and, again, the size of $m$ is a measure of the complexity of the proof. Minor progress It is good enough to take $m=n$. Proof: simplify $(\det B) \mathrm{Ad}(A) A B A$ in two ways. One way gives $$(\det B) {\Big (} \mathrm{Ad}(A) A {\Big )} B A = (\det B) (\det A) BA = (\det A) ( \det B) BA = \det(AB) BA = z^n BA$$ the other gives $$(\det B) \mathrm{Ad}(A) {\Big (}  A  B {\Big )} A =(\det B) \mathrm{Ad}(A) z A = z (\det B) (\det A) = z \det(AB) = z^{n+1}.$$ When $n=1$, we can take $m=0$, as $R$ is commutative. For $n=2$, the above example of $z=0$, $A = \left( \begin{smallmatrix} 0 & 1 \\ 0 & 0 \end{smallmatrix} \right)$ and $B = \left( \begin{smallmatrix} 1 & 0 \\ 0 & 0 \end{smallmatrix} \right)$ shows we need $m \geq 1$ and it is easy to adapt this to show that we need $m \geq 1$ for any $n \geq 1$. This is all I know! I don't even have an example to show that $m \geq 2$ is necessary for any $n$, although my gut says $m$ should grow linearly with $n$.","This question builds on a series of questions looking for elementary proofs that $AB=\mathrm{Id}$ implies $BA=\mathrm{Id}$, for $A$ and $B$ both $n \times n$ matrices over a commutative ring. First the question, then motivation, then a small bit of progress. Let $R$ be a commutative ring. Let $A$ and $B$ be $n \times n$ matrices with entries in $R$ satisfying $AB=z \mathrm{Id}_n$ for some $z \in R$. We cannot conclude that $BA = z \mathrm{Id}_n$: Take $R=\mathbb{Z}$, $z=0$, $A = \left( \begin{smallmatrix} 0 & 1 \\ 0 & 0 \end{smallmatrix} \right)$ and $B = \left( \begin{smallmatrix} 1 & 0 \\ 0 & 0 \end{smallmatrix} \right)$. However, we can conclude $z^n BA = z^{n+1} \mathrm{Id}$ (proof below). What is the smallest $m$ (as a function of $n$) for which we can conclude $z^m BA = z^{m+1} \mathrm{Id}$? Motivation Suppose that we can do this for some particular $m$ and $n$. Let $k[a,b,z]$ be the polynomial ring in $2n^2+1$ variables, $a_{ij}$, $b_{ij}$ and $z$ (over some field $k$). Let $S$ be the quotient of $k[a,b,z]$ by the $n^2$ relations gotten by expanding the matrix product $AB=z \mathrm{Id}_n$. Since this question is framed for an arbitrary commutative ring, it applies in particular to $S$. So, if the answer is ""yes"" for some $(m,n)$, then the entries of $z^m BA - z^{m+1} \mathrm{Id}_n$ are zero in $S$. In other words, they lie in the ideal of $k[a,b,z]$ generated by the entries of $AB-z \mathrm{Id}_n$. I.e. we must have polynomial relations $$z^m \left( \sum_r B_{ir} A_{rj} - z \delta_{ij} \right) = \sum_{k, \ell} P_{ij}^{k \ell}(a, b, z) \left( \sum_s A_{ks} A_{s\ell} - z \delta_{k \ell} \right) \quad (\ast)$$ for some polynomials $P_{ij}^{k \ell}(a, b, z)$. Put a grading on $k[a,b,z]$, where $\deg a_{ij} = \deg b_{ij}=1$ and $\deg z=2$. Then we may assume that the $P_{ij}^{k \ell}$ have degree $m$. Plugging in $z=1$ to $(\ast)$, we get a high school algebra proof that $AB=\mathrm{Id}_n$ implies $BA = \mathrm{Id}_n$ involving polynomials of degree $\leq m$. Conversely, if we have a proof that $AB=\mathrm{Id}_n$ implies $BA = \mathrm{Id}_n$  by pure high school algebra (adding, multiplying and dividing polynomials and substituting in the equations $AB = \mathrm{Id}_n$) then we can take every formula in that proof in replace each $1$ by an appropriate power of $z$ to make the equations homogenous. The result will be a proof of $(\ast)$ for some $m$ and, again, the size of $m$ is a measure of the complexity of the proof. Minor progress It is good enough to take $m=n$. Proof: simplify $(\det B) \mathrm{Ad}(A) A B A$ in two ways. One way gives $$(\det B) {\Big (} \mathrm{Ad}(A) A {\Big )} B A = (\det B) (\det A) BA = (\det A) ( \det B) BA = \det(AB) BA = z^n BA$$ the other gives $$(\det B) \mathrm{Ad}(A) {\Big (}  A  B {\Big )} A =(\det B) \mathrm{Ad}(A) z A = z (\det B) (\det A) = z \det(AB) = z^{n+1}.$$ When $n=1$, we can take $m=0$, as $R$ is commutative. For $n=2$, the above example of $z=0$, $A = \left( \begin{smallmatrix} 0 & 1 \\ 0 & 0 \end{smallmatrix} \right)$ and $B = \left( \begin{smallmatrix} 1 & 0 \\ 0 & 0 \end{smallmatrix} \right)$ shows we need $m \geq 1$ and it is easy to adapt this to show that we need $m \geq 1$ for any $n \geq 1$. This is all I know! I don't even have an example to show that $m \geq 2$ is necessary for any $n$, although my gut says $m$ should grow linearly with $n$.",,"['matrices', 'algebraic-geometry', 'commutative-algebra']"
60,Eigenvalues of a $12 \times 12$ Jacobian matrix,Eigenvalues of a  Jacobian matrix,12 \times 12,"Consider the set of coordinates $X_{i, j}^{(\ell)}$ and $Y_{i, j}^{(\ell)}$ , where $i \in (1, 2, 3), j \in (1, 2, 3)$ and $i \neq j$ and $\ell = \pm 1$ . The superscipt $(\ell)$ is an index. Consider the change of variables from $\mathbf{X}$ to $\mathbf{Y}$ defined by \begin{equation} Y_{i, j}^{(\ell)} = c_{i, j}^{(\ell)} + c_{i, k}^{(\ell)} X_{k, j}^{(\ell)} + c_{i, j}^{(-\ell)} X_{j, i}^{(-\ell)} X_{i, j}^{(\ell)} + c_{i, k}^{(-\ell)} X_{k, i}^{(-\ell)} X_{i, j}^{(\ell)} \tag{1} \end{equation} where $(i, j, k)$ is any permutation of $(1, 2, 3)$ and the $c_{i, j}^{(\ell)}$ are any non-zero numbers constrained, for each $i$ , by $c_{i, j}^{(1)} + c_{i, k}^{(1)} + c_{i, j}^{(-1)} + c_{i, k}^{(-1)} = K$ for some constant $K > 0$ . So, for example, if $i = 1$ then $c_{1, 2}^{(1)} + c_{1, 3}^{(1)} + c_{1, 2}^{(-1)} + c_{1, 3}^{(-1)} = K$ . Let $\mathbf{J}(\mathbf{X})= \partial \mathbf{Y}/ \partial \mathbf{X}$ be the Jacobian matrix of the transformation in Eq. $(1)$ . Let $\mathbf{1}$ be a vector of length $12$ consisting of all $1$ 's. Fact 1 : The constant $K$ is an eigenvalue of $\mathbf{J}(\mathbf{1})$ . Fact 2 : If there is a vector $\mathbf{v}$ satisfying $\mathbf{J}(\mathbf{1}) \mathbf{v} = K \mathbf{v}$ , then it has the property that $v_{i, j}^{(k)} + v_{j, i}^{(k)} = 0$ , where $v_{a, b}^{(c)}$ is the entry of $\mathbf{v}$ corresponding to coordinate $X_{a, b}^{(c)}$ . I know Fact 1 and Fact 2 are facts, because the eigensystem of a $12 \times 12$ matrix is still (barely) within the realm of tractability for computers. My question is therefore: Is there any way to see that either Fact 1 or Fact 2 is true without having to actually find the eigenvector? A relevant, similar question I have asked is here . In that case, it was ""easy"" to guess the eigenvector but I can't see how to do that here. It has to be something about the structure of Eq. $(1)$ . There's this idea that if the sum of the coefficients of a transformation is constrained to be a constant in the way I've described, then this constraint shows up as an eigenvalue of the Jacobian and I wonder how general this property is. Mathematica Code Using the convention $c_{i, j}^{(\ell)} = c[i, j, \ell]$ , the Jacobian is jac = {{c[1, 2, 1] + c[1, 3, 1], 0, 0, 0, 0, c[1, 2, 1], 0, 0, 0, c[1, 3, 1], c[1, 3, -1], 0}, {0, c[1, 2, -1] + c[1, 3, -1], 0, 0, c[1, 2, -1], 0, 0, 0, c[1, 3, -1], 0, 0, c[1, 3, 1]}, {0, 0, c[1, 2, 1] + c[1, 3, 1], 0, 0, c[1, 2, 1], c[1, 2, -1], 0, 0, c[1, 3, 1], 0, 0}, {0, 0, 0, c[1, 2, -1] + c[1, 3, -1], c[1, 2, -1], 0, 0, c[1, 2, 1], c[1, 3, -1], 0, 0, 0}, {0, c[2, 1, 1], 0, 0, c[2, 1, 1] + c[2, 3, 1], 0, 0, 0, c[2, 3, -1], 0, 0, c[2, 3, 1]}, {c[2, 1, -1], 0, 0, 0, 0, c[2, 1, -1] + c[2, 3, -1], 0, 0, 0, c[2, 3, 1], c[2, 3, -1], 0}, {0, c[2, 1, 1], c[2, 1, -1], 0, 0, 0, c[2, 1, 1] + c[2, 3, 1], 0, 0, 0, 0, c[2, 3, 1]}, {c[2, 1, -1], 0, 0, c[2, 1, 1], 0, 0, 0, c[2, 1, -1] + c[2, 3, -1], 0, 0, c[2, 3, -1], 0}, {0, 0, 0, c[3, 1, 1], c[3, 2, -1], 0, 0, c[3, 2, 1], c[3, 1, 1] + c[3, 2, 1], 0, 0, 0}, {0, 0, c[3, 1, -1], 0, 0, c[3, 2, 1], c[3, 2, -1], 0, 0, c[3, 1, -1] + c[3, 2, -1], 0, 0}, {c[3, 1, -1], 0, 0, c[3, 1, 1], 0, 0, 0, c[3, 2, 1], 0, 0, c[3, 1, 1] + c[3, 2, 1], 0}, {0, c[3, 1, 1], c[3, 1, -1], 0, 0, 0, c[3, 2, -1], 0, 0, 0, 0, c[3, 1, -1] + c[3, 2, -1]}} Where the rows are in the order rows = {c[1, 2, -1], c[1, 2, 1], c[1, 3, -1], c[1, 3, 1], c[2, 1, -1], c[2, 1, 1],       c[2, 3, -1], c[2, 3, 1], c[3, 1, -1], c[3, 1, 1], c[3, 2, -1], c[3, 2, 1]} Here's a sample set of $c_{i,j}^{(\ell)}$ where $K = 1$ : vec = {0.213509, -0.0587312, 0.321703, 0.523519, 0.255562, -0.290697,      0.211956, 0.823179, 0.0321628, 1.087800, -0.373663, 0.253700} Running the following command: Eigenvalues[jac /. Thread[rows -> vec]] Shows that $1$ is indeed an eigenvalue.","Consider the set of coordinates and , where and and . The superscipt is an index. Consider the change of variables from to defined by where is any permutation of and the are any non-zero numbers constrained, for each , by for some constant . So, for example, if then . Let be the Jacobian matrix of the transformation in Eq. . Let be a vector of length consisting of all 's. Fact 1 : The constant is an eigenvalue of . Fact 2 : If there is a vector satisfying , then it has the property that , where is the entry of corresponding to coordinate . I know Fact 1 and Fact 2 are facts, because the eigensystem of a matrix is still (barely) within the realm of tractability for computers. My question is therefore: Is there any way to see that either Fact 1 or Fact 2 is true without having to actually find the eigenvector? A relevant, similar question I have asked is here . In that case, it was ""easy"" to guess the eigenvector but I can't see how to do that here. It has to be something about the structure of Eq. . There's this idea that if the sum of the coefficients of a transformation is constrained to be a constant in the way I've described, then this constraint shows up as an eigenvalue of the Jacobian and I wonder how general this property is. Mathematica Code Using the convention , the Jacobian is jac = {{c[1, 2, 1] + c[1, 3, 1], 0, 0, 0, 0, c[1, 2, 1], 0, 0, 0, c[1, 3, 1], c[1, 3, -1], 0}, {0, c[1, 2, -1] + c[1, 3, -1], 0, 0, c[1, 2, -1], 0, 0, 0, c[1, 3, -1], 0, 0, c[1, 3, 1]}, {0, 0, c[1, 2, 1] + c[1, 3, 1], 0, 0, c[1, 2, 1], c[1, 2, -1], 0, 0, c[1, 3, 1], 0, 0}, {0, 0, 0, c[1, 2, -1] + c[1, 3, -1], c[1, 2, -1], 0, 0, c[1, 2, 1], c[1, 3, -1], 0, 0, 0}, {0, c[2, 1, 1], 0, 0, c[2, 1, 1] + c[2, 3, 1], 0, 0, 0, c[2, 3, -1], 0, 0, c[2, 3, 1]}, {c[2, 1, -1], 0, 0, 0, 0, c[2, 1, -1] + c[2, 3, -1], 0, 0, 0, c[2, 3, 1], c[2, 3, -1], 0}, {0, c[2, 1, 1], c[2, 1, -1], 0, 0, 0, c[2, 1, 1] + c[2, 3, 1], 0, 0, 0, 0, c[2, 3, 1]}, {c[2, 1, -1], 0, 0, c[2, 1, 1], 0, 0, 0, c[2, 1, -1] + c[2, 3, -1], 0, 0, c[2, 3, -1], 0}, {0, 0, 0, c[3, 1, 1], c[3, 2, -1], 0, 0, c[3, 2, 1], c[3, 1, 1] + c[3, 2, 1], 0, 0, 0}, {0, 0, c[3, 1, -1], 0, 0, c[3, 2, 1], c[3, 2, -1], 0, 0, c[3, 1, -1] + c[3, 2, -1], 0, 0}, {c[3, 1, -1], 0, 0, c[3, 1, 1], 0, 0, 0, c[3, 2, 1], 0, 0, c[3, 1, 1] + c[3, 2, 1], 0}, {0, c[3, 1, 1], c[3, 1, -1], 0, 0, 0, c[3, 2, -1], 0, 0, 0, 0, c[3, 1, -1] + c[3, 2, -1]}} Where the rows are in the order rows = {c[1, 2, -1], c[1, 2, 1], c[1, 3, -1], c[1, 3, 1], c[2, 1, -1], c[2, 1, 1],       c[2, 3, -1], c[2, 3, 1], c[3, 1, -1], c[3, 1, 1], c[3, 2, -1], c[3, 2, 1]} Here's a sample set of where : vec = {0.213509, -0.0587312, 0.321703, 0.523519, 0.255562, -0.290697,      0.211956, 0.823179, 0.0321628, 1.087800, -0.373663, 0.253700} Running the following command: Eigenvalues[jac /. Thread[rows -> vec]] Shows that is indeed an eigenvalue.","X_{i, j}^{(\ell)} Y_{i, j}^{(\ell)} i \in (1, 2, 3), j \in (1, 2, 3) i \neq j \ell = \pm 1 (\ell) \mathbf{X} \mathbf{Y} \begin{equation}
Y_{i, j}^{(\ell)} = c_{i, j}^{(\ell)} + c_{i, k}^{(\ell)} X_{k, j}^{(\ell)} + c_{i, j}^{(-\ell)} X_{j, i}^{(-\ell)} X_{i, j}^{(\ell)} + c_{i, k}^{(-\ell)} X_{k, i}^{(-\ell)} X_{i, j}^{(\ell)} \tag{1}
\end{equation} (i, j, k) (1, 2, 3) c_{i, j}^{(\ell)} i c_{i, j}^{(1)} + c_{i, k}^{(1)} + c_{i, j}^{(-1)} + c_{i, k}^{(-1)} = K K > 0 i = 1 c_{1, 2}^{(1)} + c_{1, 3}^{(1)} + c_{1, 2}^{(-1)} + c_{1, 3}^{(-1)} = K \mathbf{J}(\mathbf{X})= \partial \mathbf{Y}/ \partial \mathbf{X} (1) \mathbf{1} 12 1 K \mathbf{J}(\mathbf{1}) \mathbf{v} \mathbf{J}(\mathbf{1}) \mathbf{v} = K \mathbf{v} v_{i, j}^{(k)} + v_{j, i}^{(k)} = 0 v_{a, b}^{(c)} \mathbf{v} X_{a, b}^{(c)} 12 \times 12 (1) c_{i, j}^{(\ell)} = c[i, j, \ell] c_{i,j}^{(\ell)} K = 1 1","['matrices', 'eigenvalues-eigenvectors', 'nonlinear-system', 'jacobian']"
61,Cones of positive semidefinite matrices generated by matrices of rank $1$,Cones of positive semidefinite matrices generated by matrices of rank,1,"Let $S_n$ be the space of real $n \times n$ symmetric matrices and let $S_n^+$ be the convex cone of positive semidefinite matrices in $S_n$. The extremal rays of this cone correspond to the positive semidefinite matrices of rank $1$. My problem is that I have a subcone $C$ of $S_n^+$ which is given by (finitely many) inequalities, which are not all linear (quadratic for example). In $C$ I have another subcone $C'$ given by generators which correspond to rank $1$ matrices. Since I would like to compare $C$ and $C'$, it would be nice to have inequalities defining $C'$. So my question is if there are known methods to determine such inequalities.","Let $S_n$ be the space of real $n \times n$ symmetric matrices and let $S_n^+$ be the convex cone of positive semidefinite matrices in $S_n$. The extremal rays of this cone correspond to the positive semidefinite matrices of rank $1$. My problem is that I have a subcone $C$ of $S_n^+$ which is given by (finitely many) inequalities, which are not all linear (quadratic for example). In $C$ I have another subcone $C'$ given by generators which correspond to rank $1$ matrices. Since I would like to compare $C$ and $C'$, it would be nice to have inequalities defining $C'$. So my question is if there are known methods to determine such inequalities.",,"['matrices', 'convex-analysis', 'multilinear-algebra', 'rank-1-matrices']"
62,Is the matrix $\sum_{g \in G} a_g \rho(g)$ normal and what further properties does it have?,Is the matrix  normal and what further properties does it have?,\sum_{g \in G} a_g \rho(g),"Let $\rho$ be the regular representation of $G$ . $S \subset G$ a generating set, $|g| := |g|_S=$ word length with respect to $S$ . Then I construct such a matrix, where we have some ordering $g_i$ of the group $G$ : $$ a_{i,j} = \frac{1}{1+|g_i g_j^{-1}|}$$ This is a group matrix as defined by Dedekind and Frobenius. Let $H_G:= \sum_{g \in G} \frac{1}{|g|+1}$ be the harmonic number associated to $S$ and $G$ . Here are my conjectures concerning this matrix some of which I can prove: $H_G = |A|$ , where $|A|$ denotes the spectral norm of $A$ [proved with Perron Frobenus theorem] (If 1. is true, then by definition of $A$ we must have that $1/H_G A = 1/|A| A $ is a doubly stochastic matrix [that $\frac{1}{H_G}A_G$ is doubly stochastic and positive is clear from definition, so this is also proved] $A = \sum_{g \in G} \frac{1}{1+|g|} \rho(g)$ is the Birkhoff-Neunmann decomposition induced by the doubly stochastic matrix [ proved, by comment of darijgrinberg] Using 3. I can prove that $A$ is a normal matrix $A$ is non-singular. [that is unclear to me, but numerics suggest that's true] If we regard 3. as the definition of $A$ then I am able to show that $A$ is normal: It basically boils down to $\rho(g)^T = \rho(g)^{-1}$ as $\rho$ is a permutation matrix and hence orthogonal and that: $$\sum_{g,h \in G} \rho(g^{-1} h) = \sum_{g,h \in G} \rho(g h^{-1})$$ However I am a bit unsure about the last step. For the 1. part: I think of using the Perron Frobenius theorem on the positive and doubly stochastic matrix $$\frac{A}{H_G}$$ I can prove that $$A \cdot 1 = H_G \cdot 1$$ where $1$ is the vector consisting of all $1$ -s. For the 5. part: By a therorem (page 12) of Frobenius, we have: $$\det(X_{gh^{-1}}) = \prod_{ \rho \text{ irred }}\det(\sum_{g \in G} X_g \rho(g))^{\deg(\rho)}$$ Hence for 5. it remains to show that if $\psi$ is an irreducible representation then: $$0 \neq \det(\sum_{g \in G} \frac{1}{1+|g|} \psi(g))$$ Maybe this could be proved with Fourier Analysis of finite groups? Any help for proving 1-5 is highly appreciated. Thanks for your help! Related: https://mathoverflow.net/questions/330359/what-properties-characterize-the-function-lx-x-expx-logx","Let be the regular representation of . a generating set, word length with respect to . Then I construct such a matrix, where we have some ordering of the group : This is a group matrix as defined by Dedekind and Frobenius. Let be the harmonic number associated to and . Here are my conjectures concerning this matrix some of which I can prove: , where denotes the spectral norm of [proved with Perron Frobenus theorem] (If 1. is true, then by definition of we must have that is a doubly stochastic matrix [that is doubly stochastic and positive is clear from definition, so this is also proved] is the Birkhoff-Neunmann decomposition induced by the doubly stochastic matrix [ proved, by comment of darijgrinberg] Using 3. I can prove that is a normal matrix is non-singular. [that is unclear to me, but numerics suggest that's true] If we regard 3. as the definition of then I am able to show that is normal: It basically boils down to as is a permutation matrix and hence orthogonal and that: However I am a bit unsure about the last step. For the 1. part: I think of using the Perron Frobenius theorem on the positive and doubly stochastic matrix I can prove that where is the vector consisting of all -s. For the 5. part: By a therorem (page 12) of Frobenius, we have: Hence for 5. it remains to show that if is an irreducible representation then: Maybe this could be proved with Fourier Analysis of finite groups? Any help for proving 1-5 is highly appreciated. Thanks for your help! Related: https://mathoverflow.net/questions/330359/what-properties-characterize-the-function-lx-x-expx-logx","\rho G S \subset G |g| := |g|_S= S g_i G  a_{i,j} = \frac{1}{1+|g_i g_j^{-1}|} H_G:= \sum_{g \in G} \frac{1}{|g|+1} S G H_G = |A| |A| A A 1/H_G A = 1/|A| A  \frac{1}{H_G}A_G A = \sum_{g \in G} \frac{1}{1+|g|} \rho(g) A A A A \rho(g)^T = \rho(g)^{-1} \rho \sum_{g,h \in G} \rho(g^{-1} h) = \sum_{g,h \in G} \rho(g h^{-1}) \frac{A}{H_G} A \cdot 1 = H_G \cdot 1 1 1 \det(X_{gh^{-1}}) = \prod_{ \rho \text{ irred }}\det(\sum_{g \in G} X_g \rho(g))^{\deg(\rho)} \psi 0 \neq \det(\sum_{g \in G} \frac{1}{1+|g|} \psi(g))","['matrices', 'finite-groups']"
63,The practical usage of Arnold Matrix Trace Theorem,The practical usage of Arnold Matrix Trace Theorem,,I would like to ask about the Arnold's Matrix Trace theorem: $$\textrm{tr}\big(A^{p^k}\big)\equiv\textrm{tr}\big(A^{p^{k-1}}\big)\ (\!\!\bmod  {p^k}).$$ This theorem looks fantastic to me. But is there any practical usage of it or maybe some algorithm based on this? EDITED: Adding some reference: https://rjlipton.wordpress.com/2009/08/07/fermats-little-theorem-for-matrices/,I would like to ask about the Arnold's Matrix Trace theorem: $$\textrm{tr}\big(A^{p^k}\big)\equiv\textrm{tr}\big(A^{p^{k-1}}\big)\ (\!\!\bmod  {p^k}).$$ This theorem looks fantastic to me. But is there any practical usage of it or maybe some algorithm based on this? EDITED: Adding some reference: https://rjlipton.wordpress.com/2009/08/07/fermats-little-theorem-for-matrices/,,"['matrices', 'number-theory', 'algorithms', 'prime-numbers', 'trace']"
64,Determining sign(det(A)) for nearly-singular matrix A,Determining sign(det(A)) for nearly-singular matrix A,,"Motivation: determining whether a point $p$ is above or below a plane $\pi$, which is defined by $d$ points, in a $d$-dimensional space, is equivalent to computing the sign of a determinant of a matrix of the form A = $\begin{pmatrix}p_{1}^{\left(1\right)} & p_{1}^{\left(2\right)} & \cdots & p_{1}^{\left(d\right)} & 1\\ p_{2}^{\left(1\right)} & p_{2}^{\left(2\right)} & \cdots & p_{2}^{\left(d\right)} & 1\\ \vdots & \vdots & \ddots & \vdots & \vdots\\ p_{d}^{\left(1\right)} & p_{d}^{\left(2\right)} & \cdots & p_{d}^{\left(d\right)} & 1\\ p^{\left(1\right)} & p^{\left(2\right)} & \cdots & p^{\left(d\right)} & 1 \end{pmatrix}$ Notes - When I say sign, I mean either negative, positive, or exactly zero The entry ${p_i}^j$ means the $j$-coordinate (e.g., $x$, $y$ and $z$ when $d$=3) of the $i$'th point in my problem. The matrix is not a Vandermonde matrix. The resulting matrix is composed of floating-point numbers. It has no special structure, is not diagonally dominant, is not symmetric, and has no special reason to be positive-semidefinite. It might be sparse, though. It is, however, many times nearly singular. I'm interested in computing $\mbox{sign}(|A|)$ in a fast and robust way. Since this is Math-SE, I'm mainly asking if there are any spectral theorems which allow this computation without explicitly computing the determinant. For example, the Greshgorin Circle Theorem could be very useful if $A$ was diagonally-dominant. Alternatively, any cheap test to identify some cases would also be helpful.","Motivation: determining whether a point $p$ is above or below a plane $\pi$, which is defined by $d$ points, in a $d$-dimensional space, is equivalent to computing the sign of a determinant of a matrix of the form A = $\begin{pmatrix}p_{1}^{\left(1\right)} & p_{1}^{\left(2\right)} & \cdots & p_{1}^{\left(d\right)} & 1\\ p_{2}^{\left(1\right)} & p_{2}^{\left(2\right)} & \cdots & p_{2}^{\left(d\right)} & 1\\ \vdots & \vdots & \ddots & \vdots & \vdots\\ p_{d}^{\left(1\right)} & p_{d}^{\left(2\right)} & \cdots & p_{d}^{\left(d\right)} & 1\\ p^{\left(1\right)} & p^{\left(2\right)} & \cdots & p^{\left(d\right)} & 1 \end{pmatrix}$ Notes - When I say sign, I mean either negative, positive, or exactly zero The entry ${p_i}^j$ means the $j$-coordinate (e.g., $x$, $y$ and $z$ when $d$=3) of the $i$'th point in my problem. The matrix is not a Vandermonde matrix. The resulting matrix is composed of floating-point numbers. It has no special structure, is not diagonally dominant, is not symmetric, and has no special reason to be positive-semidefinite. It might be sparse, though. It is, however, many times nearly singular. I'm interested in computing $\mbox{sign}(|A|)$ in a fast and robust way. Since this is Math-SE, I'm mainly asking if there are any spectral theorems which allow this computation without explicitly computing the determinant. For example, the Greshgorin Circle Theorem could be very useful if $A$ was diagonally-dominant. Alternatively, any cheap test to identify some cases would also be helpful.",,"['matrices', 'algorithms', 'determinant']"
65,Is the characteristic polynomial of a matrix $\det(\lambda I-A)$ or$ \det(A-\lambda I)$?,Is the characteristic polynomial of a matrix  or?,\det(\lambda I-A)  \det(A-\lambda I),"I haven't been able to get a very clear answer on this. In the exercise that I performed to find the Characteristic Polynomial of a given Matrix, I used the determinant of $(\lambda I-A)$ to find the answer. I don't actually attend any courses or do anything that requires me to solve these problems, or even presents them to me regularly. My solving this problem is a result of me asking my friend who is in a college math course for his homework, because I'm personally interested in learning more about math. As such, I have to study the problems he gives me on my own, and can only use the internet, for the most part, in order to get the knowledge I need to solve them. While looking for the definition of 'Characteristic Polynomial', this website defines a Characteristic Polynomial as ""If $A$ is an $n\times n$ matrix, then the Characteristic Polynomial of $A$ is the function $f(\lambda )=\det(\lambda I−A)$ "". A few other resources I found also say this, while others say that it is the determinant of $(A-\lambda I)$ . I solved the problem, which, yes, did use an $n\times n$ Matrix ( $3\times 3$ to be specific), using the former equation of $(\lambda I-A)$ , and I presented the solution to my friend. He concurred, and we decided that was our answer, however when he entered it into whatever website assigns him his homework, it said that we were incorrect. We thought about it for a while, and even looked to see if the issue was that we hadn't simplified the problem properly, but everything checked out (I initially read $[\lambda I-A]$ as the correct determinant, so in my head, I didn't realize that some other websites used the inverse, and didn't think to try that). Eventually, we both gave up, and I used an online calculator to solve it, and was presented with an answer achieved by using $(A-\lambda I)$ . We put that in to the website, and sure enough it was correct. This causes some concern with me. Is $(A-\lambda I)$ always used to find the Characteristic Polynomial? How am I to remember that it is this way, and not the other? Why do we choose to define the Characteristic Polynomial as one determinant over the other? Is the website that I cited outright incorrect, and thus should be considered disreputed, or is there something unique that I fail to understand regarding certain Matrices and their polynomials?","I haven't been able to get a very clear answer on this. In the exercise that I performed to find the Characteristic Polynomial of a given Matrix, I used the determinant of to find the answer. I don't actually attend any courses or do anything that requires me to solve these problems, or even presents them to me regularly. My solving this problem is a result of me asking my friend who is in a college math course for his homework, because I'm personally interested in learning more about math. As such, I have to study the problems he gives me on my own, and can only use the internet, for the most part, in order to get the knowledge I need to solve them. While looking for the definition of 'Characteristic Polynomial', this website defines a Characteristic Polynomial as ""If is an matrix, then the Characteristic Polynomial of is the function "". A few other resources I found also say this, while others say that it is the determinant of . I solved the problem, which, yes, did use an Matrix ( to be specific), using the former equation of , and I presented the solution to my friend. He concurred, and we decided that was our answer, however when he entered it into whatever website assigns him his homework, it said that we were incorrect. We thought about it for a while, and even looked to see if the issue was that we hadn't simplified the problem properly, but everything checked out (I initially read as the correct determinant, so in my head, I didn't realize that some other websites used the inverse, and didn't think to try that). Eventually, we both gave up, and I used an online calculator to solve it, and was presented with an answer achieved by using . We put that in to the website, and sure enough it was correct. This causes some concern with me. Is always used to find the Characteristic Polynomial? How am I to remember that it is this way, and not the other? Why do we choose to define the Characteristic Polynomial as one determinant over the other? Is the website that I cited outright incorrect, and thus should be considered disreputed, or is there something unique that I fail to understand regarding certain Matrices and their polynomials?",(\lambda I-A) A n\times n A f(\lambda )=\det(\lambda I−A) (A-\lambda I) n\times n 3\times 3 (\lambda I-A) [\lambda I-A] (A-\lambda I) (A-\lambda I),['matrices']
66,Is this group of matrices cyclic? [closed],Is this group of matrices cyclic? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Is the group $H$ consisting of matrices of the form  $    \left( {\begin{array}{cc}    1 & n \\       0 & 1 \\      \end{array} } \right) $ cyclic, where $n \in \mathbb{Z}$? If not, how would you show this?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Is the group $H$ consisting of matrices of the form  $    \left( {\begin{array}{cc}    1 & n \\       0 & 1 \\      \end{array} } \right) $ cyclic, where $n \in \mathbb{Z}$? If not, how would you show this?",,"['matrices', 'cyclic-groups', 'algebraic-groups', 'linear-groups']"
67,Maximal eigenvalue is a convex function. Why?,Maximal eigenvalue is a convex function. Why?,,Let $A$ be a symmetric real matrix. Let $f(A)=\lambda_{\max}(A)$ be its largest eigenvalue. Why is $f$ convex?,Let $A$ be a symmetric real matrix. Let $f(A)=\lambda_{\max}(A)$ be its largest eigenvalue. Why is $f$ convex?,,"['matrices', 'eigenvalues-eigenvectors', 'convex-analysis']"
68,Calculating the exponential of an upper triangular matrix,Calculating the exponential of an upper triangular matrix,,Find the matrix exponential $e^A$ for $$ A = \begin{bmatrix} 2 & 1 & 1\\ 0 & 2 & 1\\ 0 & 0 & 2\\ \end{bmatrix}.$$ I think we should use the proberty If $AB = BA$ then $e^{A+B} = e^A e^B$ . We can use that $$\begin{bmatrix} 2 & 1 & 1\\ 0 & 2 & 1\\ 0 & 0 & 2\\ \end{bmatrix}  =\begin{bmatrix} 1 & 0 & 0\\ 0 & 1 & 0\\ 0 & 0 & 1\\ \end{bmatrix}  +\begin{bmatrix} 1 & 1 & 1\\ 0 & 1 & 1\\ 0 & 0 & 1\\ \end{bmatrix}$$ Both matrices obviously commute. But I dont know how to calculate the exponential of $$\begin{bmatrix} 1 & 1 & 1\\ 0 & 1 & 1\\ 0 & 0 & 1\\ \end{bmatrix}.$$ Could you help me?,Find the matrix exponential for I think we should use the proberty If then . We can use that Both matrices obviously commute. But I dont know how to calculate the exponential of Could you help me?,"e^A  A = \begin{bmatrix}
2 & 1 & 1\\
0 & 2 & 1\\
0 & 0 & 2\\
\end{bmatrix}. AB = BA e^{A+B} = e^A e^B \begin{bmatrix}
2 & 1 & 1\\
0 & 2 & 1\\
0 & 0 & 2\\
\end{bmatrix} 
=\begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1\\
\end{bmatrix} 
+\begin{bmatrix}
1 & 1 & 1\\
0 & 1 & 1\\
0 & 0 & 1\\
\end{bmatrix} \begin{bmatrix}
1 & 1 & 1\\
0 & 1 & 1\\
0 & 0 & 1\\
\end{bmatrix}.","['matrices', 'matrix-exponential']"
69,Quick method for finding eigenvalues and eigenvectors in a symmetric $5 \times 5$ matrix?,Quick method for finding eigenvalues and eigenvectors in a symmetric  matrix?,5 \times 5,"The matrix $B$: $B =  \pmatrix{ 0 &  0 &  0 &  0 &  0 \cr  0 &  8 &  0 & -8 &  0 \cr  0 &  0 &  8 &  0 & -8 \cr  0 & -8 &  0 &  8 &  0 \cr  0 &  0 & -8 &  0 &  8 \cr  }$ Which has nonzero eigenvalues $\lambda_1=16$ and $\lambda_2=16$ and corresponding eigenvectors: v$_1 =  \pmatrix{ 0\cr  \frac{1}{2} \sqrt2 \cr  0 \cr  -\frac{1}{2} \sqrt2 \cr  0\cr  }$  and  v$_2 = \pmatrix{ 0\cr  0\cr  \frac{1}{2} \sqrt2 \cr  0 \cr  -\frac{1}{2} \sqrt2 \cr  }$ What is the method for obtaining these eigenvalues and corresponding eigenvectors? It's a large matrix and I'm hoping there's some kind of easy trick to it. From what I can remember of eigen decomposition, normally I'd do: $Ax = \lambda x \implies|A-\lambda I|x = 0$ $\implies  \det \pmatrix{ 0-\lambda &  0 &  0 &  0 &  0 \cr  0 &  8-\lambda &  0 & -8 &  0 \cr  0 &  0 &  8-\lambda &  0 & -8 \cr  0 & -8 &  0 &  8-\lambda &  0 \cr  0 &  0 & -8 &  0 &  8-\lambda \cr  }$ $\pmatrix{ x_1 \cr  x_2 \cr  x_3 \cr  x_4 \cr  x_5 \cr  }$ = $\pmatrix{ 0 \cr  0 \cr  0 \cr  0 \cr  0 \cr  }$ So the determinant is $\implies -\lambda \det  \pmatrix{  8-\lambda &  0 & -8 &  0 \cr  0 &  8-\lambda &  0 & -8 \cr  -8 &  0 &  8-\lambda &  0 \cr  0 & -8 &  0 &  8-\lambda \cr  }$ $\implies -\lambda * [ (8- \lambda)\det  \pmatrix{  8-\lambda &  0 & -8 \cr  0 &  8-\lambda &  0 \cr  -8 &  0 &  8-\lambda \cr  }-8 \det \pmatrix{  0 &  8-\lambda & -8 \cr  -8 &  0 &  0 \cr  0 & -8 &  8-\lambda \cr  }]$ etc. There's got to be an easier way?","The matrix $B$: $B =  \pmatrix{ 0 &  0 &  0 &  0 &  0 \cr  0 &  8 &  0 & -8 &  0 \cr  0 &  0 &  8 &  0 & -8 \cr  0 & -8 &  0 &  8 &  0 \cr  0 &  0 & -8 &  0 &  8 \cr  }$ Which has nonzero eigenvalues $\lambda_1=16$ and $\lambda_2=16$ and corresponding eigenvectors: v$_1 =  \pmatrix{ 0\cr  \frac{1}{2} \sqrt2 \cr  0 \cr  -\frac{1}{2} \sqrt2 \cr  0\cr  }$  and  v$_2 = \pmatrix{ 0\cr  0\cr  \frac{1}{2} \sqrt2 \cr  0 \cr  -\frac{1}{2} \sqrt2 \cr  }$ What is the method for obtaining these eigenvalues and corresponding eigenvectors? It's a large matrix and I'm hoping there's some kind of easy trick to it. From what I can remember of eigen decomposition, normally I'd do: $Ax = \lambda x \implies|A-\lambda I|x = 0$ $\implies  \det \pmatrix{ 0-\lambda &  0 &  0 &  0 &  0 \cr  0 &  8-\lambda &  0 & -8 &  0 \cr  0 &  0 &  8-\lambda &  0 & -8 \cr  0 & -8 &  0 &  8-\lambda &  0 \cr  0 &  0 & -8 &  0 &  8-\lambda \cr  }$ $\pmatrix{ x_1 \cr  x_2 \cr  x_3 \cr  x_4 \cr  x_5 \cr  }$ = $\pmatrix{ 0 \cr  0 \cr  0 \cr  0 \cr  0 \cr  }$ So the determinant is $\implies -\lambda \det  \pmatrix{  8-\lambda &  0 & -8 &  0 \cr  0 &  8-\lambda &  0 & -8 \cr  -8 &  0 &  8-\lambda &  0 \cr  0 & -8 &  0 &  8-\lambda \cr  }$ $\implies -\lambda * [ (8- \lambda)\det  \pmatrix{  8-\lambda &  0 & -8 \cr  0 &  8-\lambda &  0 \cr  -8 &  0 &  8-\lambda \cr  }-8 \det \pmatrix{  0 &  8-\lambda & -8 \cr  -8 &  0 &  0 \cr  0 & -8 &  8-\lambda \cr  }]$ etc. There's got to be an easier way?",,"['matrices', 'eigenvalues-eigenvectors']"
70,Why are these determinants $0$?,Why are these determinants ?,0,"These $3$ matrices below have determinants of $0$. Increasing each element by $1$ still results in a determinant of $0$: \begin{bmatrix}1&2&3\\4&5&6\\7&8&9\end{bmatrix} \begin{bmatrix}2&3&4\\5&6&7\\8&9&10\end{bmatrix} \begin{bmatrix}3&4&5\\6&7&8\\9&10&11\end{bmatrix} Also, if I square each element of the first matrix the determinant isn't $0$, it's $-216$, but if I continue this pattern to a $4 \times 4$ matrix the determinant then is $0$: \begin{bmatrix}1&4&9\\16&25&36\\49&64&81\end{bmatrix} \begin{bmatrix}1&4&9&16\\25&36&49&64\\81&100&121&144\\169&196&225&256\end{bmatrix} This pattern seems to work for matrices with dimensions $2$ higher than what the elements are raised to: \begin{bmatrix}1&8&27&64&125\\216&343&512&729&1000\\1331&1728&2197&2744&3375\\4096&4913&5832&6859&8000\\9261&10648&12167&13824&15625\end{bmatrix} \begin{bmatrix}1&16&81&256&625&1296\\2401&4096&6561&10000&14641&20736\\28561&38416&50625&65536&83521&104976\\130321&160000&194481&234256&279841&331776\\390625&456976&531441&614656&707281&810000\\923521&1048576&1185921&1336336&1500625&1679616\end{bmatrix} Can someone please explain why this is?","These $3$ matrices below have determinants of $0$. Increasing each element by $1$ still results in a determinant of $0$: \begin{bmatrix}1&2&3\\4&5&6\\7&8&9\end{bmatrix} \begin{bmatrix}2&3&4\\5&6&7\\8&9&10\end{bmatrix} \begin{bmatrix}3&4&5\\6&7&8\\9&10&11\end{bmatrix} Also, if I square each element of the first matrix the determinant isn't $0$, it's $-216$, but if I continue this pattern to a $4 \times 4$ matrix the determinant then is $0$: \begin{bmatrix}1&4&9\\16&25&36\\49&64&81\end{bmatrix} \begin{bmatrix}1&4&9&16\\25&36&49&64\\81&100&121&144\\169&196&225&256\end{bmatrix} This pattern seems to work for matrices with dimensions $2$ higher than what the elements are raised to: \begin{bmatrix}1&8&27&64&125\\216&343&512&729&1000\\1331&1728&2197&2744&3375\\4096&4913&5832&6859&8000\\9261&10648&12167&13824&15625\end{bmatrix} \begin{bmatrix}1&16&81&256&625&1296\\2401&4096&6561&10000&14641&20736\\28561&38416&50625&65536&83521&104976\\130321&160000&194481&234256&279841&331776\\390625&456976&531441&614656&707281&810000\\923521&1048576&1185921&1336336&1500625&1679616\end{bmatrix} Can someone please explain why this is?",,"['matrices', 'determinant']"
71,Ensuring that a symmetric matrix with nonnegative elements is positive semidefinite,Ensuring that a symmetric matrix with nonnegative elements is positive semidefinite,,"I have the following matrix $A$: symmetric all positive and/or zero values the main diagonal is all the same value, $x$. To ensure that the matrix $A$, is positive semidefinite, must I only ensure that $x \geq 0$?  It seems correct from my thinking, but wanted to make sure. Thanks.","I have the following matrix $A$: symmetric all positive and/or zero values the main diagonal is all the same value, $x$. To ensure that the matrix $A$, is positive semidefinite, must I only ensure that $x \geq 0$?  It seems correct from my thinking, but wanted to make sure. Thanks.",,"['matrices', 'positive-semidefinite']"
72,Block matrix determinant formula [closed],Block matrix determinant formula [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question I have encountered an statement several times while proving determinant of a block matrix. $$\det\pmatrix{A&0\\0&D}\; = \det(A)\det(D)$$ where $A$ is $k\times k$ and $D$ is $n\times n$ matrix. How to prove this? Thanks in advance.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question I have encountered an statement several times while proving determinant of a block matrix. $$\det\pmatrix{A&0\\0&D}\; = \det(A)\det(D)$$ where $A$ is $k\times k$ and $D$ is $n\times n$ matrix. How to prove this? Thanks in advance.",,"['matrices', 'determinant', 'block-matrices']"
73,How to prove that 2-norm of matrix A is <= infinite norm of matrix A,How to prove that 2-norm of matrix A is <= infinite norm of matrix A,,"Now a bit of a disclaimer, its been two years since I last took a math class, so I have little to no memory of how to construct or go about formulating proofs. My tutor unfortunately is very and excruciatingly slow at figuring them out and this one the tutor cannot seem to solve. What I was taught is to start with ""What do we want?"" and ""What do we know?"" What we want: $||A||_2 \leq \sqrt{n}||A||_\infty$ What we know: $||A||_2^2$ = $\lambda _\max(A^tA)$ $\sqrt{n}||A||_\infty = \sqrt{n} * max_i | \Sigma a_{ij} y_j |$ Where j= $1 \dots n$ . Otherwise the maximum absolute row sum. Thus: $\sqrt{n}^2||A||_\infty^2 = n * max_i | \Sigma a_{ij} y_j |^2$ Where j= $1 \dots n$ . Or max row sum squared times n. I feel like that the square of the max times n is likely going to be bigger than the largest positive eigenvalue of A, but I have no means of showing/knowing this and nothing in the notes indicates how to find the bounds other than maybe Banach-Lemma but I think that's only if A is invertible no? And I don't think that's given. Was a homework question, but I'm way past due anyways and just working on it to understand it/prepare for the first mid term.","Now a bit of a disclaimer, its been two years since I last took a math class, so I have little to no memory of how to construct or go about formulating proofs. My tutor unfortunately is very and excruciatingly slow at figuring them out and this one the tutor cannot seem to solve. What I was taught is to start with ""What do we want?"" and ""What do we know?"" What we want: What we know: = Where j= . Otherwise the maximum absolute row sum. Thus: Where j= . Or max row sum squared times n. I feel like that the square of the max times n is likely going to be bigger than the largest positive eigenvalue of A, but I have no means of showing/knowing this and nothing in the notes indicates how to find the bounds other than maybe Banach-Lemma but I think that's only if A is invertible no? And I don't think that's given. Was a homework question, but I'm way past due anyways and just working on it to understand it/prepare for the first mid term.",||A||_2 \leq \sqrt{n}||A||_\infty ||A||_2^2 \lambda _\max(A^tA) \sqrt{n}||A||_\infty = \sqrt{n} * max_i | \Sigma a_{ij} y_j | 1 \dots n \sqrt{n}^2||A||_\infty^2 = n * max_i | \Sigma a_{ij} y_j |^2 1 \dots n,"['matrices', 'numerical-methods', 'normed-spaces']"
74,How to inverse a block diagonal matrix?,How to inverse a block diagonal matrix?,,"Given a matrix $$x = \begin{bmatrix}  40 & 0 & 0 & 0\\ 0 & 80 & 100 & 0 \\ 0 & 40 & 120 & 0 \\ 0 & 0 & 0 & 60\end{bmatrix}$$ How to find the inverse of that matrix? What I know: $\det(x) = ac-bd$ ,  inverse of a 2x2 matrix: $$x^{-1} = \frac{1}{\det(x)}\cdot \begin{bmatrix} d &-b\\ -c &a\end{bmatrix}.$$ There is a lot of content online; however none of them has a specific numerical example.","Given a matrix How to find the inverse of that matrix? What I know: ,  inverse of a 2x2 matrix: There is a lot of content online; however none of them has a specific numerical example.","x = \begin{bmatrix} 
40 & 0 & 0 & 0\\
0 & 80 & 100 & 0 \\
0 & 40 & 120 & 0 \\
0 & 0 & 0 & 60\end{bmatrix} \det(x) = ac-bd x^{-1} = \frac{1}{\det(x)}\cdot \begin{bmatrix} d &-b\\ -c &a\end{bmatrix}.","['matrices', 'numerical-linear-algebra', 'matrix-decomposition']"
75,Finding the matrix for a matrix exponential,Finding the matrix for a matrix exponential,,"I'm trying to find the matrix $A$ for which $$e^{tA}=\begin{pmatrix} \frac{1}{2}(e^t+e^{-t}) & 0 & \frac{1}{2}(e^t-e^{-t}) \\ 0 & e^t & 0 \\ \frac{1}{2}(e^t-e^{-t}) & 0 & \frac{1}{2}(e^t+e^{-t}) \end{pmatrix}$$ I know that $e^{tA}=\Psi(t)\cdot[\Psi(0)]^{-1}$, so $e^{tA}\cdot\Psi(0)=\Psi(t)$. Where $\Psi(t)=(\eta^{(1)}e^{\lambda_1x},\eta^{(2)}e^{\lambda_2x},\eta^{(3)}e^{\lambda_3x})$, with $\lambda_i$ the $i$-th eigenvalue with corresponding eigenvector $\eta^{(i)}$. However, this didn't really get me anywhere. Does anyone know how to do this?","I'm trying to find the matrix $A$ for which $$e^{tA}=\begin{pmatrix} \frac{1}{2}(e^t+e^{-t}) & 0 & \frac{1}{2}(e^t-e^{-t}) \\ 0 & e^t & 0 \\ \frac{1}{2}(e^t-e^{-t}) & 0 & \frac{1}{2}(e^t+e^{-t}) \end{pmatrix}$$ I know that $e^{tA}=\Psi(t)\cdot[\Psi(0)]^{-1}$, so $e^{tA}\cdot\Psi(0)=\Psi(t)$. Where $\Psi(t)=(\eta^{(1)}e^{\lambda_1x},\eta^{(2)}e^{\lambda_2x},\eta^{(3)}e^{\lambda_3x})$, with $\lambda_i$ the $i$-th eigenvalue with corresponding eigenvector $\eta^{(i)}$. However, this didn't really get me anywhere. Does anyone know how to do this?",,"['matrices', 'ordinary-differential-equations', 'matrix-exponential']"
76,Are all Nonograms / griddlers uniquely solvable?,Are all Nonograms / griddlers uniquely solvable?,,"A Nonogram (also called a Griddler) is a popular puzzle in which you are given a matrix of size $n \times m$ filled with zeros, and you are also given that in row $i$ there are $a_i$ ones and in column $j$ there are $b_j$ ones. This is explained more throughly here https://en.wikipedia.org/wiki/Nonogram It is a fun, popular, and rather simple to understand puzzle. My question is a simple one - Are all (legit) Griddlers uniquely solvable? Do all such matrices correspond to one picture? or is it possible to construct a grid that corresponds to more than one picture? By legit grids, I mean that it is possible to find at least one solution. It is a proper grid. There is no situation for example where we have a matrix with $10$ and $10$ columns but we are given that in the first row there are $11$ ones. Also, are there Grids where a solution exists but it is impossible to find it? Edit: If a solution exists, it is always possible to find it. The reason is that in a $n$ by $m$ grid there are $2^{nm}$ possible states. At least one of them will be a solution if a solution exists.","A Nonogram (also called a Griddler) is a popular puzzle in which you are given a matrix of size $n \times m$ filled with zeros, and you are also given that in row $i$ there are $a_i$ ones and in column $j$ there are $b_j$ ones. This is explained more throughly here https://en.wikipedia.org/wiki/Nonogram It is a fun, popular, and rather simple to understand puzzle. My question is a simple one - Are all (legit) Griddlers uniquely solvable? Do all such matrices correspond to one picture? or is it possible to construct a grid that corresponds to more than one picture? By legit grids, I mean that it is possible to find at least one solution. It is a proper grid. There is no situation for example where we have a matrix with $10$ and $10$ columns but we are given that in the first row there are $11$ ones. Also, are there Grids where a solution exists but it is impossible to find it? Edit: If a solution exists, it is always possible to find it. The reason is that in a $n$ by $m$ grid there are $2^{nm}$ possible states. At least one of them will be a solution if a solution exists.",,"['matrices', 'discrete-mathematics', 'puzzle']"
77,"$M,N\in \Bbb R ^{n\times n}$, show that $e^{(M+N)} = e^{M}e^N$ given $MN=NM$",", show that  given","M,N\in \Bbb R ^{n\times n} e^{(M+N)} = e^{M}e^N MN=NM","I am working on the following problem. Let $e^{Mt} = \sum\limits_{k=0}^{\infty} \frac{M^k t^k}{k!}$ where $M$ is an $n\times n$ matrix. Now prove that $$e^{(M+N)}   = e^{M}e^N$$ given that $MN=NM$, ie $M$ and $N$ commute. Now the left hand side of the desired equality is $$e^{(M+N)} = I+ (M+N) + \frac{(M+N)^2}{2!} + \frac{(M+N)^3}{3!} + \ldots $$  On the right hand side of the equation we have $$e^Me^N = \left(I + M + \frac{M^2}{2!}   + \frac{M^3}{3!}\ldots\right)      \left(I + N + \frac{N^2}{2!}   +   \frac{N^3}{3!} \ldots\right)   $$ Now basically this is as far as I got... I am unsure on how to work out the product of the two infinite sums. Possibly I need to expand the powers on the left hand side expression but I am unsure how to do this in an infinite sum... If anyone could give me an answer or a hint that can help me forward I would greatly appreciate it. Thanks","I am working on the following problem. Let $e^{Mt} = \sum\limits_{k=0}^{\infty} \frac{M^k t^k}{k!}$ where $M$ is an $n\times n$ matrix. Now prove that $$e^{(M+N)}   = e^{M}e^N$$ given that $MN=NM$, ie $M$ and $N$ commute. Now the left hand side of the desired equality is $$e^{(M+N)} = I+ (M+N) + \frac{(M+N)^2}{2!} + \frac{(M+N)^3}{3!} + \ldots $$  On the right hand side of the equation we have $$e^Me^N = \left(I + M + \frac{M^2}{2!}   + \frac{M^3}{3!}\ldots\right)      \left(I + N + \frac{N^2}{2!}   +   \frac{N^3}{3!} \ldots\right)   $$ Now basically this is as far as I got... I am unsure on how to work out the product of the two infinite sums. Possibly I need to expand the powers on the left hand side expression but I am unsure how to do this in an infinite sum... If anyone could give me an answer or a hint that can help me forward I would greatly appreciate it. Thanks",,['matrices']
78,Show that the identity matrix $I$ must have norm $1$.,Show that the identity matrix  must have norm .,I 1,"I am trying to understand why the identity matrix $I$ must have a norm $1$, for any choice of matrix-norm $|\cdot|$?  How would i show this?","I am trying to understand why the identity matrix $I$ must have a norm $1$, for any choice of matrix-norm $|\cdot|$?  How would i show this?",,['matrices']
79,Can I use algebraic identities with matrices?,Can I use algebraic identities with matrices?,,"Let $A$ and $B$ be square matrices, so that it is possible to add or multiply them with themselves and with each other. Can algebraic identities such as $(A+B)^2=A^2+2AB+B^2$ apply to them?","Let and be square matrices, so that it is possible to add or multiply them with themselves and with each other. Can algebraic identities such as apply to them?",A B (A+B)^2=A^2+2AB+B^2,['matrices']
80,100 row numbers to find the shortest path,100 row numbers to find the shortest path,,"I got a question that I have 100 rows of the number, as in the picture that continuous to 100 rows. There is a sequence by starting from the top, and then for each integer walk to the left or right value in the row beneath. That is if we start from the top, then 40 can only be followed by 95 or 55, 95 can only be followed by 72 or 86 and so on. And I need to find the shortest path from the top to the bottom(from the first row to 100 rows). I am thinking of plotting a graph from number 1 to 5050(cause there are in total 5050 numbers.) But how can I put weight on it later on? If I calculate weights one by one that will take ages... Is there an easier way to figure this out? This is the picture for the first nine rows: Thank you very much.","I got a question that I have 100 rows of the number, as in the picture that continuous to 100 rows. There is a sequence by starting from the top, and then for each integer walk to the left or right value in the row beneath. That is if we start from the top, then 40 can only be followed by 95 or 55, 95 can only be followed by 72 or 86 and so on. And I need to find the shortest path from the top to the bottom(from the first row to 100 rows). I am thinking of plotting a graph from number 1 to 5050(cause there are in total 5050 numbers.) But how can I put weight on it later on? If I calculate weights one by one that will take ages... Is there an easier way to figure this out? This is the picture for the first nine rows: Thank you very much.",,['graphing-functions']
81,Transvection matrices generate $ \operatorname{SL}_n(\mathbb{R}) $,Transvection matrices generate, \operatorname{SL}_n(\mathbb{R}) ,"I need to prove that the transvection matrices generate the special linear group $\operatorname{SL}_n \left(\mathbb{R}\right) $ . I want to proceed using induction on $n$ . I was able to prove the $2\times 2$ case, but I am having difficulty with the $n+1$ case. I supposed that the elementary matrices of the first type generate $\operatorname{SL}_n(\mathbb{R})$ . And I want to show that an elementary matrix of the first type of order $n+1$ can generate $\operatorname{SL}_{n+1}(\mathbb{R})$","I need to prove that the transvection matrices generate the special linear group . I want to proceed using induction on . I was able to prove the case, but I am having difficulty with the case. I supposed that the elementary matrices of the first type generate . And I want to show that an elementary matrix of the first type of order can generate",\operatorname{SL}_n \left(\mathbb{R}\right)  n 2\times 2 n+1 \operatorname{SL}_n(\mathbb{R}) n+1 \operatorname{SL}_{n+1}(\mathbb{R}),['matrices']
82,intuition behind cyclicality of the trace?,intuition behind cyclicality of the trace?,,"The trace of a matrix product is ""commutative"" (i.e. cyclical): $$\operatorname{trace}(AB)=\operatorname{trace}(BA)$$ I know how to prove this. I was wondering, are there any intuitive ways to understand why this must be true? One reason why I don't see it intuitively is that I don't really understand intuitively what the trace represents (e.g. geometrically).","The trace of a matrix product is ""commutative"" (i.e. cyclical): $$\operatorname{trace}(AB)=\operatorname{trace}(BA)$$ I know how to prove this. I was wondering, are there any intuitive ways to understand why this must be true? One reason why I don't see it intuitively is that I don't really understand intuitively what the trace represents (e.g. geometrically).",,"['matrices', 'intuition']"
83,"why do people say ""x dimensional vector"" when vectors have only one dimension?","why do people say ""x dimensional vector"" when vectors have only one dimension?",,"I am new to linear algebra and have been confused by the terminology ""n dimensional vector"" that my online course instructor uses. He refers to vectors as an ""n dimensional vector"" where n is the number of elements in the vector. For example he might say that this is a 5-dimensional vector: [10 15 20 25 30] However by definition a vector is 1D. I guess this is an etymology question, but why would people use such confusing terminology? If somebody said something was a ""5 dimensional matrix"" I assume nobody would think that means it has 5 elements, rather they would think it has 5 dimensions, so why do they talk about vectors differently?","I am new to linear algebra and have been confused by the terminology ""n dimensional vector"" that my online course instructor uses. He refers to vectors as an ""n dimensional vector"" where n is the number of elements in the vector. For example he might say that this is a 5-dimensional vector: [10 15 20 25 30] However by definition a vector is 1D. I guess this is an etymology question, but why would people use such confusing terminology? If somebody said something was a ""5 dimensional matrix"" I assume nobody would think that means it has 5 elements, rather they would think it has 5 dimensions, so why do they talk about vectors differently?",,"['matrices', 'vectors']"
84,Matrix notation in handwriting,Matrix notation in handwriting,,"I understand that typically matrices are printed in bold to distinguish them from other mathematical entities with the same symbols. However I find it difficult to actually handwrite in bold. With what symbol  is it appropriate to distinguish a matrix from an operator, eigenvalue etc. I have read this almost identical question ( How to do \mathbf in handwriting? ) but it doesn't really answer the problem because in the area of physics I am studying the matrices are always indicated by bold, lower case greeks.","I understand that typically matrices are printed in bold to distinguish them from other mathematical entities with the same symbols. However I find it difficult to actually handwrite in bold. With what symbol  is it appropriate to distinguish a matrix from an operator, eigenvalue etc. I have read this almost identical question ( How to do \mathbf in handwriting? ) but it doesn't really answer the problem because in the area of physics I am studying the matrices are always indicated by bold, lower case greeks.",,"['matrices', 'notation']"
85,Finding matrix exponential,Finding matrix exponential,,"I am trying to compute the matrix exponential for  $$A=\left( \begin{array}{ccc} 1 & 2 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & -1 & 0 \\ 0 & 0 & 1 & -1 \end{array} \right) $$ But I am stuck. There are a couple of methods I know of, but none of them seem to be working. First, I tried to see if the matrix was nilpotent. It is not. I then tried to split the matrix into the identity and hoped the remaining matrix was nilpotent. It is not. Next, I tried to do it by solving for the fundamental matrix, but the characteristic polynomial is $(x^2-1)^2$, which implies there are two eigenvalues $1$ and $-1$ with multiplicity $2$ each. I was unable to find the corresponding eigenvectors since $(A-I)^2 \neq 0$ and $(A+I)^2 \neq 0$. What am I missing?","I am trying to compute the matrix exponential for  $$A=\left( \begin{array}{ccc} 1 & 2 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & -1 & 0 \\ 0 & 0 & 1 & -1 \end{array} \right) $$ But I am stuck. There are a couple of methods I know of, but none of them seem to be working. First, I tried to see if the matrix was nilpotent. It is not. I then tried to split the matrix into the identity and hoped the remaining matrix was nilpotent. It is not. Next, I tried to do it by solving for the fundamental matrix, but the characteristic polynomial is $(x^2-1)^2$, which implies there are two eigenvalues $1$ and $-1$ with multiplicity $2$ each. I was unable to find the corresponding eigenvectors since $(A-I)^2 \neq 0$ and $(A+I)^2 \neq 0$. What am I missing?",,"['matrices', 'ordinary-differential-equations']"
86,How to prove that an M-matrix is inverse-non-negative?,How to prove that an M-matrix is inverse-non-negative?,,"Wikipedia says that The inverse of any non-singular M-matrix is a non-negative matrix."" To be more precise, if $A$ is an M-matrix , then the entries of the inverse of $A$ are all non-negative, i.e. $A^{-1} \geq 0$. How do I prove this result?","Wikipedia says that The inverse of any non-singular M-matrix is a non-negative matrix."" To be more precise, if $A$ is an M-matrix , then the entries of the inverse of $A$ are all non-negative, i.e. $A^{-1} \geq 0$. How do I prove this result?",,['matrices']
87,Can you raise a Matrix to a non integer number? [duplicate],Can you raise a Matrix to a non integer number? [duplicate],,"This question already has answers here : Arbitrary non-integer power of a matrix (3 answers) Closed 11 years ago . So I heard you can take a matrix A to the power 2, take it to a -3th power and multiply it by an irrational number. You can also do some other non-intuitive things like taking e to the power of a matrix. Does there exist some definition to take a nxn matrix to a power like 2.3 or pi?","This question already has answers here : Arbitrary non-integer power of a matrix (3 answers) Closed 11 years ago . So I heard you can take a matrix A to the power 2, take it to a -3th power and multiply it by an irrational number. You can also do some other non-intuitive things like taking e to the power of a matrix. Does there exist some definition to take a nxn matrix to a power like 2.3 or pi?",,"['matrices', 'irrational-numbers', 'exponentiation']"
88,Why does Strassen's algorithm work for $2\times 2$ matrices only when the number of multiplications is $7$?,Why does Strassen's algorithm work for  matrices only when the number of multiplications is ?,2\times 2 7,"I have been reading Introduction to Algorithms by Cormen. Before explaining Strassen algorithm the book says this: Strassen’s algorithm is not at all obvious. (This might be the biggest understatement in this book.) The book just states the algorithm but don't explain why it works. So if we take the case of multiplication of two $2 \times 2$ matrices, it reduces the number of multiplications from $8$ to $7$ thereby reducing the complexity from  $n^{3}$ to  $n^{\lg 7}$ which is roughly $n^{2.8}$. My question is why $7$ and say not $5$ or even lesser number? I mean they could have made the recursion tree even less ""bushier"".","I have been reading Introduction to Algorithms by Cormen. Before explaining Strassen algorithm the book says this: Strassen’s algorithm is not at all obvious. (This might be the biggest understatement in this book.) The book just states the algorithm but don't explain why it works. So if we take the case of multiplication of two $2 \times 2$ matrices, it reduces the number of multiplications from $8$ to $7$ thereby reducing the complexity from  $n^{3}$ to  $n^{\lg 7}$ which is roughly $n^{2.8}$. My question is why $7$ and say not $5$ or even lesser number? I mean they could have made the recursion tree even less ""bushier"".",,"['matrices', 'algorithms', 'computational-complexity', 'recursive-algorithms']"
89,Why should I avoid the Frobenius Norm?,Why should I avoid the Frobenius Norm?,,"I vaguely remember the Frobenius matrix norm  ( ${||A||}_F = \sqrt{\sum_{i,j} a_{i,j}^2}$ ) was somehow considered unsuitable for numerical analysis applications. I only remember, however, that it was not a subordinate matrix norm , but only because it did not take the identity matrix to $1$. It seems this latter problem could be solved with a rescaling, though. I don't remember my numerical analysis text considering this norm any further after introducing this fact, which seemed to be its death-knell for some reason. The question, then: for fixed $n$, when looking at $n \times n$ matrices, are there any weird gotchas, deficiencies, oddities, etc , when using the (possibly rescaled) Frobenius norm? For example, is there some weird series of matrices $A_i$ such that the Frobenius norm of the $A_i$ approaches zero while the $\ell_2$-subordinate norm does not converge to zero? (It seems like that can not happen because the $\ell_2$ norm is the square root of the largest eigenvalue of $A^*A$, and thus bounded from above by the Frobenius norm...)","I vaguely remember the Frobenius matrix norm  ( ${||A||}_F = \sqrt{\sum_{i,j} a_{i,j}^2}$ ) was somehow considered unsuitable for numerical analysis applications. I only remember, however, that it was not a subordinate matrix norm , but only because it did not take the identity matrix to $1$. It seems this latter problem could be solved with a rescaling, though. I don't remember my numerical analysis text considering this norm any further after introducing this fact, which seemed to be its death-knell for some reason. The question, then: for fixed $n$, when looking at $n \times n$ matrices, are there any weird gotchas, deficiencies, oddities, etc , when using the (possibly rescaled) Frobenius norm? For example, is there some weird series of matrices $A_i$ such that the Frobenius norm of the $A_i$ approaches zero while the $\ell_2$-subordinate norm does not converge to zero? (It seems like that can not happen because the $\ell_2$ norm is the square root of the largest eigenvalue of $A^*A$, and thus bounded from above by the Frobenius norm...)",,"['matrices', 'numerical-methods', 'normed-spaces', 'examples-counterexamples', 'matrix-norms']"
90,determinant of permutation matrix,determinant of permutation matrix,,"It's a well known fact that $\det(P)=(-1)^t$, where $t$ is the number of row exchanges in the $PA=LU$ decomposition. Can somebody point me to a (semi) formal proof as why it is so?","It's a well known fact that $\det(P)=(-1)^t$, where $t$ is the number of row exchanges in the $PA=LU$ decomposition. Can somebody point me to a (semi) formal proof as why it is so?",,"['matrices', 'determinant']"
91,How many orthogonal matrices are there,How many orthogonal matrices are there,,"this might sound like a stupid question, but what I mean is: You need $n \times n$ elements to define a square matrix $\in R^{n \times n}$. How many element do I need to define an orthogonal matrix? I have the feeling that it should be $n$ or $2n$ but cannot find a clean mathematical formulation for that. Thanks!","this might sound like a stupid question, but what I mean is: You need $n \times n$ elements to define a square matrix $\in R^{n \times n}$. How many element do I need to define an orthogonal matrix? I have the feeling that it should be $n$ or $2n$ but cannot find a clean mathematical formulation for that. Thanks!",,"['matrices', 'orthogonality', 'orthonormal']"
92,Proof of existence of square root of unitary and symmetric matrix,Proof of existence of square root of unitary and symmetric matrix,,"I'm struggling with this exercise Let $U$ be a unitary and symmetric matrix ($U^T = U$ and $U^*U = I$). Prove that there exists a complex matrix $S$ such that: $S^2 = U$ $S$ is a unitary matrix $S$ is symmetric Each matrix that commutes with $U$, commutes with $S$","I'm struggling with this exercise Let $U$ be a unitary and symmetric matrix ($U^T = U$ and $U^*U = I$). Prove that there exists a complex matrix $S$ such that: $S^2 = U$ $S$ is a unitary matrix $S$ is symmetric Each matrix that commutes with $U$, commutes with $S$",,['matrices']
93,Affine Transformations isomorphic to Heisenberg group,Affine Transformations isomorphic to Heisenberg group,,"I want to show that the lie group $G$ of affine transformations of the form $$ \begin{bmatrix} 1 & c & -\frac{c^2}{2} \\ 0 & 1 & -c \\ 0 & 0 & 1 \end{bmatrix} + \begin{bmatrix} a \\ b \\ c \end{bmatrix} $$ for $a,b,c\in\mathbb{R}$ is isomorphic to the Heisenberg group given by matrices of the form $$ \begin{bmatrix} 1 & x & z \\ 0 & 1 & y \\ 0 & 0 & 1 \end{bmatrix}$$ for $x,y,z\in\mathbb{R}$. My idea was to identify both groups with $\mathbb{R}^3$ and then I hoped to see that the induced group multiplications on $\mathbb{R}^3$ are the same for both groups. But this is not the case (at least the way I choosed my maps). If I identify an element of the Heisenberg group with a vector $(x,y,z)$ then the induced group multiplication is $(x,y,z)\cdot (x',y',z')=(x+x',y+y',z+z'+xy')$. But when I identify an element of $G$ with a vector $(a,b,c)$, then the induced multiplication is $(a,b,c)\cdot (a',b',c')=(a+a'+b'c+c'\frac{c^2}{2},b+b'-cc',c+c')$ (assuming I have no mistake in my computation). So this does not work. Is there a better way to see that those groups are isomorphic?","I want to show that the lie group $G$ of affine transformations of the form $$ \begin{bmatrix} 1 & c & -\frac{c^2}{2} \\ 0 & 1 & -c \\ 0 & 0 & 1 \end{bmatrix} + \begin{bmatrix} a \\ b \\ c \end{bmatrix} $$ for $a,b,c\in\mathbb{R}$ is isomorphic to the Heisenberg group given by matrices of the form $$ \begin{bmatrix} 1 & x & z \\ 0 & 1 & y \\ 0 & 0 & 1 \end{bmatrix}$$ for $x,y,z\in\mathbb{R}$. My idea was to identify both groups with $\mathbb{R}^3$ and then I hoped to see that the induced group multiplications on $\mathbb{R}^3$ are the same for both groups. But this is not the case (at least the way I choosed my maps). If I identify an element of the Heisenberg group with a vector $(x,y,z)$ then the induced group multiplication is $(x,y,z)\cdot (x',y',z')=(x+x',y+y',z+z'+xy')$. But when I identify an element of $G$ with a vector $(a,b,c)$, then the induced multiplication is $(a,b,c)\cdot (a',b',c')=(a+a'+b'c+c'\frac{c^2}{2},b+b'-cc',c+c')$ (assuming I have no mistake in my computation). So this does not work. Is there a better way to see that those groups are isomorphic?",,"['matrices', 'group-theory', 'manifolds', 'lie-groups', 'group-isomorphism']"
94,Notation for matrix and sum of matrix rows,Notation for matrix and sum of matrix rows,,"I have a table that describes the influence of sources (columns) on sinks (rows) where rows=$(A,B,C)$ and columns=$(A,B,C,D,E)$. So my table looks like: | A | B | C | A | 1 | 2 | 4 | B | 2 | 1 | 1 | C | 2 | 3 | 1 | D | 1 | 4 | 2 | E | 1 | 1 | 1 | If I would like to describe that table in matrix terminology how would I do that? And how are the columns and rows called? What is the correct notation if I want to describe the sum of rows (like the first row $A$ which is $7$). I'd also like to find a way to describe (sum of row)-(autoinfluence where i=j, main diagonal) e.g. row $A$ - (cell $AA$). As a very unprofessional start: $$\text{Matrix: } X = (x_{ij})$$ $$\text{where } i = (A,B,C,D,E) \text{ and } j=(A,B,C)$$ $$\text{row sum: } R = \sum_{i=1}^{N}(x_{ij}) = (1,1,...,1)X$$ $$\text{my value: } V = (1,1,...,1)X - X \text{ (where } i=j) = (1,1,...,1)X \text{ (where } i\neq j) $$ I need to describe that table and these desired values in professional looking way in 'ij' or matrix terminology for a scientific paper.","I have a table that describes the influence of sources (columns) on sinks (rows) where rows=$(A,B,C)$ and columns=$(A,B,C,D,E)$. So my table looks like: | A | B | C | A | 1 | 2 | 4 | B | 2 | 1 | 1 | C | 2 | 3 | 1 | D | 1 | 4 | 2 | E | 1 | 1 | 1 | If I would like to describe that table in matrix terminology how would I do that? And how are the columns and rows called? What is the correct notation if I want to describe the sum of rows (like the first row $A$ which is $7$). I'd also like to find a way to describe (sum of row)-(autoinfluence where i=j, main diagonal) e.g. row $A$ - (cell $AA$). As a very unprofessional start: $$\text{Matrix: } X = (x_{ij})$$ $$\text{where } i = (A,B,C,D,E) \text{ and } j=(A,B,C)$$ $$\text{row sum: } R = \sum_{i=1}^{N}(x_{ij}) = (1,1,...,1)X$$ $$\text{my value: } V = (1,1,...,1)X - X \text{ (where } i=j) = (1,1,...,1)X \text{ (where } i\neq j) $$ I need to describe that table and these desired values in professional looking way in 'ij' or matrix terminology for a scientific paper.",,"['matrices', 'notation']"
95,Is there a faster way to calculate a few diagonal elements of the inverse of a huge symmetric positive definite matrix?,Is there a faster way to calculate a few diagonal elements of the inverse of a huge symmetric positive definite matrix?,,"I asked this on SO first , but decided to move the math part of my question here. Consider a $p \times p$ symmetric and positive definite matrix $\bf A$ (where $p=70000$ , i.e., $\bf A$ is roughly 40 GB using 8-byte double s). We want to calculate the first $3$ diagonal elements of the inverse matrix, $({\bf A}^{-1})_{11}$ , $({\bf A}^{-1})_{22}$ and $({\bf A}^{-1})_{33}$ . I have found this paper by James R. Bunch who seems to solve this exact problem without calculating the full inverse $\bf A^{-1}$ . If I understand it correctly he first calculates the Cholesky decomposition, i.e. the upper triangular matrix $\bf R$ which satisfies $\bf A=R^T R$ , which needs $\frac16p^2+\frac12p^2-\frac23p$ floating point operations (multiplications/divisions) using the LINPACK function SPOFA . He then proceeds to calculate individual diagonal elements of the inverse $({\bf A^{-1}})_{ii}$ using an expression which exploits the sparsity of ${\bf R}^T{\bf y}={\bf e}_j$ and which requires $\frac12(p-i)^2+\frac52(p-i)+2$ floating point operations. (I don't understand the full details of this, so I can't currently sum it up correctly). The paper is based on LINPACK; it isn't cited by anyone, so it seems nobody cared for the last 23 years? After reading this , I'm wondering whether this is still the best way of doing things, or whether a modern LAPACK-based approach could avoid the Cholesky decomposition? In short, is there a quicker way to calculate those diagonal elements of the inverse ?","I asked this on SO first , but decided to move the math part of my question here. Consider a symmetric and positive definite matrix (where , i.e., is roughly 40 GB using 8-byte double s). We want to calculate the first diagonal elements of the inverse matrix, , and . I have found this paper by James R. Bunch who seems to solve this exact problem without calculating the full inverse . If I understand it correctly he first calculates the Cholesky decomposition, i.e. the upper triangular matrix which satisfies , which needs floating point operations (multiplications/divisions) using the LINPACK function SPOFA . He then proceeds to calculate individual diagonal elements of the inverse using an expression which exploits the sparsity of and which requires floating point operations. (I don't understand the full details of this, so I can't currently sum it up correctly). The paper is based on LINPACK; it isn't cited by anyone, so it seems nobody cared for the last 23 years? After reading this , I'm wondering whether this is still the best way of doing things, or whether a modern LAPACK-based approach could avoid the Cholesky decomposition? In short, is there a quicker way to calculate those diagonal elements of the inverse ?",p \times p \bf A p=70000 \bf A 3 ({\bf A}^{-1})_{11} ({\bf A}^{-1})_{22} ({\bf A}^{-1})_{33} \bf A^{-1} \bf R \bf A=R^T R \frac16p^2+\frac12p^2-\frac23p ({\bf A^{-1}})_{ii} {\bf R}^T{\bf y}={\bf e}_j \frac12(p-i)^2+\frac52(p-i)+2,"['matrices', 'inverse', 'numerical-linear-algebra', 'symmetric-matrices', 'positive-definite']"
96,Why intuitively do the quaternions satisfy the mixture of geometric and algebraic properties that they do?,Why intuitively do the quaternions satisfy the mixture of geometric and algebraic properties that they do?,,"[I completely rewrote the question to see if I could make it clearer. The comments below won't make any sense. In fact, my original question has been answered by Eric Wolfsey, so I may restore it.] When you read about the quaternions on Wikipedia and on many other sources, they are defined with the relation $$i^2 = j^2 = k^2 = ijk = -1$$ This comes across as completely random. There is no explanation of where this relation comes from. These sources then go on to prove that the quaternions satisfy various algebraic and geometric properties. Among these properties, they act on 3D vectors as rotations (but as a double-covering), they act on 4D space as rotations, they're associative, they're distributive. Etc. All of this comes across as a random coincidence when you compare it to the defining relation. Some of the time quaternions are thought of as 3D rotations. But this is a lossy interpretation, as a quaternion $q$ and its negation $-q$ represent the same rotation. For similar reasons, addition of quaternions starts to seem more bizarre. When defined as 4D rotations, they happen to be an ""isoclinic rotation"". I haven't thought enough about this concept... When defined as a complex matrix that satisfies a linear algebra relationship (sourced from here: https://qchu.wordpress.com/2011/02/12/su2-and-the-quaternions/ ), closure under multiplication is baked in, but closure under addition comes across as a coincidence. This relation is: $$M\text{ is a $2\times2$ matrix over $\mathbb C$}, M^\dagger M \in \mathbb R, \det(M) \geq 0 $$ Geometric Algebra textbooks show that they're an instance of a larger family of algebras with similar interpretations, called Geometric Algebra (or Clifford Algebras). But then why do Clifford Algebras exist? Why do they have their mixture of algebraic and geometric properties? It feels like you're replacing a small mystery with an even bigger mystery.","[I completely rewrote the question to see if I could make it clearer. The comments below won't make any sense. In fact, my original question has been answered by Eric Wolfsey, so I may restore it.] When you read about the quaternions on Wikipedia and on many other sources, they are defined with the relation This comes across as completely random. There is no explanation of where this relation comes from. These sources then go on to prove that the quaternions satisfy various algebraic and geometric properties. Among these properties, they act on 3D vectors as rotations (but as a double-covering), they act on 4D space as rotations, they're associative, they're distributive. Etc. All of this comes across as a random coincidence when you compare it to the defining relation. Some of the time quaternions are thought of as 3D rotations. But this is a lossy interpretation, as a quaternion and its negation represent the same rotation. For similar reasons, addition of quaternions starts to seem more bizarre. When defined as 4D rotations, they happen to be an ""isoclinic rotation"". I haven't thought enough about this concept... When defined as a complex matrix that satisfies a linear algebra relationship (sourced from here: https://qchu.wordpress.com/2011/02/12/su2-and-the-quaternions/ ), closure under multiplication is baked in, but closure under addition comes across as a coincidence. This relation is: Geometric Algebra textbooks show that they're an instance of a larger family of algebras with similar interpretations, called Geometric Algebra (or Clifford Algebras). But then why do Clifford Algebras exist? Why do they have their mixture of algebraic and geometric properties? It feels like you're replacing a small mystery with an even bigger mystery.","i^2 = j^2 = k^2 = ijk = -1 q -q M\text{ is a 2\times2 matrix over \mathbb C}, M^\dagger M \in \mathbb R, \det(M) \geq 0 ","['matrices', 'soft-question', 'hypercomplex-numbers']"
97,Elementwise maximum of two positive definite matrices,Elementwise maximum of two positive definite matrices,,"Assume that $A$ and $B$ are real, symmetric, positive definite matrices of the same size, that is, $$A \succ 0, B\succ 0.$$ Let $\operatorname{elmax(A,B)}$ be the element-wise maximum matrix, consisting of scalar maxima of elements of $A$ and $B$. Is it always the case that $$\operatorname{elmax}(A,B)\succ 0?$$ That is, does the element-wise maximum operation preserve positive definiteness?","Assume that $A$ and $B$ are real, symmetric, positive definite matrices of the same size, that is, $$A \succ 0, B\succ 0.$$ Let $\operatorname{elmax(A,B)}$ be the element-wise maximum matrix, consisting of scalar maxima of elements of $A$ and $B$. Is it always the case that $$\operatorname{elmax}(A,B)\succ 0?$$ That is, does the element-wise maximum operation preserve positive definiteness?",,"['matrices', 'convex-analysis', 'positive-definite']"
98,Does negative transpose sign mean inverse of a transposed matrix or transpose of an inverse matrix?,Does negative transpose sign mean inverse of a transposed matrix or transpose of an inverse matrix?,,I want to know meaning of $$H^{-T}$$Is it same with $$(H^{-1})^T$$or $$(H^T)^{-1}$$,I want to know meaning of $$H^{-T}$$Is it same with $$(H^{-1})^T$$or $$(H^T)^{-1}$$,,"['matrices', 'inverse', 'transpose']"
99,Infinity matrix norm example,Infinity matrix norm example,,"I have a brief question regarding the infinity matrix norm. The subordinate matrix infinity norm is defined as: $$\|A\|_{\infty} =\max_{1 \leq i \leq n}\sum_{j=1}^{n}|a_{ij}|.$$ This is derived from the general definition of a subordinate matrix norm which is defined as: $$\|A\| = \max \left\{\frac{\|Ax\|}{\|x\|} : x \in K^{n}, x \neq 0\right\}.$$ I wanted to try this out in an example.  So say we define the matrix: $$A = \begin{bmatrix} 1 & 4 & 2 \\ 3 & 1 & 2 \\ 4 & 4 & 3 \end{bmatrix}$$ and $$x = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}.$$ Now if we use the first definition, it is easy to see that $\|A\|_{\infty} = 11$ But if we use the general definition, we get: $$\|A\|_{\infty} =\max \left\{\frac{\|Ax\|_{\infty}}{\|x\|_{\infty}} : x \in K^{n}, x \neq 0\right\}.$$ Now, we have: $$Ax = \begin{bmatrix} 15 \\ 11 \\ 21 \end{bmatrix}.$$ Since the infinity vector norm is defined as: $$\|x\|_{\infty} =\max_{1 \leq i \leq n} |x_i|$$ it follows that: $$\|Ax\|_{\infty} = 21$$ and: $$\|x\|_{\infty} = 3$$ But then we have: $$\frac{\|Ax\|_{\infty}}{\|x\|_{\infty}} = \frac{21}3 = 7$$ that does not correlate with the fact that we previously found that $\|A\|_{\infty} = 11$. If anyone can explain to me what is wrong with my reasoning here, I would appreciate it!","I have a brief question regarding the infinity matrix norm. The subordinate matrix infinity norm is defined as: $$\|A\|_{\infty} =\max_{1 \leq i \leq n}\sum_{j=1}^{n}|a_{ij}|.$$ This is derived from the general definition of a subordinate matrix norm which is defined as: $$\|A\| = \max \left\{\frac{\|Ax\|}{\|x\|} : x \in K^{n}, x \neq 0\right\}.$$ I wanted to try this out in an example.  So say we define the matrix: $$A = \begin{bmatrix} 1 & 4 & 2 \\ 3 & 1 & 2 \\ 4 & 4 & 3 \end{bmatrix}$$ and $$x = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}.$$ Now if we use the first definition, it is easy to see that $\|A\|_{\infty} = 11$ But if we use the general definition, we get: $$\|A\|_{\infty} =\max \left\{\frac{\|Ax\|_{\infty}}{\|x\|_{\infty}} : x \in K^{n}, x \neq 0\right\}.$$ Now, we have: $$Ax = \begin{bmatrix} 15 \\ 11 \\ 21 \end{bmatrix}.$$ Since the infinity vector norm is defined as: $$\|x\|_{\infty} =\max_{1 \leq i \leq n} |x_i|$$ it follows that: $$\|Ax\|_{\infty} = 21$$ and: $$\|x\|_{\infty} = 3$$ But then we have: $$\frac{\|Ax\|_{\infty}}{\|x\|_{\infty}} = \frac{21}3 = 7$$ that does not correlate with the fact that we previously found that $\|A\|_{\infty} = 11$. If anyone can explain to me what is wrong with my reasoning here, I would appreciate it!",,"['matrices', 'normed-spaces', 'infinity', 'matrix-norms']"
