,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Sufficient Conditions for the Characteristic Function to Be Differentiable,Sufficient Conditions for the Characteristic Function to Be Differentiable,,"Given a random variable $X$, it's characteristic funciton is defined as: $$\phi_X(t) = \mathbb{E}[e^{itX}]$$ I'm wondering what conditions are required for the characteristic function of a random variable to be differentiable (i.e. for $\frac{d\phi_X(t)}{dt}$ to exist)?","Given a random variable $X$, it's characteristic funciton is defined as: $$\phi_X(t) = \mathbb{E}[e^{itX}]$$ I'm wondering what conditions are required for the characteristic function of a random variable to be differentiable (i.e. for $\frac{d\phi_X(t)}{dt}$ to exist)?",,"['probability', 'measure-theory']"
1,Yet another balls and bins problem,Yet another balls and bins problem,,"If $p_n$ denote the probability that when $n$ balls are randomly put in $n$ bins then there is at least one bin with exactly one ball. Is there a simple (involving only little computation) reason for why $p_{n+1}>p_{n}$ if $n>3$ ? This simple looking problem turns out to be not too simple, perhaps because ${p_{n}}/{p_{n-1}}$ turns out to be approximately $e(1-{1}/{n})^{n-1}$ and hence whatever the difference is, it is extremely small. Thanks","If $p_n$ denote the probability that when $n$ balls are randomly put in $n$ bins then there is at least one bin with exactly one ball. Is there a simple (involving only little computation) reason for why $p_{n+1}>p_{n}$ if $n>3$ ? This simple looking problem turns out to be not too simple, perhaps because ${p_{n}}/{p_{n-1}}$ turns out to be approximately $e(1-{1}/{n})^{n-1}$ and hence whatever the difference is, it is extremely small. Thanks",,"['probability', 'balls-in-bins']"
2,What is the probability of rolling a 1 on the Catenative Doomsday Dice Cascader?,What is the probability of rolling a 1 on the Catenative Doomsday Dice Cascader?,,"As seen here . Assume that each cascader bubble begins with a d6, as opposed to being determined by the prime bubble along with the number of cascader bubbles; the scene makes it ambiguous. $\frac{1}{36} = 0.02777...$ provides a simple lower bound for the probability (1 on the prime bubble and the sole cascader bubble). If the prime bubble rolls a 2, the odds are $\frac{1}{6}\left(\frac{1}{6}+\frac{1}{12}+\frac{1}{18}+\frac{1}{24}+\frac{1}{30}+\frac{1}{36}\right) = \frac{49}{720} = 0.0680555...$; multiple that by the 1/6 chance of rolling that 2 and add to the original 1/36, and you get $0.02\bar{7}+0.01134\overline{259} = 0.03912\overline{037}$. After that, it gets way beyond me.","As seen here . Assume that each cascader bubble begins with a d6, as opposed to being determined by the prime bubble along with the number of cascader bubbles; the scene makes it ambiguous. $\frac{1}{36} = 0.02777...$ provides a simple lower bound for the probability (1 on the prime bubble and the sole cascader bubble). If the prime bubble rolls a 2, the odds are $\frac{1}{6}\left(\frac{1}{6}+\frac{1}{12}+\frac{1}{18}+\frac{1}{24}+\frac{1}{30}+\frac{1}{36}\right) = \frac{49}{720} = 0.0680555...$; multiple that by the 1/6 chance of rolling that 2 and add to the original 1/36, and you get $0.02\bar{7}+0.01134\overline{259} = 0.03912\overline{037}$. After that, it gets way beyond me.",,"['probability', 'recreational-mathematics', 'dice']"
3,Fractional Part of Sum of Sequence of Independent Normal Random Variables,Fractional Part of Sum of Sequence of Independent Normal Random Variables,,"I'm trying to prove that if $X_n$ iid normal $S_n = \sum_1^n X_i$ $U_n=S_n-\lfloor S_n\rfloor$ then $U_n$ is asymptotically uniform in distribution. I've got no idea how to approach this, and it's a past exam question, so I should be able to do it inside 30 mins. My only ideas for how to approach it were (1) possibly use an ergodic theorem (since the floor function looked inviting). (2) work with characteristic functions and use Levy's Continuity Theorem (but I can't see how to work out the characteristic functions of the $U_n$). Has anyone got an idea how to proceed? A hint rather than a full solution would be ideal, since I'd quite like to have something to work out myself, once I've got an idea how to start! Many thanks.","I'm trying to prove that if $X_n$ iid normal $S_n = \sum_1^n X_i$ $U_n=S_n-\lfloor S_n\rfloor$ then $U_n$ is asymptotically uniform in distribution. I've got no idea how to approach this, and it's a past exam question, so I should be able to do it inside 30 mins. My only ideas for how to approach it were (1) possibly use an ergodic theorem (since the floor function looked inviting). (2) work with characteristic functions and use Levy's Continuity Theorem (but I can't see how to work out the characteristic functions of the $U_n$). Has anyone got an idea how to proceed? A hint rather than a full solution would be ideal, since I'd quite like to have something to work out myself, once I've got an idea how to start! Many thanks.",,"['probability', 'measure-theory', 'probability-theory', 'probability-distributions', 'normal-distribution']"
4,Mahalanobis  distance to probability between 0.0 and 1.0,Mahalanobis  distance to probability between 0.0 and 1.0,,"given the mahalanobis distance : $D_M^2(x) = (x-\mu)^T S^{-1}(x-\mu)$ how can I obtain the probability of $x = ( x_1, x_2, x_3, \dots, x_N )^T$ belonging to the data set given by covariance matrix $S$ and mean vector $\mu = ( \mu_1, \mu_2, \mu_3, \dots , \mu_N )^T$? If sample count is needed this is denoted $m$. I would like something I can use in a computer algorithm. Related to this I could ask how to obtain the hyper-ellipsoid that defines the confidence interval for e.g. 95%?","given the mahalanobis distance : $D_M^2(x) = (x-\mu)^T S^{-1}(x-\mu)$ how can I obtain the probability of $x = ( x_1, x_2, x_3, \dots, x_N )^T$ belonging to the data set given by covariance matrix $S$ and mean vector $\mu = ( \mu_1, \mu_2, \mu_3, \dots , \mu_N )^T$? If sample count is needed this is denoted $m$. I would like something I can use in a computer algorithm. Related to this I could ask how to obtain the hyper-ellipsoid that defines the confidence interval for e.g. 95%?",,"['probability', 'statistics']"
5,Application of the First Moment Method to Random Graphs,Application of the First Moment Method to Random Graphs,,"I've been trying for a few days to figure out a proof of part $(iii)$ of Lemma 2.1 of this paper , on page 4, and I could definitely use some help. You don't need to understand any of the rest of the paper to understand what lemma 2.1 is saying, it's purely about a few properties of random graphs. Could anyone explain how such a proof (just of part $(iii)$) might go? As far as I know, the ""first moment method"" is simply the observation that $\mathbb{P}(X>0) \leq \mathbb{E}(X)$. We can condition this on ""vertices $v$, $w$ are at distances at most $i$ from each other"" but I can't really get anywhere with it. I guess the expected number of length-$i$ paths between 2 fixed vertices is $(n-2)(n-3)\ldots(n-i)p^i \sim d^i/n = n^{i\alpha - 1}$. Conditioning on $d(v,w)\leq i$ (which is obviously not the same as necessarily having a path of length $i$ between the 2 vertices, although I think the probabilities of these events become arbitrarily close), I'm not sure how we pull out a $2/(1-i\alpha)$ out of the details. I think we can use part $(i)$ of the lemma to get out probabilities that a vertex is at distance at most $i$ from another, and probabilities that a vertex is at distance exactly $i$ from another (both approximately $d^i/n$). After that I tried extending the first moment method to the statement $\mathbb{P}(X>\frac{2}{1-i\alpha}) \leq \frac{1-i\alpha}{2} \mathbb{E}(X)$ but again, I'm unclear where to go from here. I'd be really appreciative of a proof of this part of the lemma - it looks to be fairly simple, so I'm not sure what I'm missing. Alternatively, if anyone has a reference which happens to prove this result then I would be more than happy to locate that, but I've looked through numerous books on random graphs and spent a lot of time searching online to no avail. Thank you for your help - Sam","I've been trying for a few days to figure out a proof of part $(iii)$ of Lemma 2.1 of this paper , on page 4, and I could definitely use some help. You don't need to understand any of the rest of the paper to understand what lemma 2.1 is saying, it's purely about a few properties of random graphs. Could anyone explain how such a proof (just of part $(iii)$) might go? As far as I know, the ""first moment method"" is simply the observation that $\mathbb{P}(X>0) \leq \mathbb{E}(X)$. We can condition this on ""vertices $v$, $w$ are at distances at most $i$ from each other"" but I can't really get anywhere with it. I guess the expected number of length-$i$ paths between 2 fixed vertices is $(n-2)(n-3)\ldots(n-i)p^i \sim d^i/n = n^{i\alpha - 1}$. Conditioning on $d(v,w)\leq i$ (which is obviously not the same as necessarily having a path of length $i$ between the 2 vertices, although I think the probabilities of these events become arbitrarily close), I'm not sure how we pull out a $2/(1-i\alpha)$ out of the details. I think we can use part $(i)$ of the lemma to get out probabilities that a vertex is at distance at most $i$ from another, and probabilities that a vertex is at distance exactly $i$ from another (both approximately $d^i/n$). After that I tried extending the first moment method to the statement $\mathbb{P}(X>\frac{2}{1-i\alpha}) \leq \frac{1-i\alpha}{2} \mathbb{E}(X)$ but again, I'm unclear where to go from here. I'd be really appreciative of a proof of this part of the lemma - it looks to be fairly simple, so I'm not sure what I'm missing. Alternatively, if anyone has a reference which happens to prove this result then I would be more than happy to locate that, but I've looked through numerous books on random graphs and spent a lot of time searching online to no avail. Thank you for your help - Sam",,"['probability', 'graph-theory']"
6,"If W is a random matrix with variance $\mathbb{E}[W W^{T}]$, what's $\mathbb{E}[W^{T} P W]$?","If W is a random matrix with variance , what's ?",\mathbb{E}[W W^{T}] \mathbb{E}[W^{T} P W],"I know quite a few identities about quadratic forms of random vectors, but I'm having difficulty coaxing something out of this quadratic form of random matrices.  Suppose I know $\mathbb{E}[W W^{T}]$ and $\mathbb{E}[W] = 0$.  Then can I deduce a closed for form $\mathbb{E}[W^{T} P W]$ for a non-random matrix $P$? EDIT Changed given from $\mathbb{E}[W^{T} W]$ to $\mathbb{E}[W W^{T}]$, added $\mathbb{E}[W] = 0$.","I know quite a few identities about quadratic forms of random vectors, but I'm having difficulty coaxing something out of this quadratic form of random matrices.  Suppose I know $\mathbb{E}[W W^{T}]$ and $\mathbb{E}[W] = 0$.  Then can I deduce a closed for form $\mathbb{E}[W^{T} P W]$ for a non-random matrix $P$? EDIT Changed given from $\mathbb{E}[W^{T} W]$ to $\mathbb{E}[W W^{T}]$, added $\mathbb{E}[W] = 0$.",,"['probability', 'statistics', 'random']"
7,Nested Expected Values,Nested Expected Values,,"Assume we have random variables $X_1,\dots,X_N$ i.i.d. $\mathcal{U}\,(0,1)$ distributed and now define $Y_i$ as  $$Y_i = f(Y_{i-1},X_i)\qquad \text{and}\qquad Y_0 \text{ arbitrary constant}$$ for some function $f:\mathbb R^2 \to \mathbb R$, i.e. we have a stochastic process where the next step depends on the last and some independent new part ($X_i$). Now I am interested in $\mathbb E \, Y_N$ and suppose I follow an appealingly simple approach: Compute inductively $$ y_0=Y_0\quad,\quad y_i = f(y_{i-1},\mathbb E X_i) = f(y_{i-1},0.5)$$ Under what assumptions does $\mathbb E\,Y_N = y_N$ hold? Motivation: The above problem is an abstraction of the analysis of a random search optimization algorithm. Assume a bijective function $g:S\to\{1,\dots,|S|\}$, which we'd like to maximize and a current iterate $x_i$. One step of the algorithm consists of choosing a random point $x_{i+1}\in S$ and accepting it if $g(x_{i+1}) > g(x_i)$. We are interested in the expected function value $g(x_N)$ reached after $N$ improving steps  when starting in the worst point (for simplicity, we do not count points with lower function value). For $N \ll |S|$, this leads to the above abstract problem with  $$f(y,x) = (|S|-y)x+y\;.$$","Assume we have random variables $X_1,\dots,X_N$ i.i.d. $\mathcal{U}\,(0,1)$ distributed and now define $Y_i$ as  $$Y_i = f(Y_{i-1},X_i)\qquad \text{and}\qquad Y_0 \text{ arbitrary constant}$$ for some function $f:\mathbb R^2 \to \mathbb R$, i.e. we have a stochastic process where the next step depends on the last and some independent new part ($X_i$). Now I am interested in $\mathbb E \, Y_N$ and suppose I follow an appealingly simple approach: Compute inductively $$ y_0=Y_0\quad,\quad y_i = f(y_{i-1},\mathbb E X_i) = f(y_{i-1},0.5)$$ Under what assumptions does $\mathbb E\,Y_N = y_N$ hold? Motivation: The above problem is an abstraction of the analysis of a random search optimization algorithm. Assume a bijective function $g:S\to\{1,\dots,|S|\}$, which we'd like to maximize and a current iterate $x_i$. One step of the algorithm consists of choosing a random point $x_{i+1}\in S$ and accepting it if $g(x_{i+1}) > g(x_i)$. We are interested in the expected function value $g(x_N)$ reached after $N$ improving steps  when starting in the worst point (for simplicity, we do not count points with lower function value). For $N \ll |S|$, this leads to the above abstract problem with  $$f(y,x) = (|S|-y)x+y\;.$$",,"['probability', 'stochastic-processes', 'markov-chains']"
8,expected value for min estimated entropy,expected value for min estimated entropy,,"We have a random generator that generates independent random bits with probability $P(x=1) = P$ and $P(x=0)=1-P$. Given $N$ random independent bits, we estimate $P$ by $\hat{P} = N_1/(N_0+N_1)$. where $N_0$ is the number of $0$'s and $N_1$ is number of $1$'s. The expected value for $\hat{P}$ can be simply shown to be $P$. a. What is the expected value for the estimated entropy defined as following:   $\hat{H}=-[ \hat{P} \log(\hat{P}) + (1-\hat{P}) \log(1-\hat{P}) ]$ b. If we take $M$ independent sets of $N$ random bits as above and each time estimate the entropy using the above equation, What is the expected value for the smallest estimated entropy among those $M$ sets ? thanks,  MG P.S. If the solution to the integral for general P is too complicated, a solution to the special case $P=1/2$ would be appreciated as well.","We have a random generator that generates independent random bits with probability $P(x=1) = P$ and $P(x=0)=1-P$. Given $N$ random independent bits, we estimate $P$ by $\hat{P} = N_1/(N_0+N_1)$. where $N_0$ is the number of $0$'s and $N_1$ is number of $1$'s. The expected value for $\hat{P}$ can be simply shown to be $P$. a. What is the expected value for the estimated entropy defined as following:   $\hat{H}=-[ \hat{P} \log(\hat{P}) + (1-\hat{P}) \log(1-\hat{P}) ]$ b. If we take $M$ independent sets of $N$ random bits as above and each time estimate the entropy using the above equation, What is the expected value for the smallest estimated entropy among those $M$ sets ? thanks,  MG P.S. If the solution to the integral for general P is too complicated, a solution to the special case $P=1/2$ would be appreciated as well.",,"['probability', 'statistics']"
9,Verifying the distribution of an ensemble of random matrices,Verifying the distribution of an ensemble of random matrices,,"Suppose one is trying to devise a method to generate random matrices with a certain distribution. How does one verify that the generated matrices follow the desired distribution? In particular, I am interested in uniformly distributed random orthogonal matrices, but I am curious how people test their codes for correctness in general.","Suppose one is trying to devise a method to generate random matrices with a certain distribution. How does one verify that the generated matrices follow the desired distribution? In particular, I am interested in uniformly distributed random orthogonal matrices, but I am curious how people test their codes for correctness in general.",,"['probability-theory', 'matrices', 'probability']"
10,Show that $\frac{1}{\sqrt{n}} S_n$ does not converge in probability. [duplicate],Show that  does not converge in probability. [duplicate],\frac{1}{\sqrt{n}} S_n,"This question already has answers here : Convergence in Central Limit Thorem (3 answers) Closed 2 days ago . Let $(X_n)$ be a sequence of stochastically independent and identically distributed random variables with $\mathbb{E}X_1 = \mu < \infty$ and $\text{Var} \, X_1 = \sigma^2 < \infty$ , where $\sigma^2 > 0$ . Let $Y_n = X_n - \mu$ and $S_n = \sum_{k=1}^{n} Y_k$ for all $n \in \mathbb{N}$ . The central limit theorem now gives the convergence $$ \frac{1}{\sqrt{n}} S_n \xrightarrow{d} X, \quad n \to \infty, $$ for an $X \sim \mathcal{N}(0, \sigma^2)$ . Show that $\frac{1}{\sqrt{n}} S_n$ does not converge in probability. Hint : Consider $Z_{2n}$ , where $Z_n = \frac{1}{\sqrt{n}} S_n$ . I don't understand why the claim should hold, so what is going wrong that it doesn't converge in probability?","This question already has answers here : Convergence in Central Limit Thorem (3 answers) Closed 2 days ago . Let be a sequence of stochastically independent and identically distributed random variables with and , where . Let and for all . The central limit theorem now gives the convergence for an . Show that does not converge in probability. Hint : Consider , where . I don't understand why the claim should hold, so what is going wrong that it doesn't converge in probability?","(X_n) \mathbb{E}X_1 = \mu < \infty \text{Var} \, X_1 = \sigma^2 < \infty \sigma^2 > 0 Y_n = X_n - \mu S_n = \sum_{k=1}^{n} Y_k n \in \mathbb{N}  \frac{1}{\sqrt{n}} S_n \xrightarrow{d} X, \quad n \to \infty,  X \sim \mathcal{N}(0, \sigma^2) \frac{1}{\sqrt{n}} S_n Z_{2n} Z_n = \frac{1}{\sqrt{n}} S_n","['probability', 'probability-theory', 'convergence-divergence', 'central-limit-theorem']"
11,Smallest eigenvalue of matrix with random elements (non-central Wishart),Smallest eigenvalue of matrix with random elements (non-central Wishart),,"Suppose that $X \in \mathbb R^{d \times n}$ is a random matrix with independent entries, each of which follows the standard normal law $\mathcal N(0, 1)$ , and that $M \in \mathbb R^{d \times n}$ is a given non-random matrix. Consider the random matrices $$ A = XX^\top \in \mathbb R^{d\times d}, \qquad B = (X + M)(X + M)^\top \in \mathbb R^{d \times d}. $$ Intuitively, it seems to me that the following statement should hold true irrespectively of $d$ , $n$ and $M$ : $$ \tag{1} \forall r \geq 0, \qquad \mathbb P\Big[\lambda_{\min}(A) \geq r \Bigl] ≤ \mathbb P\Big[\lambda_{\min}(B) \geq r \Bigl]. $$ This appears to be a simple statement, but I have not found it in the literature on non-central Wishart distributions. Does (1) follow from known results, or is there a simple approach to prove or disprove it? Case $d = 1$ . The statement is true in this case. Indeed, for each $i \in \{1, \dotsc, n\}$ let $(X_i, Y_i)$ denote a coupling  (with couplings for different values of $i$ independent of each other) such that $$ X_i \sim \mathcal N(0, 1), \qquad  Y_i \sim \mathcal N(m_i, 1), \qquad $$ and additionally $$ X_i^2 \leq Y_i^2 \qquad \text{almost surely}. $$ Showing that such a coupling exists is not difficult. Then $Y$ is equal in law to $X + M$ and $$ XX^\top = \sum_{i=1}^{n} X_i^2 \leq \sum_{i=1}^{n} Y_i^2 = YY^\top \qquad \text{almost surely.} $$ The conclusion then follows easily.","Suppose that is a random matrix with independent entries, each of which follows the standard normal law , and that is a given non-random matrix. Consider the random matrices Intuitively, it seems to me that the following statement should hold true irrespectively of , and : This appears to be a simple statement, but I have not found it in the literature on non-central Wishart distributions. Does (1) follow from known results, or is there a simple approach to prove or disprove it? Case . The statement is true in this case. Indeed, for each let denote a coupling  (with couplings for different values of independent of each other) such that and additionally Showing that such a coupling exists is not difficult. Then is equal in law to and The conclusion then follows easily.","X \in \mathbb R^{d \times n} \mathcal N(0, 1) M \in \mathbb R^{d \times n} 
A = XX^\top \in \mathbb R^{d\times d}, \qquad B = (X + M)(X + M)^\top \in \mathbb R^{d \times d}.
 d n M 
\tag{1}
\forall r \geq 0, \qquad
\mathbb P\Big[\lambda_{\min}(A) \geq r \Bigl] ≤ \mathbb P\Big[\lambda_{\min}(B) \geq r \Bigl].
 d = 1 i \in \{1, \dotsc, n\} (X_i, Y_i) i 
X_i \sim \mathcal N(0, 1), \qquad 
Y_i \sim \mathcal N(m_i, 1), \qquad
 
X_i^2 \leq Y_i^2 \qquad \text{almost surely}.
 Y X + M 
XX^\top = \sum_{i=1}^{n} X_i^2 \leq \sum_{i=1}^{n} Y_i^2 = YY^\top \qquad \text{almost surely.}
","['probability', 'statistics', 'random-matrices']"
12,When and Where will 2 Random Walks Meet for the First Time?,When and Where will 2 Random Walks Meet for the First Time?,,"This is a question I thought of recently: (Based on some set of initial conditions, i.e. initial positions and movement probabilities , and the current time and positions) When and Where will 2 Random Walks Meet for the First Time? Is there a joint probability function? I found the following questions that discuss related topics, but not something that directly relates to this question : Probability that random walk will reach state $k$ for the first time on step $n$ https://youtu.be/F_kt51Qj1RI?si=zymYfp4EoNSjMoJ7 Probability distribution of time at which 2 random walks meet Intersection of two simple random walks https://mathoverflow.net/questions/54590/intersection-probabilities-for-random-walk-in-d2 Alternate proof for two random walks in $d$ dimension meeting As an example, consider the following two Random Walks (Here, $X(t)$ represents the position of the first Random Walk at time $t$ and $Y(t)$ represents the position of the second Random Walk at time $t$ . $S_x(t)$ and $S_y(t)$ represent the random movement, i.e. outcomes at each point $t$ ): $$ X(t) =  \begin{cases}  x_0 & \text{if } t = 0 \\ X(t-1) + S_x(t) & \text{if } t > 0  \end{cases} $$ $$ Y(t) =  \begin{cases}  y_0 & \text{if } t = 0 \\ Y(t-1) + S_y(t) & \text{if } t > 0  \end{cases} $$ $$ x_0, y_0, X(t), Y(t) \in \mathbb{Z} \quad \forall t $$ $$ S_x(t) =  \begin{cases}  +1 & \text{with probability } p= p_1 \\ -1 & \text{with probability } p= 1- p_1 \end{cases} $$ $$ S_y(t) =  \begin{cases}  +1 & \text{with probability } p= p_2 \\ -1 & \text{with probability } p= 1- p_2 \end{cases} $$ Thus, is it possible to derive a probability distribution capable of describing the distribution for the ""number of steps"" (time) and ""position"" where these two Random Walks will intersect based on their initial conditions and current conditions? Can a joint probability distribution function be written? I have watched videos (e.g. https://www.youtube.com/watch?v=iH2kATv49rc ) where they explain that a Random Walk in 1 Dimension (like my question) and 2 Dimensions is guaranteed to visit each possible point (however, this is not true for Random Walks in more than 2 Dimensions). Thus, I would informally conclude that since both Random Walks can occupy any point in the same domain (i.e. set of all integers), it is reasonable to believe that their intersection is theoretically possible. However, just because they have the ability to occupy and point in the same domain, I am not sure that this theoretically guarantees their intersection. For example, suppose both Random Walks start very far from each other, and one of them has higher probability to move left and the other has higher probability move right. In this situation, it seems logical to believe that it will take more time and more steps for these two Random Walks to intersect ... compared to a situation where these two Random Walks started at positions closer to each other and had higher probabilities of moving in the same direction. I also understand that conditional on the current positions of both Random Walks, some intersection times are not possible. For example, if both Random Walks are currently situated very far from each other, an intersection will not be possible in the next time point (a certain minimum amount of time is logically required). I think this will affect the ""Support"" of these intersection distributions. I tried to run multiple simulations in R corresponding to this situation (note: if a given simulation takes more than 100,000 turns, I terminate the simulation. I record the time taken as 100,000 on the time graph, but I don't record the position on the position graph). library(data.table) library(ggplot2) library(gridExtra)  x0 <- -2 y0 <- 2  meeting_times <- integer() meeting_positions <- integer()  for (i in 1:1000) {     X <- x0     Y <- y0          t <- 0          while (X != Y && t < 100000) {         step_X <- sample(c(-1, 1), 1)         step_Y <- sample(c(-1, 1), 1)                       X <- X + step_X         Y <- Y + step_Y                           t <- t + 1     }           if (t >= 100000) {         t <- 100000     } else {               meeting_positions <- c(meeting_positions, X)     }            meeting_times <- c(meeting_times, t) }  df_times <- data.table(time = meeting_times)  p1 <- ggplot(df_times, aes(x = time)) +     geom_density(fill = ""black"", color = ""black"") +     ggtitle(paste(""Distribution of Time Needed for Random Walks to Meet\nAverage Time ="", round(mean(meeting_times), 2),                   ""\nAverage Time ="", mean(meeting_times),                   ""\nNumber of Simulations = 1000"",                   ""\nStarting Points: X0 ="", x0, "", Y0 ="", y0,                   ""\nProbabilities: P(X step) = 0.5, P(Y step) = 0.5"")) +     theme_bw()   df_positions <- data.table(position = meeting_positions)  p2 <- ggplot(df_positions, aes(x = position)) +     geom_density(fill = ""black"", color = ""black"") +     ggtitle(paste(""Distribution of Meeting Positions for Random Walks\nAverage Position ="", round(mean(meeting_positions), 2),                   ""\nNumber of Simulations = 1000"",                   ""\nStarting Points: X0 ="", x0, "", Y0 ="", y0,                   ""\nProbabilities: P(X step) = 0.5, P(Y step) = 0.5"")) +     theme_bw() The outputs of the simulation look like this: And just as an example, I showed an example of the trajectory taken by a given simulation until the intersection occurs: x0 <- -2 y0 <- 2  path_X <- integer() path_Y <- integer()  X <- x0 Y <- y0  t <- 0  while (X != Y && t < 100000) {      step_X <- sample(c(-1, 1), 1)     step_Y <- sample(c(-1, 1), 1)          X <- X + step_X     Y <- Y + step_Y           path_X <- c(path_X, X)     path_Y <- c(path_Y, Y)           t <- t + 1 }   df_paths <- data.frame(time = 1:length(path_X), X = path_X, Y = path_Y)  ggplot(df_paths, aes(x = time)) +     geom_line(aes(y = X, color = ""red"")) +     geom_line(aes(y = Y, color = ""blue"")) +     scale_color_manual(values = c(""red"" = ""red"", ""blue"" = ""blue"")) +     labs(color = ""Random Walk"", x = ""Time"", y = ""Position"") +     ggtitle(""Paths of Two Random Walks"") +     theme_bw() And here is the same plot for 1000 simulations: x0 <- -2 y0 <- 2   df_paths <- data.frame()   for (i in 1:1000) {     path_X <- integer()     path_Y <- integer()     print(i)         X <- x0     Y <- y0               t <- 0           while (X != Y && t < 100000) {                 step_X <- sample(c(-1, 1), 1)         step_Y <- sample(c(-1, 1), 1)                           X <- X + step_X         Y <- Y + step_Y                          path_X <- c(path_X, X)         path_Y <- c(path_Y, Y)                  t <- t + 1     }           df_paths_i <- data.frame(time = 1:length(path_X), X = path_X, Y = path_Y, simulation = rep(i, length(path_X)))             df_paths <- rbind(df_paths, df_paths_i) }  ggplot(df_paths, aes(x = time)) +     geom_line(aes(y = X, color = ""red""), alpha = 0.1) +     geom_line(aes(y = Y, color = ""blue""), alpha = 0.1) +     scale_color_manual(values = c(""red"" = ""red"", ""blue"" = ""blue"")) +     labs(color = ""Random Walk"", x = ""Time"", y = ""Position"") +     ggtitle(""Paths of Two Random Walks for 100 Simulations"") +     theme_bw() But going back to my question: Based on some set of initial conditions $x_0$ , $y_0$ $p_1$ and $p_2$ and their current positions, is it possible to derive probability distributions which describe the number of steps and the time required for intersection? Can a Joint Probability Distribution Function be derived? $$ G : P(T=t | x_0, y_0, p_1, p_2, T-1 = t-1) \sim  ??? $$ $$ H: P(X(t) = Y(t) | x_0, y_0, p_1, p_2, X(t-1) = x(t-1), Y(t-1) = y(t-1) ) \sim  ??? $$ $$ \text{Joint Probability Distribution Function of Intersection Time and Intersection Position: } P(G,H) \sim  ??? $$ Thanks! Note: In the above distributions $T=t$ is a random variable representing the intersection time $X(t-1) = x(t-1)$ and $Y(t-1) = y(t-1)$ are random variables representing the position of both Random Walks at the most recent time","This is a question I thought of recently: (Based on some set of initial conditions, i.e. initial positions and movement probabilities , and the current time and positions) When and Where will 2 Random Walks Meet for the First Time? Is there a joint probability function? I found the following questions that discuss related topics, but not something that directly relates to this question : Probability that random walk will reach state $k$ for the first time on step $n$ https://youtu.be/F_kt51Qj1RI?si=zymYfp4EoNSjMoJ7 Probability distribution of time at which 2 random walks meet Intersection of two simple random walks https://mathoverflow.net/questions/54590/intersection-probabilities-for-random-walk-in-d2 Alternate proof for two random walks in $d$ dimension meeting As an example, consider the following two Random Walks (Here, represents the position of the first Random Walk at time and represents the position of the second Random Walk at time . and represent the random movement, i.e. outcomes at each point ): Thus, is it possible to derive a probability distribution capable of describing the distribution for the ""number of steps"" (time) and ""position"" where these two Random Walks will intersect based on their initial conditions and current conditions? Can a joint probability distribution function be written? I have watched videos (e.g. https://www.youtube.com/watch?v=iH2kATv49rc ) where they explain that a Random Walk in 1 Dimension (like my question) and 2 Dimensions is guaranteed to visit each possible point (however, this is not true for Random Walks in more than 2 Dimensions). Thus, I would informally conclude that since both Random Walks can occupy any point in the same domain (i.e. set of all integers), it is reasonable to believe that their intersection is theoretically possible. However, just because they have the ability to occupy and point in the same domain, I am not sure that this theoretically guarantees their intersection. For example, suppose both Random Walks start very far from each other, and one of them has higher probability to move left and the other has higher probability move right. In this situation, it seems logical to believe that it will take more time and more steps for these two Random Walks to intersect ... compared to a situation where these two Random Walks started at positions closer to each other and had higher probabilities of moving in the same direction. I also understand that conditional on the current positions of both Random Walks, some intersection times are not possible. For example, if both Random Walks are currently situated very far from each other, an intersection will not be possible in the next time point (a certain minimum amount of time is logically required). I think this will affect the ""Support"" of these intersection distributions. I tried to run multiple simulations in R corresponding to this situation (note: if a given simulation takes more than 100,000 turns, I terminate the simulation. I record the time taken as 100,000 on the time graph, but I don't record the position on the position graph). library(data.table) library(ggplot2) library(gridExtra)  x0 <- -2 y0 <- 2  meeting_times <- integer() meeting_positions <- integer()  for (i in 1:1000) {     X <- x0     Y <- y0          t <- 0          while (X != Y && t < 100000) {         step_X <- sample(c(-1, 1), 1)         step_Y <- sample(c(-1, 1), 1)                       X <- X + step_X         Y <- Y + step_Y                           t <- t + 1     }           if (t >= 100000) {         t <- 100000     } else {               meeting_positions <- c(meeting_positions, X)     }            meeting_times <- c(meeting_times, t) }  df_times <- data.table(time = meeting_times)  p1 <- ggplot(df_times, aes(x = time)) +     geom_density(fill = ""black"", color = ""black"") +     ggtitle(paste(""Distribution of Time Needed for Random Walks to Meet\nAverage Time ="", round(mean(meeting_times), 2),                   ""\nAverage Time ="", mean(meeting_times),                   ""\nNumber of Simulations = 1000"",                   ""\nStarting Points: X0 ="", x0, "", Y0 ="", y0,                   ""\nProbabilities: P(X step) = 0.5, P(Y step) = 0.5"")) +     theme_bw()   df_positions <- data.table(position = meeting_positions)  p2 <- ggplot(df_positions, aes(x = position)) +     geom_density(fill = ""black"", color = ""black"") +     ggtitle(paste(""Distribution of Meeting Positions for Random Walks\nAverage Position ="", round(mean(meeting_positions), 2),                   ""\nNumber of Simulations = 1000"",                   ""\nStarting Points: X0 ="", x0, "", Y0 ="", y0,                   ""\nProbabilities: P(X step) = 0.5, P(Y step) = 0.5"")) +     theme_bw() The outputs of the simulation look like this: And just as an example, I showed an example of the trajectory taken by a given simulation until the intersection occurs: x0 <- -2 y0 <- 2  path_X <- integer() path_Y <- integer()  X <- x0 Y <- y0  t <- 0  while (X != Y && t < 100000) {      step_X <- sample(c(-1, 1), 1)     step_Y <- sample(c(-1, 1), 1)          X <- X + step_X     Y <- Y + step_Y           path_X <- c(path_X, X)     path_Y <- c(path_Y, Y)           t <- t + 1 }   df_paths <- data.frame(time = 1:length(path_X), X = path_X, Y = path_Y)  ggplot(df_paths, aes(x = time)) +     geom_line(aes(y = X, color = ""red"")) +     geom_line(aes(y = Y, color = ""blue"")) +     scale_color_manual(values = c(""red"" = ""red"", ""blue"" = ""blue"")) +     labs(color = ""Random Walk"", x = ""Time"", y = ""Position"") +     ggtitle(""Paths of Two Random Walks"") +     theme_bw() And here is the same plot for 1000 simulations: x0 <- -2 y0 <- 2   df_paths <- data.frame()   for (i in 1:1000) {     path_X <- integer()     path_Y <- integer()     print(i)         X <- x0     Y <- y0               t <- 0           while (X != Y && t < 100000) {                 step_X <- sample(c(-1, 1), 1)         step_Y <- sample(c(-1, 1), 1)                           X <- X + step_X         Y <- Y + step_Y                          path_X <- c(path_X, X)         path_Y <- c(path_Y, Y)                  t <- t + 1     }           df_paths_i <- data.frame(time = 1:length(path_X), X = path_X, Y = path_Y, simulation = rep(i, length(path_X)))             df_paths <- rbind(df_paths, df_paths_i) }  ggplot(df_paths, aes(x = time)) +     geom_line(aes(y = X, color = ""red""), alpha = 0.1) +     geom_line(aes(y = Y, color = ""blue""), alpha = 0.1) +     scale_color_manual(values = c(""red"" = ""red"", ""blue"" = ""blue"")) +     labs(color = ""Random Walk"", x = ""Time"", y = ""Position"") +     ggtitle(""Paths of Two Random Walks for 100 Simulations"") +     theme_bw() But going back to my question: Based on some set of initial conditions , and and their current positions, is it possible to derive probability distributions which describe the number of steps and the time required for intersection? Can a Joint Probability Distribution Function be derived? Thanks! Note: In the above distributions is a random variable representing the intersection time and are random variables representing the position of both Random Walks at the most recent time","X(t) t Y(t) t S_x(t) S_y(t) t 
X(t) = 
\begin{cases} 
x_0 & \text{if } t = 0 \\
X(t-1) + S_x(t) & \text{if } t > 0 
\end{cases}
 
Y(t) = 
\begin{cases} 
y_0 & \text{if } t = 0 \\
Y(t-1) + S_y(t) & \text{if } t > 0 
\end{cases}
  x_0, y_0, X(t), Y(t) \in \mathbb{Z} \quad \forall t  
S_x(t) = 
\begin{cases} 
+1 & \text{with probability } p= p_1 \\
-1 & \text{with probability } p= 1- p_1
\end{cases}
 
S_y(t) = 
\begin{cases} 
+1 & \text{with probability } p= p_2 \\
-1 & \text{with probability } p= 1- p_2
\end{cases}
 x_0 y_0 p_1 p_2  G : P(T=t | x_0, y_0, p_1, p_2, T-1 = t-1) \sim  ???   H: P(X(t) = Y(t) | x_0, y_0, p_1, p_2, X(t-1) = x(t-1), Y(t-1) = y(t-1) ) \sim  ???   \text{Joint Probability Distribution Function of Intersection Time and Intersection Position: } P(G,H) \sim  ???  T=t X(t-1) = x(t-1) Y(t-1) = y(t-1)","['probability', 'random-walk']"
13,"Constant C in Proposition 2.6.1 of Vershynin's ""High-Dimensional Probability"" : Sums of independent sub-gaussians","Constant C in Proposition 2.6.1 of Vershynin's ""High-Dimensional Probability"" : Sums of independent sub-gaussians",,"I'm searching the specific value of constant C in Proposition 2.6.1 regarding the sums of independent sub-Gaussian random variables, as presented in Vershynin's ""High-Dimensional Probability"" : https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf . Proposition 2.6.1 : Let $X_1, \ldots , X_n$ be independent mean-zero sub-gaussian random variables, then, $$\left\| \sum_{i=1}^n X_i \right\|_{\psi_2}^2 \leq C \times \sum_{i=1}^n \|X_i\|_{\psi_2}^2$$ Based on the definition of a sub-Gaussian random variable (proposition 2.5.2) from the same book (i wrote only proposition interesting for this discussion) : (i) The tails of ( X ) satisfy $\mathbb{P}(|X| \geq t) \leq 2 \exp\left(-\frac{t^2}{K_1^2}\right) \quad \text{for all} \ t > 0. $ (iv) The MGF of $ X^2 $ is bounded at some point, namely, $\mathbb{E}\exp\left(\frac{X^2}{K_4^2}\right) \leq 2. $ (v) The MGF of $ X $ satisfies $ \mathbb{E}\exp(\lambda X) \leq \exp(K_5^2 \lambda^2) \quad \text{for all} \ \lambda \in \mathbb{R}. $ So if we read the proof we get the relation $K_1=K_4$ and $K_1 = 2K_5$ so $K_4 = 2K_5 (1)$ A discussion about this proposition : Making the absolute constant in sub-gaussian characterization explicit And this is the definition of the sub-gaussian norm : $$||X||_{\psi_2} = \inf\{t > 0, \mathbb{E}(e^{-\frac{X^2}{t^2}}) \leq 2\}$$ That's to say, the smallest $K_4$ I try to prove this and find C : For all $\lambda \in \mathbb{R}$ , we have $$\begin{align*} \mathbb{E} \exp \left( \lambda \sum_{i=1}^n X_i \right) &= \prod_{i=1}^n \mathbb{E}( e^{\lambda X_i} ) \quad \text{(by independence)} \\ &\leq \prod_{i=1}^n e^{\lambda^2 \left(\frac{1}{2}\right)^2 \|X_i\|_{\psi_2}^2} \quad \text{(since $X_i$ is sub-Gaussian and we use relation (1), we determine $\frac{\|X_i\|_{\psi_2}}{2}$)} \\ &= e^{\lambda^2 K_5^2} \quad \text{where } K_5^2 = \frac{1}{4} \sum_{i=1}^n \|X_i\|_{\psi_2}^2. \end{align*} $$ So we get this, where the square of this sum yields the anticipated result. $$ \left\|\sum_{i=1}^n X_i\right\|_{\psi_2} \leq 2 K_5 $$ I would appreciate if anyone could provide insight on determining $C$ or correct my understanding if I am misinterpreting the proposition. Because I find $C=1$ and it doesn't work... Indeed, if we take $n$ identical sub gaussian $X$ we get $$ \left\| \sum_{i=1}^n X \right\|_{\psi_2}^2 \leq 1 * \times \sum_{i=1}^n \|X\|_{\psi_2}^2 $$ That's to say, $$n^2 ||X||_{\psi_2}^2 \leq n ||X||_{\psi_2}^2 $$ $$ n^2 \leq n $$ This counterexample doesn't work, I missed independance ! It seems the constant works.","I'm searching the specific value of constant C in Proposition 2.6.1 regarding the sums of independent sub-Gaussian random variables, as presented in Vershynin's ""High-Dimensional Probability"" : https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf . Proposition 2.6.1 : Let be independent mean-zero sub-gaussian random variables, then, Based on the definition of a sub-Gaussian random variable (proposition 2.5.2) from the same book (i wrote only proposition interesting for this discussion) : (i) The tails of ( X ) satisfy (iv) The MGF of is bounded at some point, namely, (v) The MGF of satisfies So if we read the proof we get the relation and so A discussion about this proposition : Making the absolute constant in sub-gaussian characterization explicit And this is the definition of the sub-gaussian norm : That's to say, the smallest I try to prove this and find C : For all , we have So we get this, where the square of this sum yields the anticipated result. I would appreciate if anyone could provide insight on determining or correct my understanding if I am misinterpreting the proposition. Because I find and it doesn't work... Indeed, if we take identical sub gaussian we get That's to say, This counterexample doesn't work, I missed independance ! It seems the constant works.","X_1, \ldots , X_n \left\| \sum_{i=1}^n X_i \right\|_{\psi_2}^2 \leq C \times \sum_{i=1}^n \|X_i\|_{\psi_2}^2 \mathbb{P}(|X| \geq t) \leq 2 \exp\left(-\frac{t^2}{K_1^2}\right) \quad \text{for all} \ t > 0.   X^2  \mathbb{E}\exp\left(\frac{X^2}{K_4^2}\right) \leq 2.   X   \mathbb{E}\exp(\lambda X) \leq \exp(K_5^2 \lambda^2) \quad \text{for all} \ \lambda \in \mathbb{R}.  K_1=K_4 K_1 = 2K_5 K_4 = 2K_5 (1) ||X||_{\psi_2} = \inf\{t > 0, \mathbb{E}(e^{-\frac{X^2}{t^2}}) \leq 2\} K_4 \lambda \in \mathbb{R} \begin{align*}
\mathbb{E} \exp \left( \lambda \sum_{i=1}^n X_i \right)
&= \prod_{i=1}^n \mathbb{E}( e^{\lambda X_i} ) \quad \text{(by independence)} \\
&\leq \prod_{i=1}^n e^{\lambda^2 \left(\frac{1}{2}\right)^2 \|X_i\|_{\psi_2}^2} \quad \text{(since X_i is sub-Gaussian and we use relation (1), we determine \frac{\|X_i\|_{\psi_2}}{2})} \\
&= e^{\lambda^2 K_5^2} \quad \text{where } K_5^2 = \frac{1}{4} \sum_{i=1}^n \|X_i\|_{\psi_2}^2.
\end{align*}
 
\left\|\sum_{i=1}^n X_i\right\|_{\psi_2} \leq 2 K_5
 C C=1 n X  \left\| \sum_{i=1}^n X \right\|_{\psi_2}^2 \leq 1 * \times \sum_{i=1}^n \|X\|_{\psi_2}^2  n^2 ||X||_{\psi_2}^2 \leq n ||X||_{\psi_2}^2   n^2 \leq n ","['probability', 'probability-theory', 'inequality', 'random-variables', 'moment-generating-functions']"
14,Expected value of fastest career hat-trick?,Expected value of fastest career hat-trick?,,"This is a sports variation of line segment (rope, stick...) divided into $n+1$ subsegments by $n$ uniformly distributed points. Assuming goals are uniformly distributed among player's career, what's the expected value of fastest hat-trick (3 goals) as measured by time between goals? (Real hat-tricks count only if they happen within single match, of course, but let's ignore this detail, it adds too much complexity to the problem). Here is the solution for fastest brace (2 goals): According to this answer expected value of shortest subsegment length: $$ \mathbb{E}[shortest\_period\_without\_goals] = \frac{career\_matches\ {\small(or\ career\_minutes)}}{(career\_goals+1)^2} $$ However, a brace can't be first or last subsegment, because there are no goals at the career beginning and end, so out of $n(n-1)$ permutations of $2$ shortest subsegments indices: The shortest subsegment is not first or last: $(n-2)(n-1)$ permutations; The shortest subsegment is first or last, but 2 nd shortest  is not: $2(n-2)$ permutations; The shortest and 2 nd shortest subsegments are first and last: $2$ permutations. After simplifying we finally get: $$ \mathbb{E}[fastest\_brace\_time] = \frac{career\_matches\ {\small(or\ career\_minutes)}}{career\_goals^2-1 } $$ But what about fastest hat-trick ? I mean, the line segment $\overline{A_0A_{n+1}}$ of length $1$ is divided to $n+1$ parts by $n$ uniformly distributed points: $\{{A_m|1\leq m \leq n, A_0\leq A_m\leq A_{m+1}\leq A_{n+1}}\}$ . Then we take $n-2$ subsegments (to avoid $A_0$ and $A_{n+1}$ which are not goals) with exactly $1$ point inside $\{{\overline{A_{m}A_{m+2}}|1\leq m \leq n-2}\}$ and choose the shortest. What will be an expected value of its length? Update : I wrote a small program that uses Monte Carlo methods to get some practical results. Probably they can help someone to find an analytic solution or/and verify it. Here are the expected values of $\{{\overline{A_{m}A_{m+2}}|0\leq m \leq n-1}\}$ lengths, sorted from shortest to longest: # Points $\mathbb{E}[1^{st}]$ $\mathbb{E}[2^{nd}]$ $\mathbb{E}[3^{rd}]$ $\mathbb{E}[4^{th}]$ $\mathbb{E}[5^{th}]$ $\mathbb{E}[6^{th}]$ $\mathbb{E}[7^{th}]$ 2 $\frac{3}{6}$ $\frac{5}{6}$ 3 $\frac{9}{32}$ $\frac{16}{32}$ $\frac{23}{32}$ 4 $\frac{34}{180}$ $\frac{55}{180}$ $\frac{85}{180}$ $\frac{114}{180}$ 5 $\frac{530}{3888}$ $\frac{850}{3888}$ $\frac{1185}{3888}$ $\frac{1708}{3888}$ $\frac{2207}{3888}$ 6 $\frac{1890}{18144}$ $\frac{2977}{18144}$ $\frac{4121}{18144}$ $\frac{5355}{18144}$ $\frac{7408}{18144}$ $\frac{9352}{18144}$ 7 $\frac{16937}{204800}$ $\frac{3309}{25600}$ $\frac{967759}{5529600}$ $\frac{19571}{86400}$ $\frac{1564889}{5529600}$ $\frac{52621}{138240}$ $\frac{11621}{24576}$ And here are the probabilistic estimation of expected values for shortest and longest segments (not all digits are verified): # Points $10$ $20$ $30$ $40$ $50$ $100$ $200$ $300$ $500$ $1000$ $\mathbb{E}[shortest]$ $\frac{1}{20.766}$ $\frac{1}{60.382}$ $\frac{1}{113.16}$ $\frac{1}{176.71}$ $\frac{1}{249.59}$ $\frac{1}{727.46}$ $\frac{1}{2107.8}$ $\frac{1}{3917.2}$ $\frac{1}{8529}$ $\frac{1}{24428}$ $\mathbb{E}[longest]$ $\small0.38163$ $\small0.23944$ $\small0.17803$ $\small0.1431$ $\small0.1203$ $\small0.0689$ $\small0.0387$ $\small0.0274$ $\small0.0176$ $\small0.0096$ It seems like $lim_{n\to\infty} \mathbb{E}[shortest]$ is close to $\mathcal{O}(1/n\sqrt{n})$ , and it makes sense: like in birthday problem , for most subsegment permutations at least 1 pair of $\mathcal{O}(\sqrt{n})$ shortest subsegments will be adjacent and the expected length of $\sqrt{n}^{th}$ shortest subsegment is also $\mathcal{O}(1/n\sqrt{n})$ . Note : those numbers can be used to answer the initial question only if they are multiplied by ${career\_matches\ {\small(or\ career\_minutes)}\ between\_first\_and\_last\_goals}$ .","This is a sports variation of line segment (rope, stick...) divided into subsegments by uniformly distributed points. Assuming goals are uniformly distributed among player's career, what's the expected value of fastest hat-trick (3 goals) as measured by time between goals? (Real hat-tricks count only if they happen within single match, of course, but let's ignore this detail, it adds too much complexity to the problem). Here is the solution for fastest brace (2 goals): According to this answer expected value of shortest subsegment length: However, a brace can't be first or last subsegment, because there are no goals at the career beginning and end, so out of permutations of shortest subsegments indices: The shortest subsegment is not first or last: permutations; The shortest subsegment is first or last, but 2 nd shortest  is not: permutations; The shortest and 2 nd shortest subsegments are first and last: permutations. After simplifying we finally get: But what about fastest hat-trick ? I mean, the line segment of length is divided to parts by uniformly distributed points: . Then we take subsegments (to avoid and which are not goals) with exactly point inside and choose the shortest. What will be an expected value of its length? Update : I wrote a small program that uses Monte Carlo methods to get some practical results. Probably they can help someone to find an analytic solution or/and verify it. Here are the expected values of lengths, sorted from shortest to longest: # Points 2 3 4 5 6 7 And here are the probabilistic estimation of expected values for shortest and longest segments (not all digits are verified): # Points It seems like is close to , and it makes sense: like in birthday problem , for most subsegment permutations at least 1 pair of shortest subsegments will be adjacent and the expected length of shortest subsegment is also . Note : those numbers can be used to answer the initial question only if they are multiplied by .","n+1 n 
\mathbb{E}[shortest\_period\_without\_goals] = \frac{career\_matches\ {\small(or\ career\_minutes)}}{(career\_goals+1)^2}
 n(n-1) 2 (n-2)(n-1) 2(n-2) 2 
\mathbb{E}[fastest\_brace\_time] = \frac{career\_matches\ {\small(or\ career\_minutes)}}{career\_goals^2-1 }
 \overline{A_0A_{n+1}} 1 n+1 n \{{A_m|1\leq m \leq n, A_0\leq A_m\leq A_{m+1}\leq A_{n+1}}\} n-2 A_0 A_{n+1} 1 \{{\overline{A_{m}A_{m+2}}|1\leq m \leq n-2}\} \{{\overline{A_{m}A_{m+2}}|0\leq m \leq n-1}\} \mathbb{E}[1^{st}] \mathbb{E}[2^{nd}] \mathbb{E}[3^{rd}] \mathbb{E}[4^{th}] \mathbb{E}[5^{th}] \mathbb{E}[6^{th}] \mathbb{E}[7^{th}] \frac{3}{6} \frac{5}{6} \frac{9}{32} \frac{16}{32} \frac{23}{32} \frac{34}{180} \frac{55}{180} \frac{85}{180} \frac{114}{180} \frac{530}{3888} \frac{850}{3888} \frac{1185}{3888} \frac{1708}{3888} \frac{2207}{3888} \frac{1890}{18144} \frac{2977}{18144} \frac{4121}{18144} \frac{5355}{18144} \frac{7408}{18144} \frac{9352}{18144} \frac{16937}{204800} \frac{3309}{25600} \frac{967759}{5529600} \frac{19571}{86400} \frac{1564889}{5529600} \frac{52621}{138240} \frac{11621}{24576} 10 20 30 40 50 100 200 300 500 1000 \mathbb{E}[shortest] \frac{1}{20.766} \frac{1}{60.382} \frac{1}{113.16} \frac{1}{176.71} \frac{1}{249.59} \frac{1}{727.46} \frac{1}{2107.8} \frac{1}{3917.2} \frac{1}{8529} \frac{1}{24428} \mathbb{E}[longest] \small0.38163 \small0.23944 \small0.17803 \small0.1431 \small0.1203 \small0.0689 \small0.0387 \small0.0274 \small0.0176 \small0.0096 lim_{n\to\infty} \mathbb{E}[shortest] \mathcal{O}(1/n\sqrt{n}) \mathcal{O}(\sqrt{n}) \sqrt{n}^{th} \mathcal{O}(1/n\sqrt{n}) {career\_matches\ {\small(or\ career\_minutes)}\ between\_first\_and\_last\_goals}","['probability', 'expected-value', 'uniform-distribution']"
15,A non-local equation on Laplace transforms.,A non-local equation on Laplace transforms.,,"I wish to find all probability distributions $\mu$ on $\mathbb R^+$ whose Laplace transform: $\varphi(z) = \int_{0}^{\infty} e^{-zx} \mu(dx), z \geqslant 0,$ solves the equation: $\varphi(z) = \beta \varphi(z) g(z) +  (1-\beta) \frac{\varphi(z+1)}{\varphi(1)},$ where $g$ again is the Laplace transform of some given distribution on $\mathbb R^+$ This equation characterizes the fixed point in distribution of some probabilistic model for the balance between selection and mutation. The difficulty comes from the non-local term (the fact that the equation mixes $z$ and $z+1$ ) All comments are welcome !",I wish to find all probability distributions on whose Laplace transform: solves the equation: where again is the Laplace transform of some given distribution on This equation characterizes the fixed point in distribution of some probabilistic model for the balance between selection and mutation. The difficulty comes from the non-local term (the fact that the equation mixes and ) All comments are welcome !,"\mu \mathbb R^+ \varphi(z) = \int_{0}^{\infty} e^{-zx} \mu(dx), z \geqslant 0, \varphi(z) = \beta \varphi(z) g(z) +  (1-\beta) \frac{\varphi(z+1)}{\varphi(1)}, g \mathbb R^+ z z+1","['probability', 'laplace-transform', 'functional-equations']"
16,Regular Variation and Maximal Moments,Regular Variation and Maximal Moments,,"Let $X$ be a non-negative random variable. We call $X$ regularly varying with tail index $\alpha>0$ if $$\lim_{u\to\infty}\frac{\mathbb P[X>ut]}{\mathbb P[X>u]}=t^{-\alpha}, \hspace{1cm}\forall t>0.$$ It is well-known, see e.g. Heavy-Tailed Time Series Proposition 1.4.6., that for such random variables we have $$\forall\beta<\alpha:\mathbb E|X|^\beta<\infty,\hspace{1cm}\forall\gamma>\alpha: \mathbb E|X|^\gamma=\infty,$$ hence we can view $\alpha$ as describing how heavy-tailed the distribution of $X$ is. Now, more generally for a random variable $X$ define $$\eta_X:=\sup\big\{\beta\geq 0: \mathbb E|X|^\beta<\infty\big\}.$$ From the basic result listed above we have for a regularly varying random variable $X$ with tail index $\alpha$ that $\eta_X=\alpha$ . I want to know whether it is possible to derive a result in the other direction: Are there any known conditions under which a random variable $X$ with $\eta_X\in(0,\infty)$ is regularly varying with index $\eta_X$ ? As a first remark, I doubt that $\eta_X$ being finite and positive is sufficient for $X$ to be regularly varying, since by Karamata's Characterization Theorem we can write any regularly varying tail function as $$(*)\hspace{1cm}\mathbb P[X>t]=t^{-\alpha}\cdot \ell(t)$$ for a slowly varying function $\ell$ . Now $\eta_X\in(0,\infty)$ only tells us that $$\int_0^\infty \mathbb P[X>t^{1/\beta}]dt<\infty, \hspace{1cm}\forall \beta<\eta_X.$$ However, i doubt that from the finiteness of these integrals alone it is possible to derive a representation as in $(*)$ .","Let be a non-negative random variable. We call regularly varying with tail index if It is well-known, see e.g. Heavy-Tailed Time Series Proposition 1.4.6., that for such random variables we have hence we can view as describing how heavy-tailed the distribution of is. Now, more generally for a random variable define From the basic result listed above we have for a regularly varying random variable with tail index that . I want to know whether it is possible to derive a result in the other direction: Are there any known conditions under which a random variable with is regularly varying with index ? As a first remark, I doubt that being finite and positive is sufficient for to be regularly varying, since by Karamata's Characterization Theorem we can write any regularly varying tail function as for a slowly varying function . Now only tells us that However, i doubt that from the finiteness of these integrals alone it is possible to derive a representation as in .","X X \alpha>0 \lim_{u\to\infty}\frac{\mathbb P[X>ut]}{\mathbb P[X>u]}=t^{-\alpha}, \hspace{1cm}\forall t>0. \forall\beta<\alpha:\mathbb E|X|^\beta<\infty,\hspace{1cm}\forall\gamma>\alpha: \mathbb E|X|^\gamma=\infty, \alpha X X \eta_X:=\sup\big\{\beta\geq 0: \mathbb E|X|^\beta<\infty\big\}. X \alpha \eta_X=\alpha X \eta_X\in(0,\infty) \eta_X \eta_X X (*)\hspace{1cm}\mathbb P[X>t]=t^{-\alpha}\cdot \ell(t) \ell \eta_X\in(0,\infty) \int_0^\infty \mathbb P[X>t^{1/\beta}]dt<\infty, \hspace{1cm}\forall \beta<\eta_X. (*)","['probability', 'probability-distributions', 'expected-value', 'distribution-tails']"
17,How to Create a Mathematical Model of an Elevator?,How to Create a Mathematical Model of an Elevator?,,"This is a problem I have been thinking about for a while. Everyday, I take an elevator and observe that people get on and off. Given the current situation (i.e. what floor I am on and how many people are currently in the elevator) - I find myself always trying to guess: When the elevator will reach my destination? What floor will the elevator stop on next? How many people will enter and exit the elevator on the next stop? While I wait in the elevator, I always think: Suppose if we had an entire elevator log that detailed every elevator trip that was made over a year , and this included information how many people got on/off at each floor: How could we create a mathematical model to represent this system and answer the above questions? To begin, let's state some of the following conditions: The elevator has a maximum capacity of $n_k$ people : once $n_k$ people have entered the elevator, no one outside the elevator can request the elevator to stop. The elevator can only stop when someone inside requests the elevator to stop or once the elevator has reached the final floor. When the elevator starts, there are currently $n_0$ people on the elevator (where $n_0$ is some positive integer) There are $m$ floors in total On the $j^{th}$ floor (where $ j\leq m$ ) , the number of people that enter can be denoted by $x_j$ and the number of people that exit can be denoted by $y_j$ The immediate future of the elevator is only decided by the current situation of the elevator (i.e. what floor we are on and how many people are in the elevator) - i.e. the elevator has the memoryless property The time of day does is not being taken into consideration and is being considered as irrelevant in this problem (i.e. peak hours like lunch break, end of day, etc. are not important). The first thing that comes to mind is to use (Discrete Time) Markov Chains. As such, I think the elevator can be thought of to contain two underlying Markov Chains: Markov Chain for Floor Transitions: A Markov Chain can be created which will look at the probability of the elevator stopping at any other floor given the current floor Markov Chain for Elevator Capacity: A Markov Chain (similar to a queue/birth-death process) can be created which will look at the net change in capacity given the current capacity My Question: Given access to the elevator log - I know how to create two independent Markov Chains to model floor transitions and elevator capacity independently. Using conditional probabilities, I could analyze the number of times the elevator was on the $i^{th}$ floor - and of those times, how many times was the next stop made at the $i^{th}$ floor: this information would serve as an estimate of $p_{i,j}$ , the probability of directly transitioning from the $i^{th}$ floor to the $j^{th}$ floor (note that $p_{j,i}$ does not need to be equal to $p_{i,j}$ ). Using conditional probabilities, I could also analyze the probability of the change in capacity. If the elevator current has 5 people - I could calculate what is the probability that after the next stop, there will now be 9 people. But it is unclear to me how to ""jointly"" model both of these Markov Chains together. For example, if the elevator reaches capacity at the 3rd last floor - it might be very likely that no one will request a stop until the final stop. However, if the elevator reaches maximum capacity early on - it might be more likely that someone on the elevator will have to go to an intermediate floor and give the elevator another chance to start accepting more passengers and take on more possible trajectories. I thought perhaps I could combine both of these chains together into a single chain - but the number of states in this single chain would explode in size , for example: State 1: 1st Floor, 0 People State 2: 1st Floor, 1 Person State 3: 1st Floor, 2 people State 4: 1st Floor, 3 people ... State 1-k: 1st Floor, $n_k$ people ... State k-m: $n_k$ people, $m^{th}$ floor Apart from resulting in a very large number states and make all computations difficult - more concerning would be the fact that I might not have enough data (i.e. joint combinations of floor transitions at varying capacity levels) to reliably estimate all state probabilities Can someone please advise me on how I can use Markov Chains to jointly model both of these processes (i.e. floor transitions and capacity)? Thanks!","This is a problem I have been thinking about for a while. Everyday, I take an elevator and observe that people get on and off. Given the current situation (i.e. what floor I am on and how many people are currently in the elevator) - I find myself always trying to guess: When the elevator will reach my destination? What floor will the elevator stop on next? How many people will enter and exit the elevator on the next stop? While I wait in the elevator, I always think: Suppose if we had an entire elevator log that detailed every elevator trip that was made over a year , and this included information how many people got on/off at each floor: How could we create a mathematical model to represent this system and answer the above questions? To begin, let's state some of the following conditions: The elevator has a maximum capacity of people : once people have entered the elevator, no one outside the elevator can request the elevator to stop. The elevator can only stop when someone inside requests the elevator to stop or once the elevator has reached the final floor. When the elevator starts, there are currently people on the elevator (where is some positive integer) There are floors in total On the floor (where ) , the number of people that enter can be denoted by and the number of people that exit can be denoted by The immediate future of the elevator is only decided by the current situation of the elevator (i.e. what floor we are on and how many people are in the elevator) - i.e. the elevator has the memoryless property The time of day does is not being taken into consideration and is being considered as irrelevant in this problem (i.e. peak hours like lunch break, end of day, etc. are not important). The first thing that comes to mind is to use (Discrete Time) Markov Chains. As such, I think the elevator can be thought of to contain two underlying Markov Chains: Markov Chain for Floor Transitions: A Markov Chain can be created which will look at the probability of the elevator stopping at any other floor given the current floor Markov Chain for Elevator Capacity: A Markov Chain (similar to a queue/birth-death process) can be created which will look at the net change in capacity given the current capacity My Question: Given access to the elevator log - I know how to create two independent Markov Chains to model floor transitions and elevator capacity independently. Using conditional probabilities, I could analyze the number of times the elevator was on the floor - and of those times, how many times was the next stop made at the floor: this information would serve as an estimate of , the probability of directly transitioning from the floor to the floor (note that does not need to be equal to ). Using conditional probabilities, I could also analyze the probability of the change in capacity. If the elevator current has 5 people - I could calculate what is the probability that after the next stop, there will now be 9 people. But it is unclear to me how to ""jointly"" model both of these Markov Chains together. For example, if the elevator reaches capacity at the 3rd last floor - it might be very likely that no one will request a stop until the final stop. However, if the elevator reaches maximum capacity early on - it might be more likely that someone on the elevator will have to go to an intermediate floor and give the elevator another chance to start accepting more passengers and take on more possible trajectories. I thought perhaps I could combine both of these chains together into a single chain - but the number of states in this single chain would explode in size , for example: State 1: 1st Floor, 0 People State 2: 1st Floor, 1 Person State 3: 1st Floor, 2 people State 4: 1st Floor, 3 people ... State 1-k: 1st Floor, people ... State k-m: people, floor Apart from resulting in a very large number states and make all computations difficult - more concerning would be the fact that I might not have enough data (i.e. joint combinations of floor transitions at varying capacity levels) to reliably estimate all state probabilities Can someone please advise me on how I can use Markov Chains to jointly model both of these processes (i.e. floor transitions and capacity)? Thanks!","n_k n_k n_0 n_0 m j^{th}  j\leq m x_j y_j i^{th} i^{th} p_{i,j} i^{th} j^{th} p_{j,i} p_{i,j} n_k n_k m^{th}","['probability', 'stochastic-processes']"
18,Intuition behind the connection between Entropy and Dirichlet Form,Intuition behind the connection between Entropy and Dirichlet Form,,"Consider some Markov process $X$ on a finite state space $E$ admitting a reversible measure $\mu$ . The following two quantities are extremely useful in its study: The relative entropy $$ H(\nu_t|\mu) = H(f_t) = -\mathbb{E}_\mu[f_t\log f_t]\geq 0, $$ where we assume for simplicity that the distribution $\nu_t$ of $X_t$ has density $f_t$ w.r.t. $\mu$ . The Dirichlet Form $$ D(f) := \langle f, (-L)f\rangle_\mu \geq 0, $$ where $L$ denotes the generator of the Markov process and $\langle\cdot,\cdot\rangle_\mu$ denotes the $L^2(\mu)$ -inner product. It is relatively easy to show (see e.g. Appendix 1, Theorem 9.2, of Scaling Limits of Interacting Particle Systems by Kipnis and Landim) that $$ \partial_t H(f_t) \leq -2D(\sqrt{f_t}) $$ which constitutes a very important statement as it allows us to estimate the rate of convergence of $X_t$ to its stationary state. The problem that I have is that I cannot find any intuitive reason for this relationship to hold. The proof itself does not help as it uses a simple but uninformative inequality to prove it. I am wondering if there is a heuristic why this should hold true or if I am too optimistic...","Consider some Markov process on a finite state space admitting a reversible measure . The following two quantities are extremely useful in its study: The relative entropy where we assume for simplicity that the distribution of has density w.r.t. . The Dirichlet Form where denotes the generator of the Markov process and denotes the -inner product. It is relatively easy to show (see e.g. Appendix 1, Theorem 9.2, of Scaling Limits of Interacting Particle Systems by Kipnis and Landim) that which constitutes a very important statement as it allows us to estimate the rate of convergence of to its stationary state. The problem that I have is that I cannot find any intuitive reason for this relationship to hold. The proof itself does not help as it uses a simple but uninformative inequality to prove it. I am wondering if there is a heuristic why this should hold true or if I am too optimistic...","X E \mu 
H(\nu_t|\mu) = H(f_t) = -\mathbb{E}_\mu[f_t\log f_t]\geq 0,
 \nu_t X_t f_t \mu 
D(f) := \langle f, (-L)f\rangle_\mu \geq 0,
 L \langle\cdot,\cdot\rangle_\mu L^2(\mu) 
\partial_t H(f_t) \leq -2D(\sqrt{f_t})
 X_t","['probability', 'stochastic-processes', 'soft-question', 'markov-process', 'entropy']"
19,How to lower bound $\tau$ based on the expression of $H$?,How to lower bound  based on the expression of ?,\tau H,"Let $A=\{a_{ij}\}_{1\le i,j\le n}$ be an $n$ by $n$ normalized symmetric Gaussian random matrix with $E[a_{ij}]=0$ and $E[a_{ij}^2]=1/n$ . Ordering its eigenvalues by $\lambda_1\le \lambda_2\le \cdots \lambda_n$ with corresponding eigenvectors $v_1,\dots, v_n \in \mathbb{R}^n$ . Let $u_0$ be a vector on $\mathbb{R}^n$ uniformly distributed on the unit sphere. (We also know that $v_i$ is uniformly distributed on the unit sphere for $i=1,\dots, n$ .) Define $H_j(t)=u_t\cdot v_j$ for $j=1,\dots, n$ and time $t\ge 0$ , solving the following ODE with initial value $H_j(0)$ : $$ \frac{1}{2}H_j'(t)=\sum_{i=1}^n[(\lambda_i-\lambda_j)H_i^2(t)]H_j(t) $$","Let be an by normalized symmetric Gaussian random matrix with and . Ordering its eigenvalues by with corresponding eigenvectors . Let be a vector on uniformly distributed on the unit sphere. (We also know that is uniformly distributed on the unit sphere for .) Define for and time , solving the following ODE with initial value :","A=\{a_{ij}\}_{1\le i,j\le n} n n E[a_{ij}]=0 E[a_{ij}^2]=1/n \lambda_1\le \lambda_2\le \cdots \lambda_n v_1,\dots, v_n \in \mathbb{R}^n u_0 \mathbb{R}^n v_i i=1,\dots, n H_j(t)=u_t\cdot v_j j=1,\dots, n t\ge 0 H_j(0) 
\frac{1}{2}H_j'(t)=\sum_{i=1}^n[(\lambda_i-\lambda_j)H_i^2(t)]H_j(t)
","['probability', 'analysis', 'statistics', 'inequality']"
20,Probability that a random element of a group has order $2$ [closed],Probability that a random element of a group has order  [closed],2,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 1 year ago . Improve this question If $G$ is a finite group, let $\tau(G)$ be equal to probability that a random element $x \in G$ satisfies $x^2=1$ , ie. $\tau(G) = \frac{|\{x \in G :\ x^2=1\}|}{|G|}$ . Can we say something about $\tau$ ? My intuition about the range of possible results we could possibly obtain here comes from another problem: taking two elements of $G$ at random, let $c(G)$ be the probability that they commute. It is quite widely known that if $c(G) > \frac{5}{8}$ , then $G$ is abelian and $c(G)=1$ . More recently it was proven that the set of possible values of $c(\cdot)$ is nowhere dense, has no irrational limit points, and is well-ordered by $>$ . We can state more or less the same problems in our case: a) Is there a constant $c<1$ such that $\tau(G)>c$ implies $\tau(G)=1$ ? b) Is the set of values of $\tau$ nowhere dense? Does it have any infinite increasing sequence (okay, this is essentially the same as well-order)? c) Is the set of values of $\tau$ closed? Parts b, c) look like they might be hard, and I don't expect anyone to write a serious paper in answer to this post, so two more questions, less specific and hopefully easier to answer. d) Are there any sources of examples of groups (or families of groups) with high value of $\tau$ ? Of course $\tau(G)=1$ if and only if $G \simeq \mathbb{Z}_2^k$ . We have $\tau(G \times H) = \tau(G) \cdot \tau(H)$ , and $\tau(\mathbb{Z}_n) = \frac{1}{n}$ for odd $n$ , $\frac{2}{n}$ for even; in particular for abelian groups other than $\mathbb{Z}_2^k$ we have $\tau(G) \leqslant \frac{1}{2}$ . We get better efficiency with dihedral groups: we always have $\tau(D_n) > \frac{1}{2}$ , although half is the limit with $n \to \infty$ . Groups such as $\operatorname{Aut}(D_8)$ also give a half. Therefore it seems like a good bound for a conjecture of the following kind: e) For every $\varepsilon > 0$ there is an integer $N$ , such that $\tau(G) \geqslant \frac{1}{2} + \varepsilon$ and $|G| \geqslant N$ imply $G \simeq H \times \mathbb{Z}_2$ for some group $H$ . (As $-\times \mathbb{Z}_2$ does not change $\tau$ , this means we have essentially finitely many examples of $G$ giving values bigger than $1/2 + \varepsilon$ , and in particular there are finitely many such values.)","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 1 year ago . Improve this question If is a finite group, let be equal to probability that a random element satisfies , ie. . Can we say something about ? My intuition about the range of possible results we could possibly obtain here comes from another problem: taking two elements of at random, let be the probability that they commute. It is quite widely known that if , then is abelian and . More recently it was proven that the set of possible values of is nowhere dense, has no irrational limit points, and is well-ordered by . We can state more or less the same problems in our case: a) Is there a constant such that implies ? b) Is the set of values of nowhere dense? Does it have any infinite increasing sequence (okay, this is essentially the same as well-order)? c) Is the set of values of closed? Parts b, c) look like they might be hard, and I don't expect anyone to write a serious paper in answer to this post, so two more questions, less specific and hopefully easier to answer. d) Are there any sources of examples of groups (or families of groups) with high value of ? Of course if and only if . We have , and for odd , for even; in particular for abelian groups other than we have . We get better efficiency with dihedral groups: we always have , although half is the limit with . Groups such as also give a half. Therefore it seems like a good bound for a conjecture of the following kind: e) For every there is an integer , such that and imply for some group . (As does not change , this means we have essentially finitely many examples of giving values bigger than , and in particular there are finitely many such values.)",G \tau(G) x \in G x^2=1 \tau(G) = \frac{|\{x \in G :\ x^2=1\}|}{|G|} \tau G c(G) c(G) > \frac{5}{8} G c(G)=1 c(\cdot) > c<1 \tau(G)>c \tau(G)=1 \tau \tau \tau \tau(G)=1 G \simeq \mathbb{Z}_2^k \tau(G \times H) = \tau(G) \cdot \tau(H) \tau(\mathbb{Z}_n) = \frac{1}{n} n \frac{2}{n} \mathbb{Z}_2^k \tau(G) \leqslant \frac{1}{2} \tau(D_n) > \frac{1}{2} n \to \infty \operatorname{Aut}(D_8) \varepsilon > 0 N \tau(G) \geqslant \frac{1}{2} + \varepsilon |G| \geqslant N G \simeq H \times \mathbb{Z}_2 H -\times \mathbb{Z}_2 \tau G 1/2 + \varepsilon,"['probability', 'group-theory', 'combinatorial-group-theory']"
21,What is the probability distribution for manhattan distance on a finite grid?,What is the probability distribution for manhattan distance on a finite grid?,,"Suppose I have a finite grid of size NxN. (Example is in two dimensions). I want to know whether there exists a closed-form formula (or approximation) for the probability distribution of the Manhattan distance between two randomly selected points, using the manhattan metric $$||a,b||_M = |x_a - x_b| + |y_a - y_b|$$ . So given two random points $a$ and $b$ , what is $P(||a,b||_M = d)$ for any value $0 \leq d < 2N$ ? By using a PRNG, and randomly trying out a billion points or so, the distribution looks like a parabola with a long tail. This is with $N = 256$ : It's possible to manually enumerate the points... (given that $N > 3$ ). $$d=0: \frac{N^2}{N^4} = \frac{1}{N^2}$$ $$d=2N-1: \frac{2}{N^4}$$ $$d=2N-2: \frac{8}{N^4}$$ $$d=2N-3: \frac{12 + 8}{N^4}$$ For $d=2N-1$ The points are in opposite corners. For $d=2N-2$ observe that one point has to be in a corner, and the other point then has two possible locations. For $d=2N-3$ there's a case with two pairs of points in opposite corners (four options, two symmetries), and one similar to previous case but with three options (three options, four symmetries). This quickly balloons in possibilities though. I wonder if there's a better way of approaching this.","Suppose I have a finite grid of size NxN. (Example is in two dimensions). I want to know whether there exists a closed-form formula (or approximation) for the probability distribution of the Manhattan distance between two randomly selected points, using the manhattan metric . So given two random points and , what is for any value ? By using a PRNG, and randomly trying out a billion points or so, the distribution looks like a parabola with a long tail. This is with : It's possible to manually enumerate the points... (given that ). For The points are in opposite corners. For observe that one point has to be in a corner, and the other point then has two possible locations. For there's a case with two pairs of points in opposite corners (four options, two symmetries), and one similar to previous case but with three options (three options, four symmetries). This quickly balloons in possibilities though. I wonder if there's a better way of approaching this.","||a,b||_M = |x_a - x_b| + |y_a - y_b| a b P(||a,b||_M = d) 0 \leq d < 2N N = 256 N > 3 d=0: \frac{N^2}{N^4} = \frac{1}{N^2} d=2N-1: \frac{2}{N^4} d=2N-2: \frac{8}{N^4} d=2N-3: \frac{12 + 8}{N^4} d=2N-1 d=2N-2 d=2N-3","['probability', 'geometry']"
22,Why doesn't the Borel-Kolmogorov paradox cause problems in practice?,Why doesn't the Borel-Kolmogorov paradox cause problems in practice?,,"The Borel-Kolmogorov paradox shows that the usual formula for conditional density $f_{X|Y}(x|y) = f_{X,Y}(x, y)/f_Y(y)$ can lead to inconsistent results depending on the coordinate system that is used to describe the problem [1]. The paradox stems from conditioning on zero probability events being undefined. One explanation comes from considering this conditioning operation, as a limit of distributions that are conditioned on events with non-zero probability. Different limits lead to different conditional densities. This leaves me with two questions that worry me: When simply applying the ""ratio of densities"" formula, it's not clear which limit is implied. So two people solving the same problem in different coordinate frames can do the calculation without needing to think about when they can be expected to end up with the same result. Is there a procedure that makes it clear when two results will end up being different? How often does this cause a problem in practice? In statistics, we condition on real-valued outcomes all the time! Why do seemingly irrelevant details like the parameterisation not end up causing problems more often? Or is this universally used formula in fact on incredibly shaky ground? To what extent is statistics or Bayesian inference parameterisation independent? In short, to what extent does this paradox matter in practice? From my current (limited) understanding, there is an answer to Question 1. Namely, if you follow the measure-theoretic definition of conditional probability, the choice of $\sigma$ -algebra influences the answer. This at least makes it clear where a choice appears, in contrast to following the ratio of densities rule. [1] See YouTube video or chapter 15 of Jaynes.","The Borel-Kolmogorov paradox shows that the usual formula for conditional density can lead to inconsistent results depending on the coordinate system that is used to describe the problem [1]. The paradox stems from conditioning on zero probability events being undefined. One explanation comes from considering this conditioning operation, as a limit of distributions that are conditioned on events with non-zero probability. Different limits lead to different conditional densities. This leaves me with two questions that worry me: When simply applying the ""ratio of densities"" formula, it's not clear which limit is implied. So two people solving the same problem in different coordinate frames can do the calculation without needing to think about when they can be expected to end up with the same result. Is there a procedure that makes it clear when two results will end up being different? How often does this cause a problem in practice? In statistics, we condition on real-valued outcomes all the time! Why do seemingly irrelevant details like the parameterisation not end up causing problems more often? Or is this universally used formula in fact on incredibly shaky ground? To what extent is statistics or Bayesian inference parameterisation independent? In short, to what extent does this paradox matter in practice? From my current (limited) understanding, there is an answer to Question 1. Namely, if you follow the measure-theoretic definition of conditional probability, the choice of -algebra influences the answer. This at least makes it clear where a choice appears, in contrast to following the ratio of densities rule. [1] See YouTube video or chapter 15 of Jaynes.","f_{X|Y}(x|y) = f_{X,Y}(x, y)/f_Y(y) \sigma","['probability', 'statistics']"
23,Conditions for $P(X_1 + \cdots X_{n+1} = 0) < P(X_1 + \cdots X_n = 0) $ to hold for all $n$?,Conditions for  to hold for all ?,P(X_1 + \cdots X_{n+1} = 0) < P(X_1 + \cdots X_n = 0)  n,"Let $X_i$ be iid random discrete variables with pmf $f$ . We may restrict ourselve to pmfs with finite even support: $f \in G_N$ ( $ f[k] > 0 \implies |k| \le N$ ) or perhaps $f \in G^{+}_N$ ( $ f[k] > 0 \iff |k| \le N$ ) . I wonder if there is some characterization on $f$ (perhaps sufficient or necesary conditions) for this property to hold: $$P(X_1 + \cdots X_{n+1} = 0)  < P(X_1 + \cdots X_n = 0)  \, \forall n\ge 1$$ Alternatively, letting $f^{(n)}$ denote the $n-$ self convolution of $f$ , the above is equivalent to $f^{(n+1)}[0]  < f^{(n)}[0]  $ In by this question it's shown that $f \in G^{+}_1$ is not enough, but $g=f^{(2)}$ is.","Let be iid random discrete variables with pmf . We may restrict ourselve to pmfs with finite even support: ( ) or perhaps ( ) . I wonder if there is some characterization on (perhaps sufficient or necesary conditions) for this property to hold: Alternatively, letting denote the self convolution of , the above is equivalent to In by this question it's shown that is not enough, but is.","X_i f f \in G_N  f[k] > 0 \implies |k| \le N f \in G^{+}_N  f[k] > 0 \iff |k| \le N f P(X_1 + \cdots X_{n+1} = 0)  < P(X_1 + \cdots X_n = 0)  \, \forall n\ge 1 f^{(n)} n- f f^{(n+1)}[0]  < f^{(n)}[0]   f \in G^{+}_1 g=f^{(2)}","['probability', 'convolution']"
24,Expectation of a multivariate Gaussian after going through a Softmax,Expectation of a multivariate Gaussian after going through a Softmax,,"Let $\varepsilon\sim N(0, I_D)$ be a $D$ -dimensional random vector, distributed normally with mean $0$ and covariance given by the identity matrix of size $D$ . In some computations I'm doing in my research, the following expectation arises: $$\mathbb{E}_{\varepsilon\sim N(0, I_D)}\left[\frac{\exp(\varepsilon_i b_i)}{\sum_{k=1}^D \exp(a_k + \varepsilon_kb_k) )}\right],$$ where $a, b\in\mathbb{R}^D$ are some $D$ -dimensional vectors. I wonder if this expectation has a closed form . It looks gnarly, though. The fact that passing a Gaussian through a sigmoid also has to be approximated gives me even less hope.","Let be a -dimensional random vector, distributed normally with mean and covariance given by the identity matrix of size . In some computations I'm doing in my research, the following expectation arises: where are some -dimensional vectors. I wonder if this expectation has a closed form . It looks gnarly, though. The fact that passing a Gaussian through a sigmoid also has to be approximated gives me even less hope.","\varepsilon\sim N(0, I_D) D 0 D \mathbb{E}_{\varepsilon\sim N(0, I_D)}\left[\frac{\exp(\varepsilon_i b_i)}{\sum_{k=1}^D \exp(a_k + \varepsilon_kb_k) )}\right], a, b\in\mathbb{R}^D D","['probability', 'expected-value']"
25,Is there a pattern to the coefficients in the piecewise equations of the Irwin–Hall distributions?,Is there a pattern to the coefficients in the piecewise equations of the Irwin–Hall distributions?,,"Intro and Problem Statement The Irwin–Hall distribution is a probability distribution of the sum of $n$ independent, uniformly-distributed, continuous random variables in the interval $[0, 1]$ . The distribution is a piecewise polynomial function composed of $n$ sections of degree $(n − 1)$ which, combined, cover the interval $[0, n]$ . The zeroth piecewise segment is always of the form $\frac{x^{n − 1}}{(n − 1)!}$ . The last segment is always of the form $\frac{(−x + n)^{n − 1}}{(n − 1)!}$ . The segments in between are more complicated, and are where I'm running into trouble. I want to find a pattern that allows me to generate the piecewise equation for an arbitrary segment in the distribution of an arbitrary number of variables. The Basics The probability distribution of a single variable looks like a horizontal line (degree zero). The equation of the probability distribution is: $$f_X(x; 1) = \begin{cases} 1 & : 0 ≤ x ≤ 1 \\ 0 & : \text{otherwise} \end{cases}$$ If we add a second identical variable to the first, their combined probability distribution runs from 0 to 2, composed of two diagonal lines (degree one). The equation of the combined probability distribution of two variables is: $$f_X(x; 2) = \begin{cases} x & : 0 ≤ x ≤ 1 \\ −x + 2 & : 1 < x ≤ 2 \\ 0 & : \text{otherwise} \end{cases}$$ If we add a third variable, the combined probability distribution runs from 0 to 3, composed of three parabolic arcs (degree two). The equation of the combined probability distribution of three variables is: $$f_X(x; 3) = \begin{cases} \frac{x²}{2} & : 0 ≤ x ≤ 1 \\ \frac{−2 x² + 6x − 3}{2} & : 1 < x ≤ 2 \\ \frac{(−x + 3)²}{2} & :  2 < x ≤ 3 \\ 0 & : \text{otherwise} \end{cases}$$ As more and more variables are added (as $n$ gets large), their combined Irwin-Hall distribution approaches a normal distribution with mean $μ = \frac{n}{2}$ and variance $σ² = \frac{n}{12}$ . How I'm Building Up the Equations The $(n − 1)$ th derivative of the $k$ th segment in the distribution, skipping the zeroth segment whose $(n − 1)$ th derivative is always 1, is of the form $\frac{(−1)^k}{k!} (n − 1) (n − 2) (n − 3) \, ... (n − k)$ , or equivalently $\frac{(−1)^k (n − 1)!}{k! (n − 1 − k)!}$ . For example, in the case of four variables, the 3rd derivatives of the four segments are: $\frac{ 1}{0!} = 1$ , $\frac{−1}{1!} (4 − 1) = −3$ , $\frac{ 1}{2!} (4 − 1) (4 − 2) = 3$ , and $\frac{−1}{3!} (4 − 1) (4 − 2) (4 − 3) = −1$ . In the case of five variables, the 4th derivatives of the five segments are: $\frac{ 1}{0!} = 1$ , $\frac{−1}{1!} (5 − 1) = −4$ , $\frac{ 1}{2!} (5 − 1) (5 − 2) = 6$ , $\frac{−1}{3!} (5 − 1) (5 − 2) (5 − 3) = −4$ , and $\frac{ 1}{4!} (5 − 1) (5 − 2) (5 − 3) (5 − 4) = 1$ . These numbers correspond to the $(n − 1)$ th row of Pascal's triangle, with every other number negative. That is, for $n = 4$ variables, the 3rd derivatives of the four segments are the four terms in the 3rd row of the triangle, with odd positions negated. To build up the full $n$ -segment distribution curve, we take the integrals of these base numbers $(n − 1)$ times, adding a new constant term each time. The constant term for a given integration step depends on the number $m$ of integrations, including the current step (starting at one). The constant we add to the zeroth segment is always zero. The constant we add to the first segment is $\frac{(−1)^{m − 1}}{0! × m!} (n)$ . The constant we add to the second segment is $\frac{(−1)^{m}}{1! × m!} (2^{m − 1} n² − (1 + 2^{m − 1}) n)$ . The constant we add to the third segment is $\frac{(−1)^{m − 1}}{2! × m!} (3^{m − 1} n³ − (2 × 2^{m − 1} +  3 × 3^{m − 1}) n² + 2! (1 + 2^{m − 1} + 3^{m − 1}) n)$ . The constant we add to the fourth segment is $\frac{(−1)^{m}}{3! × m!} (4^{m − 1}n⁴ - (3 × 3^{m − 1} +  6 × 4^{m − 1}) n³ + (6 × 2^{m − 1} + 9 × 3^{m − 1} + 11 × 4^{m − 1}) n² − 3! (1^{m − 1} + 2^{m − 1} + 3^{m − 1} + 4^{m − 1}) n)$ These are the expressions I want to figure out how to create more of. As an example, the equation for the third segment ( $k = 3$ ) after three integrations is: $$\int \left( \int \left( \int \left( \frac{−1}{3!} (n − 1) (n − 2) (n − 3) \right)  + \frac{n³ − 5n² + 6n}{2! × 1!} \right) − \frac{3n³ − 13n² + 12n}{2! × 2!} \right) + \frac{9n³ − 35n² + 28n}{2! × 3!}$$ In the case of four variables, that simplifies to: $$\int \left( \int \left( \int \left( −1 \right)  + 4 \right) − 8 \right) + \frac{32}{3}$$ Which expands into: $$\frac{−x³}{6} + 2x² − 8x + \frac{32}{3}$$ Which can be re-written as: $$\frac{−x³ + 12x² − 48x + 64}{3!}$$ Which factors into: $$\frac{(−x + 4)³}{3!}$$ The whole process is complicated, but not difficult. The Stumbling Block The problem is, I can't figure out the patterns for creating constants beyond the fourth segment. For the first four segments, I have the patterns for their constant terms and can generate equations for the probability distributions of any number of variables. Using WolframAlpha, I've worked out the equations for generating the constants out to eight segments for values of $n$ up to 6, but by the seventh or eighth segment and variable it starts running out of computation time and won't give an answer. And using only the eight equations I am able to get, I have been unable to find the patterns for all the coefficients of those equations. To recap, here are the equations for the terms that I have figured out: $0$ $\frac{(−1)^{m − 1}}{0! × m!} (n)$ $\frac{(−1)^{m}}{1! × m!} (2^{m − 1} n² − (1 + 2^{m − 1}) n)$ $\frac{(−1)^{m − 1}}{2! × m!} (3^{m − 1} n³ − (2 × 2^{m − 1} +  3 × 3^{m − 1}) n² + (1 + 2^{m − 1} + 3^{m − 1}) n)$ $\frac{(−1)^{m}}{3! × m!} (4^{m − 1}n⁴ - (3 × 3^{m − 1} +  6 × 4^{m − 1}) n³ + (6 × 2^{m − 1} + 9 × 3^{m − 1} + 11 × 4^{m − 1}) n² − 3! (1^{m − 1} + 2^{m − 1} + 3^{m − 1} + 4^{m − 1}) n)$ Beyond four, though, I'm stuck. I feel like there should be a pattern to these equations that I can use to generate the equations for the constants for an arbitrary segment, but if there is one, I can't find it. Some partial patterns do stand out to me: The expressions are polynomials in terms of $n$ with coefficients composed of sums of numbers raised to the power of $(m − 1)$ The $k$ th expression is $k$ terms long, with highest degree $k$ and containing terms of all lower degrees down to and including 1, with zero constant term The first term in the $k$ th expression is $k^{m − 1} × n^k$ The last term in the $k$ th expression is of the form $(−1)^{k − 1} × (k − 1)! × (1^{m − 1} + 2^{m − 1} + 3^{m − 1} + \, ... + k^{m − 1}) × n$ The terms alternate positive and negative — the first is always positive, the second is always negative, and so on Here are the equations for the fifth segment's constants (numbered by $m$ value): $\frac{ 1}{4! × 1!} (     n⁵ −     14n⁴ +      71n³ −     154n² +     120n)$ $\frac{−1}{4! × 2!} (    5n⁵ −     66n⁴ +     307n³ −     582n² +     360n)$ $\frac{ 1}{4! × 3!} (   25n⁵ −    314n⁴ +    1367n³ −    2374n² +    1320n)$ $\frac{−1}{4! × 4!} (  125n⁵ −   1506n⁴ +    6235n³ −   10230n² +    5400n)$ $\frac{ 1}{4! × 5!} (  625n⁵ −   7274n⁴ +   28991n³ −   45814n² +   23496n)$ $\frac{−1}{4! × 6!} ( 3125n⁵ −  35346n⁴ +  136867n³ −  210822n² +  106200n)$ $\frac{ 1}{4! × 7!} (15625n⁵ − 172634n⁴ +  653927n³ −  989254n² +  492360n)$ $\frac{−1}{4! × 8!} (78125n⁵ − 846786n⁴ + 3153835n³ − 4708950n² + 2323800n)$ Here are the equations for the sixth segment's constants (numbered by $m$ value): $\frac{−1}{5! × 1!} (      n⁶ −      20n⁵ +      155n⁴ −      580n³ +      1044n² −      720n)$ $\frac{ 1}{5! × 2!} (     6n⁶ −     115n⁵ +      840n⁴ −     2885n³ +      4554n² −     2520n)$ $\frac{−1}{5! × 3!} (    36n⁶ −     665n⁵ +     4630n⁴ −    14935n³ +     21734n² −    10920n)$ $\frac{ 1}{5! × 4!} (   216n⁶ −    3865n⁵ +    25890n⁴ −    79775n³ +    110334n² −    52920n)$ $\frac{−1}{5! × 5!} (  1296n⁶ −   22565n⁵ +   146530n⁴ −   436555n³ +    584174n² −   273000n)$ $\frac{ 1}{5! × 6!} (  7776n⁶ −  132265n⁵ +   837690n⁴ −  2433935n³ +   3184734n² −  1464120n)$ $\frac{−1}{5! × 7!} ( 46656n⁶ −  777965n⁵ +  4828930n⁴ − 13767235n³ +  17730014n² −  8060520n)$ $\frac{ 1}{5! × 8!} (279936n⁶ − 4589665n⁵ + 28028490n⁴ − 78754775n³ + 100247214n² − 45211320n)$ And here are the only equations I've been able to figure out for the seventh segment's constants (numbered by $m$ value): $\frac{ 1}{6! × 1!} ( n⁷ −  27n⁶ +  295n⁵ −  1665n⁴ +  5104n³ −  8028n² +  5040n)$ $\frac{−1}{6! × 2!} (7n⁷ − 183n⁶ + 1915n⁵ − 10185n⁴ + 28678n³ − 39672n² + 20160n)$ I've tried different ways of factoring the equations, but haven't been able to find a pattern there either. Is there a pattern for figuring out the rest of the coefficients for these equations to generate the constant terms for the integrals that lead to the probability distribution?","Intro and Problem Statement The Irwin–Hall distribution is a probability distribution of the sum of independent, uniformly-distributed, continuous random variables in the interval . The distribution is a piecewise polynomial function composed of sections of degree which, combined, cover the interval . The zeroth piecewise segment is always of the form . The last segment is always of the form . The segments in between are more complicated, and are where I'm running into trouble. I want to find a pattern that allows me to generate the piecewise equation for an arbitrary segment in the distribution of an arbitrary number of variables. The Basics The probability distribution of a single variable looks like a horizontal line (degree zero). The equation of the probability distribution is: If we add a second identical variable to the first, their combined probability distribution runs from 0 to 2, composed of two diagonal lines (degree one). The equation of the combined probability distribution of two variables is: If we add a third variable, the combined probability distribution runs from 0 to 3, composed of three parabolic arcs (degree two). The equation of the combined probability distribution of three variables is: As more and more variables are added (as gets large), their combined Irwin-Hall distribution approaches a normal distribution with mean and variance . How I'm Building Up the Equations The th derivative of the th segment in the distribution, skipping the zeroth segment whose th derivative is always 1, is of the form , or equivalently . For example, in the case of four variables, the 3rd derivatives of the four segments are: , , , and . In the case of five variables, the 4th derivatives of the five segments are: , , , , and . These numbers correspond to the th row of Pascal's triangle, with every other number negative. That is, for variables, the 3rd derivatives of the four segments are the four terms in the 3rd row of the triangle, with odd positions negated. To build up the full -segment distribution curve, we take the integrals of these base numbers times, adding a new constant term each time. The constant term for a given integration step depends on the number of integrations, including the current step (starting at one). The constant we add to the zeroth segment is always zero. The constant we add to the first segment is . The constant we add to the second segment is . The constant we add to the third segment is . The constant we add to the fourth segment is These are the expressions I want to figure out how to create more of. As an example, the equation for the third segment ( ) after three integrations is: In the case of four variables, that simplifies to: Which expands into: Which can be re-written as: Which factors into: The whole process is complicated, but not difficult. The Stumbling Block The problem is, I can't figure out the patterns for creating constants beyond the fourth segment. For the first four segments, I have the patterns for their constant terms and can generate equations for the probability distributions of any number of variables. Using WolframAlpha, I've worked out the equations for generating the constants out to eight segments for values of up to 6, but by the seventh or eighth segment and variable it starts running out of computation time and won't give an answer. And using only the eight equations I am able to get, I have been unable to find the patterns for all the coefficients of those equations. To recap, here are the equations for the terms that I have figured out: Beyond four, though, I'm stuck. I feel like there should be a pattern to these equations that I can use to generate the equations for the constants for an arbitrary segment, but if there is one, I can't find it. Some partial patterns do stand out to me: The expressions are polynomials in terms of with coefficients composed of sums of numbers raised to the power of The th expression is terms long, with highest degree and containing terms of all lower degrees down to and including 1, with zero constant term The first term in the th expression is The last term in the th expression is of the form The terms alternate positive and negative — the first is always positive, the second is always negative, and so on Here are the equations for the fifth segment's constants (numbered by value): Here are the equations for the sixth segment's constants (numbered by value): And here are the only equations I've been able to figure out for the seventh segment's constants (numbered by value): I've tried different ways of factoring the equations, but haven't been able to find a pattern there either. Is there a pattern for figuring out the rest of the coefficients for these equations to generate the constant terms for the integrals that lead to the probability distribution?","n [0, 1] n (n − 1) [0, n] \frac{x^{n − 1}}{(n − 1)!} \frac{(−x + n)^{n − 1}}{(n − 1)!} f_X(x; 1) = \begin{cases}
1 & : 0 ≤ x ≤ 1 \\
0 & : \text{otherwise}
\end{cases} f_X(x; 2) = \begin{cases}
x & : 0 ≤ x ≤ 1 \\
−x + 2 & : 1 < x ≤ 2 \\
0 & : \text{otherwise}
\end{cases} f_X(x; 3) = \begin{cases}
\frac{x²}{2} & : 0 ≤ x ≤ 1 \\
\frac{−2 x² + 6x − 3}{2} & : 1 < x ≤ 2 \\
\frac{(−x + 3)²}{2} & :  2 < x ≤ 3 \\
0 & : \text{otherwise}
\end{cases} n μ = \frac{n}{2} σ² = \frac{n}{12} (n − 1) k (n − 1) \frac{(−1)^k}{k!} (n − 1) (n − 2) (n − 3) \, ... (n − k) \frac{(−1)^k (n − 1)!}{k! (n − 1 − k)!} \frac{ 1}{0!} = 1 \frac{−1}{1!} (4 − 1) = −3 \frac{ 1}{2!} (4 − 1) (4 − 2) = 3 \frac{−1}{3!} (4 − 1) (4 − 2) (4 − 3) = −1 \frac{ 1}{0!} = 1 \frac{−1}{1!} (5 − 1) = −4 \frac{ 1}{2!} (5 − 1) (5 − 2) = 6 \frac{−1}{3!} (5 − 1) (5 − 2) (5 − 3) = −4 \frac{ 1}{4!} (5 − 1) (5 − 2) (5 − 3) (5 − 4) = 1 (n − 1) n = 4 n (n − 1) m \frac{(−1)^{m − 1}}{0! × m!} (n) \frac{(−1)^{m}}{1! × m!} (2^{m − 1} n² − (1 + 2^{m − 1}) n) \frac{(−1)^{m − 1}}{2! × m!} (3^{m − 1} n³ − (2 × 2^{m − 1} +  3 × 3^{m − 1}) n² + 2! (1 + 2^{m − 1} + 3^{m − 1}) n) \frac{(−1)^{m}}{3! × m!} (4^{m − 1}n⁴ - (3 × 3^{m − 1} +  6 × 4^{m − 1}) n³ + (6 × 2^{m − 1} + 9 × 3^{m − 1} + 11 × 4^{m − 1}) n² − 3! (1^{m − 1} + 2^{m − 1} + 3^{m − 1} + 4^{m − 1}) n) k = 3 \int \left( \int \left( \int \left( \frac{−1}{3!} (n − 1) (n − 2) (n − 3) \right)  + \frac{n³ − 5n² + 6n}{2! × 1!} \right) − \frac{3n³ − 13n² + 12n}{2! × 2!} \right) + \frac{9n³ − 35n² + 28n}{2! × 3!} \int \left( \int \left( \int \left( −1 \right)  + 4 \right) − 8 \right) + \frac{32}{3} \frac{−x³}{6} + 2x² − 8x + \frac{32}{3} \frac{−x³ + 12x² − 48x + 64}{3!} \frac{(−x + 4)³}{3!} n 0 \frac{(−1)^{m − 1}}{0! × m!} (n) \frac{(−1)^{m}}{1! × m!} (2^{m − 1} n² − (1 + 2^{m − 1}) n) \frac{(−1)^{m − 1}}{2! × m!} (3^{m − 1} n³ − (2 × 2^{m − 1} +  3 × 3^{m − 1}) n² + (1 + 2^{m − 1} + 3^{m − 1}) n) \frac{(−1)^{m}}{3! × m!} (4^{m − 1}n⁴ - (3 × 3^{m − 1} +  6 × 4^{m − 1}) n³ + (6 × 2^{m − 1} + 9 × 3^{m − 1} + 11 × 4^{m − 1}) n² − 3! (1^{m − 1} + 2^{m − 1} + 3^{m − 1} + 4^{m − 1}) n) n (m − 1) k k k k k^{m − 1} × n^k k (−1)^{k − 1} × (k − 1)! × (1^{m − 1} + 2^{m − 1} + 3^{m − 1} + \, ... + k^{m − 1}) × n m \frac{ 1}{4! × 1!} (     n⁵ −     14n⁴ +      71n³ −     154n² +     120n) \frac{−1}{4! × 2!} (    5n⁵ −     66n⁴ +     307n³ −     582n² +     360n) \frac{ 1}{4! × 3!} (   25n⁵ −    314n⁴ +    1367n³ −    2374n² +    1320n) \frac{−1}{4! × 4!} (  125n⁵ −   1506n⁴ +    6235n³ −   10230n² +    5400n) \frac{ 1}{4! × 5!} (  625n⁵ −   7274n⁴ +   28991n³ −   45814n² +   23496n) \frac{−1}{4! × 6!} ( 3125n⁵ −  35346n⁴ +  136867n³ −  210822n² +  106200n) \frac{ 1}{4! × 7!} (15625n⁵ − 172634n⁴ +  653927n³ −  989254n² +  492360n) \frac{−1}{4! × 8!} (78125n⁵ − 846786n⁴ + 3153835n³ − 4708950n² + 2323800n) m \frac{−1}{5! × 1!} (      n⁶ −      20n⁵ +      155n⁴ −      580n³ +      1044n² −      720n) \frac{ 1}{5! × 2!} (     6n⁶ −     115n⁵ +      840n⁴ −     2885n³ +      4554n² −     2520n) \frac{−1}{5! × 3!} (    36n⁶ −     665n⁵ +     4630n⁴ −    14935n³ +     21734n² −    10920n) \frac{ 1}{5! × 4!} (   216n⁶ −    3865n⁵ +    25890n⁴ −    79775n³ +    110334n² −    52920n) \frac{−1}{5! × 5!} (  1296n⁶ −   22565n⁵ +   146530n⁴ −   436555n³ +    584174n² −   273000n) \frac{ 1}{5! × 6!} (  7776n⁶ −  132265n⁵ +   837690n⁴ −  2433935n³ +   3184734n² −  1464120n) \frac{−1}{5! × 7!} ( 46656n⁶ −  777965n⁵ +  4828930n⁴ − 13767235n³ +  17730014n² −  8060520n) \frac{ 1}{5! × 8!} (279936n⁶ − 4589665n⁵ + 28028490n⁴ − 78754775n³ + 100247214n² − 45211320n) m \frac{ 1}{6! × 1!} ( n⁷ −  27n⁶ +  295n⁵ −  1665n⁴ +  5104n³ −  8028n² +  5040n) \frac{−1}{6! × 2!} (7n⁷ − 183n⁶ + 1915n⁵ − 10185n⁴ + 28678n³ − 39672n² + 20160n)","['probability', 'statistics', 'probability-distributions', 'pattern-recognition']"
26,"Prove if $\{X_n\}$ is a sequence of independent r.v. and converges to $X$ in probability, then $X$ is a constant almost surely.","Prove if  is a sequence of independent r.v. and converges to  in probability, then  is a constant almost surely.",\{X_n\} X X,"Prove that if $\{X_n\}$ is a sequence of independent random variables and converges to $X$ in probability, then $X$ is a constant almost surely. My attempt: To prove that $X$ is a constant a.s., it suffices to show that $\mathbb{P}(X<x)=0 \text{ or } 1$ . Notice that $X_n\xrightarrow[]{\mathbb{P}}X$ implies $X_n\xrightarrow[]{D}X$ . Thus, $\mathbb{P}(X\leq x)=\lim_{n\to \infty}\mathbb{P}(X_n \leq x)\leq\mathbb{P}(\lim\sup_{n\to\infty}\{X_n\leq x\}).$ By Borel's 0-1 law, we have $\mathbb{P}(\lim\sup_{n\to\infty}\{X_n\leq x\})=0\text{ or } 1$ . Similarly, $\mathbb{P}(X\leq x)\geq\mathbb{P}(\lim\inf_{n\to\infty}\{X_n\leq x\})=0\text{ or }1.$ I notice that if both $\mathbb{P}(\lim\sup_{n\to\infty}\{X_n\leq x\})$ and $\mathbb{P}(\lim\inf_{n\to\infty}\{X_n\leq x\})$ always have the same value(0 or 1), then we can prove that $\mathbb{P}(X\leq x)=0 \text{ or }1$ . But somehow I find it very hard to prove. Is there something I have not thought of? Edit: OK, I have figured it out. In this post , we have the conclusion for convergence almost surely. As for convergence in probability, we can always find its subsequence that convergences almost surely to $X$ . Then, we can apply the conclusion for convergence almost surely.","Prove that if is a sequence of independent random variables and converges to in probability, then is a constant almost surely. My attempt: To prove that is a constant a.s., it suffices to show that . Notice that implies . Thus, By Borel's 0-1 law, we have . Similarly, I notice that if both and always have the same value(0 or 1), then we can prove that . But somehow I find it very hard to prove. Is there something I have not thought of? Edit: OK, I have figured it out. In this post , we have the conclusion for convergence almost surely. As for convergence in probability, we can always find its subsequence that convergences almost surely to . Then, we can apply the conclusion for convergence almost surely.",\{X_n\} X X X \mathbb{P}(X<x)=0 \text{ or } 1 X_n\xrightarrow[]{\mathbb{P}}X X_n\xrightarrow[]{D}X \mathbb{P}(X\leq x)=\lim_{n\to \infty}\mathbb{P}(X_n \leq x)\leq\mathbb{P}(\lim\sup_{n\to\infty}\{X_n\leq x\}). \mathbb{P}(\lim\sup_{n\to\infty}\{X_n\leq x\})=0\text{ or } 1 \mathbb{P}(X\leq x)\geq\mathbb{P}(\lim\inf_{n\to\infty}\{X_n\leq x\})=0\text{ or }1. \mathbb{P}(\lim\sup_{n\to\infty}\{X_n\leq x\}) \mathbb{P}(\lim\inf_{n\to\infty}\{X_n\leq x\}) \mathbb{P}(X\leq x)=0 \text{ or }1 X,"['probability', 'probability-theory']"
27,"Game theory, probability and snooker","Game theory, probability and snooker",,"I have a question that is very simple to understand and very complex to answer. If a snooker player could elect to forego potting a coloured ball (typically worth a handful of points) and instead move onto another (worth one), would they? For simplicity, assume that a player can pot any ball of their choosing with probability p. Their opponent can pot with probability q (where p can be greater than, equal to or less than q). The aim of the game is to gain more points than is left on the table. Importantly, I am assuming that in this mathematical snooker game, the coloured balls must be potted at the end, as in regular snooker. What is the optimal strategy for a player to take? Always reds? Always trying for colours? A mix? It depends what the opponent does? It depends on p and q? (You can assume p and q are approximately $0.9$ but varying these would be interesting) If you want to try doing this computationally, feel free to reduce the number of reds from 15 to 10 or 6 as is sometimes played, unless you think that would change the result.","I have a question that is very simple to understand and very complex to answer. If a snooker player could elect to forego potting a coloured ball (typically worth a handful of points) and instead move onto another (worth one), would they? For simplicity, assume that a player can pot any ball of their choosing with probability p. Their opponent can pot with probability q (where p can be greater than, equal to or less than q). The aim of the game is to gain more points than is left on the table. Importantly, I am assuming that in this mathematical snooker game, the coloured balls must be potted at the end, as in regular snooker. What is the optimal strategy for a player to take? Always reds? Always trying for colours? A mix? It depends what the opponent does? It depends on p and q? (You can assume p and q are approximately but varying these would be interesting) If you want to try doing this computationally, feel free to reduce the number of reds from 15 to 10 or 6 as is sometimes played, unless you think that would change the result.",0.9,"['probability', 'recreational-mathematics', 'game-theory', 'dynamic-programming']"
28,"A certain country has four regions: North, East, South, and West. ...","A certain country has four regions: North, East, South, and West. ...",,"A certain country has four regions: North, East, South, and West. The population of these regions are 3 million, 4 million, 5 million, and 8 million, respectively. There are 4 cities in the North, 3 in the East, 2 in the South, and there is only 1 city in the West. Each person in the country lives in exactly one of these cities. a) What is the average size of a city in a region? (This is the arithmetic mean of the population of the cities, and is also the expected value of the population of a city chosen uniformly at random.) b) A region of the country is chosen uniformly at random, and then a city within that region is chosen uniformly at random. What is the expected population size of this randomly chosen city? For part A, letting X = the size of the city, I got E(X) = 1/10[3,000,000 + 4,000,000 + 5,000,000 + 8,000,000] = 2,000,000 For part B, 1/4[750,000 + 4/3(1,000,000) + 2,500,000 + 8,000,000] = about 3145833.33 My answers seem off but I am not sure where exactly in my work I am off. Any help would be greatly appreciated!","A certain country has four regions: North, East, South, and West. The population of these regions are 3 million, 4 million, 5 million, and 8 million, respectively. There are 4 cities in the North, 3 in the East, 2 in the South, and there is only 1 city in the West. Each person in the country lives in exactly one of these cities. a) What is the average size of a city in a region? (This is the arithmetic mean of the population of the cities, and is also the expected value of the population of a city chosen uniformly at random.) b) A region of the country is chosen uniformly at random, and then a city within that region is chosen uniformly at random. What is the expected population size of this randomly chosen city? For part A, letting X = the size of the city, I got E(X) = 1/10[3,000,000 + 4,000,000 + 5,000,000 + 8,000,000] = 2,000,000 For part B, 1/4[750,000 + 4/3(1,000,000) + 2,500,000 + 8,000,000] = about 3145833.33 My answers seem off but I am not sure where exactly in my work I am off. Any help would be greatly appreciated!",,['probability']
29,Probability of drawing an ace or 2 after an ace [closed],Probability of drawing an ace or 2 after an ace [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question From a standard 52 card deck, we draw one card at a time without replacement. After having drawn the first ace, the game continues until either We draw another ace, or We draw a 2, and the game then stop. Which events of the two is more likely, and what are the corresponding probabilities? Edit: It is not very hard to get the probability = 0.5 with calculations. but can we come up with some more intuitive explanations?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question From a standard 52 card deck, we draw one card at a time without replacement. After having drawn the first ace, the game continues until either We draw another ace, or We draw a 2, and the game then stop. Which events of the two is more likely, and what are the corresponding probabilities? Edit: It is not very hard to get the probability = 0.5 with calculations. but can we come up with some more intuitive explanations?",,"['probability', 'poker']"
30,Probabilistic Recurrence Relations,Probabilistic Recurrence Relations,,"In the paper ""The Complexity of Parallel Search"" (Karp, Upfal and Wigderson; Journal of Computer and System Sciences 36, 225 - 253, 1988) in the appendix, the authors present the iteration procedure m = n; t = 0; while m > 0 do      begin m = m - X(m); t = t+1 end T=t; where $X(m)$ is a random variable over $\{0,\dots,m\}$ and $T$ is the random variable which represents the number of iterations executed in the above procedure, depending on $X(m)$ . They give the following theorem: Theorem Suppose $\operatorname{E}[X(m)]\geq g(m)$ where $g:\mathbb R^+\to\mathbb R^+$ is monotone nondecreasing. Then $\operatorname{E}[T]\leq\int_1^n\frac{1}{g(x)}dx$ and for every $a>0$ : $$\operatorname{Pr}\left[T>(a+1)\int_1^n\frac{1}{g(x)}dx\right]<e^{-a}$$ They give no proof but refer to a forthcoming publication. However, I was not able to find any proof of this. The second part of the theorem could be obtained, I think, from Corollary 4.3 of ""Probabilistic Recurrence Relations"" (Karp; Journal of the ACM, 41 (6), 1136 - 1150, 1994). However, I can not really wrap my head around why we have $$\operatorname{E}[T]\leq\int_1^n\frac{1}{g(x)}dx.$$","In the paper ""The Complexity of Parallel Search"" (Karp, Upfal and Wigderson; Journal of Computer and System Sciences 36, 225 - 253, 1988) in the appendix, the authors present the iteration procedure m = n; t = 0; while m > 0 do      begin m = m - X(m); t = t+1 end T=t; where is a random variable over and is the random variable which represents the number of iterations executed in the above procedure, depending on . They give the following theorem: Theorem Suppose where is monotone nondecreasing. Then and for every : They give no proof but refer to a forthcoming publication. However, I was not able to find any proof of this. The second part of the theorem could be obtained, I think, from Corollary 4.3 of ""Probabilistic Recurrence Relations"" (Karp; Journal of the ACM, 41 (6), 1136 - 1150, 1994). However, I can not really wrap my head around why we have","X(m) \{0,\dots,m\} T X(m) \operatorname{E}[X(m)]\geq g(m) g:\mathbb R^+\to\mathbb R^+ \operatorname{E}[T]\leq\int_1^n\frac{1}{g(x)}dx a>0 \operatorname{Pr}\left[T>(a+1)\int_1^n\frac{1}{g(x)}dx\right]<e^{-a} \operatorname{E}[T]\leq\int_1^n\frac{1}{g(x)}dx.","['probability', 'random-variables', 'recurrence-relations', 'expected-value']"
31,Probabilitic inequality for sum of squares of zero mean Gaussian random variables,Probabilitic inequality for sum of squares of zero mean Gaussian random variables,,"Let $X_1,...,X_n$ be i.i.d. standard normal random variables. How to show that there is constant $c>0$ such that for every $a_k>0$ : $P(\sum_{k=1}^{n}a_kX_k^2>\sum_{k=1}^{n}a_k+c\cdot\sqrt{\sum_{k=1}^{n}a_k^2})>c$",Let be i.i.d. standard normal random variables. How to show that there is constant such that for every :,"X_1,...,X_n c>0 a_k>0 P(\sum_{k=1}^{n}a_kX_k^2>\sum_{k=1}^{n}a_k+c\cdot\sqrt{\sum_{k=1}^{n}a_k^2})>c","['probability', 'inequality']"
32,Probability that random variable is inside cone,Probability that random variable is inside cone,,"Suppose $x\in\mathbb{R}^n$ is a random variable with mean $\mu$ and covariance $ \Sigma$ . Consider a stochastic convex optimization problem, i.e. an optimization problem with chance constraints , meaning there is a small, but finite probability, $\Delta\leq 0.5$ , of violating the constraints. In all of the cases I've encountered so far, you assume that the constraint space, $\mathcal{X}$ , is a polytope , meaning it can be written as $$ \mathcal{X} \triangleq \bigcap_{j=1}^{M} \ \{x:\alpha_j^\intercal x \leq \beta_j\} $$ Qualitatively, this represents a finite intersection of linear inequality constraints, which is a convex region. In 2D, this is simply a polygon, with $M$ vertices. For example, if $M = 3$ , then the intersection of the three lines would form a triangle. If $M = 4$ , this would be a square, and so on. The reason people assume the constraint space is a convex polytope is because, using Boole's inequality (which gives an upper bound on the union of sets), the chance constraints can be written as $$ \begin{align} \text{Pr}(x\notin&\mathcal{X}) \leq \Delta\\ &\Updownarrow\\ \text{Pr}(\alpha_j^\intercal x \leq \beta_j) &\geq 1 - \delta_j, \ \forall j = 1,...,M\\ \sum_{j=1}^{M} \delta_j &\leq \Delta, \end{align} $$ where the joint probability of violating the constraints is split up into the individual probability of violating each $j$ th constraint. This is extremely useful, because the second expression is nothing more than the probability of a random variable with mean $\alpha_j^\intercal \mu$ and covariance $\alpha_j^\intercal \Sigma \alpha_j$ . Thus, this probability can be written in terms of the standard normal CDF ( $\Phi$ ) as $$ \Phi\Bigg[\frac{\beta_j - \alpha_j^\intercal \mu}{\sqrt{\alpha_j^\intercal \Sigma \alpha_j}} \Bigg] \geq 1 - \delta_j \Rightarrow \alpha_j^\intercal \mu + \|\Sigma^{1/2} \alpha_j\|^2 \Phi^{-1}(1-\delta_j) \leq \beta_j, $$ since $\Sigma > 0$ is always positive definite, as it represents a standard deviation. The above inequality constraint is a second order cone constraint, and the resulting optimization problem is a SOCP. However, what if the constraint space is now not an polytope (or polygon), but rather a cone , specifically a convex cone. In that case, $\mathcal{X}$ would be defined as $$ \mathcal{X} = \{x : \|Ax+b\|_2 \leq c^\intercal x + d\}. $$ Is it possible, in any way, to calculate $\text{Pr}(x\notin\mathcal{X})$ , or something like that as in the case of a polytope? You would have to make some kind of approximation or relaxation, such as Markov's inequality or Chebyshev inequality, to get rid of the probability and turn it into an expectation. However, I can't seem to figure out a solution. For my purposes, the cone is centered at the origin, so $b = d = 0$ if that makes it simpler to work with. This type of constraint is more natural in a physical setting, especially in controls, where you want to steer distributions from some initial $x\sim\mathcal{N}(\mu_0,\Sigma_0)$ , to the origin for example. I haven't found any other literature on this subject, so if anyone has any insights, it would be appreciated!","Suppose is a random variable with mean and covariance . Consider a stochastic convex optimization problem, i.e. an optimization problem with chance constraints , meaning there is a small, but finite probability, , of violating the constraints. In all of the cases I've encountered so far, you assume that the constraint space, , is a polytope , meaning it can be written as Qualitatively, this represents a finite intersection of linear inequality constraints, which is a convex region. In 2D, this is simply a polygon, with vertices. For example, if , then the intersection of the three lines would form a triangle. If , this would be a square, and so on. The reason people assume the constraint space is a convex polytope is because, using Boole's inequality (which gives an upper bound on the union of sets), the chance constraints can be written as where the joint probability of violating the constraints is split up into the individual probability of violating each th constraint. This is extremely useful, because the second expression is nothing more than the probability of a random variable with mean and covariance . Thus, this probability can be written in terms of the standard normal CDF ( ) as since is always positive definite, as it represents a standard deviation. The above inequality constraint is a second order cone constraint, and the resulting optimization problem is a SOCP. However, what if the constraint space is now not an polytope (or polygon), but rather a cone , specifically a convex cone. In that case, would be defined as Is it possible, in any way, to calculate , or something like that as in the case of a polytope? You would have to make some kind of approximation or relaxation, such as Markov's inequality or Chebyshev inequality, to get rid of the probability and turn it into an expectation. However, I can't seem to figure out a solution. For my purposes, the cone is centered at the origin, so if that makes it simpler to work with. This type of constraint is more natural in a physical setting, especially in controls, where you want to steer distributions from some initial , to the origin for example. I haven't found any other literature on this subject, so if anyone has any insights, it would be appreciated!","x\in\mathbb{R}^n \mu 
\Sigma \Delta\leq 0.5 \mathcal{X} 
\mathcal{X} \triangleq \bigcap_{j=1}^{M} \ \{x:\alpha_j^\intercal x \leq \beta_j\}
 M M = 3 M = 4 
\begin{align}
\text{Pr}(x\notin&\mathcal{X}) \leq \Delta\\
&\Updownarrow\\
\text{Pr}(\alpha_j^\intercal x \leq \beta_j) &\geq 1 - \delta_j, \ \forall j = 1,...,M\\
\sum_{j=1}^{M} \delta_j &\leq \Delta,
\end{align}
 j \alpha_j^\intercal \mu \alpha_j^\intercal \Sigma \alpha_j \Phi 
\Phi\Bigg[\frac{\beta_j - \alpha_j^\intercal \mu}{\sqrt{\alpha_j^\intercal \Sigma \alpha_j}} \Bigg] \geq 1 - \delta_j \Rightarrow \alpha_j^\intercal \mu + \|\Sigma^{1/2} \alpha_j\|^2 \Phi^{-1}(1-\delta_j) \leq \beta_j,
 \Sigma > 0 \mathcal{X} 
\mathcal{X} = \{x : \|Ax+b\|_2 \leq c^\intercal x + d\}.
 \text{Pr}(x\notin\mathcal{X}) b = d = 0 x\sim\mathcal{N}(\mu_0,\Sigma_0)","['probability', 'convex-optimization', 'convex-cone', 'second-order-cone-programming']"
33,"Given a derived random variable $Z=f(X_1,\dots, X_n)$ on $X_j$, what can we say about $X$?","Given a derived random variable  on , what can we say about ?","Z=f(X_1,\dots, X_n) X_j X","Let $Z$ be a derived random variable given by some function $f$ , i.e. $Z=f(X_1,\dots, X_n)$ , where the $X_i\sim X$ are continuous/non-atomic and independently and identically distributed. What can we say about $X$ ? The function $f$ is assumed to be non-constant and invariant under permutations of the coordinates. I have a specific class of examples in mind, where the $X_i$ are defined on a metric space and $f$ depends on the pairwise distances of the $X_i$ . Probably, finding out the distribution of $X$ is too difficult or even impossible in the general case. But what about some simpler properties, for example unimodality, symmmetry, the support, the moments or other summary statistics? Is there a general approach to this? Do you know any references? Remark: In the special case of $Z=X_1+X_2$ , the densities satisfy $\rho_Z=\rho_X*\rho_X$ , so by the convolution theorem, there exists a formal solution $\rho_X=\mathcal F^{-1}(\sqrt {\mathcal F(\rho_Z)}$ . Maybe there is a functional analytic framework which gives similar results for similar/different/arbitrary functions $f$ ?","Let be a derived random variable given by some function , i.e. , where the are continuous/non-atomic and independently and identically distributed. What can we say about ? The function is assumed to be non-constant and invariant under permutations of the coordinates. I have a specific class of examples in mind, where the are defined on a metric space and depends on the pairwise distances of the . Probably, finding out the distribution of is too difficult or even impossible in the general case. But what about some simpler properties, for example unimodality, symmmetry, the support, the moments or other summary statistics? Is there a general approach to this? Do you know any references? Remark: In the special case of , the densities satisfy , so by the convolution theorem, there exists a formal solution . Maybe there is a functional analytic framework which gives similar results for similar/different/arbitrary functions ?","Z f Z=f(X_1,\dots, X_n) X_i\sim X X f X_i f X_i X Z=X_1+X_2 \rho_Z=\rho_X*\rho_X \rho_X=\mathcal F^{-1}(\sqrt {\mathcal F(\rho_Z)} f","['probability', 'probability-theory', 'statistics', 'reference-request', 'soft-question']"
34,An element not in this (baby) cylindrical sigma-algebra?,An element not in this (baby) cylindrical sigma-algebra?,,"Consider $\{0,1\}^\mathbb N$ , a generic element of which I denote by $x= \{x(i)\}_{i \in \mathbb N}$ . Three possible sigma-algebras on this set are : The sigma-algebra $\mathcal E$ generated by the singletons. The cylindrical sigma-algebra $\mathcal F$ generated by $\{x(i)^{-1}(\{j\}) : i \in \mathbb N, j \in \{0,1\}\}$ The powerset $\mathcal G$ They verify $\mathcal E \subset \mathcal F \subset \mathcal G$ , and the first inclusion is strict since $\{0,1\}^\mathbb N$ is not countable. The sigma-algebra generated by the singletons is the set of the countable or co-countable  subsets. So $x(1)^{-1}(\{0\})$ is an element of $\mathcal F \setminus \mathcal E$ . Similarly, in case the inclusion $\mathcal F \subset \mathcal G$ is strict, can one construct an (as explicit as possible) element of $\mathcal G \setminus \mathcal F$ ? I thought about transposing the classical example of the Vitali set of the real line but this does not seem so easy... Also, the kind of example here Cylindrical sigma algebra and continuous functions. does not directly apply in the very simple setting we consider.","Consider , a generic element of which I denote by . Three possible sigma-algebras on this set are : The sigma-algebra generated by the singletons. The cylindrical sigma-algebra generated by The powerset They verify , and the first inclusion is strict since is not countable. The sigma-algebra generated by the singletons is the set of the countable or co-countable  subsets. So is an element of . Similarly, in case the inclusion is strict, can one construct an (as explicit as possible) element of ? I thought about transposing the classical example of the Vitali set of the real line but this does not seem so easy... Also, the kind of example here Cylindrical sigma algebra and continuous functions. does not directly apply in the very simple setting we consider.","\{0,1\}^\mathbb N x= \{x(i)\}_{i \in \mathbb N} \mathcal E \mathcal F \{x(i)^{-1}(\{j\}) : i \in \mathbb N, j \in \{0,1\}\} \mathcal G \mathcal E \subset \mathcal F \subset \mathcal G \{0,1\}^\mathbb N x(1)^{-1}(\{0\}) \mathcal F \setminus \mathcal E \mathcal F \subset \mathcal G \mathcal G \setminus \mathcal F","['probability', 'general-topology']"
35,Help proving a property of probability distributions,Help proving a property of probability distributions,,"Let $p = (p_1, p_2, ... p_k)$ where $p_i = \mathbb{P}(i)$ . Here, $p$ is any discrete probability distribution, but I will refer to it as the probability distribution for some $k$ sided unfair dice. Now let $\pi_i(p) = \sum\limits_{j=1}^k (p_j)^i$ . I conjecture that $\forall i \ge 2, \pi_i(p) \ge (\pi_2(p))^{i-1}$ , but I'm not sure how to go about proving it. The idea behind this conjecture is that $\pi_i(p)$ is the probability that exactly $i$ rolls of the dice have the same outcome, and that the probability of this occurring is greater than or equal to the probability of the 1st roll matching the 2nd, the 2nd roll matching the 3rd, and so on, until the $i-1$ th roll matching the $i$ th roll (which means matching 2 rolls $i-1$ times). This property would also allow me to rigorously prove a lower bound for another probability which I already suspect to be true, which is another reason why I believe this property should be true. ==================================================================== Update: I've found a way to prove a slightly weaker version of the conjecture: Let $p = (p_1, p_2, ..., p_k)$ be some discrete probability distribution, let $q = (\frac{1}{k}, \frac{1}{k}, ..., \frac{1}{k})$ be a uniform probability distribution, and let $\pi_i$ be defined as above. Then $\forall i \ge 2, \pi_i(p) \ge (\pi_2(q))^{i-1} = \big(\frac{1}{k}\big)^{i-1}$ Proof: According to the top answer on Relations between p norms , based on Hölder's inequality, $\forall 0 < a < b$ , $\Vert p \Vert_a \le k^{1/a-1/b} \Vert p \Vert_b$ Letting $a = 1$ and $b = i \ge 2$ , we have that \begin{align}      & \Vert p \Vert_1 = 1 \le k^{1-1/i} \Vert p \Vert_i \\ \iff\quad & k^{1/i-1} \le (\pi_i(p))^{1/i} \\ \iff\quad & k^{1-i} \le \pi_i(p) \\ \iff\quad & \Big(\frac{1}{k}\Big)^{i-1} \le \pi_i(p) \end{align} Additionally, we have that $\pi_2(q) = \sum\limits_{j=1}^k (q_j)^2 = \sum\limits_{j=1}^k \big(\frac{1}{k})^2 = k \cdot \frac{1}{k^2} = \frac{1}{k}$ So substituting, we have $\forall i \ge 2, \pi_i(p) \ge (\pi_2(q))^{i-1}$ $$\tag*{$\blacksquare$}$$ Additionally, $(\pi_2(q))^{i-1} = \pi_i(q)$ (this is relatively easy to prove), so this also proves that the probability of getting $i$ identical rolls on an unfair dice is greater than or equal to the probability of getting $i$ identical rolls on a fair dice, which is an interesting result in its own right (at least it is to me).","Let where . Here, is any discrete probability distribution, but I will refer to it as the probability distribution for some sided unfair dice. Now let . I conjecture that , but I'm not sure how to go about proving it. The idea behind this conjecture is that is the probability that exactly rolls of the dice have the same outcome, and that the probability of this occurring is greater than or equal to the probability of the 1st roll matching the 2nd, the 2nd roll matching the 3rd, and so on, until the th roll matching the th roll (which means matching 2 rolls times). This property would also allow me to rigorously prove a lower bound for another probability which I already suspect to be true, which is another reason why I believe this property should be true. ==================================================================== Update: I've found a way to prove a slightly weaker version of the conjecture: Let be some discrete probability distribution, let be a uniform probability distribution, and let be defined as above. Then Proof: According to the top answer on Relations between p norms , based on Hölder's inequality, , Letting and , we have that Additionally, we have that So substituting, we have Additionally, (this is relatively easy to prove), so this also proves that the probability of getting identical rolls on an unfair dice is greater than or equal to the probability of getting identical rolls on a fair dice, which is an interesting result in its own right (at least it is to me).","p = (p_1, p_2, ... p_k) p_i = \mathbb{P}(i) p k \pi_i(p) = \sum\limits_{j=1}^k (p_j)^i \forall i \ge 2, \pi_i(p) \ge (\pi_2(p))^{i-1} \pi_i(p) i i-1 i i-1 p = (p_1, p_2, ..., p_k) q = (\frac{1}{k}, \frac{1}{k}, ..., \frac{1}{k}) \pi_i \forall i \ge 2, \pi_i(p) \ge (\pi_2(q))^{i-1} = \big(\frac{1}{k}\big)^{i-1} \forall 0 < a < b \Vert p \Vert_a \le k^{1/a-1/b} \Vert p \Vert_b a = 1 b = i \ge 2 \begin{align}
     & \Vert p \Vert_1 = 1 \le k^{1-1/i} \Vert p \Vert_i \\
\iff\quad & k^{1/i-1} \le (\pi_i(p))^{1/i} \\
\iff\quad & k^{1-i} \le \pi_i(p) \\
\iff\quad & \Big(\frac{1}{k}\Big)^{i-1} \le \pi_i(p)
\end{align} \pi_2(q) = \sum\limits_{j=1}^k (q_j)^2 = \sum\limits_{j=1}^k \big(\frac{1}{k})^2 = k \cdot \frac{1}{k^2} = \frac{1}{k} \forall i \ge 2, \pi_i(p) \ge (\pi_2(q))^{i-1} \tag*{\blacksquare} (\pi_2(q))^{i-1} = \pi_i(q) i i","['probability', 'probability-distributions']"
36,What is the distribution of the angle between two random vectors?,What is the distribution of the angle between two random vectors?,,"Let $x,y\stackrel{\text{i.i.d.}}{\sim}\mathcal{N}(0,I_d)$ be two random $d$ -dimensional standard normal distributed vectors and let $\theta$ denote the angle between them. ( $I_d$ denotes the $d$ -dimensional identity matrix). Do you know about a probability distribution that models either the distribution of $\theta\in[0,\pi]$ , or the distribution of $\cos(\theta)\in[-1,1]$ , or the distribution of $\frac{\cos(\theta)+1}{2}\in[0,1]$ ? A paper (on page 18, second paragraph) says that the third option is distributed according to $Beta(d/2,d/2)$ . Can you derive that? Or can you point me to a resource proving it? Or might it be that they're just using an approximation?","Let be two random -dimensional standard normal distributed vectors and let denote the angle between them. ( denotes the -dimensional identity matrix). Do you know about a probability distribution that models either the distribution of , or the distribution of , or the distribution of ? A paper (on page 18, second paragraph) says that the third option is distributed according to . Can you derive that? Or can you point me to a resource proving it? Or might it be that they're just using an approximation?","x,y\stackrel{\text{i.i.d.}}{\sim}\mathcal{N}(0,I_d) d \theta I_d d \theta\in[0,\pi] \cos(\theta)\in[-1,1] \frac{\cos(\theta)+1}{2}\in[0,1] Beta(d/2,d/2)","['probability', 'probability-distributions']"
37,"Finding the support of the CDF of $(X,Y)$",Finding the support of the CDF of,"(X,Y)","Assume $X$ to be standard normal random variable, and define $Y$ as $$Y=\begin{cases}X,&\text{if }⌊X⌋\text{ is even}\\-X,&\text{if }⌊X⌋\text{ is odd}\end{cases}.$$ I am trying to show that $X$ and $Y$ are mutually completely dependent. For this I want to find the support of the copula of $(X,Y)$ : $$ C(u,v)=\mathbb{P}( X \leq F^{-1}(u), Y \leq G^{-1}(v)),$$ and conclude by finding the probability mass concentrated on some locus. Here $F(x) = \Phi(x)$ is the standard normal distribution of $X$ , and $G(y)$ is the distribution of $Y$ . However, I am not sure how to technically tackle the $Y$ , that is, how to properly split it between cases odd/even integer values. My approach: I am adding an excerpt from another question of mine (which is linked) with my approach. If my work is correct, I worked out that $$G(y)=\frac{1}{2}[1 + F(y) - F(-y)].$$ Further, \begin{align*} C(u,v) &= \mathbb{P}(X \leq F^{-1}(u), Y \leq G^{-1}(v))\\ &= \frac{1}{2}\left[\mathbb{P}(X \leq F^{-1}(u), X > -G^{-1}(v)) + \mathbb{P}(X \leq F^{-1}(u), X \leq G^{-1}(v)) \right]\\ &=\frac{1}{2}\left[\mathbb{P}(X \leq \min\{F^{-1}(u), G^{-1}(v)\}) + \mathbb{P}(X \in [-G^{-1}(u), F^{-1}(v)])  \right]\\  &= \frac{1}{2}\left[ v - F(G^{-1}(v)) + F(\min\{F^{-1}(u), G^{-1}(v)\})  \right]. \tag{1} \end{align*} It seems I might be able to untangle this as a function of $(u,v)$ if I find $F(G^{-1}(v))$ . However, here I'm not too sure that the usual approach of finding the inverse works. $$ y = G(G^{-1}(y)) =\frac{1}{2}[1 + F(G^{-1}(y)) - F(-G^{-1}(y))] $$ $$ 2y -1  = F(G^{-1}(y)) - F(-G^{-1}(y)) $$ $$ F^{-1}(2y -1)  = F^{-1}\left(  F(G^{-1}(y)) -F(-G^{-1}(y)) \right) \tag{2}$$ And it seems I am stuck here . I am thinking that it is enough to simplify $(1)$ to some convenient form, here my idea is that finding the expression for $G^{-1}(y)$ would help, but I got stuck at $(2)$ . Maybe another approach is better? Would it be easier to show that $\mathbb{P}(X = g(Y))= 1$ , where $g$ is some function of $Y$ ? But I still need to find the support of $C$ for following exercises. Would appreciate any hints or suggestions on how to proceed!","Assume to be standard normal random variable, and define as I am trying to show that and are mutually completely dependent. For this I want to find the support of the copula of : and conclude by finding the probability mass concentrated on some locus. Here is the standard normal distribution of , and is the distribution of . However, I am not sure how to technically tackle the , that is, how to properly split it between cases odd/even integer values. My approach: I am adding an excerpt from another question of mine (which is linked) with my approach. If my work is correct, I worked out that Further, It seems I might be able to untangle this as a function of if I find . However, here I'm not too sure that the usual approach of finding the inverse works. And it seems I am stuck here . I am thinking that it is enough to simplify to some convenient form, here my idea is that finding the expression for would help, but I got stuck at . Maybe another approach is better? Would it be easier to show that , where is some function of ? But I still need to find the support of for following exercises. Would appreciate any hints or suggestions on how to proceed!","X Y Y=\begin{cases}X,&\text{if }⌊X⌋\text{ is even}\\-X,&\text{if }⌊X⌋\text{ is odd}\end{cases}. X Y (X,Y)  C(u,v)=\mathbb{P}( X \leq F^{-1}(u), Y \leq G^{-1}(v)), F(x) = \Phi(x) X G(y) Y Y G(y)=\frac{1}{2}[1 + F(y) - F(-y)]. \begin{align*}
C(u,v) &= \mathbb{P}(X \leq F^{-1}(u), Y \leq G^{-1}(v))\\
&= \frac{1}{2}\left[\mathbb{P}(X \leq F^{-1}(u), X > -G^{-1}(v)) + \mathbb{P}(X \leq F^{-1}(u), X \leq G^{-1}(v)) \right]\\
&=\frac{1}{2}\left[\mathbb{P}(X \leq \min\{F^{-1}(u), G^{-1}(v)\}) + \mathbb{P}(X \in [-G^{-1}(u), F^{-1}(v)])  \right]\\ 
&= \frac{1}{2}\left[ v - F(G^{-1}(v)) + F(\min\{F^{-1}(u), G^{-1}(v)\})  \right]. \tag{1}
\end{align*} (u,v) F(G^{-1}(v))  y = G(G^{-1}(y)) =\frac{1}{2}[1 + F(G^{-1}(y)) - F(-G^{-1}(y))]   2y -1  = F(G^{-1}(y)) - F(-G^{-1}(y))   F^{-1}(2y -1)  = F^{-1}\left(  F(G^{-1}(y)) -F(-G^{-1}(y)) \right) \tag{2} (1) G^{-1}(y) (2) \mathbb{P}(X = g(Y))= 1 g Y C","['probability', 'probability-theory', 'probability-distributions', 'random-variables']"
38,"Bishop - Pattern Recognition & Machine Learning, Exercise 1.4","Bishop - Pattern Recognition & Machine Learning, Exercise 1.4",,"I'm working on exercise 1.4 in Bishop's Pattern Recognition & Machine Learning book. This exercise is about probability densities. I've two questions about this exercise. First, I don't understand equation 1.27. He writes: ""Under a nonlinear change of variable , a probability density transforms differently from a simple function, due to the Jacobian Factor."" I never ever heard about the Jacobian factor. What is that factor? ""For instance, if we consider a change of variables $x = g(y)$ , then a function $f(x)$ becomes $\tilde f(g(y))$ . Now consider a probabilty density $p_x(x)$ that corresponds to a density $p_y(y)$ with respect to the new variable $y$ , where the suffices denote the fact that $p_x(x)$ and $p_y(y)$ are different densities. Observations falling in range $(x, x + \delta x)$ will, for small values of $\delta x$ , be transformed into the range $(y, \delta y)$ where $p_x(x)\delta x \simeq p_y(y)\delta y$ , [...]"" What does the relation $\simeq$ mean in this context? ""[...] and hence $$ \begin{align} p_y(y) &= p_x(x) \left| \frac{\text{d}x}{\text{d}y}\right|\\ &= p_x(g(y))\left|g'(y)\right|."" \end{align} $$ This is equation 1.27. I don't understand where this equation comes from. Why is there this absolute value? ""One consquence of this property is that the concept of the maximum of a probabilty density is dependent on the choice of variable."" And at this point the book refers to exercise 1.4: ""Consider a probability density $p_x(x)$ defined over a continous variable $x$ , and suppose that we make a nonlinear change of variable using $x = g(y)$ , so that the density transforms according (1.27). By differentiating (1.27), show that the location $\hat y$ of the maximum of the density in $y$ is not in general related to the location $\hat x$ of the maximum of the density over $x$ by the simple functional relation $\hat x = g(\hat y)$ as a consequence of the Jacobian factor. This shows that the maximum of a probability density (in contrast to a simple function) is dependent on the choice of variable. Verify that, in the case of a linear transformation the location of the maximum transforms in the same way as the variable itself."" I don't understand, what this exercise asks me to do... :/ Would be great, if someone could help me...","I'm working on exercise 1.4 in Bishop's Pattern Recognition & Machine Learning book. This exercise is about probability densities. I've two questions about this exercise. First, I don't understand equation 1.27. He writes: ""Under a nonlinear change of variable , a probability density transforms differently from a simple function, due to the Jacobian Factor."" I never ever heard about the Jacobian factor. What is that factor? ""For instance, if we consider a change of variables , then a function becomes . Now consider a probabilty density that corresponds to a density with respect to the new variable , where the suffices denote the fact that and are different densities. Observations falling in range will, for small values of , be transformed into the range where , [...]"" What does the relation mean in this context? ""[...] and hence This is equation 1.27. I don't understand where this equation comes from. Why is there this absolute value? ""One consquence of this property is that the concept of the maximum of a probabilty density is dependent on the choice of variable."" And at this point the book refers to exercise 1.4: ""Consider a probability density defined over a continous variable , and suppose that we make a nonlinear change of variable using , so that the density transforms according (1.27). By differentiating (1.27), show that the location of the maximum of the density in is not in general related to the location of the maximum of the density over by the simple functional relation as a consequence of the Jacobian factor. This shows that the maximum of a probability density (in contrast to a simple function) is dependent on the choice of variable. Verify that, in the case of a linear transformation the location of the maximum transforms in the same way as the variable itself."" I don't understand, what this exercise asks me to do... :/ Would be great, if someone could help me...","x = g(y) f(x) \tilde f(g(y)) p_x(x) p_y(y) y p_x(x) p_y(y) (x, x + \delta x) \delta x (y, \delta y) p_x(x)\delta x \simeq p_y(y)\delta y \simeq 
\begin{align}
p_y(y) &= p_x(x) \left| \frac{\text{d}x}{\text{d}y}\right|\\
&= p_x(g(y))\left|g'(y)\right|.""
\end{align}
 p_x(x) x x = g(y) \hat y y \hat x x \hat x = g(\hat y)","['probability', 'machine-learning', 'density-function', 'pattern-recognition']"
39,"What applications did Laplace have in mind when he said (in 1812) that probability had become ""the most important object of human knowledge""?","What applications did Laplace have in mind when he said (in 1812) that probability had become ""the most important object of human knowledge""?",,"""It is remarkable that a science which began with the consideration of   games of chance should have become the most important object of human   knowledge."" -- Laplace, Théorie Analytique des Probabilitiés, 1812. What new applications of probability was Laplace referring to here, in 1812, when he declared that probability had become the most important object of human knowledge? What was probability used for at that time?","""It is remarkable that a science which began with the consideration of   games of chance should have become the most important object of human   knowledge."" -- Laplace, Théorie Analytique des Probabilitiés, 1812. What new applications of probability was Laplace referring to here, in 1812, when he declared that probability had become the most important object of human knowledge? What was probability used for at that time?",,"['probability', 'math-history']"
40,"When defining independence of random variables,do they need to be on the same probability space?","When defining independence of random variables,do they need to be on the same probability space?",,"Random variable X and Y with cumulative distribution functions $ F_X(x)$ and $F_Y(y)$,are independent iff the combined random variable $(X, Y)$ has a joint cumulative distribution function   $$F_{X,Y}(x,y) = F_X(x) F_Y(y)$$ In the above definition,they use joint distribution of $(X,Y)$. According to the definition of joint probability distribution of random vector such as $(X,Y)$, $X$ and $Y$ must be defined on the same probability space. Does it mean $X$ and $Y$ when defining independence should be on the same probability space? relate question: Can we define a joint probability distribution over different sample spaces / probability spaces?","Random variable X and Y with cumulative distribution functions $ F_X(x)$ and $F_Y(y)$,are independent iff the combined random variable $(X, Y)$ has a joint cumulative distribution function   $$F_{X,Y}(x,y) = F_X(x) F_Y(y)$$ In the above definition,they use joint distribution of $(X,Y)$. According to the definition of joint probability distribution of random vector such as $(X,Y)$, $X$ and $Y$ must be defined on the same probability space. Does it mean $X$ and $Y$ when defining independence should be on the same probability space? relate question: Can we define a joint probability distribution over different sample spaces / probability spaces?",,['probability']
41,Probability such that the average of those 22 numbers is the smallest possible average?,Probability such that the average of those 22 numbers is the smallest possible average?,,"A person divides a square to 100 small squares with equal sizes. Each small square is randomly selected and given a number from 1-100, such that each square is 'unique'. Then the person counts the sum of the numbers at each row, column, and each diagonal. In total, 22 numbers (22 sum results). What is the probability such that the average of those 22 numbers is the smallest possible average ..? Attempt: From my understanding, the diagonals are the two diagonals, and they certainly do not overlap each other at the center. Thisis because the size is 100 (even number). The placing of numbers on the rows and columns doesnt affect the total values. Any permutation will have a total $5050$. So the average will be affected only by the permutations in the two diagonals. The smallest average is when the numbers $1-20$ are in the diagonals. Hence the number of outcomes (the permutation) is  $$ (20 \times 19 \times ..... \times 2 \times 1) = 20! $$ And it should be multiplied by $$ 80! $$ Which is the number of placing for the non-diagonals. While the total possible outcome should be $$ 100! $$ So the probability should be : $$ \frac{80! 20!}{100!} $$ Is this accurate? Are there better methods? Thanks.","A person divides a square to 100 small squares with equal sizes. Each small square is randomly selected and given a number from 1-100, such that each square is 'unique'. Then the person counts the sum of the numbers at each row, column, and each diagonal. In total, 22 numbers (22 sum results). What is the probability such that the average of those 22 numbers is the smallest possible average ..? Attempt: From my understanding, the diagonals are the two diagonals, and they certainly do not overlap each other at the center. Thisis because the size is 100 (even number). The placing of numbers on the rows and columns doesnt affect the total values. Any permutation will have a total $5050$. So the average will be affected only by the permutations in the two diagonals. The smallest average is when the numbers $1-20$ are in the diagonals. Hence the number of outcomes (the permutation) is  $$ (20 \times 19 \times ..... \times 2 \times 1) = 20! $$ And it should be multiplied by $$ 80! $$ Which is the number of placing for the non-diagonals. While the total possible outcome should be $$ 100! $$ So the probability should be : $$ \frac{80! 20!}{100!} $$ Is this accurate? Are there better methods? Thanks.",,"['probability', 'permutations', 'contest-math']"
42,Expected number of times an object is picked from a randomly ordered list of distinct objects,Expected number of times an object is picked from a randomly ordered list of distinct objects,,"There are $x$ subscribers to a weekly reading list which contains $y$ distinct articles. The subscribers are sent the exact same list of articles but each subscriber receives the weekly list in random order. Each article $y_i$ in the list has an estimated reading time $r_i$ associated with it. It is estimated that, on average, a subscriber reads the articles in the order listed on the her/his list and spends $z$ minutes per week reading the articles on the list, where $z < r_1 + r_2 + \dots + r_y$. Let $y_e$ be an article in the weekly list. What's the shortest way to compute the expected value of the number of subscribers who get to read article $y_e$?","There are $x$ subscribers to a weekly reading list which contains $y$ distinct articles. The subscribers are sent the exact same list of articles but each subscriber receives the weekly list in random order. Each article $y_i$ in the list has an estimated reading time $r_i$ associated with it. It is estimated that, on average, a subscriber reads the articles in the order listed on the her/his list and spends $z$ minutes per week reading the articles on the list, where $z < r_1 + r_2 + \dots + r_y$. Let $y_e$ be an article in the weekly list. What's the shortest way to compute the expected value of the number of subscribers who get to read article $y_e$?",,"['probability', 'combinatorics']"
43,Symmetrization argument for dependent variables,Symmetrization argument for dependent variables,,"A standard argument in empirical process theory leads to the following inequality: let $Z_1, \dots, Z_n$ be i.i.d random variables and let $g$ be a convex function. Then it holds that $$ \mathbb{E}\left[g\left( \sum_{i=1}^n Z_i  - \mathbb{E}[Z_i] \right)\right] \leq 2 \mathbb{E} \left[ g\left( \sum_{i=1}^n \varepsilon_i Z_i \right) \right] $$ where $\varepsilon_1, \dots \varepsilon_n$ are i.i.d Rademacher variables. Question : is there a version of this argument when $Z_i$ are identically distributed but not necessarily independent?","A standard argument in empirical process theory leads to the following inequality: let $Z_1, \dots, Z_n$ be i.i.d random variables and let $g$ be a convex function. Then it holds that $$ \mathbb{E}\left[g\left( \sum_{i=1}^n Z_i  - \mathbb{E}[Z_i] \right)\right] \leq 2 \mathbb{E} \left[ g\left( \sum_{i=1}^n \varepsilon_i Z_i \right) \right] $$ where $\varepsilon_1, \dots \varepsilon_n$ are i.i.d Rademacher variables. Question : is there a version of this argument when $Z_i$ are identically distributed but not necessarily independent?",,"['probability', 'inequality', 'reference-request', 'expectation', 'empirical-processes']"
44,How to get joint probability density from bivariate distribution function,How to get joint probability density from bivariate distribution function,,"Let $X_{i} \sim \varepsilon(\lambda_{i}), i = 1,2,3$ be mutually independent ($\varepsilon$ means exponential, $\lambda_{i}$'s are parameters). Then $(T_{1},T_{2}) = (X_{1} \wedge X_{3}, X_{2} \wedge X_{3})$ has a bivariate Marshall-Olkin exponential survival function, for $t_{1}\geq 0$ and $t_{2} \geq 0$: $$ \overline{F}(t_{1},t_{2})=P(T_{1}>t_{1},\, T_{2}>t_{2}) = P(X_{1}>t_{1},\, X_{2}>t_{2}, X_{3}>t_{1} \vee t_{2}) \\ = \exp\{-\lambda_{1}t_{1}-\lambda_{2}t_{2} - \lambda_{3}(t_{1} \vee t_{2})\} $$ I need to derive the joint probability density function $f(t_{1},t_{2})$ for this distribution function $\overline{F}$, and then compute $\int \int_{\mathbb{R}_{+}^{2}}f(t_{1},t_{2})dt_{1}dt_{2}$, and then say whether there is anything strange about my answer. Usually, in the case of one variable, to get from a distribution function $F$ to a probability density function $f$, I would take the derivative of $F$. However, in the case of two variables, I am not sure what to do - do I take partial derivatives of $t_{1}$ and $t_{2}$? How does the $\vee$ operator impact that? Then, I assume I have to integrate them again, and I'm guessing that the ""strange"" part is that when I integrate, I'm not going to get back my original $\overline{F}$, possibly because the $X_{i}$ are mutually independent. But, like I said, I am not sure how to formally go about showing any of these things, primarily because of the multivariate nature of this problem, and because the relationship between the $X_{i}$ and the $T_{i}$ is confusing me. Could somebody please help me finish this? I am extremely confused and very much in need of guidance! Thank you!","Let $X_{i} \sim \varepsilon(\lambda_{i}), i = 1,2,3$ be mutually independent ($\varepsilon$ means exponential, $\lambda_{i}$'s are parameters). Then $(T_{1},T_{2}) = (X_{1} \wedge X_{3}, X_{2} \wedge X_{3})$ has a bivariate Marshall-Olkin exponential survival function, for $t_{1}\geq 0$ and $t_{2} \geq 0$: $$ \overline{F}(t_{1},t_{2})=P(T_{1}>t_{1},\, T_{2}>t_{2}) = P(X_{1}>t_{1},\, X_{2}>t_{2}, X_{3}>t_{1} \vee t_{2}) \\ = \exp\{-\lambda_{1}t_{1}-\lambda_{2}t_{2} - \lambda_{3}(t_{1} \vee t_{2})\} $$ I need to derive the joint probability density function $f(t_{1},t_{2})$ for this distribution function $\overline{F}$, and then compute $\int \int_{\mathbb{R}_{+}^{2}}f(t_{1},t_{2})dt_{1}dt_{2}$, and then say whether there is anything strange about my answer. Usually, in the case of one variable, to get from a distribution function $F$ to a probability density function $f$, I would take the derivative of $F$. However, in the case of two variables, I am not sure what to do - do I take partial derivatives of $t_{1}$ and $t_{2}$? How does the $\vee$ operator impact that? Then, I assume I have to integrate them again, and I'm guessing that the ""strange"" part is that when I integrate, I'm not going to get back my original $\overline{F}$, possibly because the $X_{i}$ are mutually independent. But, like I said, I am not sure how to formally go about showing any of these things, primarily because of the multivariate nature of this problem, and because the relationship between the $X_{i}$ and the $T_{i}$ is confusing me. Could somebody please help me finish this? I am extremely confused and very much in need of guidance! Thank you!",,['probability']
45,Quantiles of comonotone sums,Quantiles of comonotone sums,,"Let $(\Omega, \mathcal F, P)$ be a probability space. Let $\mathbf{X} = (X_1, X_2, \ldots, X_n)^T$ be a random vector and $U \sim \mathrm{uniform}(0, 1)$ be a random variable, both defined on $\Omega$. We say that $\mathbf{X}$ is comonotone if $\mathbf{X} \stackrel{d}= (F_{X_1}^{-1}(U), \ldots, F_{X_n}^{-1}(U))^T$, where, for a random variable $X$, we define $F_X^{-1}(p) := \inf\{x \in \mathbb R; F_X(x) \ge p\}$, $p \in [0, 1]$. Furthermore, define $F_X^{-1+}(p) := \sup\{x \in \mathbb R; F_X(x) \le p\}$, $p \in [0, 1]$. Set $S := \sum_{i = 1}^n F_{X_i}^{-1}(U)$. Now, suppose that $\mathbf{X}$ is comonotone. Then, for any $p \in [0, 1]$: $F_S^{-1}(p) = \sum_{i = 1}^n F_{X_i}^{-1}(p)$ and $F_S^{-1+}(p) = \sum_{i = 1}^n F_{X_i}^{-1+}(p)$. How can I prove both statements? Thank you for any help!","Let $(\Omega, \mathcal F, P)$ be a probability space. Let $\mathbf{X} = (X_1, X_2, \ldots, X_n)^T$ be a random vector and $U \sim \mathrm{uniform}(0, 1)$ be a random variable, both defined on $\Omega$. We say that $\mathbf{X}$ is comonotone if $\mathbf{X} \stackrel{d}= (F_{X_1}^{-1}(U), \ldots, F_{X_n}^{-1}(U))^T$, where, for a random variable $X$, we define $F_X^{-1}(p) := \inf\{x \in \mathbb R; F_X(x) \ge p\}$, $p \in [0, 1]$. Furthermore, define $F_X^{-1+}(p) := \sup\{x \in \mathbb R; F_X(x) \le p\}$, $p \in [0, 1]$. Set $S := \sum_{i = 1}^n F_{X_i}^{-1}(U)$. Now, suppose that $\mathbf{X}$ is comonotone. Then, for any $p \in [0, 1]$: $F_S^{-1}(p) = \sum_{i = 1}^n F_{X_i}^{-1}(p)$ and $F_S^{-1+}(p) = \sum_{i = 1}^n F_{X_i}^{-1+}(p)$. How can I prove both statements? Thank you for any help!",,"['probability', 'probability-theory', 'probability-distributions']"
46,"Equivalence between ""gambler's ruin"" and seemingly different game","Equivalence between ""gambler's ruin"" and seemingly different game",,"My question concerns two experiments with different rules, but with the same probabilities. I was wondering, is there is an intuitive explanation for this equality, or is it is a coincidence? Suppose that when Alice and Bob play chess, Alice wins with probability $p$ independently of previous games. Game 1: Alice and Bob start with $n$ dollars each. They play chess over and over. Each time, the loser pays the winner a dollar, until someone runs out of money. Let $q=1-p$. Using the classic gambler's ruin formula , $$ P(\text{Alice wins Game 1}) = \frac{1-(\frac{q}p)^n}{1-(\frac{q}p)^{2n}} = \frac{1}{1+(\frac{q}p)^n} = \frac{p^n}{p^n+q^n} $$ Game 2: Alice and Bob play $n$ games of chess. If one of them wins all $n$ games, they immediately win the series. Otherwise, they repeat, playing blocks of $n$ games until someone wins them all. Obviously, $$ P(\text{Alice wins Game 2}) = \frac{p^n}{p^n+q^n} $$","My question concerns two experiments with different rules, but with the same probabilities. I was wondering, is there is an intuitive explanation for this equality, or is it is a coincidence? Suppose that when Alice and Bob play chess, Alice wins with probability $p$ independently of previous games. Game 1: Alice and Bob start with $n$ dollars each. They play chess over and over. Each time, the loser pays the winner a dollar, until someone runs out of money. Let $q=1-p$. Using the classic gambler's ruin formula , $$ P(\text{Alice wins Game 1}) = \frac{1-(\frac{q}p)^n}{1-(\frac{q}p)^{2n}} = \frac{1}{1+(\frac{q}p)^n} = \frac{p^n}{p^n+q^n} $$ Game 2: Alice and Bob play $n$ games of chess. If one of them wins all $n$ games, they immediately win the series. Otherwise, they repeat, playing blocks of $n$ games until someone wins them all. Obviously, $$ P(\text{Alice wins Game 2}) = \frac{p^n}{p^n+q^n} $$",,"['probability', 'combinatorics', 'random-walk']"
47,Expectation of increasing transformation of random variables,Expectation of increasing transformation of random variables,,"Suppose that $X$ is a positive continuous random variable with infinitely differentiable pdf $f_\theta (x)$ and suppose that its expectation is increasing in $\theta$. That is, the function $$ g_{1}(\theta) = \mathbb{E} [X] = \int_{\mathbb{R}_+} x \, f_\theta(x) \, \mathrm{d} x   $$ is increasing in $\theta$. Now suppose that $h$ is infinitely differentiable, positive, and increasing ($h$ doesn't depend on $\theta$, either). Is it true that the function $$ g_2 (\theta) = \int_{\mathbb{R}_+} h(x) \, f_\theta(x) \, \mathrm{d} x $$ is increasing in $\theta$? (or are there any extra restrictions, such as convexity or concavity, which might ensure that the statement is correct?). I've tried some examples, like $X \sim $ Normal$(\mu,1)$ along with some increasing transformations, and it seems to work. I've started taking derivatives inside the integral, but I haven't succeeded in proving it. Any help would be much appreciated, thanks!","Suppose that $X$ is a positive continuous random variable with infinitely differentiable pdf $f_\theta (x)$ and suppose that its expectation is increasing in $\theta$. That is, the function $$ g_{1}(\theta) = \mathbb{E} [X] = \int_{\mathbb{R}_+} x \, f_\theta(x) \, \mathrm{d} x   $$ is increasing in $\theta$. Now suppose that $h$ is infinitely differentiable, positive, and increasing ($h$ doesn't depend on $\theta$, either). Is it true that the function $$ g_2 (\theta) = \int_{\mathbb{R}_+} h(x) \, f_\theta(x) \, \mathrm{d} x $$ is increasing in $\theta$? (or are there any extra restrictions, such as convexity or concavity, which might ensure that the statement is correct?). I've tried some examples, like $X \sim $ Normal$(\mu,1)$ along with some increasing transformations, and it seems to work. I've started taking derivatives inside the integral, but I haven't succeeded in proving it. Any help would be much appreciated, thanks!",,['probability']
48,Supermartingale variance bound?,Supermartingale variance bound?,,"Suppose I have a supermartingale $$ \mathbb{E}[X_{n+1} \mid X_n,  \dots , X_2, X_1] \leq X_n $$ There are 2 other conditions: bounded difference: $|X_{n+1} - X_n| \leq A$ with probability $1$ (almost surely) for all $n$, where $A$ is a constant all $X_n$ are lower bounded: $X_n \geq B$ with probability $1$ (almost surely) for all $n$, where $B$ is a constant My question is, is there a way to bound the variance $\mathrm{Var}[X_n]$ for all $n$? Ideally the bound on the variance $\mathrm{Var}[X_n]$ should $\to 0$ as $n \to \infty$ and $A \to 0$. So are there other (possibly known) conditions that are needed?","Suppose I have a supermartingale $$ \mathbb{E}[X_{n+1} \mid X_n,  \dots , X_2, X_1] \leq X_n $$ There are 2 other conditions: bounded difference: $|X_{n+1} - X_n| \leq A$ with probability $1$ (almost surely) for all $n$, where $A$ is a constant all $X_n$ are lower bounded: $X_n \geq B$ with probability $1$ (almost surely) for all $n$, where $B$ is a constant My question is, is there a way to bound the variance $\mathrm{Var}[X_n]$ for all $n$? Ideally the bound on the variance $\mathrm{Var}[X_n]$ should $\to 0$ as $n \to \infty$ and $A \to 0$. So are there other (possibly known) conditions that are needed?",,"['probability', 'probability-theory', 'reference-request', 'martingales', 'variance']"
49,Regular conditional probability as a limit,Regular conditional probability as a limit,,"The ""Alternate definition"" section of the current version of the Wikipedia article on Regular conditional probability describes an approach to conditional probability as a limiting process, in a vein similar to the intuitive description often encountered in introductory courses in probability, namely $$ P(A|X=x) = \lim_{h\downarrow 0} \frac{P(A\cap\{X\in(x-h,x+h)\})}{P(X\in(x-h,x+h))}. $$ Unfortunately, the article does not cite a reference for this alternate definition. The sole reference attached to the article is [1], however, to my best judgment, the alternate definition does not feature there. I would appreciate a reference to a source, such as a published article, a textbook, or class notes, where this alternate definition is presented and studied. It might be that [1] actually does discusses the alternate definition but that I have missed or didn't realize it, in which case I'd appreciate to be stood corrected: where in the article is it brought up? References [1] D. Leao Jr., M. Fragoso, P. Ruffino, Regular Conditional Probability, Disintegration of Probability and Radon Spaces , Proyecciones, Journal of Mathematics, Vol. 23, No 1, pp. 15-29, May 2004. ( pdf )","The ""Alternate definition"" section of the current version of the Wikipedia article on Regular conditional probability describes an approach to conditional probability as a limiting process, in a vein similar to the intuitive description often encountered in introductory courses in probability, namely $$ P(A|X=x) = \lim_{h\downarrow 0} \frac{P(A\cap\{X\in(x-h,x+h)\})}{P(X\in(x-h,x+h))}. $$ Unfortunately, the article does not cite a reference for this alternate definition. The sole reference attached to the article is [1], however, to my best judgment, the alternate definition does not feature there. I would appreciate a reference to a source, such as a published article, a textbook, or class notes, where this alternate definition is presented and studied. It might be that [1] actually does discusses the alternate definition but that I have missed or didn't realize it, in which case I'd appreciate to be stood corrected: where in the article is it brought up? References [1] D. Leao Jr., M. Fragoso, P. Ruffino, Regular Conditional Probability, Disintegration of Probability and Radon Spaces , Proyecciones, Journal of Mathematics, Vol. 23, No 1, pp. 15-29, May 2004. ( pdf )",,"['probability', 'probability-theory', 'reference-request']"
50,Does the following condition suffice to imply convergence in distribution?,Does the following condition suffice to imply convergence in distribution?,,"We know that convergence in distribution of random variables can be characterized as follows: Suppose $\{X_n\}$ and $X$ are defined on the same probability space. $X_n \stackrel{d}{\longrightarrow} X$ if and only if for any bounded and continuous function $f$,  $$ \mathbb{E}[f(X_n)] \rightarrow \mathbb{E}[f(X)] $$ My Question : Suppose that for any bounded continuous function $f$ and any bounded random variable $Y$, we have $$ \mathbb{E}[f(X_n)Y] \rightarrow \mathbb{E}[f(X)Y] $$ Then we readily have that for any random variable $Y$,  $$ \mathbb{E}[f(X_n)g(Y)] \rightarrow \mathbb{E}[f(X)g(Y)] $$ holds for all bounded continuous functions $f$ and $g$. Following the result, can we conclude that for any random variable $Y$, $(X_n, Y) \stackrel{d}{\longrightarrow} (X,Y)$, that is, for any $\varphi \in C_b(\mathbb{R}^2)$,  $$ \mathbb{E}[\varphi(X_n, Y)] \rightarrow \mathbb{E}[\varphi(X, Y)] $$ holds? It seems that if any $\varphi \in C_b(\mathbb{R}^2)$ can be approximated uniformly by product of two functions in $C_b(\mathbb{R})$, we can conclude so. But I am not sure if such topological argument is true.. Any hint will be greatly appreciated!","We know that convergence in distribution of random variables can be characterized as follows: Suppose $\{X_n\}$ and $X$ are defined on the same probability space. $X_n \stackrel{d}{\longrightarrow} X$ if and only if for any bounded and continuous function $f$,  $$ \mathbb{E}[f(X_n)] \rightarrow \mathbb{E}[f(X)] $$ My Question : Suppose that for any bounded continuous function $f$ and any bounded random variable $Y$, we have $$ \mathbb{E}[f(X_n)Y] \rightarrow \mathbb{E}[f(X)Y] $$ Then we readily have that for any random variable $Y$,  $$ \mathbb{E}[f(X_n)g(Y)] \rightarrow \mathbb{E}[f(X)g(Y)] $$ holds for all bounded continuous functions $f$ and $g$. Following the result, can we conclude that for any random variable $Y$, $(X_n, Y) \stackrel{d}{\longrightarrow} (X,Y)$, that is, for any $\varphi \in C_b(\mathbb{R}^2)$,  $$ \mathbb{E}[\varphi(X_n, Y)] \rightarrow \mathbb{E}[\varphi(X, Y)] $$ holds? It seems that if any $\varphi \in C_b(\mathbb{R}^2)$ can be approximated uniformly by product of two functions in $C_b(\mathbb{R})$, we can conclude so. But I am not sure if such topological argument is true.. Any hint will be greatly appreciated!",,"['probability', 'measure-theory']"
51,Is it justifiable to call the probability mass function by the name “discrete probability density function”?,Is it justifiable to call the probability mass function by the name “discrete probability density function”?,,"Commonly, the probability density function (PDF) is used when dealing with continuous random variables, while the probability mass function (PMF) is used for discrete random variables. This is the reason they are called “density function” and “mass function” respectively. However, my professor would talk about a “continuous PDF” and a “discrete PDF”, instead of a PDF and PMF. It seems that my professor is not the only one to use the term “discrete probability density function”. It is also used in these UBC lecture notes . Is it correct to call the probability mass function by the name “discrete probability density function”?","Commonly, the probability density function (PDF) is used when dealing with continuous random variables, while the probability mass function (PMF) is used for discrete random variables. This is the reason they are called “density function” and “mass function” respectively. However, my professor would talk about a “continuous PDF” and a “discrete PDF”, instead of a PDF and PMF. It seems that my professor is not the only one to use the term “discrete probability density function”. It is also used in these UBC lecture notes . Is it correct to call the probability mass function by the name “discrete probability density function”?",,"['probability', 'probability-theory', 'random-variables', 'terminology', 'density-function']"
52,chances of a group being all of the same sex [closed],chances of a group being all of the same sex [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question I was wondering, if there are 10 girls and 10 boys in a classroom, and they were randomly assigned in groups of four, what are the chances of there being a group with all people inside it the same sex (all boys, for example?) If possible, give the explanation and the result clearly visible a part from the rest, in percent. Example, 'there is a 20% chance for 1 group, 5 % for two', etc.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question I was wondering, if there are 10 girls and 10 boys in a classroom, and they were randomly assigned in groups of four, what are the chances of there being a group with all people inside it the same sex (all boys, for example?) If possible, give the explanation and the result clearly visible a part from the rest, in percent. Example, 'there is a 20% chance for 1 group, 5 % for two', etc.",,"['probability', 'statistics']"
53,Coupon collection with trading of doubles,Coupon collection with trading of doubles,,"I've been answering old unanswered coupon collection questions recently, and in thinking what other variations might be interesting I came up with this: There are $n$ coupon types. You successively draw coupons of independently uniformly distributed types. Your friend will give you any type of coupon for a pair of  coupons of one type. What is the expected number of draws until you can have a complete set of all types? There's no point in trading away doubles, so we can assume that you trade if you have a triple (thereby converting it into two singles). We can consider two variations: a) You can trade whenever you like. b) You have to trade immediately when you get a triple. a) is more favourable, since you can wait until the end to get exactly the coupon types you're still missing, whereas in b) you might trade for a coupon type that you'll draw afterwards anyway. The setup seems simple enough, but I don't see how to get an expression for the expected number. Unlike in the standard coupon collector's problem, a continuous approximation is straightforward. Let $m(t)$, $s(t)$ and $d(t)$ denote the fractions of missing, single and double coupon types, respectively, at time $t$, with the initial conditions $m(0)=1$, $s(0)=d(0)=0$. We can approximate the discrete procedure by a continuous process governed by differential equations. In a), we can consider any triple to be immediately converted to a single plus a pair that we can trade as soon as the number of pairs is equal to the number of missing types. Then $m'=-m$, with solution $m=\mathrm e^{-t}$. Also $s'=m+d-s$ and $d'=s-d$, so $(s-d)'=m-2(s-d)$, with solution $s-d=\mathrm e^{-t}-\mathrm e^{-2t}$. For the fraction $p$ of pairs, $p'=d$ with $p(0)=0$, so, since $d'=s-d$ and $d(0)=0$, we get $p$ by twice integrating $\mathrm e^{-t}-\mathrm e^{-2t}$, yielding $p=\frac12t-\frac34+\mathrm e^{-t}-\frac14\mathrm e^{-2t}$. The process ends when $p=m$, that is, when $\mathrm e^{-2t}=2t-3$, which occurs at $t=\frac12\left(W\left(\mathrm e^{-3}\right)+3\right)\approx1.52374$ (where $W$ is the Lambert W function). In b), where a triple is immediately converted into two singles, $$ \pmatrix{m\\s\\d}'=\pmatrix{-1&0&-1\\1&-1&2\\0&1&-1}\pmatrix{m\\s\\d}\;. $$ Diagonalising yields the solution $$ \pmatrix{m\\s\\d}=\pmatrix{-2+\sqrt5\\\frac12(1-\sqrt5)\\\frac12(3-\sqrt5)}\frac{\mathrm e^{-\frac12(3+\sqrt5)t}}{\sqrt5}+\pmatrix{2+\sqrt5\\\frac12(-1-\sqrt5)\\\frac12(-3-\sqrt5)}\frac{\mathrm e^{-\frac12(3-\sqrt5)t}}{\sqrt5}+\pmatrix{-1\\1\\1}\;, $$ so the process ends with $m=0$ when $(-2+\sqrt5)\mathrm e^{-\frac12(3+\sqrt5)t}+(2+\sqrt5)\mathrm e^{-\frac12(3-\sqrt5)t}=\sqrt5$, which occurs at $t\approx1.67614$ ( Wolfram|Alpha computation ). Simulations suggest that in both cases, for $n\to\infty$ the expected number of draws (normalised by $n$) converges to the values from these continuous approximations. ( Here's the code. ) I'd be interested in any ideas (for one or both of the variations) for obtaining the expected number of draws or the distribution of the number of draws, or at least obtaining the limits derived above combinatorially rather than by solving differential equations.","I've been answering old unanswered coupon collection questions recently, and in thinking what other variations might be interesting I came up with this: There are $n$ coupon types. You successively draw coupons of independently uniformly distributed types. Your friend will give you any type of coupon for a pair of  coupons of one type. What is the expected number of draws until you can have a complete set of all types? There's no point in trading away doubles, so we can assume that you trade if you have a triple (thereby converting it into two singles). We can consider two variations: a) You can trade whenever you like. b) You have to trade immediately when you get a triple. a) is more favourable, since you can wait until the end to get exactly the coupon types you're still missing, whereas in b) you might trade for a coupon type that you'll draw afterwards anyway. The setup seems simple enough, but I don't see how to get an expression for the expected number. Unlike in the standard coupon collector's problem, a continuous approximation is straightforward. Let $m(t)$, $s(t)$ and $d(t)$ denote the fractions of missing, single and double coupon types, respectively, at time $t$, with the initial conditions $m(0)=1$, $s(0)=d(0)=0$. We can approximate the discrete procedure by a continuous process governed by differential equations. In a), we can consider any triple to be immediately converted to a single plus a pair that we can trade as soon as the number of pairs is equal to the number of missing types. Then $m'=-m$, with solution $m=\mathrm e^{-t}$. Also $s'=m+d-s$ and $d'=s-d$, so $(s-d)'=m-2(s-d)$, with solution $s-d=\mathrm e^{-t}-\mathrm e^{-2t}$. For the fraction $p$ of pairs, $p'=d$ with $p(0)=0$, so, since $d'=s-d$ and $d(0)=0$, we get $p$ by twice integrating $\mathrm e^{-t}-\mathrm e^{-2t}$, yielding $p=\frac12t-\frac34+\mathrm e^{-t}-\frac14\mathrm e^{-2t}$. The process ends when $p=m$, that is, when $\mathrm e^{-2t}=2t-3$, which occurs at $t=\frac12\left(W\left(\mathrm e^{-3}\right)+3\right)\approx1.52374$ (where $W$ is the Lambert W function). In b), where a triple is immediately converted into two singles, $$ \pmatrix{m\\s\\d}'=\pmatrix{-1&0&-1\\1&-1&2\\0&1&-1}\pmatrix{m\\s\\d}\;. $$ Diagonalising yields the solution $$ \pmatrix{m\\s\\d}=\pmatrix{-2+\sqrt5\\\frac12(1-\sqrt5)\\\frac12(3-\sqrt5)}\frac{\mathrm e^{-\frac12(3+\sqrt5)t}}{\sqrt5}+\pmatrix{2+\sqrt5\\\frac12(-1-\sqrt5)\\\frac12(-3-\sqrt5)}\frac{\mathrm e^{-\frac12(3-\sqrt5)t}}{\sqrt5}+\pmatrix{-1\\1\\1}\;, $$ so the process ends with $m=0$ when $(-2+\sqrt5)\mathrm e^{-\frac12(3+\sqrt5)t}+(2+\sqrt5)\mathrm e^{-\frac12(3-\sqrt5)t}=\sqrt5$, which occurs at $t\approx1.67614$ ( Wolfram|Alpha computation ). Simulations suggest that in both cases, for $n\to\infty$ the expected number of draws (normalised by $n$) converges to the values from these continuous approximations. ( Here's the code. ) I'd be interested in any ideas (for one or both of the variations) for obtaining the expected number of draws or the distribution of the number of draws, or at least obtaining the limits derived above combinatorially rather than by solving differential equations.",,"['probability', 'combinatorics', 'coupon-collector']"
54,Radon-Nikodym on a Process wrt to filtration,Radon-Nikodym on a Process wrt to filtration,,"Given a probability space $(\Omega,\mathcal{F},P)$. Let $(X_t)_{t\geq0}$ be a stochastic process defined on it with cadlag paths, lets say on $(\mathcal{X},\mathcal{B}(X))$. Let be $\mathcal{F}_{t}$ be the natural filtration of $(X_t)$. Assume that $Q$ is a $\sigma$-finite measure on $\mathcal{F}_t$ for each $t\geq 0$. Denote $P^t$ the restriction of $P$ to $\mathcal{F}_t$. Then by Radon-Nikodym on $\mathcal{F}_t$ is defined by $$ L(\mathcal{F}_t)(\omega)=\frac{dP^{t}}{dQ^{t}}(\omega) \tag 1 $$ on $(\Omega,\mathcal{F})$ and this expression is $\mathcal{F}_t$ measurable. Note this is a martingale so for $s<t$ we have $\mathcal{F}_s\subseteq \mathcal{F}_t$ and thus $$ L(\mathcal{F}_s)=E_{Q}[L(\mathcal{F}_t)|\mathcal{F}_s] $$ Let $X:=(X_s)_{s\in[0,t]}$ be the trajectory on $[0,t]$. Then we have with the pushforward measure/induced measure $P^{X}$ on $(\mathcal{X},\mathcal{B}(X))$ the equality $$ L(\mathcal{F}_t)(\omega)=\frac{dP^{t}}{dQ^{t}}(\omega)=\frac{dP^{X}}{dQ^{X}} (X(\omega))\quad Q-a.s. $$ by theorem of measure transformation. How is this setup change, when we take the right continuous filtration $\tilde{\mathcal{F}}_t=\bigcap_{s>t}\sigma(X_u:u\leq s)$. This is often done in statistical inference for stochastic processes. Lets define the radon nikodym derivative like above with respect to this filtration. Can we get a representation of $L(\tilde{\mathcal{F}}_t)$ analogous to the one above? Is $$ L(\tilde{\mathcal{F}}_t)(\omega)=\frac{dP^{X(\omega)}}{dQ^{X(\omega)}} \tag2 $$ still valid? Note $X_t$ is cadlag.  Or is this defined otherwise? In many applications (change of measure theory of stochastic processes) the likelihood-functions i've seen coincide. Best regards","Given a probability space $(\Omega,\mathcal{F},P)$. Let $(X_t)_{t\geq0}$ be a stochastic process defined on it with cadlag paths, lets say on $(\mathcal{X},\mathcal{B}(X))$. Let be $\mathcal{F}_{t}$ be the natural filtration of $(X_t)$. Assume that $Q$ is a $\sigma$-finite measure on $\mathcal{F}_t$ for each $t\geq 0$. Denote $P^t$ the restriction of $P$ to $\mathcal{F}_t$. Then by Radon-Nikodym on $\mathcal{F}_t$ is defined by $$ L(\mathcal{F}_t)(\omega)=\frac{dP^{t}}{dQ^{t}}(\omega) \tag 1 $$ on $(\Omega,\mathcal{F})$ and this expression is $\mathcal{F}_t$ measurable. Note this is a martingale so for $s<t$ we have $\mathcal{F}_s\subseteq \mathcal{F}_t$ and thus $$ L(\mathcal{F}_s)=E_{Q}[L(\mathcal{F}_t)|\mathcal{F}_s] $$ Let $X:=(X_s)_{s\in[0,t]}$ be the trajectory on $[0,t]$. Then we have with the pushforward measure/induced measure $P^{X}$ on $(\mathcal{X},\mathcal{B}(X))$ the equality $$ L(\mathcal{F}_t)(\omega)=\frac{dP^{t}}{dQ^{t}}(\omega)=\frac{dP^{X}}{dQ^{X}} (X(\omega))\quad Q-a.s. $$ by theorem of measure transformation. How is this setup change, when we take the right continuous filtration $\tilde{\mathcal{F}}_t=\bigcap_{s>t}\sigma(X_u:u\leq s)$. This is often done in statistical inference for stochastic processes. Lets define the radon nikodym derivative like above with respect to this filtration. Can we get a representation of $L(\tilde{\mathcal{F}}_t)$ analogous to the one above? Is $$ L(\tilde{\mathcal{F}}_t)(\omega)=\frac{dP^{X(\omega)}}{dQ^{X(\omega)}} \tag2 $$ still valid? Note $X_t$ is cadlag.  Or is this defined otherwise? In many applications (change of measure theory of stochastic processes) the likelihood-functions i've seen coincide. Best regards",,"['probability', 'probability-theory', 'statistics', 'stochastic-processes', 'stochastic-calculus']"
55,Suppose $E[X_1] <\infty$. Show that $\lim_{n\rightarrow \infty} \frac{X_n}{S_n}=0$ a.s.,Suppose . Show that  a.s.,E[X_1] <\infty \lim_{n\rightarrow \infty} \frac{X_n}{S_n}=0,"Let $X_1,X_2,X_3,...$ be i.i.d. with $P(X_1 >0)=1$. Define $S_n =\Sigma_{i=1}^{n} X_i$. (a) Suppose $\mathbb{E}[X_1] <\infty$. Show that $\lim_{n\rightarrow \infty} \frac{X_n}{S_n}=0$ a.s. I think here $\frac{X_n}{S_n}=\frac{n}{S_n}\times \frac{X_n}{n}$ and $\frac{X_n}{n}\rightarrow 0$ because here $X_n$ is a $L^{1}$ function. I don't think it's rigorous, or may be even wrong. (b) Constrct an example in which $\mathbb{E}[X_1] =\infty$ and $\lim_{n\rightarrow \infty} \frac{X_n}{S_n}=0$ a.s is false. No idea about the second part.","Let $X_1,X_2,X_3,...$ be i.i.d. with $P(X_1 >0)=1$. Define $S_n =\Sigma_{i=1}^{n} X_i$. (a) Suppose $\mathbb{E}[X_1] <\infty$. Show that $\lim_{n\rightarrow \infty} \frac{X_n}{S_n}=0$ a.s. I think here $\frac{X_n}{S_n}=\frac{n}{S_n}\times \frac{X_n}{n}$ and $\frac{X_n}{n}\rightarrow 0$ because here $X_n$ is a $L^{1}$ function. I don't think it's rigorous, or may be even wrong. (b) Constrct an example in which $\mathbb{E}[X_1] =\infty$ and $\lim_{n\rightarrow \infty} \frac{X_n}{S_n}=0$ a.s is false. No idea about the second part.",,"['probability', 'probability-theory', 'probability-distributions', 'law-of-large-numbers']"
56,Probability that one part of a randomly cut equilateral triangle covers the other without flipping,Probability that one part of a randomly cut equilateral triangle covers the other without flipping,,"At Probability that one part of a randomly cut equilateral triangle covers the other , the case with flipping allowed was quickly solved. The case without flipping seems more difficult and hasn't been adressed, so I'm posting it as a separate question: What is the probability that randomly cutting an equilateral triangle will allow one part to cover the other if you're not allowed to flip the parts? The cuts are distributed according to Jaynes' solution to the Bertrand ""paradox"" : random straws thrown from afar, with uniformly distributed directions and uniformly distributed coordinates perpendicular to their direction. A succinct characterisation of the cuts that allow one part to cover the other would already constitute significant progress.","At Probability that one part of a randomly cut equilateral triangle covers the other , the case with flipping allowed was quickly solved. The case without flipping seems more difficult and hasn't been adressed, so I'm posting it as a separate question: What is the probability that randomly cutting an equilateral triangle will allow one part to cover the other if you're not allowed to flip the parts? The cuts are distributed according to Jaynes' solution to the Bertrand ""paradox"" : random straws thrown from afar, with uniformly distributed directions and uniformly distributed coordinates perpendicular to their direction. A succinct characterisation of the cuts that allow one part to cover the other would already constitute significant progress.",,"['probability', 'triangles', 'geometric-probability']"
57,How do we formally distinguish between zero probability events that may and may not actually occur?,How do we formally distinguish between zero probability events that may and may not actually occur?,,"Consider a random variable $X$ with pdf  \begin{equation} f(x)= \begin{cases} 3/2 &\text{ if } x\in[0,1/3]\cup[2/3,1] \\ 0 & \text{ otherwise} \end{cases} \end{equation} Here, $P(X=1/6)=P(X=1/2)=0$, but $1/6$ and $1/2$ are somehow different, because the event $X=1/6$ can actually happen , but $X=1/2$ can't. How do you differentiate between values like $1/2$ that can't happen, and $1/6$ that can, and what do I need to know about a random variable in order to know whether a value is zero probability of one type or the other?","Consider a random variable $X$ with pdf  \begin{equation} f(x)= \begin{cases} 3/2 &\text{ if } x\in[0,1/3]\cup[2/3,1] \\ 0 & \text{ otherwise} \end{cases} \end{equation} Here, $P(X=1/6)=P(X=1/2)=0$, but $1/6$ and $1/2$ are somehow different, because the event $X=1/6$ can actually happen , but $X=1/2$ can't. How do you differentiate between values like $1/2$ that can't happen, and $1/6$ that can, and what do I need to know about a random variable in order to know whether a value is zero probability of one type or the other?",,['probability']
58,Application of Doob's optional stopping theorem to an elementary probability problem,Application of Doob's optional stopping theorem to an elementary probability problem,,"The elementary probability problem is as follows. Let $(X_k)_{k\in\mathbb{N}}$ be a sequence of i.i.d. random variables such that $X_k \sim U(0,1)$ for each $k$. Define $\tau := \inf\{n\geq 0: \sum_{i=1}^n X_i > 1\}$. What is $E[\tau]$? There are plenty of clever solutions to this problem. However, I would like to find a solution that utilizes Doob's optional stopping theorem. I don't know if such a solution exists by the way. I am doing this out of sheer curiosity. To make life easier I will just assume that $E[\tau] < \infty$. I can prove this later on. So I define $Y_n$ to be $$Y_n = \sum_{i=1}^n X_i$$ Then the compensated process $Z_n := Y_n - \frac{n}{2}$ is a martingale. By hypothesis we have that $Y_{\tau} > 1$ and $Y_{\tau - i} \leq 1$ for $i = 1,2,\ldots,\tau-1$. By optional stopping we have $E[Z_{\tau}] = 0$ (There are a few intermediate steps here that I did not mention but the statement is fine). This yields the bounds $$E[\tau] > 2 \qquad E[\tau] \leq 3$$ Of course we know that $E[\tau] = \exp(1)$. At least the bounds make sense but how do I make them tighter, better yet how do I make the lower and the upper bounds equal?","The elementary probability problem is as follows. Let $(X_k)_{k\in\mathbb{N}}$ be a sequence of i.i.d. random variables such that $X_k \sim U(0,1)$ for each $k$. Define $\tau := \inf\{n\geq 0: \sum_{i=1}^n X_i > 1\}$. What is $E[\tau]$? There are plenty of clever solutions to this problem. However, I would like to find a solution that utilizes Doob's optional stopping theorem. I don't know if such a solution exists by the way. I am doing this out of sheer curiosity. To make life easier I will just assume that $E[\tau] < \infty$. I can prove this later on. So I define $Y_n$ to be $$Y_n = \sum_{i=1}^n X_i$$ Then the compensated process $Z_n := Y_n - \frac{n}{2}$ is a martingale. By hypothesis we have that $Y_{\tau} > 1$ and $Y_{\tau - i} \leq 1$ for $i = 1,2,\ldots,\tau-1$. By optional stopping we have $E[Z_{\tau}] = 0$ (There are a few intermediate steps here that I did not mention but the statement is fine). This yields the bounds $$E[\tau] > 2 \qquad E[\tau] \leq 3$$ Of course we know that $E[\tau] = \exp(1)$. At least the bounds make sense but how do I make them tighter, better yet how do I make the lower and the upper bounds equal?",,"['probability', 'probability-theory', 'martingales', 'stopping-times']"
59,What is the probability of a pen touching a bar given that the length of the pen is $10$ cm and the bars are regularly spaced at $15$ cm?,What is the probability of a pen touching a bar given that the length of the pen is  cm and the bars are regularly spaced at  cm?,10 15,"Problem: If a pen of length $10$ cm is thrown out of infinitely large window having vertical bars regularly spaced at $15$ cm, then find the probability that it will touch any of the bars. (Assume that the vertical bars are infinitely long) I don't know how to even begin with the question, so any help will be appreciated. All I know is that integration is involved in the calculation of this probability, but I don't know what to integrate, since the pen can be thrown anywhere, and at any angle.","Problem: If a pen of length $10$ cm is thrown out of infinitely large window having vertical bars regularly spaced at $15$ cm, then find the probability that it will touch any of the bars. (Assume that the vertical bars are infinitely long) I don't know how to even begin with the question, so any help will be appreciated. All I know is that integration is involved in the calculation of this probability, but I don't know what to integrate, since the pen can be thrown anywhere, and at any angle.",,"['probability', 'contest-math']"
60,What is the probability that 1 woman and 2 men are chosen if the following is given?,What is the probability that 1 woman and 2 men are chosen if the following is given?,,"In a classroom, there are 8 women and 5 men. A committee of 3 people is to be formed for a project. What is the probability that 1 woman and 2 men are chosen? For this problem, the directions say for me to use permutation or combination. I used combination as order doesn't matter. So $$ P(1W \cap 2M) = \frac{(8C1)(5C2)}{13C3} $$ but the answer I got was wrong so I think I did the whole process wrong except the denominator. How would I solve this?","In a classroom, there are 8 women and 5 men. A committee of 3 people is to be formed for a project. What is the probability that 1 woman and 2 men are chosen? For this problem, the directions say for me to use permutation or combination. I used combination as order doesn't matter. So $$ P(1W \cap 2M) = \frac{(8C1)(5C2)}{13C3} $$ but the answer I got was wrong so I think I did the whole process wrong except the denominator. How would I solve this?",,"['probability', 'combinatorics']"
61,"How to tell who is the stronger chess player, for the purpose of fine tuning chess engines? [closed]","How to tell who is the stronger chess player, for the purpose of fine tuning chess engines? [closed]",,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 8 years ago . Improve this question This is a chess like question concerning how many games are required to tell who is the stronger player. The application is in fine tuning chess engines. It is typical that during this process a chess engine will play some games with a modified version of itself. After the tests the most successful engine is used as a baseline and it again plays a series of games with a modified version of itself. This is perhaps best described as an incremental approach to fine tuning. The problem is how many games do you have to play to have statistical valid data from the games when the engines may be very closely matched in ability? I had a stab at this using the score (1 point for a win, 0.5 points for a draw and 0 for a loss) and a Chi squared table with 3 degrees of freedom and calculating what was required. I came up with this (small excerpt): Games     95%*      97.5%*      99%*        99.5%*      99.9%* 5       4.5 / 0.5   - / -       - / -       - / -       - / - 6       5.0 / 1.0   5.5 / 0.5   - / -       - / -       - / - 8       6.5 / 1.5   7.0 / 1.0   7.5 / 0.5   - / -       - / - 9       7.0 / 2.0   7.5 / 1.5   8.0 / 1.0   8.5 / 0.5   - / - 12      9.0 / 3.0   9.5 / 2.5   10.0 / 2.0  10.5 / 1.5  11.5 / 0.5 * confidence level Which I intend to use like this: if I played 5 games and the result is 4.5 to 0.5 or better then I have 95% confidence I found the stronger engine. If I played 6 games and the result is 4.5 to 1.5 or worse then I cannot tell which is the stronger engine with 95% confidence. Have I done this the correct way? Should I have awarded 1 point for the draw also? Or used 2 degrees of freedom? Or a different approach altogether?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 8 years ago . Improve this question This is a chess like question concerning how many games are required to tell who is the stronger player. The application is in fine tuning chess engines. It is typical that during this process a chess engine will play some games with a modified version of itself. After the tests the most successful engine is used as a baseline and it again plays a series of games with a modified version of itself. This is perhaps best described as an incremental approach to fine tuning. The problem is how many games do you have to play to have statistical valid data from the games when the engines may be very closely matched in ability? I had a stab at this using the score (1 point for a win, 0.5 points for a draw and 0 for a loss) and a Chi squared table with 3 degrees of freedom and calculating what was required. I came up with this (small excerpt): Games     95%*      97.5%*      99%*        99.5%*      99.9%* 5       4.5 / 0.5   - / -       - / -       - / -       - / - 6       5.0 / 1.0   5.5 / 0.5   - / -       - / -       - / - 8       6.5 / 1.5   7.0 / 1.0   7.5 / 0.5   - / -       - / - 9       7.0 / 2.0   7.5 / 1.5   8.0 / 1.0   8.5 / 0.5   - / - 12      9.0 / 3.0   9.5 / 2.5   10.0 / 2.0  10.5 / 1.5  11.5 / 0.5 * confidence level Which I intend to use like this: if I played 5 games and the result is 4.5 to 0.5 or better then I have 95% confidence I found the stronger engine. If I played 6 games and the result is 4.5 to 1.5 or worse then I cannot tell which is the stronger engine with 95% confidence. Have I done this the correct way? Should I have awarded 1 point for the draw also? Or used 2 degrees of freedom? Or a different approach altogether?",,"['probability', 'statistics']"
62,St. Petersburg's Paradox elaboration [closed],St. Petersburg's Paradox elaboration [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Find N (the entrance fee in dollars) such that you would expect to break even if you played the St. Petersburg's lottery every minute of every day of your life. For more information on the St. Petersburg lottery, see the Wikipedia entry: https://en.wikipedia.org/wiki/St._Petersburg_paradox Edit I'm going to assume the topic-closers closed this topic because they weren't sure which question I was asking.  That is, break even and stop, or break even and keep playing anyway. Both.  Or neither.  Or pick your own version!!!","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Find N (the entrance fee in dollars) such that you would expect to break even if you played the St. Petersburg's lottery every minute of every day of your life. For more information on the St. Petersburg lottery, see the Wikipedia entry: https://en.wikipedia.org/wiki/St._Petersburg_paradox Edit I'm going to assume the topic-closers closed this topic because they weren't sure which question I was asking.  That is, break even and stop, or break even and keep playing anyway. Both.  Or neither.  Or pick your own version!!!",,"['probability', 'statistics']"
63,Uncountable random graphs,Uncountable random graphs,,"There is a theorem from Erdos and Renyi that says that a random graph on $\aleph_0$ vertices (where each pair of vertices is connected with probability equal to $\frac{1}{2}$) will be isomorphic to the Rado graph with probability 1.  Does an analogous result hold for random graphs on $\kappa$ vertices, for $\kappa > \aleph_0$?  The proof I have read of the Erdos-Renyi theorem does not appear to generalize to larger cardinals. More generally, how does one compute probabilities in cases like this (where, in general, the sample space and the event are both sets of the same cardinality $\lambda > 2^{\aleph_0}$.  What sorts of arguments are used to show that such an event has probability $0$ or $1$.  If anyone has an example of such an problem and solution, I would appreciate it!","There is a theorem from Erdos and Renyi that says that a random graph on $\aleph_0$ vertices (where each pair of vertices is connected with probability equal to $\frac{1}{2}$) will be isomorphic to the Rado graph with probability 1.  Does an analogous result hold for random graphs on $\kappa$ vertices, for $\kappa > \aleph_0$?  The proof I have read of the Erdos-Renyi theorem does not appear to generalize to larger cardinals. More generally, how does one compute probabilities in cases like this (where, in general, the sample space and the event are both sets of the same cardinality $\lambda > 2^{\aleph_0}$.  What sorts of arguments are used to show that such an event has probability $0$ or $1$.  If anyone has an example of such an problem and solution, I would appreciate it!",,"['probability', 'graph-theory', 'infinitary-combinatorics']"
64,Bingo probability of a tie with 20 players,Bingo probability of a tie with 20 players,,"Assume ""standard"" bingo (75 numbers) with the columns ranging the following inclusive ""semi-random"" values B: 1 to 15, I: 16 to 30, N: 31 to 45, G: 46 to 60, O: 61 to 75. By semi-random I mean restricted to a small range (15 at a time).  There is a free space in the middle of the 5x5 playing board.  Numbers (from 1 to 75 inclusive) are randomly drawn one a time without replacement (without any repeats in each game) and with equal probability of being drawn.  A win (Bingo) is defined as a completed line segment of 5 adjacent squares made only from the drawn numbers but which may include the free space (and must include it if it is beneficial).  A bingo card has 25 of these board squares arranged in a 5x5 matrix. So my question is if there are 20 players, each with a unique playing card (randomly generated by computer out of I think 552 septillion possible playing cards), what are the chances/probability that 2 or more players will get Bingo on the same drawn number?.  For example, someone could win bingo with as few as 4 drawn numbers but likely it would take much more.  So I am asking if balls are drawn until at least one person wins, what are the chances that at least 2 people will win at the same time?  The game is considered finished / decided when there is at least 1 winner for that game.  You can assume that all players are good enough not to make any mistakes (not true in real bingo but assume here). I am not sure how to set this up mathematically and because there are so many possible bingo cards, computer simulation of all of them is not a good idea.  Perhaps what can be done with simulation is to first simulate 20 legitimate bingo cards (out of 552 septillion), and then have the computer draw one random number at a time until we have at least 1 winner.  Do this for maybe 1 million trials and count how many have simultaneous multiwinners.  For example, after 11 balls drawn there are no winners yet for that game but on the 12 drawn ball, there are 2 or more winners.  I would like to know how often the multiwinner situation occurs. I could probably do the simulation with a fair amount of work but wanted to know if this problem can be done mathematically or if is too difficult to set up. One concern I see is that if one card has for its first column (the B column), from top to bottom, 1, 4, 7, 10, and 15 and some other card has for column B (also from top to bottom), 15, 10, 7, 4, and 1 in that order.  Problem there is even though the cards have different order for the B column numbers, if those 5 numbers are drawn, both players may win at the same time.  So my point is it makes a difference if we say (or not) that multiple bingo cards cannot have the same exact 5 numbers in a row, column, or diagonal but just in a different order.  That might be an interesting problem in itself to figure out how many fewer ""legit"" bingo cards there are with that constraint so someone could comment about it but the actual question is about the multiwinner probability.  I think the answer to the no permutation bingo card restriction is about 111 quadrillion legit bingo cards.","Assume ""standard"" bingo (75 numbers) with the columns ranging the following inclusive ""semi-random"" values B: 1 to 15, I: 16 to 30, N: 31 to 45, G: 46 to 60, O: 61 to 75. By semi-random I mean restricted to a small range (15 at a time).  There is a free space in the middle of the 5x5 playing board.  Numbers (from 1 to 75 inclusive) are randomly drawn one a time without replacement (without any repeats in each game) and with equal probability of being drawn.  A win (Bingo) is defined as a completed line segment of 5 adjacent squares made only from the drawn numbers but which may include the free space (and must include it if it is beneficial).  A bingo card has 25 of these board squares arranged in a 5x5 matrix. So my question is if there are 20 players, each with a unique playing card (randomly generated by computer out of I think 552 septillion possible playing cards), what are the chances/probability that 2 or more players will get Bingo on the same drawn number?.  For example, someone could win bingo with as few as 4 drawn numbers but likely it would take much more.  So I am asking if balls are drawn until at least one person wins, what are the chances that at least 2 people will win at the same time?  The game is considered finished / decided when there is at least 1 winner for that game.  You can assume that all players are good enough not to make any mistakes (not true in real bingo but assume here). I am not sure how to set this up mathematically and because there are so many possible bingo cards, computer simulation of all of them is not a good idea.  Perhaps what can be done with simulation is to first simulate 20 legitimate bingo cards (out of 552 septillion), and then have the computer draw one random number at a time until we have at least 1 winner.  Do this for maybe 1 million trials and count how many have simultaneous multiwinners.  For example, after 11 balls drawn there are no winners yet for that game but on the 12 drawn ball, there are 2 or more winners.  I would like to know how often the multiwinner situation occurs. I could probably do the simulation with a fair amount of work but wanted to know if this problem can be done mathematically or if is too difficult to set up. One concern I see is that if one card has for its first column (the B column), from top to bottom, 1, 4, 7, 10, and 15 and some other card has for column B (also from top to bottom), 15, 10, 7, 4, and 1 in that order.  Problem there is even though the cards have different order for the B column numbers, if those 5 numbers are drawn, both players may win at the same time.  So my point is it makes a difference if we say (or not) that multiple bingo cards cannot have the same exact 5 numbers in a row, column, or diagonal but just in a different order.  That might be an interesting problem in itself to figure out how many fewer ""legit"" bingo cards there are with that constraint so someone could comment about it but the actual question is about the multiwinner probability.  I think the answer to the no permutation bingo card restriction is about 111 quadrillion legit bingo cards.",,['probability']
65,Book on Convergence Concepts in Probability without Measure Theory,Book on Convergence Concepts in Probability without Measure Theory,,"I am looking for a comprehensive book on Probability which discusses Convergence of Random Variables in detail, excluding portions of Measure Theory. Allan Gut's ""Probability: A Graduate Course"" seems fabulous, but it has way too much content on Measure Theory, which I do not know at all. There is a wealth of professors and undergrad students of Stats and Maths on this website; and I would like to directly ask you: which book should suit my situation? Because I CAN learn Measure Theory but considering I am only a first year undergrad, and Measure Theory will be taught to me three years later, I suppose it will only be a wastage of time.","I am looking for a comprehensive book on Probability which discusses Convergence of Random Variables in detail, excluding portions of Measure Theory. Allan Gut's ""Probability: A Graduate Course"" seems fabulous, but it has way too much content on Measure Theory, which I do not know at all. There is a wealth of professors and undergrad students of Stats and Maths on this website; and I would like to directly ask you: which book should suit my situation? Because I CAN learn Measure Theory but considering I am only a first year undergrad, and Measure Theory will be taught to me three years later, I suppose it will only be a wastage of time.",,"['probability', 'probability-theory', 'convergence-divergence', 'random-variables', 'book-recommendation']"
66,Generalized Binomial Model independent in the limit,Generalized Binomial Model independent in the limit,,"Start with a generalized binomial model $$P(X_{n+1}=1\mid \mathcal{F}_n)=\theta_n+ n^{-1} d_n \sum_{i=1}^n X_i$$ $$P(X_{n+1}=1)=p_{n+1}=\theta_n + n^{-1}d_n \sum_{i=1}^n p_i$$ With $0\leq \theta_n+ d_n <1$. Then if we impose $p_n=p_m=p$ for any $n,m$, the distribution is identical and exchangeable (given no information). Correct? This requires $$p=\theta_n + d_np \iff p = \theta_n /(1-d_n),$$ but not necessarily that $d_n$ or $\theta_n$ converge as $n\rightarrow \infty$. Yet, for an exchangeable sequence, it will be an independent sequence in the limit. This leads me to believe $d_n \rightarrow 0$. But I don't know if this is justified or how to prove it if it is. Any ideas on how to interpret this kind of probability model if we require that each $X_n\in\{0,1\}$ is identically distributed? Terminology question too: If $p_m=p_n$, would you call $X$ an exchangeable sequence with respect to $\mathcal{F}_0=\{\emptyset, \Omega\}$ ? It seems like it should be exchangeable only in some kind of ex ante sense.","Start with a generalized binomial model $$P(X_{n+1}=1\mid \mathcal{F}_n)=\theta_n+ n^{-1} d_n \sum_{i=1}^n X_i$$ $$P(X_{n+1}=1)=p_{n+1}=\theta_n + n^{-1}d_n \sum_{i=1}^n p_i$$ With $0\leq \theta_n+ d_n <1$. Then if we impose $p_n=p_m=p$ for any $n,m$, the distribution is identical and exchangeable (given no information). Correct? This requires $$p=\theta_n + d_np \iff p = \theta_n /(1-d_n),$$ but not necessarily that $d_n$ or $\theta_n$ converge as $n\rightarrow \infty$. Yet, for an exchangeable sequence, it will be an independent sequence in the limit. This leads me to believe $d_n \rightarrow 0$. But I don't know if this is justified or how to prove it if it is. Any ideas on how to interpret this kind of probability model if we require that each $X_n\in\{0,1\}$ is identically distributed? Terminology question too: If $p_m=p_n$, would you call $X$ an exchangeable sequence with respect to $\mathcal{F}_0=\{\emptyset, \Omega\}$ ? It seems like it should be exchangeable only in some kind of ex ante sense.",,"['probability', 'probability-theory', 'probability-distributions']"
67,Maximizing heads/number of flips game [duplicate],Maximizing heads/number of flips game [duplicate],,"This question already has answers here : Why is this coin-flipping probability problem unsolved? (2 answers) Expected Ratio of Coin Flips (1 answer) Closed last year . Flip a coin until you wish to stop.  Your goal is to maximize the ratio number of heads/total number of flips.  What is the expected value of this game?  Additionally, how would one play this game?","This question already has answers here : Why is this coin-flipping probability problem unsolved? (2 answers) Expected Ratio of Coin Flips (1 answer) Closed last year . Flip a coin until you wish to stop.  Your goal is to maximize the ratio number of heads/total number of flips.  What is the expected value of this game?  Additionally, how would one play this game?",,['probability']
68,Minimal number of edges removed to make a graph triangle free,Minimal number of edges removed to make a graph triangle free,,"I'm interested in finding an upper bound on the expected value of the minimal number of edges one needs to remove from a random graph $G_{n,p}$ (where each edge appears with probability $p$) in order to make it triangle free. Particularly, I would really be happy if it was of size less than $\Theta(pn^2)$.  Has anyone came across something like this?","I'm interested in finding an upper bound on the expected value of the minimal number of edges one needs to remove from a random graph $G_{n,p}$ (where each edge appears with probability $p$) in order to make it triangle free. Particularly, I would really be happy if it was of size less than $\Theta(pn^2)$.  Has anyone came across something like this?",,"['probability', 'graph-theory', 'random-graphs']"
69,Bus arrival poisson paradox,Bus arrival poisson paradox,,"I have a question about the waiting time paradox for poisson processes(in this case in terms of bus arrivals). Suppose I know that buses arrive with poisson distribution(lambda). I arrive at fixed time t. I want to figure out three things - The mean length of time I will wait until the next bus. The mean length of time since the last bus. Mean length of time between bus arrivals. I think 1 should be 1/lambda - the poisson process is memoryless, so it shouldn't matter when I arrrive. I think 3 should also be 1/lambda, because that is the mean of the exponential distribution, which determines the distribution of times between buses. I'm not sure about 2. Can anyone help/tell me if my reasoning is wrong/right?","I have a question about the waiting time paradox for poisson processes(in this case in terms of bus arrivals). Suppose I know that buses arrive with poisson distribution(lambda). I arrive at fixed time t. I want to figure out three things - The mean length of time I will wait until the next bus. The mean length of time since the last bus. Mean length of time between bus arrivals. I think 1 should be 1/lambda - the poisson process is memoryless, so it shouldn't matter when I arrrive. I think 3 should also be 1/lambda, because that is the mean of the exponential distribution, which determines the distribution of times between buses. I'm not sure about 2. Can anyone help/tell me if my reasoning is wrong/right?",,"['probability', 'probability-distributions', 'paradoxes']"
70,"Understanding the setup for the probability that $Ax^2+Bx+C$ has real roots if A, B, and C are random variables uniformly distributed over (0,1).","Understanding the setup for the probability that  has real roots if A, B, and C are random variables uniformly distributed over (0,1).",Ax^2+Bx+C,"Suppose that $A, B,$ and $C$ are independent random variables, each being uniformly distributed over $(0,1)$ . What is the probability that $Ax^2 + Bx + C$ has real roots? First, I set $P(B^2 - 4AC \ge 0)$ Then I am told that $$\begin{align} \int_0^1 \int_0^1 \int_{\min\{1, \sqrt{4ac}\}}^1 1 \;\text{d}b\,\text{d}c\,\text{d} &a= \int_0^1 \int_0^{\min(1, \frac{1}{4a})}\int_{\sqrt{4ac}}^1  1\;\text{d}b\,\text{d}c\,\text{d}a\\ &= \int_0^{\frac{1}{4}} \int_0^1 \int_{\sqrt{4ac}}^1 1\;\text{d}b\,\text{d}c\,\text{d}a + \int_{\frac{1}{4}}^1 \int_0^{\frac{1}{4a}}\int_{\sqrt{4ac}}^1 1\;\text{d}b\,\text{d}c\,\text{d}a \end{align}$$ why the middle integrate from $0$ to $\min(1, \frac{1}{4a})$ from the second integral...where does $\frac{1}{4a}$ come from? why the min{...} does not go to the front integral? why they break up into last step like this (I refer to one integral + another integral) ? Thanks a lot","Suppose that and are independent random variables, each being uniformly distributed over . What is the probability that has real roots? First, I set Then I am told that why the middle integrate from to from the second integral...where does come from? why the min{...} does not go to the front integral? why they break up into last step like this (I refer to one integral + another integral) ? Thanks a lot","A, B, C (0,1) Ax^2 + Bx + C P(B^2 - 4AC \ge 0) \begin{align}
\int_0^1 \int_0^1 \int_{\min\{1, \sqrt{4ac}\}}^1 1 \;\text{d}b\,\text{d}c\,\text{d}
&a= \int_0^1 \int_0^{\min(1, \frac{1}{4a})}\int_{\sqrt{4ac}}^1  1\;\text{d}b\,\text{d}c\,\text{d}a\\
&= \int_0^{\frac{1}{4}} \int_0^1 \int_{\sqrt{4ac}}^1 1\;\text{d}b\,\text{d}c\,\text{d}a + \int_{\frac{1}{4}}^1 \int_0^{\frac{1}{4a}}\int_{\sqrt{4ac}}^1 1\;\text{d}b\,\text{d}c\,\text{d}a
\end{align} 0 \min(1, \frac{1}{4a}) \frac{1}{4a}","['probability', 'probability-theory', 'multivariable-calculus', 'probability-distributions']"
71,$\pi$ Monte-Carlo - Probability that O-Lock hit a Spoke?,Monte-Carlo - Probability that O-Lock hit a Spoke?,\pi,"(Edit: can someone please help me migrate this to physics stack? I think they would be more interested in helping me out with this problem. Thanks.) I have a bicycle with one of those O-locks on it and too often when I park the bike and I want to lock it, the lock hits one of the spokes of the rim. This can be frustrating and surprises me that it occurs so often. I mean, the spokes are so thin and not that many really so one would think that this should not happen that often (like every day or so). And so every time this happened I reminded myself to calculate the probability of this happening . I just did it (after a year) and now I want to know what you guys think about my calculation, is it OK? This is not a textbook example so there is no answer to look up or anything, that's why I need your feedback. I have modeled the situation as shown in the figure below, where I have included one spoke only. Let the O-lock have diameter $d_{L}$ and the spoke have a diameter $d_S$. Let $R$ denote the ""radius"" from center of wheel to the point where the O-lock comes and goes (this is approximately equal to the radius of the rim). Then, we have that the corresponding angles are given by $w_S=d_S/R$ and $w_L = d_L/R$ so that the spoke will hit the lock (the shaded disk in figure below) when $\theta$ is in the interval $[0, w_L+w_S]$ modulo $2\pi$ (See figure: $R$ is fixed so the point $(R,\theta)$ is on a circle). Then the probability for hit (wheel with one spoke) is given by  $$ P(1) = \frac{w_L+w_S}{2\pi}\times 1 = \frac{1}{2\pi R}(d_L+d_S).$$ Generalizing to $N$ equally spaced spokes we get  $$ P(N) = \frac{N}{2\pi R}(d_L+d_S). $$ Example: plugging in some typical numbers; $N=36, R\approx 0.3$m, $d_S\approx 2\times 10^{-3}$m , $d_L\approx \times 10^{-2}$m we find $$P(36) \approx 0.23$$ which kind of agrees with my everyday experience of this problem. I guess one could obtain a (Monte-Carlo)-value for $\pi$ this way. Could someone tell me what I have done wrong above? Or perhaps derive the correct expression for the probability?","(Edit: can someone please help me migrate this to physics stack? I think they would be more interested in helping me out with this problem. Thanks.) I have a bicycle with one of those O-locks on it and too often when I park the bike and I want to lock it, the lock hits one of the spokes of the rim. This can be frustrating and surprises me that it occurs so often. I mean, the spokes are so thin and not that many really so one would think that this should not happen that often (like every day or so). And so every time this happened I reminded myself to calculate the probability of this happening . I just did it (after a year) and now I want to know what you guys think about my calculation, is it OK? This is not a textbook example so there is no answer to look up or anything, that's why I need your feedback. I have modeled the situation as shown in the figure below, where I have included one spoke only. Let the O-lock have diameter $d_{L}$ and the spoke have a diameter $d_S$. Let $R$ denote the ""radius"" from center of wheel to the point where the O-lock comes and goes (this is approximately equal to the radius of the rim). Then, we have that the corresponding angles are given by $w_S=d_S/R$ and $w_L = d_L/R$ so that the spoke will hit the lock (the shaded disk in figure below) when $\theta$ is in the interval $[0, w_L+w_S]$ modulo $2\pi$ (See figure: $R$ is fixed so the point $(R,\theta)$ is on a circle). Then the probability for hit (wheel with one spoke) is given by  $$ P(1) = \frac{w_L+w_S}{2\pi}\times 1 = \frac{1}{2\pi R}(d_L+d_S).$$ Generalizing to $N$ equally spaced spokes we get  $$ P(N) = \frac{N}{2\pi R}(d_L+d_S). $$ Example: plugging in some typical numbers; $N=36, R\approx 0.3$m, $d_S\approx 2\times 10^{-3}$m , $d_L\approx \times 10^{-2}$m we find $$P(36) \approx 0.23$$ which kind of agrees with my everyday experience of this problem. I guess one could obtain a (Monte-Carlo)-value for $\pi$ this way. Could someone tell me what I have done wrong above? Or perhaps derive the correct expression for the probability?",,"['probability', 'pi', 'monte-carlo']"
72,About cutting Almonds,About cutting Almonds,,"Every year, during Christmas baking, I chop almonds, which causes me to puzzle over the same question, and I don't quite know how to approach it. I start out with N almonds. Let's assume they are all the same size, so if I drew a histogram of number over size, I'd get a single spike. Now I start cutting. The first cut bisects some randomly selected almonds, at a random angle. The second and subsequent cuts may also bisect some of the pieces that result from a previous cut. Some rather irregular shapes result. The experimental experience is that after some time, there are still surprisingly many large pieces, while some of the smaller pieces have become small enough to be hard to distinguish with the eye. So my histogram has ""smeared out"". There are different ways to draw this histogram. I'm thinking volume of a particle may be best on the x-axis, and number (or perhaps number times volume) on the y-axis. The question: how does this histogram evolve over time? It starts as a single spike, and then what? Does it have inflection points? If I want no more than M particles to be smaller than a certain minimum size, after how many cuts should I stop cutting? P.S. I realize this post has a high likelihood of being marked as off-topic. Almonds? What was he thinking? But methinks it's an interesting problem to think of, and applicable by many people at this time of year who want their baking to work out just right. One year later: I cut some more almonds, and I'm just as puzzled! This year I've been pondering the shapes the pieces end up having. It appears that without having a good idea of the shapes, the original question may not be answerable. Perhaps they pieces can be approximated as being round, in which case it would be straightforward to determine the size distribution of the resulting pieces after the piece is cut.","Every year, during Christmas baking, I chop almonds, which causes me to puzzle over the same question, and I don't quite know how to approach it. I start out with N almonds. Let's assume they are all the same size, so if I drew a histogram of number over size, I'd get a single spike. Now I start cutting. The first cut bisects some randomly selected almonds, at a random angle. The second and subsequent cuts may also bisect some of the pieces that result from a previous cut. Some rather irregular shapes result. The experimental experience is that after some time, there are still surprisingly many large pieces, while some of the smaller pieces have become small enough to be hard to distinguish with the eye. So my histogram has ""smeared out"". There are different ways to draw this histogram. I'm thinking volume of a particle may be best on the x-axis, and number (or perhaps number times volume) on the y-axis. The question: how does this histogram evolve over time? It starts as a single spike, and then what? Does it have inflection points? If I want no more than M particles to be smaller than a certain minimum size, after how many cuts should I stop cutting? P.S. I realize this post has a high likelihood of being marked as off-topic. Almonds? What was he thinking? But methinks it's an interesting problem to think of, and applicable by many people at this time of year who want their baking to work out just right. One year later: I cut some more almonds, and I'm just as puzzled! This year I've been pondering the shapes the pieces end up having. It appears that without having a good idea of the shapes, the original question may not be answerable. Perhaps they pieces can be approximated as being round, in which case it would be straightforward to determine the size distribution of the resulting pieces after the piece is cut.",,"['probability', 'geometry', 'statistics']"
73,Puzzle - zero knowledge proof,Puzzle - zero knowledge proof,,"I am solving the following problem : I have edge-matching puzzles, where all pieces are squares and the grid has $n$*$n$ format. There is no global image to guide a puzzle solver. Despite the puzzles match locally, they have not to stay together also in the final solution. (it can be verified whether to edges of two pieces match in $\theta(1)$) I should prove to Bob, without revealing any useful information to him, that I have solved the puzzles. I tried to apply the approaches based on well-known zero-knowledge problems like graph-coloring or Hamilton's path in graph but I think, I am still missing something. Do you have any hints please?","I am solving the following problem : I have edge-matching puzzles, where all pieces are squares and the grid has $n$*$n$ format. There is no global image to guide a puzzle solver. Despite the puzzles match locally, they have not to stay together also in the final solution. (it can be verified whether to edges of two pieces match in $\theta(1)$) I should prove to Bob, without revealing any useful information to him, that I have solved the puzzles. I tried to apply the approaches based on well-known zero-knowledge problems like graph-coloring or Hamilton's path in graph but I think, I am still missing something. Do you have any hints please?",,"['probability', 'logic', 'puzzle', 'problem-solving']"
74,walks on hypercubes,walks on hypercubes,,"Let's say I start at the $(0,0,...,0)$ vertex of $n$-dimensional hypercube. After each unit of time $l$, I either stay where I am with probability $p$, or move to an adjacent vertice with probability $q = \frac{1-p}{n}$. What is the probability I end up back where  I started after $l$ units of time? Having difficulty wrapping my head around this question... If $p = 0$, then the answer is simply $$ \frac{1}{2^n n^l}\sum \limits_{i = 0}^{n}\binom{n}{i}(n-2i)^l$$ messing around with $n=2$ I found that for general $p$: $P(0) = 1, P(1) = p, P(2) = p^2 + 2q^2, P(3) = p^3 + 2p^2q + 4pq^2, P(4) = p^3 + 11p^2q^2 + 7q^4$. I'm not seeing any obvious pattern here, and I'm not sure how to proceed to figure out a closed form for general $n$.","Let's say I start at the $(0,0,...,0)$ vertex of $n$-dimensional hypercube. After each unit of time $l$, I either stay where I am with probability $p$, or move to an adjacent vertice with probability $q = \frac{1-p}{n}$. What is the probability I end up back where  I started after $l$ units of time? Having difficulty wrapping my head around this question... If $p = 0$, then the answer is simply $$ \frac{1}{2^n n^l}\sum \limits_{i = 0}^{n}\binom{n}{i}(n-2i)^l$$ messing around with $n=2$ I found that for general $p$: $P(0) = 1, P(1) = p, P(2) = p^2 + 2q^2, P(3) = p^3 + 2p^2q + 4pq^2, P(4) = p^3 + 11p^2q^2 + 7q^4$. I'm not seeing any obvious pattern here, and I'm not sure how to proceed to figure out a closed form for general $n$.",,"['probability', 'combinatorics']"
75,Inequality between incomplete beta and gamma functions,Inequality between incomplete beta and gamma functions,,"Let the regularized incomplete beta and gamma functions be defined as usual: \begin{equation} I_p(z,w) = \frac a {B(z,w)} \int_0^p t^{z-1} (1-t)^{w-1} \,\mathrm dt, \end{equation} \begin{equation} \gamma(r,t) = \frac 1 {\Gamma(r)} \int_0^t s^{r-1} e^{-s} \,\mathrm ds. \end{equation} Let $r \in \mathbb N$, and consider $a, b > 0$. As is well known, $I_p(r,a/p-b) \rightarrow \gamma(r,a)$ as $p \rightarrow 0$. In the context of a statistical estimation problem (I can provide details if you are interested), I have been able to prove that the difference between the incomplete beta function and its limit, \begin{equation} I_p(r,a/p-b) - \gamma(r,a), \end{equation} is positive for all $p \in (0,1)$ if $a$ is sufficiently larger than $r$, and negative if $a$ is sufficiently smaller than $r$. For example, if you have access to Matlab you can check that (note that Matlab's gammainc uses reverse order for its arguments) p = 1e-3;  r = 5; a = 10; b = 5; betainc(p,r,a/p-b) - gammainc(a,r) is positive, whereas p = 1e-3;  r = 5; a = 2; b = 5; betainc(p,r,a/p-b) - gammainc(a,r) is negative. I'd like to know the widest range of values of $a$ for which that difference is assured to be positive/negative. So, my question is: Is there a known theorem that gives sufficient conditions on $a, b, r$ that assure that $I_p(r,a/p-b) - \gamma(r,a)$ is positive/negative? Even if you don't know any such theorem, a pointer in the right direction would be appreciated.","Let the regularized incomplete beta and gamma functions be defined as usual: \begin{equation} I_p(z,w) = \frac a {B(z,w)} \int_0^p t^{z-1} (1-t)^{w-1} \,\mathrm dt, \end{equation} \begin{equation} \gamma(r,t) = \frac 1 {\Gamma(r)} \int_0^t s^{r-1} e^{-s} \,\mathrm ds. \end{equation} Let $r \in \mathbb N$, and consider $a, b > 0$. As is well known, $I_p(r,a/p-b) \rightarrow \gamma(r,a)$ as $p \rightarrow 0$. In the context of a statistical estimation problem (I can provide details if you are interested), I have been able to prove that the difference between the incomplete beta function and its limit, \begin{equation} I_p(r,a/p-b) - \gamma(r,a), \end{equation} is positive for all $p \in (0,1)$ if $a$ is sufficiently larger than $r$, and negative if $a$ is sufficiently smaller than $r$. For example, if you have access to Matlab you can check that (note that Matlab's gammainc uses reverse order for its arguments) p = 1e-3;  r = 5; a = 10; b = 5; betainc(p,r,a/p-b) - gammainc(a,r) is positive, whereas p = 1e-3;  r = 5; a = 2; b = 5; betainc(p,r,a/p-b) - gammainc(a,r) is negative. I'd like to know the widest range of values of $a$ for which that difference is assured to be positive/negative. So, my question is: Is there a known theorem that gives sufficient conditions on $a, b, r$ that assure that $I_p(r,a/p-b) - \gamma(r,a)$ is positive/negative? Even if you don't know any such theorem, a pointer in the right direction would be appreciated.",,"['probability', 'inequality', 'special-functions', 'gamma-function']"
76,Is there a way to standardize the Poisson distribution?,Is there a way to standardize the Poisson distribution?,,"For example, a variable of Normal distribution, $T$, with mean $\mu$ and variance $\sigma^2$ can be standardized into $S$ like this: $$ S=\frac{T-\mu}{\sigma}\;\Longrightarrow\;F(x)=\Phi\left(\frac{x-\mu}{\sigma}\right) $$ My question is, for the Poisson distribution with probability function $$ f(k;\lambda)=\Pr(X=k)=\frac{\lambda^k e^{-\lambda}}{k!}. $$ Is there a way to standardize the $X$ if I define the standard Poisson Distribution as the distribution that $\lambda=1$?","For example, a variable of Normal distribution, $T$, with mean $\mu$ and variance $\sigma^2$ can be standardized into $S$ like this: $$ S=\frac{T-\mu}{\sigma}\;\Longrightarrow\;F(x)=\Phi\left(\frac{x-\mu}{\sigma}\right) $$ My question is, for the Poisson distribution with probability function $$ f(k;\lambda)=\Pr(X=k)=\frac{\lambda^k e^{-\lambda}}{k!}. $$ Is there a way to standardize the $X$ if I define the standard Poisson Distribution as the distribution that $\lambda=1$?",,"['probability', 'probability-distributions']"
77,Law of large numbers for a Subordinator.,Law of large numbers for a Subordinator.,,"Let $\left(  X_{t}\right)  _{t\geq0}$ be a subordinator with the Laplace exponent given by $$ \Phi\left(  \lambda\right)  =d\lambda+\int_{0}^{\infty}\left(  1-e^{-\lambda x}\right)  \nu\left(  dx\right) $$ Show that almost surely $$ \lim_{t\rightarrow\infty}\frac{X_{t}}{t}=d+\int_{0}^{\infty}x\nu\left( dx\right) $$ I first use the Levy-Khintchine formula, where we have $$ X_{t}=dt+Y_{t}% $$ and $\left\{  Y_{t}\right\}  _{t\geq0}$ is a subordinator whose the Laplace exponent is given by $$ \Phi^{\prime}\left(  \lambda\right)  =\int_{0}^{\infty}\left(  1-e^{-\lambda x}\right)  \nu\left(  dx\right) $$ Thus, $$ \frac{X_{t}}{t}=d+\frac{Y_{t}}{t}% $$ Now, I think it suffices to show that almost surely $\lim_{t\rightarrow\infty }\frac{Y_{t}}{t}=\int_{0}^{\infty}x\nu\left(  dx\right)  .$ However, I don't know if I can do this and what should I do next.","Let $\left(  X_{t}\right)  _{t\geq0}$ be a subordinator with the Laplace exponent given by $$ \Phi\left(  \lambda\right)  =d\lambda+\int_{0}^{\infty}\left(  1-e^{-\lambda x}\right)  \nu\left(  dx\right) $$ Show that almost surely $$ \lim_{t\rightarrow\infty}\frac{X_{t}}{t}=d+\int_{0}^{\infty}x\nu\left( dx\right) $$ I first use the Levy-Khintchine formula, where we have $$ X_{t}=dt+Y_{t}% $$ and $\left\{  Y_{t}\right\}  _{t\geq0}$ is a subordinator whose the Laplace exponent is given by $$ \Phi^{\prime}\left(  \lambda\right)  =\int_{0}^{\infty}\left(  1-e^{-\lambda x}\right)  \nu\left(  dx\right) $$ Thus, $$ \frac{X_{t}}{t}=d+\frac{Y_{t}}{t}% $$ Now, I think it suffices to show that almost surely $\lim_{t\rightarrow\infty }\frac{Y_{t}}{t}=\int_{0}^{\infty}x\nu\left(  dx\right)  .$ However, I don't know if I can do this and what should I do next.",,"['probability', 'stochastic-processes', 'levy-processes']"
78,Convex Hull of Sampled Random Points,Convex Hull of Sampled Random Points,,"Consider $N$ i.i.d. random points $x_1,x_2,..., x_N\in \mathbb{R}^n$, sampled from a given distribution $d$ defined on $\mathbb{R}^n$. Let $\mathcal{C}_x \subset \mathbb{R}^n$ be the convex hull of these points $\{x_1,x_2,..., x_N\}$. Then consider $N$ new, again i.i.d., random points $y_1,y_2,..., y_N\in \mathbb{R}^n$, sampled from the same distribution $d$. Questions. What is the ""Expected Value"" for the Number of Points $y_i$ not in $\mathcal{C}_x$? If $\mathcal{C}_y$ is the convex hull of the points $\{y_1,y_2,..., y_N\}$, what is the ""Expected Value"" of the ration between the two volume, i.e. $\text{vol}(\mathcal{C}_y)/\text{vol}(\mathcal{C}_x)$? How do the above answers change if $\mathcal{C}_x$ is based on $N_1$ points and $\mathcal{C}_y$ is based on $N_2>N_1$ points? Comment. The aim of these questions is trying to understand the behaviour of the convex hull of some samples. Moreover, it would interesting to relate $\text{vol}(\mathcal{C}_y)$ to $\text{vol}(\mathcal{C}_x)$, especially for $N_2>N_1$, in order to ""estimate"" $\text{conv}\{y_1,y_2,..., y_{N_2}\}$ based on $\text{conv}\{x_1,x_2,..., x_{N_1}\}$. I do not know what to read to address this arguments.","Consider $N$ i.i.d. random points $x_1,x_2,..., x_N\in \mathbb{R}^n$, sampled from a given distribution $d$ defined on $\mathbb{R}^n$. Let $\mathcal{C}_x \subset \mathbb{R}^n$ be the convex hull of these points $\{x_1,x_2,..., x_N\}$. Then consider $N$ new, again i.i.d., random points $y_1,y_2,..., y_N\in \mathbb{R}^n$, sampled from the same distribution $d$. Questions. What is the ""Expected Value"" for the Number of Points $y_i$ not in $\mathcal{C}_x$? If $\mathcal{C}_y$ is the convex hull of the points $\{y_1,y_2,..., y_N\}$, what is the ""Expected Value"" of the ration between the two volume, i.e. $\text{vol}(\mathcal{C}_y)/\text{vol}(\mathcal{C}_x)$? How do the above answers change if $\mathcal{C}_x$ is based on $N_1$ points and $\mathcal{C}_y$ is based on $N_2>N_1$ points? Comment. The aim of these questions is trying to understand the behaviour of the convex hull of some samples. Moreover, it would interesting to relate $\text{vol}(\mathcal{C}_y)$ to $\text{vol}(\mathcal{C}_x)$, especially for $N_2>N_1$, in order to ""estimate"" $\text{conv}\{y_1,y_2,..., y_{N_2}\}$ based on $\text{conv}\{x_1,x_2,..., x_{N_1}\}$. I do not know what to read to address this arguments.",,"['probability', 'geometry', 'probability-theory']"
79,Need advice: what should be my next step?,Need advice: what should be my next step?,,"I am dealing with a quite algebraic question and I arrived at some good point. I had $2$ equations with $2$ unknowns and I was able to eleminate one of the variables. My final equation still seems abit ugly but it seems that there is some structure which might be of some help. Here is my final equation: $$\small{D(y_u)=L(y_u)\int_{-\infty}^{-y_u}f_1(y)\mbox{d}y+\frac{L(y_u)}{\left(\sqrt{L(y_u)}+1\right)^2}\int_{-y_u}^{y_u}\left(\sqrt{f_0(y)}+\sqrt{f_1(y)}\right)^2 \mbox{d}y +\int_{y_u}^{\infty}f_1(y)\mbox{d}y-\frac{1}{\left(1-\epsilon\right)^2}\left(\sqrt{L(y_u)}\int_{-\infty}^{-y_u}f_1(y)\mbox{d}y+\frac{\sqrt{L(y_u)}}{\sqrt{L(y_u)}+1}\int_{-y_u}^{y_u}\left(\sqrt{f_0(y)}+\sqrt{f_1(y)}\right)\sqrt{f_1(y)}\mbox{d}y+\int_{y_u}^{\infty}f_1(y)\mbox{d}y\right)^2=0}$$ Here $f_0$ and $f_1$ are some density functions, $y_u\in\mathbb{R}^+$ and $L(y)=\frac{f_1(y)}{f_0(y)}$ is monotinically increasing. Additionally $f_1(y)=f_0(-y)$ and as a result of this we have $L(0)=1$. What I did is as follows: I evaluated the equation in some extreme $y_u$ values. For example when $y_u=0$, I have $L(0)=1$, therefore the equation above simplifies considerably and I get $\epsilon=0$. Similarly, when $\lim_{y_u\rightarrow\infty}D(y_u)$ gives me $$\epsilon_{y_u\rightarrow \infty}=1-\sqrt{\frac{\int_{-\infty}^{\infty}\sqrt{f_0(y)f_1(y)}\mbox{d}y+1}{2}}\geq 0$$ The inequality holds since $\int_{-\infty}^{\infty}\sqrt{f_0(y)f_1(y)}\mbox{d}y\leq 1$ My question: For given densities $f_0$ and $f_1$ the minimum of $\epsilon$ is $0$ and the maximum of $\epsilon$ is reached when ${y_u\rightarrow\infty}$, namely I have  $$0 \leq\epsilon\leq 1-\sqrt{\frac{\int_{-\infty}^{\infty}\sqrt{f_0(y)f_1(y)}\mbox{d}y+1}{2}}$$ I need to show that when $y_u$ increases from $0$ to $\infty$, $\epsilon$ increases from $0$ to $1-\sqrt{\frac{\int_{-\infty}^{\infty}\sqrt{f_0(y)f_1(y)}\mbox{d}y+1}{2}}$ What should be my next step? Thank you very much. I appreciate any of your comments even if you are not familiar with the topic.","I am dealing with a quite algebraic question and I arrived at some good point. I had $2$ equations with $2$ unknowns and I was able to eleminate one of the variables. My final equation still seems abit ugly but it seems that there is some structure which might be of some help. Here is my final equation: $$\small{D(y_u)=L(y_u)\int_{-\infty}^{-y_u}f_1(y)\mbox{d}y+\frac{L(y_u)}{\left(\sqrt{L(y_u)}+1\right)^2}\int_{-y_u}^{y_u}\left(\sqrt{f_0(y)}+\sqrt{f_1(y)}\right)^2 \mbox{d}y +\int_{y_u}^{\infty}f_1(y)\mbox{d}y-\frac{1}{\left(1-\epsilon\right)^2}\left(\sqrt{L(y_u)}\int_{-\infty}^{-y_u}f_1(y)\mbox{d}y+\frac{\sqrt{L(y_u)}}{\sqrt{L(y_u)}+1}\int_{-y_u}^{y_u}\left(\sqrt{f_0(y)}+\sqrt{f_1(y)}\right)\sqrt{f_1(y)}\mbox{d}y+\int_{y_u}^{\infty}f_1(y)\mbox{d}y\right)^2=0}$$ Here $f_0$ and $f_1$ are some density functions, $y_u\in\mathbb{R}^+$ and $L(y)=\frac{f_1(y)}{f_0(y)}$ is monotinically increasing. Additionally $f_1(y)=f_0(-y)$ and as a result of this we have $L(0)=1$. What I did is as follows: I evaluated the equation in some extreme $y_u$ values. For example when $y_u=0$, I have $L(0)=1$, therefore the equation above simplifies considerably and I get $\epsilon=0$. Similarly, when $\lim_{y_u\rightarrow\infty}D(y_u)$ gives me $$\epsilon_{y_u\rightarrow \infty}=1-\sqrt{\frac{\int_{-\infty}^{\infty}\sqrt{f_0(y)f_1(y)}\mbox{d}y+1}{2}}\geq 0$$ The inequality holds since $\int_{-\infty}^{\infty}\sqrt{f_0(y)f_1(y)}\mbox{d}y\leq 1$ My question: For given densities $f_0$ and $f_1$ the minimum of $\epsilon$ is $0$ and the maximum of $\epsilon$ is reached when ${y_u\rightarrow\infty}$, namely I have  $$0 \leq\epsilon\leq 1-\sqrt{\frac{\int_{-\infty}^{\infty}\sqrt{f_0(y)f_1(y)}\mbox{d}y+1}{2}}$$ I need to show that when $y_u$ increases from $0$ to $\infty$, $\epsilon$ increases from $0$ to $1-\sqrt{\frac{\int_{-\infty}^{\infty}\sqrt{f_0(y)f_1(y)}\mbox{d}y+1}{2}}$ What should be my next step? Thank you very much. I appreciate any of your comments even if you are not familiar with the topic.",,"['probability', 'probability-distributions', 'convex-optimization']"
80,Disintegration of Measures,Disintegration of Measures,,"I was thinking about this exercise and I can't see how to end it. I'm sorry about the long post and thank you for the attention. Before asking the question, I need some background. Let $(\Omega, \Sigma,\mu)$ be a probability space. If $\mathcal{P}$ and $\mathcal{Q}$ are measurable partitions of $\Omega$ such that all elements of $\mathcal{Q}$ are subset of a element of $\mathcal{P}$. It's possible to give to each partition a structure of a probability space that comes from $\pi_C=\Omega \rightarrow C$, with $\pi_C(x)=$the element $A$ of $C$ such that $x\in A$, where $C\in\{\mathcal{P}, \mathcal{Q}\}$, such that i) The $\sigma$-algebra is: $A\subset C$ is measurable iff $\pi_C^{-1}(A)\subset \Omega$ is measurable ii) The measure in $C$ is given by $\hat{\mu}_C(A) = \mu(\pi_C^{-1}(A))$. We say that a famyly $\{\mu_A: A \in C\}$ of probabylities in $\Omega$ is a disintegration with respect to the partition $C$ if i) $\mu_A(A)=1$ for $\hat{\mu}_C$-almost $A\in C$. ii) The map $\phi_{(C,E)}: C\rightarrow \mathbb{R}$ defined by $\phi_{(C,E)}(A)=\mu_A(E)$ is measurable for all $E\subset \Omega$ measurable. iii) $\mu(E)=\int_{C} \mu_A(E) d\hat{\mu}_C$ Here is the question: Consider the disintegration of $\mu$ with respect to $\mathcal{P}$ given by the family $\{\mu_P: P \in \mathcal{P}\}$. Let $\{\mu_{P,Q}: Q \subset P, Q\in\mathcal{Q}\}$ be a disintegration of $\mu_P$. Prove that $\{\mu_{P,Q}: Q\in\mathcal{Q}\}$ is a disintegration of $\mu$ with respect to $\mathcal{Q}$. I got stuck in the 3rd part of the definition. Here is what I got $$\mu(E)=\int_{\mathcal{P}} \mu_P(E) d\hat{\mu}_\mathcal{P} = \int_\mathcal{P} \left( \int_\mathcal{Q} \mu_{P,Q}(E) d\hat{\mu}_{P,\mathcal{Q}}\right) d\hat{\mu}_\mathcal{P}$$ and I can't manage to make someking of change of variables to get $\int_\mathcal{Q} \mu_{P,Q}(E) d\hat{\mu}_\mathcal{Q}$. Do you guys have anykind of hint? Any other approach or idea?","I was thinking about this exercise and I can't see how to end it. I'm sorry about the long post and thank you for the attention. Before asking the question, I need some background. Let $(\Omega, \Sigma,\mu)$ be a probability space. If $\mathcal{P}$ and $\mathcal{Q}$ are measurable partitions of $\Omega$ such that all elements of $\mathcal{Q}$ are subset of a element of $\mathcal{P}$. It's possible to give to each partition a structure of a probability space that comes from $\pi_C=\Omega \rightarrow C$, with $\pi_C(x)=$the element $A$ of $C$ such that $x\in A$, where $C\in\{\mathcal{P}, \mathcal{Q}\}$, such that i) The $\sigma$-algebra is: $A\subset C$ is measurable iff $\pi_C^{-1}(A)\subset \Omega$ is measurable ii) The measure in $C$ is given by $\hat{\mu}_C(A) = \mu(\pi_C^{-1}(A))$. We say that a famyly $\{\mu_A: A \in C\}$ of probabylities in $\Omega$ is a disintegration with respect to the partition $C$ if i) $\mu_A(A)=1$ for $\hat{\mu}_C$-almost $A\in C$. ii) The map $\phi_{(C,E)}: C\rightarrow \mathbb{R}$ defined by $\phi_{(C,E)}(A)=\mu_A(E)$ is measurable for all $E\subset \Omega$ measurable. iii) $\mu(E)=\int_{C} \mu_A(E) d\hat{\mu}_C$ Here is the question: Consider the disintegration of $\mu$ with respect to $\mathcal{P}$ given by the family $\{\mu_P: P \in \mathcal{P}\}$. Let $\{\mu_{P,Q}: Q \subset P, Q\in\mathcal{Q}\}$ be a disintegration of $\mu_P$. Prove that $\{\mu_{P,Q}: Q\in\mathcal{Q}\}$ is a disintegration of $\mu$ with respect to $\mathcal{Q}$. I got stuck in the 3rd part of the definition. Here is what I got $$\mu(E)=\int_{\mathcal{P}} \mu_P(E) d\hat{\mu}_\mathcal{P} = \int_\mathcal{P} \left( \int_\mathcal{Q} \mu_{P,Q}(E) d\hat{\mu}_{P,\mathcal{Q}}\right) d\hat{\mu}_\mathcal{P}$$ and I can't manage to make someking of change of variables to get $\int_\mathcal{Q} \mu_{P,Q}(E) d\hat{\mu}_\mathcal{Q}$. Do you guys have anykind of hint? Any other approach or idea?",,"['probability', 'analysis', 'measure-theory', 'probability-theory']"
81,Martingale and bounded stopping time,Martingale and bounded stopping time,,"A theorem of submartingale and bounded stopping time says: Theorem 5.4.1. If $X_n$ is a submartingale and $N$ is a stopping time with $\mathbb P (N \le  k) = 1$ then $\mathbb EX_0 ≤ \mathbb EX_N ≤ \mathbb EX_k$. An exercise for this theorem is Example 5.4.1. Random walks. If we let $S_n = \xi_1 + · · · + \xi_n$ where the $ξ_m$   are independent and have $\mathbb E \xi_m = 0$, $\sigma_m^2 = \mathbb E \xi_m^2 < \infty$. Suppose we have that $|\xi_m | \le K$ and let $s^2_n = \sum_{m \le n} \sigma^2_m$. Note that $S_n^2 − s^2_n$ is a martingale. Use this fact and Theorem 5.4.1 to conclude    $$\mathbb P \left(\max_{1 \le m \le n} |S_m| ≤ x \right) ≤ (x + K)^2/ \mathbb E(S_n^2)$$ Let $A = \{\max_{1 \le m \le n} |S_m| ≤ x\}$. Let $X_n = S^2_n - s^2_n$. Let $N = \inf\{m:|S_m| \ge x~\text{or}~n+1\}$. So $N$ is a bounded stopping time. Thus by the previous theorem we have $$ 0 = \mathbb E{X_1} = \mathbb E{X_N} = \mathbb E{X_{n+1}}. $$ Since $X_{n+1} = X_N$ on $A^c$, we have $\mathbb E (X_{n+1} 1_A) = \mathbb E (X_N 1_A)$. Therefore, as long as we have $\mathbb E (X_N 1_A) \ge 0$, (which I don't know how to prove), we have $\mathbb E (X_{n+1} 1_A) \ge 0$. It follows that $$ \mathbb E(s_n^2 1_A) \le \mathbb E(s_{n+1}^2 1_A) \le \mathbb E(S_{n+1}^2 1_A) \le (x + K)^2. $$ But I have problem to show that $\mathbb E(X_N 1_A) \ge 0$. Am I on the right direction?","A theorem of submartingale and bounded stopping time says: Theorem 5.4.1. If $X_n$ is a submartingale and $N$ is a stopping time with $\mathbb P (N \le  k) = 1$ then $\mathbb EX_0 ≤ \mathbb EX_N ≤ \mathbb EX_k$. An exercise for this theorem is Example 5.4.1. Random walks. If we let $S_n = \xi_1 + · · · + \xi_n$ where the $ξ_m$   are independent and have $\mathbb E \xi_m = 0$, $\sigma_m^2 = \mathbb E \xi_m^2 < \infty$. Suppose we have that $|\xi_m | \le K$ and let $s^2_n = \sum_{m \le n} \sigma^2_m$. Note that $S_n^2 − s^2_n$ is a martingale. Use this fact and Theorem 5.4.1 to conclude    $$\mathbb P \left(\max_{1 \le m \le n} |S_m| ≤ x \right) ≤ (x + K)^2/ \mathbb E(S_n^2)$$ Let $A = \{\max_{1 \le m \le n} |S_m| ≤ x\}$. Let $X_n = S^2_n - s^2_n$. Let $N = \inf\{m:|S_m| \ge x~\text{or}~n+1\}$. So $N$ is a bounded stopping time. Thus by the previous theorem we have $$ 0 = \mathbb E{X_1} = \mathbb E{X_N} = \mathbb E{X_{n+1}}. $$ Since $X_{n+1} = X_N$ on $A^c$, we have $\mathbb E (X_{n+1} 1_A) = \mathbb E (X_N 1_A)$. Therefore, as long as we have $\mathbb E (X_N 1_A) \ge 0$, (which I don't know how to prove), we have $\mathbb E (X_{n+1} 1_A) \ge 0$. It follows that $$ \mathbb E(s_n^2 1_A) \le \mathbb E(s_{n+1}^2 1_A) \le \mathbb E(S_{n+1}^2 1_A) \le (x + K)^2. $$ But I have problem to show that $\mathbb E(X_N 1_A) \ge 0$. Am I on the right direction?",,"['probability-theory', 'martingales']"
82,Does Multiplicative Version of Azuma's Inequality Hold?,Does Multiplicative Version of Azuma's Inequality Hold?,,"We know that there are multiplicative version concentration inequalities for sums of independent random variables. For example, the following multiplicative version Chernoff bound. Chernoff bound: Let $X_1,\ldots,X_n$ be independent random variables and $X_i \in \{0,1\}$. Let $X=\sum_{i=1}^n X_i$. Then for any $\delta>0$, $\Pr\left(X \ge (1+\delta)EX \right) \le e^{-c\cdot(EX)\delta ^2},$ where $c$ is some absolute constant. Now we consider dependent random variables. A slight variant of Azuma 's inequality states the following. Azuma's Inequality: Let $X_1,\ldots,X_n$ be (dependent) random variables and $X_i \in \{0,1\}$. Assume that there exists $m$, such that $\Pr\left( \sum_{i=1}^n \mathbb{E}[X_i|X_{<i}] \le m\right) = 1.$ Let $X=\sum_{i=1}^n X_i$. Then for any $\lambda > 0$, $\Pr\left(X \ge m+\lambda \right) \le e^{-2 \lambda^2/n}.$ Clearly Azuma's inequality is additive. My question is that does a multiplicative version of Azuma's inequality such as the following hold? My question: Let $X_1,\ldots,X_n$ be (dependent) random variables and $X_i \in \{0,1\}$. Assume that there exists $m$, such that $\Pr\left( \sum_{i=1}^n \mathbb{E}[X_i|X_{<i}] \le m\right) = 1.$ Let $X=\sum_{i=1}^n X_i$. Then for any $\delta >0$ $\Pr\left(X \ge (1+\delta)m \right) \le e^{-c\cdot m \delta^2},$ where $c$ is some absolute constant. Note that the standard Azuma's inequality does not imply the multiplicative version when $m \ll \sqrt{n}$.","We know that there are multiplicative version concentration inequalities for sums of independent random variables. For example, the following multiplicative version Chernoff bound. Chernoff bound: Let $X_1,\ldots,X_n$ be independent random variables and $X_i \in \{0,1\}$. Let $X=\sum_{i=1}^n X_i$. Then for any $\delta>0$, $\Pr\left(X \ge (1+\delta)EX \right) \le e^{-c\cdot(EX)\delta ^2},$ where $c$ is some absolute constant. Now we consider dependent random variables. A slight variant of Azuma 's inequality states the following. Azuma's Inequality: Let $X_1,\ldots,X_n$ be (dependent) random variables and $X_i \in \{0,1\}$. Assume that there exists $m$, such that $\Pr\left( \sum_{i=1}^n \mathbb{E}[X_i|X_{<i}] \le m\right) = 1.$ Let $X=\sum_{i=1}^n X_i$. Then for any $\lambda > 0$, $\Pr\left(X \ge m+\lambda \right) \le e^{-2 \lambda^2/n}.$ Clearly Azuma's inequality is additive. My question is that does a multiplicative version of Azuma's inequality such as the following hold? My question: Let $X_1,\ldots,X_n$ be (dependent) random variables and $X_i \in \{0,1\}$. Assume that there exists $m$, such that $\Pr\left( \sum_{i=1}^n \mathbb{E}[X_i|X_{<i}] \le m\right) = 1.$ Let $X=\sum_{i=1}^n X_i$. Then for any $\delta >0$ $\Pr\left(X \ge (1+\delta)m \right) \le e^{-c\cdot m \delta^2},$ where $c$ is some absolute constant. Note that the standard Azuma's inequality does not imply the multiplicative version when $m \ll \sqrt{n}$.",,"['probability', 'statistics', 'probability-theory']"
83,Expectation of a product of Brownian Motion.,Expectation of a product of Brownian Motion.,,"Regarding Brownian Motion formula below, how does $E[W(s)W(t)]$ turn into $$E\left[W(s)\big(W(t)−W(s)\big)+W(s)^2\right]\;??$$ I have asked a question using the formula below, but this and that are totally different questions. Thanks for all the help!! Assuming $t>s$, $$\begin{align*} E[W(s)W(t)]&=E\left[W(s)\big(W(t)−W(s)\big)+W(s)^2\right]\\ &=E[W(s)]E[W(t)−W(s)]+E\left[W(s)^2\right]\\ &=0+s\\ &=\min(s,t)\;. \end{align*}$$","Regarding Brownian Motion formula below, how does $E[W(s)W(t)]$ turn into $$E\left[W(s)\big(W(t)−W(s)\big)+W(s)^2\right]\;??$$ I have asked a question using the formula below, but this and that are totally different questions. Thanks for all the help!! Assuming $t>s$, $$\begin{align*} E[W(s)W(t)]&=E\left[W(s)\big(W(t)−W(s)\big)+W(s)^2\right]\\ &=E[W(s)]E[W(t)−W(s)]+E\left[W(s)^2\right]\\ &=0+s\\ &=\min(s,t)\;. \end{align*}$$",,"['probability', 'probability-theory', 'probability-distributions']"
84,Asymptotics for a partial sum of binomial coefficients,Asymptotics for a partial sum of binomial coefficients,,"Good afternoon, I would like to ask, if anyone knows how to evaluate a sum $$\sum_{k=0}^{\lambda n}{n \choose k}$$ for fixed $\lambda < 1/2$ with absolute error $O(n^{-1})$, or better. In Concrete Mathematics (Graham, Knuth, Patashnik), it is shown how to evaluate this sum with absolute error $O(1)$, but it is not clear to me, how to obtain better absolute error in a straightforward manner. Thank you in advance.","Good afternoon, I would like to ask, if anyone knows how to evaluate a sum $$\sum_{k=0}^{\lambda n}{n \choose k}$$ for fixed $\lambda < 1/2$ with absolute error $O(n^{-1})$, or better. In Concrete Mathematics (Graham, Knuth, Patashnik), it is shown how to evaluate this sum with absolute error $O(1)$, but it is not clear to me, how to obtain better absolute error in a straightforward manner. Thank you in advance.",,"['discrete-mathematics', 'binomial-coefficients', 'asymptotics']"
85,Estimate number of distinct items,Estimate number of distinct items,,"I have a large array of $n$ integers, some of which may be repeated, and I want to estimate how many distinct integers are in the array.  Say the number of distinct integers is $N$. I can sample with replacement easily but can't afford to sample anything like $n$ samples as $n$ is too big.  If I sample $y$ positions uniformly with replacement, let $X$ be the number of distinct integers I get in the sample.  How can we use $X$ to give an estimate for $N$? When $y$ is $\Omega(n \log n)$ then we expect to have seen every position in the array and so  $X=N$ with high probability by an application of http://en.wikipedia.org/wiki/Coupon_collector%27s_problem .  When $y$ is much smaller than $n$ it seems we might only be able to give probabilistic upper and lower bounds as estimates for $N$ depending on the distribution of duplicates in the input array. EDIT: As pointed out in the comments, there was an error in the original question (now fixed).","I have a large array of $n$ integers, some of which may be repeated, and I want to estimate how many distinct integers are in the array.  Say the number of distinct integers is $N$. I can sample with replacement easily but can't afford to sample anything like $n$ samples as $n$ is too big.  If I sample $y$ positions uniformly with replacement, let $X$ be the number of distinct integers I get in the sample.  How can we use $X$ to give an estimate for $N$? When $y$ is $\Omega(n \log n)$ then we expect to have seen every position in the array and so  $X=N$ with high probability by an application of http://en.wikipedia.org/wiki/Coupon_collector%27s_problem .  When $y$ is much smaller than $n$ it seems we might only be able to give probabilistic upper and lower bounds as estimates for $N$ depending on the distribution of duplicates in the input array. EDIT: As pointed out in the comments, there was an error in the original question (now fixed).",,"['probability', 'statistics', 'parameter-estimation']"
86,Pairwise spacings of an ordered sequence of uniform random numbers,Pairwise spacings of an ordered sequence of uniform random numbers,,"Given an ordered list of $m$ uniform random numbers in the range $1$ to $n$ $$a_{1} \le a_{2} \le \ldots \le a_{m}, \forall a_{i}: a_{i}\in [1;n] \cap \mathbb{N}$$ compute the pairwise spacings of these numbers: $$d_{1} = a_{2}-a_{1}   d_{2} = a_{3}-a_{2}   \ldots   d_{m-1} = a_{m}-a_{m-1}$$ Then, define $Y$ as the number of distinct $d_{i}$ that appear more than once: $$ Y = | \bigcup \limits_{i=1}^{m} \{d_{i}: \exists j \ne i : d_{i} = d_{j} \}| $$ (Note that for each distinct value that appears more than once, Y is increased by only one, no matter how often this value actually appears). George Marsaglia [1] has stated that Y is Poisson-distributed with $\lambda = \frac{m^{3}}{4m}$, but I couldn't find a proof for that anywhere and I've just spent an entire day trying to figure it out myself. I seem to be unable to even get the probability of 2 $d_{i}$s having the same value. Does anyone have an idea how to start? Or is there even a ""simple"" explanation for the Poisson-distribution? I would be happy already if I could only prove that Y is Poisson-distributed at all, but I can't even do that by myself. Thanks a million times for any help! [1] http://www.stat.fsu.edu/pub/diehard/cdrom/pscript/keynote.ps","Given an ordered list of $m$ uniform random numbers in the range $1$ to $n$ $$a_{1} \le a_{2} \le \ldots \le a_{m}, \forall a_{i}: a_{i}\in [1;n] \cap \mathbb{N}$$ compute the pairwise spacings of these numbers: $$d_{1} = a_{2}-a_{1}   d_{2} = a_{3}-a_{2}   \ldots   d_{m-1} = a_{m}-a_{m-1}$$ Then, define $Y$ as the number of distinct $d_{i}$ that appear more than once: $$ Y = | \bigcup \limits_{i=1}^{m} \{d_{i}: \exists j \ne i : d_{i} = d_{j} \}| $$ (Note that for each distinct value that appears more than once, Y is increased by only one, no matter how often this value actually appears). George Marsaglia [1] has stated that Y is Poisson-distributed with $\lambda = \frac{m^{3}}{4m}$, but I couldn't find a proof for that anywhere and I've just spent an entire day trying to figure it out myself. I seem to be unable to even get the probability of 2 $d_{i}$s having the same value. Does anyone have an idea how to start? Or is there even a ""simple"" explanation for the Poisson-distribution? I would be happy already if I could only prove that Y is Poisson-distributed at all, but I can't even do that by myself. Thanks a million times for any help! [1] http://www.stat.fsu.edu/pub/diehard/cdrom/pscript/keynote.ps",,"['probability', 'random']"
87,How do I solve this probability problem of randomly drawing balls from a urn?,How do I solve this probability problem of randomly drawing balls from a urn?,,"In an urn there are $a$ azure balls and $c$ carmine balls, $ac\ne0$. To begin with, you randomly pick a ball, throw it away, and then each time you randomly pick a ball, if it has the same color with its predecessor, throw it away, otherwise put it back. Then what's the probability that the last one thrown from the urn is azure? For instance, a possible round: draw    urn ----------------         AAACCCCC A       AACCCCC C       AACCCCC C       AACCCC C       AACCC A       AACCC C       AACCC A       AACCC C       AACCC C       AACC C       AAC C       AA A       AA A       A A       - In this round, the last one thrown is an azure ball.","In an urn there are $a$ azure balls and $c$ carmine balls, $ac\ne0$. To begin with, you randomly pick a ball, throw it away, and then each time you randomly pick a ball, if it has the same color with its predecessor, throw it away, otherwise put it back. Then what's the probability that the last one thrown from the urn is azure? For instance, a possible round: draw    urn ----------------         AAACCCCC A       AACCCCC C       AACCCCC C       AACCCC C       AACCC A       AACCC C       AACCC A       AACCC C       AACCC C       AACC C       AAC C       AA A       AA A       A A       - In this round, the last one thrown is an azure ball.",,['probability']
88,Problem of cumulants,Problem of cumulants,,"I take the problem of cumulants to be this: given a sequence $(\kappa_1,\kappa_2,\kappa_3,\ldots)$, is it the sequence of cumulants of some probability distribution?  In one sense, this is trivially equivalent to the problem of moments: the $n$th moment is a polynomial in the first $n$ cumulants and vice-versa.  But cumulant sequences have a nice property that moment sequences don't have: the set of all such sequences is closed under addition.  So draw a ray out from the origin $(0,0,0,\ldots)$.  If the ray bumps into a cumulant sequence $(\kappa_1,\kappa_2,\kappa_3,\ldots)$, then $2(\kappa_1,\kappa_2,\kappa_3,\ldots),3(\kappa_1,\kappa_2,\kappa_3,\ldots),\ldots$ are also cumulant sequences. For infinitely divisible distributions, for every real $t\ge 0$, the sequence $t(\kappa_1,\kappa_2,\kappa_3,\ldots)$ is a cumulant sequence. Besides the nonnegative integers and the nonnegative reals, there are other sets of nonnegative reals closed under addition. For which sets $T$ of nonnegative reals that are closed under addition is it the case that for some cumulant sequence $(\kappa_1,\kappa_2,\kappa_3,\ldots)$, for every real $t\ge 0$, $$ t\in T \quad\text{iff}\quad t(\kappa_1,\kappa_2,\kappa_3,\ldots)\text{ is a cumulant sequence ?} $$ (I'm guessing only closed sets, but there should be more than that to say about it, I would think.) Later edit: Since there's no mad rush to answer this here, I've posted it to mathoverflow .","I take the problem of cumulants to be this: given a sequence $(\kappa_1,\kappa_2,\kappa_3,\ldots)$, is it the sequence of cumulants of some probability distribution?  In one sense, this is trivially equivalent to the problem of moments: the $n$th moment is a polynomial in the first $n$ cumulants and vice-versa.  But cumulant sequences have a nice property that moment sequences don't have: the set of all such sequences is closed under addition.  So draw a ray out from the origin $(0,0,0,\ldots)$.  If the ray bumps into a cumulant sequence $(\kappa_1,\kappa_2,\kappa_3,\ldots)$, then $2(\kappa_1,\kappa_2,\kappa_3,\ldots),3(\kappa_1,\kappa_2,\kappa_3,\ldots),\ldots$ are also cumulant sequences. For infinitely divisible distributions, for every real $t\ge 0$, the sequence $t(\kappa_1,\kappa_2,\kappa_3,\ldots)$ is a cumulant sequence. Besides the nonnegative integers and the nonnegative reals, there are other sets of nonnegative reals closed under addition. For which sets $T$ of nonnegative reals that are closed under addition is it the case that for some cumulant sequence $(\kappa_1,\kappa_2,\kappa_3,\ldots)$, for every real $t\ge 0$, $$ t\in T \quad\text{iff}\quad t(\kappa_1,\kappa_2,\kappa_3,\ldots)\text{ is a cumulant sequence ?} $$ (I'm guessing only closed sets, but there should be more than that to say about it, I would think.) Later edit: Since there's no mad rush to answer this here, I've posted it to mathoverflow .",,"['probability', 'stochastic-processes']"
89,Bounding function involving Beta functions,Bounding function involving Beta functions,,"Given $\frac{a}{x-1} \leq \frac{b}{y-1} \leq \frac{c}{z-1}$ with $a,b,c > 0$ and $x,y,z > 1$, I want to show that $$\frac{(\frac{a}{a+b})^{x-1}(\frac{b}{a+b})^{y-1}}{B(x,y)\cdot (x+y-1)} + \frac{(\frac{a}{a+c})^{x-1}(\frac{c}{a+c})^{z-1}}{B(x,z)\cdot (x+z-1)} + \frac{(\frac{b}{b+c})^{y-1}(\frac{c}{b+c})^{z-1}}{B(y,z)\cdot (y+z-1)}$$ is smaller than $$\frac{1}{x}\left(\frac{a(y-1)}{x\cdot b}\right)^{x-1} +\frac{1}{x}\left(\frac{a(z-1)}{x\cdot c}\right)^{x-1}+\frac{1}{y}\left(\frac{b(z-1)}{y\cdot c}\right)^{y-1}+2$$ I already know that the first formula is bounded above by 3, and each of the individual terms in both formulas is between 0 and 1. Further, I ran a lot of simulations with different values for $a,b,c,x,y,z$ and the bound seems tight only when $a$ approaches $\infty$ and $x=a+1$, while $b,c$ approach 0 and $y = b+1$ and $z=c+1$. The problem is that I cannot show it formally, because I don't know how to simplify or bound the functions involving the beta functions. Thanks in advance, crsm","Given $\frac{a}{x-1} \leq \frac{b}{y-1} \leq \frac{c}{z-1}$ with $a,b,c > 0$ and $x,y,z > 1$, I want to show that $$\frac{(\frac{a}{a+b})^{x-1}(\frac{b}{a+b})^{y-1}}{B(x,y)\cdot (x+y-1)} + \frac{(\frac{a}{a+c})^{x-1}(\frac{c}{a+c})^{z-1}}{B(x,z)\cdot (x+z-1)} + \frac{(\frac{b}{b+c})^{y-1}(\frac{c}{b+c})^{z-1}}{B(y,z)\cdot (y+z-1)}$$ is smaller than $$\frac{1}{x}\left(\frac{a(y-1)}{x\cdot b}\right)^{x-1} +\frac{1}{x}\left(\frac{a(z-1)}{x\cdot c}\right)^{x-1}+\frac{1}{y}\left(\frac{b(z-1)}{y\cdot c}\right)^{y-1}+2$$ I already know that the first formula is bounded above by 3, and each of the individual terms in both formulas is between 0 and 1. Further, I ran a lot of simulations with different values for $a,b,c,x,y,z$ and the bound seems tight only when $a$ approaches $\infty$ and $x=a+1$, while $b,c$ approach 0 and $y = b+1$ and $z=c+1$. The problem is that I cannot show it formally, because I don't know how to simplify or bound the functions involving the beta functions. Thanks in advance, crsm",,"['probability', 'inequality', 'special-functions']"
90,An application of the Optional Sampling Theorem,An application of the Optional Sampling Theorem,,"let $S(k), k\geq 0$ a discrete random process. Suppose $S(N)$ is with probability one either 100 or 0 and that $S(0)=50$. Suppose further there is at least a sixty percent probability that the price will at some point dip below 40 and then subsequently rise above 60 before time $N$.  How do you prove that $S(k)$ cannot be a martingale? By advance, thank you very much for your help.","let $S(k), k\geq 0$ a discrete random process. Suppose $S(N)$ is with probability one either 100 or 0 and that $S(0)=50$. Suppose further there is at least a sixty percent probability that the price will at some point dip below 40 and then subsequently rise above 60 before time $N$.  How do you prove that $S(k)$ cannot be a martingale? By advance, thank you very much for your help.",,"['probability', 'stochastic-processes', 'martingales']"
91,"If $x\sim\mathcal{N}(\mu, \sigma^2)$, what is $\mathbb{E}\left(\frac{\exp(x)}{a+\exp(x)}\right)$ for $a>0$?","If , what is  for ?","x\sim\mathcal{N}(\mu, \sigma^2) \mathbb{E}\left(\frac{\exp(x)}{a+\exp(x)}\right) a>0","Let $x\sim\mathcal{N}(\mu, \sigma^2)$ be a normal random variable. I'm interested in a new random variable, defined as  $$ z = \frac{\exp(x)}{a+\exp(x)},\quad x\in\mathbb{R}, a>0. $$ More specifically, I would like to find the closed form (if any) of its expected value, $\mathbb{E}[z]$, but I don't know how to proceed. As a first step I could use an intermediate random variable $y=\exp(x)$, which follows the log-normal distribution (since $x$ is normal) and its mean and variance are given w.r.t. to $x$'s mean and variance. But again, I'm not sure if that helps.","Let $x\sim\mathcal{N}(\mu, \sigma^2)$ be a normal random variable. I'm interested in a new random variable, defined as  $$ z = \frac{\exp(x)}{a+\exp(x)},\quad x\in\mathbb{R}, a>0. $$ More specifically, I would like to find the closed form (if any) of its expected value, $\mathbb{E}[z]$, but I don't know how to proceed. As a first step I could use an intermediate random variable $y=\exp(x)$, which follows the log-normal distribution (since $x$ is normal) and its mean and variance are given w.r.t. to $x$'s mean and variance. But again, I'm not sure if that helps.",,"['real-analysis', 'probability', 'random-variables', 'normal-distribution', 'expected-value']"
92,Are the rules of this tournament fair?,Are the rules of this tournament fair?,,"My daughter just took part to a volleyball tournament and she wonders whether the rules of the tournament were fair or not. There are 10 teams, gathered into 3 groups: Group 1 with 4 teams and Groups 2 and 3 with 3 teams. Each team plays a match against each other team in the same group. The result of each match being either 2-0 or 2-1, 1-2 or 0-2 (there is no tie in volleyball), it gives each team either 2, 1, -1 or -2 points for the tournament ranking. After this first series of matches, the teams are redistributed into three new groups as follows: Group A (4 teams: the three winners of each group and the best among the seconds of the three groups), Group B (3 teams: the remaining seconds of each group and the best among the thirds of groups 1, 2 and 3), Group C (the remaining 3 teams). The key rule explains how to decide which team is best second (respectively best third). The rule consists to add the points obtained by each team and to divide it by the number of matches. For instance, if a team of Group 1 won the first match by 2-1, won the second one by 2-0 and lost the third one by 1-2, this team would get a score of $(1 + 2 - 1)/3 = 2/3$. In case of tie, the total number of points scored in each set can be used. Question . My daughter had the feeling that the teams in Group 1 (the one with 4 teams) had a slight advantage to end up being in either Group A or Group B. Is this feeling justified? In a more mathematical setting, assuming that the results of the matches are randomly distributed with equal probability 1/4 for each score $(-2, -1, 1, 2)$, what is the probability for a team of Group 1 (respectively 2 or 3) to end up in Group A (respectively B and C)? Although my daugther is mainly interested in the case of 10 teams divided into 3 groups, a mathematical argument for the case of $n$ teams divided into $r$ groups would be appreciated.","My daughter just took part to a volleyball tournament and she wonders whether the rules of the tournament were fair or not. There are 10 teams, gathered into 3 groups: Group 1 with 4 teams and Groups 2 and 3 with 3 teams. Each team plays a match against each other team in the same group. The result of each match being either 2-0 or 2-1, 1-2 or 0-2 (there is no tie in volleyball), it gives each team either 2, 1, -1 or -2 points for the tournament ranking. After this first series of matches, the teams are redistributed into three new groups as follows: Group A (4 teams: the three winners of each group and the best among the seconds of the three groups), Group B (3 teams: the remaining seconds of each group and the best among the thirds of groups 1, 2 and 3), Group C (the remaining 3 teams). The key rule explains how to decide which team is best second (respectively best third). The rule consists to add the points obtained by each team and to divide it by the number of matches. For instance, if a team of Group 1 won the first match by 2-1, won the second one by 2-0 and lost the third one by 1-2, this team would get a score of $(1 + 2 - 1)/3 = 2/3$. In case of tie, the total number of points scored in each set can be used. Question . My daughter had the feeling that the teams in Group 1 (the one with 4 teams) had a slight advantage to end up being in either Group A or Group B. Is this feeling justified? In a more mathematical setting, assuming that the results of the matches are randomly distributed with equal probability 1/4 for each score $(-2, -1, 1, 2)$, what is the probability for a team of Group 1 (respectively 2 or 3) to end up in Group A (respectively B and C)? Although my daugther is mainly interested in the case of 10 teams divided into 3 groups, a mathematical argument for the case of $n$ teams divided into $r$ groups would be appreciated.",,"['probability', 'discrete-mathematics']"
93,Do you need true randomness to beat the two-envelope game?,Do you need true randomness to beat the two-envelope game?,,"A well-known (non-)paradox in probability involves a two-envelope game played between two players, $A$ and $B$: $A$ selects two distinct (real) numbers, $x$ and $y$, writing each one down on a card and sealing each card in an envelope, then presenting the two envelopes to $B$. $B$ chooses one of the envelopes and looks at the card inside.  They then guess whether the number on the card they've chosen is the larger or smaller of the two numbers. The 'paradox' here is that regardless of $A$'s scheme for choosing numbers — and even if $A$ knows $B$'s strategy in advance — there's a strategy for $B$ that will achieve a better-than-even success rate in the long run: choose an envelope at random, then map the number in it onto the interval $(0,1)$ using some (arbitrary) monotonic function.  Choose a random deviate $U\in(0,1)$, and then guess 'higher' or 'lower' according to whether (the mapping of) the number looked at is higher or lower than the generated random deviate. I'll skip the analysis of this strategy here (see Do better than chance or Who discovered this number-guessing paradox? for more details), but note that it explicitly relies on having a source of random deviates. My question is whether this is necessary for $B$ to have the advantage.  More specifically, consider the following variant of the game: B chooses computable functions $f():\mathbb{N}\mapsto\{0,1\}$ and $g():\mathbb{N}\mapsto\mathbb{Q}\cap(0,1)$.  Note that $A$ knows nothing about these functions, other than that they are computable. For each integer $n$, in turn: $A$ selects two distinct real numbers $x,y\in(0,1)$, writing each down on a card and presenting them in sealed envelopes.  (I'm restricting the numbers here to eliminate the mathematically-moot mapping step.) $B$ computes $f(n)$; if $f(n)=0$ then $B$ chooses $x$, and if $f(n)=1$ then $B$ chooses $y$.  Call $B$'s chosen number $z$. $B$ computes $g(n)$; if $g(n)\leq z$ then $B$ guesses 'higher', otherwise $B$ guesses 'lower'. Note that this is essentially $B$ following the strategy in the usual version of the game, except that $B$ is following a computable strategy rather than a purely randomized one. Can $A$ win this game in the long run? Since $A$ doesn't know what computable strategy $B$ is following, they'll clearly have to take some dovetailing approach; instinctively it feels like randomness is inherent in $B$'s ability to win with the usual strategy and that $A$, with the knowledge that $B$'s strategy is actually computable, should be able to 'game the system' and win.  Unfortunately, I can't see a clear proof here (and I wouldn't be entirely shocked to learn that I'm wrong).  Is anything known about this problem? EDIT: to clarify, I should point out that unlike $B$, the strategy that $A$ follows does not have to be computable; $A$ can, for instance, take advantage of an oracle that enumerates the total computable functions.  For example, this ensures that $A$ can guarantee they'll win at least once: enumerate all possible pairs $\langle f_n(), g_n()\rangle$ of recursive functions and in round $i$ behave as though $B$'s selections for the round will be $f_i(i)$ and $g_i(i)$ (by choosing values that will win given that these are $B$'s selections).  Also, I suspect $f()$ is actually superfluous and that we can ask the analogous question for the strategy where $B$ always guesses $x$, but if the presence or absence of $f()$ does matter then it'd be interesting to know that too.","A well-known (non-)paradox in probability involves a two-envelope game played between two players, $A$ and $B$: $A$ selects two distinct (real) numbers, $x$ and $y$, writing each one down on a card and sealing each card in an envelope, then presenting the two envelopes to $B$. $B$ chooses one of the envelopes and looks at the card inside.  They then guess whether the number on the card they've chosen is the larger or smaller of the two numbers. The 'paradox' here is that regardless of $A$'s scheme for choosing numbers — and even if $A$ knows $B$'s strategy in advance — there's a strategy for $B$ that will achieve a better-than-even success rate in the long run: choose an envelope at random, then map the number in it onto the interval $(0,1)$ using some (arbitrary) monotonic function.  Choose a random deviate $U\in(0,1)$, and then guess 'higher' or 'lower' according to whether (the mapping of) the number looked at is higher or lower than the generated random deviate. I'll skip the analysis of this strategy here (see Do better than chance or Who discovered this number-guessing paradox? for more details), but note that it explicitly relies on having a source of random deviates. My question is whether this is necessary for $B$ to have the advantage.  More specifically, consider the following variant of the game: B chooses computable functions $f():\mathbb{N}\mapsto\{0,1\}$ and $g():\mathbb{N}\mapsto\mathbb{Q}\cap(0,1)$.  Note that $A$ knows nothing about these functions, other than that they are computable. For each integer $n$, in turn: $A$ selects two distinct real numbers $x,y\in(0,1)$, writing each down on a card and presenting them in sealed envelopes.  (I'm restricting the numbers here to eliminate the mathematically-moot mapping step.) $B$ computes $f(n)$; if $f(n)=0$ then $B$ chooses $x$, and if $f(n)=1$ then $B$ chooses $y$.  Call $B$'s chosen number $z$. $B$ computes $g(n)$; if $g(n)\leq z$ then $B$ guesses 'higher', otherwise $B$ guesses 'lower'. Note that this is essentially $B$ following the strategy in the usual version of the game, except that $B$ is following a computable strategy rather than a purely randomized one. Can $A$ win this game in the long run? Since $A$ doesn't know what computable strategy $B$ is following, they'll clearly have to take some dovetailing approach; instinctively it feels like randomness is inherent in $B$'s ability to win with the usual strategy and that $A$, with the knowledge that $B$'s strategy is actually computable, should be able to 'game the system' and win.  Unfortunately, I can't see a clear proof here (and I wouldn't be entirely shocked to learn that I'm wrong).  Is anything known about this problem? EDIT: to clarify, I should point out that unlike $B$, the strategy that $A$ follows does not have to be computable; $A$ can, for instance, take advantage of an oracle that enumerates the total computable functions.  For example, this ensures that $A$ can guarantee they'll win at least once: enumerate all possible pairs $\langle f_n(), g_n()\rangle$ of recursive functions and in round $i$ behave as though $B$'s selections for the round will be $f_i(i)$ and $g_i(i)$ (by choosing values that will win given that these are $B$'s selections).  Also, I suspect $f()$ is actually superfluous and that we can ask the analogous question for the strategy where $B$ always guesses $x$, but if the presence or absence of $f()$ does matter then it'd be interesting to know that too.",,"['probability', 'game-theory', 'computability', 'random']"
94,Second marble is of same color,Second marble is of same color,,"A bag contains 3 white, 4 black, and 2 red marbles. Two marbles are drawn from the bag. If replacement is not allowed, what is the probability that the second marble drawn will be red? I disagree with the given answer $\frac29$. Can someone please point why my solution is wrong? P(Both Red) + P(Second is Red): P(Both Red) $=\frac{\binom22}{\binom92}$ P(Second is Red) $=\frac{\binom71\binom21}{\binom92}$ This comes out to be $\frac5{12}$.","A bag contains 3 white, 4 black, and 2 red marbles. Two marbles are drawn from the bag. If replacement is not allowed, what is the probability that the second marble drawn will be red? I disagree with the given answer $\frac29$. Can someone please point why my solution is wrong? P(Both Red) + P(Second is Red): P(Both Red) $=\frac{\binom22}{\binom92}$ P(Second is Red) $=\frac{\binom71\binom21}{\binom92}$ This comes out to be $\frac5{12}$.",,"['probability', 'combinatorics']"
95,Two players alternate flipping a coin until the result is head. How to derive that the probability for the first player to win is $2/3$? [duplicate],Two players alternate flipping a coin until the result is head. How to derive that the probability for the first player to win is ? [duplicate],2/3,"This question already has answers here : Two players alternately flip a coin; what is the probability of winning by getting a head? (5 answers) Closed 5 years ago . Two players, $A$ and $B$ , alternately and independently flip a coin and   the first player to obtain a head wins. Player $A$ flips first. What is   the probability that $A$ wins? Official answer: $2/3$ , but I cannot arrive at it. Thought process: Find the probability that a head comes on the $n$ th trial. That's easy to do, it's just a Geometric random variable. Then find the probability that $n$ th turn is player's A turn. Finally, multiply both probabilities. When I came up with each probability, both of them depended on the amount of trials $n$ , so my answer was a non-constant function of $n$ . However, what I find quite fantastic is that the answer is a constant, so the amount of tries until a head comes doesn't seem to matter.","This question already has answers here : Two players alternately flip a coin; what is the probability of winning by getting a head? (5 answers) Closed 5 years ago . Two players, and , alternately and independently flip a coin and   the first player to obtain a head wins. Player flips first. What is   the probability that wins? Official answer: , but I cannot arrive at it. Thought process: Find the probability that a head comes on the th trial. That's easy to do, it's just a Geometric random variable. Then find the probability that th turn is player's A turn. Finally, multiply both probabilities. When I came up with each probability, both of them depended on the amount of trials , so my answer was a non-constant function of . However, what I find quite fantastic is that the answer is a constant, so the amount of tries until a head comes doesn't seem to matter.",A B A A 2/3 n n n n,"['probability', 'discrete-mathematics', 'binomial-distribution', 'geometric-series']"
96,Probably that an $80\%$-truthful person actually rolled a $6$ [duplicate],Probably that an -truthful person actually rolled a  [duplicate],80\% 6,"This question already has answers here : A man who lies a fourth of the time throws a die and says it is a six. What is the probability it is actually a six? (2 answers) Closed 6 years ago . A person, $A$, speaks the truth $4$ out of $5$ times. The person throws a die and reports that he obtained a $6$. What is the probability that he actually rolled a $6$? I know there is a similar question like this  but my doubts are different from it and also I want to identify and solve total probability theorem questions so I posted a side doubt also. In my attempt, I defined the events \begin{align*}  E_1&: \text{The person tells the truth.} \\  E_2&: \text{The person lies.} \\  E_3&: \text{The person reports that the die landed on a 6.}  \end{align*} I noted that $P(E_1)=\frac{4}{5}$, $P(E_2)=\frac{1}{5}$, $P(E_3|E_1)=6^{-1}$ and $P(E_3|E_2)=0$ and obtained \begin{align*}  P(E_3) = \frac{4}{5} \cdot \frac{1}{6} + \frac{1}{5} \cdot 0 = \frac{2}{15}. \end{align*} However, the correct answer is, $\frac{4}{9}$. What did I do wrong? Side doubt: Even though the first experiment (truth and lying) is different from the second experiment, can we still apply total probability theorem? In my book the dependent experiment lies inside the sample space associated with the mutually and exhaustive events.","This question already has answers here : A man who lies a fourth of the time throws a die and says it is a six. What is the probability it is actually a six? (2 answers) Closed 6 years ago . A person, $A$, speaks the truth $4$ out of $5$ times. The person throws a die and reports that he obtained a $6$. What is the probability that he actually rolled a $6$? I know there is a similar question like this  but my doubts are different from it and also I want to identify and solve total probability theorem questions so I posted a side doubt also. In my attempt, I defined the events \begin{align*}  E_1&: \text{The person tells the truth.} \\  E_2&: \text{The person lies.} \\  E_3&: \text{The person reports that the die landed on a 6.}  \end{align*} I noted that $P(E_1)=\frac{4}{5}$, $P(E_2)=\frac{1}{5}$, $P(E_3|E_1)=6^{-1}$ and $P(E_3|E_2)=0$ and obtained \begin{align*}  P(E_3) = \frac{4}{5} \cdot \frac{1}{6} + \frac{1}{5} \cdot 0 = \frac{2}{15}. \end{align*} However, the correct answer is, $\frac{4}{9}$. What did I do wrong? Side doubt: Even though the first experiment (truth and lying) is different from the second experiment, can we still apply total probability theorem? In my book the dependent experiment lies inside the sample space associated with the mutually and exhaustive events.",,[]
97,"if two computers are playing tic-tac-toe, but they are choosing their squares randomly, what is the chance for X to win?","if two computers are playing tic-tac-toe, but they are choosing their squares randomly, what is the chance for X to win?",,"Tic-tac-toe is a children's board game that's notorious for draws. It's easy to write a program for either player (X or O) that always draws the game. I would like to know how the outlook changes if both players play completely randomly. Ie, X places their first move uniformly among the 9 squares, then O does the same, and so on until someone wins. Intuitively, X should be better because they'll usually get more squares and first player advantage, but I'm not sure. I really have no Idea, but I would really like to know. I am good at math, but not that good.","Tic-tac-toe is a children's board game that's notorious for draws. It's easy to write a program for either player (X or O) that always draws the game. I would like to know how the outlook changes if both players play completely randomly. Ie, X places their first move uniformly among the 9 squares, then O does the same, and so on until someone wins. Intuitively, X should be better because they'll usually get more squares and first player advantage, but I'm not sure. I really have no Idea, but I would really like to know. I am good at math, but not that good.",,['probability']
98,Chances of being picked last in a hat [duplicate],Chances of being picked last in a hat [duplicate],,"This question already has answers here : Probability: 10th ball is blue (9 answers) Closed 8 years ago . A couple of friends and I are struggling with coming up with an answer to this question.  It's seemingly simple but I need a little help. 6 people put their name into a hat.  One by one, names are pulled without replacement .  What are the odds that your name will be picked last ? My thoughts are: in each pick you're looking at the odds that your name is not getting picked. So first it's $\frac{5}{6}$, then $\frac{4}{5}$, then $\frac{3}{4}$, $\frac{2}{3}$, $\frac{1}{2}$. And it's the chance you don't get picked first AND you don't get picked second, third ... so $$\frac{5}{6} \cdot \frac{4}{5} \cdot \frac{3}{4} \cdot \frac{2}{3} \cdot \frac{1}{2} = \frac{1}{6}$$ Agree/Disagree? Thank you in advance for your help.","This question already has answers here : Probability: 10th ball is blue (9 answers) Closed 8 years ago . A couple of friends and I are struggling with coming up with an answer to this question.  It's seemingly simple but I need a little help. 6 people put their name into a hat.  One by one, names are pulled without replacement .  What are the odds that your name will be picked last ? My thoughts are: in each pick you're looking at the odds that your name is not getting picked. So first it's $\frac{5}{6}$, then $\frac{4}{5}$, then $\frac{3}{4}$, $\frac{2}{3}$, $\frac{1}{2}$. And it's the chance you don't get picked first AND you don't get picked second, third ... so $$\frac{5}{6} \cdot \frac{4}{5} \cdot \frac{3}{4} \cdot \frac{2}{3} \cdot \frac{1}{2} = \frac{1}{6}$$ Agree/Disagree? Thank you in advance for your help.",,['probability']
99,Does the gamblers fallacy not apply to Bayesian probability?,Does the gamblers fallacy not apply to Bayesian probability?,,Bayesian probability is an alternative probability theory that uses data from past outcomes to predict future outcomes. Do they have some work-around for the gamblers fallacy or do they just ignore it? Or is Bayesian probability literally just the gamblers fallacy by another name?,Bayesian probability is an alternative probability theory that uses data from past outcomes to predict future outcomes. Do they have some work-around for the gamblers fallacy or do they just ignore it? Or is Bayesian probability literally just the gamblers fallacy by another name?,,"['probability', 'probability-theory', 'bayesian']"
