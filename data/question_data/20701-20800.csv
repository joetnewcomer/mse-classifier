,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Idiotic determinant mistake?,Idiotic determinant mistake?,,I need to calculate $$\begin{vmatrix} \lambda & -1 & 0 & 0\\ -1 & \lambda & 0 & 0 \\ 0 & 0 & \lambda & -1 \\  0 & 0 & -1 & \lambda \end{vmatrix}.$$ For the life of me I don't see what my mistake is: expanding in the first row we have $$\lambda^2(\lambda ^2-1)-(-1)(-1)(\lambda ^2-1)= (\lambda^2-1)^2 .$$ What is my error?,I need to calculate $$\begin{vmatrix} \lambda & -1 & 0 & 0\\ -1 & \lambda & 0 & 0 \\ 0 & 0 & \lambda & -1 \\  0 & 0 & -1 & \lambda \end{vmatrix}.$$ For the life of me I don't see what my mistake is: expanding in the first row we have $$\lambda^2(\lambda ^2-1)-(-1)(-1)(\lambda ^2-1)= (\lambda^2-1)^2 .$$ What is my error?,,"['linear-algebra', 'determinant']"
1,Showing $A+B$ is invertible?,Showing  is invertible?,A+B,"Question number two of this released exam asks: Let $A$, $B$ be two $n \times n$ matrices with real elements such that $A^3 = B^5 = I_n$ and $AB = BA$. Prove that $A+B$ is invertible. I am not exactly sure what to do. I have observed that: $$A(A^2) = I_n$$ $$B(B^4) = I_n$$ So: $$A^{-1} = A^2$$ $$B^{-1} = B^4$$ My general approach to the problem has been to consider the binomial expansion of $(A+B)^n$ and see if things cancel out to be $I_n$ or some multiple of $I_n$, and with that construct an explicit inverse of $A+B$. For $n \in \{1,2,3,4,5\}$ nothing seems to reduce down to the identity, leading me to believe this is the wrong approach. Any hints as how to approach this problem would be greatly appreciated thanks.","Question number two of this released exam asks: Let $A$, $B$ be two $n \times n$ matrices with real elements such that $A^3 = B^5 = I_n$ and $AB = BA$. Prove that $A+B$ is invertible. I am not exactly sure what to do. I have observed that: $$A(A^2) = I_n$$ $$B(B^4) = I_n$$ So: $$A^{-1} = A^2$$ $$B^{-1} = B^4$$ My general approach to the problem has been to consider the binomial expansion of $(A+B)^n$ and see if things cancel out to be $I_n$ or some multiple of $I_n$, and with that construct an explicit inverse of $A+B$. For $n \in \{1,2,3,4,5\}$ nothing seems to reduce down to the identity, leading me to believe this is the wrong approach. Any hints as how to approach this problem would be greatly appreciated thanks.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
2,Show that $A$ and $A^T$ do not have the same eigenvectors in general,Show that  and  do not have the same eigenvectors in general,A A^T,"I understood that $A$ and $A^T$ have the same eigenvalues, since $$\det(A - \lambda I)= \det(A^T  - \lambda I) = \det(A - \lambda I)^T$$ The problem is to show that $A$ and $A^T$ do not have the same eigenvectors. I have seen around some posts, but I have not understood yet why. Could you please provide an exhaustive explanation of why in general $A$ and $A^T$ do not have the same eigenvectors?","I understood that $A$ and $A^T$ have the same eigenvalues, since $$\det(A - \lambda I)= \det(A^T  - \lambda I) = \det(A - \lambda I)^T$$ The problem is to show that $A$ and $A^T$ do not have the same eigenvectors. I have seen around some posts, but I have not understood yet why. Could you please provide an exhaustive explanation of why in general $A$ and $A^T$ do not have the same eigenvectors?",,"['linear-algebra', 'matrices']"
3,$A^2=A^*A$. Why is matrix $A$ Hermitian? [duplicate],. Why is matrix  Hermitian? [duplicate],A^2=A^*A A,"This question already has answers here : $TT^*=T^2$, show that $T$ is self-adjoint (2 answers) Closed 7 years ago . Let $A$ be $n \times n$ matrix and $A^2=A^*A$.  Why is $A$ a Hermitian matrix?","This question already has answers here : $TT^*=T^2$, show that $T$ is self-adjoint (2 answers) Closed 7 years ago . Let $A$ be $n \times n$ matrix and $A^2=A^*A$.  Why is $A$ a Hermitian matrix?",,"['linear-algebra', 'matrices', 'hermitian-matrices']"
4,Mathematical expression to form a vector from diagonal elements,Mathematical expression to form a vector from diagonal elements,,"I would like to know is there any way to express mathematically (using matrices multiplication, addition, etc.) a vector which is formed from the diagonal elements of a matrix. For example, I have a matrix called $\mathbf{M}$ and I want to create a vector  $\mathbf{v}$ such that $\mathbf{v}= \text{diagonal elements of } \mathbf{M} $ Any matrix algebra or operation do that ? Thank you.","I would like to know is there any way to express mathematically (using matrices multiplication, addition, etc.) a vector which is formed from the diagonal elements of a matrix. For example, I have a matrix called $\mathbf{M}$ and I want to create a vector  $\mathbf{v}$ such that $\mathbf{v}= \text{diagonal elements of } \mathbf{M} $ Any matrix algebra or operation do that ? Thank you.",,"['linear-algebra', 'matrices']"
5,Exercise books in linear algebra and geometry,Exercise books in linear algebra and geometry,,"I'm studying Brannan's Geometry and Lang's Introduction to Linear Algebra and I was wondering if there are some exercise books (that is, books with solved problems and exercises) that I can use as companions. The books I'm searching for should be: full of hard , non-obvious , non-common , and thought-provoking problems; rich of complete , step by step , rigorous , and enlightening solutions.","I'm studying Brannan's Geometry and Lang's Introduction to Linear Algebra and I was wondering if there are some exercise books (that is, books with solved problems and exercises) that I can use as companions. The books I'm searching for should be: full of hard , non-obvious , non-common , and thought-provoking problems; rich of complete , step by step , rigorous , and enlightening solutions.",,"['linear-algebra', 'geometry', 'reference-request', 'soft-question', 'book-recommendation']"
6,basis of vector space of real sequences over $\mathbb{R}$,basis of vector space of real sequences over,\mathbb{R},"It turns out I cannot find a basis for the vector space of all functions from $\mathbb{N}$ to $\mathbb{R}$ (over $\mathbb{R}$). By Zorn's Lemma, there is a basis. So I guess it cannot be written out constructively? What is the dimension then? I am thinking $2^{\aleph_0}$. It cannot be countable. If it were, then by a bijection of basis, it will be isomorphic to the set of sequences with finitely non-zero entries. My intuition says it is absurd. How to prove this?","It turns out I cannot find a basis for the vector space of all functions from $\mathbb{N}$ to $\mathbb{R}$ (over $\mathbb{R}$). By Zorn's Lemma, there is a basis. So I guess it cannot be written out constructively? What is the dimension then? I am thinking $2^{\aleph_0}$. It cannot be countable. If it were, then by a bijection of basis, it will be isomorphic to the set of sequences with finitely non-zero entries. My intuition says it is absurd. How to prove this?",,"['linear-algebra', 'set-theory']"
7,Anti-commutative matrices,Anti-commutative matrices,,"If $A$ and $B$ are anti-commutative square matrices, so $AB+BA=0$, how do you a) prove that $\mathrm{tr}(A)=\mathrm{tr}(B)=0$ and b) prove that the order of the matrices is even?","If $A$ and $B$ are anti-commutative square matrices, so $AB+BA=0$, how do you a) prove that $\mathrm{tr}(A)=\mathrm{tr}(B)=0$ and b) prove that the order of the matrices is even?",,"['linear-algebra', 'matrices', 'trace']"
8,REVISITED $^2$: Does a solution in $\mathbb{R}^n$ imply a solution in $\mathbb{Q}^n$? [duplicate],REVISITED : Does a solution in  imply a solution in ? [duplicate],^2 \mathbb{R}^n \mathbb{Q}^n,"This question already has an answer here : System of linear equations having a real solution has also a rational solution. (1 answer) Closed 8 years ago . Let $A ∈ M_{m\times n}(\mathbb{Q})$ and $B ∈ \mathbb{Q}^m$. Suppose that the system of linear equations $AX = B$ has a solution in $\mathbb{R}^n$. Does it necessarily have a solution in $\mathbb{Q}^n$? Where do I start? I feel I can use this to help : $$x_i=\frac{1}{a'_{ii}}\left(b'_i - \sum_{j=i+1}^{k} a'_{ij} x_j \right).$$ Here is a thought that just crossed my mind, but I'm not sure if this is legal: Suppose $Y\in \{\mathbb{R}\setminus \mathbb{Q}\}^n$ and $Y'\in\mathbb{Q}^n$ are solutions to $AX=B$. Then $AY=AY'=B$, or $A(Y-Y')=A(Y+(-Y'))=0$, but  addition between $\{\mathbb{R}\setminus \mathbb{Q}\}^n$ and $\mathbb{Q}^n$ is . . .","This question already has an answer here : System of linear equations having a real solution has also a rational solution. (1 answer) Closed 8 years ago . Let $A ∈ M_{m\times n}(\mathbb{Q})$ and $B ∈ \mathbb{Q}^m$. Suppose that the system of linear equations $AX = B$ has a solution in $\mathbb{R}^n$. Does it necessarily have a solution in $\mathbb{Q}^n$? Where do I start? I feel I can use this to help : $$x_i=\frac{1}{a'_{ii}}\left(b'_i - \sum_{j=i+1}^{k} a'_{ij} x_j \right).$$ Here is a thought that just crossed my mind, but I'm not sure if this is legal: Suppose $Y\in \{\mathbb{R}\setminus \mathbb{Q}\}^n$ and $Y'\in\mathbb{Q}^n$ are solutions to $AX=B$. Then $AY=AY'=B$, or $A(Y-Y')=A(Y+(-Y'))=0$, but  addition between $\{\mathbb{R}\setminus \mathbb{Q}\}^n$ and $\mathbb{Q}^n$ is . . .",,"['linear-algebra', 'matrices']"
9,Isomorphism between infinite dimensional vector spaces,Isomorphism between infinite dimensional vector spaces,,"Does defining an isomorphism $\theta: \mathbb R^{\mathbb N} \to \{\text{polynomials}\}$ make sense? It does intuitively, but I am worried about the infinite nature. Thanks.","Does defining an isomorphism $\theta: \mathbb R^{\mathbb N} \to \{\text{polynomials}\}$ make sense? It does intuitively, but I am worried about the infinite nature. Thanks.",,"['linear-algebra', 'vector-spaces']"
10,The leap to infinite dimensions,The leap to infinite dimensions,,"Extending this question , page 447 of Gilbert Strang's Algebra book says What does it mean for a vector to have infinitely many components?  There are two different answers, both good: 1)  The vector becomes $v = (v_1, v_2, v_3 ... )$ 2)  The vector becomes a function $f(x)$ .  It could be $\sin(x)$. I don't quite see in what sense the function is ""infinite dimensional"". Is it because a function is continuous, and so represents infinitely many points? The best way I can explain it is: 1D space has 1 DOF, so each ""vector"" takes you on ""one trip"" 2D space has 2 DOF, so by following each component in a 2D (x,y) vector you end up going on ""two trips"" ... $\infty$D space has $\infty$ DOF, so each component in an $\infty$D vector takes you on ""$\infty$ trips"" How does it ever end then?  3d space has 3 components to travel (x,y,z) to reach a destination point.  If we have infinite components to travel on, how do we ever reach a destination point? We should be resolving components against infinite axes and so never reach a final destination point.","Extending this question , page 447 of Gilbert Strang's Algebra book says What does it mean for a vector to have infinitely many components?  There are two different answers, both good: 1)  The vector becomes $v = (v_1, v_2, v_3 ... )$ 2)  The vector becomes a function $f(x)$ .  It could be $\sin(x)$. I don't quite see in what sense the function is ""infinite dimensional"". Is it because a function is continuous, and so represents infinitely many points? The best way I can explain it is: 1D space has 1 DOF, so each ""vector"" takes you on ""one trip"" 2D space has 2 DOF, so by following each component in a 2D (x,y) vector you end up going on ""two trips"" ... $\infty$D space has $\infty$ DOF, so each component in an $\infty$D vector takes you on ""$\infty$ trips"" How does it ever end then?  3d space has 3 components to travel (x,y,z) to reach a destination point.  If we have infinite components to travel on, how do we ever reach a destination point? We should be resolving components against infinite axes and so never reach a final destination point.",,"['linear-algebra', 'functions', 'fourier-series', 'infinity']"
11,Significance of Matrix-Vector multiplication,Significance of Matrix-Vector multiplication,,Can someone give me an example illustrating physical significance of the matrix-vector multiplication? Does multiplying a vector by matrix transforms it in some way? Do left & right multiplication signify two different things? Is matrix a scalar thing? (EDIT) Thank you .,Can someone give me an example illustrating physical significance of the matrix-vector multiplication? Does multiplying a vector by matrix transforms it in some way? Do left & right multiplication signify two different things? Is matrix a scalar thing? (EDIT) Thank you .,,"['linear-algebra', 'matrices']"
12,Eigenvalues of Symmetric Matrix Plus Diagonal Matrix,Eigenvalues of Symmetric Matrix Plus Diagonal Matrix,,"Let $M$ be a real symmetric matrix and let $D$ be a real diagonal matrix. Can we say something about the eigenvalues of $M+D$ in terms of the   eigenvalues of $M$? For example, if $D$ is a constant multiple of $I$ ($D = cI$), then the eigenvalues of $M+D$ is just $c$ plus the eigenvalues of $M$. Can we get bounds on the locations of the eigenvalues of $M+D$ ?","Let $M$ be a real symmetric matrix and let $D$ be a real diagonal matrix. Can we say something about the eigenvalues of $M+D$ in terms of the   eigenvalues of $M$? For example, if $D$ is a constant multiple of $I$ ($D = cI$), then the eigenvalues of $M+D$ is just $c$ plus the eigenvalues of $M$. Can we get bounds on the locations of the eigenvalues of $M+D$ ?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
13,Eigenvalues for a product of matrices,Eigenvalues for a product of matrices,,"It was mentioned in one MSE answer that eigenvalues of products of square matrices are equal (see the answer of user1551 for Eigenvalues of Matrices and Eigenvalue of product of Matrices ) Let's denote this fact: $ \ \ \ \ $ $\text{eig}(AB)=\text{eig}(BA)$. However .. how can this be explained in the case where matrices don't commute? Does some kind of geometrical interpretation of this    statement exist - at least in the case of 3D orthogonal matrices where    it is known that they usually don't commute ? Can the statement be extended for a case of product of more number of matrices, for example: $\text{eig}(A_1{A_2} ... A_n)=\text{eig}(A_n{A_{n-1}} ... A_1)=    \text{eig}(A_{n-1}{A_{n-2}} ... A_n)=$  etc... ?","It was mentioned in one MSE answer that eigenvalues of products of square matrices are equal (see the answer of user1551 for Eigenvalues of Matrices and Eigenvalue of product of Matrices ) Let's denote this fact: $ \ \ \ \ $ $\text{eig}(AB)=\text{eig}(BA)$. However .. how can this be explained in the case where matrices don't commute? Does some kind of geometrical interpretation of this    statement exist - at least in the case of 3D orthogonal matrices where    it is known that they usually don't commute ? Can the statement be extended for a case of product of more number of matrices, for example: $\text{eig}(A_1{A_2} ... A_n)=\text{eig}(A_n{A_{n-1}} ... A_1)=    \text{eig}(A_{n-1}{A_{n-2}} ... A_n)=$  etc... ?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
14,Trace of the $k$-th Exterior Power of a Linear Operator,Trace of the -th Exterior Power of a Linear Operator,k,"Let $V$ be an $n$ dimensional vector space over a field $F$ and $T$ be a linear operator over $V$ . Assume that the characteristic of $F$ is not $2$ . Definition. Consider the map $f_1:V^n\to \Lambda^n V$ as $$f(v_1, \ldots, v_n)= \sum_{i=1}^n v_1\wedge \cdots \wedge v_{i-1}\wedge Tv_i\wedge v_{i+1} \wedge \cdots \wedge v_n$$ This is an alternating multilinear map and thus it induces a unique linear map $\Lambda^n V\to \Lambda^n V$ . Since $\dim(\Lambda^n V)=1$ , this linear map is multiplication by a constant which we call the trace of $T$ . The above is standard and it naturally calls for the following generalization before which we discuss a notation. Given an $n$ tuple $(v_1, \ldots, v_n)$ of vectors in $V$ and an increasing $k$ -tuple $I=(i_1, \ldots , i_k)$ of integers between $1$ and $n$ , write $v_{I, j}$ to denote $Tv_j$ if $j$ appears in $I$ and simply $v_j$ if $j$ does not appear in $I$ . Further write $v_I$ to denote $v_{I, 1}\wedge \cdots \wedge v_{I, n}$ . Definition. Let $f_k:V^n\to \Lambda^n V$ be defined as $$f_k(v_1, \ldots, v_n)= \sum_{I \text{ an increasing }k\text{-tuple}}v_I$$ Then $f_k$ is an alternating multilinear map and this induces a unique linear map $\Lambda^n V\to \Lambda^n V$ . Again, this linear map is multiplication by a constant which we call the $k$ -th trace of $T$ and denote it as $\text{trace}_k(T)$ . From this post I have am convinced that the following is true Statement. $\text{trace}_k(T)= \text{trace}(\Lambda^k T)$ . I am unable to prove this.","Let be an dimensional vector space over a field and be a linear operator over . Assume that the characteristic of is not . Definition. Consider the map as This is an alternating multilinear map and thus it induces a unique linear map . Since , this linear map is multiplication by a constant which we call the trace of . The above is standard and it naturally calls for the following generalization before which we discuss a notation. Given an tuple of vectors in and an increasing -tuple of integers between and , write to denote if appears in and simply if does not appear in . Further write to denote . Definition. Let be defined as Then is an alternating multilinear map and this induces a unique linear map . Again, this linear map is multiplication by a constant which we call the -th trace of and denote it as . From this post I have am convinced that the following is true Statement. . I am unable to prove this.","V n F T V F 2 f_1:V^n\to \Lambda^n V f(v_1, \ldots, v_n)= \sum_{i=1}^n v_1\wedge \cdots \wedge v_{i-1}\wedge Tv_i\wedge v_{i+1} \wedge \cdots \wedge v_n \Lambda^n V\to \Lambda^n V \dim(\Lambda^n V)=1 T n (v_1, \ldots, v_n) V k I=(i_1, \ldots , i_k) 1 n v_{I, j} Tv_j j I v_j j I v_I v_{I, 1}\wedge \cdots \wedge v_{I, n} f_k:V^n\to \Lambda^n V f_k(v_1, \ldots, v_n)= \sum_{I \text{ an increasing }k\text{-tuple}}v_I f_k \Lambda^n V\to \Lambda^n V k T \text{trace}_k(T) \text{trace}_k(T)= \text{trace}(\Lambda^k T)","['linear-algebra', 'multilinear-algebra', 'trace', 'exterior-algebra']"
15,Families of Square Roots of Identity Matrices,Families of Square Roots of Identity Matrices,,"I just analysed this equation for real matrices $$ A^2=\begin{pmatrix}a&b\\c&d\end{pmatrix}^2=I $$ From the main diagonal of $A^2$ we must have $a^2+bc=bc+d^2=1$ showing that $d=\pm a$. CASE 1: If $b=0$ we have $a^2=d^2=1$ so then $c=0$. If $c=0$ the same argument renders $b=0$. CASE 2: Otherwise assume $b,c\neq 0$. Then if $d=a$ the opposite diagonal of $A^2$ shows that $2ab=2ac=0$. Thus $a=d=0$ and the equation $a^2+bc=1$ yields $bc=1$. CASE 3: Now, if $d=-a$ we have $ab-ab=ac-ac=0$ which is always true, so then only the first equation $a^2+bc=1$ matters. So we have three sets of solutions: CASE 1: gives the four solutions $b=c=0$ and $a,d\in\{-1,1\}$. CASE 2: gives an infinite family of solutions $a=d=0$ and $b=x,c=x^{-1}$ with $x\in\mathbb R\setminus\{0\}$. CASE 3: gives a last family of solutions: choose $(x,y)\in\mathbb R\times\left(\mathbb R\setminus\{0\}\right)$ and define $a=x,d=-x$ and $b=y,c=(1-x^2)y^{-1}$. I find it interesting how the solutions divide nicely into advancing stages of first discrete, then one-dimensional, and finally two-dimensional parametrisations in this analysis. I wonder if that is purely a coincidence ... QUESTION: What would a similar analysis of $A^2=I$ where we are dealing with $3\times 3$ matrices look like? The case of $3\times 3$ looks rather complicated already, so I would hardly dream of considering the general case of $n\times n$ matrices.","I just analysed this equation for real matrices $$ A^2=\begin{pmatrix}a&b\\c&d\end{pmatrix}^2=I $$ From the main diagonal of $A^2$ we must have $a^2+bc=bc+d^2=1$ showing that $d=\pm a$. CASE 1: If $b=0$ we have $a^2=d^2=1$ so then $c=0$. If $c=0$ the same argument renders $b=0$. CASE 2: Otherwise assume $b,c\neq 0$. Then if $d=a$ the opposite diagonal of $A^2$ shows that $2ab=2ac=0$. Thus $a=d=0$ and the equation $a^2+bc=1$ yields $bc=1$. CASE 3: Now, if $d=-a$ we have $ab-ab=ac-ac=0$ which is always true, so then only the first equation $a^2+bc=1$ matters. So we have three sets of solutions: CASE 1: gives the four solutions $b=c=0$ and $a,d\in\{-1,1\}$. CASE 2: gives an infinite family of solutions $a=d=0$ and $b=x,c=x^{-1}$ with $x\in\mathbb R\setminus\{0\}$. CASE 3: gives a last family of solutions: choose $(x,y)\in\mathbb R\times\left(\mathbb R\setminus\{0\}\right)$ and define $a=x,d=-x$ and $b=y,c=(1-x^2)y^{-1}$. I find it interesting how the solutions divide nicely into advancing stages of first discrete, then one-dimensional, and finally two-dimensional parametrisations in this analysis. I wonder if that is purely a coincidence ... QUESTION: What would a similar analysis of $A^2=I$ where we are dealing with $3\times 3$ matrices look like? The case of $3\times 3$ looks rather complicated already, so I would hardly dream of considering the general case of $n\times n$ matrices.",,"['linear-algebra', 'matrices']"
16,"If A+tB is nilpotent for n+1 distinct values of t, then A and B are nilpotent.","If A+tB is nilpotent for n+1 distinct values of t, then A and B are nilpotent.",,"Suppose $A$ and $B$ are $n\times n$ matrices over $\mathbb{R}$ such that for $n+1$ distinct $t \in \mathbb{R}$ , the matrix $A+tB$ is nilpotent. Prove that $A$ and $B$ are nilpotent. What I've tried so far: Define $f(t)=(A+tB)^n$ . Then $f(t)$ has polynomials of degree at most n as its entries. Each of these polynomials has n+1 distinct roots, and hence is the constant zero polynomial. This shows that for all $t \in \mathbb{R} \ f(t)=0$ , which in particular gives $A^n= f(0) =0$ that shows A is nilpotent. Now if none of the $t_1,...t_{n+1}$ , that make A+tB nilpotent, is zero, we can use $g(t)=(tA+B)^n$ with roots $\frac{1}{t_i}$ similarly and show that B, too, is nilpotent. But I have no idea in case one of the $t_i$ is zero. Thanks in advance.","Suppose and are matrices over such that for distinct , the matrix is nilpotent. Prove that and are nilpotent. What I've tried so far: Define . Then has polynomials of degree at most n as its entries. Each of these polynomials has n+1 distinct roots, and hence is the constant zero polynomial. This shows that for all , which in particular gives that shows A is nilpotent. Now if none of the , that make A+tB nilpotent, is zero, we can use with roots similarly and show that B, too, is nilpotent. But I have no idea in case one of the is zero. Thanks in advance.","A B n\times n \mathbb{R} n+1 t \in \mathbb{R} A+tB A B f(t)=(A+tB)^n f(t) t \in \mathbb{R} \ f(t)=0 A^n= f(0) =0 t_1,...t_{n+1} g(t)=(tA+B)^n \frac{1}{t_i} t_i","['linear-algebra', 'matrices', 'nilpotence']"
17,"Prove that if the sum of each row of A equals s, then s is an eigenvalue of A. [duplicate]","Prove that if the sum of each row of A equals s, then s is an eigenvalue of A. [duplicate]",,"This question already has answers here : Prove that if the sum of each row of $A$ equals $s$, then $s$ is an eigenvalue of $A$. (2 answers) Closed 9 years ago . Consider an $n \times n$ matrix $A$ with the property that the row sums all equal the same number $s$. Show that  $s$ is an eigenvalue of $A$. [Hint: Find an eigenvector] My attempt: By definition: $Ax = sx$ which implies that $(A - sI)x = 0$ $s$ is an eigenvalue for $A$ iff $\det(A - sI)  = 0$ When you do $A - sI$ the sum of each row is now $0$. I think that's important but I don't know what it means. So this is where I'm stuck","This question already has answers here : Prove that if the sum of each row of $A$ equals $s$, then $s$ is an eigenvalue of $A$. (2 answers) Closed 9 years ago . Consider an $n \times n$ matrix $A$ with the property that the row sums all equal the same number $s$. Show that  $s$ is an eigenvalue of $A$. [Hint: Find an eigenvector] My attempt: By definition: $Ax = sx$ which implies that $(A - sI)x = 0$ $s$ is an eigenvalue for $A$ iff $\det(A - sI)  = 0$ When you do $A - sI$ the sum of each row is now $0$. I think that's important but I don't know what it means. So this is where I'm stuck",,"['linear-algebra', 'matrices', 'determinant', 'matrix-equations']"
18,How can we compute Pseudoinverse for any Matrix,How can we compute Pseudoinverse for any Matrix,,"If we have a system of linear equations $Ax=b$, then in case $A$ is invertible it easy to say that the solution is $x=A^{-1}b$. In all other cases, even if $A$ is invertible, the solution if it exist will be in the form $x=A^{+}b+(I-A^{+}A)w$, where $A^{+}$is called pseudoinvers of  $A$, and $w$ is a vector of free parameters. My question here is how to compute $A^{+}$ for any matrix $A$? what is the way for doing that? It is easy to compute $A^{+}$ if the column are linearly independent (so that $m>=n$), $A^{+}=(A^{T}A)^{-1}A$,  and also if the rows are linearly independent (so that $m<=n)$, $A^{+}=A^{T}(AA^{T})^{-1}$, but I dont know how to compute $A^{+}$ if $A$ is sequare non invertible matrix or if the columns or rows are not linearly independents. The above does not works. If anyone have any idea about computing pseudoinverse or if there is easier method for finding the solution $x$ for system of linear equations $Ax=b$, please help me in all cases, and please keep in mind that I want to find the general form for the solution if it exist not just for a particular $b$.","If we have a system of linear equations $Ax=b$, then in case $A$ is invertible it easy to say that the solution is $x=A^{-1}b$. In all other cases, even if $A$ is invertible, the solution if it exist will be in the form $x=A^{+}b+(I-A^{+}A)w$, where $A^{+}$is called pseudoinvers of  $A$, and $w$ is a vector of free parameters. My question here is how to compute $A^{+}$ for any matrix $A$? what is the way for doing that? It is easy to compute $A^{+}$ if the column are linearly independent (so that $m>=n$), $A^{+}=(A^{T}A)^{-1}A$,  and also if the rows are linearly independent (so that $m<=n)$, $A^{+}=A^{T}(AA^{T})^{-1}$, but I dont know how to compute $A^{+}$ if $A$ is sequare non invertible matrix or if the columns or rows are not linearly independents. The above does not works. If anyone have any idea about computing pseudoinverse or if there is easier method for finding the solution $x$ for system of linear equations $Ax=b$, please help me in all cases, and please keep in mind that I want to find the general form for the solution if it exist not just for a particular $b$.",,"['linear-algebra', 'pseudoinverse']"
19,How to make a matrix positive semidefinite,How to make a matrix positive semidefinite,,"We have a symmetric matrix $A$, with some entries specified and others not. We are trying to find the values of the unspecified entries so that the matrix $A$ becomes positive semidefinite. How can I prove that I can assume that the diagonal entries of $A$ are specified? Intuitively, if we do not specify a  diagonal entry, say $i$th entry, we can take it to infinity. Then the positive definiteness of $A$ is equal to the positive definiteness of the new matrix $A[-i,-i]$ where we remove the $i$th column and row. I do not know how to show this mathematically.","We have a symmetric matrix $A$, with some entries specified and others not. We are trying to find the values of the unspecified entries so that the matrix $A$ becomes positive semidefinite. How can I prove that I can assume that the diagonal entries of $A$ are specified? Intuitively, if we do not specify a  diagonal entry, say $i$th entry, we can take it to infinity. Then the positive definiteness of $A$ is equal to the positive definiteness of the new matrix $A[-i,-i]$ where we remove the $i$th column and row. I do not know how to show this mathematically.",,"['linear-algebra', 'matrices']"
20,Odd-dimensional complex skew-symmetric matrix has eigenvalue $0$,Odd-dimensional complex skew-symmetric matrix has eigenvalue,0,"There is the standard proof using $$\det(A)=\det( A^{T} ) = \det(-A)=(-1)^n \det(A)$$  I would like a proof that avoids this. Specifically, there is the proof that for $A$ a $\bf{real} $ matrix, the transpose is the same as the adjoint, which gives (using the complex inner product) $\lambda \|x\|^2 =\langle  Ax, x \rangle= \langle  x, -Ax \rangle=-\overline{\lambda } \|x\|^{2}$, so any eigenvalue is purely imaginary. Then we conclude that, since any odd-dimensional real matrix has a real eigenvalue, that eigenvalue must be zero.  This argument doesn't work for a general complex skew-symmetric matrix.  Is there something I'm missing, is there a way to modify this argument to get that zero is an eigenvalue for the complex case?  Also, can somebody please give a geometric reason why odd-dimensional skew-symmetric matrices have zero determinant (equiv., a zero eigenvalue)? Thanks!","There is the standard proof using $$\det(A)=\det( A^{T} ) = \det(-A)=(-1)^n \det(A)$$  I would like a proof that avoids this. Specifically, there is the proof that for $A$ a $\bf{real} $ matrix, the transpose is the same as the adjoint, which gives (using the complex inner product) $\lambda \|x\|^2 =\langle  Ax, x \rangle= \langle  x, -Ax \rangle=-\overline{\lambda } \|x\|^{2}$, so any eigenvalue is purely imaginary. Then we conclude that, since any odd-dimensional real matrix has a real eigenvalue, that eigenvalue must be zero.  This argument doesn't work for a general complex skew-symmetric matrix.  Is there something I'm missing, is there a way to modify this argument to get that zero is an eigenvalue for the complex case?  Also, can somebody please give a geometric reason why odd-dimensional skew-symmetric matrices have zero determinant (equiv., a zero eigenvalue)? Thanks!",,"['linear-algebra', 'eigenvalues-eigenvectors']"
21,"Prove that if a matrix A is symmetric, then it is diagonalisable","Prove that if a matrix A is symmetric, then it is diagonalisable",,"I need to prove that if a matrix $A_{2 \times 2}$ is symmetric, i.e., $A^t = A$, then it is diagonalisable. I know that a matrix $M_{n \times n}$ is diagonalisable, if and only if there is a basis of $K^n$ consisting of its eigenvectors. That is, I must have $n$ linear independent eigenvectors of $M$. So, in order to prove that the matrix $A$ is diagonalisable, I must prove that it has $n$ linear independent eigenvector. Matrix A has the following form: $$ A = \left[ {\begin{array}{cc}  a & b \\  b & c \end{array} } \right] $$ Therefore, its characteristic equation is: $$ (\lambda -a)(\lambda -c) -b^2 =0 $$ Now I need to solve this equation in order to find its eigenvalues in terms of $a$, $b$ and $c$. I'm not sure how to follow from here.","I need to prove that if a matrix $A_{2 \times 2}$ is symmetric, i.e., $A^t = A$, then it is diagonalisable. I know that a matrix $M_{n \times n}$ is diagonalisable, if and only if there is a basis of $K^n$ consisting of its eigenvectors. That is, I must have $n$ linear independent eigenvectors of $M$. So, in order to prove that the matrix $A$ is diagonalisable, I must prove that it has $n$ linear independent eigenvector. Matrix A has the following form: $$ A = \left[ {\begin{array}{cc}  a & b \\  b & c \end{array} } \right] $$ Therefore, its characteristic equation is: $$ (\lambda -a)(\lambda -c) -b^2 =0 $$ Now I need to solve this equation in order to find its eigenvalues in terms of $a$, $b$ and $c$. I'm not sure how to follow from here.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
22,The 'ratio' of a 2x2 matrix,The 'ratio' of a 2x2 matrix,,"Define the 'ratio' of a 2x2 matrix $$A= \begin{pmatrix} a & b\\ c & d \end{pmatrix} $$ to be $\frac{b}{c}$ when $c\neq 0$ . Show that the ratio of $A^n$ is equal to the ratio of $A$ , when the ratio of $A^n$ is well-defined. My instinct is to go with a proof by induction, but I really can't see a way to prove this.","Define the 'ratio' of a 2x2 matrix to be when . Show that the ratio of is equal to the ratio of , when the ratio of is well-defined. My instinct is to go with a proof by induction, but I really can't see a way to prove this.",A= \begin{pmatrix} a & b\\ c & d \end{pmatrix}  \frac{b}{c} c\neq 0 A^n A A^n,"['linear-algebra', 'matrices']"
23,The existence of an algebra homomorphism between $\mathcal{M}_n({\mathbb{K}})$ and $\mathcal{M}_s(\mathbb{K})$ implies $n | s$,The existence of an algebra homomorphism between  and  implies,\mathcal{M}_n({\mathbb{K}}) \mathcal{M}_s(\mathbb{K}) n | s,"Let $n,s \geq 1$ be integers and $\mathbb{K}$ a field. We assume there exist $\Phi : \mathcal{M}_n(\mathbb{K}) \rightarrow \mathcal{M}_s(\mathbb{K})$ an unital algebra homomorphism ( $\Phi(I_n)=I_s$ ). See here for the definition . We must show that necessarily $n | s$ and there exists $P \in \textrm{GL}_s(\mathbb{K})$ such that for all $A \in \mathcal{M}_n(\mathbb{K})$ : $$ P \cdot \Phi(A) \cdot P^{-1} = \begin{pmatrix} A & & (0)\\  & \ddots &\\ (0) & & A \\ \end{pmatrix} $$ I have tried, but I cannot find a way to exploit $\Phi$ to show this. I am looking for a solution of this that uses only ""basic"" theorems of linear algebra (Bachelor's level). Any help is welcome.","Let be integers and a field. We assume there exist an unital algebra homomorphism ( ). See here for the definition . We must show that necessarily and there exists such that for all : I have tried, but I cannot find a way to exploit to show this. I am looking for a solution of this that uses only ""basic"" theorems of linear algebra (Bachelor's level). Any help is welcome.","n,s \geq 1 \mathbb{K} \Phi : \mathcal{M}_n(\mathbb{K}) \rightarrow \mathcal{M}_s(\mathbb{K}) \Phi(I_n)=I_s n | s P \in \textrm{GL}_s(\mathbb{K}) A \in \mathcal{M}_n(\mathbb{K}) 
P \cdot \Phi(A) \cdot P^{-1} = \begin{pmatrix}
A & & (0)\\
 & \ddots &\\
(0) & & A \\
\end{pmatrix}
 \Phi","['linear-algebra', 'abstract-algebra', 'matrices', 'ring-homomorphism', 'similar-matrices']"
24,Motivation of Adjoints and Normal Operators,Motivation of Adjoints and Normal Operators,,"What is the motivation of adjoints and normal operators. By ""motivation,"" I mean an example, such as a proof, where it is natural to use them.","What is the motivation of adjoints and normal operators. By ""motivation,"" I mean an example, such as a proof, where it is natural to use them.",,"['linear-algebra', 'functional-analysis', 'analysis', 'operator-theory', 'spectral-theory']"
25,Quasinilpotent operators satisfying a polynomial identity,Quasinilpotent operators satisfying a polynomial identity,,"Suppose $T$ is a bounded quasinilpotent operator (that is, spectrum is zero) on an infinite dimensional Banach space such that $p(T)=0$ for some polynomial $p$. Does it follow that $T$ is actually nilpotent? In the finite dimensional case, quasinilpotent and nilpotent are the same, but not in the infinite dimensional case.","Suppose $T$ is a bounded quasinilpotent operator (that is, spectrum is zero) on an infinite dimensional Banach space such that $p(T)=0$ for some polynomial $p$. Does it follow that $T$ is actually nilpotent? In the finite dimensional case, quasinilpotent and nilpotent are the same, but not in the infinite dimensional case.",,"['linear-algebra', 'functional-analysis', 'operator-theory', 'hilbert-spaces', 'banach-spaces']"
26,Davenport's Q-method (Finding an orientation matching a set of point samples),Davenport's Q-method (Finding an orientation matching a set of point samples),,"I have an initial set of 3D positions that form a shape.  After letting them move independently, my goal is to find the best rotation of the original configuration to try to match the current state.  This is for a soft body physics simulation, the idea being that if I can construct an optimal 'rigid' frame for the deformed shape then I can apply a shape matching constraint that removes deformation without introducing energy. Existing solutions tend to find the optimal linear transformation representing the deformation, and then use various methods to decompose the matrix into rotation and scale/shear components.  However, I found the orientations provided by such methods tended to not be very stable.  After significant searching I discovered that my problem was identical to a problem solved by NASA to determine satellite orientations.  When I implemented their solution my simulation was remarkably stable.  I want to gain a better understanding of why it works. Details of Davenport's Q-method are here .  Somehow, after taking a bunch of outer, cross and dot products of the original and deformed samples, jamming them into a symmetric 4x4 matrix, and then computing the eigenbasis for that matrix, the eigenvector corresponding to the largest eigenvalue can be reinterpreted as a quaternion that is the best orientation to use.  The author of the linked paper claims this result is easy to prove, but I guess easy is relative.  Can anyone walk me through why this works?","I have an initial set of 3D positions that form a shape.  After letting them move independently, my goal is to find the best rotation of the original configuration to try to match the current state.  This is for a soft body physics simulation, the idea being that if I can construct an optimal 'rigid' frame for the deformed shape then I can apply a shape matching constraint that removes deformation without introducing energy. Existing solutions tend to find the optimal linear transformation representing the deformation, and then use various methods to decompose the matrix into rotation and scale/shear components.  However, I found the orientations provided by such methods tended to not be very stable.  After significant searching I discovered that my problem was identical to a problem solved by NASA to determine satellite orientations.  When I implemented their solution my simulation was remarkably stable.  I want to gain a better understanding of why it works. Details of Davenport's Q-method are here .  Somehow, after taking a bunch of outer, cross and dot products of the original and deformed samples, jamming them into a symmetric 4x4 matrix, and then computing the eigenbasis for that matrix, the eigenvector corresponding to the largest eigenvalue can be reinterpreted as a quaternion that is the best orientation to use.  The author of the linked paper claims this result is easy to prove, but I guess easy is relative.  Can anyone walk me through why this works?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'physics', 'rotations', 'quaternions']"
27,If $U$ is unitary operator then spectrum $\sigma(U)$ is contained inside the unit circle,If  is unitary operator then spectrum  is contained inside the unit circle,U \sigma(U),"In a Hilbert space, let $U$ be a continuous operator which it unitary. Prove $\sigma (U)\subseteq \Bbb{S}^1$ . It is important for me to know how I am doing, and I didn't come by a clear explanation so it would be appreciated to have your correcting and guiding. Attempt: $U^{*}U=UU^{*}=I$ and therefore $U^{-1}=U$ . I have $$ \| U x \|^2 = \langle U x, U x \rangle = \langle U^* U x, x \rangle = \langle x, x \rangle = \|x\|^2 $$ and the same goes for $U^*$ and therefore $\| U \| = \| U^{-1} \| = 1$ . Looking at $U-\lambda$ I can take $$ -\lambda U(U^{-1}-{1\over \lambda}I) =U-\lambda I. $$ Clearly, for $\lambda \ne 0$ , $-\lambda U$ is defined and is a linear operator. $(U^{-1}-{1\over \lambda}I)$ is invertible if $|\lambda|\ge 1$ and therefore $\sigma (A^{-1})=\sigma (A)\subseteq \{|z|\le1\}$ . But from the RHS, $U-\lambda I$ is invertible if $|\lambda|\le 1$ and therefore $\sigma(A)\subseteq \Bbb{S}^1$ . I have noticed my claims tend to have holes in them many times which I cannot spot. If it is substantially true, how can I make sure it is completely formal?","In a Hilbert space, let be a continuous operator which it unitary. Prove . It is important for me to know how I am doing, and I didn't come by a clear explanation so it would be appreciated to have your correcting and guiding. Attempt: and therefore . I have and the same goes for and therefore . Looking at I can take Clearly, for , is defined and is a linear operator. is invertible if and therefore . But from the RHS, is invertible if and therefore . I have noticed my claims tend to have holes in them many times which I cannot spot. If it is substantially true, how can I make sure it is completely formal?","U \sigma (U)\subseteq \Bbb{S}^1 U^{*}U=UU^{*}=I U^{-1}=U 
\| U x \|^2
= \langle U x, U x \rangle
= \langle U^* U x, x \rangle
= \langle x, x \rangle
= \|x\|^2
 U^* \| U \| = \| U^{-1} \| = 1 U-\lambda 
-\lambda U(U^{-1}-{1\over \lambda}I)
=U-\lambda I.
 \lambda \ne 0 -\lambda U (U^{-1}-{1\over \lambda}I) |\lambda|\ge 1 \sigma (A^{-1})=\sigma (A)\subseteq \{|z|\le1\} U-\lambda I |\lambda|\le 1 \sigma(A)\subseteq \Bbb{S}^1","['linear-algebra', 'vector-spaces', 'hilbert-spaces', 'solution-verification']"
28,Is it possible to find a companion matrix of a polynomial which is also hermitian?,Is it possible to find a companion matrix of a polynomial which is also hermitian?,,"The eigenvalues of a square matrix $A$ coincide with the roots of its characteristic polynomial $p[A]$. Conversely, if I have a polynomial $$ a_0 + a_1 x + \cdots + a_{n-1}x^{n-1} + x^n ~, $$ I can define a companion matrix $$ A[p]=\begin{bmatrix} 0 & 0 & \dots & 0 & -a_0 \\ 1 & 0 & \dots & 0 & -a_1 \\ 0 & 1 & \dots & 0 & -a_2 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & \dots & 1 & -a_{n-1} \end{bmatrix}.$$ The characteristic polynomial of the matrix $A[p]$ is the original polynomial $p$.  The companion matrix defined in this way is not Hermitian. Edit: Consider only hyperbolic polynomials, i.e., polynomials which have only real roots. However, the companion matrix is not the only matrix whose characteristic polynomial is the polynomial $p$. For an example, just consider the diagonal matrix of the eigenvalues ${\rm diag}{(\lambda_1,\lambda_2,\ldots,\lambda_n)}$. This matrix is indeed Hermitian, but one needs to calculate eigenvalues, which can be a difficult task for large matrix sizes. Therefore my question is: Is it possible to define a ""companion"" matrix (a matrix whose characteristic polynomial is the given polynomial $p$) which is Hermitian, and which is easier to calculate than the diagonal matrix of the eigenvalues ${\rm diag}{(\lambda_1,\lambda_2,\ldots,\lambda_n)}$? With easy to calculate I mean a matrix which can be written in terms of the parameters $a_i$ and without calculating the eigenvalues.","The eigenvalues of a square matrix $A$ coincide with the roots of its characteristic polynomial $p[A]$. Conversely, if I have a polynomial $$ a_0 + a_1 x + \cdots + a_{n-1}x^{n-1} + x^n ~, $$ I can define a companion matrix $$ A[p]=\begin{bmatrix} 0 & 0 & \dots & 0 & -a_0 \\ 1 & 0 & \dots & 0 & -a_1 \\ 0 & 1 & \dots & 0 & -a_2 \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & \dots & 1 & -a_{n-1} \end{bmatrix}.$$ The characteristic polynomial of the matrix $A[p]$ is the original polynomial $p$.  The companion matrix defined in this way is not Hermitian. Edit: Consider only hyperbolic polynomials, i.e., polynomials which have only real roots. However, the companion matrix is not the only matrix whose characteristic polynomial is the polynomial $p$. For an example, just consider the diagonal matrix of the eigenvalues ${\rm diag}{(\lambda_1,\lambda_2,\ldots,\lambda_n)}$. This matrix is indeed Hermitian, but one needs to calculate eigenvalues, which can be a difficult task for large matrix sizes. Therefore my question is: Is it possible to define a ""companion"" matrix (a matrix whose characteristic polynomial is the given polynomial $p$) which is Hermitian, and which is easier to calculate than the diagonal matrix of the eigenvalues ${\rm diag}{(\lambda_1,\lambda_2,\ldots,\lambda_n)}$? With easy to calculate I mean a matrix which can be written in terms of the parameters $a_i$ and without calculating the eigenvalues.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'hermitian-matrices', 'companion-matrices']"
29,What does inner product actually mean?,What does inner product actually mean?,,"What does inner product actually mean? So far most of the cases that I encounter seems to suggest that dot product is the only useful inner product. I mean most of the things that we discuss about defines inner product as dot product. For example, when we talk about norm or length of a vector. If that is the case, why do we need inner product? I don't really get the purpose of defining such thing.","What does inner product actually mean? So far most of the cases that I encounter seems to suggest that dot product is the only useful inner product. I mean most of the things that we discuss about defines inner product as dot product. For example, when we talk about norm or length of a vector. If that is the case, why do we need inner product? I don't really get the purpose of defining such thing.",,"['linear-algebra', 'soft-question']"
30,Minimize the Frobenius norm of the difference of two matrices with respect to matrix: $\underset{B} {\mathrm{argmin}} \left\| A- B \right\|_F$,Minimize the Frobenius norm of the difference of two matrices with respect to matrix:,\underset{B} {\mathrm{argmin}} \left\| A- B \right\|_F,"The following question is similar to this one, but I think that it is not straightforward to move from one to the other, so please take a look. Otherwise, please let me know and I will delete it. Let $A,B\in\Bbb{R}^{n\times n}$ two square real $n\times n$ matrices with the additional properties that $A$ is also symmetric, and $B$ is diagonal with entries $\{b_i\colon b_i\in\Bbb{R}, i=1,\ldots,n\}$, that is, $B=\operatorname{diag}(b_1,\ldots,b_n)$. We want to minimize the Frobenius norm of the difference of $A$, $B$ with respect to the matrix $B$. Let $B^{\star}$ denote the minimizer of the aforementioned norm, that is $$ B^{\star} = \underset{B} {\mathrm{argmin}} \left\| A- B \right\|_F, $$ where $\lVert A\rVert_F$ denotes the Frobenius norm of an $n\times n$ real symmetric matrix $A=\big(a_{ij}\big)_{i,j=1}^n$, and is given by $$ \lVert A\rVert_F = \left(\sum_{i,j=1}^n \left\| a_{ij} \right\|^2\right)^{\frac{1}{2}} = \sqrt{\operatorname{tr}(A^\top A)} = \sqrt{\operatorname{tr}(AA^\top)}. $$","The following question is similar to this one, but I think that it is not straightforward to move from one to the other, so please take a look. Otherwise, please let me know and I will delete it. Let $A,B\in\Bbb{R}^{n\times n}$ two square real $n\times n$ matrices with the additional properties that $A$ is also symmetric, and $B$ is diagonal with entries $\{b_i\colon b_i\in\Bbb{R}, i=1,\ldots,n\}$, that is, $B=\operatorname{diag}(b_1,\ldots,b_n)$. We want to minimize the Frobenius norm of the difference of $A$, $B$ with respect to the matrix $B$. Let $B^{\star}$ denote the minimizer of the aforementioned norm, that is $$ B^{\star} = \underset{B} {\mathrm{argmin}} \left\| A- B \right\|_F, $$ where $\lVert A\rVert_F$ denotes the Frobenius norm of an $n\times n$ real symmetric matrix $A=\big(a_{ij}\big)_{i,j=1}^n$, and is given by $$ \lVert A\rVert_F = \left(\sum_{i,j=1}^n \left\| a_{ij} \right\|^2\right)^{\frac{1}{2}} = \sqrt{\operatorname{tr}(A^\top A)} = \sqrt{\operatorname{tr}(AA^\top)}. $$",,"['linear-algebra', 'matrices', 'optimization']"
31,How to prove $\exp(\ln M)=M$,How to prove,\exp(\ln M)=M,Given a $n\times n$ real (complex) matrix $A$. Let me define: $$\exp A=\sum_{n=0}^\infty \frac{A^n}{n!}$$ and $$\ln A=\sum_{n=1}^\infty (-1)^{n+1}\frac{(A-I)^n}{n}$$ Let assume that the $2$ above series converge for $A=M$. How can I prove that: $$\exp(\ln M)=M$$,Given a $n\times n$ real (complex) matrix $A$. Let me define: $$\exp A=\sum_{n=0}^\infty \frac{A^n}{n!}$$ and $$\ln A=\sum_{n=1}^\infty (-1)^{n+1}\frac{(A-I)^n}{n}$$ Let assume that the $2$ above series converge for $A=M$. How can I prove that: $$\exp(\ln M)=M$$,,['linear-algebra']
32,Proof that Every Positive Operator on V has a Unique Positive Square Root,Proof that Every Positive Operator on V has a Unique Positive Square Root,,"Suppose $V$ is a finite-dimensional, nonzero, inner-product space over $\Bbb{F}$, and $\Bbb{F}$ denotes $\Bbb{R}$ or $\Bbb{C}$. My thought is : suppose $T$ is a positive operator; thus, $T$ is self-adjoint. Every self-adjoint operator on $V$ has a diagonal matrix with respect to some orthonormal basis of $V$. But this doesn't tell me that $T$ has distinct eigenvalues. It tells me that $V$ has an orthonormal basis consisting of eigenvectors of $V$, of course, they are linear independent, but it doesn't tell me each vector from the basis has a unique eigenvalue. It seems that, without distinct eigenvalues, I can't prove the uniqueness of positive square root.","Suppose $V$ is a finite-dimensional, nonzero, inner-product space over $\Bbb{F}$, and $\Bbb{F}$ denotes $\Bbb{R}$ or $\Bbb{C}$. My thought is : suppose $T$ is a positive operator; thus, $T$ is self-adjoint. Every self-adjoint operator on $V$ has a diagonal matrix with respect to some orthonormal basis of $V$. But this doesn't tell me that $T$ has distinct eigenvalues. It tells me that $V$ has an orthonormal basis consisting of eigenvectors of $V$, of course, they are linear independent, but it doesn't tell me each vector from the basis has a unique eigenvalue. It seems that, without distinct eigenvalues, I can't prove the uniqueness of positive square root.",,['linear-algebra']
33,"Let $J$ be a $k \times k$ jordan block, prove that any matrix which commutes with $J$ is a polynomial in $J$","Let  be a  jordan block, prove that any matrix which commutes with  is a polynomial in",J k \times k J J,"Let $J$ be a $k \times k$ jordan block, prove that any matrix which commutes with $J$ is a polynomial in $J$. I appreciate your hints, Thanks","Let $J$ be a $k \times k$ jordan block, prove that any matrix which commutes with $J$ is a polynomial in $J$. I appreciate your hints, Thanks",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
34,Injectivity of $A-\lambda I$,Injectivity of,A-\lambda I,I'm reading a paper on determinants and on one point the author states that: A complex number $\lambda$ is called an eigenvalue of matrix $A$ if $A-\lambda I$ is not injective. Why is this? Could someone clarify :) Thank you! =),I'm reading a paper on determinants and on one point the author states that: A complex number $\lambda$ is called an eigenvalue of matrix $A$ if $A-\lambda I$ is not injective. Why is this? Could someone clarify :) Thank you! =),,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
35,Operators on a Tensor Product Space,Operators on a Tensor Product Space,,Suppose $V$ and $W$ are vector spaces over the same field.  Is $\text{End}(V \otimes W)$ the same as $\text{End}(V) \otimes \text{End}(W)$?  Are there special names for these spaces?,Suppose $V$ and $W$ are vector spaces over the same field.  Is $\text{End}(V \otimes W)$ the same as $\text{End}(V) \otimes \text{End}(W)$?  Are there special names for these spaces?,,"['linear-algebra', 'vector-spaces', 'tensor-products']"
36,Existence of T-invariant complement of T-invariant subspace when T is diagonalisable,Existence of T-invariant complement of T-invariant subspace when T is diagonalisable,,"Let $V$ be a complex linear space of dimension $n$. Let $T \in End(V)$ such that $T$ is diagonalisable. Prove that each $T$-invariant subspace $W$ of $V$ has a complementary $T$-invariant subspace $W'$ such that $V= W \oplus W'$. Note: Let $\{e_1,...e_n\}$ be the set of eigenvectors together with eigenspaces $V_{\lambda_1},...V_{\lambda_n}$ of $T$. It's sufficient to show that every $T$-invariant subspace $W$ must be a direct sum of eigenspaces, then it'll be trivial to find $W'$ (just take the rest eigenspaces not in the direct sum and glue them to $W$).. But how to prove $W$ is a direct sum of eigenspaces?","Let $V$ be a complex linear space of dimension $n$. Let $T \in End(V)$ such that $T$ is diagonalisable. Prove that each $T$-invariant subspace $W$ of $V$ has a complementary $T$-invariant subspace $W'$ such that $V= W \oplus W'$. Note: Let $\{e_1,...e_n\}$ be the set of eigenvectors together with eigenspaces $V_{\lambda_1},...V_{\lambda_n}$ of $T$. It's sufficient to show that every $T$-invariant subspace $W$ must be a direct sum of eigenspaces, then it'll be trivial to find $W'$ (just take the rest eigenspaces not in the direct sum and glue them to $W$).. But how to prove $W$ is a direct sum of eigenspaces?",,['linear-algebra']
37,"If $V_0$ is the subspace of matrices of the form $C=AB-BA$ for some $A,B$ in a vector space $V$ then $V_0=\{A\in V|\operatorname{Trace} (A)=0\}$",If  is the subspace of matrices of the form  for some  in a vector space  then,"V_0 C=AB-BA A,B V V_0=\{A\in V|\operatorname{Trace} (A)=0\}","If $V_0$ is the subspace consisting of matrices of the form $C=AB-BA$ for some $A,B$ in a vector space $V$ then $V_0=\{A\in V|\operatorname{Trace}(A)=0\}$. The problem above is one of the past qualifying exam problems. I can prove that  $V_0\subset\{A\in V|\operatorname{Trace}(A)=0\}$ but I do not know what to do with converse. Thank you in advance.","If $V_0$ is the subspace consisting of matrices of the form $C=AB-BA$ for some $A,B$ in a vector space $V$ then $V_0=\{A\in V|\operatorname{Trace}(A)=0\}$. The problem above is one of the past qualifying exam problems. I can prove that  $V_0\subset\{A\in V|\operatorname{Trace}(A)=0\}$ but I do not know what to do with converse. Thank you in advance.",,['linear-algebra']
38,Find out trace of a given matrix $A$ with entries from $\mathbb{Z}_{227}$,Find out trace of a given matrix  with entries from,A \mathbb{Z}_{227},Let $A$ be a $227\times 227$ matrix with entries in $\mathbb{Z}_{227}$ such that all of its eigen values are distinct. What would be its trace? I think it is zero by adding all 227 elements but i am not sure. Edited: Here I have assumed that eigenvalues are in a base field.,Let $A$ be a $227\times 227$ matrix with entries in $\mathbb{Z}_{227}$ such that all of its eigen values are distinct. What would be its trace? I think it is zero by adding all 227 elements but i am not sure. Edited: Here I have assumed that eigenvalues are in a base field.,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
39,large sets of commuting linearly independent matrices,large sets of commuting linearly independent matrices,,"Given a set of $n \times n$ real matrices which are linearly independent and commute with one another, how large can the cardinality of this set be? By using diagonal matrices we can have such a set of size $n$ and since diagonal matrices commute with upper triangular ones, we can get $n+1$ too. Can we do better? What about the case of matrices over finite fields?","Given a set of $n \times n$ real matrices which are linearly independent and commute with one another, how large can the cardinality of this set be? By using diagonal matrices we can have such a set of size $n$ and since diagonal matrices commute with upper triangular ones, we can get $n+1$ too. Can we do better? What about the case of matrices over finite fields?",,"['linear-algebra', 'matrices']"
40,GRE linear algebra question,GRE linear algebra question,,"The following is a question from the sample GRE Mathematics Subject Test found on the ETS website: Let $M$ be a $5\times 5$ real matrix. Exactly four of the following five conditions on $M$ are equivalent to each other. Which of the five conditions is equivalent to NONE of the other four? (A) For any two distinct column vectors $u$ and $v$ of $M$, the set {$u,v$} is linearly independent. (B) The homogeneous system $Mx=0$ has only the trivial solution. (C) The system of equations $Mx=b$ has a unique solution for each real $5\times 1$ column vector $b$. (D) The determinant of $M$ is nonzero. (E) There exists a $5\times 5$ real matrix $N$ such that $NM$ is the $5\times 5$ identity matrix. Apparently, the correct answer is (A), but I can't figure out why this is true. If $M$ is nonsingular, as is implied by statements (B)-(E), then isn't that equivalent to its column vectors being linearly independent? And if the 5 column vectors are independent, then I can easily show that each pair of vectors are independent. What am I missing?","The following is a question from the sample GRE Mathematics Subject Test found on the ETS website: Let $M$ be a $5\times 5$ real matrix. Exactly four of the following five conditions on $M$ are equivalent to each other. Which of the five conditions is equivalent to NONE of the other four? (A) For any two distinct column vectors $u$ and $v$ of $M$, the set {$u,v$} is linearly independent. (B) The homogeneous system $Mx=0$ has only the trivial solution. (C) The system of equations $Mx=b$ has a unique solution for each real $5\times 1$ column vector $b$. (D) The determinant of $M$ is nonzero. (E) There exists a $5\times 5$ real matrix $N$ such that $NM$ is the $5\times 5$ identity matrix. Apparently, the correct answer is (A), but I can't figure out why this is true. If $M$ is nonsingular, as is implied by statements (B)-(E), then isn't that equivalent to its column vectors being linearly independent? And if the 5 column vectors are independent, then I can easily show that each pair of vectors are independent. What am I missing?",,"['linear-algebra', 'matrices', 'gre-exam']"
41,"If $(AB)^3=BA$, then all matrices commute for a closed set","If , then all matrices commute for a closed set",(AB)^3=BA,"Let $\mathcal{M}$ be a subset of $M_n(\mathbb{R})$ , the vector space of all $n\times n$ matrices over $\mathbb{R}$ . If $AB\in\mathcal{M}$ and $(AB)^3=BA$ hold for all matrices $A, B\in \mathcal{M}$ , then (1) $AB=BA$ for all $A, B\in \mathcal{M}$ ; (2) if $I_n\in\mathcal{M}$ , then ${\rm det}(A)=\pm 1$ or $0$ for all $A\in\mathcal{M}$ . It seems to have some connections with abstract algebra. The first statement seems similar to proving that for a semi-group $G$ , if $(gh)^3=hg$ , then $G$ is abelian. The assumption of possessing the identity matrix in the second statement makes it a monoid. Maybe we do not need to apply abstract algebra here, just a thought. But I’m stuck when trying to proceed. Any help is highly appreciated.","Let be a subset of , the vector space of all matrices over . If and hold for all matrices , then (1) for all ; (2) if , then or for all . It seems to have some connections with abstract algebra. The first statement seems similar to proving that for a semi-group , if , then is abelian. The assumption of possessing the identity matrix in the second statement makes it a monoid. Maybe we do not need to apply abstract algebra here, just a thought. But I’m stuck when trying to proceed. Any help is highly appreciated.","\mathcal{M} M_n(\mathbb{R}) n\times n \mathbb{R} AB\in\mathcal{M} (AB)^3=BA A, B\in \mathcal{M} AB=BA A, B\in \mathcal{M} I_n\in\mathcal{M} {\rm det}(A)=\pm 1 0 A\in\mathcal{M} G (gh)^3=hg G","['linear-algebra', 'abstract-algebra']"
42,Verify a Matrix Proof Given $A = A^2$,Verify a Matrix Proof Given,A = A^2,"Problem Given an arbitrary matrix $ A = A^2 $ , prove that $ I - 2A = (I - 2A)^{-1}.$ Attempt $$ I - 2A = (I - 2A)^{-1} $$ $$ (I - 2A)(I - 2A) = (I - 2A)(I - 2A)^{-1} $$ $$ (I - 2A)(I - 2A) = I $$ $$ (I - 2A)^2 = I $$ This is where I'm not sure if the distributive property can be applied to the following equation. Assuming it can be, $$ (I - 2A)^2 = I $$ $$ I^2 - 4A + 4A^2 = I $$ $$ I^2 - 4A + 4A = I $$ $$ I^2 = I $$ $$ I = I $$ $$ [LHS] = [RHS] $$ Notes Is it possible to prove this property is satisfied without using the distributive property as I did above? And what would be an actual example of such a matrix A that satisfies this property? Solution to Proof After reviewing and understanding multiple solutions, including that of @JMoravitz, I've come to the following solution. Consider $ (I - 2A)^{2} $ . We must first show that $ (I - 2A)^{2} = I$ . $$ (I - 2A)^2 = (I - 2A)(I - 2A) $$ $$ (I - 2A)^2 = I^2 - 4A + 4A^2 $$ $$ (I - 2A)^2 = I^2 - 4A + 4A $$ $$ (I - 2A)^2 = I^2 $$ $$ (I - 2A)^2 = I $$ Therefore, $ I - 2A $ is the inverse of $ I - 2A $ , by the definition of matrix inversion. An example of such a matrix that satisfies this property is the $ I_{2 \times 2} $ matrix, which is an idempotent matrix . \begin{bmatrix}  1 & 0 \\ 0 & 1  \end{bmatrix}","Problem Given an arbitrary matrix , prove that Attempt This is where I'm not sure if the distributive property can be applied to the following equation. Assuming it can be, Notes Is it possible to prove this property is satisfied without using the distributive property as I did above? And what would be an actual example of such a matrix A that satisfies this property? Solution to Proof After reviewing and understanding multiple solutions, including that of @JMoravitz, I've come to the following solution. Consider . We must first show that . Therefore, is the inverse of , by the definition of matrix inversion. An example of such a matrix that satisfies this property is the matrix, which is an idempotent matrix ."," A = A^2   I - 2A = (I - 2A)^{-1}.  I - 2A = (I - 2A)^{-1}   (I - 2A)(I - 2A) = (I - 2A)(I - 2A)^{-1}   (I - 2A)(I - 2A) = I   (I - 2A)^2 = I   (I - 2A)^2 = I   I^2 - 4A + 4A^2 = I   I^2 - 4A + 4A = I   I^2 = I   I = I   [LHS] = [RHS]   (I - 2A)^{2}   (I - 2A)^{2} = I  (I - 2A)^2 = (I - 2A)(I - 2A)   (I - 2A)^2 = I^2 - 4A + 4A^2   (I - 2A)^2 = I^2 - 4A + 4A   (I - 2A)^2 = I^2   (I - 2A)^2 = I   I - 2A   I - 2A   I_{2 \times 2}  \begin{bmatrix} 
1 & 0 \\
0 & 1 
\end{bmatrix}","['linear-algebra', 'matrices', 'proof-verification']"
43,linear operator $f(X) = AXB$,linear operator,f(X) = AXB,"What are the eigenvalues of the linear operator in vector space $M_n(\mathbb R)$ $$ f(X) =  AXA^T $$   and $$ f(X) = AXA^{-1} $$   when eigenvalues of $A$ are $ \lambda_1, \lambda_2, ..., \lambda_n $? I suspect that in first case is $\{\lambda_i \cdot \lambda_j \ | \  i,j \in \{1,2,3, ..., n\}\} $ and in second $\{\lambda_i/\lambda_j \  |\  i,j \in \{1,2,3, ..., n\}\} $, but I can't prove it. More generally what are eigenvalues of $$ f(X) = AXB $$ when we know eigenvalues of $A$ and $B$?","What are the eigenvalues of the linear operator in vector space $M_n(\mathbb R)$ $$ f(X) =  AXA^T $$   and $$ f(X) = AXA^{-1} $$   when eigenvalues of $A$ are $ \lambda_1, \lambda_2, ..., \lambda_n $? I suspect that in first case is $\{\lambda_i \cdot \lambda_j \ | \  i,j \in \{1,2,3, ..., n\}\} $ and in second $\{\lambda_i/\lambda_j \  |\  i,j \in \{1,2,3, ..., n\}\} $, but I can't prove it. More generally what are eigenvalues of $$ f(X) = AXB $$ when we know eigenvalues of $A$ and $B$?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'linear-transformations']"
44,Can we extend a linearly independent list in a f.g. module of a commutative ring?,Can we extend a linearly independent list in a f.g. module of a commutative ring?,,"Let's say we're given a commutative ring $A$ with identity. We are given that $M$ is a f.g. $A$ -module. For any elements $m_1,\dots,m_n$ of $M$ , we have an $A$ -module homomorphism $\phi_{m_1,\dots,m_n}$ from $A^n$ to $M$ defined by $\phi_{m_1,\dots,m_n}(a_1,\dots,a_n)=a_1m_1+\dots+a_nm_n$ . Thus $m_1,\dots,m_n$ becomes a linearly independent list if and only if $\phi_{m_1,\dots,m_n}$ is injective, a span list if and only if $\phi_{m_1,\dots,m_n}$ is surjective. Now we say that $m_1,\dots,m_n$ is a basis for $M$ if and only if $\phi_{m_1,\dots,m_n}$ is bijective. Suppose that we are given a linearly independent list $n_1,\dots,n_k$ of $M$ , we say $n_1,\dots,n_k$ can be extended to a basis of $M$ if $n_1,\dots,n_k,m'_1,\dots,m'_r$ is a basis for $M$ for some $m'_1,\dots,m'_r \in M$ . I came up with this question when I was trying to generalize the following proposition on Sheldon Axler's Linear Algebra Done Right : Every linearly independent list of vectors in a finite-dimensional vector space can be extended to a basis of the vector space.","Let's say we're given a commutative ring with identity. We are given that is a f.g. -module. For any elements of , we have an -module homomorphism from to defined by . Thus becomes a linearly independent list if and only if is injective, a span list if and only if is surjective. Now we say that is a basis for if and only if is bijective. Suppose that we are given a linearly independent list of , we say can be extended to a basis of if is a basis for for some . I came up with this question when I was trying to generalize the following proposition on Sheldon Axler's Linear Algebra Done Right : Every linearly independent list of vectors in a finite-dimensional vector space can be extended to a basis of the vector space.","A M A m_1,\dots,m_n M A \phi_{m_1,\dots,m_n} A^n M \phi_{m_1,\dots,m_n}(a_1,\dots,a_n)=a_1m_1+\dots+a_nm_n m_1,\dots,m_n \phi_{m_1,\dots,m_n} \phi_{m_1,\dots,m_n} m_1,\dots,m_n M \phi_{m_1,\dots,m_n} n_1,\dots,n_k M n_1,\dots,n_k M n_1,\dots,n_k,m'_1,\dots,m'_r M m'_1,\dots,m'_r \in M","['linear-algebra', 'commutative-algebra', 'modules']"
45,Is there an injective operator with a dense nonclosed one-codimensional range?,Is there an injective operator with a dense nonclosed one-codimensional range?,,"Let $H$ be an infinite dimensional separable Hilbert Space. Is there an operator $A\in B\left( H\right) $ such that $Im\left( A\right) \neq \overline{Im\left( A\right) }=H$, $ codim\left(Im% \left( A\right)\right) =1$ and  $\ker \left( A\right) =\left\{ 0\right\} $ ?","Let $H$ be an infinite dimensional separable Hilbert Space. Is there an operator $A\in B\left( H\right) $ such that $Im\left( A\right) \neq \overline{Im\left( A\right) }=H$, $ codim\left(Im% \left( A\right)\right) =1$ and  $\ker \left( A\right) =\left\{ 0\right\} $ ?",,"['linear-algebra', 'functional-analysis', 'operator-theory', 'hilbert-spaces', 'linear-transformations']"
46,Does $\mathrm{adj}(A)=\mathrm{adj}(B)$ imply $A=B$?,Does  imply ?,\mathrm{adj}(A)=\mathrm{adj}(B) A=B,"If $A$ and $B$ are any two square matrices of same order and if $\mathrm{adj}(A)=\mathrm{adj}(B)$, does it imply $A=B$? I am pretty sure if $A$ and $B$ are invertible and if $A^{-1}=B^{-1}$, then $A=B$. So is it true for Adjoint?","If $A$ and $B$ are any two square matrices of same order and if $\mathrm{adj}(A)=\mathrm{adj}(B)$, does it imply $A=B$? I am pretty sure if $A$ and $B$ are invertible and if $A^{-1}=B^{-1}$, then $A=B$. So is it true for Adjoint?",,"['linear-algebra', 'matrices', 'determinant']"
47,Pseudo Inverse of product of Matrices,Pseudo Inverse of product of Matrices,,"Let $A$ and $B$ are two matrices where $A \in \mathbb{R}^{m\times p}$ and $B \in \mathbb{R}^{p\times n}$ and both $A$ and $B$ are full rank matrices  Now I really want to know in what cases $(AB)^+ = B^+A^+$  ,where $A^+$ is Moore-Penrose Pseudo-inverse of $A$ Here $m,p$ and $n$ are in any order like $m<p<d$ or $m>p<d$ etc.","Let $A$ and $B$ are two matrices where $A \in \mathbb{R}^{m\times p}$ and $B \in \mathbb{R}^{p\times n}$ and both $A$ and $B$ are full rank matrices  Now I really want to know in what cases $(AB)^+ = B^+A^+$  ,where $A^+$ is Moore-Penrose Pseudo-inverse of $A$ Here $m,p$ and $n$ are in any order like $m<p<d$ or $m>p<d$ etc.",,"['linear-algebra', 'pseudoinverse']"
48,Direct sum $\mathbb{R} \oplus \mathbb{R}$ - isn't intersection non-zero?,Direct sum  - isn't intersection non-zero?,\mathbb{R} \oplus \mathbb{R},"I am just starting to learn about direct sums and every definition I have read about direct sums say that the intersection of subspaces must be zero. So, how can $$\mathbb{R} \oplus \mathbb{R}$$ be a direct sum when they are identical? Thanks in advance!","I am just starting to learn about direct sums and every definition I have read about direct sums say that the intersection of subspaces must be zero. So, how can $$\mathbb{R} \oplus \mathbb{R}$$ be a direct sum when they are identical? Thanks in advance!",,"['linear-algebra', 'direct-sum']"
49,Simple definition of a positive definite matrix,Simple definition of a positive definite matrix,,"Aside from what it is written in the books about the definition of a positive definite matrix, can anyone explain it in a more earthly method?","Aside from what it is written in the books about the definition of a positive definite matrix, can anyone explain it in a more earthly method?",,"['linear-algebra', 'matrices', 'numerical-methods', 'positive-definite']"
50,Mapping sphere surface to a vector space such that distances are preserved?,Mapping sphere surface to a vector space such that distances are preserved?,,"I have a unit radius sphere (say in 3D) centered on the origin. Thus the shortest distance between two points on the sphere is the geodesic. Is there a transformation (linear or non-linear) on the points on the sphere to a higher-dimensional space (or lower if it exists), such that the distance between the points on the sphere is preserved in the transformed space as well. To make it more rigorous: let $x_1$ and $x_2$ be the two points on the sphere. Let $y_1=T(x_1)$ and $y_2=T(x_2)$ be the transformed points. Let $d_s(x_1,x_2)$ be the shortest distance between $x_1$ and $x_2$ on the sphere's surface. Is there any T(.) such that $$d_s(x_1,x_2)=||y_1-y_2||_2^2$$ (the Euclidean distance) in the transformed space holds. Any reference to relevant research or literature is welcome as well. Any engineering approaches are welcome too.","I have a unit radius sphere (say in 3D) centered on the origin. Thus the shortest distance between two points on the sphere is the geodesic. Is there a transformation (linear or non-linear) on the points on the sphere to a higher-dimensional space (or lower if it exists), such that the distance between the points on the sphere is preserved in the transformed space as well. To make it more rigorous: let $x_1$ and $x_2$ be the two points on the sphere. Let $y_1=T(x_1)$ and $y_2=T(x_2)$ be the transformed points. Let $d_s(x_1,x_2)$ be the shortest distance between $x_1$ and $x_2$ on the sphere's surface. Is there any T(.) such that $$d_s(x_1,x_2)=||y_1-y_2||_2^2$$ (the Euclidean distance) in the transformed space holds. Any reference to relevant research or literature is welcome as well. Any engineering approaches are welcome too.",,"['linear-algebra', 'matrices', 'geometry', 'functional-analysis', 'differential-geometry']"
51,Is there any inverse-commutator for matrices,Is there any inverse-commutator for matrices,,"My question is very simple. Given a symmetric real matrix $A$ , and a square real matrix $C$ , how can one solve the equation $[A,X]=C$ , where $[A,B]$ is commutator of $A$ and $B$ , i.e., $[A,B]=AB-BA$ . We are more interested in symmetric matrices for $X$ . Thanks. Update : Uranix' approach in his last edit is a great solution. Since it is closed-form enough, it can be used to prove any properties of interest for your application as it was the case for me. Tnx","My question is very simple. Given a symmetric real matrix , and a square real matrix , how can one solve the equation , where is commutator of and , i.e., . We are more interested in symmetric matrices for . Thanks. Update : Uranix' approach in his last edit is a great solution. Since it is closed-form enough, it can be used to prove any properties of interest for your application as it was the case for me. Tnx","A C [A,X]=C [A,B] A B [A,B]=AB-BA X","['linear-algebra', 'matrices']"
52,What is mathematical structure?,What is mathematical structure?,,"When we have an isomorphism, between $2$ groups or vector spaces let us say, then it is said to be structure preserving. An isomorphism exists when there is at least one mutually invertible morphism between sets (and this is arbitrary when a set is mapped on to itself). What do we mean intuitively, and perhaps somewhat analytically, when we say structure preserving? What structure do these sets, groups, vector spaces, etc . . . come with? This is a question I'm looking for more intuition on than anything else but specific examples are very helpful. Here is are related threads What does structure preserving mean? Mathematical Structures Preserving Structures Isomorphisms: preserve structure, operation, or order? I sought the answer throughout the forum but I never really found the questions asked/answer in a satisfactory way. Thanks everyone! edit: For example, if we have sets $A = \{a, b, c\}$ and $B = \{1, 2, 3\}$ and we have a bijection $f$ between them such that $f(a) = 1$ $f(b) = 2$ $f(c) = 3$ Is structure preserved or does that require an operation of some kind to also be present? If structure is preserved, what is it?","When we have an isomorphism, between groups or vector spaces let us say, then it is said to be structure preserving. An isomorphism exists when there is at least one mutually invertible morphism between sets (and this is arbitrary when a set is mapped on to itself). What do we mean intuitively, and perhaps somewhat analytically, when we say structure preserving? What structure do these sets, groups, vector spaces, etc . . . come with? This is a question I'm looking for more intuition on than anything else but specific examples are very helpful. Here is are related threads What does structure preserving mean? Mathematical Structures Preserving Structures Isomorphisms: preserve structure, operation, or order? I sought the answer throughout the forum but I never really found the questions asked/answer in a satisfactory way. Thanks everyone! edit: For example, if we have sets and and we have a bijection between them such that Is structure preserved or does that require an operation of some kind to also be present? If structure is preserved, what is it?","2 A = \{a, b, c\} B = \{1, 2, 3\} f f(a) = 1 f(b) = 2 f(c) = 3","['linear-algebra', 'abstract-algebra', 'group-theory', 'vector-spaces', 'soft-question']"
53,Calculate a determinant.,Calculate a determinant.,,"Let $a_{1},  \cdots, a_{n}$ and $b$ be real numbers. I like to know the determinant of the matrix $$\det\begin{pmatrix}   a_{1}+b & b & \cdots & b \\  b & a_{2}+b & \cdots & b \\   \vdots  & \vdots  & \ddots & \vdots  \\   b & b & \cdots & a_{n}+b  \end{pmatrix}=?$$ I guess the answer is $$a_{1} \cdots a_{n}+ \sum_{i=1}^{n} a_{1} \cdots a_{i-1} b a_{i+1}\cdots a_{n} $$ after some direct calculations  for $n=2,3$. The question is how to calculate it for general $n$. Thanks!","Let $a_{1},  \cdots, a_{n}$ and $b$ be real numbers. I like to know the determinant of the matrix $$\det\begin{pmatrix}   a_{1}+b & b & \cdots & b \\  b & a_{2}+b & \cdots & b \\   \vdots  & \vdots  & \ddots & \vdots  \\   b & b & \cdots & a_{n}+b  \end{pmatrix}=?$$ I guess the answer is $$a_{1} \cdots a_{n}+ \sum_{i=1}^{n} a_{1} \cdots a_{i-1} b a_{i+1}\cdots a_{n} $$ after some direct calculations  for $n=2,3$. The question is how to calculate it for general $n$. Thanks!",,"['linear-algebra', 'determinant']"
54,Are two matrices similar iff they have the same Jordan Canonical form?,Are two matrices similar iff they have the same Jordan Canonical form?,,"Are two matrices similar if and only if they have the same Jordan Canonical form? Does the Jordan form have to have ordered eigenvalues? For example, if $\lambda_1$ and $\lambda_2$ are eigenvalues of $A$, are $\begin{pmatrix}\lambda_1&0\\0&\lambda_2\end{pmatrix}$ and $\begin{pmatrix}\lambda_2&0\\0&\lambda_1\end{pmatrix}$ both Jordan forms of $A$?","Are two matrices similar if and only if they have the same Jordan Canonical form? Does the Jordan form have to have ordered eigenvalues? For example, if $\lambda_1$ and $\lambda_2$ are eigenvalues of $A$, are $\begin{pmatrix}\lambda_1&0\\0&\lambda_2\end{pmatrix}$ and $\begin{pmatrix}\lambda_2&0\\0&\lambda_1\end{pmatrix}$ both Jordan forms of $A$?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
55,Matrix conditioning and eigenvalue conditioning,Matrix conditioning and eigenvalue conditioning,,"I cannot understand the difference between the two kinds of conditioning in the title. During a lecture,our prof said that a well-conditioned matrix can have ill-conditioned eigenvalues and vice versa . Probably I do not understand what the expression ''conditioning of the eigenvalues'' means, since I use to define the conditioning number as $cond (A) = \mid\mid A \mid\mid \; \mid\mid A^{-1}\mid\mid$ and I do not have a matrix when dealing with eigenvalues. Can someone please suggest a reference, possibly with examples of matrices with well and ill conditioned eigenvalues? Thanks!","I cannot understand the difference between the two kinds of conditioning in the title. During a lecture,our prof said that a well-conditioned matrix can have ill-conditioned eigenvalues and vice versa . Probably I do not understand what the expression ''conditioning of the eigenvalues'' means, since I use to define the conditioning number as $cond (A) = \mid\mid A \mid\mid \; \mid\mid A^{-1}\mid\mid$ and I do not have a matrix when dealing with eigenvalues. Can someone please suggest a reference, possibly with examples of matrices with well and ill conditioned eigenvalues? Thanks!",,"['linear-algebra', 'matrices']"
56,Proof of Linear independence of generalized eigenvectors without applying generalized eigenspace decomposition.,Proof of Linear independence of generalized eigenvectors without applying generalized eigenspace decomposition.,,"Let $V$ be a finite-dimensional complex vector space. Let $T\in \mathcal L(V)$ be an endomorphism. A vector $v\in V \setminus\{0\}$ is called a generalized eigenvector to an eigenvalue $\lambda\in\mathbb C$ of $T$ iff there exists $k> 0$ such that $$ (\lambda I - T)^kv=0. $$ From the generalized eigenspace decomposition it follows that generalized eigenvectors to different eigenvalues are linearly independent. My Question is: Is there an elementary proof for this result? Maybe along the lines of the proof for linear independence of (ordinary) eigenvectors: Let $v_1,v_2$ be eigenvectors to eigenvalues $\lambda_1\ne \lambda_2$ . Then $$ a_1 v_1 + a_2v_2=0 $$ implies (1: apply $T$ , 2: multiply equation by $\lambda_2$ , subtract) $$ a_1 (\lambda_1-\lambda_2)v_1=0 $$ hence $a_1=0$ , and $a_2=0$ .","Let be a finite-dimensional complex vector space. Let be an endomorphism. A vector is called a generalized eigenvector to an eigenvalue of iff there exists such that From the generalized eigenspace decomposition it follows that generalized eigenvectors to different eigenvalues are linearly independent. My Question is: Is there an elementary proof for this result? Maybe along the lines of the proof for linear independence of (ordinary) eigenvectors: Let be eigenvectors to eigenvalues . Then implies (1: apply , 2: multiply equation by , subtract) hence , and .","V T\in \mathcal L(V) v\in V \setminus\{0\} \lambda\in\mathbb C T k> 0 
(\lambda I - T)^kv=0.
 v_1,v_2 \lambda_1\ne \lambda_2 
a_1 v_1 + a_2v_2=0
 T \lambda_2 
a_1 (\lambda_1-\lambda_2)v_1=0
 a_1=0 a_2=0","['linear-algebra', 'eigenvalues-eigenvectors']"
57,A rigorous book on a First Course in linear algebra,A rigorous book on a First Course in linear algebra,,"I am just going to start linear algebra for my undergraduate course. And which is the best book available for linear algebra in terms of rigor and a book similar to calculus by Tom M Apostol (I like this book because of it's rigorous and clear ideas, the qualities which I want). Note: I have browsed through other similar topics here and didn't found anything helpful.","I am just going to start linear algebra for my undergraduate course. And which is the best book available for linear algebra in terms of rigor and a book similar to calculus by Tom M Apostol (I like this book because of it's rigorous and clear ideas, the qualities which I want). Note: I have browsed through other similar topics here and didn't found anything helpful.",,"['linear-algebra', 'reference-request']"
58,Vector 2 norm and infinity norm proof,Vector 2 norm and infinity norm proof,,"So I've already proven why $\left\lVert x\right\rVert_2\geq \left\lVert x\right\rVert_\infty$. I'm having trouble proving that $\sqrt{m}{\left\lVert x\right\rVert_\infty}\geq \left\lVert x\right\rVert_2$. I've tried looking at the individual elements, but I'm not getting anywhere. I think I may have to use Holder's inequality, but I'm not sure if that's applicable, or how I would use it. How should I do this?","So I've already proven why $\left\lVert x\right\rVert_2\geq \left\lVert x\right\rVert_\infty$. I'm having trouble proving that $\sqrt{m}{\left\lVert x\right\rVert_\infty}\geq \left\lVert x\right\rVert_2$. I've tried looking at the individual elements, but I'm not getting anywhere. I think I may have to use Holder's inequality, but I'm not sure if that's applicable, or how I would use it. How should I do this?",,"['linear-algebra', 'vector-spaces']"
59,Why are hyperbolic toral automorphisms (e.g. Arnold's cat map) ergodic?,Why are hyperbolic toral automorphisms (e.g. Arnold's cat map) ergodic?,,"Let $\varphi : \mathbb{T}^2 \to \mathbb{T}^2$ be a hyperbolic automorphism of the torus, induced by a linear map $A : \mathbb{R}^2 \to \mathbb{R}^2$ of determinant $\pm 1$ with no eigenvalues of modulus 1. What is an easy way to prove that $\varphi$ is ergodic? It's true that the stable and unstable manifolds at $(0,0) \in \mathbb{T}^2$ (projections of the eigenspaces of $A$ to the torus) are dense in the torus, which can be used to prove that such maps are topologically mixing . An example is Arnold's cat map . Arnold and Avez show in Ergodic Problems in Classical Mechanics that Arnold's cat map is ergodic by proving that it has ""Lebesgue spectrum"", which implies that it is strong mixing , which implies that it is ergodic. Is there a more direct way to prove this?","Let $\varphi : \mathbb{T}^2 \to \mathbb{T}^2$ be a hyperbolic automorphism of the torus, induced by a linear map $A : \mathbb{R}^2 \to \mathbb{R}^2$ of determinant $\pm 1$ with no eigenvalues of modulus 1. What is an easy way to prove that $\varphi$ is ergodic? It's true that the stable and unstable manifolds at $(0,0) \in \mathbb{T}^2$ (projections of the eigenspaces of $A$ to the torus) are dense in the torus, which can be used to prove that such maps are topologically mixing . An example is Arnold's cat map . Arnold and Avez show in Ergodic Problems in Classical Mechanics that Arnold's cat map is ergodic by proving that it has ""Lebesgue spectrum"", which implies that it is strong mixing , which implies that it is ergodic. Is there a more direct way to prove this?",,"['linear-algebra', 'general-topology', 'dynamical-systems', 'ergodic-theory']"
60,Intuition behind Jacobian of the SVD,Intuition behind Jacobian of the SVD,,"I'm having a little trouble understanding the meaning behind the Jacobian of an SVD. I understand what the Jacobian is, but I don't see how you can derive a Jacobian from the SVD. To me, the SVD is just USV_transpose - I don't see how a matrix can be differentiated, since they don't really seem to be functions of anything. http://www.ics.forth.gr/_publications/2000_eccv_SVD_jacobian.pdf I've been looking at the above pdf just to get a better understanding, but equation (7) (the differentiation of the singular values) is really where I get lost.","I'm having a little trouble understanding the meaning behind the Jacobian of an SVD. I understand what the Jacobian is, but I don't see how you can derive a Jacobian from the SVD. To me, the SVD is just USV_transpose - I don't see how a matrix can be differentiated, since they don't really seem to be functions of anything. http://www.ics.forth.gr/_publications/2000_eccv_SVD_jacobian.pdf I've been looking at the above pdf just to get a better understanding, but equation (7) (the differentiation of the singular values) is really where I get lost.",,['linear-algebra']
61,"How to show that $\mathrm{SL}(2,\mathbb Z) = \langle A, B\rangle$?",How to show that ?,"\mathrm{SL}(2,\mathbb Z) = \langle A, B\rangle","Show, that if $\mathbf{A}= \left( \begin{array}{cc} 1&1\\ 0&1 \end{array} \right)$, $\mathbf{B}= \left( \begin{array}{cc} 0&1\\ -1&0 \end{array} \right)$ and $\mathrm{SL}(2, \mathbb{Z}) := \{ \mathbf{C}\in\mathrm{M}(2\times 2;\mathbb{Z})\, |\,  \det(\mathbf{C}) = 1\}$ then $\langle\mathbf{A}, \mathbf{B}\rangle = \mathrm{SL}(2, \mathbb{Z})$. I found this exercise in a textbook for linear algebra in a chapter about the determinant, so it should be solved rather elementarily and without any deeper understanding of group theory ($\mathrm{SL}(2, \mathbb{Z})$ is defined only for the exercise). Showing $\langle\mathbf{A}, \mathbf{B}\rangle \subseteq \mathrm{SL}(2, \mathbb{Z})$ was easy but I got stuck with the opposite direction. Any help would be appreciated.","Show, that if $\mathbf{A}= \left( \begin{array}{cc} 1&1\\ 0&1 \end{array} \right)$, $\mathbf{B}= \left( \begin{array}{cc} 0&1\\ -1&0 \end{array} \right)$ and $\mathrm{SL}(2, \mathbb{Z}) := \{ \mathbf{C}\in\mathrm{M}(2\times 2;\mathbb{Z})\, |\,  \det(\mathbf{C}) = 1\}$ then $\langle\mathbf{A}, \mathbf{B}\rangle = \mathrm{SL}(2, \mathbb{Z})$. I found this exercise in a textbook for linear algebra in a chapter about the determinant, so it should be solved rather elementarily and without any deeper understanding of group theory ($\mathrm{SL}(2, \mathbb{Z})$ is defined only for the exercise). Showing $\langle\mathbf{A}, \mathbf{B}\rangle \subseteq \mathrm{SL}(2, \mathbb{Z})$ was easy but I got stuck with the opposite direction. Any help would be appreciated.",,"['linear-algebra', 'determinant']"
62,What is $\frac{\det(A+tI)}{\det(B+tI)}$ as $t\to0$?,What is  as ?,\frac{\det(A+tI)}{\det(B+tI)} t\to0,If $A$ and $B$ are two real $2\times 2$ matrices with $\det A = 0 $ and $\det B = 0 $ and $\mathrm{tr}(B)$ is non zero. then what will be limit of $$\lim_{t\to0}\frac{\det(A+tI)}{\det(B+tI)}$$  I used the formula $\lambda^2-\mathrm{tr} A+\det A = 0$. then i think answer is $\dfrac{\mathrm{tr}(A)}{\mathrm{tr}(B)}$. Am I correct? What would be expansion of $\det(A+tI)$ for a $2\times 2$ matrices?,If $A$ and $B$ are two real $2\times 2$ matrices with $\det A = 0 $ and $\det B = 0 $ and $\mathrm{tr}(B)$ is non zero. then what will be limit of $$\lim_{t\to0}\frac{\det(A+tI)}{\det(B+tI)}$$  I used the formula $\lambda^2-\mathrm{tr} A+\det A = 0$. then i think answer is $\dfrac{\mathrm{tr}(A)}{\mathrm{tr}(B)}$. Am I correct? What would be expansion of $\det(A+tI)$ for a $2\times 2$ matrices?,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
63,generalizations of determinant and trace,generalizations of determinant and trace,,"There are $n$ symmetric polynomials in the eigenvalues of a square matrix. Two of these are the determinant and the trace, each of which have countless applications and interpretations in algebra and geometry. What about the other symmetric polynomials? They are also similarity invariants, yet I've never seen them used or referenced. Are there any geometric interpretations, or applications, for these other invariants?","There are $n$ symmetric polynomials in the eigenvalues of a square matrix. Two of these are the determinant and the trace, each of which have countless applications and interpretations in algebra and geometry. What about the other symmetric polynomials? They are also similarity invariants, yet I've never seen them used or referenced. Are there any geometric interpretations, or applications, for these other invariants?",,['linear-algebra']
64,Domain and Co-domain of a linear transformation not defined over the same Field,Domain and Co-domain of a linear transformation not defined over the same Field,,"I was thinking about linear tramsformations and i came up with this example: $$f:\mathbb{R}^n \to \mathbb{C}^n\\  f(x)=ix$$ for this example, domain and co-domain are not defined over the same field and all linear transformations that i encountered by now had domain and co-domain defined over the same field. I was wondering that if this is a valid linear transformation or not? and if not, why did we put such a constraint? also, if it is possible, keep the explanation simple because i'm pretty new in pure math. thank you in advance.","I was thinking about linear tramsformations and i came up with this example: for this example, domain and co-domain are not defined over the same field and all linear transformations that i encountered by now had domain and co-domain defined over the same field. I was wondering that if this is a valid linear transformation or not? and if not, why did we put such a constraint? also, if it is possible, keep the explanation simple because i'm pretty new in pure math. thank you in advance.","f:\mathbb{R}^n \to \mathbb{C}^n\\ 
f(x)=ix","['linear-algebra', 'linear-transformations']"
65,Why did mathematicians choose the inner product to be linear in the first argument instead of the second?,Why did mathematicians choose the inner product to be linear in the first argument instead of the second?,,"From my limited experience with inner product spaces, it seems like the inner product being linear in the second argument would facilitate smoother notation. For instance, for $x \in H$ , we could define $x^* \in H^*$ by $$x^*y = \langle x, y\rangle $$ Then this would generalize the fact that $x^T y = \langle x, y\rangle$ on $\mathbb{R}^n $ . Does linearity in the first argument make for smoother notation in some other aspect of Hilbert space theory?","From my limited experience with inner product spaces, it seems like the inner product being linear in the second argument would facilitate smoother notation. For instance, for , we could define by Then this would generalize the fact that on . Does linearity in the first argument make for smoother notation in some other aspect of Hilbert space theory?","x \in H x^* \in H^* x^*y = \langle x, y\rangle  x^T y = \langle x, y\rangle \mathbb{R}^n ","['linear-algebra', 'hilbert-spaces', 'inner-products']"
66,Find $B=A^2+A$ knowing that $A^3=\begin{bmatrix}4&3\\-3&-2\end{bmatrix}$,Find  knowing that,B=A^2+A A^3=\begin{bmatrix}4&3\\-3&-2\end{bmatrix},Find $B=A^2+A$ knowing that $A^3=\begin{bmatrix}4&3\\-3&-2\end{bmatrix}$ Is there a way to solve this rather than just declaring a matrix $$A=\begin{bmatrix}a&b\\c&d\end{bmatrix}$$ and then trying to solve a system of cubic equations? My attempt: $$A^3 -I_2 = \begin{bmatrix} 3 & 3\\ -3 & -3\end{bmatrix} = 9 \begin{bmatrix} 1 & 1\\ -1 & -1\end{bmatrix}$$ and $$B=\frac {A^3-I_2}{A-I_2}-I_2$$ $$(A-I_2)(A^2+A+I_2)=9\begin{bmatrix}1&1\\-1&-1\end{bmatrix}$$ But I get stuck here.,Find $B=A^2+A$ knowing that $A^3=\begin{bmatrix}4&3\\-3&-2\end{bmatrix}$ Is there a way to solve this rather than just declaring a matrix $$A=\begin{bmatrix}a&b\\c&d\end{bmatrix}$$ and then trying to solve a system of cubic equations? My attempt: $$A^3 -I_2 = \begin{bmatrix} 3 & 3\\ -3 & -3\end{bmatrix} = 9 \begin{bmatrix} 1 & 1\\ -1 & -1\end{bmatrix}$$ and $$B=\frac {A^3-I_2}{A-I_2}-I_2$$ $$(A-I_2)(A^2+A+I_2)=9\begin{bmatrix}1&1\\-1&-1\end{bmatrix}$$ But I get stuck here.,,"['linear-algebra', 'matrices', 'matrix-equations']"
67,"Prove $X^tX$, where $X$ is a matrix of full column rank, is positive definite?","Prove , where  is a matrix of full column rank, is positive definite?",X^tX X,"Let $X$ be a matrix of dimension $n\times k$ where $n>k$,  $\text{rk}(X)=k$ so $X$ is of full column rank. Then how do I prove $X^tX$ is always positive definite, where $X^t$ is transpose of $X$? This is given sortta like a lemma in our lecture slides without proof but would like to have some reasoning behind this. Thank you  for your help!","Let $X$ be a matrix of dimension $n\times k$ where $n>k$,  $\text{rk}(X)=k$ so $X$ is of full column rank. Then how do I prove $X^tX$ is always positive definite, where $X^t$ is transpose of $X$? This is given sortta like a lemma in our lecture slides without proof but would like to have some reasoning behind this. Thank you  for your help!",,"['linear-algebra', 'matrices', 'positive-definite']"
68,Standard inner product of matrices [closed],Standard inner product of matrices [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question What is the correct definition of the standard inner product of two given matrices?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question What is the correct definition of the standard inner product of two given matrices?",,"['linear-algebra', 'matrices', 'inner-products']"
69,Comparing LU or QR decompositions for solving least squares,Comparing LU or QR decompositions for solving least squares,,"Let $X \in R^{m\times n}$ with $m>n$. We aim to solve $y=X\beta$ where $\hat\beta$ is the least square estimator. The least squares solution for   $\hat\beta = (X^TX)^{-1}X^Ty$ can be obtained using QR decomposition   on $X$ and $LU$ decomposition on $X^TX$. The aim to compare these. I noticed that we can use Cholesky decomposition instead of $LU$, since $X^TX$ is symmetric and positive definite. Using $LU$ we have: $\hat\beta = (X^TX)^{-1}X^Ty=(LU)^{-1}X^Ty$, solve $a=X^Ty$ which is order $O(2nm)$, then $L^{-1}b=a$ at cost $\sum_1^{k=n} (2k-1)$ and finally $U^{-1}a$ at the same cost of $\sum_1^{k=n} (2k-1)$. I didn't count the cost of computing $L^{-1}$ and $U^{-1}$. Using $QR$ we have: $\hat\beta = (X^TX)^{-1}X^Ty=((QR)^TQR)^{-1}R^TQ^Ty=R^{-1}Q^Ty$, where we solve $Q^Ty=a$ at cost $O(n^2)$ and $R^{-1}a$ with cost $\sum_1^{k=n} (2k-1)$. Comparing the decompositions: It seems that QR decomposition is much better than LU. I think the cost of computing QR is higher than LU, which is why we could prefer to use LU. On the other hand if we are given the decompositions, we should use QR. $SVD$ decomposition: Is there any advantage to use SVD decomposition?","Let $X \in R^{m\times n}$ with $m>n$. We aim to solve $y=X\beta$ where $\hat\beta$ is the least square estimator. The least squares solution for   $\hat\beta = (X^TX)^{-1}X^Ty$ can be obtained using QR decomposition   on $X$ and $LU$ decomposition on $X^TX$. The aim to compare these. I noticed that we can use Cholesky decomposition instead of $LU$, since $X^TX$ is symmetric and positive definite. Using $LU$ we have: $\hat\beta = (X^TX)^{-1}X^Ty=(LU)^{-1}X^Ty$, solve $a=X^Ty$ which is order $O(2nm)$, then $L^{-1}b=a$ at cost $\sum_1^{k=n} (2k-1)$ and finally $U^{-1}a$ at the same cost of $\sum_1^{k=n} (2k-1)$. I didn't count the cost of computing $L^{-1}$ and $U^{-1}$. Using $QR$ we have: $\hat\beta = (X^TX)^{-1}X^Ty=((QR)^TQR)^{-1}R^TQ^Ty=R^{-1}Q^Ty$, where we solve $Q^Ty=a$ at cost $O(n^2)$ and $R^{-1}a$ with cost $\sum_1^{k=n} (2k-1)$. Comparing the decompositions: It seems that QR decomposition is much better than LU. I think the cost of computing QR is higher than LU, which is why we could prefer to use LU. On the other hand if we are given the decompositions, we should use QR. $SVD$ decomposition: Is there any advantage to use SVD decomposition?",,"['linear-algebra', 'matrices', 'statistics', 'matrix-decomposition', 'least-squares']"
70,How many sheets can a hyperboloid have in $n$-dimensions?,How many sheets can a hyperboloid have in -dimensions?,n,"For the equation describing a hyperboloid in an indefinite number of dimensions centered at $v$: $$ (x-v)^TA(x-v)=1$$ I read that the two sheeted hyperboloid $A$ has one positive eigenvalue and two negative, vice versa for the one sheeted. Since an $n\times n$ matrix can have up to $n$ eigenvalues, how do you determine how many sheets a hyperboloid has in $n$ dimensions?","For the equation describing a hyperboloid in an indefinite number of dimensions centered at $v$: $$ (x-v)^TA(x-v)=1$$ I read that the two sheeted hyperboloid $A$ has one positive eigenvalue and two negative, vice versa for the one sheeted. Since an $n\times n$ matrix can have up to $n$ eigenvalues, how do you determine how many sheets a hyperboloid has in $n$ dimensions?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
71,Why does this trick gives precisely the formulas for the sum of the $n$ first naturals and the $n$ first squares?,Why does this trick gives precisely the formulas for the sum of the  first naturals and the  first squares?,n n,"I've learned a cool trick several days ago. Suppose I want to find a polynomial that gives me: $$f(1)=1, \quad f(2)=5,\quad  f(3)=14,\quad f(4)=30\tag{1}$$ I could do the following: Take a polynomial of degree $3$ and make the following system with it: \begin{eqnarray*}   {ax^3+bx^2+cx+d}&=&{1} \\    {ax^3+bx^2+cx+d}&=&{5} \\    {ax^3+bx^2+cx+d}&=&{14} \\    {ax^3+bx^2+cx+d}&=&{30}  \end{eqnarray*} I just need to substitute the $x^n$'s according to $(1)$. This gives me: \begin{eqnarray*}   {a1^3+b1^2+c1+d}&=&{1} \\    {a2^3+b2^2+c2+d}&=&{5} \\    {a3^3+b3^2+c3+d}&=&{14} \\    {a4^3+b4^2+c4+d}&=&{30}  \end{eqnarray*} Now this is easy, we need only to find the coefficients $a,b,c,d$, that is, find the inverse of a matrix $A$ in the matricial equations $Ax=b$: $$\left( \begin{array}{cccc}  1 & 1 & 1 & 1 \\  2 & 4 & 8 & 16 \\  3 & 9 & 27 & 81 \\  4 & 16 & 64 & 256 \\ \end{array} \right)$$ And then: $x=A^{-1}b$. This gives me: $$a=\frac{1}{3}\quad\quad  b= \frac{1}{2}\quad\quad  c= \frac{1}{6}\quad\quad   d= 0$$ That is, our polynomial is: $\cfrac{x^3}{3}+\cfrac{x^2}{2}+\cfrac{x}{6}$ and this is - at least to me - surprising because it is the formula for the sum of the first $n$ square numbers. What baffles me more is that if I do the same with a polynomial of degree $4$ and a $5\times 5$ matrix and $f(5)=55$ it will give me the same formula. So I have three questions: When I did this trick, I used only $4$ and $5$ values of the sum of the first $n$ squares. So why does it give me exactly the polynomial for the sum of the first $n$ squares instead of any other polynomial? There is an infinite abyss of possible polynomials, why exactly this one? For example: Why didn't it give a polynomial that gives me: $$f(1)=1, \quad f(2)=5,\quad  f(3)=14,\quad f(4)=30 \quad f(5)=55\quad  f(6)=2\quad ?$$ Why does the result holds for a higher $n$, that is: Why doing the same thing with $n+1$ gives me the same polynomial it gave me for $n$ instead of any other polynomial in the behemothic chasm of possible polynomials? I've tried for other formulas in the past, just like the sum of the first $n$ positive integers. Why does it hit exactly the polynomial for the sum of the first $n$ positive integers instead of any other polynomial in the humongous crevasse of possible polynomials? Sorry if the question is too stupid, but I've figured this trick some time ago and couldn't find a clue to why this is hapenning. The main drama of the thing for me is that it seems as if I were doing incomplete induction and then BAM! it just hits exactly where I wanted it to hit.","I've learned a cool trick several days ago. Suppose I want to find a polynomial that gives me: $$f(1)=1, \quad f(2)=5,\quad  f(3)=14,\quad f(4)=30\tag{1}$$ I could do the following: Take a polynomial of degree $3$ and make the following system with it: \begin{eqnarray*}   {ax^3+bx^2+cx+d}&=&{1} \\    {ax^3+bx^2+cx+d}&=&{5} \\    {ax^3+bx^2+cx+d}&=&{14} \\    {ax^3+bx^2+cx+d}&=&{30}  \end{eqnarray*} I just need to substitute the $x^n$'s according to $(1)$. This gives me: \begin{eqnarray*}   {a1^3+b1^2+c1+d}&=&{1} \\    {a2^3+b2^2+c2+d}&=&{5} \\    {a3^3+b3^2+c3+d}&=&{14} \\    {a4^3+b4^2+c4+d}&=&{30}  \end{eqnarray*} Now this is easy, we need only to find the coefficients $a,b,c,d$, that is, find the inverse of a matrix $A$ in the matricial equations $Ax=b$: $$\left( \begin{array}{cccc}  1 & 1 & 1 & 1 \\  2 & 4 & 8 & 16 \\  3 & 9 & 27 & 81 \\  4 & 16 & 64 & 256 \\ \end{array} \right)$$ And then: $x=A^{-1}b$. This gives me: $$a=\frac{1}{3}\quad\quad  b= \frac{1}{2}\quad\quad  c= \frac{1}{6}\quad\quad   d= 0$$ That is, our polynomial is: $\cfrac{x^3}{3}+\cfrac{x^2}{2}+\cfrac{x}{6}$ and this is - at least to me - surprising because it is the formula for the sum of the first $n$ square numbers. What baffles me more is that if I do the same with a polynomial of degree $4$ and a $5\times 5$ matrix and $f(5)=55$ it will give me the same formula. So I have three questions: When I did this trick, I used only $4$ and $5$ values of the sum of the first $n$ squares. So why does it give me exactly the polynomial for the sum of the first $n$ squares instead of any other polynomial? There is an infinite abyss of possible polynomials, why exactly this one? For example: Why didn't it give a polynomial that gives me: $$f(1)=1, \quad f(2)=5,\quad  f(3)=14,\quad f(4)=30 \quad f(5)=55\quad  f(6)=2\quad ?$$ Why does the result holds for a higher $n$, that is: Why doing the same thing with $n+1$ gives me the same polynomial it gave me for $n$ instead of any other polynomial in the behemothic chasm of possible polynomials? I've tried for other formulas in the past, just like the sum of the first $n$ positive integers. Why does it hit exactly the polynomial for the sum of the first $n$ positive integers instead of any other polynomial in the humongous crevasse of possible polynomials? Sorry if the question is too stupid, but I've figured this trick some time ago and couldn't find a clue to why this is hapenning. The main drama of the thing for me is that it seems as if I were doing incomplete induction and then BAM! it just hits exactly where I wanted it to hit.",,"['linear-algebra', 'polynomials']"
72,Commutativity of matrix and its transpose,Commutativity of matrix and its transpose,,If a matrix is symmetric or skew-symmetric it commutes in the obvious way with its transpose. (For symmetric: $SS^T=S^2$ and $S^TS =S^2$) The less obvious is the case of commutativity  for orthogonal matrix but such matrix also commutes with its transpose because $RR^T=RR^{-1}= I= R^{-1}R=R^TR$. Questions: Are there  other cases when a matrix commutes with its transpose ? What is the general property of such matrix which allows it to commute with its transpose ?,If a matrix is symmetric or skew-symmetric it commutes in the obvious way with its transpose. (For symmetric: $SS^T=S^2$ and $S^TS =S^2$) The less obvious is the case of commutativity  for orthogonal matrix but such matrix also commutes with its transpose because $RR^T=RR^{-1}= I= R^{-1}R=R^TR$. Questions: Are there  other cases when a matrix commutes with its transpose ? What is the general property of such matrix which allows it to commute with its transpose ?,,"['linear-algebra', 'matrices']"
73,Convergence of the powers of a Markov transition matrix,Convergence of the powers of a Markov transition matrix,,I have a Markov matrix $$P=\begin{bmatrix}1&0&0&0&0&0\\\frac{1}{2}&0&\frac{1}{2}&0&0&0\\\frac{1}{4}&0&\frac{1}{4}&\frac{1}{2}&0&0\\\frac{1}{8}&0&\frac{1}{8}&\frac{1}{4}&\frac{1}{2}&0\\\frac{1}{16}&0&\frac{1}{16}&\frac{1}{8}&\frac{1}{4}&\frac{1}{2}\\0&0&0&0&0&1\end{bmatrix}$$ and I want to show that the powers of $P$ converge. I can see numerically that $$\displaystyle\lim_{n\to\infty}P^n=\begin{bmatrix}1&0&0&0&0&0\\.8&0&0&0&0&.2\\.6&0&0&0&0&.4\\.4&0&0&0&0&.6\\.2&0&0&0&0&.8\\0&0&0&0&0&1\end{bmatrix}$$ but are there any general results I can use to actually prove that $\displaystyle\lim_{n\to\infty}P^n$ exists?,I have a Markov matrix $$P=\begin{bmatrix}1&0&0&0&0&0\\\frac{1}{2}&0&\frac{1}{2}&0&0&0\\\frac{1}{4}&0&\frac{1}{4}&\frac{1}{2}&0&0\\\frac{1}{8}&0&\frac{1}{8}&\frac{1}{4}&\frac{1}{2}&0\\\frac{1}{16}&0&\frac{1}{16}&\frac{1}{8}&\frac{1}{4}&\frac{1}{2}\\0&0&0&0&0&1\end{bmatrix}$$ and I want to show that the powers of $P$ converge. I can see numerically that $$\displaystyle\lim_{n\to\infty}P^n=\begin{bmatrix}1&0&0&0&0&0\\.8&0&0&0&0&.2\\.6&0&0&0&0&.4\\.4&0&0&0&0&.6\\.2&0&0&0&0&.8\\0&0&0&0&0&1\end{bmatrix}$$ but are there any general results I can use to actually prove that $\displaystyle\lim_{n\to\infty}P^n$ exists?,,"['linear-algebra', 'probability', 'matrices', 'markov-process']"
74,Matrices with rank exactly r as variety,Matrices with rank exactly r as variety,,"Assume $M(n,m)$ denote the vector space of $n\times m$ matrices. Consider this as an affine space. Now consider the subset of matrices with rank exactly $r$. Is this subset an irreducible subvariety? If it is, can we find its dimension? The only thing I can recall about rank is that we can use minors of a matrix to determine the rank. In this way I guess I can prove the set is locally closed (in the Zarisky topology). So this should be a subvariety. About irreducibility, I guess I should express this subset as the image of some map, but I am not sure. Can I decompose a matrix with rank r by two other matrices? Of course, the hardest part is the dimension. Totally no idea.","Assume $M(n,m)$ denote the vector space of $n\times m$ matrices. Consider this as an affine space. Now consider the subset of matrices with rank exactly $r$. Is this subset an irreducible subvariety? If it is, can we find its dimension? The only thing I can recall about rank is that we can use minors of a matrix to determine the rank. In this way I guess I can prove the set is locally closed (in the Zarisky topology). So this should be a subvariety. About irreducibility, I guess I should express this subset as the image of some map, but I am not sure. Can I decompose a matrix with rank r by two other matrices? Of course, the hardest part is the dimension. Totally no idea.",,"['linear-algebra', 'matrices']"
75,Under what conditions is a linear automorphism an isometry of some inner product?,Under what conditions is a linear automorphism an isometry of some inner product?,,"Assume $V$ is a finite-dimensional vector space over $\mathbb{R}$, and that $T: V \to V$ is a (linear) isomorphism. When is it possible to construct an inner product on $V$    making $T$ an isometry? (Hopefully, I am looking for necessary & sufficient conditions $T$ should satisfy, i.e. a full characterization of the situation). What I have so far: A necessary condition: all the real eigenvalues of $T$ are of absolute value $1$. (Since $ T(v)=\lambda v \Rightarrow  \langle v,v\rangle=\langle Tv,Tv\rangle = \langle \lambda v, \lambda v\rangle = \lambda^2\langle v, v\rangle$ and an eigenvector $v$ must be nonzero.) This condition is certainly not sufficient: For example look at $A$ = $\begin{pmatrix} 1 & 1 \\\ 0 & 1 \end{pmatrix}: \mathbb{R}^2 \to \mathbb{R}^2$. It is an automorphism which has only one eigenvalue ($\lambda = 1$). However, $A\begin{pmatrix} x \\ y \end{pmatrix}= \begin{pmatrix} x+y \\ y \end{pmatrix}$, hence $A^n\begin{pmatrix} x \\ y \end{pmatrix}= \begin{pmatrix} x+ny \\ y \end{pmatrix}$ and the requirement $A:(\mathbb{R}^2,\langle \rangle) \to (\mathbb{R}^2,\langle \rangle) $ to be an isometry for some inner product $\langle \rangle$ implies: $\lVert \begin{pmatrix} x \\ y \end{pmatrix}\rVert^2=\lVert A^n\begin{pmatrix} x \\ y \end{pmatrix}\rVert^2\Rightarrow x^2 \lVert e_1\rVert^2+y^2 \lVert e_2\rVert^2+2xy\langle e_1,e_2\rangle = (x+ny)^2 \lVert e_1\rVert^2+y^2 \lVert e_2\rVert^2+2y(x+ny) \langle e_1,e_2\rangle \Rightarrow 0=(2nxy+n^2y^2)\lVert e_1\rVert^2+2ny^2 \langle e_1,e_2\rangle$. So we get that $0=(2xy+ny^2)\lVert e_1\rVert^2+2y^2 \langle e_1,e_2\rangle$ for any $x,y\in \mathbb{R}, n\in \mathbb{N}$ which is a contradiction since $\lVert e_1 \rVert > 0$. Some sufficient conditions: 1)  If $T$ is diagonalizable over $\mathbb {R}$ (with all eigenvalues $1$ or $-1$, by our necessary condition), then let ${V_1,...,V_n}$ be a basis of eigenvectors of $T$ , and define $\langle v_i,v_j\rangle = \delta_{ij}$. $T$ will be an isometry. This condition is certainly not necessary: just take a rotation (say by $90^{\circ}$) in the plane. note that it is diagonalizable over $\mathbb{C}$. My guess is that if our transformation is diagonalizable over $\mathbb{C}$ (with all eigenvalues with absolute value 1) a similar construction like the above will work. One problem I see with this approach is that an odd-dimensional $\mathbb{R}$-vector space cannot even be considered as a $\mathbb{C}$-vector space. (Though we can always complexify...). 2) $T$ is of finite order. (Then we just start with any inner product on $V$ and construct a new one via summing over iterates of $T$, i.e: $\langle v,w \rangle ' = \sum_{i=0}^{n-1} \langle T^iv,T^iw \rangle $). Note that (as explained for instance here ) this implies $T$ is diagonalizable over $\mathbb{C}$, but of course not necessarily over $\mathbb{R}$. (Think about our rotation again.) Actually, I have now understood that condition (1) implies $T$ is of order 2, (I think the reverse implication also holds, i.e $T^2=Id\Rightarrow T$ is diagonalizable). So condition (1) is a particular case of (2). However, (2) is  not necessary, since any rotation of irrational multiple of 2$\pi$ is an isometry w.r.t the standard product, but of infinite order. I somehow think the right way to handle this question is to think over $\mathbb{C}$, but I am not sure how to do this.","Assume $V$ is a finite-dimensional vector space over $\mathbb{R}$, and that $T: V \to V$ is a (linear) isomorphism. When is it possible to construct an inner product on $V$    making $T$ an isometry? (Hopefully, I am looking for necessary & sufficient conditions $T$ should satisfy, i.e. a full characterization of the situation). What I have so far: A necessary condition: all the real eigenvalues of $T$ are of absolute value $1$. (Since $ T(v)=\lambda v \Rightarrow  \langle v,v\rangle=\langle Tv,Tv\rangle = \langle \lambda v, \lambda v\rangle = \lambda^2\langle v, v\rangle$ and an eigenvector $v$ must be nonzero.) This condition is certainly not sufficient: For example look at $A$ = $\begin{pmatrix} 1 & 1 \\\ 0 & 1 \end{pmatrix}: \mathbb{R}^2 \to \mathbb{R}^2$. It is an automorphism which has only one eigenvalue ($\lambda = 1$). However, $A\begin{pmatrix} x \\ y \end{pmatrix}= \begin{pmatrix} x+y \\ y \end{pmatrix}$, hence $A^n\begin{pmatrix} x \\ y \end{pmatrix}= \begin{pmatrix} x+ny \\ y \end{pmatrix}$ and the requirement $A:(\mathbb{R}^2,\langle \rangle) \to (\mathbb{R}^2,\langle \rangle) $ to be an isometry for some inner product $\langle \rangle$ implies: $\lVert \begin{pmatrix} x \\ y \end{pmatrix}\rVert^2=\lVert A^n\begin{pmatrix} x \\ y \end{pmatrix}\rVert^2\Rightarrow x^2 \lVert e_1\rVert^2+y^2 \lVert e_2\rVert^2+2xy\langle e_1,e_2\rangle = (x+ny)^2 \lVert e_1\rVert^2+y^2 \lVert e_2\rVert^2+2y(x+ny) \langle e_1,e_2\rangle \Rightarrow 0=(2nxy+n^2y^2)\lVert e_1\rVert^2+2ny^2 \langle e_1,e_2\rangle$. So we get that $0=(2xy+ny^2)\lVert e_1\rVert^2+2y^2 \langle e_1,e_2\rangle$ for any $x,y\in \mathbb{R}, n\in \mathbb{N}$ which is a contradiction since $\lVert e_1 \rVert > 0$. Some sufficient conditions: 1)  If $T$ is diagonalizable over $\mathbb {R}$ (with all eigenvalues $1$ or $-1$, by our necessary condition), then let ${V_1,...,V_n}$ be a basis of eigenvectors of $T$ , and define $\langle v_i,v_j\rangle = \delta_{ij}$. $T$ will be an isometry. This condition is certainly not necessary: just take a rotation (say by $90^{\circ}$) in the plane. note that it is diagonalizable over $\mathbb{C}$. My guess is that if our transformation is diagonalizable over $\mathbb{C}$ (with all eigenvalues with absolute value 1) a similar construction like the above will work. One problem I see with this approach is that an odd-dimensional $\mathbb{R}$-vector space cannot even be considered as a $\mathbb{C}$-vector space. (Though we can always complexify...). 2) $T$ is of finite order. (Then we just start with any inner product on $V$ and construct a new one via summing over iterates of $T$, i.e: $\langle v,w \rangle ' = \sum_{i=0}^{n-1} \langle T^iv,T^iw \rangle $). Note that (as explained for instance here ) this implies $T$ is diagonalizable over $\mathbb{C}$, but of course not necessarily over $\mathbb{R}$. (Think about our rotation again.) Actually, I have now understood that condition (1) implies $T$ is of order 2, (I think the reverse implication also holds, i.e $T^2=Id\Rightarrow T$ is diagonalizable). So condition (1) is a particular case of (2). However, (2) is  not necessary, since any rotation of irrational multiple of 2$\pi$ is an isometry w.r.t the standard product, but of infinite order. I somehow think the right way to handle this question is to think over $\mathbb{C}$, but I am not sure how to do this.",,"['linear-algebra', 'inner-products', 'linear-transformations', 'isometry']"
76,What properties does a rank-$1$ matrix have?,What properties does a rank- matrix have?,1,I have seen a lot of papers mentioning that a certain matrix is rank-$1$. What properties does a rank-$1$ matrix have? I know that if a matrix is rank-$1$ then there are no independent columns or rows in that matrix.,I have seen a lot of papers mentioning that a certain matrix is rank-$1$. What properties does a rank-$1$ matrix have? I know that if a matrix is rank-$1$ then there are no independent columns or rows in that matrix.,,"['linear-algebra', 'matrices', 'matrix-rank', 'rank-1-matrices']"
77,Kronecker product and matrix multiplication property,Kronecker product and matrix multiplication property,,"Given two symmetric matrices, $A \in \mathbb{R}^{n \times n}$ and $B \in \mathbb{R}^{m \times m}$ , is there any property of the Kronecker product which relates to matrix multiplication? More specifically, what is $(A \otimes B)C$ ?  And what should the dimensions of $C$ be?","Given two symmetric matrices, and , is there any property of the Kronecker product which relates to matrix multiplication? More specifically, what is ?  And what should the dimensions of be?",A \in \mathbb{R}^{n \times n} B \in \mathbb{R}^{m \times m} (A \otimes B)C C,"['linear-algebra', 'matrices', 'kronecker-product']"
78,"Proof of ""Eigenvectors corresponding to different eigenvalues are linearly independent."" [duplicate]","Proof of ""Eigenvectors corresponding to different eigenvalues are linearly independent."" [duplicate]",,"This question already has answers here : How to prove that eigenvectors from different eigenvalues are linearly independent [duplicate] (8 answers) Closed 10 years ago . ""Eigenvectors corresponding to different eigenvalues are linearly independent."" My professor told us this during a lecture, but gave no proof or explanation.","This question already has answers here : How to prove that eigenvectors from different eigenvalues are linearly independent [duplicate] (8 answers) Closed 10 years ago . ""Eigenvectors corresponding to different eigenvalues are linearly independent."" My professor told us this during a lecture, but gave no proof or explanation.",,['linear-algebra']
79,Is the product of $3$ positive semidefinite matrices positive semidefinite?,Is the product of  positive semidefinite matrices positive semidefinite?,3,"Is the product of $3$ positive semidefinite (PSD) matrices positive semidefinite if the product is symmetric? If so, any proof or reference, please? Eugene P. Wigner, On weakly positive matrices , Canadian Journal of Mathematics, Volume 15, 1963. states that the product of three positive definite matrices is positive definite iff the product is symmetric, but it doesn't extend the statement to the case of PSD.","Is the product of positive semidefinite (PSD) matrices positive semidefinite if the product is symmetric? If so, any proof or reference, please? Eugene P. Wigner, On weakly positive matrices , Canadian Journal of Mathematics, Volume 15, 1963. states that the product of three positive definite matrices is positive definite iff the product is symmetric, but it doesn't extend the statement to the case of PSD.",3,"['linear-algebra', 'matrices', 'reference-request', 'symmetric-matrices', 'positive-semidefinite']"
80,Geometric multiplicity of repeated Eigenvalues,Geometric multiplicity of repeated Eigenvalues,,"I am still finding it difficult to determine the geometric multiplicity for repeated eigenvalues and the resultant eigenspace. For example, I am not quite sure what to do with the following matrix, where repeated Eigenvalues $\lambda_1 = \lambda_2 = 5$: $$A=\begin{bmatrix} 5 & -4 & 0\\  1 & 0 & 2\\ 0 & 2 & 5 \end{bmatrix}, [A-\lambda I] = \begin{bmatrix} 0 & -4 & 0\\  1 & -5 & 2\\ 0 & 2 & 0 \end{bmatrix}$$ It is not obvious how to determine the Eigenvectors from this, as there are no free variables, and moving $e_2$ for example (for the first row) such that $-4e_2 = 0 \rightarrow e_2 = 0$ shows that all other values result to zero as well (which is not a valid eigenvector). How does one go about determining the geometric multiplicity and the Eigenspace with such a matrix?","I am still finding it difficult to determine the geometric multiplicity for repeated eigenvalues and the resultant eigenspace. For example, I am not quite sure what to do with the following matrix, where repeated Eigenvalues $\lambda_1 = \lambda_2 = 5$: $$A=\begin{bmatrix} 5 & -4 & 0\\  1 & 0 & 2\\ 0 & 2 & 5 \end{bmatrix}, [A-\lambda I] = \begin{bmatrix} 0 & -4 & 0\\  1 & -5 & 2\\ 0 & 2 & 0 \end{bmatrix}$$ It is not obvious how to determine the Eigenvectors from this, as there are no free variables, and moving $e_2$ for example (for the first row) such that $-4e_2 = 0 \rightarrow e_2 = 0$ shows that all other values result to zero as well (which is not a valid eigenvector). How does one go about determining the geometric multiplicity and the Eigenspace with such a matrix?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
81,"$^t$, $^\dagger$, $^*$, $^H$, $^⊤$, and $^T$ : Which is which, and what do each mean?",", , , , , and  : Which is which, and what do each mean?",^t ^\dagger ^* ^H ^⊤ ^T,"I think this question's answer(s) will be of profound use to the future generation of human beings who happen to stumble upon the website math.stackexchange.com . What are the differences between $^t$, $^\dagger$, $^*$, $^H$, and $^T$. Further, to which is which associated, and are there some cases where one is used for both? I'd like to see definitions for each group of superscript symbols and how $T$ the transformation and $A$ the matrix are related. I feel it would be good to have this as an answer here for all eyes to see.","I think this question's answer(s) will be of profound use to the future generation of human beings who happen to stumble upon the website math.stackexchange.com . What are the differences between $^t$, $^\dagger$, $^*$, $^H$, and $^T$. Further, to which is which associated, and are there some cases where one is used for both? I'd like to see definitions for each group of superscript symbols and how $T$ the transformation and $A$ the matrix are related. I feel it would be good to have this as an answer here for all eyes to see.",,"['linear-algebra', 'matrices', 'notation']"
82,What are these bracketing symbols and what do they mean?,What are these bracketing symbols and what do they mean?,,"What do the matching ""L"" shapes (near .5 and 20 ) mean in this forumla? The document where I found this formula can be found here: http://www.cs.cmu.edu/~hcwong/Pdfs/icip02.ps","What do the matching ""L"" shapes (near .5 and 20 ) mean in this forumla? The document where I found this formula can be found here: http://www.cs.cmu.edu/~hcwong/Pdfs/icip02.ps",,"['linear-algebra', 'notation']"
83,Determinant of a linear map given by conjugation,Determinant of a linear map given by conjugation,,"Suppose we have two fields $K\subset L$ and $A\in GL(n,L)$. See $L^{n\times n}$ as a $K$-vectorspace, then $$C_A\colon L^{n\times n}\rightarrow L^{n\times n},B\mapsto ABA^{-1}$$ is a $K$-linear map. What is the determinant of $C_A$? It seems trivial, but I'm too stupid right now. I could go into deep calculating to get a representation matrix for this linear map and compute the determinant of this matrix, but I'm sure that must work much easier. Thanks in advance! Edit: Even if I just consider the example $n=2$, $K=\mathbb{R}$ and $L=\mathbb{C}$, I'm not getting any idea of the determinant without starting to calculate the representation matrix. Does someone have an idea at least to this specific example?","Suppose we have two fields $K\subset L$ and $A\in GL(n,L)$. See $L^{n\times n}$ as a $K$-vectorspace, then $$C_A\colon L^{n\times n}\rightarrow L^{n\times n},B\mapsto ABA^{-1}$$ is a $K$-linear map. What is the determinant of $C_A$? It seems trivial, but I'm too stupid right now. I could go into deep calculating to get a representation matrix for this linear map and compute the determinant of this matrix, but I'm sure that must work much easier. Thanks in advance! Edit: Even if I just consider the example $n=2$, $K=\mathbb{R}$ and $L=\mathbb{C}$, I'm not getting any idea of the determinant without starting to calculate the representation matrix. Does someone have an idea at least to this specific example?",,"['linear-algebra', 'matrices', 'determinant']"
84,Minimize $ \arg \min_{A} {\| A x - b \|}_{2} $ (Least Squares Minimization on the Matrix),Minimize  (Least Squares Minimization on the Matrix), \arg \min_{A} {\| A x - b \|}_{2} ,"I have a machine learning regression problem. I need to minimize $$\sum_i \|Ax_i-b_i\|_2^2$$ However I am trying to find matrix $A$, not the usual $x$, and I have lots of example data for $x_i$ and $b_i$. In general $A$ has more rows than columns. Additionally I would like a solution for minimizing the Mahalanobis distance version, where $C$ is the SPSD covariance matrix: $$\sum_i(Ax_i-b_i)^TC^{-1}(Ax_i-b_i)$$ I'm thinking that my problem can be re-written into the usual least squares problem but not sure if I am doing it correctly.","I have a machine learning regression problem. I need to minimize $$\sum_i \|Ax_i-b_i\|_2^2$$ However I am trying to find matrix $A$, not the usual $x$, and I have lots of example data for $x_i$ and $b_i$. In general $A$ has more rows than columns. Additionally I would like a solution for minimizing the Mahalanobis distance version, where $C$ is the SPSD covariance matrix: $$\sum_i(Ax_i-b_i)^TC^{-1}(Ax_i-b_i)$$ I'm thinking that my problem can be re-written into the usual least squares problem but not sure if I am doing it correctly.",,"['linear-algebra', 'matrices', 'regression', 'machine-learning']"
85,What is the structure of invariant matrix polynomials?,What is the structure of invariant matrix polynomials?,,"Let the field $\mathbb F$ be either $\mathbb R$ or $\mathbb C$ and $M_n(\mathbb F)$ all $n \times n$ matrixes. We denote by $I_n(\mathbb F)$ the space of all functions: $$P : M_n(\mathbb F) \rightarrow \mathbb F$$ which are polynomial (in the sense that $P(A)$ is a polynomial in the entries of $A$ ), and which are invariant under the conjugation, i.e. $P(gAg^{-1}) = P(A)$ for all $g \in GL_n(\mathbb F)$ . For each $p \ge  0$ , we define $\Sigma_p \in I_n(\mathbb F)$ as $\Sigma_p(A)=\mathrm{Tr}(A^p)$ . Then do we have an isomorphism of algebras $I_n(\mathbb F) \cong \mathbb F[\Sigma_0,\Sigma_1,\dots,\Sigma_n]$ ?","Let the field be either or and all matrixes. We denote by the space of all functions: which are polynomial (in the sense that is a polynomial in the entries of ), and which are invariant under the conjugation, i.e. for all . For each , we define as . Then do we have an isomorphism of algebras ?","\mathbb F \mathbb R \mathbb C M_n(\mathbb F) n \times n I_n(\mathbb F) P : M_n(\mathbb F) \rightarrow \mathbb F P(A) A P(gAg^{-1}) = P(A) g \in GL_n(\mathbb F) p \ge  0 \Sigma_p \in I_n(\mathbb F) \Sigma_p(A)=\mathrm{Tr}(A^p) I_n(\mathbb F) \cong \mathbb F[\Sigma_0,\Sigma_1,\dots,\Sigma_n]","['linear-algebra', 'abstract-algebra', 'matrices']"
86,Inverse of transformation matrix,Inverse of transformation matrix,,"I am preparing for a computer 3D graphics test and have a sample question which I am unable to solve. The question is as follows: For the following 3D transfromation matrix M, find its inverse. Note that M is a composite matrix built from fundamental geometric affine transformations only. Show the initial transformation sequence of M, invert it, and write down the final inverted matrix of M. $M =\begin{pmatrix}0&0&1&5\\0&3&0&3\\-1&0&0&2\\0&0&0&1\end{pmatrix} $ I only know basic linear algebra and I don't think it is the purpose to just invert the matrix but to use the information in the question to solve this. Can anyone help? Thanks","I am preparing for a computer 3D graphics test and have a sample question which I am unable to solve. The question is as follows: For the following 3D transfromation matrix M, find its inverse. Note that M is a composite matrix built from fundamental geometric affine transformations only. Show the initial transformation sequence of M, invert it, and write down the final inverted matrix of M. $M =\begin{pmatrix}0&0&1&5\\0&3&0&3\\-1&0&0&2\\0&0&0&1\end{pmatrix} $ I only know basic linear algebra and I don't think it is the purpose to just invert the matrix but to use the information in the question to solve this. Can anyone help? Thanks",,"['linear-algebra', 'matrices', 'computer-science']"
87,Changing the spectrum of a symmetric matrix by diagonal perturbations,Changing the spectrum of a symmetric matrix by diagonal perturbations,,"Given a fixed symmetric matrix $S$, can one change the spectrum of $S$ to any desired set of eigenvalues $\{\lambda_1,\dots,\lambda_n\}$ by adding a diagonal matrix $D$ to $S$?","Given a fixed symmetric matrix $S$, can one change the spectrum of $S$ to any desired set of eigenvalues $\{\lambda_1,\dots,\lambda_n\}$ by adding a diagonal matrix $D$ to $S$?",,['linear-algebra']
88,"Injective, surjective and bijective for linear maps","Injective, surjective and bijective for linear maps",,"I have problems showing that $\phi$ is surjective. My understanding is, that I have to show for every $u \in \mathbb{R}^3$ that there exists a $v \in \mathbb{R}^3$ but I am not sure how. Let $a,b,c \in \mathbb{R}$. Let's examine the $\mathbb{R}$-linear map $\phi:\mathbb{R}^3\rightarrow\mathbb{R}^3$ defined through:   $\phi(e_1)=\begin{bmatrix}b \\ -c \\ 1\end{bmatrix}, \phi(e_2)=\begin{bmatrix}a \\ 1 \\ 0\end{bmatrix}, \phi(e_3)=\begin{bmatrix}1 \\ 0 \\ 0\end{bmatrix}$ with $e_1, e_2, e_3$ as standard basis of $\mathbb{R}^3$. Is $\phi$ injective, surjective, bijective? If $\phi$ is bijective, find the inverse function $\phi^{-1}$. My first step was to determine the linear map itself by: $\phi(x,y,z) = x\phi(e_1) + y\phi(e_2) + z\phi(e_3) = x\begin{pmatrix}b \\ -c \\1\end{pmatrix} + y\begin{pmatrix}a \\ 1 \\0\end{pmatrix} + z\begin{pmatrix}1 \\ 0 \\0\end{pmatrix}$ Now I want to show, that $\phi$ is injective: Since $\ker \phi$ must be $\{0\}$ I have following linear system of equations:   $\begin{align} bx+ay+z = 0 \\ -cx + y = 0\\x = 0\end{align}$ So it follows $x = y = z = 0$. Therefore $\phi$ must be injective. Now if I want to show that $\phi$ is surjective can I just say, if $u \in \mathbb{R}^3$ then there is obviously a $v = \phi(u_1, u_2, u_3) \in \mathbb{R}^3$?","I have problems showing that $\phi$ is surjective. My understanding is, that I have to show for every $u \in \mathbb{R}^3$ that there exists a $v \in \mathbb{R}^3$ but I am not sure how. Let $a,b,c \in \mathbb{R}$. Let's examine the $\mathbb{R}$-linear map $\phi:\mathbb{R}^3\rightarrow\mathbb{R}^3$ defined through:   $\phi(e_1)=\begin{bmatrix}b \\ -c \\ 1\end{bmatrix}, \phi(e_2)=\begin{bmatrix}a \\ 1 \\ 0\end{bmatrix}, \phi(e_3)=\begin{bmatrix}1 \\ 0 \\ 0\end{bmatrix}$ with $e_1, e_2, e_3$ as standard basis of $\mathbb{R}^3$. Is $\phi$ injective, surjective, bijective? If $\phi$ is bijective, find the inverse function $\phi^{-1}$. My first step was to determine the linear map itself by: $\phi(x,y,z) = x\phi(e_1) + y\phi(e_2) + z\phi(e_3) = x\begin{pmatrix}b \\ -c \\1\end{pmatrix} + y\begin{pmatrix}a \\ 1 \\0\end{pmatrix} + z\begin{pmatrix}1 \\ 0 \\0\end{pmatrix}$ Now I want to show, that $\phi$ is injective: Since $\ker \phi$ must be $\{0\}$ I have following linear system of equations:   $\begin{align} bx+ay+z = 0 \\ -cx + y = 0\\x = 0\end{align}$ So it follows $x = y = z = 0$. Therefore $\phi$ must be injective. Now if I want to show that $\phi$ is surjective can I just say, if $u \in \mathbb{R}^3$ then there is obviously a $v = \phi(u_1, u_2, u_3) \in \mathbb{R}^3$?",,['linear-algebra']
89,$\det(A^4+I)=29$ is not solvable by any $A\in M_4(\mathbb Z)$,is not solvable by any,\det(A^4+I)=29 A\in M_4(\mathbb Z),"I recently encountered the following problem. Given any $A \in M_4(\mathbb Z)$, show that $\det(A^4+I)\ne29$, where $I$ denotes the identity matrix. LHS can be written as the product of $1+{\lambda _i}^4$ where $\lambda _i$ denotes the eigenvalues of A. By using AM-GM inequality, I found that A is either invertible in $M_4(\mathbb Z)$ or has a zero determinant. I cannot go further. Can anyone help me?","I recently encountered the following problem. Given any $A \in M_4(\mathbb Z)$, show that $\det(A^4+I)\ne29$, where $I$ denotes the identity matrix. LHS can be written as the product of $1+{\lambda _i}^4$ where $\lambda _i$ denotes the eigenvalues of A. By using AM-GM inequality, I found that A is either invertible in $M_4(\mathbb Z)$ or has a zero determinant. I cannot go further. Can anyone help me?",,"['linear-algebra', 'matrices']"
90,An operator is semi-simple iff it is diagonalizable,An operator is semi-simple iff it is diagonalizable,,"I was reading on direct sums when I got into this Wikipedia page A linear operator T on a finite-dimensional vector space is semi-simple if every T-invariant subspace has a complementary T-invariant subspace. An important result regarding semi-simple operators is that, a linear operator on a finite dimensional vector space over an algebraically closed field is semi-simple if and only if it is diagonalizable I thought about how we could prove it, and I am following what is says in the page: This is because such an operator always has an eigenvector; if it is, in addition, semi-simple, then it has a complementary invariant hyperplane, which itself has an eigenvector, and thus by induction is diagonalizable. Conversely, diagonalizable operators are easily seen to be semi-simple, as invariant subspaces are direct sums of eigenspaces, and any basis for this space can be extended to an eigenbasis. I understand that we can write $T=\bigoplus_i T_{\lambda_i}$ where $T_{\lambda_i}=T|_{U_{\lambda_i}}$ and ${U_{\lambda_i}}$ is the eigenspace spanned by one eigenvector, but why does that imply that $T$ is diagonalizable?","I was reading on direct sums when I got into this Wikipedia page A linear operator T on a finite-dimensional vector space is semi-simple if every T-invariant subspace has a complementary T-invariant subspace. An important result regarding semi-simple operators is that, a linear operator on a finite dimensional vector space over an algebraically closed field is semi-simple if and only if it is diagonalizable I thought about how we could prove it, and I am following what is says in the page: This is because such an operator always has an eigenvector; if it is, in addition, semi-simple, then it has a complementary invariant hyperplane, which itself has an eigenvector, and thus by induction is diagonalizable. Conversely, diagonalizable operators are easily seen to be semi-simple, as invariant subspaces are direct sums of eigenspaces, and any basis for this space can be extended to an eigenbasis. I understand that we can write where and is the eigenspace spanned by one eigenvector, but why does that imply that is diagonalizable?",T=\bigoplus_i T_{\lambda_i} T_{\lambda_i}=T|_{U_{\lambda_i}} {U_{\lambda_i}} T,"['linear-algebra', 'eigenvalues-eigenvectors', 'diagonalization']"
91,Examples of inner products on a polynomial vector space,Examples of inner products on a polynomial vector space,,"So far, I have only seen examples like the following: $$ \langle p, q \rangle = \int_0^1 p(x)q(x)\ dx, $$ where $p$ and $q$ are elements (polynomials) of a finite-dimensional polynomial vector space. I'm wondering if there other kinds of inner products involving polynomials, without involving integrals or more interesting ones of the same kind.","So far, I have only seen examples like the following: $$ \langle p, q \rangle = \int_0^1 p(x)q(x)\ dx, $$ where $p$ and $q$ are elements (polynomials) of a finite-dimensional polynomial vector space. I'm wondering if there other kinds of inner products involving polynomials, without involving integrals or more interesting ones of the same kind.",,"['linear-algebra', 'inner-products']"
92,"If a linear map $T$ has a $k$-dimensional invariant subspace, does it admit an $n-k$ invariant subspace?","If a linear map  has a -dimensional invariant subspace, does it admit an  invariant subspace?",T k n-k,"Let $V$ be an $n$ -dimensional real vector space, and let $1<k<n-1$ be fixed. Let $T: V\to V$ be a linear map, and suppose that there exists a $k$ -dimensional $T$ invariant subspace of $V$ . Does there exist an $(n-k)$ $T$ -invariant subspace of $V$ ? The smallest possible dimensions where a counter-example might be  is when $k=2,n=5$ . Two comments: By duality, $T$ has a $k$ -dimensional invariant subspace if and only if the dual map $T^*:V^* \to V^*$ has an $n-k$ -dimensional invariant subspace. (If $U$ is $T$ -invariant, then the subspace of $V^*$ whose restriction to $U$ is zero is $T^*$ -invariant). I excluded the cases $k=1$ and $k=n-1$ , since I know that the answer is positive for those. Indeed, since the characteristic polynomials of $T$ and $T^{*}$ are identical, we have $$T \, \text{  has an eigenvector  if and only if } \, T^{*} \, \text{  has an eigenvector } \tag{1}.$$ By the previous comment, we also have $$T \, \text{  has an eigenvector  if and only if } \, T^{*} \, \text{  has a co-dimension one invariant subspace } \tag{2}.$$ Combining $(1)$ and $(2)$ , we conclude that $T^*$ has an eigenvector  if and only if $T^*$ has a co-dimension one invariant subspace. Since any map is the dual of its dual, this holds for any endomorphism $T$ .","Let be an -dimensional real vector space, and let be fixed. Let be a linear map, and suppose that there exists a -dimensional invariant subspace of . Does there exist an -invariant subspace of ? The smallest possible dimensions where a counter-example might be  is when . Two comments: By duality, has a -dimensional invariant subspace if and only if the dual map has an -dimensional invariant subspace. (If is -invariant, then the subspace of whose restriction to is zero is -invariant). I excluded the cases and , since I know that the answer is positive for those. Indeed, since the characteristic polynomials of and are identical, we have By the previous comment, we also have Combining and , we conclude that has an eigenvector  if and only if has a co-dimension one invariant subspace. Since any map is the dual of its dual, this holds for any endomorphism .","V n 1<k<n-1 T: V\to V k T V (n-k) T V k=2,n=5 T k T^*:V^* \to V^* n-k U T V^* U T^* k=1 k=n-1 T T^{*} T \, \text{  has an eigenvector  if and only if } \, T^{*} \, \text{  has an eigenvector } \tag{1}. T \, \text{  has an eigenvector  if and only if } \, T^{*} \, \text{  has a co-dimension one invariant subspace } \tag{2}. (1) (2) T^* T^* T","['linear-algebra', 'linear-transformations', 'invariant-subspace']"
93,Can any linear transformation be represented by a matrix?,Can any linear transformation be represented by a matrix?,,"Use $\cal L$ to denote a linear transformation on some vector space. We know any matrix $\bf{A}$ can be viewed as a linear transformation by defining $\cal L:= \cal L(\bf{v})= Av$ where $\bf{v}$ is a vector. I am curious is any linear transformation can be represented by a matrix? If so, why? If not, can anyone help provide a counterexample? In particular, projection of vectors onto a vector $\bf{b}$ is a linear transformation defined by ${\cal L(\bf{v})}=({\mathbf{v}} \cdot \frac{{\mathbf{b}}}{{|{\mathbf{b}}|}})\frac{{\mathbf{b}}}{{|{\mathbf{b}}|}}$. Can this linear transformation be represented by a matrix? Thank you!","Use $\cal L$ to denote a linear transformation on some vector space. We know any matrix $\bf{A}$ can be viewed as a linear transformation by defining $\cal L:= \cal L(\bf{v})= Av$ where $\bf{v}$ is a vector. I am curious is any linear transformation can be represented by a matrix? If so, why? If not, can anyone help provide a counterexample? In particular, projection of vectors onto a vector $\bf{b}$ is a linear transformation defined by ${\cal L(\bf{v})}=({\mathbf{v}} \cdot \frac{{\mathbf{b}}}{{|{\mathbf{b}}|}})\frac{{\mathbf{b}}}{{|{\mathbf{b}}|}}$. Can this linear transformation be represented by a matrix? Thank you!",,"['linear-algebra', 'matrices']"
94,Existence of an $n\times n$ real matrix $A$ such that $A^2=-I$.,Existence of an  real matrix  such that .,n\times n A A^2=-I,"Let $A$ be a $n\times n$ real matrix $A$ such that $A^2=-I$. Such an $A$ cannot be, Orthogonal. Invertible. Skew-symmetric. Symmetric. Diagonalizable. I tried to figure out the answer by looking at the [possible] determinant of $A$, using the fact $\det(AB)=\det(A)\det(B)$. So $\det(A^2)=(\det(A))^2=(-1)^n\det(I)$. If $n$ is even, then $\det(A)=\pm1$ and if odd then $\det(A)=\pm i$. There I stuck, if $n$ is even, I can try some guessing, but when $n$ is odd, the determinant becomes complex and I have no logic to go forward. Is there any other approach that might be more correct? How can I do this? Help me out.","Let $A$ be a $n\times n$ real matrix $A$ such that $A^2=-I$. Such an $A$ cannot be, Orthogonal. Invertible. Skew-symmetric. Symmetric. Diagonalizable. I tried to figure out the answer by looking at the [possible] determinant of $A$, using the fact $\det(AB)=\det(A)\det(B)$. So $\det(A^2)=(\det(A))^2=(-1)^n\det(I)$. If $n$ is even, then $\det(A)=\pm1$ and if odd then $\det(A)=\pm i$. There I stuck, if $n$ is even, I can try some guessing, but when $n$ is odd, the determinant becomes complex and I have no logic to go forward. Is there any other approach that might be more correct? How can I do this? Help me out.",,"['linear-algebra', 'matrices', 'determinant']"
95,How does the determinant link to the cross product,How does the determinant link to the cross product,,For a $2\times 2$ matrix $$\begin{pmatrix}a&b\\c&d \end{pmatrix} $$ The determinant is given by $ad-bc$. And the cross product of $$\begin{pmatrix} a\\b\\0\end{pmatrix}\times \begin{pmatrix} c\\d\\0\end{pmatrix} =\begin{pmatrix} 0\\0\\ad-bc\end{pmatrix}$$ We can also note that $|a\times b|=|a|b|\sin\theta$ where $\theta$ is the angle between $a$ and $b$. Hence is there any way to relate the determinant to the equation with sine? I recently saw that $$\text{Re}(a)\text{Im}(b)-\text{Im}(a)\text{Re}(b)=|ab|\sin{\text{arg}(a/b)}$$ How would one verify/prove this?,For a $2\times 2$ matrix $$\begin{pmatrix}a&b\\c&d \end{pmatrix} $$ The determinant is given by $ad-bc$. And the cross product of $$\begin{pmatrix} a\\b\\0\end{pmatrix}\times \begin{pmatrix} c\\d\\0\end{pmatrix} =\begin{pmatrix} 0\\0\\ad-bc\end{pmatrix}$$ We can also note that $|a\times b|=|a|b|\sin\theta$ where $\theta$ is the angle between $a$ and $b$. Hence is there any way to relate the determinant to the equation with sine? I recently saw that $$\text{Re}(a)\text{Im}(b)-\text{Im}(a)\text{Re}(b)=|ab|\sin{\text{arg}(a/b)}$$ How would one verify/prove this?,,"['linear-algebra', 'matrices', 'determinant']"
96,How to prove if something is a linear transformation?,How to prove if something is a linear transformation?,,"(I think) My textbook says something is a linear transformation if $L(ax) = aL(x)$ $L(x+y) = L(x) + L(y)$ $L(x) = A(x)$ But a lot of sites I've been on haven't proved these 3 things, so I just wanted to make sure that this is the proper way to prove it. For example, if $L(x) = (x_1, x_2, x_1 + 2x_2)^T$, then is it a linear transformation from $R^2$ to $R^3$? I know that it is, but I'm not sure if what I have above is how I'm supposed to prove it.","(I think) My textbook says something is a linear transformation if $L(ax) = aL(x)$ $L(x+y) = L(x) + L(y)$ $L(x) = A(x)$ But a lot of sites I've been on haven't proved these 3 things, so I just wanted to make sure that this is the proper way to prove it. For example, if $L(x) = (x_1, x_2, x_1 + 2x_2)^T$, then is it a linear transformation from $R^2$ to $R^3$? I know that it is, but I'm not sure if what I have above is how I'm supposed to prove it.",,"['linear-algebra', 'matrices', 'linear-transformations']"
97,Finding kernel and range of a linear transformation,Finding kernel and range of a linear transformation,,"We are given: Find $\ker(T)$, and $\textrm{rng}(T)$, where $T$ is the linear transformation given by $$T:\mathbb{R^3} \rightarrow \mathbb{R^3}$$ with standard matrix $$ A = \left[\begin{array}{rrr}     1 & -1 & 3\\     5 & 6 & -4\\     7 & 4 & 2\\     \end{array}\right]\textrm{.} $$ The kernel can be found in a $2 \times 2$ matrix as follows: $$ L = \left[\begin{array}{rrr}     a & b\\     c & d\\     \end{array}\right] = (a+d) + (b+c)t $$ Then to find the kernel of $L$ we set $$(a+d) + (b+c)t = 0$$ $$d = -a$$ $$c = -b$$ so that the kernel of $L$ is the set of all matrices of the form $$ A = \left[\begin{array}{rrr}     a & b\\     -b & -a\\     \end{array}\right] $$ but I do not know how to apply that to this problem.","We are given: Find $\ker(T)$, and $\textrm{rng}(T)$, where $T$ is the linear transformation given by $$T:\mathbb{R^3} \rightarrow \mathbb{R^3}$$ with standard matrix $$ A = \left[\begin{array}{rrr}     1 & -1 & 3\\     5 & 6 & -4\\     7 & 4 & 2\\     \end{array}\right]\textrm{.} $$ The kernel can be found in a $2 \times 2$ matrix as follows: $$ L = \left[\begin{array}{rrr}     a & b\\     c & d\\     \end{array}\right] = (a+d) + (b+c)t $$ Then to find the kernel of $L$ we set $$(a+d) + (b+c)t = 0$$ $$d = -a$$ $$c = -b$$ so that the kernel of $L$ is the set of all matrices of the form $$ A = \left[\begin{array}{rrr}     a & b\\     -b & -a\\     \end{array}\right] $$ but I do not know how to apply that to this problem.",,"['linear-algebra', 'linear-transformations']"
98,What operations can I do to simplify calculations of determinant?,What operations can I do to simplify calculations of determinant?,,"My question is simple. Given an $n \times n$ matrix $A$, what operations can we do to the rows and columns of $A$ to make the calculation of its determinant easier? I know we can put it into row echelon form, then the determinant is simply $\operatorname{trace}(A)$. Another question is, like operations in an augmented matrix $(A \mid b)$, can we swap rows? Would it affect the determinant?","My question is simple. Given an $n \times n$ matrix $A$, what operations can we do to the rows and columns of $A$ to make the calculation of its determinant easier? I know we can put it into row echelon form, then the determinant is simply $\operatorname{trace}(A)$. Another question is, like operations in an augmented matrix $(A \mid b)$, can we swap rows? Would it affect the determinant?",,"['linear-algebra', 'matrices', 'determinant']"
99,If $\operatorname{rank}\left( \begin{bmatrix} A &B \\ C &D \end{bmatrix}\right)=n$ prove that $\det(AD)=\det(BC)$ [closed],If  prove that  [closed],\operatorname{rank}\left( \begin{bmatrix} A &B \\ C &D \end{bmatrix}\right)=n \det(AD)=\det(BC),"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Let $A,\ B,\ C,\ D \in \mathcal{M}_n(\mathbb{C})$. If $\operatorname{rank}\left( \begin{bmatrix} A &B \\ C &D \end{bmatrix}\right)=n$, prove that $\det(AD)=\det(BC)$.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Let $A,\ B,\ C,\ D \in \mathcal{M}_n(\mathbb{C})$. If $\operatorname{rank}\left( \begin{bmatrix} A &B \\ C &D \end{bmatrix}\right)=n$, prove that $\det(AD)=\det(BC)$.",,"['linear-algebra', 'matrices']"
