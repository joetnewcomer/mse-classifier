,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"In mean value theorem, does the mean value vary continuously?","In mean value theorem, does the mean value vary continuously?",,"Let $f\colon\mathbb R\to\mathbb R$ be continuously differentiable and let's say, for simplicity, that $f(0)=0$. Then by mean value theorem it's $$f(x)=f'(\xi)\cdot x \,\text{ for some } \xi \in (0, x)$$ What I wondered is: What can we tell about the $\xi$ as we change $x$? My intuition says we should at least be able to find some $\xi\equiv \xi(x)$ that varies continuously with respect to $x$. Or isn't this necessarily the case?  Thanks for any ideas.","Let $f\colon\mathbb R\to\mathbb R$ be continuously differentiable and let's say, for simplicity, that $f(0)=0$. Then by mean value theorem it's $$f(x)=f'(\xi)\cdot x \,\text{ for some } \xi \in (0, x)$$ What I wondered is: What can we tell about the $\xi$ as we change $x$? My intuition says we should at least be able to find some $\xi\equiv \xi(x)$ that varies continuously with respect to $x$. Or isn't this necessarily the case?  Thanks for any ideas.",,['real-analysis']
1,continuity and $C^2$ solution of a series,continuity and  solution of a series,C^2,"For $\alpha$ with $|\alpha|=2$ let $P$ be a homogenous harmonic Polynom of degree $2$ with $D^\alpha P\ne0$ (e.g. take $P=2x_1x_2$). Choose $\eta\in C^\infty_0(\{x:|x|<2\})$ with $\eta=1$ when $|x|<1$ and $\eta=0$ when $|x|\geq2$, set $t_k=2^k$ and $c_k=\frac1k$ with $\sum c_k$ divergent. Define $f(x)=\sum\limits_0^\infty c_k\Delta(\eta P)(t_kx)$. How do you proof that $f$ is continuous but that $\Delta u=f$ does not have a $C^2$ solution in any neighborhood of the origin?","For $\alpha$ with $|\alpha|=2$ let $P$ be a homogenous harmonic Polynom of degree $2$ with $D^\alpha P\ne0$ (e.g. take $P=2x_1x_2$). Choose $\eta\in C^\infty_0(\{x:|x|<2\})$ with $\eta=1$ when $|x|<1$ and $\eta=0$ when $|x|\geq2$, set $t_k=2^k$ and $c_k=\frac1k$ with $\sum c_k$ divergent. Define $f(x)=\sum\limits_0^\infty c_k\Delta(\eta P)(t_kx)$. How do you proof that $f$ is continuous but that $\Delta u=f$ does not have a $C^2$ solution in any neighborhood of the origin?",,['real-analysis']
2,Connection between Legendre polynomial and Bessel function,Connection between Legendre polynomial and Bessel function,,"In Abramovitz and Stegun (Eq. 9.1.71) I found this curious relation $$\lim_{\nu\to\infty} \left[ \nu^\mu P_\nu^{-\mu}\left(\cos \frac{x}{\nu} \right) \right]= J_\mu(x) \qquad(1)$$ valid for $x>0$. In fact it can be used to obtain a rather good approximation $$ P_\nu^{-\mu}(\cos\theta) \approx  \frac{1}{\nu^\mu} J_\mu(\nu \theta)$$ of the Legendre polynomial in terms of a Bessel function for small $\theta$ (but $\nu\theta$ potentially large). This relation is a way to understand the eikonal approximation of wave scattering (which is the reason I noted it in the first place). As I am looking into the eikonal approximation, I would appreciate if somebody could help me   proving equation (1)?","In Abramovitz and Stegun (Eq. 9.1.71) I found this curious relation $$\lim_{\nu\to\infty} \left[ \nu^\mu P_\nu^{-\mu}\left(\cos \frac{x}{\nu} \right) \right]= J_\mu(x) \qquad(1)$$ valid for $x>0$. In fact it can be used to obtain a rather good approximation $$ P_\nu^{-\mu}(\cos\theta) \approx  \frac{1}{\nu^\mu} J_\mu(\nu \theta)$$ of the Legendre polynomial in terms of a Bessel function for small $\theta$ (but $\nu\theta$ potentially large). This relation is a way to understand the eikonal approximation of wave scattering (which is the reason I noted it in the first place). As I am looking into the eikonal approximation, I would appreciate if somebody could help me   proving equation (1)?",,"['calculus', 'real-analysis', 'limits', 'special-functions', 'orthogonal-polynomials']"
3,"Continuous but not Hölder continuous function on $[0,1]$",Continuous but not Hölder continuous function on,"[0,1]","Does there exist a continuous function $F$ on $[0,1]$ which is not Hölder continuous of order $\alpha$ at any point $X_{0}$ on $[0,1]$.  $0 < \alpha \le 1$. I am trying to prove that such a function does exist. also I couldn't find a good example.","Does there exist a continuous function $F$ on $[0,1]$ which is not Hölder continuous of order $\alpha$ at any point $X_{0}$ on $[0,1]$.  $0 < \alpha \le 1$. I am trying to prove that such a function does exist. also I couldn't find a good example.",,"['real-analysis', 'examples-counterexamples']"
4,$C(X)$ separates points?,separates points?,C(X),Let $X$ be a compact Hausdorff space and $C(X)$ the space of continuous functions on $X$. In many proofs in abstract harmonic analysis we use the fact $C(X)$ separates points in $X$. It might seem trivial but the only way I can think of why this is true is by applying Urysohn's lemma. I wonder whether there is some more elementary way to show that $C(X)$ separates points. Thanks!,Let $X$ be a compact Hausdorff space and $C(X)$ the space of continuous functions on $X$. In many proofs in abstract harmonic analysis we use the fact $C(X)$ separates points in $X$. It might seem trivial but the only way I can think of why this is true is by applying Urysohn's lemma. I wonder whether there is some more elementary way to show that $C(X)$ separates points. Thanks!,,"['real-analysis', 'general-topology', 'functions']"
5,Prove variant of triangle inequality containing p-th power for 0 < p < 1,Prove variant of triangle inequality containing p-th power for 0 < p < 1,,"Sorry if this is a trivial question, but I am kind of stuck with proving the following inequality and have been searching for a while: $\rho \left( \sum\limits_i^n d_i \right) \leq \sum\limits_i^n \;\rho(d_i)\;$ with $\rho(d_k) := |d_k|^p, \quad 0 < p < 1 $ and $ d_i \in \mathbb{R}$. I encountered this situation while dealing with an energy-based regularization approach. While I do not expect this inequality to hold for $p > 1$, I basically believe that it does so for $0 < p < 1$, implying that the energy $\rho$ for the sum of all $d_i$ is lower than the sum of energies of the individual $d_i$, under such a quasi-norm. I attempted to show this for the less general case where $p := \frac{q}{r}$ is a rational number, by raising both sides to the $r$-th power: $\mid\sum\limits_i^n d_i\mid^p \leq \sum\limits_i^n \mid d_i \mid^p \; \Leftrightarrow \; \mid\sum\limits_i^n d_i\mid^q \leq \left( \sum\limits_i^n \mid d_i \mid^p \right)^r = \sum\limits_i^n  \mid d_i \mid^{pr} + u  = \sum\limits_i^n  \mid d_i \mid^{q} + u$, where $u$ contains a sum of mixed terms. Next, I tried to get rid of the $q$ exponent in order to somehow make use of the triangle inequality. However, it seems like I am going in circles here. Could somebody give me a hint on how to better approach this? Is there an existing inequality that I could use in proving this? I guess these are basics that have been studied before, I am just not sure what keywords to search for. Thank you very much!","Sorry if this is a trivial question, but I am kind of stuck with proving the following inequality and have been searching for a while: $\rho \left( \sum\limits_i^n d_i \right) \leq \sum\limits_i^n \;\rho(d_i)\;$ with $\rho(d_k) := |d_k|^p, \quad 0 < p < 1 $ and $ d_i \in \mathbb{R}$. I encountered this situation while dealing with an energy-based regularization approach. While I do not expect this inequality to hold for $p > 1$, I basically believe that it does so for $0 < p < 1$, implying that the energy $\rho$ for the sum of all $d_i$ is lower than the sum of energies of the individual $d_i$, under such a quasi-norm. I attempted to show this for the less general case where $p := \frac{q}{r}$ is a rational number, by raising both sides to the $r$-th power: $\mid\sum\limits_i^n d_i\mid^p \leq \sum\limits_i^n \mid d_i \mid^p \; \Leftrightarrow \; \mid\sum\limits_i^n d_i\mid^q \leq \left( \sum\limits_i^n \mid d_i \mid^p \right)^r = \sum\limits_i^n  \mid d_i \mid^{pr} + u  = \sum\limits_i^n  \mid d_i \mid^{q} + u$, where $u$ contains a sum of mixed terms. Next, I tried to get rid of the $q$ exponent in order to somehow make use of the triangle inequality. However, it seems like I am going in circles here. Could somebody give me a hint on how to better approach this? Is there an existing inequality that I could use in proving this? I guess these are basics that have been studied before, I am just not sure what keywords to search for. Thank you very much!",,"['real-analysis', 'inequality', 'exponentiation', 'absolute-value']"
6,When do the partial derivative and integral of different variables commute? [duplicate],When do the partial derivative and integral of different variables commute? [duplicate],,"This question already has answers here : Closed 13 years ago . Possible Duplicate: Will moving differentiation from inside, to outside an integral, change the result? For which real functions $f(x,y)$ is this true? $$\dfrac{\partial}{\partial x} \int{\mathrm dy f(x,y)} =  \int{\mathrm dy \dfrac{\partial}{\partial x} f(x,y)}$$ The integrals should be interpreted as keeping $x$ constant and integrating over an interval of $y$.","This question already has answers here : Closed 13 years ago . Possible Duplicate: Will moving differentiation from inside, to outside an integral, change the result? For which real functions $f(x,y)$ is this true? $$\dfrac{\partial}{\partial x} \int{\mathrm dy f(x,y)} =  \int{\mathrm dy \dfrac{\partial}{\partial x} f(x,y)}$$ The integrals should be interpreted as keeping $x$ constant and integrating over an interval of $y$.",,['real-analysis']
7,What kind of completeness is the completeness of $\mathbb{R}$?,What kind of completeness is the completeness of ?,\mathbb{R},"As opposed to the algebraic completion of $\mathbb{Q}$, which yields the algebraic numbers, we can say that $\mathbb{R}$ is complete in the sense that every non-empty subset of $\mathbb{R}$ bounded by above has a supremum. So, it isn't algebraically complete, but is it topologically or metrically complete? What would be the right word to describe its completeness? Thanks.","As opposed to the algebraic completion of $\mathbb{Q}$, which yields the algebraic numbers, we can say that $\mathbb{R}$ is complete in the sense that every non-empty subset of $\mathbb{R}$ bounded by above has a supremum. So, it isn't algebraically complete, but is it topologically or metrically complete? What would be the right word to describe its completeness? Thanks.",,"['real-analysis', 'general-topology', 'terminology']"
8,How to find the inverse of a series?,How to find the inverse of a series?,,"Imagine a series like $f(x) = \sum_{n=0}^{\infty } f_n(x)$ . Is there any formula to find the $f^{-1}(x)$ as series expansion like $f^{-1}(x) = \sum_{n=0}^{\infty } g_n(x)$ ? Actually the the question is ""How to find all of $g_n$ functions only using $f_n$ functions?"" For instance; $\sin(x) = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + ...$ ,  and $\arcsin(x) = x + \frac{x^3}{6} + \frac{3x^5}{40} + \frac{5x^7}{112} + \frac{35x^9}{1152} ... $ Of course we have a lot of information about $\sin(x)$ . But what if the given function is a very custom function? For example how to find the inverse of this kind of function: $$f(x) = \sum_{n=0}^{\infty } \frac{x^{11n+5}}{(3n+2)!}$$ In general, if $f(x) = \sum_{n=0}^{\infty } f_n(x)$ , I am searching for an operator, $\text{Inv}[f_0, f_1, f_2, ... ] = [g_0, g_1, g_2, ...]$ such that $f^{-1}(x) = \sum_{n=0}^{\infty } g_n(x)$ .","Imagine a series like . Is there any formula to find the as series expansion like ? Actually the the question is ""How to find all of functions only using functions?"" For instance; ,  and Of course we have a lot of information about . But what if the given function is a very custom function? For example how to find the inverse of this kind of function: In general, if , I am searching for an operator, such that .","f(x) = \sum_{n=0}^{\infty } f_n(x) f^{-1}(x) f^{-1}(x) = \sum_{n=0}^{\infty } g_n(x) g_n f_n \sin(x) = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + ... \arcsin(x) = x + \frac{x^3}{6} + \frac{3x^5}{40} + \frac{5x^7}{112} + \frac{35x^9}{1152} ...  \sin(x) f(x) = \sum_{n=0}^{\infty } \frac{x^{11n+5}}{(3n+2)!} f(x) = \sum_{n=0}^{\infty } f_n(x) \text{Inv}[f_0, f_1, f_2, ... ] = [g_0, g_1, g_2, ...] f^{-1}(x) = \sum_{n=0}^{\infty } g_n(x)","['real-analysis', 'sequences-and-series', 'analysis', 'taylor-expansion', 'inverse']"
9,Why doesn't the trace operator preserve regularity?,Why doesn't the trace operator preserve regularity?,,"The trace operator maps $H^1(\Omega)$ into $H^{1/2}(\partial \Omega)$ , I have used this fact several times, and know of references where to find the proof. Let us assume that the boundaries are arbitrarily smooth. The trace operator then maps $C^1(\Omega)$ into $C^1(\partial \Omega)$ . Do you have an intuitive reason why we lose regularity in the weak case? I think this may be a good way to get valuable understanding of how weak and strong derivatives differ.","The trace operator maps into , I have used this fact several times, and know of references where to find the proof. Let us assume that the boundaries are arbitrarily smooth. The trace operator then maps into . Do you have an intuitive reason why we lose regularity in the weak case? I think this may be a good way to get valuable understanding of how weak and strong derivatives differ.",H^1(\Omega) H^{1/2}(\partial \Omega) C^1(\Omega) C^1(\partial \Omega),"['real-analysis', 'functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
10,Does there any diffentiable function $f$ such that $f'$ is discontinuous exactly on $\Bbb{Q} $ and continuous on $\Bbb{R}\setminus \Bbb{Q}$?,Does there any diffentiable function  such that  is discontinuous exactly on  and continuous on ?,f f' \Bbb{Q}  \Bbb{R}\setminus \Bbb{Q},"Does there exists any diffentiable function $f$ such that $f'$ is discontinuous exactly on $\Bbb{Q} $ and continuous on $\Bbb{R}\setminus \Bbb{Q}$ ? Since $\Bbb{Q}$ is $F_{\sigma}$ , we can produce a function which is discontinuous only on $\Bbb{Q}$ . For an example we can pick Thomae's function .But Thomae's function has no primitive. Because if thomae's function $f$ has a primitive $F$ then $F'=f $ . Since $F'$ is Darboux function, image of $F'=f$ must contains an intervals and this is not possible as Thomae's function doesn't attain irrational values. By choosing a particular example, we can conclude the impossibility of existence of such function. If $f'$ is Darboux function and belongs to Baire class $1$ then $f'$ has a primitive $f$ . Hence our goal is to create a Darboux function $f'$ of Baire class $1$ which is continuous on $\Bbb{Q}$ and discontinuous on $\Bbb{R}\setminus \Bbb{Q}$ . How to produce such function?","Does there exists any diffentiable function such that is discontinuous exactly on and continuous on ? Since is , we can produce a function which is discontinuous only on . For an example we can pick Thomae's function .But Thomae's function has no primitive. Because if thomae's function has a primitive then . Since is Darboux function, image of must contains an intervals and this is not possible as Thomae's function doesn't attain irrational values. By choosing a particular example, we can conclude the impossibility of existence of such function. If is Darboux function and belongs to Baire class then has a primitive . Hence our goal is to create a Darboux function of Baire class which is continuous on and discontinuous on . How to produce such function?",f f' \Bbb{Q}  \Bbb{R}\setminus \Bbb{Q} \Bbb{Q} F_{\sigma} \Bbb{Q} f F F'=f  F' F'=f f' 1 f' f f' 1 \Bbb{Q} \Bbb{R}\setminus \Bbb{Q},"['real-analysis', 'integration', 'analysis', 'derivatives', 'elementary-set-theory']"
11,What does it mean to show that an integral exists?,What does it mean to show that an integral exists?,,"Question I am slightly unclear on some terminology that I have come across in the following question: Let $f(x, t) = xe^{−xt}$ Show that the integral $I(x) = \int_0^{\infty} f(x, t)\ dt $ exists for all $x ≥ 0$ . What does it mean to show that an integral exists? Does this simply mean to show that it converges? I looked this up for some clarification but could only find questions rather than a precise description of what this means. I would be grateful for any explanation.",Question I am slightly unclear on some terminology that I have come across in the following question: Let Show that the integral exists for all . What does it mean to show that an integral exists? Does this simply mean to show that it converges? I looked this up for some clarification but could only find questions rather than a precise description of what this means. I would be grateful for any explanation.,"f(x, t) = xe^{−xt} I(x) = \int_0^{\infty} f(x, t)\ dt  x ≥ 0","['real-analysis', 'integration', 'analysis', 'convergence-divergence', 'terminology']"
12,Prove that $\int_0^1xf(x) dx \leq \frac{2}{3}(\int_0^1f(x)dx)^2$,Prove that,\int_0^1xf(x) dx \leq \frac{2}{3}(\int_0^1f(x)dx)^2,"Let $f:[0,1] \to [0,+\infty)$ be non negative continuous concave function, such that $f(0) = 1$ . Prove that $$\int_0^1xf(x) dx \leq \frac{2}{3}(\int_0^1f(x)dx)^2$$ Work: My professor left us some hints, but I am not sure if I am using them correctly, also I am not sure how to use all of them. Hints are: First hint, substitute $t=\lambda x$ in integral $$F(x)=\int_0^xf(t)dt.$$ What I have got is $$F(x)=\lambda \int_0^xf(\lambda x)dt.$$ Second hint is, using concavity and $f(0)=1$ prove that $$F(x)\geq \frac{x}{2}(1+f(x)).$$ Okay so, for concave functions there is inequality such as this $$f(y)\leq f(x)-f'(x)(y-x).$$ But I haven't got anything from that. Last hint is, using partial integration, conclude that $$\int_0^1 x f(x) dx \leq F(1) - \frac{1}{4} - \frac{1}{2} \int_0^1 x f(x) dx.$$ I think I have proven that, here is what I did: From partial integration I have got $$\int_0^1 x f(x) dx = x F(x){|}_0^1 - \int_0^1 F(x) dx \leq F(1) - \int_0^1 \frac{x}{2} (1+f(x)) dx \leq F(1) - \frac{1}{4} - \frac{1}{2} \int_0^1xf(x) dx$$ From that we have $$\int_0^1x f(x) dx \leq \frac{2}{3} (F(1) - \frac{1}{4}).$$ Using basic inequality $A^2-A+1/4 \geq 0$ we get $F(1) - \frac{1}{4} \leq F^2(1)$ and proof is finished. Conclusion: My question is how to prove the second hint and where we use substitution here, is that step necessary? Edit: I have realised that substitution I wrote above was not correct, should be $$\int_0^1 f(\lambda)d\lambda.$$ I guess we use it to make integral be $\int_0^1$ . Still haven't proven second hint. Any help will be welcome.","Let be non negative continuous concave function, such that . Prove that Work: My professor left us some hints, but I am not sure if I am using them correctly, also I am not sure how to use all of them. Hints are: First hint, substitute in integral What I have got is Second hint is, using concavity and prove that Okay so, for concave functions there is inequality such as this But I haven't got anything from that. Last hint is, using partial integration, conclude that I think I have proven that, here is what I did: From partial integration I have got From that we have Using basic inequality we get and proof is finished. Conclusion: My question is how to prove the second hint and where we use substitution here, is that step necessary? Edit: I have realised that substitution I wrote above was not correct, should be I guess we use it to make integral be . Still haven't proven second hint. Any help will be welcome.","f:[0,1] \to [0,+\infty) f(0) = 1 \int_0^1xf(x) dx \leq \frac{2}{3}(\int_0^1f(x)dx)^2 t=\lambda x F(x)=\int_0^xf(t)dt. F(x)=\lambda \int_0^xf(\lambda x)dt. f(0)=1 F(x)\geq \frac{x}{2}(1+f(x)). f(y)\leq f(x)-f'(x)(y-x). \int_0^1 x f(x) dx \leq F(1) - \frac{1}{4} - \frac{1}{2} \int_0^1 x f(x) dx. \int_0^1 x f(x) dx = x F(x){|}_0^1 - \int_0^1 F(x) dx \leq F(1) - \int_0^1 \frac{x}{2} (1+f(x)) dx \leq F(1) - \frac{1}{4} - \frac{1}{2} \int_0^1xf(x) dx \int_0^1x f(x) dx \leq \frac{2}{3} (F(1) - \frac{1}{4}). A^2-A+1/4 \geq 0 F(1) - \frac{1}{4} \leq F^2(1) \int_0^1 f(\lambda)d\lambda. \int_0^1","['real-analysis', 'calculus', 'integration', 'inequality']"
13,"Do these ""hyper-discontinuous"" functions exist?","Do these ""hyper-discontinuous"" functions exist?",,"Suppose $\ X\subset \mathbb{R}\ $ and $\ Y\subset \mathbb{R}.$ Definition: A function $\ f: X \to Y\ $ is hyper-discontinuous if for every $\ x\in X,\ \ \exists\ \delta>0,\ \varepsilon>0\ $ such that $\ y\in X \setminus \{x\},\  \vert y-x \vert < \delta,\ \implies \vert f(x) - f(y) \vert \geq \varepsilon.$ Hyper-discontinuous is a term I just made up, but it seems appropriate, because $\ f\ $ is hyper-discontinuous implies $\ f\ $ is nowhere continuous, whereas the converse must be false, and nowhere continuous is the concept most closely relating to ""most discontinuous function"" that I am aware of. My questions are the following: Is there a hyper-discontinuous function $\ f:[0,1] \to [0,1]\ ?$ Is there a hyper-discontinuous function $\ f:[0,1]\cap\mathbb{Q} \to [0,1]\cap\mathbb{Q}\ ?$ I've spent a while on these questions, but keep getting confused. I think Blumberg's theorem might be related to this, but I'm not sure.","Suppose and Definition: A function is hyper-discontinuous if for every such that Hyper-discontinuous is a term I just made up, but it seems appropriate, because is hyper-discontinuous implies is nowhere continuous, whereas the converse must be false, and nowhere continuous is the concept most closely relating to ""most discontinuous function"" that I am aware of. My questions are the following: Is there a hyper-discontinuous function Is there a hyper-discontinuous function I've spent a while on these questions, but keep getting confused. I think Blumberg's theorem might be related to this, but I'm not sure.","\ X\subset \mathbb{R}\  \ Y\subset \mathbb{R}. \ f: X \to Y\  \ x\in X,\ \ \exists\ \delta>0,\ \varepsilon>0\  \ y\in X \setminus \{x\},\  \vert y-x \vert < \delta,\ \implies \vert f(x) - f(y) \vert \geq \varepsilon. \ f\  \ f\  \ f:[0,1] \to [0,1]\ ? \ f:[0,1]\cap\mathbb{Q} \to [0,1]\cap\mathbb{Q}\ ?","['real-analysis', 'general-topology', 'continuity', 'recreational-mathematics']"
14,Is there a pathological continuous function $\ f:\mathbb{R}\to\mathbb{R}\ $ that is nowhere increasing or decreasing and has no local extrema?,Is there a pathological continuous function  that is nowhere increasing or decreasing and has no local extrema?,\ f:\mathbb{R}\to\mathbb{R}\ ,"Is there a pathological yet continuous function $\ f:\mathbb{R}\to\mathbb{R}\ $ such that: For every $\ x\in\mathbb{R}\ $ and $\ \delta>0,\ \exists\ a,b,\ $ both in $\ (x,x+\delta),\ $ such that $\ f(a)<f(x)<f(b)\qquad (*)\quad ? $ This makes me think of Weierstrass function because it is nowhere increasing and nowhere decreasing. However, at some points of the Weierstrass function a local maximum is attained. Let $\ x\ $ be a local maximum. Then by definition of local maximum, $\ \exists\ \delta>0\ $ such that $\ a\in(x,x+\delta)\implies f(a)\leq f(x).\ $ Therefore the Weierstrass function does not satisfy $\ (*).$ Maybe such a function is well known? Or if not, I was wondering if we can construct such a function using transfinite induction/recursion, although it might be difficult to prove that the function remains continuous in the limit case? Or maybe there is some clever argument that any real function with the property $\ (*)\ $ cannot be continuous everywhere (in particular, the function cannot be continuous at $ x)$ ?","Is there a pathological yet continuous function such that: For every and both in such that This makes me think of Weierstrass function because it is nowhere increasing and nowhere decreasing. However, at some points of the Weierstrass function a local maximum is attained. Let be a local maximum. Then by definition of local maximum, such that Therefore the Weierstrass function does not satisfy Maybe such a function is well known? Or if not, I was wondering if we can construct such a function using transfinite induction/recursion, although it might be difficult to prove that the function remains continuous in the limit case? Or maybe there is some clever argument that any real function with the property cannot be continuous everywhere (in particular, the function cannot be continuous at ?","\
f:\mathbb{R}\to\mathbb{R}\  \ x\in\mathbb{R}\  \ \delta>0,\ \exists\ a,b,\  \ (x,x+\delta),\  \ f(a)<f(x)<f(b)\qquad (*)\quad ?  \ x\  \ \exists\ \delta>0\  \ a\in(x,x+\delta)\implies f(a)\leq f(x).\  \ (*). \ (*)\   x)","['real-analysis', 'general-topology', 'continuity', 'problem-solving', 'transfinite-induction']"
15,Sequence of antiderivatives of a continuous function,Sequence of antiderivatives of a continuous function,,"Let $f:\mathbb{R}\to \mathbb{R}$ be a continuous function and $(f_n)_{n\geq 0}$ a sequence of functions such that $f_0=f$ and $f_{n+1}$ is an antiderivative (primitive) of $f_n$ for each $n\geq 0$ , with the property that for each $x\in \mathbb{R}$ there exists $n\in \mathbb{N}$ such that $f_n(x)=0$ . Prove that $f$ is identically 0. Honestly, I do not have a viable starting point; but the problem looks very nice. Obviously, $f_n^{(n)}=f$ and one can use the general formula for the solutions of this differential equation of order $n$ , but...","Let be a continuous function and a sequence of functions such that and is an antiderivative (primitive) of for each , with the property that for each there exists such that . Prove that is identically 0. Honestly, I do not have a viable starting point; but the problem looks very nice. Obviously, and one can use the general formula for the solutions of this differential equation of order , but...",f:\mathbb{R}\to \mathbb{R} (f_n)_{n\geq 0} f_0=f f_{n+1} f_n n\geq 0 x\in \mathbb{R} n\in \mathbb{N} f_n(x)=0 f f_n^{(n)}=f n,"['real-analysis', 'integration']"
16,$\lim_{ x \to 0 } \frac{e-(1+\arctan x)^{\frac{1}{x}}}{x}$,,\lim_{ x \to 0 } \frac{e-(1+\arctan x)^{\frac{1}{x}}}{x},I have to evaluate this limit: $$\lim_{ x \to 0 } 	\frac{e-(1+\arctan x)^{\frac{1}{x}}}{x}$$ I have an indeterminated form $[\frac{0}{0}]$ . So I applied the Hopital rule $$\frac{e-(1+\arctan x)^{\frac{1}{x}}}{x} \sim $$ $$-e^{\frac{\log(1+\arctan x)}{x}} \cdot \frac{\frac{x}{(1+\arctan x)(1+x^2)}-\log(1+\arctan x)}{x^2} \sim$$ $$ -e \cdot \frac{1}{x^2} \cdot \left(\frac{x}{(1+x)(1+x^2)}-x\right) \sim +e $$ According to my book and wolfram alpha the final result should be $\frac{e}{2}$ and I don't know where I made mistakes.,I have to evaluate this limit: I have an indeterminated form . So I applied the Hopital rule According to my book and wolfram alpha the final result should be and I don't know where I made mistakes.,"\lim_{ x \to 0 } 	\frac{e-(1+\arctan x)^{\frac{1}{x}}}{x} [\frac{0}{0}] \frac{e-(1+\arctan x)^{\frac{1}{x}}}{x} \sim  -e^{\frac{\log(1+\arctan x)}{x}} \cdot \frac{\frac{x}{(1+\arctan x)(1+x^2)}-\log(1+\arctan x)}{x^2} \sim  -e \cdot \frac{1}{x^2} \cdot \left(\frac{x}{(1+x)(1+x^2)}-x\right)
\sim +e  \frac{e}{2}","['real-analysis', 'limits']"
17,How to evaluate $\int _0^{\pi }x\sin \left(x\right)\operatorname{Li}_2\left(\cos \left(2x\right)\right)\:dx$.,How to evaluate .,\int _0^{\pi }x\sin \left(x\right)\operatorname{Li}_2\left(\cos \left(2x\right)\right)\:dx,"How can i evaluate $$\int _0^{\pi }x\sin \left(x\right)\operatorname{Li}_2\left(\cos \left(2x\right)\right)\:dx$$ $$=\frac{\pi ^3}{6}-\frac{\pi ^3}{6\sqrt{2}}-4\pi +6\pi \ln \left(2\right)-\frac{\pi }{2\sqrt{2}}\ln ^2\left(2\sqrt{2}+3\right)-\sqrt{2}\pi \operatorname{Li}_2\left(2\sqrt{2}-3\right)$$ This is what I've done thus far. \begin{align*} &\int _0^{\pi }x\sin \left(x\right)\operatorname{Li}_2\left(\cos \left(2x\right)\right)\:dx\\ &=\frac{\pi }{2}\int _0^{\pi }\sin \left(x\right)\operatorname{Li}_2\left(\cos \left(2x\right)\right)\:dx\\[2mm] &=\frac{\pi }{4}\int _0^{\pi }\sin \left(\frac{x}{2}\right)\operatorname{Li}_2\left(\cos \left(x\right)\right)\:dx+\frac{\pi }{4}\int _{\pi }^{2\pi }\sin \left(\frac{x}{2}\right)\operatorname{Li}_2\left(\cos \left(x\right)\right)\:dx\\[2mm] &=\frac{\pi }{2}\int _0^{\pi }\sqrt{\frac{1+\cos \left(x\right)}{2}}\operatorname{Li}_2\left(-\cos \left(x\right)\right)\:dx=\pi \int _0^{\infty }\frac{\operatorname{Li}_2\left(\frac{t^2-1}{1+t^2}\right)}{\left(1+t^2\right)\sqrt{1+t^2}}\:dt\\[2mm] &=\frac{\pi }{2}\int _1^{\infty }\frac{\operatorname{Li}_2\left(\frac{x-2}{x}\right)}{x\sqrt{x}\sqrt{x-1}}\:dx=\frac{\pi }{2}\int _0^1\frac{\operatorname{Li}_2\left(1-2x\right)}{\sqrt{1-x}}\:dx\\[2mm] &=\pi \zeta \left(2\right)+\frac{\pi }{\sqrt{2}}\int _{-1}^1\frac{\sqrt{1+x}\ln \left(1-x\right)}{x}\:dx \end{align*} But I'm not sure how to proceed with either that polylogarithmic integral on the $5$ th line nor the last one. I'd appreciate any hints or ideas, thanks.","How can i evaluate This is what I've done thus far. But I'm not sure how to proceed with either that polylogarithmic integral on the th line nor the last one. I'd appreciate any hints or ideas, thanks.","\int _0^{\pi }x\sin \left(x\right)\operatorname{Li}_2\left(\cos \left(2x\right)\right)\:dx =\frac{\pi ^3}{6}-\frac{\pi ^3}{6\sqrt{2}}-4\pi +6\pi \ln \left(2\right)-\frac{\pi }{2\sqrt{2}}\ln ^2\left(2\sqrt{2}+3\right)-\sqrt{2}\pi \operatorname{Li}_2\left(2\sqrt{2}-3\right) \begin{align*}
&\int _0^{\pi }x\sin \left(x\right)\operatorname{Li}_2\left(\cos \left(2x\right)\right)\:dx\\
&=\frac{\pi }{2}\int _0^{\pi }\sin \left(x\right)\operatorname{Li}_2\left(\cos \left(2x\right)\right)\:dx\\[2mm]
&=\frac{\pi }{4}\int _0^{\pi }\sin \left(\frac{x}{2}\right)\operatorname{Li}_2\left(\cos \left(x\right)\right)\:dx+\frac{\pi }{4}\int _{\pi }^{2\pi }\sin \left(\frac{x}{2}\right)\operatorname{Li}_2\left(\cos \left(x\right)\right)\:dx\\[2mm]
&=\frac{\pi }{2}\int _0^{\pi }\sqrt{\frac{1+\cos \left(x\right)}{2}}\operatorname{Li}_2\left(-\cos \left(x\right)\right)\:dx=\pi \int _0^{\infty }\frac{\operatorname{Li}_2\left(\frac{t^2-1}{1+t^2}\right)}{\left(1+t^2\right)\sqrt{1+t^2}}\:dt\\[2mm]
&=\frac{\pi }{2}\int _1^{\infty }\frac{\operatorname{Li}_2\left(\frac{x-2}{x}\right)}{x\sqrt{x}\sqrt{x-1}}\:dx=\frac{\pi }{2}\int _0^1\frac{\operatorname{Li}_2\left(1-2x\right)}{\sqrt{1-x}}\:dx\\[2mm]
&=\pi \zeta \left(2\right)+\frac{\pi }{\sqrt{2}}\int _{-1}^1\frac{\sqrt{1+x}\ln \left(1-x\right)}{x}\:dx
\end{align*} 5","['real-analysis', 'integration']"
18,Integral inequality - École Polytechnique International Entrance Exam,Integral inequality - École Polytechnique International Entrance Exam,,"(The exercise shown below is from a past admission exam taken in order to get accepted at the École Polytechnique, France, as an international student.) In that exercise, I was able to solve the first two items (1.1 and 1.2), but I'm having some problems in getting the right answer in the last one, 1.3. To solve item 1.3, I'm using the fact that $$ \left| \int_{a}^{b} f(t) dt \right|  \leq \int_{a}^{b} \left| f(t) \right| dt \qquad (*)$$ Furthermore, I'm using the first result presented in the item 1.1. By integrating the inequality shown in that result from $a$ to $b$ , I was able to get that $$ \int_{a}^{b} \left| f(t) \right| dt \leq K\frac{(b-a)^2}{2}$$ Then, it follows from (*) that $$ \left| \int_{a}^{b} f(t) dt \right|  \leq K\frac{(b-a)^2}{2} $$ However, the answer expected is $ \left| \int_{a}^{b} f(t) dt \right|  \leq K\frac{(b-a)^2}{4} $ It seems to me that I should also use the result obtained at item 1.2, but it isn't really clear to me how it would help me solve the last item. Could someone help me identify where my (possible) mistake is, or suggest a way to consider the result from item 1.2? Thanks. (P.S. My english isn't that good -- Sorry)","(The exercise shown below is from a past admission exam taken in order to get accepted at the École Polytechnique, France, as an international student.) In that exercise, I was able to solve the first two items (1.1 and 1.2), but I'm having some problems in getting the right answer in the last one, 1.3. To solve item 1.3, I'm using the fact that Furthermore, I'm using the first result presented in the item 1.1. By integrating the inequality shown in that result from to , I was able to get that Then, it follows from (*) that However, the answer expected is It seems to me that I should also use the result obtained at item 1.2, but it isn't really clear to me how it would help me solve the last item. Could someone help me identify where my (possible) mistake is, or suggest a way to consider the result from item 1.2? Thanks. (P.S. My english isn't that good -- Sorry)", \left| \int_{a}^{b} f(t) dt \right|  \leq \int_{a}^{b} \left| f(t) \right| dt \qquad (*) a b  \int_{a}^{b} \left| f(t) \right| dt \leq K\frac{(b-a)^2}{2}  \left| \int_{a}^{b} f(t) dt \right|  \leq K\frac{(b-a)^2}{2}   \left| \int_{a}^{b} f(t) dt \right|  \leq K\frac{(b-a)^2}{4} ,"['real-analysis', 'calculus', 'integration', 'integral-inequality']"
19,Show that $(\sum a_{n}^{3} \sin n)$ converges given $\sum{a_n}$ converges,Show that  converges given  converges,(\sum a_{n}^{3} \sin n) \sum{a_n},"Given that $\sum a_{n}$ converges $\left(a_{n}>0\right) ;$ Then $(\sum a_{n}^{3} \sin n)$ is My approach: Since, $\sum a_{n}$ converges, we have $\lim _{n \rightarrow \infty} n \cdot a_{n}$ converges. i.e. $\left|n \cdot a_{n}\right| \leq 1$ for $n \geq K(\text { say })$ $\Rightarrow n \cdot a_{n}<1 \quad\left[\because a_{n}>0\right]$ $\Rightarrow a_{n}<\frac{1}{n}$ $\therefore a_{n}^{3}<\frac{1}{n^{3}}$ $\Rightarrow a_{n}^{3} \sin n \leq \frac{1}{n^{3}} \sin n \leq \frac{1}{n^{3}}$ $\Rightarrow \sum a_{n}^{3} \sin n \leq \sum \frac{1}{n^{3}}$ $\because \mathrm{RHS}$ converges so LHS will also converge. Any other better approach will be highly appreciated and correct me If I am wrong","Given that converges Then is My approach: Since, converges, we have converges. i.e. for converges so LHS will also converge. Any other better approach will be highly appreciated and correct me If I am wrong",\sum a_{n} \left(a_{n}>0\right) ; (\sum a_{n}^{3} \sin n) \sum a_{n} \lim _{n \rightarrow \infty} n \cdot a_{n} \left|n \cdot a_{n}\right| \leq 1 n \geq K(\text { say }) \Rightarrow n \cdot a_{n}<1 \quad\left[\because a_{n}>0\right] \Rightarrow a_{n}<\frac{1}{n} \therefore a_{n}^{3}<\frac{1}{n^{3}} \Rightarrow a_{n}^{3} \sin n \leq \frac{1}{n^{3}} \sin n \leq \frac{1}{n^{3}} \Rightarrow \sum a_{n}^{3} \sin n \leq \sum \frac{1}{n^{3}} \because \mathrm{RHS},"['real-analysis', 'sequences-and-series']"
20,"No function continuous at rational points and discontinuous at irrational points in $[0,1]$",No function continuous at rational points and discontinuous at irrational points in,"[0,1]","Let $C_f$ and $D_f$ mean sets where a function is continuous and discontinuous. I’m trying to prove there is no function $f:[0,1] \to \mathbb{R}$ such that $C_f = [0,1] \cap \mathbb{Q}$ and $D_f = [0,1] \setminus \mathbb{Q}$ . I have seen a proof using the Baire category theorem that there cannot be two functions $f$ and $g$ where $C_f$ and $C_g$ are both dense and $C_f = D_g$ . Thomae’s function is continuous at the irrationals and discontinuous at the rationals, so it is impossible to have a function that is discontinuous on irrationals and continuous on rationals.  The proof uses a lemma that if $C_f$ is dense then $D_f$ is first category, and some of the steps are not clear  to me. Is there a more elementary proof that there is no function discontinuous on irrationals and continuous on rationals?","Let and mean sets where a function is continuous and discontinuous. I’m trying to prove there is no function such that and . I have seen a proof using the Baire category theorem that there cannot be two functions and where and are both dense and . Thomae’s function is continuous at the irrationals and discontinuous at the rationals, so it is impossible to have a function that is discontinuous on irrationals and continuous on rationals.  The proof uses a lemma that if is dense then is first category, and some of the steps are not clear  to me. Is there a more elementary proof that there is no function discontinuous on irrationals and continuous on rationals?","C_f D_f f:[0,1] \to \mathbb{R} C_f = [0,1] \cap \mathbb{Q} D_f = [0,1] \setminus \mathbb{Q} f g C_f C_g C_f = D_g C_f D_f",['real-analysis']
21,Limit of a certain expression,Limit of a certain expression,,"Let $f$ be a twice differentiable function on $(0,1)$ s.t. $\lim\limits_{x\to0+} f(x)=0$ and estimates $$ |f^{(k)}(x)|\le Cx^{-k},\quad  k=0,1,2;\ x\in(0,1), $$ hold. Is it true that $$\lim_{x\to0+} xf'(x)=0?$$ Or, for function $g(x)=xf(x)$, it is equivalent to $\lim_{x\to0+} g'(x)=0$ since $g'(x)=f(x)+xf'(x)$. Remark. If to drop the condition on derivatives for $k=2$, i.e. $|f''(x)|\le Cx^{-2}$, the statement is false as the following example shows: Let $f(x)=x \cos \frac1x$, then$$ x f'(x)=\sin \frac{1}{x}+x \cos \frac{1}{x} $$ does not tend to $0$. In this case,$$ f''(x)=-\frac1{x^{3}}\cos \frac{1}{x}. $$","Let $f$ be a twice differentiable function on $(0,1)$ s.t. $\lim\limits_{x\to0+} f(x)=0$ and estimates $$ |f^{(k)}(x)|\le Cx^{-k},\quad  k=0,1,2;\ x\in(0,1), $$ hold. Is it true that $$\lim_{x\to0+} xf'(x)=0?$$ Or, for function $g(x)=xf(x)$, it is equivalent to $\lim_{x\to0+} g'(x)=0$ since $g'(x)=f(x)+xf'(x)$. Remark. If to drop the condition on derivatives for $k=2$, i.e. $|f''(x)|\le Cx^{-2}$, the statement is false as the following example shows: Let $f(x)=x \cos \frac1x$, then$$ x f'(x)=\sin \frac{1}{x}+x \cos \frac{1}{x} $$ does not tend to $0$. In this case,$$ f''(x)=-\frac1{x^{3}}\cos \frac{1}{x}. $$",,"['real-analysis', 'limits']"
22,Difference between polynomial and power series,Difference between polynomial and power series,,"My lecture notes say: A polynomial on $\mathbb{R}$ is a function $f: \mathbb{R} \to \mathbb{R}$ with $x \mapsto \sum_{k=0}^n a_k x^k$ with $n \in \mathbb{N}$ and $a_k \in \mathbb{R}$ . A power series in $x \in \mathbb{R}$ is the expression $\sum_{k=0}^\infty a_k x^k$ with $a_k \in \mathbb{R}$ . I don't really understand the difference here. What should ""expression"" mean in this case? Except that the second sum goes to infinity the definitions look the same to me. Can somebody explain me the differences?","My lecture notes say: A polynomial on is a function with with and . A power series in is the expression with . I don't really understand the difference here. What should ""expression"" mean in this case? Except that the second sum goes to infinity the definitions look the same to me. Can somebody explain me the differences?",\mathbb{R} f: \mathbb{R} \to \mathbb{R} x \mapsto \sum_{k=0}^n a_k x^k n \in \mathbb{N} a_k \in \mathbb{R} x \in \mathbb{R} \sum_{k=0}^\infty a_k x^k a_k \in \mathbb{R},"['real-analysis', 'functions', 'power-series']"
23,Existence of continuous function $f$ on $\Bbb R$ which vanishes exactly on $A\subset \Bbb R$,Existence of continuous function  on  which vanishes exactly on,f \Bbb R A\subset \Bbb R,"Question For any closed subset $A \subset \Bbb R$, does there exist a continuous function $f$ on $\Bbb R$ which vanishes exactly on $A$? If we take $A=[a,\infty)$ or $(-\infty,a]$ or $\{a_1,a_2,\cdots,a_n\}$ or $\bigcup_{i=1}^{k} [a_i,b_i]$, then indeed we can have such a continuous function. But when I thought about cantor set or the set $\{\frac 1n : n \in \Bbb N\} \cup \{0\}$, which are closed in $\Bbb R$, I couldn't think of a continuous map vanishing exactly at these two sets. How should I go about this problem?","Question For any closed subset $A \subset \Bbb R$, does there exist a continuous function $f$ on $\Bbb R$ which vanishes exactly on $A$? If we take $A=[a,\infty)$ or $(-\infty,a]$ or $\{a_1,a_2,\cdots,a_n\}$ or $\bigcup_{i=1}^{k} [a_i,b_i]$, then indeed we can have such a continuous function. But when I thought about cantor set or the set $\{\frac 1n : n \in \Bbb N\} \cup \{0\}$, which are closed in $\Bbb R$, I couldn't think of a continuous map vanishing exactly at these two sets. How should I go about this problem?",,"['real-analysis', 'general-topology', 'analysis', 'metric-spaces', 'continuity']"
24,How to calculate $\lim_{x\to0} \sum_{n=1}^\infty \frac{\sin x}{4+n^4x^4}$ using calculus,How to calculate  using calculus,\lim_{x\to0} \sum_{n=1}^\infty \frac{\sin x}{4+n^4x^4},"I have problem with calculating this limit, I suppose i should transform it into some integral but I don't know how: $$ \lim_{x\to0} \sum_{n=1}^\infty \frac{\sin x}{4+n^4x^4} $$","I have problem with calculating this limit, I suppose i should transform it into some integral but I don't know how: $$ \lim_{x\to0} \sum_{n=1}^\infty \frac{\sin x}{4+n^4x^4} $$",,"['calculus', 'real-analysis', 'limits']"
25,Finding Radon-Nikodym derivative,Finding Radon-Nikodym derivative,,"Let $m$ be Lebesgue measure on $\mathbb R_+=(0,\infty)$ and $\mathcal A = \sigma\left(( \frac 1{n+1} , \frac 1n ]:n=1,2,...\right)$. Define a new measure $\lambda$ on $\mathcal A$, for each $E \in \mathcal A$, by $\lambda(E)= \int_E fdm $, where $f(x)=2x^2$ . Find the Radon-Nikodym derivative $\frac{d\lambda}{dm}$. It is clear that $\lambda $ is absolutely continuous with respect to $m$ by definition, and Lebesgue measure is $\sigma$-finite in $(\mathbb R_+ , \mathfrak M_+)$, where $\mathfrak M_+$ is the collection of all Lebesgue measurable subsets of $\mathbb R_+$, so $m$ is $\sigma$-finite in $(\mathbb R_+, \mathcal A)$ since every element of $\mathcal A$ is also Lebesgue measurable. However, to apply Radon-Nikodym theorem, $\lambda$ must be a finite measure, but $\lambda(E)= \int_E fdm = \infty$ if $E=\mathbb R_+ - ( \frac 12,1]$, so we cannot apply that theorem. Is there any other approach to this problem?","Let $m$ be Lebesgue measure on $\mathbb R_+=(0,\infty)$ and $\mathcal A = \sigma\left(( \frac 1{n+1} , \frac 1n ]:n=1,2,...\right)$. Define a new measure $\lambda$ on $\mathcal A$, for each $E \in \mathcal A$, by $\lambda(E)= \int_E fdm $, where $f(x)=2x^2$ . Find the Radon-Nikodym derivative $\frac{d\lambda}{dm}$. It is clear that $\lambda $ is absolutely continuous with respect to $m$ by definition, and Lebesgue measure is $\sigma$-finite in $(\mathbb R_+ , \mathfrak M_+)$, where $\mathfrak M_+$ is the collection of all Lebesgue measurable subsets of $\mathbb R_+$, so $m$ is $\sigma$-finite in $(\mathbb R_+, \mathcal A)$ since every element of $\mathcal A$ is also Lebesgue measurable. However, to apply Radon-Nikodym theorem, $\lambda$ must be a finite measure, but $\lambda(E)= \int_E fdm = \infty$ if $E=\mathbb R_+ - ( \frac 12,1]$, so we cannot apply that theorem. Is there any other approach to this problem?",,"['real-analysis', 'measure-theory', 'lebesgue-measure', 'absolute-continuity', 'radon-nikodym']"
26,Difference between limit superior & supremum of a sequence,Difference between limit superior & supremum of a sequence,,"Can anyone please tell what's the difference between the terms limit superior, limit inferior, supremum, infimum. They are confusing me.","Can anyone please tell what's the difference between the terms limit superior, limit inferior, supremum, infimum. They are confusing me.",,"['real-analysis', 'sequences-and-series']"
27,Prove: if $f(0)=0$ and $f'(0)=0$ then $f''(0)\geq 0$,Prove: if  and  then,f(0)=0 f'(0)=0 f''(0)\geq 0,"let $f$ be a nonnegative and differentiable twice in the interval $[-1,1]$ Prove: if $f(0)=0$ and $f'(0)=0$ then $f''(0)\geq 0$ Are all the assumptions on $f$ necessary for the result to hold ? what can be said if $f''(0)= 0$ ? Looking at the taylor polynomial and lagrange remainder we get: $$f(x)=f(0)+f'(0)x+\frac{f''(c)x^2}{2}$$ $$f(x)=\frac{f''(c)x^2}{2}$$ Because the function is nonnegative and $\frac{x^2}{2}\geq 0$ so $f''(c)\geq 0$ For 1., all the data is needed but I can not find a valid reason. For 2., can we conclude that the function the null function?","let be a nonnegative and differentiable twice in the interval Prove: if and then Are all the assumptions on necessary for the result to hold ? what can be said if ? Looking at the taylor polynomial and lagrange remainder we get: Because the function is nonnegative and so For 1., all the data is needed but I can not find a valid reason. For 2., can we conclude that the function the null function?","f [-1,1] f(0)=0 f'(0)=0 f''(0)\geq 0 f f''(0)= 0 f(x)=f(0)+f'(0)x+\frac{f''(c)x^2}{2} f(x)=\frac{f''(c)x^2}{2} \frac{x^2}{2}\geq 0 f''(c)\geq 0","['real-analysis', 'derivatives']"
28,What is the regularity of the greatest eigenvalue of the Hessian matrix?,What is the regularity of the greatest eigenvalue of the Hessian matrix?,,"Let $f:\mathbb{R}^n\to\mathbb{R}$ be twice continuously differentiable, i.e. $f\in\mathcal{C}^2\left(\mathbb{R}^n\right)$. Define $\lambda_f:\mathbb{R}^n\to\mathbb{R}$ as the function which associates $\vec x$ with the greatest eigenvalue of the Hessian matrix of $f$ in $\vec x$. Then what can we say about the regularity of $\lambda_f$? Is it continuous/differentiable? The question came to my mind at todays analysis exam. I have absolutely no idea how one may argue; the definition of $\lambda_f$ doesn't seem to be easily useable. Edit: Or even more generally: is the function $\Lambda_f:\mathbb{R}^n\to\mathbb{R}^n$ which assignes to $\vec x$ the ordered $n$-tuple (with multiplicities) of the Hessian of $f$ at $\vec x$ (i.e. $\Lambda_f\left(\vec x\right)=\left(\lambda_1,...,\lambda_n\right)$ where $\lambda_1≥...≥\lambda_n$ are the eigenvalues of $H_f\left(\vec x\right)$) continuous/differntiable? If so, can we calculate the Jacobian matrix?","Let $f:\mathbb{R}^n\to\mathbb{R}$ be twice continuously differentiable, i.e. $f\in\mathcal{C}^2\left(\mathbb{R}^n\right)$. Define $\lambda_f:\mathbb{R}^n\to\mathbb{R}$ as the function which associates $\vec x$ with the greatest eigenvalue of the Hessian matrix of $f$ in $\vec x$. Then what can we say about the regularity of $\lambda_f$? Is it continuous/differentiable? The question came to my mind at todays analysis exam. I have absolutely no idea how one may argue; the definition of $\lambda_f$ doesn't seem to be easily useable. Edit: Or even more generally: is the function $\Lambda_f:\mathbb{R}^n\to\mathbb{R}^n$ which assignes to $\vec x$ the ordered $n$-tuple (with multiplicities) of the Hessian of $f$ at $\vec x$ (i.e. $\Lambda_f\left(\vec x\right)=\left(\lambda_1,...,\lambda_n\right)$ where $\lambda_1≥...≥\lambda_n$ are the eigenvalues of $H_f\left(\vec x\right)$) continuous/differntiable? If so, can we calculate the Jacobian matrix?",,"['calculus', 'real-analysis', 'hessian-matrix']"
29,Theorem 2.43 in Baby Rudin: How to understand the proof?,Theorem 2.43 in Baby Rudin: How to understand the proof?,,"Here's Theorem 2.43 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Let $P$ be a non-empty perfect set in $\mathbb{R}^k$ . Then $P$ is uncountable. Here's the definition of a perfect set: Let $(X,d)$ be a metric space, and let $P \subset X$ . Then $P$ is perfect if it is closed (i.e. it contains all of its limit points) and every point of $P$ is also a limit point of $P$ . Now here's the proof Rudin gives: Since $P$ has limit points, $P$ must be infinite. Suppose $P$ is countable, and denote the points of $P$ by $x_1, x_2, x_3, \ldots$ . We shall construct a sequence $\{V_n\}$ of neighborhoods as follows: Let $V_1$ be any neighborhood of $x_1$ . If $V_1$ consists of all $y \in \mathbb{R}^k$ such that $\vert y - x_1 \vert < r$ , the closure $\overline{V_1}$ of $V_1$ is the set of all $y \in \mathbb{R}^k$ such that $\vert y - x_1 \vert \leq r$ . Supose $V_n$ has been constructed, so that $V_n \cap P$ is not empty. Since every point of $P$ is a limit point of $P$ , there is a neighborhood $V_{n+1}$ such that (i) $\overline{V_{n+1}} \subset V_n$ , (ii) $x_n \not\in \overline{V_{n+1}}$ , (iii) $V_{n+1} \cap P$ is not empty. By (iii), $V_{n+1}$ satisfies our induction hypothesis, and the construction can proceed. Put $K_n = \overline{V_n} \cap P$ . Since $\overline{V_n}$ is closed and bounded, $\overline{V_n}$ is compact. Since $x_n \not\in K_{n+1}$ , no point of $P$ lies in $\cap_1^\infty K_n$ . Since $K_n \subset P$ , this implies that $\cap_1^\infty K_n$ is empty. But each $K_n$ is non-empty, by (iii), and $K_n \supset K_{n+1}$ , by (i); this contradicts the Corollary to Theorem 2.36. Finally, here's Theorem 2.36: If $\{K_\alpha\}$ is a collection of compact subsets of a metric space $X$ such that the intersection of every finite subcollection of $\{K_\alpha\}$ is nonempty, then $\cap K_\alpha$ is nonempty. And, here's the Corollary to Theorem 2.36: If $\{K_n\}$ is a sequence of nonempty compact sets such that $K_n \supset K_{n+1}$ ( $n=1, 2, 3, \ldots$ ), then $\cap_1^\infty K_n$ is not empty. Now my question is, what exactly is the induction hypothesis that Rudin refers to in the proof? For $V_1$ , he only says that it is any neighborhood of the point $x_1$ . What is the induction hypothesis? Please also proceed from $V_1$ to $V_2$ , and then from $V_2$ to $V_3$ for the sake of illustration. After reading David's comment, I've modified my proof as follows: Since $P$ is nonempty, it has a point $a$ , which is also a limit point of $P$ since $P$ is perfect. But no finite set in a metric space can have a limit point. So $P$ must be infinite. Suppose that $P$ is countable, and let's denote the points of $P$ by $x_1, x_2, x_3, \ldots$ . We shall construct a sequence $V_n$ of neighborhoods such that, for each $n \in \mathbb{N}$ , the intersection $V_n \cap P$ is non-empty and $x_n \not\in V_{n+1}$ . Let $V_1$ be any neighborhood of $x_1$ . Then we can show that if $$V_1  = \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - x_1 \vert < \epsilon_1 \ \right\},$$ where $\epsilon_1$ is some positive real number, then the closure $\overline{V_1}$ of $V_1$ is given by $$ \overline{V_1} = \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - x_1 \vert  \leq \epsilon_1 \ \right\}.$$ Since $x_1$ is a limit point of set $P$ , the nieghborhood $V_1$ of $x_1$ contains a point, say, $y_1$ of $P$ other than the point $x_1$ itself. Now $y_1 \in V_1$ and $V_1$ is an open set in the metric space $\mathbb{R}^k$ . So there is a positive real number $\delta_1$ such that $N_{\delta_1} (y_1) \subset V_1$ , where $$N_{\delta_1} (y_1)  \colon= \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - y_1 \vert  < \delta_1 \ \right\}.$$ Let $$\epsilon_2 \colon= \frac 1 2 \min \left( \delta_1, \vert x_1 - y_1 \vert \right).$$ Then $\epsilon_2 > 0$ .   Let $$V_2 \colon= \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - y_1 \vert  < \epsilon_2 \ \right\}.$$ Then $$\overline{V_2} = \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - y_1 \vert  \leq  \epsilon_2 \ \right\}.$$ Thus, $V_2$ is a neighborhood such that $ y_1 \in V_2 \cap P$ so that $V_2 \cap P$ is non-empty, $\overline{V_2} \subset V_1$ , but $x_1 \not\in \overline{V_2}$ . If $ y_1  \not= x_2$ ,  then let $$\epsilon_3 \colon= \frac 1 2 \min \left( \epsilon_2 , \vert p - x_2 \vert \right).$$ Then $\epsilon_3 > 0$ .   Let $$V_3 \colon= \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - y_1 \vert < \epsilon_3 \ \right\}.$$ Then $$\overline{V_3} = \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - y_1 \vert \leq  \epsilon_3 \ \right\}.$$ Thus, $y_1 \in V_3 \cap P$ , and so $V_3 \cap P$ is non-empty; moreover, $\overline{V_3} \subset V_2$ and $x_2 \not\in \overline{V_3}$ . On the other hand, if $y_1 = x_2$ , then since $y_1$ is a limit point of set $P$ , and $V_2$ is a neighborhood of $y_1$ , this neighborhood $V_2$ contains a point $y_2$ , say, of $P$ other than the point $y_1 = x_2$ itself. Now as $y_2 \in V_2$ and $V_2$ is an open set in $\mathbb{R}^k$ , so there is some positive real number $\delta_2  > 0$ such that $$N_{\delta_2 } (y_2) \subset V_2.$$ So if we take $$\epsilon_3 \colon= \frac 1 2 \min \left( \delta_2 - \vert y_2 - x_2 \vert , \  \vert y_2 - x_2 \vert \right),$$ then $\epsilon_3 > 0$ . Let $$V_3 \colon= \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - y_2 \vert < \epsilon_3 \ \right\}.$$ Then $$\overline{V_3} \colon= \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - y_2 \vert \leq  \epsilon_3 \ \right\}.$$ Thus, $y_2 \in V_3 \cap P$ and so $V_3 \cap P$ is non-empty; moreover, $\overline{V_3} \subset V_2$ , and $x_2 \not\in V_3$ . Thus, in either case we have obtained a neighborhood $V_3$ such that $V_3 \cap P$ is non-empty, $\overline{V_3} \subset V_2$ , but $x_2 \not\in V_3$ . The step from $V_2$ to $V_3$ is redundant in the formal  presentation of the proof, but this step perhaps more vividly illustrates how to proceed. Now suppose that a neighborhood $V_n$ ( $n= 3, 4, 5, \ldots$ ) has been constructed such that $x_{n-1} \not\in \overline{V_n}$ , $\overline{V_n} \subset V_{n-1}$ , and $V_n \cap P$ is non-empty.   Let $$V_n \colon= \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - p \vert < \epsilon  \ \right\},$$ where $\epsilon$ is some positive real number and $p$ is some point in $\mathbb{R}^k$ . We now construct $V_{n+1}$ such that $V_{n+1} \cap P$ is non-empty, $\overline{V_{n+1}} \subset V_n$ , and $x_n \not\in \overline{V_{n+1}}$ . Suppose that $q \in V_n \cap P$ . As $q \in V_n$ and $V_n$ is open, there is some positive real number $\delta_n$ such that $$N_{\delta_n} (q) \subset V_n,$$ where $$N_{\delta_n} (q) \colon= \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - q \vert < \delta_n \ \right\}.$$ If $q \not= x_n$ , then let's take $$\epsilon_{n+1} \colon= \frac 1 2 \min \left( \delta_n , \vert q - x_n \vert \right).$$ So $\epsilon_{n+1} > 0$ , and let $$V_{n+1} \colon= \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - q \vert < \epsilon_{n+1} \ \right\}.$$ Then $$\overline{V_{n+1}} \colon= \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - q \vert \leq  \epsilon_{n+1} \ \right\}.$$ Thus, $V_{n+1}$ is a neighborhood such that $q \in V_{n+1} \cap P$ so that $V_{n+1} \cap P$ is non-empty, $\overline{V_{n+1}} \subset V_n$ , and $x_n \not\in \overline{V_{n+1}}$ . On the contrary, if $q = x_n$ , then since $q$ is a limit point of $P$ , the neighborhood $N_{\delta_n} (q)$ contains a point $b$ , say, of $P$ other than the point $q = x_n$ itself. Now if we take $$\epsilon_{n+1} \colon= \frac 1 2 \min \left( \delta_n - \vert b - x_n \vert, \  \vert b - x_n \vert \right),$$ then $\epsilon_{n+1} > 0$ . Let $$V_{n+1} \colon= \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - b \vert < \epsilon_{n+1} \ \right\}.$$ Then $$\overline{V_{n+1}} = \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - b \vert = \epsilon_{n+1} \ \right\}.$$ So, $b \in V_{n+1} \cap P$ , which implies that $V_{n+1} \cap P$ is non-empty, $\overline{V_{n+1}} \subset V_n$ , and $x_n \not\in \overline{V_{n+1}}$ . Thus, from $V_n$ , in either case, we have constructed  a neighborhood $V_{n+1}$ such that $V_{n+1} \cap P$ is non-empty, $\overline{V_{n+1}} \subset V_n$ , but $x_n \not\in \overline{V_{n+1}}$ . Thus, we have inductively obtained a sequence $\{V_n\}_{n \in \mathbb{N}}$ of neighborhoods such that, for each $n \in \mathbb{N}$ , the intersection $V_n \cap P$ is non-empty, $\overline{V_{n+1}} \subset V_n$ , and $x_n \not\in \overline{V_{n+1}}$ . Put $K_n \colon= \overline{V_n} \cap P$ for each $n \in \mathbb{N}$ . Let $n \in \mathbb{N}$ be arbitrary. Now $\overline{V_n}$ , being a closed and bounded subset of $\mathbb{R}^k$ , is compact. Moreover, as both $P$ and $\overline{V_n}$ are closed, so is $K_n$ . Thus, $K_n$ , being a closed subset of the compact set $\overline{V_n}$ , is also compact. Since $V_n \cap P$ is non-empty, so $K_n$ is also non-empty. Moreover, as $$\overline{V_{n+1}} \subset V_n \subset \overline{V_n},$$ so we have $K_{n+1} \subset K_n$ . Finally, as $$P = \left\{ \ x_1, x_2, x_3, \ldots \ \right\}$$ and as $x_n \not\in \overline{V_{n+1}}$ , so $x_n \not\in K_{n+1}$ and hence $$x_n \not\in  K_1 \cap K_2 \cap K_3 \cap \ldots,$$ which implies that this intersection is empty. Let $m_1, m_2, \ldots, m_r \in \mathbb{N}$ , and let's take $$m \colon= \max \left( m_1, \ldots, m_r \right).$$ Then $$K_{m_1} \cap \ldots \cap K_{m_r} = K_m,$$ which is non-empty. Thus, $\{K_n\}_{n\in\mathbb{N}}$ is a sequence of non-empty compact sets in the metric space $\mathbb{R}^k$ such that the intersection of any finitely many of these sets is non-empty but the intersection $\cap_{n \in \mathbb{N}} K_n$ is empty. But this cannot hold in any metric space, by Theorem 2.36 in Rudin. So our suppose that $P$ is a non-empty perfect set in $\mathbb{R}^k$ and $P$ is also countable is wrong. Hence Theorem 2.43 in Rudin holds. Is the above proof correct? If so, is the presentation clear enough (or any clearer than Rudin's presentation)? Where does this proof need improvement?","Here's Theorem 2.43 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Let be a non-empty perfect set in . Then is uncountable. Here's the definition of a perfect set: Let be a metric space, and let . Then is perfect if it is closed (i.e. it contains all of its limit points) and every point of is also a limit point of . Now here's the proof Rudin gives: Since has limit points, must be infinite. Suppose is countable, and denote the points of by . We shall construct a sequence of neighborhoods as follows: Let be any neighborhood of . If consists of all such that , the closure of is the set of all such that . Supose has been constructed, so that is not empty. Since every point of is a limit point of , there is a neighborhood such that (i) , (ii) , (iii) is not empty. By (iii), satisfies our induction hypothesis, and the construction can proceed. Put . Since is closed and bounded, is compact. Since , no point of lies in . Since , this implies that is empty. But each is non-empty, by (iii), and , by (i); this contradicts the Corollary to Theorem 2.36. Finally, here's Theorem 2.36: If is a collection of compact subsets of a metric space such that the intersection of every finite subcollection of is nonempty, then is nonempty. And, here's the Corollary to Theorem 2.36: If is a sequence of nonempty compact sets such that ( ), then is not empty. Now my question is, what exactly is the induction hypothesis that Rudin refers to in the proof? For , he only says that it is any neighborhood of the point . What is the induction hypothesis? Please also proceed from to , and then from to for the sake of illustration. After reading David's comment, I've modified my proof as follows: Since is nonempty, it has a point , which is also a limit point of since is perfect. But no finite set in a metric space can have a limit point. So must be infinite. Suppose that is countable, and let's denote the points of by . We shall construct a sequence of neighborhoods such that, for each , the intersection is non-empty and . Let be any neighborhood of . Then we can show that if where is some positive real number, then the closure of is given by Since is a limit point of set , the nieghborhood of contains a point, say, of other than the point itself. Now and is an open set in the metric space . So there is a positive real number such that , where Let Then .   Let Then Thus, is a neighborhood such that so that is non-empty, , but . If ,  then let Then .   Let Then Thus, , and so is non-empty; moreover, and . On the other hand, if , then since is a limit point of set , and is a neighborhood of , this neighborhood contains a point , say, of other than the point itself. Now as and is an open set in , so there is some positive real number such that So if we take then . Let Then Thus, and so is non-empty; moreover, , and . Thus, in either case we have obtained a neighborhood such that is non-empty, , but . The step from to is redundant in the formal  presentation of the proof, but this step perhaps more vividly illustrates how to proceed. Now suppose that a neighborhood ( ) has been constructed such that , , and is non-empty.   Let where is some positive real number and is some point in . We now construct such that is non-empty, , and . Suppose that . As and is open, there is some positive real number such that where If , then let's take So , and let Then Thus, is a neighborhood such that so that is non-empty, , and . On the contrary, if , then since is a limit point of , the neighborhood contains a point , say, of other than the point itself. Now if we take then . Let Then So, , which implies that is non-empty, , and . Thus, from , in either case, we have constructed  a neighborhood such that is non-empty, , but . Thus, we have inductively obtained a sequence of neighborhoods such that, for each , the intersection is non-empty, , and . Put for each . Let be arbitrary. Now , being a closed and bounded subset of , is compact. Moreover, as both and are closed, so is . Thus, , being a closed subset of the compact set , is also compact. Since is non-empty, so is also non-empty. Moreover, as so we have . Finally, as and as , so and hence which implies that this intersection is empty. Let , and let's take Then which is non-empty. Thus, is a sequence of non-empty compact sets in the metric space such that the intersection of any finitely many of these sets is non-empty but the intersection is empty. But this cannot hold in any metric space, by Theorem 2.36 in Rudin. So our suppose that is a non-empty perfect set in and is also countable is wrong. Hence Theorem 2.43 in Rudin holds. Is the above proof correct? If so, is the presentation clear enough (or any clearer than Rudin's presentation)? Where does this proof need improvement?","P \mathbb{R}^k P (X,d) P \subset X P P P P P P P x_1, x_2, x_3, \ldots \{V_n\} V_1 x_1 V_1 y \in \mathbb{R}^k \vert y - x_1 \vert < r \overline{V_1} V_1 y \in \mathbb{R}^k \vert y - x_1 \vert \leq r V_n V_n \cap P P P V_{n+1} \overline{V_{n+1}} \subset V_n x_n \not\in \overline{V_{n+1}} V_{n+1} \cap P V_{n+1} K_n = \overline{V_n} \cap P \overline{V_n} \overline{V_n} x_n \not\in K_{n+1} P \cap_1^\infty K_n K_n \subset P \cap_1^\infty K_n K_n K_n \supset K_{n+1} \{K_\alpha\} X \{K_\alpha\} \cap K_\alpha \{K_n\} K_n \supset K_{n+1} n=1, 2, 3, \ldots \cap_1^\infty K_n V_1 x_1 V_1 V_2 V_2 V_3 P a P P P P P x_1, x_2, x_3, \ldots V_n n \in \mathbb{N} V_n \cap P x_n \not\in V_{n+1} V_1 x_1 V_1  = \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - x_1 \vert < \epsilon_1 \ \right\}, \epsilon_1 \overline{V_1} V_1  \overline{V_1} = \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - x_1 \vert  \leq \epsilon_1 \ \right\}. x_1 P V_1 x_1 y_1 P x_1 y_1 \in V_1 V_1 \mathbb{R}^k \delta_1 N_{\delta_1} (y_1) \subset V_1 N_{\delta_1} (y_1)  \colon= \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - y_1 \vert  < \delta_1 \ \right\}. \epsilon_2 \colon= \frac 1 2 \min \left( \delta_1, \vert x_1 - y_1 \vert \right). \epsilon_2 > 0 V_2 \colon= \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - y_1 \vert  < \epsilon_2 \ \right\}. \overline{V_2} = \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - y_1 \vert  \leq  \epsilon_2 \ \right\}. V_2  y_1 \in V_2 \cap P V_2 \cap P \overline{V_2} \subset V_1 x_1 \not\in \overline{V_2}  y_1  \not= x_2 \epsilon_3 \colon= \frac 1 2 \min \left( \epsilon_2 , \vert p - x_2 \vert \right). \epsilon_3 > 0 V_3 \colon= \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - y_1 \vert < \epsilon_3 \ \right\}. \overline{V_3} = \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - y_1 \vert \leq  \epsilon_3 \ \right\}. y_1 \in V_3 \cap P V_3 \cap P \overline{V_3} \subset V_2 x_2 \not\in \overline{V_3} y_1 = x_2 y_1 P V_2 y_1 V_2 y_2 P y_1 = x_2 y_2 \in V_2 V_2 \mathbb{R}^k \delta_2  > 0 N_{\delta_2 } (y_2) \subset V_2. \epsilon_3 \colon= \frac 1 2 \min \left( \delta_2 - \vert y_2 - x_2 \vert , \  \vert y_2 - x_2 \vert \right), \epsilon_3 > 0 V_3 \colon= \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - y_2 \vert < \epsilon_3 \ \right\}. \overline{V_3} \colon= \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - y_2 \vert \leq  \epsilon_3 \ \right\}. y_2 \in V_3 \cap P V_3 \cap P \overline{V_3} \subset V_2 x_2 \not\in V_3 V_3 V_3 \cap P \overline{V_3} \subset V_2 x_2 \not\in V_3 V_2 V_3 V_n n= 3, 4, 5, \ldots x_{n-1} \not\in \overline{V_n} \overline{V_n} \subset V_{n-1} V_n \cap P V_n \colon= \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - p \vert < \epsilon  \ \right\}, \epsilon p \mathbb{R}^k V_{n+1} V_{n+1} \cap P \overline{V_{n+1}} \subset V_n x_n \not\in \overline{V_{n+1}} q \in V_n \cap P q \in V_n V_n \delta_n N_{\delta_n} (q) \subset V_n, N_{\delta_n} (q) \colon= \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - q \vert < \delta_n \ \right\}. q \not= x_n \epsilon_{n+1} \colon= \frac 1 2 \min \left( \delta_n , \vert q - x_n \vert \right). \epsilon_{n+1} > 0 V_{n+1} \colon= \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - q \vert < \epsilon_{n+1} \ \right\}. \overline{V_{n+1}} \colon= \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - q \vert \leq  \epsilon_{n+1} \ \right\}. V_{n+1} q \in V_{n+1} \cap P V_{n+1} \cap P \overline{V_{n+1}} \subset V_n x_n \not\in \overline{V_{n+1}} q = x_n q P N_{\delta_n} (q) b P q = x_n \epsilon_{n+1} \colon= \frac 1 2 \min \left( \delta_n - \vert b - x_n \vert, \  \vert b - x_n \vert \right), \epsilon_{n+1} > 0 V_{n+1} \colon= \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - b \vert < \epsilon_{n+1} \ \right\}. \overline{V_{n+1}} = \left\{ \ y \in \mathbb{R}^k \ \colon \ \vert y - b \vert = \epsilon_{n+1} \ \right\}. b \in V_{n+1} \cap P V_{n+1} \cap P \overline{V_{n+1}} \subset V_n x_n \not\in \overline{V_{n+1}} V_n V_{n+1} V_{n+1} \cap P \overline{V_{n+1}} \subset V_n x_n \not\in \overline{V_{n+1}} \{V_n\}_{n \in \mathbb{N}} n \in \mathbb{N} V_n \cap P \overline{V_{n+1}} \subset V_n x_n \not\in \overline{V_{n+1}} K_n \colon= \overline{V_n} \cap P n \in \mathbb{N} n \in \mathbb{N} \overline{V_n} \mathbb{R}^k P \overline{V_n} K_n K_n \overline{V_n} V_n \cap P K_n \overline{V_{n+1}} \subset V_n \subset \overline{V_n}, K_{n+1} \subset K_n P = \left\{ \ x_1, x_2, x_3, \ldots \ \right\} x_n \not\in \overline{V_{n+1}} x_n \not\in K_{n+1} x_n \not\in  K_1 \cap K_2 \cap K_3 \cap \ldots, m_1, m_2, \ldots, m_r \in \mathbb{N} m \colon= \max \left( m_1, \ldots, m_r \right). K_{m_1} \cap \ldots \cap K_{m_r} = K_m, \{K_n\}_{n\in\mathbb{N}} \mathbb{R}^k \cap_{n \in \mathbb{N}} K_n P \mathbb{R}^k P","['real-analysis', 'analysis']"
30,countable or uncountable sets NBHM 2016,countable or uncountable sets NBHM 2016,,The set of all algebraic numbers the set of all strictly increasing infinite sequences of positive integers the set of all infinite sequences of integers which are in arithmetic progression . I know 1 is definitely countable set.  and i am not sure about 2 and 3 but it seems uncountable as they are infinite sequences. Thanks in advance,The set of all algebraic numbers the set of all strictly increasing infinite sequences of positive integers the set of all infinite sequences of integers which are in arithmetic progression . I know 1 is definitely countable set.  and i am not sure about 2 and 3 but it seems uncountable as they are infinite sequences. Thanks in advance,,['real-analysis']
31,Prove that the product of two continuous functions is continuous,Prove that the product of two continuous functions is continuous,,"I was trying to prove an important theorem concerning continuous functions, namely that the product of two continuous functions is continuous. I am nearly at the end of the proof but I do not understand the last step my teacher made... I'm sure some of you is able to help me out :) Thanks in advance! Theorem Let $f, g\colon \Bbb R \to\Bbb R $ be continuous. Then $F\colon \Bbb R\to\Bbb R$ defined by $F(x) = f(x)g(x)$ is continuous. Proof Let $f, g\colon \Bbb R \to\Bbb R $ be given such that $f$ and $g$ are continuous. Let $F\colon \Bbb R\to\Bbb R$ be defined by $F(x) = f(x)g(x)$. We want to show: $F$ is continuous, that is, for all $a \in\Bbb R$, for every $\epsilon > 0$, there exists some $\delta > 0$ such that for all $x \in \Bbb R$ with $|x - a| < \delta$, $|F(x)-F(a)| < \epsilon$. Let $a \in\Bbb R$ be given. Let $\epsilon > 0$ be given. Let $\delta_f > 0$ be such that for all $x \in\Bbb R$ with $|x-a| < \delta_f$, $|f(x)-f(a)| < \frac\epsilon{2(|g(a)| + \epsilon)}$. Such a $\delta_f$ exists since $f$ is continuous. Let $\delta_g > 0$ be such that for all $x \in\Bbb R$ with $|x-a| < \delta_g$, $|g(x)-g(a)| < \frac\epsilon{ 2|f(a)| + 1}$. Such a $\delta_g$ exists since $g$ is continuous. Let $\delta_3 > 0$ be such that for all $x\in\Bbb R$ with $|x-a| < \delta_3$, $|g(x)-g(a)| < \epsilon$, which implies $|g(x)| < |g(a)| + \epsilon$. Such a $\delta_3$ exists since $g$ is continuous. Choose $\delta = \min\{\delta_f, \delta_g, \delta_3\}$. Then for all $x \in\Bbb R$ with $|x-a| < \delta$:  $$\begin{align}|F(x) - F(a)| &= |f(x)g(x) - f(a)g(a)| \\ &= |f(x)g(x) - f(a)g(x) + f(a)g(x) - f(a)g(a)| \\ &\le |f(x)g(x) - f(a)g(x)| + |f(a)g(x) - f(a)g(a)|\\ &= |(g(x)[f(x) - f(a)]| + |f(a)[g(x) - g(a)]|\\&= |g(x)| * |f(x)-f(a)| + |f(a)| * |g(x) - g(a)| \\ &< [|g(x)| + \epsilon] \cdot \frac \epsilon {2(|g(a)| + \epsilon)} + |f(a)| \cdot |g(x) - g(a)| \\ &= \frac\epsilon{2 + |f(a)| \cdot |g(x) - g(a)|}\end{align}$$ Problem Now we need $|f(a)| \cdot |g(x) - g(a)|$ to be equal to $\frac \epsilon 2$ so that we have $\frac\epsilon2+\frac\epsilon 2= \epsilon$ (which completes the proof). The problem is I do not see why/how $|f(a)| \cdot |g(x) - g(a)|$ is equal to $\frac\epsilon2$. Or is there maybe an error somewhere?","I was trying to prove an important theorem concerning continuous functions, namely that the product of two continuous functions is continuous. I am nearly at the end of the proof but I do not understand the last step my teacher made... I'm sure some of you is able to help me out :) Thanks in advance! Theorem Let $f, g\colon \Bbb R \to\Bbb R $ be continuous. Then $F\colon \Bbb R\to\Bbb R$ defined by $F(x) = f(x)g(x)$ is continuous. Proof Let $f, g\colon \Bbb R \to\Bbb R $ be given such that $f$ and $g$ are continuous. Let $F\colon \Bbb R\to\Bbb R$ be defined by $F(x) = f(x)g(x)$. We want to show: $F$ is continuous, that is, for all $a \in\Bbb R$, for every $\epsilon > 0$, there exists some $\delta > 0$ such that for all $x \in \Bbb R$ with $|x - a| < \delta$, $|F(x)-F(a)| < \epsilon$. Let $a \in\Bbb R$ be given. Let $\epsilon > 0$ be given. Let $\delta_f > 0$ be such that for all $x \in\Bbb R$ with $|x-a| < \delta_f$, $|f(x)-f(a)| < \frac\epsilon{2(|g(a)| + \epsilon)}$. Such a $\delta_f$ exists since $f$ is continuous. Let $\delta_g > 0$ be such that for all $x \in\Bbb R$ with $|x-a| < \delta_g$, $|g(x)-g(a)| < \frac\epsilon{ 2|f(a)| + 1}$. Such a $\delta_g$ exists since $g$ is continuous. Let $\delta_3 > 0$ be such that for all $x\in\Bbb R$ with $|x-a| < \delta_3$, $|g(x)-g(a)| < \epsilon$, which implies $|g(x)| < |g(a)| + \epsilon$. Such a $\delta_3$ exists since $g$ is continuous. Choose $\delta = \min\{\delta_f, \delta_g, \delta_3\}$. Then for all $x \in\Bbb R$ with $|x-a| < \delta$:  $$\begin{align}|F(x) - F(a)| &= |f(x)g(x) - f(a)g(a)| \\ &= |f(x)g(x) - f(a)g(x) + f(a)g(x) - f(a)g(a)| \\ &\le |f(x)g(x) - f(a)g(x)| + |f(a)g(x) - f(a)g(a)|\\ &= |(g(x)[f(x) - f(a)]| + |f(a)[g(x) - g(a)]|\\&= |g(x)| * |f(x)-f(a)| + |f(a)| * |g(x) - g(a)| \\ &< [|g(x)| + \epsilon] \cdot \frac \epsilon {2(|g(a)| + \epsilon)} + |f(a)| \cdot |g(x) - g(a)| \\ &= \frac\epsilon{2 + |f(a)| \cdot |g(x) - g(a)|}\end{align}$$ Problem Now we need $|f(a)| \cdot |g(x) - g(a)|$ to be equal to $\frac \epsilon 2$ so that we have $\frac\epsilon2+\frac\epsilon 2= \epsilon$ (which completes the proof). The problem is I do not see why/how $|f(a)| \cdot |g(x) - g(a)|$ is equal to $\frac\epsilon2$. Or is there maybe an error somewhere?",,"['real-analysis', 'continuity', 'epsilon-delta']"
32,Prove that $f(x)=0$ for all $x\in\mathbb{R}$,Prove that  for all,f(x)=0 x\in\mathbb{R},"The problem at which I am currently stuck is, Let $f:\mathbb{R}\to\mathbb{R}$ be a continuous function such that $f(m+n\sqrt{2})=0$ for all $m,n\in\mathbb{Z}$. Prove that $f(x)=0$ for all $x\in\mathbb{R}$. I have noted that to solve this problem what I need to show is that the set $\{m+n\sqrt{2}\mid m,n\in\mathbb{Z}\}$ is dense in $\mathbb{R}$ but I can't prove it. Can anyone help me?","The problem at which I am currently stuck is, Let $f:\mathbb{R}\to\mathbb{R}$ be a continuous function such that $f(m+n\sqrt{2})=0$ for all $m,n\in\mathbb{Z}$. Prove that $f(x)=0$ for all $x\in\mathbb{R}$. I have noted that to solve this problem what I need to show is that the set $\{m+n\sqrt{2}\mid m,n\in\mathbb{Z}\}$ is dense in $\mathbb{R}$ but I can't prove it. Can anyone help me?",,[]
33,$\int_{0}^{a} f(t) dt +\int_{0}^{b} f^{-1}(t)dt \geq ab$,,\int_{0}^{a} f(t) dt +\int_{0}^{b} f^{-1}(t)dt \geq ab,"Want to prove $$\int_{0}^{a} f(t) dt +\int_{0}^{b} f^{-1}(t)dt \geq ab$$ where $f$ is continuous, strictly increasing and $f(0)=0$, $a>0$ and $b\in f((0,\infty))$. I know how to do the question if $f$ is differentiable, which uses integration by substitution. But when the condition is changed to continuous, I cannot even think of a direction. Thank you.","Want to prove $$\int_{0}^{a} f(t) dt +\int_{0}^{b} f^{-1}(t)dt \geq ab$$ where $f$ is continuous, strictly increasing and $f(0)=0$, $a>0$ and $b\in f((0,\infty))$. I know how to do the question if $f$ is differentiable, which uses integration by substitution. But when the condition is changed to continuous, I cannot even think of a direction. Thank you.",,"['calculus', 'real-analysis']"
34,Name for Theorem 3.27 from baby Rudin?,Name for Theorem 3.27 from baby Rudin?,,"Rudin rarely gives names to the theorems in this book.  Theorem 3.27 states if $\{a_n\}$ is a monotonically decreasing sequence of  positive reals, then $$\sum_{n=1}^\infty a_n\,\text{ converges}\iff\sum_{k=0}^\infty 2^ka_{2^k} \text{ converges}.$$","Rudin rarely gives names to the theorems in this book.  Theorem 3.27 states if $\{a_n\}$ is a monotonically decreasing sequence of  positive reals, then $$\sum_{n=1}^\infty a_n\,\text{ converges}\iff\sum_{k=0}^\infty 2^ka_{2^k} \text{ converges}.$$",,"['real-analysis', 'terminology']"
35,"A version of the first mean value theorem for integrals, proof","A version of the first mean value theorem for integrals, proof",,"I am asked to prove a version of the first mean value theorem for integrals: Let $f$ and $g$ be defined on $[a,b]$ with $g$ continuous, $f\ge 0$ , and $f$ integrable. Then there is a point $x_0 \in (a,b)$ such that $$ \int_{a}^{b}f(x)g(x)dx = g(x_0) \int_{a}^{b}f(x)dx. $$ My (incomplete) proof goes as follows: Since $ [a,b] $ is compact and $ g $ is continuous, we know there exist $ x_1, x_2 \in [a,b] $ such that $ g(x_1) \le g(x) \le g(x_2) $ for any $ x \in [a,b] $ . Thus, we have the following string of inequalities $$ f(x)g(x_1) \le f(x)g(x) \le f(x)g(x_2), \, \, \forall x \in [a,b], $$ $$ \Rightarrow g(x_1)\int_{a}^{b} f(x)dx \,\le \, \int_{a}^{b} f(x)g(x) dx \, \le  \, g(x_2)\int_{a}^{b}f(x)dx. $$ My problem now is to show the final equality. It seems to me the intermediate value theorem is somehow involved, but I cannot see how to use it. Furthermore, how do I make use of the $f\ge 0$ condition in the hypothesis?","I am asked to prove a version of the first mean value theorem for integrals: Let and be defined on with continuous, , and integrable. Then there is a point such that My (incomplete) proof goes as follows: Since is compact and is continuous, we know there exist such that for any . Thus, we have the following string of inequalities My problem now is to show the final equality. It seems to me the intermediate value theorem is somehow involved, but I cannot see how to use it. Furthermore, how do I make use of the condition in the hypothesis?","f g [a,b] g f\ge 0 f x_0 \in (a,b) 
\int_{a}^{b}f(x)g(x)dx = g(x_0) \int_{a}^{b}f(x)dx.
  [a,b]   g   x_1, x_2 \in [a,b]   g(x_1) \le g(x) \le g(x_2)   x \in [a,b]  
f(x)g(x_1) \le f(x)g(x) \le f(x)g(x_2), \, \, \forall x \in [a,b],
 
\Rightarrow g(x_1)\int_{a}^{b} f(x)dx \,\le \, \int_{a}^{b} f(x)g(x) dx \, \le  \, g(x_2)\int_{a}^{b}f(x)dx.
 f\ge 0","['real-analysis', 'integration']"
36,Find the closed form of $\sum_{k=1}^{\infty} \sum_{n=1}^{\infty} \frac{H_n}{kn (k+n)^3}$,Find the closed form of,\sum_{k=1}^{\infty} \sum_{n=1}^{\infty} \frac{H_n}{kn (k+n)^3},"Here is a challenging double series question I wanna share with you $$\sum_{k=1}^{\infty} \sum_{n=1}^{\infty} \frac{H_n}{kn (k+n)^3}$$ together with the question: what tools would you like  employ for computing it? EDIT: thank you, no need for further work. I know how to do it now, but it's a long way.","Here is a challenging double series question I wanna share with you $$\sum_{k=1}^{\infty} \sum_{n=1}^{\infty} \frac{H_n}{kn (k+n)^3}$$ together with the question: what tools would you like  employ for computing it? EDIT: thank you, no need for further work. I know how to do it now, but it's a long way.",,"['calculus', 'real-analysis', 'sequences-and-series', 'harmonic-numbers']"
37,Completeness of the space of sets with distance defined by the measure of symmetric difference,Completeness of the space of sets with distance defined by the measure of symmetric difference,,"Let $m$ be the measure defined on the set semiring $\mathfrak{S}_m$ and $m'$ its extension to the minimal ring $\mathfrak{R}(\mathfrak{S}_m)$. I read that $m'(A\triangle B)$ can be used as a distance in $\mathfrak{R}(\mathfrak{S}_m)$. I would rather say that it can be used as a distance in the quotient $\mathfrak{R}(\mathfrak{S}_m)/\sim$ where we identify $A\sim A'$ when $m'(A\triangle A')=0$ [ EDIT Mar 7'15: which is an equivalence relation, I would say, thanking future answerers for confirming or refuting this]: am I right? I also read , in Kolmogorov-Fomin's Elements of theory of functions and functional analysis , that, with such a metric, the space $\mathfrak{M}$ of Lebesgue measurable sets, where $A$ and $A'$ are identified when $\mu(A\triangle A')=0$,  is complete, but I cannot see why, if $\{A_n\}\subset\mathfrak{M}$ is a sequence such that$$\forall\varepsilon>0\quad\exists N\in\mathbb{N}^+:\forall n,m\geq N\quad\mu(A_n\triangle A_m)<\varepsilon$$then a measurable $A$ must exists such that $\forall\varepsilon>0\quad\exists N\in\mathbb{N}^+:\forall n\geq N\quad\mu(A_n\triangle A)<\varepsilon$. EDIT: Care Bear, whom I thank very much, has provided a proof for the case that $\bigcup_n A_n\in\mathfrak{M}$. According to Kolmogorov-Fomin's, $\bigcup_{n=1}^\infty A_n$ is Lebesgue-measurable only under the restrictive condition that $\exists K\geq 0:\forall N\in\mathbb{N}^+\quad\mu(\bigcup_{n=1}^N A_n)\leq K$ (problem 5 (b) here ). Can anybody explain whether the lemma is valid in any case? I $\infty$-ly thank you all for any answer!!!","Let $m$ be the measure defined on the set semiring $\mathfrak{S}_m$ and $m'$ its extension to the minimal ring $\mathfrak{R}(\mathfrak{S}_m)$. I read that $m'(A\triangle B)$ can be used as a distance in $\mathfrak{R}(\mathfrak{S}_m)$. I would rather say that it can be used as a distance in the quotient $\mathfrak{R}(\mathfrak{S}_m)/\sim$ where we identify $A\sim A'$ when $m'(A\triangle A')=0$ [ EDIT Mar 7'15: which is an equivalence relation, I would say, thanking future answerers for confirming or refuting this]: am I right? I also read , in Kolmogorov-Fomin's Elements of theory of functions and functional analysis , that, with such a metric, the space $\mathfrak{M}$ of Lebesgue measurable sets, where $A$ and $A'$ are identified when $\mu(A\triangle A')=0$,  is complete, but I cannot see why, if $\{A_n\}\subset\mathfrak{M}$ is a sequence such that$$\forall\varepsilon>0\quad\exists N\in\mathbb{N}^+:\forall n,m\geq N\quad\mu(A_n\triangle A_m)<\varepsilon$$then a measurable $A$ must exists such that $\forall\varepsilon>0\quad\exists N\in\mathbb{N}^+:\forall n\geq N\quad\mu(A_n\triangle A)<\varepsilon$. EDIT: Care Bear, whom I thank very much, has provided a proof for the case that $\bigcup_n A_n\in\mathfrak{M}$. According to Kolmogorov-Fomin's, $\bigcup_{n=1}^\infty A_n$ is Lebesgue-measurable only under the restrictive condition that $\exists K\geq 0:\forall N\in\mathbb{N}^+\quad\mu(\bigcup_{n=1}^N A_n)\leq K$ (problem 5 (b) here ). Can anybody explain whether the lemma is valid in any case? I $\infty$-ly thank you all for any answer!!!",,"['real-analysis', 'measure-theory', 'metric-spaces', 'lebesgue-measure']"
38,Is every convex function on an open interval continuous? [duplicate],Is every convex function on an open interval continuous? [duplicate],,"This question already has answers here : Prove that every convex function is continuous (11 answers) Closed 3 years ago . Let $f:(a,b)\rightarrow \mathbb{R}$. $f$ satisfied the following property: If $\forall x_{1},x_{0},x_{2}\in(a,b)$ and $x_{1}<x_{0}<x_{2};$then$\frac{f(x_{0})-f(x_{1})}{x_{0}-x_{1}}\geq \frac{f(x_{2})-f(x_{0})}{x_{2}-x_{0}}.$ My question : whether the function $f\in C((a,b))?$ We can get  $\displaystyle\frac{f(x_{1})-f(x_{0})}{x_{1}-x_{0}}\geq \frac{f(x_{2})-f(x_{0})}{x_{2}-x_{0}}. $Let $\displaystyle g(x)=\frac{f(x)-f(x_{0})}{x-x_{0}},$ if we can prove $g(x)$ is bound and  decreasing (not strictly ) in $(x_{0}-\delta ,x_{0});$ then we have $f^{'}_{-}(x_{0}) $ exists .In a similar way ,$f^{'}_{+}(x_{0}) $ exists.  $f^{'}_{-}(x_{0}) $ exists $\Rightarrow\displaystyle\lim_{x\rightarrow x_{0}-}f(x)=f(x_{0});$ $f^{'}_{+}(x_{0}) $ exists $\Rightarrow\displaystyle\lim_{x\rightarrow x_{0}+}f(x)=f(x_{0}).$  Obviously, $f$ is coutinuous at $x=x_{0}.$Further, $f\in C((a,b))!$ But  I failed to prove  $g(x)$ is bound and  decreasing (not strictly ) in $(x_{0}-\delta ,x_{0}).$Sometimes  I doubt the conculsion that  $f\in C((a,b)).$  Either make a counterexample to deny it ,or prove the conculsion is correct?","This question already has answers here : Prove that every convex function is continuous (11 answers) Closed 3 years ago . Let $f:(a,b)\rightarrow \mathbb{R}$. $f$ satisfied the following property: If $\forall x_{1},x_{0},x_{2}\in(a,b)$ and $x_{1}<x_{0}<x_{2};$then$\frac{f(x_{0})-f(x_{1})}{x_{0}-x_{1}}\geq \frac{f(x_{2})-f(x_{0})}{x_{2}-x_{0}}.$ My question : whether the function $f\in C((a,b))?$ We can get  $\displaystyle\frac{f(x_{1})-f(x_{0})}{x_{1}-x_{0}}\geq \frac{f(x_{2})-f(x_{0})}{x_{2}-x_{0}}. $Let $\displaystyle g(x)=\frac{f(x)-f(x_{0})}{x-x_{0}},$ if we can prove $g(x)$ is bound and  decreasing (not strictly ) in $(x_{0}-\delta ,x_{0});$ then we have $f^{'}_{-}(x_{0}) $ exists .In a similar way ,$f^{'}_{+}(x_{0}) $ exists.  $f^{'}_{-}(x_{0}) $ exists $\Rightarrow\displaystyle\lim_{x\rightarrow x_{0}-}f(x)=f(x_{0});$ $f^{'}_{+}(x_{0}) $ exists $\Rightarrow\displaystyle\lim_{x\rightarrow x_{0}+}f(x)=f(x_{0}).$  Obviously, $f$ is coutinuous at $x=x_{0}.$Further, $f\in C((a,b))!$ But  I failed to prove  $g(x)$ is bound and  decreasing (not strictly ) in $(x_{0}-\delta ,x_{0}).$Sometimes  I doubt the conculsion that  $f\in C((a,b)).$  Either make a counterexample to deny it ,or prove the conculsion is correct?",,"['real-analysis', 'continuity', 'convex-analysis']"
39,Does absolute convergence of a sequence imply convergence?,Does absolute convergence of a sequence imply convergence?,,In my real analysis notes I've got that absolute convergence of a real SERIES implies convergence of the series. However what about absolute convergence of a sequence? Does this imply convergence of the sequence?,In my real analysis notes I've got that absolute convergence of a real SERIES implies convergence of the series. However what about absolute convergence of a sequence? Does this imply convergence of the sequence?,,['real-analysis']
40,Can we find uncountably many disjoint measurable subsets of $\mathbb{R}$ with strictly postive Lebesgue measure?,Can we find uncountably many disjoint measurable subsets of  with strictly postive Lebesgue measure?,\mathbb{R},Can we find uncountably many disjoint measurable subsets of $\mathbb{R}$ with strictly positive Lebesgue measure?,Can we find uncountably many disjoint measurable subsets of $\mathbb{R}$ with strictly positive Lebesgue measure?,,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
41,"Why, intuitively, does $log(x)$ come in as the integral of $1/x$, wheras the integral of other powers of $x$ are powers of $x$? [duplicate]","Why, intuitively, does  come in as the integral of , wheras the integral of other powers of  are powers of ? [duplicate]",log(x) 1/x x x,"This question already has answers here : What is so special about $\alpha=-1$ in the integral of $x^\alpha$? (7 answers) Closed 9 years ago . Question in title really, something I always found strange when I was learning calculus. I can see that $\int \frac{1}{x} dx$ can't be $\frac{x^0}{0}$ since this is not defined, and then the definite integral $\int_1^t \frac{1}{x} dx$ comes down to $$\lim_{\delta \rightarrow 0} \frac{t^{\delta}-1}{\delta} = \log (t).$$ But this understanding just comes from l'Hopital's rule, and also it still just seems really bizaare that the log function should fit into the set of power functions like this.  Can anyone de-mystify this at all?","This question already has answers here : What is so special about $\alpha=-1$ in the integral of $x^\alpha$? (7 answers) Closed 9 years ago . Question in title really, something I always found strange when I was learning calculus. I can see that $\int \frac{1}{x} dx$ can't be $\frac{x^0}{0}$ since this is not defined, and then the definite integral $\int_1^t \frac{1}{x} dx$ comes down to $$\lim_{\delta \rightarrow 0} \frac{t^{\delta}-1}{\delta} = \log (t).$$ But this understanding just comes from l'Hopital's rule, and also it still just seems really bizaare that the log function should fit into the set of power functions like this.  Can anyone de-mystify this at all?",,['real-analysis']
42,"How to find integral $\underbrace{\int\sqrt{2+\sqrt{2+\sqrt{2+\cdots+\sqrt{2+x}}}}}_{n}dx,x>-2$",How to find integral,"\underbrace{\int\sqrt{2+\sqrt{2+\sqrt{2+\cdots+\sqrt{2+x}}}}}_{n}dx,x>-2","Find the integral $$\int\underbrace{\sqrt{2+\sqrt{2+\sqrt{2+\cdots+\sqrt{2+x}}}}}_{n}dx,x>-2$$ where $n$ define  the number of  the square I know  this  if  $0 \le x\le 2$, then let $$x=2\cos{t},0\le t\le\dfrac{\pi}{2}$$ so $$\sqrt{2+x}=\sqrt{2+2\cos{t}}=2\cos{\dfrac{t}{2}}$$ so $$\sqrt{2+\sqrt{2+x}}=2\cos{\dfrac{t}{2^2}}$$ so $$\int\sqrt{2+\sqrt{2+\sqrt{2+\cdots+\sqrt{2+x}}}}dx=\int2\cos{\dfrac{t}{2^n}}(-2\sin{t})dt$$ and for $x\ge 2$ case, I  let $x=\cosh{t}$, but for  $-2\le x\le 0$ case, I can't do it.","Find the integral $$\int\underbrace{\sqrt{2+\sqrt{2+\sqrt{2+\cdots+\sqrt{2+x}}}}}_{n}dx,x>-2$$ where $n$ define  the number of  the square I know  this  if  $0 \le x\le 2$, then let $$x=2\cos{t},0\le t\le\dfrac{\pi}{2}$$ so $$\sqrt{2+x}=\sqrt{2+2\cos{t}}=2\cos{\dfrac{t}{2}}$$ so $$\sqrt{2+\sqrt{2+x}}=2\cos{\dfrac{t}{2^2}}$$ so $$\int\sqrt{2+\sqrt{2+\sqrt{2+\cdots+\sqrt{2+x}}}}dx=\int2\cos{\dfrac{t}{2^n}}(-2\sin{t})dt$$ and for $x\ge 2$ case, I  let $x=\cosh{t}$, but for  $-2\le x\le 0$ case, I can't do it.",,"['real-analysis', 'integration', 'indefinite-integrals']"
43,Is this a proper proof of (-1)(-1) = 1?,Is this a proper proof of (-1)(-1) = 1?,,"I am a novice in proof writing and have just started a book on analysis. I have no other pure math experience or knowledge of abstract algebra. I am trying to prove that $(-1)*(-1) = 1$. I will first show my attempt and follow it with the standard technique that I have found online. I would like to understand why the second method is preferred (or perhaps the only valid one). Method 1 (my attempt): We are given the nine field axioms for $\Bbb R$. Using these, it has been shown (in the text) that $(-1)x = -x$ for all $x \in \Bbb R$. Therefore, $(-1)(-1) = -(-1) = 1$ because we have the identity element of multiplication $1 \in \Bbb R$ and there exists $ -1 \in \Bbb R$ such that $-1$ is the additive inverse of $1$. Method 2 (found by Google search): We know that $(0)(0) = 0$ because it was shown (in the text) that $0x = 0$ for all $x \in \Bbb R$. Then $$(0)(0) = ((-1) + 1)*((-1) + 1) = (-1)(-1) + (-1)(1) + (1)(-1) + (1)(1) \\= (-1)(-1) - 1 - 1 + 1 = (-1)(-1) - 1 = 0$$ Therefore $(-1)(-1) = 1$ because we see that $(-1)(-1)$ is the additive inverse of $-1$. Is the second method simply a more precise version of the first method, while the first is overly verbal? Or is the first somehow using circular reasoning?","I am a novice in proof writing and have just started a book on analysis. I have no other pure math experience or knowledge of abstract algebra. I am trying to prove that $(-1)*(-1) = 1$. I will first show my attempt and follow it with the standard technique that I have found online. I would like to understand why the second method is preferred (or perhaps the only valid one). Method 1 (my attempt): We are given the nine field axioms for $\Bbb R$. Using these, it has been shown (in the text) that $(-1)x = -x$ for all $x \in \Bbb R$. Therefore, $(-1)(-1) = -(-1) = 1$ because we have the identity element of multiplication $1 \in \Bbb R$ and there exists $ -1 \in \Bbb R$ such that $-1$ is the additive inverse of $1$. Method 2 (found by Google search): We know that $(0)(0) = 0$ because it was shown (in the text) that $0x = 0$ for all $x \in \Bbb R$. Then $$(0)(0) = ((-1) + 1)*((-1) + 1) = (-1)(-1) + (-1)(1) + (1)(-1) + (1)(1) \\= (-1)(-1) - 1 - 1 + 1 = (-1)(-1) - 1 = 0$$ Therefore $(-1)(-1) = 1$ because we see that $(-1)(-1)$ is the additive inverse of $-1$. Is the second method simply a more precise version of the first method, while the first is overly verbal? Or is the first somehow using circular reasoning?",,"['real-analysis', 'proof-verification']"
44,"For what values of $\alpha,\beta$ is $x^{\alpha}\sin{x^\beta}\in L^1((0,1])$?",For what values of  is ?,"\alpha,\beta x^{\alpha}\sin{x^\beta}\in L^1((0,1])","Let $E=(0,1]$. For every $\alpha,\beta\in\mathbb{R}$, let $f(x)=x^{\alpha}\sin{x^\beta}$. For what values of $\alpha,\beta$ is $f\in L^1(E)$? I think I know the answer: when $\alpha>-1$ or $\alpha+\beta>-1$. I don't know how to prove that $f$ is not integrable when neither of these inequalities hold. First suppose that $\alpha>-1$. Since $\left|\sin{x^\beta}\right|\leq 1$, we have that $\left|x^{\alpha}\sin{x^\beta}\right|\leq x^{\alpha}$. By the monotonicity of Lebesgue integration, the monotone convergence theorem, and the fact that the Lebesgue and Riemann integral of continuous functions over closed intervals are equal, we can see: $$ \int_E\left|f(x)\right|\,d\lambda\leq\int_E\left|x^\alpha\right|\,d\lambda=\int_Ex^\alpha\,d\lambda=\lim_{n\to\infty}\int_{\frac{1}{n}}^1x^\alpha\,d\lambda=\lim_{n\to\infty}\int_{\frac{1}{n}}^1x^\alpha\,dx=\lim_{n\to\infty}\frac{1^{\alpha+1}-n^{-\alpha-1}}{\alpha+1}=\frac{1}{\alpha+1}$$ Now suppose that $\alpha+\beta>-1$. So $\underset{n\to\infty}{\lim}n^{-\alpha-\beta-1}=0$. Note that $\left|\sin{u}\right|\leq u$ for all $u\geq 0$. Thus, for $x>0$ we have that $\left|\sin{x^\beta}\right|\leq x^\beta$ and so $\left| f(x)\right|\leq x^{\alpha+\beta}$ for $x\in E$. Thus, we have: $$\int_E\left|f\right|\,d\lambda\leq\int_E\left|x^{\alpha+\beta}\right|\,d\lambda=\int_Ex^{\alpha+\beta}=\lim_{n\to\infty}\int_{\frac{1}{n}}^1x^{\alpha+\beta}\,d\lambda=\lim_{n\to\infty}\int_{\frac{1}{n}}^1x^{\alpha+\beta}\,dx=\lim_{n\to\infty}\frac{1^{\alpha+\beta+1}-n^{-\alpha-\beta-1}}{\alpha+\beta+1}=\frac{1}{\alpha+\beta+1}.$$ That's all I have so far. If anyone can help me show that $f$ is not integrable when $\alpha\leq -1$ and $\alpha+\beta\leq -1$, I would be grateful. Or perhaps I am wrong and these are not the right constraints.","Let $E=(0,1]$. For every $\alpha,\beta\in\mathbb{R}$, let $f(x)=x^{\alpha}\sin{x^\beta}$. For what values of $\alpha,\beta$ is $f\in L^1(E)$? I think I know the answer: when $\alpha>-1$ or $\alpha+\beta>-1$. I don't know how to prove that $f$ is not integrable when neither of these inequalities hold. First suppose that $\alpha>-1$. Since $\left|\sin{x^\beta}\right|\leq 1$, we have that $\left|x^{\alpha}\sin{x^\beta}\right|\leq x^{\alpha}$. By the monotonicity of Lebesgue integration, the monotone convergence theorem, and the fact that the Lebesgue and Riemann integral of continuous functions over closed intervals are equal, we can see: $$ \int_E\left|f(x)\right|\,d\lambda\leq\int_E\left|x^\alpha\right|\,d\lambda=\int_Ex^\alpha\,d\lambda=\lim_{n\to\infty}\int_{\frac{1}{n}}^1x^\alpha\,d\lambda=\lim_{n\to\infty}\int_{\frac{1}{n}}^1x^\alpha\,dx=\lim_{n\to\infty}\frac{1^{\alpha+1}-n^{-\alpha-1}}{\alpha+1}=\frac{1}{\alpha+1}$$ Now suppose that $\alpha+\beta>-1$. So $\underset{n\to\infty}{\lim}n^{-\alpha-\beta-1}=0$. Note that $\left|\sin{u}\right|\leq u$ for all $u\geq 0$. Thus, for $x>0$ we have that $\left|\sin{x^\beta}\right|\leq x^\beta$ and so $\left| f(x)\right|\leq x^{\alpha+\beta}$ for $x\in E$. Thus, we have: $$\int_E\left|f\right|\,d\lambda\leq\int_E\left|x^{\alpha+\beta}\right|\,d\lambda=\int_Ex^{\alpha+\beta}=\lim_{n\to\infty}\int_{\frac{1}{n}}^1x^{\alpha+\beta}\,d\lambda=\lim_{n\to\infty}\int_{\frac{1}{n}}^1x^{\alpha+\beta}\,dx=\lim_{n\to\infty}\frac{1^{\alpha+\beta+1}-n^{-\alpha-\beta-1}}{\alpha+\beta+1}=\frac{1}{\alpha+\beta+1}.$$ That's all I have so far. If anyone can help me show that $f$ is not integrable when $\alpha\leq -1$ and $\alpha+\beta\leq -1$, I would be grateful. Or perhaps I am wrong and these are not the right constraints.",,"['real-analysis', 'lebesgue-integral']"
45,Must an uncountable subset of R have uncountably many accumulation points? [duplicate],Must an uncountable subset of R have uncountably many accumulation points? [duplicate],,"This question already has answers here : Accumulation points of uncountable sets (3 answers) Closed 10 years ago . This question is taken from problem 4.1.8 of ""Real Analysis and Foundations"" by Krantz The question reads: ""Let S be an uncountable subset of $\mathbb{R}$. Prove that S must have infinitely many accumulation points. Must it have uncountably many?"" The first part of the question took some work but ended up coming out pretty smoothly; however, I'm at a complete loss as to how to go about addressing the second problem. Intuitively, I think the answer is yes. My first attempt was to try to show that a countable number of accumulation points allowed one to order the elements of S in such a way that they are countable (ie prove the contrapositive), but I did not manage to get much further than that. My second attempt was to show that $S-{s_{1},s_{2}...}$ where $s_{1},s_{2},...$ are countably many accumulation points of S is uncountable and thus must have an accumulation point, so S has an accumulation point that is not one of the countably infinite set. Hence, the number of accumulation points is uncountable. My question is, is this logic valid? I would have to prove that an uncountable set minus a countable set is uncountable, which shouldn't be too difficult. Any hints/points in the right direction/outright answers are greatly appreciated.","This question already has answers here : Accumulation points of uncountable sets (3 answers) Closed 10 years ago . This question is taken from problem 4.1.8 of ""Real Analysis and Foundations"" by Krantz The question reads: ""Let S be an uncountable subset of $\mathbb{R}$. Prove that S must have infinitely many accumulation points. Must it have uncountably many?"" The first part of the question took some work but ended up coming out pretty smoothly; however, I'm at a complete loss as to how to go about addressing the second problem. Intuitively, I think the answer is yes. My first attempt was to try to show that a countable number of accumulation points allowed one to order the elements of S in such a way that they are countable (ie prove the contrapositive), but I did not manage to get much further than that. My second attempt was to show that $S-{s_{1},s_{2}...}$ where $s_{1},s_{2},...$ are countably many accumulation points of S is uncountable and thus must have an accumulation point, so S has an accumulation point that is not one of the countably infinite set. Hence, the number of accumulation points is uncountable. My question is, is this logic valid? I would have to prove that an uncountable set minus a countable set is uncountable, which shouldn't be too difficult. Any hints/points in the right direction/outright answers are greatly appreciated.",,['real-analysis']
46,Rewrite $\Gamma(-z)$ in terms of $\Gamma(z)$,Rewrite  in terms of,\Gamma(-z) \Gamma(z),Is it possible to rewrite $\Gamma(-z)$ in terms of $\Gamma(z)$ where $\Gamma(z) = \int^\infty_0 t^{z-1}e^{-t}dt$?,Is it possible to rewrite $\Gamma(-z)$ in terms of $\Gamma(z)$ where $\Gamma(z) = \int^\infty_0 t^{z-1}e^{-t}dt$?,,"['real-analysis', 'complex-analysis', 'analysis', 'functions', 'special-functions']"
47,Extended integral over parabola open set,Extended integral over parabola open set,,"Let $f(x,y)=1/(y+1)^2$ for all $x,y\in\mathbb{R}$. Let $A$ be the open set $$A=\{(x,y)\mid x>0, x^2<y<2x^2\}.$$ Show that $\int_A f$ exists, and calculate it. Since $A$ is an unbounded open set, we have to use the ""extended integral"", which has two equivalent definitions. 1) $\int_A f$ is the supremum of the numbers $\int_D f$, as $D$ ranges over all compact Jordan-measurable subsets of $A$. 2) If $C_1,C_2,\ldots$ is a sequence of compact Jordan-measurable subsets of $A$ such that $\cup_{i=1}^{\infty}C_i=A$ and $C_i$ belongs to the interior of $C_{i+1}$ for all $i$ . Then $\int_A f=\lim_{i\rightarrow\infty}\int_{C_i}f$. Definition 2) is easier to deal with. Nevertheless, for a set like the set $A$, with a really strange shape, it is really hard to imagine what compact, rectifiable subsets we should take to satisfy the conditions in 2). Also, it must be easy to calculate the integrals of $f$ over these subsets, so that we can compute the limit to be the integral of $f$ over $A$. Which subsets $C_1,C_2,\ldots$ should be chosen to satisfy the conditions in  2), so that we can calculate the integrals over them?","Let $f(x,y)=1/(y+1)^2$ for all $x,y\in\mathbb{R}$. Let $A$ be the open set $$A=\{(x,y)\mid x>0, x^2<y<2x^2\}.$$ Show that $\int_A f$ exists, and calculate it. Since $A$ is an unbounded open set, we have to use the ""extended integral"", which has two equivalent definitions. 1) $\int_A f$ is the supremum of the numbers $\int_D f$, as $D$ ranges over all compact Jordan-measurable subsets of $A$. 2) If $C_1,C_2,\ldots$ is a sequence of compact Jordan-measurable subsets of $A$ such that $\cup_{i=1}^{\infty}C_i=A$ and $C_i$ belongs to the interior of $C_{i+1}$ for all $i$ . Then $\int_A f=\lim_{i\rightarrow\infty}\int_{C_i}f$. Definition 2) is easier to deal with. Nevertheless, for a set like the set $A$, with a really strange shape, it is really hard to imagine what compact, rectifiable subsets we should take to satisfy the conditions in 2). Also, it must be easy to calculate the integrals of $f$ over these subsets, so that we can compute the limit to be the integral of $f$ over $A$. Which subsets $C_1,C_2,\ldots$ should be chosen to satisfy the conditions in  2), so that we can calculate the integrals over them?",,"['real-analysis', 'integration', 'improper-integrals']"
48,How does a geometry-oriented mind learn analysis?,How does a geometry-oriented mind learn analysis?,,"I find it very difficult to understand analysis, because I can't find a way to learn it geometrically. To make my point clearer, let me take calculus as the example in contrast. I find calculus very geometric and I'm very comfortable with it, to name a few famous examples: Newton-Leibniz theorem: a strip under the graph of a (continuous)function, divided by its width, will approach the value of the height as width goes to 0; L'Hospital's Rule: if two functions are approaching 0 at the same point, as we approach that point, the ratio of the heights of the two graphs will be the same as the ratio of the slopes, because $\text{height}\approx \text{slope}\times\Delta x$, and $\Delta x$ is the same for both graphs. I can keep going, and in fact, most of the important theorems in calculus have such geometric intepretations. I'm aware that there are technicalities that cannot be captureed by the above geometric arguments, but I believe they are still very to the point and I feel I can never forget them. It seems to be another story for analysis(especially real analysis). Although occasionally there are important content that can be understood geometrically, more often I encounter theorems and concepts that after quite some struggling still no firm geometric intuition can be developed. A lot of times some partial intuition can be developed, but usually trusting these partial intuitions will quickly get myself into errors. I can follow most of the analysis textbooks, i.e. all their logical deductions, proving theorems etc. but the problem is, when I can't form pictures, I can't find positions in my mind for these knowledge, so although I have read a considerable amount of analysis(and done quite some exercises), not much can be ""triggered"" from my mind when I see a analysis-related problem , say, 4 months after I stop reading an analysis book. In fact, I post the question here only after reading Poincare's Intuition and Logic in Mathematics , before reading this I assumed everyone thinks in the geometric way so if I can't understand analysis I probably should just try harder. It's quite a headache to me because nothing else(calculus, linear algebra, modern differential geometry etc.) has caused this much of discomfort. Now I'm wondering if I've learnt analysis in a wrong way, any comments or suggestions?","I find it very difficult to understand analysis, because I can't find a way to learn it geometrically. To make my point clearer, let me take calculus as the example in contrast. I find calculus very geometric and I'm very comfortable with it, to name a few famous examples: Newton-Leibniz theorem: a strip under the graph of a (continuous)function, divided by its width, will approach the value of the height as width goes to 0; L'Hospital's Rule: if two functions are approaching 0 at the same point, as we approach that point, the ratio of the heights of the two graphs will be the same as the ratio of the slopes, because $\text{height}\approx \text{slope}\times\Delta x$, and $\Delta x$ is the same for both graphs. I can keep going, and in fact, most of the important theorems in calculus have such geometric intepretations. I'm aware that there are technicalities that cannot be captureed by the above geometric arguments, but I believe they are still very to the point and I feel I can never forget them. It seems to be another story for analysis(especially real analysis). Although occasionally there are important content that can be understood geometrically, more often I encounter theorems and concepts that after quite some struggling still no firm geometric intuition can be developed. A lot of times some partial intuition can be developed, but usually trusting these partial intuitions will quickly get myself into errors. I can follow most of the analysis textbooks, i.e. all their logical deductions, proving theorems etc. but the problem is, when I can't form pictures, I can't find positions in my mind for these knowledge, so although I have read a considerable amount of analysis(and done quite some exercises), not much can be ""triggered"" from my mind when I see a analysis-related problem , say, 4 months after I stop reading an analysis book. In fact, I post the question here only after reading Poincare's Intuition and Logic in Mathematics , before reading this I assumed everyone thinks in the geometric way so if I can't understand analysis I probably should just try harder. It's quite a headache to me because nothing else(calculus, linear algebra, modern differential geometry etc.) has caused this much of discomfort. Now I'm wondering if I've learnt analysis in a wrong way, any comments or suggestions?",,"['real-analysis', 'geometry', 'intuition']"
49,$L^p$ derivative vs normal derivative.,derivative vs normal derivative.,L^p,"Let $f, g : \mathbb{R} \rightarrow \mathbb{C}$ be Lebesgue measurable functions, and let $1 \leq p < \infty$. If $f, g \in L^p$ and $$ \large \lim_{y \rightarrow 0} \normalsize \left\| \frac{f(\bullet + y) - f(\bullet)}{y} - g(\bullet)\right\|_p= 0,$$ where $\|~\|_p$ is the $L^p$ norm, and we integrate with respect to the omitted variable, then we say that $f$ is $L^p$ differentiable, and $g$ is the $L^p$ derivative of $f$ . Now, suppose $f \in L^p$ and $f$ is differentiable in the usual sense . Moreover, assume that $f^{\prime} \in L^p$. My questions are: 1) Does it follow that $f$ is also $L^p$ differentiable and $f^{\prime}$ is it's $L^p$ derivative? 2) What if we assume that $f$ is $C^1$, in addition to the other hypotheses? Here's what I've tried: under the above hypotheses, we have that $$\left| \frac{f(x + y) - f(x)}{y} - f^{\prime}(x)\right|^p \longrightarrow 0, ~\text{as}~y \rightarrow 0,$$ so we could apply the Dominated Convergence Theorem (switching from $y$ to an arbitrary sequence $(y_n)_n$ of course) if we could show that the above quantity is bounded by some fixed $L^p$ function of $x$ alone, that is, some function that doesn't depend on $y$. This is where I got stuck; I can't really eliminate the dependence on $y$. Thank you.","Let $f, g : \mathbb{R} \rightarrow \mathbb{C}$ be Lebesgue measurable functions, and let $1 \leq p < \infty$. If $f, g \in L^p$ and $$ \large \lim_{y \rightarrow 0} \normalsize \left\| \frac{f(\bullet + y) - f(\bullet)}{y} - g(\bullet)\right\|_p= 0,$$ where $\|~\|_p$ is the $L^p$ norm, and we integrate with respect to the omitted variable, then we say that $f$ is $L^p$ differentiable, and $g$ is the $L^p$ derivative of $f$ . Now, suppose $f \in L^p$ and $f$ is differentiable in the usual sense . Moreover, assume that $f^{\prime} \in L^p$. My questions are: 1) Does it follow that $f$ is also $L^p$ differentiable and $f^{\prime}$ is it's $L^p$ derivative? 2) What if we assume that $f$ is $C^1$, in addition to the other hypotheses? Here's what I've tried: under the above hypotheses, we have that $$\left| \frac{f(x + y) - f(x)}{y} - f^{\prime}(x)\right|^p \longrightarrow 0, ~\text{as}~y \rightarrow 0,$$ so we could apply the Dominated Convergence Theorem (switching from $y$ to an arbitrary sequence $(y_n)_n$ of course) if we could show that the above quantity is bounded by some fixed $L^p$ function of $x$ alone, that is, some function that doesn't depend on $y$. This is where I got stuck; I can't really eliminate the dependence on $y$. Thank you.",,"['real-analysis', 'functional-analysis', 'derivatives']"
50,Closed form of $\operatorname{Li}_2(\varphi)$ and $\operatorname{Li}_2(\varphi-1)$,Closed form of  and,\operatorname{Li}_2(\varphi) \operatorname{Li}_2(\varphi-1),"I am trying to calculate the dilogarithm of the golden ratio and its conjugate $\Phi = \varphi-1$. Eg the solutions of the equation $u^2 - u = 1$.  From Wikipdia one has the following \begin{align*}    \operatorname{Li}_2\left( \frac{1 + \sqrt{5}}{2} \right)    & = -\int_0^\varphi \frac{\log(1-t)}{t}\,\mathrm{d}t       =  \phantom{-}\frac{\pi^2}{10} - \log^2\left( \Phi\right) \\   \operatorname{Li}_2\left( \frac{1 - \sqrt{5}}{2} \right)    & = -\int_0^\Phi \frac{\log(1-t)}{t}\,\mathrm{d}t       =  -\frac{\pi^2}{15} - \log^2\left( -\Phi\right) \\  \end{align*} I am quite certain that these two special values can be shown by combining the identites for the dilogarithm, and forming a system of equations. But I am having some problems obtaining a set of equations only involving $\operatorname{Li}_2(\varphi)$ and $\operatorname{Li}_2(\Phi)$. Can anyone show me how to set up the system of equations from the identites, or perhaps a different path in showing these two values?","I am trying to calculate the dilogarithm of the golden ratio and its conjugate $\Phi = \varphi-1$. Eg the solutions of the equation $u^2 - u = 1$.  From Wikipdia one has the following \begin{align*}    \operatorname{Li}_2\left( \frac{1 + \sqrt{5}}{2} \right)    & = -\int_0^\varphi \frac{\log(1-t)}{t}\,\mathrm{d}t       =  \phantom{-}\frac{\pi^2}{10} - \log^2\left( \Phi\right) \\   \operatorname{Li}_2\left( \frac{1 - \sqrt{5}}{2} \right)    & = -\int_0^\Phi \frac{\log(1-t)}{t}\,\mathrm{d}t       =  -\frac{\pi^2}{15} - \log^2\left( -\Phi\right) \\  \end{align*} I am quite certain that these two special values can be shown by combining the identites for the dilogarithm, and forming a system of equations. But I am having some problems obtaining a set of equations only involving $\operatorname{Li}_2(\varphi)$ and $\operatorname{Li}_2(\Phi)$. Can anyone show me how to set up the system of equations from the identites, or perhaps a different path in showing these two values?",,"['real-analysis', 'integration', 'special-functions', 'polylogarithm']"
51,The set of jumps of a càdlàg function is countable,The set of jumps of a càdlàg function is countable,,"Let $f$ be defined on $[0, \infty)$ have all its left limits and be right continuous. Then is the set of discontinuity points of $f$ which are all jumps necessarily countable?","Let $f$ be defined on $[0, \infty)$ have all its left limits and be right continuous. Then is the set of discontinuity points of $f$ which are all jumps necessarily countable?",,"['real-analysis', 'analysis']"
52,Prove that $\lim \limits_{n \to \infty} n^2 \int_{0}^{\frac{1}{n}} x^{x+1} dx = \dfrac{1}{2} $,Prove that,\lim \limits_{n \to \infty} n^2 \int_{0}^{\frac{1}{n}} x^{x+1} dx = \dfrac{1}{2} ,Prove that $$\lim \limits_{n \to \infty} n^2 \int_{0}^{\frac{1}{n}} x^{x+1} dx = \dfrac{1}{2} $$,Prove that $$\lim \limits_{n \to \infty} n^2 \int_{0}^{\frac{1}{n}} x^{x+1} dx = \dfrac{1}{2} $$,,"['real-analysis', 'integration', 'limits']"
53,Prove the limit at infinity is 0,Prove the limit at infinity is 0,,"Suppose $f$ is continuous. For all $x>0$, the limit of $f(nx)$ when $n$ goes to infinity is $0$. Then please prove that the limit of $f(x)$ as $x$ goes to infinity is $0$. (I totally stuck at it) I think it suffices to show that f is uniformly continuous on $[0, \infty)$.","Suppose $f$ is continuous. For all $x>0$, the limit of $f(nx)$ when $n$ goes to infinity is $0$. Then please prove that the limit of $f(x)$ as $x$ goes to infinity is $0$. (I totally stuck at it) I think it suffices to show that f is uniformly continuous on $[0, \infty)$.",,"['real-analysis', 'analysis', 'limits']"
54,Proving $\lim_{n\to\infty}\left(n-\sum_{k=2}^{n}\frac{1}{\sum_{i=1}^{\infty}\frac{1}{i^k}}\right)=1+\sum_{p\in P}\frac{1}{p\left(p-1\right)}$?,Proving ?,\lim_{n\to\infty}\left(n-\sum_{k=2}^{n}\frac{1}{\sum_{i=1}^{\infty}\frac{1}{i^k}}\right)=1+\sum_{p\in P}\frac{1}{p\left(p-1\right)},"$$\lim_{n\to\infty}\left(n-\sum_{k=2}^{n}\frac{1}{\sum_{i=1}^{\infty}\frac{1}{i^k}}\right)=1+\sum_{p\in P}\frac{1}{p\left(p-1\right)}$$ $P$ is primes. Interesting question ran across while tutoring. Not sure how to solve it, or even what class it's from.","$$\lim_{n\to\infty}\left(n-\sum_{k=2}^{n}\frac{1}{\sum_{i=1}^{\infty}\frac{1}{i^k}}\right)=1+\sum_{p\in P}\frac{1}{p\left(p-1\right)}$$ $P$ is primes. Interesting question ran across while tutoring. Not sure how to solve it, or even what class it's from.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'prime-numbers']"
55,Fubini's theorem for Riemann integrals?,Fubini's theorem for Riemann integrals?,,The integrals in Fubini's theorem are all Lebesgue integrals. I was wondering if there is a theorem with conclusions similar to Fubini's but only involving Riemann integrals? Thanks and regards!,The integrals in Fubini's theorem are all Lebesgue integrals. I was wondering if there is a theorem with conclusions similar to Fubini's but only involving Riemann integrals? Thanks and regards!,,['real-analysis']
56,Cantor set - a question about being metrizable and about the connected components,Cantor set - a question about being metrizable and about the connected components,,"I have a question regarding Cantor set given to me as a homework question (well, part of it): a. Prove that the only connected components of Cantor set are the   singletons $\{x\}$ where $x\in C$ b. Prove that $C$ is metrizable I am having some problems with this exercise: My thoughts about $a$: I know that in general path connectedness and connectedness are not equivalent, but I know that$\mathbb{R}$ is path connected, I want to say something like that since if $\gamma(t):C\to C$ is continues then $\gamma(t)\equiv x$ for some $x\in C$ then I have it that the connected components of $C$ can be only the singltons. But I lack any justification - connectedness and path connectedness are not the same thing - but maybe since $\mathbb{R}$ is path connected we can justify somehow that if $C$ had any connected component then it is also path connected ? another thing that confuses me is that the open sets relative to $C$ and relative to $\mathbb{R}$ are not the same so I am also having a problem working with the definition of when a space is called connected My thoughts about b: Myabe there is something that I don't understand - but isn't $C$ metrizable since its a subspace of a $[0,1]$ with the topology that comes from the standard metric on $\mathbb{R}$ ? I would appreciate any explanations and help with this exercise!","I have a question regarding Cantor set given to me as a homework question (well, part of it): a. Prove that the only connected components of Cantor set are the   singletons $\{x\}$ where $x\in C$ b. Prove that $C$ is metrizable I am having some problems with this exercise: My thoughts about $a$: I know that in general path connectedness and connectedness are not equivalent, but I know that$\mathbb{R}$ is path connected, I want to say something like that since if $\gamma(t):C\to C$ is continues then $\gamma(t)\equiv x$ for some $x\in C$ then I have it that the connected components of $C$ can be only the singltons. But I lack any justification - connectedness and path connectedness are not the same thing - but maybe since $\mathbb{R}$ is path connected we can justify somehow that if $C$ had any connected component then it is also path connected ? another thing that confuses me is that the open sets relative to $C$ and relative to $\mathbb{R}$ are not the same so I am also having a problem working with the definition of when a space is called connected My thoughts about b: Myabe there is something that I don't understand - but isn't $C$ metrizable since its a subspace of a $[0,1]$ with the topology that comes from the standard metric on $\mathbb{R}$ ? I would appreciate any explanations and help with this exercise!",,"['real-analysis', 'general-topology', 'metric-spaces']"
57,Is the limit of a $L^2$-convergent sequence of random variables unique up to a.e.?,Is the limit of a -convergent sequence of random variables unique up to a.e.?,L^2,"Is the limit of a $L^2$-convergent sequence of random variables unique up to a.e.? In other words, if $X$ and $Y$ are both limits, will $X=Y$ a.e.? If yes, is Ito integral, which is defined as $L^2$ limit of a sequence of Ito integrals of simple processes, defined only up to a.e.? Conversely, if the sequence converges to a random variable $X$, and $Y$ is another random variable same as $X$ a.e., will $Y$ also be the limit of the $L^2$-convergent sequence? Similar questions for a sequence of random variables that converges in probability. Thanks in advance!","Is the limit of a $L^2$-convergent sequence of random variables unique up to a.e.? In other words, if $X$ and $Y$ are both limits, will $X=Y$ a.e.? If yes, is Ito integral, which is defined as $L^2$ limit of a sequence of Ito integrals of simple processes, defined only up to a.e.? Conversely, if the sequence converges to a random variable $X$, and $Y$ is another random variable same as $X$ a.e., will $Y$ also be the limit of the $L^2$-convergent sequence? Similar questions for a sequence of random variables that converges in probability. Thanks in advance!",,"['real-analysis', 'probability-theory']"
58,Relation between integral by parts and Fubini's theorem,Relation between integral by parts and Fubini's theorem,,"In probability, I have seen some examples for which both Fubini's theorem and integration by parts (for Riemann-Stieltjes integrals with cdf as integrator) provide different but correct solutions. For example In proving $E(|X|)=\int_0^\infty P(|X| > t)dt$ , Edvin and Did used Fubini's theorem, while Ben used integration by parts; In proving $\operatorname{median}(X)$ solves $\min_{c \in \mathbb{R}} E |X-c|$ , Did used Fubini's theorem, while Sivaram used integration by parts in Edit. So I wonder if the two are related somehow?  For example, in some cases (especially the two examples above), can one lead to the other? A wide guess for going from Fubini's theorem to integration by parts is: Integration by parts says $$     \begin{align} f(b)g(b) - f(a)g(a) & =  \int_a^b  g(x) \, df(x) + \int_a^b f(x) \, dg(x). \end{align}  $$ If there is some $c \in \mathbb{R}$ such that $g(c)=0$, then $$ \int_a^b g(x) \, df(x) = \int_a^b \int_c^x dg(t)  \, df(x ) $$ If Fubini's theorem or some of its variants can apply, then for some $d \in \mathbb{R}$, $$ \int_a^b \int_c^x dg(t)  \, df(x ) = \int_c^b \int_d^t df(x)  \, dg(t )  = \int_c^b \int_d^x df(t)  \, dg(x )  $$ one step closer to $\int_a^b f(x) \, dg(x)$, but still far away from integration by parts. No idea yet about going from integration by parts to Fubini's theorem.","In probability, I have seen some examples for which both Fubini's theorem and integration by parts (for Riemann-Stieltjes integrals with cdf as integrator) provide different but correct solutions. For example In proving $E(|X|)=\int_0^\infty P(|X| > t)dt$ , Edvin and Did used Fubini's theorem, while Ben used integration by parts; In proving $\operatorname{median}(X)$ solves $\min_{c \in \mathbb{R}} E |X-c|$ , Did used Fubini's theorem, while Sivaram used integration by parts in Edit. So I wonder if the two are related somehow?  For example, in some cases (especially the two examples above), can one lead to the other? A wide guess for going from Fubini's theorem to integration by parts is: Integration by parts says $$     \begin{align} f(b)g(b) - f(a)g(a) & =  \int_a^b  g(x) \, df(x) + \int_a^b f(x) \, dg(x). \end{align}  $$ If there is some $c \in \mathbb{R}$ such that $g(c)=0$, then $$ \int_a^b g(x) \, df(x) = \int_a^b \int_c^x dg(t)  \, df(x ) $$ If Fubini's theorem or some of its variants can apply, then for some $d \in \mathbb{R}$, $$ \int_a^b \int_c^x dg(t)  \, df(x ) = \int_c^b \int_d^t df(x)  \, dg(t )  = \int_c^b \int_d^x df(t)  \, dg(x )  $$ one step closer to $\int_a^b f(x) \, dg(x)$, but still far away from integration by parts. No idea yet about going from integration by parts to Fubini's theorem.",,"['real-analysis', 'probability-theory', 'integration']"
59,Approximation of Riemann integrable function with a continuous function,Approximation of Riemann integrable function with a continuous function,,"I have proved that if $f \in R[a,b]$ and given $\epsilon > 0$ there exists a continuous function $g$ such that $$\int_a^b |f-g| < \epsilon$$ I was wondering if using this fact there is some way to show that there is also some continuous function $h$ such that $$\int_a^b |f-h|^2 < \epsilon$$ Any help will be appreciated, thanks :)","I have proved that if $f \in R[a,b]$ and given $\epsilon > 0$ there exists a continuous function $g$ such that $$\int_a^b |f-g| < \epsilon$$ I was wondering if using this fact there is some way to show that there is also some continuous function $h$ such that $$\int_a^b |f-h|^2 < \epsilon$$ Any help will be appreciated, thanks :)",,['real-analysis']
60,"Arithmetic on $[0,\infty]$: is $0 \cdot \infty = 0$ the only reasonable choice?",Arithmetic on : is  the only reasonable choice?,"[0,\infty] 0 \cdot \infty = 0","On page 18 of Rudin's Real and Complex analysis he defines $0 \cdot \infty = 0$ and says that ""with this definition the commutative, associative, and distributive laws hold in $[0,\infty]$ without any restriction"". What is not clear to me is whether the quoted statement is a justification of the definition or just a consequence. Wouldn't the commutative, associative, and distributive laws also hold if we define $0 \cdot \infty = \infty$?","On page 18 of Rudin's Real and Complex analysis he defines $0 \cdot \infty = 0$ and says that ""with this definition the commutative, associative, and distributive laws hold in $[0,\infty]$ without any restriction"". What is not clear to me is whether the quoted statement is a justification of the definition or just a consequence. Wouldn't the commutative, associative, and distributive laws also hold if we define $0 \cdot \infty = \infty$?",,"['real-analysis', 'number-systems']"
61,How to generalise the limit of the product $\lim _{n \rightarrow \infty} \prod_{k=1}^n\left(1+\frac{1}{n}+\frac{k}{n^2}\right) $?,How to generalise the limit of the product ?,\lim _{n \rightarrow \infty} \prod_{k=1}^n\left(1+\frac{1}{n}+\frac{k}{n^2}\right) ,"When I met the result in the post that $$ \boxed{\lim _{n \rightarrow \infty}\prod_{k=1}^n\left(1+\frac{k}{n^2}\right) =\sqrt{e}}, $$ I appreciated very much the application of $G.M.\leq A.M.$ in the answer and modified the solution as below: $$ P_n^2=\prod_{k=0}^n\left[1+\frac{1}{n}+\frac{k(n-k)}{n^4}\right] $$ Using $G.M.\leq A.M.$ , we have $$ 0 \leqslant k(n-k) \leqslant \frac{n^2}{4} $$ Plugging back yields $$ \left(1+\frac{1}{n}\right)^n \leqslant p_n^2 \leqslant \prod_{k=0}^n\left(1+\frac{1}{n}+\frac{1}{4 n^2}\right)=\left(1+\frac{1}{2 n}\right)^{2 n} $$ Since $$ \lim _{n \rightarrow \infty}\left(1+\frac{1}{2 n}\right)^{2 n}=e=\lim _{n \rightarrow \infty}\left(1+\frac{1}{n}\right)^n, $$ By Squeezing Theorem, we conclude that $$ \lim _{n \rightarrow \infty} \prod_{k=1}^n\left(1+\frac{k}{n^2}\right)=\sqrt{e} $$ I then tried to investigate the limit $$ \lim _{n \rightarrow \infty} \prod_{k=1}^n\left(1+\frac{1}{n}+\frac{k}{n^2}\right) $$ by the same technique. Let the product be $Q_n$ . Firstly we are going to find similarly the upper bound of $Q_n^2$ in terms of $n$ . $$\displaystyle \begin{aligned}Q_n^2 & =\prod_{k=1}^n\left[\left(1+\frac{1}{n}\right)^2+\left(1+\frac{1}{n}\right) \frac{1}{n}+\frac{k(n-k)}{n^4}\right]\end{aligned}\tag*{} $$ For the last terms, using $G.M.\leq A.M.$ again, we have $\displaystyle 0\leq k(n-k) \leqslant \frac{n^2}{4}\tag*{} $ Then plugging back yields $$\displaystyle \begin{aligned}Q_n^2 & \leqslant \prod_{k=1}^n\left[\left(1+\frac{1}{n}\right)^2+\left(1+\frac{1}{n}\right) \frac{1}{n}+\frac{1}{4 n^2}\right] \\& =\prod_{k=1}^n\left(1+\frac{1}{n}+\frac{1}{2 n}\right)^2 \\& =\prod_{k=1}^n\left(1+\frac{3}{2 n}\right)^2\\&=\left(1+\frac{3}{2 n}\right)^{2 n} \end{aligned}\tag*{} $$ Next we are going to find the lower bound of $Q_n^2$ in terms of $n$ . $$\displaystyle Q_n^2\geqslant \prod_{k=1}^n\left[\left(1+\frac{1}{n}\right)^2+\left(1+\frac{1}{n}\right) \frac{1}{n}\right]= \prod_{k=1}^n \left[\left(1+\frac{1}{n}\right)\left(1+\frac{2}{n}\right)\right]\\=\left(1+\frac{1}{n}\right)^n\left(1+\frac{2}{n}\right)^n\tag*{} $$ Now we can find the limits of the bounds of $Q_n^2$ as $n$ tends to $\infty$ . $$\displaystyle \lim _{n \rightarrow \infty}\left(1+\frac{3}{2 n}\right)^{2n}=\left[\lim _{n \rightarrow \infty}\left(1+\frac{1}{\frac{2 n}{3}}\right)^{\frac{2 n}{3}}\right]^{3}=e^3\tag*{} $$ and $$\displaystyle \lim _{n \rightarrow \infty}\left(1+\frac{1}{n}\right)^n\left(1+\frac{2}{n}\right)^n=e \cdot e^2=e^3\tag*{} $$ By the Squeeze Theorem, $\displaystyle \lim _{n \rightarrow \infty} Q_n^2=e^3\tag*{} $ Hence we can conclude that $\displaystyle \boxed{\lim _{n \rightarrow \infty} \prod_{k=1}^n\left(1+\frac{1}{n}+\frac{k}{n^2}\right)=e^{\frac{3}{2}}\,} \tag*{} $ I am curious about what happens when both the number of terms and the power of $n$ in the product increase. I started to investigate the limit and surprisingly found that $$\boxed{\lim _{n \rightarrow \infty} \prod_{k=1}^n\left(1+\frac{1}{n}+\frac{1}{n^2}+\cdots +\frac{1}{n^{m-1}} +\frac{k}{n^m}\right)=e\,}$$ where $m\ge 3$ . How to prove it? Let $$R_n=\prod_{k=1}^n\left(1+\frac{1}{n}+\frac{1}{n^2}+\cdots +\frac{1}{n^{m-1}} +\frac{k}{n^m}\right)$$ and $$ A=1+\frac{1}{n}+\frac{1}{n^2}+\ldots+\frac{1}{n^{m-1}} $$ Then $$R_n^2 =\prod_{k=1}^n\left[\left(A+\frac{k}{n^m}\right)\left(A+\frac{n-k}{n^m}\right) \right] =\prod_{k=1}^n\left[A^2+\frac{A}{n^{m-1}}+\frac{k( n-k)}{n^{2m}}\right] $$ Using $ 0 \leqslant k(n-k) \leqslant \frac{n^2}{4},  $ we have $$ A^n\left(A+\frac{1}{n^{m-1}}\right)^n \leqslant R_n^2 \leqslant \prod_{k=1}^n\left(A+\frac{1}{2 n^{m-1}}\right)^2=\left(A+\frac{1}{2 n^{m-1}}\right)^{2 n} $$ Then we finish the proof if we can prove that $$ \lim _{n \rightarrow \infty} A^n\left(A+\frac{1}{n^{m-1}}\right)^n=e^2= \lim _{n \rightarrow \infty}\left(A+\frac{1}{2 n^{m-1}}\right)^{2 n} $$ My attempt: $$\left(1+\frac{1}{n}\right)^n \leqslant A^n=\left(1+\frac{1}{n}+\frac{1}{n^2}+\ldots+\frac{1}{n^{m-1}}\right)^n \leqslant\left(\frac{1}{1-\frac{1}{n}}\right)^n \Rightarrow  \lim _{n \rightarrow \infty} A^n=e $$ My Question: How to prove that $$ \lim _{n \rightarrow \infty}\left(A+\frac{1}{n^{m-1}}\right)^n=\lim _{n \rightarrow \infty}\left(A+\frac{1}{2 n^{m-1}}\right)^n=e? $$","When I met the result in the post that I appreciated very much the application of in the answer and modified the solution as below: Using , we have Plugging back yields Since By Squeezing Theorem, we conclude that I then tried to investigate the limit by the same technique. Let the product be . Firstly we are going to find similarly the upper bound of in terms of . For the last terms, using again, we have Then plugging back yields Next we are going to find the lower bound of in terms of . Now we can find the limits of the bounds of as tends to . and By the Squeeze Theorem, Hence we can conclude that I am curious about what happens when both the number of terms and the power of in the product increase. I started to investigate the limit and surprisingly found that where . How to prove it? Let and Then Using we have Then we finish the proof if we can prove that My attempt: My Question: How to prove that","
\boxed{\lim _{n \rightarrow \infty}\prod_{k=1}^n\left(1+\frac{k}{n^2}\right) =\sqrt{e}},
 G.M.\leq A.M. 
P_n^2=\prod_{k=0}^n\left[1+\frac{1}{n}+\frac{k(n-k)}{n^4}\right]
 G.M.\leq A.M. 
0 \leqslant k(n-k) \leqslant \frac{n^2}{4}
 
\left(1+\frac{1}{n}\right)^n \leqslant p_n^2 \leqslant \prod_{k=0}^n\left(1+\frac{1}{n}+\frac{1}{4 n^2}\right)=\left(1+\frac{1}{2 n}\right)^{2 n}
 
\lim _{n \rightarrow \infty}\left(1+\frac{1}{2 n}\right)^{2 n}=e=\lim _{n \rightarrow \infty}\left(1+\frac{1}{n}\right)^n,
 
\lim _{n \rightarrow \infty} \prod_{k=1}^n\left(1+\frac{k}{n^2}\right)=\sqrt{e}
 
\lim _{n \rightarrow \infty} \prod_{k=1}^n\left(1+\frac{1}{n}+\frac{k}{n^2}\right)
 Q_n Q_n^2 n \displaystyle \begin{aligned}Q_n^2 & =\prod_{k=1}^n\left[\left(1+\frac{1}{n}\right)^2+\left(1+\frac{1}{n}\right) \frac{1}{n}+\frac{k(n-k)}{n^4}\right]\end{aligned}\tag*{}  G.M.\leq A.M. \displaystyle 0\leq k(n-k) \leqslant \frac{n^2}{4}\tag*{}  \displaystyle \begin{aligned}Q_n^2 & \leqslant \prod_{k=1}^n\left[\left(1+\frac{1}{n}\right)^2+\left(1+\frac{1}{n}\right) \frac{1}{n}+\frac{1}{4 n^2}\right] \\& =\prod_{k=1}^n\left(1+\frac{1}{n}+\frac{1}{2 n}\right)^2 \\& =\prod_{k=1}^n\left(1+\frac{3}{2 n}\right)^2\\&=\left(1+\frac{3}{2 n}\right)^{2 n} \end{aligned}\tag*{}  Q_n^2 n \displaystyle Q_n^2\geqslant \prod_{k=1}^n\left[\left(1+\frac{1}{n}\right)^2+\left(1+\frac{1}{n}\right) \frac{1}{n}\right]= \prod_{k=1}^n \left[\left(1+\frac{1}{n}\right)\left(1+\frac{2}{n}\right)\right]\\=\left(1+\frac{1}{n}\right)^n\left(1+\frac{2}{n}\right)^n\tag*{}  Q_n^2 n \infty \displaystyle \lim _{n \rightarrow \infty}\left(1+\frac{3}{2 n}\right)^{2n}=\left[\lim _{n \rightarrow \infty}\left(1+\frac{1}{\frac{2 n}{3}}\right)^{\frac{2 n}{3}}\right]^{3}=e^3\tag*{}  \displaystyle \lim _{n \rightarrow \infty}\left(1+\frac{1}{n}\right)^n\left(1+\frac{2}{n}\right)^n=e \cdot e^2=e^3\tag*{}  \displaystyle \lim _{n \rightarrow \infty} Q_n^2=e^3\tag*{}  \displaystyle \boxed{\lim _{n \rightarrow \infty} \prod_{k=1}^n\left(1+\frac{1}{n}+\frac{k}{n^2}\right)=e^{\frac{3}{2}}\,} \tag*{}  n \boxed{\lim _{n \rightarrow \infty} \prod_{k=1}^n\left(1+\frac{1}{n}+\frac{1}{n^2}+\cdots +\frac{1}{n^{m-1}} +\frac{k}{n^m}\right)=e\,} m\ge 3 R_n=\prod_{k=1}^n\left(1+\frac{1}{n}+\frac{1}{n^2}+\cdots +\frac{1}{n^{m-1}} +\frac{k}{n^m}\right) 
A=1+\frac{1}{n}+\frac{1}{n^2}+\ldots+\frac{1}{n^{m-1}}
 R_n^2 =\prod_{k=1}^n\left[\left(A+\frac{k}{n^m}\right)\left(A+\frac{n-k}{n^m}\right) \right] =\prod_{k=1}^n\left[A^2+\frac{A}{n^{m-1}}+\frac{k( n-k)}{n^{2m}}\right]  
0 \leqslant k(n-k) \leqslant \frac{n^2}{4}, 
 
A^n\left(A+\frac{1}{n^{m-1}}\right)^n \leqslant R_n^2 \leqslant \prod_{k=1}^n\left(A+\frac{1}{2 n^{m-1}}\right)^2=\left(A+\frac{1}{2 n^{m-1}}\right)^{2 n}
 
\lim _{n \rightarrow \infty} A^n\left(A+\frac{1}{n^{m-1}}\right)^n=e^2= \lim _{n \rightarrow \infty}\left(A+\frac{1}{2 n^{m-1}}\right)^{2 n}  \left(1+\frac{1}{n}\right)^n \leqslant A^n=\left(1+\frac{1}{n}+\frac{1}{n^2}+\ldots+\frac{1}{n^{m-1}}\right)^n \leqslant\left(\frac{1}{1-\frac{1}{n}}\right)^n \Rightarrow 
\lim _{n \rightarrow \infty} A^n=e
 
\lim _{n \rightarrow \infty}\left(A+\frac{1}{n^{m-1}}\right)^n=\lim _{n \rightarrow \infty}\left(A+\frac{1}{2 n^{m-1}}\right)^n=e?
","['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'products']"
62,A function with positive $n$-th derivative has at most $n$ roots – an inequality version of the Fundamental theorem of Algebra.,A function with positive -th derivative has at most  roots – an inequality version of the Fundamental theorem of Algebra.,n n,"Claim: Let $n\in \mathbb N$ , and let $f:\mathbb R \to \mathbb R$ be such that its $n$ -th derivative $f^{(n)}(x)>0, \ \forall x\in \mathbb R$ , then $f$ has at most $n$ roots. Context: The fundamental theorem of algebra states (when only the real numbers are concerned) that an $n$ -th degree polynomial has at most $n$ real roots. Since an $n$ -th degree polynomial can be characterized as a function $f:\mathbb R \to \mathbb R$ with constant $n$ -th derivative, the fundamental theorem of algebra can be equivalently stated as: Let $n\in \mathbb N$ , and let $f:\mathbb R \to \mathbb R$ has constant $n$ -th derivative, then $f$ has at most $n$ roots. Analogy: Thus the claim I'm questioning can be interpreted as a version of the fundamental theorem of algebra with the condition that the function's $n$ -th derivative is constant replaced by the condition that the function's $n$ -th derivative is strictly positive. Special cases: for $n=1$ the claim follows from the fact that strictly increasing functions have at most one root; likewise for $n=2$ the claim follows from the fact that strictly increasing functions have at most two roots (strictly convex function is strictly negative between any two roots, thus there can not be a third root). Does the claim hold for $n\geq 3$ as well?","Claim: Let , and let be such that its -th derivative , then has at most roots. Context: The fundamental theorem of algebra states (when only the real numbers are concerned) that an -th degree polynomial has at most real roots. Since an -th degree polynomial can be characterized as a function with constant -th derivative, the fundamental theorem of algebra can be equivalently stated as: Let , and let has constant -th derivative, then has at most roots. Analogy: Thus the claim I'm questioning can be interpreted as a version of the fundamental theorem of algebra with the condition that the function's -th derivative is constant replaced by the condition that the function's -th derivative is strictly positive. Special cases: for the claim follows from the fact that strictly increasing functions have at most one root; likewise for the claim follows from the fact that strictly increasing functions have at most two roots (strictly convex function is strictly negative between any two roots, thus there can not be a third root). Does the claim hold for as well?","n\in \mathbb N f:\mathbb R \to \mathbb R n f^{(n)}(x)>0, \ \forall x\in \mathbb R f n n n n f:\mathbb R \to \mathbb R n n\in \mathbb N f:\mathbb R \to \mathbb R n f n n n n=1 n=2 n\geq 3","['real-analysis', 'derivatives', 'roots', 'rolles-theorem', 'real-algebraic-geometry']"
63,"$(X,\mu)$ is a measure space. Show that, $L^\infty(X;\mu)$ is either finite dimensional or non-separable.","is a measure space. Show that,  is either finite dimensional or non-separable.","(X,\mu) L^\infty(X;\mu)","Suppose $L^\infty(X;\mu)$ is not finite dimensional. We have to prove it is non-separable. Suppose not i.e. $L^\infty(X,\mu)$ is separable. Then $\mathcal{F}=\{f\in L^\infty(X;\mu):\  \text{Range}(f)\subseteq\{0,1\}\}=\{\chi_A:\ A\text{ is measurable}\}$ is separable in $\lVert\cdot\rVert_\infty$ norm. I have proved that for $f,g\in\mathcal{F}$ with $f\ne g$ , we have $\lVert f-g\rVert_\infty=1$ . Hence, $\mathcal{F}$ is discrete space. As $\mathcal{F}$ is separable, $\mathcal{F}$ should be countable. From here, I want to prove $L^\infty(X;\mu)$ is finite dimensional. Usually the cardinality of the set $\{f:X\to\Bbb{C}:\ \text{Range}(f)\subseteq\{0,1\}\}$ is $2^{|X|}$ but when we consider the functions as member of $L^\infty$ , two distict maps $f,g$ (as set theoretic) may be equal more specifically, $\chi_A=\chi_B\iff \mu(A\triangle B)=0$ . Can anyone help me complete the proof? Thanks for your help in advance.","Suppose is not finite dimensional. We have to prove it is non-separable. Suppose not i.e. is separable. Then is separable in norm. I have proved that for with , we have . Hence, is discrete space. As is separable, should be countable. From here, I want to prove is finite dimensional. Usually the cardinality of the set is but when we consider the functions as member of , two distict maps (as set theoretic) may be equal more specifically, . Can anyone help me complete the proof? Thanks for your help in advance.","L^\infty(X;\mu) L^\infty(X,\mu) \mathcal{F}=\{f\in L^\infty(X;\mu):\  \text{Range}(f)\subseteq\{0,1\}\}=\{\chi_A:\ A\text{ is measurable}\} \lVert\cdot\rVert_\infty f,g\in\mathcal{F} f\ne g \lVert f-g\rVert_\infty=1 \mathcal{F} \mathcal{F} \mathcal{F} L^\infty(X;\mu) \{f:X\to\Bbb{C}:\ \text{Range}(f)\subseteq\{0,1\}\} 2^{|X|} L^\infty f,g \chi_A=\chi_B\iff \mu(A\triangle B)=0","['real-analysis', 'functional-analysis', 'measure-theory', 'normed-spaces', 'lp-spaces']"
64,Baby Rudin Theorem 7.18,Baby Rudin Theorem 7.18,,"Here is Theorem 7.18 from Baby Rudin: There exists a real continuous function on the real line which is nowhere differentiable. Here is a proof of the theorem: Define $$\tag{34} \varphi(x) = \lvert x \rvert \qquad \qquad (-1 \leq x \leq 1) $$ and extend the definition of $\varphi(x)$ to all real $x$ by requiring that $$ \tag{35} \varphi(x+2) = \varphi(x). $$ Then, for all $s$ and $t$ , $$\tag{36}  \lvert \varphi(s) - \varphi(t) \rvert \leq \lvert s-t \rvert. $$ In particular, $\varphi$ is continuous on $\mathbb{R}^1$ . Define $$ \tag{37} f(x) = \sum_{n=0}^\infty \left( \frac{3}{4} \right)^n \varphi \left( 4^n x \right). $$ Since $0 \leq \varphi \leq 1$ , Theorem 7.10 shows that the series (37) converges uniformly on $\mathbb{R}^1$ . By Theorem 7.12, $f$ is continuous on $\mathbb{R}^1$ . Now fix a real number $x$ and a positive integer $m$ . Put $$ \tag{38} \delta_m = \pm \frac{1}{2} \cdot 4^{-m} $$ where the sign is so chosen that no integer lies between $4^m x$ and $4^m \left( x + \delta_m \right)$ . This can be done, since $4^m \left\lvert \delta_m \right\rvert = \frac{1}{2}$ . Define $$ \tag{39} \gamma_n = \frac{ \varphi \left( 4^n \left( x + \delta_m \right)  \right) - \varphi \left( 4^n x \right)  }{ \delta_m }. $$ When $n > m$ , then $4^n \delta_m$ is an even integer, so that $\gamma_n = 0$ . When $0 \leq n \leq m$ , (36) implies that $\left\lvert \gamma_n \right\rvert \leq 4^n$ . Since $\left\lvert \gamma_m \right\rvert = 4^m$ , we conclude that $$ \begin{align} \left\lvert \frac{ f \left( x + \delta_m \right) - f(x)  }{ \delta_m  }  \right\rvert &= \left\lvert \sum_{n=0}^m \left( \frac{3}{4} \right)^n \gamma_n  \right\rvert \\  &\geq 3^m - \sum_{n=0}^{m-1} 3^n \\ &= \frac{1}{2} \left( 3^m + 1 \right). \end{align} $$ As $m \to \infty$ , $\gamma_m \to 0$ . It follows that $f$ is not differentiable at $x$ . I have two questions regarding the proof. At $(38)$ it's stated that the sign is so chosen that no integer lies between $4^m x$ and $4^m \left( x + \delta_m \right)$ . Why is it necessary that no integer should be within that range and what would happen if they were? At the later part of the proof we have: $$ \begin{align} \left\lvert \frac{ f \left( x + \delta_m \right) - f(x)  }{ \delta_m  }  \right\rvert &= \left\lvert \sum_{n=0}^m \left( \frac{3}{4} \right)^n \gamma_n  \right\rvert \\  &\geq 3^m - \sum_{n=0}^{m-1} 3^n \\ &= \frac{1}{2} \left( 3^m + 1 \right). \end{align} $$ I want to know why is the following part true: $$ \begin{align} \left\lvert \sum_{n=0}^m \left( \frac{3}{4} \right)^n \gamma_n  \right\rvert  &\geq 3^m - \sum_{n=0}^{m-1} 3^n \\ \end{align} $$ Any help is appreciated!","Here is Theorem 7.18 from Baby Rudin: There exists a real continuous function on the real line which is nowhere differentiable. Here is a proof of the theorem: Define and extend the definition of to all real by requiring that Then, for all and , In particular, is continuous on . Define Since , Theorem 7.10 shows that the series (37) converges uniformly on . By Theorem 7.12, is continuous on . Now fix a real number and a positive integer . Put where the sign is so chosen that no integer lies between and . This can be done, since . Define When , then is an even integer, so that . When , (36) implies that . Since , we conclude that As , . It follows that is not differentiable at . I have two questions regarding the proof. At it's stated that the sign is so chosen that no integer lies between and . Why is it necessary that no integer should be within that range and what would happen if they were? At the later part of the proof we have: I want to know why is the following part true: Any help is appreciated!","\tag{34} \varphi(x) = \lvert x \rvert \qquad \qquad (-1 \leq x \leq 1)  \varphi(x) x  \tag{35} \varphi(x+2) = \varphi(x).  s t \tag{36}  \lvert \varphi(s) - \varphi(t) \rvert \leq \lvert s-t \rvert.  \varphi \mathbb{R}^1  \tag{37} f(x) = \sum_{n=0}^\infty \left( \frac{3}{4} \right)^n \varphi \left( 4^n x \right).  0 \leq \varphi \leq 1 \mathbb{R}^1 f \mathbb{R}^1 x m  \tag{38} \delta_m = \pm \frac{1}{2} \cdot 4^{-m}  4^m x 4^m \left( x + \delta_m \right) 4^m \left\lvert \delta_m \right\rvert = \frac{1}{2}  \tag{39} \gamma_n = \frac{ \varphi \left( 4^n \left( x + \delta_m \right)  \right) - \varphi \left( 4^n x \right)  }{ \delta_m }.  n > m 4^n \delta_m \gamma_n = 0 0 \leq n \leq m \left\lvert \gamma_n \right\rvert \leq 4^n \left\lvert \gamma_m \right\rvert = 4^m 
\begin{align}
\left\lvert \frac{ f \left( x + \delta_m \right) - f(x)  }{ \delta_m  }  \right\rvert &= \left\lvert \sum_{n=0}^m \left( \frac{3}{4} \right)^n \gamma_n  \right\rvert \\ 
&\geq 3^m - \sum_{n=0}^{m-1} 3^n \\
&= \frac{1}{2} \left( 3^m + 1 \right).
\end{align}
 m \to \infty \gamma_m \to 0 f x (38) 4^m x 4^m \left( x + \delta_m \right) 
\begin{align}
\left\lvert \frac{ f \left( x + \delta_m \right) - f(x)  }{ \delta_m  }  \right\rvert &= \left\lvert \sum_{n=0}^m \left( \frac{3}{4} \right)^n \gamma_n  \right\rvert \\ 
&\geq 3^m - \sum_{n=0}^{m-1} 3^n \\
&= \frac{1}{2} \left( 3^m + 1 \right).
\end{align}
 
\begin{align}
\left\lvert \sum_{n=0}^m \left( \frac{3}{4} \right)^n \gamma_n  \right\rvert 
&\geq 3^m - \sum_{n=0}^{m-1} 3^n \\
\end{align}
","['real-analysis', 'sequences-and-series', 'analysis', 'derivatives']"
65,Real Analysis Covid lectures using Rudin?,Real Analysis Covid lectures using Rudin?,,Just wondering if someone can point me to any intro real analysis video lectures that used Rudin. Because of Covid I thought that there might be a lot more out there but a cursory glance doesn't seem to give results.,Just wondering if someone can point me to any intro real analysis video lectures that used Rudin. Because of Covid I thought that there might be a lot more out there but a cursory glance doesn't seem to give results.,,"['real-analysis', 'reference-request']"
66,How to prove that the logarithm is a transcendental function,How to prove that the logarithm is a transcendental function,,"Let's consider the function $\log x$ ; how can I prove that it is a transcendental function on the function field of rational functions, i.e. that a polynomial in two variables $p(x,y)$ such that $p(x,\log x)=0$ identically does not exist? I have been trying different approaches: seeing the logarithm on the real numbers, like a formal series or like a holomorphic function on an open in the complex plane, but I was not able to do this. I have also tried looking for the differences with $\sqrt{(1+x)}$ which is algebraic on the rational function, and can be defined on a subset of the reals, with a formal series or on an open subset of the complex plane. Could you please help me?","Let's consider the function ; how can I prove that it is a transcendental function on the function field of rational functions, i.e. that a polynomial in two variables such that identically does not exist? I have been trying different approaches: seeing the logarithm on the real numbers, like a formal series or like a holomorphic function on an open in the complex plane, but I was not able to do this. I have also tried looking for the differences with which is algebraic on the rational function, and can be defined on a subset of the reals, with a formal series or on an open subset of the complex plane. Could you please help me?","\log x p(x,y) p(x,\log x)=0 \sqrt{(1+x)}","['real-analysis', 'complex-analysis', 'analysis', 'functions', 'galois-theory']"
67,"Calculate $\lim_{n\rightarrow \infty }\int_{0}^{\pi /2}\sqrt[n]{\sin^nx+\cos^nx}\,dx$",Calculate,"\lim_{n\rightarrow \infty }\int_{0}^{\pi /2}\sqrt[n]{\sin^nx+\cos^nx}\,dx","I solved an interesting limit sometime, maybe someone will suggest a simpler solution, perhaps through the Lebesgue measure. \begin{align*} \lim_{n\rightarrow \infty }\int_{0}^{\pi /2}\sqrt[n]{\sin^nx+\cos^nx}\,dx&=\lim_{n\rightarrow \infty }\left [\int_{0}^{\pi /4}\sqrt[n]{\sin^nx+\cos^nx}\,dx+\int_{\pi /4}^{\pi /2}\sqrt[n]{\sin^nx+\cos^nx} \,dx\right ]\\ &=\lim_{n\rightarrow \infty} \left [ \int_{0}^{\pi /4}\sqrt[n]{\sin^nx+\cos^nx}\,dx+\int_{-\pi /4}^{0}\sqrt[n]{\cos^nx+\left ( -\sin x \right )^n} \,dx\right]\\& =2\lim_{n\rightarrow \infty }\int_{0}^{\pi /4}\sqrt[n]{\sin^nx+\cos^nx}\,dx \\ &=2\lim_{n\rightarrow \infty }\int_{0}^{\pi /4}\cos x\sqrt[n]{\tan^nx+1}\,dx\\ &=\sqrt{2} \end{align*} $$\int_{0}^{\pi /4}\cos x\,dx<\int_{0}^{\pi /4}\operatorname{cos}x\sqrt[n]{\tan^nx+1}\,dx<\sqrt[n]{2}\int_{0}^{\pi /4}\cos x\,dx$$","I solved an interesting limit sometime, maybe someone will suggest a simpler solution, perhaps through the Lebesgue measure.","\begin{align*}
\lim_{n\rightarrow \infty }\int_{0}^{\pi /2}\sqrt[n]{\sin^nx+\cos^nx}\,dx&=\lim_{n\rightarrow \infty }\left [\int_{0}^{\pi /4}\sqrt[n]{\sin^nx+\cos^nx}\,dx+\int_{\pi /4}^{\pi /2}\sqrt[n]{\sin^nx+\cos^nx} \,dx\right ]\\
&=\lim_{n\rightarrow \infty} \left [ \int_{0}^{\pi /4}\sqrt[n]{\sin^nx+\cos^nx}\,dx+\int_{-\pi /4}^{0}\sqrt[n]{\cos^nx+\left ( -\sin x \right )^n} \,dx\right]\\&
=2\lim_{n\rightarrow \infty }\int_{0}^{\pi /4}\sqrt[n]{\sin^nx+\cos^nx}\,dx
\\
&=2\lim_{n\rightarrow \infty }\int_{0}^{\pi /4}\cos x\sqrt[n]{\tan^nx+1}\,dx\\
&=\sqrt{2}
\end{align*} \int_{0}^{\pi /4}\cos x\,dx<\int_{0}^{\pi /4}\operatorname{cos}x\sqrt[n]{\tan^nx+1}\,dx<\sqrt[n]{2}\int_{0}^{\pi /4}\cos x\,dx","['real-analysis', 'calculus', 'integration']"
68,Null measure set and integrals,Null measure set and integrals,,"Let $f,g:[a,b] \rightarrow \mathbb{R}$ be integrable functions and $$X=\{x \in [a,b];f(x) \ne g(x)\}$$ be a set with zero measure. Prove that $$\int_a^b f(x)dx=\int_a^b g(x)dx$$ and show two functions $f \ne g$ such that $X$ is infinite and has zero measure. How can i do this in a context of Riemann integration? I saw some answers in other questions but it wasn't very helpful.",Let be integrable functions and be a set with zero measure. Prove that and show two functions such that is infinite and has zero measure. How can i do this in a context of Riemann integration? I saw some answers in other questions but it wasn't very helpful.,"f,g:[a,b] \rightarrow \mathbb{R} X=\{x \in [a,b];f(x) \ne g(x)\} \int_a^b f(x)dx=\int_a^b g(x)dx f \ne g X","['real-analysis', 'integration', 'measure-theory']"
69,"Is ""Convolution operator"" well-defined and injective?","Is ""Convolution operator"" well-defined and injective?",,"Problem: Let $g_t(x)=\frac{1}{\sqrt{2\pi}} e^{ \frac{- (x-t)^2}{2}}$ be the Gaussian function centered at $t\in \mathbb R$ . Let $I$ be a subset of $\mathbb R$ . Consider the ""convolution operator"" \begin{align} \mathcal A: L^1(I) & \longrightarrow L^2(I)\\ f & \longmapsto \left(x\longmapsto  \int_{I} g_t(x)f(t) dt \right). \end{align} How can I prove (or disprove) that the operator is well-defined and injective? My attempts: Here are some of my attempts to prove the well-defined property. I have tried to use Cauchy inequality but with no success: \begin{align} \int_{I} \left( \int_{I} g_t(x)f(t)dt\right)^2 dx \leq \int_{I} \int_{I} g_t^2(x)dx dt \int_{I}f^2(t)dt. \end{align} as the right-hand-side may be infinite as $f$ does not belong to $L^2(I)$ . I have tried to use the boundedness of Gaussian function but nothing better. We have \begin{align} \int_{I} \left( \int_{I} g_t(x)f(t)dt\right)^2 dx \leq M \left( \int_{I} f(t)dt \right)^2 \int_I dx. \end{align} and the right-hand-side may be infinite as $I$ is unbounded.","Problem: Let be the Gaussian function centered at . Let be a subset of . Consider the ""convolution operator"" How can I prove (or disprove) that the operator is well-defined and injective? My attempts: Here are some of my attempts to prove the well-defined property. I have tried to use Cauchy inequality but with no success: as the right-hand-side may be infinite as does not belong to . I have tried to use the boundedness of Gaussian function but nothing better. We have and the right-hand-side may be infinite as is unbounded.","g_t(x)=\frac{1}{\sqrt{2\pi}} e^{ \frac{- (x-t)^2}{2}} t\in \mathbb R I \mathbb R \begin{align}
\mathcal A: L^1(I) & \longrightarrow L^2(I)\\
f & \longmapsto \left(x\longmapsto  \int_{I} g_t(x)f(t) dt \right).
\end{align} \begin{align}
\int_{I} \left( \int_{I} g_t(x)f(t)dt\right)^2 dx \leq \int_{I} \int_{I} g_t^2(x)dx dt \int_{I}f^2(t)dt.
\end{align} f L^2(I) \begin{align}
\int_{I} \left( \int_{I} g_t(x)f(t)dt\right)^2 dx \leq
M \left( \int_{I} f(t)dt \right)^2 \int_I dx.
\end{align} I","['real-analysis', 'calculus', 'analysis', 'operator-theory']"
70,Does every positive Lebesgue measure set in $\mathbb{R}^2$ contain a product of positive Lebesgue measure sets up to a null set?,Does every positive Lebesgue measure set in  contain a product of positive Lebesgue measure sets up to a null set?,\mathbb{R}^2,"Let $P\subset \mathbb{R}^2$ be a positive Lebesgue measure set. Then $P$ may not contain a subset of the form $A\times B$ where $A,B\subset \mathbb{R}$ are of positive Lebesgue measure. For example consider $P=\{(x,y)\in [0,1]\times[0,1]:x-y\notin \mathbb{Q}\}.$ This example leads me to ask: Given any $P\subset \mathbb{R}^2,$ a positive Lebesgue measure set, does there exists a measure zero set $U\subset \mathbb{R}^2$ such that $P\cup U$ contains a subset of the form $A\times B$ where $A,B\subset \mathbb{R}$ are of positive Lebesgue measure?","Let be a positive Lebesgue measure set. Then may not contain a subset of the form where are of positive Lebesgue measure. For example consider This example leads me to ask: Given any a positive Lebesgue measure set, does there exists a measure zero set such that contains a subset of the form where are of positive Lebesgue measure?","P\subset \mathbb{R}^2 P A\times B A,B\subset \mathbb{R} P=\{(x,y)\in [0,1]\times[0,1]:x-y\notin \mathbb{Q}\}. P\subset \mathbb{R}^2, U\subset \mathbb{R}^2 P\cup U A\times B A,B\subset \mathbb{R}","['real-analysis', 'measure-theory', 'lebesgue-measure', 'examples-counterexamples']"
71,Show that $\lim\limits_{h \rightarrow 0}\frac{f(\xi+h)-f(\xi-h)}{2h}$ exists and is $f'(\xi)$,Show that  exists and is,\lim\limits_{h \rightarrow 0}\frac{f(\xi+h)-f(\xi-h)}{2h} f'(\xi),"Let $f:I \longrightarrow \mathbb{R}$ be differentiable in a inner point $\xi \in I$ . Show that $$\lim\limits_{h \rightarrow 0}\frac{f(\xi+h)-f(\xi-h)}{2h}$$ exists and is $f'(\xi)$ . Also give an example which shows, that the existence of this limit, doesnt mean, that $f$ is necessary differentiable in $\xi$ . My attempt: Given is that: $$\lim\limits_{h\rightarrow0}\frac{f(\xi+h)-f(\xi)}{h}=f'(\xi)$$ exists. If $\,\lim\limits_{h \rightarrow 0}\frac{f(\xi+h)-f(\xi-h)}{2h}=\lim\limits_{h\rightarrow0}\frac{f(\xi+h)-f(\xi)}{h}=f'(\xi)$ then: $$\lim\limits_{h\rightarrow0}\left(\frac{f(\xi+h)-f(\xi)}{h}-\frac{f(\xi+h)-f(\xi-h)}{2h}\right)=0$$ $$\lim\limits_{h\rightarrow0}\frac{2(f(\xi+h)-f(\xi))-f(\xi+h)+f(\xi-h)}{2h}=\lim\limits_{h\rightarrow0}\frac{f(\xi+h)-f(\xi)+f(\xi-h)-f(\xi)}{2h}=0$$ $$=\frac{1}{2}\lim\limits_{h\rightarrow0}\frac{f(\xi+h)-f(\xi)}{h}+\frac{1}{2}\lim\limits_{h\rightarrow0}\frac{f(\xi-h)-f(\xi)}{h}=\frac{1}{2}\lim\limits_{h\rightarrow0}\frac{f(\xi+h)-f(\xi)}{h}-\frac{1}{2}\lim\limits_{h\rightarrow0}\frac{f(\xi)-f(\xi-h)}{h}=\frac{1}{2}f'(\xi)-\frac{1}{2}f'(\xi)=0$$ $\Box$ Let $f:\mathbb{R}\longrightarrow \mathbb{R}:x \mapsto |x|$ Then for $\xi=0$ : $$\lim\limits_{h \rightarrow 0}\frac{f(\xi+h)-f(\xi-h)}{2h}=\lim\limits_{h \rightarrow 0}\frac{|h|-|-h|}{2h}=\lim\limits_{h \rightarrow 0}\frac{|h|-|h|}{2h}=\lim\limits_{h \rightarrow 0}0=0$$ $\Longrightarrow$ the limit exists, but $f$ is not differentiatable at $\xi=0$ Hello, as always it would help me alot, if someone could look over it and give me feedback weither my work is correct. And if not, what is wrong :) thank you","Let be differentiable in a inner point . Show that exists and is . Also give an example which shows, that the existence of this limit, doesnt mean, that is necessary differentiable in . My attempt: Given is that: exists. If then: Let Then for : the limit exists, but is not differentiatable at Hello, as always it would help me alot, if someone could look over it and give me feedback weither my work is correct. And if not, what is wrong :) thank you","f:I \longrightarrow \mathbb{R} \xi \in I \lim\limits_{h \rightarrow 0}\frac{f(\xi+h)-f(\xi-h)}{2h} f'(\xi) f \xi \lim\limits_{h\rightarrow0}\frac{f(\xi+h)-f(\xi)}{h}=f'(\xi) \,\lim\limits_{h \rightarrow 0}\frac{f(\xi+h)-f(\xi-h)}{2h}=\lim\limits_{h\rightarrow0}\frac{f(\xi+h)-f(\xi)}{h}=f'(\xi) \lim\limits_{h\rightarrow0}\left(\frac{f(\xi+h)-f(\xi)}{h}-\frac{f(\xi+h)-f(\xi-h)}{2h}\right)=0 \lim\limits_{h\rightarrow0}\frac{2(f(\xi+h)-f(\xi))-f(\xi+h)+f(\xi-h)}{2h}=\lim\limits_{h\rightarrow0}\frac{f(\xi+h)-f(\xi)+f(\xi-h)-f(\xi)}{2h}=0 =\frac{1}{2}\lim\limits_{h\rightarrow0}\frac{f(\xi+h)-f(\xi)}{h}+\frac{1}{2}\lim\limits_{h\rightarrow0}\frac{f(\xi-h)-f(\xi)}{h}=\frac{1}{2}\lim\limits_{h\rightarrow0}\frac{f(\xi+h)-f(\xi)}{h}-\frac{1}{2}\lim\limits_{h\rightarrow0}\frac{f(\xi)-f(\xi-h)}{h}=\frac{1}{2}f'(\xi)-\frac{1}{2}f'(\xi)=0 \Box f:\mathbb{R}\longrightarrow \mathbb{R}:x \mapsto |x| \xi=0 \lim\limits_{h \rightarrow 0}\frac{f(\xi+h)-f(\xi-h)}{2h}=\lim\limits_{h \rightarrow 0}\frac{|h|-|-h|}{2h}=\lim\limits_{h \rightarrow 0}\frac{|h|-|h|}{2h}=\lim\limits_{h \rightarrow 0}0=0 \Longrightarrow f \xi=0","['real-analysis', 'limits', 'derivatives']"
72,"Is there a formula for ""power integration,"" with integrand $f(x)^{dx}-1$, rather than $f(x)dx$?","Is there a formula for ""power integration,"" with integrand , rather than ?",f(x)^{dx}-1 f(x)dx,"I was joking around with integrals, thinking, ""what else can I do to a function (such as $f(x)dx$ ) that results in something I can sum infinitely as in an integral?"" The first thing I came up with was $f(x)^{dx}-1$ . Loosely speaking, since $dx$ is infinitely small, this expression is infinitesimal, so it could be added as though it were a Riemann integral. This may be something new, but I have stumbled across something new, like integration, but defined differently. Similar to a Riemann integral adding up an infinite number of ""slices"" of a function, represented by $f(x)dx$ , this construction adds up an infinite number of these: $f(x)^{dx}-1$ . I'm not sure how to intuitively describe it (like the ""slices"" description on a Riemann integral). It is similar to a product integral in that $dx$ is in the exponent, but different in that the terms of the product integral are added rather than multiplied (subtracting 1 after each term so that the sum doesn't diverge). The notation would be something along the lines of this: $$\int_a^b\left(f(x)^{dx}-1\right)$$ A more formal definition, similar to the definition of a Riemann integral, would be as follows: $$\int_a^b\left(f(x)^{dx}-1\right)=\lim_{n\to\infty}\sum_{k = 0}^{n-1} \left[-1+f\left(a+\frac{b-a}{n}k\right)^{\frac{b-a}{n}}\right]$$ Note: since raising a negative number to an irrational power is undefined on the real numbers, the argument $f(x)$ must be positive on the interval $(a,b)$ . There may be a way around this, but I haven't found it because evaluating limits of sums is very difficult when the indices are in the exponents. I have discovered several interesting results from this ""power integral,"" by experimentation. None of these have I been able to prove, but by approximating these sums (up to 500 terms) on Desmos, I have been able to conclude at least some properties: $$\int_a^b\left(\left(f(x)^{-1}\right)^{dx}-1\right) = -\int_a^b\left(f(x)^{dx}-1\right)$$ or, equivalently, $$\int_a^b\left(f(x)^{-dx}-1\right) = -\int_a^b\left(f(x)^{dx}-1\right)$$ ""Power integrals"" do not add by their intervals, $$\int_a^b\left(f(x)^{dx}-1\right)\neq\int_a^c\left(f(x)^{dx}-1\right)+\int_c^b\left(f(x)^{dx}-1\right)$$ nor do they preserve linearity, as a result of the differential being an exponent rather than a factor. $$\int_a^b\left(\left(f(x)+g(x)\right)^{dx}-1\right)\neq\int_a^b\left(f(x)^{dx}-1\right)+\int_a^b\left(g(x)^{dx}-1\right)$$ Examples of some things I tried and their outputs: $$\int_0^x\left(C^{dt}-1\right)=0$$ $$\int_0^x\left(\left(Ct\right)^{dt}-1\right)=x\left(\ln(Cx)-1\right)$$ $$\int_0^x\left(\left(t^k\right)^{dt}-1\right)=kx\left(\ln(x)-1\right)$$ $$\int_0^x\left(\left(e^t\right)^{dt}-1\right)=\frac{1}{2}x^2$$","I was joking around with integrals, thinking, ""what else can I do to a function (such as ) that results in something I can sum infinitely as in an integral?"" The first thing I came up with was . Loosely speaking, since is infinitely small, this expression is infinitesimal, so it could be added as though it were a Riemann integral. This may be something new, but I have stumbled across something new, like integration, but defined differently. Similar to a Riemann integral adding up an infinite number of ""slices"" of a function, represented by , this construction adds up an infinite number of these: . I'm not sure how to intuitively describe it (like the ""slices"" description on a Riemann integral). It is similar to a product integral in that is in the exponent, but different in that the terms of the product integral are added rather than multiplied (subtracting 1 after each term so that the sum doesn't diverge). The notation would be something along the lines of this: A more formal definition, similar to the definition of a Riemann integral, would be as follows: Note: since raising a negative number to an irrational power is undefined on the real numbers, the argument must be positive on the interval . There may be a way around this, but I haven't found it because evaluating limits of sums is very difficult when the indices are in the exponents. I have discovered several interesting results from this ""power integral,"" by experimentation. None of these have I been able to prove, but by approximating these sums (up to 500 terms) on Desmos, I have been able to conclude at least some properties: or, equivalently, ""Power integrals"" do not add by their intervals, nor do they preserve linearity, as a result of the differential being an exponent rather than a factor. Examples of some things I tried and their outputs:","f(x)dx f(x)^{dx}-1 dx f(x)dx f(x)^{dx}-1 dx \int_a^b\left(f(x)^{dx}-1\right) \int_a^b\left(f(x)^{dx}-1\right)=\lim_{n\to\infty}\sum_{k = 0}^{n-1} \left[-1+f\left(a+\frac{b-a}{n}k\right)^{\frac{b-a}{n}}\right] f(x) (a,b) \int_a^b\left(\left(f(x)^{-1}\right)^{dx}-1\right) = -\int_a^b\left(f(x)^{dx}-1\right) \int_a^b\left(f(x)^{-dx}-1\right) = -\int_a^b\left(f(x)^{dx}-1\right) \int_a^b\left(f(x)^{dx}-1\right)\neq\int_a^c\left(f(x)^{dx}-1\right)+\int_c^b\left(f(x)^{dx}-1\right) \int_a^b\left(\left(f(x)+g(x)\right)^{dx}-1\right)\neq\int_a^b\left(f(x)^{dx}-1\right)+\int_a^b\left(g(x)^{dx}-1\right) \int_0^x\left(C^{dt}-1\right)=0 \int_0^x\left(\left(Ct\right)^{dt}-1\right)=x\left(\ln(Cx)-1\right) \int_0^x\left(\left(t^k\right)^{dt}-1\right)=kx\left(\ln(x)-1\right) \int_0^x\left(\left(e^t\right)^{dt}-1\right)=\frac{1}{2}x^2","['real-analysis', 'calculus', 'integration']"
73,On the derivative of a function that is its own inverse,On the derivative of a function that is its own inverse,,"I was recently doing the following question: Let $f$ be a differentiable function such that $f(f(x))$ = $x$ for all $x\in[0,1]$ . Suppose $f(0)=1$ . Determine the value of $$\int_0^1(x-f(x))^{2016} dx$$ Now, in the light of the fact that $f(f(x))=x$ I thought the substitution $t=f(x)$ may be something to investigate. Of course, if $t=f(x)$ then $dx = {dt\over f’(x)}$ so we may write the integral as $$\int_1^0(f(t)-t){dt\over f’(x)}$$ Now, I noticed that we might be able to exploit the fact* that $f(x)$ is its own inverse to resolve $f’(x)$ . Notice that $${f^{-1}}’(x) = \frac 1{f’(x)}$$ But, since $f(x)=f^{-1}(x)$ we have $$(f’(x))^2 = 1$$ save for the degenerate case that $f’(x) = 0$ — which if true means the function isn’t invertible in the first place (even if it’s only at some certain points those may be extrema and may challenge the invertibility of the function, but I digress). But obviously it is not necessary that a function’s derivative to be $1$ or $-1$ for it to be its own inverse, right? I thought of a function on $[0,\infty)$ $$g(x)=\left(a-x^n\right)^\frac 1n$$ (where $n$ is a natural number) whose derivative is $(1-n)x^{n-1}(a-x^n)^{\frac 1n -1}$ (clearly not $\pm 1$ ) and its inverse is $g^{-1}(x)=(a-x^n)^\frac 1n$ (clearly $g(x)$ , not even making an attempt to disguise itself !). So what’s the deal here? Please help me make sense of it. *I figured that it isn’t actually necessarily true that $f(f(x)) = x$ means $f \equiv f^{-1}$ , since $f^{-1}$ may not even exist, and I ended up solving that problem by instead trying the substitution $x=f(t)$ . Interestingly, continuing with the original substitution and assumption that $f(x)=f^{-1}(x)$ and writing $f’(x)$ as a function of $t$ is consistent with the other substitution. However, this has no actual bearing on the question.","I was recently doing the following question: Let be a differentiable function such that = for all . Suppose . Determine the value of Now, in the light of the fact that I thought the substitution may be something to investigate. Of course, if then so we may write the integral as Now, I noticed that we might be able to exploit the fact* that is its own inverse to resolve . Notice that But, since we have save for the degenerate case that — which if true means the function isn’t invertible in the first place (even if it’s only at some certain points those may be extrema and may challenge the invertibility of the function, but I digress). But obviously it is not necessary that a function’s derivative to be or for it to be its own inverse, right? I thought of a function on (where is a natural number) whose derivative is (clearly not ) and its inverse is (clearly , not even making an attempt to disguise itself !). So what’s the deal here? Please help me make sense of it. *I figured that it isn’t actually necessarily true that means , since may not even exist, and I ended up solving that problem by instead trying the substitution . Interestingly, continuing with the original substitution and assumption that and writing as a function of is consistent with the other substitution. However, this has no actual bearing on the question.","f f(f(x)) x x\in[0,1] f(0)=1 \int_0^1(x-f(x))^{2016} dx f(f(x))=x t=f(x) t=f(x) dx = {dt\over f’(x)} \int_1^0(f(t)-t){dt\over f’(x)} f(x) f’(x) {f^{-1}}’(x) = \frac 1{f’(x)} f(x)=f^{-1}(x) (f’(x))^2 = 1 f’(x) = 0 1 -1 [0,\infty) g(x)=\left(a-x^n\right)^\frac 1n n (1-n)x^{n-1}(a-x^n)^{\frac 1n -1} \pm 1 g^{-1}(x)=(a-x^n)^\frac 1n g(x) f(f(x)) = x f \equiv f^{-1} f^{-1} x=f(t) f(x)=f^{-1}(x) f’(x) t","['real-analysis', 'calculus', 'derivatives', 'contest-math']"
74,Resistant integral $\int_0^1\left(\frac{\ln^2(1-x)\ln^2(1+x)}{1-x}-\frac{\ln^2(2)\ln^2(1-x)}{1-x}\right)\ dx$,Resistant integral,\int_0^1\left(\frac{\ln^2(1-x)\ln^2(1+x)}{1-x}-\frac{\ln^2(2)\ln^2(1-x)}{1-x}\right)\ dx,"Prove, without using harmonic series, that $$I=\int_0^1\left(\frac{\ln^2(1-x)\ln^2(1+x)}{1-x}-\frac{\ln^2(2)\ln^2(1-x)}{1-x}\right)\ dx$$ $$=\frac18\zeta(5)-\frac12\ln2\zeta(4)+2\ln^22\zeta(3)-\frac23\ln^32\zeta(2)-2\zeta(2)\zeta(3)+\frac1{10}\ln^52+4\operatorname{Li}_5\left(\frac12\right)$$ This problem was proposed by Cornel and can be found here . The main reason behind such constraint is that this integral can be simplified into $S=\sum_{n=1}^\infty\frac{H_n}{n^42^n}$ which was calculated here using real and complex methods. So evaluating $I$ without using harmonic series means we are providing a third solution to $S$ . I have already computed this integral ( will be posted soon) but I would like to see variant approaches. Thanks. Added: In case the reader is curious about how this integral is related to $\sum_{n=1}^\infty\frac{H_n}{n^42^n}$ , here is the steps By integration by parts we have \begin{align} I&=\frac23\int_0^1\frac{\ln^3(1-x)\ln(1+x)}{1+x}\ dx\overset{\color{red}{1-x\ \mapsto\ x}}{=}\frac13\int_0^1\frac{\ln^3x\ln(2-x)}{1-x/2}\ dx\\ &=\frac{\ln2}{3}\int_0^1\frac{\ln^3x}{1-x/2}\ dx+\frac13\int_0^1\frac{\ln^3x\ln(1-x/2)}{1-x/2}\ dx\\ &=\frac{\ln2}{3}\sum_{n=1}^\infty\frac{1}{2^{n-1}}\int_0^1x^{n-1}\ln^3x\ dx-\frac13\sum_{n=1}^\infty\frac{H_n}{2^n}\int_0^1x^n\ln^3x\ dx\\ &=\frac{\ln2}{3}\sum_{n=1}^\infty\frac{1}{2^{n-1}}\left(-\frac{6}{n^4}\right)-\frac13\sum_{n=1}^\infty\frac{H_n}{2^n}\left(-\frac{6}{(n+1)^4}\right)\\ &=-4\ln2\sum_{n=1}^\infty\frac{1}{n^42^n}+2\sum_{n=1}^\infty\frac{H_n}{(n+1)^42^n}\\ &=-4\ln2\operatorname{Li}_4\left(\frac12\right)+4\sum_{n=1}^\infty\frac{H_n}{n^42^n}-4\operatorname{Li}_5\left(\frac12\right) \end{align}","Prove, without using harmonic series, that This problem was proposed by Cornel and can be found here . The main reason behind such constraint is that this integral can be simplified into which was calculated here using real and complex methods. So evaluating without using harmonic series means we are providing a third solution to . I have already computed this integral ( will be posted soon) but I would like to see variant approaches. Thanks. Added: In case the reader is curious about how this integral is related to , here is the steps By integration by parts we have","I=\int_0^1\left(\frac{\ln^2(1-x)\ln^2(1+x)}{1-x}-\frac{\ln^2(2)\ln^2(1-x)}{1-x}\right)\ dx =\frac18\zeta(5)-\frac12\ln2\zeta(4)+2\ln^22\zeta(3)-\frac23\ln^32\zeta(2)-2\zeta(2)\zeta(3)+\frac1{10}\ln^52+4\operatorname{Li}_5\left(\frac12\right) S=\sum_{n=1}^\infty\frac{H_n}{n^42^n} I S \sum_{n=1}^\infty\frac{H_n}{n^42^n} \begin{align}
I&=\frac23\int_0^1\frac{\ln^3(1-x)\ln(1+x)}{1+x}\ dx\overset{\color{red}{1-x\ \mapsto\ x}}{=}\frac13\int_0^1\frac{\ln^3x\ln(2-x)}{1-x/2}\ dx\\
&=\frac{\ln2}{3}\int_0^1\frac{\ln^3x}{1-x/2}\ dx+\frac13\int_0^1\frac{\ln^3x\ln(1-x/2)}{1-x/2}\ dx\\
&=\frac{\ln2}{3}\sum_{n=1}^\infty\frac{1}{2^{n-1}}\int_0^1x^{n-1}\ln^3x\ dx-\frac13\sum_{n=1}^\infty\frac{H_n}{2^n}\int_0^1x^n\ln^3x\ dx\\
&=\frac{\ln2}{3}\sum_{n=1}^\infty\frac{1}{2^{n-1}}\left(-\frac{6}{n^4}\right)-\frac13\sum_{n=1}^\infty\frac{H_n}{2^n}\left(-\frac{6}{(n+1)^4}\right)\\
&=-4\ln2\sum_{n=1}^\infty\frac{1}{n^42^n}+2\sum_{n=1}^\infty\frac{H_n}{(n+1)^42^n}\\
&=-4\ln2\operatorname{Li}_4\left(\frac12\right)+4\sum_{n=1}^\infty\frac{H_n}{n^42^n}-4\operatorname{Li}_5\left(\frac12\right)
\end{align}","['real-analysis', 'calculus', 'integration', 'alternative-proof', 'harmonic-numbers']"
75,$\int \frac{1}{f’(x)}$ diverges,diverges,\int \frac{1}{f’(x)},"I got this question in a prelims exam. $f$ is a positive, continuously differentiable function on $(0,\infty)$ such that $f’ >0$ . If $f(x) \leq Cx^2$ for some $C>0$ then $\int \frac{1}{f’(x)}$ diverges. There is a hint in the problem which asks to establish that if $\int \frac{1}{f’(x)}< \infty$ then $\int\limits_{a}^{\infty} \frac{1}{f’(x)} \to 0$ as $a\to \infty$ . I can establish the claim made in the hint by writing $\int f = \sum_{n} \int_{n}^{n+1} f$ and then observing that the above sum converges absolutely and therefore the tail of the series (which is the required integral) goes to 0.  I believe that I am now supposed to show that in our case $\int_{a}^{\infty} \frac{1}{f’(x)}$ does not go to zero as $a\to \infty$ . But, I am unable to make use of the fact that $f(x)\leq Cx^2$ . I do not know if it is useful to note that $\frac{1}{f’(x)}=(f^{-1})’(f(x))$ . Any help would be highly appreciated. I would like some hint on how to use this condition.","I got this question in a prelims exam. is a positive, continuously differentiable function on such that . If for some then diverges. There is a hint in the problem which asks to establish that if then as . I can establish the claim made in the hint by writing and then observing that the above sum converges absolutely and therefore the tail of the series (which is the required integral) goes to 0.  I believe that I am now supposed to show that in our case does not go to zero as . But, I am unable to make use of the fact that . I do not know if it is useful to note that . Any help would be highly appreciated. I would like some hint on how to use this condition.","f (0,\infty) f’ >0 f(x) \leq Cx^2 C>0 \int \frac{1}{f’(x)} \int \frac{1}{f’(x)}< \infty \int\limits_{a}^{\infty} \frac{1}{f’(x)} \to 0 a\to \infty \int f = \sum_{n} \int_{n}^{n+1} f \int_{a}^{\infty} \frac{1}{f’(x)} a\to \infty f(x)\leq Cx^2 \frac{1}{f’(x)}=(f^{-1})’(f(x))","['real-analysis', 'integration', 'convergence-divergence']"
76,Prove that $\mathbb{Q}$ is dense in $\mathbb{R}$,Prove that  is dense in,\mathbb{Q} \mathbb{R},"So, I'm kinda old (26yrs) for this, but I cracked open Apostol's Vol 1, and I'm making quite slow progress! There's a math blog that I go to for help (stumblingrobot.com), but I don't really understand their proof for this question (I 3.12 qn 6). It says, given any arbitrary real $x$ , $y$ , with $x < y$ , prove that at least 1 rational number $r$ exists, such that $x < r < y$ , and hence infinitely many. In the same section qn1, we proved that for any arbitrary real $x$ , $y$ , with $x < y$ , there is at least 1 real $z$ such that $x < z < y$ . Since rational numbers $\mathbb{Q}$ are a subset of real numbers $\mathbb{R}$ , shouldn't this be a sufficient proof?","So, I'm kinda old (26yrs) for this, but I cracked open Apostol's Vol 1, and I'm making quite slow progress! There's a math blog that I go to for help (stumblingrobot.com), but I don't really understand their proof for this question (I 3.12 qn 6). It says, given any arbitrary real , , with , prove that at least 1 rational number exists, such that , and hence infinitely many. In the same section qn1, we proved that for any arbitrary real , , with , there is at least 1 real such that . Since rational numbers are a subset of real numbers , shouldn't this be a sufficient proof?",x y x < y r x < r < y x y x < y z x < z < y \mathbb{Q} \mathbb{R},"['real-analysis', 'calculus']"
77,Find all polynomials $p(x)$ such that $(p(x))^2 = 1 + x \cdot p(x + 1)$ for all $x\in \mathbb{R}$,Find all polynomials  such that  for all,p(x) (p(x))^2 = 1 + x \cdot p(x + 1) x\in \mathbb{R},Find all polynomials $p(x)$ such that $(p(x))^2 = 1 + x \cdot p(x + 1)$ for all $x\in \mathbb{R}$ . I assume that $O(p(x)=n)$ (Where O(p) denotes the order of p) let $p(x)=a_nx^n+a_{n-1}x^{n-1}+a_{n-2}x^{n-2} \dots \dots +a_{1}x^{1}+a_{n-1}$ also $a_n \neq 0$ Then we have $a_n^2x^{2n} +\dots +a_0^2=1+a_nx^{n+1}+ \dots a_0x$ since $a_n \neq 0$ we must have $2n=n+1$ and hence $n=1$ hence the polynomial must be linear and hence there doesn't exist any solution(I proved assuming $p(x)=ax+b$ ) Is it correct?,Find all polynomials such that for all . I assume that (Where O(p) denotes the order of p) let also Then we have since we must have and hence hence the polynomial must be linear and hence there doesn't exist any solution(I proved assuming ) Is it correct?,p(x) (p(x))^2 = 1 + x \cdot p(x + 1) x\in \mathbb{R} O(p(x)=n) p(x)=a_nx^n+a_{n-1}x^{n-1}+a_{n-2}x^{n-2} \dots \dots +a_{1}x^{1}+a_{n-1} a_n \neq 0 a_n^2x^{2n} +\dots +a_0^2=1+a_nx^{n+1}+ \dots a_0x a_n \neq 0 2n=n+1 n=1 p(x)=ax+b,"['real-analysis', 'abstract-algebra', 'polynomials']"
78,"Defining the Cosine Function from First Principles, intuitively","Defining the Cosine Function from First Principles, intuitively",,"Throughout most of my mathematics education, the cosine function has been defined formally either using its series expansion, as $\mathfrak{Re}(\exp i\theta)$ , or as the unique solution to $y+y''=0$ with $y(0)=1$ and $y'(0) = 0$ . Although these definitions make the mathematics more convenient, I've always been interested to see what it would be like to try to formalise it directly with the geometric intuition from the unit circle, assuming only the notion of distance (i.e. that we have a complete metric space with metric $d\colon\mathbb R^2 \times \mathbb R^2 \to \mathbb R$ ). Since, by geometric intuition, we have that the cosine function is the $x$ -coordinate obtained by walking along the unit circle, my idea was to take  a line segment from the point $(1,0)$ to another point on the circle such that the distance is $\theta$ , and split it into two line segments whose sum of lengths is also $\theta$ . We then split into 3 line segments, and so on, in a limiting manner,  in such a way that the sum of lengths is always $\theta$ , as illustrated below with $\theta = 2\pi/5$ . [Desmos link] When this procedure converges, the $x$ - and $y$ -coordinates of the limiting point should be the cosine and sine of $\theta$ respectively. The problem is I can't find an easy way to express the $n$ th coordinate of this procedure (assuming we are splitting the line into $n-1$ segments) so that I can take the limit. I was going to try and do things the other way round, that is, define $\cos^{-1}\colon [-1,1] \to [0,\pi]$ by approaching the circle from below and finding the length. I did manage to do this, and got that \begin{align*}     \cos^{-1}(x)  &= \sum_{k=1}^\infty d\left[\left(\frac{(k-1)x}{n}, \sqrt{1-\left(\frac{(k-1)x}{n}\right)^2}\right), \left(\frac{kx}{n}, \sqrt{1-\left(\frac{kx}{n}\right)^2}\right)\right]\\[4pt]                   &= \sum_{k=1}^\infty \sqrt{2    \left(1-\sqrt{1-\frac{(k-1)^2    x^2}{n^2}}    \sqrt{1-\frac{k^2    x^2}{n^2}}\right)-\frac{2    k(k-1)  x^2}{n^2}}, \end{align*} and I suppose one can extrapolate from here to obtain the $\cos$ and $\sin$ functions and extend them appropriately. But it feels like the other way, that is, finding that limiting point, should be possible. I appreciate any assistance with this.","Throughout most of my mathematics education, the cosine function has been defined formally either using its series expansion, as , or as the unique solution to with and . Although these definitions make the mathematics more convenient, I've always been interested to see what it would be like to try to formalise it directly with the geometric intuition from the unit circle, assuming only the notion of distance (i.e. that we have a complete metric space with metric ). Since, by geometric intuition, we have that the cosine function is the -coordinate obtained by walking along the unit circle, my idea was to take  a line segment from the point to another point on the circle such that the distance is , and split it into two line segments whose sum of lengths is also . We then split into 3 line segments, and so on, in a limiting manner,  in such a way that the sum of lengths is always , as illustrated below with . [Desmos link] When this procedure converges, the - and -coordinates of the limiting point should be the cosine and sine of respectively. The problem is I can't find an easy way to express the th coordinate of this procedure (assuming we are splitting the line into segments) so that I can take the limit. I was going to try and do things the other way round, that is, define by approaching the circle from below and finding the length. I did manage to do this, and got that and I suppose one can extrapolate from here to obtain the and functions and extend them appropriately. But it feels like the other way, that is, finding that limiting point, should be possible. I appreciate any assistance with this.","\mathfrak{Re}(\exp i\theta) y+y''=0 y(0)=1 y'(0) = 0 d\colon\mathbb R^2 \times \mathbb R^2 \to \mathbb R x (1,0) \theta \theta \theta \theta = 2\pi/5 x y \theta n n-1 \cos^{-1}\colon [-1,1] \to [0,\pi] \begin{align*}
    \cos^{-1}(x)  &= \sum_{k=1}^\infty d\left[\left(\frac{(k-1)x}{n}, \sqrt{1-\left(\frac{(k-1)x}{n}\right)^2}\right), \left(\frac{kx}{n}, \sqrt{1-\left(\frac{kx}{n}\right)^2}\right)\right]\\[4pt]
                  &= \sum_{k=1}^\infty \sqrt{2
   \left(1-\sqrt{1-\frac{(k-1)^2
   x^2}{n^2}}
   \sqrt{1-\frac{k^2
   x^2}{n^2}}\right)-\frac{2
   k(k-1)  x^2}{n^2}},
\end{align*} \cos \sin","['real-analysis', 'geometry', 'trigonometry']"
79,Simplify $\sum_{l=0}^\infty \sum_{r=0}^\infty\frac{\Gamma(L+r-2q)}{\Gamma(L+r-1+2q)} \frac{\Gamma(L+r+l-1+2q)}{\Gamma(L+r+l+2)}\frac{r+1}{r+l+2}$,Simplify,\sum_{l=0}^\infty \sum_{r=0}^\infty\frac{\Gamma(L+r-2q)}{\Gamma(L+r-1+2q)} \frac{\Gamma(L+r+l-1+2q)}{\Gamma(L+r+l+2)}\frac{r+1}{r+l+2},"This question is a continuation of this post. Let $r,l,L\geq 1$ be integers.  Assume that $q\in [0,1]$ is a real number. The authors obtained the following equation $36$ in their paper (I express in summation forms). $$\sum_{r=1}^\infty \sum_{l=1}^\infty \frac{r}{r+l} 4(1-q)q \frac{Γ(L)}{Γ(L − 2p)} \frac{Γ(L + l − 1 − 2p)}{Γ(L + l − 2q)} \frac{Γ(L + l + r − 1 − 2q)}{ Γ(L + l + r) } = $$ $$ 1 - \frac{2q^2}{(1-2q)^2} - \frac{2\pi q(1-q)}{(1-2q)(3-4q)} \cot(2\pi q).$$ Note that the above formula are equal in the sense of asymptotically. In the midst of obtaining the above formula, I stuck at the following. Question : Is it possible to express $$\sum_{l=0}^\infty \sum_{r=0}^\infty\frac{\Gamma(L+r-2q)}{\Gamma(L+r-1+2q)} \frac{\Gamma(L+r+l-1+2q)}{\Gamma(L+r+l+2)}\frac{r+1}{r+l+2}$$ in closed form independent of summations (possibly in terms of Gamma function)? I tried to express above double summations using Appell's series . However, I stuck at $$\frac{\Gamma(L-2q)}{\Gamma(L+2)} \sum_{l=0}^\infty \sum_{r=0}^\infty \frac{(L-1+2q)_{l+r}}{(L+2)_{l+r}} \frac{(L-2q)_r}{(L-1+2q)_r} \frac{(1)_l}{l!r!} \frac{(2)_r}{(r+l+2)}.$$ In particular, can we express $$\frac{(L-2q)_r}{(L-1+2q)_r (r+l+2)}$$ to be something which allows us to use Appell series? I also tried to evaluate the summation over $l$ , that is, $$\sum_{l=0}^\infty \frac{\Gamma(L+r-1+2q+l)}{\Gamma(L+r+2+l)(r+l+2)},$$ and I arrive at $$_3F_2(L+r-1+2q,1,r+2; L+r+2,r+3;1)\frac{\Gamma(L+r-1+2q)}{\Gamma(L+r+2)(r+2)}.$$ However, I could not find any formula to evaluate $_3F_2(L+r-1+2q,1,r+2; L+r+2,r+3;1)$ in Wolfram Alpha . Updated (2 Nov 2018): @Nikos Bagis uses the Mathematica 10 to obtain a closed form for my question.  However, I would like to have a detailed calculations on how to obtain the answer.","This question is a continuation of this post. Let be integers.  Assume that is a real number. The authors obtained the following equation in their paper (I express in summation forms). Note that the above formula are equal in the sense of asymptotically. In the midst of obtaining the above formula, I stuck at the following. Question : Is it possible to express in closed form independent of summations (possibly in terms of Gamma function)? I tried to express above double summations using Appell's series . However, I stuck at In particular, can we express to be something which allows us to use Appell series? I also tried to evaluate the summation over , that is, and I arrive at However, I could not find any formula to evaluate in Wolfram Alpha . Updated (2 Nov 2018): @Nikos Bagis uses the Mathematica 10 to obtain a closed form for my question.  However, I would like to have a detailed calculations on how to obtain the answer.","r,l,L\geq 1 q\in [0,1] 36 \sum_{r=1}^\infty \sum_{l=1}^\infty \frac{r}{r+l} 4(1-q)q \frac{Γ(L)}{Γ(L − 2p)} \frac{Γ(L + l − 1 − 2p)}{Γ(L + l − 2q)} \frac{Γ(L + l + r − 1 − 2q)}{ Γ(L + l + r) } =   1 - \frac{2q^2}{(1-2q)^2} - \frac{2\pi q(1-q)}{(1-2q)(3-4q)} \cot(2\pi q). \sum_{l=0}^\infty \sum_{r=0}^\infty\frac{\Gamma(L+r-2q)}{\Gamma(L+r-1+2q)} \frac{\Gamma(L+r+l-1+2q)}{\Gamma(L+r+l+2)}\frac{r+1}{r+l+2} \frac{\Gamma(L-2q)}{\Gamma(L+2)} \sum_{l=0}^\infty \sum_{r=0}^\infty \frac{(L-1+2q)_{l+r}}{(L+2)_{l+r}} \frac{(L-2q)_r}{(L-1+2q)_r} \frac{(1)_l}{l!r!} \frac{(2)_r}{(r+l+2)}. \frac{(L-2q)_r}{(L-1+2q)_r (r+l+2)} l \sum_{l=0}^\infty \frac{\Gamma(L+r-1+2q+l)}{\Gamma(L+r+2+l)(r+l+2)}, _3F_2(L+r-1+2q,1,r+2; L+r+2,r+3;1)\frac{\Gamma(L+r-1+2q)}{\Gamma(L+r+2)(r+2)}. _3F_2(L+r-1+2q,1,r+2; L+r+2,r+3;1)","['real-analysis', 'sequences-and-series', 'special-functions', 'hypergeometric-function']"
80,Heisenberg uncertainty principle: a proof for a beginner,Heisenberg uncertainty principle: a proof for a beginner,,"Scrolling in a book on real analysis I found, as a last exercise, the request to prove the Heisenberg uncertainty principle. The exercise states Let $f\in \mathcal L_{1}^{2}(\mathbb R)$ , such that $\|f\|_{2}=\|\hat f\|_{2}=1$ . Prove that $$\left(\int_{\mathbb R}|x|^{2}|f(x)|^{2}\,\mathrm dx\right)\cdot\left(\int_{\mathbb R}|\xi|^{2}|\hat f(\xi)|^{2}\,\mathrm d\xi\right)\geq \frac{1}{(4\pi)^{2}}$$ Hint : suppose that $f\in C_{c}^{\infty}$ and use the following identities: $$\int_{\mathbb R}x\overline{f}(x)f'(x) = -{1\over 2} \|f\|_{2}^{s}$$ Plancherel's identity and Cauchy-Schwarz inequality. The book states that $$\mathcal L^2_s(\mathbb R^d) := \{f:\mathbb R^d\rightarrow\mathbb C \text{ measurable} \text{ : }(1+|x|^2)^{s\over 2}f\in L^2(\mathbb R^d)\}$$ which calles it weighted $L^2$ spaces . I was very curious on how to solve this problem because I don't have nearly as much knowledge as it's needed to solve this! So I'm here to ask you if you could give me the solution. I can't give you my working because, as just stated, I don't know much about real analysis and I was just very curious to see the solution! Probably I'll understand it if I see one but searching on the internet didn't gave me any help.","Scrolling in a book on real analysis I found, as a last exercise, the request to prove the Heisenberg uncertainty principle. The exercise states Let , such that . Prove that Hint : suppose that and use the following identities: Plancherel's identity and Cauchy-Schwarz inequality. The book states that which calles it weighted spaces . I was very curious on how to solve this problem because I don't have nearly as much knowledge as it's needed to solve this! So I'm here to ask you if you could give me the solution. I can't give you my working because, as just stated, I don't know much about real analysis and I was just very curious to see the solution! Probably I'll understand it if I see one but searching on the internet didn't gave me any help.","f\in \mathcal L_{1}^{2}(\mathbb R) \|f\|_{2}=\|\hat f\|_{2}=1 \left(\int_{\mathbb R}|x|^{2}|f(x)|^{2}\,\mathrm dx\right)\cdot\left(\int_{\mathbb R}|\xi|^{2}|\hat f(\xi)|^{2}\,\mathrm d\xi\right)\geq \frac{1}{(4\pi)^{2}} f\in C_{c}^{\infty} \int_{\mathbb R}x\overline{f}(x)f'(x) = -{1\over 2} \|f\|_{2}^{s} \mathcal L^2_s(\mathbb R^d) := \{f:\mathbb R^d\rightarrow\mathbb C \text{ measurable} \text{ : }(1+|x|^2)^{s\over 2}f\in L^2(\mathbb R^d)\} L^2","['real-analysis', 'physics']"
81,Functional equation related to $\sin$: $f(x+y)=f(x)f'(y)+f'(x)f(y)$,Functional equation related to :,\sin f(x+y)=f(x)f'(y)+f'(x)f(y),"Find all differentiable $f:\mathbb R \to \mathbb R$ such that $$\forall (x,y)\in \mathbb R^2, f(x+y)=f(x)f'(y)+f'(x)f(y)$$ It's easy to check that the only constant solution is $0$ and the only polynomial solution is $x\mapsto x$. Besides, it's also easy to check that $\sin$ is a solution, as well as $\sinh$. More generally, $x\mapsto \frac {\sin(ax)}{a}$ and $x\mapsto \frac {\sinh(ax)}{a}$ are solutions. Setting $x=y=0$ yields $f(0)(1-2f'(0))=0$. By letting $y=0$, one gets the ODE $ f(0)f'(x)+(f'(0)-1)f(x)=0$. If $f(0)\neq 0$, then $f'(0)=\frac 12$ and this is easily solved as $x\mapsto \frac {\exp(ax)}{2a}$ If $f(0)=0$, either $f'(0)\neq 1$ and then $f=0$ , or $f'(0)=1$ and the ODE is now useless. How should I continue ? Are there other solutions ?","Find all differentiable $f:\mathbb R \to \mathbb R$ such that $$\forall (x,y)\in \mathbb R^2, f(x+y)=f(x)f'(y)+f'(x)f(y)$$ It's easy to check that the only constant solution is $0$ and the only polynomial solution is $x\mapsto x$. Besides, it's also easy to check that $\sin$ is a solution, as well as $\sinh$. More generally, $x\mapsto \frac {\sin(ax)}{a}$ and $x\mapsto \frac {\sinh(ax)}{a}$ are solutions. Setting $x=y=0$ yields $f(0)(1-2f'(0))=0$. By letting $y=0$, one gets the ODE $ f(0)f'(x)+(f'(0)-1)f(x)=0$. If $f(0)\neq 0$, then $f'(0)=\frac 12$ and this is easily solved as $x\mapsto \frac {\exp(ax)}{2a}$ If $f(0)=0$, either $f'(0)\neq 1$ and then $f=0$ , or $f'(0)=1$ and the ODE is now useless. How should I continue ? Are there other solutions ?",,"['real-analysis', 'ordinary-differential-equations', 'functional-equations']"
82,Rudin Theorem 6.10 doubt,Rudin Theorem 6.10 doubt,,"This question is about Riemann Stieltjes integral: Here is definition: Let $\alpha$ be a monotonically increasing function on $[a, b]$ (since $\alpha(a)$ and $\alpha(b)$ are finite, it follows that $\alpha$ is bounded on $[a, b]$). Corresponding to each partition $P$ of $[a, b]$, we write    $$ \Delta \alpha_i = \alpha \left( x_i \right) -  \alpha \left( x_{i-1} \right). $$   It is clear that $\Delta \alpha_i \geq 0$. For any real function $f$ which is bounded on $[a, b]$ we put    $$  \begin{align} U(P, f, \alpha) &= \sum_{i=1}^n M_i \Delta \alpha_i, \\ L(P, f, \alpha) &= \sum_{i=1}^n m_i \Delta \alpha_i,  \end{align} $$   where $M_i$, $m_i$ have the same meaning as in Definition 6.1, and we define    $$ \begin{align} \tag{5} \overline{\int_a^b} f d \alpha = \inf U(P, f, \alpha), \\ \tag{6} \underline{\int_a^b} f d \alpha = \sup L(P, f, \alpha), \\\, \end{align} $$   the $\inf$ and $\sup$ again being taken over all partitions. If the left members of (5) and (6) are equal, we denote their common value by    $$ \tag{7} \int_a^b f d \alpha $$   or sometimes by    $$ \tag{8} \int_a^b f(x) d \alpha(x). $$   This is the Riemann-Stieltjes integral (or simply the Stieltjes integral ) of $f$ with respect to $\alpha$, over $[a, b]$. If (7) exists, i.e., if (5) and (6) are equal, we say that $f$ is integrable with respect to $\alpha$, in the Riemann sense, and write $f \in \mathscr{R}(\alpha)$. This is Rudin PMA's Th 6.10: Suppose $f$ is bounded on $[a, b]$, $f$ has only finitely many points of discontinuity on $[a, b]$, and $\alpha$ is continuous at every point at which $f$ is discontinuous. Then $f \in \mathscr{R}(\alpha)$. Proof : Let $\varepsilon > 0$ be given. Put $M = \sup \left\vert f(x) \right\vert$, let $E$ be the set of points at which $f$ is discontinuous. Since $E$ is finite and $\alpha$ is continuous at every point of $E$, we can cover $E$ by finitely many disjoint intervals $\left[ u_j, v_j \right] \subset [a, b]$ such that the sum of the corresponding differences $\alpha\left(v_j\right) - \alpha \left( u_j \right)$ is less than $\varepsilon$. Furthermore, we can place these intervals in such a way that every point of $E \cap (a, b)$ lies in the interior of some $\left[ u_j, v_j \right]$. Remove the segments $\left( u_j, v_j \right)$ from $[a, b]$. The remaining set $K$ is compact. Hence $f$ is uniformly continuous on $K$, and there exists $\delta > 0$ such that $\left\vert f(s) - f(t) \right\vert < \varepsilon$ if $s \in K$, $t \in K$, $\left\vert s-t \right\vert < \delta$. Now form a partition $P = \left\{ x_0, x_1, \ldots, x_n \right\}$ of $[a, b]$, as follows: Each $u_j$ occurs in $P$. Each $v_j$ occurs in $P$. No point of any segment $\left( u_j, v_j \right)$ occurs in $P$. If $x_{i-1}$ is not one of the $u_j$, then $\Delta \alpha_i < \delta$. Note that $M_i - m_i \leq 2M$ for every $i$, and that $M_i - m_i \leq \varepsilon$ unless $x_{i-1}$ is one of the $u_j$. Hence, as in the proof of Theorem 6.8,    $$ U(P, f, \alpha) - L(P, f, \alpha) \leq \left[ \alpha(b) - \alpha(a) \right] \varepsilon + 2M \varepsilon.$$   Since $\varepsilon$ is arbitrary, Theorem 6.6 shows that $f \in \mathscr{R}(\alpha)$. [Code borrowed from here .] My question is: What I can't understand is: Why do we treat $u_j$'s differently? How do we get $U(P, f, \alpha) - L(P, f, \alpha) \leq \left[ \alpha(b) - \alpha(a) \right] \varepsilon + 2M \varepsilon$ in last line?","This question is about Riemann Stieltjes integral: Here is definition: Let $\alpha$ be a monotonically increasing function on $[a, b]$ (since $\alpha(a)$ and $\alpha(b)$ are finite, it follows that $\alpha$ is bounded on $[a, b]$). Corresponding to each partition $P$ of $[a, b]$, we write    $$ \Delta \alpha_i = \alpha \left( x_i \right) -  \alpha \left( x_{i-1} \right). $$   It is clear that $\Delta \alpha_i \geq 0$. For any real function $f$ which is bounded on $[a, b]$ we put    $$  \begin{align} U(P, f, \alpha) &= \sum_{i=1}^n M_i \Delta \alpha_i, \\ L(P, f, \alpha) &= \sum_{i=1}^n m_i \Delta \alpha_i,  \end{align} $$   where $M_i$, $m_i$ have the same meaning as in Definition 6.1, and we define    $$ \begin{align} \tag{5} \overline{\int_a^b} f d \alpha = \inf U(P, f, \alpha), \\ \tag{6} \underline{\int_a^b} f d \alpha = \sup L(P, f, \alpha), \\\, \end{align} $$   the $\inf$ and $\sup$ again being taken over all partitions. If the left members of (5) and (6) are equal, we denote their common value by    $$ \tag{7} \int_a^b f d \alpha $$   or sometimes by    $$ \tag{8} \int_a^b f(x) d \alpha(x). $$   This is the Riemann-Stieltjes integral (or simply the Stieltjes integral ) of $f$ with respect to $\alpha$, over $[a, b]$. If (7) exists, i.e., if (5) and (6) are equal, we say that $f$ is integrable with respect to $\alpha$, in the Riemann sense, and write $f \in \mathscr{R}(\alpha)$. This is Rudin PMA's Th 6.10: Suppose $f$ is bounded on $[a, b]$, $f$ has only finitely many points of discontinuity on $[a, b]$, and $\alpha$ is continuous at every point at which $f$ is discontinuous. Then $f \in \mathscr{R}(\alpha)$. Proof : Let $\varepsilon > 0$ be given. Put $M = \sup \left\vert f(x) \right\vert$, let $E$ be the set of points at which $f$ is discontinuous. Since $E$ is finite and $\alpha$ is continuous at every point of $E$, we can cover $E$ by finitely many disjoint intervals $\left[ u_j, v_j \right] \subset [a, b]$ such that the sum of the corresponding differences $\alpha\left(v_j\right) - \alpha \left( u_j \right)$ is less than $\varepsilon$. Furthermore, we can place these intervals in such a way that every point of $E \cap (a, b)$ lies in the interior of some $\left[ u_j, v_j \right]$. Remove the segments $\left( u_j, v_j \right)$ from $[a, b]$. The remaining set $K$ is compact. Hence $f$ is uniformly continuous on $K$, and there exists $\delta > 0$ such that $\left\vert f(s) - f(t) \right\vert < \varepsilon$ if $s \in K$, $t \in K$, $\left\vert s-t \right\vert < \delta$. Now form a partition $P = \left\{ x_0, x_1, \ldots, x_n \right\}$ of $[a, b]$, as follows: Each $u_j$ occurs in $P$. Each $v_j$ occurs in $P$. No point of any segment $\left( u_j, v_j \right)$ occurs in $P$. If $x_{i-1}$ is not one of the $u_j$, then $\Delta \alpha_i < \delta$. Note that $M_i - m_i \leq 2M$ for every $i$, and that $M_i - m_i \leq \varepsilon$ unless $x_{i-1}$ is one of the $u_j$. Hence, as in the proof of Theorem 6.8,    $$ U(P, f, \alpha) - L(P, f, \alpha) \leq \left[ \alpha(b) - \alpha(a) \right] \varepsilon + 2M \varepsilon.$$   Since $\varepsilon$ is arbitrary, Theorem 6.6 shows that $f \in \mathscr{R}(\alpha)$. [Code borrowed from here .] My question is: What I can't understand is: Why do we treat $u_j$'s differently? How do we get $U(P, f, \alpha) - L(P, f, \alpha) \leq \left[ \alpha(b) - \alpha(a) \right] \varepsilon + 2M \varepsilon$ in last line?",,"['real-analysis', 'riemann-integration']"
83,"Given $\sum a_n$ converges and $a_n \ge 0$. Want to find $\{b_n\}$, such that $b_n\to \infty$ and $\sum a_nb_n$ converges","Given  converges and . Want to find , such that  and  converges",\sum a_n a_n \ge 0 \{b_n\} b_n\to \infty \sum a_nb_n,"The question is as in the title. I think $\{b_n\}$ should be some function of $a_n$. I was thinking about  $$b_n = \frac{1}{n\sqrt{a_n}},$$  but I don't know if $b_n$ approaches infinity. Can someone give some hint about what $b_n$ should be?","The question is as in the title. I think $\{b_n\}$ should be some function of $a_n$. I was thinking about  $$b_n = \frac{1}{n\sqrt{a_n}},$$  but I don't know if $b_n$ approaches infinity. Can someone give some hint about what $b_n$ should be?",,"['real-analysis', 'sequences-and-series']"
84,Pointwise infimum of an arbitrary collection of upper semicontinuous functions is upper semicontinuous,Pointwise infimum of an arbitrary collection of upper semicontinuous functions is upper semicontinuous,,"A real-valued function $f:X \rightarrow \mathbb{R}$ is upper semicontinuous if for each $c \in \mathbb{R}$, its pre-image $f^{-1}(-\infty,c)$ is open in $X.$ In encyclopaedia , there is the following statement: Let $\mathcal{F}$ be an arbitrary family of upper semicontinuous functions on a given topological space $X$. Then the function $F(x) = \inf_{f \in \mathcal{F}} f(x)$ is upper semicontinuous. The article stated that the proof can be found in General Topology by Bourbaki, Chapter $5 - 10$. But I do not have access to that book. In Wikipedia , similar statement appears. Likewise, the pointwise infimum of an arbitrary collection of upper semicontinuous functions is upper semicontinuous. I manage to prove the statement holds for countably many functions, its proof is similar to proving infimum of measurable functions is measurable, but I have no idea on how to prove for arbitrary case.","A real-valued function $f:X \rightarrow \mathbb{R}$ is upper semicontinuous if for each $c \in \mathbb{R}$, its pre-image $f^{-1}(-\infty,c)$ is open in $X.$ In encyclopaedia , there is the following statement: Let $\mathcal{F}$ be an arbitrary family of upper semicontinuous functions on a given topological space $X$. Then the function $F(x) = \inf_{f \in \mathcal{F}} f(x)$ is upper semicontinuous. The article stated that the proof can be found in General Topology by Bourbaki, Chapter $5 - 10$. But I do not have access to that book. In Wikipedia , similar statement appears. Likewise, the pointwise infimum of an arbitrary collection of upper semicontinuous functions is upper semicontinuous. I manage to prove the statement holds for countably many functions, its proof is similar to proving infimum of measurable functions is measurable, but I have no idea on how to prove for arbitrary case.",,"['real-analysis', 'general-topology', 'supremum-and-infimum', 'semicontinuous-functions']"
85,Prove the subdifferential of $f(x)=\max_i f_i(x)$ is $\partial f(x) = \operatorname{conv}\left( \bigcup \partial f_i(x)\right) $,Prove the subdifferential of  is,f(x)=\max_i f_i(x) \partial f(x) = \operatorname{conv}\left( \bigcup \partial f_i(x)\right) ,"How to prove that the subdifferential of $f(x) = \max_{i=1,\dots, n} f_i(x)$ satisfies         \begin{align}         \partial f(x) = \operatorname{conv}\left( \bigcup \partial f_i(x)  \right)     \end{align} How am I suppose to utilize the property of convex hull here?","How to prove that the subdifferential of $f(x) = \max_{i=1,\dots, n} f_i(x)$ satisfies         \begin{align}         \partial f(x) = \operatorname{conv}\left( \bigcup \partial f_i(x)  \right)     \end{align} How am I suppose to utilize the property of convex hull here?",,"['real-analysis', 'convex-analysis', 'subgradient']"
86,How does this inequality follow from Hölder's inequality?,How does this inequality follow from Hölder's inequality?,,"In a paper that I am reading, an inequality is given and the justification is Hölder's inequality: $$\left(\sum_{i=1}^n a_{ii}\right)^m \le n^{m-1} \sum_{i=1}^n a_{ii}^m$$ where $a_{ii}\ge0$ for all $i$. However, I do not know why the justification works. I've tried justifying it by writing $a_{ii}$ as $a_{ii}\cdot1$, then applying the inequality. I get somewhat close, since $n$ raised to a power comes out as a factor, but unfortunately I'm not arriving at the above inequality.","In a paper that I am reading, an inequality is given and the justification is Hölder's inequality: $$\left(\sum_{i=1}^n a_{ii}\right)^m \le n^{m-1} \sum_{i=1}^n a_{ii}^m$$ where $a_{ii}\ge0$ for all $i$. However, I do not know why the justification works. I've tried justifying it by writing $a_{ii}$ as $a_{ii}\cdot1$, then applying the inequality. I get somewhat close, since $n$ raised to a power comes out as a factor, but unfortunately I'm not arriving at the above inequality.",,"['real-analysis', 'inequality', 'summation', 'holder-inequality']"
87,What is... A Parsimonious History?,What is... A Parsimonious History?,,"Interpreting historical mathematicians involves a recognition of the fact that most of them viewed the continuum as not being made out of points. Rather they viewed points as marking locations on a continuum taken more or less as a primitive notion. Modern foundational theories starting around 1870 are based on a continuum made of points and therefore cannot serve as a basis for interpreting the thinking of the earlier mathematicians as far as the foundations are concerned. What one can however seek to interpret are the techniques and procedures (rather than foundations) of the earlier authors, using techniques and procedures available in modern frameworks. In the case of analysis, the modern frameworks available are those developed by Weierstrass and his followers around 1870 and based on an Archimedean continuum, as well as more recently those developed by Robinson and his followers, and based on a continuum containing infinitesimals, and other frameworks such as the one developed by Lawvere, Kock, and others. I was therefore a bit puzzled by the following comment by a historian expressed here : Recently there have been attempts to argue that Leibniz, Euler, and even Cauchy could have been thinking in some informal version of rigorous modern non-standard analysis, in which infinite and infinitesimal quantities do exist. However, a historical interpretation such as the one sketched above that aims to understand Leibniz on his own terms, and that confers upon him both insight and consistency, has a lot to recommend it over an interpretation that has only been possible to defend in the last few decades. It is parsimonious and requires no expert defence for which modern concepts seem essential and therefore create more problems than they solve (e.g. with infinite series). The same can be said of non-standard readings of Euler; etc. Question 1. Is this historian choosing one foundational framework over another in interpreting the techniques and procedures of the historical authors? Question 2. What exactly is a Parsimonious History? Question 3. Gray and Bottazzini reportedly make a rather poetic proposal in the following terms: "" The best policy is to read on in a spirit of dialogue with the earlier authors. "" The proposal of such a conversation with, say, Euler sounds intriguing. I am only wondering about Gray's comment here that "" Euler’s attempts at explaining the foundations of calculus in terms of differentials, which are and are not zero, are dreadfully weak. "" Isn't such an opening line in a conversation likely to be a conversation-stopper? Question 4. In connection with the work of Laugwitz mentioned by one of the responders, one could ask why Gray does not cite explicitly the work of any of the authors that he wishes explicitly to criticize for using modern infinitesimals? Laugwitz's article ( Laugwitz, Detlef. Definite values of infinite sums: aspects of the foundations of infinitesimal analysis around 1820. Arch. Hist. Exact Sci.  39  (1989),  no. 3, 195–245 ) appeared in Archive for History of Exact Sciences , clearly a reputable journal since Jeremy Gray happens to be its Editor-in-Chief . Similarly, the article McKinzie, Mark; Tuckey, Curtis. Hidden lemmas in Euler's summation of the reciprocals of the squares. Arch. Hist. Exact Sci.  51  (1997),  no. 1, 29–57 appeared in the same journal and exploited Robinson's framework to clarify some of Euler's procedures; it, too, is being stonewalled by the Editor-in-Chief.","Interpreting historical mathematicians involves a recognition of the fact that most of them viewed the continuum as not being made out of points. Rather they viewed points as marking locations on a continuum taken more or less as a primitive notion. Modern foundational theories starting around 1870 are based on a continuum made of points and therefore cannot serve as a basis for interpreting the thinking of the earlier mathematicians as far as the foundations are concerned. What one can however seek to interpret are the techniques and procedures (rather than foundations) of the earlier authors, using techniques and procedures available in modern frameworks. In the case of analysis, the modern frameworks available are those developed by Weierstrass and his followers around 1870 and based on an Archimedean continuum, as well as more recently those developed by Robinson and his followers, and based on a continuum containing infinitesimals, and other frameworks such as the one developed by Lawvere, Kock, and others. I was therefore a bit puzzled by the following comment by a historian expressed here : Recently there have been attempts to argue that Leibniz, Euler, and even Cauchy could have been thinking in some informal version of rigorous modern non-standard analysis, in which infinite and infinitesimal quantities do exist. However, a historical interpretation such as the one sketched above that aims to understand Leibniz on his own terms, and that confers upon him both insight and consistency, has a lot to recommend it over an interpretation that has only been possible to defend in the last few decades. It is parsimonious and requires no expert defence for which modern concepts seem essential and therefore create more problems than they solve (e.g. with infinite series). The same can be said of non-standard readings of Euler; etc. Question 1. Is this historian choosing one foundational framework over another in interpreting the techniques and procedures of the historical authors? Question 2. What exactly is a Parsimonious History? Question 3. Gray and Bottazzini reportedly make a rather poetic proposal in the following terms: "" The best policy is to read on in a spirit of dialogue with the earlier authors. "" The proposal of such a conversation with, say, Euler sounds intriguing. I am only wondering about Gray's comment here that "" Euler’s attempts at explaining the foundations of calculus in terms of differentials, which are and are not zero, are dreadfully weak. "" Isn't such an opening line in a conversation likely to be a conversation-stopper? Question 4. In connection with the work of Laugwitz mentioned by one of the responders, one could ask why Gray does not cite explicitly the work of any of the authors that he wishes explicitly to criticize for using modern infinitesimals? Laugwitz's article ( Laugwitz, Detlef. Definite values of infinite sums: aspects of the foundations of infinitesimal analysis around 1820. Arch. Hist. Exact Sci.  39  (1989),  no. 3, 195–245 ) appeared in Archive for History of Exact Sciences , clearly a reputable journal since Jeremy Gray happens to be its Editor-in-Chief . Similarly, the article McKinzie, Mark; Tuckey, Curtis. Hidden lemmas in Euler's summation of the reciprocals of the squares. Arch. Hist. Exact Sci.  51  (1997),  no. 1, 29–57 appeared in the same journal and exploited Robinson's framework to clarify some of Euler's procedures; it, too, is being stonewalled by the Editor-in-Chief.",,"['real-analysis', 'soft-question', 'math-history', 'philosophy', 'nonstandard-analysis']"
88,"Triangle inequality fails in $L^{1,\infty}$",Triangle inequality fails in,"L^{1,\infty}","It can be proved that $\forall\varepsilon>0$ there exists $C(\epsilon)>0$ such that  for all $f,g\in L^{1,\infty}(\Bbb R^n)$ we have that $$ ||f+g||_{1,\infty}\le(1+\varepsilon)||f||_{1,\infty}+C(\varepsilon)||g||_{1,\infty}$$ for example $C(\epsilon)=1+\frac1{\varepsilon}$ works. I have some problem in proving this inequality fails for $\varepsilon=0$. I should prove that for every $C>0$ there exist $f_C,g_C\in L^{1,\infty}(\Bbb R^n)$ such that $$ ||f+g||_{1,\infty}>||f||_{1,\infty}+C||g||_{1,\infty} $$  but it seems really hard. I thought I can take some sequence of functions but I can't put this idea into concrete computations. Can someone help me? EDIT : Given $f:\Bbb R^n\to\Bbb R$ measurable we define, for $\alpha>0$, $\lambda_f(\alpha):=|\{x\in\Bbb R^n\;:\;|f(x)|>\alpha\}|$ (given a subset $E\subseteq\Bbb R^n$, we set |E| to denote its Lebesgue measure). Then we set $$ L^{1,\infty}(\Bbb R^n):=\{f:\Bbb R^n\to\Bbb R\;\mbox{measurable}\;:\exists C>0\;\;\mbox{s. t.}\;\; \lambda_f(\alpha)\le\frac C{\alpha}\;\forall \alpha>0\}\;\;\;. $$ Finally we set $||f||_{1,\infty}$ as the infimum of such $C$'s.","It can be proved that $\forall\varepsilon>0$ there exists $C(\epsilon)>0$ such that  for all $f,g\in L^{1,\infty}(\Bbb R^n)$ we have that $$ ||f+g||_{1,\infty}\le(1+\varepsilon)||f||_{1,\infty}+C(\varepsilon)||g||_{1,\infty}$$ for example $C(\epsilon)=1+\frac1{\varepsilon}$ works. I have some problem in proving this inequality fails for $\varepsilon=0$. I should prove that for every $C>0$ there exist $f_C,g_C\in L^{1,\infty}(\Bbb R^n)$ such that $$ ||f+g||_{1,\infty}>||f||_{1,\infty}+C||g||_{1,\infty} $$  but it seems really hard. I thought I can take some sequence of functions but I can't put this idea into concrete computations. Can someone help me? EDIT : Given $f:\Bbb R^n\to\Bbb R$ measurable we define, for $\alpha>0$, $\lambda_f(\alpha):=|\{x\in\Bbb R^n\;:\;|f(x)|>\alpha\}|$ (given a subset $E\subseteq\Bbb R^n$, we set |E| to denote its Lebesgue measure). Then we set $$ L^{1,\infty}(\Bbb R^n):=\{f:\Bbb R^n\to\Bbb R\;\mbox{measurable}\;:\exists C>0\;\;\mbox{s. t.}\;\; \lambda_f(\alpha)\le\frac C{\alpha}\;\forall \alpha>0\}\;\;\;. $$ Finally we set $||f||_{1,\infty}$ as the infimum of such $C$'s.",,"['real-analysis', 'lp-spaces', 'harmonic-analysis', 'weak-lp-spaces']"
89,There exist meager subsets of $\mathbb{R}$ whose complements have Lebesgue measure zero,There exist meager subsets of  whose complements have Lebesgue measure zero,\mathbb{R},"The Baire Category Theorem - Let $X$ be a complete metric space a.) If $\{U_n\}_1^\infty$ is a sequence of open dense subsets of $X$, then $\bigcap_1^\infty U_n$ is dense in $X$. b.) $X$ is not a countable union of nowhere dense sets. The name for this theorem comes from Baire's terminology for sets: If $X$ is a topological space, a set $E\subset X$ if of the first category, or meager, according to Baire, if $E$ is a countable union of nowhere dense sets; otherwise $E$ is of the second category. Problem 5.3.27 from Folland's Real Analysis: There exist meager subsets of $\mathbb{R}$ whose complements have Lebesgue measure zero Attempted proof: Let $X$ be a topological space in $\mathbb{R}$ and $E\subset X$ be of the first category (meager). Set $\{x_k\}$ to be an enumeration of the rational numbers, let $$E_n = \bigcup_{k=1}^{\infty} \left(x_k - \frac{1}{2^{k-1} n},x_k + \frac{1}{2^{k-1}n}\right)$$ Consider the set $E = \cap_{1}^{\infty} E_n$, then $$m(E_n) \leq \sum_{k=1}^{\infty}\frac{1}{2^k n} = \frac{1}{n}$$ hence $m(E) = 0$ I am not sure if I am right, any suggestions is greatly appreciated.","The Baire Category Theorem - Let $X$ be a complete metric space a.) If $\{U_n\}_1^\infty$ is a sequence of open dense subsets of $X$, then $\bigcap_1^\infty U_n$ is dense in $X$. b.) $X$ is not a countable union of nowhere dense sets. The name for this theorem comes from Baire's terminology for sets: If $X$ is a topological space, a set $E\subset X$ if of the first category, or meager, according to Baire, if $E$ is a countable union of nowhere dense sets; otherwise $E$ is of the second category. Problem 5.3.27 from Folland's Real Analysis: There exist meager subsets of $\mathbb{R}$ whose complements have Lebesgue measure zero Attempted proof: Let $X$ be a topological space in $\mathbb{R}$ and $E\subset X$ be of the first category (meager). Set $\{x_k\}$ to be an enumeration of the rational numbers, let $$E_n = \bigcup_{k=1}^{\infty} \left(x_k - \frac{1}{2^{k-1} n},x_k + \frac{1}{2^{k-1}n}\right)$$ Consider the set $E = \cap_{1}^{\infty} E_n$, then $$m(E_n) \leq \sum_{k=1}^{\infty}\frac{1}{2^k n} = \frac{1}{n}$$ hence $m(E) = 0$ I am not sure if I am right, any suggestions is greatly appreciated.",,"['real-analysis', 'general-topology', 'measure-theory', 'baire-category']"
90,Prove that $\sum_{n=1}^\infty \ln\left(\frac{n(n+2)}{(n+1)^2}\right)$ converges and find its sum,Prove that  converges and find its sum,\sum_{n=1}^\infty \ln\left(\frac{n(n+2)}{(n+1)^2}\right),"Since this is not a geometric series, I know that I should use the definition of a convergent series, so $$S_n = \sum_{i=1}^n \ln\left(\frac{i(i+2)}{(i+1)^2}\right)$$ After this, I tried two different ways: 1) I simplified the fraction to read $$\frac{\ln(i^2+2i)}{i^2+2i+1}$$ and then I used long division to get $$\ln((1)-\frac{1}{(i+1)^2})$$ However, once I start plugging in i starting at 1, I don't know where to go from there. 2) I simplified the equation to read $\ln(i^2+2i)-\ln(i^2+2i+1)$, but again, once I start plugging in i starting at 1, I don't know where to go from there.  What can I do? Note: Originally the series was presented as $\sum_{n=1}^\infty \frac{\ln(n(n+2))}{(n+1)^2}$ which is what some of the answers below are adressing.","Since this is not a geometric series, I know that I should use the definition of a convergent series, so $$S_n = \sum_{i=1}^n \ln\left(\frac{i(i+2)}{(i+1)^2}\right)$$ After this, I tried two different ways: 1) I simplified the fraction to read $$\frac{\ln(i^2+2i)}{i^2+2i+1}$$ and then I used long division to get $$\ln((1)-\frac{1}{(i+1)^2})$$ However, once I start plugging in i starting at 1, I don't know where to go from there. 2) I simplified the equation to read $\ln(i^2+2i)-\ln(i^2+2i+1)$, but again, once I start plugging in i starting at 1, I don't know where to go from there.  What can I do? Note: Originally the series was presented as $\sum_{n=1}^\infty \frac{\ln(n(n+2))}{(n+1)^2}$ which is what some of the answers below are adressing.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
91,Exists a uniformly convex norm on Banach space satisfying certain condition?,Exists a uniformly convex norm on Banach space satisfying certain condition?,,"Let $E$ be a Banach space with norm $\|\cdot\|$. Assume that there exists on $E$ an equivalent norm, denoted by $|\cdot|$, that is uniformly convex. Given any $k > 1$, does there exist a uniformly convex norm $\|\cdot\|'$ on $E$ such that$$\|x\| \le \|x\|' \le k\|x\| \text{ for all }x \in E?$$ Progress. Maybe we could set$$\|x\|'^2 = \|x\|^2 + \alpha|x|^2$$with $\alpha > 0$ small enough? I am not sure what to do from there on out though.","Let $E$ be a Banach space with norm $\|\cdot\|$. Assume that there exists on $E$ an equivalent norm, denoted by $|\cdot|$, that is uniformly convex. Given any $k > 1$, does there exist a uniformly convex norm $\|\cdot\|'$ on $E$ such that$$\|x\| \le \|x\|' \le k\|x\| \text{ for all }x \in E?$$ Progress. Maybe we could set$$\|x\|'^2 = \|x\|^2 + \alpha|x|^2$$with $\alpha > 0$ small enough? I am not sure what to do from there on out though.",,"['calculus', 'real-analysis', 'functional-analysis', 'banach-spaces', 'normed-spaces']"
92,"Differentiable function such that $f(x+y),f(x)f(y),f(x-y)$ are an arithmetic progression for all $x,y$",Differentiable function such that  are an arithmetic progression for all,"f(x+y),f(x)f(y),f(x-y) x,y","If $f$ is a differentiable function on $\mathbb{R}$ such that $f(x+y),f(x)f(y),f(x-y)$(taken in that order) are in arithmetic progression for all $x,y\in \mathbb{R}$ and $f(0)\neq0,$then $(A)f'(0)=-1\hspace{1cm}(B)f'(0)=1\hspace{1cm}(C)f'(1)-f'(-1)=0\hspace{1cm}(D)f'(1)+f'(-1)=0$ My Try: As $f(x+y),f(x)f(y),f(x-y)$ are in A.P. $2f(x)f(y)=f(x+y)+f(x-y)$.........(1) Put $x=0,y=0$ in the above equation $2f^2(0)=2(0)$ $f(0)=1$ because $f(0)\neq0$ Differentiating the equation $(1)$, $2f(x)f'(y)\frac{dy}{dx}+2f(y)f'(x)=f'(x+y)(1+\frac{dy}{dx})+f'(x-y)(1-\frac{dy}{dx})$ I am stuck here .Now i dont know how to go further.Please help me.","If $f$ is a differentiable function on $\mathbb{R}$ such that $f(x+y),f(x)f(y),f(x-y)$(taken in that order) are in arithmetic progression for all $x,y\in \mathbb{R}$ and $f(0)\neq0,$then $(A)f'(0)=-1\hspace{1cm}(B)f'(0)=1\hspace{1cm}(C)f'(1)-f'(-1)=0\hspace{1cm}(D)f'(1)+f'(-1)=0$ My Try: As $f(x+y),f(x)f(y),f(x-y)$ are in A.P. $2f(x)f(y)=f(x+y)+f(x-y)$.........(1) Put $x=0,y=0$ in the above equation $2f^2(0)=2(0)$ $f(0)=1$ because $f(0)\neq0$ Differentiating the equation $(1)$, $2f(x)f'(y)\frac{dy}{dx}+2f(y)f'(x)=f'(x+y)(1+\frac{dy}{dx})+f'(x-y)(1-\frac{dy}{dx})$ I am stuck here .Now i dont know how to go further.Please help me.",,"['calculus', 'real-analysis', 'derivatives', 'functional-equations']"
93,Find the complex (or real) roots of $e^{\frac{3 x}{2}}+2 \cos \left(\frac{\sqrt{3} x}{2}\right)$,Find the complex (or real) roots of,e^{\frac{3 x}{2}}+2 \cos \left(\frac{\sqrt{3} x}{2}\right),"Define for natural $n\geq 2$ $$G(x,n)= \sum _{k=0}^\infty \frac{x^{k n}}{(k n)!}= \frac{\sum _{k=0}^{n-1} e^{x e^{\frac{2 i \pi  k}{n}}}}{n}= G(x e^{\frac{2 i \pi}{n}},n)= \prod_{m=1}^\infty \left(1+\left(\frac{x}{r(n,m)}\right)^n\right) $$ where $r(n,m)$ is the $m$th biggest absolute value of a root of $G(x,n)$. The complex roots for $G(x,2)$ and $G(x,4)$ are trivial to find. $r(2,m)=\left(m-\frac{1}{2}\right)\pi$ and $r(4,m)=\left(m-\frac{1}{2}\right)\pi\sqrt{2}$. Because of rotational symmetry $G(x,3)=G(x e^{\frac{2 i \pi}{3}},3)$, we only need to find the roots on the real line. Note that $$G(x,3)=\frac{1}{3} \left(e^x+2 e^{-\frac{x}{2}} \cos \left(\frac{\sqrt{3} x}{2}\right)\right)$$ From this we can see that as $x\rightarrow -\infty$, the roots of $G(x,3)$ become arbitrarily close to the roots of $\cos \left(\frac{\sqrt{3} x}{2}\right)$ Because of this, we can use Newton's Method to find the roots of $G(x,3)$ by starting with the roots of $\cos \left(\frac{\sqrt{3} x}{2}\right)$. The Mathematica code for $r(3,m)$ is -x /. FindRoot[E^(3 x/2) + 2 Cos[(Sqrt[3] x)/2], {x, -2 Pi (-1/2 + m)/Sqrt[3]}] This is not good enough though. I desire a formula for $r(3,m)$ which does not not involve a FindRoot. Also, because you could define Newton's method exactly with a limit, limits are not allowed in the final formula. Because of input from the comments, we now have a new formula for $r(3,m)$: $$\frac{\pi(2m-1)}{\sqrt{3}}-\sum _{n=1}^{\infty } \frac{\left(-e^{\frac{1}{2} \sqrt{3} (\pi -2 \pi  m)}\right)^n \left(\lim_{x\to \frac{\pi -2 \pi  m}{\sqrt{3}}} \, \frac{\partial ^{n-1}}{\partial x^{n-1}}\left(\frac{x-\frac{\pi -2 \pi  m}{\sqrt{3}}}{-e^{\frac{1}{2} \sqrt{3} (\pi -2 \pi  m)}+e^{\frac{3 x}{2}}+2 \cos \left(\frac{\sqrt{3} x}{2}\right)}\right)^n\right)}{n!}$$ Though Mathematica cannot calculate this, the corresponding Mathematica code is: -(([Pi] - 2 m [Pi])/Sqrt[3]) -   Sum[(-E^(1/2 Sqrt[3] ([Pi] - 2 m [Pi])))^n/n!*    Assuming[{Element[m, Integers]},      Limit[D[((x - ([Pi] - 2 m [Pi])/Sqrt[3])/(E^(3 x/2) +             2 Cos[(Sqrt[3] x)/2] - E^(            1/2 Sqrt[3] ([Pi] - 2 m [Pi]))))^n, {x, n - 1}],       x -> ([Pi] - 2 m [Pi])/Sqrt[3]]], {n, 1, Infinity}] The remaining work is to find the general $(n-1)$th derivative in the sum and to get rid of the limit in the sum. Here is a contour plot showing the roots of $G(x,3)$. The orange curve is $\Im(G(x,3))=0$. The blue curve is $\Re(G(x,3))=0$. The plot shows $-10<\Re(x)<10,-10<\Im(x)<10$. Also, by comparing the sum and product forms of $G(x,n)$, we can see $$ \frac{1}{n!}=\sum_{m=1}^\infty\frac{1}{r(n,m)^n}$$","Define for natural $n\geq 2$ $$G(x,n)= \sum _{k=0}^\infty \frac{x^{k n}}{(k n)!}= \frac{\sum _{k=0}^{n-1} e^{x e^{\frac{2 i \pi  k}{n}}}}{n}= G(x e^{\frac{2 i \pi}{n}},n)= \prod_{m=1}^\infty \left(1+\left(\frac{x}{r(n,m)}\right)^n\right) $$ where $r(n,m)$ is the $m$th biggest absolute value of a root of $G(x,n)$. The complex roots for $G(x,2)$ and $G(x,4)$ are trivial to find. $r(2,m)=\left(m-\frac{1}{2}\right)\pi$ and $r(4,m)=\left(m-\frac{1}{2}\right)\pi\sqrt{2}$. Because of rotational symmetry $G(x,3)=G(x e^{\frac{2 i \pi}{3}},3)$, we only need to find the roots on the real line. Note that $$G(x,3)=\frac{1}{3} \left(e^x+2 e^{-\frac{x}{2}} \cos \left(\frac{\sqrt{3} x}{2}\right)\right)$$ From this we can see that as $x\rightarrow -\infty$, the roots of $G(x,3)$ become arbitrarily close to the roots of $\cos \left(\frac{\sqrt{3} x}{2}\right)$ Because of this, we can use Newton's Method to find the roots of $G(x,3)$ by starting with the roots of $\cos \left(\frac{\sqrt{3} x}{2}\right)$. The Mathematica code for $r(3,m)$ is -x /. FindRoot[E^(3 x/2) + 2 Cos[(Sqrt[3] x)/2], {x, -2 Pi (-1/2 + m)/Sqrt[3]}] This is not good enough though. I desire a formula for $r(3,m)$ which does not not involve a FindRoot. Also, because you could define Newton's method exactly with a limit, limits are not allowed in the final formula. Because of input from the comments, we now have a new formula for $r(3,m)$: $$\frac{\pi(2m-1)}{\sqrt{3}}-\sum _{n=1}^{\infty } \frac{\left(-e^{\frac{1}{2} \sqrt{3} (\pi -2 \pi  m)}\right)^n \left(\lim_{x\to \frac{\pi -2 \pi  m}{\sqrt{3}}} \, \frac{\partial ^{n-1}}{\partial x^{n-1}}\left(\frac{x-\frac{\pi -2 \pi  m}{\sqrt{3}}}{-e^{\frac{1}{2} \sqrt{3} (\pi -2 \pi  m)}+e^{\frac{3 x}{2}}+2 \cos \left(\frac{\sqrt{3} x}{2}\right)}\right)^n\right)}{n!}$$ Though Mathematica cannot calculate this, the corresponding Mathematica code is: -(([Pi] - 2 m [Pi])/Sqrt[3]) -   Sum[(-E^(1/2 Sqrt[3] ([Pi] - 2 m [Pi])))^n/n!*    Assuming[{Element[m, Integers]},      Limit[D[((x - ([Pi] - 2 m [Pi])/Sqrt[3])/(E^(3 x/2) +             2 Cos[(Sqrt[3] x)/2] - E^(            1/2 Sqrt[3] ([Pi] - 2 m [Pi]))))^n, {x, n - 1}],       x -> ([Pi] - 2 m [Pi])/Sqrt[3]]], {n, 1, Infinity}] The remaining work is to find the general $(n-1)$th derivative in the sum and to get rid of the limit in the sum. Here is a contour plot showing the roots of $G(x,3)$. The orange curve is $\Im(G(x,3))=0$. The blue curve is $\Re(G(x,3))=0$. The plot shows $-10<\Re(x)<10,-10<\Im(x)<10$. Also, by comparing the sum and product forms of $G(x,n)$, we can see $$ \frac{1}{n!}=\sum_{m=1}^\infty\frac{1}{r(n,m)^n}$$",,"['real-analysis', 'complex-analysis', 'roots']"
94,Continuous one-to-one mapping from a subset $K \subset \mathbb{R}^n$ of positive measure to $\mathbb{R}^{n-1}$,Continuous one-to-one mapping from a subset  of positive measure to,K \subset \mathbb{R}^n \mathbb{R}^{n-1},Let $f\colon \mathbb{R}^n \to \mathbb{R}^{n-1}$ be a continuous function and $K\subset \mathbb{R}^n$ a subset of positive Lebesgue measure. Is it possible that $f$ is one-to-one on $K$? If $K$ contains a (nonempty) open set this is impossible because of the invariance of domain theorem. But can we say anything for arbitrary (measurable) sets?,Let $f\colon \mathbb{R}^n \to \mathbb{R}^{n-1}$ be a continuous function and $K\subset \mathbb{R}^n$ a subset of positive Lebesgue measure. Is it possible that $f$ is one-to-one on $K$? If $K$ contains a (nonempty) open set this is impossible because of the invariance of domain theorem. But can we say anything for arbitrary (measurable) sets?,,"['real-analysis', 'measure-theory', 'dimension-theory-analysis']"
95,Borel $\sigma$-algebra of continuous functions,Borel -algebra of continuous functions,\sigma,"Let $B(C(T,\mathbb{R}))$ be the Borel sigma algebra of continuous functions mapping from the compact metric space $T$ to $S$ defined by the canonical metric $\|\cdot\|_\infty.$ Now I was wondering whether we can write this sigma algebra also as a sigma-algebra generated by some cylinder sets? If anything is unclear, please let me know.","Let $B(C(T,\mathbb{R}))$ be the Borel sigma algebra of continuous functions mapping from the compact metric space $T$ to $S$ defined by the canonical metric $\|\cdot\|_\infty.$ Now I was wondering whether we can write this sigma algebra also as a sigma-algebra generated by some cylinder sets? If anything is unclear, please let me know.",,"['real-analysis', 'analysis', 'measure-theory', 'stochastic-processes', 'stochastic-calculus']"
96,Gradient flow of Dirichlet energy,Gradient flow of Dirichlet energy,,"I have heard that the gradient flow of the Dirichlet energy gives a solution of the heat equation, i.e. if $u(t,x) \in C^1( [0,\infty) \times \mathbb R^d)$ solves  $$ u_t(t,x) = - dE(u(t,x)), $$ where $$  E(u) := \dfrac{1}{2} \int \|\nabla_x u\|^2, $$ then $$ \partial_t u = \Delta_x u := \sum_i \partial_{x_i,x_i}^2 u. $$ As I am trying to prove this fact, I have some problems in calculating the Frechet derivative of the Dirichlet energy $dE$. If I understand correctly, fixed $f$, $dE$ is a linear operator from $C^1$ into $\mathbb R$ which should satisfy $$ \lim_{\|h\|_{C^1} \to 0} \dfrac{E(f+h) - E(f) - dE(h) }{\|h\|_{C^1}} =  \lim_{\|h\|_{C^1} \to 0} \dfrac{(\int 2 \nabla f \cdot \nabla h + \nabla h \cdot \nabla h) - dE(h)}{\|h\|_{C^1}} = 0 ,$$ but how to find it? Or should I not compute the differential of $E$ at all?","I have heard that the gradient flow of the Dirichlet energy gives a solution of the heat equation, i.e. if $u(t,x) \in C^1( [0,\infty) \times \mathbb R^d)$ solves  $$ u_t(t,x) = - dE(u(t,x)), $$ where $$  E(u) := \dfrac{1}{2} \int \|\nabla_x u\|^2, $$ then $$ \partial_t u = \Delta_x u := \sum_i \partial_{x_i,x_i}^2 u. $$ As I am trying to prove this fact, I have some problems in calculating the Frechet derivative of the Dirichlet energy $dE$. If I understand correctly, fixed $f$, $dE$ is a linear operator from $C^1$ into $\mathbb R$ which should satisfy $$ \lim_{\|h\|_{C^1} \to 0} \dfrac{E(f+h) - E(f) - dE(h) }{\|h\|_{C^1}} =  \lim_{\|h\|_{C^1} \to 0} \dfrac{(\int 2 \nabla f \cdot \nabla h + \nabla h \cdot \nabla h) - dE(h)}{\|h\|_{C^1}} = 0 ,$$ but how to find it? Or should I not compute the differential of $E$ at all?",,"['real-analysis', 'reference-request', 'partial-differential-equations', 'gradient-flows']"
97,Good Kernel's Properties,Good Kernel's Properties,,"I am recently studying properties about a good kernel, and came across a problem. Definition: A kernel $K_\delta$ is 'good' if they are Lebesgue integrable and satisfy the following conditions for $\delta>0$: $\int_{\mathbb R^d}K_\delta(x)dx=1$ $\int_{\mathbb R^d}|K_\delta(x)|dx\le A$ For every $\eta>0$, $$\int_{|x|\ge\eta}|K_\delta(x)|dx\to0\text{  }\text{ }\text{ as }\delta\to0$$ Here $A$ is a constant independent of $\delta$ The question is to prove that for every point of continuity of an integrable function $f$, $(f\star K_\delta)(x)\to f(x)$ as $\delta \to 0$, where we define $$(f\star K_\delta)(x)=\int_{\mathbb R^d}f(x-y)K_\delta(y)dy$$","I am recently studying properties about a good kernel, and came across a problem. Definition: A kernel $K_\delta$ is 'good' if they are Lebesgue integrable and satisfy the following conditions for $\delta>0$: $\int_{\mathbb R^d}K_\delta(x)dx=1$ $\int_{\mathbb R^d}|K_\delta(x)|dx\le A$ For every $\eta>0$, $$\int_{|x|\ge\eta}|K_\delta(x)|dx\to0\text{  }\text{ }\text{ as }\delta\to0$$ Here $A$ is a constant independent of $\delta$ The question is to prove that for every point of continuity of an integrable function $f$, $(f\star K_\delta)(x)\to f(x)$ as $\delta \to 0$, where we define $$(f\star K_\delta)(x)=\int_{\mathbb R^d}f(x-y)K_\delta(y)dy$$",,"['real-analysis', 'fourier-analysis']"
98,Solving for $a$ in this equation $\sin^a(a) = b$,Solving for  in this equation,a \sin^a(a) = b,$$\sin^{a}a=b$$ Do you know how to solve for $a$ here algebraicaly? This is a little bit too abstract for my understanding. Thank you very much.,$$\sin^{a}a=b$$ Do you know how to solve for $a$ here algebraicaly? This is a little bit too abstract for my understanding. Thank you very much.,,"['calculus', 'real-analysis']"
99,Question about Dominated Convergence Theorem.,Question about Dominated Convergence Theorem.,,"How to compute $$\lim_{n \to \infty}\int_0^{\infty}\Big(1+\frac{x}{n}\Big)^{-n}\sin\Big(\frac{x}{n}\Big) dx$$ I want to use the Dominated Convergence Theorem. so it becomes $$\int_0^{\infty}\lim_{n\to \infty}\Big(1+\frac{x}{n}\Big)^{\frac{n}{x}(-x)}\sin\Big(\frac{x}{n}\Big) dx=\int_0^{\infty}e^{-x}\times 0 dx=0$$ I have trouble finding the dominating term, which is essential for me to apply dominated convergence theorem. Could anyone help me with this? Thanks!","How to compute $$\lim_{n \to \infty}\int_0^{\infty}\Big(1+\frac{x}{n}\Big)^{-n}\sin\Big(\frac{x}{n}\Big) dx$$ I want to use the Dominated Convergence Theorem. so it becomes $$\int_0^{\infty}\lim_{n\to \infty}\Big(1+\frac{x}{n}\Big)^{\frac{n}{x}(-x)}\sin\Big(\frac{x}{n}\Big) dx=\int_0^{\infty}e^{-x}\times 0 dx=0$$ I have trouble finding the dominating term, which is essential for me to apply dominated convergence theorem. Could anyone help me with this? Thanks!",,"['real-analysis', 'lebesgue-integral']"
