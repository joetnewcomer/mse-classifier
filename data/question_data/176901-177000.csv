,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Can normal chain rule be used for total derivative?,Can normal chain rule be used for total derivative?,,"Chain rule states that $$\frac{df}{dx} \frac{dx}{dt} = \frac{df}{dt}$$. Suppose that $f$ is function $f(x,y)$. In this case, would normal chain rule still work?","Chain rule states that $$\frac{df}{dx} \frac{dx}{dt} = \frac{df}{dt}$$. Suppose that $f$ is function $f(x,y)$. In this case, would normal chain rule still work?",,"['calculus', 'multivariable-calculus']"
1,"$f(x,y)=\frac{x^2y}{x^4+y^2}$ for $(x,y)\neq(0.0)$ and $f(0,0)=0$. Show $\partial f/\partial y$ is not bounded",for  and . Show  is not bounded,"f(x,y)=\frac{x^2y}{x^4+y^2} (x,y)\neq(0.0) f(0,0)=0 \partial f/\partial y","I was working on this problem and I need to show that Let $f:\mathbb R^2 \rightarrow \mathbb R$ be defined as follows: $f(x,y)=\frac{x^2y}{x^4+y^2}$ for $(x,y)\neq(0.0)$ and $f(0,0)=0$. How do I show rigorously that $\frac{\partial f}{\partial y}$ is not bounded. I kind of have a fair idea as to why it is not bounded. You would simply compute the partial and see how the degrees of the two polynomials behave in the denominator and numerator. But I am not sure if this is acceptable. Also, is there a general criteria in showing that a function of two variables is not bounded?. Can just show that the limit goes to infinity as $(x,y)$ goes to infinity? This is something that has escaped me when I learned multivariable calculus. Can anyone help me?. Thanks for your help.","I was working on this problem and I need to show that Let $f:\mathbb R^2 \rightarrow \mathbb R$ be defined as follows: $f(x,y)=\frac{x^2y}{x^4+y^2}$ for $(x,y)\neq(0.0)$ and $f(0,0)=0$. How do I show rigorously that $\frac{\partial f}{\partial y}$ is not bounded. I kind of have a fair idea as to why it is not bounded. You would simply compute the partial and see how the degrees of the two polynomials behave in the denominator and numerator. But I am not sure if this is acceptable. Also, is there a general criteria in showing that a function of two variables is not bounded?. Can just show that the limit goes to infinity as $(x,y)$ goes to infinity? This is something that has escaped me when I learned multivariable calculus. Can anyone help me?. Thanks for your help.",,['multivariable-calculus']
2,"Finding local maxima, minima, and saddle points of $f(x,y)=xy + \frac1x +\frac1y$","Finding local maxima, minima, and saddle points of","f(x,y)=xy + \frac1x +\frac1y","Can anyone show me whether my answer below is correct and complete?  Specifically, I am not sure whether or not I defined the extrema in explicit-enough terms.  Also, the graph of the function using http://web.monroecc.edu/manila/webfiles/calcNSF/JavaCode/CalcPlot3D.htm seems to present a somewhat different answer than I am getting using equations below.  Perhaps someone might be able to point out other things I could improve to make this answer more correct.  This is question 14.7.12 in the seventh edition of Stewart Calculus. ""Find the local maximum and minimum values, and saddle points, of $f(x,y)=xy + \frac{1}{x} +\frac{1}{y}$."" Here is my work: $f_x(x,y) = y+\frac{-1}{x^2}=0$, so $yx^2 =1$ $f_y(x,y)=x+\frac{-1}{y^2}=0$, so $xy^2=1$ Thus, $yx^2=xy^2=1$ and $\frac{xy^2}{yx^2}=1=\frac{y}{x}=\frac{x}{y}$, so the critical points occur when $y=x$ $f_{xx}=\frac{2}{x^3}$ $f_{yy}=\frac{2}{y^3}$ $f_{xy}=1$ Use Second Derivatives Test: $D=D(a,b)=f_{xx}(a,b)f_{yy}(a,b)-[f_{xy}(a,b)]^2$ For $x=y$, we have $D=\frac{4}{x^6}-1>0$, thus, there are no saddle points . $f_{xx} = \frac{2}{x^3}=\begin{cases}<0 ,&  x < 0 \\udf ,& x=0\\>0,& x>0\end{cases}$ Thus, $x=y$ defines local maxima when $x<0$ and defines local minima when $x>0$ Highest maxima occur as $x\to\infty$ and as $x\to 0^+$ Lowest minima occur as $x\to -\infty$ and as $x\to 0^-$","Can anyone show me whether my answer below is correct and complete?  Specifically, I am not sure whether or not I defined the extrema in explicit-enough terms.  Also, the graph of the function using http://web.monroecc.edu/manila/webfiles/calcNSF/JavaCode/CalcPlot3D.htm seems to present a somewhat different answer than I am getting using equations below.  Perhaps someone might be able to point out other things I could improve to make this answer more correct.  This is question 14.7.12 in the seventh edition of Stewart Calculus. ""Find the local maximum and minimum values, and saddle points, of $f(x,y)=xy + \frac{1}{x} +\frac{1}{y}$."" Here is my work: $f_x(x,y) = y+\frac{-1}{x^2}=0$, so $yx^2 =1$ $f_y(x,y)=x+\frac{-1}{y^2}=0$, so $xy^2=1$ Thus, $yx^2=xy^2=1$ and $\frac{xy^2}{yx^2}=1=\frac{y}{x}=\frac{x}{y}$, so the critical points occur when $y=x$ $f_{xx}=\frac{2}{x^3}$ $f_{yy}=\frac{2}{y^3}$ $f_{xy}=1$ Use Second Derivatives Test: $D=D(a,b)=f_{xx}(a,b)f_{yy}(a,b)-[f_{xy}(a,b)]^2$ For $x=y$, we have $D=\frac{4}{x^6}-1>0$, thus, there are no saddle points . $f_{xx} = \frac{2}{x^3}=\begin{cases}<0 ,&  x < 0 \\udf ,& x=0\\>0,& x>0\end{cases}$ Thus, $x=y$ defines local maxima when $x<0$ and defines local minima when $x>0$ Highest maxima occur as $x\to\infty$ and as $x\to 0^+$ Lowest minima occur as $x\to -\infty$ and as $x\to 0^-$",,"['multivariable-calculus', 'optimization']"
3,Determinant of a Modified Jacobian of a Function,Determinant of a Modified Jacobian of a Function,,"Suppose $f = \left( f_1, f_2, \ldots, f_{n-1} \right) : \mathbb{R}^{n} \mapsto \mathbb{R}^{n-1}$ is a $C^{2}$ function, then show that the symbolic determinant \begin{align} \begin{vmatrix} \frac{\partial}{\partial x_{1}} &\frac{\partial f_{1}}{\partial x_{1}} &\frac{\partial f_{2}}{\partial x_{1}} &\cdots &\frac{\partial f_{n-1}}{\partial x_{1}} \\\\ \frac{\partial}{\partial x_{2}} &\frac{\partial f_{1}}{\partial x_{2}} &\frac{\partial f_{2}}{\partial x_{2}} &\cdots &\frac{\partial f_{n-1}}{\partial x_{2}} \\ \vdots &\vdots &\vdots &\ddots &\vdots \\ \frac{\partial}{\partial x_{n}} &\frac{\partial f_{1}}{\partial x_{n}} &\frac{\partial f_{2}}{\partial x_{n}} &\cdots &\frac{\partial f_{n-1}}{\partial x_{n}}  \end{vmatrix} \end{align} vanishes identically. I have been trying to rack my brains thinking of various methods which can be used to solve the following problem, but I am getting nowhere, I am not particularly good at theoretical multivariable calculus, and hence might be missing some basic concept here. I would be thankful if someone could point out a direction for me to work through. P.S. This problem is from the entrance examination, 2010 to the Graduate School at Chennai Mathematical Institute.","Suppose $f = \left( f_1, f_2, \ldots, f_{n-1} \right) : \mathbb{R}^{n} \mapsto \mathbb{R}^{n-1}$ is a $C^{2}$ function, then show that the symbolic determinant \begin{align} \begin{vmatrix} \frac{\partial}{\partial x_{1}} &\frac{\partial f_{1}}{\partial x_{1}} &\frac{\partial f_{2}}{\partial x_{1}} &\cdots &\frac{\partial f_{n-1}}{\partial x_{1}} \\\\ \frac{\partial}{\partial x_{2}} &\frac{\partial f_{1}}{\partial x_{2}} &\frac{\partial f_{2}}{\partial x_{2}} &\cdots &\frac{\partial f_{n-1}}{\partial x_{2}} \\ \vdots &\vdots &\vdots &\ddots &\vdots \\ \frac{\partial}{\partial x_{n}} &\frac{\partial f_{1}}{\partial x_{n}} &\frac{\partial f_{2}}{\partial x_{n}} &\cdots &\frac{\partial f_{n-1}}{\partial x_{n}}  \end{vmatrix} \end{align} vanishes identically. I have been trying to rack my brains thinking of various methods which can be used to solve the following problem, but I am getting nowhere, I am not particularly good at theoretical multivariable calculus, and hence might be missing some basic concept here. I would be thankful if someone could point out a direction for me to work through. P.S. This problem is from the entrance examination, 2010 to the Graduate School at Chennai Mathematical Institute.",,[]
4,$L_{2}$ norm of the gradient of a vector valued function.,norm of the gradient of a vector valued function.,L_{2},"I have a vector valued function $U(x,y)=\Big(u_{1}(x,y),u_{2}(x,y)\Big)$. I want to find $\|\nabla U\|_{L_{2}(0,1)}$, but i could not figure how can do it. Do you have any idea?","I have a vector valued function $U(x,y)=\Big(u_{1}(x,y),u_{2}(x,y)\Big)$. I want to find $\|\nabla U\|_{L_{2}(0,1)}$, but i could not figure how can do it. Do you have any idea?",,"['functions', 'multivariable-calculus', 'normed-spaces']"
5,Change of variables in double integral. Finding limits of integration,Change of variables in double integral. Finding limits of integration,,"I need to integrate $\int_ \! \int \sin \frac{1}{2}(x+y) \cos\frac{1}{2}(x-y)\,dx\,dy$ over region $R$:{triangle with vertices $(0,0),(0,2),(1,1)$}. They ask to use $u=\frac{1}{2}(x+y)$ and $v=\frac{1}{2}(x-y)$. Attempt :First, I transformed $(x,y)$ to $(x=x(u,v),y=y(u,v))$. Namely, I solved for x and y: $$\begin{cases}u=\frac{1}{2}(x+y)\\v=\frac{1}{2}(x-y)\end{cases}$$ The Jacobian I found is $J(u,v)=\frac{\partial (x,y)}{\partial (u,v)}=-1$. I am having hard time founding the limits of integration. In xy-plane $R$ looks like that: So, the region R is bounded by $\begin{cases} y=0\\y=x\\y=-x+2 \end{cases}$ In uv-plane it looks like: The region S is bounded by $\begin{cases} u=1\\ u=v\end{cases}$ Now the double integral looks like: $$\int_0^1 \! \int_0^v \sin u \cos v\,du\,dv$$ When, I solve it I get $$\int_0^1 \! \int_0^v \sin u \cos v\,du\,dv=\frac{1}{2} (\frac{1}{2} \sin2 -1)$$ But in the answer key the answer is $1-\frac{1}{2} \sin2Â $ Can you please tell me what I am doing wrong. Hints please.","I need to integrate $\int_ \! \int \sin \frac{1}{2}(x+y) \cos\frac{1}{2}(x-y)\,dx\,dy$ over region $R$:{triangle with vertices $(0,0),(0,2),(1,1)$}. They ask to use $u=\frac{1}{2}(x+y)$ and $v=\frac{1}{2}(x-y)$. Attempt :First, I transformed $(x,y)$ to $(x=x(u,v),y=y(u,v))$. Namely, I solved for x and y: $$\begin{cases}u=\frac{1}{2}(x+y)\\v=\frac{1}{2}(x-y)\end{cases}$$ The Jacobian I found is $J(u,v)=\frac{\partial (x,y)}{\partial (u,v)}=-1$. I am having hard time founding the limits of integration. In xy-plane $R$ looks like that: So, the region R is bounded by $\begin{cases} y=0\\y=x\\y=-x+2 \end{cases}$ In uv-plane it looks like: The region S is bounded by $\begin{cases} u=1\\ u=v\end{cases}$ Now the double integral looks like: $$\int_0^1 \! \int_0^v \sin u \cos v\,du\,dv$$ When, I solve it I get $$\int_0^1 \! \int_0^v \sin u \cos v\,du\,dv=\frac{1}{2} (\frac{1}{2} \sin2 -1)$$ But in the answer key the answer is $1-\frac{1}{2} \sin2Â $ Can you please tell me what I am doing wrong. Hints please.",,['multivariable-calculus']
6,Poincare inequality?,Poincare inequality?,,"Let $\Omega$ be a smooth bounded open subset of $\mathbb{R}^n$.  Does there exist $A = A(\Omega)$ with the property that for any $f \in C^\infty(\bar{\Omega})$ with $f = 0$ on $\partial \Omega$, $\int_\Omega |f(x)|\,dx \leq A\int_\Omega |\nabla f(x)|\,dx$?  I think that this is known as some version of ``Poincare's inequality''.","Let $\Omega$ be a smooth bounded open subset of $\mathbb{R}^n$.  Does there exist $A = A(\Omega)$ with the property that for any $f \in C^\infty(\bar{\Omega})$ with $f = 0$ on $\partial \Omega$, $\int_\Omega |f(x)|\,dx \leq A\int_\Omega |\nabla f(x)|\,dx$?  I think that this is known as some version of ``Poincare's inequality''.",,"['multivariable-calculus', 'sobolev-spaces']"
7,Why $\int_x^1\int_x^1\frac{ds\;dt}{1-st}=2\int_x^1\log(1+t)\frac{dt}{t}$,Why,\int_x^1\int_x^1\frac{ds\;dt}{1-st}=2\int_x^1\log(1+t)\frac{dt}{t},"How can one derive the above equality? Oh, there's a condition on $x$, which is $0<x<1$ Any help would be very much appreciated!","How can one derive the above equality? Oh, there's a condition on $x$, which is $0<x<1$ Any help would be very much appreciated!",,"['calculus', 'integration', 'multivariable-calculus']"
8,Question about supremum,Question about supremum,,"Let $f:U\rightarrow\mathbb{R}^n$ be a differentiable function over an open convex set $U\subset\mathbb{R}^m$. Show that $\sup_{x\neq y}\dfrac{|f(x)-f(y)|}{|x-y|}=\sup_{z\in U}|(Df)_z|.$ I know I must use the Mean Value theorem to show this, and since the set $U$ is convex I will have the equality, but how can I reach this equation?","Let $f:U\rightarrow\mathbb{R}^n$ be a differentiable function over an open convex set $U\subset\mathbb{R}^m$. Show that $\sup_{x\neq y}\dfrac{|f(x)-f(y)|}{|x-y|}=\sup_{z\in U}|(Df)_z|.$ I know I must use the Mean Value theorem to show this, and since the set $U$ is convex I will have the equality, but how can I reach this equation?",,['multivariable-calculus']
9,Find the derivative of the following equation..,Find the derivative of the following equation..,,"I have a question in my manual and I am not able to answer it, I'd appreciate some help please. Find $ \dfrac{dy}{dx} $  if  $ 2x^2y + 3xy^2 = 6 $ I'm confused with = 6.. Thanks !","I have a question in my manual and I am not able to answer it, I'd appreciate some help please. Find $ \dfrac{dy}{dx} $  if  $ 2x^2y + 3xy^2 = 6 $ I'm confused with = 6.. Thanks !",,"['calculus', 'multivariable-calculus', 'derivatives', 'implicit-differentiation']"
10,how do I find the taylor polynomial of multivariable functions?,how do I find the taylor polynomial of multivariable functions?,,"I know taylor polynomial for single variable functions but I am having trouble understanding how to find taylor polynomials for multivariable functions. I know how to find partial derivatives as well as the chain rule for multivariable functions. Can someone please explain how I can find the taylor polynomial of a multivariable function? for simplicity lets do f(x,y)=x^5-y^4 Above is text from my textbook that I read but it doesnt make any sense to me especially step 3 and 4 If you know of a site or source that can better explain how to find multivariable function's taylor series I would appreciate that so that this way you dont spend so much time in typing explanation for me.","I know taylor polynomial for single variable functions but I am having trouble understanding how to find taylor polynomials for multivariable functions. I know how to find partial derivatives as well as the chain rule for multivariable functions. Can someone please explain how I can find the taylor polynomial of a multivariable function? for simplicity lets do f(x,y)=x^5-y^4 Above is text from my textbook that I read but it doesnt make any sense to me especially step 3 and 4 If you know of a site or source that can better explain how to find multivariable function's taylor series I would appreciate that so that this way you dont spend so much time in typing explanation for me.",,"['multivariable-calculus', 'taylor-expansion']"
11,The way to figure out whether the multi-variable polynomial has roots.,The way to figure out whether the multi-variable polynomial has roots.,,"Let's have a multi-variable high degree polynomial $f(x_1, x_2, \dots, x_n)$. I am looking for efficient way to figure out if there are any roots in real numbers for: $0 \le x_1 \le 1\\ 0 \le x_2 \le 1\\ \dots\\ 0 \le x_n \le 1$ Can you give some hints, theorems or algorithms for performing such tasks. And I don't need to find roots exactly. I just need to know if there are roots or not.","Let's have a multi-variable high degree polynomial $f(x_1, x_2, \dots, x_n)$. I am looking for efficient way to figure out if there are any roots in real numbers for: $0 \le x_1 \le 1\\ 0 \le x_2 \le 1\\ \dots\\ 0 \le x_n \le 1$ Can you give some hints, theorems or algorithms for performing such tasks. And I don't need to find roots exactly. I just need to know if there are roots or not.",,"['real-analysis', 'polynomials', 'multivariable-calculus', 'roots']"
12,"(vector/multi variable) calculus, potential in non conservative fields","(vector/multi variable) calculus, potential in non conservative fields",,"I know that finding a potential is a sufficient condition to show that a vector field is conservative. My question is if the those statements are equivalent. I've found a vector field which isn't conservative, does this imply that there is no potential to the vector field? kind reg,","I know that finding a potential is a sufficient condition to show that a vector field is conservative. My question is if the those statements are equivalent. I've found a vector field which isn't conservative, does this imply that there is no potential to the vector field? kind reg,",,"['calculus', 'multivariable-calculus']"
13,Proof for the projection of vector u on v,Proof for the projection of vector u on v,,"I was trying to prove that $proj_v(u)=\frac{vÂ·u}{||v||^2}$ v , and I was getting close, but then a friend spoiled the fun of completing the proof by giving me what he called a ""hint"". Blurting out a key part in a proof isn't a ""hint"". Needless to say, after this, I immediately knew what to do to complete the proof, but as I looked back at my original approach, I can't help but wonder what was wrong with it. I tried it, but with it I got answers like the zero vector or even an infinite number of answers, and I don't know why. What I did was I defined the projection of u onto v to be the vector x such that ( u - x )Â· v =0 (because of the orthogonality). If I break these vectors down into their component forms, with the components of x $\langle$ x$_1$, x$_2\rangle$ being treated as variables, (and the components of u and v as constants) then we get an infinite number of answers, since we have 2 variables. Why does this approach to the problem give me an infinite number of answers, when there is clearly one unique solution? Thank you.","I was trying to prove that $proj_v(u)=\frac{vÂ·u}{||v||^2}$ v , and I was getting close, but then a friend spoiled the fun of completing the proof by giving me what he called a ""hint"". Blurting out a key part in a proof isn't a ""hint"". Needless to say, after this, I immediately knew what to do to complete the proof, but as I looked back at my original approach, I can't help but wonder what was wrong with it. I tried it, but with it I got answers like the zero vector or even an infinite number of answers, and I don't know why. What I did was I defined the projection of u onto v to be the vector x such that ( u - x )Â· v =0 (because of the orthogonality). If I break these vectors down into their component forms, with the components of x $\langle$ x$_1$, x$_2\rangle$ being treated as variables, (and the components of u and v as constants) then we get an infinite number of answers, since we have 2 variables. Why does this approach to the problem give me an infinite number of answers, when there is clearly one unique solution? Thank you.",,['multivariable-calculus']
14,What does $d_{\textbf{a}} f$ mean?,What does  mean?,d_{\textbf{a}} f,"I have a question regarding the differential $d_{\textbf a} f$. Suppose we have the function $f(x,y)= xy$, and the vectors $\textbf a = (1,1)$ and $\textbf u = (2,1)$. Then, if I understand this correctly, $$d_{\textbf a} f(\textbf u) = \nabla f(\textbf a) \cdot \textbf u = (1,1)\cdot (2,1) = 2+1 = 3,$$ where $\nabla f(\textbf a) = (\partial f/\partial x, \partial f/\partial y)$. But what if my assignment is to calculate $d_{\textbf a} f$? I don't know what it means. Do they want me to calculate $d_{\textbf a} f(x,y) = (1,1)\cdot (x,y) = x+y$, or something else? Edit: Note that it is not the directional derivative that I'm asking about.","I have a question regarding the differential $d_{\textbf a} f$. Suppose we have the function $f(x,y)= xy$, and the vectors $\textbf a = (1,1)$ and $\textbf u = (2,1)$. Then, if I understand this correctly, $$d_{\textbf a} f(\textbf u) = \nabla f(\textbf a) \cdot \textbf u = (1,1)\cdot (2,1) = 2+1 = 3,$$ where $\nabla f(\textbf a) = (\partial f/\partial x, \partial f/\partial y)$. But what if my assignment is to calculate $d_{\textbf a} f$? I don't know what it means. Do they want me to calculate $d_{\textbf a} f(x,y) = (1,1)\cdot (x,y) = x+y$, or something else? Edit: Note that it is not the directional derivative that I'm asking about.",,['multivariable-calculus']
15,Differentiation at the origin,Differentiation at the origin,,"In a homework exercise, three functions are given, and it is asked which one of those is differentiable at the origin. The correct is answer is the third, which is indeed differentiable at the origin. My question, however, is why isn't the second one (shown below) differentiable? $$ f(x, y) = \begin{cases} \frac{xy^2}{x^2+y^2}& \Leftarrow (x, y) \ne (0, 0)\\  0&\Leftarrow (x, y) = (0, 0) \end{cases}$$ Both the partial derivatives at the origin exist and are continuous, so $f \in C^1$, and thus it should be differentiable at all points of its domain, in particular at the origin. To help understand the problem, I plotted the function, and indeed, it is not differentiable at the origin. So I ask: where did I made a mistake? Is my assessment about the partial derivatives wrong? Thanks in advance for the help.","In a homework exercise, three functions are given, and it is asked which one of those is differentiable at the origin. The correct is answer is the third, which is indeed differentiable at the origin. My question, however, is why isn't the second one (shown below) differentiable? $$ f(x, y) = \begin{cases} \frac{xy^2}{x^2+y^2}& \Leftarrow (x, y) \ne (0, 0)\\  0&\Leftarrow (x, y) = (0, 0) \end{cases}$$ Both the partial derivatives at the origin exist and are continuous, so $f \in C^1$, and thus it should be differentiable at all points of its domain, in particular at the origin. To help understand the problem, I plotted the function, and indeed, it is not differentiable at the origin. So I ask: where did I made a mistake? Is my assessment about the partial derivatives wrong? Thanks in advance for the help.",,['multivariable-calculus']
16,Why this vector field $f$ belongs to $C^1({\bf R}^2\times {\bf R})$?,Why this vector field  belongs to ?,f C^1({\bf R}^2\times {\bf R}),"The following system is an example in a book of dynamical system(in the section about Hopf Bifurcation). $$ \begin{align} \dot{x}=\mu x- y-x\sqrt{x^2+y^2} \\   \dot{y}=x + \mu y-y\sqrt{x^2+y^2} \end{align} $$ But I don't understand why it can be called a $C^1$-system, i.e., the vector field $f:R^2\times R\to R^2$ defined by this systemm belongs to $C^1(R^2\times R)$, i.e., all of the first partial derivatives with respect to $x,y$ and $\mu$ are continuous for all $x,y$ and $\mu$. I set it as an exercise, and found that $$ \frac{\partial f}{\partial x}= \left(   \begin{array}{c}      \mu-(\sqrt{x^2+y^2}+\frac{x^2}{\sqrt{x^2+y^2}})  \\      1-\frac{xy}{\sqrt{x^2+y^2}}  \\   \end{array} \right) $$ This is not even defined at $(0,0,\mu)$. Then why $f\in C^1(R^2\times R)$? Edit: The original question finally boils down to another question: What exactly is the definition of   $C^1$ functions, or more generally   $C^k$ functions? Are the $k$-th derivatives allowed to   have removable-discontinuity points?","The following system is an example in a book of dynamical system(in the section about Hopf Bifurcation). $$ \begin{align} \dot{x}=\mu x- y-x\sqrt{x^2+y^2} \\   \dot{y}=x + \mu y-y\sqrt{x^2+y^2} \end{align} $$ But I don't understand why it can be called a $C^1$-system, i.e., the vector field $f:R^2\times R\to R^2$ defined by this systemm belongs to $C^1(R^2\times R)$, i.e., all of the first partial derivatives with respect to $x,y$ and $\mu$ are continuous for all $x,y$ and $\mu$. I set it as an exercise, and found that $$ \frac{\partial f}{\partial x}= \left(   \begin{array}{c}      \mu-(\sqrt{x^2+y^2}+\frac{x^2}{\sqrt{x^2+y^2}})  \\      1-\frac{xy}{\sqrt{x^2+y^2}}  \\   \end{array} \right) $$ This is not even defined at $(0,0,\mu)$. Then why $f\in C^1(R^2\times R)$? Edit: The original question finally boils down to another question: What exactly is the definition of   $C^1$ functions, or more generally   $C^k$ functions? Are the $k$-th derivatives allowed to   have removable-discontinuity points?",,['multivariable-calculus']
17,Find s in terms of p and c for certain equations,Find s in terms of p and c for certain equations,,"$s$, $p$ and $c$ are vectors; I need to find $s$ in terms of the other two for: (1) $| s - c | = 1$ (2) $s = \lambda p$ ( $\lambda$ is a constant ) How can I use the constant $\alpha = (p \cdot c)^2 - p^2 * (c^2 - 1)$? There may be no solution.","$s$, $p$ and $c$ are vectors; I need to find $s$ in terms of the other two for: (1) $| s - c | = 1$ (2) $s = \lambda p$ ( $\lambda$ is a constant ) How can I use the constant $\alpha = (p \cdot c)^2 - p^2 * (c^2 - 1)$? There may be no solution.",,['multivariable-calculus']
18,A question on notation: What does $\nabla |\vec{a} \times \vec{r}|^n$ mean?,A question on notation: What does  mean?,\nabla |\vec{a} \times \vec{r}|^n,"I sort of asked a version of this question before and it was unclear; try I will now to make an honest attempt to state everything clerly. I am trying to evaluate the following, namely $\nabla w = \nabla |\vec{a} \times \vec{r}|^n$, where $\vec{a}$ is a constant vector and $\vec{r}$ is the vector $<x_1,x_2,\ldots x_n>$. Now say that I use the chain rule, first say by setting $\vec{u}$ to be equal to the cross product of $\vec{a}$ and $\vec{r}$. Now here's the part that I'm confused. How do we extend the chain rule over when dealing with the gradient? Do I take $\nabla |\vec{a} \times \vec{r}|^n$ to be equal to $\nabla |\vec{u}|^n$ $\times$ $\nabla (\vec{a} \times \vec{r})$, where $\times$ denotes the cross product? The first bit $\nabla |\vec{u}|^n$ is easy, it just evaluates to $n|\vec{a} \times \vec{r}|^{n-2} (\vec{a} \times \vec {r})$, remembering that $\vec{u} = \vec{a} \times \vec{r}$. I am guessing that $\nabla |\vec{a} \times \vec{r}|^n$ $\neq$ $\nabla |\vec{u}|^n$ $\times$ $\nabla (\vec{a} \times \vec{r})$, as to even speak about $\nabla (\vec{a} \times \vec{r})$, i.e. the gradient of a vector we would have to talk about either the cross product or dot product of the gradient and $\nabla (\vec{a} \times \vec{r})$ By the way, I am told the answer given is $\nabla |\vec{a} \times \vec{r}|^n$ = $n|\vec{a} \times \vec{r}|^{n-2} \Big(\vec{a} \times (\vec{r} \times \vec{a})\Big)$. So let's say that I try a component wise approach, i.e. we look first at $\frac{\partial w}{\partial x_1}$. Then is it true (I could be wrong) that: $\frac{\partial w}{\partial x_1} = n|\vec{a} \times \vec{r}|^{n-2} \quad \vec{u_1} \times \frac{\partial}{\partial x_1} \Big(\vec{a} \times \vec{r}\Big) = n|\vec{a} \times \vec{r}|^{n-2} \quad \vec{u_1} \times  \Big(\vec{a} \times \frac{\partial\vec{r}}{\partial x_1}\Big)$, as $\vec{a}$ is a constant vector? Here $\vec{u_1}$ denotes the first component of the vector $\vec{a} \times \vec{r}$. I would really aprreciate an interpretation of this, it is just that I am confused about what to take and the meanings of these operations.","I sort of asked a version of this question before and it was unclear; try I will now to make an honest attempt to state everything clerly. I am trying to evaluate the following, namely $\nabla w = \nabla |\vec{a} \times \vec{r}|^n$, where $\vec{a}$ is a constant vector and $\vec{r}$ is the vector $<x_1,x_2,\ldots x_n>$. Now say that I use the chain rule, first say by setting $\vec{u}$ to be equal to the cross product of $\vec{a}$ and $\vec{r}$. Now here's the part that I'm confused. How do we extend the chain rule over when dealing with the gradient? Do I take $\nabla |\vec{a} \times \vec{r}|^n$ to be equal to $\nabla |\vec{u}|^n$ $\times$ $\nabla (\vec{a} \times \vec{r})$, where $\times$ denotes the cross product? The first bit $\nabla |\vec{u}|^n$ is easy, it just evaluates to $n|\vec{a} \times \vec{r}|^{n-2} (\vec{a} \times \vec {r})$, remembering that $\vec{u} = \vec{a} \times \vec{r}$. I am guessing that $\nabla |\vec{a} \times \vec{r}|^n$ $\neq$ $\nabla |\vec{u}|^n$ $\times$ $\nabla (\vec{a} \times \vec{r})$, as to even speak about $\nabla (\vec{a} \times \vec{r})$, i.e. the gradient of a vector we would have to talk about either the cross product or dot product of the gradient and $\nabla (\vec{a} \times \vec{r})$ By the way, I am told the answer given is $\nabla |\vec{a} \times \vec{r}|^n$ = $n|\vec{a} \times \vec{r}|^{n-2} \Big(\vec{a} \times (\vec{r} \times \vec{a})\Big)$. So let's say that I try a component wise approach, i.e. we look first at $\frac{\partial w}{\partial x_1}$. Then is it true (I could be wrong) that: $\frac{\partial w}{\partial x_1} = n|\vec{a} \times \vec{r}|^{n-2} \quad \vec{u_1} \times \frac{\partial}{\partial x_1} \Big(\vec{a} \times \vec{r}\Big) = n|\vec{a} \times \vec{r}|^{n-2} \quad \vec{u_1} \times  \Big(\vec{a} \times \frac{\partial\vec{r}}{\partial x_1}\Big)$, as $\vec{a}$ is a constant vector? Here $\vec{u_1}$ denotes the first component of the vector $\vec{a} \times \vec{r}$. I would really aprreciate an interpretation of this, it is just that I am confused about what to take and the meanings of these operations.",,['notation']
19,Jacobian of a Matrix,Jacobian of a Matrix,,Usually Jacobian matrix encapsulate partial derivatives of a vector f w.r.t. another vector v. More generally how to define/get the Jacobian matrix of a matrix F w.r.t another matrix V? Is it just taking the elements of F and V and arranging them as vectors and getting the usual Jacobian matrix of 2 vectors?,Usually Jacobian matrix encapsulate partial derivatives of a vector f w.r.t. another vector v. More generally how to define/get the Jacobian matrix of a matrix F w.r.t another matrix V? Is it just taking the elements of F and V and arranging them as vectors and getting the usual Jacobian matrix of 2 vectors?,,"['linear-algebra', 'multivariable-calculus']"
20,How would I calculate the flux of a given vector field through a surface?,How would I calculate the flux of a given vector field through a surface?,,"I'm trying to work out a problem where I need to calculate the flux of the vector field $A= \langle xy, yz, zx \rangle$ through the shape pictured below. So far I've set up the following integral... $\oint_{S} \mathbf{A} \cdot \mathrm{d} \mathbf{a}$ ; with $\mathrm{d}\mathbf{a}$ being an infintesimally small area. And I've worked it out to the following result: $$\begin{align*} \oint_{S} \mathbf{A} \cdot \mathrm{d}\mathbf{a}  &= \int_S \left( xy \, \mathrm{d}y \, \mathrm{d}z + yz \, \mathrm{d}x \, \mathrm{d}z + zx \, \mathrm{d}x \, \mathrm{d}y \right) \\ &= \int_0^c \int_0^b xy \, \mathrm{d}y \, \mathrm{d}z + \int_0^c \int_0^a yz \, \mathrm{d}x \, \mathrm{d}z + \int_0^b \int_0^a xy \, \mathrm{d}x \, \mathrm{d}y \end{align*}$$ Is there no way around evaluating these three double integrals, or did I just do the problem wrong? I know I can get the same answer by evaluating $\int \nabla \cdot \mathbf{A}\, \mathrm{d}\tau$ over the volume of the shape, but I specifically want to know how to setup the problem using the surface integral.","I'm trying to work out a problem where I need to calculate the flux of the vector field through the shape pictured below. So far I've set up the following integral... ; with being an infintesimally small area. And I've worked it out to the following result: Is there no way around evaluating these three double integrals, or did I just do the problem wrong? I know I can get the same answer by evaluating over the volume of the shape, but I specifically want to know how to setup the problem using the surface integral.","A= \langle xy, yz, zx \rangle \oint_{S} \mathbf{A} \cdot \mathrm{d} \mathbf{a} \mathrm{d}\mathbf{a} \begin{align*}
\oint_{S} \mathbf{A} \cdot \mathrm{d}\mathbf{a} 
&= \int_S \left( xy \, \mathrm{d}y \, \mathrm{d}z + yz \, \mathrm{d}x \, \mathrm{d}z + zx \, \mathrm{d}x \, \mathrm{d}y \right) \\
&= \int_0^c \int_0^b xy \, \mathrm{d}y \, \mathrm{d}z + \int_0^c \int_0^a yz \, \mathrm{d}x \, \mathrm{d}z + \int_0^b \int_0^a xy \, \mathrm{d}x \, \mathrm{d}y
\end{align*} \int \nabla \cdot \mathbf{A}\, \mathrm{d}\tau","['calculus', 'multivariable-calculus', 'vector-analysis', 'vector-fields', 'surface-integrals']"
21,Regular curve and the implicit function theorem?,Regular curve and the implicit function theorem?,,"Given $\gamma(t)=(x(t),y(t)):\mathbb{R}\to\mathbb{R^2}$ be a smooth regular curve. i.e. $\gamma'(t)\neq0$ . Is there any relationship between such a curve and the implicit function theorem? In particular, is it true that at any point in which the curve is regular, it can be represented as a graph of a function $x=x(y)$ or $y=y(x)$ , at least locally? I am asking because if e.g. $x'(t_0)\neq0$ then the tangent at this point is not parallel to the y-axis, and at least graphically, it seems like it would imply that there is a neighborhood of $(x(t_0),y(t_0)) $ in which the curve is a function $y=y(x)$ . Thank you","Given be a smooth regular curve. i.e. . Is there any relationship between such a curve and the implicit function theorem? In particular, is it true that at any point in which the curve is regular, it can be represented as a graph of a function or , at least locally? I am asking because if e.g. then the tangent at this point is not parallel to the y-axis, and at least graphically, it seems like it would imply that there is a neighborhood of in which the curve is a function . Thank you","\gamma(t)=(x(t),y(t)):\mathbb{R}\to\mathbb{R^2} \gamma'(t)\neq0 x=x(y) y=y(x) x'(t_0)\neq0 (x(t_0),y(t_0))  y=y(x)","['multivariable-calculus', 'differential-geometry']"
22,"Calculate the double integral: $\iint \frac{x^2}{x^2 + y^2}dxdy$ over the area $T = \{ (x, y) \in R^2 \mid 0 \le y \le x, \frac{1}{2} \le x \le 1\}$",Calculate the double integral:  over the area,"\iint \frac{x^2}{x^2 + y^2}dxdy T = \{ (x, y) \in R^2 \mid 0 \le y \le x, \frac{1}{2} \le x \le 1\}","I first started by setting the bounds inside the integral as follows: $$ T = \{ (x, y) \in R^2 \vert 0 \le y \le x, \frac{1}{2} \le x \le 1\}$$ $$\int_\frac{1}{2}^1 \int_0^x \frac{x^2}{x^2 + y^2}dydx$$ This integral is straight forward to evaluate: \begin{align} \int_\frac{1}{2}^1 \int_0^x \frac{x^2}{x^2 + y^2}dydx & =  \int_\frac{1}{2}^1 \int_0^x \frac{1}{1 + ({\frac{y}{x}})^2}dydx\\ & =\int_\frac{1}{2}^1 \left.\left[x\arctan(\frac{y}{x})\right]\right\vert_{y=0}^{y=x} dx\\  & =\int_\frac{1}{2}^1 x\arctan(1) dx\\  & =\frac{\pi}{4}\int_\frac{1}{2}^1 x dx\\  & =\frac{\pi}{4}\left.\left[\frac{x^2}{2}\right]\right\vert_\frac{1}{2}^1\\ & =\frac{\pi}{4}(\frac{1}{2}-\frac{1}{8})\\ & =\frac{3\pi}{32} \end{align} However, the solution shows another answer: Solution screenshot Sorry it's in french, but to translate: they consider the area $T \cup T'$ which is symmetric (since $T \cap T' = \emptyset$ we can use the additivity property). From there, their final answer is $\frac{3}{16}$ . To top it off, I checked the value of the integral with symbolab just in case I made a calculation error, and it gives me $\frac{3\pi}{32}$ , which is what I got. The answer in the solution makes sense, but I don't understand why my method is wrong. I figure my error lies in how I interpreted the integral bounds, but when drawing the area $T$ on the cartesian plane, the bounds seem perfectly fine. If someone could please clarify the error.","I first started by setting the bounds inside the integral as follows: This integral is straight forward to evaluate: However, the solution shows another answer: Solution screenshot Sorry it's in french, but to translate: they consider the area which is symmetric (since we can use the additivity property). From there, their final answer is . To top it off, I checked the value of the integral with symbolab just in case I made a calculation error, and it gives me , which is what I got. The answer in the solution makes sense, but I don't understand why my method is wrong. I figure my error lies in how I interpreted the integral bounds, but when drawing the area on the cartesian plane, the bounds seem perfectly fine. If someone could please clarify the error."," T = \{ (x, y) \in R^2 \vert 0 \le y \le x, \frac{1}{2} \le x \le 1\} \int_\frac{1}{2}^1 \int_0^x \frac{x^2}{x^2 + y^2}dydx \begin{align}
\int_\frac{1}{2}^1 \int_0^x \frac{x^2}{x^2 + y^2}dydx & = 
\int_\frac{1}{2}^1 \int_0^x \frac{1}{1 + ({\frac{y}{x}})^2}dydx\\
& =\int_\frac{1}{2}^1 \left.\left[x\arctan(\frac{y}{x})\right]\right\vert_{y=0}^{y=x} dx\\ 
& =\int_\frac{1}{2}^1 x\arctan(1) dx\\ 
& =\frac{\pi}{4}\int_\frac{1}{2}^1 x dx\\ 
& =\frac{\pi}{4}\left.\left[\frac{x^2}{2}\right]\right\vert_\frac{1}{2}^1\\
& =\frac{\pi}{4}(\frac{1}{2}-\frac{1}{8})\\
& =\frac{3\pi}{32}
\end{align} T \cup T' T \cap T' = \emptyset \frac{3}{16} \frac{3\pi}{32} T","['real-analysis', 'multivariable-calculus', 'definite-integrals', 'volume']"
23,"How to evaluate $\int_0^1 \int_0^1 \int_0^{\frac{\pi}{4}} \frac{a^2 + b^2 \sin^2 x}{b^2 + a^2 \cos^2 x} \tan x \, dx \, db \, da $ [closed]",How to evaluate  [closed],"\int_0^1 \int_0^1 \int_0^{\frac{\pi}{4}} \frac{a^2 + b^2 \sin^2 x}{b^2 + a^2 \cos^2 x} \tan x \, dx \, db \, da ","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 6 months ago . Improve this question $$ \Omega = \int_0^1 \int_0^1 \int_0^{\frac{\pi}{4}} \frac{a^2 + b^2 \sin^2 x}{b^2 + a^2 \cos^2 x} \tan x \, dx \, db \, da $$ $$ \Omega_1 = \int_0^{\frac\pi4} \frac{a^2 + b^2 \sin^2 x}{b^2 + a^2 \cos^2 x} \tan x \, dx \quad \text{(let } t = \tan x \text{)} $$ $$ = \int_0^1 \frac{a^2 + (a^2 + b^2) t^2}{b^2 + a^2 + b^2 t^2} \cdot \frac{t}{1 + t^2} \, dt $$ $$ = \frac{1}{2} \int_0^1 \frac{a^2 + (a^2 + b^2) t}{(a^2 + b^2 + b^2 t)(1 + t)} \, dt $$ $$ = \frac{a^4 + b^4 + a^2 b^2}{2a^2 b^2} \int_0^1 \frac{b^2}{a^2 + b^2 + b^2 t} \, dt - \frac{b^2}{2a^2} \int_0^1 \frac{1}{1 + t} \, dt $$ $$ = \frac{1}{2} \left( \left(\frac{b}{a}\right)^2 + \left(\frac{a}{b}\right)^2 + 1 \right) \log \left(1 + \frac{\left(\frac{b}{a}\right)^2}{1 + \left(\frac{b}{a}\right)^2} \right) - \frac{\log(2)}{2} \left(\frac{b}{a}\right)^2 $$","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 6 months ago . Improve this question","
\Omega = \int_0^1 \int_0^1 \int_0^{\frac{\pi}{4}} \frac{a^2 + b^2 \sin^2 x}{b^2 + a^2 \cos^2 x} \tan x \, dx \, db \, da
 
\Omega_1 = \int_0^{\frac\pi4} \frac{a^2 + b^2 \sin^2 x}{b^2 + a^2 \cos^2 x} \tan x \, dx \quad \text{(let } t = \tan x \text{)}
 
= \int_0^1 \frac{a^2 + (a^2 + b^2) t^2}{b^2 + a^2 + b^2 t^2} \cdot \frac{t}{1 + t^2} \, dt
 
= \frac{1}{2} \int_0^1 \frac{a^2 + (a^2 + b^2) t}{(a^2 + b^2 + b^2 t)(1 + t)} \, dt
 
= \frac{a^4 + b^4 + a^2 b^2}{2a^2 b^2} \int_0^1 \frac{b^2}{a^2 + b^2 + b^2 t} \, dt - \frac{b^2}{2a^2} \int_0^1 \frac{1}{1 + t} \, dt
 
= \frac{1}{2} \left( \left(\frac{b}{a}\right)^2 + \left(\frac{a}{b}\right)^2 + 1 \right) \log \left(1 + \frac{\left(\frac{b}{a}\right)^2}{1 + \left(\frac{b}{a}\right)^2} \right) - \frac{\log(2)}{2} \left(\frac{b}{a}\right)^2
","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
24,How to prove $\sqrt{4a+1}+\sqrt{4b+1}+\sqrt{4c+1}\ge \frac{\sqrt{abc+4}+4\sqrt{ab+bc+ca+4}}{2}.$,How to prove,\sqrt{4a+1}+\sqrt{4b+1}+\sqrt{4c+1}\ge \frac{\sqrt{abc+4}+4\sqrt{ab+bc+ca+4}}{2}.,"Question. If $a,b,c\ge 0: a+b+c=2,$ prove that $$\color{blue}{\sqrt{4a+1}+\sqrt{4b+1}+\sqrt{4c+1}\ge \frac{\sqrt{abc+4}+4\sqrt{ab+bc+ca+4}}{2}.}$$ Equality holds iff $(a,b,c)=\{(0,1,1);(0,0,2)\}.$ Source: From a Israel Mathematical book. My strategy is using Jichen lemma, but the first condition is already wrong. Indeed, we can rewrite the original inequality as $$\color{black}{\sqrt{4a+1}+\sqrt{4b+1}+\sqrt{4c+1}\ge \sqrt{\frac{abc+4}{4}}+2\cdot\sqrt{ab+bc+ca+4}.}$$ Now, note that $$4(a+b+c)+3\ge \frac{abc+4}{4}+2(ab+bc+ca+4) $$ $$\iff 2\ge \frac{abc}{4}+2(ab+bc+ca),$$ which is already wrong when $a=b=c=\dfrac{2}{3}.$ I think that Jichen lemma is a good approach to kill this kind of inequality but the lemma doesn't work actually. I hope someone can find better ideas. All idea and comment are welcome.","Question. If prove that Equality holds iff Source: From a Israel Mathematical book. My strategy is using Jichen lemma, but the first condition is already wrong. Indeed, we can rewrite the original inequality as Now, note that which is already wrong when I think that Jichen lemma is a good approach to kill this kind of inequality but the lemma doesn't work actually. I hope someone can find better ideas. All idea and comment are welcome.","a,b,c\ge 0: a+b+c=2, \color{blue}{\sqrt{4a+1}+\sqrt{4b+1}+\sqrt{4c+1}\ge \frac{\sqrt{abc+4}+4\sqrt{ab+bc+ca+4}}{2}.} (a,b,c)=\{(0,1,1);(0,0,2)\}. \color{black}{\sqrt{4a+1}+\sqrt{4b+1}+\sqrt{4c+1}\ge \sqrt{\frac{abc+4}{4}}+2\cdot\sqrt{ab+bc+ca+4}.} 4(a+b+c)+3\ge \frac{abc+4}{4}+2(ab+bc+ca+4)  \iff 2\ge \frac{abc}{4}+2(ab+bc+ca), a=b=c=\dfrac{2}{3}.","['multivariable-calculus', 'inequality', 'lagrange-multiplier', 'a.m.-g.m.-inequality', 'jensen-inequality']"
25,"A Lagrange optimisation that I donât understand. $f(x, y, z) = x^2 + y^2$ is constrained by $5x^2 + 6xy + 5y^2 = 1$",A Lagrange optimisation that I donât understand.  is constrained by,"f(x, y, z) = x^2 + y^2 5x^2 + 6xy + 5y^2 = 1","The function $f(x, y, z) = x^2 + y^2$ is constrained by $5x^2 + 6xy + 5y^2 = 1$ . Using Lagrange, maximise and minimise this function. My original Lagrange equation was $$L = x^2 + y^2 + \lambda(1 - 5x^2 - 6xy - 5y^2).$$ After partially differentiating I got: $$\begin{aligned} L_x &= 2x - 10\lambda x - 6\lambda y ,\\ L_y &= 2y - 6\lambda x - 10\lambda y , \\ L_\lambda &= 1 - 5x^2 - 6xy - 5y^2 . \end{aligned}$$ As Iâm attempting to find a minimum and maximum, I set all the equations equal to $0$ . Rearranging the partial derivative w.r.t. $x$ gave me $$\lambda = \frac{2x}{10x + 6y}$$ and rearranging the partial derivative w.r.t. $y$ gave me $\lambda = 2y/(10y + 6x)$ . Putting these two equations equal to one another gives $$\frac{2y}{10y + 6x} = \frac{2x}{10x +6y}.$$ Multiplying both sides out led to $20xy + 12y^2 = 20xy + 12x^2$ . As such I could equate $x$ to $y$ . Subbing in $x$ for $y$ into the constraint gave me $16x^2 = 1$ and as such $x$ (and $y$ ) $= \pm \frac14$ . However, I donât think either $\left(\frac14, \frac14\right)$ or $\left(-\frac14, -\frac14\right)$ are the minimum and maximum points. If I have done something wrong please correct me but as of right now I am unsure what to do. Also, $z$ being listed in the function but actually having no part in any of the question confused me too. Was just hoping someone could help me understand.","The function is constrained by . Using Lagrange, maximise and minimise this function. My original Lagrange equation was After partially differentiating I got: As Iâm attempting to find a minimum and maximum, I set all the equations equal to . Rearranging the partial derivative w.r.t. gave me and rearranging the partial derivative w.r.t. gave me . Putting these two equations equal to one another gives Multiplying both sides out led to . As such I could equate to . Subbing in for into the constraint gave me and as such (and ) . However, I donât think either or are the minimum and maximum points. If I have done something wrong please correct me but as of right now I am unsure what to do. Also, being listed in the function but actually having no part in any of the question confused me too. Was just hoping someone could help me understand.","f(x, y, z) = x^2 + y^2 5x^2 + 6xy + 5y^2 = 1 L = x^2 + y^2 + \lambda(1 - 5x^2 - 6xy - 5y^2). \begin{aligned}
L_x &= 2x - 10\lambda x - 6\lambda y ,\\
L_y &= 2y - 6\lambda x - 10\lambda y , \\
L_\lambda &= 1 - 5x^2 - 6xy - 5y^2 .
\end{aligned} 0 x \lambda = \frac{2x}{10x + 6y} y \lambda = 2y/(10y + 6x) \frac{2y}{10y + 6x} = \frac{2x}{10x +6y}. 20xy + 12y^2 = 20xy + 12x^2 x y x y 16x^2 = 1 x y = \pm \frac14 \left(\frac14, \frac14\right) \left(-\frac14, -\frac14\right) z","['multivariable-calculus', 'optimization', 'lagrange-multiplier', 'qcqp']"
26,Chain rule in multivarible calculus,Chain rule in multivarible calculus,,"I recently was reading spivak calculus on manifold and I've arrived to chain rule he wrote that \begin{gather}F(x)=f(g_{1}(x),...,g_{m}(x))\end{gather} then he took the derivative which lead to \begin{gather} \frac{d F(x)}{dx}=\sum_{j=1}^{m}\frac{\partial f}{\partial g_{j}}.\frac{dg_{j} }{dx}\end{gather} let's talk about the case that $x$ is a scalar and $g_{1}(x)=x$ and $g_{2}(x)=x^2$ \begin{gather} \frac{d F(x)}{dx}=\frac{\partial f}{\partial x}.\frac{d x}{dx}+\frac{\partial f}{\partial (x^2)}.2x \end{gather} doesn't that mean that we've considered $f$ as being function of $2$ variables ,because we took the partial derivatives w.r.t  variables inside it ?","I recently was reading spivak calculus on manifold and I've arrived to chain rule he wrote that then he took the derivative which lead to let's talk about the case that is a scalar and and doesn't that mean that we've considered as being function of variables ,because we took the partial derivatives w.r.t  variables inside it ?","\begin{gather}F(x)=f(g_{1}(x),...,g_{m}(x))\end{gather} \begin{gather} \frac{d F(x)}{dx}=\sum_{j=1}^{m}\frac{\partial f}{\partial g_{j}}.\frac{dg_{j} }{dx}\end{gather} x g_{1}(x)=x g_{2}(x)=x^2 \begin{gather} \frac{d F(x)}{dx}=\frac{\partial f}{\partial x}.\frac{d x}{dx}+\frac{\partial f}{\partial (x^2)}.2x \end{gather} f 2",['multivariable-calculus']
27,How to solve an improper triple integral,How to solve an improper triple integral,,"Let $D=[(x,y,z)\in R^3: x^2+y^2+z^2\le 1, z\ge 0, z^2-x^2-y^2\le0]$ and let $f(x,y,z)=\frac {z} {\sqrt{x^2+y^2}}$ . The exercise is about calculating the integral: $\iiint_D f(x,y,z) dxdydz$ . The text tells to be careful because it is an improper integral. Now, I know how to use integration ""by wires"", that is used when we can write the domain $D$ as $D=[(x,y,z)\in R^3: g_1(x,y)\le z\le g_2(x,y); (x,y)\in E]$ , and ""by layers"", that is used when we can write the domain $D$ as $D=[(x,y,z)\in R^3:h_1\le z \le h_2; (x,y)\in E(z)]$ . I also know the Jacobians of spherical coordinates [ $J=r^2cos(\phi)$ ] and of cylindrical coordinates [ $J=r$ ] and how to use the change of variables in a triple integral. I know, at last, that $z^2-x^2-y^2\le 0$ is an infinite cone with vertex in the origin. Still, I don't know where to start the exercise.","Let and let . The exercise is about calculating the integral: . The text tells to be careful because it is an improper integral. Now, I know how to use integration ""by wires"", that is used when we can write the domain as , and ""by layers"", that is used when we can write the domain as . I also know the Jacobians of spherical coordinates [ ] and of cylindrical coordinates [ ] and how to use the change of variables in a triple integral. I know, at last, that is an infinite cone with vertex in the origin. Still, I don't know where to start the exercise.","D=[(x,y,z)\in R^3: x^2+y^2+z^2\le 1, z\ge 0, z^2-x^2-y^2\le0] f(x,y,z)=\frac {z} {\sqrt{x^2+y^2}} \iiint_D f(x,y,z) dxdydz D D=[(x,y,z)\in R^3: g_1(x,y)\le z\le g_2(x,y); (x,y)\in E] D D=[(x,y,z)\in R^3:h_1\le z \le h_2; (x,y)\in E(z)] J=r^2cos(\phi) J=r z^2-x^2-y^2\le 0",[]
28,Functions that are coercive along every line,Functions that are coercive along every line,,"A function $f:\mathbb{R}^n\to\mathbb{R}$ is called coercive if $$ \lim_{\|x\|\to\infty}f(x)=\infty. $$ To show that $f$ is coercive, we need to prove that for every sequence $\{x_n\}$ with $\|x_n\|\to\infty$ , it holds that $f(x_n)\to\infty$ . My question: Is it sufficient to check that $f$ is coercive along every line? That is, does the condition $$ \lim_{t\to\infty}f(x+td)=\infty,\quad\forall\,x\in\mathbb{R}^n,\quad\forall\,d\in\mathbb{R}^n\backslash\{0\} $$ imply that $f$ is coercive? If not, what additional assumptions (e.g., convexity and/or $C^1$ ) can we impose on $f$ to make this true?","A function is called coercive if To show that is coercive, we need to prove that for every sequence with , it holds that . My question: Is it sufficient to check that is coercive along every line? That is, does the condition imply that is coercive? If not, what additional assumptions (e.g., convexity and/or ) can we impose on to make this true?","f:\mathbb{R}^n\to\mathbb{R} 
\lim_{\|x\|\to\infty}f(x)=\infty.
 f \{x_n\} \|x_n\|\to\infty f(x_n)\to\infty f 
\lim_{t\to\infty}f(x+td)=\infty,\quad\forall\,x\in\mathbb{R}^n,\quad\forall\,d\in\mathbb{R}^n\backslash\{0\}
 f C^1 f","['real-analysis', 'multivariable-calculus', 'convex-analysis', 'coercive']"
29,"2D Laplacian with 1 Non-Homogeneous Dirichlet, 1 Homogeneous Dirichlet and 2 Homogeneous Neumann BCs, Rectangular Domain","2D Laplacian with 1 Non-Homogeneous Dirichlet, 1 Homogeneous Dirichlet and 2 Homogeneous Neumann BCs, Rectangular Domain",,"To preface, I am very new to PDEs and not overly familiar with hyperbolic trig so forgive me if there are obvious mistakes - I have tried my best to research this problem before asking this question. Additionally, I am not an applied mathematician so intuition and applications don't really help my understanding. The question: $$ u_{xx} + u_{yy} = 0, \;\; 0<x<1,\;\; 0<y<1 \\ u(x,0) = f(x), \;\; u(x,1) = 0 \\ u_x(0,y) = u_x(1,y) = 0 $$ Where $f(x)$ is an integrable function defined on $0\leq x\leq 1$ that has vanishing derivative at $x=0$ and $x=1$ . My Solution: Seek a solution of the form $u(x,y) = X(x)Y(y)$ , then the PDE becomes: $$ X''Y + XY'' = 0 \implies \frac{X''}{X}=-\frac{Y''}{Y} = \lambda $$ Implement homogeneous boundary conditions: $$ u_x(0,y) = X'(0)Y(y) = 0 \implies X'(0) = 0 \\ u_x(1,y) = X'(1)Y(y) = 0 \implies X'(1) = 0 \\ u_x(x,1) = X'(x)Y(1) = 0 \implies Y'(1) = 0  $$ Now, consider the ODE for $X(x)$ : $$ X'' - \lambda X = 0 $$ We require $\lambda < 0$ so introduce $\omega$ such that $\lambda = - \omega^2$ . Thus, we get general solution, $$\begin{align} X &= A\sin(\omega x) + B\cos(\omega x)\\ \implies X' &= A\omega \cos(\omega x) - B\omega \sin(\omega x)\\ \\ X'(0) &= A\omega \implies A = 0, \;\; X'(1) = -B\omega \sin(\omega) = 0 \implies \sin(\omega) = 0 \\ \end{align}$$ Thus, we get the solution for $X(x)$ : $$ X_n(x) = B_n \cos(\omega_n x), \;\; \lambda_n = -\omega_{n}^2, \;\; \omega_n = \pi n,\;\; n = 1, 2, 3,... $$ Now, the ODE for $Y(y)$ becomes: $$ Y_n'' - \omega_{n}^2 Y_n = 0 $$ Which has general solution, $$ Y_n = C_n \sinh(\pi n y) + D_n \cosh(\pi n y) $$ And, this is where I am stuck... Implementing the homogeneous boundary condition on $Y_n$ , we get $$ Y_n(1) = C_n \sinh(\pi n) + D_n \cosh(\pi n) = 0 $$ But, it seems to me that the only way to satisfy this is with the trivial solution $C_n = D_n = 0$ . So, my question is, am I missing something with how to implement this boundary condition? Or, have I done something wrong earlier in the question?","To preface, I am very new to PDEs and not overly familiar with hyperbolic trig so forgive me if there are obvious mistakes - I have tried my best to research this problem before asking this question. Additionally, I am not an applied mathematician so intuition and applications don't really help my understanding. The question: Where is an integrable function defined on that has vanishing derivative at and . My Solution: Seek a solution of the form , then the PDE becomes: Implement homogeneous boundary conditions: Now, consider the ODE for : We require so introduce such that . Thus, we get general solution, Thus, we get the solution for : Now, the ODE for becomes: Which has general solution, And, this is where I am stuck... Implementing the homogeneous boundary condition on , we get But, it seems to me that the only way to satisfy this is with the trivial solution . So, my question is, am I missing something with how to implement this boundary condition? Or, have I done something wrong earlier in the question?","
u_{xx} + u_{yy} = 0, \;\; 0<x<1,\;\; 0<y<1 \\
u(x,0) = f(x), \;\; u(x,1) = 0 \\
u_x(0,y) = u_x(1,y) = 0
 f(x) 0\leq x\leq 1 x=0 x=1 u(x,y) = X(x)Y(y) 
X''Y + XY'' = 0 \implies \frac{X''}{X}=-\frac{Y''}{Y} = \lambda
 
u_x(0,y) = X'(0)Y(y) = 0 \implies X'(0) = 0 \\
u_x(1,y) = X'(1)Y(y) = 0 \implies X'(1) = 0 \\
u_x(x,1) = X'(x)Y(1) = 0 \implies Y'(1) = 0 
 X(x) 
X'' - \lambda X = 0
 \lambda < 0 \omega \lambda = - \omega^2 \begin{align}
X &= A\sin(\omega x) + B\cos(\omega x)\\
\implies X' &= A\omega \cos(\omega x) - B\omega \sin(\omega x)\\
\\
X'(0) &= A\omega \implies A = 0, \;\; X'(1) = -B\omega \sin(\omega) = 0 \implies \sin(\omega) = 0 \\
\end{align} X(x) 
X_n(x) = B_n \cos(\omega_n x), \;\; \lambda_n = -\omega_{n}^2, \;\; \omega_n = \pi n,\;\; n = 1, 2, 3,...
 Y(y) 
Y_n'' - \omega_{n}^2 Y_n = 0
 
Y_n = C_n \sinh(\pi n y) + D_n \cosh(\pi n y)
 Y_n 
Y_n(1) = C_n \sinh(\pi n) + D_n \cosh(\pi n) = 0
 C_n = D_n = 0","['multivariable-calculus', 'partial-differential-equations', 'harmonic-functions', 'boundary-value-problem', 'hyperbolic-functions']"
30,Show that $\textbf{A}\cdot\nabla G=0$ for given $G$ and $\bf{A}$,Show that  for given  and,\textbf{A}\cdot\nabla G=0 G \bf{A},"I have solved $$yu_x+u_y=u^2$$ using $x=-\frac{s^2}{2},\,y=s>0,\,u=1$ and obtained the solution $$u(x,y)=\frac{1}{\sqrt{\frac{y^2}{2}-x}+1-y}$$ Now I am trying to show $\textbf{A}\cdot\nabla G=0$ where $$G(x,y,u)=\sqrt{\frac{y^2}{2}-x}+1-y-\dfrac{1}{u}=0, \qquad\textbf{A}=(y,1,u^2)$$ Therefore, as $$\nabla G = \left(-\frac{1}{2}\left(\frac{y^2}{2}-x\right)^{-\frac{1}{2}}+\dfrac{u_x}{u^2},\,\frac{y}{2}\left(\frac{y^2}{2}-x\right)^{-\frac{1}{2}}-1+\dfrac{u_y}{u^2},\,\frac{1}{u^2}\right)$$ I find that \begin{align} \textbf{A}\cdot{\nabla}G &= -\frac{y}{2}\left(\frac{y^2}{2}-x\right)^{-\frac{1}{2}}+y\dfrac{u_x}{u^2}+\frac{y}{2}\left(\frac{y^2}{2}-x\right)^{-\frac{1}{2}}-1+\dfrac{u_y}{u^2}+1 \\ &= y\frac{u_x}{u^2}+\frac{u_y}{u^2} \\ &= 1 \end{align} This should be equal to zero though so where have I gone wrong?","I have solved using and obtained the solution Now I am trying to show where Therefore, as I find that This should be equal to zero though so where have I gone wrong?","yu_x+u_y=u^2 x=-\frac{s^2}{2},\,y=s>0,\,u=1 u(x,y)=\frac{1}{\sqrt{\frac{y^2}{2}-x}+1-y} \textbf{A}\cdot\nabla G=0 G(x,y,u)=\sqrt{\frac{y^2}{2}-x}+1-y-\dfrac{1}{u}=0, \qquad\textbf{A}=(y,1,u^2) \nabla G = \left(-\frac{1}{2}\left(\frac{y^2}{2}-x\right)^{-\frac{1}{2}}+\dfrac{u_x}{u^2},\,\frac{y}{2}\left(\frac{y^2}{2}-x\right)^{-\frac{1}{2}}-1+\dfrac{u_y}{u^2},\,\frac{1}{u^2}\right) \begin{align}
\textbf{A}\cdot{\nabla}G &= -\frac{y}{2}\left(\frac{y^2}{2}-x\right)^{-\frac{1}{2}}+y\dfrac{u_x}{u^2}+\frac{y}{2}\left(\frac{y^2}{2}-x\right)^{-\frac{1}{2}}-1+\dfrac{u_y}{u^2}+1 \\
&= y\frac{u_x}{u^2}+\frac{u_y}{u^2} \\
&= 1
\end{align}",['multivariable-calculus']
31,Is it possible to explicitly determine max and min in this case?,Is it possible to explicitly determine max and min in this case?,,"On the set $$A = \{(x, y, z) \in \mathbb{R}^3; xy + xz + yz \leq 1\}$$ does the following function admit global/local max/min? $$f(x, y, z) = xyz$$ attempts So, firt of all I proved we cannot appeal to Weierstrass Theorem, for the set $A$ is closed but not bounded. It's closed because set of the form $[a, +\infty)$ (in one dimension) are closed. It's not bounded because, say, restricting on $x = y$ and $x = z$ , I obtain the restricted condition $- z^2 \leq 1$ , hence $z$ can be large enough such that no ball $B_r(0)$ can contain $A$ . (I proved it more rigorously; I omit this part because it's not important for the question). Now, for some reason I understood that $xy + xz + yz \leq 1$ represents a so called elliptic hyperboloid (when we take the $=$ sign, thanks to GeoGebra). I really do not know how to reduce that equation into the canonical form for an elliptic hyperboloid which shall be $$\frac{x^2}{a^2} + \frac{y^2}{b^2} - \frac{z^2}{c^2} = - 1$$ (some help about would be appreciated, even just to understand the method, but again: not really fundamental now). Since Weierstrass does not hold, there is no certainty about the existence of global max/min over $A$ . Question: is there a way to find those max/min, or to completely exclude their existence (or to prove it)? I thought I could study the behaviour of $f$ in the internal points of $A$ (with the gradient of $f$ ) and the points on the boundary. Though the gradient returns $$\nabla f = (0, 0, 0) \rightarrow \begin{cases} yz = 0 \\ xz = 0 \\ xy = 0 \end{cases}$$ Which are satisfied for points of the form $(0, 0, z)$ or $(0, y, 0)$ or $(x, 0, 0)$ . In all those points, the function returns zero as a value. I am stuck however on the boundary $\partial A$ , because I am not able to write down the equation for $\partial A$ . If I could, I would study the restriction of $f(x, y, z)$ on the boundary, and maybe manage a bit the thing. So is there a more efficient way? How to write down $\partial A$ ?","On the set does the following function admit global/local max/min? attempts So, firt of all I proved we cannot appeal to Weierstrass Theorem, for the set is closed but not bounded. It's closed because set of the form (in one dimension) are closed. It's not bounded because, say, restricting on and , I obtain the restricted condition , hence can be large enough such that no ball can contain . (I proved it more rigorously; I omit this part because it's not important for the question). Now, for some reason I understood that represents a so called elliptic hyperboloid (when we take the sign, thanks to GeoGebra). I really do not know how to reduce that equation into the canonical form for an elliptic hyperboloid which shall be (some help about would be appreciated, even just to understand the method, but again: not really fundamental now). Since Weierstrass does not hold, there is no certainty about the existence of global max/min over . Question: is there a way to find those max/min, or to completely exclude their existence (or to prove it)? I thought I could study the behaviour of in the internal points of (with the gradient of ) and the points on the boundary. Though the gradient returns Which are satisfied for points of the form or or . In all those points, the function returns zero as a value. I am stuck however on the boundary , because I am not able to write down the equation for . If I could, I would study the restriction of on the boundary, and maybe manage a bit the thing. So is there a more efficient way? How to write down ?","A = \{(x, y, z) \in \mathbb{R}^3; xy + xz + yz \leq 1\} f(x, y, z) = xyz A [a, +\infty) x = y x = z - z^2 \leq 1 z B_r(0) A xy + xz + yz \leq 1 = \frac{x^2}{a^2} + \frac{y^2}{b^2} - \frac{z^2}{c^2} = - 1 A f A f \nabla f = (0, 0, 0) \rightarrow \begin{cases} yz = 0 \\ xz = 0 \\ xy = 0 \end{cases} (0, 0, z) (0, y, 0) (x, 0, 0) \partial A \partial A f(x, y, z) \partial A","['real-analysis', 'multivariable-calculus', 'optimization']"
32,What does it mean for $f:\mathbb{R}^2 \to \mathbb{R}$ to be twice differentiable [duplicate],What does it mean for  to be twice differentiable [duplicate],f:\mathbb{R}^2 \to \mathbb{R},"This question already has an answer here : A question about derivatives between Euclidean spaces: how should we construct it and interpret its definition? (1 answer) Closed last year . Given some function $g:\mathbb{R} \to \mathbb{R}$ , it is twice differentiable if $g'(x)$ and $g''(x)$ both exist. But what about something like $f:\mathbb{R}^2 \to \mathbb{R}$ . Now I have partials to worry about. What is the definition of being twice differentiable in this case?","This question already has an answer here : A question about derivatives between Euclidean spaces: how should we construct it and interpret its definition? (1 answer) Closed last year . Given some function , it is twice differentiable if and both exist. But what about something like . Now I have partials to worry about. What is the definition of being twice differentiable in this case?",g:\mathbb{R} \to \mathbb{R} g'(x) g''(x) f:\mathbb{R}^2 \to \mathbb{R},"['multivariable-calculus', 'derivatives']"
33,"What if the partial derivates don't equal to zero for any value, how to find the critical point?","What if the partial derivates don't equal to zero for any value, how to find the critical point?",,"Given the function: $f(x,y) = \cos y\cdot e^x$ Question is to determine the nature of its critical points. I've calculated the partial derivatives as following : $\frac{\partial f}{\partial x} (x,y)=\cos y\cdot e^x$ $\frac{\partial f}{\partial y} (x,y)=-\sin y\cdot e^x$ None of these can equal to zero does that mean that the function doesn't have a critical point or is there another method?",Given the function: Question is to determine the nature of its critical points. I've calculated the partial derivatives as following : None of these can equal to zero does that mean that the function doesn't have a critical point or is there another method?,"f(x,y) = \cos y\cdot e^x \frac{\partial f}{\partial x} (x,y)=\cos y\cdot e^x \frac{\partial f}{\partial y} (x,y)=-\sin y\cdot e^x","['calculus', 'multivariable-calculus', 'optimization', 'partial-derivative', 'maxima-minima']"
34,Notation for a partially evaluated $n$ dimensional function,Notation for a partially evaluated  dimensional function,n,"I canât seem to find the right phrasing to google this question. Suppose I have function $$f:\mathbb{R}\times\mathbb{R}\to\mathbb{R}$$ you might denote the evaluation of $f$ at point $(x,y)$ as $f(x,y)$ . For any $x_0\in\mathbb{R}$ , you can make a function $g:\mathbb{R}\to\mathbb{R}, y\mapsto f(x_0,y)$ . Is there any standard notation for this function $g$ (which refers to $f$ and $x_0$ in some way)? (Note: I want to use this notation (above) to turn a function with $n$ variables mapping into a ( $n$ - $m$ ) dimensional codomain, the function $f$ above is just an example)","I canât seem to find the right phrasing to google this question. Suppose I have function you might denote the evaluation of at point as . For any , you can make a function . Is there any standard notation for this function (which refers to and in some way)? (Note: I want to use this notation (above) to turn a function with variables mapping into a ( - ) dimensional codomain, the function above is just an example)","f:\mathbb{R}\times\mathbb{R}\to\mathbb{R} f (x,y) f(x,y) x_0\in\mathbb{R} g:\mathbb{R}\to\mathbb{R}, y\mapsto f(x_0,y) g f x_0 n n m f","['multivariable-calculus', 'notation']"
35,What does the result of this integral mean or represent,What does the result of this integral mean or represent,,"If we have a wire shaped like C parabola $y=1-x^2$ , the length of the wire should be $\int _C dl$ where dl is a infinitesimal element of the length what will $\int_C F(x,y) dl$ generally  mean or represent ?","If we have a wire shaped like C parabola , the length of the wire should be where dl is a infinitesimal element of the length what will generally  mean or represent ?","y=1-x^2 \int _C dl \int_C F(x,y) dl","['calculus', 'integration', 'multivariable-calculus']"
36,Parameterize a closed surface.,Parameterize a closed surface.,,"We are told to find a tangent plane of the surface $$x^2 +2y^2+3z^2=36$$ at the point $(1,2,3)$ . Is it possible to parameterize this surface in 2 variables, perhaps with a spherical or cylindrical coordinate system? I attempted to solve it by creating $$F(x,y,z)=x^2 +2y^2+3z^2-36$$ and noting that the gradient of a function is normal to its surface, and all you need to define a tangent plane is a normal vector and a positional vector and you can denote the plane as $$r\cdot n=a\cdot n$$ Is this approach correct? Is there a better way to do this?","We are told to find a tangent plane of the surface at the point . Is it possible to parameterize this surface in 2 variables, perhaps with a spherical or cylindrical coordinate system? I attempted to solve it by creating and noting that the gradient of a function is normal to its surface, and all you need to define a tangent plane is a normal vector and a positional vector and you can denote the plane as Is this approach correct? Is there a better way to do this?","x^2 +2y^2+3z^2=36 (1,2,3) F(x,y,z)=x^2 +2y^2+3z^2-36 r\cdot n=a\cdot n",['multivariable-calculus']
37,"Why the divergence of vector field F(x, y) = (x, y) equal to 2 at every point?","Why the divergence of vector field F(x, y) = (x, y) equal to 2 at every point?",,"Suppose there's a 2d vector field: $F(x,y) = (x,y)$ . Meaning that at the coordinate $(0,0)$ it's a $0$ -vector, at $(0,1)$ it's a $(0,1)$ vector and so on. Here's a picture of it: If you mathematically calculate the divergence of this vector field, you will get the following scalar field: $F(x,y) = 2$ . So, at every point the divergence is constant and equals 2. Now I'm trying to understand what that does physically mean. According to Wikipedia: In physical terms, the divergence of a vector field is the extent to which the vector field flux behaves like a source at a given point. It is a local measure of its ""outgoingness"" â the extent to which there are more of the field vectors exiting from an infinitesimal region of space than entering it. <..>  The greater the flux of field through a small surface enclosing a given point, the greater the value of divergence at that point. So, for example, we take a point $(0, 0)$ and draw a small circle around it and see how much of the outward going flux there is: According to this picture, all the arrows point outwards from the circle, no matter how small the circle is. So divergence at point $(0,0)$ is positive, which checks out with the mathematically calculated value of 2. So, at point $(0,0)$ the vector field acts like a 'source', which makes sense. Now we take another point $(1,1)$ and also draw a circle around it: On this image there are arrows entering the circle area and the same amount of arrows leaving the circle area, so the net flux at the point $(1,1)$ should be 0, and the divergence at this point also should be 0, but mathematically calculated value is 2. Why? The same applies to any other point which isn't $(0, 0)$ .","Suppose there's a 2d vector field: . Meaning that at the coordinate it's a -vector, at it's a vector and so on. Here's a picture of it: If you mathematically calculate the divergence of this vector field, you will get the following scalar field: . So, at every point the divergence is constant and equals 2. Now I'm trying to understand what that does physically mean. According to Wikipedia: In physical terms, the divergence of a vector field is the extent to which the vector field flux behaves like a source at a given point. It is a local measure of its ""outgoingness"" â the extent to which there are more of the field vectors exiting from an infinitesimal region of space than entering it. <..>  The greater the flux of field through a small surface enclosing a given point, the greater the value of divergence at that point. So, for example, we take a point and draw a small circle around it and see how much of the outward going flux there is: According to this picture, all the arrows point outwards from the circle, no matter how small the circle is. So divergence at point is positive, which checks out with the mathematically calculated value of 2. So, at point the vector field acts like a 'source', which makes sense. Now we take another point and also draw a circle around it: On this image there are arrows entering the circle area and the same amount of arrows leaving the circle area, so the net flux at the point should be 0, and the divergence at this point also should be 0, but mathematically calculated value is 2. Why? The same applies to any other point which isn't .","F(x,y) = (x,y) (0,0) 0 (0,1) (0,1) F(x,y) = 2 (0, 0) (0,0) (0,0) (1,1) (1,1) (0, 0)","['multivariable-calculus', 'terminology', 'divergence-operator']"
38,Find coordinates of centroid of an area,Find coordinates of centroid of an area,,"I would like to find the centroid $(x_c^A,y_c^A)$ of the orange area $A$ , marked in the attached picture, i.e. of the area between $$ f(x)=\ln(x)\qquad\textrm{ and }\qquad g(x)=1, 0\leqslant x\leqslant e. $$ To this end, I first determined that $A=e-1$ , because $B=\int_{1}^e\ln(x)\, dx=1$ . Hence $C=A+B=e$ . Now, the $x$ -coordinate $x_c^C$ of the centroid of $C$ , which is a rectangle satisfies $$ x_c^C=\frac{(x_c^A\cdot A)+(x_c^B\cdot B)}{C}=\frac{e}{2}. $$ Moreover, $$ x_c^B=\frac{1}{B}\int_1^e x\ln x\, dx=\frac{1}{4}(e^2+1), $$ so that $$ x_c^A=\frac{e^2-1}{4(e-1)}. $$ Equivalently, I get $$ y_c^A=\frac{1}{e-1}. $$ Would like to know if I am correct.","I would like to find the centroid of the orange area , marked in the attached picture, i.e. of the area between To this end, I first determined that , because . Hence . Now, the -coordinate of the centroid of , which is a rectangle satisfies Moreover, so that Equivalently, I get Would like to know if I am correct.","(x_c^A,y_c^A) A 
f(x)=\ln(x)\qquad\textrm{ and }\qquad g(x)=1, 0\leqslant x\leqslant e.
 A=e-1 B=\int_{1}^e\ln(x)\, dx=1 C=A+B=e x x_c^C C 
x_c^C=\frac{(x_c^A\cdot A)+(x_c^B\cdot B)}{C}=\frac{e}{2}.
 
x_c^B=\frac{1}{B}\int_1^e x\ln x\, dx=\frac{1}{4}(e^2+1),
 
x_c^A=\frac{e^2-1}{4(e-1)}.
 
y_c^A=\frac{1}{e-1}.
","['multivariable-calculus', 'centroid']"
39,Green's First and Second Identities,Green's First and Second Identities,,"I was doing some research on potential theory and Green's identities and noticed that most of the literature I find on the subject tends to define the vector field $$\tag{1}\vec F=\phi\,\text{grad}(\psi)$$ where $\phi$ and $\psi$ are $C^2$ scalar fields. Question 1 : If $\vec F$ is supposed to be a conservative field (i.e, curl-free), then wouldn't the $\phi$ term make the curl of the gradient non-zero and hence make $\vec F$ non-conservative? If potential theory is supposed to be built on conservative fields, then what is the purpose of the $\phi$ term? Are we just assuming some general form of $\vec F$ ? Question 2 : By plugging in $(1)$ into the Divergence theorem in $\mathbb R^3$ $$\iint_{\partial V}\vec F\cdot \hat n\;dS=\iiint_V\text{div}(\vec F)\;dV,$$ we can obtain $$\tag{G1}\iint_{\partial V}\phi\frac{\partial \psi}{\partial \hat n}\;dS=\iiint_V(\vec\nabla\phi\cdot\vec\nabla\psi+\phi\nabla^2\psi)\;dV,$$ which is known as Green's first identity. By interchanging $\phi$ and $\psi$ in $(1)$ (i.e, letting $\vec F=\psi\,\text{grad}(\phi)$ ), we can obtain $$\tag{G1'}\iint_{\partial V}\psi\frac{\partial \phi}{\partial \hat n}\;dS=\iiint_V(\vec\nabla\psi\cdot\vec\nabla\phi+\psi\nabla^2\phi)\;dV.$$ Subtracting equation $(G1')$ from $(G1)$ will then yield the famous Green's second identity $$\tag{G2}\iint_{\partial V}\bigg(\phi\frac{\partial \psi}{\partial \hat n}-\psi\frac{\partial \phi}{\partial \hat n}\bigg)\;dS=\iiint_V\big(\phi\nabla^2\psi-\psi\nabla^2\phi\big)\;dV.$$ My question: Why are we allowed to interchange $\phi$ and $\psi$ in $(1)$ ? Doesn't switching these terms results in a completely new vector field? I appreciate any and all clarification! :)","I was doing some research on potential theory and Green's identities and noticed that most of the literature I find on the subject tends to define the vector field where and are scalar fields. Question 1 : If is supposed to be a conservative field (i.e, curl-free), then wouldn't the term make the curl of the gradient non-zero and hence make non-conservative? If potential theory is supposed to be built on conservative fields, then what is the purpose of the term? Are we just assuming some general form of ? Question 2 : By plugging in into the Divergence theorem in we can obtain which is known as Green's first identity. By interchanging and in (i.e, letting ), we can obtain Subtracting equation from will then yield the famous Green's second identity My question: Why are we allowed to interchange and in ? Doesn't switching these terms results in a completely new vector field? I appreciate any and all clarification! :)","\tag{1}\vec F=\phi\,\text{grad}(\psi) \phi \psi C^2 \vec F \phi \vec F \phi \vec F (1) \mathbb R^3 \iint_{\partial V}\vec F\cdot \hat n\;dS=\iiint_V\text{div}(\vec F)\;dV, \tag{G1}\iint_{\partial V}\phi\frac{\partial \psi}{\partial \hat n}\;dS=\iiint_V(\vec\nabla\phi\cdot\vec\nabla\psi+\phi\nabla^2\psi)\;dV, \phi \psi (1) \vec F=\psi\,\text{grad}(\phi) \tag{G1'}\iint_{\partial V}\psi\frac{\partial \phi}{\partial \hat n}\;dS=\iiint_V(\vec\nabla\psi\cdot\vec\nabla\phi+\psi\nabla^2\phi)\;dV. (G1') (G1) \tag{G2}\iint_{\partial V}\bigg(\phi\frac{\partial \psi}{\partial \hat n}-\psi\frac{\partial \phi}{\partial \hat n}\bigg)\;dS=\iiint_V\big(\phi\nabla^2\psi-\psi\nabla^2\phi\big)\;dV. \phi \psi (1)","['multivariable-calculus', 'partial-differential-equations', 'vector-analysis', 'greens-function', 'potential-theory']"
40,Getting different values of the integral when integrating with different order,Getting different values of the integral when integrating with different order,,"I want to determine the volume of the shape given by the following $$ K = \{(x,y,z): x^2+y^2+z^2 \leq 2 \, \, , x+y>0 \, \, , \, z\leq 1\} $$ I thought that i can integrate with respect to $xy$ -plane first by using polar coordinates $$ \int_{-\sqrt{2}}^1 \biggl( \int_{-\frac{\pi}{4}}^{\frac{3 \pi}{4}} \int_0^{\sqrt{2-z^2}} r \, \, drd\theta \biggr) \, dz \quad=\quad \frac{\pi}{2} \int_{-\sqrt{2}}^1 2-z^2 \, dz \quad=\quad ..... \quad=\quad \frac{\pi}{2} \biggl( \frac{5-2\sqrt{2}}{3} + 2\sqrt{2} \biggr) $$ But if I integrate with respect to $z$ first I get the following $$ \iint_D \biggl( \int_{-\sqrt{2-(x^2+y^2)}}^1 \, dz \biggr) \, dxdy \quad=\quad \int_{-\frac{\pi}{4}}^{\frac{3 \pi}{4}} \int_0^{\sqrt{2}} \biggl(1+\sqrt{2-r^2} \biggr) r \, \, drd\theta  = \, \, ...... \, \, = \quad \frac{\pi}{3} \biggl( 3 + 2 \sqrt{2} \biggr) $$ Which is very confusing since the value of the integral should be the same regardless of the order of integration! Can anyone see where I missed up?",I want to determine the volume of the shape given by the following I thought that i can integrate with respect to -plane first by using polar coordinates But if I integrate with respect to first I get the following Which is very confusing since the value of the integral should be the same regardless of the order of integration! Can anyone see where I missed up?," K = \{(x,y,z): x^2+y^2+z^2 \leq 2 \, \, , x+y>0 \, \, , \, z\leq 1\}  xy  \int_{-\sqrt{2}}^1 \biggl( \int_{-\frac{\pi}{4}}^{\frac{3 \pi}{4}} \int_0^{\sqrt{2-z^2}} r \, \, drd\theta \biggr) \, dz \quad=\quad \frac{\pi}{2} \int_{-\sqrt{2}}^1 2-z^2 \, dz \quad=\quad ..... \quad=\quad \frac{\pi}{2} \biggl( \frac{5-2\sqrt{2}}{3} + 2\sqrt{2} \biggr)  z  \iint_D \biggl( \int_{-\sqrt{2-(x^2+y^2)}}^1 \, dz \biggr) \, dxdy \quad=\quad \int_{-\frac{\pi}{4}}^{\frac{3 \pi}{4}} \int_0^{\sqrt{2}} \biggl(1+\sqrt{2-r^2} \biggr) r \, \, drd\theta  = \, \, ...... \, \, = \quad \frac{\pi}{3} \biggl( 3 + 2 \sqrt{2} \biggr) ","['calculus', 'integration', 'multivariable-calculus', 'polar-coordinates', 'volume']"
41,Find the value of $\int_{0}^{\sqrt{3}}\int_{0}^{\tan^{-1}y}\sqrt{xy}\ dxdy$,Find the value of,\int_{0}^{\sqrt{3}}\int_{0}^{\tan^{-1}y}\sqrt{xy}\ dxdy,How can I find the value of $I = \int_{0}^{\sqrt{3}}\int_{0}^{\tan^{-1}y}\sqrt{xy}\ dxdy$ ? I tried changing the limits and got the new integral as $I = \int_{0}^{\pi/3}\int_{\tan(x)}^{\sqrt{3}}\sqrt{xy}\ dydx$ And then tried evaluating it to get $I = \frac{2}{3} \int_{0}^{\frac{\pi}{3}}\sqrt{x}(3^\frac{3}{4} - \tan^\frac{3}{2}x)\ dx$ but I do not know how to proceed further to get the result Wolfram alpha gives the same answer on all cases so I don't think my steps are wrong. Or is this an XY problem at this point?,How can I find the value of ? I tried changing the limits and got the new integral as And then tried evaluating it to get but I do not know how to proceed further to get the result Wolfram alpha gives the same answer on all cases so I don't think my steps are wrong. Or is this an XY problem at this point?,I = \int_{0}^{\sqrt{3}}\int_{0}^{\tan^{-1}y}\sqrt{xy}\ dxdy I = \int_{0}^{\pi/3}\int_{\tan(x)}^{\sqrt{3}}\sqrt{xy}\ dydx I = \frac{2}{3} \int_{0}^{\frac{\pi}{3}}\sqrt{x}(3^\frac{3}{4} - \tan^\frac{3}{2}x)\ dx,"['integration', 'multivariable-calculus', 'definite-integrals']"
42,"Evaluate $\iint (\frac{(x-y)}{x+y})^4$ over the triangular region bounded by $x+y=1$, $x$-axis ,$y$-axis","Evaluate  over the triangular region bounded by , -axis ,-axis",\iint (\frac{(x-y)}{x+y})^4 x+y=1 x y,"Evaluate $\iint (\frac{(x-y)}{x+y})^4$ over the triangular region bounded by $x+y=1$ , $x$ -axis, $y$ -axis. My attempt: I tried using the change of variable concept: Let $u=x-y$ and $v=x+y$ , $|J|= \frac{1}{2}$ Then $x=\frac{u+v}{2}$ and $y=\frac{u-v}{2}$ Now we have to find the limit which is where I am stuck. We were told in class that we could plug in the points and get the equations: In this case the points are $(0,0),(0,1),(1,0)$ . So if we put the first point we get $u=0,v=0$ or $u=-v,u=v$ If we put in the second point we get $u=-1,v=1$ , $u=-v,u=v+2$ Similarly $u=1,v=-1$ , $u=v,u=v-2$ How do I proceed after this? This is making me even more confused on what values to take and what values not to take.can anyone help me out here with an easy explanation?","Evaluate over the triangular region bounded by , -axis, -axis. My attempt: I tried using the change of variable concept: Let and , Then and Now we have to find the limit which is where I am stuck. We were told in class that we could plug in the points and get the equations: In this case the points are . So if we put the first point we get or If we put in the second point we get , Similarly , How do I proceed after this? This is making me even more confused on what values to take and what values not to take.can anyone help me out here with an easy explanation?","\iint (\frac{(x-y)}{x+y})^4 x+y=1 x y u=x-y v=x+y |J|= \frac{1}{2} x=\frac{u+v}{2} y=\frac{u-v}{2} (0,0),(0,1),(1,0) u=0,v=0 u=-v,u=v u=-1,v=1 u=-v,u=v+2 u=1,v=-1 u=v,u=v-2","['calculus', 'integration', 'multivariable-calculus', 'multiple-integral']"
43,Bounds on Jacobian in terms of Volume Distortion,Bounds on Jacobian in terms of Volume Distortion,,"Let $V\subset U\subseteq \mathbb{R}^k$ and let $\varphi:U\to V$ be a diffeomorphism. Let $J_{\varphi}$ be its Jacobian, so $vol(V) = \int_U |\det J_{\varphi}|$ . I am wondering if there are any general purpose lower bounds on $|\det J_{\varphi}(u)|$ over all $u\in U$ . For example, if $\varphi$ is measure/volume-preserving, $|\det J_{\varphi}| = 1$ . My intuition is that something like $$|\det J_{\varphi}|\ge \inf_{S\subset U}\frac{vol(S)}{vol(\varphi(S))}$$ should hold (with the convention that $\tfrac{0}{0} = 1$ ), but I'm unable to find any resources (and have not been able to furnish a proof nor counterexample -- my multivariable calculus is very rusty). Also, for my particular purpose I know that $vol(\varphi(S))\le vol(S)$ for all $S\subset U$ . Any help/hints are appreciated! EDIT: My intuition also says that an inequality of the other direction should hold as well: $$|\det J_{\varphi^{-1}}|\le \sup_{T\subset V}\frac{vol(\varphi^{-1}(T))}{vol(T)}.$$ Also, in my particular setup, $U = [0,1]^k$ . An idea I have for this is to bound the Jacobian by that of a linear map that $\phi([0,s]^k) = [0,1]^k$ that scales points up by a factor $1/s$ (for $0 < s < 1$ ). We'd choose $s$ to be minimal such that $[0,s]^k\supset V$ or something. But I'm not sure how to make meaningful use of this train of thought.","Let and let be a diffeomorphism. Let be its Jacobian, so . I am wondering if there are any general purpose lower bounds on over all . For example, if is measure/volume-preserving, . My intuition is that something like should hold (with the convention that ), but I'm unable to find any resources (and have not been able to furnish a proof nor counterexample -- my multivariable calculus is very rusty). Also, for my particular purpose I know that for all . Any help/hints are appreciated! EDIT: My intuition also says that an inequality of the other direction should hold as well: Also, in my particular setup, . An idea I have for this is to bound the Jacobian by that of a linear map that that scales points up by a factor (for ). We'd choose to be minimal such that or something. But I'm not sure how to make meaningful use of this train of thought.","V\subset U\subseteq \mathbb{R}^k \varphi:U\to V J_{\varphi} vol(V) = \int_U |\det J_{\varphi}| |\det J_{\varphi}(u)| u\in U \varphi |\det J_{\varphi}| = 1 |\det J_{\varphi}|\ge \inf_{S\subset U}\frac{vol(S)}{vol(\varphi(S))} \tfrac{0}{0} = 1 vol(\varphi(S))\le vol(S) S\subset U |\det J_{\varphi^{-1}}|\le \sup_{T\subset V}\frac{vol(\varphi^{-1}(T))}{vol(T)}. U = [0,1]^k \phi([0,s]^k) = [0,1]^k 1/s 0 < s < 1 s [0,s]^k\supset V","['multivariable-calculus', 'differential-geometry', 'jacobian', 'diffeomorphism']"
44,Solution verification for a line integral,Solution verification for a line integral,,"Question: Sami walked from the point $(0,1,2)$ to $(3,2,4)$ in a straight line. Given that $A$ is the line segment from which she walked, determine the value of the following integral(Hint: Use parametric equations.): $$ \int_{A}^{} x^{2}dx + y^{2}dy + xzdz $$ Answer:(Sorry if this seems like more of a discussion than an answer, i'm still learning and it helps when I write down everything going on in my head.) First we parameterise the curve $A$ . First, find the direction vector for the line segment. In order to do this we subtract the two points that we originally have, thus: $(3,2,4)-(0,1,2) = \langle 3,1,2 \rangle$ . Now let us define the parametric function $r(t)$ , in order to parameterise the curve we must multiply by $t$ . Thus: $r(t) = \langle 0,1,2 \rangle + t\langle 3,1,2 \rangle $ , simplifying we eventually get $r(t) = \langle 3t,t + 1,2t + 2 \rangle$ . In order for us to get a definite integral we must find some sort of interval. To do this let us test the points $0,1$ . It follows that: $r(0) = \langle 0,1,2 \rangle$ and that, $r(1) = \langle 3,2,4 \rangle$ . Now we can finally get rid of this Line integral and replace it with a definite one with the interval $[0,1]$ , However first we must calculate a few derivatives. $$A: \begin{cases} x(t)=0+3t,\\y(t)=1+t,\\z(t)+2+2t\end{cases},\quad 0\leqslant t\leqslant 1.$$ Therefore, $x'(t)=3,y'(t)=1,z'(t)=2$ . Now that we have these values we can proceed to evaluate the definite integral: $$\begin{equation}\begin{aligned} \int_{A}^{} x^{2}dx + y^{2}dy + xzdz &= \int_{0}^{1} x^{2}dx + y^{2}dy + xzdz \\  &= \int_{0}^{1} (3t)^{2}(3) + (1 + t)^{2}(1) + (3t)(2t + 2)(2) dt \\   &= \int_{0}^{1} 27t^{2} + t^{2} + 2t + 1 + 12t^{2} + 12t dt \\   &= \int_{0}^{1} 40t^{2} + 1 + 14t dt \end{aligned}\end{equation}$$ Now we can simply evaluate this using the power rule: $$ \int_{0}^{1} \frac{40t^{3}}{3} + t + 7t^{2} dt= \left[\frac{40(1)^{3}}{3} + (1)^{2} + 7(1)^{2}\right] - \left[\frac{40(0)^{3}}{3} + (0)^{2} + 7(0)^{2}\right]=\frac{64}{3}.$$","Question: Sami walked from the point to in a straight line. Given that is the line segment from which she walked, determine the value of the following integral(Hint: Use parametric equations.): Answer:(Sorry if this seems like more of a discussion than an answer, i'm still learning and it helps when I write down everything going on in my head.) First we parameterise the curve . First, find the direction vector for the line segment. In order to do this we subtract the two points that we originally have, thus: . Now let us define the parametric function , in order to parameterise the curve we must multiply by . Thus: , simplifying we eventually get . In order for us to get a definite integral we must find some sort of interval. To do this let us test the points . It follows that: and that, . Now we can finally get rid of this Line integral and replace it with a definite one with the interval , However first we must calculate a few derivatives. Therefore, . Now that we have these values we can proceed to evaluate the definite integral: Now we can simply evaluate this using the power rule:","(0,1,2) (3,2,4) A  \int_{A}^{} x^{2}dx + y^{2}dy + xzdz  A (3,2,4)-(0,1,2) = \langle 3,1,2 \rangle r(t) t r(t) = \langle 0,1,2 \rangle + t\langle 3,1,2 \rangle  r(t) = \langle 3t,t + 1,2t + 2 \rangle 0,1 r(0) = \langle 0,1,2 \rangle r(1) = \langle 3,2,4 \rangle [0,1] A: \begin{cases} x(t)=0+3t,\\y(t)=1+t,\\z(t)+2+2t\end{cases},\quad 0\leqslant t\leqslant 1. x'(t)=3,y'(t)=1,z'(t)=2 \begin{equation}\begin{aligned}
\int_{A}^{} x^{2}dx + y^{2}dy + xzdz &= \int_{0}^{1} x^{2}dx + y^{2}dy + xzdz \\
 &= \int_{0}^{1} (3t)^{2}(3) + (1 + t)^{2}(1) + (3t)(2t + 2)(2) dt \\
  &= \int_{0}^{1} 27t^{2} + t^{2} + 2t + 1 + 12t^{2} + 12t dt \\
  &= \int_{0}^{1} 40t^{2} + 1 + 14t dt
\end{aligned}\end{equation}  \int_{0}^{1} \frac{40t^{3}}{3} + t + 7t^{2} dt= \left[\frac{40(1)^{3}}{3} + (1)^{2} + 7(1)^{2}\right] - \left[\frac{40(0)^{3}}{3} + (0)^{2} + 7(0)^{2}\right]=\frac{64}{3}.","['multivariable-calculus', 'solution-verification', 'line-integrals']"
45,Integration by parts on dot products of two vectors,Integration by parts on dot products of two vectors,,"How can we take the integration by parts of the following, $$ \int_{\mathcal{V}} (\nabla a) \cdot (\nabla b) \;d\tau$$ Where one can also consider $a$ as a scalar over $\mathcal{V}$ . I'm not sure how to approach doing integration by parts for a dot product of two gradient vectors. How does one approach this integral?","How can we take the integration by parts of the following, Where one can also consider as a scalar over . I'm not sure how to approach doing integration by parts for a dot product of two gradient vectors. How does one approach this integral?", \int_{\mathcal{V}} (\nabla a) \cdot (\nabla b) \;d\tau a \mathcal{V},"['multivariable-calculus', 'vector-analysis']"
46,Hessian Calculation in Higher Dimensions,Hessian Calculation in Higher Dimensions,,"In this paper https://arxiv.org/pdf/2003.00307.pdf , section $3$ , author has written the hessian in compact form which is not clear to me. He considers an over-parametrisation system $\mathcal{F}(\mathbf{w}):\mathbb{R}^m\rightarrow\mathbb{R}^n$ , where $m>n$ , with the square loss function $\mathcal{L}(F(\mathbf{w}),\mathbf{y})=\frac{1}{2}\|\mathcal{F}(\mathbf{w})-\mathbf{y}\|^2$ . Then the hessian matrix of the loss function takes the form $$H_{\mathcal{L}}(\mathbf{w})=D\mathcal{F}(\mathbf{w})^T\frac{\partial^2\mathcal{L}}{\partial\mathcal{F}^2}D\mathcal{F}(\mathbf{w}) +\sum_{i=1}^n(\mathcal{F}(\mathbf{w})-\mathbf{y})_iH_{\mathcal{F}_i}(\mathbf{w})$$ . My Approach: I tried to apply chain rule as follows: $$\frac{\partial\mathcal{L}(F(\mathbf{w}),\mathbf{y})}{\partial\mathbf{w}}=\frac{\partial\mathcal{L}(F(\mathbf{w}),\mathbf{y})}{\partial\mathcal{F}(\mathbf{w})}\times\frac{\partial\mathcal{F}(\mathbf{w})}{\partial\mathbf{w}}\\ =(\mathcal{F}(\mathbf{w})-\mathbf{y})D\mathcal{F}(\mathbf{w})$$ Again, we differentiate with $\mathbf{w}$ as follows: $$\frac{\partial^2\mathcal{L}(F(\mathbf{w}),\mathbf{y})}{\partial\mathbf{w}^2}=\frac{\partial(\mathcal{F}(\mathbf{w})-\mathbf{y})}{\partial\mathbf{w}}D\mathcal{F}(\mathbf{w})+(\mathcal{F}(\mathbf{w})-\mathbf{y})D^2\mathcal{F}(\mathbf{w})$$ please explain me in detail if possible.","In this paper https://arxiv.org/pdf/2003.00307.pdf , section , author has written the hessian in compact form which is not clear to me. He considers an over-parametrisation system , where , with the square loss function . Then the hessian matrix of the loss function takes the form . My Approach: I tried to apply chain rule as follows: Again, we differentiate with as follows: please explain me in detail if possible.","3 \mathcal{F}(\mathbf{w}):\mathbb{R}^m\rightarrow\mathbb{R}^n m>n \mathcal{L}(F(\mathbf{w}),\mathbf{y})=\frac{1}{2}\|\mathcal{F}(\mathbf{w})-\mathbf{y}\|^2 H_{\mathcal{L}}(\mathbf{w})=D\mathcal{F}(\mathbf{w})^T\frac{\partial^2\mathcal{L}}{\partial\mathcal{F}^2}D\mathcal{F}(\mathbf{w}) +\sum_{i=1}^n(\mathcal{F}(\mathbf{w})-\mathbf{y})_iH_{\mathcal{F}_i}(\mathbf{w}) \frac{\partial\mathcal{L}(F(\mathbf{w}),\mathbf{y})}{\partial\mathbf{w}}=\frac{\partial\mathcal{L}(F(\mathbf{w}),\mathbf{y})}{\partial\mathcal{F}(\mathbf{w})}\times\frac{\partial\mathcal{F}(\mathbf{w})}{\partial\mathbf{w}}\\ =(\mathcal{F}(\mathbf{w})-\mathbf{y})D\mathcal{F}(\mathbf{w}) \mathbf{w} \frac{\partial^2\mathcal{L}(F(\mathbf{w}),\mathbf{y})}{\partial\mathbf{w}^2}=\frac{\partial(\mathcal{F}(\mathbf{w})-\mathbf{y})}{\partial\mathbf{w}}D\mathcal{F}(\mathbf{w})+(\mathcal{F}(\mathbf{w})-\mathbf{y})D^2\mathcal{F}(\mathbf{w})","['multivariable-calculus', 'machine-learning', 'hessian-matrix', 'non-convex-optimization']"
47,Implicit function theorem for overdetermined system of nonlinear equations,Implicit function theorem for overdetermined system of nonlinear equations,,"Consider a sufficiently regular ( $C^1$ ?) function $$F:\mathbb{R}^{m}\times\mathbb{R}^{n}\to\mathbb{R}^{n+k}$$ with $k>0$ . And assume an implicit function $y(x)$ is locally well defined by the condition $$F(x,y(x))=0$$ I am interested in implicit differentiation techniques which apply in this contest, extending the implicit function theorem. EDIT My situation of interest is the following. There is a $C^1$ function $$G:\mathbb{R}^{m}\times\mathbb{R}^{n}\to\mathbb{R}^{n}$$ where I think of $x\in\mathbb{R}^{m}$ as parameters and $y\in\mathbb{R}^{n}$ as variables. Of course, for each $\bar{x},\bar{y}$ such that $$G(\bar{x},\bar{y})=0$$ The IFT ensures there exists a local function $$f:U\ni \bar{x}\to V\ni \bar{y}$$ such that $$G(x,f(x))=0\quad\forall x\in U$$ Moreover, it tells that $$D_x f(\bar{x})=-[D_y G(\bar{x},f(\bar{x}))]^{-1}[D_x G(\bar{x},f(\bar{x}))]$$ Now, I am interested in the value of $D_x f(\bar{x})$ at points which not only satisfy $G(\bar{x},\bar{y})=0$ , but also an additional feasibility condition $$s(\bar{x},\bar{y})=0\quad\text{where}\quad s:\mathbb{R}^{m}\times\mathbb{R}^{n}\to\mathbb{R}$$ My doubt : Is it enough to consider $D_x f(\bar{x})=-[D_y G(\bar{x},f(\bar{x}))]^{-1}[D_x G(\bar{x},f(\bar{x}))]$ at points where $s(\bar{x},\bar{y})=0$ or does the additional constraint changes the shape of the implicit function $f$ , so that we need a different approach? To get this other with, I thought of considering the function $$F\equiv\binom{G}{s}:\mathbb{R}^{m}\times\mathbb{R}^{n}\to\mathbb{R}^{n+1}$$ which already ""selects"" the zeros I am interested in. Observe that it still makes sense to consider $(\bar{x},\bar{y})\in \mathbb{R}^{m}\times\mathbb{R}^{n}$ such that $$F(\bar{x},\bar{y})=0$$ and, assuming there exists $f:U\ni \bar{x}\to V\ni \bar{y}$ such that $F(x,f(x))=0\quad\forall x\in U$ , to ask for implicit differentiation methods to compute the jacobian $D_xf$ . Of course, in this case, the IFT cannot be applied off-the-shelf to ensure $f$ exists, nor to compute such a Jacobian. Hence my question.","Consider a sufficiently regular ( ?) function with . And assume an implicit function is locally well defined by the condition I am interested in implicit differentiation techniques which apply in this contest, extending the implicit function theorem. EDIT My situation of interest is the following. There is a function where I think of as parameters and as variables. Of course, for each such that The IFT ensures there exists a local function such that Moreover, it tells that Now, I am interested in the value of at points which not only satisfy , but also an additional feasibility condition My doubt : Is it enough to consider at points where or does the additional constraint changes the shape of the implicit function , so that we need a different approach? To get this other with, I thought of considering the function which already ""selects"" the zeros I am interested in. Observe that it still makes sense to consider such that and, assuming there exists such that , to ask for implicit differentiation methods to compute the jacobian . Of course, in this case, the IFT cannot be applied off-the-shelf to ensure exists, nor to compute such a Jacobian. Hence my question.","C^1 F:\mathbb{R}^{m}\times\mathbb{R}^{n}\to\mathbb{R}^{n+k} k>0 y(x) F(x,y(x))=0 C^1 G:\mathbb{R}^{m}\times\mathbb{R}^{n}\to\mathbb{R}^{n} x\in\mathbb{R}^{m} y\in\mathbb{R}^{n} \bar{x},\bar{y} G(\bar{x},\bar{y})=0 f:U\ni \bar{x}\to V\ni \bar{y} G(x,f(x))=0\quad\forall x\in U D_x f(\bar{x})=-[D_y G(\bar{x},f(\bar{x}))]^{-1}[D_x G(\bar{x},f(\bar{x}))] D_x f(\bar{x}) G(\bar{x},\bar{y})=0 s(\bar{x},\bar{y})=0\quad\text{where}\quad s:\mathbb{R}^{m}\times\mathbb{R}^{n}\to\mathbb{R} D_x f(\bar{x})=-[D_y G(\bar{x},f(\bar{x}))]^{-1}[D_x G(\bar{x},f(\bar{x}))] s(\bar{x},\bar{y})=0 f F\equiv\binom{G}{s}:\mathbb{R}^{m}\times\mathbb{R}^{n}\to\mathbb{R}^{n+1} (\bar{x},\bar{y})\in \mathbb{R}^{m}\times\mathbb{R}^{n} F(\bar{x},\bar{y})=0 f:U\ni \bar{x}\to V\ni \bar{y} F(x,f(x))=0\quad\forall x\in U D_xf f","['real-analysis', 'multivariable-calculus', 'reference-request', 'implicit-function-theorem']"
48,Equivalent definition of Differentiability of function of two variable. [closed],Equivalent definition of Differentiability of function of two variable. [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I know the definition of FrÃ©chet derivative of function between two normed space.and one can think it is a generalization of our usual definition of derivative of real function.But in a book i faced (equivalent) definition for real valued function of two variable.so can i have hint how can i show this definition is equivalent to FrÃ©chet derivative : I dont even know how to start as in definition which i faced they define two error term what can i do from Frechet definition is only 1 error term.so how to get another?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I know the definition of FrÃ©chet derivative of function between two normed space.and one can think it is a generalization of our usual definition of derivative of real function.But in a book i faced (equivalent) definition for real valued function of two variable.so can i have hint how can i show this definition is equivalent to FrÃ©chet derivative : I dont even know how to start as in definition which i faced they define two error term what can i do from Frechet definition is only 1 error term.so how to get another?",,['multivariable-calculus']
49,"Stokes theorem and parameterization of an ""ellipse""","Stokes theorem and parameterization of an ""ellipse""",,"Calculate the work done by the field $F$ along the curve $L$ with anticlockwise orientation where $F(x,y,z)=(\sin (x^2)-y,2x+y+z,\arctan (z)-x)$ and $L=\{(x,y,z)\mid 2x+4y+3z=0,x^2+y^2+z^2=1\}$ . My thoughts: $$\nabla \times F=(-1,1,3),$$ which motivates the use of Stokes' Theorem, $$\int \limits _LF\cdot dr=\iint \limits _S(\nabla \times F)\cdot n\,dS,$$ where $S$ is a surface such that $\partial S=L$ . I decided to work with $$S=\{(x,y,z)\mid 2x+4y+3z=0,x^2+y^2+z^2\leq 1\},$$ with an upward facing normal. My question is: How can I parameterize $S$ ? My attempt: Taking $z=\frac{-2x-4y}{3}$ implies $x^2+y^2+\left (\frac{-2x-4y}{3}\right )^2\leq 1$ , which is equivalent to $13x^2+16xy+25y^2\leq 9$ . This implies that a possible parameterization is $$r(x,y)=\left (x,y,\frac{-2x-4y}{3} \right),\,\,13x^2+16xy+25y^2\leq 9.$$ My main issue is with the parameterization of $13x^2+16xy+25y^2\leq 9$ in the $xy$ plane. This is a rotated ellipse, and I am not sure what might be the best way to approach this. Thanks!","Calculate the work done by the field along the curve with anticlockwise orientation where and . My thoughts: which motivates the use of Stokes' Theorem, where is a surface such that . I decided to work with with an upward facing normal. My question is: How can I parameterize ? My attempt: Taking implies , which is equivalent to . This implies that a possible parameterization is My main issue is with the parameterization of in the plane. This is a rotated ellipse, and I am not sure what might be the best way to approach this. Thanks!","F L F(x,y,z)=(\sin (x^2)-y,2x+y+z,\arctan (z)-x) L=\{(x,y,z)\mid 2x+4y+3z=0,x^2+y^2+z^2=1\} \nabla \times F=(-1,1,3), \int \limits _LF\cdot dr=\iint \limits _S(\nabla \times F)\cdot n\,dS, S \partial S=L S=\{(x,y,z)\mid 2x+4y+3z=0,x^2+y^2+z^2\leq 1\}, S z=\frac{-2x-4y}{3} x^2+y^2+\left (\frac{-2x-4y}{3}\right )^2\leq 1 13x^2+16xy+25y^2\leq 9 r(x,y)=\left (x,y,\frac{-2x-4y}{3} \right),\,\,13x^2+16xy+25y^2\leq 9. 13x^2+16xy+25y^2\leq 9 xy","['multivariable-calculus', 'stokes-theorem']"
50,"If $g(y)=\max\limits_{x \in X} f(x,y)=f(h(y),y).$ Prove that $h(y)$ is continuous",If  Prove that  is continuous,"g(y)=\max\limits_{x \in X} f(x,y)=f(h(y),y). h(y)","Let $ X\subset\mathbb{R}^m$ and $Y\subset\mathbb{R}^n$ non empty, convex and compact sets, and let $f : X\times Y\rightarrow\mathbb{R}$ continuous funtion. If $f(\cdot,y):Y\rightarrow\mathbb{R}$ convex for every $y\in Y$ $f(x,\cdot):X\rightarrow\mathbb{R}$ concave for every $x\in X$ If $g(y)=\max\limits_{x \in X} f(x,y)=f(h(y),y).$ Prove that $\cdot \quad g(y)$ is continuous My attempt. Proof: Suppose g(y) is not continuous. So there exist $ \epsilon>0$ and  a sequence $\{y_i\}\subseteq Y$ : $ y_i \rightarrow y_0 \in Y$ ,such that $\Vert g(y_i)-g(y_0)\Vert>\varepsilon$ for every $i$ . and for the function $f(h(y).\cdot)$ : $|f(h(y_i),y_0)-f(h(y_0),y_0)|>\delta,\quad i=1,2,...$ we subtract and add $f(h(y_0),y_i)$ : $\delta < |f(h(y_i),y_0)-f(h(y_0),y_0)+f(h(y_0),y_i)-f(h(y_0),y_i)|$ $ = |f(h(y_i),y_0)-f(h(y_0),y_i)+f(h(y_0),y_i)-f(h(y_0),y_0)|$ $\leq |f(h(y_i),y_0)-f(h(y_i),y_0)|+ |f(h(y_0),y_i)-f(h(y_0),y_0)|$ I'm not sure if this the correct way of proving it. I suppose i could try applying that f is continuous on a compact set.","Let and non empty, convex and compact sets, and let continuous funtion. If convex for every concave for every If Prove that is continuous My attempt. Proof: Suppose g(y) is not continuous. So there exist and  a sequence : ,such that for every . and for the function : we subtract and add : I'm not sure if this the correct way of proving it. I suppose i could try applying that f is continuous on a compact set."," X\subset\mathbb{R}^m Y\subset\mathbb{R}^n f : X\times Y\rightarrow\mathbb{R} f(\cdot,y):Y\rightarrow\mathbb{R} y\in Y f(x,\cdot):X\rightarrow\mathbb{R} x\in X g(y)=\max\limits_{x \in X} f(x,y)=f(h(y),y). \cdot \quad g(y)  \epsilon>0 \{y_i\}\subseteq Y  y_i \rightarrow y_0 \in Y \Vert g(y_i)-g(y_0)\Vert>\varepsilon i f(h(y).\cdot) |f(h(y_i),y_0)-f(h(y_0),y_0)|>\delta,\quad i=1,2,... f(h(y_0),y_i) \delta < |f(h(y_i),y_0)-f(h(y_0),y_0)+f(h(y_0),y_i)-f(h(y_0),y_i)|  = |f(h(y_i),y_0)-f(h(y_0),y_i)+f(h(y_0),y_i)-f(h(y_0),y_0)| \leq |f(h(y_i),y_0)-f(h(y_i),y_0)|+ |f(h(y_0),y_i)-f(h(y_0),y_0)|","['real-analysis', 'calculus', 'multivariable-calculus']"
51,Differentiate under the integral sign,Differentiate under the integral sign,,"Let $\Sigma\subset \Bbb{R}^n$ be some compact hypersurface without boundary in $\Bbb{R}^n$ ,fixed $v = -H$ where $H$ is mean curvature vector, assume we have a family of hypersurface $$\Sigma_{s} = \{x+sv(x)\mid x\in \Sigma\}$$ Prove the differentiation under the integral sign formula $$\frac{d}{dt}\int_{\Sigma_t} u = \int_{\Sigma_t} u_t -\int _{\Sigma_t}u|H|^2$$ where $H$ is the mean curvature vector. My attempt using the differentiation under the integral sign formula in calculus we have $$\frac{d}{dt}\int_{\Sigma_t}u\ dV_g=  \int_{\Sigma_t}u_t  \ d V_g + \int_{\partial\Sigma_t} u \langle v,n\rangle\ dS$$ Correct where $v$ is the velocity direction of the change of $\Sigma _t$ and $n$ is the out normal direction of $\Sigma_s$ . However, as $\Sigma_t$ does not has boundary, the second term on the right hand side is zero?Sorry for the silly question, I can't see where goes wrong with my reasoning?","Let be some compact hypersurface without boundary in ,fixed where is mean curvature vector, assume we have a family of hypersurface Prove the differentiation under the integral sign formula where is the mean curvature vector. My attempt using the differentiation under the integral sign formula in calculus we have Correct where is the velocity direction of the change of and is the out normal direction of . However, as does not has boundary, the second term on the right hand side is zero?Sorry for the silly question, I can't see where goes wrong with my reasoning?","\Sigma\subset \Bbb{R}^n \Bbb{R}^n v = -H H \Sigma_{s} = \{x+sv(x)\mid x\in \Sigma\} \frac{d}{dt}\int_{\Sigma_t} u = \int_{\Sigma_t} u_t -\int _{\Sigma_t}u|H|^2 H \frac{d}{dt}\int_{\Sigma_t}u\ dV_g=  \int_{\Sigma_t}u_t  \ d V_g + \int_{\partial\Sigma_t} u \langle v,n\rangle\ dS v \Sigma _t n \Sigma_s \Sigma_t","['real-analysis', 'multivariable-calculus', 'differential-geometry', 'manifolds']"
52,"What is my error in this $\nabla_{\vec{v}} f(x,y,z)$ at $\vec{a} = (-1, -1, 4)$ and $\vec{v} = (\frac{\sqrt 2}{2}, \frac{1}{2}, \frac{1}{2})$ problem",What is my error in this  at  and  problem,"\nabla_{\vec{v}} f(x,y,z) \vec{a} = (-1, -1, 4) \vec{v} = (\frac{\sqrt 2}{2}, \frac{1}{2}, \frac{1}{2})","I want to find gradient of $f(x,y,z) = \sqrt{xyz}$ in the direction of $\vec{v}$ at a point $\vec{a}$ . That is, $\nabla_{\vec{v}} f(x,y,z)$ at $\vec{a} = (-1, -1, 4)$ and $\vec{v} = (\frac{\sqrt 2}{2}, \frac{1}{2}, \frac{1}{2})$ I computed gradient of $f(x,y,z)$ along $\vec{v}$ to be $\left(\begin{matrix} \sqrt{\frac{yz}{4x}} \\ \sqrt{\frac{xz}{4y}} \\ \sqrt{\frac{xy}{4z}} \end{matrix}\right)$ . So my answer for the value of gradient at $\vec{a}$ is $\left(\begin{matrix} 1 \\ 1 \\ \frac{1}{4}\end{matrix}\right)$ . But the answer given is $\left(\begin{matrix} \frac{yz}{\sqrt{4xyz}} \\ \frac{xz}{\sqrt{4xyz}} \\ \frac{xy}{\sqrt{4xyz}} \end{matrix}\right)$ . So the accepted answer for value of gradient at $\vec{a}$ is $\left(\begin{matrix} -1 \\ -1 \\ \frac{1}{4}\end{matrix}\right)$ . Why is my answer wrong?","I want to find gradient of in the direction of at a point . That is, at and I computed gradient of along to be . So my answer for the value of gradient at is . But the answer given is . So the accepted answer for value of gradient at is . Why is my answer wrong?","f(x,y,z) = \sqrt{xyz} \vec{v} \vec{a} \nabla_{\vec{v}} f(x,y,z) \vec{a} = (-1, -1, 4) \vec{v} = (\frac{\sqrt 2}{2}, \frac{1}{2}, \frac{1}{2}) f(x,y,z) \vec{v} \left(\begin{matrix} \sqrt{\frac{yz}{4x}} \\ \sqrt{\frac{xz}{4y}} \\ \sqrt{\frac{xy}{4z}} \end{matrix}\right) \vec{a} \left(\begin{matrix} 1 \\ 1 \\ \frac{1}{4}\end{matrix}\right) \left(\begin{matrix} \frac{yz}{\sqrt{4xyz}} \\ \frac{xz}{\sqrt{4xyz}} \\ \frac{xy}{\sqrt{4xyz}} \end{matrix}\right) \vec{a} \left(\begin{matrix} -1 \\ -1 \\ \frac{1}{4}\end{matrix}\right)","['multivariable-calculus', 'partial-derivative', 'vector-analysis']"
53,Integrating a scalar with respect to a vector with vector limits?,Integrating a scalar with respect to a vector with vector limits?,,"The following is a line of reasoning you'll often see in physics textbooks. Newton's second law can be formulated in terms of momentum, which yields the following fundamental statement: $$\frac{d \vec{p}}{dt} = \vec{F},$$ which in simple terms states that the the time rate of change of the linear momentum of a particle is equal to the net force acting on the particle. This statement is often rewritten as $$d\vec{p}=\vec{F}dt$$ and integrated to yield a new quantity, referred to as impulse. Now, here's where things start getting confusing. Many authors will integrate with respect to $d\vec{p}$ using vector bounds : $$\int_{\vec{p_1}}^{\vec{p_2}}  d\vec{p}=\int_{t_1}^{t_2} \vec{F} dt $$ which yields $$\vec{p_2}-\vec{p_1} = \int_{t_1}^{t_2} \vec{F} dt$$ or, abbreviated to: $$\vec{p_2}-\vec{p_1} = J$$ where $J$ is then defined to be the impulse . The right-hand side is not problematic here, it's the left-hand side that's bugging me. That brings me to my questions: What does it mean to integrate with respect to a vector (though, not in the usual line integral sense where there's a dot product that can be resolved into a scalar quantity)? Is there a way to visualize what's going on here? What does it mean to have vectors as the limits of integration? Does all of this imply that you could have functions of vectors as the integrand? I.e., with regular integration, you integrate with respect to $x$ , and your integrand is some function of $x$ (say, $\frac{2x}{x^3 +3x + 2}$ ). Do all the regular rules of calculus apply to vectors - in this particular sense? Now, mind you, I'm aware that you could express the integral in terms of a variable transformation as $\int_{t_1}^{t_2} \frac{d\vec{p}}{dt} dt$ using the vector differential definition ( $d\vec{p} = \frac{d\vec{p}}{dt} dt$ ), but I'd like to emphasize that that's not my question as I've seen many authors do this explicitly without the use of a variable transformation.","The following is a line of reasoning you'll often see in physics textbooks. Newton's second law can be formulated in terms of momentum, which yields the following fundamental statement: which in simple terms states that the the time rate of change of the linear momentum of a particle is equal to the net force acting on the particle. This statement is often rewritten as and integrated to yield a new quantity, referred to as impulse. Now, here's where things start getting confusing. Many authors will integrate with respect to using vector bounds : which yields or, abbreviated to: where is then defined to be the impulse . The right-hand side is not problematic here, it's the left-hand side that's bugging me. That brings me to my questions: What does it mean to integrate with respect to a vector (though, not in the usual line integral sense where there's a dot product that can be resolved into a scalar quantity)? Is there a way to visualize what's going on here? What does it mean to have vectors as the limits of integration? Does all of this imply that you could have functions of vectors as the integrand? I.e., with regular integration, you integrate with respect to , and your integrand is some function of (say, ). Do all the regular rules of calculus apply to vectors - in this particular sense? Now, mind you, I'm aware that you could express the integral in terms of a variable transformation as using the vector differential definition ( ), but I'd like to emphasize that that's not my question as I've seen many authors do this explicitly without the use of a variable transformation.","\frac{d \vec{p}}{dt} = \vec{F}, d\vec{p}=\vec{F}dt d\vec{p} \int_{\vec{p_1}}^{\vec{p_2}}  d\vec{p}=\int_{t_1}^{t_2} \vec{F} dt  \vec{p_2}-\vec{p_1} = \int_{t_1}^{t_2} \vec{F} dt \vec{p_2}-\vec{p_1} = J J x x \frac{2x}{x^3 +3x + 2} \int_{t_1}^{t_2} \frac{d\vec{p}}{dt} dt d\vec{p} = \frac{d\vec{p}}{dt} dt","['calculus', 'integration', 'multivariable-calculus', 'vector-analysis', 'physics']"
54,Line Integral. Vector field. Parametrization,Line Integral. Vector field. Parametrization,,"LetÂ´s say we want to find the circumferences of the plane $C$ that make the line integral $\int_C y^2dx + x^2dy$ worth zero. My attempt : The vector field given is: $\mathbf{F}(x,y)=(y^2,x^2)$ . The first thing I try to do is find a parametrization for the path, but I am not sure whether I am looking for the parametrization of a general circumference centered at $(a,b)$ with radius $R$ : $\mathbf{r}(t)=(a+R \cdot \cos(t),b+R \cdot\sin(t))$ where $0 \leq t \le 2\pi$ $\mathbf{r'}(t)=(-R\sin(t),R\cos(t))$ Being that the case: $\mathbf{F}(\mathbf{r}(t))=\mathbf{F}(x(t),y(t))=((b+R \sin(t))^2,(a+R\cos(t))^2$ ) obtaining: $\mathbf{F}(\mathbf{r}(t))\cdot\mathbf{r'}(t)= ((b+R \cdot \sin(t))^2,(a+R \cdot\cos(t))^2) \cdot (-R\sin(t),R\cos(t)) $ Finally, we must impose: $\int_{0}^{2\pi}\mathbf{F}(\mathbf{r}(t))\cdot\mathbf{r'}(t) dt =0.$ However, we aim to find three parameters using only one condition. Perhaps, while calculating the integral we are left with fewer unknowns, but, despite that, the calculation part is still hard. Any suggestions are welcome.","LetÂ´s say we want to find the circumferences of the plane that make the line integral worth zero. My attempt : The vector field given is: . The first thing I try to do is find a parametrization for the path, but I am not sure whether I am looking for the parametrization of a general circumference centered at with radius : where Being that the case: ) obtaining: Finally, we must impose: However, we aim to find three parameters using only one condition. Perhaps, while calculating the integral we are left with fewer unknowns, but, despite that, the calculation part is still hard. Any suggestions are welcome.","C \int_C y^2dx + x^2dy \mathbf{F}(x,y)=(y^2,x^2) (a,b) R \mathbf{r}(t)=(a+R \cdot \cos(t),b+R \cdot\sin(t)) 0 \leq t \le 2\pi \mathbf{r'}(t)=(-R\sin(t),R\cos(t)) \mathbf{F}(\mathbf{r}(t))=\mathbf{F}(x(t),y(t))=((b+R \sin(t))^2,(a+R\cos(t))^2 \mathbf{F}(\mathbf{r}(t))\cdot\mathbf{r'}(t)= ((b+R \cdot \sin(t))^2,(a+R \cdot\cos(t))^2) \cdot (-R\sin(t),R\cos(t))  \int_{0}^{2\pi}\mathbf{F}(\mathbf{r}(t))\cdot\mathbf{r'}(t) dt =0.","['integration', 'multivariable-calculus']"
55,Differentiable but not continuously-differentiable function: not the usual one,Differentiable but not continuously-differentiable function: not the usual one,,"It is well-known that the function $f:\mathbb R^2\to \mathbb R $ defined by $$f(x,y)=\begin{cases}(x^2+y^2)\sin\left(\frac1 {\sqrt{x^2+y^2}}\right),&(x,y)\neq 0\\0,&(x,y)=0\end{cases}$$ is differentiable everywhere but $\dfrac{\partial f}{\partial x}(x,y)$ and $\dfrac{\partial f}{\partial y}(x,y)$ are not continuous at $(0,0)$ , this is the standard example to prove that there exist differentiable but not continuously-differentiable functions (e.g., see https://math.stackexchange.com/q/3338764 ). My question: is there any other (reasonable) example (from $\mathbb R^2$ to $\mathbb R$ ) that differs significantly from this one? I mean: no radial simmetry and not obtained by continuous transformation from the above (and possibly avoiding the $\sin$ function) and such that the calculation can be performed by undergraduate students.","It is well-known that the function defined by is differentiable everywhere but and are not continuous at , this is the standard example to prove that there exist differentiable but not continuously-differentiable functions (e.g., see https://math.stackexchange.com/q/3338764 ). My question: is there any other (reasonable) example (from to ) that differs significantly from this one? I mean: no radial simmetry and not obtained by continuous transformation from the above (and possibly avoiding the function) and such that the calculation can be performed by undergraduate students.","f:\mathbb R^2\to \mathbb R  f(x,y)=\begin{cases}(x^2+y^2)\sin\left(\frac1 {\sqrt{x^2+y^2}}\right),&(x,y)\neq 0\\0,&(x,y)=0\end{cases} \dfrac{\partial f}{\partial x}(x,y) \dfrac{\partial f}{\partial y}(x,y) (0,0) \mathbb R^2 \mathbb R \sin","['real-analysis', 'multivariable-calculus', 'derivatives']"
56,Does the electrostatic potential have a local maximum on the sphere?,Does the electrostatic potential have a local maximum on the sphere?,,"Let $$M=\{(x_1,x_2,x_3,x_4) \in  \mathbb{S}^2 \times \mathbb{S}^2 \times \mathbb{S}^2 \times \mathbb{S}^2 \, |\,\, \text{ all the } x_i \, \text{ are distinct}\} $$ $M$ is an open subset of $( \mathbb{S}^2)^4$ . Let $E:M \to \mathbb{R}$ be defined by $$E(x_1,x_2,x_3,x_4)=\sum_{i < j}\frac{1}{\| x_i - x_j \|},$$ where $\| x_i - x_j \|$ denotes the Euclidean distance in $\mathbb{R}^3$ . Question: Does $E$ have a point of local maximum? Clearly $E$ doesn't have a global maximum, since it's unbounded. Intuitively, given any point $p \in M$ , we can move some of the $x_i$ closer to each other, thus increasing the energy. However, when we move say $x_2$ closer to $x_1$ , we might be pushing it further away from $x_3$ or $x_4$ , so there are ""competing"" changes in the contribution of each $\| x_i - x_j \|^{-1}$ term. Is there an easy way to see that one can always choose a direction where $E$ strictly increases?","Let is an open subset of . Let be defined by where denotes the Euclidean distance in . Question: Does have a point of local maximum? Clearly doesn't have a global maximum, since it's unbounded. Intuitively, given any point , we can move some of the closer to each other, thus increasing the energy. However, when we move say closer to , we might be pushing it further away from or , so there are ""competing"" changes in the contribution of each term. Is there an easy way to see that one can always choose a direction where strictly increases?","M=\{(x_1,x_2,x_3,x_4) \in  \mathbb{S}^2 \times \mathbb{S}^2 \times \mathbb{S}^2 \times \mathbb{S}^2 \, |\,\, \text{ all the } x_i \, \text{ are distinct}\}  M ( \mathbb{S}^2)^4 E:M \to \mathbb{R} E(x_1,x_2,x_3,x_4)=\sum_{i < j}\frac{1}{\| x_i - x_j \|}, \| x_i - x_j \| \mathbb{R}^3 E E p \in M x_i x_2 x_1 x_3 x_4 \| x_i - x_j \|^{-1} E","['multivariable-calculus', 'differential-geometry', 'optimization', 'physics', 'maxima-minima']"
57,Prove that the directional derivative of a composition exists.,Prove that the directional derivative of a composition exists.,,"Let $E \subseteq \mathbb{R}^{n}$ and $D \subseteq \mathbb{R}^{m}$ be two open sets, and let $f : E \rightarrow \mathbb{R}^{m}$ and $g : D \rightarrow \mathbb{R}^{k}$ be functions such that $f(E) \subseteq D$ . Suppose that $g$ is differentiable at $f(x)$ for some $x \in E$ , and that for some $v \in \mathbb{R}^{n}$ with $\vert \vert v \vert\vert= 1$ the directional derivative $D_{v}f(x)$ exists. Show that the directional derivative $D_{v}(g \circ f)(x)$ exists and give a formula to compute it. I tried to begin by applying the definition of differentiability for $g$ , but I'm not sure how to do that with $f(x)$ . I am also not sure what exactly I am supposed to prove, i.e, what does ""show that the directional derivative $D_{v}(g \circ f)(x)$ exists"" require? Would greatly appreciate any help!","Let and be two open sets, and let and be functions such that . Suppose that is differentiable at for some , and that for some with the directional derivative exists. Show that the directional derivative exists and give a formula to compute it. I tried to begin by applying the definition of differentiability for , but I'm not sure how to do that with . I am also not sure what exactly I am supposed to prove, i.e, what does ""show that the directional derivative exists"" require? Would greatly appreciate any help!","E \subseteq \mathbb{R}^{n} D \subseteq \mathbb{R}^{m} f : E \rightarrow \mathbb{R}^{m} g : D \rightarrow \mathbb{R}^{k} f(E) \subseteq D g f(x) x \in E v \in \mathbb{R}^{n} \vert \vert v
\vert\vert= 1 D_{v}f(x) D_{v}(g \circ f)(x) g f(x) D_{v}(g \circ f)(x)","['real-analysis', 'multivariable-calculus', 'chain-rule']"
58,Discontinuous vector field with curl 0,Discontinuous vector field with curl 0,,"Let $S$ be a part of the paraboloid $z=1-x^2-y^2$ such that $z\geq 2|y|$ . They ask to calculate $$ \int_C\frac{-y}{x^2+y^2}dx+\frac{x}{x^2+y^2}dy+\frac{1+e^z}{1+z^2}dz $$ where the curve $C$ is traversed once in an anticlockwise direction if it is observed from the point $(0,0,1)$ . It is easy to see that the curl of $F$ is $(0,0,0)$ . So my initial idea was to use Stokes' theorem with which the answer would be zero. But realizing that $F$ is not a continuous field this is not possible, now in reality it would be necessary to look for a surface that has two borders: one of them $C$ and another $C_0$ (which would be easier to calculate). The following occurs to me, taking the same surface but bounded above with $z=15/16$ . That would make the new surface no longer go through the z axis. My attempt Let $\lambda(t)=(\frac{1}{4}\cos t,\frac{1}{4}\sin t,\frac{15}{16})$ then $\lambda'(t)=(-\frac{1}{4}\sin t,\frac{1}{4}\cos t,0)$ . And we have $F(\lambda(t))=(-4\sin t,4\cos t,\frac{1+e^{15/16}}{1+(15/16)^2})$ . \begin{align} \int_{C_0} F\cdot dr&=\int_{C} F\cdot dr+\iint_S (\nabla\times F)\cdot dS=\int_C F\cdot dr\\ \int_{C_0} F\cdot dr&=\int_0^{2\pi}\cos^2 t+\sin^2 tdt=2\pi \end{align}","Let be a part of the paraboloid such that . They ask to calculate where the curve is traversed once in an anticlockwise direction if it is observed from the point . It is easy to see that the curl of is . So my initial idea was to use Stokes' theorem with which the answer would be zero. But realizing that is not a continuous field this is not possible, now in reality it would be necessary to look for a surface that has two borders: one of them and another (which would be easier to calculate). The following occurs to me, taking the same surface but bounded above with . That would make the new surface no longer go through the z axis. My attempt Let then . And we have .","S z=1-x^2-y^2 z\geq 2|y| 
\int_C\frac{-y}{x^2+y^2}dx+\frac{x}{x^2+y^2}dy+\frac{1+e^z}{1+z^2}dz
 C (0,0,1) F (0,0,0) F C C_0 z=15/16 \lambda(t)=(\frac{1}{4}\cos t,\frac{1}{4}\sin t,\frac{15}{16}) \lambda'(t)=(-\frac{1}{4}\sin t,\frac{1}{4}\cos t,0) F(\lambda(t))=(-4\sin t,4\cos t,\frac{1+e^{15/16}}{1+(15/16)^2}) \begin{align}
\int_{C_0} F\cdot dr&=\int_{C} F\cdot dr+\iint_S (\nabla\times F)\cdot dS=\int_C F\cdot dr\\
\int_{C_0} F\cdot dr&=\int_0^{2\pi}\cos^2 t+\sin^2 tdt=2\pi
\end{align}","['multivariable-calculus', 'vector-analysis', 'vector-fields', 'stokes-theorem']"
59,"Proving $\int_0^1\int_0^1\chi_Sdxdy=0,$ while $\int_S\chi_S$ doesn't exist.",Proving  while  doesn't exist.,"\int_0^1\int_0^1\chi_Sdxdy=0, \int_S\chi_S","Let $S=\{(x,y)\in\Bbb R^2\mid x\in\Bbb Q,0<x<1,\text{ and if } x=\frac{p}m,\gcd(p,m)=1,y=\frac{k}m,k=1,\ldots,m-1\}.\tag 1$ Prove that $\displaystyle \int_0^1\int_0^1\chi_S dxdy=0$ while $\displaystyle \int_S\chi_S$ doesn't exist. My thoughts: I tried to write down $(1)$ differently, so I fixed some $y\in\Bbb Q.$ Then $y_0=\frac{p_0}{q_0},\gcd(p_0,q_0)=1.$ Then, for every $n\in\Bbb N$ all the points of the form $$\left(y_0,\frac{p}{q_0n}\right),\gcd(p,q_0n)=1,p<q_0n\tag 2$$ are in $S.$ I tried to show that the set of discontinuities of the function $$x\mapsto\begin{cases}1, &(x,y_0)\in S\\0,&(x,y_0)\notin S\end{cases}$$ is of measure zero and exactly the set $S$ and that the function $$y\mapsto\int_0^1\chi_Sdx$$ is non-zero on the set of the Jordan measure $0$ . I thought I could prove that, for a given $y_0,$ the points in $(2)$ might form a discrete set,which would have a boundary of the Jordan measure zero, but I failed. For the second integral, I thought one might argue that, even though, for some $x_0\in\Bbb Q\cap(0,1)$ there are only finitely many points $(x_0,y)\in S,$ since $\Bbb Q\cap(0,1)$ is dense in $[0,1],$ if we take a good enough subdivision of $[0,1]\times[0,1],$ letting $m\to\infty,$ there are infinitely many points from $S$ in a rectangle of the subdivision so the lower Darboux sum would equal $0,$ while the upper would be $1$ and therefore, $\displaystyle\int_{[0,1]^2}\chi_S$ wouldn't exist, and neither would the given integral. I'm not sure if I'm on the right track at all and what I've written seems messy even to me. Does anybody have any advice/hint on what to do?","Let Prove that while doesn't exist. My thoughts: I tried to write down differently, so I fixed some Then Then, for every all the points of the form are in I tried to show that the set of discontinuities of the function is of measure zero and exactly the set and that the function is non-zero on the set of the Jordan measure . I thought I could prove that, for a given the points in might form a discrete set,which would have a boundary of the Jordan measure zero, but I failed. For the second integral, I thought one might argue that, even though, for some there are only finitely many points since is dense in if we take a good enough subdivision of letting there are infinitely many points from in a rectangle of the subdivision so the lower Darboux sum would equal while the upper would be and therefore, wouldn't exist, and neither would the given integral. I'm not sure if I'm on the right track at all and what I've written seems messy even to me. Does anybody have any advice/hint on what to do?","S=\{(x,y)\in\Bbb R^2\mid x\in\Bbb Q,0<x<1,\text{ and if } x=\frac{p}m,\gcd(p,m)=1,y=\frac{k}m,k=1,\ldots,m-1\}.\tag 1 \displaystyle \int_0^1\int_0^1\chi_S dxdy=0 \displaystyle \int_S\chi_S (1) y\in\Bbb Q. y_0=\frac{p_0}{q_0},\gcd(p_0,q_0)=1. n\in\Bbb N \left(y_0,\frac{p}{q_0n}\right),\gcd(p,q_0n)=1,p<q_0n\tag 2 S. x\mapsto\begin{cases}1, &(x,y_0)\in S\\0,&(x,y_0)\notin S\end{cases} S y\mapsto\int_0^1\chi_Sdx 0 y_0, (2) x_0\in\Bbb Q\cap(0,1) (x_0,y)\in S, \Bbb Q\cap(0,1) [0,1], [0,1]\times[0,1], m\to\infty, S 0, 1 \displaystyle\int_{[0,1]^2}\chi_S","['integration', 'multivariable-calculus', 'proof-writing']"
60,How to calculate $\nabla_{\mathbf{x}}(\mathbf{c}\mathbf{x}-A)\mathbf{x}^t$?,How to calculate ?,\nabla_{\mathbf{x}}(\mathbf{c}\mathbf{x}-A)\mathbf{x}^t,"How to calculate $\nabla_{\mathbf{x}}(\mathbf{c}\mathbf{x}-A)\mathbf{x}^t$ directly? $\mathbf{x}\in\mathbb{R}^{1\times n}$ , $\mathbf{c}\in\mathbb{R}^{m\times 1}$ , $A\in\mathbb{R}^{m\times n}$ . My attempt: Denote the expression $(\mathbb{c}\mathbf{x}-A)\mathbf{x}^t$ by $f$ . Then, find $\frac{\partial f_i}{\partial x_j}$ $f_i = ((\mathbf{c}\mathbf{x}-A)\mathbf{x}^t)_i = \sum_j c_ix_j^2-\sum_jA_{ij}x_j$ $\frac{\partial f_i}{\partial x_j} = 2c_ix_j - A_{ij}$ So, $\nabla_\mathbf{x}(\mathbb{c}\mathbf{x}-A)\mathbf{x}^t = 2\mathbf{c}\mathbf{x}-A.$ Instead of calculating the $ij$ -th component, how to compute the gradient directly?","How to calculate directly? , , . My attempt: Denote the expression by . Then, find So, Instead of calculating the -th component, how to compute the gradient directly?",\nabla_{\mathbf{x}}(\mathbf{c}\mathbf{x}-A)\mathbf{x}^t \mathbf{x}\in\mathbb{R}^{1\times n} \mathbf{c}\in\mathbb{R}^{m\times 1} A\in\mathbb{R}^{m\times n} (\mathbb{c}\mathbf{x}-A)\mathbf{x}^t f \frac{\partial f_i}{\partial x_j} f_i = ((\mathbf{c}\mathbf{x}-A)\mathbf{x}^t)_i = \sum_j c_ix_j^2-\sum_jA_{ij}x_j \frac{\partial f_i}{\partial x_j} = 2c_ix_j - A_{ij} \nabla_\mathbf{x}(\mathbb{c}\mathbf{x}-A)\mathbf{x}^t = 2\mathbf{c}\mathbf{x}-A. ij,"['multivariable-calculus', 'derivatives', 'vector-analysis', 'matrix-calculus']"
61,Laplacian and inequality of a function,Laplacian and inequality of a function,,"Let $D=\{(x,y) | x^2+y^2 < 1 \}$ . Let $f$ and $g$ be a $C^2(D)$ be such that g is bounded and $f$ approaches infinity as $x^2+y^2$ approaches $1$ and moreover $\Delta f=e^f$ and $\Delta g \geq e^g$ at all points of $ D$ . Here $\Delta$ is the Laplacian. Prove $f \geq g$ on D. My strategy is to prove by contradiction. Suppose at point say $(x_0,y_0)$ , $g>f$ . Now we define a new function h=g-f.At this point $\Delta h >0$ . Now I want to conclude that h is less than average of of all the values and arrive at the contradiction. How to proceed after this? This class just the knowledge of real analysis. So I cannot use tools from PDE.","Let . Let and be a be such that g is bounded and approaches infinity as approaches and moreover and at all points of . Here is the Laplacian. Prove on D. My strategy is to prove by contradiction. Suppose at point say , . Now we define a new function h=g-f.At this point . Now I want to conclude that h is less than average of of all the values and arrive at the contradiction. How to proceed after this? This class just the knowledge of real analysis. So I cannot use tools from PDE.","D=\{(x,y) | x^2+y^2 < 1 \} f g C^2(D) f x^2+y^2 1 \Delta f=e^f \Delta g \geq e^g  D \Delta f \geq g (x_0,y_0) g>f \Delta h >0","['real-analysis', 'calculus', 'multivariable-calculus']"
62,"Continuity of this function as $(x,y)$ tends to $(0,0)$",Continuity of this function as  tends to,"(x,y) (0,0)","Here's a function in $x$ and $y$ defined piecewise as $$f(x,y)= \left\{\begin{array}{ll}       0 & (x,y)=(0,0)\\  \frac{x^2y}{x^4+y^2} & (x,y) \neq (0,0) \\ \end{array}\right.$$ Examine its continuity as the ordered pair tends to $(0,0)$ . Okay, so I first tried this by assuming $x=\frac{1}{n}$ and $y=\frac{1}{n^2}$ , where $n\rightarrow \infty$ . The limit of the function as $(x,y) \rightarrow (0,0)$ came out to be $\frac{1}{2}$ , and since this is not equal to the value of the function at the said point, its discontinuous at the origin. But when I assumed $x=\frac{1}{n}$ and $y=\frac{1}{n}$ , I got the limit zero: $$f(\frac{1}{n},\frac{1}{n})= \frac{\frac{1}{n^3}}{\frac{1}{n^4}+ \frac{1}{n^2}} = \frac{\frac{1}{n}}{\frac{1}{n^2}+ 1} $$ Since $x,y \rightarrow 0$ , $f(\frac{1}{n},\frac{1}{n}) \rightarrow \frac{0}{0+1}=0$ Where am I going wrong?","Here's a function in and defined piecewise as Examine its continuity as the ordered pair tends to . Okay, so I first tried this by assuming and , where . The limit of the function as came out to be , and since this is not equal to the value of the function at the said point, its discontinuous at the origin. But when I assumed and , I got the limit zero: Since , Where am I going wrong?","x y f(x,y)=
\left\{\begin{array}{ll}
      0 & (x,y)=(0,0)\\
 \frac{x^2y}{x^4+y^2} & (x,y) \neq (0,0) \\
\end{array}\right. (0,0) x=\frac{1}{n} y=\frac{1}{n^2} n\rightarrow \infty (x,y) \rightarrow (0,0) \frac{1}{2} x=\frac{1}{n} y=\frac{1}{n} f(\frac{1}{n},\frac{1}{n})= \frac{\frac{1}{n^3}}{\frac{1}{n^4}+ \frac{1}{n^2}} = \frac{\frac{1}{n}}{\frac{1}{n^2}+ 1}  x,y \rightarrow 0 f(\frac{1}{n},\frac{1}{n}) \rightarrow \frac{0}{0+1}=0","['multivariable-calculus', 'continuity']"
63,finding derivatives of variables in multivariable taylor polynomial,finding derivatives of variables in multivariable taylor polynomial,,"Given: $$F(x,y) = -6 -4(x-4) +6(y-6) +8(x-4)^2 +9(x-4)(y-6) -4(y-6)^2 + R_2$$ and that $F(4,6)=-6$ find y'(4), y''(4), make sure that the function is applicable for the implicit function theorem. my attempt: so I've basically made sure that the conditions applied, implicitly derived F and found out y'. It was a lot of hard work and while I got a result my method didn't at all utilize the fact it's taylor -I think there must be an easier way but I just can't put my finger on it. I think it'll utilize the fact that $-4 = F_x(x,y)$ and that $6 = F_y(x,y)$ - and maybe the chain rule? since $$-4 = \frac{\partial F(4,6)}{\partial{x}}=\frac{\partial{y}}{\partial{x}}* \frac{\partial F(4,6)}{\partial{y}} = 6*y'(x) \Rightarrow y'(x) = \frac{-2}3$$ but both my calculation and this calculator https://www.emathhelp.net/en/calculators/calculus-1/implicit-differentiation-calculator/?f=-4%28x-4%29%2B6%28y-6%29%2B8%28x-4%29%5E2+%2B9%28x-4%29%28y-6%29+-4%28y-6%29%5E2%3D0&type=x&px=4&py=6 output $2/3$ To sum up - where is my logic faulty? is there a similar easier way? how do I extend this to y''(4)?","Given: and that find y'(4), y''(4), make sure that the function is applicable for the implicit function theorem. my attempt: so I've basically made sure that the conditions applied, implicitly derived F and found out y'. It was a lot of hard work and while I got a result my method didn't at all utilize the fact it's taylor -I think there must be an easier way but I just can't put my finger on it. I think it'll utilize the fact that and that - and maybe the chain rule? since but both my calculation and this calculator https://www.emathhelp.net/en/calculators/calculus-1/implicit-differentiation-calculator/?f=-4%28x-4%29%2B6%28y-6%29%2B8%28x-4%29%5E2+%2B9%28x-4%29%28y-6%29+-4%28y-6%29%5E2%3D0&type=x&px=4&py=6 output To sum up - where is my logic faulty? is there a similar easier way? how do I extend this to y''(4)?","F(x,y) = -6 -4(x-4) +6(y-6) +8(x-4)^2 +9(x-4)(y-6) -4(y-6)^2 + R_2 F(4,6)=-6 -4 = F_x(x,y) 6 = F_y(x,y) -4 = \frac{\partial F(4,6)}{\partial{x}}=\frac{\partial{y}}{\partial{x}}* \frac{\partial F(4,6)}{\partial{y}} = 6*y'(x) \Rightarrow y'(x) = \frac{-2}3 2/3","['multivariable-calculus', 'taylor-expansion', 'implicit-differentiation', 'implicit-function-theorem']"
64,What is the direction of vector $\left[\begin{smallmatrix}d x \\ d y\end{smallmatrix}\right]$,What is the direction of vector,\left[\begin{smallmatrix}d x \\ d y\end{smallmatrix}\right],"Recently I came across a topic "" total differential "" which comes with a result $d f=\frac{\partial f}{\partial x} d x+\frac{\partial f}{\partial y} d y$ As much I learned in multivariable calculus this can be simplified as $\nabla f \cdot\left[\begin{array}{l}d x \\ d y\end{array}\right]$ which graphically means taking directional Derivative in diraction of $\left[\begin{array}{l}d x \\ d y\end{array}\right]$ But is it  Makes any sense ? someone plese explain","Recently I came across a topic "" total differential "" which comes with a result As much I learned in multivariable calculus this can be simplified as which graphically means taking directional Derivative in diraction of But is it  Makes any sense ? someone plese explain",d f=\frac{\partial f}{\partial x} d x+\frac{\partial f}{\partial y} d y \nabla f \cdot\left[\begin{array}{l}d x \\ d y\end{array}\right] \left[\begin{array}{l}d x \\ d y\end{array}\right],"['calculus', 'multivariable-calculus', 'derivatives', 'vectors']"
65,"Use the Lagrange multiplier method to find the shortest distance from $(1, 1, 1)$ to $2x + y - z = 5$",Use the Lagrange multiplier method to find the shortest distance from  to,"(1, 1, 1) 2x + y - z = 5","This is my working so far: let $$f=d^2=(x-1)^2+(y-1)^2+(z-1)^2$$ $$g=2x+y-z-5$$ $$\psi=f+\lambda g$$ from this we can make simultaneous equations: $$\psi_x=2(x-1)+2\lambda =0$$ $$\psi_y=2(y-1)+\lambda =0$$ $$\psi_z=2(z-1)-\lambda =0$$ $$2x+y-z=5$$ solving these gives: $$x=\frac 65,\,y=\frac {11}{10},\,z=\frac 9{10}$$ so then to calculate the shortest distance we would do the following: $$d=\sqrt{\left(x-\frac6{5}\right)^2+\left(y-\frac{11}{10}\right)^2+\left(z-\frac{9}{10}^2\right)}=\frac{\sqrt6}{10}$$ but the answer given is $$\frac{\sqrt6}{2}$$",This is my working so far: let from this we can make simultaneous equations: solving these gives: so then to calculate the shortest distance we would do the following: but the answer given is,"f=d^2=(x-1)^2+(y-1)^2+(z-1)^2 g=2x+y-z-5 \psi=f+\lambda g \psi_x=2(x-1)+2\lambda =0 \psi_y=2(y-1)+\lambda =0 \psi_z=2(z-1)-\lambda =0 2x+y-z=5 x=\frac 65,\,y=\frac {11}{10},\,z=\frac 9{10} d=\sqrt{\left(x-\frac6{5}\right)^2+\left(y-\frac{11}{10}\right)^2+\left(z-\frac{9}{10}^2\right)}=\frac{\sqrt6}{10} \frac{\sqrt6}{2}","['multivariable-calculus', 'lagrange-multiplier']"
66,Mean Value Theorem on a Closed Set,Mean Value Theorem on a Closed Set,,"For mean value theorem on the real line, we consider a closed interval $[a,b]$ : Let $f:[a,b] \to \mathbb{R}$ be a continuous function on the closed interval $[a,b]$ , and differentiable on the open interval $(a,b)$ . Then there exists some $c$ in $(a,b)$ such that $$f'(c)=\frac {f(b)-f(a)}{b-a}.$$ However, for the generalization to higher dimensional case (Mean value theorem in several variables) there are multiple sources that define $f$ on an open set. For example, this is adopted from Wikipedia: Let $G$ be an open convex subset of $\mathbb {R}^{n}$ , and let $f:G\to \mathbb {R}$ be a differentiable function. Fix points $x,y\in G$ , and define $g(t)=f((1-t)x+ty)$ . Since $g$ is a differentiable function in one variable, then we can apply mean value theorem. 1- Why is $G$ taken to be an open set? 2- And what would be difference if we change it to any arbitrary convex set? (It maight have boundaries but we can define continuity and differentiability at the boundaries).","For mean value theorem on the real line, we consider a closed interval : Let be a continuous function on the closed interval , and differentiable on the open interval . Then there exists some in such that However, for the generalization to higher dimensional case (Mean value theorem in several variables) there are multiple sources that define on an open set. For example, this is adopted from Wikipedia: Let be an open convex subset of , and let be a differentiable function. Fix points , and define . Since is a differentiable function in one variable, then we can apply mean value theorem. 1- Why is taken to be an open set? 2- And what would be difference if we change it to any arbitrary convex set? (It maight have boundaries but we can define continuity and differentiability at the boundaries).","[a,b] f:[a,b] \to \mathbb{R} [a,b] (a,b) c (a,b) f'(c)=\frac {f(b)-f(a)}{b-a}. f G \mathbb {R}^{n} f:G\to \mathbb {R} x,y\in G g(t)=f((1-t)x+ty) g G","['real-analysis', 'calculus', 'multivariable-calculus']"
67,Sard's theorem proof - Using Implicit Function Theorem to construct a new coordinate representation,Sard's theorem proof - Using Implicit Function Theorem to construct a new coordinate representation,,"The following is part of the proof of the Sard's theorem from John Lee's Introduction to Smooth Manifolds. I am struggling to understand the second paragraph from Step 1. So assuming that $\partial F^1 / \partial x^1(a)\neq 0 $ , how do we define new smooth coordinates $(u,v)=(u,v^2,\dots , v^m)$ in some neighborhood $V_a$ of $a$ in $U$ by $u=F^1, v^2 = x^2 ,\dots ,v^m=x^m?$ I think this is some form of the implicit function theorem, but from the theorem stated in the text, I cannot figure out how we can construct such a coordinate chart. Also, in terms of these coordinates how does F have the coordinate representation $$F(u,v^2, \dots, v^m) = (u,F^2(u,v),\dots ,F^n(u,v))?$$ These are probably simple applications of the theorem but I am really struggling to understand this formally. I would greatly appreciate some help.","The following is part of the proof of the Sard's theorem from John Lee's Introduction to Smooth Manifolds. I am struggling to understand the second paragraph from Step 1. So assuming that , how do we define new smooth coordinates in some neighborhood of in by I think this is some form of the implicit function theorem, but from the theorem stated in the text, I cannot figure out how we can construct such a coordinate chart. Also, in terms of these coordinates how does F have the coordinate representation These are probably simple applications of the theorem but I am really struggling to understand this formally. I would greatly appreciate some help.","\partial F^1 / \partial x^1(a)\neq 0  (u,v)=(u,v^2,\dots , v^m) V_a a U u=F^1, v^2 = x^2 ,\dots ,v^m=x^m? F(u,v^2, \dots, v^m) = (u,F^2(u,v),\dots ,F^n(u,v))?","['calculus', 'multivariable-calculus', 'differential-geometry', 'manifolds', 'smooth-manifolds']"
68,"""Local parametrizations"" implying ""Local flat"" in the two definitions of submanifolds of Euclidean spaces","""Local parametrizations"" implying ""Local flat"" in the two definitions of submanifolds of Euclidean spaces",,"I am trying proving the four definitions for submanifolds of Euclidean space providing in https://www.mathematik.uni-muenchen.de/~tvogel/Vorlesungen/TMP/skript-TMP.pdf are equivalent (I came to this reference for reading the book Vector Calculus, Linear Algebra, and Differential Forms of J.H. Hubbard and B.B. Hubbard.): For equivalent definitions of the notion of submanifold of dimension $k\in \mathbb{N}^+ = \{1,2,...\}$ . In all four of them, $M \in \mathbb{R}^n$ . And in all all four of them, smooth means belonging to $C^l$ for some positive integer $l$ , or infinitely differentiable. Condition (a) Local parametrizations: For all $p \in M$ there is an open set $U \in \mathbb{R}^k$ , a neighborhodd $V \in \mathbb{R}^n$ of $p$ and a smooth map $\varphi: U \rightarrow \mathbb{R}^n$ such that $\varphi$ is a homeomorphism onto $V \cap M$ , and for all $x \in U$ the differential $D_x\varphi: \mathbb{R}^k \rightarrow \mathbb{R}^n$ is injective. Condition (b) Locally flat: For all $p \in M$ there are an open neighbourhood $V \subset \mathbb{R}^n$ of $p$ and $W \subset \mathbb{R}^n$ and a diffeomorphism $\phi: V \rightarrow W$ such that $\phi(p) = 0$ and $\phi(V\cap M) = (\mathbb{R}^k \times \{0\in \mathbb{R}^{n-k}\}) \cap W$ Condition (c) Locally regular level set: For all $p \in M$ there is an open neighborhood $V$ and a smooth function $F: V \rightarrow \mathbb{R}^{n-k}$ such that $F^{-1}(0) = V\cap M$ , and for all $q \in M \cap V$ the differential $D_q F : \mathbb{R}^n \rightarrow \mathbb{R}^{n-k}$ is surjective Condition (d) Locally a graph: For all $p \in M$ there is an open neighbourhood $V \in \mathbb{R}^{n}$ and an open subset $U \in \mathbb{R}^k$ and a smooth function $g: U \rightarrow \mathbb{R}^{n-k}$ and a permutation $\sigma \in S_n$ such that $V\cap M = \{(y_{\sigma(1)}, y_{\sigma(2)}, \dots, y_{\sigma(n)})| y = (x, g(x)) \text{ where } x \in U\}$ I found that proving their equivalence in details seems quite tedious. I tried to restate the proof of $a \Rightarrow b$ providing on the book Calculus on manifolds by Michael Spivak as follows, because I think the proof in it is missing some details. Could someone please check if the proof is missing any details or if there is some more elegant proof? Thanks! Proof (a) => (b) in details Denote $x_0 = \varphi^{-1}(p)$ . By item 2 of Condition (a), $D_p \varphi : \mathbb{R}^k \rightarrow \mathbb{R}^n$ is injective, hence its Jacobian matrix: $$ D_x\varphi(x_0)= \left(\begin{array}{cccc} \frac{\partial \varphi_{1}}{\partial x_{1}} & \frac{\partial \varphi_{1}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{1}}{\partial x_{k}} \\ \frac{\partial \varphi_{2}}{\partial x_{1}} & \frac{\partial \varphi_{2}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{2}}{\partial x_{k}} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial \varphi_{n}}{\partial x_{1}} & \frac{\partial \varphi_{n}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{n}}{\partial x_{k}} \end{array}\right) \bigg\rvert_{(x_1, x_2, \cdots, x_k) = x_0} $$ has rank $k$ . Hence there are positive integers $1 \leq r_1 \leq r_2 \leq \dots \leq r_k \leq n$ such that $$ \left(\begin{array}{cccc} \frac{\partial \varphi_{r_1}}{\partial x_{r_1}} & \frac{\partial \varphi_{r_1}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_1}}{\partial x_{k}} \\ \frac{\partial \varphi_{r_2}}{\partial x_{1}} & \frac{\partial \varphi_{r_2}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_2}}{\partial x_{k}} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial \varphi_{r_k}}{\partial x_{1}} & \frac{\partial \varphi_{r_k}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_k}}{\partial x_{k}} \end{array}\right) \bigg\rvert_{(x_1, x_2, \cdots, x_k) = x_0} $$ has nonzero determinant. Let $\sigma$ be any permutation of $\{1,2,\cdots, n\}$ such that $\sigma(1) = r_1, \sigma(2) = r2, ...,\sigma(k) = r_k$ . And define $P: \mathbb{R}^n \rightarrow \mathbb{R}^n$ by $P(y_1, y_2, \cdots, y_n) = (y_{\sigma(1)}, y_{\sigma(2)}, \cdots, y_{\sigma(n)}) = (y_{r_1}, y_{r_2}, \cdots, y_{r_k}, \cdots)$ . Then obviously(In fact I think the proof of it is quite tedious. Though it do be just some permutation of the coordinates.) $P$ is a diffeomorphism of $\mathbb{R}^n$ . And $$  D_x(P \circ \varphi) = \left(\begin{array}{cccc} \frac{\partial \varphi_{r_1}}{\partial x_{1}} & \frac{\partial \varphi_{r_1}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_1}}{\partial x_{k}} \\ \frac{\partial \varphi_{r_2}}{\partial x_{1}} & \frac{\partial \varphi_{r_2}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_2}}{\partial x_{k}} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial \varphi_{r_k}}{\partial x_{1}} & \frac{\partial \varphi_{r_k}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_k}}{\partial x_{k}} \\ \vdots & \vdots & \vdots & \vdots \end{array}\right) .  $$ The first $k$ row of it's value at $x_0=\varphi^{-1}(p)$ has nonzero determinant by the previous assumption. Since $\varphi$ is smooth, so is $P \circ \varphi$ . So there is a neighborhood $U_1 \subset U$ of $x_0$ such that $D_x(P \circ \varphi)$ has nonzero determinant is it. Define $\Phi: U_1 \times \mathbb{R}^{n-k} \rightarrow \mathbb{R}^n$ by $$ \Phi(x,y) = P(\varphi(x)) + (0, y) $$ where $x \in U, y \in \mathbb{R}^k$ and $(0, y)$ means that the first $k$ coordinates are zero and the last $n-k$ coordinates is $y$ . Then $$ D\Phi =   \left(\begin{array}{cccccccc} \frac{\partial \varphi_{r_1}}{\partial x_{1}} & \frac{\partial \varphi_{r_1}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_1}}{\partial x_{k}} & 0 & 0 & \cdots & 0 \\ \frac{\partial \varphi_{r_2}}{\partial x_{1}} & \frac{\partial \varphi_{r_2}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_2}}{\partial x_{k}} & 0 & 0 & \cdots & 0\\ \vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\ \frac{\partial \varphi_{r_k}}{\partial x_{1}} & \frac{\partial \varphi_{r_k}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_k}}{\partial x_{k}} &0 & 0 & \cdots & 0 \\ \vdots & \vdots & \vdots & \vdots & 1 & 0 & \cdots & 0 \\ \vdots & \vdots & \vdots & \vdots  & 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\ \vdots & \vdots & \vdots & \vdots  & 0 & 0 & \cdots & 1  \end{array}\right) $$ has nonzero determinant on $U_1 \times \mathbb{R}^{n-k}$ . Define $\Phi_1: U_1 \times \mathbb{R}^{n-k} \rightarrow \mathbb{R}^n$ by $\Phi_1 = P^{-1} \circ \Phi$ , then its Jacobian matrix has nonzero  determinant on $U_1 \times \mathbb{R}^{n-k}$ , and $\Phi_1(x_0, 0) = P^{-1}(P(\varphi(x_0))+(0,0)) = \varphi(x_0) = p$ .( $P^{-1}$ is linear and $\Phi_1(x,y) = P^{-1}(P(\varphi(x))+(0,y)) = \varphi(x)+P^{-1}(0,y)$ . Define $\Phi_1$ directly seems very hard to write the statement.) By the inverse function theorem, there is neighborhood $W_1 \subset U_1 \times \mathbb{R}^{n-k}$ of $(x_0, 0)$ and neighborhood $V_1$ of $p$ and the restriction $\Phi_1: W_1 \rightarrow V_1$ is a diffeomorphism (Normally the textbook state the inverse function theorem without saying the inverse function is $C^{k}$ if the original function is $C^{k}$ . But it do be seems the Jacobian matrix of the inverse function is a raitional function of the Jacobian matrix of the original function. Ref: Inverse function theorem: how show $F \in C^k \Rightarrow F^{-1} \in C^k$ with this method? maybe.). Since $\varphi: U \rightarrow V\cap M$ is a homeomorphism, and $W_1 \cap (\mathbb{R}^k \times \{0\in \mathbb{R}^{n-k}\})$ is a open neighborhood of $x_0$ (In fact there is still a homeomorphism between them, but stating it clearly is very tedious.) and subset of $U_1 \subset U$ . Hence $\Phi_1(W_1 \cap (\mathbb{R}^k \times \{0\in \mathbb{R}^{n-k}\})) = V_2 \cap M$ for some $V_2$ (Is there some counterexample to show that $V_2$ cannot be $V_1$ ?). Let $V_3 = V_1 \cap V_2$ , and $W_3 = \Phi_1^{-1}(V_3)$ , and $\phi_3: V_3 \rightarrow W_3$ be the restriction of $\Phi_1^{-1}$ on $V_3$ . Then $\phi_3$ satisfies: $\phi_3: V_3 \rightarrow W_3$ is diffeomorphism. $\phi_3(p) = (x_0, 0)$ $\phi_3(V_3 \cap M) = W_3 \cap (\mathbb{R}^k \times \{0\in \mathbb{R}^{n-k}\})$ Take $\phi_4(p) = \phi_3(p) - (x_0, 0)$ . Then $V_3$ , $\phi_4(V_3)$ , $\phi_4$ is the $V, W, \phi$ in condition (b). â¬ Though the idea seems quite simple. The proof is so tedious, and the part for proving that $\phi_3(V_3 \cap M) = W_3 \cap (\mathbb{R}^k \times \{0\in \mathbb{R}^{n-k}\})$ seems not right or well stated. Could someone help to check the proof or provide some more rigorous proof please? And any hint for proving the other directions is greatly appreciated too! (Though the keypoint as I know is the inverse/implicit theorem, but using it to state a rigorous proof of the equivalent of the above four definition seems still quite hard for me.) Thanks!","I am trying proving the four definitions for submanifolds of Euclidean space providing in https://www.mathematik.uni-muenchen.de/~tvogel/Vorlesungen/TMP/skript-TMP.pdf are equivalent (I came to this reference for reading the book Vector Calculus, Linear Algebra, and Differential Forms of J.H. Hubbard and B.B. Hubbard.): For equivalent definitions of the notion of submanifold of dimension . In all four of them, . And in all all four of them, smooth means belonging to for some positive integer , or infinitely differentiable. Condition (a) Local parametrizations: For all there is an open set , a neighborhodd of and a smooth map such that is a homeomorphism onto , and for all the differential is injective. Condition (b) Locally flat: For all there are an open neighbourhood of and and a diffeomorphism such that and Condition (c) Locally regular level set: For all there is an open neighborhood and a smooth function such that , and for all the differential is surjective Condition (d) Locally a graph: For all there is an open neighbourhood and an open subset and a smooth function and a permutation such that I found that proving their equivalence in details seems quite tedious. I tried to restate the proof of providing on the book Calculus on manifolds by Michael Spivak as follows, because I think the proof in it is missing some details. Could someone please check if the proof is missing any details or if there is some more elegant proof? Thanks! Proof (a) => (b) in details Denote . By item 2 of Condition (a), is injective, hence its Jacobian matrix: has rank . Hence there are positive integers such that has nonzero determinant. Let be any permutation of such that . And define by . Then obviously(In fact I think the proof of it is quite tedious. Though it do be just some permutation of the coordinates.) is a diffeomorphism of . And The first row of it's value at has nonzero determinant by the previous assumption. Since is smooth, so is . So there is a neighborhood of such that has nonzero determinant is it. Define by where and means that the first coordinates are zero and the last coordinates is . Then has nonzero determinant on . Define by , then its Jacobian matrix has nonzero  determinant on , and .( is linear and . Define directly seems very hard to write the statement.) By the inverse function theorem, there is neighborhood of and neighborhood of and the restriction is a diffeomorphism (Normally the textbook state the inverse function theorem without saying the inverse function is if the original function is . But it do be seems the Jacobian matrix of the inverse function is a raitional function of the Jacobian matrix of the original function. Ref: Inverse function theorem: how show $F \in C^k \Rightarrow F^{-1} \in C^k$ with this method? maybe.). Since is a homeomorphism, and is a open neighborhood of (In fact there is still a homeomorphism between them, but stating it clearly is very tedious.) and subset of . Hence for some (Is there some counterexample to show that cannot be ?). Let , and , and be the restriction of on . Then satisfies: is diffeomorphism. Take . Then , , is the in condition (b). â¬ Though the idea seems quite simple. The proof is so tedious, and the part for proving that seems not right or well stated. Could someone help to check the proof or provide some more rigorous proof please? And any hint for proving the other directions is greatly appreciated too! (Though the keypoint as I know is the inverse/implicit theorem, but using it to state a rigorous proof of the equivalent of the above four definition seems still quite hard for me.) Thanks!","k\in \mathbb{N}^+ = \{1,2,...\} M \in \mathbb{R}^n C^l l p \in M U \in \mathbb{R}^k V \in \mathbb{R}^n p \varphi: U \rightarrow \mathbb{R}^n \varphi V \cap M x \in U D_x\varphi: \mathbb{R}^k \rightarrow \mathbb{R}^n p \in M V \subset \mathbb{R}^n p W \subset \mathbb{R}^n \phi: V \rightarrow W \phi(p) = 0 \phi(V\cap M) = (\mathbb{R}^k \times \{0\in \mathbb{R}^{n-k}\}) \cap W p \in M V F: V \rightarrow \mathbb{R}^{n-k} F^{-1}(0) = V\cap M q \in M \cap V D_q F : \mathbb{R}^n \rightarrow \mathbb{R}^{n-k} p \in M V \in \mathbb{R}^{n} U \in \mathbb{R}^k g: U \rightarrow \mathbb{R}^{n-k} \sigma \in S_n V\cap M = \{(y_{\sigma(1)}, y_{\sigma(2)}, \dots, y_{\sigma(n)})| y = (x, g(x)) \text{ where } x \in U\} a \Rightarrow b x_0 = \varphi^{-1}(p) D_p \varphi : \mathbb{R}^k \rightarrow \mathbb{R}^n 
D_x\varphi(x_0)=
\left(\begin{array}{cccc}
\frac{\partial \varphi_{1}}{\partial x_{1}} & \frac{\partial \varphi_{1}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{1}}{\partial x_{k}} \\
\frac{\partial \varphi_{2}}{\partial x_{1}} & \frac{\partial \varphi_{2}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{2}}{\partial x_{k}} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial \varphi_{n}}{\partial x_{1}} & \frac{\partial \varphi_{n}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{n}}{\partial x_{k}}
\end{array}\right) \bigg\rvert_{(x_1, x_2, \cdots, x_k) = x_0}
 k 1 \leq r_1 \leq r_2 \leq \dots \leq r_k \leq n 
\left(\begin{array}{cccc}
\frac{\partial \varphi_{r_1}}{\partial x_{r_1}} & \frac{\partial \varphi_{r_1}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_1}}{\partial x_{k}} \\
\frac{\partial \varphi_{r_2}}{\partial x_{1}} & \frac{\partial \varphi_{r_2}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_2}}{\partial x_{k}} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial \varphi_{r_k}}{\partial x_{1}} & \frac{\partial \varphi_{r_k}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_k}}{\partial x_{k}}
\end{array}\right) \bigg\rvert_{(x_1, x_2, \cdots, x_k) = x_0}
 \sigma \{1,2,\cdots, n\} \sigma(1) = r_1, \sigma(2) = r2, ...,\sigma(k) = r_k P: \mathbb{R}^n \rightarrow \mathbb{R}^n P(y_1, y_2, \cdots, y_n) = (y_{\sigma(1)}, y_{\sigma(2)}, \cdots, y_{\sigma(n)}) = (y_{r_1}, y_{r_2}, \cdots, y_{r_k}, \cdots) P \mathbb{R}^n 
 D_x(P \circ \varphi) = \left(\begin{array}{cccc}
\frac{\partial \varphi_{r_1}}{\partial x_{1}} & \frac{\partial \varphi_{r_1}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_1}}{\partial x_{k}} \\
\frac{\partial \varphi_{r_2}}{\partial x_{1}} & \frac{\partial \varphi_{r_2}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_2}}{\partial x_{k}} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial \varphi_{r_k}}{\partial x_{1}} & \frac{\partial \varphi_{r_k}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_k}}{\partial x_{k}} \\
\vdots & \vdots & \vdots & \vdots
\end{array}\right) .
  k x_0=\varphi^{-1}(p) \varphi P \circ \varphi U_1 \subset U x_0 D_x(P \circ \varphi) \Phi: U_1 \times \mathbb{R}^{n-k} \rightarrow \mathbb{R}^n 
\Phi(x,y) = P(\varphi(x)) + (0, y)
 x \in U, y \in \mathbb{R}^k (0, y) k n-k y 
D\Phi = 
 \left(\begin{array}{cccccccc}
\frac{\partial \varphi_{r_1}}{\partial x_{1}} & \frac{\partial \varphi_{r_1}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_1}}{\partial x_{k}} & 0 & 0 & \cdots & 0 \\
\frac{\partial \varphi_{r_2}}{\partial x_{1}} & \frac{\partial \varphi_{r_2}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_2}}{\partial x_{k}} & 0 & 0 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots & \vdots & \ddots & \vdots\\
\frac{\partial \varphi_{r_k}}{\partial x_{1}} & \frac{\partial \varphi_{r_k}}{\partial x_{2}} & \cdots & \frac{\partial \varphi_{r_k}}{\partial x_{k}} &0 & 0 & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots & 1 & 0 & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots  & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
\vdots & \vdots & \vdots & \vdots  & 0 & 0 & \cdots & 1 
\end{array}\right)
 U_1 \times \mathbb{R}^{n-k} \Phi_1: U_1 \times \mathbb{R}^{n-k} \rightarrow \mathbb{R}^n \Phi_1 = P^{-1} \circ \Phi U_1 \times \mathbb{R}^{n-k} \Phi_1(x_0, 0) = P^{-1}(P(\varphi(x_0))+(0,0)) = \varphi(x_0) = p P^{-1} \Phi_1(x,y) = P^{-1}(P(\varphi(x))+(0,y)) = \varphi(x)+P^{-1}(0,y) \Phi_1 W_1 \subset U_1 \times \mathbb{R}^{n-k} (x_0, 0) V_1 p \Phi_1: W_1 \rightarrow V_1 C^{k} C^{k} \varphi: U \rightarrow V\cap M W_1 \cap (\mathbb{R}^k \times \{0\in \mathbb{R}^{n-k}\}) x_0 U_1 \subset U \Phi_1(W_1 \cap (\mathbb{R}^k \times \{0\in \mathbb{R}^{n-k}\})) = V_2 \cap M V_2 V_2 V_1 V_3 = V_1 \cap V_2 W_3 = \Phi_1^{-1}(V_3) \phi_3: V_3 \rightarrow W_3 \Phi_1^{-1} V_3 \phi_3 \phi_3: V_3 \rightarrow W_3 \phi_3(p) = (x_0, 0) \phi_3(V_3 \cap M) = W_3 \cap (\mathbb{R}^k \times \{0\in \mathbb{R}^{n-k}\}) \phi_4(p) = \phi_3(p) - (x_0, 0) V_3 \phi_4(V_3) \phi_4 V, W, \phi \phi_3(V_3 \cap M) = W_3 \cap (\mathbb{R}^k \times \{0\in \mathbb{R}^{n-k}\})","['multivariable-calculus', 'manifolds', 'smooth-manifolds']"
69,Where's my mistake evaluating the mass of $V$?,Where's my mistake evaluating the mass of ?,V,"Evaluate the mass of $V=\{ (x,y,z)|x^2+y^2+z^2\le 2, z \ge 0, x^2+y^2 \ge 1$ } While the density of mass is $\phi(x,y,z)=z$ What I did: How I visualize $V$ : The first part is a ball with a radius $2$ (it's volume), $z\ge 0$ makes me take the upper half of the ball, and for $x^2+y^2 \ge 1$ , I drew the cylinder $x^2+y^2=1$ inside the half ball, and $V$ is the volume between the ball and the cylinder. So I decided to use ball coordinates to solve: $x=r\cos\theta \sin\phi$ . $y=r\sin\theta \sin\phi.$ $z=r\cos\phi$ $|J|=r^2\sin\phi$ . And from how I visualized $V$ , I set the bounds as this: $1 \le r \le 2$ . (from the cylinder to the ball) $0\le \phi \le \frac{\pi}{2}$ . (from the positive $z$ axis to xy plane - half ball). $0 \le \theta \le 2\pi$ . (must go around all $V$ ). And so, $$\int_0^{\pi/2}d\phi \int_0^{2\pi}d\theta\int_1^2r^3\sin\phi\cos\phi=2\pi[4-\frac{1}{4}]\frac{1}{2}\int_0^{\pi/2} \sin(2\phi)d\phi = \pi[4-\frac{1}{4}]$$ But the final answer is : $\frac{\pi}{4}$ . Would love to know which mistakes I made, thanks in advance.","Evaluate the mass of } While the density of mass is What I did: How I visualize : The first part is a ball with a radius (it's volume), makes me take the upper half of the ball, and for , I drew the cylinder inside the half ball, and is the volume between the ball and the cylinder. So I decided to use ball coordinates to solve: . . And from how I visualized , I set the bounds as this: . (from the cylinder to the ball) . (from the positive axis to xy plane - half ball). . (must go around all ). And so, But the final answer is : . Would love to know which mistakes I made, thanks in advance.","V=\{ (x,y,z)|x^2+y^2+z^2\le 2, z \ge 0, x^2+y^2 \ge 1 \phi(x,y,z)=z V 2 z\ge 0 x^2+y^2 \ge 1 x^2+y^2=1 V x=r\cos\theta \sin\phi y=r\sin\theta \sin\phi. z=r\cos\phi |J|=r^2\sin\phi V 1 \le r \le 2 0\le \phi \le \frac{\pi}{2} z 0 \le \theta \le 2\pi V \int_0^{\pi/2}d\phi \int_0^{2\pi}d\theta\int_1^2r^3\sin\phi\cos\phi=2\pi[4-\frac{1}{4}]\frac{1}{2}\int_0^{\pi/2} \sin(2\phi)d\phi = \pi[4-\frac{1}{4}] \frac{\pi}{4}","['integration', 'multivariable-calculus']"
70,Explaining the integral of Laplacian of $r \mapsto \frac1r$,Explaining the integral of Laplacian of,r \mapsto \frac1r,"Consider the function $\Bbb R^3 \to \Bbb R$ defined by $$f(x,y,z) = \frac1{\left( x^2 + y^2 + z^2 \right)^{\frac12}}$$ or, written in polar coordinates, $f (r) = \frac1r$ . The Laplacian $\nabla^2 f \equiv \nabla \cdot (\nabla f)$ can be computed as $0$ everywhere except the origin. Is there a way to rigorously define its value at the origin, and a concept of integration, such that $$\int_{\Bbb R^3} \nabla^2 f = -4\pi?$$ (This is a result I saw in a physics textbook that was computed by non-rigorous means.) If not, what does the previous integral expression actual mean, mathematically?","Consider the function defined by or, written in polar coordinates, . The Laplacian can be computed as everywhere except the origin. Is there a way to rigorously define its value at the origin, and a concept of integration, such that (This is a result I saw in a physics textbook that was computed by non-rigorous means.) If not, what does the previous integral expression actual mean, mathematically?","\Bbb R^3 \to \Bbb R f(x,y,z) = \frac1{\left( x^2 + y^2 + z^2 \right)^{\frac12}} f (r) = \frac1r \nabla^2 f \equiv \nabla \cdot (\nabla f) 0 \int_{\Bbb R^3} \nabla^2 f = -4\pi?","['multivariable-calculus', 'physics', 'polar-coordinates', 'laplacian']"
71,Triple integral - how to make a projection on the $xy$ plane?,Triple integral - how to make a projection on the  plane?,xy,"I'm starting to study triple integrals. In general, I have been doing problems which require me to sketch the projection on the $xy$ plane so I can figure out the boundaries for $x$ and $y$ . For example, I had an exercise where I had to calculate the volume bound between the planes $x=0$ , $y=0$ , $z=0$ , $x+y+z=1$ which was easy. For the projection on the $xy$ plane, I set that $z=0$ , then I got $x+y=1$ which is a line. However, now I have the following problem: Calculate the volume bound between: $$z=xy$$ $$x+y+z=1$$ $$z=0$$ now I know that if I put $z=0$ into the second equation I get the equation $y=1-x$ which is a line, but I also know that $z=xy$ has to play a role in the projection. If I put $xy=0$ I don't get anything useful. Can someone help me understand how these projections work and how I can apply it here?","I'm starting to study triple integrals. In general, I have been doing problems which require me to sketch the projection on the plane so I can figure out the boundaries for and . For example, I had an exercise where I had to calculate the volume bound between the planes , , , which was easy. For the projection on the plane, I set that , then I got which is a line. However, now I have the following problem: Calculate the volume bound between: now I know that if I put into the second equation I get the equation which is a line, but I also know that has to play a role in the projection. If I put I don't get anything useful. Can someone help me understand how these projections work and how I can apply it here?",xy x y x=0 y=0 z=0 x+y+z=1 xy z=0 x+y=1 z=xy x+y+z=1 z=0 z=0 y=1-x z=xy xy=0,"['integration', 'multivariable-calculus', 'multiple-integral']"
72,Is this function differentiable? Is this directional derivative correct?,Is this function differentiable? Is this directional derivative correct?,,"I need to find the directional derivatives for all vectors $u=[u_1\ \ u_2]\in \mathbb R^2$ with $\|u\|=1$ at $P_0=(0,0)$ , and determine whether $f$ is differentiable at $P_0$ . $$f(x,y)=\begin{cases} 1 & y=x^2,x\neq 0\\ 0 & \text{else} \end{cases}$$ First of all, if $f$ is not continuous then can I always say it isn't differentiable? And my attemp was this: $$\lim_{t\rightarrow 0} \frac {f(P_0+tu)-f(P_0)} t = \lim_{t\rightarrow 0} \begin{cases} \frac{1}{t} & \text{else}\\ 0 & u_1=0 \text{ or } u_1^2\neq u_2\\ \end{cases}$$ Does the fact that $\lim_{t\rightarrow 0}\frac {1}{t}$ does not exist say anything about f being differentiable? Because $D_if(P_0)$ both exist for $i=1,2$ . So I'd like to know if my calculation is correct, and if the continuous statement is true. Thanks!","I need to find the directional derivatives for all vectors with at , and determine whether is differentiable at . First of all, if is not continuous then can I always say it isn't differentiable? And my attemp was this: Does the fact that does not exist say anything about f being differentiable? Because both exist for . So I'd like to know if my calculation is correct, and if the continuous statement is true. Thanks!","u=[u_1\ \ u_2]\in \mathbb R^2 \|u\|=1 P_0=(0,0) f P_0 f(x,y)=\begin{cases}
1 & y=x^2,x\neq 0\\
0 & \text{else}
\end{cases} f \lim_{t\rightarrow 0} \frac {f(P_0+tu)-f(P_0)} t = \lim_{t\rightarrow 0}
\begin{cases}
\frac{1}{t} & \text{else}\\
0 & u_1=0 \text{ or } u_1^2\neq u_2\\
\end{cases} \lim_{t\rightarrow 0}\frac {1}{t} D_if(P_0) i=1,2","['calculus', 'multivariable-calculus', 'solution-verification']"
73,Can you integrate a function of two variables with respect to one variable?,Can you integrate a function of two variables with respect to one variable?,,"Similar to how we take the partial derivative of a function $f(x,y)$ by holding one of the variables constant, why can we not take the integral of a function $f(x,y)$ by holding one variable constant? For example why don't these equal $\int xy \, dx = \frac{x^2}{2}y$ , assuming $y$ is a variable?","Similar to how we take the partial derivative of a function by holding one of the variables constant, why can we not take the integral of a function by holding one variable constant? For example why don't these equal , assuming is a variable?","f(x,y) f(x,y) \int xy \, dx = \frac{x^2}{2}y y","['calculus', 'multivariable-calculus']"
74,Solution Verification: Find the mass of $V$. (Triple integral),Solution Verification: Find the mass of . (Triple integral),V,"We define $V=\{(x,y,z)\in\mathbb{R}^3: x^2+y^2+z^2\le z$ } Mass Density is given by $g(x,y,z)=\sqrt{x^2+y^2+z^2}$ , Find the Mass of $V$ . My Work: So I need to find $I=\iiint_{V}\sqrt{x^2+y^2+z^2}dxdydz$ , and in order to do that, I will move to spherical coordinations: $x=rsin\phi cos\theta$ $y=rsin\phi sin\theta$ $z=rcos\phi$ . And so $x^2+y^2+z^2=r^2\le z = rcos\theta \Longrightarrow r\le cos\theta \Longrightarrow -\frac{\pi}{2} \le\theta \le \frac{\pi}{2}$ ( $cos\theta \ge 0)$ . and so there's no restrictions on $0 \le \phi \le \pi$ . and $0 \le r\le cos\theta$ . So: $$I=\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\int_{0}^{\pi}\int^{cos\theta}_{0}r.r^2sin\phi drd\phi d\theta=\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}[\int_0^{\pi}\frac{cos^4\theta}{4}sin\phi ]d\phi d\theta=\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}[\int^{\pi}_0\frac{1+cos^2(2\theta)}{2}sin\phi]d\phi d\theta = \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}[\int^{\pi}_0\frac{3+cos(4\theta)}{4}sin\phi]d\phi d\theta = \frac{1}{4} \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}[\int^{\pi}_0(3sin\phi+cos(4\theta)sin\phi)]d\phi d\theta = \frac{1}{4} \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}[(\pi3sin\phi+sin(4\pi)sin\phi)]d\phi d\theta = \frac{1}{4} \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}[(\pi3sin\phi)]d\phi=\frac{3}{4}\pi (-cos(-\pi) - cos(\pi))=\frac{6\pi}{4}$$ I would really appreciate any feedback about my solution, even the smallest mistakes, and would love to get an approval of my answer, Thanks in advance!","We define } Mass Density is given by , Find the Mass of . My Work: So I need to find , and in order to do that, I will move to spherical coordinations: . And so ( . and so there's no restrictions on . and . So: I would really appreciate any feedback about my solution, even the smallest mistakes, and would love to get an approval of my answer, Thanks in advance!","V=\{(x,y,z)\in\mathbb{R}^3: x^2+y^2+z^2\le z g(x,y,z)=\sqrt{x^2+y^2+z^2} V I=\iiint_{V}\sqrt{x^2+y^2+z^2}dxdydz x=rsin\phi cos\theta y=rsin\phi sin\theta z=rcos\phi x^2+y^2+z^2=r^2\le z = rcos\theta \Longrightarrow r\le cos\theta \Longrightarrow -\frac{\pi}{2} \le\theta \le \frac{\pi}{2} cos\theta \ge 0) 0 \le \phi \le \pi 0 \le r\le cos\theta I=\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\int_{0}^{\pi}\int^{cos\theta}_{0}r.r^2sin\phi drd\phi d\theta=\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}[\int_0^{\pi}\frac{cos^4\theta}{4}sin\phi ]d\phi d\theta=\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}[\int^{\pi}_0\frac{1+cos^2(2\theta)}{2}sin\phi]d\phi d\theta = \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}[\int^{\pi}_0\frac{3+cos(4\theta)}{4}sin\phi]d\phi d\theta = \frac{1}{4} \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}[\int^{\pi}_0(3sin\phi+cos(4\theta)sin\phi)]d\phi d\theta = \frac{1}{4} \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}[(\pi3sin\phi+sin(4\pi)sin\phi)]d\phi d\theta = \frac{1}{4} \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}[(\pi3sin\phi)]d\phi=\frac{3}{4}\pi (-cos(-\pi) - cos(\pi))=\frac{6\pi}{4}","['integration', 'multivariable-calculus', 'solution-verification']"
75,"To evaluate the surface integral $\iint_S(x\,dy\,dz+y\,dx\,dz+z\,dx\,dy)$",To evaluate the surface integral,"\iint_S(x\,dy\,dz+y\,dx\,dz+z\,dx\,dy)","How to evaluate the surface integral $$\iint_S(x\,dy\,dz+y\,dx\,dz+z\,dx\,dy)$$ where $S$ is the outer surface of the ellipsoid $$\frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}=1$$ that lie above the $xy-$ plane. My thought was to substitute $x=a \sin\theta \cos\psi, y=b \sin\theta \sin\psi, x=c \cos\theta$ , where $0\leq \theta\leq \frac{\pi}{2}$ and $0\leq \psi\leq 2\pi$ . But in this way i got the answer as $-abc\pi$ , which is suppose to be $2abc\pi$ . Any help is highly appreciated. Thank you in advance. My steps $$\frac{\delta(x,y)}{\delta(\theta,\psi)}=ab\sin\theta \cos\psi$$ $$\frac{\delta(y,z)}{\delta(\theta,\psi)}=bc\sin^2\theta \cos \psi$$ $$\frac{\delta(z,x)}{\delta(\theta,\psi)}=ac\sin^2\theta \sin \psi$$ The given integral $$=\int_{0}^{\frac{\pi}{2}}\int_{0}^{2\pi}(x\frac{\delta(y,z)}{\delta(\theta,\psi)}+y\frac{\delta(z,x)}{\delta(\theta,\psi)}+z\frac{\delta(x,y)}{\delta(\theta,\psi)})\,d\theta\, d\psi$$ $$=\int_{0}^{\frac{\pi}{2}}\int_{0}^{2\pi}(\sin \theta)\,d\theta\, d\psi$$ $$=-abc\pi$$","How to evaluate the surface integral where is the outer surface of the ellipsoid that lie above the plane. My thought was to substitute , where and . But in this way i got the answer as , which is suppose to be . Any help is highly appreciated. Thank you in advance. My steps The given integral","\iint_S(x\,dy\,dz+y\,dx\,dz+z\,dx\,dy) S \frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}=1 xy- x=a \sin\theta \cos\psi, y=b \sin\theta \sin\psi, x=c \cos\theta 0\leq \theta\leq \frac{\pi}{2} 0\leq \psi\leq 2\pi -abc\pi 2abc\pi \frac{\delta(x,y)}{\delta(\theta,\psi)}=ab\sin\theta \cos\psi \frac{\delta(y,z)}{\delta(\theta,\psi)}=bc\sin^2\theta \cos \psi \frac{\delta(z,x)}{\delta(\theta,\psi)}=ac\sin^2\theta \sin \psi =\int_{0}^{\frac{\pi}{2}}\int_{0}^{2\pi}(x\frac{\delta(y,z)}{\delta(\theta,\psi)}+y\frac{\delta(z,x)}{\delta(\theta,\psi)}+z\frac{\delta(x,y)}{\delta(\theta,\psi)})\,d\theta\, d\psi =\int_{0}^{\frac{\pi}{2}}\int_{0}^{2\pi}(\sin \theta)\,d\theta\, d\psi =-abc\pi","['multivariable-calculus', 'solution-verification', 'multiple-integral', 'surface-integrals']"
76,A conservative vector field on the unit sphere,A conservative vector field on the unit sphere,,"Consider the unit sphere $x^2+y^2+z^2=1$ and a vector field $$(x(y^2-z^2), -y(x^2+2z^2), z(x^2+2y^2)).$$ on the sphere. Show that this vector field is conservative on the sphere and find a potential function of it. I know how to show the vector field is conservative. One can simply show that the line integral of this vector field over any simple closed curve on the sphere is 0. However, I am not sure how to find a potential function of this vector field.","Consider the unit sphere and a vector field on the sphere. Show that this vector field is conservative on the sphere and find a potential function of it. I know how to show the vector field is conservative. One can simply show that the line integral of this vector field over any simple closed curve on the sphere is 0. However, I am not sure how to find a potential function of this vector field.","x^2+y^2+z^2=1 (x(y^2-z^2), -y(x^2+2z^2), z(x^2+2y^2)).","['multivariable-calculus', 'manifolds', 'vector-fields']"
77,Another question on reciprocals of partial derivatives,Another question on reciprocals of partial derivatives,,"Consider the function $f(\mathbf{x},g(\mathbf{x}))$ , where $f:\mathbb{R}^n\times\mathbb{R}^n\to \mathbb{R}$ , $\mathbf{x}\in\mathbb{R}^n$ and $g:\mathbb{R}^n\to\mathbb{R}^n$ . I want to take the gradient $\nabla_{\mathbf{x}}f(\mathbf{x},g(\mathbf{x}))$ , which I think should give me this... $$ \nabla_{\mathbf{x}}f(\mathbf{x},g(\mathbf{x})) = \left[\frac{\partial f}{\partial x_1} + \frac{\partial f}{\partial g}\frac{\partial g}{\partial x_1},\cdots,\frac{\partial f}{\partial x_n} + \frac{\partial f}{\partial g}\frac{\partial g}{\partial x_n}\right]^{T} $$ Finding the derivatives $\frac{\partial g}{\partial x_i}$ is a little tricky.  Naively I thought I could do something like $$ \frac{\partial g}{\partial x_i} = \frac{\partial g}{\partial f}\frac{\partial f}{\partial x_i} $$ under the assumption that $\frac{\partial g}{\partial f} = \left(\frac{\partial f}{\partial g}\right)^{-1}$ , yet substituting such an expression into the equation for the gradient would give me the following form $$ \nabla_{\mathbf{x}}f(\mathbf{x},g(\mathbf{x})) = \left[\cdots,2\frac{\partial f}{\partial x_i},\cdots\right] $$ Clearly the reciprocal argument used to find $\frac{\partial g}{\partial f}$ has been abused here, but I'm not clear why.  I was under the impression that such an argument could be used as long as the same variables are being held constant, and I think I am doing that.  I guess the remaining weak spot is that I have fundamentally misunderstood the application if inverse function theorem in this context?","Consider the function , where , and . I want to take the gradient , which I think should give me this... Finding the derivatives is a little tricky.  Naively I thought I could do something like under the assumption that , yet substituting such an expression into the equation for the gradient would give me the following form Clearly the reciprocal argument used to find has been abused here, but I'm not clear why.  I was under the impression that such an argument could be used as long as the same variables are being held constant, and I think I am doing that.  I guess the remaining weak spot is that I have fundamentally misunderstood the application if inverse function theorem in this context?","f(\mathbf{x},g(\mathbf{x})) f:\mathbb{R}^n\times\mathbb{R}^n\to \mathbb{R} \mathbf{x}\in\mathbb{R}^n g:\mathbb{R}^n\to\mathbb{R}^n \nabla_{\mathbf{x}}f(\mathbf{x},g(\mathbf{x})) 
\nabla_{\mathbf{x}}f(\mathbf{x},g(\mathbf{x})) = \left[\frac{\partial f}{\partial x_1} + \frac{\partial f}{\partial g}\frac{\partial g}{\partial x_1},\cdots,\frac{\partial f}{\partial x_n} + \frac{\partial f}{\partial g}\frac{\partial g}{\partial x_n}\right]^{T}
 \frac{\partial g}{\partial x_i} 
\frac{\partial g}{\partial x_i} = \frac{\partial g}{\partial f}\frac{\partial f}{\partial x_i}
 \frac{\partial g}{\partial f} = \left(\frac{\partial f}{\partial g}\right)^{-1} 
\nabla_{\mathbf{x}}f(\mathbf{x},g(\mathbf{x})) = \left[\cdots,2\frac{\partial f}{\partial x_i},\cdots\right]
 \frac{\partial g}{\partial f}","['calculus', 'multivariable-calculus', 'partial-derivative']"
78,Gradient defined on level set,Gradient defined on level set,,"Suppose I have a continuously differentiable function $f:\mathbb{R}^N\rightarrow \mathbb{R}$ where the gradient is defined everywhere. Let $c$ be some constant in the range of $f$ and let $S=\{x \in \mathbb{R}^N: f(x)>c\}$ . Assume $S$ is non-empty. Assume the gradient along the boundary of $S$ , $\partial S$ , is non-zero everywhere. This is where I'm struggling a little with phrasing my question. If people have comments to help me fix this question's phrasing please tell me and I will change it ASAP. I have drawn the below picture to help show what I am asking. I want to show that for any point, $p$ , along the boundary of $S$ , $\partial S$ , if I draw a ball around $p$ , the set $\partial S$ can only partition the ball into $2$ components. On the left-hand side of the picture is an example where this condition can be seen to be the case for all $p$ on $\partial S$ , whereas on the right-hand side it cannot be the case as the at the point $p$ labelled, the $\partial S$ divides the ball into 4 components. Intuitively, I can explain why I think situations like on the right-hand side can't happen. The gradient $\nabla f$ must be normal to the level set and, in that example, I cannot draw a (non-zero) vector normal to $\partial S$ at $p$ . However, I have no idea how to show this formally. Can anyone help me formalize my problem and solution?","Suppose I have a continuously differentiable function where the gradient is defined everywhere. Let be some constant in the range of and let . Assume is non-empty. Assume the gradient along the boundary of , , is non-zero everywhere. This is where I'm struggling a little with phrasing my question. If people have comments to help me fix this question's phrasing please tell me and I will change it ASAP. I have drawn the below picture to help show what I am asking. I want to show that for any point, , along the boundary of , , if I draw a ball around , the set can only partition the ball into components. On the left-hand side of the picture is an example where this condition can be seen to be the case for all on , whereas on the right-hand side it cannot be the case as the at the point labelled, the divides the ball into 4 components. Intuitively, I can explain why I think situations like on the right-hand side can't happen. The gradient must be normal to the level set and, in that example, I cannot draw a (non-zero) vector normal to at . However, I have no idea how to show this formally. Can anyone help me formalize my problem and solution?",f:\mathbb{R}^N\rightarrow \mathbb{R} c f S=\{x \in \mathbb{R}^N: f(x)>c\} S S \partial S p S \partial S p \partial S 2 p \partial S p \partial S \nabla f \partial S p,"['multivariable-calculus', 'vector-spaces', 'tangent-line']"
79,"In the proof of Morse's Lemma, why can it happen that $h_{ij} \neq h_{ji}$?","In the proof of Morse's Lemma, why can it happen that ?",h_{ij} \neq h_{ji},"The following is the first step in the proof of Morse's Lemma: Without loss of generality, we can assume that $\overline x$ is the origin in $\mathbb R^N$ (in the sense that the chart takes $\overline x$ to the origin) and that $f(\overline x) = f(0) = 0$ (trivial). Note that we can write, for $x$ in a neighborhood of the origin, \begin{align*} f(x)  & = \int_0^1 \sum_{i = 1}^N \frac{\partial f}{\partial x_i} (tx) x_i \ dt \\ & = \sum_{i,j = 1}^N \left(\int_0^1 \int_0^1\frac{\partial^2f}{\partial x_i \partial x_j} (stx) \ ds \ dt\right)x_ix_j \\ & = \sum_{i, j = 1}^N h_{ij}(x) x_i x_j. \end{align*} Then, Milnor argues that we can assume $h_{ij} = h_{ji}$ by taking $\overline h_{ij} = \frac 12(h_{ij} + h_{ji})$ if necessary. My question is, isn't $h_{ij} = h_{ji}$ already, since the function $f$ is smooth?","The following is the first step in the proof of Morse's Lemma: Without loss of generality, we can assume that is the origin in (in the sense that the chart takes to the origin) and that (trivial). Note that we can write, for in a neighborhood of the origin, Then, Milnor argues that we can assume by taking if necessary. My question is, isn't already, since the function is smooth?","\overline x \mathbb R^N \overline x f(\overline x) = f(0) = 0 x \begin{align*}
f(x) 
& = \int_0^1 \sum_{i = 1}^N \frac{\partial f}{\partial x_i} (tx) x_i \ dt \\
& = \sum_{i,j = 1}^N \left(\int_0^1 \int_0^1\frac{\partial^2f}{\partial x_i \partial x_j} (stx) \ ds \ dt\right)x_ix_j \\
& = \sum_{i, j = 1}^N h_{ij}(x) x_i x_j.
\end{align*} h_{ij} = h_{ji} \overline h_{ij} = \frac 12(h_{ij} + h_{ji}) h_{ij} = h_{ji} f","['calculus', 'multivariable-calculus', 'morse-theory']"
80,"Any neat way to solve the integral $\int_{-a}^a \int_{-b}^b\frac{1}{\left(x^2+y^2+z^2\right)^{3/2}}\,dxdy$?",Any neat way to solve the integral ?,"\int_{-a}^a \int_{-b}^b\frac{1}{\left(x^2+y^2+z^2\right)^{3/2}}\,dxdy","Straight to the point: given the integral $$\iint_Q \frac{1}{\left(x^2+y^2+z^2\right)^{3/2}}\,dxdy$$ where $Q=[-a,a]\times[-b,b]$ , can you think of any neat way to solve it? At a first glance it looked quite innocent. No need to say that Iâve changed my mind. EDIT: Integrating first w.r.t. y, using the integral $$\int \frac{1}{\left(\xi^2+\alpha^2\right)^{3/2}}\,d\xi = \frac{\xi}{\alpha^2\sqrt{\xi^2+\alpha^2}} + \text{constant}, $$ one gets to $$ 2b \int_{-a}^a \frac{1}{\left(x^2+z^2\right)\sqrt{x^2+b^2+z^2}}\,dx $$ and here I get pretty stuck to be honest, so if someone could give me any hints that would be appreciated. Still, I think there should be some nicer way to approach this from the start. Since I was asked for the source of this problem: I simply asked myself a basic physics question, and trying to find an answer led to this integral.","Straight to the point: given the integral where , can you think of any neat way to solve it? At a first glance it looked quite innocent. No need to say that Iâve changed my mind. EDIT: Integrating first w.r.t. y, using the integral one gets to and here I get pretty stuck to be honest, so if someone could give me any hints that would be appreciated. Still, I think there should be some nicer way to approach this from the start. Since I was asked for the source of this problem: I simply asked myself a basic physics question, and trying to find an answer led to this integral.","\iint_Q \frac{1}{\left(x^2+y^2+z^2\right)^{3/2}}\,dxdy Q=[-a,a]\times[-b,b] \int \frac{1}{\left(\xi^2+\alpha^2\right)^{3/2}}\,d\xi = \frac{\xi}{\alpha^2\sqrt{\xi^2+\alpha^2}} + \text{constant},   2b \int_{-a}^a \frac{1}{\left(x^2+z^2\right)\sqrt{x^2+b^2+z^2}}\,dx ","['integration', 'multivariable-calculus', 'multiple-integral']"
81,Confused about partial derivatives,Confused about partial derivatives,,"I am having some issues understanding what should I keep constant and what not in certain cases when I take partial derivatives. Specifically in this kind of situation: say we have a function $$f(x,y) = x^3+7y^2$$ and we also know that $y=2x+1$ and we need to find the partial derivative with respect to $x$ at a given point, say $\frac{\partial f(2,3)}{\partial x}$ . From what I understood, when taking partial derivatives with respect to a variable, you need to keep the other constant, in which case, if I do that above (keeping $y$ constant) I would get, $\frac{\partial f(x,y)}{\partial x}=3x^2$ , so I get $12$ . However, if I plug $x$ in $y$ explicitly I would get $$f(x,y) = x^3+7(2x+1)^2$$ so, $$\frac{\partial f(x,y)}{\partial x}=3x^2+28(2x+1)$$ So, I get $152$ . What should I do? Thank you!","I am having some issues understanding what should I keep constant and what not in certain cases when I take partial derivatives. Specifically in this kind of situation: say we have a function and we also know that and we need to find the partial derivative with respect to at a given point, say . From what I understood, when taking partial derivatives with respect to a variable, you need to keep the other constant, in which case, if I do that above (keeping constant) I would get, , so I get . However, if I plug in explicitly I would get so, So, I get . What should I do? Thank you!","f(x,y) = x^3+7y^2 y=2x+1 x \frac{\partial f(2,3)}{\partial x} y \frac{\partial f(x,y)}{\partial x}=3x^2 12 x y f(x,y) = x^3+7(2x+1)^2 \frac{\partial f(x,y)}{\partial x}=3x^2+28(2x+1) 152","['multivariable-calculus', 'partial-derivative']"
82,"How to take the directional derivative of an integral over $u$ from $(x,y)$ to $(1,1)$",How to take the directional derivative of an integral over  from  to,"u (x,y) (1,1)","Please consider the following function $$ v(x,y) = \int_{(x,y)}^{(1, 1)}u\,dS $$ $v(x,y)$ is the integral along the straight line from $(x,y)$ to $(1, 1)$ . I'm wondering how I can take the directional derivative in the direction of the vector from $(x,y)$ to $(1, 1)$ . Let's say that the vector going from the origin through $(x,y)$ and through $(1, 1)$ is called $\beta$ . How do I take the directional derivative in the direction of $\beta$ at the point $(x,y)$ ? Intuitively I would say that $$ \beta\cdot\nabla v(x,y) = - u(x,y) $$ but I'm not sure how I can show this rigorously.",Please consider the following function is the integral along the straight line from to . I'm wondering how I can take the directional derivative in the direction of the vector from to . Let's say that the vector going from the origin through and through is called . How do I take the directional derivative in the direction of at the point ? Intuitively I would say that but I'm not sure how I can show this rigorously.,"
v(x,y) = \int_{(x,y)}^{(1, 1)}u\,dS
 v(x,y) (x,y) (1, 1) (x,y) (1, 1) (x,y) (1, 1) \beta \beta (x,y) 
\beta\cdot\nabla v(x,y) = - u(x,y)
","['integration', 'multivariable-calculus', 'derivatives']"
83,compute $\int_\gamma F\cdot dr$ using Stokes,compute  using Stokes,\int_\gamma F\cdot dr,"Evaluate $$ \int_\gamma F\cdot dr$$ where $ F=(ye^x,e^x+x^3,z^5)$ and $ \gamma\ $ is the intersection between $x^2+y^2=1 $ and $z=2xy$ oriented in such a way that the orthogonal projection on the $xy$ plane is oriented counterclockwise. Here is how I have tried to solve this problem: I used Stokes and first calculated the curl of F which is $(0,0,3x^2)$ and then  doted this with unit normal but it become zero, however the answer should not become zero. What am I doing wrong ? Any suggestion would be great , Thanks","Evaluate where and is the intersection between and oriented in such a way that the orthogonal projection on the plane is oriented counterclockwise. Here is how I have tried to solve this problem: I used Stokes and first calculated the curl of F which is and then  doted this with unit normal but it become zero, however the answer should not become zero. What am I doing wrong ? Any suggestion would be great , Thanks"," \int_\gamma F\cdot dr  F=(ye^x,e^x+x^3,z^5)  \gamma\  x^2+y^2=1  z=2xy xy (0,0,3x^2)","['integration', 'multivariable-calculus', 'line-integrals', 'stokes-theorem']"
84,Equality of triple integrals over unit sphere $ \iiint_{\text{unit ball}} x e^{ax + by + cz} dV$,Equality of triple integrals over unit sphere, \iiint_{\text{unit ball}} x e^{ax + by + cz} dV,"I have to calculate $$ \iiint_{\text{unit ball}} x e^{ax + by + cz} \,dV,$$ where by ""unit ball"" I mean the region $x^2 + y^2 + z^2 \leq 1$ . I know how to calculate this (rotation matrix that takes $(a,b,c)$ to $(0,0,\sqrt{a^2 + b^2 + c^2})$ and then spherical coordinates). The answer gives $$\frac{4\pi a}{r^5}((3 + r^2)\sinh(r) - 3r \cosh(r)), \,\,\,\,\,r = \sqrt{a^2 + b^2 + c^2}$$ Now this would suggest that we have the following equality: $$ \iiint_{\text{unit ball}} \frac{x}{a} e^{ax + by + cz} \,dV = \iiint_{\text{unit ball}} \frac{y}{b} e^{ax + by + cz} \,dV = \iiint_{\text{unit ball}} \frac{z}{c} e^{ax + by + cz} \,dV$$ Is it true? How would one quickly prove it if it is? It would be relevant because if one could quickly spot and prove this, then by naming the value of the above integrals $I$ , we would have $$(a^2 + b^2 + c^2)I = \iiint_{\text{unit ball}} (ax+by+cz) e^{ax + by + cz} \,dV$$ This integral would be much easier to calculate than the first since it wouldn't require to calculate the rotation explicitly (just considering an arbitrary rotation that orients $(a,b,c)$ along the $z$ -axis suffices.) Any ideas?","I have to calculate where by ""unit ball"" I mean the region . I know how to calculate this (rotation matrix that takes to and then spherical coordinates). The answer gives Now this would suggest that we have the following equality: Is it true? How would one quickly prove it if it is? It would be relevant because if one could quickly spot and prove this, then by naming the value of the above integrals , we would have This integral would be much easier to calculate than the first since it wouldn't require to calculate the rotation explicitly (just considering an arbitrary rotation that orients along the -axis suffices.) Any ideas?"," \iiint_{\text{unit ball}} x e^{ax + by + cz} \,dV, x^2 + y^2 + z^2 \leq 1 (a,b,c) (0,0,\sqrt{a^2 + b^2 + c^2}) \frac{4\pi a}{r^5}((3 + r^2)\sinh(r) - 3r \cosh(r)), \,\,\,\,\,r = \sqrt{a^2 + b^2 + c^2}  \iiint_{\text{unit ball}} \frac{x}{a} e^{ax + by + cz} \,dV = \iiint_{\text{unit ball}} \frac{y}{b} e^{ax + by + cz} \,dV = \iiint_{\text{unit ball}} \frac{z}{c} e^{ax + by + cz} \,dV I (a^2 + b^2 + c^2)I = \iiint_{\text{unit ball}} (ax+by+cz) e^{ax + by + cz} \,dV (a,b,c) z","['integration', 'multivariable-calculus', 'volume', 'multiple-integral']"
85,Is this definition of the gradient using the exterior derivative consistent with calculus 1?,Is this definition of the gradient using the exterior derivative consistent with calculus 1?,,"I'm trying to get a better grasp on what the exterior derivative means on my own and I'm trying to connect the language of forms to my pre-existing knowledge. I came along the following formula for the gradient on wikipedia : $$\text{grad}f=\nabla f=(df)^\sharp$$ where $d$ is the exterior derivative and $^\sharp$ is a musical isomorphism. Since $(\omega_ie^i)^\sharp=\omega^ie_i$ this gives the following definition in component form $$\nabla f=e_i\partial^if=(\partial^if)\frac{\partial}{\partial  x^i}.$$ From my calculus classes I remember the gradient being defined as a vector which is consistent with this formula. But what confuses me is that the vector component have raised indices . In Cartesian coordinates you wouldn't notice this but in polar coordinates, or any other coordinates for that matter, you would notice a difference. But again when I recall my calculus classes I remember always calculating the ordinary partial derivatives which correspond to lower indices $\partial_i f$ . So is this definition consistent with elementary calculus intuition? So my elementary calculus definition would perhaps be $(\partial_i f)e_i$ even though I know that wouldn't make sense.","I'm trying to get a better grasp on what the exterior derivative means on my own and I'm trying to connect the language of forms to my pre-existing knowledge. I came along the following formula for the gradient on wikipedia : where is the exterior derivative and is a musical isomorphism. Since this gives the following definition in component form From my calculus classes I remember the gradient being defined as a vector which is consistent with this formula. But what confuses me is that the vector component have raised indices . In Cartesian coordinates you wouldn't notice this but in polar coordinates, or any other coordinates for that matter, you would notice a difference. But again when I recall my calculus classes I remember always calculating the ordinary partial derivatives which correspond to lower indices . So is this definition consistent with elementary calculus intuition? So my elementary calculus definition would perhaps be even though I know that wouldn't make sense.",\text{grad}f=\nabla f=(df)^\sharp d ^\sharp (\omega_ie^i)^\sharp=\omega^ie_i \nabla f=e_i\partial^if=(\partial^if)\frac{\partial}{\partial  x^i}. \partial_i f (\partial_i f)e_i,"['multivariable-calculus', 'vector-analysis', 'differential-forms', 'differential']"
86,"Which book is better for learning calculus Stewart, Larson or Thomas","Which book is better for learning calculus Stewart, Larson or Thomas",,"I want to master calculus in every possible way, I'm working in my bases like algebra and trigonometry (Precalculus) since I haven't had a good start in calculus, I want to read books like Calculus by Spivak, Calculus by Apostol and Courant books from Calculus and analysis. I want to know which books of calculus those 3 authors (Stewart, Larsom, Thomas) could help me to make a good aproach to calculus, if they are any substantial differences, if you think they are others best books please tell me","I want to master calculus in every possible way, I'm working in my bases like algebra and trigonometry (Precalculus) since I haven't had a good start in calculus, I want to read books like Calculus by Spivak, Calculus by Apostol and Courant books from Calculus and analysis. I want to know which books of calculus those 3 authors (Stewart, Larsom, Thomas) could help me to make a good aproach to calculus, if they are any substantial differences, if you think they are others best books please tell me",,"['real-analysis', 'calculus', 'algebra-precalculus', 'multivariable-calculus', 'book-recommendation']"
87,An attempt to revisit the criticality of the Navier-Stokes Equation,An attempt to revisit the criticality of the Navier-Stokes Equation,,"I am looking for feedback on this attempt to revisit the criticality of the Navier-Stokes equation (NSE) for incompressible fluids in 3 dimensions. It has been said, that the NSE is supercritical (see  T. Tao, Why proving global regularity conjecture for Navier-Stokes is hard (terrytao.wordpress.com, March 18, 2007)), because the rescaling that preserves the equation is $$\vec{u} \rightarrow \frac{1}{k}\vec{u}$$ while the energy is an integral of $|\vec{u}|^2$ and thus the rescaling that preserves the energy is given by $$\vec{u} \rightarrow \frac{1}{k^{3/2}} \vec{u}$$ thereby showing that for the NSE in 3 dimensions energy is being rescaled more severely; this amplifies the rescaling at small scales $k\ll1$ . Hence, the NSE appears to be energy-supercritical. The attempt here is to show that in this reasoning, the assumption is only true in part; becaise the rescaling that preserves the NSE is not $\vec{u} \rightarrow \frac{1}{k}\vec{u}$ , but it is actually $$\vec{u}^\prime = k^{\alpha_x-\alpha_t}\vec{u},$$ where $$ \begin{split} (x,y,z)^\prime &= k^{\alpha_x}(x,y,z)\\ t^\prime & = k^{\alpha_t}t \end{split} $$ and these are identical with the above rescaling laws only when $\alpha_x=1$ , $\alpha_t=2$ but not in general. This fact was discovered only recently, see A. Ercan and M. L. Kavvas, Chaos 25, 123126 (2015). But this changes everything, because to keep the NSE invariant under scaling transformation, the velocity and energy transform as $$ \begin{split} \vec{u}^\prime & = k^{\alpha_x-\alpha_t}\vec{u} \\ E^\prime & = k^{5\alpha_x-2\alpha_t} E \end{split} $$ and the energy $E$ at a given moment $t$ , would be scale-invariant if $\vec{u}$ transformed according to the law $$\vec{u}^\prime = k^{-\frac{3}{2}\alpha_x}\vec{u}.$$ Then, appllying T. Tao's definitions of critical, subcritical and supercritical (see T. Tao, Current developments in mathematics 2006 , 255(2008))  we find that the NSE is energy-subcritical, when $\frac{\alpha_t}{\alpha_x}>\frac{5}{2}$ , energy-critical, when $\frac{\alpha_t}{\alpha_x}=\frac{5}{2}$ , energy-supercritical, when $\frac{\alpha_t}{\alpha_x}<\frac{5}{2}$ . Do you find this reasoning correct/credible? NSE criticality is quite important and relates to the smoothness of solutions Your kind thoughts and suggestions are welcome. It is better to have them put together as an answer, pointing mistakes or showing an alternative explanations, rather than having extended discussion/comments Thank you","I am looking for feedback on this attempt to revisit the criticality of the Navier-Stokes equation (NSE) for incompressible fluids in 3 dimensions. It has been said, that the NSE is supercritical (see  T. Tao, Why proving global regularity conjecture for Navier-Stokes is hard (terrytao.wordpress.com, March 18, 2007)), because the rescaling that preserves the equation is while the energy is an integral of and thus the rescaling that preserves the energy is given by thereby showing that for the NSE in 3 dimensions energy is being rescaled more severely; this amplifies the rescaling at small scales . Hence, the NSE appears to be energy-supercritical. The attempt here is to show that in this reasoning, the assumption is only true in part; becaise the rescaling that preserves the NSE is not , but it is actually where and these are identical with the above rescaling laws only when , but not in general. This fact was discovered only recently, see A. Ercan and M. L. Kavvas, Chaos 25, 123126 (2015). But this changes everything, because to keep the NSE invariant under scaling transformation, the velocity and energy transform as and the energy at a given moment , would be scale-invariant if transformed according to the law Then, appllying T. Tao's definitions of critical, subcritical and supercritical (see T. Tao, Current developments in mathematics 2006 , 255(2008))  we find that the NSE is energy-subcritical, when , energy-critical, when , energy-supercritical, when . Do you find this reasoning correct/credible? NSE criticality is quite important and relates to the smoothness of solutions Your kind thoughts and suggestions are welcome. It is better to have them put together as an answer, pointing mistakes or showing an alternative explanations, rather than having extended discussion/comments Thank you","\vec{u} \rightarrow \frac{1}{k}\vec{u} |\vec{u}|^2 \vec{u} \rightarrow \frac{1}{k^{3/2}} \vec{u} k\ll1 \vec{u} \rightarrow \frac{1}{k}\vec{u} \vec{u}^\prime = k^{\alpha_x-\alpha_t}\vec{u}, 
\begin{split}
(x,y,z)^\prime &= k^{\alpha_x}(x,y,z)\\
t^\prime & = k^{\alpha_t}t
\end{split}
 \alpha_x=1 \alpha_t=2 
\begin{split}
\vec{u}^\prime & = k^{\alpha_x-\alpha_t}\vec{u} \\
E^\prime & = k^{5\alpha_x-2\alpha_t} E
\end{split}
 E t \vec{u} \vec{u}^\prime = k^{-\frac{3}{2}\alpha_x}\vec{u}. \frac{\alpha_t}{\alpha_x}>\frac{5}{2} \frac{\alpha_t}{\alpha_x}=\frac{5}{2} \frac{\alpha_t}{\alpha_x}<\frac{5}{2}","['multivariable-calculus', 'partial-differential-equations', 'symmetry', 'fluid-dynamics']"
88,What does commutativity of partial derivatives imply about geometry about the surface of a function?,What does commutativity of partial derivatives imply about geometry about the surface of a function?,,"Suppose we have a function $f(x,y)$ and it has the property that: $$  \partial_x \partial_y f(x,y) = \partial_y \partial_x f(x,y)$$ What does this imply about the geometry of the surface described the function?",Suppose we have a function and it has the property that: What does this imply about the geometry of the surface described the function?,"f(x,y)   \partial_x \partial_y f(x,y) = \partial_y \partial_x f(x,y)",['multivariable-calculus']
89,Vector normal to the gradient,Vector normal to the gradient,,"I know that the gradient points in the direction of the maximum rate of increase, with the maximum rate given by its magnitude.  Similarly, the maximum rate of decrease is given by the negative of its magnitude.  Does a vector normal to the gradient point in the direction of zero increase?  I.e., if I were standing on a level curve and traveled in a direction normal to the gradient at that point, will I land on a level curve with the same value as when I began?","I know that the gradient points in the direction of the maximum rate of increase, with the maximum rate given by its magnitude.  Similarly, the maximum rate of decrease is given by the negative of its magnitude.  Does a vector normal to the gradient point in the direction of zero increase?  I.e., if I were standing on a level curve and traveled in a direction normal to the gradient at that point, will I land on a level curve with the same value as when I began?",,"['multivariable-calculus', 'derivatives']"
90,Understanding Leibniz rule in the framework of Banach space.,Understanding Leibniz rule in the framework of Banach space.,,"Let $E,F_1,F_2,G$ be Banach spaces and $U \subseteq E$ be an open subset. Let $f_j : U \longrightarrow F_j$ be $C^r$ maps and $B : F_1 \times F_2 \longrightarrow G$ be a bounded bilinear map. Then $g : x \mapsto B(f_1(x),f_2(x))$ is $C^r.$ Also for $k=1,2,\cdots,r$ we have $$D^kg(x) (h_1,h_2,\cdots,h_k) = \sum\limits_{j=0}^{k} \binom {k} {j} B\left (D^jf_1(x) (h_1,h_2,\cdots,h_j) ,D^{k-j}f_2(x) (h_{j+1},h_{j+2}, \cdots, h_k) \right ).$$ I have managed to prove that $g$ is $C^1$ and for any $h \in E$ we have $$Dg(x) (h) = B \left (f_1(x),Df_2(x)(h) \right ) + B \left (Df_1(x)(h),f_2(x) \right ).$$ Now how do I prove that $g$ is $C^k$ for $k \gt 1\ $ ? How do I prove first that $Dg \in C^1\ $ ? Here $Dg : U \longrightarrow \mathcal L(E,G).$ Can $Dg$ be written as a composition of Frechet differentiable functions? Can anybody please give me some suggestion? Then I will try to prove it for any $k \gt 1.$ Thanks for your time. EDIT $:$ What I get is that $$D^2 g(x) (h_1,h_2) = B \left (f_1(x),D^2f_2 (x) (h_1,h_2) \right ) + B \left (Df_1(x)(h_1), Df_2 (x) (h_2) \right ) + B \left (Df_1(x) (h_2), Df_2 (x) (h_1) \right ) + B\left (D^2f_1(x) (h_1,h_2),f_2(x) \right).$$ But I don't know why it has been claimed in the proposition that $$B \left (Df_1(x)(h_1), Df_2 (x) (h_2) \right ) = B \left (Df_1(x) (h_2), Df_2 (x) (h_1) \right ).$$",Let be Banach spaces and be an open subset. Let be maps and be a bounded bilinear map. Then is Also for we have I have managed to prove that is and for any we have Now how do I prove that is for ? How do I prove first that ? Here Can be written as a composition of Frechet differentiable functions? Can anybody please give me some suggestion? Then I will try to prove it for any Thanks for your time. EDIT What I get is that But I don't know why it has been claimed in the proposition that,"E,F_1,F_2,G U \subseteq E f_j : U \longrightarrow F_j C^r B : F_1 \times F_2 \longrightarrow G g : x \mapsto B(f_1(x),f_2(x)) C^r. k=1,2,\cdots,r D^kg(x) (h_1,h_2,\cdots,h_k) = \sum\limits_{j=0}^{k} \binom {k} {j} B\left (D^jf_1(x) (h_1,h_2,\cdots,h_j) ,D^{k-j}f_2(x) (h_{j+1},h_{j+2}, \cdots, h_k) \right ). g C^1 h \in E Dg(x) (h) = B \left (f_1(x),Df_2(x)(h) \right ) + B \left (Df_1(x)(h),f_2(x) \right ). g C^k k \gt 1\  Dg \in C^1\  Dg : U \longrightarrow \mathcal L(E,G). Dg k \gt 1. : D^2 g(x) (h_1,h_2) = B \left (f_1(x),D^2f_2 (x) (h_1,h_2) \right ) + B \left (Df_1(x)(h_1), Df_2 (x) (h_2) \right ) + B \left (Df_1(x) (h_2), Df_2 (x) (h_1) \right ) + B\left (D^2f_1(x) (h_1,h_2),f_2(x) \right). B \left (Df_1(x)(h_1), Df_2 (x) (h_2) \right ) = B \left (Df_1(x) (h_2), Df_2 (x) (h_1) \right ).","['functional-analysis', 'multivariable-calculus', 'banach-spaces', 'frechet-derivative']"
91,Conservative force: path-independent integration implies force equal to gradient,Conservative force: path-independent integration implies force equal to gradient,,"Following the wikipedia page of a Conservative force , I want to prove that given a force such that for any closed path $C$ we have $$ \oint_C \mathbf{F}\cdot \mathbf{r}=0 $$ there is a potential $U(\mathbf{r})$ such that $$ \mathbf{F}=-\nabla U(\mathbf{r}). $$ Proof. The webpage start by defining $$ U(\mathbf{r}):=-\int_{c}\mathbf{F}\cdot d \mathbf{r} $$ and conclude that the claim follows from the fundamental theorem of calculus. I want to deepen the last step because the derivation is far from obvious for me. First I adapt the definition of potential by choosing some zero potential point $\mathbf{x}_0$ : $$ U(\mathbf{r}):=-\int_{\mathbf{x}_0}^\mathbf{x}\mathbf{F}\cdot d \mathbf{r} $$ then I introduce a parametrisation of the path $$ \mathbf{s}(u),\quad u\in[0,1] $$ Thus we get $$ -\nabla \left( -\int_{c}\mathbf{F}\cdot d \mathbf{r} \right) =\nabla\int_0^1 \left( F_x\frac{dx}{du} +  F_y\frac{dy}{du}+ F_z\frac{dz}{du}\right)du $$ For the $x$ component, intuitively but with a lot of errors, we must have something like this $$ \frac{d}{dx}\int_0^1 \left( F_x\frac{dx}{du} +  F_y\frac{dy}{du}+ F_z\frac{dz}{du}\right)du =\frac{du}{dx}\frac{d}{du}\int_0^1 \left( F_x\frac{dx}{du} +  F_y\frac{dy}{du}+ F_z\frac{dz}{du}\right)du\\ =\frac{du}{dx}\left( F_x\frac{dx}{du} +  F_y\frac{dy}{du}+ F_z\frac{dz}{du}\right)_{\mathbf{x}_0}^\mathbf{x}=\cdots=F_x $$ How does the derivation looks like using formal mathematics, but remaining in calculus? EDIT My solution going back to the definition of derivative:","Following the wikipedia page of a Conservative force , I want to prove that given a force such that for any closed path we have there is a potential such that Proof. The webpage start by defining and conclude that the claim follows from the fundamental theorem of calculus. I want to deepen the last step because the derivation is far from obvious for me. First I adapt the definition of potential by choosing some zero potential point : then I introduce a parametrisation of the path Thus we get For the component, intuitively but with a lot of errors, we must have something like this How does the derivation looks like using formal mathematics, but remaining in calculus? EDIT My solution going back to the definition of derivative:","C 
\oint_C \mathbf{F}\cdot \mathbf{r}=0
 U(\mathbf{r}) 
\mathbf{F}=-\nabla U(\mathbf{r}).
 
U(\mathbf{r}):=-\int_{c}\mathbf{F}\cdot d \mathbf{r}
 \mathbf{x}_0 
U(\mathbf{r}):=-\int_{\mathbf{x}_0}^\mathbf{x}\mathbf{F}\cdot d \mathbf{r}
 
\mathbf{s}(u),\quad u\in[0,1]
 
-\nabla \left( -\int_{c}\mathbf{F}\cdot d \mathbf{r} \right)
=\nabla\int_0^1 \left( F_x\frac{dx}{du} +  F_y\frac{dy}{du}+ F_z\frac{dz}{du}\right)du
 x 
\frac{d}{dx}\int_0^1 \left( F_x\frac{dx}{du} +  F_y\frac{dy}{du}+ F_z\frac{dz}{du}\right)du
=\frac{du}{dx}\frac{d}{du}\int_0^1 \left( F_x\frac{dx}{du} +  F_y\frac{dy}{du}+ F_z\frac{dz}{du}\right)du\\
=\frac{du}{dx}\left( F_x\frac{dx}{du} +  F_y\frac{dy}{du}+ F_z\frac{dz}{du}\right)_{\mathbf{x}_0}^\mathbf{x}=\cdots=F_x
","['calculus', 'integration', 'multivariable-calculus', 'vector-analysis']"
92,Help with the gradient in different co-ordinate systems,Help with the gradient in different co-ordinate systems,,"Let $L(x,y)$ be the linear Taylor series expansion of some function $f(x,y)$ . This can be written as $$L(x,y)=f(x_0,y_0)+f_x(x-x_0)+f_y(y-y_0)$$ Or in more compact form as $$L(x,y)=f(x_0,y_0)+\nabla f ^T \vec{p}$$ where $$ \vec{p}= \begin{bmatrix} x-x_0\\ y-y_0 \\ \end{bmatrix} $$ and $$\nabla f(x,y)=f_x \vec{i}+f_y\vec{j}$$ Now, we find out that $x'= \alpha_1 x$ and $y'= \alpha_2 y$ , which is the linear transformation $A$ of the original co-ordinates and we want to find the Taylor series expansion in terms of $x' \text{ and } y'$ , i.e. the new co-ordinate system. Writing the composite function $$g(x',y')=f(x(x'),y(x'))$$ $$L(x',y')=g(x'_0,y'_0)+\frac{\partial f}{\partial x}\frac{\partial x}{\partial x'}(x'-x'_0)+\frac{\partial f}{\partial y}\frac{\partial y}{\partial y'}(y'-y'_0)$$ $\implies$ . $$L(x',y')=f(x_0,y_0)+\frac{1}{\alpha_1}\frac{\partial f}{\partial x}(x'-x'_0)+\frac{1}{\alpha_2}\frac{\partial f}{\partial y}(y'-y'_0)$$ $\implies$ $$L(x',y')=f(x_0,y_0)+\vec{p}'^TA^{-T}\nabla f(x_0,y_0)$$ This function is describing exactly the same function but in the new stretched Cartesian co-ordinate system. By inspection the gradient in terms of the new co-ordinate system is $$\nabla g(x_0',y_0') = A^{-T}\nabla f(x_0,y_0)=g_{x'} \vec{i}'+g_{y'} \vec{j}'= \frac{1}{\alpha_1}f_x \vec{i}' + \frac{1}{\alpha_2}f_y \vec{j}'$$ Now because the Cartesian co-ordinate system has been stretched the basis vectors have changed, such that now $$\vec{i}'=\frac{\vec{i}}{\alpha_1}$$ and $$\vec{j}'=\frac{\vec{j}}{\alpha_2}$$ Substitution of these into $\nabla g(x_0',y_0')$ reveals that $$\nabla g(x_0',y_0')= \frac{1}{\alpha_1^2}f_x \vec{i} + \frac{1}{\alpha_2^2}f_y \vec{j}$$ which is different to the direction in the standard Cartesian co-ordinate system. On my travels to understand this I discovered Pavel Grinfeld's video series on Tensor algebra, where he shows that to make the gradient invariant under transformation I should use the definition $$\nabla f(x,y)=\frac{f_x}{\vec{i}\cdot\vec{i}}\vec{i}+\frac{f_x}{\vec{j}\cdot\vec{j}}\vec{j}$$ My background is in EEE, and frequently use steepest decent to solve numeric problems, I had never considered the transformation until I looked into conditioning the problem. I'm having a hard time understanding why the gradient derived from the transformed system is different to the original one in the first place. If anyone can shed any light on using the basis vectors $\frac{\vec{i}}{\vec{i}\cdot\vec{i}}$ and $\frac{\vec{j}}{\vec{j}\cdot\vec{j}}$ I would be very grateful. There seems to be a contradiction in here, or a subtle detail I a missing. I have seen several posts of people just saying the gradient is variant under transformation and to use the direction in the transformed co-ordinate system. My issue is, there shouldn't be any variation in the gradient, as per the geometric definition, it points in the direction of greatest increase and this doesn't change simply because co-ordinates are transformed. Example $$f(x,y)=2x+2y$$ $$\nabla f(x,y)=2\vec{i}+2\vec{j}$$ let $x'=2x$ and $y'=2y$ $$g(x',y')=x'+y'$$ $$\nabla g(x',y')=1\vec{i'}+1\vec{j'}$$ $\vec{i}=2 \vec{i}'$ and $\vec{j}=2 \vec{j}'$ and so this gradient in terms of the old basis vectors but in the new co-ordinate system is $$\nabla g(x',y')=0.5\vec{i}+0.5\vec{j}$$ so $\nabla g(x',y') \neq \nabla f(x,x) $ but $g(x',y') = f(x(x'),y(y')) $ [Gradient in two co-ordinate systems][1] I don't understand how the following would help, can anyone help me consolidate this? $$\nabla g(x',y')=\frac{1\vec{i'}}{\vec{i'} \cdot \vec{i'}}+\frac{1\vec{j'}}{\vec{j'} \cdot \vec{j'}}$$ [1]: https://i.sstatic.net/PArGk.png EDIT: Just to add, I found this thread on the same subject Definition of the gradient for non-Cartesian coordinates . They define the gradient geometrically as the directional derivative in the unit direction of steepest ascent, dotted with the unit direction of steepest ascent. That is: $$\nabla f = d(\vec{v}_{max})\vec{v}_{max}$$ Why am I getting two different gradients in two different co-ordinate systems? Why do these: http://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/14-newton-scribed.pdf lecture notes describe the gradient decent method as affine variant? I understand that the sensitivities will be different because different co-ordinate systems are used, but the basis vectors should compensate for this such that the gradient points in the same direction regardless of the system used.","Let be the linear Taylor series expansion of some function . This can be written as Or in more compact form as where and Now, we find out that and , which is the linear transformation of the original co-ordinates and we want to find the Taylor series expansion in terms of , i.e. the new co-ordinate system. Writing the composite function . This function is describing exactly the same function but in the new stretched Cartesian co-ordinate system. By inspection the gradient in terms of the new co-ordinate system is Now because the Cartesian co-ordinate system has been stretched the basis vectors have changed, such that now and Substitution of these into reveals that which is different to the direction in the standard Cartesian co-ordinate system. On my travels to understand this I discovered Pavel Grinfeld's video series on Tensor algebra, where he shows that to make the gradient invariant under transformation I should use the definition My background is in EEE, and frequently use steepest decent to solve numeric problems, I had never considered the transformation until I looked into conditioning the problem. I'm having a hard time understanding why the gradient derived from the transformed system is different to the original one in the first place. If anyone can shed any light on using the basis vectors and I would be very grateful. There seems to be a contradiction in here, or a subtle detail I a missing. I have seen several posts of people just saying the gradient is variant under transformation and to use the direction in the transformed co-ordinate system. My issue is, there shouldn't be any variation in the gradient, as per the geometric definition, it points in the direction of greatest increase and this doesn't change simply because co-ordinates are transformed. Example let and and and so this gradient in terms of the old basis vectors but in the new co-ordinate system is so but [Gradient in two co-ordinate systems][1] I don't understand how the following would help, can anyone help me consolidate this? [1]: https://i.sstatic.net/PArGk.png EDIT: Just to add, I found this thread on the same subject Definition of the gradient for non-Cartesian coordinates . They define the gradient geometrically as the directional derivative in the unit direction of steepest ascent, dotted with the unit direction of steepest ascent. That is: Why am I getting two different gradients in two different co-ordinate systems? Why do these: http://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/14-newton-scribed.pdf lecture notes describe the gradient decent method as affine variant? I understand that the sensitivities will be different because different co-ordinate systems are used, but the basis vectors should compensate for this such that the gradient points in the same direction regardless of the system used.","L(x,y) f(x,y) L(x,y)=f(x_0,y_0)+f_x(x-x_0)+f_y(y-y_0) L(x,y)=f(x_0,y_0)+\nabla f ^T \vec{p} 
\vec{p}=
\begin{bmatrix}
x-x_0\\
y-y_0 \\
\end{bmatrix}
 \nabla f(x,y)=f_x \vec{i}+f_y\vec{j} x'= \alpha_1 x y'= \alpha_2 y A x' \text{ and } y' g(x',y')=f(x(x'),y(x')) L(x',y')=g(x'_0,y'_0)+\frac{\partial f}{\partial x}\frac{\partial x}{\partial x'}(x'-x'_0)+\frac{\partial f}{\partial y}\frac{\partial y}{\partial y'}(y'-y'_0) \implies L(x',y')=f(x_0,y_0)+\frac{1}{\alpha_1}\frac{\partial f}{\partial x}(x'-x'_0)+\frac{1}{\alpha_2}\frac{\partial f}{\partial y}(y'-y'_0) \implies L(x',y')=f(x_0,y_0)+\vec{p}'^TA^{-T}\nabla f(x_0,y_0) \nabla g(x_0',y_0') = A^{-T}\nabla f(x_0,y_0)=g_{x'} \vec{i}'+g_{y'} \vec{j}'= \frac{1}{\alpha_1}f_x \vec{i}' + \frac{1}{\alpha_2}f_y \vec{j}' \vec{i}'=\frac{\vec{i}}{\alpha_1} \vec{j}'=\frac{\vec{j}}{\alpha_2} \nabla g(x_0',y_0') \nabla g(x_0',y_0')= \frac{1}{\alpha_1^2}f_x \vec{i} + \frac{1}{\alpha_2^2}f_y \vec{j} \nabla f(x,y)=\frac{f_x}{\vec{i}\cdot\vec{i}}\vec{i}+\frac{f_x}{\vec{j}\cdot\vec{j}}\vec{j} \frac{\vec{i}}{\vec{i}\cdot\vec{i}} \frac{\vec{j}}{\vec{j}\cdot\vec{j}} f(x,y)=2x+2y \nabla f(x,y)=2\vec{i}+2\vec{j} x'=2x y'=2y g(x',y')=x'+y' \nabla g(x',y')=1\vec{i'}+1\vec{j'} \vec{i}=2 \vec{i}' \vec{j}=2 \vec{j}' \nabla g(x',y')=0.5\vec{i}+0.5\vec{j} \nabla g(x',y') \neq \nabla f(x,x)  g(x',y') = f(x(x'),y(y'))  \nabla g(x',y')=\frac{1\vec{i'}}{\vec{i'} \cdot \vec{i'}}+\frac{1\vec{j'}}{\vec{j'} \cdot \vec{j'}} \nabla f = d(\vec{v}_{max})\vec{v}_{max}","['multivariable-calculus', 'vector-analysis', 'tensors']"
93,"Computing surface integral for $F(x,y,z) = (xy,-x^2,x+z)$",Computing surface integral for,"F(x,y,z) = (xy,-x^2,x+z)","Let $F: \mathbb{R}^3Â \to \mathbb{R}^3, F(x,y,z) = (xy,-x^2,x+z)$ be a vector field. Compute the surface integral over the set $S$ which is bounded by the plane $2x+2y+z=6$ in the set $\{(x,y,z) \in \mathbb{R}^3 : x,y,z\geqslant 0 \}.$ So I would need to calculate $$\iint_{T}f(\varphi(x,y)) \|\frac{\partial\varphi}{\partial x} \times \frac{\partial\varphi}{\partial y} \|  \ dx \ dy$$ Parameterizing $F$ as $\varphi(x,y) = (x,y, 6-2x-2y)$ I have that $$\frac{\partial\varphi}{\partial x}=(1,0,-2) \text{ and } \frac{\partial\varphi}{\partial y}=(0,1,-2)$$ so $\|\frac{\partial\varphi}{\partial x} \times \frac{\partial\varphi}{\partial y} \| = \| (2,2,1)\| = 3.$ Computing the composition $f\circ\varphi$ I get that $f(x,y,6-2x-2y)=(x^2y,-y^2,x+6-2x-2y)$ , thus the integral I would need to compute is $$\iint_{T} (x^2y,-y^2,x+6-2x-2y)\cdot 3 \ dx \ dy$$ but this seems wrong, this is just $f(\varphi(x,y)$ scaled by $3$ . What am I missing here? Any hints would be appreciated.","Let be a vector field. Compute the surface integral over the set which is bounded by the plane in the set So I would need to calculate Parameterizing as I have that so Computing the composition I get that , thus the integral I would need to compute is but this seems wrong, this is just scaled by . What am I missing here? Any hints would be appreciated.","F: \mathbb{R}^3Â \to \mathbb{R}^3, F(x,y,z) = (xy,-x^2,x+z) S 2x+2y+z=6 \{(x,y,z) \in \mathbb{R}^3 : x,y,z\geqslant 0 \}. \iint_{T}f(\varphi(x,y)) \|\frac{\partial\varphi}{\partial x} \times \frac{\partial\varphi}{\partial y} \|  \ dx \ dy F \varphi(x,y) = (x,y, 6-2x-2y) \frac{\partial\varphi}{\partial x}=(1,0,-2) \text{ and } \frac{\partial\varphi}{\partial y}=(0,1,-2) \|\frac{\partial\varphi}{\partial x} \times \frac{\partial\varphi}{\partial y} \| = \| (2,2,1)\| = 3. f\circ\varphi f(x,y,6-2x-2y)=(x^2y,-y^2,x+6-2x-2y) \iint_{T} (x^2y,-y^2,x+6-2x-2y)\cdot 3 \ dx \ dy f(\varphi(x,y) 3","['integration', 'multivariable-calculus']"
94,Deriving formula for curvature of a curve in $\mathbb{R}^n$,Deriving formula for curvature of a curve in,\mathbb{R}^n,I am trying to prove that given a parametric function of a regular curve in $\mathbb{R}^n$ the curvature of the curve for each $\gamma(t)$ is given by the following expression : $$k(t)=\frac{\sqrt{||\gammaâ(t)||^2||\gammaââ(t)||^2-(\gammaâ(t)\cdot\gammaââ(t))^2}}{||\gammaâ(t)||^3}$$ I have tried to use an arc-length parametrization of the curve but it leads me nowhere.,I am trying to prove that given a parametric function of a regular curve in the curvature of the curve for each is given by the following expression : I have tried to use an arc-length parametrization of the curve but it leads me nowhere.,\mathbb{R}^n \gamma(t) k(t)=\frac{\sqrt{||\gammaâ(t)||^2||\gammaââ(t)||^2-(\gammaâ(t)\cdot\gammaââ(t))^2}}{||\gammaâ(t)||^3},"['multivariable-calculus', 'parametric', 'curvature']"
95,How do I determine from a picture of a vector field if it's a possible formula for the vector field and conservative or not?,How do I determine from a picture of a vector field if it's a possible formula for the vector field and conservative or not?,,"I have an image here of a vector field $F(x,y)$ and am tasked to do the following things: True or false: : A possible formula for $F(x, y)$ is $F(x, y) = <ây, x>$ Is $F$ (the vector field in the picture) conservative? So the first one I really have no clue how to tell if that's a possible formula. The second one I think maybe it's conservative because it's circular so the curl = 0. But how do I actually know? How do I solve these problems with just a picture?",I have an image here of a vector field and am tasked to do the following things: True or false: : A possible formula for is Is (the vector field in the picture) conservative? So the first one I really have no clue how to tell if that's a possible formula. The second one I think maybe it's conservative because it's circular so the curl = 0. But how do I actually know? How do I solve these problems with just a picture?,"F(x,y) F(x, y) F(x, y) = <ây, x> F","['calculus', 'multivariable-calculus', 'vectors', 'vector-analysis', 'vector-fields']"
96,Differentiability Implies Continuity (Multivariable Calculus),Differentiability Implies Continuity (Multivariable Calculus),,"I am reading Hubbard and Hubbard's Vector Calculus, Linear Algebra, and Differential Forms, and in it they provide the following theorem and proof. My issue is not with the theorem, but more so with the proof. Note that, in the theorem below, Equation 1.7.20 refers to the basic definition of the (multivariable) derivative, and $L$ is the linear transformation which defines the derivative. In particular, they seem to implicitly be distributing the limit in order to make the desired conclusion. That is, they seem to be using the theorem below (bullet point 1). But this theorem of course requires the hypothesis that both $f$ and $g$ have the given limit. Translating to our case in the proof of Prop. 1.7.11, use of Theorem 1.5.26 in the proof would require a priori knowledge that $$ \lim_{\mathbf{h}\to \mathbf{0}}\mathbf{f}(\mathbf{a}+\mathbf{h})-\mathbf{f}(\mathbf{a})$$ exists (the second term in the limit of course exists, as justified). But how can we know this without resorting to the epsilon-delta definition of the limit? Indeed, that this limit exists (and is 0) is precisely what we are trying to prove. Thus, is this one of those instances where the authors intend for you to fill in the details, or is there really some way to deduce the result without resorting to epsilons and deltas (I imagine it's quite simple).","I am reading Hubbard and Hubbard's Vector Calculus, Linear Algebra, and Differential Forms, and in it they provide the following theorem and proof. My issue is not with the theorem, but more so with the proof. Note that, in the theorem below, Equation 1.7.20 refers to the basic definition of the (multivariable) derivative, and is the linear transformation which defines the derivative. In particular, they seem to implicitly be distributing the limit in order to make the desired conclusion. That is, they seem to be using the theorem below (bullet point 1). But this theorem of course requires the hypothesis that both and have the given limit. Translating to our case in the proof of Prop. 1.7.11, use of Theorem 1.5.26 in the proof would require a priori knowledge that exists (the second term in the limit of course exists, as justified). But how can we know this without resorting to the epsilon-delta definition of the limit? Indeed, that this limit exists (and is 0) is precisely what we are trying to prove. Thus, is this one of those instances where the authors intend for you to fill in the details, or is there really some way to deduce the result without resorting to epsilons and deltas (I imagine it's quite simple).",L f g  \lim_{\mathbf{h}\to \mathbf{0}}\mathbf{f}(\mathbf{a}+\mathbf{h})-\mathbf{f}(\mathbf{a}),"['multivariable-calculus', 'derivatives', 'continuity', 'proof-explanation']"
97,"If the plane $\frac{x}{a}+\frac{y}{b}+\frac{z}{c}=1$ intersects the axes at points $A,B,C$ then Area of Triangle $= \sqrt{b^2c^2+c^2a^2+a^2b^2}$",If the plane  intersects the axes at points  then Area of Triangle,"\frac{x}{a}+\frac{y}{b}+\frac{z}{c}=1 A,B,C = \sqrt{b^2c^2+c^2a^2+a^2b^2}","I'm working through this problem, Compute a surface area by integration to show that if the plane $\frac{x}{a}+\frac{y}{b}+\frac{z}{c}=1$ intersects the axes at points $A,B,C$ then Area of Triangle $= \sqrt{b^2c^2+c^2a^2+a^2b^2}$ and keep hitting a bump along the way I'm not sure how to overcome.. So far I have: Let $z=f(x,y)=c\left(1-\frac{x}{a}-\frac{y}{b}\right)$ and thus used: $$A(S)=\iint_{S}dS=\iint_{D}\sqrt{1+\left(\frac{\partial{z}}{\partial{x}}\right)^2+\left(\frac{\partial{z}}{\partial{y}}\right)^2}dA=\iint_{D}\sqrt{b^2c^2+c^2a^2+a^2b^2}dA$$ Now when considering D (the projection of $S$ onto the $xy$ plane, I have found a triangle with vertices $(0,0), (a,0), (0,b)$ . Thus, I had limits of integration as: $$0\leq x\leq a$$ $$0 \leq y \leq b\left(1-\frac{x}{a}\right)$$ My issue is that evaluating this I am obtaining $$\int_{0}^{a}\int_{0}^{b\left(1-\frac{x}{a}\right)}\sqrt{b^2c^2+c^2a^2+a^2b^2} dydx = \frac{ab}{2}\sqrt{b^2c^2+c^2a^2+a^2b^2}$$ I can see if $a=b=1$ then the desired result of $\frac{1}{2}\sqrt{b^2c^2+c^2a^2+a^2b^2}$ is obtained, but I'm not sure how to state my final result from this, or if I have made an error in the integral itself. Any help massively appreciated!","I'm working through this problem, Compute a surface area by integration to show that if the plane intersects the axes at points then Area of Triangle and keep hitting a bump along the way I'm not sure how to overcome.. So far I have: Let and thus used: Now when considering D (the projection of onto the plane, I have found a triangle with vertices . Thus, I had limits of integration as: My issue is that evaluating this I am obtaining I can see if then the desired result of is obtained, but I'm not sure how to state my final result from this, or if I have made an error in the integral itself. Any help massively appreciated!","\frac{x}{a}+\frac{y}{b}+\frac{z}{c}=1 A,B,C = \sqrt{b^2c^2+c^2a^2+a^2b^2} z=f(x,y)=c\left(1-\frac{x}{a}-\frac{y}{b}\right) A(S)=\iint_{S}dS=\iint_{D}\sqrt{1+\left(\frac{\partial{z}}{\partial{x}}\right)^2+\left(\frac{\partial{z}}{\partial{y}}\right)^2}dA=\iint_{D}\sqrt{b^2c^2+c^2a^2+a^2b^2}dA S xy (0,0), (a,0), (0,b) 0\leq x\leq a 0 \leq y \leq b\left(1-\frac{x}{a}\right) \int_{0}^{a}\int_{0}^{b\left(1-\frac{x}{a}\right)}\sqrt{b^2c^2+c^2a^2+a^2b^2} dydx = \frac{ab}{2}\sqrt{b^2c^2+c^2a^2+a^2b^2} a=b=1 \frac{1}{2}\sqrt{b^2c^2+c^2a^2+a^2b^2}","['calculus', 'integration', 'multivariable-calculus', 'area', 'multiple-integral']"
98,'Guessing' local extrema of a polynomial given its roots,'Guessing' local extrema of a polynomial given its roots,,"To start with let's assume that $p$ is a degree $n>1$ polynomial in $x$ and has $n$ distinct roots $\alpha_1, \ldots, \alpha_n$ . Without loss of generality we can also stipulate that $0 = \alpha_1 < \cdots < \alpha_n = 1$ . This guarantees there are $n-1$ local extrema, occurring at locations $x_1 \in (\alpha_1, \alpha_2)$ , $x_2 \in (\alpha_2, \alpha_3)$ , and so on. One might like to guess at the values of $x_i$ based on knowledge of the roots, and I'm curious about any heuristics that could do better than guessing the midpoint of each interval. Of course, if $n$ is small, then there exists an explicit formula; e.g. when $p(x)=(x-\alpha_1)(x-\alpha_2)$ then $x_1 = (\alpha_1 + \alpha_2)/2$ , the midpoint between the roots. But as early as $n=3$ things get murky; if $p(x)=(x-\alpha_1)(x-\alpha_2)(x-\alpha_3)$ , then $$x_{1,2}=\frac{\alpha_1+\alpha_2+\alpha_3}{3}\pm\frac{\sqrt{(\alpha_1+\alpha_2+\alpha_3)^2-3(\alpha_1\alpha_2+\alpha_1\alpha_3+\alpha_2\alpha_3)}}{3}$$ From this we see that the two extrema are centered around the mean of the roots. For example when $\alpha_2=1/2$ , they are at $1/2 \pm \sqrt{3}/6$ , so they're ""pushed out"" toward $0$ and $1$ (as opposed to being evenly distributed at $1/4$ and $3/4$ ). Based on this idea, I can imagine a heuristic that says the extremal values will be nearer some roots and further away from others, where ""nearer"" and ""further"" should be taken in a relative sense; in the simple example above they would be nearer $\alpha_1=0$ and $\alpha_3=1$ , and further from $\alpha_2 = 1/2$ . Once there are more roots, do the roles of the roots alternate? (a ""near"" root, then a ""far root"", then a ""near"" root again?). A more concrete question to ask is: suppose I just guess that the extrema are at the midpoints between each pair of roots, call them $m_1 = (\alpha_1+\alpha_2)/2$ , $m_2=(\alpha_2+\alpha_3)/2$ , and so on. Call the error term $E=\frac{1}{n-1}\sum|m_i - x_i|^2$ . How does $E$ depend on the roots of $p$ ? Is it monotonic with $n$ in some sense? This is an idle curiosity; I'm just trying to dream up interesting Calc I problems and found something that is a little too interesting.","To start with let's assume that is a degree polynomial in and has distinct roots . Without loss of generality we can also stipulate that . This guarantees there are local extrema, occurring at locations , , and so on. One might like to guess at the values of based on knowledge of the roots, and I'm curious about any heuristics that could do better than guessing the midpoint of each interval. Of course, if is small, then there exists an explicit formula; e.g. when then , the midpoint between the roots. But as early as things get murky; if , then From this we see that the two extrema are centered around the mean of the roots. For example when , they are at , so they're ""pushed out"" toward and (as opposed to being evenly distributed at and ). Based on this idea, I can imagine a heuristic that says the extremal values will be nearer some roots and further away from others, where ""nearer"" and ""further"" should be taken in a relative sense; in the simple example above they would be nearer and , and further from . Once there are more roots, do the roles of the roots alternate? (a ""near"" root, then a ""far root"", then a ""near"" root again?). A more concrete question to ask is: suppose I just guess that the extrema are at the midpoints between each pair of roots, call them , , and so on. Call the error term . How does depend on the roots of ? Is it monotonic with in some sense? This is an idle curiosity; I'm just trying to dream up interesting Calc I problems and found something that is a little too interesting.","p n>1 x n \alpha_1, \ldots, \alpha_n 0 = \alpha_1 < \cdots < \alpha_n = 1 n-1 x_1 \in (\alpha_1, \alpha_2) x_2 \in (\alpha_2, \alpha_3) x_i n p(x)=(x-\alpha_1)(x-\alpha_2) x_1 = (\alpha_1 + \alpha_2)/2 n=3 p(x)=(x-\alpha_1)(x-\alpha_2)(x-\alpha_3) x_{1,2}=\frac{\alpha_1+\alpha_2+\alpha_3}{3}\pm\frac{\sqrt{(\alpha_1+\alpha_2+\alpha_3)^2-3(\alpha_1\alpha_2+\alpha_1\alpha_3+\alpha_2\alpha_3)}}{3} \alpha_2=1/2 1/2 \pm \sqrt{3}/6 0 1 1/4 3/4 \alpha_1=0 \alpha_3=1 \alpha_2 = 1/2 m_1 = (\alpha_1+\alpha_2)/2 m_2=(\alpha_2+\alpha_3)/2 E=\frac{1}{n-1}\sum|m_i - x_i|^2 E p n","['calculus', 'multivariable-calculus', 'polynomials', 'optimization', 'maxima-minima']"
99,How to define a function that has these specific properties,How to define a function that has these specific properties,,"Suppose $x = (x_1,x_2,\dots,x_K) \in \mathbb{Z}^K_{\geq 0}$ . For $x,y \in \mathbb{Z}^K_{\geq 0}$ , we write $x \succ y$ or $y \prec x$ if $x \neq y$ and \begin{align*} 		x_{i(x,y)} > y_{i(x,y)},\quad \text{	where } \quad  i(x,y) := \max\{ i: x_i \neq y_i\}. 	\end{align*} That is, for any two vectors $x$ and $y$ that are not equal, we let $i(x,y)$ be the last position on which they differ and say that $x \succ y$ if the coordinate of $x$ at $i(x,y)$ is larger than the corresponding coordinate of $y$ . We write $x \succeq y$ if either $x = y$ or $x \succ y$ , and similarly for $x \preceq y$ . This is a total order. For example, if $x = (7,2,1,0,0)$ and $y = (6,3,1,0,0)$ then $y \succ x$ because they are equal on the last three positions and the next position that they differ is the second coordinate, since 3>2 we conclude that $y \succ x$ . Now, let $mx(x) = max\{k: x_k > 0\}$ , we are interested in defining a function $f: \mathbb{Z}^K_{\geq 0} \rightarrow [0,K+1)$ that has the following properties: $f(0,0,\ldots,0) = 0$ $mx(x) \leq f(x) < mx(x)+1$ (Note that when one of the coordinates of x is 1 and the rest are 0, then $f(x)= mx(x)$ , for example let $x = (0,1,0,0,0)$ , then $f(x)=mx(x)=2$ ) $f(.)$ is strictly increasing on $\mathbb{Z}^K_{\geq 0}$ wrt. the total-ordering defined above The effect of adding a positive value to coordinate $k$ should be smaller than adding the same value to coordinate $k+1,....,K$ , having all the other values fixed, sth like convexity property but I'm not sure if the exact definition of convexity applies here. For example suppose $K=5$ , $f(0,3,0,0,0) - f(0,2,0,0,0) \leq f(0,0,3,0,0) - f(0,0,2,0,0)$ I could define a function that has the first three properties, but not the fourth one: For any $x \in \mathbb{Z}^K_{\geq 0}$ ,  let $g_{k}(x) = \prod_{i=k}^{K} (1+i)^{-x_i}$ for $k=2,\dots,K$ and $g_{K+1}(x) = 1$ . \begin{align} 	f(x) := \sum_{k=2}^{K+1} k g_k(x) \big(1 - k^{-x_{k-1}}\big). \end{align} $f(0,3,0,0,0) - f(0,2,0,0,0) = 2.888889 - 2.666667 = 0.222222$ but $f(0,0,3,0,0) - f(0,0,2,0,0) = 3.9375 - 3.75 = 0.1875$ How to define $f(.)$ so that it follows all the 4 properties?","Suppose . For , we write or if and That is, for any two vectors and that are not equal, we let be the last position on which they differ and say that if the coordinate of at is larger than the corresponding coordinate of . We write if either or , and similarly for . This is a total order. For example, if and then because they are equal on the last three positions and the next position that they differ is the second coordinate, since 3>2 we conclude that . Now, let , we are interested in defining a function that has the following properties: (Note that when one of the coordinates of x is 1 and the rest are 0, then , for example let , then ) is strictly increasing on wrt. the total-ordering defined above The effect of adding a positive value to coordinate should be smaller than adding the same value to coordinate , having all the other values fixed, sth like convexity property but I'm not sure if the exact definition of convexity applies here. For example suppose , I could define a function that has the first three properties, but not the fourth one: For any ,  let for and . but How to define so that it follows all the 4 properties?","x = (x_1,x_2,\dots,x_K) \in \mathbb{Z}^K_{\geq 0} x,y \in \mathbb{Z}^K_{\geq 0} x \succ y y \prec x x \neq y \begin{align*}
		x_{i(x,y)} > y_{i(x,y)},\quad \text{	where } \quad  i(x,y) := \max\{ i: x_i \neq y_i\}.
	\end{align*} x y i(x,y) x \succ y x i(x,y) y x \succeq y x = y x \succ y x \preceq y x = (7,2,1,0,0) y = (6,3,1,0,0) y \succ x y \succ x mx(x) = max\{k: x_k > 0\} f: \mathbb{Z}^K_{\geq 0} \rightarrow [0,K+1) f(0,0,\ldots,0) = 0 mx(x) \leq f(x) < mx(x)+1 f(x)= mx(x) x = (0,1,0,0,0) f(x)=mx(x)=2 f(.) \mathbb{Z}^K_{\geq 0} k k+1,....,K K=5 f(0,3,0,0,0) - f(0,2,0,0,0) \leq f(0,0,3,0,0) - f(0,0,2,0,0) x \in \mathbb{Z}^K_{\geq 0} g_{k}(x) = \prod_{i=k}^{K} (1+i)^{-x_i} k=2,\dots,K g_{K+1}(x) = 1 \begin{align}
	f(x) := \sum_{k=2}^{K+1} k g_k(x) \big(1 - k^{-x_{k-1}}\big).
\end{align} f(0,3,0,0,0) - f(0,2,0,0,0) = 2.888889 - 2.666667 = 0.222222 f(0,0,3,0,0) - f(0,0,2,0,0) = 3.9375 - 3.75 = 0.1875 f(.)","['multivariable-calculus', 'functions', 'order-theory']"
