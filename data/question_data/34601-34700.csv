,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Verify my solutions to counting problems,Verify my solutions to counting problems,,"I'm pretty sure I know the answers to these problems, but still want to double check. How many different ways are there of arranging all the letters of the string CALCULUSBOOK ? Solution: $12!$ since we want to arrange all the letters. other wise it would be $\dfrac{12!}{2!2!2!2!}$. What is the coefficient of $x^5$ in the expansion of $(3x - 1)^{11}$? Solution: according to the binomial theorem the binomial coefficient would be $\dbinom{11}5$ since $(1-x)^k = \ldots+\dbinom{k}kx^k$ Please verify, correct or incorrect?","I'm pretty sure I know the answers to these problems, but still want to double check. How many different ways are there of arranging all the letters of the string CALCULUSBOOK ? Solution: $12!$ since we want to arrange all the letters. other wise it would be $\dfrac{12!}{2!2!2!2!}$. What is the coefficient of $x^5$ in the expansion of $(3x - 1)^{11}$? Solution: according to the binomial theorem the binomial coefficient would be $\dbinom{11}5$ since $(1-x)^k = \ldots+\dbinom{k}kx^k$ Please verify, correct or incorrect?",,"['probability', 'combinatorics', 'discrete-mathematics']"
1,Black and white balls in 2 boxes and probability that I pick the white ball.,Black and white balls in 2 boxes and probability that I pick the white ball.,,"Maybe there already is solution for that and if it is so, then maybe someone can tell me where I can find it. I have 2 boxes. In first box there are 3 white and 2 black balls. In second box there are 4 black and 4 white balls. Then I randomly pick one ball from first box and put it in second box. What is the probability that I pick white ball from the second box? I think that I need to calculate combinations for black balls and then for white balls (for second box after I put in random ball from first box) and then multiply them.","Maybe there already is solution for that and if it is so, then maybe someone can tell me where I can find it. I have 2 boxes. In first box there are 3 white and 2 black balls. In second box there are 4 black and 4 white balls. Then I randomly pick one ball from first box and put it in second box. What is the probability that I pick white ball from the second box? I think that I need to calculate combinations for black balls and then for white balls (for second box after I put in random ball from first box) and then multiply them.",,"['probability', 'combinatorics', 'discrete-mathematics']"
2,Expected value of $xx^{T}$ for multidimensional Gaussian,Expected value of  for multidimensional Gaussian,xx^{T},"I need a bit of help understanding a step in the derivation of the expected value of $\bf{x x^{T}}$, that is, $E[\bf{x x^{T}}]$ with a Gaussian distribution. By definition, using the D-dimensional Gaussian distribution, we have: $$E[{\bf{x x^{T}}}]=\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\Sigma|^{\frac{1}{2}}} \int \exp{\left\{ -\frac{1}{2}(\bf{x-\mu)^T\Sigma^{-1}(x-}\mu) \right\}}{\bf{x x^{T}}}d\bf{x}$$ We substitute $\bf{x-\mu}$ by $\bf{z}$: $$E[{\bf{x x^{T}}}]=\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\Sigma|^{\frac{1}{2}}} \int \exp{\left\{ -\frac{1}{2}{\bf{z}}^T\Sigma^{-1}{\bf{z}} \right\}}({\bf{z+\mu}})(\bf{z+\mu})^{T}d{\bf{z}}$$ From this equation, the part that is giving me trouble is the $\bf{zz^{T}}$ term: $$\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\Sigma|^{\frac{1}{2}}} \int \exp{\left\{ -\frac{1}{2}{\bf{z}}^T\Sigma^{-1}{\bf{z}} \right\}}{\bf{z}}\bf{z}^{T}d{\bf{z}}$$ The book says: Again, we can make use of the eigenvector expansion of the covariance matrix given by (2.45), together with the completeness of the set of eigenvectors, to write ${\bf{z}} = \sum_{j=1}^{D} y_{j}{\bf{u_{j}}}$, where $y_{j} = \bf{u_{j}^{T}z}$ The equation (2.45) is the eigenvector equation: $${\bf{\Sigma u_{i}}}=\lambda_{i}{\bf{u_{i}}}$$ I know that we can express $\bf{z}$ using the eigenvectors of $\bf{\Sigma}$ but the summation above doesn't make much sense to me. By the way, $y_{j}=\bf{u_{j}^{T}z}$ comes from the exponent in the Gaussian distribution and works as a change of coordinates (from $x$ to $y$), so at least I understand that part. Here is the relevant section in the book: And this is the page referenced in the quote above: So, can anyone explain that part to me? Thanks in advance","I need a bit of help understanding a step in the derivation of the expected value of $\bf{x x^{T}}$, that is, $E[\bf{x x^{T}}]$ with a Gaussian distribution. By definition, using the D-dimensional Gaussian distribution, we have: $$E[{\bf{x x^{T}}}]=\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\Sigma|^{\frac{1}{2}}} \int \exp{\left\{ -\frac{1}{2}(\bf{x-\mu)^T\Sigma^{-1}(x-}\mu) \right\}}{\bf{x x^{T}}}d\bf{x}$$ We substitute $\bf{x-\mu}$ by $\bf{z}$: $$E[{\bf{x x^{T}}}]=\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\Sigma|^{\frac{1}{2}}} \int \exp{\left\{ -\frac{1}{2}{\bf{z}}^T\Sigma^{-1}{\bf{z}} \right\}}({\bf{z+\mu}})(\bf{z+\mu})^{T}d{\bf{z}}$$ From this equation, the part that is giving me trouble is the $\bf{zz^{T}}$ term: $$\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\Sigma|^{\frac{1}{2}}} \int \exp{\left\{ -\frac{1}{2}{\bf{z}}^T\Sigma^{-1}{\bf{z}} \right\}}{\bf{z}}\bf{z}^{T}d{\bf{z}}$$ The book says: Again, we can make use of the eigenvector expansion of the covariance matrix given by (2.45), together with the completeness of the set of eigenvectors, to write ${\bf{z}} = \sum_{j=1}^{D} y_{j}{\bf{u_{j}}}$, where $y_{j} = \bf{u_{j}^{T}z}$ The equation (2.45) is the eigenvector equation: $${\bf{\Sigma u_{i}}}=\lambda_{i}{\bf{u_{i}}}$$ I know that we can express $\bf{z}$ using the eigenvectors of $\bf{\Sigma}$ but the summation above doesn't make much sense to me. By the way, $y_{j}=\bf{u_{j}^{T}z}$ comes from the exponent in the Gaussian distribution and works as a change of coordinates (from $x$ to $y$), so at least I understand that part. Here is the relevant section in the book: And this is the page referenced in the quote above: So, can anyone explain that part to me? Thanks in advance",,"['probability', 'statistics', 'normal-distribution']"
3,"If $P(A \cup B \cup C) = 1$, $P(B) = 2P(A) $, $P(C) = 3P(A) $, $P(A \cap B) = P(A \cap C) = P(B \cap C) $, then $P(A) \le \frac14$","If , , , , then",P(A \cup B \cup C) = 1 P(B) = 2P(A)  P(C) = 3P(A)  P(A \cap B) = P(A \cap C) = P(B \cap C)  P(A) \le \frac14,"We have ($P$ is probability): $P(A \cup B \cup C) = 1$ ; $P(B) = 2P(A) $ ; $P(C) = 3P(A) $ and $P(A \cap B) = P(A \cap C) = P(B \cap C) $. Prove that $P(A) \le \frac{1}{4} $. Well, I tried with the fact that $ 1 = P(A \cup B \cup C) = 6P(A) - 3P(A \cap B) + P(A \cap B \cap C) $ but I got stuck... Could anyone help me, please?","We have ($P$ is probability): $P(A \cup B \cup C) = 1$ ; $P(B) = 2P(A) $ ; $P(C) = 3P(A) $ and $P(A \cap B) = P(A \cap C) = P(B \cap C) $. Prove that $P(A) \le \frac{1}{4} $. Well, I tried with the fact that $ 1 = P(A \cup B \cup C) = 6P(A) - 3P(A \cap B) + P(A \cap B \cap C) $ but I got stuck... Could anyone help me, please?",,['probability']
4,probability that 5 square lie along a diagonal line,probability that 5 square lie along a diagonal line,,"If $5$ squares are chosen at random from a chess board, what is the probability that they lie on a diagonal line?","If $5$ squares are chosen at random from a chess board, what is the probability that they lie on a diagonal line?",,['probability']
5,What's the difference between expected values in binomial distributions and hypergeometric distributions?,What's the difference between expected values in binomial distributions and hypergeometric distributions?,,"The formula for the expected value in a binomial distribution is: $$E(X) = nP(s)$$ where $n$ is the number of trials and $P(s)$ is the probability of success. The formula for the expected value in a hypergeometric distribution is: $$E(X) = \frac{ns}{N}$$ where $N$ is the population size, $s$ is the number of successes available in the population and $n$ is the number of trials. $$E(x) = \left( \frac{s}{N} \right)n $$ $$P(s) = \frac{s}{N}$$ $$\implies E(x) = nP(s)$$ Why do both the distributions have the same expected value? Why doesn't the independence of the events have any effect on expected value?","The formula for the expected value in a binomial distribution is: $$E(X) = nP(s)$$ where $n$ is the number of trials and $P(s)$ is the probability of success. The formula for the expected value in a hypergeometric distribution is: $$E(X) = \frac{ns}{N}$$ where $N$ is the population size, $s$ is the number of successes available in the population and $n$ is the number of trials. $$E(x) = \left( \frac{s}{N} \right)n $$ $$P(s) = \frac{s}{N}$$ $$\implies E(x) = nP(s)$$ Why do both the distributions have the same expected value? Why doesn't the independence of the events have any effect on expected value?",,"['probability', 'probability-theory', 'probability-distributions', 'stochastic-processes']"
6,Probability of the correct answer,Probability of the correct answer,,"I was learning for the test, when I spotted problem that I cannot deal with: We have an algorithm that gives correct answer with a probability $p>\frac{1}{2}$. For simplicity assume that the answer is an integer. To increase the probability of obtaining the correct result we execute this algorithm $n$ times and we take the median of the results. Estimate the probability of getting the correct answer. Maybe Markov's or Chebyshev's inequality will be useful? But I don't know how to approach taking this median. Can anyone help?","I was learning for the test, when I spotted problem that I cannot deal with: We have an algorithm that gives correct answer with a probability $p>\frac{1}{2}$. For simplicity assume that the answer is an integer. To increase the probability of obtaining the correct result we execute this algorithm $n$ times and we take the median of the results. Estimate the probability of getting the correct answer. Maybe Markov's or Chebyshev's inequality will be useful? But I don't know how to approach taking this median. Can anyone help?",,['probability']
7,"$Y \sim \mathrm{Uniform}(0,1)$, $[X|Y=y] \sim \mathrm{Uniform}(0,1/y).$ Find $f_X(x)$",",  Find","Y \sim \mathrm{Uniform}(0,1) [X|Y=y] \sim \mathrm{Uniform}(0,1/y). f_X(x)","This was a quiz question that I answered but don't understand something that is probably very important. Question: Let $Y \sim \mathrm{Uniform}(0,1)$ and $[X|Y=y] \sim \mathrm{Uniform}(0,1/y).$ Find $f_X(x)$. Solution (Attempt): We have $$f_X(x|y) = y$$ for $y \in (0,1)$ and $x \in (0,1/y)$. Then, $$f_X(x) = \int_{-\infty}^\infty f_X(x|y) \cdot f_Y(y) \mathrm{d}y = \int_0^1 y \mathrm{d}y = \frac{1}{2}.$$ From here I argued that we need $$\int_{-\infty}^\infty f_X(x) \mathrm{d}x = 1$$ so $X \sim \mathrm{Uniform}(0,2)$ will satisfy this condition. However, my professor has noted that $$f_X(x) = \frac{1}{2}$$ for $x \in [0,1]$ but I did not cover the case of $x > 1$ in which she then goes on to do the following: $$f_X(x) = \int_{0}^{ 1/x  } y \mathrm{d}y = \frac{1}{2x^2}$$ for $x > 1$. Trying to understand this, it makes me think that the former case that I did, was just for the case of $y = 1$ and then the part that professor did was for $y \in (0,1).$ But that isn't making that much sense as it seems then the integral I computed would have just been $1$. I am guessing something is happening where $X$ is distributed on some interval $(0,1/y)$ which ends at $1/y$ which varies and could be a large number.","This was a quiz question that I answered but don't understand something that is probably very important. Question: Let $Y \sim \mathrm{Uniform}(0,1)$ and $[X|Y=y] \sim \mathrm{Uniform}(0,1/y).$ Find $f_X(x)$. Solution (Attempt): We have $$f_X(x|y) = y$$ for $y \in (0,1)$ and $x \in (0,1/y)$. Then, $$f_X(x) = \int_{-\infty}^\infty f_X(x|y) \cdot f_Y(y) \mathrm{d}y = \int_0^1 y \mathrm{d}y = \frac{1}{2}.$$ From here I argued that we need $$\int_{-\infty}^\infty f_X(x) \mathrm{d}x = 1$$ so $X \sim \mathrm{Uniform}(0,2)$ will satisfy this condition. However, my professor has noted that $$f_X(x) = \frac{1}{2}$$ for $x \in [0,1]$ but I did not cover the case of $x > 1$ in which she then goes on to do the following: $$f_X(x) = \int_{0}^{ 1/x  } y \mathrm{d}y = \frac{1}{2x^2}$$ for $x > 1$. Trying to understand this, it makes me think that the former case that I did, was just for the case of $y = 1$ and then the part that professor did was for $y \in (0,1).$ But that isn't making that much sense as it seems then the integral I computed would have just been $1$. I am guessing something is happening where $X$ is distributed on some interval $(0,1/y)$ which ends at $1/y$ which varies and could be a large number.",,['probability']
8,Probability of one normdist being greater than another [duplicate],Probability of one normdist being greater than another [duplicate],,"This question already has an answer here : Probability of a point taken from a certain normal distribution will be greater than a point taken from another? (1 answer) Closed 9 years ago . I have two independant normally distributed random variables. X ~ N(657, 3) Y ~ N(661, 2)  P(x > y) = ? How do I calculate the probability of X being greater than Y? Using R for simulating, I am getting values around 0.13. How to get the exact (""theoretical"") value?","This question already has an answer here : Probability of a point taken from a certain normal distribution will be greater than a point taken from another? (1 answer) Closed 9 years ago . I have two independant normally distributed random variables. X ~ N(657, 3) Y ~ N(661, 2)  P(x > y) = ? How do I calculate the probability of X being greater than Y? Using R for simulating, I am getting values around 0.13. How to get the exact (""theoretical"") value?",,"['probability', 'probability-distributions', 'normal-distribution']"
9,Letter in the table with 8 trays,Letter in the table with 8 trays,,"Here is a problem: we have a table with 8 trays. With probability $0.5$, there is a letter somewhere in the table. What is the probability that there is a letter in a last tray, given that there is no letter in first 7 trays? It looks trivial, but now I'm really confused. Here is how I solved it: let $A$ be the probability that there is a letter in a table (so $P(A) = 0.5$); let $B_i$ be the probability that there is a letter in i-th tray; we need the probability $P(B_8 | \overline{B}_{1-7}) = P(B_8|A\overline{B}_{1-7}) * P(A) + P(B_8|\overline{A}\overline{B}_{1-7}) * P(\overline{A})$. $P(B_8|\overline{A}\overline{B}_{1-7})$ is $0$ (because there is no letter in the table at all), and $P(B_8|A\overline{B}_{1-7})$ is $1$, because we know that the letter is in the table and there is no letter in first 7 trays. So $1*0.5 + 0*0.5 = 0.5$. But then I've tried a simulation: #!/usr/bin/python  import random  test_n = 100000  table_empty = [False for x in xrange(8)]  suitable = 0 letter_in_last = 0 for i in xrange(test_n):     has_letter = random.random() >= 0.5     table = table_empty[:]     if has_letter:         table[random.randint(0,7)] = True      if any(table[:7]):         continue # there is a letter in 7 first trays     else:         suitable += 1         if table[-1]:             letter_in_last +=1  print ""made %s tests"" % test_n print ""%s haven't a letter in first 7 trays"" % suitable print ""%s have a letter in last tray"" % letter_in_last print ""probability: %s"" % (1.0 * letter_in_last / suitable) And results are like this: made 100000 tests 56291 haven't a letter in first 7 trays 6185 have a letter in last tray probability: 0.109875468547 Where is my mistake?","Here is a problem: we have a table with 8 trays. With probability $0.5$, there is a letter somewhere in the table. What is the probability that there is a letter in a last tray, given that there is no letter in first 7 trays? It looks trivial, but now I'm really confused. Here is how I solved it: let $A$ be the probability that there is a letter in a table (so $P(A) = 0.5$); let $B_i$ be the probability that there is a letter in i-th tray; we need the probability $P(B_8 | \overline{B}_{1-7}) = P(B_8|A\overline{B}_{1-7}) * P(A) + P(B_8|\overline{A}\overline{B}_{1-7}) * P(\overline{A})$. $P(B_8|\overline{A}\overline{B}_{1-7})$ is $0$ (because there is no letter in the table at all), and $P(B_8|A\overline{B}_{1-7})$ is $1$, because we know that the letter is in the table and there is no letter in first 7 trays. So $1*0.5 + 0*0.5 = 0.5$. But then I've tried a simulation: #!/usr/bin/python  import random  test_n = 100000  table_empty = [False for x in xrange(8)]  suitable = 0 letter_in_last = 0 for i in xrange(test_n):     has_letter = random.random() >= 0.5     table = table_empty[:]     if has_letter:         table[random.randint(0,7)] = True      if any(table[:7]):         continue # there is a letter in 7 first trays     else:         suitable += 1         if table[-1]:             letter_in_last +=1  print ""made %s tests"" % test_n print ""%s haven't a letter in first 7 trays"" % suitable print ""%s have a letter in last tray"" % letter_in_last print ""probability: %s"" % (1.0 * letter_in_last / suitable) And results are like this: made 100000 tests 56291 haven't a letter in first 7 trays 6185 have a letter in last tray probability: 0.109875468547 Where is my mistake?",,"['probability', 'simulation']"
10,Conditioning on zero probability event,Conditioning on zero probability event,,"I have a somewhat trivial question (no homework): Suppose $X_1, X_2$ are i.i.d. and uniform on $[0,1]$, and the realization of their maximum is $k \in (0,1]$. What is the conditional distribution of the other random variable? Does that make any sense? In words: given that the realization of the maximum of 2 random variables attains some value, what is the conditional distribution of the other random variable? In the example, is it uniform on $[0,k]$? Thanks!","I have a somewhat trivial question (no homework): Suppose $X_1, X_2$ are i.i.d. and uniform on $[0,1]$, and the realization of their maximum is $k \in (0,1]$. What is the conditional distribution of the other random variable? Does that make any sense? In words: given that the realization of the maximum of 2 random variables attains some value, what is the conditional distribution of the other random variable? In the example, is it uniform on $[0,k]$? Thanks!",,"['probability', 'probability-distributions']"
11,Probability of a person living at least $t$ years,Probability of a person living at least  years,t,Suppose the force of mortality of a person aged $x$ is $\mu_x = \frac{1}{200-x} + \frac{1}{100-x}$ where $x<100$. What is the probability that the person survives at least $t$ more years? So would this be $$\exp \left(-\int_{x}^{x+t} \frac{1}{200-s} \ ds- \int_{x}^{x+t} \frac{1}{100-s} \ ds \right)$$ $$ = \exp \left(\ln(200-s) |^{x+t}_{x} + \ln(100-s) |^{x+t}_{x} \right)$$,Suppose the force of mortality of a person aged $x$ is $\mu_x = \frac{1}{200-x} + \frac{1}{100-x}$ where $x<100$. What is the probability that the person survives at least $t$ more years? So would this be $$\exp \left(-\int_{x}^{x+t} \frac{1}{200-s} \ ds- \int_{x}^{x+t} \frac{1}{100-s} \ ds \right)$$ $$ = \exp \left(\ln(200-s) |^{x+t}_{x} + \ln(100-s) |^{x+t}_{x} \right)$$,,['probability']
12,Stopping time problem with random translations on the unit interval,Stopping time problem with random translations on the unit interval,,"You take a circle centered at the origin, spin it at random, and paint the part in the first quadrant. You repeat the spinning and painting until the circle is completely covered with paint. The problem is just to compute the average number of spins needed to do this. In other words, let $A$ be a Lebesgue measurable subset of the unit interval of positive measure, and let $\langle \theta_n:n\ge1\rangle$ be a random sequence of values independent and uniformly distributed in $[0,1]$, and define $T$ to be the least integer so that $\bigcup_{n=1}^T (A+\theta_n)\mod 1=[0,1]$. Is there a nice expression for the expected value of $T$ in terms of the measure of $A$? My progress on this problem has been limited to writing down a few hairy multiple integrals with piece-wise linear bounds to express the probability of covering the unit interval in a low number of spins. My suspicions are that this problem is well-known (i.e., except to me) and that there are elegant methods to solve these types of problems. Any direction toward a solution would be much appreciated.","You take a circle centered at the origin, spin it at random, and paint the part in the first quadrant. You repeat the spinning and painting until the circle is completely covered with paint. The problem is just to compute the average number of spins needed to do this. In other words, let $A$ be a Lebesgue measurable subset of the unit interval of positive measure, and let $\langle \theta_n:n\ge1\rangle$ be a random sequence of values independent and uniformly distributed in $[0,1]$, and define $T$ to be the least integer so that $\bigcup_{n=1}^T (A+\theta_n)\mod 1=[0,1]$. Is there a nice expression for the expected value of $T$ in terms of the measure of $A$? My progress on this problem has been limited to writing down a few hairy multiple integrals with piece-wise linear bounds to express the probability of covering the unit interval in a low number of spins. My suspicions are that this problem is well-known (i.e., except to me) and that there are elegant methods to solve these types of problems. Any direction toward a solution would be much appreciated.",,['probability']
13,Two players and two coins,Two players and two coins,,"Two players are playing a game. The first player has unlimited gold coins of 2 types, $C_1=2\$$ and $C_2=5\$$. Each turn he chooses one of these coins and hides it in his hand. If the second player guesses correctly which type of coin the first player is hiding in his hand, he gets this coin; otherwise he loses $x$ cents. Find the largest integer $x$ for which the game is beneficial to the second player. I knew the answer, but I forget how we got it. I would appreciate it if someone would explain it for me. Thank you. Answer is x=316","Two players are playing a game. The first player has unlimited gold coins of 2 types, $C_1=2\$$ and $C_2=5\$$. Each turn he chooses one of these coins and hides it in his hand. If the second player guesses correctly which type of coin the first player is hiding in his hand, he gets this coin; otherwise he loses $x$ cents. Find the largest integer $x$ for which the game is beneficial to the second player. I knew the answer, but I forget how we got it. I would appreciate it if someone would explain it for me. Thank you. Answer is x=316",,"['probability', 'game-theory']"
14,A question about probability,A question about probability,,"I have met a interesting question: If today rains, the probability that tomorrow rains is $0.6.$ If today doesn't rain, the probability that tomorrow rains is $0.2.$ Given Tuesday rained, what's the probability that Monday rained? I have no idea how to solve this. If I make the question a bit more complicated: Given Tuesday rain, what's the probability that the Sunday just before rained?","I have met a interesting question: If today rains, the probability that tomorrow rains is $0.6.$ If today doesn't rain, the probability that tomorrow rains is $0.2.$ Given Tuesday rained, what's the probability that Monday rained? I have no idea how to solve this. If I make the question a bit more complicated: Given Tuesday rain, what's the probability that the Sunday just before rained?",,['probability']
15,Probability; can't understand the maths,Probability; can't understand the maths,,"For a random variable $x$, define a probability distribution $p[x=n]=c (3^n/n!)$ when $x=0, 1, 2, \dots$ and $p(x)=0$ otherwise. Find the value of $c$. My professor provided the solution $$ \sum_{x=0}^\infty \ c\frac{3^n}{n!}=1 $$  so $c\;e^3 = 1$. I can't understand why the summation has value $1$.","For a random variable $x$, define a probability distribution $p[x=n]=c (3^n/n!)$ when $x=0, 1, 2, \dots$ and $p(x)=0$ otherwise. Find the value of $c$. My professor provided the solution $$ \sum_{x=0}^\infty \ c\frac{3^n}{n!}=1 $$  so $c\;e^3 = 1$. I can't understand why the summation has value $1$.",,['probability']
16,Approximating the logarithm of a Laplace transform,Approximating the logarithm of a Laplace transform,,"Suppose $X$ is a random variable on $\mathbb R_+$ with finite mean, i.e. $\mathbb E X <+\infty$. Let $F_X(t)$ be its c.d.f. and $\mathcal{L}_X(\cdot)$ its Laplace transform, i.e. $\mathcal{L}_X(s)=\int_0^\infty e^{-s t} d F(t)$ Can one conclude immediately that, as $s \to 0$, $\log \mathcal{L}_X(s) \approx -s \mathbb E X + o(s^2) $ ? If not, suppose now that $X$ has finite moments of all orders. Can one now conclude that, as $s \to 0$, $\log \mathcal{L}_X(s) \approx -s \mathbb E X + o(s^2) $ ?","Suppose $X$ is a random variable on $\mathbb R_+$ with finite mean, i.e. $\mathbb E X <+\infty$. Let $F_X(t)$ be its c.d.f. and $\mathcal{L}_X(\cdot)$ its Laplace transform, i.e. $\mathcal{L}_X(s)=\int_0^\infty e^{-s t} d F(t)$ Can one conclude immediately that, as $s \to 0$, $\log \mathcal{L}_X(s) \approx -s \mathbb E X + o(s^2) $ ? If not, suppose now that $X$ has finite moments of all orders. Can one now conclude that, as $s \to 0$, $\log \mathcal{L}_X(s) \approx -s \mathbb E X + o(s^2) $ ?",,"['probability', 'analysis', 'approximation', 'taylor-expansion', 'laplace-transform']"
17,What is the chance of repeating a random 19-digit alphanumeric string?,What is the chance of repeating a random 19-digit alphanumeric string?,,"On a site that I operate I use several layers of protection for our subscription content. Among the other protections, each content item is placed into a folder with a randomly generated 19-character name. That name can include numbers, lowercase letters, and uppercase letters. While it will not really lower the benefit provided by this layer of protection for a  second file to end up in a folder, I am curious about the chances of that happening. That leads me to two related questions: Assuming that we have 2500 files/folders, what is the chance that the next folder we generate will be a repeat? How many folders must exist for the chance of repetition to rise above 1%?","On a site that I operate I use several layers of protection for our subscription content. Among the other protections, each content item is placed into a folder with a randomly generated 19-character name. That name can include numbers, lowercase letters, and uppercase letters. While it will not really lower the benefit provided by this layer of protection for a  second file to end up in a folder, I am curious about the chances of that happening. That leads me to two related questions: Assuming that we have 2500 files/folders, what is the chance that the next folder we generate will be a repeat? How many folders must exist for the chance of repetition to rise above 1%?",,"['probability', 'combinatorics']"
18,How to interpret the little-$o$ notation in this definition of the Poisson process?,How to interpret the little- notation in this definition of the Poisson process?,o,"In the book Probability and Random Processes by Grimmett and Stirzaker, a Poisson process is defined to be any process $(N(t))_{t\in[0,\infty)}$ with values in $\mathbb{N}_0$ satisfying following three properties: $\hspace{20pt}$(a) $N(0) = 0$ and $s<t\Rightarrow N(s)\leq N(t)$ $\hspace{20pt}$(b) $\mathbb{P}\left(N(t+h)=n+m|\hbox{ }N(t) =n\right) =\begin{cases} o(h) & \text{if } m>1,\\ \lambda h + o(h) &\text{if } m=1,\\ 1 - \lambda h + o(h) & \text{if }m=0\\ \end{cases}$ $\hspace{20pt}$(c) if $s<t$ then $N(t)-N(s)$ is independent of the times of emissions on $[0,s]$. I am having problems with understanding the meaning $o(h)$ in (b), so my question is: How do we interpret $o(h)$ in this definition? I know $o(h)$ in the classical sense is supposed to be just a function such that $\lim_{h\to 0}\frac{o(h)}h=0$. But here, $o$ is used three times and I suspect it has a different meaning each time. (Since otherwise, we would necessarily have $o\equiv0$.) So, I did some thinking and reinterpreted (b) to read: $\hspace{20pt}$(b') There exist functions $o_1, o_2, o_3$, such that for $i=1,2,3$:$$\lim_{h\to 0}\frac{o_i(h)}h=0$$ $\hspace{38pt}$and $$\mathbb{P}\left(N(t+h)=n+m|\hbox{ }N(t) =n\right) =\begin{cases} o_1(h) & \text{if } m>1,\\ \lambda h + o_2(h) &\text{if } m=1,\\ 1 - \lambda h + o_3(h) & \text{if }m=0.\\ \end{cases}$$ But I am completely confused about the order of quantifiers in this statement (which is why I left this rewording a bit vague). Should the functions $o_1,o_2,o_3$ be the same for all $t,n$ and $m>1$? (That is: should we take a separate function $o_m$ for each m? Or even a separate function $o_{t,n,m}$ for each triple $(t,n,m)$?) Does this even matter or are these definitions miraculously equivalent? I would really like to know what exactly it is this definition is trying to define. Thanks in advance for any helpful suggestions.","In the book Probability and Random Processes by Grimmett and Stirzaker, a Poisson process is defined to be any process $(N(t))_{t\in[0,\infty)}$ with values in $\mathbb{N}_0$ satisfying following three properties: $\hspace{20pt}$(a) $N(0) = 0$ and $s<t\Rightarrow N(s)\leq N(t)$ $\hspace{20pt}$(b) $\mathbb{P}\left(N(t+h)=n+m|\hbox{ }N(t) =n\right) =\begin{cases} o(h) & \text{if } m>1,\\ \lambda h + o(h) &\text{if } m=1,\\ 1 - \lambda h + o(h) & \text{if }m=0\\ \end{cases}$ $\hspace{20pt}$(c) if $s<t$ then $N(t)-N(s)$ is independent of the times of emissions on $[0,s]$. I am having problems with understanding the meaning $o(h)$ in (b), so my question is: How do we interpret $o(h)$ in this definition? I know $o(h)$ in the classical sense is supposed to be just a function such that $\lim_{h\to 0}\frac{o(h)}h=0$. But here, $o$ is used three times and I suspect it has a different meaning each time. (Since otherwise, we would necessarily have $o\equiv0$.) So, I did some thinking and reinterpreted (b) to read: $\hspace{20pt}$(b') There exist functions $o_1, o_2, o_3$, such that for $i=1,2,3$:$$\lim_{h\to 0}\frac{o_i(h)}h=0$$ $\hspace{38pt}$and $$\mathbb{P}\left(N(t+h)=n+m|\hbox{ }N(t) =n\right) =\begin{cases} o_1(h) & \text{if } m>1,\\ \lambda h + o_2(h) &\text{if } m=1,\\ 1 - \lambda h + o_3(h) & \text{if }m=0.\\ \end{cases}$$ But I am completely confused about the order of quantifiers in this statement (which is why I left this rewording a bit vague). Should the functions $o_1,o_2,o_3$ be the same for all $t,n$ and $m>1$? (That is: should we take a separate function $o_m$ for each m? Or even a separate function $o_{t,n,m}$ for each triple $(t,n,m)$?) Does this even matter or are these definitions miraculously equivalent? I would really like to know what exactly it is this definition is trying to define. Thanks in advance for any helpful suggestions.",,"['probability', 'notation', 'asymptotics']"
19,Simple probability Question from my textbook,Simple probability Question from my textbook,,Consider a group of four people. Everybody writes down the name of one other (random) member of the group. What is the probability that there is at least one pair of people who wrote down each others name? Answer is 17/27. I think it should be 19/27. how to calculate it ?,Consider a group of four people. Everybody writes down the name of one other (random) member of the group. What is the probability that there is at least one pair of people who wrote down each others name? Answer is 17/27. I think it should be 19/27. how to calculate it ?,,['probability']
20,A question about independent events,A question about independent events,,"While studying probability, the following question arose: Let $H$ be an event and let $\mathcal{H}=\lbrace H_\lambda|\lambda\in\Lambda\rbrace$ be a family of events in probability space $(\Omega,\mathcal{F},P)$, such that for every $\lambda\in\Lambda$ the following holds: $P(H_\lambda\cap H) = P(H_\lambda)P(H)$, i.e. the events $H_\lambda$ and $H$ are independent. Let $\mathcal{G}=\sigma(\mathcal{H})$ be the $\sigma$-algebra generated by $\mathcal{H}$. Let $G\in\mathcal{G}$ be any event. Are $G$ and $H$ necessarily independent i.e. does it follow that $P(G\cap H) = P(G)P(H)$? This would be quite a useful lemma, I think. I had an idea for a proof, but the last step didn't quite work as expected. The argument went like this: We shall write $\mathcal{G}$ as a union of an increasing sequence of more simple sets. Let $\mathcal{B}_0 = \mathcal{H}$. For every successor ordinal $\alpha+1$ define $\mathcal{A}_{\alpha+1} = \lbrace\bigcup\mathcal{J}|\mathcal{J}\subseteq\mathcal{B}_\alpha,\mathrm{card}(\mathcal{J})\leq\aleph_0\rbrace$, the set of all countable unions of the previous sets, and $\mathcal{B}_{\alpha+1}=\lbrace A|\Omega - A\in\mathcal{A}_{\alpha+1} \lor A\in\mathcal{A}_{\alpha+1}\rbrace$, the same with their complements added. For limit ordinals we define $\mathcal{B}_\beta=\bigcup_{\alpha<\beta}\mathcal{B}_\alpha$. Finally define $\mathcal{B} = \bigcup_{\alpha<\omega_1}\mathcal{B}_\alpha$. I guess such an union should make sense, since at each step we stay inside $\mathcal{G}$ ... Next we prove that $\mathcal{B}$ is a $\sigma$-algebra and since every set in the construction of this $\sigma$-algebra is a subset of $\mathcal{G}$, we must have that $\mathcal{B} = \mathcal{G}$. The only tricky part in proving $\mathcal{B}$ is a $\sigma$-algebra is closure under countable unions. Let $(A_n)_n$ be a sequence of events in $\mathcal{B}$. Then for each $n\in\mathbb{N}$ there is an ordinal $\alpha_n$ such that $A_n\in\mathcal{B}_{\alpha_n}$. Then there must be some ordinal $\gamma < \omega_1$ such that $\forall n:\alpha_n \leq \gamma$. (Since otherwise $\omega_1$ would be a countable union of countably many sets which it can't be, since it isn't countable. (Assuming the axiom of choice.)) So these events are all elements of $\mathcal{B}_\gamma$ which implies their countable union must lie in $\mathcal{A}_{\gamma+1}\subseteq\mathcal{B}_{\gamma+1}$ and therefore in $\mathcal{B}$. I was hoping the rest would follow by transfinite induction: if $A\in\mathcal{B}_{\alpha+1}$ then either $A\in\mathcal{A}_{\alpha+1}$ or $\Omega-A\in\mathcal{A}_{\alpha+1}$. The second case would follow from the first case using complements. But the first case is problematic: $P(A\cap H) = P((\bigcup_{E\in\mathcal{J}}E)\cap H) = P((\bigcup_{\tilde{E}\in\mathcal{J}_0}\tilde{E})\cap H) = \sum_{\tilde{E}\in\mathcal{J}_0}P(\tilde{E}\cap H)$. Here $\mathcal{J}\subseteq\mathcal{B}_\alpha$ exists by definition of $\mathcal{A}_{\alpha+1}$ and $\mathcal{J_0}$ is a set of mutually exclusive events giving the same union. The problem is that such a set $\mathcal{J_0}$ can in this case only be proven to lie under $\mathcal{B}_{\alpha+1}$, so we cannot write $P(\tilde{E}\cap H) = P(\tilde{E})P(H)$. So the proof sadly fails at this last step. Is this proof salvageable? (Perhaps by taking relative complements instead in the definition of $\mathcal{B}_{\alpha+1}$ or something like that?) Does such a lemma even hold or do we have to modify it? Are such proofs by transfinite induction useful in probability? It seems to me probabilists implicitly use lemmas like this all the time, so I am also wondering if such a lemma or a similar one would in fact be useful. [Comment: The definition of $\mathcal{B}$ above originally used $\mathbf{On}$ which was a slight overkill, so I changed it to $\omega_1$, following the kind suggestion of Asaf Karagila.] Added: In a comment below Dilip Sarwate suggests the following variation on the problem: Let $H$ be an event and let $\mathcal{H}=\lbrace H_\lambda|\lambda\in\Lambda\rbrace$ be a family of events in probability space $(\Omega,\mathcal{F},P)$, such that the family of events $\mathcal{H}\cup\lbrace H\rbrace$ is independent i.e. for every finite $\mathcal{S}\subseteq\mathcal{H}\cup\lbrace H\rbrace$ we have $P(\bigcap_{E\in\mathcal{S}}E) = \Pi_{E\in\mathcal{S}}P(E) $, where $\Pi$ denotes the product of the probabilities, as usual. Let $\mathcal{G}=\sigma(\mathcal{H})$ be the $\sigma$-algebra generated by $\mathcal{H}$. Let $G\in\mathcal{G}$ be any event. Are $G$ and $H$ necessarily independent i.e. does it follow that $P(G\cap H) = P(G)P(H)$? This case actually interests me even more than the ""original question"" above, since it is this case that I actually needed. (I thought somehow that I can get more out of it by relaxing the conditions to what the question above says. Silly me.)","While studying probability, the following question arose: Let $H$ be an event and let $\mathcal{H}=\lbrace H_\lambda|\lambda\in\Lambda\rbrace$ be a family of events in probability space $(\Omega,\mathcal{F},P)$, such that for every $\lambda\in\Lambda$ the following holds: $P(H_\lambda\cap H) = P(H_\lambda)P(H)$, i.e. the events $H_\lambda$ and $H$ are independent. Let $\mathcal{G}=\sigma(\mathcal{H})$ be the $\sigma$-algebra generated by $\mathcal{H}$. Let $G\in\mathcal{G}$ be any event. Are $G$ and $H$ necessarily independent i.e. does it follow that $P(G\cap H) = P(G)P(H)$? This would be quite a useful lemma, I think. I had an idea for a proof, but the last step didn't quite work as expected. The argument went like this: We shall write $\mathcal{G}$ as a union of an increasing sequence of more simple sets. Let $\mathcal{B}_0 = \mathcal{H}$. For every successor ordinal $\alpha+1$ define $\mathcal{A}_{\alpha+1} = \lbrace\bigcup\mathcal{J}|\mathcal{J}\subseteq\mathcal{B}_\alpha,\mathrm{card}(\mathcal{J})\leq\aleph_0\rbrace$, the set of all countable unions of the previous sets, and $\mathcal{B}_{\alpha+1}=\lbrace A|\Omega - A\in\mathcal{A}_{\alpha+1} \lor A\in\mathcal{A}_{\alpha+1}\rbrace$, the same with their complements added. For limit ordinals we define $\mathcal{B}_\beta=\bigcup_{\alpha<\beta}\mathcal{B}_\alpha$. Finally define $\mathcal{B} = \bigcup_{\alpha<\omega_1}\mathcal{B}_\alpha$. I guess such an union should make sense, since at each step we stay inside $\mathcal{G}$ ... Next we prove that $\mathcal{B}$ is a $\sigma$-algebra and since every set in the construction of this $\sigma$-algebra is a subset of $\mathcal{G}$, we must have that $\mathcal{B} = \mathcal{G}$. The only tricky part in proving $\mathcal{B}$ is a $\sigma$-algebra is closure under countable unions. Let $(A_n)_n$ be a sequence of events in $\mathcal{B}$. Then for each $n\in\mathbb{N}$ there is an ordinal $\alpha_n$ such that $A_n\in\mathcal{B}_{\alpha_n}$. Then there must be some ordinal $\gamma < \omega_1$ such that $\forall n:\alpha_n \leq \gamma$. (Since otherwise $\omega_1$ would be a countable union of countably many sets which it can't be, since it isn't countable. (Assuming the axiom of choice.)) So these events are all elements of $\mathcal{B}_\gamma$ which implies their countable union must lie in $\mathcal{A}_{\gamma+1}\subseteq\mathcal{B}_{\gamma+1}$ and therefore in $\mathcal{B}$. I was hoping the rest would follow by transfinite induction: if $A\in\mathcal{B}_{\alpha+1}$ then either $A\in\mathcal{A}_{\alpha+1}$ or $\Omega-A\in\mathcal{A}_{\alpha+1}$. The second case would follow from the first case using complements. But the first case is problematic: $P(A\cap H) = P((\bigcup_{E\in\mathcal{J}}E)\cap H) = P((\bigcup_{\tilde{E}\in\mathcal{J}_0}\tilde{E})\cap H) = \sum_{\tilde{E}\in\mathcal{J}_0}P(\tilde{E}\cap H)$. Here $\mathcal{J}\subseteq\mathcal{B}_\alpha$ exists by definition of $\mathcal{A}_{\alpha+1}$ and $\mathcal{J_0}$ is a set of mutually exclusive events giving the same union. The problem is that such a set $\mathcal{J_0}$ can in this case only be proven to lie under $\mathcal{B}_{\alpha+1}$, so we cannot write $P(\tilde{E}\cap H) = P(\tilde{E})P(H)$. So the proof sadly fails at this last step. Is this proof salvageable? (Perhaps by taking relative complements instead in the definition of $\mathcal{B}_{\alpha+1}$ or something like that?) Does such a lemma even hold or do we have to modify it? Are such proofs by transfinite induction useful in probability? It seems to me probabilists implicitly use lemmas like this all the time, so I am also wondering if such a lemma or a similar one would in fact be useful. [Comment: The definition of $\mathcal{B}$ above originally used $\mathbf{On}$ which was a slight overkill, so I changed it to $\omega_1$, following the kind suggestion of Asaf Karagila.] Added: In a comment below Dilip Sarwate suggests the following variation on the problem: Let $H$ be an event and let $\mathcal{H}=\lbrace H_\lambda|\lambda\in\Lambda\rbrace$ be a family of events in probability space $(\Omega,\mathcal{F},P)$, such that the family of events $\mathcal{H}\cup\lbrace H\rbrace$ is independent i.e. for every finite $\mathcal{S}\subseteq\mathcal{H}\cup\lbrace H\rbrace$ we have $P(\bigcap_{E\in\mathcal{S}}E) = \Pi_{E\in\mathcal{S}}P(E) $, where $\Pi$ denotes the product of the probabilities, as usual. Let $\mathcal{G}=\sigma(\mathcal{H})$ be the $\sigma$-algebra generated by $\mathcal{H}$. Let $G\in\mathcal{G}$ be any event. Are $G$ and $H$ necessarily independent i.e. does it follow that $P(G\cap H) = P(G)P(H)$? This case actually interests me even more than the ""original question"" above, since it is this case that I actually needed. (I thought somehow that I can get more out of it by relaxing the conditions to what the question above says. Silly me.)",,"['probability', 'measure-theory']"
21,How to find the probability of truth?,How to find the probability of truth?,,"A and B are independent witness in a case. The probablity that A   speaks the truth is 'x' and that of B is 'y'.If A and B agree on a   certain statement, how to find the probability that the statement is   true ?","A and B are independent witness in a case. The probablity that A   speaks the truth is 'x' and that of B is 'y'.If A and B agree on a   certain statement, how to find the probability that the statement is   true ?",,['probability']
22,"""Physical"" meaning of higher moments (their values and their existence)","""Physical"" meaning of higher moments (their values and their existence)",,"Suppose I have a probability distribution $A$ with continuous support over $\mathbb{R}$.  Suppose $A$ has a sequence of finite (central) moments $\mu_1, \mu_2,\ldots,\mu_n$.  I understand that $\mu_1$ is the mean, and $\mu_2$, $\mu_3$ and $\mu_4$ define variance, skewness, and kurtosis of the distribution, respectively. I am wondering about the meaning of $\mu_5, \mu_6, \mu_7,\ldots$  When they are finite, what do they represent about the distribution $A$? I understand that the odd central moments of the symmetric distribution are zero, so I am assuming that odd moments are related to the skew.  What do even higher moments represent?  I am particularly curious about $\mu_6$. Also, suppose all moments of $A$ are finite.  What does that say about $A$?  Does it mean that $A$ has a specific representation? I've heard somewhere that all finite moments of $A$ with support $\mathbb{R}$ means that the tails of $A$ decay exponentially.  Is that true?  If so, can someone point me to a proof?","Suppose I have a probability distribution $A$ with continuous support over $\mathbb{R}$.  Suppose $A$ has a sequence of finite (central) moments $\mu_1, \mu_2,\ldots,\mu_n$.  I understand that $\mu_1$ is the mean, and $\mu_2$, $\mu_3$ and $\mu_4$ define variance, skewness, and kurtosis of the distribution, respectively. I am wondering about the meaning of $\mu_5, \mu_6, \mu_7,\ldots$  When they are finite, what do they represent about the distribution $A$? I understand that the odd central moments of the symmetric distribution are zero, so I am assuming that odd moments are related to the skew.  What do even higher moments represent?  I am particularly curious about $\mu_6$. Also, suppose all moments of $A$ are finite.  What does that say about $A$?  Does it mean that $A$ has a specific representation? I've heard somewhere that all finite moments of $A$ with support $\mathbb{R}$ means that the tails of $A$ decay exponentially.  Is that true?  If so, can someone point me to a proof?",,"['probability', 'probability-theory']"
23,Counterexample to Jensen's inequality,Counterexample to Jensen's inequality,,"This appeared in an exam I took. The question asked us to give an example of a convex function $g: \mathbb{R} \longmapsto \mathbb{R}$ and a measure $\mu$ on $\left(\mathbb{R}, \mathscr{B}(\mathbb{R})\right)$ such that $g\left(\int x \, d\mu(x)\right) > \int g(x)\, d\mu(x)$. I am assuming that such an example can be constructed by violating the finiteness of the measure, but I have no idea how I would construct such an example. I guess the Lebesgue measure can be used, but am having trouble finding a function that is convex and gives this result.","This appeared in an exam I took. The question asked us to give an example of a convex function $g: \mathbb{R} \longmapsto \mathbb{R}$ and a measure $\mu$ on $\left(\mathbb{R}, \mathscr{B}(\mathbb{R})\right)$ such that $g\left(\int x \, d\mu(x)\right) > \int g(x)\, d\mu(x)$. I am assuming that such an example can be constructed by violating the finiteness of the measure, but I have no idea how I would construct such an example. I guess the Lebesgue measure can be used, but am having trouble finding a function that is convex and gives this result.",,"['probability', 'measure-theory', 'probability-theory']"
24,How many correct answers does it take to fill the Trivial Pursuit receptacle?,How many correct answers does it take to fill the Trivial Pursuit receptacle?,,"My friends and I likes to play Trivial Pursuit without using the board. We play it like this: Throw a die to determine what color you get to answer. Ask a question, if the answer is correct you get a point. If enough points are awarded you win We would like to modify the game as to include the colors. There are 6 colors. The game could then be won by completing all colors or answering enough questions. We would like the the effort to complete it by numbers to be similar to that of completing it by colors. So the required number of correct answers should be the same as where it is likely that all the colors has been collected. What is the number of correct answers one needs to acquire to make it probable, P>=0.5, that all colors are collected? We dabbled in a few sums before realizing this was over our heads.","My friends and I likes to play Trivial Pursuit without using the board. We play it like this: Throw a die to determine what color you get to answer. Ask a question, if the answer is correct you get a point. If enough points are awarded you win We would like to modify the game as to include the colors. There are 6 colors. The game could then be won by completing all colors or answering enough questions. We would like the the effort to complete it by numbers to be similar to that of completing it by colors. So the required number of correct answers should be the same as where it is likely that all the colors has been collected. What is the number of correct answers one needs to acquire to make it probable, P>=0.5, that all colors are collected? We dabbled in a few sums before realizing this was over our heads.",,['probability']
25,Definition: transient random walk,Definition: transient random walk,,"What exactly does a ""transient random walk on a graph/binary tree"" mean? Does it mean that we never return to the origin (assuming there is one as for the tree) or just any vertex of the graph or tree? Thanks.","What exactly does a ""transient random walk on a graph/binary tree"" mean? Does it mean that we never return to the origin (assuming there is one as for the tree) or just any vertex of the graph or tree? Thanks.",,"['probability', 'stochastic-processes', 'definition', 'random-walk']"
26,Relationship between median and mean of a pmf,Relationship between median and mean of a pmf,,"If you consider a distribution that has many medians: $$P(X=x) = \{(1, 0.25)(2,0.25),(3,0.25),(4,0.25)\} ,$$ we know that this distribution has multiple medians between $2$ and $3$ if we define a median as a number where $P(X \geq c) \geq1/2$ and $P(X \leq c) \geq 1/2$. I basically want to prove that the set of all medians is the only set of numbers that minimizes $E[|X-a|]$. Attempt: Distributions can have multiple medians, and these multiple medians are the only numbers that minimize the  absolute value of the distance between the mean. That is, there is a 1-1 correspondence between the set of numbers that minimizes the absolute value of the distance between the mean and the set of medians. I am stuck in that I can't show a rigorous proof for this question.","If you consider a distribution that has many medians: $$P(X=x) = \{(1, 0.25)(2,0.25),(3,0.25),(4,0.25)\} ,$$ we know that this distribution has multiple medians between $2$ and $3$ if we define a median as a number where $P(X \geq c) \geq1/2$ and $P(X \leq c) \geq 1/2$. I basically want to prove that the set of all medians is the only set of numbers that minimizes $E[|X-a|]$. Attempt: Distributions can have multiple medians, and these multiple medians are the only numbers that minimize the  absolute value of the distance between the mean. That is, there is a 1-1 correspondence between the set of numbers that minimizes the absolute value of the distance between the mean and the set of medians. I am stuck in that I can't show a rigorous proof for this question.",,['probability']
27,Question regarding counting poker dice,Question regarding counting poker dice,,"Problem Poker dice is played by simultaneously rolling 5 dice. How many ways can we form ""1 pair"", ""2 pairs""? For one pair, I got the answer right away. First I consider there are 5 spots for 5 dice. Then I pick 2 places out of 5, which means there are 3 places left, so we have to choose 3 out of 3 which is 1 way. Hence, I have: $${{5}\choose{2}} \cdot 6 {{3}\choose{3}} \cdot 5 \cdot 4 \cdot 3 = 3600.$$ However, I couldn't figure out why I got two pairs wrong. First, I pick 2 places for the first pair, then its rank. Next, 2 places for the second pair, and its rank. Since there is only 1 place left, I pick the rank for the last dice. $${{5}\choose{2}} \cdot 6 {{3}\choose{2}} \cdot 5 \cdot 4 \cdot 3 = 3600.$$ But the correct answer is 1800, which means I need to divide by a factor of 2. I guess that might be the order of two pairs can be switched, but I wonder is there a better way to count it? I'm so confused! Any idea?","Problem Poker dice is played by simultaneously rolling 5 dice. How many ways can we form ""1 pair"", ""2 pairs""? For one pair, I got the answer right away. First I consider there are 5 spots for 5 dice. Then I pick 2 places out of 5, which means there are 3 places left, so we have to choose 3 out of 3 which is 1 way. Hence, I have: However, I couldn't figure out why I got two pairs wrong. First, I pick 2 places for the first pair, then its rank. Next, 2 places for the second pair, and its rank. Since there is only 1 place left, I pick the rank for the last dice. But the correct answer is 1800, which means I need to divide by a factor of 2. I guess that might be the order of two pairs can be switched, but I wonder is there a better way to count it? I'm so confused! Any idea?",{{5}\choose{2}} \cdot 6 {{3}\choose{3}} \cdot 5 \cdot 4 \cdot 3 = 3600. {{5}\choose{2}} \cdot 6 {{3}\choose{2}} \cdot 5 \cdot 4 \cdot 3 = 3600.,"['probability', 'combinatorics']"
28,Samples and random variables,Samples and random variables,,"Suppose I picked a sample of $n$ 20-year-olds. I measured the height of each to obtain $n$ numbers: $h_1, h_2, \ldots, h_n$. According to theory of probability/statistics, there are $n$ random variables associated with the sample, say $X_1, X_2, \ldots, X_n$. However, I do not understand the relationship between the $X_i$ and the $h_i$, I have yet to see it explained clearly in any book and so I have a few questions. What is the probability space corresponding to the $X_i$? It seems to me that the way one samples should effect what the probability space will look like. In this case, I am sampling without replacement and the order in which I picked the individuals is irrelevant so I believe the sample space $\Omega$ should consist of all $n$-tuples of 20-year-olds such that no two tuples contain the same individuals. In this way, $X_i(\omega)$ is the height of the $i$th individual in the $n$-tuple $\omega \in \Omega$. The sample I picked would therefore correspond to one particlar point in $\Omega$, call it $\omega_0$, such that $X_i(\omega_0) = h_i$. I surmise that the $\sigma$-algebra will be just the power set $2^\Omega$ of $\Omega$ but I haven't a clue as to what the probability measure would be. Let $(\Gamma, 2^\Gamma, P)$ be a probability space where $\Gamma$ is the set of all 20-year-olds and let $X$ be a random variable on $\Gamma$ such that $X(\gamma)$ is the height of the individual $\gamma\in\Gamma$. What is the connection between the $X_i$ and $X$ besides that afforded to us by the law of large numbers? In particular, what is the exact relationship between the probability space of $X$ and that of the $X_i$?","Suppose I picked a sample of $n$ 20-year-olds. I measured the height of each to obtain $n$ numbers: $h_1, h_2, \ldots, h_n$. According to theory of probability/statistics, there are $n$ random variables associated with the sample, say $X_1, X_2, \ldots, X_n$. However, I do not understand the relationship between the $X_i$ and the $h_i$, I have yet to see it explained clearly in any book and so I have a few questions. What is the probability space corresponding to the $X_i$? It seems to me that the way one samples should effect what the probability space will look like. In this case, I am sampling without replacement and the order in which I picked the individuals is irrelevant so I believe the sample space $\Omega$ should consist of all $n$-tuples of 20-year-olds such that no two tuples contain the same individuals. In this way, $X_i(\omega)$ is the height of the $i$th individual in the $n$-tuple $\omega \in \Omega$. The sample I picked would therefore correspond to one particlar point in $\Omega$, call it $\omega_0$, such that $X_i(\omega_0) = h_i$. I surmise that the $\sigma$-algebra will be just the power set $2^\Omega$ of $\Omega$ but I haven't a clue as to what the probability measure would be. Let $(\Gamma, 2^\Gamma, P)$ be a probability space where $\Gamma$ is the set of all 20-year-olds and let $X$ be a random variable on $\Gamma$ such that $X(\gamma)$ is the height of the individual $\gamma\in\Gamma$. What is the connection between the $X_i$ and $X$ besides that afforded to us by the law of large numbers? In particular, what is the exact relationship between the probability space of $X$ and that of the $X_i$?",,"['probability', 'statistics']"
29,Sample: don't confuse measurements with actual values?,Sample: don't confuse measurements with actual values?,,"In Wikipedia's article on Sample there is the following remark: ''Note that a sample of random variables (i.e. a set of measurable functions) must not be confused with the realizations of these variables (which are the values that these random variables take). In other words, $X_i$ is a function representing the measurement at the $i$-th experiment and $x_i = X_i(ω)$ is the value we actually get when making the measurement.'' I'm afraid I don't understand this passage, can anybody please explain the point?","In Wikipedia's article on Sample there is the following remark: ''Note that a sample of random variables (i.e. a set of measurable functions) must not be confused with the realizations of these variables (which are the values that these random variables take). In other words, $X_i$ is a function representing the measurement at the $i$-th experiment and $x_i = X_i(ω)$ is the value we actually get when making the measurement.'' I'm afraid I don't understand this passage, can anybody please explain the point?",,"['probability', 'probability-theory', 'sampling']"
30,Meaning of randomness in space,Meaning of randomness in space,,"I am a non-math person and have a question about randomness: When generating elements from a finite set using some algorithm, it is clear what it means when saying that elements should be randomly generated. How about for infinite sets? For example if I want to generate real numbers randomly, what does it mean to be random? In general, how is random defined in Euclidean n-space? How about for subsets of n-space, eg. generating random points on the (n-1) unit sphere? Thanks.","I am a non-math person and have a question about randomness: When generating elements from a finite set using some algorithm, it is clear what it means when saying that elements should be randomly generated. How about for infinite sets? For example if I want to generate real numbers randomly, what does it mean to be random? In general, how is random defined in Euclidean n-space? How about for subsets of n-space, eg. generating random points on the (n-1) unit sphere? Thanks.",,"['probability', 'random']"
31,Summation Formula for Product of Two Distinct Integers within a Range (Derivation of Covariance),Summation Formula for Product of Two Distinct Integers within a Range (Derivation of Covariance),,"I am trying to follow the derivation for the covariance of two discrete random variables. The joint distribution of the two random variables is known: $$ P(x=a, y=b) = \frac{1}{(m+n)(m+n-1)},$$ when $1 \leq a \leq m+n, 1 \leq b \leq m+n, a \neq b$. The distribution of x is the same as y, and it is known: $$ P(x=a) = P(y=a) = \frac{1}{m+n},$$ when $1 \leq a \leq m+n$ Then, to calculate $Cov(x,y)$: $$ Cov(x,y) = E[xy] - E[x]E[y] $$ $$ = \sum_{1 \leq a \leq m+n, 1 \leq b \leq m+n, a \neq b} \frac{ab}{(m+n)(m+n-1)} - \left(\frac{m+n+1}{2}\right)^2$$ The covariance is given as: $\displaystyle Cov(x,y) = -\frac{m+n+1}{12}$. How do I express the summation in the covariance equation in terms of $m,n$? It is the product of two numbers, then varying the two numbers over a range, that is getting me stuck. Thanks. P.S. I have read the post here Variance for Summing over Distinct Random Integers , and I am getting stuck on step #3 of Didier Piau's post.","I am trying to follow the derivation for the covariance of two discrete random variables. The joint distribution of the two random variables is known: $$ P(x=a, y=b) = \frac{1}{(m+n)(m+n-1)},$$ when $1 \leq a \leq m+n, 1 \leq b \leq m+n, a \neq b$. The distribution of x is the same as y, and it is known: $$ P(x=a) = P(y=a) = \frac{1}{m+n},$$ when $1 \leq a \leq m+n$ Then, to calculate $Cov(x,y)$: $$ Cov(x,y) = E[xy] - E[x]E[y] $$ $$ = \sum_{1 \leq a \leq m+n, 1 \leq b \leq m+n, a \neq b} \frac{ab}{(m+n)(m+n-1)} - \left(\frac{m+n+1}{2}\right)^2$$ The covariance is given as: $\displaystyle Cov(x,y) = -\frac{m+n+1}{12}$. How do I express the summation in the covariance equation in terms of $m,n$? It is the product of two numbers, then varying the two numbers over a range, that is getting me stuck. Thanks. P.S. I have read the post here Variance for Summing over Distinct Random Integers , and I am getting stuck on step #3 of Didier Piau's post.",,"['calculus', 'probability']"
32,Probability inequality,Probability inequality,,"Let $X,Y,Z$ be non-negative independent r.v. I should find for which $u\geq 0$ holds $$ Zu\geq Y - X(1+Z) $$ with probability one. Clearly, if $P(Z = 0)>0$ then  the value of $u$ doesn't matter and we should have $X\geq Y$ with probability one. The question is the following: what if $P(Z = 0) = 0$? Could we divide an inequality by $Z$ and continue working? If yes I expect an answer like $$ u\geq \alpha $$ for  $$ \alpha = \inf\left(a:P\left[\frac{Y-X}{Z}-X\leq a \right]= 1\right) $$ with possibly $\alpha = \infty$.","Let $X,Y,Z$ be non-negative independent r.v. I should find for which $u\geq 0$ holds $$ Zu\geq Y - X(1+Z) $$ with probability one. Clearly, if $P(Z = 0)>0$ then  the value of $u$ doesn't matter and we should have $X\geq Y$ with probability one. The question is the following: what if $P(Z = 0) = 0$? Could we divide an inequality by $Z$ and continue working? If yes I expect an answer like $$ u\geq \alpha $$ for  $$ \alpha = \inf\left(a:P\left[\frac{Y-X}{Z}-X\leq a \right]= 1\right) $$ with possibly $\alpha = \infty$.",,"['probability', 'probability-theory']"
33,Distribution of dot product?,Distribution of dot product?,,"What is the distribution of the random variable $$X = a \cdot b$$ where $a, b$ are unit $m$-vectors independently drawn from the uniform distribution on the unit $m$-sphere?  Is there a special name for this distribution? EDIT: Thanks to Joriki for his answer.  The distribution of the dot product of two vectors in $S^{p-1}$ is also the null distribution for Pearson's rho given $p+1$ observations from two independent normally distributed populations.  Alternatively, $X^2 \sim Beta(\frac{1}{2},\frac{p-1}{2})$.","What is the distribution of the random variable $$X = a \cdot b$$ where $a, b$ are unit $m$-vectors independently drawn from the uniform distribution on the unit $m$-sphere?  Is there a special name for this distribution? EDIT: Thanks to Joriki for his answer.  The distribution of the dot product of two vectors in $S^{p-1}$ is also the null distribution for Pearson's rho given $p+1$ observations from two independent normally distributed populations.  Alternatively, $X^2 \sim Beta(\frac{1}{2},\frac{p-1}{2})$.",,['probability']
34,How do you compute the steady state probabilities of a continuous time markov chain?,How do you compute the steady state probabilities of a continuous time markov chain?,,"Given a markov model with transition rate matrix $Q$, where the probability of each state at time $t$ is given by $P(t) = P(0)e^{Qt}$, where e is the matrix exponential, the steady state probabilities should be given by taking the $\lim_{t\to\infty}P(t)$. Is this the best way, or are there alternatives, and if so, how do you solve them with those methods? If not, how does one take the limit of the matrix exponential as t goes to infinity so that you can arrive at an analytical solution.","Given a markov model with transition rate matrix $Q$, where the probability of each state at time $t$ is given by $P(t) = P(0)e^{Qt}$, where e is the matrix exponential, the steady state probabilities should be given by taking the $\lim_{t\to\infty}P(t)$. Is this the best way, or are there alternatives, and if so, how do you solve them with those methods? If not, how does one take the limit of the matrix exponential as t goes to infinity so that you can arrive at an analytical solution.",,['probability']
35,Similarity between Entropy of information source & Expectation of a random varible,Similarity between Entropy of information source & Expectation of a random varible,,"While I was watching a lecture on Information theory, I found that entropy of an information source is the average amount of information that it provides in terms of bits (or nats, decits or whatever), which's actually the weighted average of information contained in all the symbols that source provides (weighted by probabilities of individual symbols).. I found a striking similarity between this & concept of Expectation of a random variable which has similar stuff in it's explanation. Am I right on this ? I mean is their any intuitive connection between the two concepts? The explanation of both the concepts taking into consideration their similarity (if find any) is also welcome. Thank you .","While I was watching a lecture on Information theory, I found that entropy of an information source is the average amount of information that it provides in terms of bits (or nats, decits or whatever), which's actually the weighted average of information contained in all the symbols that source provides (weighted by probabilities of individual symbols).. I found a striking similarity between this & concept of Expectation of a random variable which has similar stuff in it's explanation. Am I right on this ? I mean is their any intuitive connection between the two concepts? The explanation of both the concepts taking into consideration their similarity (if find any) is also welcome. Thank you .",,"['probability', 'information-theory']"
36,random walk on the nonnegative integers,random walk on the nonnegative integers,,"Consider a simple random walk on the integers with a reflecting barrier at $0$. What is the expected time the walk spends in the set $\{0, \ldots, k\}$ in the first $n$ steps? I'm curious about how the answer scales with $n,k$, and don't really care about the constants.","Consider a simple random walk on the integers with a reflecting barrier at $0$. What is the expected time the walk spends in the set $\{0, \ldots, k\}$ in the first $n$ steps? I'm curious about how the answer scales with $n,k$, and don't really care about the constants.",,['probability']
37,How long to choose n out of 2n numbers?,How long to choose n out of 2n numbers?,,"Choose numbers from $1$ to $2n$ uniformly at random.  How many numbers must be chosen, on average, before at least $n$ numbers have been picked? This is similar to the coupon-collector problem, but looking for only partial completion. Note: Choosing an appropriate meaning of 'random' is part of the question.","Choose numbers from $1$ to $2n$ uniformly at random.  How many numbers must be chosen, on average, before at least $n$ numbers have been picked? This is similar to the coupon-collector problem, but looking for only partial completion. Note: Choosing an appropriate meaning of 'random' is part of the question.",,"['combinatorics', 'probability']"
38,Best strategy to determine which of two coins is biased using only two flips,Best strategy to determine which of two coins is biased using only two flips,,"We have two coins - one fair, and one biased with a probability of 0.6 for flipping a head. We get to make exactly two flips before making a guess as to which coin is biased. The question is which strategy gives us a higher probability of being able to determine the biased coin - flipping each coin once, or any one coin twice? Using simple probability rules and Bayes' Theorem, I determined the following probabilities, though I'm not sure how to proceed from here to determine the best strategy. Case I: We flip any one coin twice $$\mathrm P(Biased\ Coin\ |\ HH) \approx 0.59$$ $$\mathrm P(Biased\ Coin\ |\ TT) \approx 0.39$$ $$\mathrm P(Biased\ Coin\ |\ TH) \approx 0.49$$ $$\mathrm P(Biased\ Coin\ |\ HT) \approx 0.49$$ Case II: We flip both coins once each For the outcomes HH and TT, we get no new information about the coin and the probability of either of the two being biased given those outcomes is still 0.5. $$\mathrm P(First\ Coin\ Biased\ |\ HT) = P(Second\ Coin\ Biased\ |\ TH) = 0.6$$ $$\mathrm P(First\ Coin\ Biased\ |\ TH) = P(Second\ Coin\ Biased\ |\ HT) = 0.4$$ How do we decide which is a better strategy to guess the biased coin correctly? Is one even better than the other? And do we need the probabilities I've calculated for determining the same?","We have two coins - one fair, and one biased with a probability of 0.6 for flipping a head. We get to make exactly two flips before making a guess as to which coin is biased. The question is which strategy gives us a higher probability of being able to determine the biased coin - flipping each coin once, or any one coin twice? Using simple probability rules and Bayes' Theorem, I determined the following probabilities, though I'm not sure how to proceed from here to determine the best strategy. Case I: We flip any one coin twice Case II: We flip both coins once each For the outcomes HH and TT, we get no new information about the coin and the probability of either of the two being biased given those outcomes is still 0.5. How do we decide which is a better strategy to guess the biased coin correctly? Is one even better than the other? And do we need the probabilities I've calculated for determining the same?",\mathrm P(Biased\ Coin\ |\ HH) \approx 0.59 \mathrm P(Biased\ Coin\ |\ TT) \approx 0.39 \mathrm P(Biased\ Coin\ |\ TH) \approx 0.49 \mathrm P(Biased\ Coin\ |\ HT) \approx 0.49 \mathrm P(First\ Coin\ Biased\ |\ HT) = P(Second\ Coin\ Biased\ |\ TH) = 0.6 \mathrm P(First\ Coin\ Biased\ |\ TH) = P(Second\ Coin\ Biased\ |\ HT) = 0.4,"['probability', 'bayes-theorem']"
39,Identity regarding the sum of products of binomial coefficients.,Identity regarding the sum of products of binomial coefficients.,,"Consider the following toy problem Person A and Person B have $n$ and $n+1$ fair coins respectively. If they both flip all their coins at the same time, what is the probability person B has more heads than person A? The answer, regardless of $n$ , is 50% -- which is pretty surprising.  To convince myself, I derived the distribution for the difference in heads between B and A.  Let $D=B-A$ and so $\Pr(D=k)$ is $$ \Pr(D=k \mid k>0) = \sum_{i=1}^{n+1} {n+1 \choose i}{n \choose i-k}2^{-2n-1} $$ and $$ \Pr(D=k \mid k\leq0) = \sum_{i=0}^{n} {n \choose i}{n+1 \choose i-k}2^{-2n-1} $$ If the answer to the question is 50%, this might mean that the distribution is symmetric about $k=0$ .  Naturally, I assumed that $\Pr(k=1) = \Pr(k=0)$ and tried to prove myself wrong or right. Using Maple (computer algebra) I tried to evaluate the sums for each, and in each case I am told that the sums (excluding the factor of $2^{-2n-1}$ ) are equal and equivalent to $$ {2n+1 \choose n}$$ See below Question Are the sums which involve the product of binomial coefficients some sort of identity? If so, what is the name of said identity and how are the two equivalent (I presume the answer lies in some index manipulation).","Consider the following toy problem Person A and Person B have and fair coins respectively. If they both flip all their coins at the same time, what is the probability person B has more heads than person A? The answer, regardless of , is 50% -- which is pretty surprising.  To convince myself, I derived the distribution for the difference in heads between B and A.  Let and so is and If the answer to the question is 50%, this might mean that the distribution is symmetric about .  Naturally, I assumed that and tried to prove myself wrong or right. Using Maple (computer algebra) I tried to evaluate the sums for each, and in each case I am told that the sums (excluding the factor of ) are equal and equivalent to See below Question Are the sums which involve the product of binomial coefficients some sort of identity? If so, what is the name of said identity and how are the two equivalent (I presume the answer lies in some index manipulation).",n n+1 n D=B-A \Pr(D=k)  \Pr(D=k \mid k>0) = \sum_{i=1}^{n+1} {n+1 \choose i}{n \choose i-k}2^{-2n-1}   \Pr(D=k \mid k\leq0) = \sum_{i=0}^{n} {n \choose i}{n+1 \choose i-k}2^{-2n-1}  k=0 \Pr(k=1) = \Pr(k=0) 2^{-2n-1}  {2n+1 \choose n},"['probability', 'combinatorics', 'summation', 'binomial-coefficients']"
40,Hamming distance in real analysis,Hamming distance in real analysis,,"Given two binary strings $x, y\in (0,1)^*$ such that $|x|=|y|$ , then the set $$\delta{(x,y)}=\frac{|\{i\in[|x|]:x_i\neq y_i\}|}{|x|}$$ is called relative hamming distance. Given $x\in (0,1)^*$ and $S:$ = a collection of binary strings, define $$\delta_S(x)=\min_{y\in S, |x|=|y|}\delta(x,y).$$ Given $\epsilon>0,x\in (0,1)^*$ is said to be $\epsilon$ -far from $S$ provided $\delta_S(x)>\epsilon.$ The majority language is given by: $$\text{MAJ}:=\{x\in (0,1)^*:\sum_{i=1}^ {|x|}x_i>\frac{|x|}{2}\},\text{where $x_i$ is the $i$-th position value(either $0$ or $1$) of $x$}.$$ My question is  how can I prove if $\delta_{MAJ}(x)>\epsilon,$ then $$\frac{\sum_{i=1}^ {|x|} x_i}{|x|}<\frac{1}{2}-\epsilon?$$","Given two binary strings such that , then the set is called relative hamming distance. Given and = a collection of binary strings, define Given is said to be -far from provided The majority language is given by: My question is  how can I prove if then","x, y\in (0,1)^* |x|=|y| \delta{(x,y)}=\frac{|\{i\in[|x|]:x_i\neq y_i\}|}{|x|} x\in (0,1)^* S: \delta_S(x)=\min_{y\in S, |x|=|y|}\delta(x,y). \epsilon>0,x\in (0,1)^* \epsilon S \delta_S(x)>\epsilon. \text{MAJ}:=\{x\in (0,1)^*:\sum_{i=1}^ {|x|}x_i>\frac{|x|}{2}\},\text{where x_i is the i-th position value(either 0 or 1) of x}. \delta_{MAJ}(x)>\epsilon, \frac{\sum_{i=1}^ {|x|} x_i}{|x|}<\frac{1}{2}-\epsilon?","['real-analysis', 'probability']"
41,Uniform Integrability and Proving martingale of Poisson product process,Uniform Integrability and Proving martingale of Poisson product process,,"So I have been stuck on the following question that I stumbled across in a textbook. It is trying to show the following product process is a martingale. $$M_n = n! \prod_{k=1}^{n} X_k \ \text{where } X_{k} \sim \text{Poi}(1/k) \text{ and } X_k \text{ are independent} $$ I know I have to prove adaptedness, integrability and the martingale equality but I am struggling to show the integrability as I have the working out below that I cannot seem to bound above to prove $ \mathbb{E}|M_n| < \infty$ My working is as below, $$ \mathbb{E}|M_n| = \mathbb{E}\left| n! \prod_{k=1}^n X_k \right| = n! \cdot  \mathbb{E} \prod_{k=1}^{n} X_k = n! \prod_{k=1}^{n} \mathbb{E} X_k = n!  \cdot (\mathbb{E} X_k)^n =\dfrac{n!}{k^n} < \  ? $$ Any help would be greatly appreciated. However a follow up to this is also I am unsure how to determine whether M is a uniformly integrable martingale or not. I have tried using the Doob-Kolmogorov inequality but it breaks down when I have attempted to show it holds for L-1 norm. Below is my attempt at trying to show that $M_n$ converges almost surely to a limit and thus is UI. use the Doob-Kolmogorov Inequality for  martingales but for $p=1$ , $$ \mathbb{P}\left( \max_{k \leq n} | M_{k} - 0 | > b  \right) \leq \dfrac{\mathbb{E}|M_n|}{b} \quad \text{ for } b > 0. $$ However we know that $\mathbb{E}|M_n| = \frac{n!}{k^n}$ therefore taking the limit of the above inequality and take $n \to \infty$ , $$\lim_{n \to \infty} \mathbb{P} \left( \max_{k \leq n} | M_{k} - 0 | > b \right) \leq \lim_{n \to \infty} \dfrac{\mathbb{E}|M_n|}{b} = \lim_{n \to \infty} \dfrac{n!}{k^n b} = 0 $$ However, I dont think it works in this case as the limit is not 0. Any help on this would be great too!","So I have been stuck on the following question that I stumbled across in a textbook. It is trying to show the following product process is a martingale. I know I have to prove adaptedness, integrability and the martingale equality but I am struggling to show the integrability as I have the working out below that I cannot seem to bound above to prove My working is as below, Any help would be greatly appreciated. However a follow up to this is also I am unsure how to determine whether M is a uniformly integrable martingale or not. I have tried using the Doob-Kolmogorov inequality but it breaks down when I have attempted to show it holds for L-1 norm. Below is my attempt at trying to show that converges almost surely to a limit and thus is UI. use the Doob-Kolmogorov Inequality for  martingales but for , However we know that therefore taking the limit of the above inequality and take , However, I dont think it works in this case as the limit is not 0. Any help on this would be great too!","M_n = n! \prod_{k=1}^{n} X_k \ \text{where } X_{k} \sim \text{Poi}(1/k) \text{ and } X_k \text{ are independent}   \mathbb{E}|M_n| < \infty 
\mathbb{E}|M_n| = \mathbb{E}\left| n! \prod_{k=1}^n X_k \right| = n! \cdot  \mathbb{E} \prod_{k=1}^{n} X_k = n! \prod_{k=1}^{n} \mathbb{E} X_k = n!  \cdot (\mathbb{E} X_k)^n =\dfrac{n!}{k^n} < \  ?  M_n p=1  \mathbb{P}\left( \max_{k \leq n} | M_{k} - 0 | > b  \right) \leq \dfrac{\mathbb{E}|M_n|}{b} \quad \text{ for } b > 0.
 \mathbb{E}|M_n| = \frac{n!}{k^n} n \to \infty \lim_{n \to \infty} \mathbb{P} \left( \max_{k \leq n} | M_{k} - 0 | > b \right) \leq \lim_{n \to \infty} \dfrac{\mathbb{E}|M_n|}{b} = \lim_{n \to \infty} \dfrac{n!}{k^n b} = 0 ","['real-analysis', 'probability', 'statistics', 'martingales']"
42,Calculating mean absolute deviation for Poisson distribution,Calculating mean absolute deviation for Poisson distribution,,"I'm trying to solve the ex 5.11 from ""Probability Essentials"", which asks to show that if X is Poisson(λ) then 𝐸{|𝑋−𝜆|}= $\frac{2\lambda^\lambda e^{-\lambda}}{(\lambda -1)!} $ . I've shown that 𝐸{|𝑋−𝜆|} = ${2\lambda^\lambda e^{-\lambda}} $ $\sum_{k=1}^\infty \left(\frac{k\lambda^k}{(k+\lambda)!}\right) $ but I'm having trouble calculating the sum of this series: $\sum_{k=1}^\infty \left(\frac{k\lambda^k}{(k+\lambda)!}\right) $ . Could somebody help? EDIT: This is what I have done 𝐸{|𝑋−𝜆|} = $\sum_{j=0}^\infty \left(\frac{|j-\lambda|\lambda^je^{-\lambda}}{j!}\right) $ = $\sum_{j=0}^\lambda \left(\frac{(\lambda - j)\lambda^je^{-\lambda}}{j!}\right)$ + $\sum_{j=\lambda+1}^\infty \left(\frac{(j-\lambda)\lambda^je^{-\lambda}}{j!}\right) $ = $\sum_{j=0}^\infty \left(\frac{(\lambda - j)\lambda^je^{-\lambda}}{j!}\right)$ - $\sum_{j=\lambda+1}^\infty \left(\frac{(\lambda-j)\lambda^je^{-\lambda}}{j!}\right) $ + $\sum_{j=\lambda+1}^\infty \left(\frac{(j-\lambda)\lambda^je^{-\lambda}}{j!}\right) $ = $2\sum_{j=\lambda+1}^\infty \left(\frac{(j-\lambda)\lambda^je^{-\lambda}}{j!}\right) $ = $2\sum_{k=1}^\infty \left(\frac{k\lambda^{k+\lambda}e^{-\lambda}}{(k+\lambda)!}\right) $ = $2 e^{-\lambda} \lambda^{\lambda}\sum_{k=1}^\infty \left(\frac{k\lambda^{k}}{(k+\lambda)!}\right) $ .","I'm trying to solve the ex 5.11 from ""Probability Essentials"", which asks to show that if X is Poisson(λ) then 𝐸{|𝑋−𝜆|}= . I've shown that 𝐸{|𝑋−𝜆|} = but I'm having trouble calculating the sum of this series: . Could somebody help? EDIT: This is what I have done 𝐸{|𝑋−𝜆|} = = + = - + = = = .",\frac{2\lambda^\lambda e^{-\lambda}}{(\lambda -1)!}  {2\lambda^\lambda e^{-\lambda}}  \sum_{k=1}^\infty \left(\frac{k\lambda^k}{(k+\lambda)!}\right)  \sum_{k=1}^\infty \left(\frac{k\lambda^k}{(k+\lambda)!}\right)  \sum_{j=0}^\infty \left(\frac{|j-\lambda|\lambda^je^{-\lambda}}{j!}\right)  \sum_{j=0}^\lambda \left(\frac{(\lambda - j)\lambda^je^{-\lambda}}{j!}\right) \sum_{j=\lambda+1}^\infty \left(\frac{(j-\lambda)\lambda^je^{-\lambda}}{j!}\right)  \sum_{j=0}^\infty \left(\frac{(\lambda - j)\lambda^je^{-\lambda}}{j!}\right) \sum_{j=\lambda+1}^\infty \left(\frac{(\lambda-j)\lambda^je^{-\lambda}}{j!}\right)  \sum_{j=\lambda+1}^\infty \left(\frac{(j-\lambda)\lambda^je^{-\lambda}}{j!}\right)  2\sum_{j=\lambda+1}^\infty \left(\frac{(j-\lambda)\lambda^je^{-\lambda}}{j!}\right)  2\sum_{k=1}^\infty \left(\frac{k\lambda^{k+\lambda}e^{-\lambda}}{(k+\lambda)!}\right)  2 e^{-\lambda} \lambda^{\lambda}\sum_{k=1}^\infty \left(\frac{k\lambda^{k}}{(k+\lambda)!}\right) ,"['calculus', 'probability', 'sequences-and-series']"
43,The expected max inner product between a random vector and the sum of random vectors,The expected max inner product between a random vector and the sum of random vectors,,"Given: Fix $n, k \in \mathbb{N}$ , we generate $n$ random vectors $X_i \in \mathbb{R}^k$ with $\|X_i\|_2 = 1$ . We want to compute: The expected max Euclidean inner product between a random vector and the sum of the random vectors, i.e., $$ \mathbb E \max_{i \in [n]} \langle X_i, \sum_{j \in [n]} X_j \rangle. $$ What I have tried: When $n = 2$ , $\langle X_1, X_1 + X_2 \rangle = \langle X_2, X_1 + X_2 \rangle = 1 + \langle X_1, X_2 \rangle$ ; $\mathbb E \langle X_1, X_2 \rangle = 0$ due to symmetry, and thus $\mathbb E \max_{i \in [n]} \langle X_i, \sum_{j \in [n]} X_j \rangle = 1$ . When $n \geq 3$ , it becomes unclear to me. Intuitively it might get larger as $n$ increases since we have more options, and would approach some limit, possibly related to $\| \sum_{j \in [n]} X_j \|$ . Simulations (1,000,000 trials) with $n = 3$ and $k = 2$ gives $1.5319441791547532 \pm 0.7104635978561801$ . Simulations (1,000,000 trials) with $n = 4$ and $k = 2$ gives $1.6632600316282025 \pm 0.9042611238131294$ . Simulations (1,000,000 trials) with $n = 5$ and $k = 2$ gives $1.9472506556915035 \pm 0.9787530817053821$ . Simulations (1,000,000 trials) with $n = 100$ and $k = 2$ gives $8.86393808099978 \pm 4.6286431725480295$ . Simulations (1,000,000 trials) with $n = 100$ and $k = 3$ gives $9.083624932218164 \pm 3.8388222137766603$ Simulations (1,000,000 trials) with $n = 10,000$ and $k = 2$ gives $88.64022827148438 \pm 46.3698844909668$ . Simulations (1,000,000 trials) with $n = 10,000$ and $k = 3$ gives $92.19273147896439 \pm 38.86980525584397$ . Simulations (1,000,000 trials) with $n = 1,000,000$ and $k = 2$ gives $886.809001225146 \pm 463.51510862527164$ Simulations (1,000,000 trials) with $n = 1,000,000$ and $k = 3$ gives $921.3826932102985 \pm 388.68428531397456$ Simulations (1,000,000 trials) with $n = 1,000,000$ and $k = 4$ gives $939.8279114022998 \pm 341.27201929744365$ It looks like the order is near $\sqrt{n}$ , and I consequently have a conjecture that it would be something close to $\sqrt{n}$ . I saw something $0.886$ for $k = 2$ , and I know $\sqrt{\pi} / 2 \approx 0.8862$ ; not sure it is related or not. Limit of the integral is the square root of pi over 2 https://en.wikipedia.org/wiki/Gaussian_integral I also suspect that it is related to the sphere-cylinder ratio; see, e.g., Volume of Region in 5D Space As @Sal pointed out, this is related to random walks (see, e.g., Expected Value of Random Walk ), and seemingly $$ \mathbb E \| \sum_{j \in [n]} X_j \| \approx \sqrt{\dfrac{2n}{k}} \dfrac{\Gamma(\frac{k+1}{2})}{\Gamma(\frac{k}{2})}, $$ which is $\sqrt{n} \frac{\Gamma(1.5)}{\Gamma(1)} = \frac{\sqrt{\pi}}{2}\sqrt{n} \approx 0.886227 \sqrt{n}$ for $k = 2$ as I suspected, $\sqrt{\frac{8}{3\pi}} \sqrt{n} \approx 0.921318 \sqrt{n}$ for $k = 3$ , $\frac{3}{4} \sqrt{\frac{\pi}2} \sqrt{n} \approx 0.939986 \sqrt{n}$ for $k = 4$ , and indeed approaches $\sqrt{n}$ as $k \to \infty$ . However, this only gives us the asymptotic results as $n \to \infty$ , while the cases for small $n$ are still unclear.","Given: Fix , we generate random vectors with . We want to compute: The expected max Euclidean inner product between a random vector and the sum of the random vectors, i.e., What I have tried: When , ; due to symmetry, and thus . When , it becomes unclear to me. Intuitively it might get larger as increases since we have more options, and would approach some limit, possibly related to . Simulations (1,000,000 trials) with and gives . Simulations (1,000,000 trials) with and gives . Simulations (1,000,000 trials) with and gives . Simulations (1,000,000 trials) with and gives . Simulations (1,000,000 trials) with and gives Simulations (1,000,000 trials) with and gives . Simulations (1,000,000 trials) with and gives . Simulations (1,000,000 trials) with and gives Simulations (1,000,000 trials) with and gives Simulations (1,000,000 trials) with and gives It looks like the order is near , and I consequently have a conjecture that it would be something close to . I saw something for , and I know ; not sure it is related or not. Limit of the integral is the square root of pi over 2 https://en.wikipedia.org/wiki/Gaussian_integral I also suspect that it is related to the sphere-cylinder ratio; see, e.g., Volume of Region in 5D Space As @Sal pointed out, this is related to random walks (see, e.g., Expected Value of Random Walk ), and seemingly which is for as I suspected, for , for , and indeed approaches as . However, this only gives us the asymptotic results as , while the cases for small are still unclear.","n, k \in \mathbb{N} n X_i \in \mathbb{R}^k \|X_i\|_2 = 1 
\mathbb E \max_{i \in [n]} \langle X_i, \sum_{j \in [n]} X_j \rangle.
 n = 2 \langle X_1, X_1 + X_2 \rangle = \langle X_2, X_1 + X_2 \rangle = 1 + \langle X_1, X_2 \rangle \mathbb E \langle X_1, X_2 \rangle = 0 \mathbb E \max_{i \in [n]} \langle X_i, \sum_{j \in [n]} X_j \rangle = 1 n \geq 3 n \| \sum_{j \in [n]} X_j \| n = 3 k = 2 1.5319441791547532 \pm 0.7104635978561801 n = 4 k = 2 1.6632600316282025 \pm 0.9042611238131294 n = 5 k = 2 1.9472506556915035 \pm 0.9787530817053821 n = 100 k = 2 8.86393808099978 \pm 4.6286431725480295 n = 100 k = 3 9.083624932218164 \pm 3.8388222137766603 n = 10,000 k = 2 88.64022827148438 \pm 46.3698844909668 n = 10,000 k = 3 92.19273147896439 \pm 38.86980525584397 n = 1,000,000 k = 2 886.809001225146 \pm 463.51510862527164 n = 1,000,000 k = 3 921.3826932102985 \pm 388.68428531397456 n = 1,000,000 k = 4 939.8279114022998 \pm 341.27201929744365 \sqrt{n} \sqrt{n} 0.886 k = 2 \sqrt{\pi} / 2 \approx 0.8862 
\mathbb E \| \sum_{j \in [n]} X_j \| \approx \sqrt{\dfrac{2n}{k}} \dfrac{\Gamma(\frac{k+1}{2})}{\Gamma(\frac{k}{2})},
 \sqrt{n} \frac{\Gamma(1.5)}{\Gamma(1)} = \frac{\sqrt{\pi}}{2}\sqrt{n} \approx 0.886227 \sqrt{n} k = 2 \sqrt{\frac{8}{3\pi}} \sqrt{n} \approx 0.921318 \sqrt{n} k = 3 \frac{3}{4} \sqrt{\frac{\pi}2} \sqrt{n} \approx 0.939986 \sqrt{n} k = 4 \sqrt{n} k \to \infty n \to \infty n","['probability', 'random-variables', 'expected-value']"
44,Orthant probability,Orthant probability,,"Let $(X_i)_{i \in \mathbb{N}} \sim \mathcal{N}(0,1)$ , where all variables are i.i.d. For the random vector $$\mathbf{X} = \left [ X_1, \ \ \ \ X_1 + X_2, \ \ \ \  X_2 + X_3, \ \ \ \  \cdots \ \ \ \   X_{n-2} + X_{n-1}, \ \ \ \ X_{n-1} \right ]^T,$$ how can I find $\mathbb{P}(\mathbf{X} \geq 0)?$ In other words, this is the probability that all components are non-negative. For small $n$ , the probabilities (although tedious) are doable. I was wondering if there is a general form, or bounds, for this probability. I know that orthant probabilities for $n >5$ do not have a closed form, but in this specific case I believe something can be done, since this is not a classic orthant problem. The tricky part about this is that the covariance matrix is non-invertible, so I cannot integrate joint densities. I would appreciate a detailed answer (if one exists). Thanks!","Let , where all variables are i.i.d. For the random vector how can I find In other words, this is the probability that all components are non-negative. For small , the probabilities (although tedious) are doable. I was wondering if there is a general form, or bounds, for this probability. I know that orthant probabilities for do not have a closed form, but in this specific case I believe something can be done, since this is not a classic orthant problem. The tricky part about this is that the covariance matrix is non-invertible, so I cannot integrate joint densities. I would appreciate a detailed answer (if one exists). Thanks!","(X_i)_{i \in \mathbb{N}} \sim \mathcal{N}(0,1) \mathbf{X} = \left [ X_1, \ \ \ \ X_1 + X_2, \ \ \ \  X_2 + X_3, \ \ \ \  \cdots \ \ \ \   X_{n-2} + X_{n-1}, \ \ \ \ X_{n-1} \right ]^T, \mathbb{P}(\mathbf{X} \geq 0)? n n >5",['probability']
45,MGF dominated by an exponential function,MGF dominated by an exponential function,,"Let $X$ be a mean-zero random variable whose moment generating function is bounded by a symmetric exponential function, e.g: $$ \mathbb{E}[e^{\lambda X}] \leq \exp(c|\lambda|) $$ with $c > 0$ . Question : Is it true that $X$ must be bounded ? I asked one of my professors this and he told me it was the case, but I am not sure if it is true and if so how it can be proved. For proofs like this it seems like one would taylor expand both sides and take $\lambda$ to $0$ to get some sort of bounds, but I don't think this method works here because $e^{c|\lambda|}$ decays slower than $e^{c\lambda^2}$ , and the variables for which the second holds are the sub-gaussians, which of course include some unbounded variables (e.g. the usual Gaussian). I would imagine instead one should look at $\lambda$ large, but I am not sure how to carry out the argument.","Let be a mean-zero random variable whose moment generating function is bounded by a symmetric exponential function, e.g: with . Question : Is it true that must be bounded ? I asked one of my professors this and he told me it was the case, but I am not sure if it is true and if so how it can be proved. For proofs like this it seems like one would taylor expand both sides and take to to get some sort of bounds, but I don't think this method works here because decays slower than , and the variables for which the second holds are the sub-gaussians, which of course include some unbounded variables (e.g. the usual Gaussian). I would imagine instead one should look at large, but I am not sure how to carry out the argument.","X 
\mathbb{E}[e^{\lambda X}] \leq \exp(c|\lambda|)
 c > 0 X \lambda 0 e^{c|\lambda|} e^{c\lambda^2} \lambda","['probability', 'moment-generating-functions']"
46,"$m$ people choosing $l$ elements out of $n$ independently, what is the probability of each element being chosen at least $k$ times?","people choosing  elements out of  independently, what is the probability of each element being chosen at least  times?",m l n k,"We have $m$ people and a set of $n$ elements. Each person chooses $l$ elements out of $n$ , independently from each other. What is the probability that each element is chosen at least $k$ times? Let $X_1, \dots X_n$ be the random variables counting how many times each element is chosen. I want to determine the density $\mathbb{P}(X_1=k_1, \dots X_n=k_n)$ , then I can sum over all values greater than $k$ to determine the probability. I am having a hard time trying to compute the combinations. The denominator of the probability is easy, because it's all possible combinations of $l$ elements out of $n$ , repeated $m$ times, due to independence: $$\binom{n}{l}^m$$ But how can I determine only those combinations, such that the elements are chosen exactly $k_1, \dots k_n$ times? I don't know if a simple formula exists for this. Example: Let's say we have $m=10$ people voting for $n=5$ parties. Each person is expressing $l=2$ votes (more precisely, they randomly choose $2$ different parties). What is the probability that each party gets at least $k=1$ vote? Addendum: I am not sure if it helps, but I also thought of formalizing this problem in a different way. Basically it's a random $m \times n$ binary matrix, with each row summing to $l$ and each column summing to a number $\geq k$ . I need to count all the possible matrices.","We have people and a set of elements. Each person chooses elements out of , independently from each other. What is the probability that each element is chosen at least times? Let be the random variables counting how many times each element is chosen. I want to determine the density , then I can sum over all values greater than to determine the probability. I am having a hard time trying to compute the combinations. The denominator of the probability is easy, because it's all possible combinations of elements out of , repeated times, due to independence: But how can I determine only those combinations, such that the elements are chosen exactly times? I don't know if a simple formula exists for this. Example: Let's say we have people voting for parties. Each person is expressing votes (more precisely, they randomly choose different parties). What is the probability that each party gets at least vote? Addendum: I am not sure if it helps, but I also thought of formalizing this problem in a different way. Basically it's a random binary matrix, with each row summing to and each column summing to a number . I need to count all the possible matrices.","m n l n k X_1, \dots X_n \mathbb{P}(X_1=k_1, \dots X_n=k_n) k l n m \binom{n}{l}^m k_1, \dots k_n m=10 n=5 l=2 2 k=1 m \times n l \geq k","['probability', 'combinatorics']"
47,A $2D$ random walk with step size $1$ starts on the edge of a disk of radius $r$. What is probability that the walk will return to the disk?,A  random walk with step size  starts on the edge of a disk of radius . What is probability that the walk will return to the disk?,2D 1 r,"Consider a two dimensional random walk with step size $1$ and each step in a random direction, with the angle $\theta$ uniformly distrbuted in $[0,2\pi)$ . The walk starts on the perimeter of a disk of radius $r$ . What is probability that the walk will ever return to the disk, in terms of $r$ ? By ""the walk will ever return to the disk"", I mean the walk will have a vertex on or within the perimeter of the disk, besides the initial one. I came up with this question when thinking about the fact that a two dimensional lattice walk will return to the origin with probability $1$ , as proved by Polya in 1921. I have not been able to find any reference that answers my question. Context:","Consider a two dimensional random walk with step size and each step in a random direction, with the angle uniformly distrbuted in . The walk starts on the perimeter of a disk of radius . What is probability that the walk will ever return to the disk, in terms of ? By ""the walk will ever return to the disk"", I mean the walk will have a vertex on or within the perimeter of the disk, besides the initial one. I came up with this question when thinking about the fact that a two dimensional lattice walk will return to the origin with probability , as proved by Polya in 1921. I have not been able to find any reference that answers my question. Context:","1 \theta [0,2\pi) r r 1","['probability', 'reference-request', 'random-walk']"
48,Probability of no friends meeting at a coffee house,Probability of no friends meeting at a coffee house,,"I came across what seems to be in my opinion a very challenging problem: A number n of friends each visit Old Slaughter’s coffee house independently and uniformly at random during their lunch break from noon to 1pm. Each leaves after δ hours (or at 1pm if that is sooner), where δ < 1/(n−1). Show that the probability that none of them meet inside is $(1−(n−1)\delta)^n$ . I've tried to consider the events $A_{i,j}$ that correspond to "" $i$ and $j$ meet inside the coffee house"", and by calling the arrival times of the $k$ -th person $U_k \sim Uniform([0,1])$ , I can indeed compute the $\mathbb{P}(A_{i,j}) = \mathbb{P}(|U_i - U_j| < \delta)$ by using the continuous version of the total probability formula and chopping up my integral in several pieces : $\mathbb{P}(A_{i,j}) = \int_0^1 \mathbb{P}(U_j - \delta < U_i < U_j + \delta | U_j = x)dx = \int_0^1\int_{max(0,x-\delta)}^{min(1,x+\delta)}dydx$ etc... But I don't think I'm on the right track because I've got no plan to go from $\mathbb{P}(A_{i,j})$ to $\mathbb{P}(\cap_{i<j}A_{i,j}^C)$ . By peeking at the Grimmet & Stirzaker solution manual I get the impression that it might have something to do with the ""order statistic"" but it all seems pretty cryptic to me, humble probability novice.","I came across what seems to be in my opinion a very challenging problem: A number n of friends each visit Old Slaughter’s coffee house independently and uniformly at random during their lunch break from noon to 1pm. Each leaves after δ hours (or at 1pm if that is sooner), where δ < 1/(n−1). Show that the probability that none of them meet inside is . I've tried to consider the events that correspond to "" and meet inside the coffee house"", and by calling the arrival times of the -th person , I can indeed compute the by using the continuous version of the total probability formula and chopping up my integral in several pieces : etc... But I don't think I'm on the right track because I've got no plan to go from to . By peeking at the Grimmet & Stirzaker solution manual I get the impression that it might have something to do with the ""order statistic"" but it all seems pretty cryptic to me, humble probability novice.","(1−(n−1)\delta)^n A_{i,j} i j k U_k \sim Uniform([0,1]) \mathbb{P}(A_{i,j}) = \mathbb{P}(|U_i - U_j| < \delta) \mathbb{P}(A_{i,j}) = \int_0^1 \mathbb{P}(U_j - \delta < U_i < U_j + \delta | U_j = x)dx = \int_0^1\int_{max(0,x-\delta)}^{min(1,x+\delta)}dydx \mathbb{P}(A_{i,j}) \mathbb{P}(\cap_{i<j}A_{i,j}^C)","['probability', 'uniform-distribution', 'order-statistics']"
49,Epsilon-Delta analysis for the Law of Large Numbers?,Epsilon-Delta analysis for the Law of Large Numbers?,,"Law of Large Numbers: For a sequence of independent and identically distributed random variables $X_1, X_2, X_3, ..., X_n$ , each with an expected value $E[X_i] = \mu$ and sample estimator $\overline{X_n}$ = $\frac{1}{n} \sum_{{i=1}}^{n} x_i$ : $$\lim_{{n \to \infty}} P\left( \left| \overline{X_n} - \mu \right| \geq \epsilon \right) = 0$$ My Question: Provided $X_1, X_2, X_3, ..., X_n$ are all iid and all come from some specific Probability Distribution Function $g(x; \theta)$ : What value of $n$ is required to ""probabilistically"" achieve a certain value of $\epsilon$ ? As an example, suppose I have 100 randomly generated iid data points. I believe these points  came from a Normal Distribution.  Suppose there was some ""magic"" way of knowing that these $n$ = 100 data points were actually generated from  a Normal Distribution with $\mu=5$ , $\sigma=5$ . Then, on average (i.e. if we had access to the infinite universe of all samples of size $n$ = 100 from a Normal Distribution with $\mu=5$ , $\sigma=5$ ) : For $n$ = 100, what would be the average value of $\epsilon$ ? Can this be answered using Cramer's Theorem of Large Deviations? https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_theorem_(large_deviations) ? Thanks! Note: For $n$ = 100 ,would this average value of $\epsilon$ vary for different Normal Distributions? e.g. ( $\mu=5$ , $\sigma=5$ ) vs ( $\mu=2$ , $\sigma=3$ ) For $n$ = 100, would this average value of $\epsilon$ vary for different Probability Distributions Functions? e.g.  Normal Distribution vs Exponential Distribution vs Gamma Distirbution, etc? Follow up Question: Applying Hoeffding's Inequality in Real Life","Law of Large Numbers: For a sequence of independent and identically distributed random variables , each with an expected value and sample estimator = : My Question: Provided are all iid and all come from some specific Probability Distribution Function : What value of is required to ""probabilistically"" achieve a certain value of ? As an example, suppose I have 100 randomly generated iid data points. I believe these points  came from a Normal Distribution.  Suppose there was some ""magic"" way of knowing that these = 100 data points were actually generated from  a Normal Distribution with , . Then, on average (i.e. if we had access to the infinite universe of all samples of size = 100 from a Normal Distribution with , ) : For = 100, what would be the average value of ? Can this be answered using Cramer's Theorem of Large Deviations? https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_theorem_(large_deviations) ? Thanks! Note: For = 100 ,would this average value of vary for different Normal Distributions? e.g. ( , ) vs ( , ) For = 100, would this average value of vary for different Probability Distributions Functions? e.g.  Normal Distribution vs Exponential Distribution vs Gamma Distirbution, etc? Follow up Question: Applying Hoeffding's Inequality in Real Life","X_1, X_2, X_3, ..., X_n E[X_i] = \mu \overline{X_n} \frac{1}{n} \sum_{{i=1}}^{n} x_i \lim_{{n \to \infty}} P\left( \left| \overline{X_n} - \mu \right| \geq \epsilon \right) = 0 X_1, X_2, X_3, ..., X_n g(x; \theta) n \epsilon n \mu=5 \sigma=5 n \mu=5 \sigma=5 n \epsilon n \epsilon \mu=5 \sigma=5 \mu=2 \sigma=3 n \epsilon","['probability', 'law-of-large-numbers']"
50,Equivalent characterization of the martingale property,Equivalent characterization of the martingale property,,"I saw in several books and articles that the following lemma has been used: Let $M_t$ be a stochastic process such that $\mathbb{E}[|M_t|] < \infty$ for all $t$ . Is that true that $M_t$ is a martingale (w.r.t its natural filtration) if and only if $\mathbb{E}[(M_{t_{n+1}} - M_{t_n})\prod_{k=1}^{n}f_k(M_{t_k})] = 0$ for all choices of $t_1 < t_2 < ... < t_{n+1}$ and for all choices of bounded measurable functions $f_1, ...., f_n$ . I wanted to know, is the above statement correct? Is there a reference for it? How does one prove such a statement? I know that it is true if we take some arbitrary $\mathcal{F}_{t_n}$ measurable R.V. instead of $\prod_{i=1}^{n}f_k(M_{t_k})$ , but why such a product is sufficient?","I saw in several books and articles that the following lemma has been used: Let be a stochastic process such that for all . Is that true that is a martingale (w.r.t its natural filtration) if and only if for all choices of and for all choices of bounded measurable functions . I wanted to know, is the above statement correct? Is there a reference for it? How does one prove such a statement? I know that it is true if we take some arbitrary measurable R.V. instead of , but why such a product is sufficient?","M_t \mathbb{E}[|M_t|] < \infty t M_t \mathbb{E}[(M_{t_{n+1}} - M_{t_n})\prod_{k=1}^{n}f_k(M_{t_k})] = 0 t_1 < t_2 < ... < t_{n+1} f_1, ...., f_n \mathcal{F}_{t_n} \prod_{i=1}^{n}f_k(M_{t_k})","['probability', 'probability-theory', 'probability-distributions', 'conditional-expectation', 'martingales']"
51,"Is $A \vert B$ an event, or is $P(A \vert B)$ an abuse of notation?","Is  an event, or is  an abuse of notation?",A \vert B P(A \vert B),"Suppose we have a probability space $(\Omega, \mathcal{F}, P)$ . Thus, when we write $P(A \vert B)$ , either $A \vert B$ is an element of the event space $\mathcal{F}$ (i.e. the domain of $P$ ), or this an abuse of notation. I suspect that this is simply an abuse of notation, but I'm looking for two things here: Am I interpreting (that is ""unabusing"") this notation correctly? Is it possible to view $A \vert B$ as an event, even if this isn't the standard way of thinking about it? Notational Abuse? Entertaining the second possibility (notational ""abuse"") I can easily imagine how to ""desugar"" this notation into something more formal. For instance, we could read $P(A \vert B)$ as $P_B(A)$ , where $P_B$ is a probability measure on a new/derived probability space $(B, \mathcal{F}_B, P_B)$ . That is, this new space has $B$ as its sample space and a different event space derived from our original event space. We can then define $P_B(A)$ in terms of our original probability measure $P$ as: $P_B(A) = \frac{P(A \cap B)}{P(B)}$ . I haven't yet worked out the details (e.g. What is $\mathcal{F}_B$ ? , Is $P_B$ a probability measure? , etc.), but I suspect they're not too difficult. Conditional Events? But, is it possible to view $A \mid B$ as an event (that is, some element of $\mathcal{F}$ )? This section of the Wikipedia article on Conditional Probability states that Conditional probability can be defined as the probability of a conditional event $A_B$ . it then proceeds to define the ""Goodman–Nguyen–Van Fraassen conditional event"", but I haven't been able to decipher this yet. Possible Counterexample However, there appear to be simple examples in which $A \mid B$ just can't be an event. For instance, suppose we have a sample space $\Omega = \{ TT, TH, HT, HH \}$ and we define a probability measure using the mass function $p : \Omega \to [0, 1]$ like so: $$ \begin{cases} p(TT) &= \frac{1}{6} \\ p(TH) &= \frac{1}{6} \\ p(HT) &= \frac{1}{6} \\ p(HH) &= \frac{1}{2} \end{cases} $$ We can think of this as some experiment involving flipping two coins that are biased in some strange way that makes double-heads more likely than other outcomes. Then the probability that ( $A$ ) both coins land heads up given ( $B$ ) the first coin lands heads up is $$ \begin{align*} P(A \mid B) &= \frac{P(A \cap B)}{P(B)} \\             &= \frac{P(\{ HH \} \cap \{ HT, HH \})}{P(\{ HT, HH \})} \\             &= \frac{P(\{ HH \})}{P(\{ HT, HH \})} \\             &= \frac{p(HH)}{p(HT) + p(HH)} \\             &= \frac{\frac{1}{2}}{\frac{1}{6} + \frac{1}{2}} \\             &= \frac{3}{4} \end{align*} $$ However, no subset of the sample space has probability $\frac{3}{4}$ , so $A \mid B$ can't be an event in the original probability space. Have I made a mistake in this example, or am I misunderstanding the basic idea of ""conditional events""?","Suppose we have a probability space . Thus, when we write , either is an element of the event space (i.e. the domain of ), or this an abuse of notation. I suspect that this is simply an abuse of notation, but I'm looking for two things here: Am I interpreting (that is ""unabusing"") this notation correctly? Is it possible to view as an event, even if this isn't the standard way of thinking about it? Notational Abuse? Entertaining the second possibility (notational ""abuse"") I can easily imagine how to ""desugar"" this notation into something more formal. For instance, we could read as , where is a probability measure on a new/derived probability space . That is, this new space has as its sample space and a different event space derived from our original event space. We can then define in terms of our original probability measure as: . I haven't yet worked out the details (e.g. What is ? , Is a probability measure? , etc.), but I suspect they're not too difficult. Conditional Events? But, is it possible to view as an event (that is, some element of )? This section of the Wikipedia article on Conditional Probability states that Conditional probability can be defined as the probability of a conditional event . it then proceeds to define the ""Goodman–Nguyen–Van Fraassen conditional event"", but I haven't been able to decipher this yet. Possible Counterexample However, there appear to be simple examples in which just can't be an event. For instance, suppose we have a sample space and we define a probability measure using the mass function like so: We can think of this as some experiment involving flipping two coins that are biased in some strange way that makes double-heads more likely than other outcomes. Then the probability that ( ) both coins land heads up given ( ) the first coin lands heads up is However, no subset of the sample space has probability , so can't be an event in the original probability space. Have I made a mistake in this example, or am I misunderstanding the basic idea of ""conditional events""?","(\Omega, \mathcal{F}, P) P(A \vert B) A \vert B \mathcal{F} P A \vert B P(A \vert B) P_B(A) P_B (B, \mathcal{F}_B, P_B) B P_B(A) P P_B(A) = \frac{P(A \cap B)}{P(B)} \mathcal{F}_B P_B A \mid B \mathcal{F} A_B A \mid B \Omega = \{ TT, TH, HT, HH \} p : \Omega \to [0, 1] 
\begin{cases}
p(TT) &= \frac{1}{6} \\
p(TH) &= \frac{1}{6} \\
p(HT) &= \frac{1}{6} \\
p(HH) &= \frac{1}{2}
\end{cases}
 A B 
\begin{align*}
P(A \mid B) &= \frac{P(A \cap B)}{P(B)} \\
            &= \frac{P(\{ HH \} \cap \{ HT, HH \})}{P(\{ HT, HH \})} \\
            &= \frac{P(\{ HH \})}{P(\{ HT, HH \})} \\
            &= \frac{p(HH)}{p(HT) + p(HH)} \\
            &= \frac{\frac{1}{2}}{\frac{1}{6} + \frac{1}{2}} \\
            &= \frac{3}{4}
\end{align*}
 \frac{3}{4} A \mid B",['probability']
52,Proving a math problem is only solvable numerically?,Proving a math problem is only solvable numerically?,,"This is a question I have always wondered about: Classical theorems in calculus (e.g. Extreme Value Theorem) tells us that for some function over a given interval, a set of inputs must exist such that the function reaches a maximum and a minimum. The aim of the game is now to determine if these inputs can be determined analytically or numerically Now, consider a system of Maximum Likelihood Equations In some cases, a given system of Maximum Likelihood Equations has an ""analytical"" solution : we can find a general relationship between the parameters of the probability distribution with respect to the random variables. This makes things very convenient - if this can be done, in the future, no matter what dataset we encounter, we can very quickly calculate the parameters for any dataset because we have found a closed form solution However, many times, this is not possible and we are required to solve numerically Thus, it makes me wonder, perhaps in the future, a ""cool mathematical trick"" would be discovered that would allow for this same problem to be solved analytically Yet, in most references I read (e.g. textbooks in probability/statistics) - I have never encountered a mathematical proof which shows that certain systems of Maximum Likelihood Equations fundamentally do not have closed form solutions. There is always this tone being implied that perhaps a closed form solution exists, perhaps it doesn't - but currently, we solve numerically. I was always curious to know if we can conclusively prove that a closed form solution is guaranteed to not exist. I have asked similar questions in the past (see references below) and have never been able to find an exact answer on this topic. I am now trying to reformulate my question in a more concise way: For a given system of maximum likelihood equations, is it possible to mathematically prove that an ""elementary solution"" will never exist ... no matter how much the field of mathematics ever progresses? Thanks! References Prove that an equation has no elementary solution How do we know that some System of Equations doesn't have an ""Analytical Solution""? Is it Possible that some Non-Analytically Integrable Functions might actually have Analytical Integrals? https://en.wikipedia.org/wiki/Liouville%27s_theorem_(differential_algebra)","This is a question I have always wondered about: Classical theorems in calculus (e.g. Extreme Value Theorem) tells us that for some function over a given interval, a set of inputs must exist such that the function reaches a maximum and a minimum. The aim of the game is now to determine if these inputs can be determined analytically or numerically Now, consider a system of Maximum Likelihood Equations In some cases, a given system of Maximum Likelihood Equations has an ""analytical"" solution : we can find a general relationship between the parameters of the probability distribution with respect to the random variables. This makes things very convenient - if this can be done, in the future, no matter what dataset we encounter, we can very quickly calculate the parameters for any dataset because we have found a closed form solution However, many times, this is not possible and we are required to solve numerically Thus, it makes me wonder, perhaps in the future, a ""cool mathematical trick"" would be discovered that would allow for this same problem to be solved analytically Yet, in most references I read (e.g. textbooks in probability/statistics) - I have never encountered a mathematical proof which shows that certain systems of Maximum Likelihood Equations fundamentally do not have closed form solutions. There is always this tone being implied that perhaps a closed form solution exists, perhaps it doesn't - but currently, we solve numerically. I was always curious to know if we can conclusively prove that a closed form solution is guaranteed to not exist. I have asked similar questions in the past (see references below) and have never been able to find an exact answer on this topic. I am now trying to reformulate my question in a more concise way: For a given system of maximum likelihood equations, is it possible to mathematically prove that an ""elementary solution"" will never exist ... no matter how much the field of mathematics ever progresses? Thanks! References Prove that an equation has no elementary solution How do we know that some System of Equations doesn't have an ""Analytical Solution""? Is it Possible that some Non-Analytically Integrable Functions might actually have Analytical Integrals? https://en.wikipedia.org/wiki/Liouville%27s_theorem_(differential_algebra)",,"['probability', 'optimization']"
53,Stochastic Process : what is the probability space?,Stochastic Process : what is the probability space?,,"According to my lecture notes, a stochastic process is a sequence $(X_n)_{n \in \mathbb N}$ of random variables defined on $(\Omega, \mathcal F, P)$ (which is a probability space). Then follows the definition of a filtration, and then the definition of the canonical filtration as $\mathcal F_n = \sigma(X_1, \dots, X_n)$ . I search on this site questions about filtration to have a better understanding of it, and I came accros this question . Although the answer makes sense, I don't understand how $\sigma(X_1, \dots, X_n)$ can be subset of $\mathcal F$ , because for me $(\Omega, \mathcal F)$ is the measurable space over which each $X_i$ is defind, but not the entire sequence of $X_i$ . This might be wrong, but then I don't know how to understand the definitions of my lecture.","According to my lecture notes, a stochastic process is a sequence of random variables defined on (which is a probability space). Then follows the definition of a filtration, and then the definition of the canonical filtration as . I search on this site questions about filtration to have a better understanding of it, and I came accros this question . Although the answer makes sense, I don't understand how can be subset of , because for me is the measurable space over which each is defind, but not the entire sequence of . This might be wrong, but then I don't know how to understand the definitions of my lecture.","(X_n)_{n \in \mathbb N} (\Omega, \mathcal F, P) \mathcal F_n = \sigma(X_1, \dots, X_n) \sigma(X_1, \dots, X_n) \mathcal F (\Omega, \mathcal F) X_i X_i","['probability', 'stochastic-processes']"
54,"Roll a fair 6−sided die until a 6 appears.Given that the first 6 occurs before the first 5, find the expected number of times the die was rolled","Roll a fair 6−sided die until a 6 appears.Given that the first 6 occurs before the first 5, find the expected number of times the die was rolled",,"I am trying to solve this problem but my answer is coming out to be 1.5 where the answer is 3. I even ran a simulation to verify it and its correct. I am not really sure whats wrong in my method and how can I correct it. I considered for getting my first 6 on Nth trials, without any 5's till now, I have probability of $$P(A^N) = \left(\frac{4}{6}\right)^{N-1} \times \frac{1}{6}$$ Now to get the expected number of rolls, $$\sum_{N=1}^{\infty} N \times \left(\frac{4}{6}\right)^{N-1} \times \frac{1}{6}$$ So, If we bring 1/6 outside, and consider x = 4/6 then the above equation is the 1st derivative of x/(1-x) $$\frac{d}{dx} \left( \frac{x}{1 - x} \right) = \frac{1}{{(1 - x)}^2}$$ Putting back the values and multiplying by 1/6 we get 1.5 (9/6). Where am I going wrong?","I am trying to solve this problem but my answer is coming out to be 1.5 where the answer is 3. I even ran a simulation to verify it and its correct. I am not really sure whats wrong in my method and how can I correct it. I considered for getting my first 6 on Nth trials, without any 5's till now, I have probability of Now to get the expected number of rolls, So, If we bring 1/6 outside, and consider x = 4/6 then the above equation is the 1st derivative of x/(1-x) Putting back the values and multiplying by 1/6 we get 1.5 (9/6). Where am I going wrong?",P(A^N) = \left(\frac{4}{6}\right)^{N-1} \times \frac{1}{6} \sum_{N=1}^{\infty} N \times \left(\frac{4}{6}\right)^{N-1} \times \frac{1}{6} \frac{d}{dx} \left( \frac{x}{1 - x} \right) = \frac{1}{{(1 - x)}^2},"['probability', 'conditional-probability']"
55,Prove function is a PDF,Prove function is a PDF,,"I'm trying to figure out how to prove $\theta y^{\theta} x^{-\theta -1}$ where $y>0, \theta>1, x\geq y$ is a PDF. So far I have $\int_{-\infty}^\infty \theta y^{\theta} x^{-\theta -1}dx=-\frac{y^\theta}{x^\theta}|_{-\infty}^\infty$ but don't know where to go from there. I know given the dependencies that the fraction will result in something at most 1, but don't how to make the integral equal to 1. Edit: I think I figured it out by changing the bounds. Since $x\geq y >0$ , then the bounds are from $y$ to $\infty$ . Thus $-\frac{y^\theta}{x^\theta}|_{y}^\infty=1$ . Any confirmation would be great. It is also trivial that the function is $\geq0$ , so it satisfies both properties to be a PDF.","I'm trying to figure out how to prove where is a PDF. So far I have but don't know where to go from there. I know given the dependencies that the fraction will result in something at most 1, but don't how to make the integral equal to 1. Edit: I think I figured it out by changing the bounds. Since , then the bounds are from to . Thus . Any confirmation would be great. It is also trivial that the function is , so it satisfies both properties to be a PDF.","\theta y^{\theta} x^{-\theta -1} y>0, \theta>1, x\geq y \int_{-\infty}^\infty \theta y^{\theta} x^{-\theta -1}dx=-\frac{y^\theta}{x^\theta}|_{-\infty}^\infty x\geq y >0 y \infty -\frac{y^\theta}{x^\theta}|_{y}^\infty=1 \geq0","['probability', 'statistics', 'density-function']"
56,Show that $\mathbb{P}(X>u)\leq\mathbb{E}(g(X)\mathbf{1}_{X>u})$ for all $u\in\mathbb{R}$ where $g(x)$ is nonnegative and weakly increasing.,Show that  for all  where  is nonnegative and weakly increasing.,\mathbb{P}(X>u)\leq\mathbb{E}(g(X)\mathbf{1}_{X>u}) u\in\mathbb{R} g(x),"I am trying to solve the following exercise: Let $X$ be an absolutely continuous random variable with density $f$ . Let $g$ be a non-negative non-decreasing function with $\mathbb{E}(g(X))=1$ . Show that $$\mathbb{P}(X>u)\leq \mathbb{E}(g(X)\mathbf{1}_{X>u}),\ \ \forall u\in\mathbb{R}.$$ I have the following attempt: since $\mathbb{E}g(X)=1$ , we have that $$1=\int_{-\infty}^{u}g(x)f(x)dx+\mathbb{E}(g(X)\mathbf{1}_{X>u})\leq g(u)\int_{-\infty}^{u}f(x)dx+\mathbb{E}(g(X)\mathbf{1}_{X>u})=g(u)\mathbb{P}(X\leq u)+\mathbb{E}(g(X)\mathbf{1}_{X>u}).$$ Write $\mathbb{P}(X\leq u)=1-\mathbb{P}(X>u)$ , we have that $$g(u)\mathbb{P}(X>u)\leq g(u)-1+\mathbb{E}(g(X)\mathbf{1}_{X>u}),$$ and thus $$\mathbb{P}(X>u)\leq 1-\dfrac{1}{g(u)}+\dfrac{\mathbb{E}(g(X)\mathbf{1}_{X>u})}{g(u)}\leq 1+\dfrac{\mathbb{E}(g(X)\mathbf{1}_{X>u})}{g(u)}.$$ I do not know how to push it further. Another attempt is to use Markov inequality. Since $X$ is absolutely continuous and $g$ is non-decreasing and nonnegative, we have $$\mathbb{P}(X>u)=\mathbb{P}(X\geq u)=\mathbb{P}(g(X)\geq g(u))\leq\dfrac{\mathbb{E}g(X)}{g(u)}=\dfrac{1}{g(u)}.$$ What have I missed?","I am trying to solve the following exercise: Let be an absolutely continuous random variable with density . Let be a non-negative non-decreasing function with . Show that I have the following attempt: since , we have that Write , we have that and thus I do not know how to push it further. Another attempt is to use Markov inequality. Since is absolutely continuous and is non-decreasing and nonnegative, we have What have I missed?","X f g \mathbb{E}(g(X))=1 \mathbb{P}(X>u)\leq \mathbb{E}(g(X)\mathbf{1}_{X>u}),\ \ \forall u\in\mathbb{R}. \mathbb{E}g(X)=1 1=\int_{-\infty}^{u}g(x)f(x)dx+\mathbb{E}(g(X)\mathbf{1}_{X>u})\leq g(u)\int_{-\infty}^{u}f(x)dx+\mathbb{E}(g(X)\mathbf{1}_{X>u})=g(u)\mathbb{P}(X\leq u)+\mathbb{E}(g(X)\mathbf{1}_{X>u}). \mathbb{P}(X\leq u)=1-\mathbb{P}(X>u) g(u)\mathbb{P}(X>u)\leq g(u)-1+\mathbb{E}(g(X)\mathbf{1}_{X>u}), \mathbb{P}(X>u)\leq 1-\dfrac{1}{g(u)}+\dfrac{\mathbb{E}(g(X)\mathbf{1}_{X>u})}{g(u)}\leq 1+\dfrac{\mathbb{E}(g(X)\mathbf{1}_{X>u})}{g(u)}. X g \mathbb{P}(X>u)=\mathbb{P}(X\geq u)=\mathbb{P}(g(X)\geq g(u))\leq\dfrac{\mathbb{E}g(X)}{g(u)}=\dfrac{1}{g(u)}.","['probability', 'probability-theory', 'analysis', 'random-variables', 'expected-value']"
57,Calculating variance for normal distribution,Calculating variance for normal distribution,,"A random variable is said to be normally distributed with parameters $\mu$ and $\sigma^2$ , if its density function is given by $$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x-\mu)^2}{2\sigma^2}}\;\;\;-\infty<x<\infty$$ . Now I need find the variance of the normal distribution i.e. the integral $$\int_{-\infty}^{\infty}(x-\mu)^2f(x)dx$$ . Now to prove we use the fact that $$\int_{-\infty}^{\infty}e^{-x^2}=\sqrt\pi.$$ I have proved the above identity using multivariable calculus, double integration, and polar coordinates. So now I consider $\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}\sigma}(x-\mu)^2e^{\frac{-(x-\mu)^2}{2\sigma^2}}dx$ = $\lim_{R\longrightarrow  \infty}\frac{1}{\sqrt{2\pi}\sigma}\int_{-R}^{R}(x-\mu)^2e^{\frac{-(x-\mu)^2}{2\sigma^2}}dx $ . Now I use substitution and take $y=\frac{x-\mu}{\sigma}$ . So I get $\lim_{R\longrightarrow  \infty}\frac{\sigma^2}{\sqrt{2\pi}}\int_{\frac{-R-\mu}{\sigma}}^{\frac{R-\mu}{\sigma}}y^2e^{\frac{-y^2}{2}}dy $ . Now I use integration by parts and get $$\lim_{R\longrightarrow  \infty}\frac{\sigma^2}{\sqrt{2\pi}}\left\{-e^{-\frac{y^2}{2}}y\Biggr|_{\frac{-R-\mu}{\sigma}}^{\frac{R-\mu}{\sigma}}   +\int_{\frac{-R-\mu}{\sigma}}^{\frac{R-\mu}{\sigma}}e^{\frac{-y^2}{2}}dy\right\} .$$ My question is how do I proceed now and use that $\int_{-\infty}^{\infty}e^{-x^2}=\sqrt\pi?$ . My main issue is that I want to compute the variance using the definition of improper integral i.e. the infinite limit of definite integrals.","A random variable is said to be normally distributed with parameters and , if its density function is given by . Now I need find the variance of the normal distribution i.e. the integral . Now to prove we use the fact that I have proved the above identity using multivariable calculus, double integration, and polar coordinates. So now I consider = . Now I use substitution and take . So I get . Now I use integration by parts and get My question is how do I proceed now and use that . My main issue is that I want to compute the variance using the definition of improper integral i.e. the infinite limit of definite integrals.",\mu \sigma^2 f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x-\mu)^2}{2\sigma^2}}\;\;\;-\infty<x<\infty \int_{-\infty}^{\infty}(x-\mu)^2f(x)dx \int_{-\infty}^{\infty}e^{-x^2}=\sqrt\pi. \int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}\sigma}(x-\mu)^2e^{\frac{-(x-\mu)^2}{2\sigma^2}}dx \lim_{R\longrightarrow  \infty}\frac{1}{\sqrt{2\pi}\sigma}\int_{-R}^{R}(x-\mu)^2e^{\frac{-(x-\mu)^2}{2\sigma^2}}dx  y=\frac{x-\mu}{\sigma} \lim_{R\longrightarrow  \infty}\frac{\sigma^2}{\sqrt{2\pi}}\int_{\frac{-R-\mu}{\sigma}}^{\frac{R-\mu}{\sigma}}y^2e^{\frac{-y^2}{2}}dy  \lim_{R\longrightarrow  \infty}\frac{\sigma^2}{\sqrt{2\pi}}\left\{-e^{-\frac{y^2}{2}}y\Biggr|_{\frac{-R-\mu}{\sigma}}^{\frac{R-\mu}{\sigma}}   +\int_{\frac{-R-\mu}{\sigma}}^{\frac{R-\mu}{\sigma}}e^{\frac{-y^2}{2}}dy\right\} . \int_{-\infty}^{\infty}e^{-x^2}=\sqrt\pi?,"['probability', 'probability-theory', 'probability-distributions', 'improper-integrals', 'normal-distribution']"
58,An urn with r red balls and b blue balls and a special way of taking the balls out until a single color is left,An urn with r red balls and b blue balls and a special way of taking the balls out until a single color is left,,"This is problem 2.13 b) in David Stirzaker's book: ""Elementary probability"". The urn contains r red balls and b blue balls and we are to take the balls out until a single color is left in the urn. The special way of taking the balls is as follows: We are to follow a process where we first grab a ball and take it out of the urn, register its color and then we randomly take balls from the urn until we take out one that is of different color. That ball of different color is RETURNED to the urn and we are to repeat the process until a color is left. Do note that when we start a new process, the first ball that we take will always be pulled out of the urn, so it is well defined to talk about the probability that a color is left (after a finite amount of steps, a color will be singled out in the urn). Now we are asked to find the probability that the color left in the urn is red. My main approach was to use double induction on the number of red and number of blue balls. One gets that when fixing the number of red balls to 1 and using induction to generalize the number of blue balls, the answer is 1/2. This is also true for the case of r=2: by induction on b we get 1/2. I got stuck on the inductive step when we generalize the number of red balls: using conditional probability a complicated sum is formed and I am not sure how to get 1/2 as the answer. Perhaps there is an easier way so I hope someone can share some ideas or even give the whole solution.","This is problem 2.13 b) in David Stirzaker's book: ""Elementary probability"". The urn contains r red balls and b blue balls and we are to take the balls out until a single color is left in the urn. The special way of taking the balls is as follows: We are to follow a process where we first grab a ball and take it out of the urn, register its color and then we randomly take balls from the urn until we take out one that is of different color. That ball of different color is RETURNED to the urn and we are to repeat the process until a color is left. Do note that when we start a new process, the first ball that we take will always be pulled out of the urn, so it is well defined to talk about the probability that a color is left (after a finite amount of steps, a color will be singled out in the urn). Now we are asked to find the probability that the color left in the urn is red. My main approach was to use double induction on the number of red and number of blue balls. One gets that when fixing the number of red balls to 1 and using induction to generalize the number of blue balls, the answer is 1/2. This is also true for the case of r=2: by induction on b we get 1/2. I got stuck on the inductive step when we generalize the number of red balls: using conditional probability a complicated sum is formed and I am not sure how to get 1/2 as the answer. Perhaps there is an easier way so I hope someone can share some ideas or even give the whole solution.",,"['probability', 'induction']"
59,Combinatorial proof for an integral,Combinatorial proof for an integral,,"The normalizing constant of the Dirichlet distribution implies that $$ \int_{\Delta_k} x_1^{n_1}\dots x_k^{n_k} dx_1 \dots dx_k = \frac{n_1! \dots n_k!}{(n_1+\dots+n_k+k-1)!} $$ where $$ \Delta_k = \{(x_1, \dots, x_k) \in [0,1]^k: x_1+\dots+x_k=1\} $$ for $n_1,\dots,n_k \in \mathbb N$ . Is there a combinatorial proof for this?",The normalizing constant of the Dirichlet distribution implies that where for . Is there a combinatorial proof for this?,"
\int_{\Delta_k} x_1^{n_1}\dots x_k^{n_k} dx_1 \dots dx_k = \frac{n_1! \dots n_k!}{(n_1+\dots+n_k+k-1)!}
 
\Delta_k = \{(x_1, \dots, x_k) \in [0,1]^k: x_1+\dots+x_k=1\}
 n_1,\dots,n_k \in \mathbb N","['probability', 'integration', 'combinatorics', 'reference-request', 'combinatorial-proofs']"
60,Parseval-Plancherel type identity for probability generating function,Parseval-Plancherel type identity for probability generating function,,"Assume that $f,g \in L^2(\mathbb R)$ and define the Fourier transform of $f$ by $$\hat{f}(\xi) = \int_{\mathbb R} \mathrm{e}^{-i\,x\,\xi}\, f(x)\,\mathrm{d}\xi, \quad \xi \in \mathbb R.$$ The well-known Parseval-Plancherel identity links the $2$ -norm of $f-g$ and the $2$ -norm of $\hat{f} - \hat{g}$ via $$\int_{\mathbb R} |f(x)-g(x)|^2\,\mathrm{d} x = \frac{1}{2\,\pi}\,\int_{\mathbb R} |\hat{f}(\xi)-\hat{g}(\xi)|^2\,\mathrm{d} \xi \label{1}\tag{1}.$$ Now suppose that $f = (f_0,f_1,\ldots) \in \mathcal{P}(\mathbb N)$ and $g = (g_0,g_1,\ldots) \in \mathcal{P}(\mathbb N)$ are probability distributions over $\mathbb N$ , and define for each $z \in (0,1)$ the probability generating function of $f \in \mathcal{P}(\mathbb N)$ by $$\hat{f}(z) = \sum_{n\geq 0} z^n\,f_n. $$ I am wondering if there exists a ""natural"" analog of the Parseval-Plancherel identity \eqref{1} involving the probability generating function. Unfortunately, I did not find any useful reference or related literature about this question. Edit: Motivated by the answer provided by Andrew, I am wondering if there exists a collection of functions (not necessarily a orthonormal basis) $\{\varphi_n(z)\}_{n \geq 0}$ such that $$f_n = \int_0^1 \hat{f}(z)\,\varphi_n(z)\,\mathrm{d} z \label{2}\tag{2}$$ for each $n \in \mathbb N$ . I want to view \eqref{2} as a sort of analog of the inverse Fourier transform (but now it involves the probability generating function instead).","Assume that and define the Fourier transform of by The well-known Parseval-Plancherel identity links the -norm of and the -norm of via Now suppose that and are probability distributions over , and define for each the probability generating function of by I am wondering if there exists a ""natural"" analog of the Parseval-Plancherel identity \eqref{1} involving the probability generating function. Unfortunately, I did not find any useful reference or related literature about this question. Edit: Motivated by the answer provided by Andrew, I am wondering if there exists a collection of functions (not necessarily a orthonormal basis) such that for each . I want to view \eqref{2} as a sort of analog of the inverse Fourier transform (but now it involves the probability generating function instead).","f,g \in L^2(\mathbb R) f \hat{f}(\xi) = \int_{\mathbb R} \mathrm{e}^{-i\,x\,\xi}\, f(x)\,\mathrm{d}\xi, \quad \xi \in \mathbb R. 2 f-g 2 \hat{f} - \hat{g} \int_{\mathbb R} |f(x)-g(x)|^2\,\mathrm{d} x = \frac{1}{2\,\pi}\,\int_{\mathbb R} |\hat{f}(\xi)-\hat{g}(\xi)|^2\,\mathrm{d} \xi \label{1}\tag{1}. f = (f_0,f_1,\ldots) \in \mathcal{P}(\mathbb N) g = (g_0,g_1,\ldots) \in \mathcal{P}(\mathbb N) \mathbb N z \in (0,1) f \in \mathcal{P}(\mathbb N) \hat{f}(z) = \sum_{n\geq 0} z^n\,f_n.  \{\varphi_n(z)\}_{n \geq 0} f_n = \int_0^1 \hat{f}(z)\,\varphi_n(z)\,\mathrm{d} z \label{2}\tag{2} n \in \mathbb N","['probability', 'generating-functions', 'moment-generating-functions', 'parsevals-identity']"
61,Suppose $X_{n}\sim bern(p_{n})$ and $\sum_{n}p_{n}<\infty$. Then $\frac{\sum_{k=1}^{n}X_{k}}{\sum_{n}p_{n}}$ does not converge to $1$ in probability,Suppose  and . Then  does not converge to  in probability,X_{n}\sim bern(p_{n}) \sum_{n}p_{n}<\infty \frac{\sum_{k=1}^{n}X_{k}}{\sum_{n}p_{n}} 1,"This may seem like a weird question. Suppose $X_{n}\sim \operatorname{bern}(p_{n})$ , independent and $\sum_{k=1}^{\infty}p_{k}<\infty$ . Then does $\dfrac{\sum_{k=1}^{n}X_{k}}{\sum_{k=1}^{n}p_{k}}$ converge to $1$ in probability? Well, the first thing that one notices is that $\operatorname{Var}(\dfrac{\sum_{k=1}^{n}X_{k}}{\sum_{k=1}^{n}p_{k}})$ does not go to $0$ which is the variance of $1$ . Ideally, if $(\sum_{k=1}^{n}X_{k})^{2}$ was uniformly integrable, we can conclude that as $\dfrac{\sum_{k=1}^{n}X_{k}}{\sum_{k=1}^{n}p_{n}}$ does not converge in $L^{2}$ to $1$ and hence it cannot converge in proability to $1$ (by using uniform integrability). The other argument is that $S_{n}$ should converge then to $\sum_{n}p_{n}$ which if we assume that it is an integer, then $S_{n}$ must also converge almost surely to it as $S_{n}$ 's are monotone but this is getting me nowhere. To try and conclude uniform integrability, I look at $\bigg(\sum_{k=1}^{n}X_{k}\bigg)^{4}$ and try and show that $\sup_{n}\mathbb{E}\left(\sum_{k=1}^{n}X_{k}\right)^{4}<\infty$ . But after opening the brackets and using multinomial theorem, I am getting complicated expressions. Well, $\sum_{k=1}^{n}X_{k}$ is uniformly bounded in $L^{2}$ but I also think that $\sum_{k=1}^{n}X_{k}$ will be bounded in $L^{4}$ but I am having trouble to show it rigorously. Any help? Is there an easier way to do this?","This may seem like a weird question. Suppose , independent and . Then does converge to in probability? Well, the first thing that one notices is that does not go to which is the variance of . Ideally, if was uniformly integrable, we can conclude that as does not converge in to and hence it cannot converge in proability to (by using uniform integrability). The other argument is that should converge then to which if we assume that it is an integer, then must also converge almost surely to it as 's are monotone but this is getting me nowhere. To try and conclude uniform integrability, I look at and try and show that . But after opening the brackets and using multinomial theorem, I am getting complicated expressions. Well, is uniformly bounded in but I also think that will be bounded in but I am having trouble to show it rigorously. Any help? Is there an easier way to do this?",X_{n}\sim \operatorname{bern}(p_{n}) \sum_{k=1}^{\infty}p_{k}<\infty \dfrac{\sum_{k=1}^{n}X_{k}}{\sum_{k=1}^{n}p_{k}} 1 \operatorname{Var}(\dfrac{\sum_{k=1}^{n}X_{k}}{\sum_{k=1}^{n}p_{k}}) 0 1 (\sum_{k=1}^{n}X_{k})^{2} \dfrac{\sum_{k=1}^{n}X_{k}}{\sum_{k=1}^{n}p_{n}} L^{2} 1 1 S_{n} \sum_{n}p_{n} S_{n} S_{n} \bigg(\sum_{k=1}^{n}X_{k}\bigg)^{4} \sup_{n}\mathbb{E}\left(\sum_{k=1}^{n}X_{k}\right)^{4}<\infty \sum_{k=1}^{n}X_{k} L^{2} \sum_{k=1}^{n}X_{k} L^{4},"['probability', 'probability-theory', 'probability-distributions', 'bernoulli-distribution']"
62,Show a convergence in probability using Markov inequality,Show a convergence in probability using Markov inequality,,"Let $(X)_{j\in \mathbb Z}$ be a discrete random process such that $$X_{j}= \theta X_{j-1}+ \epsilon_j, \quad (\epsilon_j) \overset{iid}{\sim} N(0,1), |\theta|<1 $$ How to show that $$P\left(\max_{1\leq j \leq n} |X_j| \geq n^{3/4} \right)\to 0, \quad (n \to \infty)$$ I'm a little unsure about doing the following and I still don't know how to complete it. $$P\left(\max_{1\leq j \leq n} |X_j| \geq n^{3/4} \right)= P\left(\left[\max_{1\leq j \leq n} |X_j|\right]^2 \geq n^{3/2} \right)\leq \frac{E\left( \left[\max_{1\leq j \leq n} |X_j|\right]^2 \right)}{n^{3/2}}$$ or $$P\left(\max_{1\leq j \leq n} |X_j| \geq n^{3/4} \right)= P\left(\left[\max_{1\leq j \leq n} |X_j|\right]^2 \geq n^{3/2} \right)= P\left(\max_{1\leq j \leq n} \{|X_j|^2\} \geq n^{3/2} \right) \leq  \frac{E\left( \max_{1\leq j \leq n} \{|X_j|^2\} \right)}{n^{3/2}}$$ In both cases, I use the Markov inequality. Which alternative is right and how to conclude? I know that in both cases it would suffice to show that the last Expectation is finite. How to justify this?","Let be a discrete random process such that How to show that I'm a little unsure about doing the following and I still don't know how to complete it. or In both cases, I use the Markov inequality. Which alternative is right and how to conclude? I know that in both cases it would suffice to show that the last Expectation is finite. How to justify this?","(X)_{j\in \mathbb Z} X_{j}= \theta X_{j-1}+ \epsilon_j, \quad (\epsilon_j) \overset{iid}{\sim} N(0,1), |\theta|<1  P\left(\max_{1\leq j \leq n} |X_j| \geq n^{3/4} \right)\to 0, \quad (n \to \infty) P\left(\max_{1\leq j \leq n} |X_j| \geq n^{3/4} \right)= P\left(\left[\max_{1\leq j \leq n} |X_j|\right]^2 \geq n^{3/2} \right)\leq \frac{E\left( \left[\max_{1\leq j \leq n} |X_j|\right]^2 \right)}{n^{3/2}} P\left(\max_{1\leq j \leq n} |X_j| \geq n^{3/4} \right)= P\left(\left[\max_{1\leq j \leq n} |X_j|\right]^2 \geq n^{3/2} \right)= P\left(\max_{1\leq j \leq n} \{|X_j|^2\} \geq n^{3/2} \right) \leq  \frac{E\left( \max_{1\leq j \leq n} \{|X_j|^2\} \right)}{n^{3/2}}","['probability', 'probability-theory', 'convergence-divergence']"
63,Which indicator variable best predicts a sampled value?,Which indicator variable best predicts a sampled value?,,"There are two players in this cooperative game.  I will first describe the basic game and then the real problem. Consider first that player 1 is to sample a single value from a standard normal distribution. Player 2 knows the distribution player 1 samples from. After player 1 has seen the value they have sampled they can send player 2 a single bit of information. That is a single indicator value. Player 1 and 2 have agreed beforehand what the indicator value will be indicating. To start with, player 2's prior belief is that player 1 has sampled from a standard normal distribution (because that is what was agreed). After receiving the single bit of information, player 2 will have some posterior belief about the distribution of the sampled value.  The goal of both players is to agree a strategy to minimize the variance of player 2's posterior belief on average. For example, they could agree that player 1 will tell player 2 the sign of the sampled value. If player 1 samples a positive value then player 2's posterior belief will now be the half normal distribution with variance $1 - \frac{2}{\pi}$ . The same variance would occur if player 1 had sampled a negative value Could they have chosen a different strategy which would have given a lower variance on average? Now to the real problem. In the real version player 1 is to sample from some arbitrary continuous distribution with finite support.  They can still confer to agree a strategy beforehand and player 2 still knows the distribution that player 1 will sample from. Can they do any better than agreeing that player 1 will send to player 2 an indicator that indicates if the sampled value is above the median or not? Bounty question If we constrain the support to be $[0, 1]$ , what distribution gives the largest possible gap from the result you get from sending an indicator that optimally  minimizes the variance of player 2's posterior belief and the variance of player 2's belief if they were simply informed whether the sampled value is above the median or not?","There are two players in this cooperative game.  I will first describe the basic game and then the real problem. Consider first that player 1 is to sample a single value from a standard normal distribution. Player 2 knows the distribution player 1 samples from. After player 1 has seen the value they have sampled they can send player 2 a single bit of information. That is a single indicator value. Player 1 and 2 have agreed beforehand what the indicator value will be indicating. To start with, player 2's prior belief is that player 1 has sampled from a standard normal distribution (because that is what was agreed). After receiving the single bit of information, player 2 will have some posterior belief about the distribution of the sampled value.  The goal of both players is to agree a strategy to minimize the variance of player 2's posterior belief on average. For example, they could agree that player 1 will tell player 2 the sign of the sampled value. If player 1 samples a positive value then player 2's posterior belief will now be the half normal distribution with variance . The same variance would occur if player 1 had sampled a negative value Could they have chosen a different strategy which would have given a lower variance on average? Now to the real problem. In the real version player 1 is to sample from some arbitrary continuous distribution with finite support.  They can still confer to agree a strategy beforehand and player 2 still knows the distribution that player 1 will sample from. Can they do any better than agreeing that player 1 will send to player 2 an indicator that indicates if the sampled value is above the median or not? Bounty question If we constrain the support to be , what distribution gives the largest possible gap from the result you get from sending an indicator that optimally  minimizes the variance of player 2's posterior belief and the variance of player 2's belief if they were simply informed whether the sampled value is above the median or not?","1 - \frac{2}{\pi} [0, 1]",['probability']
64,Unable to reason about game theory question involving marbles,Unable to reason about game theory question involving marbles,,"Here is the prompt: You choose to put 1-100 marbles in a bag. Your opponent also has the same option. Then we pick one marble from the bag. If your marble gets picked from the bag you get the amount of marbles you did not put in as your payout. So if you put in 20 marbles and your marble gets picked you get 80. What is the optimal number of marbles to put in? I understand the basics of game theory and Nash equilibria but I don't know how to reason about a question like this where the opponent has so many options. Here is what I have so far: If the opponent picks $k$ marbles then I should pick $j$ marbles to maximize my payout, where the payout is given by $$\frac{j}{j + k} (100 - j)$$ using the definition of expected value. However, clearly the value of $j$ that optimizes this function varies with respect to $k$ , the number of marbles the opponent picks. I am guessing I need to use the value of $k$ that the opponent would pick if they were to maximize their own payout, but I don't know how to find that. Any direction would be appreciated, thanks.","Here is the prompt: You choose to put 1-100 marbles in a bag. Your opponent also has the same option. Then we pick one marble from the bag. If your marble gets picked from the bag you get the amount of marbles you did not put in as your payout. So if you put in 20 marbles and your marble gets picked you get 80. What is the optimal number of marbles to put in? I understand the basics of game theory and Nash equilibria but I don't know how to reason about a question like this where the opponent has so many options. Here is what I have so far: If the opponent picks marbles then I should pick marbles to maximize my payout, where the payout is given by using the definition of expected value. However, clearly the value of that optimizes this function varies with respect to , the number of marbles the opponent picks. I am guessing I need to use the value of that the opponent would pick if they were to maximize their own payout, but I don't know how to find that. Any direction would be appreciated, thanks.",k j \frac{j}{j + k} (100 - j) j k k,"['probability', 'game-theory']"
65,Convergence in distribution of empirical cdf almost surely.,Convergence in distribution of empirical cdf almost surely.,,"Given i.i.d. random variables $\{X_n\}$ , define the empirical cdf $$ \hat{F}_n(\omega, x) = \frac{1}{n} \sum_{i = 1}^n \mathbf{1}_{X_i(\omega) \leq x}$$ where $\omega \in \Omega$ and $x \in \mathbb{R}$ . Show that $\hat{F}_n(\cdot, \cdot) \overset{d}{\to} F$ as $n \to \infty$ , i.e. $$ P\left(\left\{\omega : \lim_{n \to \infty} \hat{F}_n(\omega, x) = F(x)\text{ for every } x \in C(F)\right\}\right)=1$$ I know that each $\hat{F}_n(\cdot, x)$ is a random variable, and each $\hat{F}_n(\omega, \cdot)$ is a cdf. Furthermore, for fixed $x \in \mathbb{R}$ , we have $\hat{F}_n(\cdot, x) \overset{as}{\to} F$ by the Strong Law of Large Numbers; thus, each $$ P\left(\left\{\omega : \lim_{n \to \infty} \hat{F}_n(\omega, x) = F(x)\right\}\right) := P(A_x) = 1. $$ The problem is extending this to almost all $x$ . If I only had to deal with countably many $x$ , then I could just take a countable union of $A_x^c$ and obtain the result. It has been hinted that the problem can be reduced to this case, but I am still unable to figure out how.","Given i.i.d. random variables , define the empirical cdf where and . Show that as , i.e. I know that each is a random variable, and each is a cdf. Furthermore, for fixed , we have by the Strong Law of Large Numbers; thus, each The problem is extending this to almost all . If I only had to deal with countably many , then I could just take a countable union of and obtain the result. It has been hinted that the problem can be reduced to this case, but I am still unable to figure out how.","\{X_n\}  \hat{F}_n(\omega, x) = \frac{1}{n} \sum_{i = 1}^n \mathbf{1}_{X_i(\omega) \leq x} \omega \in \Omega x \in \mathbb{R} \hat{F}_n(\cdot, \cdot) \overset{d}{\to} F n \to \infty  P\left(\left\{\omega : \lim_{n \to \infty} \hat{F}_n(\omega, x) = F(x)\text{ for every } x \in C(F)\right\}\right)=1 \hat{F}_n(\cdot, x) \hat{F}_n(\omega, \cdot) x \in \mathbb{R} \hat{F}_n(\cdot, x) \overset{as}{\to} F  P\left(\left\{\omega : \lim_{n \to \infty} \hat{F}_n(\omega, x) = F(x)\right\}\right) := P(A_x) = 1.  x x A_x^c","['probability', 'probability-theory', 'probability-distributions', 'random-variables', 'probability-limit-theorems']"
66,Bagging a random sample with duplicate,Bagging a random sample with duplicate,,"this is a question posed by an engineer friend who had this probability problem arise at work. It goes like this, Say we have 5000 marbles. Of them, we know that 50 are duplicates. They are not necessarily all the same duplicates, and not necessarily perfect distinct pairs of duplicates. So, we know that there are 4950 distinct marbles and 50 marbles that are duplicates. Consider randomly bagging these marbles into 100 bags of 50 marbles. What is the probability that at least one bag has at least one pair of duplicate marbles? Another way of phrasing: What is the probability that after bagging, one or more bags contain two (or more) of the same marbles?  (You can simplify this if you would like as it is a bit vague, but an answer to the general question is preferred. So, if you want to assume that the duplicates are all the same or the duplicates all show up in perfect pairs.) I gave this a shot myself, but it is a bit more complicated than I am capable of solving with certainty. The engineers working with this problem seem to believe the probability is very low. However, I disagree and believe that the probability is considerably high. Any solution including a simplification of the problem to give a rough approximation (or what we hope would be an approximation) is great. I'd prefer to see steps and reasoning to justify your answer as well as teach me how to approach such a problem. Thank you.","this is a question posed by an engineer friend who had this probability problem arise at work. It goes like this, Say we have 5000 marbles. Of them, we know that 50 are duplicates. They are not necessarily all the same duplicates, and not necessarily perfect distinct pairs of duplicates. So, we know that there are 4950 distinct marbles and 50 marbles that are duplicates. Consider randomly bagging these marbles into 100 bags of 50 marbles. What is the probability that at least one bag has at least one pair of duplicate marbles? Another way of phrasing: What is the probability that after bagging, one or more bags contain two (or more) of the same marbles?  (You can simplify this if you would like as it is a bit vague, but an answer to the general question is preferred. So, if you want to assume that the duplicates are all the same or the duplicates all show up in perfect pairs.) I gave this a shot myself, but it is a bit more complicated than I am capable of solving with certainty. The engineers working with this problem seem to believe the probability is very low. However, I disagree and believe that the probability is considerably high. Any solution including a simplification of the problem to give a rough approximation (or what we hope would be an approximation) is great. I'd prefer to see steps and reasoning to justify your answer as well as teach me how to approach such a problem. Thank you.",,"['probability', 'recreational-mathematics', 'problem-solving', 'word-problem']"
67,Drawing from a uniform distribution,Drawing from a uniform distribution,,"Suppose I have an unlimited number of draws from a uniform distribution [0,1]. I want to keep on drawing until the sum of all my a values >=x (where 0<x<1). I want to find the probability that my score ends between x and 1 when trying to do this and when it does, the expected value of my score. Obviously the percentage chance of 'busting' is just 1 - the chance that my score is between x and 1. Intuitively, I'm tempted to say that when my score is between x and 1, on average it is halfway between x and 1, but I'm not 100% positive on that. I looked at some other forums and found some things that seemed like they were kind of similar, but I wasn't quite sure where to take it from there. I looked at this: Irwin Hall distribution with varying n right now this is where I'm at, I sum up from n=1 to n=inf $$\frac{(n-z) z^{n-1}}{n!}$$ but for each n here I need to subtract the probability that the sum made it to over 1 for that n and I need to figure out how to find out that out... for n draws out of a uniform distribution, I believe the probability that the sum is less than 1 is 1/n! but I can't just multiply what I have by 1/n! for each sum in the series because that would be 'overcounting' some of the area.","Suppose I have an unlimited number of draws from a uniform distribution [0,1]. I want to keep on drawing until the sum of all my a values >=x (where 0<x<1). I want to find the probability that my score ends between x and 1 when trying to do this and when it does, the expected value of my score. Obviously the percentage chance of 'busting' is just 1 - the chance that my score is between x and 1. Intuitively, I'm tempted to say that when my score is between x and 1, on average it is halfway between x and 1, but I'm not 100% positive on that. I looked at some other forums and found some things that seemed like they were kind of similar, but I wasn't quite sure where to take it from there. I looked at this: Irwin Hall distribution with varying n right now this is where I'm at, I sum up from n=1 to n=inf but for each n here I need to subtract the probability that the sum made it to over 1 for that n and I need to figure out how to find out that out... for n draws out of a uniform distribution, I believe the probability that the sum is less than 1 is 1/n! but I can't just multiply what I have by 1/n! for each sum in the series because that would be 'overcounting' some of the area.",\frac{(n-z) z^{n-1}}{n!},"['probability', 'uniform-distribution']"
68,Proof of Running Maximum of Brownian motion has continuous distribution without using the density or the fact that it is absolutely continuous,Proof of Running Maximum of Brownian motion has continuous distribution without using the density or the fact that it is absolutely continuous,,"I wanted to show that the running maximum say $\max_{t\in [0,1]}W_{t}$ has continuous distribution without taking help from the fact that it is absolutely continuous and has the distribution of $|W_{1}|$ . To clarify, I want to just use the definition of Brownian motion and to prove this. (I am always talking about the standard brownian motion here). Let us restrict to the set (of probability $1$ ) where $W_{t}$ is continuous. Then I want to show that $P(\{\max_{t\in[0,1]}W_{t}=x\})=0$ for each $x\in\Bbb{R}$ . So let us consider an enumeration of rationals $\{r_{n}\}_{n\in\Bbb{N}}$ in $[0,1]$ and define $Q_{n}=\{r_{1},...,r_{n}\}$ . We have that $\max_{t\in[0,1]}W_{t}=\sup_{r\in\Bbb{Q}\cap[0,1]}W_{r}$ . Also $\sup_{r\in\Bbb{Q}\cap[0,1]}W_{r}=x\implies W_{r}\leq x\,,\forall r\in\Bbb{Q}\cap[0,1]$ and for each $m\in\Bbb{N}$ , there exists $r\in\Bbb{Q}\cap[0,1]$ such that $W_{r}\in (x,x-\frac{1}{m}]$ . So I want to write $\displaystyle P(\sup_{r\in[0,1]\cap\Bbb{Q}} W_{r}=x)\leq P\big(\bigcap_{m\in\Bbb{N}}\bigcup_{n\in\Bbb{N}}\{\exists r\in Q_{n}\,: W_{r}\in (x-\frac{1}{m},x]\}\big)=\lim_{m\to\infty}P(\bigcup_{n\in\Bbb{N}}\{\exists r\in Q_{n}: W_{r}\in(x-\frac{1}{m},x])$ $$\leq \lim_{m\to\infty}P\bigg(\bigcup_{n\in\Bbb{N}}\big\{\exists r\in Q_{n}:W_{r}\in (x-\frac{1}{m2^{n}},x]\big\}\bigg)\leq \lim_{m\to\infty}\lim_{n\to\infty}\sum_{k=1}^{n}P(W_{r_{k}}\in(x-\frac{1}{m2^{n}},x])$$ $$\leq \lim_{m\to\infty}\lim_{n\to\infty}\sum_{k=1}^{n}\frac{n}{m2^{n}}=0$$ However I am having trouble making this precise and I obviously am making a mistake in the step of considering $\lim_{m\to\infty}\lim_{n\to\infty}\sum_{k=1}^{n}P(W_{r_{k}}\in(x-\frac{1}{m2^{n}},x])$ . Can anyone tell me how do I make it correct?. Also is there an easier way to do this? (I mean without using the fact that it has a density). Another way I tried to think of is that if I can show that $\max_{t\in[0,1]} W_{t}=\max_{r\in\Bbb{Q}\cap[0,1]}W_{r}$ (note that I mean $\max$ and not $\sup$ ) , then I can do it easily as for any finitely many points $r_{1},...,r_{n}$ , we have $P(\max_{r\in Q_{n}}W_{r}=x)\leq \sum_{k=1}^{n} P(W_{r_{k}}=x)=0$ for all $n$ . So I can proceed with the countable union and conclude what I want. But this is not necessarily true for continuous functions. I mean for example , $\sin(2x)$ achieves it's maximum at $x=\frac{\pi}{4}\in[0,1]$ and not at a rational point. EDIT: I also tried the following but I am unsure of it. Consider the sets $A_{n}=\bigcap_{m\in\Bbb{N}}\bigg\{\exists r\in Q_{n}:W_{r}\in(x-\frac{1}{m},x]\bigg\}$ . Then $A_{n}$ 's increase to $\bigcap_{m\in\Bbb{N}}\bigg\{\exists r\in \Bbb{Q}\cap[0,1]:W_{r}\in(x-\frac{1}{m},x]\bigg\}$ . But $\displaystyle P(A_{n})=P\bigg(\bigcap_{m\in\Bbb{N}}\bigg\{\bigcup_{r\in Q_{n}}W_{r}\in (x-\frac{1}{m},x]\bigg\}\bigg)=\lim_{m\to\infty}P\bigg(\bigcup_{r\in Q_{n}}\bigg\{W_{r}\in(x-\frac{1}{m},x]\bigg\}\bigg)$ (We could switch the intersection over $m$ to limit because for each fixed $n$ , the sets $\{\exists r\in Q_{n}:W_{r}\in(x-\frac{1}{m},x]\}$ is a decreasing sequence) $$\leq \lim_{m\to\infty} \sum_{k=1}^{n}\frac{1}{m}\frac{1}{\sqrt{r_{k}}}=0$$ (By using a simple bound for standard gaussian that $P(X\in(a,b))\leq (b-a)$ for $X\sim N(0,1))$ Also to note that the maximum is always $\geq 0$ a.s. . So we can ignore $r=0$ and consider $r\in\Bbb{Q}\cap(0,1]$ as $W_{0}=0$ a.s. So $P(A_{n})=0\,,\forall n\in\Bbb{N}$ and hence $P(\bigcup_{n\in\Bbb{N}}A_{n})=0$ which is what is required.","I wanted to show that the running maximum say has continuous distribution without taking help from the fact that it is absolutely continuous and has the distribution of . To clarify, I want to just use the definition of Brownian motion and to prove this. (I am always talking about the standard brownian motion here). Let us restrict to the set (of probability ) where is continuous. Then I want to show that for each . So let us consider an enumeration of rationals in and define . We have that . Also and for each , there exists such that . So I want to write However I am having trouble making this precise and I obviously am making a mistake in the step of considering . Can anyone tell me how do I make it correct?. Also is there an easier way to do this? (I mean without using the fact that it has a density). Another way I tried to think of is that if I can show that (note that I mean and not ) , then I can do it easily as for any finitely many points , we have for all . So I can proceed with the countable union and conclude what I want. But this is not necessarily true for continuous functions. I mean for example , achieves it's maximum at and not at a rational point. EDIT: I also tried the following but I am unsure of it. Consider the sets . Then 's increase to . But (We could switch the intersection over to limit because for each fixed , the sets is a decreasing sequence) (By using a simple bound for standard gaussian that for Also to note that the maximum is always a.s. . So we can ignore and consider as a.s. So and hence which is what is required.","\max_{t\in [0,1]}W_{t} |W_{1}| 1 W_{t} P(\{\max_{t\in[0,1]}W_{t}=x\})=0 x\in\Bbb{R} \{r_{n}\}_{n\in\Bbb{N}} [0,1] Q_{n}=\{r_{1},...,r_{n}\} \max_{t\in[0,1]}W_{t}=\sup_{r\in\Bbb{Q}\cap[0,1]}W_{r} \sup_{r\in\Bbb{Q}\cap[0,1]}W_{r}=x\implies W_{r}\leq x\,,\forall r\in\Bbb{Q}\cap[0,1] m\in\Bbb{N} r\in\Bbb{Q}\cap[0,1] W_{r}\in (x,x-\frac{1}{m}] \displaystyle P(\sup_{r\in[0,1]\cap\Bbb{Q}} W_{r}=x)\leq P\big(\bigcap_{m\in\Bbb{N}}\bigcup_{n\in\Bbb{N}}\{\exists r\in Q_{n}\,: W_{r}\in (x-\frac{1}{m},x]\}\big)=\lim_{m\to\infty}P(\bigcup_{n\in\Bbb{N}}\{\exists r\in Q_{n}: W_{r}\in(x-\frac{1}{m},x]) \leq \lim_{m\to\infty}P\bigg(\bigcup_{n\in\Bbb{N}}\big\{\exists r\in Q_{n}:W_{r}\in (x-\frac{1}{m2^{n}},x]\big\}\bigg)\leq \lim_{m\to\infty}\lim_{n\to\infty}\sum_{k=1}^{n}P(W_{r_{k}}\in(x-\frac{1}{m2^{n}},x]) \leq \lim_{m\to\infty}\lim_{n\to\infty}\sum_{k=1}^{n}\frac{n}{m2^{n}}=0 \lim_{m\to\infty}\lim_{n\to\infty}\sum_{k=1}^{n}P(W_{r_{k}}\in(x-\frac{1}{m2^{n}},x]) \max_{t\in[0,1]} W_{t}=\max_{r\in\Bbb{Q}\cap[0,1]}W_{r} \max \sup r_{1},...,r_{n} P(\max_{r\in Q_{n}}W_{r}=x)\leq \sum_{k=1}^{n} P(W_{r_{k}}=x)=0 n \sin(2x) x=\frac{\pi}{4}\in[0,1] A_{n}=\bigcap_{m\in\Bbb{N}}\bigg\{\exists r\in Q_{n}:W_{r}\in(x-\frac{1}{m},x]\bigg\} A_{n} \bigcap_{m\in\Bbb{N}}\bigg\{\exists r\in \Bbb{Q}\cap[0,1]:W_{r}\in(x-\frac{1}{m},x]\bigg\} \displaystyle P(A_{n})=P\bigg(\bigcap_{m\in\Bbb{N}}\bigg\{\bigcup_{r\in Q_{n}}W_{r}\in (x-\frac{1}{m},x]\bigg\}\bigg)=\lim_{m\to\infty}P\bigg(\bigcup_{r\in Q_{n}}\bigg\{W_{r}\in(x-\frac{1}{m},x]\bigg\}\bigg) m n \{\exists r\in Q_{n}:W_{r}\in(x-\frac{1}{m},x]\} \leq \lim_{m\to\infty} \sum_{k=1}^{n}\frac{1}{m}\frac{1}{\sqrt{r_{k}}}=0 P(X\in(a,b))\leq (b-a) X\sim N(0,1)) \geq 0 r=0 r\in\Bbb{Q}\cap(0,1] W_{0}=0 P(A_{n})=0\,,\forall n\in\Bbb{N} P(\bigcup_{n\in\Bbb{N}}A_{n})=0","['probability', 'probability-theory', 'probability-distributions', 'solution-verification', 'brownian-motion']"
69,Minimizing the expected time to get a string of heads with biased coins,Minimizing the expected time to get a string of heads with biased coins,,"Imagine you have $n$ coins. Coin $i$ has a probability $p_i$ to get heads. You can choose these probabilities so long as $\sum_{i=1}^n p_i = \alpha$ for some constant $\alpha \leq n$ . Now that you've chosen the probabilities, you must play a game. Flip coin 1; if it comes up tails, you must restart the game. However, if it comes up heads, flip coin 2; if it comes up tails, you must restart the game. To win the game you must flip $n$ heads in a row. You want to assign the probabilities $p_i$ such that the expected number of coin flips is minimized. Note that if the question were to assign $p_i$ such that the probability of winning is maximized, a standard result is to choose $p_i = \frac{\alpha}{n}$ for all $i$ . Formally, this problem can be stated as: $$\min_{p_i} E_n \text{ where } E_n = \left(n \prod_{i=1}^n p_i + \sum_{i=1}^n (E_n + i) \ p_1 p_2 \ldots p_{i-1} (1 - p_i)\right)$$ With the following constraints: $$\sum_{i=1}^n p_i = \alpha \text{ and } p_i \in [0, 1]$$ In the case of $n=2$ this is easy, we have $p_2 = \alpha - p_1$ and: $$E_2 = 2 p_1 (\alpha - p_1) + (E_2 + 1)(1 - p_1) + (E_2 + 2)p_1(1 - \alpha + p_1)$$ Solving for $E_2$ and setting $\frac{\partial E_2}{\partial p_1} = 0$ , we eventually find: $$p_1 = \sqrt{1 + \alpha} - 1$$ For $\alpha = 1$ , this gives $p_1 \approx 0.414$ . This agrees with intuition - instead of doling out the probabilities equally, you should favor lower probabilities for low indices $i$ and higher probabilities for high indices $i$ . When $\alpha \geq 2$ , we would expect the formula to return $p_1 \geq 1$ , since $p_1 = p_2 = 1$ is optimal, but for some reason it does not do this. However, it already becomes very difficult when $n=3$ . Is there a general way to attack this problem? I'm able to derive the following: $$E_n = \frac{1 + p_1 + p_1 p_2 + \ldots + p_1 p_2 \cdots p_{n-1}}{p_1 p_2 \cdots p_n}$$ Using the method of Lagrange multipliters, I can get the following $n+1$ nonlinear equations in $n+1$ variables: $$\frac{\partial E_n}{\partial p_i} = -\frac{1 + p_1 + p_1 p_2 + \ldots + p_1 p_2 \cdots p_{i-1}}{p_1 p_2 \cdots p_{i-1} p_i^2 p_{i+1} \cdots p_n} + \lambda = 0$$ $$\frac{\partial E_n}{\partial \lambda} = p_1 + p_2 + \ldots + p_n - \alpha = 0$$ But I don't see how to solve this set of nonlinear equations.","Imagine you have coins. Coin has a probability to get heads. You can choose these probabilities so long as for some constant . Now that you've chosen the probabilities, you must play a game. Flip coin 1; if it comes up tails, you must restart the game. However, if it comes up heads, flip coin 2; if it comes up tails, you must restart the game. To win the game you must flip heads in a row. You want to assign the probabilities such that the expected number of coin flips is minimized. Note that if the question were to assign such that the probability of winning is maximized, a standard result is to choose for all . Formally, this problem can be stated as: With the following constraints: In the case of this is easy, we have and: Solving for and setting , we eventually find: For , this gives . This agrees with intuition - instead of doling out the probabilities equally, you should favor lower probabilities for low indices and higher probabilities for high indices . When , we would expect the formula to return , since is optimal, but for some reason it does not do this. However, it already becomes very difficult when . Is there a general way to attack this problem? I'm able to derive the following: Using the method of Lagrange multipliters, I can get the following nonlinear equations in variables: But I don't see how to solve this set of nonlinear equations.","n i p_i \sum_{i=1}^n p_i = \alpha \alpha \leq n n p_i p_i p_i = \frac{\alpha}{n} i \min_{p_i} E_n \text{ where } E_n = \left(n \prod_{i=1}^n p_i + \sum_{i=1}^n (E_n + i) \ p_1 p_2 \ldots p_{i-1} (1 - p_i)\right) \sum_{i=1}^n p_i = \alpha \text{ and } p_i \in [0, 1] n=2 p_2 = \alpha - p_1 E_2 = 2 p_1 (\alpha - p_1) + (E_2 + 1)(1 - p_1) + (E_2 + 2)p_1(1 - \alpha + p_1) E_2 \frac{\partial E_2}{\partial p_1} = 0 p_1 = \sqrt{1 + \alpha} - 1 \alpha = 1 p_1 \approx 0.414 i i \alpha \geq 2 p_1 \geq 1 p_1 = p_2 = 1 n=3 E_n = \frac{1 + p_1 + p_1 p_2 + \ldots + p_1 p_2 \cdots p_{n-1}}{p_1 p_2 \cdots p_n} n+1 n+1 \frac{\partial E_n}{\partial p_i} = -\frac{1 + p_1 + p_1 p_2 + \ldots + p_1 p_2 \cdots p_{i-1}}{p_1 p_2 \cdots p_{i-1} p_i^2 p_{i+1} \cdots p_n} + \lambda = 0 \frac{\partial E_n}{\partial \lambda} = p_1 + p_2 + \ldots + p_n - \alpha = 0","['probability', 'statistics', 'optimization']"
70,Probability of choosing the right coin given the number of tosses,Probability of choosing the right coin given the number of tosses,,"I'm trying to solve the following problem There are two indistinguishable coins, one of them is balanced and the other one is biased in a way that the probability of getting heads after a toss is twice the probability of getting tails after a toss. We choose one of them and toss it repeatedly. Find the probability of having chosen the balanced coin if i) The coin was tossed $15$ times and $5$ of them were heads. ii) We needed $15$ tosses to get $5$ heads. Here is my attempt: i) Let $B$ be the event of choosing the balanced coin and let $D$ be the event of choosing the coin with ""probability of heads=double of probability of tails"". Consider the random variable $X$ counting the number of heads obtained. We have $$\mathbb{P}(X=5\mid B)=\binom{15}{5}\left (\frac{1}{2}\right )^5\left (\frac{1}{2}\right )^{15-5}=\frac{3003}{2^{15}}$$ because if $B$ was chosen then $X\sim \operatorname{Bin}\left (15,\frac{1}{2}\right )$ and $$\mathbb{P}(X=5\mid D)=\binom{15}{5}\left (\frac{2}{3}\right )^5\left (\frac{1}{3}\right )^{15-5}=\frac{96096}{3^{15}}$$ because if $D$ was chosen then $X\sim \operatorname{Bin}\left (15,\frac{2}{3}\right )$ . We also have $\mathbb{P}(B)=\mathbb{P}(D)=\frac{1}{2}$ because the coins are indistinguishable. Hence using Bayes' Theorem we find $$\mathbb{P}(B\mid X=5)=\frac{\mathbb{P}(X=5\mid B)\cdot \mathbb{P}(B)}{\mathbb{P}(X=5\mid B)\cdot \mathbb{P}(B)+\mathbb{P}(X=5\mid D)\cdot \mathbb{P}(D)}=\frac{\frac{3003}{2^{15}}}{\frac{96096}{3^{15}}}\approx 0.93$$ as the desired probability. ii) Let $B$ be the event of choosing the balanced coin and let $D$ be the event of choosing the coin with ""probability of heads=double of probability of tails"". Consider the random variable $X$ counting the number of tosses needed to obtain $5$ heads. We have $$\mathbb{P}(X=15\mid B)=\dbinom{15-1}{5-1}\left (\dfrac{1}{2}\right )^5\left (\dfrac{1}{2}\right )^{15-5}=\dbinom{14}{4}\dfrac{1}{2^{15}}$$ because if $B$ was chosen then $X\sim \operatorname{Pa}\left (\frac{1}{2},5\right )$ and $$\mathbb{P}(X=15\mid D)=\dbinom{15-1}{5-1}\left (\dfrac{2}{3}\right )^5\left (\dfrac{1}{3}\right )^{15-5}=\dbinom{14}{4}\dfrac{2^5}{3^{15}}$$ because if $D$ was chosen then $X\sim \operatorname{Pa}\left (\frac{1}{3},5\right )$ . Then similar computations give the same answer as the first item. I would like to know if my solution is correct.","I'm trying to solve the following problem There are two indistinguishable coins, one of them is balanced and the other one is biased in a way that the probability of getting heads after a toss is twice the probability of getting tails after a toss. We choose one of them and toss it repeatedly. Find the probability of having chosen the balanced coin if i) The coin was tossed times and of them were heads. ii) We needed tosses to get heads. Here is my attempt: i) Let be the event of choosing the balanced coin and let be the event of choosing the coin with ""probability of heads=double of probability of tails"". Consider the random variable counting the number of heads obtained. We have because if was chosen then and because if was chosen then . We also have because the coins are indistinguishable. Hence using Bayes' Theorem we find as the desired probability. ii) Let be the event of choosing the balanced coin and let be the event of choosing the coin with ""probability of heads=double of probability of tails"". Consider the random variable counting the number of tosses needed to obtain heads. We have because if was chosen then and because if was chosen then . Then similar computations give the same answer as the first item. I would like to know if my solution is correct.","15 5 15 5 B D X \mathbb{P}(X=5\mid B)=\binom{15}{5}\left (\frac{1}{2}\right )^5\left (\frac{1}{2}\right )^{15-5}=\frac{3003}{2^{15}} B X\sim \operatorname{Bin}\left (15,\frac{1}{2}\right ) \mathbb{P}(X=5\mid D)=\binom{15}{5}\left (\frac{2}{3}\right )^5\left (\frac{1}{3}\right )^{15-5}=\frac{96096}{3^{15}} D X\sim \operatorname{Bin}\left (15,\frac{2}{3}\right ) \mathbb{P}(B)=\mathbb{P}(D)=\frac{1}{2} \mathbb{P}(B\mid X=5)=\frac{\mathbb{P}(X=5\mid B)\cdot \mathbb{P}(B)}{\mathbb{P}(X=5\mid B)\cdot \mathbb{P}(B)+\mathbb{P}(X=5\mid D)\cdot \mathbb{P}(D)}=\frac{\frac{3003}{2^{15}}}{\frac{96096}{3^{15}}}\approx 0.93 B D X 5 \mathbb{P}(X=15\mid B)=\dbinom{15-1}{5-1}\left (\dfrac{1}{2}\right )^5\left (\dfrac{1}{2}\right )^{15-5}=\dbinom{14}{4}\dfrac{1}{2^{15}} B X\sim \operatorname{Pa}\left (\frac{1}{2},5\right ) \mathbb{P}(X=15\mid D)=\dbinom{15-1}{5-1}\left (\dfrac{2}{3}\right )^5\left (\dfrac{1}{3}\right )^{15-5}=\dbinom{14}{4}\dfrac{2^5}{3^{15}} D X\sim \operatorname{Pa}\left (\frac{1}{3},5\right )","['probability', 'probability-theory', 'solution-verification']"
71,Bayesian MAP inference with change of variables theorem,Bayesian MAP inference with change of variables theorem,,"Question: I want to do a Bayesian inference on a constrained variable. I want to use change of variables theorem to remove the constraint, before doing the Bayesian inference. How does the prior on the variable should change? Context: Say, I have a likelihood function, $P(y|\theta_a, \theta_b)$ . Further suppose that the parameter $\theta_a$ is unconstrained, yet the parameter $\theta_b$ is constrained such that $\theta_b \geq 0$ (non-negativity constraint). MLE solution is not very attractive, so I decided to use MAP estimate. To do so, we can use Bayes' rule as follows: $P(\theta_a, \theta_b|y) \propto P(y|\theta_a, \theta_b)P(\theta_a)P(\theta_b)$ Here, I am assuming $P(\theta_a, \theta_b) = P(\theta_a)P(\theta_b)$ , independence between the variables. Assuming we have a specific prior distribution for $\theta_a$ (not important for this question), but we want to use a uninformative uniform prior on $\theta_b$ , such that $p(\theta_b) \propto 1$ . Then the posterior reduces to $P(\theta_a, \theta_b|y) \propto P(y|\theta_a, \theta_b)P(\theta_a)$ Then the MAP solution can be obtained by solving the following optimization problem. $\hat{\theta}_a, \hat{\theta}_b = argmax_{\theta_a, \theta_b} L(\theta_a, \theta_b|y) + L(\theta_a)$ subject to $\theta_b \geq 0$ Where $L(\theta_a, \theta_b|y), L(\theta_a)$ are log-likelihoods. Now, I want to solve the above constrained optimization problem with an unconstrained optimization algorithm. So, I can change the variable, such that $\theta_b = e^{\theta'_b}$ . with this change of variable, we solve a different unconstrained optimization problem, $\hat{\theta}_a, \hat{\theta}'_b = argmax_{\theta_a, \theta'_b} L(\theta_a, e^{\theta'_b}|y) + L(\theta_a)$ The optimum $\hat{\theta}_b$ can be recovered by $\hat{\theta}_b = e^{\hat{\theta}_b'}$ The question is, don't we have to change the variable for the $p(\theta_b)$ ? Change of variable theorem will yield a different prior for $\theta'_b$ , like $p_{\theta'_b}(\theta'_b) = p_{\theta_b}(e^{\theta'_b})\lvert \frac{d\theta_b}{d\theta'_b} \rvert \propto e^{\theta'_b}$ ? Then adding the log-likelihood to the above unconstrained optimization problem will yield a different solution. I cannot seem to find what is wrong with these two trains of thoughts: one without changing the variable for $p(\theta_b)$ and one with the change of variable for $p(\theta_b)$ . They'll clearly yield different solutions. Why?","Question: I want to do a Bayesian inference on a constrained variable. I want to use change of variables theorem to remove the constraint, before doing the Bayesian inference. How does the prior on the variable should change? Context: Say, I have a likelihood function, . Further suppose that the parameter is unconstrained, yet the parameter is constrained such that (non-negativity constraint). MLE solution is not very attractive, so I decided to use MAP estimate. To do so, we can use Bayes' rule as follows: Here, I am assuming , independence between the variables. Assuming we have a specific prior distribution for (not important for this question), but we want to use a uninformative uniform prior on , such that . Then the posterior reduces to Then the MAP solution can be obtained by solving the following optimization problem. subject to Where are log-likelihoods. Now, I want to solve the above constrained optimization problem with an unconstrained optimization algorithm. So, I can change the variable, such that . with this change of variable, we solve a different unconstrained optimization problem, The optimum can be recovered by The question is, don't we have to change the variable for the ? Change of variable theorem will yield a different prior for , like ? Then adding the log-likelihood to the above unconstrained optimization problem will yield a different solution. I cannot seem to find what is wrong with these two trains of thoughts: one without changing the variable for and one with the change of variable for . They'll clearly yield different solutions. Why?","P(y|\theta_a, \theta_b) \theta_a \theta_b \theta_b \geq 0 P(\theta_a, \theta_b|y) \propto P(y|\theta_a, \theta_b)P(\theta_a)P(\theta_b) P(\theta_a, \theta_b) = P(\theta_a)P(\theta_b) \theta_a \theta_b p(\theta_b) \propto 1 P(\theta_a, \theta_b|y) \propto P(y|\theta_a, \theta_b)P(\theta_a) \hat{\theta}_a, \hat{\theta}_b = argmax_{\theta_a, \theta_b} L(\theta_a, \theta_b|y) + L(\theta_a) \theta_b \geq 0 L(\theta_a, \theta_b|y), L(\theta_a) \theta_b = e^{\theta'_b} \hat{\theta}_a, \hat{\theta}'_b = argmax_{\theta_a, \theta'_b} L(\theta_a, e^{\theta'_b}|y) + L(\theta_a) \hat{\theta}_b \hat{\theta}_b = e^{\hat{\theta}_b'} p(\theta_b) \theta'_b p_{\theta'_b}(\theta'_b) = p_{\theta_b}(e^{\theta'_b})\lvert \frac{d\theta_b}{d\theta'_b} \rvert \propto e^{\theta'_b} p(\theta_b) p(\theta_b)","['probability', 'optimization', 'bayesian', 'parameter-estimation', 'bayes-theorem']"
72,$Y\sim Poi(\lambda)$ trials are conducted with a probability of success $p$. Find the distribution of the number of successful trials.,trials are conducted with a probability of success . Find the distribution of the number of successful trials.,Y\sim Poi(\lambda) p,"For a given experiment, $Y\sim Poi(\lambda)$ independent trials are conducted, with each trial having a probability of success $p$ and a probability of failure $1-p$ . Find the distribution of the number of successful trials $X$ ."" First of all, $Y$ Bernouilli trials are conducted, thus $$X\sim\mathit{Bin}(Y, p)$$ Therefore $$P(X=k)=\binom{Y}{k}p^k(1-p)^{Y-k}$$ From $Y\sim Poi(\lambda)\Longrightarrow P(Y=t)=\frac{\lambda^t}{t!}e^{-\lambda}$ Am I on the right path? How can I continue now?","For a given experiment, independent trials are conducted, with each trial having a probability of success and a probability of failure . Find the distribution of the number of successful trials ."" First of all, Bernouilli trials are conducted, thus Therefore From Am I on the right path? How can I continue now?","Y\sim Poi(\lambda) p 1-p X Y X\sim\mathit{Bin}(Y, p) P(X=k)=\binom{Y}{k}p^k(1-p)^{Y-k} Y\sim Poi(\lambda)\Longrightarrow P(Y=t)=\frac{\lambda^t}{t!}e^{-\lambda}","['probability', 'poisson-distribution', 'binomial-distribution']"
73,"Conditional expectation of $X\sim U[0,2]$ given $\min(X,t)$, where $t\in [0,2]$.","Conditional expectation of  given , where .","X\sim U[0,2] \min(X,t) t\in [0,2]","I want to determine the conditional expectation of $X\sim U[0,2]$ given $\min(X,t)$ , where $t\in [0,2]$ . Using the definition $$\mathbb E[X|\min(X,t)] = \frac{\mathbb E[X\chi_{\min(X,t)}]}{\mathbb P(\min(X,t))}$$ does not look that inviting to me. I am unsure if I should consider the probability for a specific value $\mathbb P(\min(X,t) = x)$ for $x\in [t,2]$ if the minimum is larger than $t$ or the probability that $\mathbb P(\min(X,t) \ne t)$ Intuitively I would say that since we know $t$ if $\min(X,t)$ is not $t$ , it has to be $X$ , and thus the conditional expectation has to be $\min(X,t) = X$ which seems tautological. If $\min(X,t)$ we know that $X>t$ and because $X\sim U[0,2]$ it follows that $X$ is uniformly distributed between $t$ and $2$ . The conditional expectation, in that case, should be $\frac{2+t}{2}.$ Is this correct? If so, how does this follow from the definition given above?","I want to determine the conditional expectation of given , where . Using the definition does not look that inviting to me. I am unsure if I should consider the probability for a specific value for if the minimum is larger than or the probability that Intuitively I would say that since we know if is not , it has to be , and thus the conditional expectation has to be which seems tautological. If we know that and because it follows that is uniformly distributed between and . The conditional expectation, in that case, should be Is this correct? If so, how does this follow from the definition given above?","X\sim U[0,2] \min(X,t) t\in [0,2] \mathbb E[X|\min(X,t)] = \frac{\mathbb E[X\chi_{\min(X,t)}]}{\mathbb P(\min(X,t))} \mathbb P(\min(X,t) = x) x\in [t,2] t \mathbb P(\min(X,t) \ne t) t \min(X,t) t X \min(X,t) = X \min(X,t) X>t X\sim U[0,2] X t 2 \frac{2+t}{2}.","['probability', 'probability-theory', 'solution-verification', 'conditional-expectation']"
74,Coming up with reasonable estimate for dice EV problem,Coming up with reasonable estimate for dice EV problem,,"There's this question on 100-sided die probability: The question is as follows: You are given a 100-sided die. After you roll once, you can choose to either get paid the dollar amount of that roll OR pay one dollar for one more roll. What is the expected value of the game? There is no limit on number of rolls. There's an answer here: If the expected value of this game is $a$ , then at a die roll of $X$ you have the choice of either collecting $X$ or paying a dollar and restart, which gives you an expected value of $a-1$ . To maximize the expected value, you should take $X$ if $X> a-1$ and start over if $X\le a-1$ (it does not really matter what we do when $X=a-1$ ). We obtain therefore $$ a = \frac1{100}\left(\lfloor a-1\rfloor\cdot a+\sum_{k=\lfloor a-1\rfloor+1}^{100}k\right) =\frac1{100}\left(\lfloor a-1\rfloor\cdot a+\frac{100\cdot101}{2}-\frac{\lfloor a-1\rfloor \cdot\lfloor a\rfloor}{2}\right). $$ I find numerically (didn't do much code checking, but the results are somewhat plausible) $$a\approx87.3571 $$ which seems to be exactly (and of course the true result must be rational) $$a=87\frac{5}{14}.$$ But I'm sure you can do the justification after the fact, i.e. show that the strategy that consists in continuing until you roll at least $87$ gives you $87\frac{5}{14}$ as expected value. Question: Is there a good way to quickly estimate something reasonably close to $87$ without writing a program, using WolframAlpha, or doing a calculation using scratch paper?","There's this question on 100-sided die probability: The question is as follows: You are given a 100-sided die. After you roll once, you can choose to either get paid the dollar amount of that roll OR pay one dollar for one more roll. What is the expected value of the game? There is no limit on number of rolls. There's an answer here: If the expected value of this game is , then at a die roll of you have the choice of either collecting or paying a dollar and restart, which gives you an expected value of . To maximize the expected value, you should take if and start over if (it does not really matter what we do when ). We obtain therefore I find numerically (didn't do much code checking, but the results are somewhat plausible) which seems to be exactly (and of course the true result must be rational) But I'm sure you can do the justification after the fact, i.e. show that the strategy that consists in continuing until you roll at least gives you as expected value. Question: Is there a good way to quickly estimate something reasonably close to without writing a program, using WolframAlpha, or doing a calculation using scratch paper?","a X X a-1 X X> a-1 X\le a-1 X=a-1  a = \frac1{100}\left(\lfloor a-1\rfloor\cdot a+\sum_{k=\lfloor a-1\rfloor+1}^{100}k\right)
=\frac1{100}\left(\lfloor a-1\rfloor\cdot a+\frac{100\cdot101}{2}-\frac{\lfloor a-1\rfloor \cdot\lfloor a\rfloor}{2}\right).
 a\approx87.3571  a=87\frac{5}{14}. 87 87\frac{5}{14} 87","['probability', 'combinatorics', 'inequality', 'expected-value', 'estimation']"
75,Wrong expected value of sum of Poisson process wait times [duplicate],Wrong expected value of sum of Poisson process wait times [duplicate],,"This question already has an answer here : Expected value of the total waiting time of all passengers catching a train. (1 answer) Closed 1 year ago . Assuming a Poisson process $N_t$ and denoting wait times $S_k$ (i.e. times until the $k$ -th jump), I want to find the expected value of their sum: $$ \mathrm{E}\left[\sum_{k=1}^{N_t} S_k\right]\;. $$ It is an exercise in O. Calin's An Informal Introduction to Stochastic Calculus with Applications and I actually know how to do it. For instance using the integrated Poisson process $$ U_t = \int_0^t N_s \,\mathrm{d}s $$ and the result $$ U_t = tN_t - \sum_{k=1}^{N_t} S_k\;. $$ Taking the expected value of both sides and using its linearity gives $$ \frac12 \lambda t^2 = \lambda t^2 - \mathrm{E}\left[\sum_{k=1}^{N_t} S_k\right], $$ so the answer is $\frac12\lambda t^2$ . The problem is that I can calculate the expected value using a different straighforward method, getting a different (apparently wrong) answer. And the question is why. In the second method we note that $N_t=n$ for different $n$ are disjoint events. So we can calculate the conditional expectations $$ \mathrm{E}\left[\sum_{k=1}^{N_t} S_k \middle| N_t = n\right] $$ and sum over $n$ with the corresponding proabilities: $$ \mathrm{E}\left[\sum_{k=1}^{N_t} S_k\right] = \sum_{n=0}^\infty \mathrm{Pr}(N_t=n)\times\mathrm{E}\left[\sum_{k=1}^{N_t} S_k \middle| N_t = n\right]\;. $$ Using $$ S_1 + S_2 + S_3 + \dots + S_n = nT_1 + (n-1)T_2 + \dots + 2T_{n-1} + T_n $$ and independence of inter-arrival times $T_k$ we get for the conditional expectation $$ \mathrm{E}\left[\sum_{k=1}^{N_t} S_k \middle| N_t = n\right] = \frac{n(n+1)}{2\lambda}\;. $$ This is in fact the result of another exercise (3.11.4). The probability is just Poisson $$ \mathrm{Pr}(N_t=n) = \mathrm{e}^{-\lambda t} \frac{(\lambda t)^n}{n!}\;. $$ Putting it together: $$ \mathrm{E}\left[\sum_{k=1}^{N_t} S_k\right] = \sum_{n=0}^\infty \mathrm{e}^{-\lambda t} \frac{(\lambda t)^n}{n!} \times \frac{n(n+1)}{2\lambda} = \mathrm{e}^{-\lambda t} \frac t2 \sum_{n=0}^\infty \frac{(\lambda t)^n}{n!} (n+2) = \frac12\lambda t^2 + t\;. $$ So there is an extra $t$ . Now I am stuck and can't find neither an error in the calculation, nor a reason why the summation over $n$ could be wrong.","This question already has an answer here : Expected value of the total waiting time of all passengers catching a train. (1 answer) Closed 1 year ago . Assuming a Poisson process and denoting wait times (i.e. times until the -th jump), I want to find the expected value of their sum: It is an exercise in O. Calin's An Informal Introduction to Stochastic Calculus with Applications and I actually know how to do it. For instance using the integrated Poisson process and the result Taking the expected value of both sides and using its linearity gives so the answer is . The problem is that I can calculate the expected value using a different straighforward method, getting a different (apparently wrong) answer. And the question is why. In the second method we note that for different are disjoint events. So we can calculate the conditional expectations and sum over with the corresponding proabilities: Using and independence of inter-arrival times we get for the conditional expectation This is in fact the result of another exercise (3.11.4). The probability is just Poisson Putting it together: So there is an extra . Now I am stuck and can't find neither an error in the calculation, nor a reason why the summation over could be wrong.","N_t S_k k  \mathrm{E}\left[\sum_{k=1}^{N_t} S_k\right]\;.   U_t = \int_0^t N_s \,\mathrm{d}s   U_t = tN_t - \sum_{k=1}^{N_t} S_k\;.   \frac12 \lambda t^2 = \lambda t^2 - \mathrm{E}\left[\sum_{k=1}^{N_t} S_k\right],  \frac12\lambda t^2 N_t=n n  \mathrm{E}\left[\sum_{k=1}^{N_t} S_k \middle| N_t = n\right]  n  \mathrm{E}\left[\sum_{k=1}^{N_t} S_k\right] = \sum_{n=0}^\infty \mathrm{Pr}(N_t=n)\times\mathrm{E}\left[\sum_{k=1}^{N_t} S_k \middle| N_t = n\right]\;.   S_1 + S_2 + S_3 + \dots + S_n = nT_1 + (n-1)T_2 + \dots + 2T_{n-1} + T_n  T_k  \mathrm{E}\left[\sum_{k=1}^{N_t} S_k \middle| N_t = n\right] = \frac{n(n+1)}{2\lambda}\;.   \mathrm{Pr}(N_t=n) = \mathrm{e}^{-\lambda t} \frac{(\lambda t)^n}{n!}\;.   \mathrm{E}\left[\sum_{k=1}^{N_t} S_k\right] = \sum_{n=0}^\infty \mathrm{e}^{-\lambda t} \frac{(\lambda t)^n}{n!} \times \frac{n(n+1)}{2\lambda} = \mathrm{e}^{-\lambda t} \frac t2 \sum_{n=0}^\infty \frac{(\lambda t)^n}{n!} (n+2) = \frac12\lambda t^2 + t\;.  t n","['probability', 'stochastic-processes', 'expected-value', 'poisson-process']"
76,Limit of distribution function of Poisson distributed random variables,Limit of distribution function of Poisson distributed random variables,,"Let $\left\{X_n\right\}_{n \geq 1}$ be i.i.d. random variables with Poisson $(\lambda)$ distribution, $\lambda>0$ . For each $n \geq 1$ , let $F_n$ denote the distribution function of $$ \frac{1}{\sqrt{n}} \sum_{i=1}^n\left(X_{2 i-1}-X_{2 i}\right) . $$ How can I calculate the limit $\lim _{n \rightarrow \infty} F_n(x), x \in \mathbb{R}$ ? I thought I could apply the weak law of large numbers? Let $\left\{X_{n}\right\}$ be a sequence of random variables with finite expected values. We will say that $\left\{X_{n}\right\}$ satisfies the weak law of large numbers if $$ \frac{1}{n} \sum_{i=1}^{n} X_{i}-\frac{1}{n} \sum_{i=1}^{n} \mathbb{E}\left[X_{i}\right] \stackrel{P}{\rightarrow} 0. $$ The problem here is that this converges in probability but above I just take the limit. Any hints?","Let be i.i.d. random variables with Poisson distribution, . For each , let denote the distribution function of How can I calculate the limit ? I thought I could apply the weak law of large numbers? Let be a sequence of random variables with finite expected values. We will say that satisfies the weak law of large numbers if The problem here is that this converges in probability but above I just take the limit. Any hints?","\left\{X_n\right\}_{n \geq 1} (\lambda) \lambda>0 n \geq 1 F_n 
\frac{1}{\sqrt{n}} \sum_{i=1}^n\left(X_{2 i-1}-X_{2 i}\right) .
 \lim _{n \rightarrow \infty} F_n(x), x \in \mathbb{R} \left\{X_{n}\right\} \left\{X_{n}\right\} 
\frac{1}{n} \sum_{i=1}^{n} X_{i}-\frac{1}{n} \sum_{i=1}^{n} \mathbb{E}\left[X_{i}\right] \stackrel{P}{\rightarrow} 0.
","['probability', 'poisson-distribution', 'probability-limit-theorems']"
77,Can A Probability Ever Be Outside of $0$ and $1$?,Can A Probability Ever Be Outside of  and ?,0 1,"Recently, I have been studying the Multinomial Probability Distribution Suppose you go to a casino and there is a game that involves rolling a six-sided die (i.e. one dice). However, you are not told what is the probability that this die lands on any one of these sides - this raises your suspicions and leads you to believe that perhaps the die might not be fair, therefore it might not be worth playing this game. You are still considering whether its worth playing this game - and suddenly find out that the casino has a large screen television that displays the last $100$ numbers that came from this die. Since you know that a die follows a Multinomial Distribution, you can use this fact to estimate the probabilities of the die assuming any given number, as well as the ""spread"" (i.e. variance) for each of these probabilities. Using the Maximum Likelihood Estimation , I have been trying to derive the formulae for the parameters of the Multinomial Probability Distribution. In short, given an event $i$ (e.g. the number $2$ on a die), the (very obvious) estimate for the probability $p_i$ of this event is $$\hat{p_{i}}_{\text{MLE}} = \frac{n_{i}}{N}$$ where the number of times that the event $n_{i}$ appears and $N$ is the total number of events that were recorded. As always, probabilities are only defined between $0$ and $1$ - therefore these individual estimates for $p_{i}$ can never be greater than $1$ or less than $0$ . Next, using the equivalence between the (inverse) Fisher Information and Variance, I was able to work out the formula for the ""variance of these probabilities"". In short, the variance of $p_{i}$ is given by $$\text{var}(\hat{p_{i}}_{\text{MLE}}) = \frac{p_{i}^{2}}{n_{i}}$$ Finally, using the theory of Asymptotic Normality of MLE , we can derive Confidence Intervals for the estimates of these parameter estimates (i.e. each individual value of $p_{i}$ ). That is, you might have observed that the probability of rolling a $2$ on this die is $0.31$ - but there is also a $95\%$ chance that the probability of rolling a $2$ might be anywhere between $(0.28, 0.33)$ . We can construct a $95\%$ Confidence Interval for any of these probabilities as: $$p_{i} \pm 1.96 \cdot \left( \sqrt{\frac{p_{i}^{2}}{n_{i}}} \right)$$ Question: I am worried that for certain values of $p_{i}$ and $n_{i}$ , this expression $$p_{i} \pm 1.96 \cdot \left( \sqrt{\frac{p_{i}^{2}}{n_{i}}} \right)$$ might be greater than $1$ or less than $0$ . As an example, if $p_{i} = 0.9$ and $n_{i} = 16$ , this results in a range estimate for the probability exceeding $1$ , i.e. $$0.9 + 1.96 \cdot \sqrt{\frac{0.9^2}{16}}$$ Have I done this correctly? Is it really possible for a probability value to be outside a range of $(0,1)$ ? Thanks! Note: I obviously think I have done something wrong, because I don't know much in math - but out of the few things I know, probabilities will never be outside the range of $[0,1]$ .","Recently, I have been studying the Multinomial Probability Distribution Suppose you go to a casino and there is a game that involves rolling a six-sided die (i.e. one dice). However, you are not told what is the probability that this die lands on any one of these sides - this raises your suspicions and leads you to believe that perhaps the die might not be fair, therefore it might not be worth playing this game. You are still considering whether its worth playing this game - and suddenly find out that the casino has a large screen television that displays the last numbers that came from this die. Since you know that a die follows a Multinomial Distribution, you can use this fact to estimate the probabilities of the die assuming any given number, as well as the ""spread"" (i.e. variance) for each of these probabilities. Using the Maximum Likelihood Estimation , I have been trying to derive the formulae for the parameters of the Multinomial Probability Distribution. In short, given an event (e.g. the number on a die), the (very obvious) estimate for the probability of this event is where the number of times that the event appears and is the total number of events that were recorded. As always, probabilities are only defined between and - therefore these individual estimates for can never be greater than or less than . Next, using the equivalence between the (inverse) Fisher Information and Variance, I was able to work out the formula for the ""variance of these probabilities"". In short, the variance of is given by Finally, using the theory of Asymptotic Normality of MLE , we can derive Confidence Intervals for the estimates of these parameter estimates (i.e. each individual value of ). That is, you might have observed that the probability of rolling a on this die is - but there is also a chance that the probability of rolling a might be anywhere between . We can construct a Confidence Interval for any of these probabilities as: Question: I am worried that for certain values of and , this expression might be greater than or less than . As an example, if and , this results in a range estimate for the probability exceeding , i.e. Have I done this correctly? Is it really possible for a probability value to be outside a range of ? Thanks! Note: I obviously think I have done something wrong, because I don't know much in math - but out of the few things I know, probabilities will never be outside the range of .","100 i 2 p_i \hat{p_{i}}_{\text{MLE}} = \frac{n_{i}}{N} n_{i} N 0 1 p_{i} 1 0 p_{i} \text{var}(\hat{p_{i}}_{\text{MLE}}) = \frac{p_{i}^{2}}{n_{i}} p_{i} 2 0.31 95\% 2 (0.28, 0.33) 95\% p_{i} \pm 1.96 \cdot \left( \sqrt{\frac{p_{i}^{2}}{n_{i}}} \right) p_{i} n_{i} p_{i} \pm 1.96 \cdot \left( \sqrt{\frac{p_{i}^{2}}{n_{i}}} \right) 1 0 p_{i} = 0.9 n_{i} = 16 1 0.9 + 1.96 \cdot \sqrt{\frac{0.9^2}{16}} (0,1) [0,1]","['probability', 'probability-theory', 'statistics', 'estimation', 'confidence-interval']"
78,"Let $X$ and $Y$ be non negative i.i.d random variables such that $E[X]<\infty$. Show that $E[min(X,Y)^2]<\infty$",Let  and  be non negative i.i.d random variables such that . Show that,"X Y E[X]<\infty E[min(X,Y)^2]<\infty","Let $X$ and $Y$ be non negative i.i.d random variables such that $E[X]<\infty$ . Without assuming that $E[X^2]<\infty$ , show that $E[\min(X,Y)^2]<\infty$ . We defined the expectation as: Let $X:\Omega\rightarrow S$ be a random element of $(S,\mathcal{S})$ with distribution $\mu$ and let a measurable function $h:S\rightarrow \mathbb{R}$ then $$E[h(x)]=\int_{S}h(x)\mu(dx)$$ whenever LHS or RHS are well defined. First of all I am actually clueless on how to proceed to solve the question. But this might be due to the fact that I'm not so comfortable with this definition. I don't really understand what $\mu$ is and what does $\mu(dx)$ represent. I also feel like we should suppose that $E[Y]<\infty$ as well.","Let and be non negative i.i.d random variables such that . Without assuming that , show that . We defined the expectation as: Let be a random element of with distribution and let a measurable function then whenever LHS or RHS are well defined. First of all I am actually clueless on how to proceed to solve the question. But this might be due to the fact that I'm not so comfortable with this definition. I don't really understand what is and what does represent. I also feel like we should suppose that as well.","X Y E[X]<\infty E[X^2]<\infty E[\min(X,Y)^2]<\infty X:\Omega\rightarrow S (S,\mathcal{S}) \mu h:S\rightarrow \mathbb{R} E[h(x)]=\int_{S}h(x)\mu(dx) \mu \mu(dx) E[Y]<\infty","['probability', 'probability-theory', 'lebesgue-integral', 'expected-value']"
79,"Is $\mathbb{P}(A):=\lim_{N\to\infty}\max_{n\geq N}\frac{1}{\pi(n)}\#\{\text{primes }p\leq n,\ p\in A\}$ a Probability Measure?",Is  a Probability Measure?,"\mathbb{P}(A):=\lim_{N\to\infty}\max_{n\geq N}\frac{1}{\pi(n)}\#\{\text{primes }p\leq n,\ p\in A\}","I'm taking a class in probability, and we had a question which was to prove or disprove that $$\mathbb{P}(A):=\lim_{N\to\infty}\max_{n\geq N}\frac{1}{\pi(n)}|\{\text{primes }p\leq n,\ p\in A\}|$$ is a probability measure. Here $\pi(n):=|\{\text{primes }p\leq n\}|$ is the prime counting function. I know $$\mathbb{P}(\mathbb{N})=\lim_{N\to\infty}\max_{n\geq N}\frac{1}{\pi(n)}|\{\text{primes }p\leq n\}|=1,$$ and clearly $\mathbb{P}(A)\in[0;1]$ for any $A\subseteq\mathbb{N}$ . But for additivity, I can only get to \begin{align} \mathbb{P}(A\sqcup B)&=\lim_{N\to\infty}\max_{n\geq N}\frac{1}{\pi(n)}|\{\text{primes }p\leq n,\ p\in A\sqcup B\}|\\ &=\lim_{N\to\infty}\max_{n\geq N}\frac{1}{\pi(n)}(|\{\text{primes }p\leq n,\ p\in A\}|+|\{\text{primes }p\leq n,\ p\in B\}|) \end{align} and in general the maximum of sums need not be equal to the sum of the maxima, so I'm beginning to suspect this is not satisfied. I'm not sure how to prove it though, so could someone help me either way?","I'm taking a class in probability, and we had a question which was to prove or disprove that is a probability measure. Here is the prime counting function. I know and clearly for any . But for additivity, I can only get to and in general the maximum of sums need not be equal to the sum of the maxima, so I'm beginning to suspect this is not satisfied. I'm not sure how to prove it though, so could someone help me either way?","\mathbb{P}(A):=\lim_{N\to\infty}\max_{n\geq N}\frac{1}{\pi(n)}|\{\text{primes }p\leq n,\ p\in A\}| \pi(n):=|\{\text{primes }p\leq n\}| \mathbb{P}(\mathbb{N})=\lim_{N\to\infty}\max_{n\geq N}\frac{1}{\pi(n)}|\{\text{primes }p\leq n\}|=1, \mathbb{P}(A)\in[0;1] A\subseteq\mathbb{N} \begin{align}
\mathbb{P}(A\sqcup B)&=\lim_{N\to\infty}\max_{n\geq N}\frac{1}{\pi(n)}|\{\text{primes }p\leq n,\ p\in A\sqcup B\}|\\
&=\lim_{N\to\infty}\max_{n\geq N}\frac{1}{\pi(n)}(|\{\text{primes }p\leq n,\ p\in A\}|+|\{\text{primes }p\leq n,\ p\in B\}|)
\end{align}","['probability', 'probability-theory', 'probability-distributions']"
80,What is the probability that you stop at only the first stop light?,What is the probability that you stop at only the first stop light?,,"There are two traffic lights. Let $E$ be the event that you stop at the first light and $F$ be the event that you stop at the second light. Given: $P(E) = .6$ $P(F) = .4$ $P(E \text{ and } F) = .25$ What is the probability that you stop at only the first traffic light? My attempt: We want $P(E \text{ and } F^c)$ $P(E \text{ and } F^c) = P(E|F^c)P(F^c)$ The question then becomes what is $P(E|F^c)$ ? Conditioning on $F$ leads to: $P(E) = P(E|F) + P(E|F^c)$ Where $P(E|F) = \frac{P(E \text{ and } F)}{P(F)}$ $P(E) - P(E|F) = P(E|F^c)$ $P(E) - \frac{P(E \text{ and } F)}{P(F)} = P(E|F^c)$ But $P(E) = .6$ and $\frac{P(E \text{ and } F)}{P(F)}=.625$ so I get a negative number.... What did I do wrong, and what's the easiest way to solve this problem? Thanks","There are two traffic lights. Let be the event that you stop at the first light and be the event that you stop at the second light. Given: What is the probability that you stop at only the first traffic light? My attempt: We want The question then becomes what is ? Conditioning on leads to: Where But and so I get a negative number.... What did I do wrong, and what's the easiest way to solve this problem? Thanks",E F P(E) = .6 P(F) = .4 P(E \text{ and } F) = .25 P(E \text{ and } F^c) P(E \text{ and } F^c) = P(E|F^c)P(F^c) P(E|F^c) F P(E) = P(E|F) + P(E|F^c) P(E|F) = \frac{P(E \text{ and } F)}{P(F)} P(E) - P(E|F) = P(E|F^c) P(E) - \frac{P(E \text{ and } F)}{P(F)} = P(E|F^c) P(E) = .6 \frac{P(E \text{ and } F)}{P(F)}=.625,[]
81,"An upper bound of $\mathbb{P}(|S_n - \log n| \geq C \log n)$, where $S_n$ is a sum of $n$ independent Bernoulli$-\frac{1}{i}$ random variables","An upper bound of , where  is a sum of  independent Bernoulli random variables",\mathbb{P}(|S_n - \log n| \geq C \log n) S_n n -\frac{1}{i},"Let $(X_i)_{i=1}^n$ be independent Bernoulli random variables with parameter $\frac{1}{i}$ . Let $S_n = \sum_{i=1}^nX_i$ and $C > 0$ . I need a bound of $$\mathbb{P}(|S_n - \log n| \geq C \log n)$$ and apparently I can use Chebyshev's inequality to obtain it. My approach? Well, I first compute the expectation of $S_n$ . Then $$\mathbb{E}[S_n] = \sum_{i=1}^n\mathbb{E}[X_i] = \sum_{i=1}^n\frac{1}{i}.$$ Note that $\mathbb{P}(|S_n - \log n| \geq C \log n)$ can be rewritten as $$\mathbb{P}\left(\left|S_n - \sum_{i=1}^n\frac{1}{i} + \sum_{i=1}^n\frac{1}{i} -\log n\right| \geq C \log n\right).$$ But how should I now proceed? There is a hint that $-\log n + \sum_{i=1}^n\frac{1}{i} \to \gamma$ , as $n \to \infty$ . Here, $\gamma$ is the Euler-Mascheroni constant.","Let be independent Bernoulli random variables with parameter . Let and . I need a bound of and apparently I can use Chebyshev's inequality to obtain it. My approach? Well, I first compute the expectation of . Then Note that can be rewritten as But how should I now proceed? There is a hint that , as . Here, is the Euler-Mascheroni constant.",(X_i)_{i=1}^n \frac{1}{i} S_n = \sum_{i=1}^nX_i C > 0 \mathbb{P}(|S_n - \log n| \geq C \log n) S_n \mathbb{E}[S_n] = \sum_{i=1}^n\mathbb{E}[X_i] = \sum_{i=1}^n\frac{1}{i}. \mathbb{P}(|S_n - \log n| \geq C \log n) \mathbb{P}\left(\left|S_n - \sum_{i=1}^n\frac{1}{i} + \sum_{i=1}^n\frac{1}{i} -\log n\right| \geq C \log n\right). -\log n + \sum_{i=1}^n\frac{1}{i} \to \gamma n \to \infty \gamma,"['probability', 'inequality', 'bernoulli-distribution']"
82,What is the distribution shape of the frequencies of a random uniform distribution?,What is the distribution shape of the frequencies of a random uniform distribution?,,"Suppose $x$ is randomly sampled uniformly from $[0,N)$ . Suppose we take $M$ samples. In my testing, I have been using $N=50000, M = 1000000$ . The expected number of times each $i \in[0,N)$ was sampled is $M/N$ . The shape of this distrubition is uniform. However, suppose we took this random uniform distribution and counted, not the number of times $x$ was $0$ , but the number of values which were sampled $0$ times. Let's call that the $\mathbf{freq}(0)$ . We can get the $\mathbf{freq}(j)$ values for all $[0,M]$ . This distrubution is not uniform . It looks kind of like a Poisson distrubution centered at $M/N$ , but I'm not sure. In case I wasn't totally clear, here is an example. Suppose we randomly sample uniformly from $[0,10)$ 54 times. The number of times each was sampled is a random uniform distribution looking something like $\{5, 4, 5, 5, 6, 4, 3, 5, 6, 7, 4\}$ , meaning that among the 54 samples, 0 was sampled 5 times, 1 was sampled 4 times, 2 was sampled 5 times, etc. But the distribution of these frequencies is $\{0, 0, 0, 1, 3, 4, 2, 1,0, 0\dots\}$ since there is one value which was sampled three times, 3 values which were sampled four times, 4 values which were sampled five times, etc. What is the shape of this distribution?","Suppose is randomly sampled uniformly from . Suppose we take samples. In my testing, I have been using . The expected number of times each was sampled is . The shape of this distrubition is uniform. However, suppose we took this random uniform distribution and counted, not the number of times was , but the number of values which were sampled times. Let's call that the . We can get the values for all . This distrubution is not uniform . It looks kind of like a Poisson distrubution centered at , but I'm not sure. In case I wasn't totally clear, here is an example. Suppose we randomly sample uniformly from 54 times. The number of times each was sampled is a random uniform distribution looking something like , meaning that among the 54 samples, 0 was sampled 5 times, 1 was sampled 4 times, 2 was sampled 5 times, etc. But the distribution of these frequencies is since there is one value which was sampled three times, 3 values which were sampled four times, 4 values which were sampled five times, etc. What is the shape of this distribution?","x [0,N) M N=50000, M = 1000000 i \in[0,N) M/N x 0 0 \mathbf{freq}(0) \mathbf{freq}(j) [0,M] M/N [0,10) \{5, 4, 5, 5, 6, 4, 3, 5, 6, 7, 4\} \{0, 0, 0, 1, 3, 4, 2, 1,0, 0\dots\}","['probability', 'probability-distributions', 'uniform-distribution', 'binomial-distribution']"
83,Simulate a biased coin with a fair coin using a fixed number of tosses,Simulate a biased coin with a fair coin using a fixed number of tosses,,"For which values of $p$ can you simulate a $p$ -biased coin using a fair coin in a fixed number of tosses (the ""reverse"" direction of this problem )? I have read of an approach where you consider the binary expansion of $p$ , let's call it $0.b_1b_2b_3\dots$ . Then, we toss the fair coin until it lands on heads. Let's say this took $n$ tosses. If $b_n=1$ , then we map this to a heads for our $p$ -biased coin, otherwise if $b_n=0$ we map this to a tails. This works because the probability of mapping to heads in our $p$ -biased coin is simply $$\sum_{i|b_i=1}\frac{1}{2^i}=0.b_1b_2b_3\dots=p$$ But this still gives an expected run time of $2$ flips. Is there an approach which takes a constant worst case number of flips? Or is the binary representation of $p$ in base 2 terminating a necessary condition for this to be the case? Any ideas are appreciated!","For which values of can you simulate a -biased coin using a fair coin in a fixed number of tosses (the ""reverse"" direction of this problem )? I have read of an approach where you consider the binary expansion of , let's call it . Then, we toss the fair coin until it lands on heads. Let's say this took tosses. If , then we map this to a heads for our -biased coin, otherwise if we map this to a tails. This works because the probability of mapping to heads in our -biased coin is simply But this still gives an expected run time of flips. Is there an approach which takes a constant worst case number of flips? Or is the binary representation of in base 2 terminating a necessary condition for this to be the case? Any ideas are appreciated!",p p p 0.b_1b_2b_3\dots n b_n=1 p b_n=0 p \sum_{i|b_i=1}\frac{1}{2^i}=0.b_1b_2b_3\dots=p 2 p,"['probability', 'random-variables', 'expected-value', 'binary']"
84,probability of number of comparisons of randomized quicksort,probability of number of comparisons of randomized quicksort,,"Let's assume we have an array of length $5$ which contains pairwise different integers. The subcript denotes the order of the respective integer, so $i_1<i_2<i_3<i_4<i_5$ . We apply the randomized quicksort algorithm to the unordered array and would like to know the probability $P(A_{24})$ that two integers $s_2$ and $s_4$ are being compared. (Note that every item in the array is chosen as pivot with equal probability) In our lecture the professor simply says that if we think about it for a while it becomes trivial that $P(A_{24})=\frac{2}{4-2+1}=\frac{2}{3}$ . However it didn't.... I would like to have a mathematical frame to argue that $P(A_{24})=\frac{2}{3}$ . My attempt: So far I have only assumed that we already have a discrete probability space $(\Omega,P)$ where $\Omega=\{i_1,i_2,i_3,i_4,i_5\}$ and $B_i$ denotes the event that $s_i$ has been chosen as pivot element. So I simply collect all the events where $s_2$ or $s_4$ are the pivot at some stage during the algorithm and get \begin{align*} &P(A_{24})=\\ &P(B_2)+P(B_4)+P(B_1)P(B_2\mid B_1)+P(B_5)P(B_2\mid B_5)+P(B_5)P(B_4\mid B_5)+P(B_1)P(B_4\mid B_1)\\ &+P(B_1)P(B_5\mid B_1)P(B_2\mid B_1\cap B_5)+P(B_5)P(B_1\mid B_5)P(B_2\mid B_5\cap B_1)\\ &+P(B_1)P(B_5\mid B_1)P(B_4\mid B_1\cap B_5)+P(B_5)P(B_1\mid B_5)P(B_4\mid B_5\cap B_1)\dots \end{align*} If we use some intuition and the fact that every element has the same probability to be chosen as pivot we get (conditional) probabilities \begin{align*} &\dots=\frac{1}{5}+\frac{1}{5}+\frac{1}{5}\cdot\frac{1}{4}+\frac{1}{5}\cdot\frac{1}{4}+\frac{1}{5}\cdot\frac{1}{4}+\frac{1}{5}\cdot\frac{1}{4}\\ &+\frac{1}{5}\cdot\frac{1}{4}\cdot\frac{1}{3}+\frac{1}{5}\cdot\frac{1}{4}\cdot\frac{1}{3}+\frac{1}{5}\cdot\frac{1}{4}\cdot\frac{1}{3}+\frac{1}{5}\cdot\frac{1}{4}\cdot\frac{1}{3}\\ &=\frac{1}{5}+\frac{1}{5}+\frac{1}{5}+\frac{1}{15}=\frac{2}{3}. \end{align*} Though this seems much clearer to me why we get $\frac{2}{3}$ I am not sure about the actual definition of the probability space. How would you set up an appropriate probability space in order to derive $P(A_{24})$ ? Or is there a simpler approach to show why $P(A_{24})=\frac{2}{3}$ ?","Let's assume we have an array of length which contains pairwise different integers. The subcript denotes the order of the respective integer, so . We apply the randomized quicksort algorithm to the unordered array and would like to know the probability that two integers and are being compared. (Note that every item in the array is chosen as pivot with equal probability) In our lecture the professor simply says that if we think about it for a while it becomes trivial that . However it didn't.... I would like to have a mathematical frame to argue that . My attempt: So far I have only assumed that we already have a discrete probability space where and denotes the event that has been chosen as pivot element. So I simply collect all the events where or are the pivot at some stage during the algorithm and get If we use some intuition and the fact that every element has the same probability to be chosen as pivot we get (conditional) probabilities Though this seems much clearer to me why we get I am not sure about the actual definition of the probability space. How would you set up an appropriate probability space in order to derive ? Or is there a simpler approach to show why ?","5 i_1<i_2<i_3<i_4<i_5 P(A_{24}) s_2 s_4 P(A_{24})=\frac{2}{4-2+1}=\frac{2}{3} P(A_{24})=\frac{2}{3} (\Omega,P) \Omega=\{i_1,i_2,i_3,i_4,i_5\} B_i s_i s_2 s_4 \begin{align*}
&P(A_{24})=\\
&P(B_2)+P(B_4)+P(B_1)P(B_2\mid B_1)+P(B_5)P(B_2\mid B_5)+P(B_5)P(B_4\mid B_5)+P(B_1)P(B_4\mid B_1)\\
&+P(B_1)P(B_5\mid B_1)P(B_2\mid B_1\cap B_5)+P(B_5)P(B_1\mid B_5)P(B_2\mid B_5\cap B_1)\\
&+P(B_1)P(B_5\mid B_1)P(B_4\mid B_1\cap B_5)+P(B_5)P(B_1\mid B_5)P(B_4\mid B_5\cap B_1)\dots
\end{align*} \begin{align*}
&\dots=\frac{1}{5}+\frac{1}{5}+\frac{1}{5}\cdot\frac{1}{4}+\frac{1}{5}\cdot\frac{1}{4}+\frac{1}{5}\cdot\frac{1}{4}+\frac{1}{5}\cdot\frac{1}{4}\\
&+\frac{1}{5}\cdot\frac{1}{4}\cdot\frac{1}{3}+\frac{1}{5}\cdot\frac{1}{4}\cdot\frac{1}{3}+\frac{1}{5}\cdot\frac{1}{4}\cdot\frac{1}{3}+\frac{1}{5}\cdot\frac{1}{4}\cdot\frac{1}{3}\\
&=\frac{1}{5}+\frac{1}{5}+\frac{1}{5}+\frac{1}{15}=\frac{2}{3}.
\end{align*} \frac{2}{3} P(A_{24}) P(A_{24})=\frac{2}{3}","['probability', 'combinatorics', 'probability-theory', 'computer-science', 'sorting']"
85,"When the mean of a non-negative, integer-valued random variable goes to zero, does this imply anything about the other raw moments?","When the mean of a non-negative, integer-valued random variable goes to zero, does this imply anything about the other raw moments?",,"When the mean of a non-negative, integer-valued random variable $X_n$ , for example counting paths in a random graph on $n$ vertices between two fixed vertices, goes to zero, $$\lim_{n \to \infty}\mathbb{E}[X_n] =0$$ can we have $\lim_{n \to \infty}\mathbb{E}[X_n^3] = \infty$ , or under what conditions does this occur? This seems reasonable given we just need two different sequences to have different limits, but a bit counter-intuitive, since we're counting objects, and the raw moments give the typical number of ordered pairs, triples, etc.","When the mean of a non-negative, integer-valued random variable , for example counting paths in a random graph on vertices between two fixed vertices, goes to zero, can we have , or under what conditions does this occur? This seems reasonable given we just need two different sequences to have different limits, but a bit counter-intuitive, since we're counting objects, and the raw moments give the typical number of ordered pairs, triples, etc.",X_n n \lim_{n \to \infty}\mathbb{E}[X_n] =0 \lim_{n \to \infty}\mathbb{E}[X_n^3] = \infty,"['probability', 'random-graphs']"
86,Bounding Binomial distribution tail,Bounding Binomial distribution tail,,"I have a random variable $X\sim \text{Bin}(3k, q)$ where $q < \frac{1}{2}$ and $q(1-q) \leq \frac{1}{5}$ . I want to show that for $k=O(\log_2 r)$ I can bound the probability $\mathbb{P}(X\geq k)$ by $\frac{1}{2^r}$ . I tried several upper bounds for the tails of binomial distributions I found online but none of them seems to do the trick. Any help would be appreciated!",I have a random variable where and . I want to show that for I can bound the probability by . I tried several upper bounds for the tails of binomial distributions I found online but none of them seems to do the trick. Any help would be appreciated!,"X\sim \text{Bin}(3k, q) q < \frac{1}{2} q(1-q) \leq \frac{1}{5} k=O(\log_2 r) \mathbb{P}(X\geq k) \frac{1}{2^r}","['probability', 'binomial-distribution', 'upper-lower-bounds']"
87,Maximum number of $\pm 1$ valued vectors with pairwise negative inner product,Maximum number of  valued vectors with pairwise negative inner product,\pm 1,"Let $S$ be a subset of $\{\pm 1\}^n$ such that $\forall x,y\in S$ ( $x\neq y$ ), $x\cdot y<0$ . Determine the upper bound of $|S|$ as precise as possible. (Thanks to the example from @kodlu, the proof is revised.) What I have already proved is $\mathrm{sup}|S|\leq n$ for odd $n$ , $\mathrm{sup}|S|\leq n/2$ for even $n$ , via probabilistic methods: For any possible $S$ , $m:=|S|$ . Let $X_i=1$ whenever the chosen pair differs in the $i$ -th entry, $X_i=0$ otherwise. The expectation of pairs in $S$ different in the $i$ -th entry is $$\mathbb E X_i=\dfrac{m_i(m-m_i)}{\binom{m}{2}}\leq \dfrac{m^2}{4\cdot m(m-1)/2}=\dfrac{m}{2(m-1)}.$$ Here $m_i$ is the number of $v\in S$ taking value $1$ in the $i$ -th entry. Since $\sum_{i=1}^n X_i$ is the total number of entries where pairs taking different values, we shall exclude those $m$ such that $$\mathbb E\sum_{i=1}^n X_i\leq\dfrac{mn}{2(m-1)}<\dfrac{n+1}{2}\quad \text{when } n \text{ is odd},$$ $$\mathbb E\sum_{i=1}^n X_i\leq\dfrac{mn}{2(m-1)}<\dfrac{n}{2}+1\quad \text{when } n \text{ is even}.$$ Therefore, $\mathrm{sup}|S|\leq n+1$ for odd $n$ , $\mathrm{sup}|S|\leq n/2$ for even $n$ . There is no contradiction to the example from @kodlu. The theorey of Plotkin bound is exactly what I'm looking for. See the answer by @Mike Earnest.","Let be a subset of such that ( ), . Determine the upper bound of as precise as possible. (Thanks to the example from @kodlu, the proof is revised.) What I have already proved is for odd , for even , via probabilistic methods: For any possible , . Let whenever the chosen pair differs in the -th entry, otherwise. The expectation of pairs in different in the -th entry is Here is the number of taking value in the -th entry. Since is the total number of entries where pairs taking different values, we shall exclude those such that Therefore, for odd , for even . There is no contradiction to the example from @kodlu. The theorey of Plotkin bound is exactly what I'm looking for. See the answer by @Mike Earnest.","S \{\pm 1\}^n \forall x,y\in S x\neq y x\cdot y<0 |S| \mathrm{sup}|S|\leq n n \mathrm{sup}|S|\leq n/2 n S m:=|S| X_i=1 i X_i=0 S i \mathbb E X_i=\dfrac{m_i(m-m_i)}{\binom{m}{2}}\leq \dfrac{m^2}{4\cdot m(m-1)/2}=\dfrac{m}{2(m-1)}. m_i v\in S 1 i \sum_{i=1}^n X_i m \mathbb E\sum_{i=1}^n X_i\leq\dfrac{mn}{2(m-1)}<\dfrac{n+1}{2}\quad \text{when } n \text{ is odd}, \mathbb E\sum_{i=1}^n X_i\leq\dfrac{mn}{2(m-1)}<\dfrac{n}{2}+1\quad \text{when } n \text{ is even}. \mathrm{sup}|S|\leq n+1 n \mathrm{sup}|S|\leq n/2 n","['probability', 'combinatorics', 'optimization', 'coding-theory', 'extremal-combinatorics']"
88,A Simple Two-State Markov Chain : How to Explain a Strange Result?,A Simple Two-State Markov Chain : How to Explain a Strange Result?,,"Context : After reading answers to my previous post (you don't need to read , this post is self sufficient)  , I obtained a strange result which I'm unable to explain . Problem : An opera singer is due to perform a long series of concerts. Having a bad temper, singer is liable to pull out each night with probability $f(a)\in (0,1]$ . Once this has happened singer will not sing again until the promoter convinces singer of the promoter’s high regard. This the promoter does by sending flowers every day until the singer returns. Flowers costing $b$ thousand GBP , $0 ≤ b ≤ 1$ , bring about a reconciliation with probability $g(b) \in (0,1] $ .The promoter stands to make $a$ thousand GBP from each successful concert , $0 ≤ a ≤ 1$ . How much should promoter spend on flowers? This's equivalent to finding the expected eventual daily profit then maximizing . The later is irrelevant here . This could be solved by (thanks to user Sharky Kesa ): Suppose the opera singer has $n$ performance days . Let $$ U_i = \left\{ \begin{array}{cc} 1 & \text{ if performance occurs on day } i \\ 0 & \text{ else } \end{array} \right. $$ for $i\in \{1,...,n\} $ . $(U_i)_{i\ge 0}$ is a Markov chain on state space $\{0,1\}$ . The profit is $M = (a + b)(U_1+...+U_n) - nb $ . We have transition matrix $$ P^n = \begin{bmatrix} 1 - g(b) & g(b) \\ f(a) & 1 - f(a)  \end{bmatrix}^n = \begin{bmatrix} 1 & 1 \\ 1 & - \frac{f(a)}{g(b)}  \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & (1-f(a)-g(b))^n \end{bmatrix} \begin{bmatrix} \frac{f(a)}{f(a)+g(b)} & \frac{g(b)}{f(a)+g(b)} \\ \frac{g(b)}{f(a)+g(b)} & \frac{-g(b)}{f(a)+g(b)} \end{bmatrix} $$ Let the distribution of $U_i$ be $u_i$ , we have $$ u_1 = \begin{bmatrix}\frac{1}{2} &  \frac{1}{2} \end{bmatrix} $$ $$ \implies u_i = u_1 P^{i-1} =  \begin{bmatrix} \frac{1}{2}\frac{ 2f(a) - (f(a)-g(b) )(  1 -f(a) - g(b) )^{i-1}   }{f(a) + g(b) } & \frac{1}{2}\frac{ 2g(b) + (f(a)-g(b))(1-f(a)-g(b))^{i-1} }{f(a)+g(b)} \end{bmatrix} $$ Let $U = \lim_{i\to\infty} U_i  $ and the eventual daily profit $D =  (a+b)U - b $ , so $$ ED  =  \lim_{i\to \infty} (a+b)EU_i - b =  \lim_{i\to \infty} (a+b)\left( \frac{1}{2}\frac{ 2g(b) + (f(a)-g(b))(1-f(a)-g(b))^{i-1} }{f(a)+g(b)} \right) - b $$ $f(a) , g(b) \in (0,1] \implies | 1-f(a)-g(b) | < 1$ or $f(a)-g(b) = 0 $ so $$  = (a+b)\left( \frac{1}{2}\frac{ 2g(b)  }{f(a)+g(b)} \right) - b $$ $$  =  \frac{ ag(b)  - bf(a)  }{f(a)+g(b)}   $$ $$  =  \frac{ a\frac{1}{f(a)}  - b\frac{1}{g(b)}  }{\frac{1}{f(a)}+\frac{1}{g(b)}}  $$ $$ =  \frac{ aEX - bEY  }{EX+EY}  $$ with $X\sim Geo(f(a)) , Y \sim Geo(g(b))$ and there's no restriction on their dependency . You could also arrive here by solving for invariant distribution and applying theorem 1.10.2 here . How to explain this result ? e.g. Could it be explained by the 2 approaches I mentioned or does it suggest a third approach to the problem ? A possible third approach : (thanks to user Jafego ) Instead of viewing $(U_{n})_{n\ge 0}$ as a ""straight line"" , warp it to form numerous cycles according to some rules  (like a spring except size of each cycle is random)  . Each cycle has 2 components : A consecutive number of days $X\sim Geo(f(a)) $ where the promoter makes a profit $a$ thousand GBP per day . A consecutive number of days $Y\sim Geo(g(b))$ where the promoter losses $b$ thousand GBP per day . A cycle is of length $X + Y$ . So the ""sample mean"" of daily profit in a cycle is $ \frac{aX-bY}{X+Y}$ .  I think this ""sample mean"" is also i.i.d for each cycle .   However , it has variable sample size so I'm unable to proceed from here . This's as close as I could get to explaining the result .","Context : After reading answers to my previous post (you don't need to read , this post is self sufficient)  , I obtained a strange result which I'm unable to explain . Problem : An opera singer is due to perform a long series of concerts. Having a bad temper, singer is liable to pull out each night with probability . Once this has happened singer will not sing again until the promoter convinces singer of the promoter’s high regard. This the promoter does by sending flowers every day until the singer returns. Flowers costing thousand GBP , , bring about a reconciliation with probability .The promoter stands to make thousand GBP from each successful concert , . How much should promoter spend on flowers? This's equivalent to finding the expected eventual daily profit then maximizing . The later is irrelevant here . This could be solved by (thanks to user Sharky Kesa ): Suppose the opera singer has performance days . Let for . is a Markov chain on state space . The profit is . We have transition matrix Let the distribution of be , we have Let and the eventual daily profit , so or so with and there's no restriction on their dependency . You could also arrive here by solving for invariant distribution and applying theorem 1.10.2 here . How to explain this result ? e.g. Could it be explained by the 2 approaches I mentioned or does it suggest a third approach to the problem ? A possible third approach : (thanks to user Jafego ) Instead of viewing as a ""straight line"" , warp it to form numerous cycles according to some rules  (like a spring except size of each cycle is random)  . Each cycle has 2 components : A consecutive number of days where the promoter makes a profit thousand GBP per day . A consecutive number of days where the promoter losses thousand GBP per day . A cycle is of length . So the ""sample mean"" of daily profit in a cycle is .  I think this ""sample mean"" is also i.i.d for each cycle .   However , it has variable sample size so I'm unable to proceed from here . This's as close as I could get to explaining the result .","f(a)\in (0,1] b 0 ≤ b ≤ 1 g(b) \in (0,1]  a 0 ≤ a ≤ 1 n 
U_i = \left\{ \begin{array}{cc}
1 & \text{ if performance occurs on day } i \\
0 & \text{ else }
\end{array} \right.
 i\in \{1,...,n\}  (U_i)_{i\ge 0} \{0,1\} M = (a + b)(U_1+...+U_n) - nb  
P^n = \begin{bmatrix}
1 - g(b) & g(b) \\
f(a) & 1 - f(a) 
\end{bmatrix}^n
= \begin{bmatrix}
1 & 1 \\
1 & - \frac{f(a)}{g(b)} 
\end{bmatrix}
\begin{bmatrix}
1 & 0 \\
0 & (1-f(a)-g(b))^n
\end{bmatrix}
\begin{bmatrix}
\frac{f(a)}{f(a)+g(b)} & \frac{g(b)}{f(a)+g(b)} \\
\frac{g(b)}{f(a)+g(b)} & \frac{-g(b)}{f(a)+g(b)}
\end{bmatrix}
 U_i u_i 
u_1 = \begin{bmatrix}\frac{1}{2} &  \frac{1}{2} \end{bmatrix}
 
\implies u_i = u_1 P^{i-1} = 
\begin{bmatrix}
\frac{1}{2}\frac{ 2f(a) - (f(a)-g(b) )(  1 -f(a) - g(b) )^{i-1}   }{f(a) + g(b) } &
\frac{1}{2}\frac{ 2g(b) + (f(a)-g(b))(1-f(a)-g(b))^{i-1} }{f(a)+g(b)}
\end{bmatrix}
 U = \lim_{i\to\infty} U_i   D =  (a+b)U - b  
ED  = 
\lim_{i\to \infty} (a+b)EU_i - b = 
\lim_{i\to \infty} (a+b)\left( \frac{1}{2}\frac{ 2g(b) + (f(a)-g(b))(1-f(a)-g(b))^{i-1} }{f(a)+g(b)} \right) - b
 f(a) , g(b) \in (0,1] \implies | 1-f(a)-g(b) | < 1 f(a)-g(b) = 0  
 = (a+b)\left( \frac{1}{2}\frac{ 2g(b)  }{f(a)+g(b)} \right) - b
 
 =  \frac{ ag(b)  - bf(a)  }{f(a)+g(b)}  
 
 =  \frac{ a\frac{1}{f(a)}  - b\frac{1}{g(b)}  }{\frac{1}{f(a)}+\frac{1}{g(b)}} 
 
=  \frac{ aEX - bEY  }{EX+EY} 
 X\sim Geo(f(a)) , Y \sim Geo(g(b)) (U_{n})_{n\ge 0} X\sim Geo(f(a))  a Y\sim Geo(g(b)) b X + Y  \frac{aX-bY}{X+Y}","['probability', 'probability-theory', 'discrete-mathematics', 'markov-chains', 'game-theory']"
89,How do I compute the density of the random variable $\frac{U}{\sqrt R}$?,How do I compute the density of the random variable ?,\frac{U}{\sqrt R},"Let $R$ be a positive random variable with density $$g(r)=\frac{1}{\sqrt{ \pi r }}e^{-r}~~~~,~~~~r>0$$ Let $U$ be uniformly distributed on $[0,1]$ and independent of $R$ . Let me define $X=\frac{U}{\sqrt R}$ . We need to compute the density of $X$ . My idea was the following. Let $f$ be a measurable bounded function. Then consider $$\Bbb{E}(f(X))=\Bbb{E}\left(f\left(\frac{U}{\sqrt R}\right)\right)=\int_{\Bbb{R}^2}f\left(\frac{u}{\sqrt r}\right)P_{(U,R)}(du~dr)\stackrel{independent}{=}\int_0^\infty \int_0^1 f\left(\frac{u}{\sqrt r}\right)g(r)~~du~dr$$ Now let me substitute $x=\frac{u}{\sqrt r}$ Then I get $$\Bbb{E}(f(X))=\frac{1}{\sqrt \pi}\int_0^\infty \int_0^{\frac{1}{\sqrt r}} f(x)e^{-r}~~dx~dr=\frac{1}{\sqrt \pi}\int_0^\infty \int_0^{\frac{1}{x^2}} f(x)e^{-r}~~dr~dx=\int_0^\infty f(x)h(x)~dx$$ where $h(x)=\int_0^{\frac{1}{x^2}} \frac{1}{\sqrt \pi}e^{-r}~~dr$ for $x>0$ and $h(x)=0$ else.  Then I can explixitly compute $h(x)$ and get $$h(x)=\left(\frac{1}{\sqrt \pi}-\frac{e^{-\frac{1}{x^2}}}{\sqrt \pi}\right)\Bbb{1}_{\{x>0\}}(x)$$ Then this $h$ is our density. Now I wanted to ask if this is correct like this or if I did something wrong. Thanks for your help.",Let be a positive random variable with density Let be uniformly distributed on and independent of . Let me define . We need to compute the density of . My idea was the following. Let be a measurable bounded function. Then consider Now let me substitute Then I get where for and else.  Then I can explixitly compute and get Then this is our density. Now I wanted to ask if this is correct like this or if I did something wrong. Thanks for your help.,"R g(r)=\frac{1}{\sqrt{ \pi r }}e^{-r}~~~~,~~~~r>0 U [0,1] R X=\frac{U}{\sqrt R} X f \Bbb{E}(f(X))=\Bbb{E}\left(f\left(\frac{U}{\sqrt R}\right)\right)=\int_{\Bbb{R}^2}f\left(\frac{u}{\sqrt r}\right)P_{(U,R)}(du~dr)\stackrel{independent}{=}\int_0^\infty \int_0^1 f\left(\frac{u}{\sqrt r}\right)g(r)~~du~dr x=\frac{u}{\sqrt r} \Bbb{E}(f(X))=\frac{1}{\sqrt \pi}\int_0^\infty \int_0^{\frac{1}{\sqrt r}} f(x)e^{-r}~~dx~dr=\frac{1}{\sqrt \pi}\int_0^\infty \int_0^{\frac{1}{x^2}} f(x)e^{-r}~~dr~dx=\int_0^\infty f(x)h(x)~dx h(x)=\int_0^{\frac{1}{x^2}} \frac{1}{\sqrt \pi}e^{-r}~~dr x>0 h(x)=0 h(x) h(x)=\left(\frac{1}{\sqrt \pi}-\frac{e^{-\frac{1}{x^2}}}{\sqrt \pi}\right)\Bbb{1}_{\{x>0\}}(x) h","['probability', 'integration', 'probability-theory', 'solution-verification', 'density-function']"
90,Bound on the probability of infinity norm of a Gaussian vector,Bound on the probability of infinity norm of a Gaussian vector,,"I consider a random vector $X\in \mathbb{R}^N$ following a multivariate Gaussian distribution of mean zero and covariance matrix $\Sigma_X$ such that $\forall 1\le i \le N, (\Sigma_X)_{ii}=1$ (there are only 1's on the diagonal). I am interrested in the maximum of the absolute value of the elements of $X$ : $\lVert X \rVert_\infty = \max_{1\le n \le N} |X_n|$ . I have the intuition that the following inequality is true: \begin{equation}\label{goal} \forall K>0, \; \mathbb{P}(\lVert X \rVert_\infty \le K) \ge \mathbb{P}(\lVert Y \rVert_\infty \le K) \qquad \mathrm{where} \qquad Y\sim \mathcal{N}(0,I_N) \end{equation} In other words, the probability of $\lVert X \rVert_\infty$ being smaller than $K$ is minimum in the case where all the elements of $X$ are iid centered normal variables. I managed to prove this in the case $N=2$ by direct calculation of the above probability but the integrals get too complicated for bigger $N$ . Does this result holds for $N>2$ and is there a proof ? In the case that the result is not true, are there smaller classes of covariance matrices $\Sigma_X$ (for example Toeplitz matrices, or Toeplitz matrices with $1, \rho, \rho^2,\rho^3,\dots$ on the first line) for which the inequality holds ?","I consider a random vector following a multivariate Gaussian distribution of mean zero and covariance matrix such that (there are only 1's on the diagonal). I am interrested in the maximum of the absolute value of the elements of : . I have the intuition that the following inequality is true: In other words, the probability of being smaller than is minimum in the case where all the elements of are iid centered normal variables. I managed to prove this in the case by direct calculation of the above probability but the integrals get too complicated for bigger . Does this result holds for and is there a proof ? In the case that the result is not true, are there smaller classes of covariance matrices (for example Toeplitz matrices, or Toeplitz matrices with on the first line) for which the inequality holds ?","X\in \mathbb{R}^N \Sigma_X \forall 1\le i \le N, (\Sigma_X)_{ii}=1 X \lVert X \rVert_\infty = \max_{1\le n \le N} |X_n| \begin{equation}\label{goal}
\forall K>0, \; \mathbb{P}(\lVert X \rVert_\infty \le K) \ge \mathbb{P}(\lVert Y \rVert_\infty \le K) \qquad \mathrm{where} \qquad Y\sim \mathcal{N}(0,I_N)
\end{equation} \lVert X \rVert_\infty K X N=2 N N>2 \Sigma_X 1, \rho, \rho^2,\rho^3,\dots","['real-analysis', 'probability', 'probability-distributions']"
91,Probability of buying a defective laptop,Probability of buying a defective laptop,,"There are two brand shops - A and B. A manufactures 1000 laptops, of which 100 are defective. B manufactures 100 laptops, of which 10 are defective. When I go to buy at a shop of any brand, they simply pick a laptop at random and give it to me. Which shop should I buy from - A or B? I was given this problem by a friend who's into tricky puzzles, so I'm skeptical that this will be straightforward. I'm thinking that in both cases there's a 10% chance of getting a defective laptop. So I guess both shops are equally fine to buy from? Am I missing something here and is there a more involved solution to this?","There are two brand shops - A and B. A manufactures 1000 laptops, of which 100 are defective. B manufactures 100 laptops, of which 10 are defective. When I go to buy at a shop of any brand, they simply pick a laptop at random and give it to me. Which shop should I buy from - A or B? I was given this problem by a friend who's into tricky puzzles, so I'm skeptical that this will be straightforward. I'm thinking that in both cases there's a 10% chance of getting a defective laptop. So I guess both shops are equally fine to buy from? Am I missing something here and is there a more involved solution to this?",,['probability']
92,Converge almost surely to zero with arbitrary $a_{nk}$,Converge almost surely to zero with arbitrary,a_{nk},"Let $X_1,X_2,\dots,$ be i.i.d. random variables with $\mathbb{E}[X_1]=0,\operatorname{Var}(X_1)<\infty$ . Assume that $\sum_{k=1}^{n}a_{nk}^2=\frac{1}{n}$ , let $S_n=\sum_{k=1}^{n}a_{nk}X_k$ , prove that $$S_n\rightarrow0~~a.s.$$ It seems that this looks like the triangle array of some law of large numbers, we wish to use Borel-Cantelli to deal with this problem, but it seems hard since all $a_{nk}$ are arbitrary. How should we deal with the $a_{nk}$ s here in this problem?","Let be i.i.d. random variables with . Assume that , let , prove that It seems that this looks like the triangle array of some law of large numbers, we wish to use Borel-Cantelli to deal with this problem, but it seems hard since all are arbitrary. How should we deal with the s here in this problem?","X_1,X_2,\dots, \mathbb{E}[X_1]=0,\operatorname{Var}(X_1)<\infty \sum_{k=1}^{n}a_{nk}^2=\frac{1}{n} S_n=\sum_{k=1}^{n}a_{nk}X_k S_n\rightarrow0~~a.s. a_{nk} a_{nk}","['probability', 'probability-theory']"
93,Multiplication of two random matrices over a finite field,Multiplication of two random matrices over a finite field,,"Consider a matrix $\mathrm{X}$ sampled uniformly at random from the set of all rank $r$ matrices over $\mathbb{F}_q^{m \times n}$ and a matrix $\mathrm{Y}$ sampled uniformly at random from the set of all full rank matrices over $\mathbb{F}_q^{n \times n}$ . Let $\mathrm{Z} = \mathrm{X}\mathrm{Y}$ . I am trying to prove something like the statement that $\mathrm{Z}$ is uniform over the set of all rank $r$ matrices over $\mathbb{F}_q^{m \times n}$ . Assume $m > n$ and $r \leq n$ . This old stackexchange answer says that this is indeed the case, but it does not explain how to reach the result. Is this a common result in random matrix theory, and if so, can someone provide a reference or a proof?","Consider a matrix sampled uniformly at random from the set of all rank matrices over and a matrix sampled uniformly at random from the set of all full rank matrices over . Let . I am trying to prove something like the statement that is uniform over the set of all rank matrices over . Assume and . This old stackexchange answer says that this is indeed the case, but it does not explain how to reach the result. Is this a common result in random matrix theory, and if so, can someone provide a reference or a proof?",\mathrm{X} r \mathbb{F}_q^{m \times n} \mathrm{Y} \mathbb{F}_q^{n \times n} \mathrm{Z} = \mathrm{X}\mathrm{Y} \mathrm{Z} r \mathbb{F}_q^{m \times n} m > n r \leq n,"['linear-algebra', 'probability', 'matrices', 'finite-fields', 'random-matrices']"
94,Why $[(\mathbf{I}_N-\mathbf{A}^\top \mathbf{A})\mathbf{x}]$ is Gaussian with i.i.d. Gaussian $\mathbf{A}$?,Why  is Gaussian with i.i.d. Gaussian ?,[(\mathbf{I}_N-\mathbf{A}^\top \mathbf{A})\mathbf{x}] \mathbf{A},"1. Background: It is presented in the paper of approximate message passing (AMP) algorithm [ Paper Link ] that (the conclusion below is slightly modified without changing its original meaning): Given a fixed vector $\mathbf{x}\in\mathbb{R}^N$ , for a random measurement matrix $\mathbf{A}\in\mathbb{R}^{M\times N}$ with $M\ll N$ , in which the entries are i.i.d. and $\mathbf{A}_{ij}\sim\mathcal{N}(0,\frac{1}{M})$ , then $[(\mathbf{I}_N-\mathbf{A}^\top \mathbf{A})\mathbf{x}]$ is also a Gaussian vector whose entries have variance $\frac{\lVert \mathbf{x}\rVert_2^2}{M}$ (the assumption of $\mathbf{A}$ is in Sec. I-A , and the above conclusion is in Sec. I-D , please see 5. Appendix for more details). 2. My Problems: I am confused by the statements in the paper about the above conclusion in [ 1. Background ], and have three subproblems posted here (the first two subproblems are coupled, and the second one is more general): $(2.1)$ How to prove the above conclusion in [ 1. Background ]? $(2.2)$ For a more general case: $\mathbf{A}_{ij}\sim\mathcal{N}(\mu,\sigma^2)$ with $\mu\in\mathbb{R}$ and $\sigma\in\mathbb{R}^+$ , given $\rho\in\mathbb{R}$ , will the vector $[(\mathbf{I}_N-\rho\mathbf{A}^\top \mathbf{A})\mathbf{x}]$ be still Gaussian? What are the distribution parameters ( e.g. means and variances)? $(2.3)$ When $\mathbf{A}$ is a random Bernoulli matrix, i.e. , the entries are i.i.d. and $P(\mathbf{A}_{ij}=a)=P(\mathbf{A}_{ij}=b)=\frac{1}{2}$ with $a<b$ , will the vector $[(\mathbf{I}_N-\rho\mathbf{A}^\top \mathbf{A})\mathbf{x}]$ obey a special distribution? What are the parameters ( e.g. means and variances)? 3. My Efforts: $(3.1)$ I have been convinced by the above conclusion in [ 1. Background ] by writing a program to randomly generate hundreds of vectors with various $M$ s and $N$ s. The histograms are approximately Gaussian with matched variances. $(3.2)$ I write out the expression of each element of the result vector, but it is too complicated for me. Especially, the elements of $\mathbf{A}^\top\mathbf{A}$ are difficult for me to analyse, since the multiplication of two Gaussian variables are not Gaussian [ Reference ]. $(3.3)$ After a long struggle, I still do not know if there is an efficient way to analyse the subproblems $(2.1)$ , $(2.2)$ and $(2.3)$ . 4. My Experiments: Here is my Python code for experiments: import numpy as np  for i in range(3):     n = np.random.randint(2, 10000)     m = np.random.randint(1, n)      A = np.random.randn(m, n) * ((1 / m) ** 0.5)     x = np.random.rand(n,)     e = np.matmul(np.eye(n) - np.matmul(np.transpose(A, [1, 0]), A), x)  # (I-A'A)x      print(np.linalg.norm(x) * ((1/m) ** 0.5))     print(np.std(e))     print('===')      try:         from matplotlib import pyplot as plt         import seaborn         plt.cla()         seaborn.histplot(e, bins=300)         plt.title('mean = %f, var = %f' % (float(e.mean()), float(e.std())))         plt.savefig('%d.png' % i, dpi=300)     except:         pass The standard deviation and the predicted one are generally matched (I have run the code multiple times): 0.7619008975832263 0.7371794446157226 === 0.5792213637974852 0.5936062808535417 === 0.5991335956466841 0.6256026437096703 === 5. Appendix Here are two fragments of the paper: (from Sec. I-A ) (from Sec. I-D ) Additionally, this paper [ Paper Link ] published on  IEEE Transactions on Image Processing 2021 refers this conclusion (around the Equation (6)):","1. Background: It is presented in the paper of approximate message passing (AMP) algorithm [ Paper Link ] that (the conclusion below is slightly modified without changing its original meaning): Given a fixed vector , for a random measurement matrix with , in which the entries are i.i.d. and , then is also a Gaussian vector whose entries have variance (the assumption of is in Sec. I-A , and the above conclusion is in Sec. I-D , please see 5. Appendix for more details). 2. My Problems: I am confused by the statements in the paper about the above conclusion in [ 1. Background ], and have three subproblems posted here (the first two subproblems are coupled, and the second one is more general): How to prove the above conclusion in [ 1. Background ]? For a more general case: with and , given , will the vector be still Gaussian? What are the distribution parameters ( e.g. means and variances)? When is a random Bernoulli matrix, i.e. , the entries are i.i.d. and with , will the vector obey a special distribution? What are the parameters ( e.g. means and variances)? 3. My Efforts: I have been convinced by the above conclusion in [ 1. Background ] by writing a program to randomly generate hundreds of vectors with various s and s. The histograms are approximately Gaussian with matched variances. I write out the expression of each element of the result vector, but it is too complicated for me. Especially, the elements of are difficult for me to analyse, since the multiplication of two Gaussian variables are not Gaussian [ Reference ]. After a long struggle, I still do not know if there is an efficient way to analyse the subproblems , and . 4. My Experiments: Here is my Python code for experiments: import numpy as np  for i in range(3):     n = np.random.randint(2, 10000)     m = np.random.randint(1, n)      A = np.random.randn(m, n) * ((1 / m) ** 0.5)     x = np.random.rand(n,)     e = np.matmul(np.eye(n) - np.matmul(np.transpose(A, [1, 0]), A), x)  # (I-A'A)x      print(np.linalg.norm(x) * ((1/m) ** 0.5))     print(np.std(e))     print('===')      try:         from matplotlib import pyplot as plt         import seaborn         plt.cla()         seaborn.histplot(e, bins=300)         plt.title('mean = %f, var = %f' % (float(e.mean()), float(e.std())))         plt.savefig('%d.png' % i, dpi=300)     except:         pass The standard deviation and the predicted one are generally matched (I have run the code multiple times): 0.7619008975832263 0.7371794446157226 === 0.5792213637974852 0.5936062808535417 === 0.5991335956466841 0.6256026437096703 === 5. Appendix Here are two fragments of the paper: (from Sec. I-A ) (from Sec. I-D ) Additionally, this paper [ Paper Link ] published on  IEEE Transactions on Image Processing 2021 refers this conclusion (around the Equation (6)):","\mathbf{x}\in\mathbb{R}^N \mathbf{A}\in\mathbb{R}^{M\times N} M\ll
N \mathbf{A}_{ij}\sim\mathcal{N}(0,\frac{1}{M}) [(\mathbf{I}_N-\mathbf{A}^\top \mathbf{A})\mathbf{x}] \frac{\lVert
\mathbf{x}\rVert_2^2}{M} \mathbf{A} (2.1) (2.2) \mathbf{A}_{ij}\sim\mathcal{N}(\mu,\sigma^2) \mu\in\mathbb{R} \sigma\in\mathbb{R}^+ \rho\in\mathbb{R} [(\mathbf{I}_N-\rho\mathbf{A}^\top \mathbf{A})\mathbf{x}] (2.3) \mathbf{A} P(\mathbf{A}_{ij}=a)=P(\mathbf{A}_{ij}=b)=\frac{1}{2} a<b [(\mathbf{I}_N-\rho\mathbf{A}^\top \mathbf{A})\mathbf{x}] (3.1) M N (3.2) \mathbf{A}^\top\mathbf{A} (3.3) (2.1) (2.2) (2.3)","['probability', 'statistics', 'normal-distribution', 'convex-optimization', 'random-matrices']"
95,Cooking French Fries as a Stochastic Process?,Cooking French Fries as a Stochastic Process?,,"I always had this question since I was a kid: Suppose you place 100 french fries on to a pan over a stove For this problem, let's assume that each french fry can only have 2 ""states"" : ""face up"" or ""face down"" Each french fry needs to be cooked for 1 minute on each side - if a french fry is cooked for more than 1 minute on any side, it is considered as burnt You place the french fries on the pan and after one minute you shake the pan - some of the french fries get flipped in the air and land on the pan either ""face up"" or ""face down"", but some of the french fries never got flipped at all. After another minute has passed, you shake the pan again. For the sake of this question, let's assume that each time you shake the pan, each individual french fry has a 50% chance of getting flipped in the air, and the french fries that were flipped in the air have a 50% chance of landing ""face down"" or ""face up"". Here is the question: After 2 minutes, how many of the 100 french fries are perfectly cooked and how many french fries are burnt? How many minutes need to pass until all french fries are guaranteed to have been cooked on both sides (even though many of them will be burnt)? I tried writing some computer simulations to simulate how many french fries are burnt/perfectly cooked after ""n"" minutes, then repeat the simulation many times to try and take the average proportion of burnt/perfectly cooked fries for all these simulations ... but I was looking for a more ""mathematical way"" to solve this problem (e.g. some equation). Can this problem somehow be modelled as a Stochastic Process or using Markov Chains, and then we can derive a general formula that shows how many fries are burnt/cooked as the Markov Chain is raised to the power of ""n""? Thanks!","I always had this question since I was a kid: Suppose you place 100 french fries on to a pan over a stove For this problem, let's assume that each french fry can only have 2 ""states"" : ""face up"" or ""face down"" Each french fry needs to be cooked for 1 minute on each side - if a french fry is cooked for more than 1 minute on any side, it is considered as burnt You place the french fries on the pan and after one minute you shake the pan - some of the french fries get flipped in the air and land on the pan either ""face up"" or ""face down"", but some of the french fries never got flipped at all. After another minute has passed, you shake the pan again. For the sake of this question, let's assume that each time you shake the pan, each individual french fry has a 50% chance of getting flipped in the air, and the french fries that were flipped in the air have a 50% chance of landing ""face down"" or ""face up"". Here is the question: After 2 minutes, how many of the 100 french fries are perfectly cooked and how many french fries are burnt? How many minutes need to pass until all french fries are guaranteed to have been cooked on both sides (even though many of them will be burnt)? I tried writing some computer simulations to simulate how many french fries are burnt/perfectly cooked after ""n"" minutes, then repeat the simulation many times to try and take the average proportion of burnt/perfectly cooked fries for all these simulations ... but I was looking for a more ""mathematical way"" to solve this problem (e.g. some equation). Can this problem somehow be modelled as a Stochastic Process or using Markov Chains, and then we can derive a general formula that shows how many fries are burnt/cooked as the Markov Chain is raised to the power of ""n""? Thanks!",,"['probability', 'stochastic-processes', 'markov-chains']"
96,Question about conditional expectation fact,Question about conditional expectation fact,,"This fact comes from *Concentration of Measure for the Analysis of Randomised Algorithms * by Dubhashi and Panconesi (page 76, equation 5.2). Let $X$ and $Y$ be two discrete random variables and two arbitrary functions $f$ and $g$ . Then $$E[E[f(X)g(X,Y) | X]] = E[f(X)E[g(X,Y)|X]]$$ This is how far I got in the proof: By law of iterated expectation $E[X] = E[E[X|Y]]$ we have that: $$E[E[f(X)g(X,Y) | X]] = E[f(X) \cdot g(X,Y)]$$ Now I would like to apply something the $E[XY] = E[X] \cdot  E[Y]$ which only holds for two independent variables $X$ and $Y$ . But I don't see $f(X)$ and $g(X,Y)$ as independent. Am I missing something here? If they were independent I'd continue: $E[f(X) \cdot g(X,Y)] = E[f(X)] \cdot E[g(X,Y)]$ Re-apply law of iterated expectation: $$E[f(X)] \cdot E[g(X,Y)] = E[f(X)] \cdot E[E[g(X,Y)|X]] = E[f(X) \cdot E[g(X,Y)|X]] $$ $\square$ Could someone assist me in my thinking here?","This fact comes from *Concentration of Measure for the Analysis of Randomised Algorithms * by Dubhashi and Panconesi (page 76, equation 5.2). Let and be two discrete random variables and two arbitrary functions and . Then This is how far I got in the proof: By law of iterated expectation we have that: Now I would like to apply something the which only holds for two independent variables and . But I don't see and as independent. Am I missing something here? If they were independent I'd continue: Re-apply law of iterated expectation: Could someone assist me in my thinking here?","X Y f g E[E[f(X)g(X,Y) | X]] = E[f(X)E[g(X,Y)|X]] E[X] = E[E[X|Y]] E[E[f(X)g(X,Y) | X]] = E[f(X) \cdot g(X,Y)] E[XY] = E[X] \cdot  E[Y] X Y f(X) g(X,Y) E[f(X) \cdot g(X,Y)] = E[f(X)] \cdot E[g(X,Y)] E[f(X)] \cdot E[g(X,Y)] = E[f(X)] \cdot E[E[g(X,Y)|X]] = E[f(X) \cdot E[g(X,Y)|X]]  \square","['probability', 'probability-theory', 'expected-value', 'conditional-expectation']"
97,Expectation of Spherically Symmetric Random Vector,Expectation of Spherically Symmetric Random Vector,,"Suppose we are considering a vector valued random variable $X \in \mathbb{R}^d$ that is ""spherically symmetric"" in the sense that $$ E \frac{X}{\|X\| } = 0.$$ Suppose $a_n \in \mathbb{R}$ is a deterministic sequence tending to zero. What I want to know is:  what additional conditions (if any) are needed to imply that, for any fixed nonzero vector $u$ , $$ E \frac{X}{\|X - a_n u\| } \to 0 ?  $$ And $$ E \frac{\|X\|^2}{\|X - a_n u\|^2} \mbox{ is bounded}? $$ Intuitively it seems these should follow from some ""dominated convergence"" condition since as $a_n \to 0$ , $\frac{X}{\|X - a_n u\| } \to X/\|X\|$ almost surely, and $\| X/\|X\| \| =1$ . Any help is much appreciated!","Suppose we are considering a vector valued random variable that is ""spherically symmetric"" in the sense that Suppose is a deterministic sequence tending to zero. What I want to know is:  what additional conditions (if any) are needed to imply that, for any fixed nonzero vector , And Intuitively it seems these should follow from some ""dominated convergence"" condition since as , almost surely, and . Any help is much appreciated!","X \in \mathbb{R}^d  E \frac{X}{\|X\| } = 0. a_n \in \mathbb{R} u 
E \frac{X}{\|X - a_n u\| } \to 0 ? 
 
E \frac{\|X\|^2}{\|X - a_n u\|^2} \mbox{ is bounded}?
 a_n \to 0 \frac{X}{\|X - a_n u\| } \to X/\|X\| \| X/\|X\| \| =1","['probability', 'probability-theory']"
98,Probability that N i.i.d. draws from a multinomial distribution have made all events appear,Probability that N i.i.d. draws from a multinomial distribution have made all events appear,,"Consider a multinomial distribution $\mathbb{P}$ on $S$ states $\{s_1,\dots,s_S\}$ where $S\in \mathbb{N}$ and $S\geq 2$ , with probabilities $\mathbb{P}(s_i)=:p_i$ .  Now consider $N$ i.i.d. draws $X_1,\dots,X_N$ from $\mathbb{P}$ and suppose that $N\geq S$ .  What is the probability that each state has occured? So far I have $$ \mathbb{P}\left(\forall s\in S\, (\exists X_i)\, X_i=s\right) = 1- \mathbb{P}(\exists s\in S \, s\not\in \{X_n\}_{n=1}^N)... $$ But I'm completely stuck","Consider a multinomial distribution on states where and , with probabilities .  Now consider i.i.d. draws from and suppose that .  What is the probability that each state has occured? So far I have But I'm completely stuck","\mathbb{P} S \{s_1,\dots,s_S\} S\in \mathbb{N} S\geq 2 \mathbb{P}(s_i)=:p_i N X_1,\dots,X_N \mathbb{P} N\geq S 
\mathbb{P}\left(\forall s\in S\, (\exists X_i)\, X_i=s\right) = 1- \mathbb{P}(\exists s\in S \, s\not\in \{X_n\}_{n=1}^N)...
","['probability', 'combinatorics', 'recreational-mathematics', 'problem-solving', 'coupon-collector']"
99,Quickly calculate the probability of 12 dice having sum less than 30,Quickly calculate the probability of 12 dice having sum less than 30,,"I try to quickly answer the question of whether one should play a game: roll 12 fair dice and sum up the face values; if the sum is less than 30, win 10 dollars, otherwise lose 1 dollar. Let $S_{12}=\sum\limits_{i=1}^{12} x_i$ be the sum where $x_i$ is the value for the ith die. I think if the expected gain is greater than 0, we should play the game, and the gain is $$E = 10 \times P(S_{12}<30) - 1\times [1-P(S_{12}< 30)] >0 ?$$ So I tried to quickly calculate or approximate $P(S_{12}<30)$ . My way is through CLT, $$\frac{S_{12} - 12E[x_i]}{\sqrt{12 var(x_i)} } \xrightarrow{d} N(0,1)$$ $$\Rightarrow P(S_{12} < 30) \approx p(z < \frac{30-42}{\sqrt{35}}) = \Phi \left( \frac{30-42}{\sqrt{35}} \right)  $$ But this approach still seems to require a certain amount of computation. Especially, it requires evaluating the normal cdf $\Phi(z)$ . So I was wondering if there is a better/quicker way without referring to computers/calculators. Thanks in advance.","I try to quickly answer the question of whether one should play a game: roll 12 fair dice and sum up the face values; if the sum is less than 30, win 10 dollars, otherwise lose 1 dollar. Let be the sum where is the value for the ith die. I think if the expected gain is greater than 0, we should play the game, and the gain is So I tried to quickly calculate or approximate . My way is through CLT, But this approach still seems to require a certain amount of computation. Especially, it requires evaluating the normal cdf . So I was wondering if there is a better/quicker way without referring to computers/calculators. Thanks in advance.","S_{12}=\sum\limits_{i=1}^{12} x_i x_i E = 10 \times P(S_{12}<30) - 1\times [1-P(S_{12}< 30)] >0 ? P(S_{12}<30) \frac{S_{12} - 12E[x_i]}{\sqrt{12 var(x_i)} } \xrightarrow{d} N(0,1) \Rightarrow P(S_{12} < 30) \approx p(z < \frac{30-42}{\sqrt{35}}) = \Phi \left( \frac{30-42}{\sqrt{35}} \right)   \Phi(z)","['probability', 'combinatorics', 'statistics', 'approximation', 'combinatorial-game-theory']"
