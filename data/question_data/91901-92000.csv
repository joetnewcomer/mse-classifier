,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Winding number (demonstration),Winding number (demonstration),,"How could I explain mathematically, that the winding number of a closed curve $\gamma$  around $a$ ($a \notin \gamma$) gives always an integer value. $$ W(\gamma,a)=\frac{1}{2\pi i} \int_{\gamma} \frac{dw}{w-a} $$  where $W(\gamma,a)\in \mathbb{Z}$","How could I explain mathematically, that the winding number of a closed curve $\gamma$  around $a$ ($a \notin \gamma$) gives always an integer value. $$ W(\gamma,a)=\frac{1}{2\pi i} \int_{\gamma} \frac{dw}{w-a} $$  where $W(\gamma,a)\in \mathbb{Z}$",,"['complex-analysis', 'winding-number']"
1,Integrating $\int_{-\infty}^\infty \frac{1}{1 + x^4}dx$ with the residue theorem,Integrating  with the residue theorem,\int_{-\infty}^\infty \frac{1}{1 + x^4}dx,Calculate integral $$\int\limits_{-\infty}^{\infty}\frac{1}{x^4+1} dx$$ with residue theorem. Can I evaluate $\frac 12\int_C  \dfrac{1}{z^4+1} dz$ where $C$ is simple closed contour of the upper half of unit circle like this? And find the roots of polynomial $z^4 +1$ which are the fourth roots of $-1$. In $C$ there is $z_1 =e^{i\pi/4}=\frac{1}{\sqrt{2}}+\frac{i}{\sqrt{2}}$ and $z_2=e^{3\pi/4}=-\frac{1}{\sqrt{2}}+\frac{i}{\sqrt{2}}$. So the residuals $B_1$ and $B_2$ for $z_1$ and $z_2$ are simple poles and that \begin{align} B_1&=\frac{1}{4 z_1^3}\frac{z_1}{z_1}=-\frac{z_1}{4} \\ B_2&=\frac{1}{4z_2^3}\frac{z_2}{z_2}=-\frac{z_2}{4} \end{align} And the sum of residuals is $$B_1+B_2=-\frac{1}{4}(z_1 + z_2)=-\frac{1}{4}\left(\frac{1}{\sqrt{2}}+\frac{1}{\sqrt{2}} \right)=-\frac{i}{2 \sqrt{2}}$$ So my integral should be $$\int\limits_{-\infty}^{\infty}\frac{1}{x^4+1} dx =\frac 12 \times 2\pi i (B_1+B_2)=\frac{\pi}{\sqrt{2}}$$ Is this valid?,Calculate integral $$\int\limits_{-\infty}^{\infty}\frac{1}{x^4+1} dx$$ with residue theorem. Can I evaluate $\frac 12\int_C  \dfrac{1}{z^4+1} dz$ where $C$ is simple closed contour of the upper half of unit circle like this? And find the roots of polynomial $z^4 +1$ which are the fourth roots of $-1$. In $C$ there is $z_1 =e^{i\pi/4}=\frac{1}{\sqrt{2}}+\frac{i}{\sqrt{2}}$ and $z_2=e^{3\pi/4}=-\frac{1}{\sqrt{2}}+\frac{i}{\sqrt{2}}$. So the residuals $B_1$ and $B_2$ for $z_1$ and $z_2$ are simple poles and that \begin{align} B_1&=\frac{1}{4 z_1^3}\frac{z_1}{z_1}=-\frac{z_1}{4} \\ B_2&=\frac{1}{4z_2^3}\frac{z_2}{z_2}=-\frac{z_2}{4} \end{align} And the sum of residuals is $$B_1+B_2=-\frac{1}{4}(z_1 + z_2)=-\frac{1}{4}\left(\frac{1}{\sqrt{2}}+\frac{1}{\sqrt{2}} \right)=-\frac{i}{2 \sqrt{2}}$$ So my integral should be $$\int\limits_{-\infty}^{\infty}\frac{1}{x^4+1} dx =\frac 12 \times 2\pi i (B_1+B_2)=\frac{\pi}{\sqrt{2}}$$ Is this valid?,,"['complex-analysis', 'residue-calculus']"
2,Why is Riemann integration used in complex analysis and not Lebesgue integration?,Why is Riemann integration used in complex analysis and not Lebesgue integration?,,"In the development of complex analysis you use Riemann integration and not Lebesgue integration to define line integrals. My questions are: Are the theories developed the same? (i.e. does it not matter which integral you use in the development? Since all the functions usually involved are analytic or meromorphic can you use things such as analytic is equivalent to having a power series representation and uniform convergence within the radius of convergence to somehow show that the choice doesn't matter. I feel as if the function involved is analytic and has a finite radius of convergence this should be the case but I'm not so sure about what would happen if the function was meromorphic and/or has an infinite radius of convergence (Am I on the right track?)) If the theories developed are the same, does it become significantly easier to develop the theory with Riemann integration rather than Lebesgue integration. If they are not the same, what are examples to show that show they are different?","In the development of complex analysis you use Riemann integration and not Lebesgue integration to define line integrals. My questions are: Are the theories developed the same? (i.e. does it not matter which integral you use in the development? Since all the functions usually involved are analytic or meromorphic can you use things such as analytic is equivalent to having a power series representation and uniform convergence within the radius of convergence to somehow show that the choice doesn't matter. I feel as if the function involved is analytic and has a finite radius of convergence this should be the case but I'm not so sure about what would happen if the function was meromorphic and/or has an infinite radius of convergence (Am I on the right track?)) If the theories developed are the same, does it become significantly easier to develop the theory with Riemann integration rather than Lebesgue integration. If they are not the same, what are examples to show that show they are different?",,['complex-analysis']
3,Unusual evaluation of $\sum \frac{1}{n^2}$,Unusual evaluation of,\sum \frac{1}{n^2},"Assume the formula $$\sum_{n=-\infty}^\infty\frac{1}{(n+u)^2}=\frac{\pi^2}{(\sin \pi u)^2},$$ where $u\notin\Bbb Z$. I have been trying to prove that $$\sum_{n=1}^\infty\frac{1}{n^2}=\frac{\pi^2}{6}.$$Setting $u=1/2$, I was able to derive $$\sum_{n=0}^\infty\frac{1}{(2n+1)^2}=\frac{\pi^2}{8},$$but I haven't made any progress toward proving the value of $\zeta(2)$. Any suggestions?","Assume the formula $$\sum_{n=-\infty}^\infty\frac{1}{(n+u)^2}=\frac{\pi^2}{(\sin \pi u)^2},$$ where $u\notin\Bbb Z$. I have been trying to prove that $$\sum_{n=1}^\infty\frac{1}{n^2}=\frac{\pi^2}{6}.$$Setting $u=1/2$, I was able to derive $$\sum_{n=0}^\infty\frac{1}{(2n+1)^2}=\frac{\pi^2}{8},$$but I haven't made any progress toward proving the value of $\zeta(2)$. Any suggestions?",,['complex-analysis']
4,Is $\sqrt{z}$ a meromorphic function?,Is  a meromorphic function?,\sqrt{z},"The literature seems rather coy on this point. While $\sqrt{z}$ is not meromorphic on the complex plane $\mathbb{C}$, can it be regarded as globally meromorphic on the appropriate Riemann surface (two branched copies of $\mathbb{C}$), or (equivalently?) locally meromorphic at $z=0$? Moreover, can the root of the function at $z=0$ be regarded as a zero of order $1/2$? And moreover, is $1/\sqrt{z}$ also meromorphic on the surface, and can it be regarded as having a pole of order $1/2$? EDIT: Clarified(?) that I was asking whether the function globally meromorphic on $2 \mathbb{C}$.","The literature seems rather coy on this point. While $\sqrt{z}$ is not meromorphic on the complex plane $\mathbb{C}$, can it be regarded as globally meromorphic on the appropriate Riemann surface (two branched copies of $\mathbb{C}$), or (equivalently?) locally meromorphic at $z=0$? Moreover, can the root of the function at $z=0$ be regarded as a zero of order $1/2$? And moreover, is $1/\sqrt{z}$ also meromorphic on the surface, and can it be regarded as having a pole of order $1/2$? EDIT: Clarified(?) that I was asking whether the function globally meromorphic on $2 \mathbb{C}$.",,"['complex-analysis', 'riemann-surfaces']"
5,The infinite integral of $\frac{\sin x}{x}$ using complex analysis,The infinite integral of  using complex analysis,\frac{\sin x}{x},"The problem i came across is the evaluation of $$\int_0^\infty\frac{\sin x}{x}\,dx$$ I chose the function $f(z) = \dfrac{e^{iz}}{z}$ and took a contour of $[\varepsilon , R ] + [R , R+iy] + [-R+iy , R+iy] + [-R,-R+iy]+[-R, -\varepsilon]$ . The problem is how do I continue now to find integrals on each of these segments ?","The problem i came across is the evaluation of $$\int_0^\infty\frac{\sin x}{x}\,dx$$ I chose the function $f(z) = \dfrac{e^{iz}}{z}$ and took a contour of $[\varepsilon , R ] + [R , R+iy] + [-R+iy , R+iy] + [-R,-R+iy]+[-R, -\varepsilon]$ . The problem is how do I continue now to find integrals on each of these segments ?",,['complex-analysis']
6,Conformally mapping an ellipse into the unit circle,Conformally mapping an ellipse into the unit circle,,"I'm currently studying for a complex analysis final and I don't think I've really developed the intuition for conformal mappings yet. I'm attempting a problem from Ahlfors: map the outside of the ellipse $(x/a)^2+(y/b)^2=1$ onto $|w|<1$ with preservation of symmetries. I believe I should use the inverse of the Joukowski transformation at some point (as it maps ellipses to circles) to get a circle of radius $R$ and then rescale. However, I run into trouble when I try to find an $R$ that will work. Any thoughts?","I'm currently studying for a complex analysis final and I don't think I've really developed the intuition for conformal mappings yet. I'm attempting a problem from Ahlfors: map the outside of the ellipse $(x/a)^2+(y/b)^2=1$ onto $|w|<1$ with preservation of symmetries. I believe I should use the inverse of the Joukowski transformation at some point (as it maps ellipses to circles) to get a circle of radius $R$ and then rescale. However, I run into trouble when I try to find an $R$ that will work. Any thoughts?",,"['complex-analysis', 'analysis', 'conformal-geometry']"
7,What's the difference between Complex infinity and undefined?,What's the difference between Complex infinity and undefined?,,Can somebody please expand upon the specific meaning of these two similar mathematical ideas and provide usage examples of each one?  Thank you!,Can somebody please expand upon the specific meaning of these two similar mathematical ideas and provide usage examples of each one?  Thank you!,,"['complex-analysis', 'recreational-mathematics', 'examples-counterexamples', 'infinity']"
8,"difference between conformal map, biholomorphic map and automorphism","difference between conformal map, biholomorphic map and automorphism",,"Could anyone tell me what the difference is between a map which is conformal, bi-holomorphic and an automorphism from $D\rightarrow D$ or $D$ to the upper half plane (in that case I know that is not automorphism)? Maybe I am getting confused about terminology? Please someone explain with examples.","Could anyone tell me what the difference is between a map which is conformal, bi-holomorphic and an automorphism from or to the upper half plane (in that case I know that is not automorphism)? Maybe I am getting confused about terminology? Please someone explain with examples.",D\rightarrow D D,['complex-analysis']
9,Why does $z^{-1}$ not have an antiderivative?,Why does  not have an antiderivative?,z^{-1},"I had been given the question as shown in the following, with the answer also given. EXAMPLE 6.13. Let $\gamma$ be the circle with centre 0 and radius 1 traced anticlockwise. By integrating along $\gamma$ , show that there is no function on $\mathbb{C} \backslash\{0\}$ with derivative $z^{-1}$ . SOLUTION. We can parametrise $\gamma$ as $z=e^{i t}$ for $0 \leq t \leq 2 \pi$ . We have $$ d z=i e^{i t} d t=i z d t $$ hence $$ \int_{\gamma} z^{-1} d z=\int_{0}^{2 \pi} z^{-1} i z d t=\int_{0}^{2 \pi} i d t=2 \pi i \neq 0 $$ Since $\gamma$ is a closed path, it follows that $z^{-1}$ cannot have an anti-derivative on $\mathbb{C} \backslash\{0\}$ . Surely however the anti-derivative of $z^{-1}$ would be $\log(z)$ ? I have seen a similar question asked, however, I didn't see the answer to this specific question.","I had been given the question as shown in the following, with the answer also given. EXAMPLE 6.13. Let be the circle with centre 0 and radius 1 traced anticlockwise. By integrating along , show that there is no function on with derivative . SOLUTION. We can parametrise as for . We have hence Since is a closed path, it follows that cannot have an anti-derivative on . Surely however the anti-derivative of would be ? I have seen a similar question asked, however, I didn't see the answer to this specific question.","\gamma \gamma \mathbb{C} \backslash\{0\} z^{-1} \gamma z=e^{i t} 0 \leq t \leq 2 \pi 
d z=i e^{i t} d t=i z d t
 
\int_{\gamma} z^{-1} d z=\int_{0}^{2 \pi} z^{-1} i z d t=\int_{0}^{2 \pi} i d t=2 \pi i \neq 0
 \gamma z^{-1} \mathbb{C} \backslash\{0\} z^{-1} \log(z)","['complex-analysis', 'logarithms']"
10,Integrating $\int_0^\infty \frac{1-\cos x }{x^2}dx$ via contour integral.,Integrating  via contour integral.,\int_0^\infty \frac{1-\cos x }{x^2}dx,"In Stein's Complex Analysis notes, the following exampleis given. They then proceed to calculate the integral over the small semicircle. My question is, why is it necessary to dodge the origin? Afterall, the singularity at $z=0$ is removable?","In Stein's Complex Analysis notes, the following exampleis given. They then proceed to calculate the integral over the small semicircle. My question is, why is it necessary to dodge the origin? Afterall, the singularity at $z=0$ is removable?",,['complex-analysis']
11,Why holomorphic injection on $\mathbb{C}^n$ must be biholomorphic?,Why holomorphic injection on  must be biholomorphic?,\mathbb{C}^n,This result is certainly right in the 1-dimensional case. But I don't know how to show the general case by induction. Can anyone tell me the detail please?,This result is certainly right in the 1-dimensional case. But I don't know how to show the general case by induction. Can anyone tell me the detail please?,,"['complex-analysis', 'complex-geometry', 'several-complex-variables']"
12,About the determination of complex logarithm,About the determination of complex logarithm,,"Although it must be a silly question, I am really confused. For complex logarithm, in general, $$\log(z_1z_2)\neq\log(z_1)+\log(z_2)$$ even if the logarithm is already determined. I think this is true, right? However, in some proof from complex analysis, one uses logarithm to let a product (even infinite product) be a summation without considering any determination of logarithm. Is that correct? For example: We know that $$\prod_{p}\left(1-\frac{1}{p^s}\right)^{-1}=\sum_{n=1}^\infty\frac{1}{n^s}=\zeta(s)$$ holds for $Re s>1$. In some books, the author states that by  $$\log\zeta(s)=-\sum_p\log\left(1-\frac{1}{p^s}\right)$$ Is that correct? Sometimes, I think complex logarithm is really ""annoying"", because I always have to worry about the determination of it. Do I have to worry about that? Thank you very much!","Although it must be a silly question, I am really confused. For complex logarithm, in general, $$\log(z_1z_2)\neq\log(z_1)+\log(z_2)$$ even if the logarithm is already determined. I think this is true, right? However, in some proof from complex analysis, one uses logarithm to let a product (even infinite product) be a summation without considering any determination of logarithm. Is that correct? For example: We know that $$\prod_{p}\left(1-\frac{1}{p^s}\right)^{-1}=\sum_{n=1}^\infty\frac{1}{n^s}=\zeta(s)$$ holds for $Re s>1$. In some books, the author states that by  $$\log\zeta(s)=-\sum_p\log\left(1-\frac{1}{p^s}\right)$$ Is that correct? Sometimes, I think complex logarithm is really ""annoying"", because I always have to worry about the determination of it. Do I have to worry about that? Thank you very much!",,['complex-analysis']
13,How to determine the type of singularities,How to determine the type of singularities,,"I have the following functions: a) $\displaystyle f:\mathbb{C}\backslash\{0\}\rightarrow\mathbb{C},\ f(z)=\frac{1}{e^{\frac{1}{z}}-1}$ b) $\displaystyle f:\mathbb{C}\backslash\{0,2\}\rightarrow\mathbb{C},\ f(z)=\frac{\sin z ^2}{z^2(z-2)}$ c)  $\displaystyle f:\mathbb{C}\backslash\{0\}\rightarrow\mathbb{C},\ f(z)=\cos\left(\frac{1}{z}\right)$ d) $\displaystyle f:\mathbb{C}\backslash\{0\}\rightarrow\mathbb{C},\ f(z)=\frac{1}{1-\cos\left(\frac{1}{z}\right)}$ e) $\displaystyle f:\mathbb{C}\backslash\{0\}\rightarrow\mathbb{C},\ f(z)=\frac{1}{\sin\left(\frac{1}{z}\right)}$ What would the quickest approach to determine if $f$ has a removable singularity, a pole or an essential singularity? What would be the thinking $behind$ the approach? Edit: What I know/ What I have tried: I know that if we have an open set $\Omega \subseteq \mathbb{C}$, then we call an isolated singularity, a point, where $f$ is not analytic in $\Omega$ ($f \in H(\Omega \backslash \{a\}$). The functions in (a)-(e) are not defined on some values. So I suspect, that these are the first candidates for singularities. For instance in (a), it would be 0. In (b), it would be 0 and 2. Question: Could there be any other points where these functions are not analytic? Let's call our isolated singularity $a$. Furthermore I know that we have 3 types of singularities: 1) removable This would be the case when $f$ is bounded on the disk $D(a,r)$ for some $r>0$. 2) pole There is $c_1, ... , c_m \in \mathbb{C},\ m\in\mathbb{N}$ with $c_m \neq 0$, so that: $$f(z)-\sum\limits_{k=1}^m c_k\cdot\frac{1}{(z-a)^k},\ z \in \Omega \backslash \{a\})$$ has a removable singularity in $a$, then we call $a$ a pole. We also know that in this case: $|f(z)|\rightarrow \infty$ when $z\rightarrow a$. 3) essential If the disk $D(a,r) \subseteq \Omega$, then $f(D(a,r)\backslash\{a\})$ is dense in $\mathbb{C}$ and we call $a$ essential singularity. The books that I have been using (Zill - Complex Analysis and Murray Spiegel - Complex Analysis) both expand the function as a Laurent series and then check the singularities. But how do I do this, if I use the definitions above? It doesn't seem to me to be so straight forward... What I would want to learn a method which allows me to do the following: I look at the function and the I try approach X to determine if it has a removable singularity. If not continue with approach Y to see if we have a pole and if not Z, to see if we have an essential singularity. An algorithmic set of steps so to speak, to check such functions as presented in (a) to (e). Edit 2: This is not homework and I would start a bounty if I could, because I need to understand how this works by tommorow. Unfortunately I can start a bounty only tommorow... Edit 3: Is this so easy? Because using the definitions, I am getting nowhere in determing the types of singularities...","I have the following functions: a) $\displaystyle f:\mathbb{C}\backslash\{0\}\rightarrow\mathbb{C},\ f(z)=\frac{1}{e^{\frac{1}{z}}-1}$ b) $\displaystyle f:\mathbb{C}\backslash\{0,2\}\rightarrow\mathbb{C},\ f(z)=\frac{\sin z ^2}{z^2(z-2)}$ c)  $\displaystyle f:\mathbb{C}\backslash\{0\}\rightarrow\mathbb{C},\ f(z)=\cos\left(\frac{1}{z}\right)$ d) $\displaystyle f:\mathbb{C}\backslash\{0\}\rightarrow\mathbb{C},\ f(z)=\frac{1}{1-\cos\left(\frac{1}{z}\right)}$ e) $\displaystyle f:\mathbb{C}\backslash\{0\}\rightarrow\mathbb{C},\ f(z)=\frac{1}{\sin\left(\frac{1}{z}\right)}$ What would the quickest approach to determine if $f$ has a removable singularity, a pole or an essential singularity? What would be the thinking $behind$ the approach? Edit: What I know/ What I have tried: I know that if we have an open set $\Omega \subseteq \mathbb{C}$, then we call an isolated singularity, a point, where $f$ is not analytic in $\Omega$ ($f \in H(\Omega \backslash \{a\}$). The functions in (a)-(e) are not defined on some values. So I suspect, that these are the first candidates for singularities. For instance in (a), it would be 0. In (b), it would be 0 and 2. Question: Could there be any other points where these functions are not analytic? Let's call our isolated singularity $a$. Furthermore I know that we have 3 types of singularities: 1) removable This would be the case when $f$ is bounded on the disk $D(a,r)$ for some $r>0$. 2) pole There is $c_1, ... , c_m \in \mathbb{C},\ m\in\mathbb{N}$ with $c_m \neq 0$, so that: $$f(z)-\sum\limits_{k=1}^m c_k\cdot\frac{1}{(z-a)^k},\ z \in \Omega \backslash \{a\})$$ has a removable singularity in $a$, then we call $a$ a pole. We also know that in this case: $|f(z)|\rightarrow \infty$ when $z\rightarrow a$. 3) essential If the disk $D(a,r) \subseteq \Omega$, then $f(D(a,r)\backslash\{a\})$ is dense in $\mathbb{C}$ and we call $a$ essential singularity. The books that I have been using (Zill - Complex Analysis and Murray Spiegel - Complex Analysis) both expand the function as a Laurent series and then check the singularities. But how do I do this, if I use the definitions above? It doesn't seem to me to be so straight forward... What I would want to learn a method which allows me to do the following: I look at the function and the I try approach X to determine if it has a removable singularity. If not continue with approach Y to see if we have a pole and if not Z, to see if we have an essential singularity. An algorithmic set of steps so to speak, to check such functions as presented in (a) to (e). Edit 2: This is not homework and I would start a bounty if I could, because I need to understand how this works by tommorow. Unfortunately I can start a bounty only tommorow... Edit 3: Is this so easy? Because using the definitions, I am getting nowhere in determing the types of singularities...",,['complex-analysis']
14,Show that an entire function $f$ s.t. $|f(z)|>1$ for $|z|>1$ is a polynomial,Show that an entire function  s.t.  for  is a polynomial,f |f(z)|>1 |z|>1,"I have been struggling on the following problem. Suppose $f$ is an entire analytic function such that $|f(z)|>1$ if $|z|>1$. Show that $f$ is a polynomial. My idea is as followed: all zeros of $|f(z)|$ lie inside $|z|\leq 1$. Applying Argument Principle, we can show that number of zeros of $f$ is bounded. So we can assume $f(z)=(z-z_1)...(z-z_M)g(z)$ where g is entire analytic without any zeros. Then I would like to apply Liouville's Theorem: the point is that it isn't too clear to me why $|\dfrac{1}{g(z)}|$ is a bounded function.","I have been struggling on the following problem. Suppose $f$ is an entire analytic function such that $|f(z)|>1$ if $|z|>1$. Show that $f$ is a polynomial. My idea is as followed: all zeros of $|f(z)|$ lie inside $|z|\leq 1$. Applying Argument Principle, we can show that number of zeros of $f$ is bounded. So we can assume $f(z)=(z-z_1)...(z-z_M)g(z)$ where g is entire analytic without any zeros. Then I would like to apply Liouville's Theorem: the point is that it isn't too clear to me why $|\dfrac{1}{g(z)}|$ is a bounded function.",,['complex-analysis']
15,Taking the derivative under a principal value integral,Taking the derivative under a principal value integral,,"I'm interested in showing that: $$ \frac{d}{dt}P \; \int_{-\infty}^{\infty} \frac{\phi(x)}{x-t}dt = P \int_{-\infty}^{\infty}\frac{\phi(x)-\phi(t)}{(x-t)^2}dt $$ where $\phi(x)$ is a test function (goes to zero at $-\infty$ and $\infty$ fast enough that we don't have to worry about x not going to zero fast enough) The problem is that when I attempt this: $$ \frac{d}{dt}P \; \int_{-\infty}^{\infty} \frac{\phi(x)}{x-t}dt = \lim_{\epsilon \rightarrow 0} \left\{  -\frac{\phi(t+\epsilon)}{\epsilon}-\frac{\phi(t-\epsilon)}{\epsilon} +\int_{-\infty}^{t-\epsilon}\frac{\phi(x)}{(x-t)^2} + \int_{t+\epsilon}^{\infty}\frac{\phi(x)}{(x-t)^2} \right\} $$ But I don't know where to go from here... and I'm not sure I'm on the right track.  The $\phi(t)$ term doesn't seem to want to pop out. Could this be some sort of dirac delta identity I'm missing? Thanks!","I'm interested in showing that: $$ \frac{d}{dt}P \; \int_{-\infty}^{\infty} \frac{\phi(x)}{x-t}dt = P \int_{-\infty}^{\infty}\frac{\phi(x)-\phi(t)}{(x-t)^2}dt $$ where $\phi(x)$ is a test function (goes to zero at $-\infty$ and $\infty$ fast enough that we don't have to worry about x not going to zero fast enough) The problem is that when I attempt this: $$ \frac{d}{dt}P \; \int_{-\infty}^{\infty} \frac{\phi(x)}{x-t}dt = \lim_{\epsilon \rightarrow 0} \left\{  -\frac{\phi(t+\epsilon)}{\epsilon}-\frac{\phi(t-\epsilon)}{\epsilon} +\int_{-\infty}^{t-\epsilon}\frac{\phi(x)}{(x-t)^2} + \int_{t+\epsilon}^{\infty}\frac{\phi(x)}{(x-t)^2} \right\} $$ But I don't know where to go from here... and I'm not sure I'm on the right track.  The $\phi(t)$ term doesn't seem to want to pop out. Could this be some sort of dirac delta identity I'm missing? Thanks!",,"['complex-analysis', 'improper-integrals']"
16,What is the difference between a singularity and a pole?,What is the difference between a singularity and a pole?,,"From what I could find, a singularity is a point at which an equation, surface, etc., blows up or becomes degenerate. And a pole of a function is an isolated singular point a of single-valued character of an analytic function $f(z)$ of the complex variable $z$ for which $|f(z)|$ increases without bound when $z$ approaches $a$: $\lim_{z\rightarrow a}f(z) = \infty.$ I really don't fully understand this definition of a pole, like (what is an isolated singular point) and the limit says for $\lim_{z\rightarrow a}f(z) = \infty.$ What is $a$ that $z$ should approach? $f = 1/(z-1) \ e^{z}$ Can I say $f$ has a singularity at $z = 1$ because we get $1/0$ at that point i.e. blows up and gives $\infty$?","From what I could find, a singularity is a point at which an equation, surface, etc., blows up or becomes degenerate. And a pole of a function is an isolated singular point a of single-valued character of an analytic function $f(z)$ of the complex variable $z$ for which $|f(z)|$ increases without bound when $z$ approaches $a$: $\lim_{z\rightarrow a}f(z) = \infty.$ I really don't fully understand this definition of a pole, like (what is an isolated singular point) and the limit says for $\lim_{z\rightarrow a}f(z) = \infty.$ What is $a$ that $z$ should approach? $f = 1/(z-1) \ e^{z}$ Can I say $f$ has a singularity at $z = 1$ because we get $1/0$ at that point i.e. blows up and gives $\infty$?",,['complex-analysis']
17,"If $f$ is an entire function where every power series expansion has at least one 0 term, show it is a polynomial","If  is an entire function where every power series expansion has at least one 0 term, show it is a polynomial",f,Suppose $f$ is entire and that in every power series $f(z)=\sum_{n=0}^\infty c_n(z-a)^n$ at least one coefficient is $0$. Prove that $f$ is a polynomial.,Suppose $f$ is entire and that in every power series $f(z)=\sum_{n=0}^\infty c_n(z-a)^n$ at least one coefficient is $0$. Prove that $f$ is a polynomial.,,['complex-analysis']
18,Calculus of residue of function around poles of fractional order (complex analysis),Calculus of residue of function around poles of fractional order (complex analysis),,The complex function $f(z)=\frac{1}{\sqrt{z^2+r_0z}}$ with $r_0>0$ has two poles (at $z=0$ and $z=-r_0$). But they are not simple poles. They are poles of fractional order. Am I right? How I can calculate residue of the function at the poles? Please help me. Thanks Vahid,The complex function $f(z)=\frac{1}{\sqrt{z^2+r_0z}}$ with $r_0>0$ has two poles (at $z=0$ and $z=-r_0$). But they are not simple poles. They are poles of fractional order. Am I right? How I can calculate residue of the function at the poles? Please help me. Thanks Vahid,,"['complex-analysis', 'residue-calculus']"
19,"If $f$ is a nonconstant entire function such that $|f(z)|\geq M|z|^n$ for $|z|\geq R$, then $f$ is a polynomial of degree atleast $n$.","If  is a nonconstant entire function such that  for , then  is a polynomial of degree atleast .",f |f(z)|\geq M|z|^n |z|\geq R f n,"I have a question in my assignment : If $f$ is a nonconstant entire function such that $|f(z)|\geq M|z|^n$ for $|z|\geq R$ for some $n\in\mathbb N$ and some $M$ and $R$ in $(0,\infty)$ show that $f$ is a polynomial of degree atleast $n$ . Now , I defined a function $\ g(z) = \frac {1}{f(z)}\ $ such that $\ |g(z)| \le \frac{1}{M{|z|}^n}.$ Now , by using the cauchy inequality $$|g^{(n)}(z)| \le  \frac{n!}{R^n |z|^nM}.$$ Considering that $ g(z) $ is an analytic function , it has a radius of convergence $ \infty $ $ \implies\  g^{(n)}(z) = 0.$ But if we go by this approach , then $ g^{(n)}(z) = 0 \ $ for any n . Also how can we be so sure that $ f(z) \neq 0 $ for any z ? Is my reasoning correct or is there some other way to solve it ?","I have a question in my assignment : If is a nonconstant entire function such that for for some and some and in show that is a polynomial of degree atleast . Now , I defined a function such that Now , by using the cauchy inequality Considering that is an analytic function , it has a radius of convergence But if we go by this approach , then for any n . Also how can we be so sure that for any z ? Is my reasoning correct or is there some other way to solve it ?","f |f(z)|\geq M|z|^n |z|\geq R n\in\mathbb N M R (0,\infty) f n \ g(z) = \frac {1}{f(z)}\  \ |g(z)| \le \frac{1}{M{|z|}^n}. |g^{(n)}(z)| \le  \frac{n!}{R^n |z|^nM}.  g(z)   \infty   \implies\  g^{(n)}(z) = 0.  g^{(n)}(z) = 0 \   f(z) \neq 0 ","['complex-analysis', 'polynomials', 'cauchy-integral-formula', 'entire-functions']"
20,If $f(a+re^{it})\in \Bbb{R}$ for all $t\in \Bbb{R}$ then $f$ is constant.,If  for all  then  is constant.,f(a+re^{it})\in \Bbb{R} t\in \Bbb{R} f,"I would like to prove that if $f(a+re^{it})\in \Bbb{R}$ for all $t\in \Bbb{R}$ then $f$ is constant. Of course $f$ is holomorphic on a domain $U$ and $r>0$ such that $\overline{D(a,r)}$ is included in $U$. This question arose from another one witch is if $t\mapsto\vert f(a+re^{it}) \vert$ is constant and doesn't vanish on $U$ (domain) then $f$ is constant. I am stuck here, It's cleary related to the maximum principle but how can I use it here ?","I would like to prove that if $f(a+re^{it})\in \Bbb{R}$ for all $t\in \Bbb{R}$ then $f$ is constant. Of course $f$ is holomorphic on a domain $U$ and $r>0$ such that $\overline{D(a,r)}$ is included in $U$. This question arose from another one witch is if $t\mapsto\vert f(a+re^{it}) \vert$ is constant and doesn't vanish on $U$ (domain) then $f$ is constant. I am stuck here, It's cleary related to the maximum principle but how can I use it here ?",,[]
21,"Show that the set of one-to-one holomorphic maps $\Bbb{C}\setminus\{a,b,c\} \to \Bbb{C}\setminus\{a,b,c\}$ forms a finite group.",Show that the set of one-to-one holomorphic maps  forms a finite group.,"\Bbb{C}\setminus\{a,b,c\} \to \Bbb{C}\setminus\{a,b,c\}","Let $\Omega = \mathbb{C}\setminus\{a, b, c\}$ be the complement of three distinct points in the complex plane. Show that the set of one-to-one holomorphic maps $f : \Omega \to \Omega$ forms a finite group. Is the order of this group independent of the choice $\{a, b, c\}$? By Casorati-Weierstrass, none of the points can be essential singularities for $f$. However, it is possible that at least one of the points is a pole, for example $f : \mathbb{C}\setminus\{-1, 0, 1\} \to \mathbb{C}\setminus\{-1, 0, 1\}$, $z \mapsto \frac{1}{z}$.   For this choice of points, we also have $z \mapsto z$, $z \mapsto -z$, and $z \mapsto -\frac{1}{z}$, so $\mathbb{Z}_2\times\mathbb{Z}_2 \leq \operatorname{Aut}(\mathbb{C}\setminus\{-1, 0, 1\})$. Furthermore, for any $a, b, c$ on a straight line, we can conjugate by a rotation, a translation, and a rescaling to get a corresponding subgroup. If I had to guess, the group will depend on whether or not $a, b, c$ are collinear. Any hints on how to proceed would be very much appreciated.","Let $\Omega = \mathbb{C}\setminus\{a, b, c\}$ be the complement of three distinct points in the complex plane. Show that the set of one-to-one holomorphic maps $f : \Omega \to \Omega$ forms a finite group. Is the order of this group independent of the choice $\{a, b, c\}$? By Casorati-Weierstrass, none of the points can be essential singularities for $f$. However, it is possible that at least one of the points is a pole, for example $f : \mathbb{C}\setminus\{-1, 0, 1\} \to \mathbb{C}\setminus\{-1, 0, 1\}$, $z \mapsto \frac{1}{z}$.   For this choice of points, we also have $z \mapsto z$, $z \mapsto -z$, and $z \mapsto -\frac{1}{z}$, so $\mathbb{Z}_2\times\mathbb{Z}_2 \leq \operatorname{Aut}(\mathbb{C}\setminus\{-1, 0, 1\})$. Furthermore, for any $a, b, c$ on a straight line, we can conjugate by a rotation, a translation, and a rescaling to get a corresponding subgroup. If I had to guess, the group will depend on whether or not $a, b, c$ are collinear. Any hints on how to proceed would be very much appreciated.",,['complex-analysis']
22,"For an analytic function $f(z)$, $|f(z)^2-1|<1$ implies $\Re f(z)>0$ or $\Re f(z)<0$?","For an analytic function ,  implies  or ?",f(z) |f(z)^2-1|<1 \Re f(z)>0 \Re f(z)<0,"Doing a bit of self study, and I'm unsure about a problem. It says, Suppose $f(z)$ (a complex valued function) is analytic and satisfies the condition $|f(z)^2-1|<1$ in a region $\Omega$. Show that either $\Re f(z)>0$ or $\Re f(z)<0$ throughout $\Omega$. I write $f=u+iv$ and suppose to the contrary that $\Re f(z)=0$ at some point $z_0$. Then $f(z_0)^2=-v(z_0)^2$. But $v$ is real valued, and so $$ |f(z_0)^2-1|=|-v(z_0)^2-1|\geq 1 $$ a contradiction. What makes me uneasy is I don't see if I used that fact that $f$ is analytic. Did I interpret the question correctly, or did it mean that $\Re f(z)>0$ on all of $\Omega$ or $\Re f(z)<0$ on all of $\Omega$, but doesn't take both positive and negative values? Thanks.","Doing a bit of self study, and I'm unsure about a problem. It says, Suppose $f(z)$ (a complex valued function) is analytic and satisfies the condition $|f(z)^2-1|<1$ in a region $\Omega$. Show that either $\Re f(z)>0$ or $\Re f(z)<0$ throughout $\Omega$. I write $f=u+iv$ and suppose to the contrary that $\Re f(z)=0$ at some point $z_0$. Then $f(z_0)^2=-v(z_0)^2$. But $v$ is real valued, and so $$ |f(z_0)^2-1|=|-v(z_0)^2-1|\geq 1 $$ a contradiction. What makes me uneasy is I don't see if I used that fact that $f$ is analytic. Did I interpret the question correctly, or did it mean that $\Re f(z)>0$ on all of $\Omega$ or $\Re f(z)<0$ on all of $\Omega$, but doesn't take both positive and negative values? Thanks.",,['complex-analysis']
23,Can there be a point on a Riemann surface such that every rational function is ramified at this point?,Can there be a point on a Riemann surface such that every rational function is ramified at this point?,,"Let $X$ be a compact connected Riemann surface, and let $S\subset X$ be a finite subset. Does there exist a morphism $f:X\to \mathbf{P}^1(\mathbf{C})$ which is unramified at the points of $S$? I'm interested in the case where $X$ is of genus at least $2$. (The genus zero case is trivial: take $f$ to be the identity.) The answer is trivial when $S$ is empty. (Any morphism $f:X\to \mathbf{P}^1(\mathbf{C})$ will do.) Let $h:X\to \mathbf{P}^1(\mathbf{C})$ be a morphism with ramification locus $R(h)$. Then, if $S\subset X\backslash R(h)$, the answer is yes. How effective can our answer be? That is, suppose that there exists such an $f$. Then, can we bound its degree? The title is a special case of the above question: take $S=\{\textrm{pt}\}$.","Let $X$ be a compact connected Riemann surface, and let $S\subset X$ be a finite subset. Does there exist a morphism $f:X\to \mathbf{P}^1(\mathbf{C})$ which is unramified at the points of $S$? I'm interested in the case where $X$ is of genus at least $2$. (The genus zero case is trivial: take $f$ to be the identity.) The answer is trivial when $S$ is empty. (Any morphism $f:X\to \mathbf{P}^1(\mathbf{C})$ will do.) Let $h:X\to \mathbf{P}^1(\mathbf{C})$ be a morphism with ramification locus $R(h)$. Then, if $S\subset X\backslash R(h)$, the answer is yes. How effective can our answer be? That is, suppose that there exists such an $f$. Then, can we bound its degree? The title is a special case of the above question: take $S=\{\textrm{pt}\}$.",,"['complex-analysis', 'algebraic-geometry', 'riemann-surfaces', 'algebraic-curves']"
24,Relation between hypergeometric and gamma functions,Relation between hypergeometric and gamma functions,,"Show that, for a positive integer $n$, $$F\left(-\frac{n}{2},-\frac{n}{2}+\frac{1}{2};n+\frac{3}{2};-\frac{1}{3}\right)=\left(\frac{8}{9}\right)^n\frac{\Gamma\left(\frac{4}{3}\right)\Gamma\left(n+\frac{3}{2}\right)}{\Gamma\left(\frac{3}{2}\right)\Gamma\left(n+\frac{4}{3}\right)}.$$ I can identify the right hand side (using the definition of the Pochhammer symbol) with: $$\left(\frac{8}{9}\right)^n\frac{\Gamma\left(\frac{4}{3}\right)\Gamma\left(n+\frac{3}{2}\right)}{\Gamma\left(\frac{3}{2}\right)\Gamma\left(n+\frac{4}{3}\right)}=\left(\frac{8}{9}\right)^n\frac{\left(\frac{3}{2}\right)_n}{\left(\frac{4}{3}\right)_n},$$ and get an expression for this in terms of factorials, but I'm not sure how to simplify the left hand side - it gets quite messy!","Show that, for a positive integer $n$, $$F\left(-\frac{n}{2},-\frac{n}{2}+\frac{1}{2};n+\frac{3}{2};-\frac{1}{3}\right)=\left(\frac{8}{9}\right)^n\frac{\Gamma\left(\frac{4}{3}\right)\Gamma\left(n+\frac{3}{2}\right)}{\Gamma\left(\frac{3}{2}\right)\Gamma\left(n+\frac{4}{3}\right)}.$$ I can identify the right hand side (using the definition of the Pochhammer symbol) with: $$\left(\frac{8}{9}\right)^n\frac{\Gamma\left(\frac{4}{3}\right)\Gamma\left(n+\frac{3}{2}\right)}{\Gamma\left(\frac{3}{2}\right)\Gamma\left(n+\frac{4}{3}\right)}=\left(\frac{8}{9}\right)^n\frac{\left(\frac{3}{2}\right)_n}{\left(\frac{4}{3}\right)_n},$$ and get an expression for this in terms of factorials, but I'm not sure how to simplify the left hand side - it gets quite messy!",,"['complex-analysis', 'hypergeometric-function']"
25,Dog Bone Contour Integral,Dog Bone Contour Integral,,"Would someone please help me understand how to integrate $$ \ \int_0^1 (x^2-1)^{-1/2}dx\, ? $$ This is a homework problem from Marsden Basic Complex Analysis. The text book suggested using a ""dog bone"" contour and finding the residue of a branch of $(z^2-1)^{-1/2}$ at infinity. I believe the residue at infinity is 1. After factoring  $$ \ (z^2-1)^{-1/2}\ = (z-1)^{-1/2}\ (z+1)^{-1/2}\  $$ I chose a branch cut of $(-\infty , -1] \;$ for $\;(z+1)^{-1/2}$ and $(-\infty , 1]$ for $(z-1)^{-1/2}$.  I pretty sure that means   $\: -\pi \: <\arg(z-1)< \:\pi$ and $\: -\pi \: <\arg(z+1)< \:\pi$. This problem is so confusing. I've working on it for days and it's driving me crazy. Any help would be greatly appreciated.","Would someone please help me understand how to integrate $$ \ \int_0^1 (x^2-1)^{-1/2}dx\, ? $$ This is a homework problem from Marsden Basic Complex Analysis. The text book suggested using a ""dog bone"" contour and finding the residue of a branch of $(z^2-1)^{-1/2}$ at infinity. I believe the residue at infinity is 1. After factoring  $$ \ (z^2-1)^{-1/2}\ = (z-1)^{-1/2}\ (z+1)^{-1/2}\  $$ I chose a branch cut of $(-\infty , -1] \;$ for $\;(z+1)^{-1/2}$ and $(-\infty , 1]$ for $(z-1)^{-1/2}$.  I pretty sure that means   $\: -\pi \: <\arg(z-1)< \:\pi$ and $\: -\pi \: <\arg(z+1)< \:\pi$. This problem is so confusing. I've working on it for days and it's driving me crazy. Any help would be greatly appreciated.",,"['complex-analysis', 'contour-integration', 'residue-calculus']"
26,Image under an entire function.,Image under an entire function.,,Let $f$ be an entire function and $B$ be a bounded open set in $\mathbb {C} $. Prove that boundary of image of $B$ under $f$ is contained in image of boundary of $B$. Does the same result is true for unbounded open set in $\mathbb C.$,Let $f$ be an entire function and $B$ be a bounded open set in $\mathbb {C} $. Prove that boundary of image of $B$ under $f$ is contained in image of boundary of $B$. Does the same result is true for unbounded open set in $\mathbb C.$,,['complex-analysis']
27,Intuitively understanding Riemann surfaces,Intuitively understanding Riemann surfaces,,"I'm looking at the Riemann surface of $f(z) = z^{1/2}$ so the set $\{(z,w) \in \mathbb{C}^2 : w^2 = z \}$. I understand that the point of the riemann surface is to understand this multi-valued function. Now I visualise this as taking two copies of the complex plane both with a slit in them (negative real axis say) and then I put them on top of each other, flip the top one and then sort of join them up along the slit and that's fine and I can see that that will form some surface where the local coordinates are given by projection onto the z or w axis. But here I am just dealing with the domain of the function $f(z)$. My difficulty is when I think of the analogous situation with say $\{(x,y) \in \mathbb{R}^2 : y^2 = x\}$ this ""surface"" is just a line and I can see how given a point on this ""line"" we have an associated pair $(x,y)$. Now when I look at the Riemann surface described above all I see is the domain of the function $f(z)$ I can't see how each point of the surface is in anyway related to a pair $(z,w)$. I realise this is quite a vague question and I am struggling to put my frustration with this concept into words - I hope this makes sense! I've now added a (terrible) diagram which may help to illustrate my issue: Thanks","I'm looking at the Riemann surface of $f(z) = z^{1/2}$ so the set $\{(z,w) \in \mathbb{C}^2 : w^2 = z \}$. I understand that the point of the riemann surface is to understand this multi-valued function. Now I visualise this as taking two copies of the complex plane both with a slit in them (negative real axis say) and then I put them on top of each other, flip the top one and then sort of join them up along the slit and that's fine and I can see that that will form some surface where the local coordinates are given by projection onto the z or w axis. But here I am just dealing with the domain of the function $f(z)$. My difficulty is when I think of the analogous situation with say $\{(x,y) \in \mathbb{R}^2 : y^2 = x\}$ this ""surface"" is just a line and I can see how given a point on this ""line"" we have an associated pair $(x,y)$. Now when I look at the Riemann surface described above all I see is the domain of the function $f(z)$ I can't see how each point of the surface is in anyway related to a pair $(z,w)$. I realise this is quite a vague question and I am struggling to put my frustration with this concept into words - I hope this makes sense! I've now added a (terrible) diagram which may help to illustrate my issue: Thanks",,"['complex-analysis', 'riemann-surfaces']"
28,"How to prove $\int_1^\infty\frac{K(x)^2}x dx=\frac{i\,\pi^3}8$?",How to prove ?,"\int_1^\infty\frac{K(x)^2}x dx=\frac{i\,\pi^3}8","How can I prove the following identity? $$\int_1^\infty\frac{K(x)^2}x dx\stackrel{\color{#B0B0B0}?}=\frac{i\,\pi^3}8,\tag1$$ where $K(x)$ is the complete elliptic integral of the 1ˢᵗ kind : $$K(x)={_2F_1}\left(\frac12,\frac12;\ 1;\ x^2\right)\cdot\frac\pi2.\tag2$$","How can I prove the following identity? $$\int_1^\infty\frac{K(x)^2}x dx\stackrel{\color{#B0B0B0}?}=\frac{i\,\pi^3}8,\tag1$$ where $K(x)$ is the complete elliptic integral of the 1ˢᵗ kind : $$K(x)={_2F_1}\left(\frac12,\frac12;\ 1;\ x^2\right)\cdot\frac\pi2.\tag2$$",,"['complex-analysis', 'definite-integrals', 'special-functions', 'closed-form', 'elliptic-integrals']"
29,how to prove $\displaystyle \frac{\sin (2n+1)\theta}{\sin \theta} = .... $,how to prove,\displaystyle \frac{\sin (2n+1)\theta}{\sin \theta} = .... ,"How to prove  $$ \displaystyle \frac{\sin (2n+1)\theta}{\sin \theta} = (2n+1) \prod_{k=1}^{n}\left(1 - \frac{\sin^2 \theta}{\sin^2 \left( \frac{k\pi }{2n+1} \right ) } \right ) $$ So far, I manage to prove $ \displaystyle \frac{\sin (2n+1)\theta}{\sin \theta} = (2n+1) \prod_{k=1}^{2n}\left(1 - \frac{\sin \theta}{\sin \left( \frac{k\pi }{2n+1} \right ) } \right ) $ though I am not sure I am aright.","How to prove  $$ \displaystyle \frac{\sin (2n+1)\theta}{\sin \theta} = (2n+1) \prod_{k=1}^{n}\left(1 - \frac{\sin^2 \theta}{\sin^2 \left( \frac{k\pi }{2n+1} \right ) } \right ) $$ So far, I manage to prove $ \displaystyle \frac{\sin (2n+1)\theta}{\sin \theta} = (2n+1) \prod_{k=1}^{2n}\left(1 - \frac{\sin \theta}{\sin \left( \frac{k\pi }{2n+1} \right ) } \right ) $ though I am not sure I am aright.",,"['complex-analysis', 'trigonometry']"
30,How to show that $f$ is an odd function?,How to show that  is an odd function?,f,"An entire function $f$ takes real $z$ to real and purely imaginary to purely imaginary. We need to show that $f$ is an odd function. well, $f=\sum_{n=0}^{\infty}a_nz^n$ what I can say is $f(\mathbb{R})\subseteq\mathbb{R}$ and $f(\mathbb{iR})\subseteq\mathbb{iR}$ How to proceed, please give me hint.","An entire function $f$ takes real $z$ to real and purely imaginary to purely imaginary. We need to show that $f$ is an odd function. well, $f=\sum_{n=0}^{\infty}a_nz^n$ what I can say is $f(\mathbb{R})\subseteq\mathbb{R}$ and $f(\mathbb{iR})\subseteq\mathbb{iR}$ How to proceed, please give me hint.",,['complex-analysis']
31,"If $F$ is entire with removable singularity at $\infty$, then $F$ is constant?","If  is entire with removable singularity at , then  is constant?",F \infty F,"On page 24 of Krantz's Complex Analysis , there is the following proof: Proposition 2: If $F$ is entire and $F$ has a removable singularity at $\infty$, then $F$ is  constant. Proof: By examining $F(1/z)$, we see that $F$ must have a finite limit at $\infty$. Thus $F$ is bounded. By Liouville's theorem, $F$ is constant. It's mysterious to me what is meant by the first sentence. What does he mean by ""examining $F(1/z)$""? I tried expanding $F$ has a Laurent series around $0$, and then using the fact that $F(1/z)$ has the origin as a removable singularity, but I didn't get the conclusion. Is the idea to do this? Let  $$ F(z)=\sum_{n=-\infty}^{-1}a_nz^n+\sum_{n=0}^\infty a_nz^n $$ be the Laurent expansion around the origin. Then $$ F(1/z)=\sum_{n=-\infty}^{-1}a_nz^{-n}+\sum_{n=0}^\infty a_nz^{-n} $$ but since $F(1/z)$ has a removable singularity at the origin, we really have $$ F(1/z)=\sum_{n=-\infty}^{-1}a_nz^{-n}+a_0. $$ So $a_n=0$ for $n>0$, and thus $F(z)=\sum_{n=-\infty}^{-1}a_nz^n+a_0$, so $\lim_{z\to\infty}F(z)=a_0<\infty$?","On page 24 of Krantz's Complex Analysis , there is the following proof: Proposition 2: If $F$ is entire and $F$ has a removable singularity at $\infty$, then $F$ is  constant. Proof: By examining $F(1/z)$, we see that $F$ must have a finite limit at $\infty$. Thus $F$ is bounded. By Liouville's theorem, $F$ is constant. It's mysterious to me what is meant by the first sentence. What does he mean by ""examining $F(1/z)$""? I tried expanding $F$ has a Laurent series around $0$, and then using the fact that $F(1/z)$ has the origin as a removable singularity, but I didn't get the conclusion. Is the idea to do this? Let  $$ F(z)=\sum_{n=-\infty}^{-1}a_nz^n+\sum_{n=0}^\infty a_nz^n $$ be the Laurent expansion around the origin. Then $$ F(1/z)=\sum_{n=-\infty}^{-1}a_nz^{-n}+\sum_{n=0}^\infty a_nz^{-n} $$ but since $F(1/z)$ has a removable singularity at the origin, we really have $$ F(1/z)=\sum_{n=-\infty}^{-1}a_nz^{-n}+a_0. $$ So $a_n=0$ for $n>0$, and thus $F(z)=\sum_{n=-\infty}^{-1}a_nz^n+a_0$, so $\lim_{z\to\infty}F(z)=a_0<\infty$?",,['complex-analysis']
32,"Prove that  $2^{2z-1}\Gamma(z)\,\Gamma(z+\frac{1}{2})=\sqrt{\pi}\,\Gamma(2z)$ using Gauss's identity.",Prove that   using Gauss's identity.,"2^{2z-1}\Gamma(z)\,\Gamma(z+\frac{1}{2})=\sqrt{\pi}\,\Gamma(2z)","I'm trying to derive the functional equation $2^{2z-1}\Gamma(z)\,\Gamma(z+\frac{1}{2})=\sqrt{\pi}\,\Gamma(2z)$ using Gauss's formula: $$\Gamma(z)=\lim_{n\to\infty}\frac{n!\,\,n^z}{z(z+1)\cdots(z+n)}\,,$$ but it's tricky business. I've started by just looking at the product $$\frac{n!\,\,n^z}{z(z+1)\cdots(z+n)}\cdot\frac{n!\,\,n^{z+1/2}}{(z+1/2)(z+3/2)\cdots(z+(2n+1)/2)}$$ and the numerator very nicely becomes $(n!\, n^{2z})(n!\,n^{1/2})$, so we have the beginning stages of $\Gamma(2z)$ and $\Gamma(1/2)=\sqrt{\pi}$. But the denominator is killing me! Any advice? Also, where in the world does this $2^{2z-1}$ come from? Thanks!","I'm trying to derive the functional equation $2^{2z-1}\Gamma(z)\,\Gamma(z+\frac{1}{2})=\sqrt{\pi}\,\Gamma(2z)$ using Gauss's formula: $$\Gamma(z)=\lim_{n\to\infty}\frac{n!\,\,n^z}{z(z+1)\cdots(z+n)}\,,$$ but it's tricky business. I've started by just looking at the product $$\frac{n!\,\,n^z}{z(z+1)\cdots(z+n)}\cdot\frac{n!\,\,n^{z+1/2}}{(z+1/2)(z+3/2)\cdots(z+(2n+1)/2)}$$ and the numerator very nicely becomes $(n!\, n^{2z})(n!\,n^{1/2})$, so we have the beginning stages of $\Gamma(2z)$ and $\Gamma(1/2)=\sqrt{\pi}$. But the denominator is killing me! Any advice? Also, where in the world does this $2^{2z-1}$ come from? Thanks!",,"['complex-analysis', 'special-functions', 'gamma-function']"
33,Questions about Weierstrass's elliptic functions,Questions about Weierstrass's elliptic functions,,"From the wikipedia: In terms of the two periods, Weierstrass's elliptic function is an elliptic function with periods $\omega_1$ and $\omega_2$ defined as $$\wp(z;\omega_1,\omega_2)=\frac{1}{z^2}+ \sum_{n^2+m^2 \ne 0} \left\{ \frac{1}{(z+m\omega_1+n\omega_2)^2}- \frac{1}{\left(m\omega_1+n\omega_2\right)^2} \right\}. $$ Then $\Lambda=\{m\omega_1+n\omega_2:m,n\in\mathbb{Z}\}$ are the points of the period lattice, so that $$ \wp(z;\Lambda)=\wp(z;\omega_1,\omega_2) $$ so I really like these elliptic functions. Much easier to understand than the jacobi ones, which I don't really get. Seems like such a nifty idea too. I don't get why these weren't the first ones that people thought of. Well, maybe they were but no one cared for them, because they didn't have applications like the jacobi ones. And it was only when people saw they had theoretical significance, that then they talked about them more. Or when they had to start teaching it, since these are easier to understand. Well my first question is, why the exponent has to be $2$ and not $4$ or $6$ or any of the other even powers. Won't those powers generate convergent nonzero elliptic functions too? Then why do we only give attention to the ones, where the exponent is $2$ ? The wikipedia says any elliptic function can be written in terms of the corresponding Weierstrass one, that has the same periods. So maybe that's why. But I just find that shocking. So you're saying that we don't even need to give the terms in the Weierstrass series their own coefficient, that just polynomials (or other functions?) of the appropriate Weierstrass elliptical function will span the space of all elliptical functions with those periods? (even the ones that look like Weisterass series except that they have higher even-powered exponents?). I think someone has some explaining to do. Secondly, why does no one mention the one dimensional analogues of Weierstrass elliptical functions. Those would be functions of one real variable, that are periodic. Why did we never study those kinds of periodic functions. Are they just not relevant to the theory of periodic functions of one real variable? Where unlike in the complex case, you can't generate all other periodic functions from those ones? I could see how you could get $\tan(x)$ out of those, maybe, since it is lucky enough to have poles at each period, but not the ones that don't have any poles (like $\sin$ and $\cos$ ).","From the wikipedia: In terms of the two periods, Weierstrass's elliptic function is an elliptic function with periods and defined as Then are the points of the period lattice, so that so I really like these elliptic functions. Much easier to understand than the jacobi ones, which I don't really get. Seems like such a nifty idea too. I don't get why these weren't the first ones that people thought of. Well, maybe they were but no one cared for them, because they didn't have applications like the jacobi ones. And it was only when people saw they had theoretical significance, that then they talked about them more. Or when they had to start teaching it, since these are easier to understand. Well my first question is, why the exponent has to be and not or or any of the other even powers. Won't those powers generate convergent nonzero elliptic functions too? Then why do we only give attention to the ones, where the exponent is ? The wikipedia says any elliptic function can be written in terms of the corresponding Weierstrass one, that has the same periods. So maybe that's why. But I just find that shocking. So you're saying that we don't even need to give the terms in the Weierstrass series their own coefficient, that just polynomials (or other functions?) of the appropriate Weierstrass elliptical function will span the space of all elliptical functions with those periods? (even the ones that look like Weisterass series except that they have higher even-powered exponents?). I think someone has some explaining to do. Secondly, why does no one mention the one dimensional analogues of Weierstrass elliptical functions. Those would be functions of one real variable, that are periodic. Why did we never study those kinds of periodic functions. Are they just not relevant to the theory of periodic functions of one real variable? Where unlike in the complex case, you can't generate all other periodic functions from those ones? I could see how you could get out of those, maybe, since it is lucky enough to have poles at each period, but not the ones that don't have any poles (like and ).","\omega_1 \omega_2 \wp(z;\omega_1,\omega_2)=\frac{1}{z^2}+ \sum_{n^2+m^2 \ne 0} \left\{ \frac{1}{(z+m\omega_1+n\omega_2)^2}- \frac{1}{\left(m\omega_1+n\omega_2\right)^2} \right\}.  \Lambda=\{m\omega_1+n\omega_2:m,n\in\mathbb{Z}\}  \wp(z;\Lambda)=\wp(z;\omega_1,\omega_2)  2 4 6 2 \tan(x) \sin \cos","['complex-analysis', 'periodic-functions', 'elliptic-functions']"
34,Summation over Weierstrass $\wp$ functions,Summation over Weierstrass  functions,\wp,"I've been trying to prove the following closed expression for a summation over Weierstrass  $\wp$-functions: \begin{equation} \sum_{k=1}^{N-1} \wp_N(k) = \frac{2}{\omega}\left(\zeta\left(\frac{\omega}{2}\right)-N\zeta_N\left(\frac{\omega}{2}\right)\right), \end{equation} where $\wp_N$ is the usual WeierstrassP function with periods $(N,\omega)$, $\zeta_N$ is the Weierstrass zeta-function with periods $(N,\omega)$ and $\zeta$ is the Weierstrass zeta-function with periods $(1,\omega)$. Also, $N \in \mathbb{N}$, $N\geq 2$ and $\omega = i\pi/\kappa $ for some $\kappa\in \mathbb{R_{>0}}$. My usual approach to such a problem is to find a quasi-periodic function $F$, i.e. satisfying $$F(z+1) = \alpha F(z),\qquad F(z+\omega)=F(z), $$  where $\alpha \in \mathbb{C}$ and $|\alpha| >0$. This $F$ should then have the left hand side of the expression above in its Laurent series. Secondly, I postulate a different function which has the exact same pole structure as $F$. Due to the Liouville theorem for elliptic functions, one can then conclude that they must be equal (quasi-periodicity dictates that their difference is not just a constant, but must be zero). Equating the Laurent-coefficients then should yield the equation given above. In this case, one could use an appropriate modification of the function $F(z) = \sum_{k=0}^{N-1} \wp_N(z+k)$, which has as its zeroth order Laurent coefficient precisely $\sum_{k=1}^{N-1} \wp_N(k)$. This $F$ is doubly-periodic with periods $(1,\omega)$ and therefore does not satisfy quasi-periodicity yet. Its Laurent expansion equals $$  F(z) = \frac{1}{z^2} + \sum_{k=1}^{N-1} \wp_N(k)+ O(z) $$ and has therefore the exact same pole structure as the $\wp(z)$ (with periods $(1,\omega)$). Does this approach work at all and if yes, which function $F$ should one use? If not, how could one prove the statement above? In the mean time, I have found a function $F$ which might do the trick. Using the argument above, I proved that  $$ F(z)=\sum_{k=0}^{N-1} e^{ipk}\wp_N(z+k) $$ equals  $$ G(z) = -\frac{\sigma(z+r)}{\sigma(z-r)}e^{\frac{p}{\pi}\zeta(\omega/2)z}\left( \wp(z) -\wp(r) +\Delta(r)\left(\frac{\wp'(z)-\wp'(r)}{\wp(z)-\wp(r)} -\frac{\wp''(r)}{\wp'(r)}    \right)  \right) $$ where $r=-ip/(4\kappa)$, $\Delta(r) = \zeta(r) +\frac{p}{2\pi}\zeta(\omega/2)$ and $\wp$ and $\zeta$ are defined on the lattice $(1,\omega)$. Here $\sigma$ is the usual Weierstrass $\sigma$ function and is also defined on the lattice $(1,\omega)$. $p$ is a non-zero complex number. The expansion of $G$ around the point $z=0$ equals, writing $\delta=\frac{p}{\pi}\zeta\left(\frac{\omega}{2}\right)$ \begin{eqnarray}\label{laurent0} G(z) &=& \left( 1+2 \zeta(r)z+2\zeta^2(r) z^2 +O(z^3)\right)\left(1+\delta z +\frac{1}{2}(\delta z)^2\right) \nonumber \\ & &\times \left\{\frac{1}{z^2}-\wp(r) +\Delta\left( -\frac{2}{z} -\wp(r)z + \wp'(r) z^2 -\frac{\wp''(r)}{\wp'(r)}\right) \right\} \nonumber \\ &=& \frac{1}{z^2} + \frac{2\zeta(r) +\delta -2\Delta}{z} + \left(-\wp(r) -\Delta\frac{\wp''(r)}{\wp'(r)} +\left(\zeta(r) +\delta/2\right)\left(-4\Delta+2(\zeta(r) +\delta/2) \right) \right) +O(z). \nonumber \\ \end{eqnarray} One sees that the term going as $\frac{1}{z}$ vanishes due to our definition of $\Delta(r)$. By equating their respective Laurent series, I found $$ \sum_{k=1}^{N-1} e^{ipk}\wp_N(k) = -\wp(r) -\Delta(r) \frac{\wp''(r)}{\wp'(r)}-2\Delta(r)^2. $$ However, when I take the limit $p\rightarrow 0$ -- by expanding the right hand side in $p$ and taking the zeroth order term -- I find $$ \sum_{k=1}^{N-1}\wp_N(k) = \frac{2}{\omega}\zeta\left(\frac{\omega}{2}\right), $$ which is not exactly the answer I know to be true found on the top of this question. Also, numerical analysis of this expression shows conclusively that the equation above cannot be true, whereas the equation on the top of this page yields correct results every time.","I've been trying to prove the following closed expression for a summation over Weierstrass  $\wp$-functions: \begin{equation} \sum_{k=1}^{N-1} \wp_N(k) = \frac{2}{\omega}\left(\zeta\left(\frac{\omega}{2}\right)-N\zeta_N\left(\frac{\omega}{2}\right)\right), \end{equation} where $\wp_N$ is the usual WeierstrassP function with periods $(N,\omega)$, $\zeta_N$ is the Weierstrass zeta-function with periods $(N,\omega)$ and $\zeta$ is the Weierstrass zeta-function with periods $(1,\omega)$. Also, $N \in \mathbb{N}$, $N\geq 2$ and $\omega = i\pi/\kappa $ for some $\kappa\in \mathbb{R_{>0}}$. My usual approach to such a problem is to find a quasi-periodic function $F$, i.e. satisfying $$F(z+1) = \alpha F(z),\qquad F(z+\omega)=F(z), $$  where $\alpha \in \mathbb{C}$ and $|\alpha| >0$. This $F$ should then have the left hand side of the expression above in its Laurent series. Secondly, I postulate a different function which has the exact same pole structure as $F$. Due to the Liouville theorem for elliptic functions, one can then conclude that they must be equal (quasi-periodicity dictates that their difference is not just a constant, but must be zero). Equating the Laurent-coefficients then should yield the equation given above. In this case, one could use an appropriate modification of the function $F(z) = \sum_{k=0}^{N-1} \wp_N(z+k)$, which has as its zeroth order Laurent coefficient precisely $\sum_{k=1}^{N-1} \wp_N(k)$. This $F$ is doubly-periodic with periods $(1,\omega)$ and therefore does not satisfy quasi-periodicity yet. Its Laurent expansion equals $$  F(z) = \frac{1}{z^2} + \sum_{k=1}^{N-1} \wp_N(k)+ O(z) $$ and has therefore the exact same pole structure as the $\wp(z)$ (with periods $(1,\omega)$). Does this approach work at all and if yes, which function $F$ should one use? If not, how could one prove the statement above? In the mean time, I have found a function $F$ which might do the trick. Using the argument above, I proved that  $$ F(z)=\sum_{k=0}^{N-1} e^{ipk}\wp_N(z+k) $$ equals  $$ G(z) = -\frac{\sigma(z+r)}{\sigma(z-r)}e^{\frac{p}{\pi}\zeta(\omega/2)z}\left( \wp(z) -\wp(r) +\Delta(r)\left(\frac{\wp'(z)-\wp'(r)}{\wp(z)-\wp(r)} -\frac{\wp''(r)}{\wp'(r)}    \right)  \right) $$ where $r=-ip/(4\kappa)$, $\Delta(r) = \zeta(r) +\frac{p}{2\pi}\zeta(\omega/2)$ and $\wp$ and $\zeta$ are defined on the lattice $(1,\omega)$. Here $\sigma$ is the usual Weierstrass $\sigma$ function and is also defined on the lattice $(1,\omega)$. $p$ is a non-zero complex number. The expansion of $G$ around the point $z=0$ equals, writing $\delta=\frac{p}{\pi}\zeta\left(\frac{\omega}{2}\right)$ \begin{eqnarray}\label{laurent0} G(z) &=& \left( 1+2 \zeta(r)z+2\zeta^2(r) z^2 +O(z^3)\right)\left(1+\delta z +\frac{1}{2}(\delta z)^2\right) \nonumber \\ & &\times \left\{\frac{1}{z^2}-\wp(r) +\Delta\left( -\frac{2}{z} -\wp(r)z + \wp'(r) z^2 -\frac{\wp''(r)}{\wp'(r)}\right) \right\} \nonumber \\ &=& \frac{1}{z^2} + \frac{2\zeta(r) +\delta -2\Delta}{z} + \left(-\wp(r) -\Delta\frac{\wp''(r)}{\wp'(r)} +\left(\zeta(r) +\delta/2\right)\left(-4\Delta+2(\zeta(r) +\delta/2) \right) \right) +O(z). \nonumber \\ \end{eqnarray} One sees that the term going as $\frac{1}{z}$ vanishes due to our definition of $\Delta(r)$. By equating their respective Laurent series, I found $$ \sum_{k=1}^{N-1} e^{ipk}\wp_N(k) = -\wp(r) -\Delta(r) \frac{\wp''(r)}{\wp'(r)}-2\Delta(r)^2. $$ However, when I take the limit $p\rightarrow 0$ -- by expanding the right hand side in $p$ and taking the zeroth order term -- I find $$ \sum_{k=1}^{N-1}\wp_N(k) = \frac{2}{\omega}\zeta\left(\frac{\omega}{2}\right), $$ which is not exactly the answer I know to be true found on the top of this question. Also, numerical analysis of this expression shows conclusively that the equation above cannot be true, whereas the equation on the top of this page yields correct results every time.",,"['complex-analysis', 'summation', 'elliptic-functions']"
35,set of all holomorphic functions is a integral domain,set of all holomorphic functions is a integral domain,,"To show $H(G)=$ set of holomorphic functions on $G$ is a integral domain . i would like to know whether my proof of   $H(G)$ does not have divisors of zero correct or not? to show if $fg\equiv 0 \implies f\equiv0$ or $g \equiv 0$ let $fg=0$  $\forall z\in G$ and $ a \in G$ if $f(a) \neq 0  \implies f(z) \neq 0 $ $\forall z \in B(a,R)$ for some  $R$ this shows $g(z) = 0 $  $\forall z \in B(a,R) $ $g^n(a)=0$ $\forall n \implies g(z)=0 $ on $G$ .","To show $H(G)=$ set of holomorphic functions on $G$ is a integral domain . i would like to know whether my proof of   $H(G)$ does not have divisors of zero correct or not? to show if $fg\equiv 0 \implies f\equiv0$ or $g \equiv 0$ let $fg=0$  $\forall z\in G$ and $ a \in G$ if $f(a) \neq 0  \implies f(z) \neq 0 $ $\forall z \in B(a,R)$ for some  $R$ this shows $g(z) = 0 $  $\forall z \in B(a,R) $ $g^n(a)=0$ $\forall n \implies g(z)=0 $ on $G$ .",,['complex-analysis']
36,On radial limits of Blaschke Products,On radial limits of Blaschke Products,,"A Blaschke product is a function of the form $$B(z):=z^k\prod_{n=1}^{\infty}\frac{a_n-z}{1-\overline{a_n}z}\frac{|a_n|}{a_n}$$ where the $a_n$ are the non-zero zeros of $B$, and satisfie $\sum_{n=1}^{\infty}(1-|a_n|) < \infty$. Blashke products are holomorphic and bounded by 1 on the unit disk. A well known theorem asserts that $B$ has radial limits almost everywhere on the unit circle, i.e. that the limit $$\lim_{r \rightarrow 1} B(re^{i \theta})$$ exist for almost every $\theta$. I'm looking for an example of Blashke product such that the radial limit does not exist at a certain point, say $1$ for example. In particular, a Blaschke product with zeros in $(0,1)$ such that  $$\limsup_{r \rightarrow 1}|B(r)| =1$$ would work. Does anyone have a construction or reference? Thank you, Malik","A Blaschke product is a function of the form $$B(z):=z^k\prod_{n=1}^{\infty}\frac{a_n-z}{1-\overline{a_n}z}\frac{|a_n|}{a_n}$$ where the $a_n$ are the non-zero zeros of $B$, and satisfie $\sum_{n=1}^{\infty}(1-|a_n|) < \infty$. Blashke products are holomorphic and bounded by 1 on the unit disk. A well known theorem asserts that $B$ has radial limits almost everywhere on the unit circle, i.e. that the limit $$\lim_{r \rightarrow 1} B(re^{i \theta})$$ exist for almost every $\theta$. I'm looking for an example of Blashke product such that the radial limit does not exist at a certain point, say $1$ for example. In particular, a Blaschke product with zeros in $(0,1)$ such that  $$\limsup_{r \rightarrow 1}|B(r)| =1$$ would work. Does anyone have a construction or reference? Thank you, Malik",,['complex-analysis']
37,Why is it called a holomorphic function?,Why is it called a holomorphic function?,,"Why is it called a Holomorphic function? The ""Holo"" means ""entire"" and ""morphē"" means ""form"" or ""apparence"", cf wiki . I understand the ""entire"", because a holomorphic function is differentiable on the entire complex plane, but why ""form"", or ""apparence""?","Why is it called a Holomorphic function? The ""Holo"" means ""entire"" and ""morphē"" means ""form"" or ""apparence"", cf wiki . I understand the ""entire"", because a holomorphic function is differentiable on the entire complex plane, but why ""form"", or ""apparence""?",,"['complex-analysis', 'terminology', 'holomorphic-functions']"
38,Convergence of $\zeta(s)$ on $\Re(s)> 1$,Convergence of  on,\zeta(s) \Re(s)> 1,"I'm aware there are been several similar questions, and some great answers with hints on how to prove this, for example here and here . But I've never really seen a detailed proof of the absolute convergence of the Riemann zeta function in the half-plane $\Re(s)> 1$. I hope there's interest on a detailed proof of this for reference. Here is my attempt. Please, fill in any detail that might be missing, or point out any mistake! On $\Re(s)=\sigma> 1$, we have $$\sum_{n=1}^{\infty}\bigg|\frac{1}{n^s}\bigg|=\sum_{n=1}^{\infty}\frac{1}{n^\sigma}$$ and $$\frac{1}{n^\sigma}\leq\frac{1}{n^{1+\epsilon}}$$ for any $\epsilon >0$, so by the direct comparison test, if $$\sum_{n=1}^{\infty}\frac{1}{n^{1+\epsilon}}$$ converges absolutely, then so do $\sum_{n=1}^{\infty}|1/n^s|$ and $\zeta(s)$. By the integral test, the convergence of $\sum_{n=1}^{\infty}1/n^{1+\epsilon}$ is equivalente to the finitude of the integral $$\int_1^\infty \frac{1}{n^{1+\epsilon}}=\bigg[-\frac{1}{\epsilon x^\epsilon}\bigg]=\frac{1}{\epsilon}$$ which holds for any $\epsilon < \infty$. Any correction or improvement is welcomed! Thanks in advance.","I'm aware there are been several similar questions, and some great answers with hints on how to prove this, for example here and here . But I've never really seen a detailed proof of the absolute convergence of the Riemann zeta function in the half-plane $\Re(s)> 1$. I hope there's interest on a detailed proof of this for reference. Here is my attempt. Please, fill in any detail that might be missing, or point out any mistake! On $\Re(s)=\sigma> 1$, we have $$\sum_{n=1}^{\infty}\bigg|\frac{1}{n^s}\bigg|=\sum_{n=1}^{\infty}\frac{1}{n^\sigma}$$ and $$\frac{1}{n^\sigma}\leq\frac{1}{n^{1+\epsilon}}$$ for any $\epsilon >0$, so by the direct comparison test, if $$\sum_{n=1}^{\infty}\frac{1}{n^{1+\epsilon}}$$ converges absolutely, then so do $\sum_{n=1}^{\infty}|1/n^s|$ and $\zeta(s)$. By the integral test, the convergence of $\sum_{n=1}^{\infty}1/n^{1+\epsilon}$ is equivalente to the finitude of the integral $$\int_1^\infty \frac{1}{n^{1+\epsilon}}=\bigg[-\frac{1}{\epsilon x^\epsilon}\bigg]=\frac{1}{\epsilon}$$ which holds for any $\epsilon < \infty$. Any correction or improvement is welcomed! Thanks in advance.",,"['complex-analysis', 'number-theory', 'riemann-zeta']"
39,The Hairy ball theorem and Möbius transformations,The Hairy ball theorem and Möbius transformations,,"I  just came across a chapter in Needham's Visual complex analysis ; in particular, these diagrams: ( p. 153 - these happen to be on the cover as well) They represent families of Möbius transformations acting on the Riemann sphere . The pictures reminded me of the famous Hairy ball theorem : one could imaging the combing as being a combined effect of the above transformations. The more striking connection is that they completely agree with the theorem: any such transformation has at least one fixed point. Is there an underlying connection here, either with the theorem itself or perhaps a dumbed-down version of it? PS: I know little about the actual proof of the theorem $-$ please forgive me if the connection (or absence thereof) is obvious.","I  just came across a chapter in Needham's Visual complex analysis ; in particular, these diagrams: ( p. 153 - these happen to be on the cover as well) They represent families of Möbius transformations acting on the Riemann sphere . The pictures reminded me of the famous Hairy ball theorem : one could imaging the combing as being a combined effect of the above transformations. The more striking connection is that they completely agree with the theorem: any such transformation has at least one fixed point. Is there an underlying connection here, either with the theorem itself or perhaps a dumbed-down version of it? PS: I know little about the actual proof of the theorem $-$ please forgive me if the connection (or absence thereof) is obvious.",,"['complex-analysis', 'vector-spaces']"
40,"If a rational function is real on the unit circle, what does that say about its roots and poles?","If a rational function is real on the unit circle, what does that say about its roots and poles?",,"While doing a bit of self study, I ran across a situation whose wording confused me. Suppose $R(z)$ is some rational function which is real on the circle $|z|=1$ in the complex plane. The question asks, how are the zeros and poles situated? I don't quite understand this, what does it mean by how they're ""situated""? Is there some trick I'm supposed to use here? Does a nice scenario pop out, like they're reflections across the origin or the axes from each other or something similar? From the comments and help, I think I've made a little progress. If $R(c)=0$, then $\overline{R(1/\bar{c})}=0$, and thus $R(1/\bar{c})=0$ by conjugating again. So switching the roles shows $c$ is a root iff $1/\bar{c}$ is a root, and $c$ is a pole iff $1/\bar{c}$ is a pole? And I think the geometric description to this situation is that inversion in the unit circle preserves poles and roots. Also, the roots and poles come in pairs except when $c=1/\bar{c}$, that is, when $|c|=1$. So the only thing I can think of is that the roots come in pairs off the unit circle, and the poles come in pairs off the unit circle, plus some possible unpaired roots and poles on the unit circle. To those more experienced, does this seem like the intended answer to how the roots and poles are situated? Many thanks,","While doing a bit of self study, I ran across a situation whose wording confused me. Suppose $R(z)$ is some rational function which is real on the circle $|z|=1$ in the complex plane. The question asks, how are the zeros and poles situated? I don't quite understand this, what does it mean by how they're ""situated""? Is there some trick I'm supposed to use here? Does a nice scenario pop out, like they're reflections across the origin or the axes from each other or something similar? From the comments and help, I think I've made a little progress. If $R(c)=0$, then $\overline{R(1/\bar{c})}=0$, and thus $R(1/\bar{c})=0$ by conjugating again. So switching the roles shows $c$ is a root iff $1/\bar{c}$ is a root, and $c$ is a pole iff $1/\bar{c}$ is a pole? And I think the geometric description to this situation is that inversion in the unit circle preserves poles and roots. Also, the roots and poles come in pairs except when $c=1/\bar{c}$, that is, when $|c|=1$. So the only thing I can think of is that the roots come in pairs off the unit circle, and the poles come in pairs off the unit circle, plus some possible unpaired roots and poles on the unit circle. To those more experienced, does this seem like the intended answer to how the roots and poles are situated? Many thanks,",,"['complex-analysis', 'roots', 'rational-functions']"
41,Constructing one-forms on a Riemann surface using the uniformization theorem,Constructing one-forms on a Riemann surface using the uniformization theorem,,"I found this statement about proving the existence of one-forms on a Riemann surface in an answer on MathOverflow : This deep fact is essentially the same as the uniformization theorem.   The problem is how to construct at least one holomorphic or   meromorphic form with prescribed singularity. All known proofs use   some Analysis, and none of them is simple. Once you have   Uniformization, it is easy to construct holomorphic forms. Why is this easy? Uniformization tells us that any Riemann surface has a universal cover that is $\mathbb D$, $\mathbb C$, or $\mathbb P^1$. It is easy to transfer forms from the base space to the covering space by pulling back. But how do we use the covering map to construct forms with prescribed singularities on the base space?","I found this statement about proving the existence of one-forms on a Riemann surface in an answer on MathOverflow : This deep fact is essentially the same as the uniformization theorem.   The problem is how to construct at least one holomorphic or   meromorphic form with prescribed singularity. All known proofs use   some Analysis, and none of them is simple. Once you have   Uniformization, it is easy to construct holomorphic forms. Why is this easy? Uniformization tells us that any Riemann surface has a universal cover that is $\mathbb D$, $\mathbb C$, or $\mathbb P^1$. It is easy to transfer forms from the base space to the covering space by pulling back. But how do we use the covering map to construct forms with prescribed singularities on the base space?",,"['complex-analysis', 'riemann-surfaces']"
42,Calculating the abscissa of convergence for general Dirichlet Series,Calculating the abscissa of convergence for general Dirichlet Series,,"I'm currently interested in proving this theorem which I have been thinking for quite a while: Define a Dirichlet Series $$\sum_{k=1}^{\infty}a_k e^{-\lambda_k z}$$ where $\lambda_k$ is a strictly increasing sequence and $z \in \mathbb{C}$. I'm interested in proving that if the abscissa of convergence $\sigma_c <0$, then  $$\sigma_c = \eta := \limsup_{n \to \infty} \frac{\log |R_n|}{\lambda_n}$$ where $R_n= \sum_{k=n+1}^{\infty}a_k$. Anyone has any idea on how to start? I'm pretty much stuck after trying to use Abel's transformation like in the proof for when $\sum a_k$ is divergent.","I'm currently interested in proving this theorem which I have been thinking for quite a while: Define a Dirichlet Series $$\sum_{k=1}^{\infty}a_k e^{-\lambda_k z}$$ where $\lambda_k$ is a strictly increasing sequence and $z \in \mathbb{C}$. I'm interested in proving that if the abscissa of convergence $\sigma_c <0$, then  $$\sigma_c = \eta := \limsup_{n \to \infty} \frac{\log |R_n|}{\lambda_n}$$ where $R_n= \sum_{k=n+1}^{\infty}a_k$. Anyone has any idea on how to start? I'm pretty much stuck after trying to use Abel's transformation like in the proof for when $\sum a_k$ is divergent.",,"['complex-analysis', 'dirichlet-series']"
43,"$f: \Omega \rightarrow \Omega$ holomorphic, $f(0) = 0$, $f'(0) = 1$ implies $f(z) = z$","holomorphic, ,  implies",f: \Omega \rightarrow \Omega f(0) = 0 f'(0) = 1 f(z) = z,"Let $\Omega$ be a bounded connected open subset of $\mathbb{C}$ containing $0$. Let $f: \Omega \rightarrow \Omega$ be holomorphic and $f(0) = 0$, $f'(0) = 1$. The problem I am working on is to show that $f(z) = z$. If $\Omega = \mathbb{D}$, then this follows from the Schwarz Lemma. I also know of a solution ( posted here ) which involves looking at the power series coefficients of $f^n := f\circ f \circ f \circ \cdots \circ f$ ($n$ times) and using the Cauchy estimates, but is there a different way of doing this problem that doesn't involve taking the $k$th derivative of $f^n$?","Let $\Omega$ be a bounded connected open subset of $\mathbb{C}$ containing $0$. Let $f: \Omega \rightarrow \Omega$ be holomorphic and $f(0) = 0$, $f'(0) = 1$. The problem I am working on is to show that $f(z) = z$. If $\Omega = \mathbb{D}$, then this follows from the Schwarz Lemma. I also know of a solution ( posted here ) which involves looking at the power series coefficients of $f^n := f\circ f \circ f \circ \cdots \circ f$ ($n$ times) and using the Cauchy estimates, but is there a different way of doing this problem that doesn't involve taking the $k$th derivative of $f^n$?",,['complex-analysis']
44,Continuity of the derivative,Continuity of the derivative,,"As we all know, all the basic properties of holomorphic functions (i.e. functions which are differentiable in the complex sense) can be deduced from Cauchy's formula. Moreover, Cauchy's formula itself can be viewed as a rather simple consequence of the Green-Riemann formula, provided that the holomorphic function you have at hand is assumed to have a continuous derivative. Of course, Cauchy's formula holds without assuming continuity of the derivative, and it yields continuity of the derivative and much more since it implies power series expansion. But Cauchy's formula (or, if you prefer, Cauchy's theorem) without assuming continuity of the derivative is a rather subtle thing, and this gives a rather ""indirect"" proof of the fact that holomorphic functions are in fact $\mathcal C^1$. So my question is the following: does anybody know a direct proof of the fact that if a function $f$ defined on an open subset of $\mathbb C$ is differentiable in the complex sense, then its derivative $f'$ is continuous? I'm pretty sure I am not the first one and will not be the last one to ask this question, at least for himself (or herself). So please feel free to close it if it has indeed been asked previously on this site. Edit. Perhaps I should say a few more words about what I mean by a ""direct proof"". Anything that relies in one way or another to Cauchy's formula or Cauchy's theorem is not considered as a direct argument. A direct proof should somehow establish ""from scratch"", or ""from very basic principles"" that holomorphic implies $\mathcal C^1$.","As we all know, all the basic properties of holomorphic functions (i.e. functions which are differentiable in the complex sense) can be deduced from Cauchy's formula. Moreover, Cauchy's formula itself can be viewed as a rather simple consequence of the Green-Riemann formula, provided that the holomorphic function you have at hand is assumed to have a continuous derivative. Of course, Cauchy's formula holds without assuming continuity of the derivative, and it yields continuity of the derivative and much more since it implies power series expansion. But Cauchy's formula (or, if you prefer, Cauchy's theorem) without assuming continuity of the derivative is a rather subtle thing, and this gives a rather ""indirect"" proof of the fact that holomorphic functions are in fact $\mathcal C^1$. So my question is the following: does anybody know a direct proof of the fact that if a function $f$ defined on an open subset of $\mathbb C$ is differentiable in the complex sense, then its derivative $f'$ is continuous? I'm pretty sure I am not the first one and will not be the last one to ask this question, at least for himself (or herself). So please feel free to close it if it has indeed been asked previously on this site. Edit. Perhaps I should say a few more words about what I mean by a ""direct proof"". Anything that relies in one way or another to Cauchy's formula or Cauchy's theorem is not considered as a direct argument. A direct proof should somehow establish ""from scratch"", or ""from very basic principles"" that holomorphic implies $\mathcal C^1$.",,['complex-analysis']
45,Proving $\int_0^r{(r^m-x^m)^{1/m}dx}=\frac{\Gamma\left(\frac{1}{m}+1\right)\Gamma\left(\frac{1}{m}+1\right)}{\Gamma\left(\frac{2}{m}+1\right)}r^2$,Proving,\int_0^r{(r^m-x^m)^{1/m}dx}=\frac{\Gamma\left(\frac{1}{m}+1\right)\Gamma\left(\frac{1}{m}+1\right)}{\Gamma\left(\frac{2}{m}+1\right)}r^2,"First let's put the question succinctly. How can I go about showing the following? $$\int_0^r{(r^m-x^m)^{1/m}dx}=\frac{\Gamma\left(\frac{1}{m}+1\right)\Gamma\left(\frac{1}{m}+1\right)}{\Gamma\left(\frac{2}{m}+1\right)}r^2$$ Now for some exposition:  I am a math enthusiast and this result kind of fell into my lap after playing around a bit with ""circles""... This is my first encounter with the $\Gamma$ function. I am not quite sure how one goes about establishing such a claim. At this point I am at the ""I better look into this $\Gamma$ function"" part of my research but I figured I would document the question and take any input offered. Consider the equation $|x|^m+|y|^m=1$, for $m \in {1,2,3}$ The $m=1$ case then corresponds to the square in the picture which has side lengths $\sqrt{2}$. The whole square has area $2$ and therefore the area of the square limited to the first quadrant is $1/2$. $$\int_0^1{(1-x)dx}=\frac{\Gamma(2)\Gamma(2)}{\Gamma(3)}=\frac{(2-1)!(2-1)!}{(3-1)!}=\frac{1}{2}$$ I only invoke the idea that over the whole numbers $\Gamma(n+1)=n!$ here because I found the formula by examining this in the case when my inputs for $\Gamma$ were whole numbers. Then I replaced my factorial symbols with $\Gamma$s to get the claim above which I have only verified empirically. For the $m=2$ case. We have the unit circle. The area in the first quadrant should be $\pi/4$. And indeed: $$\int_0^1{(1-x^2)^{1/2}dx} =\frac{\Gamma\left(\frac{3}{2}\right)\Gamma\left(\frac{3}{2}\right)}{\Gamma(2)} =\frac{ \sqrt{\pi}}{2}\frac{ \sqrt{ \pi} }{2}=\dfrac\pi4$$ Cool! So now I was excited to see that this worked not only in the cases with whole number inputs to $\Gamma$. $m=3$  Well then what's the area under the curve $|x|^3+|y|^3=1$? This corresponds to the outermost curve in the diagram. Well... I assume this value must be some transcendental number. It's construction is similar to the way we think about $\pi$. But what is it? $$\begin{align*}\int_0^1{(1-x^3)^{1/3}dx}&=\frac{\Gamma(\frac{1}{3}+1)\Gamma(\frac{1}{3}+1)}{\Gamma(\frac{2}{3}+1)}\\ &\approx 0.883319375142724978656844749824219351285934269101278765063\end{align*}$$ Which matches up with numerical integration. Wolfram alpha can present this number in a few other ways. For example, $$\frac{\Gamma(1/3)^3}{4\sqrt{3}{\pi}}$$ These other representations all seem to invoke the Gamma function.","First let's put the question succinctly. How can I go about showing the following? $$\int_0^r{(r^m-x^m)^{1/m}dx}=\frac{\Gamma\left(\frac{1}{m}+1\right)\Gamma\left(\frac{1}{m}+1\right)}{\Gamma\left(\frac{2}{m}+1\right)}r^2$$ Now for some exposition:  I am a math enthusiast and this result kind of fell into my lap after playing around a bit with ""circles""... This is my first encounter with the $\Gamma$ function. I am not quite sure how one goes about establishing such a claim. At this point I am at the ""I better look into this $\Gamma$ function"" part of my research but I figured I would document the question and take any input offered. Consider the equation $|x|^m+|y|^m=1$, for $m \in {1,2,3}$ The $m=1$ case then corresponds to the square in the picture which has side lengths $\sqrt{2}$. The whole square has area $2$ and therefore the area of the square limited to the first quadrant is $1/2$. $$\int_0^1{(1-x)dx}=\frac{\Gamma(2)\Gamma(2)}{\Gamma(3)}=\frac{(2-1)!(2-1)!}{(3-1)!}=\frac{1}{2}$$ I only invoke the idea that over the whole numbers $\Gamma(n+1)=n!$ here because I found the formula by examining this in the case when my inputs for $\Gamma$ were whole numbers. Then I replaced my factorial symbols with $\Gamma$s to get the claim above which I have only verified empirically. For the $m=2$ case. We have the unit circle. The area in the first quadrant should be $\pi/4$. And indeed: $$\int_0^1{(1-x^2)^{1/2}dx} =\frac{\Gamma\left(\frac{3}{2}\right)\Gamma\left(\frac{3}{2}\right)}{\Gamma(2)} =\frac{ \sqrt{\pi}}{2}\frac{ \sqrt{ \pi} }{2}=\dfrac\pi4$$ Cool! So now I was excited to see that this worked not only in the cases with whole number inputs to $\Gamma$. $m=3$  Well then what's the area under the curve $|x|^3+|y|^3=1$? This corresponds to the outermost curve in the diagram. Well... I assume this value must be some transcendental number. It's construction is similar to the way we think about $\pi$. But what is it? $$\begin{align*}\int_0^1{(1-x^3)^{1/3}dx}&=\frac{\Gamma(\frac{1}{3}+1)\Gamma(\frac{1}{3}+1)}{\Gamma(\frac{2}{3}+1)}\\ &\approx 0.883319375142724978656844749824219351285934269101278765063\end{align*}$$ Which matches up with numerical integration. Wolfram alpha can present this number in a few other ways. For example, $$\frac{\Gamma(1/3)^3}{4\sqrt{3}{\pi}}$$ These other representations all seem to invoke the Gamma function.",,"['complex-analysis', 'geometry', 'algebraic-geometry']"
46,Showing that $|f^{(n)}| \le n!n^n$ and then making this result sharper,Showing that  and then making this result sharper,|f^{(n)}| \le n!n^n,"Ahlfors: Show that the successive derivatives of an analytic function at a   point can never satisfy $|f^{(n)}(z)| > n!n^n$.  Formulate a sharper   theorem of the same kind. Attempt for Part One: Let $\Delta$ be a neighborhood around $z$ of radius $r$ small enough so that $f$ is analytic on $\Delta$.  Let $C$ be a circle around $z$ of radius $r$. Cauchy's integral formula then yields that $$ f^{(n)}(z) = {n! \over 2 \pi i}\int_{C} {f(\zeta) \over (\zeta - z)^{n+1}}\ d\zeta $$ Since $\mathbb{R}$ is complete, we have that $M = \max\{|f(\zeta)| : |\zeta - z| \le r \} \in \mathbb{R}$ exists.  Furthermore, we have that $|\zeta - z| \le r$ for all $\zeta$ within the perimeter of $C$.  Hence we have $$ |f^{(n)}(z)| \le \left|{n! \over 2 \pi i}\right|\int_{C} {|M| \over |r^{n+1}|}\ |d\zeta| \le {n! \over 2 \pi} {M 2 \pi r \over r^{n+1}} = {n!M \over r^{n}} $$ Hence we have Cauchy's estimate: $$ |f^{(n)}(z)| \le {n! M \over r^n} $$ We may further assume that $M > 1$ above (it doesn't affect any of the the inequality reasoning). Consider that there is a point where $n$ is large enough s.t. $n^n\ge {M \over r^n}$ (indeed, it is when $n \ge {M \over r}$).  At such a point, we have that: $$ n^n \ge {M \over r^n} \implies n^n r^n \ge M \implies r^n \ge {M \over n^n} \implies {1 \over r^n} \le {n^n \over M} $$ Then assuming $n^n \ge {M \over r^n}$, we have that $$ \underbrace{|f^{(n)}(z)| \le {n!M \over r^n}}_{\text{Cauchy's estimate}} = {n!M} \cdot \left({1 \over r^n}\right) \le \underbrace{{n!M}\cdot \left({n^n \over M}\right)}_{\text{since }{1 \over r^n} \le {n^n \over M}} = n! n^n $$ as desired. We have thus far shown that if $n \ge {M \over r}$, then $$ |f^{(n)}(z)| \le n!n^n $$ Question: In what way could we use this result to make a sharper theorem of the same kind?","Ahlfors: Show that the successive derivatives of an analytic function at a   point can never satisfy $|f^{(n)}(z)| > n!n^n$.  Formulate a sharper   theorem of the same kind. Attempt for Part One: Let $\Delta$ be a neighborhood around $z$ of radius $r$ small enough so that $f$ is analytic on $\Delta$.  Let $C$ be a circle around $z$ of radius $r$. Cauchy's integral formula then yields that $$ f^{(n)}(z) = {n! \over 2 \pi i}\int_{C} {f(\zeta) \over (\zeta - z)^{n+1}}\ d\zeta $$ Since $\mathbb{R}$ is complete, we have that $M = \max\{|f(\zeta)| : |\zeta - z| \le r \} \in \mathbb{R}$ exists.  Furthermore, we have that $|\zeta - z| \le r$ for all $\zeta$ within the perimeter of $C$.  Hence we have $$ |f^{(n)}(z)| \le \left|{n! \over 2 \pi i}\right|\int_{C} {|M| \over |r^{n+1}|}\ |d\zeta| \le {n! \over 2 \pi} {M 2 \pi r \over r^{n+1}} = {n!M \over r^{n}} $$ Hence we have Cauchy's estimate: $$ |f^{(n)}(z)| \le {n! M \over r^n} $$ We may further assume that $M > 1$ above (it doesn't affect any of the the inequality reasoning). Consider that there is a point where $n$ is large enough s.t. $n^n\ge {M \over r^n}$ (indeed, it is when $n \ge {M \over r}$).  At such a point, we have that: $$ n^n \ge {M \over r^n} \implies n^n r^n \ge M \implies r^n \ge {M \over n^n} \implies {1 \over r^n} \le {n^n \over M} $$ Then assuming $n^n \ge {M \over r^n}$, we have that $$ \underbrace{|f^{(n)}(z)| \le {n!M \over r^n}}_{\text{Cauchy's estimate}} = {n!M} \cdot \left({1 \over r^n}\right) \le \underbrace{{n!M}\cdot \left({n^n \over M}\right)}_{\text{since }{1 \over r^n} \le {n^n \over M}} = n! n^n $$ as desired. We have thus far shown that if $n \ge {M \over r}$, then $$ |f^{(n)}(z)| \le n!n^n $$ Question: In what way could we use this result to make a sharper theorem of the same kind?",,"['complex-analysis', 'analysis', 'inequality', 'derivatives', 'analyticity']"
47,"How much can we ""cheat"" and use vector knowledge in complex analysis?","How much can we ""cheat"" and use vector knowledge in complex analysis?",,"I'm an engineering-physics student taking a course in complex analysis, and it's a little frustrating, because I see all these connections to vector calculus over the reals (especially as applied to electromagnetics). For example, if we treat complex numbers like vectors, then the Cauchy-Riemann equations tell us that the derivative exists if the divergence and curl are both zero, which is equivalent to saying that our field is conservative, and there's no field sources/sinks. That these functions also satisfy Laplace's equation is also fairly obvious, since Laplace's equation describes a conservative field with no field sources. Essentially, to what extent can we pretend the complex numbers are vectors, and bring over useful results from vector calculus? If this doesn't work, then what are the major flaws, or things to watch out for? Edit: The reason I'm looking for analogs between vector analysis and complex analysis is because results in vector analysis usually have clear physical interpretations. I'm hoping to enhance my understanding of complex analysis through the similarities. Additionally, if I know a proof for certain proprerties of vector functions, I probably shouldn't focus my attention on the proof of an equivalent property in complex analysis. Instead, I'd prefer to focus attention on results that don't have a correspondence to vector analysis. So I'm wondering, what are some results in complex analysis that don't have a similar result in vector analysis? Or even for results that are similar, what are the special peculiarities to complex analysis that make the results different?","I'm an engineering-physics student taking a course in complex analysis, and it's a little frustrating, because I see all these connections to vector calculus over the reals (especially as applied to electromagnetics). For example, if we treat complex numbers like vectors, then the Cauchy-Riemann equations tell us that the derivative exists if the divergence and curl are both zero, which is equivalent to saying that our field is conservative, and there's no field sources/sinks. That these functions also satisfy Laplace's equation is also fairly obvious, since Laplace's equation describes a conservative field with no field sources. Essentially, to what extent can we pretend the complex numbers are vectors, and bring over useful results from vector calculus? If this doesn't work, then what are the major flaws, or things to watch out for? Edit: The reason I'm looking for analogs between vector analysis and complex analysis is because results in vector analysis usually have clear physical interpretations. I'm hoping to enhance my understanding of complex analysis through the similarities. Additionally, if I know a proof for certain proprerties of vector functions, I probably shouldn't focus my attention on the proof of an equivalent property in complex analysis. Instead, I'd prefer to focus attention on results that don't have a correspondence to vector analysis. So I'm wondering, what are some results in complex analysis that don't have a similar result in vector analysis? Or even for results that are similar, what are the special peculiarities to complex analysis that make the results different?",,"['complex-analysis', 'multivariable-calculus', 'vector-spaces', 'physics']"
48,Existence of an entire function with algebraically independent derivatives,Existence of an entire function with algebraically independent derivatives,,"Let $\mathbb{A}$ be the algebraic closure of $\mathbb{Q}$ in $\mathbb{C}$. A collection of functions $F=\lbrace f_i:X \rightarrow\mathbb{C}\rbrace$ is said to be algebraically independent over $\mathbb{Q}$ at $x \in X$ if the $f_i(x)$ are distinct, at most one of the $f_i(x)$ is algebraic and $ \lbrace f_i(x) \rbrace - \mathbb{A} $ is algebraically independent over $\mathbb{Q}$. (The reason why I allow one $f_i(x)$ to be algebraic will become clear shortly.) I would like to know whether there exists a 2-sided sequence of entire functions: $\ldots, f^{(-2)},f^{(-1)},f^{(0)},f^{(1)},f^{(2)}, \ldots$ such that for every $n \in \mathbb{Z}$, $f^{(n)}(z)=\frac{d}{dz}f^{(n-1)}(z)$, and the collection $\{f^{(i)} \: | \: i \in \mathbb{Z}\}$ is algebraically independent over $\mathbb{Q}$ everywhere except possibly on a ""small"" set S. Ideally, I'd like S to be empty, or at least closed and discrete, but pretty much any smallness condition you like would help me at least get some intuition for how to deal with such a problem. Note that unless S is dense, there is an open set $U$ in its complement. By the open mapping theorem, each $f^{(i)}(U)$ is open and hence contains algebraic numbers. So allowing one $f^{(i)}$ to be algebraic at each point is necessary. My thoughts (feel free to ignore them if you have your own approach): It's pretty easy with the Lindemann–Weierstrass theorem to construct $f^{(0)}$ such that the derivatives of $f^{(0)}$ at 0 are algebraically independent, and choosing integration constants appropriately for negative $n$ gives a sequence which works at 0. The problem is that evaluating the function anywhere other than 0 requires summing an infinite series, something which behaves poorly with respect to transcendence. It is likely not plausible to prove that any specific function works, since transcendence theory doesn't have many general results. On the other hand, I suspect that if such a function exists, they will be generic, or at least common. There may be an easy way to approach the problem nonconstructively with functional analysis. I couldn't think of anything in this direction, though. On the other hand, if such a function does not exist, there should be a good reason why not. I suspect it would be demonstrable with just a few derivatives based mostly on topology. By the open mapping theorem, on each open set $\mathbb{A}(f^{(i)}(z))$ takes every value possible for each $i$. This seems like a strong condition, but it also implies that one can't topologize the field extensions of $\mathbb{A}$ in any nice way, which poses a bit of an issue if one wants to continue on this route.","Let $\mathbb{A}$ be the algebraic closure of $\mathbb{Q}$ in $\mathbb{C}$. A collection of functions $F=\lbrace f_i:X \rightarrow\mathbb{C}\rbrace$ is said to be algebraically independent over $\mathbb{Q}$ at $x \in X$ if the $f_i(x)$ are distinct, at most one of the $f_i(x)$ is algebraic and $ \lbrace f_i(x) \rbrace - \mathbb{A} $ is algebraically independent over $\mathbb{Q}$. (The reason why I allow one $f_i(x)$ to be algebraic will become clear shortly.) I would like to know whether there exists a 2-sided sequence of entire functions: $\ldots, f^{(-2)},f^{(-1)},f^{(0)},f^{(1)},f^{(2)}, \ldots$ such that for every $n \in \mathbb{Z}$, $f^{(n)}(z)=\frac{d}{dz}f^{(n-1)}(z)$, and the collection $\{f^{(i)} \: | \: i \in \mathbb{Z}\}$ is algebraically independent over $\mathbb{Q}$ everywhere except possibly on a ""small"" set S. Ideally, I'd like S to be empty, or at least closed and discrete, but pretty much any smallness condition you like would help me at least get some intuition for how to deal with such a problem. Note that unless S is dense, there is an open set $U$ in its complement. By the open mapping theorem, each $f^{(i)}(U)$ is open and hence contains algebraic numbers. So allowing one $f^{(i)}$ to be algebraic at each point is necessary. My thoughts (feel free to ignore them if you have your own approach): It's pretty easy with the Lindemann–Weierstrass theorem to construct $f^{(0)}$ such that the derivatives of $f^{(0)}$ at 0 are algebraically independent, and choosing integration constants appropriately for negative $n$ gives a sequence which works at 0. The problem is that evaluating the function anywhere other than 0 requires summing an infinite series, something which behaves poorly with respect to transcendence. It is likely not plausible to prove that any specific function works, since transcendence theory doesn't have many general results. On the other hand, I suspect that if such a function exists, they will be generic, or at least common. There may be an easy way to approach the problem nonconstructively with functional analysis. I couldn't think of anything in this direction, though. On the other hand, if such a function does not exist, there should be a good reason why not. I suspect it would be demonstrable with just a few derivatives based mostly on topology. By the open mapping theorem, on each open set $\mathbb{A}(f^{(i)}(z))$ takes every value possible for each $i$. This seems like a strong condition, but it also implies that one can't topologize the field extensions of $\mathbb{A}$ in any nice way, which poses a bit of an issue if one wants to continue on this route.",,"['complex-analysis', 'transcendence-theory']"
49,"If $P$ is a polynomial with $|P(1)| = \max\limits_{|z| =1} |P(z)|$, then its root on the unit circle is separated away from 0","If  is a polynomial with , then its root on the unit circle is separated away from 0",P |P(1)| = \max\limits_{|z| =1} |P(z)|,"Let $P(z)$ be a nonzero polynomial of degree $n$ such that $$|P(1)| = \max\limits_{|z| =1} |P(z)|.$$ Furthermore let $z_0 = e^{i\varphi_0}$, $\varphi_0 \in [-\pi,\pi]$ be a root of $P$ on the unit circle. I want to prove that $|\varphi_0| \geq \pi /n$. Also, by my intuition, if this becomes an equality, $P(z)$ would be a multiple of $1+z^n$. Does anyone has an idea?","Let $P(z)$ be a nonzero polynomial of degree $n$ such that $$|P(1)| = \max\limits_{|z| =1} |P(z)|.$$ Furthermore let $z_0 = e^{i\varphi_0}$, $\varphi_0 \in [-\pi,\pi]$ be a root of $P$ on the unit circle. I want to prove that $|\varphi_0| \geq \pi /n$. Also, by my intuition, if this becomes an equality, $P(z)$ would be a multiple of $1+z^n$. Does anyone has an idea?",,"['complex-analysis', 'polynomials']"
50,What is a branch point?,What is a branch point?,,"I am really struggling with the concept of a ""branch point"". I understand that, for example, if we take the $\log$ function, by going around $2\pi$ we arrive at a different value, so therefore it is a multivalued function. However, surely this argument holds for all points in the complex plane, so I don't really understand how $z=0$ is the ONLY branch point. Additionally, the course I am revising for needs no Riemann surfaces or knowledge of that area of mathematics, just what a branch point is and how to find it. Thanks for any help.","I am really struggling with the concept of a ""branch point"". I understand that, for example, if we take the $\log$ function, by going around $2\pi$ we arrive at a different value, so therefore it is a multivalued function. However, surely this argument holds for all points in the complex plane, so I don't really understand how $z=0$ is the ONLY branch point. Additionally, the course I am revising for needs no Riemann surfaces or knowledge of that area of mathematics, just what a branch point is and how to find it. Thanks for any help.",,"['complex-analysis', 'branch-cuts', 'branch-points', 'multivalued-functions']"
51,A periodic entire function which must have a fixed point,A periodic entire function which must have a fixed point,,"I would like to check my work on the following problem: Suppose $f(z)$ is a non-constant periodic entire function satisfying $f(z+1)=f(z)$. Show that $f(z)$ has a fixed point. So my attempt is: Suppose $f(z)$ does not have a fixed point. Then $g(z)=f(z)-z$ is entire and never $0$. We compute $g(z+1)=f(z+1)-(z+1)=f(z)-z-1=g(z)-1$. But, since $g(z+1)\neq 0$ then $g(z)\neq 1$ for all $z$, hence $g(z)$ is a nonconstant entire function omitting $2$ values, contradicting Picard's theorem. Looks good?","I would like to check my work on the following problem: Suppose $f(z)$ is a non-constant periodic entire function satisfying $f(z+1)=f(z)$. Show that $f(z)$ has a fixed point. So my attempt is: Suppose $f(z)$ does not have a fixed point. Then $g(z)=f(z)-z$ is entire and never $0$. We compute $g(z+1)=f(z+1)-(z+1)=f(z)-z-1=g(z)-1$. But, since $g(z+1)\neq 0$ then $g(z)\neq 1$ for all $z$, hence $g(z)$ is a nonconstant entire function omitting $2$ values, contradicting Picard's theorem. Looks good?",,"['complex-analysis', 'complex-numbers']"
52,The Riemann zeta function $\zeta(s)$ has no zeros for $\Re(s)>1$ [duplicate],The Riemann zeta function  has no zeros for  [duplicate],\zeta(s) \Re(s)>1,"This question already has an answer here : Have all the zeros of the Riemann Zeta function real part smaller than 1? (1 answer) Closed 4 years ago . I write $\zeta(s)$ for $\Re(s)>1$ as: $\zeta(s) = \prod_{p} (1-p^{-s})^{-1}$ Using this I can show that the Riemann zeta function has no zero for $\Re(s)>1$. I'm however not sure about the next step. I would like to use the zero product property, but I know it doesn't necessary hold for infinite products. How do I handle this situation?","This question already has an answer here : Have all the zeros of the Riemann Zeta function real part smaller than 1? (1 answer) Closed 4 years ago . I write $\zeta(s)$ for $\Re(s)>1$ as: $\zeta(s) = \prod_{p} (1-p^{-s})^{-1}$ Using this I can show that the Riemann zeta function has no zero for $\Re(s)>1$. I'm however not sure about the next step. I would like to use the zero product property, but I know it doesn't necessary hold for infinite products. How do I handle this situation?",,"['complex-analysis', 'riemann-zeta']"
53,Showing that the map $f(z) = \frac{1}{z} $ maps circles into circles or lines,Showing that the map  maps circles into circles or lines,f(z) = \frac{1}{z} ,"Let $f: \mathbb{C} \setminus \{0 \} \to \mathbb{C} \setminus \{0\} $. We want to show that $f(z) = \frac{1}{z}$ maps circles into circles and lines. My professor gave the following hint: The general equation for lines and circles is $$ \alpha(x^2 + y^2) + \beta x + \gamma y + \Delta = 0 $$ where the greek letters are obviously constants. So, given this advice, We can rewrite this in the complex plane as follows: $$ \alpha |z|^2 + \frac{ \beta}{2}( z + \overline{z} ) + \frac{\gamma}{2i}( z - \overline{z} ) + \Delta = 0$$ So, now we apply $w = \frac{1}{z} $ and we obtain (with $|w|^2 = w \overline{w}$): $$ \frac{ \alpha}{w \overline{w}} + \frac{\beta}{2}\bigg( \frac{1}{w} + \frac{1}{\overline{w}}\bigg) + \frac{\gamma}{2 i}\bigg( \frac{1}{w}- \frac{1}{\overline{w}}\bigg)+ \Delta = 0$$ hence, $$ \alpha + \frac{ \beta}{2}(\overline{w} + w ) + \frac{\gamma}{2i}(\overline{w}-w) + w \overline{w} \Delta = 0 $$ Next, putting $w = u + iv$ we arrive to: $$ \alpha + \beta u - \gamma v + (u^2 + v^2 ) \Delta = 0 $$ So, in the case when we have circles in $xy$-plane, that is when $\alpha \neq 0$, we still have circles in the $uv$-plane. So $f$ sends circles to circles if $\alpha \neq 0 $. We also have circle if $\alpha \neq 0 $ and $\Delta = 0$ which in this case $\frac{1}{z} $ sends circles to lines. Is this a correct solution? IS there a shorter way to prove this?","Let $f: \mathbb{C} \setminus \{0 \} \to \mathbb{C} \setminus \{0\} $. We want to show that $f(z) = \frac{1}{z}$ maps circles into circles and lines. My professor gave the following hint: The general equation for lines and circles is $$ \alpha(x^2 + y^2) + \beta x + \gamma y + \Delta = 0 $$ where the greek letters are obviously constants. So, given this advice, We can rewrite this in the complex plane as follows: $$ \alpha |z|^2 + \frac{ \beta}{2}( z + \overline{z} ) + \frac{\gamma}{2i}( z - \overline{z} ) + \Delta = 0$$ So, now we apply $w = \frac{1}{z} $ and we obtain (with $|w|^2 = w \overline{w}$): $$ \frac{ \alpha}{w \overline{w}} + \frac{\beta}{2}\bigg( \frac{1}{w} + \frac{1}{\overline{w}}\bigg) + \frac{\gamma}{2 i}\bigg( \frac{1}{w}- \frac{1}{\overline{w}}\bigg)+ \Delta = 0$$ hence, $$ \alpha + \frac{ \beta}{2}(\overline{w} + w ) + \frac{\gamma}{2i}(\overline{w}-w) + w \overline{w} \Delta = 0 $$ Next, putting $w = u + iv$ we arrive to: $$ \alpha + \beta u - \gamma v + (u^2 + v^2 ) \Delta = 0 $$ So, in the case when we have circles in $xy$-plane, that is when $\alpha \neq 0$, we still have circles in the $uv$-plane. So $f$ sends circles to circles if $\alpha \neq 0 $. We also have circle if $\alpha \neq 0 $ and $\Delta = 0$ which in this case $\frac{1}{z} $ sends circles to lines. Is this a correct solution? IS there a shorter way to prove this?",,[]
54,Roots of partial sum of power series,Roots of partial sum of power series,,"Consider the power series $$ \sqrt{1+z} = \sum_{k=0}^\infty \left( \begin{array}{c} \frac{1}{2} \\ k \end{array} \right) z^k. $$ I am interested in characterizing the roots of the partial sum $$ s_n(z) = \sum_{k=0}^n \left( \begin{array}{c} \frac{1}{2} \\ k \end{array} \right) z^k. $$ The $n$ roots all seem to lie outside the unit circle and asymptotically approach the unit circle (and the roots seem to have equally spaced arguments). For instance, for $n=50$ , the roots (red dots) look as follows in the complex plane (solid black circle denotes the unit circle): Is there any way of getting a handle on these roots, either for a fixed (large) $n$ or asymptotically $n \to \infty$ ? More generally, I am struggling to find useful references for the more general question of understanding the roots of partial sums. Perhaps there is not that much one can say about them in general. (One nice general result I found in the literature constrains where the roots can appear as a function of the radius of convergence; however, I could not find much more.) EDIT: by calculating derivatives, it is straightforward to prove that the roots of $s_n(z)$ can be written as $z_i = - \frac{1}{\lambda_i}$ with $$ \sum_{i=1}^n \lambda_i^k = \frac{1}{2} \textrm{ for all } k =1,2,\cdots,n. $$ In other words, characterizing the roots is equivalent to solving $$ \left\{ \begin{array}{ccc} \lambda_1 + \lambda_2 + \cdots + \lambda_n & = & \frac{1}{2} \\ \lambda_1^2 + \lambda_2^2 + \cdots + \lambda_n^2 & = & \frac{1}{2} \\ & \vdots & \\ \lambda_1^n + \lambda_2^n + \cdots + \lambda_n^n & = & \frac{1}{2}.  \end{array} \right. $$","Consider the power series I am interested in characterizing the roots of the partial sum The roots all seem to lie outside the unit circle and asymptotically approach the unit circle (and the roots seem to have equally spaced arguments). For instance, for , the roots (red dots) look as follows in the complex plane (solid black circle denotes the unit circle): Is there any way of getting a handle on these roots, either for a fixed (large) or asymptotically ? More generally, I am struggling to find useful references for the more general question of understanding the roots of partial sums. Perhaps there is not that much one can say about them in general. (One nice general result I found in the literature constrains where the roots can appear as a function of the radius of convergence; however, I could not find much more.) EDIT: by calculating derivatives, it is straightforward to prove that the roots of can be written as with In other words, characterizing the roots is equivalent to solving"," \sqrt{1+z} = \sum_{k=0}^\infty \left( \begin{array}{c} \frac{1}{2} \\ k \end{array} \right) z^k.   s_n(z) = \sum_{k=0}^n \left( \begin{array}{c} \frac{1}{2} \\ k \end{array} \right) z^k.  n n=50 n n \to \infty s_n(z) z_i = - \frac{1}{\lambda_i}  \sum_{i=1}^n \lambda_i^k = \frac{1}{2} \textrm{ for all } k =1,2,\cdots,n.   \left\{ \begin{array}{ccc}
\lambda_1 + \lambda_2 + \cdots + \lambda_n & = & \frac{1}{2} \\
\lambda_1^2 + \lambda_2^2 + \cdots + \lambda_n^2 & = & \frac{1}{2} \\
& \vdots & \\
\lambda_1^n + \lambda_2^n + \cdots + \lambda_n^n & = & \frac{1}{2}.
 \end{array} \right. ","['complex-analysis', 'asymptotics', 'power-series', 'roots']"
55,Fourier transform of the critical line of zeta?,Fourier transform of the critical line of zeta?,,"Is there a known expression for the (distributional) Fourier transform of the Riemann zeta function, taken along the critical line? I'd love to say that it's a weighted sum  of delta distributions, logarithmically spaced and decreasing in amplitude, as in $\sum_n \frac{\delta(\omega-\log(n))}{n^{1/2}}$ but this fails to be a tempered distribution, and fails in general when the exponent in the denominator is less than 1.","Is there a known expression for the (distributional) Fourier transform of the Riemann zeta function, taken along the critical line? I'd love to say that it's a weighted sum  of delta distributions, logarithmically spaced and decreasing in amplitude, as in $\sum_n \frac{\delta(\omega-\log(n))}{n^{1/2}}$ but this fails to be a tempered distribution, and fails in general when the exponent in the denominator is less than 1.",,"['complex-analysis', 'number-theory', 'fourier-analysis', 'analytic-number-theory']"
56,"There exist $x_{1},x_{2},\cdots,x_{k}$ such two inequality $|x^b_{1}+x^b_{2}+\cdots+x^b_{k}|\ge 1$",There exist  such two inequality,"x_{1},x_{2},\cdots,x_{k} |x^b_{1}+x^b_{2}+\cdots+x^b_{k}|\ge 1","This problem is a 2014 Sydney mathematics competition problem (11 grade). It seems difficult to solve. (I previously posted the n=2 case for which André Nicolas and Dan Robertson proposed solutions) Let $A\subseteq X=\{1,2,3,\cdots,n\}$, $B=X\setminus A$. Show that: There exist  complex numbers $x_{1},x_{2},\cdots,x_{k}(k\ge 2)$ such that:   $$\begin{cases} \forall  a\in A,|x^a_{1}+x^a_{2}+\cdots+x^a_{m}|\le\frac{1}{a} \text{ for all } m \le k\\ \forall  b\in B,|x^b_{1}+x^b_{2}+\cdots+x^b_{k}|\ge 1. \end{cases}$$","This problem is a 2014 Sydney mathematics competition problem (11 grade). It seems difficult to solve. (I previously posted the n=2 case for which André Nicolas and Dan Robertson proposed solutions) Let $A\subseteq X=\{1,2,3,\cdots,n\}$, $B=X\setminus A$. Show that: There exist  complex numbers $x_{1},x_{2},\cdots,x_{k}(k\ge 2)$ such that:   $$\begin{cases} \forall  a\in A,|x^a_{1}+x^a_{2}+\cdots+x^a_{m}|\le\frac{1}{a} \text{ for all } m \le k\\ \forall  b\in B,|x^b_{1}+x^b_{2}+\cdots+x^b_{k}|\ge 1. \end{cases}$$",,['complex-analysis']
57,Why are analytic functions functions of $z$ and not of $\bar{z}$?,Why are analytic functions functions of  and not of ?,z \bar{z},"I was reading a note on complex analysis and was stuck on  one line: Cauchy-Riemann equations Alternatively, the C-R equations can be written as   $$\frac{\partial f}{\partial\bar z}=0,$$   where $z=x+iy$ and $\bar z=x-iy$. In some sense, analytic functions are truly functions of $z$, and not of $\bar z$.   $$\begin{array}{c} \frac\partial{\partial z} = \frac 1 2 \left(\frac\partial{\partial x}-i\frac\partial{\partial y}\right) &\frac\partial{\partial \bar z}=\frac 1 2 \left(\frac\partial{\partial x}+i\frac\partial{\partial y}\right) \end{array}$$ I do not understand what it means that ""Analytic functions are truly functions of $z$ and not of $\bar{z}$."" Can someone explain? Thanks in advance for your time.","I was reading a note on complex analysis and was stuck on  one line: Cauchy-Riemann equations Alternatively, the C-R equations can be written as   $$\frac{\partial f}{\partial\bar z}=0,$$   where $z=x+iy$ and $\bar z=x-iy$. In some sense, analytic functions are truly functions of $z$, and not of $\bar z$.   $$\begin{array}{c} \frac\partial{\partial z} = \frac 1 2 \left(\frac\partial{\partial x}-i\frac\partial{\partial y}\right) &\frac\partial{\partial \bar z}=\frac 1 2 \left(\frac\partial{\partial x}+i\frac\partial{\partial y}\right) \end{array}$$ I do not understand what it means that ""Analytic functions are truly functions of $z$ and not of $\bar{z}$."" Can someone explain? Thanks in advance for your time.",,['complex-analysis']
58,A polynomial is completely determined by any part of it,A polynomial is completely determined by any part of it,,"I was watching this Mathologer video ( https://youtu.be/YuIIjLr6vUA?t=1652 ) and he says at 27:32 First, suppose that our initial chunk is part of a parabola, or if you like a cubic, or any polynomial. If I then tell you that my mystery function is a polynomial, there's always going to be exactly one polynomial that continues our initial chunk . In other words, a polynomial is completely determined by any part of it. [...] Again, just relax if all this seems a little bit too much. So he didn't give a proof of the theorem in bold text – I think this is very important. I understand that there always exists a polynomial of degree $n$ that passes through a set of $n+1$ points (i.e. there are finitely many custom points to be passed by, the chunk has to be discrete, like $(1,1),(2,2),(3,3),(4,5)$ ). But there also exists some polynomial of degree $m$ ( $m\ne n$ ) that passes through the same set of points. But how do I prove that there exists one and only one polynomial that passes through a set of infinitely many points?","I was watching this Mathologer video ( https://youtu.be/YuIIjLr6vUA?t=1652 ) and he says at 27:32 First, suppose that our initial chunk is part of a parabola, or if you like a cubic, or any polynomial. If I then tell you that my mystery function is a polynomial, there's always going to be exactly one polynomial that continues our initial chunk . In other words, a polynomial is completely determined by any part of it. [...] Again, just relax if all this seems a little bit too much. So he didn't give a proof of the theorem in bold text – I think this is very important. I understand that there always exists a polynomial of degree that passes through a set of points (i.e. there are finitely many custom points to be passed by, the chunk has to be discrete, like ). But there also exists some polynomial of degree ( ) that passes through the same set of points. But how do I prove that there exists one and only one polynomial that passes through a set of infinitely many points?","n n+1 (1,1),(2,2),(3,3),(4,5) m m\ne n","['complex-analysis', 'polynomials', 'interpolation']"
59,How to evaluate this integral $\int_{-\infty}^{+\infty}\frac{x^2e^x}{(1+e^x)^2}dx$?,How to evaluate this integral ?,\int_{-\infty}^{+\infty}\frac{x^2e^x}{(1+e^x)^2}dx,"I need to evaluate $$\int_{-\infty}^{+\infty}\frac{x^2e^x}{(1+e^x)^2}dx$$ I think the answer is $\frac{\pi^2}{3}$, but I'm not able to calculate it.","I need to evaluate $$\int_{-\infty}^{+\infty}\frac{x^2e^x}{(1+e^x)^2}dx$$ I think the answer is $\frac{\pi^2}{3}$, but I'm not able to calculate it.",,"['complex-analysis', 'improper-integrals']"
60,What might I use to show that an entire function with positive real parts is constant?,What might I use to show that an entire function with positive real parts is constant?,,"So the question asks me to prove that an entire function with positive real parts is constant, and I was thinking that this might somehow be related to showing an entire bounded function is constant (Liouville's theorem), but are there any other theorems that might help me prove this fact?","So the question asks me to prove that an entire function with positive real parts is constant, and I was thinking that this might somehow be related to showing an entire bounded function is constant (Liouville's theorem), but are there any other theorems that might help me prove this fact?",,['complex-analysis']
61,The existence of analytical branch of the logarithm of a holomorphic function,The existence of analytical branch of the logarithm of a holomorphic function,,"$\Omega$ is a convex open set in $\mathbb {C}^n$ and $f$ is an analytical function Edit: without zero point on $\Omega$ , then can we define an analytical branch of $\ln {f}$ on $\Omega$ ?","is a convex open set in and is an analytical function Edit: without zero point on , then can we define an analytical branch of on ?",\Omega \mathbb {C}^n f \Omega \ln {f} \Omega,['complex-analysis']
62,When does $az + b\bar{z} + c = 0$ represent a line?,When does  represent a line?,az + b\bar{z} + c = 0,"$a,b,c$ and $z$ are all complex numbers. My idea was to show that it passes through the point $\infty$ in the extended complex plane, but I'm not quite sure how to execute that. Update: It says in the text that a straight line can be represented by a parametric equation $z = a+bt$, where $a$ and $b$ are complex numbers, and $b\neq 0$, $t\in\mathbb{R}$","$a,b,c$ and $z$ are all complex numbers. My idea was to show that it passes through the point $\infty$ in the extended complex plane, but I'm not quite sure how to execute that. Update: It says in the text that a straight line can be represented by a parametric equation $z = a+bt$, where $a$ and $b$ are complex numbers, and $b\neq 0$, $t\in\mathbb{R}$",,"['complex-analysis', 'complex-numbers']"
63,Define a Principal Branch in Complex Analysis,Define a Principal Branch in Complex Analysis,,"The actual problem is to define a principal branch for $a^z=e^{zlog(a)}$ and to give a general formula. From what I understand about principal branches, it is already in that form? I'm missing something key about that. But I also am having a hard time with the general formula. The transition from real numbers to complex is confusing me, since ""log"" is just defined as the inverse of $e^x$, and $e^x$ is defined by the power series. I'm just not sure where to begin.","The actual problem is to define a principal branch for $a^z=e^{zlog(a)}$ and to give a general formula. From what I understand about principal branches, it is already in that form? I'm missing something key about that. But I also am having a hard time with the general formula. The transition from real numbers to complex is confusing me, since ""log"" is just defined as the inverse of $e^x$, and $e^x$ is defined by the power series. I'm just not sure where to begin.",,['complex-analysis']
64,Proving $|f(z)|$ is constant on the boundary of a domain implies $f$ is a constant function,Proving  is constant on the boundary of a domain implies  is a constant function,|f(z)| f,"Let $D \subset \mathbb{C}$ be a bounded domain and $f$ a function holomorphic in $D$ and continuous in its closure. Suppose that $|f(z)|$ is constant on the boundary of $D$ and that $f$ does not have zeroes in $D$. Prove that $f$ is a constant function. I think that if I can prove that $f$ attains both its maximum and minimum values on the boundary, then the result follows from the maximum principle. But I've been unable to show this. Is this the right way to approach this problem? If so, how do I show this result? Thanks in advance!","Let $D \subset \mathbb{C}$ be a bounded domain and $f$ a function holomorphic in $D$ and continuous in its closure. Suppose that $|f(z)|$ is constant on the boundary of $D$ and that $f$ does not have zeroes in $D$. Prove that $f$ is a constant function. I think that if I can prove that $f$ attains both its maximum and minimum values on the boundary, then the result follows from the maximum principle. But I've been unable to show this. Is this the right way to approach this problem? If so, how do I show this result? Thanks in advance!",,['complex-analysis']
65,Isolated zeros on closure of a domain,Isolated zeros on closure of a domain,,Let $f$ be an analytic function on the open unit disk domain $D$.  Suppose also that $f$ is bounded. Since $f$ is bounded I believe that $f$ can be continuously extended to the closed unit disk. I know that the zeros of $f$ in the open disk $D$ are isolated.   Are the zeros of $f$ in the closed unit disk also necessarily isolated?,Let $f$ be an analytic function on the open unit disk domain $D$.  Suppose also that $f$ is bounded. Since $f$ is bounded I believe that $f$ can be continuously extended to the closed unit disk. I know that the zeros of $f$ in the open disk $D$ are isolated.   Are the zeros of $f$ in the closed unit disk also necessarily isolated?,,['complex-analysis']
66,Maximum of sum of finite modulus of analytic function.,Maximum of sum of finite modulus of analytic function.,,"Let $f_1,f_2,\ldots,f_n $ be  analytic complex functions in domain $D$. and $f = \sum_{k=1}^n|f_k|$ is not constant. Can I show the  maximum of $f$ only appears on boundary of $D\,$?","Let $f_1,f_2,\ldots,f_n $ be  analytic complex functions in domain $D$. and $f = \sum_{k=1}^n|f_k|$ is not constant. Can I show the  maximum of $f$ only appears on boundary of $D\,$?",,['complex-analysis']
67,Finding the Laurent series of $f(z)=1/((z-1)(z-2))$,Finding the Laurent series of,f(z)=1/((z-1)(z-2)),"Let  $$f(z)=\frac{1}{(z-1)(z-2)}$$  and let $$R_1=\Bigl\{z\Bigm| 1<|z|<2\Bigr\}\quad\text{ and }\quad R_2=\Bigl\{z\Bigm| |z|>2\Bigr\}.$$ How do you find the Laurent series convergent on $R_1$? Also how do you do it for $R_2$? I'm having serious trouble with this as I can't see how to expand things into series with n as any integer, not just natural number. Also how to apply Cauchy's integral formula to an annulus. If anyone can explain this to me I will be extremely grateful.","Let  $$f(z)=\frac{1}{(z-1)(z-2)}$$  and let $$R_1=\Bigl\{z\Bigm| 1<|z|<2\Bigr\}\quad\text{ and }\quad R_2=\Bigl\{z\Bigm| |z|>2\Bigr\}.$$ How do you find the Laurent series convergent on $R_1$? Also how do you do it for $R_2$? I'm having serious trouble with this as I can't see how to expand things into series with n as any integer, not just natural number. Also how to apply Cauchy's integral formula to an annulus. If anyone can explain this to me I will be extremely grateful.",,['complex-analysis']
68,"Is entire function constant when $ |f(z)|\le \log|z|,\ |z|>1$.",Is entire function constant when .," |f(z)|\le \log|z|,\ |z|>1","Let $ f : \mathbb{C} \to \mathbb{C} ,$ entire and $|f(z)|\le \log|z|,\ |z|>1. $ Show that $f$ is constant. What first comes to mind is Louville's theorem, but log 's problems with analyticity confuse me.","Let $ f : \mathbb{C} \to \mathbb{C} ,$ entire and $|f(z)|\le \log|z|,\ |z|>1. $ Show that $f$ is constant. What first comes to mind is Louville's theorem, but log 's problems with analyticity confuse me.",,['complex-analysis']
69,Calculating residue of pole of order $2$,Calculating residue of pole of order,2,"Is there a good way to compute the residue of $f(z)=\dfrac{1+z}{1-\sin z}$ at $z=\pi/2$, which is a pole of order $2$? Using the residue calculation formula yields $$\text{Res}_{z=\pi/2}f(z)=\lim_{z\rightarrow\pi/2}\dfrac{d}{dz}\left(\left(z-\dfrac\pi2\right)^2f(z)\right)$$ The derivative is quite ugly, and calculating the limit requires L'Hospital probably twice (or more). The calculation is just too much. Is there a better way?","Is there a good way to compute the residue of $f(z)=\dfrac{1+z}{1-\sin z}$ at $z=\pi/2$, which is a pole of order $2$? Using the residue calculation formula yields $$\text{Res}_{z=\pi/2}f(z)=\lim_{z\rightarrow\pi/2}\dfrac{d}{dz}\left(\left(z-\dfrac\pi2\right)^2f(z)\right)$$ The derivative is quite ugly, and calculating the limit requires L'Hospital probably twice (or more). The calculation is just too much. Is there a better way?",,['complex-analysis']
70,How to Compute $\zeta (0)$?,How to Compute ?,\zeta (0),"Ultimately, I am interested in analytically continuing the function $$ \eta _a(s):=\sum _{n=1}^\infty \frac{1}{(n^2+a^2)^s}, $$ where $a$ is a non-negative real number, and calculating $\eta _a$ and its derivatives (at least the first derivative) at the origin:  $\eta _a(0),\eta _a'(0),\ldots $. It is well-known that $\zeta (0)=-\tfrac{1}{2}$ and that $\zeta '(0)=-\tfrac{1}{2}\ln (2\pi)$, but I do not actually know how to obtain these ($\zeta$ is of course the Riemann Zeta function ).  I figured that, perhaps if I knew how to calculate these values, I would be able to generalize the technique to be able to calculate the corresponding values of $\eta _a$. So then, how does one calculate $\zeta (0)$, $\zeta '(0)$, etc.?  If this technique does not obviously generalize to $\eta _a$, any ideas how I might go about calculating these values?","Ultimately, I am interested in analytically continuing the function $$ \eta _a(s):=\sum _{n=1}^\infty \frac{1}{(n^2+a^2)^s}, $$ where $a$ is a non-negative real number, and calculating $\eta _a$ and its derivatives (at least the first derivative) at the origin:  $\eta _a(0),\eta _a'(0),\ldots $. It is well-known that $\zeta (0)=-\tfrac{1}{2}$ and that $\zeta '(0)=-\tfrac{1}{2}\ln (2\pi)$, but I do not actually know how to obtain these ($\zeta$ is of course the Riemann Zeta function ).  I figured that, perhaps if I knew how to calculate these values, I would be able to generalize the technique to be able to calculate the corresponding values of $\eta _a$. So then, how does one calculate $\zeta (0)$, $\zeta '(0)$, etc.?  If this technique does not obviously generalize to $\eta _a$, any ideas how I might go about calculating these values?",,"['complex-analysis', 'riemann-zeta']"
71,Visualising functions from complex numbers to complex numbers,Visualising functions from complex numbers to complex numbers,,I think that complex analysis is hard because graphs of even basic functions are 4 dimensional. Does anyone have any good visual representations of basic complex functions or know of any tools for generating them?,I think that complex analysis is hard because graphs of even basic functions are 4 dimensional. Does anyone have any good visual representations of basic complex functions or know of any tools for generating them?,,"['big-list', 'math-software', 'complex-analysis']"
72,"How much algebraic geometry is there in complex geometry (for example, Demailly)?","How much algebraic geometry is there in complex geometry (for example, Demailly)?",,"I wonder how much of modern algebraic geometry (schemes, etc) is there in complex (algebro-analytic) geometry. What I mean is complex algebraic (analytic) geometry in the sense of Demailly, Lasarsfeld etc. I wonder if scheme theory (as in Grothendieck's work) is actually useful there.","I wonder how much of modern algebraic geometry (schemes, etc) is there in complex (algebro-analytic) geometry. What I mean is complex algebraic (analytic) geometry in the sense of Demailly, Lasarsfeld etc. I wonder if scheme theory (as in Grothendieck's work) is actually useful there.",,"['complex-analysis', 'algebraic-geometry', 'differential-geometry', 'complex-geometry', 'schemes']"
73,1 to the power i [duplicate],1 to the power i [duplicate],,"This question already has answers here : Closed 11 years ago . Possible Duplicate: What is the value of $1^i$? I was thinking, what would 1^i be? Then I did: $e^{i\pi}=-1\rightarrow e^{i\pi}\cdot e^{i\pi}=e^{2i\pi}=-1\cdot -1=1$ Now raise to the power i: $1^i=(e^{2i\pi})^i=e^{2i^2\pi}=e^{-2\pi}=\frac{1}{e^{2\pi}}$ Is this correct?","This question already has answers here : Closed 11 years ago . Possible Duplicate: What is the value of $1^i$? I was thinking, what would 1^i be? Then I did: $e^{i\pi}=-1\rightarrow e^{i\pi}\cdot e^{i\pi}=e^{2i\pi}=-1\cdot -1=1$ Now raise to the power i: $1^i=(e^{2i\pi})^i=e^{2i^2\pi}=e^{-2\pi}=\frac{1}{e^{2\pi}}$ Is this correct?",,['complex-analysis']
74,Is $ \ln |f| $ harmonic?,Is  harmonic?, \ln |f| ,"I'd like to show that $\ln |f| $ is harmonic, where $f$ is holomorphic defined on a domain of the complex plane and never takes the value 0. My idea was to use the fact that $\ln |f(z)| = \operatorname{Log} f(z) - i*\operatorname{Arg}(f(z)) $, but $Log$ is only holomorphic on some part of the complex plane and $\operatorname{Arg}$ is not holomorphic at all. Any help is welcome!","I'd like to show that $\ln |f| $ is harmonic, where $f$ is holomorphic defined on a domain of the complex plane and never takes the value 0. My idea was to use the fact that $\ln |f(z)| = \operatorname{Log} f(z) - i*\operatorname{Arg}(f(z)) $, but $Log$ is only holomorphic on some part of the complex plane and $\operatorname{Arg}$ is not holomorphic at all. Any help is welcome!",,['complex-analysis']
75,Evaluate the integral $\int_0^\infty \frac{x (\ln(x))^2}{x^4 + x^2 + 1}\text{ d}x$,Evaluate the integral,\int_0^\infty \frac{x (\ln(x))^2}{x^4 + x^2 + 1}\text{ d}x,"What is the value of $\displaystyle\int_0^\infty \frac{x (\ln(x))^2}{x^4 + x^2 + 1}\text{ d}x$ ? This is a question I came up with myself. It is not homework. I constructed this example to make the following technique work: Integrate $\frac{z (\log(z))^3}{z^4 + z^2 + 1}$ along a ""key-hole"" contour. The argument can be made rigorous by splitting the contour into two parts, and using two different branch cuts for each part. Warning: This method is time-consuming and not for the faint-hearted","What is the value of ? This is a question I came up with myself. It is not homework. I constructed this example to make the following technique work: Integrate along a ""key-hole"" contour. The argument can be made rigorous by splitting the contour into two parts, and using two different branch cuts for each part. Warning: This method is time-consuming and not for the faint-hearted",\displaystyle\int_0^\infty \frac{x (\ln(x))^2}{x^4 + x^2 + 1}\text{ d}x \frac{z (\log(z))^3}{z^4 + z^2 + 1},"['complex-analysis', 'definite-integrals', 'contour-integration']"
76,Is there a holomorphic and bijective function of the unit disc onto the complex plane? [duplicate],Is there a holomorphic and bijective function of the unit disc onto the complex plane? [duplicate],,This question already has answers here : A holomorphic bijection from the open unit disc to the complex plane (2 answers) Closed 5 years ago . Is there a holomorphic and bijective function between the open unit ball of $\mathbb{C}$ and $\mathbb{C}$ ? The usual homeomorphisms $\psi(z):=\frac{z}{1+|z|}$ and his composition with the conjugate map are not holomorphic on $\mathbb{C}$ .,This question already has answers here : A holomorphic bijection from the open unit disc to the complex plane (2 answers) Closed 5 years ago . Is there a holomorphic and bijective function between the open unit ball of and ? The usual homeomorphisms and his composition with the conjugate map are not holomorphic on .,\mathbb{C} \mathbb{C} \psi(z):=\frac{z}{1+|z|} \mathbb{C},"['complex-analysis', 'holomorphic-functions']"
77,Branch of $\sqrt{1-z^2}$,Branch of,\sqrt{1-z^2},"Show that a branch of $\sqrt{1-z^2}$ can be defined in any region $\Omega$ where the points $1,-1$ are in the same component of its complement. This is a question in Ahlfors' Complex Analysis (P.148 Q5) that I came across while trying to self-study the book. I tried to tackle the problem by considering $\Omega=\mathbb{C} \backslash [-1,1]$ first, and tried the approach as in Section 4.4 Corollary 2, namely find a branch of the corresponding log first. For this $\Omega$, the image of $1-z^2$ is $\mathbb{C} \backslash [0,1]$, so a branch of $log(1-z^2)$ cannot be defined; evidently one needs to construct the branch of $\sqrt{1-z^2}$ directly. Here is where I ran out of ideas... Any help is appreciated!","Show that a branch of $\sqrt{1-z^2}$ can be defined in any region $\Omega$ where the points $1,-1$ are in the same component of its complement. This is a question in Ahlfors' Complex Analysis (P.148 Q5) that I came across while trying to self-study the book. I tried to tackle the problem by considering $\Omega=\mathbb{C} \backslash [-1,1]$ first, and tried the approach as in Section 4.4 Corollary 2, namely find a branch of the corresponding log first. For this $\Omega$, the image of $1-z^2$ is $\mathbb{C} \backslash [0,1]$, so a branch of $log(1-z^2)$ cannot be defined; evidently one needs to construct the branch of $\sqrt{1-z^2}$ directly. Here is where I ran out of ideas... Any help is appreciated!",,['complex-analysis']
78,Which book on complex analysis is good for self study?,Which book on complex analysis is good for self study?,,Which book on complex analysis is good for self study? I am an average student and have just a very basic knowledge of this subject. I want to cover up to Runge's Theorem. I heard about few books- Gamelin's Complex Analysis; a text by Churchill and another Ahlfors' Complex analysis. Thank you.,Which book on complex analysis is good for self study? I am an average student and have just a very basic knowledge of this subject. I want to cover up to Runge's Theorem. I heard about few books- Gamelin's Complex Analysis; a text by Churchill and another Ahlfors' Complex analysis. Thank you.,,"['complex-analysis', 'reference-request', 'complex-numbers', 'book-recommendation', 'several-complex-variables']"
79,Why is $\left(e^{2\pi i}\right)^i \neq e^{-2 \pi}$?,Why is ?,\left(e^{2\pi i}\right)^i \neq e^{-2 \pi},"Here's my (obviously flawed) proof that $1=e^{-2 \pi}$: $$ 1^i=1\\ e^{2 \pi i} = 1\\ \left(e^{2\pi i}\right)^i = 1^i\\ e^{-2 \pi} = 1 $$ What's the issue? I understand that exponentiation is not injective (and thus $-1 \neq 1$ even though $(-1)^2 = 1^2$), but I don't think that's an issue here: I'm only raising things to the power of $i$, which I don't think is multi-valued.","Here's my (obviously flawed) proof that $1=e^{-2 \pi}$: $$ 1^i=1\\ e^{2 \pi i} = 1\\ \left(e^{2\pi i}\right)^i = 1^i\\ e^{-2 \pi} = 1 $$ What's the issue? I understand that exponentiation is not injective (and thus $-1 \neq 1$ even though $(-1)^2 = 1^2$), but I don't think that's an issue here: I'm only raising things to the power of $i$, which I don't think is multi-valued.",,"['complex-analysis', 'complex-numbers', 'exponentiation']"
80,Where is the fallacy in the argument using Prime Number Theorem,Where is the fallacy in the argument using Prime Number Theorem,,"I am reading about Prime Number Theorem from book by Ingham. As as application of PNT I found the following theorem: Now my doubt is at the step $\frac{\log(y)}{\log(x)}\rightarrow 1$, we can say $\log(y)\rightarrow\log(x)$ and if I apply antilog I get $y\rightarrow x$ which is not true by PNT. So where is the fallacy in my argument? I think it is because we are neglecting $\log\log x$ term, but I am not sure and I am unable to find a suitable argument. Also we are applying $\log$ on PNT even though it is kind of limit, So why cant we apply antilog. Are there any conditions to apply some function for these kind of cases?","I am reading about Prime Number Theorem from book by Ingham. As as application of PNT I found the following theorem: Now my doubt is at the step $\frac{\log(y)}{\log(x)}\rightarrow 1$, we can say $\log(y)\rightarrow\log(x)$ and if I apply antilog I get $y\rightarrow x$ which is not true by PNT. So where is the fallacy in my argument? I think it is because we are neglecting $\log\log x$ term, but I am not sure and I am unable to find a suitable argument. Also we are applying $\log$ on PNT even though it is kind of limit, So why cant we apply antilog. Are there any conditions to apply some function for these kind of cases?",,"['complex-analysis', 'number-theory', 'prime-numbers', 'analytic-number-theory']"
81,Prove or Disprove that $\left|\frac{e^{2i\theta} -2e^{i\theta} - 1}{e^{2i\theta} + 2e^{i\theta} -1}\right| = 1$,Prove or Disprove that,\left|\frac{e^{2i\theta} -2e^{i\theta} - 1}{e^{2i\theta} + 2e^{i\theta} -1}\right| = 1,"Prove or disprove that $$\left|\frac{e^{2i\theta} -2e^{i\theta} - 1}{e^{2i\theta} + 2e^{i\theta} -1}\right| = 1$$ This is a step in an attempt to solve a much larger problem, thus I'm fairly sure it's true but not absolutely sure.  It looks like it should be simple but it's resisted all my attempts so far.","Prove or disprove that $$\left|\frac{e^{2i\theta} -2e^{i\theta} - 1}{e^{2i\theta} + 2e^{i\theta} -1}\right| = 1$$ This is a step in an attempt to solve a much larger problem, thus I'm fairly sure it's true but not absolutely sure.  It looks like it should be simple but it's resisted all my attempts so far.",,['complex-analysis']
82,Conformal map from the punctured unit disc onto the unit disc?,Conformal map from the punctured unit disc onto the unit disc?,,"I remember seeing this statement, I don't remember where (maybe in Lang's Complex). Is this true or do I have a faulty memory. It was always somewhere in the back of my mind but I never believed it. Is it true that there is a conformal map from the punctured unit disc onto the unit disc?","I remember seeing this statement, I don't remember where (maybe in Lang's Complex). Is this true or do I have a faulty memory. It was always somewhere in the back of my mind but I never believed it. Is it true that there is a conformal map from the punctured unit disc onto the unit disc?",,['complex-analysis']
83,Power series without analytic continuation,Power series without analytic continuation,,"Given a formal power series $\sum a_n z^n$ and a radius of convergence $R>0$, there are various ways to extend the function to the boundary such as Abel's theorem Fatou's lemma $H^\infty$ theorem. What is an example of a function that does (almost) nowhere to the boundary? Which power series are proven to not possess an analytic continuation beyond the radius of convergence. The craziest things what I can construct is a finite number of essential singularities using an entire function $f$, which is not a polynomial, and looking at something like $z \mapsto f(1/z)$.","Given a formal power series $\sum a_n z^n$ and a radius of convergence $R>0$, there are various ways to extend the function to the boundary such as Abel's theorem Fatou's lemma $H^\infty$ theorem. What is an example of a function that does (almost) nowhere to the boundary? Which power series are proven to not possess an analytic continuation beyond the radius of convergence. The craziest things what I can construct is a finite number of essential singularities using an entire function $f$, which is not a polynomial, and looking at something like $z \mapsto f(1/z)$.",,"['complex-analysis', 'power-series']"
84,$\frac{\mathrm d^2 \log(\Gamma (z))}{\mathrm dz^2} = \sum\limits_{n = 0}^{\infty} \frac{1}{(z+n)^2}$,,\frac{\mathrm d^2 \log(\Gamma (z))}{\mathrm dz^2} = \sum\limits_{n = 0}^{\infty} \frac{1}{(z+n)^2},How do I show $$\frac{\mathrm d^2 \log(\Gamma(z))}{\mathrm dz^2} = \sum_{n = 0}^{\infty} \frac{1}{(z+n)^2}$$? $\Gamma(z)$ is the gamma function.,How do I show $$\frac{\mathrm d^2 \log(\Gamma(z))}{\mathrm dz^2} = \sum_{n = 0}^{\infty} \frac{1}{(z+n)^2}$$? $\Gamma(z)$ is the gamma function.,,['complex-analysis']
85,Physical or geometric meaning of complex derivative [duplicate],Physical or geometric meaning of complex derivative [duplicate],,"This question already has answers here : Geometrical Meaning of derivative of complex function (2 answers) Closed 7 years ago . The derivative of a real-valued function at a point is the slope of the function at that point. Similarly, what is the physical or geometric meaning of the derivative of a complex-valued function at a point?","This question already has answers here : Geometrical Meaning of derivative of complex function (2 answers) Closed 7 years ago . The derivative of a real-valued function at a point is the slope of the function at that point. Similarly, what is the physical or geometric meaning of the derivative of a complex-valued function at a point?",,"['complex-analysis', 'complex-geometry']"
86,zeroes of holomorphic function,zeroes of holomorphic function,,"I know that zeroes of holomorphic functions are isolated,and I know that if a holomorphic function has zero set whic has a limit point then it is identically zero function,i know a holomorphic function can have countable zero set, does there exixt a holomorphic function which is not identically zero, and has uncountable number of zeroes?","I know that zeroes of holomorphic functions are isolated,and I know that if a holomorphic function has zero set whic has a limit point then it is identically zero function,i know a holomorphic function can have countable zero set, does there exixt a holomorphic function which is not identically zero, and has uncountable number of zeroes?",,['complex-analysis']
87,Prove that the equation $\tan (z)=z$ has only real roots.,Prove that the equation  has only real roots.,\tan (z)=z,Prove that the equation $\tan(z)=z$ has only real roots. How to do it? The idea is that the increment of the argument need to look at the boundary of the square with a side of $\pi n$ and another that $\tan(z)-z$ has a pole at $0$. I do not know how to use it.,Prove that the equation $\tan(z)=z$ has only real roots. How to do it? The idea is that the increment of the argument need to look at the boundary of the square with a side of $\pi n$ and another that $\tan(z)-z$ has a pole at $0$. I do not know how to use it.,,['complex-analysis']
88,Analyticity of $\overline {f(\bar z)}$ given $f(z)$ is analytic [duplicate],Analyticity of  given  is analytic [duplicate],\overline {f(\bar z)} f(z),"This question already has answers here : How do I rigorously show $f(z)$ is analytic if and only if $\overline{f(\bar{z})}$ is? (5 answers) Closed 8 years ago . Suppose $f$ is an analytic function on a domain $D$. Then I need to show that $\overline {f(\bar z)}$ is also analytic.  Here is what I did - Suppose $f(z) = u(x,y) + iv(x,y)$ where $u$ and $v$ are real functions of $x$ and $y$ and $z = x + iy$. Now $f(\bar z) =  u(x,-y) + iv(x,-y) $ and then $\overline {f(\bar z)} = u(x,-y) - iv(x,-y) $. To show that a function is analytic, I need to verify that it satisfy Cauchy-Riemann Equations. I differentiated $\overline {f(\bar z)}$ and checked that it actually satisfies these equations. But here is my doubt - Being analytic means that the function if complex differentiable. Now here to check for analyticity, I am differentiating my function without proving that it's actually analytic. So is this the right way to do it?","This question already has answers here : How do I rigorously show $f(z)$ is analytic if and only if $\overline{f(\bar{z})}$ is? (5 answers) Closed 8 years ago . Suppose $f$ is an analytic function on a domain $D$. Then I need to show that $\overline {f(\bar z)}$ is also analytic.  Here is what I did - Suppose $f(z) = u(x,y) + iv(x,y)$ where $u$ and $v$ are real functions of $x$ and $y$ and $z = x + iy$. Now $f(\bar z) =  u(x,-y) + iv(x,-y) $ and then $\overline {f(\bar z)} = u(x,-y) - iv(x,-y) $. To show that a function is analytic, I need to verify that it satisfy Cauchy-Riemann Equations. I differentiated $\overline {f(\bar z)}$ and checked that it actually satisfies these equations. But here is my doubt - Being analytic means that the function if complex differentiable. Now here to check for analyticity, I am differentiating my function without proving that it's actually analytic. So is this the right way to do it?",,['complex-analysis']
89,A curious integral,A curious integral,,"The following integral has been on my mind for a while $$\int_0^\infty \frac{\sin(x)}{e^x-1}\,\mathrm d x \tag{$\dagger$}$$ Let us indicate the integrand as $f(x)=\frac{\sin(x)}{e^x-1}$. The following are a couple of observations. The integrand can be extended by continuity in $0$ since $$\lim_{x\to 0}f(x)=\lim_{x\to 0}\frac{\frac{\sin(x)}{x}}{\frac{e^x-1}{x}}=1$$ This is the original reason I started playing around with this integral. Mathematica yields the result $$\int_0^\infty \frac{\sin(x)}{e^x-1}\,\mathrm d x=\frac{\pi}{2}\textrm{Coth}(\pi)-\frac 12 \approx 1.076674047$$ which, following numerical evidence, seems correct. Complex analysis may be useful here, since the integrand is a holomorphic function on $\mathbb C$. I tried writing $\sin(z)=\frac{e^{iz}-e^{-iz}}{2i}$, but I was not able to find an appropriate integration contour to solve the problem. The $-1$ in the denominator breaks the simmetry of the expression. This made most of my substitutions useless. Can this integral be evaluated correctly, preferrably through complex analytic methods?","The following integral has been on my mind for a while $$\int_0^\infty \frac{\sin(x)}{e^x-1}\,\mathrm d x \tag{$\dagger$}$$ Let us indicate the integrand as $f(x)=\frac{\sin(x)}{e^x-1}$. The following are a couple of observations. The integrand can be extended by continuity in $0$ since $$\lim_{x\to 0}f(x)=\lim_{x\to 0}\frac{\frac{\sin(x)}{x}}{\frac{e^x-1}{x}}=1$$ This is the original reason I started playing around with this integral. Mathematica yields the result $$\int_0^\infty \frac{\sin(x)}{e^x-1}\,\mathrm d x=\frac{\pi}{2}\textrm{Coth}(\pi)-\frac 12 \approx 1.076674047$$ which, following numerical evidence, seems correct. Complex analysis may be useful here, since the integrand is a holomorphic function on $\mathbb C$. I tried writing $\sin(z)=\frac{e^{iz}-e^{-iz}}{2i}$, but I was not able to find an appropriate integration contour to solve the problem. The $-1$ in the denominator breaks the simmetry of the expression. This made most of my substitutions useless. Can this integral be evaluated correctly, preferrably through complex analytic methods?",,"['complex-analysis', 'improper-integrals']"
90,Conjecture on zeros of analytic function,Conjecture on zeros of analytic function,,"I have a conjecture that I can´t prove nor disprove, any help on doing so  will be very grateful. Let $f: \{z: |z|<2\} \to \mathbb C$ be a non constant analytic function such that if $|z|=1$ then $|f(z)|=1$ . Is it true that the zeros of $f$ can not be in $\{ z: 1/2< |z| < 2 \}$ ? I have successfully proven, by the maximum modulus theorem, that $f$ must have a zero inside $\{ z: |z|<1 \}$ . However, I can not seem to prove that all the zeros must be in $\{ z:|z| < 1/2 \}$ , neither to find a counter example.","I have a conjecture that I can´t prove nor disprove, any help on doing so  will be very grateful. Let be a non constant analytic function such that if then . Is it true that the zeros of can not be in ? I have successfully proven, by the maximum modulus theorem, that must have a zero inside . However, I can not seem to prove that all the zeros must be in , neither to find a counter example.",f: \{z: |z|<2\} \to \mathbb C |z|=1 |f(z)|=1 f \{ z: 1/2< |z| < 2 \} f \{ z: |z|<1 \} \{ z:|z| < 1/2 \},"['complex-analysis', 'analyticity']"
91,"Showing that $e^z$, $\sin(z)$, $\cos(z)$ have essential singularities at $\infty$","Showing that , ,  have essential singularities at",e^z \sin(z) \cos(z) \infty,"Problem: Show that $e^z$, $\sin(z)$, $\cos(z)$ have essential singularities at $\infty$. Attempt for $e^z$: We have that $e^z$ has an essential singularity at $\infty$ iff $e^{1/z}$ has an essential singularity at $0$ iff $$ 0 \ne \lim_{z \to 0} |z|^{\alpha} |e^{1/z}| \ne \infty $$ for all values of $\alpha \in \mathbb{R}$. My intuitions must be off here, because it seems to me like $|e^{1/z}|$ tends to infinity much quicker than $|z|^\alpha$ tends to zero.  That is, it seems to me that indeed $$\lim_{z \to 0} |z|^{\alpha} |e^{1/z}| = \infty$$ which evidently is not the case.","Problem: Show that $e^z$, $\sin(z)$, $\cos(z)$ have essential singularities at $\infty$. Attempt for $e^z$: We have that $e^z$ has an essential singularity at $\infty$ iff $e^{1/z}$ has an essential singularity at $0$ iff $$ 0 \ne \lim_{z \to 0} |z|^{\alpha} |e^{1/z}| \ne \infty $$ for all values of $\alpha \in \mathbb{R}$. My intuitions must be off here, because it seems to me like $|e^{1/z}|$ tends to infinity much quicker than $|z|^\alpha$ tends to zero.  That is, it seems to me that indeed $$\lim_{z \to 0} |z|^{\alpha} |e^{1/z}| = \infty$$ which evidently is not the case.",,"['complex-analysis', 'analysis']"
92,Does $f(z+2\pi)=f(z)$ for all $z\in \mathbb{C}$?,Does  for all ?,f(z+2\pi)=f(z) z\in \mathbb{C},"If $f:\mathbb{C}\rightarrow\mathbb{C}$ is a differentiable function and $f(x+2\pi)=f(x)$ for all $x\in \mathbb{R}$, would $f(z+2\pi)=f(z)$ for all $z\in \mathbb{C}$? Is there any theorem/lemma concerning this? Are there any examples/counter examples for this?","If $f:\mathbb{C}\rightarrow\mathbb{C}$ is a differentiable function and $f(x+2\pi)=f(x)$ for all $x\in \mathbb{R}$, would $f(z+2\pi)=f(z)$ for all $z\in \mathbb{C}$? Is there any theorem/lemma concerning this? Are there any examples/counter examples for this?",,['complex-analysis']
93,Infinity plus Infinity,Infinity plus Infinity,,Let $a \in \mathbb{C}$. Ahlfors says we let $a + \infty = \infty$ and $a \cdot \infty = \infty$. But we cannot define $\infty + \infty$ without violating the laws of arithimetic (i.e. field axioms). I don't see why this is. Don't we have $\infty + \infty = \infty$ by applying the distributive law to $2\cdot \infty$? What am I misunderstanding?,Let $a \in \mathbb{C}$. Ahlfors says we let $a + \infty = \infty$ and $a \cdot \infty = \infty$. But we cannot define $\infty + \infty$ without violating the laws of arithimetic (i.e. field axioms). I don't see why this is. Don't we have $\infty + \infty = \infty$ by applying the distributive law to $2\cdot \infty$? What am I misunderstanding?,,['complex-analysis']
94,Constant Curvature Metric and Biholomorphic Equivalence,Constant Curvature Metric and Biholomorphic Equivalence,,"This is probably a dumb question, but let's try it anyway. I know two versions of the uniformization theorem, and I don't understand their equivalence. The first says that every Riemann surface has a metric under which it has constant Gauss curvature, and the second says that every Riemann surface is biholomorphically equivalent to the plane, the sphere or the disk (or the upper half plane). Why are these things the same? I have seen several proofs of uniformization, but they use PDE or variational methods in order to prescribe curvature on a surface. I'm not sure I see how this implies anything complex analytic at all though.. It is a known fact that if two surfaces have the same curvature at a point, they are locally isometric. Since the uniform metric gives it constant curvature, it is tempting to say they are globally isometric. I'm not quite sure that's right though. I know that for compact surfaces, I can construct fundamental domains in the disk which after taking quotients by some isometry gives the surface, but this is just another statement of uniformization, if I'm understand correctly. Can someone help clear up my confusion? How do I get from knowing the curvature is constant to a biholomorhic equivalence between the two?","This is probably a dumb question, but let's try it anyway. I know two versions of the uniformization theorem, and I don't understand their equivalence. The first says that every Riemann surface has a metric under which it has constant Gauss curvature, and the second says that every Riemann surface is biholomorphically equivalent to the plane, the sphere or the disk (or the upper half plane). Why are these things the same? I have seen several proofs of uniformization, but they use PDE or variational methods in order to prescribe curvature on a surface. I'm not sure I see how this implies anything complex analytic at all though.. It is a known fact that if two surfaces have the same curvature at a point, they are locally isometric. Since the uniform metric gives it constant curvature, it is tempting to say they are globally isometric. I'm not quite sure that's right though. I know that for compact surfaces, I can construct fundamental domains in the disk which after taking quotients by some isometry gives the surface, but this is just another statement of uniformization, if I'm understand correctly. Can someone help clear up my confusion? How do I get from knowing the curvature is constant to a biholomorhic equivalence between the two?",,"['complex-analysis', 'differential-geometry', 'riemann-surfaces']"
95,"Fundamental theorem of calculus for complex analysis, proof","Fundamental theorem of calculus for complex analysis, proof",,"I've been trying to verify/fill in the details of my book's proof of the complex FTOC, but have gotten stuck. Here are the statement and proof: If a continuous function $f$ has a primitive $F$ in $\Omega$, and $\gamma$ is a curve in $\Omega$ that begins at $w_{1}$ and ends at $w_{2}$, then $\int_{\gamma}f(z)dz = F(w_{2}) - F(w_{1})$. Here, a primitive for $f$ on $\Omega$ is a function $F$ that is holomorphic on $\Omega$ and such that $F'(z)=f(z)$ for all $z\in \Omega$, where that derivative is meant to be the complex derivative. Also, $\gamma$ being smooth means that any of its parametrizations $z:[a,b] \rightarrow \mathbb{C}$ is continuously differentiable. Proof: Suppose we have such a parametrization, and $z(a)=w_{1},z(b)=w_{2}$. then using the chain rule and the fundamental theorem of calculus, \begin{align*} \int_{\gamma}f(z)\,dz &= \int_{a}^{b}f(z(t))z'(t)\,dt \\ &= \int_{a}^{b}F'(z(t))z'(t)\,dt \\ &= \int_{a}^{b}\frac{d}{dt}F(z(t))\,dt \\ &= F(z(b)) - F(z(a)) \end{align*} I'm having trouble with the 3rd and 4th equalities in the above string. First, $F'$ means the complex derivative. We had a theorem earlier that said for any function $g:\mathbb{C} \rightarrow \mathbb{C}$ holomorphic at some $z_{0}$, $g'(z_{0})=\frac{\partial g}{\partial z}(z_{0})$, where we defined the operator $\frac{\partial}{\partial z} = \frac12\left(\frac{\partial}{\partial x} - i\frac{\partial}{\partial y}\right)$. Let $m,n$ be the component functions of $z$, i.e. $m,n:[a,b]\rightarrow \mathbb{R}$ such that $z(t)=m(t)+i n(t)$ for all $t\in [a,b]$. Similarly, let $U,V:\Omega \rightarrow \mathbb{C}$ denote the component functions of $F$. Then  \begin{align*} F'(z(t))z'(t) &= \frac{\partial F}{\partial z}(z(t))    \left[\frac{d}{dt}m(t) + i\frac{d}{dt}n(t)\right] \\ &= \frac{1}{2}\left[\frac{\partial F}{\partial x}(z(t))                      - i\frac{\partial F}{\partial y}(z(t))\right]    \left[\frac{d}{dt}m(t) + i\frac{d}{dt}n(t)\right] \\ &= \frac{1}{2}\left(\left[\frac{\partial U}{\partial x}                            + \frac{\partial U}{\partial y}\right]    +i\left[\frac{\partial V}{\partial y}             + \frac{\partial V}{\partial x}\right]\right)    \left[\frac{d}{dt}m(t) + i\frac{d}{dt}n(t)\right] \end{align*} On the other hand, I'm not sure how to compute $\frac{d}{dt}F(z(t))$ using the chain rule. I'd be able to do it if I considered $F$ as a function from $\mathbb{R}^{2}$ to $\mathbb{R}^{2}$. I've tried guessing that the notational convention is  \begin{align*} \frac{d}{dt}F(z(t)) &= \frac{d}{dt}U(z(t)) + i \frac{d}{dt} V(z(t)) \end{align*} where $U(z(t)),V(z(t))$ are to be computed using the multivariable(real) chain rule, but that gave me something different from what I got for $F'(z(t))z'(t)$ above. For the fourth equality, where the FTOC is supposed to be used, I'm not sure how to parse it. The FTOC that I know is for functions $:\mathbb{R} \rightarrow \mathbb{R}$. Any hints, insight, or more detail are very much appreciated.","I've been trying to verify/fill in the details of my book's proof of the complex FTOC, but have gotten stuck. Here are the statement and proof: If a continuous function $f$ has a primitive $F$ in $\Omega$, and $\gamma$ is a curve in $\Omega$ that begins at $w_{1}$ and ends at $w_{2}$, then $\int_{\gamma}f(z)dz = F(w_{2}) - F(w_{1})$. Here, a primitive for $f$ on $\Omega$ is a function $F$ that is holomorphic on $\Omega$ and such that $F'(z)=f(z)$ for all $z\in \Omega$, where that derivative is meant to be the complex derivative. Also, $\gamma$ being smooth means that any of its parametrizations $z:[a,b] \rightarrow \mathbb{C}$ is continuously differentiable. Proof: Suppose we have such a parametrization, and $z(a)=w_{1},z(b)=w_{2}$. then using the chain rule and the fundamental theorem of calculus, \begin{align*} \int_{\gamma}f(z)\,dz &= \int_{a}^{b}f(z(t))z'(t)\,dt \\ &= \int_{a}^{b}F'(z(t))z'(t)\,dt \\ &= \int_{a}^{b}\frac{d}{dt}F(z(t))\,dt \\ &= F(z(b)) - F(z(a)) \end{align*} I'm having trouble with the 3rd and 4th equalities in the above string. First, $F'$ means the complex derivative. We had a theorem earlier that said for any function $g:\mathbb{C} \rightarrow \mathbb{C}$ holomorphic at some $z_{0}$, $g'(z_{0})=\frac{\partial g}{\partial z}(z_{0})$, where we defined the operator $\frac{\partial}{\partial z} = \frac12\left(\frac{\partial}{\partial x} - i\frac{\partial}{\partial y}\right)$. Let $m,n$ be the component functions of $z$, i.e. $m,n:[a,b]\rightarrow \mathbb{R}$ such that $z(t)=m(t)+i n(t)$ for all $t\in [a,b]$. Similarly, let $U,V:\Omega \rightarrow \mathbb{C}$ denote the component functions of $F$. Then  \begin{align*} F'(z(t))z'(t) &= \frac{\partial F}{\partial z}(z(t))    \left[\frac{d}{dt}m(t) + i\frac{d}{dt}n(t)\right] \\ &= \frac{1}{2}\left[\frac{\partial F}{\partial x}(z(t))                      - i\frac{\partial F}{\partial y}(z(t))\right]    \left[\frac{d}{dt}m(t) + i\frac{d}{dt}n(t)\right] \\ &= \frac{1}{2}\left(\left[\frac{\partial U}{\partial x}                            + \frac{\partial U}{\partial y}\right]    +i\left[\frac{\partial V}{\partial y}             + \frac{\partial V}{\partial x}\right]\right)    \left[\frac{d}{dt}m(t) + i\frac{d}{dt}n(t)\right] \end{align*} On the other hand, I'm not sure how to compute $\frac{d}{dt}F(z(t))$ using the chain rule. I'd be able to do it if I considered $F$ as a function from $\mathbb{R}^{2}$ to $\mathbb{R}^{2}$. I've tried guessing that the notational convention is  \begin{align*} \frac{d}{dt}F(z(t)) &= \frac{d}{dt}U(z(t)) + i \frac{d}{dt} V(z(t)) \end{align*} where $U(z(t)),V(z(t))$ are to be computed using the multivariable(real) chain rule, but that gave me something different from what I got for $F'(z(t))z'(t)$ above. For the fourth equality, where the FTOC is supposed to be used, I'm not sure how to parse it. The FTOC that I know is for functions $:\mathbb{R} \rightarrow \mathbb{R}$. Any hints, insight, or more detail are very much appreciated.",,"['complex-analysis', 'contour-integration']"
96,Is Morera's theorem the inverse theorem of Goursat's theorem?,Is Morera's theorem the inverse theorem of Goursat's theorem?,,"While I'm reading Complex Analysis by Elias M.Stein, I found that there must be some relations between Goursat's theorem and Morera's theorem. According to Stein, the 2 theorems are as following: Goursat's theorem: If $\Omega$ is an open set in $\mathbb{C}$, and T $\subset \Omega$ a triangle whose interior is also contained in $\Omega$, then $$\int_Tf(z)\,dz=0,$$  whenever f is holomorphic in $\Omega$. Morera's theorem: Suppose $f$ is a continuous complex-valued function in the open disc $D$ in $\Bbb{C}$ such that for any triangle $T$ with its interior in $D$ we have: $$\int_Tf(z)\,dz=0,$$ then $f$ is holomorphic. As far as I'm concerned, Morera's theorem is the inverse theorem of Goursat's theorem. I mean, the former tells us a property of the holomorphic functions, while the latter tells us how to determine a function is holomorphic or not, is it right? And, what's the difference between open set $\Omega$ and open disc $D$ here? Are they interchangeable without the result changing? Please help me.","While I'm reading Complex Analysis by Elias M.Stein, I found that there must be some relations between Goursat's theorem and Morera's theorem. According to Stein, the 2 theorems are as following: Goursat's theorem: If $\Omega$ is an open set in $\mathbb{C}$, and T $\subset \Omega$ a triangle whose interior is also contained in $\Omega$, then $$\int_Tf(z)\,dz=0,$$  whenever f is holomorphic in $\Omega$. Morera's theorem: Suppose $f$ is a continuous complex-valued function in the open disc $D$ in $\Bbb{C}$ such that for any triangle $T$ with its interior in $D$ we have: $$\int_Tf(z)\,dz=0,$$ then $f$ is holomorphic. As far as I'm concerned, Morera's theorem is the inverse theorem of Goursat's theorem. I mean, the former tells us a property of the holomorphic functions, while the latter tells us how to determine a function is holomorphic or not, is it right? And, what's the difference between open set $\Omega$ and open disc $D$ here? Are they interchangeable without the result changing? Please help me.",,"['complex-analysis', 'analysis']"
97,How were Blaschke factors discovered?,How were Blaschke factors discovered?,,"The map $$\psi_\alpha(z) = \frac{\alpha - z}{1 - \overline{\alpha}z}$$ can be shown to be a conformal map from the disc onto itself that interchanges $\alpha$ and $0$. I understand how to prove this in a few ways, although I'm a little bewildered at how one would guess this function $\psi_a$ in the first place. If I wanted an automorphism of the disc that interchanged $\alpha$ and $0$, how would I have thought of the above function?","The map $$\psi_\alpha(z) = \frac{\alpha - z}{1 - \overline{\alpha}z}$$ can be shown to be a conformal map from the disc onto itself that interchanges $\alpha$ and $0$. I understand how to prove this in a few ways, although I'm a little bewildered at how one would guess this function $\psi_a$ in the first place. If I wanted an automorphism of the disc that interchanged $\alpha$ and $0$, how would I have thought of the above function?",,['complex-analysis']
98,Complex Zeros of $z^2e^z-z$,Complex Zeros of,z^2e^z-z,"Can anyone give me a hint on showing (in a relatively elegant way, as I know the answer from WolframAlpha), that the complex valued function $z^2e^z-z$ has at most 2 roots with norm less than 2?  Obviously it has one root at $z=0$.  The other root is the number $W_0(1)$, where $W_0(1)$ is the principal branch of the product log function at 1 (the inverse of the function $ze^z$, and is approximately equal to .56.  I have seen that the next possible zero is outside of the ball of radius 2, but I cannot show it algebraically.  I am primarily trying to use Rouche's Theorem, and to find a suitable function $g(z)$ such that $z^2e^z-z-g(z)|<|g(z)|$, however I have not been able to find a good one yet. Any help would be much appreciated, especially ideas about what sort of function to consider in cases like this, rather than just ""here's the function..."" Thanks!","Can anyone give me a hint on showing (in a relatively elegant way, as I know the answer from WolframAlpha), that the complex valued function $z^2e^z-z$ has at most 2 roots with norm less than 2?  Obviously it has one root at $z=0$.  The other root is the number $W_0(1)$, where $W_0(1)$ is the principal branch of the product log function at 1 (the inverse of the function $ze^z$, and is approximately equal to .56.  I have seen that the next possible zero is outside of the ball of radius 2, but I cannot show it algebraically.  I am primarily trying to use Rouche's Theorem, and to find a suitable function $g(z)$ such that $z^2e^z-z-g(z)|<|g(z)|$, however I have not been able to find a good one yet. Any help would be much appreciated, especially ideas about what sort of function to consider in cases like this, rather than just ""here's the function..."" Thanks!",,"['complex-analysis', 'approximation', 'roots']"
99,"Is there a way to ""proof"" the Wirtinger derivates?","Is there a way to ""proof"" the Wirtinger derivates?",,"I'm a bit confused about the wirtinger derivates, as I understand they define: $$df/dz := 1/2 (df/dx - idf/dy)$$ and $$df/d\bar z := 1/2 (df/dx +idf/dy)$$ Is there actually a way to derive a holomorph function with $d/d\bar z$, as of the limes definition of the derivate? Or is it just that there are there some benefits if we introduce the operater $d/d\bar z$ defined like that? If so what does this enable us to do?","I'm a bit confused about the wirtinger derivates, as I understand they define: $$df/dz := 1/2 (df/dx - idf/dy)$$ and $$df/d\bar z := 1/2 (df/dx +idf/dy)$$ Is there actually a way to derive a holomorph function with $d/d\bar z$, as of the limes definition of the derivate? Or is it just that there are there some benefits if we introduce the operater $d/d\bar z$ defined like that? If so what does this enable us to do?",,"['complex-analysis', 'derivatives', 'proof-explanation']"
