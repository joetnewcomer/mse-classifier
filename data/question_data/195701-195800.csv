,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Instantaneous rates of change,Instantaneous rates of change,,"I am having problems solving the following question. The volume, $V$, of a sphere of radius r is given by $V=f(r)=\frac{4}{3}\pi r^3$. Calculate the instantaneous rate of change of the volume, $V$, with the respect to change of the radius, $r$, at $r=36.4$. I assume the answer to this question would be $f\prime(36.4)$ where $f\prime$ is equal to; $f\prime(x) = 4\pi x^2 \\ f\prime(36.4) = 4\pi (36.4)^2 \\= 16649.93$ Although this is not the solution. Please advise me where I have went wrong.","I am having problems solving the following question. The volume, $V$, of a sphere of radius r is given by $V=f(r)=\frac{4}{3}\pi r^3$. Calculate the instantaneous rate of change of the volume, $V$, with the respect to change of the radius, $r$, at $r=36.4$. I assume the answer to this question would be $f\prime(36.4)$ where $f\prime$ is equal to; $f\prime(x) = 4\pi x^2 \\ f\prime(36.4) = 4\pi (36.4)^2 \\= 16649.93$ Although this is not the solution. Please advise me where I have went wrong.",,"['calculus', 'derivatives']"
1,The derivative of a two-to-one complex function has no zeros.,The derivative of a two-to-one complex function has no zeros.,,"Let $U$ be open in $\mathbb{C}$ and $f \in H(U)$ such that $f$ is two-to-one on $U$. Prove that $f'$ has no zeros in $U$. I am thinking about using Cauchy's Integral formula for derivatives, but I do not know what to do with two-to-one condition.","Let $U$ be open in $\mathbb{C}$ and $f \in H(U)$ such that $f$ is two-to-one on $U$. Prove that $f'$ has no zeros in $U$. I am thinking about using Cauchy's Integral formula for derivatives, but I do not know what to do with two-to-one condition.",,"['complex-analysis', 'derivatives']"
2,Find the points on the graph of $f(x) = 12(x + 9) − (x + 9)^3$ where the tangent line is horizontal.,Find the points on the graph of  where the tangent line is horizontal.,f(x) = 12(x + 9) − (x + 9)^3,"I cannot figure out how to get started on this question. Would I First simplify, and then take the derivative? Please help!","I cannot figure out how to get started on this question. Would I First simplify, and then take the derivative? Please help!",,"['calculus', 'derivatives']"
3,Partial derivative of double summation mixing rule,Partial derivative of double summation mixing rule,,"I am trying to derive the expression for fugacity for a Van der Waals gas using the fundamental thermodynamic equations. I have encountered a problem in trying to calculate the partial derivative of a double summation mixing rule as follows: $$ an^2 = \sum_i\sum_j n_in_j(1-k_{ij})\sqrt{a_ia_j} $$ I know from the Prausnitz molecular thermodynamics text that the derivative for a simpler expression is as follows: $$ an^2 = \sum_i\sum_j n_in_j\sqrt{a_ia_j} $$ $$  \frac{\partial{(an^2)}}{\partial{n_i}} = 2\sqrt a_i\sum_jn_j\sqrt a_j $$ However, when I take the derivative of the expression involving the constant $k_{ij}$ I get the following: $$ \frac{\partial{an^2}}{\partial{n_i}} = 2an_i(1-k_{ii})\sum_jn_j(1-k_{ij})\sqrt{a_j} $$ Wondering if anyone has insight into a more clear, systematic way to take a partial derivative of a double sum. NOTE: $a_i,a_j,k_{ij}$ are all considered to be constants and $n$ is the total number of moles and therefore also a constant.","I am trying to derive the expression for fugacity for a Van der Waals gas using the fundamental thermodynamic equations. I have encountered a problem in trying to calculate the partial derivative of a double summation mixing rule as follows: $$ an^2 = \sum_i\sum_j n_in_j(1-k_{ij})\sqrt{a_ia_j} $$ I know from the Prausnitz molecular thermodynamics text that the derivative for a simpler expression is as follows: $$ an^2 = \sum_i\sum_j n_in_j\sqrt{a_ia_j} $$ $$  \frac{\partial{(an^2)}}{\partial{n_i}} = 2\sqrt a_i\sum_jn_j\sqrt a_j $$ However, when I take the derivative of the expression involving the constant $k_{ij}$ I get the following: $$ \frac{\partial{an^2}}{\partial{n_i}} = 2an_i(1-k_{ii})\sum_jn_j(1-k_{ij})\sqrt{a_j} $$ Wondering if anyone has insight into a more clear, systematic way to take a partial derivative of a double sum. NOTE: $a_i,a_j,k_{ij}$ are all considered to be constants and $n$ is the total number of moles and therefore also a constant.",,"['derivatives', 'summation']"
4,How to show this function is increasing? Related to Normal distribution.,How to show this function is increasing? Related to Normal distribution.,,"Numerically, it seems the following function $F(x)$ is increasing in $x$. How can I show it analytically? $$F(x)=G(x)L'(x)$$ where $L(x)=\frac{(1-G(x))^3}{G'(x)}$ and $G(x)=\int_{-\infty}^x \frac{e^{- \frac{ 1}{2} y^2}}{\sqrt{2\pi}} dy$, and $L'(x)$ is derivative of $L(x)$. Note that $G(\cdot)$ can be interpreted as the CDF of standard normal distribution and $L(\cdot)$ is proportional to inverse hazard rate of standard normal distribution. We can show that $L(\cdot)$ is decreasing.","Numerically, it seems the following function $F(x)$ is increasing in $x$. How can I show it analytically? $$F(x)=G(x)L'(x)$$ where $L(x)=\frac{(1-G(x))^3}{G'(x)}$ and $G(x)=\int_{-\infty}^x \frac{e^{- \frac{ 1}{2} y^2}}{\sqrt{2\pi}} dy$, and $L'(x)$ is derivative of $L(x)$. Note that $G(\cdot)$ can be interpreted as the CDF of standard normal distribution and $L(\cdot)$ is proportional to inverse hazard rate of standard normal distribution. We can show that $L(\cdot)$ is decreasing.",,"['derivatives', 'normal-distribution']"
5,Derivability and Differentiability,Derivability and Differentiability,,"Consider the function $f(x,y) = (x^2y)^\frac{1}{3}$ continuous is $\mathbb{R}^2$ and its partial derivatives: $$\begin{cases} f_x(x,y) = \frac{2}{3}\left(\frac{y}{x}\right)^\frac{1}{3} \\ f_y(x,y) = \frac{1}{3}\left(\frac{x}{y}\right)^\frac{2}{3} \\ \end{cases}$$ Clearly, $f_x$ is defined for $x \neq 0$. Evaluating limits, then: $$\lim_{x \to 0} f_x(x,y) = \begin{cases} \infty & \text{if}~y\neq 0 \\ \frac{0}{0} & \text{if}~y = 0 \end{cases}$$ By the way: $$\lim_{x \to 0} f_x(x,0) = \lim_{h \to 0} \frac{f(0+h,0)-f(0,0)}{h} = \lim_{h \to 0} \frac{((0+h)^20)^\frac{1}{3}-0}{h} = 0.$$ Should I conclude that $f_x(x,y)$ is continuous in $(0,0)$ and then that $f(x,y)$ has the partial derivative with respect to $x$? A very similar question arises when dealing with $f_y$, concluding that $$\lim_{y \to 0} f_y(x,y) = \begin{cases} \infty & \text{if}~x\neq 0 \\ 0 & \text{if}~x = 0 \end{cases}$$ Since (I presume!) $f$ is derivable in $(0,0)$e, then I can conclude that the function is differentiable in $(0,0)$. But when I evaluate the limit to establish differentiability, then ... $$\lim_{(h,k) \to 0} \frac{(h^2k)^\frac{1}{3}}{(h^2+k^2)^\frac{1}{2}} = \lim_{\rho \to 0} (\cos(\theta)^2\sin(\theta))^\frac{1}{3}$$ where at the end I transformed $(x,y)$ to polar coordinates $(\rho, \theta)$ as usual. Of course, the limit does not exist. Summary : I have a function which I presume is derivable everywhere in$\mathbb{R}^2$. But I also prove that the function is not differentiable. What am I doing wrongly? Errata : I have a function which I presume is derivable in $(0,0)$ . But I also prove that the function is not differentiable in $(0,0)$ . What am I doing wrongly?","Consider the function $f(x,y) = (x^2y)^\frac{1}{3}$ continuous is $\mathbb{R}^2$ and its partial derivatives: $$\begin{cases} f_x(x,y) = \frac{2}{3}\left(\frac{y}{x}\right)^\frac{1}{3} \\ f_y(x,y) = \frac{1}{3}\left(\frac{x}{y}\right)^\frac{2}{3} \\ \end{cases}$$ Clearly, $f_x$ is defined for $x \neq 0$. Evaluating limits, then: $$\lim_{x \to 0} f_x(x,y) = \begin{cases} \infty & \text{if}~y\neq 0 \\ \frac{0}{0} & \text{if}~y = 0 \end{cases}$$ By the way: $$\lim_{x \to 0} f_x(x,0) = \lim_{h \to 0} \frac{f(0+h,0)-f(0,0)}{h} = \lim_{h \to 0} \frac{((0+h)^20)^\frac{1}{3}-0}{h} = 0.$$ Should I conclude that $f_x(x,y)$ is continuous in $(0,0)$ and then that $f(x,y)$ has the partial derivative with respect to $x$? A very similar question arises when dealing with $f_y$, concluding that $$\lim_{y \to 0} f_y(x,y) = \begin{cases} \infty & \text{if}~x\neq 0 \\ 0 & \text{if}~x = 0 \end{cases}$$ Since (I presume!) $f$ is derivable in $(0,0)$e, then I can conclude that the function is differentiable in $(0,0)$. But when I evaluate the limit to establish differentiability, then ... $$\lim_{(h,k) \to 0} \frac{(h^2k)^\frac{1}{3}}{(h^2+k^2)^\frac{1}{2}} = \lim_{\rho \to 0} (\cos(\theta)^2\sin(\theta))^\frac{1}{3}$$ where at the end I transformed $(x,y)$ to polar coordinates $(\rho, \theta)$ as usual. Of course, the limit does not exist. Summary : I have a function which I presume is derivable everywhere in$\mathbb{R}^2$. But I also prove that the function is not differentiable. What am I doing wrongly? Errata : I have a function which I presume is derivable in $(0,0)$ . But I also prove that the function is not differentiable in $(0,0)$ . What am I doing wrongly?",,"['derivatives', 'continuity']"
6,Instantaneous Rate of Change/Derivative,Instantaneous Rate of Change/Derivative,,"I am having a bit of trouble with a question on my Calc HW. Given P(x)= 3x^2+3, estimate the Instantaneous rate of change at x=5. This is what I have so far. $$f(x+h) = 3(x+h)^2+3-(3x^2+3)$$       $$= (3x+3h)(x+h)+3-(3x^2+3)$$        $$= 3x^2+3xh+3xh+3h^2+3-(3x^2+3)$$        $$= 3x^2+6xh+3h^2+3-3x^2-3$$       $$ = (6xh+3h^2/h)$$       $$ = h(6x+3h)/h$$        eliminate $h$ and left with $6x+3h$ I know this is incorrect, but don't know where I went wrong. I know for the last step, you have to plug in 5 to x and solve to get the Rate of change.","I am having a bit of trouble with a question on my Calc HW. Given P(x)= 3x^2+3, estimate the Instantaneous rate of change at x=5. This is what I have so far. $$f(x+h) = 3(x+h)^2+3-(3x^2+3)$$       $$= (3x+3h)(x+h)+3-(3x^2+3)$$        $$= 3x^2+3xh+3xh+3h^2+3-(3x^2+3)$$        $$= 3x^2+6xh+3h^2+3-3x^2-3$$       $$ = (6xh+3h^2/h)$$       $$ = h(6x+3h)/h$$        eliminate $h$ and left with $6x+3h$ I know this is incorrect, but don't know where I went wrong. I know for the last step, you have to plug in 5 to x and solve to get the Rate of change.",,"['calculus', 'derivatives']"
7,Related rates - approximation or not?,Related rates - approximation or not?,,The radius of a uniform spherical balloon is increasing at 3% per   second. Find the % rate at which its volume is increasing. My solution: Percentage is change of some value divided by that value so the percentage rate at which the volume is increasing will be given by: $$\frac{\frac{dv}{V}}{dt}$$ We already know that $$\frac{\frac{dr}{r}}{dt}=3\frac{\%}{s}$$ Then: $$\frac{\frac{dV}{V}}{dt} = \frac{\frac{dV}{V}}{\frac{dr}{r}}* \frac{\frac{dr}{r}}{dt}$$ $$\frac{\frac{dV}{V}}{dt} = \frac{dV}{V}* \frac{r}{dr} * 3$$ $$\frac{\frac{dV}{V}}{dt} = \frac{dV}{dr}* \frac{r}{V} * 3$$ $$\frac{\frac{dV}{V}}{dt} = 4\pi r^{2}* \frac{r}{V} * 3$$ $$\frac{\frac{dV}{V}}{dt} = \frac{4\pi r^{3}}{\frac{4}{3}\pi r^{3}} * 3$$ $$\frac{\frac{dV}{V}}{dt} = \frac{4\pi r^{3}}{\frac{4}{3}\pi r^{3}} * 3$$ $$\frac{\frac{dV}{V}}{dt} = 3 * 3$$ $$\frac{\frac{dV}{V}}{dt} = 9\frac{\%}{s}$$ Questions: 1) Is such reasoning correct? I'm particularly concerned about this transformation: $$\frac{\frac{dV}{V}}{\frac{dr}{r}}=\frac{dV}{dr}* \frac{r}{V}$$ I remember reading that I can't treat derivatives as fractions but it happens to work here and this confuses me. 2) Is the final result exactly 9 % per second or is this only an approximation? For example the answer to: If $V=2x^{3}$ what is the approximate percentage change in $V$ when   $x$ changes by 2%? is 6% but this is only an approximation (see Is dv only approximate of dv/dx*dx? ) Is this the case in the original exercise or is 9 % per second an exact result?,The radius of a uniform spherical balloon is increasing at 3% per   second. Find the % rate at which its volume is increasing. My solution: Percentage is change of some value divided by that value so the percentage rate at which the volume is increasing will be given by: $$\frac{\frac{dv}{V}}{dt}$$ We already know that $$\frac{\frac{dr}{r}}{dt}=3\frac{\%}{s}$$ Then: $$\frac{\frac{dV}{V}}{dt} = \frac{\frac{dV}{V}}{\frac{dr}{r}}* \frac{\frac{dr}{r}}{dt}$$ $$\frac{\frac{dV}{V}}{dt} = \frac{dV}{V}* \frac{r}{dr} * 3$$ $$\frac{\frac{dV}{V}}{dt} = \frac{dV}{dr}* \frac{r}{V} * 3$$ $$\frac{\frac{dV}{V}}{dt} = 4\pi r^{2}* \frac{r}{V} * 3$$ $$\frac{\frac{dV}{V}}{dt} = \frac{4\pi r^{3}}{\frac{4}{3}\pi r^{3}} * 3$$ $$\frac{\frac{dV}{V}}{dt} = \frac{4\pi r^{3}}{\frac{4}{3}\pi r^{3}} * 3$$ $$\frac{\frac{dV}{V}}{dt} = 3 * 3$$ $$\frac{\frac{dV}{V}}{dt} = 9\frac{\%}{s}$$ Questions: 1) Is such reasoning correct? I'm particularly concerned about this transformation: $$\frac{\frac{dV}{V}}{\frac{dr}{r}}=\frac{dV}{dr}* \frac{r}{V}$$ I remember reading that I can't treat derivatives as fractions but it happens to work here and this confuses me. 2) Is the final result exactly 9 % per second or is this only an approximation? For example the answer to: If $V=2x^{3}$ what is the approximate percentage change in $V$ when   $x$ changes by 2%? is 6% but this is only an approximation (see Is dv only approximate of dv/dx*dx? ) Is this the case in the original exercise or is 9 % per second an exact result?,,"['calculus', 'derivatives', 'approximation']"
8,Show that the function $f(x) = x^{2}|\cos(\pi /x)|$ is not differentiable at $x = 2n/(2n+1)$ but is at $0$,Show that the function  is not differentiable at  but is at,f(x) = x^{2}|\cos(\pi /x)| x = 2n/(2n+1) 0,"I found this problem while trying out different things, and I'm curious to see how this works. i've been having a lot of trouble and my teacher is out of town this week. I need to show that $$f(x) = x^{2}|\cos(\pi /x)|$$ isn't  differentiable at $x=2n/(2n+1)$ but is differentiable at $0$. I'm not sure exactly how to get the derivative. Using Wolfram Alpha I got  $$(4 x |\cos(\pi/x)|^2+\pi \sin((2 \pi)/x))/(2 |\cos(\pi/x)|)$$ but I'm unsure. Plugging in $x=2n/(2n+1)$ didn't yield anything.","I found this problem while trying out different things, and I'm curious to see how this works. i've been having a lot of trouble and my teacher is out of town this week. I need to show that $$f(x) = x^{2}|\cos(\pi /x)|$$ isn't  differentiable at $x=2n/(2n+1)$ but is differentiable at $0$. I'm not sure exactly how to get the derivative. Using Wolfram Alpha I got  $$(4 x |\cos(\pi/x)|^2+\pi \sin((2 \pi)/x))/(2 |\cos(\pi/x)|)$$ but I'm unsure. Plugging in $x=2n/(2n+1)$ didn't yield anything.",,"['real-analysis', 'limits', 'derivatives']"
9,Prove that $|x|$ isn't differentiable at 0 through epsilon-delta logic?,Prove that  isn't differentiable at 0 through epsilon-delta logic?,|x|,"I remember in high school a proof of this was to show that the right-hand and left-hand limits were not equal to each other. But I was wondering, coming off of learning the $\epsilon - \delta $ notation recently, if a more elegant method existed? I'm still fuzzy on the intricacies of this notation, so I was wondering if this example would help clarify it for me.","I remember in high school a proof of this was to show that the right-hand and left-hand limits were not equal to each other. But I was wondering, coming off of learning the $\epsilon - \delta $ notation recently, if a more elegant method existed? I'm still fuzzy on the intricacies of this notation, so I was wondering if this example would help clarify it for me.",,"['real-analysis', 'analysis', 'limits', 'derivatives']"
10,Show that $f$ is Gateaux differentiable,Show that  is Gateaux differentiable,f,"Define a function $f:\mathbb{R}^2\rightarrow \mathbb{R}$ as follows $$f(x,y)= \begin{cases}        \frac{2y \exp(-x^{-2})}{y^2+ \exp(-2x^{-2})} & x \neq 0 \\       0 & \text{otherwise}     \end{cases} $$ Show that $f$ is Gateaux differentiable at $(0,0)$ but that $f$ is not continuous there. So I think I have to show that $f'((0,0),e_1)$ and $f'((0,0),e_2)$ exits and they are both linear. So far I have that $f'((0,0),e_1) =f'((0,0),e_2)=0$","Define a function $f:\mathbb{R}^2\rightarrow \mathbb{R}$ as follows $$f(x,y)= \begin{cases}        \frac{2y \exp(-x^{-2})}{y^2+ \exp(-2x^{-2})} & x \neq 0 \\       0 & \text{otherwise}     \end{cases} $$ Show that $f$ is Gateaux differentiable at $(0,0)$ but that $f$ is not continuous there. So I think I have to show that $f'((0,0),e_1)$ and $f'((0,0),e_2)$ exits and they are both linear. So far I have that $f'((0,0),e_1) =f'((0,0),e_2)=0$",,"['real-analysis', 'derivatives', 'optimization', 'optimal-control']"
11,Is it true that $e^{\sin(3.14)}e^{3.14} \le e^{\sin(3.15)}e^{3.15}$?,Is it true that ?,e^{\sin(3.14)}e^{3.14} \le e^{\sin(3.15)}e^{3.15},I have to determine whether is it true that $$e^{\sin(3.14)}e^{3.14} \le e^{\sin(3.15)}e^{3.15}$$ and whether it is a equality. I even don't know how to begin with it...,I have to determine whether is it true that $$e^{\sin(3.14)}e^{3.14} \le e^{\sin(3.15)}e^{3.15}$$ and whether it is a equality. I even don't know how to begin with it...,,"['trigonometry', 'inequality', 'derivatives', 'exponential-function']"
12,Do roots lead to two antiderivatives that differ in their non-constant terms?,Do roots lead to two antiderivatives that differ in their non-constant terms?,,"Consider the following example: $f'(x)= x^{-3/2}$ and $f(4)=2 $ $f'(x)= x^{-3/2}\Rightarrow \frac {x^{(-3/2) + 1}} {-1/2}  \Rightarrow$ $\frac{-2}{\sqrt x} +C =f(x)$ This is where the problem arises now if I consider $x= 4$ then I get $$\frac{-2}{\sqrt 4} +C \Rightarrow \frac{-2}{\pm 2} +C $$ Obviously this leads to two different antiderivatives if we consider two different cases which doesn't seem right. I was told that we consider the positive number in these situations but why? And what does the negative case represent if we were to ""humor"" it? Your answers will appreciated. Thank you. Also I apologize if the question title is not descriptive of the case I'm describing. If you can think of a better title go ahead and change it or let me know, just so if anyone else looks for a similar problem have an easy time finding it?","Consider the following example: $f'(x)= x^{-3/2}$ and $f(4)=2 $ $f'(x)= x^{-3/2}\Rightarrow \frac {x^{(-3/2) + 1}} {-1/2}  \Rightarrow$ $\frac{-2}{\sqrt x} +C =f(x)$ This is where the problem arises now if I consider $x= 4$ then I get $$\frac{-2}{\sqrt 4} +C \Rightarrow \frac{-2}{\pm 2} +C $$ Obviously this leads to two different antiderivatives if we consider two different cases which doesn't seem right. I was told that we consider the positive number in these situations but why? And what does the negative case represent if we were to ""humor"" it? Your answers will appreciated. Thank you. Also I apologize if the question title is not descriptive of the case I'm describing. If you can think of a better title go ahead and change it or let me know, just so if anyone else looks for a similar problem have an easy time finding it?",,"['calculus', 'integration', 'derivatives']"
13,Using epsilon and delta to compute a derivative,Using epsilon and delta to compute a derivative,,"Let $a>1$, let $x\in\mathbb{Q}$, and define $f(x)=x^a$.  I am interesting in computing $f'(0)$ if it exists.  I claim that $f'(0)=0$. Attempt: Let $\epsilon > 0$.  Suppose $0 < \lvert x-0 \rvert$ = $\lvert x \rvert$ < $\delta$.  Consider $$\left\lvert \frac{f(x)-f(0)}{x-0}-L \right\rvert = \left\lvert \frac{x^a-0^a}{x-0}-0 \right\rvert = \left\lvert \frac{x^a}{x} \right\rvert = \left\lvert x^{a-1} \right\rvert$$  My initial thought was to let $\delta$ := min{1,$\epsilon$} so that $$\left\lvert x^{a-1} \right\rvert \leq \lvert x \rvert < \delta \leq \epsilon$$ however I think this fails if $1<a<2$.  Is there a way to define $\delta$ that avoids having to worry about the value of $a$ or should I break the problem into cases?","Let $a>1$, let $x\in\mathbb{Q}$, and define $f(x)=x^a$.  I am interesting in computing $f'(0)$ if it exists.  I claim that $f'(0)=0$. Attempt: Let $\epsilon > 0$.  Suppose $0 < \lvert x-0 \rvert$ = $\lvert x \rvert$ < $\delta$.  Consider $$\left\lvert \frac{f(x)-f(0)}{x-0}-L \right\rvert = \left\lvert \frac{x^a-0^a}{x-0}-0 \right\rvert = \left\lvert \frac{x^a}{x} \right\rvert = \left\lvert x^{a-1} \right\rvert$$  My initial thought was to let $\delta$ := min{1,$\epsilon$} so that $$\left\lvert x^{a-1} \right\rvert \leq \lvert x \rvert < \delta \leq \epsilon$$ however I think this fails if $1<a<2$.  Is there a way to define $\delta$ that avoids having to worry about the value of $a$ or should I break the problem into cases?",,"['real-analysis', 'derivatives']"
14,Proof that a derivative's points of discontinuity are all essential,Proof that a derivative's points of discontinuity are all essential,,"I'm reading Wikipedia's article on Darboux's theorem , and it says the following: ""Every discontinuity of a Darboux function is essential, that is, at any point of discontinuity, at least one of the left hand and right hand limits does not exist."" I've tried to look it up and all I see is that this is a corollary of Darboux's theorem. Could someone please clarify how exactly it follows?","I'm reading Wikipedia's article on Darboux's theorem , and it says the following: ""Every discontinuity of a Darboux function is essential, that is, at any point of discontinuity, at least one of the left hand and right hand limits does not exist."" I've tried to look it up and all I see is that this is a corollary of Darboux's theorem. Could someone please clarify how exactly it follows?",,"['derivatives', 'continuity']"
15,-relationship between a function and a tangent line,-relationship between a function and a tangent line,,"$f: \mathbb{R} \rightarrow \mathbb{R}$ a continuous function at $x=a$. Show that $f$ has derivate at $x=a$ iff there's only a $L(x) = m(x-a)+b $ such that  $$ \lim_{x \to a}\frac{f(x)-L(x)}{x-a} = 0 $$ $\Longrightarrow  f $ has derivate at $x=a$ and is continuous at $x=a$ then we can consider $L(x)= f'(a)(x-a)+f(a)$ that line works. $\Longleftarrow$ we know $f$ is continuous at $x=a$and there's just a line such that $$ \lim_{x \to a}\frac{f(x)-L(x)}{x-a} = 0 $$ so  $$ \lim_{x \to a}\frac{f(x)-f(a)-(f(a)-L(x))}{x-a} = 0 $$ $$ \lim_{x \to a}\frac{f(x)-f(a)}{x-a}-\lim_{x \to a}\frac{f(a)-L(x)}{x-a} = 0 $$ then I have to see $$ \lim_{x \to a}\frac{f(a)-L(x)}{x-a} = l , l\in \mathbb{R}$$ I don't know what to do next?? some help? a Solution : we know $ \lim_{x \to a}f(x) = f(a) $ (continuous function at $x=a$) and there's only a $L(x) = m(x-a)+b $ (the only one )such that  $$ \lim_{x \to a}\frac{f(x)-L(x)}{x-a} = 0 $$ then  $ \lim_{x \to a}f(x)-L(x) = f(a)-b$ (here used ""continuous function at $x=a$"" )  and  $$\lim_{x \to a}f(x)-L(x)= \lim_{x \to a}\frac{f(x)-L(x)}{(x-a)} (x-a) = \lim_{x \to a}\frac{f(x)-L(x)}{x-a} \lim_{x \to a}(x-a) = 0 $$ at the last used $ \lim_{x \to a}\frac{f(x)-L(x)}{x-a} = 0 $,then It's necessary $b=f(a)$, then again $$ 0=\lim_{x \to a}\frac{f(x)-L(x)}{x-a}= \lim_{x \to a}\frac{f(x)-m(x-a)-f(a)}{x-a}=\lim_{x \to a}\frac{f(x)-f(a)}{x-a} - m $$ then exists $f'(a)$","$f: \mathbb{R} \rightarrow \mathbb{R}$ a continuous function at $x=a$. Show that $f$ has derivate at $x=a$ iff there's only a $L(x) = m(x-a)+b $ such that  $$ \lim_{x \to a}\frac{f(x)-L(x)}{x-a} = 0 $$ $\Longrightarrow  f $ has derivate at $x=a$ and is continuous at $x=a$ then we can consider $L(x)= f'(a)(x-a)+f(a)$ that line works. $\Longleftarrow$ we know $f$ is continuous at $x=a$and there's just a line such that $$ \lim_{x \to a}\frac{f(x)-L(x)}{x-a} = 0 $$ so  $$ \lim_{x \to a}\frac{f(x)-f(a)-(f(a)-L(x))}{x-a} = 0 $$ $$ \lim_{x \to a}\frac{f(x)-f(a)}{x-a}-\lim_{x \to a}\frac{f(a)-L(x)}{x-a} = 0 $$ then I have to see $$ \lim_{x \to a}\frac{f(a)-L(x)}{x-a} = l , l\in \mathbb{R}$$ I don't know what to do next?? some help? a Solution : we know $ \lim_{x \to a}f(x) = f(a) $ (continuous function at $x=a$) and there's only a $L(x) = m(x-a)+b $ (the only one )such that  $$ \lim_{x \to a}\frac{f(x)-L(x)}{x-a} = 0 $$ then  $ \lim_{x \to a}f(x)-L(x) = f(a)-b$ (here used ""continuous function at $x=a$"" )  and  $$\lim_{x \to a}f(x)-L(x)= \lim_{x \to a}\frac{f(x)-L(x)}{(x-a)} (x-a) = \lim_{x \to a}\frac{f(x)-L(x)}{x-a} \lim_{x \to a}(x-a) = 0 $$ at the last used $ \lim_{x \to a}\frac{f(x)-L(x)}{x-a} = 0 $,then It's necessary $b=f(a)$, then again $$ 0=\lim_{x \to a}\frac{f(x)-L(x)}{x-a}= \lim_{x \to a}\frac{f(x)-m(x-a)-f(a)}{x-a}=\lim_{x \to a}\frac{f(x)-f(a)}{x-a} - m $$ then exists $f'(a)$",,"['calculus', 'derivatives']"
16,Proving uniform continuity of function of two variables.,Proving uniform continuity of function of two variables.,,"Proving uniform continuity of function:$$f(x,y)=\begin{cases} \frac{x^3-xy}{x^2+y^2},  & (x,y)\neq (0,0) \\ 0, & (x,y)=(0,0) \end{cases}$$ This is supposedly solve, but I don't understand the solution. It is proved that it is continuous and differentiable (asked in sub questions, but might be important for overall idea.) After this, it is proved that $\frac{\partial f}{\partial x}(x,y)\leq 3$ and also $\frac{\partial f}{\partial y}(x,y)\leq 2$. Then it goes onto say: $$|f(x)-f(y)|\leq k \|x-y\|\\    |f(x)-f(y)|\leq |f(x)-f(z)|+|f(z)-f(y)| \leq k \|x-z\|+ k \|z-y\|\\    \leq k(\|x-z\|+ \|z-y\|)...$$ Does this make any sense? Im interested in this variation of the answer...","Proving uniform continuity of function:$$f(x,y)=\begin{cases} \frac{x^3-xy}{x^2+y^2},  & (x,y)\neq (0,0) \\ 0, & (x,y)=(0,0) \end{cases}$$ This is supposedly solve, but I don't understand the solution. It is proved that it is continuous and differentiable (asked in sub questions, but might be important for overall idea.) After this, it is proved that $\frac{\partial f}{\partial x}(x,y)\leq 3$ and also $\frac{\partial f}{\partial y}(x,y)\leq 2$. Then it goes onto say: $$|f(x)-f(y)|\leq k \|x-y\|\\    |f(x)-f(y)|\leq |f(x)-f(z)|+|f(z)-f(y)| \leq k \|x-z\|+ k \|z-y\|\\    \leq k(\|x-z\|+ \|z-y\|)...$$ Does this make any sense? Im interested in this variation of the answer...",,"['calculus', 'real-analysis']"
17,Rigorous treatment of expressions with differentials in physics books,Rigorous treatment of expressions with differentials in physics books,,"The question is rather general, but let me give a specific example. In thermal physics we have the following identity involving differentials: $$T\,dS = dE + p\,dV$$ where $T$ = temperature, $S$ = entropy, $E$ = internal energy, $p$ = pressure, $V$ = volume of a system. In many physics books on thermal physics one can find transformations like this: $$T\,dS = dE + p\,dV \iff dE = T\,dS - p\,dV$$ It's not clear for me why we can do such transformations. For me $$T\,dS = dE + p\,dV \iff dS = \frac{1}{T}\,dE + \frac{p}{T}\,dV$$ means that for each point $(E, V)$ $dS(E, V)$ is a linear functional with the matrix representation $$\begin{bmatrix}\frac{1}{T(E, V)} & \frac{p(E, V)}{T(E, V)}\end{bmatrix}$$ which maps a two-dimensional vector $\begin{bmatrix}\Delta E \\ \Delta V\end{bmatrix}$ to a scalar value $\Delta S$. $dE, dV$ in this interpretation are just syntactic elements, so it's unclear how physicists do their tricks. The question is: how can I rigorously interpret physics books tricks like $$T\,dS = dE + p\,dV \iff dE = T\,dS - p\,dV$$?","The question is rather general, but let me give a specific example. In thermal physics we have the following identity involving differentials: $$T\,dS = dE + p\,dV$$ where $T$ = temperature, $S$ = entropy, $E$ = internal energy, $p$ = pressure, $V$ = volume of a system. In many physics books on thermal physics one can find transformations like this: $$T\,dS = dE + p\,dV \iff dE = T\,dS - p\,dV$$ It's not clear for me why we can do such transformations. For me $$T\,dS = dE + p\,dV \iff dS = \frac{1}{T}\,dE + \frac{p}{T}\,dV$$ means that for each point $(E, V)$ $dS(E, V)$ is a linear functional with the matrix representation $$\begin{bmatrix}\frac{1}{T(E, V)} & \frac{p(E, V)}{T(E, V)}\end{bmatrix}$$ which maps a two-dimensional vector $\begin{bmatrix}\Delta E \\ \Delta V\end{bmatrix}$ to a scalar value $\Delta S$. $dE, dV$ in this interpretation are just syntactic elements, so it's unclear how physicists do their tricks. The question is: how can I rigorously interpret physics books tricks like $$T\,dS = dE + p\,dV \iff dE = T\,dS - p\,dV$$?",,"['derivatives', 'physics']"
18,Problem with finding Maximum value,Problem with finding Maximum value,,"My problem states: Show that y: \begin{equation} y = e^{-t}sin(2t) \end{equation} is a maximum when \begin{equation} t = \frac{1}{2}\tan^{-1}(2) \end{equation} and determine this maximum value. So basically i have to calculate first and second derivatives of function f(t) = y and find some relation with what i must prove. Well, i've almost done it i think. But not quite. Can someone help me by checking out my answer a bit and tell me where i'm wrong, what i should improve, anything that could help me really? So first i calculate first derivative: \begin{equation} \frac{dy}{dt} = -e^{-t}\sin(2t) + 2e^{-t}\cos(2t) \end{equation} and i convert the result to a more simplified form \begin{equation} \sqrt{5}e^{-t}\cos(2t-0.464) \end{equation} which i equate to 0 to find t(which i can't find it here). And then i calculate the second derivative: \begin{equation} \frac{d^{2}y}{dt^{2}} = -\sqrt{5}e^{-t}\cos(2t-0.464)-2\sqrt{5}e^{-t}\sin(2t-0.464) \end{equation} which ""simplifies"" to: \begin{equation} 5e^{-t}\cos(2(t-\frac{tan^{-1}(2)}{2})-0.4636) \end{equation} I doubt i'm right so far but if i am then i'm close. However i don't see exactly how i can prove what is needed from here. If someone could help me a bit i would be most grateful. Thank you in advance.","My problem states: Show that y: \begin{equation} y = e^{-t}sin(2t) \end{equation} is a maximum when \begin{equation} t = \frac{1}{2}\tan^{-1}(2) \end{equation} and determine this maximum value. So basically i have to calculate first and second derivatives of function f(t) = y and find some relation with what i must prove. Well, i've almost done it i think. But not quite. Can someone help me by checking out my answer a bit and tell me where i'm wrong, what i should improve, anything that could help me really? So first i calculate first derivative: \begin{equation} \frac{dy}{dt} = -e^{-t}\sin(2t) + 2e^{-t}\cos(2t) \end{equation} and i convert the result to a more simplified form \begin{equation} \sqrt{5}e^{-t}\cos(2t-0.464) \end{equation} which i equate to 0 to find t(which i can't find it here). And then i calculate the second derivative: \begin{equation} \frac{d^{2}y}{dt^{2}} = -\sqrt{5}e^{-t}\cos(2t-0.464)-2\sqrt{5}e^{-t}\sin(2t-0.464) \end{equation} which ""simplifies"" to: \begin{equation} 5e^{-t}\cos(2(t-\frac{tan^{-1}(2)}{2})-0.4636) \end{equation} I doubt i'm right so far but if i am then i'm close. However i don't see exactly how i can prove what is needed from here. If someone could help me a bit i would be most grateful. Thank you in advance.",,['derivatives']
19,find the total differential of this equation $ xyz + \sqrt{ x^2 + y^2 + z^2} = \sqrt 2 $,find the total differential of this equation, xyz + \sqrt{ x^2 + y^2 + z^2} = \sqrt 2 ,"How to calculate the total differential of $ z= z(x,y)$, which is  $ xyz + \sqrt{ x^2  + y^2 + z^2} = \sqrt 2 $ at point (1, 0, -1)？ The evaluation of mine seems wrong, $ dz= \frac{\partial z}{\partial x} dx + \frac{\partial z}{\partial y}  dy = (yz+ \frac{x}{\sqrt{x^2  + y^2 + z^2}})dx + (xz+ \frac{y}{\sqrt{x^2  + y^2 + z^2}})dy  = \frac{dx}{\sqrt2} - dy $ Appreciate any helps.","How to calculate the total differential of $ z= z(x,y)$, which is  $ xyz + \sqrt{ x^2  + y^2 + z^2} = \sqrt 2 $ at point (1, 0, -1)？ The evaluation of mine seems wrong, $ dz= \frac{\partial z}{\partial x} dx + \frac{\partial z}{\partial y}  dy = (yz+ \frac{x}{\sqrt{x^2  + y^2 + z^2}})dx + (xz+ \frac{y}{\sqrt{x^2  + y^2 + z^2}})dy  = \frac{dx}{\sqrt2} - dy $ Appreciate any helps.",,"['calculus', 'derivatives', 'partial-differential-equations']"
20,Proving that a polilinear operator is differentiable,Proving that a polilinear operator is differentiable,,"A Polilinear map operator is $P:X^1 \times ... \times X^n \to Y$ such that the foolowing applies: $\lambda, \mu \in R$ $$ P( \lambda x_1^1 + \mu x_2 ^1, x^2,...,x^n)= \lambda P(x_1^1,x^2,...x^n)+ \mu P(x_2^1,x^2,...x^n) \\...... \\\ ...... \\ P(x^1, x^2,...,\lambda x_1^n+ \mu x_2^n)= \lambda P(x^1,x^2,...x_1^n)+ \mu P(x^1,x^2,...x_2^n)$$ My definition of differentiability: Let $X$ and $Y$ be normed vector spaces upon the same field $\mathbb R$ or $\mathbb C$ and $U$ an open set in $X$. For a function $f:U \to Y$ it is said to be differentiable in point $x \in U$ if there exists a continuous linear map $A_x:X \to Y$ such that: $$f(x+h)-f(x)=A_xh+R(h)$$ where $$\lim_{h \to 0}\frac{R(h)}{\|h\|}=0. \text{ or } R(h)=o(h)$$ Now to this example and my question:$$P(x^1+h^1,...,x^n+h^n)-P(x^1,...,x^n)=P(h^1,x^2,...,x^n)+...+P(x^1,...,x^{n-1},h^n)+ \sum P(y^1,...,y^n)\leftarrow\text{In this last sum } y^i=x^i \text{ or }y^i=h^i \text{ where }\\ y^i=h^i \text{ is at least for two indexes }i=1,...,n$$ Now it says that : $P(h^1,x^2,...,x^n)+...+P(x^1,...,x^{n-1},h^n)$is linear which is clear why and then it says: $$\| \sum P(y^1,...,y^n)  \|\leq \sum \|P\| \|y^1 \|...\|y^n \| \leq M \|P\|\|h\|^2\|x\|^{n-2}, M=2^n-n-1 \implies P(y_1,...,y_n)=o(h)$$ My question and confusion now that I have is the following: It seems I can make many different functions that are linear and have $R(h)=o(h)$ for example: $A_x=0$ and $$R(h)=P(y^1,...,y^n)\leftarrow\text{In this last sum } y^i=x^i \text{ or }y^i=h^i \text{ where }\\ y^i=h^i \text{ is at least for on index }i=1,...,n$$ $$\| \sum P(y^1,...,y^n)  \|\leq \sum \|P\| \|y^1 \|...\|y^n \| \leq M \|P\|\|h\|\|x\|^{n-1}, M=2^n \implies P(y_1,...,y_n)=o(h)$$ It is not unique, which it should be, or am I not understanding the idea. Any help?","A Polilinear map operator is $P:X^1 \times ... \times X^n \to Y$ such that the foolowing applies: $\lambda, \mu \in R$ $$ P( \lambda x_1^1 + \mu x_2 ^1, x^2,...,x^n)= \lambda P(x_1^1,x^2,...x^n)+ \mu P(x_2^1,x^2,...x^n) \\...... \\\ ...... \\ P(x^1, x^2,...,\lambda x_1^n+ \mu x_2^n)= \lambda P(x^1,x^2,...x_1^n)+ \mu P(x^1,x^2,...x_2^n)$$ My definition of differentiability: Let $X$ and $Y$ be normed vector spaces upon the same field $\mathbb R$ or $\mathbb C$ and $U$ an open set in $X$. For a function $f:U \to Y$ it is said to be differentiable in point $x \in U$ if there exists a continuous linear map $A_x:X \to Y$ such that: $$f(x+h)-f(x)=A_xh+R(h)$$ where $$\lim_{h \to 0}\frac{R(h)}{\|h\|}=0. \text{ or } R(h)=o(h)$$ Now to this example and my question:$$P(x^1+h^1,...,x^n+h^n)-P(x^1,...,x^n)=P(h^1,x^2,...,x^n)+...+P(x^1,...,x^{n-1},h^n)+ \sum P(y^1,...,y^n)\leftarrow\text{In this last sum } y^i=x^i \text{ or }y^i=h^i \text{ where }\\ y^i=h^i \text{ is at least for two indexes }i=1,...,n$$ Now it says that : $P(h^1,x^2,...,x^n)+...+P(x^1,...,x^{n-1},h^n)$is linear which is clear why and then it says: $$\| \sum P(y^1,...,y^n)  \|\leq \sum \|P\| \|y^1 \|...\|y^n \| \leq M \|P\|\|h\|^2\|x\|^{n-2}, M=2^n-n-1 \implies P(y_1,...,y_n)=o(h)$$ My question and confusion now that I have is the following: It seems I can make many different functions that are linear and have $R(h)=o(h)$ for example: $A_x=0$ and $$R(h)=P(y^1,...,y^n)\leftarrow\text{In this last sum } y^i=x^i \text{ or }y^i=h^i \text{ where }\\ y^i=h^i \text{ is at least for on index }i=1,...,n$$ $$\| \sum P(y^1,...,y^n)  \|\leq \sum \|P\| \|y^1 \|...\|y^n \| \leq M \|P\|\|h\|\|x\|^{n-1}, M=2^n \implies P(y_1,...,y_n)=o(h)$$ It is not unique, which it should be, or am I not understanding the idea. Any help?",,"['calculus', 'real-analysis', 'derivatives', 'differential-topology']"
21,Folding a paper such that the size of one sides be as minimum as possible?,Folding a paper such that the size of one sides be as minimum as possible?,,Suppose that we have an A4 paper like this: How to fold this paper such that the bottom-right corner overlap the left edge of the paper and that the size of AB side be as minimum as possible. It should be noted that the size of a typical A4 paper is 210*297mm.,Suppose that we have an A4 paper like this: How to fold this paper such that the bottom-right corner overlap the left edge of the paper and that the size of AB side be as minimum as possible. It should be noted that the size of a typical A4 paper is 210*297mm.,,"['geometry', 'derivatives', 'optimization']"
22,How to find the sum of distances so that it is minimal?,How to find the sum of distances so that it is minimal?,,"Question: $A$ and $B$ are two points on the same side of a line $l$. Denote the orthogonal projections of $A$ and $B$ onto $l$ by $A^\prime$ and $B^\prime$. Suppose that the following distance are given: $d(A,A^\prime) = 5$, $d(B,B^\prime) = 4$ and $d(A^\prime,B^\prime) = 10$. Find a point on l such that the sum $d(A,C) + d(B,C)$ is minimal. I have never seen a problem like this before, can someone help me solve this please? So I computed the sum d(A,C) + d(B,C), took the derivative and solved for C to get 1/2, however Im unsure if this is correct, could someone solve for C and tell me if I'm right?","Question: $A$ and $B$ are two points on the same side of a line $l$. Denote the orthogonal projections of $A$ and $B$ onto $l$ by $A^\prime$ and $B^\prime$. Suppose that the following distance are given: $d(A,A^\prime) = 5$, $d(B,B^\prime) = 4$ and $d(A^\prime,B^\prime) = 10$. Find a point on l such that the sum $d(A,C) + d(B,C)$ is minimal. I have never seen a problem like this before, can someone help me solve this please? So I computed the sum d(A,C) + d(B,C), took the derivative and solved for C to get 1/2, however Im unsure if this is correct, could someone solve for C and tell me if I'm right?",,"['calculus', 'geometry', 'derivatives', 'optimization', 'euclidean-geometry']"
23,"Proof of Liouville's formula , details and confusions. [Matrices, determinants..]","Proof of Liouville's formula , details and confusions. [Matrices, determinants..]",,"So I've got the homogeneous linear equation: $$x^{(n)}+a_1(t)x^{(n-1)}+...+a_{n-1}(t)x'+a_n(t)x=0.$$ where $a_1(t)...a_n(t)$ are real continuous on intervals. This is what my textbook states: If $x_1(t)...x_n(t)$ are solutions to the homogeneous equation and the Wronski matrix being $$W(t)= \begin{vmatrix} x_1 & x_2 & ... & x_n \\ x_1' & x_2' & ... & x_n' \\...&...&...& ...\\x_1^{(n-1)} & x_2^{(n-1)} & ... & x_n^{(n-1)}  \end{vmatrix} $$ Now we are trying to get Liouville's formula. $$W(t)=W(t_0)\exp({-\int_{t_0}^{t}a_1(c)dc})$$ Starts of by stating this (which I need clarification as to why,the yellow is unclear to me) Differentiating $W(t)$ we have: $$W'(t)=W_1'(t)+...+W_n'(t).$$ Where  $W_i(t)$ is the determinant that is formed when from $W(t)$ this  $i^{th}$ row is differentiated, if we notice: $W_i'(t)=0,  i=1,2,3...,n-1$ We get $$W'(t)=W_n'(t)$$","So I've got the homogeneous linear equation: $$x^{(n)}+a_1(t)x^{(n-1)}+...+a_{n-1}(t)x'+a_n(t)x=0.$$ where $a_1(t)...a_n(t)$ are real continuous on intervals. This is what my textbook states: If $x_1(t)...x_n(t)$ are solutions to the homogeneous equation and the Wronski matrix being $$W(t)= \begin{vmatrix} x_1 & x_2 & ... & x_n \\ x_1' & x_2' & ... & x_n' \\...&...&...& ...\\x_1^{(n-1)} & x_2^{(n-1)} & ... & x_n^{(n-1)}  \end{vmatrix} $$ Now we are trying to get Liouville's formula. $$W(t)=W(t_0)\exp({-\int_{t_0}^{t}a_1(c)dc})$$ Starts of by stating this (which I need clarification as to why,the yellow is unclear to me) Differentiating $W(t)$ we have: $$W'(t)=W_1'(t)+...+W_n'(t).$$ Where  $W_i(t)$ is the determinant that is formed when from $W(t)$ this  $i^{th}$ row is differentiated, if we notice: $W_i'(t)=0,  i=1,2,3...,n-1$ We get $$W'(t)=W_n'(t)$$",,"['calculus', 'linear-algebra', 'matrices', 'ordinary-differential-equations', 'derivatives']"
24,Calculating the Lie algebra representation of the regular representation on subspace of functions on $\mathbb R$.,Calculating the Lie algebra representation of the regular representation on subspace of functions on .,\mathbb R,"Let $G = \mathbb R$ and let $\pi$ be the regular representation of $G$ on $L^2(\mathbb R)$, that is, $\pi(g)(f)(x) = f(x-g)$ for $g \in G$. Let $V = \{f \in \mathcal C_c^\infty | supp f \subseteq [0,1]\}$. I am a bit confused at how to calculate the corresponding representation of the Lie algebra $\mathfrak g$ of $G$. That should be for $X \in \mathfrak g$: $(Xf)(y) = \frac{d}{dt}|_{t=0} (\pi(exp(tX))f))(y) = [f'(y-e^{tX}) \cdot (-e^{tX} X)] _{t=0} = -f'(y-1) X$.  But if this were correct, it wouldnt make sense, since now the support has moved and the above $V$ is supposed to be invariant under $\mathfrak g$. Where do I go wrong?","Let $G = \mathbb R$ and let $\pi$ be the regular representation of $G$ on $L^2(\mathbb R)$, that is, $\pi(g)(f)(x) = f(x-g)$ for $g \in G$. Let $V = \{f \in \mathcal C_c^\infty | supp f \subseteq [0,1]\}$. I am a bit confused at how to calculate the corresponding representation of the Lie algebra $\mathfrak g$ of $G$. That should be for $X \in \mathfrak g$: $(Xf)(y) = \frac{d}{dt}|_{t=0} (\pi(exp(tX))f))(y) = [f'(y-e^{tX}) \cdot (-e^{tX} X)] _{t=0} = -f'(y-1) X$.  But if this were correct, it wouldnt make sense, since now the support has moved and the above $V$ is supposed to be invariant under $\mathfrak g$. Where do I go wrong?",,"['derivatives', 'representation-theory', 'lie-groups', 'lie-algebras']"
25,Problem regarding polynomials and partial derivatives,Problem regarding polynomials and partial derivatives,,Let $P:\mathbb{R}^n\rightarrow\mathbb{R}$ be the homogeneous polynomial of degree $k$: $$P(x)=\sum_{|a|=k}c_{\alpha}x^{\alpha}$$ How can I show: $\partial^{\beta}P(x)=\beta !c_{\beta}$ for all $x\in\mathbb{R}^n$ and all multi-indices $\beta\in\mathbb{N}^n$ with $|\beta |=k$?,Let $P:\mathbb{R}^n\rightarrow\mathbb{R}$ be the homogeneous polynomial of degree $k$: $$P(x)=\sum_{|a|=k}c_{\alpha}x^{\alpha}$$ How can I show: $\partial^{\beta}P(x)=\beta !c_{\beta}$ for all $x\in\mathbb{R}^n$ and all multi-indices $\beta\in\mathbb{N}^n$ with $|\beta |=k$?,,"['calculus', 'derivatives', 'polynomials', 'partial-derivative']"
26,Trouble differentiating $\int_1^{x^3}\arcsin(t)dt$,Trouble differentiating,\int_1^{x^3}\arcsin(t)dt,I'm having trouble with an integral problem which goes like this: Differentiate $$\int_1^{x^3}\arcsin(t)dt$$ The rule I know would be that you make $t$ equal to $x^3$ and then use the chain rule to achieve: $$ 3x^2\arcsin(x^3)$$ But the answer says that it is actually: $$ 3x^2\arcsin(3x^3)$$ Why is this?,I'm having trouble with an integral problem which goes like this: Differentiate $$\int_1^{x^3}\arcsin(t)dt$$ The rule I know would be that you make $t$ equal to $x^3$ and then use the chain rule to achieve: $$ 3x^2\arcsin(x^3)$$ But the answer says that it is actually: $$ 3x^2\arcsin(3x^3)$$ Why is this?,,"['derivatives', 'definite-integrals']"
27,Find the equation of the parabola with two points and a slope,Find the equation of the parabola with two points and a slope,,"find the equation of parabola with given two points B (2, 1) and C (4, 3) and slope of the tangent line to the parabola matches the slope of the line goes through A (0, 1.5) and B (2, 1) . i have calculated, that the slope for the line is -1/4. is it correct? but i have no idea what the next step should be. Any help would be appreciated.","find the equation of parabola with given two points B (2, 1) and C (4, 3) and slope of the tangent line to the parabola matches the slope of the line goes through A (0, 1.5) and B (2, 1) . i have calculated, that the slope for the line is -1/4. is it correct? but i have no idea what the next step should be. Any help would be appreciated.",,"['calculus', 'derivatives']"
28,Need help with understanting Product Rule for Differentiation example from Math for Economists Textbook,Need help with understanting Product Rule for Differentiation example from Math for Economists Textbook,,"So I'm studying Essential Mathematics for Economic Analysis (Sydsæter, Hammond; page 180) now and stumbled upon this example aimed at illustrating why the Product rule for differentiation works the way it works. I have no problem understanding the formal proof which is given after that example, but the latter drives me crazy. I would really appreciate if someone helped me with the questions I have. Here's a screenshot from the textbook. Okay, so we have a formula for the revenue R(P)=P*D(P). Let's say P increases by one dollar and the R changes. That's comprehensible. Why does R increase? Well, R(P) increases by 1*D(P), 'cause each of the D(P) units brings in an extra dollar. But D(P) is also a function and a change in P by one dollar changes the D like: D(P+1)-D(P), which is close to D'(P) That's also more or less clear. But after that I am completely lost. The (positive) loss due to a one dollar increase in the price per unit is then−P*D'(P), which must be subtracted from D(P) to obtain R'(P), as in equation. Why do we have have positive loss? What does it even mean? And why is this loss -P*D'(P)? Why negative? And why do we have to subtract it from D(P) to obtain R'(P)? I know that the derivative of R(P)=P D(P) is R'(P)=D(P)+P D'(P). But if I didn't know the derivative why would I subtract this ""positive loss"" from D(P) to obtain R'(P)? I hope somebody would be kind enough to answer my ignorant questions. Thanks! :)","So I'm studying Essential Mathematics for Economic Analysis (Sydsæter, Hammond; page 180) now and stumbled upon this example aimed at illustrating why the Product rule for differentiation works the way it works. I have no problem understanding the formal proof which is given after that example, but the latter drives me crazy. I would really appreciate if someone helped me with the questions I have. Here's a screenshot from the textbook. Okay, so we have a formula for the revenue R(P)=P*D(P). Let's say P increases by one dollar and the R changes. That's comprehensible. Why does R increase? Well, R(P) increases by 1*D(P), 'cause each of the D(P) units brings in an extra dollar. But D(P) is also a function and a change in P by one dollar changes the D like: D(P+1)-D(P), which is close to D'(P) That's also more or less clear. But after that I am completely lost. The (positive) loss due to a one dollar increase in the price per unit is then−P*D'(P), which must be subtracted from D(P) to obtain R'(P), as in equation. Why do we have have positive loss? What does it even mean? And why is this loss -P*D'(P)? Why negative? And why do we have to subtract it from D(P) to obtain R'(P)? I know that the derivative of R(P)=P D(P) is R'(P)=D(P)+P D'(P). But if I didn't know the derivative why would I subtract this ""positive loss"" from D(P) to obtain R'(P)? I hope somebody would be kind enough to answer my ignorant questions. Thanks! :)",,"['calculus', 'algebra-precalculus', 'derivatives']"
29,"$x = \sec 2y$, Find $\dfrac {dy}{dx}$ in terms of $x$. What about $\pm$?",", Find  in terms of . What about ?",x = \sec 2y \dfrac {dy}{dx} x \pm,$$\dfrac {dx}{dy} = 2\sec 2y \tan 2y = 2x \tan 2y$$ $$=> \tan^2 2y = sec^2 2y -1$$ $$=> \tan 2y = \pm \sqrt {x^2 -1}$$ $$=> \dfrac {dy}{dx} = \pm \dfrac 1 {2x\sqrt{x^2 -1}}$$ But the solution paper specifically states $$=> \dfrac {dy}{dx} = \dfrac 1 {2x\sqrt{x^2 -1}}$$ There were no range given in the question. So why drop negative differential?,$$\dfrac {dx}{dy} = 2\sec 2y \tan 2y = 2x \tan 2y$$ $$=> \tan^2 2y = sec^2 2y -1$$ $$=> \tan 2y = \pm \sqrt {x^2 -1}$$ $$=> \dfrac {dy}{dx} = \pm \dfrac 1 {2x\sqrt{x^2 -1}}$$ But the solution paper specifically states $$=> \dfrac {dy}{dx} = \dfrac 1 {2x\sqrt{x^2 -1}}$$ There were no range given in the question. So why drop negative differential?,,"['algebra-precalculus', 'derivatives']"
30,"If $f(x) = \frac{\sin^{-1} x}{\sqrt{1- x ^2}}$, then evaluate $(1-x^2)f''(x) - xf(x)$","If , then evaluate",f(x) = \frac{\sin^{-1} x}{\sqrt{1- x ^2}} (1-x^2)f''(x) - xf(x),"$f(x) = \dfrac{\sin^{-1} x}{\sqrt{1- x ^2}}$ Differentiating the given function, we get $f'(x) = \dfrac{1 + \dfrac{x\sin^{-1}x}{\sqrt{1-x^2}}}{1-x^2}$ which can also be written as $f'(x) = \dfrac{1 + xf(x)}{1-x^2}$ Differentiating this one more time $f''(x) = \dfrac{(1-x^2)[xf'(x) + f(x)] - [1 + xf(x)](-2x)}{(1-x^2)^2}$ Now the next obvious step is to substitute for $f'(x)$ since we have no $f'(x)$ term in the question. Which further simplify the equation $f''(x) = \dfrac{3x[1 + xf(x)] + (1-x^2)f(x)}{(1-x^2)^2}$ Head scratch, I have no idea how to proceed. Possibilities are I have made a mistake above, if so please point out and help me take this ahead.","$f(x) = \dfrac{\sin^{-1} x}{\sqrt{1- x ^2}}$ Differentiating the given function, we get $f'(x) = \dfrac{1 + \dfrac{x\sin^{-1}x}{\sqrt{1-x^2}}}{1-x^2}$ which can also be written as $f'(x) = \dfrac{1 + xf(x)}{1-x^2}$ Differentiating this one more time $f''(x) = \dfrac{(1-x^2)[xf'(x) + f(x)] - [1 + xf(x)](-2x)}{(1-x^2)^2}$ Now the next obvious step is to substitute for $f'(x)$ since we have no $f'(x)$ term in the question. Which further simplify the equation $f''(x) = \dfrac{3x[1 + xf(x)] + (1-x^2)f(x)}{(1-x^2)^2}$ Head scratch, I have no idea how to proceed. Possibilities are I have made a mistake above, if so please point out and help me take this ahead.",,"['calculus', 'derivatives']"
31,confusion in using Lebiniz integral rule,confusion in using Lebiniz integral rule,,"I was trying this question - Let $$f: (0,\infty )\rightarrow \mathbb{R}$$ and $$F(x) =  \int_{0}^{x}tf(t)dt$$ If $F(x^2)= x^{4} + x^{5} $, then the value of $\sum_{r=1}^{12}f(r^{2})$ is I applied chain rule for differentiation in $F(x^2)$ to get $$2xF'(x^2)=4x^3+5x^4$$ then used Leibniz rule in $F(x^2)$ to get  $$F'(x^2)=2x(x^2)f(x^2)$$  and substituted it in above equation to get $f(r^2)$ then trying to sum it up but in the solution the correct equation is $ 2x(x^2)f(x^2)= 4x^3 + 5x^4 $ but according to my method the equation for $f(r^2)$ is $2x(2x)(x^2)f(x^2)= 4x^3 + 5x^4$, what am I doing wrong?","I was trying this question - Let $$f: (0,\infty )\rightarrow \mathbb{R}$$ and $$F(x) =  \int_{0}^{x}tf(t)dt$$ If $F(x^2)= x^{4} + x^{5} $, then the value of $\sum_{r=1}^{12}f(r^{2})$ is I applied chain rule for differentiation in $F(x^2)$ to get $$2xF'(x^2)=4x^3+5x^4$$ then used Leibniz rule in $F(x^2)$ to get  $$F'(x^2)=2x(x^2)f(x^2)$$  and substituted it in above equation to get $f(r^2)$ then trying to sum it up but in the solution the correct equation is $ 2x(x^2)f(x^2)= 4x^3 + 5x^4 $ but according to my method the equation for $f(r^2)$ is $2x(2x)(x^2)f(x^2)= 4x^3 + 5x^4$, what am I doing wrong?",,"['calculus', 'integration', 'derivatives', 'definite-integrals']"
32,Trigonometric Differentiation. Height of a wave.,Trigonometric Differentiation. Height of a wave.,,"The movement of the crest of a wave is modelled with the equation $h(t)=0.3\cos 3t+0.4\sin 3t$. Find the maximum height of the wave and the time at which it occurs. I have come up until here. please tell me if I did it right or wrong and how do I find the time the maximum height is at? $$h(t) = 0.3 \cos 3t + 0.4 \sin 3t $$ To find the maximum or minimum, take the first derivative  and equate it to 0. $$\begin{align*} \frac{dh}{dt} &= -0.3*3 \sin 3 t +0.4 * 3 \cos 3t \\ \tan 3t &= 1.2/0.9 \\  3t &= 53.06\end{align*}$$ The maximum value is $$h(t) = 0.3 \cos(53.06) + 0.4 \sin(53.06)=0.487 $$ This is the maximum. Thanks in advance.","The movement of the crest of a wave is modelled with the equation $h(t)=0.3\cos 3t+0.4\sin 3t$. Find the maximum height of the wave and the time at which it occurs. I have come up until here. please tell me if I did it right or wrong and how do I find the time the maximum height is at? $$h(t) = 0.3 \cos 3t + 0.4 \sin 3t $$ To find the maximum or minimum, take the first derivative  and equate it to 0. $$\begin{align*} \frac{dh}{dt} &= -0.3*3 \sin 3 t +0.4 * 3 \cos 3t \\ \tan 3t &= 1.2/0.9 \\  3t &= 53.06\end{align*}$$ The maximum value is $$h(t) = 0.3 \cos(53.06) + 0.4 \sin(53.06)=0.487 $$ This is the maximum. Thanks in advance.",,"['trigonometry', 'derivatives']"
33,Computing wavenumbers for discrete Fourier transform,Computing wavenumbers for discrete Fourier transform,,"I'm trying to implement a Fortran program to compute the derivative of a function using the FFT. To begin with, just to test my installation of fftpack, I computed the Fourier transform of $\mathrm{sin}(x^{2})$, followed by the inverse transform of that result, which gave me back $\mathrm{sin}(x^{2})$ as expected. So once I was satisfied that the FFT routines were working, I began trying to use it to find the derivative, via the formula $$\frac{\mathrm{d}f(x)}{\mathrm{d}x}=\mathcal{F}^{-1}[ik\hat{f}(k)],$$ where $f(x) = \mathrm{sin}(x^{2})$, for $x$ between $0$ and $2\pi$. There are $N=256$ points on the mesh, so $x_{n}=2\pi n/N.$ My simple problem is, I can't work out how to compute the values of $k$. I've tried some stuff, but it resulted in garbage. I'd really appreciate any help, thanks.","I'm trying to implement a Fortran program to compute the derivative of a function using the FFT. To begin with, just to test my installation of fftpack, I computed the Fourier transform of $\mathrm{sin}(x^{2})$, followed by the inverse transform of that result, which gave me back $\mathrm{sin}(x^{2})$ as expected. So once I was satisfied that the FFT routines were working, I began trying to use it to find the derivative, via the formula $$\frac{\mathrm{d}f(x)}{\mathrm{d}x}=\mathcal{F}^{-1}[ik\hat{f}(k)],$$ where $f(x) = \mathrm{sin}(x^{2})$, for $x$ between $0$ and $2\pi$. There are $N=256$ points on the mesh, so $x_{n}=2\pi n/N.$ My simple problem is, I can't work out how to compute the values of $k$. I've tried some stuff, but it resulted in garbage. I'd really appreciate any help, thanks.",,"['derivatives', 'algorithms', 'numerical-methods', 'fourier-analysis']"
34,How do we get the last relation?,How do we get the last relation?,,"I am looking at the conservation of momentum. The force at $W$ from the tensions at the boundary $\partial{W}$ is $$\overrightarrow{S}_{\partial{W}}=-\int_{\partial{W}}p \cdot \overrightarrow{n}dA=-\int_{W}\nabla p dV$$ where $p(\overrightarrow{x}, t)$ the pressure and $\overrightarrow{n}$ the unit perpendicular vector. The massive forces is $$\overrightarrow{B}_{W}=\int_{W}\rho \overrightarrow{b}dV$$ where $\overrightarrow{b}$ the density of massive forces. So, the total force on the fluids in the volum $W$ is $$\overrightarrow{S}_{\partial{W}}+\overrightarrow{B}_{W}=\int_{W}( \rho \overrightarrow{b}-\nabla p)dV$$ From the second Newton's law we have that $\overrightarrow{F}=m\cdot \overrightarrow{a}$ and since $m=\int \rho dV$ and $\overrightarrow{a}=\frac{D\overrightarrow{u}}{Dt}$, where $\frac{D}{Dt}$ the material derivative, we have the following: $$\int_{W}\rho \frac{D\overrightarrow{u}}{Dt}dV=\overrightarrow{S}_{\partial{W}}+\overrightarrow{B}_{W}=\int_{W}(\rho \overrightarrow{b}-\nabla p)dV$$ The differential form of the conservation of momentum is $$\rho \frac{D\overrightarrow{u}}{Dt}=-\nabla p+\rho\overrightarrow{b}$$ We are looking for the integral form of the conservation of momentum. We have $$\rho \frac{\partial{\overrightarrow{u}}}{\partial{t}}=-\rho (\overrightarrow{u}\cdot \nabla )\overrightarrow{u}-\nabla p+\rho \overrightarrow{b}$$ From the differential form of the conservation of mass ($\frac{\partial{\rho}}{\partial{t}}+\nabla \cdot (\rho \overrightarrow{u})=0$) we get the following: $$\frac{\partial}{\partial{t}}(\rho \overrightarrow{u})=-div(\rho \overrightarrow{u})\overrightarrow{u}-\rho(\overrightarrow{u}\cdot \nabla)\overrightarrow{u}-\nabla p+\rho\overrightarrow{b}$$ Could you explain to me how we get the last relation??","I am looking at the conservation of momentum. The force at $W$ from the tensions at the boundary $\partial{W}$ is $$\overrightarrow{S}_{\partial{W}}=-\int_{\partial{W}}p \cdot \overrightarrow{n}dA=-\int_{W}\nabla p dV$$ where $p(\overrightarrow{x}, t)$ the pressure and $\overrightarrow{n}$ the unit perpendicular vector. The massive forces is $$\overrightarrow{B}_{W}=\int_{W}\rho \overrightarrow{b}dV$$ where $\overrightarrow{b}$ the density of massive forces. So, the total force on the fluids in the volum $W$ is $$\overrightarrow{S}_{\partial{W}}+\overrightarrow{B}_{W}=\int_{W}( \rho \overrightarrow{b}-\nabla p)dV$$ From the second Newton's law we have that $\overrightarrow{F}=m\cdot \overrightarrow{a}$ and since $m=\int \rho dV$ and $\overrightarrow{a}=\frac{D\overrightarrow{u}}{Dt}$, where $\frac{D}{Dt}$ the material derivative, we have the following: $$\int_{W}\rho \frac{D\overrightarrow{u}}{Dt}dV=\overrightarrow{S}_{\partial{W}}+\overrightarrow{B}_{W}=\int_{W}(\rho \overrightarrow{b}-\nabla p)dV$$ The differential form of the conservation of momentum is $$\rho \frac{D\overrightarrow{u}}{Dt}=-\nabla p+\rho\overrightarrow{b}$$ We are looking for the integral form of the conservation of momentum. We have $$\rho \frac{\partial{\overrightarrow{u}}}{\partial{t}}=-\rho (\overrightarrow{u}\cdot \nabla )\overrightarrow{u}-\nabla p+\rho \overrightarrow{b}$$ From the differential form of the conservation of mass ($\frac{\partial{\rho}}{\partial{t}}+\nabla \cdot (\rho \overrightarrow{u})=0$) we get the following: $$\frac{\partial}{\partial{t}}(\rho \overrightarrow{u})=-div(\rho \overrightarrow{u})\overrightarrow{u}-\rho(\overrightarrow{u}\cdot \nabla)\overrightarrow{u}-\nabla p+\rho\overrightarrow{b}$$ Could you explain to me how we get the last relation??",,"['derivatives', 'physics', 'classical-mechanics']"
35,Understanding the Definition of a derivative as slope of a tangent line,Understanding the Definition of a derivative as slope of a tangent line,,"I'm trying to understand the derivative and am wondering why the derivative is described as the slope of the tangent line and not the slope of a function itself. Say $f(x) = 2x+5$ where $\frac{d}{dx}=2$, which is the slope of $2x+5$ We would say in algebra that $2$ is the slope of $f(x)$ but in calculus we now say that it is the slope of the tangent line. Is it incorrect to say that the derivative is the slope of the function at an infinitesimally small change in $x$?","I'm trying to understand the derivative and am wondering why the derivative is described as the slope of the tangent line and not the slope of a function itself. Say $f(x) = 2x+5$ where $\frac{d}{dx}=2$, which is the slope of $2x+5$ We would say in algebra that $2$ is the slope of $f(x)$ but in calculus we now say that it is the slope of the tangent line. Is it incorrect to say that the derivative is the slope of the function at an infinitesimally small change in $x$?",,"['calculus', 'integration', 'derivatives']"
36,A really basic integration question concerning differentials,A really basic integration question concerning differentials,,"I'm really, really confused with this. Please, please help me. $$$$ My Calculus teacher taught me that the integral symbol and the differential with respect to which we are integrating are like parenthesis. He told us to think of the integral sign as an “open parenthesis” and the $dt$ as a “close parenthesis”. If we were to integrate any function of $t$, say $v(t)$, we have to put the integral sign on the left of $v(t)$, and the differential $dt$ on the right of $v(t)$ ie $\int v(t) dt$ $$$$ In physics, while while deriving equations of motion, our physics teacher did this: since v is a linear function of t, $$dv(t)/dt=a(constant)$$ $$\Rightarrow dv= a dt$$ She then simply put an integral sign on LHS and RHS to integrate and then got $$\int dv= \int a dt$$ $$v=at+C$$ But this does not fit into what my Calculus teacher had taught us. As per what he has taught:  $$ dv= a dt$$ We now have to add an integral sign and another differential on either side of the expressions in LHS and RHS respectively. Only then can we integrate.$$$$ Could somebody please explain this idea of the integral sign as an “open parenthesis” and the $dt$ as a “close parenthesis”? Please could you explain how this is applied to the physics example?","I'm really, really confused with this. Please, please help me. $$$$ My Calculus teacher taught me that the integral symbol and the differential with respect to which we are integrating are like parenthesis. He told us to think of the integral sign as an “open parenthesis” and the $dt$ as a “close parenthesis”. If we were to integrate any function of $t$, say $v(t)$, we have to put the integral sign on the left of $v(t)$, and the differential $dt$ on the right of $v(t)$ ie $\int v(t) dt$ $$$$ In physics, while while deriving equations of motion, our physics teacher did this: since v is a linear function of t, $$dv(t)/dt=a(constant)$$ $$\Rightarrow dv= a dt$$ She then simply put an integral sign on LHS and RHS to integrate and then got $$\int dv= \int a dt$$ $$v=at+C$$ But this does not fit into what my Calculus teacher had taught us. As per what he has taught:  $$ dv= a dt$$ We now have to add an integral sign and another differential on either side of the expressions in LHS and RHS respectively. Only then can we integrate.$$$$ Could somebody please explain this idea of the integral sign as an “open parenthesis” and the $dt$ as a “close parenthesis”? Please could you explain how this is applied to the physics example?",,"['calculus', 'integration', 'derivatives', 'indefinite-integrals']"
37,Maximizing profit function given cost and demand functions,Maximizing profit function given cost and demand functions,,I am given the demand function $$D(x)=10x^2 + 50x$$ and a total cost of $$C(x) = x^3 + 10x$$ where $x$ is the number of units demanded. I am asked to maximize the profit so what I did is I used the formula $$P(x) = R(x) - C(x)$$ where $R(x) = x D(x)$. Then I took the derivative of $P(x)$ and equated to 0. But I got a negative value of x more so a value less than 1 since $$P'(x) = 27x^2 + 100x -10$$ Am I right or did I do something wrong in between my process? I don't think it's logical to have a quantity which is negative and less than 1 for this kind of problem. Please help.,I am given the demand function $$D(x)=10x^2 + 50x$$ and a total cost of $$C(x) = x^3 + 10x$$ where $x$ is the number of units demanded. I am asked to maximize the profit so what I did is I used the formula $$P(x) = R(x) - C(x)$$ where $R(x) = x D(x)$. Then I took the derivative of $P(x)$ and equated to 0. But I got a negative value of x more so a value less than 1 since $$P'(x) = 27x^2 + 100x -10$$ Am I right or did I do something wrong in between my process? I don't think it's logical to have a quantity which is negative and less than 1 for this kind of problem. Please help.,,"['calculus', 'derivatives', 'proof-verification', 'optimization', 'economics']"
38,The Fourier transform of functions with compact support is differentiable.,The Fourier transform of functions with compact support is differentiable.,,"1) How can I prove that if $f(x)$ is a continous  function with compact support (let's say $f(x)=0$ $\forall x\in B(0,R)^c$), then its Fourier transform $\hat{f}(\xi)$ is differentiable? 2) Is there any counter example that $\hat{f}(\xi)$ is differentiable if $f\in C^0 (\mathbb{R})$ (without necessarely having a compact support)? Thank you!","1) How can I prove that if $f(x)$ is a continous  function with compact support (let's say $f(x)=0$ $\forall x\in B(0,R)^c$), then its Fourier transform $\hat{f}(\xi)$ is differentiable? 2) Is there any counter example that $\hat{f}(\xi)$ is differentiable if $f\in C^0 (\mathbb{R})$ (without necessarely having a compact support)? Thank you!",,"['analysis', 'derivatives', 'fourier-analysis']"
39,Computing Fréchet derivative,Computing Fréchet derivative,,"I am reading Methods in Nonlinear Analysis by Kung-Ching Chang and having trouble in obataining a Fréchet derivative in the text. For those who has the book, it is on page 37, which concern Euler elastic rod. Let  \begin{aligned} &\ddot\varphi+\lambda\sin\varphi=0 \ \ \text{in} \ \ (0, \pi) \\ &\dot \varphi(0)=\dot \varphi(\pi). \end{aligned} Also let  \begin{aligned} &X=\{u\in C^2[0,\pi]: \dot u(0)=\dot u(\pi)=0\}\\ &Y=C[0,\pi], \end{aligned} and let  $F: X \times \mathbb{R} \rightarrow Y$ be the map $$(u,\lambda) \rightarrow u''+\lambda \sin u.$$ The book claimed that $F''_{u\lambda}(0,n^2)=\cos (nu)|_{u=0}=I$, where $I$ is the identity map. First, I am not quite sure about what the notation $F''_{u\lambda}(0,n^2)$ mean, I guess it mean double partial Fréchet derivative evaluated at $(0,n^2)$, but I am not able to arrive that result. So far, I can only obtain the partial Fréchet derivative $$ F_u(u,\lambda)h \rightarrow h''+\lambda h \cos u. $$ Help and comments greatly appreciated. Thanks.","I am reading Methods in Nonlinear Analysis by Kung-Ching Chang and having trouble in obataining a Fréchet derivative in the text. For those who has the book, it is on page 37, which concern Euler elastic rod. Let  \begin{aligned} &\ddot\varphi+\lambda\sin\varphi=0 \ \ \text{in} \ \ (0, \pi) \\ &\dot \varphi(0)=\dot \varphi(\pi). \end{aligned} Also let  \begin{aligned} &X=\{u\in C^2[0,\pi]: \dot u(0)=\dot u(\pi)=0\}\\ &Y=C[0,\pi], \end{aligned} and let  $F: X \times \mathbb{R} \rightarrow Y$ be the map $$(u,\lambda) \rightarrow u''+\lambda \sin u.$$ The book claimed that $F''_{u\lambda}(0,n^2)=\cos (nu)|_{u=0}=I$, where $I$ is the identity map. First, I am not quite sure about what the notation $F''_{u\lambda}(0,n^2)$ mean, I guess it mean double partial Fréchet derivative evaluated at $(0,n^2)$, but I am not able to arrive that result. So far, I can only obtain the partial Fréchet derivative $$ F_u(u,\lambda)h \rightarrow h''+\lambda h \cos u. $$ Help and comments greatly appreciated. Thanks.",,"['analysis', 'derivatives', 'partial-differential-equations', 'partial-derivative']"
40,The series of function $f(x)=\sum_{n\geq 1}\frac{1}{n}\ln(1+\frac{x}{n})$; the convergence and the differentiability.,The series of function ; the convergence and the differentiability.,f(x)=\sum_{n\geq 1}\frac{1}{n}\ln(1+\frac{x}{n}),"Consider the series of function $f(x)=\sum_{n\geq  1}\frac{1}{n}\ln(1+\frac{x}{n})$ for $x>-1$. a) Show that the series is pointwise convergent. Answer : I actually don't know how to show it. I could show that the series is convergent for all $x>-1$. First I noticed that \begin{equation*} \frac{1}{n}\ln\left ( 1+\frac{x}{n} \right )=\ln\left ( \left ( 1+\frac{x}{n} \right )^{1/n} \right )<\left ( 1+\frac{x}{n} \right )^{1/n} \end{equation*} for all $x>-1$. Comparing the series with $\sum_{n\geq 1}\left ( 1+\frac{x}{n} \right )^{1/n}$ that diverges makes me to find other comparisons. I have not yet found a good comparison that converges. Or, if $\epsilon>0$ is given, then I am unable to find a $K>0$ such that \begin{equation*} \left | \sum_{n= 1}^{k}\frac{1}{n}\ln\left ( 1+\frac{x}{n} \right )-f(x) \right |<\epsilon \end{equation*} for all $k>K$. b) Determine the termwise differentiated series, and show that it is   uniformly convergent. Answer : Let $f_{n}(x)=\frac{1}{n}\ln\left (1+\frac{x}{n}  \right )$. Since $f_{n}(x)$ is differentiable with $f_{n}'(x)=\frac{1}{n(n+x)}$ for $x>-1$, so the termwise differentiated series is \begin{equation*} \left ( \sum_{n\geq 1} f_{n}(x) \right )'= \sum_{n\geq 1}f_{n}'(x)=\sum_{n\geq 1}\frac{1}{n(n+x)}. \end{equation*} Since, for example, $0\in (-1,\infty)$ and $\sum_{n\geq 1}f_{n}'(0)$ is convergent, then the series $\sum_{n\geq 1}f_{n}'(x)$ is uniformly convergent for all $x\in (-1,\infty)$. c) Explain that $f$ is differentiable. Answer : Isn't the problem b) enough to say that it is differentiable? Or, if $\epsilon>0$ is given, then I am unable to find a $\delta>0$ such that for all $x\in (-1,\infty)$ \begin{equation*} \left | \frac{f(x)-f(a)}{x-a}-f'(a) \right |<\epsilon\implies \left | x-a \right |<\delta. \end{equation*}","Consider the series of function $f(x)=\sum_{n\geq  1}\frac{1}{n}\ln(1+\frac{x}{n})$ for $x>-1$. a) Show that the series is pointwise convergent. Answer : I actually don't know how to show it. I could show that the series is convergent for all $x>-1$. First I noticed that \begin{equation*} \frac{1}{n}\ln\left ( 1+\frac{x}{n} \right )=\ln\left ( \left ( 1+\frac{x}{n} \right )^{1/n} \right )<\left ( 1+\frac{x}{n} \right )^{1/n} \end{equation*} for all $x>-1$. Comparing the series with $\sum_{n\geq 1}\left ( 1+\frac{x}{n} \right )^{1/n}$ that diverges makes me to find other comparisons. I have not yet found a good comparison that converges. Or, if $\epsilon>0$ is given, then I am unable to find a $K>0$ such that \begin{equation*} \left | \sum_{n= 1}^{k}\frac{1}{n}\ln\left ( 1+\frac{x}{n} \right )-f(x) \right |<\epsilon \end{equation*} for all $k>K$. b) Determine the termwise differentiated series, and show that it is   uniformly convergent. Answer : Let $f_{n}(x)=\frac{1}{n}\ln\left (1+\frac{x}{n}  \right )$. Since $f_{n}(x)$ is differentiable with $f_{n}'(x)=\frac{1}{n(n+x)}$ for $x>-1$, so the termwise differentiated series is \begin{equation*} \left ( \sum_{n\geq 1} f_{n}(x) \right )'= \sum_{n\geq 1}f_{n}'(x)=\sum_{n\geq 1}\frac{1}{n(n+x)}. \end{equation*} Since, for example, $0\in (-1,\infty)$ and $\sum_{n\geq 1}f_{n}'(0)$ is convergent, then the series $\sum_{n\geq 1}f_{n}'(x)$ is uniformly convergent for all $x\in (-1,\infty)$. c) Explain that $f$ is differentiable. Answer : Isn't the problem b) enough to say that it is differentiable? Or, if $\epsilon>0$ is given, then I am unable to find a $\delta>0$ such that for all $x\in (-1,\infty)$ \begin{equation*} \left | \frac{f(x)-f(a)}{x-a}-f'(a) \right |<\epsilon\implies \left | x-a \right |<\delta. \end{equation*}",,"['sequences-and-series', 'derivatives', 'convergence-divergence', 'epsilon-delta']"
41,Derivative of a polar coordinate equation,Derivative of a polar coordinate equation,,"I was trying to plot the polar curve: $r=\cos(2n\theta)$ ($0\leq\theta\leq 2\pi$) and tried differentiating with respect to $\theta$ to get some information about where the petals would be. My reasoning was along the lines of: since when $\frac{\text{d}y}{\text{d}x}=0$ the function is flat, or the change in $y$ is becoming zero; so surely when $\frac{\text{d}r}{\text{d}\theta}=0$ the change is $r$ is becoming zero (around the particular $\theta$ that makes $r'(\theta)=0$), or it would be similar to a circle of suitable radius (since if $r(\theta)=c$ then $r'(\theta)=0\; \forall \theta$ so $r$ doesn't change with respect to $\theta$). Now this seems to work for the equation $r(\theta)=\cos(2n\theta)$ since we would get: $$ r'(\theta)=-2n\sin(2n\theta) \text{ which is zero when } 2n\theta=k\pi \rightarrow \theta=\frac{k\pi}{2n}$$ so that would imply that for $\cos(2n\theta)$ there would be $4n$ petals, which seems to agree with what actually happens. The issue came when trying to apply the same logic to $r(\theta)=\cos(3\theta)$ since we would get that the petals should be at: $$ 3\theta=n\pi \rightarrow \theta=\frac{n\pi}{3} $$ which means that there would be 6 petals, which isn't true. So I'm wondering where the problem is in my reasoning, the whole thing was more of a guess that anything else, but I'm curious about why it doesn't work.","I was trying to plot the polar curve: $r=\cos(2n\theta)$ ($0\leq\theta\leq 2\pi$) and tried differentiating with respect to $\theta$ to get some information about where the petals would be. My reasoning was along the lines of: since when $\frac{\text{d}y}{\text{d}x}=0$ the function is flat, or the change in $y$ is becoming zero; so surely when $\frac{\text{d}r}{\text{d}\theta}=0$ the change is $r$ is becoming zero (around the particular $\theta$ that makes $r'(\theta)=0$), or it would be similar to a circle of suitable radius (since if $r(\theta)=c$ then $r'(\theta)=0\; \forall \theta$ so $r$ doesn't change with respect to $\theta$). Now this seems to work for the equation $r(\theta)=\cos(2n\theta)$ since we would get: $$ r'(\theta)=-2n\sin(2n\theta) \text{ which is zero when } 2n\theta=k\pi \rightarrow \theta=\frac{k\pi}{2n}$$ so that would imply that for $\cos(2n\theta)$ there would be $4n$ petals, which seems to agree with what actually happens. The issue came when trying to apply the same logic to $r(\theta)=\cos(3\theta)$ since we would get that the petals should be at: $$ 3\theta=n\pi \rightarrow \theta=\frac{n\pi}{3} $$ which means that there would be 6 petals, which isn't true. So I'm wondering where the problem is in my reasoning, the whole thing was more of a guess that anything else, but I'm curious about why it doesn't work.",,"['algebra-precalculus', 'derivatives', 'polar-coordinates']"
42,"$y^5 =(x+2)^4+(e^x)(ln y)−15$ finding $\frac{dy}{dx}$ at $(0,1)$",finding  at,"y^5 =(x+2)^4+(e^x)(ln y)−15 \frac{dy}{dx} (0,1)",Unsure what to do regarding the $y^5$.  Should I convert it to a $y$= function and take the $5$ root of the other side. Then differentiate? Any help would be great thanks.,Unsure what to do regarding the $y^5$.  Should I convert it to a $y$= function and take the $5$ root of the other side. Then differentiate? Any help would be great thanks.,,"['calculus', 'derivatives', 'implicit-differentiation']"
43,$\frac{d\Phi^{-1}(y)}{dy} = \frac{1}{\frac{d}{dy}[\Phi(\Phi^{-1}(y))]}$?,?,\frac{d\Phi^{-1}(y)}{dy} = \frac{1}{\frac{d}{dy}[\Phi(\Phi^{-1}(y))]},"If $\Phi(y)$ is a monotonic decreasing function is true that $$\frac{d\Phi^{-1}(y)}{dy} = \frac{1}{\Phi'(\Phi^{-1}(y))}$$ If so, how? It works for $y = \Phi(x) = e^{-x}, \quad \Phi^{-1}(y) = -log(y), \quad \frac{d\Phi^{-1}(y)}{dy} = \frac{-1}{y}, \quad $","If $\Phi(y)$ is a monotonic decreasing function is true that $$\frac{d\Phi^{-1}(y)}{dy} = \frac{1}{\Phi'(\Phi^{-1}(y))}$$ If so, how? It works for $y = \Phi(x) = e^{-x}, \quad \Phi^{-1}(y) = -log(y), \quad \frac{d\Phi^{-1}(y)}{dy} = \frac{-1}{y}, \quad $",,"['calculus', 'derivatives', 'inverse']"
44,Differentiation method for evaluating $ \sum_{n=1}^\infty \frac{n^2}{3^n} $,Differentiation method for evaluating, \sum_{n=1}^\infty \frac{n^2}{3^n} ,"I evaluated the following infinite sum (the original and broader question regarding this sum can be found at Evaluating $\sum_{n=1}^\infty \frac{n^2}{3^n} $ ). $$ \sum_{n=1}^\infty \frac{n^2}{3^n} $$ However, I'm getting the feeling that I made some mistake(s) during my evaluations. My particular concern regards the changes made in the sum's index. The following is what I did. I first defined a power series as the function f. $$ \sum_{n=1}^\infty \frac{n^2}{3^n} = \sum_{n=1}^\infty n^2 (\frac{1}{3})^n = f(\frac{1}{3}) \Rightarrow f(x) = \sum_{n=1}^\infty n^2 x^n $$ I then attempted to manipulate the sum in order to transform it through the geometric series. This is where I'm quite unsure whether I did everything correctly. One of the things I did here looks wrong to me, but somehow, I still ended up with the correct answer (which is 3/2). $$ \begin{align*} f(x) &= \sum_{n=1}^\infty n^2 x^n \\ &= x \sum_{n=1}^\infty n^2 x^{n-1} \\ &= x \frac{d}{dx} ( \sum_{n=0}^\infty n x^n ) \\ &= x \frac{d}{dx} ( x \sum_{n=0}^\infty n x^{n-1} ) \\ &= x \frac{d}{dx} ( x \frac{d}{dx} ( \sum_{n=-1}^\infty x^n ) ) \\ &= x \frac{d}{dx} ( x \frac{d}{dx} ( \frac{1}{1-x} ) ) \\ &= x \frac{d}{dx} ( x \frac{1}{(1-x)^2} ) \\ &= x \frac{d}{dx} ( \frac{x}{(1-x)^2} ) \\ &= x \frac{1+x}{(1-x)^3} \\ &= \frac{x(1+x)}{(1-x)^3} \\ &\Rightarrow f(\frac{1}{3}) = \frac{3}{2} \end{align*} $$ The main concern of mine is the transition step to the closed-form geometric series. Of course, the proper equation for a geometric series is this: $$ \sum_{n=0}^\infty x^n = \frac{1}{1-x} $$ However, what I did is this: $$ \sum_{n=-1}^\infty x^n = \frac{1}{1-x} $$ The difference here is that the starting index is -1 instead of 0. This makes my translation of the sum incorrect. And yet, I still get the correct answer. On the other hand, I've tried the correct(?) form of an infinite geometric series that starts at n=-1: $$ \sum_{n=-1}^\infty x^n = \frac{\frac{1}{x}}{1-x} = \frac{1}{x(1-x)} $$ However, this yields an incorrect answer. I'm guessing that the final index I should've had for the sum was n=0 instead of n=-1. I'm guessing I did something wrong with the index shifts caused by the derivatives? Either way, I'm not seeing it. I do realize that there are other ways to go about evaluating this sum, but I'd really like to understand this derivative method.","I evaluated the following infinite sum (the original and broader question regarding this sum can be found at Evaluating $\sum_{n=1}^\infty \frac{n^2}{3^n} $ ). $$ \sum_{n=1}^\infty \frac{n^2}{3^n} $$ However, I'm getting the feeling that I made some mistake(s) during my evaluations. My particular concern regards the changes made in the sum's index. The following is what I did. I first defined a power series as the function f. $$ \sum_{n=1}^\infty \frac{n^2}{3^n} = \sum_{n=1}^\infty n^2 (\frac{1}{3})^n = f(\frac{1}{3}) \Rightarrow f(x) = \sum_{n=1}^\infty n^2 x^n $$ I then attempted to manipulate the sum in order to transform it through the geometric series. This is where I'm quite unsure whether I did everything correctly. One of the things I did here looks wrong to me, but somehow, I still ended up with the correct answer (which is 3/2). $$ \begin{align*} f(x) &= \sum_{n=1}^\infty n^2 x^n \\ &= x \sum_{n=1}^\infty n^2 x^{n-1} \\ &= x \frac{d}{dx} ( \sum_{n=0}^\infty n x^n ) \\ &= x \frac{d}{dx} ( x \sum_{n=0}^\infty n x^{n-1} ) \\ &= x \frac{d}{dx} ( x \frac{d}{dx} ( \sum_{n=-1}^\infty x^n ) ) \\ &= x \frac{d}{dx} ( x \frac{d}{dx} ( \frac{1}{1-x} ) ) \\ &= x \frac{d}{dx} ( x \frac{1}{(1-x)^2} ) \\ &= x \frac{d}{dx} ( \frac{x}{(1-x)^2} ) \\ &= x \frac{1+x}{(1-x)^3} \\ &= \frac{x(1+x)}{(1-x)^3} \\ &\Rightarrow f(\frac{1}{3}) = \frac{3}{2} \end{align*} $$ The main concern of mine is the transition step to the closed-form geometric series. Of course, the proper equation for a geometric series is this: $$ \sum_{n=0}^\infty x^n = \frac{1}{1-x} $$ However, what I did is this: $$ \sum_{n=-1}^\infty x^n = \frac{1}{1-x} $$ The difference here is that the starting index is -1 instead of 0. This makes my translation of the sum incorrect. And yet, I still get the correct answer. On the other hand, I've tried the correct(?) form of an infinite geometric series that starts at n=-1: $$ \sum_{n=-1}^\infty x^n = \frac{\frac{1}{x}}{1-x} = \frac{1}{x(1-x)} $$ However, this yields an incorrect answer. I'm guessing that the final index I should've had for the sum was n=0 instead of n=-1. I'm guessing I did something wrong with the index shifts caused by the derivatives? Either way, I'm not seeing it. I do realize that there are other ways to go about evaluating this sum, but I'd really like to understand this derivative method.",,"['sequences-and-series', 'derivatives', 'summation']"
45,Angle between slopes of a curve,Angle between slopes of a curve,,"I am trying to understand what the change in angle of the slope of a curve means. It is hard to explain with words so here's an image that should help. The red curve has had its derivative approximated at three points. The tangent at each point is also shown in black. These slopes can be compared to one another, and using tan(z)=(m1+m2)/(1+m1*m2) we can calculate the angle between the slope at one point and the next. As shown in the image, the angle become smaller at the curve straightens out. What I'm trying to figure out is what this change in angle represents. My first instinct is that it can be thought of as the rate of change of the slope, therefore the second derivative should be analogous to the angle change. However the angles are changing due to the curvature of the curve, so I also feel like this is a sort of rate of change of curvature. If the curve was a circle, the angle would always be the same, therefore the change in curvature would be zero. Which, if any, of these two ways makes more mathematical sense? Thanks for the help.","I am trying to understand what the change in angle of the slope of a curve means. It is hard to explain with words so here's an image that should help. The red curve has had its derivative approximated at three points. The tangent at each point is also shown in black. These slopes can be compared to one another, and using tan(z)=(m1+m2)/(1+m1*m2) we can calculate the angle between the slope at one point and the next. As shown in the image, the angle become smaller at the curve straightens out. What I'm trying to figure out is what this change in angle represents. My first instinct is that it can be thought of as the rate of change of the slope, therefore the second derivative should be analogous to the angle change. However the angles are changing due to the curvature of the curve, so I also feel like this is a sort of rate of change of curvature. If the curve was a circle, the angle would always be the same, therefore the change in curvature would be zero. Which, if any, of these two ways makes more mathematical sense? Thanks for the help.",,"['derivatives', 'approximation', 'curvature']"
46,"Is function $f(x,y)=\begin{cases}(x^{2}+y^{2})(\sin(x^{2}+y^{2}))^{-1/2}, (x,y)\neq (0,0)\\0,(x,y)=(0,0)\end{cases}$ differentiable?",Is function  differentiable?,"f(x,y)=\begin{cases}(x^{2}+y^{2})(\sin(x^{2}+y^{2}))^{-1/2}, (x,y)\neq (0,0)\\0,(x,y)=(0,0)\end{cases}","Is this function differentiable at (0,0)? . $f(x,y)=\begin{cases}\frac{x^{2}+y^{2}}{\sqrt{\sin(x^{2}+y^{2})}}, (x,y)\neq (0,0)\\0,(x,y)=(0,0)\end{cases}$ \begin{align*} \lim_{h\mapsto 0} \dfrac{f(0+h,0)-f(0,0)}{h}=& \lim_{h\mapsto 0} \dfrac{\dfrac{h^{2}}{\sqrt{\sin(h^{2})}}}{h}\\ =& \  \text{not defined} \end{align*} I tried to use the definition to figure out the partial derivatives. However I simply could not make it through. It looks like that the partial derivatives doesn't exist at $(0,0)$! This was in my exam at last week. However the teacher says that it is differentiable, and I disagree. So, simply, wich one is correct?","Is this function differentiable at (0,0)? . $f(x,y)=\begin{cases}\frac{x^{2}+y^{2}}{\sqrt{\sin(x^{2}+y^{2})}}, (x,y)\neq (0,0)\\0,(x,y)=(0,0)\end{cases}$ \begin{align*} \lim_{h\mapsto 0} \dfrac{f(0+h,0)-f(0,0)}{h}=& \lim_{h\mapsto 0} \dfrac{\dfrac{h^{2}}{\sqrt{\sin(h^{2})}}}{h}\\ =& \  \text{not defined} \end{align*} I tried to use the definition to figure out the partial derivatives. However I simply could not make it through. It looks like that the partial derivatives doesn't exist at $(0,0)$! This was in my exam at last week. However the teacher says that it is differentiable, and I disagree. So, simply, wich one is correct?",,['derivatives']
47,"How to prove the limit of ""the exponential of a sequence""","How to prove the limit of ""the exponential of a sequence""",,"So given a convergent sequence $\{a_n\}_{n=1}^\infty$ with limit $a$, I'd like to prove that $$\lim_{n\to\infty} \left(1+\frac{a_n}{n}\right)^n=e^a.\quad(1)$$ Knowing that $e$ is defined by $$e=\lim_{n\to\infty} \left(1+\frac{1}{n}\right)^n,$$ the relationship in $(1)$ certainly not unintuitive, and also very useful, but how do I prove it? In the case of a constant sequence $a_n=k\;\forall n$, it's pretty straightforward as you can write $$\left(1+\frac{k}{n}\right)^n=\exp\left[\log\left(\left(1+\frac{k}{n}\right)^n\right)\right]=\exp\left[n\log\left(1+\frac{k}{n}\right)\right]=\exp\left[\frac{\log\left(1+\frac{k}{n}\right)}{1/n}\right]$$ and then taking the limit you can apply L'Hôpital's rule to differentiate the numerator and denominator separately and then get the result after a few manipulations.  But since  $$\frac{\mathrm{d}}{\mathrm{d}x} \log\left(1+\frac{f(x)}{x}\right)=\frac{xf'(x)-f(x)}{xf(x)+x²}$$ I will need to know the derivative $(a_n)'$ with respect to $n$ of the sequence, to use this approach, which is not necessarily well-defined. Is there another way to go about this?","So given a convergent sequence $\{a_n\}_{n=1}^\infty$ with limit $a$, I'd like to prove that $$\lim_{n\to\infty} \left(1+\frac{a_n}{n}\right)^n=e^a.\quad(1)$$ Knowing that $e$ is defined by $$e=\lim_{n\to\infty} \left(1+\frac{1}{n}\right)^n,$$ the relationship in $(1)$ certainly not unintuitive, and also very useful, but how do I prove it? In the case of a constant sequence $a_n=k\;\forall n$, it's pretty straightforward as you can write $$\left(1+\frac{k}{n}\right)^n=\exp\left[\log\left(\left(1+\frac{k}{n}\right)^n\right)\right]=\exp\left[n\log\left(1+\frac{k}{n}\right)\right]=\exp\left[\frac{\log\left(1+\frac{k}{n}\right)}{1/n}\right]$$ and then taking the limit you can apply L'Hôpital's rule to differentiate the numerator and denominator separately and then get the result after a few manipulations.  But since  $$\frac{\mathrm{d}}{\mathrm{d}x} \log\left(1+\frac{f(x)}{x}\right)=\frac{xf'(x)-f(x)}{xf(x)+x²}$$ I will need to know the derivative $(a_n)'$ with respect to $n$ of the sequence, to use this approach, which is not necessarily well-defined. Is there another way to go about this?",,"['sequences-and-series', 'derivatives', 'exponential-function']"
48,Solve an initial value problem using the directional derivative,Solve an initial value problem using the directional derivative,,"In my notes there is the following example of solving an initial value problem using the directional derivative. The problem is the following: $$u_t(x,t)=u_x(x,t), x \in \mathbb{R}, t>0 \\ u(x,0)=f(x), x \in \mathbb{R}$$ We do the following: $$u_t(x,t)-u_x(x,t)=0 \\ \left (u_x(x,t), u_t(x, t)\right ) \cdot \frac{(-1,1)}{\sqrt{2}}=0$$ $\overrightarrow{v}=\left (-\frac{\sqrt{2}}{2}, \frac{\sqrt{2}}{2}\right )$ Reminder : Directional Derivative:         $v=(a,b)$, unit $|v|=\sqrt{a^2+b^2}=1$         $$\frac{\partial{u}}{\partial{v}}(x_0)=\nabla u(x_0) \cdot   v=\frac{d}{dt}u(x_0+tv)|_{t=0}=(u_x(x_0), u_y(x_0)) \cdot    (a,b)=au_x+bu_y$$ $$\frac{\partial}{\partial{\overrightarrow{v}}}u(x,t)=0 \text{ When we are moving at the direction of } \overrightarrow{v}, u \text{ doesn't change. }$$ $$h(s)=u((x,t)+sv) \Rightarrow h'(s)=\nabla u((x,t)+sv) \cdot v \\ \text{ From the Mean Value Theorem we have that } \exists \xi \text{ in the intervall } 0,s : \\ h(s)=h(0)=(s-0)h'(\xi) \\ \text{ So, } u((x,t)+sv-u(x,t)=(s-0) \nabla u((x,t)+\xi v) \cdot v=0$$ So, $\exists \phi : \mathbb{R} \rightarrow \mathbb{R}$ differentiable $u(x,t)=\phi(x+t)$ For $t=0 \Rightarrow u(x,0)=\phi(x) \Rightarrow \phi(x)=f(x), \forall x \in \mathbb{R}$ $u(x,t)=f(x+t), x \in \mathbb{R}, t>0$ $$$$ Could you explain to me this method?? How did we get that $u(x,t)=\phi(x+t)$ ?? $$$$ EDIT: To solve an initial value problem as the above one, we do the following: We are looking for the unit $\overrightarrow{v}$ such that $\frac{\partial{u}}{\partial{v}} \cdot \overrightarrow{v}=0$. That means that $u$ is constant at the direction of $\overrightarrow{v}$. Is it correct so far?? I haven't understood what we do next do find the solution... Could you explain it to me??","In my notes there is the following example of solving an initial value problem using the directional derivative. The problem is the following: $$u_t(x,t)=u_x(x,t), x \in \mathbb{R}, t>0 \\ u(x,0)=f(x), x \in \mathbb{R}$$ We do the following: $$u_t(x,t)-u_x(x,t)=0 \\ \left (u_x(x,t), u_t(x, t)\right ) \cdot \frac{(-1,1)}{\sqrt{2}}=0$$ $\overrightarrow{v}=\left (-\frac{\sqrt{2}}{2}, \frac{\sqrt{2}}{2}\right )$ Reminder : Directional Derivative:         $v=(a,b)$, unit $|v|=\sqrt{a^2+b^2}=1$         $$\frac{\partial{u}}{\partial{v}}(x_0)=\nabla u(x_0) \cdot   v=\frac{d}{dt}u(x_0+tv)|_{t=0}=(u_x(x_0), u_y(x_0)) \cdot    (a,b)=au_x+bu_y$$ $$\frac{\partial}{\partial{\overrightarrow{v}}}u(x,t)=0 \text{ When we are moving at the direction of } \overrightarrow{v}, u \text{ doesn't change. }$$ $$h(s)=u((x,t)+sv) \Rightarrow h'(s)=\nabla u((x,t)+sv) \cdot v \\ \text{ From the Mean Value Theorem we have that } \exists \xi \text{ in the intervall } 0,s : \\ h(s)=h(0)=(s-0)h'(\xi) \\ \text{ So, } u((x,t)+sv-u(x,t)=(s-0) \nabla u((x,t)+\xi v) \cdot v=0$$ So, $\exists \phi : \mathbb{R} \rightarrow \mathbb{R}$ differentiable $u(x,t)=\phi(x+t)$ For $t=0 \Rightarrow u(x,0)=\phi(x) \Rightarrow \phi(x)=f(x), \forall x \in \mathbb{R}$ $u(x,t)=f(x+t), x \in \mathbb{R}, t>0$ $$$$ Could you explain to me this method?? How did we get that $u(x,t)=\phi(x+t)$ ?? $$$$ EDIT: To solve an initial value problem as the above one, we do the following: We are looking for the unit $\overrightarrow{v}$ such that $\frac{\partial{u}}{\partial{v}} \cdot \overrightarrow{v}=0$. That means that $u$ is constant at the direction of $\overrightarrow{v}$. Is it correct so far?? I haven't understood what we do next do find the solution... Could you explain it to me??",,"['ordinary-differential-equations', 'derivatives', 'partial-differential-equations']"
49,Directional derivative vs. function restriction and then derivative,Directional derivative vs. function restriction and then derivative,,"Say I have a function of two variables, and a line in the plane, and I'd like to ""take the derivative along the line"". Is this an indication to use the directional derivative, OR is it expected that I first restrict the function to the line in the plane, so that it's a function of one variable, and then take a derivative in, say, the x variable? And whichever one is the right answer, what is the other option doing then? (this specifically is in relation to a hermite finite element example; the line in question is one side of a triangle, so two points on the line are given (two of the triangle vertices); if the directional derivative is what is in fact meant, then I guess they mean to evaluate it at one of the two points.)","Say I have a function of two variables, and a line in the plane, and I'd like to ""take the derivative along the line"". Is this an indication to use the directional derivative, OR is it expected that I first restrict the function to the line in the plane, so that it's a function of one variable, and then take a derivative in, say, the x variable? And whichever one is the right answer, what is the other option doing then? (this specifically is in relation to a hermite finite element example; the line in question is one side of a triangle, so two points on the line are given (two of the triangle vertices); if the directional derivative is what is in fact meant, then I guess they mean to evaluate it at one of the two points.)",,"['derivatives', 'finite-element-method']"
50,Differentiability of an absolute function.,Differentiability of an absolute function.,,"Check the differentiability of $f(x)=x|x|$, $x$ is in $\mathbb{R}$. I know that it is differentiable when $x>0$ and $x<0$. I am not sure about the case when $x=0$. I found that as $$\lim \limits_{h\to 0}\frac{f(x+h)-f(x)}{h}$$ exists and equal to $0$. But still something seems wrong. Is function differentiable at $0$? Thank you.","Check the differentiability of $f(x)=x|x|$, $x$ is in $\mathbb{R}$. I know that it is differentiable when $x>0$ and $x<0$. I am not sure about the case when $x=0$. I found that as $$\lim \limits_{h\to 0}\frac{f(x+h)-f(x)}{h}$$ exists and equal to $0$. But still something seems wrong. Is function differentiable at $0$? Thank you.",,"['derivatives', 'absolute-value']"
51,Cauchy–Riemann equation on differntiability,Cauchy–Riemann equation on differntiability,,"I have found that: $U_x = -\exp(y)\sin(x) $ $U_y = \exp(y)\cos(x) $ $V_x = \exp(y)\cos(x) $ $V_y = \exp(y)\sin(x) $ I need to show that $U_x=V_y$ and $U_y=-V_x$, however these aren't satisfied? does that mean $f$ is not differentiable?","I have found that: $U_x = -\exp(y)\sin(x) $ $U_y = \exp(y)\cos(x) $ $V_x = \exp(y)\cos(x) $ $V_y = \exp(y)\sin(x) $ I need to show that $U_x=V_y$ and $U_y=-V_x$, however these aren't satisfied? does that mean $f$ is not differentiable?",,['derivatives']
52,Using the binomial theorem to generate a geometric proof of the derivative.,Using the binomial theorem to generate a geometric proof of the derivative.,,"According to wikipedia, if we wanted to prove $$(x^n)'=nx^{n-1}$$ geometrically by creating an $n$-dimensional hypercube $$(x+\Delta x)^n$$ and setting $a=x$ and $b=\Delta x$, we could expand using the binomial theorem to $$x^n + nx^{n-1}\Delta x + \binom{n}{2}x^{n-2}(\Delta x)^2 + ...$$ (and this is where I lose it) the terms $(\Delta x)^2$ and higher become negligible as $\Delta x \to 0$, which yields the formula $$(x^n)'=nx^{n-1}$$ However, if $\Delta x \to 0$, wouldn't that yield the formula $$(x^n)'=x^n$$ as every term but the first in the expanded version is reduced to $0$ by being multiplied by $(\Delta x)^k$, where $1\le k \le n$? I don't understand where the leading $x^n$ goes, or why $nx^{n-1}\Delta x$ does not equal 0 as $\Delta x \to 0$","According to wikipedia, if we wanted to prove $$(x^n)'=nx^{n-1}$$ geometrically by creating an $n$-dimensional hypercube $$(x+\Delta x)^n$$ and setting $a=x$ and $b=\Delta x$, we could expand using the binomial theorem to $$x^n + nx^{n-1}\Delta x + \binom{n}{2}x^{n-2}(\Delta x)^2 + ...$$ (and this is where I lose it) the terms $(\Delta x)^2$ and higher become negligible as $\Delta x \to 0$, which yields the formula $$(x^n)'=nx^{n-1}$$ However, if $\Delta x \to 0$, wouldn't that yield the formula $$(x^n)'=x^n$$ as every term but the first in the expanded version is reduced to $0$ by being multiplied by $(\Delta x)^k$, where $1\le k \le n$? I don't understand where the leading $x^n$ goes, or why $nx^{n-1}\Delta x$ does not equal 0 as $\Delta x \to 0$",,"['derivatives', 'binomial-theorem']"
53,Differential identity $\left(x^2\frac{d}{dx}\right)^nf(x)=x^{n+1}\frac{d^n}{dx^n}\left(x^{n-1}f(x)\right)$,Differential identity,\left(x^2\frac{d}{dx}\right)^nf(x)=x^{n+1}\frac{d^n}{dx^n}\left(x^{n-1}f(x)\right),"I have found the following differential identity: $$\left(-x^2\frac{d}{dx}\right)^nf(x)=(-1)^n x^{n+1}\frac{d^n}{dx^n}\left(x^{n-1}f(x)\right)$$ I have used it to find an alternative Rodrigues representation for Bessel polynomials. My proof we have placed: $D=\frac{d}{dx}$ in order to save space. We start observing that, for n=1 the following is trivial that: \begin{equation} \left(x^{2}D\right)=x^{n+1}D^{n}x^{n-1},n=1 \end{equation} Therefore for n=1 the identity holds. For induction we proof that, if the identity holds for n, then it holds also for n+1. Applying the operator$\left(x^{2}D\right)$ to both members: \begin{equation} \left(x^{2}D\right)\left(x^{2}D\right)^{n}=\left(x^{2}D\right)\left(x^{n+1}D^{n}x^{n-1}\right) \end{equation} \begin{equation} \left(x^{2}D\right)^{n+1}=x^{2}Dxx^{n}D^{n}x^{n-1}=x^{2}Dx^{n}xD^{n}x^{n-1} \end{equation} remembering the following commutation rules: \begin{equation} Dx^{n}\centerdot-x^{n}D=nx^{n-1} \end{equation} \begin{equation} D^{n}x\centerdot-xD^{n}=nD^{n-1} \end{equation} we can write: \begin{equation} \left(x^{2}D\right)\left(x^{2}D\right)^{n}=x^{2}\left[Dx^{n}\centerdot\right]\left[xD^{n}\right]x^{n-1}=x^{2}\left[x^{n}D+nx^{n-1}\right]\left[D^{n}x\centerdot-nD^{n-1}\right]x^{n-1} \end{equation} \begin{equation} =x^{n+2}D^{n+1}x^{n}\centerdot+x^{2}\left[nx^{n-1}D^{n}x\centerdot-x^{n}DnD^{n-1}-nx^{n-1}nD^{n-1}\right]x^{n-1} \end{equation} Now we can show that the term in square parentheses is null, indeed applying above seen commutation rules: \begin{equation} =x^{n+2}D^{n+1}x^{n}\centerdot+x^{2}\left[nx^{n-1}xD^{n}+nx^{n-1}nD^{n-1}-nx^{n}D{}^{n}-nx^{n-1}nD^{n-1}\right]x^{n-1}= \end{equation} \begin{equation} =x^{n+2}D^{n+1}x^{n}\centerdot+x^{2}\left[nx^{n}D^{n}-nx^{n}D{}^{n}+nx^{n-1}nD^{n-1}-nx^{n-1}nD^{n-1}\right]x^{n-1}=x^{n+2}D^{n+1}x^{n}\centerdot \end{equation} Therefore: \begin{equation} \left(x^{2}D\right)^{n+1}=\left(x^{2}D\right)\left(x^{2}D\right)^{n}=x^{n+2}D^{n+1}x^{n}\centerdot \end{equation} Questions Is my proof correct? Is this identity already known? If yes, what are the references? Where can I find a possibly more elegant proof? Edit So, it seems the identity was true, and already known. In ""Mathematical Analysis I"" (Springer), Zorich V.A, in chapter ""The Basic Rules of Differentiation"", exercise 3 at page 213, the identity of $D^nf(1/x)=(-1)^nx^{n+1}D^n x^{n-1}f(1/x)$ has to be demonstrated. Unfortunately no hint was given and no proof is sketched, so I remain without a more straightforward proof ...)","I have found the following differential identity: $$\left(-x^2\frac{d}{dx}\right)^nf(x)=(-1)^n x^{n+1}\frac{d^n}{dx^n}\left(x^{n-1}f(x)\right)$$ I have used it to find an alternative Rodrigues representation for Bessel polynomials. My proof we have placed: $D=\frac{d}{dx}$ in order to save space. We start observing that, for n=1 the following is trivial that: \begin{equation} \left(x^{2}D\right)=x^{n+1}D^{n}x^{n-1},n=1 \end{equation} Therefore for n=1 the identity holds. For induction we proof that, if the identity holds for n, then it holds also for n+1. Applying the operator$\left(x^{2}D\right)$ to both members: \begin{equation} \left(x^{2}D\right)\left(x^{2}D\right)^{n}=\left(x^{2}D\right)\left(x^{n+1}D^{n}x^{n-1}\right) \end{equation} \begin{equation} \left(x^{2}D\right)^{n+1}=x^{2}Dxx^{n}D^{n}x^{n-1}=x^{2}Dx^{n}xD^{n}x^{n-1} \end{equation} remembering the following commutation rules: \begin{equation} Dx^{n}\centerdot-x^{n}D=nx^{n-1} \end{equation} \begin{equation} D^{n}x\centerdot-xD^{n}=nD^{n-1} \end{equation} we can write: \begin{equation} \left(x^{2}D\right)\left(x^{2}D\right)^{n}=x^{2}\left[Dx^{n}\centerdot\right]\left[xD^{n}\right]x^{n-1}=x^{2}\left[x^{n}D+nx^{n-1}\right]\left[D^{n}x\centerdot-nD^{n-1}\right]x^{n-1} \end{equation} \begin{equation} =x^{n+2}D^{n+1}x^{n}\centerdot+x^{2}\left[nx^{n-1}D^{n}x\centerdot-x^{n}DnD^{n-1}-nx^{n-1}nD^{n-1}\right]x^{n-1} \end{equation} Now we can show that the term in square parentheses is null, indeed applying above seen commutation rules: \begin{equation} =x^{n+2}D^{n+1}x^{n}\centerdot+x^{2}\left[nx^{n-1}xD^{n}+nx^{n-1}nD^{n-1}-nx^{n}D{}^{n}-nx^{n-1}nD^{n-1}\right]x^{n-1}= \end{equation} \begin{equation} =x^{n+2}D^{n+1}x^{n}\centerdot+x^{2}\left[nx^{n}D^{n}-nx^{n}D{}^{n}+nx^{n-1}nD^{n-1}-nx^{n-1}nD^{n-1}\right]x^{n-1}=x^{n+2}D^{n+1}x^{n}\centerdot \end{equation} Therefore: \begin{equation} \left(x^{2}D\right)^{n+1}=\left(x^{2}D\right)\left(x^{2}D\right)^{n}=x^{n+2}D^{n+1}x^{n}\centerdot \end{equation} Questions Is my proof correct? Is this identity already known? If yes, what are the references? Where can I find a possibly more elegant proof? Edit So, it seems the identity was true, and already known. In ""Mathematical Analysis I"" (Springer), Zorich V.A, in chapter ""The Basic Rules of Differentiation"", exercise 3 at page 213, the identity of $D^nf(1/x)=(-1)^nx^{n+1}D^n x^{n-1}f(1/x)$ has to be demonstrated. Unfortunately no hint was given and no proof is sketched, so I remain without a more straightforward proof ...)",,['derivatives']
54,When $ \lim_{x\rightarrow +\infty}\frac{f(x)}{x}=0$ implies $\lim_{x\rightarrow +\infty}f'(x)=0$?,When  implies ?, \lim_{x\rightarrow +\infty}\frac{f(x)}{x}=0 \lim_{x\rightarrow +\infty}f'(x)=0,"I have just solved a problem: Let $f:[0,+\infty)\rightarrow \mathbb{R}$ be continuous on $[0,+\infty)$ and differentiable on $(0,+\infty)$. If $\displaystyle \lim_{x\rightarrow +\infty}f'(x)=0$, prove that $\displaystyle \lim_{x\rightarrow +\infty}\frac{f(x)}{x}=0$ My questions is about the inverse: if $\displaystyle \lim_{x\rightarrow +\infty}\frac{f(x)}{x}=0$, which hypothesis can be added (if needed) so that we can conclude $\displaystyle \lim_{x\rightarrow +\infty}f'(x)=0$ ? Actually, I tried to solve the following one, but I still cannot solve it: If $f:[0,+\infty)\rightarrow [0,+\infty)$ such that its second derivative is continuous, $f'\le 0$ and $|f''|\le M$ for some $M$ for all $x\ge 0$, then $\displaystyle \lim_{x\rightarrow +\infty}f'(x)=0$. From the hypothesis, $f$ decreases and bounded below, so $f$ has a limit when $x$ tends to infinity. Thus $\displaystyle \lim_{x\rightarrow +\infty}\frac{f(x)}{x}=0$. My questions seems to be in a wider range than the second problem above. Any help would be appreciated.","I have just solved a problem: Let $f:[0,+\infty)\rightarrow \mathbb{R}$ be continuous on $[0,+\infty)$ and differentiable on $(0,+\infty)$. If $\displaystyle \lim_{x\rightarrow +\infty}f'(x)=0$, prove that $\displaystyle \lim_{x\rightarrow +\infty}\frac{f(x)}{x}=0$ My questions is about the inverse: if $\displaystyle \lim_{x\rightarrow +\infty}\frac{f(x)}{x}=0$, which hypothesis can be added (if needed) so that we can conclude $\displaystyle \lim_{x\rightarrow +\infty}f'(x)=0$ ? Actually, I tried to solve the following one, but I still cannot solve it: If $f:[0,+\infty)\rightarrow [0,+\infty)$ such that its second derivative is continuous, $f'\le 0$ and $|f''|\le M$ for some $M$ for all $x\ge 0$, then $\displaystyle \lim_{x\rightarrow +\infty}f'(x)=0$. From the hypothesis, $f$ decreases and bounded below, so $f$ has a limit when $x$ tends to infinity. Thus $\displaystyle \lim_{x\rightarrow +\infty}\frac{f(x)}{x}=0$. My questions seems to be in a wider range than the second problem above. Any help would be appreciated.",,"['real-analysis', 'derivatives']"
55,Abel's theorem for the derivative of a power series,Abel's theorem for the derivative of a power series,,"Suppose $f: \mathbb{R} \rightarrow \mathbb{R}$ is a function, $(a_0, a_1, \dots)$ is a sequence of real numbers and $x_*$ is a positive real number, such that the following two conditions hold: for all $x \in (-x_*, x_*)$, the power series $\sum_{n = 0}^\infty a_n x^n$ converges to $f(x)$ and $\sum_{n = 0}^\infty a_n {x_*}^n$ converges. Abel's theorem tells us that $$ \lim_{\substack{x \rightarrow x_* \\ x < x_*}} f(x) = \sum_{n = 0}^\infty a_n {x_*}^n $$ Suppose $f$ is continuous from the left at $x_*$. It is a standard result that $f$ is differentiable inside $(-x_*, x_*)$ and that for every $x \in (-x^*, x^*)$, the series $\sum_{n = 1}^\infty n a_n x^{n - 1}$ converges to $f'(x)$. Suppose the last series converges at $x_*$ as well, and denote this sum by $s$. Is it the case that $f$ is differentiable from the left at $x_*$? Suppose $f$ happens to be differentiable from the left at $x_*$. Is it the case that $f'(x^*) = s$, where the derivative is the left-sided one?","Suppose $f: \mathbb{R} \rightarrow \mathbb{R}$ is a function, $(a_0, a_1, \dots)$ is a sequence of real numbers and $x_*$ is a positive real number, such that the following two conditions hold: for all $x \in (-x_*, x_*)$, the power series $\sum_{n = 0}^\infty a_n x^n$ converges to $f(x)$ and $\sum_{n = 0}^\infty a_n {x_*}^n$ converges. Abel's theorem tells us that $$ \lim_{\substack{x \rightarrow x_* \\ x < x_*}} f(x) = \sum_{n = 0}^\infty a_n {x_*}^n $$ Suppose $f$ is continuous from the left at $x_*$. It is a standard result that $f$ is differentiable inside $(-x_*, x_*)$ and that for every $x \in (-x^*, x^*)$, the series $\sum_{n = 1}^\infty n a_n x^{n - 1}$ converges to $f'(x)$. Suppose the last series converges at $x_*$ as well, and denote this sum by $s$. Is it the case that $f$ is differentiable from the left at $x_*$? Suppose $f$ happens to be differentiable from the left at $x_*$. Is it the case that $f'(x^*) = s$, where the derivative is the left-sided one?",,"['real-analysis', 'sequences-and-series', 'derivatives', 'power-series']"
56,Continuity of derivative implies differentiability at a point,Continuity of derivative implies differentiability at a point,,"Suppose $I\subset\Bbb{R}$ is an open interval, $a\in I$ and $f:I\to\Bbb{R}$ is continuous. Suppose also that $f$ is diffble on $I-\{a\}$. Show that if $\lim_{x\to a} f'(x)=s$ exists, $f$ is diffble at $a$ and $f'(a)=s$. I honestly don't know where to start. Any hints?","Suppose $I\subset\Bbb{R}$ is an open interval, $a\in I$ and $f:I\to\Bbb{R}$ is continuous. Suppose also that $f$ is diffble on $I-\{a\}$. Show that if $\lim_{x\to a} f'(x)=s$ exists, $f$ is diffble at $a$ and $f'(a)=s$. I honestly don't know where to start. Any hints?",,['derivatives']
57,Prove that f'=f iff f is an exponential funtion,Prove that f'=f iff f is an exponential funtion,,"Written more formally, prove that $f' = f \iff \exists c \in \mathbb{R} : f = c * \exp$ In other words, I guess, it's enough to prove that $\exp$ and $f(x) = 0$ are the only functions that are equal to its derivatives. How can I do that? I'll be grateful for a hint instead of a full proof. Thanks!","Written more formally, prove that $f' = f \iff \exists c \in \mathbb{R} : f = c * \exp$ In other words, I guess, it's enough to prove that $\exp$ and $f(x) = 0$ are the only functions that are equal to its derivatives. How can I do that? I'll be grateful for a hint instead of a full proof. Thanks!",,"['calculus', 'derivatives', 'proof-writing']"
58,Find $y'$ for $\ln(x+y)=\arctan(xy)$,Find  for,y' \ln(x+y)=\arctan(xy),Find $y'$ for $\ln(x+y)=\arctan(xy)$ Here is my attempt at a solution. Is this correct? Any hints or advice would be appreciated.,Find for Here is my attempt at a solution. Is this correct? Any hints or advice would be appreciated.,y' \ln(x+y)=\arctan(xy),"['calculus', 'derivatives', 'solution-verification', 'implicit-differentiation']"
59,Calculate the maximum area (maximum value),Calculate the maximum area (maximum value),,TX farmer has 100 metres of fencing to use to make a rectangular enclosure for sheep as shown. He will use existing walls for two sides of the enclosure and leave an opening of 2 metres for a gate. a)  Show that the area of the enclosure is given by: $A = 102x – x^2.$ b)  Find the value of x that will give the maximum possible area. c)  Calculate the maximum possible area. How do I assign the two variables for area ? Can anyone assist me in solving this problem?,TX farmer has 100 metres of fencing to use to make a rectangular enclosure for sheep as shown. He will use existing walls for two sides of the enclosure and leave an opening of 2 metres for a gate. a)  Show that the area of the enclosure is given by: $A = 102x – x^2.$ b)  Find the value of x that will give the maximum possible area. c)  Calculate the maximum possible area. How do I assign the two variables for area ? Can anyone assist me in solving this problem?,,"['calculus', 'algebra-precalculus', 'derivatives', 'optimization']"
60,How does one bound computational error for a finite difference approximation of the second derivative?,How does one bound computational error for a finite difference approximation of the second derivative?,,"I'm trying to wrap my head around ways to minimize total computational error (defined as a sum of the bounds on the truncation and rounding errors) by taking a differentiable function $f : \mathbb{R} \rightarrow \mathbb{R}$ and a finite difference approximation of its second derivative $$ f''(x) = \frac{f(x + h) - 2f(x) + f(x-h)}{h^2} $$ I know that, by Taylor's Theorem $$ f(x + h) = f(x) + f'(x)h + f''(x)\frac{h^2}{2} + f'''(\theta)\frac{h^3}{6} $$ for some $\theta \in [x, x + h]$. How would you determine the value of $h$ for which a bound of the total computational error is minimized?","I'm trying to wrap my head around ways to minimize total computational error (defined as a sum of the bounds on the truncation and rounding errors) by taking a differentiable function $f : \mathbb{R} \rightarrow \mathbb{R}$ and a finite difference approximation of its second derivative $$ f''(x) = \frac{f(x + h) - 2f(x) + f(x-h)}{h^2} $$ I know that, by Taylor's Theorem $$ f(x + h) = f(x) + f'(x)h + f''(x)\frac{h^2}{2} + f'''(\theta)\frac{h^3}{6} $$ for some $\theta \in [x, x + h]$. How would you determine the value of $h$ for which a bound of the total computational error is minimized?",,"['derivatives', 'numerical-methods', 'error-propagation']"
61,Showing that $\lim_{x \to 0}\frac{f(x)}{g(x)} = \frac{f'(0)}{g'(0)}$.,Showing that .,\lim_{x \to 0}\frac{f(x)}{g(x)} = \frac{f'(0)}{g'(0)},"If $f$ and $g$ are differentiable functions with $f(0) = g(0) = 0$ and $g'(0) \neq 0$, show that $\lim_{x \to 0}\frac{f(x)}{g(x)} = \frac{f'(0)}{g'(0)}$. I consider that perhaps: $$ \begin{align} \\ \lim_{x \to 0}\frac{f(x)}{g(x)} &= \lim_{x \to 0}\frac{f(0+x) - f(0)}{x} \cdot \frac{1}{ \lim_{x \to 0}\frac{g(0+x) - g(0)}{x}} = f'(0) \cdot \frac{1}{g'(0)} = \frac{f'(0)}{g'(0)} \end{align} $$ But, it seems like that's maybe not quite right. I'm not certain. Insight?","If $f$ and $g$ are differentiable functions with $f(0) = g(0) = 0$ and $g'(0) \neq 0$, show that $\lim_{x \to 0}\frac{f(x)}{g(x)} = \frac{f'(0)}{g'(0)}$. I consider that perhaps: $$ \begin{align} \\ \lim_{x \to 0}\frac{f(x)}{g(x)} &= \lim_{x \to 0}\frac{f(0+x) - f(0)}{x} \cdot \frac{1}{ \lim_{x \to 0}\frac{g(0+x) - g(0)}{x}} = f'(0) \cdot \frac{1}{g'(0)} = \frac{f'(0)}{g'(0)} \end{align} $$ But, it seems like that's maybe not quite right. I'm not certain. Insight?",,"['calculus', 'real-analysis', 'limits', 'derivatives', 'proof-verification']"
62,The existence of the $n$th derivative at $c$ presumes the existence of the $(n-1)$st derivative in an interval containing $c$,The existence of the th derivative at  presumes the existence of the st derivative in an interval containing,n c (n-1) c,"The following is from Introduction to Real Analysis by Bartle. If the derivative $f'(x)$ of a function $f$ exists at every point $x$ in an interval $I$ containing a point $c$, then we can consider the existence of the derivative of the function $f'$ at the point $c$. In case $f'$ has a derivative at the point $c$, we refer to the resulting number as the second derivative of $f$ at $c$ and we denote this number by $f''(c)$. In similar fashion we define the third derivative $f^{(3)}(c)$, and the nth derivative $f^{(n)}(c)$, whenever these derivatives exist. It is noted that the existence of the $n$th derivative at $c$ presumes the existence of the $(n-1)$st derivative in an interval containing $c$ , but we do allow the possibility that $c$ might be an endpoint of such an interval. My question is, does the existence of the $n$th derivative necessarily guarantee that the $(n-1)$st derivative exists in an interval containing $c$? I understand that $c$ must be an accumulation point in the domain of the $(n-1)$st derivative, but I don't see why the domain necessarily have to be an interval. For instance, $f(x)=x^2$ defined on ${\frac{1}{n}|n\in \mathbb{N}}\cup {0}$ has a derivative at $0$ but the domain is clearly not an interval.","The following is from Introduction to Real Analysis by Bartle. If the derivative $f'(x)$ of a function $f$ exists at every point $x$ in an interval $I$ containing a point $c$, then we can consider the existence of the derivative of the function $f'$ at the point $c$. In case $f'$ has a derivative at the point $c$, we refer to the resulting number as the second derivative of $f$ at $c$ and we denote this number by $f''(c)$. In similar fashion we define the third derivative $f^{(3)}(c)$, and the nth derivative $f^{(n)}(c)$, whenever these derivatives exist. It is noted that the existence of the $n$th derivative at $c$ presumes the existence of the $(n-1)$st derivative in an interval containing $c$ , but we do allow the possibility that $c$ might be an endpoint of such an interval. My question is, does the existence of the $n$th derivative necessarily guarantee that the $(n-1)$st derivative exists in an interval containing $c$? I understand that $c$ must be an accumulation point in the domain of the $(n-1)$st derivative, but I don't see why the domain necessarily have to be an interval. For instance, $f(x)=x^2$ defined on ${\frac{1}{n}|n\in \mathbb{N}}\cup {0}$ has a derivative at $0$ but the domain is clearly not an interval.",,"['real-analysis', 'analysis', 'derivatives']"
63,How to solve this differential equation please?,How to solve this differential equation please?,,I'm trying to solve: $$\frac{dz}{dx}+2xz=2x$$ I have got the integrating factor as $$e^{\int 2x dx}=e^{x^2}$$ and so $$ze^{x^2}=\int {2xe^{x^2}} dx+ C$$ But I don't know how to proceed it's mainly an issue with calculating $$\int {2xe^{x^2}} dx$$ Any help?,I'm trying to solve: $$\frac{dz}{dx}+2xz=2x$$ I have got the integrating factor as $$e^{\int 2x dx}=e^{x^2}$$ and so $$ze^{x^2}=\int {2xe^{x^2}} dx+ C$$ But I don't know how to proceed it's mainly an issue with calculating $$\int {2xe^{x^2}} dx$$ Any help?,,"['integration', 'ordinary-differential-equations', 'derivatives']"
64,Show that the cubic equation has one real roots,Show that the cubic equation has one real roots,,"Show that $x^3+ax+b=0$  has a) only one real root when $a>0$ b) at most only one of it's roots are in $(-\sqrt{-a/3},\sqrt{-a/3})$ when $a<0$. For a) I supposed that it had two real roots and i came to the conclusion that the derivative function never attains the value $0$, i don't know what to do in b). Can someone help with the editing, I am new in this site and I think that it has to do again with Rolle's Theorem.","Show that $x^3+ax+b=0$  has a) only one real root when $a>0$ b) at most only one of it's roots are in $(-\sqrt{-a/3},\sqrt{-a/3})$ when $a<0$. For a) I supposed that it had two real roots and i came to the conclusion that the derivative function never attains the value $0$, i don't know what to do in b). Can someone help with the editing, I am new in this site and I think that it has to do again with Rolle's Theorem.",,"['calculus', 'derivatives', 'applications']"
65,Derivative of matrix with respected to vector (matrix in se(3)),Derivative of matrix with respected to vector (matrix in se(3)),,"We know that the matrix $A \in SE(3)$ can be written in exponential formula, i.e. $$e^{M} = \begin{bmatrix} R_{3\times3} & \vec{t} \\\vec{0}^T & 1 \end{bmatrix}$$ $$M = \begin{bmatrix} [\vec{\omega}]_{\times} & V^{-1}\vec{t} \\\vec{0}^T & 0 \end{bmatrix}$$ ,where $\omega = [\omega_x, \omega_y, \omega_z]^T$ can be retrieved from rodrigues' formula. and $$V = I_3 + \frac{1-cos{\theta}}{\theta^2}[\vec{\omega}]_{\times} + \frac{\theta - \sin{\theta}}{\theta^3}[\vec{\omega}]_{\times}^2$$ We know that matrix $V$ is a function of $[\omega_x, \omega_y, \omega_z]$. I wish to know how to compute the derivative of $V$ with respected to each element. My attempt is to use the definition of derivative. $$f^\prime = \lim_{h\rightarrow0}\frac{f(x+h) - f(x)}{h}$$ However, even with the help from wolframalpha, I couldn't simplify the term. The image below shows my attempt with wolframalpha for the second term. Does anyone here know more convenient method of doing this? Thank you very much.","We know that the matrix $A \in SE(3)$ can be written in exponential formula, i.e. $$e^{M} = \begin{bmatrix} R_{3\times3} & \vec{t} \\\vec{0}^T & 1 \end{bmatrix}$$ $$M = \begin{bmatrix} [\vec{\omega}]_{\times} & V^{-1}\vec{t} \\\vec{0}^T & 0 \end{bmatrix}$$ ,where $\omega = [\omega_x, \omega_y, \omega_z]^T$ can be retrieved from rodrigues' formula. and $$V = I_3 + \frac{1-cos{\theta}}{\theta^2}[\vec{\omega}]_{\times} + \frac{\theta - \sin{\theta}}{\theta^3}[\vec{\omega}]_{\times}^2$$ We know that matrix $V$ is a function of $[\omega_x, \omega_y, \omega_z]$. I wish to know how to compute the derivative of $V$ with respected to each element. My attempt is to use the definition of derivative. $$f^\prime = \lim_{h\rightarrow0}\frac{f(x+h) - f(x)}{h}$$ However, even with the help from wolframalpha, I couldn't simplify the term. The image below shows my attempt with wolframalpha for the second term. Does anyone here know more convenient method of doing this? Thank you very much.",,"['calculus', 'matrices', 'derivatives']"
66,"Boundedness of $f'(x)/x$ implies uniform continuity of $f(x)/x$ on $(1,\infty)$",Boundedness of  implies uniform continuity of  on,"f'(x)/x f(x)/x (1,\infty)","Let $f:(1,\infty) \to \mathbb{R}$ be differentiable, define $g, h:(1,\infty) \to \mathbb{R}$ by $g(x)=f'(x)/x$ and $h(x)=f(x)/x$. Suppose $g$ is bounded. Prove that $h$ is uniformly continuous. I tried by writing $h$ in terms of $f$ and $g$ as: $h(x)=f(x)g(x)/f'(x)$ and then use the fact that $g$ is bounded, meaning $h(x)\le f(x) M/f'(x)$ for some big natural $M$. But then I get stuck.","Let $f:(1,\infty) \to \mathbb{R}$ be differentiable, define $g, h:(1,\infty) \to \mathbb{R}$ by $g(x)=f'(x)/x$ and $h(x)=f(x)/x$. Suppose $g$ is bounded. Prove that $h$ is uniformly continuous. I tried by writing $h$ in terms of $f$ and $g$ as: $h(x)=f(x)g(x)/f'(x)$ and then use the fact that $g$ is bounded, meaning $h(x)\le f(x) M/f'(x)$ for some big natural $M$. But then I get stuck.",,"['real-analysis', 'derivatives', 'uniform-continuity']"
67,Rearranging an equation to form the limit definition of derivative,Rearranging an equation to form the limit definition of derivative,,"I am following a proof which starts with the following inequalities: $$S_{i}(v) \geq S_{i}(v+dv) + (-dv)P_{i}(v+dv)$$ $$S_{i}(v+dv) \geq S_{i}(v) + (dv)P_{i}(v)$$ From this, we rearrange to form: $$P_{i}(v+dv) \geq \frac{S_{i}(v+dv) - S_{i}(v)}{dv} \geq P_{i}(v)$$ Taking limits as $dv \rightarrow 0 $, we get: $$ \dfrac{dS_{i}}{dv_{i}} = P_{i}(v_{i}) $$ using the sandwich theorem and limit definition of derivative. My question is regarding rearranging the first two inequalities to form the third inequality bounded by the two variables. In this case, we assume that $dv > 0$, as the signs do not change. If we assume $dv < 0$, the signs change directions, however after taking the limit we reach the same result. But what about if $dv = 0$? Is the proof not considering this case? Or we just stating the derivative is not defined at $dv = 0$? Can you still say the derivative of $S$ wrt $v$ is $P_i(v)$? Both functions are assumed to be continuous so even if $dv = 0$, therefore the final statement is still correct?","I am following a proof which starts with the following inequalities: $$S_{i}(v) \geq S_{i}(v+dv) + (-dv)P_{i}(v+dv)$$ $$S_{i}(v+dv) \geq S_{i}(v) + (dv)P_{i}(v)$$ From this, we rearrange to form: $$P_{i}(v+dv) \geq \frac{S_{i}(v+dv) - S_{i}(v)}{dv} \geq P_{i}(v)$$ Taking limits as $dv \rightarrow 0 $, we get: $$ \dfrac{dS_{i}}{dv_{i}} = P_{i}(v_{i}) $$ using the sandwich theorem and limit definition of derivative. My question is regarding rearranging the first two inequalities to form the third inequality bounded by the two variables. In this case, we assume that $dv > 0$, as the signs do not change. If we assume $dv < 0$, the signs change directions, however after taking the limit we reach the same result. But what about if $dv = 0$? Is the proof not considering this case? Or we just stating the derivative is not defined at $dv = 0$? Can you still say the derivative of $S$ wrt $v$ is $P_i(v)$? Both functions are assumed to be continuous so even if $dv = 0$, therefore the final statement is still correct?",,"['calculus', 'limits', 'derivatives', 'proof-verification']"
68,"if $ h(x) = x\cdot g(x)$ and it is known that $ g(3) = 5$, and $g′(3) = 2$ , find $ h'(3)$","if  and it is known that , and  , find", h(x) = x\cdot g(x)  g(3) = 5 g′(3) = 2  h'(3),"This is an example in my book. (James Stewart Calculus 7E pg 132) A solution is given, but I don't understand it, even after looking at it over and over. I know I am supposed make use of the product rule $[(fg)′ = fg′ + gf′]$, but I guess I need it spelled out for me like a small child because I don't get it Thank you","This is an example in my book. (James Stewart Calculus 7E pg 132) A solution is given, but I don't understand it, even after looking at it over and over. I know I am supposed make use of the product rule $[(fg)′ = fg′ + gf′]$, but I guess I need it spelled out for me like a small child because I don't get it Thank you",,"['calculus', 'derivatives']"
69,Critical Points of a smooth map on SO(n),Critical Points of a smooth map on SO(n),,"I am given the following map $f:SO(n) \rightarrow \mathbb{R}$, $f(X) = Tr(DX)$ where $D$ is a diagonal matrix $\{d_1,\ldots,d_n\}$, $1<d_1<\cdots <d_n$. I need to find the critical points of this map, and the index of $f$ at those points. If I am right, $df = f$ since $f$ is a linear map, but I can not find any critical point. Do I need to use local charts for $SO(n)$?","I am given the following map $f:SO(n) \rightarrow \mathbb{R}$, $f(X) = Tr(DX)$ where $D$ is a diagonal matrix $\{d_1,\ldots,d_n\}$, $1<d_1<\cdots <d_n$. I need to find the critical points of this map, and the index of $f$ at those points. If I am right, $df = f$ since $f$ is a linear map, but I can not find any critical point. Do I need to use local charts for $SO(n)$?",,"['differential-geometry', 'derivatives']"
70,Application Fundamental Theorem of Calculus,Application Fundamental Theorem of Calculus,,"Let $k:[a,b]\times[a,b]\to \mathbb{C}$, $f:[a,b]\to\mathbb{C}$.  $$ f(x)=\int\limits_a^x k(x,y)\phi(y)\mathrm{d}y \qquad a\le x \le b $$ I have to compute $f'(x)$. I don't know how to apply the Fundamental Theorem of Calculus in this case.","Let $k:[a,b]\times[a,b]\to \mathbb{C}$, $f:[a,b]\to\mathbb{C}$.  $$ f(x)=\int\limits_a^x k(x,y)\phi(y)\mathrm{d}y \qquad a\le x \le b $$ I have to compute $f'(x)$. I don't know how to apply the Fundamental Theorem of Calculus in this case.",,"['real-analysis', 'integration', 'analysis', 'derivatives']"
71,"Equation for Tangent Line that passes through $(0,1)$ on the curve $y = \ln x$",Equation for Tangent Line that passes through  on the curve,"(0,1) y = \ln x","I'm totally lost. I've been trying to figure this out. This is what I've figured out: $dy/dx = 1/x$ $y$-intercept $= 1$ So I try to do $y-y_1 = m(x-x_1)+b,$ which I get as $y-1 = 1/x(x-0)+1,$ simplified to $y = 3.$ But I feel like that is totally wrong and well, obviously it isn't even an equation really. Can someone help me out with this?","I'm totally lost. I've been trying to figure this out. This is what I've figured out: $dy/dx = 1/x$ $y$-intercept $= 1$ So I try to do $y-y_1 = m(x-x_1)+b,$ which I get as $y-1 = 1/x(x-0)+1,$ simplified to $y = 3.$ But I feel like that is totally wrong and well, obviously it isn't even an equation really. Can someone help me out with this?",,"['calculus', 'derivatives', 'analytic-geometry']"
72,How to show this equation have exactly single solution?,How to show this equation have exactly single solution?,,"Consider $a=1,\:b\in \mathbb{R}$. Show that there is single solution for the equation:$$x-a\sin x\:=\:b$$ So far I defined funtion $f\left(x\right)=x-a\sin \left(x\right)-b\:=\:x-\sin \left(x\right)-b$ and learned that $\lim _{x\to \infty }\left(f\left(x\right)\right)\:=\:\infty \:,\:\lim _{x\to -\infty }\left(f\left(x\right)\right)=-\infty \:$. Now, the derivative of this function not always increasing because in $x_0=0$, $f'\left(x\right)\:=\:0$, so I can't use the Intermediate value theorem .. Any idea? tnx in advance!","Consider $a=1,\:b\in \mathbb{R}$. Show that there is single solution for the equation:$$x-a\sin x\:=\:b$$ So far I defined funtion $f\left(x\right)=x-a\sin \left(x\right)-b\:=\:x-\sin \left(x\right)-b$ and learned that $\lim _{x\to \infty }\left(f\left(x\right)\right)\:=\:\infty \:,\:\lim _{x\to -\infty }\left(f\left(x\right)\right)=-\infty \:$. Now, the derivative of this function not always increasing because in $x_0=0$, $f'\left(x\right)\:=\:0$, so I can't use the Intermediate value theorem .. Any idea? tnx in advance!",,"['calculus', 'limits', 'derivatives']"
73,Finding points on the parabola at which normal line passes through it,Finding points on the parabola at which normal line passes through it,,"Hello guys I need help with the problem: Find the points on the parabola $y = x^2 - 4x + 3$ at which normal line passes through $(2, 0)$. What I did: I first took a derivative of the equation which equals to $2x - 4$. Then I have the equation $y - y_1 = (2x - 4)(x - x_1)$. $y$ is $2$ and $x$ is $0$ here. But I'm not sure how to find $y_1$ and $x_1$ here. I'm thinking that when I get the final equation, I can plug in $(2, 0)$ and get the points. Am I approaching this problem correctly? It seems like a simple problem, but it confuses me","Hello guys I need help with the problem: Find the points on the parabola $y = x^2 - 4x + 3$ at which normal line passes through $(2, 0)$. What I did: I first took a derivative of the equation which equals to $2x - 4$. Then I have the equation $y - y_1 = (2x - 4)(x - x_1)$. $y$ is $2$ and $x$ is $0$ here. But I'm not sure how to find $y_1$ and $x_1$ here. I'm thinking that when I get the final equation, I can plug in $(2, 0)$ and get the points. Am I approaching this problem correctly? It seems like a simple problem, but it confuses me",,"['calculus', 'derivatives']"
74,Prove polynomial has at least $n-1$ distinc real roots,Prove polynomial has at least  distinc real roots,n-1,Let $W(x)$ be a polynomial with $n$ distinct real roots. Prove for any $k \in \mathbb{R}$ polynomial $P(x)=k\cdot W(x) + W'(x)$ has at least $n-1$ distinct real roots. I know how to show it for $k=0$ then it follow from roll theorem but show it for $k \neq 0$ ?,Let $W(x)$ be a polynomial with $n$ distinct real roots. Prove for any $k \in \mathbb{R}$ polynomial $P(x)=k\cdot W(x) + W'(x)$ has at least $n-1$ distinct real roots. I know how to show it for $k=0$ then it follow from roll theorem but show it for $k \neq 0$ ?,,"['calculus', 'derivatives']"
75,Calculate Laplace transform of the product of t and f(t) by differenitating f(t) (5.5-4),Calculate Laplace transform of the product of t and f(t) by differenitating f(t) (5.5-4),,"Request : Please check my work. State where errors, if any, occurred and how to correct them. Is there a better way to calculate the transform other than the present method given? Given : Find the Laplace transform $\mathcal{L}\{t\cdot \sin(3t)\}$ by differentiating $f(t)$. Solution : $$\mathcal{L}\{t\cdot f(t)\}=-F'(s)$$ $$f(t)=\sin(3t)$$ $$F(s)=\frac{s}{s^2+9}$$ $$F'(s)=\frac{s\cdot(-1)\cdot2s}{(s^2+9)^2}+\frac{1}{(s^2+9)}=-\frac{2s^2}{(s^2+9)^2}+\frac{1}{(s^2+9)}$$ $$\mathcal{L}\{t\cdot \sin(3t)\}=-F'(s)=\frac{2s^2}{(s^2+9)^2}-\frac{1}{(s^2+9)}$$","Request : Please check my work. State where errors, if any, occurred and how to correct them. Is there a better way to calculate the transform other than the present method given? Given : Find the Laplace transform $\mathcal{L}\{t\cdot \sin(3t)\}$ by differentiating $f(t)$. Solution : $$\mathcal{L}\{t\cdot f(t)\}=-F'(s)$$ $$f(t)=\sin(3t)$$ $$F(s)=\frac{s}{s^2+9}$$ $$F'(s)=\frac{s\cdot(-1)\cdot2s}{(s^2+9)^2}+\frac{1}{(s^2+9)}=-\frac{2s^2}{(s^2+9)^2}+\frac{1}{(s^2+9)}$$ $$\mathcal{L}\{t\cdot \sin(3t)\}=-F'(s)=\frac{2s^2}{(s^2+9)^2}-\frac{1}{(s^2+9)}$$",,"['derivatives', 'laplace-transform', 'solution-verification']"
76,Equivalence of two definitions of the derivative of a real function,Equivalence of two definitions of the derivative of a real function,,"The derivative of $ x $ in an interval $ [a,b] $ on which a function $ f $ is defined is defined as.. $$f'(x)=\lim_{t \to x}\frac{f(t)-f(x)}{t-x}$$ Why is this equal to $$ f'(t)=\lim_{x \to t}\frac{f(x)-f(t)}{x-t}?$$","The derivative of $ x $ in an interval $ [a,b] $ on which a function $ f $ is defined is defined as.. $$f'(x)=\lim_{t \to x}\frac{f(t)-f(x)}{t-x}$$ Why is this equal to $$ f'(t)=\lim_{x \to t}\frac{f(x)-f(t)}{x-t}?$$",,"['real-analysis', 'limits', 'derivatives', 'definition']"
77,Left and right derivative,Left and right derivative,,Find the left and right derivative of $f(x) = (2+|x|)e^x$ in x = 0. This is how I started (with the derivative from the right): $$\lim _{h\to 0^+}\frac{f(0+h) - f(0)}{h} =$$ $$\lim _{h\to 0^+}\frac{(2+|0+h|)e^{0+h} - (2+|0|)e^{0}}{h} = \lim _{h\to 0^+}\frac{(2+h)e^h - 2}{h}$$ This is as far as I have gotten. Am I approaching it the right way? How do I continue from here? I can't see how to get rid of $h$ in the denominator.,Find the left and right derivative of $f(x) = (2+|x|)e^x$ in x = 0. This is how I started (with the derivative from the right): $$\lim _{h\to 0^+}\frac{f(0+h) - f(0)}{h} =$$ $$\lim _{h\to 0^+}\frac{(2+|0+h|)e^{0+h} - (2+|0|)e^{0}}{h} = \lim _{h\to 0^+}\frac{(2+h)e^h - 2}{h}$$ This is as far as I have gotten. Am I approaching it the right way? How do I continue from here? I can't see how to get rid of $h$ in the denominator.,,"['limits', 'derivatives']"
78,complex analysis differentiation and existence of a point?,complex analysis differentiation and existence of a point?,,"If $f(z) = z^3$ prove that there is no point $c$ on line segment $[1,i]$  s.t. $(f(i)-f(1)) / (i-1) = f'(c)$. So differentiating: $$f'(c) = 3c^2$$ $$3c^2 = (f(i)-f(1))/(i-1) = (-i-1)/(i-1) = i$$ Hence $c = \sqrt{i/3}$. Am i doing this right?  Could anyone also clarify what the line segment $[1,i]$ means? Is it the diagonal line from the real axis $1$ to the Im axis $i$?","If $f(z) = z^3$ prove that there is no point $c$ on line segment $[1,i]$  s.t. $(f(i)-f(1)) / (i-1) = f'(c)$. So differentiating: $$f'(c) = 3c^2$$ $$3c^2 = (f(i)-f(1))/(i-1) = (-i-1)/(i-1) = i$$ Hence $c = \sqrt{i/3}$. Am i doing this right?  Could anyone also clarify what the line segment $[1,i]$ means? Is it the diagonal line from the real axis $1$ to the Im axis $i$?",,"['complex-analysis', 'derivatives']"
79,Derivative under integral mixed with...,Derivative under integral mixed with...,,"$$f(x,y)=\int_{e^{4y}}^{\ln^3(x)}{\frac{\sin(t)}{t}\,dt}$$ Whats the derivative $\frac{d f}{d t}$, if: $$x(t)=\cos(2+6t).4t^2$$ $$y(t)=\ln(2r+7e^{5t})$$ Really not much to say about this problem since I never saw it before. Ps: Edition has been made but the original question used $\partial f/\partial t$ (partial derivative instead of $d$). Also, if is worth adding, the first part of the question asked to calculate $\partial f/\partial x$ and $\partial f/\partial y$, but did not caused me trouble. I would really appreciate if someone could solve this.","$$f(x,y)=\int_{e^{4y}}^{\ln^3(x)}{\frac{\sin(t)}{t}\,dt}$$ Whats the derivative $\frac{d f}{d t}$, if: $$x(t)=\cos(2+6t).4t^2$$ $$y(t)=\ln(2r+7e^{5t})$$ Really not much to say about this problem since I never saw it before. Ps: Edition has been made but the original question used $\partial f/\partial t$ (partial derivative instead of $d$). Also, if is worth adding, the first part of the question asked to calculate $\partial f/\partial x$ and $\partial f/\partial y$, but did not caused me trouble. I would really appreciate if someone could solve this.",,"['calculus', 'integration', 'derivatives', 'partial-derivative']"
80,Coming Up With A Neutral Fixed Points Theorem,Coming Up With A Neutral Fixed Points Theorem,,"Question: If $f(x_0)=x_0,f'(x_0)=1$ and $f''(x_0)>0$, is $x_0$ weakly attracting, weakly repelling, or neither? (weakly attracting meaning $\exists\delta,\forall x\in B_\delta(x_0)\setminus\{x_0\},\forall n: f^n(x)\in B_\delta(x_0)$, and weakly repelling meaning not weakly attracting). Repeat 5, but this time assume that $f''(x_0)<0$. If $f(x_0)=x_0,f'(x_0)=1,f''(x_0)=0$ and $f'''(x_0)>0$, then show that $x_0$ is weakly repelling. Repeat 7, but this time assume $f'''(x_0)<0$ and show that $x_0$ is weakly attracting. Combine the results of 5-8 to state a Neutral Fixed Point Theorem. Motivation: I am taking a course in dynamical systems and we are using Devaney's A First Course in Dynamical Systems . The series of calculus questions presented above are from p. 51 of the book (though I have paraphrased them). The Neutral Fixed Point Theorem is opposed to Attracting/ Repelling Fixed Point Theorems, which say that if $|f'(x_0)|<1$/ $|f'(x_0)|>1$, then $x_0$ is an attracting/ repelling fixed point (the definitions of these are similar, ""weakly"" denotes only the slowness in the above case). I can easily answer these questions geometrically (the answers for 5 and 6 are that $x_0$ is neither w.a. nor w.r., it repels from one side and attracts from the other, e.g., $f(x)=x-x^2$) but I am looking for analytical proofs. I have been thinking about these for a while, but all I could come up with are MVT-esque expressions for points $x$ close enough to $x_0$. P.S.: These are not homework questions.","Question: If $f(x_0)=x_0,f'(x_0)=1$ and $f''(x_0)>0$, is $x_0$ weakly attracting, weakly repelling, or neither? (weakly attracting meaning $\exists\delta,\forall x\in B_\delta(x_0)\setminus\{x_0\},\forall n: f^n(x)\in B_\delta(x_0)$, and weakly repelling meaning not weakly attracting). Repeat 5, but this time assume that $f''(x_0)<0$. If $f(x_0)=x_0,f'(x_0)=1,f''(x_0)=0$ and $f'''(x_0)>0$, then show that $x_0$ is weakly repelling. Repeat 7, but this time assume $f'''(x_0)<0$ and show that $x_0$ is weakly attracting. Combine the results of 5-8 to state a Neutral Fixed Point Theorem. Motivation: I am taking a course in dynamical systems and we are using Devaney's A First Course in Dynamical Systems . The series of calculus questions presented above are from p. 51 of the book (though I have paraphrased them). The Neutral Fixed Point Theorem is opposed to Attracting/ Repelling Fixed Point Theorems, which say that if $|f'(x_0)|<1$/ $|f'(x_0)|>1$, then $x_0$ is an attracting/ repelling fixed point (the definitions of these are similar, ""weakly"" denotes only the slowness in the above case). I can easily answer these questions geometrically (the answers for 5 and 6 are that $x_0$ is neither w.a. nor w.r., it repels from one side and attracts from the other, e.g., $f(x)=x-x^2$) but I am looking for analytical proofs. I have been thinking about these for a while, but all I could come up with are MVT-esque expressions for points $x$ close enough to $x_0$. P.S.: These are not homework questions.",,"['calculus', 'derivatives', 'dynamical-systems']"
81,Please explain the algebra in the last part of derivative of the sigmoid function,Please explain the algebra in the last part of derivative of the sigmoid function,,http://www.ai.mit.edu/courses/6.892/lecture8-html/sld015.htm how does this: $${1\over 1 + e^{-x}} \cdot {-e^{-x}\over 1 + e^{-x}}$$ become this: $${1\over 1 + e^{-x}} \cdot \left (1 - {1\over 1 + e^{-x}}\right)$$,http://www.ai.mit.edu/courses/6.892/lecture8-html/sld015.htm how does this: $${1\over 1 + e^{-x}} \cdot {-e^{-x}\over 1 + e^{-x}}$$ become this: $${1\over 1 + e^{-x}} \cdot \left (1 - {1\over 1 + e^{-x}}\right)$$,,['derivatives']
82,"A function such that $f(x) = \lim_{t\to0}\frac{1}{2t}\int_{x-t}^{x+t} sf'(s)\,ds$ for all $x$",A function such that  for all,"f(x) = \lim_{t\to0}\frac{1}{2t}\int_{x-t}^{x+t} sf'(s)\,ds x","Let $f:\mathbb R\to\mathbb R$ be a function with continuous derivative such that $f(\sqrt{2})=2$ and $$f(x) = \lim_{t\to0}\frac{1}{2t}\int_{x-t}^{x+t} sf'(s)\,ds$$ for all $x\in\mathbb R$. Find $f(3)$. I guess Fundamental theorem of Calculus needs to be used to solve this. Taking derivative of x on both sides I simplified the integral to $(x+t)f'(x+t) - (x-t)f'(x-t) $ The equation becomes: $f'(x) = \lim (1/2t)(x+t)f'(x+t) - (x-t)f'(x-t) $ as t tends to 0. This is leading me nowhere. Any ideas on how to tackle this problem?","Let $f:\mathbb R\to\mathbb R$ be a function with continuous derivative such that $f(\sqrt{2})=2$ and $$f(x) = \lim_{t\to0}\frac{1}{2t}\int_{x-t}^{x+t} sf'(s)\,ds$$ for all $x\in\mathbb R$. Find $f(3)$. I guess Fundamental theorem of Calculus needs to be used to solve this. Taking derivative of x on both sides I simplified the integral to $(x+t)f'(x+t) - (x-t)f'(x-t) $ The equation becomes: $f'(x) = \lim (1/2t)(x+t)f'(x+t) - (x-t)f'(x-t) $ as t tends to 0. This is leading me nowhere. Any ideas on how to tackle this problem?",,"['calculus', 'integration', 'limits', 'derivatives', 'definite-integrals']"
83,"Is this function increasing? (standard normal distribution, Mills Ratio)","Is this function increasing? (standard normal distribution, Mills Ratio)",,"Where $\phi(z)$ and $\Phi(z)$ represent the standard normal pdf and cdf respectively. 1) Is the function $$f(z)=\frac{\phi(z)}{1-\Phi(z)}$$ increasing for all values of $z$? If so, how can I show it? 2) Is the limit as $z\rightarrow\infty$ using L'Hôpital's rule $$\lim_{z\rightarrow\infty}f(z) = \frac{\phi'(z)}{-\phi(z)}=\frac{-z\phi(z)}{-\phi(z)}=z=\infty \text{?}$$","Where $\phi(z)$ and $\Phi(z)$ represent the standard normal pdf and cdf respectively. 1) Is the function $$f(z)=\frac{\phi(z)}{1-\Phi(z)}$$ increasing for all values of $z$? If so, how can I show it? 2) Is the limit as $z\rightarrow\infty$ using L'Hôpital's rule $$\lim_{z\rightarrow\infty}f(z) = \frac{\phi'(z)}{-\phi(z)}=\frac{-z\phi(z)}{-\phi(z)}=z=\infty \text{?}$$",,"['limits', 'derivatives', 'normal-distribution']"
84,Constructing a Continuous Everywhere but Nowhere Differentiable Function,Constructing a Continuous Everywhere but Nowhere Differentiable Function,,"In Neal Carothers' Real Analysis he claims that $$f(x)=\sum_{k \mathop = 0}^\infty 2^{-k}g(2^{k}x)$$ is a continuous but non-differentiable function over the real line if $g(x)$ is the distance between $x$ and the integer closest to $x$. He then defines a dyadic rational to be a rational number of the form $\dfrac{i}{2^n}$ where $i$ is an integer, and that by two successive dyadic rationals he means $\dfrac{i}{2^n}$ and $\dfrac{i+1}{2^n}$. Skipping a few steps ahead to the part I don't understand, he writes the equation: $$\dfrac{f(v_n)-f(u_n)}{v_n-u_n}=\sum_{k \mathop = 0}^{n-1} \dfrac{g(2^{k}v_n)-g(2^{k}u_n)}{2^kv_n-2^ku_n}$$ Where $u_n$ and $v_n$ are a pair of successive dyadic rationals, and $n\ge 1$. To me, the right hand side of the equation is the $(n-1)^{th}$ term of the sequence $$f_n(x)=\sum_{k \mathop = 0}^n 2^{-k}g(2^{k}x)$$ at $x=v_n$ minus the $(n-1)^{th}$ term of the above sequence at $x=u_n$, all divided by $v_n-u_n$. How can this be equal to the expression on the left hand side? Any help at all would be much appreciated!","In Neal Carothers' Real Analysis he claims that $$f(x)=\sum_{k \mathop = 0}^\infty 2^{-k}g(2^{k}x)$$ is a continuous but non-differentiable function over the real line if $g(x)$ is the distance between $x$ and the integer closest to $x$. He then defines a dyadic rational to be a rational number of the form $\dfrac{i}{2^n}$ where $i$ is an integer, and that by two successive dyadic rationals he means $\dfrac{i}{2^n}$ and $\dfrac{i+1}{2^n}$. Skipping a few steps ahead to the part I don't understand, he writes the equation: $$\dfrac{f(v_n)-f(u_n)}{v_n-u_n}=\sum_{k \mathop = 0}^{n-1} \dfrac{g(2^{k}v_n)-g(2^{k}u_n)}{2^kv_n-2^ku_n}$$ Where $u_n$ and $v_n$ are a pair of successive dyadic rationals, and $n\ge 1$. To me, the right hand side of the equation is the $(n-1)^{th}$ term of the sequence $$f_n(x)=\sum_{k \mathop = 0}^n 2^{-k}g(2^{k}x)$$ at $x=v_n$ minus the $(n-1)^{th}$ term of the above sequence at $x=u_n$, all divided by $v_n-u_n$. How can this be equal to the expression on the left hand side? Any help at all would be much appreciated!",,"['real-analysis', 'sequences-and-series', 'analysis', 'derivatives', 'continuity']"
85,If $x=\pi a y^{1/2}$ then why is $\frac{\partial^n}{\partial x^n}=-2\left(\frac{y^{3/2}}{\pi a}\right)^n \frac{\partial^n}{\partial y^n}$?,If  then why is ?,x=\pi a y^{1/2} \frac{\partial^n}{\partial x^n}=-2\left(\frac{y^{3/2}}{\pi a}\right)^n \frac{\partial^n}{\partial y^n},"While I was reading this question , I was surprised that the transformation of a 'simple' differential operator $\displaystyle \frac{\partial^n}{\partial x^n}$ by substituting $x=\pi a y^{\frac{1}{2}} $ so that $$\displaystyle\frac{\partial^n}{\partial x^n}=-2\left(\frac{y^{\frac{3}{2}}}{\pi a}\right)^n \frac{\partial^n}{\partial y^n}$$ stumped me; I'm not sure how he derived it. I actually have a simpler question: given $\displaystyle\frac{d^n}{dx^n}$, how will it transform if $x=cu$ where $c$ is a constant? Edit: Here's a very relevant example from wikipedia , I think that the example in the article is essentially the same kind of thing that the OP in the reference link above did. The problem is that I don't really know how to manipulate operators, if I'm allowed to use a ""physicist's"" argument I'd say that for some differentiable function $f$ and if $u=x/c$: $$ \frac{df}{dx}=\frac{du}{dx}\frac{df}{du}=\frac{1}{c}\frac{df}{du} $$  $$ \therefore \frac{d}{dx}=\frac{1}{c}\frac{d}{du}. $$ Is this even remotely correct? This only addresses the case $n=1$ though, doing the same for latter values seems harder. Edit #2: The final answer from the original link seems incorrect, the OP's result is: $$ \frac{\partial^n}{\partial y^n}e^{-\frac{\pi^2a^2}{y}}=\frac{1}{2}e^{-\frac{\pi^2a^2}{y}}(-1)^{n+1}H_n(\pi a y^{-\frac{1}{2}})\left(\frac{y^{\frac{3}{2}}}{\pi a}\right)^n $$ which is obviously wrong for the smaller values $n=0,1$ (since $H_1(x)=2x)$.","While I was reading this question , I was surprised that the transformation of a 'simple' differential operator $\displaystyle \frac{\partial^n}{\partial x^n}$ by substituting $x=\pi a y^{\frac{1}{2}} $ so that $$\displaystyle\frac{\partial^n}{\partial x^n}=-2\left(\frac{y^{\frac{3}{2}}}{\pi a}\right)^n \frac{\partial^n}{\partial y^n}$$ stumped me; I'm not sure how he derived it. I actually have a simpler question: given $\displaystyle\frac{d^n}{dx^n}$, how will it transform if $x=cu$ where $c$ is a constant? Edit: Here's a very relevant example from wikipedia , I think that the example in the article is essentially the same kind of thing that the OP in the reference link above did. The problem is that I don't really know how to manipulate operators, if I'm allowed to use a ""physicist's"" argument I'd say that for some differentiable function $f$ and if $u=x/c$: $$ \frac{df}{dx}=\frac{du}{dx}\frac{df}{du}=\frac{1}{c}\frac{df}{du} $$  $$ \therefore \frac{d}{dx}=\frac{1}{c}\frac{d}{du}. $$ Is this even remotely correct? This only addresses the case $n=1$ though, doing the same for latter values seems harder. Edit #2: The final answer from the original link seems incorrect, the OP's result is: $$ \frac{\partial^n}{\partial y^n}e^{-\frac{\pi^2a^2}{y}}=\frac{1}{2}e^{-\frac{\pi^2a^2}{y}}(-1)^{n+1}H_n(\pi a y^{-\frac{1}{2}})\left(\frac{y^{\frac{3}{2}}}{\pi a}\right)^n $$ which is obviously wrong for the smaller values $n=0,1$ (since $H_1(x)=2x)$.",,"['calculus', 'derivatives', 'partial-derivative', 'coordinate-systems']"
86,Integrating Factor - Exact Equation problem.,Integrating Factor - Exact Equation problem.,,"I have stumbled with a problem I can't seem to solve. $$(x^2 - y ^2)dx - 5xy dy = 0$$ We know that $$u(x,y) = \frac{1}{(x M + y N)}$$ if the equation is HDE (Which it is..I believe). Excuse my notation, Im not very good at the edit part. $$M = \frac{1}{(x(x^2 - y^2)},\sim N = \frac{1}{y(-5 xy)}$$ Now we know that $\frac{dM}{dy} = \frac{dN}{dx}$ Is it me or the result isn't equivalent? If it is, what is it? Thank you for your time and help.","I have stumbled with a problem I can't seem to solve. $$(x^2 - y ^2)dx - 5xy dy = 0$$ We know that $$u(x,y) = \frac{1}{(x M + y N)}$$ if the equation is HDE (Which it is..I believe). Excuse my notation, Im not very good at the edit part. $$M = \frac{1}{(x(x^2 - y^2)},\sim N = \frac{1}{y(-5 xy)}$$ Now we know that $\frac{dM}{dy} = \frac{dN}{dx}$ Is it me or the result isn't equivalent? If it is, what is it? Thank you for your time and help.",,"['integration', 'ordinary-differential-equations', 'derivatives']"
87,Integrate via substitution and derivation rule,Integrate via substitution and derivation rule,,"I have to solve this integral $$\int_{-r}^{+r}\int_{-\sqrt{r^2-x^2}}^{+\sqrt{r^2-x^2}} \sqrt{1-\frac{x^2+y^2}{x^2+y^2-r^2}} \operatorname d y \operatorname d x$$ with substitution and then the trick that $\dfrac 1 {\sqrt{1-x^2}} = \dfrac{\mathsf d\;\arcsin(x)}{\mathsf d\;x\qquad\quad\;\,} $ can someone give me a tip on what I should substitute in order to continue?","I have to solve this integral $$\int_{-r}^{+r}\int_{-\sqrt{r^2-x^2}}^{+\sqrt{r^2-x^2}} \sqrt{1-\frac{x^2+y^2}{x^2+y^2-r^2}} \operatorname d y \operatorname d x$$ with substitution and then the trick that $\dfrac 1 {\sqrt{1-x^2}} = \dfrac{\mathsf d\;\arcsin(x)}{\mathsf d\;x\qquad\quad\;\,} $ can someone give me a tip on what I should substitute in order to continue?",,"['integration', 'trigonometry', 'derivatives']"
88,A question about two common definitions,A question about two common definitions,,"Two definitions make me puzzled ! 1. The definition of $\textbf{Functions Differentiable at a Point}$ : A function $f$ defined in  a neighborhood $(x_{0}-\delta,x_{0}+\delta)$of a point $x_{0}$, if  $$\lim_{x\rightarrow {x}_{0}}\frac{f(x)-f({x}_{0})}{x-{x}_{0}}=A\in\mathbb{R}$$ then     We called the  function $f$ is differentiable at the point $x_{0}$! 2. The defintion of  $\textbf{Functions Continuous at a Point}$: Let $X \subset \mathbb{R},f(x):X\rightarrow \mathbb{R},x_{0}\in X$, if $$\forall \epsilon >0, \exists\delta>0;s.t.\forall x\in X,|x-x_{0}|<\delta \Rightarrow |f(x)-f(x_{0})|<\epsilon .$$  then We called the  function $f$ is Continuous at the point $x_{0}$! My confusion: $\textbf{A isolated point} \quad\hat{x}\in X,$ according to The defintion of  $\textbf{Functions Continuous at a Point}$,  $f$ is Continuous at the point $\hat{x}$.But $f$ has no vaules in  $\hat{x}$ neighborhood $(\hat{x}-\delta_{\hat{x}},\hat{x}+\delta_{\hat{x}})$ of the point $\hat{x}$,so on the basis of The definition of  $\textbf{Functions Differentiable at a Point}$, $f$ is not differentiable at the point $\hat{x}$. But the conclusion drawn from the results described above is contradicted against $\textbf{f(x) differentiable at a point must be continuous at the point}.$ I need clartity to eliminate the confusion!Any of your help will be appreciated! I am sorry I made some obvious mistakes! There is no contradiction!","Two definitions make me puzzled ! 1. The definition of $\textbf{Functions Differentiable at a Point}$ : A function $f$ defined in  a neighborhood $(x_{0}-\delta,x_{0}+\delta)$of a point $x_{0}$, if  $$\lim_{x\rightarrow {x}_{0}}\frac{f(x)-f({x}_{0})}{x-{x}_{0}}=A\in\mathbb{R}$$ then     We called the  function $f$ is differentiable at the point $x_{0}$! 2. The defintion of  $\textbf{Functions Continuous at a Point}$: Let $X \subset \mathbb{R},f(x):X\rightarrow \mathbb{R},x_{0}\in X$, if $$\forall \epsilon >0, \exists\delta>0;s.t.\forall x\in X,|x-x_{0}|<\delta \Rightarrow |f(x)-f(x_{0})|<\epsilon .$$  then We called the  function $f$ is Continuous at the point $x_{0}$! My confusion: $\textbf{A isolated point} \quad\hat{x}\in X,$ according to The defintion of  $\textbf{Functions Continuous at a Point}$,  $f$ is Continuous at the point $\hat{x}$.But $f$ has no vaules in  $\hat{x}$ neighborhood $(\hat{x}-\delta_{\hat{x}},\hat{x}+\delta_{\hat{x}})$ of the point $\hat{x}$,so on the basis of The definition of  $\textbf{Functions Differentiable at a Point}$, $f$ is not differentiable at the point $\hat{x}$. But the conclusion drawn from the results described above is contradicted against $\textbf{f(x) differentiable at a point must be continuous at the point}.$ I need clartity to eliminate the confusion!Any of your help will be appreciated! I am sorry I made some obvious mistakes! There is no contradiction!",,"['real-analysis', 'general-topology']"
89,"reduction order method $(2-x)y'''+(2x-3)y''-xy'+y=0$, $y_1=e^x$","reduction order method ,",(2-x)y'''+(2x-3)y''-xy'+y=0 y_1=e^x,"$(2-x)y'''+(2x-3)y''-xy'+y=0$, $x<2$ being $y_1=e^x$ a solution for the homogeneous equation. making $y=ue^x$ i came to $u''+u'''=0$ making $u''=w$ ,  $w'+w=0$ this way $w=e^{-x}*c_1$ and $y=c_1+c_2xe^x+c_3e^x$. However the right solution is $y=c_1x+c_2xe^x+c_3e^x$. Where have i made the mistake? Thanks","$(2-x)y'''+(2x-3)y''-xy'+y=0$, $x<2$ being $y_1=e^x$ a solution for the homogeneous equation. making $y=ue^x$ i came to $u''+u'''=0$ making $u''=w$ ,  $w'+w=0$ this way $w=e^{-x}*c_1$ and $y=c_1+c_2xe^x+c_3e^x$. However the right solution is $y=c_1x+c_2xe^x+c_3e^x$. Where have i made the mistake? Thanks",,"['calculus', 'integration', 'ordinary-differential-equations', 'derivatives']"
90,General Solution to Almost Riccati Like Equation,General Solution to Almost Riccati Like Equation,,"Consider the differential equation $$ y' = a_0(x) + a_1(x)y + a_2(x)\frac{1}{y}$$ I am attempting to find the general solution to this. One thing I can note is that the entire equation can be rewritten as $$ y' = \frac{a_2(x) + a_0(x)y + a_1(x)y^2}{y} $$ Thus allowing us to state $$ y y' = a_2(x) + a_0(x)y + a_1(x)y^2$$ I have no idea how to progress correctly from here. By General Solution, I mean to ask if this can be re-written as a linear ODE.","Consider the differential equation $$ y' = a_0(x) + a_1(x)y + a_2(x)\frac{1}{y}$$ I am attempting to find the general solution to this. One thing I can note is that the entire equation can be rewritten as $$ y' = \frac{a_2(x) + a_0(x)y + a_1(x)y^2}{y} $$ Thus allowing us to state $$ y y' = a_2(x) + a_0(x)y + a_1(x)y^2$$ I have no idea how to progress correctly from here. By General Solution, I mean to ask if this can be re-written as a linear ODE.",,"['calculus', 'integration', 'ordinary-differential-equations', 'derivatives', 'functional-equations']"
91,Power series for inverse of truncated power series of $e$,Power series for inverse of truncated power series of,e,"Let $T_n(x)=\sum_{k=0}^{n}\frac{x^k}{k!}$.  I'm looking at the function $$f(x)=\frac{1}{T_n(x)}$$ and I would like to find the power series of this particular function.  I know I can use Maclaurin Series to find it, so taking derivatives could help here; $$f'(x)=\frac{d}{dx}\left[\frac{1}{T_n(x)}\right]=\frac{-T_{n-1}(x)}{T_n(x)^2}$$ Obviously $f(0)=1$ and now $f'(0)=-1$... However I don't want to keep doing this to find a pattern.  Is there a quicker way that I'm not seeing? Edit:  It is helpful that $T_{n-k}(0)=1, \forall n>k$ but using the quotient rule for derivatives makes this difficult because how quickly the product rule expands after very few derivatives....","Let $T_n(x)=\sum_{k=0}^{n}\frac{x^k}{k!}$.  I'm looking at the function $$f(x)=\frac{1}{T_n(x)}$$ and I would like to find the power series of this particular function.  I know I can use Maclaurin Series to find it, so taking derivatives could help here; $$f'(x)=\frac{d}{dx}\left[\frac{1}{T_n(x)}\right]=\frac{-T_{n-1}(x)}{T_n(x)^2}$$ Obviously $f(0)=1$ and now $f'(0)=-1$... However I don't want to keep doing this to find a pattern.  Is there a quicker way that I'm not seeing? Edit:  It is helpful that $T_{n-k}(0)=1, \forall n>k$ but using the quotient rule for derivatives makes this difficult because how quickly the product rule expands after very few derivatives....",,"['calculus', 'derivatives', 'power-series']"
92,Show that $\lim_{n\to\infty}\sum_{k=1}^n\bigl|k\bigl(f\bigl(\frac{1}{k}\bigr)-f\bigl(-\frac{1}{k}\bigr)\bigr)-2f'(0)\bigr|$ exists,Show that  exists,\lim_{n\to\infty}\sum_{k=1}^n\bigl|k\bigl(f\bigl(\frac{1}{k}\bigr)-f\bigl(-\frac{1}{k}\bigr)\bigr)-2f'(0)\bigr|,"Suppose $f\in C^3[-1,1]$, show that $$\lim_{n\to\infty}\sum_{k=1}^n\left|k\left(f\left(\frac{1}{k}\right)-f\left(-\frac{1}{k}\right)\right)-2f'(0)\right|$$ exists. I realized that $$\lim_{k\to\infty}k\left(f\left(\frac{1}{k}\right)-f\left(-\frac{1}{k}\right)\right)-2f'(0)=0$$ by the definition of derivative. I didn't really see how to use the condition $f\in C^3[-1,1]$. Does anybody have any hints?","Suppose $f\in C^3[-1,1]$, show that $$\lim_{n\to\infty}\sum_{k=1}^n\left|k\left(f\left(\frac{1}{k}\right)-f\left(-\frac{1}{k}\right)\right)-2f'(0)\right|$$ exists. I realized that $$\lim_{k\to\infty}k\left(f\left(\frac{1}{k}\right)-f\left(-\frac{1}{k}\right)\right)-2f'(0)=0$$ by the definition of derivative. I didn't really see how to use the condition $f\in C^3[-1,1]$. Does anybody have any hints?",,"['real-analysis', 'sequences-and-series', 'limits', 'derivatives']"
93,What is partial derivative of distance to line equation?,What is partial derivative of distance to line equation?,,"The distance from a point to a line is given by the equation: $$ \mbox{distance}\ = \frac{|ax + by +c|}{\sqrt{a^2 + b^2}}$$ What are the partial derivatives of this equation with respect to $a$, $b$, and $c$? Since this function has an absolute value in it, I think at some points the derivative is undefined. Thanks","The distance from a point to a line is given by the equation: $$ \mbox{distance}\ = \frac{|ax + by +c|}{\sqrt{a^2 + b^2}}$$ What are the partial derivatives of this equation with respect to $a$, $b$, and $c$? Since this function has an absolute value in it, I think at some points the derivative is undefined. Thanks",,"['derivatives', 'partial-derivative']"
94,Show that if $\lim_{x\to a}f'(x) = A$ then $f'(a)$ exists and equals A [duplicate],Show that if  then  exists and equals A [duplicate],\lim_{x\to a}f'(x) = A f'(a),"This question already has answers here : Prove that $f'(a)=\lim_{x\rightarrow a}f'(x)$. (6 answers) Closed 6 years ago . I ran across this problem in my Analysis class and can't seem to come up with a good solution. Here's the question: Show that if $\lim_{x\to a}f'(x) = A$ then $f'(a)$ exists and equals $A$. $f$ is continuous on $[a,b]$ and differentiable on $(a,b)$. I tried applying the Mean Value theorem like this: There exists a $c \in (a, x)$ st $f(x)-f(a)=f'(c)(x-a) \implies \frac{f(x)-f(a)}{x-a}=f'(c) \implies f'(a)=\lim_{x\to a}\frac{f(x)-f(a)}{x-a}=\lim_{c\to a}f'(c)=A$ However, I think this is wrong. I am pretty sure the Mean Value theorem is needed, but my application of it seems to be incorrect. Would somebody be able to point me in the right direction?","This question already has answers here : Prove that $f'(a)=\lim_{x\rightarrow a}f'(x)$. (6 answers) Closed 6 years ago . I ran across this problem in my Analysis class and can't seem to come up with a good solution. Here's the question: Show that if $\lim_{x\to a}f'(x) = A$ then $f'(a)$ exists and equals $A$. $f$ is continuous on $[a,b]$ and differentiable on $(a,b)$. I tried applying the Mean Value theorem like this: There exists a $c \in (a, x)$ st $f(x)-f(a)=f'(c)(x-a) \implies \frac{f(x)-f(a)}{x-a}=f'(c) \implies f'(a)=\lim_{x\to a}\frac{f(x)-f(a)}{x-a}=\lim_{c\to a}f'(c)=A$ However, I think this is wrong. I am pretty sure the Mean Value theorem is needed, but my application of it seems to be incorrect. Would somebody be able to point me in the right direction?",,"['real-analysis', 'derivatives']"
95,Problem integrating when attempting a solution with the Poincaré Lemma,Problem integrating when attempting a solution with the Poincaré Lemma,,"d) This is part I am having troubles with. I get that $$ \hat{\mathbb{X}}_t = \left(\frac{\partial}{\partial t}\hat{\Phi}_t \right) \Phi_t^{-1}  = \left(\frac{\partial}{\partial t}\hat{\Phi}_t \right) (x/t,y/t,z)  = (x/t,y/t,0) $$ so, $$\begin{align} i_{\hat{\mathbb{X}}_t}\beta &= i_{\hat{\mathbb{X}}_t} \left[\frac{3xz}{r^5} dy\wedge dz + \frac{3yz}{r^5} dz\wedge dx +\frac{2z^2-x^2-y^2}{r^5}dx \wedge dy \right] \\ &= \frac{3xz}{r^5}\left[\frac{y}{t}dz\right]+\frac{3yz}{r^5}\left[\frac{-x}{t}dz\right]+\frac{2z^2-x^2-y^2}{r^5}\left[\frac{x}{t}dy-\frac{y}{t}dx\right] \\ &= \frac{2z^2-x^2-y^2}{r^5}\left[\frac{x}{t}dy-\frac{y}{t}dx\right] \end{align}$$ Then  and so $$\begin{align} \Phi_t^*\left(i_{\hat{\mathbb{X}}_t}\beta\right) &= \frac{2z^2-t^2x^2-t^2y^2}{(t^2x^2+t^2y^2+z^2)^{5/2}}\left[txdy-tydx \right] \\ &= \frac{2z^2-t^2x^2-t^2y^2}{(t^2x^2+t^2y^2+z^2)^{5/2}}txdy - \frac{2z^2-t^2x^2-t^2y^2}{(t^2x^2+t^2y^2+z^2)^{5/2}}tydx \end{align}$$ However I cannot see how to integrate this. Help much appreciated. UPDATE: Apparently the correct answer is $\alpha=(x dy -y dx)/r^3$. This would suggest I am on the right track. The following integral may help $\displaystyle \int_0^w \frac{v^n}{(1+v)^s}dv=\frac{1}{1-s}\frac{w^n}{(1+w)^{s-1}}-\frac{n}{1-s}\int_0^w \frac{v^{n-1}}{(1+v)^{s-1}}dv$ $n>0, s\neq 1,w > -1$ I believe I may be missing something as I have had a number of problems with these type of questions. See my other question on the Poincaré Lemma application.","d) This is part I am having troubles with. I get that $$ \hat{\mathbb{X}}_t = \left(\frac{\partial}{\partial t}\hat{\Phi}_t \right) \Phi_t^{-1}  = \left(\frac{\partial}{\partial t}\hat{\Phi}_t \right) (x/t,y/t,z)  = (x/t,y/t,0) $$ so, $$\begin{align} i_{\hat{\mathbb{X}}_t}\beta &= i_{\hat{\mathbb{X}}_t} \left[\frac{3xz}{r^5} dy\wedge dz + \frac{3yz}{r^5} dz\wedge dx +\frac{2z^2-x^2-y^2}{r^5}dx \wedge dy \right] \\ &= \frac{3xz}{r^5}\left[\frac{y}{t}dz\right]+\frac{3yz}{r^5}\left[\frac{-x}{t}dz\right]+\frac{2z^2-x^2-y^2}{r^5}\left[\frac{x}{t}dy-\frac{y}{t}dx\right] \\ &= \frac{2z^2-x^2-y^2}{r^5}\left[\frac{x}{t}dy-\frac{y}{t}dx\right] \end{align}$$ Then  and so $$\begin{align} \Phi_t^*\left(i_{\hat{\mathbb{X}}_t}\beta\right) &= \frac{2z^2-t^2x^2-t^2y^2}{(t^2x^2+t^2y^2+z^2)^{5/2}}\left[txdy-tydx \right] \\ &= \frac{2z^2-t^2x^2-t^2y^2}{(t^2x^2+t^2y^2+z^2)^{5/2}}txdy - \frac{2z^2-t^2x^2-t^2y^2}{(t^2x^2+t^2y^2+z^2)^{5/2}}tydx \end{align}$$ However I cannot see how to integrate this. Help much appreciated. UPDATE: Apparently the correct answer is $\alpha=(x dy -y dx)/r^3$. This would suggest I am on the right track. The following integral may help $\displaystyle \int_0^w \frac{v^n}{(1+v)^s}dv=\frac{1}{1-s}\frac{w^n}{(1+w)^{s-1}}-\frac{n}{1-s}\int_0^w \frac{v^{n-1}}{(1+v)^{s-1}}dv$ $n>0, s\neq 1,w > -1$ I believe I may be missing something as I have had a number of problems with these type of questions. See my other question on the Poincaré Lemma application.",,"['calculus', 'ordinary-differential-equations', 'differential-geometry', 'derivatives']"
96,Question about length of curve?,Question about length of curve?,,"The question: Find length of curve defined by $\displaystyle  y=2\ln\left[\left(\frac{x}{2}\right)^2-1\right] $ from $x=4$ to $x=6$ Here is the work I have done, but I seem to keep getting it wrong. Are there any suggestions as to what else I can do or what I am doing wrong? $$\frac{dy}{dx} = 2 \ln\left[\left(\frac x2\right)^2-1\right]$$ Then I used the substitution $\displaystyle-u = \left(\frac x2\right)^2 - 1$. The answer I got was $2 \frac 1u \frac x2$. Substituting back in we get $$2 \frac{1}{\frac{x^2}4 - 1} \frac x2 = \frac{4x}{x^2  - 4}$$ Using this arclenght formula I get $$L = \int_4^6 \sqrt{1 + \left(\frac{4x}{x^2  - 4}\right)^2}dx = \int_4^6 \sqrt{1 + \frac{16x^2}{(x^2  - 4)^2}}dx = \int_4^6 1 dx + \int_4^6 \frac{4x}{x^2-4}dx = 1.66 $$ But the correct answer is $2.81$ What did I do wrong? SIDENOTE: Hi I have posed as another user with the same name, but my computer signed me off and I'm not really sure how to get that account back (cannot remember password to that email!)? If anyone has any suggestions or should I just start this account? I am also not familiar with how to add the proper notation, if someone can point me out to that. Thank you.","The question: Find length of curve defined by $\displaystyle  y=2\ln\left[\left(\frac{x}{2}\right)^2-1\right] $ from $x=4$ to $x=6$ Here is the work I have done, but I seem to keep getting it wrong. Are there any suggestions as to what else I can do or what I am doing wrong? $$\frac{dy}{dx} = 2 \ln\left[\left(\frac x2\right)^2-1\right]$$ Then I used the substitution $\displaystyle-u = \left(\frac x2\right)^2 - 1$. The answer I got was $2 \frac 1u \frac x2$. Substituting back in we get $$2 \frac{1}{\frac{x^2}4 - 1} \frac x2 = \frac{4x}{x^2  - 4}$$ Using this arclenght formula I get $$L = \int_4^6 \sqrt{1 + \left(\frac{4x}{x^2  - 4}\right)^2}dx = \int_4^6 \sqrt{1 + \frac{16x^2}{(x^2  - 4)^2}}dx = \int_4^6 1 dx + \int_4^6 \frac{4x}{x^2-4}dx = 1.66 $$ But the correct answer is $2.81$ What did I do wrong? SIDENOTE: Hi I have posed as another user with the same name, but my computer signed me off and I'm not really sure how to get that account back (cannot remember password to that email!)? If anyone has any suggestions or should I just start this account? I am also not familiar with how to add the proper notation, if someone can point me out to that. Thank you.",,"['calculus', 'integration', 'derivatives', 'definite-integrals']"
97,What's the rule for knowns and unknowns when dealing with derivatives,What's the rule for knowns and unknowns when dealing with derivatives,,So a rule of thumb when doing basic algebra is you must have as many equations as you have unknowns. For example: $0=4x+6y^2$ $3x=2\sqrt{y}$ You have two equations and two unknowns and thus can solve for both x and y However I don't know how this rule (if it even applies) works if you have a differential equation instead for example $0=4x+6y^2$ $\frac{dy}{dx}=2x$ Does the second equation still count as one equation or does it introduce additional information that's needed to solve the problem? Does the general (rule 1 equation for 1 unknown) still work for all cases? What about higher order differential equations?,So a rule of thumb when doing basic algebra is you must have as many equations as you have unknowns. For example: $0=4x+6y^2$ $3x=2\sqrt{y}$ You have two equations and two unknowns and thus can solve for both x and y However I don't know how this rule (if it even applies) works if you have a differential equation instead for example $0=4x+6y^2$ $\frac{dy}{dx}=2x$ Does the second equation still count as one equation or does it introduce additional information that's needed to solve the problem? Does the general (rule 1 equation for 1 unknown) still work for all cases? What about higher order differential equations?,,"['calculus', 'derivatives']"
98,Find the derivative of a piecewise function.,Find the derivative of a piecewise function.,,"I would just like to know if my proof here is valid. I know I left out some computational details, but I'm more concerned about the structure of the proof than those details.","I would just like to know if my proof here is valid. I know I left out some computational details, but I'm more concerned about the structure of the proof than those details.",,"['limits', 'derivatives']"
99,Finding the Lie derivative of a 2-form exercise,Finding the Lie derivative of a 2-form exercise,,"Let $\beta=-x dx \wedge dy + ydy \wedge dz$. The vector field is $X=(y,0,z)$.Find the Lie derivative. I try that $\begin{align}L_X \beta &=L_X (-x dx \wedge dy + ydy \wedge dz) = -x L_X(dx \wedge dy)+ y L_X(dy \wedge dz) \\ &= -x [ L_X(dx) \wedge dy + dx \wedge L_X(dy) ]+ y [ L_X(dy) \wedge dz + dy \wedge L_X(dz) ] \\ &= -x [ dL_X(x) \wedge dy + dx \wedge dL_X(y) ]+ y [ dL_X(y) \wedge dz + dy \wedge dL_X(z) ] \end{align}$ but how do I evaluate this further. I cannot see what $L_X(x)$ for example would be?","Let $\beta=-x dx \wedge dy + ydy \wedge dz$. The vector field is $X=(y,0,z)$.Find the Lie derivative. I try that $\begin{align}L_X \beta &=L_X (-x dx \wedge dy + ydy \wedge dz) = -x L_X(dx \wedge dy)+ y L_X(dy \wedge dz) \\ &= -x [ L_X(dx) \wedge dy + dx \wedge L_X(dy) ]+ y [ L_X(dy) \wedge dz + dy \wedge L_X(dz) ] \\ &= -x [ dL_X(x) \wedge dy + dx \wedge dL_X(y) ]+ y [ dL_X(y) \wedge dz + dy \wedge dL_X(z) ] \end{align}$ but how do I evaluate this further. I cannot see what $L_X(x)$ for example would be?",,"['calculus', 'differential-geometry', 'derivatives', 'partial-derivative', 'differential-forms']"
