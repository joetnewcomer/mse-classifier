,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Uniform limit of uniformly continuous functions,Uniform limit of uniformly continuous functions,,"Let $f_n:\mathbb{R}\to\mathbb{R}$ be a sequence of uniformly   continuous functions. Assume that $f_n$ converges uniformly on all   bounded intervals $[a,b]$ to a function $f$, i.e.   $\displaystyle\lim_{n\to \infty}\sup_{a\leq x\leq b}|f_n(x)-f(x)|=0$    for all $a<b$. Is the limit $f$ also uniformly continuous ? In the case where the convergence is uniform on the whole real line, I can prove that the limit is also uniformly continuous.","Let $f_n:\mathbb{R}\to\mathbb{R}$ be a sequence of uniformly   continuous functions. Assume that $f_n$ converges uniformly on all   bounded intervals $[a,b]$ to a function $f$, i.e.   $\displaystyle\lim_{n\to \infty}\sup_{a\leq x\leq b}|f_n(x)-f(x)|=0$    for all $a<b$. Is the limit $f$ also uniformly continuous ? In the case where the convergence is uniform on the whole real line, I can prove that the limit is also uniformly continuous.",,"['real-analysis', 'analysis', 'convergence-divergence', 'uniform-convergence', 'uniform-continuity']"
1,Convergence a.e. and of norms implies that in $L^1$ norm,Convergence a.e. and of norms implies that in  norm,L^1,"Suppose $f_n$ is as sequence of functions in $L^1[0,1]$ such that $f_n$ converges pointwise a.e. to $f\in L^1[0,1]$. Suppose also that $\int \vert f_n\vert \rightarrow \int \vert f\vert$. Is it true that $f_n$ converges to $f$ in the $L^1$ norm? From Javaman's comment below: $|f_n-f|\leqslant |f_n|+|f|$. So DCT applies. Since $f_n\rightarrow f$ a.e. we have $\lim_n\int |f_n-f|=0.$ i.e $\Vert f_n-f\Vert \rightarrow 0.$","Suppose $f_n$ is as sequence of functions in $L^1[0,1]$ such that $f_n$ converges pointwise a.e. to $f\in L^1[0,1]$. Suppose also that $\int \vert f_n\vert \rightarrow \int \vert f\vert$. Is it true that $f_n$ converges to $f$ in the $L^1$ norm? From Javaman's comment below: $|f_n-f|\leqslant |f_n|+|f|$. So DCT applies. Since $f_n\rightarrow f$ a.e. we have $\lim_n\int |f_n-f|=0.$ i.e $\Vert f_n-f\Vert \rightarrow 0.$",,['real-analysis']
2,Composition of power series,Composition of power series,,"Does anyone know how to derive a formula for the coefficients. That is if, $f(x)=\sum _{n=0}^{\infty } a_nx^n$ and $g(x)=\sum _{n=0}^{\infty } b_nx^n$ suppose the composition is an analytic function, $h(x)=f(g(x))=\sum _{n=0}^{\infty } c_nx^n$ Is there an expression we can find for the coefficients $c_n$ in terms of $a_n$ and $b_n$ ? Can someone show me how its derived. I know we could substitute $g$ into $f$ and collect powers of $x$ . But I believe a formula for general n may be written down.","Does anyone know how to derive a formula for the coefficients. That is if, and suppose the composition is an analytic function, Is there an expression we can find for the coefficients in terms of and ? Can someone show me how its derived. I know we could substitute into and collect powers of . But I believe a formula for general n may be written down.",f(x)=\sum _{n=0}^{\infty } a_nx^n g(x)=\sum _{n=0}^{\infty } b_nx^n h(x)=f(g(x))=\sum _{n=0}^{\infty } c_nx^n c_n a_n b_n g f x,"['real-analysis', 'complex-analysis', 'power-series', 'analyticity']"
3,"Prove that there exists $f,g : \mathbb{R}$ to $\mathbb{R}$ such that $f(g(x))$ is strictly increasing and $g(f(x))$ is strictly decreasing.",Prove that there exists  to  such that  is strictly increasing and  is strictly decreasing.,"f,g : \mathbb{R} \mathbb{R} f(g(x)) g(f(x))","Prove that there exists $f,g : \mathbb{R}$ to $\mathbb{R}$ such that $f(g(x))$ is strictly increasing and $g(f(x))$ is strictly decreasing. I tried cases by taking $f(x)$ as an increasing function and $g(x)$ as a decreasing function then I am getting both $f(g(x))$ and $g(f(x))$ as decreasing functions. Further I took both of them as increasing functions, but none of them are yielding results. Help","Prove that there exists $f,g : \mathbb{R}$ to $\mathbb{R}$ such that $f(g(x))$ is strictly increasing and $g(f(x))$ is strictly decreasing. I tried cases by taking $f(x)$ as an increasing function and $g(x)$ as a decreasing function then I am getting both $f(g(x))$ and $g(f(x))$ as decreasing functions. Further I took both of them as increasing functions, but none of them are yielding results. Help",,[]
4,"Proof of ""the continuous image of a connected set is connected""","Proof of ""the continuous image of a connected set is connected""",,"None of the existing questions is exactly answering my question so I'm posting a new question, but feel free to refer me to some already answered question! In Rudin Theorem 4.22, we know that If $f$ is a continuous mapping of a metric space $X$ into a metric space $Y$, and $E$ is a connected subset of $X$, then $f(E)$ is connected. In the proof, we started with consider $f(E) = A \cup B$, where $A$ and $B$ are nonempty separated subsets. Then put $G = E \cap f^{-1}(A)$ and $H = E \cap f^{-1}(B)$. Then Rudin is claiming that $E = G \cup H$. I'm a little suspicious about this. What if $f$ is non-surjective, then $f^{-1}(A) \cup f^{-1}(B)$  is only a proper subset of $E$? Is there a property of $f$ being continuous that forces $f$ to be 1-1?","None of the existing questions is exactly answering my question so I'm posting a new question, but feel free to refer me to some already answered question! In Rudin Theorem 4.22, we know that If $f$ is a continuous mapping of a metric space $X$ into a metric space $Y$, and $E$ is a connected subset of $X$, then $f(E)$ is connected. In the proof, we started with consider $f(E) = A \cup B$, where $A$ and $B$ are nonempty separated subsets. Then put $G = E \cap f^{-1}(A)$ and $H = E \cap f^{-1}(B)$. Then Rudin is claiming that $E = G \cup H$. I'm a little suspicious about this. What if $f$ is non-surjective, then $f^{-1}(A) \cup f^{-1}(B)$  is only a proper subset of $E$? Is there a property of $f$ being continuous that forces $f$ to be 1-1?",,"['real-analysis', 'continuity', 'connectedness']"
5,The series of reciprocals of the least common multiples of integers $X_1<X_2<\dots$ converges,The series of reciprocals of the least common multiples of integers  converges,X_1<X_2<\dots,"Let $X_1, X_2,\dots$ be a sequence of strictly increasing positive integers. For each $n\ge1$, let $W_n$ be the least common multiple of the first $n$ terms $X_1, X_2,\dots, X_n$ of the sequence. I need to prove the following statement: The series $1/W_1+1/W_2+\dots+1/W_n\;(n\to\infty)$ is a   convergent series. I tried several ways, including the hint given below, but I have no luck to overcome this problem. I'd like to learn the methods that can be used to prove that such a series converges.","Let $X_1, X_2,\dots$ be a sequence of strictly increasing positive integers. For each $n\ge1$, let $W_n$ be the least common multiple of the first $n$ terms $X_1, X_2,\dots, X_n$ of the sequence. I need to prove the following statement: The series $1/W_1+1/W_2+\dots+1/W_n\;(n\to\infty)$ is a   convergent series. I tried several ways, including the hint given below, but I have no luck to overcome this problem. I'd like to learn the methods that can be used to prove that such a series converges.",,"['real-analysis', 'sequences-and-series', 'number-theory', 'least-common-multiple']"
6,"Prob. 6 (d), Chap. 1, in Baby Rudin, 3rd ed: How to complete this proof?","Prob. 6 (d), Chap. 1, in Baby Rudin, 3rd ed: How to complete this proof?",,"Here is Prob. 6, Chap. 1, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Fix $b>1$ . Problem 6(a): Let $m$ , $n$ , $p$ , $q$ be integers such that $n>0$ , $q>0$ , and $r = m/n = p/q$ . Then [I've managed to] prove that $$ b^{m/n} = b^{p/q}.$$ So we can reasonably define $b^r$ as $$b^r \colon = \sqrt[n]{b^m}.$$ Problem 6(b) [From this definition we can] prove that $$ b^{r+s} = b^r \cdot b^s, $$ where $r$ and $s$ are any rational numbers. Problem 6(c): Now for a rational number $r$ , let the set $B(r)$ be defined as follows: $$B(r) \colon= \{ \, b^t \colon \, t\in \mathbb{Q}, \, t \leq r \, \}. $$ Then it is clear that $$b^r = \sup B(r).$$ So for every real $x$ , we can define $b^x$ as follows: $$b^x \colon= \sup B(x) = \sup \big\{ \, b^t \, \colon \, t \in \mathbb{Q}, \, t \leq x \, \big\}. $$ Problem 6(d): Using this definition, (we are required to) prove that, for every pair of real $x$ and $y$ , the equation $$b^{x+y} = b^x \cdot b^y$$ holds. My Attempt: If $r$ , $s \in \mathbb{Q}$ such that $r \leq x$ and $s \leq y$ , then $r+s \in \mathbb{Q}$ also and $r+s \leq x+y$ so that we can write $$ B(x) \cdot B(y)  = \{ \, b^r \cdot b^s \, \colon \, r \in \mathbb{Q}, s\in \mathbb{Q}, \, r \leq x, s\leq y \, \} \subseteq \{ \, b^t \, \colon \, t \in \mathbb{Q}, \, t \leq x+y \, \} = B(x+y),$$ and hence $$b^x \cdot b^y = \sup B(x) \cdot \sup B(y) = \sup \{ \, b^r \, \colon \, r \in \mathbb{Q}, \, r \leq x \, \} \cdot \sup \{ \, b^s \, \colon \, s \in \mathbb{Q}, \, s \leq y \, \} = sup \{ \, b^{r+s} \, \colon \, r \in \mathbb{Q}, \, s\in \mathbb{Q}, \,  r \leq x, \, s \leq y \, \} \leq \sup \{ \, b^t \, \colon \, t \in \mathbb{Q}, \, t \leq x+y \, \} = \sup B(x+y) = b^{x+y}.$$ Here we have used the following definition: Given two non-empty sets $U$ and $V$ , say of real numbers, we define the set $U \cdot V$ as follows: $$U \cdot V \colon= \{ \, u \cdot v \, \colon \, u \in U, \, v \in V \, \}.$$ And we have also used the fact that if $W$ and $Z$ are two non-empty bounded above subsets of the set of positive real numbers such that $W \subseteq Z$ , then we must have $$ \sup W \leq \sup Z.$$ So far, we have shown that $$b^x \cdot b^y \leq b^{x+y}.$$ Now how to prove the reverse inequality using the machinery developed above? That is, how to prove that $$b^{x+y} \leq b^x \cdot b^y?$$","Here is Prob. 6, Chap. 1, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Fix . Problem 6(a): Let , , , be integers such that , , and . Then [I've managed to] prove that So we can reasonably define as Problem 6(b) [From this definition we can] prove that where and are any rational numbers. Problem 6(c): Now for a rational number , let the set be defined as follows: Then it is clear that So for every real , we can define as follows: Problem 6(d): Using this definition, (we are required to) prove that, for every pair of real and , the equation holds. My Attempt: If , such that and , then also and so that we can write and hence Here we have used the following definition: Given two non-empty sets and , say of real numbers, we define the set as follows: And we have also used the fact that if and are two non-empty bounded above subsets of the set of positive real numbers such that , then we must have So far, we have shown that Now how to prove the reverse inequality using the machinery developed above? That is, how to prove that","b>1 m n p q n>0 q>0 r = m/n = p/q  b^{m/n} = b^{p/q}. b^r b^r \colon = \sqrt[n]{b^m}.  b^{r+s} = b^r \cdot b^s,  r s r B(r) B(r) \colon= \{ \, b^t \colon \, t\in \mathbb{Q}, \, t \leq r \, \}.  b^r = \sup B(r). x b^x b^x \colon= \sup B(x) = \sup \big\{ \, b^t \, \colon \, t \in \mathbb{Q}, \, t \leq x \, \big\}.  x y b^{x+y} = b^x \cdot b^y r s \in \mathbb{Q} r \leq x s \leq y r+s \in \mathbb{Q} r+s \leq x+y  B(x) \cdot B(y)  = \{ \, b^r \cdot b^s \, \colon \, r \in \mathbb{Q}, s\in \mathbb{Q}, \, r \leq x, s\leq y \, \} \subseteq \{ \, b^t \, \colon \, t \in \mathbb{Q}, \, t \leq x+y \, \} = B(x+y), b^x \cdot b^y = \sup B(x) \cdot \sup B(y) = \sup \{ \, b^r \, \colon \, r \in \mathbb{Q}, \, r \leq x \, \} \cdot \sup \{ \, b^s \, \colon \, s \in \mathbb{Q}, \, s \leq y \, \} = sup \{ \, b^{r+s} \, \colon \, r \in \mathbb{Q}, \, s\in \mathbb{Q}, \,  r \leq x, \, s \leq y \, \} \leq \sup \{ \, b^t \, \colon \, t \in \mathbb{Q}, \, t \leq x+y \, \} = \sup B(x+y) = b^{x+y}. U V U \cdot V U \cdot V \colon= \{ \, u \cdot v \, \colon \, u \in U, \, v \in V \, \}. W Z W \subseteq Z  \sup W \leq \sup Z. b^x \cdot b^y \leq b^{x+y}. b^{x+y} \leq b^x \cdot b^y?","['calculus', 'real-analysis', 'algebra-precalculus', 'analysis']"
7,How to calculate $2^{\sqrt{2}}$ by hand efficiently?,How to calculate  by hand efficiently?,2^{\sqrt{2}},"I've been trying to calculate $2^{\sqrt{2}}$ by hand efficiently, but whatever I've tried to do so far fails at some point because I need to use many decimals of $\sqrt{2}$ or $\log(2)$ to get a roughly good approximation. Is it even possible to do so without facing irrational expressions like $\sqrt{2}$ or $\log(2)$ in our calculations? EDIT It seems like no one is paying attention to the requirements in my question at all : (  You are not allowed to use use $\log(2)$ or $\sqrt{2}$ in your answers. Use of continued fractions is allowed. Let me phrase my question in this way: Find an infinite series $\displaystyle \sum_{n=0}^{\infty}a_n$ such that $a_n \in \mathbb{Q}$. There exists at least one such series, namely, the series that is obtained by writing the decimal expansion of $2^{\sqrt{2}}$, but that series is good for nothing because if we already knew the decimal expansion of $2^{\sqrt{2}}$ then we didn't need to be after approximating $2^{\sqrt{2}}$ by using infinite series. Look at the following series: $\displaystyle e = \sum_{n=0}^{\infty}\frac{1}{n!} = 2 + \frac{1}{2} + \frac{1}{6} + \frac{1}{24}+\frac{1}{120}+\frac{1}{720} + \cdots$ $\displaystyle \pi = \frac{4}{1} - \frac{4}{3} + \frac{4}{5} - \frac{4}{7} + \frac{4}{9} - \frac{4}{11} + \cdots$ $\displaystyle \pi = 3 + \frac{4}{2\times 3 \times 4} - \frac{4}{ 4 \times 5 \times 6} + \frac{4}{6 \times 7 \times 8} - \frac{4}{8 \times 9 \times 10} + \cdots $ Both $e$ and $\pi$ are irrational transcendental numbers. But we have found non-trivial infinite series with rational terms for them. Can someone possibly find a similar series for $2^{\sqrt{2}}$? This is something I proposed as a challenge to myself and I failed, now I wonder if someone on here could tackle it.","I've been trying to calculate $2^{\sqrt{2}}$ by hand efficiently, but whatever I've tried to do so far fails at some point because I need to use many decimals of $\sqrt{2}$ or $\log(2)$ to get a roughly good approximation. Is it even possible to do so without facing irrational expressions like $\sqrt{2}$ or $\log(2)$ in our calculations? EDIT It seems like no one is paying attention to the requirements in my question at all : (  You are not allowed to use use $\log(2)$ or $\sqrt{2}$ in your answers. Use of continued fractions is allowed. Let me phrase my question in this way: Find an infinite series $\displaystyle \sum_{n=0}^{\infty}a_n$ such that $a_n \in \mathbb{Q}$. There exists at least one such series, namely, the series that is obtained by writing the decimal expansion of $2^{\sqrt{2}}$, but that series is good for nothing because if we already knew the decimal expansion of $2^{\sqrt{2}}$ then we didn't need to be after approximating $2^{\sqrt{2}}$ by using infinite series. Look at the following series: $\displaystyle e = \sum_{n=0}^{\infty}\frac{1}{n!} = 2 + \frac{1}{2} + \frac{1}{6} + \frac{1}{24}+\frac{1}{120}+\frac{1}{720} + \cdots$ $\displaystyle \pi = \frac{4}{1} - \frac{4}{3} + \frac{4}{5} - \frac{4}{7} + \frac{4}{9} - \frac{4}{11} + \cdots$ $\displaystyle \pi = 3 + \frac{4}{2\times 3 \times 4} - \frac{4}{ 4 \times 5 \times 6} + \frac{4}{6 \times 7 \times 8} - \frac{4}{8 \times 9 \times 10} + \cdots $ Both $e$ and $\pi$ are irrational transcendental numbers. But we have found non-trivial infinite series with rational terms for them. Can someone possibly find a similar series for $2^{\sqrt{2}}$? This is something I proposed as a challenge to myself and I failed, now I wonder if someone on here could tackle it.",,"['calculus', 'real-analysis', 'algebra-precalculus', 'numerical-methods']"
8,Does the Jordan curve theorem apply to non-closed curves?,Does the Jordan curve theorem apply to non-closed curves?,,"A Jordan curve is a continuous closed curve in $\Bbb R^2$ which is simple, i.e. has no self-intersections. The Jordan curve theorem states that the complement of any Jordan curve has two connected components, an interior and an exterior. Now let's define an unbounded curve to be a continuous map $f: (-\infty,\infty)\to\Bbb R^2$ such that $f((-\infty,0))$ and $f((0,\infty))$ are both unbounded. My question is, does the complement of a simple unbounded curve always have two connected components? It seems intuitively true, since you'd expect the curve to have two sides, but considering how long it took to prove the Jordan curve theorem, things may not be as straightforward as they appear. Any help would be greatly appreciated. Thank You in Advance. EDIT: As @dfeuer suggested, let's also require that the curve goes off to infinity in both directions.  To make this precise, let's say that there exists two lines $L_1$ and $L_2$, parametrized by $L_1(t) = (a_1 + b_1 t, c_1 + d_1 t)$ and $L_2(t) = (a_2 + b_2 t, c_2 + d_2 t)$, such that the limit of $d(f(t), L_1(t))$ as $t$ goes to $-\infty$ is $0$, and the limit of $d(f(t), L_2(t))$ as $t$ goes to $\infty$ is $0$.  Under that condition, does the complement of the curve have two connected components?","A Jordan curve is a continuous closed curve in $\Bbb R^2$ which is simple, i.e. has no self-intersections. The Jordan curve theorem states that the complement of any Jordan curve has two connected components, an interior and an exterior. Now let's define an unbounded curve to be a continuous map $f: (-\infty,\infty)\to\Bbb R^2$ such that $f((-\infty,0))$ and $f((0,\infty))$ are both unbounded. My question is, does the complement of a simple unbounded curve always have two connected components? It seems intuitively true, since you'd expect the curve to have two sides, but considering how long it took to prove the Jordan curve theorem, things may not be as straightforward as they appear. Any help would be greatly appreciated. Thank You in Advance. EDIT: As @dfeuer suggested, let's also require that the curve goes off to infinity in both directions.  To make this precise, let's say that there exists two lines $L_1$ and $L_2$, parametrized by $L_1(t) = (a_1 + b_1 t, c_1 + d_1 t)$ and $L_2(t) = (a_2 + b_2 t, c_2 + d_2 t)$, such that the limit of $d(f(t), L_1(t))$ as $t$ goes to $-\infty$ is $0$, and the limit of $d(f(t), L_2(t))$ as $t$ goes to $\infty$ is $0$.  Under that condition, does the complement of the curve have two connected components?",,"['real-analysis', 'general-topology', 'connectedness']"
9,Theorems similar to Dini's Theorem and Egoroff's Theorem,Theorems similar to Dini's Theorem and Egoroff's Theorem,,"Dini's Theorem states that Given a sequence of real-valued continuous functions $(f_n)$ on a compact set $E\subseteq \mathbb{R},$ if $(f_n)$ decreases to a continuous function $f$ pointwise on $E$ , then $(f_n)$ converges to $f$ uniformly on $E$ . Egoroff's Theorem states that Given a measure space $(E,\mathcal{A},\mu)$ where $E \subseteq \mathbb{R}$ has finite measure. $\mathcal{A}$ is an $\sigma$ -algebra and $\mu$ is a measure on $E.$ If a sequence of measurable functions $(f_n)$ converges pointwise to $f$ almost everywhere, then for every $\varepsilon>0,$ there exists a measurable subset $F\subseteq E$ with $\mu(E\setminus F) <\varepsilon$ such that $(f_n)$ converges uniformly to $f$ on $F.$ Question: Do there exist theorems, other than Dini and Egoroff, which give sufficient conditions for pointwise convergence to be uniform convergence? I would like to see techniques involved in proving those theorems. Dini and Egoroff used similar technique, which is to define a set containing elements such that $f_n$ and $f$ are 'closed to each other'. If possible, I would like to know other technique to show pointwise convergence implies uniform convergence.","Dini's Theorem states that Given a sequence of real-valued continuous functions on a compact set if decreases to a continuous function pointwise on , then converges to uniformly on . Egoroff's Theorem states that Given a measure space where has finite measure. is an -algebra and is a measure on If a sequence of measurable functions converges pointwise to almost everywhere, then for every there exists a measurable subset with such that converges uniformly to on Question: Do there exist theorems, other than Dini and Egoroff, which give sufficient conditions for pointwise convergence to be uniform convergence? I would like to see techniques involved in proving those theorems. Dini and Egoroff used similar technique, which is to define a set containing elements such that and are 'closed to each other'. If possible, I would like to know other technique to show pointwise convergence implies uniform convergence.","(f_n) E\subseteq \mathbb{R}, (f_n) f E (f_n) f E (E,\mathcal{A},\mu) E \subseteq \mathbb{R} \mathcal{A} \sigma \mu E. (f_n) f \varepsilon>0, F\subseteq E \mu(E\setminus F) <\varepsilon (f_n) f F. f_n f","['real-analysis', 'measure-theory', 'reference-request', 'uniform-convergence', 'pointwise-convergence']"
10,Wiggly polynomials,Wiggly polynomials,,"I'd like to be able to construct polynomials $p$ whose graphs look like this: We can assume that the interval of interest is $[-1, 1]$. The requirements on $p$ are: (1) Equi-oscillation (or roughly equal, anyway) between two extremes. A variation of 10% or so in the values of the extrema would be OK. (2) Zero values and derivatives at the ends of the interval, i.e. $p(-1) = p(1) =p'(-1) = p'(1) = 0$ I want to do this for degrees up to around 30 or so. Just even degrees would be OK. If it helps, these things are a bit like Chebyshev polynomials (but different at the ends). The one in the picture has equation $0.00086992073067855669451 -   0.056750328789339152999 t^2 +   0.60002383910750621904 t^4 -   2.3217878459074773378 t^6 +   4.0661558859963998471 t^8 -   3.288511471137768132 t^{10} + t^{12}$ I got this through brute-force numerical methods (solving a system of non-linear equations, after first doing a lot of work to find good starting points for iteration). I'm looking for an approach that's more intelligent and easier to implement in code. Here is one idea that might work. Suppose we want a polynomial of degree $n$. Start with the Chebyshev polynomial $T_{n-2}(x)$. Let $Q(x) = T_{n-2}(sx)$, where the scale factor $s$ is chosen so that $Q(-1) = Q(1) = 0$. Then let $R(x) = (1-x^2)Q(x)$. This satisfies all the requirements except that its oscillations are too uneven -- they're very small near $\pm1$ and too large near zero. Redistribute the roots of $R$ a bit (somehow??) to level out the oscillations. Comments on answers Using the technique suggested by achille hui in an answer below, we can very easily construct a polynomial with the desired shape. Here is one: The only problem is that I was hoping for a polynomial of degree 12, and this one has degree 30. Also, I was expecting the solution to grow monotonically outside the interval $[-1,1]$, and this one doesn't, as you can see here:","I'd like to be able to construct polynomials $p$ whose graphs look like this: We can assume that the interval of interest is $[-1, 1]$. The requirements on $p$ are: (1) Equi-oscillation (or roughly equal, anyway) between two extremes. A variation of 10% or so in the values of the extrema would be OK. (2) Zero values and derivatives at the ends of the interval, i.e. $p(-1) = p(1) =p'(-1) = p'(1) = 0$ I want to do this for degrees up to around 30 or so. Just even degrees would be OK. If it helps, these things are a bit like Chebyshev polynomials (but different at the ends). The one in the picture has equation $0.00086992073067855669451 -   0.056750328789339152999 t^2 +   0.60002383910750621904 t^4 -   2.3217878459074773378 t^6 +   4.0661558859963998471 t^8 -   3.288511471137768132 t^{10} + t^{12}$ I got this through brute-force numerical methods (solving a system of non-linear equations, after first doing a lot of work to find good starting points for iteration). I'm looking for an approach that's more intelligent and easier to implement in code. Here is one idea that might work. Suppose we want a polynomial of degree $n$. Start with the Chebyshev polynomial $T_{n-2}(x)$. Let $Q(x) = T_{n-2}(sx)$, where the scale factor $s$ is chosen so that $Q(-1) = Q(1) = 0$. Then let $R(x) = (1-x^2)Q(x)$. This satisfies all the requirements except that its oscillations are too uneven -- they're very small near $\pm1$ and too large near zero. Redistribute the roots of $R$ a bit (somehow??) to level out the oscillations. Comments on answers Using the technique suggested by achille hui in an answer below, we can very easily construct a polynomial with the desired shape. Here is one: The only problem is that I was hoping for a polynomial of degree 12, and this one has degree 30. Also, I was expecting the solution to grow monotonically outside the interval $[-1,1]$, and this one doesn't, as you can see here:",,"['real-analysis', 'polynomials', 'numerical-methods']"
11,Compute $\lim_{n\to\infty} \left(\sum_{k=1}^n \frac{H_k}{k}-\frac{1}{2}(\ln n+\gamma)^2\right) $ [closed],Compute  [closed],\lim_{n\to\infty} \left(\sum_{k=1}^n \frac{H_k}{k}-\frac{1}{2}(\ln n+\gamma)^2\right) ,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Compute $$\lim_{n\to\infty} \left(\sum_{k=1}^n \frac{H_k}{k}-\frac{1}{2}(\ln n+\gamma)^2\right) $$ where $\gamma$ - Euler's constant.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Compute $$\lim_{n\to\infty} \left(\sum_{k=1}^n \frac{H_k}{k}-\frac{1}{2}(\ln n+\gamma)^2\right) $$ where $\gamma$ - Euler's constant.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
12,Wanted: A simple and didactically optimized integration theory,Wanted: A simple and didactically optimized integration theory,,"Supposing we want to formulate a very primitive theory of integration, the only requirement being that all continuous functions $[a, b]\longrightarrow\mathbb{R}$ be integrable. What is the simplest possible theory that includes a rigorous notion of integral and a proof that it exists for all continuous functions? Clearly the Riemann-integral gives way to the simpler regulated integral defined via uniform limits of step functions, but it still takes a little non-trivial work to show that this includes the continuous functions. A candidate to beat this is to just to take a Riemann sum with equidistant partition $$\int_a^b f(x)dx:=\lim\limits_{n\longrightarrow\infty}\frac{b-a}{n}\sum\limits_{k=1}^n f\left(a+\frac{k}{n}(b-a)\right)$$ which boils down to showing that this limit exists if $f$ is continuous, but this still requires some educated feeling for how to proceed since we can't avoid using an instance of the fact that cotinuous functions on $[a, b]$ are uniformly continuous. I am wondering if we can beat this... Is there some natural elementary way to rigorously introduce a primitive integral, for which the proof of continuous being integrable sort of writes itself?","Supposing we want to formulate a very primitive theory of integration, the only requirement being that all continuous functions $[a, b]\longrightarrow\mathbb{R}$ be integrable. What is the simplest possible theory that includes a rigorous notion of integral and a proof that it exists for all continuous functions? Clearly the Riemann-integral gives way to the simpler regulated integral defined via uniform limits of step functions, but it still takes a little non-trivial work to show that this includes the continuous functions. A candidate to beat this is to just to take a Riemann sum with equidistant partition $$\int_a^b f(x)dx:=\lim\limits_{n\longrightarrow\infty}\frac{b-a}{n}\sum\limits_{k=1}^n f\left(a+\frac{k}{n}(b-a)\right)$$ which boils down to showing that this limit exists if $f$ is continuous, but this still requires some educated feeling for how to proceed since we can't avoid using an instance of the fact that cotinuous functions on $[a, b]$ are uniformly continuous. I am wondering if we can beat this... Is there some natural elementary way to rigorously introduce a primitive integral, for which the proof of continuous being integrable sort of writes itself?",,"['calculus', 'real-analysis', 'integration', 'analysis']"
13,Lower bound for the Hardy-Littlewood maximal function implies it is not integrable,Lower bound for the Hardy-Littlewood maximal function implies it is not integrable,,"I am working on the following problem from Stein and Shakarchi: Let $f$ be an integral function on $\mathbb{R}^d$ such that $\|f\|_{L^1} = 1$ and let $f^*$ by the Hardy-Littlewood maximal function corresponding of $f$. Prove that if $f$ is integrable on $\mathbb{R}^d$, and $f$ is not identically zero, then $f^*(x) \geq c/|x|^d $ for some $c > 0$ and all $|x| > 1$. Conclude that $f^*$ is not integrable on $\mathbb{R}^d$. Then, show that the weak type estimate $m(\{x:f^*(x) > \alpha \}) \geq c/\alpha$ for all $\alpha > 0$ whenever $\int|f| = 1$, is best possible in the following sense: if $f$ is supported in the unit ball with $\int|f| = 1$, then $m(\{x:f^*(x) > \alpha \}) \geq c'/\alpha$ for some $c' > 0$ and all sufficiently small $\alpha$. [Hint: For the first part, use the fact that $\int_B|f| > 0$ for some ball $B$.] I am not really sure where to begin with this.  Any help would be greatly appreciated.","I am working on the following problem from Stein and Shakarchi: Let $f$ be an integral function on $\mathbb{R}^d$ such that $\|f\|_{L^1} = 1$ and let $f^*$ by the Hardy-Littlewood maximal function corresponding of $f$. Prove that if $f$ is integrable on $\mathbb{R}^d$, and $f$ is not identically zero, then $f^*(x) \geq c/|x|^d $ for some $c > 0$ and all $|x| > 1$. Conclude that $f^*$ is not integrable on $\mathbb{R}^d$. Then, show that the weak type estimate $m(\{x:f^*(x) > \alpha \}) \geq c/\alpha$ for all $\alpha > 0$ whenever $\int|f| = 1$, is best possible in the following sense: if $f$ is supported in the unit ball with $\int|f| = 1$, then $m(\{x:f^*(x) > \alpha \}) \geq c'/\alpha$ for some $c' > 0$ and all sufficiently small $\alpha$. [Hint: For the first part, use the fact that $\int_B|f| > 0$ for some ball $B$.] I am not really sure where to begin with this.  Any help would be greatly appreciated.",,"['real-analysis', 'harmonic-analysis']"
14,Evaluate $\sum\limits_{n=1}^\infty \left(\frac{1}{(k+n)^2+n^2}-\frac{1}{(k+1-n)^2+n^2}\right)$,Evaluate,\sum\limits_{n=1}^\infty \left(\frac{1}{(k+n)^2+n^2}-\frac{1}{(k+1-n)^2+n^2}\right),"I am trying to find the pattern for the coefficients of the closed forms for this series: $$\sum_{n=1}^\infty \left( \frac{1}{(k+n)^2+n^2}-\frac{1}{(k+1-n)^2+n^2} \right)$$ The series seems to converge for every natural $k$ , and it is easy to evaluate each of the two series using this , ending up with a bunch of digamma functions, as shown here . However, when plugging the series in wolfram for some values of $k$ this is what I got: \begin{align} k=1: &&& \frac{-5+2\pi\tanh\left(\frac{\pi}{2}\right)-\pi\coth(\pi)}{4} \\ k=2: &&& \frac{-42-10\pi\tanh\left(\frac{3\pi}{2}\right)+15\pi\coth(\pi)}{60} \\ k=3: &&& \frac{-341+120\pi\tanh\left(\frac{3\pi}{2}\right)-90\pi\coth(2\pi)}{720} \\ k=4: &&& \frac{-3189-884\pi\tanh\left(\frac{5\pi}{2}\right)+1105\pi\coth(2\pi)}{8840} \\ k=5: &&& \frac{-58076+19890\pi\tanh\left(\frac{5\pi} {2}\right)-16575\pi\coth(3\pi)}{198900} \\ k=6: &&& \frac{-21576583-6277050\pi\tanh\left(\frac{7\pi}{2}\right)+7323225\pi\coth(3\pi)}{87878700} \end{align} The arguments of $\tanh$ and $\coth$ can be guessed, respectively, to $\frac{k+(k+1\mod2)}{2}\pi$ and $\frac{k+(k\mod2)}{2}\pi$ , but the other coefficients are less easy to confront. The linear sequence in the numerators ( $5,42,341,\dots$ ) does not appear even in the OEIS, and same goes for the denominators. However, it is possible to guess the coefficients of $\tanh$ and $\coth$ . Hence the question: How to show that \begin{align} \sum_{n=1}^\infty \left( \frac{1}{(k+n)^2+n^2}-\frac{1}{(k+1-n)^2+n^2} \right)= \\ =-\frac{a_k}{b_k}+\frac{\pi}{2k}\tanh\left(\frac{k}{2}\pi\right)-\frac{\pi}{2k+2}\coth\left(\frac{k+1}{2}\pi\right) \text{,} &&&  k\  \text{odd}\\ =-\frac{a_k}{b_k}-\frac{\pi}{2k-2}\tanh\left(\frac{k+1}{2}\pi\right)+\frac{\pi}{2k}\coth\left(\frac{k}{2}\pi\right) \text{,} &&&  k\  \text{even}\\ \end{align} and what are $a_k$ and $b_k$ ? Appearently the result in terms of digamma functions can be simplified a lot to $$\sum_{n=1}^\infty \left( \frac{1}{(k+n)^2+n^2}-\frac{1}{(k+1-n)^2+n^2} \right)=$$ $$=\frac{i}{2k}\left(\psi\left(1+\bar{z}k\right)-\psi\left(1+zk\right)\right)+\frac{i}{2(k+1)}\left(\psi(z-\bar{z}k)-\psi(\bar{z}-zk)\right) \tag{*}$$ where $z=\frac{1+i}{2}$ I feel like it is almost done , but I can't perform the last steps to get rid of the digamma functions. However, I strongly believe it has to do with the following two identities for $u\in \mathbb{C}$ : $$\psi(1-u)-\psi(u)=\pi\cot(\pi u) $$ $$\psi\left(\frac12+u\right)-\psi\left(\frac12-u\right)=\pi\tan(\pi u)$$ Any ideas on how to finish?","I am trying to find the pattern for the coefficients of the closed forms for this series: The series seems to converge for every natural , and it is easy to evaluate each of the two series using this , ending up with a bunch of digamma functions, as shown here . However, when plugging the series in wolfram for some values of this is what I got: The arguments of and can be guessed, respectively, to and , but the other coefficients are less easy to confront. The linear sequence in the numerators ( ) does not appear even in the OEIS, and same goes for the denominators. However, it is possible to guess the coefficients of and . Hence the question: How to show that and what are and ? Appearently the result in terms of digamma functions can be simplified a lot to where I feel like it is almost done , but I can't perform the last steps to get rid of the digamma functions. However, I strongly believe it has to do with the following two identities for : Any ideas on how to finish?","\sum_{n=1}^\infty \left( \frac{1}{(k+n)^2+n^2}-\frac{1}{(k+1-n)^2+n^2} \right) k k \begin{align}
k=1: &&& \frac{-5+2\pi\tanh\left(\frac{\pi}{2}\right)-\pi\coth(\pi)}{4} \\
k=2: &&& \frac{-42-10\pi\tanh\left(\frac{3\pi}{2}\right)+15\pi\coth(\pi)}{60} \\
k=3: &&& \frac{-341+120\pi\tanh\left(\frac{3\pi}{2}\right)-90\pi\coth(2\pi)}{720} \\
k=4: &&& \frac{-3189-884\pi\tanh\left(\frac{5\pi}{2}\right)+1105\pi\coth(2\pi)}{8840} \\
k=5: &&& \frac{-58076+19890\pi\tanh\left(\frac{5\pi} {2}\right)-16575\pi\coth(3\pi)}{198900} \\
k=6: &&& \frac{-21576583-6277050\pi\tanh\left(\frac{7\pi}{2}\right)+7323225\pi\coth(3\pi)}{87878700}
\end{align} \tanh \coth \frac{k+(k+1\mod2)}{2}\pi \frac{k+(k\mod2)}{2}\pi 5,42,341,\dots \tanh \coth \begin{align}
\sum_{n=1}^\infty \left( \frac{1}{(k+n)^2+n^2}-\frac{1}{(k+1-n)^2+n^2} \right)= \\
=-\frac{a_k}{b_k}+\frac{\pi}{2k}\tanh\left(\frac{k}{2}\pi\right)-\frac{\pi}{2k+2}\coth\left(\frac{k+1}{2}\pi\right) \text{,} &&&  k\  \text{odd}\\
=-\frac{a_k}{b_k}-\frac{\pi}{2k-2}\tanh\left(\frac{k+1}{2}\pi\right)+\frac{\pi}{2k}\coth\left(\frac{k}{2}\pi\right) \text{,} &&&  k\  \text{even}\\
\end{align} a_k b_k \sum_{n=1}^\infty \left( \frac{1}{(k+n)^2+n^2}-\frac{1}{(k+1-n)^2+n^2} \right)= =\frac{i}{2k}\left(\psi\left(1+\bar{z}k\right)-\psi\left(1+zk\right)\right)+\frac{i}{2(k+1)}\left(\psi(z-\bar{z}k)-\psi(\bar{z}-zk)\right) \tag{*} z=\frac{1+i}{2} u\in \mathbb{C} \psi(1-u)-\psi(u)=\pi\cot(\pi u)  \psi\left(\frac12+u\right)-\psi\left(\frac12-u\right)=\pi\tan(\pi u)","['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'definite-integrals']"
15,Variation of the Kempner series – convergence of series $\sum\frac{1}{n}$ where $9$ is not a digit of $1/n$.,Variation of the Kempner series – convergence of series  where  is not a digit of .,\sum\frac{1}{n} 9 1/n,"It is easy to argue that the Kempner series converges:  $$  \sum\limits_{\substack{n \text{ s.t. 9 is}\\\text{ not a digit} \\\text{ of } n}}  \frac{1}{n} < \infty$$ Let $E \subset \Bbb N_{>0}$ the subset of the positive integers  such that $9$ is not a digit of the decimal expansion of $1/n$ (the decimal expansion is not allowed to have a trailing infinite sequence of ""$9$""s. For instance $0.24999...$ is not allowed). Here are the first numbers that don't belong to $E$ : $11,13,17,19,21,23,29,31,34,38,41,…$ (not known by the OEIS, by the way). My question is: Does the series    $$ \sum\limits_{n \in E} \frac{1}{n} \tag 1$$ converge? My attempt is : Let $1/n = 0,a_1 a_2 \dots a_k \overline{b_1 b_2 \dots b_m}$ with $n \in E$. Since $1/n$ has no digit ""9"", we have at most $9^{k+m}$ possibilities for the $a_i$'s and $b_j$'s. Moreover, $1/n ≥ 0,00...0\overline{00...01}≥1/10^{k+m}$. But then I can only bound my series $(1)$ from below, by some real number. So, this is not a clue for the divergence of the series. Apparently, the numbers of the form $n=10k+1$ don't belong to $E$. Maybe we can find sufficiently many numbers that have $9$ in the decimal representation of their reciprocals, so that $(1)$ could converge... Any comment will be appreciated !","It is easy to argue that the Kempner series converges:  $$  \sum\limits_{\substack{n \text{ s.t. 9 is}\\\text{ not a digit} \\\text{ of } n}}  \frac{1}{n} < \infty$$ Let $E \subset \Bbb N_{>0}$ the subset of the positive integers  such that $9$ is not a digit of the decimal expansion of $1/n$ (the decimal expansion is not allowed to have a trailing infinite sequence of ""$9$""s. For instance $0.24999...$ is not allowed). Here are the first numbers that don't belong to $E$ : $11,13,17,19,21,23,29,31,34,38,41,…$ (not known by the OEIS, by the way). My question is: Does the series    $$ \sum\limits_{n \in E} \frac{1}{n} \tag 1$$ converge? My attempt is : Let $1/n = 0,a_1 a_2 \dots a_k \overline{b_1 b_2 \dots b_m}$ with $n \in E$. Since $1/n$ has no digit ""9"", we have at most $9^{k+m}$ possibilities for the $a_i$'s and $b_j$'s. Moreover, $1/n ≥ 0,00...0\overline{00...01}≥1/10^{k+m}$. But then I can only bound my series $(1)$ from below, by some real number. So, this is not a clue for the divergence of the series. Apparently, the numbers of the form $n=10k+1$ don't belong to $E$. Maybe we can find sufficiently many numbers that have $9$ in the decimal representation of their reciprocals, so that $(1)$ could converge... Any comment will be appreciated !",,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'decimal-expansion']"
16,Existence of two real numbers satisfying $f(x-f(y))>yf(x)+x$,Existence of two real numbers satisfying,f(x-f(y))>yf(x)+x,"Let $f:\mathbb{R} \longrightarrow \mathbb{R}$ be a function. Is it always the case that for some $x,y \in \mathbb R$, the inequality $f(x-f(y))>yf(x)+x$ holds? Thanks in advance.","Let $f:\mathbb{R} \longrightarrow \mathbb{R}$ be a function. Is it always the case that for some $x,y \in \mathbb R$, the inequality $f(x-f(y))>yf(x)+x$ holds? Thanks in advance.",,"['real-analysis', 'functions', 'inequality']"
17,Theorem 3.54 (about certain rearrangements of a conditionally convergent series) in Baby Rudin: A couple of questions about the proof,Theorem 3.54 (about certain rearrangements of a conditionally convergent series) in Baby Rudin: A couple of questions about the proof,,"Here's the statement of Theorem 3.54 in Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Let $\sum a_n$ be a series of real numbers which converges, but not absolutely. Suppose    $$-\infty \leq \alpha \leq \beta \leq +\infty.$$   Then there exists a rearrangement $\sum a_n^\prime$ with partial sums $s_n^\prime$ such that    $$\lim_{n\to\infty}\inf s_n^\prime = \alpha, \ \ \ \mbox{ and } \ \ \ \lim_{n\to\infty}\sup s_n^\prime = \beta.$$   [ Rudin has numbered this set of inequalities as (24). ] Now I've got a couple of questions regarding Rudin's proof: First, when he has taken real-valued sequences $\{\alpha_n\}$, $\{\beta_n\}$ such that $\alpha_n \to \alpha$, $\beta_n \to \beta$. But then he has also required that $\alpha_n < \beta_n$ and $\beta_1 > 0$. Now is either of  these two inequalities necessary for the proof to proceed, especially $\beta_1 > 0$? Second, in the very last sentence Rudin states: ""Finally, it is clear that no number less than $\alpha$ or greater than $\beta$ can be a subsequential limit of the partial sums of (25)."" How is this statement true? I mean how to explicitly verify this? For those who haven't got a copy of Rudin on hand, I'll edit this question to reproduce Rudin's proof in its entirety. Let $$p_n = \frac{|a_n| + a_n}{2}, \ q_n = \frac{|a_n| - a_n}{2} \ (n = 1, 2, 3, \ldots). $$    Then $p_n - q_n = a_n$, $p_n + q_n = |a_n|$, $p_n \geq 0$, $q_n \geq 0$. The series $\sum p_n$, $\sum q_n$ must both diverge. For if both were convergent, then $$\sum \left( p_n + q_n \right) = \sum |a_n|$$ would converge, contrary to hypothesis. Since $$ \sum_{n=1}^N a_n = \sum_{n=1}^N \left( p_n - q_n \right) = \sum_{n=1}^N p_n - \sum_{n=1}^N q_n,$$ divergence of $\sum p_n$ or convergence of $\sum q_n$ (or vice versa) implies divergence of $\sum a_n$, again contrary to hypothesis. Now let $P_1, P_2, P_3, \ldots$ denote the non-negative terms of $\sum a_n$, in the order in which they occur, and let $Q_1, Q_2, Q_3, \ldots$ be the absolute values of the negative terms of $\sum a_n$, also in their original order. The series $\sum P_n$, $\sum Q_n$ differ from $\sum p_n$, $\sum q_n$ only by zero terms, and are therefore divergent. [ In fact both these series diverge to $+\infty$. Am I right? ] We shall construct sequences $\{m_n \}$, $\{k_n\}$, such that the series $$ P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} + P_{m_1 + 1} + \cdots + P_{m_2} - Q_{k_1 + 1} - \cdots - Q_{k_2} + \cdots, $$ which clearly is a rearrangement of $\sum a_n$, satisfies (24). [ Rudin has numbered the last expression as (25). ] Choose real-valued sequences $\{ \alpha_n \}$, $\{ \beta_n \}$ such that $\alpha_n \rightarrow \alpha$, $\beta_n \rightarrow \beta$, $\alpha_n < \beta_n$, $\beta_1 > 0$. [ What if $\beta_1 \leq 0$? What if $\alpha_n \geq \beta_n$ for some $n$? What will go wrong? ] Let $m_1$, $k_1$ be the smallest integers such that $$P_1 + \cdots + P_{m_1} > \beta_1,$$ $$P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} < \alpha_1;$$  let $m_2$, $k_2$ be the smallest integers such that $$P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} + P_{m_1 + 1} + \cdots + P_{m_2} > \beta_2,$$ $$P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} + P_{m_1 + 1} + \cdots + P_{m_2} - Q_{k_1 + 1} - \cdots - Q_{k_2} < \alpha_2;$$ and continue in this way. This is possible since $\sum P_n$, $\sum Q_n$ diverge. If $x_n$, $y_n$ denote the partial sums of (25) whose last terms are $P_{m_n}$, $-Q_{k_n}$, then $$ | x_n - \beta_n | \leq P_{m_n}, \ \ \ |y_n - \alpha_n | \leq Q_{k_n}. $$ Since $P_n \rightarrow 0$, $Q_n \rightarrow 0$ as $n \rightarrow \infty$, we see that $x_n \rightarrow \beta$, $y_n \rightarrow \alpha$. Finally, it is clear that no number less than $\alpha$ or greater than $\beta$ can be a subsequential limit of the partial sums of (25). [ How do we show this? ]","Here's the statement of Theorem 3.54 in Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Let $\sum a_n$ be a series of real numbers which converges, but not absolutely. Suppose    $$-\infty \leq \alpha \leq \beta \leq +\infty.$$   Then there exists a rearrangement $\sum a_n^\prime$ with partial sums $s_n^\prime$ such that    $$\lim_{n\to\infty}\inf s_n^\prime = \alpha, \ \ \ \mbox{ and } \ \ \ \lim_{n\to\infty}\sup s_n^\prime = \beta.$$   [ Rudin has numbered this set of inequalities as (24). ] Now I've got a couple of questions regarding Rudin's proof: First, when he has taken real-valued sequences $\{\alpha_n\}$, $\{\beta_n\}$ such that $\alpha_n \to \alpha$, $\beta_n \to \beta$. But then he has also required that $\alpha_n < \beta_n$ and $\beta_1 > 0$. Now is either of  these two inequalities necessary for the proof to proceed, especially $\beta_1 > 0$? Second, in the very last sentence Rudin states: ""Finally, it is clear that no number less than $\alpha$ or greater than $\beta$ can be a subsequential limit of the partial sums of (25)."" How is this statement true? I mean how to explicitly verify this? For those who haven't got a copy of Rudin on hand, I'll edit this question to reproduce Rudin's proof in its entirety. Let $$p_n = \frac{|a_n| + a_n}{2}, \ q_n = \frac{|a_n| - a_n}{2} \ (n = 1, 2, 3, \ldots). $$    Then $p_n - q_n = a_n$, $p_n + q_n = |a_n|$, $p_n \geq 0$, $q_n \geq 0$. The series $\sum p_n$, $\sum q_n$ must both diverge. For if both were convergent, then $$\sum \left( p_n + q_n \right) = \sum |a_n|$$ would converge, contrary to hypothesis. Since $$ \sum_{n=1}^N a_n = \sum_{n=1}^N \left( p_n - q_n \right) = \sum_{n=1}^N p_n - \sum_{n=1}^N q_n,$$ divergence of $\sum p_n$ or convergence of $\sum q_n$ (or vice versa) implies divergence of $\sum a_n$, again contrary to hypothesis. Now let $P_1, P_2, P_3, \ldots$ denote the non-negative terms of $\sum a_n$, in the order in which they occur, and let $Q_1, Q_2, Q_3, \ldots$ be the absolute values of the negative terms of $\sum a_n$, also in their original order. The series $\sum P_n$, $\sum Q_n$ differ from $\sum p_n$, $\sum q_n$ only by zero terms, and are therefore divergent. [ In fact both these series diverge to $+\infty$. Am I right? ] We shall construct sequences $\{m_n \}$, $\{k_n\}$, such that the series $$ P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} + P_{m_1 + 1} + \cdots + P_{m_2} - Q_{k_1 + 1} - \cdots - Q_{k_2} + \cdots, $$ which clearly is a rearrangement of $\sum a_n$, satisfies (24). [ Rudin has numbered the last expression as (25). ] Choose real-valued sequences $\{ \alpha_n \}$, $\{ \beta_n \}$ such that $\alpha_n \rightarrow \alpha$, $\beta_n \rightarrow \beta$, $\alpha_n < \beta_n$, $\beta_1 > 0$. [ What if $\beta_1 \leq 0$? What if $\alpha_n \geq \beta_n$ for some $n$? What will go wrong? ] Let $m_1$, $k_1$ be the smallest integers such that $$P_1 + \cdots + P_{m_1} > \beta_1,$$ $$P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} < \alpha_1;$$  let $m_2$, $k_2$ be the smallest integers such that $$P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} + P_{m_1 + 1} + \cdots + P_{m_2} > \beta_2,$$ $$P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} + P_{m_1 + 1} + \cdots + P_{m_2} - Q_{k_1 + 1} - \cdots - Q_{k_2} < \alpha_2;$$ and continue in this way. This is possible since $\sum P_n$, $\sum Q_n$ diverge. If $x_n$, $y_n$ denote the partial sums of (25) whose last terms are $P_{m_n}$, $-Q_{k_n}$, then $$ | x_n - \beta_n | \leq P_{m_n}, \ \ \ |y_n - \alpha_n | \leq Q_{k_n}. $$ Since $P_n \rightarrow 0$, $Q_n \rightarrow 0$ as $n \rightarrow \infty$, we see that $x_n \rightarrow \beta$, $y_n \rightarrow \alpha$. Finally, it is clear that no number less than $\alpha$ or greater than $\beta$ can be a subsequential limit of the partial sums of (25). [ How do we show this? ]",,"['real-analysis', 'sequences-and-series', 'analysis', 'conditional-convergence']"
18,Operator norm of orthogonal projection,Operator norm of orthogonal projection,,"I was assigned the following homework problem: ""Let $P: H \to H$ be bounded and linear. Assume it satisfies $P^2 = P$ and $P^\star = P$ . Show $\|P\| \le 1$ ."" This isn't too hard to show: for any $v\in H$ , $$\| Pv \|^2 = |\langle Pv, Pv \rangle| = | \langle v, Pv \rangle | \le \|v \| \cdot \|Pv\| \implies \frac{\|Pv\|}{\|v\|} \le 1 \implies \|P\|\le 1$$ However, I also noticed the following inequality: $$ \|Pv \| = \|P^2 v\| = \|P(Pv)\| \le \|P\| \cdot \|Pv \| \implies \|P \| \ge 1 $$ So $\|P\| = 1$ . But every source I've checked only says $\|P\| \le 1$ . Is that second inequality true? Why does it fail, if not?","I was assigned the following homework problem: ""Let be bounded and linear. Assume it satisfies and . Show ."" This isn't too hard to show: for any , However, I also noticed the following inequality: So . But every source I've checked only says . Is that second inequality true? Why does it fail, if not?","P: H \to H P^2 = P P^\star = P \|P\| \le 1 v\in H \| Pv \|^2 = |\langle Pv, Pv \rangle| = | \langle v, Pv \rangle | \le \|v \| \cdot \|Pv\| \implies \frac{\|Pv\|}{\|v\|} \le 1 \implies \|P\|\le 1  \|Pv \| = \|P^2 v\| = \|P(Pv)\| \le \|P\| \cdot \|Pv \| \implies \|P \| \ge 1  \|P\| = 1 \|P\| \le 1","['real-analysis', 'linear-algebra', 'hilbert-spaces']"
19,Pointwise limit of continuous functions is 1) measurable and 2) pointwise discontinuous,Pointwise limit of continuous functions is 1) measurable and 2) pointwise discontinuous,,"I'm looking for the full proof of some classic theorem stated in 1 of my textbooks with only a sketch proof. For Baire class 1 functions - pointwise limit of continuous functions: How to show that 1) they are measurable and 2) they can be discontinuous, but the set of discontinuous points is 'small'. In particular, a function f is the limit of continuous functions if and only if it is pointwise discontinuous (the set of discontinuous points is of first category / countable union of nowhere dense sets). Sketch of Lebesgue's proof (1904) - from ""A Radical Approach to Lebesgue's Theory of Integration"" by David Bressoud: Let f be the limit of continuous functions, $f_n \rightarrow f$, on $[a, b]$. Let $P_k$ denote the set of points at which the oscillation of $f$ is greater than or equal to $1/k$. If we can show that each $P_k$ is nowhere dense, then $f$ is pointwise discontinuous. We take any open interval $(\alpha, \beta) \subset [a, b]$ and partition the entire $y$-axis from $-\infty$ to $\infty$ using points $\dots < m_1 < m_0 < m_1 <m_2 <\dots$ for which $m_{i+1} - m_i < 1/2k$. Consider the set $E_i = \{x \in (\alpha, \beta) | m_i < f(x) < m_{i+2}\}$ We have that $(\alpha, \beta)= \bigcup_{i=-\infty}^{\infty}E_i$ and $x_1, x_2 \in E_ i \Rightarrow |f(x_1) - f(x_2)| < m_{i+2} - m_i < \frac{1}{k}$ The oscillation of $f$ on $E_i$ is less than $1/k$. Lebesgue begins by using the fact that $f$ is the limit of continuous functions to prove that each $E_i$ is a countable union of closed sets (this will lead to $f$ being measurable). In fact, he does more than this. He proves that $f$ is a limit of continuous functions if and only if, for each $k \in \mathbb{N}$, the domain can be represented as a countable union of closed sets so that the oscillation of $f$ on each set is strictly less than $1/k$. He next proves that given any set $E$ that is a countable union of closed sets, we can construct a function for which the points of discontinuity are precisely the points of $E$. Let $\phi_i$ be a function on $(\alpha, \beta)$ for which the points of discontinuity are precisely the points in $E_i$. Could all of the functions $\phi_i$, $-\infty < i < \infty$, be pointwise discontinuous? If they were, then there would be a point in $(\alpha, \beta)$, call it $c$, where all of them are continuous. But $f(c) \in E_j$ for some $j$, and that means that $j$ is not continuous at $c$, a contradiction. At least one of the $\phi_i$ must be totally discontinuous. If $\phi_j$ is totally discontinuous on $(\alpha, \beta)$, then there is an open subinterval of $(\alpha, \beta)$ for which $j$ is discontinuous at every point of this subinterval. By the way we defined $j$, the set $E_j$ contains an open subinterval of $(\alpha, \beta)$. From the definition of $E_j$, the oscillation is less than $1/k$ at every point in this subinterval. We have shown that $P_k$ is nowhere dense, and, therefore, $f$ is pointwise discontinuous. In the other direction, if $f$ is not the limit of continuous functions, then there is some $k$ for which the domain cannot be expressed as a countable union of closed sets with oscillation strictly less than $1/k$ on each set. Lebesgue uses this to find an open interval contained in $P_k$. The function $f$ must be totally discontinuous.","I'm looking for the full proof of some classic theorem stated in 1 of my textbooks with only a sketch proof. For Baire class 1 functions - pointwise limit of continuous functions: How to show that 1) they are measurable and 2) they can be discontinuous, but the set of discontinuous points is 'small'. In particular, a function f is the limit of continuous functions if and only if it is pointwise discontinuous (the set of discontinuous points is of first category / countable union of nowhere dense sets). Sketch of Lebesgue's proof (1904) - from ""A Radical Approach to Lebesgue's Theory of Integration"" by David Bressoud: Let f be the limit of continuous functions, $f_n \rightarrow f$, on $[a, b]$. Let $P_k$ denote the set of points at which the oscillation of $f$ is greater than or equal to $1/k$. If we can show that each $P_k$ is nowhere dense, then $f$ is pointwise discontinuous. We take any open interval $(\alpha, \beta) \subset [a, b]$ and partition the entire $y$-axis from $-\infty$ to $\infty$ using points $\dots < m_1 < m_0 < m_1 <m_2 <\dots$ for which $m_{i+1} - m_i < 1/2k$. Consider the set $E_i = \{x \in (\alpha, \beta) | m_i < f(x) < m_{i+2}\}$ We have that $(\alpha, \beta)= \bigcup_{i=-\infty}^{\infty}E_i$ and $x_1, x_2 \in E_ i \Rightarrow |f(x_1) - f(x_2)| < m_{i+2} - m_i < \frac{1}{k}$ The oscillation of $f$ on $E_i$ is less than $1/k$. Lebesgue begins by using the fact that $f$ is the limit of continuous functions to prove that each $E_i$ is a countable union of closed sets (this will lead to $f$ being measurable). In fact, he does more than this. He proves that $f$ is a limit of continuous functions if and only if, for each $k \in \mathbb{N}$, the domain can be represented as a countable union of closed sets so that the oscillation of $f$ on each set is strictly less than $1/k$. He next proves that given any set $E$ that is a countable union of closed sets, we can construct a function for which the points of discontinuity are precisely the points of $E$. Let $\phi_i$ be a function on $(\alpha, \beta)$ for which the points of discontinuity are precisely the points in $E_i$. Could all of the functions $\phi_i$, $-\infty < i < \infty$, be pointwise discontinuous? If they were, then there would be a point in $(\alpha, \beta)$, call it $c$, where all of them are continuous. But $f(c) \in E_j$ for some $j$, and that means that $j$ is not continuous at $c$, a contradiction. At least one of the $\phi_i$ must be totally discontinuous. If $\phi_j$ is totally discontinuous on $(\alpha, \beta)$, then there is an open subinterval of $(\alpha, \beta)$ for which $j$ is discontinuous at every point of this subinterval. By the way we defined $j$, the set $E_j$ contains an open subinterval of $(\alpha, \beta)$. From the definition of $E_j$, the oscillation is less than $1/k$ at every point in this subinterval. We have shown that $P_k$ is nowhere dense, and, therefore, $f$ is pointwise discontinuous. In the other direction, if $f$ is not the limit of continuous functions, then there is some $k$ for which the domain cannot be expressed as a countable union of closed sets with oscillation strictly less than $1/k$ on each set. Lebesgue uses this to find an open interval contained in $P_k$. The function $f$ must be totally discontinuous.",,"['real-analysis', 'measure-theory']"
20,Boundedness of functions,Boundedness of functions,,"Recently in my problem solving class we had the question: If $f: [0,\infty) \to \Bbb R$ is a twice differentiable function such that $f''(x) + e^x f(x) = 0$, then prove that $f$ is bounded. The solution I found was to create a function $g(x)=f(x)^2 + e^{-x}f'(x)^2$. Looking at $g'$, we have $$g'=2ff' -e^{-x}f'^2 + 2e^{-x}f'f''$$ $$g'=-e^{-x}f'^2+2ff'+2e^{-x}f'f''$$ $$g' = -e^{-x}f'^2 +2e^{-x}f' \left(e^xf + f'' \right) = -e^{-x}f'^2 \leq 0$$ Thus we have that $g$ is a decreasing function. This means that for $x>0$, we have $$g(x) \leq g(0)$$ Since $g(0)=R$ for some constant $R$, we have $$R \geq g(x)=f(x)^2+e^{-x}f'(x)^2 > f(x)^2$$ thus $f^2$ is bounded, so $f$ itself must be bounded. It is not so hard to show that this works if we replace $e^x$ with any differentiable function $k(x)$ that is positive and increasing on $[0,\infty)$. Thus we have the more general result that if $f: [0,\infty) \to \Bbb R$ is a twice differentiable function such that $f''(x) + k(x) f(x) = 0$ on $[a,\infty)$ for some $a \geq 0$ and $k(x)$ is positive, increasing , and differentiable on $[a,\infty)$, then $f$ is bounded. However, my teacher also gave us a similar problem: if $f: [0,\infty) \to \Bbb R$ is a twice differentiable function such that $f''(x) + \frac{1}{x} f(x) = 0$ on $[a,\infty)$ for some $a > 0$, then $f$ is bounded above. Unfortunately for us, our above result does not apply, since $\frac{1}{x}$ is decreasing , rather than increasing on $[a,\infty)$. In fact, defining $g(x)$ analogously as $g(x)=f(x)^2+xf'(x)^2$ gives us that $g' \geq 0$, so the technique fails. All similarly defined functions in terms of $f(x)$ and $f'(x)$ that I've tried have also failed to tell me anything about the boundedness of $f$. However, since there is a degree of intuition required for competition type problems like this, it's certainly very very likely that I missed one that does work. I will mention that defining  $h(x) = \frac{1}{x}f(x)^2 + f'(x)^2$ does tell us that $f'$ must be bounded, but I couldn't get anything about the boundedness of $f$ out of this. I've been a member of stackexchange for a couple of months now and I really wanted my first posted question to be something really interesting and I believe that I found one! Since the problem involving $e^x$ generalizes very nicely to all increasing, positive, differentiable functions, I imagine that a solution of the problem involving $\frac{1}{x}$ would also generalize to all decreasing, positive, differentiable functions, so a proof of this could possibly provide a very interesting result concerning boundedness of functions that satisfy relatively simple differential equations. I've hit a bit of an impasse in solving this problem and I would like to know if anyone has any suggestions on how to prove this.","Recently in my problem solving class we had the question: If $f: [0,\infty) \to \Bbb R$ is a twice differentiable function such that $f''(x) + e^x f(x) = 0$, then prove that $f$ is bounded. The solution I found was to create a function $g(x)=f(x)^2 + e^{-x}f'(x)^2$. Looking at $g'$, we have $$g'=2ff' -e^{-x}f'^2 + 2e^{-x}f'f''$$ $$g'=-e^{-x}f'^2+2ff'+2e^{-x}f'f''$$ $$g' = -e^{-x}f'^2 +2e^{-x}f' \left(e^xf + f'' \right) = -e^{-x}f'^2 \leq 0$$ Thus we have that $g$ is a decreasing function. This means that for $x>0$, we have $$g(x) \leq g(0)$$ Since $g(0)=R$ for some constant $R$, we have $$R \geq g(x)=f(x)^2+e^{-x}f'(x)^2 > f(x)^2$$ thus $f^2$ is bounded, so $f$ itself must be bounded. It is not so hard to show that this works if we replace $e^x$ with any differentiable function $k(x)$ that is positive and increasing on $[0,\infty)$. Thus we have the more general result that if $f: [0,\infty) \to \Bbb R$ is a twice differentiable function such that $f''(x) + k(x) f(x) = 0$ on $[a,\infty)$ for some $a \geq 0$ and $k(x)$ is positive, increasing , and differentiable on $[a,\infty)$, then $f$ is bounded. However, my teacher also gave us a similar problem: if $f: [0,\infty) \to \Bbb R$ is a twice differentiable function such that $f''(x) + \frac{1}{x} f(x) = 0$ on $[a,\infty)$ for some $a > 0$, then $f$ is bounded above. Unfortunately for us, our above result does not apply, since $\frac{1}{x}$ is decreasing , rather than increasing on $[a,\infty)$. In fact, defining $g(x)$ analogously as $g(x)=f(x)^2+xf'(x)^2$ gives us that $g' \geq 0$, so the technique fails. All similarly defined functions in terms of $f(x)$ and $f'(x)$ that I've tried have also failed to tell me anything about the boundedness of $f$. However, since there is a degree of intuition required for competition type problems like this, it's certainly very very likely that I missed one that does work. I will mention that defining  $h(x) = \frac{1}{x}f(x)^2 + f'(x)^2$ does tell us that $f'$ must be bounded, but I couldn't get anything about the boundedness of $f$ out of this. I've been a member of stackexchange for a couple of months now and I really wanted my first posted question to be something really interesting and I believe that I found one! Since the problem involving $e^x$ generalizes very nicely to all increasing, positive, differentiable functions, I imagine that a solution of the problem involving $\frac{1}{x}$ would also generalize to all decreasing, positive, differentiable functions, so a proof of this could possibly provide a very interesting result concerning boundedness of functions that satisfy relatively simple differential equations. I've hit a bit of an impasse in solving this problem and I would like to know if anyone has any suggestions on how to prove this.",,"['calculus', 'real-analysis']"
21,"If $\int_0^1 e^{- \frac{nx}{1-x}} f(x) \; dx =0$ for $n \geq 0$, then $f=0$ on $[0,1]$","If  for , then  on","\int_0^1 e^{- \frac{nx}{1-x}} f(x) \; dx =0 n \geq 0 f=0 [0,1]","Suppose that $f$ is continuous on $[0,1]$. If $$ \int_0^1 e^{- \frac{nx}{1-x}} f(x) \; dx =0 $$ for $n \geq 0$, show that $f(x)=0$ on $[0,1]$. I am unsure what I should be hoping to do to show this. I know $e^{-nx/(1-x)}$ converges pointwise to 0 on $[0,1]$. But this does not help - so I think. In the case where $n=0$, we have $\int_0^1 f(x) \; dx=0$. But this shows $f(x)=0$ only where $f(x)$ is nonnegative or nonpositive. Stone-Weierstrass only seems to complicate the matter. What should I start thinking about to solve this problems?","Suppose that $f$ is continuous on $[0,1]$. If $$ \int_0^1 e^{- \frac{nx}{1-x}} f(x) \; dx =0 $$ for $n \geq 0$, show that $f(x)=0$ on $[0,1]$. I am unsure what I should be hoping to do to show this. I know $e^{-nx/(1-x)}$ converges pointwise to 0 on $[0,1]$. But this does not help - so I think. In the case where $n=0$, we have $\int_0^1 f(x) \; dx=0$. But this shows $f(x)=0$ only where $f(x)$ is nonnegative or nonpositive. Stone-Weierstrass only seems to complicate the matter. What should I start thinking about to solve this problems?",,"['real-analysis', 'uniform-convergence']"
22,Prove that $\frac{1}{a}+\frac{1}{b}+\frac{1}{c}\le\frac{3}{\sqrt{7}}$,Prove that,\frac{1}{a}+\frac{1}{b}+\frac{1}{c}\le\frac{3}{\sqrt{7}},"Let $a,b,c>0$ such that $$\dfrac{1}{a^2+2}+\dfrac{1}{b^2+2}+\dfrac{1}{c^2+2}=\dfrac{1}{3}.$$ Show that $$\dfrac{1}{a}+\dfrac{1}{b}+\dfrac{1}{c}\le\dfrac{3}{\sqrt{7}}.$$ My try: since $$\dfrac{1}{2+a^2}=\dfrac{1}{2}\left(1-\dfrac{a^2}{a^2+2}\right)$$ so $$\dfrac{a^2}{a^2+2}+\dfrac{b^2}{b^2+2}+\dfrac{c^2}{c^2+2}=\dfrac{7}{3}$$ we only prove $\dfrac{1}{a}+\dfrac{1}{b}+\dfrac{1}{c}\le\dfrac{3}{\sqrt{7}}$ and I want use Cauchy-Schwarz inequality to prove it,But I can't works,such $$\left(\dfrac{a^2}{a^2+2}+\dfrac{b^2}{b^2+2}+\dfrac{c^2}{c^2+2}\right)(a^2+2+b^2+2+c^2+2)\ge (a+b+c)^2$$ $$(a^2+b^2+c^2+6)\ge \dfrac{3}{7}(a^2+b^2+c^2+2ab+2bc+2ac)$$ $$\Longrightarrow 4(a^2+b^2+c^2)+42\ge 6(ab+bc+ac)$$ and let $$p=a+b+c,q=ab+bc+ac,r=abc$$ so $$2p^2+21\ge 7q$$ and we only prove $$\dfrac{1}{a}+\dfrac{1}{b}+\dfrac{1}{c}=\dfrac{ab+bc+ac}{abc}=\dfrac{q}{r}\le\dfrac{3}{\sqrt{7}}$$ maybe this is not true. But this not usefull to solve this problem . Thank you","Let such that Show that My try: since so we only prove and I want use Cauchy-Schwarz inequality to prove it,But I can't works,such and let so and we only prove maybe this is not true. But this not usefull to solve this problem . Thank you","a,b,c>0 \dfrac{1}{a^2+2}+\dfrac{1}{b^2+2}+\dfrac{1}{c^2+2}=\dfrac{1}{3}. \dfrac{1}{a}+\dfrac{1}{b}+\dfrac{1}{c}\le\dfrac{3}{\sqrt{7}}. \dfrac{1}{2+a^2}=\dfrac{1}{2}\left(1-\dfrac{a^2}{a^2+2}\right) \dfrac{a^2}{a^2+2}+\dfrac{b^2}{b^2+2}+\dfrac{c^2}{c^2+2}=\dfrac{7}{3} \dfrac{1}{a}+\dfrac{1}{b}+\dfrac{1}{c}\le\dfrac{3}{\sqrt{7}} \left(\dfrac{a^2}{a^2+2}+\dfrac{b^2}{b^2+2}+\dfrac{c^2}{c^2+2}\right)(a^2+2+b^2+2+c^2+2)\ge (a+b+c)^2 (a^2+b^2+c^2+6)\ge \dfrac{3}{7}(a^2+b^2+c^2+2ab+2bc+2ac) \Longrightarrow 4(a^2+b^2+c^2)+42\ge 6(ab+bc+ac) p=a+b+c,q=ab+bc+ac,r=abc 2p^2+21\ge 7q \dfrac{1}{a}+\dfrac{1}{b}+\dfrac{1}{c}=\dfrac{ab+bc+ac}{abc}=\dfrac{q}{r}\le\dfrac{3}{\sqrt{7}}","['real-analysis', 'multivariable-calculus', 'inequality', 'contest-math', 'uvw']"
23,Measure of the Cantor set multiplied by the Cantor set,Measure of the Cantor set multiplied by the Cantor set,,"It is a quite famous fact that $C+C=[0,2]$, what about $ C \cdot C $ ? Is it measurable? Yes It's clearly measurable because it's compact, being a continuous image   of the compact set C×C. Is it possible to list all the intervals of $[0,1]\setminus C\cdot C$ ? What is its Lebesgue measure ?","It is a quite famous fact that $C+C=[0,2]$, what about $ C \cdot C $ ? Is it measurable? Yes It's clearly measurable because it's compact, being a continuous image   of the compact set C×C. Is it possible to list all the intervals of $[0,1]\setminus C\cdot C$ ? What is its Lebesgue measure ?",,"['real-analysis', 'measure-theory']"
24,The equivalence between Cauchy integral and Riemann integral for bounded functions,The equivalence between Cauchy integral and Riemann integral for bounded functions,,"Definitions Suppose $P\colon a=x_0<x_1<\dotsb<x_n=b$ is a partition of $[a,b]$. Let $\Delta x_k=x_k-x_{k-1}$ and $\lVert P\rVert$ denotes $\max_{0<k\le n}\Delta x_k$. The Cauchy integral of a function $f$ on closed interval $[a,b]$ equals to $I$ if and only if for each $\epsilon>0$, there's some $\delta>0$, for each partition $P$ of $[a,b]$ such that $\lVert P\rVert<\delta$, we have $\left\lvert\sum_{k=1}^nf(x_k)\Delta x_k-I\right\rvert<\epsilon$. Problem If $f$ is bounded on $[a,b]$ whose Cauchy integral equals to $I$, then $f$ is Riemann-integrable and $\int_a^bf=I$. Background It's an exercise from our calculus(analysis) problemset book, and there's a hint: consider the partitions whose $x_k-x_{k-1}$ is a constant for different $k$'s, and try to estimate the Riemann sum for each of these partitions through the Cauchy integral. I have no idea about such estimation. After drawing some pictures, I discouraged. I googled on the Internet and found an article . I realized that it's a quite different approach and with some advanced techniques (such as the analysis of a positive measure set -- discontinuities). I hope there will be some simpler approachers, just as the hint says. I need a more detailed hint, or a solution. Can anybody help me? Thanks!","Definitions Suppose $P\colon a=x_0<x_1<\dotsb<x_n=b$ is a partition of $[a,b]$. Let $\Delta x_k=x_k-x_{k-1}$ and $\lVert P\rVert$ denotes $\max_{0<k\le n}\Delta x_k$. The Cauchy integral of a function $f$ on closed interval $[a,b]$ equals to $I$ if and only if for each $\epsilon>0$, there's some $\delta>0$, for each partition $P$ of $[a,b]$ such that $\lVert P\rVert<\delta$, we have $\left\lvert\sum_{k=1}^nf(x_k)\Delta x_k-I\right\rvert<\epsilon$. Problem If $f$ is bounded on $[a,b]$ whose Cauchy integral equals to $I$, then $f$ is Riemann-integrable and $\int_a^bf=I$. Background It's an exercise from our calculus(analysis) problemset book, and there's a hint: consider the partitions whose $x_k-x_{k-1}$ is a constant for different $k$'s, and try to estimate the Riemann sum for each of these partitions through the Cauchy integral. I have no idea about such estimation. After drawing some pictures, I discouraged. I googled on the Internet and found an article . I realized that it's a quite different approach and with some advanced techniques (such as the analysis of a positive measure set -- discontinuities). I hope there will be some simpler approachers, just as the hint says. I need a more detailed hint, or a solution. Can anybody help me? Thanks!",,"['real-analysis', 'integration', 'definite-integrals']"
25,Is there a function whose graph intersects every tangent line at exactly 2 points?,Is there a function whose graph intersects every tangent line at exactly 2 points?,,"Is there some $f:\mathbb{R}\to\mathbb{R}$ differentiable at every point such that $\forall x$ , the tangent to $f$ at $(x,f(x))$ intersects the graph of $f$ at $2$ points (counting (x,f(x)))? This problem is delicate, as shown by the example $f(x)=x^3$ , where the condition only fails at $x=0$ . Remark: if $f$ is $C^1$ , then the function $g:\mathbb{R}\to\mathbb{R}$ which sends $x$ to the only point $y$ such that $(y,f(y))$ is in the tangent line to $x$ seems to be continuous at almost every point. I could not use this effectively though, in the non continuous case maybe you could get some weak version of this using the intermediate value property of the derivative?","Is there some differentiable at every point such that , the tangent to at intersects the graph of at points (counting (x,f(x)))? This problem is delicate, as shown by the example , where the condition only fails at . Remark: if is , then the function which sends to the only point such that is in the tangent line to seems to be continuous at almost every point. I could not use this effectively though, in the non continuous case maybe you could get some weak version of this using the intermediate value property of the derivative?","f:\mathbb{R}\to\mathbb{R} \forall x f (x,f(x)) f 2 f(x)=x^3 x=0 f C^1 g:\mathbb{R}\to\mathbb{R} x y (y,f(y)) x","['real-analysis', 'functions', 'derivatives', 'continuity', 'tangent-line']"
26,Is $\cos(\alpha x + \cos(x))$ periodic?,Is  periodic?,\cos(\alpha x + \cos(x)),"Consider the function $f: \mathbb{R} \to [-1, 1]$ defined as $$f(x) = \cos(\alpha x + \cos(x))$$ What conditions must be placed on $\alpha \in \mathbb{R}$ such that the function $f$ is periodic? First of all, I tried plotting some values on Wolfram|Alpha , and for all the values of $\alpha$ that I tested, it seems that any $\alpha$ works... But I couldn't prove it. My attempt: We want to study $\alpha$ such that the following statement is true: $$\exists \,\, T > 0 \quad \forall \,x \in \mathbb{R} \quad \cos(\alpha (x + T) + \cos(x + T)) = \cos(\alpha x + \cos(x))$$ I was able to show, with some trigonometric substitutions, that this statement is equivalent to the following statement: $$\exists \,\, T > 0 \quad \forall \,x \in \mathbb{R} \quad \exists \,\, K \in \mathbb{Z} \quad \text{such that}$$ $$\sin(x + T) = \dfrac{\alpha T - K\pi}{\sin (T)} \quad \text{or} \quad \cos(x + T) = \dfrac{K\pi - \alpha(x + T)}{\cos (T)}$$ I couldn't make any progress after that, though. EDIT: Inspired by a quick comment by @ZainPatel, I was actually able to show that all $\alpha \in \mathbb{Q}$ works! It's quite simple, I am surprised I didn't try this before. Let $\alpha \in \mathbb{Q}$, $\alpha = \dfrac{p}{q}$. Then $T = 2q\pi$ works, since $$f(x + 2q\pi) = \cos(\alpha (x + 2q\pi) + \cos(x + 2q\pi)) = \cos(\alpha x + 2p\pi + \cos(x)) = f(x)$$ The matter is still open for irrationals though!","Consider the function $f: \mathbb{R} \to [-1, 1]$ defined as $$f(x) = \cos(\alpha x + \cos(x))$$ What conditions must be placed on $\alpha \in \mathbb{R}$ such that the function $f$ is periodic? First of all, I tried plotting some values on Wolfram|Alpha , and for all the values of $\alpha$ that I tested, it seems that any $\alpha$ works... But I couldn't prove it. My attempt: We want to study $\alpha$ such that the following statement is true: $$\exists \,\, T > 0 \quad \forall \,x \in \mathbb{R} \quad \cos(\alpha (x + T) + \cos(x + T)) = \cos(\alpha x + \cos(x))$$ I was able to show, with some trigonometric substitutions, that this statement is equivalent to the following statement: $$\exists \,\, T > 0 \quad \forall \,x \in \mathbb{R} \quad \exists \,\, K \in \mathbb{Z} \quad \text{such that}$$ $$\sin(x + T) = \dfrac{\alpha T - K\pi}{\sin (T)} \quad \text{or} \quad \cos(x + T) = \dfrac{K\pi - \alpha(x + T)}{\cos (T)}$$ I couldn't make any progress after that, though. EDIT: Inspired by a quick comment by @ZainPatel, I was actually able to show that all $\alpha \in \mathbb{Q}$ works! It's quite simple, I am surprised I didn't try this before. Let $\alpha \in \mathbb{Q}$, $\alpha = \dfrac{p}{q}$. Then $T = 2q\pi$ works, since $$f(x + 2q\pi) = \cos(\alpha (x + 2q\pi) + \cos(x + 2q\pi)) = \cos(\alpha x + 2p\pi + \cos(x)) = f(x)$$ The matter is still open for irrationals though!",,"['real-analysis', 'functions', 'trigonometry', 'periodic-functions']"
27,"If the derivative of $f$ is never zero, then $f$ is one-to-one","If the derivative of  is never zero, then  is one-to-one",f f,"This is an exercise from Abbott's second edition of Understanding Analysis. Let $f$ be differentiable on an interval $A$. Show that if $f'(x) \neq 0$ on $A$, show that $f$ is one-to-one on $A$. Provide an example to show that the converse statement need not be true. Is my solution (below) correct? Let $x_{1},x_{2} \in A $ such that $x_1 \neq x_2$. Since the function satisfies all the conditions of the mean value theorem, there exists $c \in (x_1,x_2)$ [without loss of generality we consider $x_1 < x_2$] such that $f(x_2) - f(x_1) = (x_2 -x_1) f'(c) \neq 0$ as it is given that $f'(x)$ is nonzero on $A$ and $x_1 \neq x_2$ is our assumption. Therefore $x_1 \neq x_2$ implies $f(x_1) \neq  f(x_2)$ for every $x_1,x_2 \in A$. Thus $f$ is one-to one on $A$. The converse may not be true. Consider $A=[-1,1)$ and $f(x)= x^3$. Therefore $f$ is injective on $A$. Again $f'(x)= 2x^2$. Therefore $f'(0)=0, 0 \in A$. So the converse need not be true.","This is an exercise from Abbott's second edition of Understanding Analysis. Let $f$ be differentiable on an interval $A$. Show that if $f'(x) \neq 0$ on $A$, show that $f$ is one-to-one on $A$. Provide an example to show that the converse statement need not be true. Is my solution (below) correct? Let $x_{1},x_{2} \in A $ such that $x_1 \neq x_2$. Since the function satisfies all the conditions of the mean value theorem, there exists $c \in (x_1,x_2)$ [without loss of generality we consider $x_1 < x_2$] such that $f(x_2) - f(x_1) = (x_2 -x_1) f'(c) \neq 0$ as it is given that $f'(x)$ is nonzero on $A$ and $x_1 \neq x_2$ is our assumption. Therefore $x_1 \neq x_2$ implies $f(x_1) \neq  f(x_2)$ for every $x_1,x_2 \in A$. Thus $f$ is one-to one on $A$. The converse may not be true. Consider $A=[-1,1)$ and $f(x)= x^3$. Therefore $f$ is injective on $A$. Again $f'(x)= 2x^2$. Therefore $f'(0)=0, 0 \in A$. So the converse need not be true.",,"['real-analysis', 'derivatives', 'proof-verification']"
28,Help with modified Takagi functions,Help with modified Takagi functions,,"First, I need to give some definitions and background information: Define $h(x)=|x|$ for $x\in [-1,1]$. Extend this function to $\mathbb R$ by defining $h(x+2) = h(x)$. Here is a graph of $h$: Now if we define $g(x) = \sum_{n=0}^\infty {1 \over 2^n}h(2^nx)$ then $g$ is the Takagi function. One can prove that $g$ is continuous and nowhere differentiable. I did both. Continuity follows easily from the Weierstrass M-test and for non-differentiability one can first show it for dyadic points and then for non-dyadic points. Now I am interested in two different modified versions of the Takagi function: $$ g_1(x) = \sum_{n=0}^\infty {1\over 2^n}h(3^n x)$$ and $$ g_2(x) = \sum_{n=0}^\infty {1\over 3^n}h(2^n x)$$ These two are easily seen to be continuous. Could someone please show me how to determine the differentiability of both $g_1$ and $g_2$? I really tried but failed.","First, I need to give some definitions and background information: Define $h(x)=|x|$ for $x\in [-1,1]$. Extend this function to $\mathbb R$ by defining $h(x+2) = h(x)$. Here is a graph of $h$: Now if we define $g(x) = \sum_{n=0}^\infty {1 \over 2^n}h(2^nx)$ then $g$ is the Takagi function. One can prove that $g$ is continuous and nowhere differentiable. I did both. Continuity follows easily from the Weierstrass M-test and for non-differentiability one can first show it for dyadic points and then for non-dyadic points. Now I am interested in two different modified versions of the Takagi function: $$ g_1(x) = \sum_{n=0}^\infty {1\over 2^n}h(3^n x)$$ and $$ g_2(x) = \sum_{n=0}^\infty {1\over 3^n}h(2^n x)$$ These two are easily seen to be continuous. Could someone please show me how to determine the differentiability of both $g_1$ and $g_2$? I really tried but failed.",,['real-analysis']
29,On the Lebesgue measure of a cartesian product,On the Lebesgue measure of a cartesian product,,"If $X \subseteq \mathbb{R}^{l}$ and $Y \subseteq \mathbb{R}^{r}$ with $l + r = n$, is it true that $\lambda_{n}(X \times Y) = \lambda_{l}(X) \cdot \lambda_{r}(Y)$ (where $\lambda_{m}$ is the Lebesgue measure in $\mathbb{R}^{m}$)? I know the result holds if $X$ and $Y$ are rectangles, but would like to know if it is true for arbitrary sets.","If $X \subseteq \mathbb{R}^{l}$ and $Y \subseteq \mathbb{R}^{r}$ with $l + r = n$, is it true that $\lambda_{n}(X \times Y) = \lambda_{l}(X) \cdot \lambda_{r}(Y)$ (where $\lambda_{m}$ is the Lebesgue measure in $\mathbb{R}^{m}$)? I know the result holds if $X$ and $Y$ are rectangles, but would like to know if it is true for arbitrary sets.",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
30,Difference of differentiation under integral sign between Lebesgue and Riemann,Difference of differentiation under integral sign between Lebesgue and Riemann,,"Here is a consequence of Lebesgue dominated convergence theorem on differentiation under integral sign. Function $f(x, t)$ is differentiable at $x_0$ for almost all $t \in A$, and $t \to f(x, t)$ is integrable. Moreover, there exist an integrable  function $g(t)$ such that $|(\partial{f}/\partial{x})(x,t)| \le g(t)$. Then we have   $$\frac{\mathrm{d}}{\mathrm{d}x} \left( \int_A f(x,t) ~ \mathrm{d}t \right)\bigg|_{x=x_0} = \int \frac{\partial{f}}{\partial{x}}(x_0,t) ~ \mathrm{d}t. $$ Of course, here the word ""integrable"" means ""Lebesgue integrable"". But, if we read the word ""integrable"" as ""improper Riemann integrable"" and add an assumption that the partial derivative is continuous, then I think the statement is still true. Weierstrass M-test for integrals guarantees uniform convergence of the improper integral and we can interchange the differentiaion and integration. If $f$ is integrable in both of (improper) Riemann and Lebesgue sense, the only gain of interpreting the integral as Lebesgue one is gettig rid of the continuity condition of the partial derivative. Is it right?","Here is a consequence of Lebesgue dominated convergence theorem on differentiation under integral sign. Function $f(x, t)$ is differentiable at $x_0$ for almost all $t \in A$, and $t \to f(x, t)$ is integrable. Moreover, there exist an integrable  function $g(t)$ such that $|(\partial{f}/\partial{x})(x,t)| \le g(t)$. Then we have   $$\frac{\mathrm{d}}{\mathrm{d}x} \left( \int_A f(x,t) ~ \mathrm{d}t \right)\bigg|_{x=x_0} = \int \frac{\partial{f}}{\partial{x}}(x_0,t) ~ \mathrm{d}t. $$ Of course, here the word ""integrable"" means ""Lebesgue integrable"". But, if we read the word ""integrable"" as ""improper Riemann integrable"" and add an assumption that the partial derivative is continuous, then I think the statement is still true. Weierstrass M-test for integrals guarantees uniform convergence of the improper integral and we can interchange the differentiaion and integration. If $f$ is integrable in both of (improper) Riemann and Lebesgue sense, the only gain of interpreting the integral as Lebesgue one is gettig rid of the continuity condition of the partial derivative. Is it right?",,"['real-analysis', 'convergence-divergence', 'lebesgue-integral']"
31,Uniform convergence of difference quotients to the derivative,Uniform convergence of difference quotients to the derivative,,"I remember being assigned the following homework problem a few years back. Let $f:[0,1] \to \mathbb{R}$ be continuously differentiable. Prove that, for every $\epsilon > 0$, there exists $\delta > 0$ such that $0 < |h| < \delta$ implies $\left| \frac{f(x+h) - f(x)}{h} - f'(x) \right| < \epsilon$ for all appropriate $x$. I also remember how I solved it. Fix $\epsilon > 0$. Since $f'$ is a continuous function on a compact interval, it is uniformly continuous and we can find a $\delta > 0$ such that $|x-y| < \delta$ implies $|f'(x) - f'(y)| < \epsilon$ for $x ,y \in [0,1]$. Suppose $0 < |h| < \delta$ and that $x$ is such that $x,x+h \in [0,1]$. By the Mean Value Theorem, there is an $a$ between $x$ and $x+h$ such that $f'(a) = \frac{f(x+h) - f(x)}{h}$. Note $|x - a| < \delta$ clearly holds. So,   $\left|\frac{f(x+h) - f(x)}{h} - f'(x) \right| = |f'(a) - f'(x)| < \epsilon$ and we are finished. What had me baffled was that this: we had not covered the MVT at the time the problem was assigned! This suggests there should be a way to prove it without using the MVT. Can anybody think of a way? I don't think I ever did. Thanks.","I remember being assigned the following homework problem a few years back. Let $f:[0,1] \to \mathbb{R}$ be continuously differentiable. Prove that, for every $\epsilon > 0$, there exists $\delta > 0$ such that $0 < |h| < \delta$ implies $\left| \frac{f(x+h) - f(x)}{h} - f'(x) \right| < \epsilon$ for all appropriate $x$. I also remember how I solved it. Fix $\epsilon > 0$. Since $f'$ is a continuous function on a compact interval, it is uniformly continuous and we can find a $\delta > 0$ such that $|x-y| < \delta$ implies $|f'(x) - f'(y)| < \epsilon$ for $x ,y \in [0,1]$. Suppose $0 < |h| < \delta$ and that $x$ is such that $x,x+h \in [0,1]$. By the Mean Value Theorem, there is an $a$ between $x$ and $x+h$ such that $f'(a) = \frac{f(x+h) - f(x)}{h}$. Note $|x - a| < \delta$ clearly holds. So,   $\left|\frac{f(x+h) - f(x)}{h} - f'(x) \right| = |f'(a) - f'(x)| < \epsilon$ and we are finished. What had me baffled was that this: we had not covered the MVT at the time the problem was assigned! This suggests there should be a way to prove it without using the MVT. Can anybody think of a way? I don't think I ever did. Thanks.",,"['calculus', 'real-analysis']"
32,Prove that $a^{4/a} + b^{4/b} + c^{4/c} \ge 3$,Prove that,a^{4/a} + b^{4/b} + c^{4/c} \ge 3,"Let $a, b, c > 0$ with $a + b + c = 3$ . Prove that $$a^{4/a} + b^{4/b} + c^{4/c} \ge 3.$$ This question was posted recently, closed and then deleted, due to missing of contexts etc. By https://approach0.xyz/ , the problem was proposed by Grotex@AoPS. My strategy is to split into many cases. WLOG, assume that $a \ge b \ge c$ . If $a \ge 8/5$ , true. If $a \le 10/7$ , let $f(x) = x^{4/x} - 1  - 4(x - 1)$ . We have $f(x) \ge 0$ for all $x \in (0, 10/7)$ . If $10/7 < a < 8/5$ and $b \ge 4/5$ , true. (I stopped here since this approach is ugly. Actually, the proof of $x^{4/x} - 1  - 4(x - 1) \ge 0$ for all $x\in (0, 10/7)$ is complicated.) I hope to see nice proofs.","Let with . Prove that This question was posted recently, closed and then deleted, due to missing of contexts etc. By https://approach0.xyz/ , the problem was proposed by Grotex@AoPS. My strategy is to split into many cases. WLOG, assume that . If , true. If , let . We have for all . If and , true. (I stopped here since this approach is ugly. Actually, the proof of for all is complicated.) I hope to see nice proofs.","a, b, c > 0 a + b + c = 3 a^{4/a} + b^{4/b} + c^{4/c} \ge 3. a \ge b \ge c a \ge 8/5 a \le 10/7 f(x) = x^{4/x} - 1  - 4(x - 1) f(x) \ge 0 x \in (0, 10/7) 10/7 < a < 8/5 b \ge 4/5 x^{4/x} - 1  - 4(x - 1) \ge 0 x\in (0, 10/7)","['real-analysis', 'inequality']"
33,Removal of an arbitrary point of the boundary of a closed and connected $A\subseteq\Bbb R^2$ so the new set remains connected,Removal of an arbitrary point of the boundary of a closed and connected  so the new set remains connected,A\subseteq\Bbb R^2,"Prove the following statement or find a counterexample: Let $A\subseteq\Bbb R^2$ be a closed and connected set. Then, $\exists c\in\partial A$ s. t. $A\setminus\{c\}$ is still connected. I think I found some counterexamples: $x$ or $y$ axis or any other line in $\Bbb R^2,$ as well as graphs of unbounded continuous functions defined on an open interval $I\subseteq\Bbb R$ or graphs of continuous functions defined on the whole $\Bbb R.$ Question : Is the unboundedness necessary for the statement not to hold?","Prove the following statement or find a counterexample: Let be a closed and connected set. Then, s. t. is still connected. I think I found some counterexamples: or axis or any other line in as well as graphs of unbounded continuous functions defined on an open interval or graphs of continuous functions defined on the whole Question : Is the unboundedness necessary for the statement not to hold?","A\subseteq\Bbb R^2 \exists c\in\partial A A\setminus\{c\} x y \Bbb R^2, I\subseteq\Bbb R \Bbb R.","['real-analysis', 'examples-counterexamples', 'connectedness']"
34,"Let $f:K\to K$ with $\|f(x)-f(y)\|\geq ||x-y||$ for all $x,y$. Show that equality holds and that $f$ is surjective. [duplicate]",Let  with  for all . Show that equality holds and that  is surjective. [duplicate],"f:K\to K \|f(x)-f(y)\|\geq ||x-y|| x,y f","This question already has answers here : Can this intuition give a proof that an isometry $f:X \to X$ is surjective for compact metric space $X$? (1 answer) Let $(M,d)$ be a compact metric space and $f:M \to M$ such that $d(f(x),f(y)) \ge d(x,y) , \forall x,y \in M$ , then $f$ is isometry? (4 answers) Closed 8 years ago . $K$ is a compact subset of $\Bbb R^n$ and $f:K\rightarrow K $ satisfies : $$\|f(x)-f(y)\|\geq \|x-y\|$$ Show that $f$ is bijective, and that : $$\|f(x)-f(y)\| = \|x-y\| $$ It's easy to show that $f$ is injective. But I can't think of a way to prove surjectivity.","This question already has answers here : Can this intuition give a proof that an isometry $f:X \to X$ is surjective for compact metric space $X$? (1 answer) Let $(M,d)$ be a compact metric space and $f:M \to M$ such that $d(f(x),f(y)) \ge d(x,y) , \forall x,y \in M$ , then $f$ is isometry? (4 answers) Closed 8 years ago . $K$ is a compact subset of $\Bbb R^n$ and $f:K\rightarrow K $ satisfies : $$\|f(x)-f(y)\|\geq \|x-y\|$$ Show that $f$ is bijective, and that : $$\|f(x)-f(y)\| = \|x-y\| $$ It's easy to show that $f$ is injective. But I can't think of a way to prove surjectivity.",,"['real-analysis', 'metric-spaces', 'compactness']"
35,$L^p$-space is a Hilbert space if and only if $p=2$,-space is a Hilbert space if and only if,L^p p=2,"Inspired by $\ell_p$ is Hilbert if and only if $p=2$ , I try to prove that a $L^p$ -space (provided with the standard norm) is a Hilbert space if and only if $p=2$ . I already know that every $L^p$ -space is a Banach space with respect to the standard norm. This is what I got so far. To prove Let $(S, \Sigma, \mu)$ be a measure space and assume that $\mu$ is a positive, $\sigma$ -finite measure that is not the trivial measure. Let $p\in [1, +\infty]$ . The normed space $(L^p(S,\Sigma , \mu), \| \cdot \| _{L^p} )$ is a Hilbert space if and only if $p=2$ . Proof Case 1: $p=2$ The standard inner product $\langle \cdot , \cdot \rangle _{L^2}$ induces the standard norm $\| \cdot \| _{L^2}$ , so $(L^2 (S, \Sigma , \mu), \| \cdot \| _{L^2})$ is a Hilbert space. Case 2: $p\in [1, \infty) \backslash \{ 2 \}$ Assume $(L^p(S,\Sigma , \mu), \| \cdot \| _{L^p} ) $ is a Hilbert space. Then it is also an inner product space, so it must satisfy the parallelogram rule: \begin{align} \forall f,g \in L^p(S,\Sigma , \mu): \ \ \| f + g \| ^2 + \| f -g \| ^2 = 2 ( \| f \| ^2 + \| g \| ^2 ).  \end{align} Let $A, B$ be disjoint measurable sets such that $0 < \mu (A), \ \mu (B) < \infty$ . Define $f_p, g_p \in  \mathcal{L} ^p (S, \Sigma, \mu)$ by \begin{align*} f_p  := \frac{1}{(\mu (A) ) ^{1/p}} 1 _A \geq 0 \ \ \ \text{ and } \ \ \ g_p  := \frac{1}{(\mu (B) ) ^{1/p}} 1 _B  \geq 0.\end{align*} Doing some calculations gives us \begin{align*} 2 \left ( \| f_p \| _{L^p} ^2  + \| g_p \| _{L^p} ^2 \right ) = 4 \ \ \text{ and } \ \ \| f_p + g_p \| _{L^p} ^2 + \| f_p - g_p \| _{L^p} ^2 = 2 \cdot 2 ^{2/p}. \end{align*} The functions $f_p$ and $g_p$ do not satisfy the parallelogram rule, since $4\neq 2 \cdot 2 ^{2/p}$ (remember that $p\neq 2$ ). But this contradicts our earlier conclusion that all $f,g\in L^p (S, \Sigma , \mu)$ must satisfy the parallelogram rule. Therefore, our assumption that $(L^p(S,\Sigma , \mu), \| \cdot \| _{L^p} ) $ is a Hilbert space is wrong. Hence, $(L^p(S,\Sigma , \mu), \| \cdot \| _{L^p} ) $ is not a Hilbert space. Case 3: $p=\infty$ Assume $(L^{\infty} (S,\Sigma , \mu), \| \cdot \| _{L^{\infty}} ) $ is a Hilbert space. Then it is also an inner product space, so it must satisfy the parallelogram rule. Let $A$ and $B$ be disjoint measurable sets which are not null sets and consider $F=1 _A$ and $G=1_B$ . Then, we have \begin{align*} \| F + G \| _{L^{\infty}} ^2 + \| F - G \| _{L^{\infty}} ^2 = 1 + 1 = 2 \neq 4 = 2(1+1) = 2( \| F \| _{L^{\infty}} ^2 + \| G \| _{L^{\infty}} ^2 ). \end{align*} But this contradicts our earlier conclusion that all $f,g\in L^{\infty} (S, \Sigma , \mu)$ must satisfy the parallelogram rule. Therefore, our assumption that $(L^{\infty} (S,\Sigma , \mu), \| \cdot \| _{L^{\infty}} ) $ is a Hilbert space is wrong. Hence, $(L^{\infty} (S,\Sigma , \mu), \| \cdot \| _{L^{\infty}} ) $ is not a Hilbert space. Question I am not completely sure about the following statements ""Let $A, B$ be disjoint measurable sets such that $0 < \mu (A), \ \mu (B) < \infty$ "" and ""Let $A$ and $B$ be disjoint measurable sets which are not null sets"". How do I know for sure that such measurable sets $A,B$ actually exist? Should I make more assumptions about the measure space in order to make these arguments work?","Inspired by is Hilbert if and only if , I try to prove that a -space (provided with the standard norm) is a Hilbert space if and only if . I already know that every -space is a Banach space with respect to the standard norm. This is what I got so far. To prove Let be a measure space and assume that is a positive, -finite measure that is not the trivial measure. Let . The normed space is a Hilbert space if and only if . Proof Case 1: The standard inner product induces the standard norm , so is a Hilbert space. Case 2: Assume is a Hilbert space. Then it is also an inner product space, so it must satisfy the parallelogram rule: Let be disjoint measurable sets such that . Define by Doing some calculations gives us The functions and do not satisfy the parallelogram rule, since (remember that ). But this contradicts our earlier conclusion that all must satisfy the parallelogram rule. Therefore, our assumption that is a Hilbert space is wrong. Hence, is not a Hilbert space. Case 3: Assume is a Hilbert space. Then it is also an inner product space, so it must satisfy the parallelogram rule. Let and be disjoint measurable sets which are not null sets and consider and . Then, we have But this contradicts our earlier conclusion that all must satisfy the parallelogram rule. Therefore, our assumption that is a Hilbert space is wrong. Hence, is not a Hilbert space. Question I am not completely sure about the following statements ""Let be disjoint measurable sets such that "" and ""Let and be disjoint measurable sets which are not null sets"". How do I know for sure that such measurable sets actually exist? Should I make more assumptions about the measure space in order to make these arguments work?","\ell_p p=2 L^p p=2 L^p (S, \Sigma, \mu) \mu \sigma p\in [1, +\infty] (L^p(S,\Sigma , \mu), \| \cdot \| _{L^p} ) p=2 p=2 \langle \cdot , \cdot \rangle _{L^2} \| \cdot \| _{L^2} (L^2 (S, \Sigma , \mu), \| \cdot \| _{L^2}) p\in [1, \infty) \backslash \{ 2 \} (L^p(S,\Sigma , \mu), \| \cdot \| _{L^p} )  \begin{align}
\forall f,g \in L^p(S,\Sigma , \mu): \ \ \| f + g \| ^2 + \| f -g \| ^2 = 2 ( \| f \| ^2 + \| g \| ^2 ). 
\end{align} A, B 0 < \mu (A), \ \mu (B) < \infty f_p, g_p \in 
\mathcal{L} ^p (S, \Sigma, \mu) \begin{align*} f_p  := \frac{1}{(\mu (A) ) ^{1/p}} 1 _A \geq 0 \ \ \ \text{ and } \ \ \ g_p  := \frac{1}{(\mu (B) ) ^{1/p}} 1 _B  \geq 0.\end{align*} \begin{align*} 2 \left ( \| f_p \| _{L^p} ^2  + \| g_p \| _{L^p} ^2 \right ) = 4 \ \ \text{ and } \ \ \| f_p + g_p \| _{L^p} ^2 + \| f_p - g_p \| _{L^p} ^2 = 2 \cdot 2 ^{2/p}. \end{align*} f_p g_p 4\neq 2 \cdot 2 ^{2/p} p\neq 2 f,g\in L^p (S, \Sigma , \mu) (L^p(S,\Sigma , \mu), \| \cdot \| _{L^p} )  (L^p(S,\Sigma , \mu), \| \cdot \| _{L^p} )  p=\infty (L^{\infty} (S,\Sigma , \mu), \| \cdot \| _{L^{\infty}} )  A B F=1 _A G=1_B \begin{align*}
\| F + G \| _{L^{\infty}} ^2 + \| F - G \| _{L^{\infty}} ^2 = 1 + 1 = 2 \neq 4 = 2(1+1) = 2( \| F \| _{L^{\infty}} ^2 + \| G \| _{L^{\infty}} ^2 ).
\end{align*} f,g\in L^{\infty} (S, \Sigma , \mu) (L^{\infty} (S,\Sigma , \mu), \| \cdot \| _{L^{\infty}} )  (L^{\infty} (S,\Sigma , \mu), \| \cdot \| _{L^{\infty}} )  A, B 0 < \mu (A), \ \mu (B) < \infty A B A,B","['real-analysis', 'functional-analysis', 'measure-theory', 'hilbert-spaces', 'banach-spaces']"
36,Investigate maxima of Gaussian integral over sphere.,Investigate maxima of Gaussian integral over sphere.,,"Let $\alpha>0$ be a positive parameter and consider the function $$f(x) = \int_{\mathbb S^{n-1}} e^{-\alpha \left\lVert x-y \right\rVert^2} dS(y)$$ for $x \in \mathbb R^n.$ So, since this was asked, although we integrate over the unit sphere, the function ""lives"" on $\mathbb R^n.$ This function is clearly rotationally symmetric. I would like to show that the global maxima are attained at one single radius $r$ , only. The rotational symmetry implies that we can consider it as a one-dimensional function by choosing $x=(x_1,0....,0)$ , this way the exponent simplifies to $e^{-\alpha \left\lVert x-y \right\rVert^2}=e^{-\alpha (x_1-y_1)^2+1-y_1^2}.$ If anything is unclear about this question, then please let me know. I am happy to hear about any ideas how to approach this problem. EDIT: Thanks to some interesting comments below, one can say that the global maximum is always attained at some radius $r \in [0,1]$ where for small $\alpha$ it seems to be attained close to zero and for large $\alpha$ it is attained closer to one. The question remains however why is there only one radius at which the global maximum is attained? -In fact as George Lowther points out in the comments, for $\alpha \le n/2$ the unique maximum is attained at $r=0$ which leaves the case $\alpha >n/2$ when this does not hold true.","Let be a positive parameter and consider the function for So, since this was asked, although we integrate over the unit sphere, the function ""lives"" on This function is clearly rotationally symmetric. I would like to show that the global maxima are attained at one single radius , only. The rotational symmetry implies that we can consider it as a one-dimensional function by choosing , this way the exponent simplifies to If anything is unclear about this question, then please let me know. I am happy to hear about any ideas how to approach this problem. EDIT: Thanks to some interesting comments below, one can say that the global maximum is always attained at some radius where for small it seems to be attained close to zero and for large it is attained closer to one. The question remains however why is there only one radius at which the global maximum is attained? -In fact as George Lowther points out in the comments, for the unique maximum is attained at which leaves the case when this does not hold true.","\alpha>0 f(x) = \int_{\mathbb S^{n-1}} e^{-\alpha \left\lVert x-y \right\rVert^2} dS(y) x \in \mathbb R^n. \mathbb R^n. r x=(x_1,0....,0) e^{-\alpha \left\lVert x-y \right\rVert^2}=e^{-\alpha (x_1-y_1)^2+1-y_1^2}. r \in [0,1] \alpha \alpha \alpha \le n/2 r=0 \alpha >n/2","['real-analysis', 'analysis']"
37,Integral$=-\frac{4}{3}\log^3 2-\frac{\pi^2}{3}\log 2+\frac{5}{2}\zeta(3)$,Integral,=-\frac{4}{3}\log^3 2-\frac{\pi^2}{3}\log 2+\frac{5}{2}\zeta(3),"Hi I have been trying to prove this $$ I:=\int \limits_{0}^{1} \left[ \frac{1}{x(x-1)} \bigg(2\mathrm{Li}_2\bigg(\frac{1-\sqrt{1-x}}{2}\bigg)-\log\bigg(\frac{1+\sqrt{1-x}}{2}\bigg)^2 \bigg) -\frac{\zeta(2)-2\log^2 2}{x-1} \right]{dx}=\sum_{k=2}^\infty \binom{2k}{k} \frac{1}{k^2 4^k} \sum_{j=1}^{k-1} \frac{1}{j}=\color{#00f}{\large% -{4 \over 3}\log^3 2-\frac{\pi^2}{3}\log 2+\frac{5}{2}\zeta(3)    } $$ What  a beautiful result!!!! I am trying to prove this. I am not sure of what to do, perhaps we could start with a change of variables $$ \xi=\frac{1-\sqrt{1-x}}{2}, $$ but I get stuck shortly after.  This is strongly related to Mahler measures and integration. Thanks for your help. I tried the following substitution but failed, UPDATE:  I tried a change of variables given above by $\xi$, we obtain $$ I=\int\limits_{0}^{1/2}\big(2\mathrm{Li}_2(\xi)-\log^2(1-\xi)\big)\left(\frac{4}{2\xi-1}-\frac{1}{\xi-1}-\frac{1}{\xi}\right)d\xi-4(\zeta(2)-2\log^2 2) \int\limits_0^{1/2}\frac{d\xi}{2\xi-1} $$ but the integral on the right diverges so I need to use another method now. Thanks","Hi I have been trying to prove this $$ I:=\int \limits_{0}^{1} \left[ \frac{1}{x(x-1)} \bigg(2\mathrm{Li}_2\bigg(\frac{1-\sqrt{1-x}}{2}\bigg)-\log\bigg(\frac{1+\sqrt{1-x}}{2}\bigg)^2 \bigg) -\frac{\zeta(2)-2\log^2 2}{x-1} \right]{dx}=\sum_{k=2}^\infty \binom{2k}{k} \frac{1}{k^2 4^k} \sum_{j=1}^{k-1} \frac{1}{j}=\color{#00f}{\large% -{4 \over 3}\log^3 2-\frac{\pi^2}{3}\log 2+\frac{5}{2}\zeta(3)    } $$ What  a beautiful result!!!! I am trying to prove this. I am not sure of what to do, perhaps we could start with a change of variables $$ \xi=\frac{1-\sqrt{1-x}}{2}, $$ but I get stuck shortly after.  This is strongly related to Mahler measures and integration. Thanks for your help. I tried the following substitution but failed, UPDATE:  I tried a change of variables given above by $\xi$, we obtain $$ I=\int\limits_{0}^{1/2}\big(2\mathrm{Li}_2(\xi)-\log^2(1-\xi)\big)\left(\frac{4}{2\xi-1}-\frac{1}{\xi-1}-\frac{1}{\xi}\right)d\xi-4(\zeta(2)-2\log^2 2) \int\limits_0^{1/2}\frac{d\xi}{2\xi-1} $$ but the integral on the right diverges so I need to use another method now. Thanks",,"['real-analysis', 'integration', 'measure-theory', 'definite-integrals', 'special-functions']"
38,Prove that $\lim_{n \rightarrow \infty} \sum_{k=0}^{n} \frac{1}{k!} = e$,Prove that,\lim_{n \rightarrow \infty} \sum_{k=0}^{n} \frac{1}{k!} = e,"Define $e, e'$ by $$e: =\lim _{n \rightarrow \infty} \left(1+\frac{1}{n}\right)^{n} \quad \text{and} \quad e' := \lim _{n \rightarrow \infty} \sum_{k=0}^{n} \frac{1}{k!}$$ Prove that $e' \in \mathbb R$ and $e' = e$ . Could you please verify whether my attempt is fine or contains logical gaps/errors? Any suggestion is greatly appreciated! PS: I've not learned about derivative and logarithmic function yet. My attempt: To make the presentation easier to follow, I set $$e_n :=  \left(1+\frac{1}{n}\right)^{n} \quad e'_n := \sum_{k=0}^{n} \frac{1}{k!}$$ Clearly, $e'_n + 1/((n+1)!) = e'_{n+1}$ and thus the sequence $(e'_n)$ is increasing. On the other hand, $e'_n = 2 + \sum_{k=2}^{n} \frac{1}{k!} \le 2 + \sum_{k=1}^{n} \frac{1}{k(k+1)} = 2+(1-\frac{1}{n+1}) = 3 - \frac{1}{n+1} < 3$ . So the sequence $(e'_n)$ is bounded from above and thus $e'$ is well defined. By binomial theorem, $e_n = \sum_{k=0}^n {n \choose k} \frac{1}{n^k}$ where ${n \choose k} \frac{1}{n^{k}}=\frac{1}{k !} \frac{n \cdot(n-1) \cdot \cdots \cdot(n-k+1)}{n \cdot n \cdot \cdots \cdot n} \leq \frac{1}{k !}$ . As such, $e_n \le \sum_{k=0}^n \frac{1}{k !} = e'_n$ and so $e \le e'$ . Next we prove that $e \ge e'$ . For $n \ge m$ , we have $$\begin{aligned} e_{n}=\left(1+\frac{1}{n}\right)^{n} &=\sum_{k=0}^{n} {n \choose k} \frac{1}{n^{k}} \geq \sum_{k=0}^{m} {n \choose k} \frac{1}{n^{k}} \\ &=1+\sum_{k=1}^{m} \frac{1}{k !} \frac{n (n-1) \cdots (n-k+1)}{n \cdots n} \\ &= 1+\sum_{k=1}^{m} \frac{1}{k !} \left[ 1 \cdot\left(1-\frac{1}{n}\right) \cdots\left(1-\frac{k-1}{n}\right) \right] \end{aligned}$$ For $n \ge m$ , I set $x_{n,m} = 1+\sum_{k=1}^{m} \frac{1}{k !} \left [ 1 \cdot\left(1-\frac{1}{n}\right) \cdots\left(1-\frac{k-1}{n}\right) \right]$ . Then $e_n \ge x_{n,m}$ and thus $e = \lim _{n \rightarrow \infty} e_n \ge \lim _{n \rightarrow \infty} x_{n,m} = e'_m$ . As such, $e \ge e'$ . This completes the proof.","Define by Prove that and . Could you please verify whether my attempt is fine or contains logical gaps/errors? Any suggestion is greatly appreciated! PS: I've not learned about derivative and logarithmic function yet. My attempt: To make the presentation easier to follow, I set Clearly, and thus the sequence is increasing. On the other hand, . So the sequence is bounded from above and thus is well defined. By binomial theorem, where . As such, and so . Next we prove that . For , we have For , I set . Then and thus . As such, . This completes the proof.","e, e' e: =\lim _{n \rightarrow \infty} \left(1+\frac{1}{n}\right)^{n} \quad \text{and} \quad e' := \lim _{n \rightarrow \infty} \sum_{k=0}^{n} \frac{1}{k!} e' \in \mathbb R e' = e e_n :=  \left(1+\frac{1}{n}\right)^{n} \quad e'_n := \sum_{k=0}^{n} \frac{1}{k!} e'_n + 1/((n+1)!) = e'_{n+1} (e'_n) e'_n = 2 + \sum_{k=2}^{n} \frac{1}{k!} \le 2 + \sum_{k=1}^{n} \frac{1}{k(k+1)} = 2+(1-\frac{1}{n+1}) = 3 - \frac{1}{n+1} < 3 (e'_n) e' e_n = \sum_{k=0}^n {n \choose k} \frac{1}{n^k} {n \choose k} \frac{1}{n^{k}}=\frac{1}{k !} \frac{n \cdot(n-1) \cdot \cdots \cdot(n-k+1)}{n \cdot n \cdot \cdots \cdot n} \leq \frac{1}{k !} e_n \le \sum_{k=0}^n \frac{1}{k !} = e'_n e \le e' e \ge e' n \ge m \begin{aligned} e_{n}=\left(1+\frac{1}{n}\right)^{n} &=\sum_{k=0}^{n} {n \choose k} \frac{1}{n^{k}} \geq \sum_{k=0}^{m} {n \choose k} \frac{1}{n^{k}} \\ &=1+\sum_{k=1}^{m} \frac{1}{k !} \frac{n (n-1) \cdots (n-k+1)}{n \cdots n} \\ &= 1+\sum_{k=1}^{m} \frac{1}{k !} \left[ 1 \cdot\left(1-\frac{1}{n}\right) \cdots\left(1-\frac{k-1}{n}\right) \right] \end{aligned} n \ge m x_{n,m} = 1+\sum_{k=1}^{m} \frac{1}{k !} \left [ 1 \cdot\left(1-\frac{1}{n}\right) \cdots\left(1-\frac{k-1}{n}\right) \right] e_n \ge x_{n,m} e = \lim _{n \rightarrow \infty} e_n \ge \lim _{n \rightarrow \infty} x_{n,m} = e'_m e \ge e'","['real-analysis', 'sequences-and-series', 'proof-verification', 'constants']"
39,Does $f'(x)\in \mathbb Z$ a.e. implies that $f$ is an affine function?,Does  a.e. implies that  is an affine function?,f'(x)\in \mathbb Z f,"Conjecture: Let $f: \mathbb R \to \mathbb R$ be an everywhere differentiable function and  assume that $f'(x) \in \mathbb Z$ almost everywhere . Then is $f$ necessarily an affine function? Can you give me a proof or a counter-example ? I thought of the devil's staircase, but this is not differentiable everywhere.","Conjecture: Let $f: \mathbb R \to \mathbb R$ be an everywhere differentiable function and  assume that $f'(x) \in \mathbb Z$ almost everywhere . Then is $f$ necessarily an affine function? Can you give me a proof or a counter-example ? I thought of the devil's staircase, but this is not differentiable everywhere.",,"['real-analysis', 'analysis', 'measure-theory', 'derivatives']"
40,How do permutations of $\Bbb N$ affect series?,How do permutations of  affect series?,\Bbb N,"Let $G$ be the group of all permutations of $\mathbb{N}$ and $\sum a_n$ a conditionally convergent series of reals. What do we know about how $G$ ""acts"" on this series? We can partition $G$ according to how permutations $\pi$ affect the value $\sum_n a_{\pi n}$ or, more finely, the behavior of its partial sums. How much do we know about such partitions of $G$ ? There should be some subgroup $H\subset G$ of permutations that do not affect the value of any conditionally convergent series. This includes all finite permutations, but it should also include any permutation $\pi$ for which $|\pi(i)-i|$ is bounded. Does $H$ include any more permutations than this, or is this all of $H$ ? We can define $\Gamma_x\subset G$ as the set of $\pi$ for which $\sum a_{\pi n}=x$ . If $\sum a_n=x$ then this is like a stabilizer, but I don't see any obvious reason to expect it to be a subgroup. Or, for general $x$ , any reason to expect $\Gamma_x$ to act like a coset (e.g. the $\Gamma_x$ s being translates of each other). Do these sets satisfy properties similar to cosets? (One thing I can see is that each $\Gamma_x$ is a union of right cosets of $H$ .) Given a function $f:\mathbb{N}\to \mathbb{R}$ we can define $\Gamma_f$ as the set of those $\pi$ for which $\sum_{n\le N}a_{\pi n}\sim f(N)$ . There are many variations we could use on this definition, and we could even use it for divergent series $\sum a_n$ . Or, even if $S=\sum a_n$ is convergent, we could define $\Gamma_f$ (for $f\to 0$ ) according to $-S+\sum_{n\le N}a_{\pi n}\sim f(N)$ (so we partition $G$ according to how permutations affect the convergence of the series). I expect describing $\Gamma$ s becomes much harder in these situations. Do we know more answers to any of these questions for specific series, like harmonic or alternating harmonic?","Let be the group of all permutations of and a conditionally convergent series of reals. What do we know about how ""acts"" on this series? We can partition according to how permutations affect the value or, more finely, the behavior of its partial sums. How much do we know about such partitions of ? There should be some subgroup of permutations that do not affect the value of any conditionally convergent series. This includes all finite permutations, but it should also include any permutation for which is bounded. Does include any more permutations than this, or is this all of ? We can define as the set of for which . If then this is like a stabilizer, but I don't see any obvious reason to expect it to be a subgroup. Or, for general , any reason to expect to act like a coset (e.g. the s being translates of each other). Do these sets satisfy properties similar to cosets? (One thing I can see is that each is a union of right cosets of .) Given a function we can define as the set of those for which . There are many variations we could use on this definition, and we could even use it for divergent series . Or, even if is convergent, we could define (for ) according to (so we partition according to how permutations affect the convergence of the series). I expect describing s becomes much harder in these situations. Do we know more answers to any of these questions for specific series, like harmonic or alternating harmonic?",G \mathbb{N} \sum a_n G G \pi \sum_n a_{\pi n} G H\subset G \pi |\pi(i)-i| H H \Gamma_x\subset G \pi \sum a_{\pi n}=x \sum a_n=x x \Gamma_x \Gamma_x \Gamma_x H f:\mathbb{N}\to \mathbb{R} \Gamma_f \pi \sum_{n\le N}a_{\pi n}\sim f(N) \sum a_n S=\sum a_n \Gamma_f f\to 0 -S+\sum_{n\le N}a_{\pi n}\sim f(N) G \Gamma,"['real-analysis', 'sequences-and-series', 'group-theory', 'convergence-divergence', 'permutations']"
41,Bounding a polynomial from below,Bounding a polynomial from below,,"Let $\sigma >0$ be fixed. For even $k \in \mathbb{N} \cup \{0\}$ , we consider the polynomial \begin{equation} \varphi_k(x) = \sum_{j=0}^{k} (-1)^j {k \choose j} b_j \, x^{2j} \quad x \in (-1,1), \end{equation} where \begin{equation} b_j = \frac{\Big(k+\sigma+\frac12\Big)_j}{\Big(\frac12 \Big)_j} \end{equation} and for $s \in \mathbb{R}$ , $(s)_j$ denotes the Pochhammer symbol \begin{equation} (s)_{j}={\begin{cases}1&j=0\\s(s+1)\cdots (s+j-1)&j>0.\end{cases}} \end{equation} In particular, $\varphi_0(x) =1$ . My question is the following. For $-1 < a < b < 1$ , does there exist $c = c(a, b, \sigma)>0$ such that \begin{equation} \int_a^b \varphi_k(x)^2 dx \geq c \quad \end{equation} for any even $k \in \mathbb{N} \cup \{0\}$ ? Unless I am mistaken, a straightforward computation yields \begin{equation} \int_a^b \varphi_k(x)^2 dx = \sum_{j=0}^k \sum_{\ell=0}^k (-1)^{j+\ell} {k \choose j} {k \choose \ell} \frac{b_j b_{\ell}}{2(j+\ell)+1} \, (b^{2(j+\ell)+1}-a^{2(j+\ell)+1}). \end{equation} But I do not see how I may bound this double sum from below. Remark 1: I dont know if it is of any use, one may notice that $\varphi_k$ is a hypergeometric function of the form ${}_{2}F_{1}(-k,k+\sigma+\frac12;\frac12;x^2)$ (see https://en.wikipedia.org/wiki/Hypergeometric_function ). Remark 2: Using this interpretation as a hypergeometric function (which terminates), it is in fact possible to relate $\varphi_k$ to the Jacobi polynomials ( https://en.wikipedia.org/wiki/Jacobi_polynomials ): \begin{equation} \varphi_k(x) = {}_{2}F_{1}(-k,\sigma +\frac12 +k;\frac12; x^2)={\frac {k!}{(\alpha +1)_{k}}}P_{k}^{(-\frac12 ,\sigma )}(1-2x^2). \end{equation} Perhaps this observation may be of use. Comment : The same question is open for any odd $k \in \mathbb{N}$ , but this time one considers \begin{equation} \varphi_k(x) = \sum_{j=0}^{k} (-1)^j {k \choose j} c_j \, x^{2j+1}, \end{equation} with $ c_j = \frac{\Big(k+\sigma+\frac32\Big)_j}{\Big(\frac32 \Big)_j} $ . I suspect that the methodology is similar as for the case where $k$ is even.","Let be fixed. For even , we consider the polynomial where and for , denotes the Pochhammer symbol In particular, . My question is the following. For , does there exist such that for any even ? Unless I am mistaken, a straightforward computation yields But I do not see how I may bound this double sum from below. Remark 1: I dont know if it is of any use, one may notice that is a hypergeometric function of the form (see https://en.wikipedia.org/wiki/Hypergeometric_function ). Remark 2: Using this interpretation as a hypergeometric function (which terminates), it is in fact possible to relate to the Jacobi polynomials ( https://en.wikipedia.org/wiki/Jacobi_polynomials ): Perhaps this observation may be of use. Comment : The same question is open for any odd , but this time one considers with . I suspect that the methodology is similar as for the case where is even.","\sigma >0 k \in \mathbb{N} \cup \{0\} \begin{equation}
\varphi_k(x) = \sum_{j=0}^{k} (-1)^j {k \choose j} b_j \, x^{2j} \quad x \in (-1,1),
\end{equation} \begin{equation}
b_j = \frac{\Big(k+\sigma+\frac12\Big)_j}{\Big(\frac12 \Big)_j}
\end{equation} s \in \mathbb{R} (s)_j \begin{equation}
(s)_{j}={\begin{cases}1&j=0\\s(s+1)\cdots (s+j-1)&j>0.\end{cases}}
\end{equation} \varphi_0(x) =1 -1 < a < b < 1 c = c(a, b, \sigma)>0 \begin{equation}
\int_a^b \varphi_k(x)^2 dx \geq c \quad
\end{equation} k \in \mathbb{N} \cup \{0\} \begin{equation}
\int_a^b \varphi_k(x)^2 dx = \sum_{j=0}^k \sum_{\ell=0}^k (-1)^{j+\ell} {k \choose j} {k \choose \ell} \frac{b_j b_{\ell}}{2(j+\ell)+1} \, (b^{2(j+\ell)+1}-a^{2(j+\ell)+1}).
\end{equation} \varphi_k {}_{2}F_{1}(-k,k+\sigma+\frac12;\frac12;x^2) \varphi_k \begin{equation}
\varphi_k(x) = {}_{2}F_{1}(-k,\sigma +\frac12 +k;\frac12; x^2)={\frac {k!}{(\alpha +1)_{k}}}P_{k}^{(-\frac12 ,\sigma )}(1-2x^2).
\end{equation} k \in \mathbb{N} \begin{equation}
\varphi_k(x) = \sum_{j=0}^{k} (-1)^j {k \choose j} c_j \, x^{2j+1},
\end{equation} 
c_j = \frac{\Big(k+\sigma+\frac32\Big)_j}{\Big(\frac32 \Big)_j}  k","['real-analysis', 'inequality', 'polynomials', 'special-functions']"
42,Convergence of an alternating series : $ \sum_{n\geq 1} \frac{(-1)^n|\sin n|}{n}$,Convergence of an alternating series :, \sum_{n\geq 1} \frac{(-1)^n|\sin n|}{n},"Study the convergence of $$\displaystyle \sum_{n\geq 1} \frac{(-1)^n|\sin n|}{n}.$$ I am stuck with this series, we need probably some measure of irrationally of $\pi$, unfortunately I am unfamiliar with this. So here is my attempt : Let $f(x) = \sum \frac{|\sin{n}|}{n} x^n, |x| < 1$ It's not difficult to compute the Fourier series of $|\sin(x)|$ : $$ \displaystyle|\sin(x)|=\frac{2}{\pi}-\frac{4}{\pi}\sum_{n=1}^{+\infty}\frac{\cos(2nx)}{4n^2-1} $$ Then Fubini's theorem ( Series Version ) works very well (because the previous series converges absolutely at $x$ fixed ) and all calculations made, we find that for all $x\in( -1,1)$: $$ \displaystyle f(x)=\frac{2}{\pi}\sum_{n=1}^{+\infty}\frac{x^n}{n}-\frac{4}{\pi}\sum_{p=1}^{+\infty}\frac{x^2-2x\cos(p)}{(4p^2-1)(x^2-2x\cos(p)+1)} $$ However, the second sum I have not been able to show the convergence. I feel the series diverge because the following series $$ \displaystyle\sum\frac{1}{p^2\sin^2\left(\frac{p}{2}\right)} $$ diverge because  $0$ is an accumulation point of $\displaystyle (n\sin(n))$ sequence. Any ideas (for the original series) ?","Study the convergence of $$\displaystyle \sum_{n\geq 1} \frac{(-1)^n|\sin n|}{n}.$$ I am stuck with this series, we need probably some measure of irrationally of $\pi$, unfortunately I am unfamiliar with this. So here is my attempt : Let $f(x) = \sum \frac{|\sin{n}|}{n} x^n, |x| < 1$ It's not difficult to compute the Fourier series of $|\sin(x)|$ : $$ \displaystyle|\sin(x)|=\frac{2}{\pi}-\frac{4}{\pi}\sum_{n=1}^{+\infty}\frac{\cos(2nx)}{4n^2-1} $$ Then Fubini's theorem ( Series Version ) works very well (because the previous series converges absolutely at $x$ fixed ) and all calculations made, we find that for all $x\in( -1,1)$: $$ \displaystyle f(x)=\frac{2}{\pi}\sum_{n=1}^{+\infty}\frac{x^n}{n}-\frac{4}{\pi}\sum_{p=1}^{+\infty}\frac{x^2-2x\cos(p)}{(4p^2-1)(x^2-2x\cos(p)+1)} $$ However, the second sum I have not been able to show the convergence. I feel the series diverge because the following series $$ \displaystyle\sum\frac{1}{p^2\sin^2\left(\frac{p}{2}\right)} $$ diverge because  $0$ is an accumulation point of $\displaystyle (n\sin(n))$ sequence. Any ideas (for the original series) ?",,['real-analysis']
43,Increasing derivatives of recursively defined polynomials,Increasing derivatives of recursively defined polynomials,,"Consider recursively defined polynomials $f_0(x) = x$ and $f_{n+1}(x) = f_n(x) - f_n'(x) x (1-x)$. These polynomials have some special properties, for example $f_n(0) = 0$, $f_n(1) = 1$, and all $n+1$ roots of $f_n$ are in $[0,1)$.  Let $x_n$ denote the largest root of $f_n$. Then $f_n(x_n) = 0$ and $f_n'(x_n)>0$. Moreover, $x_n > x_{n-1}$ for all $n$. I want to prove the following claim: $f_{n}'(x_{n+1}) > f_{n-1}'(x_{n+1})$ for all $n \geq 2$. Note that the claim does not hold for arbitrary $x$. The derivatives are polynomials themselves, by Gauss-Lucas theorem all their roots are in $[0,x_n)$ and there are many points where $f_{n}'(x) < 0 < f_{n-1}'(x)$. However, I am quite sure that at $x \geq x_{n+1}$, the derivatives are ordered: $f_1'(x) < \dots < f_n'(x)$. Some of the first polynomials are: $f_1(x) = x^2$, $f_1'(x) = 2x$, $x_1 = 0$ $f_2(x) = 2x^3 - x^2$, $f_2'(x) = 6 x^2 -2x$, $x_2 = \frac{1}{2}$ $f_3(x) = 6 x^4 - 6x^3 + x^2$, $f_3'(x) = 24x^3-18x^2+ 2x$, $x_3 \approx 0.7887$ $f_4(x) = 24 x^5 - 36 x^4 + 14 x^3 -x^2$, $f_4'(x) =120x^4-144x^3 +42x^2 -2x$, $x_4 \approx 0.9082$ Therefore $f_2'(x)-f_1'(x) = 6x^2-4x \geq 0$ for all $x \geq \frac{2}{3}$. Note that, $x_2 < \frac{2}{3} < x_3$. For $f_3'(x) - f_2'(x) = 24 x^3-24 x^2 + 4 x \geq 0$ for all $x \geq 0.7887$. Here it turns out that at $x_3$ the inequality holds as an equality (coincidence perhaps?), but of course then for $x_4 > x_3$ it holds as a strict inequality.","Consider recursively defined polynomials $f_0(x) = x$ and $f_{n+1}(x) = f_n(x) - f_n'(x) x (1-x)$. These polynomials have some special properties, for example $f_n(0) = 0$, $f_n(1) = 1$, and all $n+1$ roots of $f_n$ are in $[0,1)$.  Let $x_n$ denote the largest root of $f_n$. Then $f_n(x_n) = 0$ and $f_n'(x_n)>0$. Moreover, $x_n > x_{n-1}$ for all $n$. I want to prove the following claim: $f_{n}'(x_{n+1}) > f_{n-1}'(x_{n+1})$ for all $n \geq 2$. Note that the claim does not hold for arbitrary $x$. The derivatives are polynomials themselves, by Gauss-Lucas theorem all their roots are in $[0,x_n)$ and there are many points where $f_{n}'(x) < 0 < f_{n-1}'(x)$. However, I am quite sure that at $x \geq x_{n+1}$, the derivatives are ordered: $f_1'(x) < \dots < f_n'(x)$. Some of the first polynomials are: $f_1(x) = x^2$, $f_1'(x) = 2x$, $x_1 = 0$ $f_2(x) = 2x^3 - x^2$, $f_2'(x) = 6 x^2 -2x$, $x_2 = \frac{1}{2}$ $f_3(x) = 6 x^4 - 6x^3 + x^2$, $f_3'(x) = 24x^3-18x^2+ 2x$, $x_3 \approx 0.7887$ $f_4(x) = 24 x^5 - 36 x^4 + 14 x^3 -x^2$, $f_4'(x) =120x^4-144x^3 +42x^2 -2x$, $x_4 \approx 0.9082$ Therefore $f_2'(x)-f_1'(x) = 6x^2-4x \geq 0$ for all $x \geq \frac{2}{3}$. Note that, $x_2 < \frac{2}{3} < x_3$. For $f_3'(x) - f_2'(x) = 24 x^3-24 x^2 + 4 x \geq 0$ for all $x \geq 0.7887$. Here it turns out that at $x_3$ the inequality holds as an equality (coincidence perhaps?), but of course then for $x_4 > x_3$ it holds as a strict inequality.",,"['real-analysis', 'polynomials', 'recurrence-relations', 'roots']"
44,Proving the set of the strictly increasing sequences of natural numbers is not enumerable.,Proving the set of the strictly increasing sequences of natural numbers is not enumerable.,,"How would one proceed to prove this statement? The set of the strictly increasing sequences of natural numbers is not enumerable. I've been trying to solve this for quite a while, however I don't even know where to start.","How would one proceed to prove this statement? The set of the strictly increasing sequences of natural numbers is not enumerable. I've been trying to solve this for quite a while, however I don't even know where to start.",,"['real-analysis', 'sequences-and-series', 'proof-writing', 'set-theory']"
45,e is irrational,e is irrational,,"Prove that e is an irrational number. Recall that $\,\mathrm{e}=\displaystyle\sum_{n=0}^\infty\frac{1}{n!},\,\,$ and assume $\,\mathrm{e}\,$ is rational, then $$\sum\limits_{k=0}^\infty \frac{1}{k!} = \frac{a}{b},\quad \text{for some positive integers}\,\,\, a,b.$$ so $$b\sum\limits_{k=0}^\infty \frac{1}{k!} =a$$ or $$ b\left(1+1+\frac{1}{2} + \frac{1}{6} +\cdots \right)= a. $$ Where can I go from here?","Prove that e is an irrational number. Recall that and assume is rational, then so or Where can I go from here?","\,\mathrm{e}=\displaystyle\sum_{n=0}^\infty\frac{1}{n!},\,\, \,\mathrm{e}\, \sum\limits_{k=0}^\infty \frac{1}{k!} = \frac{a}{b},\quad \text{for some positive integers}\,\,\, a,b. b\sum\limits_{k=0}^\infty \frac{1}{k!} =a 
b\left(1+1+\frac{1}{2} + \frac{1}{6} +\cdots \right)= a.
","['real-analysis', 'calculus', 'exponential-function', 'irrational-numbers', 'transcendental-numbers']"
46,Is there any nonconstant function that grows (at infinity) slower than all iterations of the (natural) logarithm?,Is there any nonconstant function that grows (at infinity) slower than all iterations of the (natural) logarithm?,,Is there any nonconstant function that grows at infinity slower than all iterations of the (natural) logarithm?,Is there any nonconstant function that grows at infinity slower than all iterations of the (natural) logarithm?,,"['real-analysis', 'limits', 'logarithms', 'asymptotics']"
47,"Is there a continuous bijection between an interval $[0,1]$ and a square: $[0,1] \times [0,1]$?",Is there a continuous bijection between an interval  and a square: ?,"[0,1] [0,1] \times [0,1]","Is there a continuous bijection from $[0,1]$ onto $[0,1] \times [0,1]$? That is with $I=[0,1]$ and $S=[0,1] \times [0,1]$, is there a continuous bijection $$ f: I \to S? $$ I know there is a continuous bijection $g:C \to I$ from the Cantor set $C$ to $[0,1]$. The square $S$ is compact so there is a continuous function  $$ h: C \to S. $$ But this leads nowhere. Is there a way to construct such an $f$? I ask because I have a continuous functional $F:S \to \mathbb R$. For numerical reason, I would like to convert it into the functional $$ G: I \to \mathbb R, \\ G = F \circ f , $$ so that $G$ is continuous.","Is there a continuous bijection from $[0,1]$ onto $[0,1] \times [0,1]$? That is with $I=[0,1]$ and $S=[0,1] \times [0,1]$, is there a continuous bijection $$ f: I \to S? $$ I know there is a continuous bijection $g:C \to I$ from the Cantor set $C$ to $[0,1]$. The square $S$ is compact so there is a continuous function  $$ h: C \to S. $$ But this leads nowhere. Is there a way to construct such an $f$? I ask because I have a continuous functional $F:S \to \mathbb R$. For numerical reason, I would like to convert it into the functional $$ G: I \to \mathbb R, \\ G = F \circ f , $$ so that $G$ is continuous.",,['real-analysis']
48,Definition of convolution?,Definition of convolution?,,"Why do we use $x - y$ rather than $x + y$ in the definition of the convolution? Is it just convention? (If we are thinking of convolutions as weighted averages, for instance against ""good kernels,"" it should make no difference.) Why $(f * g) (x) = \int f(y) g(x - y) dy$ rather than $(f * g) (x) = \int f(y) g(x + y) dy$? Edit: I'm finding it really hard to choose a best answer. There are at least three very good ones here.","Why do we use $x - y$ rather than $x + y$ in the definition of the convolution? Is it just convention? (If we are thinking of convolutions as weighted averages, for instance against ""good kernels,"" it should make no difference.) Why $(f * g) (x) = \int f(y) g(x - y) dy$ rather than $(f * g) (x) = \int f(y) g(x + y) dy$? Edit: I'm finding it really hard to choose a best answer. There are at least three very good ones here.",,"['real-analysis', 'fourier-analysis', 'convolution']"
49,The Main Theorems of Calculus,The Main Theorems of Calculus,,"From the J. Taylor's book, ''The completeness property is the missing ingredient in most calculus course. It is seldom discussed, but without it, one cannot prove the main theorems of calculus.'' My question is: Why (without it), one cannot prove the main theorems of calculus?","From the J. Taylor's book, ''The completeness property is the missing ingredient in most calculus course. It is seldom discussed, but without it, one cannot prove the main theorems of calculus.'' My question is: Why (without it), one cannot prove the main theorems of calculus?",,['calculus']
50,"If the series $\sum_0^\infty a_n$ converges, then so does $\sum_1^\infty \frac{\sqrt{a_n}}{n} $ [duplicate]","If the series  converges, then so does  [duplicate]",\sum_0^\infty a_n \sum_1^\infty \frac{\sqrt{a_n}}{n} ,"This question already has answers here : If $\sum_{n=1}^{\infty} a_n^{2}$ converges, then so does $\sum_{n=1}^{\infty} \frac {a_n}{n}$ (6 answers) Closed 6 years ago . Problem: Suppose that for every $n\in\mathbb{N}$, $a_n\in\mathbb{R}$ and $a_n\ge 0$. Given that   $$\sum_0^\infty a_n$$   converges, show that    $$\sum_1^\infty \frac{\sqrt{a_n}}{n} $$   converges. Source : Rudin, Principles of Mathematical Analysis , Chapter 3, Exercise 7.","This question already has answers here : If $\sum_{n=1}^{\infty} a_n^{2}$ converges, then so does $\sum_{n=1}^{\infty} \frac {a_n}{n}$ (6 answers) Closed 6 years ago . Problem: Suppose that for every $n\in\mathbb{N}$, $a_n\in\mathbb{R}$ and $a_n\ge 0$. Given that   $$\sum_0^\infty a_n$$   converges, show that    $$\sum_1^\infty \frac{\sqrt{a_n}}{n} $$   converges. Source : Rudin, Principles of Mathematical Analysis , Chapter 3, Exercise 7.",,"['real-analysis', 'sequences-and-series']"
51,why do we use 'non-increasing' instead of decreasing?,why do we use 'non-increasing' instead of decreasing?,,"In english based math language it seems that non-increasing $\Longleftrightarrow$ less or equal  (non-strict decreasing) decreasing $\Longleftrightarrow$ strict less    (    strict decreasing) Is that correct ? If so, how does it make sense ? precision I should note that even very good math teacher are making mistakes about this. Actually I asked this question after watching Boyd's video on convex optimization where even him is confused about this.... So I imagine many many people are, and there must be classes and tests about this absurd and buggy concept, which yields absolutely nothing interesting. So I just wonder if I really am missing something, or if, yes, some people decided to create an abstraction that is leaky (not not increasing $\neq$ increasing ?) verbose (4 words, with special negation logic, instead of using the word 'strict'  and keeping the usual well defined predicate logic rules) absurdity of the concept This notation is absurd for the following reason : when dealing with element instead of functions we dont apply the same logic : we dont phrase $x < y$ as  "" $x$ is less than $y$ "" nor  "" $x\leq y$ "" as "" $x$ is not-more than $y$ "". (If we did though, at least it would not be so harmful as not not-more would mean more) you have to define functions using a not notation, $f$ is non-increasing function $\Longrightarrow$ if $x$ is not-less than $y$ , say 0.3 feet and 2.5 inches, then $f(x)$ is not-more than $f(y)$ This also violates a very basic tenet in programming style 101, which is here for a reason : never define or use something with a negation, it is confusing. To apply composition rules between functions, you better be buckled up with all the not. must be a fluff of cases More profoundly, this violates a fundamental principle of logic which is that given some ambiguity, you should assume the most general case apply. It is way worse than measuring things with non integral units. This is violating logical rules, and leaving a very basic concept obfuscated .","In english based math language it seems that non-increasing less or equal  (non-strict decreasing) decreasing strict less    (    strict decreasing) Is that correct ? If so, how does it make sense ? precision I should note that even very good math teacher are making mistakes about this. Actually I asked this question after watching Boyd's video on convex optimization where even him is confused about this.... So I imagine many many people are, and there must be classes and tests about this absurd and buggy concept, which yields absolutely nothing interesting. So I just wonder if I really am missing something, or if, yes, some people decided to create an abstraction that is leaky (not not increasing increasing ?) verbose (4 words, with special negation logic, instead of using the word 'strict'  and keeping the usual well defined predicate logic rules) absurdity of the concept This notation is absurd for the following reason : when dealing with element instead of functions we dont apply the same logic : we dont phrase as  "" is less than "" nor  "" "" as "" is not-more than "". (If we did though, at least it would not be so harmful as not not-more would mean more) you have to define functions using a not notation, is non-increasing function if is not-less than , say 0.3 feet and 2.5 inches, then is not-more than This also violates a very basic tenet in programming style 101, which is here for a reason : never define or use something with a negation, it is confusing. To apply composition rules between functions, you better be buckled up with all the not. must be a fluff of cases More profoundly, this violates a fundamental principle of logic which is that given some ambiguity, you should assume the most general case apply. It is way worse than measuring things with non integral units. This is violating logical rules, and leaving a very basic concept obfuscated .",\Longleftrightarrow \Longleftrightarrow \neq x < y x y x\leq y x y f \Longrightarrow x y f(x) f(y),"['calculus', 'real-analysis', 'terminology', 'definition']"
52,cutoff function vs mollifiers,cutoff function vs mollifiers,,"$\boldsymbol{Q_1}$ What are cutoff functions? What are mollifiers? I cannot distinguish the two. Could anyone give some concrete/simple examples of cutoff functions and how they differ from mollifiers? $\boldsymbol{\text{I did check wiki (so please, I do not want wiki type answer)}}$ $\boldsymbol{Q_2}$ How to obtain compact support using a cutoff function? Could anyone give a example? Or take a look at the last sentence of Lemma 1.5 and explain what it means mathematically? enter link description here Many Thanks!",What are cutoff functions? What are mollifiers? I cannot distinguish the two. Could anyone give some concrete/simple examples of cutoff functions and how they differ from mollifiers? How to obtain compact support using a cutoff function? Could anyone give a example? Or take a look at the last sentence of Lemma 1.5 and explain what it means mathematically? enter link description here Many Thanks!,"\boldsymbol{Q_1} \boldsymbol{\text{I did check wiki (so please, I do not want wiki type answer)}} \boldsymbol{Q_2}","['real-analysis', 'partial-differential-equations', 'sobolev-spaces', 'distribution-theory', 'regularity-theory-of-pdes']"
53,Simplifying square of integral in general,Simplifying square of integral in general,,"For a real-valued function $f=f(x)$, over the real variable $x$, with the following integral $$ \left[ \int_{a}^{b} f(x)dx \right]^{2}, $$ is there a known general method/approach to handle this as to remove the squaring from over the integral, say by making changes to the integrand and/or interval, and then proceed with a form like $\int g(x)dx$ afterwards, were $g(x)$ is some other function?","For a real-valued function $f=f(x)$, over the real variable $x$, with the following integral $$ \left[ \int_{a}^{b} f(x)dx \right]^{2}, $$ is there a known general method/approach to handle this as to remove the squaring from over the integral, say by making changes to the integrand and/or interval, and then proceed with a form like $\int g(x)dx$ afterwards, were $g(x)$ is some other function?",,"['calculus', 'real-analysis', 'integration', 'functional-analysis', 'analysis']"
54,"If $\lim_{x\to 0}\left(f(x)+\frac{1}{f(x)}\right)=2,$ show that $\lim_{x\to 0}f(x)=1$.",If  show that .,"\lim_{x\to 0}\left(f(x)+\frac{1}{f(x)}\right)=2, \lim_{x\to 0}f(x)=1","Question: Suppose $f:(-\delta,\delta)\to (0,\infty)$ has the property that $$\lim_{x\to 0}\left(f(x)+\frac{1}{f(x)}\right)=2.$$ Show that $\lim_{x\to 0}f(x)=1$ . My approach: Let $h:(-\delta,\delta)\to(-1,\infty)$ be such that $h(x)=f(x)-1, \forall x\in(-\delta,\delta).$ Note that if we can show that $\lim_{x\to 0}h(x)=0$ , then we will be done. Now since we have $$\lim_{x\to 0}\left(f(x)+\frac{1}{f(x)}\right)=2\implies \lim_{x\to 0}\frac{(f(x)-1)^2}{f(x)}=0\implies \lim_{x\to 0}\frac{h^2(x)}{h(x)+1}=0.$$ Next I tried to come up with some bounds in order to use Sandwich theorem to show that $\lim_{x\to 0} h(x)=0,$ but the bounds didn't quite work out. The bounds were the following: $$\begin{cases}h(x)\ge \frac{h^2(x)}{h(x)+1},\text{when }h(x)\ge 0,\\h(x)<\frac{h^2(x)}{h(x)+1},\text{when }h(x)<0.\end{cases}$$ How to proceed after this?","Question: Suppose has the property that Show that . My approach: Let be such that Note that if we can show that , then we will be done. Now since we have Next I tried to come up with some bounds in order to use Sandwich theorem to show that but the bounds didn't quite work out. The bounds were the following: How to proceed after this?","f:(-\delta,\delta)\to (0,\infty) \lim_{x\to 0}\left(f(x)+\frac{1}{f(x)}\right)=2. \lim_{x\to 0}f(x)=1 h:(-\delta,\delta)\to(-1,\infty) h(x)=f(x)-1, \forall x\in(-\delta,\delta). \lim_{x\to 0}h(x)=0 \lim_{x\to 0}\left(f(x)+\frac{1}{f(x)}\right)=2\implies \lim_{x\to 0}\frac{(f(x)-1)^2}{f(x)}=0\implies \lim_{x\to 0}\frac{h^2(x)}{h(x)+1}=0. \lim_{x\to 0} h(x)=0, \begin{cases}h(x)\ge \frac{h^2(x)}{h(x)+1},\text{when }h(x)\ge 0,\\h(x)<\frac{h^2(x)}{h(x)+1},\text{when }h(x)<0.\end{cases}","['real-analysis', 'calculus', 'limits']"
55,"Integral $\int_0^\infty \log(1-e^{-a x})\cos (bx)\, dx=\frac{a}{2b^2}-\frac{\pi}{2b}\coth \frac{\pi b}{a}$",Integral,"\int_0^\infty \log(1-e^{-a x})\cos (bx)\, dx=\frac{a}{2b^2}-\frac{\pi}{2b}\coth \frac{\pi b}{a}","$$\mathcal{J}:=\int_0^\infty \log(1-e^{-a x})\cos (bx)\, dx=\frac{a}{2b^2}-\frac{\pi}{2b}\coth  \frac{\pi b}{a},\qquad \mathcal{Re}(a)>0, b>0. $$ I tried to write $$ \mathcal{J}=-\int_0^\infty  \sum_{n=1}^\infty\frac{e^{-anx}}{n}\cos(bx)\,dx  $$ but the taylors series, $\log (1-\xi)=-\sum_{n=1}^\infty \xi^n/n, \ |\xi|<1$, thus this is not so useful for doing the integral.  I tried to also write $$ \mathcal{J}=\frac{1}{b}\int_0^\infty \log(1-e^{-ax})d(\sin bx)=\frac{1}{b}\left(\log(1-e^{-ax})\sin (bx)\big|^\infty_0  -a\int_0^\infty  \frac{\sin (bx)}{{e^{ax}-1}}dx \right), $$ the boundary term vanishes so we have $$ \mathcal{J}=\frac{a}{b}\int_0^\infty \frac{\sin(bx)}{1-e^{ax}}dx=\frac{a}{b}\mathcal{Im}\bigg[\int_0^\infty \frac{e^{ibx}}{e^{ax}-1}dx\bigg] $$ which I am not sure how to solve.  Notice there are singularities at $x=2i\pi n/a, \ n\in \mathbb{Z}$. We need to calculate the residue for all the singularities along the imaginary axis.  The residue contribution to the integral $$ 2\pi i\cdot \sum_{n= 0}^\infty \frac{ e^{-2\pi  nb/a}}{e^{2i \pi n}}=2\pi i \sum_{n=0}^\infty e^{n( -2\pi b/a-2i\pi)}=\frac{2\pi i}{e^{-(2\pi b/a+2\pi i)}}$$ Taking the imaginary part gives and re-writing the integral gives a different result. Where did I go wrong?  How can we calculate this?  Thanks","$$\mathcal{J}:=\int_0^\infty \log(1-e^{-a x})\cos (bx)\, dx=\frac{a}{2b^2}-\frac{\pi}{2b}\coth  \frac{\pi b}{a},\qquad \mathcal{Re}(a)>0, b>0. $$ I tried to write $$ \mathcal{J}=-\int_0^\infty  \sum_{n=1}^\infty\frac{e^{-anx}}{n}\cos(bx)\,dx  $$ but the taylors series, $\log (1-\xi)=-\sum_{n=1}^\infty \xi^n/n, \ |\xi|<1$, thus this is not so useful for doing the integral.  I tried to also write $$ \mathcal{J}=\frac{1}{b}\int_0^\infty \log(1-e^{-ax})d(\sin bx)=\frac{1}{b}\left(\log(1-e^{-ax})\sin (bx)\big|^\infty_0  -a\int_0^\infty  \frac{\sin (bx)}{{e^{ax}-1}}dx \right), $$ the boundary term vanishes so we have $$ \mathcal{J}=\frac{a}{b}\int_0^\infty \frac{\sin(bx)}{1-e^{ax}}dx=\frac{a}{b}\mathcal{Im}\bigg[\int_0^\infty \frac{e^{ibx}}{e^{ax}-1}dx\bigg] $$ which I am not sure how to solve.  Notice there are singularities at $x=2i\pi n/a, \ n\in \mathbb{Z}$. We need to calculate the residue for all the singularities along the imaginary axis.  The residue contribution to the integral $$ 2\pi i\cdot \sum_{n= 0}^\infty \frac{ e^{-2\pi  nb/a}}{e^{2i \pi n}}=2\pi i \sum_{n=0}^\infty e^{n( -2\pi b/a-2i\pi)}=\frac{2\pi i}{e^{-(2\pi b/a+2\pi i)}}$$ Taking the imaginary part gives and re-writing the integral gives a different result. Where did I go wrong?  How can we calculate this?  Thanks",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
56,$K\subseteq \mathbb{R}^n$ is a compact space iff every continuous function in $K$ is bounded.,is a compact space iff every continuous function in  is bounded.,K\subseteq \mathbb{R}^n K,I need to prove that $K\subseteq \mathbb{R}^n$ is a compact space iff every continuous function  in $K$ is bounded. One direction is obvious because of Weierstrass theorem. How can i prove the other direction? I tried to assume the opposite but it didn't work for me. Thanks a lot.,I need to prove that $K\subseteq \mathbb{R}^n$ is a compact space iff every continuous function  in $K$ is bounded. One direction is obvious because of Weierstrass theorem. How can i prove the other direction? I tried to assume the opposite but it didn't work for me. Thanks a lot.,,"['real-analysis', 'general-topology', 'compactness']"
57,Why not define 'limits' to include isolated points?,Why not define 'limits' to include isolated points?,,"If I understand correctly, most definitions of 'limits' require that the function either a) be defined in an open neighborhood around the relevant point or b) more permissively, that the relevant point is a limit point; the definition of 'continuity' is then given a special case so that functions are continuous at isolated points. Why not extend the notion of 'limit' so that the limit of a function at an isolated point is just whatever the function's value is there?  Is there some good reason not to?","If I understand correctly, most definitions of 'limits' require that the function either a) be defined in an open neighborhood around the relevant point or b) more permissively, that the relevant point is a limit point; the definition of 'continuity' is then given a special case so that functions are continuous at isolated points. Why not extend the notion of 'limit' so that the limit of a function at an isolated point is just whatever the function's value is there?  Is there some good reason not to?",,"['calculus', 'real-analysis', 'definition']"
58,Infinite intersection of open sets,Infinite intersection of open sets,,"I need to prove that the infinite intersection of open sets may [must] not be open. I can show through examples that this is true, but this is not sufficient for a proof. - Can somebody give a formal proof ? Thanks.","I need to prove that the infinite intersection of open sets may [must] not be open. I can show through examples that this is true, but this is not sufficient for a proof. - Can somebody give a formal proof ? Thanks.",,['real-analysis']
59,Intuitive meaning of Limit Supremum?,Intuitive meaning of Limit Supremum?,,"I am trying to understand the difference between the following two equations: $$\bar{P} = \limsup_{t \to \infty}\frac{1}{t} \sum_{\tau = 0}^{t-1}E\{P[\tau]\} < \infty$$ and  $$\bar{P} = \lim_{t \to \infty}\frac{1}{t} \sum_{\tau = 0}^{t-1}E\{P[\tau]\} < \infty$$ where $\bar{P}$ denotes the average value of P and E stands for expectation. I have previously come across equations like the second one but I am not able to understand when to use equations of the first type. I have read the definition on Wikipedia's Supremum page but I am failing to understand the intuitive meaning of when to use what. The wiki defines it as: A set A of real numbers (shown as blue   balls), a set of upper bounds of A   (red balls), and the smallest such   upper bound, that is, the supremum of   A (shown as a red diamond). What does a set of upper bounds actually mean? I thought upper bound means the uppermost value but I guess my understanding is flawed. Can someone please tell me the difference between the two and give me some easy to understand example to understand the difference between a normal limit and supremum limit?","I am trying to understand the difference between the following two equations: $$\bar{P} = \limsup_{t \to \infty}\frac{1}{t} \sum_{\tau = 0}^{t-1}E\{P[\tau]\} < \infty$$ and  $$\bar{P} = \lim_{t \to \infty}\frac{1}{t} \sum_{\tau = 0}^{t-1}E\{P[\tau]\} < \infty$$ where $\bar{P}$ denotes the average value of P and E stands for expectation. I have previously come across equations like the second one but I am not able to understand when to use equations of the first type. I have read the definition on Wikipedia's Supremum page but I am failing to understand the intuitive meaning of when to use what. The wiki defines it as: A set A of real numbers (shown as blue   balls), a set of upper bounds of A   (red balls), and the smallest such   upper bound, that is, the supremum of   A (shown as a red diamond). What does a set of upper bounds actually mean? I thought upper bound means the uppermost value but I guess my understanding is flawed. Can someone please tell me the difference between the two and give me some easy to understand example to understand the difference between a normal limit and supremum limit?",,"['real-analysis', 'reference-request', 'limits', 'examples-counterexamples', 'limsup-and-liminf']"
60,Does $\sum_{n=4}^\infty \left(\frac{1}{\log(\log(n))}\right)^{\log(n)}$ converge?,Does  converge?,\sum_{n=4}^\infty \left(\frac{1}{\log(\log(n))}\right)^{\log(n)},"I have this series that I can't understand how to find it's character. $$\tag{1} \sum_{n=4}^\infty \left(\frac{1}{\log(\log(n))}\right)^{\log(n)} $$ To exercise, I solved a similiar series to get the ground how to solve the previous one. $$\tag{2} \sum_{n=2}^\infty \left(\frac{1}{\log(n)}\right)^{\log(n)} $$ This is my solution: $ a_n $ is a series made by positive terms, so it can't be unsolvable (diverges positively or converges). $$ \log(n)^{\log(n)} = (e^{\log(n)})^{\log(\log(n))} = n^{\log(\log(n))} \ge n^{\log(3)} $$ $ \forall n >e^3$ , $\log(3)>3$ , $\log(\log(n))>\log(3)>1$ . So we have that $$ \frac{1}{\log(n)^{\log(n)}} \le \frac{1}{n^{\log(3)}} $$ $\forall n> e^3 $ . And by the critery of asymptotic comparison by the fact that $$\tag{3} \sum \frac{1}{n^{\log(3)}}$$ is the generalized harmonic series with $p=\log(3)>1 $ so it converge. $(3)$ converges so then $(2)$ converges. I don't know how to use this to solve $(1)$ , I can't find any inequality to get it right.","I have this series that I can't understand how to find it's character. To exercise, I solved a similiar series to get the ground how to solve the previous one. This is my solution: is a series made by positive terms, so it can't be unsolvable (diverges positively or converges). , , . So we have that . And by the critery of asymptotic comparison by the fact that is the generalized harmonic series with so it converge. converges so then converges. I don't know how to use this to solve , I can't find any inequality to get it right.",\tag{1} \sum_{n=4}^\infty \left(\frac{1}{\log(\log(n))}\right)^{\log(n)}  \tag{2} \sum_{n=2}^\infty \left(\frac{1}{\log(n)}\right)^{\log(n)}   a_n   \log(n)^{\log(n)} = (e^{\log(n)})^{\log(\log(n))} = n^{\log(\log(n))} \ge n^{\log(3)}   \forall n >e^3 \log(3)>3 \log(\log(n))>\log(3)>1  \frac{1}{\log(n)^{\log(n)}} \le \frac{1}{n^{\log(3)}}  \forall n> e^3  \tag{3} \sum \frac{1}{n^{\log(3)}} p=\log(3)>1  (3) (2) (1),"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'summation']"
61,"Is a rational-valued continuous function $f\colon[0,1]\to\mathbb{R}$ constant?",Is a rational-valued continuous function  constant?,"f\colon[0,1]\to\mathbb{R}","Let $f\colon[0,1]\to\mathbb{R}$ be continuous such that $f(x)\in\mathbb{Q}$ for any $x\in[0,1]$. Intuitively I feel that $f$ is constant, since $\mathbb{Q}$ is dense in $\mathbb{R}$. How can I formally write this down?","Let $f\colon[0,1]\to\mathbb{R}$ be continuous such that $f(x)\in\mathbb{Q}$ for any $x\in[0,1]$. Intuitively I feel that $f$ is constant, since $\mathbb{Q}$ is dense in $\mathbb{R}$. How can I formally write this down?",,"['real-analysis', 'continuity']"
62,If $z_n \to z$ then $(1+z_n/n)^n \to e^z$,If  then,z_n \to z (1+z_n/n)^n \to e^z,We are dealing with $z \in \mathbb{C}$. I know that $$ \left(1+ \frac{z}{n} \right)^n \to e^{z} $$ as $n \to \infty$. So intuitively if $z_n \to z$ then we should have $$ \left(1+ \frac{z_n}{n} \right)^n \to e^{z}. $$ If $z_n \in \mathbb{R}$ I would be happy writing $$ \exp \left(n \log\left(1+ \frac{z_n}{n} \right) \right) = \exp \left(z_n \frac{\log\left(1+ \frac{z_n}{n} \right)-\log(1+0)}{\frac{z_n}{n}-0} \right) \to \exp(z \cdot 1) $$ where here we are using the definition of derivative. But if $z \in \mathbb{C}$ the logarithm is multivalued and I'm not sure that the same calculation is allowed. Is there a different way to show this for complex $z$?,We are dealing with $z \in \mathbb{C}$. I know that $$ \left(1+ \frac{z}{n} \right)^n \to e^{z} $$ as $n \to \infty$. So intuitively if $z_n \to z$ then we should have $$ \left(1+ \frac{z_n}{n} \right)^n \to e^{z}. $$ If $z_n \in \mathbb{R}$ I would be happy writing $$ \exp \left(n \log\left(1+ \frac{z_n}{n} \right) \right) = \exp \left(z_n \frac{\log\left(1+ \frac{z_n}{n} \right)-\log(1+0)}{\frac{z_n}{n}-0} \right) \to \exp(z \cdot 1) $$ where here we are using the definition of derivative. But if $z \in \mathbb{C}$ the logarithm is multivalued and I'm not sure that the same calculation is allowed. Is there a different way to show this for complex $z$?,,"['real-analysis', 'complex-analysis', 'limits', 'exponential-function']"
63,Does existence of anti-derivative imply integrability?,Does existence of anti-derivative imply integrability?,,"If $f$ has an anti-derivative in $[a,b]$ does it imply that $f$ is Riemann integrable in $[a,b]$?","If $f$ has an anti-derivative in $[a,b]$ does it imply that $f$ is Riemann integrable in $[a,b]$?",,"['calculus', 'real-analysis']"
64,"If a polynomial $P$ has only real roots, so does $P'-2xP$","If a polynomial  has only real roots, so does",P P'-2xP,Let $P$ be a polynomial such that all of the roots of $P$ are real. Prove that all the roots of $P'-2xP$ are also real. I know a proof for this fact but it is very computational and messy. Is there an elegant solution?,Let be a polynomial such that all of the roots of are real. Prove that all the roots of are also real. I know a proof for this fact but it is very computational and messy. Is there an elegant solution?,P P P'-2xP,"['real-analysis', 'polynomials', 'roots']"
65,Real analysis contradiction I cannot get rid of,Real analysis contradiction I cannot get rid of,,"Let $G$ be the Cantor set. It is well known that: $G$ is perfect and hence closed. $G$ has the cardinality of the continuum. $G$ has measure zero. For any set $S \subset \mathbb{R}$ (I will not keep writing that we are in $\mathbb{R}$) we have $S \text{ is closed} \Leftrightarrow S^c \text{ is open}$. Any open set $O$ can be written as a --- in fact unique --- countable union of disjoint open intervals. $G^c$ can thus be written as a countable union of disjoint open intervals. We now imagine this union as being superposed on the real line graphically as follows: R: <<<----(....)---(..)--(.)---------(...)--->>> where the (...) represents the open disjoint intervals (of differing size) composing $G^c$, and the --- represents the remaining non-covered real numbers (that is those in $G$). Now we can cover $\mathbb{R}$ in its entirety by ""collecting"" the --- into disjoint closed intervals. Any one of these closed intervals might of course consist of only a single element. We now obtain: R: <<<[--](....)[-](..)[](.)[-------](...)[-]>>> Take the union of these disjoint closed intervals. This must be $(G^c)^c = G$. Now it is not hard to imagine a mapping from the (...) 's to the [...] 's. Just take the next (...) in line for each [...] (and do some trivial fixing at the ends). Therefore we have written $G$ as a countable union of disjoint closed sets. However, $G$ has measure zero and therefore cannot contain any closed sets other than the single element type. Hence $G$ is countable. Contradiction. Where do I go wrong?","Let $G$ be the Cantor set. It is well known that: $G$ is perfect and hence closed. $G$ has the cardinality of the continuum. $G$ has measure zero. For any set $S \subset \mathbb{R}$ (I will not keep writing that we are in $\mathbb{R}$) we have $S \text{ is closed} \Leftrightarrow S^c \text{ is open}$. Any open set $O$ can be written as a --- in fact unique --- countable union of disjoint open intervals. $G^c$ can thus be written as a countable union of disjoint open intervals. We now imagine this union as being superposed on the real line graphically as follows: R: <<<----(....)---(..)--(.)---------(...)--->>> where the (...) represents the open disjoint intervals (of differing size) composing $G^c$, and the --- represents the remaining non-covered real numbers (that is those in $G$). Now we can cover $\mathbb{R}$ in its entirety by ""collecting"" the --- into disjoint closed intervals. Any one of these closed intervals might of course consist of only a single element. We now obtain: R: <<<[--](....)[-](..)[](.)[-------](...)[-]>>> Take the union of these disjoint closed intervals. This must be $(G^c)^c = G$. Now it is not hard to imagine a mapping from the (...) 's to the [...] 's. Just take the next (...) in line for each [...] (and do some trivial fixing at the ends). Therefore we have written $G$ as a countable union of disjoint closed sets. However, $G$ has measure zero and therefore cannot contain any closed sets other than the single element type. Hence $G$ is countable. Contradiction. Where do I go wrong?",,"['real-analysis', 'measure-theory', 'cantor-set']"
66,How is the acting of $H^{-1}$ on $H^1_0$ defined?,How is the acting of  on  defined?,H^{-1} H^1_0,"I have a question about the Sobolev Space $H^1_0(U)$, where $U$ is a open subset of $\mathbb{R}^n$. Let us denote with $H^{-1}(U)$ the dual space of $H^1_0$. How is the acting of $H^{-1}$ and $H^1_0$ defined, i.e.   $$ \langle \phi,u\rangle $$ where $\phi\in H^{-1}$ and $u\in H^1_0$? Furthermore, if I have elements $v,u\in H^1_0$, why is it true that   $$(u,v)_{L^2} = \langle u,v\rangle$$   where the latter should denote again the dual pairing of $H^1_0$ and $H^{-1}$. Thanks for your help hulik","I have a question about the Sobolev Space $H^1_0(U)$, where $U$ is a open subset of $\mathbb{R}^n$. Let us denote with $H^{-1}(U)$ the dual space of $H^1_0$. How is the acting of $H^{-1}$ and $H^1_0$ defined, i.e.   $$ \langle \phi,u\rangle $$ where $\phi\in H^{-1}$ and $u\in H^1_0$? Furthermore, if I have elements $v,u\in H^1_0$, why is it true that   $$(u,v)_{L^2} = \langle u,v\rangle$$   where the latter should denote again the dual pairing of $H^1_0$ and $H^{-1}$. Thanks for your help hulik",,['real-analysis']
67,Euler's Constant: The asymptotic behavior of $\left(\sum\limits_{j=1}^{N} \frac{1}{j}\right) - \log(N)$,Euler's Constant: The asymptotic behavior of,\left(\sum\limits_{j=1}^{N} \frac{1}{j}\right) - \log(N),"I want to show that there exists a constant $\gamma\in\mathbb{R}$ such that $$ \sum_{j=1}^N \frac1{j} = \log(N)+\gamma+O(1/N). $$  I know how to prove that the Euler-Mascheroni constant exists (which I believe $\gamma$ to be), but I am having trouble with the big-$O$ notation and the subsequent bounding. I've considered $$ \left|\left(\sum_{j=1}^N \frac1{j}\right) - \log(N)-\gamma\right|\le |K/N| $$ for some $K$, and I was approaching this by trying to show the that the left side of the inequality decays faster, but so far am stuck. Any advice for this type of problem, or analogous ones, would be appreciated.  Thanks!","I want to show that there exists a constant $\gamma\in\mathbb{R}$ such that $$ \sum_{j=1}^N \frac1{j} = \log(N)+\gamma+O(1/N). $$  I know how to prove that the Euler-Mascheroni constant exists (which I believe $\gamma$ to be), but I am having trouble with the big-$O$ notation and the subsequent bounding. I've considered $$ \left|\left(\sum_{j=1}^N \frac1{j}\right) - \log(N)-\gamma\right|\le |K/N| $$ for some $K$, and I was approaching this by trying to show the that the left side of the inequality decays faster, but so far am stuck. Any advice for this type of problem, or analogous ones, would be appreciated.  Thanks!",,"['real-analysis', 'limits', 'asymptotics']"
68,Convergence of $\sum_n \frac{|\sin(n^2)|}{n}$,Convergence of,\sum_n \frac{|\sin(n^2)|}{n},"A problem in Makarov's Selected problems in real analysis asks to investigate the convergence of $\displaystyle \sum_n \frac{|\sin(n^2)|}{n}$ I'm clueless at the moment. I can't find any good property of the sequence $|\sin(n^2)|$. $|\sin(n^2)|$ is small whenever $n\sim \sqrt{p\pi}$, and, as $p\to \infty$, the $\sqrt{p\pi}$ get closer to each other since $\sqrt{(p+1)\pi}-\sqrt{p\pi}\sim \frac 12 \sqrt{\frac{\pi}{p}}$. Any hint is appreciated.","A problem in Makarov's Selected problems in real analysis asks to investigate the convergence of $\displaystyle \sum_n \frac{|\sin(n^2)|}{n}$ I'm clueless at the moment. I can't find any good property of the sequence $|\sin(n^2)|$. $|\sin(n^2)|$ is small whenever $n\sim \sqrt{p\pi}$, and, as $p\to \infty$, the $\sqrt{p\pi}$ get closer to each other since $\sqrt{(p+1)\pi}-\sqrt{p\pi}\sim \frac 12 \sqrt{\frac{\pi}{p}}$. Any hint is appreciated.",,"['real-analysis', 'sequences-and-series']"
69,Measure Convergence Version of Lebesgue Dominated Convergence Theorem,Measure Convergence Version of Lebesgue Dominated Convergence Theorem,,I want to prove that LDCT(Lebesgue Dominated Convergence Theorem) continues to hold if I replace the hypothesis $f_n \to f$ (convergence pointwise) with $f_n\to f$ (convergence in measure):     $$\int fd\lambda=\lim_{n\to\infty}\int f_nd\lambda.$$,I want to prove that LDCT(Lebesgue Dominated Convergence Theorem) continues to hold if I replace the hypothesis $f_n \to f$ (convergence pointwise) with $f_n\to f$ (convergence in measure):     $$\int fd\lambda=\lim_{n\to\infty}\int f_nd\lambda.$$,,"['real-analysis', 'measure-theory', 'convergence-divergence', 'lebesgue-integral']"
70,"Axiom of choice, non-measurable sets, countable unions","Axiom of choice, non-measurable sets, countable unions",,"I have been looking through several mathoverflow posts, especially these ones https://mathoverflow.net/questions/32720/non-borel-sets-without-axiom-of-choice , https://mathoverflow.net/questions/73902/axiom-of-choice-and-non-measurable-set and there still are many questions I would like to ask: 1) According to the first answer of the first post ""It is consistent with ZF without choice that the reals are the countable union of countable sets"" (and therefore all sets are borel, and hence measurable), however this seems in contrast with the answer to the second post which states that ""the existence of a non-Lebesgue measurable set does not imply the axiom of choice"" (and therefore it is possible to construct a ZF model without choice where there exists a non-Lebesgue-measurable set). How can these two statements be both right? 2) I can't understand why the axiom of (countable) choice is necessary to prove that a countable union of countable sets is countable. By saying that the sets are countable, I have already assumed the existence of a bijection from every set to the set of natural numbers, in other words, I have indexed the elements of each set. So what is the problem in chosing elements from each set? This relates to the above topic in that if the AC weren't necessary to prove that countable union of countable sets is countable, then ""It is consistent with ZF without choice that the reals are the countable union of countable sets"" can no longer be correct, since this would imply that in ZF without choice the reals are countable. I am only a third year math student with no background in set theory (only naive), so please excuse the ignorance. I hope someone can answer me, thank you!","I have been looking through several mathoverflow posts, especially these ones https://mathoverflow.net/questions/32720/non-borel-sets-without-axiom-of-choice , https://mathoverflow.net/questions/73902/axiom-of-choice-and-non-measurable-set and there still are many questions I would like to ask: 1) According to the first answer of the first post ""It is consistent with ZF without choice that the reals are the countable union of countable sets"" (and therefore all sets are borel, and hence measurable), however this seems in contrast with the answer to the second post which states that ""the existence of a non-Lebesgue measurable set does not imply the axiom of choice"" (and therefore it is possible to construct a ZF model without choice where there exists a non-Lebesgue-measurable set). How can these two statements be both right? 2) I can't understand why the axiom of (countable) choice is necessary to prove that a countable union of countable sets is countable. By saying that the sets are countable, I have already assumed the existence of a bijection from every set to the set of natural numbers, in other words, I have indexed the elements of each set. So what is the problem in chosing elements from each set? This relates to the above topic in that if the AC weren't necessary to prove that countable union of countable sets is countable, then ""It is consistent with ZF without choice that the reals are the countable union of countable sets"" can no longer be correct, since this would imply that in ZF without choice the reals are countable. I am only a third year math student with no background in set theory (only naive), so please excuse the ignorance. I hope someone can answer me, thank you!",,"['real-analysis', 'logic', 'measure-theory', 'set-theory', 'axiom-of-choice']"
71,A rare integral involving $\operatorname{Li}_2$,A rare integral involving,\operatorname{Li}_2,"A rare but interesting integral problem: $$\int_{0}^{1}  \frac{\operatorname{Li}_2(-x)- \operatorname{Li}_2(1-x)+\ln(x)\ln(1+x)+\pi x\ln(1+x) -\pi x\ln(x)}{1+x^2}\frac{\text{d}x}{\sqrt{1-x^2} }  =\frac{\pi^3}{48\sqrt{2} }.$$ Where $\operatorname{Li}_2$ is dilogarithm . The integral without the factor $\frac{1}{\sqrt{1-x^2} }$ is much easier(it can be expressed using $\operatorname{Li}_3$ , $\ln(2)$ , Catalan's constant and $\pi$ ). However, the same idea dosen't work for this one. Any suggestion will be much appreciated.","A rare but interesting integral problem: Where is dilogarithm . The integral without the factor is much easier(it can be expressed using , , Catalan's constant and ). However, the same idea dosen't work for this one. Any suggestion will be much appreciated.","\int_{0}^{1} 
\frac{\operatorname{Li}_2(-x)-
\operatorname{Li}_2(1-x)+\ln(x)\ln(1+x)+\pi x\ln(1+x)
-\pi x\ln(x)}{1+x^2}\frac{\text{d}x}{\sqrt{1-x^2} } 
=\frac{\pi^3}{48\sqrt{2} }. \operatorname{Li}_2 \frac{1}{\sqrt{1-x^2} } \operatorname{Li}_3 \ln(2) \pi","['real-analysis', 'integration', 'definite-integrals', 'closed-form', 'polylogarithm']"
72,L'Hopital's rule and $\frac{\sin x}x$,L'Hopital's rule and,\frac{\sin x}x,"I have heard people say that you can't (or shouldn't) use the L'Hopital's rule to calculate $\lim\limits_{x\to 0}\frac{\sin x}x=\lim\limits_{x\to 0}\cos x=1$, because the result $\frac d{dx}\sin x=\cos x$ is derived by using that limit. But is that opinion justified? Why should I be vary of applying L'Hopital's rule to that limit? I don't see any problem with it. The sine function fulfills the conditions of the L'Hopital's  rule. Also, it is a fact that the derivative of sine is cosine, no matter how we proved it. Certainly there is a way to prove $\frac d{dx}\sin x=\cos x$ without using the said limit (if someone knows how, they can post it) so we don't even have any circular logic. Even if there isn't,  $\frac d{dx}\sin x=\cos x$ was proven sometime without referencing the L'Hopital's rule so we know it is true. Why wouldn't we then freely apply the L'Hopital's rule to $\frac {\sin x}x$? PS I'm not saying that this is the best method to derive the limit or anything, but that I don't understand why it is so frowned upon and often considered invalid.","I have heard people say that you can't (or shouldn't) use the L'Hopital's rule to calculate $\lim\limits_{x\to 0}\frac{\sin x}x=\lim\limits_{x\to 0}\cos x=1$, because the result $\frac d{dx}\sin x=\cos x$ is derived by using that limit. But is that opinion justified? Why should I be vary of applying L'Hopital's rule to that limit? I don't see any problem with it. The sine function fulfills the conditions of the L'Hopital's  rule. Also, it is a fact that the derivative of sine is cosine, no matter how we proved it. Certainly there is a way to prove $\frac d{dx}\sin x=\cos x$ without using the said limit (if someone knows how, they can post it) so we don't even have any circular logic. Even if there isn't,  $\frac d{dx}\sin x=\cos x$ was proven sometime without referencing the L'Hopital's rule so we know it is true. Why wouldn't we then freely apply the L'Hopital's rule to $\frac {\sin x}x$? PS I'm not saying that this is the best method to derive the limit or anything, but that I don't understand why it is so frowned upon and often considered invalid.",,"['calculus', 'real-analysis']"
73,Show that there's no continuous function that takes each of its values $f(x)$ exactly twice.,Show that there's no continuous function that takes each of its values  exactly twice.,f(x),"I need to prove the following: There's no continuous function $f:[a,b]\to \mathbb{R}$ that takes each of its values $f(x)$, $x\in [a,b]$  exactly twice. First of all, I didn't understand the question. For example $x^2$ takes $1$ twice, in the interval $[-1,1]$. Is it saying that it does not occur for all $x$ in the interval? But what about $f(x) = c$? Is it saying that it does not occur only exactly $2$ times, then? I have no idea about how to prove it. I know that for $f(x)$ such that $f(a)<f(x)<f(b)$, if $f$ is continuous then there is a $c\in [a,b]$ such that $f(c) = f(x)$. Now, there's the following proof in my book and I really wanted to understand it, instead of just getting a new proof Since the interval $[a,b]$ has only $2$ extreme points, then the maximum or minimum of $f$ must be in a point $c\in int([a,b])$ and and in another point $d\in [a,b]$. Then, there exists $\delta>0$ such that in the intervals $[c-\delta, c), (c,c+\delta)$ (and if $d$ is not extreme of $[a,b]$, $[d-\delta, d]$) the function takes values that are less than $f(c) = f(d)$. Let $A$ be the greatest of the numbers $f(c-\delta), f(c+\delta), f(d-\delta)$. By the intermediate value theorem, there are $x\in [c-\delta, c), y\in (c, c+\delta]$ and $z\in [d-\delta, d)$ such that $f(x)=f(y)=f(z)=A$. Contradiction. Well, why the last part? Why is it that I can apply the intermediate value theorem to these values? For example, $<f(c-\delta)<p<f(c)$, then by the theorem I know that there exists $m\in [c-\delta, c)$ such that $f(m) = p$. Same for the other intervals. But what guarantees thhat the greatest of the values between  $x\in [c-\delta, c), y\in (c, c+\delta]$ will be inside the intervals $[c-\delta), c), (c,c+\delta), [d-\delta, d)$?","I need to prove the following: There's no continuous function $f:[a,b]\to \mathbb{R}$ that takes each of its values $f(x)$, $x\in [a,b]$  exactly twice. First of all, I didn't understand the question. For example $x^2$ takes $1$ twice, in the interval $[-1,1]$. Is it saying that it does not occur for all $x$ in the interval? But what about $f(x) = c$? Is it saying that it does not occur only exactly $2$ times, then? I have no idea about how to prove it. I know that for $f(x)$ such that $f(a)<f(x)<f(b)$, if $f$ is continuous then there is a $c\in [a,b]$ such that $f(c) = f(x)$. Now, there's the following proof in my book and I really wanted to understand it, instead of just getting a new proof Since the interval $[a,b]$ has only $2$ extreme points, then the maximum or minimum of $f$ must be in a point $c\in int([a,b])$ and and in another point $d\in [a,b]$. Then, there exists $\delta>0$ such that in the intervals $[c-\delta, c), (c,c+\delta)$ (and if $d$ is not extreme of $[a,b]$, $[d-\delta, d]$) the function takes values that are less than $f(c) = f(d)$. Let $A$ be the greatest of the numbers $f(c-\delta), f(c+\delta), f(d-\delta)$. By the intermediate value theorem, there are $x\in [c-\delta, c), y\in (c, c+\delta]$ and $z\in [d-\delta, d)$ such that $f(x)=f(y)=f(z)=A$. Contradiction. Well, why the last part? Why is it that I can apply the intermediate value theorem to these values? For example, $<f(c-\delta)<p<f(c)$, then by the theorem I know that there exists $m\in [c-\delta, c)$ such that $f(m) = p$. Same for the other intervals. But what guarantees thhat the greatest of the values between  $x\in [c-\delta, c), y\in (c, c+\delta]$ will be inside the intervals $[c-\delta), c), (c,c+\delta), [d-\delta, d)$?",,"['calculus', 'real-analysis', 'functions']"
74,A finite set is closed,A finite set is closed,,"Question: Prove that a finite subset in a metric space is closed. My proof-sketch: Let $A$ be finite set. Then $A=\{x_1, x_2,\dots, x_n\}.$ We know that $A$ has no limits points. What's next? Definition: Set $E$ is called closed set if $E$ contains all his limits points. Context: Principles of Mathematical Analysis, Rudin","Question: Prove that a finite subset in a metric space is closed. My proof-sketch: Let be finite set. Then We know that has no limits points. What's next? Definition: Set is called closed set if contains all his limits points. Context: Principles of Mathematical Analysis, Rudin","A A=\{x_1, x_2,\dots, x_n\}. A E E","['real-analysis', 'metric-spaces']"
75,$C^1$ function on compact set is Lipschitz,function on compact set is Lipschitz,C^1,"Let $\emptyset\ne A\subset\mathbb{R}^n$ be open and let $f \in C^1 (A, \mathbb{R})$ be a function. Let $\emptyset\ne K\subset A$ be compact and convex. I want to prove that $f$ is  Lipschitz on $K$; that is, prove there exists a  constant $c > 0$ such that  $| f ( x  ) - f( y ) | \leq  c  \, \|  x - y  \|,   \forall  x , y \in K$. My approach: Let $x,y\in K$ be two arbitrary points. Then, since $K$ is convex, the line segment between $x$ and $y$, i.e. $Co(x,y)$, is in $K$. Thus, by MVT, there exists a vector $b\in Co(x,y)$ such that $$\text{(*) } |f(x)-f(y)|=|\langle x-y, (\nabla f)(b)\rangle|\le \|x-y\|\|(\nabla f)(b)\|$$ Now, since $K$ is compact, $f$ takes a maximum and a minimum values on $K$, so that $\exists b'\in K$ such that $\|(\nabla f)(b') \|\ge \|(\nabla f)(b) \|$, for all $b\in K$. Let $c\in \mathbb{R}$, $c:=(\nabla f)(b')$, then $$|f(x)-f(y)|\le\|x-y\|\|(\nabla f)(b)\|\le\|x-y\|\|(\nabla f)(b')\|=c\|x-y\|$$ This implies that $f$ is Lipschitz on $K$ for all $x,y\in K$. Please let me know if you think my proof is correct or not very much? I'm somewhat concerned about the part with the gradient - how exactly is the maximality of the norm of the gradient related to the EVT, that is to $f$ taking maximum and minimum values? As far as I can tell, the maximum norm of the gradient exists because that is the direction to the maximum (or minimum) point of $f$.","Let $\emptyset\ne A\subset\mathbb{R}^n$ be open and let $f \in C^1 (A, \mathbb{R})$ be a function. Let $\emptyset\ne K\subset A$ be compact and convex. I want to prove that $f$ is  Lipschitz on $K$; that is, prove there exists a  constant $c > 0$ such that  $| f ( x  ) - f( y ) | \leq  c  \, \|  x - y  \|,   \forall  x , y \in K$. My approach: Let $x,y\in K$ be two arbitrary points. Then, since $K$ is convex, the line segment between $x$ and $y$, i.e. $Co(x,y)$, is in $K$. Thus, by MVT, there exists a vector $b\in Co(x,y)$ such that $$\text{(*) } |f(x)-f(y)|=|\langle x-y, (\nabla f)(b)\rangle|\le \|x-y\|\|(\nabla f)(b)\|$$ Now, since $K$ is compact, $f$ takes a maximum and a minimum values on $K$, so that $\exists b'\in K$ such that $\|(\nabla f)(b') \|\ge \|(\nabla f)(b) \|$, for all $b\in K$. Let $c\in \mathbb{R}$, $c:=(\nabla f)(b')$, then $$|f(x)-f(y)|\le\|x-y\|\|(\nabla f)(b)\|\le\|x-y\|\|(\nabla f)(b')\|=c\|x-y\|$$ This implies that $f$ is Lipschitz on $K$ for all $x,y\in K$. Please let me know if you think my proof is correct or not very much? I'm somewhat concerned about the part with the gradient - how exactly is the maximality of the norm of the gradient related to the EVT, that is to $f$ taking maximum and minimum values? As far as I can tell, the maximum norm of the gradient exists because that is the direction to the maximum (or minimum) point of $f$.",,"['real-analysis', 'proof-verification', 'vector-analysis', 'lipschitz-functions', 'extreme-value-theorem']"
76,If $f$ is of bounded variation is $f$ Riemann integrable?,If  is of bounded variation is  Riemann integrable?,f f,"I want to know if $f$ is of bounded variation on $[a,b]$ does it follow that $f$ is Riemann integrable on $[a,b]$ ?",I want to know if is of bounded variation on does it follow that is Riemann integrable on ?,"f [a,b] f [a,b]",['real-analysis']
77,Value of $\sum_{n=1}^{\infty} \frac{\cos (n)}{n}$,Value of,\sum_{n=1}^{\infty} \frac{\cos (n)}{n},"I was trying to calculate the value of the series $\displaystyle \sum_{n=1}^{\infty} \dfrac{\cos (n)}{n}$ and I got an answer which I think could be right, but I'm not sure about some of the steps I took to get there. I was wondering if someone could provide some more insight so I can clear my doubts, and also check if I actually got the correct value. First of all, I used Dirichlet's test for the convergence of the series, since $a_n = \dfrac{1}{n}$ is monotonic and $\displaystyle \lim_{n \to \infty} a_n = 0$ , and the cosine partial sums can be bounded by a constant not dependent on $n$ (I'm pretty sure this is right since I looked other ways to do it, so I won't list exactly what I did to get the bound). With that out of the way, I tried taking the expression $\dfrac{\cos(n)}{n}$ and rewriting it as something I could attempt to sum, and got this: $$\displaystyle \int_1^{\pi} \sin(nx) \, dx = \left. -\dfrac{\cos(nx)}{n} \right|_1^{\pi} = \dfrac{(-1)^{n+1}}{n} + \dfrac{\cos(n)}{n}$$ So $$\displaystyle \int_1^{\pi} \sin(nx) \, dx + \dfrac{(-1)^{n}}{n} = \dfrac{\cos(n)}{n}$$ And then $$\displaystyle \lim_{n \to \infty} \displaystyle \sum_{k=1}^{n}\left(\displaystyle \int_1^{\pi} \sin(kx) \, dx + \dfrac{(-1)^{k}}{k}\right) = \displaystyle \lim_{n \to \infty} \displaystyle \sum_{k=1}^{n} \dfrac{\cos(k)}{k}$$ Then I tried separating the left side member into two sums, since $$\displaystyle \sum_{n=1}^{\infty} \dfrac{(-1)^n}{n} = \displaystyle -\sum_{n=1}^{\infty} \dfrac{(-1)^{n+1}}{n} = -\ln (2)$$ I believe the latter equality can be derived using the alternate series test for the convergence of the series, and the Taylor expansion around $x = 0$ of $\ln {(1+x)}$ along with Abel's theorem . As for the other sum, this is the step I'm not sure about. I did $$\displaystyle \lim_{n \to \infty} \displaystyle \sum_{k=1}^{n}\left(\displaystyle \int_1^{\pi} \sin(kx) \, dx\right) = \displaystyle \lim_{n \to \infty} \displaystyle \int_1^{\pi} \left(\displaystyle \sum_{k=1}^{n} \sin(kx)\right) \, dx$$ I'm not sure that's valid, and if it is I'm not sure why: I thought it would be fine since the partial sums could be arranged that way before taking the limit, but I suspect this thinking isn't correct, and I can't just swap the sum and the integral anytime without affecting the result. But anyways, if we take it as valid, then we can get a value for the sum by doing $$\cos {(nx+\dfrac{x}{2})} - \cos {(nx-\dfrac{x}{2})} = -2\sin {(nx)}\sin{\left(\dfrac{x}{2}\right)}$$ So $$\sin{(nx)} = \dfrac{\cos {(nx-\frac{x}{2})} + \cos {(nx+\frac{x}{2})}}{2\sin{\left(\frac{x}{2}\right)}}$$ And then $$\displaystyle \sum_{k=1}^{n} \sin{(kx)} = \displaystyle \sum_{k=1}^{n} \dfrac{\cos {(kx-\frac{x}{2})} + \cos {(kx+\frac{x}{2})}}{2\sin{\left(\frac{x}{2}\right)}}$$ Which telescopes to $$\displaystyle \sum_{k=1}^{n} \sin{(kx)} = \dfrac{\cos {\left(\frac{x}{2}\right)}-\cos {\left(\frac{2n+1}{2} \cdot x\right)}}{2\sin{\left(\frac{x}{2}\right)}}$$ Returning to the integral, we need to evaluate $$\displaystyle \lim_{n \to \infty} \displaystyle \int_1^{\pi} \left(\displaystyle \sum_{k=1}^{n} \sin(kx)\right) \, dx = \displaystyle \lim_{n \to \infty} \displaystyle \int_1^{\pi} \frac{\cos {\left(\frac{x}{2}\right)}-\cos {\left(\frac{2n+1}{2} \cdot x\right)}}{2\sin{\left(\frac{x}{2}\right)}} \, dx$$ I again tried separating it in the sum of the integrals. The first one $$\displaystyle \int_1^{\pi} \frac{\cos {\left(\frac{x}{2}\right)}}{2\sin{\left(\frac{x}{2}\right)}} \, dx = \displaystyle \int_{\sin {\frac{1}{2}}}^1 \dfrac{1}{u} \, du = -\ln({\sin{\frac {1}{2}}})$$ Via substitution $u = \sin{\frac{x}{2}}$ This won't change when $n$ goes to infinity. As for the second one $$-\dfrac{1}{2} \displaystyle \int_1^{\pi} \dfrac{\cos{\left(nx+\frac{x}{2}\right)}}{\sin{\left(\frac{x}{2}\right)}} \, dx = -\dfrac{1}{2}\left(\displaystyle \int_1^{\pi} \dfrac{\cos{(nx)}\cos{\left(\frac{x}{2}\right)}}{\sin{\left(\frac{x}{2}\right)}} \, dx - \displaystyle \int_1^{\pi} \sin(nx) \, dx \right) = $$ $$= -\dfrac{1}{2}\left(\displaystyle \int_1^{\pi} \dfrac{\cos{(nx)}\cos{\left(\frac{x}{2}\right)}}{\sin{\left(\frac{x}{2}\right)}} \, dx + \displaystyle \left. \frac{\cos(nx)}{n} \right|_1^{\pi} \right)$$ Both of these integrals go to 0 as $n$ goes to infinity, applying the Riemann-Lebesgue lemma for the first one, since the function $f(x) = \cot{\left(\frac{x}{2}\right)}$ is continuous on $[1,\pi]$ . Putting it all together gives $$\displaystyle \displaystyle \sum_{n=1}^{\infty} \dfrac{\cos(n)}{n} = -\ln2-\ln{\left(\sin{\frac{1}{2}}\right)} = \boxed{-\ln{\left(2 \cdot \sin{\frac{1}{2}}\right)}} \approx 0.0420195$$ I used Octave to try and check the result: setting $n = 10^6$ gave me $$S_{10^6} \approx 0.042020$$ Because of this, I'm inclined to think I got the correct answer, but I still doubt some of the steps I took (mainly the interchanging sum and integral one). Thanks in advance. I'm sorry if I didn't make myself clear, english isn't my first tongue. I did some search as to find something related to this value, but couldn't find anything. Very sorry if its been answered before.","I was trying to calculate the value of the series and I got an answer which I think could be right, but I'm not sure about some of the steps I took to get there. I was wondering if someone could provide some more insight so I can clear my doubts, and also check if I actually got the correct value. First of all, I used Dirichlet's test for the convergence of the series, since is monotonic and , and the cosine partial sums can be bounded by a constant not dependent on (I'm pretty sure this is right since I looked other ways to do it, so I won't list exactly what I did to get the bound). With that out of the way, I tried taking the expression and rewriting it as something I could attempt to sum, and got this: So And then Then I tried separating the left side member into two sums, since I believe the latter equality can be derived using the alternate series test for the convergence of the series, and the Taylor expansion around of along with Abel's theorem . As for the other sum, this is the step I'm not sure about. I did I'm not sure that's valid, and if it is I'm not sure why: I thought it would be fine since the partial sums could be arranged that way before taking the limit, but I suspect this thinking isn't correct, and I can't just swap the sum and the integral anytime without affecting the result. But anyways, if we take it as valid, then we can get a value for the sum by doing So And then Which telescopes to Returning to the integral, we need to evaluate I again tried separating it in the sum of the integrals. The first one Via substitution This won't change when goes to infinity. As for the second one Both of these integrals go to 0 as goes to infinity, applying the Riemann-Lebesgue lemma for the first one, since the function is continuous on . Putting it all together gives I used Octave to try and check the result: setting gave me Because of this, I'm inclined to think I got the correct answer, but I still doubt some of the steps I took (mainly the interchanging sum and integral one). Thanks in advance. I'm sorry if I didn't make myself clear, english isn't my first tongue. I did some search as to find something related to this value, but couldn't find anything. Very sorry if its been answered before.","\displaystyle \sum_{n=1}^{\infty} \dfrac{\cos (n)}{n} a_n = \dfrac{1}{n} \displaystyle \lim_{n \to \infty} a_n = 0 n \dfrac{\cos(n)}{n} \displaystyle \int_1^{\pi} \sin(nx) \, dx = \left. -\dfrac{\cos(nx)}{n} \right|_1^{\pi} = \dfrac{(-1)^{n+1}}{n} + \dfrac{\cos(n)}{n} \displaystyle \int_1^{\pi} \sin(nx) \, dx + \dfrac{(-1)^{n}}{n} = \dfrac{\cos(n)}{n} \displaystyle \lim_{n \to \infty} \displaystyle \sum_{k=1}^{n}\left(\displaystyle \int_1^{\pi} \sin(kx) \, dx + \dfrac{(-1)^{k}}{k}\right) = \displaystyle \lim_{n \to \infty} \displaystyle \sum_{k=1}^{n} \dfrac{\cos(k)}{k} \displaystyle \sum_{n=1}^{\infty} \dfrac{(-1)^n}{n} = \displaystyle -\sum_{n=1}^{\infty} \dfrac{(-1)^{n+1}}{n} = -\ln (2) x = 0 \ln {(1+x)} \displaystyle \lim_{n \to \infty} \displaystyle \sum_{k=1}^{n}\left(\displaystyle \int_1^{\pi} \sin(kx) \, dx\right) = \displaystyle \lim_{n \to \infty} \displaystyle \int_1^{\pi} \left(\displaystyle \sum_{k=1}^{n} \sin(kx)\right) \, dx \cos {(nx+\dfrac{x}{2})} - \cos {(nx-\dfrac{x}{2})} = -2\sin {(nx)}\sin{\left(\dfrac{x}{2}\right)} \sin{(nx)} = \dfrac{\cos {(nx-\frac{x}{2})} + \cos {(nx+\frac{x}{2})}}{2\sin{\left(\frac{x}{2}\right)}} \displaystyle \sum_{k=1}^{n} \sin{(kx)} = \displaystyle \sum_{k=1}^{n} \dfrac{\cos {(kx-\frac{x}{2})} + \cos {(kx+\frac{x}{2})}}{2\sin{\left(\frac{x}{2}\right)}} \displaystyle \sum_{k=1}^{n} \sin{(kx)} = \dfrac{\cos {\left(\frac{x}{2}\right)}-\cos {\left(\frac{2n+1}{2} \cdot x\right)}}{2\sin{\left(\frac{x}{2}\right)}} \displaystyle \lim_{n \to \infty} \displaystyle \int_1^{\pi} \left(\displaystyle \sum_{k=1}^{n} \sin(kx)\right) \, dx = \displaystyle \lim_{n \to \infty} \displaystyle \int_1^{\pi} \frac{\cos {\left(\frac{x}{2}\right)}-\cos {\left(\frac{2n+1}{2} \cdot x\right)}}{2\sin{\left(\frac{x}{2}\right)}} \, dx \displaystyle \int_1^{\pi} \frac{\cos {\left(\frac{x}{2}\right)}}{2\sin{\left(\frac{x}{2}\right)}} \, dx = \displaystyle \int_{\sin {\frac{1}{2}}}^1 \dfrac{1}{u} \, du = -\ln({\sin{\frac {1}{2}}}) u = \sin{\frac{x}{2}} n -\dfrac{1}{2} \displaystyle \int_1^{\pi} \dfrac{\cos{\left(nx+\frac{x}{2}\right)}}{\sin{\left(\frac{x}{2}\right)}} \, dx = -\dfrac{1}{2}\left(\displaystyle \int_1^{\pi} \dfrac{\cos{(nx)}\cos{\left(\frac{x}{2}\right)}}{\sin{\left(\frac{x}{2}\right)}} \, dx - \displaystyle \int_1^{\pi} \sin(nx) \, dx \right) =  = -\dfrac{1}{2}\left(\displaystyle \int_1^{\pi} \dfrac{\cos{(nx)}\cos{\left(\frac{x}{2}\right)}}{\sin{\left(\frac{x}{2}\right)}} \, dx + \displaystyle \left. \frac{\cos(nx)}{n} \right|_1^{\pi} \right) n f(x) = \cot{\left(\frac{x}{2}\right)} [1,\pi] \displaystyle \displaystyle \sum_{n=1}^{\infty} \dfrac{\cos(n)}{n} = -\ln2-\ln{\left(\sin{\frac{1}{2}}\right)} = \boxed{-\ln{\left(2 \cdot \sin{\frac{1}{2}}\right)}} \approx 0.0420195 n = 10^6 S_{10^6} \approx 0.042020","['real-analysis', 'sequences-and-series']"
78,A uniformly continuous function maps bounded set to bounded sets,A uniformly continuous function maps bounded set to bounded sets,,"I am trying to prove the following: If $A\subset\mathbb R$ is bounded and $\,f:A\to \mathbb R\,$ is uniformly continuous, then $f[A]$ is bounded. Could you check my proof? Let $A \subseteq [-K,K]\subseteq \mathbb R$ . Let $\varepsilon = 1$ . If $f$ is uniformly continuous there is $\delta$ wuth $|x-y| <\delta$ imply that $|f(x)-f(y)|<1$ for all $x,y\in A$ . Let $a \in A$ . Then because $A$ is bounded there is a finite number of balls $B(a_n,\delta)$ that cover $A$ . Let the number be $N$ . Then $f(a)-N \le f(x) \le f(a) + N$ for all $x\in A$ .","I am trying to prove the following: If is bounded and is uniformly continuous, then is bounded. Could you check my proof? Let . Let . If is uniformly continuous there is wuth imply that for all . Let . Then because is bounded there is a finite number of balls that cover . Let the number be . Then for all .","A\subset\mathbb R \,f:A\to \mathbb R\, f[A] A \subseteq [-K,K]\subseteq \mathbb R \varepsilon = 1 f \delta |x-y| <\delta |f(x)-f(y)|<1 x,y\in A a \in A A B(a_n,\delta) A N f(a)-N \le f(x) \le f(a) + N x\in A","['real-analysis', 'proof-verification', 'continuity', 'uniform-continuity']"
79,Relationship of Fourier series and Hilbert spaces?,Relationship of Fourier series and Hilbert spaces?,,"I just read in a textbook that a Hilbert space can be defined or represented by an appropriate Fourier series. How might that be? Is it because a Fourier series is an infinite series that adequately ""covers"" a Hilbert space? Apart from this I (a mathematical novice) have a hard time seeing the connection between a Hilbert space, a vector construct, and a Fourier series (of trigonometric functions).","I just read in a textbook that a Hilbert space can be defined or represented by an appropriate Fourier series. How might that be? Is it because a Fourier series is an infinite series that adequately ""covers"" a Hilbert space? Apart from this I (a mathematical novice) have a hard time seeing the connection between a Hilbert space, a vector construct, and a Fourier series (of trigonometric functions).",,"['real-analysis', 'sequences-and-series', 'hilbert-spaces', 'fourier-series']"
80,Limit as $n\to+\infty$ of $\prod_{k=1}^{n} \frac{2k}{2k+1}$,Limit as  of,n\to+\infty \prod_{k=1}^{n} \frac{2k}{2k+1},"I'm trying to evaluate $$\lim_{n\to+\infty} \prod_{k=1}^{n} \frac{2k}{2k+1}$$ First I notice that since $k\geq1$ it is $\frac{2k}{2k+1}>0$ for all $k\in\{1,...,n\}$ ; so $$0\leq\lim_{n\to+\infty} \prod_{k=1}^{n} \frac{2k}{2k+1}$$ Then I notice that $$\prod_{k=1}^{n} \frac{2k}{2k+1}=\exp{\ln\left(\prod_{k=1}^{n} \frac{2k}{2k+1}\right)}=\exp{\sum_{k=1}^{n}\ln\left(\frac{2k}{2k+1}\right)}=$$ $$=\exp{\sum_{k=1}^{n}\ln\left(1-\frac{1}{2k+1}\right)}$$ Since $\ln(1+x)\leq x$ for all $x>-1$ and since $\exp$ is an increasing function it follows that $$\exp{\sum_{k=1}^{n}\ln\left(1-\frac{1}{2k+1}\right)}\leq\exp{\sum_{k=1}^{n}-\frac{1}{2k+1}}$$ So $$\lim_{n\to+\infty}\prod_{k=1}^{n} \frac{2k}{2k+1}\leq\lim_{n\to+\infty}\exp{\sum_{k=1}^{n}-\frac{1}{2k+1}}$$ Since $\exp$ is a continuous function it follows that $$\lim_{n\to+\infty}\exp{\sum_{k=1}^{n}-\frac{1}{2k+1}}=\exp{\sum_{k=1}^{+\infty}-\frac{1}{2k+1}}=e^{-\infty}=0$$ So by the comparison test we deduce that the limit is $0$ . Is this correct? Thanks for your time.",I'm trying to evaluate First I notice that since it is for all ; so Then I notice that Since for all and since is an increasing function it follows that So Since is a continuous function it follows that So by the comparison test we deduce that the limit is . Is this correct? Thanks for your time.,"\lim_{n\to+\infty} \prod_{k=1}^{n} \frac{2k}{2k+1} k\geq1 \frac{2k}{2k+1}>0 k\in\{1,...,n\} 0\leq\lim_{n\to+\infty} \prod_{k=1}^{n} \frac{2k}{2k+1} \prod_{k=1}^{n} \frac{2k}{2k+1}=\exp{\ln\left(\prod_{k=1}^{n} \frac{2k}{2k+1}\right)}=\exp{\sum_{k=1}^{n}\ln\left(\frac{2k}{2k+1}\right)}= =\exp{\sum_{k=1}^{n}\ln\left(1-\frac{1}{2k+1}\right)} \ln(1+x)\leq x x>-1 \exp \exp{\sum_{k=1}^{n}\ln\left(1-\frac{1}{2k+1}\right)}\leq\exp{\sum_{k=1}^{n}-\frac{1}{2k+1}} \lim_{n\to+\infty}\prod_{k=1}^{n} \frac{2k}{2k+1}\leq\lim_{n\to+\infty}\exp{\sum_{k=1}^{n}-\frac{1}{2k+1}} \exp \lim_{n\to+\infty}\exp{\sum_{k=1}^{n}-\frac{1}{2k+1}}=\exp{\sum_{k=1}^{+\infty}-\frac{1}{2k+1}}=e^{-\infty}=0 0","['real-analysis', 'sequences-and-series', 'limits', 'infinite-product']"
81,Image of set of measure zero has measure zero if the function is absolutely continuous,Image of set of measure zero has measure zero if the function is absolutely continuous,,"I have been trying to solve this problem from Bass. Let $f$ be a real valued absolutely continuous function defined on $[0,1].$ Denote $f(A)=\{f(x): x \in A\}$ for $A \subset [0,1].$ Prove that if $A$ has Lebesgue-measure zero, then so does $f(A)$ . My attempt: By outer regularity, given an $\varepsilon>0$ , I have a collection of disjoint open intervals $\{(a_i,b_i)\}_{i \geq 1}$ such that $A \subset \bigcup_i (a_i,b_i)$ and $\sum_i (b_i-a_i) < \varepsilon$ . Can we link $f(A)$ and $\bigcup_i f((a_i,b_i))$ ? We know that $f(a,b)$ is an interval but what about it's length ? Is it related to $f(b)-f(a)?$","I have been trying to solve this problem from Bass. Let be a real valued absolutely continuous function defined on Denote for Prove that if has Lebesgue-measure zero, then so does . My attempt: By outer regularity, given an , I have a collection of disjoint open intervals such that and . Can we link and ? We know that is an interval but what about it's length ? Is it related to","f [0,1]. f(A)=\{f(x): x \in A\} A \subset [0,1]. A f(A) \varepsilon>0 \{(a_i,b_i)\}_{i \geq 1} A \subset \bigcup_i (a_i,b_i) \sum_i (b_i-a_i) < \varepsilon f(A) \bigcup_i f((a_i,b_i)) f(a,b) f(b)-f(a)?","['real-analysis', 'absolute-continuity']"
82,Difference of elements from measurable set contains open interval,Difference of elements from measurable set contains open interval,,"Let $A\subset\mathbb{R}$ be a measurable set s.t $,m(A)>0$. Prove that the set $$B=\{x-y\mid x,y\in A\}$$contains nonempty open interval around 0. I thought to take an interval in $A$, $I=(x-\frac{\epsilon}{2},x+\frac{\epsilon}{2})\subset A$ and hence taking $y$ values from I we get an epsilon - neighborhood of $0$ but I'm quite not sure that I can assume the existence of such I. How can I prove the existence of I?","Let $A\subset\mathbb{R}$ be a measurable set s.t $,m(A)>0$. Prove that the set $$B=\{x-y\mid x,y\in A\}$$contains nonempty open interval around 0. I thought to take an interval in $A$, $I=(x-\frac{\epsilon}{2},x+\frac{\epsilon}{2})\subset A$ and hence taking $y$ values from I we get an epsilon - neighborhood of $0$ but I'm quite not sure that I can assume the existence of such I. How can I prove the existence of I?",,['real-analysis']
83,"Does this sequence $\,\sqrt[n]{1+\cos2n}\,$ have a limit?",Does this sequence  have a limit?,"\,\sqrt[n]{1+\cos2n}\,","Does this $\,\lim_{n\to\infty}\sqrt[n]{1+\cos2n}\,$ exist? It took me and my tutor 2 days to find this limit. We believe that this function has no limit or if it does, we are not intellectual enough to find it. Hope someone can help us.","Does this exist? It took me and my tutor 2 days to find this limit. We believe that this function has no limit or if it does, we are not intellectual enough to find it. Hope someone can help us.","\,\lim_{n\to\infty}\sqrt[n]{1+\cos2n}\,","['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'irrational-numbers']"
84,Computing $\lim\limits_{n\to\infty} \frac{\sqrt{n}}{4^{n}}\sum\limits_{k=1}^{n} \binom{2n-1}{n-k}\frac{ 1}{(2k-1)^2+\pi^2}$,Computing,\lim\limits_{n\to\infty} \frac{\sqrt{n}}{4^{n}}\sum\limits_{k=1}^{n} \binom{2n-1}{n-k}\frac{ 1}{(2k-1)^2+\pi^2},What tools would you recommend me for computing the limit below? $$\lim_{n\to\infty} \frac{\sqrt{n}}{4^{n}}\sum_{k=1}^{n}\frac{\displaystyle \binom{2n-1}{n-k}}{(2k-1)^2+\pi^2}$$ As soon as any useful idea comes to mind I'll make the proper update with the new findings.,What tools would you recommend me for computing the limit below? $$\lim_{n\to\infty} \frac{\sqrt{n}}{4^{n}}\sum_{k=1}^{n}\frac{\displaystyle \binom{2n-1}{n-k}}{(2k-1)^2+\pi^2}$$ As soon as any useful idea comes to mind I'll make the proper update with the new findings.,,"['calculus', 'real-analysis', 'integration', 'sequences-and-series', 'limits']"
85,The difference between convergence in $L^{\infty}$ and almost uniformly,The difference between convergence in  and almost uniformly,L^{\infty},"I am reading these notes http://terrytao.wordpress.com/2010/10/02/245a-notes-4-modes-of-convergence/ by Terry Tao. I have a question about the difference between convergence in $L^{\infty}$ and convergence almost uniformly. Is the difference that convergence almost uniformly guarantees that you can get uniform convergence outside a set of arbitrarily small but still positive measure, while convergence $L^{\infty}$ gets uniform convergence outside a set of exactly measure zero? Formal definitions follow to make ideas precise. Let $(X, \mathcal{M}, \mu)$ be a measure space. Let $f, f_1, f_2, \ldots$ be a measurable functions. We say that $f_n \to f$ in $L^{\infty}$ if for all $\varepsilon > 0$ there is an $N_{\varepsilon}$ such that $|f_n(x) - f(x)| \leq \varepsilon$ $\mu$--a.e. when $n \geq N_{\varepsilon}$. We say that $f_n \to f$ almost uniformly if for all $\varepsilon > 0$ there is a set $E \in \mathcal{M}$ with $\mu(E) \leq \varepsilon$ such that $f_n \to f$ uniformly on $E^c$. I.e., for each $\delta > 0$ there is an $N_{\delta}$ such that $|f_n(x) - f(x)| \leq \delta$ for all $x \in E^c$ when $n \geq N_{\delta}$.","I am reading these notes http://terrytao.wordpress.com/2010/10/02/245a-notes-4-modes-of-convergence/ by Terry Tao. I have a question about the difference between convergence in $L^{\infty}$ and convergence almost uniformly. Is the difference that convergence almost uniformly guarantees that you can get uniform convergence outside a set of arbitrarily small but still positive measure, while convergence $L^{\infty}$ gets uniform convergence outside a set of exactly measure zero? Formal definitions follow to make ideas precise. Let $(X, \mathcal{M}, \mu)$ be a measure space. Let $f, f_1, f_2, \ldots$ be a measurable functions. We say that $f_n \to f$ in $L^{\infty}$ if for all $\varepsilon > 0$ there is an $N_{\varepsilon}$ such that $|f_n(x) - f(x)| \leq \varepsilon$ $\mu$--a.e. when $n \geq N_{\varepsilon}$. We say that $f_n \to f$ almost uniformly if for all $\varepsilon > 0$ there is a set $E \in \mathcal{M}$ with $\mu(E) \leq \varepsilon$ such that $f_n \to f$ uniformly on $E^c$. I.e., for each $\delta > 0$ there is an $N_{\delta}$ such that $|f_n(x) - f(x)| \leq \delta$ for all $x \in E^c$ when $n \geq N_{\delta}$.",,"['real-analysis', 'measure-theory']"
86,Natural derivation of the complex exponential function?,Natural derivation of the complex exponential function?,,"Bourbaki shows in  a very natural way that every continuous group isomorphism of the additive reals to the positive multiplicative reals is determined by its value at $1$, and in fact, that every such isomorphism is of the form $f_a(x)=a^x$ for $a>0$ and $a\neq 1$.  We get the standard real exponential (where $a=e$) when we notice that for any $f_a$, $(f_a)'=g(a)f_a$ where $g$ is a continuous group isomorphism from the positive multiplicative reals to the additive reals.  By the intermediate value theorem, there exists some positive real $e$ such that $g(e)=1$ (by our earlier classification of continuous group homomorphisms, we notice that $g$ is in fact the natural log). Notice that every deduction above follows from a natural question.  We never need to guess anything to proceed. Is there any natural way like the above to derive the complex exponential?  The only way I've seen it derived is as follows: Derive the real exponential by some method (inverse function to the natural log, which is the integral of $1/t$ on the interval $[1,x)$, Bourbaki's method, or some other derivation), then show that it is analytic with infinite radius of convergence (where it converges uniformly and absolutely), which means that it is equal to its Taylor series at 0, which means that we can, by a general result of complex analysis, extend it to an entire function on the complex plane. This derivation doesn't seem natural to me in the same sense as Bourbaki's derivation of the real exponential, since it requires that we notice some analytic properties of the function, instead of relying on its unique algebraic and topological properties. Does anyone know of a derivation similar to Bourbaki's for the complex exponential?","Bourbaki shows in  a very natural way that every continuous group isomorphism of the additive reals to the positive multiplicative reals is determined by its value at $1$, and in fact, that every such isomorphism is of the form $f_a(x)=a^x$ for $a>0$ and $a\neq 1$.  We get the standard real exponential (where $a=e$) when we notice that for any $f_a$, $(f_a)'=g(a)f_a$ where $g$ is a continuous group isomorphism from the positive multiplicative reals to the additive reals.  By the intermediate value theorem, there exists some positive real $e$ such that $g(e)=1$ (by our earlier classification of continuous group homomorphisms, we notice that $g$ is in fact the natural log). Notice that every deduction above follows from a natural question.  We never need to guess anything to proceed. Is there any natural way like the above to derive the complex exponential?  The only way I've seen it derived is as follows: Derive the real exponential by some method (inverse function to the natural log, which is the integral of $1/t$ on the interval $[1,x)$, Bourbaki's method, or some other derivation), then show that it is analytic with infinite radius of convergence (where it converges uniformly and absolutely), which means that it is equal to its Taylor series at 0, which means that we can, by a general result of complex analysis, extend it to an entire function on the complex plane. This derivation doesn't seem natural to me in the same sense as Bourbaki's derivation of the real exponential, since it requires that we notice some analytic properties of the function, instead of relying on its unique algebraic and topological properties. Does anyone know of a derivation similar to Bourbaki's for the complex exponential?",,['complex-analysis']
87,Testing the series $\sum\limits_{n=1}^{\infty} \frac{1}{n^{k + \cos{n}}}$,Testing the series,\sum\limits_{n=1}^{\infty} \frac{1}{n^{k + \cos{n}}},"We know that the ""Harmonic Series"" $$ \sum \frac{1}{n}$$ diverges. And for $p >1$ we have the result that the series converges $$\sum \frac{1}{n^{p}}$$ converges. One can then ask the question of testing the convergence the following 2 Series: $$\sum\limits_{n=1}^{\infty} \frac{1}{n^{k + \cos{n}}}, \quad \sum\limits_{n=1}^{\infty} \frac{1}{n^{k + \sin{n}}}$$ where $ k \in (0,2)$. Only thing which i have as tool for this problem is the inequality $| \sin{n} | \leq 1$, which i am not sure whether would applicable or not.","We know that the ""Harmonic Series"" $$ \sum \frac{1}{n}$$ diverges. And for $p >1$ we have the result that the series converges $$\sum \frac{1}{n^{p}}$$ converges. One can then ask the question of testing the convergence the following 2 Series: $$\sum\limits_{n=1}^{\infty} \frac{1}{n^{k + \cos{n}}}, \quad \sum\limits_{n=1}^{\infty} \frac{1}{n^{k + \sin{n}}}$$ where $ k \in (0,2)$. Only thing which i have as tool for this problem is the inequality $| \sin{n} | \leq 1$, which i am not sure whether would applicable or not.",,['real-analysis']
88,"How do concepts such as limits work in probability theory, as opposed to calculus?","How do concepts such as limits work in probability theory, as opposed to calculus?",,"When I am flipping a fair coin and say that as the number of trials approaches $\infty$ the number of heads approaches $50\%$ , what do I really mean? Intuitively, I would associate it with the concept of a limit, as used in calculus: $$ \lim_{t \to \infty} \left(\frac{H}{H+T}\right)=0.5 \\ \text{Where $t$ = the number of trials, $H$ = the number of heads, and $T$ = the number of tails} $$ However, this intuition seems to break down when I use the formal definition of a limit: $$ \lim_{t \to \infty} \left(\frac{H}{H+T}\right)=0.5 \text{ if and only if}\\ \text{for every $\varepsilon>0$, there exists $N>0$ such that for all $t$} \\ \text{if $t>N$ then $|0.5-\frac{H}{H+T}|<\varepsilon$} $$ Well I don't know if there will be $N > 0$ that satisifes this definition! It all depends on what comes up. However many times I flip the coin, $\frac{H}{H+T}$ might just equal $0$ . So how might I define terms such as ""approaches"" in probability theory if the conventional definition does not work? Edit: User nicomezi has pointed out that this a huge topic. Therefore, I will accept even a very short introduction to this subject as an answer.","When I am flipping a fair coin and say that as the number of trials approaches the number of heads approaches , what do I really mean? Intuitively, I would associate it with the concept of a limit, as used in calculus: However, this intuition seems to break down when I use the formal definition of a limit: Well I don't know if there will be that satisifes this definition! It all depends on what comes up. However many times I flip the coin, might just equal . So how might I define terms such as ""approaches"" in probability theory if the conventional definition does not work? Edit: User nicomezi has pointed out that this a huge topic. Therefore, I will accept even a very short introduction to this subject as an answer.","\infty 50\% 
\lim_{t \to \infty} \left(\frac{H}{H+T}\right)=0.5 \\
\text{Where t = the number of trials, H = the number of heads, and T = the number of tails}
 
\lim_{t \to \infty} \left(\frac{H}{H+T}\right)=0.5 \text{ if and only if}\\
\text{for every \varepsilon>0, there exists N>0 such that for all t} \\
\text{if t>N then |0.5-\frac{H}{H+T}|<\varepsilon}
 N > 0 \frac{H}{H+T} 0","['real-analysis', 'calculus', 'limits', 'probability-theory']"
89,limit of : $a_{n+2} =\frac{1}{a_n} + \frac{1}{a_{n+1}}$,limit of :,a_{n+2} =\frac{1}{a_n} + \frac{1}{a_{n+1}},"$a_n$ is a real sequence, $a_1,a_2$ are positive and for all $n>2$ : $$ a_{n+2} =\frac{1}{a_{n+1}} + \frac{1}{a_{n}}.$$ Prove that: $\displaystyle \lim_{n\to \infty} a_n$ exists, then find it. Mathematica gave me $\sqrt{2}$ as an approximate limit, I tried to eliminate $a_{n+2}$ then do some work with Stolz lemma but I failed, Are there any strategy to find the asymptotic expansion for this kind of sequences (I can do it only if it was a first order) ? Any help is appreciated (if you find it too easy, just post hints).","$a_n$ is a real sequence, $a_1,a_2$ are positive and for all $n>2$ : $$ a_{n+2} =\frac{1}{a_{n+1}} + \frac{1}{a_{n}}.$$ Prove that: $\displaystyle \lim_{n\to \infty} a_n$ exists, then find it. Mathematica gave me $\sqrt{2}$ as an approximate limit, I tried to eliminate $a_{n+2}$ then do some work with Stolz lemma but I failed, Are there any strategy to find the asymptotic expansion for this kind of sequences (I can do it only if it was a first order) ? Any help is appreciated (if you find it too easy, just post hints).",,"['real-analysis', 'sequences-and-series', 'limits', 'recurrence-relations']"
90,Showing inequality for harmonic series.,Showing inequality for harmonic series.,,I want to show that $$\log N<\sum_{n=1}^{N}\frac{1}{n}<1+\log N.$$ But I don't know how to show this.,I want to show that $$\log N<\sum_{n=1}^{N}\frac{1}{n}<1+\log N.$$ But I don't know how to show this.,,"['real-analysis', 'sequences-and-series', 'inequality', 'harmonic-numbers']"
91,Advantage of Lebesgue sigma-algebra over Borel?,Advantage of Lebesgue sigma-algebra over Borel?,,"What it says on the tin. Using the Borel $\sigma$-algebra on the reals instead of the Lebesgue $\sigma$-algebra has the advantage that it allows a broader class of measures, many of which are quite natural: For example the ""uniform"" measure on the Cantor set is defined on the Borel $\sigma$-algebra, but cannot be defined on the Lebesgue algebra. So why don't we just use the Borel $\sigma$-algebra for everything? What advantage does the Lebesgue $\sigma$-algebra have? I mean, it has more measurable sets, but sets that are Lebesgue-measurable but not Borel-measurable (or for that matter, sets that are not Borel-measurable, period) are extremely pathological, not explicitly constructible, and (as far as I can tell) never show up naturally. And it's complete, but I have no idea what makes that a useful property.","What it says on the tin. Using the Borel $\sigma$-algebra on the reals instead of the Lebesgue $\sigma$-algebra has the advantage that it allows a broader class of measures, many of which are quite natural: For example the ""uniform"" measure on the Cantor set is defined on the Borel $\sigma$-algebra, but cannot be defined on the Lebesgue algebra. So why don't we just use the Borel $\sigma$-algebra for everything? What advantage does the Lebesgue $\sigma$-algebra have? I mean, it has more measurable sets, but sets that are Lebesgue-measurable but not Borel-measurable (or for that matter, sets that are not Borel-measurable, period) are extremely pathological, not explicitly constructible, and (as far as I can tell) never show up naturally. And it's complete, but I have no idea what makes that a useful property.",,['real-analysis']
92,"""Fat"" Cantor Set","""Fat"" Cantor Set",,"So the standard Cantor set has an outer measure equal to $0$ , but how can you construct a ""fat"" Cantor set with a positive outer measure? I was told that it is even possible to produce one with an outer measure of $1$ . I don't see how changing the size of the ""chuck"" taken out will change the value of the outer measure. Regardless of the size, I feel like it will inevitably reach a value of $0$ as well... Are there other constraints that need to be made in order to accomplish this?","So the standard Cantor set has an outer measure equal to , but how can you construct a ""fat"" Cantor set with a positive outer measure? I was told that it is even possible to produce one with an outer measure of . I don't see how changing the size of the ""chuck"" taken out will change the value of the outer measure. Regardless of the size, I feel like it will inevitably reach a value of as well... Are there other constraints that need to be made in order to accomplish this?",0 1 0,"['real-analysis', 'cantor-set']"
93,"Relation between Sobolev Space $W^{1,\infty}$ and the Lipschitz class",Relation between Sobolev Space  and the Lipschitz class,"W^{1,\infty}","I have a Sobolev space related question. In the book 'Measure theory and fine properties of functions' by Lawrence Evans and Ronald Gariepy. I know the result that states that for $f: \Omega \rightarrow \mathbb{R}$ . $f$ is locally Lipschitz in $\Omega$ if and only if $f \in W^{1,\infty}_{loc}(\Omega)$ . I recently saw that it was stated in a book 'Nonlinear partial differential equations with applications' by Roubicek, that $W^{1,\infty}(\Omega) = C^{0,1}(\Omega)$ . Can anyone confirm this stronger result and maybe recommend a book or text which provides a proof? Thanks a lot for any help.","I have a Sobolev space related question. In the book 'Measure theory and fine properties of functions' by Lawrence Evans and Ronald Gariepy. I know the result that states that for . is locally Lipschitz in if and only if . I recently saw that it was stated in a book 'Nonlinear partial differential equations with applications' by Roubicek, that . Can anyone confirm this stronger result and maybe recommend a book or text which provides a proof? Thanks a lot for any help.","f: \Omega \rightarrow \mathbb{R} f \Omega f \in W^{1,\infty}_{loc}(\Omega) W^{1,\infty}(\Omega) = C^{0,1}(\Omega)","['real-analysis', 'functional-analysis']"
94,Continuity and Joint Continuity,Continuity and Joint Continuity,,"Consider a function $f(x,y):[0,1] \times [0,1] \rightarrow R.$ What is the difference between $f$ continuous in each argument and jointly continuous?","Consider a function $f(x,y):[0,1] \times [0,1] \rightarrow R.$ What is the difference between $f$ continuous in each argument and jointly continuous?",,['real-analysis']
95,Evaluating the Poisson Kernel in the upper half space in $n$-dimensions,Evaluating the Poisson Kernel in the upper half space in -dimensions,n,"Let  $$K(x,y) = \frac{2x_n}{n \alpha(n)} \frac{1}{|x-y|^n}$$ be the Poisson Kernel, where $x \in \mathbb{R}_+^n$ (the upper half-space of in $\mathbb{R}^n$), $y \in \mathbb{R}^n$, and $\alpha(n)$ is the volume of the $n$-dimensional unit ball. How do you show that $$\int_{\partial \mathbb{R}_+^n} K(x,y) dy =1?$$ I tried doing this in simple cases (e.g. two dimensions), and it can out pretty cleanly (I think you can also probably use complex analysis if we're in two-dimensions?). However, I couldn't figure how to solve it in general $n$ dimensions, because the exponent to the $n$-th power was giving me trouble. How would one go about showing the general case?","Let  $$K(x,y) = \frac{2x_n}{n \alpha(n)} \frac{1}{|x-y|^n}$$ be the Poisson Kernel, where $x \in \mathbb{R}_+^n$ (the upper half-space of in $\mathbb{R}^n$), $y \in \mathbb{R}^n$, and $\alpha(n)$ is the volume of the $n$-dimensional unit ball. How do you show that $$\int_{\partial \mathbb{R}_+^n} K(x,y) dy =1?$$ I tried doing this in simple cases (e.g. two dimensions), and it can out pretty cleanly (I think you can also probably use complex analysis if we're in two-dimensions?). However, I couldn't figure how to solve it in general $n$ dimensions, because the exponent to the $n$-th power was giving me trouble. How would one go about showing the general case?",,"['calculus', 'real-analysis']"
96,Does the graph of a measurable function always have zero measure?,Does the graph of a measurable function always have zero measure?,,"Question: Let $(X,\mathscr{M},\mu)$ be a measure space and $f\colon X \to [0,+\infty[$ be a measurable function. If ${\rm gr}(f) \doteq \{ (x,f(x)) \mid x \in X \}$, then does it always satisfy $$(\mu \times \newcommand{\m}{\mathfrak{m}} \m)({\rm gr}(f)) = 0$$even if the domain space is not $\sigma$-finite? Here $\m$ denotes Lebesgue measure in $[0,+\infty[$. Context: define the ""shadows"" $$\begin{align*} G_{f,<} &\doteq \{ (x,y) \in X \times \left[0,+\infty\right[ \mid y < f(x)  \} \\ G_{f,\leq} &\doteq \{ (x,y) \in X \times \left[0,+\infty\right[ \mid y \leq f(x)\}.\end{align*}$$One can prove that $$(1) \qquad (\mu \times \m)(G_{f,<}) = \int_X f(x)\,{\rm d}\mu(x)$$ and that, if $\mu$ is $\sigma$-finite , that $$(2) \qquad(\mu \times \m)(G_{f,\leq}) = \int_X f(x)\,{\rm d}\mu(x).$$This implies that if $\mu$ is $\sigma$-finite, then $(\mu \times \m)({\rm gr}(f)) = 0$. But is this hypothesis really needed? I have the following proofs, in case this helps anyone think: Proof of (1) without $\sigma$-finiteness: If $A \in \mathscr{M}$ and $f = \chi_A$, then $$(\mu \times \m)(G_{f,<}) \stackrel{(\ast)}{=} (\mu \times \m)(A \times [0,1[) = \mu(A)\m([0,1[) = \mu(A) = \int_X \chi_A(x)\,{\rm d}\mu(x),$$where $(\ast)$ holds because points have zero Lebesgue measure. The above clearly implies that the formula is valid for simple positive functions. If $f$ is measurable and positive, take a sequence $(\varphi_n)_n$ of simple and positive functions that converge to $f$, increasing. Then $\bigcup_n G_{\varphi_n,<} = G_{f,<}$, so upper continuity of the product measure and the Monotone Convergence Theorem give $$\begin{align*}(\mu \times \m)(G_{f,<})&=(\mu \times \m)\left(\bigcup_n G_{\varphi_n,<}\right) = \lim_n (\mu \times \m)(G_{\varphi_n,f})\\ &= \lim_n \int_X \varphi_n(x)\,{\rm d}\mu(x) = \int_X f(x)\,{\rm d}\mu(x),\end{align*}$$as wanted. This fails for $G_{f,\leq}$, since that set equality need not be true anymore. If we still had equality of measures, it would be fine, but I think that amounts to my initial question. Proofs assuming $\sigma$-finitess: apply Fubini-Tonelli as follows: $$\begin{align*} (\mu \times \m)(G_{f,<}) &= \int_{X \times [0,+\infty[} \chi_{G_{f,<}}(x,y)\,{\rm d}(\mu \times \m)(x,y) \\ &= \int_X \int_{[0,+\infty[} \chi_{[0,f(x)[}(y)\,{\rm d}\m(y)\,{\rm d}\mu(x) = \int_X f(x)\,{\rm d}\mu(x)\end{align*}$$Since points have zero Lebesgue measure, the same argument with $[0,f(x)]$ instead of $[0,f(x)[$ gives the formula for $G_{f,\leq}$. And using $\{f(x)\}$ instead along with $\m(\{f(x)\}) =0$ gives $(\mu \times \m)({\rm gr}(f))=0$. Bonus track: does anyone know any results in this direction if we had another measure space as co-domain? Is it possible to have ""fat"" graphics?","Question: Let $(X,\mathscr{M},\mu)$ be a measure space and $f\colon X \to [0,+\infty[$ be a measurable function. If ${\rm gr}(f) \doteq \{ (x,f(x)) \mid x \in X \}$, then does it always satisfy $$(\mu \times \newcommand{\m}{\mathfrak{m}} \m)({\rm gr}(f)) = 0$$even if the domain space is not $\sigma$-finite? Here $\m$ denotes Lebesgue measure in $[0,+\infty[$. Context: define the ""shadows"" $$\begin{align*} G_{f,<} &\doteq \{ (x,y) \in X \times \left[0,+\infty\right[ \mid y < f(x)  \} \\ G_{f,\leq} &\doteq \{ (x,y) \in X \times \left[0,+\infty\right[ \mid y \leq f(x)\}.\end{align*}$$One can prove that $$(1) \qquad (\mu \times \m)(G_{f,<}) = \int_X f(x)\,{\rm d}\mu(x)$$ and that, if $\mu$ is $\sigma$-finite , that $$(2) \qquad(\mu \times \m)(G_{f,\leq}) = \int_X f(x)\,{\rm d}\mu(x).$$This implies that if $\mu$ is $\sigma$-finite, then $(\mu \times \m)({\rm gr}(f)) = 0$. But is this hypothesis really needed? I have the following proofs, in case this helps anyone think: Proof of (1) without $\sigma$-finiteness: If $A \in \mathscr{M}$ and $f = \chi_A$, then $$(\mu \times \m)(G_{f,<}) \stackrel{(\ast)}{=} (\mu \times \m)(A \times [0,1[) = \mu(A)\m([0,1[) = \mu(A) = \int_X \chi_A(x)\,{\rm d}\mu(x),$$where $(\ast)$ holds because points have zero Lebesgue measure. The above clearly implies that the formula is valid for simple positive functions. If $f$ is measurable and positive, take a sequence $(\varphi_n)_n$ of simple and positive functions that converge to $f$, increasing. Then $\bigcup_n G_{\varphi_n,<} = G_{f,<}$, so upper continuity of the product measure and the Monotone Convergence Theorem give $$\begin{align*}(\mu \times \m)(G_{f,<})&=(\mu \times \m)\left(\bigcup_n G_{\varphi_n,<}\right) = \lim_n (\mu \times \m)(G_{\varphi_n,f})\\ &= \lim_n \int_X \varphi_n(x)\,{\rm d}\mu(x) = \int_X f(x)\,{\rm d}\mu(x),\end{align*}$$as wanted. This fails for $G_{f,\leq}$, since that set equality need not be true anymore. If we still had equality of measures, it would be fine, but I think that amounts to my initial question. Proofs assuming $\sigma$-finitess: apply Fubini-Tonelli as follows: $$\begin{align*} (\mu \times \m)(G_{f,<}) &= \int_{X \times [0,+\infty[} \chi_{G_{f,<}}(x,y)\,{\rm d}(\mu \times \m)(x,y) \\ &= \int_X \int_{[0,+\infty[} \chi_{[0,f(x)[}(y)\,{\rm d}\m(y)\,{\rm d}\mu(x) = \int_X f(x)\,{\rm d}\mu(x)\end{align*}$$Since points have zero Lebesgue measure, the same argument with $[0,f(x)]$ instead of $[0,f(x)[$ gives the formula for $G_{f,\leq}$. And using $\{f(x)\}$ instead along with $\m(\{f(x)\}) =0$ gives $(\mu \times \m)({\rm gr}(f))=0$. Bonus track: does anyone know any results in this direction if we had another measure space as co-domain? Is it possible to have ""fat"" graphics?",,"['real-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
97,Exercise books in analysis,Exercise books in analysis,,"I'm studying Rudin's Principles of mathematical analysis and I was wondering if there are some exercise books (that is, books with solved problems and exercises) that I can use as a companion to Rudin. The books I'm searching for should be: full of hard , non-obvious , non-common , and thought-provoking problems; rich of complete , step by step , rigorous , and enlightening solutions;","I'm studying Rudin's Principles of mathematical analysis and I was wondering if there are some exercise books (that is, books with solved problems and exercises) that I can use as a companion to Rudin. The books I'm searching for should be: full of hard , non-obvious , non-common , and thought-provoking problems; rich of complete , step by step , rigorous , and enlightening solutions;",,"['calculus', 'real-analysis', 'reference-request', 'soft-question', 'book-recommendation']"
98,Does the sequence $\{\sin^n(n)\}$ converge?,Does the sequence  converge?,\{\sin^n(n)\},Does the sequence $\{\sin^n(n)\}$ converge? Does the series $\sum\limits_{n=1}^\infty \sin^n(n)$ converge?,Does the sequence $\{\sin^n(n)\}$ converge? Does the series $\sum\limits_{n=1}^\infty \sin^n(n)$ converge?,,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'diophantine-approximation']"
99,Problem 6 - IMO 1985,Problem 6 - IMO 1985,,"For every real number $x_1$ construct the sequence $x_1,x_2,x_3,\ldots$ by setting $x_{n+1}=x_n(x_n+\frac{1}{n})$ for each $n \ge 1$. Prove that there exists exactly one value of $x_1$ for which $0 < x_n < x_{n+1} < 1$ for every $n$.","For every real number $x_1$ construct the sequence $x_1,x_2,x_3,\ldots$ by setting $x_{n+1}=x_n(x_n+\frac{1}{n})$ for each $n \ge 1$. Prove that there exists exactly one value of $x_1$ for which $0 < x_n < x_{n+1} < 1$ for every $n$.",,"['real-analysis', 'sequences-and-series', 'analysis', 'contest-math']"
