,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Where do numerical solvers for differential equations fail?,Where do numerical solvers for differential equations fail?,,"I am about to begin working on a numerical differential equation solver that uses heuristic optimisation methods (like evolution strategies or differential evolution ) to approximate a solution. However, since such methods can not guarantee global convergence their only ""raison d'être"" are differential equations that are numerically hard to solve. What could be some examples of such problems? Edit: The starting point for my work is this Ph.D. Thesis: Solving Differential Equations with Evolutionary Algorithms by José María Chaquet Ulldemolins . The optimiser generates approximated solutions, that are represented as a sum of shifted and distorted Gaussian Kernels. Especially interesting is the comparison performed in chapter 5.3.3 Comparison of DESCMA-ES with Numerical Methods , which compares the optimiser technique against a Runge-Kutta and an “simple explicit” method. This comparison is done on the Poisson Equation and the Wave Equation. It seems that the optimisation scheme is somewhat competitive, at least it reaches roughly the same residual error. Obviously, the classical methods are way faster.","I am about to begin working on a numerical differential equation solver that uses heuristic optimisation methods (like evolution strategies or differential evolution ) to approximate a solution. However, since such methods can not guarantee global convergence their only ""raison d'être"" are differential equations that are numerically hard to solve. What could be some examples of such problems? Edit: The starting point for my work is this Ph.D. Thesis: Solving Differential Equations with Evolutionary Algorithms by José María Chaquet Ulldemolins . The optimiser generates approximated solutions, that are represented as a sum of shifted and distorted Gaussian Kernels. Especially interesting is the comparison performed in chapter 5.3.3 Comparison of DESCMA-ES with Numerical Methods , which compares the optimiser technique against a Runge-Kutta and an “simple explicit” method. This comparison is done on the Poisson Equation and the Wave Equation. It seems that the optimisation scheme is somewhat competitive, at least it reaches roughly the same residual error. Obviously, the classical methods are way faster.",,"['ordinary-differential-equations', 'partial-differential-equations', 'numerical-methods']"
1,Long time average of solution to ODE with almost periodic structure,Long time average of solution to ODE with almost periodic structure,,"I encountered the following question in my studies: Let $f:\mathbb{R} \rightarrow \mathbb{R}$ be a Bohr almost periodic function such that $\inf_{\mathbb{R}} f = 0$ but $f(x) > 0$ for all $x\in \mathbb{R}$ . An example is $$ f(x) = 2-\sin(2\pi x) - \sin(2\pi \sqrt{2}x).$$ If $\eta(\cdot)$ is the solution to the following ODE $$ \dot{\eta}(s) = f(\eta(s)), \qquad \eta(0) = 0.$$ Is there any tools that allow us to say something about the limit $$ \lim_{s\rightarrow +\infty} \frac{\eta(s)}{s}$$ and if the limit exists (I guess, by numerical implementations) can we say anything about the rate of convergence of $\frac{\eta(s)}{s}$ to that limit?","I encountered the following question in my studies: Let be a Bohr almost periodic function such that but for all . An example is If is the solution to the following ODE Is there any tools that allow us to say something about the limit and if the limit exists (I guess, by numerical implementations) can we say anything about the rate of convergence of to that limit?","f:\mathbb{R} \rightarrow \mathbb{R} \inf_{\mathbb{R}} f = 0 f(x) > 0 x\in \mathbb{R}  f(x) = 2-\sin(2\pi x) - \sin(2\pi \sqrt{2}x). \eta(\cdot)  \dot{\eta}(s) = f(\eta(s)), \qquad \eta(0) = 0.  \lim_{s\rightarrow +\infty} \frac{\eta(s)}{s} \frac{\eta(s)}{s}","['real-analysis', 'ordinary-differential-equations', 'asymptotics', 'stability-in-odes', 'almost-periodic-functions']"
2,Proof that Singular Solution of Clairaut's Equation is the envelope of the family of General Solutions [duplicate],Proof that Singular Solution of Clairaut's Equation is the envelope of the family of General Solutions [duplicate],,"This question already has an answer here : Relation between general solutions and singular solution of Clairaut’s equation. (1 answer) Closed 2 years ago . I want to show that the singular solution is the envelope for the general solutions. Proof Outline Both solutions pass from the same point $(a,b)$ Both solutions have the same gradient at that point (are tangent to each other) Proof: The form of Clairauts equation is $$y(x) = xy' + f(y')$$ You differentiate once to get $$y' = y' + xy'' + f'(y')y''$$ You rearrange and get two solutions The general solution $$y = Cx + f(C)$$ The singular solution $$x + f'(y') = 0$$ From the general solution we get $$a = (b - f(C))/C$$ Subbing this in to the singular solution we get $$b - f(C) + Cf'(y') = 0$$ Here, I need to show that this equation holds to show that for $x=a$ , the singular solution also passes $y=b$ , which will cover (1) from my requirements. Also, any help on (2) would be greatly appreciated. I have no idea how to proceed with that.","This question already has an answer here : Relation between general solutions and singular solution of Clairaut’s equation. (1 answer) Closed 2 years ago . I want to show that the singular solution is the envelope for the general solutions. Proof Outline Both solutions pass from the same point Both solutions have the same gradient at that point (are tangent to each other) Proof: The form of Clairauts equation is You differentiate once to get You rearrange and get two solutions The general solution The singular solution From the general solution we get Subbing this in to the singular solution we get Here, I need to show that this equation holds to show that for , the singular solution also passes , which will cover (1) from my requirements. Also, any help on (2) would be greatly appreciated. I have no idea how to proceed with that.","(a,b) y(x) = xy' + f(y') y' = y' + xy'' + f'(y')y'' y = Cx + f(C) x + f'(y') = 0 a = (b - f(C))/C b - f(C) + Cf'(y') = 0 x=a y=b","['ordinary-differential-equations', 'proof-writing', 'singular-solution', 'envelope']"
3,Constrained Calculus of Variations: maximize volume given fixed surface area,Constrained Calculus of Variations: maximize volume given fixed surface area,,"We wish to find a curve $y : C^1[-a, a]$ , where $a$ is an unknown parameter, such that $y(\pm a) = 0$ , and the volume of the solid of revolution generated by rotating $y$ around the $x$ -axis has maximal volume, given that the surface area is fixed. In other words, we let $$ S(y) = \int_{-a}^a2\pi y\sqrt{1 + (y')^2}dx,\quad V(y) = \int_{-a}^a\pi y^2 dx $$ Then the goal is to find $y$ and $a$ so that $V(y)$ is maximized, subject to $S(y) = A$ , $y(\pm a) = 0$ . I'm expecting to end up with $y$ a semicircle, in which case $a = \frac12\sqrt{\frac A\pi}$ . Solving this using Lagrange multipliers and Euler-Lagrange equations however has proven tedious. The augmented problem I end up with is that of finding the stationary functions of the functional $$ \tilde{J}(y; a) = \int_{-a}^a\left(-\pi y^2 + 2\pi\lambda y\sqrt{1 + (y')^2}\right) dx $$ for some real number $\lambda$ . If we let $f(y, y')$ be the integrand, then the Euler-Lagrange equation (which has been written in an equivalent form given that $f$ doesn't depend directly on $x$ ) is $$ -f + y'f_{y'} = \text{const} $$ or, letting $C$ be our constant, $$ \pi y^2 - 2\pi \lambda y\sqrt{1 + (y')^2} + \frac{2\pi\lambda y(y')^2}{\sqrt{1 + (y')^2}} = C $$ Now, one may verify (by plugging it in) that $y(x) = \sqrt{a^2 - x^2}$ is a solution to this equation, where $\lambda = a/2$ and $C = 0$ , but for the life of me I cannot solve the equation in general (without the knowledge that the solution is a semicircle a priori). If we do some rearranging, we can end up with $$ (\pi y^2 - C)\sqrt{1 + (y')^2} - 2\pi\lambda y = 0 $$ in which case introducing the parametrization $y'(x) = \tan\theta(x)$ and rearranging gives $$ y = \lambda\left(\cos\theta\pm\sqrt{\cos^2\theta+c}\right) $$ for some other constant $c$ . We can then sub back in $y' = \tan\theta$ to get a separable equation in $\theta$ , namely: $$ d\theta\left(-\lambda\cos\theta\pm\frac{2\cos^2\theta}{\sqrt{\cos^2\theta+c}}\right) = dt $$ According to wolframalpha, however, this does not have an analytic antiderivative in general. How can I arrive at the desired result?","We wish to find a curve , where is an unknown parameter, such that , and the volume of the solid of revolution generated by rotating around the -axis has maximal volume, given that the surface area is fixed. In other words, we let Then the goal is to find and so that is maximized, subject to , . I'm expecting to end up with a semicircle, in which case . Solving this using Lagrange multipliers and Euler-Lagrange equations however has proven tedious. The augmented problem I end up with is that of finding the stationary functions of the functional for some real number . If we let be the integrand, then the Euler-Lagrange equation (which has been written in an equivalent form given that doesn't depend directly on ) is or, letting be our constant, Now, one may verify (by plugging it in) that is a solution to this equation, where and , but for the life of me I cannot solve the equation in general (without the knowledge that the solution is a semicircle a priori). If we do some rearranging, we can end up with in which case introducing the parametrization and rearranging gives for some other constant . We can then sub back in to get a separable equation in , namely: According to wolframalpha, however, this does not have an analytic antiderivative in general. How can I arrive at the desired result?","y : C^1[-a, a] a y(\pm a) = 0 y x 
S(y) = \int_{-a}^a2\pi y\sqrt{1 + (y')^2}dx,\quad V(y) = \int_{-a}^a\pi y^2 dx
 y a V(y) S(y) = A y(\pm a) = 0 y a = \frac12\sqrt{\frac A\pi} 
\tilde{J}(y; a) = \int_{-a}^a\left(-\pi y^2 + 2\pi\lambda y\sqrt{1 + (y')^2}\right) dx
 \lambda f(y, y') f x 
-f + y'f_{y'} = \text{const}
 C 
\pi y^2 - 2\pi \lambda y\sqrt{1 + (y')^2} + \frac{2\pi\lambda y(y')^2}{\sqrt{1 + (y')^2}} = C
 y(x) = \sqrt{a^2 - x^2} \lambda = a/2 C = 0 
(\pi y^2 - C)\sqrt{1 + (y')^2} - 2\pi\lambda y = 0
 y'(x) = \tan\theta(x) 
y = \lambda\left(\cos\theta\pm\sqrt{\cos^2\theta+c}\right)
 c y' = \tan\theta \theta 
d\theta\left(-\lambda\cos\theta\pm\frac{2\cos^2\theta}{\sqrt{\cos^2\theta+c}}\right) = dt
","['ordinary-differential-equations', 'calculus-of-variations']"
4,"An assertion in Euler's first paper on Elliptic integrals, E28","An assertion in Euler's first paper on Elliptic integrals, E28",,"In his first paper on Elliptic Integrals, Euler calculates the arc length of an ellipse by solving a non-separable differential equation arising from it. He previously thought no non-separable differential equation could be solved. He says: This case seemed exceedingly paradoxical to me at first; but after studying the solution more carefully I realized easily not only that a separation could not be deduced from it, but also, that if a separation were to succeed by another method, far greater absurdities would follow. One might find a comparison of the perimeters of different ellipses, which, it surely seems to me, would overturn all of analysis. My question: How might one find a comparison of perimeters of different ellipses if the differential equation were separable & why would this overturn all of analysis? Links to the original latin & English translation of Euler's article: http://eulerarchive.maa.org/pages/E028.html More context: I believe that the separation of variables in differential equations is so carefully sought because   a solution of the equation follows directly from that discovery, which is evident to one practiced   enough in these matters. Moreover the integration of differential equations, if indeed it succeeds,   is begun best by separating variables. Though certainly innumerable equations have been given,   whose integrals can be found without such a separation – the Celebrated Johann Bernoulli exhibited   a method of this type in our Comm. Tom. I page 1672 – yet all of these equations have been   arranged in such a way, so that either the separation of variables is obvious by itself, or that at   least the separation may be derived from that integration. It is indeed likely that the computation   of solutions that Analysts have found up until now are all of this type of equation, that, even if   the variables can be separated in no other way, a separation still arises from that solution. For this   reason, I have believed until now that no solvable differential equation could be produced whose   separation would elude all men. Recently however while engaged in the rectifying of an ellipse, I unexpectedly came upon a   differential equation by which I was able to solve the rectification of the ellipse, yet a separation of   variables could not be found, not even from the method of solution. In fact the equation I obtained   was $dy + \frac{y^2dx}{x} = \frac{x dx}{x^2−1}$ which closely resembles the Riccati equation, and as it happens it is as   difficult to separate as $dy + y^2 dx = x^2 dx$ . This case seemed exceedingly paradoxical to me   at first; but after studying the solution more carefully I realized easily not only that a separation   could not be deduced from it, but also, that if a separation were to succeed by another method,   far greater absurdities would follow. One might find a comparison of the perimeters of different   ellipses, which, it surely seems to me, would overturn all of analysis. This solution moreover   is extremely easy, it is completed indeed by the elongation of infinite ellipses having one of two   axes in common, and for this reason it must be substantially preferred to the usual way of solving   quadratures.","In his first paper on Elliptic Integrals, Euler calculates the arc length of an ellipse by solving a non-separable differential equation arising from it. He previously thought no non-separable differential equation could be solved. He says: This case seemed exceedingly paradoxical to me at first; but after studying the solution more carefully I realized easily not only that a separation could not be deduced from it, but also, that if a separation were to succeed by another method, far greater absurdities would follow. One might find a comparison of the perimeters of different ellipses, which, it surely seems to me, would overturn all of analysis. My question: How might one find a comparison of perimeters of different ellipses if the differential equation were separable & why would this overturn all of analysis? Links to the original latin & English translation of Euler's article: http://eulerarchive.maa.org/pages/E028.html More context: I believe that the separation of variables in differential equations is so carefully sought because   a solution of the equation follows directly from that discovery, which is evident to one practiced   enough in these matters. Moreover the integration of differential equations, if indeed it succeeds,   is begun best by separating variables. Though certainly innumerable equations have been given,   whose integrals can be found without such a separation – the Celebrated Johann Bernoulli exhibited   a method of this type in our Comm. Tom. I page 1672 – yet all of these equations have been   arranged in such a way, so that either the separation of variables is obvious by itself, or that at   least the separation may be derived from that integration. It is indeed likely that the computation   of solutions that Analysts have found up until now are all of this type of equation, that, even if   the variables can be separated in no other way, a separation still arises from that solution. For this   reason, I have believed until now that no solvable differential equation could be produced whose   separation would elude all men. Recently however while engaged in the rectifying of an ellipse, I unexpectedly came upon a   differential equation by which I was able to solve the rectification of the ellipse, yet a separation of   variables could not be found, not even from the method of solution. In fact the equation I obtained   was which closely resembles the Riccati equation, and as it happens it is as   difficult to separate as . This case seemed exceedingly paradoxical to me   at first; but after studying the solution more carefully I realized easily not only that a separation   could not be deduced from it, but also, that if a separation were to succeed by another method,   far greater absurdities would follow. One might find a comparison of the perimeters of different   ellipses, which, it surely seems to me, would overturn all of analysis. This solution moreover   is extremely easy, it is completed indeed by the elongation of infinite ellipses having one of two   axes in common, and for this reason it must be substantially preferred to the usual way of solving   quadratures.",dy + \frac{y^2dx}{x} = \frac{x dx}{x^2−1} dy + y^2 dx = x^2 dx,"['ordinary-differential-equations', 'analysis', 'math-history']"
5,Index of the closed trajectory,Index of the closed trajectory,,"Consider the system $$\begin{cases} \frac{dx}{dt}=x(1-x-y) \\ \frac{dy}{dt}=y(-1+2x) \end{cases} $$ Show that closed orbits are impossible for this system using index theory and properties of the solution curves. The fixed points of this system are: $(0,0),(1,0),(\frac{1}{2},\frac{1}{2}).$ I've drawn a global phase portrait, which clearly shows that there is no closed trajectory. But how do I show that using index theory? Note: Index of curve $𝐶$ with respect to vector ﬁeld $(𝑓,𝑔)$ is $𝐼(𝑓,𝑔,𝐶)=$ number of times the vector turns counter-clockwise","Consider the system Show that closed orbits are impossible for this system using index theory and properties of the solution curves. The fixed points of this system are: I've drawn a global phase portrait, which clearly shows that there is no closed trajectory. But how do I show that using index theory? Note: Index of curve with respect to vector ﬁeld is number of times the vector turns counter-clockwise","\begin{cases}
\frac{dx}{dt}=x(1-x-y) \\
\frac{dy}{dt}=y(-1+2x)
\end{cases}
 (0,0),(1,0),(\frac{1}{2},\frac{1}{2}). 𝐶 (𝑓,𝑔) 𝐼(𝑓,𝑔,𝐶)=",['ordinary-differential-equations']
6,Non autonomous ODE(qualitative analysis),Non autonomous ODE(qualitative analysis),,"I have non linear non-autonomous ODE system given by $$\tilde{d}(t)=d-\frac{A \cos(t)}{\bar{A}+A \sin(t)}$$ $$\dot{x}(t)=-yx(1-x)F(y)$$ $$\dot{y}(t)=-(1-y)(3xy(1-y^4)-\tilde{d}(t))$$ where $F(y)$ is a non linear function of $y$ that is decreasing in $y$ for most of the domain. I can solve this numerically using Matlab. Doing so, I get solutions for $x(t)$ and $y(t)$ that look periodic after some intial time span. Is there any way to make some statements about how ''average'' value of $x(t)$ changes with changing $A?$ Numerical solutions seem to suggest that it increases. $\tilde{d}(t)$ averaged over a cycle increases with A. I am very new to non-autonomous systems, so I would be very happy if one could guide me to the some concepts that may help me 'prove' this statement more formally.","I have non linear non-autonomous ODE system given by where is a non linear function of that is decreasing in for most of the domain. I can solve this numerically using Matlab. Doing so, I get solutions for and that look periodic after some intial time span. Is there any way to make some statements about how ''average'' value of changes with changing Numerical solutions seem to suggest that it increases. averaged over a cycle increases with A. I am very new to non-autonomous systems, so I would be very happy if one could guide me to the some concepts that may help me 'prove' this statement more formally.",\tilde{d}(t)=d-\frac{A \cos(t)}{\bar{A}+A \sin(t)} \dot{x}(t)=-yx(1-x)F(y) \dot{y}(t)=-(1-y)(3xy(1-y^4)-\tilde{d}(t)) F(y) y y x(t) y(t) x(t) A? \tilde{d}(t),"['ordinary-differential-equations', 'nonlinear-system']"
7,What do two ODEs with the same Lyapunov function have in common,What do two ODEs with the same Lyapunov function have in common,,"Assume I have two $N$ -dimensional systems ofODEs $\frac{dX}{dt}=F(X)$ with $F: \mathbb{R}^N\rightarrow\mathbb{R}^N$ and $\frac{dY}{dt}=G(Y)$ with $G: \mathbb{R}^N\rightarrow\mathbb{R}^N$ . Also, assume I have a function $V:\mathbb{R}^N\rightarrow \mathbb{R}$ such that $V$ is a complete Lyapunov function for both $F$ as well as $G$ (most importantly $V$ is strictly decreasing on parts of $\mathbb{R}^N$ which are not chain-recurrent, and iff $V(X)=V(Y)$ then $x$ and $y$ are part of the same chain-recurrent set). What else can we say then about the relationship between $F$ and $G$ ? Obviously $F$ and $G$ must have the same chain-recurrent sets, that is they have the same attractor structure. However, the flow of $F$ and $G$ can still be quite different. Intuitively it seems that $F$ and $G$ should be topologically equivalent, but I can't find any proof for that.","Assume I have two -dimensional systems ofODEs with and with . Also, assume I have a function such that is a complete Lyapunov function for both as well as (most importantly is strictly decreasing on parts of which are not chain-recurrent, and iff then and are part of the same chain-recurrent set). What else can we say then about the relationship between and ? Obviously and must have the same chain-recurrent sets, that is they have the same attractor structure. However, the flow of and can still be quite different. Intuitively it seems that and should be topologically equivalent, but I can't find any proof for that.",N \frac{dX}{dt}=F(X) F: \mathbb{R}^N\rightarrow\mathbb{R}^N \frac{dY}{dt}=G(Y) G: \mathbb{R}^N\rightarrow\mathbb{R}^N V:\mathbb{R}^N\rightarrow \mathbb{R} V F G V \mathbb{R}^N V(X)=V(Y) x y F G F G F G F G,"['ordinary-differential-equations', 'dynamical-systems', 'lyapunov-functions']"
8,Unique existence of horizontal lifts in Principal $G$-bundles,Unique existence of horizontal lifts in Principal -bundles,G,"I wanted to show that for any smooth principal $G$ -bundle $E\xrightarrow\pi B$ any smooth curve $\gamma\colon I\to B$ has a unique horizontal lift from a fixed starting point $u_0\in\pi^{-1}\left(\left\{\gamma\left(0\right)\right\}\right)$ . There are two proofs I could think of/I could find. (Convention: $I=\left[0,1\right]$ ) This is a long question, so even if you have an answer to any subpart, please do comment/answer. Proof 1 Suppose we have an Ehresmann connection $u\mapsto H_u$ on the bundle. Then as we have the map $\mathrm d\pi\vert_u\colon T_uE\to T_{\pi\left(u\right)}B$ we have $T_uE/\mathrm{ker}\,\mathrm d\pi\vert_u\simeq\mathrm{im}\,\mathrm d\pi\vert_u$ . But $\mathrm{ker}\,\mathrm d\pi\vert_u=V_u$ (the vertical subspace) and $T_uE=V_u\oplus H_u$ and $\mathrm{im}\,\mathrm d\pi\vert_u=T_{\pi\left(u\right)}B$ ( $\mathrm d\pi\vert_u$ is surjective as the bundle is locally trivialisable). So we have $H_u\simeq T_{\pi\left(u\right)}B$ . Now for smooth $\gamma\colon I\to B$ , we have the vector field $X_t=\dot\gamma\left(t\right)\equiv_\text{def}\mathrm d\gamma\vert_t\left(1\right)$ . Using the above isomorphism, we have a horizontal vector at every point u such that $\pi\left(u\right)\in\mathrm{im}\,\gamma$ . Choosing a $u_0\in\pi^{-1}\left(\left\{\gamma\left(0\right)\right\}\right)$ We can then find the integral curve of this vector field starting at $u_0$ and this will give a horizontal lift. Questions about this proof How do we guarantee that the integral curve is lift (is it because it has to stay on the submanifold $\pi^{-1}\left(\mathrm{im}\,\gamma\right)$ , is this even a submanifold?)? Do we have an existence theorem for integral curves of vector fields along submanifolds? Or an existence theorem for integral curves of vector fields defined on closed subsets? Any special case relevant here is fine too. Even if we have such an existence theorem, then won't it only give us a solution in some $\left[0,\epsilon\right]$ and not on all of $I$ ? Is there a theorem that will give me the existence of the curve on all of $I$ ? Now suppose what we constructed gives us a horizontal lift $\gamma^\mathrm{h}\colon I\to E$ , then how do we get uniqueness. What I thought is, now that we have a lift, we can apply the next proof to show uniqueness. But this uses the principal $G$ -bundle structure. Can we get uniqueness of horizontal lifts on general bundles with connection? Proof 2 We have a connection form $\omega\colon TE\to\mathrm{Lie}\, G$ ( $\mathrm{Lie}\,G$ is the lie algebra corresponding to the Lie group $G$ ). Let us assume that we have a lift $\tilde\gamma\colon I\to E$ starting at $u_0$ of $\gamma$ . Then any lift can be written as $\tilde\gamma_1\left(t\right)=\tilde\gamma\left(t\right)\cdot g\left(t\right)$ fpr some $g\colon I\to G$ (this follows from freeness and transitivity of the $G$ -action on each fibre). So in particular if a horizontal lift $\gamma^\mathrm{h}$ exists, then it too can we written as $\gamma^\mathrm{h}\left(t\right)=\tilde\gamma\left(t\right)\cdot g\left(t\right)$ . We have \begin{equation} \omega_{\gamma^\mathrm{h}\left(t\right)}\left(\dot\gamma^\mathrm{h}\left(t\right)\right)=0\Longleftrightarrow\omega_{\gamma^{h}\left(t\right)}\circ\mathrm d\gamma^\mathrm{h}\vert_t=0\Longleftrightarrow\left(\gamma^\mathrm{h}\right)^*\omega=0 \end{equation} Let $a\colon E\times G\to E,a_g\colon E\to E,a_u\colon G\to E$ be defined by $a\left(u,g\right)=a_g\left(u\right)=a_u\left(g\right)=u\cdot g$ . Then \begin{equation} \mathrm d\gamma^\mathrm{h}\vert_t=\mathrm da_{\tilde\gamma\left(t\right)}\vert_{g\left(t\right)}\circ\mathrm dg\vert_t+\mathrm da_{g\left(t\right)}\vert_{\tilde\gamma\left(t\right)}\circ\mathrm d\tilde\gamma\vert_t \end{equation} Now, $a_{\tilde\gamma\left(t\right)}=a_{\gamma^\mathrm{h}\left(t\right)\cdot g\left(t\right)^{-1}}=a_{\gamma^\mathrm{h}\left(t\right)}\circ L_{g\left(t\right)^{-1}}$ . So, $$\omega_{\gamma^\mathrm{h}\left(t\right)}\circ\mathrm d\gamma^\mathrm{h}\vert_t$$ $$=\omega_{\gamma^\mathrm{h}\left(t\right)}\circ\mathrm da_{\gamma^\mathrm{h}\left(t\right)}\vert_e\circ\mathrm dL_{g\left(t\right)^{-1}}\vert_{g\left(t\right)}\circ\mathrm dg\vert_t+\omega_{\gamma^\mathrm{h}\left(t\right)}\circ\mathrm da_{g\left(t\right)}\vert_{\tilde\gamma\left(t\right)}\circ\mathrm d\tilde\gamma\vert_t$$ $$=\mathrm dL_{g\left(t\right)^{-1}}\vert_{g\left(t\right)}\circ\mathrm dg\vert_t+\mathrm{Ad}_{g\left(t\right)^{-1}}\circ\omega_{\tilde\gamma\left(t\right)}\circ\mathrm d\tilde\gamma\vert_t$$ $$=\mathrm dL_{g\left(t\right)^{-1}}\vert_{g\left(t\right)}\circ\left(\mathrm dg\vert_t+\mathrm dR_{g\left(t\right)}\vert_e\circ\left(\tilde\gamma^*\omega\right)_t\right)=0$$ Since $L_g$ is an isomorphism, its derivative is invertible, so we have \begin{equation} \mathrm dg\vert_t=-\mathrm dR_{g\left(t\right)}\vert_e\circ\left(\tilde\gamma^*\omega\right)_t\Longleftrightarrow \dot g\left(t\right)=-\mathrm dR_{g\left(t\right)}\vert_e\left(\omega_{\tilde\gamma\left(t\right)}\left(\dot{\tilde\gamma}\left(t\right)\right)\right) \end{equation} Since $\tilde\gamma\left(0\right)=\gamma^\mathrm{h}\left(0\right)=u_0$ , we have $g\left(0\right)=e$ . So we have an ODE for $g$ , and we have a unique solution for $t\in\left[0,\epsilon\right]$ for some $\epsilon$ . We have uniqueness of lift as any lift could have written as $\tilde\gamma\left(t\right)\cdot g\left(t\right)$ . Questions about this proof We have the same extendibility problem as above. Given the final ODE for $g\left(t\right)$ , how do we know it can be found on all of $I$ and not just till some $\epsilon$ . (I know that for matrix groups we can do it using the path-ordered exponential.) How do we know that there is some lift? I tried to prove this as follows. First assume that the image of the curve lies in some open $U$ for which we have a trivialisation $\psi_U\colon \pi^{-1}\left(U\right)\xrightarrow\sim U\times G$ . Then we have the obvious lift starting at $u_0$ given by $\tilde\gamma\left(t\right)=\psi^{-1}\left(\gamma\left(t\right),g_0\right)$ where $\psi\left(u_0\right)=\left(\gamma\left(0\right),g_0\right)$ . Now suppose that the there $\mathrm{im}\,\gamma$ does not lie in a single trivialisation. Let $\left(U_p,\psi_p\colon\pi^{-1}\left(U_p\right)\xrightarrow\sim U_p\times G\right)$ be the local trivialisation of the bundle around $p\in E$ . Then we have a cover for $\mathrm{im}\,\gamma$ given by $\left\{U_p\right\}_{p\in\mathrm{im}\,\gamma}$ . Since $I$ is compact, and $\gamma$ is continuous, $\mathrm{im}\,\gamma$ is compact. So we have a finite subcover $\left\{U_{p_k}\right\}_{k=1}^n$ . Now I tried to inductively create a lift. First take the open set which contains $\gamma\left(0\right)$ , let is be $U_{p_{k_1}}$ . Take the connected component containing $0$ in $\gamma^{-1}\left(U_{p_{k_1}}\right)$ . Let the supremum of this $t_1$ . We can lift $\gamma\vert_{\left[0,t_1\right)}$ , call it $\tilde\gamma$ . Now because $\mathrm{im}\,\gamma$ is connencted, $\gamma\left(t_1\right)$ It will belong to some other open set, let that be $U_{p_{k_2}}$ . Repeat the process and lift this new strand with initial point at $\lim\limits_{t\rightarrow t_1}\tilde\gamma\left(t\right)$ (exists by compactness). repeat this process untill the enitre curve is lifted. Without loss of generality, I can assume that $U_{p_i}$ are simply connected pre-compact. I need to show that $\mathrm{im}\,\gamma\cap U$ has finitely many connected components if $U$ is simply connected and precompact. So this reduces to the following problem: Given a $\gamma\colon I\to\mathbb R^n$ , does $\mathrm{im}\,\gamma\cap B^n_1\left(0\right)$ (where $B$ is the unit ball) have finitely many connected components? I seem to be stuck here.","I wanted to show that for any smooth principal -bundle any smooth curve has a unique horizontal lift from a fixed starting point . There are two proofs I could think of/I could find. (Convention: ) This is a long question, so even if you have an answer to any subpart, please do comment/answer. Proof 1 Suppose we have an Ehresmann connection on the bundle. Then as we have the map we have . But (the vertical subspace) and and ( is surjective as the bundle is locally trivialisable). So we have . Now for smooth , we have the vector field . Using the above isomorphism, we have a horizontal vector at every point u such that . Choosing a We can then find the integral curve of this vector field starting at and this will give a horizontal lift. Questions about this proof How do we guarantee that the integral curve is lift (is it because it has to stay on the submanifold , is this even a submanifold?)? Do we have an existence theorem for integral curves of vector fields along submanifolds? Or an existence theorem for integral curves of vector fields defined on closed subsets? Any special case relevant here is fine too. Even if we have such an existence theorem, then won't it only give us a solution in some and not on all of ? Is there a theorem that will give me the existence of the curve on all of ? Now suppose what we constructed gives us a horizontal lift , then how do we get uniqueness. What I thought is, now that we have a lift, we can apply the next proof to show uniqueness. But this uses the principal -bundle structure. Can we get uniqueness of horizontal lifts on general bundles with connection? Proof 2 We have a connection form ( is the lie algebra corresponding to the Lie group ). Let us assume that we have a lift starting at of . Then any lift can be written as fpr some (this follows from freeness and transitivity of the -action on each fibre). So in particular if a horizontal lift exists, then it too can we written as . We have Let be defined by . Then Now, . So, Since is an isomorphism, its derivative is invertible, so we have Since , we have . So we have an ODE for , and we have a unique solution for for some . We have uniqueness of lift as any lift could have written as . Questions about this proof We have the same extendibility problem as above. Given the final ODE for , how do we know it can be found on all of and not just till some . (I know that for matrix groups we can do it using the path-ordered exponential.) How do we know that there is some lift? I tried to prove this as follows. First assume that the image of the curve lies in some open for which we have a trivialisation . Then we have the obvious lift starting at given by where . Now suppose that the there does not lie in a single trivialisation. Let be the local trivialisation of the bundle around . Then we have a cover for given by . Since is compact, and is continuous, is compact. So we have a finite subcover . Now I tried to inductively create a lift. First take the open set which contains , let is be . Take the connected component containing in . Let the supremum of this . We can lift , call it . Now because is connencted, It will belong to some other open set, let that be . Repeat the process and lift this new strand with initial point at (exists by compactness). repeat this process untill the enitre curve is lifted. Without loss of generality, I can assume that are simply connected pre-compact. I need to show that has finitely many connected components if is simply connected and precompact. So this reduces to the following problem: Given a , does (where is the unit ball) have finitely many connected components? I seem to be stuck here.","G E\xrightarrow\pi B \gamma\colon I\to B u_0\in\pi^{-1}\left(\left\{\gamma\left(0\right)\right\}\right) I=\left[0,1\right] u\mapsto H_u \mathrm d\pi\vert_u\colon T_uE\to T_{\pi\left(u\right)}B T_uE/\mathrm{ker}\,\mathrm d\pi\vert_u\simeq\mathrm{im}\,\mathrm d\pi\vert_u \mathrm{ker}\,\mathrm d\pi\vert_u=V_u T_uE=V_u\oplus H_u \mathrm{im}\,\mathrm d\pi\vert_u=T_{\pi\left(u\right)}B \mathrm d\pi\vert_u H_u\simeq T_{\pi\left(u\right)}B \gamma\colon I\to B X_t=\dot\gamma\left(t\right)\equiv_\text{def}\mathrm d\gamma\vert_t\left(1\right) \pi\left(u\right)\in\mathrm{im}\,\gamma u_0\in\pi^{-1}\left(\left\{\gamma\left(0\right)\right\}\right) u_0 \pi^{-1}\left(\mathrm{im}\,\gamma\right) \left[0,\epsilon\right] I I \gamma^\mathrm{h}\colon I\to E G \omega\colon TE\to\mathrm{Lie}\, G \mathrm{Lie}\,G G \tilde\gamma\colon I\to E u_0 \gamma \tilde\gamma_1\left(t\right)=\tilde\gamma\left(t\right)\cdot g\left(t\right) g\colon I\to G G \gamma^\mathrm{h} \gamma^\mathrm{h}\left(t\right)=\tilde\gamma\left(t\right)\cdot g\left(t\right) \begin{equation}
\omega_{\gamma^\mathrm{h}\left(t\right)}\left(\dot\gamma^\mathrm{h}\left(t\right)\right)=0\Longleftrightarrow\omega_{\gamma^{h}\left(t\right)}\circ\mathrm d\gamma^\mathrm{h}\vert_t=0\Longleftrightarrow\left(\gamma^\mathrm{h}\right)^*\omega=0
\end{equation} a\colon E\times G\to E,a_g\colon E\to E,a_u\colon G\to E a\left(u,g\right)=a_g\left(u\right)=a_u\left(g\right)=u\cdot g \begin{equation}
\mathrm d\gamma^\mathrm{h}\vert_t=\mathrm da_{\tilde\gamma\left(t\right)}\vert_{g\left(t\right)}\circ\mathrm dg\vert_t+\mathrm da_{g\left(t\right)}\vert_{\tilde\gamma\left(t\right)}\circ\mathrm d\tilde\gamma\vert_t
\end{equation} a_{\tilde\gamma\left(t\right)}=a_{\gamma^\mathrm{h}\left(t\right)\cdot g\left(t\right)^{-1}}=a_{\gamma^\mathrm{h}\left(t\right)}\circ L_{g\left(t\right)^{-1}} \omega_{\gamma^\mathrm{h}\left(t\right)}\circ\mathrm d\gamma^\mathrm{h}\vert_t =\omega_{\gamma^\mathrm{h}\left(t\right)}\circ\mathrm da_{\gamma^\mathrm{h}\left(t\right)}\vert_e\circ\mathrm dL_{g\left(t\right)^{-1}}\vert_{g\left(t\right)}\circ\mathrm dg\vert_t+\omega_{\gamma^\mathrm{h}\left(t\right)}\circ\mathrm da_{g\left(t\right)}\vert_{\tilde\gamma\left(t\right)}\circ\mathrm d\tilde\gamma\vert_t =\mathrm dL_{g\left(t\right)^{-1}}\vert_{g\left(t\right)}\circ\mathrm dg\vert_t+\mathrm{Ad}_{g\left(t\right)^{-1}}\circ\omega_{\tilde\gamma\left(t\right)}\circ\mathrm d\tilde\gamma\vert_t =\mathrm dL_{g\left(t\right)^{-1}}\vert_{g\left(t\right)}\circ\left(\mathrm dg\vert_t+\mathrm dR_{g\left(t\right)}\vert_e\circ\left(\tilde\gamma^*\omega\right)_t\right)=0 L_g \begin{equation}
\mathrm dg\vert_t=-\mathrm dR_{g\left(t\right)}\vert_e\circ\left(\tilde\gamma^*\omega\right)_t\Longleftrightarrow \dot g\left(t\right)=-\mathrm dR_{g\left(t\right)}\vert_e\left(\omega_{\tilde\gamma\left(t\right)}\left(\dot{\tilde\gamma}\left(t\right)\right)\right)
\end{equation} \tilde\gamma\left(0\right)=\gamma^\mathrm{h}\left(0\right)=u_0 g\left(0\right)=e g t\in\left[0,\epsilon\right] \epsilon \tilde\gamma\left(t\right)\cdot g\left(t\right) g\left(t\right) I \epsilon U \psi_U\colon \pi^{-1}\left(U\right)\xrightarrow\sim U\times G u_0 \tilde\gamma\left(t\right)=\psi^{-1}\left(\gamma\left(t\right),g_0\right) \psi\left(u_0\right)=\left(\gamma\left(0\right),g_0\right) \mathrm{im}\,\gamma \left(U_p,\psi_p\colon\pi^{-1}\left(U_p\right)\xrightarrow\sim U_p\times G\right) p\in E \mathrm{im}\,\gamma \left\{U_p\right\}_{p\in\mathrm{im}\,\gamma} I \gamma \mathrm{im}\,\gamma \left\{U_{p_k}\right\}_{k=1}^n \gamma\left(0\right) U_{p_{k_1}} 0 \gamma^{-1}\left(U_{p_{k_1}}\right) t_1 \gamma\vert_{\left[0,t_1\right)} \tilde\gamma \mathrm{im}\,\gamma \gamma\left(t_1\right) U_{p_{k_2}} \lim\limits_{t\rightarrow t_1}\tilde\gamma\left(t\right) U_{p_i} \mathrm{im}\,\gamma\cap U U \gamma\colon I\to\mathbb R^n \mathrm{im}\,\gamma\cap B^n_1\left(0\right) B","['ordinary-differential-equations', 'differential-geometry', 'fiber-bundles', 'connections']"
9,Making sense of the complex exponential function,Making sense of the complex exponential function,,"I read in a book that if we want to make sense of the $e^z$ where $z=x+iy$ , we already know how to interpret $e^x $ , so the only thing we have to make sense of is $e^{iy}$ . For $e^{iy}$ to make sense it has to obey the calculus rule. So $e^{iy}$ should be defined as the solution to the initial value problem: $$\frac{d}{dx}e^{iy}=ze^{zt}; e^{z*0}=1$$ I don't understand why for the $e^{iy}$ to make sense should be defined as the solution to the initial value problem?","I read in a book that if we want to make sense of the where , we already know how to interpret , so the only thing we have to make sense of is . For to make sense it has to obey the calculus rule. So should be defined as the solution to the initial value problem: I don't understand why for the to make sense should be defined as the solution to the initial value problem?",e^z z=x+iy e^x  e^{iy} e^{iy} e^{iy} \frac{d}{dx}e^{iy}=ze^{zt}; e^{z*0}=1 e^{iy},"['calculus', 'ordinary-differential-equations']"
10,Following stable manifolds for an everywhere generic path of functions and metrics,Following stable manifolds for an everywhere generic path of functions and metrics,,"Suppose that $M$ is a closed manifold, and that $f_t$ is a path of Morse functions, $0\leq t\leq 1$ . I know that I can ""follow"" the critical points $c_1,\dots,c_k$ of $f_0$ , meaning that there are smooth curves $\gamma_i:[0,1]\to M$ such that $\gamma_i(0)=c_i$ and $\gamma_i(t)$ is critical for $f_t$ (this is the implicit function theorem applied to $(t,x)\mapsto df_t(x)$ ). Now, suppose that $M$ is a closed manifold, and that $(f_t,g_t)$ is a path of Morse functions $f_t:M\to\mathbb{R}$ and Morse-Smale metrics $g_t:TM\otimes TM\to \mathbb{R}$ , $0\leq t\leq 1$ . Is there a way to ""follow"" the stable and unstable manifolds of the critical points $\gamma_i(t)$ for the gradient defined by $g_t(\mathrm{grad}\,f_t,\bullet)=-df_t(\bullet)$ ? In the case where $(f_t,g_t)$ is given by $(\varphi_t^*f_0,\varphi_t^*g_0)$ where $\varphi_t$ is an isotopy of $M$ , there is no problem showing that $W^u(\gamma_i(t))=\varphi^{-1}_t(W^u(c_i))$ , and so to get an identification as wanted. But this is too strong because it implies for example that the flows of all the gradients are smoothly conjugated, so it doesn't hold in the genral case. So my question is: are there some references dealing with that problem in the general case? Thanks for your answers.","Suppose that is a closed manifold, and that is a path of Morse functions, . I know that I can ""follow"" the critical points of , meaning that there are smooth curves such that and is critical for (this is the implicit function theorem applied to ). Now, suppose that is a closed manifold, and that is a path of Morse functions and Morse-Smale metrics , . Is there a way to ""follow"" the stable and unstable manifolds of the critical points for the gradient defined by ? In the case where is given by where is an isotopy of , there is no problem showing that , and so to get an identification as wanted. But this is too strong because it implies for example that the flows of all the gradients are smoothly conjugated, so it doesn't hold in the genral case. So my question is: are there some references dealing with that problem in the general case? Thanks for your answers.","M f_t 0\leq t\leq 1 c_1,\dots,c_k f_0 \gamma_i:[0,1]\to M \gamma_i(0)=c_i \gamma_i(t) f_t (t,x)\mapsto df_t(x) M (f_t,g_t) f_t:M\to\mathbb{R} g_t:TM\otimes TM\to \mathbb{R} 0\leq t\leq 1 \gamma_i(t) g_t(\mathrm{grad}\,f_t,\bullet)=-df_t(\bullet) (f_t,g_t) (\varphi_t^*f_0,\varphi_t^*g_0) \varphi_t M W^u(\gamma_i(t))=\varphi^{-1}_t(W^u(c_i))","['ordinary-differential-equations', 'differential-geometry', 'dynamical-systems', 'morse-theory']"
11,Solving differential equation with vector magnitude.,Solving differential equation with vector magnitude.,,I am trying to solve a system of three coupled differential equations. I managed to simplify them using a matrix. $$  \newcommand{\myMatrix}[1]{\bm{\mathit{#1}}} \frac{d\vec{x}}{dt}=\pmb{A}\left| \vec{x} \right|\vec{x}-\vec{d} $$ Where $\pmb{A}$ is a constant $3\times3$ matrix and $\vec{d}$ is a constant vector. I know there are ways to solve this if it weren't for the vector magnitude. Does anybody have any idea how to solve it with the vector magnitude?,I am trying to solve a system of three coupled differential equations. I managed to simplify them using a matrix. Where is a constant matrix and is a constant vector. I know there are ways to solve this if it weren't for the vector magnitude. Does anybody have any idea how to solve it with the vector magnitude?,  \newcommand{\myMatrix}[1]{\bm{\mathit{#1}}} \frac{d\vec{x}}{dt}=\pmb{A}\left| \vec{x} \right|\vec{x}-\vec{d}  \pmb{A} 3\times3 \vec{d},"['calculus', 'linear-algebra', 'ordinary-differential-equations']"
12,The Ho-Lee Model (1986),The Ho-Lee Model (1986),,"(My question) I solved the following questions. However, if you know the other solutions, please let me know those along with computation processes. Besides, $W_t$ is a S.B.M. (Thank you for your help in advance.) (Cross-link) I have posted the same question on https://quant.stackexchange.com/questions/47306/the-ho-lee-model-1986 (Original questions) Consider a short term interest rate process $(r_t)_{ t \in \mathbb{R}_+ }$ in the Ho-Lee Model with constant coefficients: \begin{eqnarray} dr_t = \theta dt + \sigma dW_t \end{eqnarray} and let $P(t, T)$ will denote the arbitrage price of a zero-coupon bond in this model: \begin{eqnarray} P(t, T) = E^{ \mathbb{P} } \left[ \exp \left( - \int^T_t r_s ds \right) | \mathcal{F}_t \right]  \end{eqnarray} where $t \in [0, T]$ . (1) State the bond pricing PDE satisfied by the function $F(t, x)$ defined via \begin{eqnarray} F(t, x) = E^{ \mathbb{P} } \left[ \exp \left( - \int^T_t r_s ds \right) | r_t=x \right]  \end{eqnarray} where $t \in [0, T]$ . (2) Compute the arbitrage price $F(t, r_t) =P(t, T)$ from its expression of $P(t, T)$ as a conditional expectation. (3) Check that the function $F(t, x)$ computed in Question (2) does satisfy the PDE derived in Question (1). (1) My answer One can derive the following PDE by Feynman-Kac Theorem. \begin{eqnarray} F(t, x) &=& E^{ \mathbb{P} } \left[ \exp \left( - \int^T_t r_s ds \right) | r_t=x \right] \\ \exp \left( - \int^t_0 r_s ds \right) F(t, x) &=& E^{ \mathbb{P} } \left[ \exp \left( - \int^T_0 r_s ds \right) | r_t=x \right]  \end{eqnarray} One adapts It $\hat{o}$ 's formula the above equation. Here, one has to pay attention that the R.H.S is zero because it is a constant value, namely an expectation value. Moreover, $F(T, x)=1$ by the R.H.S going to $1$ because of $t=T$ . \begin{eqnarray} && d\left( \exp \left( - \int^t_0 r_s ds \right) F(t, x)  \right)  \nonumber \\ && \qquad \qquad \qquad \qquad \qquad = \exp \left( - \int^t_0 r_s ds \right) \left( -r_t F(t, x) + \partial_t F(t, x) \right) dt \nonumber \\ && \qquad \qquad \qquad  \qquad \qquad \qquad  + \exp \left( - \int^t_0 r_s ds \right) \partial_x F(t, x) dr_t \nonumber \\ && \qquad \qquad \qquad \qquad \qquad \qquad  + \frac{1}{2}  \exp \left( - \int^t_0 r_s ds \right) \partial_{xx} F(t, x) d[r_t]  \end{eqnarray} One substitutes $r_t$ into the above equation. \begin{eqnarray} && d\left( \exp \left( - \int^t_0 r_s ds \right) F(t, x)  \right)  \nonumber \\ && = \exp \left( - \int^t_0 r_s ds \right) \left( -r_t F(t, x) + \partial_t F(t, x) \right) dt \nonumber \\ &&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad + \exp \left( - \int^t_0 r_s ds \right) \partial_x F(t, x) \left(  \theta dt + \sigma dW_t \right) \nonumber \\ &&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad + \frac{1}{2}  \exp \left( - \int^t_0 r_s ds \right) \partial_{xx} F(t, x) \sigma^2 dt \\ && = \exp \left( - \int^t_0 r_s ds \right) \left( -r_t F(t, x) + \partial_t F(t, x) + \theta \partial_x F(t, x) + \frac{1}{2}  \sigma^2 \partial_{xx} F(t, x) \right) dt \nonumber \\ &&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad + \exp \left( - \int^t_0 r_s ds \right)   \sigma  \partial_x F(t, x)  dW_t   \end{eqnarray} Since the coefficient of the drift term is zero due to the martingale property of S.D.E, the following equation is obtained. Here, one has to pay attention to $r_t=x$ . \begin{eqnarray} && -r_t F(t, x) + \partial_t F(t, x) + \theta \partial_x F(t, x) + \frac{1}{2}  \sigma^2 \partial_{xx} F(t, x) =0 \\ && -xF(t, x) + \partial_t F(t, x) + \theta \partial_x F(t, x) + \frac{1}{2}  \sigma^2 \partial_{xx} F(t, x) =0 \end{eqnarray} $\square$ (2) My answer Compute S.I.E by the given S.D.E. \begin{eqnarray} dr_t &=& \theta dt + \sigma dW_t \\ r_t &=& r_0 + \theta t + \sigma W_t \end{eqnarray} Let $x=r_t$ : \begin{eqnarray} && F(t, x) \\ &&= F(t, r_t) \\ &&= E^{ \mathbb{P} } \left[ \exp \left( - \int^T_t r_s ds \right) | r_t=x \right] \\ &&= E^{ \mathbb{P} } \left[ \exp \left( - \int^T_t  \left( r_0 + \theta s + \sigma W_s  \right)  ds\right) | \mathcal{F}_t\right] \\ &&= E^{ \mathbb{P} } \left[ \exp \left( - \int^T_t  \left( \left( r_0 + \theta t + \sigma W_t\right)+ \theta (s-t) + \sigma( W_s -W_t) \right)  ds \right) | \mathcal{F}_t\right] \\ &&= E^{ \mathbb{P} } \left[ \exp \left( - \int^T_t  \left( r_t + \theta (s-t) + \sigma( W_s -W_t) \right)  ds \right) | \mathcal{F}_t\right] \\ \end{eqnarray} Here, one computes $exp$ . \begin{eqnarray} && \exp \left( - \int^T_t  \left( r_t + \theta (s-t) + \sigma( W_s -W_t) \right)  ds \right) \\ && \qquad =\exp \left( - r_t \int^T_t  ds \right) \cdot  \exp \left( - \theta \int^T_t  (s-t) ds \right) \nonumber \\ &&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad \cdot \exp \left( - \sigma \int^T_t  ( W_s -W_t) ds \right) \\ && \qquad =\exp \left( - r_t (T-t)  \right) \nonumber \\ &&   \qquad \qquad \qquad \qquad  \cdot  \exp \left( - \theta \frac{1}{2} (T^2- t^2) + \theta t(T-t)  \right) \nonumber \\ &&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad \cdot \exp \left( - \sigma \int^T_t  ( W_s -W_t) ds \right) \\ && \qquad =\exp \left( - r_t (T-t)  \right) \nonumber \\ &&   \qquad \qquad \qquad \qquad  \cdot  \exp \left( - \theta (T-t) \left( \frac{1}{2} (T + t) - t\right)  \right) \nonumber \\ &&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad \cdot \exp \left( - \sigma \int^T_t  ( W_s -W_t) ds \right) \\ && \qquad =\exp \left( - r_t (T-t)  \right) \nonumber \\ &&   \qquad \qquad \qquad \qquad  \cdot  \exp \left( - \theta (T-t) \left( \frac{1}{2} (T - t) \right)  \right) \nonumber \\ &&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad \cdot \exp \left( - \sigma  \int^T_t  ( W_s -W_t)ds \right) \\ && \qquad =\exp \left( - r_t (T-t)  \right)  \cdot  \exp \left( - \frac{1}{2} \theta (T-t)^2 \ \right)  \nonumber \\ &&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad \cdot \exp \left( - \sigma  \int^T_t  ( W_s -W_t) ds \right) \\ && \qquad =\exp \left( - r_t (T-t)  - \frac{1}{2} \theta (T-t)^2  \right)  \nonumber \\ &&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad \cdot \exp \left( - \sigma \int^T_t  ( W_s -W_t)  ds \right) \\ \end{eqnarray} Substitute the above result into the Expectation. \begin{eqnarray} && F(t, r_t) \\ &&= E^{ \mathbb{P} } \left[ \exp \left( - r_t (T-t) - \frac{1}{2} \theta (T-t)^2  - \sigma  \int^T_t  ( W_s -W_t) ds \right) | \mathcal{F}_t \right] \\ &&= \exp \left( - r_t (T-t) - \frac{1}{2} \theta (T-t)^2  \right) \nonumber \\ &&   \qquad \qquad \qquad \qquad \qquad \qquad \cdot E^{ \mathbb{P} } \left[  \exp \left( - \sigma  \int^T_t  ( W_s -W_t)  ds \right) | \mathcal{F}_t \right]  \end{eqnarray} One computes the above expectation value as below. \begin{eqnarray} &&E^{ \mathbb{P} } \left[  \exp \left( - \sigma  \int^T_t  ( W_s -W_t)  ds \right) | \mathcal{F}_t \right] \nonumber \\ &&   \qquad  \qquad \qquad \qquad = E^{ \mathbb{P} } \left[  \exp \left( - \sigma  \int^T_t  W_{s-t}  ds \right) | \mathcal{F}_t \right] \\ &&   \qquad  \qquad \qquad \qquad = E^{ \mathbb{P} } \left[  \exp \left( \int^{T-t}_0  \left(  - \sigma W_s \right)  ds \right) | \mathcal{F}_t \right]  \end{eqnarray} Here, It $\hat{o}$ 's formula is applied to the exponent part, and further calculation is performed. \begin{eqnarray} d \left( - \sigma  W_s s \right) &=& - \sigma  W_s ds  -  \sigma s d W_s + \frac{1}{2} 0 d [W_s] \\ &=& - \sigma   W_s  ds  -  \sigma s d W_s \\ \int^{T-t}_0 d \left( - \sigma  W_s s \right) &=& \int^{T-t}_0 \left( - \sigma  W_s  \right) ds - \sigma \int^{T-t}_0 s d W_s \\  - \sigma  W_{T-t}  (T-t)  &=& \int^{T-t}_0 \left( - \sigma  W_s  \right) ds - \sigma \int^{T-t}_0 s d W_s \\  \int^{T-t}_0 \left( - \sigma  W_s \right) ds  &=& - \sigma  W_{T-t}  (T-t) + \int^{T-t}_0 \sigma s d W_s \\  &=& - \sigma  (T-t) \int^{T-t}_0 d W_s  + \int^{T-t}_0 \sigma s d W_s \\  &=& \sigma \int^{T-t}_0 \left( s- (T-t) \right)  d W_s \\  &=&  \int^{T-t}_0 \left( \sigma \left( s- (T-t) \right) \right)  d W_s \\ \end{eqnarray} Substitute the above result into the expectation. \begin{eqnarray} &&E^{ \mathbb{P} } \left[  \exp \left( - \sigma  \int^T_t  ( W_s -W_t)  ds \right) | \mathcal{F}_t \right] \nonumber \\ &&   \qquad  \qquad \qquad \qquad = E^{ \mathbb{P} } \left[  \exp \left( \int^{T-t}_0 \left(  - \sigma W_s \right) ds \right) | \mathcal{F}_t \right] \\ &&   \qquad  \qquad \qquad \qquad = E^{ \mathbb{P} } \left[  \exp \left( \int^{T-t}_0 \left(  \sigma \left( s - (T-t)\right) \right)  d W_s  \right) | \mathcal{F}_t \right] \\ &&   \qquad  \qquad \qquad \qquad = \exp \left( \frac{ \sigma^2}{2} \int^{T-t}_0 \left( s - (T-t) \right)^2 ds \right) \\ &&   \qquad  \qquad \qquad \qquad = \exp \left( \frac{ \sigma^2}{2}  \left[ \frac{1}{3} \left( s - (T-t) \right)^3 \right]^{T-t}_0 \right) \\ &&   \qquad  \qquad \qquad \qquad = \exp \left( \frac{ \sigma^2}{6}  (T-t)^3  \right)  \end{eqnarray} Substitute the above result into the Expectation of the $F(t, r_t)$ equation. \begin{eqnarray} && F(t, r_t) \\ &&= \exp \left( - r_t (T-t) - \frac{1}{2} \theta (T-t)^2  \right) \nonumber \\ &&   \qquad \qquad \qquad \qquad \qquad \qquad \cdot E^{ \mathbb{P} } \left[  \exp \left( - \sigma  \int^T_t  ( W_s -W_t)  ds \right) | \mathcal{F}_t \right] \\ &&= \exp \left( - r_t (T-t) - \frac{1}{2} \theta (T-t)^2  \right) \cdot \exp \left( \frac{ \sigma^2}{6}  (T-t)^3  \right) \\ &&= \exp \left( - r_t (T-t) - \frac{1}{2} \theta (T-t)^2  + \frac{ \sigma^2}{6}  (T-t)^3  \right)  \end{eqnarray} $\square$ (3) My answer Let $r_t=x$ into $F(t, r_t)$ of (2). \begin{eqnarray} F(t, x) =  \exp \left( - x (T-t) - \frac{1}{2} \theta (T-t)^2  + \frac{ \sigma^2}{6}  (T-t)^3  \right)  \end{eqnarray} Then, one reaches the following equations. \begin{eqnarray} \partial_t F(t, x) &=&  \left(  x  + \theta (T-t)  - \frac{ \sigma^2}{2}  (T-t)^2  \right) F(t, x) \\ \partial_x F(t, x) &=& - (T-t)  F(t, x) \\ \partial_{xx} F(t, x) &=&  (T-t)^2  F(t, x) \\ \end{eqnarray} Therefore, one computes the P.D.E of (1). Besides, the terminal condition is $F(T, x)=1$ . \begin{eqnarray} && -xF(t, x) + \partial_t F(t, x) + \theta \partial_x F(t, x) + \frac{1}{2}  \sigma^2 \partial_{xx} F(t, x) \\ && \qquad \qquad \qquad \qquad \qquad = -xF(t, x) \\ && \qquad \qquad \qquad \qquad \qquad \qquad  + \left(  x  + \theta (T-t)  - \frac{ \sigma^2}{2}  (T-t)^2  \right) F(t, x) \\ && \qquad \qquad \qquad \qquad \qquad \qquad - \theta (T-t)  F(t, x) + \frac{1}{2}  \sigma^2 (T-t)^2  F(t, x) \\ && \qquad \qquad \qquad \qquad \qquad = 0 \end{eqnarray} $\square$ (Thank you for your help in advance.)","(My question) I solved the following questions. However, if you know the other solutions, please let me know those along with computation processes. Besides, is a S.B.M. (Thank you for your help in advance.) (Cross-link) I have posted the same question on https://quant.stackexchange.com/questions/47306/the-ho-lee-model-1986 (Original questions) Consider a short term interest rate process in the Ho-Lee Model with constant coefficients: and let will denote the arbitrage price of a zero-coupon bond in this model: where . (1) State the bond pricing PDE satisfied by the function defined via where . (2) Compute the arbitrage price from its expression of as a conditional expectation. (3) Check that the function computed in Question (2) does satisfy the PDE derived in Question (1). (1) My answer One can derive the following PDE by Feynman-Kac Theorem. One adapts It 's formula the above equation. Here, one has to pay attention that the R.H.S is zero because it is a constant value, namely an expectation value. Moreover, by the R.H.S going to because of . One substitutes into the above equation. Since the coefficient of the drift term is zero due to the martingale property of S.D.E, the following equation is obtained. Here, one has to pay attention to . (2) My answer Compute S.I.E by the given S.D.E. Let : Here, one computes . Substitute the above result into the Expectation. One computes the above expectation value as below. Here, It 's formula is applied to the exponent part, and further calculation is performed. Substitute the above result into the expectation. Substitute the above result into the Expectation of the equation. (3) My answer Let into of (2). Then, one reaches the following equations. Therefore, one computes the P.D.E of (1). Besides, the terminal condition is . (Thank you for your help in advance.)","W_t (r_t)_{ t \in \mathbb{R}_+ } \begin{eqnarray}
dr_t = \theta dt + \sigma dW_t
\end{eqnarray} P(t, T) \begin{eqnarray}
P(t, T) = E^{ \mathbb{P} } \left[ \exp \left( - \int^T_t r_s ds \right) | \mathcal{F}_t \right] 
\end{eqnarray} t \in [0, T] F(t, x) \begin{eqnarray}
F(t, x) = E^{ \mathbb{P} } \left[ \exp \left( - \int^T_t r_s ds \right) | r_t=x \right] 
\end{eqnarray} t \in [0, T] F(t, r_t) =P(t, T) P(t, T) F(t, x) \begin{eqnarray}
F(t, x) &=& E^{ \mathbb{P} } \left[ \exp \left( - \int^T_t r_s ds \right) | r_t=x \right] \\
\exp \left( - \int^t_0 r_s ds \right) F(t, x) &=& E^{ \mathbb{P} } \left[ \exp \left( - \int^T_0 r_s ds \right) | r_t=x \right] 
\end{eqnarray} \hat{o} F(T, x)=1 1 t=T \begin{eqnarray}
&& d\left( \exp \left( - \int^t_0 r_s ds \right) F(t, x)  \right)  \nonumber \\
&& \qquad \qquad \qquad \qquad \qquad = \exp \left( - \int^t_0 r_s ds \right) \left( -r_t F(t, x) + \partial_t F(t, x) \right) dt \nonumber \\
&& \qquad \qquad \qquad  \qquad \qquad \qquad  + \exp \left( - \int^t_0 r_s ds \right) \partial_x F(t, x) dr_t \nonumber \\
&& \qquad \qquad \qquad \qquad \qquad \qquad  + \frac{1}{2}  \exp \left( - \int^t_0 r_s ds \right) \partial_{xx} F(t, x) d[r_t] 
\end{eqnarray} r_t \begin{eqnarray}
&& d\left( \exp \left( - \int^t_0 r_s ds \right) F(t, x)  \right)  \nonumber \\
&& = \exp \left( - \int^t_0 r_s ds \right) \left( -r_t F(t, x) + \partial_t F(t, x) \right) dt \nonumber \\
&&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad + \exp \left( - \int^t_0 r_s ds \right) \partial_x F(t, x) \left(  \theta dt + \sigma dW_t \right) \nonumber \\
&&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad + \frac{1}{2}  \exp \left( - \int^t_0 r_s ds \right) \partial_{xx} F(t, x) \sigma^2 dt \\
&& = \exp \left( - \int^t_0 r_s ds \right) \left( -r_t F(t, x) + \partial_t F(t, x) + \theta \partial_x F(t, x) + \frac{1}{2}  \sigma^2 \partial_{xx} F(t, x) \right) dt \nonumber \\
&&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad + \exp \left( - \int^t_0 r_s ds \right)   \sigma  \partial_x F(t, x)  dW_t  
\end{eqnarray} r_t=x \begin{eqnarray}
&& -r_t F(t, x) + \partial_t F(t, x) + \theta \partial_x F(t, x) + \frac{1}{2}  \sigma^2 \partial_{xx} F(t, x) =0 \\
&& -xF(t, x) + \partial_t F(t, x) + \theta \partial_x F(t, x) + \frac{1}{2}  \sigma^2 \partial_{xx} F(t, x) =0
\end{eqnarray} \square \begin{eqnarray}
dr_t &=& \theta dt + \sigma dW_t \\
r_t &=& r_0 + \theta t + \sigma W_t
\end{eqnarray} x=r_t \begin{eqnarray}
&& F(t, x) \\
&&= F(t, r_t) \\
&&= E^{ \mathbb{P} } \left[ \exp \left( - \int^T_t r_s ds \right) | r_t=x \right] \\
&&= E^{ \mathbb{P} } \left[ \exp \left( - \int^T_t  \left( r_0 + \theta s + \sigma W_s  \right)  ds\right) | \mathcal{F}_t\right] \\
&&= E^{ \mathbb{P} } \left[ \exp \left( - \int^T_t  \left( \left( r_0 + \theta t + \sigma W_t\right)+ \theta (s-t) + \sigma( W_s -W_t) \right)  ds \right) | \mathcal{F}_t\right] \\
&&= E^{ \mathbb{P} } \left[ \exp \left( - \int^T_t  \left( r_t + \theta (s-t) + \sigma( W_s -W_t) \right)  ds \right) | \mathcal{F}_t\right] \\
\end{eqnarray} exp \begin{eqnarray}
&& \exp \left( - \int^T_t  \left( r_t + \theta (s-t) + \sigma( W_s -W_t) \right)  ds \right) \\
&& \qquad =\exp \left( - r_t \int^T_t  ds \right) \cdot  \exp \left( - \theta \int^T_t  (s-t) ds \right) \nonumber \\
&&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad \cdot \exp \left( - \sigma \int^T_t  ( W_s -W_t) ds \right) \\
&& \qquad =\exp \left( - r_t (T-t)  \right) \nonumber \\
&&   \qquad \qquad \qquad \qquad  \cdot  \exp \left( - \theta \frac{1}{2} (T^2- t^2) + \theta t(T-t)  \right) \nonumber \\
&&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad \cdot \exp \left( - \sigma \int^T_t  ( W_s -W_t) ds \right) \\
&& \qquad =\exp \left( - r_t (T-t)  \right) \nonumber \\
&&   \qquad \qquad \qquad \qquad  \cdot  \exp \left( - \theta (T-t) \left( \frac{1}{2} (T + t) - t\right)  \right) \nonumber \\
&&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad \cdot \exp \left( - \sigma \int^T_t  ( W_s -W_t) ds \right) \\
&& \qquad =\exp \left( - r_t (T-t)  \right) \nonumber \\
&&   \qquad \qquad \qquad \qquad  \cdot  \exp \left( - \theta (T-t) \left( \frac{1}{2} (T - t) \right)  \right) \nonumber \\
&&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad \cdot \exp \left( - \sigma  \int^T_t  ( W_s -W_t)ds \right) \\
&& \qquad =\exp \left( - r_t (T-t)  \right)  \cdot  \exp \left( - \frac{1}{2} \theta (T-t)^2 \ \right)  \nonumber \\
&&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad \cdot \exp \left( - \sigma  \int^T_t  ( W_s -W_t) ds \right) \\
&& \qquad =\exp \left( - r_t (T-t)  - \frac{1}{2} \theta (T-t)^2  \right)  \nonumber \\
&&   \qquad \qquad \qquad \qquad \qquad \qquad \qquad \cdot \exp \left( - \sigma \int^T_t  ( W_s -W_t)  ds \right) \\
\end{eqnarray} \begin{eqnarray}
&& F(t, r_t) \\
&&= E^{ \mathbb{P} } \left[ \exp \left( - r_t (T-t) - \frac{1}{2} \theta (T-t)^2  - \sigma  \int^T_t  ( W_s -W_t) ds \right) | \mathcal{F}_t \right] \\
&&= \exp \left( - r_t (T-t) - \frac{1}{2} \theta (T-t)^2  \right) \nonumber \\
&&   \qquad \qquad \qquad \qquad \qquad \qquad \cdot E^{ \mathbb{P} } \left[  \exp \left( - \sigma  \int^T_t  ( W_s -W_t)  ds \right) | \mathcal{F}_t \right] 
\end{eqnarray} \begin{eqnarray}
&&E^{ \mathbb{P} } \left[  \exp \left( - \sigma  \int^T_t  ( W_s -W_t)  ds \right) | \mathcal{F}_t \right] \nonumber \\
&&   \qquad  \qquad \qquad \qquad = E^{ \mathbb{P} } \left[  \exp \left( - \sigma  \int^T_t  W_{s-t}  ds \right) | \mathcal{F}_t \right] \\
&&   \qquad  \qquad \qquad \qquad = E^{ \mathbb{P} } \left[  \exp \left( \int^{T-t}_0  \left(  - \sigma W_s \right)  ds \right) | \mathcal{F}_t \right] 
\end{eqnarray} \hat{o} \begin{eqnarray}
d \left( - \sigma  W_s s \right) &=& - \sigma  W_s ds  -  \sigma s d W_s + \frac{1}{2} 0 d [W_s] \\
&=& - \sigma   W_s  ds  -  \sigma s d W_s \\
\int^{T-t}_0 d \left( - \sigma  W_s s \right) &=& \int^{T-t}_0 \left( - \sigma  W_s  \right) ds - \sigma \int^{T-t}_0 s d W_s \\
 - \sigma  W_{T-t}  (T-t)  &=& \int^{T-t}_0 \left( - \sigma  W_s  \right) ds - \sigma \int^{T-t}_0 s d W_s \\
 \int^{T-t}_0 \left( - \sigma  W_s \right) ds  &=& - \sigma  W_{T-t}  (T-t) + \int^{T-t}_0 \sigma s d W_s \\
 &=& - \sigma  (T-t) \int^{T-t}_0 d W_s  + \int^{T-t}_0 \sigma s d W_s \\
 &=& \sigma \int^{T-t}_0 \left( s- (T-t) \right)  d W_s \\
 &=&  \int^{T-t}_0 \left( \sigma \left( s- (T-t) \right) \right)  d W_s \\
\end{eqnarray} \begin{eqnarray}
&&E^{ \mathbb{P} } \left[  \exp \left( - \sigma  \int^T_t  ( W_s -W_t)  ds \right) | \mathcal{F}_t \right] \nonumber \\
&&   \qquad  \qquad \qquad \qquad = E^{ \mathbb{P} } \left[  \exp \left( \int^{T-t}_0 \left(  - \sigma W_s \right) ds \right) | \mathcal{F}_t \right] \\
&&   \qquad  \qquad \qquad \qquad = E^{ \mathbb{P} } \left[  \exp \left( \int^{T-t}_0 \left(  \sigma \left( s - (T-t)\right) \right)  d W_s  \right) | \mathcal{F}_t \right] \\
&&   \qquad  \qquad \qquad \qquad = \exp \left( \frac{ \sigma^2}{2} \int^{T-t}_0 \left( s - (T-t) \right)^2 ds \right) \\
&&   \qquad  \qquad \qquad \qquad = \exp \left( \frac{ \sigma^2}{2}  \left[ \frac{1}{3} \left( s - (T-t) \right)^3 \right]^{T-t}_0 \right) \\
&&   \qquad  \qquad \qquad \qquad = \exp \left( \frac{ \sigma^2}{6}  (T-t)^3  \right) 
\end{eqnarray} F(t, r_t) \begin{eqnarray}
&& F(t, r_t) \\
&&= \exp \left( - r_t (T-t) - \frac{1}{2} \theta (T-t)^2  \right) \nonumber \\
&&   \qquad \qquad \qquad \qquad \qquad \qquad \cdot E^{ \mathbb{P} } \left[  \exp \left( - \sigma  \int^T_t  ( W_s -W_t)  ds \right) | \mathcal{F}_t \right] \\
&&= \exp \left( - r_t (T-t) - \frac{1}{2} \theta (T-t)^2  \right) \cdot \exp \left( \frac{ \sigma^2}{6}  (T-t)^3  \right) \\
&&= \exp \left( - r_t (T-t) - \frac{1}{2} \theta (T-t)^2  + \frac{ \sigma^2}{6}  (T-t)^3  \right) 
\end{eqnarray} \square r_t=x F(t, r_t) \begin{eqnarray}
F(t, x) =  \exp \left( - x (T-t) - \frac{1}{2} \theta (T-t)^2  + \frac{ \sigma^2}{6}  (T-t)^3  \right) 
\end{eqnarray} \begin{eqnarray}
\partial_t F(t, x) &=&  \left(  x  + \theta (T-t)  - \frac{ \sigma^2}{2}  (T-t)^2  \right) F(t, x) \\
\partial_x F(t, x) &=& - (T-t)  F(t, x) \\
\partial_{xx} F(t, x) &=&  (T-t)^2  F(t, x) \\
\end{eqnarray} F(T, x)=1 \begin{eqnarray}
&& -xF(t, x) + \partial_t F(t, x) + \theta \partial_x F(t, x) + \frac{1}{2}  \sigma^2 \partial_{xx} F(t, x) \\
&& \qquad \qquad \qquad \qquad \qquad = -xF(t, x) \\
&& \qquad \qquad \qquad \qquad \qquad \qquad  + \left(  x  + \theta (T-t)  - \frac{ \sigma^2}{2}  (T-t)^2  \right) F(t, x) \\
&& \qquad \qquad \qquad \qquad \qquad \qquad - \theta (T-t)  F(t, x) + \frac{1}{2}  \sigma^2 (T-t)^2  F(t, x) \\
&& \qquad \qquad \qquad \qquad \qquad = 0
\end{eqnarray} \square","['ordinary-differential-equations', 'partial-differential-equations', 'stochastic-processes', 'stochastic-analysis', 'stochastic-pde']"
13,Asymptotics of Mathieu functions,Asymptotics of Mathieu functions,,"Given the Mathieu equation, $\frac{d^2w}{dz^2}+(a-2q\cos 2z)y=0$ , I have the solution \begin{align} y(t)=A_1C(a,q,z)+A_2S(a,q,z)\ , \end{align} where $C(\cdot,\cdot,\cdot)$ and $S(\cdot,\cdot,\cdot)$ are the Mathieu cosine and sine solutions, respectively. Does an expression exist describing the asymptotics of the solution as $z\to\pm\infty$ ? I've tried looking at the Digital Library of Mathematical Functions , but they only have the asymptotic expansions for small and large $q$ . I know that one can let $z\to\pm iz'$ to find the modified Mathieu functions. An asymptotic series exists for this case, but it is only for $\Re z'\to\infty$ , so I don't think it is valid to take the asymptotic form of the modified Mathieu equation and let $z'\to\pm z$ as this would correspond to $\Re z'\to\pm i\infty$ . I've tried looking at academic papers, but they mainly deal with asymptotics of $q$ , rather than $z$ . This makes me wonder, is the asymptotic form of the Mathieu cosine and sine functions known for $z\to\pm\infty$ ?","Given the Mathieu equation, , I have the solution where and are the Mathieu cosine and sine solutions, respectively. Does an expression exist describing the asymptotics of the solution as ? I've tried looking at the Digital Library of Mathematical Functions , but they only have the asymptotic expansions for small and large . I know that one can let to find the modified Mathieu functions. An asymptotic series exists for this case, but it is only for , so I don't think it is valid to take the asymptotic form of the modified Mathieu equation and let as this would correspond to . I've tried looking at academic papers, but they mainly deal with asymptotics of , rather than . This makes me wonder, is the asymptotic form of the Mathieu cosine and sine functions known for ?","\frac{d^2w}{dz^2}+(a-2q\cos 2z)y=0 \begin{align}
y(t)=A_1C(a,q,z)+A_2S(a,q,z)\ ,
\end{align} C(\cdot,\cdot,\cdot) S(\cdot,\cdot,\cdot) z\to\pm\infty q z\to\pm iz' \Re z'\to\infty z'\to\pm z \Re z'\to\pm i\infty q z z\to\pm\infty","['ordinary-differential-equations', 'asymptotics', 'special-functions']"
14,Proving the existence of a negative eigenvalue in a Sturm-Liouville Problem,Proving the existence of a negative eigenvalue in a Sturm-Liouville Problem,,"Does anybody have any good references on the spectral theory of time-independent Schrödinger equations? In particular, I'm looking at an equation of the form with, $$ -y''(x)+q(x)y(x)=\lambda y(x) $$ on $(1,\infty)$ with a potential $q(x)$ that has the properties $q(1)=0$ , $\lim_{x\rightarrow \infty}q(x)=C\in \mathbb{R}_+$ , but in particular $q\not\in L^1(1,\infty)$ . I would like to know under what conditions, I can infer the existence of a negative eigenvalue $\lambda$ . A reference which discusses this situation would be ideal.","Does anybody have any good references on the spectral theory of time-independent Schrödinger equations? In particular, I'm looking at an equation of the form with, on with a potential that has the properties , , but in particular . I would like to know under what conditions, I can infer the existence of a negative eigenvalue . A reference which discusses this situation would be ideal."," -y''(x)+q(x)y(x)=\lambda y(x)  (1,\infty) q(x) q(1)=0 \lim_{x\rightarrow \infty}q(x)=C\in \mathbb{R}_+ q\not\in L^1(1,\infty) \lambda","['ordinary-differential-equations', 'reference-request', 'spectral-theory', 'sturm-liouville']"
15,Find $f(x) = x + \frac{2}{3}x^3 + \frac{2\cdot4}{3\cdot5}x^5 + \frac{2\cdot4\cdot6}{3\cdot5\cdot7}x^7+\cdots$ where $|x|<1$ [duplicate],Find  where  [duplicate],f(x) = x + \frac{2}{3}x^3 + \frac{2\cdot4}{3\cdot5}x^5 + \frac{2\cdot4\cdot6}{3\cdot5\cdot7}x^7+\cdots |x|<1,"This question already has answers here : Evaluate this power series (4 answers) Closed 4 years ago . Find $$f(x) = x + \frac{2}{3}x^3 + \frac{2\cdot4}{3\cdot5}x^5 + \frac{2\cdot4\cdot6}{3\cdot5\cdot7}x^7+\cdots +\infty\,,\quad|x|<1$$ My solution: $$f'(x) = 1 + 2x^2 + \frac{2}{3}\cdot4x^4 + \frac{2\cdot4}{3\cdot5}\cdot6x^6 + \cdots + \infty $$ $$ = 1+x\left(\frac{d}{dx} xf(x)\right) $$ (By observation) $$ = 1+x^2f'(x)+xf(x)$$ $\implies (1-x^2)\frac{dy}{dx} = 1 + xy$ (where $y=f(x), \frac{dy}{dx}=f'(x)$ ) This reduces to a linear first order differential equation, which along with $f(0)=0$ , gives $$f(x) = \frac{\sin^{-1}(x)}{\sqrt{1-x^2}}$$ Is there any other way to solve this question? Preferably along the lines of Taylor series, infinite GP, fubini theorem, etc.?","This question already has answers here : Evaluate this power series (4 answers) Closed 4 years ago . Find My solution: (By observation) (where ) This reduces to a linear first order differential equation, which along with , gives Is there any other way to solve this question? Preferably along the lines of Taylor series, infinite GP, fubini theorem, etc.?","f(x) = x + \frac{2}{3}x^3 + \frac{2\cdot4}{3\cdot5}x^5 + \frac{2\cdot4\cdot6}{3\cdot5\cdot7}x^7+\cdots +\infty\,,\quad|x|<1 f'(x) = 1 + 2x^2 + \frac{2}{3}\cdot4x^4 + \frac{2\cdot4}{3\cdot5}\cdot6x^6 + \cdots + \infty   = 1+x\left(\frac{d}{dx} xf(x)\right)   = 1+x^2f'(x)+xf(x) \implies (1-x^2)\frac{dy}{dx} = 1 + xy y=f(x), \frac{dy}{dx}=f'(x) f(0)=0 f(x) = \frac{\sin^{-1}(x)}{\sqrt{1-x^2}}","['calculus', 'integration', 'ordinary-differential-equations', 'functions', 'taylor-expansion']"
16,Derivative of ODE with respect to parameter,Derivative of ODE with respect to parameter,,"For each value of a parameter $a \in \mathbb{R}$ , let $x(a,t)$ be defined by the ODE $$\frac{dx}{dt}=F(a,x,t)$$ where $F$ is (say) smooth and, for each fixed $a$ , Lipschitz in $x$ . It is well known in this case that $x$ is well-defined and is smooth with respect to $a$ . Question (general): What is known about the derivatives of $x$ with respect to $a$ ? If this question is too general to be helpful, here's a more specific one: Question (specific) : If $F(a,x,t)$ is zero unless $x \in [-B,B]$ for some constant $B$ , does this imply that all the derivatives of $x$ with respect to $a$ are bounded functions of $t$ ? (An answer to either question could be a reference to a book) The obvious thing to do is to write down an ODE satisfied by $y:=\frac{\partial x}{\partial a}$ , which is, I think, $$\frac{dy}{dt}=\frac{\partial F}{\partial a}+\frac{\partial F}{\partial x}y$$ and this could be used to study the first derivative (and similar for the higher ones). But these equations get a little messy for higher derivatives.","For each value of a parameter , let be defined by the ODE where is (say) smooth and, for each fixed , Lipschitz in . It is well known in this case that is well-defined and is smooth with respect to . Question (general): What is known about the derivatives of with respect to ? If this question is too general to be helpful, here's a more specific one: Question (specific) : If is zero unless for some constant , does this imply that all the derivatives of with respect to are bounded functions of ? (An answer to either question could be a reference to a book) The obvious thing to do is to write down an ODE satisfied by , which is, I think, and this could be used to study the first derivative (and similar for the higher ones). But these equations get a little messy for higher derivatives.","a \in \mathbb{R} x(a,t) \frac{dx}{dt}=F(a,x,t) F a x x a x a F(a,x,t) x \in [-B,B] B x a t y:=\frac{\partial x}{\partial a} \frac{dy}{dt}=\frac{\partial F}{\partial a}+\frac{\partial F}{\partial x}y","['ordinary-differential-equations', 'reference-request', 'dynamical-systems']"
17,How can I solve the differential equation $ \frac{\mathrm d^2y}{\mathrm dx^2}(y\frac{\mathrm dx}{\mathrm dy}+x) = -A^2\frac{\mathrm dy}{\mathrm dx} $?,How can I solve the differential equation ?, \frac{\mathrm d^2y}{\mathrm dx^2}(y\frac{\mathrm dx}{\mathrm dy}+x) = -A^2\frac{\mathrm dy}{\mathrm dx} ,"I came across this differential equation while investigating how the concentration of a fluid varies with position $ x $ ( $ A $ is a constant). I tried to solve this by using a substitution, but I was unable to actually solve the equation for y. Wolfram Alpha did not come up with anything useful, either. Does anyone have any idea how to solve this? I would greatly appreciate if anyone could help me with this.","I came across this differential equation while investigating how the concentration of a fluid varies with position ( is a constant). I tried to solve this by using a substitution, but I was unable to actually solve the equation for y. Wolfram Alpha did not come up with anything useful, either. Does anyone have any idea how to solve this? I would greatly appreciate if anyone could help me with this.", x   A ,['ordinary-differential-equations']
18,How to analyse the smallest eigenvalue of this linear ODE?,How to analyse the smallest eigenvalue of this linear ODE?,,"I am trying to solve the eigensystem of a 1st-order linear ODE system in the region $(-\infty,\infty)$ and with Dirichlet boundary condition at the infinities \begin{align} -\mathrm{i} u'(x) +f^*(x) v(x) &= \lambda u(x) \\ f(x)u(x) + \mathrm{i} v'(x) &= \lambda v(x) \end{align} where $f(x)$ is a complex-valued function mainly varying around $x=0$ where its norm decreases from $1$ to $1-\delta$ and goes back and its phase varies from $0$ to $\phi$ and $\delta\leq1,\phi\leq2\pi$ . $f^*(x)$ is its complex conjugate. For example, we can have two forms of $f(x)$ (using tanh or Cauchy distribution) of the similar profile $$ f(x)= \left(1-\delta\frac{\tanh(x/a+1)-\tanh(x/a-1)}{2\tanh{1}} \right) \exp{ \left[\mathrm{i}\phi\frac{\tanh(x/a)}{2} \right]}\\ f(x)= \left(1-\delta\frac{a^2}{x^2+a^2} \right) \exp{\left[\mathrm{i}\phi\frac{\tanh(x/a)}{2} \right]} $$ where $a$ controls the width of the region in which $f(x)$ varies quickly. I tried reducing it to a 2nd-order ODE. But it messes up the eigenstructure $$v''-\frac{f'}{f}v'+(\lambda^2-|f|^2-\mathrm{i}\frac{f'}{f}\lambda)v=0.$$ I have no idea if any of the two cases can be solved analytically. If possible, it would be the best. It is known that the system will have a few (at least one) discrete real eigenvalues in $(-1,1)$ if $\delta,\phi$ are not too small and the eigenfunction is more or less localized around $x=0$ . Outside $(-1,1)$ , there will be a continuous spectrum. I am interested in the eigenvalue $\lambda_0$ closest to $0$ only. If one cannot solve the system, is it possible to (roughly) understand how $\lambda_0(a,\delta,\phi)$ behaves to some extent? Perhaps some variation trend or even more. E.g., $\lambda_0$ monotonically increases with $\delta$ or something like this.","I am trying to solve the eigensystem of a 1st-order linear ODE system in the region and with Dirichlet boundary condition at the infinities where is a complex-valued function mainly varying around where its norm decreases from to and goes back and its phase varies from to and . is its complex conjugate. For example, we can have two forms of (using tanh or Cauchy distribution) of the similar profile where controls the width of the region in which varies quickly. I tried reducing it to a 2nd-order ODE. But it messes up the eigenstructure I have no idea if any of the two cases can be solved analytically. If possible, it would be the best. It is known that the system will have a few (at least one) discrete real eigenvalues in if are not too small and the eigenfunction is more or less localized around . Outside , there will be a continuous spectrum. I am interested in the eigenvalue closest to only. If one cannot solve the system, is it possible to (roughly) understand how behaves to some extent? Perhaps some variation trend or even more. E.g., monotonically increases with or something like this.","(-\infty,\infty) \begin{align}
-\mathrm{i} u'(x) +f^*(x) v(x) &= \lambda u(x) \\
f(x)u(x) + \mathrm{i} v'(x) &= \lambda v(x)
\end{align} f(x) x=0 1 1-\delta 0 \phi \delta\leq1,\phi\leq2\pi f^*(x) f(x) 
f(x)= \left(1-\delta\frac{\tanh(x/a+1)-\tanh(x/a-1)}{2\tanh{1}} \right) \exp{ \left[\mathrm{i}\phi\frac{\tanh(x/a)}{2} \right]}\\
f(x)= \left(1-\delta\frac{a^2}{x^2+a^2} \right) \exp{\left[\mathrm{i}\phi\frac{\tanh(x/a)}{2} \right]}
 a f(x) v''-\frac{f'}{f}v'+(\lambda^2-|f|^2-\mathrm{i}\frac{f'}{f}\lambda)v=0. (-1,1) \delta,\phi x=0 (-1,1) \lambda_0 0 \lambda_0(a,\delta,\phi) \lambda_0 \delta","['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'boundary-value-problem', 'eigenfunctions', 'sturm-liouville']"
19,Proving differential equation has no bounded solutions,Proving differential equation has no bounded solutions,,"How do I prove that the $2^{nd}$ order ODE $$\ddot{x}(t) - \sin(x(t)) = \sin(t)$$ has no solutions $~\{x(0),\dot{x}(0)\} \in \mathbb{R}^2~$ for which $|x(t)| < \frac{\pi}{2} \quad \forall \ t\in[0,\infty]~$ ?",How do I prove that the order ODE has no solutions for which ?,"2^{nd} \ddot{x}(t) - \sin(x(t)) = \sin(t) ~\{x(0),\dot{x}(0)\} \in \mathbb{R}^2~ |x(t)| < \frac{\pi}{2} \quad \forall \ t\in[0,\infty]~","['real-analysis', 'ordinary-differential-equations', 'nonlinear-system']"
20,Separation of Variables $~u_{tt} + 2αu_t = c^2u_{xx}$,Separation of Variables,~u_{tt} + 2αu_t = c^2u_{xx},"Separation of Variables for : $$u_{tt} + 2αu_t = c^2u_{xx}$$ $α$ and $c$ are real positive constants such that $α < cπ$ , and is subject to boundary conditions: $$u(0, t) = u(1, t) = 0$$ The string has an initial displacement $u(x, 0) = f(x)$ and is initially at rest. Question: Use separation of variables to show that the displacement of the string is given by $$u(x, t) = \sum_{n=1}^{\infty} c_n ~e^{-αt} ~\sin(nπx)~ \left\{(γ_n/α)~ \cos(γ_n t) +\sin(γ_n t)\right\}$$ where $$γ_n = \sqrt{c^2n^2π^2-α^2~}$$ and give a formula for $~c_n~$ . ${}$ I'm quite stuck of the second part of the question. So far my working out has been: $1)\quad$ Using separation of variable: Letting $u(x,t) = X(x)T(t)$ Eventually separating the PDE into $~2~$ ODE's: $$X''(x) + λX(X) = 0\qquad \text{and}\qquad T''(t) + 2αT'(t) + c^2λT(t) =0$$ $2)\quad$ Using the boundary conditions I got $~X(0) =0~ $ and $~X(1) = 0~$ . Therefore we have a Sturm-Liouville boundary problem $~X''(x) + λX(X) = 0~$ subject to $~X(0) =0~$ and $~X(1) = 0~$ . I found the eigenvalues to be $~λ_n = n^2π^2~$ with corresponding eigen-function $~X_n(x) = \sin(nπx)~$ . Then the $T(t)$ ODE becomes $$T''(t) + 2αT'(t) + c^2n^2π^2T(t) =0$$ ${}$ Edit $~1~$ : To solve this I found the characteristic equation to be $$r^2 +2αr + c^2n^2π^2=0$$ and by completing the square I got $$(r+α)^2= α^2-c^2n^2π^2$$ since $~α<cπ~$ I got $$ ~r= -α \pm \sqrt{α^2-c^2n^2π^2}$$ Therefore the general solution for $~T~$ is given by $$T_n(t) = A_n ~e^{-αt}~\cos \left(\sqrt{c^2n^2π^2-α^2~}~ t \right) + B_n~e^{-αt}~\sin \left(\sqrt{c^2n^2π^2-α^2~}~ t \right)$$ Am I correct? Would really appreciate some help/corrections from this point on. Thanks. Edit $~2~$ : I found $~T_n'(t)~$ and using the condition $~T'(0)=0~$ I got $$T'(0) = B_n ~\sqrt{c^2}~ \sqrt{n^2}~ π =0$$ Therfore $~B_n = 0~$ and the solution for $$T_n = e^{-αt}~ A_n \cos \left(\sqrt{α^2-c^2n^2π^2~}~t \right)$$ Now to solve the entire problem $~u(x,t)~$ we have to consider a linear combination of the product of the solutions.","Separation of Variables for : and are real positive constants such that , and is subject to boundary conditions: The string has an initial displacement and is initially at rest. Question: Use separation of variables to show that the displacement of the string is given by where and give a formula for . I'm quite stuck of the second part of the question. So far my working out has been: Using separation of variable: Letting Eventually separating the PDE into ODE's: Using the boundary conditions I got and . Therefore we have a Sturm-Liouville boundary problem subject to and . I found the eigenvalues to be with corresponding eigen-function . Then the ODE becomes Edit : To solve this I found the characteristic equation to be and by completing the square I got since I got Therefore the general solution for is given by Am I correct? Would really appreciate some help/corrections from this point on. Thanks. Edit : I found and using the condition I got Therfore and the solution for Now to solve the entire problem we have to consider a linear combination of the product of the solutions.","u_{tt} + 2αu_t = c^2u_{xx} α c α < cπ u(0, t) = u(1, t) = 0 u(x, 0) = f(x) u(x, t) = \sum_{n=1}^{\infty} c_n ~e^{-αt} ~\sin(nπx)~ \left\{(γ_n/α)~ \cos(γ_n t) +\sin(γ_n t)\right\} γ_n = \sqrt{c^2n^2π^2-α^2~} ~c_n~ {} 1)\quad u(x,t) = X(x)T(t) ~2~ X''(x) + λX(X) = 0\qquad \text{and}\qquad T''(t) + 2αT'(t) + c^2λT(t) =0 2)\quad ~X(0) =0~  ~X(1) = 0~ ~X''(x) + λX(X) = 0~ ~X(0) =0~ ~X(1) = 0~ ~λ_n = n^2π^2~ ~X_n(x) = \sin(nπx)~ T(t) T''(t) + 2αT'(t) + c^2n^2π^2T(t) =0 {} ~1~ r^2 +2αr + c^2n^2π^2=0 (r+α)^2= α^2-c^2n^2π^2 ~α<cπ~  ~r= -α \pm \sqrt{α^2-c^2n^2π^2} ~T~ T_n(t) = A_n ~e^{-αt}~\cos \left(\sqrt{c^2n^2π^2-α^2~}~ t \right) + B_n~e^{-αt}~\sin \left(\sqrt{c^2n^2π^2-α^2~}~ t \right) ~2~ ~T_n'(t)~ ~T'(0)=0~ T'(0) = B_n ~\sqrt{c^2}~ \sqrt{n^2}~ π =0 ~B_n = 0~ T_n = e^{-αt}~ A_n \cos \left(\sqrt{α^2-c^2n^2π^2~}~t \right) ~u(x,t)~","['ordinary-differential-equations', 'derivatives', 'partial-differential-equations', 'fourier-transform', 'sturm-liouville']"
21,Picard Lindelöf Theorem with missing domain of $f$,Picard Lindelöf Theorem with missing domain of,f,"In my lecture we introduced the Picard-Lindelöf Theorem the following way. Theorem (Picard-Lindelöf) : Let $f(t,y)$ be continuous on a cylinder $$D = \{(t,y) \in \mathbb{R} \times \mathbb{R}^d | |t-t_0| \leq a, |y-u_0| \leq b\}$$ Let $f$ be bounded and satisfy a Lipschitz condition in its second argument. Then the IVP $$u' = f(t,u)\hspace{15pt}u(t_0) = u_0$$ is uniquely solvable on the interval $I = [t_0 - T, t_0 + T]$ for some $T> 0$ Now I'm looking at the following IVP: $$u'(t) = u(t)^\frac{1}{4}\hspace{5pt}\forall t \geq 0\hspace{30pt}u(0) = 0$$ I'm asked to give to different solutions to the IVP and show that $f(x) = x^\frac{1}{4}$ is Lipschitz-continuous on every interval $[\epsilon, \infty)$ with $\epsilon > 0$ , but not on $[0, \infty)$ . I do understand the exercise and was able to solve it. I should take home, that the existence of multiple solution is possible, since the Lipschitz-condition in the Picard-Lindelöf theorem is not fulfilled. Question: Let's look over the fact that $f$ is not Lipschitz continuous in $0$ . How would I apply the Picard-Lindelöf theorem for an IVP, where $f$ is only defined for $t \geq t_0$ , since I can't define a cylinder $D$ as in the theorem?","In my lecture we introduced the Picard-Lindelöf Theorem the following way. Theorem (Picard-Lindelöf) : Let be continuous on a cylinder Let be bounded and satisfy a Lipschitz condition in its second argument. Then the IVP is uniquely solvable on the interval for some Now I'm looking at the following IVP: I'm asked to give to different solutions to the IVP and show that is Lipschitz-continuous on every interval with , but not on . I do understand the exercise and was able to solve it. I should take home, that the existence of multiple solution is possible, since the Lipschitz-condition in the Picard-Lindelöf theorem is not fulfilled. Question: Let's look over the fact that is not Lipschitz continuous in . How would I apply the Picard-Lindelöf theorem for an IVP, where is only defined for , since I can't define a cylinder as in the theorem?","f(t,y) D = \{(t,y) \in \mathbb{R} \times \mathbb{R}^d | |t-t_0| \leq a, |y-u_0| \leq b\} f u' = f(t,u)\hspace{15pt}u(t_0) = u_0 I = [t_0 - T, t_0 + T] T> 0 u'(t) = u(t)^\frac{1}{4}\hspace{5pt}\forall t \geq 0\hspace{30pt}u(0) = 0 f(x) = x^\frac{1}{4} [\epsilon, \infty) \epsilon > 0 [0, \infty) f 0 f t \geq t_0 D","['ordinary-differential-equations', 'lipschitz-functions', 'initial-value-problems']"
22,determine solution for differential equation,determine solution for differential equation,,"$$(1-t^2)x'-tx+t^2-1=0,\quad t\in(-1,1)$$ I have to determine the solution for this DE. I've tried dividing the equation with $(1-t^2)$ , so it'd look like: $x'-\frac{t}{1-t^2}x=1$ I've tried to first solve it as a homogeneous DE: $\frac{1}{x}dx-\frac{t}{1-t^2}dt=0$ $\ln|x|+\ln(1-t^2)^{\frac{1}{2}}=C_1$ $|x|\cdot(1-t^2)^{\frac{1}{2}}=e^{C_1}$ $x=e^{C_1}(1-t^2)^{-\frac 1{2}}=C(1-t^2)^{-\frac 1{2}}$ But then I'm stuck. I can't figure out what to use this to solve the non-homogeneous form. Any help?","I have to determine the solution for this DE. I've tried dividing the equation with , so it'd look like: I've tried to first solve it as a homogeneous DE: But then I'm stuck. I can't figure out what to use this to solve the non-homogeneous form. Any help?","(1-t^2)x'-tx+t^2-1=0,\quad t\in(-1,1) (1-t^2) x'-\frac{t}{1-t^2}x=1 \frac{1}{x}dx-\frac{t}{1-t^2}dt=0 \ln|x|+\ln(1-t^2)^{\frac{1}{2}}=C_1 |x|\cdot(1-t^2)^{\frac{1}{2}}=e^{C_1} x=e^{C_1}(1-t^2)^{-\frac 1{2}}=C(1-t^2)^{-\frac 1{2}}",['ordinary-differential-equations']
23,"Let ${y(x)}$ be the general solution of $y'(x) = A_{n \times n}(x) y(x)+f (x),$ I want to show the next inequality.",Let  be the general solution of  I want to show the next inequality.,"{y(x)} y'(x) = A_{n \times n}(x) y(x)+f (x),","Let $y(x)$ be the general solution of $y'(x) = A_{n x n}(x) y(x)+f (x)$ , $x \in I$ where $A(x)$ and $f (x)$ have continuous entries over I. Let be $\phi_{x_o}$ the fundamental matrix in $x_o \in I$ . Show that for every $x_o \in I$ , we have $$|y(x)|_n \leq n \|\phi_{x_o}\|\left(|y(x_o)|_n + \int_{x_0}^x\|\phi^{-1}_{x_o}\|\cdot|f(t)|_n\,dt \right).$$ We denote $|v|_n = \sqrt{\sum_n |v_k|^2}$ , where $v=(v_1,v_2,...,v_n)^t$ and the matrix norm $\displaystyle\|A\|=\max_{1\leq i \leq n}\left\{\sum_{j=1}^{n} |a_{ij}|\right\}.$ My attempt: I know that that the general solution of the ODE is given by $$y(x)=W(x)\left(c+ \int_{x_0}^x W^{-1}(s) f(s)\,ds\right)$$ $$|y|_n \leq |W(x)c|_n + \left|W(x)\int_{x_0}^x W^{-1}(s) f(s) \,ds\right|_n$$ then $$|y|_n \leq n\|W(x)\|\,|c|_n + \int_{x_0}^x\|\phi^{-1}_{x_o}\|\cdot|f(t)|_n\, dt $$ But after that step I do not know what to do, someone could help me please? I know the next results: $$|Av|_n \leq n\cdot \|A\|\, |v|_n$$ if $\phi(t):[a,b] \rightarrow M_n(\mathbb{R}) $ and $f:[a,b] \rightarrow \mathbb{R}^n $ have continuous entries then $$\left|\int_a^b \phi(t) f(t) \,dt\right|_n \leq \int_a^b \|\phi(t)\|\, |f(t)|_n\, dt. $$","Let be the general solution of , where and have continuous entries over I. Let be the fundamental matrix in . Show that for every , we have We denote , where and the matrix norm My attempt: I know that that the general solution of the ODE is given by then But after that step I do not know what to do, someone could help me please? I know the next results: if and have continuous entries then","y(x) y'(x) = A_{n x n}(x) y(x)+f (x) x \in I A(x) f (x) \phi_{x_o} x_o \in I x_o \in I |y(x)|_n \leq n \|\phi_{x_o}\|\left(|y(x_o)|_n + \int_{x_0}^x\|\phi^{-1}_{x_o}\|\cdot|f(t)|_n\,dt \right). |v|_n = \sqrt{\sum_n |v_k|^2} v=(v_1,v_2,...,v_n)^t \displaystyle\|A\|=\max_{1\leq i \leq n}\left\{\sum_{j=1}^{n} |a_{ij}|\right\}. y(x)=W(x)\left(c+ \int_{x_0}^x W^{-1}(s) f(s)\,ds\right) |y|_n \leq |W(x)c|_n + \left|W(x)\int_{x_0}^x W^{-1}(s) f(s) \,ds\right|_n |y|_n \leq n\|W(x)\|\,|c|_n + \int_{x_0}^x\|\phi^{-1}_{x_o}\|\cdot|f(t)|_n\, dt  |Av|_n \leq n\cdot \|A\|\, |v|_n \phi(t):[a,b] \rightarrow M_n(\mathbb{R})  f:[a,b] \rightarrow \mathbb{R}^n  \left|\int_a^b \phi(t) f(t) \,dt\right|_n \leq \int_a^b \|\phi(t)\|\, |f(t)|_n\, dt. ","['ordinary-differential-equations', 'matrix-exponential', 'matrix-norms']"
24,Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio?,Is  not a ratio?,\frac{\textrm{d}y}{\textrm{d}x},"In the book Thomas's Calculus (11th edition) it is mentioned (Section 3.8 pg 225) that the derivative $\frac{\textrm{d}y}{\textrm{d}x}$ is not a ratio. Couldn't it be interpreted as a ratio, because according to the formula $\textrm{d}y = f'(x)\textrm{d}x$ we are able to plug in values for $\textrm{d}x$ and calculate a $\textrm{d}y$ (differential). Then if we rearrange we get $\frac{\textrm{d}y}{\textrm{d}x}$ which could be seen as a ratio. I wonder if the author say this because $\mbox{d}x$ is an independent variable, and $\textrm{d}y$ is a dependent variable, for $\frac{\textrm{d}y}{\textrm{d}x}$ to be a ratio both variables need to be independent.. maybe?","In the book Thomas's Calculus (11th edition) it is mentioned (Section 3.8 pg 225) that the derivative is not a ratio. Couldn't it be interpreted as a ratio, because according to the formula we are able to plug in values for and calculate a (differential). Then if we rearrange we get which could be seen as a ratio. I wonder if the author say this because is an independent variable, and is a dependent variable, for to be a ratio both variables need to be independent.. maybe?",\frac{\textrm{d}y}{\textrm{d}x} \textrm{d}y = f'(x)\textrm{d}x \textrm{d}x \textrm{d}y \frac{\textrm{d}y}{\textrm{d}x} \mbox{d}x \textrm{d}y \frac{\textrm{d}y}{\textrm{d}x},"['calculus', 'analysis', 'math-history', 'nonstandard-analysis']"
25,Prove that $x_0$ is Lyapunov unstable,Prove that  is Lyapunov unstable,x_0,"Let $X : U \subset \mathbb R^n \to \mathbb R^n$ be a $C^1$ vector field and $x_0$ a singular point F $(i.e., X(x_0)=0)$ . Let $h : V \to \mathbb R$ be  a $C^1$ map defined on neighborhood $V \subset U$ of $x_0$ such that $h(x_0) = 0 $ and such that $\dot{h}(x) = \frac{d}{dt}h(\varphi(t,x))|_{t=0}>0$ for every $x \in V\setminus\{x_0\}$ (where by $\varphi(t,x)$ we denote the solution for $x'=X(x)$ , $x(0) = x$ ). Suppose that for every neighborhood $W \subset V$ of $x_0$ there is $\tilde x \in W$ such that $h(\tilde x)>0$ . Prove that $x_0$ is Lyapunov unstable. I'll show  what I've done so far: we suppose that $x_0$ is stable. We choose a compact neighborhood $V_0$ of $x_0$ , since it is stable, then there exists an neighborhood $x_0\in V_1\subset V_0$ such that for any $x\in V_1$ we have $\varphi(t,x)\in V_0$ for every $t\geq 0$ . By hypothesis we have $h(x)>0$ and since the positive semi-orbit of $x$ is such that $\varphi(t,x)\in V_0$ and $V_0$ is compact, then $\omega(x)\neq \emptyset$ , compact and it is invariant (ie, $y\in \omega(x) \implies \varphi(t,y)\in \omega(x))$ . Now how to proceed? I've tried to use the fact that $\omega(x)$ is compact and hence $h$ would have a maximum at $\omega(x)$ and that didn't work. Any ideas, any help?","Let be a vector field and a singular point F . Let be  a map defined on neighborhood of such that and such that for every (where by we denote the solution for , ). Suppose that for every neighborhood of there is such that . Prove that is Lyapunov unstable. I'll show  what I've done so far: we suppose that is stable. We choose a compact neighborhood of , since it is stable, then there exists an neighborhood such that for any we have for every . By hypothesis we have and since the positive semi-orbit of is such that and is compact, then , compact and it is invariant (ie, . Now how to proceed? I've tried to use the fact that is compact and hence would have a maximum at and that didn't work. Any ideas, any help?","X : U \subset \mathbb R^n \to \mathbb R^n C^1 x_0 (i.e., X(x_0)=0) h : V \to \mathbb R C^1 V \subset U x_0 h(x_0) = 0  \dot{h}(x) = \frac{d}{dt}h(\varphi(t,x))|_{t=0}>0 x \in V\setminus\{x_0\} \varphi(t,x) x'=X(x) x(0) = x W \subset V x_0 \tilde x \in W h(\tilde x)>0 x_0 x_0 V_0 x_0 x_0\in V_1\subset V_0 x\in V_1 \varphi(t,x)\in V_0 t\geq 0 h(x)>0 x \varphi(t,x)\in V_0 V_0 \omega(x)\neq \emptyset y\in \omega(x) \implies \varphi(t,y)\in \omega(x)) \omega(x) h \omega(x)","['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'lyapunov-functions']"
26,Method to check if a series of one variable is periodic,Method to check if a series of one variable is periodic,,"I have been solving differential equations by series solution method and I wanted to ask if there is a method to check if the series we get is periodic in its variable. In particular, if we get a solution for some ODE as $$y \left( x \right) = \sum\limits_{n = 0}^{\infty} a_n x^n$$ where $a_n$ is governed by some recurrence relation, then can we predict if the solution is Edit:- I have performed some try: If the function $y$ is periodic of period, say $p > 0$ , then it must satisfy $y \left( x \right) = y \left( x + p \right)$ . This, when substituted in series and then comparing the coefficients of equal powers of $x$ , we get the following result for $n \geq 0$ , $$a_n = \sum\limits_{k = 0}^{\infty} \binom{k + n}{n} a_{k + n} p^k$$ If we add another condition of periodicity $y' \left( x \right) = y' \left( x + p \right)$ , we get the same result for $n \geq 1$ . If we just cancel out the first term, we get the following result for every $n \in \mathbb{N} \cup \left\lbrace 0 \right\rbrace$ $$\sum\limits_{k = 1}^{\infty} \binom{k + n}{n} a_{k + n} p^k = 0$$ Now, from here what can we say about the coefficients $a_n$ so that the function $y$ is periodic? Edit:- I have been thinking and the above equation tells us that the series $\sum\limits_{k = 1}^{\infty} \binom{k + n}{n} a_{k + n} p^k$ converges. For that to happen, a necessary condition is that $$\lim\limits_{k \rightarrow \infty} \binom{k + n}{n} a_{k + n} p^k = 0$$ for each $n \in \mathbb{N}$ . So, if the recurrence relation for $a_n$ is known from the series solution method of differential equations, can we find some conditions so that the above limit holds true? In particular, I have the recurrence relation as $a_0, a_1 \in \mathbb{R}$ are arbitrary, $a_2 = 0$ and for $n \geq 1$ we get $$a_{n + 2} = \dfrac{1}{\left( n + 2 \right) \left( n + 1 \right)} \left[ a_n - 2 \sum\limits_{k = 0}^{\left[ \frac{n}{2} \right]} \dfrac{\left( n - 2k \right) a_{n - 2k} \left( -1 \right)^k}{\left( 2k + 1 \right)!} - \sum\limits_{k = 0}^{\left[ \frac{n}{2} \right]} \dfrac{a_{n - 2k} \left( -1 \right)^k}{\left( 2k \right)!} \right]$$ Can we find the rate of convergence of this recurrence relation and then comment on the limit mentioned above?","I have been solving differential equations by series solution method and I wanted to ask if there is a method to check if the series we get is periodic in its variable. In particular, if we get a solution for some ODE as where is governed by some recurrence relation, then can we predict if the solution is Edit:- I have performed some try: If the function is periodic of period, say , then it must satisfy . This, when substituted in series and then comparing the coefficients of equal powers of , we get the following result for , If we add another condition of periodicity , we get the same result for . If we just cancel out the first term, we get the following result for every Now, from here what can we say about the coefficients so that the function is periodic? Edit:- I have been thinking and the above equation tells us that the series converges. For that to happen, a necessary condition is that for each . So, if the recurrence relation for is known from the series solution method of differential equations, can we find some conditions so that the above limit holds true? In particular, I have the recurrence relation as are arbitrary, and for we get Can we find the rate of convergence of this recurrence relation and then comment on the limit mentioned above?","y \left( x \right) = \sum\limits_{n = 0}^{\infty} a_n x^n a_n y p > 0 y \left( x \right) = y \left( x + p \right) x n \geq 0 a_n = \sum\limits_{k = 0}^{\infty} \binom{k + n}{n} a_{k + n} p^k y' \left( x \right) = y' \left( x + p \right) n \geq 1 n \in \mathbb{N} \cup \left\lbrace 0 \right\rbrace \sum\limits_{k = 1}^{\infty} \binom{k + n}{n} a_{k + n} p^k = 0 a_n y \sum\limits_{k = 1}^{\infty} \binom{k + n}{n} a_{k + n} p^k \lim\limits_{k \rightarrow \infty} \binom{k + n}{n} a_{k + n} p^k = 0 n \in \mathbb{N} a_n a_0, a_1 \in \mathbb{R} a_2 = 0 n \geq 1 a_{n + 2} = \dfrac{1}{\left( n + 2 \right) \left( n + 1 \right)} \left[ a_n - 2 \sum\limits_{k = 0}^{\left[ \frac{n}{2} \right]} \dfrac{\left( n - 2k \right) a_{n - 2k} \left( -1 \right)^k}{\left( 2k + 1 \right)!} - \sum\limits_{k = 0}^{\left[ \frac{n}{2} \right]} \dfrac{a_{n - 2k} \left( -1 \right)^k}{\left( 2k \right)!} \right]","['ordinary-differential-equations', 'power-series', 'periodic-functions']"
27,Can you help me solve this 2nd order ode?!,Can you help me solve this 2nd order ode?!,,$$\frac{d^2y}{dx^2}+\frac{1}{x}\frac{dy}{dx}-ye^{-x}=0$$ subjected to $y'(1)=-1$ and $y'(a)=0$ . I was trying to fit it to the general bessel differential equation. But no luck. :(  :(,subjected to and . I was trying to fit it to the general bessel differential equation. But no luck. :(  :(,\frac{d^2y}{dx^2}+\frac{1}{x}\frac{dy}{dx}-ye^{-x}=0 y'(1)=-1 y'(a)=0,"['ordinary-differential-equations', 'partial-differential-equations', 'bessel-functions', 'differential']"
28,Solution of $ty'' +(2t+3)y' +(t+3)y = 3e^{-t}$ via Laplace transform,Solution of  via Laplace transform,ty'' +(2t+3)y' +(t+3)y = 3e^{-t},"A recent question which was put on hold due to lack of context by the OP was the following: Solve the following ODE using Laplace transforms. $$ty'' +(2t+3)y' +(t+3)y = 3e^{-t}, \qquad y(0)=0$$ Putting the equation into the form $$t(y^{\prime\prime}+2y^\prime+y)+3(y^\prime+y)=3e^{-t} $$ which has $y_c=ce^{-t}$ as a solution of its complementary equation immediately suggests $y=Ate^{-t}$ as a particular solution. And this is borne out by substitution, with $A=1$ . Applying the initial condition yields the solution $$ y=te^{-t} $$ So why would the original OP want the equation solved using Laplace transforms? Is there a shorter path using Laplace transforms than the following? Use the fact that $(3e^{-t})^\prime+3e^{-t}=0$ to get the homogeneous equation $$ [t(y^{\prime\prime}+2y^\prime+y)+3(y^\prime+y)]^\prime+[t(y^{\prime\prime}+2y^\prime+y)+3(y^\prime+y)]=0 $$ This simplifies to the homogeneous equation $$  t(y^{\prime\prime\prime}+3y^{\prime\prime}+3y^\prime+y)+4(y^{\prime\prime}+2y^\prime+y)=0 $$ Taking the Laplace transform of this involves quite a bit of tedium which I will spare the reader, but yields the following: \begin{eqnarray}      (s+1)^3Y^\prime+2(s+1)^2Y&=&0\\\      (s+1)^2Y^\prime+2(s+1)Y&=&0\\\      \left[(s+1)^2Y\right]^\prime&=&0\\\      Y&=&\frac{c}{(s+1)^2}\\\      y&=&cte^{-t}      \end{eqnarray} So, with $c=1$ , yielding the same solution found much more easily by inspection.","A recent question which was put on hold due to lack of context by the OP was the following: Solve the following ODE using Laplace transforms. Putting the equation into the form which has as a solution of its complementary equation immediately suggests as a particular solution. And this is borne out by substitution, with . Applying the initial condition yields the solution So why would the original OP want the equation solved using Laplace transforms? Is there a shorter path using Laplace transforms than the following? Use the fact that to get the homogeneous equation This simplifies to the homogeneous equation Taking the Laplace transform of this involves quite a bit of tedium which I will spare the reader, but yields the following: So, with , yielding the same solution found much more easily by inspection.","ty'' +(2t+3)y' +(t+3)y = 3e^{-t}, \qquad y(0)=0 t(y^{\prime\prime}+2y^\prime+y)+3(y^\prime+y)=3e^{-t}  y_c=ce^{-t} y=Ate^{-t} A=1  y=te^{-t}  (3e^{-t})^\prime+3e^{-t}=0  [t(y^{\prime\prime}+2y^\prime+y)+3(y^\prime+y)]^\prime+[t(y^{\prime\prime}+2y^\prime+y)+3(y^\prime+y)]=0   
t(y^{\prime\prime\prime}+3y^{\prime\prime}+3y^\prime+y)+4(y^{\prime\prime}+2y^\prime+y)=0  \begin{eqnarray}  
   (s+1)^3Y^\prime+2(s+1)^2Y&=&0\\\  
   (s+1)^2Y^\prime+2(s+1)Y&=&0\\\  
   \left[(s+1)^2Y\right]^\prime&=&0\\\  
   Y&=&\frac{c}{(s+1)^2}\\\  
   y&=&cte^{-t}  
   \end{eqnarray} c=1","['ordinary-differential-equations', 'laplace-transform']"
29,Solve Riccati equaiton $x'(t) + x^2(t) = \sin(t) + \cos(t)$,Solve Riccati equaiton,x'(t) + x^2(t) = \sin(t) + \cos(t),"Given differential equation is $$x'(t) + x^2(t) = \sin(t) + \cos(t)$$ I was able to notice that this is a Riccati equation, but could not actually solve, since I do not know none of its partial solutions.","Given differential equation is I was able to notice that this is a Riccati equation, but could not actually solve, since I do not know none of its partial solutions.",x'(t) + x^2(t) = \sin(t) + \cos(t),['ordinary-differential-equations']
30,An ODE involving bump functions,An ODE involving bump functions,,"Consider the following initial value problem $$  \begin{cases} \frac{d}{dt} y(t) = \rho(y(t))\\ y(0) = 0 \end{cases} $$ where $\rho(x)$ is a bump function supported near $0$ on $\mathbb{R}^1$ . That is, $\rho(x)$ is a $C^\infty$ function on $\mathbb{R}^1$ such that $\rho(x) \geq 0$ and $\operatorname{supp} (\rho(x) ) \subseteq (a,b)$ for some bounded  open interval $(a,b)$ containing $0$ .  By abstract reasoning, a solution $y(t)$ exists and the image of $y(t)$ is bounded. I'm wondering if it's actually possible to solve $y(t)$ in an exact form.  Say if $\rho(x)$ is instead an strictly increasing function larger than $0$ .  Then the naive dividing- $\rho(y(t))$ -on-both-side method can lead to an somewhat exact expression of $y(t)$ .  However, as in any first ODE course,  this is not what we are supposed to do. So I wonder if there is a similar method to the one for ODE of the form: $$  \begin{cases} \frac{d}{dt} y(t) = f(t)y(t)\\ y(0) = 0 \end{cases} $$","Consider the following initial value problem where is a bump function supported near on . That is, is a function on such that and for some bounded  open interval containing .  By abstract reasoning, a solution exists and the image of is bounded. I'm wondering if it's actually possible to solve in an exact form.  Say if is instead an strictly increasing function larger than .  Then the naive dividing- -on-both-side method can lead to an somewhat exact expression of .  However, as in any first ODE course,  this is not what we are supposed to do. So I wonder if there is a similar method to the one for ODE of the form:"," 
\begin{cases}
\frac{d}{dt} y(t) = \rho(y(t))\\
y(0) = 0
\end{cases}
 \rho(x) 0 \mathbb{R}^1 \rho(x) C^\infty \mathbb{R}^1 \rho(x) \geq 0 \operatorname{supp} (\rho(x) ) \subseteq (a,b) (a,b) 0 y(t) y(t) y(t) \rho(x) 0 \rho(y(t)) y(t)  
\begin{cases}
\frac{d}{dt} y(t) = f(t)y(t)\\
y(0) = 0
\end{cases}
","['ordinary-differential-equations', 'analysis']"
31,"Why in collocation method if $h$ more smaller, the result is bad?","Why in collocation method if  more smaller, the result is bad?",h,"I want to solve BVP numerically \begin{equation} y''+4y'+13y=e^{-2x}, \text{ } 0\leq x\leq 2 \end{equation} with \begin{equation}\label{konba1} y(0)=\dfrac{10}{9} \end{equation} and \begin{equation}\label{konba2} y(2)=e^{-4}\left(\sin(6)+\cos(6)+\dfrac{1}{9}\right). \end{equation} using collocation method. Assuming the solution is \begin{equation}\label{meong} y=\sum\limits_{i=0}^n c_i x^i, \end{equation} we get system of linear equation \begin{eqnarray*} 	\left[ 	\begin{matrix} 		1&0&0\\ 		13&4+13x_1&2+8x_1+13{x_1}^2\\ 		\vdots&\vdots&\vdots\\ 		13&4+13x_{n-1}&2+8x_{n-1}+13{x_{n-1}}^2\\ 		1&2&4 	\end{matrix} 	\right. 	\left. 	\begin{matrix} 		\ldots&0\\ 		\ldots&n(n-1) {x_1}^{n-2}+4n {x_1}^{n-1}+13{x_1}^n\\ 		\ddots&\vdots\\ 		\ldots&n(n-1) {x_{n-1}}^{n-2}+4 n {x_{n-1}}^{n-1}+13 {x_{n-1}}^n\\ 		\ldots&2^n\\ 	\end{matrix} 	\right]\\ 	\begin{bmatrix} 		c_0\\c_1\\\vdots\\c_{n-1}\\c_n 	\end{bmatrix} 	= 	\begin{bmatrix} 		\dfrac{10}{9}\\e^{-2{x_1}}\\\vdots\\e^{-2x_{n-1}}\\e^{-4}\left(\sin(6)+\cos(6)+\dfrac{1}{9}\right) 	\end{bmatrix}. \end{eqnarray*} The matrix is derived like in this reference https://www.slideshare.net/SuddhasheelGhosh/point-collocation-method-used-in-the-solving-of-differential-equations-particularly-in-finite-element-methods I solve it using matlab with this code clear all; clc; h=0.1; x=0:h:2; n=length(x); fprintf('METODE KOLOKASI\n===============\n'); for i=1:n     for j=1:n         if i==1&&j==1             A(i,j)=1;         elseif i==1&&j~=1             A(i,j)=0;         elseif i==n             A(i,j)=2^(j-1);         else             A(i,j)=(j-1)*(j-2)*x(i)^(j-3)+4*(j-1)*x(i)^(j-2)+13*x(i)^(j-1);         end     end end for i=1:n     if i==1         B(i)=10/9;     elseif i==n         B(i)=exp(-4)*(sin(6)+cos(6)+1/9);     else         B(i)=exp(-2*x(i));     end end B=B'; koef=inv(A)*B; fprintf('  i        ti   y num_i    yeks_i     error\n'); for i=1:n     ynum(i)=0;     for in=1:n         ynum(i)=ynum(i)+koef(in)*x(i)^(in-1);     end     yeks(i)=exp(-2*x(i))*(cos(3*x(i))+sin(3*x(i))+1/9);     error(i)=abs(ynum(i)-yeks(i));     fprintf('%3d%10.4f%10.5f%10.5f%10.5f\n',i,x(i),ynum(i),yeks(i),error(i)); end figure(1); plot(x,ynum,'p','markersize',10,'color','b','markerfacecolor','g'); grid on; axis equal; hold on; plot(x,yeks,'-','color','k','linewidth',1.5); title(sprintf('Solusi Numerik dan Solusi Eksak Metode Kolokasi untuk h=%.3f',h)); xlabel('x'); ylabel('y'); legend('Solusi Numerik','Solusi Eksak'); figure(2); plot(x,error,'r-'); grid on; title(sprintf('Error for h=%.3f',h)); For $h=0.1$ , I get And for $h=0.01$ , I get The matlab output is showing Warning: Matrix is close to singular or badly scaled. Results may be inaccurate. RCOND = 8.316723e-75. Why if we make $h$ smaller, then the solution of $y(2)$ is not satisfy the boundary condition? And how to make the numerical solution is satisfy the boundary condition?","I want to solve BVP numerically with and using collocation method. Assuming the solution is we get system of linear equation The matrix is derived like in this reference https://www.slideshare.net/SuddhasheelGhosh/point-collocation-method-used-in-the-solving-of-differential-equations-particularly-in-finite-element-methods I solve it using matlab with this code clear all; clc; h=0.1; x=0:h:2; n=length(x); fprintf('METODE KOLOKASI\n===============\n'); for i=1:n     for j=1:n         if i==1&&j==1             A(i,j)=1;         elseif i==1&&j~=1             A(i,j)=0;         elseif i==n             A(i,j)=2^(j-1);         else             A(i,j)=(j-1)*(j-2)*x(i)^(j-3)+4*(j-1)*x(i)^(j-2)+13*x(i)^(j-1);         end     end end for i=1:n     if i==1         B(i)=10/9;     elseif i==n         B(i)=exp(-4)*(sin(6)+cos(6)+1/9);     else         B(i)=exp(-2*x(i));     end end B=B'; koef=inv(A)*B; fprintf('  i        ti   y num_i    yeks_i     error\n'); for i=1:n     ynum(i)=0;     for in=1:n         ynum(i)=ynum(i)+koef(in)*x(i)^(in-1);     end     yeks(i)=exp(-2*x(i))*(cos(3*x(i))+sin(3*x(i))+1/9);     error(i)=abs(ynum(i)-yeks(i));     fprintf('%3d%10.4f%10.5f%10.5f%10.5f\n',i,x(i),ynum(i),yeks(i),error(i)); end figure(1); plot(x,ynum,'p','markersize',10,'color','b','markerfacecolor','g'); grid on; axis equal; hold on; plot(x,yeks,'-','color','k','linewidth',1.5); title(sprintf('Solusi Numerik dan Solusi Eksak Metode Kolokasi untuk h=%.3f',h)); xlabel('x'); ylabel('y'); legend('Solusi Numerik','Solusi Eksak'); figure(2); plot(x,error,'r-'); grid on; title(sprintf('Error for h=%.3f',h)); For , I get And for , I get The matlab output is showing Warning: Matrix is close to singular or badly scaled. Results may be inaccurate. RCOND = 8.316723e-75. Why if we make smaller, then the solution of is not satisfy the boundary condition? And how to make the numerical solution is satisfy the boundary condition?","\begin{equation}
y''+4y'+13y=e^{-2x}, \text{ } 0\leq x\leq 2
\end{equation} \begin{equation}\label{konba1}
y(0)=\dfrac{10}{9}
\end{equation} \begin{equation}\label{konba2}
y(2)=e^{-4}\left(\sin(6)+\cos(6)+\dfrac{1}{9}\right).
\end{equation} \begin{equation}\label{meong}
y=\sum\limits_{i=0}^n c_i x^i,
\end{equation} \begin{eqnarray*}
	\left[
	\begin{matrix}
		1&0&0\\
		13&4+13x_1&2+8x_1+13{x_1}^2\\
		\vdots&\vdots&\vdots\\
		13&4+13x_{n-1}&2+8x_{n-1}+13{x_{n-1}}^2\\
		1&2&4
	\end{matrix}
	\right.
	\left.
	\begin{matrix}
		\ldots&0\\
		\ldots&n(n-1) {x_1}^{n-2}+4n {x_1}^{n-1}+13{x_1}^n\\
		\ddots&\vdots\\
		\ldots&n(n-1) {x_{n-1}}^{n-2}+4 n {x_{n-1}}^{n-1}+13 {x_{n-1}}^n\\
		\ldots&2^n\\
	\end{matrix}
	\right]\\
	\begin{bmatrix}
		c_0\\c_1\\\vdots\\c_{n-1}\\c_n
	\end{bmatrix}
	=
	\begin{bmatrix}
		\dfrac{10}{9}\\e^{-2{x_1}}\\\vdots\\e^{-2x_{n-1}}\\e^{-4}\left(\sin(6)+\cos(6)+\dfrac{1}{9}\right)
	\end{bmatrix}.
\end{eqnarray*} h=0.1 h=0.01 h y(2)","['ordinary-differential-equations', 'numerical-methods']"
32,"Bifurcation analysis, limit cycle collapses on two symmetric fixed points","Bifurcation analysis, limit cycle collapses on two symmetric fixed points",,"Coming back on the system I already mentioned in another post, this time I am working on some bifurcation analysis of a 2D System. The system is defined by the following equations. I am assuming $\tau_a >1$ to be kept fixed. \begin{equation} \begin{aligned} \dot d_{1} &= - d_1 - e_1 + \varepsilon f(d_1) \\ \tau_a \dot{e}_{1}&=g_a f(d_1) - e_{1}\\ f(d_1) &= \frac{e^{d_1}-1}{e^{d_1}+1} \end{aligned} \label{eq:2D_sync} \end{equation} In particular, one can show that a Hopf bifurcation occurs when $\epsilon>\epsilon^*,\ \epsilon^*=2(1 + \frac{1}{\tau_a})$ . I mentioning it here for the sake of completeness, although it is not the main problem I am trying to solve. In orange are plotted some trajectories of the system forward in time In blue are plotted some trajectories of the system backward in time In red are plotted the nullclines After Hopf bifurcation: By increasing the parameter $\epsilon$ , the size of the limit cycle increases until two fixed points emerges ""from inside"" the limit cycle. In the picture below you can see the intersection of the two nullclines. By further increasing $\epsilon$ the limit cycle collapses on the fixed points that become stable. Zooming on the new stable fixed points. It looks again as an Hopf bifurcation but I did not find anything to classify it properly. Do you have any suggestion on some more rigorous analysis/example? UPDATE Numerically I computed the value of $\epsilon^{o}=13.12$ at which the off-origin equilibria bifurcates. It is a subcritical Hopf bifurcation. In the figure below I plot the system's evolution with $\epsilon=13.23$ . and just before the catastrophe ( $\epsilon=13.246$ ) (the two blue limit cycles also merge, it seems). In the plot below, only two trajectories are plotted (forward in time in orange, backward in time in blue). The initial transient has been removed as well. I would say that the unstable (in blue) and stable (in orange) limit cycles collides and annihilate but I think I need something more rigorous or a similar system to motivate this intuitive consideration.","Coming back on the system I already mentioned in another post, this time I am working on some bifurcation analysis of a 2D System. The system is defined by the following equations. I am assuming to be kept fixed. In particular, one can show that a Hopf bifurcation occurs when . I mentioning it here for the sake of completeness, although it is not the main problem I am trying to solve. In orange are plotted some trajectories of the system forward in time In blue are plotted some trajectories of the system backward in time In red are plotted the nullclines After Hopf bifurcation: By increasing the parameter , the size of the limit cycle increases until two fixed points emerges ""from inside"" the limit cycle. In the picture below you can see the intersection of the two nullclines. By further increasing the limit cycle collapses on the fixed points that become stable. Zooming on the new stable fixed points. It looks again as an Hopf bifurcation but I did not find anything to classify it properly. Do you have any suggestion on some more rigorous analysis/example? UPDATE Numerically I computed the value of at which the off-origin equilibria bifurcates. It is a subcritical Hopf bifurcation. In the figure below I plot the system's evolution with . and just before the catastrophe ( ) (the two blue limit cycles also merge, it seems). In the plot below, only two trajectories are plotted (forward in time in orange, backward in time in blue). The initial transient has been removed as well. I would say that the unstable (in blue) and stable (in orange) limit cycles collides and annihilate but I think I need something more rigorous or a similar system to motivate this intuitive consideration.","\tau_a >1 \begin{equation}
\begin{aligned}
\dot d_{1} &= - d_1 - e_1 + \varepsilon f(d_1) \\
\tau_a \dot{e}_{1}&=g_a f(d_1) - e_{1}\\
f(d_1) &= \frac{e^{d_1}-1}{e^{d_1}+1}
\end{aligned}
\label{eq:2D_sync}
\end{equation} \epsilon>\epsilon^*,\ \epsilon^*=2(1 + \frac{1}{\tau_a}) \epsilon \epsilon \epsilon^{o}=13.12 \epsilon=13.23 \epsilon=13.246","['ordinary-differential-equations', 'dynamical-systems', 'bifurcation', 'limit-cycles', 'catastrophe-theory']"
33,Why does the rotating wave approximation work?,Why does the rotating wave approximation work?,,"Consider two coupled oscillators with position coordinates $X_a$ and $X_b$ . In general, the motion is described by a system of coupled first order linear differential equations: $$ \frac{d}{dt} \begin{pmatrix} a \\ b \\ a^* \\ b^* \end{pmatrix} = -i \begin{bmatrix}   \omega_a & g & 0 & -g \\   g & \omega_b & -g & 0 \\   0 & g & -\omega_a & -g \\   g & 0 & -g & -\omega_b \end{bmatrix} \begin{pmatrix} a \\ b \\ a^* \\ b^* \end{pmatrix} $$ where $a \equiv X_a + i \dot X_a$ and similarly for $b$ . It is common in the analysis of this problem to drop the anti-diagonal terms in the matrix, thus decoupling the upper left and lower right blocks. Dropping the anti-diagonal terms is called the ""rotating wave approximation"" (RWA) and is supposedly good when $g \ll \omega_a, \omega_b$ . Why is the RWA justified? Physically, dropping the anti-diagonal means that the variables $a$ and $b$ which would be purely clockwise-rotating in the uncoupled $(g=0)$ case couple only to each other and not to the counterclockwise-rotating terms $a^*$ and $b^*$ . Perhaps more interestingly I noticed that the matrix can be written in an algebraic form $^{[a]}$ $$ -i \sigma_z \otimes \left(   g \sigma_x + \frac{\Delta}{2} \sigma_z   + \frac{S}{2} \mathbb{I} \right) - i g (\sigma_y \otimes \sigma_x) $$ where $\Delta \equiv (\omega_a - \omega_b) / 2$ and $S \equiv (\omega_a + \omega_b) / 2$ . In this form, the RWA corresponds precisely to dropping the $-ig(\sigma_y \otimes \sigma_x)$ term. I could imagine that this algebraic representation might help explain why the RWA works. Another final observation is that the eigenvalues of the matrix change only sightly when using the RWA in the limit of $g \ll \omega_a, \omega_b$ , which seems to suggest that the RWA is indeed a good approximation. But still, why does it work? $[a]$ The $\sigma$ 's refer to the Pauli matrices . This question is more or less a rewrite of question 467342 from the Physics site. Here I'm trying to draw more specific attention to the question of why the RWA works at all, and to get insight from the mathematics community.","Consider two coupled oscillators with position coordinates and . In general, the motion is described by a system of coupled first order linear differential equations: where and similarly for . It is common in the analysis of this problem to drop the anti-diagonal terms in the matrix, thus decoupling the upper left and lower right blocks. Dropping the anti-diagonal terms is called the ""rotating wave approximation"" (RWA) and is supposedly good when . Why is the RWA justified? Physically, dropping the anti-diagonal means that the variables and which would be purely clockwise-rotating in the uncoupled case couple only to each other and not to the counterclockwise-rotating terms and . Perhaps more interestingly I noticed that the matrix can be written in an algebraic form where and . In this form, the RWA corresponds precisely to dropping the term. I could imagine that this algebraic representation might help explain why the RWA works. Another final observation is that the eigenvalues of the matrix change only sightly when using the RWA in the limit of , which seems to suggest that the RWA is indeed a good approximation. But still, why does it work? The 's refer to the Pauli matrices . This question is more or less a rewrite of question 467342 from the Physics site. Here I'm trying to draw more specific attention to the question of why the RWA works at all, and to get insight from the mathematics community.","X_a X_b 
\frac{d}{dt}
\begin{pmatrix} a \\ b \\ a^* \\ b^* \end{pmatrix}
= -i
\begin{bmatrix}
  \omega_a & g & 0 & -g \\
  g & \omega_b & -g & 0 \\
  0 & g & -\omega_a & -g \\
  g & 0 & -g & -\omega_b
\end{bmatrix}
\begin{pmatrix} a \\ b \\ a^* \\ b^* \end{pmatrix}
 a \equiv X_a + i \dot X_a b g \ll \omega_a, \omega_b a b (g=0) a^* b^* ^{[a]} 
-i \sigma_z \otimes
\left(
  g \sigma_x + \frac{\Delta}{2} \sigma_z
  + \frac{S}{2} \mathbb{I}
\right)
- i g (\sigma_y \otimes \sigma_x)
 \Delta \equiv (\omega_a - \omega_b) / 2 S \equiv (\omega_a + \omega_b) / 2 -ig(\sigma_y \otimes \sigma_x) g \ll \omega_a, \omega_b [a] \sigma","['ordinary-differential-equations', 'dynamical-systems']"
34,What is the operational way of discovering scale invariance of differential equations?,What is the operational way of discovering scale invariance of differential equations?,,"Context The answer here by @Keenan Pepper gives an instance for what it means for an algebraic or trigonometric formula to be scale invariant . For quick reference, I quote his answer here but with a slightly changed notation. He essentially says that The Euclidean distance formula $a^2 + b^2 = c^2$ is scale invariant since $$(\lambda a)^2 + (\lambda b)^2 = \lambda^2(a^2+b^2) = (\lambda c)^2.$$ In other words, the formula retains its form when all the variables are scaled by a constant multiplicative factor $\lambda$ . Question What would it mean for a differential equation to be (not to be) scale invariant? When I say differential equations, I have very simple physics equations in mind such as $$\frac{d^2x}{dt^2}+\omega_0^2x=0,~ (\text{Undamped oscillation})\\ \frac{d^2x}{dt^2}+\gamma\frac{dx}{dt}+\omega_0^2x=0,~(\text{Damped oscillation})\\ \frac{d^2x}{dt^2}+\gamma\frac{dx}{dt}+\omega_0^2x=F_0\cos\omega t~ (\text{Damped, forced oscillation})$$ where $\omega_0,\gamma,\omega,F_0$ are all constants and the variables are $x$ and $t$ . For instance, let us scale $x\to \lambda x$ , and $t\to \lambda t$ so that the LHS becomes $$\frac{d^2(\lambda x)}{d(\lambda t)^2}+\omega_0^2(\lambda x)=\frac{1}{\lambda}\frac{d^2x}{dt^2}+\lambda\omega_0^2x\neq0$$ unless $\lambda=\pm 1$ . So what's the conclusion? The first equation not scale invariant?","Context The answer here by @Keenan Pepper gives an instance for what it means for an algebraic or trigonometric formula to be scale invariant . For quick reference, I quote his answer here but with a slightly changed notation. He essentially says that The Euclidean distance formula is scale invariant since In other words, the formula retains its form when all the variables are scaled by a constant multiplicative factor . Question What would it mean for a differential equation to be (not to be) scale invariant? When I say differential equations, I have very simple physics equations in mind such as where are all constants and the variables are and . For instance, let us scale , and so that the LHS becomes unless . So what's the conclusion? The first equation not scale invariant?","a^2 + b^2 = c^2 (\lambda a)^2 + (\lambda b)^2 = \lambda^2(a^2+b^2) = (\lambda c)^2. \lambda \frac{d^2x}{dt^2}+\omega_0^2x=0,~ (\text{Undamped oscillation})\\
\frac{d^2x}{dt^2}+\gamma\frac{dx}{dt}+\omega_0^2x=0,~(\text{Damped oscillation})\\
\frac{d^2x}{dt^2}+\gamma\frac{dx}{dt}+\omega_0^2x=F_0\cos\omega t~ (\text{Damped, forced oscillation}) \omega_0,\gamma,\omega,F_0 x t x\to \lambda x t\to \lambda t \frac{d^2(\lambda x)}{d(\lambda t)^2}+\omega_0^2(\lambda x)=\frac{1}{\lambda}\frac{d^2x}{dt^2}+\lambda\omega_0^2x\neq0 \lambda=\pm 1","['ordinary-differential-equations', 'physics', 'mathematical-physics', 'symmetry', 'invariance']"
35,Asymptotic expansion of the confluent Heun function,Asymptotic expansion of the confluent Heun function,,"Is the asymptotic expansion of the confluent Heun function known?? The confluent Heun's differential equation is given by \begin{equation}  y''(z) + \left( \epsilon + \frac{\gamma}{z}+ \frac{\delta}{z-1} \right) y'(x) + \left(\frac{\alpha z-q}{z(z-1)} \right) y(z)=0 \end{equation} One of the solution, $Hc(\epsilon,\delta,\gamma,\alpha,q;z)$ , is defined by $$ Hc(\epsilon,\delta,\gamma,\alpha,q;z=0) = 1 ,\\  Hc'(\epsilon,\delta,\gamma,\alpha,q;z=0) = -\frac{q}{\gamma}.$$ I wonder if the asymptotic expansion of this function at $z\to -\infty$ is known. Thank you so much!","Is the asymptotic expansion of the confluent Heun function known?? The confluent Heun's differential equation is given by One of the solution, , is defined by I wonder if the asymptotic expansion of this function at is known. Thank you so much!","\begin{equation}
 y''(z) + \left( \epsilon + \frac{\gamma}{z}+ \frac{\delta}{z-1} \right) y'(x) + \left(\frac{\alpha z-q}{z(z-1)} \right) y(z)=0
\end{equation} Hc(\epsilon,\delta,\gamma,\alpha,q;z)  Hc(\epsilon,\delta,\gamma,\alpha,q;z=0) = 1 ,\\
 Hc'(\epsilon,\delta,\gamma,\alpha,q;z=0) = -\frac{q}{\gamma}. z\to -\infty","['ordinary-differential-equations', 'special-functions']"
36,Solving numerically a strongly stiff nonlinear ODE system with ill-conditioned Jacobian,Solving numerically a strongly stiff nonlinear ODE system with ill-conditioned Jacobian,,"Using Matlab, I am trying to solve numerically the following nonlinear system of ODEs: $$\begin{aligned}   \dot B &= -\alpha B -\nu BV   \\    \dot X &= A-\mu_1 X -c E(B)VX \\   \dot Y &= -\mu_2 Y +c E(B)VX  \\   \dot V &= kY - (\gamma B + E_0)V - \delta V \\   \end{aligned}$$ with $$E(B)=(\gamma B +E_0)e^{-\beta \gamma B}$$ The system tries to capture the dynamics of how the body of an infant reacts to the presence of the dengue virus. $V$ is the number of virus. $B$ the number of antibody. $X$ the number of healthy cells. $Y$ the number of infected cells. $E$ is a function enhancing the effect of antibodies, depending on the number of antibodies. Implementing this model and trying different solver, I noticed that solvers ode15s and ode23s are performing way better than ode45 . Hence, I deduced that my problem was stiff. But with certain set of parameters I had the following warning from Matlab: Warning: Matrix is close to singular or badly scaled. Results may be inaccurate. RCOND = 2.334107e-017. I worried for a while but I found the malicious ill-conditionned matrix. It's actually that solvers ode23s and ode15s use the Jacobian of the system. I have then computed myself this Jacobian and found the following: $$DJ(B,X,Y,V)= \begin{pmatrix} -\alpha - \nu V & 0 & 0 & - \nu B \\ -c V X E'(B) & -c V E(B)-\mu_1 & 0 & -c X E(B) \\ c V X E'(B) & c V E(B) & -\mu_2 & c E(B) X\\ -\gamma V & 0 & k & -(\gamma B+E_0+\delta) \\ \end{pmatrix} $$ with $$ E'(B)=e^{-\beta \gamma B}(\gamma-\beta \gamma^2 B-E_0 \beta \gamma)$$ at this point you have to know a bit more about the dynamics of the system. There is two stable points , one where the virus dies , the antibody disappears , the cells stays healthy $$P_0=(0, \frac{A}{\mu_1},0,0)$$ And another one, depending of the pararameters in a more complex way : $$ P_1 = \left(0,\mu_2 \frac{E_0+\delta}{k c E_0},\bar{Y}, \frac{k}{E_0+\delta} \bar{Y }\right) \text{  with } \bar{Y}= \frac{A}{\mu_2} - \frac{\mu_1(E_0+\delta)}{k c E_0} $$ which means, basically, that some cells become infected and stay infected, the virus population explodes before stabilising, and the antibodies disappear. This second point is the problem because lots of initial conditions seems to be attracted by it and it means that $V$ becomes huge (around $10^{12}$ ) when the total number of cells doesn't exceed $10^{7}$ and the population of antibodies stays below $10^{4}$ . When $V$ becomes huge, I check the Jacobian again, knowing that $\gamma \approx 1$ . We have this one value in the left down corner becoming really huge compared to the other values, also the parameter $c$ is around $10^{-10}$ , so really it's mostly about this left corner term. From there I'm not sure what to do. Is there a way to make a time-varying, well-conditioned Jacobian matrix? Or do you know about any other numerical method that could fit my needs? It doesn't have to be implemented in Matlab. For now, the results I get are mainly garbage.","Using Matlab, I am trying to solve numerically the following nonlinear system of ODEs: with The system tries to capture the dynamics of how the body of an infant reacts to the presence of the dengue virus. is the number of virus. the number of antibody. the number of healthy cells. the number of infected cells. is a function enhancing the effect of antibodies, depending on the number of antibodies. Implementing this model and trying different solver, I noticed that solvers ode15s and ode23s are performing way better than ode45 . Hence, I deduced that my problem was stiff. But with certain set of parameters I had the following warning from Matlab: Warning: Matrix is close to singular or badly scaled. Results may be inaccurate. RCOND = 2.334107e-017. I worried for a while but I found the malicious ill-conditionned matrix. It's actually that solvers ode23s and ode15s use the Jacobian of the system. I have then computed myself this Jacobian and found the following: with at this point you have to know a bit more about the dynamics of the system. There is two stable points , one where the virus dies , the antibody disappears , the cells stays healthy And another one, depending of the pararameters in a more complex way : which means, basically, that some cells become infected and stay infected, the virus population explodes before stabilising, and the antibodies disappear. This second point is the problem because lots of initial conditions seems to be attracted by it and it means that becomes huge (around ) when the total number of cells doesn't exceed and the population of antibodies stays below . When becomes huge, I check the Jacobian again, knowing that . We have this one value in the left down corner becoming really huge compared to the other values, also the parameter is around , so really it's mostly about this left corner term. From there I'm not sure what to do. Is there a way to make a time-varying, well-conditioned Jacobian matrix? Or do you know about any other numerical method that could fit my needs? It doesn't have to be implemented in Matlab. For now, the results I get are mainly garbage.","\begin{aligned}
  \dot B &= -\alpha B -\nu BV   \\ 
  \dot X &= A-\mu_1 X -c E(B)VX \\
  \dot Y &= -\mu_2 Y +c E(B)VX  \\
  \dot V &= kY - (\gamma B + E_0)V - \delta V \\
  \end{aligned} E(B)=(\gamma B +E_0)e^{-\beta \gamma B} V B X Y E DJ(B,X,Y,V)= \begin{pmatrix}
-\alpha - \nu V & 0 & 0 & - \nu B \\
-c V X E'(B) & -c V E(B)-\mu_1 & 0 & -c X E(B) \\
c V X E'(B) & c V E(B) & -\mu_2 & c E(B) X\\
-\gamma V & 0 & k & -(\gamma B+E_0+\delta) \\
\end{pmatrix}
  E'(B)=e^{-\beta \gamma B}(\gamma-\beta \gamma^2 B-E_0 \beta \gamma) P_0=(0, \frac{A}{\mu_1},0,0)  P_1 = \left(0,\mu_2 \frac{E_0+\delta}{k c E_0},\bar{Y}, \frac{k}{E_0+\delta} \bar{Y }\right) \text{  with } \bar{Y}= \frac{A}{\mu_2} - \frac{\mu_1(E_0+\delta)}{k c E_0}  V 10^{12} 10^{7} 10^{4} V \gamma \approx 1 c 10^{-10}","['ordinary-differential-equations', 'numerical-methods', 'matlab', 'nonlinear-system', 'condition-number']"
37,Prove existence of a Heteroclinic Orbit,Prove existence of a Heteroclinic Orbit,,"How would you go around proving that there exists a heteroclinic orbit between two equilibria ( in the problem I'm trying to solve, one is a stable node(say n) and the other is a saddle (say s))? I started by finding the stable manifold of the stable node and the unstable manifold of the saddle point. Next, I want to show that their intersection is not empty and so there must be a trajectory that starts at s and end at n. And therefore, a heteroclinic orbit. But the problem is that the stable manifold only ""covers"" a neighborhood of n. Same for the unstable manifold. So how do we figure out the neighborhoods I guess? Or is there a better way to do this than using the stable and unstable manifolds? Edit: This is the system of ODEs I'm dealing with. $$U' = V\\  V' = -\frac{c}{D}V -\frac{\mu}{D} U (1-U)\\$$ All of the parameters are positive. and $c>\sqrt{4\mu D}$ . So we have two equilibria: A stable node: (U,V) = (0,0). And a saddle point: (U,V) = (1,0). Thanks","How would you go around proving that there exists a heteroclinic orbit between two equilibria ( in the problem I'm trying to solve, one is a stable node(say n) and the other is a saddle (say s))? I started by finding the stable manifold of the stable node and the unstable manifold of the saddle point. Next, I want to show that their intersection is not empty and so there must be a trajectory that starts at s and end at n. And therefore, a heteroclinic orbit. But the problem is that the stable manifold only ""covers"" a neighborhood of n. Same for the unstable manifold. So how do we figure out the neighborhoods I guess? Or is there a better way to do this than using the stable and unstable manifolds? Edit: This is the system of ODEs I'm dealing with. All of the parameters are positive. and . So we have two equilibria: A stable node: (U,V) = (0,0). And a saddle point: (U,V) = (1,0). Thanks","U' = V\\
 V' = -\frac{c}{D}V -\frac{\mu}{D} U (1-U)\\ c>\sqrt{4\mu D}","['ordinary-differential-equations', 'manifolds', 'dynamical-systems', 'stability-in-odes', 'hamilton-equations']"
38,Did Lie Theory manage to unify ordinary differential equations,Did Lie Theory manage to unify ordinary differential equations,,"In Lie Gourp wikipedia entry ( https://en.wikipedia.org/wiki/Lie_group ) it is said: The initial application that Lie had in mind was to the theory of differential equations. On the model of Galois theory and polynomial equations, the driving conception was of a theory capable of unifying, by the study of symmetry, the whole area of ordinary differential equations. However, the hope that Lie Theory would unify the entire field of ordinary differential equations was not fulfilled. Symmetry methods for ODEs continue to be studied, but do not dominate the subject. But did Lie's theory finally manage to unify the entire field of ordinary differential equations? Or maybe some version of it?...","In Lie Gourp wikipedia entry ( https://en.wikipedia.org/wiki/Lie_group ) it is said: The initial application that Lie had in mind was to the theory of differential equations. On the model of Galois theory and polynomial equations, the driving conception was of a theory capable of unifying, by the study of symmetry, the whole area of ordinary differential equations. However, the hope that Lie Theory would unify the entire field of ordinary differential equations was not fulfilled. Symmetry methods for ODEs continue to be studied, but do not dominate the subject. But did Lie's theory finally manage to unify the entire field of ordinary differential equations? Or maybe some version of it?...",,"['ordinary-differential-equations', 'lie-groups', 'lie-algebras', 'unification']"
39,Equations of 1st order but not of 1st degree.,Equations of 1st order but not of 1st degree.,,"Topic : equations solvable for p Q: $(1-y^2+\frac{y^4}{x^2})p^2-\frac{2y}{x}p+\frac{y^2}{x^2}=0$ There is no $\frac{dy}{dx}$ into the equation (if it was, then it has to be replaced by 'p') Now solving the quadratic equation in p Using quadratic formula $\frac{\frac{2y}{x}\pm\frac{4y^2}{x^2}-4(1-y^2+\frac{y^4}{x^2})\frac{y^2}{x^2})}{2(1-y^2+\frac{y^4}{x^2}}$ How to solve this further","Topic : equations solvable for p Q: There is no into the equation (if it was, then it has to be replaced by 'p') Now solving the quadratic equation in p Using quadratic formula How to solve this further",(1-y^2+\frac{y^4}{x^2})p^2-\frac{2y}{x}p+\frac{y^2}{x^2}=0 \frac{dy}{dx} \frac{\frac{2y}{x}\pm\frac{4y^2}{x^2}-4(1-y^2+\frac{y^4}{x^2})\frac{y^2}{x^2})}{2(1-y^2+\frac{y^4}{x^2}},['ordinary-differential-equations']
40,Bifurcation with two parameters,Bifurcation with two parameters,,"Question: Consider the system \begin{align} \frac{dx}{dt} & = y \\ \frac{dy}{dt} & = -(x^2+\mu)y - (x^2+\nu)x \end{align} Conduct Hopf analysis at the parameter values where Hopf bifurcations occur. Attempt: When we only have one parameter, I understand that a bifurcation occurs at the parameter value where the critical point(s) change stability. However, what is the definition of a bifurcation point when there are two or more parameter values? In this particular example, I see that the critical points are at $(0,0)$ and $(\pm \sqrt{-\nu},0)$ , where the latter only exists when $\nu<0$ . Going from $\nu>0$ to $\nu<0$ , the critical point $(0,0)$ goes from being stable to unstable, and we also gain two extra critical points - so is it right to say that we have a bifurcation at $\nu=0$ ?","Question: Consider the system Conduct Hopf analysis at the parameter values where Hopf bifurcations occur. Attempt: When we only have one parameter, I understand that a bifurcation occurs at the parameter value where the critical point(s) change stability. However, what is the definition of a bifurcation point when there are two or more parameter values? In this particular example, I see that the critical points are at and , where the latter only exists when . Going from to , the critical point goes from being stable to unstable, and we also gain two extra critical points - so is it right to say that we have a bifurcation at ?","\begin{align}
\frac{dx}{dt} & = y \\
\frac{dy}{dt} & = -(x^2+\mu)y - (x^2+\nu)x
\end{align} (0,0) (\pm \sqrt{-\nu},0) \nu<0 \nu>0 \nu<0 (0,0) \nu=0","['ordinary-differential-equations', 'dynamical-systems', 'bifurcation']"
41,Hopf Bifurcation Theorem,Hopf Bifurcation Theorem,,"I have a 2D dynamical system of the form \begin{cases} \dot{x}=f(x,y,K) \\[1ex] \dot{y}=g(x,y,K) \end{cases} where $K$ is a free parameter (later I can write the system here). I've found two Hopf bifurcations at approximately $K=0.69$ and $K=0.84$ . In between these two values, there is a clear stable limit cycle, and for $K<0.69$ and $K>0.84$ the oscilations die after some time $t$ . Furthermore, after computing the eigenvalues, I can see that the real part of the eigenvalue is negative for $K<0.69$ and $K>0.84$ , and it is positive for $0.69<K<0.81$ . With all this, I think I can say that there is a supercritical bifurcation at $K=0.69$ because there are stable oscillations for $K>0.69$ . As for $K=0.84$ , I don't think we can say anything unless we go to Hopf theorem in here , and compute the derivative of the real part of the eigenvalue and the $a$ value which I think is the 1st Lyapunov coefficient (although I have no idea how the complicated expression for this coefficient turns out to be equal to this...?) Now, the problem is that I've computed these two things at $K=0.69$ and $K=0.84$ and they are both positive in the two cases. So, according to the theorem this would mean that periodic solutions exist for $K<0.69$ and $K<0.84$ . The second case is correct, but the first is not. Moreover, the theorem states that the fixed point is stable for $K<0.69$ and $K<0.84$ , and unstable for $K>0.84$ , which is clearly not the case as it is easy to see from the signal of the real part of the eigenvalues! So, what's happening here? EDIT: The system is: \begin{cases} \dot{x}=-\frac{8}{3}(-0.99e^{-0.01y}+1)(x^2-x)+Kxy\\[1ex] \dot{y}=\frac{4}{x^{3/2}}\left(-y+1+\frac{3}{2}\frac{y}{x}(x-1)-0.1\frac{x-1}{y+1}-K\right) \end{cases} and the nullclines+vector field for the case $K=0.69$ is: EDIT2: After discussing in the comments with Evgeny , I realized that I was evaluating the derivative of the real part of the egeinvalue incorrectly. Indeed, by simply analysing that it changes from negative to positive at $K=0.69$ and from positive to negative at $K=0.84$ , we can say that its derivative at these points is positive and negative, respectively. Furthermore, because the first Lyapunov coefficient is positive in both cases, we can conclude through Hopf theorem that there is a subcritical bifurcation at $K=0.69$ and a supercritical at $K=0.84$ . So this means that the stable periodic oscillations that I mentioned I can clearly see between $K=0.69$ and $K=0.84$ are due to the supercritical bifucartion at $K=0.84$ , and not the bifurcation at $K=0.69$ . EDIT3: I've just realized that the conclusions I've drawn in EDIT2 are not consistent with Hopf theorem! And so my question still remains: The real part of the eigenvalue is positive for $0.69<K<0.84$ and negative for $K<0.69$ and for $K>0.84$ . For $K<0.69$ I get this situation: which I can't understand if it's a no limit-cycle situation or an unstable situation. For $0.69<K<0.84$ I have this situation: which is clearly a stable limit cycle! Finally for $K>0.84$ I get a situation similar to $K<0.69$ . With all this, it seems that the first Lyapunoc coefficient should be negative! But when I do the calculation I get a positive one!","I have a 2D dynamical system of the form where is a free parameter (later I can write the system here). I've found two Hopf bifurcations at approximately and . In between these two values, there is a clear stable limit cycle, and for and the oscilations die after some time . Furthermore, after computing the eigenvalues, I can see that the real part of the eigenvalue is negative for and , and it is positive for . With all this, I think I can say that there is a supercritical bifurcation at because there are stable oscillations for . As for , I don't think we can say anything unless we go to Hopf theorem in here , and compute the derivative of the real part of the eigenvalue and the value which I think is the 1st Lyapunov coefficient (although I have no idea how the complicated expression for this coefficient turns out to be equal to this...?) Now, the problem is that I've computed these two things at and and they are both positive in the two cases. So, according to the theorem this would mean that periodic solutions exist for and . The second case is correct, but the first is not. Moreover, the theorem states that the fixed point is stable for and , and unstable for , which is clearly not the case as it is easy to see from the signal of the real part of the eigenvalues! So, what's happening here? EDIT: The system is: and the nullclines+vector field for the case is: EDIT2: After discussing in the comments with Evgeny , I realized that I was evaluating the derivative of the real part of the egeinvalue incorrectly. Indeed, by simply analysing that it changes from negative to positive at and from positive to negative at , we can say that its derivative at these points is positive and negative, respectively. Furthermore, because the first Lyapunov coefficient is positive in both cases, we can conclude through Hopf theorem that there is a subcritical bifurcation at and a supercritical at . So this means that the stable periodic oscillations that I mentioned I can clearly see between and are due to the supercritical bifucartion at , and not the bifurcation at . EDIT3: I've just realized that the conclusions I've drawn in EDIT2 are not consistent with Hopf theorem! And so my question still remains: The real part of the eigenvalue is positive for and negative for and for . For I get this situation: which I can't understand if it's a no limit-cycle situation or an unstable situation. For I have this situation: which is clearly a stable limit cycle! Finally for I get a situation similar to . With all this, it seems that the first Lyapunoc coefficient should be negative! But when I do the calculation I get a positive one!","\begin{cases}
\dot{x}=f(x,y,K) \\[1ex]
\dot{y}=g(x,y,K)
\end{cases} K K=0.69 K=0.84 K<0.69 K>0.84 t K<0.69 K>0.84 0.69<K<0.81 K=0.69 K>0.69 K=0.84 a K=0.69 K=0.84 K<0.69 K<0.84 K<0.69 K<0.84 K>0.84 \begin{cases}
\dot{x}=-\frac{8}{3}(-0.99e^{-0.01y}+1)(x^2-x)+Kxy\\[1ex]
\dot{y}=\frac{4}{x^{3/2}}\left(-y+1+\frac{3}{2}\frac{y}{x}(x-1)-0.1\frac{x-1}{y+1}-K\right)
\end{cases} K=0.69 K=0.69 K=0.84 K=0.69 K=0.84 K=0.69 K=0.84 K=0.84 K=0.69 0.69<K<0.84 K<0.69 K>0.84 K<0.69 0.69<K<0.84 K>0.84 K<0.69","['ordinary-differential-equations', 'dynamical-systems', 'bifurcation', 'limit-cycles']"
42,Proving a dynamical system can't have a node or a center.,Proving a dynamical system can't have a node or a center.,,"Given the differential equation $$\begin{cases}x'=P_n(x,y)\\y'=Q_n(x,y)\end{cases}$$ Where $P_n$ and $Q_n$ are homogeneous polynomials of order $n$ , I have shown that if the differential equation has an isolated critical point then it is the origin $(0,0)$ and it is unique. Now, how can I prove that if $n$ is even, then the origin can't either be a center of a node?","Given the differential equation Where and are homogeneous polynomials of order , I have shown that if the differential equation has an isolated critical point then it is the origin and it is unique. Now, how can I prove that if is even, then the origin can't either be a center of a node?","\begin{cases}x'=P_n(x,y)\\y'=Q_n(x,y)\end{cases} P_n Q_n n (0,0) n","['ordinary-differential-equations', 'dynamical-systems']"
43,Does nudging an exact differential equation nudge or destroy the identity integrating factor?,Does nudging an exact differential equation nudge or destroy the identity integrating factor?,,"This question will be related to this one , if for no other reason because a positive answer to the latter would likely help to solve the former. Consider the differential equation $(y)\ dx + (x)\ dy = 0$ .  It is already exact, so we can think of the multiplicative identity, $u(x) = 1$ , as an integrating factor.  We can also observe by separating variables that $u(x) = \frac{1}{xy}$ is an integrating factor, yielding $(\frac{1}{x})\ dx + (\frac{1}{y})\ dy = 0$ .  From here, $u(x) = xy$ allows us to return to the original form, so we can toggle freely between the two, and both produce the same solution, $y = \frac{C}{x}$ . Now instead consider the inexact equation $(y)\ dx + (x + \epsilon\ x)\ dy = 0$ , for some small value of $\epsilon$ .  By separating variables, we find the integrating factor $u(x) = \frac{1}{(x + \epsilon x)y}$ , and the solution $y = \frac{C}{x^{\frac{1}{1 + \epsilon}}}$ , which respectively are very close to $u(x) = \frac{1}{xy}$ and $ = \frac{C}{x}$ from the previous problem.  However, the original inexact form has $\frac{\partial N}{\partial x} - \frac{\partial M}{\partial y} = \epsilon$ , which is very close to $0$ .  Does this imply (not only for this problem, but in general) that an integrating factor very close to $u(x) = 1$ exists, or is the nearby exact form destroyed entirely when we nudge $\frac{\partial N}{\partial x} - \frac{\partial M}{\partial y}$ even by a tiny amount?","This question will be related to this one , if for no other reason because a positive answer to the latter would likely help to solve the former. Consider the differential equation .  It is already exact, so we can think of the multiplicative identity, , as an integrating factor.  We can also observe by separating variables that is an integrating factor, yielding .  From here, allows us to return to the original form, so we can toggle freely between the two, and both produce the same solution, . Now instead consider the inexact equation , for some small value of .  By separating variables, we find the integrating factor , and the solution , which respectively are very close to and from the previous problem.  However, the original inexact form has , which is very close to .  Does this imply (not only for this problem, but in general) that an integrating factor very close to exists, or is the nearby exact form destroyed entirely when we nudge even by a tiny amount?",(y)\ dx + (x)\ dy = 0 u(x) = 1 u(x) = \frac{1}{xy} (\frac{1}{x})\ dx + (\frac{1}{y})\ dy = 0 u(x) = xy y = \frac{C}{x} (y)\ dx + (x + \epsilon\ x)\ dy = 0 \epsilon u(x) = \frac{1}{(x + \epsilon x)y} y = \frac{C}{x^{\frac{1}{1 + \epsilon}}} u(x) = \frac{1}{xy}  = \frac{C}{x} \frac{\partial N}{\partial x} - \frac{\partial M}{\partial y} = \epsilon 0 u(x) = 1 \frac{\partial N}{\partial x} - \frac{\partial M}{\partial y},"['ordinary-differential-equations', 'partial-derivative', 'integrating-factor']"
44,"Is ""ordinary differential equations in more than two variables"" correct? Used in two books","Is ""ordinary differential equations in more than two variables"" correct? Used in two books",,"I have seen the formulation ""ordinary differential equations in more than two variables"" in two books. Is this terminology really correct? The books : The first book is Elements of Partial Differential Equations by Ian N. Sneddon. Chapter 1 is called Ordinary differential equations in more than two variables ( the chapter at Google Books ) : In this chapter we shall discuss the propertires of ordinary differential equations in more than two variables. ... ... if in the rectangular Cartesian coordinates $(x,y,z)$ of a point in three-dimensional space are connected by a single relation of the type $$ f(x,y,z)=0 \tag{1}$$ the point lies on a surface. ... To demonstrate this generally we suppose a point $(x,y,z)$ satisfying equation $(1)$ . Then any increments $(\delta x, \delta y, \delta z)$ in $(x,y,z)$ are related by the equation $$\frac{\partial f}{\partial x}\delta x + \frac{\partial f}{\partial y}\delta y +\frac{\partial f}{\partial z}\delta z=0$$ so that two of them can be chose arbitrarily. The second book is A Treatise on Differential Equations by George Boole and chapter XII is called Ordinary differential equations in more than two variables ( the chapter at Google Books ): The class of equations which we shall first consider in this Chapter, is represented by the typical form, $$ P\, dx + Q\, dy + R\, dz = 0, $$ $P$ , $Q$ and $R$ being functions of the variables $x$ , $y$ , $z$ ; and it is usually termed a total differential equation of the first order with three variables. I'm confused over the terminology of using ""ordinary"" and ""many variables"" in the same sentence.  Isn't a ordinary differential equation always one variable and if more than one variable it's  instead a partial differential equation?","I have seen the formulation ""ordinary differential equations in more than two variables"" in two books. Is this terminology really correct? The books : The first book is Elements of Partial Differential Equations by Ian N. Sneddon. Chapter 1 is called Ordinary differential equations in more than two variables ( the chapter at Google Books ) : In this chapter we shall discuss the propertires of ordinary differential equations in more than two variables. ... ... if in the rectangular Cartesian coordinates of a point in three-dimensional space are connected by a single relation of the type the point lies on a surface. ... To demonstrate this generally we suppose a point satisfying equation . Then any increments in are related by the equation so that two of them can be chose arbitrarily. The second book is A Treatise on Differential Equations by George Boole and chapter XII is called Ordinary differential equations in more than two variables ( the chapter at Google Books ): The class of equations which we shall first consider in this Chapter, is represented by the typical form, , and being functions of the variables , , ; and it is usually termed a total differential equation of the first order with three variables. I'm confused over the terminology of using ""ordinary"" and ""many variables"" in the same sentence.  Isn't a ordinary differential equation always one variable and if more than one variable it's  instead a partial differential equation?","(x,y,z)  f(x,y,z)=0 \tag{1} (x,y,z) (1) (\delta x, \delta y, \delta z) (x,y,z) \frac{\partial f}{\partial x}\delta x + \frac{\partial f}{\partial y}\delta y +\frac{\partial f}{\partial z}\delta z=0 
P\, dx + Q\, dy + R\, dz = 0,
 P Q R x y z","['ordinary-differential-equations', 'partial-differential-equations', 'terminology']"
45,Stability of the following ODE,Stability of the following ODE,,"Consider the following nonautonomous, nonlinear ODE: $$y'(t)=\rho(W(t)y(t)+b(t)),$$ where $y(t),b(t)\in\mathbb{R}^{n},W(t)\in\mathbb{R}^{n,n}$ and $\rho:\mathbb{R}\to\mathbb{R}$ is some nonlinear function that is applied element-wise. A paper I'm reading claims that this ODE is Ljapunov stable if $$\max_{i\in\{1,\cdots,n\}}Re(\lambda_i(J(t)))\leq 0\hspace{6pt}\forall t,$$ where $Re(\cdot)$ denotes the real part and $\lambda_i(J(t))\in\mathbb{R}$ is the $i^{th}$ eigenvalue of the Jacobian $J(t)\in\mathbb{R}^{n,n}$ of the righthand side of the ODE. The paper does neither prove this nor provide a reference to any known theorem proving this, which leaves me with the question: Why is this true? Can anybody point me to a result that shows this? I know similar results for linear, autonomous ODEs with and without small nonlinear perturbations, but I know no such result for the above nonlinear, nonautonomous ODE. For reference, the paper claiming this is this paper in equations (3.5) and (3.6).","Consider the following nonautonomous, nonlinear ODE: where and is some nonlinear function that is applied element-wise. A paper I'm reading claims that this ODE is Ljapunov stable if where denotes the real part and is the eigenvalue of the Jacobian of the righthand side of the ODE. The paper does neither prove this nor provide a reference to any known theorem proving this, which leaves me with the question: Why is this true? Can anybody point me to a result that shows this? I know similar results for linear, autonomous ODEs with and without small nonlinear perturbations, but I know no such result for the above nonlinear, nonautonomous ODE. For reference, the paper claiming this is this paper in equations (3.5) and (3.6).","y'(t)=\rho(W(t)y(t)+b(t)), y(t),b(t)\in\mathbb{R}^{n},W(t)\in\mathbb{R}^{n,n} \rho:\mathbb{R}\to\mathbb{R} \max_{i\in\{1,\cdots,n\}}Re(\lambda_i(J(t)))\leq 0\hspace{6pt}\forall t, Re(\cdot) \lambda_i(J(t))\in\mathbb{R} i^{th} J(t)\in\mathbb{R}^{n,n}","['ordinary-differential-equations', 'stability-in-odes', 'stability-theory']"
46,An ODE with simple condition,An ODE with simple condition,,I'm working about this : Let $x \geq 0$ and $f(x)$ be a twice differentiable and continuous function such that : $$f''(x)^2\leq f'(x)+x^2$$ With $f(0)=0$ and $f'(0)=0$ Prove that $f(x)^2\leq \frac{x^3}{4 \pi^2}(f(x)+\frac{x^3}{3})$ My friend tells me that we can use the Wirtinger's inequality but I don't see how... Can someone help me ? Thanks in advance for your time,I'm working about this : Let and be a twice differentiable and continuous function such that : With and Prove that My friend tells me that we can use the Wirtinger's inequality but I don't see how... Can someone help me ? Thanks in advance for your time,x \geq 0 f(x) f''(x)^2\leq f'(x)+x^2 f(0)=0 f'(0)=0 f(x)^2\leq \frac{x^3}{4 \pi^2}(f(x)+\frac{x^3}{3}),['ordinary-differential-equations']
47,Request for help with solving a tricky Riccati Differential Equation.,Request for help with solving a tricky Riccati Differential Equation.,,"Long story short, I'm working through a model derivation right now and have arrived at a tricky Riccati Differential Equation that I am afraid has pushed me beyond the limits of my talent. The equation is as follows: $$\frac{dE}{dt}=c\left(\frac{1-\exp(ht)}{1-g\exp(ht)}\right)+bE+aE^{2}.$$ It seems to me that this can't really be reduced down to an auxiliary equation form as would be the case if the $c$ term was constant, and I don't see any tricks with substitution that would help my cause. I have a strong feeling that I am missing something here. I would greatly appreciate if someone would please help me with this; and additionally if you can point me in the right direction,  please provide a further reference that I may investigate in case I have a case like this in the future. Thank you! Edit: I have substituted $h$ for $d$ inside the exponentials, in order to avoid confusion with the differential. Also, $a$ , $b$ , $c$ , $g$ , and $h$ are all constants.","Long story short, I'm working through a model derivation right now and have arrived at a tricky Riccati Differential Equation that I am afraid has pushed me beyond the limits of my talent. The equation is as follows: It seems to me that this can't really be reduced down to an auxiliary equation form as would be the case if the term was constant, and I don't see any tricks with substitution that would help my cause. I have a strong feeling that I am missing something here. I would greatly appreciate if someone would please help me with this; and additionally if you can point me in the right direction,  please provide a further reference that I may investigate in case I have a case like this in the future. Thank you! Edit: I have substituted for inside the exponentials, in order to avoid confusion with the differential. Also, , , , , and are all constants.",\frac{dE}{dt}=c\left(\frac{1-\exp(ht)}{1-g\exp(ht)}\right)+bE+aE^{2}. c h d a b c g h,['ordinary-differential-equations']
48,Characterizing a certain kind of bifurcation,Characterizing a certain kind of bifurcation,,"I'm dealing with the following ODE, $$ \left(\frac{dr}{d\lambda}\right)^2 = \left[1 + \frac{(C - 1)M}{(2r - 3M)}\right]\left[1 - \left(1-\frac{2M}{r}\right)\left(\frac{D}{r}\right)^2\right] \equiv f(r;C,D)\,\, , $$ where $$ r \in \mathbb{R^{+}} \quad \textrm{and} \quad C,D,M \in \mathbb{R} \,\, . $$ Taking as bifurcation parameters $C$ and $D$ , I wish to describe the bifurcations ocurring in this system, for this I used the following system to get the bifurcation curve, $$ f(r;C,D) = 0 \,\, , \\ f_{r}(r;C,D) = 0 \,\, ,  $$ Which gives me the following curve, Which suggests to me that there is a kind of cusp and saddle-node bifurcation here, but the conditions for saddle-node fail. On the other hand, the conditions for a cusp bifurcation holds, even though the non degenerancy conditions fail too. My question is, this bifurcation curve tells me that there is a more complex bifurcation happening here, not just a cusp one, which kind of bifurcation could be happening here? Especifically, I would like to know what kind of bifurcations goes in the black points. The $(r,C,D)$ -coordinates of these points considering $M = 1$ are $$ p_{1} = (3, -2, 3\sqrt{3}) \,\, , \\ p_{2} = (3, -2, -3\sqrt{3}) \,\, , \\ p_{3} = (0, 4, 0) \,\, , \\ p_{4} = (-6, 16, -3\sqrt{3}) \,\, , \\ p_{5} = (-6, 16, 3\sqrt{3}) \,\, . $$ Those with $r < 0$ are not in the domain of the system, but I would like to know the bifurcation anyway.","I'm dealing with the following ODE, where Taking as bifurcation parameters and , I wish to describe the bifurcations ocurring in this system, for this I used the following system to get the bifurcation curve, Which gives me the following curve, Which suggests to me that there is a kind of cusp and saddle-node bifurcation here, but the conditions for saddle-node fail. On the other hand, the conditions for a cusp bifurcation holds, even though the non degenerancy conditions fail too. My question is, this bifurcation curve tells me that there is a more complex bifurcation happening here, not just a cusp one, which kind of bifurcation could be happening here? Especifically, I would like to know what kind of bifurcations goes in the black points. The -coordinates of these points considering are Those with are not in the domain of the system, but I would like to know the bifurcation anyway.","
\left(\frac{dr}{d\lambda}\right)^2 = \left[1 + \frac{(C - 1)M}{(2r - 3M)}\right]\left[1 - \left(1-\frac{2M}{r}\right)\left(\frac{D}{r}\right)^2\right] \equiv f(r;C,D)\,\, ,
 
r \in \mathbb{R^{+}} \quad \textrm{and} \quad C,D,M \in \mathbb{R} \,\, .
 C D 
f(r;C,D) = 0 \,\, , \\
f_{r}(r;C,D) = 0 \,\, , 
 (r,C,D) M = 1 
p_{1} = (3, -2, 3\sqrt{3}) \,\, , \\
p_{2} = (3, -2, -3\sqrt{3}) \,\, , \\
p_{3} = (0, 4, 0) \,\, , \\
p_{4} = (-6, 16, -3\sqrt{3}) \,\, , \\
p_{5} = (-6, 16, 3\sqrt{3}) \,\, .
 r < 0","['ordinary-differential-equations', 'dynamical-systems', 'bifurcation']"
49,Solving an ODE system which depends on a periodic function,Solving an ODE system which depends on a periodic function,,"I have the following ODE system and want to show that all solutions $\gamma(t)=(x(t),y(t),z(z))$ exist for all times (or can be extended on all of $\mathbb{R}$ ). The only problem is that the system depends on an unknown function $c:\mathbb{R}\rightarrow (0,\infty)$ , which is smooth, positive and periodic. \begin{align*} \ddot{x}(t)&=\frac{1}{2}\frac{c'(z(t))}{c(z(t))}\dot{y}(t)^2 \\ \ddot{y}(t)&=-\frac{c'(z(t))}{c(z(t))}\dot{y}(t)\dot{z}(t)\\ \ddot{z}(t)&=-\frac{c'(z(t))}{c(z(t))}\dot{z}(t)^2. \end{align*} I can rewrite the last equation to get $c(z)\ddot{z}+c'(z)\dot{z}^2=0$ , so $\frac{d}{dt}(\dot{z}c(z))=0$ , which tells me that $\dot{z(t)}c(z(t))=a_1$ for some constant $a_1\neq 0$ . Solving this gives the implicit expression \begin{align*} a_1 t+a_2=\int_{a_3}^{z(t)}c(\xi)d\xi \end{align*} for $a_2,a_3$ constants. Substituting $\dot{z}=\frac{a_1}{c(z)}$ in the second equation I get $\ddot{y}=-a_1\frac{c'(z)}{c(z)^2}\dot{y}$ which can also be written as $\ddot{y}=\frac{\ddot{z}}{\dot{z}}\dot{y}$ . Now I don't know how to continue. I'm pretty sure one cannot proceed solving this without knowing what the function $c$ is. But since $c$ is periodic (so in particular $c$ and $c'$ are bounded) I have hope that one nevertheless can show that all solutions exist for all time.","I have the following ODE system and want to show that all solutions exist for all times (or can be extended on all of ). The only problem is that the system depends on an unknown function , which is smooth, positive and periodic. I can rewrite the last equation to get , so , which tells me that for some constant . Solving this gives the implicit expression for constants. Substituting in the second equation I get which can also be written as . Now I don't know how to continue. I'm pretty sure one cannot proceed solving this without knowing what the function is. But since is periodic (so in particular and are bounded) I have hope that one nevertheless can show that all solutions exist for all time.","\gamma(t)=(x(t),y(t),z(z)) \mathbb{R} c:\mathbb{R}\rightarrow (0,\infty) \begin{align*}
\ddot{x}(t)&=\frac{1}{2}\frac{c'(z(t))}{c(z(t))}\dot{y}(t)^2 \\
\ddot{y}(t)&=-\frac{c'(z(t))}{c(z(t))}\dot{y}(t)\dot{z}(t)\\
\ddot{z}(t)&=-\frac{c'(z(t))}{c(z(t))}\dot{z}(t)^2.
\end{align*} c(z)\ddot{z}+c'(z)\dot{z}^2=0 \frac{d}{dt}(\dot{z}c(z))=0 \dot{z(t)}c(z(t))=a_1 a_1\neq 0 \begin{align*}
a_1 t+a_2=\int_{a_3}^{z(t)}c(\xi)d\xi
\end{align*} a_2,a_3 \dot{z}=\frac{a_1}{c(z)} \ddot{y}=-a_1\frac{c'(z)}{c(z)^2}\dot{y} \ddot{y}=\frac{\ddot{z}}{\dot{z}}\dot{y} c c c c'","['real-analysis', 'calculus', 'ordinary-differential-equations', 'nonlinear-system']"
50,Solve nonlinear first order ordinary differential equation,Solve nonlinear first order ordinary differential equation,,"Given nonlinear first order ordinary differential equation \begin{eqnarray} \dfrac{dx_1}{dt}&=&4x_1\left(1-\dfrac{x_2}{2}\right)\\ \dfrac{dx_2}{dt}&=&3x_1\left(\dfrac{x_2}{3}-1\right)\\ \end{eqnarray} with initial value $x_1(0)=3$ and $x_2(0)=5$ , $0\leq t\leq 1$ . I can't solve that because the system is nonlinear. I only studied to solve linear system of ODE. How to solve that nonlinear first order ordinary differential equation? \begin{eqnarray}  \dfrac{dx_1}{dt}&=&4x_1\left(1-\dfrac{x_2}{2}\right)\\  \dfrac{dx_2}{dt}&=&3x_1\left(\dfrac{x_2}{3}-1\right)\\  \end{eqnarray} \begin{eqnarray} \dfrac{\dfrac{dx_1}{dt}}{\dfrac{dx_2}{dt}}=\dfrac{4x_1\left(1-\dfrac{x_2}{2}\right)}{3x_1\left(\dfrac{x_2}{3}-1\right)}\\ \dfrac{dx_1}{dx_2}=\dfrac{4\left(\dfrac{2-x_2}{2}\right)}{3\left(\dfrac{x_2-3}{3}\right)}\\ \dfrac{dx_1}{dx_2}=\dfrac{2(2-x_2)}{x_2-3}\\ x_1=\int\dfrac{2(2-x_2)}{x_2-3}dx_2 \end{eqnarray}","Given nonlinear first order ordinary differential equation with initial value and , . I can't solve that because the system is nonlinear. I only studied to solve linear system of ODE. How to solve that nonlinear first order ordinary differential equation?","\begin{eqnarray}
\dfrac{dx_1}{dt}&=&4x_1\left(1-\dfrac{x_2}{2}\right)\\
\dfrac{dx_2}{dt}&=&3x_1\left(\dfrac{x_2}{3}-1\right)\\
\end{eqnarray} x_1(0)=3 x_2(0)=5 0\leq t\leq 1 \begin{eqnarray}
 \dfrac{dx_1}{dt}&=&4x_1\left(1-\dfrac{x_2}{2}\right)\\
 \dfrac{dx_2}{dt}&=&3x_1\left(\dfrac{x_2}{3}-1\right)\\
 \end{eqnarray} \begin{eqnarray}
\dfrac{\dfrac{dx_1}{dt}}{\dfrac{dx_2}{dt}}=\dfrac{4x_1\left(1-\dfrac{x_2}{2}\right)}{3x_1\left(\dfrac{x_2}{3}-1\right)}\\
\dfrac{dx_1}{dx_2}=\dfrac{4\left(\dfrac{2-x_2}{2}\right)}{3\left(\dfrac{x_2-3}{3}\right)}\\
\dfrac{dx_1}{dx_2}=\dfrac{2(2-x_2)}{x_2-3}\\
x_1=\int\dfrac{2(2-x_2)}{x_2-3}dx_2
\end{eqnarray}",['ordinary-differential-equations']
51,What type of equation has a differential equation as its solution?,What type of equation has a differential equation as its solution?,,"In basic algebra, the solutions to equations are numbers. For differential equations, the solutions are families of functions. What type of equation has a differential equation as its solution?","In basic algebra, the solutions to equations are numbers. For differential equations, the solutions are families of functions. What type of equation has a differential equation as its solution?",,['ordinary-differential-equations']
52,The Center Manifold of an ODE,The Center Manifold of an ODE,,"Let $f:\mathbb{R}^n\to\mathbb{R}^n$ be a vector field given by $f_i(x)=\sum_{k=1}^n \sin(x_i-x_k), \forall x\in \mathbb{R}^n $ . Now consider the ODE $\dot x=f(x)$ . We observe that the Jacobian of $f(x)$ is singular (each row sums to zero). The intuition behind this singularity is that the vector field is invariant under shift, i.e., if $f(x)=f(x+a\mathbf{1}^n)$ , where $\mathbf{1}^n$ is a vector of all ones and $a$ is any constant. What can we say about the center manifold of this ODE? Is it true that the center manifold in this case is unique and is equal to the one-dimensional subspace (i.e., the null space of the Jacobian)? Another question is that how can I handle this kind of singularity? I am asking because I cannot apply many theorems (like the inverse function theorem) because of this singularity. Thank you, in advance, for your comments.","Let be a vector field given by . Now consider the ODE . We observe that the Jacobian of is singular (each row sums to zero). The intuition behind this singularity is that the vector field is invariant under shift, i.e., if , where is a vector of all ones and is any constant. What can we say about the center manifold of this ODE? Is it true that the center manifold in this case is unique and is equal to the one-dimensional subspace (i.e., the null space of the Jacobian)? Another question is that how can I handle this kind of singularity? I am asking because I cannot apply many theorems (like the inverse function theorem) because of this singularity. Thank you, in advance, for your comments.","f:\mathbb{R}^n\to\mathbb{R}^n f_i(x)=\sum_{k=1}^n \sin(x_i-x_k), \forall x\in \mathbb{R}^n  \dot x=f(x) f(x) f(x)=f(x+a\mathbf{1}^n) \mathbf{1}^n a","['ordinary-differential-equations', 'differential-geometry', 'manifolds', 'control-theory']"
53,Find path length from ODEs,Find path length from ODEs,,"I am trying to integrate a system of differential equations. Let $p(t)$ be a path in the unit disk in the complex plane with $p(0)=0$ . Write $p(t)=r\exp(i\theta)$ for some $\theta\in[0,2\pi)$ and $r\geq0$ . Suppose $$\frac{d}{dt}p(t)=\exp(i[\theta+\phi(r)])$$ where $r=|p(t)|$ and $\phi(r)=\phi_0(1-r)$ for some constant $\phi_0$ . Also define $\frac{d}{dt}p(0)=1$ . In other words, $p(t)$ is always rotating according to its distance from the origin. I have two questions: Is there an explicit formula for $p(t)$ ? What is path length of $p(t)$ from $t=1$ to $T$ ? Below is a plot of the vector field and $p(t)$ for $\phi_0=-1.197$ .","I am trying to integrate a system of differential equations. Let be a path in the unit disk in the complex plane with . Write for some and . Suppose where and for some constant . Also define . In other words, is always rotating according to its distance from the origin. I have two questions: Is there an explicit formula for ? What is path length of from to ? Below is a plot of the vector field and for .","p(t) p(0)=0 p(t)=r\exp(i\theta) \theta\in[0,2\pi) r\geq0 \frac{d}{dt}p(t)=\exp(i[\theta+\phi(r)]) r=|p(t)| \phi(r)=\phi_0(1-r) \phi_0 \frac{d}{dt}p(0)=1 p(t) p(t) p(t) t=1 T p(t) \phi_0=-1.197",[]
54,Solving differential equations of the form $f'(x)=f(x+1)$ [duplicate],Solving differential equations of the form  [duplicate],f'(x)=f(x+1),"This question already has answers here : Differential equations that are also functional (3 answers) When $f(x+1)-f(x)=f'(x)$, what are the solutions for $f(x)$? (5 answers) What function satisfies $F'(x) = F(2x)$? (1 answer) Closed 5 years ago . How to solve differential equations of the following form: $\frac{df}{dx} = f(x+1)$","This question already has answers here : Differential equations that are also functional (3 answers) When $f(x+1)-f(x)=f'(x)$, what are the solutions for $f(x)$? (5 answers) What function satisfies $F'(x) = F(2x)$? (1 answer) Closed 5 years ago . How to solve differential equations of the following form:",\frac{df}{dx} = f(x+1),['ordinary-differential-equations']
55,Show convergence of inner product of an operator and a weak convergent sequence,Show convergence of inner product of an operator and a weak convergent sequence,,"Let $(u_n)_{n\in\mathbb{N}}\subseteq (W^{1,p}_0(\Omega))$ , $u_n \rightharpoonup u \in (W^{1,p}_0(\Omega))$ and $B$ be an operator on $(W^{1,p}_0(\Omega))\times(W^{1,p}_0(\Omega))$ , where $\Omega \subset \mathbb{R}^d$ is a bounded domain with smooth boundary. B is defined by $$\langle B(w,u), v\rangle := \int_\Omega ||\nabla u - h(w)||^{p-2}(\nabla u - h(w))\nabla v dx,$$ where $h:\mathbb{R}\to\mathbb{R}^n$ is continuous and bounded and $\limsup_{n \to \infty} \langle B(u_n, u_n), u_n - u \rangle \le 0$ holds. I want to show that $\langle B(u_n, v), u_n -u\rangle \to 0$ where $v\in (W^{1,p}_0(\Omega))$ . Is it right, that I just have to show that $B(u_n, v)\in(W^{1,p}_0(\Omega))^*$ $\forall n\in\mathbb{N}$ ? Then the statement follows from the weak continuity of $(u_n)_{n\in\mathbb{N}}$ (?)","Let , and be an operator on , where is a bounded domain with smooth boundary. B is defined by where is continuous and bounded and holds. I want to show that where . Is it right, that I just have to show that ? Then the statement follows from the weak continuity of (?)","(u_n)_{n\in\mathbb{N}}\subseteq (W^{1,p}_0(\Omega)) u_n \rightharpoonup u \in (W^{1,p}_0(\Omega)) B (W^{1,p}_0(\Omega))\times(W^{1,p}_0(\Omega)) \Omega \subset \mathbb{R}^d \langle B(w,u), v\rangle := \int_\Omega ||\nabla u - h(w)||^{p-2}(\nabla u - h(w))\nabla v dx, h:\mathbb{R}\to\mathbb{R}^n \limsup_{n \to \infty} \langle B(u_n, u_n), u_n - u \rangle \le 0 \langle B(u_n, v), u_n -u\rangle \to 0 v\in (W^{1,p}_0(\Omega)) B(u_n, v)\in(W^{1,p}_0(\Omega))^* \forall n\in\mathbb{N} (u_n)_{n\in\mathbb{N}}","['functional-analysis', 'ordinary-differential-equations', 'convergence-divergence', 'sobolev-spaces', 'weak-convergence']"
56,"Solving linear ordinary, 2nd order differential equations via global integral bases.","Solving linear ordinary, 2nd order differential equations via global integral bases.",,"Consider a linear, homogenous  2nd order ODE: \begin{equation} L\left[y(x)\right] = \left[\frac{d^2}{d x^2} + a_1(x) \frac{d}{d x} + a_0(x)\right] y(x)=0 \end{equation} In https://arxiv.org/pdf/1606.01576.pdf the authors present an interesting heuristic algorithm for finding closed form solutions of those ODEs if such solutions exist. The algorithm can be summarized as follows: For all regular singular points of the ODE in question find exponents and power series expansions of solutions to a given order. If the indicial equation is degenerate (which means the second solution cannot be found) find that second solution via the Wronskian method http://mathworld.wolfram.com/Second-OrderOrdinaryDifferentialEquationSecondSolution.html . Note that the solutions in question will be of the form: \begin{equation} y_p(x):= x^{\nu_p} \sum\limits_{i=0}^\infty \left( a_i^{(0)} + a_i^{(1)} \log(x) \right)\cdot  x^i \end{equation} for $p=1,2$ . Having found the solutions in question (meaning of course series expansions being truncated to some given order) find a pair of differential operators $({\mathcal B}_0, {\mathcal B}_1)$ that are globally integral. A precise definition of that notion is given in the paper above and in references therein however for our purposes it amounts to point out the following intuitive meaning. An operator is globally integral if its action on any of the solutions yields a series expansion that comprises nonnegative powers of both $x$ and $\log(x)$ only. It turns out (see the paper for details)  that such operators have the following form: \begin{equation} {\mathcal B}_p= r_0(x) + r_1(x) \cdot \frac{d}{d x} \end{equation} for $p=1,2$ . Choose one of such operators, say ${\mathcal B}_1$ and then construct another differential operator ${\tilde L}$ such that \begin{equation} {\tilde L}\left[{\mathcal B}_1 y_p(x)\right] = 0 \end{equation} Note that this new operator ${\tilde L}$ is also w linear 2nd order operator. Now one would naively expect that this new operator will be more complicated than the original operator $L$ . This is indeed the case in general. However if it just happens that $L$ has closed form solutions in terms of special functions then the new operator will be much simpler and will allow us to find those closed form solutions. Now, I have tested this algorithm on the family of ODEs specified in the first example in my answer to Gauge transformation of differential equations I . That family is a five parameter family . I have therefore generated a random five tuple of such parameters by sampling from rational numbers then for such five tuple I have generated the operator $L$ and then found its global integral basis which I then used to find the new operator $\tilde{L}$ . To my surprise it turns out that the new operator ${\tilde L}$ was always much simpler than the original one and I could guess that it corresponds to an abscissa transformed Gaussian hypergeometric operator. Below I enclose a code snippet that I used: In[929]:= x =.; Dx =.; A =.; B =.; CC =.; order = 4;  AA = RandomInteger[{1, 10}]; {A2, A1, B2} = Rationalize[RandomReal[{-5, 5}, 3], 1/10]; {A, B, CC} = {1/7, 1/3, 1/2}; {L, mysubst} = GenerateODE[A2, A1, B2, A, B, CC, AA] g = GlobalIntegralBasis[L]; g1 = NormalizeAtInfinity[L, g] myStandardForm[L, g1[[1]]]  L00 = ChangeOfVars[{1, (CC - (A + B + 1) x)/(     x (1 - x)), -((A B)/(x (1 - x)))}, (AA x)^2]; Factor[L00/L00[[1]]]   Out[934]= {{1, -((    4 (37632 - 52920 x - 3488051 x^2 - 1014720 x^3 + 28451380 x^4 +        12779520 x^5))/(    21 x (-1 + 4 x) (1 + 4 x) (-3584 + 3360 x + 165415 x^2 +        49920 x^3))), (   2 (10752 - 15120 x - 1066847 x^2 - 577920 x^3 + 6640000 x^4 +       4792320 x^5))/(   3 x^2 (-1 + 4 x) (1 + 4 x) (-3584 + 3360 x + 165415 x^2 +       49920 x^3))},   b^n $34032_ :> (14 b^(-3 + n$ 34032))/195 - (7 b^(-2 + n $34032))/ 104 - (33083 b^(-1 + n$ 34032))/9984 /; n$34032 >= 3}  Out[936]= {{-((    112 ((80 - 15 x - 1792 x^2) f[x] +        48 x (-1 + 16 x^2) Derivative[1][f][x]))/(    x (-3584 + 3360 x + 165415 x^2 + 49920 x^3))), ((1792 + 560 x -        11763 x^2 + 16640 x^3) f[x] +     1792 x (-1 + 16 x^2) Derivative[1][f][x])/(   1792 x^2 (-3584 + 3360 x + 165415 x^2 + 49920 x^3))}, {0, 1}}  Out[937]= {1, (656 x)/(21 (-1 + 4 x) (1 + 4 x)), 64/(  21 (-1 + 4 x) (1 + 4 x))}  Out[939]= {1, (656 x)/(21 (-1 + 4 x) (1 + 4 x)), 64/(  21 (-1 + 4 x) (1 + 4 x))} As I said before for the randomly sampled parameter five tuple the code produces firstly the operator $L$ (Out[934]) then it produces the global integral basis (Out[936]) and finally it produces the new operator ${\tilde L}$ (Out[937]) along with an abscissa transformed Gaussian hypergeometric operator (Out[939]). I have played with this code and it was always that the last two lines of the output were the same, meaning Out[937] was the same as Out[939] . This means that the algorithm is always able to solve the ODE in question provided of course it has a closed form solution which in this cases it always has. I have also run the algorithm on other ODEs which do not have a closed form solution and in there the algo has failed as expected. Now having said all this my question would be as follows. Why does this algorithm work? Could anyone enlighten me why an integral basis should be useful as a means for transforming a complicated ODE into a simple one? Can we prove that this algorithm is going to work if a closed form solution exists?","Consider a linear, homogenous  2nd order ODE: In https://arxiv.org/pdf/1606.01576.pdf the authors present an interesting heuristic algorithm for finding closed form solutions of those ODEs if such solutions exist. The algorithm can be summarized as follows: For all regular singular points of the ODE in question find exponents and power series expansions of solutions to a given order. If the indicial equation is degenerate (which means the second solution cannot be found) find that second solution via the Wronskian method http://mathworld.wolfram.com/Second-OrderOrdinaryDifferentialEquationSecondSolution.html . Note that the solutions in question will be of the form: for . Having found the solutions in question (meaning of course series expansions being truncated to some given order) find a pair of differential operators that are globally integral. A precise definition of that notion is given in the paper above and in references therein however for our purposes it amounts to point out the following intuitive meaning. An operator is globally integral if its action on any of the solutions yields a series expansion that comprises nonnegative powers of both and only. It turns out (see the paper for details)  that such operators have the following form: for . Choose one of such operators, say and then construct another differential operator such that Note that this new operator is also w linear 2nd order operator. Now one would naively expect that this new operator will be more complicated than the original operator . This is indeed the case in general. However if it just happens that has closed form solutions in terms of special functions then the new operator will be much simpler and will allow us to find those closed form solutions. Now, I have tested this algorithm on the family of ODEs specified in the first example in my answer to Gauge transformation of differential equations I . That family is a five parameter family . I have therefore generated a random five tuple of such parameters by sampling from rational numbers then for such five tuple I have generated the operator and then found its global integral basis which I then used to find the new operator . To my surprise it turns out that the new operator was always much simpler than the original one and I could guess that it corresponds to an abscissa transformed Gaussian hypergeometric operator. Below I enclose a code snippet that I used: In[929]:= x =.; Dx =.; A =.; B =.; CC =.; order = 4;  AA = RandomInteger[{1, 10}]; {A2, A1, B2} = Rationalize[RandomReal[{-5, 5}, 3], 1/10]; {A, B, CC} = {1/7, 1/3, 1/2}; {L, mysubst} = GenerateODE[A2, A1, B2, A, B, CC, AA] g = GlobalIntegralBasis[L]; g1 = NormalizeAtInfinity[L, g] myStandardForm[L, g1[[1]]]  L00 = ChangeOfVars[{1, (CC - (A + B + 1) x)/(     x (1 - x)), -((A B)/(x (1 - x)))}, (AA x)^2]; Factor[L00/L00[[1]]]   Out[934]= {{1, -((    4 (37632 - 52920 x - 3488051 x^2 - 1014720 x^3 + 28451380 x^4 +        12779520 x^5))/(    21 x (-1 + 4 x) (1 + 4 x) (-3584 + 3360 x + 165415 x^2 +        49920 x^3))), (   2 (10752 - 15120 x - 1066847 x^2 - 577920 x^3 + 6640000 x^4 +       4792320 x^5))/(   3 x^2 (-1 + 4 x) (1 + 4 x) (-3584 + 3360 x + 165415 x^2 +       49920 x^3))},   b^n 34032))/195 - (7 b^(-2 + n 34032))/9984 /; n$34032 >= 3}  Out[936]= {{-((    112 ((80 - 15 x - 1792 x^2) f[x] +        48 x (-1 + 16 x^2) Derivative[1][f][x]))/(    x (-3584 + 3360 x + 165415 x^2 + 49920 x^3))), ((1792 + 560 x -        11763 x^2 + 16640 x^3) f[x] +     1792 x (-1 + 16 x^2) Derivative[1][f][x])/(   1792 x^2 (-3584 + 3360 x + 165415 x^2 + 49920 x^3))}, {0, 1}}  Out[937]= {1, (656 x)/(21 (-1 + 4 x) (1 + 4 x)), 64/(  21 (-1 + 4 x) (1 + 4 x))}  Out[939]= {1, (656 x)/(21 (-1 + 4 x) (1 + 4 x)), 64/(  21 (-1 + 4 x) (1 + 4 x))} As I said before for the randomly sampled parameter five tuple the code produces firstly the operator (Out[934]) then it produces the global integral basis (Out[936]) and finally it produces the new operator (Out[937]) along with an abscissa transformed Gaussian hypergeometric operator (Out[939]). I have played with this code and it was always that the last two lines of the output were the same, meaning Out[937] was the same as Out[939] . This means that the algorithm is always able to solve the ODE in question provided of course it has a closed form solution which in this cases it always has. I have also run the algorithm on other ODEs which do not have a closed form solution and in there the algo has failed as expected. Now having said all this my question would be as follows. Why does this algorithm work? Could anyone enlighten me why an integral basis should be useful as a means for transforming a complicated ODE into a simple one? Can we prove that this algorithm is going to work if a closed form solution exists?","\begin{equation}
L\left[y(x)\right] = \left[\frac{d^2}{d x^2} + a_1(x) \frac{d}{d x} + a_0(x)\right] y(x)=0
\end{equation} \begin{equation}
y_p(x):= x^{\nu_p} \sum\limits_{i=0}^\infty \left( a_i^{(0)} + a_i^{(1)} \log(x) \right)\cdot  x^i
\end{equation} p=1,2 ({\mathcal B}_0, {\mathcal B}_1) x \log(x) \begin{equation}
{\mathcal B}_p= r_0(x) + r_1(x) \cdot \frac{d}{d x}
\end{equation} p=1,2 {\mathcal B}_1 {\tilde L} \begin{equation}
{\tilde L}\left[{\mathcal B}_1 y_p(x)\right] = 0
\end{equation} {\tilde L} L L L \tilde{L} {\tilde L} 34032_ :> (14 b^(-3 + n 34032))/
104 - (33083 b^(-1 + n L {\tilde L}","['ordinary-differential-equations', 'special-functions', 'closed-form']"
57,Solve the homogeneous differential equation with functional coefficients,Solve the homogeneous differential equation with functional coefficients,,"I have to solve this equation: $$ y''-\frac{1}{x} y' + \frac{1}{x^3} y =0 $$ This is a nonconstant coefficient homogeneous differential equation. How should I do it? I tried with the supstitution: $y(x)=u(x)\sqrt{x}$ , and then I got this equation: $$u''+(\frac{1}{x^3}-\frac{3}{4x^2})u=0$$ , which seems to me easier to solve, but I don't know how to solve this either! If someone could help me, I would be very grateful.","I have to solve this equation: This is a nonconstant coefficient homogeneous differential equation. How should I do it? I tried with the supstitution: , and then I got this equation: , which seems to me easier to solve, but I don't know how to solve this either! If someone could help me, I would be very grateful.", y''-\frac{1}{x} y' + \frac{1}{x^3} y =0  y(x)=u(x)\sqrt{x} u''+(\frac{1}{x^3}-\frac{3}{4x^2})u=0,[]
58,"Let$~${$ f_1 , f_2 , .. , f_n$} be functions lineary indepenedante in $ I$ Prove that every Subset ...",Let{} be functions lineary indepenedante in  Prove that every Subset ...,"~  f_1 , f_2 , .. , f_n  I","Let $~$ { $ f_1 , f_2 , .. , f_n$ } be functions lineary indepenedante in $ I$ Prove that every Subset of the group is also lineary independante in $ I $ ( interval). how do i prove such claim with differential equations ? say with the wronsekian . here is my prof ( might not be right ) : lets suppose that there exist subset such that without the loss of generality { $ f_1 , f_2 , .. , f_k$ } , $k<n$ dependant. so $ \sum_i^k \alpha_i^*f_i = 0$ such that $\alpha_1^* ,\alpha_2^*,\alpha_3^*,...,\alpha_k^* $ not all $0$ ,suppose at least $\alpha_1^* \not= 0$ . so there exist $\alpha_1^* ,\alpha_2^*,\alpha_3^*,...,\alpha_k^*,0,0,0,0,...,0 $ not all $0$ such that : $ \sum_1^n \alpha_if_i = \sum_1^k \alpha_i^*f_i + 0f_{k+1}+0f_{k+2}+...+0f_{n} = 0$ so its lineary dependant but we know that its not. QED","Let { } be functions lineary indepenedante in Prove that every Subset of the group is also lineary independante in ( interval). how do i prove such claim with differential equations ? say with the wronsekian . here is my prof ( might not be right ) : lets suppose that there exist subset such that without the loss of generality { } , dependant. so such that not all ,suppose at least . so there exist not all such that : so its lineary dependant but we know that its not. QED","~  f_1 , f_2 , .. , f_n  I  I   f_1 , f_2 , .. , f_k k<n  \sum_i^k \alpha_i^*f_i = 0 \alpha_1^* ,\alpha_2^*,\alpha_3^*,...,\alpha_k^*  0 \alpha_1^* \not= 0 \alpha_1^* ,\alpha_2^*,\alpha_3^*,...,\alpha_k^*,0,0,0,0,...,0  0  \sum_1^n \alpha_if_i = \sum_1^k \alpha_i^*f_i + 0f_{k+1}+0f_{k+2}+...+0f_{n} = 0","['functional-analysis', 'ordinary-differential-equations']"
59,Definition of the weak derivative involving the mean curvature,Definition of the weak derivative involving the mean curvature,,"Source: https://homepages.warwick.ac.uk/staff/C.M.Elliott/DziEll13a.pdf Definition 2.11 of the weak derivative: A function $f \in L^1(\Gamma)$ has the weak derivative $v_i=D_if \in L^1(\Gamma)$ , $i \in \{1,...,n+1\}$ , if for every function $\phi \in C^1_0(\Gamma)$ we have the relation $$\int_\Gamma f D_i\phi \,dA =- \int_\Gamma\phi v_i \,dA+ \int_\Gamma f\phi H\ n_id\,A$$ where $\Gamma$ is a hypersurface in $\mathbb R^{n+1}$ and $H$ is its mean curvature. Question : I am familiar with the ""standard"" definition of the weak derivative ( https://en.wikipedia.org/wiki/Weak_derivative ). How are these two different defintions to be reconciled and what role does the mean curvature get into the equation? Some explanation/interpretation would be much appreciated.","Source: https://homepages.warwick.ac.uk/staff/C.M.Elliott/DziEll13a.pdf Definition 2.11 of the weak derivative: A function has the weak derivative , , if for every function we have the relation where is a hypersurface in and is its mean curvature. Question : I am familiar with the ""standard"" definition of the weak derivative ( https://en.wikipedia.org/wiki/Weak_derivative ). How are these two different defintions to be reconciled and what role does the mean curvature get into the equation? Some explanation/interpretation would be much appreciated.","f \in L^1(\Gamma) v_i=D_if \in L^1(\Gamma) i \in \{1,...,n+1\} \phi \in C^1_0(\Gamma) \int_\Gamma f D_i\phi \,dA =- \int_\Gamma\phi v_i \,dA+ \int_\Gamma f\phi H\ n_id\,A \Gamma \mathbb R^{n+1} H","['ordinary-differential-equations', 'differential-geometry', 'curvature', 'weak-derivatives']"
60,Show monotonicity of solution of Delayed Differential Equation with respect to a parameter,Show monotonicity of solution of Delayed Differential Equation with respect to a parameter,,"Short Description of the General Question Suppose we have some Delayed Differential Equation (DDE) which depends on a parameter $a$ , $x_a'(t)=f(a,x_a(t),x_a(t-s))$ for some fixed $a$ and $s$ . I would like to prove that $x_a(t)$ is increasing in $a$ . I.e. if we have $a<b$ and $x_a'(t) = f(a,x_a(t),x_a(t-s))$ and $x_b'(t)=f(b,x_b(t),x_b(t-s))$ then $x_a(t) \leq x_b(t)$ for all $t$ . Specific Scenario Consider the following Delayed Differential Equation: \begin{align*} x_a(0) &= a\\ x_a'(t) &= - a (1 - x_a(t)^2) & t \leq 1\\ x_a'(t) &= -a(x_a(t-1)^2 - x_a(t)^2) & t > 1. \end{align*} I have found numerically that for all $a \in (0,1)$ we have: $$ a \leq b \Rightarrow x_a(t) \leq x_b(t), $$ but I am unable to prove this statement. I can solve the ODE in $[0,1]$ exactly, thus on this interval it is easily verified that $x_a(t) \leq x_b(t)$ . I then use that solution to find a solution on $[1,2]$ and so on. But as $t$ grows large we can't find an exact solution anymore.","Short Description of the General Question Suppose we have some Delayed Differential Equation (DDE) which depends on a parameter , for some fixed and . I would like to prove that is increasing in . I.e. if we have and and then for all . Specific Scenario Consider the following Delayed Differential Equation: I have found numerically that for all we have: but I am unable to prove this statement. I can solve the ODE in exactly, thus on this interval it is easily verified that . I then use that solution to find a solution on and so on. But as grows large we can't find an exact solution anymore.","a x_a'(t)=f(a,x_a(t),x_a(t-s)) a s x_a(t) a a<b x_a'(t) = f(a,x_a(t),x_a(t-s)) x_b'(t)=f(b,x_b(t),x_b(t-s)) x_a(t) \leq x_b(t) t \begin{align*}
x_a(0) &= a\\
x_a'(t) &= - a (1 - x_a(t)^2) & t \leq 1\\
x_a'(t) &= -a(x_a(t-1)^2 - x_a(t)^2) & t > 1.
\end{align*} a \in (0,1) 
a \leq b \Rightarrow x_a(t) \leq x_b(t),
 [0,1] x_a(t) \leq x_b(t) [1,2] t","['real-analysis', 'calculus', 'ordinary-differential-equations', 'delay-differential-equations']"
61,Including random variables in differential equations,Including random variables in differential equations,,"I am having trouble finding information regarding the inclusion of random variables in differential equations. For example I have a simple model describing the growth of some population $X$ over time: $$ \dot{X} = X (r - \alpha X) $$ In this deterministic form we can easily solve to get the equilibrial and explicit solutions with regards to time My question comes when I want to express random variations in parameter $\alpha$ , allowing it be represented as a random variable. This means that $X$ also becomes a random variable. Can we derive equations to describe the change in this distribution and its moments over time? Note that here i am not taking about a time varying stochastic process (like brownian walks) but rather static variation in the parameters. Additionally can we include these moments in the differential equation above so that they depend on the moments of the distribution? $$ \dot{X} = X (r - \alpha X - E[X]) $$","I am having trouble finding information regarding the inclusion of random variables in differential equations. For example I have a simple model describing the growth of some population over time: In this deterministic form we can easily solve to get the equilibrial and explicit solutions with regards to time My question comes when I want to express random variations in parameter , allowing it be represented as a random variable. This means that also becomes a random variable. Can we derive equations to describe the change in this distribution and its moments over time? Note that here i am not taking about a time varying stochastic process (like brownian walks) but rather static variation in the parameters. Additionally can we include these moments in the differential equation above so that they depend on the moments of the distribution?","X 
\dot{X} = X (r - \alpha X)
 \alpha X 
\dot{X} = X (r - \alpha X - E[X])
","['calculus', 'probability', 'ordinary-differential-equations', 'mathematical-modeling']"
62,Exercise about the Sturm-Liouville problems,Exercise about the Sturm-Liouville problems,,"Under what condition on the constants $c$ and $c'$ are the boundary conditions $$f(b)=cf(a)$$ and $$f'(b)=c'f'(a)$$ self-adjoint for the operator the operator $$\mathcal{L}f=\frac{d}{dx}\left(p_0(x)\frac{df}{dx}\right)+p_2(x)f$$ on $[a,b]$ ?   ( $p_0(x),p_2(x)$ are reals) I don't know how to impose the boundary conditions to the problem, actually I don't know what to do to find that constants. Using the Lagrange identity I found $$\left\langle f|\mathcal{L}|f\right\rangle=\langle f|\mathcal{L}^{\dagger}|f\rangle+\left[p_0(x)\left(\frac{df}{dx}f^{*}(x)-f(x)\frac{df^{*}}{dx}\right)\right]_{a}^{b}$$ and this condition requires that $$\left[p_0(x)\left(\frac{df}{dx}f^{*}(x)-f(x)\frac{df^{*}}{dx}\right)\right]_{a}^{b}=0$$ so $$p_0(b)\left(\frac{df}{dx}(b)f^{*}(b)-f(b)\frac{df^{*}}{dx}(b)\right)=p_0(a)\left(\frac{df}{dx}(a)f^{*}(a)-f(a)\frac{df^{*}}{dx}(a)\right)\,.$$ The answer is $${cc'}^{*}=\frac{p_0(a)}{p_0(b)}$$ where did I go wrong?","Under what condition on the constants and are the boundary conditions and self-adjoint for the operator the operator on ?   ( are reals) I don't know how to impose the boundary conditions to the problem, actually I don't know what to do to find that constants. Using the Lagrange identity I found and this condition requires that so The answer is where did I go wrong?","c c' f(b)=cf(a) f'(b)=c'f'(a) \mathcal{L}f=\frac{d}{dx}\left(p_0(x)\frac{df}{dx}\right)+p_2(x)f [a,b] p_0(x),p_2(x) \left\langle f|\mathcal{L}|f\right\rangle=\langle f|\mathcal{L}^{\dagger}|f\rangle+\left[p_0(x)\left(\frac{df}{dx}f^{*}(x)-f(x)\frac{df^{*}}{dx}\right)\right]_{a}^{b} \left[p_0(x)\left(\frac{df}{dx}f^{*}(x)-f(x)\frac{df^{*}}{dx}\right)\right]_{a}^{b}=0 p_0(b)\left(\frac{df}{dx}(b)f^{*}(b)-f(b)\frac{df^{*}}{dx}(b)\right)=p_0(a)\left(\frac{df}{dx}(a)f^{*}(a)-f(a)\frac{df^{*}}{dx}(a)\right)\,. {cc'}^{*}=\frac{p_0(a)}{p_0(b)}","['ordinary-differential-equations', 'hilbert-spaces', 'mathematical-physics', 'differential-operators', 'sturm-liouville']"
63,How to calculate the index number for a curve around a linear system's fixed point without integrals?,How to calculate the index number for a curve around a linear system's fixed point without integrals?,,"We know $$ \phi = \tan^{-1} \frac{\dot{y}}{\dot{x}},$$ yet so far I've only been able to calculate the index of curves by using the integral $$ \frac{1}{2\pi} \oint_C \frac{\dot{y}\ddot{x} - \dot{x}\ddot{y}}{\dot{x}^2 + \dot{y}^2} dt. $$ I'm only working with simple $2\times 2$ linear system fixed points, i.e. centers, saddles, stable nodes, etc... While my method works, with $x = \cos t$ , $y = \sin t$ , and $t \in [0,2\pi]$ , I was told very quickly by my prof. that we can also directly calculate it with only the difference of $\phi$ at $t = 0, 2\pi$ . Yet, every parameterization has cancelled out to zero when I calculate arctan: when the index is suppose to 1. It seems like my parameterization will always cancel out since $0 \equiv 2\pi \pmod{2\pi}$ . What am I missing?","We know yet so far I've only been able to calculate the index of curves by using the integral I'm only working with simple linear system fixed points, i.e. centers, saddles, stable nodes, etc... While my method works, with , , and , I was told very quickly by my prof. that we can also directly calculate it with only the difference of at . Yet, every parameterization has cancelled out to zero when I calculate arctan: when the index is suppose to 1. It seems like my parameterization will always cancel out since . What am I missing?"," \phi = \tan^{-1} \frac{\dot{y}}{\dot{x}}, 
\frac{1}{2\pi} \oint_C \frac{\dot{y}\ddot{x} - \dot{x}\ddot{y}}{\dot{x}^2 + \dot{y}^2} dt.
 2\times 2 x = \cos t y = \sin t t \in [0,2\pi] \phi t = 0, 2\pi 0 \equiv 2\pi \pmod{2\pi}","['ordinary-differential-equations', 'winding-number']"
64,Getting Canonical Coordinates in differential equations from Lie Group,Getting Canonical Coordinates in differential equations from Lie Group,,"I've been trying to understand how Lie Groups can help solve differential equations ( this 12-page pdf is the most straightforward explanation I've seen). My understanding is that when a first-order differential equation $\frac{dy}{dx}=h(x,y)$ has a continuous ""translation symmetry"" $(x,y)\mapsto(x,y+\lambda)$ , it becomes very easy to solve the equation using basic integration of one variable. The problem is that most differential equations in standard $(x,y)$ coordinates don't have this nice translation symmetry, and so we need to change to a set of ""canonical coordinates"" $(X,Y)$ which do have the desired translation symmetry $(X,Y)\mapsto(X,Y+\lambda)$ . The main condition that these new canonical coordinates need to satisfy, is that the $Y$ coordinate should be aligned with the $\lambda$ variable ( $\frac{dY}{d\lambda}=1,\frac{dX}{d\lambda}=0$ ), so that we get the nice translation symmetry we want: $$\frac{dY}{d\lambda}=\frac{dY}{dx}\frac{dx}{d\lambda}+\frac{dY}{dy}\frac{dy}{d\lambda}=Y_x\xi+Y_y\eta=1$$ $$\frac{dX}{d\lambda}=\frac{dX}{dx}\frac{dx}{d\lambda}+\frac{dX}{dy}\frac{dy}{d\lambda}=X_x\xi+X_y\eta=0$$ We also require that the symmetry condition be satisfied: $$\frac{dY}{dX}=\frac{D_xY}{D_xX}=\frac{Y_x+Y_y y'}{X_x+X_y y'}=h(X,Y)$$ Here's where I'm stuck. Basically all the terms in the above equations are unknowns, and it seems incredibly difficult to solve for the new coordinates $(X,Y)$ . I've also seen the ""linearized symmetry condition"" below, but it still results in a difficult-to-solve equation: $$\eta_x-\xi_y h^2 +(\eta_y -\xi_x)h - (\xi h_x + \eta h_y) = 0$$ Is there a nice, algorithmic approach for deducing a set of canonical coordinates? Or does it ultimately come down to ""guess and check"" with the above equation?","I've been trying to understand how Lie Groups can help solve differential equations ( this 12-page pdf is the most straightforward explanation I've seen). My understanding is that when a first-order differential equation has a continuous ""translation symmetry"" , it becomes very easy to solve the equation using basic integration of one variable. The problem is that most differential equations in standard coordinates don't have this nice translation symmetry, and so we need to change to a set of ""canonical coordinates"" which do have the desired translation symmetry . The main condition that these new canonical coordinates need to satisfy, is that the coordinate should be aligned with the variable ( ), so that we get the nice translation symmetry we want: We also require that the symmetry condition be satisfied: Here's where I'm stuck. Basically all the terms in the above equations are unknowns, and it seems incredibly difficult to solve for the new coordinates . I've also seen the ""linearized symmetry condition"" below, but it still results in a difficult-to-solve equation: Is there a nice, algorithmic approach for deducing a set of canonical coordinates? Or does it ultimately come down to ""guess and check"" with the above equation?","\frac{dy}{dx}=h(x,y) (x,y)\mapsto(x,y+\lambda) (x,y) (X,Y) (X,Y)\mapsto(X,Y+\lambda) Y \lambda \frac{dY}{d\lambda}=1,\frac{dX}{d\lambda}=0 \frac{dY}{d\lambda}=\frac{dY}{dx}\frac{dx}{d\lambda}+\frac{dY}{dy}\frac{dy}{d\lambda}=Y_x\xi+Y_y\eta=1 \frac{dX}{d\lambda}=\frac{dX}{dx}\frac{dx}{d\lambda}+\frac{dX}{dy}\frac{dy}{d\lambda}=X_x\xi+X_y\eta=0 \frac{dY}{dX}=\frac{D_xY}{D_xX}=\frac{Y_x+Y_y y'}{X_x+X_y y'}=h(X,Y) (X,Y) \eta_x-\xi_y h^2 +(\eta_y -\xi_x)h - (\xi h_x + \eta h_y) = 0","['ordinary-differential-equations', 'lie-groups']"
65,Finding the Green Function,Finding the Green Function,,I'm having some trouble finding the Green function of the following differential equation: $$ \frac{d[x y'(x)]}{dx} = f(x)\\ 0 \leq x \leq 1\\  y(1) = 0 $$ $y(0)$ is finite.,I'm having some trouble finding the Green function of the following differential equation: is finite.,"
\frac{d[x y'(x)]}{dx} = f(x)\\
0 \leq x \leq 1\\ 
y(1) = 0
 y(0)","['ordinary-differential-equations', 'boundary-value-problem', 'greens-function']"
66,Need Help Solving Or Finding The Solution To The Following Darboux System Of Nonlinear Equations.,Need Help Solving Or Finding The Solution To The Following Darboux System Of Nonlinear Equations.,,"I am working on a personal math project of mine and in order for me to continue I need to know the solution to this following system of nonlinear equations I am attaching as a photo. This equation is referenced in this paper https://arxiv.org/pdf/gr-qc/9409025.pdf and the solution is further referenced in F. J. Bureau, Sur des syst emes diff´erentiels du troisi eme ordre et les ´equations diff´erentielles associ´ees, Bulletin de la Classe des Sciences LXXIII (1987) 335–353. My native language is Japanese and I can't read a word of french to save my life. In addition I can't even find this book or paper. I looked using two college libraries and Google Scholars. I would rather not need to duplicate effort and re derive the solution of these equations in terms of Hermite modular elliptic functions. If anyone who has worked with these equations before has reference with an English solution that would be nice. Or if anyone knows where I can find the french reference that would help also. If non can be found then I would appreciate any hints on how to re derive the solution. My project did not intend to run into such formidable looking differential equation, it is a bit outside of my specialty of geometry. Any guidance will be appreciated thanks!","I am working on a personal math project of mine and in order for me to continue I need to know the solution to this following system of nonlinear equations I am attaching as a photo. This equation is referenced in this paper https://arxiv.org/pdf/gr-qc/9409025.pdf and the solution is further referenced in F. J. Bureau, Sur des syst emes diff´erentiels du troisi eme ordre et les ´equations diff´erentielles associ´ees, Bulletin de la Classe des Sciences LXXIII (1987) 335–353. My native language is Japanese and I can't read a word of french to save my life. In addition I can't even find this book or paper. I looked using two college libraries and Google Scholars. I would rather not need to duplicate effort and re derive the solution of these equations in terms of Hermite modular elliptic functions. If anyone who has worked with these equations before has reference with an English solution that would be nice. Or if anyone knows where I can find the french reference that would help also. If non can be found then I would appreciate any hints on how to re derive the solution. My project did not intend to run into such formidable looking differential equation, it is a bit outside of my specialty of geometry. Any guidance will be appreciated thanks!",,"['ordinary-differential-equations', 'systems-of-equations', 'riemann-sum']"
67,Deducing the nature of critical points from implicit solution,Deducing the nature of critical points from implicit solution,,"I have the system of ODE $$x' = x(2y-1) \\ y' = y - x^2 -y^2$$ I am interested in the behavior around the critical point $(-1/2,1/2)$ . The linearization has pure imaginary eigenvalues so I would like to know if $(-1/2,1/2)$ is a spiral point or a center. First I noticed that $$\frac{dy}{dx} = \frac{y - x^2 - y^2}{-x+2xy}$$ or rewriting it, $$x^2 + y^2 - y + (-x+2xy)\frac{dy}{dx}=0$$ is an exact ODE and has solutions given implicitly by $$-xy + xy^2 + \frac{x^3}{3} = c$$ I would think that since I have now solved the ODE I should be able to determine the precise nature of the critical point $(-1/2,1/2)$ but I am at a loss of what to do. Any hints would be much appreciated. I know that I could try to use a Lyapunov function but this seems like it would be overkill since I already solved the system.","I have the system of ODE I am interested in the behavior around the critical point . The linearization has pure imaginary eigenvalues so I would like to know if is a spiral point or a center. First I noticed that or rewriting it, is an exact ODE and has solutions given implicitly by I would think that since I have now solved the ODE I should be able to determine the precise nature of the critical point but I am at a loss of what to do. Any hints would be much appreciated. I know that I could try to use a Lyapunov function but this seems like it would be overkill since I already solved the system.","x' = x(2y-1) \\ y' = y - x^2 -y^2 (-1/2,1/2) (-1/2,1/2) \frac{dy}{dx} = \frac{y - x^2 - y^2}{-x+2xy} x^2 + y^2 - y + (-x+2xy)\frac{dy}{dx}=0 -xy + xy^2 + \frac{x^3}{3} = c (-1/2,1/2)",['ordinary-differential-equations']
68,Asymptotic solution of Airy's equation using WKB,Asymptotic solution of Airy's equation using WKB,,Obtain an asymptotic representation of the solution of Airy's equation $$\frac{d^2y}{dz^2}-zy=0$$ for $|z|$ large using the change of variable $z=\epsilon^{-2/3}\xi$ . The change of variable leads to an equation $$\frac{d^2y}{d\xi^2}-\epsilon^{-2}\xi y=0.$$ Setting $K=\epsilon^{-2}$ and $q(\xi)=-\xi$ the equation is in WKB form with solution $$y=A_+q(\xi)^{-1/4} e^{iK\int \sqrt{q(\xi)}d\xi}+A_-q(\xi)^{-1/4} e^{-iK\int \sqrt{q(\xi)}d\xi} \\ =A_+(-\xi)^{-1/4} e^{i\epsilon^{-2}\frac23(-\xi)^{3/2}}+A_-(-\xi)^{-1/4} e^{-i\epsilon^{-2}\frac23(-\xi)^{3/2}}$$ My problem is how to get the solution into a form which involve only $z$ and $y$ as I don't to see how to get rid of $\epsilon$ . Do I need to use a different $K$ and $q$ ? Setting $K=\epsilon^{-4/3}$ and $q(\xi)=\epsilon^{-2/3}\xi$ would give a term $(-z)^{-1/4}$ for $q(\xi)^{-1/4}$ for example.,Obtain an asymptotic representation of the solution of Airy's equation for large using the change of variable . The change of variable leads to an equation Setting and the equation is in WKB form with solution My problem is how to get the solution into a form which involve only and as I don't to see how to get rid of . Do I need to use a different and ? Setting and would give a term for for example.,\frac{d^2y}{dz^2}-zy=0 |z| z=\epsilon^{-2/3}\xi \frac{d^2y}{d\xi^2}-\epsilon^{-2}\xi y=0. K=\epsilon^{-2} q(\xi)=-\xi y=A_+q(\xi)^{-1/4} e^{iK\int \sqrt{q(\xi)}d\xi}+A_-q(\xi)^{-1/4} e^{-iK\int \sqrt{q(\xi)}d\xi} \\ =A_+(-\xi)^{-1/4} e^{i\epsilon^{-2}\frac23(-\xi)^{3/2}}+A_-(-\xi)^{-1/4} e^{-i\epsilon^{-2}\frac23(-\xi)^{3/2}} z y \epsilon K q K=\epsilon^{-4/3} q(\xi)=\epsilon^{-2/3}\xi (-z)^{-1/4} q(\xi)^{-1/4},"['ordinary-differential-equations', 'asymptotics']"
69,Application of Lyapounov's Theorem,Application of Lyapounov's Theorem,,"I have the following exercise to fullfill: Given the system of differential equations $x'=f(x)=-\nabla{g}$ , where $x(t)\in\Bbb{R^3}$ and $g$ is $C^1$ and $f(0)=0$ and $0$ is a total maximum for $g$ , decide about the stability of the point $0$ . My attempt: I consider the function $V=g(0)-g(x)$ . Now, If the $0$ is an isolated point of maximum of $g$ and isolated equilibrium point of $f$ , we conclude that $V$ is a Lyapounov function with $\nabla{V}\cdot{f}$ $=\nabla{g}\cdot\nabla{g}$ , which is positive for $x\neq0$ . So according to the Lyapunov theorem the $0$ is unstable. My question is if we can solve the exercise given by not using the fact that $0$ is isolated, as stated in my proof. Thanks.","I have the following exercise to fullfill: Given the system of differential equations , where and is and and is a total maximum for , decide about the stability of the point . My attempt: I consider the function . Now, If the is an isolated point of maximum of and isolated equilibrium point of , we conclude that is a Lyapounov function with , which is positive for . So according to the Lyapunov theorem the is unstable. My question is if we can solve the exercise given by not using the fact that is isolated, as stated in my proof. Thanks.",x'=f(x)=-\nabla{g} x(t)\in\Bbb{R^3} g C^1 f(0)=0 0 g 0 V=g(0)-g(x) 0 g f V \nabla{V}\cdot{f} =\nabla{g}\cdot\nabla{g} x\neq0 0 0,"['ordinary-differential-equations', 'dynamical-systems', 'linearization']"
70,Constrained equilibrium for inflated sheet,Constrained equilibrium for inflated sheet,,"I am trying to figure out the shape that a disc of uniformly elastic material makes if you fix its circumference firmly to a flat table and slowly fill the interior with air. So far, I've determined that the answer should be some kind of constrained optimality problem, where there's an equilibrium between forces due to air pressure and elasticity, and the boundary is fixed. And based on symmetry, the solution ought to be rotationally symmetric. But I'm unsure how to model elastic forces or how to set up the problem more formally or if there's a quick insight to see the solution. Maybe I should set it up as a variational calculus problem, where the candidate shape of the inflated material determines the distribution of elastic forces? Any advice is appreciated. Edit: Maybe I can solve this equation in 2D instead of 3D, with a one-dimensional elastic string fixed at its endpoints and filled with air pressure? I imagine the solution to the 3D problem is something like a surface of revolution of the 2D solution. Is this problem equivalent to finding the shape of a meniscus between two fluids? Is the solution a catenary curve because it's like a hanging chain with air pressure replacing the force of gravity?","I am trying to figure out the shape that a disc of uniformly elastic material makes if you fix its circumference firmly to a flat table and slowly fill the interior with air. So far, I've determined that the answer should be some kind of constrained optimality problem, where there's an equilibrium between forces due to air pressure and elasticity, and the boundary is fixed. And based on symmetry, the solution ought to be rotationally symmetric. But I'm unsure how to model elastic forces or how to set up the problem more formally or if there's a quick insight to see the solution. Maybe I should set it up as a variational calculus problem, where the candidate shape of the inflated material determines the distribution of elastic forces? Any advice is appreciated. Edit: Maybe I can solve this equation in 2D instead of 3D, with a one-dimensional elastic string fixed at its endpoints and filled with air pressure? I imagine the solution to the 3D problem is something like a surface of revolution of the 2D solution. Is this problem equivalent to finding the shape of a meniscus between two fluids? Is the solution a catenary curve because it's like a hanging chain with air pressure replacing the force of gravity?",,"['ordinary-differential-equations', 'physics', 'calculus-of-variations', 'lagrange-multiplier']"
71,Building solutions to forth order ODEs out of products of solutions to second order ODEs.,Building solutions to forth order ODEs out of products of solutions to second order ODEs.,,"Let $r$ , $A$ , $A_0$ , $A_1$ and $A_2$ be real numbers. By generalizing the approach given in my answer to How can I solve the following higher order ODE? I have found the following. Define the following functions : \begin{eqnarray} a_2(x) &:=& 4 A^2 A_0 r^2 x^{2 r-2}&+4 A A_1 r^2 x^{r-2}&+\frac{(4 A_2-1) r^2+1}{x^2}\\ a_3(x) &:=& 12 A^2 A_0 (r-1) r^2 x^{2 r-3}&+6 A A_1 (r-2) r^2 x^{r-3}&+\frac{3 \left((1-4 A_2) r^2-1\right)}{x^3}\\ a_4(x)&:=&4 A^2 A_0 (r-1) r^2 (2 r-3) x^{2 r-4}&+2 AA_1 (r-3) (r-2) r^2 x^{r-4}&+\frac{3 (4 A_2-1) r^2+3}{x^4} \end{eqnarray} Now consider a following fourth order Ordinary Differential Equation (ODE): \begin{equation} \frac{d^4 v(x)}{d x^4} + a_2(x) \frac{d^2 v(x)}{d x^2} + a_3(x) \frac{d^1 v(x)}{d x^1} + a_4(x) v(x)=0 \end{equation} Then the solution to the ODE above reads: \begin{equation} v(x)= C_1 v_1(x)^2 + C_2 v_2(x)^2 + C_{1,2} v_1(x) v_2(x) \end{equation} where \begin{equation} v_{1,2}(x) := x^{\frac{1-r}{2}} \cdot y_{1,2}(A x^r) \end{equation} and \begin{eqnarray} y_1(x) &=& M_{-\frac{\imath A_1}{2\sqrt{A_0}},-\frac{\imath}{2} \sqrt{-1+4 A_2}}(2\imath \sqrt{A_0} x)\\ y_2(x) &=& W_{-\frac{\imath A_1}{2\sqrt{A_0}},-\frac{\imath}{2} \sqrt{-1+4 A_2}}(2\imath \sqrt{A_0} x) \end{eqnarray} where $M_{\cdot,\cdot}(x)$ and $W_{\cdot,\cdot}(x)$ are Whittaker functions https://en.wikipedia.org/wiki/Whittaker_function . The code snippet below provides a verification: In[970]:= r =.; A =.; A0 =.; A1 =.; A2 =.; {a2[x_], a3[x_],     a4[x_]} = {(1 + (-1 + 4 A2) r^2)/x^2 + 4 A A1 r^2 x^(-2 + r) +      4 A^2 A0 r^2 x^(-2 + 2 r), (3 (-1 + (1 - 4 A2) r^2))/x^3 +      6 A A1 (-2 + r) r^2 x^(-3 + r) +      12 A^2 A0 (-1 + r) r^2 x^(-3 + 2 r), (3 + 3 (-1 + 4 A2) r^2)/     x^4 + 2 A A1 r^2 (-3 + r) (-2 + r) x^(-4 + r) +      4 A^2 A0 r^2 (-1 + r) (-3 + 2 r) x^(-4 + 2 r)}; y1[x_] = WhittakerM[-((I A1)/(2 Sqrt[A0])), -(1/2) I Sqrt[-1 + 4 A2],     2 I Sqrt[A0] x]; y2[x_] = WhittakerW[-((I A1)/(2 Sqrt[A0])), -(1/2) I Sqrt[-1 + 4 A2],     2 I Sqrt[A0] x]; v1[x_] = x^((1 - r)/2) y1[A x^r]; v2[x_] = x^((1 - r)/2) y2[A x^r]; FullSimplify[(D[#, {x, 4}] + a2[x] D[#, {x, 2}] + a3[x] D[#, {x, 1}] +       a4[x] #) & /@ {v1[x]^2, v2[x]^2, v1[x] v2[x]}]  Out[976]= {0, 0, 0} Now what we have done in here is the following. We rescaled the abscissa and the ordinates of Whittaker functions and then constructed a symmetric product of those functions, which in turn satisfies a forth order ODE. This procedure follows pretty much the ideas from [1]. Update: Here is another example of a 4th order linear ODE that has a solution in terms of a symmetric product of solutions to 2nd order ODEs.  Let $n$ , $w$ and $A$ be parameters. Now consider a following ODE: \begin{eqnarray} y^{(4)}(x) + \frac{w}{x^2} y^{(2)}(x) + \frac{-8 (n+6) w-5 n (n+2) (n+4)}{16 x^3} y^{(1)}(x) + \left( 4 A x^{2 n}-\frac{3 \left(n^2-16\right) \left(7 n^2+16 n+16 w\right)}{256 x^4}\right) y(x)=0 \end{eqnarray} The fundamental set of solutions to this ODE is given by $\left\{ y_{1,\eta_1}(x) \cdot y_{2,\eta_2}(x)\right\}$ for $\eta_1=-1,1$ and $\eta_2=-1,1$ where: \begin{eqnarray} y_{1,\eta}(x)&:=& x^{\frac{1}{2}-\frac{n}{8}} \cdot J_{\eta \frac{\sqrt{-3 n^2-12 n-8 w+8}}{2 \sqrt{2} \sqrt{(n+2)^2}}}(\frac{2 \sqrt[4]{A}}{\sqrt{n^2+4 n+4}}\cdot x^{\frac{n}{2}+1})\\ y_{2,\eta}(x)&:=& x^{\frac{1}{2}-\frac{n}{8}} \cdot J_{\eta \frac{\sqrt{-3 n^2-12 n-8 w+8}}{2 \sqrt{2} \sqrt{(n+2)^2}}}(\frac{2 \sqrt[4]{A} \imath}{\sqrt{n^2+4 n+4}}\cdot x^{\frac{n}{2}+1}) \end{eqnarray} In[790]:= n =.; w =.; A =.; Clear[y1]; Clear[y2]; x =.; eX =.; y1[eta_, x_] =    x^(1/2 - n/8) BesselJ[     eta Sqrt[8 - 12 n - 3 n^2 - 8 w]/(2 Sqrt[2] Sqrt[(2 + n)^2]), (      2 A^(1/4) Sqrt[1])/Sqrt[4 + 4 n + n^2] x^(n/2 + 1)]; y2[eta_, x_] =    x^(1/2 - n/8) BesselJ[     eta Sqrt[8 - 12 n - 3 n^2 - 8 w]/(2 Sqrt[2] Sqrt[(2 + n)^2]), (      2 A^(1/4) Sqrt[-1])/Sqrt[4 + 4 n + n^2] x^(n/2 + 1)];  eX = ((-((3 (-16 + n^2) (16 n + 7 n^2 + 16 w))/(256 x^4)) +           4 A x^(2 n)) # + (-5 n (2 + n) (4 + n) - 8 (6 + n) w) /(        16 x^3) D[#, x] + w /x^2 D[#, {x, 2}] + D[#, {x, 4}]) & /@ {y1[       1, x] y2[1, x], y1[-1, x] y2[1, x], y1[1, x] y2[-1, x],      y1[-1, x] y2[-1, x]}; {n, w, A} = RandomReal[{0, 10}, 3, WorkingPrecision -> 50]; x = RandomReal[{0, 1}, WorkingPrecision -> 50]; eX   Out[796]= {0.*10^-43 + 0.*10^-43 I, 0.*10^-43 + 0.*10^-43 I,   0.*10^-43 + 0.*10^-43 I, 0.*10^-43 + 0.*10^-43 I} Now my question would be the following . Can we construct solutions of any forth order ODE with ""polynomial"" coefficients in the way we did this in here? For example can we solve the ODE given in here How can I solve the following higher order ODE? with this method?If yes what other second order ODEs -- rather than the Whittaker ODE --could we be using for that purpose. [1]M.F. Singer,  Solving Homogeneous Linear Differential Equations in Terms of Second Order Linear Differential Equations, Am. J. of Math., 107, 1985, 663-696.","Let , , , and be real numbers. By generalizing the approach given in my answer to How can I solve the following higher order ODE? I have found the following. Define the following functions : Now consider a following fourth order Ordinary Differential Equation (ODE): Then the solution to the ODE above reads: where and where and are Whittaker functions https://en.wikipedia.org/wiki/Whittaker_function . The code snippet below provides a verification: In[970]:= r =.; A =.; A0 =.; A1 =.; A2 =.; {a2[x_], a3[x_],     a4[x_]} = {(1 + (-1 + 4 A2) r^2)/x^2 + 4 A A1 r^2 x^(-2 + r) +      4 A^2 A0 r^2 x^(-2 + 2 r), (3 (-1 + (1 - 4 A2) r^2))/x^3 +      6 A A1 (-2 + r) r^2 x^(-3 + r) +      12 A^2 A0 (-1 + r) r^2 x^(-3 + 2 r), (3 + 3 (-1 + 4 A2) r^2)/     x^4 + 2 A A1 r^2 (-3 + r) (-2 + r) x^(-4 + r) +      4 A^2 A0 r^2 (-1 + r) (-3 + 2 r) x^(-4 + 2 r)}; y1[x_] = WhittakerM[-((I A1)/(2 Sqrt[A0])), -(1/2) I Sqrt[-1 + 4 A2],     2 I Sqrt[A0] x]; y2[x_] = WhittakerW[-((I A1)/(2 Sqrt[A0])), -(1/2) I Sqrt[-1 + 4 A2],     2 I Sqrt[A0] x]; v1[x_] = x^((1 - r)/2) y1[A x^r]; v2[x_] = x^((1 - r)/2) y2[A x^r]; FullSimplify[(D[#, {x, 4}] + a2[x] D[#, {x, 2}] + a3[x] D[#, {x, 1}] +       a4[x] #) & /@ {v1[x]^2, v2[x]^2, v1[x] v2[x]}]  Out[976]= {0, 0, 0} Now what we have done in here is the following. We rescaled the abscissa and the ordinates of Whittaker functions and then constructed a symmetric product of those functions, which in turn satisfies a forth order ODE. This procedure follows pretty much the ideas from [1]. Update: Here is another example of a 4th order linear ODE that has a solution in terms of a symmetric product of solutions to 2nd order ODEs.  Let , and be parameters. Now consider a following ODE: The fundamental set of solutions to this ODE is given by for and where: In[790]:= n =.; w =.; A =.; Clear[y1]; Clear[y2]; x =.; eX =.; y1[eta_, x_] =    x^(1/2 - n/8) BesselJ[     eta Sqrt[8 - 12 n - 3 n^2 - 8 w]/(2 Sqrt[2] Sqrt[(2 + n)^2]), (      2 A^(1/4) Sqrt[1])/Sqrt[4 + 4 n + n^2] x^(n/2 + 1)]; y2[eta_, x_] =    x^(1/2 - n/8) BesselJ[     eta Sqrt[8 - 12 n - 3 n^2 - 8 w]/(2 Sqrt[2] Sqrt[(2 + n)^2]), (      2 A^(1/4) Sqrt[-1])/Sqrt[4 + 4 n + n^2] x^(n/2 + 1)];  eX = ((-((3 (-16 + n^2) (16 n + 7 n^2 + 16 w))/(256 x^4)) +           4 A x^(2 n)) # + (-5 n (2 + n) (4 + n) - 8 (6 + n) w) /(        16 x^3) D[#, x] + w /x^2 D[#, {x, 2}] + D[#, {x, 4}]) & /@ {y1[       1, x] y2[1, x], y1[-1, x] y2[1, x], y1[1, x] y2[-1, x],      y1[-1, x] y2[-1, x]}; {n, w, A} = RandomReal[{0, 10}, 3, WorkingPrecision -> 50]; x = RandomReal[{0, 1}, WorkingPrecision -> 50]; eX   Out[796]= {0.*10^-43 + 0.*10^-43 I, 0.*10^-43 + 0.*10^-43 I,   0.*10^-43 + 0.*10^-43 I, 0.*10^-43 + 0.*10^-43 I} Now my question would be the following . Can we construct solutions of any forth order ODE with ""polynomial"" coefficients in the way we did this in here? For example can we solve the ODE given in here How can I solve the following higher order ODE? with this method?If yes what other second order ODEs -- rather than the Whittaker ODE --could we be using for that purpose. [1]M.F. Singer,  Solving Homogeneous Linear Differential Equations in Terms of Second Order Linear Differential Equations, Am. J. of Math., 107, 1985, 663-696.","r A A_0 A_1 A_2 \begin{eqnarray}
a_2(x) &:=& 4 A^2 A_0 r^2 x^{2 r-2}&+4 A A_1 r^2 x^{r-2}&+\frac{(4 A_2-1) r^2+1}{x^2}\\
a_3(x) &:=& 12 A^2 A_0 (r-1) r^2 x^{2 r-3}&+6 A A_1 (r-2) r^2 x^{r-3}&+\frac{3 \left((1-4 A_2) r^2-1\right)}{x^3}\\
a_4(x)&:=&4 A^2 A_0 (r-1) r^2 (2 r-3) x^{2 r-4}&+2 AA_1 (r-3) (r-2) r^2 x^{r-4}&+\frac{3 (4 A_2-1) r^2+3}{x^4}
\end{eqnarray} \begin{equation}
\frac{d^4 v(x)}{d x^4} + a_2(x) \frac{d^2 v(x)}{d x^2} + a_3(x) \frac{d^1 v(x)}{d x^1} + a_4(x) v(x)=0
\end{equation} \begin{equation}
v(x)= C_1 v_1(x)^2 + C_2 v_2(x)^2 + C_{1,2} v_1(x) v_2(x)
\end{equation} \begin{equation}
v_{1,2}(x) := x^{\frac{1-r}{2}} \cdot y_{1,2}(A x^r)
\end{equation} \begin{eqnarray}
y_1(x) &=& M_{-\frac{\imath A_1}{2\sqrt{A_0}},-\frac{\imath}{2} \sqrt{-1+4 A_2}}(2\imath \sqrt{A_0} x)\\
y_2(x) &=& W_{-\frac{\imath A_1}{2\sqrt{A_0}},-\frac{\imath}{2} \sqrt{-1+4 A_2}}(2\imath \sqrt{A_0} x)
\end{eqnarray} M_{\cdot,\cdot}(x) W_{\cdot,\cdot}(x) n w A \begin{eqnarray}
y^{(4)}(x) + \frac{w}{x^2} y^{(2)}(x) + \frac{-8 (n+6) w-5 n (n+2) (n+4)}{16 x^3} y^{(1)}(x) + \left( 4 A x^{2 n}-\frac{3 \left(n^2-16\right) \left(7 n^2+16 n+16 w\right)}{256 x^4}\right) y(x)=0
\end{eqnarray} \left\{ y_{1,\eta_1}(x) \cdot y_{2,\eta_2}(x)\right\} \eta_1=-1,1 \eta_2=-1,1 \begin{eqnarray}
y_{1,\eta}(x)&:=& x^{\frac{1}{2}-\frac{n}{8}} \cdot J_{\eta \frac{\sqrt{-3 n^2-12 n-8 w+8}}{2 \sqrt{2} \sqrt{(n+2)^2}}}(\frac{2 \sqrt[4]{A}}{\sqrt{n^2+4 n+4}}\cdot x^{\frac{n}{2}+1})\\
y_{2,\eta}(x)&:=& x^{\frac{1}{2}-\frac{n}{8}} \cdot J_{\eta \frac{\sqrt{-3 n^2-12 n-8 w+8}}{2 \sqrt{2} \sqrt{(n+2)^2}}}(\frac{2 \sqrt[4]{A} \imath}{\sqrt{n^2+4 n+4}}\cdot x^{\frac{n}{2}+1})
\end{eqnarray}","['ordinary-differential-equations', 'special-functions']"
72,A question on a $2\times 2$ matrix differential equation,A question on a  matrix differential equation,2\times 2,"Suppose we consider an equation $A f=\lambda f^*$ . Here $A$ is a differential operator with the general form $$A=a\frac{d^2}{dt^2}+b g(t)$$ where $a,b\in\mathbb{C}$ and $g(t)$ is a complex function of real variable $t$ . $f$ is complex function $f=f(t)$ , and $f^*$ is its complex conjugate. $\lambda$ is a real number. We have a complex-conjugated equation $A^* f^*=\lambda f$ . In total, we have the matrix form $$\pmatrix{\ 0\ \ A^*\\ A\ \ 0}\pmatrix{f\\ f^*}=\lambda\pmatrix{f\\ f^*}.$$ It is very easy to see that we have another equation $$\pmatrix{0\  \ A^*\\ A\ \ 0}\pmatrix{if\\ -if^*}=-\lambda\pmatrix{if\\ -if^*}.$$ Now I will conclude that $$-\lambda^2=\det\pmatrix{0\ \ A^*\\ A\ \ 0}=-\det(A^*A).\tag{1}$$ All these look fine to me up to now. But I tried another way in the following and found a disagreement. Writing $A=U+iV$ , $f=x+iy$ , then we have the following matrix equation $$\pmatrix{U\ \ -V\\ -V\ \ -U}\pmatrix{x\\ y}=\lambda\pmatrix{x\\ y}.$$ This equation is also associated with another equation $$\pmatrix{U\ \ -V\\ -V\ \ -U}\pmatrix{-y\\ x}=-\lambda\pmatrix{-y\\ x}.$$ And I conclude $$-\lambda^2=\det\pmatrix{U\ \ -V\\ -V\ \ -U}=-\det (U^2+V^2).$$ However $A^*A=U^2+V^2+i[U,V]$ . If both methods are correct then there must be $$\det (U^2+V^2+i[U,V])=\det(U^2+V^2).\tag{2}$$ Is this true? Or there is something wrong in one of these two methods or in both? For Eq.(2), one possibility that it may be true is that we know $$\det(AA^*)=\det(A^*A)$$ which gives $$\det(U^2+V^2+i[V,U])=\det(U^2+V^2+i[U,V]).$$ Somehow the part $i[V,U]$ or $i[U,V]$ does not contribute in the determinant?","Suppose we consider an equation . Here is a differential operator with the general form where and is a complex function of real variable . is complex function , and is its complex conjugate. is a real number. We have a complex-conjugated equation . In total, we have the matrix form It is very easy to see that we have another equation Now I will conclude that All these look fine to me up to now. But I tried another way in the following and found a disagreement. Writing , , then we have the following matrix equation This equation is also associated with another equation And I conclude However . If both methods are correct then there must be Is this true? Or there is something wrong in one of these two methods or in both? For Eq.(2), one possibility that it may be true is that we know which gives Somehow the part or does not contribute in the determinant?","A f=\lambda f^* A A=a\frac{d^2}{dt^2}+b g(t) a,b\in\mathbb{C} g(t) t f f=f(t) f^* \lambda A^* f^*=\lambda f \pmatrix{\ 0\ \ A^*\\
A\ \ 0}\pmatrix{f\\ f^*}=\lambda\pmatrix{f\\ f^*}. \pmatrix{0\  \ A^*\\
A\ \ 0}\pmatrix{if\\ -if^*}=-\lambda\pmatrix{if\\ -if^*}. -\lambda^2=\det\pmatrix{0\ \ A^*\\
A\ \ 0}=-\det(A^*A).\tag{1} A=U+iV f=x+iy \pmatrix{U\ \ -V\\
-V\ \ -U}\pmatrix{x\\ y}=\lambda\pmatrix{x\\ y}. \pmatrix{U\ \ -V\\
-V\ \ -U}\pmatrix{-y\\ x}=-\lambda\pmatrix{-y\\ x}. -\lambda^2=\det\pmatrix{U\ \ -V\\
-V\ \ -U}=-\det (U^2+V^2). A^*A=U^2+V^2+i[U,V] \det (U^2+V^2+i[U,V])=\det(U^2+V^2).\tag{2} \det(AA^*)=\det(A^*A) \det(U^2+V^2+i[V,U])=\det(U^2+V^2+i[U,V]). i[V,U] i[U,V]","['ordinary-differential-equations', 'operator-theory', 'determinant']"
73,"What does ""composing"" mean when symmetry reducing a differential equation?","What does ""composing"" mean when symmetry reducing a differential equation?",,"I am reading a set of lecture notes describing the process of symmetry reduction of differential equations via Lie algebra methods ( link ), and I'm stuck at what seems to be a fairly simple point. The specific example is the Korteweg-DeVries equation, $$u_t+uu_x+u_{xxx}=0.$$ The Lie algebra will be realized as a vector field $$\hat{v}=\xi(x,t,u) \partial_x+\tau(x,t,u) \partial_t+\phi(x,t,u)\partial_u.$$ Now I'll directly cut from the notes: (I am excerpting but I don't think I'm missing anything critical - check out pg 5 of that link if you want more background). Anyway, we go through the process of determining the vector field, and end up with $$\xi=1+t+x,\qquad \tau=1+3t,\qquad \phi=-2u+1$$ So now, I should be integrating these equations, subject to the initial conditions $\tilde{x}(0)=x$ , etc. That shouldn't be a problem: $$\frac{d\tilde{x}}{d\lambda}=1+\tilde{t}+\tilde{x}\rightarrow \tilde{x}=e^\lambda(1+\tilde{t}+x)-(1+\tilde{t})$$ $$\frac{d\tilde{t}}{d\lambda}=1+3\tilde {t}\rightarrow \tilde{t}=\frac{1}{3}e^{3\lambda}(1+3t)-\frac{1}{3}$$ $$\frac{d\tilde{u}}{d\lambda}=-2\tilde{u}+1\rightarrow \tilde{u}=\frac{1}{2}e^{-2\lambda}(2u+1)-\frac{1}{2}$$ (we could eliminate $\tilde{t}$ from the first equation, but that won't help what I'm going to say next) The problem is that in the notes (pg 9), the result is given as So for this to match my results I would need to take, for instance, $$t_0=\frac{1}{3}(1-e^{-3\lambda}),$$ certainly not a constant. So what am I doing wrong? I guess I am concerned with not understanding what ""composing the one-dimensional subgroups"" means, but I thought that is describing the process of getting from one solution to another. Like once you get $u(x,t)$ , you can use the results above to get another solution $\tilde{u}(\tilde{x},\tilde{t})$ .","I am reading a set of lecture notes describing the process of symmetry reduction of differential equations via Lie algebra methods ( link ), and I'm stuck at what seems to be a fairly simple point. The specific example is the Korteweg-DeVries equation, The Lie algebra will be realized as a vector field Now I'll directly cut from the notes: (I am excerpting but I don't think I'm missing anything critical - check out pg 5 of that link if you want more background). Anyway, we go through the process of determining the vector field, and end up with So now, I should be integrating these equations, subject to the initial conditions , etc. That shouldn't be a problem: (we could eliminate from the first equation, but that won't help what I'm going to say next) The problem is that in the notes (pg 9), the result is given as So for this to match my results I would need to take, for instance, certainly not a constant. So what am I doing wrong? I guess I am concerned with not understanding what ""composing the one-dimensional subgroups"" means, but I thought that is describing the process of getting from one solution to another. Like once you get , you can use the results above to get another solution .","u_t+uu_x+u_{xxx}=0. \hat{v}=\xi(x,t,u) \partial_x+\tau(x,t,u) \partial_t+\phi(x,t,u)\partial_u. \xi=1+t+x,\qquad \tau=1+3t,\qquad \phi=-2u+1 \tilde{x}(0)=x \frac{d\tilde{x}}{d\lambda}=1+\tilde{t}+\tilde{x}\rightarrow \tilde{x}=e^\lambda(1+\tilde{t}+x)-(1+\tilde{t}) \frac{d\tilde{t}}{d\lambda}=1+3\tilde {t}\rightarrow \tilde{t}=\frac{1}{3}e^{3\lambda}(1+3t)-\frac{1}{3} \frac{d\tilde{u}}{d\lambda}=-2\tilde{u}+1\rightarrow \tilde{u}=\frac{1}{2}e^{-2\lambda}(2u+1)-\frac{1}{2} \tilde{t} t_0=\frac{1}{3}(1-e^{-3\lambda}), u(x,t) \tilde{u}(\tilde{x},\tilde{t})","['ordinary-differential-equations', 'lie-groups', 'lie-algebras']"
74,Matrix Regression for linear ODE system,Matrix Regression for linear ODE system,,"Background I have the following homogeneous ODE system as an Initial Value Problem: $$ y'=A\cdot y\quad\wedge\quad y(0)=y_0 $$ where $y\in\mathbb{R}^{N\times 1}$ is the unknown vector and $A\in\mathbb{R}^{N\times N}$ is a known, constant coefficient nonsingular, diagonalizable matrix with only $N$ independent entries. For instance, if $N=3$ it could be: $$ A=\begin{pmatrix} -x_1 & 0 & 0\\ x_1 & -x_2 & 0\\ 0 & x_2 & -x_3 \end{pmatrix} $$ with $x_1,x_2,x_3$ known. It is possible to express a closed form solution of this system as: $$ y(t)=\sum_{i=1}^NK_i e^{\lambda_it}\cdot u_i $$ where $\lambda_i$ is the i-th eigenvalue of $A$ and $u_i$ is the i-th eigenvector of $A$ . The $K_i$ are constants of integration such that the Initial Condition is followed. Question Suppose to have the above ODE system, with the matrix $A$ with a known structure but the entries' values $x_1,\ldots,x_N$ are unknown. Suppose also to have $t_k$ and $y(t_k)=y_k$ values for $k$ ""experiments"" with $k\gg N$ , including the initial condition state, $k=0\to t_{k=0}=0$ . Is there a way to calculate such entries values by means of a regression? If so, what kind of regression? Also we assume that $y_k \sim \mathcal{N}(0,\sigma^2)$ . Above all, I think the answer to the first question being ""yes"" since the number of unknown parameters $N$ is way less than the number of experimental data $k$ . Edit 1 - The Brute Force Approach Since the eigenbasis of $A$ is invariant under uniform scaling, meaning that for any constant $K\neq 0$ if $u_i$ is an eigenvector of $A$ (with eigenvalue $\lambda_i$ ) then $w_i=Ku_i$ is also an eigenvector (with eigenvalue $\lambda_i$ ), since: $$ Au_i=\lambda_iu_i\quad\to\quad AKu_i=\lambda_iKu_i\quad\to\quad Aw_i=\lambda_iw_i $$ the general solution can be rewritten as a model: $$ \hat{y}(t,\lambda,W)=\sum_{i=1}^N e^{\lambda_it}\cdot w_i $$ or, by component: $$ y_j(t) = \sum_{i=1}^N e^{\lambda_it}\cdot w_{ji} $$ This yields $N$ equations with a total of $N+N^2$ parameters ( $N$ eigenvalues and $N^2$ eigenvector components of the matrix $W=\{w\}_{ji}$ ) and thus $k\gg N(N+1)$ is required at least. The regression is expressed as a minimization problem: $$ \min_{\lambda, W}\sum_{j=1}^{k} \left\lvert y_{j} - \hat{y} (t_j,\lambda,W)\right\rvert_2 $$ and once $\lambda$ and $W$ are known, then: $$ A=W^{-1}\mathrm{diag}(\lambda)W $$ This process is however tedious, boring, and does not exploit the structure and properties of A. What are the improvements?","Background I have the following homogeneous ODE system as an Initial Value Problem: where is the unknown vector and is a known, constant coefficient nonsingular, diagonalizable matrix with only independent entries. For instance, if it could be: with known. It is possible to express a closed form solution of this system as: where is the i-th eigenvalue of and is the i-th eigenvector of . The are constants of integration such that the Initial Condition is followed. Question Suppose to have the above ODE system, with the matrix with a known structure but the entries' values are unknown. Suppose also to have and values for ""experiments"" with , including the initial condition state, . Is there a way to calculate such entries values by means of a regression? If so, what kind of regression? Also we assume that . Above all, I think the answer to the first question being ""yes"" since the number of unknown parameters is way less than the number of experimental data . Edit 1 - The Brute Force Approach Since the eigenbasis of is invariant under uniform scaling, meaning that for any constant if is an eigenvector of (with eigenvalue ) then is also an eigenvector (with eigenvalue ), since: the general solution can be rewritten as a model: or, by component: This yields equations with a total of parameters ( eigenvalues and eigenvector components of the matrix ) and thus is required at least. The regression is expressed as a minimization problem: and once and are known, then: This process is however tedious, boring, and does not exploit the structure and properties of A. What are the improvements?","
y'=A\cdot y\quad\wedge\quad y(0)=y_0
 y\in\mathbb{R}^{N\times 1} A\in\mathbb{R}^{N\times N} N N=3 
A=\begin{pmatrix}
-x_1 & 0 & 0\\
x_1 & -x_2 & 0\\
0 & x_2 & -x_3
\end{pmatrix}
 x_1,x_2,x_3 
y(t)=\sum_{i=1}^NK_i e^{\lambda_it}\cdot u_i
 \lambda_i A u_i A K_i A x_1,\ldots,x_N t_k y(t_k)=y_k k k\gg N k=0\to t_{k=0}=0 y_k \sim \mathcal{N}(0,\sigma^2) N k A K\neq 0 u_i A \lambda_i w_i=Ku_i \lambda_i 
Au_i=\lambda_iu_i\quad\to\quad AKu_i=\lambda_iKu_i\quad\to\quad Aw_i=\lambda_iw_i
 
\hat{y}(t,\lambda,W)=\sum_{i=1}^N e^{\lambda_it}\cdot w_i
 
y_j(t) = \sum_{i=1}^N e^{\lambda_it}\cdot w_{ji}
 N N+N^2 N N^2 W=\{w\}_{ji} k\gg N(N+1) 
\min_{\lambda, W}\sum_{j=1}^{k} \left\lvert y_{j} - \hat{y} (t_j,\lambda,W)\right\rvert_2
 \lambda W 
A=W^{-1}\mathrm{diag}(\lambda)W
","['ordinary-differential-equations', 'regression', 'parameter-estimation']"
75,"When $f(x+1)-f(x)=f'(x)$, what are the solutions for $f(x)$?","When , what are the solutions for ?",f(x+1)-f(x)=f'(x) f(x),"The question is: When $f(x+1)-f(x)=f'(x)$, what are the solutions for $f(x)$? The most obvious solution is a linear function of the form $f(x)=ax+b$. Is this the only solution? Edit I should add that $f:\mathbb R\to\mathbb R$ to the question.","The question is: When $f(x+1)-f(x)=f'(x)$, what are the solutions for $f(x)$? The most obvious solution is a linear function of the form $f(x)=ax+b$. Is this the only solution? Edit I should add that $f:\mathbb R\to\mathbb R$ to the question.",,"['real-analysis', 'ordinary-differential-equations', 'delay-differential-equations']"
76,Second order homogenous differential equation with variable coefficients,Second order homogenous differential equation with variable coefficients,,"I have a complicated non-linear first order homogeneous differential equation for coherent states $\psi(t)$ . Via perturbation theory I obtained a linear non-homogeneous first order recursive differential equation \begin{align} (1)\qquad\frac{d}{dt}\psi^n&=-i\omega\psi^n+f^{n-1}\\ \end{align} where $n$ denotes the order of perturbation and the functions $g^{n-1}$ and $f^{n-1}$ consist of the previous solutions up to $n-1$ -th order and some other functions $\phi^i(t)$ from another differential equation I solved, so they are known. I solved these via matrix calculus for $n=0,1$ and obtained the following solutions $$ (1.1)\qquad\psi^0=A^0e^{-i\omega t}\qquad \psi^1=A^1(e^{i\omega t}-e^{-3i\omega t})\, . $$ In case of $n=0$ , one can directly read out the dispersion $\omega$ because the amplitude is time-independent. In case of $n=1$ , it is not possible to read out the dispersion directly since it is a combination of $\omega$ and $3\omega$ . My goal is to write the first order solution such that one can read out the dispersion directly. To this end, I separated the real and imaginary parts and wrote the polar form: $$ \psi^1=2A^1sin(2\omega t)e^{icot(\omega t)}\, . $$ Now, the time dependence entered into the amplitude, which is qualitatively different from the $n=0$ case, such that the exponent is not the dispersion anymore. To find an expression similar to the zeroth order case, I reformulated the differential equation while knowing the solutions $\psi^1$ which I then again solved and hoped for a ""nice"" result. Because it is $A^1=A^1(A^0)$ , one cane write $\psi^0$ in terms of $\psi^1$ and consequently it is $f^1=p(t)\psi^1$ . And plug this into the differential equation gives: $$ (2)\qquad\frac{d}{dt}\psi^1=-i\omega\psi^1+f=(-i\omega+p(t))\, \psi^1. $$ Here, I solved the equation and obtained: $$ (2.1)\qquad\psi^1=A'^{1}sin(2\omega t)e^{-i\omega t}\, . $$ Unfortunately, the amplitude is still time-dependent. So I again reformulated the differential equation and obtain several other forms of differential equations hoping to encounter a desired solution. One of the other differential equations had the following form ( $y\equiv\psi\, ,x\equiv t$ ): $$ x^2 y^{\prime\prime}(x)+2xy^{\prime}(x)-2y+axy(x)+bx^2y(x)=0\, $$ where $y: \mathbb{R}\rightarrow\mathbb{C}$ and $a,b\in\mathbb{C}$ . After several failed attempts, I found out about the Laplace transform. So I transformed it to: $$ (s^2+b)Y''(s)+(2s-a)Y'(s)-2Y(s)=0\,  $$ where $Y(s)=\int_0^{\infty}e^{-sx}y(x)dx$ . The solutions to this equation as mentioned by @paulplusx is even worse. Now this brings me to the following two questions: 1.) How can I get the desired structure, i.e., $\psi^1\sim e^{-i\omega' t}$ ? (maybe this is not even possible, after all if $z=r(t)^{i\theta(t)}$ , then why should I always be able to find a $\theta'$ such that $z=const.\,  e^{i\theta'(t)}$ , $r$ and $\theta$ are independent.) If not possible, how can I obtain the dispersion of $\psi^1$ ? 2.) Is the way, I was proceeding even consistent, I have strong doubts? Meaning: If I have a recursive differential equation $(1)$ that I solved at $n$ -th order. Then take the solution at $n-1$ -th order and rearrange it by writing it in terms of the $n$ -th order solution. Then obtain the differential equation $(2)$ and then solve it. It seems as if the two differential equations, do not have the same set of solutions because the solutions in $(2.1)$ does not solve $(1)$ , even though $(2)$ was derived from $(1)$ . I hope this is not too lengthy,....","I have a complicated non-linear first order homogeneous differential equation for coherent states . Via perturbation theory I obtained a linear non-homogeneous first order recursive differential equation where denotes the order of perturbation and the functions and consist of the previous solutions up to -th order and some other functions from another differential equation I solved, so they are known. I solved these via matrix calculus for and obtained the following solutions In case of , one can directly read out the dispersion because the amplitude is time-independent. In case of , it is not possible to read out the dispersion directly since it is a combination of and . My goal is to write the first order solution such that one can read out the dispersion directly. To this end, I separated the real and imaginary parts and wrote the polar form: Now, the time dependence entered into the amplitude, which is qualitatively different from the case, such that the exponent is not the dispersion anymore. To find an expression similar to the zeroth order case, I reformulated the differential equation while knowing the solutions which I then again solved and hoped for a ""nice"" result. Because it is , one cane write in terms of and consequently it is . And plug this into the differential equation gives: Here, I solved the equation and obtained: Unfortunately, the amplitude is still time-dependent. So I again reformulated the differential equation and obtain several other forms of differential equations hoping to encounter a desired solution. One of the other differential equations had the following form ( ): where and . After several failed attempts, I found out about the Laplace transform. So I transformed it to: where . The solutions to this equation as mentioned by @paulplusx is even worse. Now this brings me to the following two questions: 1.) How can I get the desired structure, i.e., ? (maybe this is not even possible, after all if , then why should I always be able to find a such that , and are independent.) If not possible, how can I obtain the dispersion of ? 2.) Is the way, I was proceeding even consistent, I have strong doubts? Meaning: If I have a recursive differential equation that I solved at -th order. Then take the solution at -th order and rearrange it by writing it in terms of the -th order solution. Then obtain the differential equation and then solve it. It seems as if the two differential equations, do not have the same set of solutions because the solutions in does not solve , even though was derived from . I hope this is not too lengthy,....","\psi(t) \begin{align}
(1)\qquad\frac{d}{dt}\psi^n&=-i\omega\psi^n+f^{n-1}\\
\end{align} n g^{n-1} f^{n-1} n-1 \phi^i(t) n=0,1 
(1.1)\qquad\psi^0=A^0e^{-i\omega t}\qquad \psi^1=A^1(e^{i\omega t}-e^{-3i\omega t})\, .
 n=0 \omega n=1 \omega 3\omega 
\psi^1=2A^1sin(2\omega t)e^{icot(\omega t)}\, .
 n=0 \psi^1 A^1=A^1(A^0) \psi^0 \psi^1 f^1=p(t)\psi^1 
(2)\qquad\frac{d}{dt}\psi^1=-i\omega\psi^1+f=(-i\omega+p(t))\, \psi^1.
 
(2.1)\qquad\psi^1=A'^{1}sin(2\omega t)e^{-i\omega t}\, .
 y\equiv\psi\, ,x\equiv t 
x^2 y^{\prime\prime}(x)+2xy^{\prime}(x)-2y+axy(x)+bx^2y(x)=0\,
 y: \mathbb{R}\rightarrow\mathbb{C} a,b\in\mathbb{C} 
(s^2+b)Y''(s)+(2s-a)Y'(s)-2Y(s)=0\, 
 Y(s)=\int_0^{\infty}e^{-sx}y(x)dx \psi^1\sim e^{-i\omega' t} z=r(t)^{i\theta(t)} \theta' z=const.\,  e^{i\theta'(t)} r \theta \psi^1 (1) n n-1 n (2) (2.1) (1) (2) (1)","['ordinary-differential-equations', 'laplace-transform', 'mathematical-physics', 'perturbation-theory']"
77,Interpretation of one-dimensional p-laplacian,Interpretation of one-dimensional p-laplacian,,"Do you know some interpretation or practical application of one-dimensional p-Laplacian systems (which is also an example of Euler-Lagrange system) $ \frac{d}{dt}(|\dot u(t)|^{p-2}\dot u(t))=\nabla W(t,u(t)), $ where $\nabla W(t,u)$ is the gradient $W$ in $u$ , $t\in\mathbb{R}$ , $u\in\mathbb{R}^N$ . Or even more general system $ \frac{d}{dt} \nabla G(\dot{u}(t))=\nabla W(t,u(t)), $ where $G$ is some convex, $C^1$ function? We know that in pde's one practical application is image denoising Intuition and applications for the p-Laplacian","Do you know some interpretation or practical application of one-dimensional p-Laplacian systems (which is also an example of Euler-Lagrange system) where is the gradient in , , . Or even more general system where is some convex, function? We know that in pde's one practical application is image denoising Intuition and applications for the p-Laplacian","
\frac{d}{dt}(|\dot u(t)|^{p-2}\dot u(t))=\nabla W(t,u(t)),
 \nabla W(t,u) W u t\in\mathbb{R} u\in\mathbb{R}^N 
\frac{d}{dt} \nabla G(\dot{u}(t))=\nabla W(t,u(t)),
 G C^1","['ordinary-differential-equations', 'euler-lagrange-equation', 'p-laplacian']"
78,Are there other self-similar functions like $e^x$ and $\cos x$? [duplicate],Are there other self-similar functions like  and ? [duplicate],e^x \cos x,"This question already has answers here : Find $f$ where $f'(x) = f(1+x)$ (3 answers) Closed 5 years ago . (counting $\sin x$ as a variation of $\cos x$). They are self-similar in that their derivative is also a function of themselves. Crucially, this sort of feedback means that higher derivatives are also similar, resulting in curves that are smooth in what seems to me to be a unique way. For the examples in the title: $f'(x) = f(x)$ is true for $f(x)=e^x$ $f'(x) = f(x+\pi/2)$ is true for $f(x)=\cos x$ EDIT: That's for the two functions given in the title. The question is: are there any other functions that are similar to their derivative in some other way? Just one example is enough (the ""duplicate"" doesn't address this). BTW It seems to me that these are the only solutions to these differential equations, and e.g. a different offset for the second one just changes the period of the $\cos$ function. But I don't see how to show this - or even how to think about it. BTW this was inspired by Euler's equation, relating $\cos, \sin,$ and $e$. I think the fundamental connection is that they all are self-similar, and the rest is clever bit-twiddling (like how integers alternate even/odd, and $f(x) = (-1)^x$ alternates positive/negative).","This question already has answers here : Find $f$ where $f'(x) = f(1+x)$ (3 answers) Closed 5 years ago . (counting $\sin x$ as a variation of $\cos x$). They are self-similar in that their derivative is also a function of themselves. Crucially, this sort of feedback means that higher derivatives are also similar, resulting in curves that are smooth in what seems to me to be a unique way. For the examples in the title: $f'(x) = f(x)$ is true for $f(x)=e^x$ $f'(x) = f(x+\pi/2)$ is true for $f(x)=\cos x$ EDIT: That's for the two functions given in the title. The question is: are there any other functions that are similar to their derivative in some other way? Just one example is enough (the ""duplicate"" doesn't address this). BTW It seems to me that these are the only solutions to these differential equations, and e.g. a different offset for the second one just changes the period of the $\cos$ function. But I don't see how to show this - or even how to think about it. BTW this was inspired by Euler's equation, relating $\cos, \sin,$ and $e$. I think the fundamental connection is that they all are self-similar, and the rest is clever bit-twiddling (like how integers alternate even/odd, and $f(x) = (-1)^x$ alternates positive/negative).",,"['algebra-precalculus', 'ordinary-differential-equations', 'derivatives']"
79,Equilibrium solutions and stability,Equilibrium solutions and stability,,"I want to find the equilibrium solutions and determine their stability. $(1)\left\{\begin{matrix} \dot{x}=-y-x(1-\sqrt{x^2+y^2})^2\\  \dot{y}=x-y(1-\sqrt{x^2+y^2})^2 \end{matrix}\right.$ I also want to check the behavior of the solutions of $(1)$ when $t \to \infty$. There are five possible answers. there exists exactly one equilibrium solution and it is asymptotically stable. Furthermore, $\forall$ solution $(x,y)$ of $(1)$ we have that $\lim x(t)=x_0$ and $\lim y(t)=y_0$ where $(x_0,y_0)$ equilibrium solution. there exists exactly one equilibrium solution and it is asymptotically stable. Furthermore, $\forall$ solution $(x,y)$ of $(1)$ we have that $\lim x(t) \neq x_0$ and $\lim y(t) \neq y_0$ where $(x_0,y_0)$ equilibrium solution. $\exists$ exactly one equilibrium solution and it is stable but not asymptotically stable. Also $\forall$ solution $(x,y) \neq (x_0, y_0)$ of $(1)$ we have that $\lim x(t) \neq x_0$ and $\lim y(t) \neq y_0$ where $(x_0,y_0)$ is the equilibrium solution. $\exists$ exactly one equilibrium solution and it is stable but not asymptotically stable. Also $\forall$ solution $(x,y) \neq (x_0, y_0)$ of $(1)$ with $x^2(0)+y^2(0)>1$ we have that $x^2(t)+y^2(t) \geq 1$. $\exists$ exactly one equilibrium solution and it is unstable. $\forall$ other solution $(x,y)$ of $(1)$ we have that $\lim (x^2(t)+y^2(t))=1$. I have thought the following so far. In order to find the equilibrium solutions, we set $\dot{x}=0$ and $\dot{y}=0$. $\dot{x}=0 \Rightarrow -y-x (1-\sqrt{x^2+y^2})^2=0 (\star)$ and $\dot{y}=0 \Rightarrow x-y (1-\sqrt{x^2+y^2})^2=0 \Rightarrow x=y(1-\sqrt{x^2+y^2})^2$ $(\star): -y-y(1-\sqrt{x^2+y^2})^4=0 \Rightarrow y=0 \text{ or } 1+(1-\sqrt{x^2+y^2})^4=0, \text{ which is rejected}$. So $y=0$ and $x=0$. So $(0,0)$ is the only equilibrium solution. If we set $f_1(x,y)=-y-x(1-\sqrt{x^2+y^2})^2$ and $f_2(x)=x-y(1-\sqrt{x^2+y^2})^2$, then we have $\frac{\partial{f}}{\partial{x}}=-(1-\sqrt{x^2+y^2})^2+\frac{2x^2(1-\sqrt{x^2+y^2})}{\sqrt{x^2+y^2}}$. Then we have $$\frac{\partial{f_1}}{\partial{x}}(0,0)=-1 \\ \frac{\partial{f_1}}{\partial{y}}(0,0)=-1  \\ \frac{\partial{f_2}}{\partial{x}}(0,0)=1 \\ \frac{\partial{f_2}}{\partial{y}}(0,0)=-1$$ and thus the Jacobi matrix at $(0,0)$ gets the following form: $J=\begin{pmatrix} -1 & -1\\  1 & -1 \end{pmatrix}$, right? Then we get that the eigenvalues are these ones: $-1 \pm i$ and so $Re(\lambda_1)=Re(\lambda_2)=-1<0$ which implies that the equilibrium is asymptotically unstable. So we have that there is exactly one equilibrium solution and it is asymptotically unstable, right? When $(x,t)$ is any other solution of the problem, what can we say about the limits $\lim x(t)$ and $\lim y(t)$ ?","I want to find the equilibrium solutions and determine their stability. $(1)\left\{\begin{matrix} \dot{x}=-y-x(1-\sqrt{x^2+y^2})^2\\  \dot{y}=x-y(1-\sqrt{x^2+y^2})^2 \end{matrix}\right.$ I also want to check the behavior of the solutions of $(1)$ when $t \to \infty$. There are five possible answers. there exists exactly one equilibrium solution and it is asymptotically stable. Furthermore, $\forall$ solution $(x,y)$ of $(1)$ we have that $\lim x(t)=x_0$ and $\lim y(t)=y_0$ where $(x_0,y_0)$ equilibrium solution. there exists exactly one equilibrium solution and it is asymptotically stable. Furthermore, $\forall$ solution $(x,y)$ of $(1)$ we have that $\lim x(t) \neq x_0$ and $\lim y(t) \neq y_0$ where $(x_0,y_0)$ equilibrium solution. $\exists$ exactly one equilibrium solution and it is stable but not asymptotically stable. Also $\forall$ solution $(x,y) \neq (x_0, y_0)$ of $(1)$ we have that $\lim x(t) \neq x_0$ and $\lim y(t) \neq y_0$ where $(x_0,y_0)$ is the equilibrium solution. $\exists$ exactly one equilibrium solution and it is stable but not asymptotically stable. Also $\forall$ solution $(x,y) \neq (x_0, y_0)$ of $(1)$ with $x^2(0)+y^2(0)>1$ we have that $x^2(t)+y^2(t) \geq 1$. $\exists$ exactly one equilibrium solution and it is unstable. $\forall$ other solution $(x,y)$ of $(1)$ we have that $\lim (x^2(t)+y^2(t))=1$. I have thought the following so far. In order to find the equilibrium solutions, we set $\dot{x}=0$ and $\dot{y}=0$. $\dot{x}=0 \Rightarrow -y-x (1-\sqrt{x^2+y^2})^2=0 (\star)$ and $\dot{y}=0 \Rightarrow x-y (1-\sqrt{x^2+y^2})^2=0 \Rightarrow x=y(1-\sqrt{x^2+y^2})^2$ $(\star): -y-y(1-\sqrt{x^2+y^2})^4=0 \Rightarrow y=0 \text{ or } 1+(1-\sqrt{x^2+y^2})^4=0, \text{ which is rejected}$. So $y=0$ and $x=0$. So $(0,0)$ is the only equilibrium solution. If we set $f_1(x,y)=-y-x(1-\sqrt{x^2+y^2})^2$ and $f_2(x)=x-y(1-\sqrt{x^2+y^2})^2$, then we have $\frac{\partial{f}}{\partial{x}}=-(1-\sqrt{x^2+y^2})^2+\frac{2x^2(1-\sqrt{x^2+y^2})}{\sqrt{x^2+y^2}}$. Then we have $$\frac{\partial{f_1}}{\partial{x}}(0,0)=-1 \\ \frac{\partial{f_1}}{\partial{y}}(0,0)=-1  \\ \frac{\partial{f_2}}{\partial{x}}(0,0)=1 \\ \frac{\partial{f_2}}{\partial{y}}(0,0)=-1$$ and thus the Jacobi matrix at $(0,0)$ gets the following form: $J=\begin{pmatrix} -1 & -1\\  1 & -1 \end{pmatrix}$, right? Then we get that the eigenvalues are these ones: $-1 \pm i$ and so $Re(\lambda_1)=Re(\lambda_2)=-1<0$ which implies that the equilibrium is asymptotically unstable. So we have that there is exactly one equilibrium solution and it is asymptotically unstable, right? When $(x,t)$ is any other solution of the problem, what can we say about the limits $\lim x(t)$ and $\lim y(t)$ ?",,['ordinary-differential-equations']
80,Prove that this differential equation has a unique solution defined on $\Bbb{R}^+$,Prove that this differential equation has a unique solution defined on,\Bbb{R}^+,"We consider the following O.D.E \begin{align}(a)\qquad\qquad\begin{cases}x'(t)=\dfrac{\rho^2(x(t))}{1+\rho^2(x(t))} & t\geq 0,\\x(0)=x_0\in \Bbb{R}&\end{cases}\end{align} where \begin{align}\rho:\Bbb{R}\to \Bbb{R}\end{align} is a $C^1$ function. I want to prove that $(a)$ has a unique solution defined on $\Bbb{R}^+?$ CURRENT PROOF Since $\rho:\Bbb{R}\to \Bbb{R}$ is $C^1$ function, $\rho^2$ is also a $C^1$ function. Also,  \begin{align}\dfrac{\rho^2}{1+\rho^2}\end{align} is a $C^1$ function. \begin{align}\rho^2(x(t))\leq 1+\rho^2(x(t)),\;\;\forall\,t \geq 0,\;x\in \Bbb{R}\end{align} \begin{align}f(x(t))=\dfrac{\rho^2(x(t))}{1+\rho^2(x(t))} \leq 1,\;\;\forall\,t \geq 0,\;x\in \Bbb{R}\end{align} This implies that $f$ is affine. So, $(a)$ has a unique maximal solution on $[0,t_{\max})$ and $t_{\max}=+\infty$. Therefore, $(a)$ has a unique solution defined on $\Bbb{R}^+?$ Kindly confirm if this proof is fine or not. If no, alternative proofs will be accepted. Thanks!","We consider the following O.D.E \begin{align}(a)\qquad\qquad\begin{cases}x'(t)=\dfrac{\rho^2(x(t))}{1+\rho^2(x(t))} & t\geq 0,\\x(0)=x_0\in \Bbb{R}&\end{cases}\end{align} where \begin{align}\rho:\Bbb{R}\to \Bbb{R}\end{align} is a $C^1$ function. I want to prove that $(a)$ has a unique solution defined on $\Bbb{R}^+?$ CURRENT PROOF Since $\rho:\Bbb{R}\to \Bbb{R}$ is $C^1$ function, $\rho^2$ is also a $C^1$ function. Also,  \begin{align}\dfrac{\rho^2}{1+\rho^2}\end{align} is a $C^1$ function. \begin{align}\rho^2(x(t))\leq 1+\rho^2(x(t)),\;\;\forall\,t \geq 0,\;x\in \Bbb{R}\end{align} \begin{align}f(x(t))=\dfrac{\rho^2(x(t))}{1+\rho^2(x(t))} \leq 1,\;\;\forall\,t \geq 0,\;x\in \Bbb{R}\end{align} This implies that $f$ is affine. So, $(a)$ has a unique maximal solution on $[0,t_{\max})$ and $t_{\max}=+\infty$. Therefore, $(a)$ has a unique solution defined on $\Bbb{R}^+?$ Kindly confirm if this proof is fine or not. If no, alternative proofs will be accepted. Thanks!",,"['ordinary-differential-equations', 'derivatives']"
81,Mixed formulation with Volterra integrals?,Mixed formulation with Volterra integrals?,,"I'm going straight to the point. When gathering Volterra equations of the second kind with mixed formulations, one can arrive to the following problem: Let $X,M$ Hilbert spaces and $\mathcal{J}=[0,T]$, with $T<\infty$. Find $(u,\lambda):\mathcal{J}\rightarrow X\times M$ such that \begin{equation} \label{linear-mixed-problem1} \begin{aligned} Au(t) +B'\lambda(t) &=f(t) + \int_{0}^{t}\big[\widehat{A}(t,\tau)u(\tau)+ \widehat{B}'(t,\tau)\lambda(\tau)\big]d\tau,\\ Bu(t)&= \int_{0}^{t}\widehat{B}(t,\tau)u(\tau)d\tau\hspace{0.1cm}, \end{aligned} \end{equation} a.e. in $\mathcal{J}$. Here $A\in \mathcal{L}(X;X'), B\in\mathcal{L}(X;M')$ and $B'\in\mathcal{L}(M;X')$ are  time-independent linear elliptic partial differential operators acting on an open bounded domain $\Omega\in \mathbb{R}^n$, with $A$ a self-adjoint operator and $B'$ the dual operator of $B$. The functions $f\in X'$ a.e. in $\mathcal{J}$ is a given load, and we assume that appropriate boundary conditions are given. The operators $\widehat{A}(t,\tau), \widehat{B}(t,\tau)$ and $\widehat{B}'(t,\tau)$ satisfies a ""similarity"" condition to $A,B$ and $B'$, respectively. Observe that a weak formulation can be achieved. The big question is: Is this problem make sense?. Any clue or references about how to attack it (well-posedness)?. Thanks.","I'm going straight to the point. When gathering Volterra equations of the second kind with mixed formulations, one can arrive to the following problem: Let $X,M$ Hilbert spaces and $\mathcal{J}=[0,T]$, with $T<\infty$. Find $(u,\lambda):\mathcal{J}\rightarrow X\times M$ such that \begin{equation} \label{linear-mixed-problem1} \begin{aligned} Au(t) +B'\lambda(t) &=f(t) + \int_{0}^{t}\big[\widehat{A}(t,\tau)u(\tau)+ \widehat{B}'(t,\tau)\lambda(\tau)\big]d\tau,\\ Bu(t)&= \int_{0}^{t}\widehat{B}(t,\tau)u(\tau)d\tau\hspace{0.1cm}, \end{aligned} \end{equation} a.e. in $\mathcal{J}$. Here $A\in \mathcal{L}(X;X'), B\in\mathcal{L}(X;M')$ and $B'\in\mathcal{L}(M;X')$ are  time-independent linear elliptic partial differential operators acting on an open bounded domain $\Omega\in \mathbb{R}^n$, with $A$ a self-adjoint operator and $B'$ the dual operator of $B$. The functions $f\in X'$ a.e. in $\mathcal{J}$ is a given load, and we assume that appropriate boundary conditions are given. The operators $\widehat{A}(t,\tau), \widehat{B}(t,\tau)$ and $\widehat{B}'(t,\tau)$ satisfies a ""similarity"" condition to $A,B$ and $B'$, respectively. Observe that a weak formulation can be achieved. The big question is: Is this problem make sense?. Any clue or references about how to attack it (well-posedness)?. Thanks.",,"['functional-analysis', 'ordinary-differential-equations', 'calculus-of-variations', 'convolution', 'finite-element-method']"
82,Trajectory of submarine chasing ship,Trajectory of submarine chasing ship,,here's my try First I notice that if I can define L for every point then I define the curve and from the figure $$L^2 = x^2 + (y_1-y_2)^2$$ where $y_1$ is the corresponding y coordinate of $S_1$ and $Y_2$ is the y coordinate of the curve So using the fact that $ y'(x)=\tan\theta$ -where $\theta$ is the angel made with the positive direction of the X axis- is the slope of the tangent line to the curve at any point so an equation for L is $y=y'(x)X$  Now my whole idea is to find two different representations for L and equate them to each other to get an equation I can solve and I want here to use the arc length formula..so differentiating the equation of L we get  $$y'= y''(x)X-y'(x)$$  now using the arc length formula  $$\int\sqrt {1+\left(y''(x)X-y'(x)\right)^2}dx$$..I differentiate that with respect to x to get $\sqrt{1+\left(y''(x)X-y'(x)\right)^2}$ Now I  find another formula for the length of L which I get by using the fact that $(y_1-y_2) =-x\tan\theta = -y'(x)X$ and from the pythagorean theorem  $$L =\sqrt {x^2 + (y'(x))^2x^2} = x\sqrt {1 + (y'(x))^2}$$ then differentiating this we get $$\sqrt {1 + (y'(x))^2} + \frac{xy''(x)}{\sqrt {1 + (y'(x))^2}}$$ then equating these two expressions for the derivative of the length of L we get $$\sqrt{1+\left(y''(x)X-y'(x)\right)^2} = \sqrt {1 + (y'(x))^2} + \frac{xy''(x)}{\sqrt {1 + (y'(x))^2}}$$ which I can't solve any way I know even after a lot of simplification I know I didn't use the stated fact that they're moving at constant speeds which may simplify it a little bit but I can't seem to relate the two I also can't see how using time as the independent variable here helps one last thing..I want some problem solving advice on how to deal with situations like these not related to this specific problem..situations where the complexity of the model makes it impractical or unsolvable Sorry I know this is quite long..Thanks in advance,here's my try First I notice that if I can define L for every point then I define the curve and from the figure $$L^2 = x^2 + (y_1-y_2)^2$$ where $y_1$ is the corresponding y coordinate of $S_1$ and $Y_2$ is the y coordinate of the curve So using the fact that $ y'(x)=\tan\theta$ -where $\theta$ is the angel made with the positive direction of the X axis- is the slope of the tangent line to the curve at any point so an equation for L is $y=y'(x)X$  Now my whole idea is to find two different representations for L and equate them to each other to get an equation I can solve and I want here to use the arc length formula..so differentiating the equation of L we get  $$y'= y''(x)X-y'(x)$$  now using the arc length formula  $$\int\sqrt {1+\left(y''(x)X-y'(x)\right)^2}dx$$..I differentiate that with respect to x to get $\sqrt{1+\left(y''(x)X-y'(x)\right)^2}$ Now I  find another formula for the length of L which I get by using the fact that $(y_1-y_2) =-x\tan\theta = -y'(x)X$ and from the pythagorean theorem  $$L =\sqrt {x^2 + (y'(x))^2x^2} = x\sqrt {1 + (y'(x))^2}$$ then differentiating this we get $$\sqrt {1 + (y'(x))^2} + \frac{xy''(x)}{\sqrt {1 + (y'(x))^2}}$$ then equating these two expressions for the derivative of the length of L we get $$\sqrt{1+\left(y''(x)X-y'(x)\right)^2} = \sqrt {1 + (y'(x))^2} + \frac{xy''(x)}{\sqrt {1 + (y'(x))^2}}$$ which I can't solve any way I know even after a lot of simplification I know I didn't use the stated fact that they're moving at constant speeds which may simplify it a little bit but I can't seem to relate the two I also can't see how using time as the independent variable here helps one last thing..I want some problem solving advice on how to deal with situations like these not related to this specific problem..situations where the complexity of the model makes it impractical or unsolvable Sorry I know this is quite long..Thanks in advance,,['ordinary-differential-equations']
83,Deciding the parameter value in a ODE system,Deciding the parameter value in a ODE system,,"I am working with a system that includes pharmacokinetic and pharmacodynamic elements. To model the antibiotic effect on bacteria I am using a commonly used Hill type function as,${(\phi_{max} - \phi _{min}){({A \over MIC})}^k}\over {{({A \over MIC})}^k-{\phi_{min} \over \phi_{max}}}$, where, $\phi_{max}$ – maximum bacteria growth rate in the absence of antibiotic which is only limited by resources, and so it i limited by the carrying capacity term as,  $\phi_{max}=r(1-{P \over k})$, where $k$ is the carrying capacity term and $r$ is the replication rate, and $P$ is the bacterial density that change over time. $\phi_{min}$ – minimum bacterial growth rate at high concentrations of the antibiotic. $MIC$-minimum inhibitory concentration of the antibiotic $k$ - Hill coefficient. $\phi_{min},k,MIC$ are all constant values. $A$ is the antibiotic concentration which decays exponentially so that ${dA\over dt}=-dA$ So, my question is, in almost all the articles that I found, the value of $\phi_{min}$ was selected as a negative value. But it is defined as a bacterial growth rate. So, how can a growth rate be negative? When this function was used in my model and the $\phi_{min}$ value was changed from, negative values to positive values, for negative values I get the desired behaviour (the pathogen get cleared). But, for some positive values, in Matlab I get a warning as, Unable to meet integration tolerances without reducing the step size below the smallest value allowed (1.818989e-12) at time t Could this be because, that it is possible to come up with a situation where, $\phi_{min}=\phi_{max}$ and ${({A \over MIC})}^k=1$, so that the above function reaches $0 \over 0$ case. I believe this does not happen when $\phi_{min}$ is negative, does it? In articles, they just chose low negative value for antibiotics that has high killing ability (e.g $\phi_{min}=-3$)and for lesser effective antibiotics $\phi_{min}=-0.01$. I would like to know if there is mathematical reason for choosing $\phi_{min}$ to be negative? Is my reasoning on $0 \over 0$ case reasonable?","I am working with a system that includes pharmacokinetic and pharmacodynamic elements. To model the antibiotic effect on bacteria I am using a commonly used Hill type function as,${(\phi_{max} - \phi _{min}){({A \over MIC})}^k}\over {{({A \over MIC})}^k-{\phi_{min} \over \phi_{max}}}$, where, $\phi_{max}$ – maximum bacteria growth rate in the absence of antibiotic which is only limited by resources, and so it i limited by the carrying capacity term as,  $\phi_{max}=r(1-{P \over k})$, where $k$ is the carrying capacity term and $r$ is the replication rate, and $P$ is the bacterial density that change over time. $\phi_{min}$ – minimum bacterial growth rate at high concentrations of the antibiotic. $MIC$-minimum inhibitory concentration of the antibiotic $k$ - Hill coefficient. $\phi_{min},k,MIC$ are all constant values. $A$ is the antibiotic concentration which decays exponentially so that ${dA\over dt}=-dA$ So, my question is, in almost all the articles that I found, the value of $\phi_{min}$ was selected as a negative value. But it is defined as a bacterial growth rate. So, how can a growth rate be negative? When this function was used in my model and the $\phi_{min}$ value was changed from, negative values to positive values, for negative values I get the desired behaviour (the pathogen get cleared). But, for some positive values, in Matlab I get a warning as, Unable to meet integration tolerances without reducing the step size below the smallest value allowed (1.818989e-12) at time t Could this be because, that it is possible to come up with a situation where, $\phi_{min}=\phi_{max}$ and ${({A \over MIC})}^k=1$, so that the above function reaches $0 \over 0$ case. I believe this does not happen when $\phi_{min}$ is negative, does it? In articles, they just chose low negative value for antibiotics that has high killing ability (e.g $\phi_{min}=-3$)and for lesser effective antibiotics $\phi_{min}=-0.01$. I would like to know if there is mathematical reason for choosing $\phi_{min}$ to be negative? Is my reasoning on $0 \over 0$ case reasonable?",,"['ordinary-differential-equations', 'matlab', 'mathematical-modeling', 'biology']"
84,Solving a non-homogeneous second-order differential equation with constant coefficients,Solving a non-homogeneous second-order differential equation with constant coefficients,,"I am given this equation: $$ y''(x) y(x) - 4 y(x) = -4e^{4x} $$ And I am asked to start off by finding the solution to the homogeneous equation. First, I am confused as to why the above equation is said to be constant coefficient. Wouldn't the $y''(x)$ being multiplied to $y(x)$ mean that it is not a constant coefficient equation? But, our teacher did not teach us how to solve for equation that are not constant coefficient, and strictly said that we will not look at those equations now. I went on to solve for the homogeneous solution by equating the right hand side of the equation to zero. I got the characteristic equation: $q^2 -4=0$, $\longrightarrow$ $q=+2$ or $-2$. So, i then got that the general homog. solution is: $Ae^{2t} + Be^{-2t}$, for $A,B$ constants. But, this is wrong.","I am given this equation: $$ y''(x) y(x) - 4 y(x) = -4e^{4x} $$ And I am asked to start off by finding the solution to the homogeneous equation. First, I am confused as to why the above equation is said to be constant coefficient. Wouldn't the $y''(x)$ being multiplied to $y(x)$ mean that it is not a constant coefficient equation? But, our teacher did not teach us how to solve for equation that are not constant coefficient, and strictly said that we will not look at those equations now. I went on to solve for the homogeneous solution by equating the right hand side of the equation to zero. I got the characteristic equation: $q^2 -4=0$, $\longrightarrow$ $q=+2$ or $-2$. So, i then got that the general homog. solution is: $Ae^{2t} + Be^{-2t}$, for $A,B$ constants. But, this is wrong.",,"['calculus', 'ordinary-differential-equations', 'homogeneous-equation']"
85,What mathematical prerequisites do I need to properly learn stochastic DEs?,What mathematical prerequisites do I need to properly learn stochastic DEs?,,"I'm a PhD student in biology, and I'm trying to teach myself the necessary math to get into mathematical modeling of the phenomenon we study, because this is an area of inquiry that 1) I'd like to get into and 2) the lab I work for may like to get into, and it'd be on my shoulders to do it. The phenomenon we study, among other things, has been modeled using delay differential equations and stochastic differential equations. I have courses under my belt in differential/integral calculus and am teaching myself differential equations, and possess the requisite computational background.  Multivariable calculus is next on my list.  What else do I need to know to get to a level where I can adequately make use of DDEs and SDEs?","I'm a PhD student in biology, and I'm trying to teach myself the necessary math to get into mathematical modeling of the phenomenon we study, because this is an area of inquiry that 1) I'd like to get into and 2) the lab I work for may like to get into, and it'd be on my shoulders to do it. The phenomenon we study, among other things, has been modeled using delay differential equations and stochastic differential equations. I have courses under my belt in differential/integral calculus and am teaching myself differential equations, and possess the requisite computational background.  Multivariable calculus is next on my list.  What else do I need to know to get to a level where I can adequately make use of DDEs and SDEs?",,"['ordinary-differential-equations', 'stochastic-calculus']"
86,Find a differential equation for geodesics..,Find a differential equation for geodesics..,,"A sphere in $\mathbb{R}^3$ is parametrically given by $$ x = a \sin(\theta) \cos(\phi) \\ y = a \sin(θ) \sin(\phi)\\ z = a \cos(θ) $$ where $a > 0$ is the radius of the sphere, $θ$ is the polar angle $(0 < θ < π)$ and $\phi$ is the azimuthal angle $(0 < \phi < 2π)$ . Find a differential equation for geodesics $\phi = \phi(\alpha) $ on the sphere. Show that for arbitrary constants $\phi_0$ and $\alpha$, the function $\phi(\theta) - \phi_0 = \sin^{-1} (\alpha \cos (\theta))$ satisfies the differential equation. My solution So, I think I know how the solution goes, however I am stuck on finding derivative in the middle, and after that I do not know how to proceed. So first we need basically to find geodesics. We do that knowing that $ds^2=dx^2+dy^2+dz^2$. Then we get that $ds^2 = a^2d\theta^2 + a^2\sin^2(\theta)\, d\phi^2$. (You can trust me with that). Hence we need to find a minimum of an integral  $$\int_A^B\sqrt{a^2d\theta^2 + a^2\sin^2(\theta)\, d\phi^2} =  a \int_A^B\sqrt{1+\sin^2(\theta)\phi^{'2}(x)}\,d\theta$$ since $\phi=\phi(\theta) \rightarrow d\phi = \phi'(\theta)\,d\theta$. Hence to find a minimum to that integral we need to find a solution to Euler Lagrange equation $\dfrac{dF}{d\phi} - \dfrac{d}{dx}\left(\dfrac{d\phi}{d\phi'}\right)=0$. $\dfrac{dF}{d\phi} =0$ since our equation is independent of $\phi$. So we get that $$\frac{dF}{d{\phi'}} =\text{const}$$ Now could anyone please help me to find  $$\frac{dF}{d{\phi'}}$$ with an explanation please.","A sphere in $\mathbb{R}^3$ is parametrically given by $$ x = a \sin(\theta) \cos(\phi) \\ y = a \sin(θ) \sin(\phi)\\ z = a \cos(θ) $$ where $a > 0$ is the radius of the sphere, $θ$ is the polar angle $(0 < θ < π)$ and $\phi$ is the azimuthal angle $(0 < \phi < 2π)$ . Find a differential equation for geodesics $\phi = \phi(\alpha) $ on the sphere. Show that for arbitrary constants $\phi_0$ and $\alpha$, the function $\phi(\theta) - \phi_0 = \sin^{-1} (\alpha \cos (\theta))$ satisfies the differential equation. My solution So, I think I know how the solution goes, however I am stuck on finding derivative in the middle, and after that I do not know how to proceed. So first we need basically to find geodesics. We do that knowing that $ds^2=dx^2+dy^2+dz^2$. Then we get that $ds^2 = a^2d\theta^2 + a^2\sin^2(\theta)\, d\phi^2$. (You can trust me with that). Hence we need to find a minimum of an integral  $$\int_A^B\sqrt{a^2d\theta^2 + a^2\sin^2(\theta)\, d\phi^2} =  a \int_A^B\sqrt{1+\sin^2(\theta)\phi^{'2}(x)}\,d\theta$$ since $\phi=\phi(\theta) \rightarrow d\phi = \phi'(\theta)\,d\theta$. Hence to find a minimum to that integral we need to find a solution to Euler Lagrange equation $\dfrac{dF}{d\phi} - \dfrac{d}{dx}\left(\dfrac{d\phi}{d\phi'}\right)=0$. $\dfrac{dF}{d\phi} =0$ since our equation is independent of $\phi$. So we get that $$\frac{dF}{d{\phi'}} =\text{const}$$ Now could anyone please help me to find  $$\frac{dF}{d{\phi'}}$$ with an explanation please.",,"['ordinary-differential-equations', 'derivatives']"
87,Can these equations be considered as differential equations?,Can these equations be considered as differential equations?,,"Consider a differential equation with  a term containing $y(x_0)$, for example $$y'' - 2y' + y = y(x_0)$$ $x_0 \in \mathbb{R}$ is a constant. My question is, does such equations fall under the category of differential equations? I have never studied any equation with such a term. If its a differential equation, then $y(x_0)$ can be considered a constant coefficient?","Consider a differential equation with  a term containing $y(x_0)$, for example $$y'' - 2y' + y = y(x_0)$$ $x_0 \in \mathbb{R}$ is a constant. My question is, does such equations fall under the category of differential equations? I have never studied any equation with such a term. If its a differential equation, then $y(x_0)$ can be considered a constant coefficient?",,"['real-analysis', 'ordinary-differential-equations']"
88,Is the following ODE solvable?,Is the following ODE solvable?,,"I would like to find solution to following ODE: $$\frac{1}{\sin^{n-1}(x)}\frac{d}{dx}(\sin^{n-1}(x)\frac{df}{dx})-\frac{(n-1)f}{\sin^2(x)}=0$$ where $0<x\leq1,\ n\geq 2$ and $f(0)=0$. Is there any general way to solve this kind ODE? what happens if I substitute $\sin(x)$ by general functions. Any hint and reference will be appreciated. Remark: As many of you have realised, this ODE comes from calculating the eigenfunctions of Laplacian on sphere, but actually, I care more about the following quantity (it is Steklov eigenvalue if you have seen it): $$\frac{f'}{f}\bigg|_{x=1}$$","I would like to find solution to following ODE: $$\frac{1}{\sin^{n-1}(x)}\frac{d}{dx}(\sin^{n-1}(x)\frac{df}{dx})-\frac{(n-1)f}{\sin^2(x)}=0$$ where $0<x\leq1,\ n\geq 2$ and $f(0)=0$. Is there any general way to solve this kind ODE? what happens if I substitute $\sin(x)$ by general functions. Any hint and reference will be appreciated. Remark: As many of you have realised, this ODE comes from calculating the eigenfunctions of Laplacian on sphere, but actually, I care more about the following quantity (it is Steklov eigenvalue if you have seen it): $$\frac{f'}{f}\bigg|_{x=1}$$",,"['ordinary-differential-equations', 'partial-differential-equations']"
89,Find the first moment of a probability distribution governed by a nonlinear first order ODE,Find the first moment of a probability distribution governed by a nonlinear first order ODE,,"May I ask if there is any standard way to find the first moment of a probability distribution governed by a nonlinear first-order ODE. For example, $$ \frac{\mathrm d p(x)}{\mathrm dx} = \alpha(x) p(x) $$ where $\alpha(x)$ is a non-linear function in terms of $x$. As far as I know, the most straightforward method is trying to solve the distribution $p(x)$ directly, and then find the first moment by  $$ \langle x\rangle = \int x p(x)\mathrm dx  $$ but sometimes the analytical form of this distribution is very hard to obtain, so instead of doing this is there other method?","May I ask if there is any standard way to find the first moment of a probability distribution governed by a nonlinear first-order ODE. For example, $$ \frac{\mathrm d p(x)}{\mathrm dx} = \alpha(x) p(x) $$ where $\alpha(x)$ is a non-linear function in terms of $x$. As far as I know, the most straightforward method is trying to solve the distribution $p(x)$ directly, and then find the first moment by  $$ \langle x\rangle = \int x p(x)\mathrm dx  $$ but sometimes the analytical form of this distribution is very hard to obtain, so instead of doing this is there other method?",,"['probability', 'ordinary-differential-equations', 'moment-generating-functions']"
90,Non piecewise $C^1$ solutions of conservation laws,Non piecewise  solutions of conservation laws,C^1,"I studied the following theorem:(Rankine-Hugoniot condition) Let $u:\mathbb{R} \times [0,+\infty) \rightarrow \mathbb{R} $ be a piecewise $C^1$ function. Then $u$ is a weak solution if and only if the two of the following conditions are satisfied: i) $u$ is a classical solution of in the domain where $u$ is $C^1$ ii) $u$ satisfies the jump condition $$(u_+ -u_-) \eta_t +\sum\limits_{j=1}^d{f_j(u_+) -f_j(u_-)} \eta_x=0$$ My doubts... i)Can we find an example of the  conservation law where solutions are not piecewise $C^1$(i.e the solution does not have a piece wise $C^1$ representative) so that we cannot apply Rankine-Hugoniot condition across the jump? ii) Is there any weaker versions of this theorem so that piecewise $C^1$ can be relaxed?","I studied the following theorem:(Rankine-Hugoniot condition) Let $u:\mathbb{R} \times [0,+\infty) \rightarrow \mathbb{R} $ be a piecewise $C^1$ function. Then $u$ is a weak solution if and only if the two of the following conditions are satisfied: i) $u$ is a classical solution of in the domain where $u$ is $C^1$ ii) $u$ satisfies the jump condition $$(u_+ -u_-) \eta_t +\sum\limits_{j=1}^d{f_j(u_+) -f_j(u_-)} \eta_x=0$$ My doubts... i)Can we find an example of the  conservation law where solutions are not piecewise $C^1$(i.e the solution does not have a piece wise $C^1$ representative) so that we cannot apply Rankine-Hugoniot condition across the jump? ii) Is there any weaker versions of this theorem so that piecewise $C^1$ can be relaxed?",,"['ordinary-differential-equations', 'partial-differential-equations', 'regularity-theory-of-pdes', 'hyperbolic-equations']"
91,Decay estimate for the heat equation: $\sup_{t>0}\int_{\mathbb{R}} t^\alpha |u_x|^2\ dx$,Decay estimate for the heat equation:,\sup_{t>0}\int_{\mathbb{R}} t^\alpha |u_x|^2\ dx,"Let $u$ be a solution of the heat equation $$u_t - u_{xx} = 0, \quad t>0, x \in \mathbb{R}$$ with initial data $u(0,\cdot) = u_0$. Fix $\alpha >0$. How can I estimate (without using explicitly the heat kernel) $$\sup_{t>0}\int_{\mathbb{R}} t^\alpha |u_x|^2 \  dx,$$ in terms of the initial data?  Could you point out a reference where such an estimate is obtained? Is it fair to call what we obtain a decay estimate?","Let $u$ be a solution of the heat equation $$u_t - u_{xx} = 0, \quad t>0, x \in \mathbb{R}$$ with initial data $u(0,\cdot) = u_0$. Fix $\alpha >0$. How can I estimate (without using explicitly the heat kernel) $$\sup_{t>0}\int_{\mathbb{R}} t^\alpha |u_x|^2 \  dx,$$ in terms of the initial data?  Could you point out a reference where such an estimate is obtained? Is it fair to call what we obtain a decay estimate?",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'reference-request', 'partial-differential-equations']"
92,An alternative Lyapunov-like instability proof,An alternative Lyapunov-like instability proof,,"Let $\gamma$ and $\omega$ be two positive real numbers. Consider the following 1-dimensional linear time-varying system $$ \dot{x}(t)=\underbrace{\left(-\frac{1}{2}(1+\gamma)-\cos(\omega t) + \frac{1}{2}\sqrt{4\cos^2(\omega t) +(1-\gamma)^2}\right)}_{=:a(t)}x(t), \quad x(0)\in\mathbb{R}. $$ By direct computation of the solution of the above differential equation, it is easy to check that, for $\gamma$ sufficiently small , the origin of the above system is an unstable equilibrium point. However, I'd like to prove instability of the origin via a Lyapunov approach. More precisely, supposing that $\gamma$ is sufficiently small , I'd like to find a function $v(x,t):=q(t)x(t)^2$ such that The norm of $q(t)$ is bounded in $t$, i.e. $\|q(t)\|\le k$, $k>0$, for all $t\ge 0$; The derivative of $v(x,t)$ along the trajectories of the system is strictly negative for all $t$, that is $2a(t)q(t)+\dot{q}(t)<0$ for all $t$; There exists (at least) a $\bar{t}>0$ such that $q(\bar{t})<0$. So my question is: Is it actually possible to find such a Lyapunov function $v(x,t)$? I made several attempts but no one worked. So every comment/suggestion would be greatly appreciated. Thanks!","Let $\gamma$ and $\omega$ be two positive real numbers. Consider the following 1-dimensional linear time-varying system $$ \dot{x}(t)=\underbrace{\left(-\frac{1}{2}(1+\gamma)-\cos(\omega t) + \frac{1}{2}\sqrt{4\cos^2(\omega t) +(1-\gamma)^2}\right)}_{=:a(t)}x(t), \quad x(0)\in\mathbb{R}. $$ By direct computation of the solution of the above differential equation, it is easy to check that, for $\gamma$ sufficiently small , the origin of the above system is an unstable equilibrium point. However, I'd like to prove instability of the origin via a Lyapunov approach. More precisely, supposing that $\gamma$ is sufficiently small , I'd like to find a function $v(x,t):=q(t)x(t)^2$ such that The norm of $q(t)$ is bounded in $t$, i.e. $\|q(t)\|\le k$, $k>0$, for all $t\ge 0$; The derivative of $v(x,t)$ along the trajectories of the system is strictly negative for all $t$, that is $2a(t)q(t)+\dot{q}(t)<0$ for all $t$; There exists (at least) a $\bar{t}>0$ such that $q(\bar{t})<0$. So my question is: Is it actually possible to find such a Lyapunov function $v(x,t)$? I made several attempts but no one worked. So every comment/suggestion would be greatly appreciated. Thanks!",,"['ordinary-differential-equations', 'analysis', 'dynamical-systems', 'stability-in-odes', 'lyapunov-functions']"
93,Differential equation involving inverse function theorem,Differential equation involving inverse function theorem,,"Pardon me if you find the question crazy. How do we solve the differential equation, $$\frac{df^{-1}(t)}{dt} \Biggm |_{f(t)} = \frac{1}{f(t)(1-f(t))}$$. without using the following approach: If we multiply LHS and RHS by $df$, by inverse function theorem we have for LHS, $$ \frac{df^{-1}(t)}{dt}  \Biggm |_{f(t)} df = dt $$ And thus, $$ \frac{df}{dt} = f(t)(1-f(t))$$ and we can solve this.","Pardon me if you find the question crazy. How do we solve the differential equation, $$\frac{df^{-1}(t)}{dt} \Biggm |_{f(t)} = \frac{1}{f(t)(1-f(t))}$$. without using the following approach: If we multiply LHS and RHS by $df$, by inverse function theorem we have for LHS, $$ \frac{df^{-1}(t)}{dt}  \Biggm |_{f(t)} df = dt $$ And thus, $$ \frac{df}{dt} = f(t)(1-f(t))$$ and we can solve this.",,['ordinary-differential-equations']
94,Fixed point method for non-homogeneous ode and pde,Fixed point method for non-homogeneous ode and pde,,"During my course in mathematical methods for physics, my professor introduced us to the ""method of fixed point"" for studying non-homogeneous systems of odes, odes and pdes. Although I think I understood how to use it, he wasn't so clear about WHERE to use it. Let me make some examples: $$\begin{align}&\left\{\begin{matrix}\dot{x}=3x-2y+f(t)\\\dot{y}=-2x+3y\end{matrix}\right.& f(t) = \left\{\begin{matrix}1\;\;\;\;0<t<1\\0\;\;\;\;t>1\end{matrix}\right.\end{align}$$$$\left\{\begin{matrix}\partial_{t}u = D\partial^{2}_{xx}u + f(x) \\ f(x) = \frac{d^2}{dx^2}e^{-ax^2}\\u(x,0)=e^{-ax^2}\end{matrix}\right.$$ $$\left\{\begin{matrix}\partial_{t}\phi = D\partial^{2}_{xx}\phi + \sin(2x)\\ 0<x<\pi\\ \phi(x,0) = \sin(x)+3\sin(8x) \\ \phi(0,t)=\phi(\pi,t)=0\end{matrix}\right.$$ Two of these problems, mainly the last two, I know for a fact that can be solved around their fixed points: $$\tilde{u}(x,t) = u(x,t) + {1\over D}e^{-ax^2}$$ and $$\tilde{\phi}(x,t) = \phi(x,t) + {1\over{4D}}sin(2x)$$ (even if the second one is pretty unnecessary). But for the first one I'm pretty confused. I know that that system of odes can be rewritten in the form $$\dot{\underline{x}} = \hat{A}\underline{x}+\underline{f}$$ which has solution $$\underline{x}(t) = \underline{x}(0)e^{\hat{A}t}+\int_{0}^{t}e^{\hat{A}(t-t')}\underline{f}(t')dt'$$ and from what I understood during classes I cannot study a non-homogeneous in the fixed point if the external force is a function of time , but in my notes I found that our professor studied the fist system with a variable $$\underline{z} = \underline{x}-\hat{A}^{-1}\underline{f}$$ which seems to me like a fixed point! Probably I'm confusing a lot but I cannot wrap my head around it. So coming to my question: is what I said in bold true or am I going crazy? Is it possible to study a problem like this one problem like this one with the fixed point method? Thank you very much in advance to anyone that can give me some kind of explanation. Sadly this course is very interesting but the limited time available makes it very hard to explain everything in detail.","During my course in mathematical methods for physics, my professor introduced us to the ""method of fixed point"" for studying non-homogeneous systems of odes, odes and pdes. Although I think I understood how to use it, he wasn't so clear about WHERE to use it. Let me make some examples: $$\begin{align}&\left\{\begin{matrix}\dot{x}=3x-2y+f(t)\\\dot{y}=-2x+3y\end{matrix}\right.& f(t) = \left\{\begin{matrix}1\;\;\;\;0<t<1\\0\;\;\;\;t>1\end{matrix}\right.\end{align}$$$$\left\{\begin{matrix}\partial_{t}u = D\partial^{2}_{xx}u + f(x) \\ f(x) = \frac{d^2}{dx^2}e^{-ax^2}\\u(x,0)=e^{-ax^2}\end{matrix}\right.$$ $$\left\{\begin{matrix}\partial_{t}\phi = D\partial^{2}_{xx}\phi + \sin(2x)\\ 0<x<\pi\\ \phi(x,0) = \sin(x)+3\sin(8x) \\ \phi(0,t)=\phi(\pi,t)=0\end{matrix}\right.$$ Two of these problems, mainly the last two, I know for a fact that can be solved around their fixed points: $$\tilde{u}(x,t) = u(x,t) + {1\over D}e^{-ax^2}$$ and $$\tilde{\phi}(x,t) = \phi(x,t) + {1\over{4D}}sin(2x)$$ (even if the second one is pretty unnecessary). But for the first one I'm pretty confused. I know that that system of odes can be rewritten in the form $$\dot{\underline{x}} = \hat{A}\underline{x}+\underline{f}$$ which has solution $$\underline{x}(t) = \underline{x}(0)e^{\hat{A}t}+\int_{0}^{t}e^{\hat{A}(t-t')}\underline{f}(t')dt'$$ and from what I understood during classes I cannot study a non-homogeneous in the fixed point if the external force is a function of time , but in my notes I found that our professor studied the fist system with a variable $$\underline{z} = \underline{x}-\hat{A}^{-1}\underline{f}$$ which seems to me like a fixed point! Probably I'm confusing a lot but I cannot wrap my head around it. So coming to my question: is what I said in bold true or am I going crazy? Is it possible to study a problem like this one problem like this one with the fixed point method? Thank you very much in advance to anyone that can give me some kind of explanation. Sadly this course is very interesting but the limited time available makes it very hard to explain everything in detail.",,"['ordinary-differential-equations', 'partial-differential-equations', 'fourier-analysis', 'systems-of-equations', 'fixed-point-theorems']"
95,Closed form for solution of ODE?,Closed form for solution of ODE?,,"Suppose that $g$ is defined over $\mathbb R$ and $$f'(x) = g(x) \, (g'(x))^2.$$ Is it possible to find a closed form for the indefinite integral $f$ as a function of $g$?","Suppose that $g$ is defined over $\mathbb R$ and $$f'(x) = g(x) \, (g'(x))^2.$$ Is it possible to find a closed form for the indefinite integral $f$ as a function of $g$?",,"['ordinary-differential-equations', 'indefinite-integrals']"
96,Solving a non-linear ordinary differential equation that includes $\tanh$ and $\cos$,Solving a non-linear ordinary differential equation that includes  and,\tanh \cos,"$$\big(A+B\tanh (Cy+D)\big)\frac{\mathrm dy}{\mathrm dx}+y=P\cos(\omega x),$$ where $A$, $B$, $C$, $D$ and $\omega$ are constant. I am really looking for an analytical solution to this differential equation. But I really don't know where to start. Can someone please help? Thank you.","$$\big(A+B\tanh (Cy+D)\big)\frac{\mathrm dy}{\mathrm dx}+y=P\cos(\omega x),$$ where $A$, $B$, $C$, $D$ and $\omega$ are constant. I am really looking for an analytical solution to this differential equation. But I really don't know where to start. Can someone please help? Thank you.",,"['ordinary-differential-equations', 'nonlinear-system']"
97,Equivalence between non-singular vector field without periodic orbits and the irrational flow on a torus.,Equivalence between non-singular vector field without periodic orbits and the irrational flow on a torus.,,"I'm reading the book ""The dynamics of vector Fields in dimension 3 - Matthias Moreno and Siddhartha Bhattacharya "" On page 6 the authors enunciate the following theorem: Theorem:(Poincaré, Denjoy:) Every non-singular $\mathcal{C}^2$ vector field on a compact surface   that has no periodic orbits is topologically equivalent to a linear   flow on a torus with the irrational slope. An interesting comment is that this result is not true for $\mathcal{C}^1$ vector fields. I searched for this result on the Internet but I didn't find any book/paper that demonstrates the above theorem. Does anyone know how to prove this or can give me a reference where I can learn the proof of the theorem? N.B. I only need to know how to demonstrate the theorem when the vector field is on $\mathbb{T}^2$.","I'm reading the book ""The dynamics of vector Fields in dimension 3 - Matthias Moreno and Siddhartha Bhattacharya "" On page 6 the authors enunciate the following theorem: Theorem:(Poincaré, Denjoy:) Every non-singular $\mathcal{C}^2$ vector field on a compact surface   that has no periodic orbits is topologically equivalent to a linear   flow on a torus with the irrational slope. An interesting comment is that this result is not true for $\mathcal{C}^1$ vector fields. I searched for this result on the Internet but I didn't find any book/paper that demonstrates the above theorem. Does anyone know how to prove this or can give me a reference where I can learn the proof of the theorem? N.B. I only need to know how to demonstrate the theorem when the vector field is on $\mathbb{T}^2$.",,"['ordinary-differential-equations', 'lie-groups', 'dynamical-systems', 'ergodic-theory']"
98,Help in understanding the demonstration of Seifert's Theorem (about Hopf Vector Field),Help in understanding the demonstration of Seifert's Theorem (about Hopf Vector Field),,"I'm reading the book ""The dynamics of vector fields in dimension 3"" - ""Matthias Moreno and Siddhartha Bhattacharya"". On page 10 the authors define Hopf flow on $\mathbb{S}^3$ : We identify $\mathbb{S}^3\subset \mathbb{C}^2$ with $\{ (z_1,z_2):|z_1|^2 + |z_2|^2 = 1\}$ , and we define the flow $\phi$ on $\mathbb{S}^3$ by $$\phi^t(z_1,z_2)= (e^{2i\pi t} z_1 , e^{2i\pi t} z_2). $$ The author doesn't write this in the book, but I think that the Hopf vector field is $H: \mathbb{S}^3 \to T \mathbb{S}^3$ ; $H(a+ib,c + id) = ( (a+ib, c+ id) , 2\pi(-b + i a ,-d + ic)$ . After this, the book does the following procedure (sorry for the big image). My problem is in the demonstration of the Lemma 1, I really don't understand what he did. My Doubts about the Lemma's demonstration: 1) I think in the underlined part in blue is $\pi^{-1} (C)$ instead of $C$ (because doesn't make sense an orbit of $X_i$ lies in $C$ ). 2) The underlined red part doesn't make much sense to me, how these $h$ 's are taken? Which periodic orbits were used to construct those Poincare's first return map? 3) And finally, about the green underlined text, anyone knows where can I find a demonstration of this property? Can anyone help me to understand this demonstration or propose another reference to I see the demonstration of this theorem (Seifert's Theorem)?","I'm reading the book ""The dynamics of vector fields in dimension 3"" - ""Matthias Moreno and Siddhartha Bhattacharya"". On page 10 the authors define Hopf flow on : We identify with , and we define the flow on by The author doesn't write this in the book, but I think that the Hopf vector field is ; . After this, the book does the following procedure (sorry for the big image). My problem is in the demonstration of the Lemma 1, I really don't understand what he did. My Doubts about the Lemma's demonstration: 1) I think in the underlined part in blue is instead of (because doesn't make sense an orbit of lies in ). 2) The underlined red part doesn't make much sense to me, how these 's are taken? Which periodic orbits were used to construct those Poincare's first return map? 3) And finally, about the green underlined text, anyone knows where can I find a demonstration of this property? Can anyone help me to understand this demonstration or propose another reference to I see the demonstration of this theorem (Seifert's Theorem)?","\mathbb{S}^3 \mathbb{S}^3\subset \mathbb{C}^2 \{ (z_1,z_2):|z_1|^2 + |z_2|^2 = 1\} \phi \mathbb{S}^3 \phi^t(z_1,z_2)= (e^{2i\pi t} z_1 , e^{2i\pi t} z_2).  H: \mathbb{S}^3 \to T \mathbb{S}^3 H(a+ib,c + id) = ( (a+ib, c+ id) , 2\pi(-b + i a ,-d + ic) \pi^{-1} (C) C X_i C h","['ordinary-differential-equations', 'dynamical-systems']"
99,confused about $\omega_{\pm}-$limit set of a subset X example,confused about limit set of a subset X example,\omega_{\pm}-,"I am reading Ordinary Differential Equations and Dynamical Systems by G.Theschl, and I am confused about the definition of the $\omega_{\pm}-$limit set of a set $X \subseteq M$.  The definition states that $\omega_{\pm}(X)$ is the set of all points $y \in M$ for which exists sequences $t_n \rightarrow \pm \infty$ and $x_n \in X$ with $\Phi(t_n,x_n) \rightarrow y$ Now, consider the system $\dot{x}=x(1-x^2)$, $\dot{y}=-y$. Clearly, the $x-$direction has two stable fixed points at $x=\pm1$ and an unstable fixed point at $x=0$. It is also clear that the $y-$direction has one stable fixed point at $y=0$. The author claims that $\omega_+(B_r(0))=[-1,1] \times\{0\} $, $r>0$. It is clear to me that $(0,0),(-1,0),(1,0)$ are in $\omega_+(B_r(0))$. However, I don't get why the whole interval $[-1,1]$ is also there. For a given point in $[-1,1] \times \{0\}$, what would be the two sequences $t_n$ and $x_n$ that satisfy the definition? I would really appreciate if someone can clarify this for me. Thanks!","I am reading Ordinary Differential Equations and Dynamical Systems by G.Theschl, and I am confused about the definition of the $\omega_{\pm}-$limit set of a set $X \subseteq M$.  The definition states that $\omega_{\pm}(X)$ is the set of all points $y \in M$ for which exists sequences $t_n \rightarrow \pm \infty$ and $x_n \in X$ with $\Phi(t_n,x_n) \rightarrow y$ Now, consider the system $\dot{x}=x(1-x^2)$, $\dot{y}=-y$. Clearly, the $x-$direction has two stable fixed points at $x=\pm1$ and an unstable fixed point at $x=0$. It is also clear that the $y-$direction has one stable fixed point at $y=0$. The author claims that $\omega_+(B_r(0))=[-1,1] \times\{0\} $, $r>0$. It is clear to me that $(0,0),(-1,0),(1,0)$ are in $\omega_+(B_r(0))$. However, I don't get why the whole interval $[-1,1]$ is also there. For a given point in $[-1,1] \times \{0\}$, what would be the two sequences $t_n$ and $x_n$ that satisfy the definition? I would really appreciate if someone can clarify this for me. Thanks!",,"['general-topology', 'ordinary-differential-equations', 'dynamical-systems']"
