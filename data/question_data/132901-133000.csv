,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Heat equation with discontinuous sink and zero flux boundary conditions,Heat equation with discontinuous sink and zero flux boundary conditions,,"I am trying to solve the following problem: $\frac{\partial p}{\partial t}=D\frac{\partial^2 p}{\partial r^2}-\lambda p,$ on the domain $r\in[0,r_w]$ and for times $t\in [0,\infty)$, where $ {\displaystyle \lambda={\begin{cases}\lambda_s,& \text{for } 0<r\leq r_s, \\ \lambda_w,& \text{for } r_s<r\leq r_w.\end{cases}}}$ The initial condition is a dirac delta function at the right hand end of the domain $p(r,0)=\delta(r-r_w),$ and the boundary conditions are zero flux at either end of the domain: $\frac{\partial p}{\partial r}|_{r=0}=0$ and $\frac{\partial p}{\partial r}|_{r=r_w}=0$. At the intermediate boundary I would like continuity and smoothness of the solution: $p_I(r_s)=p_O(r_s)$, and $\frac{\partial p_I}{\partial r}|_{r=r_s}=\frac{\partial p_O}{\partial r}|_{r=r_s}$, where $p_I$ and $p_O$ are the inner and outer solutions respectively. If there is a reason why these can't be satisfied, I would be interested to hear why. I have tried fourier series solutions for the two distinct parts of the domain but couldn't get the matching correct. Any help would be appreciated.","I am trying to solve the following problem: $\frac{\partial p}{\partial t}=D\frac{\partial^2 p}{\partial r^2}-\lambda p,$ on the domain $r\in[0,r_w]$ and for times $t\in [0,\infty)$, where $ {\displaystyle \lambda={\begin{cases}\lambda_s,& \text{for } 0<r\leq r_s, \\ \lambda_w,& \text{for } r_s<r\leq r_w.\end{cases}}}$ The initial condition is a dirac delta function at the right hand end of the domain $p(r,0)=\delta(r-r_w),$ and the boundary conditions are zero flux at either end of the domain: $\frac{\partial p}{\partial r}|_{r=0}=0$ and $\frac{\partial p}{\partial r}|_{r=r_w}=0$. At the intermediate boundary I would like continuity and smoothness of the solution: $p_I(r_s)=p_O(r_s)$, and $\frac{\partial p_I}{\partial r}|_{r=r_s}=\frac{\partial p_O}{\partial r}|_{r=r_s}$, where $p_I$ and $p_O$ are the inner and outer solutions respectively. If there is a reason why these can't be satisfied, I would be interested to hear why. I have tried fourier series solutions for the two distinct parts of the domain but couldn't get the matching correct. Any help would be appreciated.",,"['ordinary-differential-equations', 'partial-differential-equations']"
1,How to prove an ordinary differential equation is not separable?,How to prove an ordinary differential equation is not separable?,,"Some elementary exercises require one to determine whether or not an ordinary differential equation is separable. For example, it is understood that the equation $$y'=\frac{1-2xy}{x^2}$$ is not separable. An easier example is $y'=x+y$. Usually this is intuitive understandable; however, can one give a strict proof that the right hand of these equations cannot be written as a product of the form $p(y)q(x)$? I am aware of this question , but am not sure if the method mentioned in one of the answers is a good approach.","Some elementary exercises require one to determine whether or not an ordinary differential equation is separable. For example, it is understood that the equation $$y'=\frac{1-2xy}{x^2}$$ is not separable. An easier example is $y'=x+y$. Usually this is intuitive understandable; however, can one give a strict proof that the right hand of these equations cannot be written as a product of the form $p(y)q(x)$? I am aware of this question , but am not sure if the method mentioned in one of the answers is a good approach.",,"['ordinary-differential-equations', 'functions']"
2,Why Is the Solution of a Linear Nonhomogeneous Constant-Coefficient Differential Equation the Sum of a Particular and Homogeneous Solution?,Why Is the Solution of a Linear Nonhomogeneous Constant-Coefficient Differential Equation the Sum of a Particular and Homogeneous Solution?,,"Why Is the Solution of a Linear Nonhomogeneous Constant-Coefficient Differential Equation the Sum of a Particular and Homogeneous Solution? My textbook gives the following proof: Let $x_c(t)$ be a solution of the homogeneous equation and $x_p(t)$ be a solution of the particular equation. $a\dfrac{d^2x}{dt^2} + b\dfrac{dx}{dt} + cx = a\left( \dfrac{d^2x_c}{dt^2} + \dfrac{d^2x_p}{dt^2} \right) + b\left( \dfrac{dx_c}{dt} + \dfrac{dx_p}{dt} \right) + c(x_c + x_p)$ $= \left( a\dfrac{d^2x_c}{dt^2} + b\dfrac{dx_c}{dt} + cx_c \right) + \left( a\dfrac{d^2x_p}{dt^2} + b\dfrac{dx_p}{dt} + cx_p \right)$ $= 0 + f(t) = f(t) $ I don't see how this proves that a homogeneous solution and a particular solution are required to find the solution of a linear non homogeneous constant-coefficient differential equation? In fact, the proof shows that we end up with $= 0 + f(t) = f(t) $ where $f(t)$ is what we were seeking all along. Therefore, since $\left( a\dfrac{d^2x_c}{dt^2} + b\dfrac{dx_c}{dt} + cx_c \right) = 0$, what was the point of the solution to the homogeneous equation? It seems that the homogeneous solution was redundant from the beginning, since only the particular solution, $\left( a\dfrac{d^2x_p}{dt^2} + b\dfrac{dx_p}{dt} + cx_p \right)$, contributed towards the solution of the linear non homogeneous constant-coefficient differential equation ($\left( a\dfrac{d^2x_p}{dt^2} + b\dfrac{dx_p}{dt} + cx_p \right) = f(t)$). I would greatly appreciate it if people could please take the time to clarify concept. EDIT I've received 3 answers; none of which address my question or do it sufficiently. My question is very clear: How does the above calculation show that both a particular solution and a homogeneous solution are required for a general solution? I then mentioned that the homogeneous solutions ends up equating to $0$ and the particular solution equates to $f(x)$; in other words,  it seems that the particular solution was the only thing required to get the general solution $f(x)$, which seemingly makes the homogeneous solution redundant? What am I misunderstanding? Given the poor response it has received, I am going to request that a moderator delete this question.","Why Is the Solution of a Linear Nonhomogeneous Constant-Coefficient Differential Equation the Sum of a Particular and Homogeneous Solution? My textbook gives the following proof: Let $x_c(t)$ be a solution of the homogeneous equation and $x_p(t)$ be a solution of the particular equation. $a\dfrac{d^2x}{dt^2} + b\dfrac{dx}{dt} + cx = a\left( \dfrac{d^2x_c}{dt^2} + \dfrac{d^2x_p}{dt^2} \right) + b\left( \dfrac{dx_c}{dt} + \dfrac{dx_p}{dt} \right) + c(x_c + x_p)$ $= \left( a\dfrac{d^2x_c}{dt^2} + b\dfrac{dx_c}{dt} + cx_c \right) + \left( a\dfrac{d^2x_p}{dt^2} + b\dfrac{dx_p}{dt} + cx_p \right)$ $= 0 + f(t) = f(t) $ I don't see how this proves that a homogeneous solution and a particular solution are required to find the solution of a linear non homogeneous constant-coefficient differential equation? In fact, the proof shows that we end up with $= 0 + f(t) = f(t) $ where $f(t)$ is what we were seeking all along. Therefore, since $\left( a\dfrac{d^2x_c}{dt^2} + b\dfrac{dx_c}{dt} + cx_c \right) = 0$, what was the point of the solution to the homogeneous equation? It seems that the homogeneous solution was redundant from the beginning, since only the particular solution, $\left( a\dfrac{d^2x_p}{dt^2} + b\dfrac{dx_p}{dt} + cx_p \right)$, contributed towards the solution of the linear non homogeneous constant-coefficient differential equation ($\left( a\dfrac{d^2x_p}{dt^2} + b\dfrac{dx_p}{dt} + cx_p \right) = f(t)$). I would greatly appreciate it if people could please take the time to clarify concept. EDIT I've received 3 answers; none of which address my question or do it sufficiently. My question is very clear: How does the above calculation show that both a particular solution and a homogeneous solution are required for a general solution? I then mentioned that the homogeneous solutions ends up equating to $0$ and the particular solution equates to $f(x)$; in other words,  it seems that the particular solution was the only thing required to get the general solution $f(x)$, which seemingly makes the homogeneous solution redundant? What am I misunderstanding? Given the poor response it has received, I am going to request that a moderator delete this question.",,"['ordinary-differential-equations', 'proof-explanation']"
3,Find the solution of the pde $xu_y-yu_x=u$.,Find the solution of the pde .,xu_y-yu_x=u,"I am trying to find the equation of characteristic curves and solution of the partial differential equation $$x\frac{\partial u}{\partial y}-y\frac{\partial u}{\partial x}=u$$ $u(x,0)=\sin(\frac{\pi}{4}x)$ . What I did: $$\frac{dx}{-y}=\frac{dy}{x}=\frac{du}{u}$$ with  From first two equalities $$\frac{dx}{-y}=\frac{dy}{x}$$ by solving we get $x^2+y^2=c_1$ for some constant $c_1$. Now I am stuck with the third and rest of the equalities to come up with a suitable manipulation to solve the problem. How can I do this? Any help would be great. Thanks.","I am trying to find the equation of characteristic curves and solution of the partial differential equation $$x\frac{\partial u}{\partial y}-y\frac{\partial u}{\partial x}=u$$ $u(x,0)=\sin(\frac{\pi}{4}x)$ . What I did: $$\frac{dx}{-y}=\frac{dy}{x}=\frac{du}{u}$$ with  From first two equalities $$\frac{dx}{-y}=\frac{dy}{x}$$ by solving we get $x^2+y^2=c_1$ for some constant $c_1$. Now I am stuck with the third and rest of the equalities to come up with a suitable manipulation to solve the problem. How can I do this? Any help would be great. Thanks.",,"['ordinary-differential-equations', 'partial-differential-equations']"
4,To show the zeros of functions are distinct,To show the zeros of functions are distinct,,Show that the zeros of the functions $a \sin(x) + b \cos(x)$ and $c \sin(x)  + d \cos(x)$ are distinct and occur alternately if $ad-bc \ne 0$. I guess I need to use Sturm separation theorem. I need to show that both these functions are linearly independent. This gives me the hint to use wronskian. Is this much enough to show this?,Show that the zeros of the functions $a \sin(x) + b \cos(x)$ and $c \sin(x)  + d \cos(x)$ are distinct and occur alternately if $ad-bc \ne 0$. I guess I need to use Sturm separation theorem. I need to show that both these functions are linearly independent. This gives me the hint to use wronskian. Is this much enough to show this?,,['ordinary-differential-equations']
5,"Prove that if $f(t)$ satisfies $f(t)=f'(t)$, then $f(t+s)=f(t)f(s)$ for all $t,s\in R$ without using the fact that $f(t)=c\cdot e^t$.","Prove that if  satisfies , then  for all  without using the fact that .","f(t) f(t)=f'(t) f(t+s)=f(t)f(s) t,s\in R f(t)=c\cdot e^t","This seems like it should be super simple but I can't figure it out. Prove that if $f(t)$ satisfies $f(t)=f'(t)$, then $f(t+s)=f(t)f(s)$ for all $t,s$ belonging to R without using the fact that $f(t)=c\cdot e^t$. The only hint I'm given is that I can use the fact that two functions satisfying the same ODE $x'=x$ with the same initial condition are equal. Any ideas?","This seems like it should be super simple but I can't figure it out. Prove that if $f(t)$ satisfies $f(t)=f'(t)$, then $f(t+s)=f(t)f(s)$ for all $t,s$ belonging to R without using the fact that $f(t)=c\cdot e^t$. The only hint I'm given is that I can use the fact that two functions satisfying the same ODE $x'=x$ with the same initial condition are equal. Any ideas?",,['ordinary-differential-equations']
6,How to solve $y''+y=x^2$?,How to solve ?,y''+y=x^2,"I need to solve: $$y''+y=x^2$$ Taking the Laplace transform  (and using the fact that it is a linear operator) on both sides I get: $$\mathscr{L}(y)=\frac{2}{s^3(s^2+1)}+y(0)\frac{s}{s^2+1}+y'(0)\frac{1}{s^2+1}$$ And hence: $$y=2G(x)+y(0)\cos x +y'(0)\sin x$$ Where $G(x)$ is the inverse Laplace transform of: $$\frac{1}{s^3(s^2+1)}$$ My question is how do I find this inverse Laplace transform, I'm used to splitting the fraction into partial fractions but I don't think I'm used to doing a partial fraction like in the above.","I need to solve: $$y''+y=x^2$$ Taking the Laplace transform  (and using the fact that it is a linear operator) on both sides I get: $$\mathscr{L}(y)=\frac{2}{s^3(s^2+1)}+y(0)\frac{s}{s^2+1}+y'(0)\frac{1}{s^2+1}$$ And hence: $$y=2G(x)+y(0)\cos x +y'(0)\sin x$$ Where $G(x)$ is the inverse Laplace transform of: $$\frac{1}{s^3(s^2+1)}$$ My question is how do I find this inverse Laplace transform, I'm used to splitting the fraction into partial fractions but I don't think I'm used to doing a partial fraction like in the above.",,"['ordinary-differential-equations', 'laplace-transform']"
7,"Can a figure 8 be an orbit of $dx /dt =f(x,y)$, $dy/ dt =g(x,y)$ where $f$ and $g$ have continuous partial derivatives with respect to $x$ and $y$?","Can a figure 8 be an orbit of ,  where  and  have continuous partial derivatives with respect to  and ?","dx /dt =f(x,y) dy/ dt =g(x,y) f g x y","Can a figure 8 ever be an orbit of \begin{align} \frac{dx}{dt} & =f(x,y), \\[10pt] \frac{dy}{dt} & =g(x,y) \end{align} where $f$ and $g$ have continuous partial derivatives with respect to $x$ and $y$?","Can a figure 8 ever be an orbit of \begin{align} \frac{dx}{dt} & =f(x,y), \\[10pt] \frac{dy}{dt} & =g(x,y) \end{align} where $f$ and $g$ have continuous partial derivatives with respect to $x$ and $y$?",,['ordinary-differential-equations']
8,How exactly do you solve the ODE : $\dot x = -\sqrt{|x|}$,How exactly do you solve the ODE :,\dot x = -\sqrt{|x|},How do you deal with $ \dot x = -\sqrt{|x|}$ The square root is irksome and the absolute value inside raises serious eyebrows. The function is continuous but has sort of a nondifferentiable point at zero. No one knows how to visualize this function. Proceeding as usual is a deadend: $\dfrac{dx}{\sqrt{|x|}} = -dt$ What to do?,How do you deal with $ \dot x = -\sqrt{|x|}$ The square root is irksome and the absolute value inside raises serious eyebrows. The function is continuous but has sort of a nondifferentiable point at zero. No one knows how to visualize this function. Proceeding as usual is a deadend: $\dfrac{dx}{\sqrt{|x|}} = -dt$ What to do?,,['ordinary-differential-equations']
9,How to plot a phase portrait for this system of differential equations?,How to plot a phase portrait for this system of differential equations?,,"I beg your help.. I'd like the phase portrait for this system: \begin{aligned} \frac{dx}{dt} &= x (7-x-2y) \\ \frac{dy}{dt} &=  y (5-y-x) \end{aligned} I don't know how to use Mathematica/Matlab ... :( If anyone can make this portrait and post a print screen here, I would thank you very much..","I beg your help.. I'd like the phase portrait for this system: \begin{aligned} \frac{dx}{dt} &= x (7-x-2y) \\ \frac{dy}{dt} &=  y (5-y-x) \end{aligned} I don't know how to use Mathematica/Matlab ... :( If anyone can make this portrait and post a print screen here, I would thank you very much..",,"['ordinary-differential-equations', 'graphing-functions', 'matlab', 'mathematica']"
10,Equilibrium solutions of differential equations,Equilibrium solutions of differential equations,,Find the equilibrium solutions of the following differential equation: $$\dfrac{dy}{dt} = \dfrac{(t^2 - 1)(y^2 - 2)}{(y^2 -4)}$$ I'm not sure how to go about doing this since t appears explicitly on the right hand side. Would $y = \sqrt{2}$ or $-\sqrt{2}$ be solutions?,Find the equilibrium solutions of the following differential equation: $$\dfrac{dy}{dt} = \dfrac{(t^2 - 1)(y^2 - 2)}{(y^2 -4)}$$ I'm not sure how to go about doing this since t appears explicitly on the right hand side. Would $y = \sqrt{2}$ or $-\sqrt{2}$ be solutions?,,['ordinary-differential-equations']
11,Why does the differential equation $y' = y + 1$ have solution $y(x) = Ce^x - 1$?,Why does the differential equation  have solution ?,y' = y + 1 y(x) = Ce^x - 1,I was watching a video on differential equations for a class that I'm taking. I took calculus so long ago that I can't seem to figure why the differential equation  $y' = y + 1$ has solution $y(x) = Ce^x - 1$.,I was watching a video on differential equations for a class that I'm taking. I took calculus so long ago that I can't seem to figure why the differential equation  $y' = y + 1$ has solution $y(x) = Ce^x - 1$.,,['ordinary-differential-equations']
12,How $\frac{dx}{dy}=f(x)g(y) \Leftrightarrow \int \frac{dx}{f(x)} = \int g(y)dy$?,How ?,\frac{dx}{dy}=f(x)g(y) \Leftrightarrow \int \frac{dx}{f(x)} = \int g(y)dy,"In my intro differential equations class we have often used the ""equivalence"" stated in title. It seems to me that somehow, the intermediate step $$ \frac{dx}{f(x)} = g(y)dy$$ is being used, in which case $dx$ and $dy$ are being used as numbers from a fraction! The equivalence is intuitively clear to me, but I would like to know what the justification for this informal process is. How would one give rigor to this equivalence? To be clear, I am looking for a formal treatment as well as some motivation as to why we can work with $dx$'s in this manner.","In my intro differential equations class we have often used the ""equivalence"" stated in title. It seems to me that somehow, the intermediate step $$ \frac{dx}{f(x)} = g(y)dy$$ is being used, in which case $dx$ and $dy$ are being used as numbers from a fraction! The equivalence is intuitively clear to me, but I would like to know what the justification for this informal process is. How would one give rigor to this equivalence? To be clear, I am looking for a formal treatment as well as some motivation as to why we can work with $dx$'s in this manner.",,"['calculus', 'ordinary-differential-equations', 'nonstandard-analysis', 'infinitesimals']"
13,Can someone show me a proof of the general solution for 2nd order homogenous linear differential equations?,Can someone show me a proof of the general solution for 2nd order homogenous linear differential equations?,,"I thought I had it figured out but there's a sort of 'leap of faith' at a pivotal point that annoys me.  Can someone show me how to derive the general solution of an equation such as: $a\frac{d^2y}{dx^2}+b\frac{dy}{dx}+cy=0$ I want to avoid just saying, ""let's assume the solution is of the form $y=e^{mx}$"".  I want to see that form jumping out of an algebraic explanation. I know I'll get $y=Ae^{\alpha x}+Be^{\beta x}$ when I have 2 distinct roots, but explaining the general form for when I have two equal roots requires me to use this 'leap of faith' part.","I thought I had it figured out but there's a sort of 'leap of faith' at a pivotal point that annoys me.  Can someone show me how to derive the general solution of an equation such as: $a\frac{d^2y}{dx^2}+b\frac{dy}{dx}+cy=0$ I want to avoid just saying, ""let's assume the solution is of the form $y=e^{mx}$"".  I want to see that form jumping out of an algebraic explanation. I know I'll get $y=Ae^{\alpha x}+Be^{\beta x}$ when I have 2 distinct roots, but explaining the general form for when I have two equal roots requires me to use this 'leap of faith' part.",,"['calculus', 'ordinary-differential-equations']"
14,Differential equation $y''=\frac{1}{y^2}$,Differential equation,y''=\frac{1}{y^2},Please help me to solve the differential equation $$y''=\frac{1}{y^2}$$ Thank you very much!,Please help me to solve the differential equation $$y''=\frac{1}{y^2}$$ Thank you very much!,,['ordinary-differential-equations']
15,Solving $\frac{dy}{dx} = -xy$,Solving,\frac{dy}{dx} = -xy,"I'm trying to solve the differential equation $\frac{dy}{dx} = -xy$. So far I've got: $\frac{dy}{dx} = -xy$ $-xy\;dx = dy$ $\frac{1}{y}\;dy = -x\;dx$ $\ln(|y|) = -\frac{1}{2}x^2 + c$ (I combined both integration constants into one) $y = e^c e^{-\frac{1}{2}x^2}$ or $y = -e^c e^{-\frac{1}{2}x^2}$ It looks correct, but I'm missing one particular case: $y = 0$. $e^c$ is always positive, and thus is $-e^c$ always negative. Using both, I've included all solutions except $y = 0$ . For $y = 0$, the differential equation does hold though: $\frac{dy}{dx} = 0$ for any $x$, and $-xy = 0$ for any $x$ as well. What am I missing in my computation?","I'm trying to solve the differential equation $\frac{dy}{dx} = -xy$. So far I've got: $\frac{dy}{dx} = -xy$ $-xy\;dx = dy$ $\frac{1}{y}\;dy = -x\;dx$ $\ln(|y|) = -\frac{1}{2}x^2 + c$ (I combined both integration constants into one) $y = e^c e^{-\frac{1}{2}x^2}$ or $y = -e^c e^{-\frac{1}{2}x^2}$ It looks correct, but I'm missing one particular case: $y = 0$. $e^c$ is always positive, and thus is $-e^c$ always negative. Using both, I've included all solutions except $y = 0$ . For $y = 0$, the differential equation does hold though: $\frac{dy}{dx} = 0$ for any $x$, and $-xy = 0$ for any $x$ as well. What am I missing in my computation?",,"['calculus', 'ordinary-differential-equations']"
16,Solving the Nonlinear Second-Order ODE $u'' - u^2 + 9 = 0$,Solving the Nonlinear Second-Order ODE,u'' - u^2 + 9 = 0,"This is the one question from my prelim that still stumps me. Consider the nonlinear ordinary differential equation   $$ u'' - u^2 + 9 = 0. $$ Convert the equation to a system of first order equations and sketch some representative solutions of this first order system. Identify all constant solutions in your sketch. Find a vector field which is orthogonal to the trajectories of your first order system. Find a scalar function on the plane whose gradient is the vector field you found in (2). I don't have much experience with nonlinear ODE's, so I went in circles on my exam. The constant solutions are $u = \pm 3$. I'd like to know the general process to solve these sorts of equations, but interesting tricks and ""sleights of mind"" would also be great to see.","This is the one question from my prelim that still stumps me. Consider the nonlinear ordinary differential equation   $$ u'' - u^2 + 9 = 0. $$ Convert the equation to a system of first order equations and sketch some representative solutions of this first order system. Identify all constant solutions in your sketch. Find a vector field which is orthogonal to the trajectories of your first order system. Find a scalar function on the plane whose gradient is the vector field you found in (2). I don't have much experience with nonlinear ODE's, so I went in circles on my exam. The constant solutions are $u = \pm 3$. I'd like to know the general process to solve these sorts of equations, but interesting tricks and ""sleights of mind"" would also be great to see.",,"['ordinary-differential-equations', 'multivariable-calculus']"
17,Inverse Operator Methods for Differential Equations,Inverse Operator Methods for Differential Equations,,"We have learned in class how to use inverse operator methods to solve ODE's (i.e. with the symbolic $D$). E.g, If I were asked to find a particular solution, $y_p$ to $(D-1)(D-2)[y] = x^{2}e^{x}$, then I would use the formula $\frac{g(x)}{D-a} = e^{ax} \int e^{-ax} g(x) dx$, where $g(x) = x^2 e^x$. However, here's the difficulty I'm facing; all the ODE's that we have learned in class with operator methods have been dealing with constant coefficient ODEs, what happens when you have variable coefficients? E.g. the ODE $xy'' +(2x -1)y' - 2y = x^2$ factorises to $(xD-1)(D+2)[y] = x^2$, so a particular solution $y_p$ should be $y_p$ = $\frac{x^2}{(xD-1)(D+2)}$. I can use the method outlined above for the $D+2$ bit, but what about $\frac{1}{xD-1}$? What does the inverse operator $\frac{1}{xD-1}$ mean? Thanks, Ben","We have learned in class how to use inverse operator methods to solve ODE's (i.e. with the symbolic $D$). E.g, If I were asked to find a particular solution, $y_p$ to $(D-1)(D-2)[y] = x^{2}e^{x}$, then I would use the formula $\frac{g(x)}{D-a} = e^{ax} \int e^{-ax} g(x) dx$, where $g(x) = x^2 e^x$. However, here's the difficulty I'm facing; all the ODE's that we have learned in class with operator methods have been dealing with constant coefficient ODEs, what happens when you have variable coefficients? E.g. the ODE $xy'' +(2x -1)y' - 2y = x^2$ factorises to $(xD-1)(D+2)[y] = x^2$, so a particular solution $y_p$ should be $y_p$ = $\frac{x^2}{(xD-1)(D+2)}$. I can use the method outlined above for the $D+2$ bit, but what about $\frac{1}{xD-1}$? What does the inverse operator $\frac{1}{xD-1}$ mean? Thanks, Ben",,[]
18,Looking for help solving a second-order linear differential equation and guidance on a specific method,Looking for help solving a second-order linear differential equation and guidance on a specific method,,"I am currently working on the differential equation: $$ x \frac{d^2y}{dx^2} + \frac{1}{2} \frac{dy}{dx}+ \frac{1}{4} y = 0 $$ During a brief discussion with an expert, I was advised to try expressing the solution in the form $y=f\cdot g$ , where $f=x^n$ . The suggestion was to rewrite the equation in terms of $g$ . However, I am struggling to find relevant literature or references on this method. I would greatly appreciate any insights, guidance, or references that can help me understand and apply this approach. If anyone is familiar with this method or has encountered similar problems, kindly share your knowledge and recommend any books or resources that may assist in solving such equations. Thank you in advance for your assistance!","I am currently working on the differential equation: During a brief discussion with an expert, I was advised to try expressing the solution in the form , where . The suggestion was to rewrite the equation in terms of . However, I am struggling to find relevant literature or references on this method. I would greatly appreciate any insights, guidance, or references that can help me understand and apply this approach. If anyone is familiar with this method or has encountered similar problems, kindly share your knowledge and recommend any books or resources that may assist in solving such equations. Thank you in advance for your assistance!","
x \frac{d^2y}{dx^2} + \frac{1}{2} \frac{dy}{dx}+ \frac{1}{4} y = 0
 y=f\cdot g f=x^n g",['ordinary-differential-equations']
19,Neumann series of the derivative operator,Neumann series of the derivative operator,,"Consider the Nemuann series of the $D=\frac{d}{dx}$ operator: $T = 1 + D+ D^2+...$ Applying T on $x$ yields $x+1$ (i.e., $Tx=x+1$ ) for $x \in R$ . I would appreciate if someone helps me to resolve the following confusion. Since the sum above converges, we can rewrite $T$ as $\frac{1}{1-D}$ . Now, lets compute $Tx$ as follows: $Tx=(1 + D+ D^2+...)x=\frac{1}{1-D}x\equiv y \implies y - Dy = x \implies y-y^{'}=x,$ which is a first order differential equation with $y(x)=x+1-ce^{x}$ as the general solution, where $c$ is a constant. The question : why two different solutions? I understand that the first one is a specific case of the second general solution (with $c=0$ ). My confusion is that I was expecting to get one solution no matter which method I use. This may sound very basic but I cannot wrap my head around it. Thanks.","Consider the Nemuann series of the operator: Applying T on yields (i.e., ) for . I would appreciate if someone helps me to resolve the following confusion. Since the sum above converges, we can rewrite as . Now, lets compute as follows: which is a first order differential equation with as the general solution, where is a constant. The question : why two different solutions? I understand that the first one is a specific case of the second general solution (with ). My confusion is that I was expecting to get one solution no matter which method I use. This may sound very basic but I cannot wrap my head around it. Thanks.","D=\frac{d}{dx} T = 1 + D+ D^2+... x x+1 Tx=x+1 x \in R T \frac{1}{1-D} Tx Tx=(1 + D+ D^2+...)x=\frac{1}{1-D}x\equiv y \implies y - Dy = x \implies y-y^{'}=x, y(x)=x+1-ce^{x} c c=0","['ordinary-differential-equations', 'operator-theory', 'taylor-expansion']"
20,Is a general state space a manifold?,Is a general state space a manifold?,,"I asked a similar question on the physics SE at this link but did not really get an answer so I'll ask here. My question there is a bit more detailed. From what I have recently learned, in classical mechanics, the configuration of some mechanical systems (say, a double pendulum) can be described by a point on an n -dimensional smooth configuration manifold, $Q$ . The dynamics are then often studied using Lagrangian mechanics on $TQ$ , or using Hamiltonian mechanics on $T^*Q$ , both of which have coordinate representations in $\mathbb{R}^{2n}$ . Long before I ever heard the word manifold, I was ""doing"" dynamics using some 2n ""state space"" variables in $\mathbb{R}^{2n}$ . I know now that when using state space variables of the form $(\pmb{x},\dot{\pmb{x}})$ , I'm really working with some coordinate representation of $(\text{x},\mathbf{v})\in TQ$ . I could transform to some other coordinates $(\pmb{q},\dot{\pmb{q}})$ but, contrary to what I used to think,  this is not actually a different state space but rather just different coordinates for the same $(\text{x},\mathbf{v})\in TQ$ (right?). Similarly, when using state space variables of the form $(\pmb{x},\pmb{p}_{x})$ (phase space coordinates), I'm really working with some coordinate representation of $(\text{x},\mathbf{p})\in T^*Q$ . I could transform to some other $(\pmb{q},\pmb{p}_{q})$ but again this is not a different phase space but just different coordinates for the same $(\text{x},\mathbf{p})\in T^*Q$ (right?). My Question: When working directly with coordinates in $\mathbb{R}^{2n}$ , we also commonly use more general ""state space"" coordinates which are not necessarily of the lagrangian form, $(\pmb{x},\dot{\pmb{x}})$ , nor of the canonical/symplectic form $(\pmb{x},\pmb{p})$ , but are rather just some 2n variables that fully define the state (for example, the classic Keplerian orbit element for the two-body problem). Are such coordinates on $\mathbb{R}^{2n}$ actually a coordinate representation for some ``state space manifold '' ?  Or put differently, it seems $TQ$ and $T^*Q$ are two particular types of a ""state space manifold"" for which the coordinates and equations of motion have certain properties, but is there a more general manifold on which we can study the dynamics? note: I'm aware that in the Hamiltonian formulation we can use some canonical transformation such that the coordinates $(\pmb{x},\pmb{p})$ are not necessarily split into ""position level"" coordinates and ""velocity-level"" coordinates. The Delaunay variables for the two-body problem are an example of this. So this would be an example of ""jumbled"" coordinates on $\mathbb{R}^{2n}$ which are actually just still coordinates for the same $T^*Q$ .","I asked a similar question on the physics SE at this link but did not really get an answer so I'll ask here. My question there is a bit more detailed. From what I have recently learned, in classical mechanics, the configuration of some mechanical systems (say, a double pendulum) can be described by a point on an n -dimensional smooth configuration manifold, . The dynamics are then often studied using Lagrangian mechanics on , or using Hamiltonian mechanics on , both of which have coordinate representations in . Long before I ever heard the word manifold, I was ""doing"" dynamics using some 2n ""state space"" variables in . I know now that when using state space variables of the form , I'm really working with some coordinate representation of . I could transform to some other coordinates but, contrary to what I used to think,  this is not actually a different state space but rather just different coordinates for the same (right?). Similarly, when using state space variables of the form (phase space coordinates), I'm really working with some coordinate representation of . I could transform to some other but again this is not a different phase space but just different coordinates for the same (right?). My Question: When working directly with coordinates in , we also commonly use more general ""state space"" coordinates which are not necessarily of the lagrangian form, , nor of the canonical/symplectic form , but are rather just some 2n variables that fully define the state (for example, the classic Keplerian orbit element for the two-body problem). Are such coordinates on actually a coordinate representation for some ``state space manifold '' ?  Or put differently, it seems and are two particular types of a ""state space manifold"" for which the coordinates and equations of motion have certain properties, but is there a more general manifold on which we can study the dynamics? note: I'm aware that in the Hamiltonian formulation we can use some canonical transformation such that the coordinates are not necessarily split into ""position level"" coordinates and ""velocity-level"" coordinates. The Delaunay variables for the two-body problem are an example of this. So this would be an example of ""jumbled"" coordinates on which are actually just still coordinates for the same .","Q TQ T^*Q \mathbb{R}^{2n} \mathbb{R}^{2n} (\pmb{x},\dot{\pmb{x}}) (\text{x},\mathbf{v})\in TQ (\pmb{q},\dot{\pmb{q}}) (\text{x},\mathbf{v})\in TQ (\pmb{x},\pmb{p}_{x}) (\text{x},\mathbf{p})\in T^*Q (\pmb{q},\pmb{p}_{q}) (\text{x},\mathbf{p})\in T^*Q \mathbb{R}^{2n} (\pmb{x},\dot{\pmb{x}}) (\pmb{x},\pmb{p}) \mathbb{R}^{2n} TQ T^*Q (\pmb{x},\pmb{p}) \mathbb{R}^{2n} T^*Q","['ordinary-differential-equations', 'differential-geometry', 'dynamical-systems', 'classical-mechanics']"
21,General solution to $f^{\prime\prime} (x) + g(x) f(x) = 0$,General solution to,f^{\prime\prime} (x) + g(x) f(x) = 0,"I am trying to find the general solution to the following ODE with a variable coefficient: $$ f^{\prime\prime} (x) + g(x) f(x) = 0 \tag{1}$$ where the function $f$ is not known and $g$ is known. By doing research I only managed to find solutions of equation $(1)$ for specific cases of $g$ , but I did not find a general case where $g$ can be defined in any way. Is there a general solution for equation $(1)$ where $g$ is a general function, not a specific one? If a general solution does exist, what is it?","I am trying to find the general solution to the following ODE with a variable coefficient: where the function is not known and is known. By doing research I only managed to find solutions of equation for specific cases of , but I did not find a general case where can be defined in any way. Is there a general solution for equation where is a general function, not a specific one? If a general solution does exist, what is it?", f^{\prime\prime} (x) + g(x) f(x) = 0 \tag{1} f g (1) g g (1) g,['ordinary-differential-equations']
22,Is there a Laplace transform for an unknown squared function?,Is there a Laplace transform for an unknown squared function?,,"I was solving the differential equation $$y'-y = e^x y^2$$ I know that this is a simple differential equation, and I would solve it with a simple change of the variable, however, I was wondering if I could use Laplace transform to make it easier and more direct. I asked my professor, and she said she didn't know. So is there a way to do $\mathscr{L}([y(x)]^2)$ ?","I was solving the differential equation I know that this is a simple differential equation, and I would solve it with a simple change of the variable, however, I was wondering if I could use Laplace transform to make it easier and more direct. I asked my professor, and she said she didn't know. So is there a way to do ?",y'-y = e^x y^2 \mathscr{L}([y(x)]^2),"['ordinary-differential-equations', 'laplace-transform']"
23,"Two matrices $A,B$ such that $e^{(A+B)}=e^{A}e^{B}$ but $AB\neq BA$?",Two matrices  such that  but ?,"A,B e^{(A+B)}=e^{A}e^{B} AB\neq BA","So I saw at ODE class the properties of the exponential matrix so we can use it as the  canonical basic solution to linear ODE systems, and the teacher show us that if two matrices $A,B$ hold that $AB=BA$ then $e^{(A+B)t}=e^{At}e^{Bt}$ but said the reciprocal does not necesarily holds. I tried to find two matrices to show a counter example for the reciprocal, but couldn't find them in the matrices with real coefficients. My question is, such matrices exist? Do i need to consider them with complex coefficients? Update: I re-checked my notes to see if i made a mistake, and the propertie is without the $t$ involved, or maybe could be seen as $t=1$ (original question: ""Two matrices $A,B$ such that $e^{(A+B)t}=e^{At}e^{Bt}$ but $AB\neq BA$ ?"")","So I saw at ODE class the properties of the exponential matrix so we can use it as the  canonical basic solution to linear ODE systems, and the teacher show us that if two matrices hold that then but said the reciprocal does not necesarily holds. I tried to find two matrices to show a counter example for the reciprocal, but couldn't find them in the matrices with real coefficients. My question is, such matrices exist? Do i need to consider them with complex coefficients? Update: I re-checked my notes to see if i made a mistake, and the propertie is without the involved, or maybe could be seen as (original question: ""Two matrices such that but ?"")","A,B AB=BA e^{(A+B)t}=e^{At}e^{Bt} t t=1 A,B e^{(A+B)t}=e^{At}e^{Bt} AB\neq BA",['ordinary-differential-equations']
24,Weird integral curve on GeoGebra,Weird integral curve on GeoGebra,,"I graphed the direction field of the differential equation at GeoGebra $$y'=\frac{x-y}{x+y}$$ and here is the result: Both the direction field and the integral curve passing through $(1,1)$ look fine, but there is a really weird curve $(2,-1)$ ; I am almost certain it is incorrect even without calculation. May I know the cause of the problem and how to fix it (other than simply remove it)?","I graphed the direction field of the differential equation at GeoGebra and here is the result: Both the direction field and the integral curve passing through look fine, but there is a really weird curve ; I am almost certain it is incorrect even without calculation. May I know the cause of the problem and how to fix it (other than simply remove it)?","y'=\frac{x-y}{x+y} (1,1) (2,-1)","['ordinary-differential-equations', 'graphing-functions', 'math-software', 'plane-curves', 'online-resources']"
25,Determine a valid substitution for a differential equation,Determine a valid substitution for a differential equation,,"I am trying to determine whether any of the $v(x)$ substitutions that I'm given are are possible to make the equation first order linear in terms of $v$ . $$y' = \frac{y}{x^2} + x^3y^3$$ The given possible substitutions are $$v(x)=x^3y^3 \\v(x)=y^2 \\v(x)=y^{-2} \\v(x)=y/x$$ I don't know how to infer the answer by looking at it. So I went with a brute force approach. Solving for y in the first three yields $y = \frac{v^{1/3}}{x}$ , $y=\sqrt{v}$ , and $y=v^{-1/2}$ respectively. I went with the logic that this would not satisfy the linearity of the problem since $v$ in each case is nonlinear. Is the last one also false because it would make the equation $$vx = y \\v'x + v = y' \\v'x + v = \frac{v}{x} + x^7v^3$$ Thus keeping the equation in a nonlinear form. This is just my thinking on the matter. I want to make sure it is correct.","I am trying to determine whether any of the substitutions that I'm given are are possible to make the equation first order linear in terms of . The given possible substitutions are I don't know how to infer the answer by looking at it. So I went with a brute force approach. Solving for y in the first three yields , , and respectively. I went with the logic that this would not satisfy the linearity of the problem since in each case is nonlinear. Is the last one also false because it would make the equation Thus keeping the equation in a nonlinear form. This is just my thinking on the matter. I want to make sure it is correct.","v(x) v y' = \frac{y}{x^2} + x^3y^3 v(x)=x^3y^3
\\v(x)=y^2
\\v(x)=y^{-2}
\\v(x)=y/x y = \frac{v^{1/3}}{x} y=\sqrt{v} y=v^{-1/2} v vx = y
\\v'x + v = y'
\\v'x + v = \frac{v}{x} + x^7v^3","['ordinary-differential-equations', 'substitution']"
26,What is a possible substitution to this ODE?,What is a possible substitution to this ODE?,,"I have to solve this ODE: $$ (y-2x)\frac{dy}{dx}=3y-6x+1 $$ I have to make it in a form of separable variable equation and for that I have tried the following substitutions: $$ u=y-2x $$ and $$ u=3y-6x+1 $$ But none made it ""solvable"", for example in the first I get to a point where I have this: $$ \frac{e^{y-2x}}{|y-2x|}=x+K $$ where K is any constant. At this point I am unable to solve for y. Can anyone help me solve this?","I have to solve this ODE: I have to make it in a form of separable variable equation and for that I have tried the following substitutions: and But none made it ""solvable"", for example in the first I get to a point where I have this: where K is any constant. At this point I am unable to solve for y. Can anyone help me solve this?","
(y-2x)\frac{dy}{dx}=3y-6x+1
 
u=y-2x
 
u=3y-6x+1
 
\frac{e^{y-2x}}{|y-2x|}=x+K
",['ordinary-differential-equations']
27,Differential equation: $\dfrac{dy}{dx}= \dfrac{(x+y)^2}{(x+2)(y-2)}$,Differential equation:,\dfrac{dy}{dx}= \dfrac{(x+y)^2}{(x+2)(y-2)},"The solution of $\dfrac{dy}{dx}= \dfrac{(x+y)^2}{(x+2)(y-2)}$ is given by: a) $(x+2)^4 (1+\frac{2y}{x})= ke^{\frac{2y}{x}}$ b) $(x+2)^4 (1+ 2\frac{(y-2)}{x+2})= ke^{\frac{2(y-2)}{x+2}}$ c) $(x+2)^3 (1+ 2\frac{(y-2)}{x+2})= ke^{\frac{2(y-2)}{x+2}}$ d) None of these Attempt: I have expanded and checked but couldn't spot any exact differentials. Secondly, it's not a homogeneous equation, so couldn't use $y = vx$. How do I go about solving this problem?","The solution of $\dfrac{dy}{dx}= \dfrac{(x+y)^2}{(x+2)(y-2)}$ is given by: a) $(x+2)^4 (1+\frac{2y}{x})= ke^{\frac{2y}{x}}$ b) $(x+2)^4 (1+ 2\frac{(y-2)}{x+2})= ke^{\frac{2(y-2)}{x+2}}$ c) $(x+2)^3 (1+ 2\frac{(y-2)}{x+2})= ke^{\frac{2(y-2)}{x+2}}$ d) None of these Attempt: I have expanded and checked but couldn't spot any exact differentials. Secondly, it's not a homogeneous equation, so couldn't use $y = vx$. How do I go about solving this problem?",,['ordinary-differential-equations']
28,How do I apply integrating factor to solve this differential equation?,How do I apply integrating factor to solve this differential equation?,,$$x \frac{dy}{dx}  +  y  =  -2x^6y^4$$ I tried to find the general solution by dividing both sides by $x$  or $x^6$ but no solution I could get.. Do I even solve it with integrating factor?,$$x \frac{dy}{dx}  +  y  =  -2x^6y^4$$ I tried to find the general solution by dividing both sides by $x$  or $x^6$ but no solution I could get.. Do I even solve it with integrating factor?,,['ordinary-differential-equations']
29,Why does the Method of Characteristics matter?,Why does the Method of Characteristics matter?,,"I'm learning the method of characteristics from Evans' book on PDE (Chapter 3). From what I understand, a characteristic curve of a PDE is a curve on which the solution $u$ does not vary with respect to time. I.e, for all points $(X(t),t)$, with $X(0) = x$ that $u(X(t),t)  = u(x,0)$. But I have a couple questions. First, why do we care about characteristics? Intuitively, it seems like we can construct curves that ""cover"" all of spacetime, and then piece them together to get a global solution. But how can we apply the method to solve first order equations? And what does it mean when the curves intersect? It seems like the solution loses smoothness. Is this what the whole theory of shocks is about?","I'm learning the method of characteristics from Evans' book on PDE (Chapter 3). From what I understand, a characteristic curve of a PDE is a curve on which the solution $u$ does not vary with respect to time. I.e, for all points $(X(t),t)$, with $X(0) = x$ that $u(X(t),t)  = u(x,0)$. But I have a couple questions. First, why do we care about characteristics? Intuitively, it seems like we can construct curves that ""cover"" all of spacetime, and then piece them together to get a global solution. But how can we apply the method to solve first order equations? And what does it mean when the curves intersect? It seems like the solution loses smoothness. Is this what the whole theory of shocks is about?",,"['ordinary-differential-equations', 'partial-differential-equations', 'characteristics']"
30,"Heat equation ""like"" problem: solve $u_{tt} -u_{xx} -u_t -u_x = 0$ using differential operator","Heat equation ""like"" problem: solve  using differential operator",u_{tt} -u_{xx} -u_t -u_x = 0,"I need to find general solutions to the problem $$u_{tt} -u_{xx} -u_t -u_x=0$$ I wrote the PD operator here as $P \left( \partial_t , \partial_x \right) = {\partial_{t}}^2 -{u_x}^2 -\partial_t -\partial_x$ which in turn can be factorized as $P \left( \partial_t , \partial_x \right) = \left( \partial_t +\partial_x \right) \left( \partial_t -\partial_x -Id \right)$ where $Id$ is the identity operator. Now, the equation $Pu=0$ can be expressed by the system:  $$\cases{u_t -u_x = u+v \\ v_t + v_x =0 }$$ The second equation here has the general form of $v = \phi \left( x-t \right)$ (derived using the method of characteristics). Then I'm left with the problem $u_t - u_x = u + \phi (x-t)$ which can be written using the method of characteristics (again) as $u_t = u + \phi(x - t)$ I realize this is some kind of ODE, that I can solve for different values of $x$:  $$ u = \Psi(x) e^{t} + \Phi(x-t)$$ But when calculating $Pu$ I get  $$Pu = -{\mathrm{e}}^t\,\frac{\partial ^2}{\partial x^2} \Psi \left(x\right)-{\mathrm{e}}^t\,\frac{\partial }{\partial x} \Psi \left(x\right) $$ And for $Pu$ to nullify, $\Psi$ must be $ C e^{-x}$. Thus $u =  C e^{t-x} +\Phi \left(x - t \right)$. Surely, this solution is valid, but is this as general as it can be? How can I tell I didn't miss any other solution?","I need to find general solutions to the problem $$u_{tt} -u_{xx} -u_t -u_x=0$$ I wrote the PD operator here as $P \left( \partial_t , \partial_x \right) = {\partial_{t}}^2 -{u_x}^2 -\partial_t -\partial_x$ which in turn can be factorized as $P \left( \partial_t , \partial_x \right) = \left( \partial_t +\partial_x \right) \left( \partial_t -\partial_x -Id \right)$ where $Id$ is the identity operator. Now, the equation $Pu=0$ can be expressed by the system:  $$\cases{u_t -u_x = u+v \\ v_t + v_x =0 }$$ The second equation here has the general form of $v = \phi \left( x-t \right)$ (derived using the method of characteristics). Then I'm left with the problem $u_t - u_x = u + \phi (x-t)$ which can be written using the method of characteristics (again) as $u_t = u + \phi(x - t)$ I realize this is some kind of ODE, that I can solve for different values of $x$:  $$ u = \Psi(x) e^{t} + \Phi(x-t)$$ But when calculating $Pu$ I get  $$Pu = -{\mathrm{e}}^t\,\frac{\partial ^2}{\partial x^2} \Psi \left(x\right)-{\mathrm{e}}^t\,\frac{\partial }{\partial x} \Psi \left(x\right) $$ And for $Pu$ to nullify, $\Psi$ must be $ C e^{-x}$. Thus $u =  C e^{t-x} +\Phi \left(x - t \right)$. Surely, this solution is valid, but is this as general as it can be? How can I tell I didn't miss any other solution?",,"['real-analysis', 'ordinary-differential-equations', 'partial-derivative']"
31,example of two linearly independent functions to have a zero Wronskian??,example of two linearly independent functions to have a zero Wronskian??,,What is an example of two linearly independent functions to have a   zero Wronskian?? This question is  in reference to http://tutorial.math.lamar.edu/Classes/DE/Wronskian.aspx,What is an example of two linearly independent functions to have a   zero Wronskian?? This question is  in reference to http://tutorial.math.lamar.edu/Classes/DE/Wronskian.aspx,,"['real-analysis', 'linear-algebra', 'ordinary-differential-equations']"
32,Find the value of $f''(2).g''(2)$,Find the value of,f''(2).g''(2),If $f(x)$ and $g(x)$ are two differentiable functions on R+ such that $xf'(x)+ g(x)=0$ and $xg'(x)+ f(x)=0$ for all $x \in R^+ $ and $f(1)+g(1)=4$ then find the value of $f''(2).g''(2)$? I equated value of $x$ for both given equations and then integrated to get $g^2(x)=f^2(x)+C$ but it is not leading towards $f''(2).g''(2)$. Could someone give me some hint?,If $f(x)$ and $g(x)$ are two differentiable functions on R+ such that $xf'(x)+ g(x)=0$ and $xg'(x)+ f(x)=0$ for all $x \in R^+ $ and $f(1)+g(1)=4$ then find the value of $f''(2).g''(2)$? I equated value of $x$ for both given equations and then integrated to get $g^2(x)=f^2(x)+C$ but it is not leading towards $f''(2).g''(2)$. Could someone give me some hint?,,"['calculus', 'ordinary-differential-equations']"
33,What is the meaning of a torus defined by quotient group?,What is the meaning of a torus defined by quotient group?,,"In the paper Master Equation , I do not understand the basic notion introduced in Line 6 of Page 18: ""We work in the d-dimensional torus (i.e., periodic boundary conditions) that we denote $\mathbb T^d := \mathbb R^d/ \mathbb Z^d$."" I wonder what the exact meaning of $\mathbb R^d/ \mathbb Z^d$ is, and why it is torus?","In the paper Master Equation , I do not understand the basic notion introduced in Line 6 of Page 18: ""We work in the d-dimensional torus (i.e., periodic boundary conditions) that we denote $\mathbb T^d := \mathbb R^d/ \mathbb Z^d$."" I wonder what the exact meaning of $\mathbb R^d/ \mathbb Z^d$ is, and why it is torus?",,"['ordinary-differential-equations', 'differential-geometry']"
34,A particular solution to $\frac{d^2y}{dx^2}+y =\csc x$,A particular solution to,\frac{d^2y}{dx^2}+y =\csc x,"The solution to the BVP $\frac{d^2y}{dx^2}+y =\csc x$ , $0 < x < \frac{\pi}{2}$ $y(0)=0$ , $y(\pi/2)=0$ is $(A)$ Concave $(B)$ Convex $(C)$ Negative $(D)$ Positive For the homogeneous problem of course $\sin x$ and $\cos x $ are linearly independent solutions. I have trouble finding a particular solution to the non-homogeneous problem. Any help would be much appreciated. PS: There could be multiple correct options.","The solution to the BVP , , is Concave Convex Negative Positive For the homogeneous problem of course and are linearly independent solutions. I have trouble finding a particular solution to the non-homogeneous problem. Any help would be much appreciated. PS: There could be multiple correct options.",\frac{d^2y}{dx^2}+y =\csc x 0 < x < \frac{\pi}{2} y(0)=0 y(\pi/2)=0 (A) (B) (C) (D) \sin x \cos x ,['ordinary-differential-equations']
35,"What is ""Phase Space"" in differential equations and classical mechanics?","What is ""Phase Space"" in differential equations and classical mechanics?",,"I started reading a book on ordinary differential equations by Vladimir Arnold. He started his book of with the idea of phase space and phase points.  I seem to be confused what the general idea of what phase space, phase points and phase velocity vectors are. I have a general idea about them, but they don't make full sense to me. Could someone explain to me in the simplest way possible what those things are, and the general idea about them ? Also, how do they relate to classical mechanics and physics ? Thank You!","I started reading a book on ordinary differential equations by Vladimir Arnold. He started his book of with the idea of phase space and phase points.  I seem to be confused what the general idea of what phase space, phase points and phase velocity vectors are. I have a general idea about them, but they don't make full sense to me. Could someone explain to me in the simplest way possible what those things are, and the general idea about them ? Also, how do they relate to classical mechanics and physics ? Thank You!",,"['ordinary-differential-equations', 'classical-mechanics']"
36,Solve $\frac{dy}{dx}=\cos y\sin x$,Solve,\frac{dy}{dx}=\cos y\sin x,$$\frac{dy}{dx}=\cos y\sin x$$ I really don't know where to start since I don't know how to integrate $\sec y$ which I would have to do if I treated it as a separable equation. Please help,$$\frac{dy}{dx}=\cos y\sin x$$ I really don't know where to start since I don't know how to integrate $\sec y$ which I would have to do if I treated it as a separable equation. Please help,,['ordinary-differential-equations']
37,Solving $\frac{dx}{dt} = A \frac{ (1-x)}{(t-t^2)} - \frac{(B*x -C*x^2)}{(t-t^2 )*(t-x)}$ (using wolfram/mathematica),Solving  (using wolfram/mathematica),\frac{dx}{dt} = A \frac{ (1-x)}{(t-t^2)} - \frac{(B*x -C*x^2)}{(t-t^2 )*(t-x)},"I would like to solve the following non-linear ordinary differential equation: $$\frac{dx}{dt} = A \frac{ (1-x)}{(t-t^2)} - \frac{(B*x -C*x^2)}{(t-t^2 )*(t-x)}$$ -I need an analytic solution. -I don't know where to start (the 2 terms make it difficult), therefore I tried to use Wolfram-Alpha/ Mathematica. See here -It seems however that even Wolfram-Alpha / Mathematica is not able to solve it (computation time out). -Can someone help me out?","I would like to solve the following non-linear ordinary differential equation: $$\frac{dx}{dt} = A \frac{ (1-x)}{(t-t^2)} - \frac{(B*x -C*x^2)}{(t-t^2 )*(t-x)}$$ -I need an analytic solution. -I don't know where to start (the 2 terms make it difficult), therefore I tried to use Wolfram-Alpha/ Mathematica. See here -It seems however that even Wolfram-Alpha / Mathematica is not able to solve it (computation time out). -Can someone help me out?",,"['calculus', 'ordinary-differential-equations', 'mathematica', 'wolfram-alpha']"
38,"Serret-Frenet for Non-unit ""speed"" space curves","Serret-Frenet for Non-unit ""speed"" space curves",,"The Serret-Frenet equations form a system of linear, often non-autonomous, ordinary differential equations that recover the local tangent, normal and binormal vectors of a unit ""speed"" space curve from the curve's curvature and torsion. The equations are scaled for space curves of non unit ""speed."" ( see sec. 5 of the previous Wikipedia article ) This means that in order to find the local frame  one must also supply data about the curve parameterization. Often, one wants to find the curve parameterization for known curvature/torsion variables. Question 1: So, if one were to solve the SF equations $ \begin{align} \dfrac{d\mathbf{T}}{ds} &= & \kappa \mathbf{N}, \\ \dfrac{d\mathbf{N}}{ds} &= - \kappa \mathbf{T} & & + \tau \mathbf{B},\\ \dfrac{d\mathbf{B}}{ds} &= & -\tau \mathbf{N}, \end{align} $ for a given curvature and torsion, then would the associated space curve be identical, up to its ""speed,"" to the one found by solving $ \frac{d}{ds} \begin{bmatrix} \mathbf{T}\\ \mathbf{N}\\ \mathbf{B} \end{bmatrix} = \|\mathbf{r}'(s)\| \begin{bmatrix} 0&\kappa&0\\ -\kappa&0&\tau\\ 0&-\tau&0 \end{bmatrix} \begin{bmatrix} \mathbf{T}\\ \mathbf{N}\\ \mathbf{B} \end{bmatrix}?$ It seems that this should not be the case since the $||\textbf{r}'(s)||$ prefactor could greatly complicate the system of equations. Question 2: Assuming that the answer to question 1 is no, then what is the utility of the SF equations? Knowledge of $||\textbf{r}'(s)||$ likely comes from knowledge of $\textbf{r}'(s)$, which should allow one to find the tangent, normal and binormal vectors bypassing SF.","The Serret-Frenet equations form a system of linear, often non-autonomous, ordinary differential equations that recover the local tangent, normal and binormal vectors of a unit ""speed"" space curve from the curve's curvature and torsion. The equations are scaled for space curves of non unit ""speed."" ( see sec. 5 of the previous Wikipedia article ) This means that in order to find the local frame  one must also supply data about the curve parameterization. Often, one wants to find the curve parameterization for known curvature/torsion variables. Question 1: So, if one were to solve the SF equations $ \begin{align} \dfrac{d\mathbf{T}}{ds} &= & \kappa \mathbf{N}, \\ \dfrac{d\mathbf{N}}{ds} &= - \kappa \mathbf{T} & & + \tau \mathbf{B},\\ \dfrac{d\mathbf{B}}{ds} &= & -\tau \mathbf{N}, \end{align} $ for a given curvature and torsion, then would the associated space curve be identical, up to its ""speed,"" to the one found by solving $ \frac{d}{ds} \begin{bmatrix} \mathbf{T}\\ \mathbf{N}\\ \mathbf{B} \end{bmatrix} = \|\mathbf{r}'(s)\| \begin{bmatrix} 0&\kappa&0\\ -\kappa&0&\tau\\ 0&-\tau&0 \end{bmatrix} \begin{bmatrix} \mathbf{T}\\ \mathbf{N}\\ \mathbf{B} \end{bmatrix}?$ It seems that this should not be the case since the $||\textbf{r}'(s)||$ prefactor could greatly complicate the system of equations. Question 2: Assuming that the answer to question 1 is no, then what is the utility of the SF equations? Knowledge of $||\textbf{r}'(s)||$ likely comes from knowledge of $\textbf{r}'(s)$, which should allow one to find the tangent, normal and binormal vectors bypassing SF.",,"['ordinary-differential-equations', 'differential-geometry']"
39,How to design a differential equation to match a given general solution?,How to design a differential equation to match a given general solution?,,"I am in a first year differential equations course, and in class on Friday, the teacher did a problem from the book that I wasn't quite sure how to solve (yet I'm sure has a possibility of showing up on a test!). The question I have written in my notes is: ""create a differential equation that has $ y = C_1e^{-2x} + C_2e^{3x} + C_3xe^{3x} + e^x + x^2 + x $ as its general solution"". How would I go about doing this? I see that the general solution has $ C_1e^{-2x} + C_2e^{3x} + C_3xe^{3x} $, meaning the characteristic equation for the homogeneous equation ($Y_h$) should have roots $-2, 3, 3$. I guess for that I should make a polynomial which yields these roots? I also notice that the particular solution ($Y_p$) should be in the form $Ae^x + Bx^2 + Cx$. If we assume the DE I make is in the form $y'' + p(x)y' + q(x)y = f(x)$ - sorry if this isn't standard convention!), then $f(x)$ should contain something like $e^x + x^2 + x$ ? Perhaps I'm way off.","I am in a first year differential equations course, and in class on Friday, the teacher did a problem from the book that I wasn't quite sure how to solve (yet I'm sure has a possibility of showing up on a test!). The question I have written in my notes is: ""create a differential equation that has $ y = C_1e^{-2x} + C_2e^{3x} + C_3xe^{3x} + e^x + x^2 + x $ as its general solution"". How would I go about doing this? I see that the general solution has $ C_1e^{-2x} + C_2e^{3x} + C_3xe^{3x} $, meaning the characteristic equation for the homogeneous equation ($Y_h$) should have roots $-2, 3, 3$. I guess for that I should make a polynomial which yields these roots? I also notice that the particular solution ($Y_p$) should be in the form $Ae^x + Bx^2 + Cx$. If we assume the DE I make is in the form $y'' + p(x)y' + q(x)y = f(x)$ - sorry if this isn't standard convention!), then $f(x)$ should contain something like $e^x + x^2 + x$ ? Perhaps I'm way off.",,['ordinary-differential-equations']
40,"$\int f(y)e^{-y^2} e^{2xy}\,dy$, to prove $f=0$",", to prove","\int f(y)e^{-y^2} e^{2xy}\,dy f=0","Show that, if $f \in S(\mathbb{R})$, where $S(\mathbb{R})$ defines  Schwartz's space, and $$\int_{-\infty}^{+\infty} f(y)e^{-y^2}e^{2xy} \, dy =0,$$ for all $x \in \mathbb{R}$, then $f=0$ I don't know how start , it occurs to me some property of convolution functions , Greetings..","Show that, if $f \in S(\mathbb{R})$, where $S(\mathbb{R})$ defines  Schwartz's space, and $$\int_{-\infty}^{+\infty} f(y)e^{-y^2}e^{2xy} \, dy =0,$$ for all $x \in \mathbb{R}$, then $f=0$ I don't know how start , it occurs to me some property of convolution functions , Greetings..",,['ordinary-differential-equations']
41,How to solve $\frac{dy}{dx}=\cos(x-y)$?,How to solve ?,\frac{dy}{dx}=\cos(x-y),How to solve $\dfrac{dy}{dx}=\cos(x-y)$ ? How do I separate x and y here ? Please advise.,How to solve $\dfrac{dy}{dx}=\cos(x-y)$ ? How do I separate x and y here ? Please advise.,,"['calculus', 'ordinary-differential-equations']"
42,Solving second-order nonlinear nonhomogeneous differential equation,Solving second-order nonlinear nonhomogeneous differential equation,,"The equation I am trying to solve has the following form: $$y'' + ay^3 = b$$ where $a$ and $b$ are constant coefficients. Although the equation seems trivial to solve, the little $b$ at the end drives me mad trying to solve it analytically. My question is this: is it possible to solve this equation exactly? If not, would it be possible to approximate it somehow (preferably still analytically)? Thanks and my apologies if if a similar question has been asked already, I could not find any other questions similar enough to solve this one.","The equation I am trying to solve has the following form: $$y'' + ay^3 = b$$ where $a$ and $b$ are constant coefficients. Although the equation seems trivial to solve, the little $b$ at the end drives me mad trying to solve it analytically. My question is this: is it possible to solve this equation exactly? If not, would it be possible to approximate it somehow (preferably still analytically)? Thanks and my apologies if if a similar question has been asked already, I could not find any other questions similar enough to solve this one.",,['ordinary-differential-equations']
43,Reference help - Linear Algebra and Calculus,Reference help - Linear Algebra and Calculus,,"I went through some of the questions asking for reference help but they're not the same (definitely similar though). Maybe i missed something but here goes- I know there are Gilbert Strang videos, textbooks and other sources that are similar to learn Linear Algebra but what i want to know is - are there any concise (preferably freely available) text books to review the concepts? I say review because I've taken up both Linear Algebra and Calculus 6-7 years ago and they are rusty. I need to review these for grad school and I'm short of time to re-learn everything from scratch. I need to brush up the basics for example, Linear Algebra - Vectors, and solving linear equations Spaces Matrices (Rank, Row/Column operations) Determinants Eigen values and Eigen vectors. Calculus Limits (and some pre-calculus) solving differential equations (ODEs and PDEs et al) basic integral calculus What I'm looking for is review sources and primers that maybe some of you have found really useful. Edit - Some very helpful links, thank you all. I also found PatrickJMT.com for very basic stuff to solve a few problems and help remember high school learning. And some wandering soul might appreciate this link as well - Additional Resources","I went through some of the questions asking for reference help but they're not the same (definitely similar though). Maybe i missed something but here goes- I know there are Gilbert Strang videos, textbooks and other sources that are similar to learn Linear Algebra but what i want to know is - are there any concise (preferably freely available) text books to review the concepts? I say review because I've taken up both Linear Algebra and Calculus 6-7 years ago and they are rusty. I need to review these for grad school and I'm short of time to re-learn everything from scratch. I need to brush up the basics for example, Linear Algebra - Vectors, and solving linear equations Spaces Matrices (Rank, Row/Column operations) Determinants Eigen values and Eigen vectors. Calculus Limits (and some pre-calculus) solving differential equations (ODEs and PDEs et al) basic integral calculus What I'm looking for is review sources and primers that maybe some of you have found really useful. Edit - Some very helpful links, thank you all. I also found PatrickJMT.com for very basic stuff to solve a few problems and help remember high school learning. And some wandering soul might appreciate this link as well - Additional Resources",,"['calculus', 'linear-algebra', 'ordinary-differential-equations']"
44,Change of Coordinate in Differential Equation,Change of Coordinate in Differential Equation,,"I'm sorry, it's probably a very simple question but I'm confused between change of variable and change of coordinate in a differential equation. To take a very simple example, let's start with this equation for the function $f(x,t)$: $$ \frac{\partial f}{\partial t} = \alpha \frac{\partial^2 f}{\partial x^2} $$ and say that I want to find the new coordinate $y$ such that $f(y,t)$ verifies: $$ \frac{\partial f}{\partial t} = \frac{\partial^2 f}{\partial y^2} $$ Is the answer $y = x / \alpha$ or $y = x / \sqrt{\alpha}$? It seems to be the second one, but I can't explain rigorously why.. What if $\alpha$ was negative, would we change for a complex coordinate? This seems weird to me. What confuses me is that both the variable of f and the variable wrt which we differentiate change. The fact that both happen at once makes it unclear to me what happens exactly, and makes me suspicious about considering the $\partial \cdot^2$ simply as a ""squared"" $\partial \cdot$, as the notation suggests. Can anyone please explain clearly what's going on there? Thanks in advance.","I'm sorry, it's probably a very simple question but I'm confused between change of variable and change of coordinate in a differential equation. To take a very simple example, let's start with this equation for the function $f(x,t)$: $$ \frac{\partial f}{\partial t} = \alpha \frac{\partial^2 f}{\partial x^2} $$ and say that I want to find the new coordinate $y$ such that $f(y,t)$ verifies: $$ \frac{\partial f}{\partial t} = \frac{\partial^2 f}{\partial y^2} $$ Is the answer $y = x / \alpha$ or $y = x / \sqrt{\alpha}$? It seems to be the second one, but I can't explain rigorously why.. What if $\alpha$ was negative, would we change for a complex coordinate? This seems weird to me. What confuses me is that both the variable of f and the variable wrt which we differentiate change. The fact that both happen at once makes it unclear to me what happens exactly, and makes me suspicious about considering the $\partial \cdot^2$ simply as a ""squared"" $\partial \cdot$, as the notation suggests. Can anyone please explain clearly what's going on there? Thanks in advance.",,"['ordinary-differential-equations', 'partial-differential-equations']"
45,Solving an ODE without Lambert W function,Solving an ODE without Lambert W function,,"I have a question regarding the possibility of solving the following ODE: $$\left[2x(t)+t\right]x^{\prime}(t)=1$$ such that $x(0)=-1$. If we make the substitution $w(t)=2x(t)+t$, we obtain the following equation: $$-2\ln[w(t)+2]+w(t)=t+C$$ which can be solved for $w$ (and hence $x$), although both $w$ and $x$ will be expressed in terms of the Lambert W function. I won't be posting the full solution here - WolframAlpha shows it step-by-step. Nevertheless, after applying the initial condition that $x(0)=-1$, $x$ is simplified to a much more digestible form: $x(t)=-\frac{t}{2}-1$. Therefore, I was wondering - is it possible to arrive at this solution without the use of Lambert W function? More specifically, is it possible to somehow apply the initial condition earlier and hence to avoid having to find the general solution for $x(t)$ (and thereby avoiding stumbling on the Lambert W function)? I would be grateful for some advice on this.","I have a question regarding the possibility of solving the following ODE: $$\left[2x(t)+t\right]x^{\prime}(t)=1$$ such that $x(0)=-1$. If we make the substitution $w(t)=2x(t)+t$, we obtain the following equation: $$-2\ln[w(t)+2]+w(t)=t+C$$ which can be solved for $w$ (and hence $x$), although both $w$ and $x$ will be expressed in terms of the Lambert W function. I won't be posting the full solution here - WolframAlpha shows it step-by-step. Nevertheless, after applying the initial condition that $x(0)=-1$, $x$ is simplified to a much more digestible form: $x(t)=-\frac{t}{2}-1$. Therefore, I was wondering - is it possible to arrive at this solution without the use of Lambert W function? More specifically, is it possible to somehow apply the initial condition earlier and hence to avoid having to find the general solution for $x(t)$ (and thereby avoiding stumbling on the Lambert W function)? I would be grateful for some advice on this.",,"['ordinary-differential-equations', 'lambert-w']"
46,Legendre polynomials recurrence relation,Legendre polynomials recurrence relation,,How can i get? $$P_{n+1}=xP_n(x)-\frac{1-x^2}{n+1} P'_n(x)$$ $n>=0$ Also know as the leadder equation of the legendre polinomials i tried to use de recurrence relations as: $$P_n(x)=P_{n+1}'(x)-2xP'_n(x)$$ and $$nP_n(x)+P_{n+1}'(x)-xP'_n(x)$$,How can i get? $$P_{n+1}=xP_n(x)-\frac{1-x^2}{n+1} P'_n(x)$$ $n>=0$ Also know as the leadder equation of the legendre polinomials i tried to use de recurrence relations as: $$P_n(x)=P_{n+1}'(x)-2xP'_n(x)$$ and $$nP_n(x)+P_{n+1}'(x)-xP'_n(x)$$,,"['ordinary-differential-equations', 'recurrence-relations']"
47,Approximation of DE,Approximation of DE,,"It depends on my previous question. Closed form solution of DE I don't want to deal with Airy functions. How can I approximate this DE in continous domain $[0,1]$? $$y''(x)+(x+1)y(x)=0\quad\text{ with the initial conditions}\quad y(0)=0\quad y'(0)=1$$ What if the conditions change to $$y''(x)+(x+1)y(x)=0\quad\text{ with the initial conditions}\quad y(0)=0\quad y'(1)=1$$ May I use the same methods?","It depends on my previous question. Closed form solution of DE I don't want to deal with Airy functions. How can I approximate this DE in continous domain $[0,1]$? $$y''(x)+(x+1)y(x)=0\quad\text{ with the initial conditions}\quad y(0)=0\quad y'(0)=1$$ What if the conditions change to $$y''(x)+(x+1)y(x)=0\quad\text{ with the initial conditions}\quad y(0)=0\quad y'(1)=1$$ May I use the same methods?",,"['ordinary-differential-equations', 'approximation']"
48,How to solve $dy/dx = \frac{(3y^2+2x^2)}{ (xy)}$,How to solve,dy/dx = \frac{(3y^2+2x^2)}{ (xy)},"I got this problem in today's exam and I couldn't quite figure this out. The equation is $xy \, dy = (3y^2+2x^2) \, dx$, $M_y = 6y$ and $N_x = y$, they aren't equal so this equation is nowhere near exact, it doesn't look like I can do separable either? What to do?","I got this problem in today's exam and I couldn't quite figure this out. The equation is $xy \, dy = (3y^2+2x^2) \, dx$, $M_y = 6y$ and $N_x = y$, they aren't equal so this equation is nowhere near exact, it doesn't look like I can do separable either? What to do?",,['ordinary-differential-equations']
49,Find $f(x)=?$ functional equation [closed],Find  functional equation [closed],f(x)=?,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I would appreciate if somebody could help me with the following problem: Q: Find $f(x)$ ($f'(x)$: conti-function , $x \in\mathbb{R}$) $$f(x)=\sin ^2x+\int_{0}^{x}tf(t)dt$$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I would appreciate if somebody could help me with the following problem: Q: Find $f(x)$ ($f'(x)$: conti-function , $x \in\mathbb{R}$) $$f(x)=\sin ^2x+\int_{0}^{x}tf(t)dt$$",,"['ordinary-differential-equations', 'integral-equations']"
50,Interval of definition of the solutions of $\dot x=e^x\sin x$,Interval of definition of the solutions of,\dot x=e^x\sin x,"I'm trying to prove that the solutions of this ODE $\dot x=e^x\sin x$ are defined in $\mathbb R$. I'm really new on this subject, I'm trying to use the Picard Theorem, but this function is not locally Lipschitz, I need help here.","I'm trying to prove that the solutions of this ODE $\dot x=e^x\sin x$ are defined in $\mathbb R$. I'm really new on this subject, I'm trying to use the Picard Theorem, but this function is not locally Lipschitz, I need help here.",,['ordinary-differential-equations']
51,Solve $y^{\prime \prime}-(y^{\prime})^2-y^{\prime}=0$,Solve,y^{\prime \prime}-(y^{\prime})^2-y^{\prime}=0,Solve $y^{\prime \prime}-(y^{\prime})^2-y^{\prime}=0$. I use $$u=\frac{dy}{dx}$$ to transform the DE into $$\frac{du}{dx}-u^2-u=0$$. I know that this is an Bernoulli equation with $n=2$. I get the final solution is $$y=-ln|1-Ae^x|+D$$ where $A=+-e^c$. But my lecturer's answer is $$y=-ln|C_1+c_2e^x|$$. May I know what is the difference my answer and my lecturer answer ?,Solve $y^{\prime \prime}-(y^{\prime})^2-y^{\prime}=0$. I use $$u=\frac{dy}{dx}$$ to transform the DE into $$\frac{du}{dx}-u^2-u=0$$. I know that this is an Bernoulli equation with $n=2$. I get the final solution is $$y=-ln|1-Ae^x|+D$$ where $A=+-e^c$. But my lecturer's answer is $$y=-ln|C_1+c_2e^x|$$. May I know what is the difference my answer and my lecturer answer ?,,['ordinary-differential-equations']
52,Method of dominant balance,Method of dominant balance,,"Find the leading asymptotic behaviour as $x \rightarrow \infty$ of $$x^2y'' + (1 + 3x)y' + y = 0 $$ Can someone kindly explain me how to solve this problem? Im learning asymptotic analysis, and I want to see how the method of dominant balance works in this example. Thanks","Find the leading asymptotic behaviour as $x \rightarrow \infty$ of $$x^2y'' + (1 + 3x)y' + y = 0 $$ Can someone kindly explain me how to solve this problem? Im learning asymptotic analysis, and I want to see how the method of dominant balance works in this example. Thanks",,"['calculus', 'ordinary-differential-equations', 'asymptotics', 'approximation']"
53,How to check Linear equation,How to check Linear equation,,How the following equation is not linear? $$ \frac {dy}{dx}+xy=xy^2$$ is it not linear because its not in the below given form? $$ \frac {dy}{dx}+p(x)y=q(x)$$,How the following equation is not linear? $$ \frac {dy}{dx}+xy=xy^2$$ is it not linear because its not in the below given form? $$ \frac {dy}{dx}+p(x)y=q(x)$$,,"['linear-algebra', 'ordinary-differential-equations']"
54,$f'' + f =0$: finding $f$ using power series,: finding  using power series,f'' + f =0 f,How can one solve the following differential equation $$f'' + f =0$$ with the usage of power series? Writing: $$f(x) = \displaystyle\sum_{k=0}^{\infty} a_k x^k$$ $$f'(x) = \displaystyle\sum_{k=1}^{\infty} a_k k x^{k-1}$$ $$f'' (x) = \displaystyle\sum_{k=2}^{\infty} k (k-1) a_k x^{k-2}$$ How to continue?,How can one solve the following differential equation $$f'' + f =0$$ with the usage of power series? Writing: $$f(x) = \displaystyle\sum_{k=0}^{\infty} a_k x^k$$ $$f'(x) = \displaystyle\sum_{k=1}^{\infty} a_k k x^{k-1}$$ $$f'' (x) = \displaystyle\sum_{k=2}^{\infty} k (k-1) a_k x^{k-2}$$ How to continue?,,['ordinary-differential-equations']
55,Does ODE solution implies existence for a proportional ODE?,Does ODE solution implies existence for a proportional ODE?,,"Suppose an ODE $$\frac{d}{dt} x(t) = f(x,t)$$ has a global solution for some initial condition $x(0) = x_0$ . I am interested in whether, for an arbitrary constant $k>0$ , the proportional ODE $$\frac{d}{dt} x(t) = k \cdot f(x,t)$$ has a global solution for the same initial condition $x(0) = x_0$ . Does the existence of the solution to the first ODE weaken at all the conditions needed to ensure existence of the solution to the second ODE? In all of the examples and counterexamples to existence that I know of, multiplying $f$ by a constant does not affect existence. But I don't see a path to a general proof.","Suppose an ODE has a global solution for some initial condition . I am interested in whether, for an arbitrary constant , the proportional ODE has a global solution for the same initial condition . Does the existence of the solution to the first ODE weaken at all the conditions needed to ensure existence of the solution to the second ODE? In all of the examples and counterexamples to existence that I know of, multiplying by a constant does not affect existence. But I don't see a path to a general proof.","\frac{d}{dt} x(t) = f(x,t) x(0) = x_0 k>0 \frac{d}{dt} x(t) = k \cdot f(x,t) x(0) = x_0 f","['real-analysis', 'ordinary-differential-equations']"
56,When is $\exp(f(x))$ concave?,When is  concave?,\exp(f(x)),"I'm interested in the set of twice-differentiable functions $f$ such that $\exp(f(x))$ is concave for $x \in [0,1]$ . This is equivalent to asking for the set of functions $f$ such that $$ \left(\frac{df}{dx}(x)\right)^2 + \frac{d^2 f}{dx^2}(x) < 0 $$ for every $x \in [0,1]$ . The reasoning for this equivalence is below. I'm purposefully avoiding a non-strict inequality to avoid the trivial solution $f(x) = c$ for some constant $c \in \mathbb R$ . If the entire set cant be characterized, I would be satisfied with a single example of $f$ . Because \begin{equation} \frac{d \exp \circ f}{dx}(x) = \exp(f(x)) \cdot \frac{df}{dx}(x) \end{equation} and so \begin{align} \frac{d^2 \exp \circ f}{dx^2}(x) &= \exp(f(x)) \cdot \frac{df}{dx}(x) \cdot \frac{df}{dx}(x) + \exp(f(x)) \cdot \frac{d^2f}{dx^2}(x) \\ &= \exp(f(x)) \cdot \left[\left(\frac{df}{dx}(x)\right)^2 + \frac{d^2f}{dx^2}(x)\right] \end{align} Because $\exp(f(x)) \geq 0$ for every $x \in [0,1]$ , then we want to determine the functions $f$ such that $$ \left(\frac{df}{dx}(x)\right)^2 + \frac{d^2f}{dx^2}(x) < 0 $$ for every $x \in [0,1]$ .","I'm interested in the set of twice-differentiable functions such that is concave for . This is equivalent to asking for the set of functions such that for every . The reasoning for this equivalence is below. I'm purposefully avoiding a non-strict inequality to avoid the trivial solution for some constant . If the entire set cant be characterized, I would be satisfied with a single example of . Because and so Because for every , then we want to determine the functions such that for every .","f \exp(f(x)) x \in [0,1] f 
\left(\frac{df}{dx}(x)\right)^2 + \frac{d^2 f}{dx^2}(x) < 0
 x \in [0,1] f(x) = c c \in \mathbb R f \begin{equation}
\frac{d \exp \circ f}{dx}(x) = \exp(f(x)) \cdot \frac{df}{dx}(x)
\end{equation} \begin{align}
\frac{d^2 \exp \circ f}{dx^2}(x) &= \exp(f(x)) \cdot \frac{df}{dx}(x) \cdot \frac{df}{dx}(x) + \exp(f(x)) \cdot \frac{d^2f}{dx^2}(x) \\
&= \exp(f(x)) \cdot \left[\left(\frac{df}{dx}(x)\right)^2 + \frac{d^2f}{dx^2}(x)\right]
\end{align} \exp(f(x)) \geq 0 x \in [0,1] f 
\left(\frac{df}{dx}(x)\right)^2 + \frac{d^2f}{dx^2}(x) < 0
 x \in [0,1]","['ordinary-differential-equations', 'convex-analysis', 'convex-optimization']"
57,Solution to differential equation $(x - y/y')^2 (1 + (y')^2) = 1$,Solution to differential equation,(x - y/y')^2 (1 + (y')^2) = 1,"This differential equation showed up in a geometry problem $$ \left(x - \frac{y}{y'}\right)^2 \left(1 + \left(y'\right)^2\right) = 1 $$ I figured out by trial and error that $y(x) = \left( 1 - x^{2/3} \right)^{3/2}$ , the graph of an astroid, is a solution, but I'd like to see a way how this can be solved in a more proper manner, rather than just by getting lucky, and if multiple solutions exist.","This differential equation showed up in a geometry problem I figured out by trial and error that , the graph of an astroid, is a solution, but I'd like to see a way how this can be solved in a more proper manner, rather than just by getting lucky, and if multiple solutions exist.", \left(x - \frac{y}{y'}\right)^2 \left(1 + \left(y'\right)^2\right) = 1  y(x) = \left( 1 - x^{2/3} \right)^{3/2},['ordinary-differential-equations']
58,Differential Equation with $f'(t)=\cos(f(t))$ prove that $-2 < f(t) < 2$,Differential Equation with  prove that,f'(t)=\cos(f(t)) -2 < f(t) < 2,I cannot solve the following exercise: Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be infinetly often differentiable with $f(0)=0$ and $f'(t)=\cos(f(t))$ . Prove that $-2 < f(t) < 2 $ for any $t \in \mathbb{R}$ . I saw that $|f'(t) \leq 1|$ and tried applying mean value theorem for $0$ and $t$ but still couldn't find a solution. I also tried to integrate but still could only show that $f(t) \leq t$ .,I cannot solve the following exercise: Let be infinetly often differentiable with and . Prove that for any . I saw that and tried applying mean value theorem for and but still couldn't find a solution. I also tried to integrate but still could only show that .,f: \mathbb{R} \rightarrow \mathbb{R} f(0)=0 f'(t)=\cos(f(t)) -2 < f(t) < 2  t \in \mathbb{R} |f'(t) \leq 1| 0 t f(t) \leq t,"['real-analysis', 'ordinary-differential-equations']"
59,Function where second derivative is equal to reciprocal squared,Function where second derivative is equal to reciprocal squared,,"I was recently interested in whether there exists a closed-form solution for the position of an object (say a spacecraft) in freefall as a function of time. To complicate things, I wanted to take into account the increase in acceleration as you get nearer to the surface. The differential equation is straightforward: $ x''(t) = \frac{\mu}{x(t)^2} $ where $\mu = G \cdot m_{planet}$ How, if at all, would I go about solving this? I've tried to use the common functions that return themselves when differentiated (trig, hyperbolic trig, exponential, etc.) but none of them obviously equal the reciprocal of themselves when differentiated twice. The closest I've gotten was $B\cosh(D \cdot t)$ , but it doesn't work in the differential equation and doesn't quite match the analytical curve.","I was recently interested in whether there exists a closed-form solution for the position of an object (say a spacecraft) in freefall as a function of time. To complicate things, I wanted to take into account the increase in acceleration as you get nearer to the surface. The differential equation is straightforward: where How, if at all, would I go about solving this? I've tried to use the common functions that return themselves when differentiated (trig, hyperbolic trig, exponential, etc.) but none of them obviously equal the reciprocal of themselves when differentiated twice. The closest I've gotten was , but it doesn't work in the differential equation and doesn't quite match the analytical curve.", x''(t) = \frac{\mu}{x(t)^2}  \mu = G \cdot m_{planet} B\cosh(D \cdot t),['ordinary-differential-equations']
60,"An interesting recurrent equality, possibly easier to solve in its differential form?","An interesting recurrent equality, possibly easier to solve in its differential form?",,"I encountered an interesting inequality that I'm not sure how to approach. Here $c$ is a positive constant. $$f(n+1) - f(n) = c f(n)\sum_{m=0}^n f(m)$$ I am not familiar with techniques to solve recursive equations, so I thought about a similar differential equation of it $$\frac{\partial f}{\partial x} = cf(x)\int_{z=0}^x f(z)dz.$$ I am quite new to the topic of differential equations though, so I might be missing some obvious techniques to solve this kind of equality. With my limited knowledge I tried applying Laplace transform or the Fourier transform to this equation to no avail, but the expression looks nice enough that I suspect there is an analytical solution to it. Any help to getting an analytical expression of this equation is much appreciated! (This is coming from a real-world problem, so for now, any ""nice"" assumption on $f$ and the initial conditions and so on can be placed)","I encountered an interesting inequality that I'm not sure how to approach. Here is a positive constant. I am not familiar with techniques to solve recursive equations, so I thought about a similar differential equation of it I am quite new to the topic of differential equations though, so I might be missing some obvious techniques to solve this kind of equality. With my limited knowledge I tried applying Laplace transform or the Fourier transform to this equation to no avail, but the expression looks nice enough that I suspect there is an analytical solution to it. Any help to getting an analytical expression of this equation is much appreciated! (This is coming from a real-world problem, so for now, any ""nice"" assumption on and the initial conditions and so on can be placed)",c f(n+1) - f(n) = c f(n)\sum_{m=0}^n f(m) \frac{\partial f}{\partial x} = cf(x)\int_{z=0}^x f(z)dz. f,"['real-analysis', 'ordinary-differential-equations', 'inequality', 'recurrence-relations', 'recursion']"
61,Vector field tangent to a meridian,Vector field tangent to a meridian,,"I am reading the book Geometry theory of dynamical systems of Palis and de Melo, and I came across the following vector field $X(x,y,z)=(-xz,-yz,x^2+y^2)$ . The book mentions that this field is tangent to the meridians of $\mathbb S^2$ . I have graphed it and this is true, but how can I prove it? Any idea how to start? When it's a field in the plane it's easy, but now that the field is defined on a surface I can't see how to do it. All help is welcome. The real goal is to find the $\alpha-$ limit and $\omega-$ limit set of a point $p$ in $\mathbb S^2$ . What I don't understand well is because the stereographic projection guarantees me tangentiality, for example in the following image I can think of a vector field not tangent to a meridian, from which I extract a vector, and project it onto the plane, this projection would give us vectors with the same properties as the $\pi(X)$ projection.","I am reading the book Geometry theory of dynamical systems of Palis and de Melo, and I came across the following vector field . The book mentions that this field is tangent to the meridians of . I have graphed it and this is true, but how can I prove it? Any idea how to start? When it's a field in the plane it's easy, but now that the field is defined on a surface I can't see how to do it. All help is welcome. The real goal is to find the limit and limit set of a point in . What I don't understand well is because the stereographic projection guarantees me tangentiality, for example in the following image I can think of a vector field not tangent to a meridian, from which I extract a vector, and project it onto the plane, this projection would give us vectors with the same properties as the projection.","X(x,y,z)=(-xz,-yz,x^2+y^2) \mathbb S^2 \alpha- \omega- p \mathbb S^2 \pi(X)","['ordinary-differential-equations', 'manifolds', 'dynamical-systems', 'vector-fields']"
62,Solving Linear Inhomogeneous System of Differential Equations,Solving Linear Inhomogeneous System of Differential Equations,,"Let $x(t),y\in \mathbb{R}^p,A\in \mathbb{R}^{p\times p}$ , I want to solve the following system \begin{align} \frac{dx(t)}{dt}&=-A(x(t)-y) \end{align} with known $x(0)$ . I have 'something' but I suspect that it is wrong. I first take \begin{align} \frac{dx(t)}{dt}+Ax(t)&=Ay\\ \exp(At)\frac{dx(t)}{dt}+\exp(At)Ax(t)&=\exp(At)Ay\\ \frac{d}{dt}\exp(At)x(t)&=\exp(At)Ay\\ \exp(At)x(t)&=x(0)+\int_0^t \exp(As) ds Ay\\ \exp(At)x(t)&=x(0)+[\exp(At)-I]Ay\\ x(t)&=\exp(-At)[x(0)-Ay]+Ay \end{align} However I have reason to believe (from looking at some other material) that the answer 'should' be \begin{align} x(t)&=\exp(-At)[x(0)-y]+y \end{align} Are either of these answers (either the one I derived or the one I suspect) correct? If the latter is correct, why? If neither are correct, what is the true answer?","Let , I want to solve the following system with known . I have 'something' but I suspect that it is wrong. I first take However I have reason to believe (from looking at some other material) that the answer 'should' be Are either of these answers (either the one I derived or the one I suspect) correct? If the latter is correct, why? If neither are correct, what is the true answer?","x(t),y\in \mathbb{R}^p,A\in \mathbb{R}^{p\times p} \begin{align}
\frac{dx(t)}{dt}&=-A(x(t)-y)
\end{align} x(0) \begin{align}
\frac{dx(t)}{dt}+Ax(t)&=Ay\\
\exp(At)\frac{dx(t)}{dt}+\exp(At)Ax(t)&=\exp(At)Ay\\
\frac{d}{dt}\exp(At)x(t)&=\exp(At)Ay\\
\exp(At)x(t)&=x(0)+\int_0^t \exp(As) ds Ay\\
\exp(At)x(t)&=x(0)+[\exp(At)-I]Ay\\
x(t)&=\exp(-At)[x(0)-Ay]+Ay
\end{align} \begin{align}
x(t)&=\exp(-At)[x(0)-y]+y
\end{align}",['ordinary-differential-equations']
63,Proving the stability of the null solution of a differential equation,Proving the stability of the null solution of a differential equation,,"so I'm starting to work on Lyapunov functions and I would like to use them to prove the stability of the null solution of the following differential equation: $$\frac{d}{dt}\left(\begin{array}{c}x\\y\end{array}\right) = \left(\begin{array}{c}- \sin(x) - y + sin(y)\\1 - \cos(x) - y + y^2\end{array}\right)$$ I know that for an equation $\frac{dx}{dt} = Ax$ the stability of the null solution can be proved by showing that all eigenvalues of $A$ have negative real part. However I'm not sure in the case above what is $A$ , because of the $cos$ and $sin$ functions inside the given matrix. Thus I was wondering if someone could help my finding this matrix, and since it will be a $2\times 1$ matrix how can I find its eigenvalues ? Moreover this is the only method I've seen to solve this problem, but if there is any other I'm willing to try it, there is no obligation on how to solve this problem. I'm thinking about methods like those in the following links: Stability of the null solution of system of differential equations , How to find a Lyapunov function? and How to pick a Lyapunov function and prove stability? (if they can be applied here of course). Thanks in advance for any help, have a good day!","so I'm starting to work on Lyapunov functions and I would like to use them to prove the stability of the null solution of the following differential equation: I know that for an equation the stability of the null solution can be proved by showing that all eigenvalues of have negative real part. However I'm not sure in the case above what is , because of the and functions inside the given matrix. Thus I was wondering if someone could help my finding this matrix, and since it will be a matrix how can I find its eigenvalues ? Moreover this is the only method I've seen to solve this problem, but if there is any other I'm willing to try it, there is no obligation on how to solve this problem. I'm thinking about methods like those in the following links: Stability of the null solution of system of differential equations , How to find a Lyapunov function? and How to pick a Lyapunov function and prove stability? (if they can be applied here of course). Thanks in advance for any help, have a good day!",\frac{d}{dt}\left(\begin{array}{c}x\\y\end{array}\right) = \left(\begin{array}{c}- \sin(x) - y + sin(y)\\1 - \cos(x) - y + y^2\end{array}\right) \frac{dx}{dt} = Ax A A cos sin 2\times 1,"['ordinary-differential-equations', 'derivatives', 'proof-writing', 'lyapunov-functions']"
64,A first order ordinary differential equation,A first order ordinary differential equation,,"Below is a problem I did. However, it did not match the back of the book. I would like to know where I went wrong. Problem: Solve the following differential equation. $$ y' = \frac{y-x}{x} $$ Answer: \begin{align*} \frac{dy}{dx} &= \dfrac{y}{x} - \dfrac{x}{x} =  \dfrac{y}{x} - 1 \\ y &= xv \\ \dfrac{dy}{dx} &= x \dfrac{dv}{dx} + v \\ x \dfrac{dv}{dx} + v &= v - 1 \\ x \dfrac{dv}{dx} &= - 1 \\ dv &= - \dfrac{dx}{x} \\ v &= -\ln x + c \\ \dfrac{y}{x} &= -\ln{|x|} + c \\ y &= -x \ln {|x|} + cx \end{align*} The book's answer is: $$ y = x \ln{|\frac{k}{x}|} $$ Where did I go wrong?","Below is a problem I did. However, it did not match the back of the book. I would like to know where I went wrong. Problem: Solve the following differential equation. Answer: The book's answer is: Where did I go wrong?"," y' = \frac{y-x}{x}  \begin{align*}
\frac{dy}{dx} &= \dfrac{y}{x} - \dfrac{x}{x} =  \dfrac{y}{x} - 1 \\
y &= xv \\
\dfrac{dy}{dx} &= x \dfrac{dv}{dx} + v \\
x \dfrac{dv}{dx} + v &= v - 1 \\
x \dfrac{dv}{dx} &= - 1 \\
dv &= - \dfrac{dx}{x} \\
v &= -\ln x + c \\
\dfrac{y}{x} &= -\ln{|x|} + c \\
y &= -x \ln {|x|} + cx
\end{align*}  y = x \ln{|\frac{k}{x}|} ",['ordinary-differential-equations']
65,How do we solve $y''=e^{2y}$?,How do we solve ?,y''=e^{2y},"I tried to make the substitution $v=y'(x)$ , which would ( hopefully ) make this into a separable ODE as per the following. $$y''=e^{2y}$$ by the chain rule, we have $v'=v'(y(x)) \cdot y'(x)=\frac{dv}{dy}\frac{dy}{dx}$ . If we make the substitution, then $$\frac{dv}{dy}\frac{dy}{dx}=e^{2y}$$ So we could cancel the differential terms $dy$ ? I wonder if we can really do so, and also a better substitution might be just $v=y(x)$ . Am I on the right track here?","I tried to make the substitution , which would ( hopefully ) make this into a separable ODE as per the following. by the chain rule, we have . If we make the substitution, then So we could cancel the differential terms ? I wonder if we can really do so, and also a better substitution might be just . Am I on the right track here?",v=y'(x) y''=e^{2y} v'=v'(y(x)) \cdot y'(x)=\frac{dv}{dy}\frac{dy}{dx} \frac{dv}{dy}\frac{dy}{dx}=e^{2y} dy v=y(x),['ordinary-differential-equations']
66,Order and degree of a differential equation,Order and degree of a differential equation,,"Here is a question in my book Find the order and degree of the differential equation $$y=1+\frac{dy}{dx}+\frac{1}{2!}{\left(\frac{dy}{dx}\right)}^2+\frac{1}{3!}{\left(\frac{dy}{dx}\right)}^3+\cdots$$ At first sight we can conclude that the order is $1$ and the degree is undefined as  as the power of $\frac{dy}{dx}$ continues to increase and has no limit.However  my book gives the following solution Rewrite the DE as $$y=\exp\left({\frac{dy}{dx}}\right)$$ $$\implies \frac{dy}{dx}=\ln y$$ whose order and degree is 1 . Now ,I completely agree with this solution however I find it rather counterintuitive to my first line of thought .If the book is correct how can it be justified to prove my intuition was wrong?","Here is a question in my book Find the order and degree of the differential equation At first sight we can conclude that the order is and the degree is undefined as  as the power of continues to increase and has no limit.However  my book gives the following solution Rewrite the DE as whose order and degree is 1 . Now ,I completely agree with this solution however I find it rather counterintuitive to my first line of thought .If the book is correct how can it be justified to prove my intuition was wrong?",y=1+\frac{dy}{dx}+\frac{1}{2!}{\left(\frac{dy}{dx}\right)}^2+\frac{1}{3!}{\left(\frac{dy}{dx}\right)}^3+\cdots 1 \frac{dy}{dx} y=\exp\left({\frac{dy}{dx}}\right) \implies \frac{dy}{dx}=\ln y,"['ordinary-differential-equations', 'intuition']"
67,How do I solve a first order differential equation of the following form,How do I solve a first order differential equation of the following form,,If I had a differential equation $x'(t) + \frac{x(t)}{t} = e^{t^2}$ And I have a question saying solve the following first ODE for $x(t)$ how would I go about doing this?,If I had a differential equation And I have a question saying solve the following first ODE for how would I go about doing this?,x'(t) + \frac{x(t)}{t} = e^{t^2} x(t),"['calculus', 'ordinary-differential-equations']"
68,Two convex functions equal on the natural numbers are equal,Two convex functions equal on the natural numbers are equal,,I have to prove that there is one and only one function $f \in C^1(\Bbb R_{>0})$ that satisfy the following. : $$\begin{aligned}f(x+1) - f(x) &= \ln(x)\\ \ \  \ f \rm{\ is\ convex}&\\f(1) &= 0\end{aligned}$$ I am not asked to find $f$ but just to prove its uniqueness. So far I thought of the following. Let $g$ that satisfy the set of equations above. Then $f = g$ on $\mathbb{N}$ . So maybe the following is true : Two convex functions that are equal on the natural numbers are equal on $\Bbb R_{>0}$ . Yet I am also unable to prove it or find a counterexample.,I have to prove that there is one and only one function that satisfy the following. : I am not asked to find but just to prove its uniqueness. So far I thought of the following. Let that satisfy the set of equations above. Then on . So maybe the following is true : Two convex functions that are equal on the natural numbers are equal on . Yet I am also unable to prove it or find a counterexample.,f \in C^1(\Bbb R_{>0}) \begin{aligned}f(x+1) - f(x) &= \ln(x)\\ \ \  \ f \rm{\ is\ convex}&\\f(1) &= 0\end{aligned} f g f = g \mathbb{N} \Bbb R_{>0},"['real-analysis', 'ordinary-differential-equations', 'analysis', 'derivatives', 'logarithms']"
69,How are restrictions introduced to a differential equation addressed when we have found the equation's solutions?,How are restrictions introduced to a differential equation addressed when we have found the equation's solutions?,,"This is how I was shown how to solve the following differential equation: $xy'=\sqrt{x^2-y^2}+y  \qquad \rightarrow \qquad y'=\sqrt{1-\frac{y^2}{x^2}}+\frac{y}{x} \qquad [1] $ $\\$ let $z=\frac{y}{x}\qquad \rightarrow y=zx \qquad \rightarrow \frac{dy}{dx}=z+x\frac{dz}{dx}=\sqrt{1-z^2}+z $ $\hspace{5.6cm}\rightarrow x\frac{dz}{dx}=\sqrt{1-z^2} $ $\hspace{5.6cm}\rightarrow \int \frac{1}{\sqrt{1-z^2}} dz=\int \frac{1}{x}dx \qquad$ provided $z^2\neq1 \hspace{0.7cm}and \hspace{0.6cm}x\neq0$ $\hspace{5.6cm}\rightarrow \arcsin(z)=\ln|x|+C$ $\hspace{5.6cm}\rightarrow \frac{y}{x}=\sin(\ln|Cx|)$ since $z=\pm 1 \hspace{0.5cm}$ was excluded from our workings we consider it as a possible solution and find out that it corresponds to a solution: $y=\pm x$ I was told that dividing by x at step [1] was fine as the equation would be ""rubbish"" otherwise. I understand all the workings that follows after [1]; however, my not understanding of [1] I feel like points to a misconception I have of differential equations. In the next few lines I'll write my understanding of step [1] and would appreciate it if you'd correct me anywhere I'm wrong: I think what my tutor meant when he said, the equation is useless when $x=0$ was when $x=0$ is being considered as a line and not as the coordinate of a point. It's reasonable to say that an expression containing $y'$ is useless when we're dealing with the line $x=0$ (or, as a matter of fact, any line of the form $x=a$ ). Granted, that way of thinking about $x=0$ would make the differential equation useless for $x=0$ . But, what about $x=0$ as the coordinate of a point? If it's just a point $y'$ no longer needs to be meaningless (as long as the point whose x coordinate is 0 is a point on a continuous curve). So, in other words, $x=0$ referring to a single point should be allowed and possibly correspond to points on the integral curves of the equation. The solution $y=x$ to the equation suggests that $x=0$ makes sense as a coordinate; but I don't understand why. Why are we including points with x-coordinates $x=0$ in a solution that is dependent on a step at which we divide by x? $x=0$ should completely be excluded from any solutions that include in their derivation the step [1]. So, the solutions should instead be rewritten as: $y=\pm x \quad$ where $x\neq 0 \quad$ and $ \quad \frac{y}{x}=\sin(\ln|Cx|) \quad$ where $\quad x \neq 0 \quad $ and $\quad C \neq 0$ This would then give rise to the issue that all points with an x-coordinate $x=0$ are being completely ignored; while, they have as much right to be a part of the solution as any other point does (the fact that they're being excluded is simply because the only method that has yielded solutions uses the step [1]). So, how do we show that the point (0,0) does, in fact, belong to the integral curves $y=\pm x$ ? I really hope that I've made at least some sense in the past few lines. I'd greatly appreciate it if you would correct my misconceptions about solving differential equations.","This is how I was shown how to solve the following differential equation: let provided since was excluded from our workings we consider it as a possible solution and find out that it corresponds to a solution: I was told that dividing by x at step [1] was fine as the equation would be ""rubbish"" otherwise. I understand all the workings that follows after [1]; however, my not understanding of [1] I feel like points to a misconception I have of differential equations. In the next few lines I'll write my understanding of step [1] and would appreciate it if you'd correct me anywhere I'm wrong: I think what my tutor meant when he said, the equation is useless when was when is being considered as a line and not as the coordinate of a point. It's reasonable to say that an expression containing is useless when we're dealing with the line (or, as a matter of fact, any line of the form ). Granted, that way of thinking about would make the differential equation useless for . But, what about as the coordinate of a point? If it's just a point no longer needs to be meaningless (as long as the point whose x coordinate is 0 is a point on a continuous curve). So, in other words, referring to a single point should be allowed and possibly correspond to points on the integral curves of the equation. The solution to the equation suggests that makes sense as a coordinate; but I don't understand why. Why are we including points with x-coordinates in a solution that is dependent on a step at which we divide by x? should completely be excluded from any solutions that include in their derivation the step [1]. So, the solutions should instead be rewritten as: where and where and This would then give rise to the issue that all points with an x-coordinate are being completely ignored; while, they have as much right to be a part of the solution as any other point does (the fact that they're being excluded is simply because the only method that has yielded solutions uses the step [1]). So, how do we show that the point (0,0) does, in fact, belong to the integral curves ? I really hope that I've made at least some sense in the past few lines. I'd greatly appreciate it if you would correct my misconceptions about solving differential equations.",xy'=\sqrt{x^2-y^2}+y  \qquad \rightarrow \qquad y'=\sqrt{1-\frac{y^2}{x^2}}+\frac{y}{x} \qquad [1]  \\ z=\frac{y}{x}\qquad \rightarrow y=zx \qquad \rightarrow \frac{dy}{dx}=z+x\frac{dz}{dx}=\sqrt{1-z^2}+z  \hspace{5.6cm}\rightarrow x\frac{dz}{dx}=\sqrt{1-z^2}  \hspace{5.6cm}\rightarrow \int \frac{1}{\sqrt{1-z^2}} dz=\int \frac{1}{x}dx \qquad z^2\neq1 \hspace{0.7cm}and \hspace{0.6cm}x\neq0 \hspace{5.6cm}\rightarrow \arcsin(z)=\ln|x|+C \hspace{5.6cm}\rightarrow \frac{y}{x}=\sin(\ln|Cx|) z=\pm 1 \hspace{0.5cm} y=\pm x x=0 x=0 y' x=0 x=a x=0 x=0 x=0 y' x=0 y=x x=0 x=0 x=0 y=\pm x \quad x\neq 0 \quad  \quad \frac{y}{x}=\sin(\ln|Cx|) \quad \quad x \neq 0 \quad  \quad C \neq 0 x=0 y=\pm x,['ordinary-differential-equations']
70,How to find the first derivative of this?,How to find the first derivative of this?,,$24000(1+\frac{0.061}{12})^{12t}  + 11000\cdot e^{0.097t}$ I'm trying to find the derivative of this but keep getting confused on the steps. Can anyone explain the steps and solution to this problem.,I'm trying to find the derivative of this but keep getting confused on the steps. Can anyone explain the steps and solution to this problem.,24000(1+\frac{0.061}{12})^{12t}  + 11000\cdot e^{0.097t},"['calculus', 'ordinary-differential-equations', 'derivatives']"
71,Differential equation from an exam,Differential equation from an exam,,"Hello there yesterday in my exam I had the following problem: Knowing that $y_1=x$ is a solution, solve: $$(x^2 - 1)y''+2xy'-2y=0$$ My try was to use Liouville-Ostrogradski formula (see: https://www.encyclopediaofmath.org/index.php/Liouville-Ostrogradski_formula ) So bassically what I have is (I will denote with C the second Wronskian since is just a constant): $$y_1y_2'-y_1'y_2=Ce^{-\int{\frac{2x}{x^2-1}}dx}$$ $$xy_2'-y_2=\frac{C}{x^2-1}\rightarrow \frac{y_2'}{x}-\frac{y_2}{x^2}=\frac{C}{x^2(x^2-1)}$$  $$\left(\frac{y_2}{x}\right)'=\frac{C}{(x^2-1)x^2}=\frac{C}{(x^2-1)}-\frac{C}{x^2}$$ By integrating : $$\frac{y_2}{x}=\left(\frac{C}{2}\ln\left|\frac{x-1}{x+1}\right|+\frac{C}{x}+C_2\right)$$$$y_2=c_1x\ln\left|\frac{x-1}{x+1}\right|+c_2x +c_3$$ Was my solution correct and complete? I am wondering if  I must prove that this differential equation has only those two solutions, but I have no ideea how.","Hello there yesterday in my exam I had the following problem: Knowing that $y_1=x$ is a solution, solve: $$(x^2 - 1)y''+2xy'-2y=0$$ My try was to use Liouville-Ostrogradski formula (see: https://www.encyclopediaofmath.org/index.php/Liouville-Ostrogradski_formula ) So bassically what I have is (I will denote with C the second Wronskian since is just a constant): $$y_1y_2'-y_1'y_2=Ce^{-\int{\frac{2x}{x^2-1}}dx}$$ $$xy_2'-y_2=\frac{C}{x^2-1}\rightarrow \frac{y_2'}{x}-\frac{y_2}{x^2}=\frac{C}{x^2(x^2-1)}$$  $$\left(\frac{y_2}{x}\right)'=\frac{C}{(x^2-1)x^2}=\frac{C}{(x^2-1)}-\frac{C}{x^2}$$ By integrating : $$\frac{y_2}{x}=\left(\frac{C}{2}\ln\left|\frac{x-1}{x+1}\right|+\frac{C}{x}+C_2\right)$$$$y_2=c_1x\ln\left|\frac{x-1}{x+1}\right|+c_2x +c_3$$ Was my solution correct and complete? I am wondering if  I must prove that this differential equation has only those two solutions, but I have no ideea how.",,['ordinary-differential-equations']
72,"If $f(x) = e^{-|x|}$, show that $f''(x) - f(x) = -2\delta(x)$ (in the sense of distributions)","If , show that  (in the sense of distributions)",f(x) = e^{-|x|} f''(x) - f(x) = -2\delta(x),"Exercise : If $f(x) = e^{-|x|}$, show that for its derivatives, it is : $f''(x)-f(x) = -2\delta(x)$, in the sense of distributions. Attempt : I am completely at a loss on how to handle such an exercise as we haven't studied distributions for more than 1-2 lessons, but I know that : The function $f(x)$ is continuous and differentiable in $\mathbb R \setminus \{0\} = \mathbb R^*$, with derivative : $$f'(x) = \begin{cases} -e^{-x}, & x>0 \\ e^x, & x<0\end{cases}$$ How would I proceed with modeling the derivatives now in the sense of distributions to prove the equation asked? To be mentioned, the function $\delta(x)$ is the famous Dirac-delta function, such that: $$\delta(x) = \begin{cases} +\infty & x=0, \\ 0 & x \neq 0 \end{cases}$$","Exercise : If $f(x) = e^{-|x|}$, show that for its derivatives, it is : $f''(x)-f(x) = -2\delta(x)$, in the sense of distributions. Attempt : I am completely at a loss on how to handle such an exercise as we haven't studied distributions for more than 1-2 lessons, but I know that : The function $f(x)$ is continuous and differentiable in $\mathbb R \setminus \{0\} = \mathbb R^*$, with derivative : $$f'(x) = \begin{cases} -e^{-x}, & x>0 \\ e^x, & x<0\end{cases}$$ How would I proceed with modeling the derivatives now in the sense of distributions to prove the equation asked? To be mentioned, the function $\delta(x)$ is the famous Dirac-delta function, such that: $$\delta(x) = \begin{cases} +\infty & x=0, \\ 0 & x \neq 0 \end{cases}$$",,"['ordinary-differential-equations', 'partial-differential-equations', 'distribution-theory', 'dirac-delta']"
73,Initial value problem for second order linear differential equation : why am I only getting zero as a solution?,Initial value problem for second order linear differential equation : why am I only getting zero as a solution?,,"I am given the following ODE : $y'' - 2ay'+by = 0$ for some real constants $a,b$, along with the initial conditions $y(0) = y(1) = 0$. Furthermore I know that some function $y$ is a solution of this ODE. How do I show that $y(n) = 0$ for all natural numbers $n$? The idea would be to solve the equation : it is a second order linear differential equation. Hence, (skipping rigour) we solve $r^2 - 2ar + b = 0$ to get solutions $r_1,r_2$ which may or may not be equal (and may be complex). The solution is now given by : 1 : $y(t) = c_1 e^{r_1 t} + c_2 e^{r_2 t}$ if $r_1 \neq r_2$ (note that if they are complex then by Euler's formula we have a linear combination of trigonometric functions coming in) 2 : $y(t) = c_1 e^{r_1t} + c_2te^{r_1}t$ if $r_1 = r_2$. In case 1 , substituting $y(0) = 0$ gives $c_1 + c_2 = 0$ and combining with $y(1)=0$ gives $e^{r_1} = e^{r_2}$ and $c_1 = -c_2$, so $\color{red}{e^{r_1 t} = e^{r_2 t} \mbox{ for all } t}$ hence the solution is identically zero. In case 2, substituting $y(0) = 0$ gives $c_1 = 0$  and then $y(1) = 0$ gives $c_2 = 0$ so the solution is identically zero. There is definitely something wrong here. I'd like people to point it out, since I think I've blindly used the formula given to me here. This function is supposed to be non-zero at least at the integers, and the fact that no more is provided hints that there are non-trivial solutions. EDIT : I found the error thanks to the great people below. You may find the error colored in red above.","I am given the following ODE : $y'' - 2ay'+by = 0$ for some real constants $a,b$, along with the initial conditions $y(0) = y(1) = 0$. Furthermore I know that some function $y$ is a solution of this ODE. How do I show that $y(n) = 0$ for all natural numbers $n$? The idea would be to solve the equation : it is a second order linear differential equation. Hence, (skipping rigour) we solve $r^2 - 2ar + b = 0$ to get solutions $r_1,r_2$ which may or may not be equal (and may be complex). The solution is now given by : 1 : $y(t) = c_1 e^{r_1 t} + c_2 e^{r_2 t}$ if $r_1 \neq r_2$ (note that if they are complex then by Euler's formula we have a linear combination of trigonometric functions coming in) 2 : $y(t) = c_1 e^{r_1t} + c_2te^{r_1}t$ if $r_1 = r_2$. In case 1 , substituting $y(0) = 0$ gives $c_1 + c_2 = 0$ and combining with $y(1)=0$ gives $e^{r_1} = e^{r_2}$ and $c_1 = -c_2$, so $\color{red}{e^{r_1 t} = e^{r_2 t} \mbox{ for all } t}$ hence the solution is identically zero. In case 2, substituting $y(0) = 0$ gives $c_1 = 0$  and then $y(1) = 0$ gives $c_2 = 0$ so the solution is identically zero. There is definitely something wrong here. I'd like people to point it out, since I think I've blindly used the formula given to me here. This function is supposed to be non-zero at least at the integers, and the fact that no more is provided hints that there are non-trivial solutions. EDIT : I found the error thanks to the great people below. You may find the error colored in red above.",,['ordinary-differential-equations']
74,Solution by differential equation by Clairaut Form,Solution by differential equation by Clairaut Form,,Solve the differential equation: $$y+x \frac{dy}{dx}=x^4 \bigg(\frac{dy}{dx}\bigg)^2$$ This is given under 'Clairaut form' but I am not able to convert it to Clairaut form of type $y=px+f(p)$ where $p=dy/dx$. The general solution is given as $xy+c=c^2x$ and singular solution is $4x^2y+1=0$. Could someone give me some hint with this?,Solve the differential equation: $$y+x \frac{dy}{dx}=x^4 \bigg(\frac{dy}{dx}\bigg)^2$$ This is given under 'Clairaut form' but I am not able to convert it to Clairaut form of type $y=px+f(p)$ where $p=dy/dx$. The general solution is given as $xy+c=c^2x$ and singular solution is $4x^2y+1=0$. Could someone give me some hint with this?,,"['calculus', 'ordinary-differential-equations', 'singular-solution']"
75,Understanding time reversal symmetry in differential equations,Understanding time reversal symmetry in differential equations,,"(In Strogatz' Nonlinear dynamics and chaos, page 163) I've read that any mechanical system of the form: $mx'' = F(x)$ is symmetric under time reversal. The author notes that if we make the change t -> -t that the second derivative is unchanged while the first derivative is reversed. My first question is that I see that $x(-t)'$ = $-x(-t)'$  and $x(-t)''$ = $x(-t)''$, however, I'm not quite clear why exactly the mapping t -> -t is of interest to us. If the idea is to reverse time, then we would want to start at the end of our motion and work backwards along the trajectory we originally came in on. Suppose our original trajectory goes from time t = $t_1$ to t = $t_9$. So x(-t) will map the largest t value in our original trajectory to the ""farthest left"" t value on the negative x-axis. Does that mean after we flip our trajectory over the y-axis that we are now interested in the dynamics of a path in the time range t = -9 to t = -1? It seems a bit odd that we would now be looking at a time range over negative time values. Is this a correct interpretation? Is there a better way to look at it? Thanks.","(In Strogatz' Nonlinear dynamics and chaos, page 163) I've read that any mechanical system of the form: $mx'' = F(x)$ is symmetric under time reversal. The author notes that if we make the change t -> -t that the second derivative is unchanged while the first derivative is reversed. My first question is that I see that $x(-t)'$ = $-x(-t)'$  and $x(-t)''$ = $x(-t)''$, however, I'm not quite clear why exactly the mapping t -> -t is of interest to us. If the idea is to reverse time, then we would want to start at the end of our motion and work backwards along the trajectory we originally came in on. Suppose our original trajectory goes from time t = $t_1$ to t = $t_9$. So x(-t) will map the largest t value in our original trajectory to the ""farthest left"" t value on the negative x-axis. Does that mean after we flip our trajectory over the y-axis that we are now interested in the dynamics of a path in the time range t = -9 to t = -1? It seems a bit odd that we would now be looking at a time range over negative time values. Is this a correct interpretation? Is there a better way to look at it? Thanks.",,"['calculus', 'ordinary-differential-equations']"
76,Asymptotic behaviour of $f$ and $f'+f$,Asymptotic behaviour of  and,f f'+f,"Let $f~:\mathbb{R}\longrightarrow \mathbb{R}$ a $C^1$ map. How do you prove that if $f'+f$ vanish when $t\to +\infty$ so do $f$? If $\lim\limits_{t\to +\infty} f(t)=l$ with $l\neq 0$ : I first look at the case $l$ finite : we have $\lim\limits_{t\to +\infty} f'(t)=-l$ so for every $\varepsilon >0$ there exist $C>0$ such for all $t>C$, $|f(t)-l|$ and $|f'(t)+l|$ are both $\leq \varepsilon$. But I can't find a contradiction. We know that $f'$ has a limit in $+\infty$ does not implies that there exist an asymptote so I don't really know what to do. I also tried to write $g=f+f'$ and write $f$ has a solution of a differantial equation on $g$. Any help will be appriaciated.","Let $f~:\mathbb{R}\longrightarrow \mathbb{R}$ a $C^1$ map. How do you prove that if $f'+f$ vanish when $t\to +\infty$ so do $f$? If $\lim\limits_{t\to +\infty} f(t)=l$ with $l\neq 0$ : I first look at the case $l$ finite : we have $\lim\limits_{t\to +\infty} f'(t)=-l$ so for every $\varepsilon >0$ there exist $C>0$ such for all $t>C$, $|f(t)-l|$ and $|f'(t)+l|$ are both $\leq \varepsilon$. But I can't find a contradiction. We know that $f'$ has a limit in $+\infty$ does not implies that there exist an asymptote so I don't really know what to do. I also tried to write $g=f+f'$ and write $f$ has a solution of a differantial equation on $g$. Any help will be appriaciated.",,"['ordinary-differential-equations', 'derivatives', 'asymptotics']"
77,Proving condition is sufficient for long term existence of ODE solution,Proving condition is sufficient for long term existence of ODE solution,,"I am working on the following problem from Gerald Teschl's book on ODE's and am at a loss of how to proceed. Suppose $U=\mathbb{R} \times \mathbb{R}^n$ and that $|f(t,x)| \leq g(|x|)$ for some positive continuous function $g \in C([0,\infty))$ which satisfies   $$\int_0^{\infty} \frac{dr}{g(r)} = \infty$$   Then all solutions of the IVP $f(t,x) = \dot{x}$, $x(0) = x_0$ are defined for all $t \geq 0$. Show that the same conclusion still holds if there is such a function $g_T(r)$ for every $t \in [0,T]$. (Hint: Look at the differential equation for $r(t)^2 = |x(t)|^2$.) I am not sure how to use the hint and I have not been successful in any of my attempts at the problem.  Any help would be appreciated.  Thanks!","I am working on the following problem from Gerald Teschl's book on ODE's and am at a loss of how to proceed. Suppose $U=\mathbb{R} \times \mathbb{R}^n$ and that $|f(t,x)| \leq g(|x|)$ for some positive continuous function $g \in C([0,\infty))$ which satisfies   $$\int_0^{\infty} \frac{dr}{g(r)} = \infty$$   Then all solutions of the IVP $f(t,x) = \dot{x}$, $x(0) = x_0$ are defined for all $t \geq 0$. Show that the same conclusion still holds if there is such a function $g_T(r)$ for every $t \in [0,T]$. (Hint: Look at the differential equation for $r(t)^2 = |x(t)|^2$.) I am not sure how to use the hint and I have not been successful in any of my attempts at the problem.  Any help would be appreciated.  Thanks!",,['ordinary-differential-equations']
78,"What does ""trivial solution"" mean?","What does ""trivial solution"" mean?",,"What does ""trivial solution"" mean exactly? Must the trivial solution always be equal to the zero-solution (where all unknowns/variables are zero)?","What does ""trivial solution"" mean exactly? Must the trivial solution always be equal to the zero-solution (where all unknowns/variables are zero)?",,"['ordinary-differential-equations', 'systems-of-equations']"
79,How to determine the optimal control law?,How to determine the optimal control law?,,"Given the differential equation $$\dot x = -2x + u$$ determine the optimal control law $u = - kx$ that minimizes the performance index $$J = \int_0^{\infty} x^2 \, \mathrm d t$$ My approach was to find the state feedback $k$ . But since the value of $R$ (positive semidefinite Hermitian) is not given, that means $R=0$ . How do I determine the optimal control for this system where $R=0$ ?","Given the differential equation determine the optimal control law that minimizes the performance index My approach was to find the state feedback . But since the value of (positive semidefinite Hermitian) is not given, that means . How do I determine the optimal control for this system where ?","\dot x = -2x + u u = - kx J = \int_0^{\infty} x^2 \, \mathrm d t k R R=0 R=0","['ordinary-differential-equations', 'optimization', 'control-theory', 'optimal-control', 'linear-control']"
80,Exponential of the zero matrix,Exponential of the zero matrix,,"It is well-known that the exponential of the zero matrix is the identity matrix, but I tried to prove the opposite implication, and I failed. Can you help me to show that $e^A =I$ implies $A =0$ ? Many thanks!!","It is well-known that the exponential of the zero matrix is the identity matrix, but I tried to prove the opposite implication, and I failed. Can you help me to show that $e^A =I$ implies $A =0$ ? Many thanks!!",,"['linear-algebra', 'ordinary-differential-equations']"
81,Let the function satisfy $f(x)f'(-x)=f(-x)f'(x)$ and $f(0)=3$ for all $x$,Let the function satisfy  and  for all,f(x)f'(-x)=f(-x)f'(x) f(0)=3 x,"Question Let the function satisfy $f(x)f'(-x)=f(-x)f'(x)$ and $f(0)=3$ for all $x$. Then find the number of roots of $f(x)=0$ in $[-2,2]$ and evaluate $\displaystyle\int\limits_{-51}^{51}\frac{\mathrm dx}{3+f(x)}$. My Attempt I have solved this equation: \begin{align} f(x)f'(-x)&=f(-x)f'(x);\\\\ \frac{f'(-x)}{f(-x)}&=\frac{f'(x)}{f(x)};\\\\ \int\frac{f'(-x)}{f(-x)}\,\mathrm dx&=\int\frac{f'(x)}{f(x)}\,\mathrm dx;\\\\ \int\frac{\mathrm df(-x)}{f(-x)}&=-\int\frac{\mathrm df(x)}{f(x)};\\\\ \log(f(-x))&=-\log(f(x))+\log(c). \end{align} Using the initial condition $f(0)=3$ I have found $$f(x)f(-x)=9,$$ but I do not know how to find $f(x)$ and hence how to find the number of roots of $f(x)$ in $[-2,2]$ and how to evaluate the given integral.","Question Let the function satisfy $f(x)f'(-x)=f(-x)f'(x)$ and $f(0)=3$ for all $x$. Then find the number of roots of $f(x)=0$ in $[-2,2]$ and evaluate $\displaystyle\int\limits_{-51}^{51}\frac{\mathrm dx}{3+f(x)}$. My Attempt I have solved this equation: \begin{align} f(x)f'(-x)&=f(-x)f'(x);\\\\ \frac{f'(-x)}{f(-x)}&=\frac{f'(x)}{f(x)};\\\\ \int\frac{f'(-x)}{f(-x)}\,\mathrm dx&=\int\frac{f'(x)}{f(x)}\,\mathrm dx;\\\\ \int\frac{\mathrm df(-x)}{f(-x)}&=-\int\frac{\mathrm df(x)}{f(x)};\\\\ \log(f(-x))&=-\log(f(x))+\log(c). \end{align} Using the initial condition $f(0)=3$ I have found $$f(x)f(-x)=9,$$ but I do not know how to find $f(x)$ and hence how to find the number of roots of $f(x)$ in $[-2,2]$ and how to evaluate the given integral.",,"['calculus', 'ordinary-differential-equations', 'definite-integrals']"
82,"Find all smooth functions such that $f(x)f(y)=\int_{x-y}^{x+y}f(t)dt$ for all $x,y \in \mathbb{R}$",Find all smooth functions such that  for all,"f(x)f(y)=\int_{x-y}^{x+y}f(t)dt x,y \in \mathbb{R}","From Art of Problem Solving: Find all continuous, differentiable functions f with domain $\mathbb{R}$ such that   $$f(x)f(y)=\int_{x-y}^{x+y}f(t)dt$$   for all $x,y \in \mathbb{R}$. Find all $Polynomial Functions$ $Trigonometric Functions$ $Functions$ Then prove all functions found are the only functions that satisfy the equation. As some hints provided by the book, the following functions work: $2x$ $c$ $\frac{2}{c}\sin(cx)$ It is also noted that there is another family of functions that satisfies the equation. The problem should be able to be solved without any multivariable methods. As of now I have only been able to conclude types of functions that satisfy the equation when $x=0$ or $y=0$.","From Art of Problem Solving: Find all continuous, differentiable functions f with domain $\mathbb{R}$ such that   $$f(x)f(y)=\int_{x-y}^{x+y}f(t)dt$$   for all $x,y \in \mathbb{R}$. Find all $Polynomial Functions$ $Trigonometric Functions$ $Functions$ Then prove all functions found are the only functions that satisfy the equation. As some hints provided by the book, the following functions work: $2x$ $c$ $\frac{2}{c}\sin(cx)$ It is also noted that there is another family of functions that satisfies the equation. The problem should be able to be solved without any multivariable methods. As of now I have only been able to conclude types of functions that satisfy the equation when $x=0$ or $y=0$.",,"['calculus', 'ordinary-differential-equations']"
83,On why we have $dy = f'(x)dx$?,On why we have ?,dy = f'(x)dx,"I am following Ordinary differential equations by Tenenbaum. Page 48 The differential is defined as: $$dy(x,\Delta x) = f'(x) \Delta x$$ Note: we will want to apply this definition to the function defined by $y = x$. Therefore, in order to distinguish between the function defined by $y = x$ and the variable x, we place the symbol $\hat{}$ over the x so that:   $y = \hat{x}$ will define the function that assigns to each value of the independent variable x the same unique value to the dependent variable y. (in a Cartesian plane a horizontal line?) Theorem 6.2 If, $$y = \hat{x}$$ then $$dy(x,\Delta x)= (d\hat{x})(x,\Delta x) = \Delta x$$ Straight after this comes the thing I do not clearly understand, the book says: Comment 6.3: Replace the value found for $\Delta x$ (from theorem 6.2) in the definition of differential, we obtain: $$dy(x,\Delta x)= f'(x)(d\hat{x})(x,\Delta x) $$ ... this relation is the correct one, but in the course of time, it became customary to write it down in the familiar form: $$dy = f'(x)dx$$ The book then proceeds to use this formula (that I know is correct) in any case. But wouldnt this formula be only relevant in the case $y = \hat{x}$ since it was found relying on theorem 6.2 that is only valid if $y = \hat{x}$?","I am following Ordinary differential equations by Tenenbaum. Page 48 The differential is defined as: $$dy(x,\Delta x) = f'(x) \Delta x$$ Note: we will want to apply this definition to the function defined by $y = x$. Therefore, in order to distinguish between the function defined by $y = x$ and the variable x, we place the symbol $\hat{}$ over the x so that:   $y = \hat{x}$ will define the function that assigns to each value of the independent variable x the same unique value to the dependent variable y. (in a Cartesian plane a horizontal line?) Theorem 6.2 If, $$y = \hat{x}$$ then $$dy(x,\Delta x)= (d\hat{x})(x,\Delta x) = \Delta x$$ Straight after this comes the thing I do not clearly understand, the book says: Comment 6.3: Replace the value found for $\Delta x$ (from theorem 6.2) in the definition of differential, we obtain: $$dy(x,\Delta x)= f'(x)(d\hat{x})(x,\Delta x) $$ ... this relation is the correct one, but in the course of time, it became customary to write it down in the familiar form: $$dy = f'(x)dx$$ The book then proceeds to use this formula (that I know is correct) in any case. But wouldnt this formula be only relevant in the case $y = \hat{x}$ since it was found relying on theorem 6.2 that is only valid if $y = \hat{x}$?",,"['ordinary-differential-equations', 'notation']"
84,Partial derivative function definition paradox,Partial derivative function definition paradox,,"I've pondered this question over quite alot and haven't been able to find an answer anywhere.  I'm going to ask this question from the standpoint of basic thermodynamics.  Let's say I define $p(\rho,T) = \rho R T$ where $R$ is a constant. $\dfrac{\partial T}{\partial \rho}=0$ since T and $\rho$ are defined independent Later on lets say we rearrange the function definition and define $T(\rho,p)=\dfrac{p}{\rho R}$ then $\dfrac{\partial T(\rho,p)}{\partial \rho}=-\dfrac{p}{\rho^2 R}$ = $\dfrac{\partial T}{\partial \rho}$ $0\neq-\dfrac{p}{\rho^2 R}$ How am I supposed to know which derivative to use when using equations online where definitions are mixed like seen above?  Furthermore, it appears that $p(\rho,T)$ and $T(\rho,p)$ leads to $T(\rho,p(\rho,T))$ a seemingly recursive definition.  If you can, please elaborate. Thank you!!","I've pondered this question over quite alot and haven't been able to find an answer anywhere.  I'm going to ask this question from the standpoint of basic thermodynamics.  Let's say I define $p(\rho,T) = \rho R T$ where $R$ is a constant. $\dfrac{\partial T}{\partial \rho}=0$ since T and $\rho$ are defined independent Later on lets say we rearrange the function definition and define $T(\rho,p)=\dfrac{p}{\rho R}$ then $\dfrac{\partial T(\rho,p)}{\partial \rho}=-\dfrac{p}{\rho^2 R}$ = $\dfrac{\partial T}{\partial \rho}$ $0\neq-\dfrac{p}{\rho^2 R}$ How am I supposed to know which derivative to use when using equations online where definitions are mixed like seen above?  Furthermore, it appears that $p(\rho,T)$ and $T(\rho,p)$ leads to $T(\rho,p(\rho,T))$ a seemingly recursive definition.  If you can, please elaborate. Thank you!!",,"['real-analysis', 'ordinary-differential-equations', 'functions']"
85,Techniques for solving coupled differential equations,Techniques for solving coupled differential equations,,I am trying to solve a system of coupled differential equations to plot streamlines using Matlab. The equations are these: \begin{align} \frac{\mathrm dx}{\mathrm dt} &= -3x -5y \\ \frac{\mathrm dy}{\mathrm dt} &= 5x + 3y \end{align} What method do you suggest for solving this system? I'd greatly appreciate any insight or suggestion. No need to solve the system :) as long as you tell me what literature I can refer to. Thanks!,I am trying to solve a system of coupled differential equations to plot streamlines using Matlab. The equations are these: What method do you suggest for solving this system? I'd greatly appreciate any insight or suggestion. No need to solve the system :) as long as you tell me what literature I can refer to. Thanks!,"\begin{align}
\frac{\mathrm dx}{\mathrm dt} &= -3x -5y \\
\frac{\mathrm dy}{\mathrm dt} &= 5x + 3y
\end{align}","['ordinary-differential-equations', 'systems-of-equations']"
86,Shorter solution to differential equation?,Shorter solution to differential equation?,,"I'm looking for a shorter way to find a maximal solution to the differential equation  $$y''-2y'+y=xe^x+e^x\cos(x)$$  $$y(0)=y'(0)=1$$ At first I was hoping I could convert the right side to $e^x(g(x))$ with g(x) polynomial, but that didn't work out. So my paths turn out to be very long (with solution $e^x(x^3/6-\cos(x)+2)$). I'd appreciate any help.","I'm looking for a shorter way to find a maximal solution to the differential equation  $$y''-2y'+y=xe^x+e^x\cos(x)$$  $$y(0)=y'(0)=1$$ At first I was hoping I could convert the right side to $e^x(g(x))$ with g(x) polynomial, but that didn't work out. So my paths turn out to be very long (with solution $e^x(x^3/6-\cos(x)+2)$). I'd appreciate any help.",,['ordinary-differential-equations']
87,Intuition behind the weight function,Intuition behind the weight function,,"The inner product in a $L^2$ space can be defined as: $$\langle f,g\rangle =\int_a^b \bar{f}(x)g(x)w(x)dx$$ For Legendre polynomials, we define it as: $$\langle P_m,P_n\rangle =\int_0^1 \bar{P}_m(x)P_n(x)dx$$ so $w(x)=1$. But there are case in which $w(x)\neq 1$. For example, Laguerre $w(x)=e^{-x}$ and Hermite polynomials $w(x)=e^{-x^2}$. Is there any intuition/motivation behind different weight functions of orthogonal polynomials ? I think it might be related to measure theory and Sturm-Liouville problems.","The inner product in a $L^2$ space can be defined as: $$\langle f,g\rangle =\int_a^b \bar{f}(x)g(x)w(x)dx$$ For Legendre polynomials, we define it as: $$\langle P_m,P_n\rangle =\int_0^1 \bar{P}_m(x)P_n(x)dx$$ so $w(x)=1$. But there are case in which $w(x)\neq 1$. For example, Laguerre $w(x)=e^{-x}$ and Hermite polynomials $w(x)=e^{-x^2}$. Is there any intuition/motivation behind different weight functions of orthogonal polynomials ? I think it might be related to measure theory and Sturm-Liouville problems.",,"['measure-theory', 'ordinary-differential-equations', 'orthogonal-polynomials']"
88,"Prove that if $\phi'(x) = \phi(x)$ and $\phi(0)=0$, then $\phi(x)\equiv 0$. Use this to prove the identity $e^{a+b} = e^a e^b$.","Prove that if  and , then . Use this to prove the identity .",\phi'(x) = \phi(x) \phi(0)=0 \phi(x)\equiv 0 e^{a+b} = e^a e^b,I am given the following. hint Consider $f(x)=e^{-x} \phi(x)$. I am unsure how to approach this problem.,I am given the following. hint Consider $f(x)=e^{-x} \phi(x)$. I am unsure how to approach this problem.,,"['real-analysis', 'analysis', 'ordinary-differential-equations', 'exponential-function']"
89,How to solve $ (x + 2y - 4)dx - (2x + y - 5)dy = 0$,How to solve, (x + 2y - 4)dx - (2x + y - 5)dy = 0,"How to solve the differential equation $(x + 2y - 4)dx - (2x + y - 5)dy = 0$. It's not separable, nor exact nor homogeneous. The solution is $(x - y -1)^3 = C(x + y - 3)$. How can I achieve it? The other equations similar to this are: $(1+ x + y)dy - (1- 3x - 3y)dx = 0$ Answer: $3x + y + 2ln(-x - y +1) = k$ $(3x - y + 2)dx + (9x - 3y +1)dy = 0$ Answer: $2x + 6y + C = ln(6x - 2y +1)$ If someone point me out how to solve the first equation I will be likely to solve the others. Thank you very much. Update: Given Orangutango and Chris help I moved to a solvable d.e. But didn't get the same answer my professor listed. Did I miss some step? (X + 2Y)dX = (2X+Y)dY dY/dX = (X + 2Y)/(2X + Y) Making a substitution to solve the now homogenous: Y = VX, Y'= V + V'X V+V'X = (X + 2VX)/(2X + VX) V+V'X = (1 + 2V)/(2 + V) V'X = ((1 + 2V)/(2 + V)) - V V'X = (1 - V^2)/(2 + V) (2 + V)dV/(1-V^2) = dX/X Integrating the left side I got: int (2 + V)dV/1-V^2 = 2 * int dV / (1-V^2) + int V dV / (1-V^2) = log | (v + 1) / (v-1) | - 1/2 log | V^2 + 1 | + c1 Integrating the right side: log X + c2 Then replacing V=Y/X and then X=x-2 and Y=y-1 don't seems to yield the proposed answers. Where did my professor got this? Is the above solution correct? Thanks again.","How to solve the differential equation $(x + 2y - 4)dx - (2x + y - 5)dy = 0$. It's not separable, nor exact nor homogeneous. The solution is $(x - y -1)^3 = C(x + y - 3)$. How can I achieve it? The other equations similar to this are: $(1+ x + y)dy - (1- 3x - 3y)dx = 0$ Answer: $3x + y + 2ln(-x - y +1) = k$ $(3x - y + 2)dx + (9x - 3y +1)dy = 0$ Answer: $2x + 6y + C = ln(6x - 2y +1)$ If someone point me out how to solve the first equation I will be likely to solve the others. Thank you very much. Update: Given Orangutango and Chris help I moved to a solvable d.e. But didn't get the same answer my professor listed. Did I miss some step? (X + 2Y)dX = (2X+Y)dY dY/dX = (X + 2Y)/(2X + Y) Making a substitution to solve the now homogenous: Y = VX, Y'= V + V'X V+V'X = (X + 2VX)/(2X + VX) V+V'X = (1 + 2V)/(2 + V) V'X = ((1 + 2V)/(2 + V)) - V V'X = (1 - V^2)/(2 + V) (2 + V)dV/(1-V^2) = dX/X Integrating the left side I got: int (2 + V)dV/1-V^2 = 2 * int dV / (1-V^2) + int V dV / (1-V^2) = log | (v + 1) / (v-1) | - 1/2 log | V^2 + 1 | + c1 Integrating the right side: log X + c2 Then replacing V=Y/X and then X=x-2 and Y=y-1 don't seems to yield the proposed answers. Where did my professor got this? Is the above solution correct? Thanks again.",,['ordinary-differential-equations']
90,Non-Linear Techniques,Non-Linear Techniques,,This Question is a practice exercise assigned to us. I have the answer but I don't find it very intuitive. The answer I have is on this pdf : http://math.stanford.edu/~ralph/math53h/solution5.pdf Find a global change of coordinates that linearizes the system $x' = x + y^2$ $y' = y$ $z' = z + y^2$ Please do not give negative rating before giving me a chance to make corrections if any errors are found in the question.,This Question is a practice exercise assigned to us. I have the answer but I don't find it very intuitive. The answer I have is on this pdf : http://math.stanford.edu/~ralph/math53h/solution5.pdf Find a global change of coordinates that linearizes the system $x' = x + y^2$ $y' = y$ $z' = z + y^2$ Please do not give negative rating before giving me a chance to make corrections if any errors are found in the question.,,['ordinary-differential-equations']
91,Which of the following is gradient/Hamiltonian( Conservative) system,Which of the following is gradient/Hamiltonian( Conservative) system,,"The question that I have to solve is found below. However, I do not know how to start the solution since I am unsure about the defintion of a Gradient/Hamiltonian System. What must I check first to know whether it is a Gradient or Hamiltonian System? Which of the following is gradient/Hamiltonian( Conservative) system or other, or neither, and why? And if they are find the potential: $\dot{x}=-2xe^{-x^2-y^2} \\  \dot{y}=-2ye^{-x^2-y^2}$ $\dot{x}= y+x^2y \\ \dot{y}=-x+2xy$ $\dot{x}=y \\ \dot{y}=y^3$ $\dot{x}=y\\ \dot{y}=-y-x^3$","The question that I have to solve is found below. However, I do not know how to start the solution since I am unsure about the defintion of a Gradient/Hamiltonian System. What must I check first to know whether it is a Gradient or Hamiltonian System? Which of the following is gradient/Hamiltonian( Conservative) system or other, or neither, and why? And if they are find the potential: $\dot{x}=-2xe^{-x^2-y^2} \\  \dot{y}=-2ye^{-x^2-y^2}$ $\dot{x}= y+x^2y \\ \dot{y}=-x+2xy$ $\dot{x}=y \\ \dot{y}=y^3$ $\dot{x}=y\\ \dot{y}=-y-x^3$",,"['ordinary-differential-equations', 'physics', 'mathematical-physics']"
92,On the solutions of $y''(t)=f(y(t))$.,On the solutions of .,y''(t)=f(y(t)),"This topic has been inspired by some time spending on trying to refining my knowledge about PDE's in general. Then everybody knows how to solve $$y''(t)=\pm y(t).$$ Then I tried to slightly modify the question and I focused on $$(\therefore)\;y''(t)=f(y(t)),\; f\in C^1(\mathbb R,\mathbb R).$$ What I was trying to do was to derive some general properties about the solutions to this equation. In particular I ask to you, since I was not able to answer myself: Must a solution of $(\therefore)$, not identically zero, have necessarily a finite number of zeroes on $[0,1]$? My idea was to derive an estimate like $$|y(\eta)-y(\xi)|\leq C|\eta-\xi|^p,\; p>1;$$ Moreover, if a solution $y$ were to have an infinite number of zeroes in $[0,1]$, the the set of zeroes should have an accumulation point, and in this point all the derivatives should be equal to $0$ by continuity, then maybe the function should remain to much squeezed to be different from zero. Hope you can help me because this interests me a lot. Many thanks for your attention. (I tried to post this on mathlinks as well but nobody answered me yet)","This topic has been inspired by some time spending on trying to refining my knowledge about PDE's in general. Then everybody knows how to solve $$y''(t)=\pm y(t).$$ Then I tried to slightly modify the question and I focused on $$(\therefore)\;y''(t)=f(y(t)),\; f\in C^1(\mathbb R,\mathbb R).$$ What I was trying to do was to derive some general properties about the solutions to this equation. In particular I ask to you, since I was not able to answer myself: Must a solution of $(\therefore)$, not identically zero, have necessarily a finite number of zeroes on $[0,1]$? My idea was to derive an estimate like $$|y(\eta)-y(\xi)|\leq C|\eta-\xi|^p,\; p>1;$$ Moreover, if a solution $y$ were to have an infinite number of zeroes in $[0,1]$, the the set of zeroes should have an accumulation point, and in this point all the derivatives should be equal to $0$ by continuity, then maybe the function should remain to much squeezed to be different from zero. Hope you can help me because this interests me a lot. Many thanks for your attention. (I tried to post this on mathlinks as well but nobody answered me yet)",,['ordinary-differential-equations']
93,Solving PDEs using change of variables.,Solving PDEs using change of variables.,,"Question: By changing the variables from $x$, $y$ to $s$, $t$, where $s = xy$ and $t = x/y$, solve the   equation $x\frac{\partial u}{\partial x}-y\frac{\partial u}{\partial y} = 2x^2$ The only part of solving this type of equation I am having trouble seeing how to do is how to get the values of $u_x$ and $u_y$ but I can carry on from there to solve the equation.","Question: By changing the variables from $x$, $y$ to $s$, $t$, where $s = xy$ and $t = x/y$, solve the   equation $x\frac{\partial u}{\partial x}-y\frac{\partial u}{\partial y} = 2x^2$ The only part of solving this type of equation I am having trouble seeing how to do is how to get the values of $u_x$ and $u_y$ but I can carry on from there to solve the equation.",,['ordinary-differential-equations']
94,Finding eigenvalues by inspection?,Finding eigenvalues by inspection?,,"I need to solve the following problem, In this problem, the eigenvalues of the coefficient matrix can be   found by inspection and factoring. Apply the eigenvalue method to find   a general solution of the system. $x'_1=2x_1+x_2-x_3$ $x'_2=-4x_1-3x_2-x_3$ $x'_3=4x_1+4x_2+2x_3$ Now I know how to find the eigenvalues by using the fact that $|A-\lambda I|=0$, but how would I do it by inspection? Inspection is easy for matrices that have the sum of their rows adding up to the same value, but this coefficient matrix doesn't have that property. EDIT: Originally I didn't understand what inspection meant either. After googling it this is what I found. Imagine you have the matrix, $A = \begin{pmatrix} 2 & -1 & -1 \\ -1 & 2 & -1 \\ -1 & -1 & 2 \end{pmatrix}$ By noticing (or inspecting) that each row sums up to the same value, which is 0, we can easily see that [1, 1, 1] is an eigenvector with the associated eigenvalue of 0.","I need to solve the following problem, In this problem, the eigenvalues of the coefficient matrix can be   found by inspection and factoring. Apply the eigenvalue method to find   a general solution of the system. $x'_1=2x_1+x_2-x_3$ $x'_2=-4x_1-3x_2-x_3$ $x'_3=4x_1+4x_2+2x_3$ Now I know how to find the eigenvalues by using the fact that $|A-\lambda I|=0$, but how would I do it by inspection? Inspection is easy for matrices that have the sum of their rows adding up to the same value, but this coefficient matrix doesn't have that property. EDIT: Originally I didn't understand what inspection meant either. After googling it this is what I found. Imagine you have the matrix, $A = \begin{pmatrix} 2 & -1 & -1 \\ -1 & 2 & -1 \\ -1 & -1 & 2 \end{pmatrix}$ By noticing (or inspecting) that each row sums up to the same value, which is 0, we can easily see that [1, 1, 1] is an eigenvector with the associated eigenvalue of 0.",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors']"
95,Mysterious step while solving ODE,Mysterious step while solving ODE,,I just read a paper in which an ODE was solved using a step I don't understand. $${dI \over \kappa (x) dx} = I$$  Let $$\tau = \int_0^x \kappa(x)dx$$ Rewrite equation as $${dI \over d\tau} = I$$  and solve. How does the final expression follow from the definition of $\tau$?,I just read a paper in which an ODE was solved using a step I don't understand. $${dI \over \kappa (x) dx} = I$$  Let $$\tau = \int_0^x \kappa(x)dx$$ Rewrite equation as $${dI \over d\tau} = I$$  and solve. How does the final expression follow from the definition of $\tau$?,,"['calculus', 'ordinary-differential-equations']"
96,Asymptotic behaviour for ODE,Asymptotic behaviour for ODE,,"Imagine I have an ordinary linear homogeneous second-order differential equation of the form: $$ y''=f(x)y'+g(x)y, $$ where the functions $f$ and $g$ have the regularity you wish, are bounded, and satisfy $$ \lim_{x\rightarrow \infty}f(x)=a, \qquad \lim_{x\rightarrow \infty}g(x)=b, $$ where $a$ and $b$ are real constants. Heuristically, this tells that the asymptotics of a solution of our ODE will behave as the solution of the autonomous ODE: $$ v''=av'+bv. $$ For example, if we have a solution for the first equation, $y$ , converging to zero, and $v$ is a solution for the second equation converging to zero (i.e., exponential decay), do we have $$ y(x)=O(v(x)),\qquad v(x)=O(y(x)) $$ as $x \rightarrow \infty$ ? EDIT: To clarify my question, I will point out here an example suggested in one of the answers. Let $f(x)=e^{-x}+1$ ang $g(x)=0$ . Then we have \begin{equation} y''(x)+(e^{-x}+1)y'(x)=0 \implies y(x) = c_1e^{e^{-x}}+c_2. \end{equation} And between all these solutions, take for example the ones converging to zero, so take $c_1=-c_2$ . I claim that asymptotically this solution and the solutions converging to zero of the approximation \begin{equation} y''(x)+y'(x)=0 \implies y(x) = \tilde{c}e^{-x} \end{equation} are the same. In this case, this is true, since $$ \lim_{x \rightarrow \infty} \frac{c_1e^{e^{-x}}-c_1}{\tilde{c}e^{-x}}=c_1/\tilde{c}. $$ Question: Is this true in general? If so, is there any kind of result I can use to formalize this? Note that I assumed the solutions to converge to zero for simplicity, but I think the asymptotics would be the same between any two reasonable solutions (as far as we do not compare solutions with a finite limit and solutions going to infinity, for example). I have tried to compute the error between the approximation and the first solution, and prove that this error has a faster decay than the solution itself, but it didn't work.","Imagine I have an ordinary linear homogeneous second-order differential equation of the form: where the functions and have the regularity you wish, are bounded, and satisfy where and are real constants. Heuristically, this tells that the asymptotics of a solution of our ODE will behave as the solution of the autonomous ODE: For example, if we have a solution for the first equation, , converging to zero, and is a solution for the second equation converging to zero (i.e., exponential decay), do we have as ? EDIT: To clarify my question, I will point out here an example suggested in one of the answers. Let ang . Then we have And between all these solutions, take for example the ones converging to zero, so take . I claim that asymptotically this solution and the solutions converging to zero of the approximation are the same. In this case, this is true, since Question: Is this true in general? If so, is there any kind of result I can use to formalize this? Note that I assumed the solutions to converge to zero for simplicity, but I think the asymptotics would be the same between any two reasonable solutions (as far as we do not compare solutions with a finite limit and solutions going to infinity, for example). I have tried to compute the error between the approximation and the first solution, and prove that this error has a faster decay than the solution itself, but it didn't work.","
y''=f(x)y'+g(x)y,
 f g 
\lim_{x\rightarrow \infty}f(x)=a, \qquad \lim_{x\rightarrow \infty}g(x)=b,
 a b 
v''=av'+bv.
 y v 
y(x)=O(v(x)),\qquad v(x)=O(y(x))
 x \rightarrow \infty f(x)=e^{-x}+1 g(x)=0 \begin{equation}
y''(x)+(e^{-x}+1)y'(x)=0 \implies y(x) = c_1e^{e^{-x}}+c_2.
\end{equation} c_1=-c_2 \begin{equation}
y''(x)+y'(x)=0 \implies y(x) = \tilde{c}e^{-x}
\end{equation} 
\lim_{x \rightarrow \infty} \frac{c_1e^{e^{-x}}-c_1}{\tilde{c}e^{-x}}=c_1/\tilde{c}.
","['ordinary-differential-equations', 'asymptotics', 'dynamical-systems']"
97,Counterexample: convergence of difference quotient in Backward Euler method,Counterexample: convergence of difference quotient in Backward Euler method,,"I am pretty sure that, for the iterates of the backward Euler's method for $y'=f(t,y)$ one cannot obtain $\displaystyle \frac{y_n-y_{n-1}}{\delta t}\rightarrow y'(t^*)$ , for a suitable choice of $n$ . The reason is that $y_n=y(t_n)$ , but only at first order, so that $y_n-y_{n-1} = \delta t y'(t^*) + C\delta t$ , and this $C$ need not to approach $0$ . I was only able, though, to construct examples where we get convergence also of the difference quotients (e.g. $y'=ay$ ...). Do you have a counterexample, and possibly a descriptions of the class of functions $f$ on which backward Euler is of higher order than $1$ ? It seems that $f(t,y)=y$ , for instance, is in this class. Edit Actually, the derivative approximation property might be true by a bootstrapping argument: $\displaystyle \frac{y_n-y_{n-1}}{\delta t} = f(t_n,y_n)=f(t^*,y(t^*)) + f_t(t^*,y(t^*))(t-t^*)+f_y(t^*,y(t^*))(y_n-y(t^*)) + o((t-t^*, y_n-y(t^*)))$ . So that $f\in C^1$ suffices, thanks to the convergence $t_n\rightarrow t^*, y_n \rightarrow y(t^*)$ . What do you think?","I am pretty sure that, for the iterates of the backward Euler's method for one cannot obtain , for a suitable choice of . The reason is that , but only at first order, so that , and this need not to approach . I was only able, though, to construct examples where we get convergence also of the difference quotients (e.g. ...). Do you have a counterexample, and possibly a descriptions of the class of functions on which backward Euler is of higher order than ? It seems that , for instance, is in this class. Edit Actually, the derivative approximation property might be true by a bootstrapping argument: . So that suffices, thanks to the convergence . What do you think?","y'=f(t,y) \displaystyle \frac{y_n-y_{n-1}}{\delta t}\rightarrow y'(t^*) n y_n=y(t_n) y_n-y_{n-1} = \delta t y'(t^*) + C\delta t C 0 y'=ay f 1 f(t,y)=y \displaystyle \frac{y_n-y_{n-1}}{\delta t} = f(t_n,y_n)=f(t^*,y(t^*)) + f_t(t^*,y(t^*))(t-t^*)+f_y(t^*,y(t^*))(y_n-y(t^*)) + o((t-t^*, y_n-y(t^*))) f\in C^1 t_n\rightarrow t^*, y_n \rightarrow y(t^*)","['ordinary-differential-equations', 'numerical-methods']"
98,Solving the first-order non-linear differential equation $y' = y^2 - 2 x$,Solving the first-order non-linear differential equation,y' = y^2 - 2 x,"I am trying to solve this Cauchy's problem: $$ y' = y^2 - 2x $$ with condition $y(0) = 2$ It's very similar to Bernoulli equation $$ y' + a(x)y = b(x)y^2$$ however doesn't contain $a(x)y$ . I also tried substitution like $y = tx$ . but it hasn't helped, because i've got this $$ t'x + t = (tx)^2 + 2x $$ I've tried using computer algebra resourses, and it gave me solution in closed form which looks very awfull  with Bessel function of first kind and Gamma function. So let's make it easier, is there any way to find a series $y(x) = \sum a_n x^n$ that formally solve this differential equation (i calculated some first coeffitients, but i can't solve infinite system of linear equations on coeffitients and i can't find a good formula for $a_n$ )?","I am trying to solve this Cauchy's problem: with condition It's very similar to Bernoulli equation however doesn't contain . I also tried substitution like . but it hasn't helped, because i've got this I've tried using computer algebra resourses, and it gave me solution in closed form which looks very awfull  with Bessel function of first kind and Gamma function. So let's make it easier, is there any way to find a series that formally solve this differential equation (i calculated some first coeffitients, but i can't solve infinite system of linear equations on coeffitients and i can't find a good formula for )?","
y' = y^2 - 2x
 y(0) = 2 
y' + a(x)y = b(x)y^2 a(x)y y = tx 
t'x + t = (tx)^2 + 2x
 y(x) = \sum a_n x^n a_n","['ordinary-differential-equations', 'special-functions', 'cauchy-problem', 'airy-functions']"
99,State-space with second derivative input variable,State-space with second derivative input variable,,"I have a second order ODE with derivative terms $$ \alpha_1\frac{d^2i}{dt^2}+\alpha_2\frac{di}{dt}+\alpha_3i=-\beta_1V_r-\beta_2\frac{dV_r}{dt}-\beta_3\frac{d^2V_r}{dt^2} $$ I want to transform this system into state-space, but I'm not sure how to manage the second derivative input term of $V_r$ . I currently have defined $x_1$ and $x_2$ as the following. $$ x_1=I\left(t\right) $$ $$ x_2=\dot{I}\left(t\right)+\frac{\beta_2}{\alpha_1}V_r+\frac{\beta_3}{\alpha_1}{\dot{V}}_r$$ Its derivatives are given as $$ {\dot{x}}_1=\ \dot{I}\left(t\right)=\ x_2-\frac{\beta_2}{\alpha_1}V_r-\frac{\beta_3}{\alpha_1}{\dot{V}}_r$$ $$ {\dot{x}}_2=\ \ddot{I}\left(t\right)+\frac{\beta_2}{\alpha_1}{\dot{V}}_r+\frac{\beta_3}{\alpha_1}{\ddot{V}}_r=\ -\frac{\alpha_3}{\alpha_1}I-\frac{\alpha_2}{\alpha_1}\dot{I}-\frac{\beta_1}{\alpha_1}\ V_r $$ $$ where,  \dot{I} = x_2-\frac{\beta_2}{\alpha_1}V_r-\frac{\beta_3}{\alpha_1}{\dot{V}}_r$$ $$ {\dot{x}}_2=\ -\frac{\alpha_3}{\alpha_1}x_2-\frac{\alpha_2}{\alpha_1}\left(x_2-\frac{\beta_2}{\alpha_1}V_r-\frac{\beta_3}{\alpha_1}{\dot{V}}_r\right)-\frac{\beta_1}{\alpha_1}\ V_r$$ Because I still have a $\dot{V}_r$ term in both $\dot{x}_1$ and $\dot{x}_2$ , I am unable to transform the the system into the state-space representation. Let me know if you have any suggestions.","I have a second order ODE with derivative terms I want to transform this system into state-space, but I'm not sure how to manage the second derivative input term of . I currently have defined and as the following. Its derivatives are given as Because I still have a term in both and , I am unable to transform the the system into the state-space representation. Let me know if you have any suggestions."," \alpha_1\frac{d^2i}{dt^2}+\alpha_2\frac{di}{dt}+\alpha_3i=-\beta_1V_r-\beta_2\frac{dV_r}{dt}-\beta_3\frac{d^2V_r}{dt^2}  V_r x_1 x_2  x_1=I\left(t\right)   x_2=\dot{I}\left(t\right)+\frac{\beta_2}{\alpha_1}V_r+\frac{\beta_3}{\alpha_1}{\dot{V}}_r  {\dot{x}}_1=\ \dot{I}\left(t\right)=\ x_2-\frac{\beta_2}{\alpha_1}V_r-\frac{\beta_3}{\alpha_1}{\dot{V}}_r  {\dot{x}}_2=\ \ddot{I}\left(t\right)+\frac{\beta_2}{\alpha_1}{\dot{V}}_r+\frac{\beta_3}{\alpha_1}{\ddot{V}}_r=\ -\frac{\alpha_3}{\alpha_1}I-\frac{\alpha_2}{\alpha_1}\dot{I}-\frac{\beta_1}{\alpha_1}\ V_r   where,  \dot{I} = x_2-\frac{\beta_2}{\alpha_1}V_r-\frac{\beta_3}{\alpha_1}{\dot{V}}_r  {\dot{x}}_2=\ -\frac{\alpha_3}{\alpha_1}x_2-\frac{\alpha_2}{\alpha_1}\left(x_2-\frac{\beta_2}{\alpha_1}V_r-\frac{\beta_3}{\alpha_1}{\dot{V}}_r\right)-\frac{\beta_1}{\alpha_1}\ V_r \dot{V}_r \dot{x}_1 \dot{x}_2","['ordinary-differential-equations', 'dynamical-systems', 'control-theory', 'linear-control']"
