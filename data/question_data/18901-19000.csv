,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How to form a mental image of $\mathbf {ijk}=-1$ in quaternions?,How to form a mental image of  in quaternions?,\mathbf {ijk}=-1,"$\mathbf {ijk}=-1$ is part of the famously stone-carved formula by Sir William Rowan Hamilton, allowing the multiplication of triplets. There is already an intuition question on the topic of quaternions, and a beautifully illustrated post on 3D stereographic projections of quaternion rotations in 4D in the 3Blue1Brown youtube channel . The question is very specific about how to picture in a geometric or intuitive way the equality $\mathbf {ijk}=-1$ part in the formula $$\mathbf {i}^2 = \mathbf {j}^2 = \mathbf {k}^2 =\mathbf {ijk}=-1 $$ Attempt: $\mathbf i$ , $\mathbf j$ and $\mathbf k$ act on the left of a quaternion as a pure rotation; hence, $$\mathbf {ijk}\overset{?}=\mathbf {(ij)k}$$ and $\mathbf {i}$ acts on $\mathbf {j},$ rotating $\mathbf {j}$ into $\mathbf {k}$ in keeping with 3Brown1Blue's Grant Sanderson's ""right-hand rule"" , illustrated with the 3D diagram below, corresponding to the stereographic projection of a 4D hypersphere of quaternions with norm $1$ and $0$ real component, where the $-\infty,$ $-\mathbf i,$ $1,$ $\mathbf i,$ $+\infty$ yellow line is the projection of a circle in 4D running through $-1,$ and centered at $0,$ passing through $-\mathbf i$ and $+\mathbf i;$ while the red-blue $\mathbf j,$ $\mathbf k,$ $-\mathbf j,$ $-\mathbf k$ circle corresponds to the 3D projection of a sphere in the 4D hypersphere passing through $-1,$ and centered at $0,$ and reaching both $\pm \mathbf k$ and $\pm \mathbf j.$ Rotating in the direction of $\mathbf i$ (i.e. multiplying on the left by $\mathbf i$ ) would amount to sliding the yellow line in the direction of the thumb, while rotating the red-blue circle following the clenching of the rest of the fingers: and resulting in $$\begin{align}\mathbf {ijk}&\overset{?}=\mathbf {(ij)k}\\ &=\mathbf {kk}\\ &=\mathbf k^2 =-1 \end{align}$$","is part of the famously stone-carved formula by Sir William Rowan Hamilton, allowing the multiplication of triplets. There is already an intuition question on the topic of quaternions, and a beautifully illustrated post on 3D stereographic projections of quaternion rotations in 4D in the 3Blue1Brown youtube channel . The question is very specific about how to picture in a geometric or intuitive way the equality part in the formula Attempt: , and act on the left of a quaternion as a pure rotation; hence, and acts on rotating into in keeping with 3Brown1Blue's Grant Sanderson's ""right-hand rule"" , illustrated with the 3D diagram below, corresponding to the stereographic projection of a 4D hypersphere of quaternions with norm and real component, where the yellow line is the projection of a circle in 4D running through and centered at passing through and while the red-blue circle corresponds to the 3D projection of a sphere in the 4D hypersphere passing through and centered at and reaching both and Rotating in the direction of (i.e. multiplying on the left by ) would amount to sliding the yellow line in the direction of the thumb, while rotating the red-blue circle following the clenching of the rest of the fingers: and resulting in","\mathbf {ijk}=-1 \mathbf {ijk}=-1 \mathbf {i}^2 = \mathbf {j}^2 = \mathbf {k}^2 =\mathbf {ijk}=-1  \mathbf i \mathbf j \mathbf k \mathbf {ijk}\overset{?}=\mathbf {(ij)k} \mathbf {i} \mathbf {j}, \mathbf {j} \mathbf {k} 1 0 -\infty, -\mathbf i, 1, \mathbf i, +\infty -1, 0, -\mathbf i +\mathbf i; \mathbf j, \mathbf k, -\mathbf j, -\mathbf k -1, 0, \pm \mathbf k \pm \mathbf j. \mathbf i \mathbf i \begin{align}\mathbf {ijk}&\overset{?}=\mathbf {(ij)k}\\
&=\mathbf {kk}\\
&=\mathbf k^2 =-1
\end{align}","['linear-algebra', 'complex-numbers', 'quaternions']"
1,Is the product of any two invertible diagonalizable matrices diagonalizable?,Is the product of any two invertible diagonalizable matrices diagonalizable?,,"While studying linear algebra, I saw an example of a pair of $2 \times 2$ or $n \times n$ diagonalizable matrices, the product of which is not diagonalizable. Is there a similar example when I replace the condition ""diagonalizable"" by ""invertible and diagonalizable""?","While studying linear algebra, I saw an example of a pair of or diagonalizable matrices, the product of which is not diagonalizable. Is there a similar example when I replace the condition ""diagonalizable"" by ""invertible and diagonalizable""?",2 \times 2 n \times n,"['linear-algebra', 'matrices', 'diagonalization']"
2,Dual and adjoint operator,Dual and adjoint operator,,"Let $X$ be a Hilbert space with associated canonical isomorphism $I:X\rightarrow X^\ast$ (by the Riesz representation theorem). If $A:X\rightarrow X$ is a linear operator on $X$, then its dual $A^\ast:X^\ast\rightarrow X^\ast$ and its adjoint (or ""transpose"") $A^\dagger:X\rightarrow X$ are related to one another by $I\circ A^\dagger=A^\ast\circ I$. Is there an enlightening categorical viewpoint on the connection between $A^\ast$ and $A^\dagger$?","Let $X$ be a Hilbert space with associated canonical isomorphism $I:X\rightarrow X^\ast$ (by the Riesz representation theorem). If $A:X\rightarrow X$ is a linear operator on $X$, then its dual $A^\ast:X^\ast\rightarrow X^\ast$ and its adjoint (or ""transpose"") $A^\dagger:X\rightarrow X$ are related to one another by $I\circ A^\dagger=A^\ast\circ I$. Is there an enlightening categorical viewpoint on the connection between $A^\ast$ and $A^\dagger$?",,"['linear-algebra', 'reference-request', 'category-theory', 'hilbert-spaces', 'adjoint-functors']"
3,Frobenius Inequality Rank,Frobenius Inequality Rank,,"I was looking for an answer for this problem in terms of matrices, but I really don't know how to prove this result. The proposition says that: Let $A\in M_{m\times k}(\mathbb{C})$, $B\in M_{k\times p}(\mathbb{C})$ and $C\in M_{p\times n}(\mathbb{C})$, then $\textrm{rank}(AB)+\textrm{rank}(BC)\leq \textrm{rank}(B)+\textrm{rank}(ABC)$","I was looking for an answer for this problem in terms of matrices, but I really don't know how to prove this result. The proposition says that: Let $A\in M_{m\times k}(\mathbb{C})$, $B\in M_{k\times p}(\mathbb{C})$ and $C\in M_{p\times n}(\mathbb{C})$, then $\textrm{rank}(AB)+\textrm{rank}(BC)\leq \textrm{rank}(B)+\textrm{rank}(ABC)$",,"['linear-algebra', 'matrices']"
4,What is the correct order when multiplying both sides of an equation by matrix inverses?,What is the correct order when multiplying both sides of an equation by matrix inverses?,,"So my questions is let's say you were asked to solve for $A$, and you have something like this: $$BAC=D$$ where B, C , and D are matrices. So the way I would solve this would be to multiply both sides by $B^{-1}$ and $C^{-1}$ (inverse of B and C), but since the order in the multiplication matters $A = DB^{-1}C^{-1}$ would be different than say $A = B^{-1}DC^{-1}$. My question (maybe stupid or I am just missing something) is how do you know which order is the correct one?","So my questions is let's say you were asked to solve for $A$, and you have something like this: $$BAC=D$$ where B, C , and D are matrices. So the way I would solve this would be to multiply both sides by $B^{-1}$ and $C^{-1}$ (inverse of B and C), but since the order in the multiplication matters $A = DB^{-1}C^{-1}$ would be different than say $A = B^{-1}DC^{-1}$. My question (maybe stupid or I am just missing something) is how do you know which order is the correct one?",,"['linear-algebra', 'matrices']"
5,Eigenvalues in terms of trace and determinant for matrices larger than 2 X 2,Eigenvalues in terms of trace and determinant for matrices larger than 2 X 2,,"The eigenvalues of a $2\times2$ matrix can be expressed in terms of the trace and determinant. $\lambda_\pm = \frac{1}{2}\left(\textrm{tr} \pm \sqrt{\textrm{tr}^2-4\det}\right)$ Is there a similar formula for higher dimensional matrices? Approach The trace and determinant of a matrix are equal to the trace and determinant of the matrix in Jordan normal form. For a matrix in Jordan canonical form, $\textrm{tr } =\sum \lambda$ and $\det =\prod \lambda $. Substituting these latter two identities into the first results in an identity, which is encouraging. I'm not sure how to check this assumption for larger matrices. I'm not sure how generate more than two eigenvalues from the first formula. For the $3\times3$ case, the first formula seems to break down.","The eigenvalues of a $2\times2$ matrix can be expressed in terms of the trace and determinant. $\lambda_\pm = \frac{1}{2}\left(\textrm{tr} \pm \sqrt{\textrm{tr}^2-4\det}\right)$ Is there a similar formula for higher dimensional matrices? Approach The trace and determinant of a matrix are equal to the trace and determinant of the matrix in Jordan normal form. For a matrix in Jordan canonical form, $\textrm{tr } =\sum \lambda$ and $\det =\prod \lambda $. Substituting these latter two identities into the first results in an identity, which is encouraging. I'm not sure how to check this assumption for larger matrices. I'm not sure how generate more than two eigenvalues from the first formula. For the $3\times3$ case, the first formula seems to break down.",,['linear-algebra']
6,What is range of a matrix?,What is range of a matrix?,,"I am having some tough time understanding the basic concepts, like range of a matrix A. From what I basically understand, if a set columns in a matrix are linearly independent, i.e. one column in that set can not be derived from linear combination of others, than we can get a bunch of set of vectors by linear combination of the columns of matrix A. That set is called column space of the matrix A or its range. And those linear independent columns of matrix form basis for this range, or are called to ""span the column space"" of matrix A. Did I understand it correctly? In simplest terms can anyone explain it? Also what is Null space, rank and how they are related to a matrix?","I am having some tough time understanding the basic concepts, like range of a matrix A. From what I basically understand, if a set columns in a matrix are linearly independent, i.e. one column in that set can not be derived from linear combination of others, than we can get a bunch of set of vectors by linear combination of the columns of matrix A. That set is called column space of the matrix A or its range. And those linear independent columns of matrix form basis for this range, or are called to ""span the column space"" of matrix A. Did I understand it correctly? In simplest terms can anyone explain it? Also what is Null space, rank and how they are related to a matrix?",,"['linear-algebra', 'matrices', 'change-of-basis']"
7,square root of symmetric matrix and transposition,square root of symmetric matrix and transposition,,I have a symmetric matrix A. How do I compute a matrix B such that $B^tB=A$ where $B^t$ is the transpose of $B$. I cannot figure out if this is at all related to the square root of $A$. I've gone through wikimedia links of square root of a matrix.,I have a symmetric matrix A. How do I compute a matrix B such that $B^tB=A$ where $B^t$ is the transpose of $B$. I cannot figure out if this is at all related to the square root of $A$. I've gone through wikimedia links of square root of a matrix.,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
8,Show that the dual norm of the spectral norm is the nuclear norm,Show that the dual norm of the spectral norm is the nuclear norm,,"Could someone help me understand why the dual norm of the spectral norm is the nuclear norm? We can focus on the real field. Given a matrix $X \in \mathbb{R}^{m \times n},$  then the spectral norm is defined by $$\left \| X\right\| = \max\limits_{i \in \{1, \dots, \min\{m,n\}\} }\sigma_i (X)$$ whereas the nuclear norm is defined by $$\left \| X \right \|_* = \sum\limits_{i=1}^ {\min\{m,n\}} \sigma_i (X)$$ Can someone show me the reasoning process? Thank you in advance.","Could someone help me understand why the dual norm of the spectral norm is the nuclear norm? We can focus on the real field. Given a matrix $X \in \mathbb{R}^{m \times n},$  then the spectral norm is defined by $$\left \| X\right\| = \max\limits_{i \in \{1, \dots, \min\{m,n\}\} }\sigma_i (X)$$ whereas the nuclear norm is defined by $$\left \| X \right \|_* = \sum\limits_{i=1}^ {\min\{m,n\}} \sigma_i (X)$$ Can someone show me the reasoning process? Thank you in advance.",,"['linear-algebra', 'matrices', 'matrix-norms', 'nuclear-norm', 'spectral-norm']"
9,When does $V = \ker(f) \oplus \operatorname{im}(f)$?,When does ?,V = \ker(f) \oplus \operatorname{im}(f),"For a vector space $V$ and a linear operator $f : V \to V$, under what conditions does $V = \ker(f) \oplus \operatorname{im}(f)$? Is it always true, or only in special cases? Edit : $V$ is finite dimensional.","For a vector space $V$ and a linear operator $f : V \to V$, under what conditions does $V = \ker(f) \oplus \operatorname{im}(f)$? Is it always true, or only in special cases? Edit : $V$ is finite dimensional.",,['linear-algebra']
10,What's so special about the 4 fundamental subspaces?,What's so special about the 4 fundamental subspaces?,,"I was reading Gilbert Strang's book for Linear Algebra (along with his lectures) and I feel that he is emphasizing that the 4 fundamental subspaces (Column Space, Row Space, Null Space and Null Space of $A^T$) form the ""crux"" or ""heart"" of Linear Algebra and that the understanding of it is crucial for further study. But what I don't understand is WHY? Why are they so important? I understand that : Row Space and Null Space are orthogonal. (Same for other two) Their dimensions are governed by rank of the matrix. but beyond that I didn't find anything that I found profound or interesting. Am I missing something or is Prof. Strang over-rating the 4 fundamental subspaces?","I was reading Gilbert Strang's book for Linear Algebra (along with his lectures) and I feel that he is emphasizing that the 4 fundamental subspaces (Column Space, Row Space, Null Space and Null Space of $A^T$) form the ""crux"" or ""heart"" of Linear Algebra and that the understanding of it is crucial for further study. But what I don't understand is WHY? Why are they so important? I understand that : Row Space and Null Space are orthogonal. (Same for other two) Their dimensions are governed by rank of the matrix. but beyond that I didn't find anything that I found profound or interesting. Am I missing something or is Prof. Strang over-rating the 4 fundamental subspaces?",,"['linear-algebra', 'matrices']"
11,Cat quiz (to solve with the determinant),Cat quiz (to solve with the determinant),,"Consider $100$ cats and $100$ food bowls containing   cat food of $100$ different   brands. Every cat   likes an odd amount of brands. For each two   cats, there is   an even amount   of brands both   cats like. Show that one can   distribute the $100$ food   bowls   to the $100$ cats such that every cat is happy. Hint :   It's a   determinant   exercise   over   the field $\mathbb{F}_2 =  \mathbb{Z}/2\mathbb{Z}= \{ [0],[1] \}$ . How can this be done using the determinant? Thanks in advance!","Consider cats and food bowls containing   cat food of different   brands. Every cat   likes an odd amount of brands. For each two   cats, there is   an even amount   of brands both   cats like. Show that one can   distribute the food   bowls   to the cats such that every cat is happy. Hint :   It's a   determinant   exercise   over   the field . How can this be done using the determinant? Thanks in advance!","100 100 100 100 100 \mathbb{F}_2 = 
\mathbb{Z}/2\mathbb{Z}=
\{
[0],[1]
\}","['linear-algebra', 'matrices', 'determinant', 'finite-fields']"
12,Proof that the dimension of a matrix row space is equal to the dimension of its column space,Proof that the dimension of a matrix row space is equal to the dimension of its column space,,"I have the following theorem: Theorem 3.12. Let A be an m  n matrix. Then the dimension of its row space is equal to the dimension of its column space. And the following proof is given: Proof. Suppose that $\lbrace v_1,v_2,\dots,v_k\rbrace$ is a basis for the column space of $A$.  Then each column of $A$ can be expressed as a linear combination of these vectors; suppose that the $i$-th column $c_i$ is given by $$c_i = \gamma_{1i}v_1+\gamma_{2i}v_2+\dots+\gamma_{ki}v_k$$ Now form two matrices as follows: $B$ is an $m\times k$ matrix whose columns are the basis vectors $v_i$, while $C=(\gamma_{ij})$ is a $k\times n$ matrix whose $i$-th column contains the coefficients $\gamma_{1i},\gamma_{2i,}\dots,\gamma_{ki}$.  It then follows$^7$ that $A=BC$. However, we can also view the product $A= BC$ as expressing the rows of $A$ as a linear combination of the rows of $C$ with the $i$-th row of $B$ giving the coefficients for the linear combination that determines the $i$-th row of $A$.  Therefore, the rows of $C$ are a spanning set for the row space of $A$, and so the dimension of the row space of $A$ is at most $k$.  We conclude that: $$\dim(\operatorname{rowsp}(A))\leq\dim(\operatorname{colsp}(A))$$   Applying the same argument to $A^t$, we conclude that:$$\dim(\operatorname{colsp}(A))\leq\dim(\operatorname{rowsp}(A))$$and hence these values are equal However, I am finding this proof impossible to follow and understand. Can someone please offer an alternative proof or explain what this proof is saying? Thank you.","I have the following theorem: Theorem 3.12. Let A be an m  n matrix. Then the dimension of its row space is equal to the dimension of its column space. And the following proof is given: Proof. Suppose that $\lbrace v_1,v_2,\dots,v_k\rbrace$ is a basis for the column space of $A$.  Then each column of $A$ can be expressed as a linear combination of these vectors; suppose that the $i$-th column $c_i$ is given by $$c_i = \gamma_{1i}v_1+\gamma_{2i}v_2+\dots+\gamma_{ki}v_k$$ Now form two matrices as follows: $B$ is an $m\times k$ matrix whose columns are the basis vectors $v_i$, while $C=(\gamma_{ij})$ is a $k\times n$ matrix whose $i$-th column contains the coefficients $\gamma_{1i},\gamma_{2i,}\dots,\gamma_{ki}$.  It then follows$^7$ that $A=BC$. However, we can also view the product $A= BC$ as expressing the rows of $A$ as a linear combination of the rows of $C$ with the $i$-th row of $B$ giving the coefficients for the linear combination that determines the $i$-th row of $A$.  Therefore, the rows of $C$ are a spanning set for the row space of $A$, and so the dimension of the row space of $A$ is at most $k$.  We conclude that: $$\dim(\operatorname{rowsp}(A))\leq\dim(\operatorname{colsp}(A))$$   Applying the same argument to $A^t$, we conclude that:$$\dim(\operatorname{colsp}(A))\leq\dim(\operatorname{rowsp}(A))$$and hence these values are equal However, I am finding this proof impossible to follow and understand. Can someone please offer an alternative proof or explain what this proof is saying? Thank you.",,['linear-algebra']
13,Inverse of product of matrices,Inverse of product of matrices,,"Let $n>m$ and let $A$ and $B$ be $m\times n$ and $n\times n$ matrices. $B$ is invertible. If $A$ is square ( i.e. $n=m$ ) and invertible, then obviously $$ \left(ABA^T\right)^{-1} = A^{-T}B^{-1}A^{-1} $$ But, if $A$ is not square, can we say something (assuming that $ABA^T$ is invertible)?","Let and let and be and matrices. is invertible. If is square ( i.e. ) and invertible, then obviously But, if is not square, can we say something (assuming that is invertible)?","n>m A B m\times n n\times n B A n=m 
\left(ABA^T\right)^{-1} = A^{-T}B^{-1}A^{-1}
 A ABA^T","['linear-algebra', 'matrices']"
14,Proof: $\mathrm{adj}(\mathrm{adj}(A)) = (\mathrm{det}(A))^{n-2} \cdot A$ for $A \in \mathbb{R}^{n\times n}$,Proof:  for,\mathrm{adj}(\mathrm{adj}(A)) = (\mathrm{det}(A))^{n-2} \cdot A A \in \mathbb{R}^{n\times n},"I had my exam of linear algebra today and one of the questions was this one. Given $n\ge2$ and $ A \in \mathbb{R}^{n \times n}$ , prove that: $$\mathrm{adj}(\mathrm{adj}(A)) = (\mathrm{det}(A))^{n-2} \cdot A.$$ Of course I was not able to prove this identity, otherwise I wouldn't post it here. But I'm still curious how one can prove this identity. Could someone point me in the right direction?","I had my exam of linear algebra today and one of the questions was this one. Given and , prove that: Of course I was not able to prove this identity, otherwise I wouldn't post it here. But I'm still curious how one can prove this identity. Could someone point me in the right direction?",n\ge2  A \in \mathbb{R}^{n \times n} \mathrm{adj}(\mathrm{adj}(A)) = (\mathrm{det}(A))^{n-2} \cdot A.,['linear-algebra']
15,Generating multivariate normal samples - why Cholesky?,Generating multivariate normal samples - why Cholesky?,,"Hello everyone and happy new year! May all your hopes and aspirations come true and the forces of evil be confused and disoriented on the way to your house. With that out of the way... I am trying to write a computer code that gets a vector $\mu \in R^n $ and matrix $\Sigma \in \mathbb R^{n \times n}$ and generates random samples from the multivariate normal distribution with mean $\mu$ and covariance $\Sigma$. The problem : I am only allowed to use the program to sample from the single variable normal distribution with mean $0$ and variance $1$: $N(0, 1)$. The proposed solution : Define a vector of zeros (initially) $v \in \mathbb R^n$, now for all $i$ from $1$ to $n$, draw from a single variable normal dist: $v_i \overset{}{\sim} N(0, 1)$. Now do a Cholesky decomposition on $\Sigma$: $\Sigma = LL^T$. Now finally the random vector we want that is distributed from the multivariate gaussian is $Lv + \mu$. My question is why? I don't understand the intuition, if it was a single dimensional distribution $N(\mu, \sigma^2)$ then I understand why $\sigma ^2 v + \mu$ is a good idea, so why cholesky? Wouldn't we want $\Sigma v + \mu$?","Hello everyone and happy new year! May all your hopes and aspirations come true and the forces of evil be confused and disoriented on the way to your house. With that out of the way... I am trying to write a computer code that gets a vector $\mu \in R^n $ and matrix $\Sigma \in \mathbb R^{n \times n}$ and generates random samples from the multivariate normal distribution with mean $\mu$ and covariance $\Sigma$. The problem : I am only allowed to use the program to sample from the single variable normal distribution with mean $0$ and variance $1$: $N(0, 1)$. The proposed solution : Define a vector of zeros (initially) $v \in \mathbb R^n$, now for all $i$ from $1$ to $n$, draw from a single variable normal dist: $v_i \overset{}{\sim} N(0, 1)$. Now do a Cholesky decomposition on $\Sigma$: $\Sigma = LL^T$. Now finally the random vector we want that is distributed from the multivariate gaussian is $Lv + \mu$. My question is why? I don't understand the intuition, if it was a single dimensional distribution $N(\mu, \sigma^2)$ then I understand why $\sigma ^2 v + \mu$ is a good idea, so why cholesky? Wouldn't we want $\Sigma v + \mu$?",,"['linear-algebra', 'probability', 'numerical-methods', 'cholesky-decomposition']"
16,Proofs in Linear Algebra via Topology,Proofs in Linear Algebra via Topology,,"I'm watching the lecture series by Tadashi Tokieda on Topology and Geometry on YouTube.  In the second lecture he shows how one can prove, using a topological argument, that given two $n\times n$ matrices $P$ and $Q$, the matrix products $PQ$ and $QP$ share the same eigenvalues.  Then he claims that one can also prove that $\det(e^L)=e^{\operatorname{tr} L}$ and the Cayley-Hamilton theorem topologically as well. This has me interested.  Is there some book/ reference work that discusses linear algebra in the context of topology?  Ideally I'd like a book (at maybe the 1st year grad level) on linear algebra that uses topological arguments to prove linear algebraic statements.  Does anyone know of such of work?","I'm watching the lecture series by Tadashi Tokieda on Topology and Geometry on YouTube.  In the second lecture he shows how one can prove, using a topological argument, that given two $n\times n$ matrices $P$ and $Q$, the matrix products $PQ$ and $QP$ share the same eigenvalues.  Then he claims that one can also prove that $\det(e^L)=e^{\operatorname{tr} L}$ and the Cayley-Hamilton theorem topologically as well. This has me interested.  Is there some book/ reference work that discusses linear algebra in the context of topology?  Ideally I'd like a book (at maybe the 1st year grad level) on linear algebra that uses topological arguments to prove linear algebraic statements.  Does anyone know of such of work?",,"['linear-algebra', 'general-topology', 'reference-request', 'book-recommendation']"
17,What background is required to understand Random Matrix Theory,What background is required to understand Random Matrix Theory,,"I would like to grasp RMT. I have knowledge on linear algebra, classical probability theory, Despite the above ones what should I study to understand RMT? Where to start? Is there any beginner's guide?","I would like to grasp RMT. I have knowledge on linear algebra, classical probability theory, Despite the above ones what should I study to understand RMT? Where to start? Is there any beginner's guide?",,"['linear-algebra', 'probability-theory', 'soft-question', 'random-matrices']"
18,"For a symmetric matrix, the geometric and algebraic multiplicities are equal","For a symmetric matrix, the geometric and algebraic multiplicities are equal",,"Could anyone tell me how to prove the following proposition? Let $\lambda$ be an eigenvalue of a symmetric matrix. Then, its geometric multiplicity equals its algebraic multiplicity.","Could anyone tell me how to prove the following proposition? Let be an eigenvalue of a symmetric matrix. Then, its geometric multiplicity equals its algebraic multiplicity.",\lambda,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
19,What is the difference between the rowspace and the columnspace in linear algebra?,What is the difference between the rowspace and the columnspace in linear algebra?,,"The question is essentially all in the title, What is the difference between the rowspace and the columnspace? Additionally, do they have the same solution space?","The question is essentially all in the title, What is the difference between the rowspace and the columnspace? Additionally, do they have the same solution space?",,"['linear-algebra', 'matrices', 'vector-spaces']"
20,Prove the determinant is $0$,Prove the determinant is,0,"This is Problem 16.17 from the book Exercises in Algebra by A. I. Kostrikin. Prove that $$ \left|\begin{array}{ccccc} \dfrac{1}{2 !} & \dfrac{1}{3 !} & \dfrac{1}{4 !} & \cdots & \dfrac{1}{(2 k+2) !} \\ 1 & \dfrac{1}{2 !} & \dfrac{1}{3 !} & \cdots & \dfrac{1}{(2 k+1) !} \\ 0 & 1 & \dfrac{1}{2 !} & \cdots & \dfrac{1}{(2 k) !} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \cdots & \dfrac{1}{2 !} \end{array}\right|=0, \quad k \in \mathbb{N} $$ My Attempt: I tried to expand it by the first column, but it seemed to be more complicated when I did that. I also tried to add edges to the determinant(in the hope that it will be easier to calculate), but I still failed to work it out. So, My Question is , how to calculate this determinant?","This is Problem 16.17 from the book Exercises in Algebra by A. I. Kostrikin. Prove that My Attempt: I tried to expand it by the first column, but it seemed to be more complicated when I did that. I also tried to add edges to the determinant(in the hope that it will be easier to calculate), but I still failed to work it out. So, My Question is , how to calculate this determinant?","
\left|\begin{array}{ccccc}
\dfrac{1}{2 !} & \dfrac{1}{3 !} & \dfrac{1}{4 !} & \cdots & \dfrac{1}{(2 k+2) !} \\
1 & \dfrac{1}{2 !} & \dfrac{1}{3 !} & \cdots & \dfrac{1}{(2 k+1) !} \\
0 & 1 & \dfrac{1}{2 !} & \cdots & \dfrac{1}{(2 k) !} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & \dfrac{1}{2 !}
\end{array}\right|=0, \quad k \in \mathbb{N}
","['linear-algebra', 'combinatorics', 'determinant', 'factorial']"
21,Relationship between tensor product and wedge product,Relationship between tensor product and wedge product,,"In Gravitation (Misner, Thorne & Wheeler), the wedge product over a vector space $V$ is introduced as just the antisymmetrised tensor product: for $u, v \in V$, they define $$u \wedge v = u \otimes v - v \otimes u$$ I'm trying to work out how to fit this together with the characterisation of the exterior algebra $\Lambda V$ as the quotient of the tensor algebra $T(V)$ by the two-sided ideal $I$ generated by tensor products of the form $v \otimes v$. In that characterisation (at least in Wikipedia), the wedge product is defined by $$[u] \wedge [v] = [u \otimes v]$$ where square brackets indicate equivalence classes, for the equivalence relation $\sim$ induced by the ideal (i.e., $u \sim v$ iff $v = u + i$, for some $i \in I$). At first, I took it that the way they fit together was that we would be able to show that $$u \otimes v \sim u \otimes v - v \otimes u$$ But so far as I'm able to tell, this isn't the case: what we can show instead is that $u \otimes v \sim -v \otimes u$, and hence that $$u \otimes v \sim \frac{1}{2} (u \otimes v - v \otimes u)$$ Having browsed a bit, this answer suggests to me that what's going on is just that there are two ways of relating the exterior algebra to the tensor algebra: the MTW characterisation amounts to treating $\Lambda V$ as a subalgebra of $T(V)$ (specifically, the subalgebra generated by antisymmetric tensor products), rather than as a quotient algebra of $T(V)$. Is that correct? If so, what are the advantages of a subalgebra vs a quotient algebra construction, and vice versa? (Apologies if this is overly duplicative of the linked discussion - it was a bit too abstract for me to be sure I'd followed it correctly.)","In Gravitation (Misner, Thorne & Wheeler), the wedge product over a vector space $V$ is introduced as just the antisymmetrised tensor product: for $u, v \in V$, they define $$u \wedge v = u \otimes v - v \otimes u$$ I'm trying to work out how to fit this together with the characterisation of the exterior algebra $\Lambda V$ as the quotient of the tensor algebra $T(V)$ by the two-sided ideal $I$ generated by tensor products of the form $v \otimes v$. In that characterisation (at least in Wikipedia), the wedge product is defined by $$[u] \wedge [v] = [u \otimes v]$$ where square brackets indicate equivalence classes, for the equivalence relation $\sim$ induced by the ideal (i.e., $u \sim v$ iff $v = u + i$, for some $i \in I$). At first, I took it that the way they fit together was that we would be able to show that $$u \otimes v \sim u \otimes v - v \otimes u$$ But so far as I'm able to tell, this isn't the case: what we can show instead is that $u \otimes v \sim -v \otimes u$, and hence that $$u \otimes v \sim \frac{1}{2} (u \otimes v - v \otimes u)$$ Having browsed a bit, this answer suggests to me that what's going on is just that there are two ways of relating the exterior algebra to the tensor algebra: the MTW characterisation amounts to treating $\Lambda V$ as a subalgebra of $T(V)$ (specifically, the subalgebra generated by antisymmetric tensor products), rather than as a quotient algebra of $T(V)$. Is that correct? If so, what are the advantages of a subalgebra vs a quotient algebra construction, and vice versa? (Apologies if this is overly duplicative of the linked discussion - it was a bit too abstract for me to be sure I'd followed it correctly.)",,"['linear-algebra', 'tensor-products', 'exterior-algebra']"
22,Inverse of Perspective Matrix,Inverse of Perspective Matrix,,"I am trying to calculate Image to World model for my thesis dealing with road lanes. As a disclaimer I have to say that linear algebra is not my strong suite. The idea is - given that I know yield, pitch, and position of the camera - I can translate image pixels to real world coordinates which will be useful in road recognition algorithm. I managed to get working Camera Pinhole Perspective projection. Here are the matrices used Extrinsic Matrix Translates to camera position and rotates accordingly $$\begin{pmatrix} &1  &0  &0 &-cx \\  &0  &1  &0 &-cy \\  &0  &0  &1 &-cz \\  &0  &0  &0 &1  \end{pmatrix} \begin{pmatrix} &1  &0  &0 &0 \\  &0  &\cos(\text{yaw})  &-\sin(\text{yaw}) &0 \\  &0  &\sin(\text{yaw})  &\cos(\text{yaw}) &0 \\  &0  &0  &0 &1  \end{pmatrix} \begin{pmatrix} &\cos(\text{pitch}))  &0  &\sin(\text{pitch})) &0 \\  &0  &1  &0 &0 \\  &-\sin(\text{pitch})  &0  &\cos(\text{pitch}) &0 \\  &0  &0  &0 &1  \end{pmatrix} $$ Projection $f$ is the focal length of the camera. Based on https://en.wikipedia.org/wiki/3D_projection $$\begin{pmatrix} Fx\\  Fy\\  Fz\\  Fw \end{pmatrix} = \begin{pmatrix} &1  &0  &1/f &0 \\  &0  &1  &1/f &0 \\  &0  &0  &1 &0 \\  &0  &0  &1/f &0  \end{pmatrix} \begin{pmatrix} dx\\  dy\\  dz\\  1 \end{pmatrix}$$ $$p = \begin{pmatrix} Fx/Fw\\  Fy/Fw\\  1 \end{pmatrix}$$ Intrinsic Matrix Scales to pixel units and moves origin to center. $w$ is the width of screen and $W$ is the width of the sensor. Similarily with height $Fx = w/W$ $Fy = h/H$ $$\begin{pmatrix} &Fx  &0  &w/2  \\  &0  &Fy  &h/2  \\  &0  &0  &1 \\  \end{pmatrix} $$ In  typical projection I first multiply 3d point with extrinsic matrix, then project it using Projection matrix and then apply Intrinsic matrix. But how can I reverse the process? I can use assumption that all points lie on the road plane (Y == 0). Yet I am not sure how to fit it with all these matrixes. I know I can invert Intrinsic and Extrinsic Matrix, but I can't do it with the projection matrix, because is singular. Any lead would be useful. Thanks","I am trying to calculate Image to World model for my thesis dealing with road lanes. As a disclaimer I have to say that linear algebra is not my strong suite. The idea is - given that I know yield, pitch, and position of the camera - I can translate image pixels to real world coordinates which will be useful in road recognition algorithm. I managed to get working Camera Pinhole Perspective projection. Here are the matrices used Extrinsic Matrix Translates to camera position and rotates accordingly $$\begin{pmatrix} &1  &0  &0 &-cx \\  &0  &1  &0 &-cy \\  &0  &0  &1 &-cz \\  &0  &0  &0 &1  \end{pmatrix} \begin{pmatrix} &1  &0  &0 &0 \\  &0  &\cos(\text{yaw})  &-\sin(\text{yaw}) &0 \\  &0  &\sin(\text{yaw})  &\cos(\text{yaw}) &0 \\  &0  &0  &0 &1  \end{pmatrix} \begin{pmatrix} &\cos(\text{pitch}))  &0  &\sin(\text{pitch})) &0 \\  &0  &1  &0 &0 \\  &-\sin(\text{pitch})  &0  &\cos(\text{pitch}) &0 \\  &0  &0  &0 &1  \end{pmatrix} $$ Projection $f$ is the focal length of the camera. Based on https://en.wikipedia.org/wiki/3D_projection $$\begin{pmatrix} Fx\\  Fy\\  Fz\\  Fw \end{pmatrix} = \begin{pmatrix} &1  &0  &1/f &0 \\  &0  &1  &1/f &0 \\  &0  &0  &1 &0 \\  &0  &0  &1/f &0  \end{pmatrix} \begin{pmatrix} dx\\  dy\\  dz\\  1 \end{pmatrix}$$ $$p = \begin{pmatrix} Fx/Fw\\  Fy/Fw\\  1 \end{pmatrix}$$ Intrinsic Matrix Scales to pixel units and moves origin to center. $w$ is the width of screen and $W$ is the width of the sensor. Similarily with height $Fx = w/W$ $Fy = h/H$ $$\begin{pmatrix} &Fx  &0  &w/2  \\  &0  &Fy  &h/2  \\  &0  &0  &1 \\  \end{pmatrix} $$ In  typical projection I first multiply 3d point with extrinsic matrix, then project it using Projection matrix and then apply Intrinsic matrix. But how can I reverse the process? I can use assumption that all points lie on the road plane (Y == 0). Yet I am not sure how to fit it with all these matrixes. I know I can invert Intrinsic and Extrinsic Matrix, but I can't do it with the projection matrix, because is singular. Any lead would be useful. Thanks",,"['linear-algebra', 'matrices', '3d', 'computer-vision']"
23,Avoiding the Cayley–Hamilton theorem [duplicate],Avoiding the Cayley–Hamilton theorem [duplicate],,"This question already has answers here : Degree of minimum polynomial at most $n$ without Cayley-Hamilton? (4 answers) Closed 8 years ago . Every $n\times n$ matrix satisfies a polynomial equation of degree at most $n^2$, simply because the space of $n\times n$ matrices has dimension $n^2$. By the Cayley–Hamilton theorem, every matrix satisfies a polynomial equation of degree $n$. Is there a simple proof that every matrix satisfies a polynomial equation of degree at most $n$ without using the Cayley–Hamilton theorem?","This question already has answers here : Degree of minimum polynomial at most $n$ without Cayley-Hamilton? (4 answers) Closed 8 years ago . Every $n\times n$ matrix satisfies a polynomial equation of degree at most $n^2$, simply because the space of $n\times n$ matrices has dimension $n^2$. By the Cayley–Hamilton theorem, every matrix satisfies a polynomial equation of degree $n$. Is there a simple proof that every matrix satisfies a polynomial equation of degree at most $n$ without using the Cayley–Hamilton theorem?",,"['linear-algebra', 'matrices']"
24,What's so useful about diagonalizing a matrix?,What's so useful about diagonalizing a matrix?,,"I'm told the the purpose of diagonalisation is to bring the matrix in a 'nice' form that allows one to quickly compute with it. However in writing the matrix in this nice diagonal form you have to express it w.r.t. a new eigenvector basis. But you'll probably want the answer of your matrix multiplication written w.r.t. to the original basis, so you'll have to do a not-nice matrix multiplication regardless. Example of what I  mean: I want to compute $Ax$, where $A$ and $x$ are given w.r.t. the standard basis ($\epsilon$). However $A$ is quite large and annoying to compute, so I calculate $A_E$ which is a diagonal matrix written w.r.t. the eigenvector basis. But to compute $Ax$ using this matrix, I still have to compute the following: $$ _\epsilon S_E  A_E\ _E S_\epsilon$$ Where the $S$ are basis-transition matrices, and those are quite likely to be at least as ugly as our original $A$, so I don't see what we're gaining here. If anything this seems to be a lot more work. The only thing I can imagine being easier this way is computing something like $A^{10000}x$ or something, because $A^{10000}$ has a really easy form when $A$ is a diagonal matrix, while this would take forever if $A$ is not a diagonal matrix. But is this really the only purpose of diagonalisation; to compute things like $A^{10000}x$?","I'm told the the purpose of diagonalisation is to bring the matrix in a 'nice' form that allows one to quickly compute with it. However in writing the matrix in this nice diagonal form you have to express it w.r.t. a new eigenvector basis. But you'll probably want the answer of your matrix multiplication written w.r.t. to the original basis, so you'll have to do a not-nice matrix multiplication regardless. Example of what I  mean: I want to compute $Ax$, where $A$ and $x$ are given w.r.t. the standard basis ($\epsilon$). However $A$ is quite large and annoying to compute, so I calculate $A_E$ which is a diagonal matrix written w.r.t. the eigenvector basis. But to compute $Ax$ using this matrix, I still have to compute the following: $$ _\epsilon S_E  A_E\ _E S_\epsilon$$ Where the $S$ are basis-transition matrices, and those are quite likely to be at least as ugly as our original $A$, so I don't see what we're gaining here. If anything this seems to be a lot more work. The only thing I can imagine being easier this way is computing something like $A^{10000}x$ or something, because $A^{10000}$ has a really easy form when $A$ is a diagonal matrix, while this would take forever if $A$ is not a diagonal matrix. But is this really the only purpose of diagonalisation; to compute things like $A^{10000}x$?",,"['linear-algebra', 'diagonalization']"
25,Tensor Book Recommendation Request,Tensor Book Recommendation Request,,Requirements Tensors Intuitive + Practical Reason for Tensor Introduction Current Knowledge Course Notes Abstract + Theoretical,Requirements Tensors Intuitive + Practical Reason for Tensor Introduction Current Knowledge Course Notes Abstract + Theoretical,,"['linear-algebra', 'reference-request', 'soft-question', 'book-recommendation', 'tensors']"
26,Why is the SVD named so?,Why is the SVD named so?,,"The SVD stands for Singular Value Decomposition. After decomposing a data matrix $\mathbf X$ using SVD, it results in three matrices, two matrices with the singular vectors $\mathbf U$ and $\mathbf V$, and one singular value matrix whose diagonal elements are the singular values. But I want to know why those values are named as singular values. Is there any connection between a singular matrix and these singular values?","The SVD stands for Singular Value Decomposition. After decomposing a data matrix $\mathbf X$ using SVD, it results in three matrices, two matrices with the singular vectors $\mathbf U$ and $\mathbf V$, and one singular value matrix whose diagonal elements are the singular values. But I want to know why those values are named as singular values. Is there any connection between a singular matrix and these singular values?",,"['linear-algebra', 'matrices', 'terminology', 'math-history', 'svd']"
27,Is there a formula for the inverse of Hadamard product?,Is there a formula for the inverse of Hadamard product?,,"Say $A$ and $B$ are two square, positive-semidefinite matrices. Is there an expression in terms of matrix product, transpose, and inverse for the Hadamard product $A∘B$? For example, ""$(A∘B)^{-1} = A^{-1} ∘ B^{-1}$"" (which is not true). Edit: I understand that $A∘B$ may not be invertible, but is there any expression if invertibility is given?","Say $A$ and $B$ are two square, positive-semidefinite matrices. Is there an expression in terms of matrix product, transpose, and inverse for the Hadamard product $A∘B$? For example, ""$(A∘B)^{-1} = A^{-1} ∘ B^{-1}$"" (which is not true). Edit: I understand that $A∘B$ may not be invertible, but is there any expression if invertibility is given?",,"['linear-algebra', 'matrices', 'hadamard-product']"
28,Finding Eigenvectors with repeated Eigenvalues,Finding Eigenvectors with repeated Eigenvalues,,"I have a matrix $A = \left(\begin{matrix} -5 & -6 & 3\\3 & 4 & -3\\0 & 0 & -2\end{matrix}\right)$ for which I am trying to find the Eigenvalues and Eigenvectors. In this case, I have repeated Eigenvalues of $\lambda_1 = \lambda_2 = -2$ and $\lambda_3 = 1$. After finding the matrix substituting for $\lambda_1$ and $\lambda_2$, I get the matrix $\left(\begin{matrix} 1 & 2 & -1\\0 & 0 & 0\\0 & 0 & 0\end{matrix}\right)$ after row-reduction. To find the result of $\left(\begin{matrix} 1 & 2 & -1\\0 & 0 & 0\\0 & 0 & 0\end{matrix}\right)$ $\left(\begin{matrix} e_1\\e_2\\e_3\end{matrix}\right) = \left(\begin{matrix} 0\\0\\0\end{matrix}\right)$, I set $e_2$ and $e_3$ as free variables $s$ and $t$, respectively, solved for $e_1$ and put into vector form: $$\left[\begin{matrix} -2s + t\\s\\t\end{matrix}\right] = s \left[\begin{matrix} -2\\1\\0\end{matrix}\right] + t \left[\begin{matrix}1\\0\\1\end{matrix}\right]$$ So for the first two Eigenvectors, I found $v_1 = [-2, 1, 0]^{T}$ and $v_2 = [1, 0, 1]^T$. However, checking my answer on Wolfram Alpha, $v_1$ is assigned to the latter and $v_2$ is assigned to the former. Does this matter? If so, should I instead find the first Eigenvector for $\lambda_1$ then using the same reduced matrix set it equal to the result of $v_1$ instead of the zero vector for $\lambda_2$? I.e., solving the above matrix for $\lambda_1$: $$e_1 = -2e_2 + e_3 \rightarrow  1 = -2(0) + 1 $$ $$v_1 = [1, 0, 1]^T$$ And then for $\lambda_2$: $$\left(\begin{matrix} 1 & 2 & -1\\0 & 0 & 0\\0 & 0 & 0\end{matrix}\right) \left(\begin{matrix} e_1\\e_2\\e_3\end{matrix}\right) = \left(\begin{matrix} 1\\0\\1\end{matrix}\right)$$ However, using $e_3$ as a free variable where $e_3 = 0$, I don't get the above vector I originally found for $v_1$: $$e_1 + 2e_2 = 1 \rightarrow 1(2) + 2(-1/2) = 1$$ $$v_2 = [2, -1/2, 0]^T$$ What am I doing wrong here?","I have a matrix $A = \left(\begin{matrix} -5 & -6 & 3\\3 & 4 & -3\\0 & 0 & -2\end{matrix}\right)$ for which I am trying to find the Eigenvalues and Eigenvectors. In this case, I have repeated Eigenvalues of $\lambda_1 = \lambda_2 = -2$ and $\lambda_3 = 1$. After finding the matrix substituting for $\lambda_1$ and $\lambda_2$, I get the matrix $\left(\begin{matrix} 1 & 2 & -1\\0 & 0 & 0\\0 & 0 & 0\end{matrix}\right)$ after row-reduction. To find the result of $\left(\begin{matrix} 1 & 2 & -1\\0 & 0 & 0\\0 & 0 & 0\end{matrix}\right)$ $\left(\begin{matrix} e_1\\e_2\\e_3\end{matrix}\right) = \left(\begin{matrix} 0\\0\\0\end{matrix}\right)$, I set $e_2$ and $e_3$ as free variables $s$ and $t$, respectively, solved for $e_1$ and put into vector form: $$\left[\begin{matrix} -2s + t\\s\\t\end{matrix}\right] = s \left[\begin{matrix} -2\\1\\0\end{matrix}\right] + t \left[\begin{matrix}1\\0\\1\end{matrix}\right]$$ So for the first two Eigenvectors, I found $v_1 = [-2, 1, 0]^{T}$ and $v_2 = [1, 0, 1]^T$. However, checking my answer on Wolfram Alpha, $v_1$ is assigned to the latter and $v_2$ is assigned to the former. Does this matter? If so, should I instead find the first Eigenvector for $\lambda_1$ then using the same reduced matrix set it equal to the result of $v_1$ instead of the zero vector for $\lambda_2$? I.e., solving the above matrix for $\lambda_1$: $$e_1 = -2e_2 + e_3 \rightarrow  1 = -2(0) + 1 $$ $$v_1 = [1, 0, 1]^T$$ And then for $\lambda_2$: $$\left(\begin{matrix} 1 & 2 & -1\\0 & 0 & 0\\0 & 0 & 0\end{matrix}\right) \left(\begin{matrix} e_1\\e_2\\e_3\end{matrix}\right) = \left(\begin{matrix} 1\\0\\1\end{matrix}\right)$$ However, using $e_3$ as a free variable where $e_3 = 0$, I don't get the above vector I originally found for $v_1$: $$e_1 + 2e_2 = 1 \rightarrow 1(2) + 2(-1/2) = 1$$ $$v_2 = [2, -1/2, 0]^T$$ What am I doing wrong here?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
29,Uniqueness of trace as linearization of the rank,Uniqueness of trace as linearization of the rank,,"It is not difficult to show that if $A \in M_n(k)$ for some field $k$ , and $A^2=A$ then $$\operatorname{tr}(A) = \dim(\operatorname{Im}(A))$$ In this comment , Terry Tao wrote: This property, together with linearity, determines the trace uniquely, and so one can view the trace as the linearised version of the dimension-counting operator. What does it mean precisely? If $f : M_n(k) \to k$ is $k$ -linear (say with $k = \Bbb C$ ), and $f(A) = \dim(\operatorname{Im}(A))$ for any $A^2= A \in M_n(k)$ , then we have that $f$ is the trace function?","It is not difficult to show that if for some field , and then In this comment , Terry Tao wrote: This property, together with linearity, determines the trace uniquely, and so one can view the trace as the linearised version of the dimension-counting operator. What does it mean precisely? If is -linear (say with ), and for any , then we have that is the trace function?",A \in M_n(k) k A^2=A \operatorname{tr}(A) = \dim(\operatorname{Im}(A)) f : M_n(k) \to k k k = \Bbb C f(A) = \dim(\operatorname{Im}(A)) A^2= A \in M_n(k) f,"['linear-algebra', 'matrices', 'matrix-rank', 'trace', 'projection-matrices']"
30,Square root of positive definite nonsymmetric matrix,Square root of positive definite nonsymmetric matrix,,"Let $N$ be a nilpotent matrix in $M_n({\mathbb R})$, such that $(I+N)^2$ is “positive definite” (but not necessarily symmetric) in the sense that $<X,(I+N)^2X>$ is positive for any nonzero $X\in{{\mathbb R}^n}$ (here $<.,.>$ denotes the usual scalar product on ${\mathbb R}^n$).  Is it true that $I+N$ must  always be “positive definite” also ? UPDATE : To clarify the question, I know about http://mathworld.wolfram.com/PositiveDefiniteMatrix.html which makes many promising related statements, but without any proof. This webpage has a list of references at the bottom, but I’ve checked them one by one and was unable to find that special result. I think this question is a rather special case of the properties of the ""unique positive square root function"" described in the link. I'm hoping for an “elementary” proof.","Let $N$ be a nilpotent matrix in $M_n({\mathbb R})$, such that $(I+N)^2$ is “positive definite” (but not necessarily symmetric) in the sense that $<X,(I+N)^2X>$ is positive for any nonzero $X\in{{\mathbb R}^n}$ (here $<.,.>$ denotes the usual scalar product on ${\mathbb R}^n$).  Is it true that $I+N$ must  always be “positive definite” also ? UPDATE : To clarify the question, I know about http://mathworld.wolfram.com/PositiveDefiniteMatrix.html which makes many promising related statements, but without any proof. This webpage has a list of references at the bottom, but I’ve checked them one by one and was unable to find that special result. I think this question is a rather special case of the properties of the ""unique positive square root function"" described in the link. I'm hoping for an “elementary” proof.",,"['linear-algebra', 'matrices']"
31,Is this similarity between trees and vector space bases just a coincidence?,Is this similarity between trees and vector space bases just a coincidence?,,"A vector space basis is a set of vectors that span the space and is linearly independent. It is well-known that for finite dimensional vector spaces this is equivalent to: The set is minimal with respect to being a spanning set. The set is maximal with respect to being linearly independent. Every element of the vector space is uniquely represented as a linear combination of the basis' vectors. Now consider finite trees. A tree is defined as a set of edges (for some fixed vertex set) such that they connect the vertices (e.g. the tree is a connected graph) and the set is ""independent"" in the sense that there are no cycles. Again, this is equivalent to: The set is minimal with respect to the connectedness of the graph. The set is maximal with respect to the independence (any edge added would create a cycle). For every two vertices, there is a unique path between them in the tree. The similarity is very striking, I believe. My question is - is this simply a coincidence or part of a greater (category-theory?) connection I'm unaware of? And are there more instances in mathematics for sets having this ""minimal-maximal-unique"" set of defining properties?","A vector space basis is a set of vectors that span the space and is linearly independent. It is well-known that for finite dimensional vector spaces this is equivalent to: The set is minimal with respect to being a spanning set. The set is maximal with respect to being linearly independent. Every element of the vector space is uniquely represented as a linear combination of the basis' vectors. Now consider finite trees. A tree is defined as a set of edges (for some fixed vertex set) such that they connect the vertices (e.g. the tree is a connected graph) and the set is ""independent"" in the sense that there are no cycles. Again, this is equivalent to: The set is minimal with respect to the connectedness of the graph. The set is maximal with respect to the independence (any edge added would create a cycle). For every two vertices, there is a unique path between them in the tree. The similarity is very striking, I believe. My question is - is this simply a coincidence or part of a greater (category-theory?) connection I'm unaware of? And are there more instances in mathematics for sets having this ""minimal-maximal-unique"" set of defining properties?",,"['linear-algebra', 'graph-theory']"
32,$\operatorname{rank} A = \operatorname{rank} A^2$ if and only if $\lim_{\lambda \to 0} (A+\lambda I)^{-1}A$ exists,if and only if  exists,\operatorname{rank} A = \operatorname{rank} A^2 \lim_{\lambda \to 0} (A+\lambda I)^{-1}A,"Let $A \in \mathbb C^{n \times n}$ . Prove that $\operatorname{rank} A = \operatorname{rank} A^2$ if and only if $\displaystyle\lim_{\lambda \to 0} (A+\lambda I)^{-1}A$ exists. I am stuck on this problem, I don't understand what the limit is supposed to mean. I would guess that if the limit exists, it should be $I$ since the invertible matrices are dense. But how can I relate this to the rank?","Let . Prove that if and only if exists. I am stuck on this problem, I don't understand what the limit is supposed to mean. I would guess that if the limit exists, it should be since the invertible matrices are dense. But how can I relate this to the rank?",A \in \mathbb C^{n \times n} \operatorname{rank} A = \operatorname{rank} A^2 \displaystyle\lim_{\lambda \to 0} (A+\lambda I)^{-1}A I,"['linear-algebra', 'matrices', 'limits', 'matrix-calculus', 'matrix-rank']"
33,What about linearity makes it so useful?,What about linearity makes it so useful?,,"Among all areas of mathematics, linear algebra is incredibly well understood. I have heard it said that the only problems we can really solve in math are linear problems- and that much of the rest of math involves reducing problems to linear algebra? But, what about the exchange of a ""multiplication"" operation and an ""addition"" operation is nice? Why is this interchange desirable, and why - among the many possible properties to specify, is linearity so important? Specifically, I am looking for: An idea of why the exchange that linearity allows is so powerful - whether appealing to some categorical or other argument about why these particular rules are so powerful An idea of why linear problems, or linearization, shows up so frequently","Among all areas of mathematics, linear algebra is incredibly well understood. I have heard it said that the only problems we can really solve in math are linear problems- and that much of the rest of math involves reducing problems to linear algebra? But, what about the exchange of a ""multiplication"" operation and an ""addition"" operation is nice? Why is this interchange desirable, and why - among the many possible properties to specify, is linearity so important? Specifically, I am looking for: An idea of why the exchange that linearity allows is so powerful - whether appealing to some categorical or other argument about why these particular rules are so powerful An idea of why linear problems, or linearization, shows up so frequently",,"['linear-algebra', 'abstract-algebra', 'soft-question', 'group-actions']"
34,Why is second smallest eigenvalue and the corresponding eigenvector used to partition a graph?,Why is second smallest eigenvalue and the corresponding eigenvector used to partition a graph?,,"In spectral graph partition theory , the eigenvector $v_2$ (also called Fiedler vector ) corresponding to the second smallest eigenvalue $\lambda_2$ (also known as Fiedler eigenvalue , which actually also defines the algebraic connectivity of a graph) of the Laplacian matrix $L$ of a graph $G$, in general, is used to partition the graph. What is the underlying philosophy behind this? Any reference to any related proofs on this?","In spectral graph partition theory , the eigenvector $v_2$ (also called Fiedler vector ) corresponding to the second smallest eigenvalue $\lambda_2$ (also known as Fiedler eigenvalue , which actually also defines the algebraic connectivity of a graph) of the Laplacian matrix $L$ of a graph $G$, in general, is used to partition the graph. What is the underlying philosophy behind this? Any reference to any related proofs on this?",,"['linear-algebra', 'matrices', 'graph-theory', 'reference-request', 'graph-laplacian']"
35,The physical meaning of the tensor product,The physical meaning of the tensor product,,"I have come across tensor products many times in physics, namely for matrices, vector-space elements, Hilbert-space elements (quantum states), and representations of groups and algebras. However, the abstract meaning of the tensor product still eludes me. Although I can understand its context in physics, I cannot see why its physical description and its abstract definitions, for example those given on Wikipedia , are equivalent. Could someone provide me with a geometrical, as well as an algebraic, interpretation of the tensor product and also explain why it is required?","I have come across tensor products many times in physics, namely for matrices, vector-space elements, Hilbert-space elements (quantum states), and representations of groups and algebras. However, the abstract meaning of the tensor product still eludes me. Although I can understand its context in physics, I cannot see why its physical description and its abstract definitions, for example those given on Wikipedia , are equivalent. Could someone provide me with a geometrical, as well as an algebraic, interpretation of the tensor product and also explain why it is required?",,"['linear-algebra', 'abstract-algebra']"
36,$AB + BA = A \implies A$ and $B$ have a common eigenvector,and  have a common eigenvector,AB + BA = A \implies A B,"Let $A$ and $B$ in $M_n(\mathbb C)$ with $n$ being odd . Suppose $AB + BA = A$. Prove that $A$ and $B$ have a common eigenvector. (It is from an oral exam.) I ask for a hint for this, not a complete solution. Thank you.","Let $A$ and $B$ in $M_n(\mathbb C)$ with $n$ being odd . Suppose $AB + BA = A$. Prove that $A$ and $B$ have a common eigenvector. (It is from an oral exam.) I ask for a hint for this, not a complete solution. Thank you.",,['linear-algebra']
37,"Why the words ""inner"" and ""outer"" to designate products?","Why the words ""inner"" and ""outer"" to designate products?",,"Does anyone know what's the rationale for using the adjectives inner and outer for certain algebraic products? Also, I've seen the term exterior algebra .  Does the exterior here have anything to do with the outer of outer product ?  If so, is there an interior algebra corresponding to inner products?","Does anyone know what's the rationale for using the adjectives inner and outer for certain algebraic products? Also, I've seen the term exterior algebra .  Does the exterior here have anything to do with the outer of outer product ?  If so, is there an interior algebra corresponding to inner products?",,"['linear-algebra', 'abstract-algebra', 'terminology', 'multilinear-algebra']"
38,Fast computation/estimation of the nuclear norm of a matrix,Fast computation/estimation of the nuclear norm of a matrix,,"The nuclear norm of a matrix is defined as the sum of its singular values, as given by the singular value decomposition (SVD) of the matrix itself. It is of central importance in Signal Processing and Statistics, where it is used for matrix completion and dimensionality reduction. A question I have is whether it's possible to compute the nuclear norm of a given matrix faster than the time needed to compute the SVD. Since we don't need all the singular values but only their sum, this seem possible. Alternatively, perhaps it could be possible to approximate it with simulation methods and/or random projections.","The nuclear norm of a matrix is defined as the sum of its singular values, as given by the singular value decomposition (SVD) of the matrix itself. It is of central importance in Signal Processing and Statistics, where it is used for matrix completion and dimensionality reduction. A question I have is whether it's possible to compute the nuclear norm of a given matrix faster than the time needed to compute the SVD. Since we don't need all the singular values but only their sum, this seem possible. Alternatively, perhaps it could be possible to approximate it with simulation methods and/or random projections.",,"['linear-algebra', 'numerical-linear-algebra', 'svd', 'singular-values', 'nuclear-norm']"
39,Trace inequality for real matrices,Trace inequality for real matrices,,"Is there any general result characterizing real matrices $A$ such that $$[\mathrm{tr}(A)]^2\leq n\mathrm{tr}(A^2)?$$ I can see that the inequality holds if: all eigenvalues of $A$ are real (by the Cauchy-Schwarz inequality) or $A$ is a nonnegative matrix. To see this write $$n\mathrm{tr}(A^2)=n\sum_{i=1}^{n}(A_{ii})^{2}+n\sum_{i,j=1,i\neq j}^{n}A_{ij}A_{ji},$$ and note that, by the sum of squares inequality, $$n\sum_{i=1}^{n}(A_{ii})^{2}\geq\left( \sum_{i=1}^{n}A_{ii}\right)^{2}=\left[\mathrm{tr}(A)\right]^{2}.$$ If $A$ is nonnegative $$n\sum_{i,j=1,i\neq j}^{n}A_{ij}A_{ji}\geq 0,$$ and therefore the inequality holds. But what about matrices not satisfying 1. or 2.? Are there more general conditions (or other specific ones) under which the inequality above holds?","Is there any general result characterizing real matrices $A$ such that $$[\mathrm{tr}(A)]^2\leq n\mathrm{tr}(A^2)?$$ I can see that the inequality holds if: all eigenvalues of $A$ are real (by the Cauchy-Schwarz inequality) or $A$ is a nonnegative matrix. To see this write $$n\mathrm{tr}(A^2)=n\sum_{i=1}^{n}(A_{ii})^{2}+n\sum_{i,j=1,i\neq j}^{n}A_{ij}A_{ji},$$ and note that, by the sum of squares inequality, $$n\sum_{i=1}^{n}(A_{ii})^{2}\geq\left( \sum_{i=1}^{n}A_{ii}\right)^{2}=\left[\mathrm{tr}(A)\right]^{2}.$$ If $A$ is nonnegative $$n\sum_{i,j=1,i\neq j}^{n}A_{ij}A_{ji}\geq 0,$$ and therefore the inequality holds. But what about matrices not satisfying 1. or 2.? Are there more general conditions (or other specific ones) under which the inequality above holds?",,"['linear-algebra', 'matrices', 'inequality', 'trace']"
40,Inverse of Symmetric Matrix Plus Diagonal Matrix if Square Matrix's Inverse Is Known,Inverse of Symmetric Matrix Plus Diagonal Matrix if Square Matrix's Inverse Is Known,,Let $A$ be an $n \times n$ symmetric matrix of rank $n$ with known inverse $A^{-1}$.  Let $D$ be a diagonal matrix with the same dimensions and rank.  What is the fastest way to compute $(A+D)^{-1}$?  Assume all diagonal elements of $A+D$ are positive and that $A+D$ is invertible.,Let $A$ be an $n \times n$ symmetric matrix of rank $n$ with known inverse $A^{-1}$.  Let $D$ be a diagonal matrix with the same dimensions and rank.  What is the fastest way to compute $(A+D)^{-1}$?  Assume all diagonal elements of $A+D$ are positive and that $A+D$ is invertible.,,"['linear-algebra', 'matrices']"
41,Is the set $\{X \in \mathcal{M}({m \times n}) : \rho(M-NX) < 1\} $ connected?,Is the set  connected?,\{X \in \mathcal{M}({m \times n}) : \rho(M-NX) < 1\} ,"Suppose $M \in \mathcal M(n \times n; \mathbb R)$ and $N \in \mathcal M(n \times m; \mathbb R)$ are fixed with $N\neq 0$. Let \begin{align*}   E =  \{X \in \mathcal{M}(m \times n; \mathbb R) : \rho(M-NX) < 1\}, \end{align*} where $\rho(\cdot)$ denotes the spectral radius of a square matrix. We assume $E$ is not empty. This set is open. (see my other question concerning the closure). I would also like to know whether it is connected. In this case, equivalently, is the set path-connected?","Suppose $M \in \mathcal M(n \times n; \mathbb R)$ and $N \in \mathcal M(n \times m; \mathbb R)$ are fixed with $N\neq 0$. Let \begin{align*}   E =  \{X \in \mathcal{M}(m \times n; \mathbb R) : \rho(M-NX) < 1\}, \end{align*} where $\rho(\cdot)$ denotes the spectral radius of a square matrix. We assume $E$ is not empty. This set is open. (see my other question concerning the closure). I would also like to know whether it is connected. In this case, equivalently, is the set path-connected?",,"['linear-algebra', 'general-topology', 'connectedness', 'path-connected']"
42,The size of the set of matrices with spectral radius less than 1,The size of the set of matrices with spectral radius less than 1,,"Consider the set of all matrices with spectral radius $<1$, denoted $\mathcal C=\{A\in \mathbb{C}^{n\times n}:\rho(A)<1\}$. This is an interesting set, partly because $A\in \mathcal C\iff \lim\limits_{k\to\infty} A^k= 0$. It seems difficult to describe the geometry of $\mathcal C$, but let's ask about its size in terms of the Lebesgue measure $\mu$ on   $\mathbb{C}^{n\times n}$. Question : What is the asymptotic behavior of $\mu(\mathcal C\cap \{A:\|A\|\le r\})$ as $r\to\infty$? (That is, is it some power of $r$, or $\log r$, etc?) Remarks It does not matter what matrix norm is used here, since they are all equivalent. Let's exclude the trivial case $n=1$. $\mu(\mathcal C)=\infty$, which can be shown as follows. Let $\epsilon>0$ be such that any $n\times n$ matrix with all entries satisfying $|a_{ij}|<\epsilon$ has spectral radius less than $1$. Denote by $\mathcal E$ the set of matrices with all entries between $\epsilon/2$ and $\epsilon$ in absolute value.  Let $D$ be the diagonal matrix with diagonal entries $(2,1,\dots,1)$. Then the sets $D^k\mathcal E D^{-k}$, $k\in\mathbb Z$, are disjoint, have the same (positive) Lebesgue measure, and are contained in $\mathcal C$. The claim follows. The argument in item 3 shows that $\mu(\mathcal C\cap \{A:\|A\|\le r\}) \gtrsim \log r$.","Consider the set of all matrices with spectral radius $<1$, denoted $\mathcal C=\{A\in \mathbb{C}^{n\times n}:\rho(A)<1\}$. This is an interesting set, partly because $A\in \mathcal C\iff \lim\limits_{k\to\infty} A^k= 0$. It seems difficult to describe the geometry of $\mathcal C$, but let's ask about its size in terms of the Lebesgue measure $\mu$ on   $\mathbb{C}^{n\times n}$. Question : What is the asymptotic behavior of $\mu(\mathcal C\cap \{A:\|A\|\le r\})$ as $r\to\infty$? (That is, is it some power of $r$, or $\log r$, etc?) Remarks It does not matter what matrix norm is used here, since they are all equivalent. Let's exclude the trivial case $n=1$. $\mu(\mathcal C)=\infty$, which can be shown as follows. Let $\epsilon>0$ be such that any $n\times n$ matrix with all entries satisfying $|a_{ij}|<\epsilon$ has spectral radius less than $1$. Denote by $\mathcal E$ the set of matrices with all entries between $\epsilon/2$ and $\epsilon$ in absolute value.  Let $D$ be the diagonal matrix with diagonal entries $(2,1,\dots,1)$. Then the sets $D^k\mathcal E D^{-k}$, $k\in\mathbb Z$, are disjoint, have the same (positive) Lebesgue measure, and are contained in $\mathcal C$. The claim follows. The argument in item 3 shows that $\mu(\mathcal C\cap \{A:\|A\|\le r\}) \gtrsim \log r$.",,"['linear-algebra', 'matrices']"
43,Upper triangular matrices $B$ that commute with every upper triangular matrix commuting with $A$,Upper triangular matrices  that commute with every upper triangular matrix commuting with,B A,"I remember being told that this was true by a professor, but I haven't been able to find a source for it yet. In the theorem as stated, $\mathbb{F}$ is any field and $T_n(\mathbb{F})$ denotes the algebra of upper triangular $n\times n$ matrices over $\mathbb{F}$. Theorem: Let $A,B\in T_n(\mathbb{F})$ be such that for all $X\in T_n(\mathbb{F})$, $$AX=XA\implies BX=XB$$ Then $B=p(A)$ for some $p\in \mathbb{F}[t]$. If we replace $T_n(\mathbb{F})$ by $M_n(\mathbb{F})$, the question is answered in this paper . Unfortunately, the argument doesn't seem to translate directly, as I can't find a way to force the $M_i$ maps to be upper-triangular. Update: I have re-asked the question here on MO. Thanks to David E Speyer, we now know that this theorem is false. In particular, if  $$A=\left[\begin{array}{cccc}0&0&0&1\\ &0&1&0\\ & &0&0\\ & & & 0\end{array}\right]$$ then the matrices commuting with $A$ are those of the form $$\left[\begin{array}{cccc}a&0&*&*\\ &b&*&*\\ & &b&0\\ & & & a\end{array}\right]$$ The matrix $$B=\left[\begin{array}{cccc}0&0&0&1\\ &0&0&0\\ & &0&0\\ & & & 0\end{array}\right]$$ commutes with all matrices of this form, but is clearly not a polynomial in $A$.","I remember being told that this was true by a professor, but I haven't been able to find a source for it yet. In the theorem as stated, $\mathbb{F}$ is any field and $T_n(\mathbb{F})$ denotes the algebra of upper triangular $n\times n$ matrices over $\mathbb{F}$. Theorem: Let $A,B\in T_n(\mathbb{F})$ be such that for all $X\in T_n(\mathbb{F})$, $$AX=XA\implies BX=XB$$ Then $B=p(A)$ for some $p\in \mathbb{F}[t]$. If we replace $T_n(\mathbb{F})$ by $M_n(\mathbb{F})$, the question is answered in this paper . Unfortunately, the argument doesn't seem to translate directly, as I can't find a way to force the $M_i$ maps to be upper-triangular. Update: I have re-asked the question here on MO. Thanks to David E Speyer, we now know that this theorem is false. In particular, if  $$A=\left[\begin{array}{cccc}0&0&0&1\\ &0&1&0\\ & &0&0\\ & & & 0\end{array}\right]$$ then the matrices commuting with $A$ are those of the form $$\left[\begin{array}{cccc}a&0&*&*\\ &b&*&*\\ & &b&0\\ & & & a\end{array}\right]$$ The matrix $$B=\left[\begin{array}{cccc}0&0&0&1\\ &0&0&0\\ & &0&0\\ & & & 0\end{array}\right]$$ commutes with all matrices of this form, but is clearly not a polynomial in $A$.",,"['linear-algebra', 'reference-request']"
44,Dual Basis: Finite Versus Infinite Dimensional Linear Spaces,Dual Basis: Finite Versus Infinite Dimensional Linear Spaces,,"Motivation I know that in a finite dimensional Euclidean space $\Bbb{E}^n$, for every basis $G=\{g_1,g_2,...,g_n\} \subset \Bbb{E}^n$ we can define a dual basis $G'=\{g^1,g^2,...,g^n\} \subset \Bbb{E}^n$ such that $$g_i \cdot g^j = \delta_{i}^{j} \tag{1}$$ Also, it can be proved that such a basis exists and it is unique. The main advantage of dual bases is that when we write an arbitrary vector as a linear combination of the original basis $G$ then we can obtain the coefficients of the linear combination by just using the orthogonality property of $G$ and $G'$ like below $$\begin{align}  x &= \sum_{i=1}^{n}x^i g_i \\ x \cdot g^j &= \sum_{i=1}^{n} x^i g_i \cdot g^j = \sum_{i=1}^{n} x^i \delta_{i}^{j} = x^j \\ x &= \sum_{i=1}^{n}(x \cdot g^i) g_i \end{align} \tag{2}$$ Now, let us go into the infinite dimensional space of infinitely differentiable functions $f(x)$ over the interval $[-a,a]$. We all know that the eigen-functions of the Sturm-Liouville operator can form an orthonormal basis for such functions with proper boundary conditions at the end points. However, this is really a nice operator, a self-adjoint one which produces orthonormal eigen-functions. In some Partial Differential Equations (PDEs) , we encounter non-self-adjoint operators and we should expand our solution and boundary data in terms of their eigen-functions which unfortunately are not orthogonal anymore! Just to give an example, consider the following biharmonic boundary value problem (BVP) $$\begin{array}{lll} \Delta^2 \Phi=0 & -a \le x \le a & -b \le y \le b \\ \Phi(a,y)=0 & \Phi_x(a,y)=0 & \\ \Phi(-a,y)=0 & \Phi_x(-a,y)=0 & \\ \Phi(x,b)=f(x) & \Phi_y(x,b)=0 & \\ \Phi(x,-b)=f(x) & \Phi_y(x,-b)=0 & \\ \end{array} \tag{3}$$ where we have the symmetry $f(-x)=f(x)$. Also, for the sake of continuity of boundary conditions at the corners we require that $$f(a)=f(-a)=f'(a)=f'(-a)=0 \tag{4}$$ Then solving this BVP leads to the following eigen-value problem $$ \left( \frac{d^4}{dx^4}+2\omega^2\frac{d^2}{dx^2}+\omega^4 \right)X(x)=0 \\    X(a)=X(-a)=X'(a)=X'(-a)=0 \tag{5}$$ which has a non-self adjoint operator. The eigen-functions are known as Papkovich-Fadle eigen-functions . They can form a basis for the infinite dimensional space of infinitely differentiable functions $f(x)$ satisfying $(4)$ over the interval $[-a,a]$. As I told before, these eigen-functions are not orthogonal and this makes finding the coefficients $c_i$ of the expansions $$f(x)= \sum_{i=1}^{\infty} c_i X(\omega_i;x) \tag{6}$$ really difficult leading to solve an infinite system of linear algebraic equations for the unknown coefficients $c_i$! Questions $1.$ Is there a dual basis thing for the basis $X(\omega_i;x)$'s that can make the computation of $c_i$'s easier? To be more specific, is there some basis $Y(\omega_j;x)$ such that $$\int_{-a}^{a} X(\omega_i;x) Y(\omega_j;x) dx =\delta_{ij} \tag{7}$$ which can be considered to have the similar role of $g^j$. If such a thing existed then we could compute the $c_i$'s by using $(6)$ and the orthogonality in $(7)$ easily $$\int_{-a}^{a} f(x) Y(\omega_j;x) dx = \sum_{i=1}^{\infty} c_i \int_{-a}^{a} X(\omega_i;x)  Y(\omega_j;x) dx = \sum_{i=1}^{\infty} c_i \delta_{ij} = c_j \tag{8}$$ $2.$ If the answer to question $1$ is YES , how it can be computed? If NO , Why?","Motivation I know that in a finite dimensional Euclidean space $\Bbb{E}^n$, for every basis $G=\{g_1,g_2,...,g_n\} \subset \Bbb{E}^n$ we can define a dual basis $G'=\{g^1,g^2,...,g^n\} \subset \Bbb{E}^n$ such that $$g_i \cdot g^j = \delta_{i}^{j} \tag{1}$$ Also, it can be proved that such a basis exists and it is unique. The main advantage of dual bases is that when we write an arbitrary vector as a linear combination of the original basis $G$ then we can obtain the coefficients of the linear combination by just using the orthogonality property of $G$ and $G'$ like below $$\begin{align}  x &= \sum_{i=1}^{n}x^i g_i \\ x \cdot g^j &= \sum_{i=1}^{n} x^i g_i \cdot g^j = \sum_{i=1}^{n} x^i \delta_{i}^{j} = x^j \\ x &= \sum_{i=1}^{n}(x \cdot g^i) g_i \end{align} \tag{2}$$ Now, let us go into the infinite dimensional space of infinitely differentiable functions $f(x)$ over the interval $[-a,a]$. We all know that the eigen-functions of the Sturm-Liouville operator can form an orthonormal basis for such functions with proper boundary conditions at the end points. However, this is really a nice operator, a self-adjoint one which produces orthonormal eigen-functions. In some Partial Differential Equations (PDEs) , we encounter non-self-adjoint operators and we should expand our solution and boundary data in terms of their eigen-functions which unfortunately are not orthogonal anymore! Just to give an example, consider the following biharmonic boundary value problem (BVP) $$\begin{array}{lll} \Delta^2 \Phi=0 & -a \le x \le a & -b \le y \le b \\ \Phi(a,y)=0 & \Phi_x(a,y)=0 & \\ \Phi(-a,y)=0 & \Phi_x(-a,y)=0 & \\ \Phi(x,b)=f(x) & \Phi_y(x,b)=0 & \\ \Phi(x,-b)=f(x) & \Phi_y(x,-b)=0 & \\ \end{array} \tag{3}$$ where we have the symmetry $f(-x)=f(x)$. Also, for the sake of continuity of boundary conditions at the corners we require that $$f(a)=f(-a)=f'(a)=f'(-a)=0 \tag{4}$$ Then solving this BVP leads to the following eigen-value problem $$ \left( \frac{d^4}{dx^4}+2\omega^2\frac{d^2}{dx^2}+\omega^4 \right)X(x)=0 \\    X(a)=X(-a)=X'(a)=X'(-a)=0 \tag{5}$$ which has a non-self adjoint operator. The eigen-functions are known as Papkovich-Fadle eigen-functions . They can form a basis for the infinite dimensional space of infinitely differentiable functions $f(x)$ satisfying $(4)$ over the interval $[-a,a]$. As I told before, these eigen-functions are not orthogonal and this makes finding the coefficients $c_i$ of the expansions $$f(x)= \sum_{i=1}^{\infty} c_i X(\omega_i;x) \tag{6}$$ really difficult leading to solve an infinite system of linear algebraic equations for the unknown coefficients $c_i$! Questions $1.$ Is there a dual basis thing for the basis $X(\omega_i;x)$'s that can make the computation of $c_i$'s easier? To be more specific, is there some basis $Y(\omega_j;x)$ such that $$\int_{-a}^{a} X(\omega_i;x) Y(\omega_j;x) dx =\delta_{ij} \tag{7}$$ which can be considered to have the similar role of $g^j$. If such a thing existed then we could compute the $c_i$'s by using $(6)$ and the orthogonality in $(7)$ easily $$\int_{-a}^{a} f(x) Y(\omega_j;x) dx = \sum_{i=1}^{\infty} c_i \int_{-a}^{a} X(\omega_i;x)  Y(\omega_j;x) dx = \sum_{i=1}^{\infty} c_i \delta_{ij} = c_j \tag{8}$$ $2.$ If the answer to question $1$ is YES , how it can be computed? If NO , Why?",,"['linear-algebra', 'functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
45,Combinatorics in finite vector space,Combinatorics in finite vector space,,"Let $q$ be a prime power and $V$ a finite $\mathbb F_q$-vector space with two subspaces $I$ and $J$.   Let $k$, $a$ and $b$ be non-negative integers.   Determine the number of subspaces $K$ of $V$ with $\dim(K) = k$, $\dim(K\cap I) = a$ and $\dim(K\cap J) = b$. The answer should depend on the numbers $a,b,k$ and $v = \dim(V)$, $i = \dim(I)$, $j = \dim(J), c = \dim(I\cap J)$, but otherwise be independent of the exact choice of $I$ and $J$. In the case that only a single subspace $I$ is given (say $J = \{0\}$ and $b = 0$), I found the answer $$ q^{(k-a)(i-a)}\binom{i}{a}_{\!q} \binom{v-i}{k-a}_{\!q}, $$ where $\binom{x}{y}_q$ denotes the Gaussian binomial coefficient. However for two subspaces $I$ and $J$, this problems looks quite hard to me. Any ideas are welcome. Even the special cases $I \cap J = \{0\}$ or $J \subseteq I$ (both generalizing the case $J = \{0\}$ discussed above) are not clear to me.","Let $q$ be a prime power and $V$ a finite $\mathbb F_q$-vector space with two subspaces $I$ and $J$.   Let $k$, $a$ and $b$ be non-negative integers.   Determine the number of subspaces $K$ of $V$ with $\dim(K) = k$, $\dim(K\cap I) = a$ and $\dim(K\cap J) = b$. The answer should depend on the numbers $a,b,k$ and $v = \dim(V)$, $i = \dim(I)$, $j = \dim(J), c = \dim(I\cap J)$, but otherwise be independent of the exact choice of $I$ and $J$. In the case that only a single subspace $I$ is given (say $J = \{0\}$ and $b = 0$), I found the answer $$ q^{(k-a)(i-a)}\binom{i}{a}_{\!q} \binom{v-i}{k-a}_{\!q}, $$ where $\binom{x}{y}_q$ denotes the Gaussian binomial coefficient. However for two subspaces $I$ and $J$, this problems looks quite hard to me. Any ideas are welcome. Even the special cases $I \cap J = \{0\}$ or $J \subseteq I$ (both generalizing the case $J = \{0\}$ discussed above) are not clear to me.",,"['linear-algebra', 'combinatorics', 'vector-spaces', 'finite-fields']"
46,Number of real solutions of a random equation,Number of real solutions of a random equation,,"Let $(J_{ij})$ be an $n \times n$ random matrix with i.i.d Gaussian centered coefficients with $\displaystyle \mathbb{E}[J_{ij}^2] = \frac{\sigma^2}{n}$. Let the random variable $A_n(\sigma)$ defined as the number of real solutions in $\mathbb{R}^n$ of : $$-x_i + \sum_{j=1}^n J_{ij} \phi(x_j) = 0\mbox{ for all }1\leq i \leq n$$ where $\phi(x) = \arctan(x)$. The question is :  what is the law of $A_n(\sigma)$ ? In particular, its expectation ? I know how to solve ""by hand"" the case n=1, and n=2, but then it becomes really painful. Any idea? Thank you! Edit : I have a conjecture but I do not know if it is true: $$\lim_{n \to \infty} \frac{1}{n}\log \mathbb{E}[A_n(\sigma)] = C(\sigma)$$ with $C(\sigma)=0$ for $\sigma <1$ and $C(\sigma)=O((\sigma-1)^2)$ for $\sigma \to 1^+$. What do you think of this?","Let $(J_{ij})$ be an $n \times n$ random matrix with i.i.d Gaussian centered coefficients with $\displaystyle \mathbb{E}[J_{ij}^2] = \frac{\sigma^2}{n}$. Let the random variable $A_n(\sigma)$ defined as the number of real solutions in $\mathbb{R}^n$ of : $$-x_i + \sum_{j=1}^n J_{ij} \phi(x_j) = 0\mbox{ for all }1\leq i \leq n$$ where $\phi(x) = \arctan(x)$. The question is :  what is the law of $A_n(\sigma)$ ? In particular, its expectation ? I know how to solve ""by hand"" the case n=1, and n=2, but then it becomes really painful. Any idea? Thank you! Edit : I have a conjecture but I do not know if it is true: $$\lim_{n \to \infty} \frac{1}{n}\log \mathbb{E}[A_n(\sigma)] = C(\sigma)$$ with $C(\sigma)=0$ for $\sigma <1$ and $C(\sigma)=O((\sigma-1)^2)$ for $\sigma \to 1^+$. What do you think of this?",,"['linear-algebra', 'probability', 'geometry']"
47,Estimating a complex integral for a probability problem: $HH$ is more likely to appear than $THT$ or $TTT$ in $n$ coin flips,Estimating a complex integral for a probability problem:  is more likely to appear than  or  in  coin flips,HH THT TTT n,"The probability problem asks us to toss a coin $n$ times (actually $100$ times) and Alice gets a point whenever there is a $HH$ and Bob gets a point whenever there is a $THT$ or a $TTT$ . Who is more likely to win? [As an example, consider the sequence $THHHTHTHTTT$ . This sequence gives $2$ points to ALice and $3$ points to Bob.] Using (two separate) Markov chains and matrix multiplications (done on a computer), we found out that Alice is more likely to win (Alice wins $622452888834764723839990461444$ times while Bob wins $597223930770097681231606561162$ times when we have $100$ tosses). We tried it for several other $n$ , and the graph shows that Alice is always more likely to win. We verified this answer using a recursive solution (also done using a computer), so we believe that the answers are correct. So we tried to do this analytically by looking at the generating function for the probabilities as such. We denote by $p_{n,k,TT}=\mathbb P(X_n=k|Y_{n-1}=T,Y_n = T)$ where $X_i$ is a random variable for the difference of scores between Alice and Bob after $i$ -th toss, and $Y_j$ represents the outcome of the $j$ -th toss. Similarly, we define $p_{n,k,TH},p_{n,k,HT},p_{n,k,HH}$ . The recursion that comes out is the following: $$ \begin{align} p_{n,k,TT}&=\frac12(p_{n-1,k+1,TT}+p_{n-1,k,HT}) \\ p_{n,k,TH}&=\frac12(p_{n-1,k,HT}+p_{n-1,k,TT}) \\ p_{n,k,HT}&=\frac12(p_{n-1,k+1,TH}+p_{n-1,k,HH}) \\ p_{n,k,HH}&=\frac12(p_{n-1,k-1,TH}+p_{n-1,k-1,HH}) \\ \end{align} $$ We define the generating function $\varphi_n(z) = \sum_k \mathbb P(X_n=k)z^k$ and define $\varphi_{n,TT}(z),\varphi_{n,TH}(z),\varphi_{n,HT}(z),\varphi_{n,HH}(z)$ as the respective conditional generating functions. Along with proper base cases, this gives $$ \varphi_n(z) = \frac1{2^n}\begin{bmatrix}1 & 1 & 1 & 1\end{bmatrix}\begin{bmatrix}\frac1z & 0 & 1 & 0 \\ 1 & 0 & 1 & 0\\ 0 & \frac1z & 0 & 1\\ 0 & z & 0 & z\end{bmatrix}^{n-2}\begin{bmatrix}1 \\ 1 \\ 1 \\ z\end{bmatrix} $$ Now realizing that $$ \mathbb P(\text{Bob winning})={\frac 1 {2\pi i}}\int_C\left(\sum_{k<0}z^{-k-1}\right)\varphi_n(z)dz $$ $$ \mathbb P(\text{Alice winning})={\frac 1 {2\pi i}}\int_{C'}\left(\sum_{k>0}z^{-k-1}\right)\varphi_n(z)dz $$ for proper contours $C,C'$ we define later. Doing some complex analysis magic, we get $$\mathbb P(\text{Alice winning})-\mathbb P(\text{Bob winning})={\frac 1 {2\pi i}}\int_C {\frac {\varphi_n(z^{-1})-\varphi_n(z)} {1-z}}~\mathrm dz$$ where $C$ is a contour satisfying $\max \{|z|: z\in C\}<1$ and containing the origin inside (the half radius circle should work). Now we want to show that this integral is greater than $0$ for all $n\geq3$ but really stuck. We tried diagonalizing the matrix to understand $\varphi_n(z)$ but it was horrible. Any help is appreciated.","The probability problem asks us to toss a coin times (actually times) and Alice gets a point whenever there is a and Bob gets a point whenever there is a or a . Who is more likely to win? [As an example, consider the sequence . This sequence gives points to ALice and points to Bob.] Using (two separate) Markov chains and matrix multiplications (done on a computer), we found out that Alice is more likely to win (Alice wins times while Bob wins times when we have tosses). We tried it for several other , and the graph shows that Alice is always more likely to win. We verified this answer using a recursive solution (also done using a computer), so we believe that the answers are correct. So we tried to do this analytically by looking at the generating function for the probabilities as such. We denote by where is a random variable for the difference of scores between Alice and Bob after -th toss, and represents the outcome of the -th toss. Similarly, we define . The recursion that comes out is the following: We define the generating function and define as the respective conditional generating functions. Along with proper base cases, this gives Now realizing that for proper contours we define later. Doing some complex analysis magic, we get where is a contour satisfying and containing the origin inside (the half radius circle should work). Now we want to show that this integral is greater than for all but really stuck. We tried diagonalizing the matrix to understand but it was horrible. Any help is appreciated.","n 100 HH THT TTT THHHTHTHTTT 2 3 622452888834764723839990461444 597223930770097681231606561162 100 n p_{n,k,TT}=\mathbb P(X_n=k|Y_{n-1}=T,Y_n = T) X_i i Y_j j p_{n,k,TH},p_{n,k,HT},p_{n,k,HH} 
\begin{align}
p_{n,k,TT}&=\frac12(p_{n-1,k+1,TT}+p_{n-1,k,HT}) \\
p_{n,k,TH}&=\frac12(p_{n-1,k,HT}+p_{n-1,k,TT}) \\
p_{n,k,HT}&=\frac12(p_{n-1,k+1,TH}+p_{n-1,k,HH}) \\
p_{n,k,HH}&=\frac12(p_{n-1,k-1,TH}+p_{n-1,k-1,HH}) \\
\end{align}
 \varphi_n(z) = \sum_k \mathbb P(X_n=k)z^k \varphi_{n,TT}(z),\varphi_{n,TH}(z),\varphi_{n,HT}(z),\varphi_{n,HH}(z) 
\varphi_n(z) = \frac1{2^n}\begin{bmatrix}1 & 1 & 1 & 1\end{bmatrix}\begin{bmatrix}\frac1z & 0 & 1 & 0 \\
1 & 0 & 1 & 0\\
0 & \frac1z & 0 & 1\\
0 & z & 0 & z\end{bmatrix}^{n-2}\begin{bmatrix}1 \\ 1 \\ 1 \\ z\end{bmatrix}
 
\mathbb P(\text{Bob winning})={\frac 1 {2\pi i}}\int_C\left(\sum_{k<0}z^{-k-1}\right)\varphi_n(z)dz
 
\mathbb P(\text{Alice winning})={\frac 1 {2\pi i}}\int_{C'}\left(\sum_{k>0}z^{-k-1}\right)\varphi_n(z)dz
 C,C' \mathbb P(\text{Alice winning})-\mathbb P(\text{Bob winning})={\frac 1 {2\pi i}}\int_C {\frac {\varphi_n(z^{-1})-\varphi_n(z)} {1-z}}~\mathrm dz C \max \{|z|: z\in C\}<1 0 n\geq3 \varphi_n(z)","['linear-algebra', 'probability', 'complex-analysis', 'contour-integration']"
48,Is the power of $2$ in the Euclidean norm related to the fact that the algebraic closure of the reals is $2$-dimensional?,Is the power of  in the Euclidean norm related to the fact that the algebraic closure of the reals is -dimensional?,2 2,"Consider any local field $K$ , endowed with its topological field structure. We define the function $| \cdot | : K \to \mathbb{R}_{\ge 0}$ as $$|x| = \frac{\mu(xS)}{\mu(S)},$$ where $\mu$ is any Haar measure (which exists because a local field is additively a locally compact group), and $S$ is any measurable set of nonzero measure (see e.g. Definition 12.28 here ). This definition can be shown to be independent of $\mu$ and $S$ . Now consider a vector space $V \simeq K^n$ over $K$ , and equip it with the $p$ -length function $$||(x_1,x_2,\ldots,x_n)||_p = (|x_1|^p+|x_2|^p+\ldots+|x_n|^p)^{1/p}$$ for some $p \in [1,\infty]$ (where as usual we define $|| \cdot ||_\infty$ as the limit of $|| \cdot ||_p$ as $p\to \infty$ ). There is a unique critical value $p=p^*(K)$ for which this length function becomes especially symmetric, in that its associated group of isometries (i.e., the subgroup of $GL(V)$ that preserves $|| \cdot ||_{p^*(K)}$ ) acts transitively on the corresponding unit sphere. The amazing fact is that this critical value $p^*(K)$ always coincides with the degree $[\bar{K}:K]$ of the algebraic closure of $K$ ! Here is a proof sketch: From the classification of local fields, it is known ${}^{(\dagger)}$ that $K$ is either $\mathbb{R}$ , $\mathbb{C}$ or a non-Archimedean local field (that is, a finite extension of either $\mathbb{Q}_P$ or $\mathbb{F}_P((t))$ for some prime $P$ ). If $K=\mathbb{R}$ , $|\cdot|$ is the usual absolute value, and the critical length function turns out to be $|| \cdot ||_2$ (the usual Euclidean length), invariant under $O(n)$ . On the other hand, $[\bar{\mathbb{R}}:\mathbb{R}]=[\mathbb{C}:\mathbb{R}]=2$ . If $K=\mathbb{C}$ , we have $|x|=\bar{x} x$ , and the corresponding critical length function is $|| \cdot ||_1$ , invariant under $U(n)$ . ${}^{(\ddagger)}$ On the other hand, $[\bar{\mathbb{C}}:\mathbb{C}]=[\mathbb{C}:\mathbb{C}]=1$ . Note that neither $|\cdot|$ nor $||\cdot||$ satisfy the triangle inequality in this particular case (this is why I avoided using the terms absolute value and norm throughout the question). If $K$ is non-Archimedean, $|\cdot|$ is a suitably normalized $\chi$ -adic absolute value for some $\chi$ , and the corresponding critical length function is the supremum norm $|| \cdot ||_\infty$ , which can be shown to be invariant under $GL(\mathcal{O}_K^n)$ , where $\mathcal{O}_K$ is the ring of integers of $K$ . On the other hand, it is known that the algebraic closure of $K$ has infinite degree, i.e. $[\bar{K}:K]=\infty$ . Thus in all cases we have $p^*(K) = [\bar{K}:K]$ . A problem with this proof is that it gives no indication as to why the relationship holds: after all, the values could just happen to be the same by complete coincidence. For that reason, I am wondering if there exists an alternative proof of this fact that is "" $K$ -agnostic"", i.e. a proof that does not use at any point the Artin-Schreier theorem , nor the classification of local fields, nor any properties from a specific such field (such as the existence of a symmetric/Hermitian inner product) other than the degree of an algebraic closure and properties derived from it. Such a proof would allow one to meaningfully speak of a relationship between those two quantities, and to state things like ""in a field with a $3$ -dimensional algebraic closure, the natural analogue of the Pythagorean theorem would be $|a|^3+|b|^3=|c|^3$ "" without appealing to the principle of explosion. I know it is hard in general to formally determine whether a proof does or does not use a fact, hence why I'm labeling this with the soft-question tag, but hopefully the kind of proof I want is clear, at least informally. In summary, my question is: Given a local field $K$ , with $| \cdot |$ defined as above, is there any $K$ -agnostic proof that the group of isometries of $||\cdot||_p$ (defined in terms of $| \cdot |$ as above) acts transitively on unit spheres if and only if $p = [\bar{K}:K]$ ? It seems reasonable to try proving first that $p^*(K) = 1$ iff $K$ is algebraically closed. That is where I am currently stuck. The thing that makes linear algebra over an algebraically closed field ""special"" is that every linear transformation has an eigenvalue, but I am not sure how to apply that fact to show that the critical length function is $||\cdot||_1$ . UPDATE : As suggested by Torsten Schoeneberg in the comments, perhaps it will be easier to prove the relationship $p^*(K) = [L:K]\: p^*(L)$ for a finite field extension $L/K$ . Consider such an extension with $[L:K]=n$ , and suppose we know $L$ has a critical $p$ -length function for some $p$ . Now, $L \simeq K^n$ is additively a vector space over $K$ , which is equipped with the product topology. This means that, if we identify $k\in K $ with its image under the inclusion $K \subseteq L$ , we have $$|k|_L = \frac{\mu_L(k S^n)}{\mu_L(S^n)} = \left(\frac{\mu_K(k S)}{\mu_K(S)}\right)^n = |k|_K^n.$$ On the other hand, note that the subgroup of $GL_1(L) = L^\times$ preserving $|\cdot|_L$ , i.e. the group of invertible elements of norm $1$ , acts transitively on the associated unit sphere (which coincides with the group itself) in a tautological way, since any element $x$ of norm $1$ can be sent to any other element $y$ of norm $1$ through multiplication by $x^{-1}y$ . Since $GL_1(L) \subseteq GL_n(K)$ , this proves that the length function on $K^n$ defined through the identification $L \simeq K^n$ as $||\cdot|| = |\cdot|_L^{1/n}$ is a critical length function, that moreover satisfies the homogeneity property $||kv||=|k|_L ||v||$ (which follows by the above relationship between the Haar measures and multiplicativity of $|\cdot|$ ). In a similar way, from the inclusion $GL_m(L) \subseteq GL_{mn}(K)$ we can prove that any critical length function $||\cdot||$ on $L^m$ induces a corresponding critical length function on $K^{mn}$ . The only ingredient left would be to prove is that there always exists some basis $\{1=a_0, a_1, \ldots, a_{n-1}\}$ of $L$ such that the above defined length functions are of the required form, that is, $|\sum_{i=0}^{n-1} k_i a_i|_L = \sum_{i=0}^{n-1} |k_i|_K^n$ . Once we have that, we can show using isotropy subgroups that the length function induced from $L$ is critical in any dimension, not just multiples of $n$ , so that $p^*(K) = [L:K]\: p^*(L)$ . However, I don't know how to prove the existence of such a basis in a $K$ -agnostic way. EDIT: A possible way to restate the problem is by noticing that for any $(v,w)$ in the direct sum $V \oplus W$ , we have $||(v,w)||^p_p = ||v||^p_p + ||w||^p_p$ , i.e. the $p$ th power of $||\cdot||_p$ acts additively on ""independent"" vectors (for finite $p$ , of course). Given a length function on vector spaces $V$ , we can define the associated Gaussian function as an integrable function $g : V \to \mathbb{R}_{\ge 0}$ depending only on length (i.e. factorizing as the composition $f \circ ||\cdot||_p : V \to \mathbb{R}_{\ge 0} \to \mathbb{R}_{\ge 0}$ for some $f$ ), and satisfying the independence property $$g( (v,w) ) = g(v) g(w).$$ With the above choice of maximally symmetric length functions, using the aforementioned additivity property immediately leads to the usual Gaussian function $\exp(-k ||x||_2^2)$ for $\mathbb{R}$ and to $\exp(-k' ||x||_1)$ for $\mathbb{C}$ (where the constants $k, k' >0$ depend on the normalization convention). For $p=\infty$ , this reasoning can't be used directly, but if we interpret it as a limit like before, we do recover the standard Gaussian function for non-Archimedean fields $$\lim_{p\to \infty} \exp(-k'' ||x||_p^p) = \begin{cases} 1 & ||x||_p \le 1 \\ 0 & \text{otherwise} \end{cases} = \mathbf{1}_\mathcal{O_K^n}(x),$$ which is the indicator function of $\mathcal{O}_K^n$ in $V=K^n$ . Focusing on the $1$ -dimensional case $V=K$ , these Gaussian functions coincide with the standard ones as used e.g. in Tate's thesis . So if I'm not mistaken, the problem can be restated as proving that $[\bar{K} : K] = p$ if and only if the standard Gaussian function associated to $K$ is $\exp(-k ||x||_p^p)$ (with a limit intended if $p=\infty$ ). A possible advantage of this restatement is that standard Gaussian functions have different characterizations that do not previously assume any length function (for example, they are their own Fourier transform, or satisfy analogues of the central limit theorem, see e.g. here ). $(\dagger)$ Note that by the Artin-Schreier theorem, the only degrees $[\bar{K}:K]$ that could possibly occur for any field $K$ are $1, 2, \infty$ (this, in turn, seems to be related to the fact that a nonzero integer can only have multiplicative order $1, 2$ or $\infty$ , but that is a topic for another question). $(\ddagger)$ As a fun fact, this can be shown to imply that the hypervolume of the $n$ -dimensional complex unit ball $|x_1|+|x_2|+\ldots+|x_n| = 1$ , i.e. the ordinary $2n$ -dimensional unit ball, equals $V(B_{2n})=V(B_{2})^n /n!=\pi^n /n!$ ; compare with the hypervolume $V(X_{n})=V(X_{1})^n /n!=2^n /n!$ of the $n$ -dimensional cross-polytope $|x_1|+|x_2|+\ldots+|x_n| = 1$ , where $| \cdot |$ now denotes the real absolute value.","Consider any local field , endowed with its topological field structure. We define the function as where is any Haar measure (which exists because a local field is additively a locally compact group), and is any measurable set of nonzero measure (see e.g. Definition 12.28 here ). This definition can be shown to be independent of and . Now consider a vector space over , and equip it with the -length function for some (where as usual we define as the limit of as ). There is a unique critical value for which this length function becomes especially symmetric, in that its associated group of isometries (i.e., the subgroup of that preserves ) acts transitively on the corresponding unit sphere. The amazing fact is that this critical value always coincides with the degree of the algebraic closure of ! Here is a proof sketch: From the classification of local fields, it is known that is either , or a non-Archimedean local field (that is, a finite extension of either or for some prime ). If , is the usual absolute value, and the critical length function turns out to be (the usual Euclidean length), invariant under . On the other hand, . If , we have , and the corresponding critical length function is , invariant under . On the other hand, . Note that neither nor satisfy the triangle inequality in this particular case (this is why I avoided using the terms absolute value and norm throughout the question). If is non-Archimedean, is a suitably normalized -adic absolute value for some , and the corresponding critical length function is the supremum norm , which can be shown to be invariant under , where is the ring of integers of . On the other hand, it is known that the algebraic closure of has infinite degree, i.e. . Thus in all cases we have . A problem with this proof is that it gives no indication as to why the relationship holds: after all, the values could just happen to be the same by complete coincidence. For that reason, I am wondering if there exists an alternative proof of this fact that is "" -agnostic"", i.e. a proof that does not use at any point the Artin-Schreier theorem , nor the classification of local fields, nor any properties from a specific such field (such as the existence of a symmetric/Hermitian inner product) other than the degree of an algebraic closure and properties derived from it. Such a proof would allow one to meaningfully speak of a relationship between those two quantities, and to state things like ""in a field with a -dimensional algebraic closure, the natural analogue of the Pythagorean theorem would be "" without appealing to the principle of explosion. I know it is hard in general to formally determine whether a proof does or does not use a fact, hence why I'm labeling this with the soft-question tag, but hopefully the kind of proof I want is clear, at least informally. In summary, my question is: Given a local field , with defined as above, is there any -agnostic proof that the group of isometries of (defined in terms of as above) acts transitively on unit spheres if and only if ? It seems reasonable to try proving first that iff is algebraically closed. That is where I am currently stuck. The thing that makes linear algebra over an algebraically closed field ""special"" is that every linear transformation has an eigenvalue, but I am not sure how to apply that fact to show that the critical length function is . UPDATE : As suggested by Torsten Schoeneberg in the comments, perhaps it will be easier to prove the relationship for a finite field extension . Consider such an extension with , and suppose we know has a critical -length function for some . Now, is additively a vector space over , which is equipped with the product topology. This means that, if we identify with its image under the inclusion , we have On the other hand, note that the subgroup of preserving , i.e. the group of invertible elements of norm , acts transitively on the associated unit sphere (which coincides with the group itself) in a tautological way, since any element of norm can be sent to any other element of norm through multiplication by . Since , this proves that the length function on defined through the identification as is a critical length function, that moreover satisfies the homogeneity property (which follows by the above relationship between the Haar measures and multiplicativity of ). In a similar way, from the inclusion we can prove that any critical length function on induces a corresponding critical length function on . The only ingredient left would be to prove is that there always exists some basis of such that the above defined length functions are of the required form, that is, . Once we have that, we can show using isotropy subgroups that the length function induced from is critical in any dimension, not just multiples of , so that . However, I don't know how to prove the existence of such a basis in a -agnostic way. EDIT: A possible way to restate the problem is by noticing that for any in the direct sum , we have , i.e. the th power of acts additively on ""independent"" vectors (for finite , of course). Given a length function on vector spaces , we can define the associated Gaussian function as an integrable function depending only on length (i.e. factorizing as the composition for some ), and satisfying the independence property With the above choice of maximally symmetric length functions, using the aforementioned additivity property immediately leads to the usual Gaussian function for and to for (where the constants depend on the normalization convention). For , this reasoning can't be used directly, but if we interpret it as a limit like before, we do recover the standard Gaussian function for non-Archimedean fields which is the indicator function of in . Focusing on the -dimensional case , these Gaussian functions coincide with the standard ones as used e.g. in Tate's thesis . So if I'm not mistaken, the problem can be restated as proving that if and only if the standard Gaussian function associated to is (with a limit intended if ). A possible advantage of this restatement is that standard Gaussian functions have different characterizations that do not previously assume any length function (for example, they are their own Fourier transform, or satisfy analogues of the central limit theorem, see e.g. here ). Note that by the Artin-Schreier theorem, the only degrees that could possibly occur for any field are (this, in turn, seems to be related to the fact that a nonzero integer can only have multiplicative order or , but that is a topic for another question). As a fun fact, this can be shown to imply that the hypervolume of the -dimensional complex unit ball , i.e. the ordinary -dimensional unit ball, equals ; compare with the hypervolume of the -dimensional cross-polytope , where now denotes the real absolute value.","K | \cdot | : K \to \mathbb{R}_{\ge 0} |x| = \frac{\mu(xS)}{\mu(S)}, \mu S \mu S V \simeq K^n K p ||(x_1,x_2,\ldots,x_n)||_p = (|x_1|^p+|x_2|^p+\ldots+|x_n|^p)^{1/p} p \in [1,\infty] || \cdot ||_\infty || \cdot ||_p p\to \infty p=p^*(K) GL(V) || \cdot ||_{p^*(K)} p^*(K) [\bar{K}:K] K {}^{(\dagger)} K \mathbb{R} \mathbb{C} \mathbb{Q}_P \mathbb{F}_P((t)) P K=\mathbb{R} |\cdot| || \cdot ||_2 O(n) [\bar{\mathbb{R}}:\mathbb{R}]=[\mathbb{C}:\mathbb{R}]=2 K=\mathbb{C} |x|=\bar{x} x || \cdot ||_1 U(n) {}^{(\ddagger)} [\bar{\mathbb{C}}:\mathbb{C}]=[\mathbb{C}:\mathbb{C}]=1 |\cdot| ||\cdot|| K |\cdot| \chi \chi || \cdot ||_\infty GL(\mathcal{O}_K^n) \mathcal{O}_K K K [\bar{K}:K]=\infty p^*(K) = [\bar{K}:K] K 3 |a|^3+|b|^3=|c|^3 K | \cdot | K ||\cdot||_p | \cdot | p = [\bar{K}:K] p^*(K) = 1 K ||\cdot||_1 p^*(K) = [L:K]\: p^*(L) L/K [L:K]=n L p p L \simeq K^n K k\in K  K \subseteq L |k|_L = \frac{\mu_L(k S^n)}{\mu_L(S^n)} = \left(\frac{\mu_K(k S)}{\mu_K(S)}\right)^n = |k|_K^n. GL_1(L) = L^\times |\cdot|_L 1 x 1 y 1 x^{-1}y GL_1(L) \subseteq GL_n(K) K^n L \simeq K^n ||\cdot|| = |\cdot|_L^{1/n} ||kv||=|k|_L ||v|| |\cdot| GL_m(L) \subseteq GL_{mn}(K) ||\cdot|| L^m K^{mn} \{1=a_0, a_1, \ldots, a_{n-1}\} L |\sum_{i=0}^{n-1} k_i a_i|_L = \sum_{i=0}^{n-1} |k_i|_K^n L n p^*(K) = [L:K]\: p^*(L) K (v,w) V \oplus W ||(v,w)||^p_p = ||v||^p_p + ||w||^p_p p ||\cdot||_p p V g : V \to \mathbb{R}_{\ge 0} f \circ ||\cdot||_p : V \to \mathbb{R}_{\ge 0} \to \mathbb{R}_{\ge 0} f g( (v,w) ) = g(v) g(w). \exp(-k ||x||_2^2) \mathbb{R} \exp(-k' ||x||_1) \mathbb{C} k, k' >0 p=\infty \lim_{p\to \infty} \exp(-k'' ||x||_p^p) = \begin{cases} 1 & ||x||_p \le 1 \\ 0 & \text{otherwise} \end{cases} = \mathbf{1}_\mathcal{O_K^n}(x), \mathcal{O}_K^n V=K^n 1 V=K [\bar{K} : K] = p K \exp(-k ||x||_p^p) p=\infty (\dagger) [\bar{K}:K] K 1, 2, \infty 1, 2 \infty (\ddagger) n |x_1|+|x_2|+\ldots+|x_n| = 1 2n V(B_{2n})=V(B_{2})^n /n!=\pi^n /n! V(X_{n})=V(X_{1})^n /n!=2^n /n! n |x_1|+|x_2|+\ldots+|x_n| = 1 | \cdot |","['linear-algebra', 'number-theory', 'measure-theory', 'soft-question', 'local-field']"
49,Bounding the minimum singular value of a block triangular matrix,Bounding the minimum singular value of a block triangular matrix,,"Question: What is the sharpest known lower bound for the minimum singular value of the block triangular matrix $$M:=\begin{bmatrix} A & B \\ 0 & D \end{bmatrix}$$ in terms of the properties of its constituent matrices? Motivation: Block triangular matrices are ubiquitous in numerical linear algebra, and the minimum singular value is a basic property of any matrix. So, it would be useful to have a canonical answer to this question in an easily searchable place on the internet. A matrix of this form came up in my research on numerical methods, and I need to estimate its minimum singular value in order to understand the stability of a method I'm working on. My bound: I was able to come up with the bounds $$\boxed{\frac{1}{\sigma_\text{min}(M)} \le \sqrt{\left\Vert A^{-1}\right\Vert^2\left(1 + \left\Vert BD^{-1} \right\Vert^2 \right) + \left\Vert D^{-1} \right\Vert^2}}$$ and $$\boxed{\frac{1}{\sigma_\text{min}(M)} \le \sqrt{\left\Vert D^{-1}\right\Vert^2\left(1 + \left\Vert A^{-1}B \right\Vert^2 \right) + \left\Vert A^{-1} \right\Vert^2}.}$$ using the following argument (for the first bound, the second one is the same argument but on the transpose): The minimum singular value of a matrix is the inverse of the norm of the inverse matrix: $$\frac{1}{\sigma_\text{min}(M)} = \left\Vert M^{-1} \right\Vert.$$ But inverse of a block triangular matrix has the following exact formula : $$M^{-1} = \begin{bmatrix} A^{-1} & -A^{-1}BD^{-1} \\ 0 & D^{-1} \end{bmatrix}.$$ Estimating the norm of the inverse directly from the definition and using the submultiplicative property of norms yields the boxed bound above: \begin{align*} \left\Vert M^{-1} \right\Vert^2 &= \sup_{||u||^2+||v||^2=1} \left\Vert \begin{bmatrix} A^{-1} & -A^{-1}BD^{-1} \\ 0 & D^{-1} \end{bmatrix} \begin{bmatrix}u \\ v\end{bmatrix} \right\Vert^2 \\ &= \sup_{||u||^2+||v||^2=1} \left\Vert A^{-1} u - A^{-1}BD^{-1}v \right\Vert^2 + \left\Vert D^{-1} v\right\Vert^2 \\ &\le \left\Vert A^{-1}\right\Vert^2 \left\Vert\begin{bmatrix}I & -BD^{-1}\end{bmatrix}\right\Vert^2 + \left\Vert D^{-1} \right\Vert^2 \\ &= \left\Vert A^{-1}\right\Vert^2\left(1 + \left\Vert BD^{-1} \right\Vert^2 \right) + \left\Vert D^{-1} \right\Vert^2. \end{align*} Conjectured bound: The obvious conjecture based on the two bounds above is the following symmetrized version: $$\frac{1}{\sigma_\text{min}(M)} \overset{?}{\le} \sqrt{\left\Vert D^{-1} \right\Vert^2 + \left\Vert A^{-1}BD^{-1} \right\Vert ^2 + \left\Vert A^{-1} \right\Vert ^2},$$ which I have been unable to prove. If one applies the triangle inequality to the blocks of $M^{-1}$, one gets the similar looking bound: $$\frac{1}{\sigma_\text{min}(M)} \le \left\Vert D^{-1} \right\Vert + \left\Vert A^{-1}BD^{-1} \right\Vert + \left\Vert A^{-1} \right\Vert,$$ but this is strictly worse than the conjectured bound since it is a sum rather than the square root of a sum of squares. Edit: After getting no answers here, I posted on mathoverflow , where it was answered. The conjectured bound is true.","Question: What is the sharpest known lower bound for the minimum singular value of the block triangular matrix $$M:=\begin{bmatrix} A & B \\ 0 & D \end{bmatrix}$$ in terms of the properties of its constituent matrices? Motivation: Block triangular matrices are ubiquitous in numerical linear algebra, and the minimum singular value is a basic property of any matrix. So, it would be useful to have a canonical answer to this question in an easily searchable place on the internet. A matrix of this form came up in my research on numerical methods, and I need to estimate its minimum singular value in order to understand the stability of a method I'm working on. My bound: I was able to come up with the bounds $$\boxed{\frac{1}{\sigma_\text{min}(M)} \le \sqrt{\left\Vert A^{-1}\right\Vert^2\left(1 + \left\Vert BD^{-1} \right\Vert^2 \right) + \left\Vert D^{-1} \right\Vert^2}}$$ and $$\boxed{\frac{1}{\sigma_\text{min}(M)} \le \sqrt{\left\Vert D^{-1}\right\Vert^2\left(1 + \left\Vert A^{-1}B \right\Vert^2 \right) + \left\Vert A^{-1} \right\Vert^2}.}$$ using the following argument (for the first bound, the second one is the same argument but on the transpose): The minimum singular value of a matrix is the inverse of the norm of the inverse matrix: $$\frac{1}{\sigma_\text{min}(M)} = \left\Vert M^{-1} \right\Vert.$$ But inverse of a block triangular matrix has the following exact formula : $$M^{-1} = \begin{bmatrix} A^{-1} & -A^{-1}BD^{-1} \\ 0 & D^{-1} \end{bmatrix}.$$ Estimating the norm of the inverse directly from the definition and using the submultiplicative property of norms yields the boxed bound above: \begin{align*} \left\Vert M^{-1} \right\Vert^2 &= \sup_{||u||^2+||v||^2=1} \left\Vert \begin{bmatrix} A^{-1} & -A^{-1}BD^{-1} \\ 0 & D^{-1} \end{bmatrix} \begin{bmatrix}u \\ v\end{bmatrix} \right\Vert^2 \\ &= \sup_{||u||^2+||v||^2=1} \left\Vert A^{-1} u - A^{-1}BD^{-1}v \right\Vert^2 + \left\Vert D^{-1} v\right\Vert^2 \\ &\le \left\Vert A^{-1}\right\Vert^2 \left\Vert\begin{bmatrix}I & -BD^{-1}\end{bmatrix}\right\Vert^2 + \left\Vert D^{-1} \right\Vert^2 \\ &= \left\Vert A^{-1}\right\Vert^2\left(1 + \left\Vert BD^{-1} \right\Vert^2 \right) + \left\Vert D^{-1} \right\Vert^2. \end{align*} Conjectured bound: The obvious conjecture based on the two bounds above is the following symmetrized version: $$\frac{1}{\sigma_\text{min}(M)} \overset{?}{\le} \sqrt{\left\Vert D^{-1} \right\Vert^2 + \left\Vert A^{-1}BD^{-1} \right\Vert ^2 + \left\Vert A^{-1} \right\Vert ^2},$$ which I have been unable to prove. If one applies the triangle inequality to the blocks of $M^{-1}$, one gets the similar looking bound: $$\frac{1}{\sigma_\text{min}(M)} \le \left\Vert D^{-1} \right\Vert + \left\Vert A^{-1}BD^{-1} \right\Vert + \left\Vert A^{-1} \right\Vert,$$ but this is strictly worse than the conjectured bound since it is a sum rather than the square root of a sum of squares. Edit: After getting no answers here, I posted on mathoverflow , where it was answered. The conjectured bound is true.",,"['linear-algebra', 'matrices', 'block-matrices', 'singular-values']"
50,Matrix diagonalization theorems and counterexamples: reference-request.,Matrix diagonalization theorems and counterexamples: reference-request.,,"I'm looking for exhaustive list of diagonalization theorems and counterexamples in linear algebra. In this question I understand the question of matrix diagonalization very broadly: suppose we have some group $G$ of matrices, where $G$ is one of $\operatorname{Gl}(n,\mathbb{R})$ - real invertible matrices, $\operatorname{Gl}(n,\mathbb{C})$ - complex invertible matrices, $\operatorname{O}(n)$ - orthogonal matrices, $\operatorname{SO}(n)$ - special orthogonal matrices, $\operatorname{U}(n)$, $\operatorname{SU}(n)$ - unitary and special unitary matrices, generalization of 3-5: matrices with real or complex coefficients, satisfying either of $M C M^{-1} = C$ or $M C M^* = C$ for some fixed matrix $C$. This includes e.g. Lorentz group, group of symplectic matrices. Also we have some set $S$ of matrices we want to diagonalize, where $S$ is one of $M^n(\mathbb{R})$ - all real valued matrices, $M^n(\mathbb{C})$ - all complex-valued matrices, self-adjoint matrices over $\mathbb{R}$ or $\mathbb{C}$, anti-symmetric matrices over $\mathbb{R}$, normal matrices over $\mathbb{R}$ or $\mathbb{C}$. I'm interested in the orbits of action of $G$ on $S$, where action is one of $s \mapsto gsg^{-1}$ ($s$ is interpreted as an endomorphism) and $s\mapsto gsg^*$ ($s$ is interpreted as a bilinear form): what is the ""nicest"" form of matrix, which we can find on any orbit? May be its not exactly diagonal, but Jordan, or made from $2\times 2$ blocks on the diagonal - all of these are examples of ""nice"" forms. The typical questions I would ask are Is it true, that for a real matrix $m$, its diagonizability by $\operatorname{Gl}(n,\mathbb{C})$ (with the obtained diagonal matrix being real) implies the diagonizability by $\operatorname{Gl}(n,\mathbb{R})$ (as an endomorphism)? (Yes) Is every (real) symmetric matrix diagonizable as a bilinear form by Lorentz transformations (in $\mathbb{R}^4$)? Can every real antisymmetric form be transformed by the same Lorentz group to a matrix with blocks of the form $\left(\begin{smallmatrix} 0 & -\lambda \\ \lambda & 0 \end{smallmatrix}\right)$ on the diagonal? Can any real matrix $m$ be transformed to a matrix with $1\times1$ and $2\times 2$ blocks on the diagonal by $SO(n)$? ( No ) What if you are allowed to have Jordan-like form, i.e. whenever two blocks on the diagonal are the same, one can have identity matrix on the diagonal next to it? (Still no by the dimension counting argument) Ok, what is the simplest shape one can hope for? Note that two transformation types coincide for $SO(n)$, $O(n)$, $U(n)$, $SU(n)$, so I don't need to specify one in the question #3. Is there any extensive reference I can look up all these questions and future similar questions?","I'm looking for exhaustive list of diagonalization theorems and counterexamples in linear algebra. In this question I understand the question of matrix diagonalization very broadly: suppose we have some group $G$ of matrices, where $G$ is one of $\operatorname{Gl}(n,\mathbb{R})$ - real invertible matrices, $\operatorname{Gl}(n,\mathbb{C})$ - complex invertible matrices, $\operatorname{O}(n)$ - orthogonal matrices, $\operatorname{SO}(n)$ - special orthogonal matrices, $\operatorname{U}(n)$, $\operatorname{SU}(n)$ - unitary and special unitary matrices, generalization of 3-5: matrices with real or complex coefficients, satisfying either of $M C M^{-1} = C$ or $M C M^* = C$ for some fixed matrix $C$. This includes e.g. Lorentz group, group of symplectic matrices. Also we have some set $S$ of matrices we want to diagonalize, where $S$ is one of $M^n(\mathbb{R})$ - all real valued matrices, $M^n(\mathbb{C})$ - all complex-valued matrices, self-adjoint matrices over $\mathbb{R}$ or $\mathbb{C}$, anti-symmetric matrices over $\mathbb{R}$, normal matrices over $\mathbb{R}$ or $\mathbb{C}$. I'm interested in the orbits of action of $G$ on $S$, where action is one of $s \mapsto gsg^{-1}$ ($s$ is interpreted as an endomorphism) and $s\mapsto gsg^*$ ($s$ is interpreted as a bilinear form): what is the ""nicest"" form of matrix, which we can find on any orbit? May be its not exactly diagonal, but Jordan, or made from $2\times 2$ blocks on the diagonal - all of these are examples of ""nice"" forms. The typical questions I would ask are Is it true, that for a real matrix $m$, its diagonizability by $\operatorname{Gl}(n,\mathbb{C})$ (with the obtained diagonal matrix being real) implies the diagonizability by $\operatorname{Gl}(n,\mathbb{R})$ (as an endomorphism)? (Yes) Is every (real) symmetric matrix diagonizable as a bilinear form by Lorentz transformations (in $\mathbb{R}^4$)? Can every real antisymmetric form be transformed by the same Lorentz group to a matrix with blocks of the form $\left(\begin{smallmatrix} 0 & -\lambda \\ \lambda & 0 \end{smallmatrix}\right)$ on the diagonal? Can any real matrix $m$ be transformed to a matrix with $1\times1$ and $2\times 2$ blocks on the diagonal by $SO(n)$? ( No ) What if you are allowed to have Jordan-like form, i.e. whenever two blocks on the diagonal are the same, one can have identity matrix on the diagonal next to it? (Still no by the dimension counting argument) Ok, what is the simplest shape one can hope for? Note that two transformation types coincide for $SO(n)$, $O(n)$, $U(n)$, $SU(n)$, so I don't need to specify one in the question #3. Is there any extensive reference I can look up all these questions and future similar questions?",,"['linear-algebra', 'matrices', 'reference-request', 'block-matrices', 'diagonalization']"
51,I need to calculate $x^{50}$ [duplicate],I need to calculate  [duplicate],x^{50},"This question already has answers here : for a $3 \times 3$ matrix A ,value of $ A^{50} $ is (3 answers) Closed 10 years ago . $x=\begin{pmatrix}1&0&0\\1&0&1\\0&1&0\end{pmatrix}$, I need to calculate $x^{50}$ Could anyone tell me how to proceed? Thank you.","This question already has answers here : for a $3 \times 3$ matrix A ,value of $ A^{50} $ is (3 answers) Closed 10 years ago . $x=\begin{pmatrix}1&0&0\\1&0&1\\0&1&0\end{pmatrix}$, I need to calculate $x^{50}$ Could anyone tell me how to proceed? Thank you.",,"['linear-algebra', 'matrices']"
52,Do matrices really rotate and stretch vectors or is that definition incorrect?,Do matrices really rotate and stretch vectors or is that definition incorrect?,,"I come from the applied math and statistics world, but I was talking to my friend who comes from the pure math and number theory world—in particular Galois representation theory. I mentioned something about the confusing definition of ""matrices"" in textbooks. Many textbooks talk about a matrix as the solution to a linear system of equations, or other abstract descriptions. The definition that I have always found useful, was the sense that matrices rotate and scale vectors through linear transformations. Now, coming from the pure math side, my friend said that this definition was not accurate. I am trying to paraphrase some of his comments, but he said that in higher dimensions, matrices can stretch and rotate vectors only locally . He also said that it depends on what vectors the matrix acts upon. His response threw me for a loop and I was trying to understand how to resolve his statements. First, is my understanding of matrices incorrect? It is fine if this idea of rotating and stretching a vector is incorrect, but I am not sure what the alternative definition of a matrix would be. Like when my friend says that a matrix may only stretch/rotate a vector locally, I am trying to think of a practical example of this kind of phenomena. I will follow up with my friend on what he means, but I was hoping someone could set me straight on how I should think of a matrix, or the definition of a matrix.","I come from the applied math and statistics world, but I was talking to my friend who comes from the pure math and number theory world—in particular Galois representation theory. I mentioned something about the confusing definition of ""matrices"" in textbooks. Many textbooks talk about a matrix as the solution to a linear system of equations, or other abstract descriptions. The definition that I have always found useful, was the sense that matrices rotate and scale vectors through linear transformations. Now, coming from the pure math side, my friend said that this definition was not accurate. I am trying to paraphrase some of his comments, but he said that in higher dimensions, matrices can stretch and rotate vectors only locally . He also said that it depends on what vectors the matrix acts upon. His response threw me for a loop and I was trying to understand how to resolve his statements. First, is my understanding of matrices incorrect? It is fine if this idea of rotating and stretching a vector is incorrect, but I am not sure what the alternative definition of a matrix would be. Like when my friend says that a matrix may only stretch/rotate a vector locally, I am trying to think of a practical example of this kind of phenomena. I will follow up with my friend on what he means, but I was hoping someone could set me straight on how I should think of a matrix, or the definition of a matrix.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'representation-theory']"
53,Prove that $A+I$ is invertible if $A$ is nilpotent [duplicate],Prove that  is invertible if  is nilpotent [duplicate],A+I A,This question already has answers here : Closed 12 years ago . Possible Duplicate: Units and Nilpotents Given $A^{2012}=0$ prove that $A+I$ is invertible and find an expression for $(A+I)^{-1}$ in terms of $A$. ($I$ is the identity matrix).,This question already has answers here : Closed 12 years ago . Possible Duplicate: Units and Nilpotents Given $A^{2012}=0$ prove that $A+I$ is invertible and find an expression for $(A+I)^{-1}$ in terms of $A$. ($I$ is the identity matrix).,,"['linear-algebra', 'matrices', 'nilpotence', 'faq']"
54,Simple linear algebra problem: prove a matrix is invertible,Simple linear algebra problem: prove a matrix is invertible,,"I'm preparing for a test in linear algebra and I've come across a problem I'm having trouble with for some reason: Given a square matrix A, $A^2=2I$, prove that $A-I$ is invertible. I know this is pretty simple but I can't seem to play with the equations to get it so that for some $B$, $B(A-I)=I$ It's pretty easy to see that $A^{-1}=\frac{1}{2}A$, but beyond that I haven't been able to get very far. Can anyone help with this?","I'm preparing for a test in linear algebra and I've come across a problem I'm having trouble with for some reason: Given a square matrix A, $A^2=2I$, prove that $A-I$ is invertible. I know this is pretty simple but I can't seem to play with the equations to get it so that for some $B$, $B(A-I)=I$ It's pretty easy to see that $A^{-1}=\frac{1}{2}A$, but beyond that I haven't been able to get very far. Can anyone help with this?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'ring-theory']"
55,How to prove a matrix is nilpotent?,How to prove a matrix is nilpotent?,,"Given an $n\times n$ upper triangular matrix $A$ with zero on main diagonal, show that $A^n = 0$. I did some matrix operation and noticed that the diagonal moves up, ultimately all entries will be zero. Is there a nicer way to do it?","Given an $n\times n$ upper triangular matrix $A$ with zero on main diagonal, show that $A^n = 0$. I did some matrix operation and noticed that the diagonal moves up, ultimately all entries will be zero. Is there a nicer way to do it?",,['linear-algebra']
56,Why is the exponential function not in the subspace of all polynomials?,Why is the exponential function not in the subspace of all polynomials?,,"The exponential function can be written as $$e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \dots.$$ The subspace of all polynomials is $$\text{span}\{1, x,x^2, \dots \}$$ Sure $e^x$ is in this set?","The exponential function can be written as $$e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \dots.$$ The subspace of all polynomials is $$\text{span}\{1, x,x^2, \dots \}$$ Sure $e^x$ is in this set?",,"['linear-algebra', 'functional-analysis', 'polynomials', 'vector-spaces']"
57,Symmetric matrix multiplication,Symmetric matrix multiplication,,"Let $A$ and $B$ be symmetric matrices. Prove: $AB=BA$ $AB$ is a symmetric matrix As for 1. due to the axiom $(AB)^T=B^T A^T$ so $AB=BA$ As for 2. I did not find any axiom that can support the claim, but from test I found that it is true for symmetric matrices when the entries on the diagonal are equal.","Let $A$ and $B$ be symmetric matrices. Prove: $AB=BA$ $AB$ is a symmetric matrix As for 1. due to the axiom $(AB)^T=B^T A^T$ so $AB=BA$ As for 2. I did not find any axiom that can support the claim, but from test I found that it is true for symmetric matrices when the entries on the diagonal are equal.",,['linear-algebra']
58,"$A$ is normal and nilpotent, show $A=0$","is normal and nilpotent, show",A A=0,"Given a matrix $A \in R^{n \times n}$ which is normal ( $AA^H=A^HA$ where $A^H$ is hermitian of $A$ ) and nilpotent ( $A^k=0$ for some $k$ ). Now we need to show that $A=0$ . (This is essentially exercise 5(b) in sec. 80 on p.162 of Paul R. Halmos' Finite-Dimensional Vector Spaces .) I tried to show in the following way, we know that, $AA^H=A^HA$ pre-multiply by $A^{k-1}\implies A^kA^H=A^{k-1}A^HA$ Now, we have $0 = A^{k-1}A^HA$ , since $A$ nilpotent. I am not sure how to proceed from here to show $A=0$ . Can someone help me in this problem?","Given a matrix which is normal ( where is hermitian of ) and nilpotent ( for some ). Now we need to show that . (This is essentially exercise 5(b) in sec. 80 on p.162 of Paul R. Halmos' Finite-Dimensional Vector Spaces .) I tried to show in the following way, we know that, pre-multiply by Now, we have , since nilpotent. I am not sure how to proceed from here to show . Can someone help me in this problem?",A \in R^{n \times n} AA^H=A^HA A^H A A^k=0 k A=0 AA^H=A^HA A^{k-1}\implies A^kA^H=A^{k-1}A^HA 0 = A^{k-1}A^HA A A=0,"['linear-algebra', 'matrices', 'nilpotence']"
59,Are inverse matrices unique?,Are inverse matrices unique?,,"Does a matrix have only one inverse matrix (like the inverse of an element in a field)? If so, does this mean that $A,B \text{ have the same inverse matrix} \iff A=B$?","Does a matrix have only one inverse matrix (like the inverse of an element in a field)? If so, does this mean that $A,B \text{ have the same inverse matrix} \iff A=B$?",,"['linear-algebra', 'matrices']"
60,Eigenvectors and ''eigenrows'',Eigenvectors and ''eigenrows'',,"Usually we search the eigenvectors of a matrix $M$ as the vectors that span a subspace that is invariant by left multiplication by the matrix: $M\vec x= \lambda \vec x$. If we take the transpose problem $\mathbf x M=\lambda \mathbf x$, where $\mathbf x$ is a row vector, we see that the eigenvalues are the same, but the ''eigenrows'' are not the transpose of the eigenvector (in general). E.g., for the matrix  $$\left[ \matrix{0&1\\2&1} \right] $$ we find, for the eigenvalue $\lambda=2$, $$ \begin{bmatrix} 0&1\\2&1 \end{bmatrix} \left[ \matrix{1\\2} \right]= 2\left[ \matrix{1\\2} \right] $$ $$\left[ \matrix{1&1} \right] \begin{bmatrix} 0&1\\2&1 \end{bmatrix} = 2\left[ \matrix{1&1} \right] \;\;. $$ So my questions are: Is there some relation between these ''right'' and ''left'' eigenspaces of the same matrix? Is there some reason why the ''eigenrows'' are not so studied as the eigenvectors?","Usually we search the eigenvectors of a matrix $M$ as the vectors that span a subspace that is invariant by left multiplication by the matrix: $M\vec x= \lambda \vec x$. If we take the transpose problem $\mathbf x M=\lambda \mathbf x$, where $\mathbf x$ is a row vector, we see that the eigenvalues are the same, but the ''eigenrows'' are not the transpose of the eigenvector (in general). E.g., for the matrix  $$\left[ \matrix{0&1\\2&1} \right] $$ we find, for the eigenvalue $\lambda=2$, $$ \begin{bmatrix} 0&1\\2&1 \end{bmatrix} \left[ \matrix{1\\2} \right]= 2\left[ \matrix{1\\2} \right] $$ $$\left[ \matrix{1&1} \right] \begin{bmatrix} 0&1\\2&1 \end{bmatrix} = 2\left[ \matrix{1&1} \right] \;\;. $$ So my questions are: Is there some relation between these ''right'' and ''left'' eigenspaces of the same matrix? Is there some reason why the ''eigenrows'' are not so studied as the eigenvectors?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
61,How to solve this recurrence relation? $f_n = 3f_{n-1} + 12(-1)^n$,How to solve this recurrence relation?,f_n = 3f_{n-1} + 12(-1)^n,"How to solve this particular recurrence relation  ? $$f_n = 3f_{n-1} + 12(-1)^n,\quad f_1 = 0$$ such that $f_2 = 12, f_3 = 24$ and so on. I tried out a lot but due to $(-1)^n$ I am not able to solve this recurrence? Any help will be highly appreciated. Please this is no homework. I came across this while solving a problem on SPOJ","How to solve this particular recurrence relation  ? $$f_n = 3f_{n-1} + 12(-1)^n,\quad f_1 = 0$$ such that $f_2 = 12, f_3 = 24$ and so on. I tried out a lot but due to $(-1)^n$ I am not able to solve this recurrence? Any help will be highly appreciated. Please this is no homework. I came across this while solving a problem on SPOJ",,"['linear-algebra', 'recurrence-relations', 'recursive-algorithms']"
62,Prerequisites/Books for A First Course in Linear Algebra,Prerequisites/Books for A First Course in Linear Algebra,,"What mathematical knowledge do I need to begin studying linear algebra? In particular, how much calculus do I need to know? Also, do you have a favorite linear algebra book you can recommend?","What mathematical knowledge do I need to begin studying linear algebra? In particular, how much calculus do I need to know? Also, do you have a favorite linear algebra book you can recommend?",,"['linear-algebra', 'reference-request', 'big-list', 'book-recommendation']"
63,Formulas for the (top) coefficients of the characteristic polynomial of a matrix,Formulas for the (top) coefficients of the characteristic polynomial of a matrix,,"The characteristic polynomial of a matrix $A$ is defined as: $$\chi(A) = \det(xI-A) = \sum_{i=0}^n (-1)^i\cdot \operatorname{tr}^{(i)}(A) \cdot x^{n-i}$$ The trace is the sum of the eigenvalues of a matrix, tr(A) = tr (1) (A).  It is also the sum of the diagonal entries: $$\operatorname{tr}(A) = \sum_{i=1}^n A_{ii}$$ The sum of the products of pairs of eigenvalues is like the next trace. Is this formula valid? $$\operatorname{tr}^{(2)}(A) = \sum_{1 \leq i < j \leq n } A_{ii} A_{jj} - A_{ij} A_{ji}$$ What about this one? $$\operatorname{tr}^{(2)}(A) = \tfrac12(\operatorname{tr}(A)^2 - \operatorname{tr}(A^2))$$ Are there corresponding formulas for the next one, tr (3) ?","The characteristic polynomial of a matrix is defined as: The trace is the sum of the eigenvalues of a matrix, tr(A) = tr (1) (A).  It is also the sum of the diagonal entries: The sum of the products of pairs of eigenvalues is like the next trace. Is this formula valid? What about this one? Are there corresponding formulas for the next one, tr (3) ?",A \chi(A) = \det(xI-A) = \sum_{i=0}^n (-1)^i\cdot \operatorname{tr}^{(i)}(A) \cdot x^{n-i} \operatorname{tr}(A) = \sum_{i=1}^n A_{ii} \operatorname{tr}^{(2)}(A) = \sum_{1 \leq i < j \leq n } A_{ii} A_{jj} - A_{ij} A_{ji} \operatorname{tr}^{(2)}(A) = \tfrac12(\operatorname{tr}(A)^2 - \operatorname{tr}(A^2)),"['linear-algebra', 'matrices', 'determinant']"
64,Does SO(3) preserve the cross product?,Does SO(3) preserve the cross product?,,"Let $g\in SO(3)$ and $p,q\in\mathbb{R}^3$ . I wondered whether it is true that $$g(p\times q)=gp\times gq$$ I am not sure how to prove this. I guess I will use at some point that the last row $g_3$ of $g$ can be obtained by $g_3=g_1\times g_2$ . But I assume there is an easier proof than writing everything out.",Let and . I wondered whether it is true that I am not sure how to prove this. I guess I will use at some point that the last row of can be obtained by . But I assume there is an easier proof than writing everything out.,"g\in SO(3) p,q\in\mathbb{R}^3 g(p\times q)=gp\times gq g_3 g g_3=g_1\times g_2","['linear-algebra', 'cross-product', 'orthogonal-matrices', 'linear-groups']"
65,How to prove that complex numbers $(\mathbb{C})$ are linear space over real numbers $(\mathbb{R})$ field?,How to prove that complex numbers  are linear space over real numbers  field?,(\mathbb{C}) (\mathbb{R}),How to prove that complex numbers $\mathbb{C}$ are linear space over real numbers field $\mathbb{R}$ ?,How to prove that complex numbers are linear space over real numbers field ?,\mathbb{C} \mathbb{R},"['linear-algebra', 'vector-spaces']"
66,Prove that $A^k = 0 $ iff $A^2 = 0$,Prove that  iff,A^k = 0  A^2 = 0,Let $A$ be a $  2 \times  2 $ matrix and  a positive integer $k \geq 2$. Prove that $A^k = 0 $ iff $A^2 = 0$. I can make it to do this exercise if I have $ \det (A^k) = (\det A)^k $. But this question comes before this. Thank you very much for your help!,Let $A$ be a $  2 \times  2 $ matrix and  a positive integer $k \geq 2$. Prove that $A^k = 0 $ iff $A^2 = 0$. I can make it to do this exercise if I have $ \det (A^k) = (\det A)^k $. But this question comes before this. Thank you very much for your help!,,"['linear-algebra', 'matrices']"
67,What is orthogonal transformation?,What is orthogonal transformation?,,"When $A^{T}A = AA^{T} = I$ , I am told it is an orthogonal transformation $A$ . But don't really get what it means. Hope to hear some explanations. $\begin{bmatrix}\cos\theta & \sin\theta \\ -\sin\theta & \cos\theta\end{bmatrix} \begin{bmatrix}\cos\theta & -\sin\theta \\ \sin\theta & \cos\theta\end{bmatrix} = \begin{bmatrix}1 & 0 \\ 0 & 1\end{bmatrix}$","When , I am told it is an orthogonal transformation . But don't really get what it means. Hope to hear some explanations.",A^{T}A = AA^{T} = I A \begin{bmatrix}\cos\theta & \sin\theta \\ -\sin\theta & \cos\theta\end{bmatrix} \begin{bmatrix}\cos\theta & -\sin\theta \\ \sin\theta & \cos\theta\end{bmatrix} = \begin{bmatrix}1 & 0 \\ 0 & 1\end{bmatrix},"['matrices', 'linear-algebra']"
68,Are all Vectors of a Basis Orthogonal?,Are all Vectors of a Basis Orthogonal?,,"Assuming we have a basis for a set $\mathbb{R}^n$, would any set of linearly independent vectors that form a basis for $\mathbb{R}^n$ also be orthogonal to each other? Take the trivial case of $(1,0)$ and $(0,1)$. Now any set of linear independent vectors would be a scalar multiple of these two vectors that form a Basis for $\mathbb{R}^2$ hence they have to be orthogonal. Right?","Assuming we have a basis for a set $\mathbb{R}^n$, would any set of linearly independent vectors that form a basis for $\mathbb{R}^n$ also be orthogonal to each other? Take the trivial case of $(1,0)$ and $(0,1)$. Now any set of linear independent vectors would be a scalar multiple of these two vectors that form a Basis for $\mathbb{R}^2$ hence they have to be orthogonal. Right?",,"['linear-algebra', 'matrices']"
69,Finding the shortest distance between two lines,Finding the shortest distance between two lines,,"I know how to find the distance between a point and a line, not between two lines. Find the shortest distance between the lines $(-1,1,4) + t(1,1,-1)$ and $(5,3,-3) + s(-2,0,1)$ Any help would be appreciated.","I know how to find the distance between a point and a line, not between two lines. Find the shortest distance between the lines $(-1,1,4) + t(1,1,-1)$ and $(5,3,-3) + s(-2,0,1)$ Any help would be appreciated.",,['linear-algebra']
70,"When will matrix multiplication become just ""concatenation""?","When will matrix multiplication become just ""concatenation""?",,"Saw below entertaining matrix multiplication examples. Obviously they are just coincidences. But I am curious when does below hold? \begin{equation} \begin{pmatrix} a_1 & b_1 \\ c_1 & d_1  \end{pmatrix} \times \begin{pmatrix} a_2 & b_2 \\ c_2 & d_2  \end{pmatrix} = \begin{pmatrix} \overline{a_1 a_2} & \overline{b_1 b_2} \\ \overline{c_1 c_2} & \overline{d_1 d_2} \end{pmatrix} \end{equation} here $\overline{a_1 a_2}$ means gluing the integers together, not multiplications.. assuming we are working on integer matrices.","Saw below entertaining matrix multiplication examples. Obviously they are just coincidences. But I am curious when does below hold? here means gluing the integers together, not multiplications.. assuming we are working on integer matrices.","\begin{equation}
\begin{pmatrix}
a_1 & b_1 \\
c_1 & d_1 
\end{pmatrix} \times \begin{pmatrix}
a_2 & b_2 \\
c_2 & d_2 
\end{pmatrix} = \begin{pmatrix}
\overline{a_1 a_2} & \overline{b_1 b_2} \\
\overline{c_1 c_2} & \overline{d_1 d_2}
\end{pmatrix}
\end{equation} \overline{a_1 a_2}","['linear-algebra', 'matrices', 'number-theory']"
71,Why do similar matrices have the same rank?,Why do similar matrices have the same rank?,,"I have seen some proofs on the Internet, which make use of the transformation map. However, I couldn't understand the methods since what I learned about the transformation map is so superficial. Can you use a simple way to explain it? And for now, just restrict the similar matrices to square matrices.","I have seen some proofs on the Internet, which make use of the transformation map. However, I couldn't understand the methods since what I learned about the transformation map is so superficial. Can you use a simple way to explain it? And for now, just restrict the similar matrices to square matrices.",,"['linear-algebra', 'matrices', 'matrix-rank', 'similar-matrices']"
72,Does $\mathbb{R}^\mathbb{R}$ have a basis?,Does  have a basis?,\mathbb{R}^\mathbb{R},"I'm studying linear algebra on my own and saw basis for $\mathbb{R}, \mathbb{R}[X], ...$ but there is no example of $\mathbb{R}^\mathbb{R}$ (even though it is used for many examples). What is a basis for it? Thank you","I'm studying linear algebra on my own and saw basis for $\mathbb{R}, \mathbb{R}[X], ...$ but there is no example of $\mathbb{R}^\mathbb{R}$ (even though it is used for many examples). What is a basis for it? Thank you",,[]
73,Why do determinants have their particular form?,Why do determinants have their particular form?,,"I know that for a matrix $A$, if $\det(A)=0$ then the matrix does not have an inverse, and hence the associated system of equations does not have a unique solution. However, why do the determinant formulas have the form they do? Why all the complicated co-factor expansions and alternating signs ? To sum it up: I know what determinants do, but its unclear to me why. Is there an intuitive explanation that can be attached to a co-factor expansion??..","I know that for a matrix $A$, if $\det(A)=0$ then the matrix does not have an inverse, and hence the associated system of equations does not have a unique solution. However, why do the determinant formulas have the form they do? Why all the complicated co-factor expansions and alternating signs ? To sum it up: I know what determinants do, but its unclear to me why. Is there an intuitive explanation that can be attached to a co-factor expansion??..",,['linear-algebra']
74,Is it true that every element of $V \otimes W$ is a simple tensor $v \otimes w$?,Is it true that every element of  is a simple tensor ?,V \otimes W v \otimes w,"I know that every vector in a tensor product $V \otimes W$ is a sum of simple tensors $v \otimes w$ with $v \in V$ and $w \in W$. In other words, any $u \in V \otimes W$ can be expressed in the form$$u = \sum_{i=1}^r v_i \otimes w_i$$for some vectors $v_i \in V$ and $w_i \in W$. This follows from the proof of the existence of $V \otimes W$, where one shows that $V \otimes W$ is spanned by the simple tensors $v \otimes w$; the assertion now follows from the fact that, in forming linear combinations, the scales can be absorbed in the vectors: $c(v \otimes w) = (cv) \otimes w = v\otimes (cw)$. My question is, is it true in general that every element of $V \otimes W$ is a simple tensor $v \otimes w$?","I know that every vector in a tensor product $V \otimes W$ is a sum of simple tensors $v \otimes w$ with $v \in V$ and $w \in W$. In other words, any $u \in V \otimes W$ can be expressed in the form$$u = \sum_{i=1}^r v_i \otimes w_i$$for some vectors $v_i \in V$ and $w_i \in W$. This follows from the proof of the existence of $V \otimes W$, where one shows that $V \otimes W$ is spanned by the simple tensors $v \otimes w$; the assertion now follows from the fact that, in forming linear combinations, the scales can be absorbed in the vectors: $c(v \otimes w) = (cv) \otimes w = v\otimes (cw)$. My question is, is it true in general that every element of $V \otimes W$ is a simple tensor $v \otimes w$?",,"['linear-algebra', 'abstract-algebra']"
75,Finding the rotation matrix in n-dimensions,Finding the rotation matrix in n-dimensions,,"Suppose that we know two real vectors with n components, which are linked by some arbitrary transformation/scaling/rotation/shearing... Now, I think that it is possible to know which is the scaling matrix and the rotation matrix. For example, the scaling matrix would be a diagonal matrix with n entries representing the n scaling factors. On the other side, I can normalize the two vectors and then compute the rotation matrix between the two, isn't it? 1) How can I retrieve the rotation matrix? (if it is possible, obviously) 2) I need such matrix in order to use it in a computational mechanics context. Then, how would you find it in an operative way? I have been reading somewhere that in multiple dimensions we can apply rotations that are combination of rotations in n-2 hyperplanes. Is this a convenient way to compute such matrix? Thank you very much!! EDIT: Let us put this way, in order to understand better what I should do... Vector $x= [ 2, 4, 5, 3, 6 ]^T$ and vector $y= [ 6, 2, 0, 1, 7 ]^T$. I would like to find the rotation matrix that aligns vector x to vector y. First of all, I understood that one need to find the base of the orthogonal complement to the two vectors, i.e. find the hyperplane containing such two vectors. Consequently, one first compute the null space: Reducing rows: $$A= \left[ \begin{array}{ccccc}  1 & 0 & -1/2 & -1/10 & 4/5 \\  0 & 1 & 3/2 & 4/5 & 11/10 \\ \end{array} \right] $$ Vectors of orthogonal space (linked to $x_3, x_4, x_5$) $$v_1^{\perp}= \left[ \begin{array}{c}  1/2 \\ -3/2 \\ 1 \\ 0 \\ 0 \\ \end{array} \right], v_2^{\perp}= \left[ \begin{array}{c}  1/10 \\ -4/5 \\ 0 \\ 1 \\ 0 \\ \end{array} \right], v_3^{\perp}= \left[ \begin{array}{c}  -4/5 \\ -11/10 \\ 0 \\ 0 \\ 1 \\ \end{array} \right] $$ Then one can do Gram-Schmidt over the two groups of vectors, the one representing the plane containing the two initial vectors and the one representing the orthogonal subspace. The resulting matrix presents in the column vectors the basis of the space: $$E= \left[ \begin{array}{ccccc}     0.7255 &  -0.0117 &   0.2673  & -0.0716 &  -0.6301 \\          0 &   0.4429 &  -0.8018  & -0.2409 &  -0.3209 \\    -0.3627 &   0.6701 &   0.5345  & -0.3255 &  -0.1663 \\    -0.0725 &   0.3555 &        0  &  0.9115 &  -0.1937 \\     0.5804 &   0.4778 &        0  &       0 &   0.6594 \\ \end{array} \right] $$ The last matrix is correct and represent a base because obviously the scalar product between two columns i,j is equal to the Kronecker delta $\delta_{i,j}$. Now, I have information about almost everything, I can compute the angle between the two vectors saying that $$ \cos(\theta)=\frac{x \cdot y}{\left|| x \right|| \: \left|| y \right||} $$ but now how do I construct the rotation matrix?","Suppose that we know two real vectors with n components, which are linked by some arbitrary transformation/scaling/rotation/shearing... Now, I think that it is possible to know which is the scaling matrix and the rotation matrix. For example, the scaling matrix would be a diagonal matrix with n entries representing the n scaling factors. On the other side, I can normalize the two vectors and then compute the rotation matrix between the two, isn't it? 1) How can I retrieve the rotation matrix? (if it is possible, obviously) 2) I need such matrix in order to use it in a computational mechanics context. Then, how would you find it in an operative way? I have been reading somewhere that in multiple dimensions we can apply rotations that are combination of rotations in n-2 hyperplanes. Is this a convenient way to compute such matrix? Thank you very much!! EDIT: Let us put this way, in order to understand better what I should do... Vector $x= [ 2, 4, 5, 3, 6 ]^T$ and vector $y= [ 6, 2, 0, 1, 7 ]^T$. I would like to find the rotation matrix that aligns vector x to vector y. First of all, I understood that one need to find the base of the orthogonal complement to the two vectors, i.e. find the hyperplane containing such two vectors. Consequently, one first compute the null space: Reducing rows: $$A= \left[ \begin{array}{ccccc}  1 & 0 & -1/2 & -1/10 & 4/5 \\  0 & 1 & 3/2 & 4/5 & 11/10 \\ \end{array} \right] $$ Vectors of orthogonal space (linked to $x_3, x_4, x_5$) $$v_1^{\perp}= \left[ \begin{array}{c}  1/2 \\ -3/2 \\ 1 \\ 0 \\ 0 \\ \end{array} \right], v_2^{\perp}= \left[ \begin{array}{c}  1/10 \\ -4/5 \\ 0 \\ 1 \\ 0 \\ \end{array} \right], v_3^{\perp}= \left[ \begin{array}{c}  -4/5 \\ -11/10 \\ 0 \\ 0 \\ 1 \\ \end{array} \right] $$ Then one can do Gram-Schmidt over the two groups of vectors, the one representing the plane containing the two initial vectors and the one representing the orthogonal subspace. The resulting matrix presents in the column vectors the basis of the space: $$E= \left[ \begin{array}{ccccc}     0.7255 &  -0.0117 &   0.2673  & -0.0716 &  -0.6301 \\          0 &   0.4429 &  -0.8018  & -0.2409 &  -0.3209 \\    -0.3627 &   0.6701 &   0.5345  & -0.3255 &  -0.1663 \\    -0.0725 &   0.3555 &        0  &  0.9115 &  -0.1937 \\     0.5804 &   0.4778 &        0  &       0 &   0.6594 \\ \end{array} \right] $$ The last matrix is correct and represent a base because obviously the scalar product between two columns i,j is equal to the Kronecker delta $\delta_{i,j}$. Now, I have information about almost everything, I can compute the angle between the two vectors saying that $$ \cos(\theta)=\frac{x \cdot y}{\left|| x \right|| \: \left|| y \right||} $$ but now how do I construct the rotation matrix?",,"['linear-algebra', 'rotations']"
76,Proof that trace of 'hat' matrix in linear regression is rank of X,Proof that trace of 'hat' matrix in linear regression is rank of X,,"I understand that the trace of the projection matrix (also known as the ""hat"" matrix) X*Inv(X'X)*X' in linear regression is equal to the rank of X.  How can we prove that from first principles, i.e. without simply asserting that the trace of a projection matrix always equals its rank? I am aware of the post Proving: ""The trace of an idempotent matrix equals the rank of the matrix"" , but need an integrated proof.","I understand that the trace of the projection matrix (also known as the ""hat"" matrix) X*Inv(X'X)*X' in linear regression is equal to the rank of X.  How can we prove that from first principles, i.e. without simply asserting that the trace of a projection matrix always equals its rank? I am aware of the post Proving: ""The trace of an idempotent matrix equals the rank of the matrix"" , but need an integrated proof.",,"['linear-algebra', 'matrices', 'statistics', 'linear-regression']"
77,Trouble understanding Sum of Subspaces,Trouble understanding Sum of Subspaces,,"I started reviewing linear algebra, from a different textbook (Axler's), after taking a fast paced summer class. Unfortunately, I've become confused with a concept that is introduced at the end of chapter one. That is, sum of subspaces. Axler's text defines the sum of subspaces as follows. Let $U_1,U_2,...,U_m$ be subspaces of a vectorspace $V$. Then we say $U_1+U_2+...+U_m=\{u_1+...+u_m:u_1\in U_1,...,u_m\in U_m\}$ I thought I understood this concept, but I'm afraid I don't because I am having trouble answering the following assertions he asks us to verify. First that if we let $U_1,U_2,...,U_m$ be subspaces of a vectorspace $V$, then the sum of those subspaces is a subspace of $V$. Also, this is what really had me tricked to thinking I understood it. Let $U=\{(x,0,0)\in \mathbb R^3: x\in \mathbb R\}$ and $W=\{(0,y,0)\in \mathbb R^3:y\in \mathbb R\}$, then $U+W= \{(x,y,0):x,y\in \mathbb R\}$. So this example made me think it was pretty straight forward and that I understood it, but in the next few lines he says let $Z= \{(y,y,0)\in \mathbb R^3:y\in\mathbb R\}$. Then $U+W=U+Z$ (which I am asked to verify). Could someone please help me understand the definition and the verifications? EDIT: I currently see the definition to say that when we take a collection of sets that are subspaces the sum of the sets is a set which consists of the sum  of all their elements. However when I say that it seems to me the summed set consists of just one elements (the total sum of all the elements). Thank You","I started reviewing linear algebra, from a different textbook (Axler's), after taking a fast paced summer class. Unfortunately, I've become confused with a concept that is introduced at the end of chapter one. That is, sum of subspaces. Axler's text defines the sum of subspaces as follows. Let $U_1,U_2,...,U_m$ be subspaces of a vectorspace $V$. Then we say $U_1+U_2+...+U_m=\{u_1+...+u_m:u_1\in U_1,...,u_m\in U_m\}$ I thought I understood this concept, but I'm afraid I don't because I am having trouble answering the following assertions he asks us to verify. First that if we let $U_1,U_2,...,U_m$ be subspaces of a vectorspace $V$, then the sum of those subspaces is a subspace of $V$. Also, this is what really had me tricked to thinking I understood it. Let $U=\{(x,0,0)\in \mathbb R^3: x\in \mathbb R\}$ and $W=\{(0,y,0)\in \mathbb R^3:y\in \mathbb R\}$, then $U+W= \{(x,y,0):x,y\in \mathbb R\}$. So this example made me think it was pretty straight forward and that I understood it, but in the next few lines he says let $Z= \{(y,y,0)\in \mathbb R^3:y\in\mathbb R\}$. Then $U+W=U+Z$ (which I am asked to verify). Could someone please help me understand the definition and the verifications? EDIT: I currently see the definition to say that when we take a collection of sets that are subspaces the sum of the sets is a set which consists of the sum  of all their elements. However when I say that it seems to me the summed set consists of just one elements (the total sum of all the elements). Thank You",,"['linear-algebra', 'vector-spaces']"
78,"Upper bound for the rank of a nilpotent matrix , if $A^2 \ne 0$","Upper bound for the rank of a nilpotent matrix , if",A^2 \ne 0,"I came across the fact that the rank of a $n \times n$ -matrix A with $A^2=0$ is at most $\frac{n}{2}$ . The easiest way to proof this is using the inequality $\operatorname{rank}(A)+\operatorname{rank}(B)-n\leqslant\operatorname{rank}(AB)$ . With $A=B$ and $A^2=0$ , this immediately yields $\operatorname{rank}(A)\leqslant\frac{n}{2}$ Can this result be generalized? What is the upper bound for the rank of a $n\times n$ -matrix $A$ with the property $A^k=0$ ? Additional question : Is the above bound for $k = 2$ sharp? In other words, is there  a $n \times n$ matrix $A$ with $\operatorname{rank}\left\lfloor\frac{n}{2}\right\rfloor$ and $A^2=0\;\forall n\in\mathbb N$ ?","I came across the fact that the rank of a -matrix A with is at most . The easiest way to proof this is using the inequality . With and , this immediately yields Can this result be generalized? What is the upper bound for the rank of a -matrix with the property ? Additional question : Is the above bound for sharp? In other words, is there  a matrix with and ?",n \times n A^2=0 \frac{n}{2} \operatorname{rank}(A)+\operatorname{rank}(B)-n\leqslant\operatorname{rank}(AB) A=B A^2=0 \operatorname{rank}(A)\leqslant\frac{n}{2} n\times n A A^k=0 k = 2 n \times n A \operatorname{rank}\left\lfloor\frac{n}{2}\right\rfloor A^2=0\;\forall n\in\mathbb N,"['linear-algebra', 'matrices', 'nilpotence']"
79,Set of Linear equation has no solution or unique solution or infinite solution?,Set of Linear equation has no solution or unique solution or infinite solution?,,For the system  $$ \left\{ \begin{array}{rcrcrcr}  x &+ &3y &-  &z &= &-4 \\ 4x &-  &y &+ &2z &= &3 \\ 2x &-  &y &- &3z &= &1 \end{array} \right.  $$ what is the condition to determine if there is no solution or unique solution or infinite solution? Thank you!,For the system  $$ \left\{ \begin{array}{rcrcrcr}  x &+ &3y &-  &z &= &-4 \\ 4x &-  &y &+ &2z &= &3 \\ 2x &-  &y &- &3z &= &1 \end{array} \right.  $$ what is the condition to determine if there is no solution or unique solution or infinite solution? Thank you!,,"['linear-algebra', 'determinant', 'systems-of-equations']"
80,"If matrix A is invertible, is it diagonalizable as well?","If matrix A is invertible, is it diagonalizable as well?",,"If a matrix A is invertible, then it is diagonalizable. Is it true or false?","If a matrix A is invertible, then it is diagonalizable. Is it true or false?",,"['linear-algebra', 'matrices', 'inverse', 'diagonalization']"
81,What does it mean for a matrix to be orthogonally diagonalizable?,What does it mean for a matrix to be orthogonally diagonalizable?,,"I'm a little confused as to when a matrix is orthogonally diagonalizable. I understand that if symmetric, it's always orthogonally diagonalizable, but in what other cases can you orthogonally diagonalize a matrix?","I'm a little confused as to when a matrix is orthogonally diagonalizable. I understand that if symmetric, it's always orthogonally diagonalizable, but in what other cases can you orthogonally diagonalize a matrix?",,['linear-algebra']
82,Is the set of non invertible matrices simply connected? What are their homotopy and homology groups?,Is the set of non invertible matrices simply connected? What are their homotopy and homology groups?,,"It is fairly easy to see that the set of non invertible matrices is path connected. Are they simply connected? If not what is their fundamental group? What are their homotopy and homology groups. I'm looking for the answer to any of these questions. Any examples for particular (non 0 or 1) dimensions are welcome as well, as well as for real or complex coefficients. (Sorry about the phrasing, it is currently 3:30 am and I will edit the question in the morning)","It is fairly easy to see that the set of non invertible matrices is path connected. Are they simply connected? If not what is their fundamental group? What are their homotopy and homology groups. I'm looking for the answer to any of these questions. Any examples for particular (non 0 or 1) dimensions are welcome as well, as well as for real or complex coefficients. (Sorry about the phrasing, it is currently 3:30 am and I will edit the question in the morning)",,['linear-algebra']
83,"Linear independence of functions: $x_1(t) = 3$, $x_2(t)=3\sin^2t$, $x_3(t)=4\cos^2t$","Linear independence of functions: , ,",x_1(t) = 3 x_2(t)=3\sin^2t x_3(t)=4\cos^2t,"I want to determine whether 3 functions are linearly independent: \begin{align*} x_1(t) & = 3 \\ x_2(t) & = 3\sin^2(t) \\ x_3(t) & = 4\cos^2(t) \end{align*} Definition of Linear Independence: $c_1x_1 + c_2x_2 + c_3x_3 = 0 \implies c_1=c_2=c_3=0$ (only the trivial solution) So we have: \begin{align} 3c_1 + 3c_2\sin^2(t) + 4c_3\cos^2(t) = 0 \end{align} My first idea is to differentiate both sides and get: $6c_2\sin(t)\cos(t) - 8c_3\cos(t)\sin(t) = 0$ Then we can factor to get: $\sin(t)\cos(t)(6c_2 - 8c_3) = 0$ So $c_3= \dfrac{6}{8}c_2$ gives the equation equals zero. Thus all $c$ are not $0$ and thus $x_1, x_2, x_3$ are linearly dependent. Is this correct? Or is there a cleaner way to do this?","I want to determine whether 3 functions are linearly independent: \begin{align*} x_1(t) & = 3 \\ x_2(t) & = 3\sin^2(t) \\ x_3(t) & = 4\cos^2(t) \end{align*} Definition of Linear Independence: $c_1x_1 + c_2x_2 + c_3x_3 = 0 \implies c_1=c_2=c_3=0$ (only the trivial solution) So we have: \begin{align} 3c_1 + 3c_2\sin^2(t) + 4c_3\cos^2(t) = 0 \end{align} My first idea is to differentiate both sides and get: $6c_2\sin(t)\cos(t) - 8c_3\cos(t)\sin(t) = 0$ Then we can factor to get: $\sin(t)\cos(t)(6c_2 - 8c_3) = 0$ So $c_3= \dfrac{6}{8}c_2$ gives the equation equals zero. Thus all $c$ are not $0$ and thus $x_1, x_2, x_3$ are linearly dependent. Is this correct? Or is there a cleaner way to do this?",,"['linear-algebra', 'functions', 'vector-spaces']"
84,Non-numerical vector space examples,Non-numerical vector space examples,,"I've recently been thinking about why my peers and other people I've helped learn vector spaces had trouble intuitively understanding the concept, and it occurred to me that non-numerical (i.e. nothing like $\langle 3,2,3 \rangle$ or obvious addition/multiplication operations) examples could reinforce intuition. For example, a huge problem was understanding that a vector space is simply a set of vectors with two operations that follow 10 axioms, and that a zero vector isn't necessarily all zeroes, and so on. Does anyone have any great examples of vector spaces (and the vectors and operations in them, of course) that are non-numerical, and thus can't lead to those trying to prove their validity to being stuck in ruts (like assuming the zero vector is all zeroes, that the inverse vector is the negative scalar multiple, etc.)? Pictures, letters, and any others would certainly be interesting! Note: I know that there are questions about vector spaces with unusual (and only partially valid) characteristics or out of the ordinary operations , but I'm looking for examples that have minimal numbers involved, to remove all automatic assumptions that are involved in them.","I've recently been thinking about why my peers and other people I've helped learn vector spaces had trouble intuitively understanding the concept, and it occurred to me that non-numerical (i.e. nothing like $\langle 3,2,3 \rangle$ or obvious addition/multiplication operations) examples could reinforce intuition. For example, a huge problem was understanding that a vector space is simply a set of vectors with two operations that follow 10 axioms, and that a zero vector isn't necessarily all zeroes, and so on. Does anyone have any great examples of vector spaces (and the vectors and operations in them, of course) that are non-numerical, and thus can't lead to those trying to prove their validity to being stuck in ruts (like assuming the zero vector is all zeroes, that the inverse vector is the negative scalar multiple, etc.)? Pictures, letters, and any others would certainly be interesting! Note: I know that there are questions about vector spaces with unusual (and only partially valid) characteristics or out of the ordinary operations , but I'm looking for examples that have minimal numbers involved, to remove all automatic assumptions that are involved in them.",,"['linear-algebra', 'big-list']"
85,Conjugate function of log-sum-exp?,Conjugate function of log-sum-exp?,,"In Boyd's Convex Optimization book , Example 3.25 finds the conjugate function $f^*(y):=\sup_{x\in\text{dom}(f)}(y^Tx-f(x))$ of the log-sum-exp function $f(x):=\log(\sum_{i=1}^ne^{x_i})$. First, the gradient of $y^Tx-f(x)$ is taken to yield the condition: $$ y_i=\frac{e^{x_i}}{\sum_{j=1}^ne^{x_j}}\quad i=1,...,n $$ where we see that a solution for $y$ exists if and only if $y\succ 0$ and $\textbf{1}^Ty=1$. Then the book simply says: By substituting the expression for $y_i$ into $y^Tx-f(x)$ we obtain $f^*(y)=\sum_{i=1}^ny_i\log(y_i)$. So far I've been unsuccessful in deriving this. How does one proceed? All I see is: $$ y^Tx-f(x)=\sum_{i=1}^ny_ix_i-\log(\sum_{i=1}^ne^{x_i})=\frac{\sum_{i=1}^nx_ie^{x_i}}{\sum_{j=1}^ne^{x_j}}-\log(\sum_{i=1}^ne^{x_i}) $$ But from here on I do not knonw how to proceed.","In Boyd's Convex Optimization book , Example 3.25 finds the conjugate function $f^*(y):=\sup_{x\in\text{dom}(f)}(y^Tx-f(x))$ of the log-sum-exp function $f(x):=\log(\sum_{i=1}^ne^{x_i})$. First, the gradient of $y^Tx-f(x)$ is taken to yield the condition: $$ y_i=\frac{e^{x_i}}{\sum_{j=1}^ne^{x_j}}\quad i=1,...,n $$ where we see that a solution for $y$ exists if and only if $y\succ 0$ and $\textbf{1}^Ty=1$. Then the book simply says: By substituting the expression for $y_i$ into $y^Tx-f(x)$ we obtain $f^*(y)=\sum_{i=1}^ny_i\log(y_i)$. So far I've been unsuccessful in deriving this. How does one proceed? All I see is: $$ y^Tx-f(x)=\sum_{i=1}^ny_ix_i-\log(\sum_{i=1}^ne^{x_i})=\frac{\sum_{i=1}^nx_ie^{x_i}}{\sum_{j=1}^ne^{x_j}}-\log(\sum_{i=1}^ne^{x_i}) $$ But from here on I do not knonw how to proceed.",,"['linear-algebra', 'proof-verification', 'convex-analysis']"
86,Prove $\det(\mathbf I+\mathbf A^T\mathbf A) = \det (\mathbf I+\mathbf A\mathbf A^T)$,Prove,\det(\mathbf I+\mathbf A^T\mathbf A) = \det (\mathbf I+\mathbf A\mathbf A^T),"Given a matrix $\mathbf A\in \mathbb R^{m\times n}$ and an identity matrix $\mathbf I$ of appropriate dimensions, how do you prove $\det(\mathbf I+\mathbf A^T\mathbf A) = \det (\mathbf I+\mathbf A\mathbf A^T)$?","Given a matrix $\mathbf A\in \mathbb R^{m\times n}$ and an identity matrix $\mathbf I$ of appropriate dimensions, how do you prove $\det(\mathbf I+\mathbf A^T\mathbf A) = \det (\mathbf I+\mathbf A\mathbf A^T)$?",,['linear-algebra']
87,Cutting a unit square into smaller squares,Cutting a unit square into smaller squares,,"My algebra professor gave me this puzzle a while back.  I'm pretty sure I've found the right solution; nonetheless, I wanted to share it and see if you come up with anything really nice or unexpected. Prove that if you take a unit square and cut it into a finite number of smaller squares (in any way you can think of), the side lengths of the smaller squares are all rational. P.S. The first tag was my professor's hint. [Edit] Just to be clear, every piece must actually be a square (e.g. no gluing two triangles into a square).","My algebra professor gave me this puzzle a while back.  I'm pretty sure I've found the right solution; nonetheless, I wanted to share it and see if you come up with anything really nice or unexpected. Prove that if you take a unit square and cut it into a finite number of smaller squares (in any way you can think of), the side lengths of the smaller squares are all rational. P.S. The first tag was my professor's hint. [Edit] Just to be clear, every piece must actually be a square (e.g. no gluing two triangles into a square).",,"['linear-algebra', 'combinatorics', 'puzzle']"
88,Simultaneous Diagonalizability of Multiple Commuting Matrices,Simultaneous Diagonalizability of Multiple Commuting Matrices,,"I know that for two given diagonalizable matrices $A_1$ and $A_2$ , they commute if and only if they are simultaneously diagonalizable. I was wondering if a similar condition held for multiple pairwise commuting matrices. Specifically, if we have a list of diagonalizable matrices $A_1, \cdots, A_n$ and $A_i$ commutes with $A_j$ for all $1 \leq i, j \leq n$ , then does there exist a simultaneous eigenbasis of all the $A_i$ ? That is, does there exist $S$ such that $S A_i S^{-1}$ is diagonal for all $i$ ? If this is not in general true, what kinds of non-trivial conditions are sufficient to make such a statement true?","I know that for two given diagonalizable matrices and , they commute if and only if they are simultaneously diagonalizable. I was wondering if a similar condition held for multiple pairwise commuting matrices. Specifically, if we have a list of diagonalizable matrices and commutes with for all , then does there exist a simultaneous eigenbasis of all the ? That is, does there exist such that is diagonal for all ? If this is not in general true, what kinds of non-trivial conditions are sufficient to make such a statement true?","A_1 A_2 A_1, \cdots, A_n A_i A_j 1 \leq i, j \leq n A_i S S A_i S^{-1} i","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'linear-transformations', 'diagonalization']"
89,"Simultaneously diagonalisable iff A,B commute","Simultaneously diagonalisable iff A,B commute",,"Yes, this is a repeat, however I have not seen anyone explain it fully (or such that I can comprehend it, and believe me, I have searched thoroughly for answers). If the (linear) endomorphisms $A,B: V \to V$ are diagonalisable, show that they are simultaneously diagonalisable $\iff AB=BA$ The initial implication is trivial. I have shown the case for when all eigenvalues are distinct. It is when there are not necessarily distinct that I cannot seem to get my head around the problem. (For instance, minimal polynomials are too unfamiliar to me to be constructive).  Any links, proofs, hints or explanations are deeply, deeply appreciated. Thanks!","Yes, this is a repeat, however I have not seen anyone explain it fully (or such that I can comprehend it, and believe me, I have searched thoroughly for answers). If the (linear) endomorphisms $A,B: V \to V$ are diagonalisable, show that they are simultaneously diagonalisable $\iff AB=BA$ The initial implication is trivial. I have shown the case for when all eigenvalues are distinct. It is when there are not necessarily distinct that I cannot seem to get my head around the problem. (For instance, minimal polynomials are too unfamiliar to me to be constructive).  Any links, proofs, hints or explanations are deeply, deeply appreciated. Thanks!",,[]
90,How to figure out the Argument of complex number?,How to figure out the Argument of complex number?,,"I have the absolute value of complex number , $$ r = |z| = \sqrt{x^2 + y^2}$$ when $z = x + iy$ is a complex number. How can I calculate the Argument of $z$? Thanks.","I have the absolute value of complex number , $$ r = |z| = \sqrt{x^2 + y^2}$$ when $z = x + iy$ is a complex number. How can I calculate the Argument of $z$? Thanks.",,"['linear-algebra', 'trigonometry', 'complex-numbers']"
91,Substochastic matrix spectral radius,Substochastic matrix spectral radius,,"Let $M$ be a row substochastic matrix, with at least one row having sum less than 1. Also, suppose $M$ is irreducible in the sense of a Markov chain. Is there an easy way to show the largest eigenvalue must be strictly less than 1? I hope that this result is true as stated. I know that Cauchy interlacing theorem gives me $\leq$ ,","Let be a row substochastic matrix, with at least one row having sum less than 1. Also, suppose is irreducible in the sense of a Markov chain. Is there an easy way to show the largest eigenvalue must be strictly less than 1? I hope that this result is true as stated. I know that Cauchy interlacing theorem gives me ,",M M \leq,"['linear-algebra', 'probability', 'matrices', 'spectral-radius']"
92,"Let the matrix $A=[a_{ij}]_{n×n}$ be defined by $a_{ij}=\gcd(i,j )$. How prove that $A$ is invertible, and compute $\det(A)$?","Let the matrix  be defined by . How prove that  is invertible, and compute ?","A=[a_{ij}]_{n×n} a_{ij}=\gcd(i,j ) A \det(A)","Let $A=[a_{ij}]_{n×n}$ be the matrix defined by letting $a_{ij}$ be the rational number such that $$a_{ij}=\gcd(i,j ).$$ How prove that $A$ is invertible, and compute $\det(A)$? thanks in advance","Let $A=[a_{ij}]_{n×n}$ be the matrix defined by letting $a_{ij}$ be the rational number such that $$a_{ij}=\gcd(i,j ).$$ How prove that $A$ is invertible, and compute $\det(A)$? thanks in advance",,"['linear-algebra', 'matrices', 'contest-math', 'determinant']"
93,The form of $2 \times 2$ unitary matrices,The form of  unitary matrices,2 \times 2,"I've been working through ""Groups and Symmetry"" (Armstrong) and came across this problem in chapter 9 which I can't figure out. Any hints/help would be greatly appreciated! Show that every $2\times 2$ unitary matrix has the form $$ \left(\begin{array}{c c} w & z \\ -e^{i \theta} z^{*} & e^{i \theta} w^{*} \end{array}\right) $$ for some $\theta\in\mathbb{R}$ and $w,z\in\mathbb{C}$. (A matrix is said to be unitary if it is invertible with its adjoint as the inverse. The symbol ""*"" denotes complex conjugate.)","I've been working through ""Groups and Symmetry"" (Armstrong) and came across this problem in chapter 9 which I can't figure out. Any hints/help would be greatly appreciated! Show that every $2\times 2$ unitary matrix has the form $$ \left(\begin{array}{c c} w & z \\ -e^{i \theta} z^{*} & e^{i \theta} w^{*} \end{array}\right) $$ for some $\theta\in\mathbb{R}$ and $w,z\in\mathbb{C}$. (A matrix is said to be unitary if it is invertible with its adjoint as the inverse. The symbol ""*"" denotes complex conjugate.)",,['linear-algebra']
94,The determinant of block triangular matrix as product of determinants of diagonal blocks,The determinant of block triangular matrix as product of determinants of diagonal blocks,,"I am given the following partitioned, upper-triangular matrix: $$ \begin{bmatrix}   A_1 &*  &*  &*  &*  &*  \\    0& A_2 &*  &*  &*  &*  \\    .&  0& A_3 &*  &*  &*  \\    .&  0&  0 &...  &*  &. \\    .&  0&  0&  0& ... &.  \\    0&  .&  ...&  0&0 & A_m \end{bmatrix} $$  where each of the $A_i$ are block-matrices. I have to prove that the determinant of this general matrix is: $$ \prod_{n=1}^m \det A_i. $$ We already proved in class that it is true for $ m=2 $ and we can use it in our proof. Thanks for your answers!","I am given the following partitioned, upper-triangular matrix: $$ \begin{bmatrix}   A_1 &*  &*  &*  &*  &*  \\    0& A_2 &*  &*  &*  &*  \\    .&  0& A_3 &*  &*  &*  \\    .&  0&  0 &...  &*  &. \\    .&  0&  0&  0& ... &.  \\    0&  .&  ...&  0&0 & A_m \end{bmatrix} $$  where each of the $A_i$ are block-matrices. I have to prove that the determinant of this general matrix is: $$ \prod_{n=1}^m \det A_i. $$ We already proved in class that it is true for $ m=2 $ and we can use it in our proof. Thanks for your answers!",,"['linear-algebra', 'matrices', 'determinant', 'block-matrices']"
95,Proof of this result related to Fibonacci numbers: $\begin{pmatrix}1&1\\1&0\end{pmatrix}^n=\begin{pmatrix}F_{n+1}&F_n\\F_n&F_{n-1}\end{pmatrix}$?,Proof of this result related to Fibonacci numbers: ?,\begin{pmatrix}1&1\\1&0\end{pmatrix}^n=\begin{pmatrix}F_{n+1}&F_n\\F_n&F_{n-1}\end{pmatrix},"$$\begin{pmatrix}1&1\\1&0\end{pmatrix}^n=\begin{pmatrix}F_{n+1}&F_n\\F_n&F_{n-1}\end{pmatrix}$$ Somebody has any idea how to go about proving this result? I know a proof by mathematical induction, but that, in my opinion, will be a verification of this result only. I tried to find through the net, but in vain, if someone has some link or pointer to its proof, please provide, I'll appreciate that. Thanks a lot!","$$\begin{pmatrix}1&1\\1&0\end{pmatrix}^n=\begin{pmatrix}F_{n+1}&F_n\\F_n&F_{n-1}\end{pmatrix}$$ Somebody has any idea how to go about proving this result? I know a proof by mathematical induction, but that, in my opinion, will be a verification of this result only. I tried to find through the net, but in vain, if someone has some link or pointer to its proof, please provide, I'll appreciate that. Thanks a lot!",,"['linear-algebra', 'matrices', 'discrete-mathematics', 'recurrence-relations', 'fibonacci-numbers']"
96,"$U^*\otimes V$ versus $L(U,V)$ for infinite dimensional spaces",versus  for infinite dimensional spaces,"U^*\otimes V L(U,V)","It's well known that $U^*\otimes V\cong L(U,V)$ for finite dimensional spaces. However, why people say that $U^*\otimes V$ is not isomorphic to $L(U,V)$ for infinite dimensional spaces and many books don't say anything related to this?","It's well known that $U^*\otimes V\cong L(U,V)$ for finite dimensional spaces. However, why people say that $U^*\otimes V$ is not isomorphic to $L(U,V)$ for infinite dimensional spaces and many books don't say anything related to this?",,"['linear-algebra', 'abstract-algebra', 'tensor-products']"
97,Theorem about positive matrices,Theorem about positive matrices,,"We will call a matrix positive matrix if all elements in the matrix are positive, and we will denote the largest eigenvalue with $\lambda_{\max}$ , what is exist because of the Perron–Frobenius theorem . Theorem. Let $A$ be a positive square matrix. If any element increases in the matrix then $\lambda_{\max}$ increases. My questions. Is there a name for this theorem and can anybody say books or papers what refer to it? How to prove it?","We will call a matrix positive matrix if all elements in the matrix are positive, and we will denote the largest eigenvalue with , what is exist because of the Perron–Frobenius theorem . Theorem. Let be a positive square matrix. If any element increases in the matrix then increases. My questions. Is there a name for this theorem and can anybody say books or papers what refer to it? How to prove it?",\lambda_{\max} A \lambda_{\max},"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'positive-matrices']"
98,What does $\|u\|$ mean?,What does  mean?,\|u\|,"What does $\left\Vert \mathbf{u}\right\Vert$ mean in this equation? How would this equation be performed? I'm extremely terrible in discrete mathematics and a simplistic answer would be ideal. (Don't answer it directly, I want to practice)","What does $\left\Vert \mathbf{u}\right\Vert$ mean in this equation? How would this equation be performed? I'm extremely terrible in discrete mathematics and a simplistic answer would be ideal. (Don't answer it directly, I want to practice)",,"['linear-algebra', 'vector-spaces', 'vectors', 'normed-spaces']"
99,Gauss elimination: Difference between partial and complete pivoting,Gauss elimination: Difference between partial and complete pivoting,,"I have some trouble with understanding the difference between partial and complete pivoting in Gauss elimination. I've found a few sources which are saying different things about what is allowed in each pivoting. From my understanding, in partial pivoting we are only allowed to change the columns (and are looking only at particular row), while in complete pivoting we look for highest value in whole matrix, and move it ""to the top"", by changing columns and rows. Is this correct, or am I wrong?","I have some trouble with understanding the difference between partial and complete pivoting in Gauss elimination. I've found a few sources which are saying different things about what is allowed in each pivoting. From my understanding, in partial pivoting we are only allowed to change the columns (and are looking only at particular row), while in complete pivoting we look for highest value in whole matrix, and move it ""to the top"", by changing columns and rows. Is this correct, or am I wrong?",,"['linear-algebra', 'matrices', 'numerical-methods', 'gaussian-elimination']"
