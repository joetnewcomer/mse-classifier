,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Multivariable limit $\lim_{(x,y)\to (0,0)} \frac {x^2 + y^2}{\sqrt{x^2 +y^2 + 1} - 1}$",Multivariable limit,"\lim_{(x,y)\to (0,0)} \frac {x^2 + y^2}{\sqrt{x^2 +y^2 + 1} - 1}","$$ \lim \limits_{(x,y)\to (0,0)} \frac {x^2 + y^2}{\sqrt{x^2 +y^2 + 1} - 1} $$ According to my textbook the limit equals $2$. What I have tried: Using the squeeze theorem: $$ \lim \limits_{(x,y)\to (0,0)} \frac {x^2 + y^2}{\sqrt{x^2 +y^2 + 1}} \le \lim\limits_{(x,y)\to (0,0)} \frac {x^2 + y^2}{\sqrt{x^2 +y^2 + 1} - 1} \le \lim\limits_{(x,y)\to (0,0)} \frac {x^2 + y^2}{\sqrt{x^2 +y^2 + 1} - 2} $$ $$ 0 \le \lim\limits_{(x,y)\to (0,0)} \frac {x^2 + y^2}{\sqrt{x^2 +y^2 + 1} - 1} \le 0 $$ I have also tried to use the squeeze theorem with two other equations and obtained different values: $$ \lim\limits_{(x,y)\to (0,0)} \frac {x^2 + y^2 - 1}{\sqrt{x^2 +y^2 + 1}} \le \lim\limits_{(x,y)\to (0,0)} \frac {x^2 + y^2}{\sqrt{x^2 +y^2 + 1} - 1} \le \lim\limits_{(x,y)\to (0,0)} \frac {x^2 + y^2 + 1}{\sqrt{x^2 +y^2 + 1} - 2} $$ $$ -1 \le \lim\limits_{(x,y)\to (0,0)} \frac {x^2 + y^2}{\sqrt{x^2 +y^2 + 1} - 1} \le -1 $$","$$ \lim \limits_{(x,y)\to (0,0)} \frac {x^2 + y^2}{\sqrt{x^2 +y^2 + 1} - 1} $$ According to my textbook the limit equals $2$. What I have tried: Using the squeeze theorem: $$ \lim \limits_{(x,y)\to (0,0)} \frac {x^2 + y^2}{\sqrt{x^2 +y^2 + 1}} \le \lim\limits_{(x,y)\to (0,0)} \frac {x^2 + y^2}{\sqrt{x^2 +y^2 + 1} - 1} \le \lim\limits_{(x,y)\to (0,0)} \frac {x^2 + y^2}{\sqrt{x^2 +y^2 + 1} - 2} $$ $$ 0 \le \lim\limits_{(x,y)\to (0,0)} \frac {x^2 + y^2}{\sqrt{x^2 +y^2 + 1} - 1} \le 0 $$ I have also tried to use the squeeze theorem with two other equations and obtained different values: $$ \lim\limits_{(x,y)\to (0,0)} \frac {x^2 + y^2 - 1}{\sqrt{x^2 +y^2 + 1}} \le \lim\limits_{(x,y)\to (0,0)} \frac {x^2 + y^2}{\sqrt{x^2 +y^2 + 1} - 1} \le \lim\limits_{(x,y)\to (0,0)} \frac {x^2 + y^2 + 1}{\sqrt{x^2 +y^2 + 1} - 2} $$ $$ -1 \le \lim\limits_{(x,y)\to (0,0)} \frac {x^2 + y^2}{\sqrt{x^2 +y^2 + 1} - 1} \le -1 $$",,"['limits', 'multivariable-calculus']"
1,Evaluate $\lim_{n \to \infty }\frac{(n!)^{1/n}}{n}$. [duplicate],Evaluate . [duplicate],\lim_{n \to \infty }\frac{(n!)^{1/n}}{n},This question already has answers here : Closed 11 years ago . Possible Duplicate: Finding the limit of $\frac {n}{\sqrt[n]{n!}}$ Evaluate  $$\lim_{n \to \infty }\frac{(n!)^{1/n}}{n}.$$ Can anyone help me with this? I have no idea how to start with. Thank you.,This question already has answers here : Closed 11 years ago . Possible Duplicate: Finding the limit of $\frac {n}{\sqrt[n]{n!}}$ Evaluate  $$\lim_{n \to \infty }\frac{(n!)^{1/n}}{n}.$$ Can anyone help me with this? I have no idea how to start with. Thank you.,,['limits']
2,Connecting $\sqrt{i \sqrt{i \sqrt{i \sqrt{i \dots}}}}$ to an infinite process?,Connecting  to an infinite process?,\sqrt{i \sqrt{i \sqrt{i \sqrt{i \dots}}}},"I just watched this YouTube video by Michael Penn about the expression $$\sqrt{i \sqrt{i \sqrt{i \sqrt{i \dots}}}}$$ and how to evaluate it. At the end of the video, he mentions that if we do not interpret square roots as just being principal square roots, then the solutions take the form $$ e^{\frac{i \pi}{2} + i \pi \bigl( n_0 + \frac{n_1}{2} + \frac{n_2}{4} + \frac{n_3}{8} + \cdots \bigr)} $$ where $n_0, n_1, n_2, ... \in \{0, 1\}$ . Because all real numbers in $[0, 2)$ can be expressed as binary numbers of the form $b_0.b_1b_2b_3b_4b_5\ldots\,$ , this means that solutions to the original nested radical work out to ""any complex number of modulus $1$ ."" (However, if we do restrict ourselves to the principal square root, we'd get back a single answer .) I'm trying to interpret exactly what this claim means and was wondering if the following line of reasoning works. Let's start with $\sqrt{i}$ . This means ""a number that, if squared and divided by $i$ , is equal to $1$ ."" There are two such numbers. Now, let's try $\sqrt{i\sqrt{i}}$ . This means ""some number that, if squared and divided by $i$ , then squared and divided by $i$ again, gives $1$ ."" Let's then try $\sqrt{i\sqrt{i\sqrt{i}}}$ . That would mean ""some number that if squared and divided by $i$ , then squared again and divided by $i$ , then squared again and divided by $i$ , you get $1$ ."" And more generally, any finite expansion of the nested radical can be interpreted as ""some number that, if repeated squared and divided by $i$ a total of $n$ times, gives $1$ ."" For starters - is this a correct way of thinking about the partial terms of the series? Assuming that this is the case, I'm struggling to make sense of what the infinitely nested radical would mean. The above process-based formulation breaks down if you repeat this process infinitely many times, since at a first reading it would mean ""a number where, if you square and divide by $i$ infinitely many times, yields $i$ ."" That doesn't make sense to me, nor does it feel like a legal strategy for extending the finite case to a limit. Rather, it seems like what's going on here is that if you repeatedly increase the number of iterations of ""square and divide by $i$ ,"" you start filling up more and more of the complex unit circle. And the infinite limit working out to ""all complex numbers of modulus $1$ "" then would mean something to the effect of the following: Let $z$ be an arbitrary complex number of modulus $1$ . Then by repeatedly squaring $z$ and dividing by $i$ , we can make the number get as close to $1$ as we'd like. Is this a reasonable conclusion to draw? Or is this not an appropriate way of thinking about the infinitely nested radical? Thanks!","I just watched this YouTube video by Michael Penn about the expression and how to evaluate it. At the end of the video, he mentions that if we do not interpret square roots as just being principal square roots, then the solutions take the form where . Because all real numbers in can be expressed as binary numbers of the form , this means that solutions to the original nested radical work out to ""any complex number of modulus ."" (However, if we do restrict ourselves to the principal square root, we'd get back a single answer .) I'm trying to interpret exactly what this claim means and was wondering if the following line of reasoning works. Let's start with . This means ""a number that, if squared and divided by , is equal to ."" There are two such numbers. Now, let's try . This means ""some number that, if squared and divided by , then squared and divided by again, gives ."" Let's then try . That would mean ""some number that if squared and divided by , then squared again and divided by , then squared again and divided by , you get ."" And more generally, any finite expansion of the nested radical can be interpreted as ""some number that, if repeated squared and divided by a total of times, gives ."" For starters - is this a correct way of thinking about the partial terms of the series? Assuming that this is the case, I'm struggling to make sense of what the infinitely nested radical would mean. The above process-based formulation breaks down if you repeat this process infinitely many times, since at a first reading it would mean ""a number where, if you square and divide by infinitely many times, yields ."" That doesn't make sense to me, nor does it feel like a legal strategy for extending the finite case to a limit. Rather, it seems like what's going on here is that if you repeatedly increase the number of iterations of ""square and divide by ,"" you start filling up more and more of the complex unit circle. And the infinite limit working out to ""all complex numbers of modulus "" then would mean something to the effect of the following: Let be an arbitrary complex number of modulus . Then by repeatedly squaring and dividing by , we can make the number get as close to as we'd like. Is this a reasonable conclusion to draw? Or is this not an appropriate way of thinking about the infinitely nested radical? Thanks!","\sqrt{i \sqrt{i \sqrt{i \sqrt{i \dots}}}} 
e^{\frac{i \pi}{2} + i \pi \bigl( n_0 + \frac{n_1}{2} + \frac{n_2}{4} + \frac{n_3}{8} + \cdots \bigr)}
 n_0, n_1, n_2, ... \in \{0, 1\} [0, 2) b_0.b_1b_2b_3b_4b_5\ldots\, 1 \sqrt{i} i 1 \sqrt{i\sqrt{i}} i i 1 \sqrt{i\sqrt{i\sqrt{i}}} i i i 1 i n 1 i i i 1 z 1 z i 1","['limits', 'complex-numbers', 'recreational-mathematics']"
3,Weird limit with $e^x$,Weird limit with,e^x,"Here's a limit that is testing my strengths. $$\lim\limits_{x\to -\infty} [(x^2+1)e^x]$$ Personal work: $$\lim\limits_{x\to -\infty} [(x^2+1)e^x] = \lim\limits_{x\to -\infty} (x^2e^x+e^x) = \lim\limits_{x\to -\infty} (x^2e^x)=L.$$ Let $x^2=u \iff x=-\sqrt u$ , then $u_0=\lim\limits_{x\to+\infty}{x^2}=+\infty$ So, $$ L=\lim\limits_{u\to+\infty}{(u*e^{-\sqrt u})} =...$$ Although it looks correct for me, both Microsoft mathematics and symbolab show me the answer "" $0$ "" so what am I doing wrong?","Here's a limit that is testing my strengths. Personal work: Let , then So, Although it looks correct for me, both Microsoft mathematics and symbolab show me the answer "" "" so what am I doing wrong?",\lim\limits_{x\to -\infty} [(x^2+1)e^x] \lim\limits_{x\to -\infty} [(x^2+1)e^x] = \lim\limits_{x\to -\infty} (x^2e^x+e^x) = \lim\limits_{x\to -\infty} (x^2e^x)=L. x^2=u \iff x=-\sqrt u u_0=\lim\limits_{x\to+\infty}{x^2}=+\infty  L=\lim\limits_{u\to+\infty}{(u*e^{-\sqrt u})} =... 0,['limits']
4,Why is $x^x$ only defined for $x>0$,Why is  only defined for,x^x x>0,Consider the real function $x^x$. I understand that $0^0$ is undefined so $x \neq 0$ but $x$ values like $-1$ and $-2$ have well defined function values (although curiously opposite sign). Why isn't the curve well defined for $x<0$? Note: the domain that I would like to investigate this function for is x is an element of the real numbers,Consider the real function $x^x$. I understand that $0^0$ is undefined so $x \neq 0$ but $x$ values like $-1$ and $-2$ have well defined function values (although curiously opposite sign). Why isn't the curve well defined for $x<0$? Note: the domain that I would like to investigate this function for is x is an element of the real numbers,,"['limits', 'functions']"
5,Calculate: $\lim\limits_{n\to\infty} \sum_{k=0}^{n} \frac{2n+k}{n^2+(2n+k)^2}$,Calculate:,\lim\limits_{n\to\infty} \sum_{k=0}^{n} \frac{2n+k}{n^2+(2n+k)^2},"Calculate: $$\lim\limits_{n\to\infty} \sum_{k=0}^{n} \frac{2n+k}{n^2+(2n+k)^2}$$ I thought a Riemann sum could lead to something, but couldn't find a suitable partition. Hint, please?","Calculate: $$\lim\limits_{n\to\infty} \sum_{k=0}^{n} \frac{2n+k}{n^2+(2n+k)^2}$$ I thought a Riemann sum could lead to something, but couldn't find a suitable partition. Hint, please?",,"['limits', 'summation']"
6,double integral getting different results,double integral getting different results,,"I am trying to calculate the double integral $$\lim_{b \to 0^+} \int_{b}^1 \int_b^1 \frac{y-x}{(y+x)^3}dydx$$ If you plug this into wolfram, you get $-\frac{1}{2}$ and if you plug it into symbolab you get $\frac{1}{2}$ I will show you my steps, I just want to make sure I got the right answer. $$\lim_{b \to 0^+} \int_{b}^1 \int_b^1 \frac{y-x}{(y+x)^3}dydx=\lim_{b \to 0^+} \int_{b}^1 \int_b^1 \frac{y+x}{(y+x)^3}-\frac{2x}{(y+x)^3}dydx$$ $$=\lim_{b \to 0^+} \int_{b}^1 \frac{-1}{(1+x)^2}dx=\lim_{b \to 0^+} \frac{1}{1+x}\Big|_b^1=\frac{-1}{2}$$ I just wanted to verify because these two different websites are giving me different answers.","I am trying to calculate the double integral $$\lim_{b \to 0^+} \int_{b}^1 \int_b^1 \frac{y-x}{(y+x)^3}dydx$$ If you plug this into wolfram, you get $-\frac{1}{2}$ and if you plug it into symbolab you get $\frac{1}{2}$ I will show you my steps, I just want to make sure I got the right answer. $$\lim_{b \to 0^+} \int_{b}^1 \int_b^1 \frac{y-x}{(y+x)^3}dydx=\lim_{b \to 0^+} \int_{b}^1 \int_b^1 \frac{y+x}{(y+x)^3}-\frac{2x}{(y+x)^3}dydx$$ $$=\lim_{b \to 0^+} \int_{b}^1 \frac{-1}{(1+x)^2}dx=\lim_{b \to 0^+} \frac{1}{1+x}\Big|_b^1=\frac{-1}{2}$$ I just wanted to verify because these two different websites are giving me different answers.",,"['limits', 'definite-integrals']"
7,Finding the limits of a multivariable function $\frac{x^2y^2}{x^4+y^2}$,Finding the limits of a multivariable function,\frac{x^2y^2}{x^4+y^2},"Question: Given the following function, determine whether the following function is continuous at $(0,0)$ $$ f(x,y)=\begin{cases}\frac{x^2y^2}{x^4+y^2}, &x^2+y^2 \neq 0,\\ 0 ,&x^2+y^2=0. \end{cases}$$ I know of three methods for approaching such problems: rewrite the function in terms of polar coordinates, then try to find the limit, bound the function from above and below, then apply the squeeze theorem, and take limits along the line $y=x$ . Using the first technique of rewriting the function in terms of polar coordinates, it appears that the function is continuous at $(0,0)$ .  I would like to be able to answer the question using the squeeze theorem.  I can bound the numerator, but I don't know how to bound the denominator so that the limit goes to zero at the origin.","Question: Given the following function, determine whether the following function is continuous at I know of three methods for approaching such problems: rewrite the function in terms of polar coordinates, then try to find the limit, bound the function from above and below, then apply the squeeze theorem, and take limits along the line . Using the first technique of rewriting the function in terms of polar coordinates, it appears that the function is continuous at .  I would like to be able to answer the question using the squeeze theorem.  I can bound the numerator, but I don't know how to bound the denominator so that the limit goes to zero at the origin.","(0,0) 
f(x,y)=\begin{cases}\frac{x^2y^2}{x^4+y^2}, &x^2+y^2 \neq 0,\\
0 ,&x^2+y^2=0. \end{cases} y=x (0,0)","['limits', 'multivariable-calculus']"
8,Limit of $\sum_{k=0}^{\lfloor n/2 \rfloor} 2^{-2nk} \binom{n}{2k}\left(\binom{2k}{k}^n\right)$,Limit of,\sum_{k=0}^{\lfloor n/2 \rfloor} 2^{-2nk} \binom{n}{2k}\left(\binom{2k}{k}^n\right),I can see numerically that $$\lim_{n \to \infty} \sum_{k=0}^{\lfloor n/2 \rfloor} 2^{-2nk} \binom{n}{2k}\left(\binom{2k}{k}^n\right) = 1$$ but how can you prove this?  Using Stirling's approximation doesn't seem to be enough.,I can see numerically that $$\lim_{n \to \infty} \sum_{k=0}^{\lfloor n/2 \rfloor} 2^{-2nk} \binom{n}{2k}\left(\binom{2k}{k}^n\right) = 1$$ but how can you prove this?  Using Stirling's approximation doesn't seem to be enough.,,['limits']
9,Limit of the hypergeometric function,Limit of the hypergeometric function,,"I dont' have experience with hypergeoemtric functions, but need to compute the following limit:$$\lim_{z\rightarrow0+}F\left(1,\alpha;\frac{\beta}{z};\frac{\gamma}{z}\right),$$  where $\alpha$ is non-integer real and $\beta$ and $\gamma$ are purely imaginary parameters. It seems the limit exists and finite. I tried to use an integral representation and a few standard transformations (such as Pfaff), but could not get the result. Any help would be appreciated.","I dont' have experience with hypergeoemtric functions, but need to compute the following limit:$$\lim_{z\rightarrow0+}F\left(1,\alpha;\frac{\beta}{z};\frac{\gamma}{z}\right),$$  where $\alpha$ is non-integer real and $\beta$ and $\gamma$ are purely imaginary parameters. It seems the limit exists and finite. I tried to use an integral representation and a few standard transformations (such as Pfaff), but could not get the result. Any help would be appreciated.",,"['limits', 'hypergeometric-function']"
10,How prove this limit $\lim\limits_{n\rightarrow \infty} \frac{f_n}{f_{n+1}}=a$ given two other limits related to $f_n$,How prove this limit  given two other limits related to,\lim\limits_{n\rightarrow \infty} \frac{f_n}{f_{n+1}}=a f_n,"Let $(f_n)$- real sequence such that $$ \lim_{n\rightarrow \infty} \frac{f_{n+1}f_n-f_{n-1}f_{n+2}}{f_{n+1}^2-f_nf_{n+2}}=a+b, $$ and $$ \lim_{n\rightarrow \infty} \frac{f_{n}^2-f_{n-1}f_{n+1}}{f_{n+1}^2-f_nf_{n+2}}=ab   \quad    (|a|<|b|). $$ Prove that:$$\lim_{n\rightarrow \infty} \frac{f_n}{f_{n+1}}=a $$ I think we must prove $\displaystyle\lim_{n\rightarrow \infty} \frac{f_n}{f_{n+1}} $ exists,and we prove this limit is $a$,But I can't prove this limit exists. My idea: since $$\lim_{n\to\infty}\dfrac{\dfrac{f_{n}}{f_{n+1}}-\dfrac{f_{n-1}}{f_{n}}\dfrac{f_{n}}{f_{n+1}}\dfrac{f_{n+2}}{f_{n+1}}}{1-\dfrac{f_{n}}{f_{n+1}}\dfrac{f_{n+2}}{f_{n+1}}}=a+b$$ and  $$\lim_{n\to\infty}\dfrac{\left(\dfrac{f_{n}}{f_{n+1}}\right)^2-\dfrac{f_{n-1}}{f_{n}}\dfrac{f_{n}}{f_{n+1}}}{1-\dfrac{f_{n}}{f_{n+1}}\dfrac{f_{n+2}}{f_{n+1}}}=ab$$ But I felt this deal is not useful, Other idea: I want to take the  Fibonacci sequence  to solve this problem, But I can't, Thank you","Let $(f_n)$- real sequence such that $$ \lim_{n\rightarrow \infty} \frac{f_{n+1}f_n-f_{n-1}f_{n+2}}{f_{n+1}^2-f_nf_{n+2}}=a+b, $$ and $$ \lim_{n\rightarrow \infty} \frac{f_{n}^2-f_{n-1}f_{n+1}}{f_{n+1}^2-f_nf_{n+2}}=ab   \quad    (|a|<|b|). $$ Prove that:$$\lim_{n\rightarrow \infty} \frac{f_n}{f_{n+1}}=a $$ I think we must prove $\displaystyle\lim_{n\rightarrow \infty} \frac{f_n}{f_{n+1}} $ exists,and we prove this limit is $a$,But I can't prove this limit exists. My idea: since $$\lim_{n\to\infty}\dfrac{\dfrac{f_{n}}{f_{n+1}}-\dfrac{f_{n-1}}{f_{n}}\dfrac{f_{n}}{f_{n+1}}\dfrac{f_{n+2}}{f_{n+1}}}{1-\dfrac{f_{n}}{f_{n+1}}\dfrac{f_{n+2}}{f_{n+1}}}=a+b$$ and  $$\lim_{n\to\infty}\dfrac{\left(\dfrac{f_{n}}{f_{n+1}}\right)^2-\dfrac{f_{n-1}}{f_{n}}\dfrac{f_{n}}{f_{n+1}}}{1-\dfrac{f_{n}}{f_{n+1}}\dfrac{f_{n+2}}{f_{n+1}}}=ab$$ But I felt this deal is not useful, Other idea: I want to take the  Fibonacci sequence  to solve this problem, But I can't, Thank you",,['limits']
11,"How to calculate $\sum \limits_{x=0}^{n} \frac{n!}{(n-x)!\,n^x}\left(1-\frac{x(x-1)}{n(n-1)}\right)$",How to calculate,"\sum \limits_{x=0}^{n} \frac{n!}{(n-x)!\,n^x}\left(1-\frac{x(x-1)}{n(n-1)}\right)","What are the asymptotics of the following sum as $n$ goes to infinity? $$ S =\sum\limits_{x=0}^{n} \frac{n!}{(n-x)!\,n^x}\left(1-\frac{x(x-1)}{n(n-1)}\right) $$ The sum comes from CDF related to sampling with replacement . Consider a random process where integers are sampled uniformly with replacement from $\{1...n\}$.  Let $X$ be a random variable that represents the number of samples until either a duplicate is found or both the values $1$ and $2$ have been found.  So if the samples where $1,6,3,5,1$ then $X=5$ and if it was $1,6,3,2$ then $X=4$. This sum is therefore $\mathbb{E}(X)$. We therefore know that $S = \mathbb{E}(X) \leq \text{mean time to find a duplicate} \sim \sqrt{\frac{\pi}{2} n}$. Taking the first part of the sum,  $$\sum_{x=0}^{n} \frac{n!}{(n-x)!\,n^x} = \left(\frac{e}{n} \right)^n \Gamma(n+1,n) \sim \left(\frac{e}{n} \right)^n  \frac{n!}{2} \sim  \sqrt{\frac{\pi}{2} n}. $$","What are the asymptotics of the following sum as $n$ goes to infinity? $$ S =\sum\limits_{x=0}^{n} \frac{n!}{(n-x)!\,n^x}\left(1-\frac{x(x-1)}{n(n-1)}\right) $$ The sum comes from CDF related to sampling with replacement . Consider a random process where integers are sampled uniformly with replacement from $\{1...n\}$.  Let $X$ be a random variable that represents the number of samples until either a duplicate is found or both the values $1$ and $2$ have been found.  So if the samples where $1,6,3,5,1$ then $X=5$ and if it was $1,6,3,2$ then $X=4$. This sum is therefore $\mathbb{E}(X)$. We therefore know that $S = \mathbb{E}(X) \leq \text{mean time to find a duplicate} \sim \sqrt{\frac{\pi}{2} n}$. Taking the first part of the sum,  $$\sum_{x=0}^{n} \frac{n!}{(n-x)!\,n^x} = \left(\frac{e}{n} \right)^n \Gamma(n+1,n) \sim \left(\frac{e}{n} \right)^n  \frac{n!}{2} \sim  \sqrt{\frac{\pi}{2} n}. $$",,['limits']
12,Squeeze Theorem Problem,Squeeze Theorem Problem,,"I'm busy studying for my Calculus A exam tomorrow and I've come across quite a tough question. I know I shouldn't post such localized questions, so if you don't want to answer, you can just push me in the right direction. I had to use the squeeze theorem to determine: $$\lim_{x\to\infty} \dfrac{\sin(x^2)}{x^3}$$ This was easy enough and I got the limit to equal 0. Now the second part of that question was to use that to determine: $$\lim_{x\to\infty} \dfrac{2x^3 + \sin(x^2)}{1 + x^3}$$ Obvously I can see that I'm going to have to sub in the answer I got from the first limit into this equation, but I can't seem to figure how how to do it. Any help would really be appreciated! Thanks in advance!","I'm busy studying for my Calculus A exam tomorrow and I've come across quite a tough question. I know I shouldn't post such localized questions, so if you don't want to answer, you can just push me in the right direction. I had to use the squeeze theorem to determine: $$\lim_{x\to\infty} \dfrac{\sin(x^2)}{x^3}$$ This was easy enough and I got the limit to equal 0. Now the second part of that question was to use that to determine: $$\lim_{x\to\infty} \dfrac{2x^3 + \sin(x^2)}{1 + x^3}$$ Obvously I can see that I'm going to have to sub in the answer I got from the first limit into this equation, but I can't seem to figure how how to do it. Any help would really be appreciated! Thanks in advance!",,['limits']
13,Show that $\lim\limits_{n\to\infty}\frac1{n}\sum\limits_{k=1}^{\infty}\left\lfloor\frac{n}{3^k}\right\rfloor=\frac{1}{2}$,Show that,\lim\limits_{n\to\infty}\frac1{n}\sum\limits_{k=1}^{\infty}\left\lfloor\frac{n}{3^k}\right\rfloor=\frac{1}{2},Show that   $$\lim_{n\to\infty}\frac1n\sum_{k=1}^{\infty}\left\lfloor\dfrac{n}{3^k}\right\rfloor=\frac{1}{2}$$ I can do right hand. $$\sum_{k=1}^{\infty}\left\lfloor\dfrac{n}{3^k}\right\rfloor\le \sum_{k=1}^{\infty}\dfrac{n}{3^k}=\dfrac{n}{2}$$ But how to solve left hand?,Show that   $$\lim_{n\to\infty}\frac1n\sum_{k=1}^{\infty}\left\lfloor\dfrac{n}{3^k}\right\rfloor=\frac{1}{2}$$ I can do right hand. $$\sum_{k=1}^{\infty}\left\lfloor\dfrac{n}{3^k}\right\rfloor\le \sum_{k=1}^{\infty}\dfrac{n}{3^k}=\dfrac{n}{2}$$ But how to solve left hand?,,[]
14,Multivariable limit .... no L'Hopital rule?,Multivariable limit .... no L'Hopital rule?,,"I am looking a bit at limits for multivariable functions by myself, and I can't figure it out; my book only mentions them shortly, but now I am looking at an ""assignments for those interested"" and it says $$\lim_{(x,y) \to (0,0)} \frac{\sin(xy)}{xy} \cos(x+y)$$ But that's a $0$ over $0$ expression... ? Do we have a L'Hopital rule for these types of functions? How would I solve it?","I am looking a bit at limits for multivariable functions by myself, and I can't figure it out; my book only mentions them shortly, but now I am looking at an ""assignments for those interested"" and it says $$\lim_{(x,y) \to (0,0)} \frac{\sin(xy)}{xy} \cos(x+y)$$ But that's a $0$ over $0$ expression... ? Do we have a L'Hopital rule for these types of functions? How would I solve it?",,"['limits', 'multivariable-calculus']"
15,How find this limit $\lim_{x\to 1}\Gamma{(1-x)}\cos\left(\dfrac{\pi}{2}x\right)$,How find this limit,\lim_{x\to 1}\Gamma{(1-x)}\cos\left(\dfrac{\pi}{2}x\right),"Find this limit $$I=\lim_{x \to 1}\Gamma\left(1 - x\right)\cos\left({\pi \over 2}\,x\right)$$ where  $\Gamma{(x)}$ is http://en.wikipedia.org/wiki/Gamma_function My idea: let $u=1-x$,then $$I=\lim_{u\to 0}\Gamma{(u)}\sin{u}$$ then I can't,Thank you","Find this limit $$I=\lim_{x \to 1}\Gamma\left(1 - x\right)\cos\left({\pi \over 2}\,x\right)$$ where  $\Gamma{(x)}$ is http://en.wikipedia.org/wiki/Gamma_function My idea: let $u=1-x$,then $$I=\lim_{u\to 0}\Gamma{(u)}\sin{u}$$ then I can't,Thank you",,['limits']
16,Limits notation: equals or arrow,Limits notation: equals or arrow,,"Recently I was using the following notation to express the limit in a publication: $$ \lim_{x \rightarrow \infty} f(x) = 0 $$ The reviewer said this is wrong. Instead it should read: $$ \lim_{x \rightarrow \infty} f(x) \rightarrow 0 $$ Is there a semantic difference between these two expressions? I did not find anything that would clarify the difference. In case it is just a matter of notational preference: Would you agree that the former notation is more common and maybe ""more correct"" since the limit actually is equal to the right hand side?","Recently I was using the following notation to express the limit in a publication: $$ \lim_{x \rightarrow \infty} f(x) = 0 $$ The reviewer said this is wrong. Instead it should read: $$ \lim_{x \rightarrow \infty} f(x) \rightarrow 0 $$ Is there a semantic difference between these two expressions? I did not find anything that would clarify the difference. In case it is just a matter of notational preference: Would you agree that the former notation is more common and maybe ""more correct"" since the limit actually is equal to the right hand side?",,"['limits', 'notation']"
17,Is $\lim _{x\rightarrow 0}\frac {1} {x}=\infty$ right?,Is  right?,\lim _{x\rightarrow 0}\frac {1} {x}=\infty,"I just learned a little about the limit by myself, and I wonder the result of $\lim _{x\rightarrow 0}\dfrac {1} {x}$. In order to get the answer, I asked one of my friends, and he told me that it is equal to $\infty$: $$\lim _{x\rightarrow 0}\dfrac {1} {x}=\infty$$ But I was still puzzled. In my opinion, the variable $x$ can approach $0$ from both positive direction and negative direction. So I get $\lim _{x\rightarrow 0^+}\dfrac {1} {x}=+\infty$ and $\lim _{x\rightarrow 0^-}\dfrac {1} {x}=-\infty$. Could you tell me your ideas about the result of $\lim _{x\rightarrow 0}\dfrac {1} {x}$? Thanks a lot!","I just learned a little about the limit by myself, and I wonder the result of $\lim _{x\rightarrow 0}\dfrac {1} {x}$. In order to get the answer, I asked one of my friends, and he told me that it is equal to $\infty$: $$\lim _{x\rightarrow 0}\dfrac {1} {x}=\infty$$ But I was still puzzled. In my opinion, the variable $x$ can approach $0$ from both positive direction and negative direction. So I get $\lim _{x\rightarrow 0^+}\dfrac {1} {x}=+\infty$ and $\lim _{x\rightarrow 0^-}\dfrac {1} {x}=-\infty$. Could you tell me your ideas about the result of $\lim _{x\rightarrow 0}\dfrac {1} {x}$? Thanks a lot!",,['limits']
18,Calculate $\lim_{x\to 0}\frac{1}{x^{\sin(x)}}$,Calculate,\lim_{x\to 0}\frac{1}{x^{\sin(x)}},"Calculate $$\lim_{x\to 0}\dfrac{1}{x^{\sin(x)}}$$ I'm pretty much clueless here, only that there is L'hospital obviously here. Would appreciate any help.","Calculate $$\lim_{x\to 0}\dfrac{1}{x^{\sin(x)}}$$ I'm pretty much clueless here, only that there is L'hospital obviously here. Would appreciate any help.",,"['limits', 'trigonometry', 'indeterminate-forms']"
19,How to calculate the limit of $\frac{\sin(ax)}{x}$ for $x\to0$,How to calculate the limit of  for,\frac{\sin(ax)}{x} x\to0,"How can I calculate the following limit epsilon-delta definition? $$\lim_{x \to 0} \left(\frac{\sin(ax)}{x}\right)$$ Edited the equation, sorry...","How can I calculate the following limit epsilon-delta definition? $$\lim_{x \to 0} \left(\frac{\sin(ax)}{x}\right)$$ Edited the equation, sorry...",,"['limits', 'trigonometry']"
20,What is the meaning of a.s.?,What is the meaning of a.s.?,,"What is the meaning of a.s. behind a limit formula (I found this in a paper about stochastic processes) , or sometimes P-a.s. ?","What is the meaning of a.s. behind a limit formula (I found this in a paper about stochastic processes) , or sometimes P-a.s. ?",,"['limits', 'stochastic-processes']"
21,Evaluate $\lim_{x\to \infty} (x+5)\tan^{-1}(x+5)- (x+1)\tan^{-1}(x+1)$,Evaluate,\lim_{x\to \infty} (x+5)\tan^{-1}(x+5)- (x+1)\tan^{-1}(x+1),$\lim_{x\to \infty} (x+5)\tan^{-1}(x+5)- (x+1)\tan^{-1}(x+1)$ What are the good/ clever methods to evaluate this limit? I tried taking $\tan^{-1} (x+5) = \theta$ to avoid inverse functions but its not helpful and makes it even more complicated. I also tried $\tan^{-1}a - \tan^{-1}b$ formula for the terms attached to x but that does not help to get rid of other terms multiplied by $1$ and $5$ . Edit: (Please address this in your answer) Can't we directly do this: $\lim_{x\to \infty} (x+5)\tan^{-1}(x+5)- (x+1)\tan^{-1}(x+1)$ $= (x+5)\dfrac{\pi}{2} - (x+1)\dfrac{\pi}{2}$ $ = \dfrac {5\pi - \pi}{2} = 2\pi$ I don't see anything wrong with it and it gives the right answer. Is this method correct? Can it be used in other questions too?,What are the good/ clever methods to evaluate this limit? I tried taking to avoid inverse functions but its not helpful and makes it even more complicated. I also tried formula for the terms attached to x but that does not help to get rid of other terms multiplied by and . Edit: (Please address this in your answer) Can't we directly do this: I don't see anything wrong with it and it gives the right answer. Is this method correct? Can it be used in other questions too?,\lim_{x\to \infty} (x+5)\tan^{-1}(x+5)- (x+1)\tan^{-1}(x+1) \tan^{-1} (x+5) = \theta \tan^{-1}a - \tan^{-1}b 1 5 \lim_{x\to \infty} (x+5)\tan^{-1}(x+5)- (x+1)\tan^{-1}(x+1) = (x+5)\dfrac{\pi}{2} - (x+1)\dfrac{\pi}{2}  = \dfrac {5\pi - \pi}{2} = 2\pi,['limits']
22,$\int _0^\infty f(x) $ exists and $f(x)$ is differentiable then $\lim _{x \to \infty} f '(x)$ exists. Counter example of this statement.,exists and  is differentiable then  exists. Counter example of this statement.,\int _0^\infty f(x)  f(x) \lim _{x \to \infty} f '(x),Can anyone give me a counter example of the statement If $\int_0^\infty f(x) $ exists and $f(x)$ is differentiable then $\lim _{x \to \infty} f'(x)$ exists. My attempt: I have thought one. First I draw $1/x^2$ in the first quadrant and $-1/x^2$ in the fourth quadrant.  The area under the following curves are finite. 1) $1/x^2$ 2) $-1/x^2$ 3) $x= 1$ . Now I have drawn infinite number of $y = x+c $ at equal distances in that region. Then I joined those infinite lines by some smooth curve so that the curve remains differentiable.  Now I think this function can be a counter example. I am uploading one picture of my attempt. Can anyone please check it and if possible suggest me a better function.,Can anyone give me a counter example of the statement If exists and is differentiable then exists. My attempt: I have thought one. First I draw in the first quadrant and in the fourth quadrant.  The area under the following curves are finite. 1) 2) 3) . Now I have drawn infinite number of at equal distances in that region. Then I joined those infinite lines by some smooth curve so that the curve remains differentiable.  Now I think this function can be a counter example. I am uploading one picture of my attempt. Can anyone please check it and if possible suggest me a better function.,\int_0^\infty f(x)  f(x) \lim _{x \to \infty} f'(x) 1/x^2 -1/x^2 1/x^2 -1/x^2 x= 1 y = x+c ,"['limits', 'analysis', 'derivatives', 'proof-verification', 'continuity']"
23,Evaluate $\lim_{n \to \infty} \sqrt[n]{3^n+4^n}$ [duplicate],Evaluate  [duplicate],\lim_{n \to \infty} \sqrt[n]{3^n+4^n},"This question already has answers here : How to show $\lim_{n \to \infty} \sqrt[n]{a^n+b^n}=\max \{a,b\}$? [duplicate] (3 answers) Closed 6 years ago . $$\lim_{n \to \infty} \sqrt[n]{3^n+4^n}$$ Is there there a way to solve this without using $e^{ln(3^n+4^n)}$? Maybe: $\displaystyle\lim_{n \to \infty} \sqrt[n]{4^n}=4\,\leq\,\lim_{n \to \infty} \sqrt[n]{3^n+4^n}\,\leq\,\lim_{n \to \infty} \sqrt[n]{2\cdot4^n}=4$?","This question already has answers here : How to show $\lim_{n \to \infty} \sqrt[n]{a^n+b^n}=\max \{a,b\}$? [duplicate] (3 answers) Closed 6 years ago . $$\lim_{n \to \infty} \sqrt[n]{3^n+4^n}$$ Is there there a way to solve this without using $e^{ln(3^n+4^n)}$? Maybe: $\displaystyle\lim_{n \to \infty} \sqrt[n]{4^n}=4\,\leq\,\lim_{n \to \infty} \sqrt[n]{3^n+4^n}\,\leq\,\lim_{n \to \infty} \sqrt[n]{2\cdot4^n}=4$?",,"['limits', 'radicals']"
24,Find: $\lim_{x\to0}\left(\lim_{n\to\infty}2^{2n}\left(1-\left(f ^{\circ n}(x)\right)\right)\right)$,Find:,\lim_{x\to0}\left(\lim_{n\to\infty}2^{2n}\left(1-\left(f ^{\circ n}(x)\right)\right)\right),"Let $$f:[0,1]\to\Bbb R\;\;\mbox{defined by}\;\;\; f(x)=\sqrt{\frac{1+x}{2}}$$ Find: $$\lim_{x\to0}\left(\lim_{n\to\infty}2^{2n}\left(1-\left(\overbrace {f \circ f \circ f \cdot\cdot\cdot \circ f}^{n}(x)\right)\right)\right)$$ I mainly need help with simplifying the composite function. I'll try to take it on from there. Any (substantial) hints or solutions will be greatly and sincerely appreciated.","Let $$f:[0,1]\to\Bbb R\;\;\mbox{defined by}\;\;\; f(x)=\sqrt{\frac{1+x}{2}}$$ Find: $$\lim_{x\to0}\left(\lim_{n\to\infty}2^{2n}\left(1-\left(\overbrace {f \circ f \circ f \cdot\cdot\cdot \circ f}^{n}(x)\right)\right)\right)$$ I mainly need help with simplifying the composite function. I'll try to take it on from there. Any (substantial) hints or solutions will be greatly and sincerely appreciated.",,"['limits', 'function-and-relation-composition']"
25,Calculate Limit 0f nested square roots,Calculate Limit 0f nested square roots,,It is an interesting task to try finding the limit of nested square root expressions. $$\lim_{n \to \infty}\left( 1 + \sqrt{2 + \sqrt{3+ ... + \sqrt {n + \sqrt{n+1}}}}\right)$$ How to solve this one?,It is an interesting task to try finding the limit of nested square root expressions. $$\lim_{n \to \infty}\left( 1 + \sqrt{2 + \sqrt{3+ ... + \sqrt {n + \sqrt{n+1}}}}\right)$$ How to solve this one?,,"['limits', 'nested-radicals']"
26,Limit of the sum $\sum_{k=1}^\infty\frac{\sin(kx)}{kx}$,Limit of the sum,\sum_{k=1}^\infty\frac{\sin(kx)}{kx},"The sum $$S(x)=\sum_{k=1}^\infty\frac{\sin(kx)}{kx}$$  can be written in a closed form: $$S(x)=\frac{1}{x}\left(\frac{1}{2}i\left(\ln(1-\exp(ix)\right)-\ln\left(1-\exp(-ix)\right)\right)$$ I am in trouble in calculating the limit: $$S_0=\lim_{x\to\infty}S(x)$$ Transforming the $S(x)$ in trigonometric functions I get: $$S(x)=-\frac{1}{2x}\arctan\left(-\sin(x),1-\cos(x)\right)+\frac{1}{2x}\arctan(\sin(x),1-\cos(x))$$ but this doesn't help me to evaluate $S_0$. I would like to have some suggestion useful to solve the problem. Thanks.","The sum $$S(x)=\sum_{k=1}^\infty\frac{\sin(kx)}{kx}$$  can be written in a closed form: $$S(x)=\frac{1}{x}\left(\frac{1}{2}i\left(\ln(1-\exp(ix)\right)-\ln\left(1-\exp(-ix)\right)\right)$$ I am in trouble in calculating the limit: $$S_0=\lim_{x\to\infty}S(x)$$ Transforming the $S(x)$ in trigonometric functions I get: $$S(x)=-\frac{1}{2x}\arctan\left(-\sin(x),1-\cos(x)\right)+\frac{1}{2x}\arctan(\sin(x),1-\cos(x))$$ but this doesn't help me to evaluate $S_0$. I would like to have some suggestion useful to solve the problem. Thanks.",,"['limits', 'summation']"
27,Show that $\lim\limits_{n\to\infty}\sum\limits_{k=1}^n\frac{1}{n^2\log(1+\frac{k^2}{n^2})}=\frac{{\pi}^2}{6}$,Show that,\lim\limits_{n\to\infty}\sum\limits_{k=1}^n\frac{1}{n^2\log(1+\frac{k^2}{n^2})}=\frac{{\pi}^2}{6},How can I expand this following limit? $$\lim_{n\to\infty}\sum_{k=1}^n\frac{1}{n^2\log(1+\frac{k^2}{n^2})}=\frac{{\pi}^2}{6}.$$,How can I expand this following limit? $$\lim_{n\to\infty}\sum_{k=1}^n\frac{1}{n^2\log(1+\frac{k^2}{n^2})}=\frac{{\pi}^2}{6}.$$,,[]
28,What is this limit called? Is it a different kind of derivative?,What is this limit called? Is it a different kind of derivative?,,"(first I should notice you this is not something I can look up in a textbook, because I'm learning partial derivatives, alike I do with most Maths, as a hobby. If something below is wrong, blame the internet and please let me know) I know I can take a mixed partial derivative of a multi-argument function, for expl.: $$ w = f(x; y; z) = x^2 y+ y^2z + xz^2\\ \frac{\partial^2 w}{\partial x\ \partial y} = \frac{\partial w}{\partial x}\frac{\partial w}{\partial y} = \frac{\partial}{\partial y}(2xy+z^2)= 2x = ^*\frac{\partial}{\partial x}(2yz+x^2) $$ (* = double check) But that's taking 2 partial derivatives. What If I want more than one variable to vary at once? $$ \lim_{h \to 0} \left( \frac{f(x+h;y+h;z)-f(x;y;z)}{h}\right)=\\ =\lim_{h \to 0} \left( \frac{(x+h)^2 (y+h)+(y+h)^2 z+(x+h)^2z^2-(x^2 y+ y^2z + xz^2)}{h}\right)=\\ =\lim_{h \to 0} \left( \frac{(x^2+2xh+h^2)(y+h)+(y^2+2hy+h^2) z+(x+h)z^2-(x^2 y+ y^2z + xz^2)}{h}\right)=\\ =\lim_{h \to 0} \left( \frac{2hxy+h^2y+hx^2+h^2x+h^3+2hyz+h^2z+hz^2}{h}\right)=\\ =\lim_{h \to 0} \left( 2xy+hy+x^2+hx+h^2+2yz+hz+z^2\right) =2xy+x^2+2yz+z^2 $$ Note: I took a while to write this, and only now I notice that the result of the above is the sum of $\frac{\partial w}{\partial x}$ and $\frac{\partial w}{\partial y}$, while the mixed partial in the beginning is its multiplication Nonetheless, the question remains: what is this limit called? Can it be seen as another type of derivative? What are its applications? Is there a notation in which I can write it such as the one for other derivatives? Thank you readers in advance (and for reading, my questions are not usually short).","(first I should notice you this is not something I can look up in a textbook, because I'm learning partial derivatives, alike I do with most Maths, as a hobby. If something below is wrong, blame the internet and please let me know) I know I can take a mixed partial derivative of a multi-argument function, for expl.: $$ w = f(x; y; z) = x^2 y+ y^2z + xz^2\\ \frac{\partial^2 w}{\partial x\ \partial y} = \frac{\partial w}{\partial x}\frac{\partial w}{\partial y} = \frac{\partial}{\partial y}(2xy+z^2)= 2x = ^*\frac{\partial}{\partial x}(2yz+x^2) $$ (* = double check) But that's taking 2 partial derivatives. What If I want more than one variable to vary at once? $$ \lim_{h \to 0} \left( \frac{f(x+h;y+h;z)-f(x;y;z)}{h}\right)=\\ =\lim_{h \to 0} \left( \frac{(x+h)^2 (y+h)+(y+h)^2 z+(x+h)^2z^2-(x^2 y+ y^2z + xz^2)}{h}\right)=\\ =\lim_{h \to 0} \left( \frac{(x^2+2xh+h^2)(y+h)+(y^2+2hy+h^2) z+(x+h)z^2-(x^2 y+ y^2z + xz^2)}{h}\right)=\\ =\lim_{h \to 0} \left( \frac{2hxy+h^2y+hx^2+h^2x+h^3+2hyz+h^2z+hz^2}{h}\right)=\\ =\lim_{h \to 0} \left( 2xy+hy+x^2+hx+h^2+2yz+hz+z^2\right) =2xy+x^2+2yz+z^2 $$ Note: I took a while to write this, and only now I notice that the result of the above is the sum of $\frac{\partial w}{\partial x}$ and $\frac{\partial w}{\partial y}$, while the mixed partial in the beginning is its multiplication Nonetheless, the question remains: what is this limit called? Can it be seen as another type of derivative? What are its applications? Is there a notation in which I can write it such as the one for other derivatives? Thank you readers in advance (and for reading, my questions are not usually short).",,"['limits', 'derivatives', 'partial-derivative', 'infinitesimals']"
29,Problem evaluating limits with the variable in the exponent,Problem evaluating limits with the variable in the exponent,,"I have problem evaluating limits with the variable in power, like the following limits: $\lim_{x \to 0} (1+ \sin 2x)^{\frac{1}{x}}$ $\lim_{x \to \infty} \big(\frac{2x+5}{2x-1})^{2x}$ I asked the question like this to get the main idea behind evaluating these kind of limits, so I can solve all the related questions.","I have problem evaluating limits with the variable in power, like the following limits: $\lim_{x \to 0} (1+ \sin 2x)^{\frac{1}{x}}$ $\lim_{x \to \infty} \big(\frac{2x+5}{2x-1})^{2x}$ I asked the question like this to get the main idea behind evaluating these kind of limits, so I can solve all the related questions.",,['limits']
30,How many divisors does $n!$ have?,How many divisors does  have?,n!,"Let $d(n!)$ be the number of divisors of $n!$ . I observed that graph of $\log(d(n!)$ is similar to that of $\pi(x)$ , the number of prime not exceeding $x$ except for a scaling factor. In other words, the following limit must exist: $$ \lim_{n \to \infty}\frac{\log(d(n!))}{\pi(n)} \approx 1.26 $$ Question : What is known about the number of divisors of $n!$ ? Can it be proved or disproved?","Let be the number of divisors of . I observed that graph of is similar to that of , the number of prime not exceeding except for a scaling factor. In other words, the following limit must exist: Question : What is known about the number of divisors of ? Can it be proved or disproved?","d(n!) n! \log(d(n!) \pi(x) x 
\lim_{n \to \infty}\frac{\log(d(n!))}{\pi(n)} \approx 1.26
 n!","['number-theory', 'limits', 'prime-numbers', 'divisibility', 'analytic-number-theory']"
31,A limit that involves two variables,A limit that involves two variables,,"I'm trying to compute this limit $\lim_{(x,y) \to (0,0)}2x\sin^2(\frac{1}{y})$, but WolframAlpha says that it does not exist. I'm not quite sure why. I do understand that there are oscillations coming from the $\sin(1/y)$. However, $x \to 0$ as well. Shouldn't that crush the function to zero? Also, I know that $\lim_{x \to 0} x \sin(\frac{1}{x}) = 0$. Isn't that pretty much the same idea as the limit in question?","I'm trying to compute this limit $\lim_{(x,y) \to (0,0)}2x\sin^2(\frac{1}{y})$, but WolframAlpha says that it does not exist. I'm not quite sure why. I do understand that there are oscillations coming from the $\sin(1/y)$. However, $x \to 0$ as well. Shouldn't that crush the function to zero? Also, I know that $\lim_{x \to 0} x \sin(\frac{1}{x}) = 0$. Isn't that pretty much the same idea as the limit in question?",,['limits']
32,Finding $\lim_{ n \to \infty }(1-\tan^2\frac{x}{2})(1-\tan^2\frac{x}{4})(1-\tan^2\frac{x}{8})...(1-\tan^2\frac{x}{2^m})=?$,Finding,\lim_{ n \to \infty }(1-\tan^2\frac{x}{2})(1-\tan^2\frac{x}{4})(1-\tan^2\frac{x}{8})...(1-\tan^2\frac{x}{2^m})=?,Find the limit : $$\lim_{ n \to \infty }(1-\tan^2\frac{x}{2})(1-\tan^2\frac{x}{4})(1-\tan^2\frac{x}{8})...(1-\tan^2\frac{x}{2^n})=?$$ My try : $$1-\tan^2 y = \frac{2\tan y }{\tan(2y)}$$ $$\lim_{ n \to \infty }\left( \frac{2\tan\frac{x}{2} }{\tan(x)}\right)( \frac{2\tan\frac{x}{4} }{\tan(\frac{x}{2})})( \frac{2\tan\frac{x}{8} }{\tan(\frac{x}{4})})...( \frac{2\tan\frac{x}{2^n} }{\tan(\frac{x}{2^{n-1}})})=?$$ Now?,Find the limit : $$\lim_{ n \to \infty }(1-\tan^2\frac{x}{2})(1-\tan^2\frac{x}{4})(1-\tan^2\frac{x}{8})...(1-\tan^2\frac{x}{2^n})=?$$ My try : $$1-\tan^2 y = \frac{2\tan y }{\tan(2y)}$$ $$\lim_{ n \to \infty }\left( \frac{2\tan\frac{x}{2} }{\tan(x)}\right)( \frac{2\tan\frac{x}{4} }{\tan(\frac{x}{2})})( \frac{2\tan\frac{x}{8} }{\tan(\frac{x}{4})})...( \frac{2\tan\frac{x}{2^n} }{\tan(\frac{x}{2^{n-1}})})=?$$ Now?,,['limits']
33,Find limit without using L'hopital or Taylor's series,Find limit without using L'hopital or Taylor's series,,I'm  trying to solve this limit $without$ using L'hopital's Rule or Taylor Series . Any help is appreciated! $$\lim\limits_{x\rightarrow 0^+}{\dfrac{e^x-\sin x-1}{x^2}}$$,I'm  trying to solve this limit $without$ using L'hopital's Rule or Taylor Series . Any help is appreciated! $$\lim\limits_{x\rightarrow 0^+}{\dfrac{e^x-\sin x-1}{x^2}}$$,,"['limits', 'limits-without-lhopital']"
34,Checking my proof that $\lim_{n \to \infty} 1/(2n - 3) = 0$,Checking my proof that,\lim_{n \to \infty} 1/(2n - 3) = 0,"My goal is to prove that the sequence $(x_n)$ defined as $x_n := \frac{1}{2n-3}$ converges and the limit is $0$ . In addition to determining if my proof is correct I also wanted to know if there is an alternative way to do the proof, maybe with an inequality that holds for all $n \in \mathbb{N}$ , still just using the definition of the limit of a sequence. My proof: Let $\varepsilon > 0$ . By the Archimedean Property there exists some $K_{*} \in \mathbb{N}$ such that $1/\varepsilon < K_{*}$ , therefore $1/K_{*} < \varepsilon$ . Let $K = \max\{K_{*}, 3\}$ and suppose $n \geq K$ . Then $$\bigg|\frac{1}{2n-3} - 0 \bigg| = \bigg|\frac{1}{2n-3}\bigg| \leq \bigg|\frac{1}{n}\bigg| = \frac{1}{n} \leq \frac{1}{K} < \varepsilon,$$ where the first inequality holds for all $n \geq 3$ .","My goal is to prove that the sequence defined as converges and the limit is . In addition to determining if my proof is correct I also wanted to know if there is an alternative way to do the proof, maybe with an inequality that holds for all , still just using the definition of the limit of a sequence. My proof: Let . By the Archimedean Property there exists some such that , therefore . Let and suppose . Then where the first inequality holds for all .","(x_n) x_n := \frac{1}{2n-3} 0 n \in \mathbb{N} \varepsilon > 0 K_{*} \in \mathbb{N} 1/\varepsilon < K_{*} 1/K_{*} < \varepsilon K = \max\{K_{*}, 3\} n \geq K \bigg|\frac{1}{2n-3} - 0 \bigg| = \bigg|\frac{1}{2n-3}\bigg| \leq \bigg|\frac{1}{n}\bigg| = \frac{1}{n} \leq \frac{1}{K} < \varepsilon, n \geq 3","['limits', 'solution-verification']"
35,Find $\lim\limits_{x\to0}\frac 1x(x^{-\sin x}-(\sin x)^{-x})$,Find,\lim\limits_{x\to0}\frac 1x(x^{-\sin x}-(\sin x)^{-x}),"The question is to evaluate this limit:$$\lim_{x\to0}\frac{\big(\frac{1}{x}\big)^{\sin x}-\big(\frac{1}{\sin x}\big)^x}{x}$$ I tried using l'Hospital's rule, taking the logarithm, doing some manipulations using known limits, but without success.","The question is to evaluate this limit:$$\lim_{x\to0}\frac{\big(\frac{1}{x}\big)^{\sin x}-\big(\frac{1}{\sin x}\big)^x}{x}$$ I tried using l'Hospital's rule, taking the logarithm, doing some manipulations using known limits, but without success.",,['limits']
36,limit exists or not,limit exists or not,,"Consider the function $f$:R$\rightarrow $R defined by  $$f(x) =\begin{cases}x-1,  &\text{if $x$ is rational} \\5-x,&\text{if $x$ is irrational}\end{cases}$$ Then $\space\lim\limits_{x\to a}$$f(x)$, $a\in\ R-\{\ 3\}$, exists or not ? Solution : Let $a$ be a irrational number .Then Right hand limit and left hand limit are as follows; $\space\lim\limits_{x\to a^+}$$f(x)$ =$\space\lim\limits_{h\to 0}$$f(a+h)$;   $\space$$\space\lim\limits_{x\to a^-}$$f(x)$ =$\space\lim\limits_{h\to 0}$$f(a-h)$ As h$\rightarrow$$0$, now let us assume that $h$ be a rational number, then $a+h$ and $a-h$ both are irrational . Therefore R.H.L. =$\space\lim\limits_{x\to a^+}$$f(x)$ =$\space\lim\limits_{h\to 0}$$f(a+h)$=$\space\lim\limits_{h\to o}$$5-(a+h)$$\space$=$\space$$5-a$ Similiarly L.H.L.$\space$=$5-a$ Hence the limit exists. Now again let us assume that $h$ be a irrational then $a+h$ and $a-h$ may be a rational or irrational, then the L.H.L. and R.H.L. may or may not be equal and hence limit may or may not be exist . But in my booklet the question says that the limit exists only if $a=3$ . Is it true or wrong ?","Consider the function $f$:R$\rightarrow $R defined by  $$f(x) =\begin{cases}x-1,  &\text{if $x$ is rational} \\5-x,&\text{if $x$ is irrational}\end{cases}$$ Then $\space\lim\limits_{x\to a}$$f(x)$, $a\in\ R-\{\ 3\}$, exists or not ? Solution : Let $a$ be a irrational number .Then Right hand limit and left hand limit are as follows; $\space\lim\limits_{x\to a^+}$$f(x)$ =$\space\lim\limits_{h\to 0}$$f(a+h)$;   $\space$$\space\lim\limits_{x\to a^-}$$f(x)$ =$\space\lim\limits_{h\to 0}$$f(a-h)$ As h$\rightarrow$$0$, now let us assume that $h$ be a rational number, then $a+h$ and $a-h$ both are irrational . Therefore R.H.L. =$\space\lim\limits_{x\to a^+}$$f(x)$ =$\space\lim\limits_{h\to 0}$$f(a+h)$=$\space\lim\limits_{h\to o}$$5-(a+h)$$\space$=$\space$$5-a$ Similiarly L.H.L.$\space$=$5-a$ Hence the limit exists. Now again let us assume that $h$ be a irrational then $a+h$ and $a-h$ may be a rational or irrational, then the L.H.L. and R.H.L. may or may not be equal and hence limit may or may not be exist . But in my booklet the question says that the limit exists only if $a=3$ . Is it true or wrong ?",,['limits']
37,How to prove that $\frac1{n\cdot 2^n}\sum\limits_{k=0}^{n}k^m\binom{n}{k}\to\frac{1}{2^m}$ when $n\to\infty$,How to prove that  when,\frac1{n\cdot 2^n}\sum\limits_{k=0}^{n}k^m\binom{n}{k}\to\frac{1}{2^m} n\to\infty,"I have solve following sum $$\sum_{k=0}^{n}k\binom{n}{k}=n2^{n-1}\Longrightarrow \dfrac{\displaystyle\sum_{k=0}^{n}k\binom{n}{k}}{n\cdot 2^n}\to\dfrac{1}{2},n\to\infty$$ $$\sum_{k=0}^{n}k^2\binom{n}{k}=n(n+1)2^{n-2}\Longrightarrow \dfrac{\displaystyle\sum_{k=0}^{n}k^2\binom{n}{k}}{n\cdot 2^n}\to\dfrac{1}{2^2},n\to\infty$$ $$\sum_{k=0}^{n}k^3\binom{n}{k}=2^{n-3}n^2(n+3)\Longrightarrow \dfrac{\displaystyle\sum_{k=0}^{n}k^3\binom{n}{k}}{n\cdot 2^n}\to\dfrac{1}{2^3},n\to\infty$$ so I conjecture the following: $$\dfrac{\displaystyle\sum_{k=0}^{n}k^m\binom{n}{k}}{n\cdot 2^n}\to\dfrac{1}{2^m},n\to\infty$$ $$\cdots\cdots$$ so I conjecture for $m$ be positive real number also hold.","I have solve following sum $$\sum_{k=0}^{n}k\binom{n}{k}=n2^{n-1}\Longrightarrow \dfrac{\displaystyle\sum_{k=0}^{n}k\binom{n}{k}}{n\cdot 2^n}\to\dfrac{1}{2},n\to\infty$$ $$\sum_{k=0}^{n}k^2\binom{n}{k}=n(n+1)2^{n-2}\Longrightarrow \dfrac{\displaystyle\sum_{k=0}^{n}k^2\binom{n}{k}}{n\cdot 2^n}\to\dfrac{1}{2^2},n\to\infty$$ $$\sum_{k=0}^{n}k^3\binom{n}{k}=2^{n-3}n^2(n+3)\Longrightarrow \dfrac{\displaystyle\sum_{k=0}^{n}k^3\binom{n}{k}}{n\cdot 2^n}\to\dfrac{1}{2^3},n\to\infty$$ so I conjecture the following: $$\dfrac{\displaystyle\sum_{k=0}^{n}k^m\binom{n}{k}}{n\cdot 2^n}\to\dfrac{1}{2^m},n\to\infty$$ $$\cdots\cdots$$ so I conjecture for $m$ be positive real number also hold.",,['limits']
38,"If $\frac{p_{n+1}}{np_n} \to p > 0 $, then $\sqrt[n+1]{p_{n+1}}-\sqrt[n]{p_{n}} \to \frac{p}{e}$","If , then",\frac{p_{n+1}}{np_n} \to p > 0  \sqrt[n+1]{p_{n+1}}-\sqrt[n]{p_{n}} \to \frac{p}{e},"Problem: Prove that, if a sequence ${p_n}$ satisfies $p_n > 0$ and $\lim\limits_{n \to \infty} \frac{p_{n+1}}{np_n} = p > 0 $, then $\lim\limits_{n \to \infty} \left(\sqrt[n+1]{p_{n+1}}-\sqrt[n]{p_{n}} \right) =\frac{p}{e} $. This lemma occurs  in problem B-1151 in the current Fibonacci Quarterly (August 2015) with a reference to a paper published by the University of Belgrade and not written in English. It is used to prove things like $\lim\limits_{n \to \infty} \left(\sqrt[n+1]{(n+1)!F_{n+1}}-\sqrt[n]{n!F_{n}} \right) =\frac{\alpha}{e} $ where $\alpha =\frac{1+\sqrt{5}}{2} $. This looks like an interesting result, so I thought that I would try to prove it. Here is my attempt. We have $\frac{p_{n+1}}{np_n}  \to p $. If this was $\frac{p_{n+1}}{(n+1)p_n}  \to p $, this could be divided by $\frac{n!}{n!}$ to get $\frac{p_{n+1}/(n+1)!}{p_n/n!}  \to p $. From this, setting $p_n/n! = q_n$, this would become $\frac{q_{n+1}}{q_n} \to p $, and things look hopeful. But, since $\frac{n}{n+1} \to 1$, $p =\lim\limits_{n \to \infty}\frac{p_{n+1}}{np_n} =\lim\limits_{n \to \infty}\frac{n}{n+1} \frac{p_{n+1}}{np_n} =\lim\limits_{n \to \infty}\frac{p_{n+1}}{(n+1)p_n}  $. Proceeding as described, letting $p_n/n! =q_n $, $p =\lim\limits_{n \to \infty} \frac{q_{n+1}}{q_n} $, so $\lim\limits_{n \to \infty} \frac{q_n}{p^n} = a $ for some $a > 0$. Therefore $p_n \approx n!p^n a $. Since $(n!)^{1/n} \to \frac{n}{e} $ and $a^{1/n} \to 1 $, $\sqrt[n]{p_{n}}  \approx \sqrt[n]{n!p^n a} \to \frac{n}{e}p  $ so $\sqrt[n+1]{p_{n+1}}-\sqrt[n]{p_{n}}  \to  \frac{n+1}{e}p- \frac{n}{e}p = \frac{p}{e} $ and we are done. My questions are: (1) Is my prove valid? (2) Is there are better proof? (3) Is there a more refined result, with more terms beyond $\frac{p}{e}$?","Problem: Prove that, if a sequence ${p_n}$ satisfies $p_n > 0$ and $\lim\limits_{n \to \infty} \frac{p_{n+1}}{np_n} = p > 0 $, then $\lim\limits_{n \to \infty} \left(\sqrt[n+1]{p_{n+1}}-\sqrt[n]{p_{n}} \right) =\frac{p}{e} $. This lemma occurs  in problem B-1151 in the current Fibonacci Quarterly (August 2015) with a reference to a paper published by the University of Belgrade and not written in English. It is used to prove things like $\lim\limits_{n \to \infty} \left(\sqrt[n+1]{(n+1)!F_{n+1}}-\sqrt[n]{n!F_{n}} \right) =\frac{\alpha}{e} $ where $\alpha =\frac{1+\sqrt{5}}{2} $. This looks like an interesting result, so I thought that I would try to prove it. Here is my attempt. We have $\frac{p_{n+1}}{np_n}  \to p $. If this was $\frac{p_{n+1}}{(n+1)p_n}  \to p $, this could be divided by $\frac{n!}{n!}$ to get $\frac{p_{n+1}/(n+1)!}{p_n/n!}  \to p $. From this, setting $p_n/n! = q_n$, this would become $\frac{q_{n+1}}{q_n} \to p $, and things look hopeful. But, since $\frac{n}{n+1} \to 1$, $p =\lim\limits_{n \to \infty}\frac{p_{n+1}}{np_n} =\lim\limits_{n \to \infty}\frac{n}{n+1} \frac{p_{n+1}}{np_n} =\lim\limits_{n \to \infty}\frac{p_{n+1}}{(n+1)p_n}  $. Proceeding as described, letting $p_n/n! =q_n $, $p =\lim\limits_{n \to \infty} \frac{q_{n+1}}{q_n} $, so $\lim\limits_{n \to \infty} \frac{q_n}{p^n} = a $ for some $a > 0$. Therefore $p_n \approx n!p^n a $. Since $(n!)^{1/n} \to \frac{n}{e} $ and $a^{1/n} \to 1 $, $\sqrt[n]{p_{n}}  \approx \sqrt[n]{n!p^n a} \to \frac{n}{e}p  $ so $\sqrt[n+1]{p_{n+1}}-\sqrt[n]{p_{n}}  \to  \frac{n+1}{e}p- \frac{n}{e}p = \frac{p}{e} $ and we are done. My questions are: (1) Is my prove valid? (2) Is there are better proof? (3) Is there a more refined result, with more terms beyond $\frac{p}{e}$?",,"['limits', 'fibonacci-numbers']"
39,Find multivariable limit $\frac{x^2y}{x^2+y^3}$,Find multivariable limit,\frac{x^2y}{x^2+y^3},"Find multivariable limit of: $$\lim_{ \left( x,y\right) \rightarrow \left(0,0 \right)}\frac{x^2y}{x^2+y^3}$$ How to find that limit? I was trying to do the following, but i am not able to find a proper inequality: $$| \frac{x^2y}{x^2+y^3} | = |y-\frac{y^4}{x^2+y^3}| \le$$","Find multivariable limit of: $$\lim_{ \left( x,y\right) \rightarrow \left(0,0 \right)}\frac{x^2y}{x^2+y^3}$$ How to find that limit? I was trying to do the following, but i am not able to find a proper inequality: $$| \frac{x^2y}{x^2+y^3} | = |y-\frac{y^4}{x^2+y^3}| \le$$",,"['limits', 'multivariable-calculus', 'inequality']"
40,Limit of $0/x$ as x goes to 0,Limit of  as x goes to 0,0/x,"What is the limit of $0/x$ as x goes to $0$, without using L'Hopital's rule? Clearly it should be $0$, but I'm not sure how it is any different from something like the divergence of $1/r^2$ which yields a dirac delta.","What is the limit of $0/x$ as x goes to $0$, without using L'Hopital's rule? Clearly it should be $0$, but I'm not sure how it is any different from something like the divergence of $1/r^2$ which yields a dirac delta.",,[]
41,How find the limit $\lim_{n\to+\infty}\sum_{i=2}^{n}\frac{\ln{i^2}}{i^2}$?,How find the limit ?,\lim_{n\to+\infty}\sum_{i=2}^{n}\frac{\ln{i^2}}{i^2},find the $$\lim_{n\to+\infty}\left(\dfrac{\ln{2^2}}{2^2}+\dfrac{\ln{3^2}}{3^2}+\dfrac{\ln{4^2}}{4^2}+\cdots+\dfrac{\ln{n^2}}{n^2}\right)$$ My try: $$\lim_{n\to+\infty}\left(\dfrac{\ln{2^2}}{2^2}+\dfrac{\ln{3^2}}{3^2}+\dfrac{\ln{4^2}}{4^2}+\cdots+\dfrac{\ln{n^2}}{n^2}\right)=2\sum_{n=2}^{\infty}\dfrac{\ln{n}}{n^2}$$ and I know  solve this following $$\sum_{n=2}^{\infty}(-1)^n\dfrac{\ln{n}}{n}=\ln{2}\left(C-\dfrac{\ln{2}}{2}\right)$$ where $C$ is Euler constant Solution:note this following  $$\lim_{n\to\infty}\left(\dfrac{\ln{1}}{1}+\dfrac{\ln{2}}{2}+\cdots+\dfrac{\ln{n}}{n}-\dfrac{(\ln{n})^2}{2}\right)=l$$ we let $$S_{n}=\sum_{k=1}^{n}\dfrac{(-1)^k\ln{k}}{k}$$,find the $$\lim_{n\to+\infty}\left(\dfrac{\ln{2^2}}{2^2}+\dfrac{\ln{3^2}}{3^2}+\dfrac{\ln{4^2}}{4^2}+\cdots+\dfrac{\ln{n^2}}{n^2}\right)$$ My try: $$\lim_{n\to+\infty}\left(\dfrac{\ln{2^2}}{2^2}+\dfrac{\ln{3^2}}{3^2}+\dfrac{\ln{4^2}}{4^2}+\cdots+\dfrac{\ln{n^2}}{n^2}\right)=2\sum_{n=2}^{\infty}\dfrac{\ln{n}}{n^2}$$ and I know  solve this following $$\sum_{n=2}^{\infty}(-1)^n\dfrac{\ln{n}}{n}=\ln{2}\left(C-\dfrac{\ln{2}}{2}\right)$$ where $C$ is Euler constant Solution:note this following  $$\lim_{n\to\infty}\left(\dfrac{\ln{1}}{1}+\dfrac{\ln{2}}{2}+\cdots+\dfrac{\ln{n}}{n}-\dfrac{(\ln{n})^2}{2}\right)=l$$ we let $$S_{n}=\sum_{k=1}^{n}\dfrac{(-1)^k\ln{k}}{k}$$,,"['limits', 'summation']"
42,Does the fraction of positive integers not being a Carmichael value have a limit?,Does the fraction of positive integers not being a Carmichael value have a limit?,,"Let $f(n)$ be the number of positive integers $x\le n$ such that $\lambda(k)=x$ has no solution, where $\lambda(k)$ denotes the Carmichael-function. Does $$\lim_{n\rightarrow \infty} \frac{f(n)}{n}$$ exist , and if yes, is it $1$ or some smaller value ? The last few lines in a numerical analysis were : 579000000  0.77069874093264248704663212435233160622 580000000  0.77070391551724137931034482758620689655 581000000  0.77070938382099827882960413080895008606 582000000  0.77071472164948453608247422680412371134 583000000  0.77071994511149228130360205831903945112 584000000  0.77072559931506849315068493150684931507 585000000  0.77073061196581196581196581196581196581 This indicates a slow increase, but when I tried small ranges with larger values , the frequency seemed to still increase (above $0.8$ ). Any ideas ?","Let be the number of positive integers such that has no solution, where denotes the Carmichael-function. Does exist , and if yes, is it or some smaller value ? The last few lines in a numerical analysis were : 579000000  0.77069874093264248704663212435233160622 580000000  0.77070391551724137931034482758620689655 581000000  0.77070938382099827882960413080895008606 582000000  0.77071472164948453608247422680412371134 583000000  0.77071994511149228130360205831903945112 584000000  0.77072559931506849315068493150684931507 585000000  0.77073061196581196581196581196581196581 This indicates a slow increase, but when I tried small ranges with larger values , the frequency seemed to still increase (above ). Any ideas ?",f(n) x\le n \lambda(k)=x \lambda(k) \lim_{n\rightarrow \infty} \frac{f(n)}{n} 1 0.8,"['limits', 'elementary-number-theory', 'carmichael-function']"
43,"How can I show $\lim\limits_{x\to a}e^x=e^a$ just using limit ,without ""continuous""","How can I show  just using limit ,without ""continuous""",\lim\limits_{x\to a}e^x=e^a,"Effort: We know that $e^x=\lim\limits_{n\to \infty}\left(1+\dfrac{x}{n}\right)^n$ And if we can say $\lim\limits_{y\to b}\left[\lim\limits_{x\to a}f\right]=\lim\limits_{x\to a}\left[\lim\limits_{y\to b}f\right]$  for this problem; $\lim\limits_{x\to a}e^x=\lim\limits_{x\to a} \left[\lim\limits_{n\to \infty}\left(1+\dfrac{x}{n}\right)^n\right]=\lim\limits_{n\to \infty}\left[\lim\limits_{x\to a}\left(1+\dfrac{x}{n}\right)^n\right]=\lim\limits_{n\to \infty}\left[\left(1+\dfrac{a}{n}\right)^n\right]=e^a$ and I proof $\boxed{\boxed{\lim\limits_{x\to a}e^x=e^a}}$ . İs this proof true? And please help ,how I can prove this, with ""accurately"".","Effort: We know that $e^x=\lim\limits_{n\to \infty}\left(1+\dfrac{x}{n}\right)^n$ And if we can say $\lim\limits_{y\to b}\left[\lim\limits_{x\to a}f\right]=\lim\limits_{x\to a}\left[\lim\limits_{y\to b}f\right]$  for this problem; $\lim\limits_{x\to a}e^x=\lim\limits_{x\to a} \left[\lim\limits_{n\to \infty}\left(1+\dfrac{x}{n}\right)^n\right]=\lim\limits_{n\to \infty}\left[\lim\limits_{x\to a}\left(1+\dfrac{x}{n}\right)^n\right]=\lim\limits_{n\to \infty}\left[\left(1+\dfrac{a}{n}\right)^n\right]=e^a$ and I proof $\boxed{\boxed{\lim\limits_{x\to a}e^x=e^a}}$ . İs this proof true? And please help ,how I can prove this, with ""accurately"".",,['limits']
44,On the proof of the continuity of the inner product.,On the proof of the continuity of the inner product.,,I am having problems with the following proof and I need to fill in some details: I understand that continuity is being proven by the sequence definition but I do not get why (a) follows immediately after (2.1.10). Could I get the extra steps? Also could I have a brief epsilon-delta proof for point (b) after having applied Cauchy-Swartz as I am unsure on how do to this by epsilon-delta?,I am having problems with the following proof and I need to fill in some details: I understand that continuity is being proven by the sequence definition but I do not get why (a) follows immediately after (2.1.10). Could I get the extra steps? Also could I have a brief epsilon-delta proof for point (b) after having applied Cauchy-Swartz as I am unsure on how do to this by epsilon-delta?,,"['limits', 'convergence-divergence', 'hilbert-spaces', 'normed-spaces', 'inner-products']"
45,To find the limit of $\frac{1}{\sin n}+\frac{1}{\cos n}$,To find the limit of,\frac{1}{\sin n}+\frac{1}{\cos n},What should be the value of $\lim (\frac{1}{\cos n}+\frac{1}{\sin n})$ ? I think the limit does not exist. Thanks in advance,What should be the value of $\lim (\frac{1}{\cos n}+\frac{1}{\sin n})$ ? I think the limit does not exist. Thanks in advance,,['limits']
46,Fourier transform extended to $L^2$,Fourier transform extended to,L^2,"Let $f\in L^1(\mathbb{R})\cap L^2(\mathbb{R})$, and let $f_k$ be functions in the Schwartz class such that $\|f-f_k\|_1+\|f-f_k\|_2\rightarrow 0$ as $k\rightarrow\infty$. Define $$g_k(t)=\int_\mathbb{R}f_k(x)e^{-itx}dx \text{    and    } g(t)=\int_\mathbb{R}f(x)e^{-itx}dx$$ It can be shown that $\lim_{k\rightarrow\infty}g_k(t)=g(t)$ for all $t$. Let $T_1:L^2(\mathbb{R})\rightarrow L^2(\mathbb{R})$ be defined as the unique continuous mapping that extends the mapping $T:S\rightarrow L^2(\mathbb{R})$, where $S$ is the Schwartz class, and the Fourier transform of a function in the Schwartz class is defined using the $L^1$ definition (like $g$ and $g_k$ above.) How can I prove that $$\lim_{k\rightarrow\infty}\|g_k-T_1f\|_2=0$$ and also that $$g(t)=(T_1f)(t)$$ pointwise? EDIT : The first one is a consequence of Plancherel theorem, as Daniel Fischer mentioned in the comment. What about the second one?","Let $f\in L^1(\mathbb{R})\cap L^2(\mathbb{R})$, and let $f_k$ be functions in the Schwartz class such that $\|f-f_k\|_1+\|f-f_k\|_2\rightarrow 0$ as $k\rightarrow\infty$. Define $$g_k(t)=\int_\mathbb{R}f_k(x)e^{-itx}dx \text{    and    } g(t)=\int_\mathbb{R}f(x)e^{-itx}dx$$ It can be shown that $\lim_{k\rightarrow\infty}g_k(t)=g(t)$ for all $t$. Let $T_1:L^2(\mathbb{R})\rightarrow L^2(\mathbb{R})$ be defined as the unique continuous mapping that extends the mapping $T:S\rightarrow L^2(\mathbb{R})$, where $S$ is the Schwartz class, and the Fourier transform of a function in the Schwartz class is defined using the $L^1$ definition (like $g$ and $g_k$ above.) How can I prove that $$\lim_{k\rightarrow\infty}\|g_k-T_1f\|_2=0$$ and also that $$g(t)=(T_1f)(t)$$ pointwise? EDIT : The first one is a consequence of Plancherel theorem, as Daniel Fischer mentioned in the comment. What about the second one?",,"['limits', 'fourier-analysis']"
47,Geometric Mean limit of $\ell_p$ norm of sums,Geometric Mean limit of  norm of sums,\ell_p,"My analysis professor introduced the $\ell_p$ norm to our class as: \begin{align} \| x \|_p = \left(\frac{1}{n}\sum_{j=1}^{n} |x_j|^p\right)^{1/p}. \end{align} We are asked to prove the following: \begin{align} \lim_{p \to 0} \|x\|_p &= \left( \prod_{j=1}^{n} |x_j| \right)^{1/n}, \end{align} Can anyone give me a sort of ""intuition"" as to why this is true, and a hint as to how to approach the problem? All the reading material I come across uses measure theory and integrals instead of sums so I can't quite follow it. On another note, is there any way to improve mathematical intuition? It seems that every proof in the class relies on little mathematical ""tricks"", which I find frustrating because I don't even know where to begin for the problems we're assigned. (I find that it's not at all the same in my Algebra or Probability classes) Thank you!","My analysis professor introduced the $\ell_p$ norm to our class as: \begin{align} \| x \|_p = \left(\frac{1}{n}\sum_{j=1}^{n} |x_j|^p\right)^{1/p}. \end{align} We are asked to prove the following: \begin{align} \lim_{p \to 0} \|x\|_p &= \left( \prod_{j=1}^{n} |x_j| \right)^{1/n}, \end{align} Can anyone give me a sort of ""intuition"" as to why this is true, and a hint as to how to approach the problem? All the reading material I come across uses measure theory and integrals instead of sums so I can't quite follow it. On another note, is there any way to improve mathematical intuition? It seems that every proof in the class relies on little mathematical ""tricks"", which I find frustrating because I don't even know where to begin for the problems we're assigned. (I find that it's not at all the same in my Algebra or Probability classes) Thank you!",,"['limits', 'intuition', 'normed-spaces', 'lp-spaces']"
48,Tetration limit,Tetration limit,,"For each $n$, define $f_n:\mathbb R^+\rightarrow \mathbb R^+$ by  $f_n(x) = \underbrace{x^{x^{x^{...^{x^x}}}}}_n$ Is it true that $\lim\limits_{n \to \infty} f_n(\frac{n+1}{n}) = 1$ ? A few computations using Wolfram Alpha seem to suggest so, but I am unsure of how to prove it. One way to think about why it might be true intuitively is to compare it to the limit definition of $e$, but even this intuition is handwavy (although it seems like it is true based on computations). Furthermore, I'm unsure if the Binomial expansion actually helps us here. I'm interested in hearing your thoughts.","For each $n$, define $f_n:\mathbb R^+\rightarrow \mathbb R^+$ by  $f_n(x) = \underbrace{x^{x^{x^{...^{x^x}}}}}_n$ Is it true that $\lim\limits_{n \to \infty} f_n(\frac{n+1}{n}) = 1$ ? A few computations using Wolfram Alpha seem to suggest so, but I am unsure of how to prove it. One way to think about why it might be true intuitively is to compare it to the limit definition of $e$, but even this intuition is handwavy (although it seems like it is true based on computations). Furthermore, I'm unsure if the Binomial expansion actually helps us here. I'm interested in hearing your thoughts.",,"['limits', 'tetration']"
49,How to find the limit $\lim\limits _{ x\to \infty } \left( \sqrt { x^2+3x } -\sqrt { x^2+x } \right) $?,How to find the limit ?,\lim\limits _{ x\to \infty } \left( \sqrt { x^2+3x } -\sqrt { x^2+x } \right) ,When I try to do this type of indeterminations I reach to this point: $\lim\limits_{ x\to \infty  } \dfrac { 2x }{ \sqrt { x^ 2 +3x } +\sqrt { x^2 +x }  } $ but I don't know how to continue. Thanks.,When I try to do this type of indeterminations I reach to this point: $\lim\limits_{ x\to \infty  } \dfrac { 2x }{ \sqrt { x^ 2 +3x } +\sqrt { x^2 +x }  } $ but I don't know how to continue. Thanks.,,"['limits', 'radicals']"
50,Prove the Rational Limit Theorem.,Prove the Rational Limit Theorem.,,"Rational Limit Theorem. For $f(x, y) =\frac{|x|^a|y|^b}{|x|^c+|y|^d}$ , with $a, b, c, d$ positive, $$\lim_{(x,y)→(0,0)}f(x, y) \text{ exists and equals zero}\Leftrightarrow \frac{a}{c}+\frac{b}{d}>1.$$ We will break this down into three parts:  first proving one direction by our techniques to show limits don’t exist, then using a famous inequality to help prove the other direction. As a guideline, each proof can be written in two or three lines. $1.$ Show that if $\frac{a}{c}+\frac{b}{d}≤1$ , then the limit does not exist. For the next two problems, you are free to use the following inequality: Young’s Theorem. For positive real numbers $w$ , $z$ and any $0≤t≤1$ , $$w^tz^{1−t}≤tw+ (1−t)z$$ $2.$ Show  that  if $\frac{a}{c}+\frac{b}{d}=  1$ ,  where $a, b, c, d$ are  all  positive,  then $f(x, y)≤1$ for  all $(x, y)∈\mathbb{R^2}\backslash\{(0,0)\}$ . $3.$ Show that if $\frac{a}{c}+\frac{b}{d}>1$ , then $$\lim_{(x,y)→(0,0)}f(x, y) = 0.$$ Before I start to prove this, i'm thinking why $1-3$ together implies $$\forall a,b,c,d>0,\lim_{(x,y)→(0,0)}\frac{|x|^a|y|^b}{|x|^c+|y|^d} \text{ exists and equals zero}\Leftrightarrow \frac{a}{c}+\frac{b}{d}>1$$ "" $1.$ "" looks like the contrapositive of direction "" $\Rightarrow$ "", actually, "" $1.$ "" implies "" $\Rightarrow$ "", but "" $\Rightarrow$ "" doesn't implies "" $1.$ "", this makes the statement stronger, which is good. To show "" $1.$ "" implies "" $\Rightarrow$ "" Let $f(x,y)=\frac{|x|^a|y|^b}{|x|^c+|y|^d}\text{ and },a,b,c,d > 0$ , assume "" $1.$ "" we have $$\frac{a}{c}+\frac{b}{d}\le1\rightarrow \lim_{(x,y)→(0,0)} f(x,y) \text{ not exists}$$ $$\Rightarrow(\frac{a}{c}+\frac{b}{d}\le1 \wedge \lim_{(x,y)→(0,0)} f(x,y)=0)\rightarrow \lim_{(x,y)→(0,0)} f(x,y) \text{ not exists}$$ Since $((a \wedge b)\rightarrow c)\Leftrightarrow(a\rightarrow(\neg b \vee c))$ we have: $$\Leftrightarrow\frac{a}{c}+\frac{b}{d}\le1\rightarrow (\lim_{(x,y)→(0,0)} f(x,y) \text{ not exists} \vee \lim_{(x,y)→(0,0)} f(x,y)\neq0)$$ Which is the contrapositive of "" $\Rightarrow$ "", so this make sense $\dots$ logically. $\tag*{$\square$}$ Then I suppose "" $2.$ "" and "" $3.$ "" together should implies direction "" $\Leftarrow$ "". "" $2.$ "" states the following: $$\forall a,b,c,d>0, \frac{a}{c}+\frac{b}{d}=1\rightarrow \forall (x,y)\in\mathbb{R^2}\backslash\{(0,0)\},f(x,y)\le 1$$ "" $3.$ "" says that: $$\forall a,b,c,d>0,\frac{a}{c}+\frac{b}{d}>1\rightarrow\lim_{(x,y)→(0,0)}f(x, y) = 0.$$ And together should implies "" $\Rightarrow$ "": $$\forall a,b,c,d>0,\frac{a}{c}+\frac{b}{d}>1\rightarrow\lim_{(x,y)→(0,0)} f(x,y) \text{ exists} \wedge \lim_{(x,y)→(0,0)} f(x,y)=0$$ Proof. Assume "" $3.$ "" have: $$\forall a,b,c,d>0,\frac{a}{c}+\frac{b}{d}>1\rightarrow\lim_{(x,y)→(0,0)}f(x, y) = 0.$$ Since $$\lim_{(x,y)→(0,0)} f(x,y)=0\rightarrow \lim_{(x,y)→(0,0)} f(x,y) \text{ exists}$$ Directly implies "" $\Rightarrow$ "" $$\forall a,b,c,d>0,\frac{a}{c}+\frac{b}{d}>1\rightarrow\lim_{(x,y)→(0,0)} f(x,y) \text{ exists} \wedge \lim_{(x,y)→(0,0)} f(x,y)=0\tag*{$\square$}$$ $1.$ $$\text{WTS }\forall a,b,c,d>0,\frac{a}{c}+\frac{b}{d}\le1\rightarrow \lim_{(x,y)→(0,0)} f(x,y) \text{ not exists}$$ Proof. Let $a,b,c,d\in\mathbb(0,\infty)\cap{\mathbb{R}}, S=\mathbb{R^2}\backslash\{(0,0)\}$ Assume $$\frac{a}{c}+\frac{b}{d}\le1$$ Show the limit does not exist Let $x=t^\frac{1}{c}, y=mt^\frac{1}{d}$ where $m\ge0$ , we have the following $$\lim_{(x,y)→(0,0)}f(x,y)=\lim_{t→0}\frac{|t^\frac{1}{c}|^a|mt^\frac{1}{d}|^b}{|t^\frac{1}{c}|^c+|mt^\frac{1}{d}|^d}$$ Try approch $t$ from right side, so everything is positive, then we have: $$\lim_{t→0^+}\frac{mt^{\frac{a}{c}+\frac{b}{d}}}{(m+1)t} =\lim_{t→0^+}\frac{1}{m+1}t^{\frac{a}{c}+\frac{b}{d}-1}$$ Consider two cases: Case 1: $\frac{a}{c}+\frac{b}{d}=1$ Have $$\lim_{t→0^+}\frac{1}{m+1}t^{0}=\frac{1}{m+1}$$ The limit depend on the value of $m$ , that implies limit d.n.e Case 2: $\frac{a}{c}+\frac{b}{d}<1$ Have $\frac{a}{c}+\frac{b}{d}-1$ is negative, implies limit diviges, that $$\lim_{t→0^+}\frac{1}{m+1}t^{\frac{a}{c}+\frac{b}{d}-1}=\infty$$ Therefore in both cases limit d.n.e $\tag*{$\square$}$ $2.$ $$\text{WTS }\forall a,b,c,d>0, \frac{a}{c}+\frac{b}{d}=1\rightarrow \forall (x,y)\in\mathbb{R^2}\backslash\{(0,0)\},f(x,y)=\frac{|x|^a|y|^b}{|x|^c+|y|^d}\le 1$$ Maybe I can use Young's Theorem here $$\forall w,z\in\mathbb{R},t\in[0,1]\cap\mathbb{R}, w^tz^{1−t}≤tw+ (1−t)z$$ Let $p=\frac{a}{c}>0,q=\frac{b}{d}>0,r=|x|^c,s=|y|^d$ , have $$\frac{|x|^a|y|^b}{|x|^c+|y|^d}=\frac{(|x|^c)^{\frac{a}{c}}(|y|^d)^{\frac{b}{d}}}{|x|^c+|y|^d}=\frac{r^ps^q}{r+s}=\frac{r^ps^{1-p}}{r+s}s^{p+q-1}=\frac{r^ps^{1-p}}{r+s}$$ $$\le\frac{pr+(1-p)s}{r+s}\le(\frac{pr}{r}+\frac{(1-p)s}{s})=1\tag*{$\square$}$$ $3.$ $$\forall a,b,c,d>0,\frac{a}{c}+\frac{b}{d}>1\rightarrow\lim_{(x,y)→(0,0)}f(x, y) = 0.$$ Proof. Let $a,b,c,d\in\mathbb(0,\infty)\cap{\mathbb{R}}$ Assume $\frac{a}{c}+\frac{b}{d}>1$ Show $\lim_{(x,y)→(0,0)} \frac{|x|^a|y|^b}{|x|^c+|y|^d}=0$ Let $p = \frac{ad}{c}-d+b$ , that $\frac{a}{c}+\frac{b-p}{d}=1$ , we can apply 2) on this Implies $0\le\frac{|x|^a|y|^{b-p}}{|x|^c+|y|^d}\le1$ Have $$\lim_{(x,y)→(0,0)} \frac{|x|^a|y|^b}{|x|^c+|y|^d}$$ $$=\lim_{(x,y)→(0,0)} |y|^p\frac{|x|^a|y|^{b-p}}{|x|^c+|y|^d}=0\tag*{$\square$}$$","Rational Limit Theorem. For , with positive, We will break this down into three parts:  first proving one direction by our techniques to show limits don’t exist, then using a famous inequality to help prove the other direction. As a guideline, each proof can be written in two or three lines. Show that if , then the limit does not exist. For the next two problems, you are free to use the following inequality: Young’s Theorem. For positive real numbers , and any , Show  that  if ,  where are  all  positive,  then for  all . Show that if , then Before I start to prove this, i'm thinking why together implies "" "" looks like the contrapositive of direction "" "", actually, "" "" implies "" "", but "" "" doesn't implies "" "", this makes the statement stronger, which is good. To show "" "" implies "" "" Let , assume "" "" we have Since we have: Which is the contrapositive of "" "", so this make sense logically. Then I suppose "" "" and "" "" together should implies direction "" "". "" "" states the following: "" "" says that: And together should implies "" "": Proof. Assume "" "" have: Since Directly implies "" "" Proof. Let Assume Show the limit does not exist Let where , we have the following Try approch from right side, so everything is positive, then we have: Consider two cases: Case 1: Have The limit depend on the value of , that implies limit d.n.e Case 2: Have is negative, implies limit diviges, that Therefore in both cases limit d.n.e Maybe I can use Young's Theorem here Let , have Proof. Let Assume Show Let , that , we can apply 2) on this Implies Have","f(x, y) =\frac{|x|^a|y|^b}{|x|^c+|y|^d} a, b, c, d \lim_{(x,y)→(0,0)}f(x, y) \text{ exists and equals zero}\Leftrightarrow \frac{a}{c}+\frac{b}{d}>1. 1. \frac{a}{c}+\frac{b}{d}≤1 w z 0≤t≤1 w^tz^{1−t}≤tw+ (1−t)z 2. \frac{a}{c}+\frac{b}{d}=  1 a, b, c, d f(x, y)≤1 (x, y)∈\mathbb{R^2}\backslash\{(0,0)\} 3. \frac{a}{c}+\frac{b}{d}>1 \lim_{(x,y)→(0,0)}f(x, y) = 0. 1-3 \forall a,b,c,d>0,\lim_{(x,y)→(0,0)}\frac{|x|^a|y|^b}{|x|^c+|y|^d} \text{ exists and equals zero}\Leftrightarrow \frac{a}{c}+\frac{b}{d}>1 1. \Rightarrow 1. \Rightarrow \Rightarrow 1. 1. \Rightarrow f(x,y)=\frac{|x|^a|y|^b}{|x|^c+|y|^d}\text{ and },a,b,c,d > 0 1. \frac{a}{c}+\frac{b}{d}\le1\rightarrow \lim_{(x,y)→(0,0)} f(x,y) \text{ not exists} \Rightarrow(\frac{a}{c}+\frac{b}{d}\le1 \wedge \lim_{(x,y)→(0,0)} f(x,y)=0)\rightarrow \lim_{(x,y)→(0,0)} f(x,y) \text{ not exists} ((a \wedge b)\rightarrow c)\Leftrightarrow(a\rightarrow(\neg b \vee c)) \Leftrightarrow\frac{a}{c}+\frac{b}{d}\le1\rightarrow (\lim_{(x,y)→(0,0)} f(x,y) \text{ not exists} \vee \lim_{(x,y)→(0,0)} f(x,y)\neq0) \Rightarrow \dots \tag*{\square} 2. 3. \Leftarrow 2. \forall a,b,c,d>0, \frac{a}{c}+\frac{b}{d}=1\rightarrow \forall (x,y)\in\mathbb{R^2}\backslash\{(0,0)\},f(x,y)\le 1 3. \forall a,b,c,d>0,\frac{a}{c}+\frac{b}{d}>1\rightarrow\lim_{(x,y)→(0,0)}f(x, y) = 0. \Rightarrow \forall a,b,c,d>0,\frac{a}{c}+\frac{b}{d}>1\rightarrow\lim_{(x,y)→(0,0)} f(x,y) \text{ exists} \wedge \lim_{(x,y)→(0,0)} f(x,y)=0 3. \forall a,b,c,d>0,\frac{a}{c}+\frac{b}{d}>1\rightarrow\lim_{(x,y)→(0,0)}f(x, y) = 0. \lim_{(x,y)→(0,0)} f(x,y)=0\rightarrow \lim_{(x,y)→(0,0)} f(x,y) \text{ exists} \Rightarrow \forall a,b,c,d>0,\frac{a}{c}+\frac{b}{d}>1\rightarrow\lim_{(x,y)→(0,0)} f(x,y) \text{ exists} \wedge \lim_{(x,y)→(0,0)} f(x,y)=0\tag*{\square} 1. \text{WTS }\forall a,b,c,d>0,\frac{a}{c}+\frac{b}{d}\le1\rightarrow \lim_{(x,y)→(0,0)} f(x,y) \text{ not exists} a,b,c,d\in\mathbb(0,\infty)\cap{\mathbb{R}}, S=\mathbb{R^2}\backslash\{(0,0)\} \frac{a}{c}+\frac{b}{d}\le1 x=t^\frac{1}{c}, y=mt^\frac{1}{d} m\ge0 \lim_{(x,y)→(0,0)}f(x,y)=\lim_{t→0}\frac{|t^\frac{1}{c}|^a|mt^\frac{1}{d}|^b}{|t^\frac{1}{c}|^c+|mt^\frac{1}{d}|^d} t \lim_{t→0^+}\frac{mt^{\frac{a}{c}+\frac{b}{d}}}{(m+1)t}
=\lim_{t→0^+}\frac{1}{m+1}t^{\frac{a}{c}+\frac{b}{d}-1} \frac{a}{c}+\frac{b}{d}=1 \lim_{t→0^+}\frac{1}{m+1}t^{0}=\frac{1}{m+1} m \frac{a}{c}+\frac{b}{d}<1 \frac{a}{c}+\frac{b}{d}-1 \lim_{t→0^+}\frac{1}{m+1}t^{\frac{a}{c}+\frac{b}{d}-1}=\infty \tag*{\square} 2. \text{WTS }\forall a,b,c,d>0, \frac{a}{c}+\frac{b}{d}=1\rightarrow \forall (x,y)\in\mathbb{R^2}\backslash\{(0,0)\},f(x,y)=\frac{|x|^a|y|^b}{|x|^c+|y|^d}\le 1 \forall w,z\in\mathbb{R},t\in[0,1]\cap\mathbb{R}, w^tz^{1−t}≤tw+ (1−t)z p=\frac{a}{c}>0,q=\frac{b}{d}>0,r=|x|^c,s=|y|^d \frac{|x|^a|y|^b}{|x|^c+|y|^d}=\frac{(|x|^c)^{\frac{a}{c}}(|y|^d)^{\frac{b}{d}}}{|x|^c+|y|^d}=\frac{r^ps^q}{r+s}=\frac{r^ps^{1-p}}{r+s}s^{p+q-1}=\frac{r^ps^{1-p}}{r+s} \le\frac{pr+(1-p)s}{r+s}\le(\frac{pr}{r}+\frac{(1-p)s}{s})=1\tag*{\square} 3. \forall a,b,c,d>0,\frac{a}{c}+\frac{b}{d}>1\rightarrow\lim_{(x,y)→(0,0)}f(x, y) = 0. a,b,c,d\in\mathbb(0,\infty)\cap{\mathbb{R}} \frac{a}{c}+\frac{b}{d}>1 \lim_{(x,y)→(0,0)} \frac{|x|^a|y|^b}{|x|^c+|y|^d}=0 p = \frac{ad}{c}-d+b \frac{a}{c}+\frac{b-p}{d}=1 0\le\frac{|x|^a|y|^{b-p}}{|x|^c+|y|^d}\le1 \lim_{(x,y)→(0,0)} \frac{|x|^a|y|^b}{|x|^c+|y|^d} =\lim_{(x,y)→(0,0)} |y|^p\frac{|x|^a|y|^{b-p}}{|x|^c+|y|^d}=0\tag*{\square}","['limits', 'multivariable-calculus', 'proof-verification', 'inequality']"
51,Pet Peeve (notation for limits assumes uniqueness...),Pet Peeve (notation for limits assumes uniqueness...),,"I suppose I should disguise this rant as a question. Q: Are there any calculus books that give the definition of limits in a logically correct manner? (See below for the problem I'm complaining about with the typical presentation.) Teaching baby complex. The book defines "" $\lim_{z\to a}f(z)=w$ "" as usual, and then of course the first result is that limits are unique if they exist: Thm. If $\lim_{z\to a}f(z)=w_1$ and $\lim_{z\to a}f(z)=w_2$ then $w_1=w_2$ . This appears literally trivial: ""Proof. "" $w_1=\lim_{z\to  a}f(z)=w_2$ , qed. There are a few sharp students in the class, actually interested in math, so I feel I should point out why that's bogus; the theorem as stated looks like it has zero content. The point is we have no business using the notation $\lim_{z\to a}f(z)$ until after we've proved uniqueness, because the notation, especially if read "" the limit..."" presupposes uniqueness. To do it right we should instead  define "" $f(z)\to w$ as $z\to a$ "", prove uniqueness, and then introduce the notation $\lim$ . Making it clear why uniqueness is actually something that requires proof...","I suppose I should disguise this rant as a question. Q: Are there any calculus books that give the definition of limits in a logically correct manner? (See below for the problem I'm complaining about with the typical presentation.) Teaching baby complex. The book defines "" "" as usual, and then of course the first result is that limits are unique if they exist: Thm. If and then . This appears literally trivial: ""Proof. "" , qed. There are a few sharp students in the class, actually interested in math, so I feel I should point out why that's bogus; the theorem as stated looks like it has zero content. The point is we have no business using the notation until after we've proved uniqueness, because the notation, especially if read "" the limit..."" presupposes uniqueness. To do it right we should instead  define "" as "", prove uniqueness, and then introduce the notation . Making it clear why uniqueness is actually something that requires proof...",\lim_{z\to a}f(z)=w \lim_{z\to a}f(z)=w_1 \lim_{z\to a}f(z)=w_2 w_1=w_2 w_1=\lim_{z\to  a}f(z)=w_2 \lim_{z\to a}f(z) f(z)\to w z\to a \lim,"['limits', 'reference-request']"
52,How much rigour is this proof of multivariable chain rule?,How much rigour is this proof of multivariable chain rule?,,"I have seen some statements and proofs of multivariable chain rule in various sites. I ""somewhat"" grasp them but seems too complicated for me to fully understand them. To make my life easy, I have come up with a simple statement and a simple ""rigorous"" proof of multivariable chain rule . Please explain to what extent it is plausible. PLEASE NOTE: In my statement of multivariable chain rule "" $f[x(t),y(t)]$ is differentiable at $t=a$ "" is a condition rather than a provable result. I think it is the only way in which my statement differs from the usual statement. I am a graduate Physics student and everywhere in my text (Electricity and Magnetism, Thermodynamics, etc) there is no mention of differentiability even though multivariable chain rule is used quite often. It seems to me the book just assumes that all functions used in the book are differentiable everywhere. So with this little change in the statement, I do not think it will have any affect on my rigorous Physics study. Am I right? $\text{}$ Statement: If $f[x(t),y(t)]$ , $x(t)$ and $y(t)$ are differentiable at $t=a$ ; and $f(x,y)$ is differentiable at $x(t)=x(a)$ and $y(t)=y(a)$ ; then at $t=a$ $$\dfrac{df[x(t),y(t)]}{dt}=\dfrac{\partial f[x(t),y(t)]}{\partial x(t)}\ \dfrac{dx(t)}{dt}+\dfrac{\partial f[x(t),y(t)]}{\partial y(t)}\ \dfrac{dy(t)}{dt}$$ $\text{}$ Proof: \begin{align} \Delta f[x,y]&=f[x+\Delta x, y+\Delta y]-f[x,y]\\ &=f[x+\Delta x, y+\Delta y]-f[x,y+\Delta y]+f[x,y+\Delta y]-f[x,y]\\ &=\delta f_x[x,y]+\delta f_y[x,y]\\ \Rightarrow\ \Delta f[x(t),y(t)]&=\delta f_x[x(t),y(t)]+\delta f_y[x(t),y(t)]\\ \Rightarrow \dfrac{\Delta f[x(t),y(t)]}{\Delta t}&=\dfrac{\delta f_x[x(t),y(t)]}{\delta x(t)}\dfrac{\Delta x(t)}{\Delta t}+...\\ \Rightarrow \lim\limits_{\Delta t \to 0} \dfrac{\Delta f[x(t),y(t)]}{\Delta t}&= \lim\limits_{\Delta t \to 0} \left( \dfrac{\delta f_x[x(t),y(t)]}{\delta x(t)}\dfrac{\Delta x(t)}{\Delta t}   \right)+...\\ \Rightarrow \lim\limits_{\Delta t \to 0} \dfrac{\Delta f[x(t),y(t)]}{\Delta t}&= \lim\limits_{\Delta t \to 0} \left( \dfrac{\delta f_x[x(t),y(t)]}{\delta x(t)} \right) \lim\limits_{\Delta t \to 0} \left(    \dfrac{\Delta x(t)}{\Delta t}   \right)+...\\ &\text{}\\ &\text{It is given that $x(t)$ is differentiable at $t=a$.}\\ &\text{Therefore $\lim\limits_{\Delta t \to 0} \dfrac{\Delta x(t)}{\Delta t}$ exists.}\\ &\text{Therefore when $\Delta t \to 0$, $\Delta x(t) \to 0$.}\\ &\text{}\\ \Rightarrow \lim\limits_{\Delta t \to 0} \dfrac{\Delta f[x(t),y(t)]}{\Delta t}&= \lim\limits_{\Delta x(t) \to 0} \left( \dfrac{\delta f_x[x(t),y(t)]}{\delta x(t)} \right) \lim\limits_{\Delta t \to 0} \left(    \dfrac{\Delta x(t)}{\Delta t}   \right)+...\\ &\text{}\\ &\text{It is given that $f[x(t),y(t)]$, $x(t)$ and $y(t)$ are differentiable at $t=a$;} \\ &\text{and $f(x,y)$ is differentiable at $x(t)=x(a)$ and $y(t)=y(a)$}\\ &\text{}\\ &\text{Therefore we can replace the limits with derivatives.}\\ &\text{}\\ \Rightarrow \dfrac{df[x(t),y(t)]}{dt} &= \dfrac{\partial f_x[x(t),y(t)]}{\partial x(t)}\ \dfrac{dx(t)}{dt}  +...\\ \end{align} So this is the statement and proof I have come up with. Again, please explain to what extent is it plausible (whether it is completely or partially rigour).","I have seen some statements and proofs of multivariable chain rule in various sites. I ""somewhat"" grasp them but seems too complicated for me to fully understand them. To make my life easy, I have come up with a simple statement and a simple ""rigorous"" proof of multivariable chain rule . Please explain to what extent it is plausible. PLEASE NOTE: In my statement of multivariable chain rule "" is differentiable at "" is a condition rather than a provable result. I think it is the only way in which my statement differs from the usual statement. I am a graduate Physics student and everywhere in my text (Electricity and Magnetism, Thermodynamics, etc) there is no mention of differentiability even though multivariable chain rule is used quite often. It seems to me the book just assumes that all functions used in the book are differentiable everywhere. So with this little change in the statement, I do not think it will have any affect on my rigorous Physics study. Am I right? Statement: If , and are differentiable at ; and is differentiable at and ; then at Proof: So this is the statement and proof I have come up with. Again, please explain to what extent is it plausible (whether it is completely or partially rigour).","f[x(t),y(t)] t=a \text{} f[x(t),y(t)] x(t) y(t) t=a f(x,y) x(t)=x(a) y(t)=y(a) t=a \dfrac{df[x(t),y(t)]}{dt}=\dfrac{\partial f[x(t),y(t)]}{\partial x(t)}\ \dfrac{dx(t)}{dt}+\dfrac{\partial f[x(t),y(t)]}{\partial y(t)}\ \dfrac{dy(t)}{dt} \text{} \begin{align}
\Delta f[x,y]&=f[x+\Delta x, y+\Delta y]-f[x,y]\\
&=f[x+\Delta x, y+\Delta y]-f[x,y+\Delta y]+f[x,y+\Delta y]-f[x,y]\\
&=\delta f_x[x,y]+\delta f_y[x,y]\\
\Rightarrow\ \Delta f[x(t),y(t)]&=\delta f_x[x(t),y(t)]+\delta f_y[x(t),y(t)]\\
\Rightarrow \dfrac{\Delta f[x(t),y(t)]}{\Delta t}&=\dfrac{\delta f_x[x(t),y(t)]}{\delta x(t)}\dfrac{\Delta x(t)}{\Delta t}+...\\
\Rightarrow \lim\limits_{\Delta t \to 0} \dfrac{\Delta f[x(t),y(t)]}{\Delta t}&= \lim\limits_{\Delta t \to 0} \left( \dfrac{\delta f_x[x(t),y(t)]}{\delta x(t)}\dfrac{\Delta x(t)}{\Delta t}   \right)+...\\
\Rightarrow \lim\limits_{\Delta t \to 0} \dfrac{\Delta f[x(t),y(t)]}{\Delta t}&=
\lim\limits_{\Delta t \to 0} \left( \dfrac{\delta f_x[x(t),y(t)]}{\delta x(t)} \right)
\lim\limits_{\Delta t \to 0} \left(    \dfrac{\Delta x(t)}{\Delta t}   \right)+...\\
&\text{}\\
&\text{It is given that x(t) is differentiable at t=a.}\\
&\text{Therefore \lim\limits_{\Delta t \to 0} \dfrac{\Delta x(t)}{\Delta t} exists.}\\
&\text{Therefore when \Delta t \to 0, \Delta x(t) \to 0.}\\
&\text{}\\
\Rightarrow \lim\limits_{\Delta t \to 0} \dfrac{\Delta f[x(t),y(t)]}{\Delta t}&=
\lim\limits_{\Delta x(t) \to 0} \left( \dfrac{\delta f_x[x(t),y(t)]}{\delta x(t)} \right)
\lim\limits_{\Delta t \to 0} \left(    \dfrac{\Delta x(t)}{\Delta t}   \right)+...\\
&\text{}\\
&\text{It is given that f[x(t),y(t)], x(t) and y(t) are differentiable at t=a;} \\
&\text{and f(x,y) is differentiable at x(t)=x(a) and y(t)=y(a)}\\
&\text{}\\
&\text{Therefore we can replace the limits with derivatives.}\\
&\text{}\\
\Rightarrow \dfrac{df[x(t),y(t)]}{dt} &=
\dfrac{\partial f_x[x(t),y(t)]}{\partial x(t)}\
\dfrac{dx(t)}{dt}  +...\\
\end{align}","['limits', 'multivariable-calculus', 'proof-verification', 'partial-derivative', 'chain-rule']"
53,If $f(x) =\frac{1}{3} ( \frac {5}{f(x+2)}+f(x+1))$ then $\underset{x \to \infty}{\lim}f(x) = ?$,If  then,f(x) =\frac{1}{3} ( \frac {5}{f(x+2)}+f(x+1)) \underset{x \to \infty}{\lim}f(x) = ?,If $f(x) =\frac{1}{3} ( \frac {5}{f(x+2)}+f(x+1))$ and $f(x) > 0$ for all $x \in \mathbb R$    then $\underset{x \to \infty}{\lim}f(x) = ?$ can anyone please give me a hint to find it? I know how to find the limit when the limit exists. But I have no idea how to prove it's existence.,If $f(x) =\frac{1}{3} ( \frac {5}{f(x+2)}+f(x+1))$ and $f(x) > 0$ for all $x \in \mathbb R$    then $\underset{x \to \infty}{\lim}f(x) = ?$ can anyone please give me a hint to find it? I know how to find the limit when the limit exists. But I have no idea how to prove it's existence.,,"['limits', 'functions', 'limits-without-lhopital']"
54,Prove or disprove : $\sqrt{1 + \sqrt{4+\sqrt{16+\sqrt{256...}}}} = \sqrt{2+\sqrt{5}}$,Prove or disprove :,\sqrt{1 + \sqrt{4+\sqrt{16+\sqrt{256...}}}} = \sqrt{2+\sqrt{5}},"Edit: My initial question was regarding the expressions: $\sqrt{1 + \sqrt{4+\sqrt{16+\sqrt{64... + \sqrt{4^{n}}}}}}$ And in general: $\sqrt{1 + \sqrt{k+\sqrt{k^2+\sqrt{k^3... + \sqrt{k^{n}}}}}}$ And their limits, however, my working out was wrong. So, if someone can explain how to find the values of the above two expressions, that would be great! This is just something I've noticed playing around with the radicals, and I honestly don't have much idea on how to prove it. These were my ideas: Let $\sqrt{1 + \sqrt{4+\sqrt{16+\sqrt{256... + \sqrt{4^{2^n}}}}}} = A_n$ My logic was as follows: $A_n = \sqrt{1 + \sqrt{4+\sqrt{16+\sqrt{256... + \sqrt{4^{2^n}}}}}}$ $A_n^2 = 1 + \sqrt{4+\sqrt{16+\sqrt{256... + \sqrt{4^{2^n}}}}}$ $A_n^2 = 1 + 2\sqrt{1+\frac{1}{4}\sqrt{16+\sqrt{256... + \sqrt{4^{2^n}}}}}$ $A_n^2 = 1 + 2\sqrt{1+\sqrt{1+\frac{1}{16}\sqrt{256+... + \sqrt{4^{2^n}}}}}$ $A_n^2 = 1 + 2\sqrt{1+\sqrt{1+\sqrt{1+... + \sqrt{1}}}}$ $A_{\infty}^2 = 1 + 2\sqrt{1+\sqrt{1+\sqrt{1+... }}} = 1 + 2\phi = 2 + \sqrt{5}$ $A_{\infty} = \sqrt{2 + \sqrt{5}}$ Then, rather than this specific case, how would we evaluate: $L = \sqrt{1 + \sqrt{k+\sqrt{k^2+\sqrt{k^4+\sqrt{k^8+...}}}}}$ Would we also get $L = \sqrt{\sqrt{k}\phi + 1}$? And what about expressions such as: $\sqrt{1 + \sqrt{2+\sqrt{3+...}}}$ Thanks for any answers and guidance!","Edit: My initial question was regarding the expressions: $\sqrt{1 + \sqrt{4+\sqrt{16+\sqrt{64... + \sqrt{4^{n}}}}}}$ And in general: $\sqrt{1 + \sqrt{k+\sqrt{k^2+\sqrt{k^3... + \sqrt{k^{n}}}}}}$ And their limits, however, my working out was wrong. So, if someone can explain how to find the values of the above two expressions, that would be great! This is just something I've noticed playing around with the radicals, and I honestly don't have much idea on how to prove it. These were my ideas: Let $\sqrt{1 + \sqrt{4+\sqrt{16+\sqrt{256... + \sqrt{4^{2^n}}}}}} = A_n$ My logic was as follows: $A_n = \sqrt{1 + \sqrt{4+\sqrt{16+\sqrt{256... + \sqrt{4^{2^n}}}}}}$ $A_n^2 = 1 + \sqrt{4+\sqrt{16+\sqrt{256... + \sqrt{4^{2^n}}}}}$ $A_n^2 = 1 + 2\sqrt{1+\frac{1}{4}\sqrt{16+\sqrt{256... + \sqrt{4^{2^n}}}}}$ $A_n^2 = 1 + 2\sqrt{1+\sqrt{1+\frac{1}{16}\sqrt{256+... + \sqrt{4^{2^n}}}}}$ $A_n^2 = 1 + 2\sqrt{1+\sqrt{1+\sqrt{1+... + \sqrt{1}}}}$ $A_{\infty}^2 = 1 + 2\sqrt{1+\sqrt{1+\sqrt{1+... }}} = 1 + 2\phi = 2 + \sqrt{5}$ $A_{\infty} = \sqrt{2 + \sqrt{5}}$ Then, rather than this specific case, how would we evaluate: $L = \sqrt{1 + \sqrt{k+\sqrt{k^2+\sqrt{k^4+\sqrt{k^8+...}}}}}$ Would we also get $L = \sqrt{\sqrt{k}\phi + 1}$? And what about expressions such as: $\sqrt{1 + \sqrt{2+\sqrt{3+...}}}$ Thanks for any answers and guidance!",,"['limits', 'radicals', 'limits-without-lhopital', 'nested-radicals']"
55,A limit involving $\cot$ that seemingly shouldn't exist,A limit involving  that seemingly shouldn't exist,\cot,"According to Wolfram Alpha , $$\lim_{x \to \infty} \frac{x - \cot x}{x} =1.$$ But does the limit even exist? Isn't $\frac{x - \cot x}{x}$ unbounded near $x= n \pi$ for all $n \in \mathbb{N}$? Assuming that the limit doesn't actually exist, what might explain why Wolfram Alpha thinks that it does exist?","According to Wolfram Alpha , $$\lim_{x \to \infty} \frac{x - \cot x}{x} =1.$$ But does the limit even exist? Isn't $\frac{x - \cot x}{x}$ unbounded near $x= n \pi$ for all $n \in \mathbb{N}$? Assuming that the limit doesn't actually exist, what might explain why Wolfram Alpha thinks that it does exist?",,['limits']
56,What is the product of the areas of every regular polygon inscribed in a circle of area $1$?,What is the product of the areas of every regular polygon inscribed in a circle of area ?,1,"What is a closed form of $P=\prod\limits_{k=3}^{\infty}\frac{k}{2\pi}\sin{\left(\frac{2\pi}{k}\right)}\approx 0.05934871...$ ? This is the product of the areas of every regular polygon inscribed in a circle of area $1$ . I do not know how to evaluate this product. I have been trying to use complex numbers, to no avail. Remarks: $P$ is remarkably close to $\frac{1}{6\pi-2}\approx 0.0593487\color{red}{45}...$ But this is not a closed form, because the product decreases as the number of factors increases, and according to desmos, $$\prod\limits_{k=3}^{10^8}\frac{k}{2\pi}\sin{\left(\frac{2\pi}{k}\right)}<\frac{1}{6\pi-2}$$ There is a related question: What is the product of the circumferences of every regular polygon inscribed in a circle of circumference $1$ ? This is $P'=\prod\limits_{k=3}^{\infty}\frac{k}{\pi}\sin{\left(\frac{\pi}{k}\right)}\approx 0.51633595...$ The ratio of $P$ to $P'$ is $\prod\limits_{k=3}^{\infty}\cos{\left(\frac{\pi}{k}\right)}\approx 0.11494204...$ which is the Kepler-Boukamp constant and has no known closed form. But is there a closed form of $P$ ? EDIT The product of the areas of every odd -gon inscribed in a circle of area $1$ , equals $\frac{\pi}{2}\times$ the Kepler-Boukamp constant , as shown here .","What is a closed form of ? This is the product of the areas of every regular polygon inscribed in a circle of area . I do not know how to evaluate this product. I have been trying to use complex numbers, to no avail. Remarks: is remarkably close to But this is not a closed form, because the product decreases as the number of factors increases, and according to desmos, There is a related question: What is the product of the circumferences of every regular polygon inscribed in a circle of circumference ? This is The ratio of to is which is the Kepler-Boukamp constant and has no known closed form. But is there a closed form of ? EDIT The product of the areas of every odd -gon inscribed in a circle of area , equals the Kepler-Boukamp constant , as shown here .",P=\prod\limits_{k=3}^{\infty}\frac{k}{2\pi}\sin{\left(\frac{2\pi}{k}\right)}\approx 0.05934871... 1 P \frac{1}{6\pi-2}\approx 0.0593487\color{red}{45}... \prod\limits_{k=3}^{10^8}\frac{k}{2\pi}\sin{\left(\frac{2\pi}{k}\right)}<\frac{1}{6\pi-2} 1 P'=\prod\limits_{k=3}^{\infty}\frac{k}{\pi}\sin{\left(\frac{\pi}{k}\right)}\approx 0.51633595... P P' \prod\limits_{k=3}^{\infty}\cos{\left(\frac{\pi}{k}\right)}\approx 0.11494204... P 1 \frac{\pi}{2}\times,"['limits', 'area', 'closed-form', 'infinite-product', 'polygons']"
57,The variation of a Ukrainian Olympiad problem: 10982,The variation of a Ukrainian Olympiad problem: 10982,,"Given a recursion $a_{n+ 1}= \dfrac{a_{n}}{n}+ \dfrac{n}{a_{n}}$ with $a_{1}= 1.$ Prove that $$\lim a_{n}^{2}- n= \frac{1}{2}$$ Source: StachMath/@RiverLi _ The limit and asymptotic analysis of $a_n^2 - n$ from $a_{n+1} = \frac{a_n}{n} + \frac{n}{a_n}$ The original problem already has an answer, I'll suggest this way of thinking, which is not mine, but @twelve_sakuya Let $b_{n}:=a_{n}^{2}- n,$ so $$a_{n+ 1}= \frac{a_{n}}{n}+ \frac{n}{a_{n}}\Leftrightarrow b_{n+ 1}- \frac{1}{2}= -\frac{n}{b_{n}+ n}\left ( b_{n}- \frac{1}{2} \right )+ \frac{b_{n}}{2\left ( b_{n}+ n \right )}+ \frac{b_{n}}{n^{2}}+ \frac{1}{n}$$ That means $\left | b_{n+ 1}- \dfrac{1}{2} \right |\leq\dfrac{n}{\left | b_{n}+ n \right |}\left | b_{n}- \dfrac{1}{2} \right |+ \dfrac{\left | b_{n} \right |}{2\left | b_{n}+ n \right |}+ \dfrac{\left | b_{n} \right |}{n^{2}}+ \dfrac{1}{n}.$ Therefore, if we can get the evaluations of $\left | b_{n} \right |$ or $\dfrac{\left | b_{n} \right |}{n},$ maybe there exists a number $\beta\in\left ( 0, 1 \right )$ so that $$\left | b_{n}- \frac{1}{2} \right |\leq\beta^{n}B\left ( n \right )\rightarrow 0\,{\rm as}\,n\rightarrow\infty$$ My friend has no confidence to continue, he also said that is the variation of a Ukrainian Olympiad problem #10982 (I searched and got the result, but I couldn't access it). I need to the help, thanks a real lot !","Given a recursion with Prove that Source: StachMath/@RiverLi _ The limit and asymptotic analysis of $a_n^2 - n$ from $a_{n+1} = \frac{a_n}{n} + \frac{n}{a_n}$ The original problem already has an answer, I'll suggest this way of thinking, which is not mine, but @twelve_sakuya Let so That means Therefore, if we can get the evaluations of or maybe there exists a number so that My friend has no confidence to continue, he also said that is the variation of a Ukrainian Olympiad problem #10982 (I searched and got the result, but I couldn't access it). I need to the help, thanks a real lot !","a_{n+ 1}= \dfrac{a_{n}}{n}+ \dfrac{n}{a_{n}} a_{1}= 1. \lim a_{n}^{2}- n= \frac{1}{2} b_{n}:=a_{n}^{2}- n, a_{n+ 1}= \frac{a_{n}}{n}+ \frac{n}{a_{n}}\Leftrightarrow b_{n+ 1}- \frac{1}{2}= -\frac{n}{b_{n}+ n}\left ( b_{n}- \frac{1}{2} \right )+ \frac{b_{n}}{2\left ( b_{n}+ n \right )}+ \frac{b_{n}}{n^{2}}+ \frac{1}{n} \left | b_{n+ 1}- \dfrac{1}{2} \right |\leq\dfrac{n}{\left | b_{n}+ n \right |}\left | b_{n}- \dfrac{1}{2} \right |+ \dfrac{\left | b_{n} \right |}{2\left | b_{n}+ n \right |}+ \dfrac{\left | b_{n} \right |}{n^{2}}+ \dfrac{1}{n}. \left | b_{n} \right | \dfrac{\left | b_{n} \right |}{n}, \beta\in\left ( 0, 1 \right ) \left | b_{n}- \frac{1}{2} \right |\leq\beta^{n}B\left ( n \right )\rightarrow 0\,{\rm as}\,n\rightarrow\infty","['limits', 'asymptotics']"
58,Does $\lim\limits_{x\to0}\operatorname{sgn} (x)$ exist?,Does  exist?,\lim\limits_{x\to0}\operatorname{sgn} (x),"I have a problem with this exercise Does this limit exist? $$\lim\limits_{x\to0} \operatorname{sgn} (x)$$ this limit should exist and its value is $0$ according to our textbook. It is also written, that we can prove it by using one-sided limits. And there is a problem, because as I see it $$\lim_{x\to0^-} \operatorname{sgn} (x) = -1$$ $$\lim_{x\to0^+} \operatorname{sgn} (x) = 1$$ (Because the limit goes very close to $0$, but it never reaches it. I also think it is very similar to prove of non-existence $\displaystyle \lim_{x\to0} \sin\frac 1 x$) I also tried online limit calculators and they said, that one-sided limits equals $0$. Could you help me find a problem in my approach? Thanks for your time!","I have a problem with this exercise Does this limit exist? $$\lim\limits_{x\to0} \operatorname{sgn} (x)$$ this limit should exist and its value is $0$ according to our textbook. It is also written, that we can prove it by using one-sided limits. And there is a problem, because as I see it $$\lim_{x\to0^-} \operatorname{sgn} (x) = -1$$ $$\lim_{x\to0^+} \operatorname{sgn} (x) = 1$$ (Because the limit goes very close to $0$, but it never reaches it. I also think it is very similar to prove of non-existence $\displaystyle \lim_{x\to0} \sin\frac 1 x$) I also tried online limit calculators and they said, that one-sided limits equals $0$. Could you help me find a problem in my approach? Thanks for your time!",,['limits']
59,For what $n$ is $\sum_{i=1}^\infty \frac{\cos (it)}{i^n}$ bounded and why doesn't a sine behave the same way?,For what  is  bounded and why doesn't a sine behave the same way?,n \sum_{i=1}^\infty \frac{\cos (it)}{i^n},"I've been looking at a parametric curve $$\pmatrix{X \\ Y}=\pmatrix{\sum_{i=1}^N \frac{\cos (it)}{i^n} \\ \sum_{i=1}^N \frac{\sin (it)}{i^n}}$$ where, for the plots below, $N$ runs from $1 \rightarrow 300$ and $n=1,2$, respectively. It seems that $X$ is unbounded/divergent in one, and bounded/convergent in the other, whereas $Y$ seems to be indifferent to $n$ and always be bounded. I use the term ""bounded"" to encapsulate that several different properties ($ \max (X),$ the area enclosed by $X,...$) could be a measure of this. My question: For which $n \in \mathbb{R}$ is $X,Y$ bounded, and why does the sine always seem to be bounded? I guess it could have something to do with this post, but I don't quite see how the argument in that post would be used for this problem, mostly because of my $t$, but perhaps it doesn't change anything? As a sidenote, I tried introducing a $(-1)^{i+1}$ in the sums, but all this did was to mirror the graph around the left-most point of the original graph, so that $X$ diverged to $-\infty$ instead. Any ideas of why this is the case? $n=1$ $n=2$ Oh, and here is a nice and wobbly version with $(-1)^{i}$: $n=1$ Any insights are much appreciated!","I've been looking at a parametric curve $$\pmatrix{X \\ Y}=\pmatrix{\sum_{i=1}^N \frac{\cos (it)}{i^n} \\ \sum_{i=1}^N \frac{\sin (it)}{i^n}}$$ where, for the plots below, $N$ runs from $1 \rightarrow 300$ and $n=1,2$, respectively. It seems that $X$ is unbounded/divergent in one, and bounded/convergent in the other, whereas $Y$ seems to be indifferent to $n$ and always be bounded. I use the term ""bounded"" to encapsulate that several different properties ($ \max (X),$ the area enclosed by $X,...$) could be a measure of this. My question: For which $n \in \mathbb{R}$ is $X,Y$ bounded, and why does the sine always seem to be bounded? I guess it could have something to do with this post, but I don't quite see how the argument in that post would be used for this problem, mostly because of my $t$, but perhaps it doesn't change anything? As a sidenote, I tried introducing a $(-1)^{i+1}$ in the sums, but all this did was to mirror the graph around the left-most point of the original graph, so that $X$ diverged to $-\infty$ instead. Any ideas of why this is the case? $n=1$ $n=2$ Oh, and here is a nice and wobbly version with $(-1)^{i}$: $n=1$ Any insights are much appreciated!",,"['limits', 'parametric']"
60,Does this sequence of polynomials have a limit?,Does this sequence of polynomials have a limit?,,"Consider the sequence of polynomials $p_n$ defined as follows: $p_n$ is the unique polynomial of degree $2n+1$ satisfying $$p_n(0) = 0$$ $$p_n(1) = 1$$ $$p_n^{(k)}(0) = p_n^{(k)}(1) = 0 \text{ for $k=1$ to $n$}$$ where $p_n^{(k)}$ is the $k$th derivative of $p_n$. For example, $$p_0(x)=x$$ $$p_1(x)=-2 x^3+3 x^2$$ $$p_2(x)=6 x^5-15 x^4+10 x^3$$ $$p_3(x)=-20 x^7+70 x^6-84 x^5+35 x^4$$ $$p_4(x)=70 x^9-315 x^8+540 x^7-420 x^6+126 x^5$$ $$p_5(x)=-252 x^{11}+1386 x^{10}-3080 x^9+3465 x^8-1980 x^7+462 x^6$$ To give some intuition, here is an animated plot of $p_n$ for $n=0$ to $50$. How can I determine, with proof, $\lim_{n \to \infty} p_n(x)$ (if it exists)? I've never worked with functions implicitly defined in this way before, and I have no idea where to begin.","Consider the sequence of polynomials $p_n$ defined as follows: $p_n$ is the unique polynomial of degree $2n+1$ satisfying $$p_n(0) = 0$$ $$p_n(1) = 1$$ $$p_n^{(k)}(0) = p_n^{(k)}(1) = 0 \text{ for $k=1$ to $n$}$$ where $p_n^{(k)}$ is the $k$th derivative of $p_n$. For example, $$p_0(x)=x$$ $$p_1(x)=-2 x^3+3 x^2$$ $$p_2(x)=6 x^5-15 x^4+10 x^3$$ $$p_3(x)=-20 x^7+70 x^6-84 x^5+35 x^4$$ $$p_4(x)=70 x^9-315 x^8+540 x^7-420 x^6+126 x^5$$ $$p_5(x)=-252 x^{11}+1386 x^{10}-3080 x^9+3465 x^8-1980 x^7+462 x^6$$ To give some intuition, here is an animated plot of $p_n$ for $n=0$ to $50$. How can I determine, with proof, $\lim_{n \to \infty} p_n(x)$ (if it exists)? I've never worked with functions implicitly defined in this way before, and I have no idea where to begin.",,"['limits', 'polynomials']"
61,Sufficient conditions for differentiability of multivariate functions,Sufficient conditions for differentiability of multivariate functions,,"Claim: If a function $f:\mathbb R^2\to\mathbb R$ has partial derivatives in a neighborhood $D$ of $(x_0,y_0)$ , and if these are continuous at $(x_0,y_0)$ , then $f$ is differentiable at $(x_0,y_0)$ Proof: As long as $(x,y)\in D$ we have $$f(x,y)-f(x_0,y_0)=f(x,y)-f(x_0,y)+f(x_0,y)-f(x_0,y_0)$$ Now applying the mean value theorem gives us, for some $z\in(x_0,x)$ and $w\in(y_0,y)$ $$f(x,y)-f(x_0,y_0)=f'_x(z,y)(x-x_0)+f'_y(x_0,w)(y-y_0)$$ Let $E(x,y)=f'_x(z,y)(x-x_0)+f'_y(x_0,w)(y-y_0)-f'_x(x_0,y_0)(x-x_0)-f'_y(x_0,y_0)(y-y_0)$ Then, $$f(x,y)=f(x_0,y_0)+f'_x(x_0,y_0)(x-x_0)+f'_y(x_0,y_0)(y-y_0)+E(x,y)$$ All we have left to see is that $$\lim_{(x,y)\to (x_0,y_0)}\frac{E(x,y)}{\sqrt{(x-x_0)^2+(y-y_0)^2}}=0$$ By continuity of the partial derivatives for any $\varepsilon>0$ exists $\delta>0$ such that whenever $|x_0-x|<\delta$ and $|y_0-y|<\delta$ we have $|f'_x(x,y)-f'_x(x_0,y_0)|<\frac{\varepsilon}{2}$ and $|f'_y(x,y)-f'_y(x_0,y_0)|<\frac{\varepsilon}{2}$ Therefore, if $|x-x_0|<\min\{\delta,\varepsilon\}$ and $|y-y_0|<\min\{\delta,\varepsilon\}$ then $|E(x,y)|=|f_x'(z,y)(x-x_0)+f'_y(x_0,w)(y-y_0)-f'_x(x_0,y_0)(x-x_0)-f'_y(x_0,y_0)(y-y_0)|<\frac12\varepsilon h + \frac12\varepsilon k<\frac12\varepsilon\varepsilon+\frac12\varepsilon\varepsilon=\varepsilon^2$ Problem: How can we see that $$\lim_{(x,y)\to (x_0,y_0)}\frac{E(x,y)}{\sqrt{(x-x_0)^2+(y-y_0)^2}}=0$$ That is, how can we  see that the error $E$ tends to zero faster that the distance betwee $(x,y)$ and $(x_0,y_0)$ ?","Claim: If a function has partial derivatives in a neighborhood of , and if these are continuous at , then is differentiable at Proof: As long as we have Now applying the mean value theorem gives us, for some and Let Then, All we have left to see is that By continuity of the partial derivatives for any exists such that whenever and we have and Therefore, if and then Problem: How can we see that That is, how can we  see that the error tends to zero faster that the distance betwee and ?","f:\mathbb R^2\to\mathbb R D (x_0,y_0) (x_0,y_0) f (x_0,y_0) (x,y)\in D f(x,y)-f(x_0,y_0)=f(x,y)-f(x_0,y)+f(x_0,y)-f(x_0,y_0) z\in(x_0,x) w\in(y_0,y) f(x,y)-f(x_0,y_0)=f'_x(z,y)(x-x_0)+f'_y(x_0,w)(y-y_0) E(x,y)=f'_x(z,y)(x-x_0)+f'_y(x_0,w)(y-y_0)-f'_x(x_0,y_0)(x-x_0)-f'_y(x_0,y_0)(y-y_0) f(x,y)=f(x_0,y_0)+f'_x(x_0,y_0)(x-x_0)+f'_y(x_0,y_0)(y-y_0)+E(x,y) \lim_{(x,y)\to (x_0,y_0)}\frac{E(x,y)}{\sqrt{(x-x_0)^2+(y-y_0)^2}}=0 \varepsilon>0 \delta>0 |x_0-x|<\delta |y_0-y|<\delta |f'_x(x,y)-f'_x(x_0,y_0)|<\frac{\varepsilon}{2} |f'_y(x,y)-f'_y(x_0,y_0)|<\frac{\varepsilon}{2} |x-x_0|<\min\{\delta,\varepsilon\} |y-y_0|<\min\{\delta,\varepsilon\} |E(x,y)|=|f_x'(z,y)(x-x_0)+f'_y(x_0,w)(y-y_0)-f'_x(x_0,y_0)(x-x_0)-f'_y(x_0,y_0)(y-y_0)|<\frac12\varepsilon h + \frac12\varepsilon k<\frac12\varepsilon\varepsilon+\frac12\varepsilon\varepsilon=\varepsilon^2 \lim_{(x,y)\to (x_0,y_0)}\frac{E(x,y)}{\sqrt{(x-x_0)^2+(y-y_0)^2}}=0 E (x,y) (x_0,y_0)","['limits', 'multivariable-calculus', 'derivatives']"
62,"Prove $\lim\limits_{n \rightarrow \infty} \frac{1}{n} \int_0^n xg(x)\,dx=0$",Prove,"\lim\limits_{n \rightarrow \infty} \frac{1}{n} \int_0^n xg(x)\,dx=0","If $g$ is a Lebesgue integrable function in $E=\lbrack 0,\infty)$ , prove that $$\lim_{n \rightarrow \infty}\frac{1}{n}\int_0^n xg(x)\,dx=0.$$ I want to use the absolute continuity of the Lebesgue integral, i.e, if $\epsilon >0$ there is $\delta > 0$ such as if $|E|<\delta$ (Lebesgue measure) then $\int_E g(x)\,dx<\epsilon.$ I would like to split the integral in the subsets $\lbrack 0 , n-\delta\rbrack$ and $\lbrack n-\delta, n)$ (the last one has measure $\delta$ ) but I think this is wrong. Thank you very much for your help!","If is a Lebesgue integrable function in , prove that I want to use the absolute continuity of the Lebesgue integral, i.e, if there is such as if (Lebesgue measure) then I would like to split the integral in the subsets and (the last one has measure ) but I think this is wrong. Thank you very much for your help!","g E=\lbrack 0,\infty) \lim_{n \rightarrow \infty}\frac{1}{n}\int_0^n xg(x)\,dx=0. \epsilon >0 \delta > 0 |E|<\delta \int_E g(x)\,dx<\epsilon. \lbrack 0 , n-\delta\rbrack \lbrack n-\delta, n) \delta","['limits', 'measure-theory', 'lebesgue-integral']"
63,A limits question,A limits question,,"The following equals? $$ \lim_{x \to 1}\frac{\displaystyle\int_1^x \sin(t) \, dt}{x^2-1} $$ I think this can be converted to $$ \lim_{x \to 1}\frac{\sin(x)}{2x} = \frac{\sin(1)}{2} $$ by using the fundamental theorem of calculus. But the correct answer is $1/2$. So where I made a mistake?","The following equals? $$ \lim_{x \to 1}\frac{\displaystyle\int_1^x \sin(t) \, dt}{x^2-1} $$ I think this can be converted to $$ \lim_{x \to 1}\frac{\sin(x)}{2x} = \frac{\sin(1)}{2} $$ by using the fundamental theorem of calculus. But the correct answer is $1/2$. So where I made a mistake?",,[]
64,"Two-variable limit, quotient of polynomials","Two-variable limit, quotient of polynomials",,"I'm trying to evaluate the following limit, $$ \lim_{(x,y)\to(0,0)} \frac{x^3-y^2}{x^2-y} $$ which I think it doesn't exist, since for the curve $\alpha :[0,1]\to \mathbb R^2$, $\alpha(t) = (t, t^2)$ it isn't well defined, and if the limit exists it is because for every continuous curve $\gamma:[0,1]\to \mathbb R^2$ such that $\gamma(0) = (0,0)$ and $\gamma(t) \neq (0,0)$ for all $t\neq 0$, and $\lim_{t\downarrow 0} f(\gamma(t)) = 0.$  Is that correct? How would you evaluate the limit? Any hint? Thanks in advance.","I'm trying to evaluate the following limit, $$ \lim_{(x,y)\to(0,0)} \frac{x^3-y^2}{x^2-y} $$ which I think it doesn't exist, since for the curve $\alpha :[0,1]\to \mathbb R^2$, $\alpha(t) = (t, t^2)$ it isn't well defined, and if the limit exists it is because for every continuous curve $\gamma:[0,1]\to \mathbb R^2$ such that $\gamma(0) = (0,0)$ and $\gamma(t) \neq (0,0)$ for all $t\neq 0$, and $\lim_{t\downarrow 0} f(\gamma(t)) = 0.$  Is that correct? How would you evaluate the limit? Any hint? Thanks in advance.",,"['limits', 'multivariable-calculus', 'continuity']"
65,Multivariable Epsilon-Delta Proof?,Multivariable Epsilon-Delta Proof?,,"I am lost on this problem: State whether the following limit exists and prove it: $$ \lim_{(x,y) \rightarrow (0,0)} \frac{\sqrt[2]{|x|}y}{x^2+y^2} $$ All of the examples in class used the $\epsilon$ - $\delta$ proof technique.  I am still getting used to this proof technique and I understand what $\epsilon$ and $\delta$ represent, but I don't know how to begin (or end) $\epsilon$ - $\delta$ proofs. What I know: Trying to prove $\forall\epsilon>0, \exists \delta$ such that $|\sqrt[2]{x^2+y^2}|<\delta \implies |\frac{\sqrt[2]{|x|}y}{x^2+y^2}-?|<\epsilon$ Specific questions: How do I proceed with the proof if I don't know what ? is? What does $|f(\mathbf x)-\mathbf a|<\epsilon$ mean in a multivariable case? What are general $\epsilon$ - $\delta$ proof techniques? I think if I can figure out this problem I can figure out the rest.","I am lost on this problem: State whether the following limit exists and prove it: $$ \lim_{(x,y) \rightarrow (0,0)} \frac{\sqrt[2]{|x|}y}{x^2+y^2} $$ All of the examples in class used the $\epsilon$ - $\delta$ proof technique.  I am still getting used to this proof technique and I understand what $\epsilon$ and $\delta$ represent, but I don't know how to begin (or end) $\epsilon$ - $\delta$ proofs. What I know: Trying to prove $\forall\epsilon>0, \exists \delta$ such that $|\sqrt[2]{x^2+y^2}|<\delta \implies |\frac{\sqrt[2]{|x|}y}{x^2+y^2}-?|<\epsilon$ Specific questions: How do I proceed with the proof if I don't know what ? is? What does $|f(\mathbf x)-\mathbf a|<\epsilon$ mean in a multivariable case? What are general $\epsilon$ - $\delta$ proof techniques? I think if I can figure out this problem I can figure out the rest.",,"['limits', 'multivariable-calculus']"
66,How to find a limit of this sequence? [closed],How to find a limit of this sequence? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I would appreciate if somebody could help me with the following problem: Let the sequences $\{a_n\}$, $\{b_n\}$ be defined as   $$a_n=\int\limits_0^{\pi/2}\sin^{3n} x \;dx,\qquad b_n=\int\limits_0^{\pi/2}\sin^{2n}x \cos^nx\;dx.$$   Find $$\lim_{n\to \infty}\frac{1}{2n}\left(\frac{(2n)!\, a_n}{n!\, b_n} \right)^ {1/n}.$$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I would appreciate if somebody could help me with the following problem: Let the sequences $\{a_n\}$, $\{b_n\}$ be defined as   $$a_n=\int\limits_0^{\pi/2}\sin^{3n} x \;dx,\qquad b_n=\int\limits_0^{\pi/2}\sin^{2n}x \cos^nx\;dx.$$   Find $$\lim_{n\to \infty}\frac{1}{2n}\left(\frac{(2n)!\, a_n}{n!\, b_n} \right)^ {1/n}.$$",,"['limits', 'contest-math']"
67,Which function grows faster: $(n!)!$ or $((n-1)!)!(n-1)!^{n!}$?,Which function grows faster:  or ?,(n!)! ((n-1)!)!(n-1)!^{n!},"Of course, I can use Stirling's approximation, but for me it is quite interesting, that, if we define $k = (n-1)!$, then the left function will be $(nk)!$, and the right one will be $k! k^{n!}$. I don't think that it is a coincidence. It seems, that there should be smarter solution for this, other than Stirling's approximation.","Of course, I can use Stirling's approximation, but for me it is quite interesting, that, if we define $k = (n-1)!$, then the left function will be $(nk)!$, and the right one will be $k! k^{n!}$. I don't think that it is a coincidence. It seems, that there should be smarter solution for this, other than Stirling's approximation.",,['limits']
68,Lebesgue measure and the curse of dimensionality (application),Lebesgue measure and the curse of dimensionality (application),,"Apologies for the cryptic, Harry Potter -esque title; I really did not know how to name this! (Very much open to suggestions of more informative titles.) Fix $\varepsilon>0$ and $a>0$ . For each $n\in\{2,3,...\}$ , define $$ S_n := \left\{(x_1,...,x_n)\in [0,a]^n : \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}< \varepsilon \ \forall i\in\{1,...,n\}\right\} $$ I'm having difficulty calculating the Lebesgue measure (denoted by $\lambda(\cdot)$ ) of $S_n$ . Is this possible? If so, advice on how to do this would be very appreciated. If not, would there be another way for me to analytically characterize $\text{limit}_{n\to\infty} \frac{\lambda(S_n)}{a^n}$ ? (I.e. how ""large"" $S_n$ is relative to $[0,a]^n$ as $n$ becomes large?.) Note: Assume $\varepsilon$ is such that $S_n$ is non-empty for at least some $n\in\{2,3,...\}.$","Apologies for the cryptic, Harry Potter -esque title; I really did not know how to name this! (Very much open to suggestions of more informative titles.) Fix and . For each , define I'm having difficulty calculating the Lebesgue measure (denoted by ) of . Is this possible? If so, advice on how to do this would be very appreciated. If not, would there be another way for me to analytically characterize ? (I.e. how ""large"" is relative to as becomes large?.) Note: Assume is such that is non-empty for at least some","\varepsilon>0 a>0 n\in\{2,3,...\} 
S_n := \left\{(x_1,...,x_n)\in [0,a]^n : \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}< \varepsilon \ \forall i\in\{1,...,n\}\right\}
 \lambda(\cdot) S_n \text{limit}_{n\to\infty} \frac{\lambda(S_n)}{a^n} S_n [0,a]^n n \varepsilon S_n n\in\{2,3,...\}.","['limits', 'measure-theory', 'lebesgue-measure', 'recreational-mathematics', 'applications']"
69,Why does Tao define limits this way?,Why does Tao define limits this way?,,"When defining the limit of a function at a point, Terence Tao (Analysis I, 2016, 3e) also adds ""in $E$ "". I think with the example below, most texts would simply say that $\lim_{x\rightarrow 0}f(x)=0$ . But Tao makes a distinction between $\lim_{x\rightarrow 0;x\in \mathbb{R}- \{0\}}f(x)$ (equals $0$ ) and $\lim_{x\rightarrow 0;x\in \mathbb{R}}f(x)$ (undefined). Why does he do this? What's the point/advantage gained?","When defining the limit of a function at a point, Terence Tao (Analysis I, 2016, 3e) also adds ""in "". I think with the example below, most texts would simply say that . But Tao makes a distinction between (equals ) and (undefined). Why does he do this? What's the point/advantage gained?",E \lim_{x\rightarrow 0}f(x)=0 \lim_{x\rightarrow 0;x\in \mathbb{R}- \{0\}}f(x) 0 \lim_{x\rightarrow 0;x\in \mathbb{R}}f(x),[]
70,Graphic proof of the limit = $\sqrt{2 + \sqrt{2+\sqrt{2+\sqrt{2+\sqrt{2+\sqrt{2 + ...}}}}}} $ [duplicate],Graphic proof of the limit =  [duplicate],\sqrt{2 + \sqrt{2+\sqrt{2+\sqrt{2+\sqrt{2+\sqrt{2 + ...}}}}}} ,"This question already has answers here : Geometric visualization of nested radicals (1 answer) Evaluating $\sqrt{2 + \sqrt{2 + \sqrt{2 + \sqrt{2 + \cdots }}}}$ [duplicate] (3 answers) Closed 4 years ago . I am trying to prove that: $\sqrt{2 + \sqrt{2+\sqrt{2+\sqrt{2+\sqrt{2+\sqrt{2 + \cdots}}}}}} = 2$ with the following figure: Figure The approach I'm trying to follow is to show that you can evaluate the sum and it will be between, for example, $2+\sqrt{2}$ and $4$ . Also, I see a pattern with the triangles that are formed by the evaluation of the functions and how they decrease in size and are similar. Any suggestion?","This question already has answers here : Geometric visualization of nested radicals (1 answer) Evaluating $\sqrt{2 + \sqrt{2 + \sqrt{2 + \sqrt{2 + \cdots }}}}$ [duplicate] (3 answers) Closed 4 years ago . I am trying to prove that: with the following figure: Figure The approach I'm trying to follow is to show that you can evaluate the sum and it will be between, for example, and . Also, I see a pattern with the triangles that are formed by the evaluation of the functions and how they decrease in size and are similar. Any suggestion?",\sqrt{2 + \sqrt{2+\sqrt{2+\sqrt{2+\sqrt{2+\sqrt{2 + \cdots}}}}}} = 2 2+\sqrt{2} 4,"['limits', 'convergence-divergence', 'graphing-functions', 'radicals']"
71,Inconsistency of limits,Inconsistency of limits,,"Let $I_n(x)$ and $L_n(x)$ be the modified Bessel and modified Struve functions of order $n$ , respectively. Assuming $x$ is real, I am interested in the following limit: $$ \lim_{x\to\infty} \frac{I_0(x)L_1(x) - I_1(x)L_0(x)}{x^2I_2(x)}. $$ Let's call the function $G(x)/x^2$ . Now, using Wolfram Alpha I find that $$ \lim_{x\to\infty} G(x) = \lim_{x\to\infty} \frac{I_0(x)L_1(x) - I_1(x)L_0(x)}{I_2(x)} = -\frac{2}{\pi}. $$ So it seems like $$ \lim_{x\to\infty} \frac{I_0(x)L_1(x) - I_1(x)L_0(x)}{x^2I_2(x)} = \lim_{x\to\infty} \frac{G(x)}{x^2} = \left(\lim_{x\to\infty} G(x)\right)\left(\lim_{x\to\infty} \frac{1}{x^2}\right) = 0 $$ On the other hand, Wolfram Alpha gives me $$ \lim_{x\to\infty} \frac{G(x)}{x^2} = -\infty. $$ What went wrong?","Let and be the modified Bessel and modified Struve functions of order , respectively. Assuming is real, I am interested in the following limit: Let's call the function . Now, using Wolfram Alpha I find that So it seems like On the other hand, Wolfram Alpha gives me What went wrong?",I_n(x) L_n(x) n x  \lim_{x\to\infty} \frac{I_0(x)L_1(x) - I_1(x)L_0(x)}{x^2I_2(x)}.  G(x)/x^2  \lim_{x\to\infty} G(x) = \lim_{x\to\infty} \frac{I_0(x)L_1(x) - I_1(x)L_0(x)}{I_2(x)} = -\frac{2}{\pi}.   \lim_{x\to\infty} \frac{I_0(x)L_1(x) - I_1(x)L_0(x)}{x^2I_2(x)} = \lim_{x\to\infty} \frac{G(x)}{x^2} = \left(\lim_{x\to\infty} G(x)\right)\left(\lim_{x\to\infty} \frac{1}{x^2}\right) = 0   \lim_{x\to\infty} \frac{G(x)}{x^2} = -\infty. ,"['limits', 'special-functions', 'bessel-functions', 'wolfram-alpha']"
72,Largest root and asymptotics.,Largest root and asymptotics.,,"I am considering the polynomial $$ p_c(x)=x^{c+1}-x^c-1. $$ Its largest root, denoted by $\lambda_c$, is contained in the open interval $(1,1+\frac{\ln c}{c})$. Making the ansatz $\lambda_c=1+x_c$ for a zero sequence $(x_c)$, I want to express more explicitly what happens with $\lambda_c$ as $c\to\infty$. Putting the ansatz into the polynomial, I get $$ (1+x_c)^cx_c=1 $$ which yields, when applying logarithm, $$ c\ln(1+x_c)+\ln(x_c)=0. $$ Using the approximation $\ln(1+x_c)=x_c+f(x_c)$ with $f(x_c)=O(x_c^2)$ as $c\to\infty$, this gives $$ cx_c+cf(x_c)+\ln(x_c)=0. $$ Exponentiating and multiplicating with the factor $c$ yields $$ cx_ce^{cx_c}=ce^{-cf(x_c)} $$ and using Lamberts W-Function, ones gets $$ x_c=c^{-1}W(ce^{-cf(x_c)}). $$ Using the large argument approximation $$ W(x)=\ln x-\ln(\ln(x))+o(1)\text{ as }x\to\infty, $$ gives me $$ x_c=\frac{\ln c}{c}-f(x_c)-\frac{\ln(\ln c)}{c}-\frac{\ln(1-\frac{cf(x_c)}{\ln c})}{c}+o\left(\frac{1}{c}\right)\text{ as }c\to\infty $$ My question is whether this can be also expressed as     $$ x_c=\frac{\ln c}{c}+o\left(\frac{\ln c}{c}\right)\text{ as }c\to\infty? $$ For the summands $o\left(\frac{1}{c}\right)$ and $\frac{\ln(\ln c)}{c}$, I can see immediately that they are indeed of order $o\left(\frac{\ln c}{c}\right)$ as $c\to\infty$. But for the summands $f(x_c)$ and $\frac{\ln(1-\frac{cf(x_c)}{\ln c})}{c}$ this is absolutely not clear to me. Edit (due to the comments): $x_c=o(1)$ and $x_c=O(\ln c/c)$. Both together imply $x_c^2=o(\ln c/c)$, as $c\to\infty$. Moreoever, $f(x_c)=O(x_c^2)$ and $x_c^2=o(\ln c/c)$ imply that $f(x_c)=o(\ln c/c)$ as $c\to\infty$. Hence, it remains to clarify whether $\frac{\ln(1-\frac{cf(x_c)}{\ln c})}{c}=o(\ln c/c)$ as $c\to\infty$.","I am considering the polynomial $$ p_c(x)=x^{c+1}-x^c-1. $$ Its largest root, denoted by $\lambda_c$, is contained in the open interval $(1,1+\frac{\ln c}{c})$. Making the ansatz $\lambda_c=1+x_c$ for a zero sequence $(x_c)$, I want to express more explicitly what happens with $\lambda_c$ as $c\to\infty$. Putting the ansatz into the polynomial, I get $$ (1+x_c)^cx_c=1 $$ which yields, when applying logarithm, $$ c\ln(1+x_c)+\ln(x_c)=0. $$ Using the approximation $\ln(1+x_c)=x_c+f(x_c)$ with $f(x_c)=O(x_c^2)$ as $c\to\infty$, this gives $$ cx_c+cf(x_c)+\ln(x_c)=0. $$ Exponentiating and multiplicating with the factor $c$ yields $$ cx_ce^{cx_c}=ce^{-cf(x_c)} $$ and using Lamberts W-Function, ones gets $$ x_c=c^{-1}W(ce^{-cf(x_c)}). $$ Using the large argument approximation $$ W(x)=\ln x-\ln(\ln(x))+o(1)\text{ as }x\to\infty, $$ gives me $$ x_c=\frac{\ln c}{c}-f(x_c)-\frac{\ln(\ln c)}{c}-\frac{\ln(1-\frac{cf(x_c)}{\ln c})}{c}+o\left(\frac{1}{c}\right)\text{ as }c\to\infty $$ My question is whether this can be also expressed as     $$ x_c=\frac{\ln c}{c}+o\left(\frac{\ln c}{c}\right)\text{ as }c\to\infty? $$ For the summands $o\left(\frac{1}{c}\right)$ and $\frac{\ln(\ln c)}{c}$, I can see immediately that they are indeed of order $o\left(\frac{\ln c}{c}\right)$ as $c\to\infty$. But for the summands $f(x_c)$ and $\frac{\ln(1-\frac{cf(x_c)}{\ln c})}{c}$ this is absolutely not clear to me. Edit (due to the comments): $x_c=o(1)$ and $x_c=O(\ln c/c)$. Both together imply $x_c^2=o(\ln c/c)$, as $c\to\infty$. Moreoever, $f(x_c)=O(x_c^2)$ and $x_c^2=o(\ln c/c)$ imply that $f(x_c)=o(\ln c/c)$ as $c\to\infty$. Hence, it remains to clarify whether $\frac{\ln(1-\frac{cf(x_c)}{\ln c})}{c}=o(\ln c/c)$ as $c\to\infty$.",,"['limits', 'analysis', 'asymptotics']"
73,"quick question about the limit of a two-variable function as $x,y\to\infty$",quick question about the limit of a two-variable function as,"x,y\to\infty","$$\lim_{x,y\to\infty} \frac{x-y}{x^2+y^2}\tag{$\star$}$$ I'm used to do the following substitution when I see $``x^2+y^2""$ and that $x,y\to 0$ $$x^2+y^2 = r^2,\;x=r\cos\theta,\;y=r\sin\theta$$ plug these values in the function and compute the limit as $r\to0$ I know I can do that because the only way for $x$ & $y$ to approach $0$ is $r$ approaching $0.$ I cannot do this substitution everytime because if for example: $(x,y)\to(-1,7)$ there's no value $u$ that guarantee me if $r\to u$ then $(x,y)\to(-1,7).$ but here since $x,y\to\infty$ I think that logically this phenomenon can only happen if $r\to\infty$ as well. So computing $(\star)$ is the same as computing this : $$ \lim_{r\to\infty} \frac{r\cos\theta-r\sin\theta}{r^2} =\lim_{r\to\infty} \frac{\cos\theta-\sin\theta}{r}=0. $$ I'm 90% sure that what I've done is correct but I still want a confirmation and if possible show me other ways to compute this limit. Sorry if this question sounds kinda dumb but I'm still new to multivariable calculus and today is my first time dealing with MVC limits. Thank you !","$$\lim_{x,y\to\infty} \frac{x-y}{x^2+y^2}\tag{$\star$}$$ I'm used to do the following substitution when I see $``x^2+y^2""$ and that $x,y\to 0$ $$x^2+y^2 = r^2,\;x=r\cos\theta,\;y=r\sin\theta$$ plug these values in the function and compute the limit as $r\to0$ I know I can do that because the only way for $x$ & $y$ to approach $0$ is $r$ approaching $0.$ I cannot do this substitution everytime because if for example: $(x,y)\to(-1,7)$ there's no value $u$ that guarantee me if $r\to u$ then $(x,y)\to(-1,7).$ but here since $x,y\to\infty$ I think that logically this phenomenon can only happen if $r\to\infty$ as well. So computing $(\star)$ is the same as computing this : $$ \lim_{r\to\infty} \frac{r\cos\theta-r\sin\theta}{r^2} =\lim_{r\to\infty} \frac{\cos\theta-\sin\theta}{r}=0. $$ I'm 90% sure that what I've done is correct but I still want a confirmation and if possible show me other ways to compute this limit. Sorry if this question sounds kinda dumb but I'm still new to multivariable calculus and today is my first time dealing with MVC limits. Thank you !",,"['limits', 'multivariable-calculus']"
74,Asymptotic approximation for confluent hypergeometric function,Asymptotic approximation for confluent hypergeometric function,,"I have the following nasty expression that I would like to expand in powers of $\frac{1}{N}$: \begin{align} \frac{2^{\frac{3}{2}} 3^{\frac{1}{2}} \Biggl[ \sqrt{u} \cdot \Gamma\left(\frac{2+N}{4}\right)                   \cdot {}_1F_1 \left( \frac{2+N}{4},\frac{1}{2},\frac{3r^2}{2u} \right)                   -\sqrt{6} r \cdot  \Gamma \left( \frac{4+N}{4} \right) \cdot {}_1F_1 \left( \frac{4+N}{4},\frac{3}{2},\frac{3r^2}{2u} \right) \Biggr] }{N \cdot  u^{\frac{1}{2}} \Biggl[ \sqrt{u} \cdot \Gamma\left(\frac{N}{4}\right) \cdot {}_1F_1 \left( \frac{N}{4},\frac{1}{2},\frac{3r^2}{2u} \right) -\sqrt{6} r \cdot  \Gamma \left( \frac{2+N}{4} \right) \cdot {}_1F_1 \left( \frac{2+N}{4},\frac{3}{2},\frac{3r^2}{2u} \right) \Biggr]} \end{align} where you can assume that $N \in \mathbb{N}$ (but could be analytically continued to $\mathbb{R}^+$), $u \in \mathbb{R}^+$, and $r \in \mathbb{R}^+$. Furthermore, ${}_1F_1$ is the confluent hypergeometric function sometimes written as $M(a,b,z)$. Using a different route I have obtained a value for the limit $N \to \infty$, but I'd like to a) reproduce this result using the above expression and b) find the $O\left(\frac{1}{N}\right)$ corrections. So far I have tried numerous identities from the NIST Handbook of Mathematical Functions, but I simply seem to lack the experience to make real progress. If anyone knows a solution or has an idea of how to proceed next, I'd greatly appreciate their help. With best regards, Jan","I have the following nasty expression that I would like to expand in powers of $\frac{1}{N}$: \begin{align} \frac{2^{\frac{3}{2}} 3^{\frac{1}{2}} \Biggl[ \sqrt{u} \cdot \Gamma\left(\frac{2+N}{4}\right)                   \cdot {}_1F_1 \left( \frac{2+N}{4},\frac{1}{2},\frac{3r^2}{2u} \right)                   -\sqrt{6} r \cdot  \Gamma \left( \frac{4+N}{4} \right) \cdot {}_1F_1 \left( \frac{4+N}{4},\frac{3}{2},\frac{3r^2}{2u} \right) \Biggr] }{N \cdot  u^{\frac{1}{2}} \Biggl[ \sqrt{u} \cdot \Gamma\left(\frac{N}{4}\right) \cdot {}_1F_1 \left( \frac{N}{4},\frac{1}{2},\frac{3r^2}{2u} \right) -\sqrt{6} r \cdot  \Gamma \left( \frac{2+N}{4} \right) \cdot {}_1F_1 \left( \frac{2+N}{4},\frac{3}{2},\frac{3r^2}{2u} \right) \Biggr]} \end{align} where you can assume that $N \in \mathbb{N}$ (but could be analytically continued to $\mathbb{R}^+$), $u \in \mathbb{R}^+$, and $r \in \mathbb{R}^+$. Furthermore, ${}_1F_1$ is the confluent hypergeometric function sometimes written as $M(a,b,z)$. Using a different route I have obtained a value for the limit $N \to \infty$, but I'd like to a) reproduce this result using the above expression and b) find the $O\left(\frac{1}{N}\right)$ corrections. So far I have tried numerous identities from the NIST Handbook of Mathematical Functions, but I simply seem to lack the experience to make real progress. If anyone knows a solution or has an idea of how to proceed next, I'd greatly appreciate their help. With best regards, Jan",,"['limits', 'special-functions']"
75,How to solve this limit?,How to solve this limit?,,"I have a regression model: $y_i=\exp(a \sin(\frac{2 \pi i}{n}) + b \cos(\frac{2 \pi i} {n})+\varepsilon_i)$ where a, b are the regression parameters. Let ${\varepsilon}_i = {\varepsilon}_i(t)$ be independent identically distributed random processes. I want to evaluate an accuracy. Let $\hat{y} = \exp(\hat{a} \sin(\frac{2 \pi i}{n}) + \hat{b} \cos(\frac{2 \pi i} {n}))$ be the model with estimated parameters $\hat{a}, \hat{b}$. Then, $\hat{\varepsilon}_i = \ln{y_i} - \ln{\hat{y_i}}$ is a residual on ith point. $$Z_n(t)=\frac{1}{\sigma \sqrt{n}} \sum_{i=1}^{[nt]} \hat{\varepsilon_i}.$$ I need to find $\lim_{n\to\infty} Z_n(t)$ in distribution. I tried to proceed as follows: $$Z_n(t)=\frac{1}{\sigma \sqrt{n}} \sum_{i=1}^{[nt]} \hat{\varepsilon_i}=\frac{1}{\sigma \sqrt{n}} \sum_{i=1}^{[nt]}[\ln{y_i} - \ln{\hat{y_i}}]$$ $$Z_n(t)=\frac{1}{\sigma \sqrt{n}} \sum_{i=1}^{[nt]} [(a-\hat{a}) \sin(\frac{2 \pi i}{n}) + (b-\hat{b}) \cos(\frac{2 \pi i} {n})+\varepsilon_i]$$ $\frac{1}{\sigma \sqrt{n}} \sum_{i=1}^{[nt]} \varepsilon_i \to_{n\to\infty} N(0, 1)$. Here $N(0,1)$ is a standart normal distribution. $$Z_n(t) \to N(0,1) + \lim_{n\to\infty}\frac{1}{\sigma \sqrt{n}} \sum_{i=1}^{[nt]} [(a-\hat{a}) \sin(\frac{2 \pi i}{n}) + (b-\hat{b}) \cos(\frac{2 \pi i} {n})]$$ Now I think we can simplify the sum limit to something like $(a-\hat{a}) + (b-\hat{b})$ and can get $Z_n(t)\to N(a-\hat{a} + b-\hat{b}, 1)$. My question is how to get it and is it all correct with my calculations? Thanks in advance. EDIT: Well, now I have got a partial solution of this problem... First of all, we should get the OLS-estimators of $\hat{a}$ and $\hat{b}$. Let $\hat{u}_i = \ln(\hat{y}_i)$. Then, our model is $U = X \theta$, where $\theta=\begin{pmatrix} a \\ b \end{pmatrix}$, $X = \begin{pmatrix} \sin(\frac{2 \pi 1}{n}) & \cos(\frac{2 \pi 1} {n}) \\ ... & ... \\ \sin(\frac{2 \pi n}{n}) & \cos(\frac{2 \pi n} {n})\end{pmatrix}$ An assessment can be found by using the formula: $\hat{\theta} = (X^T X)^{-1} X^T U $. After some calculations, $\hat{\theta} = \begin{pmatrix} 2 \overline{u_i \cos(\frac{2 \pi i}{n}}) \\ 2 \overline{u_i \sin(\frac{2 \pi i}{n}}) \end{pmatrix}$. So, $\hat{\varepsilon} = u - \hat{u} = -n\cdot \overline{U}$. Here $\overline{U}$ is a mean value of $U$, i.e. $\frac{1}{n}\Sigma_{i=1}^n u_i$. Now we want to describe a random process $Z_n(t)=\frac{1}{\sigma \sqrt{n}} \sum_{i=1}^{[nt]} \hat{\varepsilon_i}$. AFAIK, Ian B. MacNeill's theorem is just about it, but I can't yet understand it... Could you please help me to complete solution here?","I have a regression model: $y_i=\exp(a \sin(\frac{2 \pi i}{n}) + b \cos(\frac{2 \pi i} {n})+\varepsilon_i)$ where a, b are the regression parameters. Let ${\varepsilon}_i = {\varepsilon}_i(t)$ be independent identically distributed random processes. I want to evaluate an accuracy. Let $\hat{y} = \exp(\hat{a} \sin(\frac{2 \pi i}{n}) + \hat{b} \cos(\frac{2 \pi i} {n}))$ be the model with estimated parameters $\hat{a}, \hat{b}$. Then, $\hat{\varepsilon}_i = \ln{y_i} - \ln{\hat{y_i}}$ is a residual on ith point. $$Z_n(t)=\frac{1}{\sigma \sqrt{n}} \sum_{i=1}^{[nt]} \hat{\varepsilon_i}.$$ I need to find $\lim_{n\to\infty} Z_n(t)$ in distribution. I tried to proceed as follows: $$Z_n(t)=\frac{1}{\sigma \sqrt{n}} \sum_{i=1}^{[nt]} \hat{\varepsilon_i}=\frac{1}{\sigma \sqrt{n}} \sum_{i=1}^{[nt]}[\ln{y_i} - \ln{\hat{y_i}}]$$ $$Z_n(t)=\frac{1}{\sigma \sqrt{n}} \sum_{i=1}^{[nt]} [(a-\hat{a}) \sin(\frac{2 \pi i}{n}) + (b-\hat{b}) \cos(\frac{2 \pi i} {n})+\varepsilon_i]$$ $\frac{1}{\sigma \sqrt{n}} \sum_{i=1}^{[nt]} \varepsilon_i \to_{n\to\infty} N(0, 1)$. Here $N(0,1)$ is a standart normal distribution. $$Z_n(t) \to N(0,1) + \lim_{n\to\infty}\frac{1}{\sigma \sqrt{n}} \sum_{i=1}^{[nt]} [(a-\hat{a}) \sin(\frac{2 \pi i}{n}) + (b-\hat{b}) \cos(\frac{2 \pi i} {n})]$$ Now I think we can simplify the sum limit to something like $(a-\hat{a}) + (b-\hat{b})$ and can get $Z_n(t)\to N(a-\hat{a} + b-\hat{b}, 1)$. My question is how to get it and is it all correct with my calculations? Thanks in advance. EDIT: Well, now I have got a partial solution of this problem... First of all, we should get the OLS-estimators of $\hat{a}$ and $\hat{b}$. Let $\hat{u}_i = \ln(\hat{y}_i)$. Then, our model is $U = X \theta$, where $\theta=\begin{pmatrix} a \\ b \end{pmatrix}$, $X = \begin{pmatrix} \sin(\frac{2 \pi 1}{n}) & \cos(\frac{2 \pi 1} {n}) \\ ... & ... \\ \sin(\frac{2 \pi n}{n}) & \cos(\frac{2 \pi n} {n})\end{pmatrix}$ An assessment can be found by using the formula: $\hat{\theta} = (X^T X)^{-1} X^T U $. After some calculations, $\hat{\theta} = \begin{pmatrix} 2 \overline{u_i \cos(\frac{2 \pi i}{n}}) \\ 2 \overline{u_i \sin(\frac{2 \pi i}{n}}) \end{pmatrix}$. So, $\hat{\varepsilon} = u - \hat{u} = -n\cdot \overline{U}$. Here $\overline{U}$ is a mean value of $U$, i.e. $\frac{1}{n}\Sigma_{i=1}^n u_i$. Now we want to describe a random process $Z_n(t)=\frac{1}{\sigma \sqrt{n}} \sum_{i=1}^{[nt]} \hat{\varepsilon_i}$. AFAIK, Ian B. MacNeill's theorem is just about it, but I can't yet understand it... Could you please help me to complete solution here?",,['limits']
76,An infinite product for $\frac{\pi}{2}$,An infinite product for,\frac{\pi}{2},Please help prove $$ \begin{align} \frac{\pi}{2}&=\left(\frac{1}{2}\right)^{2/1}\left(\frac{2^{2}}{1^{1}}\right)^{4/(1\cdot 3)}\left(\frac{1}{4}\right)^{2/3}\left(\frac{2^{2}\cdot4^{4}}{1^{1}\cdot3^{3}}\right)^{4/(3\cdot 5)}\left(\frac{1}{6}\right)^{2/5}\left(\frac{2^{2}\cdot4^{4}\cdot6^{6}}{1^{1}\cdot3^{3}\cdot5^{5}}\right)^{4/(5\cdot 7)}\cdots\\[5pt] &=\prod_{n=1}^{\infty}\left(\frac{1}{2n}\right)^{\frac{2}{2n-1}}\left(\prod_{k=1}^{n}\frac{\left(2k\right)^{2k}}{\left(2k-1\right)^{2k-1}}\right)^{\frac{4}{\left(2n-1\right)\left(2n+1\right)}} \end{align} $$ Here's my progress $$ \begin{align} \frac{\pi}{2}&=\prod_{n=1}^{\infty}\left(\frac{1}{2n}\right)^{\frac{2}{2n-1}}\left(\prod_{k=1}^{n}\frac{\left(2k\right)^{2k}}{\left(2k-1\right)^{2k-1}}\right)^{\frac{4}{\left(2n-1\right)\left(2n+1\right)}}\\[5pt] &=\prod_{n=1}^{\infty}\left(\frac{1}{2n}\right)^{\frac{2}{2n-1}}\left(\prod_{k=1}^{n}\frac{\left(2k\right)^{2k}}{\left(2k-1\right)^{2k-1}}\right)^{\frac{2}{2n-1}-\frac{2}{2n+1}}\\[5pt] &=\lim\limits_{m\to\infty}\frac{\prod_{n=1}^{m}\left(\frac{1}{2n}\right)^{\frac{2}{2n-1}}\left(\prod_{k=1}^{n}\frac{\left(2k\right)^{2k}}{\left(2k-1\right)^{2k-1}}\right)^{\frac{2}{2n-1}}}{\left(\prod_{k=1}^{m}\frac{\left(2k\right)^{2k}}{\left(2k-1\right)^{2k-1}}\right)^{\frac{2}{2m+1}}\prod_{n=2}^{m}\left(\prod_{k=1}^{n-1}\frac{\left(2k\right)^{2k}}{\left(2k-1\right)^{2k-1}}\right)^{\frac{2}{2n-1}}}\\[5pt] &=\lim\limits_{m\to\infty}\frac{\prod_{n=1}^{m}\left(\frac{1}{2n}\right)^{\frac{2}{2n-1}}\left(\frac{\left(2n\right)^{2n}}{\left(2n-1\right)^{2n-1}}\right)^{\frac{2}{2n-1}}}{\left(\prod_{k=1}^{m}\frac{\left(2k\right)^{2k}}{\left(2k-1\right)^{2k-1}}\right)^{\frac{2}{2m+1}}}\\[5pt] &=\lim\limits_{m\to\infty}\frac{\left(\prod_{n=1}^{m}\frac{2n}{2n-1}\right)^{2}}{\left(\prod_{k=1}^{m}\frac{\left(2k\right)^{2k}}{\left(2k-1\right)^{2k-1}}\right)^{\frac{2}{2m+1}}}\\[5pt] &=\lim\limits_{m\to\infty}\frac{\pi\left(\frac{\Gamma(m+1)}{\Gamma(m+\frac12)}\right)^{2}}{\left(\prod_{k=1}^{m}\frac{\left(2k\right)^{2k}}{\left(2k-1\right)^{2k-1}}\right)^{\frac{1}{m}}}\\[5pt] \iff \frac12&=\lim\limits_{m\to\infty}\frac{\left(\frac{\Gamma(m+1)}{\Gamma(m+\frac12)}\right)^{2}}{\left(\prod_{k=1}^{m}\frac{\left(2k\right)^{2k}}{\left(2k-1\right)^{2k-1}}\right)^{\frac{1}{m}}} \end{align} $$ I'm stuck here for now. Edit: Drawing from this post we apparently just need to show that $$\left(\prod_{k=1}^{m}\frac{\left(2k\right)^{2k}}{\left(2k-1\right)^{2k-1}}\right)^{\frac{1}{m}}\sim 2m$$,Please help prove Here's my progress I'm stuck here for now. Edit: Drawing from this post we apparently just need to show that,"
\begin{align}
\frac{\pi}{2}&=\left(\frac{1}{2}\right)^{2/1}\left(\frac{2^{2}}{1^{1}}\right)^{4/(1\cdot 3)}\left(\frac{1}{4}\right)^{2/3}\left(\frac{2^{2}\cdot4^{4}}{1^{1}\cdot3^{3}}\right)^{4/(3\cdot 5)}\left(\frac{1}{6}\right)^{2/5}\left(\frac{2^{2}\cdot4^{4}\cdot6^{6}}{1^{1}\cdot3^{3}\cdot5^{5}}\right)^{4/(5\cdot 7)}\cdots\\[5pt]
&=\prod_{n=1}^{\infty}\left(\frac{1}{2n}\right)^{\frac{2}{2n-1}}\left(\prod_{k=1}^{n}\frac{\left(2k\right)^{2k}}{\left(2k-1\right)^{2k-1}}\right)^{\frac{4}{\left(2n-1\right)\left(2n+1\right)}}
\end{align}
 
\begin{align}
\frac{\pi}{2}&=\prod_{n=1}^{\infty}\left(\frac{1}{2n}\right)^{\frac{2}{2n-1}}\left(\prod_{k=1}^{n}\frac{\left(2k\right)^{2k}}{\left(2k-1\right)^{2k-1}}\right)^{\frac{4}{\left(2n-1\right)\left(2n+1\right)}}\\[5pt]
&=\prod_{n=1}^{\infty}\left(\frac{1}{2n}\right)^{\frac{2}{2n-1}}\left(\prod_{k=1}^{n}\frac{\left(2k\right)^{2k}}{\left(2k-1\right)^{2k-1}}\right)^{\frac{2}{2n-1}-\frac{2}{2n+1}}\\[5pt]
&=\lim\limits_{m\to\infty}\frac{\prod_{n=1}^{m}\left(\frac{1}{2n}\right)^{\frac{2}{2n-1}}\left(\prod_{k=1}^{n}\frac{\left(2k\right)^{2k}}{\left(2k-1\right)^{2k-1}}\right)^{\frac{2}{2n-1}}}{\left(\prod_{k=1}^{m}\frac{\left(2k\right)^{2k}}{\left(2k-1\right)^{2k-1}}\right)^{\frac{2}{2m+1}}\prod_{n=2}^{m}\left(\prod_{k=1}^{n-1}\frac{\left(2k\right)^{2k}}{\left(2k-1\right)^{2k-1}}\right)^{\frac{2}{2n-1}}}\\[5pt]
&=\lim\limits_{m\to\infty}\frac{\prod_{n=1}^{m}\left(\frac{1}{2n}\right)^{\frac{2}{2n-1}}\left(\frac{\left(2n\right)^{2n}}{\left(2n-1\right)^{2n-1}}\right)^{\frac{2}{2n-1}}}{\left(\prod_{k=1}^{m}\frac{\left(2k\right)^{2k}}{\left(2k-1\right)^{2k-1}}\right)^{\frac{2}{2m+1}}}\\[5pt]
&=\lim\limits_{m\to\infty}\frac{\left(\prod_{n=1}^{m}\frac{2n}{2n-1}\right)^{2}}{\left(\prod_{k=1}^{m}\frac{\left(2k\right)^{2k}}{\left(2k-1\right)^{2k-1}}\right)^{\frac{2}{2m+1}}}\\[5pt]
&=\lim\limits_{m\to\infty}\frac{\pi\left(\frac{\Gamma(m+1)}{\Gamma(m+\frac12)}\right)^{2}}{\left(\prod_{k=1}^{m}\frac{\left(2k\right)^{2k}}{\left(2k-1\right)^{2k-1}}\right)^{\frac{1}{m}}}\\[5pt]
\iff \frac12&=\lim\limits_{m\to\infty}\frac{\left(\frac{\Gamma(m+1)}{\Gamma(m+\frac12)}\right)^{2}}{\left(\prod_{k=1}^{m}\frac{\left(2k\right)^{2k}}{\left(2k-1\right)^{2k-1}}\right)^{\frac{1}{m}}}
\end{align}
 \left(\prod_{k=1}^{m}\frac{\left(2k\right)^{2k}}{\left(2k-1\right)^{2k-1}}\right)^{\frac{1}{m}}\sim 2m","['limits', 'closed-form', 'products', 'pi', 'infinite-product']"
77,Evaluate $\lim_{t\to1^-}(1-t)\sum_{r=1}^{\infty}\frac{t^r}{1+t^r}$,Evaluate,\lim_{t\to1^-}(1-t)\sum_{r=1}^{\infty}\frac{t^r}{1+t^r},$\lim_{t\to1^-}(1-t)\sum_{r=1}^{\infty}\frac{t^r}{1+t^r}$ My approach $\frac{t^r}{1+t^r}=t^r-t^{2r}+t^{3r}-\cdots$ $\implies \sum_{r=1}^{\infty}\frac{t^r}{1+t^r}=\frac{t}{1-t}-\frac{t^2}{1-t^2}+\frac{t^3}{1-t^3}-\cdots$ $\implies(1-t)\sum_{r=1}^{\infty}\frac{t^r}{1+t^r}=t-\frac{t^2}{1+t}+\frac{t^3}{1+t+t^2}-\cdots$ $\implies \lim_{t\to1^-}(1-t)\sum_{r=1}^{\infty}\frac{t^r}{1+t^r}=1-\frac12+\frac13-\frac14+\cdots=\ln 2$ Is this correct? Any other more rigorous approach ? Solution provided by problem poser: $\lim_{t\to1^-}(1-t)\sum_{r=1}^{\infty}\frac{t^r}{1+t^r}=\lim_{t\to1^-}-\ln t\sum_{r=1}^{\infty}\frac{1}{1+e^{-r\ln t}}$ $=\lim_{n\to\infty}\frac1n\sum_{r=1}^{\infty}\frac{1}{1+e^{\frac rn}}$ $=\int_{0}^{1}\frac{1}{1+e^x}dx=\ln\frac {2e}{1+e}$ This answer seems to be incorrect as $\frac{t^r}{2}<\frac{t^r}{1+t^r}<t^r\implies \frac{t}{2(1-t)}\le\sum_{r=1}^{\infty}\frac{t^r}{1+t^r}\le\frac{t}{1-t}\implies\frac12\le\lim_{t\to1^-}(1-t)\sum_{r=1}^{\infty}\frac{t^r}{1+t^r}\le 1.$ But $\ln\frac{2e}{1+e}\approx 0.38$,My approach Is this correct? Any other more rigorous approach ? Solution provided by problem poser: This answer seems to be incorrect as But,\lim_{t\to1^-}(1-t)\sum_{r=1}^{\infty}\frac{t^r}{1+t^r} \frac{t^r}{1+t^r}=t^r-t^{2r}+t^{3r}-\cdots \implies \sum_{r=1}^{\infty}\frac{t^r}{1+t^r}=\frac{t}{1-t}-\frac{t^2}{1-t^2}+\frac{t^3}{1-t^3}-\cdots \implies(1-t)\sum_{r=1}^{\infty}\frac{t^r}{1+t^r}=t-\frac{t^2}{1+t}+\frac{t^3}{1+t+t^2}-\cdots \implies \lim_{t\to1^-}(1-t)\sum_{r=1}^{\infty}\frac{t^r}{1+t^r}=1-\frac12+\frac13-\frac14+\cdots=\ln 2 \lim_{t\to1^-}(1-t)\sum_{r=1}^{\infty}\frac{t^r}{1+t^r}=\lim_{t\to1^-}-\ln t\sum_{r=1}^{\infty}\frac{1}{1+e^{-r\ln t}} =\lim_{n\to\infty}\frac1n\sum_{r=1}^{\infty}\frac{1}{1+e^{\frac rn}} =\int_{0}^{1}\frac{1}{1+e^x}dx=\ln\frac {2e}{1+e} \frac{t^r}{2}<\frac{t^r}{1+t^r}<t^r\implies \frac{t}{2(1-t)}\le\sum_{r=1}^{\infty}\frac{t^r}{1+t^r}\le\frac{t}{1-t}\implies\frac12\le\lim_{t\to1^-}(1-t)\sum_{r=1}^{\infty}\frac{t^r}{1+t^r}\le 1. \ln\frac{2e}{1+e}\approx 0.38,['limits']
78,What is the minimum value of $f_\infty=\frac{x}{\sqrt{x-\sqrt[3]{x-\sqrt[4]{x-\cdots}}}}$?,What is the minimum value of ?,f_\infty=\frac{x}{\sqrt{x-\sqrt[3]{x-\sqrt[4]{x-\cdots}}}},"In a similar vein to What is the maximum value of this nested radical? , I'd like to share a similar nested radical, but this time with changing fractional powers. What is the minimum value of $$f_\infty=\frac{x}{\sqrt{x-\sqrt[3]{x-\sqrt[4]{x-\cdots}}}}$$ where the radicals go up by one each time? Here is a plot of $f_{19}$ . We can see that as $x\to 1^+$ , $\min f_{19}\to 1.7186$ which is strange as the denominator can only take the binary values $0$ or $1$ at $x=1$ . The curve is monotonically increasing from $1$ onwards, which is expected as the numerator dominates. Actually, a simulation in PARI/GP up to $f_{100}$ yields a minimum value of around $1.718565$ , which is somewhat close to $e-1$ , although I strongly doubt that it will ever reach that value. Note that $f_k$ is defined in $(1,\infty)$ for all positive integers $k$ , but the curve swings wildly for $(-\infty,1)$ . It is, of course, not a good idea to differentiate $f_\infty$ directly, but unfortunately we can't take $x$ and $\sqrt{x-\sqrt[3]{x-\sqrt[4]{x-\cdots}}}$ separately as both are increasing. Another interesting question: Why is the minimum value of $f_k$ for large $k$ not equal to the expected $0,1$ or $\pm\infty$ ? Is it possible to manipulate $f_\infty$ so that L'Hopital can be used to find the value of $1.718\cdots$ ? Related are Evaluating the limit of $\sqrt[2]{2+\sqrt[3]{2+\sqrt[4]{2+\cdots+\sqrt[n]{2}}}}$ when $n\to\infty$ Find $\sqrt{4+\sqrt[3]{4+\sqrt[4]{4+\sqrt[5]{4+\cdots}}}}$ but neither have been solved as of now.","In a similar vein to What is the maximum value of this nested radical? , I'd like to share a similar nested radical, but this time with changing fractional powers. What is the minimum value of where the radicals go up by one each time? Here is a plot of . We can see that as , which is strange as the denominator can only take the binary values or at . The curve is monotonically increasing from onwards, which is expected as the numerator dominates. Actually, a simulation in PARI/GP up to yields a minimum value of around , which is somewhat close to , although I strongly doubt that it will ever reach that value. Note that is defined in for all positive integers , but the curve swings wildly for . It is, of course, not a good idea to differentiate directly, but unfortunately we can't take and separately as both are increasing. Another interesting question: Why is the minimum value of for large not equal to the expected or ? Is it possible to manipulate so that L'Hopital can be used to find the value of ? Related are Evaluating the limit of when Find but neither have been solved as of now.","f_\infty=\frac{x}{\sqrt{x-\sqrt[3]{x-\sqrt[4]{x-\cdots}}}} f_{19} x\to 1^+ \min f_{19}\to 1.7186 0 1 x=1 1 f_{100} 1.718565 e-1 f_k (1,\infty) k (-\infty,1) f_\infty x \sqrt{x-\sqrt[3]{x-\sqrt[4]{x-\cdots}}} f_k k 0,1 \pm\infty f_\infty 1.718\cdots \sqrt[2]{2+\sqrt[3]{2+\sqrt[4]{2+\cdots+\sqrt[n]{2}}}} n\to\infty \sqrt{4+\sqrt[3]{4+\sqrt[4]{4+\sqrt[5]{4+\cdots}}}}","['limits', 'functions', 'recursion', 'maxima-minima', 'nested-radicals']"
79,How find this $\lim_{n\to\infty}a_{n}$,How find this,\lim_{n\to\infty}a_{n},"let sequence $\{a_{n}\}$,such  $a_{1}=2\pi-6$, and $$a_{n}=\left\lceil\dfrac{2\pi}{a_{n-1}}\right\rceil\cdot a_{n-1}-2\pi$$ Find the  $$\lim_{n\to\infty}a_{n}$$ where $\left\lceil\dfrac{2\pi}{a_{n-1}}\right\rceil$is the smallest integer not less than $\dfrac{2\pi}{a_{n-1}}$ My try: since $$a_{n}>0,\dfrac{a_{n}}{a_{n-1}}=\lceil\dfrac{2\pi}{a_{n-1}}\rceil-\dfrac{2\pi}{a_{n-1}}\le 1$$ and then I can't Find this limit,and maybe other nice methods, Thank you  very much","let sequence $\{a_{n}\}$,such  $a_{1}=2\pi-6$, and $$a_{n}=\left\lceil\dfrac{2\pi}{a_{n-1}}\right\rceil\cdot a_{n-1}-2\pi$$ Find the  $$\lim_{n\to\infty}a_{n}$$ where $\left\lceil\dfrac{2\pi}{a_{n-1}}\right\rceil$is the smallest integer not less than $\dfrac{2\pi}{a_{n-1}}$ My try: since $$a_{n}>0,\dfrac{a_{n}}{a_{n-1}}=\lceil\dfrac{2\pi}{a_{n-1}}\rceil-\dfrac{2\pi}{a_{n-1}}\le 1$$ and then I can't Find this limit,and maybe other nice methods, Thank you  very much",,['limits']
80,Is my proof of $\lim_{x\to \infty}\frac 1x = 0$ correct?,Is my proof of  correct?,\lim_{x\to \infty}\frac 1x = 0,"I tried to prove $$\lim_{x\to \infty}\frac 1x = 0$$ I started as thus $$\lim_{x\to \infty}\frac 1x=\lim_{x\to \infty}\frac x{x^2}$$ Applying L'Hospital's Rule $$\lim_{x\to \infty}\frac 1x=\lim_{x\to \infty}\frac x{x^2}=\lim_{x\to \infty}\frac 1{2x}=\frac12\lim_{x\to \infty}\frac 1x$$ Thus, $$\frac12\lim_{x\to \infty}\frac 1x=\lim_{x\to \infty}\frac 1x$$ which therefore implies  $$\lim_{x\to \infty}\frac 1x = 0$$ QED.","I tried to prove $$\lim_{x\to \infty}\frac 1x = 0$$ I started as thus $$\lim_{x\to \infty}\frac 1x=\lim_{x\to \infty}\frac x{x^2}$$ Applying L'Hospital's Rule $$\lim_{x\to \infty}\frac 1x=\lim_{x\to \infty}\frac x{x^2}=\lim_{x\to \infty}\frac 1{2x}=\frac12\lim_{x\to \infty}\frac 1x$$ Thus, $$\frac12\lim_{x\to \infty}\frac 1x=\lim_{x\to \infty}\frac 1x$$ which therefore implies  $$\lim_{x\to \infty}\frac 1x = 0$$ QED.",,"['limits', 'proof-verification']"
81,$\lim_{n\rightarrow \infty}(1+\frac{r}{n})^n$ is equal to ${e^{r}}$?,is equal to ?,\lim_{n\rightarrow \infty}(1+\frac{r}{n})^n {e^{r}},"Since  $$\lim_{n\rightarrow \infty}\left(1+\frac{1}{n}\right)^n={e}$$ My strong hunch is that the following statement must also be true $$\lim_{n\rightarrow \infty}\left(1+\frac{r}{n}\right)^n = {e^{r}}$$ for all $r>0$. But I can neither prove or disprove it, any idea on how to prove it? Or if the statement is not true, how it should be modified so that it is true?","Since  $$\lim_{n\rightarrow \infty}\left(1+\frac{1}{n}\right)^n={e}$$ My strong hunch is that the following statement must also be true $$\lim_{n\rightarrow \infty}\left(1+\frac{r}{n}\right)^n = {e^{r}}$$ for all $r>0$. But I can neither prove or disprove it, any idea on how to prove it? Or if the statement is not true, how it should be modified so that it is true?",,['limits']
82,Limit $\lim_{x\to+\infty} {e^{2x}−1\over e^x−1}$,Limit,\lim_{x\to+\infty} {e^{2x}−1\over e^x−1},"I am having problem in solving this indetermination: $$\lim_{x\to+\infty} {e^{2x}−1\over e^x−1}$$ I tried to leave the term in common in evidence and cut them. I also tried to separate the limit for other notable limits but I always end up with the wrong solution. Could you guys give a hint, please? Thank you","I am having problem in solving this indetermination: $$\lim_{x\to+\infty} {e^{2x}−1\over e^x−1}$$ I tried to leave the term in common in evidence and cut them. I also tried to separate the limit for other notable limits but I always end up with the wrong solution. Could you guys give a hint, please? Thank you",,"['limits', 'logarithms', 'exponential-function']"
83,How to compute $\lim_{n\rightarrow\infty}e^{-n}\left(1+n+\frac{n^2}{2!}\cdots+\frac{n^n}{n!}\right)$ [duplicate],How to compute  [duplicate],\lim_{n\rightarrow\infty}e^{-n}\left(1+n+\frac{n^2}{2!}\cdots+\frac{n^n}{n!}\right),"This question already has answers here : Evaluating $\lim\limits_{n\to\infty} e^{-n} \sum\limits_{k=0}^{n} \frac{n^k}{k!}$ (9 answers) Closed 9 years ago . There is a probabilistic method to solve it. But I am not familiar with probability. I am trying to compute it by analytic method, such as using L Hospital's rule or Stolz formula, but they are not working.","This question already has answers here : Evaluating $\lim\limits_{n\to\infty} e^{-n} \sum\limits_{k=0}^{n} \frac{n^k}{k!}$ (9 answers) Closed 9 years ago . There is a probabilistic method to solve it. But I am not familiar with probability. I am trying to compute it by analytic method, such as using L Hospital's rule or Stolz formula, but they are not working.",,['limits']
84,Avoid L'hopital's rule [closed],Avoid L'hopital's rule [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question $$\lim_{x\to 0} {\ln(\cos x)\over \sin^2x} = ?$$ I can solve this by using L'Hopital's rule but how would I do this without this?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question $$\lim_{x\to 0} {\ln(\cos x)\over \sin^2x} = ?$$ I can solve this by using L'Hopital's rule but how would I do this without this?",,"['limits', 'limits-without-lhopital']"
85,Evaluate $\lim_{x\to 0} \frac{a^x -1}{x}$ without applying L'Hopital's Rule. [duplicate],Evaluate  without applying L'Hopital's Rule. [duplicate],\lim_{x\to 0} \frac{a^x -1}{x},"This question already has answers here : Show $\lim_{h\to 0} \frac{(a^h-1)}{h}$ exists without l'Hôpital or even referencing $e$ or natural log (5 answers) Closed 7 years ago . The questions is: Evaluate $$\lim_{x\to 0} \frac{a^x -1}{x}$$ without applying L'Hopital's Rule. Does this question fundamentally same as asking if the $\lim_{x\to 0} \frac{a^x -1}{x}$ exists? rather than straightway asking to find the limit. That means are questions (1) proving if the limit of a function exists and (2) asking what is the limit of that function, essentially same question?","This question already has answers here : Show $\lim_{h\to 0} \frac{(a^h-1)}{h}$ exists without l'Hôpital or even referencing $e$ or natural log (5 answers) Closed 7 years ago . The questions is: Evaluate $$\lim_{x\to 0} \frac{a^x -1}{x}$$ without applying L'Hopital's Rule. Does this question fundamentally same as asking if the $\lim_{x\to 0} \frac{a^x -1}{x}$ exists? rather than straightway asking to find the limit. That means are questions (1) proving if the limit of a function exists and (2) asking what is the limit of that function, essentially same question?",,"['limits', 'limits-without-lhopital']"
86,Prove that $\lim_{x\to \infty}\left(x-\ln\cosh x\right)=\ln 2$,Prove that,\lim_{x\to \infty}\left(x-\ln\cosh x\right)=\ln 2,"Prove that $\lim_{x\to \infty}\left(x-\ln\cosh x\right)=\ln 2$ I used L Hospital Rule but it does not simplify.Then i expanded $\cosh x$ by using McLaurin series but due to $x\to \infty$,this is also not working.How should i evaluate this limit?","Prove that $\lim_{x\to \infty}\left(x-\ln\cosh x\right)=\ln 2$ I used L Hospital Rule but it does not simplify.Then i expanded $\cosh x$ by using McLaurin series but due to $x\to \infty$,this is also not working.How should i evaluate this limit?",,['limits']
87,Limit with sin indeterminate,Limit with sin indeterminate,,How do I calculate the following limit $$\lim_{x\to\infty} \frac{3x-\sin x}{x+\sin x}$$ It's an indeterminate limit but how can I solve it? Does it help if I split it?The answer I got is $-1$ but it's $3$.,How do I calculate the following limit $$\lim_{x\to\infty} \frac{3x-\sin x}{x+\sin x}$$ It's an indeterminate limit but how can I solve it? Does it help if I split it?The answer I got is $-1$ but it's $3$.,,['limits']
88,"In the epsilon-delta definition, what is wrong if I said: ""given delta, there exists an epsilon""?","In the epsilon-delta definition, what is wrong if I said: ""given delta, there exists an epsilon""?",,"WHY are we always given $\epsilon > 0$ first, then solving for a $\delta>0$ ? This is in the limit definition. I want to ask: Can we say ""given $\delta>0$ , there exists $\epsilon>0$ ""? Since we can always solve for one given the other. I found three counterexamples, but I don't understand them: Let $f(x) = \sin x$ , let $L$ and $\delta$ be arbitrary real numbers.  Then $\epsilon = |L| + 2$ satisfies your definition. (from post ) Q: What's wrong with setting $\epsilon = |L| + 2$ ? It's big, but it's not wrong! Let $f(x) = 1/x$ , and let $a = 1$ .  The definition fails for $\delta \ge 1$ , since for any $\epsilon$ we can choose $x=1/(L+\epsilon)$ if $L+\epsilon > 1$ , so that $f(x)-L \ge \epsilon$ . (from post ) Q: What are they saying here? At $x=1$ , the definition fails for $\epsilon \ge 1$ too! The problem is not $\delta$ . The problem is the function is undefined for $x \le 0$ . Counterexample: $\lim\limits_{x \to 0} f(x) = L$ $f(x) = \begin{cases} \sin \frac{1}{x}, & x \ne 0 \\ 0, & x = 0 \end{cases}$ Given any $\delta > 0$ , we can  find $\epsilon > 0$ such that $|f(x) - L| < \epsilon$ whenever $|x| < \delta$ .  For instance, set $\epsilon = 2$ ; then any choice of $L \in (-1,1)$ will satisfy this ""reversed"" situation. (from post ) Q: I don't see how setting $\epsilon = 2$ violates any definition. I mean, we did find a $\epsilon$ for a given $\delta$ . Thanks all for the pouring answers, I'll get back to each one personally. If I did not choose an answer, that means all submissions are still welcomed! The best answer will be chosen based on # of upvotes (50%) and if I understood it and agree it's the best (50%).","WHY are we always given first, then solving for a ? This is in the limit definition. I want to ask: Can we say ""given , there exists ""? Since we can always solve for one given the other. I found three counterexamples, but I don't understand them: Let , let and be arbitrary real numbers.  Then satisfies your definition. (from post ) Q: What's wrong with setting ? It's big, but it's not wrong! Let , and let .  The definition fails for , since for any we can choose if , so that . (from post ) Q: What are they saying here? At , the definition fails for too! The problem is not . The problem is the function is undefined for . Counterexample: Given any , we can  find such that whenever .  For instance, set ; then any choice of will satisfy this ""reversed"" situation. (from post ) Q: I don't see how setting violates any definition. I mean, we did find a for a given . Thanks all for the pouring answers, I'll get back to each one personally. If I did not choose an answer, that means all submissions are still welcomed! The best answer will be chosen based on # of upvotes (50%) and if I understood it and agree it's the best (50%).","\epsilon > 0 \delta>0 \delta>0 \epsilon>0 f(x) = \sin x L \delta \epsilon = |L| + 2 \epsilon = |L| + 2 f(x) = 1/x a = 1 \delta \ge 1 \epsilon x=1/(L+\epsilon) L+\epsilon > 1 f(x)-L \ge \epsilon x=1 \epsilon \ge 1 \delta x \le 0 \lim\limits_{x \to 0} f(x) = L f(x) = \begin{cases} \sin \frac{1}{x}, & x \ne 0 \\ 0, & x = 0 \end{cases} \delta > 0 \epsilon > 0 |f(x) - L| < \epsilon |x| < \delta \epsilon = 2 L \in (-1,1) \epsilon = 2 \epsilon \delta","['limits', 'continuity', 'epsilon-delta']"
89,Puzzled by $\lim\limits_{x \to - \infty} \sqrt{x^2+x}-x$,Puzzled by,\lim\limits_{x \to - \infty} \sqrt{x^2+x}-x,"I am preparing for the next Semester and therefore review a few of my Analysis I limits, I have found this example in C.T. Michaels Analysis I: Compute $ \displaystyle \lim_{x \to - \infty} \sqrt{x^2+x}-x$ Aprior to this exercise I computed the same limit but as $x$ approaches $\infty$ rather than $- \infty$. So I thought that this should be a piece of cake, but apparently the $- \infty$ makes all the difference for me. My approach : This is the general approach I take when it comes to roots, especially square roots. Consider: $$ \sqrt{x^2+x}-x= \left(\sqrt{x^2+x}-x\right)\cdot \frac{\sqrt{x^2+x}+x}{\sqrt{x^2+x}+x}= \frac{x^2+x-x^2}{\sqrt{x^2+x}+x}=\frac{x}{\sqrt{x^2+x}+x}$$ Factoring out an $x$ will get me to: $$ \frac{x}{x\left(\sqrt{1+\frac{1}{x}}+1\right)}=\frac{1}{\sqrt{1+\frac{1}{x}}+1}$$ So as I take the limit of the above expressing as $x$ approaches $\infty$ I obtain the correct answer $1/2$. However, when I study the limit as $x$ approaches $- \infty$ I don't see how that would make a difference since $1 / - \infty=0$, but the correct answer in that case would be $\infty$ http://www.wolframalpha.com/input/?i=lim+x+to+-infty+sqrt%28x%5E2%2Bx%29-x My question (s): Where is/are my mistakes? Is it not possible to use the same methods for $- \infty$ as for $\infty$ when studying limits?","I am preparing for the next Semester and therefore review a few of my Analysis I limits, I have found this example in C.T. Michaels Analysis I: Compute $ \displaystyle \lim_{x \to - \infty} \sqrt{x^2+x}-x$ Aprior to this exercise I computed the same limit but as $x$ approaches $\infty$ rather than $- \infty$. So I thought that this should be a piece of cake, but apparently the $- \infty$ makes all the difference for me. My approach : This is the general approach I take when it comes to roots, especially square roots. Consider: $$ \sqrt{x^2+x}-x= \left(\sqrt{x^2+x}-x\right)\cdot \frac{\sqrt{x^2+x}+x}{\sqrt{x^2+x}+x}= \frac{x^2+x-x^2}{\sqrt{x^2+x}+x}=\frac{x}{\sqrt{x^2+x}+x}$$ Factoring out an $x$ will get me to: $$ \frac{x}{x\left(\sqrt{1+\frac{1}{x}}+1\right)}=\frac{1}{\sqrt{1+\frac{1}{x}}+1}$$ So as I take the limit of the above expressing as $x$ approaches $\infty$ I obtain the correct answer $1/2$. However, when I study the limit as $x$ approaches $- \infty$ I don't see how that would make a difference since $1 / - \infty=0$, but the correct answer in that case would be $\infty$ http://www.wolframalpha.com/input/?i=lim+x+to+-infty+sqrt%28x%5E2%2Bx%29-x My question (s): Where is/are my mistakes? Is it not possible to use the same methods for $- \infty$ as for $\infty$ when studying limits?",,"['limits', 'analysis', 'radicals']"
90,Can I break this limit into individual terms?,Can I break this limit into individual terms?,,$$\lim_{x\to \infty}  {\frac{x}{x^2+1} +\frac{x}{x^2+2} + ... + \frac{x}{x^2+x} }$$ It seems obvious that the result is zero for each term but in order to break the limit into its individual  parts we must know that every term's limit exists .,$$\lim_{x\to \infty}  {\frac{x}{x^2+1} +\frac{x}{x^2+2} + ... + \frac{x}{x^2+x} }$$ It seems obvious that the result is zero for each term but in order to break the limit into its individual  parts we must know that every term's limit exists .,,['limits']
91,"$\lim x \sin (1/x)$, when $x \to 0$",", when",\lim x \sin (1/x) x \to 0,"Here's my solution: $$\lim x\sin (1/x) = \lim\, x \dfrac{\sin (1/x)}{x(1/x)} = \lim\, x/x = 1$$ when $x \to 0$ However on the internet I read that the solution of this equation is 0. How can this be? Where a I making a mistake?","Here's my solution: $$\lim x\sin (1/x) = \lim\, x \dfrac{\sin (1/x)}{x(1/x)} = \lim\, x/x = 1$$ when $x \to 0$ However on the internet I read that the solution of this equation is 0. How can this be? Where a I making a mistake?",,"['limits', 'trigonometry']"
92,How to evaluate $\lim\limits_{n\to+\infty} \prod\limits_{k=1}^n (1+k/n^2)$?,How to evaluate ?,\lim\limits_{n\to+\infty} \prod\limits_{k=1}^n (1+k/n^2),I've got a limit which puzzle me several days. The question is $$ \lim_{n\to+\infty} \prod_{k=1}^n\left(1+\frac{k}{n^2}\right).$$ Can you help me? Thank you in advance,I've got a limit which puzzle me several days. The question is $$ \lim_{n\to+\infty} \prod_{k=1}^n\left(1+\frac{k}{n^2}\right).$$ Can you help me? Thank you in advance,,"['limits', 'products']"
93,$\lim\limits_{x \to \infty}\frac{2^x}{3^{x^2}}$,,\lim\limits_{x \to \infty}\frac{2^x}{3^{x^2}},Find $$\lim\limits_{x \to \infty}\frac{2^x}{3^{x^2}}$$ I can only reason with this intuitively. since $3^{x^2}$ grows much faster than $2^x$ the limit as $x \to \infty$ of $f$ must be 0. Is there a more rigorous way to show this?,Find $$\lim\limits_{x \to \infty}\frac{2^x}{3^{x^2}}$$ I can only reason with this intuitively. since $3^{x^2}$ grows much faster than $2^x$ the limit as $x \to \infty$ of $f$ must be 0. Is there a more rigorous way to show this?,,['limits']
94,Limit of $\frac{1}{\sqrt[n]{n!}}$ as $n$ approaches infinity [duplicate],Limit of  as  approaches infinity [duplicate],\frac{1}{\sqrt[n]{n!}} n,"This question already has answers here : $\lim\limits_{n \to{+}\infty}{\sqrt[n]{n!}}$ is infinite (12 answers) Closed 9 years ago . So i was trying to evalue this limit: $$\lim_{n \to \infty}\frac{1}{\sqrt[n]{n!}}, n \in \mathbb{N}$$ This, of course, by common sense is equal to zero (since factorial grows a lot faster). Is there a way to prove this limit without having to tackle with proving function growth rate. I;m not sure how that would be done, but I believe i would have to expand the factorial function to set $\mathbb{R}$ in order to compare, and that's still beyond my abilities. Thanks.","This question already has answers here : $\lim\limits_{n \to{+}\infty}{\sqrt[n]{n!}}$ is infinite (12 answers) Closed 9 years ago . So i was trying to evalue this limit: $$\lim_{n \to \infty}\frac{1}{\sqrt[n]{n!}}, n \in \mathbb{N}$$ This, of course, by common sense is equal to zero (since factorial grows a lot faster). Is there a way to prove this limit without having to tackle with proving function growth rate. I;m not sure how that would be done, but I believe i would have to expand the factorial function to set $\mathbb{R}$ in order to compare, and that's still beyond my abilities. Thanks.",,['limits']
95,Why does the following limit not exist?,Why does the following limit not exist?,,"The limit is $$\lim\limits_{(x,y)\to(0,0)}\frac{\sqrt y}{\sqrt x}$$  I thought it was 1. Also what about $$\lim\limits_{(x,y)\to(0,0)}\frac{ y}{ x}$$","The limit is $$\lim\limits_{(x,y)\to(0,0)}\frac{\sqrt y}{\sqrt x}$$  I thought it was 1. Also what about $$\lim\limits_{(x,y)\to(0,0)}\frac{ y}{ x}$$",,"['limits', 'multivariable-calculus']"
96,$\displaystyle \lim_{x\to 0} \frac{e^{2x}-1}{3x} $ help please?,help please?,\displaystyle \lim_{x\to 0} \frac{e^{2x}-1}{3x} ,So I have to find the $$\displaystyle \lim_{x\to 0} \frac{e^{2x}-1}{3x} $$ I first solved this by L'hopital and got $\frac{2}{3}$ but now I read carefully and it says in my book that I shouldn't solve this by L'hopital..any hints?,So I have to find the $$\displaystyle \lim_{x\to 0} \frac{e^{2x}-1}{3x} $$ I first solved this by L'hopital and got $\frac{2}{3}$ but now I read carefully and it says in my book that I shouldn't solve this by L'hopital..any hints?,,['limits']
97,Evaluating $\lim\limits_{n\rightarrow \infty} \frac1{n^2}\ln \left( \frac{(n!)^n}{(0!1!2!...n!)^2} \right)$,Evaluating,\lim\limits_{n\rightarrow \infty} \frac1{n^2}\ln \left( \frac{(n!)^n}{(0!1!2!...n!)^2} \right),"Evaluating $$\lim\limits_{n\rightarrow \infty} \frac1{n^2}\ln \left( \frac{(n!)^n}{(0!1!2!...n!)^2} \right)$$ I'm not quite sure where to start in evaluating this. Some pointers, or a solution, would greatly be appreciated.","Evaluating $$\lim\limits_{n\rightarrow \infty} \frac1{n^2}\ln \left( \frac{(n!)^n}{(0!1!2!...n!)^2} \right)$$ I'm not quite sure where to start in evaluating this. Some pointers, or a solution, would greatly be appreciated.",,"['limits', 'summation', 'gamma-function']"
98,Derivative of $x^x$ at $x=1$ from first principles,Derivative of  at  from first principles,x^x x=1,Find the derivative of $x^x$ at $x=1$ by definition (i.e. using the limit of the incremental ratio). The only trick I know is $x^x = e^{x \ln x}$ but it doesn't work.,Find the derivative of $x^x$ at $x=1$ by definition (i.e. using the limit of the incremental ratio). The only trick I know is $x^x = e^{x \ln x}$ but it doesn't work.,,"['limits', 'derivatives']"
99,Evaluating $\lim_{\theta \to 0^+}\frac{\sin\theta}{\theta^2}$,Evaluating,\lim_{\theta \to 0^+}\frac{\sin\theta}{\theta^2},"How do I evaluate $$\lim_{\theta \to 0^+}\frac{\sin\theta}{\theta^2}?$$ I tried the following: $$\lim_{\theta \to 0^+}\frac{\sin\theta}{\theta^2} = \lim_{\theta \to 0^+}\frac{1}{\theta}\cdot \lim_{\theta \to 0^+}\frac{\sin\theta}{\theta} = \lim_{\theta \to 0^+}\frac{1}{\theta} = +\infty$$ However, I feel that there is an error with my work, since I believe it isn't acceptable to separate a limit when it separates into something that has a value of infinity. Is there an issue with my work here?","How do I evaluate $$\lim_{\theta \to 0^+}\frac{\sin\theta}{\theta^2}?$$ I tried the following: $$\lim_{\theta \to 0^+}\frac{\sin\theta}{\theta^2} = \lim_{\theta \to 0^+}\frac{1}{\theta}\cdot \lim_{\theta \to 0^+}\frac{\sin\theta}{\theta} = \lim_{\theta \to 0^+}\frac{1}{\theta} = +\infty$$ However, I feel that there is an error with my work, since I believe it isn't acceptable to separate a limit when it separates into something that has a value of infinity. Is there an issue with my work here?",,['limits']
