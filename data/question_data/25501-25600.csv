,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Exponential growth of a cow farm with constraints in Minecraft,Exponential growth of a cow farm with constraints in Minecraft,,"This question is distinct from Exponential growth of cow populations in Minecraft in that an important constraint present in Minecraft is missing from that post. Here are the following constraints: We start with $2$ adult cows, and $0$ baby cows. Any given pair of adult cows can only be bred to form $1$ baby cow every $5$ minutes. ( not included in the other post ) A baby cow becomes an adult after $20$ minutes. My first (failed) attempt at solving this was to intialise the following functions $A(0)=2, B(0)=0$ , where $A$ and $B$ are funtions of $t$ (time, in minutes) representing the number of Adult and Baby cows respectively. Naively, I represented this via a system of two linear differential equations given the above initial conditions: $$\frac{dA}{dt}=\frac{B}{20}$$ $$\frac{dB}{dt}=\frac{A}{5}$$ Where my thought process was that every 20 minutes there would be an addition of $B$ adults, and every 5 minutes there would be an addition of $\frac{A}{2}$ baby cows. By use of linear algebra, it can be shown that the equations of $A$ and $B$ are as follows: $$A(t)=e^{-\frac{t}{10\sqrt{2}}}+e^{\frac{t}{10\sqrt{2}}}$$ $$B(t)=\frac{1}{\sqrt{2}}e^{\frac{-t}{10\sqrt{2}}}\left(e^{\frac{t}{5\sqrt{2}}}-1\right)$$ Where the total number of cows, call this $C(t)$ is some rounded form of $A(t)+B(t)$ . The problem with this: Given that at any given time, the 'age' of the babies within the pool will be different (non-uniform), the formation of adults will be staggered. This is the point I'm struggling to continue. I've tried to think of $B$ as forming subsequences every $5$ minutes, but I am really unsure how to implement this in any meaningful way. So how can this formula be generated? Edit : On second thoughts, this might be reducible to the post linked at the start. Not too sure though. Edit 2 : As pointed out in the comments by Daniel Mathias, this question is distinct from the linked post.","This question is distinct from Exponential growth of cow populations in Minecraft in that an important constraint present in Minecraft is missing from that post. Here are the following constraints: We start with adult cows, and baby cows. Any given pair of adult cows can only be bred to form baby cow every minutes. ( not included in the other post ) A baby cow becomes an adult after minutes. My first (failed) attempt at solving this was to intialise the following functions , where and are funtions of (time, in minutes) representing the number of Adult and Baby cows respectively. Naively, I represented this via a system of two linear differential equations given the above initial conditions: Where my thought process was that every 20 minutes there would be an addition of adults, and every 5 minutes there would be an addition of baby cows. By use of linear algebra, it can be shown that the equations of and are as follows: Where the total number of cows, call this is some rounded form of . The problem with this: Given that at any given time, the 'age' of the babies within the pool will be different (non-uniform), the formation of adults will be staggered. This is the point I'm struggling to continue. I've tried to think of as forming subsequences every minutes, but I am really unsure how to implement this in any meaningful way. So how can this formula be generated? Edit : On second thoughts, this might be reducible to the post linked at the start. Not too sure though. Edit 2 : As pointed out in the comments by Daniel Mathias, this question is distinct from the linked post.","2 0 1 5 20 A(0)=2, B(0)=0 A B t \frac{dA}{dt}=\frac{B}{20} \frac{dB}{dt}=\frac{A}{5} B \frac{A}{2} A B A(t)=e^{-\frac{t}{10\sqrt{2}}}+e^{\frac{t}{10\sqrt{2}}} B(t)=\frac{1}{\sqrt{2}}e^{\frac{-t}{10\sqrt{2}}}\left(e^{\frac{t}{5\sqrt{2}}}-1\right) C(t) A(t)+B(t) B 5","['linear-algebra', 'ordinary-differential-equations', 'recurrence-relations', 'exponential-function', 'recreational-mathematics']"
1,Show that the Matrix mapping f defined as $f(A) = A^p$ is differentiable,Show that the Matrix mapping f defined as  is differentiable,f(A) = A^p,"I've been asked to show that for a function $f:M_{n\times n}(\mathbb R)\longrightarrow M_{n\times n}(\mathbb R) $ that defined as $f(A) = A^p$ for all $2\leq p\in \mathbb N$ is differentiable. I've tried solving this using induction and showing that the partial derivatives are continues and then to conclude that f is differentiable. On the case of p = 2 is pretty simple to show that, but for the case of multiplying p matrices is more complex and I feel like i'm not on the right way. Any suggestions or alternative ways of solving it? Edit: I found a different approach for this case, in addition to the solution mentioned below. I can define the function $h:M_{n\times n}(\mathbb R)\longrightarrow M_{n\times n}(\mathbb R)\times M_{n\times n}(\mathbb R)$ which is defined by $h(X) = (X,X^{p-1})$ and then define the bilnear map $g:M_{n\times n}(\mathbb R)\times M_{n\times n}(\mathbb R) \longrightarrow M_{n\times n}(\mathbb R)$ and now we get that $f=g\circ h$ . now $g$ is a bilnear map thus it's differentiable, and now by induction on $p$ we can solve the problem using the following formula: Derivative of the bilnear map $\beta:\mathbb R^m\times\mathbb R^n\longrightarrow\mathbb R^d$ is $D\beta(x_0,y_0)(h,k)=\beta (x_0,k)+\beta(h,y_0)$ And the chain rule.","I've been asked to show that for a function that defined as for all is differentiable. I've tried solving this using induction and showing that the partial derivatives are continues and then to conclude that f is differentiable. On the case of p = 2 is pretty simple to show that, but for the case of multiplying p matrices is more complex and I feel like i'm not on the right way. Any suggestions or alternative ways of solving it? Edit: I found a different approach for this case, in addition to the solution mentioned below. I can define the function which is defined by and then define the bilnear map and now we get that . now is a bilnear map thus it's differentiable, and now by induction on we can solve the problem using the following formula: Derivative of the bilnear map is And the chain rule.","f:M_{n\times n}(\mathbb R)\longrightarrow M_{n\times n}(\mathbb R)  f(A) = A^p 2\leq p\in \mathbb N h:M_{n\times n}(\mathbb R)\longrightarrow M_{n\times n}(\mathbb R)\times M_{n\times n}(\mathbb R) h(X) = (X,X^{p-1}) g:M_{n\times n}(\mathbb R)\times M_{n\times n}(\mathbb R) \longrightarrow M_{n\times n}(\mathbb R) f=g\circ h g p \beta:\mathbb R^m\times\mathbb R^n\longrightarrow\mathbb R^d D\beta(x_0,y_0)(h,k)=\beta (x_0,k)+\beta(h,y_0)","['real-analysis', 'linear-algebra', 'matrices', 'derivatives']"
2,"How to find *all* roots of arbitrarily high degree polynomials (in particular, characteristic polynomials)?","How to find *all* roots of arbitrarily high degree polynomials (in particular, characteristic polynomials)?",,"In eigenvalue characteristic equation computations of large matrices, it is often necessary to find all the the roots of this polynomial. I understand that there is no general explicit solution for irrational roots of polynomials of degree higher than 5. We might thus use numerical approximations, such as Newton's method, to find a root for the polynomial equation. My question is, for very high degree polynomials, eg. characteristic equation of degree 100, how can we make systematic/algorithmic computation to find all the 100 roots to this polynomial? Is there a consistent method that will always find all the roots to this eigenvalue characteristic equation? Thank you!","In eigenvalue characteristic equation computations of large matrices, it is often necessary to find all the the roots of this polynomial. I understand that there is no general explicit solution for irrational roots of polynomials of degree higher than 5. We might thus use numerical approximations, such as Newton's method, to find a root for the polynomial equation. My question is, for very high degree polynomials, eg. characteristic equation of degree 100, how can we make systematic/algorithmic computation to find all the 100 roots to this polynomial? Is there a consistent method that will always find all the roots to this eigenvalue characteristic equation? Thank you!",,"['linear-algebra', 'matrices', 'polynomials', 'eigenvalues-eigenvectors', 'roots']"
3,Decomposition of matrix occuring in problem of finding $n+1$ vectors in $\mathbb{R}^n$ with pairwise equal inner product,Decomposition of matrix occuring in problem of finding  vectors in  with pairwise equal inner product,n+1 \mathbb{R}^n,"I was toying with my intuition that there are always $n+1$ unit vectors in $\mathbb{R}^n$ such that every pair $v_i$ and $v_j$ ( $i\neq j$ ) has the same angle between them. As those vectors are normalized this is equivalent to $$v_i\cdot v_j=\begin{cases} 1,& i=j \\m, & i\neq j \end{cases}$$ with $|m|<1$ . So if $V$ is the $n\times (n+1)$ matrix whose columns are formed by the $v_i$ we have $$M:=V^TV=\begin{pmatrix}1 & m & \cdots &m \\m&1&\cdots&m\\ \vdots &&\ddots&\vdots \\ m&m&\cdots&1 \end{pmatrix}$$ $M$ is a $(n+1)\times (n+1)$ matrix and must be singular (as it is the product of two rank $n$ matrices). The sum of its rows is $(nm+1,nm+1,\ldots,nm+1)$ . For $m=-\frac{1}{n}$ this is the null vector giving the required dot product $m$ as a function of $n$ . E.g., for $n=2$ , $m=-\frac{1}{2}$ corresponding to the expected angle of $120°=\arccos{-\frac{1}{2}}$ . Now I'm asking is there a matrix decomposition so that I can get $V$ back from the product $M$ ? (I know that $V$ is not unique since I can freely rotate the vectors $v_i$ about an arbitrarily chosen fixed axis without changing their pairwise dot product.) I tried spectral decomposition of $M$ but that gives $n+1$ dimensional vectors and the $n$ nonzero eigenvalues of $M$ are all equal to $\frac{n+1}{n}$ which means that any linear combination of eigenvectors corresponding to that eigenvalue is also an eigenvector making the dot product between those eigenvectors rather arbitrary.","I was toying with my intuition that there are always unit vectors in such that every pair and ( ) has the same angle between them. As those vectors are normalized this is equivalent to with . So if is the matrix whose columns are formed by the we have is a matrix and must be singular (as it is the product of two rank matrices). The sum of its rows is . For this is the null vector giving the required dot product as a function of . E.g., for , corresponding to the expected angle of . Now I'm asking is there a matrix decomposition so that I can get back from the product ? (I know that is not unique since I can freely rotate the vectors about an arbitrarily chosen fixed axis without changing their pairwise dot product.) I tried spectral decomposition of but that gives dimensional vectors and the nonzero eigenvalues of are all equal to which means that any linear combination of eigenvectors corresponding to that eigenvalue is also an eigenvector making the dot product between those eigenvectors rather arbitrary.","n+1 \mathbb{R}^n v_i v_j i\neq j v_i\cdot v_j=\begin{cases} 1,& i=j \\m, & i\neq j \end{cases} |m|<1 V n\times (n+1) v_i M:=V^TV=\begin{pmatrix}1 & m & \cdots &m \\m&1&\cdots&m\\ \vdots &&\ddots&\vdots \\ m&m&\cdots&1 \end{pmatrix} M (n+1)\times (n+1) n (nm+1,nm+1,\ldots,nm+1) m=-\frac{1}{n} m n n=2 m=-\frac{1}{2} 120°=\arccos{-\frac{1}{2}} V M V v_i M n+1 n M \frac{n+1}{n}","['linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors', 'matrix-decomposition']"
4,Center of the symplectic group,Center of the symplectic group,,"I am trying to figure out what the center of $Sp(n)$ is. I know that $Sp(n) = U(n) \cap Sp(n,\mathbb{C})$ , where $Sp(n,\mathbb{C})$ are all $2n \times 2n$ complex matrices $A$ such that $A^TJA = J$ , where $$ J = \begin{pmatrix} 0 & -I_n\\ I_n & 0 \end{pmatrix}. $$ I was able to find the center of most of the other classical matrix groups using the Spectral theorem and permutation matrices, but for this one I'm having a hard time. Maybe after I use the spectral theorem on an element of the center, I could decompose the unitary matrix that diagonalizes it by an element in $Sp(n)$ and some other matrix?","I am trying to figure out what the center of is. I know that , where are all complex matrices such that , where I was able to find the center of most of the other classical matrix groups using the Spectral theorem and permutation matrices, but for this one I'm having a hard time. Maybe after I use the spectral theorem on an element of the center, I could decompose the unitary matrix that diagonalizes it by an element in and some other matrix?","Sp(n) Sp(n) = U(n) \cap Sp(n,\mathbb{C}) Sp(n,\mathbb{C}) 2n \times 2n A A^TJA = J 
J
=
\begin{pmatrix}
0 & -I_n\\
I_n & 0
\end{pmatrix}.
 Sp(n)","['linear-algebra', 'group-theory', 'lie-groups', 'lie-algebras', 'matrix-decomposition']"
5,Inner product of polynomials defined using determinant.,Inner product of polynomials defined using determinant.,,"The following question was on my qualifying exam. I'd like to understand this question, but I've never seen anything like this, so I don't even know what key words to research or where to look for more information. I'm not necessarily looking for an answer to the following question. Rather, in years past, exams have routinely featured a question that explores the connection between determinants and polynomials, such as the following question. So any sources with more information about this topic, or any hints about the following problem, would be much appreciated! Question. Consider a continuous function $f: \mathbb{R} \rightarrow (0, \infty)$ such that $\int_{-\infty}^\infty |x|^j f(x) \ dx < \infty$ for all $j \geq 0$ . Denote by $\mathcal{P}_m$ the vector space of all real-valued polynomials of degree at most $m$ , equipped with the inner product $$\langle g, h \rangle = \int_{-\infty}^\infty g(x)h(x)f(x) \ dx.$$ Let $m_j = \int_{-\infty}^\infty x^j f(x) \ dx \ (j = 0, 1, 2, \dots)$ and consider the polynomials $p_0(x) = m_0$ and \begin{align} p_n(x) = \det \left( \begin{matrix}  m_0 & m_1 & m_2 & \dots & m_n \\ m_1 & m_2 & m_3 & \dots & m_{n+1} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ m_{n-1} & m_n & m_{n+1} & \dots & m_{2n-1} \\ 1 & x & x^2 & \dots & x^n  \end{matrix} \right) \ \ \ \ n = 1, 2, \dots \end{align} Prove that $p_0, \dots, p_m$ are orthogonal in $\mathcal{P}_m$ . Observations. I have been able to ascertain the following intuition: Any (nonzero) polynomial cannot be integrated from $-\infty$ to $\infty$ , so for polynomials $g, h \in \mathcal{P}_m$ , their product will certainly fail to have an integral. Therefore, the function $f$ is introduced. The function $f$ must have tails that go to zero fast enough so that not only can $f$ can be integrated across all of $\mathbb{R}$ , but also $f$ modulates the end behavior of the term $x^j$ so that it can be integrated. Then, the product of $f$ and $g$ will be a polynomial, the integral of a polynomial can be split across the terms, the coefficients pulled out of the integrals, and the result will be of the form $\sum_{j=1}^{2m} a_j \int_{-\infty}^\infty x^j f(x) \ dx = \sum_{j=1}^{2m} a_j m_j$ , where $a_j$ is some constant and $m_j$ is defined as per the problem. Lastly, I have checked a couple of lower order polynomials and confirmed that their inner product is zero. The computations have felt a little like the kind of index-tracking exercises that I've done during my limited exposure to multilinear algebra. So I am thinking that maybe with more powerful tools for tracking indices, I can prove the desired result. Or maybe it is an induction on $m$ . Or maybe it has something to do with somehow forcing a determinant to have linearly dependent rows. If you have read this far, bless your heart!","The following question was on my qualifying exam. I'd like to understand this question, but I've never seen anything like this, so I don't even know what key words to research or where to look for more information. I'm not necessarily looking for an answer to the following question. Rather, in years past, exams have routinely featured a question that explores the connection between determinants and polynomials, such as the following question. So any sources with more information about this topic, or any hints about the following problem, would be much appreciated! Question. Consider a continuous function such that for all . Denote by the vector space of all real-valued polynomials of degree at most , equipped with the inner product Let and consider the polynomials and Prove that are orthogonal in . Observations. I have been able to ascertain the following intuition: Any (nonzero) polynomial cannot be integrated from to , so for polynomials , their product will certainly fail to have an integral. Therefore, the function is introduced. The function must have tails that go to zero fast enough so that not only can can be integrated across all of , but also modulates the end behavior of the term so that it can be integrated. Then, the product of and will be a polynomial, the integral of a polynomial can be split across the terms, the coefficients pulled out of the integrals, and the result will be of the form , where is some constant and is defined as per the problem. Lastly, I have checked a couple of lower order polynomials and confirmed that their inner product is zero. The computations have felt a little like the kind of index-tracking exercises that I've done during my limited exposure to multilinear algebra. So I am thinking that maybe with more powerful tools for tracking indices, I can prove the desired result. Or maybe it is an induction on . Or maybe it has something to do with somehow forcing a determinant to have linearly dependent rows. If you have read this far, bless your heart!","f: \mathbb{R} \rightarrow (0, \infty) \int_{-\infty}^\infty |x|^j f(x) \ dx < \infty j \geq 0 \mathcal{P}_m m \langle g, h \rangle = \int_{-\infty}^\infty g(x)h(x)f(x) \ dx. m_j = \int_{-\infty}^\infty x^j f(x) \ dx \ (j = 0, 1, 2, \dots) p_0(x) = m_0 \begin{align}
p_n(x) = \det \left( \begin{matrix} 
m_0 & m_1 & m_2 & \dots & m_n \\
m_1 & m_2 & m_3 & \dots & m_{n+1} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
m_{n-1} & m_n & m_{n+1} & \dots & m_{2n-1} \\
1 & x & x^2 & \dots & x^n 
\end{matrix} \right) \ \ \ \ n = 1, 2, \dots
\end{align} p_0, \dots, p_m \mathcal{P}_m -\infty \infty g, h \in \mathcal{P}_m f f f \mathbb{R} f x^j f g \sum_{j=1}^{2m} a_j \int_{-\infty}^\infty x^j f(x) \ dx = \sum_{j=1}^{2m} a_j m_j a_j m_j m","['linear-algebra', 'determinant', 'inner-products', 'orthogonal-polynomials']"
6,For which $n$ is the symmetric group $S_n$ a subgroup of the special orthogonal group $SO(3)$?,For which  is the symmetric group  a subgroup of the special orthogonal group ?,n S_n SO(3),"For which $n$ is the symmetric group $S_n$ a subgroup of the special orthogonal group $SO(3)$ ? For example, this holds for $n≤4$ , however I don't know if it holds for $n=5$ or what happens for larger $n$ .","For which is the symmetric group a subgroup of the special orthogonal group ? For example, this holds for , however I don't know if it holds for or what happens for larger .",n S_n SO(3) n≤4 n=5 n,"['linear-algebra', 'group-theory', 'finite-groups', 'symmetric-groups']"
7,"Prove that $T(v) = v,$ for some $v \in \Bbb R^3 \setminus \{0\}.$",Prove that  for some,"T(v) = v, v \in \Bbb R^3 \setminus \{0\}.","Let $T \in \text {SO} (3) : = \left \{T \in M_3 (\Bbb R)\ |\ TT^t = T^tT = I,\ \det (T) = 1 \right \}.$ Prove that there exists $v \in \Bbb R^3 \setminus \{0\}$ such that $T(v) = v.$ Hence conclude that the elements of $\text {SO} (3)$ are rotations by some angle about some uniquely determined axis. The first part is easier. To prove that $T(v) = v,$ for some $0 \neq v \in \Bbb R^3$ it is enough to show that $\det (T - I) = 0.$ Now $$\begin{align*} \det (T - I) & = 1 \cdot \det (T - I) \\ & = \det (T) \det (T - I) \\ & = \det (T^t) \det (T - I) \\ & = \det (T^t T - T^t) \\ & = \det (I - T^t) \\ & = \det (I^t - T^t) \\ & = \det ((I - T)^t) \\ & = \det (I - T) \\ & = (-1)^3 \det (T - I) \\ & = - \det (T - I) \end{align*}$$ This shows that $2 \det (T - I) = 0 \implies \det (T - I) = 0,$ as required. How to prove the second conclusion? Any help in this regard will be appreciated. Thanks in advance.",Let Prove that there exists such that Hence conclude that the elements of are rotations by some angle about some uniquely determined axis. The first part is easier. To prove that for some it is enough to show that Now This shows that as required. How to prove the second conclusion? Any help in this regard will be appreciated. Thanks in advance.,"T \in \text {SO} (3) : = \left \{T \in M_3 (\Bbb R)\ |\ TT^t = T^tT = I,\ \det (T) = 1 \right \}. v \in \Bbb R^3 \setminus \{0\} T(v) = v. \text {SO} (3) T(v) = v, 0 \neq v \in \Bbb R^3 \det (T - I) = 0. \begin{align*} \det (T - I) & = 1 \cdot \det (T - I) \\ & = \det (T) \det (T - I) \\ & = \det (T^t) \det (T - I) \\ & = \det (T^t T - T^t) \\ & = \det (I - T^t) \\ & = \det (I^t - T^t) \\ & = \det ((I - T)^t) \\ & = \det (I - T) \\ & = (-1)^3 \det (T - I) \\ & = - \det (T - I) \end{align*} 2 \det (T - I) = 0 \implies \det (T - I) = 0,","['linear-algebra', 'eigenvalues-eigenvectors', 'rotations', 'orthogonal-matrices', 'fixed-points']"
8,Prove that $\det(\lambda I-A)\le\lambda^n-1$ [closed],Prove that  [closed],\det(\lambda I-A)\le\lambda^n-1,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let $A$ be an $n\times n$ nonnegative matrix with spectral radius $1$ . Suppose $\lambda>1$ , then prove $\det(\lambda I-A)\le\lambda^n-1$ .","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let be an nonnegative matrix with spectral radius . Suppose , then prove .",A n\times n 1 \lambda>1 \det(\lambda I-A)\le\lambda^n-1,"['linear-algebra', 'matrices', 'graph-theory']"
9,What is $\det(A + I)$ when $AA^t = I$ and $\det(A) < 0$?,What is  when  and ?,\det(A + I) AA^t = I \det(A) < 0,"Notation: $A^t$ : transpose of matrix $A$ . $\det A$ : determinant of matrix $A$ . $I$ : the identity matrix. We know: $A \in M_{n\times n}(\mathbb{R}), \\ AA^t = I, \ \mathrm{and \ det}(A) < 0$ We want: $\det(A + I)$ From $\det A = \det A^t$ , $\det A^{-1} = \frac{1}{\det A}$ and $\det A < 0$ we can deduce: $\det A = -1$ .  I tried calculating $$\det(A + I) = \det(A(I + A^t)) = \det(A) \det(I + A^t) = - \det(I + A^t)$$ but that didn't lead to anything. I tried using the fact that $A + A^t$ is symmetric and $A - A^t$ is skew-symmetric but I couldn't achieve anything with that either. I thought about interpreting it via a linear transformation but couldn't come up with anything. I learned that matrices, where $AA^t = I$ , are called orthogonal but that's all I know about orthogonal matrices so I'm hoping to find a solution that doesn't get too deep into orthogonality. Hints would be appreciated. Thank you in advance!","Notation: : transpose of matrix . : determinant of matrix . : the identity matrix. We know: We want: From , and we can deduce: .  I tried calculating but that didn't lead to anything. I tried using the fact that is symmetric and is skew-symmetric but I couldn't achieve anything with that either. I thought about interpreting it via a linear transformation but couldn't come up with anything. I learned that matrices, where , are called orthogonal but that's all I know about orthogonal matrices so I'm hoping to find a solution that doesn't get too deep into orthogonality. Hints would be appreciated. Thank you in advance!","A^t A \det A A I A \in M_{n\times n}(\mathbb{R}), \\
AA^t = I, \ \mathrm{and \ det}(A) < 0 \det(A + I) \det A = \det A^t \det A^{-1} = \frac{1}{\det A} \det A < 0 \det A = -1 \det(A + I) = \det(A(I + A^t)) = \det(A) \det(I + A^t) = - \det(I + A^t) A + A^t A - A^t AA^t = I","['linear-algebra', 'matrices', 'determinant', 'orthogonal-matrices']"
10,Infinitesimal rotation matrix close to the identity,Infinitesimal rotation matrix close to the identity,,"Suppose $R_{ij}$ is a matrix that corresponds to an ""infinitesimal"" rotation. Then my notes mention that such matrix can be rewritten as: $$R_{ij} = \delta _{ij} + \epsilon w_{ij}$$ where $w_{ij} = - w_{ji}$ is antisymmetric and $\epsilon$ is a small quantity. Now I know that a rotation matrix must be orthogonal,but I am unsure on why is $w_{ij}$ must be antisymmetric. Also why would such a matrix represent an infinitesimal rotation?","Suppose is a matrix that corresponds to an ""infinitesimal"" rotation. Then my notes mention that such matrix can be rewritten as: where is antisymmetric and is a small quantity. Now I know that a rotation matrix must be orthogonal,but I am unsure on why is must be antisymmetric. Also why would such a matrix represent an infinitesimal rotation?",R_{ij} R_{ij} = \delta _{ij} + \epsilon w_{ij} w_{ij} = - w_{ji} \epsilon w_{ij},"['linear-algebra', 'linear-transformations', 'rotations']"
11,Least Singular Value of bidiagonal matrix,Least Singular Value of bidiagonal matrix,,"Consider the $n \times n$ , bidiagonal matrix $$ \left(\begin{array}{ccccc} x & \\ 1 & x \\ & 1& x \\ & & & \ddots \\ & & & 1 & x \end{array} \right) $$ It is claimed that the least singular value for this matrix is less than $O(2^{-n})$ when $x$ is fixed and $|x-1| > 1/2$ .  Is there a simple way to see this?  I have thought about it as a diagonal shift of a nilpotent matrix, but this does not seem to help.","Consider the , bidiagonal matrix It is claimed that the least singular value for this matrix is less than when is fixed and .  Is there a simple way to see this?  I have thought about it as a diagonal shift of a nilpotent matrix, but this does not seem to help.","n \times n 
\left(\begin{array}{ccccc}
x & \\
1 & x \\
& 1& x \\
& & & \ddots \\
& & & 1 & x
\end{array} \right)
 O(2^{-n}) x |x-1| > 1/2",['linear-algebra']
12,Prove $\det(A+B)=\det(B)$ implies that $A = 0$,Prove  implies that,\det(A+B)=\det(B) A = 0,"first time here. I tried my best to translate the question : Show that the only matrix $A$ of size $n\times n$ that verify $\det(A+B)=\det(B)$ , for any $n\times n$ matrix $B$ , is the null matrix (the matrix full of zero, here of size n). I thought about using the formula $\det(A)$ that uses matrices of size $(n-1)$ : image But can't find to make it work. Any ideas ? I don't need a full proof, only the start of it so I can try myself because i'm stuck","first time here. I tried my best to translate the question : Show that the only matrix of size that verify , for any matrix , is the null matrix (the matrix full of zero, here of size n). I thought about using the formula that uses matrices of size : image But can't find to make it work. Any ideas ? I don't need a full proof, only the start of it so I can try myself because i'm stuck",A n\times n \det(A+B)=\det(B) n\times n B \det(A) (n-1),"['linear-algebra', 'matrices', 'determinant']"
13,Prove/Disprove an inner product on a complex linear space restricted to its real structure is also an inner product,Prove/Disprove an inner product on a complex linear space restricted to its real structure is also an inner product,,"Let $V$ be an $n$ -dimensional linear space and $(\cdot, \cdot)$ be an inner product on it. Define the conjugation map $\sigma: V \to V$ such that for any $\alpha, \beta \in V$ and $\lambda \in \mathbb{C}$ , $\sigma(\alpha + \beta) = \sigma(\alpha) + \sigma(\beta)$ , $\sigma(\lambda\alpha) =  \bar{\lambda}\sigma(\alpha)$ , $\sigma^2(\alpha) = \alpha$ . The space \begin{align*} R_\sigma(V) = \{\alpha \in V: \sigma(\alpha) = \alpha\} \end{align*} is known as the real structure of $V$ . In the link, it has been shown $R_\sigma(V)$ is an $n$ -dimensional real linear space. I felt the inner product $(\cdot, \cdot)$ , which is originally defined on $V$ , when restricted to $R_\sigma(V)$ , is also an inner product. While the positiveness and bilinearity are easy to prove, it seems difficult to show the symmetry. In particular, how to show $(\alpha, \beta)$ is a real number when $\alpha, \beta \in  R_\sigma(V)$ ?","Let be an -dimensional linear space and be an inner product on it. Define the conjugation map such that for any and , , , . The space is known as the real structure of . In the link, it has been shown is an -dimensional real linear space. I felt the inner product , which is originally defined on , when restricted to , is also an inner product. While the positiveness and bilinearity are easy to prove, it seems difficult to show the symmetry. In particular, how to show is a real number when ?","V n (\cdot, \cdot) \sigma: V \to V \alpha, \beta \in V \lambda \in \mathbb{C} \sigma(\alpha + \beta) = \sigma(\alpha) + \sigma(\beta) \sigma(\lambda\alpha) = 
\bar{\lambda}\sigma(\alpha) \sigma^2(\alpha) = \alpha \begin{align*}
R_\sigma(V) = \{\alpha \in V: \sigma(\alpha) = \alpha\}
\end{align*} V R_\sigma(V) n (\cdot, \cdot) V R_\sigma(V) (\alpha, \beta) \alpha, \beta \in 
R_\sigma(V)","['linear-algebra', 'inner-products']"
14,Matrix multiplied by its pseudo-inverse doesn't give the identity matrix. Why?,Matrix multiplied by its pseudo-inverse doesn't give the identity matrix. Why?,,"Using Matlab, I randomly generate matrix $A \in \Bbb C^{2 \times 1}$ and compute its pseudo-inverse $A^{+}$ . I notice that $AA^{+} \neq I$ , and yet $\mbox{Tr}(AA^{+}) = 1$ . For other sizes it seems like the trace is equal to the smaller dimension of $A$ . I couldn't find this property explained. Could anyone help me understand these two facts?","Using Matlab, I randomly generate matrix and compute its pseudo-inverse . I notice that , and yet . For other sizes it seems like the trace is equal to the smaller dimension of . I couldn't find this property explained. Could anyone help me understand these two facts?",A \in \Bbb C^{2 \times 1} A^{+} AA^{+} \neq I \mbox{Tr}(AA^{+}) = 1 A,"['linear-algebra', 'matrices', 'inverse', 'trace', 'pseudoinverse']"
15,Is the determinant a tensor?,Is the determinant a tensor?,,"I was reading Schutz's book on General Relativity. In it, he says that a(n) $M \choose N$ tensor is a linear function of $M$ one-forms and $N$ vectors into the real numbers. So does that mean the determinant of an $n \times n$ matrix is a $0 \choose n$ tensor because it is a function that maps the $n$ column vectors of the matrix to a real number (the value of the determinant)? But then, the determinant also maps the $n$ column vectors of the matrix to the same real number (the value of the determinant). So would the tensor representation of the determinant be different if you choose the map for the column vectors than the map for the row vectors?","I was reading Schutz's book on General Relativity. In it, he says that a(n) tensor is a linear function of one-forms and vectors into the real numbers. So does that mean the determinant of an matrix is a tensor because it is a function that maps the column vectors of the matrix to a real number (the value of the determinant)? But then, the determinant also maps the column vectors of the matrix to the same real number (the value of the determinant). So would the tensor representation of the determinant be different if you choose the map for the column vectors than the map for the row vectors?",M \choose N M N n \times n 0 \choose n n n,"['linear-algebra', 'determinant', 'tensors', 'tensor-rank']"
16,What is the image of $x^{\rm T}Qx\le 1$ under a linear map $x \mapsto Cx$?,What is the image of  under a linear map ?,x^{\rm T}Qx\le 1 x \mapsto Cx,"Let $Q$ be a real symmetric positive semidefinite $n \times n$ matrix. Consider a set $$ \Big\{ x \in \mathbb{R}^n \;\Big| \; x^{\rm T}Qx\le 1\Big\}, $$ which can be loosely described as an ""elliptic cylinder"". (It would be an ellipsoid if $Q$ was positive definite). Question. What is the image of this set under a linear map $y = Cx$ ? One can assume that $C$ has full row rank, but no more than that. I think that it will be $$ \Big\{ y \in \mathbb{R}^m \;\Big| \; y^{\rm T}Ry\le 1\Big\}, $$ where $R$ is some positive semidefinite matrix. But that is not enough: I actually want to find an explicit formula for $R$ (in terms of $Q$ and $C$ ) $-$ as elementary as it can be.","Let be a real symmetric positive semidefinite matrix. Consider a set which can be loosely described as an ""elliptic cylinder"". (It would be an ellipsoid if was positive definite). Question. What is the image of this set under a linear map ? One can assume that has full row rank, but no more than that. I think that it will be where is some positive semidefinite matrix. But that is not enough: I actually want to find an explicit formula for (in terms of and ) as elementary as it can be.","Q n \times n 
\Big\{ x \in \mathbb{R}^n \;\Big| \; x^{\rm T}Qx\le 1\Big\},
 Q y = Cx C 
\Big\{ y \in \mathbb{R}^m \;\Big| \; y^{\rm T}Ry\le 1\Big\},
 R R Q C -","['linear-algebra', 'linear-transformations', 'quadratic-forms', 'positive-definite', 'positive-semidefinite']"
17,Is every module automorphism diagonalized by irreps?,Is every module automorphism diagonalized by irreps?,,"Let $G$ denote a finite group, and let $V$ be a complex and finite dimensional $G$ module; that is, there is a group representation $G\to GL(V)$ . Finally, let $T\colon V\to V$ be an intertwining map, that is, a linear map such that $T(gv)=g T(v)$ for all $v\in V$ and $g\in G$ . By the theorem of Maschke , there are irreducible $G$ -modules $V_1, V_2, \ldots, V_n$ such that $$ V=V_1\oplus V_2 \oplus \ldots \oplus V_n.$$ Question . Is it true that $$T v_j=\lambda_j v_j, \qquad \forall v_j\in V_j,$$ for some $\lambda_1,\ldots,\lambda_n\in\mathbb C$ ? The lemma of Schur says that this is the case provided that $$\tag{*}T(V_j)\subset V_j.$$ This is because, in this case, $T$ restricts to an intertwining self-map of $V_j$ , which is irreducible. Therefore such restriction must be a scalar multiple of the identity. But is it true that (*) always holds with the given assumptions? EDIT . This question arises from the following observations. Suppose that $G$ is finite and abelian, and let $L^2(G)$ denote the space of all complex-valued functions on $G$ , which is a $G$ -module with the representation $gf(x):=f(x-g)$ . (This seemingly complicated notation hints at more general cases, with infinite groups). Let $\chi\in L^2(G)$ denote a character, that is, a homomorphism of $G$ into $\mathbb C^\times$ . Then an intertwining map $T\colon L^2(G)\to L^2(G)$ satisfies $$T\chi=\lambda_\chi \chi, $$ as it is easy to prove. And since the irreps are in this case the 1-dimensional subspaces $$ \operatorname*{span}(\chi), $$ it follows that intertwining maps of the $G$ -module $L^2(G)$ are diagonalized by irreps. (As the accepted answer clearly shows, intertwining maps of other $G$ -modules need not be even diagonalizable! This is the reason why I found that answer surprising and enlightening). Let us consider an infinite and non-abelian case. Suppose that $T\colon L^2(\mathbb S^{d-1})\to L^2(\mathbb S^{d-1})$ is rotation-invariant; $$ (Tf)(R^{-1}x)=T(f(R^{-1}\cdot))(x).$$ Then $T$ is diagonalized by spherical harmonics. Precisely, letting $$\{Y_{n, j}\ :\ j=1, \ldots, N(n)\}$$ denote a complete orthonormal system of spherical harmonics of degree $n$ , we have that $$ Tf=\sum_{n=0}^\infty \lambda_n \sum_{j=1}^{N(n)} \hat{f}(n, j) Y_{n, j}, $$ where we have let $\hat{f}(n, j)$ denote the coefficient $\langle f | Y_{n, j}\rangle.$ This latter example actually is a consequence of the lemma of Schur. Indeed, the decomposition of the $SO(d)$ -module $L^2(\mathbb S^{d-1})$ into irreps is precisely $$ \bigoplus_{n=0}^\infty \operatorname*{span}\{ Y_{n,j}\ :\ j=1, \ldots, N(n)\}, $$ and these irreps are pairwise non-isomorphic, because $N(n)$ is not a constant. (Actually, $N(n)$ has a well-known combinatorial expression, which there is no need to write down explicitly here).","Let denote a finite group, and let be a complex and finite dimensional module; that is, there is a group representation . Finally, let be an intertwining map, that is, a linear map such that for all and . By the theorem of Maschke , there are irreducible -modules such that Question . Is it true that for some ? The lemma of Schur says that this is the case provided that This is because, in this case, restricts to an intertwining self-map of , which is irreducible. Therefore such restriction must be a scalar multiple of the identity. But is it true that (*) always holds with the given assumptions? EDIT . This question arises from the following observations. Suppose that is finite and abelian, and let denote the space of all complex-valued functions on , which is a -module with the representation . (This seemingly complicated notation hints at more general cases, with infinite groups). Let denote a character, that is, a homomorphism of into . Then an intertwining map satisfies as it is easy to prove. And since the irreps are in this case the 1-dimensional subspaces it follows that intertwining maps of the -module are diagonalized by irreps. (As the accepted answer clearly shows, intertwining maps of other -modules need not be even diagonalizable! This is the reason why I found that answer surprising and enlightening). Let us consider an infinite and non-abelian case. Suppose that is rotation-invariant; Then is diagonalized by spherical harmonics. Precisely, letting denote a complete orthonormal system of spherical harmonics of degree , we have that where we have let denote the coefficient This latter example actually is a consequence of the lemma of Schur. Indeed, the decomposition of the -module into irreps is precisely and these irreps are pairwise non-isomorphic, because is not a constant. (Actually, has a well-known combinatorial expression, which there is no need to write down explicitly here).","G V G G\to GL(V) T\colon V\to V T(gv)=g T(v) v\in V g\in G G V_1, V_2, \ldots, V_n 
V=V_1\oplus V_2 \oplus \ldots \oplus V_n. T v_j=\lambda_j v_j, \qquad \forall v_j\in V_j, \lambda_1,\ldots,\lambda_n\in\mathbb C \tag{*}T(V_j)\subset V_j. T V_j G L^2(G) G G gf(x):=f(x-g) \chi\in L^2(G) G \mathbb C^\times T\colon L^2(G)\to L^2(G) T\chi=\lambda_\chi \chi,  
\operatorname*{span}(\chi),  G L^2(G) G T\colon L^2(\mathbb S^{d-1})\to L^2(\mathbb S^{d-1}) 
(Tf)(R^{-1}x)=T(f(R^{-1}\cdot))(x). T \{Y_{n, j}\ :\ j=1, \ldots, N(n)\} n 
Tf=\sum_{n=0}^\infty \lambda_n \sum_{j=1}^{N(n)} \hat{f}(n, j) Y_{n, j},  \hat{f}(n, j) \langle f | Y_{n, j}\rangle. SO(d) L^2(\mathbb S^{d-1}) 
\bigoplus_{n=0}^\infty \operatorname*{span}\{ Y_{n,j}\ :\ j=1, \ldots, N(n)\},  N(n) N(n)","['linear-algebra', 'representation-theory', 'harmonic-analysis']"
18,An invertible matrix minus the diagonal is nilpotent,An invertible matrix minus the diagonal is nilpotent,,"Let $A=(a_{ij})$ be an $n\times n$ invertible matrix over $\mathbb{C}$ and $D=diag(a_{11},a_{22},\dots,a_{nn})$ be the diagonal matrix whoes diagonal entries are same as $A$ . Suppose $A-D$ is nilpotent. Is it true that $D$ is invertible? For $n=2$ , since every nilpotent matrix with zero diagonal entries is either upper- or lower-triangular, I already know this is true for $n=2$ . Thank you! Thank you for user1551 for giving a counter example for $n\geq 3$ . I have modified the question a bit. I would like to assume the matrix $A$ having the property that $$a_{ij}\neq 0\Rightarrow a_{ji}=0.$$ Will it be true that $D$ is invertible under this assumption?","Let be an invertible matrix over and be the diagonal matrix whoes diagonal entries are same as . Suppose is nilpotent. Is it true that is invertible? For , since every nilpotent matrix with zero diagonal entries is either upper- or lower-triangular, I already know this is true for . Thank you! Thank you for user1551 for giving a counter example for . I have modified the question a bit. I would like to assume the matrix having the property that Will it be true that is invertible under this assumption?","A=(a_{ij}) n\times n \mathbb{C} D=diag(a_{11},a_{22},\dots,a_{nn}) A A-D D n=2 n=2 n\geq 3 A a_{ij}\neq 0\Rightarrow a_{ji}=0. D","['linear-algebra', 'matrices']"
19,Norms on polynomials,Norms on polynomials,,"Let $p_N (x) = d^N /dx^N ((x^2 -1)^N)$ for $N=0,1,2....$ Consider these polynomials as elements of the space $C[−1, 1]$ with the norm $||.||_2$ Show that the inner product of $p_N$ and $p_M = 0$ if $N$ $\ne$$ M$ Find the norm $||p_N||_2$ I have been to to use IBP for the first part, but I'm unsure where to begin. I know how to find the innerproduct normally, but not of something like this. I also am confused by the second part, despite knowing that to find that norm, you do Pythagoras on the polynomial.","Let for Consider these polynomials as elements of the space with the norm Show that the inner product of and if Find the norm I have been to to use IBP for the first part, but I'm unsure where to begin. I know how to find the innerproduct normally, but not of something like this. I also am confused by the second part, despite knowing that to find that norm, you do Pythagoras on the polynomial.","p_N (x) = d^N /dx^N ((x^2 -1)^N) N=0,1,2.... C[−1, 1] ||.||_2 p_N p_M = 0 N \ne M ||p_N||_2","['real-analysis', 'calculus', 'linear-algebra', 'functional-analysis', 'polynomials']"
20,The closest matrix to a given matrix in $\text{SO}(n)$ is unique iff the smallest singular value is a strict minimum,The closest matrix to a given matrix in  is unique iff the smallest singular value is a strict minimum,\text{SO}(n),"$\newcommand{\SOn}{\operatorname{SO}_n}$ $\newcommand{\On}{\operatorname{O}_n}$ $\newcommand{\Sym}{\operatorname{Sym}_n}$ $\newcommand{\Skew}{\operatorname{Skew}_n}$ $\newcommand{\dist}{\operatorname{dist}}$ $\newcommand{\Sig}{\Sigma}$ $\newcommand{\sig}{\sigma}$ $\newcommand{\al}{\alpha}$ $\newcommand{\id}{\operatorname{Id}}$ This question is a reference request. Claim: Let $A $ be a $n \times n$ real matrix with non-positive determinant. Then there is a unique closest matrix $Q \in \SOn$ to $A$ (w.r.t the Euclidean  Frobenius norm) if and only if the smallest singular value of $A$ is strictly smaller than the rest of the singular values. Question: Is this claim ""known""? Where can I find a reference for it? Note that I am looking for a reference , not a proof. (I have a proof...) Also, I am specifically asking for the distance minimizer in $\SOn$ . If we replace $\SOn$ with $\On$ , then the minimizer(s) is the orthogonal polar factor from polar decomposition. (and is unique whenever the matrix $A$ is invertible).","This question is a reference request. Claim: Let be a real matrix with non-positive determinant. Then there is a unique closest matrix to (w.r.t the Euclidean  Frobenius norm) if and only if the smallest singular value of is strictly smaller than the rest of the singular values. Question: Is this claim ""known""? Where can I find a reference for it? Note that I am looking for a reference , not a proof. (I have a proof...) Also, I am specifically asking for the distance minimizer in . If we replace with , then the minimizer(s) is the orthogonal polar factor from polar decomposition. (and is unique whenever the matrix is invertible).",\newcommand{\SOn}{\operatorname{SO}_n} \newcommand{\On}{\operatorname{O}_n} \newcommand{\Sym}{\operatorname{Sym}_n} \newcommand{\Skew}{\operatorname{Skew}_n} \newcommand{\dist}{\operatorname{dist}} \newcommand{\Sig}{\Sigma} \newcommand{\sig}{\sigma} \newcommand{\al}{\alpha} \newcommand{\id}{\operatorname{Id}} A  n \times n Q \in \SOn A A \SOn \SOn \On A,"['linear-algebra', 'reference-request', 'optimization', 'matrix-calculus', 'orthogonal-matrices']"
21,Can one make a diagonalizable matrix from a non-diagonalizable square matrix by adding its symmetric part?,Can one make a diagonalizable matrix from a non-diagonalizable square matrix by adding its symmetric part?,,"Let $M$ be a (real) non-diagonalizable square matrix, and $S=\frac{1}{2}(M+M^T)$ the symmetric part of $M$ . Is the matrix $M+S$ always diagonalizable? This is easily verified to be true if $M$ is a $2\times 2$ matrix. Can someone provide a proof or a counterexample for $n\times n$ matrices with $n>2$ ? Note: Let $M=S+A$ , with $A=\frac{1}{2}(M-M^T)$ the antisymmetric part of $M$ . Let $Q$ be an orthogonal matrix that diagonalizes $S$ , and put $Q^TSQ=\Lambda$ and $Q^TAQ=B$ . The question can then be restated as: given an antisymmetric square matrix $B$ and a diagonal matrix $\Lambda$ with $B+\Lambda$ non-diagonalizable, is $B+2\Lambda$ diagonalizable? Thanks! [This question arises naturally in research work I am doing; a positive answer to the question would lead to considerable progress.]","Let be a (real) non-diagonalizable square matrix, and the symmetric part of . Is the matrix always diagonalizable? This is easily verified to be true if is a matrix. Can someone provide a proof or a counterexample for matrices with ? Note: Let , with the antisymmetric part of . Let be an orthogonal matrix that diagonalizes , and put and . The question can then be restated as: given an antisymmetric square matrix and a diagonal matrix with non-diagonalizable, is diagonalizable? Thanks! [This question arises naturally in research work I am doing; a positive answer to the question would lead to considerable progress.]",M S=\frac{1}{2}(M+M^T) M M+S M 2\times 2 n\times n n>2 M=S+A A=\frac{1}{2}(M-M^T) M Q S Q^TSQ=\Lambda Q^TAQ=B B \Lambda B+\Lambda B+2\Lambda,"['linear-algebra', 'matrices']"
22,Informative proof that any real-valued symmetric matrix only has real eigenvalues,Informative proof that any real-valued symmetric matrix only has real eigenvalues,,"I am looking for an informative proof that any real-valued symmetric matrix only has real eigenvalues. By informative, I mean that there is an explanation accompanying the proof, rather than just a copy-and-paste job, which is not informative. I came across this question, but (1) the top-rated answer by Lepidopterist is, according to himself, not a proof of the result, but rather an explanation of why the displayed method is not a proof of the desired result, and (2) none of the proofs posted offer explanations and are just copy-and-paste jobs. Also, none of the answers in the question have been accepted by the author, so it seems that they might have also found the answers to be unsatisfactory. I'm seeking a proof and an accompanying explanation, so that I can properly learn the reasoning behind how any real-valued symmetric matrix only has real eigenvalues. I would greatly appreciate it if someone could please take the time to clarify this.","I am looking for an informative proof that any real-valued symmetric matrix only has real eigenvalues. By informative, I mean that there is an explanation accompanying the proof, rather than just a copy-and-paste job, which is not informative. I came across this question, but (1) the top-rated answer by Lepidopterist is, according to himself, not a proof of the result, but rather an explanation of why the displayed method is not a proof of the desired result, and (2) none of the proofs posted offer explanations and are just copy-and-paste jobs. Also, none of the answers in the question have been accepted by the author, so it seems that they might have also found the answers to be unsatisfactory. I'm seeking a proof and an accompanying explanation, so that I can properly learn the reasoning behind how any real-valued symmetric matrix only has real eigenvalues. I would greatly appreciate it if someone could please take the time to clarify this.",,"['linear-algebra', 'multivariable-calculus', 'eigenvalues-eigenvectors', 'symmetric-matrices', 'hessian-matrix']"
23,What is a Gradient?,What is a Gradient?,,"I am having trouble understanding visually what a gradient is. My understanding is it is a generalisation of tangential slopes to higher dimensions and gives the direction of steepest ascent. There are 4 different pictures I have: 1) From Khan Academy. How does it make sense to have a 2d gradient when your function is 3d? And shouldn't it be tangential to the function? The only way this makes sense to me is if you consider it as the projection of the tangential gradient vector onto the x-y plane. Is this right? 2) From a Medium article explaining Lagrange Multipliers. I understand the gradient for f but I'm not able to understand the gradient of g visually. It is a plane right? 3) I have plotted some samples from MacOS Grapher. I'm assuming Vector Field Cartesian form is the Gradient Vector. Why are the arrows going inward? Shouldn't they be tangential to the curve? 4) Finally, I have drawn some 3d curve. Could you tell me which gradient is correct? A, B or C? Any help appreciated. Sorry for the lengthy post but I have been breaking my head on this for a while. Thanks in advance. Edit: Changed Legrande to Lagrange.","I am having trouble understanding visually what a gradient is. My understanding is it is a generalisation of tangential slopes to higher dimensions and gives the direction of steepest ascent. There are 4 different pictures I have: 1) From Khan Academy. How does it make sense to have a 2d gradient when your function is 3d? And shouldn't it be tangential to the function? The only way this makes sense to me is if you consider it as the projection of the tangential gradient vector onto the x-y plane. Is this right? 2) From a Medium article explaining Lagrange Multipliers. I understand the gradient for f but I'm not able to understand the gradient of g visually. It is a plane right? 3) I have plotted some samples from MacOS Grapher. I'm assuming Vector Field Cartesian form is the Gradient Vector. Why are the arrows going inward? Shouldn't they be tangential to the curve? 4) Finally, I have drawn some 3d curve. Could you tell me which gradient is correct? A, B or C? Any help appreciated. Sorry for the lengthy post but I have been breaking my head on this for a while. Thanks in advance. Edit: Changed Legrande to Lagrange.",,"['calculus', 'linear-algebra']"
24,Is there a geometric interpretation about the euclidean distance between of 2 matrices?,Is there a geometric interpretation about the euclidean distance between of 2 matrices?,,"The Euclidean distance between points p and q is the length of the line segment connecting them ( $\overline{\mathbf{p}\mathbf{q}}$ ). $$\begin{aligned}d(\mathbf {p} ,\mathbf {q} )=d(\mathbf {q} ,\mathbf {p} )&={\sqrt {(q_{1}-p_{1})^{2}+(q_{2}-p_{2})^{2}+\cdots +(q_{n}-p_{n})^{2}}}\\[8pt]&={\sqrt {\sum _{i=1}^{n}(q_{i}-p_{i})^{2}}}.\end{aligned} $$ The Euclidean norm, or Euclidean length, or magnitude of a vector measures the length of the vector $$\left\|{\mathbf  {p}}\right\|={\sqrt  {p_{1}^{2}+p_{2}^{2}+\cdots +p_{n}^{2}}}={\sqrt  {{\mathbf  {p}}\cdot {\mathbf  {p}}}},$$ similar to above, is there a geometric interpretation about the euclidean distance between of 2 matrices? take this concrete example. X = [[0, 1],      [2, 3]]  Y = [[1, 2],      [3, 4]] what does this quantity represent? euclidean_distances(X , Y)  array([[1.41421356, 4.24264069],        [1.41421356, 1.41421356]])","The Euclidean distance between points p and q is the length of the line segment connecting them ( ). The Euclidean norm, or Euclidean length, or magnitude of a vector measures the length of the vector similar to above, is there a geometric interpretation about the euclidean distance between of 2 matrices? take this concrete example. X = [[0, 1],      [2, 3]]  Y = [[1, 2],      [3, 4]] what does this quantity represent? euclidean_distances(X , Y)  array([[1.41421356, 4.24264069],        [1.41421356, 1.41421356]])","\overline{\mathbf{p}\mathbf{q}} \begin{aligned}d(\mathbf {p} ,\mathbf {q} )=d(\mathbf {q} ,\mathbf {p} )&={\sqrt {(q_{1}-p_{1})^{2}+(q_{2}-p_{2})^{2}+\cdots +(q_{n}-p_{n})^{2}}}\\[8pt]&={\sqrt {\sum _{i=1}^{n}(q_{i}-p_{i})^{2}}}.\end{aligned}
 \left\|{\mathbf  {p}}\right\|={\sqrt  {p_{1}^{2}+p_{2}^{2}+\cdots +p_{n}^{2}}}={\sqrt  {{\mathbf  {p}}\cdot {\mathbf  {p}}}},","['linear-algebra', 'matrices', 'geometry']"
25,Computing $(A\otimes I + I \otimes A)^{-1} \text{vec}B$,Computing,(A\otimes I + I \otimes A)^{-1} \text{vec}B,"Suppose I have two positive semi-definite $n$ -by- $n$ matrices $A$ , $B$ and an $n$ -by- $n$ identity matrix $I$ , and I'm looking for a way to compute, approximate or bound the following quantity: $$(A\otimes I + I \otimes A)^{-1} \text{vec}B$$ Concretely I'm dealing with matrices with $n$ ranging from $100$ to $4000$ , so $A$ is easy to invert, while $A \otimes I$ is too large, so need a way to compute this using operations on $n$ -by- $n$ matrices Additionally, I found the following to give a decent approximation when $A$ , $B$ can be Kronecker-factored, wondering if there's a reason for that. $$0.5 A^{-0.5} B  A^{-0.5}$$ Any tips or literature pointers are appreciated!","Suppose I have two positive semi-definite -by- matrices , and an -by- identity matrix , and I'm looking for a way to compute, approximate or bound the following quantity: Concretely I'm dealing with matrices with ranging from to , so is easy to invert, while is too large, so need a way to compute this using operations on -by- matrices Additionally, I found the following to give a decent approximation when , can be Kronecker-factored, wondering if there's a reason for that. Any tips or literature pointers are appreciated!",n n A B n n I (A\otimes I + I \otimes A)^{-1} \text{vec}B n 100 4000 A A \otimes I n n A B 0.5 A^{-0.5} B  A^{-0.5},"['linear-algebra', 'inverse', 'matrix-equations', 'control-theory', 'kronecker-product']"
26,Is there a matrix that can be used to find the transpose of a matrix?,Is there a matrix that can be used to find the transpose of a matrix?,,"Let $A$ be a general $n\times n$ invertible matrix. Let $T^A$ be the ""transposer"" matrix i.e. $T^A A = A'$ . (Does that $T^A$ multiplied by $A$ equal the transpose of $A$ ?) Then does $T^A$ depend on the matrix $A$ : is $T^A = T^B$ for all invertible Matrices? Prove your claim. So can you take a matrix times the given matrix in order to find its transpose? If so, is that a general form that can be used for all invertible matrices?  I'm guessing it does not, but I am not totally sure or know how to go about proving that and I cannot find anything online about it.","Let be a general invertible matrix. Let be the ""transposer"" matrix i.e. . (Does that multiplied by equal the transpose of ?) Then does depend on the matrix : is for all invertible Matrices? Prove your claim. So can you take a matrix times the given matrix in order to find its transpose? If so, is that a general form that can be used for all invertible matrices?  I'm guessing it does not, but I am not totally sure or know how to go about proving that and I cannot find anything online about it.",A n\times n T^A T^A A = A' T^A A A T^A A T^A = T^B,"['linear-algebra', 'matrices']"
27,Proving all variables are equal,Proving all variables are equal,,"I came across a puzzle lately. Let $A$ be a matrix of size $(2n+1)$ by $(2n+1)$ . The diagonal of $A$ is all zeros. Every other entry of $A$ is either $+1$ or $-1$ . Each row of $A$ sum to zero. In other words, each row has $n$ of $+1$ and $n$ of $-1$ . Now we need to show that the system of equations $$ A (x_1, \dots, x_{2n+1})^T = (0, \dots, 0)^T $$ can always be reduced to $x_1=x_2=\dots=x_{2n+1}$ . This is pretty easy to see for $n=1$ . For $n=2$ , one can show this by case analysis. (Or simply write a piece of code to exhaustively check all possible $A$ ).  But I was not able to generalize this to arbitrary $n$ .","I came across a puzzle lately. Let be a matrix of size by . The diagonal of is all zeros. Every other entry of is either or . Each row of sum to zero. In other words, each row has of and of . Now we need to show that the system of equations can always be reduced to . This is pretty easy to see for . For , one can show this by case analysis. (Or simply write a piece of code to exhaustively check all possible ).  But I was not able to generalize this to arbitrary .","A (2n+1) (2n+1) A A +1 -1 A n +1 n -1 
A (x_1, \dots, x_{2n+1})^T = (0, \dots, 0)^T
 x_1=x_2=\dots=x_{2n+1} n=1 n=2 A n","['linear-algebra', 'puzzle']"
28,Converting Jordan Normal Form into Real Jordan Form,Converting Jordan Normal Form into Real Jordan Form,,"Given the matrix $$\begin{bmatrix}     0 & 0 & 0 & -8\\     1 & 0 & 0& 16 \\     0 & 1 & 0 & -14 \\     0 & 0 & 1 & 6 \\     \end{bmatrix}$$ find its real canonical form. Thanks to Wiki, I got the part where I finished Jordan normal form like below : \begin{bmatrix} 1-i & 0 & 0 & 0\\ 0 & 1+i & 0 & 0 \\ 0 & 0 & 2 & 1 \\ 0 & 0 & 0 & 2 \\ \end{bmatrix} Now, I am stuck and have no clue how to convert this into ""REAL"" jordan form.  Thank you.","Given the matrix find its real canonical form. Thanks to Wiki, I got the part where I finished Jordan normal form like below : Now, I am stuck and have no clue how to convert this into ""REAL"" jordan form.  Thank you.","\begin{bmatrix}
    0 & 0 & 0 & -8\\
    1 & 0 & 0& 16 \\
    0 & 1 & 0 & -14 \\
    0 & 0 & 1 & 6 \\
    \end{bmatrix} \begin{bmatrix}
1-i & 0 & 0 & 0\\
0 & 1+i & 0 & 0 \\
0 & 0 & 2 & 1 \\
0 & 0 & 0 & 2 \\
\end{bmatrix}","['linear-algebra', 'matrices', 'matrix-decomposition', 'jordan-normal-form']"
29,Rank/determinant of the $n\times n$ matrix $((a_{ij}))$ where $a_{ij}=(i+j-1)^2$,Rank/determinant of the  matrix  where,n\times n ((a_{ij})) a_{ij}=(i+j-1)^2,"I am trying to find the rank and determinant of the following $n\times n$ matrix : $$A=\begin{bmatrix}1^2&2^2&3^2&\cdots&n^2    \\ 2^2&3^2&4^2&\cdots&(n+1)^2    \\\vdots&\vdots&\vdots&\ddots&\vdots\\n^2&(n+1)^2&(n+2)^2&\cdots&(2n-1)^2    \end{bmatrix}$$ I verified that the determinant of $A$ vanishes for $n> 3$ , implying that $A$ has full rank for $n\le 3$ . And the rank seems to remain $3$ for $n>3$ . But I could not provide a rigorous proof. Is there a way to deduce the rank/determinant without reducing $A$ to echelon form or any other shortcut? Or how can I easily find the number of linearly independent rows/columns? Any hints would be great. Related question: Determinant of the matrix with $a_{i,j} = (i+j)^2$ .","I am trying to find the rank and determinant of the following matrix : I verified that the determinant of vanishes for , implying that has full rank for . And the rank seems to remain for . But I could not provide a rigorous proof. Is there a way to deduce the rank/determinant without reducing to echelon form or any other shortcut? Or how can I easily find the number of linearly independent rows/columns? Any hints would be great. Related question: Determinant of the matrix with $a_{i,j} = (i+j)^2$ .","n\times n A=\begin{bmatrix}1^2&2^2&3^2&\cdots&n^2
   \\ 2^2&3^2&4^2&\cdots&(n+1)^2
   \\\vdots&\vdots&\vdots&\ddots&\vdots\\n^2&(n+1)^2&(n+2)^2&\cdots&(2n-1)^2
   \end{bmatrix} A n> 3 A n\le 3 3 n>3 A","['linear-algebra', 'determinant', 'matrix-rank']"
30,What is the probability of the matrix being Singular,What is the probability of the matrix being Singular,,"Consider the set of all boolean square matrices of order $3 \times 3$ as shown below where a,b,c,d,e,f can be either 0 or 1. $\begin{bmatrix}     a&b&c\\     0&d&e\\     0&0&f   \end{bmatrix}$ Out of all possible boolean matrices of above type, a matrix is chosen at random.The probability that the matrix is singular is? My Work The above matrix is a triangular matrix and it's determinant can be 0, if either one or more from a,d and f are 0 in which case 0 will be an eigen value of the matrix and hence determinant 0. Number of ways in which I can set $a,d,f$ to zero are: $\binom{3}{1}+\binom{3}{2}+\binom{3}{3}=7$ ways. Now, total given boolean matrices possible are $2^6=64$ So, the required probability must be $\frac{7}{64}$ Is my answer correct?","Consider the set of all boolean square matrices of order as shown below where a,b,c,d,e,f can be either 0 or 1. Out of all possible boolean matrices of above type, a matrix is chosen at random.The probability that the matrix is singular is? My Work The above matrix is a triangular matrix and it's determinant can be 0, if either one or more from a,d and f are 0 in which case 0 will be an eigen value of the matrix and hence determinant 0. Number of ways in which I can set to zero are: ways. Now, total given boolean matrices possible are So, the required probability must be Is my answer correct?","3 \times 3 \begin{bmatrix}
    a&b&c\\
    0&d&e\\
    0&0&f
  \end{bmatrix} a,d,f \binom{3}{1}+\binom{3}{2}+\binom{3}{3}=7 2^6=64 \frac{7}{64}","['linear-algebra', 'probability', 'matrices']"
31,Prove that there exists number $k\in \mathbb{N}$ such that $ V = \operatorname{Ker}A^{k} \dot{+} \operatorname{Im}A^{k}$,Prove that there exists number  such that,k\in \mathbb{N}  V = \operatorname{Ker}A^{k} \dot{+} \operatorname{Im}A^{k},"Problem: Let A be linear operator A $\in L(V)$ . Prove that there exists number $k\in \mathbb{N}$ such that $ V = \operatorname{Ker}A^{k} \dot{+} \operatorname{Im}A^{k}$ . Then prove that operator $\left.A\right|_{\operatorname{Ker}A^{k}}$ is nilpotent and operator $\left.A\right|_{\operatorname{Im}A^{k}}$ is regular. My attempt: We know that $A$ can be written as sum of two operators: $A = N + S $ where $N$ is nilpotent and $S$ diagonalizable operator. Then I construct $A^k = \sum_{j=0}^{k} {k\choose j}S^{k-j}N^{j}$ where $k = \operatorname{ind}(N)$ but this does not give anything good. I know that I need to show somehow that for every $v\in V$ : $v=a+b, a\in \operatorname{Ker}A^k, b\in \operatorname{Im}A^k$ but I do not know how. I have no idea how to proceed. Thanks for any help.",Problem: Let A be linear operator A . Prove that there exists number such that . Then prove that operator is nilpotent and operator is regular. My attempt: We know that can be written as sum of two operators: where is nilpotent and diagonalizable operator. Then I construct where but this does not give anything good. I know that I need to show somehow that for every : but I do not know how. I have no idea how to proceed. Thanks for any help.,"\in L(V) k\in \mathbb{N}  V = \operatorname{Ker}A^{k} \dot{+} \operatorname{Im}A^{k} \left.A\right|_{\operatorname{Ker}A^{k}} \left.A\right|_{\operatorname{Im}A^{k}} A A = N + S  N S A^k = \sum_{j=0}^{k} {k\choose j}S^{k-j}N^{j} k = \operatorname{ind}(N) v\in V v=a+b, a\in \operatorname{Ker}A^k, b\in \operatorname{Im}A^k","['linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors', 'diagonalization', 'jordan-normal-form']"
32,The characteristic polynomial of $A$ is $x^n$ if and only if $\text{Tr}(A^i)=0$ for all $1\le i \le n$. [duplicate],The characteristic polynomial of  is  if and only if  for all . [duplicate],A x^n \text{Tr}(A^i)=0 1\le i \le n,This question already has answers here : Traces of all positive powers of a matrix are zero implies it is nilpotent (4 answers) If $A$ is nilpotent matrix then $tr(A)=0$ [closed] (4 answers) Closed 5 years ago . Let $k$ be a field of characteristic zero and $A$ be a matrix over $k$ . Then the characteristic polynomial of $A$ is $x^n$ if and only if $\text{Tr}(A^i)=0$ for all $1\le i \le n$ . The only proof I can think of is by applying the Jordan normal form to $A$ (considered as a matrix over $\overline{k}$ ). Is there any slick proof without invoking this theorem?,This question already has answers here : Traces of all positive powers of a matrix are zero implies it is nilpotent (4 answers) If $A$ is nilpotent matrix then $tr(A)=0$ [closed] (4 answers) Closed 5 years ago . Let be a field of characteristic zero and be a matrix over . Then the characteristic polynomial of is if and only if for all . The only proof I can think of is by applying the Jordan normal form to (considered as a matrix over ). Is there any slick proof without invoking this theorem?,k A k A x^n \text{Tr}(A^i)=0 1\le i \le n A \overline{k},"['linear-algebra', 'abstract-algebra']"
33,Show that the matrix $A = \begin{bmatrix} 2 & 0 \\ -1 & 2 \end{bmatrix}$ is not diagonalizable.,Show that the matrix  is not diagonalizable.,A = \begin{bmatrix} 2 & 0 \\ -1 & 2 \end{bmatrix},"I have this question: Show that the matrix $$A = \begin{bmatrix} 2 & 0 \\ -1 & 2 \end{bmatrix}$$ is not diagonalizable. So is the general strategy is To Find the eigenvectors and then Show that the matrix of eigenvectors is not invertible? If they are invertible, then it has a unique solution to ( $\lambda \bf {I - A)x = 0}$ which would imply that they are linearly independent. If it's linearly independent, then it would be diagonalizable? I'm following this theorem Condition for Diagonalization A $n \times n$ matrix is diagonalizable iff it has $n$ linearly independent eigenvectors. So I have to find the eigenvalue first, which is $2$ because the $2$ is on the diagonal of this matrix in a triangular matrix, using this theorem. Eigenvalues of Triangular Matrices If $A$ is a $n \times n$ triangular matrix, then its eigenvalues are the entries on its main diagonal. Solving for $\lambda {\bf I - A}$ : $$\begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix} - \begin{bmatrix} 2 & 0 \\ -1 & 2 \end{bmatrix} = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} $$ Since this matrix is not invertible, it is not diagonalizable. Is this right? This is the proof that I'm relying on:","I have this question: Show that the matrix is not diagonalizable. So is the general strategy is To Find the eigenvectors and then Show that the matrix of eigenvectors is not invertible? If they are invertible, then it has a unique solution to ( which would imply that they are linearly independent. If it's linearly independent, then it would be diagonalizable? I'm following this theorem Condition for Diagonalization A matrix is diagonalizable iff it has linearly independent eigenvectors. So I have to find the eigenvalue first, which is because the is on the diagonal of this matrix in a triangular matrix, using this theorem. Eigenvalues of Triangular Matrices If is a triangular matrix, then its eigenvalues are the entries on its main diagonal. Solving for : Since this matrix is not invertible, it is not diagonalizable. Is this right? This is the proof that I'm relying on:",A = \begin{bmatrix} 2 & 0 \\ -1 & 2 \end{bmatrix} \lambda \bf {I - A)x = 0} n \times n n 2 2 A n \times n \lambda {\bf I - A} \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix} - \begin{bmatrix} 2 & 0 \\ -1 & 2 \end{bmatrix} = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix} ,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization', 'triangularization']"
34,"If $D$ is a derivation of a Lie Algebra, and $D X = \lambda X$ $\Rightarrow$ $\text{ad}(X)$ is nilpotent","If  is a derivation of a Lie Algebra, and    is nilpotent",D D X = \lambda X \Rightarrow \text{ad}(X),"I'm very stuck in this exercise, can anyone help me or give me some hints? Exercise: Let $\mathfrak{g}$ be a finite dimensional Lie Algebra (over a zero characteristic field), $D:  \mathfrak{g}\to \mathfrak{g}$ a derivation and $X \in \mathfrak{g}$,   such that $D(X) = \lambda X$ (with $\lambda \neq 0$). Then $\text{ad}(X)$ is nilpotent. I could only conclude that $\text{tr}(\text{ad}(X)^2) =0$. In fact, if $\langle \cdot,\cdot \rangle$ is the Killing form , then $$ \langle X,X \rangle= \frac{1}{\lambda} \langle D X, X \rangle=\frac{-1}{\lambda} \langle  X, DX \rangle = - \langle X, X \rangle,$$ follows that $ \langle X,X \rangle = 0$, and therefore $\text{tr}(\text{ad}(X)^2) = 0.$ However, this result does not give me a useful conclusion.","I'm very stuck in this exercise, can anyone help me or give me some hints? Exercise: Let $\mathfrak{g}$ be a finite dimensional Lie Algebra (over a zero characteristic field), $D:  \mathfrak{g}\to \mathfrak{g}$ a derivation and $X \in \mathfrak{g}$,   such that $D(X) = \lambda X$ (with $\lambda \neq 0$). Then $\text{ad}(X)$ is nilpotent. I could only conclude that $\text{tr}(\text{ad}(X)^2) =0$. In fact, if $\langle \cdot,\cdot \rangle$ is the Killing form , then $$ \langle X,X \rangle= \frac{1}{\lambda} \langle D X, X \rangle=\frac{-1}{\lambda} \langle  X, DX \rangle = - \langle X, X \rangle,$$ follows that $ \langle X,X \rangle = 0$, and therefore $\text{tr}(\text{ad}(X)^2) = 0.$ However, this result does not give me a useful conclusion.",,"['linear-algebra', 'lie-algebras']"
35,In Search of a map $\phi:\mathbf{C}\to\mathbf{C}$ satisfying additivity.,In Search of a map  satisfying additivity.,\phi:\mathbf{C}\to\mathbf{C},"I am trying without much luck to come up with an example of a function $\phi:\mathbf{C}\to\mathbf{C}$ such that $\phi(x+y) = \phi(x)+\phi(y),\forall x,y\in\mathbf{C}$ and yet for some $a,\alpha\in\mathbf{C}$ we have $\phi(a\alpha)\neq a\phi(\alpha)$. Any hints on how to get me started?","I am trying without much luck to come up with an example of a function $\phi:\mathbf{C}\to\mathbf{C}$ such that $\phi(x+y) = \phi(x)+\phi(y),\forall x,y\in\mathbf{C}$ and yet for some $a,\alpha\in\mathbf{C}$ we have $\phi(a\alpha)\neq a\phi(\alpha)$. Any hints on how to get me started?",,['linear-algebra']
36,How to expand in a non-orthogonal basis in an intuitive way?,How to expand in a non-orthogonal basis in an intuitive way?,,"I have a basis that consists of four non-orthogonal vectors $\{|u_i\rangle\}, 1 \le i \le 4$. Can the formula for an orthonormal expansion be modified so that it holds true for any given basis? $$|v>=\sum_i |u_i\rangle\langle u_i|v\rangle$$ I could always write $|v\rangle$ as a linear conbination of $\{u_i\}$ and solve the equation system, but I would like to approach the problem from a different perspective. I am not interested in finding an orthonormal basis with the Gram–Schmidt process.","I have a basis that consists of four non-orthogonal vectors $\{|u_i\rangle\}, 1 \le i \le 4$. Can the formula for an orthonormal expansion be modified so that it holds true for any given basis? $$|v>=\sum_i |u_i\rangle\langle u_i|v\rangle$$ I could always write $|v\rangle$ as a linear conbination of $\{u_i\}$ and solve the equation system, but I would like to approach the problem from a different perspective. I am not interested in finding an orthonormal basis with the Gram–Schmidt process.",,['linear-algebra']
37,"If $\sigma:M_3\rightarrow S_3$ is a linear map such that $\sigma(PMP^{-1})=P\sigma(M)P^{-1}$, then $\sigma(M)=\ldots$","If  is a linear map such that , then",\sigma:M_3\rightarrow S_3 \sigma(PMP^{-1})=P\sigma(M)P^{-1} \sigma(M)=\ldots,"In page 32 of the book A Mathematical Introduction to Fluid Mechanics , by Alexandre Chorin and Jerrold E. Marsden, it is used and commented the following property: let $M_3$ be the space of $3\times 3$ matrices and $S_3$ be the subspace of symmetric matrices. Let $\sigma:M_3\rightarrow S_3$ be a linear map, with the property that $\sigma(PMP^{-1})=P\sigma(M)P^{-1}$ for each orthogonal matrix $P$ and each $M\in M_3$. Then there exist constants $\lambda$ and $\mu$ such that $\sigma(M)=\lambda\text{Trace}(M)I_3+\mu(M+M^T)$. I do not understand the brief proof from the book. Could you provide a detailed proof?","In page 32 of the book A Mathematical Introduction to Fluid Mechanics , by Alexandre Chorin and Jerrold E. Marsden, it is used and commented the following property: let $M_3$ be the space of $3\times 3$ matrices and $S_3$ be the subspace of symmetric matrices. Let $\sigma:M_3\rightarrow S_3$ be a linear map, with the property that $\sigma(PMP^{-1})=P\sigma(M)P^{-1}$ for each orthogonal matrix $P$ and each $M\in M_3$. Then there exist constants $\lambda$ and $\mu$ such that $\sigma(M)=\lambda\text{Trace}(M)I_3+\mu(M+M^T)$. I do not understand the brief proof from the book. Could you provide a detailed proof?",,"['linear-algebra', 'matrices', 'symmetric-matrices', 'orthogonal-matrices']"
38,Does the typical vector space of polynomials up to a certain degree need to have the constant vectors,Does the typical vector space of polynomials up to a certain degree need to have the constant vectors,,"I helped a classmate with an exercise about sets of vectors spanning (or not) certain vector spaces. The question was as follows: Does the set $$\{ t, t+t^2, t^3-3t^4, 2t-t^2, -4t^4-t^3 \}$$ span the space $$P_4$$ where this vector space was the polynomials with degree $\leq 4$. (where $t \in \mathbb{R}$) (this vector space is defined the way it usually is, with coordinates of a vector corresponding with the coefficients of a polynomial) I said yes because when you write the vectors as a $5$ by $4$ matrix you can use gaussian elimination to end up with 4 monomial vectors (by that I mean the vectors $(1,0,0,0)$ and so on which you would then use as the coefficients of a polynomial). This answer was wrong because the set does not include a constant vector. Which brings to me to my actual question as to why specifically this is. As far as I have tried you could talk about a vector space of polynomials with degree $\leq k$ and no constant part just fine. I have tried finding where the vector space axioms don't work but I can't seem to find an error. So is there something I am overlooking where a vector space of polynomials without a constant part would not work? I understand it would be pretty strange since if you're talking about polynomials there is no reason to disregard constant polynomials, especially since they don't cause any problems here, but let's disregard that for a moment. P.S. yes I corrected my mistake to the classmate","I helped a classmate with an exercise about sets of vectors spanning (or not) certain vector spaces. The question was as follows: Does the set $$\{ t, t+t^2, t^3-3t^4, 2t-t^2, -4t^4-t^3 \}$$ span the space $$P_4$$ where this vector space was the polynomials with degree $\leq 4$. (where $t \in \mathbb{R}$) (this vector space is defined the way it usually is, with coordinates of a vector corresponding with the coefficients of a polynomial) I said yes because when you write the vectors as a $5$ by $4$ matrix you can use gaussian elimination to end up with 4 monomial vectors (by that I mean the vectors $(1,0,0,0)$ and so on which you would then use as the coefficients of a polynomial). This answer was wrong because the set does not include a constant vector. Which brings to me to my actual question as to why specifically this is. As far as I have tried you could talk about a vector space of polynomials with degree $\leq k$ and no constant part just fine. I have tried finding where the vector space axioms don't work but I can't seem to find an error. So is there something I am overlooking where a vector space of polynomials without a constant part would not work? I understand it would be pretty strange since if you're talking about polynomials there is no reason to disregard constant polynomials, especially since they don't cause any problems here, but let's disregard that for a moment. P.S. yes I corrected my mistake to the classmate",,"['linear-algebra', 'polynomials', 'vector-spaces', 'vectors']"
39,How can I get the gradient of the normal equation for weighted linear regression?,How can I get the gradient of the normal equation for weighted linear regression?,,"The normal equation for weighted linear regression looks like this: $$J(\theta) = (X\theta - y)^TW(X\theta - y),$$ where $X\in\Re^{m\times n}$, $\theta\in\Re^{n\times n}$, $y\in\Re^{m\times 1}$, $W\in\Re^{m\times m}$, and $W$ is a diagonal matrix, with the diagonals being the $m$ different weights. As a point of reference, this can also be written as $$J(\theta) = \frac{1}{2}\sum_{i=1}^mw^{(i)}(\theta^Tx^{(i)} - y^{(i)}),$$ where the superscripts indicate the $i$-th element of that vector or diagonal. I am trying to use the normal equation to show solve for $\theta$, specifically by taking the gradient of the normal equation and setting it equal to zero. My attempt I begin by expanding the equation: \begin{equation} \begin{split} J(\theta) & = (\theta^TX^T - y^T)(WX\theta - Wy) \\ & = \theta^TX^TWX\theta - \theta^TX^TWy - y^TWX\theta + y^TWy \end{split} \end{equation} Now I know that the answer to my problem is $$\nabla_{\theta}J(\theta) = 2(X^TWX\theta - X^TWy),$$ and the only way I can see to achieve this from what I have is if $$J(\theta) = \theta^TX^TWX\theta -2y^TWX\theta + y^TWy.$$ From my initial expansion of $J(\theta)$, this would imply that $$\theta^TX^TWy = y^TWX\theta.$$ I can't see how this last equality could hold. I'm quite new to linear algebra, so there is the possibility that I'm making some cardinal error. If so, could someone please outline where my line of reasoning here breaks down? If not, could someone please outline how the last equality holds?","The normal equation for weighted linear regression looks like this: $$J(\theta) = (X\theta - y)^TW(X\theta - y),$$ where $X\in\Re^{m\times n}$, $\theta\in\Re^{n\times n}$, $y\in\Re^{m\times 1}$, $W\in\Re^{m\times m}$, and $W$ is a diagonal matrix, with the diagonals being the $m$ different weights. As a point of reference, this can also be written as $$J(\theta) = \frac{1}{2}\sum_{i=1}^mw^{(i)}(\theta^Tx^{(i)} - y^{(i)}),$$ where the superscripts indicate the $i$-th element of that vector or diagonal. I am trying to use the normal equation to show solve for $\theta$, specifically by taking the gradient of the normal equation and setting it equal to zero. My attempt I begin by expanding the equation: \begin{equation} \begin{split} J(\theta) & = (\theta^TX^T - y^T)(WX\theta - Wy) \\ & = \theta^TX^TWX\theta - \theta^TX^TWy - y^TWX\theta + y^TWy \end{split} \end{equation} Now I know that the answer to my problem is $$\nabla_{\theta}J(\theta) = 2(X^TWX\theta - X^TWy),$$ and the only way I can see to achieve this from what I have is if $$J(\theta) = \theta^TX^TWX\theta -2y^TWX\theta + y^TWy.$$ From my initial expansion of $J(\theta)$, this would imply that $$\theta^TX^TWy = y^TWX\theta.$$ I can't see how this last equality could hold. I'm quite new to linear algebra, so there is the possibility that I'm making some cardinal error. If so, could someone please outline where my line of reasoning here breaks down? If not, could someone please outline how the last equality holds?",,"['linear-algebra', 'regression', 'least-squares', 'linear-regression', 'weighted-least-squares']"
40,Commutability transformation [closed],Commutability transformation [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Suppose that V is a finite dimensional vector space and $f\in {\rm End}\ (V)$ is diagonalizable with ${\rm dim}\ V$ distint eigenvalues. Show for $Z(f):=\lbrace g\in {\rm End}\ (V)| fg=gf\rbrace$ we have ${\rm dim}\ Z(f)={\rm dim }\ V$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Suppose that V is a finite dimensional vector space and $f\in {\rm End}\ (V)$ is diagonalizable with ${\rm dim}\ V$ distint eigenvalues. Show for $Z(f):=\lbrace g\in {\rm End}\ (V)| fg=gf\rbrace$ we have ${\rm dim}\ Z(f)={\rm dim }\ V$",,"['linear-algebra', 'linear-transformations']"
41,Is this group the 4-permutation group?,Is this group the 4-permutation group?,,"Recently I came along the set of $3\times 3$ matrices defined by taking any three vectors of (-1,-1,-1) or the three standard unit basis vectors as your columns. For example, one might take $$\left(\begin{array}{ccc} -1 &-1& -1\\ 1 &0 &0 \\ 0& 1 &0 \end{array}\right),$$ or the identity. Both are of the sort I am interested in. More specifically, I suspect that there is an isomorphism between this set of matrices and the 4-permutation group, $S_4$. However, I am not sure how to prove this. Help, of course, would be appreciated.","Recently I came along the set of $3\times 3$ matrices defined by taking any three vectors of (-1,-1,-1) or the three standard unit basis vectors as your columns. For example, one might take $$\left(\begin{array}{ccc} -1 &-1& -1\\ 1 &0 &0 \\ 0& 1 &0 \end{array}\right),$$ or the identity. Both are of the sort I am interested in. More specifically, I suspect that there is an isomorphism between this set of matrices and the 4-permutation group, $S_4$. However, I am not sure how to prove this. Help, of course, would be appreciated.",,"['linear-algebra', 'group-theory', 'permutations', 'group-isomorphism']"
42,An $n×n$ matrix $A$ is formed with each element $(a_{ij})$ randomly selected,An  matrix  is formed with each element  randomly selected,n×n A (a_{ij}),"Let $p$ be a prime number.Given a positive integer $n$, an $n×n$ matrix $A$ is formed with each element $(a_{ij})$ randomly selected, with equal probability, from $[0, 1, ..., p − 1]$. Let $q_n$ be probability that $\text{det}A \equiv 1 (\ mod \ p)$.Find $\lim_{n \rightarrow \infty} q_n$ I have the idea of looking each entry of the matrix in terms of modulo $p$. I was working with small cases for $p$ like $5$. But it left me clueless.","Let $p$ be a prime number.Given a positive integer $n$, an $n×n$ matrix $A$ is formed with each element $(a_{ij})$ randomly selected, with equal probability, from $[0, 1, ..., p − 1]$. Let $q_n$ be probability that $\text{det}A \equiv 1 (\ mod \ p)$.Find $\lim_{n \rightarrow \infty} q_n$ I have the idea of looking each entry of the matrix in terms of modulo $p$. I was working with small cases for $p$ like $5$. But it left me clueless.",,['linear-algebra']
43,Linear algebra objective type question csir 2017.,Linear algebra objective type question csir 2017.,,For every $4\times 4$ real non singular symmetric matrix $A$ there exist a positive integer $p$ such that $pI+A$ is positive definite. $A^p$ is positive definite. $A^{-p}$ is positive definite. $ e^{pA}-I$ is positive definite. $ 1$st option is correct as we can choose $p$ which is greater than each eigen value of matrix $A$. Similarly option second and third is also correct as in these case choose  $p$ an even integer. But I don’t know how to deal with third option . Please help me in this case . Thanks .,For every $4\times 4$ real non singular symmetric matrix $A$ there exist a positive integer $p$ such that $pI+A$ is positive definite. $A^p$ is positive definite. $A^{-p}$ is positive definite. $ e^{pA}-I$ is positive definite. $ 1$st option is correct as we can choose $p$ which is greater than each eigen value of matrix $A$. Similarly option second and third is also correct as in these case choose  $p$ an even integer. But I don’t know how to deal with third option . Please help me in this case . Thanks .,,['linear-algebra']
44,Special formula for the permanent of the sum of two matrices,Special formula for the permanent of the sum of two matrices,,"Dear math stack exchange community, I was told that in the paper http://www.tandfonline.com/doi/abs/10.1080/03081088708817770 there was a formula for the permanent of the sum of two matrices $X$ and $Y$ via permanents of certain submatrices of $X$ and $Y$. Unfortunately, I don't have access to this paper (and thus, to this formula) at the moment. I would be very grateful. if somebody could give the formula here.","Dear math stack exchange community, I was told that in the paper http://www.tandfonline.com/doi/abs/10.1080/03081088708817770 there was a formula for the permanent of the sum of two matrices $X$ and $Y$ via permanents of certain submatrices of $X$ and $Y$. Unfortunately, I don't have access to this paper (and thus, to this formula) at the moment. I would be very grateful. if somebody could give the formula here.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'algebra-precalculus', 'permanent']"
45,Quotient Group of $\mathbb{Z}^4$ and a Lattice,Quotient Group of  and a Lattice,\mathbb{Z}^4,"I am working on problem 2 of the Rutgers 2017 Fall Algebra Qualifier where we are tasked with determining the structure of $\mathbb{Z}^4 /S$ where $S$ is the group generated by the vectors $(5,-2,-4,1)$, $(-5,4,4,1)$, $(0,6,0,6)$. So the first thing I noted was that $$(5,-2,-4,1) + (-5,4,4,1) = (0,2,0,2).$$ So it follows the third vector given $(0,6,0,6)$ is in the span of first two, and so the question remains to show: Find $\mathbb{Z}^4 / \lbrace a (5,-2,-4,1)  + b (-5,4,4,1), a, b \in \mathbb{Z} \rbrace$ Now I tried to look for similar problems to this to make sense of it and came across the following: Is $\mathbb{Z}\times\mathbb{Z}/((6,5),(3,4))$ is finitely generated? But I'm not sure how to use the matrix techniques there correctly and rigorously. So I now I'm working with equivalence classes but the ease with which one can declare $$ \mathbb{Z} / k\mathbb{Z} = \mathbb{Z}_k$$ seems to be lost when I move into the 2 basis vector situation.","I am working on problem 2 of the Rutgers 2017 Fall Algebra Qualifier where we are tasked with determining the structure of $\mathbb{Z}^4 /S$ where $S$ is the group generated by the vectors $(5,-2,-4,1)$, $(-5,4,4,1)$, $(0,6,0,6)$. So the first thing I noted was that $$(5,-2,-4,1) + (-5,4,4,1) = (0,2,0,2).$$ So it follows the third vector given $(0,6,0,6)$ is in the span of first two, and so the question remains to show: Find $\mathbb{Z}^4 / \lbrace a (5,-2,-4,1)  + b (-5,4,4,1), a, b \in \mathbb{Z} \rbrace$ Now I tried to look for similar problems to this to make sense of it and came across the following: Is $\mathbb{Z}\times\mathbb{Z}/((6,5),(3,4))$ is finitely generated? But I'm not sure how to use the matrix techniques there correctly and rigorously. So I now I'm working with equivalence classes but the ease with which one can declare $$ \mathbb{Z} / k\mathbb{Z} = \mathbb{Z}_k$$ seems to be lost when I move into the 2 basis vector situation.",,"['linear-algebra', 'abstract-algebra']"
46,"Are there any applications of matrices, or linear algebra to chess? If so, are there good books on it?","Are there any applications of matrices, or linear algebra to chess? If so, are there good books on it?",,"Chess has never had any appeal to me, but recently my brother bought a chess set, and I realized that the board can be represented as an 8x8 matrix, and each type of of piece as a number from 0 to 6, the 6 pieces, and the empty square. So I've been looking for some info on the internet, but I haven't found too much. Do you know if anybody has made some research on this, or published books, or articles. Matrices have applications on stochastic processes, optimization, best-decision taking, so wouldn't it be possible to create models for best moves according to a situation, and such? What do you think?","Chess has never had any appeal to me, but recently my brother bought a chess set, and I realized that the board can be represented as an 8x8 matrix, and each type of of piece as a number from 0 to 6, the 6 pieces, and the empty square. So I've been looking for some info on the internet, but I haven't found too much. Do you know if anybody has made some research on this, or published books, or articles. Matrices have applications on stochastic processes, optimization, best-decision taking, so wouldn't it be possible to create models for best moves according to a situation, and such? What do you think?",,"['linear-algebra', 'matrices', 'soft-question', 'applications', 'chessboard']"
47,How to find the left null space from rref(A),How to find the left null space from rref(A),,"I was working through a problem and was wondering if there was an easier way of finding the basis of the left null space of a given matrix. (For a simple example) Suppose we have a matrix $A = \begin{bmatrix} 1 & 2 & 4 \\ 2 & 4 & 8  \end{bmatrix}$ when reduced we can write it as $\text{rref}(A) = \begin{bmatrix} 1 & 2 & 4 \\ 0 & 0 & 0 \end{bmatrix} $ from rref(A) it is clear that: Basis for $C(A) = \left\{ \begin{pmatrix} 1 \\ 2\end{pmatrix} \right\}$ Basis for $C(A^T) = \left\{ \begin{pmatrix} 1,&2, & 4 \end{pmatrix} \right\}$ Basis for $N(A) = \left\{ \begin{pmatrix} -2 \\ 1 \\ 0\end{pmatrix} , \begin{pmatrix} -4 \\ 0 \\ 1 \end{pmatrix}\right\}$ Now my question is am I able to deduce the left null space just from rref(A)? Else, I would take the transpose of A, row reduce it and then find the left null space that way but I was wondering if there is an easier way?","I was working through a problem and was wondering if there was an easier way of finding the basis of the left null space of a given matrix. (For a simple example) Suppose we have a matrix $A = \begin{bmatrix} 1 & 2 & 4 \\ 2 & 4 & 8  \end{bmatrix}$ when reduced we can write it as $\text{rref}(A) = \begin{bmatrix} 1 & 2 & 4 \\ 0 & 0 & 0 \end{bmatrix} $ from rref(A) it is clear that: Basis for $C(A) = \left\{ \begin{pmatrix} 1 \\ 2\end{pmatrix} \right\}$ Basis for $C(A^T) = \left\{ \begin{pmatrix} 1,&2, & 4 \end{pmatrix} \right\}$ Basis for $N(A) = \left\{ \begin{pmatrix} -2 \\ 1 \\ 0\end{pmatrix} , \begin{pmatrix} -4 \\ 0 \\ 1 \end{pmatrix}\right\}$ Now my question is am I able to deduce the left null space just from rref(A)? Else, I would take the transpose of A, row reduce it and then find the left null space that way but I was wondering if there is an easier way?",,['linear-algebra']
48,Prove $\mathrm{exp}(\mathrm{Tr}(A)) = \det(\mathrm{exp}(A))$ via maximal tori,Prove  via maximal tori,\mathrm{exp}(\mathrm{Tr}(A)) = \det(\mathrm{exp}(A)),"This is an exercise in Kris Tapp's little AMS booklet Matrix Groups for Undergraduates . I have reviewed the proof given earlier in the text but that proof relies on properties of exp and other aspects of commuting matrices, the definition of the derivative and other simpler observations.  Ah! Commuting matrices must be one  key here but I am afraid I haven't made enough of  the connections. My feeling is that conjugation will be helpful as well, since all maximal tori are conjugates of each other. I believe I have all the parts of the jigsaw puzzle but have yet to fit the appropriate pieces. Maximal Tori is the name of the Chapter (#9) and they are, hence, a NEW topic, which again points toward their properties to be the glue to  hold the pieces together. Any suggestions?","This is an exercise in Kris Tapp's little AMS booklet Matrix Groups for Undergraduates . I have reviewed the proof given earlier in the text but that proof relies on properties of exp and other aspects of commuting matrices, the definition of the derivative and other simpler observations.  Ah! Commuting matrices must be one  key here but I am afraid I haven't made enough of  the connections. My feeling is that conjugation will be helpful as well, since all maximal tori are conjugates of each other. I believe I have all the parts of the jigsaw puzzle but have yet to fit the appropriate pieces. Maximal Tori is the name of the Chapter (#9) and they are, hence, a NEW topic, which again points toward their properties to be the glue to  hold the pieces together. Any suggestions?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant', 'trace']"
49,Rational canonical form of diagonal matrix,Rational canonical form of diagonal matrix,,"I'm trying to determine the rational canonical form of a diagonal matrix  $$ A=\begin{pmatrix} a_1 & 0 & \cdots & 0\\ 0 & a_2 & \cdots & 0\\ \vdots & \vdots & & \vdots\\ 0 &0 & \cdots & a_n \end{pmatrix} $$ where the $a_i$'s are all different. If my intuition is correct, since the characteristic polynomial (in this case also the minimal polynomial) of $A$ is just the product $(x-a_1)\cdots(x-a_n)$ and all the factors are different we have that the $(x-a_i)$'s are the invariant factors of $A$. Then the rational canonical form of $A$ is again $A$. Is this correct? Is there a more formal way to work this problem? I'd appreciate any suggestions. Thanks in advance.","I'm trying to determine the rational canonical form of a diagonal matrix  $$ A=\begin{pmatrix} a_1 & 0 & \cdots & 0\\ 0 & a_2 & \cdots & 0\\ \vdots & \vdots & & \vdots\\ 0 &0 & \cdots & a_n \end{pmatrix} $$ where the $a_i$'s are all different. If my intuition is correct, since the characteristic polynomial (in this case also the minimal polynomial) of $A$ is just the product $(x-a_1)\cdots(x-a_n)$ and all the factors are different we have that the $(x-a_i)$'s are the invariant factors of $A$. Then the rational canonical form of $A$ is again $A$. Is this correct? Is there a more formal way to work this problem? I'd appreciate any suggestions. Thanks in advance.",,"['linear-algebra', 'abstract-algebra', 'free-modules']"
50,proof of roots of characteristic polynomial are eigenvalues,proof of roots of characteristic polynomial are eigenvalues,,How do I prove the 2 directions of this statement?,How do I prove the 2 directions of this statement?,,"['linear-algebra', 'eigenvalues-eigenvectors', 'linear-transformations']"
51,If $X$ is diagonal with distinct diagonal entries and $XY = YX$ then $Y$ is also diagonal matrix.,If  is diagonal with distinct diagonal entries and  then  is also diagonal matrix.,X XY = YX Y,"If $X$ is diagonal with distinct diagonal entries and $XY = YX$ then $Y$ is also diagonal. I was trying to prove this. I tried as follows - Since we know that $X$ is of distinct diagonal entries which means that distinct eigenvalues implying that $X$ is diagonalizable. that is $X = P^{-1}XP$ where $P$ is the matrix whose columns are the eigenvectors of $X$. Now we are given that $XY = YX$ or $X = Y^{-1}XY$ but this is for all $Y$ I guess. Now for $Y = P$ it is clear that $Y$ is diagonalizable, but for other $Y$ how we can prove that they are diagonalizable?","If $X$ is diagonal with distinct diagonal entries and $XY = YX$ then $Y$ is also diagonal. I was trying to prove this. I tried as follows - Since we know that $X$ is of distinct diagonal entries which means that distinct eigenvalues implying that $X$ is diagonalizable. that is $X = P^{-1}XP$ where $P$ is the matrix whose columns are the eigenvectors of $X$. Now we are given that $XY = YX$ or $X = Y^{-1}XY$ but this is for all $Y$ I guess. Now for $Y = P$ it is clear that $Y$ is diagonalizable, but for other $Y$ how we can prove that they are diagonalizable?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
52,"If $T$ is an upper triangular matrix, and $TT^{H}=T^{H}T$ , show that $T$ has to be a diagonal matrix","If  is an upper triangular matrix, and  , show that  has to be a diagonal matrix",T TT^{H}=T^{H}T T,"Problem If $T \in \mathbb{C}^{n \times n}$ is an upper triangular matrix, and $TT^{H}=T^{H}T$, where $T^H$ means the Hermitian transpose of $T$, show that $T$ has to be a diagonal matrix. What I Have Done This seems to be obvious, but writing $T = [t_1, t_2, \cdots, t_n]$ does not help. I also tried to write $TT^{H}$ in an element-wise form, but I got stuck halfway. Could anyone help me, thank you in advance.","Problem If $T \in \mathbb{C}^{n \times n}$ is an upper triangular matrix, and $TT^{H}=T^{H}T$, where $T^H$ means the Hermitian transpose of $T$, show that $T$ has to be a diagonal matrix. What I Have Done This seems to be obvious, but writing $T = [t_1, t_2, \cdots, t_n]$ does not help. I also tried to write $TT^{H}$ in an element-wise form, but I got stuck halfway. Could anyone help me, thank you in advance.",,['linear-algebra']
53,Similar nilpotent matrices with same minimal polynomial and rank,Similar nilpotent matrices with same minimal polynomial and rank,,"Given two 6x6 nilpotent matrices with the same minimal polynomial and same rank, prove they're similar. Also prove that this is not necessarily the case if the two matrices are 7x7. If two matrix have the minimal polynomial and same rank, then the following can be generalized: 1) they have the same eigenvalue, 0 2) then have the same nilpotent index 3) they have the same geometric multiplicity But I'm not seeing how this can explicitly imply similarity and how the 7x7 case is any different.","Given two 6x6 nilpotent matrices with the same minimal polynomial and same rank, prove they're similar. Also prove that this is not necessarily the case if the two matrices are 7x7. If two matrix have the minimal polynomial and same rank, then the following can be generalized: 1) they have the same eigenvalue, 0 2) then have the same nilpotent index 3) they have the same geometric multiplicity But I'm not seeing how this can explicitly imply similarity and how the 7x7 case is any different.",,"['linear-algebra', 'nilpotence']"
54,How do I find Jordan basis?,How do I find Jordan basis?,,"I have a matrix: $$A=\begin{pmatrix}0&1&0\\-4&4&0\\-2&1&2\end{pmatrix}$$ solving $\det|A-\lambda{I}|$ I got characteristic polynom that equals to $(2-\lambda)^3 = 0$ for eigenvalue found two eigenvectors and one generalized eigenvector: $v_1=(1,2,0)\quad v_2=(0,0,1) \quad v_3=(1,0,0)$ What do I have to do to find Jordan basis here? (and what do I need to find Jordan basis in general, I mean is there appropriate alghoritm?, What I read did not make things more clear).","I have a matrix: $$A=\begin{pmatrix}0&1&0\\-4&4&0\\-2&1&2\end{pmatrix}$$ solving $\det|A-\lambda{I}|$ I got characteristic polynom that equals to $(2-\lambda)^3 = 0$ for eigenvalue found two eigenvectors and one generalized eigenvector: $v_1=(1,2,0)\quad v_2=(0,0,1) \quad v_3=(1,0,0)$ What do I have to do to find Jordan basis here? (and what do I need to find Jordan basis in general, I mean is there appropriate alghoritm?, What I read did not make things more clear).",,"['linear-algebra', 'eigenvalues-eigenvectors', 'jordan-normal-form']"
55,"Prove: $\operatorname{rank} A \leq 1$ iff $A=xy^T$ for some $x,y \in \mathbb{R}^{n \times 1}$. [duplicate]",Prove:  iff  for some . [duplicate],"\operatorname{rank} A \leq 1 A=xy^T x,y \in \mathbb{R}^{n \times 1}","This question already has answers here : A rank-one matrix is the product of two vectors (3 answers) Closed 6 years ago . Let $A \in \mathbb{R}^{n \times n}$. Then $\operatorname{rank} A \leq 1$ if and only if $A = xy^T$ for some $x, y \in \mathbb{R}^{n \times 1}$. Need help. I can't find sufficient facts nor theorems to support my proof on the statement.","This question already has answers here : A rank-one matrix is the product of two vectors (3 answers) Closed 6 years ago . Let $A \in \mathbb{R}^{n \times n}$. Then $\operatorname{rank} A \leq 1$ if and only if $A = xy^T$ for some $x, y \in \mathbb{R}^{n \times 1}$. Need help. I can't find sufficient facts nor theorems to support my proof on the statement.",,['linear-algebra']
56,Are eigenvalues continuous functions of a matrix function?,Are eigenvalues continuous functions of a matrix function?,,"Consider a matrix-valued function $A(t):\mathbb{R}_+ \to \mathbb{R}^{n \times n}$. Suppose that each element $a_{ij}(t)$ is a smooth function with bounded derivative, e.g. $a_{ij}(t)=A_{ij}\sin(\omega_{ij} t)$. Define $f(t)$ as a minimum by absolute value eigenvalue of $A(t)$: $$f(t) = \min_i |\lambda_i\{A(t)\}|.$$ Is it true that $f(t)$ is continuous differentiable and has bounded derivative? If so, then how it can be proven, or in which book/paper it can be found? Update Ok, $f(t)$ is continuous, but probably not differentiable. I have revised my main problem and I see that I can alleviate the question. So, now I need $f(t)$ to be Lipschitz. Actually, what I really need it to show that if for some $t_0 \in [t_a,t_b]$ all eignvalus of $A(t)$ are nonzero, then $$\int_{t_a}^{t_b}\left(\det\{A(s)\}\right)^2ds>0.$$ My intention was to use $\left(\det\{A(t)\}\right)^2=\prod_i^n|\lambda_i\{A(t)\}|^2\ge \left(\min_i |\lambda_i\{A(t)\}|\right)^{2n} = f(t)^{2n}$. Note also that all $a_{ij}(t)$ are bounded.","Consider a matrix-valued function $A(t):\mathbb{R}_+ \to \mathbb{R}^{n \times n}$. Suppose that each element $a_{ij}(t)$ is a smooth function with bounded derivative, e.g. $a_{ij}(t)=A_{ij}\sin(\omega_{ij} t)$. Define $f(t)$ as a minimum by absolute value eigenvalue of $A(t)$: $$f(t) = \min_i |\lambda_i\{A(t)\}|.$$ Is it true that $f(t)$ is continuous differentiable and has bounded derivative? If so, then how it can be proven, or in which book/paper it can be found? Update Ok, $f(t)$ is continuous, but probably not differentiable. I have revised my main problem and I see that I can alleviate the question. So, now I need $f(t)$ to be Lipschitz. Actually, what I really need it to show that if for some $t_0 \in [t_a,t_b]$ all eignvalus of $A(t)$ are nonzero, then $$\int_{t_a}^{t_b}\left(\det\{A(s)\}\right)^2ds>0.$$ My intention was to use $\left(\det\{A(t)\}\right)^2=\prod_i^n|\lambda_i\{A(t)\}|^2\ge \left(\min_i |\lambda_i\{A(t)\}|\right)^{2n} = f(t)^{2n}$. Note also that all $a_{ij}(t)$ are bounded.",,"['linear-algebra', 'matrices', 'continuity']"
57,What are the matrices $ A $ such as $ \exp(A+B) = \exp(A)\exp(B) $ for all $ B $,What are the matrices  such as  for all, A   \exp(A+B) = \exp(A)\exp(B)   B ,"In  $ M_n(\mathbb{C})  $ , if two matrices commute, then the exponential of their sum is the product of their exponentials. This property invited me to reflect on the matrices  $ A $ for which $ \exp(A+B)  =  \exp(A)\exp(B)  $ is true for all complex matrix  $ B $. I would like to show that such matrices are scalar (proportional to  $ I_n $ ). What if  $ \mathbb{C} $  is replaced by  $ \mathbb{R} $?","In  $ M_n(\mathbb{C})  $ , if two matrices commute, then the exponential of their sum is the product of their exponentials. This property invited me to reflect on the matrices  $ A $ for which $ \exp(A+B)  =  \exp(A)\exp(B)  $ is true for all complex matrix  $ B $. I would like to show that such matrices are scalar (proportional to  $ I_n $ ). What if  $ \mathbb{C} $  is replaced by  $ \mathbb{R} $?",,"['linear-algebra', 'matrices', 'lie-groups', 'lie-algebras', 'matrix-exponential']"
58,Matrix determinant as Dickson polynomial $\frac{x^{n+1}-y^{n+1}}{x-y}$,Matrix determinant as Dickson polynomial,\frac{x^{n+1}-y^{n+1}}{x-y},"Given matrix $$ A=\begin{bmatrix} x+y&xy&0& .&.&. &0\\ 1&x+y&xy&0& .&.&0 \\ 0&1&x+y&xy&.&.&. \\ .&.&.&.&.&.&. \\ .&.&.&.&.&.&0 \\ .&.&.&.&.&.&xy \\ 0&.&.&.&0&1&x+y \end{bmatrix} $$ prove by induction that $$|A|=\frac{x^{n+1}-y^{n+1}}{x-y}$$ $x \neq y$, $A_{n \times n}$. The determinant expression appears to be Dickson polynomial of second kind. Let $D_n$ be the determinant of $A_n$. We can see that the appropriate recurrence relation is $$D_n=(x+y)D_{n-1}-xyD_{n-2}$$ Base cases: $$D_1=x+y=\frac{x^2-y^2}{x-y}$$ $$ D_2=(x+y)^2-xy=x^2+xy+y^2=\frac{x^3-y^3}{x-y} $$ Suppose that $$D_n=(x+y)D_{n-1}-xyD_{n-2}$$ Then we need to prove that $$D_{n+1}=(x+y)D_{n}-xyD_{n-1}$$ Which can be developed as: $$ D_{n+1}=(x+y)((x+y)D_{n-1}-xyD_{n-2})-xyD_{n-1}= $$ $$ =(x+y)^2D_{n-1}-xy(x+y)D_{n-2}-xyD_{n-1}= $$ $$ =(x^2+xy+y^2)D_{n-1}-xy(x+y)D_{n-2}= $$ $$ =\frac{x^3-y^3}{x-y}D_{n-1}-xy(x+y)D_{n-2} $$ I tried doing this up to $D_{n-6}$ in order to get any insights into possible simplification but I'm pretty stuck.","Given matrix $$ A=\begin{bmatrix} x+y&xy&0& .&.&. &0\\ 1&x+y&xy&0& .&.&0 \\ 0&1&x+y&xy&.&.&. \\ .&.&.&.&.&.&. \\ .&.&.&.&.&.&0 \\ .&.&.&.&.&.&xy \\ 0&.&.&.&0&1&x+y \end{bmatrix} $$ prove by induction that $$|A|=\frac{x^{n+1}-y^{n+1}}{x-y}$$ $x \neq y$, $A_{n \times n}$. The determinant expression appears to be Dickson polynomial of second kind. Let $D_n$ be the determinant of $A_n$. We can see that the appropriate recurrence relation is $$D_n=(x+y)D_{n-1}-xyD_{n-2}$$ Base cases: $$D_1=x+y=\frac{x^2-y^2}{x-y}$$ $$ D_2=(x+y)^2-xy=x^2+xy+y^2=\frac{x^3-y^3}{x-y} $$ Suppose that $$D_n=(x+y)D_{n-1}-xyD_{n-2}$$ Then we need to prove that $$D_{n+1}=(x+y)D_{n}-xyD_{n-1}$$ Which can be developed as: $$ D_{n+1}=(x+y)((x+y)D_{n-1}-xyD_{n-2})-xyD_{n-1}= $$ $$ =(x+y)^2D_{n-1}-xy(x+y)D_{n-2}-xyD_{n-1}= $$ $$ =(x^2+xy+y^2)D_{n-1}-xy(x+y)D_{n-2}= $$ $$ =\frac{x^3-y^3}{x-y}D_{n-1}-xy(x+y)D_{n-2} $$ I tried doing this up to $D_{n-6}$ in order to get any insights into possible simplification but I'm pretty stuck.",,"['linear-algebra', 'induction', 'recurrence-relations', 'determinant']"
59,Adjoint of an operator on $L^2$,Adjoint of an operator on,L^2,"Consider the Hilbert space $L^2 (1,\infty)$ and a real function $g(x)=x^2$ defined on the interval $(1,\infty)$. Define an operator $T$ by the formula $T(f)=f \cdot g$ for all $f$ in the domain $$D(T)=\{ f \in L^2 (1,\infty); \space  \space f \cdot g \in L^2 (1,\infty), \space \int_1^{\infty}f(x)\mathrm{d}x=0\}.$$ I'm trying to find the adjoint operator $T^*$, i.e. the operator defined on the set $$D(T^*)=\{h \in L^2 (1,\infty); \space  \space  \mathrm{the \space mapping \space} f \mapsto \langle Tf,h \rangle \mathrm{\space is \space continuous \space on \space} D(T)\}$$ such that $\langle Tf,h \rangle = \langle f,T^* h \rangle$ for every $f \in D(T), \space h \in D(T^*)$. I managed to prove that the operator $T$ is densely defined (therefore $T^*$ is well-defined and uniquely determined) and closed (i.e. it has a closed graph). Unfortunately, I'm unable to find a nice characterisation of the set $D(T^*)$. My approach is following: Suppose that $h \in D(T^*)$. Then there exists a uniquely determined (thanks to the density of the domain) function $\Psi \in L^2 (1,\infty)$ such that $\langle  Tf,h \rangle = \langle f,\Psi \rangle$ for all $f \in D(T)$. Using the definition of the inner product on $L^2 (1,\infty)$ we find out that for all $f \in D(T)$ $$\int_1^{\infty}f \cdot \left( \bar{h} \cdot g \space - \space \bar{\Psi}  \right )=0.$$ But now I'm stuck because I can't guarantee that the function $\bar{h} \cdot g$ is in $L^2(1,\infty)$. If I knew that, then I would just say that $\bar{h} \cdot g$ is equal to $\Psi$ almost everywhere, but without that I don't know how to continue. Thanks for any advice.","Consider the Hilbert space $L^2 (1,\infty)$ and a real function $g(x)=x^2$ defined on the interval $(1,\infty)$. Define an operator $T$ by the formula $T(f)=f \cdot g$ for all $f$ in the domain $$D(T)=\{ f \in L^2 (1,\infty); \space  \space f \cdot g \in L^2 (1,\infty), \space \int_1^{\infty}f(x)\mathrm{d}x=0\}.$$ I'm trying to find the adjoint operator $T^*$, i.e. the operator defined on the set $$D(T^*)=\{h \in L^2 (1,\infty); \space  \space  \mathrm{the \space mapping \space} f \mapsto \langle Tf,h \rangle \mathrm{\space is \space continuous \space on \space} D(T)\}$$ such that $\langle Tf,h \rangle = \langle f,T^* h \rangle$ for every $f \in D(T), \space h \in D(T^*)$. I managed to prove that the operator $T$ is densely defined (therefore $T^*$ is well-defined and uniquely determined) and closed (i.e. it has a closed graph). Unfortunately, I'm unable to find a nice characterisation of the set $D(T^*)$. My approach is following: Suppose that $h \in D(T^*)$. Then there exists a uniquely determined (thanks to the density of the domain) function $\Psi \in L^2 (1,\infty)$ such that $\langle  Tf,h \rangle = \langle f,\Psi \rangle$ for all $f \in D(T)$. Using the definition of the inner product on $L^2 (1,\infty)$ we find out that for all $f \in D(T)$ $$\int_1^{\infty}f \cdot \left( \bar{h} \cdot g \space - \space \bar{\Psi}  \right )=0.$$ But now I'm stuck because I can't guarantee that the function $\bar{h} \cdot g$ is in $L^2(1,\infty)$. If I knew that, then I would just say that $\bar{h} \cdot g$ is equal to $\Psi$ almost everywhere, but without that I don't know how to continue. Thanks for any advice.",,"['linear-algebra', 'functional-analysis', 'analysis', 'lebesgue-integral']"
60,Does $AA^T$ = I iff A is an orthogonal matrix?,Does  = I iff A is an orthogonal matrix?,AA^T,"I know that if $A$ is an orthogonal matrix, then $AA^T = I$. However, is it possible to have a non orthogonal square matrix but $AA^T = I$ as well? A square matrix of size $n$ is orthogonal if the row spaces and column spaces form an orthogonal basis of ${\bf R} ^n$","I know that if $A$ is an orthogonal matrix, then $AA^T = I$. However, is it possible to have a non orthogonal square matrix but $AA^T = I$ as well? A square matrix of size $n$ is orthogonal if the row spaces and column spaces form an orthogonal basis of ${\bf R} ^n$",,"['linear-algebra', 'transpose', 'orthogonal-matrices']"
61,Tensor product. Simple tensors.,Tensor product. Simple tensors.,,"I have the following problem in my assignment: Prove that every element of $V \otimes W$ can be represented as $\sum \limits_{i = 1}^{k} v_i \otimes w_i$, where $v_i$ are linearly independent and $w_i$ are linearly independent. I thought that we can do the following: Every tensor can be written as a linear combination of basis vectors $e_i \otimes f_j$, so we can represent this tensor as sum: $\sum \limits_{i = 1}^{n} e_i \otimes w_i$ (using tensor and summation properties). So, I have similar representation, but second components might be linearly dependent. My question is am I thinking in the right direction, and if so how I can transform this representation to make vectors in second component linearly independent?","I have the following problem in my assignment: Prove that every element of $V \otimes W$ can be represented as $\sum \limits_{i = 1}^{k} v_i \otimes w_i$, where $v_i$ are linearly independent and $w_i$ are linearly independent. I thought that we can do the following: Every tensor can be written as a linear combination of basis vectors $e_i \otimes f_j$, so we can represent this tensor as sum: $\sum \limits_{i = 1}^{n} e_i \otimes w_i$ (using tensor and summation properties). So, I have similar representation, but second components might be linearly dependent. My question is am I thinking in the right direction, and if so how I can transform this representation to make vectors in second component linearly independent?",,"['linear-algebra', 'tensor-products', 'tensors']"
62,Equation of three lines forming a equilateral triangle.,Equation of three lines forming a equilateral triangle.,,"Prove that if general equation $$\cos \left(3\alpha\right)\left(x^3 -3xy^2\right) + \sin3\alpha\left(y^3 - 3x^2y\right) + 3a\left(x^2 + y^2\right) - 4a^3 = 0$$ represents three lines then they form a equilateral triangle of area $3a^2\sqrt 3$. This equation looks like an good candidate to solve in polar coordinates than in cartesian coordinates. Converting it into polar coordinates by parts \begin{align} \left(x^3 -3xy^2\right) &= r^3 \left(\cos^3 \theta - 3\cos \theta \sin^2 \theta\right) =r^3 \left(4\cos^3 \theta - 3\cos \theta\right) = r^3\cos 3\theta \tag1\label1 \\ \left(y^3 - 3x^2y\right) &= r^3\left(4\sin^3 \theta - 3\sin\theta\right) = r^3\left(\cos\left(3\pi/2 - 3\theta\right)\right) = -r^3\sin 3\theta\tag2\label2 \end{align} Using $\eqref1$, $\eqref2$ \begin{align} r^3\left(\cos 3\alpha\cos 3\theta - \sin3\alpha\sin 3\theta\right) + 3ar^2 - 4a^3 &= 0 \\ r^3\cos \left(3\alpha+ 3\theta\right) + 3ar^2 - 4a^3 &= 0 \end{align} On dividing by $a^3$ and substituting $z = r/a$ $$z^3\cos \left(3\alpha+ 3\theta\right) + 3z^2 - 4 = 0.$$ Now this happens very often to me, I am not able to solve this cubic for $z$.I did try to find the factor by putting $\,z = \sin\left(3\alpha + 3\theta\right),\,\tan\left(3\alpha + 3\theta\right),\,\ldots$ What should I do now ? how to solve this cubic ?","Prove that if general equation $$\cos \left(3\alpha\right)\left(x^3 -3xy^2\right) + \sin3\alpha\left(y^3 - 3x^2y\right) + 3a\left(x^2 + y^2\right) - 4a^3 = 0$$ represents three lines then they form a equilateral triangle of area $3a^2\sqrt 3$. This equation looks like an good candidate to solve in polar coordinates than in cartesian coordinates. Converting it into polar coordinates by parts \begin{align} \left(x^3 -3xy^2\right) &= r^3 \left(\cos^3 \theta - 3\cos \theta \sin^2 \theta\right) =r^3 \left(4\cos^3 \theta - 3\cos \theta\right) = r^3\cos 3\theta \tag1\label1 \\ \left(y^3 - 3x^2y\right) &= r^3\left(4\sin^3 \theta - 3\sin\theta\right) = r^3\left(\cos\left(3\pi/2 - 3\theta\right)\right) = -r^3\sin 3\theta\tag2\label2 \end{align} Using $\eqref1$, $\eqref2$ \begin{align} r^3\left(\cos 3\alpha\cos 3\theta - \sin3\alpha\sin 3\theta\right) + 3ar^2 - 4a^3 &= 0 \\ r^3\cos \left(3\alpha+ 3\theta\right) + 3ar^2 - 4a^3 &= 0 \end{align} On dividing by $a^3$ and substituting $z = r/a$ $$z^3\cos \left(3\alpha+ 3\theta\right) + 3z^2 - 4 = 0.$$ Now this happens very often to me, I am not able to solve this cubic for $z$.I did try to find the factor by putting $\,z = \sin\left(3\alpha + 3\theta\right),\,\tan\left(3\alpha + 3\theta\right),\,\ldots$ What should I do now ? how to solve this cubic ?",,"['linear-algebra', 'geometry']"
63,Differentiation wrt matrix involvoing Khatri-rao product,Differentiation wrt matrix involvoing Khatri-rao product,,"I got a following minimization problem $$\min_{\mathbf{X}^{(1)}, \, \mathbf{X}^{(2)}} \;\left\| \mathbf{B} - \mathbf{A} (\mathbf{X}^{(1)} \odot \mathbf{X}^{(2)}) \right\|^{2}_{F},$$ where the matrices $\mathbf{B}\in \mathbb{R}^{100 \times 3}$, $\mathbf{A}\in \mathbb{R}^{100\times 36}$, $\mathbf{X}^{(1)}\in \mathbb{R}^{9 \times 3}$ and $\mathbf{X}^{(2)}\in \mathbb{R}^{4 \times 3}$. The operation $\odot$ refers to the Khatri-rao product . Given matrices $\mathbf{A}$ and $\mathbf{B}$, my problem is to find out matrices $\mathbf{X}^{(1)}$ and $\mathbf{X}^{(2)}$  such that $$\mathbb{f} = \left\| \mathbf{B} - \mathbf{A} (\mathbf{X}^{(1)} \odot \mathbf{X}^{(2)}) \right\|^{2}_{F}$$ is minimized. My idea is to compute the gradient of $\mathbb{f}$ with respect to $\mathbf{X}^{(1)}$ and $\mathbf{X}^{(2)}$ respectively. My question is, how to do differentiation with respect to a matrix? I have consulted a reference but the situation seems different because it involves a Khatri-rao product in $\mathbb{f}$. Thanks in advance. $\dfrac{\partial \mathbf{f}}{\partial \mathbf{X}^{(1)}}$ and $\dfrac{\partial \mathbf{f}}{\partial \mathbf{X}^{(2)}} $?","I got a following minimization problem $$\min_{\mathbf{X}^{(1)}, \, \mathbf{X}^{(2)}} \;\left\| \mathbf{B} - \mathbf{A} (\mathbf{X}^{(1)} \odot \mathbf{X}^{(2)}) \right\|^{2}_{F},$$ where the matrices $\mathbf{B}\in \mathbb{R}^{100 \times 3}$, $\mathbf{A}\in \mathbb{R}^{100\times 36}$, $\mathbf{X}^{(1)}\in \mathbb{R}^{9 \times 3}$ and $\mathbf{X}^{(2)}\in \mathbb{R}^{4 \times 3}$. The operation $\odot$ refers to the Khatri-rao product . Given matrices $\mathbf{A}$ and $\mathbf{B}$, my problem is to find out matrices $\mathbf{X}^{(1)}$ and $\mathbf{X}^{(2)}$  such that $$\mathbb{f} = \left\| \mathbf{B} - \mathbf{A} (\mathbf{X}^{(1)} \odot \mathbf{X}^{(2)}) \right\|^{2}_{F}$$ is minimized. My idea is to compute the gradient of $\mathbb{f}$ with respect to $\mathbf{X}^{(1)}$ and $\mathbf{X}^{(2)}$ respectively. My question is, how to do differentiation with respect to a matrix? I have consulted a reference but the situation seems different because it involves a Khatri-rao product in $\mathbb{f}$. Thanks in advance. $\dfrac{\partial \mathbf{f}}{\partial \mathbf{X}^{(1)}}$ and $\dfrac{\partial \mathbf{f}}{\partial \mathbf{X}^{(2)}} $?",,"['calculus', 'linear-algebra', 'matrices', 'multivariable-calculus', 'optimization']"
64,"$(1,1)$ tensor vs a linear transformation (matrix)",tensor vs a linear transformation (matrix),"(1,1)","Take $d$-dimensional Vector space $V$ with Field $R$. A typical linear algebra linear transformation $V \to V$ can be represented by a $d \times d$ matrix $A$ such that for some $v,w \in V$, $Av=w$. I'm learning about tensors, and I understand that a $(1,1)$ tensor $T$ is a linear transformation $V^* \times V \to R$. I've read that such a $(1,1)$ tensor is equivalent to such a matrix. However, I find it very difficult to imagine what $V^*$ (the dual space, i.e. set of all maps $V\to R$) has to do with a simple linear transformation from $R^d$ to $R^d$. Moreover, the tensor components apparently are defined as $T^i_{\space \space j}=T(\epsilon_i, e^j)$, where $e^j, \epsilon _i$ are the $d$ bases of $V$ and $V^*$ respectively. This means that if we would write $T$ as a 2-dimensional array, it would have nothing to do with a matrix as in linear algebra. So how are these two concepts connected? This post is related to my question, but it doesn't really go into the difference between the matrix and tensor form.","Take $d$-dimensional Vector space $V$ with Field $R$. A typical linear algebra linear transformation $V \to V$ can be represented by a $d \times d$ matrix $A$ such that for some $v,w \in V$, $Av=w$. I'm learning about tensors, and I understand that a $(1,1)$ tensor $T$ is a linear transformation $V^* \times V \to R$. I've read that such a $(1,1)$ tensor is equivalent to such a matrix. However, I find it very difficult to imagine what $V^*$ (the dual space, i.e. set of all maps $V\to R$) has to do with a simple linear transformation from $R^d$ to $R^d$. Moreover, the tensor components apparently are defined as $T^i_{\space \space j}=T(\epsilon_i, e^j)$, where $e^j, \epsilon _i$ are the $d$ bases of $V$ and $V^*$ respectively. This means that if we would write $T$ as a 2-dimensional array, it would have nothing to do with a matrix as in linear algebra. So how are these two concepts connected? This post is related to my question, but it doesn't really go into the difference between the matrix and tensor form.",,"['linear-algebra', 'matrices', 'tensors']"
65,Proof of $L^2$ inner product?,Proof of  inner product?,L^2,"Can anyone help me prove that the $L^2$ inner product is in fact an inner product? I'm particularly struggling to prove that it is conjugate-symmetric and that length is positive. This is the inner product in question: For $f,g \in L^2([a,b])$,  $\langle f,g \rangle = \int_a^b f(t) \overline{g(t)}dt$","Can anyone help me prove that the $L^2$ inner product is in fact an inner product? I'm particularly struggling to prove that it is conjugate-symmetric and that length is positive. This is the inner product in question: For $f,g \in L^2([a,b])$,  $\langle f,g \rangle = \int_a^b f(t) \overline{g(t)}dt$",,"['linear-algebra', 'complex-analysis']"
66,Trace inequality on the product of positive semi-definite matrices,Trace inequality on the product of positive semi-definite matrices,,Let $A_1$ and $A_2$ be positive semi-definite matrices such that Tr$(A_1) \leq$ Tr$(A_2)$. Let $B$ be another positive semi-definite matrix. Is it true that Tr$(A_1B) \leq$ Tr$(A_2B)$?,Let $A_1$ and $A_2$ be positive semi-definite matrices such that Tr$(A_1) \leq$ Tr$(A_2)$. Let $B$ be another positive semi-definite matrix. Is it true that Tr$(A_1B) \leq$ Tr$(A_2B)$?,,"['linear-algebra', 'matrices', 'inequality', 'trace', 'positive-semidefinite']"
67,How can I show that $v_i\otimes v_j$ is non zero in $V \otimes_\mathbb {F}V$,How can I show that  is non zero in,v_i\otimes v_j V \otimes_\mathbb {F}V,"Let $V$ is a finite dimensional vector space over a field $\mathbb F.$ Let $\rm dim (V)=n>0$ and $\mathcal {B}=\{v_1,\ldots,v_n\}$ be a basis of $V.$ Now we know dimension of  $V \otimes_\mathbb {F}V$ is $n^2$ as $V \otimes_\mathbb {F}V \cong \mathbb {F^{n^2}}.$ Now since the set $\mathcal {A}=\{v_i\otimes v_j:1 \leq i,j\leq n\}$ spans  $V \otimes_\mathbb {F}V$ and the number of elements in $\mathcal {A}$ is $n^2$, $\mathcal {A}$ forms a basis for  $V \otimes_\mathbb {F}V$ as a vector space over $\mathbb F.$ In this way every element in $\mathcal {A}$ is non zero. Now my question is if I don't want to use the above arguments how can I show that for any $i$ and $j$ the element  $v_i\otimes v_j$ is non zero in  $V \otimes_\mathbb {F}V$. By the construction of tensor product if it can be shown then it will help me. Help me. Many Thanks.","Let $V$ is a finite dimensional vector space over a field $\mathbb F.$ Let $\rm dim (V)=n>0$ and $\mathcal {B}=\{v_1,\ldots,v_n\}$ be a basis of $V.$ Now we know dimension of  $V \otimes_\mathbb {F}V$ is $n^2$ as $V \otimes_\mathbb {F}V \cong \mathbb {F^{n^2}}.$ Now since the set $\mathcal {A}=\{v_i\otimes v_j:1 \leq i,j\leq n\}$ spans  $V \otimes_\mathbb {F}V$ and the number of elements in $\mathcal {A}$ is $n^2$, $\mathcal {A}$ forms a basis for  $V \otimes_\mathbb {F}V$ as a vector space over $\mathbb F.$ In this way every element in $\mathcal {A}$ is non zero. Now my question is if I don't want to use the above arguments how can I show that for any $i$ and $j$ the element  $v_i\otimes v_j$ is non zero in  $V \otimes_\mathbb {F}V$. By the construction of tensor product if it can be shown then it will help me. Help me. Many Thanks.",,"['linear-algebra', 'vector-spaces', 'tensor-products']"
68,"$\ker S\subset\ker T\Leftrightarrow\exists R, T=RS$",,"\ker S\subset\ker T\Leftrightarrow\exists R, T=RS","Let $V,W$ be two vector spaces over $F=\mathbb{R}$ or $\mathbb{C}$, such that $W$ is finite dimensional and $S,T\in L(V,W)$. Show that $\ker S\subset\ker T$ if, and only if, there exists a linear operator $R:W\to W$ such that $T=RS$. The converse was easy to verify, since if $Sv = 0$, then $Tv = RSv = R(0) = 0$. I tried to prove the other assertion by constructing such $R$ considering basis for $\operatorname{range} T$ and $\operatorname{range} S$ and send one to the other, but this is as far as I got. Could anyone give me a hint?","Let $V,W$ be two vector spaces over $F=\mathbb{R}$ or $\mathbb{C}$, such that $W$ is finite dimensional and $S,T\in L(V,W)$. Show that $\ker S\subset\ker T$ if, and only if, there exists a linear operator $R:W\to W$ such that $T=RS$. The converse was easy to verify, since if $Sv = 0$, then $Tv = RSv = R(0) = 0$. I tried to prove the other assertion by constructing such $R$ considering basis for $\operatorname{range} T$ and $\operatorname{range} S$ and send one to the other, but this is as far as I got. Could anyone give me a hint?",,"['linear-algebra', 'vector-spaces', 'linear-transformations']"
69,When is the matrix $\text{diag}(\mathbf{x}) + \mathbf{1}$ invertible?,When is the matrix  invertible?,\text{diag}(\mathbf{x}) + \mathbf{1},"Given a vector $\mathbf{x} \in \mathbb{R}^N$, let's define: $$\text{diag}(\mathbf{x}) = \begin{pmatrix} x_1 & 0 & \ldots & 0 \\ 0 & x_2 & \ldots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \ldots & x_N \end{pmatrix}.$$ Moreover, let $$\mathbf{1}= \begin{pmatrix} 1 & 1 & \ldots & 1 \\ 1 & 1 & \ldots & 1 \\ \vdots & \vdots & \ddots & \vdots \\ 1 & 1 & \ldots & 1 \end{pmatrix}.$$ Here is my question: When is the matrix $\mathbf{M} = \text{diag}(\mathbf{x}) + \mathbf{1}$ invertible? I was able to find some results when $x_1 = x_2 = \ldots = x_N = x$. Indeed, the matrix $M$ is singular when: $x=0$. This is trivial since $\mathbf{M} = \mathbf{1}$... $x=-N$. In this case, if you sum up all the rows (or columns) of the matrix $M$, you get the zero vector. What can I say in the general case when $\text{diag}(\mathbf{x})$ is a generic vector?","Given a vector $\mathbf{x} \in \mathbb{R}^N$, let's define: $$\text{diag}(\mathbf{x}) = \begin{pmatrix} x_1 & 0 & \ldots & 0 \\ 0 & x_2 & \ldots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \ldots & x_N \end{pmatrix}.$$ Moreover, let $$\mathbf{1}= \begin{pmatrix} 1 & 1 & \ldots & 1 \\ 1 & 1 & \ldots & 1 \\ \vdots & \vdots & \ddots & \vdots \\ 1 & 1 & \ldots & 1 \end{pmatrix}.$$ Here is my question: When is the matrix $\mathbf{M} = \text{diag}(\mathbf{x}) + \mathbf{1}$ invertible? I was able to find some results when $x_1 = x_2 = \ldots = x_N = x$. Indeed, the matrix $M$ is singular when: $x=0$. This is trivial since $\mathbf{M} = \mathbf{1}$... $x=-N$. In this case, if you sum up all the rows (or columns) of the matrix $M$, you get the zero vector. What can I say in the general case when $\text{diag}(\mathbf{x})$ is a generic vector?",,"['linear-algebra', 'matrices', 'inverse']"
70,"Showing $e^A$ is positive definite, for any Hermitian matrix $A$","Showing  is positive definite, for any Hermitian matrix",e^A A,"Show that $e^A$ is positive definite for any Hermitian Matrix $A\in M_{n\times n}(\mathbb{C})$. I'm not sure  that my argument is valid, but by following lemma it seems fine to me. I appreciate any correction and suggestion. Lemma: If $V$ is an inner product space finitely generated over $\mathbb{C}$. A nomral endomorphism of $\alpha$ is positive definite iff each of its eigenvalues is positive. Define $\alpha:v\to Av$ where $v\in \mathbb{C}^n$. Since $A$ is Hermitian, so $\alpha$ is normal, and it's orthogonality diagonalizable. Hence there exists an orthonormal basis $B=\{v_1,\dots,v_n\}$ for $\mathbb{C}^n$, composed of eigenvectors of $\alpha.$ Thus, $\phi_{BB}(\alpha)=[a_{ij}]$ is a diagonal matrix where each $a_{ii}$ is eigenvalue of $\alpha$ associated with the eigenvector $v_i$ for all $1\leq i \leq n$.  $A$ is similar to $\phi_{BB}(\alpha)$, so there exists a nonsingular matrix $P$ satisfying $A=P^{-1}\phi_{BB}(A)P$. Since $A$ is hermitian then $e^A$ is Hermitian, so it's normal, and by the properties of an exponential matrix, we can get $$e^A=P^{-1}\begin{bmatrix} e^{a_{11}}&\dots& 0\\  \vdots&\ddots&\vdots\\ 0&\dots &e^{a_{nn}} \end{bmatrix}P $$ As, all of eigenvalues of $e^A$ is positive, hence it's positive definite.","Show that $e^A$ is positive definite for any Hermitian Matrix $A\in M_{n\times n}(\mathbb{C})$. I'm not sure  that my argument is valid, but by following lemma it seems fine to me. I appreciate any correction and suggestion. Lemma: If $V$ is an inner product space finitely generated over $\mathbb{C}$. A nomral endomorphism of $\alpha$ is positive definite iff each of its eigenvalues is positive. Define $\alpha:v\to Av$ where $v\in \mathbb{C}^n$. Since $A$ is Hermitian, so $\alpha$ is normal, and it's orthogonality diagonalizable. Hence there exists an orthonormal basis $B=\{v_1,\dots,v_n\}$ for $\mathbb{C}^n$, composed of eigenvectors of $\alpha.$ Thus, $\phi_{BB}(\alpha)=[a_{ij}]$ is a diagonal matrix where each $a_{ii}$ is eigenvalue of $\alpha$ associated with the eigenvector $v_i$ for all $1\leq i \leq n$.  $A$ is similar to $\phi_{BB}(\alpha)$, so there exists a nonsingular matrix $P$ satisfying $A=P^{-1}\phi_{BB}(A)P$. Since $A$ is hermitian then $e^A$ is Hermitian, so it's normal, and by the properties of an exponential matrix, we can get $$e^A=P^{-1}\begin{bmatrix} e^{a_{11}}&\dots& 0\\  \vdots&\ddots&\vdots\\ 0&\dots &e^{a_{nn}} \end{bmatrix}P $$ As, all of eigenvalues of $e^A$ is positive, hence it's positive definite.",,"['linear-algebra', 'inner-products', 'orthonormal', 'matrix-exponential']"
71,Probability of making it across a path of $n$ tiles through random walk,Probability of making it across a path of  tiles through random walk,n,"The problem Imagine someone moving across a path laid out on a 2D grid: The white tiles are the path; the surrounding red tiles are, say, deadly lava. They repeatedly move randomly north , east , south , or west , with equal probability, and have to make it from the Start tile to the End tile without stepping onto the lava. What is the probability $p(n)$ that they succeed on an $n$-square-long path ($n=5$ is shown above)? What I’ve tried Number the tiles $1, \dots, n$, so that we’re walking from $1$ to $n$. Call $a_k$ the probability we succeed when starting at the $k$-th tile. Clearly, $a_n = 1$, and we’re interested in the value of $a_1$. From the $k$-the tile, there’s a 25% chance we move back to tile $k-1$, a 25% chance we move forward to tile $k+1$, and a 50% chance we step north or south onto a red tile and lose. So $a_k = \frac 14 \left( a_{k-1} + a_{k+1} \right)$, where $a_0 = 0$. Putting the equations for $a_1, \dots, a_{n}$ in a matrix system gets us: \begin{equation} \newcommand{\mof}{-1/4} \begin{bmatrix} 1 & \mof & 0 & \dots & 0 & 0 & 0 \\ \mof & 1 & \mof & \dots & 0 & 0 & 0 \\ 0 & \mof & 1 & \dots & 0 & 0 & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \dots & 1 & \mof & 0 \\ 0 & 0 & 0 & \dots & \mof & 1 & \mof \\ 0 & 0 & 0 & \dots & 0 & \color{red}0 & 1 \\ \end{bmatrix} \begin{bmatrix} a_1 \\ a_2 \\ a_3 \\ \vdots \\ a_{n-2} \\ a_{n-1} \\ a_n \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \\ \vdots \\ 0 \\ 0 \\ 1 \end{bmatrix} \tag{1} \end{equation} I wrote some Mathematica code to solve this system for a given $n$ and give the value of $a_1$: p[n_] :=   LinearSolve[     Table[If[x == y, 1,             If[y == n, If[x == n,1,0],               If[Abs[x - y] == 1, -1/4, 0]]],           {y, 1, n}, {x, 1, n}],     Table[If[x == n, 1, 0], {x, 1, n}]   ][[1]] The first few values $p(1), p(2), \dots$ are $$ \frac11, \frac1{4}, \frac1{15}, \frac1{56}, \frac1{209}, \frac1{780}, \dots, $$ the inverses of A001353 in the OEIS, which suggests a closed form: $$p(n) = \frac{2 \sqrt{3}}{\left( 2 + \sqrt{3} \right)^n - \left( 2 - \sqrt{3} \right)^n}$$ But I’m not sure how to get there. I doubt there’s a nice way to solve a system like $(1)$ by hand. Maybe a combinatoric approach yields this formula without taking a detour through solving a system.","The problem Imagine someone moving across a path laid out on a 2D grid: The white tiles are the path; the surrounding red tiles are, say, deadly lava. They repeatedly move randomly north , east , south , or west , with equal probability, and have to make it from the Start tile to the End tile without stepping onto the lava. What is the probability $p(n)$ that they succeed on an $n$-square-long path ($n=5$ is shown above)? What I’ve tried Number the tiles $1, \dots, n$, so that we’re walking from $1$ to $n$. Call $a_k$ the probability we succeed when starting at the $k$-th tile. Clearly, $a_n = 1$, and we’re interested in the value of $a_1$. From the $k$-the tile, there’s a 25% chance we move back to tile $k-1$, a 25% chance we move forward to tile $k+1$, and a 50% chance we step north or south onto a red tile and lose. So $a_k = \frac 14 \left( a_{k-1} + a_{k+1} \right)$, where $a_0 = 0$. Putting the equations for $a_1, \dots, a_{n}$ in a matrix system gets us: \begin{equation} \newcommand{\mof}{-1/4} \begin{bmatrix} 1 & \mof & 0 & \dots & 0 & 0 & 0 \\ \mof & 1 & \mof & \dots & 0 & 0 & 0 \\ 0 & \mof & 1 & \dots & 0 & 0 & 0 \\ \vdots & \vdots & \vdots & \ddots & \vdots & \vdots & \vdots \\ 0 & 0 & 0 & \dots & 1 & \mof & 0 \\ 0 & 0 & 0 & \dots & \mof & 1 & \mof \\ 0 & 0 & 0 & \dots & 0 & \color{red}0 & 1 \\ \end{bmatrix} \begin{bmatrix} a_1 \\ a_2 \\ a_3 \\ \vdots \\ a_{n-2} \\ a_{n-1} \\ a_n \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \\ \vdots \\ 0 \\ 0 \\ 1 \end{bmatrix} \tag{1} \end{equation} I wrote some Mathematica code to solve this system for a given $n$ and give the value of $a_1$: p[n_] :=   LinearSolve[     Table[If[x == y, 1,             If[y == n, If[x == n,1,0],               If[Abs[x - y] == 1, -1/4, 0]]],           {y, 1, n}, {x, 1, n}],     Table[If[x == n, 1, 0], {x, 1, n}]   ][[1]] The first few values $p(1), p(2), \dots$ are $$ \frac11, \frac1{4}, \frac1{15}, \frac1{56}, \frac1{209}, \frac1{780}, \dots, $$ the inverses of A001353 in the OEIS, which suggests a closed form: $$p(n) = \frac{2 \sqrt{3}}{\left( 2 + \sqrt{3} \right)^n - \left( 2 - \sqrt{3} \right)^n}$$ But I’m not sure how to get there. I doubt there’s a nice way to solve a system like $(1)$ by hand. Maybe a combinatoric approach yields this formula without taking a detour through solving a system.",,"['linear-algebra', 'probability', 'combinatorics', 'systems-of-equations', 'random-walk']"
72,Showing that a matrix is invertible by factorisation,Showing that a matrix is invertible by factorisation,,Let $P_5$ be the vector space of polynomials of degree $\leq$ 5 over $Q$. $P_5 = {a_0 + a_1x + a_2x^2 + a_3x^3 + a_4x^4 + a_5x^5: a_i \in Q}$ and let $D: P_5 \longrightarrow P_5$ be the linear map $D(\alpha) = \frac{d \alpha}{dx}$. By factorising the expression $D^6 - Id$ show that $D^4+D^2+Id: P_5 \longrightarrow P_5$ is invertible and write down its inverse. I am not quite sure how I can show that $D^4+D^2+Id$ is invertible but I tried factorising the given expression in the following way: $$ \begin{align*} D^6 - Id &= D^6 - Id^6\\ &= (D-Id) (D^5 + D^4Id + D^3Id^2 + D^2Id^3 + DId^4 + Id^5)\\ &= (D-Id)(D^5 + D^4 + D^3 + D^2 + D + Id)\\ &= (D-Id) (D^3(D^2+D+Id) + D^2 + D+Id)\\ &= (D-Id) (D+Id) (D^4+D^2+Id)\\ &= (D^2 - Id) (D^4 + D^2 + Id)\end{align*} $$ and now I am not sure what to do next Thank you in advance!,Let $P_5$ be the vector space of polynomials of degree $\leq$ 5 over $Q$. $P_5 = {a_0 + a_1x + a_2x^2 + a_3x^3 + a_4x^4 + a_5x^5: a_i \in Q}$ and let $D: P_5 \longrightarrow P_5$ be the linear map $D(\alpha) = \frac{d \alpha}{dx}$. By factorising the expression $D^6 - Id$ show that $D^4+D^2+Id: P_5 \longrightarrow P_5$ is invertible and write down its inverse. I am not quite sure how I can show that $D^4+D^2+Id$ is invertible but I tried factorising the given expression in the following way: $$ \begin{align*} D^6 - Id &= D^6 - Id^6\\ &= (D-Id) (D^5 + D^4Id + D^3Id^2 + D^2Id^3 + DId^4 + Id^5)\\ &= (D-Id)(D^5 + D^4 + D^3 + D^2 + D + Id)\\ &= (D-Id) (D^3(D^2+D+Id) + D^2 + D+Id)\\ &= (D-Id) (D+Id) (D^4+D^2+Id)\\ &= (D^2 - Id) (D^4 + D^2 + Id)\end{align*} $$ and now I am not sure what to do next Thank you in advance!,,"['linear-algebra', 'matrices', 'vector-spaces']"
73,Fixed and current axes of rotation,Fixed and current axes of rotation,,"I was astonished by ingenuity of many users who demonstrated  reasons for why rotational matrices are not commutative. However in 3d rotations I'm more puzzled by some other theorem ... How intuitively to show that composition of rotations about fixed axes of global frame is equal to composition of the same rotations about their current X,Y,Z axes but made in reverse order . In the previous question the most interesting to me was example with permutations... Maybe someone also knows such nice examples..","I was astonished by ingenuity of many users who demonstrated  reasons for why rotational matrices are not commutative. However in 3d rotations I'm more puzzled by some other theorem ... How intuitively to show that composition of rotations about fixed axes of global frame is equal to composition of the same rotations about their current X,Y,Z axes but made in reverse order . In the previous question the most interesting to me was example with permutations... Maybe someone also knows such nice examples..",,"['linear-algebra', 'matrices', 'rotations']"
74,Compute the equivalence classes,Compute the equivalence classes,,"Define an equivalence relation on $\mathbb{R}^2$ by $\textbf{x}\sim\textbf{y}$ iff $\exists A\in GL_2(\mathbb{R})$ such that $A\mathbf{x}=\mathbf{y}$. Compute the equivalence classes of this equivalence relation. My attempt: Let $\mathbf{x}=\begin{bmatrix}     0 \\     0  \end{bmatrix}$. $A\mathbf{x}=\begin{bmatrix}     0 \\     0  \end{bmatrix}$ $\forall A\in GL_2(\mathbb{R})$ So, it seems that the zero vector resides alone in its equivalence class. My hunch is that all the other (nonzero) vectors reside in the other equivalence class, making a total of 2 equivalence classes. But I don't know how to prove this as there doesn't seem to be any obvious way to solve for the matrix $A$ in the equation $A\mathbf{x}=\mathbf{y}$. Can someone please tell me how to proceed?","Define an equivalence relation on $\mathbb{R}^2$ by $\textbf{x}\sim\textbf{y}$ iff $\exists A\in GL_2(\mathbb{R})$ such that $A\mathbf{x}=\mathbf{y}$. Compute the equivalence classes of this equivalence relation. My attempt: Let $\mathbf{x}=\begin{bmatrix}     0 \\     0  \end{bmatrix}$. $A\mathbf{x}=\begin{bmatrix}     0 \\     0  \end{bmatrix}$ $\forall A\in GL_2(\mathbb{R})$ So, it seems that the zero vector resides alone in its equivalence class. My hunch is that all the other (nonzero) vectors reside in the other equivalence class, making a total of 2 equivalence classes. But I don't know how to prove this as there doesn't seem to be any obvious way to solve for the matrix $A$ in the equation $A\mathbf{x}=\mathbf{y}$. Can someone please tell me how to proceed?",,['linear-algebra']
75,constructing a matrix given its column space and null space,constructing a matrix given its column space and null space,,"This is not homework, class or a project. I've been out of college for some time now and decided to learn math on my own time. I can't figure out how to solve the following problem: Construct a 4 x 4 matrix A whose column space R and null space N are given by $$ R = \alpha \begin{bmatrix} 1\\2\\0\\0 \end{bmatrix} + \beta \begin{bmatrix} 0\\1\\2\\0 \end{bmatrix}$$ $$ N = \alpha \begin{bmatrix} 1\\2\\0\\0 \end{bmatrix} + \beta \begin{bmatrix} 0\\1\\2\\0 \end{bmatrix}$$ How do I approach this problem?","This is not homework, class or a project. I've been out of college for some time now and decided to learn math on my own time. I can't figure out how to solve the following problem: Construct a 4 x 4 matrix A whose column space R and null space N are given by $$ R = \alpha \begin{bmatrix} 1\\2\\0\\0 \end{bmatrix} + \beta \begin{bmatrix} 0\\1\\2\\0 \end{bmatrix}$$ $$ N = \alpha \begin{bmatrix} 1\\2\\0\\0 \end{bmatrix} + \beta \begin{bmatrix} 0\\1\\2\\0 \end{bmatrix}$$ How do I approach this problem?",,"['linear-algebra', 'vector-spaces']"
76,Special Woodbury identity,Special Woodbury identity,,"I wish to understand linear transformations of the type $M = \left(\alpha I_n+ D^TD\right)^{-1}$ where $\alpha \neq 0$ and $D$ is a (full-rank) $k$ by $n$ matrix. ($k<n$) My understanding is that though $D^TD$ is singular and thus does not have an inverse, $\left(\alpha I_n+ D^TD\right)^{-1}$ for $\alpha \ll 1$ could be a ""good"" approximation for it. (or am I mistaken?) Ideally I would like to relate the eigenvectors of $M$ to my original $D$ (probably dependent also on $\alpha$) (see footnote [1]) So my question is: Is it possible to compute the eigenvalues and eigenvectors of this matrix $M$?  Is there any interesting fact that you know about these types of matrices (thinking them of linear transformations) I tried to use Woodbury identity to arrive at an expression $$M = \left(\alpha I_n+ D^TD\right)^{-1}\\ =\alpha^{-1}I_n - \alpha^{-1}D^T\left(I_k+\alpha^{-1}DD^T\right)^{-1}D\alpha^{-1}\\ =\alpha^{-1}I_n - \alpha^{-1}D^T\left(\alpha I_k+DD^T\right)^{-1}D\\ = \alpha^{-1}I_n - \alpha^{-1}D^T\left(\alpha^{-1}I_k - \alpha^{-1}D\left(\alpha I_n+D^TD\right)^{-1}D^T\right)D\\ = \alpha^{-1}I_n - \alpha^{-2}D^TD + \alpha^{-2}D^TD\left(\alpha I_n+D^TD\right)^{-1}D^TD\\ = \alpha^{-1}I_n - \alpha^{-2}D^TD + \alpha^{-2}D^TDMD^TD$$ and got to an implicit equation for $M$ but could not solve it. Any help is appreciated either in finding a closed-form solution or geometrical interpretations of the effect of $M$ [1] Some initial experimentation with simple $D$ lead me to believe that the rows of $D$ are eigenvectors for $M$ is this possible?","I wish to understand linear transformations of the type $M = \left(\alpha I_n+ D^TD\right)^{-1}$ where $\alpha \neq 0$ and $D$ is a (full-rank) $k$ by $n$ matrix. ($k<n$) My understanding is that though $D^TD$ is singular and thus does not have an inverse, $\left(\alpha I_n+ D^TD\right)^{-1}$ for $\alpha \ll 1$ could be a ""good"" approximation for it. (or am I mistaken?) Ideally I would like to relate the eigenvectors of $M$ to my original $D$ (probably dependent also on $\alpha$) (see footnote [1]) So my question is: Is it possible to compute the eigenvalues and eigenvectors of this matrix $M$?  Is there any interesting fact that you know about these types of matrices (thinking them of linear transformations) I tried to use Woodbury identity to arrive at an expression $$M = \left(\alpha I_n+ D^TD\right)^{-1}\\ =\alpha^{-1}I_n - \alpha^{-1}D^T\left(I_k+\alpha^{-1}DD^T\right)^{-1}D\alpha^{-1}\\ =\alpha^{-1}I_n - \alpha^{-1}D^T\left(\alpha I_k+DD^T\right)^{-1}D\\ = \alpha^{-1}I_n - \alpha^{-1}D^T\left(\alpha^{-1}I_k - \alpha^{-1}D\left(\alpha I_n+D^TD\right)^{-1}D^T\right)D\\ = \alpha^{-1}I_n - \alpha^{-2}D^TD + \alpha^{-2}D^TD\left(\alpha I_n+D^TD\right)^{-1}D^TD\\ = \alpha^{-1}I_n - \alpha^{-2}D^TD + \alpha^{-2}D^TDMD^TD$$ and got to an implicit equation for $M$ but could not solve it. Any help is appreciated either in finding a closed-form solution or geometrical interpretations of the effect of $M$ [1] Some initial experimentation with simple $D$ lead me to believe that the rows of $D$ are eigenvectors for $M$ is this possible?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'matrix-equations']"
77,Pseudo-inverse of a matrix that is neither fat nor tall?,Pseudo-inverse of a matrix that is neither fat nor tall?,,"Given a matrix $A\in\mathbb R^{m\times n}$, let us define: $A$ is a fat matrix if $m\le n$ and $\text{null}(A^T)=\{0\}$ $A$ is a tall matrix is $m\ge n$ and $\text{range}(A)=\mathbb R^n$ Using the finite rank lemma, we can find that: When $A$ is a fat matrix, its (right) pseudo-inverse is $A^\dagger = A^T(AA^T)^{-1}$ When $A$ is a tall matrix, its (left) pseudo-inverse is $A^\ddagger = (A^TA)^{-1}A^T$ My question is what is the pseudo-inverse when $A$ is neither fat nor tall (in the sense of the above definitions), i.e. it is a matrix such that $\text{null}(A^T)\ne \{0\}$ (i.e. the null space is non-trivial) and $\text{range}(A)\ne\mathbb R^n$? An example of such a matrix is: $$ A = \begin{bmatrix} 1 & 1 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 0 \\ 0 & 3 & 0 \end{bmatrix} $$ which clearly does not map to full $\mathbb R^4$ and whose null space is $\text{span}\left\{\begin{bmatrix}0 \\ 0 \\ 1\end{bmatrix}\right\}$.","Given a matrix $A\in\mathbb R^{m\times n}$, let us define: $A$ is a fat matrix if $m\le n$ and $\text{null}(A^T)=\{0\}$ $A$ is a tall matrix is $m\ge n$ and $\text{range}(A)=\mathbb R^n$ Using the finite rank lemma, we can find that: When $A$ is a fat matrix, its (right) pseudo-inverse is $A^\dagger = A^T(AA^T)^{-1}$ When $A$ is a tall matrix, its (left) pseudo-inverse is $A^\ddagger = (A^TA)^{-1}A^T$ My question is what is the pseudo-inverse when $A$ is neither fat nor tall (in the sense of the above definitions), i.e. it is a matrix such that $\text{null}(A^T)\ne \{0\}$ (i.e. the null space is non-trivial) and $\text{range}(A)\ne\mathbb R^n$? An example of such a matrix is: $$ A = \begin{bmatrix} 1 & 1 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 0 \\ 0 & 3 & 0 \end{bmatrix} $$ which clearly does not map to full $\mathbb R^4$ and whose null space is $\text{span}\left\{\begin{bmatrix}0 \\ 0 \\ 1\end{bmatrix}\right\}$.",,"['linear-algebra', 'matrices', 'pseudoinverse']"
78,Linear algebra for computer scientists,Linear algebra for computer scientists,,I'm planning a linear algebra course for computer science freshmen. Do you have any good textbook to recommend?,I'm planning a linear algebra course for computer science freshmen. Do you have any good textbook to recommend?,,"['linear-algebra', 'matrices']"
79,Orthogonal eigenvectors in symmetrical matrices with repeated eigenvalues and diagonalization,Orthogonal eigenvectors in symmetrical matrices with repeated eigenvalues and diagonalization,,"Symmetrical matrices have orthogonal eigenvectors. However, there is the special case when eigenvalues are repeated. The ultimate scenario is that of the identity matrix. Professor Strang mentions here that ""if an eigenvalue is repeated, then there is a whole plane of eigenvectors, and in that plane we can choose perpendicular ones""... a ""real substantial freedom."" And he goes on to note that symmetrical matrices can be diagonalized as ${\bf A = Q\Lambda Q'}$. Although he does mention ""... I also mean that there is a full set of them [eigenvectors]"", it sounds (in the video) as though the ${\bf Q\Lambda Q'}$ is not necessarily jeopardized by the presence of repeat eigenvalues. However, and in general for square matrices (not limiting ourselves to symmetrical), repeated eigenvalues can render the matrix non-diagonalizable as $\bf{A=S\Lambda S^{-1}}$. How does all this come together into a question? The ${\bf A'A}$ matrix has many properties shared by positive semidefinite matrices . Among them is its being diagonalizable. Now its eigenvalues do not have to necessarily be distinct (real, yes; but not necessarily of algebraic multiplicity of $1$). So... Is it correct to say that symmetrical matrices have always orthogonal eigenvectors (or we can choose them so that they are), guaranteeing the ${\bf Q\Lambda Q'}$ decomposition, regardless of the possible presence of repeat eigenvalues? If (1) is not true, are we then stuck with a caveat to the assertion that ${\bf A'A}$ matrices are diagonalizable? Can we say that ${\bf A'A}$ is diagonalizable as a blanket statement?","Symmetrical matrices have orthogonal eigenvectors. However, there is the special case when eigenvalues are repeated. The ultimate scenario is that of the identity matrix. Professor Strang mentions here that ""if an eigenvalue is repeated, then there is a whole plane of eigenvectors, and in that plane we can choose perpendicular ones""... a ""real substantial freedom."" And he goes on to note that symmetrical matrices can be diagonalized as ${\bf A = Q\Lambda Q'}$. Although he does mention ""... I also mean that there is a full set of them [eigenvectors]"", it sounds (in the video) as though the ${\bf Q\Lambda Q'}$ is not necessarily jeopardized by the presence of repeat eigenvalues. However, and in general for square matrices (not limiting ourselves to symmetrical), repeated eigenvalues can render the matrix non-diagonalizable as $\bf{A=S\Lambda S^{-1}}$. How does all this come together into a question? The ${\bf A'A}$ matrix has many properties shared by positive semidefinite matrices . Among them is its being diagonalizable. Now its eigenvalues do not have to necessarily be distinct (real, yes; but not necessarily of algebraic multiplicity of $1$). So... Is it correct to say that symmetrical matrices have always orthogonal eigenvectors (or we can choose them so that they are), guaranteeing the ${\bf Q\Lambda Q'}$ decomposition, regardless of the possible presence of repeat eigenvalues? If (1) is not true, are we then stuck with a caveat to the assertion that ${\bf A'A}$ matrices are diagonalizable? Can we say that ${\bf A'A}$ is diagonalizable as a blanket statement?",,['linear-algebra']
80,Rank of Moore-Penrose pseudoinverse matrix [closed],Rank of Moore-Penrose pseudoinverse matrix [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question If $A$ is a matrix of rank $r$, is its Moore-Penrose pseudoinverse also of rank $r$?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question If $A$ is a matrix of rank $r$, is its Moore-Penrose pseudoinverse also of rank $r$?",,"['linear-algebra', 'pseudoinverse']"
81,Transpose of a projection matrix,Transpose of a projection matrix,,"The projection matrix for some matrix $A$, which has $m>n$ is: $$P = A(A^TA)^{-1}A^T$$ A property of the projection matrix is that: $$P^T=P$$ If I call $(A^TA)^{-1}=B$, then: $$P=(AB)A^T$$ Now I take transpose: $$P^T = ((AB)A^T)^T=A(AB)^T=AB^TA^T$$ or $$P^T=A((A^TA)^{-1})^TA^T$$ And is this equal to $P$? That middle term is transposed...","The projection matrix for some matrix $A$, which has $m>n$ is: $$P = A(A^TA)^{-1}A^T$$ A property of the projection matrix is that: $$P^T=P$$ If I call $(A^TA)^{-1}=B$, then: $$P=(AB)A^T$$ Now I take transpose: $$P^T = ((AB)A^T)^T=A(AB)^T=AB^TA^T$$ or $$P^T=A((A^TA)^{-1})^TA^T$$ And is this equal to $P$? That middle term is transposed...",,"['linear-algebra', 'matrices', 'projection-matrices']"
82,Proof of a statement about eigenvalues and eigenvectors.,Proof of a statement about eigenvalues and eigenvectors.,,"How can i proof the following: Let $\mathbb L: V\rightarrow V $ be a linear mapping. Let $v_1,v_2,..,v_n$ non-zero eigenvectors with eigenvalues $c_1,c_2,..,c_n$ respectively, also let the eigenvalues be pairwise differents, show that $v_1,v_2,..,v_n$ are linearly independents. I tried starting to write 0 as a linear combination of $v_1,v_2,..,v_n$ and then do the mapping, and use induction to proof but i could not finish, have another way to proof?","How can i proof the following: Let $\mathbb L: V\rightarrow V $ be a linear mapping. Let $v_1,v_2,..,v_n$ non-zero eigenvectors with eigenvalues $c_1,c_2,..,c_n$ respectively, also let the eigenvalues be pairwise differents, show that $v_1,v_2,..,v_n$ are linearly independents. I tried starting to write 0 as a linear combination of $v_1,v_2,..,v_n$ and then do the mapping, and use induction to proof but i could not finish, have another way to proof?",,"['linear-algebra', 'proof-writing', 'alternative-proof', 'proof-explanation']"
83,Can matrices with dependent columns being QR factorization?,Can matrices with dependent columns being QR factorization?,,"The problem comes from the $18.06$ Linear Algebra by MIT Open Courseware. The answer: I am very confused. According to the definition, Matrix A -> QR means that A has independent columns. BUT it is obviously that the matrix B is singular in the problem. But it can be diagnoalized with 3 independent eigenvectors. How could that happen? Could you explain ? Thanks!","The problem comes from the $18.06$ Linear Algebra by MIT Open Courseware. The answer: I am very confused. According to the definition, Matrix A -> QR means that A has independent columns. BUT it is obviously that the matrix B is singular in the problem. But it can be diagnoalized with 3 independent eigenvectors. How could that happen? Could you explain ? Thanks!",,['linear-algebra']
84,"Let $A,B,C$ be $n \times n$ square real matrices with $ABC=0$. What is the maximum rank of $CBA$?",Let  be  square real matrices with . What is the maximum rank of ?,"A,B,C n \times n ABC=0 CBA","Let $A,B,C$ be $n \times n$ square real matrices with $ABC=0$. What is the maximum rank of $CBA$? From an old written examination . I've looked at the kernel and range of each matrix for simple cases (like if $n=2$), but how can we generalize to any $n$?","Let $A,B,C$ be $n \times n$ square real matrices with $ABC=0$. What is the maximum rank of $CBA$? From an old written examination . I've looked at the kernel and range of each matrix for simple cases (like if $n=2$), but how can we generalize to any $n$?",,['linear-algebra']
85,Definition of basis in infinite-dimensional vector space,Definition of basis in infinite-dimensional vector space,,"I am struggling to understand the definition of a basis in an infinite dimensional vector space. Specifically, the definition I know says: A subset $B$ of a vector space $V$ is a basis for $V$ if every element of $V$ can be written in a unique way as a finite linear combination of elements from $B$. However, for any non-empty subset $X$ of a vector space $V$, the zero element of the space can be written in more than one way as a finite linear combination of elements from $X$. For example, $0 = 0v = 0w$, where $v \neq w$ are from $X$. So therefore, no subset $X$ of a vector space $V$ could be a basis for $V$. What am I missing? What exactly does the definition mean?","I am struggling to understand the definition of a basis in an infinite dimensional vector space. Specifically, the definition I know says: A subset $B$ of a vector space $V$ is a basis for $V$ if every element of $V$ can be written in a unique way as a finite linear combination of elements from $B$. However, for any non-empty subset $X$ of a vector space $V$, the zero element of the space can be written in more than one way as a finite linear combination of elements from $X$. For example, $0 = 0v = 0w$, where $v \neq w$ are from $X$. So therefore, no subset $X$ of a vector space $V$ could be a basis for $V$. What am I missing? What exactly does the definition mean?",,['linear-algebra']
86,Determinant from Paul Garrett's Definition of the Characteristic Polynomial.,Determinant from Paul Garrett's Definition of the Characteristic Polynomial.,,"$\DeclareMathOperator{\id}{id} \DeclareMathOperator{\End}{End}$ On pg. 390 of Paul Garrett's notes on Algebra, a definition for the characteristic polynomial is given, which I discuss here. Let $V$ be an $n$ -dimensional vector space over a field $F$ and let $T$ be a linear operator on $V$ . Consider the $F$ -module $M=F[x]\otimes_F V$ and give it an $F[x]$ -module structure by extending the scalars via the natural inclusion $F\to F[x]$ . Thus $M$ is a free $F[x]$ -module of rank $n$ . Now we can think of $1\otimes T-x\otimes \id_V$ as a member of $\End_{F[x]}(M)$ . Definition. The characteristic polynomial of $T$ is then defined as the unique polynomial $p(x)\in F[x]$ such that the $n$ -th exterior power map $$\Lambda^n_{F[x]}(1\otimes T-x\otimes \id_V):\Lambda^n_{F[x]} M\to \Lambda^n_{F[x]}M$$ is same as multiplication by $p(x)$ with the identity map on $\Lambda^n_{F[x]}M$ . On the other hand, the following is well known: Definition. Let $\lambda\in F$ is a scalar in $F$ , we have a linear operator $T-\lambda I$ on $V$ , whose determinant is defined as the unique scalar $\theta\in F$ such that the map $$\Lambda^n_F (T-\lambda I):\Lambda^n_F V\to \Lambda^n_F V$$ is $\theta$ times the identity map on $V$ . Question. How do we see that $\det(T-\lambda I)=p(\lambda)$ under the above definitions. I have a feeling that this should be rather straightforward from the definitions but I can't see it. I am just beginning to learn exterior algebra over modules.","On pg. 390 of Paul Garrett's notes on Algebra, a definition for the characteristic polynomial is given, which I discuss here. Let be an -dimensional vector space over a field and let be a linear operator on . Consider the -module and give it an -module structure by extending the scalars via the natural inclusion . Thus is a free -module of rank . Now we can think of as a member of . Definition. The characteristic polynomial of is then defined as the unique polynomial such that the -th exterior power map is same as multiplication by with the identity map on . On the other hand, the following is well known: Definition. Let is a scalar in , we have a linear operator on , whose determinant is defined as the unique scalar such that the map is times the identity map on . Question. How do we see that under the above definitions. I have a feeling that this should be rather straightforward from the definitions but I can't see it. I am just beginning to learn exterior algebra over modules.",\DeclareMathOperator{\id}{id} \DeclareMathOperator{\End}{End} V n F T V F M=F[x]\otimes_F V F[x] F\to F[x] M F[x] n 1\otimes T-x\otimes \id_V \End_{F[x]}(M) T p(x)\in F[x] n \Lambda^n_{F[x]}(1\otimes T-x\otimes \id_V):\Lambda^n_{F[x]} M\to \Lambda^n_{F[x]}M p(x) \Lambda^n_{F[x]}M \lambda\in F F T-\lambda I V \theta\in F \Lambda^n_F (T-\lambda I):\Lambda^n_F V\to \Lambda^n_F V \theta V \det(T-\lambda I)=p(\lambda),"['linear-algebra', 'modules', 'determinant', 'multilinear-algebra', 'exterior-algebra']"
87,How to explain why the angle between two vectors in $\mathbb{R}^n$ is defined the way it is.,How to explain why the angle between two vectors in  is defined the way it is.,\mathbb{R}^n,"It is given in couple of the textbooks I have seen that they just define the angle between two vectors $\vec{x}, \vec{y} \in \mathbb{R}^n$ to be $\theta$ such that  $$ \cos \theta = \frac{\vec{x} \cdot \vec{y} }{ \|\vec{x}\| \|\vec{y}\|}. $$  I wanted to explain this to a first year undergraduate student why this makes some sense and that it is not completely arbitrary, but I wasn't really sure how to do so. I would greatly appreciate any explanation! Thank you!!","It is given in couple of the textbooks I have seen that they just define the angle between two vectors $\vec{x}, \vec{y} \in \mathbb{R}^n$ to be $\theta$ such that  $$ \cos \theta = \frac{\vec{x} \cdot \vec{y} }{ \|\vec{x}\| \|\vec{y}\|}. $$  I wanted to explain this to a first year undergraduate student why this makes some sense and that it is not completely arbitrary, but I wasn't really sure how to do so. I would greatly appreciate any explanation! Thank you!!",,"['linear-algebra', 'trigonometry']"
88,Why are there no vectors of length $30$ which are orthogonal to a rotation?,Why are there no vectors of length  which are orthogonal to a rotation?,30,"Consider all $30$-dimensional vectors $v$ such that $v_i \in \{-1,1\}$.  It seems that none of them has the property that $(v_1, \dots, v_{30})$ is orthogonal to $(v_{30}, v_1, \dots, v_{29})$. Without just exhaustively enumerating all $2^{30}$  such vectors, how   can one prove this?","Consider all $30$-dimensional vectors $v$ such that $v_i \in \{-1,1\}$.  It seems that none of them has the property that $(v_1, \dots, v_{30})$ is orthogonal to $(v_{30}, v_1, \dots, v_{29})$. Without just exhaustively enumerating all $2^{30}$  such vectors, how   can one prove this?",,['linear-algebra']
89,Prove rayleigh quotient = operator norm without referring to eigenvalues,Prove rayleigh quotient = operator norm without referring to eigenvalues,,"Let $H$ be a Hilbert and $T \in \mathcal{L}(H,H)$ a symmetric operator. Prove  $$ \|T\|  = \sup\{|(x,Tx)| : x \in H, \|x\| = 1\} $$ without referring to the eigenvalues of $T$ (which is what all proofs I could find do).","Let $H$ be a Hilbert and $T \in \mathcal{L}(H,H)$ a symmetric operator. Prove  $$ \|T\|  = \sup\{|(x,Tx)| : x \in H, \|x\| = 1\} $$ without referring to the eigenvalues of $T$ (which is what all proofs I could find do).",,"['linear-algebra', 'eigenvalues-eigenvectors']"
90,Prove that determinant of the matrix is non-zero,Prove that determinant of the matrix is non-zero,,"Given a square matrix $A$ of order $2n$ such that $a_{ii}=0$ and $a_{ij}\in\{-1,1\},\space i\neq j$, prove that $\det(A)\neq0$.","Given a square matrix $A$ of order $2n$ such that $a_{ii}=0$ and $a_{ij}\in\{-1,1\},\space i\neq j$, prove that $\det(A)\neq0$.",,['linear-algebra']
91,Orthogonality of the degenerate eigenvectors of a real symmetric matrix,Orthogonality of the degenerate eigenvectors of a real symmetric matrix,,"It is relatively easy to show for a real symmetric matrix $ A $ that its eigenvectors belonging to distinct eigenvalues are orthogonal; it comes down to $(\lambda_i - \lambda_j) u_i^Tu_j=0$ and since eigenvalues are different; the eigenvectors have to be orthogonal. When the eigenvalues are equal, I know that we can pick eigenvectors which are orthogonal to eachother and to all other eigenvectors, enabling to build an orthogonal basis of eigenvectors which span $\mathbb {R^N} $ for a $ N \times N $ matrix. I try to show how one can pick orthogonal vectors for a shared eigenvalue $\lambda $. I tried to use the characteristic polynomial $ det (A - \lambda I_N)=0$ which has multiple roots at a shared eigenvalue $\lambda $. Assuming that $\lambda $ has multiplicity of $ m $ I tried to show then the matrix $ A - \lambda I_N $ has an $ m $ dimensional nullspace, spanned by $ m $ eigenvectors, but failed to reach any conclusions. How can we construct a proof of that?","It is relatively easy to show for a real symmetric matrix $ A $ that its eigenvectors belonging to distinct eigenvalues are orthogonal; it comes down to $(\lambda_i - \lambda_j) u_i^Tu_j=0$ and since eigenvalues are different; the eigenvectors have to be orthogonal. When the eigenvalues are equal, I know that we can pick eigenvectors which are orthogonal to eachother and to all other eigenvectors, enabling to build an orthogonal basis of eigenvectors which span $\mathbb {R^N} $ for a $ N \times N $ matrix. I try to show how one can pick orthogonal vectors for a shared eigenvalue $\lambda $. I tried to use the characteristic polynomial $ det (A - \lambda I_N)=0$ which has multiple roots at a shared eigenvalue $\lambda $. Assuming that $\lambda $ has multiplicity of $ m $ I tried to show then the matrix $ A - \lambda I_N $ has an $ m $ dimensional nullspace, spanned by $ m $ eigenvectors, but failed to reach any conclusions. How can we construct a proof of that?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'vectors']"
92,Why is A diagonalizable but not orthogonally diagonalizable?,Why is A diagonalizable but not orthogonally diagonalizable?,,Could you give me an example of a matrix A that is diagonalizable but not orthogonally diagonalizable? and why? I'm trying to understand the difference between the two. I think I got it. Thank you :),Could you give me an example of a matrix A that is diagonalizable but not orthogonally diagonalizable? and why? I'm trying to understand the difference between the two. I think I got it. Thank you :),,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
93,find arc between two tips of vectors in 3D,find arc between two tips of vectors in 3D,,"The figure helps explaining the question: I want to use TiKz to draw an arc between $A$ and $B$. Please note several things. This is a true 3D problem. There are already questions asked and solved for the 2D case. We assume that $\|B-O\|=\|A-O\|$. The common point between the two vectors $O$ does not have to be the origin, but it is not a big deal to translate the whole thing. I do not want 3D rotation matrices. I know the exact three coordinates of each of the three points $O$, $A$, and $B$. It is easy to compute the angle between the two vectors using dot product. So we have the range of angles. I want an equation wich is parametrized as a function of an angle between 0 and the angle between the two vectors. Any point in the plane of the two vectors can be written as: $P = O + s(B-O) + t(A-O)$. I would like simple expressions for $s$ and  $t$ in terms of the components of the vectors. A point $X$ in the arc satisfies the equation $\|X-O\|=\|B-O\|$, $X=O + s(B-O)+t(A-O)$.  4 equations with 5 unknowns. I am missing an equation here. Actualy no. We need to eliminate 4 variables to come to the parametrization of the curve. Is there an easy way to solve this? (by easy I mean no 3D rotation matrices, no large system of equations. Still I would like a solution even if it is messy. Thanks.","The figure helps explaining the question: I want to use TiKz to draw an arc between $A$ and $B$. Please note several things. This is a true 3D problem. There are already questions asked and solved for the 2D case. We assume that $\|B-O\|=\|A-O\|$. The common point between the two vectors $O$ does not have to be the origin, but it is not a big deal to translate the whole thing. I do not want 3D rotation matrices. I know the exact three coordinates of each of the three points $O$, $A$, and $B$. It is easy to compute the angle between the two vectors using dot product. So we have the range of angles. I want an equation wich is parametrized as a function of an angle between 0 and the angle between the two vectors. Any point in the plane of the two vectors can be written as: $P = O + s(B-O) + t(A-O)$. I would like simple expressions for $s$ and  $t$ in terms of the components of the vectors. A point $X$ in the arc satisfies the equation $\|X-O\|=\|B-O\|$, $X=O + s(B-O)+t(A-O)$.  4 equations with 5 unknowns. I am missing an equation here. Actualy no. We need to eliminate 4 variables to come to the parametrization of the curve. Is there an easy way to solve this? (by easy I mean no 3D rotation matrices, no large system of equations. Still I would like a solution even if it is messy. Thanks.",,"['linear-algebra', 'geometry']"
94,When is $\dim(V^*)\gt \dim(V)$ $?$,When is,\dim(V^*)\gt \dim(V) ?,"Let $V$  be  an  infinite dimensional vector space  over  $\mathbb Z_2$  with  countably  infinite  basis  $\{e_1,e_2,e_3,.....,e_n,....\}$ and  write  $$V=S+T$$  s.t. $$S=\langle e_1 \rangle$$  and  $$T=\langle e_2,e_3,...,e_n,...\rangle.$$ Then $$T^*=S^0$$  is  easy  to  see but  how  can  I  prove  that $$T^*\cong V^*$$ and  $$\dim(V^*)\gt \dim(V)\ ?$$ Clearly $V^*$  has  a  function  that  takes  $e_1$  to  $1$  and  the  rest  to  $0$  which  is  not   in  $T^*$ .  Although  that  is  not  going  to  stop  isomorphism  as  the  sets  are  infinite. But  how  to  prove  $?$","Let $V$  be  an  infinite dimensional vector space  over  $\mathbb Z_2$  with  countably  infinite  basis  $\{e_1,e_2,e_3,.....,e_n,....\}$ and  write  $$V=S+T$$  s.t. $$S=\langle e_1 \rangle$$  and  $$T=\langle e_2,e_3,...,e_n,...\rangle.$$ Then $$T^*=S^0$$  is  easy  to  see but  how  can  I  prove  that $$T^*\cong V^*$$ and  $$\dim(V^*)\gt \dim(V)\ ?$$ Clearly $V^*$  has  a  function  that  takes  $e_1$  to  $1$  and  the  rest  to  $0$  which  is  not   in  $T^*$ .  Although  that  is  not  going  to  stop  isomorphism  as  the  sets  are  infinite. But  how  to  prove  $?$",,"['linear-algebra', 'abstract-algebra', 'vector-spaces']"
95,Solving matrix equation involving trace,Solving matrix equation involving trace,,"Let $A$ and $B$ be two symmetric and positive definite matrices and $c\in \mathbb{R}$. Consider the following matrix equation ($A$, $B$, and $c$ are known): $$A=X+c^2\,tr(BX)B$$ Solution of the equation can be obtained by using $vec$ operator and it is clear that it is symmetric. Is it positive definite? It seems the answer is ""yes"", but how can I prove it?","Let $A$ and $B$ be two symmetric and positive definite matrices and $c\in \mathbb{R}$. Consider the following matrix equation ($A$, $B$, and $c$ are known): $$A=X+c^2\,tr(BX)B$$ Solution of the equation can be obtained by using $vec$ operator and it is clear that it is symmetric. Is it positive definite? It seems the answer is ""yes"", but how can I prove it?",,"['linear-algebra', 'matrices']"
96,Geometric meaning of a vector space,Geometric meaning of a vector space,,"Let $Q = \{(x,y) \in \mathbb{R}^2: x,y > 0 \}$ be a vector space over the real numbers with the operations $$+:Q\times Q \rightarrow Q$$ $$(x_1,y_1)+(x_2,y_2)=(x_1x_2,y_1y_2)$$ and $$\cdot : \mathbb{R} \times Q \rightarrow Q$$ $$c\cdot(x,y)=(x^c,y^c)$$ It's easy to show that this structure is a vector space, but what about the geometrical meaning of these operations? Is there any?","Let $Q = \{(x,y) \in \mathbb{R}^2: x,y > 0 \}$ be a vector space over the real numbers with the operations $$+:Q\times Q \rightarrow Q$$ $$(x_1,y_1)+(x_2,y_2)=(x_1x_2,y_1y_2)$$ and $$\cdot : \mathbb{R} \times Q \rightarrow Q$$ $$c\cdot(x,y)=(x^c,y^c)$$ It's easy to show that this structure is a vector space, but what about the geometrical meaning of these operations? Is there any?",,"['linear-algebra', 'vector-spaces']"
97,$S$ is a $3\times 3$ real matrices. Then $S$ contains which matrices,is a  real matrices. Then  contains which matrices,S 3\times 3 S,"Let , $S$ be the set of $3\times 3$ real matrices $A$ with $$AA^T=\left(\begin{matrix}1&0&0\\0&0&0\\0&0&0\end{matrix}\right).$$Then the set $S$ contains : (A) a nilpotent matrix. (B) a matrix of rank $1$. (C) a matrix of rank $2$. (D) a non-zero skew-symmetric matrix. Attempt : Clearly , $\left(\begin{matrix}1&0&0\\0&0&0\\0&0&0\end{matrix}\right)\in S$. So, (B) is TRUE. Again $rank(AA^T)=rank(A)$. As , $rank(AA^T)=1$ , so $S$ does not contain a matrix of rank $2$. So (C) is FALSE. Again we know the rank of a non-zero skew-symmetric matrix can never be $1$. So , $S$ does not contain a non-zero skew-symmetric matrix. So (D) is FALSE. But I am unable to understand the option (A) whether it is correct or NOT ? Any hint ?","Let , $S$ be the set of $3\times 3$ real matrices $A$ with $$AA^T=\left(\begin{matrix}1&0&0\\0&0&0\\0&0&0\end{matrix}\right).$$Then the set $S$ contains : (A) a nilpotent matrix. (B) a matrix of rank $1$. (C) a matrix of rank $2$. (D) a non-zero skew-symmetric matrix. Attempt : Clearly , $\left(\begin{matrix}1&0&0\\0&0&0\\0&0&0\end{matrix}\right)\in S$. So, (B) is TRUE. Again $rank(AA^T)=rank(A)$. As , $rank(AA^T)=1$ , so $S$ does not contain a matrix of rank $2$. So (C) is FALSE. Again we know the rank of a non-zero skew-symmetric matrix can never be $1$. So , $S$ does not contain a non-zero skew-symmetric matrix. So (D) is FALSE. But I am unable to understand the option (A) whether it is correct or NOT ? Any hint ?",,"['linear-algebra', 'matrices']"
98,Trouble with understanding dual space $V^{*}$.,Trouble with understanding dual space .,V^{*},"I've just began the study of linear functionals and the dual base. And this book I'm reading says the dual space $V^{*}$ may be identified with the space of row vectors. This notion seems very important, but I'm having trouble understanding it. Here is the text: Let $\sigma$ be an element of the dual space $V^{*}$, i.e. a linear   map $\sigma: V \rightarrow K$. Choose a basis for $V$, say the usual   the basis, then $\sigma$ is represented by a matrix $[\sigma]$.   However, such a matrix $[\sigma]$ is a row vector. Also, the map   $\sigma \rightarrow [\sigma]$ is a vector space isomorphism. On the other hand, any row vector $\phi = (a_1, \ldots, a_n)$ defines   a linear functional $\phi: V \rightarrow K$ by \begin{align*} \phi(x_1, \ldots, x_n) = (a_1, \ldots, a_n) \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} \end{align*} or simply $\phi(x_1, \ldots,  x_n) = a_1 x_1 + a_2 x_2 + \ldots + a_n x_n$. The author speaks of the matrixrepresentation $[\sigma]$, but he doesn't really explain it. Why is this matrix a row vector? Also, the second part of the text: is this merely a definition? Why does he claim $\phi(x_1, \ldots, x_n) = a_1 x_1 + \ldots + a_n x_n$? The output of a linear functional is suppose to be a scalar, and not a vector? And this is clearly a linear combination of vectors... Maybe some of the advanced mathematicians here could give me some examples, because I can't get my head around this at the moment.","I've just began the study of linear functionals and the dual base. And this book I'm reading says the dual space $V^{*}$ may be identified with the space of row vectors. This notion seems very important, but I'm having trouble understanding it. Here is the text: Let $\sigma$ be an element of the dual space $V^{*}$, i.e. a linear   map $\sigma: V \rightarrow K$. Choose a basis for $V$, say the usual   the basis, then $\sigma$ is represented by a matrix $[\sigma]$.   However, such a matrix $[\sigma]$ is a row vector. Also, the map   $\sigma \rightarrow [\sigma]$ is a vector space isomorphism. On the other hand, any row vector $\phi = (a_1, \ldots, a_n)$ defines   a linear functional $\phi: V \rightarrow K$ by \begin{align*} \phi(x_1, \ldots, x_n) = (a_1, \ldots, a_n) \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} \end{align*} or simply $\phi(x_1, \ldots,  x_n) = a_1 x_1 + a_2 x_2 + \ldots + a_n x_n$. The author speaks of the matrixrepresentation $[\sigma]$, but he doesn't really explain it. Why is this matrix a row vector? Also, the second part of the text: is this merely a definition? Why does he claim $\phi(x_1, \ldots, x_n) = a_1 x_1 + \ldots + a_n x_n$? The output of a linear functional is suppose to be a scalar, and not a vector? And this is clearly a linear combination of vectors... Maybe some of the advanced mathematicians here could give me some examples, because I can't get my head around this at the moment.",,['linear-algebra']
99,Proof that transpose of Hadamard Matrix is also a Hadamard matrix,Proof that transpose of Hadamard Matrix is also a Hadamard matrix,,"The question is self explaining from the title, but let me elaborate it. In most of the articles/books I've read, fact that the transpose of Hadamard matrix is also a Hadamard matrix is used, but I was not able to find or deduce a proof for it. I can basically state that transposing a matrix will (probably?) not affect its orthogonality, but I would prefer a correct proof. Thank you,","The question is self explaining from the title, but let me elaborate it. In most of the articles/books I've read, fact that the transpose of Hadamard matrix is also a Hadamard matrix is used, but I was not able to find or deduce a proof for it. I can basically state that transposing a matrix will (probably?) not affect its orthogonality, but I would prefer a correct proof. Thank you,",,"['linear-algebra', 'abstract-algebra', 'matrices', 'orthogonality']"
