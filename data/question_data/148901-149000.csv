,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Local integrability of the convolution of a function with a distribuition,Local integrability of the convolution of a function with a distribuition,,"Let $G_n$ be the following distribuitions for $n\geq3$  (for $n=2$ it is just a function) in $\mathbb{R^n}$ (the fundamental solutions of the Laplace equation in $\mathbb{R^n}$ ): $$G_n=\left\{\begin{matrix} \frac{1}{2 \pi} \cdot \log \left \| x \right \|, \; \; \; n=2 \\ \frac{\left \| x \right \|^{2-n}}{(2-n) \sigma_{n-1}}, \; \; \; n \geq 3 \end{matrix}\right.$$ Where $\sigma_{n-1}$ is the surface area of the unit radius $n$-sphere. If f $\in L^1(\mathbb{R^n})$ for $n\geq 3$ and $g(y)=f(y) \cdot \log(\left \| y \right \|) \in L^1(\mathbb{R^2})$ for $n=2$ can I get any hint as to how to show that $U=G_n \ast f$ is locally integrable in $\mathbb{R^n}$ for any $n \geq 2$?","Let $G_n$ be the following distribuitions for $n\geq3$  (for $n=2$ it is just a function) in $\mathbb{R^n}$ (the fundamental solutions of the Laplace equation in $\mathbb{R^n}$ ): $$G_n=\left\{\begin{matrix} \frac{1}{2 \pi} \cdot \log \left \| x \right \|, \; \; \; n=2 \\ \frac{\left \| x \right \|^{2-n}}{(2-n) \sigma_{n-1}}, \; \; \; n \geq 3 \end{matrix}\right.$$ Where $\sigma_{n-1}$ is the surface area of the unit radius $n$-sphere. If f $\in L^1(\mathbb{R^n})$ for $n\geq 3$ and $g(y)=f(y) \cdot \log(\left \| y \right \|) \in L^1(\mathbb{R^2})$ for $n=2$ can I get any hint as to how to show that $U=G_n \ast f$ is locally integrable in $\mathbb{R^n}$ for any $n \geq 2$?",,"['real-analysis', 'analysis', 'distribution-theory']"
1,Closed form of an integral,Closed form of an integral,,Is there a closed form of $$\int\limits_0^1\frac{\arctan(\sqrt{x^2+2})}{(1+x^2)\cdot(\sqrt{x^2+2})}dx \quad \text{?}$$ I just know that $\int\limits_0^1\frac{\arctan(\sqrt{x^2+2})}{(1+x^2)\cdot(\sqrt{x^2+2})}dx = 0.514042...$,Is there a closed form of $$\int\limits_0^1\frac{\arctan(\sqrt{x^2+2})}{(1+x^2)\cdot(\sqrt{x^2+2})}dx \quad \text{?}$$ I just know that $\int\limits_0^1\frac{\arctan(\sqrt{x^2+2})}{(1+x^2)\cdot(\sqrt{x^2+2})}dx = 0.514042...$,,['analysis']
2,alternating series test for $\sum_{n=1}^{\infty}(-1)^n\frac{\sqrt{n}}{n+4}$,alternating series test for,\sum_{n=1}^{\infty}(-1)^n\frac{\sqrt{n}}{n+4},"I must determine if this series converges (using specifically the alternating series test) $$\sum_{n=4}^{\infty}(-1)^n\frac{\sqrt{n}}{n+4}.$$ I know the necessary and sufficient conditions are: The series $\displaystyle\sum_{n=1}^{\infty}(-1)^na_n$ converges if $a_{(n+1)} \leq a_n$ (or if $f'(x) < 0$) and $\displaystyle\lim_{n\to\infty}a_n = 0.$ So first I tried to show the derivative, $\frac{d}{dx}(\frac{\sqrt{x}}{x+4}) < 0$ and after a bunch of algebra got $\displaystyle\frac{2-x}{\sqrt{x^5} + \sqrt{x^3} + 16 \sqrt{x}} < 0$, but I don't know how to demonstrate if that's really true for all $x \geq 1$. Next I tried to show $a_{n+1} \leq a_n\  \forall \ n \geq 1$, or  $\frac{\sqrt{n+1}}{4(n+1)^2 + 1} \leq \frac{\sqrt{n}}{4n^2+1}. \ $ After cross multiplying and some rearranging I get $(4n^2 + 1)\sqrt{n+1} \leq 4n^{\frac{5}{2}} + 8n^\frac{3}{2} + 2\sqrt{n}$, and I don't know how to tell if that's true for all $n \geq 1$. So I'm stumped.  I know there are other tests.  But this is in the alternating series section so either the alternating series test can show it is convergent, or if not then I know it's divergent. But the answer in the back of the book says it's convergent.  I just don't know how to prove it. Any suggestions? n.b.  The original series goes from $n=4\to\infty$ but I don't know if the 4 is a typo or is intended.","I must determine if this series converges (using specifically the alternating series test) $$\sum_{n=4}^{\infty}(-1)^n\frac{\sqrt{n}}{n+4}.$$ I know the necessary and sufficient conditions are: The series $\displaystyle\sum_{n=1}^{\infty}(-1)^na_n$ converges if $a_{(n+1)} \leq a_n$ (or if $f'(x) < 0$) and $\displaystyle\lim_{n\to\infty}a_n = 0.$ So first I tried to show the derivative, $\frac{d}{dx}(\frac{\sqrt{x}}{x+4}) < 0$ and after a bunch of algebra got $\displaystyle\frac{2-x}{\sqrt{x^5} + \sqrt{x^3} + 16 \sqrt{x}} < 0$, but I don't know how to demonstrate if that's really true for all $x \geq 1$. Next I tried to show $a_{n+1} \leq a_n\  \forall \ n \geq 1$, or  $\frac{\sqrt{n+1}}{4(n+1)^2 + 1} \leq \frac{\sqrt{n}}{4n^2+1}. \ $ After cross multiplying and some rearranging I get $(4n^2 + 1)\sqrt{n+1} \leq 4n^{\frac{5}{2}} + 8n^\frac{3}{2} + 2\sqrt{n}$, and I don't know how to tell if that's true for all $n \geq 1$. So I'm stumped.  I know there are other tests.  But this is in the alternating series section so either the alternating series test can show it is convergent, or if not then I know it's divergent. But the answer in the back of the book says it's convergent.  I just don't know how to prove it. Any suggestions? n.b.  The original series goes from $n=4\to\infty$ but I don't know if the 4 is a typo or is intended.",,"['calculus', 'sequences-and-series', 'analysis']"
3,"Proof using Rolle's Theorem to show there is c such that f$^4$(c) = 0, for a < c < b","Proof using Rolle's Theorem to show there is c such that f(c) = 0, for a < c < b",^4,"The question is as follows: Give 3 information: (1) f is a polynomial (thus I claim f is continuous at every point) (2) $f(a) = f'(a) = f''(a) = f'''(a) = 0$ (3) $f(b) = 0$ Goal: use Rolle's Theorem to show that there is c satisfying $a < c < b$ such that $f^4(c) = 0$ Here is my attempt: 1/ Recall Rolle's Theorem: If f is continuous on $[a,b]$ and f is differentiable on $(a,b) $ [ i.e: f'(x) exists in a < x < b ], and $f(a) = f(b)$ Then there is c such that $a < c < b$ and $f'(c) = 0$ 2/ By condition (2) and (3), $f(a) = f(b) = 0.$ So there is k satisfying $a < k < b$ and $f'(k) = 0$ by Rolle's Now $f'(k) = f'(a) = 0$ , then use Rolle's again, there is m satisfying $a < m < k < b$ and $f''(m) = 0$ Continue up to the 3rd derivative, where I should get $f'''(n) = f'''(a) = 0$ where $a < n < m < k < b$ .  Then use Rolle's again, I say there is c satisfying $a < c < n <  m < k < b$ such that $f^4(c) = 0$ .  c definitely satisfies a < c < b, since c < something < b, that ""something"" namely is n, m, k. **Would someone please check my proof for any mistakes? Somehow I feel my proof is a bit too obvious to be true >_<  But since the problem asks me to specifically use Rolle's Theorem, this approach is the first way that I can think of. Thank you in advance ^_^","The question is as follows: Give 3 information: (1) f is a polynomial (thus I claim f is continuous at every point) (2) (3) Goal: use Rolle's Theorem to show that there is c satisfying such that Here is my attempt: 1/ Recall Rolle's Theorem: If f is continuous on and f is differentiable on [ i.e: f'(x) exists in a < x < b ], and Then there is c such that and 2/ By condition (2) and (3), So there is k satisfying and by Rolle's Now , then use Rolle's again, there is m satisfying and Continue up to the 3rd derivative, where I should get where .  Then use Rolle's again, I say there is c satisfying such that .  c definitely satisfies a < c < b, since c < something < b, that ""something"" namely is n, m, k. **Would someone please check my proof for any mistakes? Somehow I feel my proof is a bit too obvious to be true >_<  But since the problem asks me to specifically use Rolle's Theorem, this approach is the first way that I can think of. Thank you in advance ^_^","f(a) = f'(a) = f''(a) = f'''(a) = 0 f(b) = 0 a < c < b f^4(c) = 0 [a,b] (a,b)  f(a) = f(b) a < c < b f'(c) = 0 f(a) = f(b) = 0. a < k < b f'(k) = 0 f'(k) = f'(a) = 0 a < m < k < b f''(m) = 0 f'''(n) = f'''(a) = 0 a < n < m < k < b a < c < n <  m < k < b f^4(c) = 0",['analysis']
4,What is the *standard duality argument?,What is the *standard duality argument?,,What is the standard duality argument? I saw this foor exemplo in the following statement. The case $p < 2$ follows from the standard duality argument. To prove Theorem: [Calderón Zigmund] If $u$ is a solution of   \begin{equation} \Delta u = f \quad \mbox{in} \quad B_2 \end{equation}   then   \begin{equation} \int_{B_2} | D^2u|^p \le \Bigl(\int_{B_2} |f|^p + \int_{B_2} |u|^p \Bigr) \quad \mbox{for any} 1<p<  + \infty. \end{equation},What is the standard duality argument? I saw this foor exemplo in the following statement. The case $p < 2$ follows from the standard duality argument. To prove Theorem: [Calderón Zigmund] If $u$ is a solution of   \begin{equation} \Delta u = f \quad \mbox{in} \quad B_2 \end{equation}   then   \begin{equation} \int_{B_2} | D^2u|^p \le \Bigl(\int_{B_2} |f|^p + \int_{B_2} |u|^p \Bigr) \quad \mbox{for any} 1<p<  + \infty. \end{equation},,"['analysis', 'partial-differential-equations']"
5,General solution for the Eikonal equation $| \nabla u|^2=1$,General solution for the Eikonal equation,| \nabla u|^2=1,"Does there exist a formula for the general solution of the Eikonal equation? $| \nabla u|^2=1$. I'm looking for something similar to ""the general solution of $\dfrac{\partial u}{\partial x}(x,y)=0$ is $u=\varphi(y)$, for an arbitrary function $\varphi$"". That is, the formula should include one arbitrary function. Thank you","Does there exist a formula for the general solution of the Eikonal equation? $| \nabla u|^2=1$. I'm looking for something similar to ""the general solution of $\dfrac{\partial u}{\partial x}(x,y)=0$ is $u=\varphi(y)$, for an arbitrary function $\varphi$"". That is, the formula should include one arbitrary function. Thank you",,"['analysis', 'partial-differential-equations']"
6,Finding a $C^1$ surface inside a convex open set (Rudin chapter 10 problem 29),Finding a  surface inside a convex open set (Rudin chapter 10 problem 29),C^1,"The problem is as follows (with $n>1)$: Let $E \subseteq \mathbb R^n$ be a convex open set, and let $F \subseteq \mathbb R^{n-1}$ be it's projection onto the first $n-1$ coordinates. It is clear that $F$ is a convex open set in $\mathbb R^{n-1}$. I need to prove that there exists a function $\alpha:F \to \mathbb R$ of class $\bf{ C^1}$ , such that  it's graph lies in $E$. In other words, for every $x \in F$, $(x,\alpha(x)) \in E$ The book says that the proof is trivial if, for instance, $E$ is a ball, since $\alpha$ can be chosen constant. I have no idea where to start in the general case. I tried using the inverse/implicit function theorems without success. Thank you.","The problem is as follows (with $n>1)$: Let $E \subseteq \mathbb R^n$ be a convex open set, and let $F \subseteq \mathbb R^{n-1}$ be it's projection onto the first $n-1$ coordinates. It is clear that $F$ is a convex open set in $\mathbb R^{n-1}$. I need to prove that there exists a function $\alpha:F \to \mathbb R$ of class $\bf{ C^1}$ , such that  it's graph lies in $E$. In other words, for every $x \in F$, $(x,\alpha(x)) \in E$ The book says that the proof is trivial if, for instance, $E$ is a ball, since $\alpha$ can be chosen constant. I have no idea where to start in the general case. I tried using the inverse/implicit function theorems without success. Thank you.",,"['real-analysis', 'analysis', 'multivariable-calculus']"
7,How much pure math should a physics/microelectronics person know [closed],How much pure math should a physics/microelectronics person know [closed],,"Closed. This question is off-topic . It is not currently accepting answers. Want to improve this question? Update the question so it's on-topic for Mathematics Stack Exchange. Closed 11 years ago . Improve this question I do condensed matter physics modeling in my phd and I was struck up learning quite an amount of physics. But while having done lot of physics courses, I see that if I learn pure math I would understand lot of things better and also probably my modeling skills also might get better. A math prof asked me to do all courses from basic analysis to differential geometry. But that would take time as I see I am taking time doing rudin(principles of mathematical analysis) . So I am little confused whether am going the right way. Any suggestions?","Closed. This question is off-topic . It is not currently accepting answers. Want to improve this question? Update the question so it's on-topic for Mathematics Stack Exchange. Closed 11 years ago . Improve this question I do condensed matter physics modeling in my phd and I was struck up learning quite an amount of physics. But while having done lot of physics courses, I see that if I learn pure math I would understand lot of things better and also probably my modeling skills also might get better. A math prof asked me to do all courses from basic analysis to differential geometry. But that would take time as I see I am taking time doing rudin(principles of mathematical analysis) . So I am little confused whether am going the right way. Any suggestions?",,['analysis']
8,Some questions about functions of bounded variation: Jordan's theorem,Some questions about functions of bounded variation: Jordan's theorem,,"I was trying to do some of these questions to check my understanding about the topic, but I'm not sure if they're correct. Here are my answers. 1) Suppose $f$ is continuous on $[0,1]$. Must there be a nondegenerate closed subinterval $[a,b]$ of $[0,1]$ for which the restriction of $f$ to $[a,b]$ is of bounded variation? No, because $f(x)$ might be a constant function. In this case, $f(x)$ will not be the difference of two increasing functions...and so will not be of bounded variation. 2) Let $f$ be the Dirichlet function, the characteristic function of the rationals in $[0,1]$. Is $f$ of bounded variation on $[0,1]$? No, because we cannot express it as the difference of two increasing functions. 3) Let $f$ be a step function on $[a,b]$. Find a formula for its total variation. If we partition $[a,b]$ in such a way that for each $[x_{i-1}, x_i]$ (which is part of the partition), $x_{i-1}$ and $x_i$ each correspond to the midpoint of a step. I know this is not a formal answer, but I'm just trying to check my understanding.","I was trying to do some of these questions to check my understanding about the topic, but I'm not sure if they're correct. Here are my answers. 1) Suppose $f$ is continuous on $[0,1]$. Must there be a nondegenerate closed subinterval $[a,b]$ of $[0,1]$ for which the restriction of $f$ to $[a,b]$ is of bounded variation? No, because $f(x)$ might be a constant function. In this case, $f(x)$ will not be the difference of two increasing functions...and so will not be of bounded variation. 2) Let $f$ be the Dirichlet function, the characteristic function of the rationals in $[0,1]$. Is $f$ of bounded variation on $[0,1]$? No, because we cannot express it as the difference of two increasing functions. 3) Let $f$ be a step function on $[a,b]$. Find a formula for its total variation. If we partition $[a,b]$ in such a way that for each $[x_{i-1}, x_i]$ (which is part of the partition), $x_{i-1}$ and $x_i$ each correspond to the midpoint of a step. I know this is not a formal answer, but I'm just trying to check my understanding.",,['real-analysis']
9,lsc function on compact set it attains its maximum minimum?,lsc function on compact set it attains its maximum minimum?,,Is this true if so how to show it? if not true can you give a counter example: A lower semicontinuous function f on a compact set K attaings its minimum on K. A lower semicontinuous function f on a compact set K attains its maximum on K. Thanks a lot!,Is this true if so how to show it? if not true can you give a counter example: A lower semicontinuous function f on a compact set K attaings its minimum on K. A lower semicontinuous function f on a compact set K attains its maximum on K. Thanks a lot!,,"['real-analysis', 'analysis', 'convex-analysis']"
10,why $\exp({1\over-x^2})$ is not real analytic?,why  is not real analytic?,\exp({1\over-x^2}),Real analytic function. Can someone explain why $\exp({1\over-x^2})$ is not real analytic? i have read a book talking about smoothness of functions and it talks about this function is not real analytic but don't quite get it.,Real analytic function. Can someone explain why $\exp({1\over-x^2})$ is not real analytic? i have read a book talking about smoothness of functions and it talks about this function is not real analytic but don't quite get it.,,"['real-analysis', 'analysis']"
11,"baby rudin, chapter 10, (differential forms) theorem 10.27","baby rudin, chapter 10, (differential forms) theorem 10.27",,"I'm having difficulties with the reasoning in the proof of theorem 10.27 (regarding integration over oriented simplexes). say $\sigma=[p_0,p_1,\dots,p_j,\dots,p_k],\bar{\sigma}=[p_j,p_1,\dots,p_0,\dots,p_k]$ ($\bar{\sigma}$ is obtained from $\sigma$ by interchanging $p_0,p_j$) and $\omega=f({\bf x}) dx_{i_1} \wedge dx_{i_2} \wedge \dots \wedge dx_{i_k}$ (the proof will then follow from linearity), I want to show that $\int_{\bar{\sigma}} \omega=-\int_{\sigma} \omega$. I can see why the Jacobians $J_\bar{\sigma},J_\sigma$ have opposite signs (the column argument given in the text), which gives: $\int_\bar{\sigma} \omega=\int_{Q^k}f(\bar \sigma( {\bf u})) J_\bar{\sigma} d {\bf u}=-\int_{Q^k} f(\bar{\sigma}({\bf u})) J_\sigma d {\bf u}$ I only miss the last part, where in the last expression $f(\bar \sigma({\bf u}))$ becomes $f(\sigma( {\bf u}))$. if it matters, both Jacobians are constant, and can be moved outside the integrals. Thanks in advance, Michael","I'm having difficulties with the reasoning in the proof of theorem 10.27 (regarding integration over oriented simplexes). say $\sigma=[p_0,p_1,\dots,p_j,\dots,p_k],\bar{\sigma}=[p_j,p_1,\dots,p_0,\dots,p_k]$ ($\bar{\sigma}$ is obtained from $\sigma$ by interchanging $p_0,p_j$) and $\omega=f({\bf x}) dx_{i_1} \wedge dx_{i_2} \wedge \dots \wedge dx_{i_k}$ (the proof will then follow from linearity), I want to show that $\int_{\bar{\sigma}} \omega=-\int_{\sigma} \omega$. I can see why the Jacobians $J_\bar{\sigma},J_\sigma$ have opposite signs (the column argument given in the text), which gives: $\int_\bar{\sigma} \omega=\int_{Q^k}f(\bar \sigma( {\bf u})) J_\bar{\sigma} d {\bf u}=-\int_{Q^k} f(\bar{\sigma}({\bf u})) J_\sigma d {\bf u}$ I only miss the last part, where in the last expression $f(\bar \sigma({\bf u}))$ becomes $f(\sigma( {\bf u}))$. if it matters, both Jacobians are constant, and can be moved outside the integrals. Thanks in advance, Michael",,"['real-analysis', 'analysis', 'integration', 'differential-forms']"
12,Prove existence positive integers $\epsilon <|h\sqrt{m}-k\sqrt{n}|<2\epsilon$,Prove existence positive integers,\epsilon <|h\sqrt{m}-k\sqrt{n}|<2\epsilon,"Given positive integers $h,k$ and $\epsilon>0$ show that there exist positive integers $m,n$ so that the inequality $\epsilon<|h\sqrt{m}-k\sqrt{n}|<2\epsilon$","Given positive integers $h,k$ and $\epsilon>0$ show that there exist positive integers $m,n$ so that the inequality $\epsilon<|h\sqrt{m}-k\sqrt{n}|<2\epsilon$",,"['real-analysis', 'analysis']"
13,Rudin's Construction of Real Numbers,Rudin's Construction of Real Numbers,,"At the end of chapter 1 of Principles of Mathematical Analysis, Rudin provides a proof of the construction of real numbers. The first step in the proof is to define members of $\mathbb{R}$ that are subsets of $\mathbb {Q}$ known as cuts . Rudin gives the following definition of a cut: $\alpha$ is not empty, and $\alpha \neq \mathbb{Q}$. If $p \in \alpha, q \in \mathbb {Q}$, and $q < p$, then $ q \in \alpha$ If $p \in \alpha$, then $p < r$ for some $r \in \alpha$. The letters $p, q, r, \ldots$ will always denote rational numbers, and $\alpha, \beta, \gamma, \ldots$ will denote cuts. My question is the following: Doesn't the definition of the cut provided lead to the conclusion that each cut contains all rational numbers? In other words, that $\alpha = \mathbb {Q}$ for all $\alpha$? The third property of cuts provided shows that there is no maximal element in a cut. The second property implies that given an element in a cut, all rational numbers less than that element are in the cut. Doesn't this clearly imply that cuts include all the rationals? I was hoping someone could clear this up. Thanks.","At the end of chapter 1 of Principles of Mathematical Analysis, Rudin provides a proof of the construction of real numbers. The first step in the proof is to define members of $\mathbb{R}$ that are subsets of $\mathbb {Q}$ known as cuts . Rudin gives the following definition of a cut: $\alpha$ is not empty, and $\alpha \neq \mathbb{Q}$. If $p \in \alpha, q \in \mathbb {Q}$, and $q < p$, then $ q \in \alpha$ If $p \in \alpha$, then $p < r$ for some $r \in \alpha$. The letters $p, q, r, \ldots$ will always denote rational numbers, and $\alpha, \beta, \gamma, \ldots$ will denote cuts. My question is the following: Doesn't the definition of the cut provided lead to the conclusion that each cut contains all rational numbers? In other words, that $\alpha = \mathbb {Q}$ for all $\alpha$? The third property of cuts provided shows that there is no maximal element in a cut. The second property implies that given an element in a cut, all rational numbers less than that element are in the cut. Doesn't this clearly imply that cuts include all the rationals? I was hoping someone could clear this up. Thanks.",,"['real-analysis', 'analysis']"
14,A function that has a derivative but is not integrable,A function that has a derivative but is not integrable,,"How is it possible that the function $F(x)$ defined by : $$       F(x)=\left\{\begin{array}{ll} x\sqrt{x}\sin\frac{1}{x}, & x> 0 \\          0, & x=0\end{array}\right.  $$ $$       F'(x)=\left\{\begin{array}{ll} \frac{3}{2}\sqrt{x}\sin\frac{1}{x}-\frac{1}{\sqrt{x}}\cos\frac{1}{x}, & x> 0 \\          0, & x=0\end{array}\right.  $$ has a derivative, which  is not Riemann- integrable on any interval $[0,|b|]$, the function is continuous and every continuous function should have a R-Integral?","How is it possible that the function $F(x)$ defined by : $$       F(x)=\left\{\begin{array}{ll} x\sqrt{x}\sin\frac{1}{x}, & x> 0 \\          0, & x=0\end{array}\right.  $$ $$       F'(x)=\left\{\begin{array}{ll} \frac{3}{2}\sqrt{x}\sin\frac{1}{x}-\frac{1}{\sqrt{x}}\cos\frac{1}{x}, & x> 0 \\          0, & x=0\end{array}\right.  $$ has a derivative, which  is not Riemann- integrable on any interval $[0,|b|]$, the function is continuous and every continuous function should have a R-Integral?",,"['real-analysis', 'analysis', 'multivariable-calculus']"
15,Prove that $f$ is uniform continuous,Prove that  is uniform continuous,f,"Suppose $f$ is a continuous real valued function on $[0,+\infty)$ where $\lim_{x\to\infty}f(x)=M$ for some $M\in \mathbb{R}$. Prove that $f$ is uniform continuous. Attempts: Suppose the contrary $f$ is not uniform continuous and hence $\exists \epsilon>0,\forall \delta>0 \exists x,y\in\mathbb{R},s.t. d(x,y)<\delta\implies d(f(x),f(y))\ge\epsilon $. Then i try to show that it is not converging or not continuous but not sure how show it.","Suppose $f$ is a continuous real valued function on $[0,+\infty)$ where $\lim_{x\to\infty}f(x)=M$ for some $M\in \mathbb{R}$. Prove that $f$ is uniform continuous. Attempts: Suppose the contrary $f$ is not uniform continuous and hence $\exists \epsilon>0,\forall \delta>0 \exists x,y\in\mathbb{R},s.t. d(x,y)<\delta\implies d(f(x),f(y))\ge\epsilon $. Then i try to show that it is not converging or not continuous but not sure how show it.",,"['real-analysis', 'analysis']"
16,Proof that Laplacian is surjective $\mathcal{P}^n\to\mathcal{P}^{n-2}$,Proof that Laplacian is surjective,\mathcal{P}^n\to\mathcal{P}^{n-2},"Let $\mathcal{P}^n$ denote the vector space of homogeneous polynomials on $\mathbb{R}^3$ of degree $n$. I need to prove that $\Delta|_{\mathcal{P}^n}:\mathcal{P}_n\to\mathcal{P}_{n-2}$, for $n\geq2$ is surjective, where $\Delta$ is the Laplace operator. The hint says that the proof should be done by inductive argument: in the inductive step I should conclude from the formula $$\Delta(x_1^{q_1}x_2^{q_2}x_3^{q_3})=q_1(q_1-1)x_1^{q_1-2}x_2^{q_2}x_3^{q_3}+q_2(q_2-1)x_1^{q_1}x_2^{q_2-2}x_3^{q_3}+q_3(q_3-1)x_1^{q_1}x_2^{q_2}x_3^{q_3-2}$$ that surjectivity of $\Delta|_{\mathcal{P}^n}:\mathcal{P}_n\to\mathcal{P}_{n-2}$ implies surjectivity of $\Delta|_{\mathcal{P}^{n+2}}:\mathcal{P}_{n+2}\to\mathcal{P}_n$. I've tried things and things and I simply don't see how to do this. Help!","Let $\mathcal{P}^n$ denote the vector space of homogeneous polynomials on $\mathbb{R}^3$ of degree $n$. I need to prove that $\Delta|_{\mathcal{P}^n}:\mathcal{P}_n\to\mathcal{P}_{n-2}$, for $n\geq2$ is surjective, where $\Delta$ is the Laplace operator. The hint says that the proof should be done by inductive argument: in the inductive step I should conclude from the formula $$\Delta(x_1^{q_1}x_2^{q_2}x_3^{q_3})=q_1(q_1-1)x_1^{q_1-2}x_2^{q_2}x_3^{q_3}+q_2(q_2-1)x_1^{q_1}x_2^{q_2-2}x_3^{q_3}+q_3(q_3-1)x_1^{q_1}x_2^{q_2}x_3^{q_3-2}$$ that surjectivity of $\Delta|_{\mathcal{P}^n}:\mathcal{P}_n\to\mathcal{P}_{n-2}$ implies surjectivity of $\Delta|_{\mathcal{P}^{n+2}}:\mathcal{P}_{n+2}\to\mathcal{P}_n$. I've tried things and things and I simply don't see how to do this. Help!",,['analysis']
17,Upper bound for the absolute value of an inner product,Upper bound for the absolute value of an inner product,,"I am trying to prove the inequality $$  \left|\sum\limits_{i=1}^n a_{i}x_{i} \right|  \leq \frac{1}{2}(x_{(n)} - x_{(1)}) \sum\limits_{i=1}^n \left| a_{i} \right| \>,$$ where $x_{(n)} = \max_i x_i$ and $x_{(1)} = \min_i x_i$, subject to the condition $\sum_i a_i = 0$. I've tried squaring and applying Samuelson's inequality to bound the distance between any particular observation and the sample mean, but am making very little headway. I also don't quite understand what's going on with the linear combination of observations out front. Can you guys point me in the right direction on how to get started with this thing?","I am trying to prove the inequality $$  \left|\sum\limits_{i=1}^n a_{i}x_{i} \right|  \leq \frac{1}{2}(x_{(n)} - x_{(1)}) \sum\limits_{i=1}^n \left| a_{i} \right| \>,$$ where $x_{(n)} = \max_i x_i$ and $x_{(1)} = \min_i x_i$, subject to the condition $\sum_i a_i = 0$. I've tried squaring and applying Samuelson's inequality to bound the distance between any particular observation and the sample mean, but am making very little headway. I also don't quite understand what's going on with the linear combination of observations out front. Can you guys point me in the right direction on how to get started with this thing?",,"['sequences-and-series', 'analysis']"
18,"Given a real valued $C^1$ function $f$, show there exists a continuous vector-valued function $F$ with $f(X) = X \cdot F(X)$","Given a real valued  function , show there exists a continuous vector-valued function  with",C^1 f F f(X) = X \cdot F(X),"Assume $f:\mathbb{R}^{n} \rightarrow \mathbb{R}$ is a function with continuous first order partial derivatives such that $f(0)=0$. Show there exists a continuous function $F:\mathbb{R}^{n}\rightarrow \mathbb{R}^{n}$ such that $f(X)=X \cdot F(X)$ on $\mathbb{R}^{n}$. It seems like the function $F(X):=(\int_{0}^{1} (\partial_{j}f)(tX)dt))_{1\leq j \leq n}$ is the right idea, but it doesn't seem to work out. I think I'm missing something...would appreciate any help.","Assume $f:\mathbb{R}^{n} \rightarrow \mathbb{R}$ is a function with continuous first order partial derivatives such that $f(0)=0$. Show there exists a continuous function $F:\mathbb{R}^{n}\rightarrow \mathbb{R}^{n}$ such that $f(X)=X \cdot F(X)$ on $\mathbb{R}^{n}$. It seems like the function $F(X):=(\int_{0}^{1} (\partial_{j}f)(tX)dt))_{1\leq j \leq n}$ is the right idea, but it doesn't seem to work out. I think I'm missing something...would appreciate any help.",,"['analysis', 'multivariable-calculus']"
19,Uniqueness existence for a PDE,Uniqueness existence for a PDE,,"I'm trying to solve a set of exercises in order to prepare myself for a test. This question verses about energy method on partial differential equations and I would like to ask for help on that, and, if possible, a refference on energy methods and maximum principles. I'm a begginer on partial differential equations. Show that there is at most one solution to the problem $$\begin{cases}u_t=\alpha^2u_{xx}+g,\textrm{ in }(0,L)\times(0,\infty)\\ u(0,t)=u(L,t)=0,t\geqslant 0\\ u(x,0)=u_0(x),\textrm{ in }[0,L]\\ u\in C^2([0,L]\times(0,\infty))\cap C([0,L]\times[0,\infty)) \end{cases}$$ if $u$ is a continuous, differentiable by parts, $u_0(0)=u_0(L)=0$, $g\in C((0,L)\times(0,\infty))$, using: (a) the maximum principle; (b) the energy method. Obtain the candidate for a solution, of the form $u(x,t)=\sum_{n=1}^{\infty}c_n(t)\sin\left(\frac{n\pi x}{L}\right)$. Thanks in advance! P.S.: Oscar Niemeyer lives forever!","I'm trying to solve a set of exercises in order to prepare myself for a test. This question verses about energy method on partial differential equations and I would like to ask for help on that, and, if possible, a refference on energy methods and maximum principles. I'm a begginer on partial differential equations. Show that there is at most one solution to the problem $$\begin{cases}u_t=\alpha^2u_{xx}+g,\textrm{ in }(0,L)\times(0,\infty)\\ u(0,t)=u(L,t)=0,t\geqslant 0\\ u(x,0)=u_0(x),\textrm{ in }[0,L]\\ u\in C^2([0,L]\times(0,\infty))\cap C([0,L]\times[0,\infty)) \end{cases}$$ if $u$ is a continuous, differentiable by parts, $u_0(0)=u_0(L)=0$, $g\in C((0,L)\times(0,\infty))$, using: (a) the maximum principle; (b) the energy method. Obtain the candidate for a solution, of the form $u(x,t)=\sum_{n=1}^{\infty}c_n(t)\sin\left(\frac{n\pi x}{L}\right)$. Thanks in advance! P.S.: Oscar Niemeyer lives forever!",,"['analysis', 'partial-differential-equations', 'heat-equation']"
20,Does the limit of a convergent sequence depend on the norm?,Does the limit of a convergent sequence depend on the norm?,,"Let $X$ be a vector space, and $\|\cdot\|_1$ and $\|\cdot\|_2$ two different (non-equivalent) Norms on $X.$ Let $(x_n)\subset X$ be a sequence and $x\in X$ such that $\lim_{n\to\infty}\|x_n-x\|_1=0.$  The question is: If $\lim_{n\to\infty}\|x_n-x\|_2>0$ can you say that the sequence $(x_n)$ does not converge (to any other limit) with respect to $\|\cdot\|_2$?  Proof?, example?  Thanks in advance!","Let $X$ be a vector space, and $\|\cdot\|_1$ and $\|\cdot\|_2$ two different (non-equivalent) Norms on $X.$ Let $(x_n)\subset X$ be a sequence and $x\in X$ such that $\lim_{n\to\infty}\|x_n-x\|_1=0.$  The question is: If $\lim_{n\to\infty}\|x_n-x\|_2>0$ can you say that the sequence $(x_n)$ does not converge (to any other limit) with respect to $\|\cdot\|_2$?  Proof?, example?  Thanks in advance!",,"['analysis', 'convergence-divergence', 'normed-spaces']"
21,Manifold geometry and Non - Euclidean geometry,Manifold geometry and Non - Euclidean geometry,,What is the difference between Manifold geometry and Non-Euclidean geometry; what connection is there between them?,What is the difference between Manifold geometry and Non-Euclidean geometry; what connection is there between them?,,"['analysis', 'geometric-topology']"
22,"If $\sum a_n$ converges, and if {$b_n$} is monotonic and bounded, prove $\sum a_n b_n$ converge.","If  converges, and if {} is monotonic and bounded, prove  converge.",\sum a_n b_n \sum a_n b_n,"I proved this way: Since {$b_n$} is bounded, let $|b_n| \le \alpha$ and $\alpha$ is an upper bound. Since $\sum a_n$ converges, there exists N such that $|\sum_{k=m}^{n} a_k| \le \frac{\epsilon}{\alpha}$ for every N $\le$ n,m. Then $|\sum_{k=m}^{n} a_k \alpha| \le \epsilon$ for every N $\le$ n,m. Now, $|\sum_{k=m}^{n} a_k b_k| \le |\sum_{k=m}^{n} a_k \alpha| \le \epsilon$, thus $\sum a_n b_n$ converges. But the professor said the last inequality is wrong. What is the problem? How to fix it?","I proved this way: Since {$b_n$} is bounded, let $|b_n| \le \alpha$ and $\alpha$ is an upper bound. Since $\sum a_n$ converges, there exists N such that $|\sum_{k=m}^{n} a_k| \le \frac{\epsilon}{\alpha}$ for every N $\le$ n,m. Then $|\sum_{k=m}^{n} a_k \alpha| \le \epsilon$ for every N $\le$ n,m. Now, $|\sum_{k=m}^{n} a_k b_k| \le |\sum_{k=m}^{n} a_k \alpha| \le \epsilon$, thus $\sum a_n b_n$ converges. But the professor said the last inequality is wrong. What is the problem? How to fix it?",,['analysis']
23,Continuity of the orthogonal projection into tangent space.,Continuity of the orthogonal projection into tangent space.,,"Let $\mathcal M \subset \mathbb R^d$ be a smooth manifold, and for each $s \in \mathcal M$ let $T_s[\mathcal M]$ denote the tangent space of $\mathcal M$ at $s$. Also, for each $s \in \mathcal M$ let $P_s$ denote the orthogonal projection of $\mathbb R^d$ into $T_s[\mathcal M]$. What can be said about the continuity properties of $P_s$ in $s$? In particular, I seek that $s$ has a modulus of continuity.","Let $\mathcal M \subset \mathbb R^d$ be a smooth manifold, and for each $s \in \mathcal M$ let $T_s[\mathcal M]$ denote the tangent space of $\mathcal M$ at $s$. Also, for each $s \in \mathcal M$ let $P_s$ denote the orthogonal projection of $\mathbb R^d$ into $T_s[\mathcal M]$. What can be said about the continuity properties of $P_s$ in $s$? In particular, I seek that $s$ has a modulus of continuity.",,"['analysis', 'differential-geometry', 'riemannian-geometry']"
24,When does a measure have a density?,When does a measure have a density?,,"Consider a measure space $(X, \Sigma, \mu)$ and another measure $\nu$ on the same space. I'm interested in the conditions under which $\nu$ can be represented by a density function $f$ on $X$, so for measurable $S$, $\nu(S) = \int_S f \, \text{d}\mu$. A necessary condition is that for any $S$, if $\mu(S) = 0$ then $\nu(S) = 0$. Is this a sufficient condition? If not, what other conditions need to be included?","Consider a measure space $(X, \Sigma, \mu)$ and another measure $\nu$ on the same space. I'm interested in the conditions under which $\nu$ can be represented by a density function $f$ on $X$, so for measurable $S$, $\nu(S) = \int_S f \, \text{d}\mu$. A necessary condition is that for any $S$, if $\mu(S) = 0$ then $\nu(S) = 0$. Is this a sufficient condition? If not, what other conditions need to be included?",,"['analysis', 'measure-theory']"
25,Mean of the terms of convergent sequence [duplicate],Mean of the terms of convergent sequence [duplicate],,"This question already has answers here : Prove convergence of the sequence $(z_1+z_2+\cdots + z_n)/n$ of Cesaro means [duplicate] (3 answers) Closed 10 years ago . Consider a convergent sequence  $a_1,a_2,a_3\cdots a_n$ tending to a limit A. Now consider the sequence $K_1,K_2,K_3 \cdots K_n$ such that $K_n =\cfrac {a_1+a_2+...a_n}n$. Now what I guess is that as an tends to infinity $K_n$ tends to $a_n$.  What I can say is that the sequence $K_n$ is a bounded one. As such it must have a convergent sub sequence. Is this sequence also convergent. I tried to resolve this by first trying to apply squeeze theorem as $K_n$ always lies between the minimum and maximum of that sequence. But I could not meet up rigor or any clear result. Next I tried to apply Cauchy's principle, but here I don't think that one can reach to a strong consequence except few suggestive apprehensions. I hope someone can guide me on this problem with a proof.","This question already has answers here : Prove convergence of the sequence $(z_1+z_2+\cdots + z_n)/n$ of Cesaro means [duplicate] (3 answers) Closed 10 years ago . Consider a convergent sequence  $a_1,a_2,a_3\cdots a_n$ tending to a limit A. Now consider the sequence $K_1,K_2,K_3 \cdots K_n$ such that $K_n =\cfrac {a_1+a_2+...a_n}n$. Now what I guess is that as an tends to infinity $K_n$ tends to $a_n$.  What I can say is that the sequence $K_n$ is a bounded one. As such it must have a convergent sub sequence. Is this sequence also convergent. I tried to resolve this by first trying to apply squeeze theorem as $K_n$ always lies between the minimum and maximum of that sequence. But I could not meet up rigor or any clear result. Next I tried to apply Cauchy's principle, but here I don't think that one can reach to a strong consequence except few suggestive apprehensions. I hope someone can guide me on this problem with a proof.",,"['sequences-and-series', 'analysis']"
26,Continuous mappings pull back closed sets to closed sets,Continuous mappings pull back closed sets to closed sets,,"George F Simmons, Topology and Modern Analysis pg.79 Problem 4 Let $X$ and $Y$ be metric spaces. Show that an into mapping $f:X \rightarrow Y$ is continuous $\iff$ $f^{-1}\left(G\right)$ is closed in $X$ whenever $G$ is closed in $Y$. I can prove the problem for open sets, and I have been trying hard for closed. However, seems like I am stuck somewhere missing something obvious. Please don't answer directly, just give a small hint if possible. EDIT : I am using the definition that $f^{-1}\left(G\right)$ exists only when $f$ is onto and if it is not then $f^{−1}\left(G\right)$ is a loose term for $f^{-1}\left(H\right)$ where $H$ is the range of $f$ in $G$.","George F Simmons, Topology and Modern Analysis pg.79 Problem 4 Let $X$ and $Y$ be metric spaces. Show that an into mapping $f:X \rightarrow Y$ is continuous $\iff$ $f^{-1}\left(G\right)$ is closed in $X$ whenever $G$ is closed in $Y$. I can prove the problem for open sets, and I have been trying hard for closed. However, seems like I am stuck somewhere missing something obvious. Please don't answer directly, just give a small hint if possible. EDIT : I am using the definition that $f^{-1}\left(G\right)$ exists only when $f$ is onto and if it is not then $f^{−1}\left(G\right)$ is a loose term for $f^{-1}\left(H\right)$ where $H$ is the range of $f$ in $G$.",,['analysis']
27,"Explanation of statements in paper required (pde, calculus, analysis)","Explanation of statements in paper required (pde, calculus, analysis)",,"(I am reading a paper called Shortening Complete Plane Curves by Kai-Seng Chou & Xi-Ping Zhu. It is available at Link . Page 476 is relevant) Consider the partial differential equation on the domain $\mathbb{R}\times [0,T]$ : $$u_t - A(x,t)u_{xx} + \text{l.o.t} = f$$ where $$A(x,t) = \frac{(1-k_0(x)^2t)^2}{[(1-k_0(x)^2t)^2 + (k_{0_x}(x)t)^2]^2}$$ where $k_0(x)$ is the curvature of the curve $\gamma_0:\mathbb{R} \to \mathbb{R}^2$ . Recall that a PDE is uniformly parabolic if there exist positive constants $a$ and $b$ such that the term in the front of the Laplacian sits between $a$ and $b$ , i.e., $a \leq A(x,t) \leq b$ . Questions: Why is it true that $A$ is bounded in $C^{k, \alpha}(\mathbb{R} \times [0,T])$ if $\gamma_0 \in C^{k+4, \alpha}(\mathbb{R})$ ? Something to do with the fact that $k_0$ depends on $(\gamma_0)_{xx}$ ? Why is it true that If we restrict $T$ so that, for example, $$T < \frac{1}{2}\inf_x \frac{1}{1+k_0^2(x)},$$ then the PDE is uniformly parabolic. I don't see where that comes from at all. Thanks for any help.","(I am reading a paper called Shortening Complete Plane Curves by Kai-Seng Chou & Xi-Ping Zhu. It is available at Link . Page 476 is relevant) Consider the partial differential equation on the domain : where where is the curvature of the curve . Recall that a PDE is uniformly parabolic if there exist positive constants and such that the term in the front of the Laplacian sits between and , i.e., . Questions: Why is it true that is bounded in if ? Something to do with the fact that depends on ? Why is it true that If we restrict so that, for example, then the PDE is uniformly parabolic. I don't see where that comes from at all. Thanks for any help.","\mathbb{R}\times [0,T] u_t - A(x,t)u_{xx} + \text{l.o.t} = f A(x,t) = \frac{(1-k_0(x)^2t)^2}{[(1-k_0(x)^2t)^2 + (k_{0_x}(x)t)^2]^2} k_0(x) \gamma_0:\mathbb{R} \to \mathbb{R}^2 a b a b a \leq A(x,t) \leq b A C^{k, \alpha}(\mathbb{R} \times [0,T]) \gamma_0 \in C^{k+4, \alpha}(\mathbb{R}) k_0 (\gamma_0)_{xx} T T < \frac{1}{2}\inf_x \frac{1}{1+k_0^2(x)},","['analysis', 'partial-differential-equations']"
28,Exercise about expressing grad f(x) in another basis.,Exercise about expressing grad f(x) in another basis.,,"I'm having difficulties with this exercise (from Elon LIMA's Curso de Análise , Vol. 2): $f:U\longrightarrow\mathbb{R}$ is function, differentiable on the open set $U\subset\mathbb{R}^n$. Let $\{v_1,...,v_n\}$ be an arbitrary basis of $\mathbb{R}^n$ and $g^{ij}:=\left<v_i,v_j\right>$. Show that grad$f(x)$ in this basis is $$\textrm{grad} f(x) = \sum_i(\sum_j g^{ij}\frac{\partial f}{\partial v_j})v_i$$ where $\frac{\partial f}{\partial v_j}$ is the directional derivative of $f$ along the vector $v_j$. I tried starting with the RHS with $\frac{\partial f}{\partial v_j}=\left<\textrm{grad}f(x),v_j\right>$ and arriving at $\sum_i \left<\textrm{grad}f(x),e_i\right>e_i $ which is the ""canonical"" (see Note below) expression for the grad; writing each $v_i$ as $v_i=\sum_j \left<v_i,e_j\right>$ I come down to a sum of the form $\sum_{i,j}\left<v_j,e_k\right>g^{ji}\left<v_i,e_l\right>$ which I don't recognize and doesn't seem to simplify. Am I approaching it incorrectly? How must this problem be tackled? I'm obviously missing something, or misinterpreting things. NOTE: In the textbook, grad$f(x)$ is defined as $\sum_i \frac{\partial f}{\partial x_i}(x)e_i$ where $\frac{\partial f}{\partial x_i}(x)$ is the usual i -th partial derivative at the point $x$.","I'm having difficulties with this exercise (from Elon LIMA's Curso de Análise , Vol. 2): $f:U\longrightarrow\mathbb{R}$ is function, differentiable on the open set $U\subset\mathbb{R}^n$. Let $\{v_1,...,v_n\}$ be an arbitrary basis of $\mathbb{R}^n$ and $g^{ij}:=\left<v_i,v_j\right>$. Show that grad$f(x)$ in this basis is $$\textrm{grad} f(x) = \sum_i(\sum_j g^{ij}\frac{\partial f}{\partial v_j})v_i$$ where $\frac{\partial f}{\partial v_j}$ is the directional derivative of $f$ along the vector $v_j$. I tried starting with the RHS with $\frac{\partial f}{\partial v_j}=\left<\textrm{grad}f(x),v_j\right>$ and arriving at $\sum_i \left<\textrm{grad}f(x),e_i\right>e_i $ which is the ""canonical"" (see Note below) expression for the grad; writing each $v_i$ as $v_i=\sum_j \left<v_i,e_j\right>$ I come down to a sum of the form $\sum_{i,j}\left<v_j,e_k\right>g^{ji}\left<v_i,e_l\right>$ which I don't recognize and doesn't seem to simplify. Am I approaching it incorrectly? How must this problem be tackled? I'm obviously missing something, or misinterpreting things. NOTE: In the textbook, grad$f(x)$ is defined as $\sum_i \frac{\partial f}{\partial x_i}(x)e_i$ where $\frac{\partial f}{\partial x_i}(x)$ is the usual i -th partial derivative at the point $x$.",,"['calculus', 'linear-algebra', 'real-analysis', 'analysis', 'multivariable-calculus']"
29,Extending the Hardy Littlewood Maximal Function Estimate to the Case $1<p<\infty$,Extending the Hardy Littlewood Maximal Function Estimate to the Case,1<p<\infty,"This question pertains to the extension of the weak-type estimate of the maximal function in $L^{1}$ to a ""sharp"" estimate in $L^{p}$ for $1<p<\infty$ (the case $p=\infty$ is evidently trivial). If $f$ is integrable on $\mathbb{R}^{d}$, then the Hardy-Littlewood Maximal function $f$ is defined to be: \begin{equation*} f^{*}(x)=\sup_{x\in B}\frac{1}{m(B)}\int_{B}|f(y)|dy, \end{equation*} where the supremum is taken over all balls containing $x$ (it can be interpreted as a sort of maximal average of $f$). The well-known weak-type estimate for $f^{*}$ in $L^{1}$ is given by \begin{equation*} m(\{x:f^{*}(x)>\alpha\})\leq\frac{A}{\alpha}||f||_{L^{1}(\mathbb{R}^{d})} \end{equation*} where $A$ depends only on $d$ (taking $A=3^{d}$ is sufficient for the proof). It is well-known that this is the best estimate one can hope for, for despite the estimate showing $f^{*}$ is not too much bigger than $f$, it is nevertheless true that in general $f\in L^{1}$ does not imply $f^{*}\in L^{1}$. Anyway, from this exercise, apparently we can modify the situation slightly to obtain a positive result on the integrability of $f^{*}$. Let $f$ be a measurable function on $\mathbb{R}^{d}$ such that: \begin{equation*} ||f||_{L^{p}(\mathbb{R}^{d})}=\int_{\mathbb{R}^{d}}|f(x)|^{p}dx<\infty. \end{equation*} Then the task is to prove the following inequality: \begin{equation*} ||f^{*}||_{L^{p}(\mathbb{R}^{d})}\leq C||f||_{L^{p}(\mathbb{R}^{d})}, \end{equation*} where $C$ depends only on $p$ and $d$. There are some preliminary results which I proved which make it possible to prove this without any advanced ""machinery"" in functional analysis: \begin{equation*} \text{(1) } \int_{\mathbb{R}^{d}}|f(x)|^{p}dx=\int\limits_{0}^{\infty}p\alpha^{p-1}m(\alpha)d\alpha, \end{equation*} \begin{equation*} \text{(2) } m^{*}(\alpha)=m\left(\{x:f^{*}(x)>\alpha\}\right)\leq\frac{2A}{\alpha}\int_{\{x:|f(x)>\frac{\alpha}{2}\}}|f(x)dx. \end{equation*} In the above results $A$ is a constant depending on only $d$, $1<p<\infty$, $\alpha>0$ and $m(\alpha)$ is the same as in LHS of $(2)$ except for $f$. These results I've proved, and using them I compute the following to prove the statement: \begin{align*} ||f^{\star}||_{L^{p}(\mathbb{R}^{d})} &=\int_{\mathbb{R}^{d}}|f^{*}(x)|^{p}dx\\ &=\int_{\mathbb{R}^{d}}\int_{0}^{|f^{*}(x)|^{p}}p\alpha^{p-1}d\alpha dx\\ &=\int_{0}^{\infty}p\alpha^{p-1}m^{\star}(\alpha)d\alpha\\ &\leq\int_{0}^{\infty}p\alpha^{p-1}\left(\frac{2A}{\alpha}\int_{\{x:|f(x)|>\frac{\alpha}{2}\}}|f(x)|dx\right)d\alpha\\ &=2Ap\int_{0}^{\infty}\alpha^{p-2}\left(\int_{\{x:|f(x)|>\frac{\alpha}{2}\}}|f(x)|dx\right)d\alpha\\ &=2Ap\int_{0}^{\infty}\int_{\{x:|f(x)|>\frac{\alpha}{2}\}}\alpha^{p-2}|f(x)|dxd\alpha\\ &=\ldots\\ &=C\int_{\mathbb{R}^{d}}|f(x)|^{p}dx\\ &=C||f||_{L^{p}(\mathbb{R}^{d})}. \end{align*} Unfortunately, I am having trouble filling in the $\ldots$ to obtain the conclusion.  I've tried messing rewriting expressions in terms of the previous results and working backwards, but I can't get things to work out.","This question pertains to the extension of the weak-type estimate of the maximal function in $L^{1}$ to a ""sharp"" estimate in $L^{p}$ for $1<p<\infty$ (the case $p=\infty$ is evidently trivial). If $f$ is integrable on $\mathbb{R}^{d}$, then the Hardy-Littlewood Maximal function $f$ is defined to be: \begin{equation*} f^{*}(x)=\sup_{x\in B}\frac{1}{m(B)}\int_{B}|f(y)|dy, \end{equation*} where the supremum is taken over all balls containing $x$ (it can be interpreted as a sort of maximal average of $f$). The well-known weak-type estimate for $f^{*}$ in $L^{1}$ is given by \begin{equation*} m(\{x:f^{*}(x)>\alpha\})\leq\frac{A}{\alpha}||f||_{L^{1}(\mathbb{R}^{d})} \end{equation*} where $A$ depends only on $d$ (taking $A=3^{d}$ is sufficient for the proof). It is well-known that this is the best estimate one can hope for, for despite the estimate showing $f^{*}$ is not too much bigger than $f$, it is nevertheless true that in general $f\in L^{1}$ does not imply $f^{*}\in L^{1}$. Anyway, from this exercise, apparently we can modify the situation slightly to obtain a positive result on the integrability of $f^{*}$. Let $f$ be a measurable function on $\mathbb{R}^{d}$ such that: \begin{equation*} ||f||_{L^{p}(\mathbb{R}^{d})}=\int_{\mathbb{R}^{d}}|f(x)|^{p}dx<\infty. \end{equation*} Then the task is to prove the following inequality: \begin{equation*} ||f^{*}||_{L^{p}(\mathbb{R}^{d})}\leq C||f||_{L^{p}(\mathbb{R}^{d})}, \end{equation*} where $C$ depends only on $p$ and $d$. There are some preliminary results which I proved which make it possible to prove this without any advanced ""machinery"" in functional analysis: \begin{equation*} \text{(1) } \int_{\mathbb{R}^{d}}|f(x)|^{p}dx=\int\limits_{0}^{\infty}p\alpha^{p-1}m(\alpha)d\alpha, \end{equation*} \begin{equation*} \text{(2) } m^{*}(\alpha)=m\left(\{x:f^{*}(x)>\alpha\}\right)\leq\frac{2A}{\alpha}\int_{\{x:|f(x)>\frac{\alpha}{2}\}}|f(x)dx. \end{equation*} In the above results $A$ is a constant depending on only $d$, $1<p<\infty$, $\alpha>0$ and $m(\alpha)$ is the same as in LHS of $(2)$ except for $f$. These results I've proved, and using them I compute the following to prove the statement: \begin{align*} ||f^{\star}||_{L^{p}(\mathbb{R}^{d})} &=\int_{\mathbb{R}^{d}}|f^{*}(x)|^{p}dx\\ &=\int_{\mathbb{R}^{d}}\int_{0}^{|f^{*}(x)|^{p}}p\alpha^{p-1}d\alpha dx\\ &=\int_{0}^{\infty}p\alpha^{p-1}m^{\star}(\alpha)d\alpha\\ &\leq\int_{0}^{\infty}p\alpha^{p-1}\left(\frac{2A}{\alpha}\int_{\{x:|f(x)|>\frac{\alpha}{2}\}}|f(x)|dx\right)d\alpha\\ &=2Ap\int_{0}^{\infty}\alpha^{p-2}\left(\int_{\{x:|f(x)|>\frac{\alpha}{2}\}}|f(x)|dx\right)d\alpha\\ &=2Ap\int_{0}^{\infty}\int_{\{x:|f(x)|>\frac{\alpha}{2}\}}\alpha^{p-2}|f(x)|dxd\alpha\\ &=\ldots\\ &=C\int_{\mathbb{R}^{d}}|f(x)|^{p}dx\\ &=C||f||_{L^{p}(\mathbb{R}^{d})}. \end{align*} Unfortunately, I am having trouble filling in the $\ldots$ to obtain the conclusion.  I've tried messing rewriting expressions in terms of the previous results and working backwards, but I can't get things to work out.",,"['analysis', 'integration']"
30,How to find the function $f$ using this equation?,How to find the function  using this equation?,f,Assume that we have a function $f:\mathbb{R}^+\rightarrow\mathbb{R}^+$ and $D$ a constant. How can we solve the following equation for $f$: $$ \int^{b}_{x_2=a} \int^{b}_{x_1=a} \frac{1}{(f(x_1)+f(x_2))^2}d x_1 d x_2 = D  \log(\tfrac{b}{a})$$ where $0<a<b$. Thanks.,Assume that we have a function $f:\mathbb{R}^+\rightarrow\mathbb{R}^+$ and $D$ a constant. How can we solve the following equation for $f$: $$ \int^{b}_{x_2=a} \int^{b}_{x_1=a} \frac{1}{(f(x_1)+f(x_2))^2}d x_1 d x_2 = D  \log(\tfrac{b}{a})$$ where $0<a<b$. Thanks.,,"['analysis', 'integration']"
31,Does $f_{n}(x)=(1+x^{2n})^{1/2n}$ converge uniformly on $\mathbb{R}$.,Does  converge uniformly on .,f_{n}(x)=(1+x^{2n})^{1/2n} \mathbb{R},Does the sequence of functions defined by $f_{n}(x)=(1+x^{2n})^{1/2n}$ converge uniformly on $\mathbb{R}$. For testing uniform convergence i know if the sequence $x_{n} = \sup \: \{ |f_{n}(x)-f(x) | : x \in \mathbb{R}\}$ converges to $0$ then $f_{n} \to f$ uniformly. But I don't know how to actually apply this result.,Does the sequence of functions defined by $f_{n}(x)=(1+x^{2n})^{1/2n}$ converge uniformly on $\mathbb{R}$. For testing uniform convergence i know if the sequence $x_{n} = \sup \: \{ |f_{n}(x)-f(x) | : x \in \mathbb{R}\}$ converges to $0$ then $f_{n} \to f$ uniformly. But I don't know how to actually apply this result.,,"['real-analysis', 'analysis']"
32,Weighted average vs. weighted mean,Weighted average vs. weighted mean,,"Is there a formal difference between weighted average and weighted mean ? I get corrected to the latter if I type in the former in wikipedia, and then there is a lot of stuff about the name ""average"" so I'm not sure about anything anymore. As a small bonus question: Logically, what is the field which introduces weighted averages? To associate it with measure theory seems a little over the top.","Is there a formal difference between weighted average and weighted mean ? I get corrected to the latter if I type in the former in wikipedia, and then there is a lot of stuff about the name ""average"" so I'm not sure about anything anymore. As a small bonus question: Logically, what is the field which introduces weighted averages? To associate it with measure theory seems a little over the top.",,"['analysis', 'statistics', 'measure-theory', 'notation', 'average']"
33,Pointwise convergence counter example.,Pointwise convergence counter example.,,"Can anyone think of a counter example that for $f_n:[a,b] \to \mathbb{R}$ regulated and $f_n \to f$ pointwise but $f$ is not a regulated function? Thanks!","Can anyone think of a counter example that for $f_n:[a,b] \to \mathbb{R}$ regulated and $f_n \to f$ pointwise but $f$ is not a regulated function? Thanks!",,[]
34,Dilogarithm asymptotics for an exponential parameter.,Dilogarithm asymptotics for an exponential parameter.,,So this question is about this dilogarithm function. Assume the argument $z$ is real then I want to show the formula $$\operatorname{Li}_2(e^{-z})=\frac{\pi^2}{6} + z\log z -z+O(z^2) $$ as $z$ approaches $0$ from positive value How can I show this?,So this question is about this dilogarithm function. Assume the argument $z$ is real then I want to show the formula $$\operatorname{Li}_2(e^{-z})=\frac{\pi^2}{6} + z\log z -z+O(z^2) $$ as $z$ approaches $0$ from positive value How can I show this?,,"['analysis', 'special-functions', 'asymptotics']"
35,Combining Taylor expansions,Combining Taylor expansions,,"How do you taylor expand the function $F(x)={x\over \ln(x+1)}$ using standard results? (I know that WA offers the answer, but I want to know how to get it myself.) I know that $\ln(x+1)=x-{x^2\over 2}+{x^3\over 3}+…$ But I don't know how to take the reciprocal. In general, given a function $g(x)$ with a known Taylor series, how might I find $(g(x))^n$, for some $n\in \mathbb Q$? Also, how might I evaluate expressions like $\ln(1+g(x))$ where I know the Taylor expansion of $g(x)$ (and $\ln x$). How do I combine them? Thank you.","How do you taylor expand the function $F(x)={x\over \ln(x+1)}$ using standard results? (I know that WA offers the answer, but I want to know how to get it myself.) I know that $\ln(x+1)=x-{x^2\over 2}+{x^3\over 3}+…$ But I don't know how to take the reciprocal. In general, given a function $g(x)$ with a known Taylor series, how might I find $(g(x))^n$, for some $n\in \mathbb Q$? Also, how might I evaluate expressions like $\ln(1+g(x))$ where I know the Taylor expansion of $g(x)$ (and $\ln x$). How do I combine them? Thank you.",,"['analysis', 'taylor-expansion']"
36,Polynomial interpolation,Polynomial interpolation,,"Let $P=[a,b]\times (c,d)$. Assume that we have given  $n$ points $(x_1,y_1),...,(x_n,y_n)\in P$, such that $x_i\neq x_j$ for $i\neq j$; $i,j=1,...,n$. Does there exist a polynomial $f$ such that $f(x_i)=y_i$ for $i=1,...,n$ and $c<f(x)<d$ for $x \in [a,b]$?","Let $P=[a,b]\times (c,d)$. Assume that we have given  $n$ points $(x_1,y_1),...,(x_n,y_n)\in P$, such that $x_i\neq x_j$ for $i\neq j$; $i,j=1,...,n$. Does there exist a polynomial $f$ such that $f(x_i)=y_i$ for $i=1,...,n$ and $c<f(x)<d$ for $x \in [a,b]$?",,"['analysis', 'interpolation']"
37,Solving integral equation with Laplace's Transform.,Solving integral equation with Laplace's Transform.,,"I'm trying to prove the following $$\int\limits_0^\infty  {\frac{{\cos tu}}{{{u^2} + 1}}\log udu}  =  - \frac{\pi }{2}\int\limits_0^\infty  {\frac{{\sin tu}}{{{u^2} + 1}}du} $$ The original problem suggested the use of the Laplace Transform, thus I have $$\int\limits_0^\infty  {\frac{s}{{{u^2} + {s^2}}}\frac{{\log u}}{{{u^2} + 1}}du = }  - \frac{\pi }{2}\int\limits_0^\infty  {\frac{u}{{{s^2} + {u^2}}}\frac{{du}}{{{u^2} + 1}}}$$ ADD The RHS transform can be evaluated as follows: $$- \frac{\pi }{2}\int\limits_0^\infty  {\frac{u}{{{s^2} + {u^2}}}\frac{{du}}{{{u^2} + 1}}}$$ $$ - \frac{\pi }{4}\int\limits_0^\infty  {\frac{{dm}}{{{s^2} + m}}\frac{1}{{m + 1}}} $$ Now by partial fractions you can separate and get: $$ - \frac{\pi }{4}\int\limits_0^\infty  {\frac{{dm}}{{{s^2} + m}}\frac{1}{{m + 1}}}  =  - \frac{\pi }{4}\frac{1}{{{s^2} - 1}}\left[ {\log \left( {\frac{{m + 1}}{{m + {s^2}}}} \right)} \right]_0^\infty $$ $$ - \frac{\pi }{4}\int\limits_0^\infty  {\frac{{dm}}{{{s^2} + m}}\frac{1}{{m + 1}}}  = -\frac{\pi }{2}\frac{{\log s}}{{{s^2} - 1}}$$ For the second one I make a similar manipulation: $$\frac{1}{{{s^2} + {u^2}}}\frac{1}{{{u^2} + 1}} = \frac{1}{{{s^2} - 1}}\left( {\frac{1}{{{u^2} + 1}} - \frac{1}{{{u^2} + {s^2}}}} \right)$$ $$\frac{s}{{{s^2} - 1}}\int\limits_0^\infty  {\left( {\frac{{\log u}}{{{u^2} + 1}} - \frac{{\log u}}{{{u^2} + {s^2}}}} \right)du} $$ Substitution to show integral over $\mathbb{R}$ of an odd function is zero so, $$\frac{s}{{{s^2} - 1}}\left( {\int\limits_{ - \infty }^\infty  {\frac{{x{e^x}}}{{{e^{2x}} + 1}}dx}  - \int\limits_0^\infty  {\frac{{\log u}}{{{u^2} + {s^2}}}} }du \right)$$ $$ - \frac{s}{{{s^2} - 1}}\int\limits_0^\infty  {\frac{{\log u}}{{{u^2} + {s^2}}}}du $$ And again a suitable $u = m s$ produces $$ - \frac{1}{{{s^2} - 1}}\int\limits_0^\infty  {\frac{{\log m + \log s}}{{{m^2} + 1}}} dm =  - \frac{{\log s}}{{{s^2} - 1}}\int\limits_0^\infty  {\frac{1}{{{m^2} + 1}}} dm =  - \frac{\pi }{2}\frac{{\log s}}{{{s^2} - 1}}$$","I'm trying to prove the following $$\int\limits_0^\infty  {\frac{{\cos tu}}{{{u^2} + 1}}\log udu}  =  - \frac{\pi }{2}\int\limits_0^\infty  {\frac{{\sin tu}}{{{u^2} + 1}}du} $$ The original problem suggested the use of the Laplace Transform, thus I have $$\int\limits_0^\infty  {\frac{s}{{{u^2} + {s^2}}}\frac{{\log u}}{{{u^2} + 1}}du = }  - \frac{\pi }{2}\int\limits_0^\infty  {\frac{u}{{{s^2} + {u^2}}}\frac{{du}}{{{u^2} + 1}}}$$ ADD The RHS transform can be evaluated as follows: $$- \frac{\pi }{2}\int\limits_0^\infty  {\frac{u}{{{s^2} + {u^2}}}\frac{{du}}{{{u^2} + 1}}}$$ $$ - \frac{\pi }{4}\int\limits_0^\infty  {\frac{{dm}}{{{s^2} + m}}\frac{1}{{m + 1}}} $$ Now by partial fractions you can separate and get: $$ - \frac{\pi }{4}\int\limits_0^\infty  {\frac{{dm}}{{{s^2} + m}}\frac{1}{{m + 1}}}  =  - \frac{\pi }{4}\frac{1}{{{s^2} - 1}}\left[ {\log \left( {\frac{{m + 1}}{{m + {s^2}}}} \right)} \right]_0^\infty $$ $$ - \frac{\pi }{4}\int\limits_0^\infty  {\frac{{dm}}{{{s^2} + m}}\frac{1}{{m + 1}}}  = -\frac{\pi }{2}\frac{{\log s}}{{{s^2} - 1}}$$ For the second one I make a similar manipulation: $$\frac{1}{{{s^2} + {u^2}}}\frac{1}{{{u^2} + 1}} = \frac{1}{{{s^2} - 1}}\left( {\frac{1}{{{u^2} + 1}} - \frac{1}{{{u^2} + {s^2}}}} \right)$$ $$\frac{s}{{{s^2} - 1}}\int\limits_0^\infty  {\left( {\frac{{\log u}}{{{u^2} + 1}} - \frac{{\log u}}{{{u^2} + {s^2}}}} \right)du} $$ Substitution to show integral over $\mathbb{R}$ of an odd function is zero so, $$\frac{s}{{{s^2} - 1}}\left( {\int\limits_{ - \infty }^\infty  {\frac{{x{e^x}}}{{{e^{2x}} + 1}}dx}  - \int\limits_0^\infty  {\frac{{\log u}}{{{u^2} + {s^2}}}} }du \right)$$ $$ - \frac{s}{{{s^2} - 1}}\int\limits_0^\infty  {\frac{{\log u}}{{{u^2} + {s^2}}}}du $$ And again a suitable $u = m s$ produces $$ - \frac{1}{{{s^2} - 1}}\int\limits_0^\infty  {\frac{{\log m + \log s}}{{{m^2} + 1}}} dm =  - \frac{{\log s}}{{{s^2} - 1}}\int\limits_0^\infty  {\frac{1}{{{m^2} + 1}}} dm =  - \frac{\pi }{2}\frac{{\log s}}{{{s^2} - 1}}$$",,"['analysis', 'integration', 'parametric', 'laplace-transform']"
38,Differentiability in $\mathbb{R}^{2}$,Differentiability in,\mathbb{R}^{2},"Here is my question: Find all the points $\left ( x,y \right )$ in $\mathbb{R}^{2}$ where the following function is differentiable: $f\left ( x,y \right )=\left | e^{x}-e^{y} \right |.\left ( x+y-2 \right )$ The only thing that came to my mind is to distinguish three cases:$x> y$, $x<y$ and $x=y$. In each case the function $f$ can be written differently. But, no idea how to find the points where the function is differentiable. Any one can help?","Here is my question: Find all the points $\left ( x,y \right )$ in $\mathbb{R}^{2}$ where the following function is differentiable: $f\left ( x,y \right )=\left | e^{x}-e^{y} \right |.\left ( x+y-2 \right )$ The only thing that came to my mind is to distinguish three cases:$x> y$, $x<y$ and $x=y$. In each case the function $f$ can be written differently. But, no idea how to find the points where the function is differentiable. Any one can help?",,"['real-analysis', 'analysis', 'multivariable-calculus']"
39,Uniform Lipschitz condition,Uniform Lipschitz condition,,"Does the function $f(x)=\sqrt{x}\sin(1/x),x\in(0,1],f(0)=0,$ satisfy the uniform Lipschitz condition $|f(x)-f(y)|<M|x-y|^{1/2},M>0$? Any help is appreciated.  Thanks","Does the function $f(x)=\sqrt{x}\sin(1/x),x\in(0,1],f(0)=0,$ satisfy the uniform Lipschitz condition $|f(x)-f(y)|<M|x-y|^{1/2},M>0$? Any help is appreciated.  Thanks",,['analysis']
40,Help understanding this formula on mutual information (used in bioinformatics),Help understanding this formula on mutual information (used in bioinformatics),,"I'm a bit lost on understanding this formula in my bioinformatics text, and I appreciate any tips or advice. Mutual Information, $\operatorname{MI}(X; Y)$ is:    $$ \mu = \sum_x \sum_y p(xy) \log \left( \frac{p(xy)}{p(x) p(y)} \right). $$   If $X$ and $Y$ are independent r.v.'s, then $\operatorname{MI} = 0$. Original link . By the way, It's part of this larger picture Thank You very much.","I'm a bit lost on understanding this formula in my bioinformatics text, and I appreciate any tips or advice. Mutual Information, $\operatorname{MI}(X; Y)$ is:    $$ \mu = \sum_x \sum_y p(xy) \log \left( \frac{p(xy)}{p(x) p(y)} \right). $$   If $X$ and $Y$ are independent r.v.'s, then $\operatorname{MI} = 0$. Original link . By the way, It's part of this larger picture Thank You very much.",,"['analysis', 'statistics', 'logarithms']"
41,Is $1-\sin(x)$ a contraction mapping? And is there a sufficient/necessary condition for a real-valued function to be a contraction?,Is  a contraction mapping? And is there a sufficient/necessary condition for a real-valued function to be a contraction?,1-\sin(x),"This came up as I was thinking about how to solve an assignment problem I've been given. I'm not posting the assignment problem here as I want to think about it myself, but one approach I am currently working on is to try to show that $$f:\mathbb{R}\to\mathbb{R},\quad f(x) = 1-\sin(x)$$ is a contraction mapping. At the moment I have a feeling it isn't. For example take $x_1 < 0 < x_2$, then $$ \left|\frac{(1-\sin x_1)-(1-\sin x_2)}{x_1-x_2}\right| = \left|\frac{\sin(x_1) - \sin(x_2)}{x_1-x_2}\right| \to 1$$ as $x_1 \to 0^-, x_2 \to 0^+$. Hence there does not exist a non-negative $k<1 \in \mathbb{R}$ such that $|\sin(x_1)-\sin(x_2)| \leq k|x_1-x_2|$ for all $x_1,x_2 \in \mathbb{R}$. (Just take $x_1,x_2$ arbitrarily close to $0$.) So $f$ is not a contraction mapping. That was the idea. Thinking just a little bit further, we see that this occurs because $|f'(x)| = 1$ at $x=0$. So perhaps we have a necessary and sufficient condition for a function on the reals to be a contraction mapping, namely that $\forall x\in\mathbb{R},|f'(x)|\leq k<1$ for some non-negative real $k$. Is this the case? Or have I missed some subtle (or not-so-subtle) point?","This came up as I was thinking about how to solve an assignment problem I've been given. I'm not posting the assignment problem here as I want to think about it myself, but one approach I am currently working on is to try to show that $$f:\mathbb{R}\to\mathbb{R},\quad f(x) = 1-\sin(x)$$ is a contraction mapping. At the moment I have a feeling it isn't. For example take $x_1 < 0 < x_2$, then $$ \left|\frac{(1-\sin x_1)-(1-\sin x_2)}{x_1-x_2}\right| = \left|\frac{\sin(x_1) - \sin(x_2)}{x_1-x_2}\right| \to 1$$ as $x_1 \to 0^-, x_2 \to 0^+$. Hence there does not exist a non-negative $k<1 \in \mathbb{R}$ such that $|\sin(x_1)-\sin(x_2)| \leq k|x_1-x_2|$ for all $x_1,x_2 \in \mathbb{R}$. (Just take $x_1,x_2$ arbitrarily close to $0$.) So $f$ is not a contraction mapping. That was the idea. Thinking just a little bit further, we see that this occurs because $|f'(x)| = 1$ at $x=0$. So perhaps we have a necessary and sufficient condition for a function on the reals to be a contraction mapping, namely that $\forall x\in\mathbb{R},|f'(x)|\leq k<1$ for some non-negative real $k$. Is this the case? Or have I missed some subtle (or not-so-subtle) point?",,"['real-analysis', 'analysis']"
42,Root estimation,Root estimation,,"What is the estimation for the positive root of the following equation $$ ax^k = (x+1)^{k-1} $$ where $a > 0$ (specifically $0 < a \leq 1$). Could you point out some reference related to the question?","What is the estimation for the positive root of the following equation $$ ax^k = (x+1)^{k-1} $$ where $a > 0$ (specifically $0 < a \leq 1$). Could you point out some reference related to the question?",,"['real-analysis', 'analysis']"
43,Is this proof correct? (Undergraduate Real Analysis),Is this proof correct? (Undergraduate Real Analysis),,"Problem. Let $(x_{n})_{n \in \mathbb{N}}$ be a sequence of real numbers and let $x$ be in $\mathbb{R}$ . Prove that if $\lim_{n \to +\infty} x_n = x$ and if $x > 0$ , then there exists a natural number $M$ such that $x_n > 0$ for all $n \geq M$ . Proof: Suppose $\lim_{n \to +\infty} x_n = x$ and $x > 0$ . Then for all $\epsilon > 0$ there exists a natural number $K$ such that $n \geq K$ implies that $|x_n – x| < \epsilon$ . Let a natural number $M$ be given such that $x_n > 0$ for all $n \geq M$ . Notice that if $x_n > 0$ , then $|x_n| > 0$ . But $$|x_n| = |x_n-x+x| \leq |x_n -x|+|x| < \epsilon + |x| = \epsilon + x. $$ So, $|x_n| - x < \epsilon$ . But since $x_n, x  > 0$ , We have that $$ |x_n| - x = |x_n-x| < \epsilon. $$ Sorry the format is so awful, but $x_n$ is a sequence.","Problem. Let be a sequence of real numbers and let be in . Prove that if and if , then there exists a natural number such that for all . Proof: Suppose and . Then for all there exists a natural number such that implies that . Let a natural number be given such that for all . Notice that if , then . But So, . But since , We have that Sorry the format is so awful, but is a sequence.","(x_{n})_{n \in \mathbb{N}} x \mathbb{R} \lim_{n \to +\infty} x_n = x x > 0 M x_n > 0 n \geq M \lim_{n \to +\infty} x_n = x x > 0 \epsilon > 0 K n \geq K |x_n – x| < \epsilon M x_n > 0 n \geq M x_n > 0 |x_n| > 0 |x_n| = |x_n-x+x| \leq |x_n -x|+|x| < \epsilon + |x| = \epsilon + x.  |x_n| - x < \epsilon x_n, x  > 0  |x_n| - x = |x_n-x| < \epsilon.  x_n","['sequences-and-series', 'analysis']"
44,On the solution of these equations,On the solution of these equations,,"Do not forget to see the Good News at the end of the problem. This problem is linked to the previous one , up to a changes of coordinates. However, that question is actually about only the first three equations here. I add three new equations and hope that we can treat them as a system of 6 variables. Let $(a,b,c,u,v,w)\in\mathbb{R}^6$ , $$ T := -a u -b v- c w,\quad S := T + u +v +w. $$ Consider the following 6 equations: $$ (1-a)S^2= u^2  -a T^2, $$ $$ (1-b)S^2 =v^2 -b T^2, $$ $$ (1-c)S^2=w^2 -c T^2, $$ $$ \Big( (1-a)^2 S -a^2 T-u\Big)\Big((1-b)^2 S -b^2 T-v\Big)=\Big((1-a)(1-b)S-abT\Big)^2, $$ $$ \Big( (1-a)^2 S -a^2 T-u\Big)\Big((1-c)^2 S -c^2 T-w\Big)=\Big((1-a)(1-c)S-acT\Big)^2, $$ $$ \Big( (1-b)^2 S -b^2 T-v\Big)\Big((1-c)^2 S -c^2 T-w\Big)=\Big((1-b)(1-c)S-bcT\Big)^2. $$ I want to find the solutions. Original problem is: Consider the homogeneous function $$ f(u,v,w) = (u+v + w +T)^3 - (u^3 + v^3 + w^3 + T^3). $$ I want to show if $x_0$ is a critical point $f$ , that is, $\nabla f(x_0)=0$ , then $\nabla^2 f(x_0)$ has rank 2, where $\nabla^2 f$ is the Hessian of $f$ . So I get the six equation above. How can I figure it out? Maybe the symmetry is helpful. Good News: After expressing $a,b,c$ in terms of $(T,S,u,v,w)$ from the first three equations, and then put them into the left three equations to get a system of 4 equations on $(T, u, v, w)$ . Mathematica tells me that the non-zero solutions of the original 6 equations must have the form $$ (1,0,0,u,0,0)\ \ \mbox{or}\ \ (0,1,0,0,v,0)\ \ \mbox{or} \  \  (0,0,1,0,0,w). $$ In fact, the four new equations for $(T,u,v,w)$ now are: $$ (eq.1)\ \ S(v(T^2-u^2)^2+ u(T^2-v^2)^2)+ (u^2-v^2)^2 ST = uv(T^2-S^2)^2+ T((u^2-S^2)^2 v + (v^2-S^2)^2 u) $$ $$ (eq.2)\ \ S(w(T^2-u^2)^2+ u(T^2-w^2)^2)+ (u^2-w^2)^2 ST = uw(T^2-S^2)^2+ T((u^2-S^2)^2 w + (w^2-S^2)^2 u) $$ $$ (eq.3)\ \ S(w(T^2-v^2)^2+ v(T^2-w^2)^2)+ (v^2-w^2)^2 ST = vw(T^2-S^2)^2+ T((v^2-S^2)^2 w + (w^2-S^2)^2 v) $$ $$ (eq.4) \ \ S^3 = T^3 + u^3 + v^3 + w^3,\ \ \mbox{where} \ S= T+u+v+w. $$ You can only work on them, Mathematica tells me the real solution of this system must take the form, e.g. [ $T=-u$ , $v=w=0$ ] or [ $T=w=0$ , $u=-v$ ] (other solution are similar by symmetry).","Do not forget to see the Good News at the end of the problem. This problem is linked to the previous one , up to a changes of coordinates. However, that question is actually about only the first three equations here. I add three new equations and hope that we can treat them as a system of 6 variables. Let , Consider the following 6 equations: I want to find the solutions. Original problem is: Consider the homogeneous function I want to show if is a critical point , that is, , then has rank 2, where is the Hessian of . So I get the six equation above. How can I figure it out? Maybe the symmetry is helpful. Good News: After expressing in terms of from the first three equations, and then put them into the left three equations to get a system of 4 equations on . Mathematica tells me that the non-zero solutions of the original 6 equations must have the form In fact, the four new equations for now are: You can only work on them, Mathematica tells me the real solution of this system must take the form, e.g. [ , ] or [ , ] (other solution are similar by symmetry).","(a,b,c,u,v,w)\in\mathbb{R}^6 
T := -a u -b v- c w,\quad S := T + u +v +w.
 
(1-a)S^2= u^2  -a T^2,
 
(1-b)S^2 =v^2 -b T^2,
 
(1-c)S^2=w^2 -c T^2,
 
\Big( (1-a)^2 S -a^2 T-u\Big)\Big((1-b)^2 S -b^2 T-v\Big)=\Big((1-a)(1-b)S-abT\Big)^2,
 
\Big( (1-a)^2 S -a^2 T-u\Big)\Big((1-c)^2 S -c^2 T-w\Big)=\Big((1-a)(1-c)S-acT\Big)^2,
 
\Big( (1-b)^2 S -b^2 T-v\Big)\Big((1-c)^2 S -c^2 T-w\Big)=\Big((1-b)(1-c)S-bcT\Big)^2.
 
f(u,v,w) = (u+v + w +T)^3 - (u^3 + v^3 + w^3 + T^3).
 x_0 f \nabla f(x_0)=0 \nabla^2 f(x_0) \nabla^2 f f a,b,c (T,S,u,v,w) (T, u, v, w) 
(1,0,0,u,0,0)\ \ \mbox{or}\ \ (0,1,0,0,v,0)\ \ \mbox{or} \  \  (0,0,1,0,0,w).
 (T,u,v,w) 
(eq.1)\ \ S(v(T^2-u^2)^2+ u(T^2-v^2)^2)+ (u^2-v^2)^2 ST = uv(T^2-S^2)^2+ T((u^2-S^2)^2 v + (v^2-S^2)^2 u)
 
(eq.2)\ \ S(w(T^2-u^2)^2+ u(T^2-w^2)^2)+ (u^2-w^2)^2 ST = uw(T^2-S^2)^2+ T((u^2-S^2)^2 w + (w^2-S^2)^2 u)
 
(eq.3)\ \ S(w(T^2-v^2)^2+ v(T^2-w^2)^2)+ (v^2-w^2)^2 ST = vw(T^2-S^2)^2+ T((v^2-S^2)^2 w + (w^2-S^2)^2 v)
 
(eq.4) \ \ S^3 = T^3 + u^3 + v^3 + w^3,\ \ \mbox{where} \ S= T+u+v+w.
 T=-u v=w=0 T=w=0 u=-v","['real-analysis', 'analysis', 'systems-of-equations', 'nonlinear-system']"
45,"How to construct a concrete sequence $\{(a_i,b_i]\}$ such that the inequality $\sum_i(F(b_i)-F(a_i)) < \epsilon$ holds for all $\epsilon>0$?",How to construct a concrete sequence  such that the inequality  holds for all ?,"\{(a_i,b_i]\} \sum_i(F(b_i)-F(a_i)) < \epsilon \epsilon>0","I got stuck on the following problem: Let $F:\mathbb{R}\to\mathbb{R}$ be a bounded, nondecreasing, and right-continuous function that satisfies $\lim_{x\to-\infty}F(x)=0$ . Define a function $\mu^*:\mathcal{P}(\mathbb{R})\to[0,+\infty]$ by letting $\mu^*(A)$ be the infimum of the set of sums $\sum_{n=1}^{\infty}(F(b_n)-F(a_n))$ , where $\{(a_n,b_n]\}$ ranges over the set of sequences of half-open ivtervals that cover $A$ , in the sense that $A \subseteq \bigcup_{n=1}^{\infty}(a_n,b_n]$ . Prove that $\mu^*(\emptyset) = 0$ . I would like to show it in this way: For each positive number $\epsilon$ there is a sequence $\{(a_i,b_i]\}$ of half-open intervals (whose unions necessarily includes $\emptyset$ ) such that $\sum_i(F(b_i)-F(a_i)) < \epsilon$ . However, I failed to give a concrete argument showing that the above claim is true; for example, I couldn't construct such a concrete sequence $\{(a_i,b_i]\}$ such that the inequality $\sum_i(F(b_i)-F(a_i)) < \epsilon$ holds for all $\epsilon>0$ . Could someone please help me out? Thanks a lot in advance!","I got stuck on the following problem: Let be a bounded, nondecreasing, and right-continuous function that satisfies . Define a function by letting be the infimum of the set of sums , where ranges over the set of sequences of half-open ivtervals that cover , in the sense that . Prove that . I would like to show it in this way: For each positive number there is a sequence of half-open intervals (whose unions necessarily includes ) such that . However, I failed to give a concrete argument showing that the above claim is true; for example, I couldn't construct such a concrete sequence such that the inequality holds for all . Could someone please help me out? Thanks a lot in advance!","F:\mathbb{R}\to\mathbb{R} \lim_{x\to-\infty}F(x)=0 \mu^*:\mathcal{P}(\mathbb{R})\to[0,+\infty] \mu^*(A) \sum_{n=1}^{\infty}(F(b_n)-F(a_n)) \{(a_n,b_n]\} A A \subseteq \bigcup_{n=1}^{\infty}(a_n,b_n] \mu^*(\emptyset) = 0 \epsilon \{(a_i,b_i]\} \emptyset \sum_i(F(b_i)-F(a_i)) < \epsilon \{(a_i,b_i]\} \sum_i(F(b_i)-F(a_i)) < \epsilon \epsilon>0","['real-analysis', 'sequences-and-series', 'analysis', 'measure-theory', 'proof-writing']"
46,Representation of $V$ as $\mathbb{C}^{2}$,Representation of  as,V \mathbb{C}^{2},"Let $V$ be a finite-dimensional complex inner product space and suppose that there is an operator (a matrix) $A$ on $V$ that satisfies the following anti-commutation relations: $$AA + AA = 0$$ $$A^{*}A + AA^{*} = I,$$ where $I$ is the identity matrix and $A^{*}$ is the adjoint of $A$ . In this case, one can show that $V$ has an even dimension and that $A$ has a representation as a $2 \times 2$ matrix: $$A = \begin{pmatrix} 0 & 1 \\ 0 & 0  \end{pmatrix} \tag{1}\label{1} $$ This is done in Simon's book . The basic idea is that $V$ has an orthogonal direct sum decomposition $V = \operatorname{Ker}(A) \oplus \operatorname{Ker}(A^{*})$ and, thus, every vector $\phi \in V$ can be seen as a vector: $$\phi = \begin{pmatrix} \varphi \\ \psi \end{pmatrix} \tag{2}\label{2}$$ with $\varphi \in \operatorname{Ker}(A)$ and $\psi \in \operatorname{Ker}(A^{*})$ . In this case, $A$ acts as a $2 \times 2$ matrix on these vectors. I have some questions regarding some terminology and concepts written on Simon's book. First, I suppose the representation (\ref{1}) should be, strictly speaking, $$A = \begin{pmatrix} 0 & I \\ 0 & 0 \end{pmatrix} $$ that is, with $1$ replaced by the identity matrix $I$ on $V$ , but then I can simply see $I$ as $1$ in some sense. This is fine. The terminology ""representation"" (as in $A$ has a representation...) is a bit more troublesome. A representation of a group $G$ in a vector space $U$ is a group homomorphism $\rho: G \to \operatorname{GL}(U)$ . Of course, the space of all linear operators (i.e. complex matrices) on $V$ is a group, and we can talk about a representation of these matrices on $U = \mathbb{C}^{2}$ . But the above setting only deals with one matrix $A$ , not all of them , so I am a bit confused with the usage of the term representation in this context. Does it have any rigorous meaning or it is just a way of writing $A$ can be informally seen as the right hand side of (\ref{1})? More importantly, Simon seems to imply that the ""representation"" of $A$ as the $2 \times 2$ matrix in (\ref{1}) imply that $V$ itself can be viewed as $\mathbb{C}^{2}$ . This is very confusing to me. On one hand, it makes sense because $A$ can be seen as a $2 \times 2$ matrix, so it should act on elements of $\mathbb{C}^{2}$ . However, the only decomposition of vectors of $V$ being used is (\ref{2}), and it is absolutely not clear why this can be seen as $\mathbb{C}^{2}$ if each entry $\varphi$ and $\psi$ is not uniquely determined by a complex number (that is, $\phi$ in (\ref{2}) is not a a vector on $\mathbb{C}^{2}$ ). Can someone please help me with finding the right definitions of the concepts being used by Simon and justifying its way of seing $V$ as $\mathbb{C}^{2}$ ?","Let be a finite-dimensional complex inner product space and suppose that there is an operator (a matrix) on that satisfies the following anti-commutation relations: where is the identity matrix and is the adjoint of . In this case, one can show that has an even dimension and that has a representation as a matrix: This is done in Simon's book . The basic idea is that has an orthogonal direct sum decomposition and, thus, every vector can be seen as a vector: with and . In this case, acts as a matrix on these vectors. I have some questions regarding some terminology and concepts written on Simon's book. First, I suppose the representation (\ref{1}) should be, strictly speaking, that is, with replaced by the identity matrix on , but then I can simply see as in some sense. This is fine. The terminology ""representation"" (as in has a representation...) is a bit more troublesome. A representation of a group in a vector space is a group homomorphism . Of course, the space of all linear operators (i.e. complex matrices) on is a group, and we can talk about a representation of these matrices on . But the above setting only deals with one matrix , not all of them , so I am a bit confused with the usage of the term representation in this context. Does it have any rigorous meaning or it is just a way of writing can be informally seen as the right hand side of (\ref{1})? More importantly, Simon seems to imply that the ""representation"" of as the matrix in (\ref{1}) imply that itself can be viewed as . This is very confusing to me. On one hand, it makes sense because can be seen as a matrix, so it should act on elements of . However, the only decomposition of vectors of being used is (\ref{2}), and it is absolutely not clear why this can be seen as if each entry and is not uniquely determined by a complex number (that is, in (\ref{2}) is not a a vector on ). Can someone please help me with finding the right definitions of the concepts being used by Simon and justifying its way of seing as ?","V A V AA + AA = 0 A^{*}A + AA^{*} = I, I A^{*} A V A 2 \times 2 A = \begin{pmatrix}
0 & 1 \\
0 & 0 
\end{pmatrix}
\tag{1}\label{1}
 V V = \operatorname{Ker}(A) \oplus \operatorname{Ker}(A^{*}) \phi \in V \phi = \begin{pmatrix}
\varphi \\
\psi
\end{pmatrix}
\tag{2}\label{2} \varphi \in \operatorname{Ker}(A) \psi \in \operatorname{Ker}(A^{*}) A 2 \times 2 A = \begin{pmatrix}
0 & I \\
0 & 0
\end{pmatrix}
 1 I V I 1 A G U \rho: G \to \operatorname{GL}(U) V U = \mathbb{C}^{2} A A A 2 \times 2 V \mathbb{C}^{2} A 2 \times 2 \mathbb{C}^{2} V \mathbb{C}^{2} \varphi \psi \phi \mathbb{C}^{2} V \mathbb{C}^{2}","['linear-algebra', 'abstract-algebra', 'analysis', 'vector-spaces', 'representation-theory']"
47,Taylor series of $\sum_{n=0}^{\infty} \frac{\cos(n^2 x)}{2^n}$,Taylor series of,\sum_{n=0}^{\infty} \frac{\cos(n^2 x)}{2^n},"I have the following problem. I must show that the following function $f$ is infinitely differentiable, then find its Taylor series centered at $0$ , and the corresponding radius of convergence: $$f(x) = \sum_{n=0}^{\infty} \frac{\cos(n^2 x)}{2^n}.$$ I think that, since the series is absolutely convergent, it follows that it is infinitely differentiable, but I am not sure about that. Furthermore, I have no clue as to how to find the Taylor series. I was thinking maybe I could use the series expansion of the cosine, but I am not sure about that either.","I have the following problem. I must show that the following function is infinitely differentiable, then find its Taylor series centered at , and the corresponding radius of convergence: I think that, since the series is absolutely convergent, it follows that it is infinitely differentiable, but I am not sure about that. Furthermore, I have no clue as to how to find the Taylor series. I was thinking maybe I could use the series expansion of the cosine, but I am not sure about that either.",f 0 f(x) = \sum_{n=0}^{\infty} \frac{\cos(n^2 x)}{2^n}.,"['sequences-and-series', 'analysis', 'taylor-expansion']"
48,Verification of a proof for supremum and infimum of the set $A=${$(n^3+1)/(n^4+16):n\in\mathbb{N}$},Verification of a proof for supremum and infimum of the set {},A= (n^3+1)/(n^4+16):n\in\mathbb{N},"I came across an example while studying which asks us to solve (and prove by definition), the supremum and infimum of the following set: A = { $\frac{n^3+1}{n^4+16}$ : $n = 1,2,3...$ } Here is my approach. I would like to know if this works or if I'm getting lost in the objective to prove. I start by looking for the $\inf(A)$ . Clearly the infimum is $0$ due to the dominating exponent in the denominator. So by the definition, I need to show; $0$ is a lower bound for $A$ $\forall $ $ \epsilon>0$ , $0 + \epsilon$ is not a lower bound for A I begin with a rough proof to find the logic I need to follow for 1. So, show: \begin{align} 0 \leq \frac{n^3+1}{n^4+16} \\ 0 \leq n^3+1\\ 0 \leq n+1 \leq n^3+1\\ -1 \leq n \end{align} Which then lets me start the actual proof: By the Archimedean property, $\exists$ $n \in \mathbb{N}$ s.t. $n \geq -1$ \begin{align} n+1 \geq 0\\ n^3+1 \geq n+1 \geq 0\\ \frac{n^3+1}{n^3+16} \geq 0 \end{align} So $0$ is a lower bound for A Secondly, we need to show that $\epsilon + 0$ is not a lower bound for A. So, starting with a rough proof, I need to show that there exists some $n$ s.t \begin{align} \frac{n^3+1}{n^4+16} \leq \epsilon\\ \frac{n^3+1}{n^4+16} \leq \frac{n^3}{n^4+16}+1 \leq \epsilon\\ \frac{n^3}{n^4+16} \leq \epsilon -1\\ \frac{n^3}{n^4+16} \leq \frac{n^3}{n^4} \leq \epsilon -1\\ \frac{1}{n} \leq \epsilon -1\\ n \geq \frac{1}{\epsilon -1} \end{align} And with this, I worked backward to get the implication I needed (starting with the A.P as stated above). I won't write out my entire work for the supA, but it follows the same sort of method I used here. I want to make sure these step are coherent, and is there anything I can do to get to the desired values quicker as this is quite a tedious process to do explicitly. Thanks! Edit: In my supremum proof I've found an inequality I can't seem to manipulate. After proving 28/97 is an upper bound (through inspection of the sequence, n=3 yields the largest value). In the second step of showing that $\frac{28}{97} -\epsilon$ is not an upper bound, I begin like usual (showing that there exists an n s.t.): \begin{align} \frac{n^3+1}{n^4+16} \geq \frac{28}{97} - \epsilon\\ \frac{n^3+1}{n^4+16} \geq \frac{n^3}{n^4+16} \geq \frac{28}{97} - \epsilon \end{align} And I seem to be stuck. No matter the manipulation I can think of, I can't get myself to a form of $n \geq f(\epsilon)$ to use the Archimedean property. Any hints?","I came across an example while studying which asks us to solve (and prove by definition), the supremum and infimum of the following set: A = { : } Here is my approach. I would like to know if this works or if I'm getting lost in the objective to prove. I start by looking for the . Clearly the infimum is due to the dominating exponent in the denominator. So by the definition, I need to show; is a lower bound for , is not a lower bound for A I begin with a rough proof to find the logic I need to follow for 1. So, show: Which then lets me start the actual proof: By the Archimedean property, s.t. So is a lower bound for A Secondly, we need to show that is not a lower bound for A. So, starting with a rough proof, I need to show that there exists some s.t And with this, I worked backward to get the implication I needed (starting with the A.P as stated above). I won't write out my entire work for the supA, but it follows the same sort of method I used here. I want to make sure these step are coherent, and is there anything I can do to get to the desired values quicker as this is quite a tedious process to do explicitly. Thanks! Edit: In my supremum proof I've found an inequality I can't seem to manipulate. After proving 28/97 is an upper bound (through inspection of the sequence, n=3 yields the largest value). In the second step of showing that is not an upper bound, I begin like usual (showing that there exists an n s.t.): And I seem to be stuck. No matter the manipulation I can think of, I can't get myself to a form of to use the Archimedean property. Any hints?","\frac{n^3+1}{n^4+16} n = 1,2,3... \inf(A) 0 0 A \forall   \epsilon>0 0 + \epsilon \begin{align}
0 \leq \frac{n^3+1}{n^4+16} \\
0 \leq n^3+1\\
0 \leq n+1 \leq n^3+1\\
-1 \leq n
\end{align} \exists n \in \mathbb{N} n \geq -1 \begin{align}
n+1 \geq 0\\
n^3+1 \geq n+1 \geq 0\\
\frac{n^3+1}{n^3+16} \geq 0
\end{align} 0 \epsilon + 0 n \begin{align}
\frac{n^3+1}{n^4+16} \leq \epsilon\\
\frac{n^3+1}{n^4+16} \leq \frac{n^3}{n^4+16}+1 \leq \epsilon\\
\frac{n^3}{n^4+16} \leq \epsilon -1\\
\frac{n^3}{n^4+16} \leq \frac{n^3}{n^4} \leq \epsilon -1\\
\frac{1}{n} \leq \epsilon -1\\
n \geq \frac{1}{\epsilon -1}
\end{align} \frac{28}{97} -\epsilon \begin{align}
\frac{n^3+1}{n^4+16} \geq \frac{28}{97} - \epsilon\\
\frac{n^3+1}{n^4+16} \geq \frac{n^3}{n^4+16} \geq \frac{28}{97} - \epsilon
\end{align} n \geq f(\epsilon)","['real-analysis', 'analysis', 'solution-verification', 'supremum-and-infimum']"
49,Prove that a function of $n$ variables is concave if and only if the set below its graph in $\mathbb{R}^{\mathbf{n+1}}$ is a convex set.,Prove that a function of  variables is concave if and only if the set below its graph in  is a convex set.,n \mathbb{R}^{\mathbf{n+1}},"I know that an analogous result for convex function is proved here . But my question is about using a specific method to proceed. Here is the question: Poblem Prove that a function of $n$ variables is concave if and only if the set below its graph in $\mathbb{R}^{\mathbf{n+1}}$ is a convex set. Prove this statement for functions of one variable first. Then apply the following theorem for the case of $n$ variables. Theorem $\quad$ Let $f$ be a function defined on a convex subset $U$ of $\mathbb{R}^{\mathbf{n}}$ . Then, $f$ is concave if and only if its restriction to every line segment in $U$ is a concave function of one variable. My Question I do not have problem proving the statement for functions of one variable (see My Attempt below). But I am not sure how to apply the above theorem for the case of functions of $n$ variables. Could someone please help me with this? Basically, how to formally and rigorously extend the proof of the statement from functions of one variable to $n$ variables? Thanks a lot in advance! My Attempt Here is what I have so far: Proof $\quad$ We first prove that a function of one variable $f$ is concave if and only if the set on or below its graph in $\mathbb{R}^2$ is a convex set. Suppose that $f$ is concave, and that $(x_1,y_1)$ and $(x_2,y_2)$ lie in the set on or below its graph $G$ ; that is, $f(x_1) \geq y_1$ and $f(x_2) \geq y_2$ . Any point on the line segment $L$ joining these two points can be represented by $(tx_2 + (1-t)x_1, ty_2 + (1-t)y_1)$ , where $t \in [0,1]$ . Since $f$ is concave, we have \begin{align*} f(tx_2 + (1-t)x_1) \geq tf(x_2) + (1-t)f(x_1) \geq ty_2 + (1-t)y_1. \end{align*} Thus, the segment $L$ lies in the set on or below $G$ . Hence, the set on or below $G$ is convex. Conversely, suppose the set on or below $G$ is convex, and that $(x_1,f(x_1))$ and $(x_2,f(x_2))$ are in this set. Then, for all $t \in [0,1]$ , the point $(tx_1 + (1-t)x_2, tf(x_1) + (1-t)f(x_2))$ is also in this set. Thus, \begin{align*} tf(x_1) + (1-t)f(x_2) \leq f(tx_1 + (1-t)x_2). \end{align*} So, $f$ is concave. Now, let $f$ be a function defined on a convex subset $U$ of $\mathbb{R}^{\mathbf{n}}$ . $\dots$ (This is where I got stuck.)","I know that an analogous result for convex function is proved here . But my question is about using a specific method to proceed. Here is the question: Poblem Prove that a function of variables is concave if and only if the set below its graph in is a convex set. Prove this statement for functions of one variable first. Then apply the following theorem for the case of variables. Theorem Let be a function defined on a convex subset of . Then, is concave if and only if its restriction to every line segment in is a concave function of one variable. My Question I do not have problem proving the statement for functions of one variable (see My Attempt below). But I am not sure how to apply the above theorem for the case of functions of variables. Could someone please help me with this? Basically, how to formally and rigorously extend the proof of the statement from functions of one variable to variables? Thanks a lot in advance! My Attempt Here is what I have so far: Proof We first prove that a function of one variable is concave if and only if the set on or below its graph in is a convex set. Suppose that is concave, and that and lie in the set on or below its graph ; that is, and . Any point on the line segment joining these two points can be represented by , where . Since is concave, we have Thus, the segment lies in the set on or below . Hence, the set on or below is convex. Conversely, suppose the set on or below is convex, and that and are in this set. Then, for all , the point is also in this set. Thus, So, is concave. Now, let be a function defined on a convex subset of . (This is where I got stuck.)","n \mathbb{R}^{\mathbf{n+1}} n \quad f U \mathbb{R}^{\mathbf{n}} f U n n \quad f \mathbb{R}^2 f (x_1,y_1) (x_2,y_2) G f(x_1) \geq y_1 f(x_2) \geq y_2 L (tx_2 + (1-t)x_1, ty_2 + (1-t)y_1) t \in [0,1] f \begin{align*}
f(tx_2 + (1-t)x_1) \geq tf(x_2) + (1-t)f(x_1) \geq ty_2 + (1-t)y_1.
\end{align*} L G G G (x_1,f(x_1)) (x_2,f(x_2)) t \in [0,1] (tx_1 + (1-t)x_2, tf(x_1) + (1-t)f(x_2)) \begin{align*}
tf(x_1) + (1-t)f(x_2) \leq f(tx_1 + (1-t)x_2).
\end{align*} f f U \mathbb{R}^{\mathbf{n}} \dots","['real-analysis', 'analysis', 'convex-analysis']"
50,Is the set of 'crossing points' of every curve closed?,Is the set of 'crossing points' of every curve closed?,,"I want to know if the following statement is true or not: $\gamma : [a, b] \to \mathbb{R}^k$ is continuous then $E := \{ t \vert \exists s \neq t: \gamma (t) = \gamma (s) \}$ is closed. So here, $E$ is just the set of 'crossing points' of the curve $\gamma$ . I hope this is true so that I can find 'the first' crossing point $t_0 = \min (E)$ . I found the statement is true for a special case where $\gamma$ is a polygonal path(finite number of consecutive line segments) but failed in this general case. Edit: The statement is simply false, as the counterexamples in the comments show. What I wanted to prove was the existence of $\min (E)$ and I succeeded with polygonal path since it had a simple structure: finite number of edges. My question was messed up while thinking about the general case. FYI, My original problem was Show that any closed polygonal path can be decomposed into a finite union of simple closed polygonal paths , which is Exercise 8.7 of Joseph Bak, Complex Analysis. Here I wanted to use $\min (E)$ and got curious about the general case.","I want to know if the following statement is true or not: is continuous then is closed. So here, is just the set of 'crossing points' of the curve . I hope this is true so that I can find 'the first' crossing point . I found the statement is true for a special case where is a polygonal path(finite number of consecutive line segments) but failed in this general case. Edit: The statement is simply false, as the counterexamples in the comments show. What I wanted to prove was the existence of and I succeeded with polygonal path since it had a simple structure: finite number of edges. My question was messed up while thinking about the general case. FYI, My original problem was Show that any closed polygonal path can be decomposed into a finite union of simple closed polygonal paths , which is Exercise 8.7 of Joseph Bak, Complex Analysis. Here I wanted to use and got curious about the general case.","\gamma : [a, b] \to \mathbb{R}^k E := \{ t \vert \exists s \neq t: \gamma (t) = \gamma (s) \} E \gamma t_0 = \min (E) \gamma \min (E) \min (E)",['analysis']
51,Help check that function is non-negative,Help check that function is non-negative,,"Consider the function: $$f_a(x) = [\phi(a+x)(a+x) + \phi(a-x)(a-x)][\Phi(a-x)-\Phi(-a-x)]+[\phi(a+x)-\phi(a-x)]^2$$ where $a>0$ is a real number and $\phi(\cdot), \Phi(\cdot)$ denote the probability density function and the cumulative distribution function of a standard normal random variable, respectively. I would like to prove that the function is non negative. The graph (blue) of $f_3$ is shown below. The claim looks right, and it is easy to check it for the interval $x \in [-a,a]$ . Any help to prove the remaining part of the real line will be of great help. Note that by the symmetry of the function we only need to check it for $x>0$ , or as I have already done a little, for $x>a$ . Edit 1: We also see that the function is such that $f_a(x) \to 0$ as $x \to \infty$ . I am thinking we could check that $f_a''(0)>0$ and that for $x>0$ the function has a unique critical point (or not more than one). Then it will follow our claim, since if we prove this then there can not be some $x^*>0$ such that the function changes sign since then there would be another critical point. But of course to analyse the derivative and to prove doesn't look easy. Edit 2: Changed the first sign, had a typo. Edit 3: Graph (red) of the function $\phi(a+x)(a+x) + \phi(a-x)(a-x)$ for $a=3$ again. Edit 4: In black the same function divided by $[\Phi(a-x)-\Phi(-a-x)]^2$ is plotted. Since this is a non negative function, clearly the original is non negative iff this quotient is. This graph seems to indicate that the $f_a(x)$ is indeed non negative.","Consider the function: where is a real number and denote the probability density function and the cumulative distribution function of a standard normal random variable, respectively. I would like to prove that the function is non negative. The graph (blue) of is shown below. The claim looks right, and it is easy to check it for the interval . Any help to prove the remaining part of the real line will be of great help. Note that by the symmetry of the function we only need to check it for , or as I have already done a little, for . Edit 1: We also see that the function is such that as . I am thinking we could check that and that for the function has a unique critical point (or not more than one). Then it will follow our claim, since if we prove this then there can not be some such that the function changes sign since then there would be another critical point. But of course to analyse the derivative and to prove doesn't look easy. Edit 2: Changed the first sign, had a typo. Edit 3: Graph (red) of the function for again. Edit 4: In black the same function divided by is plotted. Since this is a non negative function, clearly the original is non negative iff this quotient is. This graph seems to indicate that the is indeed non negative.","f_a(x) = [\phi(a+x)(a+x) + \phi(a-x)(a-x)][\Phi(a-x)-\Phi(-a-x)]+[\phi(a+x)-\phi(a-x)]^2 a>0 \phi(\cdot), \Phi(\cdot) f_3 x \in [-a,a] x>0 x>a f_a(x) \to 0 x \to \infty f_a''(0)>0 x>0 x^*>0 \phi(a+x)(a+x) + \phi(a-x)(a-x) a=3 [\Phi(a-x)-\Phi(-a-x)]^2 f_a(x)",['analysis']
52,Derivation of parametric integral function,Derivation of parametric integral function,,"Good afternoon, consider the integral function $f(x):=\int_{0}^{+\infty}\mathrm{d}t\,\frac{\cos(xt)}{\sqrt{t^2+1}}$ . I would like to derive $f$ with respect to $x$ to obtain $\frac{\mathrm{d}f(x)}{\mathrm{d}x}=-\int_{0}^{+\infty}\mathrm{d}t\,\frac{t\sin(xt)}{\sqrt{t^2+1}}$ , but the integral is not absolutely convergent and Lebesgue's dominated convergence theorem cannot be applied here to justify the derivation under the integral sign. How could one proceed here?","Good afternoon, consider the integral function . I would like to derive with respect to to obtain , but the integral is not absolutely convergent and Lebesgue's dominated convergence theorem cannot be applied here to justify the derivation under the integral sign. How could one proceed here?","f(x):=\int_{0}^{+\infty}\mathrm{d}t\,\frac{\cos(xt)}{\sqrt{t^2+1}} f x \frac{\mathrm{d}f(x)}{\mathrm{d}x}=-\int_{0}^{+\infty}\mathrm{d}t\,\frac{t\sin(xt)}{\sqrt{t^2+1}}","['real-analysis', 'calculus', 'integration', 'analysis', 'improper-integrals']"
53,Prove that the vector space of real convergent power series does not have a countable basis,Prove that the vector space of real convergent power series does not have a countable basis,,"Define $V$ as the subset of $\mathbb{R}[[x]]$ , the $\mathbb{R}$ -vector space of formal power series with real coefficients, such that for any $f\in V$ , for any $r\in\mathbb{R}$ , the series $f$ always converges when evaluated at $r$ . It can be shown that $V$ is a subspace of $\mathbb{R}[[x]]$ . I am trying to show that $V$ does not have a countable basis. Suppose for contradiction that $S$ is a countable basis of $V$ . Write the $i$ th element in $S$ as $f_i:=\sum_j a_{ij}x^j$ . One attempt to construct an element $g$ in $V$ such that $g$ is not spanned by $S$ would be some kind of diagonal argument, i.e. $g:=\sum_j h(a_{jj})x^j$ for some suitable function $h$ . However, I struggle to find an $h$ such that $g$ is convergent because the $a_{jj}$ can be quite arbitrary. Is there any other promising way I can do this?","Define as the subset of , the -vector space of formal power series with real coefficients, such that for any , for any , the series always converges when evaluated at . It can be shown that is a subspace of . I am trying to show that does not have a countable basis. Suppose for contradiction that is a countable basis of . Write the th element in as . One attempt to construct an element in such that is not spanned by would be some kind of diagonal argument, i.e. for some suitable function . However, I struggle to find an such that is convergent because the can be quite arbitrary. Is there any other promising way I can do this?",V \mathbb{R}[[x]] \mathbb{R} f\in V r\in\mathbb{R} f r V \mathbb{R}[[x]] V S V i S f_i:=\sum_j a_{ij}x^j g V g S g:=\sum_j h(a_{jj})x^j h h g a_{jj},"['linear-algebra', 'sequences-and-series', 'analysis', 'hamel-basis']"
54,confusion about chain rule in linearity proof,confusion about chain rule in linearity proof,,"Suppose $v,p\in\mathbb{R}^n$ , then $D_v|_p:C^\infty\to\mathbb{R}$ is defined by $$D_v|_p(f):=\frac{d}{dt}|_{t=0}f(p+tv).$$ Now I'm looking at a proposition that states that the map $v\mapsto D_v|_p$ is a linear isomorphism from $\mathbb{R}^n\to T_p\mathbb{R}^n$ . I understand most of the proof, but I'm confused by the linearity part: $$\frac{d}{dt}|_{t=0}f(p+tv+\lambda tw) = \frac{d}{dt}|_{t=0}f(p+tv) + \lambda\cdot\frac{d}{dt}|_{t=0}f(p+tw)$$ for $v,w,p\in\mathbb{R}^n$ and $\lambda\in\mathbb{R}$ . This equality should immediately follow from the chain rule, but I don't see how. Probably I'm just a bit confused by the notation of (directional) derivatives... but I already searched quite a bit and just can't see how the chain rule is used here. Can someone explain this?","Suppose , then is defined by Now I'm looking at a proposition that states that the map is a linear isomorphism from . I understand most of the proof, but I'm confused by the linearity part: for and . This equality should immediately follow from the chain rule, but I don't see how. Probably I'm just a bit confused by the notation of (directional) derivatives... but I already searched quite a bit and just can't see how the chain rule is used here. Can someone explain this?","v,p\in\mathbb{R}^n D_v|_p:C^\infty\to\mathbb{R} D_v|_p(f):=\frac{d}{dt}|_{t=0}f(p+tv). v\mapsto D_v|_p \mathbb{R}^n\to T_p\mathbb{R}^n \frac{d}{dt}|_{t=0}f(p+tv+\lambda tw) = \frac{d}{dt}|_{t=0}f(p+tv) + \lambda\cdot\frac{d}{dt}|_{t=0}f(p+tw) v,w,p\in\mathbb{R}^n \lambda\in\mathbb{R}","['calculus', 'analysis', 'smooth-manifolds', 'chain-rule', 'tangent-spaces']"
55,Calculting the infimum,Calculting the infimum,,"Consider the maps $u, \phi:\mathbb{R}^n \to \mathbb{R^n}$ . Let $\mathcal{F}$ be some class of functions and consider the quantity $$\inf_{u \in \mathcal{F}} \left( \frac{|u|^2}{2}+u\cdot\phi\right) .$$ Assuming that $-\phi \in \mathcal{F}$ , I guess the infimum will be attained at $u=-\phi$ as this is a parabola. Is this observation correct and can something be said if we consider $$\inf_{u \in \mathcal{F}} \left( \frac{|u|^{2n}}{2n}+u\cdot\phi\right) $$ instead?","Consider the maps . Let be some class of functions and consider the quantity Assuming that , I guess the infimum will be attained at as this is a parabola. Is this observation correct and can something be said if we consider instead?","u, \phi:\mathbb{R}^n \to \mathbb{R^n} \mathcal{F} \inf_{u \in \mathcal{F}} \left( \frac{|u|^2}{2}+u\cdot\phi\right) . -\phi \in \mathcal{F} u=-\phi \inf_{u \in \mathcal{F}} \left( \frac{|u|^{2n}}{2n}+u\cdot\phi\right) ","['real-analysis', 'analysis']"
56,Is every function $f\colon\mathbb N\to\mathbb R$ Lebesgue-integrable?,Is every function  Lebesgue-integrable?,f\colon\mathbb N\to\mathbb R,"I believe this is the case as the natural numbers have Lebesgue-measure $0$ , so the Lebesgue-integral of any function $f\colon\mathbb N\to\mathbb R$ is also $0$ . Is that correct?","I believe this is the case as the natural numbers have Lebesgue-measure , so the Lebesgue-integral of any function is also . Is that correct?",0 f\colon\mathbb N\to\mathbb R 0,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
57,Spivak Calculus Chapter 10 problem 19,Spivak Calculus Chapter 10 problem 19,,"I have a question about Spivak's Calculus, chapter 10 problem 19. I feel I must be missing something very simple, but I don't know what it is. Here is the question: Prove that if $f^{n}(g(a))$ and $g^{n}(a)$ both exist, then $(f \circ g)^{n}(a)$ exists. A little experimentation should convince you that it is unwise to seek a formula for $(f \circ g)^{n}(a)$ . In order to prove that $(f \circ g)^{n}(a)$ exists you will therefore have to devise a reasonable assertion about $(f\circ g)^{n}(a)$ which can be proved by induction. Here is my conjecture. $\textbf{Conjecture}$ . If $g^{n}(a)$ and $f^{n}(g(a))$ both exist, then $(f\circ g)^{n}(a)$ exists and is a sum of products of the form: $$c[g{'}(a)]^{m_{1}}[g^{2}(a)]^{m_{2}} \dots [g^{n}(a)]^{m_{n}}f^{k}(g(a)) $$ where $c$ is a constant, $m_{1}, \dots , m_{n}$ are natural numbers, and $k$ is a natural number such that $k\leq n$ . The base case for $n=1$ is straightforward. If $g{'}(a)$ exists and $f{'}(g(a))$ exists, then the Chain Rule tells us that $(f\circ g)^{'}(a)$ exists and is equal to $g{'}(a)f{'}(g(a))$ .  Thus, the conjecture holds for $n=1$ if we let $c=m_{1}=k=1$ . Next we assume: $\textbf{Inductive Hypothesis}$ If $g^{n}(a)$ and $f^{n}(g(a))$ exist, then $(f\circ g)^{n}(a)$ exists and is a sum of products of the form: $$c[g{'}(a)]^{m_{1}}[g^{2}(a)]^{m_{2}} \dots [g^{n}(a)]^{m_{n}}f^{k}(g(a)) $$ We now want to show: If $g^{n+1}(a)$ exists and $f^{n+1}(g(a))$ exists, then $(f\circ g)^{n+1}(a)$ exists and is a sum of products of the form (where $k\leq n+1$ ): $$c[g{'}(a)]^{m_{1}}[g^{2}(a)]^{m_{2}} \dots [g^{n}(a)]^{m_{n}}[g^{n+1}(a)]^{m_{n+1}}f^{k}(g(a)) $$ I am confused about the answers that I have found in the solution book and on stack exchange here: Spivak Chapter 10, Exercise 19 solution verification. Prove that if $f^{(n)}(g(a))$ and $g^{(n)}(a)$ both exist, then $(f\circ g)^{(n)}(a)$ exists. . For the inductive step, the author shows that if $f^{n+1}(g(a))$ exists and $g^{n+1}(a)$ exist, then it follows that for all x in some interval around a, $f^{n}(g(x))$ and $g^{n}(x)$ both exist.  They then conclude from the Inductive Hypothesis that, in this interval, $(f\circ g)^{n}$ exists and is equal to a sum of products of the form: $$c[g{'}(x)]^{m_{1}}[g^{2}(x)]^{m_{2}} \dots [g^{n}(x)]^{m_{n}}f^{k}(g(x)) $$ Here the author is not talking about $(f\circ g)^{n}$ at a single point a. Instead they are talking about $(f \circ g)^{n}(x)$ at all points in an interval around $a$ . This the part that I don't understand. $\textbf{Why do we need to establish that $(f\circ g)^{n}(x)$ exists for all x in some interval around $a$?}$ Why can't we just say: If $g^{n+1}(a)$ exists, then $g^{n}(a)$ exists.  By the definition of $g^{n+1}$ , we have: $$g^{n+1}(a) = \lim_{h\to 0}\frac{g^{n}(a+h) - g^{n}(a)}{h}$$ So $g^{n}(a)$ must exist. Similarly, we know that $f^{n}(g(a))$ exists since: $$f^{n+1}(g(a)) = \lim_{h\to 0}\frac{f^{n}(g(a) + h) - f^{n}(g(a))}{h}$$ So by the Inductive Hypothesis, it follows that $(f\circ g)^{n}(a)$ exists and is a sum of terms of products of the form: $$c[g{'}(a)]^{m_{1}}[g^{2}(a)]^{m_{2}} \dots [g^{n}(a)]^{m_{n}}f^{k}(g(a)) $$ It is then easy to use the product, chain, and sum rules for the derivative to show that $(f\circ g)^{n+1}(a) is a sum of products that all have the required form.","I have a question about Spivak's Calculus, chapter 10 problem 19. I feel I must be missing something very simple, but I don't know what it is. Here is the question: Prove that if and both exist, then exists. A little experimentation should convince you that it is unwise to seek a formula for . In order to prove that exists you will therefore have to devise a reasonable assertion about which can be proved by induction. Here is my conjecture. . If and both exist, then exists and is a sum of products of the form: where is a constant, are natural numbers, and is a natural number such that . The base case for is straightforward. If exists and exists, then the Chain Rule tells us that exists and is equal to .  Thus, the conjecture holds for if we let . Next we assume: If and exist, then exists and is a sum of products of the form: We now want to show: If exists and exists, then exists and is a sum of products of the form (where ): I am confused about the answers that I have found in the solution book and on stack exchange here: Spivak Chapter 10, Exercise 19 solution verification. Prove that if $f^{(n)}(g(a))$ and $g^{(n)}(a)$ both exist, then $(f\circ g)^{(n)}(a)$ exists. . For the inductive step, the author shows that if exists and exist, then it follows that for all x in some interval around a, and both exist.  They then conclude from the Inductive Hypothesis that, in this interval, exists and is equal to a sum of products of the form: Here the author is not talking about at a single point a. Instead they are talking about at all points in an interval around . This the part that I don't understand. Why can't we just say: If exists, then exists.  By the definition of , we have: So must exist. Similarly, we know that exists since: So by the Inductive Hypothesis, it follows that exists and is a sum of terms of products of the form: It is then easy to use the product, chain, and sum rules for the derivative to show that $(f\circ g)^{n+1}(a) is a sum of products that all have the required form.","f^{n}(g(a)) g^{n}(a) (f \circ g)^{n}(a) (f \circ g)^{n}(a) (f \circ g)^{n}(a) (f\circ g)^{n}(a) \textbf{Conjecture} g^{n}(a) f^{n}(g(a)) (f\circ g)^{n}(a) c[g{'}(a)]^{m_{1}}[g^{2}(a)]^{m_{2}} \dots [g^{n}(a)]^{m_{n}}f^{k}(g(a))  c m_{1}, \dots , m_{n} k k\leq n n=1 g{'}(a) f{'}(g(a)) (f\circ g)^{'}(a) g{'}(a)f{'}(g(a)) n=1 c=m_{1}=k=1 \textbf{Inductive Hypothesis} g^{n}(a) f^{n}(g(a)) (f\circ g)^{n}(a) c[g{'}(a)]^{m_{1}}[g^{2}(a)]^{m_{2}} \dots [g^{n}(a)]^{m_{n}}f^{k}(g(a))  g^{n+1}(a) f^{n+1}(g(a)) (f\circ g)^{n+1}(a) k\leq n+1 c[g{'}(a)]^{m_{1}}[g^{2}(a)]^{m_{2}} \dots [g^{n}(a)]^{m_{n}}[g^{n+1}(a)]^{m_{n+1}}f^{k}(g(a))  f^{n+1}(g(a)) g^{n+1}(a) f^{n}(g(x)) g^{n}(x) (f\circ g)^{n} c[g{'}(x)]^{m_{1}}[g^{2}(x)]^{m_{2}} \dots [g^{n}(x)]^{m_{n}}f^{k}(g(x))  (f\circ g)^{n} (f \circ g)^{n}(x) a \textbf{Why do we need to establish that (f\circ g)^{n}(x) exists for all x in some interval around a?} g^{n+1}(a) g^{n}(a) g^{n+1} g^{n+1}(a) = \lim_{h\to 0}\frac{g^{n}(a+h) - g^{n}(a)}{h} g^{n}(a) f^{n}(g(a)) f^{n+1}(g(a)) = \lim_{h\to 0}\frac{f^{n}(g(a) + h) - f^{n}(g(a))}{h} (f\circ g)^{n}(a) c[g{'}(a)]^{m_{1}}[g^{2}(a)]^{m_{2}} \dots [g^{n}(a)]^{m_{n}}f^{k}(g(a)) ","['analysis', 'derivatives']"
58,Which Subsets Of $\mathbf{R}^n$ Make Sense To Define Differentiable Functions On Them?,Which Subsets Of  Make Sense To Define Differentiable Functions On Them?,\mathbf{R}^n,"Just a random thought, for $A\subseteq\mathbf{R}^n$ with $0\in A$ we can define a derivative of a function $f:A\to\mathbf{R}$ at $0$ as a linear function $g:\mathbf{R}^n\to\mathbf{R}$ such that $\lim_{x\to 0}(g(x)-f(x))/\|x\|=0$ but this $g$ doesn't have to be unique.  This is the case of course when e.g. $A$ is some proper linear subspace of $\mathbf{R}^n$ , but for a less trivial example, consider $A:=\{x\in\mathbf{R}^2:|x_1|\le x_0^2\}$ and $f(x):=0$ .  Then $f$ itself is one derivative, but also $g(x):=x_1$ , as $$\frac{|g(x)-f(x)|}{\|x\|}=\frac{|x_1|}{\sqrt{x_0^2+x_1^2}}\le\frac{x_0^2}{\sqrt{x_0^2}}=x_0$$ and $\lim_{x\to 0}x_0=0$ . So, my question, what property does $A$ need to satisfy such that all functions on $A$ have at most one derivative at $0$ ?  My thought was something like there exists $\delta>0$ such that for $\epsilon>0$ there exists $P\in A^n \subseteq\mathbf{R}^{n\times n}$ with $\|P\|<\epsilon$ and $\det(P)\ge\delta\|P\|$ for some fixed matrix norm $\|.\|$ .  In other words, $A$ contains arbitrary small bases of $\mathbf{R}^n$ whose determinant is not arbitrary small in relation to the length of the vectors in that base. Is this characterization correct and/or is there a simpler way to express this property? [EDIT] Aphelli noted that $\det(P)\in O(\|P\|^n)$ so I guess it's better to say $\det(P)\ge\delta\|P\|^n$ ?","Just a random thought, for with we can define a derivative of a function at as a linear function such that but this doesn't have to be unique.  This is the case of course when e.g. is some proper linear subspace of , but for a less trivial example, consider and .  Then itself is one derivative, but also , as and . So, my question, what property does need to satisfy such that all functions on have at most one derivative at ?  My thought was something like there exists such that for there exists with and for some fixed matrix norm .  In other words, contains arbitrary small bases of whose determinant is not arbitrary small in relation to the length of the vectors in that base. Is this characterization correct and/or is there a simpler way to express this property? [EDIT] Aphelli noted that so I guess it's better to say ?","A\subseteq\mathbf{R}^n 0\in A f:A\to\mathbf{R} 0 g:\mathbf{R}^n\to\mathbf{R} \lim_{x\to 0}(g(x)-f(x))/\|x\|=0 g A \mathbf{R}^n A:=\{x\in\mathbf{R}^2:|x_1|\le x_0^2\} f(x):=0 f g(x):=x_1 \frac{|g(x)-f(x)|}{\|x\|}=\frac{|x_1|}{\sqrt{x_0^2+x_1^2}}\le\frac{x_0^2}{\sqrt{x_0^2}}=x_0 \lim_{x\to 0}x_0=0 A A 0 \delta>0 \epsilon>0 P\in A^n
\subseteq\mathbf{R}^{n\times n} \|P\|<\epsilon \det(P)\ge\delta\|P\| \|.\| A \mathbf{R}^n \det(P)\in O(\|P\|^n) \det(P)\ge\delta\|P\|^n","['analysis', 'recreational-mathematics']"
59,How do we make sense of normal derivatives in PDE at boundary points?,How do we make sense of normal derivatives in PDE at boundary points?,,"In PDE, the normal derivative on a boundary comes up a lot (defined to be the $<\nabla u, v>$ where $v$ is the unit normal vector of our domain. I want to ask how exactly do we define the gradient on boundary points because plenty of textbooks for analysis in $R^n$ explicitly state that we need open sets for partial derivatives to make sense. Is there some adapted definition for derivatives for boundary points? Now closely related to PDE and normal derivatives is Hopf lemma which I was just reading about: https://en.wikipedia.org/wiki/Hopf_lemma It states that if a harmonic functions assumes a strict maximum on a boundary point, then its normal derivative must also be strictly positive. Now if the point was an interior point, then we know the gradient must be zero since it is a extrema, but since we have boundary point, this is not the case. So intuitively, are we suppose to imagine that our domain is actually part of some larger space and our function is actually increasing in the direction of the normal at this point in this larger space?","In PDE, the normal derivative on a boundary comes up a lot (defined to be the where is the unit normal vector of our domain. I want to ask how exactly do we define the gradient on boundary points because plenty of textbooks for analysis in explicitly state that we need open sets for partial derivatives to make sense. Is there some adapted definition for derivatives for boundary points? Now closely related to PDE and normal derivatives is Hopf lemma which I was just reading about: https://en.wikipedia.org/wiki/Hopf_lemma It states that if a harmonic functions assumes a strict maximum on a boundary point, then its normal derivative must also be strictly positive. Now if the point was an interior point, then we know the gradient must be zero since it is a extrema, but since we have boundary point, this is not the case. So intuitively, are we suppose to imagine that our domain is actually part of some larger space and our function is actually increasing in the direction of the normal at this point in this larger space?","<\nabla u, v> v R^n","['analysis', 'derivatives', 'partial-differential-equations']"
60,weak-star limit of a sequence of integrable functions is an integrable function,weak-star limit of a sequence of integrable functions is an integrable function,,"Let $(f_n)_{n\geq1}\subset L^1(\mathbb{R}^d,\mathbb{C})$ be a sequence of Lebesgue integrable functions, and let $g\in L^1(\mathbb{R}^d)$ be a non-negative integrable function be such that \begin{equation} \vert f_n(x)\vert\leq g(x) \end{equation} holds for every $x\in\mathbb{R}^d$ and every $n\geq 1$ . Since the sequence $(f_n)_n$ is bounded in $L^1(\mathbb{R}^d)\subset \mathcal{M}(\mathbb{R}^d)=C_{0}(\mathbb{R}^d)^*$ , there is a finite complex measure $\mu\in\mathcal{M}(\mathbb{R}^d)$ and a subsequence $(n_k)_k$ such that $f_{n_k}\overset{\ast}{\rightharpoonup} \mu$ as $k\rightarrow\infty$ , i.e. such that $$\lim_{k\to\infty}\int_{\mathbb{R}^d}f_{n_k}(x)\varphi(x)dx = \int\varphi(x)d\mu(x)$$ holds for every $\varphi\in C_{0}(\mathbb{R}^d)$ , continuous function vanishing at infinity. How can we deduce in this particular case that $\mu$ is absolutely continuous with respect to Lebesgue measure, or equivalently that that $\mu=fdx$ for some $f\in L^1(\mathbb{R}^{d})$ ?","Let be a sequence of Lebesgue integrable functions, and let be a non-negative integrable function be such that holds for every and every . Since the sequence is bounded in , there is a finite complex measure and a subsequence such that as , i.e. such that holds for every , continuous function vanishing at infinity. How can we deduce in this particular case that is absolutely continuous with respect to Lebesgue measure, or equivalently that that for some ?","(f_n)_{n\geq1}\subset L^1(\mathbb{R}^d,\mathbb{C}) g\in L^1(\mathbb{R}^d) \begin{equation}
\vert f_n(x)\vert\leq g(x)
\end{equation} x\in\mathbb{R}^d n\geq 1 (f_n)_n L^1(\mathbb{R}^d)\subset \mathcal{M}(\mathbb{R}^d)=C_{0}(\mathbb{R}^d)^* \mu\in\mathcal{M}(\mathbb{R}^d) (n_k)_k f_{n_k}\overset{\ast}{\rightharpoonup} \mu k\rightarrow\infty \lim_{k\to\infty}\int_{\mathbb{R}^d}f_{n_k}(x)\varphi(x)dx = \int\varphi(x)d\mu(x) \varphi\in C_{0}(\mathbb{R}^d) \mu \mu=fdx f\in L^1(\mathbb{R}^{d})","['real-analysis', 'integration', 'analysis', 'measure-theory', 'lebesgue-integral']"
61,How to show positivity of Fresnel C integral?,How to show positivity of Fresnel C integral?,,"The Fresnel $C$ -integral is defined as follows. $$C(x) = \int_0^x \cos(t^2) \, dt $$ From the plot found on Wikipedia it seems to be non-negative for all $x \geq 0$ however it is not obvious to me why this is. For example, if you make the change of variables $t = u^\frac{1}{2}$ then you find $$C(x) = \frac{1}{2} \int_0^{x^2} u^{-\frac{1}{2}} \cos(u) \, du$$ My first thought was to split it into positive and negative regions and make a crude bound. For example, $\cos(u)$ will be positive on $[0,\pi/2]$ and negative on $[\pi/2,3\pi/2]$ . A crude lower bound on the first region would be $\left(\frac{\pi}{2}\right)^{-\frac{1}{2}} \int_0^\frac{\pi}{2} \cos(u) \, du$ and on the second region would be $\left(\frac{\pi}{2}\right)^{-\frac{1}{2}} \int_\frac{\pi}{2}^\frac{3\pi}{2} \cos(u) \, du$ . However adding these two bounds together yields something negative and so it doesn't work (however, this approach does work to show the non-negativity of the Fresnel $S$ -integral).","The Fresnel -integral is defined as follows. From the plot found on Wikipedia it seems to be non-negative for all however it is not obvious to me why this is. For example, if you make the change of variables then you find My first thought was to split it into positive and negative regions and make a crude bound. For example, will be positive on and negative on . A crude lower bound on the first region would be and on the second region would be . However adding these two bounds together yields something negative and so it doesn't work (however, this approach does work to show the non-negativity of the Fresnel -integral).","C C(x) = \int_0^x \cos(t^2) \, dt  x \geq 0 t = u^\frac{1}{2} C(x) = \frac{1}{2} \int_0^{x^2} u^{-\frac{1}{2}} \cos(u) \, du \cos(u) [0,\pi/2] [\pi/2,3\pi/2] \left(\frac{\pi}{2}\right)^{-\frac{1}{2}} \int_0^\frac{\pi}{2} \cos(u) \, du \left(\frac{\pi}{2}\right)^{-\frac{1}{2}} \int_\frac{\pi}{2}^\frac{3\pi}{2} \cos(u) \, du S","['analysis', 'special-functions', 'fresnel-integrals']"
62,Theorem 10.2 Rudin,Theorem 10.2 Rudin,,"$\mathscr b(X)$ denotes the set of all complex-valued, continuous, bounded functions with domain $X$ . I don't understand why is $L(h)$ equal of $\prod_{i=1}^k $ $\int_{a_i}^{b_i} h_i(x_i)dx_i$ and then why it's equal of $L'(h)$ . As I know $\int f(x)g(x)dx$ is not equal of $\int f(x) dx \int g(x) dx $ . The proof of is theorem is absolutely ununderstandable for me. I also  don't understand why is $L(g)$ equal of $L'(g)$ for all $ g $ $\in$ $\mathscr A $ . I would be grateful for any kind of help","denotes the set of all complex-valued, continuous, bounded functions with domain . I don't understand why is equal of and then why it's equal of . As I know is not equal of . The proof of is theorem is absolutely ununderstandable for me. I also  don't understand why is equal of for all . I would be grateful for any kind of help",\mathscr b(X) X L(h) \prod_{i=1}^k  \int_{a_i}^{b_i} h_i(x_i)dx_i L'(h) \int f(x)g(x)dx \int f(x) dx \int g(x) dx  L(g) L'(g)  g  \in \mathscr A ,"['abstract-algebra', 'integration']"
63,weak convergence of a function in $L^2$,weak convergence of a function in,L^2,"Suppose that $\textbf{a}:\mathbb R^n \to \mathbb R^n$ is a smooth map. We have a condition on $\textbf{a}$ : $\forall p\in \mathbb R^n,\,|\textbf{a}(p)|\le C\,(1+|p|)$ . Let $U$ be a bounded, open, connected subset in $\mathbb R^n$ . Recall that $H^1(U)=W^{1,2}(U)$ . Suppose $u,v\in H_0^1(U)$ , which is the closure of $C_c^\infty(U)$ in $H^1(U)$ . Let $\lambda>0.$ Now I want to prove that $$\int_U \textbf{a}(\mathrm Du-\lambda \mathrm Dv)\cdot \mathrm Dv \,\mathrm dx \to \int_U \textbf{a}(\mathrm Du)\cdot \mathrm Dv \,\mathrm dx \qquad\text{ as } \lambda \to 0.$$ The main problem which I encounter is that $(\mathrm Du-\lambda \mathrm Dv)(x)$ and $\mathrm Du(x)$ may not be bounded in $U$ . If both of them are bounded, then I can prove the convergence by Holder inequality and the smoothness of $\textbf{a}$ easily because we can use the mean value theorem in $\mathbb R^n$ to dominate the term $\textbf{a}(\mathrm Du-\lambda \mathrm Dv) - \textbf{a}(\mathrm Du)$ . I also thought of the norm continuity of $L^2$ . By the condition on $\textbf{a}$ , we know $\textbf{a}(\mathrm Du)$ is in $L^2(U)$ . So we know $\| \textbf{a}(\mathrm Du+h) -\textbf{a}(\mathrm Du)\|_{L^2} \to 0$ when $|h|\to 0.$ By Holder inequality, we are done. Can we regard $(\lambda Dv)(x)$ as a constant vector $h$ ? If we can, then why? Thanks. Any other method is also welcome. My purpose is to prove that $$\int_U \textbf{a}(\mathrm Du-\lambda \mathrm Dv)\cdot \mathrm Dv \,\mathrm dx \to \int_U \textbf{a}(\mathrm Du)\cdot \mathrm Dv \,\mathrm dx \qquad\text{ as } \lambda \to 0.$$ Thanks again. Edit: Here is a possibly useless condition on $\textbf{a}$ : $\textbf{a}$ is a monotone operator, which means $$(\textbf{a}(p)- \textbf{a}(q))\cdot (p-q)\ge 0$$ for all $p,q\in \mathbb R^n.$","Suppose that is a smooth map. We have a condition on : . Let be a bounded, open, connected subset in . Recall that . Suppose , which is the closure of in . Let Now I want to prove that The main problem which I encounter is that and may not be bounded in . If both of them are bounded, then I can prove the convergence by Holder inequality and the smoothness of easily because we can use the mean value theorem in to dominate the term . I also thought of the norm continuity of . By the condition on , we know is in . So we know when By Holder inequality, we are done. Can we regard as a constant vector ? If we can, then why? Thanks. Any other method is also welcome. My purpose is to prove that Thanks again. Edit: Here is a possibly useless condition on : is a monotone operator, which means for all","\textbf{a}:\mathbb R^n \to \mathbb R^n \textbf{a} \forall p\in \mathbb R^n,\,|\textbf{a}(p)|\le C\,(1+|p|) U \mathbb R^n H^1(U)=W^{1,2}(U) u,v\in H_0^1(U) C_c^\infty(U) H^1(U) \lambda>0. \int_U \textbf{a}(\mathrm Du-\lambda \mathrm Dv)\cdot \mathrm Dv \,\mathrm dx \to \int_U \textbf{a}(\mathrm Du)\cdot \mathrm Dv \,\mathrm dx \qquad\text{ as } \lambda \to 0. (\mathrm Du-\lambda \mathrm Dv)(x) \mathrm Du(x) U \textbf{a} \mathbb R^n \textbf{a}(\mathrm Du-\lambda \mathrm Dv) - \textbf{a}(\mathrm Du) L^2 \textbf{a} \textbf{a}(\mathrm Du) L^2(U) \| \textbf{a}(\mathrm Du+h) -\textbf{a}(\mathrm Du)\|_{L^2} \to 0 |h|\to 0. (\lambda Dv)(x) h \int_U \textbf{a}(\mathrm Du-\lambda \mathrm Dv)\cdot \mathrm Dv \,\mathrm dx \to \int_U \textbf{a}(\mathrm Du)\cdot \mathrm Dv \,\mathrm dx \qquad\text{ as } \lambda \to 0. \textbf{a} \textbf{a} (\textbf{a}(p)- \textbf{a}(q))\cdot (p-q)\ge 0 p,q\in \mathbb R^n.","['real-analysis', 'analysis', 'lp-spaces', 'sobolev-spaces', 'weak-convergence']"
64,"Show that $\int_1^x \log^kt\log^l\frac{x}{t} dt = xQ(\log x),$ where $Q(t)$ is a polynomial with leading term $l!t^k$",Show that  where  is a polynomial with leading term,"\int_1^x \log^kt\log^l\frac{x}{t} dt = xQ(\log x), Q(t) l!t^k","I need to show that $$\int_1^x  \log^kt\log^l\frac{x}{t} dt = xQ(\log x),$$ where $Q(t)$ is a polynomial with leading term $l!t^k$ . Here $k,l$ are nonnegative integers. I wrote $\int_1^x  \log^kt\log^l\frac{x}{t} dt = \int_1^x  \log^kt(\log x -\log t)^l dt = \sum_{j=0}^{l} (-1)^j {l \choose j} \log^{l-j}x \int_1^x \log^{k+j}t dt$ . However I calculated that $\int_1^x \log^{k+j}t dt = xq_j(\log x)$ where $q_j(t)$ is a polynomial of degree $k+j$ . Therefore $\log^{l-j}x \int_1^x \log^{k+j}t dt$ should be of the form $xR(\log x)$ where $R(t)$ is a polynomial of degree $k+l$ . So its seems like I am getting a polynomial of the wrong degree and I'm not sure where the $l!$ coefficient would come from either.",I need to show that where is a polynomial with leading term . Here are nonnegative integers. I wrote . However I calculated that where is a polynomial of degree . Therefore should be of the form where is a polynomial of degree . So its seems like I am getting a polynomial of the wrong degree and I'm not sure where the coefficient would come from either.,"\int_1^x  \log^kt\log^l\frac{x}{t} dt = xQ(\log x), Q(t) l!t^k k,l \int_1^x  \log^kt\log^l\frac{x}{t} dt = \int_1^x  \log^kt(\log x -\log t)^l dt = \sum_{j=0}^{l} (-1)^j {l \choose j} \log^{l-j}x \int_1^x \log^{k+j}t dt \int_1^x \log^{k+j}t dt = xq_j(\log x) q_j(t) k+j \log^{l-j}x \int_1^x \log^{k+j}t dt xR(\log x) R(t) k+l l!","['calculus', 'integration', 'analysis']"
65,Prove that this is a measure,Prove that this is a measure,,"I have problems with this exercise Let $X$ be an uncountable set, and let $\Sigma \subseteq{}\mathcal{P}  (X) $ the $\sigma$ -algebra. $    $$\boldsymbol{\beta} = \{A \subseteq{X} : A$ it is countable or $A^c$ it is countable $\}$ Define $\mu: \boldsymbol{\beta} \to  \overline{\mathbb{R}_{+}} $ ( $\mathbb{R}_{+} = [0, + \infty]$ ) as $\mu (A) = \left \{ \begin{matrix} 0 & \mbox{if }A\mbox{ is countable}  \\ 1 & \mbox{if }A^c\mbox{ is countable}\end{matrix}\right. $ and prove that $\mu$ is a measure. My attempt: $\mu : C \rightarrow{\overline{\mathbb{R}}}$ is a measure if: i) $\mu( \emptyset ) =0$ ii) If ${A_n}_{n \in \mathbb{N}} \subseteq{C}$ such that $ \displaystyle\bigcup_{n \in \mathbb{N}}^{}{A_n } \in C$ and the $A_n$ 's are disjoint $$\mu(\displaystyle\bigcup_{n \in \mathbb{N}}^{}{A_n }) = \displaystyle\sum_{n \in \mathbb{N}}^{}\mu(A_n)$$ i) $\mu( \emptyset ) =0$ because the cardinality of $\emptyset = 0$ ii) Let $\{A_n\mid n \in \Bbb N\}$ a collection of sets of $\boldsymbol{\beta}$ disjoint two by two. Note that, since two by two are disjoint, there can be at most one with a countable complement. Then there are two possibilities: either they are all countable (and therefore their union is countable) or there is exactly one with a countable complement and the others are countable (and therefore the union has countable complement). Thanks","I have problems with this exercise Let be an uncountable set, and let the -algebra. it is countable or it is countable Define ( ) as and prove that is a measure. My attempt: is a measure if: i) ii) If such that and the 's are disjoint i) because the cardinality of ii) Let a collection of sets of disjoint two by two. Note that, since two by two are disjoint, there can be at most one with a countable complement. Then there are two possibilities: either they are all countable (and therefore their union is countable) or there is exactly one with a countable complement and the others are countable (and therefore the union has countable complement). Thanks","X \Sigma \subseteq{}\mathcal{P}
 (X)  \sigma     \boldsymbol{\beta} = \{A \subseteq{X} : A A^c \} \mu: \boldsymbol{\beta} \to
 \overline{\mathbb{R}_{+}}  \mathbb{R}_{+} = [0, + \infty] \mu (A) = \left \{ \begin{matrix} 0 & \mbox{if }A\mbox{ is countable}
 \\ 1 & \mbox{if }A^c\mbox{ is countable}\end{matrix}\right.  \mu \mu : C \rightarrow{\overline{\mathbb{R}}} \mu( \emptyset ) =0 {A_n}_{n \in \mathbb{N}} \subseteq{C}  \displaystyle\bigcup_{n \in \mathbb{N}}^{}{A_n } \in C A_n \mu(\displaystyle\bigcup_{n \in \mathbb{N}}^{}{A_n }) = \displaystyle\sum_{n \in \mathbb{N}}^{}\mu(A_n) \mu( \emptyset ) =0 \emptyset = 0 \{A_n\mid n \in \Bbb N\} \boldsymbol{\beta}","['analysis', 'measure-theory']"
66,How do I prove $\int_a^b |f(x)|^2 dx \leq \frac{(b-a)^2}{2} \int_a^b |f'(x)|^2 dx$. [duplicate],How do I prove . [duplicate],\int_a^b |f(x)|^2 dx \leq \frac{(b-a)^2}{2} \int_a^b |f'(x)|^2 dx,"This question already has answers here : Show that $\int_{a}^{b} |f|^2\le\frac{(b-a)^2}{2}\int_{a}^{b} |f'|^2$ (2 answers) Closed 3 years ago . Let $f$ be $C^1[a,b]$ continuously differentiable) with $f(a)=0$ . [This may be generalized to $f(b)=0$ or $f(a)f(b)=0$ ] I want to show $$\int_a^b |f(x)|^2\ \mathrm{d}x\ \leq\ \frac{(b-a)^2}{2} \int_a^b  |f'(x)|^2\ \mathrm{d}x.$$ Since $f$ is differentiable, it is continuous so it is integrable.  And since $f'$ is continuous so it is integrable.  So, L.H.S and R.H.S are well defined. My first trial was the usage of integration by parts, but since I am dealing with $|\cdot |$ , the absolute value of a function, it seems it is not a good direction.","This question already has answers here : Show that $\int_{a}^{b} |f|^2\le\frac{(b-a)^2}{2}\int_{a}^{b} |f'|^2$ (2 answers) Closed 3 years ago . Let be continuously differentiable) with . [This may be generalized to or ] I want to show Since is differentiable, it is continuous so it is integrable.  And since is continuous so it is integrable.  So, L.H.S and R.H.S are well defined. My first trial was the usage of integration by parts, but since I am dealing with , the absolute value of a function, it seems it is not a good direction.","f C^1[a,b] f(a)=0 f(b)=0 f(a)f(b)=0 \int_a^b |f(x)|^2\ \mathrm{d}x\ \leq\ \frac{(b-a)^2}{2} \int_a^b
 |f'(x)|^2\ \mathrm{d}x. f f' |\cdot |",['analysis']
67,"Verification:if $I_n=[a_n,b_n]$ is sequence such that $\forall n\in\mathbb{N},I_{n+1}\subset I_n$. Show that $\bigcap_{n=0}^{+\infty}I_n\neq\emptyset$",Verification:if  is sequence such that . Show that,"I_n=[a_n,b_n] \forall n\in\mathbb{N},I_{n+1}\subset I_n \bigcap_{n=0}^{+\infty}I_n\neq\emptyset","I'd like a proof verification. I tend to forget/miss details. I hope it is right, if not I'd love to here what went wrong. Also, if you know another way to show that statement, I'd be very curious to hear about it. Statement: Let $I_n=[a_n,b_n]$ be a sequence of intervals such that $\forall n\in\mathbb{N}, I_{n+1}\subset I_n$ . Show that $\bigcap_{n=0}^{+\infty}I_n\neq\emptyset$ Proof: If $I_{n+1}\subset I_n$ , then the sequence $(I_n)_{n\in\mathbb{N}}$ is decreassing. We notice that $I_{n+1}\subset I_n\iff[a_{n+1},b_{n+1}]\subset[a_n,b_n]$ , which implies that $(a_n)_{n\in\mathbb{N}}$ is increassing and $(b_n)_{n\in\mathbb{N}}$ decreasing. Because $\mathbb{R}$ is an ordered set, $a_n\leq b_n$ , which implies that $\exists(a,b)\in(\mathbb{R}\cap I_0)^2$ such that $a_n\leq a$ and $b_n\geq b,\forall n\in\mathbb{N}$ . As $(a_n)_{n\in\mathbb{N}}$ (resp. $(b_n)_{n\in\mathbb{N}}$ ) is increasing (resp. decreasing) and has an upperbound (resp. lowerbound), both $(a_n)_{n\in\mathbb{N}}$ and $(b_n)_{n\in\mathbb{N}}$ converge and $\bigcap_{n=0}^{+\infty}I_n=[a,b]$ , where $a=$ sup $\{a_n|n\in\mathbb{N}\}$ and $b=$ inf $\{b_n|n\in\mathbb{N}\}$ . Also, if lim $_{n\rightarrow +\infty}(a_n-b_n)=0$ , then $a=b$","I'd like a proof verification. I tend to forget/miss details. I hope it is right, if not I'd love to here what went wrong. Also, if you know another way to show that statement, I'd be very curious to hear about it. Statement: Let be a sequence of intervals such that . Show that Proof: If , then the sequence is decreassing. We notice that , which implies that is increassing and decreasing. Because is an ordered set, , which implies that such that and . As (resp. ) is increasing (resp. decreasing) and has an upperbound (resp. lowerbound), both and converge and , where sup and inf . Also, if lim , then","I_n=[a_n,b_n] \forall n\in\mathbb{N}, I_{n+1}\subset I_n \bigcap_{n=0}^{+\infty}I_n\neq\emptyset I_{n+1}\subset I_n (I_n)_{n\in\mathbb{N}} I_{n+1}\subset I_n\iff[a_{n+1},b_{n+1}]\subset[a_n,b_n] (a_n)_{n\in\mathbb{N}} (b_n)_{n\in\mathbb{N}} \mathbb{R} a_n\leq b_n \exists(a,b)\in(\mathbb{R}\cap I_0)^2 a_n\leq a b_n\geq b,\forall n\in\mathbb{N} (a_n)_{n\in\mathbb{N}} (b_n)_{n\in\mathbb{N}} (a_n)_{n\in\mathbb{N}} (b_n)_{n\in\mathbb{N}} \bigcap_{n=0}^{+\infty}I_n=[a,b] a= \{a_n|n\in\mathbb{N}\} b= \{b_n|n\in\mathbb{N}\} _{n\rightarrow +\infty}(a_n-b_n)=0 a=b","['real-analysis', 'analysis', 'solution-verification']"
68,Sufficient criteria to know if a complex measure is the zero measure,Sufficient criteria to know if a complex measure is the zero measure,,"I'm trying to see if the following condition is enough to determine if a complex measure in $\mathbb{R}^n$ is the zero measure, however I dont see a clear way to handle the question. Suppose that $\int p\mathop{}\!d \mu=0$ for any complex polynomial $p$ , can we say that $\mu$ is the zero measure? Some help will be appreciated. An idea could be to see if the integral of any simple function is zero knowing that the integral of any polynomial is zero, however it doesn't seem true that a simple function is the limit of a sequence of polynomials.","I'm trying to see if the following condition is enough to determine if a complex measure in is the zero measure, however I dont see a clear way to handle the question. Suppose that for any complex polynomial , can we say that is the zero measure? Some help will be appreciated. An idea could be to see if the integral of any simple function is zero knowing that the integral of any polynomial is zero, however it doesn't seem true that a simple function is the limit of a sequence of polynomials.",\mathbb{R}^n \int p\mathop{}\!d \mu=0 p \mu,"['analysis', 'measure-theory']"
69,"If $(X,d)$ is a compact metric space then is $A$ a closed subset of $K$?",If  is a compact metric space then is  a closed subset of ?,"(X,d) A K","Since $(X,d)$ is compact it is totally bounded so we will have a finite set $A_n$ such that $\displaystyle\bigcup_{x \in A_n} B_d\left(x,\frac{1}{n}\right)$ covers $(X,d)$ . Now we take $A$ as the union of these $A_n$ . Is $A$ closed?",Since is compact it is totally bounded so we will have a finite set such that covers . Now we take as the union of these . Is closed?,"(X,d) A_n \displaystyle\bigcup_{x \in A_n} B_d\left(x,\frac{1}{n}\right) (X,d) A A_n A","['analysis', 'metric-spaces', 'compactness']"
70,"$n^{n+1} \geq (n+1)^{n} , n \in \mathbb{N}, n > 2$ Problematic induction proof,","Problematic induction proof,","n^{n+1} \geq (n+1)^{n} , n \in \mathbb{N}, n > 2","I need to prove that: $$n^{n+1} \geq (n+1)^{n} , n \in \mathbb{N}, n > 2$$ I mean it is pretty obvious that it is true, but I can not prove that with induction. for $n + 1$ I get that: $$(n+1)^{n+2} \geq (n+2)^{n+1}$$ $$(n+1)^{n}*(n+1)^2 \geq (n+2)^{n}*(n+2)$$ And I don't see how could I plug my thesis into the inequality. I could do something like this, but it doesn't get me anywhere. $$(n+1)^{n}*(n+1)^2 \geq n^{n}*(n+1)^2 \geq  (n+2)^{n}*(n+2)$$ The problem here is $(n+2)^{n}$ , I can make it only bigger to keep my inequality valid, even if I use Bernoulis inequality.","I need to prove that: I mean it is pretty obvious that it is true, but I can not prove that with induction. for I get that: And I don't see how could I plug my thesis into the inequality. I could do something like this, but it doesn't get me anywhere. The problem here is , I can make it only bigger to keep my inequality valid, even if I use Bernoulis inequality.","n^{n+1} \geq (n+1)^{n} , n \in \mathbb{N}, n > 2 n + 1 (n+1)^{n+2} \geq (n+2)^{n+1} (n+1)^{n}*(n+1)^2 \geq (n+2)^{n}*(n+2) (n+1)^{n}*(n+1)^2 \geq n^{n}*(n+1)^2 \geq  (n+2)^{n}*(n+2) (n+2)^{n}","['real-analysis', 'analysis', 'proof-writing', 'induction']"
71,"If $\sum_{n=0}^\infty a_n f(x,n) = \sum_{n=0}^\infty b_n f(x,n)$, what conditions on $f$ allow us to conclude that $a_n=b_n$ for all $n$?","If , what conditions on  allow us to conclude that  for all ?","\sum_{n=0}^\infty a_n f(x,n) = \sum_{n=0}^\infty b_n f(x,n) f a_n=b_n n","If we have an equality of the form: $$\sum_{n=0}^\infty a_n x^n = \sum_{n=0}^\infty b_n x^n $$ We can conclude that $a_n=b_n$ for all $n$ . This can be easily seen by say, differentiating term-wise and showing that $a_n=\frac{f^{(n)}(0)}{n!}$ and $b_n=\frac{g^{(n)}(0)}{n!}$ for LHS $f$ and RHS $g$ . However, it's clear that in general we do not have for any $f(x,n)$ that \begin{equation} \sum_{n=0}^\infty a_n f(x,n) = \sum_{n=0}^\infty b_n f(x,n) \implies \forall n \ a_n=b_n \end{equation} My question is, what is the most general restrictions we can put on $f$ in order for the implication above to be true? I can go through some special cases, i.e. polynomials, but I don't believe I have the background to work with general $f$ . I'm in my first undergrad analysis class, so apologies if this is a well-known result.","If we have an equality of the form: We can conclude that for all . This can be easily seen by say, differentiating term-wise and showing that and for LHS and RHS . However, it's clear that in general we do not have for any that My question is, what is the most general restrictions we can put on in order for the implication above to be true? I can go through some special cases, i.e. polynomials, but I don't believe I have the background to work with general . I'm in my first undergrad analysis class, so apologies if this is a well-known result.","\sum_{n=0}^\infty a_n x^n = \sum_{n=0}^\infty b_n x^n  a_n=b_n n a_n=\frac{f^{(n)}(0)}{n!} b_n=\frac{g^{(n)}(0)}{n!} f g f(x,n) \begin{equation}
\sum_{n=0}^\infty a_n f(x,n) = \sum_{n=0}^\infty b_n f(x,n)
\implies \forall n \ a_n=b_n
\end{equation} f f","['real-analysis', 'calculus', 'sequences-and-series', 'analysis', 'power-series']"
72,A question regarding the existence of a differentiable function with certain simple measure-theoretic properties,A question regarding the existence of a differentiable function with certain simple measure-theoretic properties,,"Does there exist a set $[a, b]\subseteq \mathbb{R}\cup\{-\infty, \infty\}$ and a continuous function $F:[a, b] \rightarrow \mathbb{R}$ such that $F^\prime(x) := \lim_{h\rightarrow 0}\frac{F(x+h)-F(x)}{h}$ exists in $\mathbb{R}\cup\{-\infty, \infty\}$ for every $x \in (a, b)$ and $\{F(x): x \in (a, b) \land |F^\prime(x)|= \infty\}$ has positive Lebesgue outer measure?",Does there exist a set and a continuous function such that exists in for every and has positive Lebesgue outer measure?,"[a, b]\subseteq \mathbb{R}\cup\{-\infty, \infty\} F:[a, b] \rightarrow \mathbb{R} F^\prime(x) := \lim_{h\rightarrow 0}\frac{F(x+h)-F(x)}{h} \mathbb{R}\cup\{-\infty, \infty\} x \in (a, b) \{F(x): x \in (a, b) \land |F^\prime(x)|= \infty\}","['real-analysis', 'analysis', 'functions', 'derivatives', 'lebesgue-measure']"
73,Properties of the sequence,Properties of the sequence,,"Given a sequence $\{a_i\}_{i\in \mathbb{Z}},$ consider the sequence defined by $b_i:=F(a_{i-1},a_{i},a_{i+1}),$ where $F:\mathbb{R}^3 \rightarrow \mathbb{R}$ is increasing in each of the variable, and $F(a,a,a)=a.$ Suppose total variation of the sequence $\{a_i\}_{i\in \mathbb{Z}}$ is finite i.e. $$\sup\limits_{k\in \mathbb{N}} \sum_{i=-k}^k |a_i-a_{i-1}| < \infty$$ then how to prove the following $$\sup\limits_{k\in \mathbb{N}} \sum_{i=-k}^k |b_i-b_{i-1}| \leq \sup\limits_{k\in \mathbb{N}} \sum_{i=-k}^k |a_i-a_{i-1}| $$ I have an intuitive idea of the proof but some how unable to give a rigorous proof. The idea is local extremas of $b_i$ are bounded below and above by some $a_m$ and $a_n$ because of the monotonicity..How to give a rigorous proof? I could not succeed with mathematical induction either.. :(","Given a sequence consider the sequence defined by where is increasing in each of the variable, and Suppose total variation of the sequence is finite i.e. then how to prove the following I have an intuitive idea of the proof but some how unable to give a rigorous proof. The idea is local extremas of are bounded below and above by some and because of the monotonicity..How to give a rigorous proof? I could not succeed with mathematical induction either.. :(","\{a_i\}_{i\in \mathbb{Z}}, b_i:=F(a_{i-1},a_{i},a_{i+1}), F:\mathbb{R}^3 \rightarrow \mathbb{R} F(a,a,a)=a. \{a_i\}_{i\in \mathbb{Z}} \sup\limits_{k\in \mathbb{N}} \sum_{i=-k}^k |a_i-a_{i-1}| < \infty \sup\limits_{k\in \mathbb{N}} \sum_{i=-k}^k |b_i-b_{i-1}| \leq \sup\limits_{k\in \mathbb{N}} \sum_{i=-k}^k |a_i-a_{i-1}|  b_i a_m a_n","['real-analysis', 'sequences-and-series', 'analysis', 'total-variation']"
74,A question about the proof of theorem 1 section 2.2.1 of the evans pde,A question about the proof of theorem 1 section 2.2.1 of the evans pde,,"My question is about the part of proof of the theorem 1 section 2.2.1 of the evans pde(p24): The only thing that I don't understand is the  inequality： $$\int_{B(0, \varepsilon)} \Phi(y) \Delta_{x} f(x-y) d y\leq C\left\|D^{2} f\right\|_{L^{\infty}\left(\mathbb{R}^{n}\right)} \int_{B(0, \varepsilon)}|\Phi(y)| d y \leq\left\{\begin{array}{ll} C \varepsilon^{2}|\log \varepsilon| & (n=2) \\ C \varepsilon^{2} & (n \geq 3) \end{array}\right.$$ $$ f \in C_{\mathrm{c}}^{2}\left(\mathbb{R}^{n}\right),\Phi(x):=\left\{\begin{array}{ll} -\frac{1}{2 \pi} \log |x| & (n=2) \\ \frac{1}{n(n-2) \alpha(n)} \frac{1}{|x|^{n-2}} & (n \geq 3) \end{array}\right.$$ I don't understand the meaning of $C$ ,it may be connected with the  inequality in p22： $$|D \Phi(x)| \leq \frac{C}{|x|^{n-1}},\left|D^{2} \Phi(x)\right| \leq \frac{C}{|x|^{n}} \quad(x \neq 0)$$ Someone can help me to  prove detailedly that inequality ？ Thanks in advance.","My question is about the part of proof of the theorem 1 section 2.2.1 of the evans pde(p24): The only thing that I don't understand is the  inequality： I don't understand the meaning of ,it may be connected with the  inequality in p22： Someone can help me to  prove detailedly that inequality ？ Thanks in advance.","\int_{B(0, \varepsilon)} \Phi(y) \Delta_{x} f(x-y) d y\leq C\left\|D^{2} f\right\|_{L^{\infty}\left(\mathbb{R}^{n}\right)} \int_{B(0, \varepsilon)}|\Phi(y)| d y \leq\left\{\begin{array}{ll}
C \varepsilon^{2}|\log \varepsilon| & (n=2) \\
C \varepsilon^{2} & (n \geq 3)
\end{array}\right.  f \in C_{\mathrm{c}}^{2}\left(\mathbb{R}^{n}\right),\Phi(x):=\left\{\begin{array}{ll}
-\frac{1}{2 \pi} \log |x| & (n=2) \\
\frac{1}{n(n-2) \alpha(n)} \frac{1}{|x|^{n-2}} & (n \geq 3)
\end{array}\right. C |D \Phi(x)| \leq \frac{C}{|x|^{n-1}},\left|D^{2} \Phi(x)\right| \leq \frac{C}{|x|^{n}} \quad(x \neq 0)","['calculus', 'analysis', 'inequality', 'partial-differential-equations', 'harmonic-functions']"
75,non measurable sets,non measurable sets,,"In Royden's text Vitali's theorem states that ""Any set E of real numbers with positive outer measure contains a subset that fails to be measurable"". So my question the Cantor set is a set of real numbers with outer measure zero does this set have a subset which fails to be measurable?","In Royden's text Vitali's theorem states that ""Any set E of real numbers with positive outer measure contains a subset that fails to be measurable"". So my question the Cantor set is a set of real numbers with outer measure zero does this set have a subset which fails to be measurable?",,['analysis']
76,Weakly convex functions in the sense of support functions,Weakly convex functions in the sense of support functions,,"Let $I\subset\mathbb{R}$ be an interval, $f:I\to\mathbb{R}$ a continuous function. $f$ is called convex if for any $x,y\in I$ , $t\in[0,1]$ , we have $$f((1-t)x+ty)\leq (1-t)f(x)+tf(y).$$ $f$ is called weakly convex in the distribution sense if $f''\geq0$ as a distribution. $f$ is called weakly convex in the support sense if for any $p\in I$ , $\varepsilon>0$ , there exists a neighborhood $U$ of $p$ in $I$ and a $C^2$ function $f_\varepsilon:U\to\mathbb{R}$ such that (1) $f_\varepsilon''(p)\geq-\varepsilon$ ; (2) $f\geq f_\varepsilon$ on $U$ , with equality at $p$ . How are these notions related? Of course all three are equivalent when $f$ is $C^2$ . But otherwise I'm unable to prove or disprove the implications. In particular, I need the fact convexity is equivalent to weak convexity in the sense of support functions (which I can't say for sure is correct). Edit: It is Exercise 7.5.3 in Peter Petersen's Riemannian Geometry to prove that being weakly convex in the support sense is equivalent to being convex. I especially need this result because this idea of using support functions has been used repeatedly later in the book. Thanks in advance!","Let be an interval, a continuous function. is called convex if for any , , we have is called weakly convex in the distribution sense if as a distribution. is called weakly convex in the support sense if for any , , there exists a neighborhood of in and a function such that (1) ; (2) on , with equality at . How are these notions related? Of course all three are equivalent when is . But otherwise I'm unable to prove or disprove the implications. In particular, I need the fact convexity is equivalent to weak convexity in the sense of support functions (which I can't say for sure is correct). Edit: It is Exercise 7.5.3 in Peter Petersen's Riemannian Geometry to prove that being weakly convex in the support sense is equivalent to being convex. I especially need this result because this idea of using support functions has been used repeatedly later in the book. Thanks in advance!","I\subset\mathbb{R} f:I\to\mathbb{R} f x,y\in I t\in[0,1] f((1-t)x+ty)\leq (1-t)f(x)+tf(y). f f''\geq0 f p\in I \varepsilon>0 U p I C^2 f_\varepsilon:U\to\mathbb{R} f_\varepsilon''(p)\geq-\varepsilon f\geq f_\varepsilon U p f C^2","['real-analysis', 'analysis']"
77,Jensen's Inequality in $L^1$.,Jensen's Inequality in .,L^1,"Let be $(X,A,μ)$ a probability space and let $\textit{f}\in L^{1}(X)$ a real function such that $-\infty<a<f(x)<b<\inf$ for all $x\in X$ . Let $\varphi:(a,b)\rightarrow \mathbf{R}$ be a convex function. Prove the so called Jensen's inequality: 􏰇 $$\varphi(\int_{X}\textit{f}d\mu)=\int_{X}\varphi(\textit{f})d\mu. $$ With the help of this inequality prove that if $h:X\rightarrow[0,\infty)$ is a measurable function, then $$\sqrt{1+\left(\int_{X}h\,d\mu\right)^2}\le\int_{X}\sqrt{1+h^2}\,d\mu\le1+\int_{X}h\,d\mu$$ -I've just proved in general the Jensen's inequality $\textit{f}:X\rightarrow \mathbb{R}$ if $f $ is $\mu$ -integrable and $\varphi$ convex and with $\operatorname {Dom}(\varphi)=\mathbb{R} but here I have different hypothesis. How could I do? Thanks you very much!!","Let be a probability space and let a real function such that for all . Let be a convex function. Prove the so called Jensen's inequality: 􏰇 With the help of this inequality prove that if is a measurable function, then -I've just proved in general the Jensen's inequality if is -integrable and convex and with $\operatorname {Dom}(\varphi)=\mathbb{R} but here I have different hypothesis. How could I do? Thanks you very much!!","(X,A,μ) \textit{f}\in L^{1}(X) -\infty<a<f(x)<b<\inf x\in X \varphi:(a,b)\rightarrow \mathbf{R} \varphi(\int_{X}\textit{f}d\mu)=\int_{X}\varphi(\textit{f})d\mu.  h:X\rightarrow[0,\infty) \sqrt{1+\left(\int_{X}h\,d\mu\right)^2}\le\int_{X}\sqrt{1+h^2}\,d\mu\le1+\int_{X}h\,d\mu \textit{f}:X\rightarrow \mathbb{R} f  \mu \varphi","['analysis', 'inequality']"
78,Is my argumentation for proving that a monotonic sequence is convergent if and only if it is bounded enough?,Is my argumentation for proving that a monotonic sequence is convergent if and only if it is bounded enough?,,"I am studying for my first exam in Analysis 1 and one I have to know how to prove that a monotonic sequence in $\mathbb{R}$ is convergent if and only if it is bounded. I start by proving the first implication, thus that monotonic and convergent => bounded. This is easy as I am allowed to use an earlier proof from my book to say that any convergent sequence in $\mathbb{R^n}$ is bounded. However the other implication gets a little bit trickier. I know that I have to prove that monotonic and bounded => convergent. I first start by assuming that the sequence $(a_k)_{k=1}^\infty$ is increasing. Thus the set $\{a_k : k \in \mathbb{N}$ } of the values of the sequence is non-empty and bounded and therefore it has a well-defined supremum. I let $a=sup\{a_k : k \in \mathbb{N}\}$ and claim that the sequence converges to a. Thus I let $\epsilon > 0 $ and use the definiton of convergence $\forall \epsilon > 0 \exists N \in \mathbb{N} : k \geq N \rightarrow ||a_k-a||< \epsilon$ which I negate for a contradiction, ie. $\exists \epsilon > 0 \forall N \in \mathbb{N} : \exists k \geq N$ and $a-a_k \geq \epsilon $ .' Then I create a new, strictly positive sequence $(k_j)_{j=1}^\infty$ with $a-a_{kj} \geq \epsilon $ and $j \in \mathbb{N}$ but why does it need to be strictly positive and not just positive. Does this make a difference? I now let $N=1$ to obtain that $k_1 \geq 1$ . By constructing $k_{j-1}$ it immediately follows that $k_j \geq k_{j-1} + 1 > k_{j-1}$ . Is this because that I have just seen that for $N=1$ that $k_1 \geq 1$ which means that $k_j \geq k_{j-1} +1$ as $j \in \mathbb{N}$ ? I am not sure how to explain it .. As I now have constructed a strictly positive sequence I know that $\forall k \in \mathbb{N}$ I can find a $j \in \mathbb{N}$ such that $k \leq k_j$ but why is this? My friend tried to explain it is because that I first pick a $k$ and then afterwards I can pick a $j$ and therefore $k \leq k_j$ . Likewise this means that, by induction, $a_k \leq a_{kj}$ . Can this also be explained by saying that the sequences are positive? Or only by induction? I dont have time for that if I have to present this subject for my professor. Now I know this means that $a-a_k \geq a-a_{kj} \geq \epsilon $ but then $a-a_k \geq \epsilon $ and solving for $a_k$ we obtain that $a-\epsilon \geq a_k$ implying that this is an upper bound for the sequence contradicting the fact that $a = \sup \{a_k \ k \in \mathbb{N}$ } which completes the proof. I hope you can clarify my questions. Thanks in advance.","I am studying for my first exam in Analysis 1 and one I have to know how to prove that a monotonic sequence in is convergent if and only if it is bounded. I start by proving the first implication, thus that monotonic and convergent => bounded. This is easy as I am allowed to use an earlier proof from my book to say that any convergent sequence in is bounded. However the other implication gets a little bit trickier. I know that I have to prove that monotonic and bounded => convergent. I first start by assuming that the sequence is increasing. Thus the set } of the values of the sequence is non-empty and bounded and therefore it has a well-defined supremum. I let and claim that the sequence converges to a. Thus I let and use the definiton of convergence which I negate for a contradiction, ie. and .' Then I create a new, strictly positive sequence with and but why does it need to be strictly positive and not just positive. Does this make a difference? I now let to obtain that . By constructing it immediately follows that . Is this because that I have just seen that for that which means that as ? I am not sure how to explain it .. As I now have constructed a strictly positive sequence I know that I can find a such that but why is this? My friend tried to explain it is because that I first pick a and then afterwards I can pick a and therefore . Likewise this means that, by induction, . Can this also be explained by saying that the sequences are positive? Or only by induction? I dont have time for that if I have to present this subject for my professor. Now I know this means that but then and solving for we obtain that implying that this is an upper bound for the sequence contradicting the fact that } which completes the proof. I hope you can clarify my questions. Thanks in advance.",\mathbb{R} \mathbb{R^n} (a_k)_{k=1}^\infty \{a_k : k \in \mathbb{N} a=sup\{a_k : k \in \mathbb{N}\} \epsilon > 0  \forall \epsilon > 0 \exists N \in \mathbb{N} : k \geq N \rightarrow ||a_k-a||< \epsilon \exists \epsilon > 0 \forall N \in \mathbb{N} : \exists k \geq N a-a_k \geq \epsilon  (k_j)_{j=1}^\infty a-a_{kj} \geq \epsilon  j \in \mathbb{N} N=1 k_1 \geq 1 k_{j-1} k_j \geq k_{j-1} + 1 > k_{j-1} N=1 k_1 \geq 1 k_j \geq k_{j-1} +1 j \in \mathbb{N} \forall k \in \mathbb{N} j \in \mathbb{N} k \leq k_j k j k \leq k_j a_k \leq a_{kj} a-a_k \geq a-a_{kj} \geq \epsilon  a-a_k \geq \epsilon  a_k a-\epsilon \geq a_k a = \sup \{a_k \ k \in \mathbb{N},['analysis']
79,Proof of irrationality of infinite continued fractions,Proof of irrationality of infinite continued fractions,,We have the identity $$\tan x=\frac{x}{1+\underset{n=1}{\overset{\infty}{\mathrm K}} \frac{-x^2}{2n+1}}.$$ From the Wikipedia article on Proof that π is irrational : [...] Lambert proved that if x is non-zero and rational then this expression must be irrational. But how can we prove Lambert's assertion? I couldn't find any resource containing the proof.,We have the identity From the Wikipedia article on Proof that π is irrational : [...] Lambert proved that if x is non-zero and rational then this expression must be irrational. But how can we prove Lambert's assertion? I couldn't find any resource containing the proof.,\tan x=\frac{x}{1+\underset{n=1}{\overset{\infty}{\mathrm K}} \frac{-x^2}{2n+1}}.,"['calculus', 'sequences-and-series', 'analysis', 'trigonometry', 'pi']"
80,"Prove that the series $\sum_{x∈X}(f(x) + g(x))$ is absolutely convergent, and $ \sum_{x∈X}(f(x) + g(x)) = \sum_{x∈X}f(x) + \sum_{x∈X}g(x)$","Prove that the series  is absolutely convergent, and",\sum_{x∈X}(f(x) + g(x))  \sum_{x∈X}(f(x) + g(x)) = \sum_{x∈X}f(x) + \sum_{x∈X}g(x),"Let $X$ be an arbitrary set (possibly uncountable), and let $f:X → R$ and $g: X → R$ be functions such that the series $\sum_{x∈X} f(x)$ and $\sum_{x∈X} g(x)$ are both absolutely convergent.   Prove: The series $\sum_{x∈X}(f(x) + g(x))$ is absolutely convergent, and $$ \sum_{x∈X}(f(x) + g(x)) = \sum_{x∈X}f(x) + \sum_{x∈X}g(x)$$ . If the set $X$ is finite then I have the result. If it is countable, then $h: N \to X$ is a bijection and $\sum_{n=0}^{\infty} (f+g)(h(n))$ is absolutely convergent by definition. In other words, $\sum_{n=0}^{\infty} |f(h(n)+g(h(n))| = L$ . Since it is known that $\sum_{x∈X} f(x)$ and $\sum_{x∈X} g(x)$ are both absolutely convergent then $\sum_{x∈X} |f(x)| = M$ and $\sum_{x∈X} |g(x)|=K$ . I don't know how to proceed. It seems to me I found a solution for the uncountable case here Proving Proposition 8.2.6 from Terence Tao's Analysis I but still I got the problem with the countable one.(Frankly speaking I am not entirly getting the uncountable either).","Let be an arbitrary set (possibly uncountable), and let and be functions such that the series and are both absolutely convergent.   Prove: The series is absolutely convergent, and . If the set is finite then I have the result. If it is countable, then is a bijection and is absolutely convergent by definition. In other words, . Since it is known that and are both absolutely convergent then and . I don't know how to proceed. It seems to me I found a solution for the uncountable case here Proving Proposition 8.2.6 from Terence Tao's Analysis I but still I got the problem with the countable one.(Frankly speaking I am not entirly getting the uncountable either).",X f:X → R g: X → R \sum_{x∈X} f(x) \sum_{x∈X} g(x) \sum_{x∈X}(f(x) + g(x))  \sum_{x∈X}(f(x) + g(x)) = \sum_{x∈X}f(x) + \sum_{x∈X}g(x) X h: N \to X \sum_{n=0}^{\infty} (f+g)(h(n)) \sum_{n=0}^{\infty} |f(h(n)+g(h(n))| = L \sum_{x∈X} f(x) \sum_{x∈X} g(x) \sum_{x∈X} |f(x)| = M \sum_{x∈X} |g(x)|=K,"['real-analysis', 'sequences-and-series', 'analysis', 'elementary-set-theory']"
81,How to solve this equation for x?,How to solve this equation for x?,,"can somebody help me with this? $$2=2^{1-x}+(\frac{2}{3})^{1-x}$$ I guess I somehow have to isolate this 1-x term and then use the ln. But I don't get how.. Thanks in advance! Mostly irrelevant side quest: I have the (paid) Wolframalpha app which is supposed to show step-by-step solutions. However, 9/10 times it says ""step-by-step solution unavailable"". Anyone knows what is up with that?","can somebody help me with this? I guess I somehow have to isolate this 1-x term and then use the ln. But I don't get how.. Thanks in advance! Mostly irrelevant side quest: I have the (paid) Wolframalpha app which is supposed to show step-by-step solutions. However, 9/10 times it says ""step-by-step solution unavailable"". Anyone knows what is up with that?",2=2^{1-x}+(\frac{2}{3})^{1-x},"['analysis', 'logarithms']"
82,How to prove that $\limsup_{n\to\infty}n M_n\geq 1/\ln 2$ for the following sequence?,How to prove that  for the following sequence?,\limsup_{n\to\infty}n M_n\geq 1/\ln 2,"Let $(x_i)_i$ be a sequence of distinct numbers in $[0,1]$ . Note that $[0, 1] \setminus \{x_1, \cdots, x_{n-1} \}$ can be written as a disjoint union of non-empty and non-singleton intervals $C_{n, k}$ . Let $M_n \equiv \max_k |C_{n,k}|$ . How do I prove that $\limsup\limits_{n \to \infty} n M_n\geq 1/\ln 2$ ?",Let be a sequence of distinct numbers in . Note that can be written as a disjoint union of non-empty and non-singleton intervals . Let . How do I prove that ?,"(x_i)_i [0,1] [0, 1] \setminus \{x_1, \cdots, x_{n-1} \} C_{n, k} M_n \equiv \max_k |C_{n,k}| \limsup\limits_{n \to \infty} n M_n\geq 1/\ln 2","['real-analysis', 'sequences-and-series', 'analysis', 'limsup-and-liminf']"
83,"Show that $\int_{\mathbb R^d}(|f_n|^p-|f_n-f|^p)\,dx\to\int_{\mathbb R^d}|f|^p\,dx.$",Show that,"\int_{\mathbb R^d}(|f_n|^p-|f_n-f|^p)\,dx\to\int_{\mathbb R^d}|f|^p\,dx.","Let $1\leq p<\infty$ . Suppose $\{f_n\}\subset L^p(\mathbb R^d)$ satisfies $\int_{\mathbb R^d} |f_n|^p\,dx\leq M$ and $f_n\to f$ for almost every $x\in\mathbb R^d$ . Show that $$\int_{\mathbb R^d}(|f_n|^p-|f_n-f|^p)\,dx\to\int_{\mathbb R^d}|f|^p\,dx.$$ Applying Fatou's lemma I get that $\int_{\mathbb R^d}|f|^p\,dx\leq \liminf_{n\to\infty} \int_{\mathbb R^d} |f_n|^p\,dx\leq M$ , so $f\in L^p(\mathbb R^d)$ . But I can't move forward. If I had $\int_{\mathbb R^d} |f_n|^p\,dx\to \int_{\mathbb R^d}|f|^p\,dx$ , I could prove that $\int_{\mathbb R^d}|f_n-f|^p\,dx\to 0$ so we are done. But it is not always true. Any help please?","Let . Suppose satisfies and for almost every . Show that Applying Fatou's lemma I get that , so . But I can't move forward. If I had , I could prove that so we are done. But it is not always true. Any help please?","1\leq p<\infty \{f_n\}\subset L^p(\mathbb R^d) \int_{\mathbb R^d} |f_n|^p\,dx\leq M f_n\to f x\in\mathbb R^d \int_{\mathbb R^d}(|f_n|^p-|f_n-f|^p)\,dx\to\int_{\mathbb R^d}|f|^p\,dx. \int_{\mathbb R^d}|f|^p\,dx\leq \liminf_{n\to\infty} \int_{\mathbb R^d} |f_n|^p\,dx\leq M f\in L^p(\mathbb R^d) \int_{\mathbb R^d} |f_n|^p\,dx\to \int_{\mathbb R^d}|f|^p\,dx \int_{\mathbb R^d}|f_n-f|^p\,dx\to 0","['real-analysis', 'analysis']"
84,Prove that $inf(S_1)=sup(S_2)$,Prove that,inf(S_1)=sup(S_2),"$f:I \to \Bbb R$ , $I=[0,1]$ continuous and differentiable on $(0,1)$ s.t $f(0)<0<f(1)$ and $f'(x)\neq 0$ $\forall x \in (0,1)$ . Let $S_1=\{x \in I | f(x) >0\}$ and $S_2=\{x \in I | f(x) <0\}$ . Prove that $inf(S_1)=sup(S_2)$ . My attempt: If $\exists x\in I$ s.t $f(x)<f(0)$ then $\exists c\in I$ s.t $f(c)=f(0)$ contradiction by Rolle's thm. So $f'(0)>0$ Claim: $f'(x)>0$ , $\forall x \in (0,1)$ Suppose not then $\exists x_1\in I$ s.t $f'(x_1)<0$ . Consider, $d=inf\{x\in I|f'(x)<0\}$ If we can prove that $d \neq 0$ then $\exists a,b\in (d-\epsilon,d+\epsilon)\subseteq [0,1]$ s.t $f(a)=f(b)$ contradiction by Rolle's thm. So, $f'(x)>0$ , $\forall x \in (0,1)$ $\Rightarrow inf(S_1)=sup(S_2)$ . Done. My question is: How can we prove that $d \neq 0$ ?",", continuous and differentiable on s.t and . Let and . Prove that . My attempt: If s.t then s.t contradiction by Rolle's thm. So Claim: , Suppose not then s.t . Consider, If we can prove that then s.t contradiction by Rolle's thm. So, , . Done. My question is: How can we prove that ?","f:I \to \Bbb R I=[0,1] (0,1) f(0)<0<f(1) f'(x)\neq 0 \forall x \in (0,1) S_1=\{x \in I | f(x) >0\} S_2=\{x \in I | f(x) <0\} inf(S_1)=sup(S_2) \exists x\in I f(x)<f(0) \exists c\in I f(c)=f(0) f'(0)>0 f'(x)>0 \forall x \in (0,1) \exists x_1\in I f'(x_1)<0 d=inf\{x\in I|f'(x)<0\} d \neq 0 \exists a,b\in (d-\epsilon,d+\epsilon)\subseteq [0,1] f(a)=f(b) f'(x)>0 \forall x \in (0,1) \Rightarrow inf(S_1)=sup(S_2) d \neq 0","['real-analysis', 'calculus', 'analysis', 'derivatives', 'continuity']"
85,Improper Integrals in Analysis,Improper Integrals in Analysis,,"Let $f:[0,\infty) \rightarrow \mathbb{R}$ be a continuous function and let $g(x)=\frac{1}{x}\int_1^x f(t)dt$ ; $x>0$ . Assume that $\lim_{x\rightarrow \infty} g(x)=B$ exists. Let $0 < a < b$ be two fixed numbers. Show $$\lim_{T\rightarrow \infty}\int_{Ta}^{Tb}\frac{f(x)}{x}dx=B\ln\left(\frac{b}{a}\right)$$ Here's my partial solution: $g(x)=\frac{1}{x}\left(F(x)-F(1)\right) \Rightarrow f(x)=g(x)+xg'(x)$ , Therefore: \begin{align*} \lim_{T\rightarrow \infty}\int_{Ta}^{Tb}\frac{f(x)}{x}dx&=\lim_{T\rightarrow \infty}\int_{Ta}^{Tb}\frac{g(x)}{x}dx+\int_{Ta}^{Tb}g'(x)dx\\ &=\lim_{T\rightarrow\infty}\ln(Tb)g(Tb)-\ln(Ta)g(Ta)-\int_{Ta}^{Tb}g'(x)\ln(x)+\int_{Ta}^{Tb}g'(x)dx\\ &=\lim_{T\rightarrow \infty}B\ln\left(\frac{Tb}{Ta}\right)-\int_{Ta}^{Tb}g'(x)\ln(x)+\int_{Ta}^{Tb}g'(x)dx\\ &=B\ln\left(\frac{b}{a}\right)+\dots \end{align*} I can see how $\lim_{T\rightarrow\infty}\int_{Ta}^{Tb}g'(x)dx=\lim_{T\rightarrow \infty}g(Tb)-g(Ta)=0$ but I can't see to make the $-\int_{Ta}^{Tb}g'(x)\ln(x)$ go to zero. Any help on that?","Let be a continuous function and let ; . Assume that exists. Let be two fixed numbers. Show Here's my partial solution: , Therefore: I can see how but I can't see to make the go to zero. Any help on that?","f:[0,\infty) \rightarrow \mathbb{R} g(x)=\frac{1}{x}\int_1^x f(t)dt x>0 \lim_{x\rightarrow \infty} g(x)=B 0 < a < b \lim_{T\rightarrow \infty}\int_{Ta}^{Tb}\frac{f(x)}{x}dx=B\ln\left(\frac{b}{a}\right) g(x)=\frac{1}{x}\left(F(x)-F(1)\right) \Rightarrow f(x)=g(x)+xg'(x) \begin{align*}
\lim_{T\rightarrow \infty}\int_{Ta}^{Tb}\frac{f(x)}{x}dx&=\lim_{T\rightarrow \infty}\int_{Ta}^{Tb}\frac{g(x)}{x}dx+\int_{Ta}^{Tb}g'(x)dx\\
&=\lim_{T\rightarrow\infty}\ln(Tb)g(Tb)-\ln(Ta)g(Ta)-\int_{Ta}^{Tb}g'(x)\ln(x)+\int_{Ta}^{Tb}g'(x)dx\\
&=\lim_{T\rightarrow \infty}B\ln\left(\frac{Tb}{Ta}\right)-\int_{Ta}^{Tb}g'(x)\ln(x)+\int_{Ta}^{Tb}g'(x)dx\\
&=B\ln\left(\frac{b}{a}\right)+\dots
\end{align*} \lim_{T\rightarrow\infty}\int_{Ta}^{Tb}g'(x)dx=\lim_{T\rightarrow \infty}g(Tb)-g(Ta)=0 -\int_{Ta}^{Tb}g'(x)\ln(x)","['real-analysis', 'calculus', 'analysis', 'improper-integrals']"
86,Proof that a sequence is Cauchy.,Proof that a sequence is Cauchy.,,"Show that $\left(  x_{n}\right)  $ is a Cauchy sequence, where $$ x_{n}=\frac{\sin1}{2}+\frac{\sin2}{2^{2}}+\ldots+\frac{\sin n}{2^{n}}. $$ We try to evaluate $\left\vert x_{n+p}-x_{n}\right\vert $ and to show that this one is arbitrary small. So \begin{align*} \left\vert a_{n+p}-a_{n}\right\vert  & =\left\vert \frac{\sin\left( n+1\right)  }{2^{n+1}}+\frac{\sin\left(  n+2\right)  }{2^{n+2}}+\ldots +\frac{\sin\left(  n+p\right)  }{2^{n+p}}\right\vert \\ & \leq\frac{1}{2^{n+1}}+\frac{1}{2^{n+2}}+\ldots+\frac{1}{2^{n+p}}. \end{align*} Now, $\left(  \frac{1}{2^{n+j}}\right)  $ converge all to zero. Hence $\left\vert a_{n+p}-a_{n}\right\vert \rightarrow0$ .  Does it work this argument according to the definition of Cauchy sequence $$ \forall\varepsilon>0, \quad \exists n_{\varepsilon}\in \mathbb{N} , \quad \forall n\in \mathbb{N} , \quad \forall p\in \mathbb{N} :n,p\geq n_{\varepsilon}\Rightarrow\left\vert a_{n+p}-a_{n}\right\vert <\varepsilon? $$ My question was born from the fact that for the sequence $$ a_{n}=1+\frac{1}{2}+\ldots+\frac{1}{n}, $$ we have that \begin{align*} \left\vert a_{n+p}-a_{n}\right\vert  & =\left\vert \frac{1}{n+1}+\frac{1}{n+2}+\ldots+\frac{1}{n+p}\right\vert \\ & \leq\frac{1}{n+1}+\frac{1}{n+2}+\ldots+\frac{1}{n+p} \end{align*} and also all sequences $\left(  \frac{1}{n+j}\right)  _{n}$ converge to zero, but this time $\left(  a_{n}\right)  $ is not a Cauchy sequence.","Show that is a Cauchy sequence, where We try to evaluate and to show that this one is arbitrary small. So Now, converge all to zero. Hence .  Does it work this argument according to the definition of Cauchy sequence My question was born from the fact that for the sequence we have that and also all sequences converge to zero, but this time is not a Cauchy sequence.","\left(  x_{n}\right)   
x_{n}=\frac{\sin1}{2}+\frac{\sin2}{2^{2}}+\ldots+\frac{\sin n}{2^{n}}.
 \left\vert x_{n+p}-x_{n}\right\vert  \begin{align*}
\left\vert a_{n+p}-a_{n}\right\vert  & =\left\vert \frac{\sin\left(
n+1\right)  }{2^{n+1}}+\frac{\sin\left(  n+2\right)  }{2^{n+2}}+\ldots
+\frac{\sin\left(  n+p\right)  }{2^{n+p}}\right\vert \\
& \leq\frac{1}{2^{n+1}}+\frac{1}{2^{n+2}}+\ldots+\frac{1}{2^{n+p}}.
\end{align*} \left(  \frac{1}{2^{n+j}}\right)   \left\vert a_{n+p}-a_{n}\right\vert \rightarrow0 
\forall\varepsilon>0,
\quad
\exists n_{\varepsilon}\in
\mathbb{N}
,
\quad
\forall n\in
\mathbb{N}
,
\quad
\forall p\in
\mathbb{N}
:n,p\geq n_{\varepsilon}\Rightarrow\left\vert a_{n+p}-a_{n}\right\vert
<\varepsilon?
 
a_{n}=1+\frac{1}{2}+\ldots+\frac{1}{n},
 \begin{align*}
\left\vert a_{n+p}-a_{n}\right\vert  & =\left\vert \frac{1}{n+1}+\frac{1}{n+2}+\ldots+\frac{1}{n+p}\right\vert \\
& \leq\frac{1}{n+1}+\frac{1}{n+2}+\ldots+\frac{1}{n+p}
\end{align*} \left(  \frac{1}{n+j}\right)  _{n} \left(  a_{n}\right)  ",['analysis']
87,"Is $\sum_{n,m \in \mathbb Z^2} e^{-\Vert n-m \Vert} \frac{1}{1+\Vert n \Vert^{1+\varepsilon}} \frac{1}{1+\Vert m \Vert^{1+\varepsilon}}$ summable?",Is  summable?,"\sum_{n,m \in \mathbb Z^2} e^{-\Vert n-m \Vert} \frac{1}{1+\Vert n \Vert^{1+\varepsilon}} \frac{1}{1+\Vert m \Vert^{1+\varepsilon}}","I would like to ask whether the expression $$\sum_{n,m \in \mathbb Z^2} e^{-\Vert n-m \Vert} \frac{1}{1+\Vert n \Vert^{1+\varepsilon}} \frac{1}{1+\Vert m \Vert^{1+\varepsilon}}$$ is finite? Intuitively, this should be the case as away from the diagonal $n=m$ the exponential is rapidly decaying and on the diagonal, this expression is summable, but I cannot make it rigorous. EDIT: The sum is over $n$ and $m$ both in $\mathbb Z^2.$","I would like to ask whether the expression is finite? Intuitively, this should be the case as away from the diagonal the exponential is rapidly decaying and on the diagonal, this expression is summable, but I cannot make it rigorous. EDIT: The sum is over and both in","\sum_{n,m \in \mathbb Z^2} e^{-\Vert n-m \Vert} \frac{1}{1+\Vert n \Vert^{1+\varepsilon}} \frac{1}{1+\Vert m \Vert^{1+\varepsilon}} n=m n m \mathbb Z^2.","['real-analysis', 'calculus']"
88,How to demonstrate that a differentiable function $f: \mathbb Q\rightarrow \mathbb Q$ is not uniformly continuous,How to demonstrate that a differentiable function  is not uniformly continuous,f: \mathbb Q\rightarrow \mathbb Q,"Following the scheme given by mercio in the answer to this question , one can construct a function $f: \mathbb Q \rightarrow \mathbb Q$ , differentiable everywhere, with derivative function $$ f'(x) = \begin{cases} 0, & (x \neq 0),\\ 1, & (x = 0). \end{cases}\tag{1}\label{one} $$ Such function cannot be uniformly continuous, because otherwise, in any closed interval containing $0$ , $f$ could be extended to a continuous function on $\mathbb R$ . Such extended function would give \begin{equation} \lim_{h \rightarrow 0} \frac{f(h)-f(0)}{h} = 1 \end{equation} and, at the same time, \begin{equation} \lim_{x\rightarrow 0}f'(x) = 0, \end{equation} which would contradict de l'Hôpital's rule. My question is whether it is possible or not to demonstrate that $f$ is not uniformly continuous by using only condition \eqref{one} and the definition of uniform continuity, i.e. (for the sake of completeness) \begin{equation} \forall \epsilon,\ \exists \delta:\ \forall x,\ y \in \mathbb Q,\ |x-y|<\delta \Rightarrow |f(x)-f(y)|<\epsilon . \end{equation} EDIT : Wojowu pointed out to me that the function may not be differentiable once extended on $\mathbb R$ (thus not contradicting de l'Hôpital's rule) . So either $f$ is not uniformly continuous or its extension to the reals fails to be differentiable. So the question now is whether this can be estabished only by means of \eqref{one} or not, and how.","Following the scheme given by mercio in the answer to this question , one can construct a function , differentiable everywhere, with derivative function Such function cannot be uniformly continuous, because otherwise, in any closed interval containing , could be extended to a continuous function on . Such extended function would give and, at the same time, which would contradict de l'Hôpital's rule. My question is whether it is possible or not to demonstrate that is not uniformly continuous by using only condition \eqref{one} and the definition of uniform continuity, i.e. (for the sake of completeness) EDIT : Wojowu pointed out to me that the function may not be differentiable once extended on (thus not contradicting de l'Hôpital's rule) . So either is not uniformly continuous or its extension to the reals fails to be differentiable. So the question now is whether this can be estabished only by means of \eqref{one} or not, and how.","f: \mathbb Q \rightarrow \mathbb Q 
f'(x) = \begin{cases}
0, & (x \neq 0),\\
1, & (x = 0).
\end{cases}\tag{1}\label{one}
 0 f \mathbb R \begin{equation}
\lim_{h \rightarrow 0} \frac{f(h)-f(0)}{h} = 1
\end{equation} \begin{equation}
\lim_{x\rightarrow 0}f'(x) = 0,
\end{equation} f \begin{equation}
\forall \epsilon,\ \exists \delta:\ \forall x,\ y \in \mathbb Q,\ |x-y|<\delta \Rightarrow |f(x)-f(y)|<\epsilon .
\end{equation} \mathbb R f","['real-analysis', 'analysis']"
89,Show some theorem concerning of the uniform convergence on compacts of a sequence of polynomials of order $<k$,Show some theorem concerning of the uniform convergence on compacts of a sequence of polynomials of order,<k,"Let $k,s\in \mathbb N$ , let $x_0,x_1,...,x_s$ be given pairweise different real numbers, let $m_0,m_1,...,m_s$ be given nonnegative integers such that $\sum_{i=0}^s m_i=k$ , and let $$ P_n(x)=a_{n0}+a_{n1}x+...+a_{n,k-1}x^{k-1} (\textrm{ for } n\in \mathbb N, x\in \mathbb R) $$ be a sequence of real polynomials of order $\leq k-1$ . Assume that there exist limits of derivatives: $$ \lim_{n\rightarrow \infty} P_n^{(j)}(x_0) (\textrm{ for } j=0,1,...,m_0); $$ $$ \lim_{n\rightarrow \infty} P_n^{(j)}(x_1) (\textrm{ for }   j=0,1,...,m_1); $$ ........................... $$ \lim_{n\rightarrow \infty} P_n^{(j)}(x_s) (\textrm{ for }  j=0,1,...,m_s). $$ I wish to know that $P_n(x)$ is uniformly convergent on compact intervals. It would be sufficient to show that there exist limits: $$ \lim_{n\rightarrow \infty} a_{nj}       (\textrm{ for } j=0,...,k-1). $$ Maybe proof or references. Thanks.","Let , let be given pairweise different real numbers, let be given nonnegative integers such that , and let be a sequence of real polynomials of order . Assume that there exist limits of derivatives: ........................... I wish to know that is uniformly convergent on compact intervals. It would be sufficient to show that there exist limits: Maybe proof or references. Thanks.","k,s\in \mathbb N x_0,x_1,...,x_s m_0,m_1,...,m_s \sum_{i=0}^s m_i=k 
P_n(x)=a_{n0}+a_{n1}x+...+a_{n,k-1}x^{k-1} (\textrm{ for } n\in \mathbb N, x\in \mathbb R)
 \leq k-1 
\lim_{n\rightarrow \infty} P_n^{(j)}(x_0) (\textrm{ for } j=0,1,...,m_0);
 
\lim_{n\rightarrow \infty} P_n^{(j)}(x_1) (\textrm{ for }   j=0,1,...,m_1);
 
\lim_{n\rightarrow \infty} P_n^{(j)}(x_s) (\textrm{ for }  j=0,1,...,m_s).
 P_n(x) 
\lim_{n\rightarrow \infty} a_{nj}       (\textrm{ for } j=0,...,k-1).
","['analysis', 'polynomials', 'convergence-divergence', 'uniform-convergence']"
90,Continuity at a point of a piecewise function,Continuity at a point of a piecewise function,,"Let $ f(x) = \left\{   \begin{array}{lr}     \frac{x}{3} &  x ~\textrm{is rational},\\     x & x ~\textrm{is irrational}.   \end{array} \right. $ Show that $f$ is continuous at $a \in \mathbb{R}$ if and only if $a=0$ . My initial approach is to use the sequential criterion with the use of density of rational numbers but I wasn't successful. Any help is much appreciated.",Let Show that is continuous at if and only if . My initial approach is to use the sequential criterion with the use of density of rational numbers but I wasn't successful. Any help is much appreciated.,"
f(x) = \left\{
  \begin{array}{lr}
    \frac{x}{3} &  x ~\textrm{is rational},\\
    x & x ~\textrm{is irrational}.
  \end{array}
\right.
 f a \in \mathbb{R} a=0","['calculus', 'real-analysis']"
91,Are there any general guidelines for proving limits of multivariable functions?,Are there any general guidelines for proving limits of multivariable functions?,,"Today I was trying to prove that $$\lim_{(x, y) \to (0, 0)}\dfrac {x^2y^2}{x^2+y^2} = 0 $$ I got really lucky because the AM-GM inequality directly applies here to give us $$\dfrac {x^2y^2}{x^2+y^2} \le \dfrac {x^4 + y^4}{x^2+y^2} \le \dfrac {(x^2+y^2)^2}{x^2+y^2} = x^2+y^2$$ And thus we may choose $\delta = \sqrt \epsilon$ . However, this was really lucky to turn out so cleanly. My question is, when it isn't so clean, are there any general guidelines? For example: Do you look at the $|f(x, y)- L| < \epsilon$ and try to manipulate it? What exactly is the goal when you try to manipulate this? Do you look the $\sqrt {(x-a)^2 + (y-b)^2} < \delta$ term and try to manipulate it? What exactly is the goal when you try to manipulate this? Etc. Thank you.","Today I was trying to prove that I got really lucky because the AM-GM inequality directly applies here to give us And thus we may choose . However, this was really lucky to turn out so cleanly. My question is, when it isn't so clean, are there any general guidelines? For example: Do you look at the and try to manipulate it? What exactly is the goal when you try to manipulate this? Do you look the term and try to manipulate it? What exactly is the goal when you try to manipulate this? Etc. Thank you.","\lim_{(x, y) \to (0, 0)}\dfrac {x^2y^2}{x^2+y^2} = 0  \dfrac {x^2y^2}{x^2+y^2} \le \dfrac {x^4 + y^4}{x^2+y^2} \le \dfrac {(x^2+y^2)^2}{x^2+y^2} = x^2+y^2 \delta = \sqrt \epsilon |f(x, y)- L| < \epsilon \sqrt {(x-a)^2 + (y-b)^2} < \delta","['calculus', 'real-analysis', 'analysis', 'inequality', 'soft-question']"
92,"If $f(x+y)=f(x)+f(y) ,\forall\;x,y\in\Bbb{R}$, then if $f$ is continuous at $0$, then it is continuous on $\Bbb{R}.$ [duplicate]","If , then if  is continuous at , then it is continuous on  [duplicate]","f(x+y)=f(x)+f(y) ,\forall\;x,y\in\Bbb{R} f 0 \Bbb{R}.","This question already has an answer here : Overview of basic facts about Cauchy functional equation (1 answer) Closed 5 years ago . I know that this question has been asked here before but I want to use a different approach. Here is the question. A function $f:\Bbb{R}\to\Bbb{R}$ is such that  \begin{align} f(x+y)=f(x)+f(y) ,\;\;\forall\;x,y\in\Bbb{R}\qquad\qquad\qquad(1)\end{align} I want to show that if $f$ is continuous at $0$, it is continuous on $\Bbb{R}.$ MY WORK Since $(1)$ holds for all $x\in \Bbb{R},$ we let \begin{align} x=x-y+y\end{align} Then,  \begin{align} f(x-y+y)=f(x-y)+f(y)\end{align} \begin{align} f(x-y)=f(x)-f(y)\end{align} Let $x_0\in \Bbb{R}, \;\epsilon>$ and $y=x-x_0,\;\;\forall\,x\in\Bbb{R}.$ Then,  \begin{align} f(x-(x-x_0))=f(x)-f(x-x_0)\end{align}  \begin{align} f(x_0)=f(x)-f(x-x_0)\end{align}  \begin{align} f(y)=f(x_0)-f(x)\end{align} HINTS BY MY PDF: Let $x_0\in \Bbb{R}, \;\epsilon>$ and $y=x-x_0,\;\;\forall\,x\in\Bbb{R}.$ Then, show that  \begin{align} \left|f(x_0)-f(x)\right|=\left|f(y)-f(0)\right|\end{align} Using this equation and the continuity of $f$ at $0$, establish properly that  \begin{align}\left|f(y)-f(0)\right|<\epsilon,\end{align} in some neighbourhood of $0$. My problem is how to put this hint together to complete the proof. Please, I need assistance, thanks!","This question already has an answer here : Overview of basic facts about Cauchy functional equation (1 answer) Closed 5 years ago . I know that this question has been asked here before but I want to use a different approach. Here is the question. A function $f:\Bbb{R}\to\Bbb{R}$ is such that  \begin{align} f(x+y)=f(x)+f(y) ,\;\;\forall\;x,y\in\Bbb{R}\qquad\qquad\qquad(1)\end{align} I want to show that if $f$ is continuous at $0$, it is continuous on $\Bbb{R}.$ MY WORK Since $(1)$ holds for all $x\in \Bbb{R},$ we let \begin{align} x=x-y+y\end{align} Then,  \begin{align} f(x-y+y)=f(x-y)+f(y)\end{align} \begin{align} f(x-y)=f(x)-f(y)\end{align} Let $x_0\in \Bbb{R}, \;\epsilon>$ and $y=x-x_0,\;\;\forall\,x\in\Bbb{R}.$ Then,  \begin{align} f(x-(x-x_0))=f(x)-f(x-x_0)\end{align}  \begin{align} f(x_0)=f(x)-f(x-x_0)\end{align}  \begin{align} f(y)=f(x_0)-f(x)\end{align} HINTS BY MY PDF: Let $x_0\in \Bbb{R}, \;\epsilon>$ and $y=x-x_0,\;\;\forall\,x\in\Bbb{R}.$ Then, show that  \begin{align} \left|f(x_0)-f(x)\right|=\left|f(y)-f(0)\right|\end{align} Using this equation and the continuity of $f$ at $0$, establish properly that  \begin{align}\left|f(y)-f(0)\right|<\epsilon,\end{align} in some neighbourhood of $0$. My problem is how to put this hint together to complete the proof. Please, I need assistance, thanks!",,"['real-analysis', 'analysis', 'functions', 'continuity']"
93,Convergence of $\sum_{n=1}^\infty \frac{1}{n^{a_n}}$,Convergence of,\sum_{n=1}^\infty \frac{1}{n^{a_n}},"Let $\lim _{n\to\infty}a_n=l$. Show that $\sum_{n=1}^\infty \frac{1}{n^{a_n}}$ converges if $l>1$ and diverges if $l<1$. What happens if $l=1$? I tried to use the ratio test, but could not get a good estimate, I have difficulties, that there is the series $a_n$ involved. I know that $\sum_{n=1}^\infty \frac{1}{n^p}$ converges if $p>1$ and diverges if $p\geq 1$ but I am not sure how to use this exactly. For $l=1$, I guess both things could happen? Definitely, we can take $a_n=1$ for all $n$ and then we get the harmonic series which is divergent. Is there an example where the series converges?","Let $\lim _{n\to\infty}a_n=l$. Show that $\sum_{n=1}^\infty \frac{1}{n^{a_n}}$ converges if $l>1$ and diverges if $l<1$. What happens if $l=1$? I tried to use the ratio test, but could not get a good estimate, I have difficulties, that there is the series $a_n$ involved. I know that $\sum_{n=1}^\infty \frac{1}{n^p}$ converges if $p>1$ and diverges if $p\geq 1$ but I am not sure how to use this exactly. For $l=1$, I guess both things could happen? Definitely, we can take $a_n=1$ for all $n$ and then we get the harmonic series which is divergent. Is there an example where the series converges?",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
94,Evaluate $\int_0^1\ln(\frac{1+x}{1-x})dx$ using power series,Evaluate  using power series,\int_0^1\ln(\frac{1+x}{1-x})dx,"I would like to evaluate the integral $\int_0^1\ln(\frac{1+x}{1-x})dx$ by expanding the integrands into power series. Asking wolframalpha the answer should be $2\ln(2)$ but I don't get there. What I tried so far: I know that $\ln(x)=\sum_{k=1}^\infty \frac{(-1)^{k-1}}{k}(x-1)^k$ if $|x-1|\leq 1$ and $x\neq 0$. We also have $\ln(\frac{1+x}{1-x})=\ln(1+x)-\ln(1-x)$. Using the formula for the power series, I get $\ln(1+x)=\sum_{k=1}^\infty \frac{(-1)^{k-1}}{k}x^k$ on $[0,1]$ and $\ln(1-x)=\sum_{k=1}^\infty \frac{(-1)^{k-1}}{k}(-x)^k=-\sum_{k=1}^\infty \frac{1}{k}x^k$ on $[0,1)$. Can I just substract this know? Is it right that then $\ln(\frac{1+x}{1-x})=\ln(1+x)-\ln(1-x)=2\sum_{k=0}^\infty \frac{x^{2k+1}}{2k+1}$? I am not sure if this is right and how can I proceed from there?","I would like to evaluate the integral $\int_0^1\ln(\frac{1+x}{1-x})dx$ by expanding the integrands into power series. Asking wolframalpha the answer should be $2\ln(2)$ but I don't get there. What I tried so far: I know that $\ln(x)=\sum_{k=1}^\infty \frac{(-1)^{k-1}}{k}(x-1)^k$ if $|x-1|\leq 1$ and $x\neq 0$. We also have $\ln(\frac{1+x}{1-x})=\ln(1+x)-\ln(1-x)$. Using the formula for the power series, I get $\ln(1+x)=\sum_{k=1}^\infty \frac{(-1)^{k-1}}{k}x^k$ on $[0,1]$ and $\ln(1-x)=\sum_{k=1}^\infty \frac{(-1)^{k-1}}{k}(-x)^k=-\sum_{k=1}^\infty \frac{1}{k}x^k$ on $[0,1)$. Can I just substract this know? Is it right that then $\ln(\frac{1+x}{1-x})=\ln(1+x)-\ln(1-x)=2\sum_{k=0}^\infty \frac{x^{2k+1}}{2k+1}$? I am not sure if this is right and how can I proceed from there?",,"['real-analysis', 'integration', 'analysis', 'logarithms']"
95,Show that:$\sum_{i=1}^n x_{i} \cdot \sum_{i=1}^n \frac{1}{x_{i}} \geq n^2 $ [duplicate],Show that: [duplicate],\sum_{i=1}^n x_{i} \cdot \sum_{i=1}^n \frac{1}{x_{i}} \geq n^2 ,"This question already has answers here : Inequality : $\sum_{k=1}^n x_k\cdot \sum_{k=1}^n \frac{1}{x_k} \geq n^2$ (6 answers) Closed 5 years ago . Show that:$$\sum_{i=1}^n x_{i} \cdot \sum_{i=1}^n \frac{1}{x_{i}} \geq n^2 $$ The following hints are also given: $$\left(\frac{x}{y} + \frac{y}{x} \geq 2 \right) \land x,y \gt 0$$ Base Case: For n = 2 $$\left(1+2\right) \cdot \left(\frac{1}{1}+\frac{1}{2}\right) \geq 2^2$$ Inductive hypothesis: $$\sum_{i=1}^{n+1} x_{i} \cdot \sum_{i=1}^{n+1} \frac{1}{x_{i}} \geq \left(n+1\right)^2 = n^2+2n+1$$ Inductive step: $$\sum_{i=1}^{n+1} x_{i} \cdot \sum_{i=1}^{n+1} \frac{1}{x_{i}} = \left(\sum_{i=1}^n x_{i}+(n+1)\right) \cdot \left(\sum_{i=1}^n \frac{1}{x_{i}}+\frac{1}{n+1}\right) = \left(\sum_{i=1}^n x_{i} \cdot \sum_{i=1}^n \frac{1}{x_{i}}\right) + \left((n+1) \cdot \sum_{i=1}^n \frac{1}{x_{i}}\right) + \left(\sum_{i=1}^n x_{i} \cdot \frac{1}{n+1}\right) + (n+1) \cdot \frac{1}{n+1}$$ Final words: I came to the conclusion that: $$\left(\sum_{i=1}^n x_{i} \cdot \sum_{i=1}^n \frac{1}{x_{i}}\right) = n^2$$ I inserted for n = 1 so that $$\left((n+1) \cdot \sum_{i=1}^n \frac{1}{x_{i}}\right) = \frac{2}{1} \land \left(\sum_{i=1}^n x_{i} \cdot \frac{1}{n+1}\right) = \frac{1}{2}$$ Since $$\left(\frac{x}{y} + \frac{y}{x} \geq 2 \right) \land x,y$$ was given as a hint in the beginning I thougt I can say that $$\left((n+1) \cdot \sum_{i=1}^n \frac{1}{x_{i}}\right) + \left(\sum_{i=1}^n x_{i} \cdot \frac{1}{n+1}\right) = 2n$$ Furthermore it's obvious that: $$(n+1) \cdot \frac{1}{n+1} = 1$$ It's pretty standard proof by induction and I hope you can maybe give me some advices on what I could have done differently and verify the legitimacy of this proof.","This question already has answers here : Inequality : $\sum_{k=1}^n x_k\cdot \sum_{k=1}^n \frac{1}{x_k} \geq n^2$ (6 answers) Closed 5 years ago . Show that:$$\sum_{i=1}^n x_{i} \cdot \sum_{i=1}^n \frac{1}{x_{i}} \geq n^2 $$ The following hints are also given: $$\left(\frac{x}{y} + \frac{y}{x} \geq 2 \right) \land x,y \gt 0$$ Base Case: For n = 2 $$\left(1+2\right) \cdot \left(\frac{1}{1}+\frac{1}{2}\right) \geq 2^2$$ Inductive hypothesis: $$\sum_{i=1}^{n+1} x_{i} \cdot \sum_{i=1}^{n+1} \frac{1}{x_{i}} \geq \left(n+1\right)^2 = n^2+2n+1$$ Inductive step: $$\sum_{i=1}^{n+1} x_{i} \cdot \sum_{i=1}^{n+1} \frac{1}{x_{i}} = \left(\sum_{i=1}^n x_{i}+(n+1)\right) \cdot \left(\sum_{i=1}^n \frac{1}{x_{i}}+\frac{1}{n+1}\right) = \left(\sum_{i=1}^n x_{i} \cdot \sum_{i=1}^n \frac{1}{x_{i}}\right) + \left((n+1) \cdot \sum_{i=1}^n \frac{1}{x_{i}}\right) + \left(\sum_{i=1}^n x_{i} \cdot \frac{1}{n+1}\right) + (n+1) \cdot \frac{1}{n+1}$$ Final words: I came to the conclusion that: $$\left(\sum_{i=1}^n x_{i} \cdot \sum_{i=1}^n \frac{1}{x_{i}}\right) = n^2$$ I inserted for n = 1 so that $$\left((n+1) \cdot \sum_{i=1}^n \frac{1}{x_{i}}\right) = \frac{2}{1} \land \left(\sum_{i=1}^n x_{i} \cdot \frac{1}{n+1}\right) = \frac{1}{2}$$ Since $$\left(\frac{x}{y} + \frac{y}{x} \geq 2 \right) \land x,y$$ was given as a hint in the beginning I thougt I can say that $$\left((n+1) \cdot \sum_{i=1}^n \frac{1}{x_{i}}\right) + \left(\sum_{i=1}^n x_{i} \cdot \frac{1}{n+1}\right) = 2n$$ Furthermore it's obvious that: $$(n+1) \cdot \frac{1}{n+1} = 1$$ It's pretty standard proof by induction and I hope you can maybe give me some advices on what I could have done differently and verify the legitimacy of this proof.",,"['analysis', 'proof-verification', 'inequality', 'induction']"
96,Convergence of slowly decreasing sequences,Convergence of slowly decreasing sequences,,"Let $A$ be a large number, say $A=100$, and $a$ be a small number, say $a=0.01$. Let $x_1=1$ and define $x_{n+1}-x_n=-a \exp(-A/x_n)$. Is it true that $x_n\to 0$? We know that $x_n$ is strictly decreasing. But I do not know how to show that $x_n>0$ for all $n$, since mathematical induction fails here. Note that $x=0$ is a fixed point for the iteration.","Let $A$ be a large number, say $A=100$, and $a$ be a small number, say $a=0.01$. Let $x_1=1$ and define $x_{n+1}-x_n=-a \exp(-A/x_n)$. Is it true that $x_n\to 0$? We know that $x_n$ is strictly decreasing. But I do not know how to show that $x_n>0$ for all $n$, since mathematical induction fails here. Note that $x=0$ is a fixed point for the iteration.",,"['sequences-and-series', 'analysis']"
97,Is this class of series all demonstrably transcendental?,Is this class of series all demonstrably transcendental?,,"Question: For a vector with integer entries $[a_0, a_1,  \dots, a_{k-1}]$ is it true that when $\sum_{n=1}^\infty{\frac{a_{n-1 \mod k}}{n}}$ is not divergent it limits to some transcendental number or zero? Musings: I will adopt something like the notation of this post. We might call these Weinberger series. Err... I dunno we might call them something else. Let $\vec{v}=[a_0, a_1, \dots a_k]$ be a vector with integer entries. $ \sum{\vec{v}}=[\overline{a_0,a_1, \dots, a_{k-1}}]=\sum_{n=1}^\infty{\frac{a_{n-1 \mod k}}{n}}$. I should say that I suspect that when the sum of entries of $\vec{v}$ is not zero we have that $\sum{\vec{v}}$ is divergent. All the following have that property that the sum  of entries is zero (This makes the 4th entry not ambiguous). Let me show you a few! In this notation: $\begin{array}{lclr}  \\ \frac{\pi\sqrt{2}}{4} & = & [\overline{1,0,1,0,-1,0,-1,0}] & \text{Why [1]} \\ \frac{\pi\sqrt{3}}{9} & = & [\overline{1,-1,0}]  & \text{Don't [2]} \\ \frac{\pi\sqrt{7}}{7} & = & [\overline{1,-1,-1,1,-1,1,0}] & \text{Hyperlinks [3]} \\ \ln{k} & = & [\overline{1,1,\dots,1, 1-k}] & \text{Work[4]} \\ \frac{\sqrt{3}\pi+3\ln\left(2\right)}{9}  & = & [\overline{1,0,0,-1,0,0}] & \text{In [5]} \\ \frac{\pi+2\coth^{-1}\left(\sqrt{2}\right)}{4\sqrt{2}} & = & [\overline{1,0,0,0,-1,0,0,0}] & \text{Arrays [6]} \end{array} $ Why 1 don't 2 hyperlinks 3 work 4 in 5 arrays 6 ? I suspect that these are all transcendental when they are not $0$ or $\infty$. In fact! I am hoping to be able to say that they all fit neatly into some class. They all look to be $\alpha \pi+ \beta\ln(\gamma)+\delta$ for some algebraic constants $\alpha, \beta, \gamma, \delta$. But I would settle for just seeing that the guys need to be transcendental (or some clever counterexample that I am missing.) I suspect that their periodic nature should give rise to a demonstration that these are not algebraic numbers. How can I do that? Let me defend my use of $\vec{v}$. One should only use this notation if they are vectors is some sense. And they are. Note that we can define a type of scalar multiplication with the rationals so that $$\frac{3}{5}\ln(2)= \frac{3}{5} [\overline{1, -1}] = [\overline{0,0,0,0,3,0,0,0,0,-3}]$$ This is really not me saying much more than $$ \frac{3}{5}\sum_{n=1}^\infty{\frac{(-1)^{n+1}}{n}}=\sum_{n=1}^\infty\frac{3(-1)^{n+1}}{5n}$$ We have all the properties that one desires of a vector space: These values are closed under addition and have a type of multiplication with rational numbers. It leaves me wondering what the right type basis should be for this type of exploration.","Question: For a vector with integer entries $[a_0, a_1,  \dots, a_{k-1}]$ is it true that when $\sum_{n=1}^\infty{\frac{a_{n-1 \mod k}}{n}}$ is not divergent it limits to some transcendental number or zero? Musings: I will adopt something like the notation of this post. We might call these Weinberger series. Err... I dunno we might call them something else. Let $\vec{v}=[a_0, a_1, \dots a_k]$ be a vector with integer entries. $ \sum{\vec{v}}=[\overline{a_0,a_1, \dots, a_{k-1}}]=\sum_{n=1}^\infty{\frac{a_{n-1 \mod k}}{n}}$. I should say that I suspect that when the sum of entries of $\vec{v}$ is not zero we have that $\sum{\vec{v}}$ is divergent. All the following have that property that the sum  of entries is zero (This makes the 4th entry not ambiguous). Let me show you a few! In this notation: $\begin{array}{lclr}  \\ \frac{\pi\sqrt{2}}{4} & = & [\overline{1,0,1,0,-1,0,-1,0}] & \text{Why [1]} \\ \frac{\pi\sqrt{3}}{9} & = & [\overline{1,-1,0}]  & \text{Don't [2]} \\ \frac{\pi\sqrt{7}}{7} & = & [\overline{1,-1,-1,1,-1,1,0}] & \text{Hyperlinks [3]} \\ \ln{k} & = & [\overline{1,1,\dots,1, 1-k}] & \text{Work[4]} \\ \frac{\sqrt{3}\pi+3\ln\left(2\right)}{9}  & = & [\overline{1,0,0,-1,0,0}] & \text{In [5]} \\ \frac{\pi+2\coth^{-1}\left(\sqrt{2}\right)}{4\sqrt{2}} & = & [\overline{1,0,0,0,-1,0,0,0}] & \text{Arrays [6]} \end{array} $ Why 1 don't 2 hyperlinks 3 work 4 in 5 arrays 6 ? I suspect that these are all transcendental when they are not $0$ or $\infty$. In fact! I am hoping to be able to say that they all fit neatly into some class. They all look to be $\alpha \pi+ \beta\ln(\gamma)+\delta$ for some algebraic constants $\alpha, \beta, \gamma, \delta$. But I would settle for just seeing that the guys need to be transcendental (or some clever counterexample that I am missing.) I suspect that their periodic nature should give rise to a demonstration that these are not algebraic numbers. How can I do that? Let me defend my use of $\vec{v}$. One should only use this notation if they are vectors is some sense. And they are. Note that we can define a type of scalar multiplication with the rationals so that $$\frac{3}{5}\ln(2)= \frac{3}{5} [\overline{1, -1}] = [\overline{0,0,0,0,3,0,0,0,0,-3}]$$ This is really not me saying much more than $$ \frac{3}{5}\sum_{n=1}^\infty{\frac{(-1)^{n+1}}{n}}=\sum_{n=1}^\infty\frac{3(-1)^{n+1}}{5n}$$ We have all the properties that one desires of a vector space: These values are closed under addition and have a type of multiplication with rational numbers. It leaves me wondering what the right type basis should be for this type of exploration.",,"['sequences-and-series', 'analysis', 'vector-spaces', 'transcendental-numbers', 'dirichlet-series']"
98,Sequences Eventually and Frequently in a Set,Sequences Eventually and Frequently in a Set,,"A sequence $(a_n)$ is eventually in an set $A \subseteq \mathbb R$ if there exists an $N \in \mathbb N$ such that $a_n \in A$ for all $n \ge N.$ A sequence $(a_n)$ is frequently in an set $A \subseteq \mathbb R$ if, for every $N \in \mathbb N$ there exists an $n \ge N$ such that $a_n \in A$. Question: Which definition is stronger? Does frequently imply eventually or eventually imply frequently? Intuitively I am inclined to say that eventually implies frequently because from examples I reason that the sequence ${\lfloor \frac{5}{n}\rfloor}$ is eventually in $\{0\}$ and also frequently in $\{0\}$ but a sequence like $(-1)^n$ which is frequently in $\{1\}$ but not eventually. However from looking at the quantifiers it seems to me that the converse should be true for I reason $$\forall N \exists n(n \ge N \Rightarrow a_n \in A)$$ should imply  $$\exists N \exists n(n \ge N \Rightarrow a_n \in A)$$ (i.e. frequnctly implies eventually)? Am I making a mistake with the quantifiers? Which one is correct? Thanks in advance.","A sequence $(a_n)$ is eventually in an set $A \subseteq \mathbb R$ if there exists an $N \in \mathbb N$ such that $a_n \in A$ for all $n \ge N.$ A sequence $(a_n)$ is frequently in an set $A \subseteq \mathbb R$ if, for every $N \in \mathbb N$ there exists an $n \ge N$ such that $a_n \in A$. Question: Which definition is stronger? Does frequently imply eventually or eventually imply frequently? Intuitively I am inclined to say that eventually implies frequently because from examples I reason that the sequence ${\lfloor \frac{5}{n}\rfloor}$ is eventually in $\{0\}$ and also frequently in $\{0\}$ but a sequence like $(-1)^n$ which is frequently in $\{1\}$ but not eventually. However from looking at the quantifiers it seems to me that the converse should be true for I reason $$\forall N \exists n(n \ge N \Rightarrow a_n \in A)$$ should imply  $$\exists N \exists n(n \ge N \Rightarrow a_n \in A)$$ (i.e. frequnctly implies eventually)? Am I making a mistake with the quantifiers? Which one is correct? Thanks in advance.",,"['real-analysis', 'sequences-and-series', 'analysis', 'logic', 'predicate-logic']"
99,Is my argument correct?,Is my argument correct?,,"In the book of Analysis on Manifolds by Munkres, at page 71, it is asked that Let $A$ be open in $\mathbb{R}^n $; let $f : A \to \mathbb{R}^n $ be of   class $C^r$; assume $Df(x)$ is non-singular for $x\in A$. Show that   even if $f$ is not one-to-one on $A$, the set $B = f(A)$ is open in   $\mathbb{R}^n $. I have argued that: By the inverse function theorem, for $x \in A$, $\exists U_x$ s.t $U_x \subseteq A$ and $U_x$ is open, and $f$ is one-to-one on $U_x$.Moreover, we have $\cup_{x\in A} U_x = B $. Since $B$ is the union of the open sets, $B$ is also open. Is there any problem in this argument ? Could someone provide me a feedback, please.","In the book of Analysis on Manifolds by Munkres, at page 71, it is asked that Let $A$ be open in $\mathbb{R}^n $; let $f : A \to \mathbb{R}^n $ be of   class $C^r$; assume $Df(x)$ is non-singular for $x\in A$. Show that   even if $f$ is not one-to-one on $A$, the set $B = f(A)$ is open in   $\mathbb{R}^n $. I have argued that: By the inverse function theorem, for $x \in A$, $\exists U_x$ s.t $U_x \subseteq A$ and $U_x$ is open, and $f$ is one-to-one on $U_x$.Moreover, we have $\cup_{x\in A} U_x = B $. Since $B$ is the union of the open sets, $B$ is also open. Is there any problem in this argument ? Could someone provide me a feedback, please.",,"['real-analysis', 'analysis']"
