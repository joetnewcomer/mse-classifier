,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"How can I find the asymptotics of $h\int_0^{\pi}\frac{\sin^2(i\pi-he^{it})}{\sinh^2(he^{it})}dt$, $h\to0$, to evaluate a certain contour integral?","How can I find the asymptotics of , , to evaluate a certain contour integral?",h\int_0^{\pi}\frac{\sin^2(i\pi-he^{it})}{\sinh^2(he^{it})}dt h\to0,"$\newcommand{\d}{\,\mathrm{d}}\newcommand{\sech}{\operatorname{sech}}\newcommand{\csch}{\operatorname{csch}}$ It was recently asked, and then deleted, how to evaluate the following using contour integration: $$I=\int_0^\infty\frac{\sin^2(x)}{\sinh^2(x)}\d x\overset{?}{=}\frac{\pi\coth\pi-1}{2}$$ There is a simple real method, and I credit @KStar for finding the series expansions: If $f(x)=-\frac{2}{1+e^{2x}}$ , then $f'(x)=\sech^2(x)$ and by expanding $f$ as a geometric series we find, for $x\gt0$ : $$\sech^2(x)=4\sum_{n=1}^\infty(-1)^{n-1}n\cdot e^{-2nx}$$ And letting $x\mapsto x+i\pi/2$ yields: $$\csch^2(x)=4\sum_{n=1}^\infty n\cdot e^{-2nx}$$ For $x\gt0$ . Then: $$\begin{align}\int_0^\infty\frac{\sin^2(x)}{\sinh^2(x)}\d x&=2\sum_{n=1}^\infty n\cdot\int_0^\infty(1-\cos(2x))e^{-2nx}\d x\\&=2\sum_{n=1}^\infty n\cdot\left(\frac{1}{2n}-\frac{2n}{4n^2+4}\right)\\&=\sum_{n=1}^\infty\frac{1}{n^2+1}\\&=\frac{\pi\coth\pi-1}{2}\end{align}$$ By the Mittag-Leffler expansion of $\coth$ , or equivalently via an argument using the digamma function. The new user who posted and subsequently deleted their question suggested taking a rectangular contour, limiting in $R\to\infty$ on the rectangle with base $-R\to R$ , of height $i\pi$ , and with a semicircular inward indent around the point $i\pi$ , say of radius $\varepsilon$ . The integrand $f(z)=\frac{\sin^2z}{\sinh^2z}$ is holomorphic on the boundary and interior of this contour, so the integrals over all paths sum to zero. Moreover the small strips $R\to R+i\pi,-R\to-R+i\pi$ obviously vanish. We have: $$\begin{align}\sin(x+i\pi)&=\sin(x)\cosh(\pi)+i\sinh(\pi)\cos(x)\\\sin^2(x+i\pi)&=\sin^2(x)\cosh^2(\pi)-\cos^2(x)\sinh^2(\pi)+2i\sin(x)\cos(x)\sinh(\pi)\cosh(\pi)\\&=\sin^2(x)(1+2\sinh^2(\pi))-\sinh^2(\pi)+2i\sin(x)\cos(x)\sinh(\pi)\cosh(\pi)\end{align}$$ For asymptotically large $R$ , asymptotically small $\varepsilon\gt0$ we then need to use: $$0=o(1)+I-\int_{[-R,-\varepsilon]\cup[\varepsilon,R]}\frac{\sin^2(x)(1+2\sinh^2(\pi))-\sinh^2(\pi)}{\sinh^2(x)}\d x+i\varepsilon\int_{-\pi}^0\frac{\sin^2(i\pi+\varepsilon e^{it})}{\sinh^2(\varepsilon e^{it})}e^{it}\d t\\\overset{R\to\infty}{\longrightarrow}-2\sinh^2(\pi)\cdot I+\\\lim_{\varepsilon\to0^+}\left[2\sinh^2(\pi)(1-\coth(\varepsilon))+i\varepsilon\int_{0}^\pi\frac{\sin^2(i\pi-\varepsilon e^{it})}{\sinh^2(\varepsilon e^{it})}e^{it}\d t\right]$$ The original asker claimed that it is possible to use this method to calculate the final answer (they didn't go as far, but it was implied). Taking this on good faith, I will assume this works - but I am very uncertain how to do this. The limit of the semicircular integral is, well, nasty - I'd appreciate help with the asymptotics here. I am fairly certain my calculations thus far are correct, but I unfortunately do not own a copy of Mathematica or equivalent to help me here. It is my (purely intuitive) suspicion that the limit does not exist, but this is weird since the limit must exist, as all the other limits do and the total limit of zero obviously exists. To present a concrete target - to get the correct evaluation, we need to show that: $$\begin{align}2\sinh^2(\pi)(I-1)&=\pi\sinh(\pi)\cosh(\pi)-3\sinh^2(\pi)\\&\overset{?}{=}\lim_{\varepsilon\to0^+}\left[i\varepsilon\int_0^\pi\frac{\sin^2(i\pi-\varepsilon e^{it})}{\sinh^2(\varepsilon e^{it})}\d t-2\sinh^2(\pi)\coth(\varepsilon)\right]\end{align}$$","It was recently asked, and then deleted, how to evaluate the following using contour integration: There is a simple real method, and I credit @KStar for finding the series expansions: If , then and by expanding as a geometric series we find, for : And letting yields: For . Then: By the Mittag-Leffler expansion of , or equivalently via an argument using the digamma function. The new user who posted and subsequently deleted their question suggested taking a rectangular contour, limiting in on the rectangle with base , of height , and with a semicircular inward indent around the point , say of radius . The integrand is holomorphic on the boundary and interior of this contour, so the integrals over all paths sum to zero. Moreover the small strips obviously vanish. We have: For asymptotically large , asymptotically small we then need to use: The original asker claimed that it is possible to use this method to calculate the final answer (they didn't go as far, but it was implied). Taking this on good faith, I will assume this works - but I am very uncertain how to do this. The limit of the semicircular integral is, well, nasty - I'd appreciate help with the asymptotics here. I am fairly certain my calculations thus far are correct, but I unfortunately do not own a copy of Mathematica or equivalent to help me here. It is my (purely intuitive) suspicion that the limit does not exist, but this is weird since the limit must exist, as all the other limits do and the total limit of zero obviously exists. To present a concrete target - to get the correct evaluation, we need to show that:","\newcommand{\d}{\,\mathrm{d}}\newcommand{\sech}{\operatorname{sech}}\newcommand{\csch}{\operatorname{csch}} I=\int_0^\infty\frac{\sin^2(x)}{\sinh^2(x)}\d x\overset{?}{=}\frac{\pi\coth\pi-1}{2} f(x)=-\frac{2}{1+e^{2x}} f'(x)=\sech^2(x) f x\gt0 \sech^2(x)=4\sum_{n=1}^\infty(-1)^{n-1}n\cdot e^{-2nx} x\mapsto x+i\pi/2 \csch^2(x)=4\sum_{n=1}^\infty n\cdot e^{-2nx} x\gt0 \begin{align}\int_0^\infty\frac{\sin^2(x)}{\sinh^2(x)}\d x&=2\sum_{n=1}^\infty n\cdot\int_0^\infty(1-\cos(2x))e^{-2nx}\d x\\&=2\sum_{n=1}^\infty n\cdot\left(\frac{1}{2n}-\frac{2n}{4n^2+4}\right)\\&=\sum_{n=1}^\infty\frac{1}{n^2+1}\\&=\frac{\pi\coth\pi-1}{2}\end{align} \coth R\to\infty -R\to R i\pi i\pi \varepsilon f(z)=\frac{\sin^2z}{\sinh^2z} R\to R+i\pi,-R\to-R+i\pi \begin{align}\sin(x+i\pi)&=\sin(x)\cosh(\pi)+i\sinh(\pi)\cos(x)\\\sin^2(x+i\pi)&=\sin^2(x)\cosh^2(\pi)-\cos^2(x)\sinh^2(\pi)+2i\sin(x)\cos(x)\sinh(\pi)\cosh(\pi)\\&=\sin^2(x)(1+2\sinh^2(\pi))-\sinh^2(\pi)+2i\sin(x)\cos(x)\sinh(\pi)\cosh(\pi)\end{align} R \varepsilon\gt0 0=o(1)+I-\int_{[-R,-\varepsilon]\cup[\varepsilon,R]}\frac{\sin^2(x)(1+2\sinh^2(\pi))-\sinh^2(\pi)}{\sinh^2(x)}\d x+i\varepsilon\int_{-\pi}^0\frac{\sin^2(i\pi+\varepsilon e^{it})}{\sinh^2(\varepsilon e^{it})}e^{it}\d t\\\overset{R\to\infty}{\longrightarrow}-2\sinh^2(\pi)\cdot I+\\\lim_{\varepsilon\to0^+}\left[2\sinh^2(\pi)(1-\coth(\varepsilon))+i\varepsilon\int_{0}^\pi\frac{\sin^2(i\pi-\varepsilon e^{it})}{\sinh^2(\varepsilon e^{it})}e^{it}\d t\right] \begin{align}2\sinh^2(\pi)(I-1)&=\pi\sinh(\pi)\cosh(\pi)-3\sinh^2(\pi)\\&\overset{?}{=}\lim_{\varepsilon\to0^+}\left[i\varepsilon\int_0^\pi\frac{\sin^2(i\pi-\varepsilon e^{it})}{\sinh^2(\varepsilon e^{it})}\d t-2\sinh^2(\pi)\coth(\varepsilon)\right]\end{align}","['complex-analysis', 'asymptotics', 'contour-integration']"
1,Given $f(z)$ can you find $g(z)$ such that $g(z+1) = f(z) + g(z)$,Given  can you find  such that,f(z) g(z) g(z+1) = f(z) + g(z),"More precisely, given a holomorphic function $f$ on a simply connected domain $\Omega \subset \mathbb{C}$ where for any $z \in \Omega$, $z+1$ is also in $\Omega$, does there exist a holomorphic function $g(z)$ such that $g(z+1) = f(z) + g(z)$? Obviously, it won't be unique, but I'm wondering if existence is guaranteed. It seems like it should be but I don't know how to prove it. I am aware of the Euler-Maclaurin formula which gives an asymptotic expression for $g(z)$ but it doesn't seem obvious to me how this would give a holomorphic function here.","More precisely, given a holomorphic function $f$ on a simply connected domain $\Omega \subset \mathbb{C}$ where for any $z \in \Omega$, $z+1$ is also in $\Omega$, does there exist a holomorphic function $g(z)$ such that $g(z+1) = f(z) + g(z)$? Obviously, it won't be unique, but I'm wondering if existence is guaranteed. It seems like it should be but I don't know how to prove it. I am aware of the Euler-Maclaurin formula which gives an asymptotic expression for $g(z)$ but it doesn't seem obvious to me how this would give a holomorphic function here.",,"['complex-analysis', 'functional-equations']"
2,Maximizing $|f'(0)|$ of an analytic function with $f(1/2)=0$.,Maximizing  of an analytic function with .,|f'(0)| f(1/2)=0,"Let f be an analytic function from the unit disk D to the unit disk D. Assume that $f(1/2)=0$, prove $|f'(0)| \leq 25/32$. I am currently stucked with this problem. I have tried adding some conformal self-map g of the unit disk so that $fg(0)=0$ to apply Schwarz lemma,but this only gives upper bound for $|f'(1/2)|$. Is there any other approach for this problem?","Let f be an analytic function from the unit disk D to the unit disk D. Assume that $f(1/2)=0$, prove $|f'(0)| \leq 25/32$. I am currently stucked with this problem. I have tried adding some conformal self-map g of the unit disk so that $fg(0)=0$ to apply Schwarz lemma,but this only gives upper bound for $|f'(1/2)|$. Is there any other approach for this problem?",,"['complex-analysis', 'inequality', 'optimization']"
3,Paley-Weiner theorem and the Fourier transform of a non-analytic smooth function,Paley-Weiner theorem and the Fourier transform of a non-analytic smooth function,,"Many Paley-Weiner theorems are variations on the theme ""the faster a function $f(x)$ falls off as $x \rightarrow \infty$, the smoother its Fourier transform $\tilde{f}(k)$ is as $k \rightarrow 0$.""  In particular, we know that if $f(x)$ decays exponentially at large $x$ then $\tilde{f}(k)$ is analytic, and if $f(x) = 1/x^n$ (with $n \in \mathbb{Z}$) then $\tilde{f}(k) \propto k^{n-1} \mathrm{sgn}( k)$.  But I'm wondering about the (inverse) FT of $\tilde{f}(k) = 1 - e^{-1/k^2}$.  This function is smooth and falls off as $1/k^2$ at large $k$, so its inverse FT $f(x)$ should exist and not be too pathological.  It should fall off faster than any power-law at large $x$, or else some finite derivative of $\tilde{f}(k)$ would be discontinuous, but $\tilde{f}(k)$ is smooth.  But it cannot fall off as fast as an exponential, or else $\tilde{f}(k)$ would be analytic, which it isn't.  So its large-$x$ falloff must lie somewhere in between power-law and exponential.  But what is it?  I have no idea how to evaluate the Fourier transform because of the essential singularity at $k = 0$. Edit : Follow-up question: Is it true that any smooth function that falls off faster than any power-law but slower than any exponential has a non-analytic smooth FT?","Many Paley-Weiner theorems are variations on the theme ""the faster a function $f(x)$ falls off as $x \rightarrow \infty$, the smoother its Fourier transform $\tilde{f}(k)$ is as $k \rightarrow 0$.""  In particular, we know that if $f(x)$ decays exponentially at large $x$ then $\tilde{f}(k)$ is analytic, and if $f(x) = 1/x^n$ (with $n \in \mathbb{Z}$) then $\tilde{f}(k) \propto k^{n-1} \mathrm{sgn}( k)$.  But I'm wondering about the (inverse) FT of $\tilde{f}(k) = 1 - e^{-1/k^2}$.  This function is smooth and falls off as $1/k^2$ at large $k$, so its inverse FT $f(x)$ should exist and not be too pathological.  It should fall off faster than any power-law at large $x$, or else some finite derivative of $\tilde{f}(k)$ would be discontinuous, but $\tilde{f}(k)$ is smooth.  But it cannot fall off as fast as an exponential, or else $\tilde{f}(k)$ would be analytic, which it isn't.  So its large-$x$ falloff must lie somewhere in between power-law and exponential.  But what is it?  I have no idea how to evaluate the Fourier transform because of the essential singularity at $k = 0$. Edit : Follow-up question: Is it true that any smooth function that falls off faster than any power-law but slower than any exponential has a non-analytic smooth FT?",,"['complex-analysis', 'fourier-analysis', 'asymptotics', 'analyticity']"
4,Proof of an inequality in $\mathbb{C}$,Proof of an inequality in,\mathbb{C},"Let $z\in \mathbb{C}, n \geq 2$. Show this complex inequality $$|z^n-1|^2\le |z-1|^2\left(1+|z|^2+\dfrac{2}{n-1}\Re{(z)}\right)^{n-1}$$ For $n=2$ the inequality is easy to prove:  $$|z^2-1|^2\le |z-1|^2\left(1+|z|^2+2\Re{(z)}\right)$$ $$\Longleftrightarrow |z+1|^2\le 1+|z|^2+2\Re{(z)}$$ $$\Longleftrightarrow z+\overline{z}\le 2\Re{(z)}$$ which is in fact an equality, thus is exact. for $n=3$. $$\Longleftrightarrow |z^2+z+1|^2\le (1+|z|^2+\Re{(z)})^2$$ $$\Longleftrightarrow(z^2+z+1)(\overline {z^2}+\overline{z}+1)\le |z|^4+2|z|^2+1+\Re^2{z}+2\Re{(z)}+2|z|^2\cdot\Re{(z)}$$ $$\Longleftrightarrow |z|^4+|z|^2+|z|^2(z+\overline{z})+1+(z+\overline{z})+(z^2+(\overline{z})^2)\le |z|^4+2|z|^2+1+\Re^2{z}+2\Re{(z)}+2|z|^2\cdot\Re{(z)}$$ $$\Longleftrightarrow |z|^2+(\Re{(z)})^2\ge z^2+(\overline{z})^2=(z+\overline{z})^2-2|z|^2$$ $$\Longleftrightarrow 3|z|^2\ge 3(\Re{(z)})^2$$ It is clear Is it true for a general $n$?","Let $z\in \mathbb{C}, n \geq 2$. Show this complex inequality $$|z^n-1|^2\le |z-1|^2\left(1+|z|^2+\dfrac{2}{n-1}\Re{(z)}\right)^{n-1}$$ For $n=2$ the inequality is easy to prove:  $$|z^2-1|^2\le |z-1|^2\left(1+|z|^2+2\Re{(z)}\right)$$ $$\Longleftrightarrow |z+1|^2\le 1+|z|^2+2\Re{(z)}$$ $$\Longleftrightarrow z+\overline{z}\le 2\Re{(z)}$$ which is in fact an equality, thus is exact. for $n=3$. $$\Longleftrightarrow |z^2+z+1|^2\le (1+|z|^2+\Re{(z)})^2$$ $$\Longleftrightarrow(z^2+z+1)(\overline {z^2}+\overline{z}+1)\le |z|^4+2|z|^2+1+\Re^2{z}+2\Re{(z)}+2|z|^2\cdot\Re{(z)}$$ $$\Longleftrightarrow |z|^4+|z|^2+|z|^2(z+\overline{z})+1+(z+\overline{z})+(z^2+(\overline{z})^2)\le |z|^4+2|z|^2+1+\Re^2{z}+2\Re{(z)}+2|z|^2\cdot\Re{(z)}$$ $$\Longleftrightarrow |z|^2+(\Re{(z)})^2\ge z^2+(\overline{z})^2=(z+\overline{z})^2-2|z|^2$$ $$\Longleftrightarrow 3|z|^2\ge 3(\Re{(z)})^2$$ It is clear Is it true for a general $n$?",,"['complex-analysis', 'inequality']"
5,Showing properties of a subset of $\mathbb{C}$,Showing properties of a subset of,\mathbb{C},Let $\omega(k)=\alpha_{n}k^{n}+\alpha_{n-1}k^{n-1}+\dots+\alpha_{0}$ be a polynomial of degree $n\in\mathbb{N}$ on $\mathbb{C}$. Define $D=\{k\in\mathbb{C}:\text{Re}(\omega(k))<0 \}$. How do i show that $\mathbb{C}-\partial D$ is a union of disjoint unbounded simply connected open sets? There is a hint to use the fact that the real part of $\omega(k)$ is a harmonic function. Note: I have limited experiences with topology hence i would like an answer that is as elementary as possible.,Let $\omega(k)=\alpha_{n}k^{n}+\alpha_{n-1}k^{n-1}+\dots+\alpha_{0}$ be a polynomial of degree $n\in\mathbb{N}$ on $\mathbb{C}$. Define $D=\{k\in\mathbb{C}:\text{Re}(\omega(k))<0 \}$. How do i show that $\mathbb{C}-\partial D$ is a union of disjoint unbounded simply connected open sets? There is a hint to use the fact that the real part of $\omega(k)$ is a harmonic function. Note: I have limited experiences with topology hence i would like an answer that is as elementary as possible.,,['complex-analysis']
6,Prove that $\zeta(4)=\pi^4/90$,Prove that,\zeta(4)=\pi^4/90,"I am asked to ""use the calculus of residues"" to prove that $$\displaystyle\sum\limits_{n=1}^{\infty} \frac{1}{n^4}=\frac{\pi^4}{90}$$ I think I can do this given the Laurent series for $\cot z$ centered at the origin, but I don't know how to find the first few terms of the Laurent series (I can use Cauchy's Integral Formula to find the first coefficient).","I am asked to ""use the calculus of residues"" to prove that $$\displaystyle\sum\limits_{n=1}^{\infty} \frac{1}{n^4}=\frac{\pi^4}{90}$$ I think I can do this given the Laurent series for $\cot z$ centered at the origin, but I don't know how to find the first few terms of the Laurent series (I can use Cauchy's Integral Formula to find the first coefficient).",,"['complex-analysis', 'riemann-zeta', 'residue-calculus', 'laurent-series']"
7,Final Step in calculating option prices under the Heston Stochastic Volatility Model,Final Step in calculating option prices under the Heston Stochastic Volatility Model,,"Let: $$ \alpha = -\frac{u^2}{2}-\frac{iu}{2}+jiu\\ \beta = \lambda-\rho \eta i u - j \rho \eta\\ \gamma = \frac{\eta ^2}{2}\\ $$ where $j \in \{0,1\}$ and $i^2=-1$ , $g=\frac{r_-}{r_+}$ and $r_{\pm}=\frac{\beta\pm \sqrt{\beta^2-4\alpha\gamma}}{2\gamma}=\frac{\beta \pm d }{\eta^2}$ . Then let: $$ D(u,\tau)=r_-\frac{1-e^{-d\tau}}{1-ge^{-d\tau}}\\ C(u,\tau)=\lambda \left[\tau r_--\frac{2}{\eta^2}log(\frac{1-ge^{-d\tau}}{1-g}) \right] $$ Where $\lambda$ is a constant. For the function: $$ \hat{P}_j(u,v,\tau)=\frac{1}{iu}\exp\left[C(u,\tau)\bar{v}+D(u,\tau)v \right] $$ $\bar{v}\in \mathscr R$ . Show that: $$ P_j(x,v,\tau)=\int_{-\infty}^{\infty}\frac{e^{iux}}{2\pi} \hat{P_j}(u,v,\tau)du\\ = \frac{1}{2}+\frac{1}{\pi}\int_0^\infty Re\left[\frac{\exp[C_j(u,\tau)\bar{v}+D_j(u,\tau)v+iux]}{iu} \right]du $$ This result comes up in a text I am reading on stochastic volatility models, it is stated but not proven, the math involved is probably way above my level, but it would be great to see the steps involved in proving this. I guess it just amounts to showing the imaginary part of this integral is equal to half. I don't mean for the question to be lazy, I have worked through all other parts of this relatively long proof, this technique seems to be used a lot in stochastic volatility/liquidity models which I am interested in studying further, and that's the reason I posted.","Let: where and , and . Then let: Where is a constant. For the function: . Show that: This result comes up in a text I am reading on stochastic volatility models, it is stated but not proven, the math involved is probably way above my level, but it would be great to see the steps involved in proving this. I guess it just amounts to showing the imaginary part of this integral is equal to half. I don't mean for the question to be lazy, I have worked through all other parts of this relatively long proof, this technique seems to be used a lot in stochastic volatility/liquidity models which I am interested in studying further, and that's the reason I posted.","
\alpha = -\frac{u^2}{2}-\frac{iu}{2}+jiu\\
\beta = \lambda-\rho \eta i u - j \rho \eta\\
\gamma = \frac{\eta ^2}{2}\\
 j \in \{0,1\} i^2=-1 g=\frac{r_-}{r_+} r_{\pm}=\frac{\beta\pm \sqrt{\beta^2-4\alpha\gamma}}{2\gamma}=\frac{\beta \pm d }{\eta^2} 
D(u,\tau)=r_-\frac{1-e^{-d\tau}}{1-ge^{-d\tau}}\\
C(u,\tau)=\lambda \left[\tau r_--\frac{2}{\eta^2}log(\frac{1-ge^{-d\tau}}{1-g}) \right]
 \lambda 
\hat{P}_j(u,v,\tau)=\frac{1}{iu}\exp\left[C(u,\tau)\bar{v}+D(u,\tau)v \right]
 \bar{v}\in \mathscr R  P_j(x,v,\tau)=\int_{-\infty}^{\infty}\frac{e^{iux}}{2\pi} \hat{P_j}(u,v,\tau)du\\ = \frac{1}{2}+\frac{1}{\pi}\int_0^\infty Re\left[\frac{\exp[C_j(u,\tau)\bar{v}+D_j(u,\tau)v+iux]}{iu} \right]du ","['complex-analysis', 'stochastic-calculus', 'finance']"
8,Infinite number of poles and residue theorem,Infinite number of poles and residue theorem,,"I suppose a stupid question but I was wondering about it for a while: Can one apply the residue theorem to a function $f$ which is defined and holomorphic on $U-\{a_1,a_2,\dots\}$ where $U$ is simply connected open subset of the complex plane and $a_k$ for $k\geq 1$ are all simple poles of $f(z)$. In particular I am thinking about the case when $f(z)= \Gamma(z)$, the gamma function. It is known that it has simple poles at $z=-k$, $k=0,1,2,\dots$ with resdue $(-1)^k/k!$. I hope the question is clear. Excuse me in case it is to trivial. I am not an expert in complex analysis. The Wikipedia-article demands a finite number of points $a_k$. Will one get a problem with a suitable choosen closed contour? To be more precise I would like to calculate the following integral: $$\int_{-\infty}^{\infty} dx f(x)$$ I was wondering if the typical ""trick"" of constructing a closed half-circle in the upper-half plane would work too when the poles continue ad infinitum. EDIT: My initial motivation is to invert a Mellin transform using the Mellin inversion theorem, when the Mellin transform has an infinite number of isolated poles. For example: I am able to show (by using a  specific symmetry of my problem) that the Melin transform of a function $P(x)$ for $0\leq x\leq x_c$ fulfills the following equation: $$M(s) = \frac{2x_c^s}{s-2+x_c^s}$$ where $0<x_c<1$. Now when I try to apply the Mellin inversion theorem I need to know where the poles lie. Unfortunately I think there is an infinite number poles of $M(s)$.","I suppose a stupid question but I was wondering about it for a while: Can one apply the residue theorem to a function $f$ which is defined and holomorphic on $U-\{a_1,a_2,\dots\}$ where $U$ is simply connected open subset of the complex plane and $a_k$ for $k\geq 1$ are all simple poles of $f(z)$. In particular I am thinking about the case when $f(z)= \Gamma(z)$, the gamma function. It is known that it has simple poles at $z=-k$, $k=0,1,2,\dots$ with resdue $(-1)^k/k!$. I hope the question is clear. Excuse me in case it is to trivial. I am not an expert in complex analysis. The Wikipedia-article demands a finite number of points $a_k$. Will one get a problem with a suitable choosen closed contour? To be more precise I would like to calculate the following integral: $$\int_{-\infty}^{\infty} dx f(x)$$ I was wondering if the typical ""trick"" of constructing a closed half-circle in the upper-half plane would work too when the poles continue ad infinitum. EDIT: My initial motivation is to invert a Mellin transform using the Mellin inversion theorem, when the Mellin transform has an infinite number of isolated poles. For example: I am able to show (by using a  specific symmetry of my problem) that the Melin transform of a function $P(x)$ for $0\leq x\leq x_c$ fulfills the following equation: $$M(s) = \frac{2x_c^s}{s-2+x_c^s}$$ where $0<x_c<1$. Now when I try to apply the Mellin inversion theorem I need to know where the poles lie. Unfortunately I think there is an infinite number poles of $M(s)$.",,['complex-analysis']
9,How to obtaining the lattice corresponding to an elliptic curve,How to obtaining the lattice corresponding to an elliptic curve,,Let $C$ be a complex elliptic curve given by the quation $y^2=4x^3-g_2 x -g_3$. How do I find the lattice $\Lambda$ such that $C \cong \mathbb{C}/\Lambda$? I need the lattice (and corresponding Weierstrass $P$ function) but I don't know how to get it explicitly from the elliptic invaiants $g_2$ and $g_3$.,Let $C$ be a complex elliptic curve given by the quation $y^2=4x^3-g_2 x -g_3$. How do I find the lattice $\Lambda$ such that $C \cong \mathbb{C}/\Lambda$? I need the lattice (and corresponding Weierstrass $P$ function) but I don't know how to get it explicitly from the elliptic invaiants $g_2$ and $g_3$.,,"['complex-analysis', 'elliptic-curves', 'elliptic-functions', 'vector-lattices']"
10,Calculating $\text{erf}^{-1}(z)$ for $z\in\mathbb{C}$,Calculating  for,\text{erf}^{-1}(z) z\in\mathbb{C},"All the information I found about inverse error function $\text{erf}^{-1}(z)$ was about $z\in\mathbb{R}$. Also I found some Taylor expansions for it, but as the function is unbounded near $z=\pm1$, these expansions only converge in the disk $|z|<1$. I want to look at real and imaginary parts of this function for $z\in\mathbb{C}$. I tried using Mathematica's InverseErf[z] , but it appeared to only support real arguments. I then tried using FindRoot to determine values and plot, but I got somewhat strange results, which don't disappear when I increase WorkingPrecision (tried up to 50 decimal places). Here's what I got for real (left) and imaginary (right) parts on DensityPlot from FindRoot : Very light and very dark regions correspond to values outside of $-3<f<3$ range. As one can see, using Taylor series won't help me with these strange regions, because all they are outside of disk of convergence. So, my questions are: (answered) How can one compute $\text{erf}^{-1}(z)$ for $z\in\mathbb{C}$ including those $|z|\ge1$? Are there any packages which are able to compute it without me having to implement the algorithm? Are properties of this function for complex arguments described anywhere?","All the information I found about inverse error function $\text{erf}^{-1}(z)$ was about $z\in\mathbb{R}$. Also I found some Taylor expansions for it, but as the function is unbounded near $z=\pm1$, these expansions only converge in the disk $|z|<1$. I want to look at real and imaginary parts of this function for $z\in\mathbb{C}$. I tried using Mathematica's InverseErf[z] , but it appeared to only support real arguments. I then tried using FindRoot to determine values and plot, but I got somewhat strange results, which don't disappear when I increase WorkingPrecision (tried up to 50 decimal places). Here's what I got for real (left) and imaginary (right) parts on DensityPlot from FindRoot : Very light and very dark regions correspond to values outside of $-3<f<3$ range. As one can see, using Taylor series won't help me with these strange regions, because all they are outside of disk of convergence. So, my questions are: (answered) How can one compute $\text{erf}^{-1}(z)$ for $z\in\mathbb{C}$ including those $|z|\ge1$? Are there any packages which are able to compute it without me having to implement the algorithm? Are properties of this function for complex arguments described anywhere?",,"['complex-analysis', 'special-functions', 'inverse', 'error-function']"
11,"What does ""toy-contour"" mean?","What does ""toy-contour"" mean?",,"When I reading Complex Analysis written by Stein and Shakarchi. In Chapter 2, he had introduced a notion ""toy contour ""without explaining. what does this exactly mean?","When I reading Complex Analysis written by Stein and Shakarchi. In Chapter 2, he had introduced a notion ""toy contour ""without explaining. what does this exactly mean?",,"['complex-analysis', 'terminology']"
12,"If $(f'_n)$ converges uniformly, does $(f_n)$ necessarily converge uniformly?","If  converges uniformly, does  necessarily converge uniformly?",(f'_n) (f_n),"I've been studying complex analysis problems, and get stuck on the following: Let $D \subseteq \mathbb{C}$ be a domain (open connected set) and $z_0 \in D$. Assume that $(f_n)$ is a sequence of analytic functions in $D$ such that $\lim_{n \to \infty} f_n(z_0) = w_0 \in \mathbb{C}$ and that sequence of derivatives $(f'_n)$ converges uniformly on compact subsets of $D$ to a function $g$.  Is it true that there exists an analytic function $f$ in $D$ such that $f_n \to f$ uniformly on compact subsets of $D$? Prove or give a counterexample. I don't even know if this is true. I thought I had a solution (see below) but I realized that I was using the Fundamental Theorem of Calculus on sets which were not necessarily simply connected, which is invalid. However, I have been unsuccessful in finding a counterexample either. Does anyone know a way to fix the hole in my reasoning, or a counterexample if the statement is false?  Thanks. Suppose $K$ is connected compact subset of $D$ which contains $z_0$ with Lebesgue number $\delta$. Then there exists of an open cover of $K$ by some number $k$ of $\delta$-disks. For each $m$ there exists  $M>0$ such that $|f'_n(z)-g(z)| < 1/m$ and $|f_n(z_0) - w_0| < 1/m$ for all $n > M$ and all $z \in K$.  Now let $z \in K$, and let $\gamma$ be a path of minimal length from $z_0$ to $z$ inside the $\delta$-cover of $D$. Then for $n > M$,  $$\left | \int_\gamma f'_n(t) - g(t) dt\right| \leq  \ell (\gamma) /m \leq 2\delta k /m. $$ Then  \begin{align*} \left| f_n(z) - \left(w_0 + \int_\gamma g(t) dt\right ) \right | &= \left| \left(f_n(z)- f_n(z_0) - \int_\gamma g(t) dt \right) + (w_0- f_n(z_0))\right| \\ & \leq \left| f_n(z)- f_n(z_0) - \int_\gamma g(t) dt \right| + |w_0- f_n(z_0)| \\ &\leq (2\delta k+1)/m \\ & \to 0 \text{ as } m \to \infty \end{align*} Thus $f_n$ converges uniformly to $w_0 + \int_{z_0} ^ z g(t) dt $ on compact subsets of $D$ [every compact subset is contained in a  connected compact subset which contains $z_0$].","I've been studying complex analysis problems, and get stuck on the following: Let $D \subseteq \mathbb{C}$ be a domain (open connected set) and $z_0 \in D$. Assume that $(f_n)$ is a sequence of analytic functions in $D$ such that $\lim_{n \to \infty} f_n(z_0) = w_0 \in \mathbb{C}$ and that sequence of derivatives $(f'_n)$ converges uniformly on compact subsets of $D$ to a function $g$.  Is it true that there exists an analytic function $f$ in $D$ such that $f_n \to f$ uniformly on compact subsets of $D$? Prove or give a counterexample. I don't even know if this is true. I thought I had a solution (see below) but I realized that I was using the Fundamental Theorem of Calculus on sets which were not necessarily simply connected, which is invalid. However, I have been unsuccessful in finding a counterexample either. Does anyone know a way to fix the hole in my reasoning, or a counterexample if the statement is false?  Thanks. Suppose $K$ is connected compact subset of $D$ which contains $z_0$ with Lebesgue number $\delta$. Then there exists of an open cover of $K$ by some number $k$ of $\delta$-disks. For each $m$ there exists  $M>0$ such that $|f'_n(z)-g(z)| < 1/m$ and $|f_n(z_0) - w_0| < 1/m$ for all $n > M$ and all $z \in K$.  Now let $z \in K$, and let $\gamma$ be a path of minimal length from $z_0$ to $z$ inside the $\delta$-cover of $D$. Then for $n > M$,  $$\left | \int_\gamma f'_n(t) - g(t) dt\right| \leq  \ell (\gamma) /m \leq 2\delta k /m. $$ Then  \begin{align*} \left| f_n(z) - \left(w_0 + \int_\gamma g(t) dt\right ) \right | &= \left| \left(f_n(z)- f_n(z_0) - \int_\gamma g(t) dt \right) + (w_0- f_n(z_0))\right| \\ & \leq \left| f_n(z)- f_n(z_0) - \int_\gamma g(t) dt \right| + |w_0- f_n(z_0)| \\ &\leq (2\delta k+1)/m \\ & \to 0 \text{ as } m \to \infty \end{align*} Thus $f_n$ converges uniformly to $w_0 + \int_{z_0} ^ z g(t) dt $ on compact subsets of $D$ [every compact subset is contained in a  connected compact subset which contains $z_0$].",,"['complex-analysis', 'contour-integration']"
13,Multiple choice question: Let $f$ be an entire function such that $\lim_{|z|\rightarrow\infty}|f(z)|$ = $\infty$.,Multiple choice question: Let  be an entire function such that  = .,f \lim_{|z|\rightarrow\infty}|f(z)| \infty,"Let $\displaystyle f$ be an entire function such that $$\lim_{|z|\rightarrow \infty} |f(z)| = \infty .$$ Then, $f(\frac {1}{z})$ has an essential singularity at 0. $f$ cannot be a polynomial. $f$ has finitely many zeros. $f(\frac {1}{z})$ has a pole at 0. Please suggest which of the options seem correct. I am thinking that $f$ can be a polynomial and so option (2) does not hold. Further, if $f(z) = \sin z $ then it has infinitely many zeros... which rules out (3) while for $f(z) = z$ indicates that it has a simple pole at $0$ and option (4) seems correct.","Let $\displaystyle f$ be an entire function such that $$\lim_{|z|\rightarrow \infty} |f(z)| = \infty .$$ Then, $f(\frac {1}{z})$ has an essential singularity at 0. $f$ cannot be a polynomial. $f$ has finitely many zeros. $f(\frac {1}{z})$ has a pole at 0. Please suggest which of the options seem correct. I am thinking that $f$ can be a polynomial and so option (2) does not hold. Further, if $f(z) = \sin z $ then it has infinitely many zeros... which rules out (3) while for $f(z) = z$ indicates that it has a simple pole at $0$ and option (4) seems correct.",,['complex-analysis']
14,Using the Identity Theorem to prove $\sin^2 z+ \cos^2 z =1$,Using the Identity Theorem to prove,\sin^2 z+ \cos^2 z =1,"I have a question. Using Identity theorem, we can give proof of the standard trigonometric identities; in particular, $\sin^2 z+ \cos^2 z =1$ . My approach: Let $f(z) = \sin^2 z+ \cos^2 z -1$ , so $f$ is an entire function. We know that $\sin^2 x+ \cos^2 x -1 =0$ for every real number. Let $x_0$ be a real number. Then any neighborhood of $x_0$ contains a point other that $x_0$ for which $\sin^2 x+ \cos^2 x -1 =0$ . So, $x_0$ is a limit point of zero set, $Z(f)$ in $\mathbb{C}$ . So, by the identity theorem $f=0$ throughout $\mathbb{C}$ . So $\sin^2 z+ \cos^2 z =1$ . Am I correct?","I have a question. Using Identity theorem, we can give proof of the standard trigonometric identities; in particular, . My approach: Let , so is an entire function. We know that for every real number. Let be a real number. Then any neighborhood of contains a point other that for which . So, is a limit point of zero set, in . So, by the identity theorem throughout . So . Am I correct?",\sin^2 z+ \cos^2 z =1 f(z) = \sin^2 z+ \cos^2 z -1 f \sin^2 x+ \cos^2 x -1 =0 x_0 x_0 x_0 \sin^2 x+ \cos^2 x -1 =0 x_0 Z(f) \mathbb{C} f=0 \mathbb{C} \sin^2 z+ \cos^2 z =1,['complex-analysis']
15,Pole lying in the middle of a logarithm's branch cut in $\int_{-\infty}^{\infty} dz \frac{\ln(a+z^2)}{1+z^2}$,Pole lying in the middle of a logarithm's branch cut in,\int_{-\infty}^{\infty} dz \frac{\ln(a+z^2)}{1+z^2},"Consider the integral, for $a\geq0$ , $$I = \int_{-\infty}^{\infty} dz \frac{\ln(a+z^2)}{1+z^2} = 2 \pi \ln(1+\sqrt{a})$$ This is straightforward to do with real methods via differentiation with respect to $a$ , partial fraction decomposition, and integration with respect to $a$ . My question is about evaluating this integral directly using a contour integral in the complex plane. I am running into a problem for $a\leq1$ where the pole $z=i$ lies directly in the middle of the branch cut, yielding an extra, seemingly spurious, imaginary contribution. I will define $\ln(z)$ to have a branch cut on the negative real axis. Looking in the upper half plane, the branch point $z = \sqrt{a} i$ yields a branch cut in $\ln(a+z^2)$ moving up the imaginary axis (this follows since the imaginary part of $a+x^2$ changes sign on crossing the imaginary axis, and the real part is non-positive.) Consider the integral for $a>1$ , and consider the following contour: The contour integral via the pole at $z=i$ evaluates to $\pi \ln(a-1)$ . Looking at the different contributions, and noting that the arc does not contribute because the function decays quickly enough, we have $$\pi \ln(a-1) = I + \int_{\sqrt{a}}^\infty i dy \frac{2\pi i}{1-y^2}$$ Here, the second contribution on the right hand side comes from the vertical lines on the sides of the branch cut. It is straightforwardly evaluated as $\pi \ln(\frac{\sqrt{a}-1}{\sqrt{a}+1}$ ). Solving for $I$ yields, as anticipated, $$I = 2\pi \ln(1+\sqrt{a})$$ However, I'm having much more trouble for $a \leq i$ , for which the pole lies in the middle of the branch cut. I keep getting an extra contribution! The contour integral now evaluates to zero, but there are significant contributions from the small semicircle arcs about the pole. My trouble is that the small semicircle arcs are on different sides of the branch cut and hence contribute different amounts. I find $$0 = I + PV \int_{\sqrt{a}}^\infty i dy \frac{2\pi i}{1-y^2} + (-\pi \ln(a-1)/2) + (-\pi \ln(a-1)/2 \color{red}{+ 2 \pi i (-\pi i)/(2i)}  ) $$ From left to right, I have the original integral, the contribution from the vertical parts of the contour, the right small semicircle, and the left small semicircle. The left small semicircle is on the side of the branch cut with an extra $2 \pi i$ contribution from the log, which gives the extra red contribution. Without the red contribution, I get the right answer after rearranging and simplifying. With the red contribution, I suffer an imaginary contribution that yields an incorrect result for $I$ . How does one properly handle a pole lying in the middle of a (logarithm's) branch cut? Am I missing another imaginary contribution that cancels with the one in red?","Consider the integral, for , This is straightforward to do with real methods via differentiation with respect to , partial fraction decomposition, and integration with respect to . My question is about evaluating this integral directly using a contour integral in the complex plane. I am running into a problem for where the pole lies directly in the middle of the branch cut, yielding an extra, seemingly spurious, imaginary contribution. I will define to have a branch cut on the negative real axis. Looking in the upper half plane, the branch point yields a branch cut in moving up the imaginary axis (this follows since the imaginary part of changes sign on crossing the imaginary axis, and the real part is non-positive.) Consider the integral for , and consider the following contour: The contour integral via the pole at evaluates to . Looking at the different contributions, and noting that the arc does not contribute because the function decays quickly enough, we have Here, the second contribution on the right hand side comes from the vertical lines on the sides of the branch cut. It is straightforwardly evaluated as ). Solving for yields, as anticipated, However, I'm having much more trouble for , for which the pole lies in the middle of the branch cut. I keep getting an extra contribution! The contour integral now evaluates to zero, but there are significant contributions from the small semicircle arcs about the pole. My trouble is that the small semicircle arcs are on different sides of the branch cut and hence contribute different amounts. I find From left to right, I have the original integral, the contribution from the vertical parts of the contour, the right small semicircle, and the left small semicircle. The left small semicircle is on the side of the branch cut with an extra contribution from the log, which gives the extra red contribution. Without the red contribution, I get the right answer after rearranging and simplifying. With the red contribution, I suffer an imaginary contribution that yields an incorrect result for . How does one properly handle a pole lying in the middle of a (logarithm's) branch cut? Am I missing another imaginary contribution that cancels with the one in red?",a\geq0 I = \int_{-\infty}^{\infty} dz \frac{\ln(a+z^2)}{1+z^2} = 2 \pi \ln(1+\sqrt{a}) a a a\leq1 z=i \ln(z) z = \sqrt{a} i \ln(a+z^2) a+x^2 a>1 z=i \pi \ln(a-1) \pi \ln(a-1) = I + \int_{\sqrt{a}}^\infty i dy \frac{2\pi i}{1-y^2} \pi \ln(\frac{\sqrt{a}-1}{\sqrt{a}+1} I I = 2\pi \ln(1+\sqrt{a}) a \leq i 0 = I + PV \int_{\sqrt{a}}^\infty i dy \frac{2\pi i}{1-y^2} + (-\pi \ln(a-1)/2) + (-\pi \ln(a-1)/2 \color{red}{+ 2 \pi i (-\pi i)/(2i)}  )  2 \pi i I,"['complex-analysis', 'definite-integrals', 'contour-integration', 'branch-cuts']"
16,Non-constant entire functions,Non-constant entire functions,,"Question: If $g$ is a non-constant entire function does it follow that $G_1(z)=g(z)-g\left(z+e^{g(z)}\right)$ is non-constant? The reason I care is it would imply Prop 3 below, which in turn implies Prop 1, giving a proof that might seem better motivated than the proof here . It seems at least plausible. The only thought I've had is a little vague: $-G_1(z)$ is something somewhat like $g'(z)e^{g(z)}$. And $g'e^g$ cannot  be constant:  If $g'e^g=c$ then $e^g=cz+d$. So $cz+d$ has no zero, hence $c=0$, and it follows that $g$ is constant. The problem is that the size of $g'(z)e^{g(z)}+G_1(z)$ depends on the size of $e^g$ and also on the size of $g''$... Context: The other day at the link above I learned something totally new to me: Prop 1. Suppose $f$ is entire. Then $f\circ f$ has no fixed point if and only if $f$ is a nontrivial translation ($f(z)=z+b$, $b\ne0$.) The obvious question is when $f$ has a fixed point - this is trivial: Prop 2. The entire function $f$ has no fixed point if and only if $f(z)=z+e^{g(z)}$ for some entire function $g$. (Proof: $f$ has no fixed point if and only if $f(z)-z$ has no zero...) It occurred to me to try to use Prop 2 to prove Prop 1: Suppose $f\circ f$ has no fixed point. Then $f$ has no  fixed point, so $f(z)=z+e^{g(z)}$, and now we have something to work with in investigating when $f\circ f$ has a fixed point... It  turns out that to get to Prop 1 from Prop 2 we need this: Prop  3 If $g$ is a non-constant entire function then $G_2(z)=e^{g(z)}+e^{g\left(z+e^{g(z)}\right)}$ has a zero. Proving two assertions above: If the answer to the question is yes then Prop 3 follows: If $G_1$ is non-constant then little  Picard shows that there exist $k\in\Bbb Z$ and $z\in\Bbb C$ with $G_1(z)=(2k+1)\pi i$: this shows that $G_2(z)=0$. Prop  3 implies Prop 1: Suppose $f\circ f$ has no FP Then $f$ has no FP, so $f(z)=z+e^{g(z)}$. But now $f(f(z))=z+G_2(z)$,  so saying $f\circ f$ has no FP is saying exactly that $G_2$ has no zero. So Prop 3 implies that $g$ is constant, hence $f$ is a non-trivial translation. In fact the two are equivalent (and hence Prop 3 is in fact true): Prop 1 implies  Prop 3: Say $g$ is non-constant and let $f(z)=z+e^{g(z)}$. Then $f$ is not a translation, so $f\circ f$ has a FP, hence as before $G_2$ has a zero. My ""might seem better motivated"" might be worth justifying, given the bizarre appearance of Prop 3. It's  really perfectly natural: If we're investigating when $f\circ f$ has a FP it's natural to first wonder when $f$ has a FP. Then Prop 2 is trivial, and given Prop 2 the question of when $f\circ f$ has a FP leads naturally to Prop 3, since $f(f(z))=z+G_2(z)$. (And now saying $G_2$ has a zero is clearly equivalent to saying $G_1(z)=(2k+1)\pi i$ for some $z$, and to prove that we only need to show that $G_1$ is non-constant, hence the question ...) Perfectly clear, qed.","Question: If $g$ is a non-constant entire function does it follow that $G_1(z)=g(z)-g\left(z+e^{g(z)}\right)$ is non-constant? The reason I care is it would imply Prop 3 below, which in turn implies Prop 1, giving a proof that might seem better motivated than the proof here . It seems at least plausible. The only thought I've had is a little vague: $-G_1(z)$ is something somewhat like $g'(z)e^{g(z)}$. And $g'e^g$ cannot  be constant:  If $g'e^g=c$ then $e^g=cz+d$. So $cz+d$ has no zero, hence $c=0$, and it follows that $g$ is constant. The problem is that the size of $g'(z)e^{g(z)}+G_1(z)$ depends on the size of $e^g$ and also on the size of $g''$... Context: The other day at the link above I learned something totally new to me: Prop 1. Suppose $f$ is entire. Then $f\circ f$ has no fixed point if and only if $f$ is a nontrivial translation ($f(z)=z+b$, $b\ne0$.) The obvious question is when $f$ has a fixed point - this is trivial: Prop 2. The entire function $f$ has no fixed point if and only if $f(z)=z+e^{g(z)}$ for some entire function $g$. (Proof: $f$ has no fixed point if and only if $f(z)-z$ has no zero...) It occurred to me to try to use Prop 2 to prove Prop 1: Suppose $f\circ f$ has no fixed point. Then $f$ has no  fixed point, so $f(z)=z+e^{g(z)}$, and now we have something to work with in investigating when $f\circ f$ has a fixed point... It  turns out that to get to Prop 1 from Prop 2 we need this: Prop  3 If $g$ is a non-constant entire function then $G_2(z)=e^{g(z)}+e^{g\left(z+e^{g(z)}\right)}$ has a zero. Proving two assertions above: If the answer to the question is yes then Prop 3 follows: If $G_1$ is non-constant then little  Picard shows that there exist $k\in\Bbb Z$ and $z\in\Bbb C$ with $G_1(z)=(2k+1)\pi i$: this shows that $G_2(z)=0$. Prop  3 implies Prop 1: Suppose $f\circ f$ has no FP Then $f$ has no FP, so $f(z)=z+e^{g(z)}$. But now $f(f(z))=z+G_2(z)$,  so saying $f\circ f$ has no FP is saying exactly that $G_2$ has no zero. So Prop 3 implies that $g$ is constant, hence $f$ is a non-trivial translation. In fact the two are equivalent (and hence Prop 3 is in fact true): Prop 1 implies  Prop 3: Say $g$ is non-constant and let $f(z)=z+e^{g(z)}$. Then $f$ is not a translation, so $f\circ f$ has a FP, hence as before $G_2$ has a zero. My ""might seem better motivated"" might be worth justifying, given the bizarre appearance of Prop 3. It's  really perfectly natural: If we're investigating when $f\circ f$ has a FP it's natural to first wonder when $f$ has a FP. Then Prop 2 is trivial, and given Prop 2 the question of when $f\circ f$ has a FP leads naturally to Prop 3, since $f(f(z))=z+G_2(z)$. (And now saying $G_2$ has a zero is clearly equivalent to saying $G_1(z)=(2k+1)\pi i$ for some $z$, and to prove that we only need to show that $G_1$ is non-constant, hence the question ...) Perfectly clear, qed.",,"['complex-analysis', 'entire-functions']"
17,It is possible to get a closed-form for $1+2^i+3^i+\cdots (N-1)^i$?,It is possible to get a closed-form for ?,1+2^i+3^i+\cdots (N-1)^i,"Let $i=\sqrt{-1}$ the complex imaginary unit, taking $$arg(2)=0$$ for the definition of the summand $2^i$ in $$1^i+2^i+3^i+\cdots (N-1)^i,$$ as $$2^i=\cos\log 2+ i\sin\log 2,$$ see [1]. Question. It is possible to get a closed-form (or the best approximation possible), for an integer $N\geq 1$    $$1+2^i+3^i+\cdots (N-1)^i,$$   where the summands are defined in the same way, taking principal branches of complex argument and complex exponentiation ? Thanks in advance , my goal is start to refresh some easy facts in complex variable, please tell me if there are mistakes in the use of previous  definitions. References: [1] MathWorld, http://mathworld.wolfram.com/ComplexExponentiation.html http://mathworld.wolfram.com/ComplexArgument.html","Let $i=\sqrt{-1}$ the complex imaginary unit, taking $$arg(2)=0$$ for the definition of the summand $2^i$ in $$1^i+2^i+3^i+\cdots (N-1)^i,$$ as $$2^i=\cos\log 2+ i\sin\log 2,$$ see [1]. Question. It is possible to get a closed-form (or the best approximation possible), for an integer $N\geq 1$    $$1+2^i+3^i+\cdots (N-1)^i,$$   where the summands are defined in the same way, taking principal branches of complex argument and complex exponentiation ? Thanks in advance , my goal is start to refresh some easy facts in complex variable, please tell me if there are mistakes in the use of previous  definitions. References: [1] MathWorld, http://mathworld.wolfram.com/ComplexExponentiation.html http://mathworld.wolfram.com/ComplexArgument.html",,['complex-analysis']
18,Does there exist an analytic function s.t. $f\left(\frac{1}{n}\right)=2^{-n}.$,Does there exist an analytic function s.t.,f\left(\frac{1}{n}\right)=2^{-n}.,"Prove that there does not exist an analytic function $f$  in an unit disc containing $0$ such that $$f\left(\frac{1}{n}\right)=2^{-n}.$$ I tried by using Identity theorem. Suppose that $f$ is analytic in the unit disc. Consider the function $g(z)=f(z)-2^{-\frac{1}{z}}$. Then the zeros of the function $g(z)$ are $\{\frac{1}{n}:n\in \mathbb N\}$ which has a limit point $0$ in the disc. So, $g$ is identically zero. Then, $f(z)=2^{-1/z}$. But, I am unable to find a point such that we arrive at a contradiction. Please help to find it OR any other technique to prove the question.","Prove that there does not exist an analytic function $f$  in an unit disc containing $0$ such that $$f\left(\frac{1}{n}\right)=2^{-n}.$$ I tried by using Identity theorem. Suppose that $f$ is analytic in the unit disc. Consider the function $g(z)=f(z)-2^{-\frac{1}{z}}$. Then the zeros of the function $g(z)$ are $\{\frac{1}{n}:n\in \mathbb N\}$ which has a limit point $0$ in the disc. So, $g$ is identically zero. Then, $f(z)=2^{-1/z}$. But, I am unable to find a point such that we arrive at a contradiction. Please help to find it OR any other technique to prove the question.",,"['complex-analysis', 'complex-numbers']"
19,Writing the roots of a polynomial with varying coefficients as continuous functions?,Writing the roots of a polynomial with varying coefficients as continuous functions?,,"Consider the monic polynomial $$p_{\zeta}(z) = z^n + a_{n-1}(\zeta)z^{n-1} + \dots + a_0(\zeta), $$ where the $a_{i}$'s are continuous functions defined over $\mathbb{C}$. As is well known, the roots of this polynomial depend 'continuously' on the $a_{i}$'s in some sense. However, I was wondering if a stronger statement holds: is it possible to pick continuous functions $f_1, \dots, f_n$, at least locally, such that the roots of $p_{\zeta}(z)$ are  exactly $f_1(\zeta), \dots, f_n(\zeta)$, counting multiplicities? Edit: Note this is easily answered in the affirmative around a $\zeta_0$ such that $p_{\zeta_0}(z)$ has $n$ distinct roots. The real difficulty is in dealing with the case where $p_{\zeta}(z)$ has repeated roots.","Consider the monic polynomial $$p_{\zeta}(z) = z^n + a_{n-1}(\zeta)z^{n-1} + \dots + a_0(\zeta), $$ where the $a_{i}$'s are continuous functions defined over $\mathbb{C}$. As is well known, the roots of this polynomial depend 'continuously' on the $a_{i}$'s in some sense. However, I was wondering if a stronger statement holds: is it possible to pick continuous functions $f_1, \dots, f_n$, at least locally, such that the roots of $p_{\zeta}(z)$ are  exactly $f_1(\zeta), \dots, f_n(\zeta)$, counting multiplicities? Edit: Note this is easily answered in the affirmative around a $\zeta_0$ such that $p_{\zeta_0}(z)$ has $n$ distinct roots. The real difficulty is in dealing with the case where $p_{\zeta}(z)$ has repeated roots.",,"['complex-analysis', 'analysis', 'algebraic-geometry', 'polynomials', 'roots']"
20,Where is $\operatorname{Log}(z^2-1)$ Analytic?,Where is  Analytic?,\operatorname{Log}(z^2-1),"$\newcommand{\Log}{\operatorname{Log}}$ The question stands as Where is the function $\Log(z^2-1)$ analytic , where $\Log$ stands for the principal complex logarithm. My understanding is that The domain of analyticity of any function $f(z) = \Log\left[g(z)\right]$ , where $g(z)$ is analytic, will be the set of points $z$ such that $g(z)$ is defined and $g(z)$ does not belong to the set $\left \{z = x + iy\ |\ −\infty < x \leq 0, y = 0\right \}$ . Following this definition it would imply that the function $f(z)$ is analytic everywhere in complex plane except for the points where $-\infty<\Re(z^2-1)\leq0$ and $\Im(z^2-1)=0$ . So I get $x^2-y^2-1\leq0$ and $2xy=0$ . Graphically it must be analytic everywhere except on the real x axis, the imaginary y-axis and in the region inside the hyperbola $x^2-y^2=1$ . The answers say Everywhere except $\{z\in\mathbb{R}:|z|\leq1\}\bigcup\{iy:y\in\mathbb{R}\}$ . Please help correct my understanding. Thank you in advance.","The question stands as Where is the function analytic , where stands for the principal complex logarithm. My understanding is that The domain of analyticity of any function , where is analytic, will be the set of points such that is defined and does not belong to the set . Following this definition it would imply that the function is analytic everywhere in complex plane except for the points where and . So I get and . Graphically it must be analytic everywhere except on the real x axis, the imaginary y-axis and in the region inside the hyperbola . The answers say Everywhere except . Please help correct my understanding. Thank you in advance.","\newcommand{\Log}{\operatorname{Log}} \Log(z^2-1) \Log f(z) = \Log\left[g(z)\right] g(z) z g(z) g(z) \left \{z = x + iy\ |\ −\infty < x \leq 0, y = 0\right \} f(z) -\infty<\Re(z^2-1)\leq0 \Im(z^2-1)=0 x^2-y^2-1\leq0 2xy=0 x^2-y^2=1 \{z\in\mathbb{R}:|z|\leq1\}\bigcup\{iy:y\in\mathbb{R}\}","['complex-analysis', 'analyticity']"
21,Exponentiation of a Dirichlet series,Exponentiation of a Dirichlet series,,"I'm trying to understand a proof in Chandrasekharan's Introduction to Analytic Number Theory .  Specifically, the proof of the lemma on p.118 before Dirichlet's theorem on primes in arithmetic progressions. Define $$ Q(s) = \log P(s) $$ for some particular branch of the logarithm for $\sigma > 1$.  If $$ Q(s) = \sum_{n=1}^{\infty} \frac{a_n}{n^s}, $$ which converges absolutely for $\sigma > 1$ and such that the coefficients $a_n$ are nonnegative, how can I conclude that $$ P(s) = e^{Q(s)} = 1 + Q(s) + \frac{Q^2(s)}{2!} + \cdots $$ can be written as a Dirichlet series which is convergent for $\sigma > 1$ and whose coefficients are nonnegative? I know that a product of Dirichlet series with nonnegative coefficients is again a Dirichlet series of nonnegative coefficients which converges on the intersection of the two half-planes of convergence, and hence each of the terms here is a Dirichlet series, but I don't see why an infinite sum of Dirichlet series is necessarily itself a Dirichlet series. Also, how do I show that, if the Dirichlet series of $Q(s)$ converges, so does the Dirichlet series of $P(s)$, and vice versa?","I'm trying to understand a proof in Chandrasekharan's Introduction to Analytic Number Theory .  Specifically, the proof of the lemma on p.118 before Dirichlet's theorem on primes in arithmetic progressions. Define $$ Q(s) = \log P(s) $$ for some particular branch of the logarithm for $\sigma > 1$.  If $$ Q(s) = \sum_{n=1}^{\infty} \frac{a_n}{n^s}, $$ which converges absolutely for $\sigma > 1$ and such that the coefficients $a_n$ are nonnegative, how can I conclude that $$ P(s) = e^{Q(s)} = 1 + Q(s) + \frac{Q^2(s)}{2!} + \cdots $$ can be written as a Dirichlet series which is convergent for $\sigma > 1$ and whose coefficients are nonnegative? I know that a product of Dirichlet series with nonnegative coefficients is again a Dirichlet series of nonnegative coefficients which converges on the intersection of the two half-planes of convergence, and hence each of the terms here is a Dirichlet series, but I don't see why an infinite sum of Dirichlet series is necessarily itself a Dirichlet series. Also, how do I show that, if the Dirichlet series of $Q(s)$ converges, so does the Dirichlet series of $P(s)$, and vice versa?",,"['number-theory', 'complex-analysis', 'analytic-number-theory', 'dirichlet-series']"
22,Characterization of rotations of the Riemann sphere?,Characterization of rotations of the Riemann sphere?,,"Out of curiosity, is there a nice characterization of the linear fractional transformations which give rotations of the Riemann sphere? My thinking was a rotation of the Riemann sphere rotates about some axis, and the two points where the sphere intersects the axis will be two fixed points. What more be said of this?","Out of curiosity, is there a nice characterization of the linear fractional transformations which give rotations of the Riemann sphere? My thinking was a rotation of the Riemann sphere rotates about some axis, and the two points where the sphere intersects the axis will be two fixed points. What more be said of this?",,['complex-analysis']
23,Asymptotics of an integral by two methods,Asymptotics of an integral by two methods,,"I want to compute the asymptotic behavior of the integral  $$ f(K,a)=\int_0^1 (1-x)^Ke^{iKa\frac{x}{1-x}}x^2dx$$ when $K$ is large and $0<a<1$. I tried two different approaches. 1) My first idea was that the exponential, a fast-oscillating function around $x=1$, is killed by the $(1-x)^K$, and the integral should be dominated by the vicinity of $x=0$. Therefore, I put $x=y/K$ and approximate $(1-y/K)^K\approx e^{-y}$ and $\frac{x}{1-x}\approx \frac{y}{K}$ to get $$f(K,a)\approx \frac{1}{K^3}\int_0^\infty e^{-y+iay}y^2dy=\frac{2}{K^3(1-ia)^3}.$$ 2) On the other hand, the stationary phase approximation should be valid. If I write  $$f(K,a)=\int_0^1 e^{KS(x)}x^2dx,$$ with $S(x)=\log(1-x)+iax/(1-x)$, the equation $S'(x_0)=0$ gives $x_0=1-ia$. Second derivative is $S''(x_0)=-1/a^2$. Hence, this idea leads to $$f(K,a)\approx e^{KS(x_0)}x_0^2\sqrt{\frac{\pi a^2}{K}}=(ia)^Ke^{K(1-ia)}(1-ia)^2a\sqrt{\frac{\pi}{K}}.$$ These two results are completely different! I need help understanding this.","I want to compute the asymptotic behavior of the integral  $$ f(K,a)=\int_0^1 (1-x)^Ke^{iKa\frac{x}{1-x}}x^2dx$$ when $K$ is large and $0<a<1$. I tried two different approaches. 1) My first idea was that the exponential, a fast-oscillating function around $x=1$, is killed by the $(1-x)^K$, and the integral should be dominated by the vicinity of $x=0$. Therefore, I put $x=y/K$ and approximate $(1-y/K)^K\approx e^{-y}$ and $\frac{x}{1-x}\approx \frac{y}{K}$ to get $$f(K,a)\approx \frac{1}{K^3}\int_0^\infty e^{-y+iay}y^2dy=\frac{2}{K^3(1-ia)^3}.$$ 2) On the other hand, the stationary phase approximation should be valid. If I write  $$f(K,a)=\int_0^1 e^{KS(x)}x^2dx,$$ with $S(x)=\log(1-x)+iax/(1-x)$, the equation $S'(x_0)=0$ gives $x_0=1-ia$. Second derivative is $S''(x_0)=-1/a^2$. Hence, this idea leads to $$f(K,a)\approx e^{KS(x_0)}x_0^2\sqrt{\frac{\pi a^2}{K}}=(ia)^Ke^{K(1-ia)}(1-ia)^2a\sqrt{\frac{\pi}{K}}.$$ These two results are completely different! I need help understanding this.",,"['complex-analysis', 'definite-integrals', 'asymptotics']"
24,Uniformly bounded sequence of holomorphic functions converges uniformly,Uniformly bounded sequence of holomorphic functions converges uniformly,,"Consider an open connected set $\Omega\subset \mathbb{C}$, and $f_n\subset H(\Omega)$. Suppose $f(z)=\lim_{n\to\infty}f_n(z)$ exists and $|f_n(z)|\leq M$ for all $z\in \Omega$. Show that $$\lim_{n\to\infty}\sup_{z\in K}|f_n(z)-f(z)|=0$$ for any compact $K\subset \Omega$. Note: If correct, I am sure this is a well known result; but I don't see why it's correct, and I haven't found a reference, either.","Consider an open connected set $\Omega\subset \mathbb{C}$, and $f_n\subset H(\Omega)$. Suppose $f(z)=\lim_{n\to\infty}f_n(z)$ exists and $|f_n(z)|\leq M$ for all $z\in \Omega$. Show that $$\lim_{n\to\infty}\sup_{z\in K}|f_n(z)-f(z)|=0$$ for any compact $K\subset \Omega$. Note: If correct, I am sure this is a well known result; but I don't see why it's correct, and I haven't found a reference, either.",,"['complex-analysis', 'limits', 'uniform-convergence']"
25,Complex Analysis - Location of roots of a polynomial,Complex Analysis - Location of roots of a polynomial,,"How many roots does the polynomial $z^4 + 3z^2 + z + 1$ have in the right-half complex plane (i.e. $Re(z) \gt 0$)? I honestly can't think of how to approach the problem as it seems different from the regular Rouche's Theorem problems. I can only say that the answer is either 0, 2 or 4 as all the roots come in complex conjugate pairs. (By the Rational Roots Theorem tested on +1 and -1, the polynomial has no real roots.) Attempt at Solution [1 hour after posting question] After pondering on this question a bit, I wonder if the following argument will work: [The Rational Roots Theorem bit above shows that the number of roots in the right-half plane is either 0, 2 or 4.] The coefficient of $z^3$ in the polynomial is 0, indicating that the sum of the roots of the polynomial is 0. If all 4 roots were in the right-half complex plane, or left-half complex plane, then this coefficient would not be 0. Thus, the polynomial has 2 roots in the right-half complex plane. Could someone comment/help to verify this please? Thanks.","How many roots does the polynomial $z^4 + 3z^2 + z + 1$ have in the right-half complex plane (i.e. $Re(z) \gt 0$)? I honestly can't think of how to approach the problem as it seems different from the regular Rouche's Theorem problems. I can only say that the answer is either 0, 2 or 4 as all the roots come in complex conjugate pairs. (By the Rational Roots Theorem tested on +1 and -1, the polynomial has no real roots.) Attempt at Solution [1 hour after posting question] After pondering on this question a bit, I wonder if the following argument will work: [The Rational Roots Theorem bit above shows that the number of roots in the right-half plane is either 0, 2 or 4.] The coefficient of $z^3$ in the polynomial is 0, indicating that the sum of the roots of the polynomial is 0. If all 4 roots were in the right-half complex plane, or left-half complex plane, then this coefficient would not be 0. Thus, the polynomial has 2 roots in the right-half complex plane. Could someone comment/help to verify this please? Thanks.",,"['complex-analysis', 'polynomials', 'roots']"
26,Books on complex analysis,Books on complex analysis,,"Is there any book on $1$-dimensional complex analysis, where all is written in the language of sheaf theory? It's clear, that a lot of constructions can be formulated in simplier way using it. There are a lot of such books of n-dimensional complex analysis. And what about 1-dimensional?","Is there any book on $1$-dimensional complex analysis, where all is written in the language of sheaf theory? It's clear, that a lot of constructions can be formulated in simplier way using it. There are a lot of such books of n-dimensional complex analysis. And what about 1-dimensional?",,"['complex-analysis', 'reference-request', 'riemann-surfaces', 'sheaf-theory']"
27,How to show that the entire function $f(z) = z^2 + \cos{z}$ has range all of $\mathbb{C}$?,How to show that the entire function  has range all of ?,f(z) = z^2 + \cos{z} \mathbb{C},"I have been thinking about the following exercise from an old complex analysis qualifier exam for some days but I still don't know how to solve it. The problem is as follows: Show that the entire function $f(z) := z^2 + \cos{z}$  has range all of $\mathbb{C}$. At first I thought that maybe I could use Picard's Little Theorem to get a contradiction. I thought that maybe by considering the function $e^{f(z)}$ I could get the contradiction by asssuming that $f(z)$ misses one point so that the exponential would miss two points and that would contradict Picard's Little Theorem, but since the exponential is periodic this argument doesn't work. So my question is how can this be proved? Thank you very much for any help.","I have been thinking about the following exercise from an old complex analysis qualifier exam for some days but I still don't know how to solve it. The problem is as follows: Show that the entire function $f(z) := z^2 + \cos{z}$  has range all of $\mathbb{C}$. At first I thought that maybe I could use Picard's Little Theorem to get a contradiction. I thought that maybe by considering the function $e^{f(z)}$ I could get the contradiction by asssuming that $f(z)$ misses one point so that the exponential would miss two points and that would contradict Picard's Little Theorem, but since the exponential is periodic this argument doesn't work. So my question is how can this be proved? Thank you very much for any help.",,['complex-analysis']
28,Another Tangent on Tangents,Another Tangent on Tangents,,"This question asked yesterday got me thinking. While the derivatives of the tangent function span an infinite dimensional vector space over $\mathbb{C},$ the transcendence degree of the field generated by these derivatives is finite. Here are two ways to see this. One is to observe that $$X^2 - DX + 1 = 0$$ where $X$ denotes the tangent function. While a better way is to observe that via the sum/product rule, the field obtained by adjoining the set of derivatives of a function $f$ to $\mathbb{C}$ denoted $\mathbb{C}(\mathcal{D}f)$ is closed under differentiation and $\mathbb{C}(\mathrm{tan}) \subset \mathbb{C}(Exp) = \mathbb{C}(\mathcal{D}Exp)$ In fact, using the basic rules of differentiation, one can show that if the transcendence degrees over $\mathbb{C}$ of $\mathbb{C}(\mathcal{D}f)$ and $\mathbb{C}(\mathcal{D}g)$ are finite so too are the transcendence degrees of $\mathbb{C}(\mathcal{D}(f + g)),$ $\mathbb{C}(\mathcal{D}(fg))$ and $\mathbb{C}(\mathcal{D}(f \circ g)).$ Which brings me to my question: Do there exist any functions $f$ which are meromorhic on $\mathbb{C}$ such that the transcendence degree of  $\mathbb{C}(\mathcal{D}f)$ over $\mathbb{C}$ is infinite?","This question asked yesterday got me thinking. While the derivatives of the tangent function span an infinite dimensional vector space over $\mathbb{C},$ the transcendence degree of the field generated by these derivatives is finite. Here are two ways to see this. One is to observe that $$X^2 - DX + 1 = 0$$ where $X$ denotes the tangent function. While a better way is to observe that via the sum/product rule, the field obtained by adjoining the set of derivatives of a function $f$ to $\mathbb{C}$ denoted $\mathbb{C}(\mathcal{D}f)$ is closed under differentiation and $\mathbb{C}(\mathrm{tan}) \subset \mathbb{C}(Exp) = \mathbb{C}(\mathcal{D}Exp)$ In fact, using the basic rules of differentiation, one can show that if the transcendence degrees over $\mathbb{C}$ of $\mathbb{C}(\mathcal{D}f)$ and $\mathbb{C}(\mathcal{D}g)$ are finite so too are the transcendence degrees of $\mathbb{C}(\mathcal{D}(f + g)),$ $\mathbb{C}(\mathcal{D}(fg))$ and $\mathbb{C}(\mathcal{D}(f \circ g)).$ Which brings me to my question: Do there exist any functions $f$ which are meromorhic on $\mathbb{C}$ such that the transcendence degree of  $\mathbb{C}(\mathcal{D}f)$ over $\mathbb{C}$ is infinite?",,"['complex-analysis', 'algebraic-geometry', 'ordinary-differential-equations', 'function-fields']"
29,How to properly understand branches of complex functions,How to properly understand branches of complex functions,,"$\DeclareMathOperator{\Log}{Log}$ I have several problems to understand the concept of branches and how to find analytic branches. From what I learned, for example for the complex logarithm, it is a multi valued function, and if we want it to be analytic we have to cut some part of the domain (because otherwise we get different limits in the same point). I understand then why $\Log(z)$ is analytic in the branch $\mathbb{C} \setminus (-\infty ,0]$ , since we can never complete a full circle around $0$ . Here it is a simple case so it is easy to see that we always need to throw a ray from the origin. My confusion starts when the function is not that simple. Let's take the function $Log(z^2-1)$ . I can understand why on the domain $\{ |z| < 1\}$ an analytic branch would be $\mathbb{C} \setminus [0,\infty )$ , since this function takes the unit circle to itself and moves it left by $1$ . So, the ray $[0,\infty )$ doesn't intersect with it. But what if the domain is $\{ |z| > 1\}$ ? How do I work with it since there is not such a pretty way? I thought of maybe using the main branch of the logarithm, and seeing where $z^2-1 \in (-\infty ,0]$ , but is it what needs to be done. Moreover, what about functions like $\sqrt{z^2-1}$ ? How do I start to look for an analytic branch there? It seems logical that the points $1$ and $-1$ play a part here but I am not sure how. Another thing is, how do I solve integral with such functions? For example $$\int_{|z| = 2} \sqrt{z^2-1}$$ When the branch is defined in the following way: $$\sqrt{z^2-1} = z\sqrt{1-\frac {1}{z^2}} = z\exp[\frac{1}{2}Log(1-\frac {1}{z^2})]$$ How does the definition of the branch even play a part here? Another example could be the integral: $$\int_{|z|=2} \frac{1}{\sqrt{z^4+4z+1}}$$ when $\sqrt{25} = 5$ Help would be tremendously appreciated. I someone could walk me thorugh an entire example, I would be really glad.","I have several problems to understand the concept of branches and how to find analytic branches. From what I learned, for example for the complex logarithm, it is a multi valued function, and if we want it to be analytic we have to cut some part of the domain (because otherwise we get different limits in the same point). I understand then why is analytic in the branch , since we can never complete a full circle around . Here it is a simple case so it is easy to see that we always need to throw a ray from the origin. My confusion starts when the function is not that simple. Let's take the function . I can understand why on the domain an analytic branch would be , since this function takes the unit circle to itself and moves it left by . So, the ray doesn't intersect with it. But what if the domain is ? How do I work with it since there is not such a pretty way? I thought of maybe using the main branch of the logarithm, and seeing where , but is it what needs to be done. Moreover, what about functions like ? How do I start to look for an analytic branch there? It seems logical that the points and play a part here but I am not sure how. Another thing is, how do I solve integral with such functions? For example When the branch is defined in the following way: How does the definition of the branch even play a part here? Another example could be the integral: when Help would be tremendously appreciated. I someone could walk me thorugh an entire example, I would be really glad.","\DeclareMathOperator{\Log}{Log} \Log(z) \mathbb{C} \setminus (-\infty ,0] 0 Log(z^2-1) \{ |z| < 1\} \mathbb{C} \setminus [0,\infty ) 1 [0,\infty ) \{ |z| > 1\} z^2-1 \in (-\infty ,0] \sqrt{z^2-1} 1 -1 \int_{|z| = 2} \sqrt{z^2-1} \sqrt{z^2-1} = z\sqrt{1-\frac {1}{z^2}} = z\exp[\frac{1}{2}Log(1-\frac {1}{z^2})] \int_{|z|=2} \frac{1}{\sqrt{z^4+4z+1}} \sqrt{25} = 5","['complex-analysis', 'branch-cuts', 'branch-points']"
30,Show existence of holomorphic function $g$ such that $f(g(z)) = g(z^n)$,Show existence of holomorphic function  such that,g f(g(z)) = g(z^n),"Let $f:\mathbb{C} \rightarrow \mathbb{C}$ be a holomorphic function, with $f(0)=0$, and $n\ge 0$ the multiplicity of $0$ as a zero of $f$. Show that there exist a holomorphic function $g:\mathbb{C}\rightarrow \mathbb{C}$ with $g(0)=0$ such that $$f(g(z)) = g(z^n).$$ I know the existence of $g$ such that $f(g(z)) = z^n$, but the above seems out of reach. Any ideas or possible sources for further research?","Let $f:\mathbb{C} \rightarrow \mathbb{C}$ be a holomorphic function, with $f(0)=0$, and $n\ge 0$ the multiplicity of $0$ as a zero of $f$. Show that there exist a holomorphic function $g:\mathbb{C}\rightarrow \mathbb{C}$ with $g(0)=0$ such that $$f(g(z)) = g(z^n).$$ I know the existence of $g$ such that $f(g(z)) = z^n$, but the above seems out of reach. Any ideas or possible sources for further research?",,['complex-analysis']
31,How to find roots of $f(z)=(a+yg(z))^2+g(z)^2=0$?,How to find roots of ?,f(z)=(a+yg(z))^2+g(z)^2=0,"I want to find the roots of  $$f(z)=\left[a+zg(z)\right]^2+g(z)^2=0$$ Where $a$ is real number and: $$ g(z)=\frac{1}{2\sqrt{z^2+1}}\ln\left(\frac{z+\sqrt{z^2+1}}{z-\sqrt{z^2+1}}\right) $$ It said that $f(z)=0$ has double complex roots when $a\in(-\pi/2,\pi/2)$, no complex root when $a>\pi/2$ and four complex roots when $a<-\pi/2$. It also said that the complex roots are pure imaginary, and come in conjugate pairs. The paper is published in a well known journal, here is the link , see page 3. What I tried is that expand f(z) around $z=0$, I get: $$ f(z)=(a^2-\frac{\pi^2}{4}) +i\pi(a+1)z +(1+2a)z^2+O(z^3) $$ The constant term is $a^2-\pi^2/4$, does this constant term give the above claim? I don't know. My question is: 1). how to determine the number of complex roots for different value of $a$? 2).If possible, can you express the roots as a function of $a$? 3).If it is very hard to express analytically, could you give a hint how can I find all of them numerically?","I want to find the roots of  $$f(z)=\left[a+zg(z)\right]^2+g(z)^2=0$$ Where $a$ is real number and: $$ g(z)=\frac{1}{2\sqrt{z^2+1}}\ln\left(\frac{z+\sqrt{z^2+1}}{z-\sqrt{z^2+1}}\right) $$ It said that $f(z)=0$ has double complex roots when $a\in(-\pi/2,\pi/2)$, no complex root when $a>\pi/2$ and four complex roots when $a<-\pi/2$. It also said that the complex roots are pure imaginary, and come in conjugate pairs. The paper is published in a well known journal, here is the link , see page 3. What I tried is that expand f(z) around $z=0$, I get: $$ f(z)=(a^2-\frac{\pi^2}{4}) +i\pi(a+1)z +(1+2a)z^2+O(z^3) $$ The constant term is $a^2-\pi^2/4$, does this constant term give the above claim? I don't know. My question is: 1). how to determine the number of complex roots for different value of $a$? 2).If possible, can you express the roots as a function of $a$? 3).If it is very hard to express analytically, could you give a hint how can I find all of them numerically?",,"['complex-analysis', 'numerical-methods', 'roots']"
32,"Lower boundary for $ |f(z) - 1/z| $, where $ f(z) $ is holomorphic","Lower boundary for , where  is holomorphic", |f(z) - 1/z|   f(z) ,"I've been trying to prove the following statement: Let $ f:U \rightarrow \mathbb{C} $ be holomorphic with $ \overline{B(0, R)} \subset U$. Suppose $ r < R $. Prove that  $$ \sup\limits_{r \leq |z| \leq R}\left(\left|f(z) - \frac{1}{z}\right| \right) \geq \frac{1}{R}$$ It's a way of proving that $ \frac{1}{z} $ is not a uniform limit of a sequence of polynomials on a ,,ring'' $ r \leq |z| \leq R $. I tried to use Cauchy's integral theorem to evaluate $ f(z) $, however it didn't lead me anywhere close to the solution. I would appreciate some help with this exercise","I've been trying to prove the following statement: Let $ f:U \rightarrow \mathbb{C} $ be holomorphic with $ \overline{B(0, R)} \subset U$. Suppose $ r < R $. Prove that  $$ \sup\limits_{r \leq |z| \leq R}\left(\left|f(z) - \frac{1}{z}\right| \right) \geq \frac{1}{R}$$ It's a way of proving that $ \frac{1}{z} $ is not a uniform limit of a sequence of polynomials on a ,,ring'' $ r \leq |z| \leq R $. I tried to use Cauchy's integral theorem to evaluate $ f(z) $, however it didn't lead me anywhere close to the solution. I would appreciate some help with this exercise",,['complex-analysis']
33,What are some good sources to learn about real analytic manifolds?,What are some good sources to learn about real analytic manifolds?,,"Asking this question as someone with a graduate student level understanding of smooth differential/Riemannian geometry (May be a bit more that Riemannian Geometry by Do Carmo). I am trying to upgrade my knowledge to the real-analytic set up. Also my target is not to go in the direction of complex algebraic geometry (like Griffiths-Harris). I am primarily interested in building ""analytic diffeomorphisms"" and ""analytic flows"" on abstract analytic manifolds. I am interested in some basic materials which will eventually lead me in that direction. Another question I asked, that may be more precise: What are some general strategies to build measure preserving real-analytic diffeomorphisms?","Asking this question as someone with a graduate student level understanding of smooth differential/Riemannian geometry (May be a bit more that Riemannian Geometry by Do Carmo). I am trying to upgrade my knowledge to the real-analytic set up. Also my target is not to go in the direction of complex algebraic geometry (like Griffiths-Harris). I am primarily interested in building ""analytic diffeomorphisms"" and ""analytic flows"" on abstract analytic manifolds. I am interested in some basic materials which will eventually lead me in that direction. Another question I asked, that may be more precise: What are some general strategies to build measure preserving real-analytic diffeomorphisms?",,"['complex-analysis', 'reference-request', 'differential-geometry', 'manifolds', 'complex-geometry']"
34,What is the sum of all complex integers?,What is the sum of all complex integers?,,"In line with $$\zeta(-1)=-1/12$$ Could we, by considering $$f(s)=\sum_{a,b\in\mathbb Z,\;(a,b)\neq(0,0)}\frac{1}{(a+bi)^{s}}$$ Evaluate the sum of all complex integers?","In line with $$\zeta(-1)=-1/12$$ Could we, by considering $$f(s)=\sum_{a,b\in\mathbb Z,\;(a,b)\neq(0,0)}\frac{1}{(a+bi)^{s}}$$ Evaluate the sum of all complex integers?",,"['complex-analysis', 'divergent-series', 'zeta-functions']"
35,$n$ Torus contained in the closure of the image of the unit disc under a holomorphic map?,Torus contained in the closure of the image of the unit disc under a holomorphic map?,n,"I have the following question. Does there exists a holomorphic function $\varphi\in\mathcal{O}(\mathbb{D},\mathbb{D}^{n})$ such that $\mathbb{T}^n\subseteq\overline{\varphi(\mathbb{D})},$ where $n\geq2$?","I have the following question. Does there exists a holomorphic function $\varphi\in\mathcal{O}(\mathbb{D},\mathbb{D}^{n})$ such that $\mathbb{T}^n\subseteq\overline{\varphi(\mathbb{D})},$ where $n\geq2$?",,"['complex-analysis', 'several-complex-variables']"
36,"Fixed, attracting points are Fatou points","Fixed, attracting points are Fatou points",,"Let $f$ be a holomorphic function on an open, connected set $\Omega\subset \mathbb{C}$ with $z_0\in \Omega$ a fixed point, and $\{f^n\}_{n\in \mathbb{N}}$ the sequence of iterates. I want to prove that if $|f'(z_0)|<1$ then there is a neighborhood of $z_0$ such that $\{f^n\}$ is normal in it. I don't really know what to do, because I don't know how to handle the iterates. I think the fact that $|f(z)| < |z-z_0| + |z_0| + \epsilon |z-z_0|$ might be useful to apply Montel's theorem, but I didn't get very far. This is not homework, it's a problem I'm trying to solve as preparation for a complex analysis exam. Also: I'd be grateful to have a geometrical interpretation of $|f'(z_0)|<1$ or of $|f'|$ in general. I don't really understand what does it mean for $|f'(z_0)|<1$, for example in Schwarz's lemma.","Let $f$ be a holomorphic function on an open, connected set $\Omega\subset \mathbb{C}$ with $z_0\in \Omega$ a fixed point, and $\{f^n\}_{n\in \mathbb{N}}$ the sequence of iterates. I want to prove that if $|f'(z_0)|<1$ then there is a neighborhood of $z_0$ such that $\{f^n\}$ is normal in it. I don't really know what to do, because I don't know how to handle the iterates. I think the fact that $|f(z)| < |z-z_0| + |z_0| + \epsilon |z-z_0|$ might be useful to apply Montel's theorem, but I didn't get very far. This is not homework, it's a problem I'm trying to solve as preparation for a complex analysis exam. Also: I'd be grateful to have a geometrical interpretation of $|f'(z_0)|<1$ or of $|f'|$ in general. I don't really understand what does it mean for $|f'(z_0)|<1$, for example in Schwarz's lemma.",,"['complex-analysis', 'dynamical-systems']"
37,Intuition for a Proof of the Fundamental Theorem of Algebra (According to 3Blue1Brown),Intuition for a Proof of the Fundamental Theorem of Algebra (According to 3Blue1Brown),,"I have recently watched this video by $3$ Blue $1$ Brown, regarding winding numbers and the fundamental theorem of algebra. I am trying to formalize the idea he shows in the video (starting at 21:26). Specifically, I am trying to connect his proof to a one I've seen in complex analysis classes: Theorem. Fundamental theorem of algebra. Let $P(z)=z^d+a_1 z^{d-1}+a_2z^{d-2}+\ldots+a_{d-1}z+a_d\in \Bbb{C}[x]$ be a monic complex polynomial of degree $d$ . So the sum of all orders of $P$ 's zeroes is exactly $d$ . Proof. For a function $f$ , denote the sum of orders of all its zeros as $N_f$ . Let $Q(z)=z^d$ . It is obvious that $N_Q=d$ . Subtracting $Q$ from $P$ gives a polynomial of degree $d-1$ , therefore $\frac{|P(z)-Q(z)|}{|Q(z)|}\to0$ as $z\to\infty$ . Let $R_0$ be large enough so that for every $R>R_0$ , every $|z|=R$ satisfies $|P(z)-Q(z)|\le |Q(z)|$ . Using Rouché's theorem for the path $\gamma(t)=R e^{it}$ for $t\in[0,2\pi]$ , we deduce that $N_P=N_Q=d$ . $\tag*{$\blacksquare$}$ Understanding 3Blue1Brwon's Video In his video, $3$ B $1$ B defines a winding number of a complex function on a path as the total number of times the image of the function on the path ""goes through all hues"" of the color map (with negative and positive directions), and explains why a nonzero winding number means that the function has a zero within the area of the path. Qustion 1: What does he mean by this winding number? I.e., what is the formal definition of a ""winding number"" of a function ? I am familiar with the definition $$\operatorname{Ind}_\gamma (a) := \frac{1}{2\pi i} \oint_\gamma \frac{\text{d}z}{z-a},$$ which defines the index of a (closed) path in respect to a point . He then explains that the winding number of $Q(z)=z^d$ (in his video, $d=5$ ) around $0$ is $d$ (around some circular path). This I can intuitively understand - as $x$ goes along the unit circle, for example, the function $x^d$ goes around the circle $d$ times. Qustion 2: How is it connected to $0$ being a zero of order $d$ of $Q(z)$ ? Now, as $z\to\infty$ , the leading term of $P(z)$ is the only significant one, so $3$ B $1$ B explains that in a large enough circle, the index of $P$ and $Q$ will be the same. This argument is quite similar to the one in the proof above - Qustion 3: I assume that using Rouché's theorem ""hides under the surface"" some of the intuition in the proof. Is there a way of explicitly using this index equality of $P$ and $Q$ in a large enough circle formally in order to prove the fundamental theorem? Moreover, I know that paths that are homotopy equivalences, have the same index. Are $P$ and $Q$ in fact homotopy equivalences in the area outside the circle? Summing up, I am looking for a more ""mathematically complete"" explanation of $3$ B $1$ B's argument, rigorous yet intuitive. Edit: Question 4: Rouché's theorem is proved using the so called argument principle, as Oliver Diaz's answer mentions. I assume that, intuitivly, we would have wanted to write $\int_\gamma \frac{f'(z)}{f(z)} \ \text{d}z=\ln(b)-\ln(a)$ (if $\gamma(0)=a$ and $\gamma(1)=b$ ), but that is of course nonsense, since the complex logarithm is not holomorphic in $[-\infty,0]$ . That does remind me of the work done in a non-conservative field. Is this really the intuitive reason that the argument principle ""works""?","I have recently watched this video by Blue Brown, regarding winding numbers and the fundamental theorem of algebra. I am trying to formalize the idea he shows in the video (starting at 21:26). Specifically, I am trying to connect his proof to a one I've seen in complex analysis classes: Theorem. Fundamental theorem of algebra. Let be a monic complex polynomial of degree . So the sum of all orders of 's zeroes is exactly . Proof. For a function , denote the sum of orders of all its zeros as . Let . It is obvious that . Subtracting from gives a polynomial of degree , therefore as . Let be large enough so that for every , every satisfies . Using Rouché's theorem for the path for , we deduce that . Understanding 3Blue1Brwon's Video In his video, B B defines a winding number of a complex function on a path as the total number of times the image of the function on the path ""goes through all hues"" of the color map (with negative and positive directions), and explains why a nonzero winding number means that the function has a zero within the area of the path. Qustion 1: What does he mean by this winding number? I.e., what is the formal definition of a ""winding number"" of a function ? I am familiar with the definition which defines the index of a (closed) path in respect to a point . He then explains that the winding number of (in his video, ) around is (around some circular path). This I can intuitively understand - as goes along the unit circle, for example, the function goes around the circle times. Qustion 2: How is it connected to being a zero of order of ? Now, as , the leading term of is the only significant one, so B B explains that in a large enough circle, the index of and will be the same. This argument is quite similar to the one in the proof above - Qustion 3: I assume that using Rouché's theorem ""hides under the surface"" some of the intuition in the proof. Is there a way of explicitly using this index equality of and in a large enough circle formally in order to prove the fundamental theorem? Moreover, I know that paths that are homotopy equivalences, have the same index. Are and in fact homotopy equivalences in the area outside the circle? Summing up, I am looking for a more ""mathematically complete"" explanation of B B's argument, rigorous yet intuitive. Edit: Question 4: Rouché's theorem is proved using the so called argument principle, as Oliver Diaz's answer mentions. I assume that, intuitivly, we would have wanted to write (if and ), but that is of course nonsense, since the complex logarithm is not holomorphic in . That does remind me of the work done in a non-conservative field. Is this really the intuitive reason that the argument principle ""works""?","3 1 P(z)=z^d+a_1 z^{d-1}+a_2z^{d-2}+\ldots+a_{d-1}z+a_d\in \Bbb{C}[x] d P d f N_f Q(z)=z^d N_Q=d Q P d-1 \frac{|P(z)-Q(z)|}{|Q(z)|}\to0 z\to\infty R_0 R>R_0 |z|=R |P(z)-Q(z)|\le |Q(z)| \gamma(t)=R e^{it} t\in[0,2\pi] N_P=N_Q=d \tag*{\blacksquare} 3 1 \operatorname{Ind}_\gamma (a) := \frac{1}{2\pi i} \oint_\gamma \frac{\text{d}z}{z-a}, Q(z)=z^d d=5 0 d x x^d d 0 d Q(z) z\to\infty P(z) 3 1 P Q P Q P Q 3 1 \int_\gamma \frac{f'(z)}{f(z)} \ \text{d}z=\ln(b)-\ln(a) \gamma(0)=a \gamma(1)=b [-\infty,0]","['complex-analysis', 'intuition', 'complex-integration']"
38,Questions on formulas for the reciprocal gamma function $\frac{1}{\Gamma(s)}$,Questions on formulas for the reciprocal gamma function,\frac{1}{\Gamma(s)},"This question is related to the following formulas for the reciprocal gamma function $\frac{1}{\Gamma(s)}$ where formula (2) represents the analytic continuation of the sum over $k$ in formula (1). The parameter $f$ in formula (1) is assumed to be a positive integer. (1) $\quad\frac{1}{\Gamma(s)}=\underset{N,f\to\infty\land M(N)=0}{\text{lim}}\left(e^2\,2^{-s}\sum\limits_{n=1}^N\mu(n)\,n^{s-1}\sum\limits_{k=1}^{f\,n}\frac{(n+i \pi k)^s+(n-i \pi k)^s}{\left(n^2+\pi^2 k^2\right)^s}\right),\quad\Re(s)>0$ (2) $\quad\frac{1}{\Gamma[s]}=\underset{N\to\infty\land M(N)=0}{\text{lim}}\left(e^2\,(2 \pi)^{-s}\sum\limits_{n=1}^N\mu(n)\,n^{s-1}\left(i^s \zeta\left(s,1+\frac{i n}{\pi}\right)+(-i)^s \zeta\left(s,1-\frac{i n}{\pi}\right)\right)\right),\quad s\in\mathbb{C}$ A conditional convergence requirement of the two formulas above is $M(N)=0$ where $M(N)=\sum_{n\le N}\mu(n)$ is the Mertens function . Note the condition $M(N)=0$ can be met for arbitrarily large magnitudes of $N$ since the Mertens function has an infinite number of integer zeros. See https://oeis.org/A028442 for the zeros of the Mertens function. Formulas (1) and (2) above for $\frac{1}{\Gamma(s)}$ are illustrated in Figures (1) to (6) following the questions below. Question (1) : Is it true formulas (1) and (2) for $\frac{1}{\Gamma(s)}$ converge for $\Re(s)>0$ and $s\in\mathbb{C}$ respectively? In response to the answer posted by reuns, I added information on the derivation of formula (1) for $\frac{1}{\Gamma(s)}$ to the end of my question below. I don't believe the derivation in the answer posted by reuns is equivalent to my derivation as I don't believe an expression in terms of $\zeta(s,nr/2\pi)$ is equivalent to an expression in terms of $\left(i^s \zeta\left(s,1+\frac{i n}{\pi}\right)+(-i)^s \zeta\left(s,1-\frac{i n}{\pi}\right)\right)$ . The Mellin and Laplace transforms of $\frac{1}{\Gamma(s)}$ defined in formula (1) above are defined in formulas (3) and (4) below. I find it interesting that the $\Gamma(z)$ term appears in the Mellin transform of $\frac{1}{\Gamma(s)}$ illustrated in formula (3) below. I'm not sure the Laplace transform defined in formula (4) below is valid as Mathematica indicates the transform integral of the underlying term of the series in formula (1) for $\frac{1}{\Gamma(s)}$ is only valid for $n\le 1$ . Formulas (3) and (4) are illustrated in Figures (7) and (8) following the question below. (3) $\quad\mathcal{M}_s\left[\frac{1}{\Gamma (s)}\right](z)=\int\limits_0^\infty \frac{1}{\Gamma(s)} s^{z-1}\,ds=\\$ $\qquad\underset{N,f\to\infty\land M(N)=0}{\text{lim}}\left(e^2\,\Gamma(z)\sum\limits_{n=1}^N\frac{\mu(n)}{n}\sum\limits_{k=1}^{f n}\left(\left(\log(2)-\log\left(\frac{n}{n+i \pi k}\right)\right)^{-z}+\left(\log(2)-\log\left(\frac{n}{n-i \pi k}\right)\right)^{-z}\right)\right)$ (4) $\quad\mathcal{L}_s\left[\frac{1}{\Gamma (s)}\right](z)=\int\limits_0^\infty \frac{1}{\Gamma(s)} e^{-z s}\,ds=\\$ $\qquad\underset{N,f\to\infty\land M(N)=0}{\text{lim}}\left(e^2\sum\limits_{n=1}^N\frac{\mu(n)}{n}\sum\limits_{k=1}^{f n} \left(\frac{1}{z+\log\left(2-\frac{2 i \pi k}{n}\right)}+\frac{1}{z+\log\left(2+\frac{2 i \pi k}{n}\right)}\right)\right)$ Question (2) : Are there a closed form expressions for the Mellin and Laplace transforms of $\frac{1}{\Gamma(s)}$ ? If not, are there other formulas for the Mellin and Laplace transforms of $\frac{1}{\Gamma(s)}$ that can be used to compare against formulas (3) and (4) above? Figure (1) below illustrates formula (1) for $\frac{1}{\Gamma(s)}$ for real $s$ , and figures (2) and (3) below illustrate the real and imaginary parts of formula (1) for $\frac{1}{\Gamma(s)}$ evaluated along the line $s=1+i t$ . All three plots are evaluated at $f=4$ and the orange and green curves represent the evaluation limits $N=39$ and $N=101$ respectively overlaid on the reference function in blue. Figure (1) : Illustration of formula (1) for $\frac{1}{\Gamma(s)}$ Figure (2) : Illustration of formula (1) for $\Re\left(\frac{1}{\Gamma(1+i t)}\right)$ Figure (3) : Illustration of formula (1) for $\Im\left(\frac{1}{\Gamma(1+i t)}\right)$ Figure (4) below illustrates formula (2) for $\frac{1}{\Gamma(s)}$ for real $s$ , and figures (5) and (6) below illustrate the real and imaginary parts of formula (2) for $\frac{1}{\Gamma(s)}$ evaluated along the line $s=i t$ . In all three plots the orange and green curves represent the evaluation limits $N=39$ and $N=101$ respectively overlaid on the reference function in blue. Figure (4) : Illustration of formula (2) for $\frac{1}{\Gamma(s)}$ Figure (5) : Illustration of formula (2) for $\Re\left(\frac{1}{\Gamma(i t)}\right)$ Figure (6) : Illustration of formula (2) for $\Im\left(\frac{1}{\Gamma(i t)}\right)$ Figures (7) and (8) below illustrates formulas (3) and (4) which are the Mellin and Laplace transforms of formula (1) for $\frac{1}{\Gamma(s)}$ . Both plots are evaluated at $f=4$ and the orange and green curves represent the evaluation limits $N=39$ and $N=101$ respectively. Figure (7) : Illustration of formula (3) which is the Mellin transform of formula (1) for $\frac{1}{\Gamma(s)}$ Figure (8) : Illustration of formula (4) which is the Laplace transform of formula (1) for $\frac{1}{\Gamma(s)}$ In response to the answer posted below by reuns below, actually I derived formula (1) from the relationship $$y^{-s}=e^2\int\limits_1^\infty x^{-3}\,\delta(\log(x)-1)\frac{\left(\frac{y}{\log(x)}\right)^{-s}}{\log(x)}\,dx\tag{5}$$ using the nested Fourier series representation of $\delta(x-1)$ . This leads to $$y^{-s}=\underset{N,f\to\infty\land M(N)=0}{\text{lim}}\left(e^2\,2^{-s}\,y^{-s}\,\Gamma(s)\sum\limits_{n=1}^N\mu(n)\,n^{s-1}\sum\limits_{k=1}^{f\,n}\frac{(n+i \pi k)^s+(n-i \pi k)^s}{\left(\pi^2 k^2+n^2\right)^s}\right)\tag{6}$$ which is valid for $\Re(s)>0$ . Dividing both sides by $y^{-s}\,\Gamma(s)$ leads to $$\frac{1}{\Gamma(s)}=\underset{N,f\to\infty\land M(N)=0}{\text{lim}}\left(e^2\,2^{-s}\sum\limits_{n=1}^N\mu(n)\,n^{s-1}\sum\limits_{k=1}^{f\,n}\frac{(n+i \pi k)^s+(n-i \pi k)^s}{\left(\pi^2 k^2+n^2\right)^s}\right)\tag{7}$$ which is also valid for $\Re(s)>0$ and identical to formula (1) above. The $M(N)=0$ restriction is because the nested Fourier series representation of $\delta(x-1)$ only converges at $x=0$ when $M(N)=0$ . For more information on derivation of formulas via Mellin convolutions using the nested Fourier series representation of $\delta(x-1)$ , see the answer I posted to my own question at https://math.stackexchange.com/q/2380164 . The analytic continuation $$\sum _{k=1}^{\infty } \frac{(n+i \pi  k)^s+(n-i \pi  k)^s}{\left(\pi^2 k^2+n^2\right)^s}=\pi^{-s} \left(i^s \zeta\left(s,1+\frac{i n}{\pi}\right)+(-i)^s \zeta\left(s,1-\frac{i n}{\pi}\right)\right)\tag{8}$$ leads to formula (2) above for $\frac{1}{\Gamma(s)}$ .  I don't believe the derivation in the answer posted by reuns is equivalent to my derivation as I don't believe an expression in terms of $\zeta(s,nr/2\pi)$ is equivalent to an expression in terms of $\left(i^s \zeta\left(s,1+\frac{i n}{\pi}\right)+(-i)^s \zeta\left(s,1-\frac{i n}{\pi}\right)\right)$ .","This question is related to the following formulas for the reciprocal gamma function where formula (2) represents the analytic continuation of the sum over in formula (1). The parameter in formula (1) is assumed to be a positive integer. (1) (2) A conditional convergence requirement of the two formulas above is where is the Mertens function . Note the condition can be met for arbitrarily large magnitudes of since the Mertens function has an infinite number of integer zeros. See https://oeis.org/A028442 for the zeros of the Mertens function. Formulas (1) and (2) above for are illustrated in Figures (1) to (6) following the questions below. Question (1) : Is it true formulas (1) and (2) for converge for and respectively? In response to the answer posted by reuns, I added information on the derivation of formula (1) for to the end of my question below. I don't believe the derivation in the answer posted by reuns is equivalent to my derivation as I don't believe an expression in terms of is equivalent to an expression in terms of . The Mellin and Laplace transforms of defined in formula (1) above are defined in formulas (3) and (4) below. I find it interesting that the term appears in the Mellin transform of illustrated in formula (3) below. I'm not sure the Laplace transform defined in formula (4) below is valid as Mathematica indicates the transform integral of the underlying term of the series in formula (1) for is only valid for . Formulas (3) and (4) are illustrated in Figures (7) and (8) following the question below. (3) (4) Question (2) : Are there a closed form expressions for the Mellin and Laplace transforms of ? If not, are there other formulas for the Mellin and Laplace transforms of that can be used to compare against formulas (3) and (4) above? Figure (1) below illustrates formula (1) for for real , and figures (2) and (3) below illustrate the real and imaginary parts of formula (1) for evaluated along the line . All three plots are evaluated at and the orange and green curves represent the evaluation limits and respectively overlaid on the reference function in blue. Figure (1) : Illustration of formula (1) for Figure (2) : Illustration of formula (1) for Figure (3) : Illustration of formula (1) for Figure (4) below illustrates formula (2) for for real , and figures (5) and (6) below illustrate the real and imaginary parts of formula (2) for evaluated along the line . In all three plots the orange and green curves represent the evaluation limits and respectively overlaid on the reference function in blue. Figure (4) : Illustration of formula (2) for Figure (5) : Illustration of formula (2) for Figure (6) : Illustration of formula (2) for Figures (7) and (8) below illustrates formulas (3) and (4) which are the Mellin and Laplace transforms of formula (1) for . Both plots are evaluated at and the orange and green curves represent the evaluation limits and respectively. Figure (7) : Illustration of formula (3) which is the Mellin transform of formula (1) for Figure (8) : Illustration of formula (4) which is the Laplace transform of formula (1) for In response to the answer posted below by reuns below, actually I derived formula (1) from the relationship using the nested Fourier series representation of . This leads to which is valid for . Dividing both sides by leads to which is also valid for and identical to formula (1) above. The restriction is because the nested Fourier series representation of only converges at when . For more information on derivation of formulas via Mellin convolutions using the nested Fourier series representation of , see the answer I posted to my own question at https://math.stackexchange.com/q/2380164 . The analytic continuation leads to formula (2) above for .  I don't believe the derivation in the answer posted by reuns is equivalent to my derivation as I don't believe an expression in terms of is equivalent to an expression in terms of .","\frac{1}{\Gamma(s)} k f \quad\frac{1}{\Gamma(s)}=\underset{N,f\to\infty\land M(N)=0}{\text{lim}}\left(e^2\,2^{-s}\sum\limits_{n=1}^N\mu(n)\,n^{s-1}\sum\limits_{k=1}^{f\,n}\frac{(n+i \pi k)^s+(n-i \pi k)^s}{\left(n^2+\pi^2 k^2\right)^s}\right),\quad\Re(s)>0 \quad\frac{1}{\Gamma[s]}=\underset{N\to\infty\land M(N)=0}{\text{lim}}\left(e^2\,(2 \pi)^{-s}\sum\limits_{n=1}^N\mu(n)\,n^{s-1}\left(i^s \zeta\left(s,1+\frac{i n}{\pi}\right)+(-i)^s \zeta\left(s,1-\frac{i n}{\pi}\right)\right)\right),\quad s\in\mathbb{C} M(N)=0 M(N)=\sum_{n\le N}\mu(n) M(N)=0 N \frac{1}{\Gamma(s)} \frac{1}{\Gamma(s)} \Re(s)>0 s\in\mathbb{C} \frac{1}{\Gamma(s)} \zeta(s,nr/2\pi) \left(i^s \zeta\left(s,1+\frac{i n}{\pi}\right)+(-i)^s \zeta\left(s,1-\frac{i n}{\pi}\right)\right) \frac{1}{\Gamma(s)} \Gamma(z) \frac{1}{\Gamma(s)} \frac{1}{\Gamma(s)} n\le 1 \quad\mathcal{M}_s\left[\frac{1}{\Gamma (s)}\right](z)=\int\limits_0^\infty \frac{1}{\Gamma(s)} s^{z-1}\,ds=\\ \qquad\underset{N,f\to\infty\land M(N)=0}{\text{lim}}\left(e^2\,\Gamma(z)\sum\limits_{n=1}^N\frac{\mu(n)}{n}\sum\limits_{k=1}^{f n}\left(\left(\log(2)-\log\left(\frac{n}{n+i \pi k}\right)\right)^{-z}+\left(\log(2)-\log\left(\frac{n}{n-i \pi k}\right)\right)^{-z}\right)\right) \quad\mathcal{L}_s\left[\frac{1}{\Gamma (s)}\right](z)=\int\limits_0^\infty \frac{1}{\Gamma(s)} e^{-z s}\,ds=\\ \qquad\underset{N,f\to\infty\land M(N)=0}{\text{lim}}\left(e^2\sum\limits_{n=1}^N\frac{\mu(n)}{n}\sum\limits_{k=1}^{f n} \left(\frac{1}{z+\log\left(2-\frac{2 i \pi k}{n}\right)}+\frac{1}{z+\log\left(2+\frac{2 i \pi k}{n}\right)}\right)\right) \frac{1}{\Gamma(s)} \frac{1}{\Gamma(s)} \frac{1}{\Gamma(s)} s \frac{1}{\Gamma(s)} s=1+i t f=4 N=39 N=101 \frac{1}{\Gamma(s)} \Re\left(\frac{1}{\Gamma(1+i t)}\right) \Im\left(\frac{1}{\Gamma(1+i t)}\right) \frac{1}{\Gamma(s)} s \frac{1}{\Gamma(s)} s=i t N=39 N=101 \frac{1}{\Gamma(s)} \Re\left(\frac{1}{\Gamma(i t)}\right) \Im\left(\frac{1}{\Gamma(i t)}\right) \frac{1}{\Gamma(s)} f=4 N=39 N=101 \frac{1}{\Gamma(s)} \frac{1}{\Gamma(s)} y^{-s}=e^2\int\limits_1^\infty x^{-3}\,\delta(\log(x)-1)\frac{\left(\frac{y}{\log(x)}\right)^{-s}}{\log(x)}\,dx\tag{5} \delta(x-1) y^{-s}=\underset{N,f\to\infty\land M(N)=0}{\text{lim}}\left(e^2\,2^{-s}\,y^{-s}\,\Gamma(s)\sum\limits_{n=1}^N\mu(n)\,n^{s-1}\sum\limits_{k=1}^{f\,n}\frac{(n+i \pi k)^s+(n-i \pi k)^s}{\left(\pi^2 k^2+n^2\right)^s}\right)\tag{6} \Re(s)>0 y^{-s}\,\Gamma(s) \frac{1}{\Gamma(s)}=\underset{N,f\to\infty\land M(N)=0}{\text{lim}}\left(e^2\,2^{-s}\sum\limits_{n=1}^N\mu(n)\,n^{s-1}\sum\limits_{k=1}^{f\,n}\frac{(n+i \pi k)^s+(n-i \pi k)^s}{\left(\pi^2 k^2+n^2\right)^s}\right)\tag{7} \Re(s)>0 M(N)=0 \delta(x-1) x=0 M(N)=0 \delta(x-1) \sum _{k=1}^{\infty } \frac{(n+i \pi  k)^s+(n-i \pi  k)^s}{\left(\pi^2 k^2+n^2\right)^s}=\pi^{-s} \left(i^s \zeta\left(s,1+\frac{i n}{\pi}\right)+(-i)^s \zeta\left(s,1-\frac{i n}{\pi}\right)\right)\tag{8} \frac{1}{\Gamma(s)} \zeta(s,nr/2\pi) \left(i^s \zeta\left(s,1+\frac{i n}{\pi}\right)+(-i)^s \zeta\left(s,1-\frac{i n}{\pi}\right)\right)","['complex-analysis', 'number-theory', 'laplace-transform', 'gamma-function', 'mellin-transform']"
39,Are branch cuts always ‘cancellable’?,Are branch cuts always ‘cancellable’?,,"Many functions can be analytically continued to $\mathbb C$ except the branch cut. However, it appears to me that for every function $f(z)$ that has a branch cut, there always exists a non-constant meromorphic/entire function $g(z)$ such that $f(g(z))$ can be analytically continued to the whole $\mathbb C$. For example, $$\sqrt {x^2}=x$$ $$\ln e^x=x$$ $$\arccos\cos x =x$$ $$\ln\ln e^{e^x}=x$$ $$\operatorname{W}(xe^x)=x$$ More complicated examples: $$f(x)=\sqrt{(x+1)(x+3)}=\sqrt{(x+2)^2-1}$$ $$g(x)=-2+\cosh x$$ $$f(x)=x^\alpha\qquad{\alpha\in\mathbb C}$$ $$g(x)=e^x$$ Is this true?","Many functions can be analytically continued to $\mathbb C$ except the branch cut. However, it appears to me that for every function $f(z)$ that has a branch cut, there always exists a non-constant meromorphic/entire function $g(z)$ such that $f(g(z))$ can be analytically continued to the whole $\mathbb C$. For example, $$\sqrt {x^2}=x$$ $$\ln e^x=x$$ $$\arccos\cos x =x$$ $$\ln\ln e^{e^x}=x$$ $$\operatorname{W}(xe^x)=x$$ More complicated examples: $$f(x)=\sqrt{(x+1)(x+3)}=\sqrt{(x+2)^2-1}$$ $$g(x)=-2+\cosh x$$ $$f(x)=x^\alpha\qquad{\alpha\in\mathbb C}$$ $$g(x)=e^x$$ Is this true?",,['complex-analysis']
40,Reference for Hyperbolic Geometry related with Complex Analysis?,Reference for Hyperbolic Geometry related with Complex Analysis?,,"I'm a first year Graduate student.In my first semester complex analysis course there was a topic hyperbolic geometry but due to time limit unfortunately this topic was not touched during the course.Now i am trying to read this topic on my own from the point of view of complex analysis.I tried to find out some books but could not find much,the only book which i found was Gamelin's complex analysis.This book also does not contain,it discuss some hyperbolic geometry on disc only. What are the interesting result in complex analysis related with hyperbolic geometry? Are there any good notes/book which contains the same material? Thank you","I'm a first year Graduate student.In my first semester complex analysis course there was a topic hyperbolic geometry but due to time limit unfortunately this topic was not touched during the course.Now i am trying to read this topic on my own from the point of view of complex analysis.I tried to find out some books but could not find much,the only book which i found was Gamelin's complex analysis.This book also does not contain,it discuss some hyperbolic geometry on disc only. What are the interesting result in complex analysis related with hyperbolic geometry? Are there any good notes/book which contains the same material? Thank you",,"['complex-analysis', 'reference-request', 'book-recommendation', 'hyperbolic-geometry', 'mobius-transformation']"
41,How to show that the Weierstrass function is meromorphic in whole of $\mathbb{C}$,How to show that the Weierstrass function is meromorphic in whole of,\mathbb{C},"The Weierstrass function is defined as: $$\wp(z) = \frac{1}{z^2}+ {\sum\limits_{\omega\in\Gamma'}} \left (\frac{1}{(z-\omega)^2}-\frac{1}{\omega^2}\right).$$ Fix $r>0$. For $\left|z\right| \leq r,$ we split the sum in two parts: \begin{align} \wp(z) = \frac{1}{z^2}+{\sum_{\substack{\omega\in\Gamma', \\ \left|\omega \right|\leq 2r}}} \left (\frac{1}{(z-\omega)^2}-\frac{1}{\omega^2}\right) + {\sum_{\substack{\omega\in\Gamma', \\ \left|\omega \right|>2r}}} \left (\frac{1}{(z-\omega)^2}-\frac{1}{\omega^2}\right). \end{align} Clearly $z=0$ is a pole of $\wp$. Also, the first part of the summation meromorphic function. Now we will show that the second part of the summation is bounded above and therefore cannot have any pole. Note that $$ \frac{1}{(z-\omega)^2}-\frac{1}{\omega^2} = \frac{z(2\omega - z)}{\omega^2 \left(z-w\right)^2}. $$ Assuming $\left|\omega\right| > 2\left|z\right|$, $$\left| 2\omega - z \right| \leq \left| 2\omega \right| + \left| z \right| < \frac{5}{2}\left| \omega \right|,$$ $$ \left| z-\omega \right| \geq \left| \omega \right| - \left| z \right| > \frac{\left| \omega \right|}{2}.$$ This implies $$ \left| \dfrac{1}{(z-\omega)^2}-\dfrac{1}{\omega^2} \right| \leq \dfrac{\left| z \right| \times \dfrac{5}{2} \left| \omega \right|}{\left| \omega \right|^2 \times \dfrac{\left| \omega \right|^2}{4}} = \dfrac{10\left|z\right|}{\left|\omega\right|^3} \leq \dfrac{10r}{\left|\omega\right|^3}$$ We know that $\sum \frac{1}{\omega^3} < \infty$. Hence $\wp$ is a meromorphic function in $|z|\leq r$. Now how to show that $\wp$ is meromorphic in $\mathbb{C}$? I have seen some proofs and I am not able to understand how do they proceed after this step.","The Weierstrass function is defined as: $$\wp(z) = \frac{1}{z^2}+ {\sum\limits_{\omega\in\Gamma'}} \left (\frac{1}{(z-\omega)^2}-\frac{1}{\omega^2}\right).$$ Fix $r>0$. For $\left|z\right| \leq r,$ we split the sum in two parts: \begin{align} \wp(z) = \frac{1}{z^2}+{\sum_{\substack{\omega\in\Gamma', \\ \left|\omega \right|\leq 2r}}} \left (\frac{1}{(z-\omega)^2}-\frac{1}{\omega^2}\right) + {\sum_{\substack{\omega\in\Gamma', \\ \left|\omega \right|>2r}}} \left (\frac{1}{(z-\omega)^2}-\frac{1}{\omega^2}\right). \end{align} Clearly $z=0$ is a pole of $\wp$. Also, the first part of the summation meromorphic function. Now we will show that the second part of the summation is bounded above and therefore cannot have any pole. Note that $$ \frac{1}{(z-\omega)^2}-\frac{1}{\omega^2} = \frac{z(2\omega - z)}{\omega^2 \left(z-w\right)^2}. $$ Assuming $\left|\omega\right| > 2\left|z\right|$, $$\left| 2\omega - z \right| \leq \left| 2\omega \right| + \left| z \right| < \frac{5}{2}\left| \omega \right|,$$ $$ \left| z-\omega \right| \geq \left| \omega \right| - \left| z \right| > \frac{\left| \omega \right|}{2}.$$ This implies $$ \left| \dfrac{1}{(z-\omega)^2}-\dfrac{1}{\omega^2} \right| \leq \dfrac{\left| z \right| \times \dfrac{5}{2} \left| \omega \right|}{\left| \omega \right|^2 \times \dfrac{\left| \omega \right|^2}{4}} = \dfrac{10\left|z\right|}{\left|\omega\right|^3} \leq \dfrac{10r}{\left|\omega\right|^3}$$ We know that $\sum \frac{1}{\omega^3} < \infty$. Hence $\wp$ is a meromorphic function in $|z|\leq r$. Now how to show that $\wp$ is meromorphic in $\mathbb{C}$? I have seen some proofs and I am not able to understand how do they proceed after this step.",,['complex-analysis']
42,Mittag-Leffler Expansion,Mittag-Leffler Expansion,,"I am attempting to perform what is described in my notes as a ""Mittag-Leffler Expansion"", but first I must prove that this expansion is valid. Given that $$ f(z) = \frac{1}{\sin{z}} - \frac{1}{z}$$ Let $C$ be the positively oriented boundary of the rectangle $-\left(n+\frac{1}{2}\right)\pi \le x \le \left(n+\frac{1}{2}\right)\pi$ , $-n\pi \le y \le n\pi$ , where $z= x  + iy$ .  Show that $|\sin(z)| = \mathcal{O}(e^{|n|\pi})$ on the top and bottom, and so $f(z) = \mathcal{O}\left(\frac{1}{n}\right)$ there.  Show also that $|\sin(z)| = \cosh(y)$ on the sides, use this to bound $| f(z) |$ by a constant there, and so bound $|f(z)|$ by a constant along the entire $C$ . My work: I was able to prove that $|\sin(z)| = \cosh(y)$ on the sides. I got that $|\sin(z)| = \sinh(y)$ on the top and bottom by the following $$\lvert \sin(x+iy)\rvert = \lvert \frac{e^{-i(x+iy)}-e^{i(x+iy)}}{2} \rvert \le \frac{e^{|y|}-e^{-|y|}}{2} = \sinh(y) = \sinh(|n|\pi)$$ I am unsure if this is correct, as I am unsure what the script $\mathcal O$ means.  I also do not see how I am supposed to ""bound $| f(z) |$ by a constant"", as $\cosh(y)$ is not bounded as $y$ approaches $\infty$ .","I am attempting to perform what is described in my notes as a ""Mittag-Leffler Expansion"", but first I must prove that this expansion is valid. Given that Let be the positively oriented boundary of the rectangle , , where .  Show that on the top and bottom, and so there.  Show also that on the sides, use this to bound by a constant there, and so bound by a constant along the entire . My work: I was able to prove that on the sides. I got that on the top and bottom by the following I am unsure if this is correct, as I am unsure what the script means.  I also do not see how I am supposed to ""bound by a constant"", as is not bounded as approaches .", f(z) = \frac{1}{\sin{z}} - \frac{1}{z} C -\left(n+\frac{1}{2}\right)\pi \le x \le \left(n+\frac{1}{2}\right)\pi -n\pi \le y \le n\pi z= x  + iy |\sin(z)| = \mathcal{O}(e^{|n|\pi}) f(z) = \mathcal{O}\left(\frac{1}{n}\right) |\sin(z)| = \cosh(y) | f(z) | |f(z)| C |\sin(z)| = \cosh(y) |\sin(z)| = \sinh(y) \lvert \sin(x+iy)\rvert = \lvert \frac{e^{-i(x+iy)}-e^{i(x+iy)}}{2} \rvert \le \frac{e^{|y|}-e^{-|y|}}{2} = \sinh(y) = \sinh(|n|\pi) \mathcal O | f(z) | \cosh(y) y \infty,['complex-analysis']
43,Evaluating definite integrals,Evaluating definite integrals,,"This question came up when I was reading through this question . Are there definite integrals which cannot be computed using any real analysis techniques but are amenable using only complex analysis techniques? If not, is there any reason to believe that if a definite integral can be evaluated using a complex analysis technique, then there must exist a way to compute the same definite integral using only real analysis techniques? EDIT Started a bounty for this question.","This question came up when I was reading through this question . Are there definite integrals which cannot be computed using any real analysis techniques but are amenable using only complex analysis techniques? If not, is there any reason to believe that if a definite integral can be evaluated using a complex analysis technique, then there must exist a way to compute the same definite integral using only real analysis techniques? EDIT Started a bounty for this question.",,"['complex-analysis', 'definite-integrals']"
44,Are there some Python implementations of the Schwarz Christoffel mapping?,Are there some Python implementations of the Schwarz Christoffel mapping?,,"I am interested in Python implementations of the Schwarz Christoffel mapping for simply and doubly connected polygonal domains. Well, I know that by now multiply connected domains can be dealt with, with the works of several authors (Tom DeLillo, Alan Elcrat, John Pfaltzgraff, and Darren Crowdy, to name a few). But to start with, I would like to know if someone has implemented some numerical algorithms in Python, similar (to some extent) to say the Schwarz-Christoffel Matlab toolbox, developed I think originally by Driscoll and Trefethen. For instance, one big issue is to find the preimages of the polygonal vertices, the so-called parameters of the mapping. This is non-trivial. There are also various numerical issues with branch cuts. These are some numerical issues that one has to deal with. Of course, the Matlab toolbox solves all these issues, but since I only have access now to Python, I would like to know if these have been implemented in Python. If not, I may be interested in trying to implement them. Edit: I was able to find some Python codes, to accomplish part of what I wanted, namely the programs linked to from http://planetmath.org/SchwarzChristoffelTransformationCircularVersion , in section 0.3. They seem to implement the mapping itself, when the ""preimages"" are also input. But they do not solve the issue of finding the actual preimages, that would produce a desired polygonal region. So this is good, but the tough part remains to be programmed, it seems. Update: I have now implemented the Schwarz-Christoffel forward mapping in Python. However, while finding the preimages, some preimages tend to cluster really close together, so much that they are practically indistinguishable, numerically speaking. Anyone knows how to solve this clustering issue? I have only read so far Trefethen's original algorithm. By reading a little bit one of Driscoll's articles on the Matlab SC-toolbox, it seems that the algorithm using cross-ratios and Delaunay triangulations handles well the crowding problem. The only problem is, it would take me some time to implement it. Using the more basic algorithm by Trefethen, and using the usual Gauss quadrature (rather than the more suitable Gauss-Jacobi one) but with smaller and smaller intervals as one approaches the preimages, I was only able to get conformal maps into rectangles of aspect ratios less than 5 approximately (at least this is the case with the current version of my Python code).","I am interested in Python implementations of the Schwarz Christoffel mapping for simply and doubly connected polygonal domains. Well, I know that by now multiply connected domains can be dealt with, with the works of several authors (Tom DeLillo, Alan Elcrat, John Pfaltzgraff, and Darren Crowdy, to name a few). But to start with, I would like to know if someone has implemented some numerical algorithms in Python, similar (to some extent) to say the Schwarz-Christoffel Matlab toolbox, developed I think originally by Driscoll and Trefethen. For instance, one big issue is to find the preimages of the polygonal vertices, the so-called parameters of the mapping. This is non-trivial. There are also various numerical issues with branch cuts. These are some numerical issues that one has to deal with. Of course, the Matlab toolbox solves all these issues, but since I only have access now to Python, I would like to know if these have been implemented in Python. If not, I may be interested in trying to implement them. Edit: I was able to find some Python codes, to accomplish part of what I wanted, namely the programs linked to from http://planetmath.org/SchwarzChristoffelTransformationCircularVersion , in section 0.3. They seem to implement the mapping itself, when the ""preimages"" are also input. But they do not solve the issue of finding the actual preimages, that would produce a desired polygonal region. So this is good, but the tough part remains to be programmed, it seems. Update: I have now implemented the Schwarz-Christoffel forward mapping in Python. However, while finding the preimages, some preimages tend to cluster really close together, so much that they are practically indistinguishable, numerically speaking. Anyone knows how to solve this clustering issue? I have only read so far Trefethen's original algorithm. By reading a little bit one of Driscoll's articles on the Matlab SC-toolbox, it seems that the algorithm using cross-ratios and Delaunay triangulations handles well the crowding problem. The only problem is, it would take me some time to implement it. Using the more basic algorithm by Trefethen, and using the usual Gauss quadrature (rather than the more suitable Gauss-Jacobi one) but with smaller and smaller intervals as one approaches the preimages, I was only able to get conformal maps into rectangles of aspect ratios less than 5 approximately (at least this is the case with the current version of my Python code).",,"['complex-analysis', 'numerical-methods', 'conformal-geometry']"
45,What is my method of rendering the Mandelbrot set called?,What is my method of rendering the Mandelbrot set called?,,"Recently I've dived into studying complex analysis on my own and it led me to find an interesting way of displaying the Mandelbrot set: Consider $$f(f(f(f(...x))))$$ Then,  $$ \frac{d}{dx} (f(f(f(f(...x))))) = f'(f(f(f(...x))))\cdot f'(f(f(...x)))...$$ Assuming that this function converges for $x$,  $$\frac{d}{dx}(f(f(...x))) =  \prod_{p=1}^{\infty} f'(f^{p}(x))$$ Since all non-edge-points in a Julia set always diverge or converge to the same cycle, this gave me an idea. Even though this product is always equal to $0$ or $\infty$, I wondered if it was possible to still compare the derivatives by how quickly they converge to $0$. I thought about the best way to do this for a while and came up with the following expression to color the points in the Mandelbrot set: $$ 1-4\sqrt[p]{ \prod_{i=1}^{p} d(v_{i}) }$$ where $p$ is the length of the cycle the point converges to, $v_i$ is the $i$th point of the cycle, and $d(x)$ is a function which returns the squared distance of $x$ from the origin. The radical normalizes the value of the product based on the length of the cycle, in effect determining the ""average"" value of $f'(f^p(x))$. Finding the cycle and its values can be done efficiently using pre-existing algorithms. When I made a program to implement this algorithm, I got the following really cool image: If this has been done before, what is this method of displaying the set called?","Recently I've dived into studying complex analysis on my own and it led me to find an interesting way of displaying the Mandelbrot set: Consider $$f(f(f(f(...x))))$$ Then,  $$ \frac{d}{dx} (f(f(f(f(...x))))) = f'(f(f(f(...x))))\cdot f'(f(f(...x)))...$$ Assuming that this function converges for $x$,  $$\frac{d}{dx}(f(f(...x))) =  \prod_{p=1}^{\infty} f'(f^{p}(x))$$ Since all non-edge-points in a Julia set always diverge or converge to the same cycle, this gave me an idea. Even though this product is always equal to $0$ or $\infty$, I wondered if it was possible to still compare the derivatives by how quickly they converge to $0$. I thought about the best way to do this for a while and came up with the following expression to color the points in the Mandelbrot set: $$ 1-4\sqrt[p]{ \prod_{i=1}^{p} d(v_{i}) }$$ where $p$ is the length of the cycle the point converges to, $v_i$ is the $i$th point of the cycle, and $d(x)$ is a function which returns the squared distance of $x$ from the origin. The radical normalizes the value of the product based on the length of the cycle, in effect determining the ""average"" value of $f'(f^p(x))$. Finding the cycle and its values can be done efficiently using pre-existing algorithms. When I made a program to implement this algorithm, I got the following really cool image: If this has been done before, what is this method of displaying the set called?",,"['complex-analysis', 'fractals']"
46,Uniform limit of one-to-one analytic functions is either constant or one-to-one,Uniform limit of one-to-one analytic functions is either constant or one-to-one,,"Let $U$ be a complex domain, and $(f_n)_{n\in \mathbb{N}}$ be a sequence on one-to-one analytic functions defined on $U$ . Suppose that $f_n$ converges to $f$ uniformly on every compact subset of $U$ . Prove that $f$ is either constant or one-to-one on $U$ . Here's my proof. I would appreciate if you guys point out any possible mistakes, or maybe give a different proof that uses different techniques. We may assume that the zeros of $f$ have no accumulation point in $U$ , otherwise, $f$ is identically zero. Now we proceed by contradiction; suppose, WLOG, that $f$ has two zeros in $U$ ; say $f(a)=f(b)=0$ . There exists $\delta >0$ such that $a$ and $b$ are the only zeroes of $f$ in $B(a,\delta)$ and $B(b,\delta)$ , respectively. Moreover, the Maximum Modulus Principle implies that $|f(z)|>0$ on the boundaries of the two balls above; say $$|f(z)|>m>0\, \text{ for all }\, z\in \partial B(a,\delta)\cup \partial B(b,\delta).$$ Since $f_n \rightarrow f$ uniformly on compact sets, then for $n$ large enough, we have $$|f_n (z)-f(z)|<m<|f(z)| \, \text { for all } \,z\in \partial B(a,\delta)\cup \partial B(b,\delta).$$ By Rouche's Theorem, $f_n$ and $f$ have the same number of zeroes in $B(a,\delta)$ and $B(b,\delta)$ , namely one in each ball (not counting multiplicities). But this contradicts the assumption that $f_n$ is one-to-one.","Let be a complex domain, and be a sequence on one-to-one analytic functions defined on . Suppose that converges to uniformly on every compact subset of . Prove that is either constant or one-to-one on . Here's my proof. I would appreciate if you guys point out any possible mistakes, or maybe give a different proof that uses different techniques. We may assume that the zeros of have no accumulation point in , otherwise, is identically zero. Now we proceed by contradiction; suppose, WLOG, that has two zeros in ; say . There exists such that and are the only zeroes of in and , respectively. Moreover, the Maximum Modulus Principle implies that on the boundaries of the two balls above; say Since uniformly on compact sets, then for large enough, we have By Rouche's Theorem, and have the same number of zeroes in and , namely one in each ball (not counting multiplicities). But this contradicts the assumption that is one-to-one.","U (f_n)_{n\in \mathbb{N}} U f_n f U f U f U f f U f(a)=f(b)=0 \delta >0 a b f B(a,\delta) B(b,\delta) |f(z)|>0 |f(z)|>m>0\, \text{ for all }\, z\in \partial B(a,\delta)\cup \partial B(b,\delta). f_n \rightarrow f n |f_n (z)-f(z)|<m<|f(z)| \, \text { for all } \,z\in \partial B(a,\delta)\cup \partial B(b,\delta). f_n f B(a,\delta) B(b,\delta) f_n","['complex-analysis', 'proof-verification']"
47,A map from zeros of $\zeta(s)$ to zeros of $C(s)?$,A map from zeros of  to zeros of,\zeta(s) C(s)?,"Let $P(s),C(s),\zeta(s)$ be the prime zeta function, the analogous composite zeta function, and the classical zeta function. I do not know whether it is known that there are infinitely many zeros of C, (or P). As an exercise I tried to show this. My question is whether A-C below is a plausible argument (in the sense that it could be made into a proof) for the proposition: Proposition: There are infinitely many zeros of $C(s)$ (and $ P(s)$). A. Loosely speaking, a zero of an analytic function $f$ is a continuous function of $f.$ Since $\zeta,P,C$ are analytic, it seems that Paul Rosenbloom's 1969 paper in J. of Approximation Thy, ""Perturbation of the Zeros of Analytic Functions I"", applies, in particular Hurwitz's thm. If $f_n\to f$ the limit pts. of the zeros of $f_n$ are the zeros of f. More to the point, he appears to show that (with conditions) if f is close to g, near any point where g is close to 0 there is a point where g is exactly zero. B. We can map every nontrivial zero of $\zeta(s)$ to a zero or set of zeros of $C(s)$ and conversely. Starting at a nontrivial zero of $\zeta(s)$ we subtract (if necessary, small multiples of) terms of the analytic continuation of $P(s)$ from $\zeta(s),$ which shifts the zero of $\zeta$ stepwise towards that of $C(s).$ In a small finite number of steps the zero of the perturbed function approximates that of $C(s).$ C. Two zeros may map to the same zero of C (or P) but this is not I think an issue. If we can show that for given zero $z$ of $\zeta$ and the (finite) set of zeros $s$ to which $z$ can be perturbed, the largest distance $|z-s_o|$ is finite, then we only have to choose the next $z$--as $\Im(z)$ increases--sufficiently large that none of the zeros to which it perturbs could possibly be $s_o.$ Continuing in this way we get an infinite sequence of zeros of C. One possible problem is that one may be drawing a zero of $P$ (or $C$) towards a point which is a zero of $\zeta,$ hence a badly-behaved point of (the analytic continuation of) P. For example, it seems hard to find a path from $C(0.38+12.4~i)= 0$ to $\zeta(.5+14.1~i)=0,$ but by adding a convenient number $z$ we can get a path $C\to C+z \to \zeta+z \to \zeta$ which avoids discontinuities. Since  we have a path from $C(.32+15.4~i)=0$ to $\zeta(0.5+14.1~i)=0$ the mapping is evidently many-to-one, but still would show infinitely many zeros of P and C if the idea is otherwise sound. Example. There is an analytic continuation of $\zeta$ for $\sigma \leq 1.$ There is an analytic continuation of $P$ valid for $ 0 < \sigma. $ The continuation for $P$ obtained by Mobius inversion is $$P(s)= \sum_{k=1}^\infty  \frac{\mu(k)}{k}\log \zeta(k s).$$ Let $P_N(s) = \sum_{k=1}^N \frac{\mu(k)}{k}\log \zeta (k s).$ $\zeta(1/2+56.446... i) = 0.$ $\zeta(.5+55.8~ i) - P_1(.5+55.8 ~i)\approx 0. $ $\zeta(.65+55.8 ~i)-P_2(.65+55.8 ~i)\approx 0.$ ... $\zeta(.64475 +55.8908~ i) -  P_{20}(.64475+55.8908~ i) \approx C(.64475+55.8908 ~i)\approx 0.$ Detail of the above calculation with different zero: perturbing the first nontrivial zero of $\zeta$ to a zero of $\zeta(s)-(\mu(1)/1)\log \zeta(1(s))$ in increments of 1/20. For the table the perturbation begins at the first nontrivial zero of $\zeta\approx (0.5+ 14.13~i).$ $\zeta(a+b~ i)- (m/20)(\mu(1)/1)\log \zeta(1(a+b~i))\approx 0$ $$\begin{array}{c | c | c | }  m & a & b   \\ \hline 0 & 0.5 & 14.13   \\ \hline 1 & 0.42 &14.3  \\ \hline    2 & 0.36& 14.4\\ \hline 3 & 0.33& 14.5 \\ \hline 4 & 0.32 & 14.6   \\ \hline     ... & ... & ... \\  \hline 17& 0.24 & 15.4 \\   \hline 18 & 0.22 & 15.5\\    \hline 19 & 0.22 & 15.58\\ \hline 20 & 0.22 & 15.6 \\ \hline  \end{array}$$ The analogous problem for $P(s)$ is intended as part of the question.","Let $P(s),C(s),\zeta(s)$ be the prime zeta function, the analogous composite zeta function, and the classical zeta function. I do not know whether it is known that there are infinitely many zeros of C, (or P). As an exercise I tried to show this. My question is whether A-C below is a plausible argument (in the sense that it could be made into a proof) for the proposition: Proposition: There are infinitely many zeros of $C(s)$ (and $ P(s)$). A. Loosely speaking, a zero of an analytic function $f$ is a continuous function of $f.$ Since $\zeta,P,C$ are analytic, it seems that Paul Rosenbloom's 1969 paper in J. of Approximation Thy, ""Perturbation of the Zeros of Analytic Functions I"", applies, in particular Hurwitz's thm. If $f_n\to f$ the limit pts. of the zeros of $f_n$ are the zeros of f. More to the point, he appears to show that (with conditions) if f is close to g, near any point where g is close to 0 there is a point where g is exactly zero. B. We can map every nontrivial zero of $\zeta(s)$ to a zero or set of zeros of $C(s)$ and conversely. Starting at a nontrivial zero of $\zeta(s)$ we subtract (if necessary, small multiples of) terms of the analytic continuation of $P(s)$ from $\zeta(s),$ which shifts the zero of $\zeta$ stepwise towards that of $C(s).$ In a small finite number of steps the zero of the perturbed function approximates that of $C(s).$ C. Two zeros may map to the same zero of C (or P) but this is not I think an issue. If we can show that for given zero $z$ of $\zeta$ and the (finite) set of zeros $s$ to which $z$ can be perturbed, the largest distance $|z-s_o|$ is finite, then we only have to choose the next $z$--as $\Im(z)$ increases--sufficiently large that none of the zeros to which it perturbs could possibly be $s_o.$ Continuing in this way we get an infinite sequence of zeros of C. One possible problem is that one may be drawing a zero of $P$ (or $C$) towards a point which is a zero of $\zeta,$ hence a badly-behaved point of (the analytic continuation of) P. For example, it seems hard to find a path from $C(0.38+12.4~i)= 0$ to $\zeta(.5+14.1~i)=0,$ but by adding a convenient number $z$ we can get a path $C\to C+z \to \zeta+z \to \zeta$ which avoids discontinuities. Since  we have a path from $C(.32+15.4~i)=0$ to $\zeta(0.5+14.1~i)=0$ the mapping is evidently many-to-one, but still would show infinitely many zeros of P and C if the idea is otherwise sound. Example. There is an analytic continuation of $\zeta$ for $\sigma \leq 1.$ There is an analytic continuation of $P$ valid for $ 0 < \sigma. $ The continuation for $P$ obtained by Mobius inversion is $$P(s)= \sum_{k=1}^\infty  \frac{\mu(k)}{k}\log \zeta(k s).$$ Let $P_N(s) = \sum_{k=1}^N \frac{\mu(k)}{k}\log \zeta (k s).$ $\zeta(1/2+56.446... i) = 0.$ $\zeta(.5+55.8~ i) - P_1(.5+55.8 ~i)\approx 0. $ $\zeta(.65+55.8 ~i)-P_2(.65+55.8 ~i)\approx 0.$ ... $\zeta(.64475 +55.8908~ i) -  P_{20}(.64475+55.8908~ i) \approx C(.64475+55.8908 ~i)\approx 0.$ Detail of the above calculation with different zero: perturbing the first nontrivial zero of $\zeta$ to a zero of $\zeta(s)-(\mu(1)/1)\log \zeta(1(s))$ in increments of 1/20. For the table the perturbation begins at the first nontrivial zero of $\zeta\approx (0.5+ 14.13~i).$ $\zeta(a+b~ i)- (m/20)(\mu(1)/1)\log \zeta(1(a+b~i))\approx 0$ $$\begin{array}{c | c | c | }  m & a & b   \\ \hline 0 & 0.5 & 14.13   \\ \hline 1 & 0.42 &14.3  \\ \hline    2 & 0.36& 14.4\\ \hline 3 & 0.33& 14.5 \\ \hline 4 & 0.32 & 14.6   \\ \hline     ... & ... & ... \\  \hline 17& 0.24 & 15.4 \\   \hline 18 & 0.22 & 15.5\\    \hline 19 & 0.22 & 15.58\\ \hline 20 & 0.22 & 15.6 \\ \hline  \end{array}$$ The analogous problem for $P(s)$ is intended as part of the question.",,"['complex-analysis', 'analytic-number-theory', 'zeta-functions']"
48,"Contour Integral solution to differential equations, Euler transformation?","Contour Integral solution to differential equations, Euler transformation?",,"The book Spain, B. and Smith, M. G., Functions of Mathematical Physics , Volume 81, Volume 8 of New university mathematics series , Van Nostrand Reinhold Company, 1970,  introduces the contour integral method of solving ODEs. The baseic idea is: given an ODE $\sum_0^m a_r(t) \frac {d^rf}{dt^r} = 0$ , a solution may be sought after in the form: $$ \int_c{k(z,t)v(t)dt} $$ The ingenuity is really in the choice of $k$ and a contour can then be chosen to make sure 'boundary terms' vanish. The apeal is of course for example in finding integral representations of special functions. The Laplace method is when choice of $k$ is of the form $k = e^{t*h(z)}$ (in general) and we frequently end up with a Mellin inversion formula or similar looking solution after finding the solution to a much simpler problem satisfied by $v(t)$ . For example the integral representation of Airy functions. Another popular choice of $k$ is of the form $ k = (t - z)^\mu $ which is called the euler transformation. where $\mu$ is a parameter to be determined. For example, the integral representation of Gauss's hypergeometric function looks like this. For example, the nth order ODE with linear polynomial coefficients: $$ \sum_{k=0}^n {(a_k + b_kt)\frac {d^kw}{dt^k}} = 0; $$ We can try a Laplace-type choice for $k$ say $k = e^{zt}$ which upon substitution yields two polynomials in z: $F(z) = \sum_{k=0}^n{a_kz^k}$ and $G(z) = \sum_{k=0}^n{b_kz^k}$ and the ODE is tranformed to: $$ \int_c {e^{zt} v(z)(F(z)+tG(z))}dz = 0 $$ Upon integration by parts (i'll ommit the z's, primes denote differntiation wrt z): $$\int_c{e^{zt}(Fv - (Gv)^\prime)dz} + [GSe^{zt}]_{C} = 0$$ We then choose a contour that ensures the last term in the LHS above vanishes and solve the 1st order system in the parentheses in $z$ intead to find $v$ as: $$v(z) = \frac {A}{G(z)} exp\{\int^z \frac {F(z)}{G(z)} dz\}$$ for some arbitrary constant. In general tis type of choice of $k$ 'reflects the order' of the problem to be solved. For example above an nth order ODE is solved by finding a 1st order ODE that $v$ satisfies. If the order of the polynomials in $a_k,b_k$ were quadratic we would have a 2nd order ODE that $v$ satisfies thereby not have simplified the problem a great deal. This is usually when the Euler-type choice of k is useful. For example in proving the integral representation of Legendre polynomials. My long preamble is to get others interested in the elegance of this as I see it isn't very popular. My question is: Is there a more modern text that covers this method of solving ODEs? Perhaps the name 'Euler transformation' is not the most popular name for the second choice of $k$ , if so does anyone know another name for the method? Alternatively, does anyone know a different proof of the integral representation of Legrendre polynomials and how to show equivalence of the different types of representations?","The book Spain, B. and Smith, M. G., Functions of Mathematical Physics , Volume 81, Volume 8 of New university mathematics series , Van Nostrand Reinhold Company, 1970,  introduces the contour integral method of solving ODEs. The baseic idea is: given an ODE , a solution may be sought after in the form: The ingenuity is really in the choice of and a contour can then be chosen to make sure 'boundary terms' vanish. The apeal is of course for example in finding integral representations of special functions. The Laplace method is when choice of is of the form (in general) and we frequently end up with a Mellin inversion formula or similar looking solution after finding the solution to a much simpler problem satisfied by . For example the integral representation of Airy functions. Another popular choice of is of the form which is called the euler transformation. where is a parameter to be determined. For example, the integral representation of Gauss's hypergeometric function looks like this. For example, the nth order ODE with linear polynomial coefficients: We can try a Laplace-type choice for say which upon substitution yields two polynomials in z: and and the ODE is tranformed to: Upon integration by parts (i'll ommit the z's, primes denote differntiation wrt z): We then choose a contour that ensures the last term in the LHS above vanishes and solve the 1st order system in the parentheses in intead to find as: for some arbitrary constant. In general tis type of choice of 'reflects the order' of the problem to be solved. For example above an nth order ODE is solved by finding a 1st order ODE that satisfies. If the order of the polynomials in were quadratic we would have a 2nd order ODE that satisfies thereby not have simplified the problem a great deal. This is usually when the Euler-type choice of k is useful. For example in proving the integral representation of Legendre polynomials. My long preamble is to get others interested in the elegance of this as I see it isn't very popular. My question is: Is there a more modern text that covers this method of solving ODEs? Perhaps the name 'Euler transformation' is not the most popular name for the second choice of , if so does anyone know another name for the method? Alternatively, does anyone know a different proof of the integral representation of Legrendre polynomials and how to show equivalence of the different types of representations?","\sum_0^m a_r(t) \frac {d^rf}{dt^r} = 0  \int_c{k(z,t)v(t)dt}  k k k = e^{t*h(z)} v(t) k  k = (t - z)^\mu  \mu  \sum_{k=0}^n {(a_k + b_kt)\frac {d^kw}{dt^k}} = 0;  k k = e^{zt} F(z) = \sum_{k=0}^n{a_kz^k} G(z) = \sum_{k=0}^n{b_kz^k}  \int_c {e^{zt} v(z)(F(z)+tG(z))}dz = 0  \int_c{e^{zt}(Fv - (Gv)^\prime)dz} + [GSe^{zt}]_{C} = 0 z v v(z) = \frac {A}{G(z)} exp\{\int^z \frac {F(z)}{G(z)} dz\} k v a_k,b_k v k","['complex-analysis', 'ordinary-differential-equations', 'reference-request', 'special-functions', 'integral-transforms']"
49,Subadditivity for Analytic Capacity Disjoint Compacts separated by a Line,Subadditivity for Analytic Capacity Disjoint Compacts separated by a Line,,"The following problem is asked in Greene and Krantz, Problem 9, page 382: Suppose that $C_1$ and $C_2$ are disjoint compact sets in $\mathbb{C}$ that can be separated by a line $l$ with $C_1 \cap l = C_2 \cap l = \emptyset$. Show that   $$\gamma(C_1 \cup C_2) \leq \gamma(C_1) + \gamma(C_2).$$ Here, $\gamma(C)$ is the analytic capacity of the compact set $C\subset \mathbb{C}$. All my ideas to solve this problem reach a dead end pretty quickly. Two ideas initially felt right. One was Schwarz reflection. The other was finding an open disk containing $C_1$ but that is disjoint with $C_2$, then use the Cauchy integral formula on this disk to help define a function that is holomorphic on $\mathbb{C} \backslash C_2$ that has norm $\leq$ 1. I don't think either of these ideas are useful.","The following problem is asked in Greene and Krantz, Problem 9, page 382: Suppose that $C_1$ and $C_2$ are disjoint compact sets in $\mathbb{C}$ that can be separated by a line $l$ with $C_1 \cap l = C_2 \cap l = \emptyset$. Show that   $$\gamma(C_1 \cup C_2) \leq \gamma(C_1) + \gamma(C_2).$$ Here, $\gamma(C)$ is the analytic capacity of the compact set $C\subset \mathbb{C}$. All my ideas to solve this problem reach a dead end pretty quickly. Two ideas initially felt right. One was Schwarz reflection. The other was finding an open disk containing $C_1$ but that is disjoint with $C_2$, then use the Cauchy integral formula on this disk to help define a function that is holomorphic on $\mathbb{C} \backslash C_2$ that has norm $\leq$ 1. I don't think either of these ideas are useful.",,['complex-analysis']
50,Weierstrass product expression for Klein's j-invariant,Weierstrass product expression for Klein's j-invariant,,"The first sentence of @ccorn's answer to a previous question of mine was: “Because of the modular symmetries of $j(\tau)$, the zeros of $j(\tau)$ are precisely the $\operatorname{SL}(2,\mathbb{Z})$-transforms of the fundamental-domain zero $\zeta_3=\mathrm{e}^{2\pi\mathrm{i}/3}$.” To what extent can $j(\tau)$ be written as an infinite product over those zeros?, something like this: $$j(\tau)\stackrel{?}{=}u(\tau)\cdot\prod_{A\in\mathbf{SL}(2,\mathbf{Z})}\left[1-\frac{\tau}{A(e^{2\pi i/3})}\right]^{3}$$ And,  if such an expression were valid, what would $u(\tau)$ be?","The first sentence of @ccorn's answer to a previous question of mine was: “Because of the modular symmetries of $j(\tau)$, the zeros of $j(\tau)$ are precisely the $\operatorname{SL}(2,\mathbb{Z})$-transforms of the fundamental-domain zero $\zeta_3=\mathrm{e}^{2\pi\mathrm{i}/3}$.” To what extent can $j(\tau)$ be written as an infinite product over those zeros?, something like this: $$j(\tau)\stackrel{?}{=}u(\tau)\cdot\prod_{A\in\mathbf{SL}(2,\mathbf{Z})}\left[1-\frac{\tau}{A(e^{2\pi i/3})}\right]^{3}$$ And,  if such an expression were valid, what would $u(\tau)$ be?",,"['complex-analysis', 'special-functions', 'analytic-number-theory', 'modular-forms', 'infinite-product']"
51,On the continuation of a polynomial,On the continuation of a polynomial,,"This exrcise is from the first section of Marden: Exercise 12. Let the interior of a piecewise regular curve $C$ contain the origin $\cal O$ and be star-shaped with respect to $\cal O$.  If the complex numbers $a_1, a_2, \ldots, a_m$ are given, then $n$ and $b_{m+1}, b_{m+2}, \ldots, b_n$ may be determined so that all of the zeros of $F(z) = 1 + a_1 z + \cdots + a_m z^m + b_{m+1} z^{m+1} + \cdots + b_n z^n$ lie on $C$. Hint: Choose the zeros $\zeta_j$ of $G(z) = z^n F(1/z)$ so that $z_j = 1/\zeta_j$ are points of $C$ and so that the Newton-Girard formulas $s_k + s_{k-1} a_1 + \cdots + k a_k = 0$ are satisfied by the sums $s_r$ of the $r^{\text{th}}$ powers of the $\zeta_j$. I understand how the requirements of the hint will imply the result, but I do not know how to establish them.  Indeed, if we can satisfy those requirements, then the remaining $b_j$ will be determined by the further Newton-Girard formulas. This is a generalization of the previous problem in the book: Exercise 11: If $F(z) = 1 + a_1 z + b_2 z^2 + \cdots + b_n z^n$, the quantities $n, b_2, \ldots, b_n$ may be so determined so that all the zeros of F lie on the unit circle. The solution to this is simpler: we choose $n > -a_1$ and values $\zeta_1, \ldots, \zeta_n$ on the unit circle with centroid $-a_1/n$.  Viète's formulas tell us that these are the zeros of a polynomial $z^n + a_1 z^{n-1} + b_2 z^{n-2} + \cdots + b_n = z^n F(1/z)$, so that $1/\zeta_j$ are the zeros of $F(z)$ and lie on the unit circle. My problem with Exercise 12 lies in ensuring the Newton-Girard formulas are satisfied; it is not as simple as choosing a centroid, and I can't see a way to do it even for the case when $C$ is the unit circle.  How do I know that a solution exists here?  Can I extend this to the general case, or is a separate argument needed? Also, how do I center formulas? Edit: I forgot to mention that Marden gives some citations of this result: Gavrilov, L., On the continuation of polynomials Gavrilov, L., On the K-extension of polynomials Cebotarev, N. G., Über die Fortsetzbarkeit von Polynomen auf geschlassene Kurven Cebotarev, N. G., On Hurwitz's problem for transcendental functions Unfortunately I read neither Russian nor German and I can't seem to locate the last one, so these aren't of direct help to me.","This exrcise is from the first section of Marden: Exercise 12. Let the interior of a piecewise regular curve $C$ contain the origin $\cal O$ and be star-shaped with respect to $\cal O$.  If the complex numbers $a_1, a_2, \ldots, a_m$ are given, then $n$ and $b_{m+1}, b_{m+2}, \ldots, b_n$ may be determined so that all of the zeros of $F(z) = 1 + a_1 z + \cdots + a_m z^m + b_{m+1} z^{m+1} + \cdots + b_n z^n$ lie on $C$. Hint: Choose the zeros $\zeta_j$ of $G(z) = z^n F(1/z)$ so that $z_j = 1/\zeta_j$ are points of $C$ and so that the Newton-Girard formulas $s_k + s_{k-1} a_1 + \cdots + k a_k = 0$ are satisfied by the sums $s_r$ of the $r^{\text{th}}$ powers of the $\zeta_j$. I understand how the requirements of the hint will imply the result, but I do not know how to establish them.  Indeed, if we can satisfy those requirements, then the remaining $b_j$ will be determined by the further Newton-Girard formulas. This is a generalization of the previous problem in the book: Exercise 11: If $F(z) = 1 + a_1 z + b_2 z^2 + \cdots + b_n z^n$, the quantities $n, b_2, \ldots, b_n$ may be so determined so that all the zeros of F lie on the unit circle. The solution to this is simpler: we choose $n > -a_1$ and values $\zeta_1, \ldots, \zeta_n$ on the unit circle with centroid $-a_1/n$.  Viète's formulas tell us that these are the zeros of a polynomial $z^n + a_1 z^{n-1} + b_2 z^{n-2} + \cdots + b_n = z^n F(1/z)$, so that $1/\zeta_j$ are the zeros of $F(z)$ and lie on the unit circle. My problem with Exercise 12 lies in ensuring the Newton-Girard formulas are satisfied; it is not as simple as choosing a centroid, and I can't see a way to do it even for the case when $C$ is the unit circle.  How do I know that a solution exists here?  Can I extend this to the general case, or is a separate argument needed? Also, how do I center formulas? Edit: I forgot to mention that Marden gives some citations of this result: Gavrilov, L., On the continuation of polynomials Gavrilov, L., On the K-extension of polynomials Cebotarev, N. G., Über die Fortsetzbarkeit von Polynomen auf geschlassene Kurven Cebotarev, N. G., On Hurwitz's problem for transcendental functions Unfortunately I read neither Russian nor German and I can't seem to locate the last one, so these aren't of direct help to me.",,"['complex-analysis', 'polynomials']"
52,How are Zeta function values calculated from within the Critical Strip?,How are Zeta function values calculated from within the Critical Strip?,,"We note that for $Re(s) > 1$ $$ \zeta(s) = \sum_{i=1}^{\infty}\frac{1}{i^s} $$ Furthermore $$\zeta(s) = 2^s \pi^{s-1} \sin \left(\frac{\pi s}{2} \right) \Gamma(1-s) \zeta(1-s)$$ Allows us to define the zeta function for all values where $$Re(s) < 0$$ By using the values where $$Re(s) > 1$$ But how do we define it over $$ 0 \le Re(s) \le 1$$ Which is where most of the ""action"" regarding the function happens anyways...","We note that for $Re(s) > 1$ $$ \zeta(s) = \sum_{i=1}^{\infty}\frac{1}{i^s} $$ Furthermore $$\zeta(s) = 2^s \pi^{s-1} \sin \left(\frac{\pi s}{2} \right) \Gamma(1-s) \zeta(1-s)$$ Allows us to define the zeta function for all values where $$Re(s) < 0$$ By using the values where $$Re(s) > 1$$ But how do we define it over $$ 0 \le Re(s) \le 1$$ Which is where most of the ""action"" regarding the function happens anyways...",,"['complex-analysis', 'number-theory', 'functional-equations', 'riemann-zeta', 'zeta-functions']"
53,Why is there no continuous square root function on $\mathbb{C}$?,Why is there no continuous square root function on ?,\mathbb{C},"I know that what taking square roots for reals, we can choose the standard square root in such a way that the square root function is continuous, with respect to the metric. Why is that not the case over $\mathbb{C}$, with respect the the $\mathbb{R}^2$ metric? I suppose what I'm trying to ask is why is there not continuous function $f$ on $\mathbb{C}$ such that $f(z)^2=z$ for all $z$? This is what I was reading, but didn't get: Suppose there exists some $f$, and restrict attention to $S^1$. Given $t\in[0,2\pi)$, we can write $$ f(\cos t+i\sin t)=\cos(\psi (t))+i\sin(\psi (t)) $$ for unique $\psi(t)\in\{t/2,t/2+\pi\}$. (I don't understand this assertion of why the displayed equality works, and why $\psi$ only takes those two possible values.) If $f$ is continuous, then $\psi:[0,2\pi)\to[0,2\pi)$ is continuous. Then $t\mapsto \psi(t)-t/2$ is continuous, and takes values in $\{0,\pi\}$ and is thus constant. This constant must equal $\psi(0)$, so $\psi(t)=\psi(0)+t/2$. Thus $\lim_{t\to 2\pi}\psi(t)=\psi(0)+\pi$. Then  $$ \lim_{t\to 2\pi} f(\cos t+i\sin t)=-f(1). $$ (How is $-f(1)$ found on the RHS?) Since $f$ is continuous, $f(1)=-f(1)$, impossible since $f(1)\neq 0$. I hope someone can clear up the two problems I have understanding the proof. Thanks.","I know that what taking square roots for reals, we can choose the standard square root in such a way that the square root function is continuous, with respect to the metric. Why is that not the case over $\mathbb{C}$, with respect the the $\mathbb{R}^2$ metric? I suppose what I'm trying to ask is why is there not continuous function $f$ on $\mathbb{C}$ such that $f(z)^2=z$ for all $z$? This is what I was reading, but didn't get: Suppose there exists some $f$, and restrict attention to $S^1$. Given $t\in[0,2\pi)$, we can write $$ f(\cos t+i\sin t)=\cos(\psi (t))+i\sin(\psi (t)) $$ for unique $\psi(t)\in\{t/2,t/2+\pi\}$. (I don't understand this assertion of why the displayed equality works, and why $\psi$ only takes those two possible values.) If $f$ is continuous, then $\psi:[0,2\pi)\to[0,2\pi)$ is continuous. Then $t\mapsto \psi(t)-t/2$ is continuous, and takes values in $\{0,\pi\}$ and is thus constant. This constant must equal $\psi(0)$, so $\psi(t)=\psi(0)+t/2$. Thus $\lim_{t\to 2\pi}\psi(t)=\psi(0)+\pi$. Then  $$ \lim_{t\to 2\pi} f(\cos t+i\sin t)=-f(1). $$ (How is $-f(1)$ found on the RHS?) Since $f$ is continuous, $f(1)=-f(1)$, impossible since $f(1)\neq 0$. I hope someone can clear up the two problems I have understanding the proof. Thanks.",,"['complex-analysis', 'functions']"
54,integrate $\int_0^{2\pi} e^{\cos \theta} \cos( \sin \theta) d\theta$ [closed],integrate  [closed],\int_0^{2\pi} e^{\cos \theta} \cos( \sin \theta) d\theta,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question How to integrate $ 1)\displaystyle \int_0^{2\pi} e^{\cos \theta} \cos( \sin \theta) d\theta$ $ 2)\displaystyle \int_0^{2\pi} e^{\cos \theta} \sin ( \sin \theta) d\theta$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question How to integrate $ 1)\displaystyle \int_0^{2\pi} e^{\cos \theta} \cos( \sin \theta) d\theta$ $ 2)\displaystyle \int_0^{2\pi} e^{\cos \theta} \sin ( \sin \theta) d\theta$",,"['complex-analysis', 'definite-integrals']"
55,Maybe Things Can be Divided by Zero,Maybe Things Can be Divided by Zero,,In the equation: $$\frac{z^2-1}{z-1}$$ $z$ can not be equal to $1$. However $$\begin{align} \frac{z^2-1}{z-1}&=\frac{(z-1)(z+1)}{z-1}\\ &=(z+1) \end{align}$$ So then if  $z$ is equal to $1$ we have $$\frac{z^2-1}{z-1}=2$$ Can someone explain that please?,In the equation: $$\frac{z^2-1}{z-1}$$ $z$ can not be equal to $1$. However $$\begin{align} \frac{z^2-1}{z-1}&=\frac{(z-1)(z+1)}{z-1}\\ &=(z+1) \end{align}$$ So then if  $z$ is equal to $1$ we have $$\frac{z^2-1}{z-1}=2$$ Can someone explain that please?,,['complex-analysis']
56,Inversion mapping complex function,Inversion mapping complex function,,"Show that the inversion mapping $w = f(z) = \frac{1}{z}$ maps: the   circle $|z-1|=1$ onto the vertical line $x=\frac{1}{2}$. From what I know thus far, I can see that $|z-1|=1$ take $\theta$ from $2\pi > \theta > 0$ will traverse the circle at $z= 1 + e^{i\theta}$, am I right on that since the graph of the function is shifted to the right with radius one, thus $z= 1 + e^{i\theta}$? I do not know how to finish the proof.","Show that the inversion mapping $w = f(z) = \frac{1}{z}$ maps: the   circle $|z-1|=1$ onto the vertical line $x=\frac{1}{2}$. From what I know thus far, I can see that $|z-1|=1$ take $\theta$ from $2\pi > \theta > 0$ will traverse the circle at $z= 1 + e^{i\theta}$, am I right on that since the graph of the function is shifted to the right with radius one, thus $z= 1 + e^{i\theta}$? I do not know how to finish the proof.",,['complex-analysis']
57,There does not exist an entire function which satisfies $f({1\over n})={1\over 2^n}$?,There does not exist an entire function which satisfies ?,f({1\over n})={1\over 2^n},"There does not exist an entire function which satisfies $f({1\over n})={1\over 2^n}$, what I tried is if possible then define $g(z)=f(z)-{1\over 2^{1\over z}}$ Then $g({1\over n})=0$ and so $g(z)$ is entire and its $0$ set has limit point in it and so $f(z)={1\over 2^{1\over z}}$ which is not analytic at $0$? Please help! Edit: OOps! the way I defined $g(z)$ that is not entire! could any one give me hint?","There does not exist an entire function which satisfies $f({1\over n})={1\over 2^n}$, what I tried is if possible then define $g(z)=f(z)-{1\over 2^{1\over z}}$ Then $g({1\over n})=0$ and so $g(z)$ is entire and its $0$ set has limit point in it and so $f(z)={1\over 2^{1\over z}}$ which is not analytic at $0$? Please help! Edit: OOps! the way I defined $g(z)$ that is not entire! could any one give me hint?",,['complex-analysis']
58,using contour integration,using contour integration,,"I am trying to understand using contour integration to evaluate definite integrals. I still don't understand how it works for rational functions in $x$. So can anyone please elaborate this method using any particular function like say $\int_0^{\infty} \frac {1}{1+x^3} \ dx$. I'ld appreciate that. I meant I know what I should be doing but am having problems applying them. So, basically I am interested in how to proceed rather than ""Take this contour and so on"". I'ld appreciate any help you people can give me. Thanks.","I am trying to understand using contour integration to evaluate definite integrals. I still don't understand how it works for rational functions in $x$. So can anyone please elaborate this method using any particular function like say $\int_0^{\infty} \frac {1}{1+x^3} \ dx$. I'ld appreciate that. I meant I know what I should be doing but am having problems applying them. So, basically I am interested in how to proceed rather than ""Take this contour and so on"". I'ld appreciate any help you people can give me. Thanks.",,"['complex-analysis', 'contour-integration']"
59,How to show that this series does not converge uniformly on the open unit disc?,How to show that this series does not converge uniformly on the open unit disc?,,"Given the series $\sum_{k=0}^\infty z^k $, it is easy to see that it converges locally, but how do I go about showing that it does not also converge uniformly on the open unit disc? I know that for it to converge uniformly on the open disc that $sup{|g(z) - g_k(z)|}$, z element of open unit disc, must equal zero. However, I am finding it difficult to show that this series does not go to zero as k goes to infinity. Edit:Fixed confusing terminology as mentioned in answer.","Given the series $\sum_{k=0}^\infty z^k $, it is easy to see that it converges locally, but how do I go about showing that it does not also converge uniformly on the open unit disc? I know that for it to converge uniformly on the open disc that $sup{|g(z) - g_k(z)|}$, z element of open unit disc, must equal zero. However, I am finding it difficult to show that this series does not go to zero as k goes to infinity. Edit:Fixed confusing terminology as mentioned in answer.",,['complex-analysis']
60,"If $|f| \le |g|$, does analytic continuation of $g$ imply analytic continuation of $f$?","If , does analytic continuation of  imply analytic continuation of ?",|f| \le |g| g f,"Let $f,g$ be two holomorphic functions on a domain $D$ such that $|f(z)| \le |g(z)|$ for all $z \in D$. Further suppose that there is an analytic continuation of $g$ to a bigger domain $D'$. Does that imply that there will be an analytic continuation of $f$ on $D'$? If so, will it be necessarily true that $|f(z)| \le |g(z)|$ for all $z \in D'$?","Let $f,g$ be two holomorphic functions on a domain $D$ such that $|f(z)| \le |g(z)|$ for all $z \in D$. Further suppose that there is an analytic continuation of $g$ to a bigger domain $D'$. Does that imply that there will be an analytic continuation of $f$ on $D'$? If so, will it be necessarily true that $|f(z)| \le |g(z)|$ for all $z \in D'$?",,"['complex-analysis', 'maximum-principle', 'analytic-continuation']"
61,Rational function with absolute value $1$ on unit circle,Rational function with absolute value  on unit circle,1,"What is the general form of a rational function which has absolute value $1$ on the circle $|z|=1$? In particular, how are the zeros and poles related to each other? So, write $R(z)=\dfrac{P(z)}{Q(z)}$, where $P,Q$ are polynomials in $z$. The condition specifies that $|R(z)|=1$ for all $z$ such that $|z|=1$. In other words, $|P(z)|=|Q(z)|$ for all $z$ such that $|z|=1$. What can we say about $P$ and $Q$?","What is the general form of a rational function which has absolute value $1$ on the circle $|z|=1$? In particular, how are the zeros and poles related to each other? So, write $R(z)=\dfrac{P(z)}{Q(z)}$, where $P,Q$ are polynomials in $z$. The condition specifies that $|R(z)|=1$ for all $z$ such that $|z|=1$. In other words, $|P(z)|=|Q(z)|$ for all $z$ such that $|z|=1$. What can we say about $P$ and $Q$?",,['complex-analysis']
62,Final year project ideas - complex analysis,Final year project ideas - complex analysis,,"For my final year, I have to do a project for a module. I want to investigate something in the complex analysis area. I've only covered the basics of analysis, like Cauchy's IT/IF, residue theorem etc. The only thing that's been suggested so far is the mathematics of Aerofoils. Just wondering if anyone has ideas of areas I could look at?","For my final year, I have to do a project for a module. I want to investigate something in the complex analysis area. I've only covered the basics of analysis, like Cauchy's IT/IF, residue theorem etc. The only thing that's been suggested so far is the mathematics of Aerofoils. Just wondering if anyone has ideas of areas I could look at?",,"['complex-analysis', 'soft-question', 'big-list']"
63,Stronger Liouville theorem,Stronger Liouville theorem,,"""Every bounded function that is holomorphic on $A$ is is constant."" For which $A\subseteq\mathbb{C}$ is this true? Are there well-known examples of unbounded sets $A\subseteq\mathbb{C}$ on which there are non-constant bounded holomorphic functions? Later edit: My striking through the second question was meant only to de-emphasize it.  Feel free to post further on it if you wish.  Some of the examples posted in response to it were already well known to me; I'd have thought of them if my attention had been on the second question rather than the first. I'm envisioning a couple of possibilities: (1) Various other sorts of sets $A$ will be mentioned in answers; and (2) An answer will say that some nice theorem says this is true of a set $A$ if and only if whatever, where ""whatever"" is something non-trivially different from a tautologous ""if and only if every bounded holomorphic function on $A$ is constant"", and maybe ""whatever"" is somehow elegant or at least simple.","""Every bounded function that is holomorphic on $A$ is is constant."" For which $A\subseteq\mathbb{C}$ is this true? Are there well-known examples of unbounded sets $A\subseteq\mathbb{C}$ on which there are non-constant bounded holomorphic functions? Later edit: My striking through the second question was meant only to de-emphasize it.  Feel free to post further on it if you wish.  Some of the examples posted in response to it were already well known to me; I'd have thought of them if my attention had been on the second question rather than the first. I'm envisioning a couple of possibilities: (1) Various other sorts of sets $A$ will be mentioned in answers; and (2) An answer will say that some nice theorem says this is true of a set $A$ if and only if whatever, where ""whatever"" is something non-trivially different from a tautologous ""if and only if every bounded holomorphic function on $A$ is constant"", and maybe ""whatever"" is somehow elegant or at least simple.",,['complex-analysis']
64,Proof for law of complex exponents using only differential equation,Proof for law of complex exponents using only differential equation,,"I just read that an elegant proof exists that the law of exponents also holds for complex numbers ($a,b,z$ all complex): $$e^{a+b}=e^ae^b,$$ which only uses the definition that $$y=e^{zt}$$ is a solution to $$dy/dt=zy,$$ with initial condition $y(0)=1$, so in particular $e^z=y(1).$ I can only find a proofs which use the trig-representation of complex numbers. Can anybody help? Thank you!","I just read that an elegant proof exists that the law of exponents also holds for complex numbers ($a,b,z$ all complex): $$e^{a+b}=e^ae^b,$$ which only uses the definition that $$y=e^{zt}$$ is a solution to $$dy/dt=zy,$$ with initial condition $y(0)=1$, so in particular $e^z=y(1).$ I can only find a proofs which use the trig-representation of complex numbers. Can anybody help? Thank you!",,"['complex-analysis', 'ordinary-differential-equations', 'complex-numbers', 'exponentiation']"
65,"Show that if $(z+1)^{100} = (z-1)^{100}$, then $z$ is purely imaginary","Show that if , then  is purely imaginary",(z+1)^{100} = (z-1)^{100} z,"Let $z$ be a complex number satisfying   $$(z+1)^{100} = (z-1)^{100}$$   Show that $z$ is purely imaginary, i.e. that $\Re(z) = 0$. Rearrange to $$\left(\frac{z+1}{z-1}\right)^{100} = 1$$ I tried using $z = x+iy$ and trying to multiply numerator and denominator by the conjugate, but I hit a roadblock. I also tried substituting $1 = -e^{i\pi}$, but that also doesn't seem to get me anywhere. How can I prove this? Any help is appreciated, thank you!","Let $z$ be a complex number satisfying   $$(z+1)^{100} = (z-1)^{100}$$   Show that $z$ is purely imaginary, i.e. that $\Re(z) = 0$. Rearrange to $$\left(\frac{z+1}{z-1}\right)^{100} = 1$$ I tried using $z = x+iy$ and trying to multiply numerator and denominator by the conjugate, but I hit a roadblock. I also tried substituting $1 = -e^{i\pi}$, but that also doesn't seem to get me anywhere. How can I prove this? Any help is appreciated, thank you!",,"['complex-analysis', 'complex-numbers']"
66,Can the derivative of a real function be imaginary?,Can the derivative of a real function be imaginary?,,Suppose $f$ is a real function (e.g. mapping $\mathbb{R} \rightarrow \mathbb{R}$ ). Is it possible for its derivative (i.e. $\frac{d}{dx}f(x)$ ) to be imaginary?,Suppose is a real function (e.g. mapping ). Is it possible for its derivative (i.e. ) to be imaginary?,f \mathbb{R} \rightarrow \mathbb{R} \frac{d}{dx}f(x),"['complex-analysis', 'derivatives']"
67,Proving that $\Gamma \left(n+ \frac{1}{2}\right) = \frac{(2n)!\sqrt{\pi}}{2^{2n}n!}$.,Proving that .,\Gamma \left(n+ \frac{1}{2}\right) = \frac{(2n)!\sqrt{\pi}}{2^{2n}n!},"The proof I am dealing with is worded exactly as follows: Prove $\Gamma\left(n+ \frac{1}{2}\right) = \frac{(2n)!\sqrt{\pi}}{2^{2n}n!}$. The proof itself can be done easily with induction, I assume. However, my issue is with the domain of the given $n$; granted, the factorial operator is only defined for positive integer values. However, the gamma function, as far as I know, is defined for all complex numbers bar $\mathbb{Z}^-$. I don't think induction will suffice for generalizing the proof for said domain. Would that be necessary, or should I be looking for other methods to tackle the proof?","The proof I am dealing with is worded exactly as follows: Prove $\Gamma\left(n+ \frac{1}{2}\right) = \frac{(2n)!\sqrt{\pi}}{2^{2n}n!}$. The proof itself can be done easily with induction, I assume. However, my issue is with the domain of the given $n$; granted, the factorial operator is only defined for positive integer values. However, the gamma function, as far as I know, is defined for all complex numbers bar $\mathbb{Z}^-$. I don't think induction will suffice for generalizing the proof for said domain. Would that be necessary, or should I be looking for other methods to tackle the proof?",,['complex-analysis']
68,Significance of Roots of Unity,Significance of Roots of Unity,,"A strong precalculus course (think AMC, JEE) will teach complex numbers, specifically $n^{\text{th}}$ roots of unity with their properties: They lie equally spaced on the unit circle A power of an $n^{\text{th}}$ root is another $n^{\text{th}}$ root Their sum What applications do roots of unity have in theoretical or applied math?","A strong precalculus course (think AMC, JEE) will teach complex numbers, specifically roots of unity with their properties: They lie equally spaced on the unit circle A power of an root is another root Their sum What applications do roots of unity have in theoretical or applied math?",n^{\text{th}} n^{\text{th}} n^{\text{th}},"['complex-analysis', 'algebra-precalculus', 'complex-numbers', 'soft-question', 'roots-of-unity']"
69,Laurent Series at Infinity,Laurent Series at Infinity,,"I thought that finding the Laurent series was something that was straightforward, however, I am having some difficulty of finding the Laurent series of $$f(z) = \frac{1}{z(1-z)}$$ for $z= \infty$.  Any suggestions?","I thought that finding the Laurent series was something that was straightforward, however, I am having some difficulty of finding the Laurent series of $$f(z) = \frac{1}{z(1-z)}$$ for $z= \infty$.  Any suggestions?",,"['complex-analysis', 'laurent-series']"
70,Prove that $|e^{i\theta_1}-e^{i\theta_2}| \leq |\theta_1 - \theta_2|$,Prove that,|e^{i\theta_1}-e^{i\theta_2}| \leq |\theta_1 - \theta_2|,"I'm trying to prove the inequality $$|e^{i\theta_1}-e^{i\theta_2}| \leq |\theta_1 - \theta_2|$$ I have tried to use Taylor's formula and got this $$|e^{i\theta_1}-e^{i\theta_2}| = |(1+i\theta_1 - \frac{\theta_1^2}{2} + \ldots) -(1 +i\theta_2 - \frac{\theta_2^2}{2} + \ldots)| = |i(\theta_1-\theta_2) + \frac{\theta_2^2-\theta_1^2}{2}+\ldots|$$ The first term looks right, but how do I proceed?","I'm trying to prove the inequality $$|e^{i\theta_1}-e^{i\theta_2}| \leq |\theta_1 - \theta_2|$$ I have tried to use Taylor's formula and got this $$|e^{i\theta_1}-e^{i\theta_2}| = |(1+i\theta_1 - \frac{\theta_1^2}{2} + \ldots) -(1 +i\theta_2 - \frac{\theta_2^2}{2} + \ldots)| = |i(\theta_1-\theta_2) + \frac{\theta_2^2-\theta_1^2}{2}+\ldots|$$ The first term looks right, but how do I proceed?",,"['complex-analysis', 'inequality', 'exponential-function']"
71,Showing that $a$ is a removable singularity if $\mathrm{Im}(f(z))$ is bounded from above,Showing that  is a removable singularity if  is bounded from above,a \mathrm{Im}(f(z)),"Problem: Suppose $f$ is analytic on the domain $\Omega$ except at the isolated   singularity $a \in \Omega$.  Show that $a$ is a removable singularity   if $\mathrm{Im}(f(z))$ is bounded from above. Attempt: We have that $a$ is a removable singularity if and only if we have that $$ \lim_{z\rightarrow a} (z-a)f(z) =0 $$ Since $\mathrm{Im}(f(z))$ is bounded from above, there exists some $M \in \mathbb{R}_{\ge 0}$ s.t. $\mathrm{Im}(f(z)) \le M$ for all $z \in \Omega$. If we could show $$ \mathrm{Im}(f(z)) \le M \implies |f(z)| \le M_1 \text{ for some } M_1 \in \mathbb{R}_{\ge 0} $$ then we would immediately have (1) since $$ \lim_{z \rightarrow a}|(z-a)f(z)| \le \lim_{z \rightarrow a}|(z-a)|M_1 = 0 $$ Question: Is this the right approach?","Problem: Suppose $f$ is analytic on the domain $\Omega$ except at the isolated   singularity $a \in \Omega$.  Show that $a$ is a removable singularity   if $\mathrm{Im}(f(z))$ is bounded from above. Attempt: We have that $a$ is a removable singularity if and only if we have that $$ \lim_{z\rightarrow a} (z-a)f(z) =0 $$ Since $\mathrm{Im}(f(z))$ is bounded from above, there exists some $M \in \mathbb{R}_{\ge 0}$ s.t. $\mathrm{Im}(f(z)) \le M$ for all $z \in \Omega$. If we could show $$ \mathrm{Im}(f(z)) \le M \implies |f(z)| \le M_1 \text{ for some } M_1 \in \mathbb{R}_{\ge 0} $$ then we would immediately have (1) since $$ \lim_{z \rightarrow a}|(z-a)f(z)| \le \lim_{z \rightarrow a}|(z-a)|M_1 = 0 $$ Question: Is this the right approach?",,"['complex-analysis', 'analysis']"
72,Can a holomorphic function satisfy $f(1/n)=1/(n+1)$?,Can a holomorphic function satisfy ?,f(1/n)=1/(n+1),Does there exist a function $f$ which is holomorphic on $B_0(2)$ (open disc of radius 2 in the complex plane) such that $f(1/n)=1/(n+1) \forall n \in \mathbb{N}$? At the moment I'm thinking not but a proof is seeming elusive. Any hints would be appreciated.,Does there exist a function $f$ which is holomorphic on $B_0(2)$ (open disc of radius 2 in the complex plane) such that $f(1/n)=1/(n+1) \forall n \in \mathbb{N}$? At the moment I'm thinking not but a proof is seeming elusive. Any hints would be appreciated.,,['complex-analysis']
73,Evaluate $\sum\limits_{n=0}^\infty \frac{1}{4n^2+1}$ by using complex contour integration,Evaluate  by using complex contour integration,\sum\limits_{n=0}^\infty \frac{1}{4n^2+1},"How do you evaluate $\displaystyle\sum\limits_{n=0}^\infty \frac{1}{4n^2+1}$ by using complex contour integration? I'm trying to attempt this question by considering the integral of some function about a square in the complex plane, whose residues at each singularity on the real axis evaluate to $\large\frac{1}{4k^2+1}$ for all integers $k$. Maybe a function similar to $$\frac {\cot \pi z}{4z^2+1}$$ Maybe then define a square centred on the origin with sides of length $2N+1$ then letting $N \to \infty$ we can split up the integral to evaluate the sum of the residues? Which would be our summation. Sorry if this is poorly explained, but as I say, i'm having trouble understanding this, so i'm not that sure myself, I know it's possible though!","How do you evaluate $\displaystyle\sum\limits_{n=0}^\infty \frac{1}{4n^2+1}$ by using complex contour integration? I'm trying to attempt this question by considering the integral of some function about a square in the complex plane, whose residues at each singularity on the real axis evaluate to $\large\frac{1}{4k^2+1}$ for all integers $k$. Maybe a function similar to $$\frac {\cot \pi z}{4z^2+1}$$ Maybe then define a square centred on the origin with sides of length $2N+1$ then letting $N \to \infty$ we can split up the integral to evaluate the sum of the residues? Which would be our summation. Sorry if this is poorly explained, but as I say, i'm having trouble understanding this, so i'm not that sure myself, I know it's possible though!",,['complex-analysis']
74,What is this set notation called?,What is this set notation called?,,"So I was watching a youtube video about complex analysis and the professor wrote the equation on the board: $e^{i\theta}=e^{i\beta}$ And then he said that: $\theta=\beta+2\pi k$ For $k\in \Bbb{Z}$ No problems so far. Then he says, ""or in shorthand"": $\theta\in\beta+2\pi\Bbb{Z}$ Two questions: First what is the name of this notation? What would be the similar notation for the case where: $\theta=ae+b\pi$ With $a\in\Bbb{Z}$ and $b\in\Bbb{Z}$? I want to write: $\theta\in e\Bbb{Z}+\pi\Bbb{Z}$ But I know that that is wrong because $a$ and $b$ could be different integers.","So I was watching a youtube video about complex analysis and the professor wrote the equation on the board: $e^{i\theta}=e^{i\beta}$ And then he said that: $\theta=\beta+2\pi k$ For $k\in \Bbb{Z}$ No problems so far. Then he says, ""or in shorthand"": $\theta\in\beta+2\pi\Bbb{Z}$ Two questions: First what is the name of this notation? What would be the similar notation for the case where: $\theta=ae+b\pi$ With $a\in\Bbb{Z}$ and $b\in\Bbb{Z}$? I want to write: $\theta\in e\Bbb{Z}+\pi\Bbb{Z}$ But I know that that is wrong because $a$ and $b$ could be different integers.",,"['complex-analysis', 'elementary-set-theory', 'notation', 'terminology']"
75,Conditions implying that image of $f$ contains the unit disc,Conditions implying that image of  contains the unit disc,f,"I'm stuck with this problem from Stein-Shakarchi: Let $f$ be non-constant and holomorphic in an open set containing the closed unit disc. a) Show that if $|f(z)| = 1$ whenever $|z| = 1$, then the image of $f$ contains the unit disc. b) If $|f(z)| \geq 1$ whenever $|z| = 1$ and there exists $z_{0} \in D(0,1) $ such that $|f(z_{0})| < 1$, then the image of $f$ contains the unit disc. Any idea ?","I'm stuck with this problem from Stein-Shakarchi: Let $f$ be non-constant and holomorphic in an open set containing the closed unit disc. a) Show that if $|f(z)| = 1$ whenever $|z| = 1$, then the image of $f$ contains the unit disc. b) If $|f(z)| \geq 1$ whenever $|z| = 1$ and there exists $z_{0} \in D(0,1) $ such that $|f(z_{0})| < 1$, then the image of $f$ contains the unit disc. Any idea ?",,"['complex-analysis', 'analysis']"
76,Integrate $\int_{-\infty}^\infty \frac{x}{\sinh x}~dx$,Integrate,\int_{-\infty}^\infty \frac{x}{\sinh x}~dx,"From a Problem Set on residues: Evaluate $$\int_0^\infty \frac{\log x}{(x-1) \sqrt{x}}~dx.$$ After the substitution $x = e^u$ and easy computations, the integral becomes $$\int_{-\infty}^\infty \frac{x}{\sinh x}~dx.$$","From a Problem Set on residues: Evaluate After the substitution and easy computations, the integral becomes",\int_0^\infty \frac{\log x}{(x-1) \sqrt{x}}~dx. x = e^u \int_{-\infty}^\infty \frac{x}{\sinh x}~dx.,"['complex-analysis', 'contour-integration']"
77,"What is “entire finite complex plane""?","What is “entire finite complex plane""?",,"The question is from the following problem: If $f(z)$ is an analytic function that maps the entire finite complex plane into the real axis, then the imaginary axis must be mapped onto A. the entire real axis B. a point C. a ray D. an open finite interval E. the empty set A search on Google didn't return any satisfying result of the concept ""the entire finite complex plane"". The word ""entire"" and ""finite"" seem to be a contradiction to me. Here are my questions: What is ""the entire finite complex plane""? What properties does one need to use about analytic function to solve this problem?","The question is from the following problem: If $f(z)$ is an analytic function that maps the entire finite complex plane into the real axis, then the imaginary axis must be mapped onto A. the entire real axis B. a point C. a ray D. an open finite interval E. the empty set A search on Google didn't return any satisfying result of the concept ""the entire finite complex plane"". The word ""entire"" and ""finite"" seem to be a contradiction to me. Here are my questions: What is ""the entire finite complex plane""? What properties does one need to use about analytic function to solve this problem?",,['reference-request']
78,Why does $\lim_{n \to \infty} \sqrt[n]{(-1)^n \cdot n^2 + 1} = 1$?,Why does ?,\lim_{n \to \infty} \sqrt[n]{(-1)^n \cdot n^2 + 1} = 1,"As the title suggests, I want to know as to why the following function converges to 1 for $n \to \infty$: $$ \lim_{n \to \infty} \sqrt[n]{(-1)^n \cdot n^2 + 1} = 1 $$ For even $n$'s only $n^2+1$ has to be shown, which I did in the following way: $$\sqrt[n]{n^2} \le \sqrt[n]{n^2 + 1} \le \sqrt[n]{n^3}$$ Assuming we have already proven that $\lim_{n \to \infty}\sqrt[n]{n^k} = 1$ we can conclude that $$1 \le \sqrt[n]{n^2+1} \le 1 \Rightarrow \lim_{n \to \infty} \sqrt[n]{n^2+1} = 1.$$ For odd $n$'s I can't find the solution. I tried going the same route as for even $n$'s: $$\sqrt[n]{-n^2} \le \sqrt[n]{-n^2 + 1} \le \sqrt[n]{-n^3}$$ And it seems that it comes down to $$\lim_{n \to \infty} \sqrt[n]{-n^k}$$ I checked the limit using both Wolfram Alpha and a CAS and it converges to 1. Why is that?","As the title suggests, I want to know as to why the following function converges to 1 for $n \to \infty$: $$ \lim_{n \to \infty} \sqrt[n]{(-1)^n \cdot n^2 + 1} = 1 $$ For even $n$'s only $n^2+1$ has to be shown, which I did in the following way: $$\sqrt[n]{n^2} \le \sqrt[n]{n^2 + 1} \le \sqrt[n]{n^3}$$ Assuming we have already proven that $\lim_{n \to \infty}\sqrt[n]{n^k} = 1$ we can conclude that $$1 \le \sqrt[n]{n^2+1} \le 1 \Rightarrow \lim_{n \to \infty} \sqrt[n]{n^2+1} = 1.$$ For odd $n$'s I can't find the solution. I tried going the same route as for even $n$'s: $$\sqrt[n]{-n^2} \le \sqrt[n]{-n^2 + 1} \le \sqrt[n]{-n^3}$$ And it seems that it comes down to $$\lim_{n \to \infty} \sqrt[n]{-n^k}$$ I checked the limit using both Wolfram Alpha and a CAS and it converges to 1. Why is that?",,"['complex-analysis', 'analysis', 'limits']"
79,Proving an entire  function which misses a ball is constant,Proving an entire  function which misses a ball is constant,,Let $f$ be an entire function s.t $f(\mathbb{C}) \cap B_R(z_0) = \varnothing$ for some $z_0$ and some $R$. Then $f$ is constant. I guess since the image of the whole plane isn't dense then $f$ doesn't have an essential singularity at infinity. Now I have to exclude it has a polar singularity at infinity..,Let $f$ be an entire function s.t $f(\mathbb{C}) \cap B_R(z_0) = \varnothing$ for some $z_0$ and some $R$. Then $f$ is constant. I guess since the image of the whole plane isn't dense then $f$ doesn't have an essential singularity at infinity. Now I have to exclude it has a polar singularity at infinity..,,['complex-analysis']
80,"When $f(z) = 1/z$ and $g(z) = 1-z$, why is $f \circ g \circ f \circ g \circ f \circ g (z) = g \circ f \circ g \circ f \circ g \circ f (z) = z$?","When  and , why is ?",f(z) = 1/z g(z) = 1-z f \circ g \circ f \circ g \circ f \circ g (z) = g \circ f \circ g \circ f \circ g \circ f (z) = z,"A recent contest question I was attempting involved a long string of repeated and random applications of the two complex functions $f(z) = 1/z \,$ and $\, g(z) = 1-z \,$ to an unknown complex number other than $0$ or $1$ . Since $f$ and $g$ are obvious involutions, such a string could be reduced to a string of alternating applications of $f$ and $g$ . When I started to analyze such a string, I quickly came across the following: $$f \circ g \circ f \circ g \circ f \circ g \, (z) = g \circ f \circ g \circ f \circ g \circ f \, (z) = z$$ This may be a well-known property of these two functions, but I'd never come across it before and I was quite surprised by it, in particular that it took three iterations of each to become an identity - somehow I think I would have been less surprised if it had taken two or four iterations. The algebra is quite straightforward; it's easy to see that applying the first of the two above compositions gives $$z \to \frac{1}{z} \to \frac{z-1}{z} \to \frac{z}{z-1} \to \frac{1}{1-z} \to 1-z \to z$$ and that the second gives the same chain in reverse. However, this gave me no insight into why three is the magic number. I tried looking at this geometrically, but my complex geometry is admittedly pretty weak. As best as I could see, $f(z)$ reflects a point in the real axis and then inverts the image in the circle $r=1$ ; and $g(z)$ reflects a point in the line $x=\frac{1}{2}$ and then reflects the image in the real axis. Since for either function the order of the two transformations can be interchanged, when applying one function followed by the other the two reflections in the real axis would cancel each other out and result in an inversion in the circle $r=1$ followed by a reflection in the line $x=\frac{1}{2}$ , or vice-versa. However, this did not help as I could not see why doing this three times would return a point to its original position. Even restricting to just real values didn't lead to any enlightenment as the transformations would still be essentially the same. So, is there some not-too-complicated way to see why it is that three alternating iterations of each of these two functions is an identity?","A recent contest question I was attempting involved a long string of repeated and random applications of the two complex functions and to an unknown complex number other than or . Since and are obvious involutions, such a string could be reduced to a string of alternating applications of and . When I started to analyze such a string, I quickly came across the following: This may be a well-known property of these two functions, but I'd never come across it before and I was quite surprised by it, in particular that it took three iterations of each to become an identity - somehow I think I would have been less surprised if it had taken two or four iterations. The algebra is quite straightforward; it's easy to see that applying the first of the two above compositions gives and that the second gives the same chain in reverse. However, this gave me no insight into why three is the magic number. I tried looking at this geometrically, but my complex geometry is admittedly pretty weak. As best as I could see, reflects a point in the real axis and then inverts the image in the circle ; and reflects a point in the line and then reflects the image in the real axis. Since for either function the order of the two transformations can be interchanged, when applying one function followed by the other the two reflections in the real axis would cancel each other out and result in an inversion in the circle followed by a reflection in the line , or vice-versa. However, this did not help as I could not see why doing this three times would return a point to its original position. Even restricting to just real values didn't lead to any enlightenment as the transformations would still be essentially the same. So, is there some not-too-complicated way to see why it is that three alternating iterations of each of these two functions is an identity?","f(z) = 1/z \, \, g(z) = 1-z \, 0 1 f g f g f \circ g \circ f \circ g \circ f \circ g \, (z) = g \circ f \circ g \circ f \circ g \circ f \, (z) = z z \to \frac{1}{z} \to \frac{z-1}{z} \to \frac{z}{z-1} \to \frac{1}{1-z} \to 1-z \to z f(z) r=1 g(z) x=\frac{1}{2} r=1 x=\frac{1}{2}","['complex-analysis', 'functions']"
81,Why can't we order Complex Numbers? [duplicate],Why can't we order Complex Numbers? [duplicate],,"This question already has answers here : Total order on complex numbers [closed] (2 answers) Closed 8 years ago . I know this may very well be a silly question. I always hear that Complex numbers cannot be ordered. But there's something I'm missing... Why can't we just compare two complex numbers $z_1,z_2$ as follows: A number is less then the other if the modulo is less or equal modulo but angle less from the x axis, in other words $z_1$<$z_2$ if  $\rho_1<\rho_2$; or  $\rho_1=\rho_2$ and $\theta_1<\theta_2$ where obvously $$z_1=\rho_1 e^{i\theta_1}, z_2=\rho_2 e^{i\theta_2}, \mathrm{with\ }\theta_1,\theta_2 \in [0,2\pi)$$ Why don't we do it? And if we do, why everyone always says that the Complex numbers cannot be ordered?","This question already has answers here : Total order on complex numbers [closed] (2 answers) Closed 8 years ago . I know this may very well be a silly question. I always hear that Complex numbers cannot be ordered. But there's something I'm missing... Why can't we just compare two complex numbers $z_1,z_2$ as follows: A number is less then the other if the modulo is less or equal modulo but angle less from the x axis, in other words $z_1$<$z_2$ if  $\rho_1<\rho_2$; or  $\rho_1=\rho_2$ and $\theta_1<\theta_2$ where obvously $$z_1=\rho_1 e^{i\theta_1}, z_2=\rho_2 e^{i\theta_2}, \mathrm{with\ }\theta_1,\theta_2 \in [0,2\pi)$$ Why don't we do it? And if we do, why everyone always says that the Complex numbers cannot be ordered?",,"['complex-analysis', 'analysis', 'elementary-set-theory', 'complex-numbers', 'soft-question']"
82,Proof that Radius of Convergence Extend to Nearest Singularity,Proof that Radius of Convergence Extend to Nearest Singularity,,"Can someone provide a proof for the fact that the radius of convergence of the power series of an analytic function is the distance to the nearest singularity? I've read the identity theorem, but I don't see how it implies that the two functions must be equal everywhere .","Can someone provide a proof for the fact that the radius of convergence of the power series of an analytic function is the distance to the nearest singularity? I've read the identity theorem, but I don't see how it implies that the two functions must be equal everywhere .",,"['complex-analysis', 'convergence-divergence']"
83,Finding the residue at $z=0$ for complex function $1/(z^2\sin z$),Finding the residue at  for complex function ),z=0 1/(z^2\sin z,Find the residue at $z=0$ for complex function $1/(z^2\sin z$) I know $z=0$ is a pole of order 3 but can't seem to calculate the residue value for it.,Find the residue at $z=0$ for complex function $1/(z^2\sin z$) I know $z=0$ is a pole of order 3 but can't seem to calculate the residue value for it.,,['complex-analysis']
84,Integration (Fourier transform),Integration (Fourier transform),,"$\mathrm{Re}(i) = 0$, but the fourier transform of $f(x) = e^{-ix^2}$ is $g(\alpha) = \sqrt{\pi\over i}\times e^{i\alpha^2 \over 4}$, is it not? Is there an easy to show that it is so, knowing the fourier transform of $h(x) = e^{-x^2}$? (It is $k(\alpha) = \sqrt\pi\times e^{-\alpha^2 \over 4}$.) The non-unitary, angular frequency Fourier transform is taken into consideration.","$\mathrm{Re}(i) = 0$, but the fourier transform of $f(x) = e^{-ix^2}$ is $g(\alpha) = \sqrt{\pi\over i}\times e^{i\alpha^2 \over 4}$, is it not? Is there an easy to show that it is so, knowing the fourier transform of $h(x) = e^{-x^2}$? (It is $k(\alpha) = \sqrt\pi\times e^{-\alpha^2 \over 4}$.) The non-unitary, angular frequency Fourier transform is taken into consideration.",,"['complex-analysis', 'fourier-analysis']"
85,Interpolation of analytic function on unit disk,Interpolation of analytic function on unit disk,,"Been thinking about this problem for a long time without any progress, can someone help? Consider a bounded function $f: \mathbb{D} \rightarrow \mathbb{D}$ with the following property : for every finite sequence $z_1, z_2, \dots z_n \in \mathbb{D}$, there exists an analytic function $g: \mathbb{D}\rightarrow \mathbb{D}$ such that $g(z_j)=f(z_j)$ for $j=1,2,\dots, n$. Show that $f$ itself is analytic. Thanks","Been thinking about this problem for a long time without any progress, can someone help? Consider a bounded function $f: \mathbb{D} \rightarrow \mathbb{D}$ with the following property : for every finite sequence $z_1, z_2, \dots z_n \in \mathbb{D}$, there exists an analytic function $g: \mathbb{D}\rightarrow \mathbb{D}$ such that $g(z_j)=f(z_j)$ for $j=1,2,\dots, n$. Show that $f$ itself is analytic. Thanks",,['complex-analysis']
86,Multiplying complex numbers in polar form?,Multiplying complex numbers in polar form?,,"Could someone explain why you multiply the lengths and add the angles when multiplying polar coordinates? I tried multiplying the polar forms ($r_1\left(\cos\theta_1 + i\sin\theta_1\right)\cdot r_2\left(\cos\theta_2 + i\sin\theta_2\right)$), and expanding/factoring the result, and end up multiplying the lengths but can't seem to come to an equation where you add the angles.","Could someone explain why you multiply the lengths and add the angles when multiplying polar coordinates? I tried multiplying the polar forms ($r_1\left(\cos\theta_1 + i\sin\theta_1\right)\cdot r_2\left(\cos\theta_2 + i\sin\theta_2\right)$), and expanding/factoring the result, and end up multiplying the lengths but can't seem to come to an equation where you add the angles.",,['complex-analysis']
87,A problem about generalization of Bezout equation to entire functions,A problem about generalization of Bezout equation to entire functions,,"Let $f_1,f_2,\ldots, f_n$ be $n$ entire functions, and they don't have any common zero as a whole (not in pairs), then can we assert that there exist $n$ entire functions $g_1,g_2,\ldots,g_n$,such that $F=f_1g_1+f_2g_2+\cdots+f_ng_n$ is zero free? We know that if $f_1,\ldots,f_n$ are known to be polynomials, the conclusion follows from Bezout equation and induction. But things become complicated when infinite products get involved. (The original formulation of this problem is: Each finitely generated ideal in the ring of entire functions must be principal, which evidently can be reduced to the problem above.)","Let $f_1,f_2,\ldots, f_n$ be $n$ entire functions, and they don't have any common zero as a whole (not in pairs), then can we assert that there exist $n$ entire functions $g_1,g_2,\ldots,g_n$,such that $F=f_1g_1+f_2g_2+\cdots+f_ng_n$ is zero free? We know that if $f_1,\ldots,f_n$ are known to be polynomials, the conclusion follows from Bezout equation and induction. But things become complicated when infinite products get involved. (The original formulation of this problem is: Each finitely generated ideal in the ring of entire functions must be principal, which evidently can be reduced to the problem above.)",,['complex-analysis']
88,Find the number of roots of a polynomial using Rouche's Theorem,Find the number of roots of a polynomial using Rouche's Theorem,,"Use Rouche's theorem to find the number of roots of the polynomial $z^5+3z^2+1$ in the anulus $1<|z|<2$. I am looking for a solution to this problem. My thoughts: This is a topic that was explained rather quickly at the end of class last week, and I'm struggling on how to answer it.  I know that Rouche's Theorem says: If $f$ and $h$ are each functions that are analytic inside and on a simple closed contour C and if the strict inequality $|h(z)|<|f(z)|$ holds at each point on C, then $f$ and $f+h$ must have the same total number of zeros inside C. My confusion with this problem begins with the fact that the theorem is describing two functions and I've only been given one. I believe that I can split up the function I've been given into two analytic functions and approach it with the theorem that way; but I'm not sure how to decide what to split the function into.","Use Rouche's theorem to find the number of roots of the polynomial $z^5+3z^2+1$ in the anulus $1<|z|<2$. I am looking for a solution to this problem. My thoughts: This is a topic that was explained rather quickly at the end of class last week, and I'm struggling on how to answer it.  I know that Rouche's Theorem says: If $f$ and $h$ are each functions that are analytic inside and on a simple closed contour C and if the strict inequality $|h(z)|<|f(z)|$ holds at each point on C, then $f$ and $f+h$ must have the same total number of zeros inside C. My confusion with this problem begins with the fact that the theorem is describing two functions and I've only been given one. I believe that I can split up the function I've been given into two analytic functions and approach it with the theorem that way; but I'm not sure how to decide what to split the function into.",,"['complex-analysis', 'polynomials', 'roots', 'analyticity']"
89,Existence of Holomorphic function (Application of Schwarz-Lemma),Existence of Holomorphic function (Application of Schwarz-Lemma),,"Let, $D=\{z\in \mathbb C:|z|<1\}$ . Which are correct? there exists a holomorphic function $f:D \to D$ with $f(0)=0$ & $f'(0)=2$ . there exists a holomorphic function $f:D \to D$ with $f\left(\dfrac{3}{4}\right)=\dfrac{3}{4}$ & $f'\left(\dfrac{2}{3}\right)=\dfrac{3}{4}$ . there exists a holomorphic function $f:D \to D$ with $f\left(\dfrac{3}{4}\right)=-\dfrac{3}{4}$ & $f'\left(\dfrac{3}{4}\right)=-\dfrac{3}{4}$ there exists a holomorphic function $f:D \to D$ with $f\left(\dfrac{1}{2}\right)=-\dfrac{1}{2}$ & $f'\left(\dfrac{1}{4}\right)=1$ . With the help of Schwarz lemma & its applications, we find that $(1)$ is false & $(3)$ is true. But, I can not think about options $(2)$ & $(4)$ .","Let, . Which are correct? there exists a holomorphic function with & . there exists a holomorphic function with & . there exists a holomorphic function with & there exists a holomorphic function with & . With the help of Schwarz lemma & its applications, we find that is false & is true. But, I can not think about options & .",D=\{z\in \mathbb C:|z|<1\} f:D \to D f(0)=0 f'(0)=2 f:D \to D f\left(\dfrac{3}{4}\right)=\dfrac{3}{4} f'\left(\dfrac{2}{3}\right)=\dfrac{3}{4} f:D \to D f\left(\dfrac{3}{4}\right)=-\dfrac{3}{4} f'\left(\dfrac{3}{4}\right)=-\dfrac{3}{4} f:D \to D f\left(\dfrac{1}{2}\right)=-\dfrac{1}{2} f'\left(\dfrac{1}{4}\right)=1 (1) (3) (2) (4),['complex-analysis']
90,Complex Integration poles real axis,Complex Integration poles real axis,,"In class my professor said that $$ \int_{-\infty}^{\infty}\frac{e^{iax}}{x^2 - b^2}dx = -\frac{2\pi}{b}\sin(ab) $$ where $a,b > 0$. However, since the poles are on the real axis, isn't the integral equal to $$ \pi i\sum_{\text{real axis}}\text{Res}(f(z); z_j)\mbox{?} $$ If that is the case, the integral is $$ -\frac{\pi}{b}\sin(ab). $$","In class my professor said that $$ \int_{-\infty}^{\infty}\frac{e^{iax}}{x^2 - b^2}dx = -\frac{2\pi}{b}\sin(ab) $$ where $a,b > 0$. However, since the poles are on the real axis, isn't the integral equal to $$ \pi i\sum_{\text{real axis}}\text{Res}(f(z); z_j)\mbox{?} $$ If that is the case, the integral is $$ -\frac{\pi}{b}\sin(ab). $$",,"['complex-analysis', 'contour-integration', 'residue-calculus']"
91,What is the analytic continuation of a multifactorial?,What is the analytic continuation of a multifactorial?,,"The $\Gamma$ function is the analytic continuation of the factorial function.  Is there a similar analog for multifactorials? I am particularly interested in the double factorial.  All Google has given me is the following formula relating the $\Gamma$ function to the double factorial for half integer values: $$\Gamma\left(n+\frac{1}{2}\right)=\frac{(2n-1)!!}{2^n}\sqrt{\pi}$$ But I want to double factorial nonintegers, so this is not really helpful.","The $\Gamma$ function is the analytic continuation of the factorial function.  Is there a similar analog for multifactorials? I am particularly interested in the double factorial.  All Google has given me is the following formula relating the $\Gamma$ function to the double factorial for half integer values: $$\Gamma\left(n+\frac{1}{2}\right)=\frac{(2n-1)!!}{2^n}\sqrt{\pi}$$ But I want to double factorial nonintegers, so this is not really helpful.",,"['number-theory', 'complex-analysis', 'analytic-number-theory', 'factorial']"
92,The Bound of the 8th Derivative of an Analytic Function,The Bound of the 8th Derivative of an Analytic Function,,"This is another question from a recent qualifying exam that really stumped me.  I was thinking of using something with the Cauchy estimate for derivatives, but was clueless beyond that. Let $f:[-1,1]\times [0,2]\rightarrow \mathbb{C}$ be real valued on the interval $[-1,1]$ in the $x$-axis.  Show that if $|f|\leq 1$ on its domain, and is analytic on the interior of its domain, then $|f^{(8)}(\frac{i}{4})|\leq 120$.","This is another question from a recent qualifying exam that really stumped me.  I was thinking of using something with the Cauchy estimate for derivatives, but was clueless beyond that. Let $f:[-1,1]\times [0,2]\rightarrow \mathbb{C}$ be real valued on the interval $[-1,1]$ in the $x$-axis.  Show that if $|f|\leq 1$ on its domain, and is analytic on the interior of its domain, then $|f^{(8)}(\frac{i}{4})|\leq 120$.",,['complex-analysis']
93,determination of a holomorphic function by its poles and zeros,determination of a holomorphic function by its poles and zeros,,"While reading a text about the application of complex analysis to elasticity, I thought about the following problem: Let $f$ be a holomorphic function in all $\mathbb{C}$. Is $f$ uniquely determined by the list of its poles and zeros (and their orders, of course)? EDIT: By ""the list of its poles and zeros"" I include also the point at $\infty$. I assume that $f$ has a proper limit at infinity. I guess that if that was true it was an undergrad theorem that I'm supposed to know.","While reading a text about the application of complex analysis to elasticity, I thought about the following problem: Let $f$ be a holomorphic function in all $\mathbb{C}$. Is $f$ uniquely determined by the list of its poles and zeros (and their orders, of course)? EDIT: By ""the list of its poles and zeros"" I include also the point at $\infty$. I assume that $f$ has a proper limit at infinity. I guess that if that was true it was an undergrad theorem that I'm supposed to know.",,['complex-analysis']
94,Continuous Extension of a Bounded Holomorphic Function on the unit disk?,Continuous Extension of a Bounded Holomorphic Function on the unit disk?,,"Let $f$ be a bounded holomorphic function on the upper-half plane. Is it true that such a map always admits an (unique) extension to real line, so that $f$ is continuous on the closure of the upper half plane?  I feel like the answer to this question is ""no,"" since a lot of theorems concerning bounded holomorphic functions start out with, ""Assuming that $f$ is continuous on the closure,"" but I'm not having much luck with either a proof or a simple counterexample. If the statement is false, what if we further assume that $f$ is periodic with period 1? That is, if $f$ is a bounded holomorphic function on the upper half plane satisfying $f(z+1) = f(z)$?","Let $f$ be a bounded holomorphic function on the upper-half plane. Is it true that such a map always admits an (unique) extension to real line, so that $f$ is continuous on the closure of the upper half plane?  I feel like the answer to this question is ""no,"" since a lot of theorems concerning bounded holomorphic functions start out with, ""Assuming that $f$ is continuous on the closure,"" but I'm not having much luck with either a proof or a simple counterexample. If the statement is false, what if we further assume that $f$ is periodic with period 1? That is, if $f$ is a bounded holomorphic function on the upper half plane satisfying $f(z+1) = f(z)$?",,['complex-analysis']
95,Maximum value of a complex polynomial on the unit disk,Maximum value of a complex polynomial on the unit disk,,"The polynomial is $p(z)=\sum^n_{k=0} a_kz^k$. And I want to prove the following inequality on the unit disk$$\max_{B_1(0)}|p(z)|\geq |a_n|+|a_0|$$ By the maximum modulus principle, the maximum must be on the unit circle and greater than $|a_0|$ by considering $p(0)$. However, I cannot make further conclusions from this, since any attempt of using the triangle inequality will result in the opposite direction of the wanted result. I have also seen a similar problem , although I can conclude $\max_{|z|=1}|p(z)|$is greater than any of the two on RHS, but since there is no relation of $\max_{k\in\{0,\ldots,n\}}|a_k|\geq|a_0|+|a_k|$, a tighter bound is needed. I also tried expanding it into trig functions, and consider the roots, but it didn't work as expected.","The polynomial is $p(z)=\sum^n_{k=0} a_kz^k$. And I want to prove the following inequality on the unit disk$$\max_{B_1(0)}|p(z)|\geq |a_n|+|a_0|$$ By the maximum modulus principle, the maximum must be on the unit circle and greater than $|a_0|$ by considering $p(0)$. However, I cannot make further conclusions from this, since any attempt of using the triangle inequality will result in the opposite direction of the wanted result. I have also seen a similar problem , although I can conclude $\max_{|z|=1}|p(z)|$is greater than any of the two on RHS, but since there is no relation of $\max_{k\in\{0,\ldots,n\}}|a_k|\geq|a_0|+|a_k|$, a tighter bound is needed. I also tried expanding it into trig functions, and consider the roots, but it didn't work as expected.",,"['complex-analysis', 'maximum-principle']"
96,Can a function be analytic and satisfy $f\left(\frac 1 n\right) =\frac{1}{\log{n}}.$?,Can a function be analytic and satisfy ?,f\left(\frac 1 n\right) =\frac{1}{\log{n}}.,"Let $\Omega = \{z\in\mathbb{C}:\,|z|<2\}$. Prove or disprove that there exists an analytic function $f:\Omega\rightarrow\mathbb{C}$ such that for $n\geq2$: $$f\left(\frac 1 n\right) =\frac{1}{\log{n}}.$$ Usually this question would fall under the uniqueness theorem for analytic functions; an example where the uniqueness theorem works is in disproving the existence of an analytic $f$ satisfying $$f\left(\frac 1 n\right) =\frac{(-1)^n}{n^2}.$$ But in the case of the $\log$, I think the issue of existence is more ""substantial"". Is it true that if such $f$ were to exist, then $f(z) = -\frac{1}{\log z}$? And if so, what is the main reason this function cannot be analytic in $\Omega$? Thanks in Advance!","Let $\Omega = \{z\in\mathbb{C}:\,|z|<2\}$. Prove or disprove that there exists an analytic function $f:\Omega\rightarrow\mathbb{C}$ such that for $n\geq2$: $$f\left(\frac 1 n\right) =\frac{1}{\log{n}}.$$ Usually this question would fall under the uniqueness theorem for analytic functions; an example where the uniqueness theorem works is in disproving the existence of an analytic $f$ satisfying $$f\left(\frac 1 n\right) =\frac{(-1)^n}{n^2}.$$ But in the case of the $\log$, I think the issue of existence is more ""substantial"". Is it true that if such $f$ were to exist, then $f(z) = -\frac{1}{\log z}$? And if so, what is the main reason this function cannot be analytic in $\Omega$? Thanks in Advance!",,"['complex-analysis', 'complex-numbers']"
97,What's the intuition for extending $\mathbb{C}$ to $\mathbb{H}$?,What's the intuition for extending  to ?,\mathbb{C} \mathbb{H},"It seems to me that there is a clear, intuitive reason for extending the real number system to the complex number system. Namely, some polynomial equations that have no solutions in $\mathbb{R}$ become soluble in $\mathbb{C}$. When we do this, we lose almost none of the nice algebraic properties of the reals, and pick up some nice new ones along the way (e.g. fundamental theorem of algebra). However, I cannot see such an intuitive reason for similarly extending $\mathbb{C}$ to $\mathbb{H}$, other than ""because we can."" In so doing, we actually lose the important property of commutative multiplication. So precisely what problems do we solve by moving from $\mathbb{C}$ to $\mathbb{H}$? I am of course aware of the quaternions' numerous applications to 3D geometry; what I am really interested in here are the analytic properties they provide. Does quaternionic analysis offer results comparable to those of complex analysis?","It seems to me that there is a clear, intuitive reason for extending the real number system to the complex number system. Namely, some polynomial equations that have no solutions in $\mathbb{R}$ become soluble in $\mathbb{C}$. When we do this, we lose almost none of the nice algebraic properties of the reals, and pick up some nice new ones along the way (e.g. fundamental theorem of algebra). However, I cannot see such an intuitive reason for similarly extending $\mathbb{C}$ to $\mathbb{H}$, other than ""because we can."" In so doing, we actually lose the important property of commutative multiplication. So precisely what problems do we solve by moving from $\mathbb{C}$ to $\mathbb{H}$? I am of course aware of the quaternions' numerous applications to 3D geometry; what I am really interested in here are the analytic properties they provide. Does quaternionic analysis offer results comparable to those of complex analysis?",,"['complex-analysis', 'soft-question', 'quaternions']"
98,how to understand the tensor product canonical line bundle $\otimes$ dual bundle,how to understand the tensor product canonical line bundle  dual bundle,\otimes,"Suppose we have a Riemann surface $M$ together with a holomorphic vector bundle $E \to M$ of rank n. let $K$ denote the canonical line bundle and let $E^*$ denote the dual bundle I am trying to understand the tensor product $K \otimes E^*$. I have lots of trouble, because I need to do this on my own and my background in differential geometry and multilinear algebra is not strong. I shall try to explain how I would describe $K \otimes E^*$ locally, if somebody could comment on what goes wrong that would be immensely helpful! The canonical bundle $K$ over a Riemann surface $M$ is the cotangent bundle, or the bundle of holomorphic $1$-forms. In local coordinates $(z,\overline{z})$ an element of $K$ can be written as   $$ \omega = \frac{i}{2} f\,dVol_h $$   where $f$ is a holomorphic function on $M$ (or at least defined locally) and $dVol_h = \frac{i}{2}\, h dz\wedge d\overline{z}$, the Volume element induced by the metric $h\,dz\,d\overline{z}$ on $M$ (here again $h$ is a holomorphic function on $M$). Question 1: Is this a correct way to describe the canonical bundle locally ? The dual bundle $E^*$ can be described locally given a choice of basis $\{e_1,\dots, e_n\}$ for the bundle $E$. We take the dual basis $\{\phi^1,\dots\phi^n\}$ (so $\phi_k (e_l) = \delta^k_l$) and write   $$  \sigma = \sum^n_{k = 1} g_k\,\phi_k $$   where the coeficients $g_k$ are holomorphic functions locally defined on $M$. Question 2: is this description sufficient ? i fear the locality of what I want to show is not emphasized enough but i am not sure how the local coordinates $(z,\overline{z})$ on a patch $V \subset M$ (say) should be mentioned here. do i need to invoke local coordinates on $E$, or is this done by specifying the basis? Therefore the bundle $K \otimes E^*$ consists of tensor fields which have local description   $$ \omega \otimes \sigma = \sum^n_{k = 1} (\frac{i}{2}\,h\,f\,g_k)\, dz \wedge d\overline{z} \otimes \phi_k $$ Question 3: does this formula makes sens ? I am ""very unsure"" here - I have a wedge product and a tensor symbol and don't know how to write this in a correct way. Below I attempt to understand such an object, maybe my last lines give away more of my misunderstandings: an element in $K \otimes E^*$ can be interpret as a map $TM \otimes E \to \mathbb{C}$. alternatively we can also think of it as something that can be integrated, that which would amount to a contraction of the tensor. Question 4: do these interpretations make sense ? how would I write them out as rigorous definitions? Many thanks for comments and help!!!","Suppose we have a Riemann surface $M$ together with a holomorphic vector bundle $E \to M$ of rank n. let $K$ denote the canonical line bundle and let $E^*$ denote the dual bundle I am trying to understand the tensor product $K \otimes E^*$. I have lots of trouble, because I need to do this on my own and my background in differential geometry and multilinear algebra is not strong. I shall try to explain how I would describe $K \otimes E^*$ locally, if somebody could comment on what goes wrong that would be immensely helpful! The canonical bundle $K$ over a Riemann surface $M$ is the cotangent bundle, or the bundle of holomorphic $1$-forms. In local coordinates $(z,\overline{z})$ an element of $K$ can be written as   $$ \omega = \frac{i}{2} f\,dVol_h $$   where $f$ is a holomorphic function on $M$ (or at least defined locally) and $dVol_h = \frac{i}{2}\, h dz\wedge d\overline{z}$, the Volume element induced by the metric $h\,dz\,d\overline{z}$ on $M$ (here again $h$ is a holomorphic function on $M$). Question 1: Is this a correct way to describe the canonical bundle locally ? The dual bundle $E^*$ can be described locally given a choice of basis $\{e_1,\dots, e_n\}$ for the bundle $E$. We take the dual basis $\{\phi^1,\dots\phi^n\}$ (so $\phi_k (e_l) = \delta^k_l$) and write   $$  \sigma = \sum^n_{k = 1} g_k\,\phi_k $$   where the coeficients $g_k$ are holomorphic functions locally defined on $M$. Question 2: is this description sufficient ? i fear the locality of what I want to show is not emphasized enough but i am not sure how the local coordinates $(z,\overline{z})$ on a patch $V \subset M$ (say) should be mentioned here. do i need to invoke local coordinates on $E$, or is this done by specifying the basis? Therefore the bundle $K \otimes E^*$ consists of tensor fields which have local description   $$ \omega \otimes \sigma = \sum^n_{k = 1} (\frac{i}{2}\,h\,f\,g_k)\, dz \wedge d\overline{z} \otimes \phi_k $$ Question 3: does this formula makes sens ? I am ""very unsure"" here - I have a wedge product and a tensor symbol and don't know how to write this in a correct way. Below I attempt to understand such an object, maybe my last lines give away more of my misunderstandings: an element in $K \otimes E^*$ can be interpret as a map $TM \otimes E \to \mathbb{C}$. alternatively we can also think of it as something that can be integrated, that which would amount to a contraction of the tensor. Question 4: do these interpretations make sense ? how would I write them out as rigorous definitions? Many thanks for comments and help!!!",,"['complex-analysis', 'differential-geometry', 'riemann-surfaces', 'multilinear-algebra', 'differential-forms']"
99,Why is $\sqrt{-2} \sqrt{-3} \neq \sqrt{6}$?,Why is ?,\sqrt{-2} \sqrt{-3} \neq \sqrt{6},Why is $\sqrt{-2} \cdot \sqrt{-3} \neq \sqrt{6}$? Are there other examples where regular arithmetic goes wrong for complex numbers?,Why is $\sqrt{-2} \cdot \sqrt{-3} \neq \sqrt{6}$? Are there other examples where regular arithmetic goes wrong for complex numbers?,,['complex-analysis']
