,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Alternative Almost Complex Structures,Alternative Almost Complex Structures,,"Let $V$ be a real vector space. An almost complex structure on $V$ is a map $J : V \to V$ such that $J^2 = -\mathrm{id}_V$ . An almost complex structure gives $V$ the structure of a complex vector space by defining $(a+bi)v = av + bJ(v)$ . The idea behind this definition is that $J$ represents multiplication by $i$ . What if you instead consider multiplication by other complex numbers? Let $n > 2$ be a fixed positive integer and let $J : V \to V$ be such that $J^n = -\mathrm{id}_V$ which I will refer to as an alternative almost complex structure . One can regard $J$ as trying to capture multiplication by $\zeta = \exp\left(\frac{\pi i}{n}\right)$ . In particular, as $\{1, \zeta\}$ is linearly independent over $\mathbb{R}$ , it is a basis for $\mathbb{C}$ as a real vector space. One can then endow $V$ with a complex structure by defining $(a + b\zeta)v = av + bJ(v)$ . Do alternative almost complex structures give rise to the same results as the standard almost complex structures? In particular, in the case where we extend them to bundle endomorphisms of the tangent bundle of a manifold. If not, what fails? If so, is dealing with almost complex structures easier than dealing with alternative almost complex structures?","Let be a real vector space. An almost complex structure on is a map such that . An almost complex structure gives the structure of a complex vector space by defining . The idea behind this definition is that represents multiplication by . What if you instead consider multiplication by other complex numbers? Let be a fixed positive integer and let be such that which I will refer to as an alternative almost complex structure . One can regard as trying to capture multiplication by . In particular, as is linearly independent over , it is a basis for as a real vector space. One can then endow with a complex structure by defining . Do alternative almost complex structures give rise to the same results as the standard almost complex structures? In particular, in the case where we extend them to bundle endomorphisms of the tangent bundle of a manifold. If not, what fails? If so, is dealing with almost complex structures easier than dealing with alternative almost complex structures?","V V J : V \to V J^2 = -\mathrm{id}_V V (a+bi)v = av + bJ(v) J i n > 2 J : V \to V J^n = -\mathrm{id}_V J \zeta = \exp\left(\frac{\pi i}{n}\right) \{1, \zeta\} \mathbb{R} \mathbb{C} V (a + b\zeta)v = av + bJ(v)","['linear-algebra', 'differential-geometry', 'vector-spaces', 'complex-geometry', 'almost-complex']"
1,Find a basis for a solution set of a linear system,Find a basis for a solution set of a linear system,,"I'm trying hard with this exercise, but it is breaking my back. Find a basis for the solution set of the given homogeneous linear system $3x_1+x_2+x_3=0$ $6x_1+2x_2+2x_3=0$ $-9x_1-3x_2-3x_3=0$ I do what I know I need to do. First I get the solution set of the system by reducing like this: $\begin{pmatrix} 3 & 1 & 1 \\ 6 & 2 & 2 \\ -9 & -3 & -3 \end{pmatrix} \leadsto \begin{pmatrix} 3 & 1 & 1 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix} \leadsto\begin{pmatrix} 1 & 1/3 & 1/3 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}$ So I know $\vec x = \begin{bmatrix} x_1\\ x_2\\ x_3\end{bmatrix} = \begin{bmatrix} 1-\frac{1}{3}r-\frac{1}{3}s\\ r\\ s\end{bmatrix}$ That being the general solution. Now, giving the values for $r$ and $s$ according to the standard vectors $i$, $j$ $\vec x = \begin{bmatrix} x_1\\ x_2\\ x_3\end{bmatrix} = \begin{bmatrix} 1-\frac{1}{3}r-\frac{1}{3}s\\ r\\ s\end{bmatrix} = r \begin{bmatrix} \frac{2}{3}\\ 1\\ 0\end{bmatrix} + s\begin{bmatrix} \frac{2}{3}\\ 0\\ 1\end{bmatrix}$ From my results, the basis will be: $ ( \begin{bmatrix} \frac{2}{3}\\ 1\\ 0\end{bmatrix}, \begin{bmatrix} \frac{2}{3}\\ 0\\ 1\end{bmatrix})$ But instead, the book answer (I'm self-studying )is: $ ( \begin{bmatrix} -1\\ 3\\ 0\end{bmatrix}, \begin{bmatrix} -1\\ 0\\ 3\end{bmatrix})$ Any idea on what I'm doing wrong? Thank you :)","I'm trying hard with this exercise, but it is breaking my back. Find a basis for the solution set of the given homogeneous linear system $3x_1+x_2+x_3=0$ $6x_1+2x_2+2x_3=0$ $-9x_1-3x_2-3x_3=0$ I do what I know I need to do. First I get the solution set of the system by reducing like this: $\begin{pmatrix} 3 & 1 & 1 \\ 6 & 2 & 2 \\ -9 & -3 & -3 \end{pmatrix} \leadsto \begin{pmatrix} 3 & 1 & 1 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix} \leadsto\begin{pmatrix} 1 & 1/3 & 1/3 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}$ So I know $\vec x = \begin{bmatrix} x_1\\ x_2\\ x_3\end{bmatrix} = \begin{bmatrix} 1-\frac{1}{3}r-\frac{1}{3}s\\ r\\ s\end{bmatrix}$ That being the general solution. Now, giving the values for $r$ and $s$ according to the standard vectors $i$, $j$ $\vec x = \begin{bmatrix} x_1\\ x_2\\ x_3\end{bmatrix} = \begin{bmatrix} 1-\frac{1}{3}r-\frac{1}{3}s\\ r\\ s\end{bmatrix} = r \begin{bmatrix} \frac{2}{3}\\ 1\\ 0\end{bmatrix} + s\begin{bmatrix} \frac{2}{3}\\ 0\\ 1\end{bmatrix}$ From my results, the basis will be: $ ( \begin{bmatrix} \frac{2}{3}\\ 1\\ 0\end{bmatrix}, \begin{bmatrix} \frac{2}{3}\\ 0\\ 1\end{bmatrix})$ But instead, the book answer (I'm self-studying )is: $ ( \begin{bmatrix} -1\\ 3\\ 0\end{bmatrix}, \begin{bmatrix} -1\\ 0\\ 3\end{bmatrix})$ Any idea on what I'm doing wrong? Thank you :)",,"['linear-algebra', 'systems-of-equations']"
2,"Connectedness of the orthogonal subgroup $O^+_+(k,l)$",Connectedness of the orthogonal subgroup,"O^+_+(k,l)","Let $O(k,l)$ be the orthogonal group associated to the quadratic form $q$ on $\mathbb{R}^{k+l}$ with signature $(k,l)$. Let $O^+_+(k,l)$ be the connected component of the identity, i.e. the connected components of the elements in $O(k,l)$ which maintein the orientation on each maximal positive subspace $W\subset \mathbb{R}^{k+l}$ and $W^\perp$. I want to prove that $O^+_+(k,l)$ is connected. I've read the references of the wiki page http://en.wikipedia.org/wiki/Indefinite_orthogonal_group but no one seems to really prove it. So i thought that maybe we can apply the same argument of the proof of the connectedness of $SO(k)$. I consider the hyperboloid $q(x)=1$ for $x\in \mathbb{R}^{k+l}$ and I have to prove that $O^+_+(k,l)$ acts transitively on one connected component of the hyperboloid (this seems legit to me, although I'm not sure about how many connected components the hyperboloid has). Then the stabilizer of a point is of the form  \begin{pmatrix}   1 & 0 & \cdots & 0 \\   0 & a_{1,1} & \cdots & a_{1,k+l-1} \\   \vdots  & \vdots  & \ddots & \vdots  \\   0 & a_{k+l-1,1} & \cdots & a_{k+l-1,k+l-1}  \end{pmatrix} where  \begin{pmatrix}   a_{1,1} & \cdots & a_{1,k+l-1} \\   \vdots   & \ddots & \vdots  \\   a_{k+l-1,1} & \cdots & a_{k+l-1,k+l-1}  \end{pmatrix} is in $O^+_+(k-1,l)$ which is connected by induction hypothesis. And so also $O^+_+(k,l)$ is connected. But i don't know how to find out how many connected components the hyperboloid has and how to prove that $O^+_+$ acts transitively on one of them.. Do you have any ideas?","Let $O(k,l)$ be the orthogonal group associated to the quadratic form $q$ on $\mathbb{R}^{k+l}$ with signature $(k,l)$. Let $O^+_+(k,l)$ be the connected component of the identity, i.e. the connected components of the elements in $O(k,l)$ which maintein the orientation on each maximal positive subspace $W\subset \mathbb{R}^{k+l}$ and $W^\perp$. I want to prove that $O^+_+(k,l)$ is connected. I've read the references of the wiki page http://en.wikipedia.org/wiki/Indefinite_orthogonal_group but no one seems to really prove it. So i thought that maybe we can apply the same argument of the proof of the connectedness of $SO(k)$. I consider the hyperboloid $q(x)=1$ for $x\in \mathbb{R}^{k+l}$ and I have to prove that $O^+_+(k,l)$ acts transitively on one connected component of the hyperboloid (this seems legit to me, although I'm not sure about how many connected components the hyperboloid has). Then the stabilizer of a point is of the form  \begin{pmatrix}   1 & 0 & \cdots & 0 \\   0 & a_{1,1} & \cdots & a_{1,k+l-1} \\   \vdots  & \vdots  & \ddots & \vdots  \\   0 & a_{k+l-1,1} & \cdots & a_{k+l-1,k+l-1}  \end{pmatrix} where  \begin{pmatrix}   a_{1,1} & \cdots & a_{1,k+l-1} \\   \vdots   & \ddots & \vdots  \\   a_{k+l-1,1} & \cdots & a_{k+l-1,k+l-1}  \end{pmatrix} is in $O^+_+(k-1,l)$ which is connected by induction hypothesis. And so also $O^+_+(k,l)$ is connected. But i don't know how to find out how many connected components the hyperboloid has and how to prove that $O^+_+$ acts transitively on one of them.. Do you have any ideas?",,"['linear-algebra', 'algebraic-topology', 'lie-groups']"
3,Eigenvalue decomposition of block covariance matrix for Canonical Correlation Analysis (CCA),Eigenvalue decomposition of block covariance matrix for Canonical Correlation Analysis (CCA),,Edited: My question is related to a tutorial I was reading. The covariance matrix is a block matrix where $C_{xx}$ and $C_{yy}$ are within-set covariance matrices and $C_{xy} = C_{yx}^T$ are between-sets covariance matrices. $$   \left[\begin{array}{r r}     C_{xx} & C_{xy}\\     C_{yx} & C_{yy}   \end{array}\right] $$ The tutorial says that the canonical correlations between $x$ and $y$ can be found by solving the eigenvalue equations $$   C_{xx}^{-1}C_{xy}C_{yy}^{-1}C_{yx} \hat w_x = \rho^2 \hat w_x \\   C_{yy}^{-1}C_{yx}C_{xx}^{-1}C_{xy} \hat w_y = \rho^2 \hat w_y  $$ where the eigenvalues  are the squared canonical correlations and the eigenvectors  and  are the normalized canonical correlation basis vectors. What I do not understand is how the eigenvalue equations are found by using the covariance matrix? Can someone please explain how we get those sets of equations? Thanks.,Edited: My question is related to a tutorial I was reading. The covariance matrix is a block matrix where and are within-set covariance matrices and are between-sets covariance matrices. The tutorial says that the canonical correlations between and can be found by solving the eigenvalue equations where the eigenvalues  are the squared canonical correlations and the eigenvectors  and  are the normalized canonical correlation basis vectors. What I do not understand is how the eigenvalue equations are found by using the covariance matrix? Can someone please explain how we get those sets of equations? Thanks.,"C_{xx} C_{yy} C_{xy} = C_{yx}^T 
  \left[\begin{array}{r r}
    C_{xx} & C_{xy}\\
    C_{yx} & C_{yy}
  \end{array}\right]
 x y 
  C_{xx}^{-1}C_{xy}C_{yy}^{-1}C_{yx} \hat w_x = \rho^2 \hat w_x \\
  C_{yy}^{-1}C_{yx}C_{xx}^{-1}C_{xy} \hat w_y = \rho^2 \hat w_y 
","['linear-algebra', 'statistics', 'eigenvalues-eigenvectors', 'correlation', 'block-matrices']"
4,How to calculate the Mahalanobis distance,How to calculate the Mahalanobis distance,,"I am really stuck on calculating the Mahalanobis distance. I have two vectors, and I want to find the Mahalanobis distance between them. Wikipedia gives me the formula of $$     d\left(\vec{x}, \vec{y}\right) = \sqrt{\left(\vec{x}-\vec{y}\right)^\top S^{-1} \left(\vec{x}-\vec{y}\right) } $$ Suppose my $\vec{y}$ is $(1,9,10)$ and my $\vec{x}$ is $(17, 8, 26)$ (These are just random), well $\vec{x}-\vec{y}$ is really easy to calculate (it would be $(16, -1, 16)$), but how in the world do I calculate $S^{-1}$. I understand that it is a covariance matrix , but that is about it. I see an ""X"" everywhere, but that only seems like one vector to me. What do I do with the Y vector? Can someone briefly walk me through this one example? Or any other example involving two vectors.","I am really stuck on calculating the Mahalanobis distance. I have two vectors, and I want to find the Mahalanobis distance between them. Wikipedia gives me the formula of $$     d\left(\vec{x}, \vec{y}\right) = \sqrt{\left(\vec{x}-\vec{y}\right)^\top S^{-1} \left(\vec{x}-\vec{y}\right) } $$ Suppose my $\vec{y}$ is $(1,9,10)$ and my $\vec{x}$ is $(17, 8, 26)$ (These are just random), well $\vec{x}-\vec{y}$ is really easy to calculate (it would be $(16, -1, 16)$), but how in the world do I calculate $S^{-1}$. I understand that it is a covariance matrix , but that is about it. I see an ""X"" everywhere, but that only seems like one vector to me. What do I do with the Y vector? Can someone briefly walk me through this one example? Or any other example involving two vectors.",,"['linear-algebra', 'probability', 'statistics']"
5,Prove tensor product of two multilinear forms is commutative only if one of them is zero,Prove tensor product of two multilinear forms is commutative only if one of them is zero,,Prove $L \otimes M = M \otimes L$ only if either $L=0$ or $M=0$ I saw this statement on Linear Algebra (2ed) written by Hoffman and Kunze. I can't figure out how to prove it. The multilinear forms $L$ (and $M$) are from $V^r$ (and $V^s$) into $K$ where $K$ is a commutative ring with identity and $V$ is a $K$-module. I think it is reasonable to exclude $L=M$. Thanks. Added by the crowd. Here's the relevant excerpt.,Prove $L \otimes M = M \otimes L$ only if either $L=0$ or $M=0$ I saw this statement on Linear Algebra (2ed) written by Hoffman and Kunze. I can't figure out how to prove it. The multilinear forms $L$ (and $M$) are from $V^r$ (and $V^s$) into $K$ where $K$ is a commutative ring with identity and $V$ is a $K$-module. I think it is reasonable to exclude $L=M$. Thanks. Added by the crowd. Here's the relevant excerpt.,,"['linear-algebra', 'abstract-algebra', 'tensor-products']"
6,Multiplicity of eigenvalues,Multiplicity of eigenvalues,,"Suppose $A$ is an $n\times n$ complex matrix. How to show the following two properties If $\lambda$ is an eigenvalue of $A\bar{A}$, so is $\bar{\lambda}$. Here $\bar{A}$ means the entrywise conjugate of $A$. The algebraic multiplicity of negative eigenvalues (if any) of $A\bar{A}$ are even.","Suppose $A$ is an $n\times n$ complex matrix. How to show the following two properties If $\lambda$ is an eigenvalue of $A\bar{A}$, so is $\bar{\lambda}$. Here $\bar{A}$ means the entrywise conjugate of $A$. The algebraic multiplicity of negative eigenvalues (if any) of $A\bar{A}$ are even.",,['linear-algebra']
7,Ideals in the ring of endomorphisms of a vector space of uncountably infinite dimension.,Ideals in the ring of endomorphisms of a vector space of uncountably infinite dimension.,,"I know that if $V$ is a vector space over a field $k,$ then $\operatorname{End}(V)$ has no non-trivial ideals if $\dim V<\infty;$ $\operatorname{End}(V)$ has exactly one non-trivial ideal if $\dim V=\aleph_0.$ Do we know how many ideals $\operatorname{End}(V)$ can have when $\dim V>\aleph_0?$ Can we describe them?","I know that if $V$ is a vector space over a field $k,$ then $\operatorname{End}(V)$ has no non-trivial ideals if $\dim V<\infty;$ $\operatorname{End}(V)$ has exactly one non-trivial ideal if $\dim V=\aleph_0.$ Do we know how many ideals $\operatorname{End}(V)$ can have when $\dim V>\aleph_0?$ Can we describe them?",,"['linear-algebra', 'ring-theory']"
8,Decomposing a discrete signal into a sum of rectangle functions,Decomposing a discrete signal into a sum of rectangle functions,,"Hello math@stackexchange community ! I have a simple question that seems to have a non trivial answer. Given a discrete one dimensional signal $w(x)$ defined in a finite range, and the boxcar (rectangular) function $r(x)$ $$r(x)=\begin{cases} 1 & \mbox{if }0\leq x \leq 1; \\ 0 & \mbox{elsewhere} \end{cases}$$ I would like to find the coefficients $a_i,\ b_i,\ c_i $ of the sum $$w' = \sum_{i=0}^{N}\ { a_i \cdot r\left(\frac{x}{b_i} - c_i\right)}$$ (""sum of $N$ rectangles in any range and of any height"") such as $\sum_i\ \left| w_i - w_i'\right|$ is minimized (for a given $N$). This problem seems related to: Discrete wavelet transform $l_1$ regularized solution of an overdetermined linear system Maximum subarray problem However, to my understanding it does not fit any of these cases: $r(x)$ is not a wavelet basis, the problem cannot be solved (practically) as a linear system because the (finite) set of $a_i,\ b_i,\ c_i $ values is too large to compute explicitly (length of $w$ in the order of $10^4$), Since $a_i$ is undefined, it does not fit as a maximum subarray problem. Right now I have an approximate solution (iteratively solving the problem via maximum subarray formulation by brute force exploring a subset of possible $a_i$ values), however the idea of ""decomposing a signal as a sum of rectangles"" seems general enough to think that someone has already addressed it in the past. Do any of you have a suggestion on how to tackle this problem ? Has it already been solved in the past, by a method I am not aware of ? Thank you very much for your answers.","Hello math@stackexchange community ! I have a simple question that seems to have a non trivial answer. Given a discrete one dimensional signal $w(x)$ defined in a finite range, and the boxcar (rectangular) function $r(x)$ $$r(x)=\begin{cases} 1 & \mbox{if }0\leq x \leq 1; \\ 0 & \mbox{elsewhere} \end{cases}$$ I would like to find the coefficients $a_i,\ b_i,\ c_i $ of the sum $$w' = \sum_{i=0}^{N}\ { a_i \cdot r\left(\frac{x}{b_i} - c_i\right)}$$ (""sum of $N$ rectangles in any range and of any height"") such as $\sum_i\ \left| w_i - w_i'\right|$ is minimized (for a given $N$). This problem seems related to: Discrete wavelet transform $l_1$ regularized solution of an overdetermined linear system Maximum subarray problem However, to my understanding it does not fit any of these cases: $r(x)$ is not a wavelet basis, the problem cannot be solved (practically) as a linear system because the (finite) set of $a_i,\ b_i,\ c_i $ values is too large to compute explicitly (length of $w$ in the order of $10^4$), Since $a_i$ is undefined, it does not fit as a maximum subarray problem. Right now I have an approximate solution (iteratively solving the problem via maximum subarray formulation by brute force exploring a subset of possible $a_i$ values), however the idea of ""decomposing a signal as a sum of rectangles"" seems general enough to think that someone has already addressed it in the past. Do any of you have a suggestion on how to tackle this problem ? Has it already been solved in the past, by a method I am not aware of ? Thank you very much for your answers.",,"['linear-algebra', 'fourier-analysis', 'optimization', 'discrete-mathematics', 'wavelets']"
9,"Can I build a program that will tell me if a real world data set looks linear, logarithmic, exponential etc?","Can I build a program that will tell me if a real world data set looks linear, logarithmic, exponential etc?",,"I have a bunch of real world data sets and from manually plotting some of the data in graphs, I've discovered some data sets look pretty much logarithmic and some look linear, or exponential (and some look like a mess :). I've been reading up on curve fitting / data fitting on wikipedia and if I understand it correctly (which I seriously doubt) I can calculate a curve of best fit using least squares calculations, but I have to determine if I want to have the curve fit a logarithm, linear or exponential (etc) pattern first. What I would really like to do is to pass a data set into a function (I'm a programmer with poor math skills) and have that return something like ""this data set looks more linear than logarithmic"" or ""this looks exponential"". My question is: is that even possible, without a human looking at a graph and recognizing the pattern ? My guess is: yes. But before I invest a ton of time in figuring out how to program this, I just want to make sure I'm not barking up the wrong tree and confirm this with you guys if possible. Sorry if this is a dumb question, but just to be clear, I'm not looking for a how-to answer, just a simple yes or no will do, however if you have suggestions on how to tackle the problem, that would be awesome of course.","I have a bunch of real world data sets and from manually plotting some of the data in graphs, I've discovered some data sets look pretty much logarithmic and some look linear, or exponential (and some look like a mess :). I've been reading up on curve fitting / data fitting on wikipedia and if I understand it correctly (which I seriously doubt) I can calculate a curve of best fit using least squares calculations, but I have to determine if I want to have the curve fit a logarithm, linear or exponential (etc) pattern first. What I would really like to do is to pass a data set into a function (I'm a programmer with poor math skills) and have that return something like ""this data set looks more linear than logarithmic"" or ""this looks exponential"". My question is: is that even possible, without a human looking at a graph and recognizing the pattern ? My guess is: yes. But before I invest a ton of time in figuring out how to program this, I just want to make sure I'm not barking up the wrong tree and confirm this with you guys if possible. Sorry if this is a dumb question, but just to be clear, I'm not looking for a how-to answer, just a simple yes or no will do, however if you have suggestions on how to tackle the problem, that would be awesome of course.",,"['linear-algebra', 'statistics', 'logarithms', 'regression', 'exponentiation']"
10,Geometric intuition for the Householder transformation,Geometric intuition for the Householder transformation,,"I am studying QR decomposition. Could you explain the geometric intuition for what the Householder transformation does in that context, and why it's sometimes referred to as the Householder reflection .","I am studying QR decomposition. Could you explain the geometric intuition for what the Householder transformation does in that context, and why it's sometimes referred to as the Householder reflection .",,"['linear-algebra', 'matrices', 'reflection']"
11,Bounding $\|A^r - B^r\|$ for $r \geq 1$.,Bounding  for .,\|A^r - B^r\| r \geq 1,"Let $A$ , $B$ be positive semi-definite self-adjoint matrices. Is it true that, for $r \geq 1$ , $r \in \mathbb{R}$ , $$ \|A^r - B^r\| \leq rc^{r-1}\|A - B\| $$ where $\|\cdot\|$ denotes the operator norm and $c = \max\{\|A\|, \|B\|\}$ . When $r \in [0,1]$ , using that $f:x \to x^r$ is operator monotone we get the classical inequality $$ \|A^r - B^r \| \leq \|A - B \|^r. $$ And if $A \geq \alpha I$ and $B \geq \alpha I$ with $\alpha >0$ , we have (see Matrix Analysis, Rajendra Bhatia) $$ \|A^r - B^r\| \leq r\alpha^{r-1}\|A - B\|. $$ However when $r \geq 1$ we loose the operator monotonicity and I then struggle to prove the first inequality.","Let , be positive semi-definite self-adjoint matrices. Is it true that, for , , where denotes the operator norm and . When , using that is operator monotone we get the classical inequality And if and with , we have (see Matrix Analysis, Rajendra Bhatia) However when we loose the operator monotonicity and I then struggle to prove the first inequality.","A B r \geq 1 r \in \mathbb{R} 
\|A^r - B^r\| \leq rc^{r-1}\|A - B\|
 \|\cdot\| c = \max\{\|A\|, \|B\|\} r \in [0,1] f:x \to x^r 
\|A^r - B^r \| \leq \|A - B \|^r.
 A \geq \alpha I B \geq \alpha I \alpha >0 
\|A^r - B^r\| \leq r\alpha^{r-1}\|A - B\|.
 r \geq 1","['linear-algebra', 'operator-theory', 'monotone-operator-theory']"
12,Vector Component Transformation Matrix in Shilov's Linear Algebra,Vector Component Transformation Matrix in Shilov's Linear Algebra,,"I am working through Georgi Shilov's Linear Algebra, and I am having trouble understanding the vector component transformation matrix definition he gives in section 5.31. I will describe this definition below. Let $e_1, e_2, \dots, e_n$ and $f_1, f_2, \dots, f_n$ be two bases in a vector field of dimension $n$ such that for some quantities $p^{(j)}_i$ we have $$f_j = p^{(j)}_1 e_1 + p^{(j)}_2 e_2 + \dots + p^{(j)}_n e_n$$ Shilov now defines the matrix of the transformation from basis $\{e\}$ to basis $\{f\}$ as $$P =  \begin{bmatrix}  p^{(1)}_1 & p^{(2)}_1 & \dots & p^{(n)}_1 \\ p^{(1)}_2 & p^{(2)}_2 & \dots & p^{(n)}_2 \\ \vdots & \vdots & \ddots & \vdots \\ p^{(1)}_n & p^{(2)}_n & \dots & p^{(n)}_b \end{bmatrix} $$ So, $P$ is the matrix with components of $f_i$ in terms of the basis ${e}$ as columns. Now, suppose we have some vector $x = \xi_1 e_1 + \xi_2 e_2 + \dots + \xi_n e_n = \eta_1 f_1 + \dots + \eta_n f_n$ . Then, the author claims that the matrix describing the transformation from the components $\xi_1, \dots, \xi_n$ to the components $\eta_1, \dots , \eta_n$ is $$S = (P^{-1})^T$$ In my understanding, the ""matrix describing the transformation from the components $\xi_1, \dots, \xi_n$ to the components $\eta_1, \dots , \eta_n$ "" means that $$S\begin{bmatrix} \xi_1 \\ \xi_2 \\ \vdots \\ \xi_n \end{bmatrix} = \begin{bmatrix} \eta_1 \\ \eta_2 \\ \vdots \\ \eta_n \end{bmatrix}$$ However, that doesn't seem to be the case. Consider an example where $e_1 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, e_2 = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, e_3 = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$ and $f_1 = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}, f_2 = \begin{bmatrix} 0 \\ 1 \\ 1 \end{bmatrix}, f_3 = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$ . Then, the matrix of the transformation from $\{e\}$ to $\{f\}$ and the respective component transformation matrix is $$P = \begin{bmatrix} 1 & 0 & 0 \\ 1 & 1 & 0 \\ 1 & 1 & 1 \end{bmatrix} \quad S = \begin{bmatrix} 1 & -1 & 0 \\ 0 & 1 & -1 \\ 0 & 0 & 1 \end{bmatrix}^T = \begin{bmatrix} 1 & 0 & 0 \\ -1 & 1 & 0 \\ 0 & -1 & 1 \end{bmatrix}$$ However, if we try to use it to transform the vector $[5, 1, 2]_{\{e\}} = [5, -4, 1]_{\{f\}}$ we get $$\begin{bmatrix} 1 & 0 & 0 \\ -1 & 1 & 0 \\ 0 & -1 & 1 \end{bmatrix}\begin{bmatrix} 5 \\ 1 \\ 2 \end{bmatrix} = \begin{bmatrix} 4 \\ -1 \\ 2 \end{bmatrix}$$ Which is not what I expected. However, if we instead take $S = P^{-1}$ , it seems to work out: $$\begin{bmatrix} 1 & -1 & 0 \\ 0 & 1 & -1 \\ 0 & 0 & 1 \end{bmatrix}\begin{bmatrix} 5 \\ 1 \\ 2 \end{bmatrix} = \begin{bmatrix} 5 \\ -4 \\ 1 \end{bmatrix}$$ So, am I misunderstanding what the component transformation matrix is supposed to do, or did I construct $P$ incorrectly? I did my best to write the definitions as they are written in the text, however, it is very possible I just mixed up my indices.","I am working through Georgi Shilov's Linear Algebra, and I am having trouble understanding the vector component transformation matrix definition he gives in section 5.31. I will describe this definition below. Let and be two bases in a vector field of dimension such that for some quantities we have Shilov now defines the matrix of the transformation from basis to basis as So, is the matrix with components of in terms of the basis as columns. Now, suppose we have some vector . Then, the author claims that the matrix describing the transformation from the components to the components is In my understanding, the ""matrix describing the transformation from the components to the components "" means that However, that doesn't seem to be the case. Consider an example where and . Then, the matrix of the transformation from to and the respective component transformation matrix is However, if we try to use it to transform the vector we get Which is not what I expected. However, if we instead take , it seems to work out: So, am I misunderstanding what the component transformation matrix is supposed to do, or did I construct incorrectly? I did my best to write the definitions as they are written in the text, however, it is very possible I just mixed up my indices.","e_1, e_2, \dots, e_n f_1, f_2, \dots, f_n n p^{(j)}_i f_j = p^{(j)}_1 e_1 + p^{(j)}_2 e_2 + \dots + p^{(j)}_n e_n \{e\} \{f\} P = 
\begin{bmatrix} 
p^{(1)}_1 & p^{(2)}_1 & \dots & p^{(n)}_1 \\
p^{(1)}_2 & p^{(2)}_2 & \dots & p^{(n)}_2 \\
\vdots & \vdots & \ddots & \vdots \\
p^{(1)}_n & p^{(2)}_n & \dots & p^{(n)}_b
\end{bmatrix}
 P f_i {e} x = \xi_1 e_1 + \xi_2 e_2 + \dots + \xi_n e_n = \eta_1 f_1 + \dots + \eta_n f_n \xi_1, \dots, \xi_n \eta_1, \dots , \eta_n S = (P^{-1})^T \xi_1, \dots, \xi_n \eta_1, \dots , \eta_n S\begin{bmatrix} \xi_1 \\ \xi_2 \\ \vdots \\ \xi_n \end{bmatrix} = \begin{bmatrix} \eta_1 \\ \eta_2 \\ \vdots \\ \eta_n \end{bmatrix} e_1 = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, e_2 = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, e_3 = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} f_1 = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}, f_2 = \begin{bmatrix} 0 \\ 1 \\ 1 \end{bmatrix}, f_3 = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} \{e\} \{f\} P = \begin{bmatrix} 1 & 0 & 0 \\ 1 & 1 & 0 \\ 1 & 1 & 1 \end{bmatrix} \quad S = \begin{bmatrix} 1 & -1 & 0 \\ 0 & 1 & -1 \\ 0 & 0 & 1 \end{bmatrix}^T = \begin{bmatrix} 1 & 0 & 0 \\ -1 & 1 & 0 \\ 0 & -1 & 1 \end{bmatrix} [5, 1, 2]_{\{e\}} = [5, -4, 1]_{\{f\}} \begin{bmatrix} 1 & 0 & 0 \\ -1 & 1 & 0 \\ 0 & -1 & 1 \end{bmatrix}\begin{bmatrix} 5 \\ 1 \\ 2 \end{bmatrix} = \begin{bmatrix} 4 \\ -1 \\ 2 \end{bmatrix} S = P^{-1} \begin{bmatrix} 1 & -1 & 0 \\ 0 & 1 & -1 \\ 0 & 0 & 1 \end{bmatrix}\begin{bmatrix} 5 \\ 1 \\ 2 \end{bmatrix} = \begin{bmatrix} 5 \\ -4 \\ 1 \end{bmatrix} P","['linear-algebra', 'matrices', 'vector-spaces', 'change-of-basis']"
13,When does a matrix have a positive eigenvector?,When does a matrix have a positive eigenvector?,,"The generalised problem is as follows: Is there a condition on a symmetric positive-semi-definite matrix $A$ that ensures that it has a positive eigenvector, i.e. an eigenvector $Av = \lambda v$ such that $ v_i > 0$ for all $i$ ? Or, indeed, even a weakly positive eigenvector, i.e. an eigenvector $Av = \lambda v$ such that $ v_i \geq 0$ for all $i$ ? I am aware of the Perron-Frobenius theorem, but the assumptions for that theorem are too strong. The problem I actually want to solve is more specific, in case the extra information helps. If $B$ is the incidence matrix of a directed graph $G$ , then $B$ can be viewed as the linear transformation whose input is vertex weightings and whose output is the corresponding edge weightings obtained by the vertex weight differences. Then the matrix that I want to understand is $A := BVB^T$ , for a diagonal matrix with positive entries $V$ (for comparison, the graph Laplacian is $\Delta = B^TB$ ). This is a map from edge weightings to edge weightings. When does this admit a positive eigenvector? Thanks to everyone in advance.","The generalised problem is as follows: Is there a condition on a symmetric positive-semi-definite matrix that ensures that it has a positive eigenvector, i.e. an eigenvector such that for all ? Or, indeed, even a weakly positive eigenvector, i.e. an eigenvector such that for all ? I am aware of the Perron-Frobenius theorem, but the assumptions for that theorem are too strong. The problem I actually want to solve is more specific, in case the extra information helps. If is the incidence matrix of a directed graph , then can be viewed as the linear transformation whose input is vertex weightings and whose output is the corresponding edge weightings obtained by the vertex weight differences. Then the matrix that I want to understand is , for a diagonal matrix with positive entries (for comparison, the graph Laplacian is ). This is a map from edge weightings to edge weightings. When does this admit a positive eigenvector? Thanks to everyone in advance.",A Av = \lambda v  v_i > 0 i Av = \lambda v  v_i \geq 0 i B G B A := BVB^T V \Delta = B^TB,"['linear-algebra', 'matrices', 'graph-theory', 'eigenvalues-eigenvectors', 'spectral-graph-theory']"
14,"Given a set of unitary matrices, can one find a vector whose images under these unitary matrices span the underlying Hilbert space?","Given a set of unitary matrices, can one find a vector whose images under these unitary matrices span the underlying Hilbert space?",,"Given a set of (linearly independent) $d\times d$ complex unitary matrices $\{U_i\}_{i=1}^n \subseteq M_d$ with $n\geq d$ , does there exist a vector $v\in \mathbb{C}^d$ such that $$\text{span} \{U_1v, U_2v, \ldots , U_nv\} = \mathbb{C}^d ?$$ The motivation for this question comes from the theory of mixed unitary quantum channels. A quantum channel $\Phi: M_d \rightarrow M_d$ is a completely positive and trace preserving linear map. Any such map admits a Kraus representation of the form $\Phi (X) = \sum_{i=1}^k A_i X A_i^*$ , where $\{A_i \}_{i=1}^k \subseteq M_d$ and $\sum_{i=1}^k A_i^* A_i = \mathbb{I}_d$ . We say that a quantum channel is mixed unitary if it can be expressed as a convex combination of unitary conjugations: $\Phi (X) = \sum_{i=1}^n p_i U_i X U_i^*$ . Our aim then is to look for a rank one input projector $X = vv^*$ for some $v\in \mathbb{C}^d$ such that the output $\Phi (vv^*) = \sum_{i=1}^n p_i (U_i v) (U_i v)^*$ has full rank. This is possible only if $n\geq d$ . To avoid trivial counterexamples, we can also assume that $\{ U_i\}_{i=1}^n \subseteq M_d$ is linearly independent. Follow-up question: Since it has been shown below that the question can be answered in the negative for $d\geq 4$ , the natural way of progression would be to ask if one can provide a classification of all the sets of (linearly independent) unitary matrices $\{U_i\}_{i=1}^n \subseteq M_d$ which allow for the existence of $v\in \mathbb{C}^d$ such that $$\text{span}\{U_1v, U_2v, \ldots ,U_nv\}=\mathbb{C}^d.$$ One can also try to answer this question for (linearly independent) sets of arbitrary complex matrices: $\{ A_i \}_{i=1}^k \subseteq M_d$ .","Given a set of (linearly independent) complex unitary matrices with , does there exist a vector such that The motivation for this question comes from the theory of mixed unitary quantum channels. A quantum channel is a completely positive and trace preserving linear map. Any such map admits a Kraus representation of the form , where and . We say that a quantum channel is mixed unitary if it can be expressed as a convex combination of unitary conjugations: . Our aim then is to look for a rank one input projector for some such that the output has full rank. This is possible only if . To avoid trivial counterexamples, we can also assume that is linearly independent. Follow-up question: Since it has been shown below that the question can be answered in the negative for , the natural way of progression would be to ask if one can provide a classification of all the sets of (linearly independent) unitary matrices which allow for the existence of such that One can also try to answer this question for (linearly independent) sets of arbitrary complex matrices: .","d\times d \{U_i\}_{i=1}^n \subseteq M_d n\geq d v\in \mathbb{C}^d \text{span} \{U_1v, U_2v, \ldots , U_nv\} = \mathbb{C}^d ? \Phi: M_d \rightarrow M_d \Phi (X) = \sum_{i=1}^k A_i X A_i^* \{A_i \}_{i=1}^k \subseteq M_d \sum_{i=1}^k A_i^* A_i = \mathbb{I}_d \Phi (X) = \sum_{i=1}^n p_i U_i X U_i^* X = vv^* v\in \mathbb{C}^d \Phi (vv^*) = \sum_{i=1}^n p_i (U_i v) (U_i v)^* n\geq d \{ U_i\}_{i=1}^n \subseteq M_d d\geq 4 \{U_i\}_{i=1}^n \subseteq M_d v\in \mathbb{C}^d \text{span}\{U_1v, U_2v, \ldots ,U_nv\}=\mathbb{C}^d. \{ A_i \}_{i=1}^k \subseteq M_d","['linear-algebra', 'matrices', 'unitary-matrices']"
15,Commuting Matrices have a common eigenvector (using Hilbert's Nullstellensatz),Commuting Matrices have a common eigenvector (using Hilbert's Nullstellensatz),,"I am aware that there exists elementary proof of the fact that commuting matrices have a common eigenvector. But recently I came across this following statement from the Wikipedia article ""https://en.wikipedia.org/wiki/Triangular_matrix#Simultaneous_triangularisability"" which goes as: ""The fact that commuting matrices have a common eigenvector can be interpreted as a result of Hilbert's Nullstellensatz: commuting matrices form a commutative algebra $k[A_1,A_2, \cdots A_n]$ over $K[x_{1}, x_{2}, \cdots x_n]$ which can be interpreted as a variety in k-dimensional affine space, and the existence of a (common) eigenvalue (and hence a common eigenvector) corresponds to this variety having a point (being non-empty), which is the content of the (weak) Nullstellensatz. In algebraic terms, these operators correspond to an algebra representation of the polynomial algebra in k variables."" I am having a hard time understanding the statements made here. In particular, I want to know a)How can we interpret the algebra $k[A_1,A_2, \cdots A_n]$ as a variety? b)What does the article mean by the statement ""a common eigenvalue and hence a common eigenvector""? We know that a common eigenvalue for two matrices does not mean they have the same eigenvector! It will be nice if someone can help me out with this.","I am aware that there exists elementary proof of the fact that commuting matrices have a common eigenvector. But recently I came across this following statement from the Wikipedia article ""https://en.wikipedia.org/wiki/Triangular_matrix#Simultaneous_triangularisability"" which goes as: ""The fact that commuting matrices have a common eigenvector can be interpreted as a result of Hilbert's Nullstellensatz: commuting matrices form a commutative algebra over which can be interpreted as a variety in k-dimensional affine space, and the existence of a (common) eigenvalue (and hence a common eigenvector) corresponds to this variety having a point (being non-empty), which is the content of the (weak) Nullstellensatz. In algebraic terms, these operators correspond to an algebra representation of the polynomial algebra in k variables."" I am having a hard time understanding the statements made here. In particular, I want to know a)How can we interpret the algebra as a variety? b)What does the article mean by the statement ""a common eigenvalue and hence a common eigenvector""? We know that a common eigenvalue for two matrices does not mean they have the same eigenvector! It will be nice if someone can help me out with this.","k[A_1,A_2, \cdots A_n] K[x_{1}, x_{2}, \cdots x_n] k[A_1,A_2, \cdots A_n]","['linear-algebra', 'abstract-algebra', 'algebraic-geometry']"
16,"About the existence of an isomorphism of algebras between the algebras of $\mathcal{C}^0$ and $\mathcal{C}^1$ functions from $[0, 1]$ to $\mathbb{R}$",About the existence of an isomorphism of algebras between the algebras of  and  functions from  to,"\mathcal{C}^0 \mathcal{C}^1 [0, 1] \mathbb{R}","Let $\mathcal{C}^0 ( [0, 1], \mathbb{R})$ denote the set of continuous functions from $[0, 1]$ to $\mathbb{R}$ and $\mathcal{C}^1 ( [0, 1], \mathbb{R})$ denote the set of class $\mathcal{C}^1$ functions from $[0, 1]$ to $\mathbb{R}$ , both of these sets are algebras over the field $\mathbb{R}$ . The question is whether we can find an isomorphism of algebras : $\Phi:\mathcal{C}^0 ( [0, 1], \mathbb{R})\longrightarrow\mathcal{C}^1 ( [0, 1], \mathbb{R})$ it seems, well at least to me, that such a cheesy isomorphism cannot exist, however I couldn't prove it. I tried the obvious choice, which is proof by contradiction, and tried taking the inverse of such a function which would map differentiable functions to continuous ones, considering that one set already contains the other, however no contradiction seemed to arise.","Let denote the set of continuous functions from to and denote the set of class functions from to , both of these sets are algebras over the field . The question is whether we can find an isomorphism of algebras : it seems, well at least to me, that such a cheesy isomorphism cannot exist, however I couldn't prove it. I tried the obvious choice, which is proof by contradiction, and tried taking the inverse of such a function which would map differentiable functions to continuous ones, considering that one set already contains the other, however no contradiction seemed to arise.","\mathcal{C}^0 ( [0, 1], \mathbb{R}) [0, 1] \mathbb{R} \mathcal{C}^1 ( [0, 1], \mathbb{R}) \mathcal{C}^1 [0, 1] \mathbb{R} \mathbb{R} \Phi:\mathcal{C}^0 ( [0, 1], \mathbb{R})\longrightarrow\mathcal{C}^1 ( [0, 1], \mathbb{R})","['linear-algebra', 'abstract-algebra', 'vector-space-isomorphism']"
17,Determinant of a $2 \times 2$ complex block matrix is nonnegative,Determinant of a  complex block matrix is nonnegative,2 \times 2,"Let $n \geq 1$ and $A, B \in M_n(\mathbb C)$ . Form the matrix $$g=  \begin{bmatrix}     A & -B \\     \overline B & \overline A \end{bmatrix} \in M_{2n}(\mathbb C)$$ I would like to prove that the matrix $g$ has non negative determinant. Actually, I can prove this in the case $A$ and $B$ have real entries, this is a classic exercise. To do this, I would make some operations on columns and lines to reduce to an upper-triangular matrix by blocks, which would result in the identity $\det(g)=\det(A+iB)\det(A-iB)\geq0$ . However, this method seems to fail in the case of complex entries. Could someone give me a hand with this exercise? EDIT:  Using density of invertible matrices, we may assume that $A$ is invertible. Using the formula given by Schur complement, I can reduce this problem to the following. Given $X$ a square matrix with complex entries, we have $\det(I+X\overline{X})\geq 0$ . I am currently trying to prove this, but I have not been able to conclude yet. Note that if I use the notation of the initial problem, then $X = A^{-1}B$ .","Let and . Form the matrix I would like to prove that the matrix has non negative determinant. Actually, I can prove this in the case and have real entries, this is a classic exercise. To do this, I would make some operations on columns and lines to reduce to an upper-triangular matrix by blocks, which would result in the identity . However, this method seems to fail in the case of complex entries. Could someone give me a hand with this exercise? EDIT:  Using density of invertible matrices, we may assume that is invertible. Using the formula given by Schur complement, I can reduce this problem to the following. Given a square matrix with complex entries, we have . I am currently trying to prove this, but I have not been able to conclude yet. Note that if I use the notation of the initial problem, then .","n \geq 1 A, B \in M_n(\mathbb C) g= 
\begin{bmatrix}
    A & -B \\
    \overline B & \overline A
\end{bmatrix}
\in M_{2n}(\mathbb C) g A B \det(g)=\det(A+iB)\det(A-iB)\geq0 A X \det(I+X\overline{X})\geq 0 X = A^{-1}B","['linear-algebra', 'matrices', 'determinant', 'block-matrices']"
18,Intuitive Explanation of the Inner Product,Intuitive Explanation of the Inner Product,,"I'm currently studying Linear Algebra, and we've arrived at the topic of Inner Products and orthogonality. I have been searching online to try and help myself understand what the inner product actually is. I understand it is supposedly an generalisation of the dot product, and I understood that the dot product was useful in calculating the angle between two vectors in 3D space, etc. What is the inner product of two elements of real/ complex spaces? Is it a formula? What is the use of the inner product, and can it be applied to any vector space or specifically the real/ complex spaces? How does this idea extend to inner product spaces? I don't understand it intuitively and any help/ guidance on how to grasp the concept better would be really appreciated. Thanks.","I'm currently studying Linear Algebra, and we've arrived at the topic of Inner Products and orthogonality. I have been searching online to try and help myself understand what the inner product actually is. I understand it is supposedly an generalisation of the dot product, and I understood that the dot product was useful in calculating the angle between two vectors in 3D space, etc. What is the inner product of two elements of real/ complex spaces? Is it a formula? What is the use of the inner product, and can it be applied to any vector space or specifically the real/ complex spaces? How does this idea extend to inner product spaces? I don't understand it intuitively and any help/ guidance on how to grasp the concept better would be really appreciated. Thanks.",,"['linear-algebra', 'vector-spaces', 'inner-products']"
19,Solving the matrix equation $X^tA+A^tX=0$ for $X$ in terms of $A$,Solving the matrix equation  for  in terms of,X^tA+A^tX=0 X A,"Suppose that I know $A$ . And all matrices in the equation are square matrices. I want to solve for $X$ given that $$X^tA + A^tX = 0$$ I'm not really good at matrix calculus. Is it possible to solve this problem in the sense that we find a closed form solution for $X$ in terms of $A$ and possibly some other vector B (as a free parameter, if necessary)? For example, when $A = I$ , we see that $X=B$ for any anti-symmetric matrix $B$ . The motivation for this question comes from computer vision. We know that a homography $H$ between two photos is induced by a plane in space if and only if $H^tF$ is anti-symmetric where $F$ is the fundamental matrix. I also know that $F$ can be parametrized as $F=[e]_{\times}M$ where $M$ is invertible. Now I want to find a general form for $H$ . Hence, this question. Edit: Since the general case might be too broad and challenging, let's narrow down our attention to the simpler case where $A$ and $X$ are $3 \times 3$ matrices and $A$ is not invertible. The general case seems very interesting too.","Suppose that I know . And all matrices in the equation are square matrices. I want to solve for given that I'm not really good at matrix calculus. Is it possible to solve this problem in the sense that we find a closed form solution for in terms of and possibly some other vector B (as a free parameter, if necessary)? For example, when , we see that for any anti-symmetric matrix . The motivation for this question comes from computer vision. We know that a homography between two photos is induced by a plane in space if and only if is anti-symmetric where is the fundamental matrix. I also know that can be parametrized as where is invertible. Now I want to find a general form for . Hence, this question. Edit: Since the general case might be too broad and challenging, let's narrow down our attention to the simpler case where and are matrices and is not invertible. The general case seems very interesting too.",A X X^tA + A^tX = 0 X A A = I X=B B H H^tF F F F=[e]_{\times}M M H A X 3 \times 3 A,"['linear-algebra', 'matrix-equations', 'matrix-calculus', 'matrix-decomposition', 'computer-vision']"
20,linear combination of two coprime polynomials such that the roots are disctinct?,linear combination of two coprime polynomials such that the roots are disctinct?,,"Let $a = (a_0, \dots, a_{n-1})^T \in \mathbb R^n$ and $b = (b_0, \dots, b_{n-1})^T \in \mathbb R^n$ . We define two polynomials by \begin{align*} f_a(x) &= x^n + a_{n-1} x^{n-1} + \cdots + a_0 \\ f_b(x) &= b_{n-1} x^{n-1} + \cdots +b_0. \end{align*} Suppose we require $f_a$ and $f_b$ are coprime in $\mathbb C$ . Can we find a scalar $s \in \mathbb R$ such that $f_a(x) + s f_b(x)$ has distinct roots in $\mathbb C$ ? If $f_a(x) + s f_b(x) = x^n + (a_{n-1} + s b_{n-1})x^{n-1} + \cdots + (a_0 + sb_0)$ has multiple roots for all $s \in \mathbb R$ , this would imply the discriminant which is a polynomial in $s$ would be identically $0$ for $s \in \mathbb R$ . I don't think this can happen but could not figure out how to argue this part. To not cause confusion, this sentence is crossed out. This should be a separate question. The coprime condition just conveniently comes out of my situation. I am not sure this is necessary. Intuitively, I would think $b \neq 0$ should enough.","Let and . We define two polynomials by Suppose we require and are coprime in . Can we find a scalar such that has distinct roots in ? If has multiple roots for all , this would imply the discriminant which is a polynomial in would be identically for . I don't think this can happen but could not figure out how to argue this part. To not cause confusion, this sentence is crossed out. This should be a separate question. The coprime condition just conveniently comes out of my situation. I am not sure this is necessary. Intuitively, I would think should enough.","a = (a_0, \dots, a_{n-1})^T \in \mathbb R^n b = (b_0, \dots, b_{n-1})^T \in \mathbb R^n \begin{align*}
f_a(x) &= x^n + a_{n-1} x^{n-1} + \cdots + a_0 \\
f_b(x) &= b_{n-1} x^{n-1} + \cdots +b_0.
\end{align*} f_a f_b \mathbb C s \in \mathbb R f_a(x) + s f_b(x) \mathbb C f_a(x) + s f_b(x) = x^n + (a_{n-1} + s b_{n-1})x^{n-1} + \cdots + (a_0 + sb_0) s \in \mathbb R s 0 s \in \mathbb R b \neq 0","['linear-algebra', 'abstract-algebra', 'polynomials']"
21,How to calculate $\dfrac{\partial a^{\rm T}A^{-\rm T}bb^{\rm T}A^{-1}a}{\partial A}$?,How to calculate ?,\dfrac{\partial a^{\rm T}A^{-\rm T}bb^{\rm T}A^{-1}a}{\partial A},"How can I calculate $\dfrac{\partial a^{\rm T}A^{-\rm T}bb^{\rm T}A^{-1}a}{\partial A}$, where $A\in\mathbb{R}^{n\times n}$ and $a,b\in\mathbb{R}^n$?","How can I calculate $\dfrac{\partial a^{\rm T}A^{-\rm T}bb^{\rm T}A^{-1}a}{\partial A}$, where $A\in\mathbb{R}^{n\times n}$ and $a,b\in\mathbb{R}^n$?",,"['linear-algebra', 'matrices', 'partial-derivative']"
22,${A_1}^k + {A_2}^k + \cdots +{A_n}^k = 0$ for all $k \in \mathbb N_{>0}$ $\implies$ $A_i$ are all nilpotent.,for all    are all nilpotent.,{A_1}^k + {A_2}^k + \cdots +{A_n}^k = 0 k \in \mathbb N_{>0} \implies A_i,"Does the following statement hold true? Given $n$ real matrices $A_1, A_2, \cdots A_n$, if their $k$-th power sum is zero for all $k \in \mathbb N_{>0}$ then they are all nilpotent. Given $n$ real variables $x_1, x_2, \cdots x_n$, if their $k$-th power sum is zero for all $k \in \mathbb N_{>0}$ then they are all identical to zero. This can be shown by gathering up the same values and using the well known fact that the Vandermonte matrix is invertible. I tried the same method. The entry ${x_i}^j$ turns into ${A_i}^j \otimes I$ so the determinant is given by the product of ${(\det(A_i -A_j))}^n$ which is not necessarily nonzero. I got stuck here.","Does the following statement hold true? Given $n$ real matrices $A_1, A_2, \cdots A_n$, if their $k$-th power sum is zero for all $k \in \mathbb N_{>0}$ then they are all nilpotent. Given $n$ real variables $x_1, x_2, \cdots x_n$, if their $k$-th power sum is zero for all $k \in \mathbb N_{>0}$ then they are all identical to zero. This can be shown by gathering up the same values and using the well known fact that the Vandermonte matrix is invertible. I tried the same method. The entry ${x_i}^j$ turns into ${A_i}^j \otimes I$ so the determinant is given by the product of ${(\det(A_i -A_j))}^n$ which is not necessarily nonzero. I got stuck here.",,"['linear-algebra', 'matrices']"
23,How to convert index notation equations to matrix/tensor equations?,How to convert index notation equations to matrix/tensor equations?,,"In many areas within computer science, one often arrives at an equation that uses index notation on some scalar elements of a vector/matrix/tensor, for example: $$ a_i^{(s)} = \sum_j \frac{a_j^{(s+1)} \cdot h_{ij}^{(s)} \cdot b_i^{(s)}}{\sum_k h_{kj}^{(s)} \cdot b_{k}^{(s)}} $$ where $a_i^{(s)}$, $a_i^{(s+1)}$, $b_i^{(s)}$ are elements of vectors $\bf{a^{(s)}}$, $\bf{a}^{(s+1)}$, and $\bf{b^{(s)}}$, and $h_{ij}^{(s)}$ is an element of a matrix $H^{(s)}$. Then, it is often desirable to convert such an equation to its tensor form, in this case (making some assumptions about the compatibility of dimensions): $$ \mathbf{a}^{(s)} = \mathbf{b}^{(s)} \odot (H^{(s)}. \Big( \mathbf{a}^{(s+1)} \oslash (H^{(s)T} \cdot \mathbf{b}^{(s)})\Big)) $$ Where $\oslash$ and $\odot$ are Hadamard division and product respectively. However, converting such an equation in this way seems to take a lot of guesswork and effort (even though I do not consider myself foreign to linear algebra), whilst one may often encounter even more complicated equations with more than two dimensions. My question then is: is there some sort of method for dealing with these sort of equations, or even a way of practicing to be able to do this quickly?","In many areas within computer science, one often arrives at an equation that uses index notation on some scalar elements of a vector/matrix/tensor, for example: $$ a_i^{(s)} = \sum_j \frac{a_j^{(s+1)} \cdot h_{ij}^{(s)} \cdot b_i^{(s)}}{\sum_k h_{kj}^{(s)} \cdot b_{k}^{(s)}} $$ where $a_i^{(s)}$, $a_i^{(s+1)}$, $b_i^{(s)}$ are elements of vectors $\bf{a^{(s)}}$, $\bf{a}^{(s+1)}$, and $\bf{b^{(s)}}$, and $h_{ij}^{(s)}$ is an element of a matrix $H^{(s)}$. Then, it is often desirable to convert such an equation to its tensor form, in this case (making some assumptions about the compatibility of dimensions): $$ \mathbf{a}^{(s)} = \mathbf{b}^{(s)} \odot (H^{(s)}. \Big( \mathbf{a}^{(s+1)} \oslash (H^{(s)T} \cdot \mathbf{b}^{(s)})\Big)) $$ Where $\oslash$ and $\odot$ are Hadamard division and product respectively. However, converting such an equation in this way seems to take a lot of guesswork and effort (even though I do not consider myself foreign to linear algebra), whilst one may often encounter even more complicated equations with more than two dimensions. My question then is: is there some sort of method for dealing with these sort of equations, or even a way of practicing to be able to do this quickly?",,"['linear-algebra', 'tensors']"
24,How many bases can a vector space have? [closed],How many bases can a vector space have? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Hello guys I am currently taking a linear Algebra class; I stumbled upon this question: How many bases can a vector space have? Are they unique? Thanks in advance.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Hello guys I am currently taking a linear Algebra class; I stumbled upon this question: How many bases can a vector space have? Are they unique? Thanks in advance.",,"['linear-algebra', 'vector-spaces']"
25,A question about the definition of generalized eigenspace,A question about the definition of generalized eigenspace,,"Given an $n\times n$ matrix $A$ , its generalized eigenspace pertaining to an eigenvalue $\lambda_i$ is defined as $V_{\lambda_i}=\{x:(A-\lambda_i I)^n x=0\}$ The question is to prove $V_{\lambda_i}=\{x:(A-\lambda_i I)^n x=0\} = \{x:(A-\lambda_i I)^{m(\lambda_i)} x=0\}$ where $m(\lambda_i)$ is the algebraic multiplicity of $\lambda_i$ . It is obvious that $\{x:(A-\lambda_i I)^{m(\lambda_i)} x=0\} \subseteq \{x:(A-\lambda_i I)^n x=0\}$ , since $(A-\lambda_i I)^{m(\lambda_i)} x=0\Rightarrow (A-\lambda_i I)^n x=0$ . The problem is the other direction. The solution hints that using Hamilton-Cayley theorem, i.e. $\prod\limits_{{\lambda } \in \sigma (A)} {{{(A - {\lambda}I)}^{m({\lambda })}}}  = O$ where ${\sigma (A)}$ is the spectrum of $A$","Given an matrix , its generalized eigenspace pertaining to an eigenvalue is defined as The question is to prove where is the algebraic multiplicity of . It is obvious that , since . The problem is the other direction. The solution hints that using Hamilton-Cayley theorem, i.e. where is the spectrum of",n\times n A \lambda_i V_{\lambda_i}=\{x:(A-\lambda_i I)^n x=0\} V_{\lambda_i}=\{x:(A-\lambda_i I)^n x=0\} = \{x:(A-\lambda_i I)^{m(\lambda_i)} x=0\} m(\lambda_i) \lambda_i \{x:(A-\lambda_i I)^{m(\lambda_i)} x=0\} \subseteq \{x:(A-\lambda_i I)^n x=0\} (A-\lambda_i I)^{m(\lambda_i)} x=0\Rightarrow (A-\lambda_i I)^n x=0 \prod\limits_{{\lambda } \in \sigma (A)} {{{(A - {\lambda}I)}^{m({\lambda })}}}  = O {\sigma (A)} A,"['linear-algebra', 'matrices']"
26,Is it important to understand Linear algebra geometrically?,Is it important to understand Linear algebra geometrically?,,"I am currently self studying linear algebra, using Axler's text. And I always find myself trying to picture every theorem and proposition by considering some low dimensional cases, in order to understand why the theorems make sense. As I get to the chapter for eigenvalues and eigenvectors, things are getting a bit abstract. Specifically, he proved that every linear operator over an odd dimensional real vector space must have an eigenvalue by using induction, he considered the case where $U$ is a $2$ dimensional invariant subspace of $V$, set $W$ such that $U\oplus W=V$. Then consider the projection of $T\vec w$ onto $W$ along $U$ as an operator in $L(W)$, which must have an eigenvalue $\lambda$ by induction hypothesis. In the end, he showed that $T-\lambda I$ is not injective, thus $\lambda$ is also an eigenvalue of $T$. I understand everything he's written, but failed to grasp the bigger picture as I can only imagine vector space up to $3$ dimensions (in which case $W$ is only one dimensional). The construction is remarkable for me, as two different linear operators have the same eigenvalue, which I have no idea what the picture would look like (e.g. where does the eigenvector lie on) in higher dimensional spaces. So my question: Is understanding linear algebra through picture important? In some sense, I feel like it helps me understand more why some theorems would be true, but the proofs/constructions do not rely on picture at all as they mostly involve algebraic manipulations.","I am currently self studying linear algebra, using Axler's text. And I always find myself trying to picture every theorem and proposition by considering some low dimensional cases, in order to understand why the theorems make sense. As I get to the chapter for eigenvalues and eigenvectors, things are getting a bit abstract. Specifically, he proved that every linear operator over an odd dimensional real vector space must have an eigenvalue by using induction, he considered the case where $U$ is a $2$ dimensional invariant subspace of $V$, set $W$ such that $U\oplus W=V$. Then consider the projection of $T\vec w$ onto $W$ along $U$ as an operator in $L(W)$, which must have an eigenvalue $\lambda$ by induction hypothesis. In the end, he showed that $T-\lambda I$ is not injective, thus $\lambda$ is also an eigenvalue of $T$. I understand everything he's written, but failed to grasp the bigger picture as I can only imagine vector space up to $3$ dimensions (in which case $W$ is only one dimensional). The construction is remarkable for me, as two different linear operators have the same eigenvalue, which I have no idea what the picture would look like (e.g. where does the eigenvector lie on) in higher dimensional spaces. So my question: Is understanding linear algebra through picture important? In some sense, I feel like it helps me understand more why some theorems would be true, but the proofs/constructions do not rely on picture at all as they mostly involve algebraic manipulations.",,"['linear-algebra', 'soft-question', 'eigenvalues-eigenvectors']"
27,"Diagonal Matrix, just eigenvalues?","Diagonal Matrix, just eigenvalues?",,"Assuming I've tested for diagonalization, can I just take the eigenvalues and arbitrarily place them in in the i,j cells to produce a diagonal matrix? Say I have a matrix $M$ with eigenvalues $\lambda_1 = 4,\; \lambda_2 = \lambda_3 = -2.$ $$M =\begin{bmatrix}{1} & {-3} & {3} \\ {3} & {-5} & {3} \\ {6} & {-6} & {4}\end{bmatrix}$$ And let, $$A = \begin{bmatrix}4&0&0\\0&-2&0\\0&0&-2\end{bmatrix} , \quad B = \begin{bmatrix}-2&0&0\\0&4&0\\0&0&-2\end{bmatrix} , \quad C = \begin{bmatrix}-2&0&0\\0&-2&0\\0&0&4\end{bmatrix}$$ Are $A , B , C$ valid diagonal matrices , or does the order of the eigenvalues matter?","Assuming I've tested for diagonalization, can I just take the eigenvalues and arbitrarily place them in in the i,j cells to produce a diagonal matrix? Say I have a matrix with eigenvalues And let, Are valid diagonal matrices , or does the order of the eigenvalues matter?","M \lambda_1 = 4,\; \lambda_2 = \lambda_3 = -2. M =\begin{bmatrix}{1} & {-3} & {3} \\ {3} & {-5} & {3} \\ {6} & {-6} & {4}\end{bmatrix} A = \begin{bmatrix}4&0&0\\0&-2&0\\0&0&-2\end{bmatrix} , \quad B = \begin{bmatrix}-2&0&0\\0&4&0\\0&0&-2\end{bmatrix} , \quad C = \begin{bmatrix}-2&0&0\\0&-2&0\\0&0&4\end{bmatrix} A , B , C",['linear-algebra']
28,"If all entries of matrix $X$ are the same, then $\det (A+X)\det (A-X) \leq \det (A^2)$","If all entries of matrix  are the same, then",X \det (A+X)\det (A-X) \leq \det (A^2),I want to prove that $\det (A+X)\det (A-X) \leq \det (A^2)$ where $X $ is a matrix whose $n^2$ entries are all the same. I tried to write down the expressions involved but that didn't help me prove the inequality.,I want to prove that $\det (A+X)\det (A-X) \leq \det (A^2)$ where $X $ is a matrix whose $n^2$ entries are all the same. I tried to write down the expressions involved but that didn't help me prove the inequality.,,"['linear-algebra', 'matrices', 'inequality', 'determinant']"
29,Discrete subgroups of $\mathbb R^n$ are free,Discrete subgroups of  are free,\mathbb R^n,"Let $G$ be a nonzero subgroup of the additive group $\mathbb{R}^n$. Assume $G$ is discrete in the sense that for any $x \in G$, there exists an open set $U \subset \mathbb{R}^n$ such that $U \cap G = \{x\}$. Does it follow that there exists an integer $1 \le k \le n$ and $k$-tuple $x_1, \dots, x_k \in \mathbb{R}^n$ of $\mathbb{R}$-linearly independent vectors such that $G = \mathbb{Z}x_1 + \cdots + \mathbb{Z}x_k$, i.e., such that $G$ is the free abelian group with basis $x_1, \dots, x_k$? I suspect the answer is yes... perhaps we could chose an element in $G$ of minimal Euclidean length of use induction on $n$? But then I am stuck. Can anyone help me?","Let $G$ be a nonzero subgroup of the additive group $\mathbb{R}^n$. Assume $G$ is discrete in the sense that for any $x \in G$, there exists an open set $U \subset \mathbb{R}^n$ such that $U \cap G = \{x\}$. Does it follow that there exists an integer $1 \le k \le n$ and $k$-tuple $x_1, \dots, x_k \in \mathbb{R}^n$ of $\mathbb{R}$-linearly independent vectors such that $G = \mathbb{Z}x_1 + \cdots + \mathbb{Z}x_k$, i.e., such that $G$ is the free abelian group with basis $x_1, \dots, x_k$? I suspect the answer is yes... perhaps we could chose an element in $G$ of minimal Euclidean length of use induction on $n$? But then I am stuck. Can anyone help me?",,"['linear-algebra', 'abstract-algebra']"
30,Relation between the eigenvectors of $A$ and $A^TA$,Relation between the eigenvectors of  and,A A^TA,"Is there a relation between the eigenvectors of matrix $A$ and of matrix $A^TA$? This question is related to eigenvectors, not eigenvalues. Further, the sizes of matrices $A$ and $A^TA$ are different if $A$ is not a square matrix. Thus, my question has no relation to the question about eigenvalues of $A$ ans $A^T$.","Is there a relation between the eigenvectors of matrix $A$ and of matrix $A^TA$? This question is related to eigenvectors, not eigenvalues. Further, the sizes of matrices $A$ and $A^TA$ are different if $A$ is not a square matrix. Thus, my question has no relation to the question about eigenvalues of $A$ ans $A^T$.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
31,"If I know $AB$, how can I calculate $BA$?","If I know , how can I calculate ?",AB BA,"Let $A\mathscr{M}_{32}(\mathbb{R})$ and $B\mathscr{M}_{2\times3}(\mathbb{R})$ be matrices satisfying   $AB =\begin{bmatrix} 8 &2 &2\\ 2 &5 &4\\ 2 &4& 5 \end{bmatrix}$. Calculate $BA$. (Golan, The Linear Algebra a Beginning Graduate Student Ought to Know , Exercise 426.) Maybe it can be solved by solving a system of equations, but I think there is a shorter way since this problem was in my exam. Thanks.","Let $A\mathscr{M}_{32}(\mathbb{R})$ and $B\mathscr{M}_{2\times3}(\mathbb{R})$ be matrices satisfying   $AB =\begin{bmatrix} 8 &2 &2\\ 2 &5 &4\\ 2 &4& 5 \end{bmatrix}$. Calculate $BA$. (Golan, The Linear Algebra a Beginning Graduate Student Ought to Know , Exercise 426.) Maybe it can be solved by solving a system of equations, but I think there is a shorter way since this problem was in my exam. Thanks.",,"['linear-algebra', 'abstract-algebra', 'matrices']"
32,Is there a generalization of the Lagrange polynomial to 3D?,Is there a generalization of the Lagrange polynomial to 3D?,,What is a way to construct a smooth polynomial surface ($\mathbb{R}^2 \rightarrow \mathbb{R}$) with Lagrange-polynomial properties in every partial derivative? I want to try this for image interpolation.,What is a way to construct a smooth polynomial surface ($\mathbb{R}^2 \rightarrow \mathbb{R}$) with Lagrange-polynomial properties in every partial derivative? I want to try this for image interpolation.,,"['linear-algebra', 'polynomials', '3d', 'interpolation']"
33,Analogy of transpose for a function?,Analogy of transpose for a function?,,"In the page 2 of Linear algebra explained in four pages reference , it has a box describing the relationship between functions and linear transformation. It states that the set of zeroes of a function ($f(x)=0$) is analogous to the null space of a matrix ($A \vec x=\vec 0$), and the function inverse ($f^{-1}(x)$) is analogous to the matrix inverse ($A^{-1} \vec x$). Is there a functional analogy of the matrix transpose ($A^\text{T}$), and the functional analogy of the null space of a transpose ($A^\text{T} \vec x = \vec 0$)?","In the page 2 of Linear algebra explained in four pages reference , it has a box describing the relationship between functions and linear transformation. It states that the set of zeroes of a function ($f(x)=0$) is analogous to the null space of a matrix ($A \vec x=\vec 0$), and the function inverse ($f^{-1}(x)$) is analogous to the matrix inverse ($A^{-1} \vec x$). Is there a functional analogy of the matrix transpose ($A^\text{T}$), and the functional analogy of the null space of a transpose ($A^\text{T} \vec x = \vec 0$)?",,['linear-algebra']
34,Minimum dimension to hold $N$ points with given distances?,Minimum dimension to hold  points with given distances?,N,"Suppose you're given $N$ points along with an $N\times N$ matrix $D$ with entries $d_{ij}$ giving the distances between the points (assume that the $d_{ij}$ satisfy the usual requirements of a distance, i.e. , $d_{ii}=0$, $d_{ij}=d_{ji}$, and $d_{ij} \leq d_{ik} + d_{jk}$). From this information alone, can you determine what's the minimum dimension of an Euclidean space ( i.e. , what's the minimum $n$ in $\mathbb{R}^n$) that can hold these points? In other words, what's the minimum $n$ such that there exist $N$ points in $\mathbb{R}^n$ with distances as given in the matrix $D$?","Suppose you're given $N$ points along with an $N\times N$ matrix $D$ with entries $d_{ij}$ giving the distances between the points (assume that the $d_{ij}$ satisfy the usual requirements of a distance, i.e. , $d_{ii}=0$, $d_{ij}=d_{ji}$, and $d_{ij} \leq d_{ik} + d_{jk}$). From this information alone, can you determine what's the minimum dimension of an Euclidean space ( i.e. , what's the minimum $n$ in $\mathbb{R}^n$) that can hold these points? In other words, what's the minimum $n$ such that there exist $N$ points in $\mathbb{R}^n$ with distances as given in the matrix $D$?",,"['linear-algebra', 'geometry', 'euclidean-geometry', 'analytic-geometry']"
35,Prove that : $a^n+b^n+c^n=x^n+y^n+z^n$; $\forall n\in \mathbb{N}$,Prove that : ;,a^n+b^n+c^n=x^n+y^n+z^n \forall n\in \mathbb{N},$a;b;c;x;y;z \in \mathbb{R}$ such that : \begin{matrix} a+b+c=x+y+z & \\  a^2+b^2+c^2=x^2+y^2+z^2 & \\  a^3+b^3+c^3=x^3+y^3+z^3 &  \end{matrix} Prove that : $a^n+b^n+c^n=x^n+y^n+z^n$; $\forall n\in \mathbb{N}$ P/s : I don't have any ideas about this problem..!! Thanks :),$a;b;c;x;y;z \in \mathbb{R}$ such that : \begin{matrix} a+b+c=x+y+z & \\  a^2+b^2+c^2=x^2+y^2+z^2 & \\  a^3+b^3+c^3=x^3+y^3+z^3 &  \end{matrix} Prove that : $a^n+b^n+c^n=x^n+y^n+z^n$; $\forall n\in \mathbb{N}$ P/s : I don't have any ideas about this problem..!! Thanks :),,['linear-algebra']
36,Problem with Smith normal form over a PID that is not an Euclidean domain,Problem with Smith normal form over a PID that is not an Euclidean domain,,"This is an homework exercise of the Algebra lecture. I need to evaluate the Smith normal form of the following matrix    $$A:=\begin{pmatrix}1 & -\xi & \xi-1\\2 \xi&8&8\xi+7\\\xi& 4 & 3\xi +2 \end{pmatrix} \in M(3\times 3;\Bbb{Z}[\xi]),$$   where $\xi := \frac{1+\sqrt{-19}}{2}$. We have seen in the lecture, that similarly as for rings, there exists the Smith Normal Form also for PID's. To solve the problem (we don't need to find $S, T \in M(3 \times 3; \Bbb{Z}[\xi])$, such that $SAT$ is in Smith Normal Form) we applied row, and column operations, paying particular attention to not multiply rows and columns by nonunits, since this is not allowed. (Also according to this answer). The steps that we performed are: \begin{align*}\begin{pmatrix}1 & -\xi & \xi-1\\2 \xi&8&8\xi+7\\\xi& 4 & 3\xi +2 \end{pmatrix} &\overset{2\text{column}+\xi \text{ first column}}{\leadsto} \begin{pmatrix}1 & 0 & \xi-1\\2 \xi&8+2\xi^2&8\xi+7\\\xi& 4+\xi^2 & 3\xi +2 \end{pmatrix}\\ \overset{3\text{column}-(\xi-1)  \text{ first column}}{\leadsto} \begin{pmatrix}1 & 0 & 0\\2 \xi&8+2\xi^2&-2\xi^2+10\xi+7\\\xi& 4+\xi^2 & -\xi^2+4\xi+2 \end{pmatrix} &\overset{2\text{row}-(2\xi)  \text{ first row}}{\leadsto} \begin{pmatrix}1 & 0 & 0\\0&8+2\xi^2&-2\xi^2+10\xi+7\\\xi& 4+\xi^2 & -\xi^2+4\xi+2 \end{pmatrix}\\ \overset{3\text{row}-(\xi)  \text{ first row}}{\leadsto} \begin{pmatrix}1 & 0 & 0\\0&8+2\xi^2&-2\xi^2+10\xi+7\\0& 4+\xi^2 & -\xi^2+4\xi+2 \end{pmatrix}&  \overset{2\text{row}-2  \text{ third row}}{\leadsto} \begin{pmatrix}1 & 0 & 0\\0&0&2\xi+3\\0& 4+\xi^2 & -\xi^2+4\xi+2 \end{pmatrix}\\ \overset{\text{swap columns}}{\leadsto} \begin{pmatrix}1 & 0 & 0\\0&2\xi +3&0\\0& -\xi^2+4\xi+2 & 4+\xi^2 \end{pmatrix} & \\ \overset{\text{second column $+$ third column}}{\leadsto} \begin{pmatrix}1 & 0 & 0\\0&2\xi +3&0\\0& 4 \xi +6 & 4+\xi^2 \end{pmatrix} & \overset{\text{third row } - 2 \text{ second row}}{\leadsto} \begin{pmatrix}1 & 0 & 0\\0&2\xi +3&0\\0& 0 & 4+\xi^2 \end{pmatrix}=: B \end{align*} Unfortunately the term $b_{33}$ is not divisible by the term $b_{22}$ as it should be. We have done the steps many many times, but we can't find the error (I'm doing this homework exercise with my classmates). Do you maybe find it? Thank you in advance for any help.","This is an homework exercise of the Algebra lecture. I need to evaluate the Smith normal form of the following matrix    $$A:=\begin{pmatrix}1 & -\xi & \xi-1\\2 \xi&8&8\xi+7\\\xi& 4 & 3\xi +2 \end{pmatrix} \in M(3\times 3;\Bbb{Z}[\xi]),$$   where $\xi := \frac{1+\sqrt{-19}}{2}$. We have seen in the lecture, that similarly as for rings, there exists the Smith Normal Form also for PID's. To solve the problem (we don't need to find $S, T \in M(3 \times 3; \Bbb{Z}[\xi])$, such that $SAT$ is in Smith Normal Form) we applied row, and column operations, paying particular attention to not multiply rows and columns by nonunits, since this is not allowed. (Also according to this answer). The steps that we performed are: \begin{align*}\begin{pmatrix}1 & -\xi & \xi-1\\2 \xi&8&8\xi+7\\\xi& 4 & 3\xi +2 \end{pmatrix} &\overset{2\text{column}+\xi \text{ first column}}{\leadsto} \begin{pmatrix}1 & 0 & \xi-1\\2 \xi&8+2\xi^2&8\xi+7\\\xi& 4+\xi^2 & 3\xi +2 \end{pmatrix}\\ \overset{3\text{column}-(\xi-1)  \text{ first column}}{\leadsto} \begin{pmatrix}1 & 0 & 0\\2 \xi&8+2\xi^2&-2\xi^2+10\xi+7\\\xi& 4+\xi^2 & -\xi^2+4\xi+2 \end{pmatrix} &\overset{2\text{row}-(2\xi)  \text{ first row}}{\leadsto} \begin{pmatrix}1 & 0 & 0\\0&8+2\xi^2&-2\xi^2+10\xi+7\\\xi& 4+\xi^2 & -\xi^2+4\xi+2 \end{pmatrix}\\ \overset{3\text{row}-(\xi)  \text{ first row}}{\leadsto} \begin{pmatrix}1 & 0 & 0\\0&8+2\xi^2&-2\xi^2+10\xi+7\\0& 4+\xi^2 & -\xi^2+4\xi+2 \end{pmatrix}&  \overset{2\text{row}-2  \text{ third row}}{\leadsto} \begin{pmatrix}1 & 0 & 0\\0&0&2\xi+3\\0& 4+\xi^2 & -\xi^2+4\xi+2 \end{pmatrix}\\ \overset{\text{swap columns}}{\leadsto} \begin{pmatrix}1 & 0 & 0\\0&2\xi +3&0\\0& -\xi^2+4\xi+2 & 4+\xi^2 \end{pmatrix} & \\ \overset{\text{second column $+$ third column}}{\leadsto} \begin{pmatrix}1 & 0 & 0\\0&2\xi +3&0\\0& 4 \xi +6 & 4+\xi^2 \end{pmatrix} & \overset{\text{third row } - 2 \text{ second row}}{\leadsto} \begin{pmatrix}1 & 0 & 0\\0&2\xi +3&0\\0& 0 & 4+\xi^2 \end{pmatrix}=: B \end{align*} Unfortunately the term $b_{33}$ is not divisible by the term $b_{22}$ as it should be. We have done the steps many many times, but we can't find the error (I'm doing this homework exercise with my classmates). Do you maybe find it? Thank you in advance for any help.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'principal-ideal-domains', 'smith-normal-form']"
37,$A = B\cdot p(A)$. Show $A$ and $B$ commute.,. Show  and  commute.,A = B\cdot p(A) A B,"A problem my professor sent out: Suppose $p$ is a polynomial with constant term nonzero. Suppose   $A,B\in M_n(\mathbb{C})$ such that $A=B\cdot p(A)$. Show that $A$ and   $B$ commute. This is a generalization of the problem: suppose $A + B = AB$. Show $A$ and $B$ commute. Here we can note that $(I-A)=(I-B)^{-1}$. I've been trying to adapt that strategy to the more general case, without luck so far.","A problem my professor sent out: Suppose $p$ is a polynomial with constant term nonzero. Suppose   $A,B\in M_n(\mathbb{C})$ such that $A=B\cdot p(A)$. Show that $A$ and   $B$ commute. This is a generalization of the problem: suppose $A + B = AB$. Show $A$ and $B$ commute. Here we can note that $(I-A)=(I-B)^{-1}$. I've been trying to adapt that strategy to the more general case, without luck so far.",,"['linear-algebra', 'matrices', 'ring-theory']"
38,Angle between two planes in four dimensions,Angle between two planes in four dimensions,,"Suppose I have two planes defined in 4D space, either in terms of vectors spanning the planes, $X = t_1 A_1 + t_2 B_2$ and $X = t_3 A_3 + t_4 B_4$ (where $X$, $A$'s, and $B$'s are vectors with four elements and $t$'s are scalars), or in terms of null space, $[C_1; D_1] X = 0$ and $[C_2; D_2] X = 0$ (where the matrices are $2$ x $4$ and $X$ has four elements). I understand that these two planes generally intersect in just a single point (unless the matrix $[C_1; D_1; C_2; D_2]$ is rank deficient). But is it meaningful to ask what the angle is between the planes? If so, how would it be computed? There is an explicit formula for the 3D case: simply the angle between the normals to the planes. Is there no equivalent explicit expression for the 4D case?","Suppose I have two planes defined in 4D space, either in terms of vectors spanning the planes, $X = t_1 A_1 + t_2 B_2$ and $X = t_3 A_3 + t_4 B_4$ (where $X$, $A$'s, and $B$'s are vectors with four elements and $t$'s are scalars), or in terms of null space, $[C_1; D_1] X = 0$ and $[C_2; D_2] X = 0$ (where the matrices are $2$ x $4$ and $X$ has four elements). I understand that these two planes generally intersect in just a single point (unless the matrix $[C_1; D_1; C_2; D_2]$ is rank deficient). But is it meaningful to ask what the angle is between the planes? If so, how would it be computed? There is an explicit formula for the 3D case: simply the angle between the normals to the planes. Is there no equivalent explicit expression for the 4D case?",,['linear-algebra']
39,My fun conjecture about linearly independence,My fun conjecture about linearly independence,,"In the $\mathbb{R}^n$ vector space, there are distinct $m$ vectors $v_i$'s ($1< i\leq m)$ such that each component has value 0 or 1. Let $A_i$ be the set of $j$'s where $j$-th component of $v_i$ is 1. Also, for each $i \neq j$, $A_i$ and $A_j$ has common $k$ elements. Where $k$ is a given integer $1\leq k <n$. For example, when $n=3, k=1$. $v_1=(1,1,0), v_2=(1,0,1), v_3=(0,1,1)$ satisfy those conditions since $A_1=\{1,2\},A_2=\{1,3\},A_3=\{2,3\}$. My conjecture is : those $v_i$'s are linearly independent. With some rough programming, this conjecture was true when $n \leq 10$. I tried to prove this conjecture with induction on $k$, but I failed. *Some people misunderstood question. Actually question is : For given $n,m,k$, is every families of vectors with above condition are linearly independent. Can you prove or disprove this conjectrue?","In the $\mathbb{R}^n$ vector space, there are distinct $m$ vectors $v_i$'s ($1< i\leq m)$ such that each component has value 0 or 1. Let $A_i$ be the set of $j$'s where $j$-th component of $v_i$ is 1. Also, for each $i \neq j$, $A_i$ and $A_j$ has common $k$ elements. Where $k$ is a given integer $1\leq k <n$. For example, when $n=3, k=1$. $v_1=(1,1,0), v_2=(1,0,1), v_3=(0,1,1)$ satisfy those conditions since $A_1=\{1,2\},A_2=\{1,3\},A_3=\{2,3\}$. My conjecture is : those $v_i$'s are linearly independent. With some rough programming, this conjecture was true when $n \leq 10$. I tried to prove this conjecture with induction on $k$, but I failed. *Some people misunderstood question. Actually question is : For given $n,m,k$, is every families of vectors with above condition are linearly independent. Can you prove or disprove this conjectrue?",,"['linear-algebra', 'combinatorics']"
40,Gram-Schmidt process on complex space,Gram-Schmidt process on complex space,,"Let $\mathbb{C}^3$ be equipped with the standard complex inner product. Apply the   Gram-Schmidt process to the basis: $v_1=(1,0,i)^t$, $v_2=(-1,i,1)^t$, $v_3=(0,-1,i+1)^t$ to find an orthonormal basis $\{u_1,u_2,u_3\}$. I have found $u_1 = \dfrac{1}{\sqrt{2}} (1,0,i)^t$ and $u_2 = \dfrac{1}{\sqrt{2}}\left(\dfrac{i-1}{2},0,\dfrac{i+1}{2}\right)^t$. I then try to use the following formulae to work out $u_3$ but I keep going wrong and can't figure out why: $w_3 = v_3 - \langle v_3, u_1\rangle u_1 - \langle v_2, u_2\rangle u_2$ and $u_3 = \dfrac{w_3}{\|w_3\|}$.","Let $\mathbb{C}^3$ be equipped with the standard complex inner product. Apply the   Gram-Schmidt process to the basis: $v_1=(1,0,i)^t$, $v_2=(-1,i,1)^t$, $v_3=(0,-1,i+1)^t$ to find an orthonormal basis $\{u_1,u_2,u_3\}$. I have found $u_1 = \dfrac{1}{\sqrt{2}} (1,0,i)^t$ and $u_2 = \dfrac{1}{\sqrt{2}}\left(\dfrac{i-1}{2},0,\dfrac{i+1}{2}\right)^t$. I then try to use the following formulae to work out $u_3$ but I keep going wrong and can't figure out why: $w_3 = v_3 - \langle v_3, u_1\rangle u_1 - \langle v_2, u_2\rangle u_2$ and $u_3 = \dfrac{w_3}{\|w_3\|}$.",,"['linear-algebra', 'gram-schmidt']"
41,"A is similar to $A^k$, then each eigenvalue of $A$ is a root of unity","A is similar to , then each eigenvalue of  is a root of unity",A^k A,"Let $A \in \mathbb{C}(n,n)$ and $k \geq 2$ be an integer such that $$A \sim A^k$$ . Show that if $A$ is non-singular then each eigenvalue of $A$ is a root of unity. Attempt: Since $A \sim A^k$ , $$PA = A^kP$$ where $P$ is an invertible matrix. Since $A$ is invertible, $0$ cannot be an eigenvalue of $A$ . Suppose $$Av = \lambda v \quad v \neq 0$$ then $$PAv = \lambda Pv$$ $$\therefore A^k(Pv) = \lambda (Pv) $$ which  implies that $Pv$ is an eigenvector of $A^k$ . But the eigenvalues of $A^k$ are $\lambda^k$ $$\therefore \lambda^k=\lambda$$ which gives the conclusion required. My questions is: Is the logic correct? If so, Am I missing any details? If not, then how could I approach this? Thanks!","Let and be an integer such that . Show that if is non-singular then each eigenvalue of is a root of unity. Attempt: Since , where is an invertible matrix. Since is invertible, cannot be an eigenvalue of . Suppose then which  implies that is an eigenvector of . But the eigenvalues of are which gives the conclusion required. My questions is: Is the logic correct? If so, Am I missing any details? If not, then how could I approach this? Thanks!","A \in \mathbb{C}(n,n) k \geq 2 A \sim A^k A A A \sim A^k PA = A^kP P A 0 A Av = \lambda v \quad v \neq 0 PAv = \lambda Pv \therefore A^k(Pv) = \lambda (Pv)  Pv A^k A^k \lambda^k \therefore \lambda^k=\lambda","['linear-algebra', 'eigenvalues-eigenvectors']"
42,What's special about characteristic 2?,What's special about characteristic 2?,,"I'm trying to get the big picture of how bilinear forms and quadratic forms relate over fields $F$ with $char(F) = 2$ and fields with $char(F) \neq 2$. What I gather so far is that if $char(F) \neq 2$ then there is a one-to-one correspondence between quadratic forms and bilinear forms so that the corresponding theories are equivalent. However, if $char(F) = 2$, while it's still possible to define a bilinear form in terms of a quadratic form it's no longer possible to recover the quadratic form from it. Is this irreversibility what distinguishes the case $char(F) = 2$ from $char(F) \neq 2$ or is there a more fundamental difference that I'm not aware of?","I'm trying to get the big picture of how bilinear forms and quadratic forms relate over fields $F$ with $char(F) = 2$ and fields with $char(F) \neq 2$. What I gather so far is that if $char(F) \neq 2$ then there is a one-to-one correspondence between quadratic forms and bilinear forms so that the corresponding theories are equivalent. However, if $char(F) = 2$, while it's still possible to define a bilinear form in terms of a quadratic form it's no longer possible to recover the quadratic form from it. Is this irreversibility what distinguishes the case $char(F) = 2$ from $char(F) \neq 2$ or is there a more fundamental difference that I'm not aware of?",,"['linear-algebra', 'abstract-algebra', 'algebraic-number-theory']"
43,Basis for $\mathbb{[Q(\pi):Q]}$,Basis for,\mathbb{[Q(\pi):Q]},I'm trying to figure out whether the basis of $\mathbb{Q}(x)$ over $\mathbb{Q}$ is countable when $x$ is transcendental. I know that the elements in $\mathbb{Q}(x)$ will be rational functions in $x$ and so they are countable like algebraic numbers. Let the rank be the sum of coefficients and degrees of polynomials in denominator and numerator. So it passes the necessary condition for countable basis outlined by Asaf Karagila in Countable/uncountable basis of vector space . I have no more ideas to go with. Edit: Obviously a basis will have to be countable if the space is countable. So how do I go about finding a basis?,I'm trying to figure out whether the basis of $\mathbb{Q}(x)$ over $\mathbb{Q}$ is countable when $x$ is transcendental. I know that the elements in $\mathbb{Q}(x)$ will be rational functions in $x$ and so they are countable like algebraic numbers. Let the rank be the sum of coefficients and degrees of polynomials in denominator and numerator. So it passes the necessary condition for countable basis outlined by Asaf Karagila in Countable/uncountable basis of vector space . I have no more ideas to go with. Edit: Obviously a basis will have to be countable if the space is countable. So how do I go about finding a basis?,,"['linear-algebra', 'vector-spaces', 'rational-functions']"
44,Is the inverse of an invertible totally unimodular matrix also totally unimodular?,Is the inverse of an invertible totally unimodular matrix also totally unimodular?,,"My question is learned from here . Let me restate it as follows: A unimodular matrix $M$ is a square integer matrix having determinant $+1$ or $1$. A totally unimodular matrix (TU matrix) is a matrix for which every square non-singular submatrix is unimodular. Now suppose an $n\times n$ non-singular matrix $A$ is totally unimodular. Can we prove that $A^{1}$ is also totally unimodular? Or if it is not correct, can we have a counterexample? Any help is much appreciated. Edit : What I have already known is that the statement is true when $n=2$ and $3$. It follows from the definition of unimodular matrix and the fact that if $A$ is unimodular, then $$A^{-1}=\det A\cdot \mathrm{adj}(A)=\pm \mathrm{adj}(A).$$","My question is learned from here . Let me restate it as follows: A unimodular matrix $M$ is a square integer matrix having determinant $+1$ or $1$. A totally unimodular matrix (TU matrix) is a matrix for which every square non-singular submatrix is unimodular. Now suppose an $n\times n$ non-singular matrix $A$ is totally unimodular. Can we prove that $A^{1}$ is also totally unimodular? Or if it is not correct, can we have a counterexample? Any help is much appreciated. Edit : What I have already known is that the statement is true when $n=2$ and $3$. It follows from the definition of unimodular matrix and the fact that if $A$ is unimodular, then $$A^{-1}=\det A\cdot \mathrm{adj}(A)=\pm \mathrm{adj}(A).$$",,"['linear-algebra', 'linear-programming', 'integer-programming']"
45,"In stating that the union of vector subspaces is a subspace iff they are ordered, why require $F$ finite?","In stating that the union of vector subspaces is a subspace iff they are ordered, why require  finite?",F,"On the bottom of page 38 of Roman's Advanced Linear Algebra is written the following (here $V$ is a vector space over the field $F$ and $\mathcal{S}(V)$ is the set of linear subspaces of $V$): ""...if $S$, $T\in \mathcal{S}(V)$ (and $F$ is infinite), then $S \cup T\in \mathcal{S}(V)$ iff $S \subseteq T$ or $T \subseteq S$."" I cannot for the life of me figure out why the finiteness of $F$ is mentioned; it seems irrelevant. The proof of $\Leftarrow$ is obvious in any case, and the proof of $\Rightarrow$ can be done by contrapositive as follows: take $s \in S \setminus T$ and $t \in T \setminus S$ and note $s+t \in T \Rightarrow s\in T$ (contradiction) and similarly for $S$, so $s+t \notin S \cup T$. How could this argument break down for finite $F$?","On the bottom of page 38 of Roman's Advanced Linear Algebra is written the following (here $V$ is a vector space over the field $F$ and $\mathcal{S}(V)$ is the set of linear subspaces of $V$): ""...if $S$, $T\in \mathcal{S}(V)$ (and $F$ is infinite), then $S \cup T\in \mathcal{S}(V)$ iff $S \subseteq T$ or $T \subseteq S$."" I cannot for the life of me figure out why the finiteness of $F$ is mentioned; it seems irrelevant. The proof of $\Leftarrow$ is obvious in any case, and the proof of $\Rightarrow$ can be done by contrapositive as follows: take $s \in S \setminus T$ and $t \in T \setminus S$ and note $s+t \in T \Rightarrow s\in T$ (contradiction) and similarly for $S$, so $s+t \notin S \cup T$. How could this argument break down for finite $F$?",,"['linear-algebra', 'vector-spaces']"
46,Relate the singular values of $A$ and $\frac{A^T+A}{2}$,Relate the singular values of  and,A \frac{A^T+A}{2},Consider a square matrix with real entries $A\in\mathbb{R}^{n\times n}$ such that the symmetric matrix $\frac{A^T+A}{2}$ is positive definite. Is it possible to find a relationship linking together the singular values of $\frac{A^T+A}{2}$ with the singular values of $A$? The relationship can either be an equality or an inequality. EDIT: I've tried some numerical examples in some $2 \times 2$ matrices and it appears that when $(A^T+A)/2$  is positive definite then the two singular values are perturbed by the same amount. For example: Example 1: $$ G=\begin{bmatrix} 100 &10\\4 & 5\end{bmatrix} $$ Singular values of $G$: $$ 4.5726253611256148668420314380021\\  100.59866349662300975905062166134$$ $$ \frac{G^T+G}{2}=\begin{bmatrix} 100 &7\\7& 5\end{bmatrix} $$ Singular values of $\frac{G^T+G}{2}$: $$  4.4869809322513025538957048883323 \\  100.51301906774869744610429511167$$ The difference is: $$0.085644428874312312946326549669858\\  0.085644428874312312946326549669858$$ Example 2: $$ G=\begin{bmatrix} 20 &8\\2 & 7\end{bmatrix} $$ Singular values of $G$: $$  5.6287069525109683834660548089479\\   22.02992641936769354654076803634$$ $$ \frac{G^T+G}{2}=\begin{bmatrix} 20 &5\\5& 7\end{bmatrix} $$ Singular values of $\frac{G^T+G}{2}$: $$  5.2993902665716374184626433863038\\  21.700609733428362581537356613696$$ The difference is: $$  0.32931668593933096500341142264419\\  0.32931668593933096500341142264419$$,Consider a square matrix with real entries $A\in\mathbb{R}^{n\times n}$ such that the symmetric matrix $\frac{A^T+A}{2}$ is positive definite. Is it possible to find a relationship linking together the singular values of $\frac{A^T+A}{2}$ with the singular values of $A$? The relationship can either be an equality or an inequality. EDIT: I've tried some numerical examples in some $2 \times 2$ matrices and it appears that when $(A^T+A)/2$  is positive definite then the two singular values are perturbed by the same amount. For example: Example 1: $$ G=\begin{bmatrix} 100 &10\\4 & 5\end{bmatrix} $$ Singular values of $G$: $$ 4.5726253611256148668420314380021\\  100.59866349662300975905062166134$$ $$ \frac{G^T+G}{2}=\begin{bmatrix} 100 &7\\7& 5\end{bmatrix} $$ Singular values of $\frac{G^T+G}{2}$: $$  4.4869809322513025538957048883323 \\  100.51301906774869744610429511167$$ The difference is: $$0.085644428874312312946326549669858\\  0.085644428874312312946326549669858$$ Example 2: $$ G=\begin{bmatrix} 20 &8\\2 & 7\end{bmatrix} $$ Singular values of $G$: $$  5.6287069525109683834660548089479\\   22.02992641936769354654076803634$$ $$ \frac{G^T+G}{2}=\begin{bmatrix} 20 &5\\5& 7\end{bmatrix} $$ Singular values of $\frac{G^T+G}{2}$: $$  5.2993902665716374184626433863038\\  21.700609733428362581537356613696$$ The difference is: $$  0.32931668593933096500341142264419\\  0.32931668593933096500341142264419$$,,"['linear-algebra', 'matrices']"
47,Nonnegative orthogonal matrices,Nonnegative orthogonal matrices,,Assume that $A \in \mathbb{R}^{n \times n}$ has nonnegative entries and $AA^T = I_n$ where $I_n$ is the identity matrix. Is it true that $A$ should be a permutation matrix? EDIT: I seem to have a proof for doubly stochastic matrices based on the Birkhoff theorem. Here is another related question: Is the set of nonnegative matrices the conic hull of permutation matrices?,Assume that $A \in \mathbb{R}^{n \times n}$ has nonnegative entries and $AA^T = I_n$ where $I_n$ is the identity matrix. Is it true that $A$ should be a permutation matrix? EDIT: I seem to have a proof for doubly stochastic matrices based on the Birkhoff theorem. Here is another related question: Is the set of nonnegative matrices the conic hull of permutation matrices?,,"['linear-algebra', 'matrices', 'orthogonal-matrices', 'nonnegative-matrices', 'permutation-matrices']"
48,Linear Algebra and Geometry by Kostrikin and Manin: Remark regarding diagrams and graphic representations.,Linear Algebra and Geometry by Kostrikin and Manin: Remark regarding diagrams and graphic representations.,,"On page 5 of this book there is a particular section of the book that I am having trouble trying to understand as to what the authors' are trying to point across. It is concerning linear algebra. I will place in bold the parts I need additional explaining and number them as (1),(2),(3) which will be associated with the numbered questions below. So it begins like this: Remarks regarding diagrams and graphic representations. Many general concepts and theorems of linear algebra are conveniently illustrated   by diagrams and pictures. We want to warn the reader immediately about   the dangers (1) of such illustrations. a)Low dimensionality. We live in a three-dimensional space and our   diagrams usually portray two- or three-dimensional images. In linear   algebra we work with space of any finite number of dimensions and in   functional analysis we work with infinite-dimensional spaces. Our   ""low-dimensional"" intuition can be greatly developed, but it must be developed systematically (2). Here is a simple example how are we to   imagine the general arrangement of two planes in four-dimensional   space ? Imagine two planes in $\mathbb{R}^3$ intersecting along a   straight line which splay out everywhere along this straight line   except at the origin, vanishing into the fourth dimension (3). 1) How does the particular example above show the dangers of such an illustration? 2)The author didn't elaborate much on this point. What does it mean to develop such intuition systematically and how? 3) I'm not quite sure what the author is trying to say about this, and with no pictures in the book it is quite difficult for me to figure out what it trying to be put across by the authors. If someone could try to explain so that I can have a mental ""picture"" in my head what is actually intended by the author. Diagrams and pictures accompanying an explanation would also be greatly appreciated (though one is not obligated to provide one.) NB: I guess part of the reason why I don't fully capture what the author is trying to get across is because I can't quite get my head around the example about the two planes in four dimensional space.","On page 5 of this book there is a particular section of the book that I am having trouble trying to understand as to what the authors' are trying to point across. It is concerning linear algebra. I will place in bold the parts I need additional explaining and number them as (1),(2),(3) which will be associated with the numbered questions below. So it begins like this: Remarks regarding diagrams and graphic representations. Many general concepts and theorems of linear algebra are conveniently illustrated   by diagrams and pictures. We want to warn the reader immediately about   the dangers (1) of such illustrations. a)Low dimensionality. We live in a three-dimensional space and our   diagrams usually portray two- or three-dimensional images. In linear   algebra we work with space of any finite number of dimensions and in   functional analysis we work with infinite-dimensional spaces. Our   ""low-dimensional"" intuition can be greatly developed, but it must be developed systematically (2). Here is a simple example how are we to   imagine the general arrangement of two planes in four-dimensional   space ? Imagine two planes in $\mathbb{R}^3$ intersecting along a   straight line which splay out everywhere along this straight line   except at the origin, vanishing into the fourth dimension (3). 1) How does the particular example above show the dangers of such an illustration? 2)The author didn't elaborate much on this point. What does it mean to develop such intuition systematically and how? 3) I'm not quite sure what the author is trying to say about this, and with no pictures in the book it is quite difficult for me to figure out what it trying to be put across by the authors. If someone could try to explain so that I can have a mental ""picture"" in my head what is actually intended by the author. Diagrams and pictures accompanying an explanation would also be greatly appreciated (though one is not obligated to provide one.) NB: I guess part of the reason why I don't fully capture what the author is trying to get across is because I can't quite get my head around the example about the two planes in four dimensional space.",,['linear-algebra']
49,are there any bounds on the eigenvalues of products of positive-semidefinite matrices?,are there any bounds on the eigenvalues of products of positive-semidefinite matrices?,,"I have real positive semidefinite matrices (symmetric) $A$ and $B$, both are $n \times n$. I am looking for upper bounds and lower bounds on the $m$th largest eigenvalue of $AB$, in terms of the eigenvalues of $A$ and $B$. I know, for example, that the $\det(AB) = \det(A)\det(B)$ and that the determinant of a square matrix equals the product of its eigenvalues, but that wouldn't give me what I am looking for, because I want to focus only on the top $m$ largest eigenvalues of $AB$.","I have real positive semidefinite matrices (symmetric) $A$ and $B$, both are $n \times n$. I am looking for upper bounds and lower bounds on the $m$th largest eigenvalue of $AB$, in terms of the eigenvalues of $A$ and $B$. I know, for example, that the $\det(AB) = \det(A)\det(B)$ and that the determinant of a square matrix equals the product of its eigenvalues, but that wouldn't give me what I am looking for, because I want to focus only on the top $m$ largest eigenvalues of $AB$.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'spectral-theory']"
50,"How can I prove $\mathrm{max \, mag}(A)=\frac{1}{\mathrm{min \, mag}(A^{-1})}$, and $\mathrm{max \, mag}(A^{-1})=\frac{1}{\mathrm{min \, mag}(A)}$?","How can I prove , and ?","\mathrm{max \, mag}(A)=\frac{1}{\mathrm{min \, mag}(A^{-1})} \mathrm{max \, mag}(A^{-1})=\frac{1}{\mathrm{min \, mag}(A)}","Using $\mathrm{max \, mag}(A)=\max_{x\neq 0}\frac{\|Ax\|}{\|x\|}$ , and $\mathrm{min \, mag}(A)=\min_{x\neq 0}\frac{\|Ax\|}{\|x\|}$ I found this quite simple to prove using a proposition stating that $$\kappa(A)=\frac{\mathrm{max \, mag}(A)}{\mathrm{min \, mag}(A)}$$ for all nonsingular A. However, I think that propostion follows from the conclusion I am trying to prove. I don't think I can use it because I can't prove it without the using relationship I am trying to use it to prove, if that makes sense. How can I use only the definitions of maxmag and minmag to prove this?","Using , and I found this quite simple to prove using a proposition stating that for all nonsingular A. However, I think that propostion follows from the conclusion I am trying to prove. I don't think I can use it because I can't prove it without the using relationship I am trying to use it to prove, if that makes sense. How can I use only the definitions of maxmag and minmag to prove this?","\mathrm{max \, mag}(A)=\max_{x\neq 0}\frac{\|Ax\|}{\|x\|} \mathrm{min \, mag}(A)=\min_{x\neq 0}\frac{\|Ax\|}{\|x\|} \kappa(A)=\frac{\mathrm{max \, mag}(A)}{\mathrm{min \, mag}(A)}","['linear-algebra', 'numerical-linear-algebra']"
51,Is there a unique solution for this quadratic matrix equation?,Is there a unique solution for this quadratic matrix equation?,,"Here is the quadratic matrix equation I've been looking at lately: $$ Q_{r,r}=A_{r,r}X_{r,r}^2+B_{r,r}X_{r,r}+C_{r,r}=0_{r,r} $$ Note that $A, B, C,$ and $X$ are $r \times r$ matrices. $A$ contains known elements, $B$ contains known elements, $C$ contains known elements, and $X$ contains the unknown elements that you are solving for. $ 0_{r,r} $ is just the $r \times r$ null matrix. Is there any solution for $X$ in terms of $A, B,$ and $C$ (making no easy assumptions)? (e.g. $X$ is a diagonal matrix, $A=B=C$, or anything of that sort.) I have tried to solve this and nothing has worked out. I attempted solving it generally by manipulating the matrices in variable form (i.e. actually writing out the matrices $A, B, C,$ and $X$ in variables) and finding a unique solution for all of the elements of $X$ in terms of the elements of $A, B,$ and $C$. That didn't work out beyond the case of $r=1$. Trying to solve it by looking at $r$ at different values did not work out either; I ended up with very abysmal equations at just $r=2$. I don't know exactly how to make this appealing to the denizens of math.stackexchange, but it (as far as I know) isn't a heavily studied problem. There is a very high possibility that I've just been doing elementary techniques and nothing of note, so I hope someone or a group of people could shed light on this.","Here is the quadratic matrix equation I've been looking at lately: $$ Q_{r,r}=A_{r,r}X_{r,r}^2+B_{r,r}X_{r,r}+C_{r,r}=0_{r,r} $$ Note that $A, B, C,$ and $X$ are $r \times r$ matrices. $A$ contains known elements, $B$ contains known elements, $C$ contains known elements, and $X$ contains the unknown elements that you are solving for. $ 0_{r,r} $ is just the $r \times r$ null matrix. Is there any solution for $X$ in terms of $A, B,$ and $C$ (making no easy assumptions)? (e.g. $X$ is a diagonal matrix, $A=B=C$, or anything of that sort.) I have tried to solve this and nothing has worked out. I attempted solving it generally by manipulating the matrices in variable form (i.e. actually writing out the matrices $A, B, C,$ and $X$ in variables) and finding a unique solution for all of the elements of $X$ in terms of the elements of $A, B,$ and $C$. That didn't work out beyond the case of $r=1$. Trying to solve it by looking at $r$ at different values did not work out either; I ended up with very abysmal equations at just $r=2$. I don't know exactly how to make this appealing to the denizens of math.stackexchange, but it (as far as I know) isn't a heavily studied problem. There is a very high possibility that I've just been doing elementary techniques and nothing of note, so I hope someone or a group of people could shed light on this.",,"['linear-algebra', 'matrices', 'matrix-equations']"
52,Diagonalisable matrix,Diagonalisable matrix,,"Let $A$ be a $n\times n$ matrix with complex entries. Suppose that $\operatorname{tr}(A)=\operatorname{tr}(A^{2})=\cdots \ =\operatorname{tr}(A^{n-1})=0  $, and $A^{n}\neq 0 $. Then $A$ is diagonalisable.","Let $A$ be a $n\times n$ matrix with complex entries. Suppose that $\operatorname{tr}(A)=\operatorname{tr}(A^{2})=\cdots \ =\operatorname{tr}(A^{n-1})=0  $, and $A^{n}\neq 0 $. Then $A$ is diagonalisable.",,['linear-algebra']
53,Showing a subgroup of $\mathbb{R}^n$ is a free abelian group on $\leq n$ generators,Showing a subgroup of  is a free abelian group on  generators,\mathbb{R}^n \leq n,"I'm a bit stuck on Exercise III.5 of Lang's Algebra . (Page 166.) Let $A$ be an additive subgroup of Euclidean space $\mathbb R^n$, and assume that in every bounded region of space, there is only a finite number of elements of $A$. Show that $A$ is a free abelian group on $\leq n$ generators. [ Hint: Induction on the maximal number of linearly independent elements of $A$ over $\mathbb R$. Let $v_1, \ldots, v_m$ be a maximal set of such elements, and let $A_0$ be the subgroup of $A$ contained in the $\mathbb R$-space generated by $v_1, \ldots, v_{m-1}$. By induction, one may assume that any element of $A_0$ is a linear integral combination of $v_1, \ldots, v_{m-1}$. Let $S$ be the subset of elements $v \in A$ of the form $v= a_1 v_1 + \cdots + a_m v_m$ with real coefficients $a_i$ satisfying    $$ \begin{eqnarray*} 0 \leq a_i < 1, &\ \ \text{if } i=1,2,\ldots, m-1 \\ 0 \leq a_m \leq 1 .& \end{eqnarray*} $$   If $v'_m$ is an element of $S$ with the smallest $a_m \neq 0$, show that $\{ v_1, \ldots, v_{m-1}, v'_m \}$ is a basis of $A$ over $\mathbb Z$.] Following the hint, I suppose $c_1v_1+\cdots+c_{m-1}v_{m-1}+c_mv'_m=0$ for $c_i\in\mathbb{Z}$. Then $$ c_1v_1+\cdots+c_{m-1}v_{m-1}+c_m(a_1v_1+\cdots+a_mv_m)=0 $$ implies $$ (c_1+a_mc_1)v_1+\cdots+(c_{m-1}+c_ma_{m-1})v_{m-1}+c_ma_mv_m=0. $$ But $v_1,\dots,v_m$ are linearly independent, so $c_ma_m=0$, thus $c_m=0$ since $a_m\neq 0$. Then since $c_i+c_ma_i=0$ for all other $i$, $c_i=0$ for $i=1,\dots,m-1$, and the vectors are linearly independent over $\mathbb{Z}$. I'm trying to show $\{v_1,\dots,v_{m-1},v'_m\}$ also span $A$ over $\mathbb{Z}$. Since $v_1,\dots,v_m$ is a maximal linearly independent set, I think I can write any $x\in A$ as $$ x=c_1v_1+\cdots+c_mv_m,\quad c_i\in\mathbb{R}. $$ I realized $$ x=(\lfloor c_1\rfloor v_1+\cdots+\lfloor c_m\rfloor v_m)+(c'_1v_1+\cdots+c'_mv_m) $$ where $\lfloor \cdot\rfloor$ is the floor function, and $0\leq c'_i<1$. So the second summand in parentheses is in $S$, and the first summand is a linear integral combination of $v_1,\dots,v_m$. I'm not sure if this observation leads anywhere, and I'm not sure where the fact that $A$ has only finitely many elements in every bounded region of space comes in. What's the right way to proceed? Thanks. Added: With Arturo Magidin's help, $ka_m=c'_m$ for some positive integer $k$. Thus $$ x=(\lfloor c_1\rfloor v_1+\cdots+\lfloor c_m\rfloor v_m)+k((c'_1/k)v_1+\cdots+a_mv_m), $$ so taking $v'_m=(c'_1/k)v_1+\cdots+a_mv_m$, $\{v_1,\dots,v_m,v'_m\}$ is a spanning set of $A$ over $\mathbb{Z}$. How can I show from this that $\{v_1,\dots,v_{m-1},v'_m\}$ spans $A$ over $\mathbb{Z}$?","I'm a bit stuck on Exercise III.5 of Lang's Algebra . (Page 166.) Let $A$ be an additive subgroup of Euclidean space $\mathbb R^n$, and assume that in every bounded region of space, there is only a finite number of elements of $A$. Show that $A$ is a free abelian group on $\leq n$ generators. [ Hint: Induction on the maximal number of linearly independent elements of $A$ over $\mathbb R$. Let $v_1, \ldots, v_m$ be a maximal set of such elements, and let $A_0$ be the subgroup of $A$ contained in the $\mathbb R$-space generated by $v_1, \ldots, v_{m-1}$. By induction, one may assume that any element of $A_0$ is a linear integral combination of $v_1, \ldots, v_{m-1}$. Let $S$ be the subset of elements $v \in A$ of the form $v= a_1 v_1 + \cdots + a_m v_m$ with real coefficients $a_i$ satisfying    $$ \begin{eqnarray*} 0 \leq a_i < 1, &\ \ \text{if } i=1,2,\ldots, m-1 \\ 0 \leq a_m \leq 1 .& \end{eqnarray*} $$   If $v'_m$ is an element of $S$ with the smallest $a_m \neq 0$, show that $\{ v_1, \ldots, v_{m-1}, v'_m \}$ is a basis of $A$ over $\mathbb Z$.] Following the hint, I suppose $c_1v_1+\cdots+c_{m-1}v_{m-1}+c_mv'_m=0$ for $c_i\in\mathbb{Z}$. Then $$ c_1v_1+\cdots+c_{m-1}v_{m-1}+c_m(a_1v_1+\cdots+a_mv_m)=0 $$ implies $$ (c_1+a_mc_1)v_1+\cdots+(c_{m-1}+c_ma_{m-1})v_{m-1}+c_ma_mv_m=0. $$ But $v_1,\dots,v_m$ are linearly independent, so $c_ma_m=0$, thus $c_m=0$ since $a_m\neq 0$. Then since $c_i+c_ma_i=0$ for all other $i$, $c_i=0$ for $i=1,\dots,m-1$, and the vectors are linearly independent over $\mathbb{Z}$. I'm trying to show $\{v_1,\dots,v_{m-1},v'_m\}$ also span $A$ over $\mathbb{Z}$. Since $v_1,\dots,v_m$ is a maximal linearly independent set, I think I can write any $x\in A$ as $$ x=c_1v_1+\cdots+c_mv_m,\quad c_i\in\mathbb{R}. $$ I realized $$ x=(\lfloor c_1\rfloor v_1+\cdots+\lfloor c_m\rfloor v_m)+(c'_1v_1+\cdots+c'_mv_m) $$ where $\lfloor \cdot\rfloor$ is the floor function, and $0\leq c'_i<1$. So the second summand in parentheses is in $S$, and the first summand is a linear integral combination of $v_1,\dots,v_m$. I'm not sure if this observation leads anywhere, and I'm not sure where the fact that $A$ has only finitely many elements in every bounded region of space comes in. What's the right way to proceed? Thanks. Added: With Arturo Magidin's help, $ka_m=c'_m$ for some positive integer $k$. Thus $$ x=(\lfloor c_1\rfloor v_1+\cdots+\lfloor c_m\rfloor v_m)+k((c'_1/k)v_1+\cdots+a_mv_m), $$ so taking $v'_m=(c'_1/k)v_1+\cdots+a_mv_m$, $\{v_1,\dots,v_m,v'_m\}$ is a spanning set of $A$ over $\mathbb{Z}$. How can I show from this that $\{v_1,\dots,v_{m-1},v'_m\}$ spans $A$ over $\mathbb{Z}$?",,"['linear-algebra', 'modules']"
54,Properties of a matrix whose row vectors are dependent,Properties of a matrix whose row vectors are dependent,,"When a column vector in a matrix is a made up of ""combination"" of its other column vectors, it is said to be linearly dependant. Say... $$ A=\begin{bmatrix} 2 & 1 & 0\\  4 & 5 & -6\\  3 & 1 & 1 \end{bmatrix} $$ $$ 1\begin{bmatrix} 2\\  4\\  3 \end{bmatrix}-2\begin{bmatrix} 1\\  5\\  1 \end{bmatrix}=\begin{bmatrix} 0\\  -6\\  1 \end{bmatrix}  $$ Otherwise, it is linearly independent. And being linearly dependent, it has the properties of being a singular matrix and therefore may have an infinite solutions or no solutions at all depending on the result matrix. Then being linearly independent, the matrix is more often a good matrix that can span the entire $R^{n}$ space and has a unique solution to its system of equations. Then I just thinking what happens if a row vector in a matrix is made up of ""combination"" of its other row vectors? Say... $$P=\begin{bmatrix} 2 & 5 & 1\\  12 & 13 & 3\\  8 & 3 & 1 \end{bmatrix} $$ $$ 2\begin{bmatrix} 2 & 5 & 1 \end{bmatrix}+1\begin{bmatrix} 8 & 3 & 1 \end{bmatrix}=\begin{bmatrix} 12 & 13 & 3 \end{bmatrix} $$ Does a matrix have any special properties too if its row vectors are linearly independent and linearly dependent? Thanks!","When a column vector in a matrix is a made up of ""combination"" of its other column vectors, it is said to be linearly dependant. Say... $$ A=\begin{bmatrix} 2 & 1 & 0\\  4 & 5 & -6\\  3 & 1 & 1 \end{bmatrix} $$ $$ 1\begin{bmatrix} 2\\  4\\  3 \end{bmatrix}-2\begin{bmatrix} 1\\  5\\  1 \end{bmatrix}=\begin{bmatrix} 0\\  -6\\  1 \end{bmatrix}  $$ Otherwise, it is linearly independent. And being linearly dependent, it has the properties of being a singular matrix and therefore may have an infinite solutions or no solutions at all depending on the result matrix. Then being linearly independent, the matrix is more often a good matrix that can span the entire $R^{n}$ space and has a unique solution to its system of equations. Then I just thinking what happens if a row vector in a matrix is made up of ""combination"" of its other row vectors? Say... $$P=\begin{bmatrix} 2 & 5 & 1\\  12 & 13 & 3\\  8 & 3 & 1 \end{bmatrix} $$ $$ 2\begin{bmatrix} 2 & 5 & 1 \end{bmatrix}+1\begin{bmatrix} 8 & 3 & 1 \end{bmatrix}=\begin{bmatrix} 12 & 13 & 3 \end{bmatrix} $$ Does a matrix have any special properties too if its row vectors are linearly independent and linearly dependent? Thanks!",,"['linear-algebra', 'matrices', 'vector-spaces']"
55,3 variable systems,3 variable systems,,"$$\begin{align*} x + 2y - z &= -3\\ 2x - 4y + z &= -7\\ -2x + 2y - 3z &= 4 \end{align*}$$ I multiply (1) by $-2$ and get $-2x-4y+2z=6$ and then add it to (2) which gives me $-8y+3z = -1$. I add (2) and (3) and get $-2y-2z=-3$ I take the first result and get  $$y=\frac{-1-3z}{8}$$ putting that into $-2y-2z=-3$ I get $$\frac{-2(-1-3z)}{8-2z}=-3$$ which gives me $2+6z-16z = -24$, which gives me $10z=-26$ which I know isn't right already. I messed up the math somewhere, but I have done this problem about a dozen times and always get the wrong answer no matter what.","$$\begin{align*} x + 2y - z &= -3\\ 2x - 4y + z &= -7\\ -2x + 2y - 3z &= 4 \end{align*}$$ I multiply (1) by $-2$ and get $-2x-4y+2z=6$ and then add it to (2) which gives me $-8y+3z = -1$. I add (2) and (3) and get $-2y-2z=-3$ I take the first result and get  $$y=\frac{-1-3z}{8}$$ putting that into $-2y-2z=-3$ I get $$\frac{-2(-1-3z)}{8-2z}=-3$$ which gives me $2+6z-16z = -24$, which gives me $10z=-26$ which I know isn't right already. I messed up the math somewhere, but I have done this problem about a dozen times and always get the wrong answer no matter what.",,"['linear-algebra', 'algebra-precalculus']"
56,Perron-Frobenius Theorem and Graph Laplacians,Perron-Frobenius Theorem and Graph Laplacians,,"How can the Perron-Frobenius theorem be used to show that for a connected graph, there is a simple eigenvector that is (i) real and (ii) smallest in magnitude and (iii) has an associated eigenvector that is positive? The graph Laplacian is given as $L = D-A$, where $A$ is the non-negative adjacency matrix of the graph.  The Perron-Frobenius theorem allows us to state that $\rho(A) > 0$ and is a simple eigenvalue of $A$ $Ax = \rho(A)x$ with all elements of $x$ positive. The matrix $D$ is diagonal with positive elements.  It is well-known that for a connected graph, 0 is the smallest eigenvalue of $L$ and it is simple, and $x=\mathbb{1}$ (vector of all ones) is the associated positive eigenvector. I am confused mainly because $L$ is no longer a non-negative (or non-positive) matrix.  Any ideas?","How can the Perron-Frobenius theorem be used to show that for a connected graph, there is a simple eigenvector that is (i) real and (ii) smallest in magnitude and (iii) has an associated eigenvector that is positive? The graph Laplacian is given as $L = D-A$, where $A$ is the non-negative adjacency matrix of the graph.  The Perron-Frobenius theorem allows us to state that $\rho(A) > 0$ and is a simple eigenvalue of $A$ $Ax = \rho(A)x$ with all elements of $x$ positive. The matrix $D$ is diagonal with positive elements.  It is well-known that for a connected graph, 0 is the smallest eigenvalue of $L$ and it is simple, and $x=\mathbb{1}$ (vector of all ones) is the associated positive eigenvector. I am confused mainly because $L$ is no longer a non-negative (or non-positive) matrix.  Any ideas?",,"['linear-algebra', 'matrices', 'graph-theory', 'graph-laplacian']"
57,Diagonal update of the inverse of $XDX^T$,Diagonal update of the inverse of,XDX^T,"I have a matrix $F=XDX^T$, where $D$ is $m\times m$ and diagonal and $X$ is $n\times m$. Now, I compute $F^{-1}$. Is there an efficient method to update $F^{-1}$ if $D$ is updated by $D'=D+G$, where $G$ is a sparse diagonal matrix?","I have a matrix $F=XDX^T$, where $D$ is $m\times m$ and diagonal and $X$ is $n\times m$. Now, I compute $F^{-1}$. Is there an efficient method to update $F^{-1}$ if $D$ is updated by $D'=D+G$, where $G$ is a sparse diagonal matrix?",,"['linear-algebra', 'matrices']"
58,Characterization of Determinant using multiplicativity and image of diagonal matrices,Characterization of Determinant using multiplicativity and image of diagonal matrices,,"I found a question in the very end of this video, which essentially is the following: Let $n$ be some natural number. Suppose there is a function $f : \mathbb{R}^{n \times n} \rightarrow \mathbb{R}$ such that it satisfies the following 2 properties: $f(A\cdot B) = f(A)\cdot f(B),\ \forall A,B \in \mathbb{R}^{n \times n} $ $f(\text{diag}(a_1,a_2,...,a_n)) = \prod_{i=1}^{n}a_i$ , for any diagonal matrix $\text{diag}(a_1,a_2,...,a_n) \in \mathbb{R}^{n \times n}$ whose diagonal entries (from top-left to bottom-right) are $a_1, ... ,a_n$ . Show that $f(A) = det(A) \text{ (determinant of A)} \; \forall A \in \mathbb{R}^{n \times n}$ , i.e., show that the above two properties characterize the determinant function for real square matrices. My solution : Suppose $A$ is not invertible. Then, we can find invertible matrices $P$ and $Q$ such that $Q^{-1} A P = A' $ is diagonal with ones or zeros on its diagonal. As $A$ is not invertible, $A'$ must have atleast one zero in the diagonal. Hence, $f(Q^{-1})f(A)f(P) = f(A') = 0$ , as $A'$ is diagonal with a zero in the diagonal. But, for any invertible matrix $C,$ $\ 1 = f(I) = f(CC^{-1}) = f(C)f(C^{-1}) \Rightarrow f(C) \neq 0$ . So, $f(Q^{-1})\neq 0$ and $f(P) \neq 0$ . Hence $f(A) = 0 = det(A)$ for any non-invertible $A$ . Next, we know that any invertible matrix $X$ can be written as a product of elementary matrices $X=E_1 ... E_n$ , so if we can show that for an elementary matrix $E$ , $f(E) = det(E)$ , then $f(X) = f(E_1)...f(E_n) = det(E_1)...det(E_n) = det(E_1...E_n) = det(X)$ , and hence we would be done. Hence, it suffices to prove that for an elementary matrix $E$ , $f(E) = det(E)$ . [One useful observation : Suppose $Y$ is a diagonalizable matrix, then $\ \exists$ invertible matrix $M$ such that $D = M^{-1} Y M$ is diagonal. Clearly, $f(D) = det(D) = $ product of diagonal entries, for any diagonal matrix $D$ . So, $det(Y) = det(D) = f(D) = f(M^{-1})f(Y)f(M) = f(Y)f(M^{-1}M) = f(Y)f(I) = f(Y)$ . So, $f(Y) = det(Y)$ for any diagonalizable $Y$ .] If $E$ is an elementary matrix corresponding to multiplication of a row by a non-zero constant $c$ , then $E$ would be diagonal with one diagonal entry $c$ and all other diagonal entries being $1$ , and hence $f(E) = c = det(E)$ . If $E$ is an elementary matrix corresponding to swapping row $i$ and row $j$ ( $i\neq j$ ), then E is symmetric, hence diagonalizable by the spectral theorem for real symmetric matrices. Hence, $f(E) = det(E)$ , by above observation. If $E$ is an elementary matrix corresponding to adding $c$ times the ith row to the jth row ( $i\neq j$ ), then it is the square of the matrix $E'$ which corresponds to adding $c/2$ times the ith row to the jth row. So, $f(E) = f(E')^2$ . By taking $Z$ as the matrix which swaps row i and j (which is its own inverse), it can be shown that $Z^{-1} E Z$ = $E^T$ , so $f(E) = f(E^T)$ . As $EE^T$ is symmetric, by the spectral theorem for real symmetric matrices, it is diagonalizable, hence $f(EE^T) = det(EE^T)\Rightarrow f(E)^2 = det(E)^2 = 1^2 = 1.$ As $E'$ as defined before is an elementary matrix of the same type as $E$ , $f(E')^2 = 1$ as well. So, $f(E) = f(E')^2 = 1 = det(E)$ . This finishes the proof for real square matrices. Note that in the above proof, I never used the fact that the field is the field of real numbers, except for the part where I used the spectral theorem to conclude that a real symmetric matrix is diagonalizable. But this is not true in every field ( Is symmetric matrix over a field F always diagonalizable? ). So my question is as follows : Can we give a proof which works in every field, or can we find a field where the two above mentioned properties do not characterize the determinant? I would be very grateful for any help. Geometric interpretation (along similar lines of the video from which I picked this question) would be highly appreciated. Note : For a field with just 2 elements (0 and 1), like $\mathbb{Z}/2\mathbb{Z}$ , the solution is trivial, because for a non-invertible $A$ , $f(A)=0=det(A)$ , and for invertible $B$ , $f(B)$ and $det(B)$ are each non-zero, so they must each be $=1$ . And hence, we are done in this case.","I found a question in the very end of this video, which essentially is the following: Let be some natural number. Suppose there is a function such that it satisfies the following 2 properties: , for any diagonal matrix whose diagonal entries (from top-left to bottom-right) are . Show that , i.e., show that the above two properties characterize the determinant function for real square matrices. My solution : Suppose is not invertible. Then, we can find invertible matrices and such that is diagonal with ones or zeros on its diagonal. As is not invertible, must have atleast one zero in the diagonal. Hence, , as is diagonal with a zero in the diagonal. But, for any invertible matrix . So, and . Hence for any non-invertible . Next, we know that any invertible matrix can be written as a product of elementary matrices , so if we can show that for an elementary matrix , , then , and hence we would be done. Hence, it suffices to prove that for an elementary matrix , . [One useful observation : Suppose is a diagonalizable matrix, then invertible matrix such that is diagonal. Clearly, product of diagonal entries, for any diagonal matrix . So, . So, for any diagonalizable .] If is an elementary matrix corresponding to multiplication of a row by a non-zero constant , then would be diagonal with one diagonal entry and all other diagonal entries being , and hence . If is an elementary matrix corresponding to swapping row and row ( ), then E is symmetric, hence diagonalizable by the spectral theorem for real symmetric matrices. Hence, , by above observation. If is an elementary matrix corresponding to adding times the ith row to the jth row ( ), then it is the square of the matrix which corresponds to adding times the ith row to the jth row. So, . By taking as the matrix which swaps row i and j (which is its own inverse), it can be shown that = , so . As is symmetric, by the spectral theorem for real symmetric matrices, it is diagonalizable, hence As as defined before is an elementary matrix of the same type as , as well. So, . This finishes the proof for real square matrices. Note that in the above proof, I never used the fact that the field is the field of real numbers, except for the part where I used the spectral theorem to conclude that a real symmetric matrix is diagonalizable. But this is not true in every field ( Is symmetric matrix over a field F always diagonalizable? ). So my question is as follows : Can we give a proof which works in every field, or can we find a field where the two above mentioned properties do not characterize the determinant? I would be very grateful for any help. Geometric interpretation (along similar lines of the video from which I picked this question) would be highly appreciated. Note : For a field with just 2 elements (0 and 1), like , the solution is trivial, because for a non-invertible , , and for invertible , and are each non-zero, so they must each be . And hence, we are done in this case.","n f : \mathbb{R}^{n \times n} \rightarrow \mathbb{R} f(A\cdot B) = f(A)\cdot f(B),\ \forall A,B \in \mathbb{R}^{n \times n}  f(\text{diag}(a_1,a_2,...,a_n)) = \prod_{i=1}^{n}a_i \text{diag}(a_1,a_2,...,a_n) \in \mathbb{R}^{n \times n} a_1, ... ,a_n f(A) = det(A) \text{ (determinant of A)} \; \forall A \in \mathbb{R}^{n \times n} A P Q Q^{-1} A P = A'  A A' f(Q^{-1})f(A)f(P) = f(A') = 0 A' C, \ 1 = f(I) = f(CC^{-1}) = f(C)f(C^{-1}) \Rightarrow f(C) \neq 0 f(Q^{-1})\neq 0 f(P) \neq 0 f(A) = 0 = det(A) A X X=E_1 ... E_n E f(E) = det(E) f(X) = f(E_1)...f(E_n) = det(E_1)...det(E_n) = det(E_1...E_n) = det(X) E f(E) = det(E) Y \ \exists M D = M^{-1} Y M f(D) = det(D) =  D det(Y) = det(D) = f(D) = f(M^{-1})f(Y)f(M) = f(Y)f(M^{-1}M) = f(Y)f(I) = f(Y) f(Y) = det(Y) Y E c E c 1 f(E) = c = det(E) E i j i\neq j f(E) = det(E) E c i\neq j E' c/2 f(E) = f(E')^2 Z Z^{-1} E Z E^T f(E) = f(E^T) EE^T f(EE^T) = det(EE^T)\Rightarrow f(E)^2 = det(E)^2 = 1^2 = 1. E' E f(E')^2 = 1 f(E) = f(E')^2 = 1 = det(E) \mathbb{Z}/2\mathbb{Z} A f(A)=0=det(A) B f(B) det(B) =1","['linear-algebra', 'matrices', 'determinant']"
59,Every polynomial sequence satisfies a linear recurrence?,Every polynomial sequence satisfies a linear recurrence?,,"I noticed an interesting pattern, which is that the sequence $a_n = n^2$ satisfies the recurrence relation $1a_{n-3} - 3 a_{n-2} + 3 a_{n-1} - 1a_n = 0.$ This is an alternating weighted sum of 4 consecutive terms in the sequence, with the weights taken from Pascal's triangle [1,3,3,1]. I noticed that the pattern seems to extend to other polynomials, such as $a_n=n$ (with $1a_{n-2} - 2 a_{n-1} + 1a_n = 0$ ) and $a_n = n^3$ (with $1a_{n-4} - 4a_{n-3} + 6a_{n-2}-4a_{n-1}+1a_n = 0)$ . I am looking for a (hopefully elegant) proof that this pascal-weighted alternating sum recurrence holds for any polynomial sequence $a_n = n^p$ , with an explanation for why the pascal weights in particular appear.  I know several facts that are potentially useful, but I haven't seen how to put them together. The integer points of every degree $d$ polynomial have constant order- $d$ successive differences, for the same reason that in a continuous setting, the $d$ th derivative is constant. This means that some fixed number of previous terms in the sequence convey enough data about the various-order differences to construct the next term in the sequence. The terms from Pascal's triangle represent the coefficients of the binomial expansion of $(x+y)^n$ . Presumably this alternating sum causes a bunch of nice cancellations, but I don't yet see how. The terms from Pascal's triangle are also recursive, in that each row is generated as the sum of the two neighboring terms in the previous row. In short, why does it seem that: $$f(x)=\sum_{k=0}^{p+1} {p+1\choose k} (-1)^{k} (x-k)^p \stackrel{?}{\equiv} 0. $$","I noticed an interesting pattern, which is that the sequence satisfies the recurrence relation This is an alternating weighted sum of 4 consecutive terms in the sequence, with the weights taken from Pascal's triangle [1,3,3,1]. I noticed that the pattern seems to extend to other polynomials, such as (with ) and (with . I am looking for a (hopefully elegant) proof that this pascal-weighted alternating sum recurrence holds for any polynomial sequence , with an explanation for why the pascal weights in particular appear.  I know several facts that are potentially useful, but I haven't seen how to put them together. The integer points of every degree polynomial have constant order- successive differences, for the same reason that in a continuous setting, the th derivative is constant. This means that some fixed number of previous terms in the sequence convey enough data about the various-order differences to construct the next term in the sequence. The terms from Pascal's triangle represent the coefficients of the binomial expansion of . Presumably this alternating sum causes a bunch of nice cancellations, but I don't yet see how. The terms from Pascal's triangle are also recursive, in that each row is generated as the sum of the two neighboring terms in the previous row. In short, why does it seem that:",a_n = n^2 1a_{n-3} - 3 a_{n-2} + 3 a_{n-1} - 1a_n = 0. a_n=n 1a_{n-2} - 2 a_{n-1} + 1a_n = 0 a_n = n^3 1a_{n-4} - 4a_{n-3} + 6a_{n-2}-4a_{n-1}+1a_n = 0) a_n = n^p d d d (x+y)^n f(x)=\sum_{k=0}^{p+1} {p+1\choose k} (-1)^{k} (x-k)^p \stackrel{?}{\equiv} 0. ,"['linear-algebra', 'algebra-precalculus', 'recurrence-relations']"
60,What can we do to matrices that we can do to the regular good ol' numbers?,What can we do to matrices that we can do to the regular good ol' numbers?,,"I am a high school student and my formal Linear Algebra education consisted merely of the definition of matrices as a list of numbers and then some random properties. While reading the BetterExplained article on Linear Algebra and some of 3Blue1Brown's videos on the same did help, I still often face difficulties while solving problems and when I look up the solutions, all I'm thinking is ""Wait. You're allowed to do that to matrices too?"" So my question is - what can we do to these list of numbers that we can do to individual real and complex numbers? To clarify, I do know that we can add matrices by adding their individual elements together and multiply matrices in a row-column order and stuff like that. My doubts are along the lines of the ones listed below: Does $AB = CD$ imply that $B^{-1}A^{-1}=D^{-1}C^{-1}$ ? (where A, B, C and D are 4 non-singular matrices of appropriate orders) Is $A \times A^n = A^n \times A$ valid? (where A is a matrix and n is a natural number) Is multiplication of matrices commutative only when a matrix is being multiplied by a null matrix or unit matrix of appropriate order? Is the inverse of the matrix $A^n$ the same as the inverse of $A$ multiplied $n$ times to itself? I am not asking for proofs of the problems listed above - instead, I would greatly appreciate it if someone who has noticed some ""patterns"" in the doubts listed above would point me to some resource that I can study to clear all such doubts in my conceptual understanding. Alternatively, how should I approach matrix multiplication and their inverses in general that would solve these and other similar problems that I may be having? Thank you. Edit: These issues have been solved adequately and then some by Dave L. Renfro's comments on this question.","I am a high school student and my formal Linear Algebra education consisted merely of the definition of matrices as a list of numbers and then some random properties. While reading the BetterExplained article on Linear Algebra and some of 3Blue1Brown's videos on the same did help, I still often face difficulties while solving problems and when I look up the solutions, all I'm thinking is ""Wait. You're allowed to do that to matrices too?"" So my question is - what can we do to these list of numbers that we can do to individual real and complex numbers? To clarify, I do know that we can add matrices by adding their individual elements together and multiply matrices in a row-column order and stuff like that. My doubts are along the lines of the ones listed below: Does imply that ? (where A, B, C and D are 4 non-singular matrices of appropriate orders) Is valid? (where A is a matrix and n is a natural number) Is multiplication of matrices commutative only when a matrix is being multiplied by a null matrix or unit matrix of appropriate order? Is the inverse of the matrix the same as the inverse of multiplied times to itself? I am not asking for proofs of the problems listed above - instead, I would greatly appreciate it if someone who has noticed some ""patterns"" in the doubts listed above would point me to some resource that I can study to clear all such doubts in my conceptual understanding. Alternatively, how should I approach matrix multiplication and their inverses in general that would solve these and other similar problems that I may be having? Thank you. Edit: These issues have been solved adequately and then some by Dave L. Renfro's comments on this question.",AB = CD B^{-1}A^{-1}=D^{-1}C^{-1} A \times A^n = A^n \times A A^n A n,"['linear-algebra', 'matrices', 'matrix-equations']"
61,"If $a_{n+1}=2a_n n^2+n$ Define a sequence $a_n$ that satisfy the recurrence relation as described above, with $a_1 = 3$","If  Define a sequence  that satisfy the recurrence relation as described above, with",a_{n+1}=2a_n n^2+n a_n a_1 = 3,"If $$a_{n+1}=2a_n n^2+n$$ Define a sequence $a_n$ that satisfy the recurrence relation as described above, with $a_1 = 3$ Find the value of $$\dfrac{ |a_{20} - a_{15} | }{18133} $$ Attempt First  evaluate $a_{0}$ $$a_{1} = 2a_{0} \Rightarrow a_{0}= \frac{3}{2}$$ Then, use Z-transform: $$a_{n+1} - 2a_{n} + n^2 - n = 0$$ $$z(\mathbf{A}(z)-a_{0}) - 2\mathbf{A}(z) + \dfrac{z(z+1)}{(z-1)^3} - \dfrac{z}{(z-1)^2} = 0$$ $$\Rightarrow \mathbf{A}(z) = \dfrac{z(3z^3 -9z^2 + 9z - 7)}{2(z-2)(z-1)^3}$$  $$\Rightarrow \mathbf{A}(z) = \dfrac{2z}{z-1} + \dfrac{z}{(z-1)^2} + \dfrac{z(z+1)}{(z-1)^3} - \dfrac{z}{2(z-2)}$$ The inverse of the Z-transform will be: $$\boxed{a_{n} = 2 + n + n^2 - 2^{n-1}}$$  Now: $$a_{20} = 422-2^{19}$$ $$a_{15} = 242-2^{14}$$ Is it Correct?? Any other precise solution will be highly appreciated","If Define a sequence that satisfy the recurrence relation as described above, with Find the value of Attempt First  evaluate Then, use Z-transform:  The inverse of the Z-transform will be:  Now: Is it Correct?? Any other precise solution will be highly appreciated",a_{n+1}=2a_n n^2+n a_n a_1 = 3 \dfrac{ |a_{20} - a_{15} | }{18133}  a_{0} a_{1} = 2a_{0} \Rightarrow a_{0}= \frac{3}{2} a_{n+1} - 2a_{n} + n^2 - n = 0 z(\mathbf{A}(z)-a_{0}) - 2\mathbf{A}(z) + \dfrac{z(z+1)}{(z-1)^3} - \dfrac{z}{(z-1)^2} = 0 \Rightarrow \mathbf{A}(z) = \dfrac{z(3z^3 -9z^2 + 9z - 7)}{2(z-2)(z-1)^3} \Rightarrow \mathbf{A}(z) = \dfrac{2z}{z-1} + \dfrac{z}{(z-1)^2} + \dfrac{z(z+1)}{(z-1)^3} - \dfrac{z}{2(z-2)} \boxed{a_{n} = 2 + n + n^2 - 2^{n-1}} a_{20} = 422-2^{19} a_{15} = 242-2^{14},"['linear-algebra', 'recurrence-relations', 'z-transform']"
62,Neighborhood in orthogonal group,Neighborhood in orthogonal group,,"Let $A\in O(n)$ . Assume that $|a_{i,i}|\neq 1$ for every $i$ . Prove that in every neighborhood of $A$ there exists $B\in O(n)$ such that $|b_{i,i}|>|a_{i,i}| \text{ for every } i \text{ and } |b_{i,j}|\leq |a_{i,j}| \text{ for every } i\neq j$ . I have thought about projecting $A+\epsilon I$ on $O(n)$ (wlog I assume $a_{ii}\geq 0$ here), but there doesn't seem to be a nice formula indicating whether this projection would work or not. Similarly I am not sure about Gram-Schmidt. Or maybe it's better to find a prove that doesn't use explicit constructions, but I don't know much about neighborhood in $O(n)$ . Any suggestions? Edit: the main difficulty I faced was that, if $a_{i,j}=0$ then we must force $b_{i,j}=0$ , which prevents methods that involve 'small perturbation of every entry'. Edit 2: As a counter-example is given below, I wonder whether the claim is true if $A$ is close to $I$ , say if $A$ is closest to $I$ among all signed permutation matrices. (In the counter-example, $A\neq I$ itself is a signed permutation matrix). Note that this claim is true is $A$ is close enough to $I$ , as we can form a path $B_t=\text{exp}(t\log(A))$ . Since the entries of $B_t$ is analytic on $t$ , if $A$ is sufficiently close to $I$ every $B_t$ would satisfy the conditions for $t\in[0,1]$ .","Let . Assume that for every . Prove that in every neighborhood of there exists such that . I have thought about projecting on (wlog I assume here), but there doesn't seem to be a nice formula indicating whether this projection would work or not. Similarly I am not sure about Gram-Schmidt. Or maybe it's better to find a prove that doesn't use explicit constructions, but I don't know much about neighborhood in . Any suggestions? Edit: the main difficulty I faced was that, if then we must force , which prevents methods that involve 'small perturbation of every entry'. Edit 2: As a counter-example is given below, I wonder whether the claim is true if is close to , say if is closest to among all signed permutation matrices. (In the counter-example, itself is a signed permutation matrix). Note that this claim is true is is close enough to , as we can form a path . Since the entries of is analytic on , if is sufficiently close to every would satisfy the conditions for .","A\in O(n) |a_{i,i}|\neq 1 i A B\in O(n) |b_{i,i}|>|a_{i,i}| \text{ for every } i \text{ and } |b_{i,j}|\leq |a_{i,j}| \text{ for every } i\neq j A+\epsilon I O(n) a_{ii}\geq 0 O(n) a_{i,j}=0 b_{i,j}=0 A I A I A\neq I A I B_t=\text{exp}(t\log(A)) B_t t A I B_t t\in[0,1]","['linear-algebra', 'matrices', 'topological-groups', 'orthogonal-matrices']"
63,"What does ""transform like"" mean?","What does ""transform like"" mean?",,"I read on a pdf that considering $SU(2)$ the spinor $(\xi_1, \xi_2)^T$ transform the same way as $(-\xi_2^*, \xi_1^*)^T$ . What does it mean that they transform the same way? I don't know what's the meaning of ""two things transform in the same way"".","I read on a pdf that considering the spinor transform the same way as . What does it mean that they transform the same way? I don't know what's the meaning of ""two things transform in the same way"".","SU(2) (\xi_1, \xi_2)^T (-\xi_2^*, \xi_1^*)^T","['linear-algebra', 'group-theory', 'transformation', 'rotations', 'spin-geometry']"
64,8 Vs 10 Axioms / Properties of a Vector Space: Should Closure of Addition and Scalar Multiplication Be Included?,8 Vs 10 Axioms / Properties of a Vector Space: Should Closure of Addition and Scalar Multiplication Be Included?,,"In every physical textbook on linear algebra that I own, vector spaces are defined as a set $\mathcal{S}$ , along with two operations: (vector) addition $\oplus$ , and scalar multiplication $\odot$ , that, together, satisfy ten properties (5 properties of addition, 5 properties of scalar multiplication). However, the Wikipedia article on Vector Spaces lists only 8 axioms / properties, stating (emphasis added): Vector addition and scalar multiplication are operations, satisfying the closure property: $\vec{u} + \vec{v}$ and $a\vec{v}$ are in $\mathcal{V}$ for all $a$ in $\mathbb{F}$ , and $\vec{u},\, \vec{v}$ in $\mathcal{V}$ . Some older sources mention these properties as separate axioms. This statement seems to suggest that the closure axioms are somehow included in the other 8 axioms. Unfortunately, the reason as to why closure under vector addition and scalar multiplication need not be included is not explained. Further searches online have turned up lists of 8, 9 or 10 properties of Vectors Spaces, so I am a bit confused as to what's going on, here? N.B. when defining vector addition and and scalar multiplication (see the end of this post for the complete quote), the Wikipedia article does specify that the resultant vector is also an element of the set $\mathcal{V}$ so are they basically shifting the ""burden"" of this property onto the operations, themselves? That is certainly what it seems like, but it is not obvious as to why they would make this move with these specific properties, and not the others . Any clarification would be greatly appreciated! Complete Definition from Wikipedia: A vector space over a field ${F}$ is a set $V$ together with two operations that satisfy the eight axioms listed below. In the following, $V  V$ denotes the Cartesian product of $V$ with itself, and  denotes a mapping from one set to another. The first operation, called vector addition or simply addition + : $V  V$  $V$ , takes any two vectors $\mathbf v$ and $\mathbf w$ and assigns to them a third vector which is commonly written as $\mathbf v + \mathbf w$ , and called the sum of these two vectors. (The resultant vector is also an element of the set $V$ .) The second operation, called scalar multiplication  : $F  V$  $V$  takes any scalar $a$ and any vector $\mathbf v$ and gives another vector $a \mathbf v$ . (Similarly, the vector $a \mathbf v$ is an element of the set $V$ ...)","In every physical textbook on linear algebra that I own, vector spaces are defined as a set , along with two operations: (vector) addition , and scalar multiplication , that, together, satisfy ten properties (5 properties of addition, 5 properties of scalar multiplication). However, the Wikipedia article on Vector Spaces lists only 8 axioms / properties, stating (emphasis added): Vector addition and scalar multiplication are operations, satisfying the closure property: and are in for all in , and in . Some older sources mention these properties as separate axioms. This statement seems to suggest that the closure axioms are somehow included in the other 8 axioms. Unfortunately, the reason as to why closure under vector addition and scalar multiplication need not be included is not explained. Further searches online have turned up lists of 8, 9 or 10 properties of Vectors Spaces, so I am a bit confused as to what's going on, here? N.B. when defining vector addition and and scalar multiplication (see the end of this post for the complete quote), the Wikipedia article does specify that the resultant vector is also an element of the set so are they basically shifting the ""burden"" of this property onto the operations, themselves? That is certainly what it seems like, but it is not obvious as to why they would make this move with these specific properties, and not the others . Any clarification would be greatly appreciated! Complete Definition from Wikipedia: A vector space over a field is a set together with two operations that satisfy the eight axioms listed below. In the following, denotes the Cartesian product of with itself, and  denotes a mapping from one set to another. The first operation, called vector addition or simply addition + :  , takes any two vectors and and assigns to them a third vector which is commonly written as , and called the sum of these two vectors. (The resultant vector is also an element of the set .) The second operation, called scalar multiplication  :   takes any scalar and any vector and gives another vector . (Similarly, the vector is an element of the set ...)","\mathcal{S} \oplus \odot \vec{u} + \vec{v} a\vec{v} \mathcal{V} a \mathbb{F} \vec{u},\, \vec{v} \mathcal{V} \mathcal{V} {F} V V  V V V  V V \mathbf v \mathbf w \mathbf v + \mathbf w V F  V V a \mathbf v a \mathbf v a \mathbf v V","['linear-algebra', 'vector-spaces', 'axioms']"
65,"Are these two explicitly given $10 \times 10$ matrices similar over the integers? Or equivalently, are they shift equivalent over $\mathbb{Z}$?","Are these two explicitly given  matrices similar over the integers? Or equivalently, are they shift equivalent over ?",10 \times 10 \mathbb{Z},"Are the following two $10 \times 10$ matrices $$A = \left[ \begin {array}{cccccccccc} 0&1&0&0&1&0&0&0&0&0 \\ 0&0&1&0&0&1&0&0&0&0\\ 1&0&0&1&0 &0&1&0&0&0\\ 1&0&0&0&0&0&0&1&0&0 \\ 0&0&0&0&0&1&0&0&0&0\\ 0&0&0&0&0 &0&1&0&0&0\\ 0&0&0&0&1&0&0&1&0&0 \\ 0&0&0&0&1&0&0&0&1&0\\ 0&0&0&0&0 &0&0&1&1&1\\ 0&0&0&0&0&0&0&0&1&1\end {array}  \right] $$ and $$ B = \left[ \begin {array}{cccccccccc} 1&1&0&0&0&0&0&0&0&0 \\ 1&1&1&0&0&0&0&0&0&0\\ 0&1&0&0&0 &1&5&1&5&5\\ 0&0&0&0&1&0&6&10&5&1 \\ 0&0&1&0&0&1&6&6&10&5\\ 0&0&0&1&0 &0&10&5&1&5\\ 0&0&0&0&0&0&0&1&0&0 \\ 0&0&0&0&0&0&0&0&1&0\\ 0&0&0&0&0 &0&1&0&0&1\\ 0&0&0&0&0&0&1&0&0&0\end {array}  \right] $$ similar over $\mathbb{Z}$ ? I believe the answer to be no, but if the answer is yes, then these matrices form a counterexample to something (to be described below) I've been trying to prove recently in my PhD project. Let me give some context to my question by first indicating what I've tried and found out so far, and then giving some motivation and background after that. What I've tried: Both matrices have characteristic polynomial $${t}^{10}-2\,{t}^{9}-{t}^{8}-{t}^{7}+2\,{t}^{6}+6\,{t}^{5}-{t}^{3}-4\,{ t}^{2}-2\,t+1,$$ and they both have the same Frobenius/Rational canonical form, this being just the companion matrix of said polynomial: $$ F = \left[ \begin {array}{cccccccccc} 0&0&0&0&0&0&0&0&0&-1 \\ 1&0&0&0&0&0&0&0&0&2\\ 0&1&0&0&0 &0&0&0&0&4\\ 0&0&1&0&0&0&0&0&0&1 \\ 0&0&0&1&0&0&0&0&0&0\\ 0&0&0&0&1 &0&0&0&0&-6\\ 0&0&0&0&0&1&0&0&0&-2 \\ 0&0&0&0&0&0&1&0&0&1\\ 0&0&0&0&0 &0&0&1&0&1\\ 0&0&0&0&0&0&0&0&1&2\end {array}  \right] $$ So we know that $A$ and $B$ are similar over $\mathbb{Q}$ (as they are both similar to $F$ ). I have actually been able to show that $A$ is similar to $F$ also over $\mathbb{Z}$ . This was done using a ""lucky"" computer search: I ran through all size $10$ vectors $\vec v$ with entries in $\{-1,0,1\}$ and checked whether the matrix with columns $\vec v$ , $A \vec v$ , $\ldots$ , $A^9 \vec v$ had determinant $\pm 1$ . This was the case for a handful of $\vec v$ 's, so these matrices witness the integral similarity of $A$ and $F$ . The same search for $B$ did not yield any positive results though. I would like to try a search on a larger set of vectors, but this would require more computing power than I posess currently (being home for the summer holidays). If the matrices are not similar, this method will of course never give the answer though... From the above we nevertheless have the following equivalent question: Are the matrices $B$ and $F$ above similar over $\mathbb{Z}$ ? Another idea: Another idea I've had is to consider the matrices $A$ and $B$ (or $F$ and $B$ ) as matrices over the finite field $\mathbb{F}_p$ for various choices of the prime $p$ . As this can be decided using the Frobenius form or Smith normal form. If there is a $p$ for which the matrices are not similar over $\mathbb{F}_p$ , then they are not similar over $\mathbb{Z}$ either.  So that would settle it. However, if for some large $p$ , $A$ and $B$ are similar over $\mathbb{F}_p$ , I hope that a the transformation matrix witnessing this similarity also could work over $\mathbb{Z}$ , because the $p$ is so large. This is just speculation though. My problem is that I'm not sure which software I could use to perform these finite field calculations effeciently. Any feedback on this is greatly appreciated! Motivation: This part will be a bit brief/cryptic, in order to keep it short, so please just let me know if I should give more details on something. This question has come out of my recent attempts to prove that shift equivalence implies flow equivalence for (certain) non-negative integral matrices and their associated shifts of finite type. The dynamical systems called shifts of finite type , which are associated to a non-negative integer matrix, are described in the introductory textbook ""An Introduction to Symbolic Dynamics and Coding"" by Douglas Lind and Brian Marcus. Shift equivalence of matrices and flow equivalence of shifts of finite type are also explained in the book (starting on pages 233 and 453). For non-negative integer matrices which are irreducible (meaning that for each $i,j$ $A^n(i,j) > 0$ for some $n$ ), it is known that shift equivalence implies flow equivalence. This is because in the irreducible case there is a complete algebraic invariant in terms of the defining matrix which determines flow equivalence. And it is not hard to see that shift equivalence over $\mathbb{Z}$ (which is weaker than mere shift equivalence) preserves the invariant. For reducible matrices, however, the invariant is a lot more complicated. So in this case it is an open problem whether shift equivalence (over $\mathbb{Z}$ ) implies flow equivalence. It is not clear at all whether shift equivalence preserves the invariant. There is a recent paper in another field which hints at this potentially being true, so that is why I've spent some time trying to prove it. After failing to prove it I started searching for counterexamples instead. There are counterexamples if one allows the matrices to have ""cyclic components"", meaning that one irreducible component is essentially a permutaton matrix, but this makes the shifts of finite type a bit ""degenerate"" in some sense, so I want to avoid that. The way shift equivalence over $\mathbb{Z}$ connects with similarity over $\mathbb{Z}$ is that this is actually equivalent, for matrices with determinant $\pm 1$ (i.e. those which are invertible over $\mathbb{Z}$ ). This is the case for $A$ and $B$ . The matrices $A$ and $B$ are constructed to not be flow equivalent. They both have a similar block structure which is a by-product of the construction (edit: see the comments for the block structure), but I have chosen to not get into that. Edit: Just to be clear, what I ""hope for"" is that the matrices are similar . For that would settle my underlying research question. If they turn out to not be similar (or if we cannot tell), then what I would like is to either try to construct some other matrices that might be similar (while still not being flow equivalent) based on the feedback here, or to try find some ""underlying reason"" why they can never be similar. For that might help me prove that shift equivalence implies flow equivalence.","Are the following two matrices and similar over ? I believe the answer to be no, but if the answer is yes, then these matrices form a counterexample to something (to be described below) I've been trying to prove recently in my PhD project. Let me give some context to my question by first indicating what I've tried and found out so far, and then giving some motivation and background after that. What I've tried: Both matrices have characteristic polynomial and they both have the same Frobenius/Rational canonical form, this being just the companion matrix of said polynomial: So we know that and are similar over (as they are both similar to ). I have actually been able to show that is similar to also over . This was done using a ""lucky"" computer search: I ran through all size vectors with entries in and checked whether the matrix with columns , , , had determinant . This was the case for a handful of 's, so these matrices witness the integral similarity of and . The same search for did not yield any positive results though. I would like to try a search on a larger set of vectors, but this would require more computing power than I posess currently (being home for the summer holidays). If the matrices are not similar, this method will of course never give the answer though... From the above we nevertheless have the following equivalent question: Are the matrices and above similar over ? Another idea: Another idea I've had is to consider the matrices and (or and ) as matrices over the finite field for various choices of the prime . As this can be decided using the Frobenius form or Smith normal form. If there is a for which the matrices are not similar over , then they are not similar over either.  So that would settle it. However, if for some large , and are similar over , I hope that a the transformation matrix witnessing this similarity also could work over , because the is so large. This is just speculation though. My problem is that I'm not sure which software I could use to perform these finite field calculations effeciently. Any feedback on this is greatly appreciated! Motivation: This part will be a bit brief/cryptic, in order to keep it short, so please just let me know if I should give more details on something. This question has come out of my recent attempts to prove that shift equivalence implies flow equivalence for (certain) non-negative integral matrices and their associated shifts of finite type. The dynamical systems called shifts of finite type , which are associated to a non-negative integer matrix, are described in the introductory textbook ""An Introduction to Symbolic Dynamics and Coding"" by Douglas Lind and Brian Marcus. Shift equivalence of matrices and flow equivalence of shifts of finite type are also explained in the book (starting on pages 233 and 453). For non-negative integer matrices which are irreducible (meaning that for each for some ), it is known that shift equivalence implies flow equivalence. This is because in the irreducible case there is a complete algebraic invariant in terms of the defining matrix which determines flow equivalence. And it is not hard to see that shift equivalence over (which is weaker than mere shift equivalence) preserves the invariant. For reducible matrices, however, the invariant is a lot more complicated. So in this case it is an open problem whether shift equivalence (over ) implies flow equivalence. It is not clear at all whether shift equivalence preserves the invariant. There is a recent paper in another field which hints at this potentially being true, so that is why I've spent some time trying to prove it. After failing to prove it I started searching for counterexamples instead. There are counterexamples if one allows the matrices to have ""cyclic components"", meaning that one irreducible component is essentially a permutaton matrix, but this makes the shifts of finite type a bit ""degenerate"" in some sense, so I want to avoid that. The way shift equivalence over connects with similarity over is that this is actually equivalent, for matrices with determinant (i.e. those which are invertible over ). This is the case for and . The matrices and are constructed to not be flow equivalent. They both have a similar block structure which is a by-product of the construction (edit: see the comments for the block structure), but I have chosen to not get into that. Edit: Just to be clear, what I ""hope for"" is that the matrices are similar . For that would settle my underlying research question. If they turn out to not be similar (or if we cannot tell), then what I would like is to either try to construct some other matrices that might be similar (while still not being flow equivalent) based on the feedback here, or to try find some ""underlying reason"" why they can never be similar. For that might help me prove that shift equivalence implies flow equivalence.","10 \times 10 A = \left[ \begin {array}{cccccccccc} 0&1&0&0&1&0&0&0&0&0
\\ 0&0&1&0&0&1&0&0&0&0\\ 1&0&0&1&0
&0&1&0&0&0\\ 1&0&0&0&0&0&0&1&0&0
\\ 0&0&0&0&0&1&0&0&0&0\\ 0&0&0&0&0
&0&1&0&0&0\\ 0&0&0&0&1&0&0&1&0&0
\\ 0&0&0&0&1&0&0&0&1&0\\ 0&0&0&0&0
&0&0&1&1&1\\ 0&0&0&0&0&0&0&0&1&1\end {array}
 \right]   B = \left[ \begin {array}{cccccccccc} 1&1&0&0&0&0&0&0&0&0
\\ 1&1&1&0&0&0&0&0&0&0\\ 0&1&0&0&0
&1&5&1&5&5\\ 0&0&0&0&1&0&6&10&5&1
\\ 0&0&1&0&0&1&6&6&10&5\\ 0&0&0&1&0
&0&10&5&1&5\\ 0&0&0&0&0&0&0&1&0&0
\\ 0&0&0&0&0&0&0&0&1&0\\ 0&0&0&0&0
&0&1&0&0&1\\ 0&0&0&0&0&0&1&0&0&0\end {array}
 \right]
 \mathbb{Z} {t}^{10}-2\,{t}^{9}-{t}^{8}-{t}^{7}+2\,{t}^{6}+6\,{t}^{5}-{t}^{3}-4\,{
t}^{2}-2\,t+1,  F = \left[ \begin {array}{cccccccccc} 0&0&0&0&0&0&0&0&0&-1
\\ 1&0&0&0&0&0&0&0&0&2\\ 0&1&0&0&0
&0&0&0&0&4\\ 0&0&1&0&0&0&0&0&0&1
\\ 0&0&0&1&0&0&0&0&0&0\\ 0&0&0&0&1
&0&0&0&0&-6\\ 0&0&0&0&0&1&0&0&0&-2
\\ 0&0&0&0&0&0&1&0&0&1\\ 0&0&0&0&0
&0&0&1&0&1\\ 0&0&0&0&0&0&0&0&1&2\end {array}
 \right]  A B \mathbb{Q} F A F \mathbb{Z} 10 \vec v \{-1,0,1\} \vec v A \vec v \ldots A^9 \vec v \pm 1 \vec v A F B B F \mathbb{Z} A B F B \mathbb{F}_p p p \mathbb{F}_p \mathbb{Z} p A B \mathbb{F}_p \mathbb{Z} p i,j A^n(i,j) > 0 n \mathbb{Z} \mathbb{Z} \mathbb{Z} \mathbb{Z} \pm 1 \mathbb{Z} A B A B","['linear-algebra', 'matrices', 'algebraic-number-theory', 'dynamical-systems', 'computational-mathematics']"
66,Does $A$ admit a square root with integer entries?,Does  admit a square root with integer entries?,A,"Let us consider the matrix $$ A =  \left( \begin{array}{crc} 0 & 0 & 3 \\  81 & 0 & 0 \\ 0 & 3 & 0 \end{array}  \right).$$ Does the matrix $A$ admit a square root each of whose entry is an integer? Please help me in this regard. Thank you very much. EDIT $:$ I have tried by taking determinant of $A$ but that is not working since $\det (A) =729,$ which is a perfect square. I don't know what other tools should I require to solve this. I think I can also use the trace argument here. If $B$ was the square root of $A$ then I also found that trace of $B$ is $2\sqrt 2 \cos \left (\frac {3 \pi} {8} \right) + 3,$ which is not an integer. Because $\lambda$ is an eigen value of $A$ iff $\sqrt {\lambda}$ is an eigen value of $B.$ So such a $B$ cannot be found. Please check my argument above whether it holds good or not.",Let us consider the matrix Does the matrix admit a square root each of whose entry is an integer? Please help me in this regard. Thank you very much. EDIT I have tried by taking determinant of but that is not working since which is a perfect square. I don't know what other tools should I require to solve this. I think I can also use the trace argument here. If was the square root of then I also found that trace of is which is not an integer. Because is an eigen value of iff is an eigen value of So such a cannot be found. Please check my argument above whether it holds good or not.," A =
 \left( \begin{array}{crc} 0 & 0 & 3 \\
 81 & 0 & 0 \\ 0 & 3 & 0 \end{array}
 \right). A : A \det (A) =729, B A B 2\sqrt 2 \cos \left (\frac {3 \pi} {8} \right) + 3, \lambda A \sqrt {\lambda} B. B",['linear-algebra']
67,Why SVD is not unique but the Moore-Penrose pseudo inverse is unique?,Why SVD is not unique but the Moore-Penrose pseudo inverse is unique?,,"I feel confused about the uniqueness of the Moore-Penrose inverse generated from SVD. For any matrix $A$ , if $X$ satisfied $$AXA=A, XAX=X, (AX)^\mathrm{T}=AX, (XA)^\mathrm{T}=XA $$ then $X$ is called the Moore-Penrose inverse of $A$ . If $A$ has the SVD(singular value decomposition) $$A=P\left[\begin{matrix}\Lambda_r&0\\0&0\end{matrix}\right]Q^\mathrm{T}$$ then it is easy to prove that $$A^+ = Q\left[\begin{matrix}\Lambda_r^{-1}&0\\0&0\end{matrix}\right]P^\mathrm{T}$$ is a Moore-Penrose inverse. If $X$ and $Y$ are both Moore-Penrose inverse of $A$ , from the equation $$X=XAX=X(AX)^\mathrm{T}=XX^\mathrm{T}A^\mathrm{T}=XX^\mathrm{T}(AYA)^\mathrm{T}=X(AX)^\mathrm{T}(AY)^\mathrm{T}=(XAX)AY=XAY=(XA)^\mathrm{T}YAY=A^\mathrm{T}X^\mathrm{T}A^\mathrm{T}Y^\mathrm{T}Y=A^\mathrm{T}Y^\mathrm{T}Y=(YA)^\mathrm{T}Y=YAY=Y$$ we can see that the Moore-Penrose inverse is unique. However, the Moore-Penrose inverse depends on the SVD and SVD is not unique. How to explain it?","I feel confused about the uniqueness of the Moore-Penrose inverse generated from SVD. For any matrix , if satisfied then is called the Moore-Penrose inverse of . If has the SVD(singular value decomposition) then it is easy to prove that is a Moore-Penrose inverse. If and are both Moore-Penrose inverse of , from the equation we can see that the Moore-Penrose inverse is unique. However, the Moore-Penrose inverse depends on the SVD and SVD is not unique. How to explain it?","A X AXA=A, XAX=X, (AX)^\mathrm{T}=AX, (XA)^\mathrm{T}=XA  X A A A=P\left[\begin{matrix}\Lambda_r&0\\0&0\end{matrix}\right]Q^\mathrm{T} A^+ = Q\left[\begin{matrix}\Lambda_r^{-1}&0\\0&0\end{matrix}\right]P^\mathrm{T} X Y A X=XAX=X(AX)^\mathrm{T}=XX^\mathrm{T}A^\mathrm{T}=XX^\mathrm{T}(AYA)^\mathrm{T}=X(AX)^\mathrm{T}(AY)^\mathrm{T}=(XAX)AY=XAY=(XA)^\mathrm{T}YAY=A^\mathrm{T}X^\mathrm{T}A^\mathrm{T}Y^\mathrm{T}Y=A^\mathrm{T}Y^\mathrm{T}Y=(YA)^\mathrm{T}Y=YAY=Y","['linear-algebra', 'matrices', 'matrix-decomposition', 'svd', 'pseudoinverse']"
68,"If $A$ is an $n$ by $n$ integer matrix such that $A^3 = I$, then $\operatorname{tr}(A) = n\mod3$","If  is an  by  integer matrix such that , then",A n n A^3 = I \operatorname{tr}(A) = n\mod3,"Attempt: We work with $A'$ , the matrix with entries $a_{ij}\mod 3$ . Note that cubing $A'$ still gives $I$ as $I$ is unchanged by considering remainders $\mod 3$ . For the rest of the proof, we will not differentiate between $A$ and $A'$ . The minimal polynomial of $A$ divides $x^{3} - 1$ so each eigenvalue $\lambda$ of $A$ satisfies $\lambda^{3} = 1$ . The trace of $A^3$ is clearly just $n$ as $A^3 = I$ . Next, note that for integers $a_1, ...., a_n$ we have that $(a_1 +... + a_n)^k = a_1^{k} + a_2^{k} ... + a_n^{k}\mod k$ . Now this is where I am stuck. I would like to say that this implies $\operatorname{tr}(A)^3 =\operatorname{tr}(A^3)$ , but why is it that $\operatorname{tr}(A^3)$ is the sum of the cubed diagonal entries of $A$ ? I can only say that $\operatorname{tr}(A^3)$ is the sum of the cubed eigenvalues of $A$ , but these eigenvalues need not be integers so the argument fails. If I am able to prove this, then the result follows since I have $a^3 = a\mod 3$ for all $a$ in $\{0,1,2\}$ . Edit: I can confirm that my proof does work since $\operatorname{tr}(A)^p = \operatorname{tr}(A^p)\mod p$ for prime $p$ as said here https://rjlipton.wordpress.com/2009/08/07/fermats-little-theorem-for-matrices/ But I can't find the proof for this statement itself.","Attempt: We work with , the matrix with entries . Note that cubing still gives as is unchanged by considering remainders . For the rest of the proof, we will not differentiate between and . The minimal polynomial of divides so each eigenvalue of satisfies . The trace of is clearly just as . Next, note that for integers we have that . Now this is where I am stuck. I would like to say that this implies , but why is it that is the sum of the cubed diagonal entries of ? I can only say that is the sum of the cubed eigenvalues of , but these eigenvalues need not be integers so the argument fails. If I am able to prove this, then the result follows since I have for all in . Edit: I can confirm that my proof does work since for prime as said here https://rjlipton.wordpress.com/2009/08/07/fermats-little-theorem-for-matrices/ But I can't find the proof for this statement itself.","A' a_{ij}\mod 3 A' I I \mod 3 A A' A x^{3} - 1 \lambda A \lambda^{3} = 1 A^3 n A^3 = I a_1, ...., a_n (a_1 +... + a_n)^k = a_1^{k} + a_2^{k} ... + a_n^{k}\mod k \operatorname{tr}(A)^3 =\operatorname{tr}(A^3) \operatorname{tr}(A^3) A \operatorname{tr}(A^3) A a^3 = a\mod 3 a \{0,1,2\} \operatorname{tr}(A)^p = \operatorname{tr}(A^p)\mod p p","['linear-algebra', 'eigenvalues-eigenvectors', 'modular-arithmetic', 'trace', 'minimal-polynomials']"
69,What is dimension over $\mathbb R$ of the space of $n\times n$ Hermitian matrices? [duplicate],What is dimension over  of the space of  Hermitian matrices? [duplicate],\mathbb R n\times n,"This question already has answers here : What is a basis for the space of $n\times n$ Hermitian matrices? (3 answers) Closed 3 years ago . what is dimension over $\mathbb{R}$ of $H_n( \mathbb{C})$ , the set of $n \times n$ Hermitian matrices? My attempt: every real number is a complex number as  all symmetric matrices  are  Hermitian. In my view the dimension of $H_n( \mathbb{C})$ is $\frac{n(n+1)}{2}$","This question already has answers here : What is a basis for the space of $n\times n$ Hermitian matrices? (3 answers) Closed 3 years ago . what is dimension over of , the set of Hermitian matrices? My attempt: every real number is a complex number as  all symmetric matrices  are  Hermitian. In my view the dimension of is",\mathbb{R} H_n( \mathbb{C}) n \times n H_n( \mathbb{C}) \frac{n(n+1)}{2},"['linear-algebra', 'matrices', 'hermitian-matrices']"
70,Matrix projection onto positive semidefinite cone with respect to the spectral norm,Matrix projection onto positive semidefinite cone with respect to the spectral norm,,"On page 399 of Boyd & Vandenberghe's Convex Optimization , it is stated that the projection of a symmetric $n \times n$ matrix $X_0$ onto the set of symmetric $n \times n$ positive semidefinite matrices $S^n_+$ is found in the following way: Find the spectral (eigenvalue) decomposition $$X_0 = \sum_{i=1}^n \lambda_i v_i v_i^T$$ Define the projection $$X = \sum_{i=1}^n \max\{\lambda_i, 0\} v_i v_i^T$$ It is proven in the book that $X$ is the solution of the problem $$\underset{X}{\text{minimize}} \quad \| X - X_0 \|_F^2 \quad \text{subject to} \quad X \succeq 0$$ where $\|A\|_F^2 = \operatorname{tr} \left(A^T A\right)$ is a square of the Frobenius norm. In other words, $X$ is the projection of $X_0$ onto symmetric positive semidefinite matrix cone with respect to the Frobenius norm. I have also found a proof of this in this question . Now the book also states (without a proof, or at least it is not obvious for me from the material) that $X$ is also a solution to the problem $$\underset{X}{\text{minimize}} \quad \| X - X_0 \|_2 \quad \text{subject to} \quad X \succeq 0$$ where $\|A\|_2$ ( $A$ being symmetric) is the spectral norm, $$\|A\|_2 = \max_{i=1, \dots, n}|\lambda_i| = \max\{\lambda_1, -\lambda_n\}$$ where $\lambda_i$ are the eigenvalues of $A$ , $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_n$ . How to prove this?","On page 399 of Boyd & Vandenberghe's Convex Optimization , it is stated that the projection of a symmetric matrix onto the set of symmetric positive semidefinite matrices is found in the following way: Find the spectral (eigenvalue) decomposition Define the projection It is proven in the book that is the solution of the problem where is a square of the Frobenius norm. In other words, is the projection of onto symmetric positive semidefinite matrix cone with respect to the Frobenius norm. I have also found a proof of this in this question . Now the book also states (without a proof, or at least it is not obvious for me from the material) that is also a solution to the problem where ( being symmetric) is the spectral norm, where are the eigenvalues of , . How to prove this?","n \times n X_0 n \times n S^n_+ X_0 = \sum_{i=1}^n \lambda_i v_i v_i^T X = \sum_{i=1}^n \max\{\lambda_i, 0\} v_i v_i^T X \underset{X}{\text{minimize}} \quad \| X - X_0 \|_F^2 \quad \text{subject to} \quad X \succeq 0 \|A\|_F^2 = \operatorname{tr} \left(A^T A\right) X X_0 X \underset{X}{\text{minimize}} \quad \| X - X_0 \|_2 \quad \text{subject to} \quad X \succeq 0 \|A\|_2 A \|A\|_2 = \max_{i=1, \dots, n}|\lambda_i| = \max\{\lambda_1, -\lambda_n\} \lambda_i A \lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_n","['linear-algebra', 'convex-optimization', 'least-squares', 'positive-semidefinite', 'projection']"
71,When are the inverses of stochastic matrices also stochastic matrices?,When are the inverses of stochastic matrices also stochastic matrices?,,"A stochastic matrix, with elements $\in[0,1]$ and rows summing to 1 are known to have one eigenvalue 1 (stationary distribution) and the rest of lower magnitude. However I don't know about many results regarding their inverses. In which cases is it possible to find another stochastic matrix being the inverse to the first? If we can not find an inverse fulfilling the requirements, can we find some ""pseudo"" inverse? Can we ""expand"" the probability space to find a larger space where it is possible to find a matrix ? EDIT: The closest I have come so far is to expand the space to double number of states and allowing elements > 1. Let us call the $N\times N$ probability transition matrix $\bf P$, then assuming $\bf P$ nonsingular we can calculate ${\bf P}^{-1}$, build the new matrices : $${\bf P_e = P \otimes I}_2\hspace{1cm}{\bf P_{ie}} = |{\bf P}^{-1}|\otimes ({{\bf 1}_2{\bf 1}_2}^T-{\bf I}_2)^{(1-\text{sgn}({\bf P}))\otimes {\bf I}_2}$$ Where the $|\cdot|$ and power are scalar wise operations. If we do this then of course $\bf P_{ie}P_e \neq I\hspace{1cm}\bf P_{e}P_{ie} \neq I$, but that is because we are not finished yet! If we now turn each $2\times2$ block $\bf A$ into a scalar by the following calculation: $[1,0] {\bf A} [1,-1]^T$ And systematically apply to all blocks: $$\cases{({\bf I}_N \otimes [1,0])({\bf P_{ie}P_e})({\bf I}_N \otimes [1,-1]^T)\\ ({\bf I}_N \otimes [1,0])({\bf P_{e}P_{ie}})({\bf I}_N \otimes [1,-1]^T)}$$ Now we do indeed get identities! Row sums of $\bf P_{ie}$ equals 1 if we treat every second bin as $-1$ (as the construction above does). But how to interpret the elements of $\bf {P_{ie}}$ which can be >1 ? Furthermore $\sum_j |({\bf P_{ie}})_{ij}|$ seems to be a measure of the confusion, it goes to $\infty$ as ${\bf P} \to \frac{1}{N^2}{{\bf 1}_N} {{\bf 1}_N}^T$ which is indeed the maximally confusing distribution as we lose all information except the mean values ( all other eigenvalues except stationary distribution will be 0 ).","A stochastic matrix, with elements $\in[0,1]$ and rows summing to 1 are known to have one eigenvalue 1 (stationary distribution) and the rest of lower magnitude. However I don't know about many results regarding their inverses. In which cases is it possible to find another stochastic matrix being the inverse to the first? If we can not find an inverse fulfilling the requirements, can we find some ""pseudo"" inverse? Can we ""expand"" the probability space to find a larger space where it is possible to find a matrix ? EDIT: The closest I have come so far is to expand the space to double number of states and allowing elements > 1. Let us call the $N\times N$ probability transition matrix $\bf P$, then assuming $\bf P$ nonsingular we can calculate ${\bf P}^{-1}$, build the new matrices : $${\bf P_e = P \otimes I}_2\hspace{1cm}{\bf P_{ie}} = |{\bf P}^{-1}|\otimes ({{\bf 1}_2{\bf 1}_2}^T-{\bf I}_2)^{(1-\text{sgn}({\bf P}))\otimes {\bf I}_2}$$ Where the $|\cdot|$ and power are scalar wise operations. If we do this then of course $\bf P_{ie}P_e \neq I\hspace{1cm}\bf P_{e}P_{ie} \neq I$, but that is because we are not finished yet! If we now turn each $2\times2$ block $\bf A$ into a scalar by the following calculation: $[1,0] {\bf A} [1,-1]^T$ And systematically apply to all blocks: $$\cases{({\bf I}_N \otimes [1,0])({\bf P_{ie}P_e})({\bf I}_N \otimes [1,-1]^T)\\ ({\bf I}_N \otimes [1,0])({\bf P_{e}P_{ie}})({\bf I}_N \otimes [1,-1]^T)}$$ Now we do indeed get identities! Row sums of $\bf P_{ie}$ equals 1 if we treat every second bin as $-1$ (as the construction above does). But how to interpret the elements of $\bf {P_{ie}}$ which can be >1 ? Furthermore $\sum_j |({\bf P_{ie}})_{ij}|$ seems to be a measure of the confusion, it goes to $\infty$ as ${\bf P} \to \frac{1}{N^2}{{\bf 1}_N} {{\bf 1}_N}^T$ which is indeed the maximally confusing distribution as we lose all information except the mean values ( all other eigenvalues except stationary distribution will be 0 ).",,"['linear-algebra', 'matrices', 'inverse', 'stochastic-matrices']"
72,Mathematical operator to extract diagonal elements of a square matrix,Mathematical operator to extract diagonal elements of a square matrix,,"I was wondering if anyone knows if an operator that extracts the main diagonal elements of a square matrix exists. I'm interested mainly in the mathematical definition and its properties (not in how to get the diagonal elements using MATLAB). Thanks! Edit: As pointed out in comments, it naturally exists. Again, I'm interested in the properties of such an operator. Example: assume that the operator is denoted by $dd\{\bullet\}$ where $\bullet$ is a square matrix. Is there any useful property to compute $dd\{\bullet + \bullet^{-1}\}$, provided that $\bullet^{-1}$ exists?","I was wondering if anyone knows if an operator that extracts the main diagonal elements of a square matrix exists. I'm interested mainly in the mathematical definition and its properties (not in how to get the diagonal elements using MATLAB). Thanks! Edit: As pointed out in comments, it naturally exists. Again, I'm interested in the properties of such an operator. Example: assume that the operator is denoted by $dd\{\bullet\}$ where $\bullet$ is a square matrix. Is there any useful property to compute $dd\{\bullet + \bullet^{-1}\}$, provided that $\bullet^{-1}$ exists?",,"['linear-algebra', 'matrices', 'linear-transformations', 'diagonalization']"
73,Geometric Interpretation of the Permanent,Geometric Interpretation of the Permanent,,"The Permanent of a square matrix M = ($m_{i,j}$) is defined as follows: $Perm(M) = \sum_{\sigma\in S_n}\prod_{i=1}^{n} m_{i,\sigma(i)}$ The Permanent is quite similar to the Determinant of a square matrix, defined as follows: $Det(M) = \sum_{\sigma\in S_n}sign(\sigma)\prod_{i=1}^{n} m_{i,\sigma(i)}$ The Determinant has an intuitive geometric interpretation. Is anything similar known about the Permanent? If not, why does the signed sum of the permutations lend itself to a geometric interpretation and the unsigned sum does not?","The Permanent of a square matrix M = ($m_{i,j}$) is defined as follows: $Perm(M) = \sum_{\sigma\in S_n}\prod_{i=1}^{n} m_{i,\sigma(i)}$ The Permanent is quite similar to the Determinant of a square matrix, defined as follows: $Det(M) = \sum_{\sigma\in S_n}sign(\sigma)\prod_{i=1}^{n} m_{i,\sigma(i)}$ The Determinant has an intuitive geometric interpretation. Is anything similar known about the Permanent? If not, why does the signed sum of the permutations lend itself to a geometric interpretation and the unsigned sum does not?",,"['linear-algebra', 'determinant', 'permanent']"
74,Are eigenvalues of A and B the same if B is a permutation of A?,Are eigenvalues of A and B the same if B is a permutation of A?,,"Let $A\in \mathbb{R}^{n\times n}$, and $B$ be a permutation of $A$, such that the rows and columns have the same permutation. That is, if row 1 and row 2 are interchanged, then column 1 and column 2 are also interchanged. Are the eigenvalues of $A$ and $B$ the same? How are the eigenvectors affected? Most of my searches bring up eigenvalues of permutation matrices or deal with only row permutations. Is there a name for this type of permutation? (So I know what to search for next time). Thank you in advance","Let $A\in \mathbb{R}^{n\times n}$, and $B$ be a permutation of $A$, such that the rows and columns have the same permutation. That is, if row 1 and row 2 are interchanged, then column 1 and column 2 are also interchanged. Are the eigenvalues of $A$ and $B$ the same? How are the eigenvectors affected? Most of my searches bring up eigenvalues of permutation matrices or deal with only row permutations. Is there a name for this type of permutation? (So I know what to search for next time). Thank you in advance",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
75,Any method to efficiently compute SVD of a perturbation of matrix $\bf A$ if the SVD of $\bf A$ is already known?,Any method to efficiently compute SVD of a perturbation of matrix  if the SVD of  is already known?,\bf A \bf A,"Suppose we know the SVD of matrix $\bf A$, and $\bf B$ is a slight perturbation of $A$ (e.g. $\|{\bf B}-{\bf A}\|_{\text F}$ is relatively small), then is there any method that can efficiently compute the SVD of $\bf B$? That is, can the knowledge of SVD of $\bf A$ be helpful for SVD of $\bf B$? I searched a little bit and found there are some papers on the bound of perturbation, e.g. Perturbation Theory for the Singular Value Decomposition , but I currently have no luck in finding a method to compute SVD of perturbation taking advantage of the SVD of the original matrix. Any help or reference will be very much appreciated!","Suppose we know the SVD of matrix $\bf A$, and $\bf B$ is a slight perturbation of $A$ (e.g. $\|{\bf B}-{\bf A}\|_{\text F}$ is relatively small), then is there any method that can efficiently compute the SVD of $\bf B$? That is, can the knowledge of SVD of $\bf A$ be helpful for SVD of $\bf B$? I searched a little bit and found there are some papers on the bound of perturbation, e.g. Perturbation Theory for the Singular Value Decomposition , but I currently have no luck in finding a method to compute SVD of perturbation taking advantage of the SVD of the original matrix. Any help or reference will be very much appreciated!",,"['linear-algebra', 'numerical-methods', 'numerical-linear-algebra', 'svd']"
76,On a function from a group with values lying in an inner product space,On a function from a group with values lying in an inner product space,,"Let $(G,+)$ be a group and $V$ be an inner product space (over $\mathbb R$ , or $ \mathbb C $ ) ; let $f:G \to V$ be a function such that $||f(x+y)||\ge ||f(x)+f(y)|| , \forall x,y\in G$ , then how to prove that $f(x+y)=f(x)+f(y),\forall x,y \in G$ ? If $(S,+)$ is a semigroup and $V$ be an inner product space (over $\mathbb R$ , or $ \mathbb C $ ) ; let $f:S \to V$ be a function such that $||f(x+y)||= ||f(x)+f(y)|| , \forall x,y\in S$ , then I can prove that  $f(x+y)=f(x)+f(y),\forall x,y \in S$ . But I don't know how to do the the aforementioned problem. Please help . Thanks in advance","Let $(G,+)$ be a group and $V$ be an inner product space (over $\mathbb R$ , or $ \mathbb C $ ) ; let $f:G \to V$ be a function such that $||f(x+y)||\ge ||f(x)+f(y)|| , \forall x,y\in G$ , then how to prove that $f(x+y)=f(x)+f(y),\forall x,y \in G$ ? If $(S,+)$ is a semigroup and $V$ be an inner product space (over $\mathbb R$ , or $ \mathbb C $ ) ; let $f:S \to V$ be a function such that $||f(x+y)||= ||f(x)+f(y)|| , \forall x,y\in S$ , then I can prove that  $f(x+y)=f(x)+f(y),\forall x,y \in S$ . But I don't know how to do the the aforementioned problem. Please help . Thanks in advance",,"['linear-algebra', 'group-theory']"
77,What can we say if the dot product of two vectors is equal to 1,What can we say if the dot product of two vectors is equal to 1,,The question really is in the title. I know what it means if the dot product equals 0 but I find it interesting thinking what it means when it equals exactly 1 and can't seem to find anything online to enlighten me. Thanks,The question really is in the title. I know what it means if the dot product equals 0 but I find it interesting thinking what it means when it equals exactly 1 and can't seem to find anything online to enlighten me. Thanks,,['linear-algebra']
78,Sample points from a multivariate normal distribution using only the precision matrix?,Sample points from a multivariate normal distribution using only the precision matrix?,,"I have a problem where I can directly compute the (sparse) precision matrix (inverse of the covariance) of a multivariate normal distribution, but the covariance itself is not sparse and I don't want to invert things. I would like to sample points using the precision matrix. What's a fast way to do this? I do know that the standard procedure is to compute the cholesky decomposition of the covariance , that is find lower triangular matrix L such that $LL' = \Sigma$ and then use a univariate generator to compute a vector of iid normal points $u$ so that finally the vector $z = \mu + Lu$ has the correct covariance. But that would require me computing the covariance AND then taking its Cholesky decomposition. Is there anything faster, using the fact that I have the precision matrix?","I have a problem where I can directly compute the (sparse) precision matrix (inverse of the covariance) of a multivariate normal distribution, but the covariance itself is not sparse and I don't want to invert things. I would like to sample points using the precision matrix. What's a fast way to do this? I do know that the standard procedure is to compute the cholesky decomposition of the covariance , that is find lower triangular matrix L such that $LL' = \Sigma$ and then use a univariate generator to compute a vector of iid normal points $u$ so that finally the vector $z = \mu + Lu$ has the correct covariance. But that would require me computing the covariance AND then taking its Cholesky decomposition. Is there anything faster, using the fact that I have the precision matrix?",,['linear-algebra']
79,"Show any orthogonal matrix is similar to an almost diagonal matrix, with either $\pm 1$ or a 2D rotation on the diagonal","Show any orthogonal matrix is similar to an almost diagonal matrix, with either  or a 2D rotation on the diagonal",\pm 1,"Let $A \in O(n).$  Show that $A$ is similar to a matrix which consists of $2 \times 2$ blocks down the diagonal of the form $$ \begin{pmatrix} \cos{\theta} & \sin{\theta}\\-\sin{\theta} & \cos{\theta} \end{pmatrix},$$ along with some diagonal elements which are $+1$ and $-1.$ For example, maybe $A \in O(5)$ is similar to, $$\begin{pmatrix}  1 & 0 & 0 & 0 & 0\\ 0 & \cos{\theta} & \sin{\theta} & 0 & 0\\ 0 & -\sin{\theta} & \cos{\theta} & 0 & 0\\ 0 & 0 & 0 & 1 & 0\\ 0 & 0 & 0 & 0 & -1 \end{pmatrix}.$$ This makes intuitive sense to me, though I am not sure how I'd go about the proof.  I've been trying to use a subspace invariant method, like $W$ is a subspace of $\mathbb{R^n}$ that is $A$-invariant, then its orthogonal complement is too, but it hasn't gotten me much closer.  Any hints/solutions?","Let $A \in O(n).$  Show that $A$ is similar to a matrix which consists of $2 \times 2$ blocks down the diagonal of the form $$ \begin{pmatrix} \cos{\theta} & \sin{\theta}\\-\sin{\theta} & \cos{\theta} \end{pmatrix},$$ along with some diagonal elements which are $+1$ and $-1.$ For example, maybe $A \in O(5)$ is similar to, $$\begin{pmatrix}  1 & 0 & 0 & 0 & 0\\ 0 & \cos{\theta} & \sin{\theta} & 0 & 0\\ 0 & -\sin{\theta} & \cos{\theta} & 0 & 0\\ 0 & 0 & 0 & 1 & 0\\ 0 & 0 & 0 & 0 & -1 \end{pmatrix}.$$ This makes intuitive sense to me, though I am not sure how I'd go about the proof.  I've been trying to use a subspace invariant method, like $W$ is a subspace of $\mathbb{R^n}$ that is $A$-invariant, then its orthogonal complement is too, but it hasn't gotten me much closer.  Any hints/solutions?",,"['linear-algebra', 'matrices', 'orthogonality', 'orthogonal-matrices']"
80,Similarity classes of matrices,Similarity classes of matrices,,"Let $M_n(K)$ be the set of all $n\times n$ matrices over a field $K$. If $\mathcal{R}$ is the equivalence relation defined by matrix similarity, what does the quotient $M_n(K)/\mathcal{R}$ looks like? Is there something that characterizes it in terms of cardinality? Is there a way to extend matrix operations to equivalence classes, making it an algebraic quotient structure of $M_n(K)$? Is there a good interpretation of $M_n(K)/\mathcal{R}$ in terms of similarity invariants (e.g., matrix determinants)? Today, on my linear algebra class, we were introduced the concept of determinants. Our teacher used geometric isometry invariants (such as area and volume) to introduce the motivation for matrix determinants. I then asked myself what would be the generalized interpretation of the determinant as some type of ''invariant''. Some Google search led me to matrix similarity, and my algebraic intuition says that this information would be codified in the quotient structure - once we ''kill' the big structure by the invariants, we would find exactly what is not varying. Well, though I never though about verifying it for any pathological case, I believe that the map $\det: M_n(K) \to K$ is surjective. That would mean that the quotient $M_n(K)/\mathcal{R}$ is at least the same cardinality as $K$ (because similar matrices have the same determinant, but I'm not sure about the converse. If it is true, matrix with the same determinant are similar, then cardinality equality would follow). I'm not sure how the operations can be extended. Maybe the quotient can be seen as a vector space over $K$ as well: scalar multiplication is surely well defined, but I'm not sure about the sum. If I restrict myself to some subset of $M_n(K)$, like the general linear group or the special linear group, could I get something more? And the interpretation or mathematical application is exactly what I long for.","Let $M_n(K)$ be the set of all $n\times n$ matrices over a field $K$. If $\mathcal{R}$ is the equivalence relation defined by matrix similarity, what does the quotient $M_n(K)/\mathcal{R}$ looks like? Is there something that characterizes it in terms of cardinality? Is there a way to extend matrix operations to equivalence classes, making it an algebraic quotient structure of $M_n(K)$? Is there a good interpretation of $M_n(K)/\mathcal{R}$ in terms of similarity invariants (e.g., matrix determinants)? Today, on my linear algebra class, we were introduced the concept of determinants. Our teacher used geometric isometry invariants (such as area and volume) to introduce the motivation for matrix determinants. I then asked myself what would be the generalized interpretation of the determinant as some type of ''invariant''. Some Google search led me to matrix similarity, and my algebraic intuition says that this information would be codified in the quotient structure - once we ''kill' the big structure by the invariants, we would find exactly what is not varying. Well, though I never though about verifying it for any pathological case, I believe that the map $\det: M_n(K) \to K$ is surjective. That would mean that the quotient $M_n(K)/\mathcal{R}$ is at least the same cardinality as $K$ (because similar matrices have the same determinant, but I'm not sure about the converse. If it is true, matrix with the same determinant are similar, then cardinality equality would follow). I'm not sure how the operations can be extended. Maybe the quotient can be seen as a vector space over $K$ as well: scalar multiplication is surely well defined, but I'm not sure about the sum. If I restrict myself to some subset of $M_n(K)$, like the general linear group or the special linear group, could I get something more? And the interpretation or mathematical application is exactly what I long for.",,"['linear-algebra', 'matrices', 'determinant', 'invariance']"
81,Let $A$ be an $n \times n$ matrix over $\mathbb{C}$ or $\mathbb{R}$. Does $\det(e^A) = e^{\mathrm{tr}(A)}$ always hold?,Let  be an  matrix over  or . Does  always hold?,A n \times n \mathbb{C} \mathbb{R} \det(e^A) = e^{\mathrm{tr}(A)},"Let $A$ be an $n \times n$ matrix over a field $\mathbb{K}$ where $\mathbb{K} = \mathbb{C}$ or $\mathbb{K} = \mathbb{R}$. Does $\det(e^A) = e^{\mathrm{tr}(A)}$ always hold? If the field is $\mathbb{C}$, this can be easily proven using the Jordan canonical form. Now, if the field is $\mathbb{R}$, the Jordan canonical form may not exist for $A$. So does $\det(e^A) = e^{\mathrm{tr}(A)}$ hold for any square matrix in $\mathbb{R}$ or only when $A$ meets certain conditions (e.g. $A$ has a Jordan canonical form)?","Let $A$ be an $n \times n$ matrix over a field $\mathbb{K}$ where $\mathbb{K} = \mathbb{C}$ or $\mathbb{K} = \mathbb{R}$. Does $\det(e^A) = e^{\mathrm{tr}(A)}$ always hold? If the field is $\mathbb{C}$, this can be easily proven using the Jordan canonical form. Now, if the field is $\mathbb{R}$, the Jordan canonical form may not exist for $A$. So does $\det(e^A) = e^{\mathrm{tr}(A)}$ hold for any square matrix in $\mathbb{R}$ or only when $A$ meets certain conditions (e.g. $A$ has a Jordan canonical form)?",,['linear-algebra']
82,AB and BA have identical nonsingular Jordan blocks,AB and BA have identical nonsingular Jordan blocks,,If A and B are square matrices of the same size I know how to show that AB and BA have the same eigenvalues and characteristic polynomials. But I want to show that they have identical nonsingular Jordan Blocks. I am not really sure how to proceed. Maybe some argument about the dimension of the kernel of AB and BA?,If A and B are square matrices of the same size I know how to show that AB and BA have the same eigenvalues and characteristic polynomials. But I want to show that they have identical nonsingular Jordan Blocks. I am not really sure how to proceed. Maybe some argument about the dimension of the kernel of AB and BA?,,"['linear-algebra', 'matrices', 'jordan-normal-form']"
83,Does an eigenspace of a matrix depend continuously on its components?,Does an eigenspace of a matrix depend continuously on its components?,,"Let $M(x)$ be a diagonalisable $n \times n$ complex matrix whose components are continuous functions of $x$ and suppose that, for all $x$, $M$ has eigenvalue $0$ with multiplicity $m < n$ (independent of $x$).  Is it possible to choose a basis for the $0$-eigenspace of $M$ whose components are continuous functions of $x$?","Let $M(x)$ be a diagonalisable $n \times n$ complex matrix whose components are continuous functions of $x$ and suppose that, for all $x$, $M$ has eigenvalue $0$ with multiplicity $m < n$ (independent of $x$).  Is it possible to choose a basis for the $0$-eigenspace of $M$ whose components are continuous functions of $x$?",,['linear-algebra']
84,Properties of the per-element exponential (Hadamard exponential) for matrices,Properties of the per-element exponential (Hadamard exponential) for matrices,,"I'm asking this question mostly out of curiosity, though I do also have a potential application. In linear algebra we usually define the matrix emponential as $e^A = I + A + \frac{1}{2}A^2 + \frac{1}{6}A^3 + \dots$, which has lots of nice properties. However, we could also define a different kind of ""matrix exponentiation"", which I'll write $e^{\circ A}$, where $(e^{\circ A})_{ij} = e^{A_{ij}}$, i.e. we just apply the exponential function to each element independently. After writing this question I guessed that the name of this operation would be ""Hadamard exponential."" An internet search revealed that it's mentioned by this name in a few textbooks and research papers, but in general I can find very little written about its properties from a linear algebra point of view. (I've edited this post to use what seems to be standard notation for the Hadamard exponential.) One obvious thing is that it inherits all the usual properties of exponentiation, as long as we use the Hadamrd product $(\circ)$ (i.e. per-element multiplication) instead of the usual matrix product. Then we can immediately apply results like the Schur product theorem to conclude that if $e^{\circ A}$ and $e^{\circ B}$ are both positive definite then so is $e^{\circ (A+B)}$. Another obvious property is that for real matrices, the elements of $e^{\circ A}$ are all positive, and hence the Perron-Frobenius theorem applies. However, what I would particularly like to know is whether anything can be said about the eigenvalues and eigenvectors of $e^{\circ A}$ in terms of the eigendecomposition of $A$. I suspect that there is no straightforward relationship in general, but I would expect there to be inequality constraints. In short, my question is, has the operation I've called $\operatorname{eexp}$ been studied in linear algebra, and what is known about its properties?","I'm asking this question mostly out of curiosity, though I do also have a potential application. In linear algebra we usually define the matrix emponential as $e^A = I + A + \frac{1}{2}A^2 + \frac{1}{6}A^3 + \dots$, which has lots of nice properties. However, we could also define a different kind of ""matrix exponentiation"", which I'll write $e^{\circ A}$, where $(e^{\circ A})_{ij} = e^{A_{ij}}$, i.e. we just apply the exponential function to each element independently. After writing this question I guessed that the name of this operation would be ""Hadamard exponential."" An internet search revealed that it's mentioned by this name in a few textbooks and research papers, but in general I can find very little written about its properties from a linear algebra point of view. (I've edited this post to use what seems to be standard notation for the Hadamard exponential.) One obvious thing is that it inherits all the usual properties of exponentiation, as long as we use the Hadamrd product $(\circ)$ (i.e. per-element multiplication) instead of the usual matrix product. Then we can immediately apply results like the Schur product theorem to conclude that if $e^{\circ A}$ and $e^{\circ B}$ are both positive definite then so is $e^{\circ (A+B)}$. Another obvious property is that for real matrices, the elements of $e^{\circ A}$ are all positive, and hence the Perron-Frobenius theorem applies. However, what I would particularly like to know is whether anything can be said about the eigenvalues and eigenvectors of $e^{\circ A}$ in terms of the eigendecomposition of $A$. I suspect that there is no straightforward relationship in general, but I would expect there to be inequality constraints. In short, my question is, has the operation I've called $\operatorname{eexp}$ been studied in linear algebra, and what is known about its properties?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'exponential-function']"
85,Textbook on Graph Theory using Linear Algebra,Textbook on Graph Theory using Linear Algebra,,Is there any undergraduate textbook on graph theory using linear algebra? A request is a beginning with graph matrices that explain most concepts in graph theory? P.s. This thread has more specific requests than this thread What are good books to learn graph theory? .,Is there any undergraduate textbook on graph theory using linear algebra? A request is a beginning with graph matrices that explain most concepts in graph theory? P.s. This thread has more specific requests than this thread What are good books to learn graph theory? .,,"['linear-algebra', 'reference-request', 'graph-theory', 'network']"
86,Find eigenspace for eigenvalues of a linear transformation $T(M)=-2M^t+M$,Find eigenspace for eigenvalues of a linear transformation,T(M)=-2M^t+M,"Let $V$ be the matrix space $4 \times 4$ over $\Bbb R$. $T: V \to V$ is a linear transformation defined by: $$T(M)=-2M^t+M$$ for all $M \in V$. Find the minimal polynomial of T. For every eigenvalue $\lambda$ of $T$, find the eigenspace $V_\lambda$ and calculate its dimension. Find $T$'s characteristic polynomial. I have solved section 1 this way: Let $S(M)=M^t$. Therefore $S^2(M)=M \Rightarrow S^2=I$. $$T=-2S(M)+S^2$$ Since $$ S^2=M$$ $$\Rightarrow T=-2S+I$$ $$\Rightarrow 2S=I-T$$ $$\Rightarrow 4S^2=(I-T)^2=I^2-2T+T^2=I-2T+T^2$$ Since $$4S^2=4I$$ $$\Rightarrow T^2-2T-3I=0$$ $$\Rightarrow(T+1)(T-3)=0$$ When further explanations we get the the eigenvalues are $$\lambda=-1$$ $$\lambda =3$$ However, to find eigenspace I need the original matrix, to calculate $$(A-\lambda I)$$ How do I find such a matrix for calculation? Thanks, Alan","Let $V$ be the matrix space $4 \times 4$ over $\Bbb R$. $T: V \to V$ is a linear transformation defined by: $$T(M)=-2M^t+M$$ for all $M \in V$. Find the minimal polynomial of T. For every eigenvalue $\lambda$ of $T$, find the eigenspace $V_\lambda$ and calculate its dimension. Find $T$'s characteristic polynomial. I have solved section 1 this way: Let $S(M)=M^t$. Therefore $S^2(M)=M \Rightarrow S^2=I$. $$T=-2S(M)+S^2$$ Since $$ S^2=M$$ $$\Rightarrow T=-2S+I$$ $$\Rightarrow 2S=I-T$$ $$\Rightarrow 4S^2=(I-T)^2=I^2-2T+T^2=I-2T+T^2$$ Since $$4S^2=4I$$ $$\Rightarrow T^2-2T-3I=0$$ $$\Rightarrow(T+1)(T-3)=0$$ When further explanations we get the the eigenvalues are $$\lambda=-1$$ $$\lambda =3$$ However, to find eigenspace I need the original matrix, to calculate $$(A-\lambda I)$$ How do I find such a matrix for calculation? Thanks, Alan",,"['linear-algebra', 'eigenvalues-eigenvectors', 'minimal-polynomials']"
87,Find all $n\times n$ matrices $A$ satisfying $\det(I+A^n)=(1+\det(A))^n$,Find all  matrices  satisfying,n\times n A \det(I+A^n)=(1+\det(A))^n,"Problem: Find all $n\times n$ matrices $A$ satisfying   $$\det(I+A^n)=(1+\det(A))^n.$$ Clearly, the identity matrix $I$ works because $$\det(I+I^n)=\det(2I)=2^n=(1+\det(I))^n.$$ Are there any other solutions?","Problem: Find all $n\times n$ matrices $A$ satisfying   $$\det(I+A^n)=(1+\det(A))^n.$$ Clearly, the identity matrix $I$ works because $$\det(I+I^n)=\det(2I)=2^n=(1+\det(I))^n.$$ Are there any other solutions?",,"['linear-algebra', 'matrices']"
88,Determinant of the Transpose of an Operator.,Determinant of the Transpose of an Operator.,,"Let $V$ be a vector space over a field $F$ of characteristic $0$. A linear operator $T$ on $V$ induces a linear operator $\Lambda^k T:\Lambda^k V\to \Lambda^k V$ such that $\Lambda^k T(v_1\wedge \cdots\wedge v_k)=Tv_1\wedge\cdots\wedge Tv_k$ for all $v_1, \ldots, v_k\in V$. If $n=\dim V$, then since $\dim(\Lambda^n V)=1$, we know that there is a unique $c\in F$ such that $\Lambda^n T(v_1 \wedge \cdots\wedge v_n)=c\cdot(v_1\wedge \cdots\wedge v_n)$. We call this constant the determinant of $T$. From this definition of the determinant, it immediately follows that $\det(TS)=(\det T)(\det S)$ for all linear operators $S$ and $T$ on $V$. Can we also easily show that $\det T^t=\det T$ for all $T\in \mathcal L(V)$? Here $T^t$ denotes the transpose of $T$.","Let $V$ be a vector space over a field $F$ of characteristic $0$. A linear operator $T$ on $V$ induces a linear operator $\Lambda^k T:\Lambda^k V\to \Lambda^k V$ such that $\Lambda^k T(v_1\wedge \cdots\wedge v_k)=Tv_1\wedge\cdots\wedge Tv_k$ for all $v_1, \ldots, v_k\in V$. If $n=\dim V$, then since $\dim(\Lambda^n V)=1$, we know that there is a unique $c\in F$ such that $\Lambda^n T(v_1 \wedge \cdots\wedge v_n)=c\cdot(v_1\wedge \cdots\wedge v_n)$. We call this constant the determinant of $T$. From this definition of the determinant, it immediately follows that $\det(TS)=(\det T)(\det S)$ for all linear operators $S$ and $T$ on $V$. Can we also easily show that $\det T^t=\det T$ for all $T\in \mathcal L(V)$? Here $T^t$ denotes the transpose of $T$.",,"['linear-algebra', 'determinant', 'tensor-products', 'multilinear-algebra']"
89,"Definitions of $\mathrm{Hom}(V,W)$",Definitions of,"\mathrm{Hom}(V,W)","I have the definition of a homomorphism as map such that $\varphi(g_1g_2)=\varphi(g_1)\varphi(g_2)$ I have the definition of $\mathrm{Hom}(V,W)$ as $$\begin{align}\mathrm{Hom}(V,W) &= \{\mathbb{C}-\text{linear maps }\varphi:V \rightarrow W \} \\ &\cong \{n \times m \text{ matrices } \} \end{align}$$ Does $\mathrm{Hom}$ stand for homomorphisms because I cannot see how it relates to the definition of homomorphism I have the definition of $\mathrm{Hom}_G$ as $$\begin{align}\mathrm{Hom}_G(V,W) &= \{\varphi \in \mathrm{Hom}(V,W) \mid g\varphi(v)=\varphi(gv), \forall g \in G, \forall v \in V\} \\ &=\{\varphi \in \mathrm{Hom}(V,W) \mid \rho(g)\cdot\varphi(v)=\varphi(\rho(g)v) , \forall g \in G, \forall v \in V\} \end{align}$$ I then have the question: Prove that if $\rho$ is an irreducible representation, then for an element $g \in G$ $$g \in Z(G) \iff \rho(g)=\lambda I$$ In the solution I have that: $(``\implies"")$ If $g \in Z(G)$ then $gh=hg \ \forall h \in G$. By definition of $\mathrm{Hom}_G$ this means that $\rho(g) \in \mathrm{Hom}_G(V,V)$. But \rho is irreducible so $\mathrm{Hom}_G(V,V)$ consists of scalar matrices by Schur's Lemma. I do not understand how ""By definition of $\mathrm{Hom}_G$ this means that $\rho(g) \in \mathrm{Hom}_G(V,V)$. "" How is this the definiton of $\mathrm{Hom}_G$? I cannot see why this is equivalent.","I have the definition of a homomorphism as map such that $\varphi(g_1g_2)=\varphi(g_1)\varphi(g_2)$ I have the definition of $\mathrm{Hom}(V,W)$ as $$\begin{align}\mathrm{Hom}(V,W) &= \{\mathbb{C}-\text{linear maps }\varphi:V \rightarrow W \} \\ &\cong \{n \times m \text{ matrices } \} \end{align}$$ Does $\mathrm{Hom}$ stand for homomorphisms because I cannot see how it relates to the definition of homomorphism I have the definition of $\mathrm{Hom}_G$ as $$\begin{align}\mathrm{Hom}_G(V,W) &= \{\varphi \in \mathrm{Hom}(V,W) \mid g\varphi(v)=\varphi(gv), \forall g \in G, \forall v \in V\} \\ &=\{\varphi \in \mathrm{Hom}(V,W) \mid \rho(g)\cdot\varphi(v)=\varphi(\rho(g)v) , \forall g \in G, \forall v \in V\} \end{align}$$ I then have the question: Prove that if $\rho$ is an irreducible representation, then for an element $g \in G$ $$g \in Z(G) \iff \rho(g)=\lambda I$$ In the solution I have that: $(``\implies"")$ If $g \in Z(G)$ then $gh=hg \ \forall h \in G$. By definition of $\mathrm{Hom}_G$ this means that $\rho(g) \in \mathrm{Hom}_G(V,V)$. But \rho is irreducible so $\mathrm{Hom}_G(V,V)$ consists of scalar matrices by Schur's Lemma. I do not understand how ""By definition of $\mathrm{Hom}_G$ this means that $\rho(g) \in \mathrm{Hom}_G(V,V)$. "" How is this the definiton of $\mathrm{Hom}_G$? I cannot see why this is equivalent.",,"['linear-algebra', 'group-theory', 'finite-groups', 'representation-theory', 'characters']"
90,How do we find eigenvalues from given eigenvectors of a given matrix?,How do we find eigenvalues from given eigenvectors of a given matrix?,,"For instance let  $$A=\begin{pmatrix}         3 & -1 & -1 \\         2 & 1 &-2 \\         0 & -1 & 2 \\         \end{pmatrix}$$ be a matrix  and  $$u_1=\begin{pmatrix}         1 \\         1  \\         1 \\         \end{pmatrix},$$ $$u_2=\begin{pmatrix}         1 \\         0 \\         1\\         \end{pmatrix},$$ $$u_3=\begin{pmatrix}         0 \\         -1 \\         1 \\         \end{pmatrix}.$$ its eigenvectors. What are its eigenvalues? Is there anything more simple than doing $A-I$?","For instance let  $$A=\begin{pmatrix}         3 & -1 & -1 \\         2 & 1 &-2 \\         0 & -1 & 2 \\         \end{pmatrix}$$ be a matrix  and  $$u_1=\begin{pmatrix}         1 \\         1  \\         1 \\         \end{pmatrix},$$ $$u_2=\begin{pmatrix}         1 \\         0 \\         1\\         \end{pmatrix},$$ $$u_3=\begin{pmatrix}         0 \\         -1 \\         1 \\         \end{pmatrix}.$$ its eigenvectors. What are its eigenvalues? Is there anything more simple than doing $A-I$?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
91,A ring without the Invariant Basis Number property,A ring without the Invariant Basis Number property,,"I was reviewing my homework and it seems I overlooked something crucial while proving some ring has no Invariant Basis Number property. This is exercise VI.1.12 in Aluffi's Algebra: Chapter 0 The setup: $V$ is a $k$-vector space and let $R = \mathrm{End}_{k}(V)$. Prove that $\mathrm{End}_{k}(V\oplus V) \cong R^4$ as an $R$-module Prove that $R$ doesn't satisfy the IBN property if $V = k^{\oplus \mathbb N}$. For the first, I used to the fact that $V \oplus V$ is both the product and coproduct (in $k$-Vect) of $V$ with itself to get the isomorphism. What I just realized is I only showed that the two are isomorphic as groups not $R$-modules. So what would be the $R$-module structure on $\mathrm{End}_{k}(V \oplus V)$? For the second, I used the fact that $V = k^{\oplus \mathbb N}$ implies $V \cong V \oplus V$ which in turn implies $R = \mathrm{End}_{k}(V) \cong \mathrm{End}_{k}(V \oplus V)$. Again, I just realized that I only showed the latter two are isomorphic as groups. It may be obvious (and maybe why my professor let it pass?) but I can't come up with a good $R$-module structure that makes the two group isomorphisms $R$-linear. Edit: Explicitly, these are the isomorphisms I'm dealing with. Let $\pi_j, i_j$ be the natural projection/inclusion maps of the $j$-th factor resp. and $\psi: k^{\oplus \mathbb N} \oplus k^{\oplus \mathbb N} \to k^{\oplus \mathbb N}$ the isomorphism given by $\psi(e_i, 0)=e_{2i-1}$ and $\psi(0, e_i)=e_{2i}$. Then the first isomorphism $\mathrm{End}_k(V \oplus V)\to R^4$ is given by $\varphi \mapsto (\pi_1\varphi i_1,\pi_2\varphi i_1,\pi_1\varphi i_2,\pi_2\varphi i_2)$ The second isomorphism $R \to \mathrm{End}_k(V \oplus V)$ is given by $\alpha \mapsto \psi^{-1} \alpha \psi$ The composition doesn't seem to be $R$-linear if I use the obvious $R$-module structure on $R$ and $R^4$.","I was reviewing my homework and it seems I overlooked something crucial while proving some ring has no Invariant Basis Number property. This is exercise VI.1.12 in Aluffi's Algebra: Chapter 0 The setup: $V$ is a $k$-vector space and let $R = \mathrm{End}_{k}(V)$. Prove that $\mathrm{End}_{k}(V\oplus V) \cong R^4$ as an $R$-module Prove that $R$ doesn't satisfy the IBN property if $V = k^{\oplus \mathbb N}$. For the first, I used to the fact that $V \oplus V$ is both the product and coproduct (in $k$-Vect) of $V$ with itself to get the isomorphism. What I just realized is I only showed that the two are isomorphic as groups not $R$-modules. So what would be the $R$-module structure on $\mathrm{End}_{k}(V \oplus V)$? For the second, I used the fact that $V = k^{\oplus \mathbb N}$ implies $V \cong V \oplus V$ which in turn implies $R = \mathrm{End}_{k}(V) \cong \mathrm{End}_{k}(V \oplus V)$. Again, I just realized that I only showed the latter two are isomorphic as groups. It may be obvious (and maybe why my professor let it pass?) but I can't come up with a good $R$-module structure that makes the two group isomorphisms $R$-linear. Edit: Explicitly, these are the isomorphisms I'm dealing with. Let $\pi_j, i_j$ be the natural projection/inclusion maps of the $j$-th factor resp. and $\psi: k^{\oplus \mathbb N} \oplus k^{\oplus \mathbb N} \to k^{\oplus \mathbb N}$ the isomorphism given by $\psi(e_i, 0)=e_{2i-1}$ and $\psi(0, e_i)=e_{2i}$. Then the first isomorphism $\mathrm{End}_k(V \oplus V)\to R^4$ is given by $\varphi \mapsto (\pi_1\varphi i_1,\pi_2\varphi i_1,\pi_1\varphi i_2,\pi_2\varphi i_2)$ The second isomorphism $R \to \mathrm{End}_k(V \oplus V)$ is given by $\alpha \mapsto \psi^{-1} \alpha \psi$ The composition doesn't seem to be $R$-linear if I use the obvious $R$-module structure on $R$ and $R^4$.",,"['linear-algebra', 'abstract-algebra', 'modules']"
92,Is the Frobenius norm induced by two vector norms?,Is the Frobenius norm induced by two vector norms?,,"In spaces $V$ and $W$ , define vector norms $\| \cdot \|_V $ and $\| \cdot \|_W $ , respectively. Consider the operator norm induced by norms $\| \cdot \|_V $ and $ \| \cdot \|_W $ $$ \| A \| = \sup{\frac{\|Ax\|_W}{\|x\|_V}} $$ Do there exist norms $ \|\cdot\|_V $ and $ \|\cdot\|_W $ such that the Frobenius norm is induced by these norms? It is well known that for any matrix norm induced by $ \|\cdot\| $ , $ \|I\| = 1 $ . But it is not truth for two different norms in $V$ and $W$ . Can anybody help?","In spaces and , define vector norms and , respectively. Consider the operator norm induced by norms and Do there exist norms and such that the Frobenius norm is induced by these norms? It is well known that for any matrix norm induced by , . But it is not truth for two different norms in and . Can anybody help?",V W \| \cdot \|_V  \| \cdot \|_W  \| \cdot \|_V   \| \cdot \|_W   \| A \| = \sup{\frac{\|Ax\|_W}{\|x\|_V}}   \|\cdot\|_V   \|\cdot\|_W   \|\cdot\|   \|I\| = 1  V W,"['linear-algebra', 'matrices', 'normed-spaces', 'matrix-norms']"
93,"For which $n, k$ is $S_{n,k}$ a basis? Fun algebra problem",For which  is  a basis? Fun algebra problem,"n, k S_{n,k}","Here it is a nice algebra problem I had some fun with Let $V$ be a vector space over $\mathbb R$ of finite dimension $\dim V = n$. Let $v = \{ v_1, \dots, v_n\}$ be a basis for $V$. Let $$S_{n,k} = \{ v_1 + v_2 + \dots + v_k, v_2 + \dots + v_{k+1}, \dots, v_n + v_1 + \dots + v_{k-1}\}$$  For which $n, k$ we have that $S_{n,k}$ is still a basis for $V$?","Here it is a nice algebra problem I had some fun with Let $V$ be a vector space over $\mathbb R$ of finite dimension $\dim V = n$. Let $v = \{ v_1, \dots, v_n\}$ be a basis for $V$. Let $$S_{n,k} = \{ v_1 + v_2 + \dots + v_k, v_2 + \dots + v_{k+1}, \dots, v_n + v_1 + \dots + v_{k-1}\}$$  For which $n, k$ we have that $S_{n,k}$ is still a basis for $V$?",,"['linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors', 'problem-solving']"
94,"""Internal"" and ""external"" in maths, and also in vector spaces","""Internal"" and ""external"" in maths, and also in vector spaces",,"I have looked at 3 books and it is clear that ""internal"" and ""external"" are two styles of defining something, I would like to know what they mean ""generally"" - that is very soft but it is clear to me that ""internally"" doing something is different from doing it ""externally"" even if the result is similar. I've read ahead and motivating examples or even uses of these definitions are not given, so if you have any examples that come to mind please do share. First it defines External direct sum using a $+$ inside a $\square$ of a finite number of vector spaces over the same field (I understand ""external"" might mean ""not considering them as subspaces of something"") $V=V_1+...+V_n$ is defined component wise (as vectors themselves) This is fine, I am happy with this. Direct product next and we have a family $\funky{F}=\{v_i|i\in K\}$ the direct product is: $\Pi_{i\in K}V_i=\{f:k\rightarrow\cup_{i\in K}V_i|f(i)\in V_i\}$ At first this seems horrible but really it's not too bad, $f$ is acting as a projection to coordinates really. This is a vector space itself and thinking about it I ask myself ""How is that not the external direct sum (provided the family is finite)"" This is not what I was taught for sum take the plane $x=0$ and the plane $y=0$ in $\mathbb{R}^3$, their union is just an extruded $+$ shape but what I was taught was the sum becomes all of $\mathbb{R}^3$ because the sum was defined as $\{u+v|u\in V_1 v \in V_2\}$ But anyway, that's direct product. External direct sum This time it is denoted not by a $+$ in a square but by a + in a circle with ""ext"" in superscript, due to lack of knowledge of the name of this symbol I must describe it to you. Anyway this sum is (on a family as above) $\{f:K\rightarrow\cup_{i\in K}V_i|f(i)\in V_i, f\text{ has finite support}\}$ finite support means $f(i)=0$ for all but a finite number of $i$ I dare not come up with examples because it's not finite. Lastly Internal direct sums This is written as a + with a circle around it, and it requires the following hold: The join of the family is V, that is: $V=\sum_{i\in K}S_i$ which is the more conventional sum I assume, and independence of the family, that is: $S_i\cap(\sum_{j\ne i}S_j)=\{0\}$ It notes that this second condition is stronger than pairwise disjoint, I cannot think of an example which shows this distinction and I'd like one. That's all the book does on these, unless they are used far later. What is going on here? Also what do ""internal"" and ""external"" mean, I can't think of an example but I think it refers to a style of definition, I've come across and gotten used to similar things before. The book is: Advanced Linear Algebra, Steven Roman, 3rd Edition, GTM # 135","I have looked at 3 books and it is clear that ""internal"" and ""external"" are two styles of defining something, I would like to know what they mean ""generally"" - that is very soft but it is clear to me that ""internally"" doing something is different from doing it ""externally"" even if the result is similar. I've read ahead and motivating examples or even uses of these definitions are not given, so if you have any examples that come to mind please do share. First it defines External direct sum using a $+$ inside a $\square$ of a finite number of vector spaces over the same field (I understand ""external"" might mean ""not considering them as subspaces of something"") $V=V_1+...+V_n$ is defined component wise (as vectors themselves) This is fine, I am happy with this. Direct product next and we have a family $\funky{F}=\{v_i|i\in K\}$ the direct product is: $\Pi_{i\in K}V_i=\{f:k\rightarrow\cup_{i\in K}V_i|f(i)\in V_i\}$ At first this seems horrible but really it's not too bad, $f$ is acting as a projection to coordinates really. This is a vector space itself and thinking about it I ask myself ""How is that not the external direct sum (provided the family is finite)"" This is not what I was taught for sum take the plane $x=0$ and the plane $y=0$ in $\mathbb{R}^3$, their union is just an extruded $+$ shape but what I was taught was the sum becomes all of $\mathbb{R}^3$ because the sum was defined as $\{u+v|u\in V_1 v \in V_2\}$ But anyway, that's direct product. External direct sum This time it is denoted not by a $+$ in a square but by a + in a circle with ""ext"" in superscript, due to lack of knowledge of the name of this symbol I must describe it to you. Anyway this sum is (on a family as above) $\{f:K\rightarrow\cup_{i\in K}V_i|f(i)\in V_i, f\text{ has finite support}\}$ finite support means $f(i)=0$ for all but a finite number of $i$ I dare not come up with examples because it's not finite. Lastly Internal direct sums This is written as a + with a circle around it, and it requires the following hold: The join of the family is V, that is: $V=\sum_{i\in K}S_i$ which is the more conventional sum I assume, and independence of the family, that is: $S_i\cap(\sum_{j\ne i}S_j)=\{0\}$ It notes that this second condition is stronger than pairwise disjoint, I cannot think of an example which shows this distinction and I'd like one. That's all the book does on these, unless they are used far later. What is going on here? Also what do ""internal"" and ""external"" mean, I can't think of an example but I think it refers to a style of definition, I've come across and gotten used to similar things before. The book is: Advanced Linear Algebra, Steven Roman, 3rd Edition, GTM # 135",,"['linear-algebra', 'abstract-algebra', 'category-theory']"
95,"Are distance functions $ d(a_1,x), ..., d(a_n,x) $ for an arbitrary metric space linearly independent?",Are distance functions  for an arbitrary metric space linearly independent?," d(a_1,x), ..., d(a_n,x) ","If we have a metric space $ (X,d) $, a finite set of distinct points $ a_1, ..., a_n $, do the distance functions $ d(a_1,x), ..., d(a_n,x) $ have to be linearly independent? That is, if $ c_1d(a_1,x) + ... + c_nd(a_n,x) = 0$ for all $ x $, then $ c_1 = ... = c_n = 0 $? This can easily be seen to be true for $ n = 2 $ and also for $ n = 3 $, but what's the answer in general? If we substitude all the $ a_i $ for x, we get an interesting system of n equations, where we can treat $ c_i $ as the unknowns, and then the matrix of the  coefficients $ d(a_i, a_j) $ is symmetric with 0s on the diagonal(apparently this is called a ""hollow matrix""). All this looks quite interesting and promising, but I don't really know where to take it.","If we have a metric space $ (X,d) $, a finite set of distinct points $ a_1, ..., a_n $, do the distance functions $ d(a_1,x), ..., d(a_n,x) $ have to be linearly independent? That is, if $ c_1d(a_1,x) + ... + c_nd(a_n,x) = 0$ for all $ x $, then $ c_1 = ... = c_n = 0 $? This can easily be seen to be true for $ n = 2 $ and also for $ n = 3 $, but what's the answer in general? If we substitude all the $ a_i $ for x, we get an interesting system of n equations, where we can treat $ c_i $ as the unknowns, and then the matrix of the  coefficients $ d(a_i, a_j) $ is symmetric with 0s on the diagonal(apparently this is called a ""hollow matrix""). All this looks quite interesting and promising, but I don't really know where to take it.",,"['linear-algebra', 'functional-analysis', 'metric-spaces']"
96,Inverse of matrix sum of identity and outer product,Inverse of matrix sum of identity and outer product,,"So before we begin, I already know the answer. I'm just having difficulty figuring out the steps for finding it. Given $u,v \in \mathbb{R}^{n}$, I want to show that $$(I+uv^{T})^{-1}= I - \frac{uv^{T}}{1+v^{T}u}$$ I know from Inverse of the sum of matrices that this is the answer, and since both O(u)=O(v)=n that $Tr(uv^{T}) = v^{T}u$. It's just a matter of getting from point A to point B.","So before we begin, I already know the answer. I'm just having difficulty figuring out the steps for finding it. Given $u,v \in \mathbb{R}^{n}$, I want to show that $$(I+uv^{T})^{-1}= I - \frac{uv^{T}}{1+v^{T}u}$$ I know from Inverse of the sum of matrices that this is the answer, and since both O(u)=O(v)=n that $Tr(uv^{T}) = v^{T}u$. It's just a matter of getting from point A to point B.",,['linear-algebra']
97,How to characterize rotations in $\mathbb{R}^n$?,How to characterize rotations in ?,\mathbb{R}^n,"I am studying the performance of an optimizer algorithm to find the $$ \textrm{argmin}_{x\in \mathbb{R}^n} f(x) \text{ where } f : \mathbb{R}^n \rightarrow \mathbb{R} $$ I would like to test how the performance changes when I rotate the coordinate system . I need to do this in a systematic manner so that each rotation has ""same probability"". In the $n=2$ case this is relatively easy, since any 2-dimensional rotational matrix can be written as: \begin{pmatrix} cos(\phi) & sin(-\phi) \\  sin(\phi) &  cos(\phi) \end{pmatrix} The rotation is characterized by a single number $\phi$ so I can sample the $\phi$ uniformly (equidistantly) on $[-\pi; \pi]$. In the $n=3$ case things get more difficult. There are 3 axes by which I can rotate, but the composition of these rotations is not commutative. So I could get biased results when I sample these 3 rotations uniformly. I was thinking about using the Euler's rotation theorem : in 3D space, any two Cartesian coordinate systems with a common origin   are related by a rotation about some fixed axis This means that the rotations in 3d are characterized by an axis and an angle. So, I could sample all possible axes of some fixed length and all possible rotations around such axis. But this approach is very nontrivial to generalize to arbitrary dimensions . How can I sample n-dimensional rotations uniformly?","I am studying the performance of an optimizer algorithm to find the $$ \textrm{argmin}_{x\in \mathbb{R}^n} f(x) \text{ where } f : \mathbb{R}^n \rightarrow \mathbb{R} $$ I would like to test how the performance changes when I rotate the coordinate system . I need to do this in a systematic manner so that each rotation has ""same probability"". In the $n=2$ case this is relatively easy, since any 2-dimensional rotational matrix can be written as: \begin{pmatrix} cos(\phi) & sin(-\phi) \\  sin(\phi) &  cos(\phi) \end{pmatrix} The rotation is characterized by a single number $\phi$ so I can sample the $\phi$ uniformly (equidistantly) on $[-\pi; \pi]$. In the $n=3$ case things get more difficult. There are 3 axes by which I can rotate, but the composition of these rotations is not commutative. So I could get biased results when I sample these 3 rotations uniformly. I was thinking about using the Euler's rotation theorem : in 3D space, any two Cartesian coordinate systems with a common origin   are related by a rotation about some fixed axis This means that the rotations in 3d are characterized by an axis and an angle. So, I could sample all possible axes of some fixed length and all possible rotations around such axis. But this approach is very nontrivial to generalize to arbitrary dimensions . How can I sample n-dimensional rotations uniformly?",,"['linear-algebra', 'geometry', 'probability-distributions']"
98,Existence of $A^2B - BA^2 = 2A \textrm{ and } AB^2 - B^2A = 2B$. in $\mathcal{M}_n({\mathbb{C}}) $,Existence of . in,A^2B - BA^2 = 2A \textrm{ and } AB^2 - B^2A = 2B \mathcal{M}_n({\mathbb{C}}) ,"This question arose in this classical exercise : Do there exist two matrices such that $AB-BA=I_n$ in $\mathcal{M}_n({\mathbb{C}}) $. Wich is impossible (by using trace to prove this) But if $AB-BA=I_n$ then $A^2B - BA^2 = 2A \textrm{ and } AB^2 - B^2A = 2B$. So my question is : Do there exist  $A$ and $B$ (non-zero) in $\mathcal{M}_n({\mathbb{C}})$ such that :    $$ \textrm{(1) } A^2B - BA^2 = 2A  $$   $$ \textrm{(2) } AB^2 - B^2A = 2B $$ For $n=2$ it's impossible (by putting $A^2$ for example). By induction, we could conclude, if all matrices of rank one switch with matrices with zero trace. Unfortunately, this is not the case. Any ideas to prove it's impossible ? Thank you.","This question arose in this classical exercise : Do there exist two matrices such that $AB-BA=I_n$ in $\mathcal{M}_n({\mathbb{C}}) $. Wich is impossible (by using trace to prove this) But if $AB-BA=I_n$ then $A^2B - BA^2 = 2A \textrm{ and } AB^2 - B^2A = 2B$. So my question is : Do there exist  $A$ and $B$ (non-zero) in $\mathcal{M}_n({\mathbb{C}})$ such that :    $$ \textrm{(1) } A^2B - BA^2 = 2A  $$   $$ \textrm{(2) } AB^2 - B^2A = 2B $$ For $n=2$ it's impossible (by putting $A^2$ for example). By induction, we could conclude, if all matrices of rank one switch with matrices with zero trace. Unfortunately, this is not the case. Any ideas to prove it's impossible ? Thank you.",,['linear-algebra']
99,"Prove if $A \in Mat_{n,n}(\mathbb F)$ is both symmetric and skew-symmetric then $A=0$",Prove if  is both symmetric and skew-symmetric then,"A \in Mat_{n,n}(\mathbb F) A=0","Prove if $A \in Mat_{n,n}(\mathbb F)$ is both symmetric and skew-symmetric then $A=0$ I know $A^T = A = -A \Rightarrow A = -A \Rightarrow A_{i,j} = -A_{i,j}$. Since $\mathbb F$ is a field we have $2A_{i,j} = 0 \Rightarrow 2 = 0 \lor A_{i,j} = 0$. However how can I verify $A_{i,j} = 0$ ? Suppose $\mathbb F = \{[0],[1]\}$. Then $2 = 0$, so I cannot conclude $A_{i,j} = 0$ ?","Prove if $A \in Mat_{n,n}(\mathbb F)$ is both symmetric and skew-symmetric then $A=0$ I know $A^T = A = -A \Rightarrow A = -A \Rightarrow A_{i,j} = -A_{i,j}$. Since $\mathbb F$ is a field we have $2A_{i,j} = 0 \Rightarrow 2 = 0 \lor A_{i,j} = 0$. However how can I verify $A_{i,j} = 0$ ? Suppose $\mathbb F = \{[0],[1]\}$. Then $2 = 0$, so I cannot conclude $A_{i,j} = 0$ ?",,['linear-algebra']
