,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"If a player is 50% as good as I am at a game, how many games will it be before she finally wins one game?","If a player is 50% as good as I am at a game, how many games will it be before she finally wins one game?",,"This is a real life problem. I play Foosball with my colleague who hasn't beaten me so far. I have won 18 in a row. She is about 50% as good as I am (the average margin of victory is 10-5 for me). Mathematically speaking, how many games should it take before she finally wins a game for the first time?","This is a real life problem. I play Foosball with my colleague who hasn't beaten me so far. I have won 18 in a row. She is about 50% as good as I am (the average margin of victory is 10-5 for me). Mathematically speaking, how many games should it take before she finally wins a game for the first time?",,['probability']
1,What is the probability that 5 digit number divisible by 6?,What is the probability that 5 digit number divisible by 6?,,"The main constraint is that each digit can only take digits from $\{1, 2, 3, 4, 5\}$. So the sample space will be $5^{5}$. What is the probability that a random number taken from this sample space will be divisible by $6$? Thanks.","The main constraint is that each digit can only take digits from $\{1, 2, 3, 4, 5\}$. So the sample space will be $5^{5}$. What is the probability that a random number taken from this sample space will be divisible by $6$? Thanks.",,"['probability', 'divisibility']"
2,Product of Combinations is Probability?,Product of Combinations is Probability?,,"For atomic orbitals: E2 orb: $\binom{N-n_1}{n_2}$ E2 orb: $\binom{N-n_1-n_2}{n_3}$ E2 orb: $\binom{N-n_1-n_2-n_3}{n_4}$ ... En orb: $\binom{n_i}{n_i}$ now probability function is: $P= N! \prod^{n}_{n=1}\frac{1}{n_{i}!}$ Why? In general? [Update] Every combination is greater than 1. So their product is greater than 1. How on earth can such multiplication lead to a probability function? Is the probability function scaled back to range $[0,1]$?","For atomic orbitals: E2 orb: $\binom{N-n_1}{n_2}$ E2 orb: $\binom{N-n_1-n_2}{n_3}$ E2 orb: $\binom{N-n_1-n_2-n_3}{n_4}$ ... En orb: $\binom{n_i}{n_i}$ now probability function is: $P= N! \prod^{n}_{n=1}\frac{1}{n_{i}!}$ Why? In general? [Update] Every combination is greater than 1. So their product is greater than 1. How on earth can such multiplication lead to a probability function? Is the probability function scaled back to range $[0,1]$?",,['probability']
3,Number of sequences that satisfy the absolute difference condition,Number of sequences that satisfy the absolute difference condition,,"Let $x_0 =0$ and let $x_1,.....x_{10}$ satisfy that $|x_i - x_{i-1}|$ = 1 for 1≤i≤10 and $x_{10} = 4$ . How many such sequences are there satisfying these conditions? I tried to calculate it by backtracking from $x_0$ and $x_{10}$ , and built an entire tree for the possible values. Based on the edges of the trees, I calculated the answer to be $2*4*6*7*7*7*7*6*4*2$ but the answer is clearly wrong. The question is supposed to be a probability question, but I was thinking of trying to solve it using dynamic programming. It is supposed to be an easy question though, can anyone help me out?","Let and let satisfy that = 1 for 1≤i≤10 and . How many such sequences are there satisfying these conditions? I tried to calculate it by backtracking from and , and built an entire tree for the possible values. Based on the edges of the trees, I calculated the answer to be but the answer is clearly wrong. The question is supposed to be a probability question, but I was thinking of trying to solve it using dynamic programming. It is supposed to be an easy question though, can anyone help me out?","x_0 =0 x_1,.....x_{10} |x_i - x_{i-1}| x_{10} = 4 x_0 x_{10} 2*4*6*7*7*7*7*6*4*2","['probability', 'sequences-and-series', 'combinatorics', 'trees', 'dynamic-programming']"
4,"Probability that $1,2$ are in the same set on randomly halving $\{1,...,10\}$?",Probability that  are in the same set on randomly halving ?,"1,2 \{1,...,10\}","Textbook problem: The integers from $1$ to $10$ inclusive are partitioned into two sets of five elements each. What is the probability that $1$ and $2$ are in the same set? My solution: $2/9$ . The total number of partitions would be $10 \choose 5$ . If $1$ and $2$ are in the same set, then there are $8 \choose 3$ ways left to create the set that contains them since two of the numbers have already been chosen.  Hence, the probability is ${8 \choose 3} / {10 \choose 5}$ which is $2/9$ . Textbook answer: $4/9$ . Question: What am I missing in my reasoning? Should I be doubling the number of ways to create a set with $1$ and $2$ in it for some reason? Textbook: The Art Of Problem Solving (Vol. 1) by Rusczyk, Chapter 26.","Textbook problem: The integers from to inclusive are partitioned into two sets of five elements each. What is the probability that and are in the same set? My solution: . The total number of partitions would be . If and are in the same set, then there are ways left to create the set that contains them since two of the numbers have already been chosen.  Hence, the probability is which is . Textbook answer: . Question: What am I missing in my reasoning? Should I be doubling the number of ways to create a set with and in it for some reason? Textbook: The Art Of Problem Solving (Vol. 1) by Rusczyk, Chapter 26.",1 10 1 2 2/9 10 \choose 5 1 2 8 \choose 3 {8 \choose 3} / {10 \choose 5} 2/9 4/9 1 2,"['probability', 'combinatorics', 'solution-verification']"
5,If a point is selected inside a rectangle what's the probability that the point is closer to center than vertex?,If a point is selected inside a rectangle what's the probability that the point is closer to center than vertex?,,"If a point is selected inside a rectangle what's the probability that the point is closer to center than vertex? I thought of working on this problem using the concept of area. If I draw two concentric rectangles where length and width of inner rectangle are half of the outer one, then the probability should be the ratio of areas of both rectangles. Therefore $P(E) = \dfrac{l\times B}{2l\times 2b} = 1/4$ But the answer given in my book is $1/2$ . What's the problem here?","If a point is selected inside a rectangle what's the probability that the point is closer to center than vertex? I thought of working on this problem using the concept of area. If I draw two concentric rectangles where length and width of inner rectangle are half of the outer one, then the probability should be the ratio of areas of both rectangles. Therefore But the answer given in my book is . What's the problem here?",P(E) = \dfrac{l\times B}{2l\times 2b} = 1/4 1/2,[]
6,Finding the Expected Value and Variance,Finding the Expected Value and Variance,,"The distribution function of a random variable $X$ is given by $P (X \leq x)$ = \begin{cases} 0 & \text{if $x<0$}, \\ x^{k} & \text{if $0 \leq x \leq 1, \,\,\,\,\,\, k\geq1$}, \\ 1     & \text{if $x > 1$}. \end{cases} Determine the mean and variance of $X$ . Workings: I am aware that I must start by calculating $\sum_{-\infty}^{\infty} x \,\cdot p_X(x)$ , but because this is a continuous distribution, I will instead have to calculate $\int_{-\infty}^{\infty} x \, \cdot p_X(x) \,dx$ . However, once I integrate it, I am confused as to what further steps as to which I am meant to take given that I get infinity as the expected value. Could someone please tell me if it is possible for the expected value to be infinite and if I have made a mistake in my workings? Thank you. \begin{align} \text{E(X)} & = \int_{-\infty}^{\infty} x \, \cdot p_X(x) \,dx \\             & = \int_{-\infty}^{0} x \, \cdot 0 \,dx \: + \int_{0}^{1} x \, \cdot x^{k} \,dx \: + \int_{1}^{\infty} x \,dx \\ & = C + \bigg{[}\frac{x^{k+2}}{k+2}\bigg{]}_{0}^{1} + \bigg{[}\frac{x^{2}}{2}\bigg{]}_{1}^{\infty} \end{align}","The distribution function of a random variable is given by = Determine the mean and variance of . Workings: I am aware that I must start by calculating , but because this is a continuous distribution, I will instead have to calculate . However, once I integrate it, I am confused as to what further steps as to which I am meant to take given that I get infinity as the expected value. Could someone please tell me if it is possible for the expected value to be infinite and if I have made a mistake in my workings? Thank you.","X P (X \leq x) \begin{cases}
0 & \text{if x<0}, \\
x^{k} & \text{if 0 \leq x \leq 1, \,\,\,\,\,\, k\geq1}, \\
1     & \text{if x > 1}.
\end{cases} X \sum_{-\infty}^{\infty} x \,\cdot p_X(x) \int_{-\infty}^{\infty} x \, \cdot p_X(x) \,dx \begin{align}
\text{E(X)} & = \int_{-\infty}^{\infty} x \, \cdot p_X(x) \,dx \\
            & = \int_{-\infty}^{0} x \, \cdot 0 \,dx \: + \int_{0}^{1} x \, \cdot x^{k} \,dx \: + \int_{1}^{\infty} x \,dx \\
& = C + \bigg{[}\frac{x^{k+2}}{k+2}\bigg{]}_{0}^{1} + \bigg{[}\frac{x^{2}}{2}\bigg{]}_{1}^{\infty}
\end{align}","['probability', 'probability-distributions', 'expected-value']"
7,Existence of independent events,Existence of independent events,,"Let $(\Omega, \mathcal {F},\mathbb {P})$ be a probability space such that $\Omega$ is countable, and $\mathcal {F}=2^{\Omega}$ . I want to show that it is impossible to exist a countable collection of events $A_{1},A_{2},\cdots\in \mathcal {F}$ which are independent, such that $\mathbb {P}(A_{i})=\frac {1}{2}$ for each $i$ . I think showing $\mathbb {P}(\omega)\leq \frac {1}{2^n}$ for $\omega\in\Omega$ and $n\in \mathbb {N}$ might help?","Let be a probability space such that is countable, and . I want to show that it is impossible to exist a countable collection of events which are independent, such that for each . I think showing for and might help?","(\Omega, \mathcal {F},\mathbb {P}) \Omega \mathcal {F}=2^{\Omega} A_{1},A_{2},\cdots\in \mathcal {F} \mathbb {P}(A_{i})=\frac {1}{2} i \mathbb {P}(\omega)\leq \frac {1}{2^n} \omega\in\Omega n\in \mathbb {N}","['probability', 'probability-theory', 'measure-theory']"
8,"x independent of y, y independent of z, then is x independent of z?","x independent of y, y independent of z, then is x independent of z?",,"using $\perp $ to indicate independence: If $x \perp y$ and $y \perp z$ then is $x \perp z$ ? I started with: $$p(x,z)=p(x,y,z)+p(x,\neg y,z)$$ $$=p(x)p(y|x)p(z|x,y)+p(x)p(\neg y|x)p(z|x,\neg y)$$ $$=p(x)p(y)\frac{p(x,z|y)}{p(x|y)}+p(x)p(\neg y)\frac{p(x,z|\neg y)}{p(x|\neg y)}$$ $$=p(x)p(y)\frac{p(x,z|y)}{p(x)}+p(x)p(\neg y)\frac{p(x,z|\neg y)}{p(x)}$$ $$=p(y)p(x,z|y)+p(\neg y)p(x,z|\neg y)$$ but now i am unsure of how to work with $p(x,z|y)$ .. if $x\perp y$ & $y \perp z$ then can you say $p(x,z|y)=p(x|y)p(z|y)=p(x)p(z)$ ? thank you",using to indicate independence: If and then is ? I started with: but now i am unsure of how to work with .. if & then can you say ? thank you,"\perp  x \perp y y \perp z x \perp z p(x,z)=p(x,y,z)+p(x,\neg y,z) =p(x)p(y|x)p(z|x,y)+p(x)p(\neg y|x)p(z|x,\neg y) =p(x)p(y)\frac{p(x,z|y)}{p(x|y)}+p(x)p(\neg y)\frac{p(x,z|\neg y)}{p(x|\neg y)} =p(x)p(y)\frac{p(x,z|y)}{p(x)}+p(x)p(\neg y)\frac{p(x,z|\neg y)}{p(x)} =p(y)p(x,z|y)+p(\neg y)p(x,z|\neg y) p(x,z|y) x\perp y y \perp z p(x,z|y)=p(x|y)p(z|y)=p(x)p(z)",['probability']
9,does a random variable always have a cumulative distribution function?,does a random variable always have a cumulative distribution function?,,"I haven't taken a measure class so maybe my question is obvious. I was wondering whether every random variable has a cdf, because for   pdf it isn't always the case. Moreover, do you know necessary condition for the existence of the pdf ?","I haven't taken a measure class so maybe my question is obvious. I was wondering whether every random variable has a cdf, because for   pdf it isn't always the case. Moreover, do you know necessary condition for the existence of the pdf ?",,"['probability', 'probability-theory']"
10,Boy-Girl probability question [closed],Boy-Girl probability question [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question You call to someone's house and asked if they have two children. The answer happens to be yes. Then you ask if one of their children's name a William. The answer happens to be yes again.(We assume William is a boy's name, and that it's possible that both children are Williams) What's the probability that the second child is a boy?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question You call to someone's house and asked if they have two children. The answer happens to be yes. Then you ask if one of their children's name a William. The answer happens to be yes again.(We assume William is a boy's name, and that it's possible that both children are Williams) What's the probability that the second child is a boy?",,['probability']
11,Help understanding proof of the following statement $E(Y) = \sum_{i = 1}^{\infty} P(Y \geq k)$,Help understanding proof of the following statement,E(Y) = \sum_{i = 1}^{\infty} P(Y \geq k),"If Y is a discrete random variable that assigns positive probabilities to only the positive integers, show that $E(Y) = \sum_{i = 1}^{\infty} P(Y \geq k)$ Where $E(Y)$ is the expected value (or mean) of Y. This is a text book question and I'm having trouble understanding the solution as they don't give any worded explanations. Here is the solution: 1 $\hspace{1.4cm}\sum_{i = 1}^{\infty} P(Y \geq k)$ 2 $\hspace{1.4cm} = \sum_{k = 1}^{\infty}\sum_{j = k}^{\infty} P(Y=k)$ 3 $\hspace{1.4cm} = \sum_{k = 1}^{\infty}\sum_{j = k}^{\infty} P(j)$ 4 $\hspace{1.4cm} = \sum_{j = 1}^{\infty}j\cdot P(j)$ 5 $\hspace{1.4cm} = \sum_{y = 1}^{\infty}y\cdot P(y) = E(Y)$ I'm not sure how to interpret the inequality $Y \geq k$ inside the probability function, does it read ""values k assigned to the random variable $Y$ such that $k$ is less than or equal to $Y$ ? But how can a value $k$ be less than a random variable $Y$ that doesn't take on any values? I'm also very confused about steps 1 to 2, and 3 to 4. (1 to 2): I don't understand how they broke down the inequality inside the probability function. I think the nested summations are making this complicated for me to understand. Sorry for the broad question and wordy post. Any quick short explanation of anything will be very much appreciated.","If Y is a discrete random variable that assigns positive probabilities to only the positive integers, show that Where is the expected value (or mean) of Y. This is a text book question and I'm having trouble understanding the solution as they don't give any worded explanations. Here is the solution: 1 2 3 4 5 I'm not sure how to interpret the inequality inside the probability function, does it read ""values k assigned to the random variable such that is less than or equal to ? But how can a value be less than a random variable that doesn't take on any values? I'm also very confused about steps 1 to 2, and 3 to 4. (1 to 2): I don't understand how they broke down the inequality inside the probability function. I think the nested summations are making this complicated for me to understand. Sorry for the broad question and wordy post. Any quick short explanation of anything will be very much appreciated.",E(Y) = \sum_{i = 1}^{\infty} P(Y \geq k) E(Y) \hspace{1.4cm}\sum_{i = 1}^{\infty} P(Y \geq k) \hspace{1.4cm} = \sum_{k = 1}^{\infty}\sum_{j = k}^{\infty} P(Y=k) \hspace{1.4cm} = \sum_{k = 1}^{\infty}\sum_{j = k}^{\infty} P(j) \hspace{1.4cm} = \sum_{j = 1}^{\infty}j\cdot P(j) \hspace{1.4cm} = \sum_{y = 1}^{\infty}y\cdot P(y) = E(Y) Y \geq k Y k Y k Y,"['probability', 'statistics', 'inequality', 'summation', 'expected-value']"
12,Suppose $E(2^X)=4$. Prove that $P(X \ge 3) \le {1 \over 2}$.,Suppose . Prove that .,E(2^X)=4 P(X \ge 3) \le {1 \over 2},Suppose $E(2^X)=4$. Prove that $P(X \ge 3) \le {1 \over 2}$. I thought that this question could be solved by using Jensen's inequality and Markov's inequality like below... $E(2^X)=4 \ge 2^{E(X)}$ which implying $2 \ge E(X)$ $P(X \ge 3) \le {E(X) \over 3} \le {2 \over 3}$ How I can get  $P(X \ge 3) \le {1 \over 2}$ from this?? Thank you.,Suppose $E(2^X)=4$. Prove that $P(X \ge 3) \le {1 \over 2}$. I thought that this question could be solved by using Jensen's inequality and Markov's inequality like below... $E(2^X)=4 \ge 2^{E(X)}$ which implying $2 \ge E(X)$ $P(X \ge 3) \le {E(X) \over 3} \le {2 \over 3}$ How I can get  $P(X \ge 3) \le {1 \over 2}$ from this?? Thank you.,,"['probability', 'expectation', 'jensen-inequality']"
13,Expected value of a lognormal distribution [duplicate],Expected value of a lognormal distribution [duplicate],,"This question already has answers here : How to compute moments of log normal distribution? (2 answers) Closed 3 years ago . I'm having trouble deriving an expression for the expected value for the lognormal distribution. I've tried the standard approach of computing $\int_{\mathbb{R^+}}xf_X(x)\,\mathrm{d}x$ for non-negative variables: $$\int_0^{\infty} \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{1}{2}\left(\frac{\ln(y)-\mu}{\sigma}\right)^2\right)\,\mathrm{d}y$$ which is beyond me. I've tried looking into moment generating functions, of which my knowledge is lacking, but stumbled upon a question claiming (and proving) that there is no such function. ( link ) I've looked at a similar question (same, really) ( link ), but I'm afraid I don't undestand the accepted answer. It seems to relate the moment generating function of the normal distribution to the lognormal one, which didn't exist? So how does one extract the expected value for the lognormal distribution, given the moment generating function of another(/the normal) distribution? Bonus question: Is this last method the most natural approach (yes/no), or is it possible to find the expected value using the first approach with some clever trick (yes/no).","This question already has answers here : How to compute moments of log normal distribution? (2 answers) Closed 3 years ago . I'm having trouble deriving an expression for the expected value for the lognormal distribution. I've tried the standard approach of computing $\int_{\mathbb{R^+}}xf_X(x)\,\mathrm{d}x$ for non-negative variables: $$\int_0^{\infty} \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{1}{2}\left(\frac{\ln(y)-\mu}{\sigma}\right)^2\right)\,\mathrm{d}y$$ which is beyond me. I've tried looking into moment generating functions, of which my knowledge is lacking, but stumbled upon a question claiming (and proving) that there is no such function. ( link ) I've looked at a similar question (same, really) ( link ), but I'm afraid I don't undestand the accepted answer. It seems to relate the moment generating function of the normal distribution to the lognormal one, which didn't exist? So how does one extract the expected value for the lognormal distribution, given the moment generating function of another(/the normal) distribution? Bonus question: Is this last method the most natural approach (yes/no), or is it possible to find the expected value using the first approach with some clever trick (yes/no).",,"['probability', 'normal-distribution']"
14,When to use the multiplication rule in probability versus when to use a tree?,When to use the multiplication rule in probability versus when to use a tree?,,"So, as I have understood it, if you have two experiments and you want to know the probability of a set of two outcomes happening concurrently, then you multiply the chance of the first outcome by the chance of the second outcome and voila, you have your probability. However, I am confused as to when this doesn't work. For example, I just did a problem: James lives in San Francisco and works in Mountain View. In the   morning, he has 3 transportation options (bus, cab, or train) to work,   and in the evening he has the same 3 choices for his trip home. What is the probability that he uses the same of mode of transportation twice? My first inclination was 1/9th but apparently I am wrong. I was told to use a tree to count the favorable outcomes. I did so, and see that the answer is 1/3, but for the life of me I can't see the difference between this question and the first type I mentioned. I am obviously missing some finer points or nuance in the question which should clue me in. What is it?","So, as I have understood it, if you have two experiments and you want to know the probability of a set of two outcomes happening concurrently, then you multiply the chance of the first outcome by the chance of the second outcome and voila, you have your probability. However, I am confused as to when this doesn't work. For example, I just did a problem: James lives in San Francisco and works in Mountain View. In the   morning, he has 3 transportation options (bus, cab, or train) to work,   and in the evening he has the same 3 choices for his trip home. What is the probability that he uses the same of mode of transportation twice? My first inclination was 1/9th but apparently I am wrong. I was told to use a tree to count the favorable outcomes. I did so, and see that the answer is 1/3, but for the life of me I can't see the difference between this question and the first type I mentioned. I am obviously missing some finer points or nuance in the question which should clue me in. What is it?",,['probability']
15,Is this true that $E(XY) =E(X) E(Y)$ implies the statistical independence of two normal random variables?,Is this true that  implies the statistical independence of two normal random variables?,E(XY) =E(X) E(Y),"Suppose $X$ and $Y$ are two normal variables and $$E(XY) = E(X) E(Y),$$ then is it true that $X$ and $Y$ are independent? I know that when $(X,Y)$ obeys bivariate normal distribution, the statement is true. But in general, I am not sure. If this statement is wrong, then is there some easy ways to prove the independence of two normal variables?","Suppose $X$ and $Y$ are two normal variables and $$E(XY) = E(X) E(Y),$$ then is it true that $X$ and $Y$ are independent? I know that when $(X,Y)$ obeys bivariate normal distribution, the statement is true. But in general, I am not sure. If this statement is wrong, then is there some easy ways to prove the independence of two normal variables?",,"['probability', 'normal-distribution']"
16,Distribution of Primitive Pythagorean Triples (PPT) and of solutions of $A^4+B^4+C^4=D^4$,Distribution of Primitive Pythagorean Triples (PPT) and of solutions of,A^4+B^4+C^4=D^4,"If we define a $PPTCountingFunction(n)$ as a function that returns the number of PPF with $c < n$ and $a>b$, then up to first $n=100,000$ it is near linear and $\dfrac{n}{PPTCountingFunction(n)}=2\pi$ I have several questions (third question is the most interesting to me): (1) Is this also an asymptotic behavior of this function, or does it have some other slowly changing factors that are not showing up when n is small? (2) Is there a clear reasoning for frequencies of PPT? (3) Can we apply similar reasoning to estimate the frequency of primitive counterexamples to Euler's hypothesis for $n=4$ (solution s of $A^{4}+B^{4}+C^{4}=D^{4}$)? Regarding (3). First solution appears at $95800^{4} + 414560^{4} + 217519^{4} = 422481^{4}$. This is the only solution with $D<2000000$. Another known solution (not necessarily second)  is  $2682440^{4}  +  15365639^{4}  +  18796760^{4}  =  20615673^{4}$. I am curious if there is a point to look for a solution between these two.","If we define a $PPTCountingFunction(n)$ as a function that returns the number of PPF with $c < n$ and $a>b$, then up to first $n=100,000$ it is near linear and $\dfrac{n}{PPTCountingFunction(n)}=2\pi$ I have several questions (third question is the most interesting to me): (1) Is this also an asymptotic behavior of this function, or does it have some other slowly changing factors that are not showing up when n is small? (2) Is there a clear reasoning for frequencies of PPT? (3) Can we apply similar reasoning to estimate the frequency of primitive counterexamples to Euler's hypothesis for $n=4$ (solution s of $A^{4}+B^{4}+C^{4}=D^{4}$)? Regarding (3). First solution appears at $95800^{4} + 414560^{4} + 217519^{4} = 422481^{4}$. This is the only solution with $D<2000000$. Another known solution (not necessarily second)  is  $2682440^{4}  +  15365639^{4}  +  18796760^{4}  =  20615673^{4}$. I am curious if there is a point to look for a solution between these two.",,"['probability', 'diophantine-equations', 'pythagorean-triples']"
17,Expected number of remaining pairs of animals,Expected number of remaining pairs of animals,,"Suppose that Noah started with $n$ pairs of animals on the ark and that $m$ animals died. If the $m$ animals were chosen randomly, what is the expected number of complete pairs left? My attempt: Total animals: $2n$ Probability of dying: $\frac{m}{2n}$ The probability that a pair $X_i$ is left intact is $$\left(1-\frac{m}{2n}\right)^2,$$ so the expected number of remaining pairs is $$n\left(1-\frac{m}{2n}\right)^2$$ Am I correct?","Suppose that Noah started with $n$ pairs of animals on the ark and that $m$ animals died. If the $m$ animals were chosen randomly, what is the expected number of complete pairs left? My attempt: Total animals: $2n$ Probability of dying: $\frac{m}{2n}$ The probability that a pair $X_i$ is left intact is $$\left(1-\frac{m}{2n}\right)^2,$$ so the expected number of remaining pairs is $$n\left(1-\frac{m}{2n}\right)^2$$ Am I correct?",,"['probability', 'expectation']"
18,Which is the correct way to analyse balls drawn from an urn (with replacement),Which is the correct way to analyse balls drawn from an urn (with replacement),,"4 balls are extracted at random - with replacement - from a urn with 3 white and 6 black balls. What is the probability that all extracted balls are 2 white and 2 black? Here is all my steps: If we use this method like in this video from 1:50 obtain: The probability that the ball extracted to be white is: $\frac{1}{3}\Rightarrow\left(\frac{1}{3}\right)^2$= probability that from 2 balls extracted, 2 are white; The probability that the ball extracted to be black is: $\frac{2}{3}\Rightarrow\left(\frac{2}{3}\right)\cdot\left(\frac{1}{3}\right)^2$= probability that from 3 balls extracted, 2 are white and 1 is black; $$\Rightarrow\left(\frac{1}{3}\right)^2\cdot\left(\frac{2}{3}\right)^2 = \text{probability that from 4 balls extracted 2 are white and 2 are black}$$ But if we use Bernoulli trial we obtain: $P={4\choose 2}\cdot\left(\frac{1}{3}\right)^2\cdot\left(\frac{2}{3}\right)^2$ Why if we use Bernoulli trial we have one more thing: ${4\choose 2}$? And which is correct from above?","4 balls are extracted at random - with replacement - from a urn with 3 white and 6 black balls. What is the probability that all extracted balls are 2 white and 2 black? Here is all my steps: If we use this method like in this video from 1:50 obtain: The probability that the ball extracted to be white is: $\frac{1}{3}\Rightarrow\left(\frac{1}{3}\right)^2$= probability that from 2 balls extracted, 2 are white; The probability that the ball extracted to be black is: $\frac{2}{3}\Rightarrow\left(\frac{2}{3}\right)\cdot\left(\frac{1}{3}\right)^2$= probability that from 3 balls extracted, 2 are white and 1 is black; $$\Rightarrow\left(\frac{1}{3}\right)^2\cdot\left(\frac{2}{3}\right)^2 = \text{probability that from 4 balls extracted 2 are white and 2 are black}$$ But if we use Bernoulli trial we obtain: $P={4\choose 2}\cdot\left(\frac{1}{3}\right)^2\cdot\left(\frac{2}{3}\right)^2$ Why if we use Bernoulli trial we have one more thing: ${4\choose 2}$? And which is correct from above?",,['probability']
19,Uniform probability distributions,Uniform probability distributions,,"I have a bit of a misunderstanding going on: If we have a continuous random variable $x$ which is uniformly distributed between $0$ and $2$, then its probability density function is $P(x)=\frac{1}2$. I would expect the mean value of $x$ to be $1$, because $x$ is uniformly distributed between $0$ and $2$ so should take the midpoint of this range - to check, $<x>=\int_0^2 x P(x) dx = 1$ Now I would expect (wrongly) the mean of $x^2$ to be $2$, because all $x$ values between $0$ and $2$ are equally likely, and each of these $x$ values gives an $x^2$ value between $0$ and $4$. All of these $x^2$ values are equally likely, because their corresponding $x$ values were. Then we expect the mean of $x^2$ to be $2$ like the mean of $x$ was $1$ - to check (that my intuition is wrong), $<x^2>=\int_0^2 x^2 P(x) dx = \frac{4}3$ So I am just wondering what is wrong with my logic in the second paragraph, and what would be the correct logic behind understanding the value of $<x^2>$? Thankyou :)","I have a bit of a misunderstanding going on: If we have a continuous random variable $x$ which is uniformly distributed between $0$ and $2$, then its probability density function is $P(x)=\frac{1}2$. I would expect the mean value of $x$ to be $1$, because $x$ is uniformly distributed between $0$ and $2$ so should take the midpoint of this range - to check, $<x>=\int_0^2 x P(x) dx = 1$ Now I would expect (wrongly) the mean of $x^2$ to be $2$, because all $x$ values between $0$ and $2$ are equally likely, and each of these $x$ values gives an $x^2$ value between $0$ and $4$. All of these $x^2$ values are equally likely, because their corresponding $x$ values were. Then we expect the mean of $x^2$ to be $2$ like the mean of $x$ was $1$ - to check (that my intuition is wrong), $<x^2>=\int_0^2 x^2 P(x) dx = \frac{4}3$ So I am just wondering what is wrong with my logic in the second paragraph, and what would be the correct logic behind understanding the value of $<x^2>$? Thankyou :)",,"['probability', 'probability-theory', 'probability-distributions']"
20,Maximizing $ P\{X=Y\}$ where $X$ and $Y$ are Binomial,Maximizing  where  and  are Binomial, P\{X=Y\} X Y,"$X\sim \text{Binomial}(N = 100, p=0.5)$ $Y\sim \text{Binomial}(N = 120, p=0.5)$ What is the  largest possible numerical value of $P\{X=Y\}$. $X$ and $Y$ are not necessarily independent.","$X\sim \text{Binomial}(N = 100, p=0.5)$ $Y\sim \text{Binomial}(N = 120, p=0.5)$ What is the  largest possible numerical value of $P\{X=Y\}$. $X$ and $Y$ are not necessarily independent.",,"['probability', 'probability-distributions']"
21,A fair coin is tossed until a head comes up for the first time. The probability of this happening on an odd number toss is?,A fair coin is tossed until a head comes up for the first time. The probability of this happening on an odd number toss is?,,A fair coin is tossed until a head comes up for the first time. The probability of this happening on an odd number toss is?  How do i approach this problem ?,A fair coin is tossed until a head comes up for the first time. The probability of this happening on an odd number toss is?  How do i approach this problem ?,,['probability']
22,Find the probability of at least 1 three appearing when you roll 4 dice. Why is the probability low?,Find the probability of at least 1 three appearing when you roll 4 dice. Why is the probability low?,,"If you roll 4 dice, what is the probability of at least 1 three appearing? I'm not sure if I did the calculation correctly but here it is: $\text{P(At least 1 three)} = P(1\mbox{ three}) + P(2\mbox{ threes}) +P(3\mbox{ threes}) +P(4\mbox{ threes})$ $(^4C_1)+(^4C_2)+(^4C_3)+(^4C_4)/6^{4} = 5/432$ I am not sure if I have calculated the probability correctly (probably not) but the probability of $5/432$ seems very low. Logically speaking, if you have more dice, you have more chances of a three appearing on at least one of them right? So why is the probability lower than the probability of getting a 3 on one dice (1/6)?","If you roll 4 dice, what is the probability of at least 1 three appearing? I'm not sure if I did the calculation correctly but here it is: $\text{P(At least 1 three)} = P(1\mbox{ three}) + P(2\mbox{ threes}) +P(3\mbox{ threes}) +P(4\mbox{ threes})$ $(^4C_1)+(^4C_2)+(^4C_3)+(^4C_4)/6^{4} = 5/432$ I am not sure if I have calculated the probability correctly (probably not) but the probability of $5/432$ seems very low. Logically speaking, if you have more dice, you have more chances of a three appearing on at least one of them right? So why is the probability lower than the probability of getting a 3 on one dice (1/6)?",,['probability']
23,Probability My Opponent Has a Specific Scrabble Letter Mid-Game,Probability My Opponent Has a Specific Scrabble Letter Mid-Game,,"How does one calculate the probability that an opponent would have a specific letter mid-game? For example: my opponent would have to have a T in his rack in order to hit the Triple Word Score off the word I am considering playing. There are 50 tiles still in the bank (so, that doesn't include the 7 tiles currently on each of our racks.) There are 7 Ts total in the game (I'm actually playing Words With Friends), 2 Ts have been used on the board and I have 1 T in my rack, leaving 4 Ts unaccounted for. So how would I approach calculating the odds that my opponent currently has 1 or more of the remaining 4 Ts on his rack? Thank you!!!","How does one calculate the probability that an opponent would have a specific letter mid-game? For example: my opponent would have to have a T in his rack in order to hit the Triple Word Score off the word I am considering playing. There are 50 tiles still in the bank (so, that doesn't include the 7 tiles currently on each of our racks.) There are 7 Ts total in the game (I'm actually playing Words With Friends), 2 Ts have been used on the board and I have 1 T in my rack, leaving 4 Ts unaccounted for. So how would I approach calculating the odds that my opponent currently has 1 or more of the remaining 4 Ts on his rack? Thank you!!!",,['probability']
24,Why do we multiply in tree diagrams?,Why do we multiply in tree diagrams?,,"In probability, we always lay out the events through tree to see what depends on what. Then we were taught to ""multiply"" through that branch to get the probability of that event. Why do we ""multiply""? I've noticed we have the same sort of rule in calculus too (chain rule) EDIT : I think my question was misinterpreted. I am asking just because we put a number (usually - if not almost fractional) beside a branch, why must we ""multiply"" through that number or that ""thing"" if I am being more abstract. Where ""thing"" could even mean a derivative operator $$\partial / \partial x$$","In probability, we always lay out the events through tree to see what depends on what. Then we were taught to ""multiply"" through that branch to get the probability of that event. Why do we ""multiply""? I've noticed we have the same sort of rule in calculus too (chain rule) EDIT : I think my question was misinterpreted. I am asking just because we put a number (usually - if not almost fractional) beside a branch, why must we ""multiply"" through that number or that ""thing"" if I am being more abstract. Where ""thing"" could even mean a derivative operator $$\partial / \partial x$$",,['probability']
25,"Intuitively, why does $\lim_{n \to \infty} \frac16 (p_{n - 1} + p_{n - 2} ... + p_{n - 6}) = 2/7$?","Intuitively, why does ?",\lim_{n \to \infty} \frac16 (p_{n - 1} + p_{n - 2} ... + p_{n - 6}) = 2/7,"Blitzstein, Introduction to Probability (2019 2 edn), Chapter 2, Exercise 48, p 94. A fair die is rolled repeatedly, and a running total is kept (which is, at each time, the total of all the rolls up until that time). Let $p_n$ be the probability that the running total is ever exactly n (assume the die will always be rolled enough times so that the running total will eventually exceed n, but it may or may not ever equal n). (c) Give an intuitive explanation for the fact that $p_n \rightarrow  1/3.5 = 2/7 \quad as \quad n \rightarrow \infty$ . From p 17 in the publicly downloadable PDF of curbed solutions. (c)An intuitive explanation is as follows. The average number thrown by the die is (total of dots)/6, which is 21/6 = 7/2, so that every throw adds on an average of 7/2. We can therefore expect to land on 2 out of every 7 numbers, and the probability of landing on any particular number is 2/7. That's the line I don't get it, why we can transfer","Blitzstein, Introduction to Probability (2019 2 edn), Chapter 2, Exercise 48, p 94. A fair die is rolled repeatedly, and a running total is kept (which is, at each time, the total of all the rolls up until that time). Let be the probability that the running total is ever exactly n (assume the die will always be rolled enough times so that the running total will eventually exceed n, but it may or may not ever equal n). (c) Give an intuitive explanation for the fact that . From p 17 in the publicly downloadable PDF of curbed solutions. (c)An intuitive explanation is as follows. The average number thrown by the die is (total of dots)/6, which is 21/6 = 7/2, so that every throw adds on an average of 7/2. We can therefore expect to land on 2 out of every 7 numbers, and the probability of landing on any particular number is 2/7. That's the line I don't get it, why we can transfer",p_n p_n \rightarrow  1/3.5 = 2/7 \quad as \quad n \rightarrow \infty,['probability']
26,Grad degree that mainly deals with probability/game theory/optimization?,Grad degree that mainly deals with probability/game theory/optimization?,,"I'm currently working but am going to take classes as a non-degree student to beef up the math part of my background. I've only taken calc 1-3, ODEs, linear algebra, logic, and decision theory so my interests aren't really defined.  Having said that, what kind of grad degree should I look into if these are my interests?  I don't want to do an econ ma/phd, but I'd like to explore game theory, etc. Any thoughts would be great. Also, if I do go back, take classes for a year (2/semester for a total of 4), I think I'll have a decent foundation but I'd have to apply the following year for admission into one of these programs (whatever one makes sense).  Is there any way around being stuck just waiting on admissions for a year?  I won't be competitive without those 4 classes... Thanks again,","I'm currently working but am going to take classes as a non-degree student to beef up the math part of my background. I've only taken calc 1-3, ODEs, linear algebra, logic, and decision theory so my interests aren't really defined.  Having said that, what kind of grad degree should I look into if these are my interests?  I don't want to do an econ ma/phd, but I'd like to explore game theory, etc. Any thoughts would be great. Also, if I do go back, take classes for a year (2/semester for a total of 4), I think I'll have a decent foundation but I'd have to apply the following year for admission into one of these programs (whatever one makes sense).  Is there any way around being stuck just waiting on admissions for a year?  I won't be competitive without those 4 classes... Thanks again,",,"['probability', 'soft-question', 'optimization', 'education']"
27,Chebyshev's Inequality Improvement,Chebyshev's Inequality Improvement,,"I'm having trouble improving the upper bound on Markov/Chebyshev's inequality in this particular example: Show that $$\lim_{n\to\infty} n\mathbb{P}(|X_1|\geq \epsilon\sqrt{n})=0$$ Clearly, Markov's inequality yields the upper bound $n\mathbb{E}[|X_1|^2]/\epsilon^2$ which is not good enough. I'd really appreciate any hints or direction.","I'm having trouble improving the upper bound on Markov/Chebyshev's inequality in this particular example: Show that Clearly, Markov's inequality yields the upper bound which is not good enough. I'd really appreciate any hints or direction.",\lim_{n\to\infty} n\mathbb{P}(|X_1|\geq \epsilon\sqrt{n})=0 n\mathbb{E}[|X_1|^2]/\epsilon^2,"['probability', 'probability-theory', 'measure-theory', 'random-variables', 'concentration-of-measure']"
28,Simulate a 10 sided die with a 6 sided die [duplicate],Simulate a 10 sided die with a 6 sided die [duplicate],,"This question already has answers here : How to generate a random number between 1 and 10 with a six-sided die? (21 answers) Closed 10 months ago . How do you simulate a 10 sided die using a 6 sided die. I came across this question in my Probability textbook My 2 approaches: E[x] of 10 sided die = 5.5, E[x] of 6 sided die = 3.5 1st Method: Roll the 6 sided twice. Now on the 3rd roll keep rolling until you get a (1,2,3,4,5) and reject/reroll a 6. Now divide the result of the 3rd roll by 2 and subtract from the sum of the 1st 2 rolls. E[x] = 3.5 + 3.5 - (3/2) = 5.5 2nd Method: Roll the 6 sided die. Now for the 2nd roll keep rolling until you get a (1,2, or 3) i.e. reject/reroll on (4,5, or 6). E[x] = 3.5 + 2 = 5.5 I realize that the 1st method is probably better in terms of fewer rolls required on average I am wondering if there is some other more optimal solution that reduces the average number of rolls required. Thanks!","This question already has answers here : How to generate a random number between 1 and 10 with a six-sided die? (21 answers) Closed 10 months ago . How do you simulate a 10 sided die using a 6 sided die. I came across this question in my Probability textbook My 2 approaches: E[x] of 10 sided die = 5.5, E[x] of 6 sided die = 3.5 1st Method: Roll the 6 sided twice. Now on the 3rd roll keep rolling until you get a (1,2,3,4,5) and reject/reroll a 6. Now divide the result of the 3rd roll by 2 and subtract from the sum of the 1st 2 rolls. E[x] = 3.5 + 3.5 - (3/2) = 5.5 2nd Method: Roll the 6 sided die. Now for the 2nd roll keep rolling until you get a (1,2, or 3) i.e. reject/reroll on (4,5, or 6). E[x] = 3.5 + 2 = 5.5 I realize that the 1st method is probably better in terms of fewer rolls required on average I am wondering if there is some other more optimal solution that reduces the average number of rolls required. Thanks!",,['probability']
29,Probablity of a card being less than or equal to 3,Probablity of a card being less than or equal to 3,,"In a box, there are 10 cards and a number from 1 to 10 is written on each card. When three cards from the box are randomly taken at a time, we define X,Y, and Z according to three numbers in ascending order. The probablity that X is less than or equal to 3 is: I tried writing out what the probablity of three situations would be where A is anything. $$1AA = 1/10 * 1 * 1$$ $$2AA (excluding 1) = 1/10 * 8/9 * 7/8$$ $$3AA (excluding 2 and 1)=  1/10 * 7/9 * 6/8$$ After adding all of these up I came no where near the answer: $17/24$ or( $85/120$ also works) Where am I going wrong with this? Also, how do I solve it? I thought about permutations, and how many different ways we could draw these cards, but it seems like the cards have to be in a strict order (ascending) so even if we draw the cards out of order, they will be put in order, so everything is just multiplied by 1, since there are no permuations (or so I think) I also thought about what if this is just asking, of a random set of three cards, what is the chance that x is less than 3? and thought XYZ, X has a 3/10 chance to be 3 or less. However, after that I got lost on how I should multiply 3/10, since the next two numbers in that sequence are fully dependent on the first number. All help is appreciated! thank you!","In a box, there are 10 cards and a number from 1 to 10 is written on each card. When three cards from the box are randomly taken at a time, we define X,Y, and Z according to three numbers in ascending order. The probablity that X is less than or equal to 3 is: I tried writing out what the probablity of three situations would be where A is anything. After adding all of these up I came no where near the answer: or( also works) Where am I going wrong with this? Also, how do I solve it? I thought about permutations, and how many different ways we could draw these cards, but it seems like the cards have to be in a strict order (ascending) so even if we draw the cards out of order, they will be put in order, so everything is just multiplied by 1, since there are no permuations (or so I think) I also thought about what if this is just asking, of a random set of three cards, what is the chance that x is less than 3? and thought XYZ, X has a 3/10 chance to be 3 or less. However, after that I got lost on how I should multiply 3/10, since the next two numbers in that sequence are fully dependent on the first number. All help is appreciated! thank you!",1AA = 1/10 * 1 * 1 2AA (excluding 1) = 1/10 * 8/9 * 7/8 3AA (excluding 2 and 1)=  1/10 * 7/9 * 6/8 17/24 85/120,"['probability', 'statistics']"
30,Five friends meeting at the same coffeehouse,Five friends meeting at the same coffeehouse,,"Five friends agree to meet at Joe's Coffeehouse in Capital City. However, there are $5$ different Joe's Coffeehouse locations in Capital City, and the friends neglected to agree on which one to meet at, so they each choose one at random. What is the probability that (a) they all end up at the same Joe's? (b) they all end up at different Joe's? For each of those parts I solved it in two different ways and got different answers, I'm not sure which one is correct in both cases. Any explanation as to which is correct, which is wrong, and why would be very well appreciated. (a) APPROACH 1: Let's fix a friend, there's a ${1\over5}$ chance of him going to each particular Joe's, and then for each of those $5$ options for the remaining $4$ friends, there's a ${1\over5}$ chance of going to that same Joe's, so the probability they all end up at the same Joe's is: $$\left({1\over5}\right)\left({1\over5}\right)^4 + \left({1\over5}\right)\left({1\over5}\right)^4 + \left({1\over5}\right)\left({1\over5}\right)^4 + \left({1\over5}\right)\left({1\over5}\right)^4 + \left({1\over5}\right)\left({1\over5}\right)^4 = \left({1\over5}\right)^4 = {1\over{625}}$$ APPROACH 2: The probability is going to be $${{\# \text{ of nonnegative integer solutions to }x_1 + x_2 + x_3 + x_4 + x_5 = 5 \text{ such that }x_i = 5 \text{ for some }i \text{ between }1\text{ and }5 \text{ inclusive}}\over{\# \text{ of nonnegative integer solutions to }x_1 + x_2 + x_3 + x_4 + x_5 = 5}} = {5\over{\binom{9}{5}}} = {5\over{126}}$$ (b) APPROACH 1: Let's fix a friend, there's a ${1\over5}$ chance of him going to each particular Joe's, and so for that particular Joe's, there's a ${4\over5}$ chance of the next guy going to a different Joe's, a ${3\over5}$ for the next guy etc. so the probability they all end up at different Joe's is: $$5 \left({1\over5}\right)\left({4\over5}\right)\left({3\over5}\right)\left({2\over5}\right) \left({1\over5}\right) = {{24}\over{625}}$$ APPROACH 2: The probability is going to be $${{\# \text{ of nonnegative integer solutions to }x_1 + x_2 + x_3 + x_4 + x_5 = 5 \text{ such that }x_i = 1 \text{ for all }i \text{ between }1\text{ and }5 \text{ inclusive}}\over{\# \text{ of nonnegative integer solutions to }x_1 + x_2 + x_3 + x_4 + x_5 = 5}} = {1\over{\binom{9}{5}}} = {1\over{126}}$$","Five friends agree to meet at Joe's Coffeehouse in Capital City. However, there are different Joe's Coffeehouse locations in Capital City, and the friends neglected to agree on which one to meet at, so they each choose one at random. What is the probability that (a) they all end up at the same Joe's? (b) they all end up at different Joe's? For each of those parts I solved it in two different ways and got different answers, I'm not sure which one is correct in both cases. Any explanation as to which is correct, which is wrong, and why would be very well appreciated. (a) APPROACH 1: Let's fix a friend, there's a chance of him going to each particular Joe's, and then for each of those options for the remaining friends, there's a chance of going to that same Joe's, so the probability they all end up at the same Joe's is: APPROACH 2: The probability is going to be (b) APPROACH 1: Let's fix a friend, there's a chance of him going to each particular Joe's, and so for that particular Joe's, there's a chance of the next guy going to a different Joe's, a for the next guy etc. so the probability they all end up at different Joe's is: APPROACH 2: The probability is going to be",5 {1\over5} 5 4 {1\over5} \left({1\over5}\right)\left({1\over5}\right)^4 + \left({1\over5}\right)\left({1\over5}\right)^4 + \left({1\over5}\right)\left({1\over5}\right)^4 + \left({1\over5}\right)\left({1\over5}\right)^4 + \left({1\over5}\right)\left({1\over5}\right)^4 = \left({1\over5}\right)^4 = {1\over{625}} {{\# \text{ of nonnegative integer solutions to }x_1 + x_2 + x_3 + x_4 + x_5 = 5 \text{ such that }x_i = 5 \text{ for some }i \text{ between }1\text{ and }5 \text{ inclusive}}\over{\# \text{ of nonnegative integer solutions to }x_1 + x_2 + x_3 + x_4 + x_5 = 5}} = {5\over{\binom{9}{5}}} = {5\over{126}} {1\over5} {4\over5} {3\over5} 5 \left({1\over5}\right)\left({4\over5}\right)\left({3\over5}\right)\left({2\over5}\right) \left({1\over5}\right) = {{24}\over{625}} {{\# \text{ of nonnegative integer solutions to }x_1 + x_2 + x_3 + x_4 + x_5 = 5 \text{ such that }x_i = 1 \text{ for all }i \text{ between }1\text{ and }5 \text{ inclusive}}\over{\# \text{ of nonnegative integer solutions to }x_1 + x_2 + x_3 + x_4 + x_5 = 5}} = {1\over{\binom{9}{5}}} = {1\over{126}},"['probability', 'combinatorics']"
31,Probability of first ball is red or second ball is red,Probability of first ball is red or second ball is red,,A bag contains $2$ red and $6$ black balls. Two balls are drawn in succession without replacement from a bag. Then find the probability that the first  ball is red or the second ball is red. What I have tried: Let $A: $ first ball is red and $B: $ second ball is red. So $P(A \cup B)=P(A)+P(B)-P(A \cap B)$ Here $\displaystyle P(A)=\frac{2}{6}\times 1=\frac{1}{3}$ And $\displaystyle P(A\cap B)=\frac{2}{6}\times \frac{1}{5}=\frac{1}{15}$ I did not understand. How do I solve $P(B)$ ? Help me please,A bag contains red and black balls. Two balls are drawn in succession without replacement from a bag. Then find the probability that the first  ball is red or the second ball is red. What I have tried: Let first ball is red and second ball is red. So Here And I did not understand. How do I solve ? Help me please,2 6 A:  B:  P(A \cup B)=P(A)+P(B)-P(A \cap B) \displaystyle P(A)=\frac{2}{6}\times 1=\frac{1}{3} \displaystyle P(A\cap B)=\frac{2}{6}\times \frac{1}{5}=\frac{1}{15} P(B),"['probability', 'combinations']"
32,I roll a fair die 4 times. Let $X$ be the number of different outcomes that I see. Find $\mathbb{E}[X]$,I roll a fair die 4 times. Let  be the number of different outcomes that I see. Find,X \mathbb{E}[X],"I roll a fair die 4 times. Let X be the number of different outcomes that I see. Find $\mathbb{E}[X]$ . My attempt: I know that I can write X as a sum of indicator random variables and then I can use the fact that $\mathbb{E}[1_A]=\mathbb{P}(A)$ . Thus, \begin{equation} I_{A_1}= \begin{cases} 1 & \text{if only one kind}\\ 0 & \text{otherwise} \end{cases} \end{equation} \begin{equation} I_{A_2}= \begin{cases} 1 & \text{if two kinds}\\ 0 & \text{otherwise} \end{cases} \end{equation} \begin{equation} I_{A_3}= \begin{cases} 1 & \text{if three kinds}\\ 0 & \text{otherwise} \end{cases} \end{equation} \begin{equation} I_{A_4}= \begin{cases} 1 & \text{all different}\\ 0 & \text{otherwise} \end{cases} \end{equation} Then the probabilities of event A happening for each is $$\mathbb{P}(I_{A_1})=\frac{6\cdot1\cdot1\cdot1}{6^4}$$ $$\mathbb{P}(I_{A_2})=\frac{6\cdot5\cdot2\cdot2}{6^4}$$ $$\mathbb{P}(I_{A_3})=\frac{6\cdot5\cdot4\cdot3}{6^4}$$ $$\mathbb{P}(I_{A_4})=\frac{6\cdot5\cdot4\cdot3}{6^4}$$ The sum of these should be my desired expectation. My question is whether or not I have found the probabilities correctly. This is my thought process for how I counted the number of choices for the numerator, using $I_{A_2}$ as an example: There are 6 choices for choosing the first number, then we don't want to get that number again so then there are 5 choices. After that, we only want to get the first or second number again, thus there are 2 choices for the third roll and 2 choices for the fourth roll. Hence, $6\times 5\times 2\times 2=120$ . Is this the correct way of thinking about it? Thanks","I roll a fair die 4 times. Let X be the number of different outcomes that I see. Find . My attempt: I know that I can write X as a sum of indicator random variables and then I can use the fact that . Thus, Then the probabilities of event A happening for each is The sum of these should be my desired expectation. My question is whether or not I have found the probabilities correctly. This is my thought process for how I counted the number of choices for the numerator, using as an example: There are 6 choices for choosing the first number, then we don't want to get that number again so then there are 5 choices. After that, we only want to get the first or second number again, thus there are 2 choices for the third roll and 2 choices for the fourth roll. Hence, . Is this the correct way of thinking about it? Thanks","\mathbb{E}[X] \mathbb{E}[1_A]=\mathbb{P}(A) \begin{equation}
I_{A_1}=
\begin{cases}
1 & \text{if only one kind}\\
0 & \text{otherwise}
\end{cases}
\end{equation} \begin{equation}
I_{A_2}=
\begin{cases}
1 & \text{if two kinds}\\
0 & \text{otherwise}
\end{cases}
\end{equation} \begin{equation}
I_{A_3}=
\begin{cases}
1 & \text{if three kinds}\\
0 & \text{otherwise}
\end{cases}
\end{equation} \begin{equation}
I_{A_4}=
\begin{cases}
1 & \text{all different}\\
0 & \text{otherwise}
\end{cases}
\end{equation} \mathbb{P}(I_{A_1})=\frac{6\cdot1\cdot1\cdot1}{6^4} \mathbb{P}(I_{A_2})=\frac{6\cdot5\cdot2\cdot2}{6^4} \mathbb{P}(I_{A_3})=\frac{6\cdot5\cdot4\cdot3}{6^4} \mathbb{P}(I_{A_4})=\frac{6\cdot5\cdot4\cdot3}{6^4} I_{A_2} 6\times 5\times 2\times 2=120","['probability', 'random-variables', 'expected-value']"
33,Conditional expectation of i.i.d random variables,Conditional expectation of i.i.d random variables,,"consider the sum of random variables $Q_k=\sum_k R_k $ , $R_k$ are i.i.d. Now I want to calculate: $$E(Q_j| Y_{k+j}=n) =j \frac{k}{k+j}$$","consider the sum of random variables , are i.i.d. Now I want to calculate:",Q_k=\sum_k R_k  R_k E(Q_j| Y_{k+j}=n) =j \frac{k}{k+j},"['probability', 'probability-theory', 'expected-value']"
34,How to evaluate $\mathbb{E}(|X_1-X_2|)?$,How to evaluate,\mathbb{E}(|X_1-X_2|)?,"I am trying to solve the question in this post using an alternative. For completeness, I will retype the question in my post. What is the average result of rolling two dice, and only taking the value of the higher dice roll? For example: I roll two dice and one comes up as a four and the other a six, the result would just be six. My attempt: Let $X_1,X_2$ be the score by dice $1$ and $2$ respectively. Since we have $$\max(X_1,X_2)=\frac{|X_1+X_2| + |X_1-X_2|}{2},$$ it follows that \begin{align*} \mathbb{E}[\max(X_1,X_2)] & = \frac{1}{2}\left[\mathbb{E}(|X_1+X_2|) + \mathbb{E}(|X_1-X_2|)\right] \\ & = \frac{1}{2}\left[ \mathbb{E} (X_1) + \mathbb{E}(X_2) + \mathbb{E}(|X_1-X_2|) \right] \\ & = \frac{1}{2}\left[ 7 + \mathbb{E}(|X_1-X_2|) \right]. \end{align*} where I apply the fact that $X_1,X_2>0$ at second equality. I got stuck at evaluating $$\mathbb{E}(|X_1-X_2|).$$ Any hint is appreciated. Just for record purpose, the answer is $$\mathbb{E}[\max(X_1,X_2)] = \frac{161}{36}.$$ This is an interview question. So, I expect that there is an easy way to calculate the expectation.","I am trying to solve the question in this post using an alternative. For completeness, I will retype the question in my post. What is the average result of rolling two dice, and only taking the value of the higher dice roll? For example: I roll two dice and one comes up as a four and the other a six, the result would just be six. My attempt: Let be the score by dice and respectively. Since we have it follows that where I apply the fact that at second equality. I got stuck at evaluating Any hint is appreciated. Just for record purpose, the answer is This is an interview question. So, I expect that there is an easy way to calculate the expectation.","X_1,X_2 1 2 \max(X_1,X_2)=\frac{|X_1+X_2| + |X_1-X_2|}{2}, \begin{align*}
\mathbb{E}[\max(X_1,X_2)] & = \frac{1}{2}\left[\mathbb{E}(|X_1+X_2|) + \mathbb{E}(|X_1-X_2|)\right] \\
& = \frac{1}{2}\left[ \mathbb{E} (X_1) + \mathbb{E}(X_2) + \mathbb{E}(|X_1-X_2|) \right] \\
& = \frac{1}{2}\left[ 7 + \mathbb{E}(|X_1-X_2|) \right].
\end{align*} X_1,X_2>0 \mathbb{E}(|X_1-X_2|). \mathbb{E}[\max(X_1,X_2)] = \frac{161}{36}.","['probability', 'random-variables', 'expected-value']"
35,Coupon collector's problem: mean and variance in number of coupons to be collected to complete a set (unequal probabilities),Coupon collector's problem: mean and variance in number of coupons to be collected to complete a set (unequal probabilities),,"There are $n$ coupons in a collection. A collector has the ability to purchase a coupon, but can't choose the coupon they purchase. Instead, the coupon is revealed to be coupon $i$ with probability $p_i=\frac 1 n$ . Let $N$ be the number of coupons they'll need to collect before they have at least one coupon of each type. Find the expected value and variance of $N$ . Bonus: generalize to the case where the probability of collecting the $j$ th coupon is $p_j$ with $\sum\limits_{j=1}^n p_j=1$ . I recently came across this problem and came up with/ unearthed various methods to solve it. I'm intending this page as a wiki with various solutions. I'll be posting all the solutions I'm aware of (4 so far) over time. EDIT: As mentioned in the comments, this question is different from the one people are saying it is a duplicate of since (for one thing) it includes an expression for the variance and it covers the general case where all coupons have unequal probabilities. The case of calculating the variance for the general case of coupons having unequal probabilities has not been covered anywhere on the site apart from an earlier post by me , which this one intends to consolidate along with other approaches to solve this problem. EDIT: Paper on the solutions on this page submitted to ArXiv: http://arxiv.org/abs/2003.04720","There are coupons in a collection. A collector has the ability to purchase a coupon, but can't choose the coupon they purchase. Instead, the coupon is revealed to be coupon with probability . Let be the number of coupons they'll need to collect before they have at least one coupon of each type. Find the expected value and variance of . Bonus: generalize to the case where the probability of collecting the th coupon is with . I recently came across this problem and came up with/ unearthed various methods to solve it. I'm intending this page as a wiki with various solutions. I'll be posting all the solutions I'm aware of (4 so far) over time. EDIT: As mentioned in the comments, this question is different from the one people are saying it is a duplicate of since (for one thing) it includes an expression for the variance and it covers the general case where all coupons have unequal probabilities. The case of calculating the variance for the general case of coupons having unequal probabilities has not been covered anywhere on the site apart from an earlier post by me , which this one intends to consolidate along with other approaches to solve this problem. EDIT: Paper on the solutions on this page submitted to ArXiv: http://arxiv.org/abs/2003.04720",n i p_i=\frac 1 n N N j p_j \sum\limits_{j=1}^n p_j=1,"['probability', 'coupon-collector']"
36,Integral of square of the Brownian Motion with respect to time,Integral of square of the Brownian Motion with respect to time,,"Is there anything known about the distribution of: $$ \int_0^1 B_t^2 \; dt \; ?$$ I know that without squaring, the distribution of the above variable is normal with mean $0$ and variance $\frac{1}{3}$ ( https://quant.stackexchange.com/questions/29504/integral-of-brownian-motion-w-r-t-time ). But if underlying process is nonnegative, the distribution obviously cannot be normal and from simulation I can only conclude that it has mean $\frac{1}{2}$ (which is actually easy to prove) and variance $\frac{1}{3}$ . But the exact distribution is unknown to me. Is there any source of information about such integral?","Is there anything known about the distribution of: I know that without squaring, the distribution of the above variable is normal with mean and variance ( https://quant.stackexchange.com/questions/29504/integral-of-brownian-motion-w-r-t-time ). But if underlying process is nonnegative, the distribution obviously cannot be normal and from simulation I can only conclude that it has mean (which is actually easy to prove) and variance . But the exact distribution is unknown to me. Is there any source of information about such integral?", \int_0^1 B_t^2 \; dt \; ? 0 \frac{1}{3} \frac{1}{2} \frac{1}{3},"['probability', 'stochastic-processes', 'stochastic-calculus', 'brownian-motion', 'stochastic-integrals']"
37,Probability to get twice as many heads as tails at some point in an infinite sequence of coin tosses,Probability to get twice as many heads as tails at some point in an infinite sequence of coin tosses,,"Let consider an infinite sequence of tosses of a fair coin ( $p = 1/2$ to get head or tail). What is the probability to get, at least once, twice as many head as tails? In another words, what is the probability that, there is $n, k \in \Bbb N^*$ such as $H_{3n} = 2n, T_{3n} = n$ (where $H_n$ denote the number of heads, and $T_n$ the number for tails at the $n$ )? What I tried: I don't even know how to start... I tried a lot of things but nothing really worked...","Let consider an infinite sequence of tosses of a fair coin ( to get head or tail). What is the probability to get, at least once, twice as many head as tails? In another words, what is the probability that, there is such as (where denote the number of heads, and the number for tails at the )? What I tried: I don't even know how to start... I tried a lot of things but nothing really worked...","p = 1/2 n, k \in \Bbb N^* H_{3n} = 2n, T_{3n} = n H_n T_n n","['probability', 'conditional-probability']"
38,Probability Notation: What does $\{\omega\in \Omega : X(\omega) \in A\}$ mean?,Probability Notation: What does  mean?,\{\omega\in \Omega : X(\omega) \in A\},"In the book Stats with Julia on p. 79 is reads ... ""The probability distribution of a random variable fully describes the   probabilities of the events such as $\{\omega\in \Omega : X(\omega) \in A\}$ for all sensible $A \subset R$ "" How would you say "" $\{\omega\in \Omega : X(\omega) \in A\}$ "" in plain English? Is it .... for every possible outcome $(\omega)$ in the $(\in)$ event space $(\Omega)$ such that $(:)$ there is some specific outcome $(X(\omega))$ in the set $A$ where set $A$ contains real numbers .. is that close??","In the book Stats with Julia on p. 79 is reads ... ""The probability distribution of a random variable fully describes the   probabilities of the events such as for all sensible "" How would you say "" "" in plain English? Is it .... for every possible outcome in the event space such that there is some specific outcome in the set where set contains real numbers .. is that close??",\{\omega\in \Omega : X(\omega) \in A\} A \subset R \{\omega\in \Omega : X(\omega) \in A\} (\omega) (\in) (\Omega) (:) (X(\omega)) A A,"['probability', 'notation', 'random-variables']"
39,What is the formalism that allows Random Variables to be treated algebraically like real or complex numbers?,What is the formalism that allows Random Variables to be treated algebraically like real or complex numbers?,,"We all know that if we have a variable x, then there is a meaning to - for example - $$y=e^x$$ . And we all know how to manipulate that algebraically and to do calculus.  For example, if $$y_1=e^{x_1}$$ and $$y_2=e^{x_2}$$ then $$y_1y_2 = e^{x_1+x_2}$$ But random variables are a very different beast. Ultimately we all know intuitively what we mean when we draw a PDF or, more accurately, the CDF which is itself an algebraic function mapping $[-\infty, \infty]$ to $[0,1]$ as part of that defines a random variable. But what is the actual math that makes that definition of an RV actually act algebraically, so that, as above, we can happily write things like $$Y_1=e^{X_1}$$ and $$Y_2=e^{X_2}$$ then $$Y_1Y_2 = e^{X_1+X_2}$$ where $X_1$ and $X_2$ are random variables - and get away with it???","We all know that if we have a variable x, then there is a meaning to - for example - . And we all know how to manipulate that algebraically and to do calculus.  For example, if and then But random variables are a very different beast. Ultimately we all know intuitively what we mean when we draw a PDF or, more accurately, the CDF which is itself an algebraic function mapping to as part of that defines a random variable. But what is the actual math that makes that definition of an RV actually act algebraically, so that, as above, we can happily write things like and then where and are random variables - and get away with it???","y=e^x y_1=e^{x_1} y_2=e^{x_2} y_1y_2 = e^{x_1+x_2} [-\infty, \infty] [0,1] Y_1=e^{X_1} Y_2=e^{X_2} Y_1Y_2 = e^{X_1+X_2} X_1 X_2","['probability', 'probability-theory', 'measure-theory', 'random-variables']"
40,probability of infinite intersection of events,probability of infinite intersection of events,,"given a sequence of mutually independent events $\{A_n\}_{n \in \mathbb{N}}$ , I am trying to prove that: $$P\left(\bigcap_{i=n}^{\infty}A_i\right) = \lim \limits_{n \to \infty} \prod_{i=n}^{n}P(A_i)$$ It is easy to prove by induction that $P(\bigcap_{i=n}^{m}A_i) = \prod_{i=n}^{m}P(A_i)$ for all $m\in \mathbb{N}$ . Now, my problem is due to the limit. In particular: Can I write $\bigcap_{i=n}^{\infty}A_i =  \lim \limits_{m \to \infty} \bigcap_{i=n}^{m}A_i$ ?  If yes, is this the traditional way to interpret intersections - unions of countable sets? Can I write $P (\lim \limits_{m \to \infty} \bigcap_{i=n}^{m}A_i) = \lim \limits_{m \to \infty} P(\bigcap_{i=n}^{m}A_i)$ ? If yes, why and does this property hold in any measurable space? For example, we know that countable additivity is an axiom of measures, but there are no axioms for the one I wrote. So in case it is correct there must be a way to prove it. Maybe applying De Morgan's laws? Thanks a lot.","given a sequence of mutually independent events , I am trying to prove that: It is easy to prove by induction that for all . Now, my problem is due to the limit. In particular: Can I write ?  If yes, is this the traditional way to interpret intersections - unions of countable sets? Can I write ? If yes, why and does this property hold in any measurable space? For example, we know that countable additivity is an axiom of measures, but there are no axioms for the one I wrote. So in case it is correct there must be a way to prove it. Maybe applying De Morgan's laws? Thanks a lot.",\{A_n\}_{n \in \mathbb{N}} P\left(\bigcap_{i=n}^{\infty}A_i\right) = \lim \limits_{n \to \infty} \prod_{i=n}^{n}P(A_i) P(\bigcap_{i=n}^{m}A_i) = \prod_{i=n}^{m}P(A_i) m\in \mathbb{N} \bigcap_{i=n}^{\infty}A_i =  \lim \limits_{m \to \infty} \bigcap_{i=n}^{m}A_i P (\lim \limits_{m \to \infty} \bigcap_{i=n}^{m}A_i) = \lim \limits_{m \to \infty} P(\bigcap_{i=n}^{m}A_i),"['probability', 'limits', 'measure-theory']"
41,"If the expectation and variance of $X$ are both not affected by $Y$, and vice versa, then must $X$ and $Y$ be independent?","If the expectation and variance of  are both not affected by , and vice versa, then must  and  be independent?",X Y X Y,"I know that if $\mathbb{E}[X]=\mathbb{E}[X|Y] , \mathbb{E}[Y]=\mathbb{E}[Y|X]$ , $X$ and $Y$ can be dependent, for example a ‘uniform’ distribution in a unit circle. Now we add the variance, if $$\mathbb{E}[X]=\mathbb{E}[X|Y], \mathbb{E}[Y]=\mathbb{E}[Y|X], $$ $$Var(X)=Var(X|Y), Var(Y)=Var(Y|X).$$ Say the expectation and variance of $X$ are both not affected by $Y$ , and vice versa, then must $X$ and $Y$ be independent? In this case I can not find a counterexample just like the uniform circle. If they are independent, how to prove it? If not, is there a counterexample? Thanks!","I know that if , and can be dependent, for example a ‘uniform’ distribution in a unit circle. Now we add the variance, if Say the expectation and variance of are both not affected by , and vice versa, then must and be independent? In this case I can not find a counterexample just like the uniform circle. If they are independent, how to prove it? If not, is there a counterexample? Thanks!","\mathbb{E}[X]=\mathbb{E}[X|Y] , \mathbb{E}[Y]=\mathbb{E}[Y|X] X Y \mathbb{E}[X]=\mathbb{E}[X|Y], \mathbb{E}[Y]=\mathbb{E}[Y|X],  Var(X)=Var(X|Y), Var(Y)=Var(Y|X). X Y X Y","['probability', 'conditional-expectation', 'independence']"
42,"Ladybug walking on a hexagon, starts at vertex A.Every minute moves to one of the two adjacent vertices.Probability she is back at A after 10 minutes?","Ladybug walking on a hexagon, starts at vertex A.Every minute moves to one of the two adjacent vertices.Probability she is back at A after 10 minutes?",,"A ladybug is walking at random on a hexagon. She starts at vertex A. Every minute, she moves to one of the two vertices (chosen at random) adjacent to the one she's currently on. What is the probability that, after 10 minutes, she is back at A? I thought that at every even minute the ladybug would be at A, C, or E so there is a $\frac13$ chance. This was wrong and I got the following message: The probability may be close to $\frac 13,$ but since there are $2^{10}$ possible paths for the ladybug and $2^{10}$ is not divisible by $3,$ the probability cannot be exactly $\frac 13.$","A ladybug is walking at random on a hexagon. She starts at vertex A. Every minute, she moves to one of the two vertices (chosen at random) adjacent to the one she's currently on. What is the probability that, after 10 minutes, she is back at A? I thought that at every even minute the ladybug would be at A, C, or E so there is a chance. This was wrong and I got the following message: The probability may be close to but since there are possible paths for the ladybug and is not divisible by the probability cannot be exactly","\frac13 \frac 13, 2^{10} 2^{10} 3, \frac 13.",['probability']
43,Conditional probability question involving balls w/o replacement,Conditional probability question involving balls w/o replacement,,"A box contains 12 balls numbered 1 through 12.  If 5 balls are selected one at a time from the​ box, without​ replacement, what is the probability that the largest number selected will be 9​? I want to just say $$\frac{9\times8\times7\times6\times5}{\binom{12}{5}}$$ but that is wrong and I don't know why.","A box contains 12 balls numbered 1 through 12.  If 5 balls are selected one at a time from the​ box, without​ replacement, what is the probability that the largest number selected will be 9​? I want to just say $$\frac{9\times8\times7\times6\times5}{\binom{12}{5}}$$ but that is wrong and I don't know why.",,['probability']
44,How can we show that the sample paths of standard Brownian motion are continuous?,How can we show that the sample paths of standard Brownian motion are continuous?,,"I know the path of Brownian mothion is continuous in probability if and only if ,for every $\delta>0$ and $t>0$, $$\lim_{\Delta t\to 0}P(|B(t+\Delta t)-B(t)|\ge \delta)=0$$ I  can't continue it . I have seen some proofs of this theorem in this site but I want to prove it by definition. So thanks","I know the path of Brownian mothion is continuous in probability if and only if ,for every $\delta>0$ and $t>0$, $$\lim_{\Delta t\to 0}P(|B(t+\Delta t)-B(t)|\ge \delta)=0$$ I  can't continue it . I have seen some proofs of this theorem in this site but I want to prove it by definition. So thanks",,"['probability', 'stochastic-processes', 'continuity', 'brownian-motion']"
45,What is the probability that $x^4-y^4$ is divisible by 5?,What is the probability that  is divisible by 5?,x^4-y^4,"Two numbers $x$ and $y$ are chosen at random without replacement from the set $\{1,2,3...,5n\}$. What is the probability that $x^4-y^4$ is divisible by $5$? I divided the numbers into groups of 5 $(1,2,3,4,5),(6,7,8,9,10),...$.The probability in the first group would itself be the answer.But how to find that?","Two numbers $x$ and $y$ are chosen at random without replacement from the set $\{1,2,3...,5n\}$. What is the probability that $x^4-y^4$ is divisible by $5$? I divided the numbers into groups of 5 $(1,2,3,4,5),(6,7,8,9,10),...$.The probability in the first group would itself be the answer.But how to find that?",,['probability']
46,Integral Over Conditional PDF,Integral Over Conditional PDF,,"If $f(x,y)$ is a joint pdf,I understand that, $$   \int_{-\infty}^\infty\int_{-\infty}^\infty  f(x,y) \ dx \ dy = 1 $$ but does this hold for the conditional pdf? $$    \int_{-\infty}^\infty f({y|x}) \ dy = 1.  $$ I would argue not. For a given $X=x$ the sum of of the pdf over the support of $Y$ should less than or equal to 1. But according to Bierens it is. In particular, in the case (3.4) we have   \begin{align} \begin{split}    E[(Y &- E[Y \mid X]) I(X \in B)] \\   &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (y - g(x)) I(x \in B) f(y, x) \ dy \ dx \\ &= \int_{-\infty}^{\infty} \left( \int_{-\infty}^{\infty} y f(y \mid x) \ dy \right) I(x \in B) f_x(x) \ dx \\ &\quad - \int_{-\infty}^{\infty} \left( \int_{-\infty}^{\infty} f(y \mid x) \ dy \right) g(x) I (x \in B) f_x(x) \ dx \\ &= \int_{-\infty}^{\infty} g(x) I(x \in B) f_x(x) \ dx - \int_{-\infty}^{\infty} g(x) I (x \in B) f_x(x) \ d x = 0.  \end{split} \tag{3.7} \end{align}","If $f(x,y)$ is a joint pdf,I understand that, $$   \int_{-\infty}^\infty\int_{-\infty}^\infty  f(x,y) \ dx \ dy = 1 $$ but does this hold for the conditional pdf? $$    \int_{-\infty}^\infty f({y|x}) \ dy = 1.  $$ I would argue not. For a given $X=x$ the sum of of the pdf over the support of $Y$ should less than or equal to 1. But according to Bierens it is. In particular, in the case (3.4) we have   \begin{align} \begin{split}    E[(Y &- E[Y \mid X]) I(X \in B)] \\   &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (y - g(x)) I(x \in B) f(y, x) \ dy \ dx \\ &= \int_{-\infty}^{\infty} \left( \int_{-\infty}^{\infty} y f(y \mid x) \ dy \right) I(x \in B) f_x(x) \ dx \\ &\quad - \int_{-\infty}^{\infty} \left( \int_{-\infty}^{\infty} f(y \mid x) \ dy \right) g(x) I (x \in B) f_x(x) \ dx \\ &= \int_{-\infty}^{\infty} g(x) I(x \in B) f_x(x) \ dx - \int_{-\infty}^{\infty} g(x) I (x \in B) f_x(x) \ d x = 0.  \end{split} \tag{3.7} \end{align}",,['probability']
47,"Find pmf for $i=0,1,2,3,4$",Find pmf for,"i=0,1,2,3,4","I have a problem that I'm having trouble with. Here is the problem: ""Five distinct numbers are randomly distributed to players numbered 1 through 5. Whenever two players compare their numbers, the one with the higher one is declared the winner. Initially, players 1 and 2 compare their number; the winner the compares his number with that of player 3, and so on. Let X denote the number of times player 1 is the winner. Find $P(X=i), i=0,1,2,3,4.$ "" Of course $P(X=0) = \frac{1}{2}$, but I went on to $P(X=1)$, which I thought equaled $\frac{1}{2} \times \frac{1}{2}$. When I looked up the solution it showed $P(X=1)=\frac{1}{2} \times (1- \frac{2}{3})$. My thinking was $P(X=1)=P $(player 1 wins the first and loses the second)=P(player 1 wins the first)P(player 1 loses the second)=P(player 1 wins the first)*(1-P(player 1 wins the second). Why is P(player 1 loses the second) $= \frac{1}{3}$?","I have a problem that I'm having trouble with. Here is the problem: ""Five distinct numbers are randomly distributed to players numbered 1 through 5. Whenever two players compare their numbers, the one with the higher one is declared the winner. Initially, players 1 and 2 compare their number; the winner the compares his number with that of player 3, and so on. Let X denote the number of times player 1 is the winner. Find $P(X=i), i=0,1,2,3,4.$ "" Of course $P(X=0) = \frac{1}{2}$, but I went on to $P(X=1)$, which I thought equaled $\frac{1}{2} \times \frac{1}{2}$. When I looked up the solution it showed $P(X=1)=\frac{1}{2} \times (1- \frac{2}{3})$. My thinking was $P(X=1)=P $(player 1 wins the first and loses the second)=P(player 1 wins the first)P(player 1 loses the second)=P(player 1 wins the first)*(1-P(player 1 wins the second). Why is P(player 1 loses the second) $= \frac{1}{3}$?",,"['probability', 'discrete-mathematics', 'random-variables']"
48,permutations confusion!,permutations confusion!,,"Hello this is my first post ,  I am reading a book called (probability for dummies) the answer in the book for the question below has confused me ... Suppose you have four friends named Jim , Arun , Soma , and Eric. How many ways can you rearrange the individuals in a row so that Soma and Eric don't sit next to each other ? The answer in the book is 18 of the scenarios involved the two not sitting next to each other I know there are 4! = 24 ways to seat the four people But I don't understand how the author got to 18 when 2 of them cant sit next to each other would really appreciate any help thank you","Hello this is my first post ,  I am reading a book called (probability for dummies) the answer in the book for the question below has confused me ... Suppose you have four friends named Jim , Arun , Soma , and Eric. How many ways can you rearrange the individuals in a row so that Soma and Eric don't sit next to each other ? The answer in the book is 18 of the scenarios involved the two not sitting next to each other I know there are 4! = 24 ways to seat the four people But I don't understand how the author got to 18 when 2 of them cant sit next to each other would really appreciate any help thank you",,"['probability', 'statistics', 'permutations']"
49,Understanding Sufficient statistic.,Understanding Sufficient statistic.,,A sufficient statistic for a parameter is a statistic that captures all the information about a given parameter contained in the sample. My question: Is the above sentence correct. (I think it is). If yes then what is the purpose of a sufficient statistic? I mean it does not give any additional information about the unknown parameter (to be estimated) that is not already present in the sample at the first place. So what is the use of sufficiency in Mathematical Statistics? EDIT 1: After @user164740 's response: My queries: 1) So it means that a sufficient statistic can have less information about the parameter to be estimated than present in the given sample? 2)And how would a worse statistic (in terms of information contained about the parameter) would help if the given statistic is not helpful?  I mean how is the given sufficient statistic helpful and how would a worse statistic be helpful in estimating a parameter?,A sufficient statistic for a parameter is a statistic that captures all the information about a given parameter contained in the sample. My question: Is the above sentence correct. (I think it is). If yes then what is the purpose of a sufficient statistic? I mean it does not give any additional information about the unknown parameter (to be estimated) that is not already present in the sample at the first place. So what is the use of sufficiency in Mathematical Statistics? EDIT 1: After @user164740 's response: My queries: 1) So it means that a sufficient statistic can have less information about the parameter to be estimated than present in the given sample? 2)And how would a worse statistic (in terms of information contained about the parameter) would help if the given statistic is not helpful?  I mean how is the given sufficient statistic helpful and how would a worse statistic be helpful in estimating a parameter?,,"['probability', 'statistics', 'probability-theory', 'probability-distributions', 'sampling']"
50,Present a combinatorial argument for the identiy $\sum^{n}_{k=1} k\binom{n}{k} = n\cdot 2^{n-1}$,Present a combinatorial argument for the identiy,\sum^{n}_{k=1} k\binom{n}{k} = n\cdot 2^{n-1},"This is a question in my textbook that does not provide a solution. Any help on a solution? Consider the following identity: $\sum^{n}_{k=1} k\binom{n}{k} = n\cdot 2^{n-1}$ Present  a combinatorial argument for the identity by considering a set of $n$ people and determining,in two methods, the number of ways you can select a committee of any size and a chair personfor the committee (a) How many ways possible you can select a committee of size k and its chairperson? (b) How many ways possible you can select a chairperson and the other committee members?","This is a question in my textbook that does not provide a solution. Any help on a solution? Consider the following identity: $\sum^{n}_{k=1} k\binom{n}{k} = n\cdot 2^{n-1}$ Present  a combinatorial argument for the identity by considering a set of $n$ people and determining,in two methods, the number of ways you can select a committee of any size and a chair personfor the committee (a) How many ways possible you can select a committee of size k and its chairperson? (b) How many ways possible you can select a chairperson and the other committee members?",,"['probability', 'combinatorics', 'statistics', 'combinations']"
51,Find the probability that the $4$th ball removed from the box is white,Find the probability that the th ball removed from the box is white,4,"A box has $10$ balls, $6$ of which are black and $4$ of which are white. $3$ balls are removed from the box, their color unnoted. Find the probability that a fourth ball removed from the box is white, Assume that the $10$ balls are equally likely to be drawn from the box. I got the correct result, but I used binomial coefficients, which were introduced in the next chapter, so I wanted to ask if there's a simpler way to solve the exercise, I did; $\displaystyle\Pr(4^{th}\text{Ball is white})=\frac{\binom{6}{3}\frac47+\binom{6}{2}\binom{4}{1}\frac37+\binom{6}{1}\binom{4}{2}\frac27+\binom{4}{3}\frac17}{\binom{10}{3}}=\frac25$ where the $1.$ summand, $\displaystyle\frac{\binom{6}{3}\frac47}{\binom{10}{3}}$ is the probability, that $4^{th}\text{Ball is white}$, if the first $3$ were black. etc. Is it pure chance that $\frac25=\frac{4}{10}=$(number of white balls divided by total number of balls) is the correct result ?","A box has $10$ balls, $6$ of which are black and $4$ of which are white. $3$ balls are removed from the box, their color unnoted. Find the probability that a fourth ball removed from the box is white, Assume that the $10$ balls are equally likely to be drawn from the box. I got the correct result, but I used binomial coefficients, which were introduced in the next chapter, so I wanted to ask if there's a simpler way to solve the exercise, I did; $\displaystyle\Pr(4^{th}\text{Ball is white})=\frac{\binom{6}{3}\frac47+\binom{6}{2}\binom{4}{1}\frac37+\binom{6}{1}\binom{4}{2}\frac27+\binom{4}{3}\frac17}{\binom{10}{3}}=\frac25$ where the $1.$ summand, $\displaystyle\frac{\binom{6}{3}\frac47}{\binom{10}{3}}$ is the probability, that $4^{th}\text{Ball is white}$, if the first $3$ were black. etc. Is it pure chance that $\frac25=\frac{4}{10}=$(number of white balls divided by total number of balls) is the correct result ?",,"['probability', 'balls-in-bins']"
52,sum of independent exponential random variables,sum of independent exponential random variables,,"Let $X$ be the sum of two independent exponential random variables: $X_{1}$ with parameter $\lambda_{1} = \frac{1}{5}$ and $X_{2}$ with parameter $\lambda_{2} = 2 $. These random variables have values in the interval $[0,60]$ I want to prove the variance of $X$ is $401$. I found by looking into some references that the p.d.f of $X$ is:  $$ f_{X}(t) = \sum_{i=1}^{2} {\frac{\lambda_{1} \lambda_{2} }{\prod_{j=1,j \ne i}^{2}(\lambda_{j} -\lambda_{i})} exp(-\lambda_{i} t)} $$ I wanted to know if the above density is correct or not since I did find a value different than what I'm supposed to get for the variance computation. Thanks.","Let $X$ be the sum of two independent exponential random variables: $X_{1}$ with parameter $\lambda_{1} = \frac{1}{5}$ and $X_{2}$ with parameter $\lambda_{2} = 2 $. These random variables have values in the interval $[0,60]$ I want to prove the variance of $X$ is $401$. I found by looking into some references that the p.d.f of $X$ is:  $$ f_{X}(t) = \sum_{i=1}^{2} {\frac{\lambda_{1} \lambda_{2} }{\prod_{j=1,j \ne i}^{2}(\lambda_{j} -\lambda_{i})} exp(-\lambda_{i} t)} $$ I wanted to know if the above density is correct or not since I did find a value different than what I'm supposed to get for the variance computation. Thanks.",,"['probability', 'probability-distributions']"
53,Independence of max and min of a set of random variables.,Independence of max and min of a set of random variables.,,"Suppose $X_1,\ldots,X_n$ are independent and identically distributed random variables with cdf $F_X(x)$. Define $U$ and $L$ as $U=\max\{ X_1, \ldots ,X_n\}$ and $L = \min\{X_1,\ldots,X_n\}$. Are $U$ and $L$ independent? I believe they are I just don't know how to prove it.","Suppose $X_1,\ldots,X_n$ are independent and identically distributed random variables with cdf $F_X(x)$. Define $U$ and $L$ as $U=\max\{ X_1, \ldots ,X_n\}$ and $L = \min\{X_1,\ldots,X_n\}$. Are $U$ and $L$ independent? I believe they are I just don't know how to prove it.",,"['probability', 'probability-theory', 'random-variables']"
54,Probability Question - An elevator & 5 Passengers,Probability Question - An elevator & 5 Passengers,,"I'm working on this problem: An elevator in a building starts with 5 passengers and stops at seven   7 floors. If each passenger is equally likely to get an any floor and   all the passengers leave independently of each other, what is the   probability that no two passengers will get off at the same floor? I figure you could do 7 P 5 - since the first guy has 7 floors he can get off on, the second 6, etc. But do I need to account for the order that the people can choose their floor, and therefore multiply that by 5 factorial? Or am I approaching this incorrectly in the first place? Thanks","I'm working on this problem: An elevator in a building starts with 5 passengers and stops at seven   7 floors. If each passenger is equally likely to get an any floor and   all the passengers leave independently of each other, what is the   probability that no two passengers will get off at the same floor? I figure you could do 7 P 5 - since the first guy has 7 floors he can get off on, the second 6, etc. But do I need to account for the order that the people can choose their floor, and therefore multiply that by 5 factorial? Or am I approaching this incorrectly in the first place? Thanks",,"['probability', 'combinations']"
55,Probability of dice double - why not count duplicates?,Probability of dice double - why not count duplicates?,,"What is the probability of getting double dice? (one throw) All possible dice rolls are 36 (counting even rolls like 1-4 and 4-1). But when counting the doubles, why not count 1-1 twice, for example? My book says the probability is 6/36.","What is the probability of getting double dice? (one throw) All possible dice rolls are 36 (counting even rolls like 1-4 and 4-1). But when counting the doubles, why not count 1-1 twice, for example? My book says the probability is 6/36.",,"['probability', 'dice']"
56,What does it mean that the probability density function is proportional to a function?,What does it mean that the probability density function is proportional to a function?,,I'm studying for SOA/CAS Exam P and I have a problem that says that $X$ is a continuous and positive random variable whose probability density function is proportional to: $$\frac{1}{(1+x)^5}$$ Where $0\lt x \lt \infty$ and I need to find $E(X)$. But how do I use the information that $X$ is proportional to that function? What does it mean exactly?,I'm studying for SOA/CAS Exam P and I have a problem that says that $X$ is a continuous and positive random variable whose probability density function is proportional to: $$\frac{1}{(1+x)^5}$$ Where $0\lt x \lt \infty$ and I need to find $E(X)$. But how do I use the information that $X$ is proportional to that function? What does it mean exactly?,,"['probability', 'probability-theory', 'probability-distributions', 'actuarial-science']"
57,Correlation and dependence between $X$ and $Y:=X^2$,Correlation and dependence between  and,X Y:=X^2,"If we say $X$ has a uniform distribution on $\{-1,0,1\}$ and let $Y=X^2$, are $X$ and $Y$ uncorrelated and are they independent? I would say that they are not independent since $Y$ clearly depends on $X$, but a friend told me that that's not correct. How would I show that they are dependent? (Or maybe he is correct?) Also I said that they were correlated because $Y$ changes as $X$ changes, meaning correlation right? I'm just feeling doubtful now. Some help please?","If we say $X$ has a uniform distribution on $\{-1,0,1\}$ and let $Y=X^2$, are $X$ and $Y$ uncorrelated and are they independent? I would say that they are not independent since $Y$ clearly depends on $X$, but a friend told me that that's not correct. How would I show that they are dependent? (Or maybe he is correct?) Also I said that they were correlated because $Y$ changes as $X$ changes, meaning correlation right? I'm just feeling doubtful now. Some help please?",,"['probability', 'probability-theory']"
58,What is the probability distribution on integers?,What is the probability distribution on integers?,,"Assume that in the group of integers $\mathbb{Z}$, I randomly choose two integers $a$ and $b$ and I would like to ask whether they generate the group $\mathbb{Z}$, and then, what is the probability for this event? However, to explain clearly ""probability meaning"", we need to know about probability distribution. Could any one give  me an exact definition for this then? Or here, we just reduce our consideration on each quotient group $\mathbb{Z}/n\mathbb{Z}$ for each $n$? Thanks in advance.","Assume that in the group of integers $\mathbb{Z}$, I randomly choose two integers $a$ and $b$ and I would like to ask whether they generate the group $\mathbb{Z}$, and then, what is the probability for this event? However, to explain clearly ""probability meaning"", we need to know about probability distribution. Could any one give  me an exact definition for this then? Or here, we just reduce our consideration on each quotient group $\mathbb{Z}/n\mathbb{Z}$ for each $n$? Thanks in advance.",,"['probability', 'group-theory', 'probability-theory', 'probability-distributions']"
59,Pulling cards from a deck without replacement to reach a goal: average draws needed?,Pulling cards from a deck without replacement to reach a goal: average draws needed?,,"I have the following probability problem that I think must be quite common. The problem is as follows: Let's say I have a goal of drawing 3 jacks from a regular deck of 52 cards (in which there are 4 jacks). I conduct many experiments. In each experiment, I shuffle the deck and pull cards one-by-one and discard the card without replacement. When I reach my goal of having pulled 3   jacks, I write down the number of cards I had needed to pull and stop the experiment . e.g. in the first experiment I might have hit the 3rd jack on the 40th card, so I write down '40'. I repeat this infinite times, and then average the number of cards pulled to reach 3 jacks. On average, how many cards did I need to pull before reaching 3 jacks? Note that I am stopping after pulling the third jack, so my last draw must be a successful jack draw. I think I can solve this problem using hypergeometric distributions, but the solution is ugly and complicated (it gives 31.8 draws on average are needed, which matches Monte Carlo simulations a colleague ran for me). I think I've stumbled upon a much simpler formula: average draws needed =  (n) * (x+1)/(y+1) where n is the number of jacks I want (3), x is the number of cards in the deck (52), and y is the number of jacks in the deck (4). Other than by blind luck of simple observation that I got playing around with numbers, I have no idea how to derive the above formula. Has anyone seen this problem and know how this simple formula might be derived? I should also note that the simple formula has been tested for many n, x, and y values and matches both the complicated formula and several simulations run for this problem. So there is a decent degree of confidence that it is correct.","I have the following probability problem that I think must be quite common. The problem is as follows: Let's say I have a goal of drawing 3 jacks from a regular deck of 52 cards (in which there are 4 jacks). I conduct many experiments. In each experiment, I shuffle the deck and pull cards one-by-one and discard the card without replacement. When I reach my goal of having pulled 3   jacks, I write down the number of cards I had needed to pull and stop the experiment . e.g. in the first experiment I might have hit the 3rd jack on the 40th card, so I write down '40'. I repeat this infinite times, and then average the number of cards pulled to reach 3 jacks. On average, how many cards did I need to pull before reaching 3 jacks? Note that I am stopping after pulling the third jack, so my last draw must be a successful jack draw. I think I can solve this problem using hypergeometric distributions, but the solution is ugly and complicated (it gives 31.8 draws on average are needed, which matches Monte Carlo simulations a colleague ran for me). I think I've stumbled upon a much simpler formula: average draws needed =  (n) * (x+1)/(y+1) where n is the number of jacks I want (3), x is the number of cards in the deck (52), and y is the number of jacks in the deck (4). Other than by blind luck of simple observation that I got playing around with numbers, I have no idea how to derive the above formula. Has anyone seen this problem and know how this simple formula might be derived? I should also note that the simple formula has been tested for many n, x, and y values and matches both the complicated formula and several simulations run for this problem. So there is a decent degree of confidence that it is correct.",,['probability']
60,finding $P(Y_i=X_n)$ in exponential distribution,finding  in exponential distribution,P(Y_i=X_n),"Suppose $X_1,X_2,\cdots,X_n$ are independent random variables and  we show order statistics of this random variables with $Y_1,Y_2,\cdots,Y_n$. $X_1,X_2,\cdots,X_{n-1}$ have exponential distribution with mean $1$ and $X_n$ has exponential distribution with mean $\theta$. How can find $P(Y_i=X_n)$ for ($i=1,2,\cdots,n$)?","Suppose $X_1,X_2,\cdots,X_n$ are independent random variables and  we show order statistics of this random variables with $Y_1,Y_2,\cdots,Y_n$. $X_1,X_2,\cdots,X_{n-1}$ have exponential distribution with mean $1$ and $X_n$ has exponential distribution with mean $\theta$. How can find $P(Y_i=X_n)$ for ($i=1,2,\cdots,n$)?",,"['probability', 'order-statistics']"
61,recurrence solution to gambler's ruin,recurrence solution to gambler's ruin,,"From DeGroot 2.4.2, let $a_i$ be the conditional probability that the gambler wins all $k$  given gambler is at $i$. $a_i = pa_{i+1} + (1 - p)a_{i-1} $ It's not clear from the text what steps are taken to solve for the general form $a_i$ (maybe Gaussian elimination). How do you solve the recurrence equation for $a_i$? If this can be found through the characteristic equation of a matrix $A$, then how do you construct $A$? As the problem is described, the first row has $a_0 = 0$, and last with $a_k = 1$.","From DeGroot 2.4.2, let $a_i$ be the conditional probability that the gambler wins all $k$  given gambler is at $i$. $a_i = pa_{i+1} + (1 - p)a_{i-1} $ It's not clear from the text what steps are taken to solve for the general form $a_i$ (maybe Gaussian elimination). How do you solve the recurrence equation for $a_i$? If this can be found through the characteristic equation of a matrix $A$, then how do you construct $A$? As the problem is described, the first row has $a_0 = 0$, and last with $a_k = 1$.",,"['linear-algebra', 'probability', 'recurrence-relations']"
62,"Probability to find the sequence ""Rar!"" in a random (uniform) bytes stream at a position $\le n$","Probability to find the sequence ""Rar!"" in a random (uniform) bytes stream at a position",\le n,"I have random (uniform distribution) stream of bytes (integers from $0$ upto $255$). What probability $p(n)$ to find the sequence ""Rar!"" (ASCII) at a position $\le n$ from the start of the stream?","I have random (uniform distribution) stream of bytes (integers from $0$ upto $255$). What probability $p(n)$ to find the sequence ""Rar!"" (ASCII) at a position $\le n$ from the start of the stream?",,['probability']
63,Variance of the number of empty cells,Variance of the number of empty cells,,"If I place $k$ balls in $n$ cells randomly with uniform probability, then for large enough $n$ and $k/n$ small enough, I expect about $n e^{-k/n}$ cells to be empty (using the Poisson approximation to the Binomial.)  However, I cannot find the limit of the variance of the number of empty cells? One can compute explicit cases by using the probability that exactly $m$ cells are empty, $$var = \sum_{m=0}^nm^2\binom{n}{m}\sum_{\nu=0}^{n-m}(-1)^\nu\binom{n-m}{\nu}\left(1-\frac{m+\nu}{n}\right)^k - \left(1 - \frac{1}{n}\right)^{2k}$$ but I cannot find the limit of this. The same question can be asked of singletons, doubletons, etc.","If I place $k$ balls in $n$ cells randomly with uniform probability, then for large enough $n$ and $k/n$ small enough, I expect about $n e^{-k/n}$ cells to be empty (using the Poisson approximation to the Binomial.)  However, I cannot find the limit of the variance of the number of empty cells? One can compute explicit cases by using the probability that exactly $m$ cells are empty, $$var = \sum_{m=0}^nm^2\binom{n}{m}\sum_{\nu=0}^{n-m}(-1)^\nu\binom{n-m}{\nu}\left(1-\frac{m+\nu}{n}\right)^k - \left(1 - \frac{1}{n}\right)^{2k}$$ but I cannot find the limit of this. The same question can be asked of singletons, doubletons, etc.",,['probability']
64,Sorting a deck of cards with Bogosort,Sorting a deck of cards with Bogosort,,"Suppose you have a standard deck of 52 cards which you would like to sort in a particular order. The notorious algorithm Bogosort works like this: Shuffle the deck Check if the deck is sorted. If it's not sorted, goto 1. If it's sorted, you're done. Let B(n) be the probability that Bogosort sorts the deck in n shuffles or less. B(n) is a monotonically increasing function which converges toward 1. What is the smallest value of n for which B(n) exceeds, say, 0.9? If the question is computationally infeasible then feel free to reduce the number of cards in the deck.","Suppose you have a standard deck of 52 cards which you would like to sort in a particular order. The notorious algorithm Bogosort works like this: Shuffle the deck Check if the deck is sorted. If it's not sorted, goto 1. If it's sorted, you're done. Let B(n) be the probability that Bogosort sorts the deck in n shuffles or less. B(n) is a monotonically increasing function which converges toward 1. What is the smallest value of n for which B(n) exceeds, say, 0.9? If the question is computationally infeasible then feel free to reduce the number of cards in the deck.",,"['probability', 'permutations', 'sorting']"
65,Probability of choosing 4 cards whose sum is 5 from a deck of 40 cards,Probability of choosing 4 cards whose sum is 5 from a deck of 40 cards,,"Let's say we have a deck of cards excluding face cards, so cards from Ace to 10. Which of these is the correct way of computing the probability that the sum of 4 randomly chosen cards is equal to 5? Method #1: $P(X=5)=\frac{8}{40}\cdot\frac{7}{39}\cdot\frac{6}{38}\cdot\frac{4}{37}$ My logic is that for the first card, there are 8 possibilities. 4 Aces and 4 Two's. For the second card, if the first card was an Ace, there are, 7 possibilities. 3 Aces and 4 Two's. If the second card was an Ace, the third card can be chosen from 2 Aces and 4 Two's. Finally, if the third card was an Ace, the fourth card has to be a Two, so its sample space is just 4 Two's. Method #2: $P(X=5) = \frac{8}{40}\cdot\frac{4}{39}\cdot\frac{3}{38}\cdot\frac{2}{37}$ Like Method #1, the first card can be chosen from 4 Aces and 4 Two's. This time however, we will assume that the first card turned out to be a Two. By definition, all the remaining cards have to be Aces. Method #3: $P(X=5)=\frac{4}{40}\cdot\frac{4}{39}\cdot\frac{3}{38}\cdot\frac{2}{37}$ We know that in order to have a sum of 5, we specifically need 1 Two card and 3 Ace cards. The first fraction represents the probability of choosing a Two, and the rest pertains to Aces. I think Method #3 is the most plausible of them all, but I still feel that they're all wrong and that I should just use the hypergeometric formula to find out the probability.","Let's say we have a deck of cards excluding face cards, so cards from Ace to 10. Which of these is the correct way of computing the probability that the sum of 4 randomly chosen cards is equal to 5? Method #1: My logic is that for the first card, there are 8 possibilities. 4 Aces and 4 Two's. For the second card, if the first card was an Ace, there are, 7 possibilities. 3 Aces and 4 Two's. If the second card was an Ace, the third card can be chosen from 2 Aces and 4 Two's. Finally, if the third card was an Ace, the fourth card has to be a Two, so its sample space is just 4 Two's. Method #2: Like Method #1, the first card can be chosen from 4 Aces and 4 Two's. This time however, we will assume that the first card turned out to be a Two. By definition, all the remaining cards have to be Aces. Method #3: We know that in order to have a sum of 5, we specifically need 1 Two card and 3 Ace cards. The first fraction represents the probability of choosing a Two, and the rest pertains to Aces. I think Method #3 is the most plausible of them all, but I still feel that they're all wrong and that I should just use the hypergeometric formula to find out the probability.",P(X=5)=\frac{8}{40}\cdot\frac{7}{39}\cdot\frac{6}{38}\cdot\frac{4}{37} P(X=5) = \frac{8}{40}\cdot\frac{4}{39}\cdot\frac{3}{38}\cdot\frac{2}{37} P(X=5)=\frac{4}{40}\cdot\frac{4}{39}\cdot\frac{3}{38}\cdot\frac{2}{37},"['probability', 'combinatorics', 'probability-distributions', 'combinations', 'card-games']"
66,"Estimating how many of the first $10,000$ Fibonacci numbers start with the digit $9$",Estimating how many of the first  Fibonacci numbers start with the digit,"10,000 9","Consider the problem of estimating how many of the first $10,000$ Fibonacci numbers begin with the digit $9$ . The only ideas I have so far: Obviously, if we assume that the every first digit is equally likely, the answer is around $1000$ (Note: Dietrich Burde points out that is wrong. $0$ can't be the first digit, so I should divide by $9$ , not $10$ ). Listing out the Fibonacci numbers: $1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, ...$ , we can see that their first digits, $1, 1, 2, 3, 5, 8, 1, 2, 3, 5, 8, 1, 2, 3, 6, 9, 1, ...$ seem to follow a pattern somewhat similar to $1, 2, 3, 5, 8$ , with $9$ 's introduced less often. So perhaps the answer is less than $1000$ . I'll put the answer here: 456 of the first $10,000$ Fibonacci numbers start with the digit $9$ . Any ideas/hints of how to estimate or compute this analytically?","Consider the problem of estimating how many of the first Fibonacci numbers begin with the digit . The only ideas I have so far: Obviously, if we assume that the every first digit is equally likely, the answer is around (Note: Dietrich Burde points out that is wrong. can't be the first digit, so I should divide by , not ). Listing out the Fibonacci numbers: , we can see that their first digits, seem to follow a pattern somewhat similar to , with 's introduced less often. So perhaps the answer is less than . I'll put the answer here: 456 of the first Fibonacci numbers start with the digit . Any ideas/hints of how to estimate or compute this analytically?","10,000 9 1000 0 9 10 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, ... 1, 1, 2, 3, 5, 8, 1, 2, 3, 5, 8, 1, 2, 3, 6, 9, 1, ... 1, 2, 3, 5, 8 9 1000 10,000 9","['probability', 'contest-math', 'fibonacci-numbers', 'estimation', 'decimal-expansion']"
67,"What's the probability of going to (6, 4) without ever visiting (3, 4)?","What's the probability of going to (6, 4) without ever visiting (3, 4)?",,"The ant starts at (0, 0). With each move it goes either up or right by one unit distance. After 10 moves, what is the probability that the ant settles down at point (6, 4) without ever visiting the point (3, 4)? My solution: Since the ant has to make 10 moves moving either up or right, the amount of possible routes is 2^10 = 1024. There are 10!/(6!4!) = 210 possible ways to reach (6, 4) in 10 moves. Hence, the probability of going to (6, 4) is 210/1024 ≈ 0.2051. Since the ant is only able to move either up or right, it has to make 7 moves (3 right, 4 up) to get to (3, 4). There are 2^7 = 128 possible routes to take in 7 moves. And so, there are 7!/(3!4!) = 35 possible ways to reach (3, 4) in 7 moves. So, the probability of going to (3, 4) in 7 moves should be 35/128 ≈ 0.2734. The probability of not going to (3, 4) is 1 - 0.2734 = 0.7266. With that, the probability of going to (6, 4) are 0.2051 and the probability of not going to (3, 4) is 0.7266. Since both of these events should occur to satisfy the condition, my answer would be 0.2051 * 0.7266 ≈ 0.149. However, I suppose my solution is wrong because if the ant goes to (7, 0) or (2, 5) in the first 7 moves, there's no way for it to reach (6, 4). Is there a way to fix my solution or have I gone absolutely the wrong way?","The ant starts at (0, 0). With each move it goes either up or right by one unit distance. After 10 moves, what is the probability that the ant settles down at point (6, 4) without ever visiting the point (3, 4)? My solution: Since the ant has to make 10 moves moving either up or right, the amount of possible routes is 2^10 = 1024. There are 10!/(6!4!) = 210 possible ways to reach (6, 4) in 10 moves. Hence, the probability of going to (6, 4) is 210/1024 ≈ 0.2051. Since the ant is only able to move either up or right, it has to make 7 moves (3 right, 4 up) to get to (3, 4). There are 2^7 = 128 possible routes to take in 7 moves. And so, there are 7!/(3!4!) = 35 possible ways to reach (3, 4) in 7 moves. So, the probability of going to (3, 4) in 7 moves should be 35/128 ≈ 0.2734. The probability of not going to (3, 4) is 1 - 0.2734 = 0.7266. With that, the probability of going to (6, 4) are 0.2051 and the probability of not going to (3, 4) is 0.7266. Since both of these events should occur to satisfy the condition, my answer would be 0.2051 * 0.7266 ≈ 0.149. However, I suppose my solution is wrong because if the ant goes to (7, 0) or (2, 5) in the first 7 moves, there's no way for it to reach (6, 4). Is there a way to fix my solution or have I gone absolutely the wrong way?",,"['probability', 'combinatorics', 'probability-theory', 'conditional-probability']"
68,Expected value of $100$ briefcases each with $1$ dollar except for one briefcase that resets your accumulated amount to zero?,Expected value of  briefcases each with  dollar except for one briefcase that resets your accumulated amount to zero?,100 1,"In this game there are $100$ suitcases and each contains the same dollar amount (let's say $1$ dollar) except for one suitcase that contains a bomb that resets the total value you accumulated so far to $0$ .  Naturally, you don't know which suitcase contains the bomb and you need to open every suitcase, what is the expected value of the amount you end up with? This is not a homework question, just a question I wondered about after seeing it in a gameshow. Is there a way to solve this analytically? And if so, how would you do that?","In this game there are suitcases and each contains the same dollar amount (let's say dollar) except for one suitcase that contains a bomb that resets the total value you accumulated so far to .  Naturally, you don't know which suitcase contains the bomb and you need to open every suitcase, what is the expected value of the amount you end up with? This is not a homework question, just a question I wondered about after seeing it in a gameshow. Is there a way to solve this analytically? And if so, how would you do that?",100 1 0,"['probability', 'combinatorics', 'expected-value']"
69,Discrete Probability Question With No Named Distribution,Discrete Probability Question With No Named Distribution,,"I've got a real world probability problem that I have been able to solve easily using simulation but that I am struggling to find (out of pure interest) an analytic solution to. For privacy reasons, I have changed both the context and the actual numbers. A vendor is selling tickets. Customers either purchase 1 ticket with probability 60%, 2 tickets with probability 30%, or 3 tickets with probability 10%. If the vendor needs to sell 10 tickets, how many customers on average does he need to sell to (assuming customer purchases are iid)? A simulation produces an average of 6.6 customers, but I am curious what an analytic solution to this might look like (if there exists a convenient one) as there is no named distribution I can think of that corresponds to this problem.","I've got a real world probability problem that I have been able to solve easily using simulation but that I am struggling to find (out of pure interest) an analytic solution to. For privacy reasons, I have changed both the context and the actual numbers. A vendor is selling tickets. Customers either purchase 1 ticket with probability 60%, 2 tickets with probability 30%, or 3 tickets with probability 10%. If the vendor needs to sell 10 tickets, how many customers on average does he need to sell to (assuming customer purchases are iid)? A simulation produces an average of 6.6 customers, but I am curious what an analytic solution to this might look like (if there exists a convenient one) as there is no named distribution I can think of that corresponds to this problem.",,['probability']
70,Why doesn't my combinatorial solution to simple probability problem work?,Why doesn't my combinatorial solution to simple probability problem work?,,"I need to find the probability of NOT getting an ace card on two draws from a deck of 52 cards. My first thought (which I really think is correct) was to get the probability from taking $\frac{48}{52}\frac{47}{51}=\frac{\binom{48}{2}}{\binom{52}{2}}\approx0.85$ . Isn't this correct? Then I though about it in another way. There are $\binom{52}{2}$ ways too choose two cards from 52. To not get an ace, you can to choose two out of 12 values (where the ace is excluded) in $\binom{12}{2}$ ways, and then you can choose $\binom{4}{1}=4$ different cards from each of the chosen values. Then I'm thinking you could calculate the probability by taking $$\frac{\binom{12}{2}\binom{4}{1}^{2}}{\binom{52}{2}}\approx0.80$$ This doesn't give the same as the first method I used. I'm obviously missing something, but can't really figure out what is wrong. I would appreciate some guidance on how I should think differently doing the later method! (I suppose the probability is approx. 0.85 as I got in the first place.)","I need to find the probability of NOT getting an ace card on two draws from a deck of 52 cards. My first thought (which I really think is correct) was to get the probability from taking . Isn't this correct? Then I though about it in another way. There are ways too choose two cards from 52. To not get an ace, you can to choose two out of 12 values (where the ace is excluded) in ways, and then you can choose different cards from each of the chosen values. Then I'm thinking you could calculate the probability by taking This doesn't give the same as the first method I used. I'm obviously missing something, but can't really figure out what is wrong. I would appreciate some guidance on how I should think differently doing the later method! (I suppose the probability is approx. 0.85 as I got in the first place.)",\frac{48}{52}\frac{47}{51}=\frac{\binom{48}{2}}{\binom{52}{2}}\approx0.85 \binom{52}{2} \binom{12}{2} \binom{4}{1}=4 \frac{\binom{12}{2}\binom{4}{1}^{2}}{\binom{52}{2}}\approx0.80,"['probability', 'combinatorics']"
71,Can you prove this equality? Binomial coefficients and probability frequency,Can you prove this equality? Binomial coefficients and probability frequency,,"Here's the equality : $$\frac{\sum_{k=0}^n \binom{n}{k}(k/n)}{2^n} = 1/2$$ I've tried for 1, 2, 3 and found the equality was right. I don't know how to prove it (by induction, maybe, but the formula looks too complicated for induction to me) and I'd also like to know (if there is any) the name of this equality. I found it when looking at the probability of a coin (2 sides) and I took the mean of all the frequencies weighted by their amount of occurence. Could have been a dice (6 sides) so I feel like this formula could exist for 1/6 = .... That's why I am looking for the generic name of it ! Thank you in advance for your help in this problem","Here's the equality : I've tried for 1, 2, 3 and found the equality was right. I don't know how to prove it (by induction, maybe, but the formula looks too complicated for induction to me) and I'd also like to know (if there is any) the name of this equality. I found it when looking at the probability of a coin (2 sides) and I took the mean of all the frequencies weighted by their amount of occurence. Could have been a dice (6 sides) so I feel like this formula could exist for 1/6 = .... That's why I am looking for the generic name of it ! Thank you in advance for your help in this problem",\frac{\sum_{k=0}^n \binom{n}{k}(k/n)}{2^n} = 1/2,"['probability', 'binomial-coefficients']"
72,Expected value of steps taken to hit +1 on a 1D integer random walk given you start from zero?,Expected value of steps taken to hit +1 on a 1D integer random walk given you start from zero?,,"I am trying to calculate the expected number of steps taken on a 1-dimensional random walk ( starting from zero ) to reach +1. So far my approach has been to use recursive expectation ( first step analysis ) technique but I end up creating infinite equations because there are infinite states ( since boundary is not closed and there is chance, albeit small that we go to negative infinity before coming to +1 ). By intuition ( and simulation results below ) I feel that the answer may be infinite number of steps ( for the expected value of steps taken to reach +1, starting from zero ) but I am not able to come up with a mathematical solution to it. Can someone please help me find the solution / answer to this question? Aside: I ran a simulation on my computer for this process 10,000 times ( assuming law of large numbers will help me get an answer close to theoretical average ). The simulation took roughly 2 hrs to run. Here are some few observations - You reach +1 only in odd # of steps 50.6% of times you reach +1 in 1 step ( law of large numbers in action ) Average # of steps taken to reach +1: 101,050 steps Max. steps taken ( extreme case ) to reach +1: 951,391,959 steps Distribution of steps taken is concentrated towards lower # of steps with few extreme outliers heavily skewing the mean.","I am trying to calculate the expected number of steps taken on a 1-dimensional random walk ( starting from zero ) to reach +1. So far my approach has been to use recursive expectation ( first step analysis ) technique but I end up creating infinite equations because there are infinite states ( since boundary is not closed and there is chance, albeit small that we go to negative infinity before coming to +1 ). By intuition ( and simulation results below ) I feel that the answer may be infinite number of steps ( for the expected value of steps taken to reach +1, starting from zero ) but I am not able to come up with a mathematical solution to it. Can someone please help me find the solution / answer to this question? Aside: I ran a simulation on my computer for this process 10,000 times ( assuming law of large numbers will help me get an answer close to theoretical average ). The simulation took roughly 2 hrs to run. Here are some few observations - You reach +1 only in odd # of steps 50.6% of times you reach +1 in 1 step ( law of large numbers in action ) Average # of steps taken to reach +1: 101,050 steps Max. steps taken ( extreme case ) to reach +1: 951,391,959 steps Distribution of steps taken is concentrated towards lower # of steps with few extreme outliers heavily skewing the mean.",,"['probability', 'expected-value', 'markov-process', 'random-walk']"
73,Independence of $A$ and $B$ implies the independence of $\neg A$ and $B$,Independence of  and  implies the independence of  and,A B \neg A B,Does the following apply? $$P(A\mid B)=P(A)\implies P(\neg A\mid B)=P(\neg A)$$ My rough answer is that suppose $A$ is the probability of rainy and $B$ is the probability of toothache. Then both $P(A\mid B)=P(A)$ and $P(\neg A\mid B)=P(\neg A)$ apply. But can we prove this mathematically?,Does the following apply? My rough answer is that suppose is the probability of rainy and is the probability of toothache. Then both and apply. But can we prove this mathematically?,P(A\mid B)=P(A)\implies P(\neg A\mid B)=P(\neg A) A B P(A\mid B)=P(A) P(\neg A\mid B)=P(\neg A),"['probability', 'independence']"
74,"x amount of people owned a goat, y amount of people owned a camel, z amount of people had one animal or the other but not both","x amount of people owned a goat, y amount of people owned a camel, z amount of people had one animal or the other but not both",,"In a survey, people were asked if they owned a goat or a camel. One person in fifteen said they had a goat. One person in eighteen said they had a camel and a tenth of the people had one animal or the other but not both. What proportion of the people owned neither kind of animal? Proportion of people who had a goat = 1/15 Proportion of people who had a camel = 1/18 Proportion of people who had one animal or the other = 1/10 Proportion of people who owned neither kind of animal = 1 - 1/15 - 1/18 - 1/10 = 7/9 I'm told however that the answer is actually, 8/9. Any ideas on where my understanding is breaking down?","In a survey, people were asked if they owned a goat or a camel. One person in fifteen said they had a goat. One person in eighteen said they had a camel and a tenth of the people had one animal or the other but not both. What proportion of the people owned neither kind of animal? Proportion of people who had a goat = 1/15 Proportion of people who had a camel = 1/18 Proportion of people who had one animal or the other = 1/10 Proportion of people who owned neither kind of animal = 1 - 1/15 - 1/18 - 1/10 = 7/9 I'm told however that the answer is actually, 8/9. Any ideas on where my understanding is breaking down?",,"['probability', 'statistics']"
75,Why is $\frac{n}{2(n+1)^2}\leq\frac{1}{4}$?,Why is ?,\frac{n}{2(n+1)^2}\leq\frac{1}{4},"I have the following exercise in my textbook and I'm not completely sure about one thing in the answer: Denote $\mathbb{R}_{+}=(0,\infty)$ . Consider the probability space $\mathbb{R}_+,\mathcal{B}(\mathbb{R}_+),P)$ where $P$ is the exponentiat distribution $$dP(x)=e^{-x}\mathrm{dx}$$ Consider the random variables $$f_n(x)\exp\left(\frac{n}{2(n+1)^2)}x^{1/n}\right)$$ Where $n\in\mathbb{N}$ . Using the dominated convergence theorem, prove that the limit $$\lim_{n\to\infty}E(f_n)$$ exists and find it. The answer to the question is the following: We have $$\lim+{n\to\infty}\frac{n}{2(n+1)^2}=0$$ and for each $x\in\mathbb{R}_+$ , $$\lim_{n\to\infty}x^{1/n}=x^0=1$$ Therefore, for each $x\in\mathbb{R}_+$ , $$\lim_{n\to\infty}f_n(x)=f(x),$$ Where $f(x)=x.$ For $x\in\mathbb{R}_+$ , we have $$x^{1/n}\leq \text{max}\{1,x\}\leq 1+x$$ and $$\frac{n}{2(n+1)^2}\leq\frac{1}{4}$$ The answer continues further and I understand all the logic after it. However i'm not sure why is the last inequality true. It seems to me that the maximum the LHS can attain is $\frac{1}{8}$ , for $n=1$ , since $$\frac{1}{2(4)}=\frac{1}{8}$$ Why would the author put the bound at $\frac{1}{4}$ ?","I have the following exercise in my textbook and I'm not completely sure about one thing in the answer: Denote . Consider the probability space where is the exponentiat distribution Consider the random variables Where . Using the dominated convergence theorem, prove that the limit exists and find it. The answer to the question is the following: We have and for each , Therefore, for each , Where For , we have and The answer continues further and I understand all the logic after it. However i'm not sure why is the last inequality true. It seems to me that the maximum the LHS can attain is , for , since Why would the author put the bound at ?","\mathbb{R}_{+}=(0,\infty) \mathbb{R}_+,\mathcal{B}(\mathbb{R}_+),P) P dP(x)=e^{-x}\mathrm{dx} f_n(x)\exp\left(\frac{n}{2(n+1)^2)}x^{1/n}\right) n\in\mathbb{N} \lim_{n\to\infty}E(f_n) \lim+{n\to\infty}\frac{n}{2(n+1)^2}=0 x\in\mathbb{R}_+ \lim_{n\to\infty}x^{1/n}=x^0=1 x\in\mathbb{R}_+ \lim_{n\to\infty}f_n(x)=f(x), f(x)=x. x\in\mathbb{R}_+ x^{1/n}\leq \text{max}\{1,x\}\leq 1+x \frac{n}{2(n+1)^2}\leq\frac{1}{4} \frac{1}{8} n=1 \frac{1}{2(4)}=\frac{1}{8} \frac{1}{4}","['real-analysis', 'probability', 'analysis', 'inequality']"
76,Expectation of X choose k,Expectation of X choose k,,"Let $X$ be a random variable, assume that we know $\mathbb{E}\left[ X \choose k \right] $ . Can I assume $$ \mathbb{E}\left[ X \choose k \right] =  {\mathbb{E}\left[  X \right]\choose k}$$ as k is not a random variable?","Let be a random variable, assume that we know . Can I assume as k is not a random variable?",X \mathbb{E}\left[ X \choose k \right]   \mathbb{E}\left[ X \choose k \right] =  {\mathbb{E}\left[  X \right]\choose k},"['probability', 'binomial-coefficients', 'expected-value']"
77,Expectation of sample variance,Expectation of sample variance,,"Let $s^2$ be sample variance, $\sigma^2$ be population variance $E[\frac{(n-1)s^2}{\sigma^2}] = E[\chi^2_{n-1}] = (n-1) \implies \frac{(n-1)E[s^2]}{\sigma^2} = n-1 \implies E[s^2]=\sigma^2$ But if i do in followng way, i am getting wrong answer $E[s^2]=E[\frac{1}{n-1} \sum_{i=1}^n (x_i-\bar x)^2] \\ = \frac{1}{n-1} \sum_i (E[x_i^2] + E(\bar x^2) - 2E[\bar x]E[x_i])$ I know that $E[\bar x] = \mu$ , population mean $E[x_i^2] = var(x_i)+E[x_i]^2 = \sigma^2+\mu^2 \\ E[\bar x^2] = var(\bar x)+E[\bar x]^2 = \frac{\sigma^2}{n}+\mu^2$ Plugging these in above boldface equation $E[s^2] = \frac{1}{n-1} (n(\sigma^2+\mu^2)+n(\frac{\sigma^2}{n}+\mu^2)-2\mu^2) \\ \frac{n+1}{n-1} \sigma^2$ I know this is wrong. But i donot know where i made the mistake.","Let be sample variance, be population variance But if i do in followng way, i am getting wrong answer I know that , population mean Plugging these in above boldface equation I know this is wrong. But i donot know where i made the mistake.",s^2 \sigma^2 E[\frac{(n-1)s^2}{\sigma^2}] = E[\chi^2_{n-1}] = (n-1) \implies \frac{(n-1)E[s^2]}{\sigma^2} = n-1 \implies E[s^2]=\sigma^2 E[s^2]=E[\frac{1}{n-1} \sum_{i=1}^n (x_i-\bar x)^2] \\ = \frac{1}{n-1} \sum_i (E[x_i^2] + E(\bar x^2) - 2E[\bar x]E[x_i]) E[\bar x] = \mu E[x_i^2] = var(x_i)+E[x_i]^2 = \sigma^2+\mu^2 \\ E[\bar x^2] = var(\bar x)+E[\bar x]^2 = \frac{\sigma^2}{n}+\mu^2 E[s^2] = \frac{1}{n-1} (n(\sigma^2+\mu^2)+n(\frac{\sigma^2}{n}+\mu^2)-2\mu^2) \\ \frac{n+1}{n-1} \sigma^2,"['probability', 'statistics', 'expected-value', 'self-learning', 'variance']"
78,What is the maximum possible value of $E[X_1 X_2 X_3]$?,What is the maximum possible value of ?,E[X_1 X_2 X_3],"Assume $X_1,X_2,X_3$ are discrete random varibles defined on a common probability space $\Omega$ and taking values in $\{-1,1\}$ . Further, assume that $E[X_1]=E[X_2]=E[X_3]=E[X_1 X_2]=E[X_2 X_3]=E[X_3 X_1]=0$ . Given this, what is the maximum possible value of $E[X_1 X_2 X_3]$ ? It's easy to see that $P(X_i=\pm 1)=P(X_i X_j = \pm 1)={1 \over 2}$ for each $i,j \in I_3 (i \neq j)$ . But how do I progress further? Any help would be appreciated.","Assume are discrete random varibles defined on a common probability space and taking values in . Further, assume that . Given this, what is the maximum possible value of ? It's easy to see that for each . But how do I progress further? Any help would be appreciated.","X_1,X_2,X_3 \Omega \{-1,1\} E[X_1]=E[X_2]=E[X_3]=E[X_1 X_2]=E[X_2 X_3]=E[X_3 X_1]=0 E[X_1 X_2 X_3] P(X_i=\pm 1)=P(X_i X_j = \pm 1)={1 \over 2} i,j \in I_3 (i \neq j)","['probability', 'inequality', 'random-variables', 'expected-value']"
79,The expected number of heads,The expected number of heads,,"Given $10$ fair coins: In the first round, we toss each coin once which gives us a combination of heads and tails. In the second round, we only toss those coins that landed on the tail in the first round. What is the expected number of heads after this experiment $?$ Intuition tells me is $5 + 2.5 = 7.5$ .","Given fair coins: In the first round, we toss each coin once which gives us a combination of heads and tails. In the second round, we only toss those coins that landed on the tail in the first round. What is the expected number of heads after this experiment Intuition tells me is .",10 ? 5 + 2.5 = 7.5,"['probability', 'statistics', 'random-variables']"
80,How to prove the ‘covariance inequality’ for discrete random variables?,How to prove the ‘covariance inequality’ for discrete random variables?,,"I’m trying to prove the following ‘covariance inequality’ $$ |\operatorname{Cov}(x,y)|\le\sqrt{\operatorname{Var}(x)}\sqrt{\operatorname{Var}(y)}\,, $$ where covariance and variance are defined using discrete values, \begin{align} \operatorname{Cov}(x,y) &= \frac{1}{n-1}\sum_{i=1}^n \big[(x_i-\bar x)(y_i-\bar y)\big]\,, \\ \operatorname{Var}(x) &= \frac{\displaystyle\sum_{i=1}^n(x_i-\bar x)^2}{n-1}\,, \\ \operatorname{Var}(y) &= \dfrac{\displaystyle\sum_{i=1}^n(y_i-\bar y)^2}{n-1}\,. \end{align} There are plenty of proofs of to be found online (such as this one ). However, they all either seem to be for continuous random variables, or just refer me to the Cauchy-Schwarz inequality, which I am aware of, but not sure how to apply to this particular proof. Basically, I am wondering if there is a way to prove this inequality using those above definitions. I’ve tried substituting these definitions into the inequality above, but after expanding these summations and getting rid of the $1/(n-1)$ on both sides, I’m left with a mess (as you can imagine) with summation terms on both sides, some in the absolute value, and some in the square root. I’m not sure if there’s some algebraic mistake I’m making, some summation property I’m missing, or if substitution is just the wrong way to go about this proof.","I’m trying to prove the following ‘covariance inequality’ where covariance and variance are defined using discrete values, There are plenty of proofs of to be found online (such as this one ). However, they all either seem to be for continuous random variables, or just refer me to the Cauchy-Schwarz inequality, which I am aware of, but not sure how to apply to this particular proof. Basically, I am wondering if there is a way to prove this inequality using those above definitions. I’ve tried substituting these definitions into the inequality above, but after expanding these summations and getting rid of the on both sides, I’m left with a mess (as you can imagine) with summation terms on both sides, some in the absolute value, and some in the square root. I’m not sure if there’s some algebraic mistake I’m making, some summation property I’m missing, or if substitution is just the wrong way to go about this proof.","
|\operatorname{Cov}(x,y)|\le\sqrt{\operatorname{Var}(x)}\sqrt{\operatorname{Var}(y)}\,,
 \begin{align}
\operatorname{Cov}(x,y) &= \frac{1}{n-1}\sum_{i=1}^n \big[(x_i-\bar x)(y_i-\bar y)\big]\,, \\
\operatorname{Var}(x) &= \frac{\displaystyle\sum_{i=1}^n(x_i-\bar x)^2}{n-1}\,, \\
\operatorname{Var}(y) &= \dfrac{\displaystyle\sum_{i=1}^n(y_i-\bar y)^2}{n-1}\,.
\end{align} 1/(n-1)","['probability', 'inequality', 'variance', 'covariance', 'cauchy-schwarz-inequality']"
81,Probability of selecting an even number from the set of natural numbers,Probability of selecting an even number from the set of natural numbers,,"In the above problem, if I form the sample space as the following set: $$S= \{\text{even number, odd number}\}.$$ Obviously both events are equally likely, since there is no reason to prefer one over the other. Also, this is the set of all possible outcomes of the experiment in which a number is being randomly selected from the set of natural numbers (note that an experiment can have more than one sample spaces). So why is $P(\text{selecting an even number})$ not as simple as $$\frac{1}{2}\cdot \frac{\text{number of favourable outcomes from the sample space}}{\text{total outcomes in sample space}}?$$","In the above problem, if I form the sample space as the following set: Obviously both events are equally likely, since there is no reason to prefer one over the other. Also, this is the set of all possible outcomes of the experiment in which a number is being randomly selected from the set of natural numbers (note that an experiment can have more than one sample spaces). So why is not as simple as","S= \{\text{even number, odd number}\}. P(\text{selecting an even number}) \frac{1}{2}\cdot \frac{\text{number of favourable outcomes from the sample space}}{\text{total outcomes in sample space}}?","['probability', 'probability-theory', 'probability-distributions']"
82,Probability of selecting three even number in A.P.(Arithmetic Progression) among first 100 Natural number,Probability of selecting three even number in A.P.(Arithmetic Progression) among first 100 Natural number,,"From first 100 natural numbers, 3 numbers are selected. If these three numbers are in A.P., then find the probability that these numbers are even. My approach is as follow, selection of three number from first 100 natural numbers is $^{100}C_3$ which is equal to 161700. Let the third term be represented by the series $a+2d=T$ where T is less than 100, where a and d are Natural Numbers $a+2d \le100$ As the numbers are even a=2c and d=2e $c+2e \le50$ $c\le50-2e$ e has values from 1 to 24 Total number of cases are $2*(1+2+3+..+24)=600$ Series can be reversed also hence number of cases are 1200, but the answer is $\frac{1}{66}$ , which means that the number of cases are 1225 where the five cases are missing","From first 100 natural numbers, 3 numbers are selected. If these three numbers are in A.P., then find the probability that these numbers are even. My approach is as follow, selection of three number from first 100 natural numbers is which is equal to 161700. Let the third term be represented by the series where T is less than 100, where a and d are Natural Numbers As the numbers are even a=2c and d=2e e has values from 1 to 24 Total number of cases are Series can be reversed also hence number of cases are 1200, but the answer is , which means that the number of cases are 1225 where the five cases are missing",^{100}C_3 a+2d=T a+2d \le100 c+2e \le50 c\le50-2e 2*(1+2+3+..+24)=600 \frac{1}{66},"['probability', 'combinatorics']"
83,"I need a little help with probability, specifically involving 3 or more people.","I need a little help with probability, specifically involving 3 or more people.",,"This is my question that I am trying to conquer: A, B, and C draws a card in that order from a well-shuffled pack of 52 cards. The first to draw a diamond wins ₽740. If 'A' starts, find their mean and the variance. Now frankly, my probability-assessing skills are pretty weak, so I am unable to figure out a solution to this. I thought it'll require a binomial distribution, only to notice that the information isn't enough. What method should I employ and why? I did try searching for questions of a similar kind but to no avail. I understand that when A draws, he draws with a probability of $\frac{13}{52}$ . Do B and C also draw with the same probability? Or am I even starting out incorrectly?","This is my question that I am trying to conquer: A, B, and C draws a card in that order from a well-shuffled pack of 52 cards. The first to draw a diamond wins ₽740. If 'A' starts, find their mean and the variance. Now frankly, my probability-assessing skills are pretty weak, so I am unable to figure out a solution to this. I thought it'll require a binomial distribution, only to notice that the information isn't enough. What method should I employ and why? I did try searching for questions of a similar kind but to no avail. I understand that when A draws, he draws with a probability of . Do B and C also draw with the same probability? Or am I even starting out incorrectly?",\frac{13}{52},"['probability', 'probability-theory', 'probability-distributions', 'conditional-probability']"
84,What is the probability that an unfair coin's head appears less than 50 after 100 tosses?,What is the probability that an unfair coin's head appears less than 50 after 100 tosses?,,"I met a question about probability, it seems easy but I got stuck. The question is: Suppose there is an unfair coin, the HEAD probability is $p=0.7$ . (Q1) If we toss the coin for 100 times, what is the expectation and the variance of this experiment? (Q2) Answer with reason whether or not the probability is higher than $1/10$ that the number of HEAD appear times is less than $50$ as we toss the coin for $100$ times. Q1 is easy, I know expectation is $n*p=70$ and variance is $n*p*(1-p)=21$ . But for Q2 I have no idea. At first I think it looks like... a sample distribution of sample mean used in statistics but... I don't know whether (or how) it will obey a normal distribution. Then I also try to calculate the sum of $P(H=0)+P(H=1)+...+P(H=50)$ , but the work is huge, even I use an approximation of Passion distribution... So could you share some of your thought? Thank you!","I met a question about probability, it seems easy but I got stuck. The question is: Suppose there is an unfair coin, the HEAD probability is . (Q1) If we toss the coin for 100 times, what is the expectation and the variance of this experiment? (Q2) Answer with reason whether or not the probability is higher than that the number of HEAD appear times is less than as we toss the coin for times. Q1 is easy, I know expectation is and variance is . But for Q2 I have no idea. At first I think it looks like... a sample distribution of sample mean used in statistics but... I don't know whether (or how) it will obey a normal distribution. Then I also try to calculate the sum of , but the work is huge, even I use an approximation of Passion distribution... So could you share some of your thought? Thank you!",p=0.7 1/10 50 100 n*p=70 n*p*(1-p)=21 P(H=0)+P(H=1)+...+P(H=50),['probability']
85,Notation regarding random variables,Notation regarding random variables,,"Let X be a random variable. Consider X ~ F. It can be read as X has distribution F. What is distribution referring here? Consider the following interpretations 1) If X is continuous, then F is a probability density function and if X is discrete then F is a probability mass function. 2) F is a cumulative distribution function. Which of the above is correct? If not, what is the distribution the notation referring to?","Let X be a random variable. Consider X ~ F. It can be read as X has distribution F. What is distribution referring here? Consider the following interpretations 1) If X is continuous, then F is a probability density function and if X is discrete then F is a probability mass function. 2) F is a cumulative distribution function. Which of the above is correct? If not, what is the distribution the notation referring to?",,"['notation', 'random-variables', 'elementary-probability']"
86,How to derive this relation that I found intuitively?,How to derive this relation that I found intuitively?,,"By intuition, I found that the result of evaluating the following expression $$ \frac{1}{M} \frac{\sum_{N=0}^M \frac{M!}{(M-N)!N!} N e^{cN}}{\sum_{N=0}^M \frac{M!}{(M-N)!N!}  e^{cN}}  $$ does not depend on the positive value of the integer $M$ , i.e. it only depends on $c\in\mathbb R$ . I corroborated this with the help of a simple Python script. How to show  analytically that this is true?","By intuition, I found that the result of evaluating the following expression does not depend on the positive value of the integer , i.e. it only depends on . I corroborated this with the help of a simple Python script. How to show  analytically that this is true?", \frac{1}{M} \frac{\sum_{N=0}^M \frac{M!}{(M-N)!N!} N e^{cN}}{\sum_{N=0}^M \frac{M!}{(M-N)!N!}  e^{cN}}   M c\in\mathbb R,"['probability', 'problem-solving']"
87,A box has $4$ red and $20$ white balls. A person takes $10$ balls. What is the probability that all or none of the red balls were taken?,A box has  red and  white balls. A person takes  balls. What is the probability that all or none of the red balls were taken?,4 20 10,"A box has $24$ balls, $4$ red and $20$ white. One person takes $10$ balls and the second the remaining $14$ . What is the probability that one of the two people picked up the $4$ red ones? I don't understand why is this correct. $$\frac{{20 \choose 6}+{20 \choose 10}}{24 \choose 10} $$","A box has balls, red and white. One person takes balls and the second the remaining . What is the probability that one of the two people picked up the red ones? I don't understand why is this correct.",24 4 20 10 14 4 \frac{{20 \choose 6}+{20 \choose 10}}{24 \choose 10} ,"['probability', 'combinatorics']"
88,Die is rolled until 1 appears. What is the probability of rolling it odd number of times?,Die is rolled until 1 appears. What is the probability of rolling it odd number of times?,,"Problem : Die is rolled until 1 appears. What is the probability of rolling it odd number of times? So, so far I have this: $\frac{1}{6}$ - this is a probability of rolling ""1"" on first try $\frac{5}{6} \cdot \frac{5}{6} \cdot \frac{1}{6}$ - three tries (two times something else than ""1"" and ""1"" on the 3rd try $\frac{5}{6} \cdot \frac{5}{6} \cdot \frac{5}{6} \cdot \frac{5}{6} \cdot \frac{1}{6}$ - five tries and so on... By the looks of it, it seems I could come up with following formula $$ p = \left( \frac{5}{6}\right)^{n-1} \cdot \frac{1}{6}$$ where p would be a probability for nth try. However, I cannot think of any way to get probability of all odd number of tries and not sure if I am even on right track here.","Problem : Die is rolled until 1 appears. What is the probability of rolling it odd number of times? So, so far I have this: - this is a probability of rolling ""1"" on first try - three tries (two times something else than ""1"" and ""1"" on the 3rd try - five tries and so on... By the looks of it, it seems I could come up with following formula where p would be a probability for nth try. However, I cannot think of any way to get probability of all odd number of tries and not sure if I am even on right track here.","\frac{1}{6} \frac{5}{6} \cdot \frac{5}{6} \cdot \frac{1}{6} \frac{5}{6} \cdot \frac{5}{6} \cdot \frac{5}{6} \cdot \frac{5}{6} \cdot
\frac{1}{6}  p = \left( \frac{5}{6}\right)^{n-1} \cdot \frac{1}{6}",['probability']
89,How many persons eat at least two out of the three dishes?,How many persons eat at least two out of the three dishes?,,"Out of a group of 21 persons, 9 eat vegetables, 10 eat fish and 7 eat eggs. 5 persons eat all three. How many persons eat at least two out of the three dishes? My take : Let $A∩B∩C = x$ , then $(A∩B+B∩C+A∩C)$ , this already contains $3x$ . therefore subtracting $2x$ from this should result into POSITIVE value, but it is zero.  Moreover, they are asking for ""at least 2"" which means $(A∩B+B∩C+A∩C) - 2x$ . Is something wrong with given data ? The answer given in book is any number between [5,11]. Please help me understand & solve this question. Any help is appreciated in advance.","Out of a group of 21 persons, 9 eat vegetables, 10 eat fish and 7 eat eggs. 5 persons eat all three. How many persons eat at least two out of the three dishes? My take : Let , then , this already contains . therefore subtracting from this should result into POSITIVE value, but it is zero.  Moreover, they are asking for ""at least 2"" which means . Is something wrong with given data ? The answer given in book is any number between [5,11]. Please help me understand & solve this question. Any help is appreciated in advance.",A∩B∩C = x (A∩B+B∩C+A∩C) 3x 2x (A∩B+B∩C+A∩C) - 2x,"['probability', 'combinatorics', 'elementary-set-theory']"
90,Probability that the sum of two integers is even between 20 and 40,Probability that the sum of two integers is even between 20 and 40,,"Question: What is the probability that the sum of two randomly chosen integers between $20$ and $40$ inclusive is even (the possibility of the two integers being equal is allowed)? Comment on the answers (a) $\frac{1}{2}$, (b) $\frac{11}{21}$, and (c) $\frac{11^2}{21^2}$ My attempt so far: I know there are $11$ even numbers between $20$ and $40$ and $10$ odd numbers between $20$ and $40$. So with even numbers, there is a total of $121$ options and $100$ options between odd numbers. https://answers.yahoo.com/question/index?qid=20130608150439AA7kjc3 (This is the link I am working off of to understand the problem). I understand where the $221$ comes from...where/what is the $441$?","Question: What is the probability that the sum of two randomly chosen integers between $20$ and $40$ inclusive is even (the possibility of the two integers being equal is allowed)? Comment on the answers (a) $\frac{1}{2}$, (b) $\frac{11}{21}$, and (c) $\frac{11^2}{21^2}$ My attempt so far: I know there are $11$ even numbers between $20$ and $40$ and $10$ odd numbers between $20$ and $40$. So with even numbers, there is a total of $121$ options and $100$ options between odd numbers. https://answers.yahoo.com/question/index?qid=20130608150439AA7kjc3 (This is the link I am working off of to understand the problem). I understand where the $221$ comes from...where/what is the $441$?",,"['probability', 'combinatorics']"
91,Simulating a fair die with a 5-card hand,Simulating a fair die with a 5-card hand,,"This question is inspired by an earlier post: Card game and dice rolling: Finding random variables with similar probability mass function I want to simulate a fair 6-sided die using 5-card hands from a regular deck of 52 cards.  Cards are drawn without replacement, so there are ${52 \choose 5}$ possible hands, and since this number is divisible by 6, it is possible to partition the ${52 \choose 5}$ possible hands into 6 equal-sized disjoint subsets $A_1 \cup A_2 \cup A_3 \cup A_4 \cup A_5 \cup A_6$.  Then, when I draw a 5-card hand, if it belongs to $A_j$ then I pretend I rolled the number $j$, and this process simulates a fair 6-sided die.  This is entirely possible, but my question is: What is the easiest / most elegant / simplest way to do this? If the order of drawing the 5 cards can be used (which is the question asked by the earlier post), then an easy / elegant / simple answer is: $A_1 = $ the first non-King you draw is Ace or 2, $A_2 =$ the first non-King you draw is 3 or 4, etc., up to $A_6=$ the first non-King you draw is Jack or Queen.  These sets are clearly of equal size and disjoint, and since you draw 5 cards you must draw a non-King, so their union also cover all $52 \times 51 \times 50 \times 49 \times 48$ possible draws.  However, this method requires that you distinguish based on the order of drawing those 5 cards. My question in this post requires that you do not distinguish based on the order of draw. @RossMillikan and I had a brief discussion of several attempted solutions in the comments of the earlier post, but now that I have more time to think, I think none of the solutions actually work.  Here are the (IMHO wrong) attempts: 1st Attempt: Ignore Kings, assign Ace = 1; 2 through 10 = face value; J = 11; Q = 12.  Add the 5 card values mod 6. My thoughts: This would have obviously worked if you were drawing with replacement (and can somehow ""magically"" avoid the 5-King draw), since every card is uniformly distributed mod 6.  But because you are drawing without replacement, if your first card is e.g. a 4, then the next card is no longer randomly distributed mod 6 because you have one fewer 4.  Maybe this still simulates a fair die, but I don't see an obvious proof. 2nd Attempt: Again ignore kings (until the end). If you have distinct ranks, add them mod 6. If you have at least one pair, take the rank with maximum quantity (most repeated occurrences) mod 6. This handles all draws except $xxyyz$ and $Kxxyy$. For the first, take $z$ mod 6. For the second, if the king is red take the higher rank of $x$ or $y$, if the king is black take the lower. My thoughts: This would obviously work if each subcase is partitioned evenly into the 6 subsets.  E.g. all hands of type $xxyyz$ certainly seems to be partitioned evenly.  However consider hands of type $KKKxy$ (3 Kings, 2 other different ranks).  Because we're constrained by $x \neq y$ (in this subcase), $Prob(x+y \ is\ odd) = 6/11$.  So clearly this subcase is not partitioned evenly into the 6 subsets.  Again, the overall scheme might still simulate a fair die, but I don't see an obvious proof. To summarize: the question is not whether this partition is possible - it is, simply because ${52 \choose 5}$ is divisible by 6.  The question is what is an easy / elegant / simple way to do this, i.e. to describe the partition. (Bonus if your solution generalizes to any $N$-sided die, where $N$ divides ${52 \choose 5}$.)","This question is inspired by an earlier post: Card game and dice rolling: Finding random variables with similar probability mass function I want to simulate a fair 6-sided die using 5-card hands from a regular deck of 52 cards.  Cards are drawn without replacement, so there are ${52 \choose 5}$ possible hands, and since this number is divisible by 6, it is possible to partition the ${52 \choose 5}$ possible hands into 6 equal-sized disjoint subsets $A_1 \cup A_2 \cup A_3 \cup A_4 \cup A_5 \cup A_6$.  Then, when I draw a 5-card hand, if it belongs to $A_j$ then I pretend I rolled the number $j$, and this process simulates a fair 6-sided die.  This is entirely possible, but my question is: What is the easiest / most elegant / simplest way to do this? If the order of drawing the 5 cards can be used (which is the question asked by the earlier post), then an easy / elegant / simple answer is: $A_1 = $ the first non-King you draw is Ace or 2, $A_2 =$ the first non-King you draw is 3 or 4, etc., up to $A_6=$ the first non-King you draw is Jack or Queen.  These sets are clearly of equal size and disjoint, and since you draw 5 cards you must draw a non-King, so their union also cover all $52 \times 51 \times 50 \times 49 \times 48$ possible draws.  However, this method requires that you distinguish based on the order of drawing those 5 cards. My question in this post requires that you do not distinguish based on the order of draw. @RossMillikan and I had a brief discussion of several attempted solutions in the comments of the earlier post, but now that I have more time to think, I think none of the solutions actually work.  Here are the (IMHO wrong) attempts: 1st Attempt: Ignore Kings, assign Ace = 1; 2 through 10 = face value; J = 11; Q = 12.  Add the 5 card values mod 6. My thoughts: This would have obviously worked if you were drawing with replacement (and can somehow ""magically"" avoid the 5-King draw), since every card is uniformly distributed mod 6.  But because you are drawing without replacement, if your first card is e.g. a 4, then the next card is no longer randomly distributed mod 6 because you have one fewer 4.  Maybe this still simulates a fair die, but I don't see an obvious proof. 2nd Attempt: Again ignore kings (until the end). If you have distinct ranks, add them mod 6. If you have at least one pair, take the rank with maximum quantity (most repeated occurrences) mod 6. This handles all draws except $xxyyz$ and $Kxxyy$. For the first, take $z$ mod 6. For the second, if the king is red take the higher rank of $x$ or $y$, if the king is black take the lower. My thoughts: This would obviously work if each subcase is partitioned evenly into the 6 subsets.  E.g. all hands of type $xxyyz$ certainly seems to be partitioned evenly.  However consider hands of type $KKKxy$ (3 Kings, 2 other different ranks).  Because we're constrained by $x \neq y$ (in this subcase), $Prob(x+y \ is\ odd) = 6/11$.  So clearly this subcase is not partitioned evenly into the 6 subsets.  Again, the overall scheme might still simulate a fair die, but I don't see an obvious proof. To summarize: the question is not whether this partition is possible - it is, simply because ${52 \choose 5}$ is divisible by 6.  The question is what is an easy / elegant / simple way to do this, i.e. to describe the partition. (Bonus if your solution generalizes to any $N$-sided die, where $N$ divides ${52 \choose 5}$.)",,"['probability', 'combinatorics', 'card-games']"
92,Intuitively understanding the purpose of Bayes' theorem for a $3/4$-probability truth-teller [duplicate],Intuitively understanding the purpose of Bayes' theorem for a -probability truth-teller [duplicate],3/4,This question already has answers here : A man who lies a fourth of the time throws a die and says it is a six. What is the probability it is actually a six? (2 answers) Closed 6 years ago . The problem states: The probability of a man telling the truth is = $3/4$. He rolls a dice and claims he got a six. What is the probability that he actually got a six? And my textbook solves this using Bayes' theorem and gets $3/8$ as the answer. The way I'm looking at it is: The probability that he really got what he's claiming (a six) should be equal to the probability of him telling the truth which = $3/4$. Where am I going wrong here?,This question already has answers here : A man who lies a fourth of the time throws a die and says it is a six. What is the probability it is actually a six? (2 answers) Closed 6 years ago . The problem states: The probability of a man telling the truth is = $3/4$. He rolls a dice and claims he got a six. What is the probability that he actually got a six? And my textbook solves this using Bayes' theorem and gets $3/8$ as the answer. The way I'm looking at it is: The probability that he really got what he's claiming (a six) should be equal to the probability of him telling the truth which = $3/4$. Where am I going wrong here?,,"['probability', 'bayes-theorem']"
93,"A box contains a penny, two nickels, and a dime. If two coins are selected randomly from the box, without replacement, and if X is the sum...","A box contains a penny, two nickels, and a dime. If two coins are selected randomly from the box, without replacement, and if X is the sum...",,"A box contains a penny (1¢), two nickels (5¢), and a dime (10¢). If two coins are selected randomly from the box, without replacement, and if $X$ is the sum of the values of the two coins, What is the probability distribution table of $X$? $$\begin{array}{|c|c|c|c|c|}\hline X & 6¢ & 10¢ & 11¢ & 15¢ \\ \hline f(x) & 2/6 & 1/6 & 1/6 & 2/6\\\hline\end{array}$$ What is the cumulative distribution function $F(x)$ of $X$? The cumulative distribution function, $F(x)$ of $X$ is defined as:   $F(x) = P(X ≤ x)$ So would that mean I just write: $P(X ≤ 6) = 2/6$ $P(X ≤ 10) = 1/6$ $P(X ≤ 11) = 1/6$ $P(X ≤ 15) = 2/6$ \begin{align*} P(X \leq 6) & = P(X = 6)=2/6\\ P(X \leq 10) & = P(X = 6) + P(X = 10)=2/6+1/6=1/2\\ P(X \leq 11) & = P(X = 6) + P(X = 10) + P(X = 11)=1/2+1/6=2/3\\ P(X \leq 15) & = P(X = 6) + P(X = 10) + P(X = 11) + P(X = 15)=2/3+2/6=1 \end{align*}","A box contains a penny (1¢), two nickels (5¢), and a dime (10¢). If two coins are selected randomly from the box, without replacement, and if $X$ is the sum of the values of the two coins, What is the probability distribution table of $X$? $$\begin{array}{|c|c|c|c|c|}\hline X & 6¢ & 10¢ & 11¢ & 15¢ \\ \hline f(x) & 2/6 & 1/6 & 1/6 & 2/6\\\hline\end{array}$$ What is the cumulative distribution function $F(x)$ of $X$? The cumulative distribution function, $F(x)$ of $X$ is defined as:   $F(x) = P(X ≤ x)$ So would that mean I just write: $P(X ≤ 6) = 2/6$ $P(X ≤ 10) = 1/6$ $P(X ≤ 11) = 1/6$ $P(X ≤ 15) = 2/6$ \begin{align*} P(X \leq 6) & = P(X = 6)=2/6\\ P(X \leq 10) & = P(X = 6) + P(X = 10)=2/6+1/6=1/2\\ P(X \leq 11) & = P(X = 6) + P(X = 10) + P(X = 11)=1/2+1/6=2/3\\ P(X \leq 15) & = P(X = 6) + P(X = 10) + P(X = 11) + P(X = 15)=2/3+2/6=1 \end{align*}",,"['probability', 'probability-distributions']"
94,Is this game fair? Coin Toss,Is this game fair? Coin Toss,,"Suppose there is a coin toss game where quarters are thrown onto a checkerboard. Management keeps all of the quarters; however, if a quarter lands entirely within one square of the checkerboard the management pays a dollar. Assume that the edge of each square is twice the diameter of the quarter, and that outcomes are described by coordinates chosen at random. I am having a hard time starting this problem. I understand the area of the quarter and the area of the square but am not sure what to do with these. Is the probability the difference in the area of the square and the circle?","Suppose there is a coin toss game where quarters are thrown onto a checkerboard. Management keeps all of the quarters; however, if a quarter lands entirely within one square of the checkerboard the management pays a dollar. Assume that the edge of each square is twice the diameter of the quarter, and that outcomes are described by coordinates chosen at random. I am having a hard time starting this problem. I understand the area of the quarter and the area of the square but am not sure what to do with these. Is the probability the difference in the area of the square and the circle?",,['probability']
95,Expected value of a marginal distribution is a function of $x$?,Expected value of a marginal distribution is a function of ?,x,"Let $f$ be the joint PDF of the random vector $(X, Y)$. $f(x, y) = \displaystyle\frac{x(x-y)}{8}$ for $0 < x < 2$ and $-x < y < x$, otherwise it's zero. Calculate the correlation between $X$ and $Y$. The problem I'm struggling with here is that to compute the correlation, I need $E(X)$, $E(Y)$, $E(XY)$, etc. I already calculated the $E(X)$, but when trying to calculate $E(Y)$, I get: $E(Y) = \displaystyle\int_{-x}^xyf_y(y)dy$ Where $f_y(y)$ is the marginal distribution of y, which I computed as: $f_y(y) = \displaystyle\int_0^2\frac{x(x-y)}{8}dx = \frac{1}{3}-\frac{1}{4}y$ Because of the limits of integration when computing $E(Y)$, I assumed that the integral was going to be in terms of $x$, and in fact, I computed it and got $E(Y) = -\displaystyle\frac{x^3}{6}$. Why is my expected value in terms of $x$? Is this the proper way to calculate this? Shouldn't be Y a random variable of it's own?","Let $f$ be the joint PDF of the random vector $(X, Y)$. $f(x, y) = \displaystyle\frac{x(x-y)}{8}$ for $0 < x < 2$ and $-x < y < x$, otherwise it's zero. Calculate the correlation between $X$ and $Y$. The problem I'm struggling with here is that to compute the correlation, I need $E(X)$, $E(Y)$, $E(XY)$, etc. I already calculated the $E(X)$, but when trying to calculate $E(Y)$, I get: $E(Y) = \displaystyle\int_{-x}^xyf_y(y)dy$ Where $f_y(y)$ is the marginal distribution of y, which I computed as: $f_y(y) = \displaystyle\int_0^2\frac{x(x-y)}{8}dx = \frac{1}{3}-\frac{1}{4}y$ Because of the limits of integration when computing $E(Y)$, I assumed that the integral was going to be in terms of $x$, and in fact, I computed it and got $E(Y) = -\displaystyle\frac{x^3}{6}$. Why is my expected value in terms of $x$? Is this the proper way to calculate this? Shouldn't be Y a random variable of it's own?",,['probability']
96,Expected value of rolling a $5$ followed by a $6$ different than rolling two consecutive sixes?,Expected value of rolling a  followed by a  different than rolling two consecutive sixes?,5 6,"I'm aware that the $E[X_n]$ where $X$ is the average number rolls needed to roll $n$ consecutive sixes solves to $E[X_2]=42$ when $n=2$... but why does this differ from rolling other numbers (which are distinct from eachother) such as $5$ and then $6$? For example, if I wanted $X_n$ to be the average number of rolls for getting two consecutive numbers in general, which need not be the same number. If I begin at $0$ rolls and roll a die $2$ times, the probability of getting $2$ sixes on the first two rolls is $\frac{1}{6}\cdot \frac{1}{6}=\frac{1}{36}$ and the probability of rolling a $5$ and then a $6$ is the same probability. Intuitively, this led me to believe I could extrapolate the result mentioned above regarding $n$ consecutive sixes and apply it to rolling $n$ consecutive numbers (which need not be the same). But the problem I solved did not have the answer as $42$, the answer for the expected value to roll a $5$ followed immediately by a $6$ was $36$.","I'm aware that the $E[X_n]$ where $X$ is the average number rolls needed to roll $n$ consecutive sixes solves to $E[X_2]=42$ when $n=2$... but why does this differ from rolling other numbers (which are distinct from eachother) such as $5$ and then $6$? For example, if I wanted $X_n$ to be the average number of rolls for getting two consecutive numbers in general, which need not be the same number. If I begin at $0$ rolls and roll a die $2$ times, the probability of getting $2$ sixes on the first two rolls is $\frac{1}{6}\cdot \frac{1}{6}=\frac{1}{36}$ and the probability of rolling a $5$ and then a $6$ is the same probability. Intuitively, this led me to believe I could extrapolate the result mentioned above regarding $n$ consecutive sixes and apply it to rolling $n$ consecutive numbers (which need not be the same). But the problem I solved did not have the answer as $42$, the answer for the expected value to roll a $5$ followed immediately by a $6$ was $36$.",,"['probability', 'discrete-mathematics', 'expectation']"
97,6 fair coin flips: probability of exactly 3 heads,6 fair coin flips: probability of exactly 3 heads,,"When a certain coin is flipped, the probability of heads is $0.5$. If the coin is flipped $6$ times, what is the probability that there are exactly $3$ heads? The answer is $\frac5{16}$. I wonder why it isn't $\frac12$. Since a fair coin flip results in equally likely outcomes, any sequence is equally likely… I know why it is $\frac5{16}$. We divide the number of possible outcomes with exactly 3 heads by the total possible outcomes. What bothers me is how I should think about it so I won't make a mistake anymore. Why is $\frac12$ not right? I need to get intuition.","When a certain coin is flipped, the probability of heads is $0.5$. If the coin is flipped $6$ times, what is the probability that there are exactly $3$ heads? The answer is $\frac5{16}$. I wonder why it isn't $\frac12$. Since a fair coin flip results in equally likely outcomes, any sequence is equally likely… I know why it is $\frac5{16}$. We divide the number of possible outcomes with exactly 3 heads by the total possible outcomes. What bothers me is how I should think about it so I won't make a mistake anymore. Why is $\frac12$ not right? I need to get intuition.",,['probability']
98,Expected value from a game which gives random numbers from 1 to 100 with reducing payoff,Expected value from a game which gives random numbers from 1 to 100 with reducing payoff,,"I bumped into a probability problem, proposed by an app. Here it is! ""In front of you is an infinitely-lived machine that proposes amounts of money, which you can either accept or reject. If you accept, the machine hands over the proposed amount, but shuts down and will never give you anything else. If you reject, it'll show you a new proposal next period.  Each period's proposal is an independent draw from a uniform distribution on [0, 100]. The time between periods is long — several months, say — and you are impatient: a dollar next period is worth only 0.9 to you today; similarly, a dollar two periods from now is worth 0.9*0.9 = 0.81 today, et cetera.  If your strategy were to always accept, you'd expect to make 50 dollars, i.e. the mean of the first draw. If instead you decided to accept any first proposal above 50, and — in case you reject the first — any second proposal whatsoever, your expected discounted payoff would be 60 dollars. But you can do better!  If you follow the strategy that maximizes your expected discounted payoff, what is the threshold above which you should accept the machine's first proposal?"" Now, I solved others similar problems, but this time the payoff gets reducing by 10% every time I reject the amount and go on with the game. If you could enlighten me, I'd appreciate it.","I bumped into a probability problem, proposed by an app. Here it is! ""In front of you is an infinitely-lived machine that proposes amounts of money, which you can either accept or reject. If you accept, the machine hands over the proposed amount, but shuts down and will never give you anything else. If you reject, it'll show you a new proposal next period.  Each period's proposal is an independent draw from a uniform distribution on [0, 100]. The time between periods is long — several months, say — and you are impatient: a dollar next period is worth only 0.9 to you today; similarly, a dollar two periods from now is worth 0.9*0.9 = 0.81 today, et cetera.  If your strategy were to always accept, you'd expect to make 50 dollars, i.e. the mean of the first draw. If instead you decided to accept any first proposal above 50, and — in case you reject the first — any second proposal whatsoever, your expected discounted payoff would be 60 dollars. But you can do better!  If you follow the strategy that maximizes your expected discounted payoff, what is the threshold above which you should accept the machine's first proposal?"" Now, I solved others similar problems, but this time the payoff gets reducing by 10% every time I reject the amount and go on with the game. If you could enlighten me, I'd appreciate it.",,"['probability', 'combinatorics']"
99,Conditional Probability with complements,Conditional Probability with complements,,"Events A and B are such that P(A)=0.7, P(B)=0.2, and P(A∩B)=0.2. Find P(A|B'). I found out that P(A u B) = 0.7, but I'm not sure how to work out the conditional probability - I've tried using the formula and I got P(A|B') = (0.7*0.8)/0.8, but that seems wrong.","Events A and B are such that P(A)=0.7, P(B)=0.2, and P(A∩B)=0.2. Find P(A|B'). I found out that P(A u B) = 0.7, but I'm not sure how to work out the conditional probability - I've tried using the formula and I got P(A|B') = (0.7*0.8)/0.8, but that seems wrong.",,['probability']
