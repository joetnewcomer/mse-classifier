,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Solve inhomogeneous wave equation,Solve inhomogeneous wave equation,,"I need to solve this: $$  \begin{cases}   U_{tt}= \Delta U + |x|^2 \sin t \\   U(x,0) = |x|^4 + |x|^2 \\   U_t(x,0) = |x|^4 - |x|^2 \end{cases} $$ in 3 dimensions $x = \{x_1, x_2, x_3\}$","I need to solve this: $$  \begin{cases}   U_{tt}= \Delta U + |x|^2 \sin t \\   U(x,0) = |x|^4 + |x|^2 \\   U_t(x,0) = |x|^4 - |x|^2 \end{cases} $$ in 3 dimensions $x = \{x_1, x_2, x_3\}$",,"['multivariable-calculus', 'partial-differential-equations']"
1,Product of Taylor polynomials,Product of Taylor polynomials,,"I'm trying to prove the following proposition: Let $U\in R^n$ be open, and $f,g\colon U\to R$ be $C^k$ functions, then the Taylor polynomial of $fg$ is computed as $P_{f,a}^k(a+\vec{h})\cdot P_{g,a}^k(a+\vec{h})$ and discarding the terms of degree > $k$,   where $P_{f,a}^k(a+\vec{h})$ denotes the degree $k$ Taylor polynomial of $f$ at $a$. And here's what I've got so far: $$P_{fg,a}^k=\sum_{m=0}^k\sum_{I\in I_n^m}\frac{1}{I!}D_I(fg)(a)$$ using the definition of Taylor polynomial of multi-variable functions. ($I=(i_1,i_2,...,i_n)$ and $D_If=D_1^{i_1}D_2^{i_2}...D_n^{i_n}f)$. Then I think $D_I(fg)$ can be written as $D_I(f)g+fD_I(g)$ using the product rule so that $$P_{fg,a}^k(a+\vec{h})=P_{f,a}^k(a+\vec{h})\cdot g+f\cdot P_{g,a}^k(a+\vec{h})$$ but I can't figure out how this leads to the final result.","I'm trying to prove the following proposition: Let $U\in R^n$ be open, and $f,g\colon U\to R$ be $C^k$ functions, then the Taylor polynomial of $fg$ is computed as $P_{f,a}^k(a+\vec{h})\cdot P_{g,a}^k(a+\vec{h})$ and discarding the terms of degree > $k$,   where $P_{f,a}^k(a+\vec{h})$ denotes the degree $k$ Taylor polynomial of $f$ at $a$. And here's what I've got so far: $$P_{fg,a}^k=\sum_{m=0}^k\sum_{I\in I_n^m}\frac{1}{I!}D_I(fg)(a)$$ using the definition of Taylor polynomial of multi-variable functions. ($I=(i_1,i_2,...,i_n)$ and $D_If=D_1^{i_1}D_2^{i_2}...D_n^{i_n}f)$. Then I think $D_I(fg)$ can be written as $D_I(f)g+fD_I(g)$ using the product rule so that $$P_{fg,a}^k(a+\vec{h})=P_{f,a}^k(a+\vec{h})\cdot g+f\cdot P_{g,a}^k(a+\vec{h})$$ but I can't figure out how this leads to the final result.",,"['multivariable-calculus', 'taylor-expansion']"
2,Is the problem asking to show that $r\times \nabla \psi$ satisfies wave equation wrong?,Is the problem asking to show that  satisfies wave equation wrong?,r\times \nabla \psi,"Considering the wave equation in spherical coordinates, if we know that $\psi(\vec{r})$ is a solution, then $\vec{r}\times \nabla \psi$ is also a solution. (The hint is to take the difference between $\psi(r,\theta,\phi)$ and $\psi(r,\theta ',\phi ')$) If I interpreted it correctly, it says that if $\psi $ solves $$\nabla^2\psi  - \frac{1}{c^2}\frac{\partial^2 \psi}{\partial t^2} =0$$ Then show that: $$\nabla^2(\vec{r}\times \nabla\psi)  - \frac{1}{c^2}\frac{\partial^2 (\vec{r}\times \nabla\psi)}{\partial t^2} =0$$ This question seems outright wrong. As the argument of the Laplacian is a vector. Or am I misinterpreting it?","Considering the wave equation in spherical coordinates, if we know that $\psi(\vec{r})$ is a solution, then $\vec{r}\times \nabla \psi$ is also a solution. (The hint is to take the difference between $\psi(r,\theta,\phi)$ and $\psi(r,\theta ',\phi ')$) If I interpreted it correctly, it says that if $\psi $ solves $$\nabla^2\psi  - \frac{1}{c^2}\frac{\partial^2 \psi}{\partial t^2} =0$$ Then show that: $$\nabla^2(\vec{r}\times \nabla\psi)  - \frac{1}{c^2}\frac{\partial^2 (\vec{r}\times \nabla\psi)}{\partial t^2} =0$$ This question seems outright wrong. As the argument of the Laplacian is a vector. Or am I misinterpreting it?",,"['multivariable-calculus', 'partial-differential-equations', 'vector-analysis']"
3,Bound on Derivatives,Bound on Derivatives,,"Let $\phi: \mathbb{R}^n \rightarrow \mathbb{R}$ be a $C^\infty$ function with the following properties. $\phi(x) = 1$ if $|x| \leq 1$ $\phi(x) = 0$ if $|x| \geq 2$ $0 \leq \phi \leq 1$ $\phi$ is radial. Let $\phi_k(x) = \phi(\frac{x}{k})$. How do I show that for each multiindex $\alpha$, there exists a constant $C_\alpha$ such that $|D^\alpha \phi_k| \leq \frac{C_\alpha}{k^{\textrm{deg }\alpha}}$ uniformly in $k$?","Let $\phi: \mathbb{R}^n \rightarrow \mathbb{R}$ be a $C^\infty$ function with the following properties. $\phi(x) = 1$ if $|x| \leq 1$ $\phi(x) = 0$ if $|x| \geq 2$ $0 \leq \phi \leq 1$ $\phi$ is radial. Let $\phi_k(x) = \phi(\frac{x}{k})$. How do I show that for each multiindex $\alpha$, there exists a constant $C_\alpha$ such that $|D^\alpha \phi_k| \leq \frac{C_\alpha}{k^{\textrm{deg }\alpha}}$ uniformly in $k$?",,"['multivariable-calculus', 'inequality']"
4,Generating a random point on the unit circle,Generating a random point on the unit circle,,"I'm trying to figure out a way to generate a random point on the unit circle in an application I am developing (I'm a programmer). So far I have the following (in pseudo-code), where Z is a random number between 0.0 and 1.0: theta = (2.0 * PI) * Z  2DVector.x = cos(theta) 2DVector.y = sin(theta)  result: 2DVector I know that it's wrong, as I'm getting nothing but massive x values and tiny y values. But I'm not familiar enough with the unit circle mathematics to know where I'm going wrong!","I'm trying to figure out a way to generate a random point on the unit circle in an application I am developing (I'm a programmer). So far I have the following (in pseudo-code), where Z is a random number between 0.0 and 1.0: theta = (2.0 * PI) * Z  2DVector.x = cos(theta) 2DVector.y = sin(theta)  result: 2DVector I know that it's wrong, as I'm getting nothing but massive x values and tiny y values. But I'm not familiar enough with the unit circle mathematics to know where I'm going wrong!",,"['multivariable-calculus', 'circles', 'pi']"
5,How to compute Hyper-area?,How to compute Hyper-area?,,"The function $A=(\sin(y)\sin(z)+\cos(y)\cos(z))\sin(w)\sin(x)+\cos(w)\cos(x)$, given $w\in[0,\pi], x\in[0,\pi], y\in[0,2\pi], z\in[0,2\pi]$, defines a three-dimensional ""surface"" in 4D.  ($A = f(w,x,y,z)$ represent level sets). How would I calculate the hyper-area of this surface as a function of A? thank you! p.s. I don't necessarily need a closed-form solution, I'm going to evaluate the integral numerically, but I don't know what the integral should be.","The function $A=(\sin(y)\sin(z)+\cos(y)\cos(z))\sin(w)\sin(x)+\cos(w)\cos(x)$, given $w\in[0,\pi], x\in[0,\pi], y\in[0,2\pi], z\in[0,2\pi]$, defines a three-dimensional ""surface"" in 4D.  ($A = f(w,x,y,z)$ represent level sets). How would I calculate the hyper-area of this surface as a function of A? thank you! p.s. I don't necessarily need a closed-form solution, I'm going to evaluate the integral numerically, but I don't know what the integral should be.",,"['geometry', 'differential-geometry', 'multivariable-calculus']"
6,Conditions for existence of divergence of a vector field,Conditions for existence of divergence of a vector field,,"What are the necessary and sufficient conditions on a vector field $F$ for the divergence $\nabla\cdot F$ to exist at a given point. EDIT In Divergence in the second line under the heading "" Application in Cartesian coordinates "", why is it assumed that $\vec{F}$ to be a continuously differentiable vector field ? EDIT 2 Ideally one would expect each component of $F$ to be differentiable at a given point $\vec{a}$ no matter through which continuous contour you traverse the point $\vec{a}$. EDIT 3 Or is it that each component of $F$ to be differentiable and the derivative being continuous at a given point $\vec{a}$ no matter through which continuous contour you traverse the point $\vec{a}$.","What are the necessary and sufficient conditions on a vector field $F$ for the divergence $\nabla\cdot F$ to exist at a given point. EDIT In Divergence in the second line under the heading "" Application in Cartesian coordinates "", why is it assumed that $\vec{F}$ to be a continuously differentiable vector field ? EDIT 2 Ideally one would expect each component of $F$ to be differentiable at a given point $\vec{a}$ no matter through which continuous contour you traverse the point $\vec{a}$. EDIT 3 Or is it that each component of $F$ to be differentiable and the derivative being continuous at a given point $\vec{a}$ no matter through which continuous contour you traverse the point $\vec{a}$.",,['multivariable-calculus']
7,Surface described by orthogonality condition for vectors,Surface described by orthogonality condition for vectors,,"Two vectors $\mathbf{x}=(x_1,x_2)$ and $\mathbf{y}=(y_1,y_2)$ are orthogonal if their scalar product is zero, i.e. $x_1 y_1 + x_2 y_2=0$. What surface does this equation describe in the four-dimensional $(x_1,x_2,y_1,y_2)$ space? How about the orthogonality condition for $3$-dimensional (or higher dimensional) vectors? What is the corresponding surface and how would one visualize it? Thanks for the answers.","Two vectors $\mathbf{x}=(x_1,x_2)$ and $\mathbf{y}=(y_1,y_2)$ are orthogonal if their scalar product is zero, i.e. $x_1 y_1 + x_2 y_2=0$. What surface does this equation describe in the four-dimensional $(x_1,x_2,y_1,y_2)$ space? How about the orthogonality condition for $3$-dimensional (or higher dimensional) vectors? What is the corresponding surface and how would one visualize it? Thanks for the answers.",,[]
8,Paramterizing the surface on the intersection of $x+z=a$ and interior of $x^2+y^2+z^2=a^2$,Paramterizing the surface on the intersection of  and interior of,x+z=a x^2+y^2+z^2=a^2,"So I am trying to verify Stokes' theorem for $\vec{F}=y\hat{i}+z\hat{j}+x\hat{k}$ where the curve $C$ is on the intersection of $x+z=a$ and $x^2+y^2+z^2=a^2$ . Solving these equations yields the curve $2(x-a/2)^2+y^2=a^2/2$ which I have parametrized as $x=\frac{a}{2}(1+\cos\theta)$ , $y=\frac{a}{\sqrt 2}\sin\theta$ , $z=\frac{a}{2}(1-\cos\theta)$ with $0\le\theta<2\pi$ . The line integral $\int \vec{F}\cdot d\vec{r}$ now evaluates to $-\frac{a^2\pi}{\sqrt{2}}$ . Now I want to take the surface as the intersection of $x+z=a$ and the interior of $x^2+y^2+z^2=a^2$ and prove that the surface integral $\int(\nabla\times \vec{F})\cdot d\vec{s}  $ evaluates to the same value. For this reason, I want the parametric equation of this disk. But what can it be?","So I am trying to verify Stokes' theorem for where the curve is on the intersection of and . Solving these equations yields the curve which I have parametrized as , , with . The line integral now evaluates to . Now I want to take the surface as the intersection of and the interior of and prove that the surface integral evaluates to the same value. For this reason, I want the parametric equation of this disk. But what can it be?",\vec{F}=y\hat{i}+z\hat{j}+x\hat{k} C x+z=a x^2+y^2+z^2=a^2 2(x-a/2)^2+y^2=a^2/2 x=\frac{a}{2}(1+\cos\theta) y=\frac{a}{\sqrt 2}\sin\theta z=\frac{a}{2}(1-\cos\theta) 0\le\theta<2\pi \int \vec{F}\cdot d\vec{r} -\frac{a^2\pi}{\sqrt{2}} x+z=a x^2+y^2+z^2=a^2 \int(\nabla\times \vec{F})\cdot d\vec{s}  ,"['multivariable-calculus', 'vector-analysis', 'parametrization', 'stokes-theorem']"
9,Why do we use the Jacobian determinant to change the variables of a double integral?,Why do we use the Jacobian determinant to change the variables of a double integral?,,"I've been trying to learn (the basics of) Multivariable Calculus on Khan Academy, but I've begun facing difficulties with changing the variables of a double (& triple) integral. The problem is the following: Suppose we wanted to evaluate the double integral $$S=\iint_{D}(24x^2+12y^2)\ dx\ dy$$ by first applying a change of variables from $D$ to $R$ : $$x = X_{1}(u,v)=\frac{u}{4}$$ $$y = X_{2}(u,v)=\frac{v}{3}$$ What is $S$ under the change of variables? Previously, when dealing with questions of the change of variables in functions, I was taught to just substitute $X_{1}(u,v)$ and $X_{2}(u,v)$ into the original function $f(x,y)$ , and get my answer. ( If I've done a bad job explaining this please tell me so that I can clarify with an example ). However, I had never done a problem like this and I was stumped, so I looked at the solution, which was the following: For a transformation $\mathbf{X}:R\rightarrow D$ , the integral can be rewritten as $$\iint_{D}f(x,y)dA = \iint_{R}f(\mathbf{X}(u,v))|J(\mathbf{X})|\ du\ dv$$ It then goes on to calculate the determinant of the Jacobian, and solves the problem. Although I can understand how they got to the solution, I cannot comprehend the why . So far I have only seen the Jacobian as a means of describing a local linearization , and I've been told that its determinant represents the scale factor that some arbitrary area is multiplied by as it undergoes the transformation. ( Again, please tell me if I've explained this badly. ) Whilst typically with each question there are also videos/articles to go along with it, this question has provided me with no extra material and I haven't been able to understand why the Jacobian comes into play here. Could someone please explain to me why we use the Jacobian here, and point me to any extra material which could help me understand this problem better? Additionally, could someone explain to me the purpose of changing the variables? (P.S., Unless I am mistaken, the author has made a mistake when they say $\mathbf{X}:R\rightarrow D$ , and should it be $\mathbf{X}:D\rightarrow R$ ) (Link to the quiz: https://www.khanacademy.org/math/multivariable-calculus/integrating-multivariable-functions/x786f2022:change-variables/e/change-of-variables--bound ) Thank you!","I've been trying to learn (the basics of) Multivariable Calculus on Khan Academy, but I've begun facing difficulties with changing the variables of a double (& triple) integral. The problem is the following: Suppose we wanted to evaluate the double integral by first applying a change of variables from to : What is under the change of variables? Previously, when dealing with questions of the change of variables in functions, I was taught to just substitute and into the original function , and get my answer. ( If I've done a bad job explaining this please tell me so that I can clarify with an example ). However, I had never done a problem like this and I was stumped, so I looked at the solution, which was the following: For a transformation , the integral can be rewritten as It then goes on to calculate the determinant of the Jacobian, and solves the problem. Although I can understand how they got to the solution, I cannot comprehend the why . So far I have only seen the Jacobian as a means of describing a local linearization , and I've been told that its determinant represents the scale factor that some arbitrary area is multiplied by as it undergoes the transformation. ( Again, please tell me if I've explained this badly. ) Whilst typically with each question there are also videos/articles to go along with it, this question has provided me with no extra material and I haven't been able to understand why the Jacobian comes into play here. Could someone please explain to me why we use the Jacobian here, and point me to any extra material which could help me understand this problem better? Additionally, could someone explain to me the purpose of changing the variables? (P.S., Unless I am mistaken, the author has made a mistake when they say , and should it be ) (Link to the quiz: https://www.khanacademy.org/math/multivariable-calculus/integrating-multivariable-functions/x786f2022:change-variables/e/change-of-variables--bound ) Thank you!","S=\iint_{D}(24x^2+12y^2)\ dx\ dy D R x = X_{1}(u,v)=\frac{u}{4} y = X_{2}(u,v)=\frac{v}{3} S X_{1}(u,v) X_{2}(u,v) f(x,y) \mathbf{X}:R\rightarrow D \iint_{D}f(x,y)dA = \iint_{R}f(\mathbf{X}(u,v))|J(\mathbf{X})|\ du\ dv \mathbf{X}:R\rightarrow D \mathbf{X}:D\rightarrow R",['multivariable-calculus']
10,proof of differential is the scalar product with gradient: $D_g(x)(v) = \nabla(g)(x)\cdot v$,proof of differential is the scalar product with gradient:,D_g(x)(v) = \nabla(g)(x)\cdot v,"I'm looking for a proof of this classical result for a differentiable multivariate function $g$ : let $D_g(x)$ be the differentiate (I also find total derivative ) of $g$ at point $x$ : $D_g(x)(v) = \nabla(g)(x)\cdot v$ My starting points are the following: A function $g$ is differentiable in $x$ means that there is a linear function (often called also a linear map ) $D_g(x)(v)$ such that $g(x+v) = g(x)+D_g(x)(v)+o(\lVert v\rVert)$ Gradient is defined as the vector of partial derivatives defined as below: $\frac{dg}{dx_i}=\lim_{h\to 0}\frac{g(x+he_i)-g(x)}{h}$ where $e_i$ is the $i^{th}$ vector of the canonical base. The actual property can be splitted in two: if the $g$ is differentiable, then it admits a gradient $D_g(x)(v) = \nabla(g)(x)\cdot v$ I found many ressources for definitions and list of properties, such as this one or this other one but I'm missing a proof. This video seems to takes the property as the definition of differentiability, and then proves the formula for directional derivative. Yet the definition of $D_g(x)$ seems more general (this is, for instance, the one that is used in Rademacher's theorem), and its expression as a scalar product, a consequence. I know it's a very fundamental result of multivariate calculus and even a mere link to a more complete lecture would be appreciated.","I'm looking for a proof of this classical result for a differentiable multivariate function : let be the differentiate (I also find total derivative ) of at point : My starting points are the following: A function is differentiable in means that there is a linear function (often called also a linear map ) such that Gradient is defined as the vector of partial derivatives defined as below: where is the vector of the canonical base. The actual property can be splitted in two: if the is differentiable, then it admits a gradient I found many ressources for definitions and list of properties, such as this one or this other one but I'm missing a proof. This video seems to takes the property as the definition of differentiability, and then proves the formula for directional derivative. Yet the definition of seems more general (this is, for instance, the one that is used in Rademacher's theorem), and its expression as a scalar product, a consequence. I know it's a very fundamental result of multivariate calculus and even a mere link to a more complete lecture would be appreciated.",g D_g(x) g x D_g(x)(v) = \nabla(g)(x)\cdot v g x D_g(x)(v) g(x+v) = g(x)+D_g(x)(v)+o(\lVert v\rVert) \frac{dg}{dx_i}=\lim_{h\to 0}\frac{g(x+he_i)-g(x)}{h} e_i i^{th} g D_g(x)(v) = \nabla(g)(x)\cdot v D_g(x),"['multivariable-calculus', 'derivatives']"
11,The selection of the direction of the auxiliary curve when applying green's theorem to line integral with a singular point seems to change the answer,The selection of the direction of the auxiliary curve when applying green's theorem to line integral with a singular point seems to change the answer,,"Problem Compute $$ \oint_L\frac{xdy-ydx}{4x^2+y^2} $$ where $L$ is a circle centered at $(1, 0)$ with a radius of $R > 1$ , and the direction of $L$ is counterclock-wise. Solution To by pass the singular point at $(0, 0)$ which is inside the circle $L$ surrounds, adding an auxiliary curve: $$ C:4x^2+y^2 = \delta^2 $$ The integral can then be computed by applying Green's theorem. However, different choices of the direction of $C$ produce different answers. If we choose counterclock-wise: $$ \oint_L\frac{xdy-ydx}{4x^2+y^2}=\oint_{L+C}-\oint_{C}=-\oint_{C}=-\pi $$ ， if we choose clockwise: $$ \oint_L\frac{xdy-ydx}{4x^2+y^2}=\oint_{L+C^{-1}}-\oint_{C^{-1}}=\oint_{C}=\pi $$ What I got wrong here?","Problem Compute where is a circle centered at with a radius of , and the direction of is counterclock-wise. Solution To by pass the singular point at which is inside the circle surrounds, adding an auxiliary curve: The integral can then be computed by applying Green's theorem. However, different choices of the direction of produce different answers. If we choose counterclock-wise: ， if we choose clockwise: What I got wrong here?","
\oint_L\frac{xdy-ydx}{4x^2+y^2}
 L (1, 0) R > 1 L (0, 0) L 
C:4x^2+y^2 = \delta^2
 C 
\oint_L\frac{xdy-ydx}{4x^2+y^2}=\oint_{L+C}-\oint_{C}=-\oint_{C}=-\pi
 
\oint_L\frac{xdy-ydx}{4x^2+y^2}=\oint_{L+C^{-1}}-\oint_{C^{-1}}=\oint_{C}=\pi
","['calculus', 'multivariable-calculus', 'line-integrals']"
12,Derivative of Discrete Mutual Information,Derivative of Discrete Mutual Information,,"Given a fixed channel $p(y|x)$ ,  and denoting the discrete input pmf $p(x)$ ,  and the corresponding output pmf by $p(y)$ ,  prove that: $$  \frac{\partial I(X;Y)}{\partial p(x)} = I(X=x;Y) - \log{e} $$ where, $$   I(X=x;Y) =  \sum_{y} p(y|x) \log{\left(\frac{p(y|x)}{p(y)}\right)}  $$ Note $^*$ : the $\log$ is base 2 here. Solution (thanks to @stochasticboy321): We can write $$ I(X;Y) = \sum_{i}p(x_i)I(X=x_i;Y) \Longrightarrow  \frac{\partial I(X;Y)}{\partial p(x_j)} = I(X=x_j;Y) + \sum_{i} p(x_i)\color{red}{\frac{\partial I(X=x_i;Y)}{\partial p(x_j)}}. $$ But, $$  \begin{aligned} \color{red}{\frac{\partial I(X=x_i;Y)}{\partial p(x_j)}} &= \frac{\partial }{\partial p(x_j)} \sum_{k} p(y_k|x_i) \log{\left(\frac{p(y_k|x_i)}{p(y_k)}\right)}\\ &= \frac{\partial }{\partial p(x_j)}\left(\sum_{k} p(y_k|x_i) \log{(p(y_k|x_i)) - \sum_{k} p(y_k|x_i)\log [p(y_k)]}\right) \\ &= -\frac{\partial }{\partial p(x_j)} \sum_{k} p(y_k|x_i)\log [p(y_k)] \\ &= - \sum_{k} p(y_k|x_i)\color{blue}{\frac{\partial \log [p(y_k)]}{\partial p(x_j)}} \end{aligned} $$ Where $p(y_k) =\sum_i p(y_k|x_i)p(x_i)$ . Let's continue, $$ \begin{aligned} \color{blue}{\frac{\partial \log [p(y_k)]}{\partial p(x_j)}} &= \frac{\log e}{p(y_k)} \frac{\partial }{\partial p(x_j)} \sum_i p(y_k|x_i)p(x_i)\\ &= \frac{\log e}{p(y_k)} p(y_k|x_j)  \end{aligned} $$ Hence, $$ \begin{aligned} \color{red}{\frac{\partial I(X=x_i;Y)}{\partial p(x_j)}}  &= -\log e \left[\sum_k \color{purple}{\frac{p(y_k|x_i)p(y_k|x_j)}{p(y_k)}} \right]\\ &= -\log e \left[\sum_k \color{purple}{\frac{p(x_i|y_k)p(y_k|x_j)}{p(x_i)}} \right] \end{aligned} $$ Thus, $$ \begin{aligned} \frac{\partial I(X;Y)}{\partial p(x_j)} &= I(X=x_j;Y)  -\log e \sum_{i} p(x_i)\color{red}{\left[\sum_k \color{purple}{\frac{p(x_i|y_k)p(y_k|x_j)}{p(x_i)}} \right]}\\ &= I(X=x_j;Y)  -\log e  \sum_i p(x_i|y_k) \sum_k p(y_k|x_j) \\ &\stackrel{(a)}{=} I(X=x_j;Y)  -\log e   \end{aligned} $$ (a) The two sums add up to one since they are summations over legitimate probability space ( $i, \ k$ ).","Given a fixed channel ,  and denoting the discrete input pmf ,  and the corresponding output pmf by ,  prove that: where, Note : the is base 2 here. Solution (thanks to @stochasticboy321): We can write But, Where . Let's continue, Hence, Thus, (a) The two sums add up to one since they are summations over legitimate probability space ( ).","p(y|x) p(x) p(y) 
 \frac{\partial I(X;Y)}{\partial p(x)} = I(X=x;Y) - \log{e}
 
  I(X=x;Y) =  \sum_{y} p(y|x) \log{\left(\frac{p(y|x)}{p(y)}\right)}
  ^* \log 
I(X;Y) = \sum_{i}p(x_i)I(X=x_i;Y) \Longrightarrow  \frac{\partial I(X;Y)}{\partial p(x_j)} = I(X=x_j;Y) + \sum_{i} p(x_i)\color{red}{\frac{\partial I(X=x_i;Y)}{\partial p(x_j)}}.
  
\begin{aligned}
\color{red}{\frac{\partial I(X=x_i;Y)}{\partial p(x_j)}} &= \frac{\partial }{\partial p(x_j)} \sum_{k} p(y_k|x_i) \log{\left(\frac{p(y_k|x_i)}{p(y_k)}\right)}\\
&= \frac{\partial }{\partial p(x_j)}\left(\sum_{k} p(y_k|x_i) \log{(p(y_k|x_i)) - \sum_{k} p(y_k|x_i)\log [p(y_k)]}\right) \\
&= -\frac{\partial }{\partial p(x_j)} \sum_{k} p(y_k|x_i)\log [p(y_k)] \\
&= - \sum_{k} p(y_k|x_i)\color{blue}{\frac{\partial \log [p(y_k)]}{\partial p(x_j)}}
\end{aligned}
 p(y_k) =\sum_i p(y_k|x_i)p(x_i) 
\begin{aligned}
\color{blue}{\frac{\partial \log [p(y_k)]}{\partial p(x_j)}} &= \frac{\log e}{p(y_k)} \frac{\partial }{\partial p(x_j)} \sum_i p(y_k|x_i)p(x_i)\\
&= \frac{\log e}{p(y_k)} p(y_k|x_j) 
\end{aligned}
 
\begin{aligned}
\color{red}{\frac{\partial I(X=x_i;Y)}{\partial p(x_j)}}  &= -\log e \left[\sum_k \color{purple}{\frac{p(y_k|x_i)p(y_k|x_j)}{p(y_k)}} \right]\\
&= -\log e \left[\sum_k \color{purple}{\frac{p(x_i|y_k)p(y_k|x_j)}{p(x_i)}} \right]
\end{aligned}
 
\begin{aligned}
\frac{\partial I(X;Y)}{\partial p(x_j)} &= I(X=x_j;Y)  -\log e \sum_{i} p(x_i)\color{red}{\left[\sum_k \color{purple}{\frac{p(x_i|y_k)p(y_k|x_j)}{p(x_i)}} \right]}\\
&= I(X=x_j;Y)  -\log e  \sum_i p(x_i|y_k) \sum_k p(y_k|x_j) \\
&\stackrel{(a)}{=} I(X=x_j;Y)  -\log e  
\end{aligned}
 i, \ k","['multivariable-calculus', 'information-theory', 'mutual-information']"
13,"""Correct"" way of seeing that $\frac{\partial}{\partial A} f(AB) = \frac{\partial f(X)}{\partial X} B^T$, where $X = AB$","""Correct"" way of seeing that , where",\frac{\partial}{\partial A} f(AB) = \frac{\partial f(X)}{\partial X} B^T X = AB,"Lemma: Let $A \in \mathbb{R}^{m\times n}$ and $B \in \mathbb{R}^{n\times k}$ be matrices, and let $f:\mathbb{R}^{m\times k} \to\mathbb{R}$ be a differentiable function. Let $X = AB$ . Then $$ \frac{\partial f(X)}{\partial A} = \Big(\frac{\partial f(X)}{\partial X}\Big)B^T, $$ where the right-hand side is the matrix product, and we are writing $\frac{\partial f(X)}{\partial X}$ for the matrix with $(i,j)$ -entry $\frac{\partial f(X)}{\partial X_{ij}}$ . Proof: One way to see this is by using the multivariable chain rule to get $$ \frac{\partial f(AB)}{\partial A_{ij}} = \sum_{k,l}\frac{\partial f(X)}{\partial X_{kl}}\cdot\frac{\partial X_{kl}}{\partial A_{ij}}, $$ and then observing that $$ \frac{\partial X_{kl}}{\partial A_{ij}} = \mathbb{1}_{i=k}B_{jl}, $$ which implies that $$ \sum_{k,l}\frac{\partial f(X)}{\partial X_{kl}}\cdot\frac{\partial X_{kl}}{\partial A_{ij}} = \sum_{l}\frac{\partial f(X)}{\partial X_{il}}B_{jl} = \Big(\Big(\frac{\partial f(X)}{X}\Big)B^T\Big)_{ij}. $$ $$\tag*{$\blacksquare$}$$ My question: The thing we are trying to show looks pretty much exactly like the single variable chain rule, namely that for real numbers $x = ab$ , we have $$ \frac{\partial f(x)}{\partial a} = \Big(\frac{d f(x)}{dx} \Big)\cdot b, $$ so I'm wondering if there is some appropriate ""matrix chain rule"" that lets us see the lemma immediately, without having to expand it in terms of all these scalar derivatives. I've searched around things like ""matrix/tensor chain rule"", ""matrix/tensor calculus"", but I haven't found exactly what I'm looking for. I suspect that the answer lies in tensor calculus, but the references I've found contain a lot of abstraction, and it seems they are intended for a much more general setting (i.e. general relativity or the like).","Lemma: Let and be matrices, and let be a differentiable function. Let . Then where the right-hand side is the matrix product, and we are writing for the matrix with -entry . Proof: One way to see this is by using the multivariable chain rule to get and then observing that which implies that My question: The thing we are trying to show looks pretty much exactly like the single variable chain rule, namely that for real numbers , we have so I'm wondering if there is some appropriate ""matrix chain rule"" that lets us see the lemma immediately, without having to expand it in terms of all these scalar derivatives. I've searched around things like ""matrix/tensor chain rule"", ""matrix/tensor calculus"", but I haven't found exactly what I'm looking for. I suspect that the answer lies in tensor calculus, but the references I've found contain a lot of abstraction, and it seems they are intended for a much more general setting (i.e. general relativity or the like).","A \in \mathbb{R}^{m\times n} B \in \mathbb{R}^{n\times k} f:\mathbb{R}^{m\times k} \to\mathbb{R} X = AB 
\frac{\partial f(X)}{\partial A} = \Big(\frac{\partial f(X)}{\partial X}\Big)B^T,
 \frac{\partial f(X)}{\partial X} (i,j) \frac{\partial f(X)}{\partial X_{ij}} 
\frac{\partial f(AB)}{\partial A_{ij}} = \sum_{k,l}\frac{\partial f(X)}{\partial X_{kl}}\cdot\frac{\partial X_{kl}}{\partial A_{ij}},
 
\frac{\partial X_{kl}}{\partial A_{ij}} = \mathbb{1}_{i=k}B_{jl},
 
\sum_{k,l}\frac{\partial f(X)}{\partial X_{kl}}\cdot\frac{\partial X_{kl}}{\partial A_{ij}} = \sum_{l}\frac{\partial f(X)}{\partial X_{il}}B_{jl} = \Big(\Big(\frac{\partial f(X)}{X}\Big)B^T\Big)_{ij}.
 \tag*{\blacksquare} x = ab 
\frac{\partial f(x)}{\partial a} = \Big(\frac{d f(x)}{dx} \Big)\cdot b,
","['multivariable-calculus', 'matrix-calculus', 'chain-rule']"
14,Volume enclosed by convex surface and tangent plane over a region $\mathcal R$,Volume enclosed by convex surface and tangent plane over a region,\mathcal R,"Consider a convex surface $\mathcal S$ defined by $z = f(x,y)$ . The volume between the surface and its tangent plane at point $P$ , enclosed within exterior $\mathcal S$ is minimal when $P$ is  the centroid of region $\mathcal R$ . Note :There is a region $\mathcal R$ whose outline is any general curve lying along the $xy$ -plane, and it extends along the $z$ -axis. For example, if $\mathcal R$ is a circle in the $xy$ -plane, extending it along the $z$ -axis results in a cylinder, denoted as exterior $\mathcal S$ . I have proven this for $P(x,y) = (0,0)$ when region $\mathcal R$ is a circle of radius $r$ , as $P$ is its centroid for any convex surface. However, I am struggling to prove why this result holds for any convex surface $z = f(x,y)$ and any region $\mathcal R$ . Some of the tools I used to tackle the problem include Lagrange multipliers, the Euler-Lagrange equation, and since the volume under the surface is constant, the problem is equivalent to showing that the volume under the tangent plane at point $P$ is minimum for $P$ , where $P$ is the centroid of any region $\mathcal R$ and for any surface $z= f(x,y)$ .","Consider a convex surface defined by . The volume between the surface and its tangent plane at point , enclosed within exterior is minimal when is  the centroid of region . Note :There is a region whose outline is any general curve lying along the -plane, and it extends along the -axis. For example, if is a circle in the -plane, extending it along the -axis results in a cylinder, denoted as exterior . I have proven this for when region is a circle of radius , as is its centroid for any convex surface. However, I am struggling to prove why this result holds for any convex surface and any region . Some of the tools I used to tackle the problem include Lagrange multipliers, the Euler-Lagrange equation, and since the volume under the surface is constant, the problem is equivalent to showing that the volume under the tangent plane at point is minimum for , where is the centroid of any region and for any surface .","\mathcal S z = f(x,y) P \mathcal S P \mathcal R \mathcal R xy z \mathcal R xy z \mathcal S P(x,y) = (0,0) \mathcal R r P z = f(x,y) \mathcal R P P P \mathcal R z= f(x,y)","['multivariable-calculus', 'optimization', 'calculus-of-variations']"
15,A definite integral over the unit sphere,A definite integral over the unit sphere,,"Is there a closed form for the following definite integral over the unit sphere? $$I = \int_0^{\pi}d\theta \sin\theta \int_0^{2\pi}d\phi |x (\sin\theta\cos\phi)^2 +(1-x)(\sin\theta\sin\phi)^2 - (\cos\theta)^2| $$ where $x\in[0,2]$ . This came up in a physics problem. Mahthematica (surprisingly) returned the result $I = 4 \pi  \sqrt{\frac{x}{(x+1)^3}}$ which really only agrees with the numerical integration at $x=\frac{1}{2}$ , so it is most likely wrong. Since the function inside the absolute value signs is just a weighted sum of squares of the coordinates of the unit directional vector, I thought that it might be dealt with by going to some sort of ellipsoidal coordinate system. But so far I have not found a way to make it work. Update 1 : Thanks to the comment from @SangchulLee , I was able to proceed a bit further to the following intermediate result. First, we substitute $$u(\phi)=x\cos^2\phi+(1-x)\sin^2\phi$$ Then, the integral reduces to $$ I = \int_0^{2\pi}d\phi\int_{-1}^1dt | (1-t^2) u(\phi) - t^2 | = \frac{2}{3} \int_0^{2\pi}d\phi \left[ 1-2u(\phi) + 4u(\phi)\sqrt{\frac{u(\phi)}{1+u(\phi)}}\Theta(u(\phi)) \right] $$ where $\Theta$ is the Heaviside theta function. The integral of the first two summands yields zero, so we obtain $$ I = \frac{8}{3} \int_0^{2\pi}d\phi \sqrt{\frac{u(\phi)^3}{1+u(\phi)}}\Theta(u(\phi))$$ Thus, for some special values of $x=0,1,\frac{1}{2}$ , the integral can be easily performed. However, I am not able to go further from here on.","Is there a closed form for the following definite integral over the unit sphere? where . This came up in a physics problem. Mahthematica (surprisingly) returned the result which really only agrees with the numerical integration at , so it is most likely wrong. Since the function inside the absolute value signs is just a weighted sum of squares of the coordinates of the unit directional vector, I thought that it might be dealt with by going to some sort of ellipsoidal coordinate system. But so far I have not found a way to make it work. Update 1 : Thanks to the comment from @SangchulLee , I was able to proceed a bit further to the following intermediate result. First, we substitute Then, the integral reduces to where is the Heaviside theta function. The integral of the first two summands yields zero, so we obtain Thus, for some special values of , the integral can be easily performed. However, I am not able to go further from here on.","I = \int_0^{\pi}d\theta \sin\theta \int_0^{2\pi}d\phi |x (\sin\theta\cos\phi)^2 +(1-x)(\sin\theta\sin\phi)^2 - (\cos\theta)^2|  x\in[0,2] I = 4 \pi  \sqrt{\frac{x}{(x+1)^3}} x=\frac{1}{2} u(\phi)=x\cos^2\phi+(1-x)\sin^2\phi  I = \int_0^{2\pi}d\phi\int_{-1}^1dt | (1-t^2) u(\phi) - t^2 | = \frac{2}{3} \int_0^{2\pi}d\phi \left[ 1-2u(\phi) + 4u(\phi)\sqrt{\frac{u(\phi)}{1+u(\phi)}}\Theta(u(\phi)) \right]  \Theta  I = \frac{8}{3} \int_0^{2\pi}d\phi \sqrt{\frac{u(\phi)^3}{1+u(\phi)}}\Theta(u(\phi)) x=0,1,\frac{1}{2}","['integration', 'multivariable-calculus', 'definite-integrals']"
16,"Gauss-Newton method, where did sigma inverse come from?","Gauss-Newton method, where did sigma inverse come from?",,"I'm studying the Gauss-Newton Method from ""slambook-en"" chapter 5 on optimization (the books is made free online by the author in case you need to see it). I've attached a picture of the example the author is using to elaborate the method. My doubt is with the sudden appearance of the sigma inverse in the final formulation. I understand how the general Gauss Newton method works out to approximate the hessian matrix, but I'm struggling to understand the sigma inverse. It is of course (or at least I think) related to the 'w' in page 1 that I have attached which is used to simulate the gaussian noise, but that is the extent to which I understand it. Any help or insights would be greatly appreciated!","I'm studying the Gauss-Newton Method from ""slambook-en"" chapter 5 on optimization (the books is made free online by the author in case you need to see it). I've attached a picture of the example the author is using to elaborate the method. My doubt is with the sudden appearance of the sigma inverse in the final formulation. I understand how the general Gauss Newton method works out to approximate the hessian matrix, but I'm struggling to understand the sigma inverse. It is of course (or at least I think) related to the 'w' in page 1 that I have attached which is used to simulate the gaussian noise, but that is the extent to which I understand it. Any help or insights would be greatly appreciated!",,"['multivariable-calculus', 'optimization', 'nonlinear-optimization', 'least-squares', 'jacobian']"
17,Monotonicity of a multivariate function,Monotonicity of a multivariate function,,"Let $f:\mathbb R^n\times \mathbb R^n\rightarrow \mathbb R$ . Assume $f(x,y)$ is twise continuously differentiable and the derivative with respect to both arguments (i.e., $x$ and $y$ ) are monotone in the sense that the following holds true for any $x,x'\in\mathbb R^n$ such that $x\neq x'$ : $$(x'-x)(D_xf(x,x')-D_xf(x,x))>0~\textrm{and}$$ $$(x'-x)(D_yf(x',x)-D_yf(x,x))>0~\textrm.$$ Suppose now we have $$f(x,x')-f(x,x)>0.$$ Can we prove that the following statement is true: $f(x',x')-f(x',x)\geq0?$","Let . Assume is twise continuously differentiable and the derivative with respect to both arguments (i.e., and ) are monotone in the sense that the following holds true for any such that : Suppose now we have Can we prove that the following statement is true:","f:\mathbb R^n\times \mathbb R^n\rightarrow \mathbb R f(x,y) x y x,x'\in\mathbb R^n x\neq x' (x'-x)(D_xf(x,x')-D_xf(x,x))>0~\textrm{and} (x'-x)(D_yf(x',x)-D_yf(x,x))>0~\textrm. f(x,x')-f(x,x)>0. f(x',x')-f(x',x)\geq0?","['multivariable-calculus', 'derivatives', 'monotone-functions', 'hessian-matrix']"
18,Diffeomorphism from the unit disc to the $n$-dimensional Euclidean space,Diffeomorphism from the unit disc to the -dimensional Euclidean space,n,"Let $f: B^n \to \Bbb R^n$ be a map from the unit disc given by $x \mapsto \dfrac{x}{\sqrt{1-\|x\|^2}}$ . Show that this map is a diffeomorphism. So I'm trying to find a way to derive the inverse for this map, but I find it quite difficult. I've managed to solve the $1$ -dimensional case with the following: Consider $f:B^1 \to \Bbb R$ given by $x \mapsto \dfrac{x}{\sqrt{1-|x|^2}}$ . To find the inverse of $f$ we set $y =  \dfrac{x}{\sqrt{1-|x|^2}}$ from where we can solve to get $x^2+y^2|x|^2 = y^2$ . And now the crucial point, since $|x|^2 = x^2$ we get $x^2+y^2x^2 = y^2$ and so $x= \dfrac{y^2}{\sqrt{1+y^2}}$ i.e. the inverse $f^{-1}$ is given by $y \mapsto \dfrac{y^2}{\sqrt{1+y^2}}$ . The crucial point here is that due to $x$ being a real number I have $|x|^2 = x^2$ , but $x^2$ is ambiguous when $x$ is a vector so this method wont generalize. A similar approach will only lead to $$\begin{align*} y &=  \frac{x}{\sqrt{1-\|x\|^2}} \\ y(\sqrt{1-\|x\|^2})&=x \\ y^2(1-\|x\|^²)&=x^2 \\ y^2-y^2\|x\|^2&=x^2 \end{align*}$$ so $$x^2+y^2\|x\|^2=y^2$$ but this is as far as I can get as I don't know anything I could to the term $\|x\|^2$ . What could I do here?","Let be a map from the unit disc given by . Show that this map is a diffeomorphism. So I'm trying to find a way to derive the inverse for this map, but I find it quite difficult. I've managed to solve the -dimensional case with the following: Consider given by . To find the inverse of we set from where we can solve to get . And now the crucial point, since we get and so i.e. the inverse is given by . The crucial point here is that due to being a real number I have , but is ambiguous when is a vector so this method wont generalize. A similar approach will only lead to so but this is as far as I can get as I don't know anything I could to the term . What could I do here?","f: B^n \to \Bbb R^n x \mapsto \dfrac{x}{\sqrt{1-\|x\|^2}} 1 f:B^1 \to \Bbb R x \mapsto \dfrac{x}{\sqrt{1-|x|^2}} f y =  \dfrac{x}{\sqrt{1-|x|^2}} x^2+y^2|x|^2 = y^2 |x|^2 = x^2 x^2+y^2x^2 = y^2 x= \dfrac{y^2}{\sqrt{1+y^2}} f^{-1} y \mapsto \dfrac{y^2}{\sqrt{1+y^2}} x |x|^2 = x^2 x^2 x \begin{align*} y &=  \frac{x}{\sqrt{1-\|x\|^2}} \\ y(\sqrt{1-\|x\|^2})&=x \\
y^2(1-\|x\|^²)&=x^2 \\
y^2-y^2\|x\|^2&=x^2 \end{align*} x^2+y^2\|x\|^2=y^2 \|x\|^2","['multivariable-calculus', 'diffeomorphism']"
19,Spivak 4-23 How to construct the $k$-cell?,Spivak 4-23 How to construct the -cell?,k,"I'm going through Spivak's Calculus on Manifolds, and I'm trying to do question 4-23 because I'm trying to understand how the chains and geometry works, but I'm having some trouble since most mark schemes cut off before this point.  The question is For $R>0$ and $n$ an integer, define a singular $1$ -cube $c_{R,n}:[0,1]\rightarrow\mathbb{R}^2-0$ by $c_{R,n}(t)=(R\cos2\pi nt,R\sin2\pi nt)$ .  Show that there isa singular $2$ -cube $c:[0,1]^2\rightarrow\mathbb{R}^2-0$ s.t. $c_{R_1,n}-c_{R_2,n}=\partial c$ I've been visualising $c_{R,n}$ as some phasor, and I've been working backwards, so I'm trying to construct a function in $[0,1]^2$ s.t. when $y=0$ , $f(x,0)=(\cos\left(\frac{\pi}{4}nx\right),\sin\left(\frac{\pi}{4}nx\right))$ .  I think $$f(x,y)=(R_1-R_2)\left(\cos\left(\frac{\pi}{4}nx\right)+\cos\left(\frac{\pi}{4}n(1+y)\right),\sin\left(\frac{\pi}{4}nx\right)+\sin\left(\frac{\pi}{4}n(1+y)\right)\right)$$ works, but I'm not sure whether I've done it correctly, since in my case surely this could've been done with a single $c_{R,n}$ ?  I feel as though I'm completely missed the point of the exercise.  In the words of Pauli, it seems that ""it's not even wrong""!","I'm going through Spivak's Calculus on Manifolds, and I'm trying to do question 4-23 because I'm trying to understand how the chains and geometry works, but I'm having some trouble since most mark schemes cut off before this point.  The question is For and an integer, define a singular -cube by .  Show that there isa singular -cube s.t. I've been visualising as some phasor, and I've been working backwards, so I'm trying to construct a function in s.t. when , .  I think works, but I'm not sure whether I've done it correctly, since in my case surely this could've been done with a single ?  I feel as though I'm completely missed the point of the exercise.  In the words of Pauli, it seems that ""it's not even wrong""!","R>0 n 1 c_{R,n}:[0,1]\rightarrow\mathbb{R}^2-0 c_{R,n}(t)=(R\cos2\pi nt,R\sin2\pi nt) 2 c:[0,1]^2\rightarrow\mathbb{R}^2-0 c_{R_1,n}-c_{R_2,n}=\partial c c_{R,n} [0,1]^2 y=0 f(x,0)=(\cos\left(\frac{\pi}{4}nx\right),\sin\left(\frac{\pi}{4}nx\right)) f(x,y)=(R_1-R_2)\left(\cos\left(\frac{\pi}{4}nx\right)+\cos\left(\frac{\pi}{4}n(1+y)\right),\sin\left(\frac{\pi}{4}nx\right)+\sin\left(\frac{\pi}{4}n(1+y)\right)\right) c_{R,n}","['multivariable-calculus', 'solution-verification']"
20,"Problem doing a line integral $\int_C P(x,y)dx+Q(x,y)dy$",Problem doing a line integral,"\int_C P(x,y)dx+Q(x,y)dy","Evaluate $$\int_C P(x,y)dx+Q(x,y)dy$$ where $P(x,y) =  y^2 $ , $Q(x,y) = x$ , and $C$ is the part of the graph $x = y^3$ from $(-1,-1)$ to $(1,1)$ . I was trying the parametrization: $$x = t $$ $$y = \sqrt[3] t$$ $$dx = 1$$ $$dy = \frac{1}{3}t^{-2/3}$$ from which: $$\int y^2dx+xdy=\int_{-1}^1 \left(t^{2/3}+ \frac{1}{3}t^{1/3}\right) dt= \frac{17}{10} $$ which is not the correct answer, correct answer is $\dfrac{6}{5}$ .","Evaluate where , , and is the part of the graph from to . I was trying the parametrization: from which: which is not the correct answer, correct answer is .","\int_C P(x,y)dx+Q(x,y)dy P(x,y) =  y^2  Q(x,y) = x C x = y^3 (-1,-1) (1,1) x = t  y = \sqrt[3] t dx = 1 dy = \frac{1}{3}t^{-2/3} \int y^2dx+xdy=\int_{-1}^1 \left(t^{2/3}+ \frac{1}{3}t^{1/3}\right) dt= \frac{17}{10}  \dfrac{6}{5}","['calculus', 'multivariable-calculus', 'parametrization', 'line-integrals']"
21,Clarification of some concepts - calculus of multiple variables,Clarification of some concepts - calculus of multiple variables,,"I am reading from two books a few definitions and a few theorems related to differentiability, differential, directional derivative, etc., and I got a bit confused so I want to clarify if I am understanding these things right. Let's say we have a function $w = f(x,y,z)$ which is defined in some neighborhood $U$ of the point $(a,b,c)$ . As usual, we denote $\Delta{x} = x - a$ $\Delta{y} = y - b$ $\Delta{z} = z - c$ $\Delta{w} = f(a+\Delta{x}, b+\Delta{y}, c+\Delta{z}) - f(a,b,c) $ where $(x,y,z)$ is some other point from $U$ . This is our setup here. The first statement is that when $f'_x(a,b,c), f'_y(a,b,c), f'_z(a,b,c)$ exist, that does not imply that the function $f$ is continuous in the point $(a,b,c)$ . I saw some examples i.e. counterexamples, so OK, I think I understand that. But then I am reading this statement/theorem. If $f'_x(a,b,c), f'_y(a,b,c), f'_z(a,b,c)$ exist and are continuous in the point (a,b,c), then $\Delta{w} = f(a+\Delta{x}, b+\Delta{y}, c+\Delta{z}) - f(a,b,c) $ can be written in this way $\Delta{w} = f'_x(a,b,c) \Delta{x} + f'_y(a,b,c) \Delta{y} + f'_z(a,b,c) \Delta{z} + \epsilon_1 \Delta{x} + \epsilon_2 \Delta{y} + \epsilon_3 \Delta{z} $ , where $\epsilon_1,\epsilon_2,\epsilon_3$ are functions of $a,b,c, \Delta{x}, \Delta{y} ,\Delta{z}$ which tend to $0$ as $(\Delta{x},\Delta{y},\Delta{z}) \to (0,0,0)$ From the conclusion of this theorem, I think it easily follows also that $f$ is continuous in the point $(a,b,c)$ . We just let $\Delta{x}, \Delta{y}, \Delta{z}$ go to zero and we see that $\Delta{w}$ also goes to zero. So it follows that $f$ is continuous in $(a,b,c)$ . Right? Comparing this to 1) I come to the conclusion that the difference in the two situations comes from the fact that in 2) we require also that the partial derivatives are also continuous in the point $(a,b,c)$ , and this makes it possible to claim/prove that $f$ is also continuous. Is this indeed so? I am reading then another definition. We say that $w = f(x,y,z)$ , whose partial derivatives exist in the point $(a,b,c)$ , is differentiable in the point $(a,b,c)$ , if $\Delta{w}$ can be written in the form $$\Delta{w} = f'_x(a,b,c) \Delta{x} + f'_y(a,b,c) \Delta{y} + f'_z(a,b,c) \Delta{z} + \epsilon_1 \Delta{x} + \epsilon_2 \Delta{y} + \epsilon_3 \Delta{z} \ \ \ \ \ (*)$$ where $\epsilon_1,\epsilon_2,\epsilon_3$ are functions of $a,b,c, \Delta{x}, \Delta{y} ,\Delta{z}$ which tend to $0$ as $(\Delta{x},\Delta{y},\Delta{z}) \to (0,0,0)$ From this definition I am drawing the conclusion that the situation in 2) is just one particular case in which the function $f$ is differentiable. I mean, basically 2) is saying that if the partial derivatives exist and are continuous at $(a,b,c)$ , then the function $f$ is differentiable at the point $(a,b,c)$ /differentiable in the sense of definition 3)/. Is this so, i.e. am I understanding this correctly? Also, in other words this definition is just saying that $\Delta{w} = dw + \epsilon_1 \Delta{x} + \epsilon_2 \Delta{y} + \epsilon_3 \Delta{z} $ i.e. the differential of w is ""very good""  approximation of $\Delta{w}$ (very good because these epsilons go to zero as $(\Delta{x}, \Delta{y}, \Delta{z}) \to 0$ ). Right? And when this is so, we say that $f$ is differentiable (differentiable as a whole, not partially) in the point $(a,b,c)$ . Am I getting this right? Finally, there is another theorem which says that if $f$ is differentiable (in the sense of def. 3)) in the point $(a,b,c)$ , then it also has directional derivatives at $(a,b,c)$ in the direction of any unit vector $v = (a_1, b_1, c_1)$ . The proof the authors give for this one is kind of informal, I didn't like it much. But this statement I was able to prove formally myself by using $(*)$ . It seems very easy to prove by using the definition 3) i.e. by using $(*)$ . So am I understanding all this stuff correctly? Am I drawing the correct conclusions for myself? Actually in the book these defs and theorems were stated for two variables $f(x,y)$ but I stated them here for three variables $f(x,y,z)$ again to ensure that I am understanding all this correctly.","I am reading from two books a few definitions and a few theorems related to differentiability, differential, directional derivative, etc., and I got a bit confused so I want to clarify if I am understanding these things right. Let's say we have a function which is defined in some neighborhood of the point . As usual, we denote where is some other point from . This is our setup here. The first statement is that when exist, that does not imply that the function is continuous in the point . I saw some examples i.e. counterexamples, so OK, I think I understand that. But then I am reading this statement/theorem. If exist and are continuous in the point (a,b,c), then can be written in this way , where are functions of which tend to as From the conclusion of this theorem, I think it easily follows also that is continuous in the point . We just let go to zero and we see that also goes to zero. So it follows that is continuous in . Right? Comparing this to 1) I come to the conclusion that the difference in the two situations comes from the fact that in 2) we require also that the partial derivatives are also continuous in the point , and this makes it possible to claim/prove that is also continuous. Is this indeed so? I am reading then another definition. We say that , whose partial derivatives exist in the point , is differentiable in the point , if can be written in the form where are functions of which tend to as From this definition I am drawing the conclusion that the situation in 2) is just one particular case in which the function is differentiable. I mean, basically 2) is saying that if the partial derivatives exist and are continuous at , then the function is differentiable at the point /differentiable in the sense of definition 3)/. Is this so, i.e. am I understanding this correctly? Also, in other words this definition is just saying that i.e. the differential of w is ""very good""  approximation of (very good because these epsilons go to zero as ). Right? And when this is so, we say that is differentiable (differentiable as a whole, not partially) in the point . Am I getting this right? Finally, there is another theorem which says that if is differentiable (in the sense of def. 3)) in the point , then it also has directional derivatives at in the direction of any unit vector . The proof the authors give for this one is kind of informal, I didn't like it much. But this statement I was able to prove formally myself by using . It seems very easy to prove by using the definition 3) i.e. by using . So am I understanding all this stuff correctly? Am I drawing the correct conclusions for myself? Actually in the book these defs and theorems were stated for two variables but I stated them here for three variables again to ensure that I am understanding all this correctly.","w = f(x,y,z) U (a,b,c) \Delta{x} = x - a \Delta{y} = y - b \Delta{z} = z - c \Delta{w} = f(a+\Delta{x}, b+\Delta{y}, c+\Delta{z}) - f(a,b,c)  (x,y,z) U f'_x(a,b,c), f'_y(a,b,c), f'_z(a,b,c) f (a,b,c) f'_x(a,b,c), f'_y(a,b,c), f'_z(a,b,c) \Delta{w} = f(a+\Delta{x}, b+\Delta{y}, c+\Delta{z}) - f(a,b,c)  \Delta{w} = f'_x(a,b,c) \Delta{x} + f'_y(a,b,c) \Delta{y} + f'_z(a,b,c) \Delta{z} + \epsilon_1 \Delta{x} + \epsilon_2 \Delta{y} + \epsilon_3 \Delta{z}  \epsilon_1,\epsilon_2,\epsilon_3 a,b,c, \Delta{x}, \Delta{y} ,\Delta{z} 0 (\Delta{x},\Delta{y},\Delta{z}) \to (0,0,0) f (a,b,c) \Delta{x}, \Delta{y}, \Delta{z} \Delta{w} f (a,b,c) (a,b,c) f w = f(x,y,z) (a,b,c) (a,b,c) \Delta{w} \Delta{w} = f'_x(a,b,c) \Delta{x} + f'_y(a,b,c) \Delta{y} + f'_z(a,b,c) \Delta{z} + \epsilon_1 \Delta{x} + \epsilon_2 \Delta{y} + \epsilon_3 \Delta{z} \ \ \ \ \ (*) \epsilon_1,\epsilon_2,\epsilon_3 a,b,c, \Delta{x}, \Delta{y} ,\Delta{z} 0 (\Delta{x},\Delta{y},\Delta{z}) \to (0,0,0) f (a,b,c) f (a,b,c) \Delta{w} = dw + \epsilon_1 \Delta{x} + \epsilon_2 \Delta{y} + \epsilon_3 \Delta{z}  \Delta{w} (\Delta{x}, \Delta{y}, \Delta{z}) \to 0 f (a,b,c) f (a,b,c) (a,b,c) v = (a_1, b_1, c_1) (*) (*) f(x,y) f(x,y,z)","['calculus', 'multivariable-calculus', 'derivatives', 'differential']"
22,A graph looks flatter and flatter as we zoom in,A graph looks flatter and flatter as we zoom in,,"Let $f : \mathbb{R}^n \to \mathbb{R}^m$ be a function of class $C^1$ such that $f(0) = 0$ and $Df(0) = 0$ . Let $$M = \{ (z, f(z)) \in \mathbb{R}^{n} \times \mathbb{R}^m: z \in \mathbb{R}^n \}$$ be the graph of $f$ ; it is an $n$ -dimensional submanifold of $\mathbb{R}^{n+m}$ . By hypothesis, the tangent plane of $M$ at the origin is the subspace $\mathbb{R}^n \times 0 \subset \mathbb{R}^n \times \mathbb{R}^m$ . Let $S$ be the unit sphere of $\mathbb{R}^n \times 0$ . I would like to prove that for every $\varepsilon > 0$ there exists $\delta > 0$ such that $$0 < \vert x \vert < \delta \text{ and } x\in M \implies d\left( \frac{x}{\vert x \vert}, S \right) < \varepsilon.$$ Intuitively, as we zoom in at the origin, $M$ looks flatter and flatter, so eventually the directions generated by points in $M$ get closer to directions in $S$ . How do I formalize this?","Let be a function of class such that and . Let be the graph of ; it is an -dimensional submanifold of . By hypothesis, the tangent plane of at the origin is the subspace . Let be the unit sphere of . I would like to prove that for every there exists such that Intuitively, as we zoom in at the origin, looks flatter and flatter, so eventually the directions generated by points in get closer to directions in . How do I formalize this?","f : \mathbb{R}^n \to \mathbb{R}^m C^1 f(0) = 0 Df(0) = 0 M = \{ (z, f(z)) \in \mathbb{R}^{n} \times \mathbb{R}^m: z \in \mathbb{R}^n \} f n \mathbb{R}^{n+m} M \mathbb{R}^n \times 0 \subset \mathbb{R}^n \times \mathbb{R}^m S \mathbb{R}^n \times 0 \varepsilon > 0 \delta > 0 0 < \vert x \vert < \delta \text{ and } x\in M \implies d\left( \frac{x}{\vert x \vert}, S \right) < \varepsilon. M M S","['multivariable-calculus', 'smooth-manifolds']"
23,How do we maximize noise immunity between two combinational devices by choosing a single set of voltage thresholds?,How do we maximize noise immunity between two combinational devices by choosing a single set of voltage thresholds?,,"I am taking a course on MIT OCW entitled ""Computation Structures"". There is a problem in a problem set (Problem 2) that I'd like to solve mathematically. There is a solution to the problem , but it seems to have just the answer but not the work to get there. Now, there is a lot of domain specific knowledge here to understand what the math problem is. I will write out the problem statement, then try to explain the domain knowledge, and then reach the math problem. Here is the problem The following are voltage transfer characteristics of single-input, single-output devices to be used in a new logic family Your job is to choose a single set of signaling thresholds $V_{OL},  V_{IL}, V_{OH}$ , and $V_{IH}$ to be used with both devices to give the best noise margins you can. Let me try to explain a bit about what the terms in the problem statement mean. Each of the two devices takes as an input a voltage and generates as output another voltage. Internally, each device interpret an input voltage as a 0 or 1 based on rules that we can summarize in the depiction below What this picture says is that input voltages below $V_{IL}$ are considered a 0, and input voltages above $V_{IH}$ are considered a 1. ""IL"" stands for ""Input Low"" and ""IH"" for ""Input High"". In addition, when generating an output voltage, the devices are specified to generate some voltage below $V_{OL}$ when they want to signal a 0, and to generate some voltage above $V_{OH}$ when they want to signal a 1. ""OL"" and ""OH"" stand for ""Output Low"" and ""Output High"". The graphs in the first picture show what each device outputs given each input. So, for example, in the first picture, we see that for low input voltages that device generates high output voltages. For the second device, low input voltages generate low output voltages, and high input voltages generate high output voltages. $V_{IL}, V_{IH}, V_{OL}$ , and $V_{OH}$ are like thresholds for specifying rules determining how a device interprets an input voltage and how it generates an output voltage. Here is an example of choices for these variables for one of the devices In this example, we have $$V_{OL}=0.5$$ $$V_{IL}=1.5$$ $$V_{IH}=3$$ $$V_{OH}=4.4$$ So, any input voltage of below 1.5V is interpreted by this device as a 0. If the device is to output a 1 signal for this input signal, then since it is generating an output voltage of 5 for these input voltages, we need to make sure that $V_{OH}$ is smaller than or equal to 5. Similarly, we see from $V_{IH}=3$ that any input voltage above 3 is interpreted as a 1, and to signal a 0 as output we need $V_{OH}$ to be larger than or equal to 0.5 (which are the voltages generated by the device for inputs $\geq 3$ ). Note that it must always be the case that $$V_{OL}<V_{IL}<V_{IH}<V_{OH}\tag{1}$$ otherwise it will be possible to have a valid input signal but an invalid output signal. That is, for example if $V_{OH}<V_{IH}$ for a device, then if this device wants to output a signal of 1, it may output a voltage $w$ such that $V_{OH}<w<V_{IH}$ . But then $w$ would not be interpreted as a 1 by another device with these same thresholds. Let's see a specification of the parameters that doesn't work. where all I did relative to the previous drawing was reduce $V_{IH}$ . Notice that the problem here is that there for input voltages between $V_{IH}$ and 3, we have a valid signal input of 1 but the device does not generate an output signal of 0 since these input voltages generate an output voltage that is above $V_{OL}$ . To try to fix this we would need to increase $V_{OL}$ , for example as below But now this is also an invalid specification because $V_{OL}=2>V_{IL}=1.5$ , which violates (1). We are almost at the actual problem. But first we need to define noise margin, which can be seen in the second picture above (starting at the top of the post). There are two noise margins $$V_{IL}-V_{OL}\tag{2}$$ $$V_{OH}-V_{IH}\tag{3}$$ These margins represent the amount of error we could have in an output voltage and still have that output signal the intended value. Ie, if the output voltage from a device is $V_{OH}-\epsilon$ , then if this value is above $V_{IH}$ it is still a 1 signal. Finally, noise immunity is defined as the smallest noise margin between the two. The math problem It is already a math problem to choose the parameters such that noise immunity is maximum, given (1). Then, the actual math problem in the original problem is to choose a single set of four parameters used for both devices and that maximizes the noise margins when we consider the two devices. Now, I think this means that we are trying to maximize the sum of the two noise immunities. For the problem of a single device, what I did was consider multiple cases for the value of $V_{IL}$ and try to go from there. I might write out this attempt in an answer, so that this question doesn't become too huge. Anyways, to sum up my question: how does one solve the original problem using just math (ie, without resorting to insights based on the the graphs presented initially)?","I am taking a course on MIT OCW entitled ""Computation Structures"". There is a problem in a problem set (Problem 2) that I'd like to solve mathematically. There is a solution to the problem , but it seems to have just the answer but not the work to get there. Now, there is a lot of domain specific knowledge here to understand what the math problem is. I will write out the problem statement, then try to explain the domain knowledge, and then reach the math problem. Here is the problem The following are voltage transfer characteristics of single-input, single-output devices to be used in a new logic family Your job is to choose a single set of signaling thresholds , and to be used with both devices to give the best noise margins you can. Let me try to explain a bit about what the terms in the problem statement mean. Each of the two devices takes as an input a voltage and generates as output another voltage. Internally, each device interpret an input voltage as a 0 or 1 based on rules that we can summarize in the depiction below What this picture says is that input voltages below are considered a 0, and input voltages above are considered a 1. ""IL"" stands for ""Input Low"" and ""IH"" for ""Input High"". In addition, when generating an output voltage, the devices are specified to generate some voltage below when they want to signal a 0, and to generate some voltage above when they want to signal a 1. ""OL"" and ""OH"" stand for ""Output Low"" and ""Output High"". The graphs in the first picture show what each device outputs given each input. So, for example, in the first picture, we see that for low input voltages that device generates high output voltages. For the second device, low input voltages generate low output voltages, and high input voltages generate high output voltages. , and are like thresholds for specifying rules determining how a device interprets an input voltage and how it generates an output voltage. Here is an example of choices for these variables for one of the devices In this example, we have So, any input voltage of below 1.5V is interpreted by this device as a 0. If the device is to output a 1 signal for this input signal, then since it is generating an output voltage of 5 for these input voltages, we need to make sure that is smaller than or equal to 5. Similarly, we see from that any input voltage above 3 is interpreted as a 1, and to signal a 0 as output we need to be larger than or equal to 0.5 (which are the voltages generated by the device for inputs ). Note that it must always be the case that otherwise it will be possible to have a valid input signal but an invalid output signal. That is, for example if for a device, then if this device wants to output a signal of 1, it may output a voltage such that . But then would not be interpreted as a 1 by another device with these same thresholds. Let's see a specification of the parameters that doesn't work. where all I did relative to the previous drawing was reduce . Notice that the problem here is that there for input voltages between and 3, we have a valid signal input of 1 but the device does not generate an output signal of 0 since these input voltages generate an output voltage that is above . To try to fix this we would need to increase , for example as below But now this is also an invalid specification because , which violates (1). We are almost at the actual problem. But first we need to define noise margin, which can be seen in the second picture above (starting at the top of the post). There are two noise margins These margins represent the amount of error we could have in an output voltage and still have that output signal the intended value. Ie, if the output voltage from a device is , then if this value is above it is still a 1 signal. Finally, noise immunity is defined as the smallest noise margin between the two. The math problem It is already a math problem to choose the parameters such that noise immunity is maximum, given (1). Then, the actual math problem in the original problem is to choose a single set of four parameters used for both devices and that maximizes the noise margins when we consider the two devices. Now, I think this means that we are trying to maximize the sum of the two noise immunities. For the problem of a single device, what I did was consider multiple cases for the value of and try to go from there. I might write out this attempt in an answer, so that this question doesn't become too huge. Anyways, to sum up my question: how does one solve the original problem using just math (ie, without resorting to insights based on the the graphs presented initially)?","V_{OL},
 V_{IL}, V_{OH} V_{IH} V_{IL} V_{IH} V_{OL} V_{OH} V_{IL}, V_{IH}, V_{OL} V_{OH} V_{OL}=0.5 V_{IL}=1.5 V_{IH}=3 V_{OH}=4.4 V_{OH} V_{IH}=3 V_{OH} \geq 3 V_{OL}<V_{IL}<V_{IH}<V_{OH}\tag{1} V_{OH}<V_{IH} w V_{OH}<w<V_{IH} w V_{IH} V_{IH} V_{OL} V_{OL} V_{OL}=2>V_{IL}=1.5 V_{IL}-V_{OL}\tag{2} V_{OH}-V_{IH}\tag{3} V_{OH}-\epsilon V_{IH} V_{IL}","['multivariable-calculus', 'optimization']"
24,How to find the function of a surface,How to find the function of a surface,,"Let’s say we have $f(x)=\sqrt{x}$ in range $\,x\in[0,5]\,$ and we revolved around the $x$ -axis what would the function $z=z(x,y)$ of this new surface? How can I find this surface? I have the volume of this surface: $$\int_{0}^{5}\pi xdx = V$$ in 2D the rate of change of the area under the curve is $f(x)$ will this be the same in 2D with the volume, how can I find the equation $\,z=z(x,y)\,?$ I was trying: $$A = \int\pi xdx$$ $$\frac{dA}{dx} = \pi x$$","Let’s say we have in range and we revolved around the -axis what would the function of this new surface? How can I find this surface? I have the volume of this surface: in 2D the rate of change of the area under the curve is will this be the same in 2D with the volume, how can I find the equation I was trying:","f(x)=\sqrt{x} \,x\in[0,5]\, x z=z(x,y) \int_{0}^{5}\pi xdx = V f(x) \,z=z(x,y)\,? A = \int\pi xdx \frac{dA}{dx} = \pi x","['calculus', 'multivariable-calculus', 'surfaces', 'parametric', 'solid-of-revolution']"
25,Why can the del operator cross product a triple integral be placed inside the triple integral?,Why can the del operator cross product a triple integral be placed inside the triple integral?,,"Consider the (electric) vector field $$\pmb{E}(\pmb{r})=k_e\iiint_V \frac{\rho(\pmb{r_s})}{\lVert \pmb{r}-\pmb{r_s} \lVert^2}\frac{\pmb{r}-\pmb{r_s}}{\lVert \pmb{r}-\pmb{r_s} \lVert}d\tau\tag{1}$$ where $\pmb{r_s}$ is the position vector of a source charge, $\rho(\pmb{r_s})$ is the charge-per-unit-volume at $\pmb{r_s}$ , and $\pmb{r}$ is the fixed position vector of a field point. Suppose we want to calculate $\nabla\times \pmb{E}$ . Here is one way that I saw in a physics book $$\nabla\times\pmb{E}=\iiint_V\frac{\pmb{r}-\pmb{r_s}}{\lVert \pmb{r}-\pmb{r_s} \lVert^3}\rho(\pmb{r_s})d\tau\tag{2}$$ Here is the step I don't know how to justify $$=k_e \iiint_V \nabla\times \left ( \frac{\pmb{r}-\pmb{r_s}}{\lVert \pmb{r}-\pmb{r_s} \lVert^3} \right )\rho(\pmb{r_s})d\tau\tag{3}$$ $$=0\tag{4}$$ I am fine with the final step: it results from the fact that $\nabla\times (r^n\hat{r})=0$ . But why can the cross product pass through the triple integral?","Consider the (electric) vector field where is the position vector of a source charge, is the charge-per-unit-volume at , and is the fixed position vector of a field point. Suppose we want to calculate . Here is one way that I saw in a physics book Here is the step I don't know how to justify I am fine with the final step: it results from the fact that . But why can the cross product pass through the triple integral?",\pmb{E}(\pmb{r})=k_e\iiint_V \frac{\rho(\pmb{r_s})}{\lVert \pmb{r}-\pmb{r_s} \lVert^2}\frac{\pmb{r}-\pmb{r_s}}{\lVert \pmb{r}-\pmb{r_s} \lVert}d\tau\tag{1} \pmb{r_s} \rho(\pmb{r_s}) \pmb{r_s} \pmb{r} \nabla\times \pmb{E} \nabla\times\pmb{E}=\iiint_V\frac{\pmb{r}-\pmb{r_s}}{\lVert \pmb{r}-\pmb{r_s} \lVert^3}\rho(\pmb{r_s})d\tau\tag{2} =k_e \iiint_V \nabla\times \left ( \frac{\pmb{r}-\pmb{r_s}}{\lVert \pmb{r}-\pmb{r_s} \lVert^3} \right )\rho(\pmb{r_s})d\tau\tag{3} =0\tag{4} \nabla\times (r^n\hat{r})=0,"['integration', 'multivariable-calculus', 'vector-analysis', 'cross-product', 'curl']"
26,Application of Sobolev's inequality,Application of Sobolev's inequality,,"Given $K\subset \mathbb{R}$ compact, we consider a regular function $f:K^n\to \mathbb{R}$ whose mean value is $0$ . I was wondering if we can get the inequality $ n\|f\|_{L^q(K^n)}\leq C \sum_{i=1}^n\|\nabla_{x_i}f\|_{L^2(K^n)}$ for some $C$ that only depends on $K$ (not on $n$ ), where $q$ satisfies $1/q=1/2-1/n.$ I suppose that if this is true, it should be a consequence of applying Sobolev inequality . However, the constant here depends on $n$ .","Given compact, we consider a regular function whose mean value is . I was wondering if we can get the inequality for some that only depends on (not on ), where satisfies I suppose that if this is true, it should be a consequence of applying Sobolev inequality . However, the constant here depends on .",K\subset \mathbb{R} f:K^n\to \mathbb{R} 0  n\|f\|_{L^q(K^n)}\leq C \sum_{i=1}^n\|\nabla_{x_i}f\|_{L^2(K^n)} C K n q 1/q=1/2-1/n. n,"['real-analysis', 'functional-analysis', 'multivariable-calculus', 'partial-differential-equations', 'lebesgue-integral']"
27,Change the order of this double integration,Change the order of this double integration,,"Suppose I want to determine: $$ \int_{0}^{1}\int_{x}^{e^{x}}2y-x\;dydx $$ In this order, i.e. integrating with respect to y then x, it is straight forward enough to evaluate to: $$ \int_{0}^{1}\int_{x}^{e^{x}}2y-x\;dydx = \frac{e^{2}-3}{2}\quad\text{(*)} $$ However, looking at the domain of integration, I think it should be able to be evaluated by integrating with respect to x first then y last. I'm aware that this would mean changing the limits and I think it should become: $$ \int_{0}^{e}\int_{ln|y|}^{y}2y-x\:dxdy\quad\text{(**)} $$ However, after evaluating $\text{(**)}$ , it does not equal to $\text{(*)}$ . Can someone please explain to me, if I was to change the order of integration to become $dxdy$ , what should be the limits of the integration be? Thank you. Edit: I would like to add a diagram of the domain region, which is the region bounded by the blue, green, red, and orange curves. Perhaps @Davis Yoshida is implying that because ln(y) isn't define to be all positive, we have to split the region and add them together? However, this isn't what I want. If it is possible, can the order of the integration be swap to become dxdy without splitting?","Suppose I want to determine: In this order, i.e. integrating with respect to y then x, it is straight forward enough to evaluate to: However, looking at the domain of integration, I think it should be able to be evaluated by integrating with respect to x first then y last. I'm aware that this would mean changing the limits and I think it should become: However, after evaluating , it does not equal to . Can someone please explain to me, if I was to change the order of integration to become , what should be the limits of the integration be? Thank you. Edit: I would like to add a diagram of the domain region, which is the region bounded by the blue, green, red, and orange curves. Perhaps @Davis Yoshida is implying that because ln(y) isn't define to be all positive, we have to split the region and add them together? However, this isn't what I want. If it is possible, can the order of the integration be swap to become dxdy without splitting?","
\int_{0}^{1}\int_{x}^{e^{x}}2y-x\;dydx
 
\int_{0}^{1}\int_{x}^{e^{x}}2y-x\;dydx = \frac{e^{2}-3}{2}\quad\text{(*)}
 
\int_{0}^{e}\int_{ln|y|}^{y}2y-x\:dxdy\quad\text{(**)}
 \text{(**)} \text{(*)} dxdy","['integration', 'multivariable-calculus', 'definite-integrals', 'multiple-integral']"
28,Integral of function with two linearily scaled variables,Integral of function with two linearily scaled variables,,"I have a known function $w = w(x,y)$ and I want to find $N(a,b)$ where $a$ and $b$ are variables independent of $x$ . \begin{equation} N(a,b) = \int_{-\infty}^{\infty} w(ax,bx) dx \end{equation} Assuming the integral converges, is there a way to simplify this integral? For example, in the 1D case I can use substitution to prove that: \begin{equation} \int_{-\infty}^{\infty} f(ax) dx = \frac{1}{a}\int_{-\infty}^{\infty} f(x) dx \end{equation} Is there a similar type of simplification I can do for the 2D case? Ideally, I'd like to suck the $a$ and $b$ outside the integral.","I have a known function and I want to find where and are variables independent of . Assuming the integral converges, is there a way to simplify this integral? For example, in the 1D case I can use substitution to prove that: Is there a similar type of simplification I can do for the 2D case? Ideally, I'd like to suck the and outside the integral.","w = w(x,y) N(a,b) a b x \begin{equation}
N(a,b) = \int_{-\infty}^{\infty} w(ax,bx) dx
\end{equation} \begin{equation}
\int_{-\infty}^{\infty} f(ax) dx = \frac{1}{a}\int_{-\infty}^{\infty} f(x) dx
\end{equation} a b","['calculus', 'multivariable-calculus']"
29,Volume Under a Ceiling,Volume Under a Ceiling,,"I have run into an interesting volume problem that I can't seem to figure out. I was recently in a school gym and noticed the interesting design of the ceiling and thought back to Calculus III and finding the volume under surfaces. I have been trying to come up with a rough equation that models the design of the ceiling but have not been able to. The floor is roughly 100ftx100ft and the center of the ceiling is roughly 28ft. I believe that the four curved sections are parabolic. The equations I have been playing with are $ f(x,y) = (-1/16)x^2 + 6$ and $ f(x,y) = (-1/16) y^2 + 6$ .These have just been jumping-off points and I figured their intersection region is roughly the shape of the ceiling. I'm not sure if trying to parameterize these equations is the way to go, or if I'm totally off base altogether. Any help would be appreciated!","I have run into an interesting volume problem that I can't seem to figure out. I was recently in a school gym and noticed the interesting design of the ceiling and thought back to Calculus III and finding the volume under surfaces. I have been trying to come up with a rough equation that models the design of the ceiling but have not been able to. The floor is roughly 100ftx100ft and the center of the ceiling is roughly 28ft. I believe that the four curved sections are parabolic. The equations I have been playing with are and .These have just been jumping-off points and I figured their intersection region is roughly the shape of the ceiling. I'm not sure if trying to parameterize these equations is the way to go, or if I'm totally off base altogether. Any help would be appreciated!"," f(x,y) = (-1/16)x^2 + 6  f(x,y) = (-1/16) y^2 + 6","['multivariable-calculus', 'volume', 'surfaces']"
30,When can a double integral be interpreted as a surface area?,When can a double integral be interpreted as a surface area?,,"Let $D$ be a closed, bounded domain in $\mathbb R^2$ and let $\vec r(u,v) = \langle x(u,v), y(u,v), z(u,v) \rangle$ for $(u, v) \in D$ be a parametrization of a smooth surface $S \subseteq \mathbb R^3$ .  Then the area of $S$ is $$\iint_D \| \vec r_u \times \vec r_v \| \,  dA$$ I am interested in what might be considered a kind of converse of this: Is it always possible to interpret an integral $\iint_D f \, dA$ as the area of some surface? More precisely, given a (non-negative) function $f:\mathbb R^2 \to \mathbb R$ , defined on some domain $D$ , under what conditions can we find a surface $S \subseteq \mathbb R^3$ and a parametrization $\vec r(u,v): D \to S$ such that $$f(u,v) = \| \vec r_u \times \vec r_v \|$$ so that consequently $$\iint_D f \, dA = \iint_D \| \vec r_u \times \vec r_v \| \,  dA \, ?$$","Let be a closed, bounded domain in and let for be a parametrization of a smooth surface .  Then the area of is I am interested in what might be considered a kind of converse of this: Is it always possible to interpret an integral as the area of some surface? More precisely, given a (non-negative) function , defined on some domain , under what conditions can we find a surface and a parametrization such that so that consequently","D \mathbb R^2 \vec r(u,v) = \langle x(u,v), y(u,v), z(u,v) \rangle (u, v) \in D S \subseteq \mathbb R^3 S \iint_D \| \vec r_u \times \vec r_v \| \,  dA \iint_D f \, dA f:\mathbb R^2 \to \mathbb R D S \subseteq \mathbb R^3 \vec r(u,v): D \to S f(u,v) = \| \vec r_u \times \vec r_v \| \iint_D f \, dA = \iint_D \| \vec r_u \times \vec r_v \| \,  dA \, ?",['multivariable-calculus']
31,Continuity of function of two variable by Epsilon-Delta technique.,Continuity of function of two variable by Epsilon-Delta technique.,,"Given a function $f(x,y)= \frac{x^3y^2+x}{x^2+y^2}$ when $(x,y) \neq (0,0)$ . Show that the function $f(x,y)$ is continuous at $(1,1)$ by Epsilon-Delta definition. I can show it easily by the fraction of two continuous functions is continuous. But I need to prove it directly by Epsilon-Delta technique. I'm not able to think, how to do it. Please give some hint! Effort: For any given $\epsilon>0$ , we have $|f(x,y)-1|= |\frac{x^3y^2+x}{x^2+y^2}-1|=|\frac{x^3y^2+x-x^2-y^2}{x^2+y^2}|$ Now, what's next?","Given a function when . Show that the function is continuous at by Epsilon-Delta definition. I can show it easily by the fraction of two continuous functions is continuous. But I need to prove it directly by Epsilon-Delta technique. I'm not able to think, how to do it. Please give some hint! Effort: For any given , we have Now, what's next?","f(x,y)= \frac{x^3y^2+x}{x^2+y^2} (x,y) \neq (0,0) f(x,y) (1,1) \epsilon>0 |f(x,y)-1|= |\frac{x^3y^2+x}{x^2+y^2}-1|=|\frac{x^3y^2+x-x^2-y^2}{x^2+y^2}|","['multivariable-calculus', 'epsilon-delta']"
32,Boundary Point of Subdifferential as the Limit of a Sequence of Gradients,Boundary Point of Subdifferential as the Limit of a Sequence of Gradients,,"Let $\Omega\subset\mathbb{R}^n$ be an open convex set. Let $f:\Omega\to\mathbb{R}$ be a convex function. We know that $f$ is differentiable almost everywhere. The subdifferential of $f$ at $x\in\Omega$ is defined as $$ \partial f(x)=\{g\in\mathbb{R}^n:f(y)\ge f(x)+g^T(y-x)\ \forall\,y\in\Omega\}. $$ My question: Is it true that for every $x\in\Omega$ , for every boundary point $g$ of $\partial f(x)$ , there always exists a sequence $\{x_n\}\subset\Omega$ such that $f$ is differentiable at each $x_n$ , $x_n\to x$ , and $\nabla f(x_n)\to g$ ? My attempt: This is not true for general $g\in\partial f(x)$ . For example, in $\mathbb{R}$ , the convex function $f(x)=|x|$ is differentiable on $\mathbb{R}\backslash{\{0\}}$ and $\partial f(0)=[-1,1]$ . Since $f'(y)\in\{-1,1\}$ for all $y\neq 0$ , then there does not exist $\{x_n\}\subset\mathbb{R}\backslash\{0\}$ such that $x_n\to 0$ and $\nabla f(x_n)\to 0\in\partial f(0)$ . I believe the statement is true at least in the speical case of $\mathbb{R}$ . Since $\partial f(x)\subset\mathbb{R}$ is closed and convex, then it must be an interval. So I suppose it should be true that $$ \partial f(x)=\left[\liminf_{y\to x}\frac{f(y)-f(x)}{y-x},\limsup_{y\to x}\frac{f(y)-f(x)}{y-x}\right]. $$ This should help prove the statement. I am a beginner in convex analysis. Any help would be appreciated.","Let be an open convex set. Let be a convex function. We know that is differentiable almost everywhere. The subdifferential of at is defined as My question: Is it true that for every , for every boundary point of , there always exists a sequence such that is differentiable at each , , and ? My attempt: This is not true for general . For example, in , the convex function is differentiable on and . Since for all , then there does not exist such that and . I believe the statement is true at least in the speical case of . Since is closed and convex, then it must be an interval. So I suppose it should be true that This should help prove the statement. I am a beginner in convex analysis. Any help would be appreciated.","\Omega\subset\mathbb{R}^n f:\Omega\to\mathbb{R} f f x\in\Omega 
\partial f(x)=\{g\in\mathbb{R}^n:f(y)\ge f(x)+g^T(y-x)\ \forall\,y\in\Omega\}.
 x\in\Omega g \partial f(x) \{x_n\}\subset\Omega f x_n x_n\to x \nabla f(x_n)\to g g\in\partial f(x) \mathbb{R} f(x)=|x| \mathbb{R}\backslash{\{0\}} \partial f(0)=[-1,1] f'(y)\in\{-1,1\} y\neq 0 \{x_n\}\subset\mathbb{R}\backslash\{0\} x_n\to 0 \nabla f(x_n)\to 0\in\partial f(0) \mathbb{R} \partial f(x)\subset\mathbb{R} 
\partial f(x)=\left[\liminf_{y\to x}\frac{f(y)-f(x)}{y-x},\limsup_{y\to x}\frac{f(y)-f(x)}{y-x}\right].
","['real-analysis', 'multivariable-calculus', 'convex-analysis', 'convex-optimization', 'subgradient']"
33,Stokes' Theorem and vector integrand in curvilinear coordinates,Stokes' Theorem and vector integrand in curvilinear coordinates,,"I want to compute the following vector integral: $$ \iint_\Omega d\Omega \{\nabla_s\times \mathbf{F}\}  \tag{1}\label{eq1}$$ Note that $d\Omega$ is a scalar surface element, the result should therefore be vectorial. Further we can not apply Stoke's Theorem, as it would require an oriented surface element $\mathbf{d\Omega}$ such that the integrand becomes a scalar. In the particular case I'm studying $\mathbf{F}$ is tangential to the surface $\Omega$ , so in curvilinear coordinates, using Dupin's orthogonal system $\hat{\mathbf{v}}_1, \hat{\mathbf{v}}_2, \hat{\mathbf{n}}$ : $$ F_n=0 \implies \mathbf{F} = F_1 \hat{\mathbf{v}}_1+ F_2 \hat{\mathbf{v}}_2, $$ with $\hat{\mathbf{v}}_1, \hat{\mathbf{v}}_2$ being aligned to the lines of curvature. The surface curl $\nabla_s\times$ is defined as $$ \nabla_s \times \mathbf{G} =  (\nabla_s G_n)\times\hat{\mathbf{n}}+\frac{G_2}{R_2}\hat{\mathbf{v}_1}-\frac{G_1}{R_1} \hat{\mathbf{v}_2}  + \hat{\mathbf{n}} \nabla_s\cdot(\mathbf{G}\times \hat{\mathbf{z}}) $$ This definition was found in ""J. van Bladel, Electromagnetic fields, 2nd ed., 2007"" and has the advantage that there is no normal derivative applied to the tangential components, which is important when dealing with surface vectors that are non-zero only on the surface $\Omega$ . The quantities $R_1, R_2$ denote the radii of curvature. Now, considering first a plane in Cartesian coordinates, we have $R_1=R_2=\infty$ and a constant normal which is invariant to the location, e.g., $\hat{\mathbf{n}}=\hat{\mathbf{z}},\hat{\mathbf{v}_1}=\hat{\mathbf{x}},\hat{\mathbf{v}_2}=\hat{\mathbf{y}}$ , and therefore $$ \nabla_s \times \mathbf{F} =  \hat{\mathbf{z}} \nabla_s\cdot(\mathbf{F}\times \hat{\mathbf{z}}) \\ \iint_\Omega d\Omega \{\nabla_s\times \mathbf{F}\} = \hat{\mathbf{z}} \iint_\Omega d\Omega \{ \nabla_s\cdot(\mathbf{F}\times \hat{\mathbf{z}}) \}. $$ In the last equation it was possible to move $\hat{\mathbf{z}}$ in front of the integral because $\hat{\mathbf{z}}$ does not vary over $(x,y)$ . The integrand is now a scalar, and thus the (Surface) Divergence Theorem can be applied which yields $$ \hat{\mathbf{z}} \iint_\Omega d\Omega \{ \nabla_s\cdot(\mathbf{F}\times \hat{\mathbf{z}}) \} = \hat{\mathbf{z}} \int_{\partial\Omega} dc \{ (\mathbf{F}\times \hat{\mathbf{z}})\cdot \hat{\mathbf{m}} \} = \hat{\mathbf{z}} \int_{\partial\Omega} dc \{ \mathbf{F} \cdot \hat{\mathbf{t}}\}, $$ where $\hat{\mathbf{t}}$ is the unit vector aligned to the boundary $\partial\Omega$ and $\hat{\mathbf{m}}$ is defined such that $\hat{\mathbf{n}}\times\hat{\mathbf{m}}=\hat{\mathbf{t}}$ . In other words, a transformation of $\eqref{eq1}$ into a boundary line integral is possible for a plane in Cartesian coordinates by using Stokes' or the Divergence Theorem, because the normal is invariant. As an application, e.g., take the case that $\mathbf{F}$ vanishes outside a circular region, centered at the origin. If the boundary $\partial\Omega$ encloses this region, we find $$ \iint_\Omega d\Omega \{\nabla_s\times \mathbf{F}\}  =0, $$ because $\mathbf{F} \cdot \hat{\mathbf{t}}=0$ on the entire boundary $\partial\Omega$ . Now I would like to check if the same applies for curved surfaces. More importantly, I'm wondering if the surface $\Omega$ is closed (consider e.g. a sphere), that is it has no Stokes Boundary $\partial\Omega$ , is it true that $$ \iint_\Omega d\Omega \{\nabla_s\times \mathbf{F}\}  = 0 ?$$ I was not able to find any hint in my favorite text books  ... so I'm happy for any advice. Thank you.","I want to compute the following vector integral: Note that is a scalar surface element, the result should therefore be vectorial. Further we can not apply Stoke's Theorem, as it would require an oriented surface element such that the integrand becomes a scalar. In the particular case I'm studying is tangential to the surface , so in curvilinear coordinates, using Dupin's orthogonal system : with being aligned to the lines of curvature. The surface curl is defined as This definition was found in ""J. van Bladel, Electromagnetic fields, 2nd ed., 2007"" and has the advantage that there is no normal derivative applied to the tangential components, which is important when dealing with surface vectors that are non-zero only on the surface . The quantities denote the radii of curvature. Now, considering first a plane in Cartesian coordinates, we have and a constant normal which is invariant to the location, e.g., , and therefore In the last equation it was possible to move in front of the integral because does not vary over . The integrand is now a scalar, and thus the (Surface) Divergence Theorem can be applied which yields where is the unit vector aligned to the boundary and is defined such that . In other words, a transformation of into a boundary line integral is possible for a plane in Cartesian coordinates by using Stokes' or the Divergence Theorem, because the normal is invariant. As an application, e.g., take the case that vanishes outside a circular region, centered at the origin. If the boundary encloses this region, we find because on the entire boundary . Now I would like to check if the same applies for curved surfaces. More importantly, I'm wondering if the surface is closed (consider e.g. a sphere), that is it has no Stokes Boundary , is it true that I was not able to find any hint in my favorite text books  ... so I'm happy for any advice. Thank you."," \iint_\Omega d\Omega \{\nabla_s\times \mathbf{F}\}  \tag{1}\label{eq1} d\Omega \mathbf{d\Omega} \mathbf{F} \Omega \hat{\mathbf{v}}_1, \hat{\mathbf{v}}_2, \hat{\mathbf{n}}  F_n=0 \implies \mathbf{F} = F_1 \hat{\mathbf{v}}_1+ F_2 \hat{\mathbf{v}}_2,  \hat{\mathbf{v}}_1, \hat{\mathbf{v}}_2 \nabla_s\times  \nabla_s \times \mathbf{G} =  (\nabla_s G_n)\times\hat{\mathbf{n}}+\frac{G_2}{R_2}\hat{\mathbf{v}_1}-\frac{G_1}{R_1} \hat{\mathbf{v}_2}  + \hat{\mathbf{n}} \nabla_s\cdot(\mathbf{G}\times \hat{\mathbf{z}})  \Omega R_1, R_2 R_1=R_2=\infty \hat{\mathbf{n}}=\hat{\mathbf{z}},\hat{\mathbf{v}_1}=\hat{\mathbf{x}},\hat{\mathbf{v}_2}=\hat{\mathbf{y}}  \nabla_s \times \mathbf{F} =  \hat{\mathbf{z}} \nabla_s\cdot(\mathbf{F}\times \hat{\mathbf{z}}) \\
\iint_\Omega d\Omega \{\nabla_s\times \mathbf{F}\} = \hat{\mathbf{z}} \iint_\Omega d\Omega \{ \nabla_s\cdot(\mathbf{F}\times \hat{\mathbf{z}}) \}.
 \hat{\mathbf{z}} \hat{\mathbf{z}} (x,y) 
\hat{\mathbf{z}} \iint_\Omega d\Omega \{ \nabla_s\cdot(\mathbf{F}\times \hat{\mathbf{z}}) \} = \hat{\mathbf{z}} \int_{\partial\Omega} dc \{ (\mathbf{F}\times \hat{\mathbf{z}})\cdot \hat{\mathbf{m}} \} = \hat{\mathbf{z}} \int_{\partial\Omega} dc \{ \mathbf{F} \cdot \hat{\mathbf{t}}\},
 \hat{\mathbf{t}} \partial\Omega \hat{\mathbf{m}} \hat{\mathbf{n}}\times\hat{\mathbf{m}}=\hat{\mathbf{t}} \eqref{eq1} \mathbf{F} \partial\Omega  \iint_\Omega d\Omega \{\nabla_s\times \mathbf{F}\}  =0,  \mathbf{F} \cdot \hat{\mathbf{t}}=0 \partial\Omega \Omega \partial\Omega  \iint_\Omega d\Omega \{\nabla_s\times \mathbf{F}\}  = 0 ?","['calculus', 'multivariable-calculus', 'vector-analysis', 'stokes-theorem']"
34,Calculating double integral using variables substitution,Calculating double integral using variables substitution,,"$\displaystyle   D = \left\lbrace \left. \rule{0pt}{12pt} (x,y) \; \right| \;  3 x^2 + 6 y^2 \leq 1  \right\rbrace$ Calculate $\displaystyle    \iint_D  \frac{ x^2 }{ ( 3 x^2 + 6 y^2 )^{ 3/2 } }  \; dx dy{}$ . Attempt: $x=\frac{r}{\sqrt3}cost,y=\frac{r}{\sqrt6}sint \implies |J|=\sqrt{\frac{2}{3}}r$ $3 x^2 + 6 y^2 \leq 1 \implies 0\leq r \leq 1$ $\iint_D  \frac{ x^2 }{ ( 3 x^2 + 6 y^2 )^{ 3/2 } }  \; dx dy{} =\int _0^1\:\int _0^{2\pi }\:\frac{cos^2t\sqrt{2}}{3\sqrt{3}}dtdr = \frac{\sqrt{2}\pi }{3\sqrt{3}}$ My answer isn't corect , can't find out what is wrong. Appreciate any help.","Calculate . Attempt: My answer isn't corect , can't find out what is wrong. Appreciate any help.","\displaystyle
  D = \left\lbrace \left. \rule{0pt}{12pt} (x,y) \; \right| \;  3 x^2 + 6 y^2 \leq 1  \right\rbrace \displaystyle
   \iint_D  \frac{ x^2 }{ ( 3 x^2 + 6 y^2 )^{ 3/2 } }  \; dx dy{} x=\frac{r}{\sqrt3}cost,y=\frac{r}{\sqrt6}sint \implies |J|=\sqrt{\frac{2}{3}}r 3 x^2 + 6 y^2 \leq 1 \implies 0\leq r \leq 1 \iint_D  \frac{ x^2 }{ ( 3 x^2 + 6 y^2 )^{ 3/2 } }  \; dx dy{} =\int _0^1\:\int _0^{2\pi }\:\frac{cos^2t\sqrt{2}}{3\sqrt{3}}dtdr = \frac{\sqrt{2}\pi }{3\sqrt{3}}",['multivariable-calculus']
35,Connection between the $\Bbb R^3$ and $\Bbb R^2$ gradients of a level curve of a hill and the level surface in $\Bbb R^4$ represented by the same hill,Connection between the  and  gradients of a level curve of a hill and the level surface in  represented by the same hill,\Bbb R^3 \Bbb R^2 \Bbb R^4,"I understand (or at least I think I understand) intuitively why the gradient of a surface in $\Bbb R^3$ (like a hill) must be perpendicular to the level sets that cut through the hill at various heights and that this vector represents the magnitude and direction of most rapid ascent. I think I also understand why the gradient is normal to a plane tangent to a surface at a particular point. It took me a while to reconcile these two things because I wondered how the gradient could be both of those things at once and the conclusion that I came to was that one gradient was a gradient in $\Bbb R^2$ describing the direction of greatest ascent for the hill in $\Bbb R^3$ and the gradient that describes the vector normal to a plane tangent to a surface (in this case, the hill) at a particular point is the gradient of the level surface in $\Bbb R^3$ , the hill being now being considered a level surface to a hyper-surface in $\Bbb R^4$ which is described by a different function. I guess firstly, I would like to know if my intuition is correct so far and then secondly, if it is, I would like to know if the gradient of the hyper-surface (the normal of a plane that's tangent to a particular point on the hill with the hill being considered a level surface to the hyper-surface) has any connection to the two dimensional gradient which lives in the $xy$ -plane and shows the direction and magnitude of greatest ascent up the hill and is orthogonal to the level set that lives in the $xy$ -plane. What my brain was conjuring up was that maybe the $\Bbb R^3$ gradient was connected to the $\Bbb R^2$ gradient in that the $\Bbb R^2$ gradient was the projection of the $\Bbb R^3$ gradient/normal to the tangent plane, or perhaps the normal to the tangent plane's ""underside"" when considering a particular point on the hill. Essentially that the $\Bbb R^3$ gradient (or negative $\Bbb R^3$ gradient) could be considered to produce a ""shadow"" projection beneath it on the $xy$ -plane and that projective shadow would correspond to the $\Bbb R^2$ gradient at that particular point on the hill's projection. I am struggling to explain this in words so I tried to illustrate what I mean on the attached image. I feel like I am not understanding something here, but I don't exactly know what it is. Any input would be greatly appreciated. Thanks.","I understand (or at least I think I understand) intuitively why the gradient of a surface in (like a hill) must be perpendicular to the level sets that cut through the hill at various heights and that this vector represents the magnitude and direction of most rapid ascent. I think I also understand why the gradient is normal to a plane tangent to a surface at a particular point. It took me a while to reconcile these two things because I wondered how the gradient could be both of those things at once and the conclusion that I came to was that one gradient was a gradient in describing the direction of greatest ascent for the hill in and the gradient that describes the vector normal to a plane tangent to a surface (in this case, the hill) at a particular point is the gradient of the level surface in , the hill being now being considered a level surface to a hyper-surface in which is described by a different function. I guess firstly, I would like to know if my intuition is correct so far and then secondly, if it is, I would like to know if the gradient of the hyper-surface (the normal of a plane that's tangent to a particular point on the hill with the hill being considered a level surface to the hyper-surface) has any connection to the two dimensional gradient which lives in the -plane and shows the direction and magnitude of greatest ascent up the hill and is orthogonal to the level set that lives in the -plane. What my brain was conjuring up was that maybe the gradient was connected to the gradient in that the gradient was the projection of the gradient/normal to the tangent plane, or perhaps the normal to the tangent plane's ""underside"" when considering a particular point on the hill. Essentially that the gradient (or negative gradient) could be considered to produce a ""shadow"" projection beneath it on the -plane and that projective shadow would correspond to the gradient at that particular point on the hill's projection. I am struggling to explain this in words so I tried to illustrate what I mean on the attached image. I feel like I am not understanding something here, but I don't exactly know what it is. Any input would be greatly appreciated. Thanks.",\Bbb R^3 \Bbb R^2 \Bbb R^3 \Bbb R^3 \Bbb R^4 xy xy \Bbb R^3 \Bbb R^2 \Bbb R^2 \Bbb R^3 \Bbb R^3 \Bbb R^3 xy \Bbb R^2,['multivariable-calculus']
36,I am confused with simply how to solve local optimization problems of multiple variables on a curve.,I am confused with simply how to solve local optimization problems of multiple variables on a curve.,,"For $f(x,y) = x^3 + x^2y - y^2 - 4y$ , find and classify all critical points of $f$ on the line $y = x$ . My Attempt: $$\frac{\partial}{\partial x}f(x,y) = 3x^2 + 2xy = 0 \implies x(3x + 2y) = 0 \implies x = 0,-4,1$$ $$\frac{\partial}{\partial y}f(x,y) = x^2 - 2y - 4 = 0 \implies x^2 - 2y = 4 \implies y = -2,6,-3/2$$ So, the critical points are: $(0,-2), (-4,6), (1,-3/2)$ The first thing I thought was that $y = x$ was the domain of allowed critical points, but then that would mean that no critical points would work. Next I thought we needed to plug in $x$ for $y$ in the partial derivatives to find the true critical points, but that didn't work either. What is confusing to me is when I look at the answer key, it says the critical points are $(1,1)$ and $(-2/3,-2/3)$ , but when I plug these points into the partial derivatives set equal to zero, neither of them satisfy the equations. I know I am missing something here, but I just don't know what it is.","For , find and classify all critical points of on the line . My Attempt: So, the critical points are: The first thing I thought was that was the domain of allowed critical points, but then that would mean that no critical points would work. Next I thought we needed to plug in for in the partial derivatives to find the true critical points, but that didn't work either. What is confusing to me is when I look at the answer key, it says the critical points are and , but when I plug these points into the partial derivatives set equal to zero, neither of them satisfy the equations. I know I am missing something here, but I just don't know what it is.","f(x,y) = x^3 + x^2y - y^2 - 4y f y = x \frac{\partial}{\partial x}f(x,y) = 3x^2 + 2xy = 0 \implies x(3x + 2y) = 0 \implies x = 0,-4,1 \frac{\partial}{\partial y}f(x,y) = x^2 - 2y - 4 = 0 \implies x^2 - 2y = 4 \implies y = -2,6,-3/2 (0,-2), (-4,6), (1,-3/2) y = x x y (1,1) (-2/3,-2/3)","['multivariable-calculus', 'optimization']"
37,The integral $\int_A f$ exists $\iff$ the series $\sum_i \int_A \phi_i|f|$ converges.,The integral  exists  the series  converges.,\int_A f \iff \sum_i \int_A \phi_i|f|,"The theorem 16.5 in Munkres' analysis on manifolds reads: Let $A$ be open in $\mathbb R^n$ ; let $f : A → ℝ$ be continuous. Let $\{ϕ_i\}$ be a partition of unity on $A$ having compact supports. The integral $∫_A f$ exists if and only if the series $\sum_i \int_A \phi_i|f|$ converges and in this case $∫_A f=\sum_i \int_A \phi_if.$ The theorem 16.3 reads: Let $\mathscr A$ be a collection of open sets in $ℝ^n$ ; let $A$ be their union. There exists a sequence $ϕ_1, ϕ_2,… $ of continuous functions $ϕ_i : ℝ^n → ℝ$ such that: (1) $ϕ_i(x) ≥ 0$ for all $x$ . (2) The set $S_i = \text{Support } ϕ_i$ is contained in $A$ . (3) Each point of $A$ has a neighborhood that intersects only finitely many of the sets $S_i$ . (4) $\sum_{i=1}^\infty \phi_i(x)=1 $ for each x ∈ A. (5) The functions $ϕ_i$ are of class $C^∞$ . (6) The sets $S_i$ are compact. (7) For each $i$ , the set $S_i$ is contained in an element of $A$ . A collection of functions $\{ϕ_i\}$ satisfying conditions $(1)-(4)$ is called a partition of unity on $A$ . If it satisfies (5), it is said to be of class $C^∞$ ; if it satisfies (6), it is said to have compact supports; if it satisfies (7), it said to be dominated by the collection $\mathscr A$ . I don't understand the following highlighted part in the proof of theorem 16.5. Suppose that $f$ is non negative on $A$ and that the series $\sum_i \int_A \phi_i|f|$ converges. Let $D$ be a compact rectifiable subset of $A$ . There exists an $M$ such that for all $i > M$ , the function $ϕ_i$ vanishes identically on $D$ for $x ∈ D$ . My question is: Why is the highlighted part in the last para true? Please help. Thanks","The theorem 16.5 in Munkres' analysis on manifolds reads: Let be open in ; let be continuous. Let be a partition of unity on having compact supports. The integral exists if and only if the series converges and in this case The theorem 16.3 reads: Let be a collection of open sets in ; let be their union. There exists a sequence of continuous functions such that: (1) for all . (2) The set is contained in . (3) Each point of has a neighborhood that intersects only finitely many of the sets . (4) for each x ∈ A. (5) The functions are of class . (6) The sets are compact. (7) For each , the set is contained in an element of . A collection of functions satisfying conditions is called a partition of unity on . If it satisfies (5), it is said to be of class ; if it satisfies (6), it is said to have compact supports; if it satisfies (7), it said to be dominated by the collection . I don't understand the following highlighted part in the proof of theorem 16.5. Suppose that is non negative on and that the series converges. Let be a compact rectifiable subset of . There exists an such that for all , the function vanishes identically on for . My question is: Why is the highlighted part in the last para true? Please help. Thanks","A \mathbb R^n f : A → ℝ \{ϕ_i\} A ∫_A f \sum_i \int_A \phi_i|f| ∫_A f=\sum_i \int_A \phi_if. \mathscr A ℝ^n A ϕ_1, ϕ_2,…  ϕ_i : ℝ^n → ℝ ϕ_i(x) ≥ 0 x S_i = \text{Support } ϕ_i A A S_i \sum_{i=1}^\infty \phi_i(x)=1  ϕ_i C^∞ S_i i S_i A \{ϕ_i\} (1)-(4) A C^∞ \mathscr A f A \sum_i \int_A \phi_i|f| D A M i > M ϕ_i D x ∈ D","['real-analysis', 'multivariable-calculus', 'differential-topology']"
38,Tangent plane at a point to surface $\mathbf{F}=\mathbf{0}$,Tangent plane at a point to surface,\mathbf{F}=\mathbf{0},"Let $\mathbf{F}:\mathbb{R}^5\to\mathbb{R}^3, \mathbf{F}\begin{pmatrix}x_1\\x_2\\y_1\\y_2\\y_3\end{pmatrix}=\begin{bmatrix}2x_1+x_2+y_1+y_3-1\\x_1x_2^3+x_1y_1+x_2^2y_2^3-y_2y_3\\x_2y_1y_3+x_1y_1^2+y_2y_3^2\end{bmatrix}$ and $\mathbf{a}=\begin{bmatrix}0\\1\\-1\\1\\1\end{bmatrix}$ . The equation $\mathbf{F}=\mathbf{0}$ defines $\mathbf{y}=\begin{bmatrix}y_1\\y_2\\y_3\end{bmatrix}$ as a function of $\mathbf{x}=\begin{bmatrix}x_1\\x_2\end{bmatrix}$ near $\mathbf{a}$ since $\mathbf{F(a)}=\mathbf{0}$ , $\mathbf{F}$ is $\mathcal{C}^1$ and $\frac{\partial\mathbf{F}}{\partial\mathbf{y}}(\mathbf{a})=\begin{bmatrix}1 & 0 & 1\\0 & 1 & -1\\ 1 & 1 &1 \end{bmatrix}$ is nonsingular. So we have that near $\mathbf{a}$ there is a $\mathcal{C}^1$ function $\mathbf{\phi}$ such that $\mathbf{y}=\mathbf{\phi(x)}$ and $D\mathbf{\phi}\begin{pmatrix}0\\ 1\end{pmatrix}=-\left(\frac{\partial\mathbf{F}}{\partial\mathbf{y}}(\mathbf{a})\right)^{-1} \left(\frac{\partial\mathbf{F}}{\partial\mathbf{x}}(\mathbf{a})\right)=\begin{bmatrix}-3 & -5\\1 & 2\\1 & 4\end{bmatrix}$ . My question: is it correct to say that the tangent plane at $\mathbf{a}$ of the surface $\mathbf{F}=\mathbf{0}$ is the graph of $\mathbf{g}:\mathbb{R}^2\to\mathbb{R}^5$ $\mathbf{g}\begin{pmatrix}x_1\\x_2\end{pmatrix}=\begin{bmatrix}0\\1\\-1\\1\\1\end{bmatrix}+x_1\begin{bmatrix}1\\0\\\frac{\partial\phi}{\partial x_1}\begin{pmatrix}0\\1\end{pmatrix}\end{bmatrix}+x_2\begin{bmatrix}0\\1\\\frac{\partial\phi}{\partial x_2}\begin{pmatrix}0\\1\end{pmatrix}\end{bmatrix}=\begin{bmatrix}0\\1\\-1\\1\\1\end{bmatrix}+x_1\begin{bmatrix}1\\0\\-3\\1\\1\end{bmatrix}+x_2\begin{bmatrix}0\\1\\-5\\2\\4\end{bmatrix}?$","Let and . The equation defines as a function of near since , is and is nonsingular. So we have that near there is a function such that and . My question: is it correct to say that the tangent plane at of the surface is the graph of","\mathbf{F}:\mathbb{R}^5\to\mathbb{R}^3, \mathbf{F}\begin{pmatrix}x_1\\x_2\\y_1\\y_2\\y_3\end{pmatrix}=\begin{bmatrix}2x_1+x_2+y_1+y_3-1\\x_1x_2^3+x_1y_1+x_2^2y_2^3-y_2y_3\\x_2y_1y_3+x_1y_1^2+y_2y_3^2\end{bmatrix} \mathbf{a}=\begin{bmatrix}0\\1\\-1\\1\\1\end{bmatrix} \mathbf{F}=\mathbf{0} \mathbf{y}=\begin{bmatrix}y_1\\y_2\\y_3\end{bmatrix} \mathbf{x}=\begin{bmatrix}x_1\\x_2\end{bmatrix} \mathbf{a} \mathbf{F(a)}=\mathbf{0} \mathbf{F} \mathcal{C}^1 \frac{\partial\mathbf{F}}{\partial\mathbf{y}}(\mathbf{a})=\begin{bmatrix}1 & 0 & 1\\0 & 1 & -1\\ 1 & 1 &1 \end{bmatrix} \mathbf{a} \mathcal{C}^1 \mathbf{\phi} \mathbf{y}=\mathbf{\phi(x)} D\mathbf{\phi}\begin{pmatrix}0\\ 1\end{pmatrix}=-\left(\frac{\partial\mathbf{F}}{\partial\mathbf{y}}(\mathbf{a})\right)^{-1} \left(\frac{\partial\mathbf{F}}{\partial\mathbf{x}}(\mathbf{a})\right)=\begin{bmatrix}-3 & -5\\1 & 2\\1 & 4\end{bmatrix} \mathbf{a} \mathbf{F}=\mathbf{0} \mathbf{g}:\mathbb{R}^2\to\mathbb{R}^5 \mathbf{g}\begin{pmatrix}x_1\\x_2\end{pmatrix}=\begin{bmatrix}0\\1\\-1\\1\\1\end{bmatrix}+x_1\begin{bmatrix}1\\0\\\frac{\partial\phi}{\partial x_1}\begin{pmatrix}0\\1\end{pmatrix}\end{bmatrix}+x_2\begin{bmatrix}0\\1\\\frac{\partial\phi}{\partial x_2}\begin{pmatrix}0\\1\end{pmatrix}\end{bmatrix}=\begin{bmatrix}0\\1\\-1\\1\\1\end{bmatrix}+x_1\begin{bmatrix}1\\0\\-3\\1\\1\end{bmatrix}+x_2\begin{bmatrix}0\\1\\-5\\2\\4\end{bmatrix}?","['real-analysis', 'multivariable-calculus', 'implicit-function-theorem']"
39,Change of variables for an ODE,Change of variables for an ODE,,"Question: Use the substitution $u = 2 \sqrt{x}$ to show that the equation: $$x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}-(1-x)y=0$$ becomes: $$u^2\frac{d^2y}{du^2}+u\frac{dy}{du}-(u^2-u)y = 0$$ My attempt: $$x^2\left[\frac{d^2y}{du^2}\left(\frac{du}{dx} \right)^2+\frac{dy}{du}\left(\frac{d^2u}{dx^2}\right)\right]+x\frac{dy}{du}\frac{du}{dx}-(1-x)y=0$$ simplifying with $$\frac{du}{dx}=\frac{1}{\sqrt{x}},\ \frac{d^2u}{dx^2}=-\frac{1}{2\sqrt{x}^3}$$ yielding: $$x\frac{d^2y}{du^2}+\frac{1}{2}\sqrt{x}\frac{dy}{du}-(1-x)y = 0$$ now using $\sqrt{x}=\frac{1}{2}u$ , $x = \frac{1}{4}u^2$ and multiplying across by $4$ : $$\implies u^2\frac{d^2y}{du^2}+u\frac{dy}{du}-(4-u^2)y= 0$$ Which disagrees with the proposed solution. Have I made an error somewhere here so far or is there a further manipulation possible? Attempting the substitution in reverse I find: $$ x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}-(x-\frac{1}{2}\sqrt{x})y=0$$ in disagreement with the starting equation.","Question: Use the substitution to show that the equation: becomes: My attempt: simplifying with yielding: now using , and multiplying across by : Which disagrees with the proposed solution. Have I made an error somewhere here so far or is there a further manipulation possible? Attempting the substitution in reverse I find: in disagreement with the starting equation.","u = 2 \sqrt{x} x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}-(1-x)y=0 u^2\frac{d^2y}{du^2}+u\frac{dy}{du}-(u^2-u)y = 0 x^2\left[\frac{d^2y}{du^2}\left(\frac{du}{dx} \right)^2+\frac{dy}{du}\left(\frac{d^2u}{dx^2}\right)\right]+x\frac{dy}{du}\frac{du}{dx}-(1-x)y=0 \frac{du}{dx}=\frac{1}{\sqrt{x}},\ \frac{d^2u}{dx^2}=-\frac{1}{2\sqrt{x}^3} x\frac{d^2y}{du^2}+\frac{1}{2}\sqrt{x}\frac{dy}{du}-(1-x)y = 0 \sqrt{x}=\frac{1}{2}u x = \frac{1}{4}u^2 4 \implies u^2\frac{d^2y}{du^2}+u\frac{dy}{du}-(4-u^2)y= 0  x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}-(x-\frac{1}{2}\sqrt{x})y=0","['multivariable-calculus', 'partial-differential-equations', 'bessel-functions', 'change-of-variable']"
40,"Let $x = u\cos(v)$, and $y = u\sin(v)$, and assume $f(u,v)$ is given. Determine $f_x$ and $f_y$ in terms of $u, v, f_u$, and $f_v$.","Let , and , and assume  is given. Determine  and  in terms of , and .","x = u\cos(v) y = u\sin(v) f(u,v) f_x f_y u, v, f_u f_v","Let $x = u \cos(v)$ and $y = u \sin(v)$ , and assume $f(u, v)$ is given. Determine $f_x$ and $f_y$ in terms of $u$ , $v$ , $f_u$ , and $f_v$ . I thought of chain rule like $$ \frac{df}{dx} = \frac{df}{du} \cdot \frac{du}{dx}  + \frac{df}{dv} \cdot \frac{dv}{dx}.  $$ But in order to find $du/dx$ , I have to find $u$ in terms of $v$ and $x,y$ . Is there anyway to solve this?","Let and , and assume is given. Determine and in terms of , , , and . I thought of chain rule like But in order to find , I have to find in terms of and . Is there anyway to solve this?","x = u \cos(v) y = u \sin(v) f(u, v) f_x f_y u v f_u f_v 
\frac{df}{dx} = \frac{df}{du} \cdot \frac{du}{dx} 
+ \frac{df}{dv} \cdot \frac{dv}{dx}. 
 du/dx u v x,y","['multivariable-calculus', 'partial-derivative', 'chain-rule', 'jacobian', 'inverse-function-theorem']"
41,Double integral $\int_{0}^{1}\int_{0}^{1}\frac{x^2y^3\log{(x)}\log{(y)}}{(1-x^2)(1-y^2)(1-x^2y^2)}dxdy$,Double integral,\int_{0}^{1}\int_{0}^{1}\frac{x^2y^3\log{(x)}\log{(y)}}{(1-x^2)(1-y^2)(1-x^2y^2)}dxdy,"I'm trying to evaluate this double integral: $$I=\int_{0}^{1}\int_{0}^{1}\frac{x^2y^3\log{(x)}\log{(y)}}{(1-x^2)(1-y^2)(1-x^2y^2)}dxdy$$ Here is the closed-form: $$\frac{91π^4}{11520}+\frac{21}{32} ζ(3)-\frac{7}{8} ζ(3)\log{2}-\text{Li}_4 (\frac{1}{2})+\frac{π^2}{24}\log^2{2}-\frac{π^2}{16}\log{2}-\frac{1}{24}\log^4{2}$$ My try: because of its symmetry we can rewrite it as: $$2I=\int_{0}^{1}\int_{0}^{1}\frac{(x^2y^3+x^3y^2)\log{(x)}\log{(y)}}{(1-x^2)(1-y^2)(1-x^2y^2)}dxdy$$ Then use the substitution: $x\to \frac{1}{x}$ and $y\to \frac{1}{y}$ , now it becomes: $$I=\int_{1}^{\infty}\int_{1}^{\infty}\frac{\log{(x)}\log{(y)}}{x(1-x^2)(1-y^2)(x^2y^2-1)}dxdy$$ Seem I could get rid of the power of $x$ and $y$ from numerator, but after splitting: $$I=\int_{1}^{\infty}\frac{\log{(x)}}{x(1-x^2)}dx\int_{1}^{\infty}\frac{\log{(y)}}{(1-y^2)(x^2y^2-1)}dy$$ The latter integral gives (Mathematica): $$\fbox{$-\frac{-2 x \text{Li}_2\left(x^2\right)+8 x \text{Li}_2(x)+2 x \log ^2(-x)-2 x \log ^2(x)+\pi ^2}{8 \left(x^2-1\right)}\text{ if }\Im(x)\neq 0\lor -1<\Re\left(\frac{1}{x}\right)<1$}$$ Now, I don't know how to process further. Maybe I misdirected at the beginning. Can you guys help me with this? Thank you very much.","I'm trying to evaluate this double integral: Here is the closed-form: My try: because of its symmetry we can rewrite it as: Then use the substitution: and , now it becomes: Seem I could get rid of the power of and from numerator, but after splitting: The latter integral gives (Mathematica): Now, I don't know how to process further. Maybe I misdirected at the beginning. Can you guys help me with this? Thank you very much.",I=\int_{0}^{1}\int_{0}^{1}\frac{x^2y^3\log{(x)}\log{(y)}}{(1-x^2)(1-y^2)(1-x^2y^2)}dxdy \frac{91π^4}{11520}+\frac{21}{32} ζ(3)-\frac{7}{8} ζ(3)\log{2}-\text{Li}_4 (\frac{1}{2})+\frac{π^2}{24}\log^2{2}-\frac{π^2}{16}\log{2}-\frac{1}{24}\log^4{2} 2I=\int_{0}^{1}\int_{0}^{1}\frac{(x^2y^3+x^3y^2)\log{(x)}\log{(y)}}{(1-x^2)(1-y^2)(1-x^2y^2)}dxdy x\to \frac{1}{x} y\to \frac{1}{y} I=\int_{1}^{\infty}\int_{1}^{\infty}\frac{\log{(x)}\log{(y)}}{x(1-x^2)(1-y^2)(x^2y^2-1)}dxdy x y I=\int_{1}^{\infty}\frac{\log{(x)}}{x(1-x^2)}dx\int_{1}^{\infty}\frac{\log{(y)}}{(1-y^2)(x^2y^2-1)}dy \fbox{-\frac{-2 x \text{Li}_2\left(x^2\right)+8 x \text{Li}_2(x)+2 x \log ^2(-x)-2 x \log ^2(x)+\pi ^2}{8 \left(x^2-1\right)}\text{ if }\Im(x)\neq 0\lor -1<\Re\left(\frac{1}{x}\right)<1},['multivariable-calculus']
42,Existence of a fixed point to a vector-valued mapping,Existence of a fixed point to a vector-valued mapping,,"I have the following system: $$\begin{cases} F_1(x_1,x_2) \colon= (c_1 - a_{11}x_1 - a_{21}x_2)^{a_{11}}(c_2 - a_{12}x_1 - a_{22}x_2)^{a_{12}} = x_1 \\  F_2(x_1,x_2) \colon= (c_1 - a_{11}x_1 - a_{21}x_2)^{a_{21}}(c_2 - a_{12}x_1 - a_{22}x_2)^{a_{22}} = x_2 \end{cases}$$ where $c_i, a_{ij}\geq 0$ and $c_1 + c_2 = a_{11}+a_{12} = a_{21} + a_{22} = 1.$ I strongly suspect the above system has a solution $(x_1,x_2)$ with $x_i>0,$ but not quite sure how to prove it. The Jacobian after taking $\ln$ from both sides is positive definite which can be seen easily. But I am not sure how that would help if we wanted to use Implicit Function Theorem, for instance. I thought about using Banach's fixed point theorem but the map $F = (F_1, F_2)$ does not seem to be a contraction since the derivative will be unbounded due to the exponents $a_{ij}\leq 1.$ I think a vector version of the Intermediate Value Theorem is probably most promising but all I can find is Poincare-Miranda Theorem , which I am not sure is immediately applicable. For what its worth we can assume the domain I am interested is: $$D = \{(x_1,x_2)\vert\,  a_{1i}x_1 + a_{2i}x_2 < c_i  \}\subseteq\mathbb{R}^2_{>0}$$ Finally, its one-dimensional version is an immediate consequence of the Intermediate Value Theorem","I have the following system: where and I strongly suspect the above system has a solution with but not quite sure how to prove it. The Jacobian after taking from both sides is positive definite which can be seen easily. But I am not sure how that would help if we wanted to use Implicit Function Theorem, for instance. I thought about using Banach's fixed point theorem but the map does not seem to be a contraction since the derivative will be unbounded due to the exponents I think a vector version of the Intermediate Value Theorem is probably most promising but all I can find is Poincare-Miranda Theorem , which I am not sure is immediately applicable. For what its worth we can assume the domain I am interested is: Finally, its one-dimensional version is an immediate consequence of the Intermediate Value Theorem","\begin{cases}
F_1(x_1,x_2) \colon= (c_1 - a_{11}x_1 - a_{21}x_2)^{a_{11}}(c_2 - a_{12}x_1 - a_{22}x_2)^{a_{12}} = x_1 \\ 
F_2(x_1,x_2) \colon= (c_1 - a_{11}x_1 - a_{21}x_2)^{a_{21}}(c_2 - a_{12}x_1 - a_{22}x_2)^{a_{22}} = x_2
\end{cases} c_i, a_{ij}\geq 0 c_1 + c_2 = a_{11}+a_{12} = a_{21} + a_{22} = 1. (x_1,x_2) x_i>0, \ln F = (F_1, F_2) a_{ij}\leq 1. D = \{(x_1,x_2)\vert\,  a_{1i}x_1 + a_{2i}x_2 < c_i  \}\subseteq\mathbb{R}^2_{>0}","['real-analysis', 'calculus', 'multivariable-calculus', 'nonlinear-system', 'fixed-point-theorems']"
43,Gâteaux differentiable function (with linear continuous Gateaux derivative) that is not Fréchet differentiable,Gâteaux differentiable function (with linear continuous Gateaux derivative) that is not Fréchet differentiable,,"Currently, I am following a portion of the text Methods of Nonlinear Analysis . For normed real vector spaces $(X,\|\cdot\|_X),(Y,\|\cdot\|_Y)$ and $a \in X$ , consider the following definitions from this text. Directional Derivative: Let $h \in X$ . If the limit $$\lim_{t \to 0;\ t \in \mathbb{R}} \frac{f(a+th)-f(a)}{t}$$ exists in $Y$ , then the value of the limit is called the derivative of $f$ at $a$ in the direction $h$ and is denoted $\delta f(a;h)$ . Gâteaux Derivative: Suppose $\delta f(a;h)$ exists for all $h \in X$ . If the function $Df(a) : X \to Y$ where $h \mapsto \delta f(a;h)$ is continuous and linear, then $Df(a)$ is called the Gâteaux derivative of $f$ at $a$ . Fréchet Derivative: If there exists a continuous linear transformation $A : X \to Y$ such that $$\lim_{\|h\|_X \to 0} \frac{\|f(a+h)-f(a)-Ah\|_Y}{\|h\|_X}$$ then we call $A$ the Fréchet derivative of $f$ at $a$ and denote it $f'(a)$ . Question: I am struggling to find an example of $X,Y,a,f$ such that the Gâteaux derivative of $f$ exists at $a$ but the Fréchet derivative of $f$ does not exist at $a$ . Can anyone help provide such an example? Please note that the Gâteaux derivative must be linear and continuous. Since $f$ having a Fréchet derivative at $a$ implies the continuity of $f$ at $a$ , I imagine there exists an example with $X = \mathbb{R^2}, Y = \mathbb{R}, a = (0,0)$ such that $Df(0,0)$ exists but $f$ is not continuous at $(0,0)$ . Among many other online sources, I have read the following stack exchange posts but I believe that each asks a slightly different question or uses a slightly different definition of the Gâteaux derivative: Example of a continuous and Gâteaux differentiable function that is not Fréchet differentiable. What is an example of Gâteaux differentiable but not Fréchet differentiable at a point in a finite-dimensional space? Question about discontinuous function with directional derivatives at a points","Currently, I am following a portion of the text Methods of Nonlinear Analysis . For normed real vector spaces and , consider the following definitions from this text. Directional Derivative: Let . If the limit exists in , then the value of the limit is called the derivative of at in the direction and is denoted . Gâteaux Derivative: Suppose exists for all . If the function where is continuous and linear, then is called the Gâteaux derivative of at . Fréchet Derivative: If there exists a continuous linear transformation such that then we call the Fréchet derivative of at and denote it . Question: I am struggling to find an example of such that the Gâteaux derivative of exists at but the Fréchet derivative of does not exist at . Can anyone help provide such an example? Please note that the Gâteaux derivative must be linear and continuous. Since having a Fréchet derivative at implies the continuity of at , I imagine there exists an example with such that exists but is not continuous at . Among many other online sources, I have read the following stack exchange posts but I believe that each asks a slightly different question or uses a slightly different definition of the Gâteaux derivative: Example of a continuous and Gâteaux differentiable function that is not Fréchet differentiable. What is an example of Gâteaux differentiable but not Fréchet differentiable at a point in a finite-dimensional space? Question about discontinuous function with directional derivatives at a points","(X,\|\cdot\|_X),(Y,\|\cdot\|_Y) a \in X h \in X \lim_{t \to 0;\ t \in \mathbb{R}} \frac{f(a+th)-f(a)}{t} Y f a h \delta f(a;h) \delta f(a;h) h \in X Df(a) : X \to Y h \mapsto \delta f(a;h) Df(a) f a A : X \to Y \lim_{\|h\|_X \to 0} \frac{\|f(a+h)-f(a)-Ah\|_Y}{\|h\|_X} A f a f'(a) X,Y,a,f f a f a f a f a X = \mathbb{R^2}, Y = \mathbb{R}, a = (0,0) Df(0,0) f (0,0)","['multivariable-calculus', 'examples-counterexamples', 'frechet-derivative', 'gateaux-derivative']"
44,Prove that the Jacobian is constant,Prove that the Jacobian is constant,,"Suppose that $u=u(x,y)$ and $v=v(x,y)$ have continuous second partial derivatives. If for each $f$ , we have $$\frac{\partial^2f}{\partial u^2}+\frac{\partial^2f}{\partial v^2}=\frac{\partial^2f}{\partial x^2}+\frac{\partial^2f}{\partial y^2},$$ prove that the Jacobian $\begin{pmatrix}\frac{\partial u}{\partial x}&\frac{\partial u}{\partial y}\\\frac{\partial v}{\partial x}&\frac{\partial v}{\partial y}\end{pmatrix}$ is constant. By using the chain rule, $$\frac{\partial^2f}{\partial x^2}=\left(\frac{\partial u}{\partial x}\right)^2\frac{\partial^2f}{\partial u^2}+2\frac{\partial u}{\partial x}\frac{\partial v}{\partial x}+\left(\frac{\partial v}{\partial x}\right)^2\frac{\partial^2f}{\partial v^2}\\ \frac{\partial^2f}{\partial y^2}=\left(\frac{\partial u}{\partial y}\right)^2\frac{\partial^2f}{\partial u^2}+2\frac{\partial u}{\partial y}\frac{\partial v}{\partial y}+\left(\frac{\partial v}{\partial y}\right)^2\frac{\partial^2f}{\partial v^2}$$ and we see that the Jacobian is orthogonal. But how to prove that it is constant?","Suppose that and have continuous second partial derivatives. If for each , we have prove that the Jacobian is constant. By using the chain rule, and we see that the Jacobian is orthogonal. But how to prove that it is constant?","u=u(x,y) v=v(x,y) f \frac{\partial^2f}{\partial u^2}+\frac{\partial^2f}{\partial v^2}=\frac{\partial^2f}{\partial x^2}+\frac{\partial^2f}{\partial y^2}, \begin{pmatrix}\frac{\partial u}{\partial x}&\frac{\partial u}{\partial y}\\\frac{\partial v}{\partial x}&\frac{\partial v}{\partial y}\end{pmatrix} \frac{\partial^2f}{\partial x^2}=\left(\frac{\partial u}{\partial x}\right)^2\frac{\partial^2f}{\partial u^2}+2\frac{\partial u}{\partial x}\frac{\partial v}{\partial x}+\left(\frac{\partial v}{\partial x}\right)^2\frac{\partial^2f}{\partial v^2}\\
\frac{\partial^2f}{\partial y^2}=\left(\frac{\partial u}{\partial y}\right)^2\frac{\partial^2f}{\partial u^2}+2\frac{\partial u}{\partial y}\frac{\partial v}{\partial y}+\left(\frac{\partial v}{\partial y}\right)^2\frac{\partial^2f}{\partial v^2}","['calculus', 'multivariable-calculus', 'jacobian']"
45,Is there a sense in which a function converges to its total derivative?,Is there a sense in which a function converges to its total derivative?,,"In short: Is there a way to rigorously define the total derivative (of $F$ at $x$ ) as a function $dF_x$ which is (1) linear and (2) a usual topological limit of some function $\mu:X\to Y$ , where $X$ and $Y$ are topological spaces? Motivation: Let $F:\mathbb R^n\to\mathbb R^m$ . The total derivative of $F$ at $x\in\mathbb R^n$ is defined as the linear map $dF_x:\mathbb R^n\to\mathbb R^m$ such that $$\lim_{v\to0}\frac{\|F(x+v)-F(x)-(dF_x)v\|}{\|v\|}=0,$$ if such a linear map exists. This definition makes sense, but it is a bit indirect. The idea is that $F$ looks more and more like $dF_x$ near $x$ , so it seems that the more natural definition should be something like $\lim_{v\to0} F(x+v)-F(x)=(dF_x)v$ . The following points are suggestive: Normally when $\lim_{x\to c}\|f(x)-L\|=0$ , this can be formulated as $\lim_{x\to c}f(x)=L$ . The division by $\|v\|$ in the above definition is highly reminiscent of the operator norm . I don't think there is any trouble extending the notion of operator norm to general continuous functions (although it may be infinite), via $\|F\|_\text{op}=\sup_{0\neq v\in\operatorname{dom}(F)}\frac{\|F(v)\|}{\|v\|}.$ Then, defining $\hat F_x(v)=F(x+v)-F(x)$ , we might be able to manipulate the first definition into something like $$\lim_{v\to0}\hat F_x(v)=dF_x,$$ where it is understood that the limit is with respect to the operator norm. Or rather, as we look at $F$ restricted to smaller neighborhoods of $x$ , the operator norm of $\hat F_x-dF_x$ restricted to this neighborhood approaches $0$ . $F$ would approach many functions in this sense (including itself, for example), but the derivative would be the only linear function it approaches in this sense. However, when I try making this rigorous, I run into some challenges. Topologically, the limit of a function $F$ near $x$ must be a value in the range of $F$ , not a function with the same domain and range of $F$ ; so this formulation will require speaking of the limit of some function besides $F$ , probably mapping into function spaces or spaces of germs of functions near $x$ . This is where my knowledge gets more limited and I hit a wall. Update: Here's my current progress. Let $C^0_x=\{f\in C^0(U,\mathbb R^m)\,|\,U\text{ is a neighborhood of }x\}$ . Define function addition/subtraction on $C^0_x$ as usual function addition/subtraction restricted to the intersection of both functions' domains. Topologize this space using the metric $d(f,g)=\sup_{t\in\operatorname{dom}(f-g)\setminus\{x\}}\frac{\|f(t)-g(t)\|}{\|t-x\|}$ . This metric can take infinite values, but this seems ok according to this . Finally, let $\mu_{x,F}:(0,\infty)\to C^0_x$ be defined by $\mu_{x,F}(\delta)=F\big|_{B_\delta(x)}$ . Then, perhaps, we can define $dF_x$ (if it exists) as the unique linear map defined on all of $\mathbb R^n$ such that $$\lim_{\delta\to0}\mu_{x,F}(\delta)=F(x)+dF_x.$$ Does this work? Is there a more elegant or standard way to do this?","In short: Is there a way to rigorously define the total derivative (of at ) as a function which is (1) linear and (2) a usual topological limit of some function , where and are topological spaces? Motivation: Let . The total derivative of at is defined as the linear map such that if such a linear map exists. This definition makes sense, but it is a bit indirect. The idea is that looks more and more like near , so it seems that the more natural definition should be something like . The following points are suggestive: Normally when , this can be formulated as . The division by in the above definition is highly reminiscent of the operator norm . I don't think there is any trouble extending the notion of operator norm to general continuous functions (although it may be infinite), via Then, defining , we might be able to manipulate the first definition into something like where it is understood that the limit is with respect to the operator norm. Or rather, as we look at restricted to smaller neighborhoods of , the operator norm of restricted to this neighborhood approaches . would approach many functions in this sense (including itself, for example), but the derivative would be the only linear function it approaches in this sense. However, when I try making this rigorous, I run into some challenges. Topologically, the limit of a function near must be a value in the range of , not a function with the same domain and range of ; so this formulation will require speaking of the limit of some function besides , probably mapping into function spaces or spaces of germs of functions near . This is where my knowledge gets more limited and I hit a wall. Update: Here's my current progress. Let . Define function addition/subtraction on as usual function addition/subtraction restricted to the intersection of both functions' domains. Topologize this space using the metric . This metric can take infinite values, but this seems ok according to this . Finally, let be defined by . Then, perhaps, we can define (if it exists) as the unique linear map defined on all of such that Does this work? Is there a more elegant or standard way to do this?","F x dF_x \mu:X\to Y X Y F:\mathbb R^n\to\mathbb R^m F x\in\mathbb R^n dF_x:\mathbb R^n\to\mathbb R^m \lim_{v\to0}\frac{\|F(x+v)-F(x)-(dF_x)v\|}{\|v\|}=0, F dF_x x \lim_{v\to0} F(x+v)-F(x)=(dF_x)v \lim_{x\to c}\|f(x)-L\|=0 \lim_{x\to c}f(x)=L \|v\| \|F\|_\text{op}=\sup_{0\neq v\in\operatorname{dom}(F)}\frac{\|F(v)\|}{\|v\|}. \hat F_x(v)=F(x+v)-F(x) \lim_{v\to0}\hat F_x(v)=dF_x, F x \hat F_x-dF_x 0 F F x F F F x C^0_x=\{f\in C^0(U,\mathbb R^m)\,|\,U\text{ is a neighborhood of }x\} C^0_x d(f,g)=\sup_{t\in\operatorname{dom}(f-g)\setminus\{x\}}\frac{\|f(t)-g(t)\|}{\|t-x\|} \mu_{x,F}:(0,\infty)\to C^0_x \mu_{x,F}(\delta)=F\big|_{B_\delta(x)} dF_x \mathbb R^n \lim_{\delta\to0}\mu_{x,F}(\delta)=F(x)+dF_x.","['functional-analysis', 'multivariable-calculus', 'derivatives', 'topological-vector-spaces', 'frechet-derivative']"
46,Rewrite the integral in the order dy dz dx,Rewrite the integral in the order dy dz dx,,"Rewrite the integral $$\int_0^3 \int_0^{9-y^2} \int_\frac{y}3^1 f(x,y,z) dx dz dy $$ as an interated integral in the order dy dz dx. I have trouble visualizing if my answer is the correct iterated integral or not. I got $$\int_0^1 \int_0^{9-9x^2} \int_0^{3x} f(x,y,z) dy dz dx $$ but i am unsure if this is correct or perhaps there is some part of the domain not captured in my answer.",Rewrite the integral as an interated integral in the order dy dz dx. I have trouble visualizing if my answer is the correct iterated integral or not. I got but i am unsure if this is correct or perhaps there is some part of the domain not captured in my answer.,"\int_0^3 \int_0^{9-y^2} \int_\frac{y}3^1 f(x,y,z) dx dz dy  \int_0^1 \int_0^{9-9x^2} \int_0^{3x} f(x,y,z) dy dz dx ","['integration', 'multivariable-calculus', 'definite-integrals', 'multiple-integral', 'integral-domain']"
47,Line equation tangent to convex level curve,Line equation tangent to convex level curve,,"Suppose we are given a differentiable function $f(x,y)$ such that $\forall$ $t \in \mathbb{R}$ , $f = t$ yields strictly convex level curves. If we are given a line equation $L:y = mx + c$ such that it always intersects two points of a level curve or is tangent to it or does not intersect at all, will it mean that a level curve tangent to $y = mx + c$ always exists? This is required for a different (convex optimization) problem that I am trying to solve. I am not sure if this holds, but intuitively it feels like that, although I don't have a solid intuition to solve this. Using the Mean Value Theorem, one can say that a tangent with slope $m$ exists that will intersect a given level curve. But I can't extend this to prove that the line $y = mx + c$ itself will be tangent to some level curve. Update: The problem is almost solved (thanks to @copper.hat), but there's a case that I can rule out. Please see my last comment below this post.","Suppose we are given a differentiable function such that , yields strictly convex level curves. If we are given a line equation such that it always intersects two points of a level curve or is tangent to it or does not intersect at all, will it mean that a level curve tangent to always exists? This is required for a different (convex optimization) problem that I am trying to solve. I am not sure if this holds, but intuitively it feels like that, although I don't have a solid intuition to solve this. Using the Mean Value Theorem, one can say that a tangent with slope exists that will intersect a given level curve. But I can't extend this to prove that the line itself will be tangent to some level curve. Update: The problem is almost solved (thanks to @copper.hat), but there's a case that I can rule out. Please see my last comment below this post.","f(x,y) \forall t \in \mathbb{R} f = t L:y = mx + c y = mx + c m y = mx + c","['calculus', 'multivariable-calculus', 'convex-optimization', 'curves', 'economics']"
48,Conditions for $f\left(\sqrt{x^2+y^2}\right)$ to be $C^p$,Conditions for  to be,f\left(\sqrt{x^2+y^2}\right) C^p,"Let $f : \mathbb{R} \to \mathbb{R}$ be a $C^p$ function. I'm looking for a necessary and sufficient condition for the function $$(x,y) \in \mathbb{R}^2 \mapsto f\left(\sqrt{x^2+y^2}\right)$$ to be $C^p$ as well. Now, obviously, it is $C^p$ on $\mathbb{R}^2\setminus \{(0,0)\}$ so the unique problem is at $(0,0)$ . I have used some brute force and compute partial derivatives up to order 3. It seems that a sufficient condition is the cancelation of all the derivatives of $f$ at $0$ . However it is just a conjecture and, moreover, it would only be a sufficient condition not a necessary one. Any help or thought on this subject will be much appreciated.","Let be a function. I'm looking for a necessary and sufficient condition for the function to be as well. Now, obviously, it is on so the unique problem is at . I have used some brute force and compute partial derivatives up to order 3. It seems that a sufficient condition is the cancelation of all the derivatives of at . However it is just a conjecture and, moreover, it would only be a sufficient condition not a necessary one. Any help or thought on this subject will be much appreciated.","f : \mathbb{R} \to \mathbb{R} C^p (x,y) \in \mathbb{R}^2 \mapsto f\left(\sqrt{x^2+y^2}\right) C^p C^p \mathbb{R}^2\setminus \{(0,0)\} (0,0) f 0","['real-analysis', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
49,Is the monotonicity of $f$ needed for Lagrange multiplier to be used?,Is the monotonicity of  needed for Lagrange multiplier to be used?,f,"The sets $S_1$ and $S_2$ are defined as $S_1 = \left\{(x,y,z) \in \mathbb{R}^3_{+} : z = \frac{1}{xy}\right\}$ and $S_2 = \left\{(x,y,z) \in \mathbb{R}^3 : (x-4)^2+(y-4)^2 + z^2 = 5 \right\}$ . Consider the function $f$ that is given by the points of $\max(z(S_1), z(S_2))$ . (Given $x,y$ , if $z$ exists twice, then we choose the larger $z$ and its corresponding $x,y$ .) Find $\max(f)$ subject to $3x+4y \geq 5$ . Here is a picture I plotted using Geogebra. For the function $f$ , we choose the positive values of the sphere (red) in the region where it exists, and everywhere else, it is $\frac{1}{xy}$ (purple/violet). The blue plane denotes the equation $3x+4y=5$ . Can we solve this using the Lagrange multiplier ? I am essentially trying to understand when we can and when we can not use LM. The idea behind this is to draw a function that is not monotone throughout, so I added the sphere. The goal is to check whether the LM method gives $(x,y) = (\frac{3}{5},\frac{4}{5})$ as the optimal value or the point on the sphere $(x,y) = (4,4)$ to maximize $f$ . Ideally, it should be the latter. But I couldn't calculate this. I would like to know if the monotone nature of $f$ is really needed in general or not. (Do consider the possibility of non-linear constraints.) By monotonicity of $f$ , it is referred to the partial derivatives wrt $x$ and $y$ being $> 0$ .","The sets and are defined as and . Consider the function that is given by the points of . (Given , if exists twice, then we choose the larger and its corresponding .) Find subject to . Here is a picture I plotted using Geogebra. For the function , we choose the positive values of the sphere (red) in the region where it exists, and everywhere else, it is (purple/violet). The blue plane denotes the equation . Can we solve this using the Lagrange multiplier ? I am essentially trying to understand when we can and when we can not use LM. The idea behind this is to draw a function that is not monotone throughout, so I added the sphere. The goal is to check whether the LM method gives as the optimal value or the point on the sphere to maximize . Ideally, it should be the latter. But I couldn't calculate this. I would like to know if the monotone nature of is really needed in general or not. (Do consider the possibility of non-linear constraints.) By monotonicity of , it is referred to the partial derivatives wrt and being .","S_1 S_2 S_1 = \left\{(x,y,z) \in \mathbb{R}^3_{+} : z = \frac{1}{xy}\right\} S_2 = \left\{(x,y,z) \in \mathbb{R}^3 : (x-4)^2+(y-4)^2 + z^2 = 5 \right\} f \max(z(S_1), z(S_2)) x,y z z x,y \max(f) 3x+4y \geq 5 f \frac{1}{xy} 3x+4y=5 (x,y) = (\frac{3}{5},\frac{4}{5}) (x,y) = (4,4) f f f x y > 0","['calculus', 'multivariable-calculus', 'optimization', 'nonlinear-optimization', 'lagrange-multiplier']"
50,Prove that the multivariable integral is equal to the binomial summation,Prove that the multivariable integral is equal to the binomial summation,,"I came across this multivariable integral which states that for any $s\in\mathbb{C}$ and $n>0$ , we have the equity below \begin{align} \sum_{k=0}^n (-1)^k \binom{n}{k}(u+k)^{1-s} = (s-1)_n\int_0^1 \cdots \int_0^1 (u+x_1+\cdots+x_n)^{1-s-n} \,dx_1\cdots dx_n \end{align} where $(a)_n=a(a+1)\cdots(a+n-1)$ is the Pochhammer symbol. I have notice that \begin{align} \int_0^1 \cdots \int_0^1 \bigg(u+\sum_{j=1}^n x_j\bigg)^{1-s-n} \,dx_1\cdots dx_n &= \int_0^1 \cdots \int_0^1 \left.\frac{(u+\sum_{j=1}^n x_j)^{2-s-n}}{2-s-n}\right|_{x_1=0}^1 \,dx_2\cdots dx_n\\ &= \frac{1}{s+n-2} \int_0^1 \cdots \int_0^1 \bigg(u+\sum_{j=2}^n x_j\bigg)^{2-s-n}-\bigg(u+1+\sum_{j=2}^n x_j\bigg)^{2-s-n} \,dx_2\cdots dx_n\\ &= \frac{1}{(s+n-3)(s+n-2)}\int_0^1 \cdots \int_0^1 \bigg(u+\sum_{j=3}^n x_j\bigg)^{3-s-n}-2\bigg(u+1+\sum_{j=3}^n x_j\bigg)^{3-s-n}+\bigg(u+2+\sum_{j=3}^n x_j\bigg)^{3-s-n}\,dx_3\cdots dx_n \\ &\vdots \\ &= \frac{1}{(s-1)_n}\bigg(u^{1-s}-n(u+1)^{1-s}+\cdots+(-1)^{n-1}n(u+n-1)^{1-s}+(-1)^n(u+n)^{1-s}\bigg) \end{align} by repeating this process $n$ times. Other than that, I have also try to use multinomial theorem but could not get to anywhere \begin{align} \bigg(u+\sum_{j=1}^n x_j\bigg)^{1-s-n} &= \sum_{m=0}^{1-s-n} \binom{1-s-n}{m}u^{1-s-n-m}\bigg(\sum_{j=1}^n x_j\bigg)^m\\ &= \sum_{m=0}^{1-s-n} \binom{1-s-n}{m}u^{1-s-n-m} \sum_{k_1+\cdots+k_n=m} \frac{m!}{k_1!\cdots k_n!} \prod_{t=1}^n x_t^{k_t} \end{align} I would be appreciated if someone could help me with this with a non-heuristic approach.","I came across this multivariable integral which states that for any and , we have the equity below where is the Pochhammer symbol. I have notice that by repeating this process times. Other than that, I have also try to use multinomial theorem but could not get to anywhere I would be appreciated if someone could help me with this with a non-heuristic approach.","s\in\mathbb{C} n>0 \begin{align} \sum_{k=0}^n (-1)^k \binom{n}{k}(u+k)^{1-s} = (s-1)_n\int_0^1 \cdots \int_0^1 (u+x_1+\cdots+x_n)^{1-s-n} \,dx_1\cdots dx_n \end{align} (a)_n=a(a+1)\cdots(a+n-1) \begin{align} \int_0^1 \cdots \int_0^1 \bigg(u+\sum_{j=1}^n x_j\bigg)^{1-s-n} \,dx_1\cdots dx_n &= \int_0^1 \cdots \int_0^1 \left.\frac{(u+\sum_{j=1}^n x_j)^{2-s-n}}{2-s-n}\right|_{x_1=0}^1 \,dx_2\cdots dx_n\\ &= \frac{1}{s+n-2} \int_0^1 \cdots \int_0^1 \bigg(u+\sum_{j=2}^n x_j\bigg)^{2-s-n}-\bigg(u+1+\sum_{j=2}^n x_j\bigg)^{2-s-n} \,dx_2\cdots dx_n\\ &= \frac{1}{(s+n-3)(s+n-2)}\int_0^1 \cdots \int_0^1 \bigg(u+\sum_{j=3}^n x_j\bigg)^{3-s-n}-2\bigg(u+1+\sum_{j=3}^n x_j\bigg)^{3-s-n}+\bigg(u+2+\sum_{j=3}^n x_j\bigg)^{3-s-n}\,dx_3\cdots dx_n \\ &\vdots \\ &= \frac{1}{(s-1)_n}\bigg(u^{1-s}-n(u+1)^{1-s}+\cdots+(-1)^{n-1}n(u+n-1)^{1-s}+(-1)^n(u+n)^{1-s}\bigg) \end{align} n \begin{align} \bigg(u+\sum_{j=1}^n x_j\bigg)^{1-s-n} &= \sum_{m=0}^{1-s-n} \binom{1-s-n}{m}u^{1-s-n-m}\bigg(\sum_{j=1}^n x_j\bigg)^m\\ &= \sum_{m=0}^{1-s-n} \binom{1-s-n}{m}u^{1-s-n-m} \sum_{k_1+\cdots+k_n=m} \frac{m!}{k_1!\cdots k_n!} \prod_{t=1}^n x_t^{k_t} \end{align}","['calculus', 'integration', 'multivariable-calculus', 'binomial-coefficients']"
51,Double integral substitution where the region of integration becomes a point,Double integral substitution where the region of integration becomes a point,,"I want to evaluate a double integral of some function over the region between the graphs $y=x^2$ and $y=\frac{x^2}{2}+1$ . $D=\{(x,y):-\sqrt2\leq x \leq \sqrt2, x^2\leq y \leq \frac{x^2}{2}+1\}$ The subsitution I am trying to make is $$x^2=u$$ $$y=v$$ The way I visualise this substitution is folding around the $y$ axis from left to right and making the curves straight lines. $$y=x^2 \rightarrow v=u$$ $$y=\frac{x^2}{2}+1 \rightarrow v=\frac{u}{2}+1$$ From there I get confused. The Jacobian is always zero $\pm \frac{1}{2\sqrt{u}}$ and $D$ became a single point $(2,2)$ . Doesn't this make the integral zero?",I want to evaluate a double integral of some function over the region between the graphs and . The subsitution I am trying to make is The way I visualise this substitution is folding around the axis from left to right and making the curves straight lines. From there I get confused. The Jacobian is always zero and became a single point . Doesn't this make the integral zero?,"y=x^2 y=\frac{x^2}{2}+1 D=\{(x,y):-\sqrt2\leq x \leq \sqrt2, x^2\leq y \leq \frac{x^2}{2}+1\} x^2=u y=v y y=x^2 \rightarrow v=u y=\frac{x^2}{2}+1 \rightarrow v=\frac{u}{2}+1 \pm \frac{1}{2\sqrt{u}} D (2,2)","['integration', 'multivariable-calculus']"
52,Questions about Rudin's rank theorem,Questions about Rudin's rank theorem,,"I am trying to understand the rank theorem in Rudin's Principles of Mathematical Analysis. The theorem states: Theorem Suppose $m,n,r$ are nonnegative integers, $m\ge r, n\ge r$ , $F$ is a $C^1$ mapping of an open set $E\subset \mathbb{R}^n$ into $\mathbb{R}^m$ , and $F'(x)$ has rank $r$ for every $x\in E$ . Fix $a\in E$ , put $A = F'(a)$ , let $Y_1$ be the range of $A$ , and let $P$ be a projection in $\mathbb{R}^m$ whose range is $Y_1$ and let $Y_2$ be the kernel of $P$ . Then there are open sets $U$ and $V$ in $\mathbb{R}^n$ , with $a\in U\subset E$ , and there is a 1-1 $C^1$ mapping $H$ of $V$ onto $U$ (whose inverse is also of $C^1$ ) such that $$F(H(x)) = Ax+\phi(Ax)\;\;\;\;(x\in V)$$ where $\phi$ is a $C^1$ mapping of the open set $A(V)\subset Y_1$ into $Y_2$ . I have quite a few problems trying to understand this theorem, but perhaps mainly this: What kind of projection does Rudin have in mind? All he says about the projection is that its range is $Y_1$ . Is this the projection of the entire $\mathbb{R}^m$ and the $\text{im}(P)=Y_1$ ? Can you choose any projection with such range? Does the choice of such projection affect the null space of P? What does the equation $F(H(x)) = Ax + \phi(Ax)$ tell us intuitively? Why are we mapping with $\phi$ from the range of $P$ to its nullspace?","I am trying to understand the rank theorem in Rudin's Principles of Mathematical Analysis. The theorem states: Theorem Suppose are nonnegative integers, , is a mapping of an open set into , and has rank for every . Fix , put , let be the range of , and let be a projection in whose range is and let be the kernel of . Then there are open sets and in , with , and there is a 1-1 mapping of onto (whose inverse is also of ) such that where is a mapping of the open set into . I have quite a few problems trying to understand this theorem, but perhaps mainly this: What kind of projection does Rudin have in mind? All he says about the projection is that its range is . Is this the projection of the entire and the ? Can you choose any projection with such range? Does the choice of such projection affect the null space of P? What does the equation tell us intuitively? Why are we mapping with from the range of to its nullspace?","m,n,r m\ge r, n\ge r F C^1 E\subset \mathbb{R}^n \mathbb{R}^m F'(x) r x\in E a\in E A = F'(a) Y_1 A P \mathbb{R}^m Y_1 Y_2 P U V \mathbb{R}^n a\in U\subset E C^1 H V U C^1 F(H(x)) = Ax+\phi(Ax)\;\;\;\;(x\in V) \phi C^1 A(V)\subset Y_1 Y_2 Y_1 \mathbb{R}^m \text{im}(P)=Y_1 F(H(x)) = Ax + \phi(Ax) \phi P","['real-analysis', 'multivariable-calculus', 'implicit-function-theorem', 'inverse-function-theorem']"
53,Why distance between point and image of exponential map is travel time?,Why distance between point and image of exponential map is travel time?,,"Let $(M, g)$ be a compact manifold with strictly convex boundary. The distance function $d_{g}: M \times M \rightarrow \mathbb{R}$ is given by $$ d_{g}(x, y)=\inf _{\gamma \in \Lambda_{x, y}} \ell_{g}(\gamma) $$ where $\Lambda_{x, y}$ denotes the set of smooth curves $\gamma:[0,1] \rightarrow M$ such that $\gamma(0)=x$ and $\gamma(1)=y$ and $$ \ell_{g}(\gamma):=\int_{0}^{1}|\dot{\gamma}(t)|_{g} d t. $$ Now we know that on a simple manifold, the exponential map $$ \exp _{x}: D_{x} \rightarrow M $$ is a diffeomorphism. So for any pair of points in $M$ there is a unique geodesic between them, and this geodesic minimizes length. Now if we consider $t v \in D_{x}$ and $|v|_{g}=1$ , then I need to show following $$ f\left(\exp _{x}(t v)\right)=d_{g}\left(x, \exp _{x}(t v)\right)=t. $$ What I was thinking is following: By definition: \begin{align*} d_{g}\left(x, \exp _{x}(t v)\right)&=\int_{0}^{1}|\dot{\exp_{x}}(tv)|_{g} d t\\ &=\int_{0}^{1}|v|_g|\dot\gamma_{x,v}(t)|_g dt\\ &=\int_{0}^{1}|\dot\gamma_{x,v}(t)|_g dt \end{align*} Now $\gamma$ is geodesic so it has unit length so $||\dot\gamma_{x,v}(t)|_g|=|\dot\gamma_{x,v}(0)|_g=|v|_g=1$ . But I need to show that  equals to $t$ . where is I am making mistakes Please help me. Any help/hint/reference will be highly appreciated.","Let be a compact manifold with strictly convex boundary. The distance function is given by where denotes the set of smooth curves such that and and Now we know that on a simple manifold, the exponential map is a diffeomorphism. So for any pair of points in there is a unique geodesic between them, and this geodesic minimizes length. Now if we consider and , then I need to show following What I was thinking is following: By definition: Now is geodesic so it has unit length so . But I need to show that  equals to . where is I am making mistakes Please help me. Any help/hint/reference will be highly appreciated.","(M, g) d_{g}: M \times M \rightarrow \mathbb{R} 
d_{g}(x, y)=\inf _{\gamma \in \Lambda_{x, y}} \ell_{g}(\gamma)
 \Lambda_{x, y} \gamma:[0,1] \rightarrow M \gamma(0)=x \gamma(1)=y 
\ell_{g}(\gamma):=\int_{0}^{1}|\dot{\gamma}(t)|_{g} d t.
 
\exp _{x}: D_{x} \rightarrow M
 M t v \in D_{x} |v|_{g}=1 
f\left(\exp _{x}(t v)\right)=d_{g}\left(x, \exp _{x}(t v)\right)=t.
 \begin{align*}
d_{g}\left(x, \exp _{x}(t v)\right)&=\int_{0}^{1}|\dot{\exp_{x}}(tv)|_{g} d t\\
&=\int_{0}^{1}|v|_g|\dot\gamma_{x,v}(t)|_g dt\\
&=\int_{0}^{1}|\dot\gamma_{x,v}(t)|_g dt
\end{align*} \gamma ||\dot\gamma_{x,v}(t)|_g|=|\dot\gamma_{x,v}(0)|_g=|v|_g=1 t","['multivariable-calculus', 'differential-geometry', 'manifolds', 'riemannian-geometry']"
54,Potential function for vector field $\frac{1}{r} \hat{\phi}$?,Potential function for vector field ?,\frac{1}{r} \hat{\phi},"Question. Does the vector field $\vec{F}(r, \phi) = \frac{1}{r} \hat{\phi}$ have something like an associated potential function? Context. I know that $\vec{F}$ is not conservative, and so I should not expect $\vec{F}$ to have a potential function that is defined everywhere . But could $\vec{F}$ nevertheless have a potential function that is defined almost everywhere ? Specifically, I know that taking line integrals around closed paths through $\vec{F}$ leads to a result of either 0 or $2\pi$ , and this depends on whether the closed path encircles the origin or not. So it seems like $\vec{F}$ acts like a conservative vector field when you're away from the origin. I also find it strangely suspicious that $\vec{F}$ itself will be undefined when $r = 0$ , i.e. at the origin. So I'm wondering if maybe there is an associated potential function that is defined everywhere except the origin, and has an essential singularity at the origin, so that integrating around the singularity leads to nonzero results?","Question. Does the vector field have something like an associated potential function? Context. I know that is not conservative, and so I should not expect to have a potential function that is defined everywhere . But could nevertheless have a potential function that is defined almost everywhere ? Specifically, I know that taking line integrals around closed paths through leads to a result of either 0 or , and this depends on whether the closed path encircles the origin or not. So it seems like acts like a conservative vector field when you're away from the origin. I also find it strangely suspicious that itself will be undefined when , i.e. at the origin. So I'm wondering if maybe there is an associated potential function that is defined everywhere except the origin, and has an essential singularity at the origin, so that integrating around the singularity leads to nonzero results?","\vec{F}(r, \phi) = \frac{1}{r} \hat{\phi} \vec{F} \vec{F} \vec{F} \vec{F} 2\pi \vec{F} \vec{F} r = 0","['multivariable-calculus', 'vector-fields', 'line-integrals']"
55,Chain Rule for Partial Derivatives and Multi-variable Functions,Chain Rule for Partial Derivatives and Multi-variable Functions,,"I have the following relation: $ c = 2c_1-1 $ I also know that $c_1 = c_1(z,t)$ . I want a way to simplify: $$ \frac{\partial^2}{\partial z^2}\left(\frac{\partial g(c_1)}{\partial c}\right) $$ which I know is going to involve the chain rule and then probably a product rule for the second one. My attempt is as follows: First, I notice that $c_1 = \frac{c-1}{2} $ so I have $\frac{\partial g}{\partial c}=2\frac{\partial g}{\partial c_1} $ . Then we get $$ 2\frac{\partial}{\partial z}\frac{\partial}{\partial z}\left(\frac{\partial g}{\partial c_1}\right)=2\frac{\partial}{\partial z}\left[\frac{\partial^2 g}{\partial c_1^2}\cdot\frac{\partial c_1}{\partial z}\right] $$ Applying the second partial with respect to $z$ , we then have to use the product rule: $$ = 2\left[\frac{\partial^3g}{\partial c_1^3}\cdot\left(\frac{\partial c_1}{\partial z}\right)^2+\frac{\partial^2 g}{\partial c_1^2}\cdot\frac{\partial^2 c_1}{\partial z^2}\right] $$ Is this correct? Is there a simpler way to do this?","I have the following relation: I also know that . I want a way to simplify: which I know is going to involve the chain rule and then probably a product rule for the second one. My attempt is as follows: First, I notice that so I have . Then we get Applying the second partial with respect to , we then have to use the product rule: Is this correct? Is there a simpler way to do this?"," c = 2c_1-1  c_1 = c_1(z,t)  \frac{\partial^2}{\partial z^2}\left(\frac{\partial g(c_1)}{\partial c}\right)  c_1 = \frac{c-1}{2}  \frac{\partial g}{\partial c}=2\frac{\partial g}{\partial c_1}   2\frac{\partial}{\partial z}\frac{\partial}{\partial z}\left(\frac{\partial g}{\partial c_1}\right)=2\frac{\partial}{\partial z}\left[\frac{\partial^2 g}{\partial c_1^2}\cdot\frac{\partial c_1}{\partial z}\right]  z  = 2\left[\frac{\partial^3g}{\partial c_1^3}\cdot\left(\frac{\partial c_1}{\partial z}\right)^2+\frac{\partial^2 g}{\partial c_1^2}\cdot\frac{\partial^2 c_1}{\partial z^2}\right] ","['multivariable-calculus', 'partial-derivative', 'chain-rule']"
56,The proof of $\chi_A:I\to \mathbb R$ is integrable $\iff $ $\chi_A:J\to \mathbb R$ is integrable.,The proof of  is integrable   is integrable.,\chi_A:I\to \mathbb R \iff  \chi_A:J\to \mathbb R,"I'm studying the integration on Jordan measurable sets. Let $A\subset \mathbb R^n$ be bounded and $\chi_A$ be a characteristic function of $A$ Then, I want to prove that if $I\subset \mathbb{R^n}$ and $J\subset \mathbb{R^n}$ are intervals s.t. $A\subset I \subset J,$ then \begin{align} \chi_A: I\to \mathbb R \ \mathrm{is \ Riemann \ integrable \ on\ } I \iff  \chi_A : J\to \mathbb R \mathrm{\ is\ Riemann \ integrable\ on\ } J \end{align} For the proof of this, I think the theorem below is useful. Theorem Let $I\subset \mathbb{R^n}$ be a interval and $f:I\to \mathbb R$ be bounded. $f$ is integrable on $I$ if and only if for all $\epsilon>0$ , there exists a partition of $I$ s.t. $S(f,\Delta)-s(f,\Delta)<\epsilon$ where $S$ is the upper sum and $s$ is the lower sum. Suppose $\chi_A : I\to \mathbb R$ is integrable on I. Let $\epsilon>0.$ Then, from the integrability of $\chi_A: I\to \mathbb R$ , there exists the partition of $I$ , $\Delta=\{I_k\}_{k=1}^M$ s.t. $S(\chi_A, \Delta)-s(\chi_A, \Delta)<\epsilon.$ Then, I have to find the partition of $J$ s.t. $S(\chi_A, \Delta')-s(\chi_A, \Delta')<\epsilon \  ($ or $a \cdot \epsilon \ (a>0)).$ I'm having difficulty in finding such $\Delta'.$ I'd like you to gime me any help. (This is the step of defining the integral on Jordan measurable sets.) Just so you know, the definition of the integrability of functions on a interval is here. Let $I\subset \mathbb R^n$ be an interval. $f:I\to \mathbb R$ is integrable on $I$ $\underset{\mathrm{def}}\iff$ $\overline{\displaystyle\int_I} f(x) dx=\underline{\displaystyle\int_I}f(x) dx,$ where $\overline{\displaystyle\int_I}$ is upper integral and $\underline{\displaystyle\int_I}$ is lower integral.","I'm studying the integration on Jordan measurable sets. Let be bounded and be a characteristic function of Then, I want to prove that if and are intervals s.t. then For the proof of this, I think the theorem below is useful. Theorem Let be a interval and be bounded. is integrable on if and only if for all , there exists a partition of s.t. where is the upper sum and is the lower sum. Suppose is integrable on I. Let Then, from the integrability of , there exists the partition of , s.t. Then, I have to find the partition of s.t. or I'm having difficulty in finding such I'd like you to gime me any help. (This is the step of defining the integral on Jordan measurable sets.) Just so you know, the definition of the integrability of functions on a interval is here. Let be an interval. is integrable on where is upper integral and is lower integral.","A\subset \mathbb R^n \chi_A A I\subset \mathbb{R^n} J\subset \mathbb{R^n} A\subset I \subset J, \begin{align}
\chi_A: I\to \mathbb R \ \mathrm{is \ Riemann \ integrable \ on\ } I \iff  \chi_A : J\to \mathbb R \mathrm{\ is\ Riemann \ integrable\ on\ } J
\end{align} I\subset \mathbb{R^n} f:I\to \mathbb R f I \epsilon>0 I S(f,\Delta)-s(f,\Delta)<\epsilon S s \chi_A : I\to \mathbb R \epsilon>0. \chi_A: I\to \mathbb R I \Delta=\{I_k\}_{k=1}^M S(\chi_A, \Delta)-s(\chi_A, \Delta)<\epsilon. J S(\chi_A, \Delta')-s(\chi_A, \Delta')<\epsilon \  ( a \cdot \epsilon \ (a>0)). \Delta'. I\subset \mathbb R^n f:I\to \mathbb R I \underset{\mathrm{def}}\iff \overline{\displaystyle\int_I} f(x) dx=\underline{\displaystyle\int_I}f(x) dx, \overline{\displaystyle\int_I} \underline{\displaystyle\int_I}","['real-analysis', 'calculus', 'multivariable-calculus']"
57,"Determine the tangent planes of $x^2+z^2=1$ at the points $(x, 0, z)$ and show that they are all parallel to the $y-$axis.",Determine the tangent planes of  at the points  and show that they are all parallel to the axis.,"x^2+z^2=1 (x, 0, z) y-","Determine the tangent planes of $x^2+z^2=1$ at the points $(x, 0, z)$ and show that they are all parallel to the $y-$ axis. Attempt: Let $f(x, y, z)=x^2+z^2-1$ . Then we have that $$\nabla f(x, y, z)=(2x, 0, 2z)$$ Now, the normal vector of the tangential plane at point $(x, 0, z)$ is $$\nabla f(x, 0, z)=(2x, 0, 2z)=2(x, 0, z)$$ Since $$(x, 0, z)\cdot (0,1,0)=0$$ it follows that the vector $2(x, 0, y)$ is perpendicular to the plane $y-$ axis, which implies that the tangential plane is parallel to it.","Determine the tangent planes of at the points and show that they are all parallel to the axis. Attempt: Let . Then we have that Now, the normal vector of the tangential plane at point is Since it follows that the vector is perpendicular to the plane axis, which implies that the tangential plane is parallel to it.","x^2+z^2=1 (x, 0, z) y- f(x, y, z)=x^2+z^2-1 \nabla f(x, y, z)=(2x, 0, 2z) (x, 0, z) \nabla f(x, 0, z)=(2x, 0, 2z)=2(x, 0, z) (x, 0, z)\cdot (0,1,0)=0 2(x, 0, y) y-","['multivariable-calculus', 'differential-geometry', 'tangent-spaces']"
58,Calculating line Integral with polar coordinates,Calculating line Integral with polar coordinates,,"I want to calculate the following line integral using polar coordinates and verify my calculation is correct. $$\oint\limits_{C}2x+y^2ds\, \\C=\{(x,y)\in \Bbb{R}^2 | x^2+3y^2=8\}$$ My conversion to polar coordinates is the following $$\gamma: [0,2\pi) \rightarrow \Bbb{R}^2 \\\gamma(t) = \begin{pmatrix}\sqrt8\cos(t)\\\ \frac{\sqrt8\sin(t)}{\sqrt3} \end{pmatrix}$$ Now it should be $$\oint\limits_{C}2x+y^2ds\, = \int\limits_{0}^{2\pi}4\sqrt2\cos(t)+ \frac{8\sin^2(t)}{3} dt = \frac{8\pi}{3}$$ Is this correct?",I want to calculate the following line integral using polar coordinates and verify my calculation is correct. My conversion to polar coordinates is the following Now it should be Is this correct?,"\oint\limits_{C}2x+y^2ds\, \\C=\{(x,y)\in \Bbb{R}^2 | x^2+3y^2=8\} \gamma: [0,2\pi) \rightarrow \Bbb{R}^2 \\\gamma(t) = \begin{pmatrix}\sqrt8\cos(t)\\\ \frac{\sqrt8\sin(t)}{\sqrt3} \end{pmatrix} \oint\limits_{C}2x+y^2ds\, = \int\limits_{0}^{2\pi}4\sqrt2\cos(t)+ \frac{8\sin^2(t)}{3} dt = \frac{8\pi}{3}","['calculus', 'multivariable-calculus', 'polar-coordinates']"
59,How to show $|\log | x||^{2 \varepsilon} \cdot|x|^{-2}$ is in $L_{\mathrm{loc}}^{r}$ for every $r<n / 2$,How to show  is in  for every,|\log | x||^{2 \varepsilon} \cdot|x|^{-2} L_{\mathrm{loc}}^{r} r<n / 2,"Let $\Omega=\mathbb R^{n}$ . I wanted to show that $|\log | x||^{2 \varepsilon} \cdot|x|^{-2}$ is in $L_{\mathrm{loc}}^{r}$ for every $r<n / 2$ . Only problem is with compact set including point $0$ . I thought to use polar coordinates. Consider $B=\overline{B(0,R)}$ and $n\alpha(n)=$ surface area of unit sphere. $$\begin{align}\int_B|\log | x||^{(n-\delta) \varepsilon} \cdot|x|^{-(n-\delta)}dx&=n\alpha(n)\int_0^R|\log \varrho|^{(n-\delta) \varepsilon} \cdot \varrho^{-(n-\delta)}\varrho^{n-1}d\varrho\\ &=n\alpha(n)\int_0^R|\log \varrho|^{(n-\delta) \varepsilon} \cdot \varrho^{\delta-1}d\varrho \end{align}$$ How to show that last integral is finite for any $\delta$ such that $0\le\delta\le n$ ? Any help will be appreciated.",Let . I wanted to show that is in for every . Only problem is with compact set including point . I thought to use polar coordinates. Consider and surface area of unit sphere. How to show that last integral is finite for any such that ? Any help will be appreciated.,"\Omega=\mathbb R^{n} |\log | x||^{2 \varepsilon} \cdot|x|^{-2} L_{\mathrm{loc}}^{r} r<n / 2 0 B=\overline{B(0,R)} n\alpha(n)= \begin{align}\int_B|\log | x||^{(n-\delta) \varepsilon} \cdot|x|^{-(n-\delta)}dx&=n\alpha(n)\int_0^R|\log \varrho|^{(n-\delta) \varepsilon} \cdot \varrho^{-(n-\delta)}\varrho^{n-1}d\varrho\\
&=n\alpha(n)\int_0^R|\log \varrho|^{(n-\delta) \varepsilon} \cdot \varrho^{\delta-1}d\varrho
\end{align} \delta 0\le\delta\le n","['calculus', 'functional-analysis', 'multivariable-calculus', 'partial-differential-equations', 'lp-spaces']"
60,Wave Equation Via d'Alambert,Wave Equation Via d'Alambert,,"$$u_{tt} = 4u_{xx},\ u(x,\ 0) = \sin(\pi x),\ u_t (x,\ 0) = 0.$$ I factored the differential operators and used undetermined coefficients (can go into more detail on this part but I think it matters not) to find the general solution $$u = h(2t + x) + g(2t - x)$$ for arbitrary functions $h$ and $g$ .  Applying the initial conditions yields $$\begin{cases}\sin(\pi x) = h(x) + g(-x) \\ 0 = \frac{\partial}{\partial t}[h(2t + x) + g(2t - x)]_{t = 0}\end{cases}$$ which will require the second equation to be antidifferentiated with respect to $t$ in order for the system to be solved simultaneously. Thus, I end up with $$\begin{cases}\sin(\pi x) = h(x) + g(-x) \\ f(x) = \int \frac{\partial}{\partial t}[h(2t + x) + g(2t - x)]_{t = 0}\,\mathrm dt\end{cases}$$ for an arbitrary function $f$ . I'm not really sure how to finish solving this. I could make the substitutions $y = 2t + x$ and $z = 2t - x$ , leading to $$f(x) = 2 \int \left[\frac{\partial h(y)}{\partial y} + \frac{\partial g(z)}{\partial z}\right]_{t = 0}\,\mathrm dt,$$ but other than getting the $2$ from the chain rule derivatives pulled out, I'm not sure how to continue. I don't think I can just cancel the partial differentiation with the antidifferentiation, because of the $t = 0$ evaluation operation. How can I finish solving these equations?","I factored the differential operators and used undetermined coefficients (can go into more detail on this part but I think it matters not) to find the general solution for arbitrary functions and .  Applying the initial conditions yields which will require the second equation to be antidifferentiated with respect to in order for the system to be solved simultaneously. Thus, I end up with for an arbitrary function . I'm not really sure how to finish solving this. I could make the substitutions and , leading to but other than getting the from the chain rule derivatives pulled out, I'm not sure how to continue. I don't think I can just cancel the partial differentiation with the antidifferentiation, because of the evaluation operation. How can I finish solving these equations?","u_{tt} = 4u_{xx},\ u(x,\ 0) = \sin(\pi x),\ u_t (x,\ 0) = 0. u = h(2t + x) + g(2t - x) h g \begin{cases}\sin(\pi x) = h(x) + g(-x) \\ 0 = \frac{\partial}{\partial t}[h(2t + x) + g(2t - x)]_{t = 0}\end{cases} t \begin{cases}\sin(\pi x) = h(x) + g(-x) \\ f(x) = \int \frac{\partial}{\partial t}[h(2t + x) + g(2t - x)]_{t = 0}\,\mathrm dt\end{cases} f y = 2t + x z = 2t - x f(x) = 2 \int \left[\frac{\partial h(y)}{\partial y} + \frac{\partial g(z)}{\partial z}\right]_{t = 0}\,\mathrm dt, 2 t = 0","['multivariable-calculus', 'partial-differential-equations', 'systems-of-equations', 'chain-rule', 'wave-equation']"
61,Draw the Ellipsoid $\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1$ a tangent plane which cuts off equal segments on the coordinate axis.,Draw the Ellipsoid  a tangent plane which cuts off equal segments on the coordinate axis.,\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1,"Draw the Ellipsoid $\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1$ a tangent plane which cuts off equal segments on the coordinate axis. First, I want to find the normal line of the plane which intersects the $x,y,z$ at some distance $h$ away from the origin. $$<-h,h,0> \times<-h, 0, h> = <h^2, h^2, h^2> = h^2<1,1,1>$$ So I now need to find when there is a point where the gradient is pointing in that direction $$\triangledown f = <\frac{2x}{a^2},\frac{2y}{b^2},\frac{2z}{c^2}>$$ and the point must also satisfy the ellipsoid equation. $x = ka^2/2, y = kb^2/2,z=kc^2/2$ plugging into the ellipsoid equation yields a $$k = \frac{2}{\sqrt{a^2+b^2+c^2}}$$ which makes the gradient $$\triangledown f = <\frac{a^2}{\sqrt{a^2+b^2+c^2}},\frac{b^2}{\sqrt{a^2+b^2+c^2}},\frac{c^2}{\sqrt{a^2+b^2+c^2}}>$$ knowing $k$ I can also find the $<x,y,z>$ position and use $\triangledown f \cdot<x-x_o,y-y_o,z-z_o> =0$ . I get a horrific result of : $$a^2x+b^2y+c^2z=\frac{a^4+b^4+c^4}{\sqrt{a^2+b^2+c^2}}$$ Which is not correct. Could someone aid in creating a solution?","Draw the Ellipsoid a tangent plane which cuts off equal segments on the coordinate axis. First, I want to find the normal line of the plane which intersects the at some distance away from the origin. So I now need to find when there is a point where the gradient is pointing in that direction and the point must also satisfy the ellipsoid equation. plugging into the ellipsoid equation yields a which makes the gradient knowing I can also find the position and use . I get a horrific result of : Which is not correct. Could someone aid in creating a solution?","\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1 x,y,z h <-h,h,0> \times<-h, 0, h> = <h^2, h^2, h^2> = h^2<1,1,1> \triangledown f = <\frac{2x}{a^2},\frac{2y}{b^2},\frac{2z}{c^2}> x = ka^2/2, y = kb^2/2,z=kc^2/2 k = \frac{2}{\sqrt{a^2+b^2+c^2}} \triangledown f = <\frac{a^2}{\sqrt{a^2+b^2+c^2}},\frac{b^2}{\sqrt{a^2+b^2+c^2}},\frac{c^2}{\sqrt{a^2+b^2+c^2}}> k <x,y,z> \triangledown f \cdot<x-x_o,y-y_o,z-z_o> =0 a^2x+b^2y+c^2z=\frac{a^4+b^4+c^4}{\sqrt{a^2+b^2+c^2}}","['multivariable-calculus', 'vector-analysis']"
62,"Prove $f: \mathbb{R}^{2} \to \mathbb{R}$ has directional derivatives in $(0,0)$ but is not differentiable at $(0,0)$.",Prove  has directional derivatives in  but is not differentiable at .,"f: \mathbb{R}^{2} \to \mathbb{R} (0,0) (0,0)","Let $$f(x,y) = \begin{cases} \frac{2x^2y}{x^{2}+y^{2}}, & \mbox{if } (x,y) \neq (0,0) ,  \\ 0, & \mbox{if } (x,y) =(0,0)\end{cases}$$ be a function from $\mathbb{R}^{2}$ to $\mathbb{R}$ . Prove that in $(0,0)$ the function $f$ has directional derivatives in all directions. But $f$ is not differentiable in $(0,0)$ . For the first question  I try to show that for $v=(x,y)$ and $t \in \mathbb{R}$ : $$D_{v}(f)=lim_{t \to 0} \frac{f(0+tv)-f(0)}{t}$$ . exist. This proves directional derivatives that in $(0,0)$ directional derivatives exist in any direction right? But I cant prove this limit is $0$ . So far: $$\frac{f(0+tv)-f(0)}{t}=\frac{f(tv)}{t}=\frac{f(tx,ty)}{t}=\frac{\frac{2(tx)^{2}(ty)}{(tx)^{2}+(ty)^{2}}}{t}=\frac{2x^{2}y}{x^{2}+y^{2}}.$$ Then $D_{v}(f)=lim_{t \to 0} \frac{f(0+tv)-f(0)}{t}$ exist and is equal to the constant $\frac{2x^{2}y}{x^{2}+y^{2}}$ . This proves directional derivatives exist in $(0,0)$ in all directions. In order to show that $f$ is not differentiable in $(0,0)$ , I attempted to prove $f$ is not continuous at $(0,0)$ . But cant find the path to prove this, for instance $$lim_{t \to 0}f(t,t)=lim_{t \to 0} t=0=f(0,0)$$ also tried $$lim_{t \to 0}f(t^{3},t)=lim_{t \to 0} t^{3}=0=f(0,0)$$ .","Let be a function from to . Prove that in the function has directional derivatives in all directions. But is not differentiable in . For the first question  I try to show that for and : . exist. This proves directional derivatives that in directional derivatives exist in any direction right? But I cant prove this limit is . So far: Then exist and is equal to the constant . This proves directional derivatives exist in in all directions. In order to show that is not differentiable in , I attempted to prove is not continuous at . But cant find the path to prove this, for instance also tried .","f(x,y) = \begin{cases} \frac{2x^2y}{x^{2}+y^{2}}, & \mbox{if } (x,y) \neq (0,0) ,  \\ 0, & \mbox{if } (x,y) =(0,0)\end{cases} \mathbb{R}^{2} \mathbb{R} (0,0) f f (0,0) v=(x,y) t \in \mathbb{R} D_{v}(f)=lim_{t \to 0} \frac{f(0+tv)-f(0)}{t} (0,0) 0 \frac{f(0+tv)-f(0)}{t}=\frac{f(tv)}{t}=\frac{f(tx,ty)}{t}=\frac{\frac{2(tx)^{2}(ty)}{(tx)^{2}+(ty)^{2}}}{t}=\frac{2x^{2}y}{x^{2}+y^{2}}. D_{v}(f)=lim_{t \to 0} \frac{f(0+tv)-f(0)}{t} \frac{2x^{2}y}{x^{2}+y^{2}} (0,0) f (0,0) f (0,0) lim_{t \to 0}f(t,t)=lim_{t \to 0} t=0=f(0,0) lim_{t \to 0}f(t^{3},t)=lim_{t \to 0} t^{3}=0=f(0,0)","['real-analysis', 'calculus', 'multivariable-calculus', 'derivatives', 'continuity']"
63,Do generic smooth functions have no degenerate points?,Do generic smooth functions have no degenerate points?,,"In this question, $\mathbb{N}$ includes $0$ . Let $M$ be a compact $D$ -dimensional $C^\infty$ -smooth manifold, and fix a finite atlas $\,\mathcal{U}=\{\phi_j \,\colon U_j \to \mathbb{R}^D\}_{j=1,\ldots,n}$ of $C^\infty$ charts on $M$ . Let $A=\{\boldsymbol{\alpha}_1,\ldots,\boldsymbol{\alpha}_{D+1}\}$ be a set of $D+1$ distinct multi-indices $\boldsymbol{\alpha}_i \in \mathbb{N}^D$ , and let $K=\max\{\|\boldsymbol{\alpha}_1\|_1,\ldots,\|\boldsymbol{\alpha}_{D+1}\|_1\}$ . Let $\mathcal{F}$ be the set of all $C^\infty$ functions $f \colon M \to \mathbb{R}$ with the property that for every chart $\,\phi_j \, \colon U_j \to \mathbb{R}^D$ in $\,\mathcal{U}$ , for each $\mathbf{x} \in \phi_j(U_j)$ , at least one of the multi-indices $\boldsymbol{\alpha}_i \in A$ has $\partial_{\boldsymbol{\alpha}_{i\!}}(f \circ \phi_j^{-1})(\mathbf{x}) \neq 0$ . Is it the case that for every $k \in \{K,K+1,\ldots\} \cup \{\infty\}$ , $\mathcal{F}$ is dense in $C^k(M,\mathbb{R})$ ? My intuition is that the answer is yes , and furthermore that this result – or something similar – should be a well-known result (probably with a name), or should at least exist in some textbook. Is there a reference for this result, or a name for this result?","In this question, includes . Let be a compact -dimensional -smooth manifold, and fix a finite atlas of charts on . Let be a set of distinct multi-indices , and let . Let be the set of all functions with the property that for every chart in , for each , at least one of the multi-indices has . Is it the case that for every , is dense in ? My intuition is that the answer is yes , and furthermore that this result – or something similar – should be a well-known result (probably with a name), or should at least exist in some textbook. Is there a reference for this result, or a name for this result?","\mathbb{N} 0 M D C^\infty \,\mathcal{U}=\{\phi_j \,\colon U_j \to \mathbb{R}^D\}_{j=1,\ldots,n} C^\infty M A=\{\boldsymbol{\alpha}_1,\ldots,\boldsymbol{\alpha}_{D+1}\} D+1 \boldsymbol{\alpha}_i \in \mathbb{N}^D K=\max\{\|\boldsymbol{\alpha}_1\|_1,\ldots,\|\boldsymbol{\alpha}_{D+1}\|_1\} \mathcal{F} C^\infty f \colon M \to \mathbb{R} \,\phi_j \, \colon U_j \to \mathbb{R}^D \,\mathcal{U} \mathbf{x} \in \phi_j(U_j) \boldsymbol{\alpha}_i \in A \partial_{\boldsymbol{\alpha}_{i\!}}(f \circ \phi_j^{-1})(\mathbf{x}) \neq 0 k \in \{K,K+1,\ldots\} \cup \{\infty\} \mathcal{F} C^k(M,\mathbb{R})","['real-analysis', 'multivariable-calculus', 'differential-geometry', 'reference-request']"
64,"Is there a way to unify curl, gradient, and divergence operators as special cases of a more general operator?","Is there a way to unify curl, gradient, and divergence operators as special cases of a more general operator?",,"I realize there's probably an answer to this question somewhere on this site, but it would seem I'm having trouble picking the right search terms. In my multivariable calculus class, the professor just spent some time going over the relationships between the integral theorems of calculus by essentially saying that they all are examples of how integrating some function over the boundary of a domain (path, surface, solid) is the same as integrating some form of its ""derivative"" over the interior of that domain. The issue is, while the boundary operator seems to be consistent across all of the integral theorems presented (line integral theorem, Stokes' theorem, and Gauss' theorem), the notion of the derivative is different in each one: for Stokes', it's the curl; for Gauss', it's the divergence; and for the line integral theorem, it's the gradient. I was wondering: is there any natural way of generalizing the curl, divergence, and gradient operators, so that they become special cases of some more broad notion of ""taking the derivative""? Or perhaps to other dimensions? (Or is there something special about $\mathbb R^{3}$ that makes it unique for vector calculus?) (Sorry if this question is too open-ended or ill-defined for this site; regardless, if anybody has any suggestions on topics to study or other resources to consult, I would very much appreciate those as well.)","I realize there's probably an answer to this question somewhere on this site, but it would seem I'm having trouble picking the right search terms. In my multivariable calculus class, the professor just spent some time going over the relationships between the integral theorems of calculus by essentially saying that they all are examples of how integrating some function over the boundary of a domain (path, surface, solid) is the same as integrating some form of its ""derivative"" over the interior of that domain. The issue is, while the boundary operator seems to be consistent across all of the integral theorems presented (line integral theorem, Stokes' theorem, and Gauss' theorem), the notion of the derivative is different in each one: for Stokes', it's the curl; for Gauss', it's the divergence; and for the line integral theorem, it's the gradient. I was wondering: is there any natural way of generalizing the curl, divergence, and gradient operators, so that they become special cases of some more broad notion of ""taking the derivative""? Or perhaps to other dimensions? (Or is there something special about that makes it unique for vector calculus?) (Sorry if this question is too open-ended or ill-defined for this site; regardless, if anybody has any suggestions on topics to study or other resources to consult, I would very much appreciate those as well.)",\mathbb R^{3},"['multivariable-calculus', 'vector-analysis']"
65,"Understanding the proof of how ""only vector fields that are independent of path are conservative""","Understanding the proof of how ""only vector fields that are independent of path are conservative""",,"In James Stewart's Multivariable Calculus: Concepts and Contexts, this theorem is stated: ""Suppose $\mathbf{\vec F}$ is a vector field that is continuous on an open connected region $\mathit{D}$ . If $\int_C \mathbf{\vec F} \cdot d\mathbf{\vec r}$ is independent of path in $\mathit{D}$ , then $\mathbf{\vec F}$ is a conservative vector field on $\mathit{D}$ ; that is, there exists a function $\mathit{f}$ such that $\nabla \mathit{f} = \mathbf{\vec F}$ ."" The proof is as follows: ""Let $\mathit{A(a,b)}$ be a fixed point in $\mathit{D}$ . We construct the desired potential function $\mathit{f}$ by defining $$ \mathit{f\,(x,y)} = \int_\mathit{(a,b)}^\mathit{(x,y)} \mathbf{\vec F} \cdot d\mathbf{\vec r}$$ for any point in $\mathit{(x,y)}$ in $\mathit{D}$ . Since $\int_C \mathbf{\vec F}\,d\mathbf{\vec r}$ is independent of path, it does not matter which path $\mathit{C}$ from $\mathit{(a,b)}$ to $\mathit{(x,y)}$ is used to evaluate $\mathit{f\,(x,y)}$ . Since $\mathit{D}$ is open, there exists a disk contained in $\mathit{D}$ with center $\mathit{(x,y)}$ . Choose any point $\mathit{(x_1,y)}$ in the disk with $\mathit{x_1 \lt x}$ and let $\mathit{C}$ consist of any path $\mathit{C_1}$ from $\mathit{(a,b)}$ to $\mathit{(x_1,y)}$ followed by the horizontal line segment $\mathit{C_2}$ from $\mathit{(x_1,y)}$ to $\mathit{(x,y)}$ . Then $$ \mathit{f\,(x,y)} = \int_\mathit{C_1} \mathbf{\vec F} \cdot d\mathbf{\vec r}\, + \int_\mathit{C_2} \mathbf{\vec F} \cdot d\mathbf{\vec r} = \int_\mathit{(a,b)}^\mathit{(x_1,y)} \mathbf{\vec F} \cdot d\mathbf{\vec r}\, + \int_\mathit{C_2} \mathbf{\vec F} \cdot d\mathbf{\vec r}$$ Notice that the first of these integrals does not depend on $\mathit{x}$ , so $$ \frac{\partial}{\partial x}\;\mathit{f\,(x,y)} = 0 + \frac{\partial}{\partial x}\int_\mathit{C_2} \mathbf{\vec F} \cdot d\mathbf{\vec r}$$ If we write $\mathbf{\vec F} = \mathit{P}\,\mathbf{\vec i} + \mathit{Q}\,\mathbf{\vec j}$ , then $$ \int_\mathit{C_2} \mathbf{\vec F} \cdot d\mathbf{\vec r} = \int_\mathit{C_2} P\,dx + Q\,dy$$ On $\mathit{C_2}$ , $y$ is constant, so $dy = 0$ . Using $t$ as the parameter, where $x_1 \leqslant t \leqslant x$ , we have $$ \frac{\partial}{\partial x}\;\mathit{f\,(x,y)} = \frac{\partial}{\partial x} \int_\mathit{C_2} P\,dx + Q\,dy = \frac{\partial}{\partial x} \int_\mathit{x_1}^x P(t,y)\,dt = P(x,y)$$ by Part 1 of the Fundamental Theorem of Calculus."" A similar process of deriving $\frac{\partial}{\partial y} \;\mathit{f\,(x,y)}$ is used to show that $\mathbf{\vec F} = \mathit{P}\,\mathbf{\vec i} + \mathit{Q}\,\mathbf{\vec j} = \frac{\partial f}{\partial x}\;\mathbf{\vec i} + \frac{\partial f}{\partial y}\;\mathbf{\vec j} = \nabla f$ . I have two questions: How is $\int_\mathit{(a,b)}^\mathit{(x_1,y)} \mathbf{\vec F} \cdot d\mathbf{\vec r}$ not dependent on $x$ ? How does writing $\mathbf{\vec F} = \mathit{P}\,\mathbf{\vec i} + \mathit{Q}\,\mathbf{\vec j}$ show $ \int_\mathit{C_2} \mathbf{\vec F} \cdot d\mathbf{\vec r} = \int_\mathit{C_2} P\,dx + Q\,dy$ ? Thank you so much for reading through the long post, and apologies in advance for poor formatting. Any help is greatly appreciated.","In James Stewart's Multivariable Calculus: Concepts and Contexts, this theorem is stated: ""Suppose is a vector field that is continuous on an open connected region . If is independent of path in , then is a conservative vector field on ; that is, there exists a function such that ."" The proof is as follows: ""Let be a fixed point in . We construct the desired potential function by defining for any point in in . Since is independent of path, it does not matter which path from to is used to evaluate . Since is open, there exists a disk contained in with center . Choose any point in the disk with and let consist of any path from to followed by the horizontal line segment from to . Then Notice that the first of these integrals does not depend on , so If we write , then On , is constant, so . Using as the parameter, where , we have by Part 1 of the Fundamental Theorem of Calculus."" A similar process of deriving is used to show that . I have two questions: How is not dependent on ? How does writing show ? Thank you so much for reading through the long post, and apologies in advance for poor formatting. Any help is greatly appreciated.","\mathbf{\vec F} \mathit{D} \int_C \mathbf{\vec F} \cdot d\mathbf{\vec r} \mathit{D} \mathbf{\vec F} \mathit{D} \mathit{f} \nabla \mathit{f} = \mathbf{\vec F} \mathit{A(a,b)} \mathit{D} \mathit{f}  \mathit{f\,(x,y)} = \int_\mathit{(a,b)}^\mathit{(x,y)} \mathbf{\vec F} \cdot d\mathbf{\vec r} \mathit{(x,y)} \mathit{D} \int_C \mathbf{\vec F}\,d\mathbf{\vec r} \mathit{C} \mathit{(a,b)} \mathit{(x,y)} \mathit{f\,(x,y)} \mathit{D} \mathit{D} \mathit{(x,y)} \mathit{(x_1,y)} \mathit{x_1 \lt x} \mathit{C} \mathit{C_1} \mathit{(a,b)} \mathit{(x_1,y)} \mathit{C_2} \mathit{(x_1,y)} \mathit{(x,y)}  \mathit{f\,(x,y)} = \int_\mathit{C_1} \mathbf{\vec F} \cdot d\mathbf{\vec r}\, + \int_\mathit{C_2} \mathbf{\vec F} \cdot d\mathbf{\vec r} = \int_\mathit{(a,b)}^\mathit{(x_1,y)} \mathbf{\vec F} \cdot d\mathbf{\vec r}\, + \int_\mathit{C_2} \mathbf{\vec F} \cdot d\mathbf{\vec r} \mathit{x}  \frac{\partial}{\partial x}\;\mathit{f\,(x,y)} = 0 + \frac{\partial}{\partial x}\int_\mathit{C_2} \mathbf{\vec F} \cdot d\mathbf{\vec r} \mathbf{\vec F} = \mathit{P}\,\mathbf{\vec i} + \mathit{Q}\,\mathbf{\vec j}  \int_\mathit{C_2} \mathbf{\vec F} \cdot d\mathbf{\vec r} = \int_\mathit{C_2} P\,dx + Q\,dy \mathit{C_2} y dy = 0 t x_1 \leqslant t \leqslant x  \frac{\partial}{\partial x}\;\mathit{f\,(x,y)} = \frac{\partial}{\partial x} \int_\mathit{C_2} P\,dx + Q\,dy = \frac{\partial}{\partial x} \int_\mathit{x_1}^x P(t,y)\,dt = P(x,y) \frac{\partial}{\partial y} \;\mathit{f\,(x,y)} \mathbf{\vec F} = \mathit{P}\,\mathbf{\vec i} + \mathit{Q}\,\mathbf{\vec j} = \frac{\partial f}{\partial x}\;\mathbf{\vec i} + \frac{\partial f}{\partial y}\;\mathbf{\vec j} = \nabla f \int_\mathit{(a,b)}^\mathit{(x_1,y)} \mathbf{\vec F} \cdot d\mathbf{\vec r} x \mathbf{\vec F} = \mathit{P}\,\mathbf{\vec i} + \mathit{Q}\,\mathbf{\vec j}  \int_\mathit{C_2} \mathbf{\vec F} \cdot d\mathbf{\vec r} = \int_\mathit{C_2} P\,dx + Q\,dy","['multivariable-calculus', 'vector-fields', 'line-integrals']"
66,Removing a null set from a Jordan-measurable set gives a Jordan-measurable set?,Removing a null set from a Jordan-measurable set gives a Jordan-measurable set?,,"Let $D$ be a Jordan-measurable subset of $\mathbb{R}^n$ and $S$ be a null set (By a null set I mean a set whose Lebesgue (outer) measure equals 0). In general $D\setminus S$ would not be Jordan-measurable, as the example $S=\mathbb{Q}$ shows. However, my textbook says that if we assume further that the set $D\setminus S$ is open, then $D\setminus S$ must be Jordan-measurable. The book does not give a detailed explanation so I tried to figure this out by myself. I would like to check if my argument is valid, and since my argument is somewhat lengthy, if shorter explanation is possible. We may assume $S\subset D$ . Since $S$ is a null set, its interior must be empty and we have $\overline{S}=\text{Int}(S)\cup\text{Bd}(S)=\text{Bd}(S)$ . Since $D\setminus S$ is open and disjoint from $S$ we have $(D\setminus S)\subset\text{Ext}(S)$ , in other words, $(D\setminus S)\cap\overline{S}=\varnothing$ . Let $p\in\overline{S}$ and assume that $p\notin S$ : If $p\in\text{Int}(D)$ then $p$ becomes a member of $D\setminus S$ but this is impossible since $(D\setminus S)\cap\overline{S}=\varnothing$ . Our assumption $S\subset D$ implies that $p\in\overline{S}\subset\overline{D}=\text{Int}(D)\cup\text{Bd}(D)$ , so we have $p\in\text{Bd}(D)$ . Thus if $p\in\overline{S}$ , then either $p\in S$ or $p\in\text{Bd}(D)$ : we have shown that $\overline{S}\subset S\cup\text{Bd}(D)$ . This implies $S\cup\text{Bd}(D)=\overline{S}\cup\text{Bd}(D)$ , and this last expression is a union of two compact sets. On the other hand, the expression $S\cup\text{Bd}(D)$ is a union of two null sets (since $D$ is Jordan-measurable, $\text{Bd}(D)$ is a null set), so this set is also a null set. Finally since $\text{Bd}(D\setminus S)\subset\text{Bd}(D)\cup\text{Bd}(S)=\text{Bd}(D)\cup\overline{S}$ , the set $\text{Bd}(D\setminus S)$ is a subset of a null set, hence is a null set on its own. Hence $D\setminus S$ is Jordan-measurable. Is the above argument valid?","Let be a Jordan-measurable subset of and be a null set (By a null set I mean a set whose Lebesgue (outer) measure equals 0). In general would not be Jordan-measurable, as the example shows. However, my textbook says that if we assume further that the set is open, then must be Jordan-measurable. The book does not give a detailed explanation so I tried to figure this out by myself. I would like to check if my argument is valid, and since my argument is somewhat lengthy, if shorter explanation is possible. We may assume . Since is a null set, its interior must be empty and we have . Since is open and disjoint from we have , in other words, . Let and assume that : If then becomes a member of but this is impossible since . Our assumption implies that , so we have . Thus if , then either or : we have shown that . This implies , and this last expression is a union of two compact sets. On the other hand, the expression is a union of two null sets (since is Jordan-measurable, is a null set), so this set is also a null set. Finally since , the set is a subset of a null set, hence is a null set on its own. Hence is Jordan-measurable. Is the above argument valid?",D \mathbb{R}^n S D\setminus S S=\mathbb{Q} D\setminus S D\setminus S S\subset D S \overline{S}=\text{Int}(S)\cup\text{Bd}(S)=\text{Bd}(S) D\setminus S S (D\setminus S)\subset\text{Ext}(S) (D\setminus S)\cap\overline{S}=\varnothing p\in\overline{S} p\notin S p\in\text{Int}(D) p D\setminus S (D\setminus S)\cap\overline{S}=\varnothing S\subset D p\in\overline{S}\subset\overline{D}=\text{Int}(D)\cup\text{Bd}(D) p\in\text{Bd}(D) p\in\overline{S} p\in S p\in\text{Bd}(D) \overline{S}\subset S\cup\text{Bd}(D) S\cup\text{Bd}(D)=\overline{S}\cup\text{Bd}(D) S\cup\text{Bd}(D) D \text{Bd}(D) \text{Bd}(D\setminus S)\subset\text{Bd}(D)\cup\text{Bd}(S)=\text{Bd}(D)\cup\overline{S} \text{Bd}(D\setminus S) D\setminus S,"['multivariable-calculus', 'lebesgue-measure', 'riemann-integration']"
67,Prove : $\frac{a}{b+c}+\frac{b}{a+c}+\frac{c}{a+b}\geq\sqrt{\frac{9}{4}+\frac{3}{2}\frac{(a-b)^2(a+b+c)}{(a+b)(b+c)(c+a)}}$,Prove :,\frac{a}{b+c}+\frac{b}{a+c}+\frac{c}{a+b}\geq\sqrt{\frac{9}{4}+\frac{3}{2}\frac{(a-b)^2(a+b+c)}{(a+b)(b+c)(c+a)}},"It's an inequality based on two found on the website MSE (see the reference): Let $a,b,c>0$ then we have: $$\frac{a}{b+c}+\frac{b}{a+c}+\frac{c}{a+b}\geq\sqrt{\frac{9}{4}+\frac{3}{2}\frac{(a-b)^2(a+b+c)}{(a+b)(b+c)(c+a)}}$$ Lemma 1 : $a,b,c>0$ then we have : $$\sum_{cyc}\frac{a}{b+c}\geq P(a,b,c)=\sqrt{\frac{9}{4}+\frac{9}{4}\frac{(c^4+a^4+b^4-c^2a^2-b^2a^2-c^2b^2)}{((a+b+c)\frac{3}{4}+\frac{3}{4}(abc)^{\frac{1}{3}})(a+b)(b+c)(c+a)}+\frac{(c^2+a^2+b^2-ca-ba-cb)(a+b+c)}{(a+b)(b+c)(c+a)}}$$ Proof of lemma 1 : First we remark that the inequality is homogenous and we can try the substitution $3u=a+b+c$ , $3v^2=ab+bc+ca$ and $w^3=abc$ and apply the uvw's method . We have : $$a^4+b^4+c^4=(9u^2-6v^2)^2-2(9v^4-6uw^3)$$ $$a^2b^2+b^2c^2+c^2a^2=9v^4-6uw^3$$ $$a^2+b^2+c^2=9u^2-6v^2$$ And : $\left(\frac{((3u)((3u)^2-4(3v^2))+5w^3)}{3u3v^2-w^3}+2\right)^2\geq \frac{9}{4}+\frac{(9/4)((9u^2-6v^2)^2-2(9v^4-6uw^3)-(9v^4-6uw^3))+(2.25u+0.75w)(9u^2-9v^2)(3u)}{(2.25u+0.75w)(3u3v^2-w^3)}$ it's enough to find an extreme value of our expression for the extreme value of $w^3$ wich happens for an equality case of two variables . Since the last inequality is homogeneous, we can assume that $b=c=1$ . $$\frac{2}{a+1}+\frac{a}{2}\geq\sqrt{\frac{9}{4}+\frac{9}{4}\frac{(a^4+1-2a^2)}{((a+2)\frac{3}{4}+\frac{3}{4}(a)^{\frac{1}{3}})(a+1)^2(2)}+\frac{(a^2+1-2a)(a+2)}{(a+1)^2(2)}}$$ Now it seems to be clear : we get a polynomial  with a root equal to one . See the factorization by Wolfram alpha . End of the proof of the lemma 1 Remains to show that $ a\geq b \geq c>0$ : $$P(a,b,c)\geq\sqrt{\frac{9}{4}+\frac{3}{2}\frac{(a-b)^2(a+b+c)}{(a+b)(b+c)(c+a)}}$$ Wich is not hard I think . Question : How to show it ? Reference : M. A. Rozenberg, “uvw–Method in Proving Inequalities”, Math. Ed., 2011, no. 3-4(59-60), 6–14 If $x,y,z>0$, prove that: $\frac{x}{y+z}+\frac{y}{x+z}+\frac{z}{x+y}\ge \sqrt{2}\sqrt{2-\frac{7xyz}{(x+y)(y+z)(x+z)}}$ Stronger than Nesbitt inequality","It's an inequality based on two found on the website MSE (see the reference): Let then we have: Lemma 1 : then we have : Proof of lemma 1 : First we remark that the inequality is homogenous and we can try the substitution , and and apply the uvw's method . We have : And : it's enough to find an extreme value of our expression for the extreme value of wich happens for an equality case of two variables . Since the last inequality is homogeneous, we can assume that . Now it seems to be clear : we get a polynomial  with a root equal to one . See the factorization by Wolfram alpha . End of the proof of the lemma 1 Remains to show that : Wich is not hard I think . Question : How to show it ? Reference : M. A. Rozenberg, “uvw–Method in Proving Inequalities”, Math. Ed., 2011, no. 3-4(59-60), 6–14 If $x,y,z>0$, prove that: $\frac{x}{y+z}+\frac{y}{x+z}+\frac{z}{x+y}\ge \sqrt{2}\sqrt{2-\frac{7xyz}{(x+y)(y+z)(x+z)}}$ Stronger than Nesbitt inequality","a,b,c>0 \frac{a}{b+c}+\frac{b}{a+c}+\frac{c}{a+b}\geq\sqrt{\frac{9}{4}+\frac{3}{2}\frac{(a-b)^2(a+b+c)}{(a+b)(b+c)(c+a)}} a,b,c>0 \sum_{cyc}\frac{a}{b+c}\geq P(a,b,c)=\sqrt{\frac{9}{4}+\frac{9}{4}\frac{(c^4+a^4+b^4-c^2a^2-b^2a^2-c^2b^2)}{((a+b+c)\frac{3}{4}+\frac{3}{4}(abc)^{\frac{1}{3}})(a+b)(b+c)(c+a)}+\frac{(c^2+a^2+b^2-ca-ba-cb)(a+b+c)}{(a+b)(b+c)(c+a)}} 3u=a+b+c 3v^2=ab+bc+ca w^3=abc a^4+b^4+c^4=(9u^2-6v^2)^2-2(9v^4-6uw^3) a^2b^2+b^2c^2+c^2a^2=9v^4-6uw^3 a^2+b^2+c^2=9u^2-6v^2 \left(\frac{((3u)((3u)^2-4(3v^2))+5w^3)}{3u3v^2-w^3}+2\right)^2\geq \frac{9}{4}+\frac{(9/4)((9u^2-6v^2)^2-2(9v^4-6uw^3)-(9v^4-6uw^3))+(2.25u+0.75w)(9u^2-9v^2)(3u)}{(2.25u+0.75w)(3u3v^2-w^3)} w^3 b=c=1 \frac{2}{a+1}+\frac{a}{2}\geq\sqrt{\frac{9}{4}+\frac{9}{4}\frac{(a^4+1-2a^2)}{((a+2)\frac{3}{4}+\frac{3}{4}(a)^{\frac{1}{3}})(a+1)^2(2)}+\frac{(a^2+1-2a)(a+2)}{(a+1)^2(2)}}  a\geq b \geq c>0 P(a,b,c)\geq\sqrt{\frac{9}{4}+\frac{3}{2}\frac{(a-b)^2(a+b+c)}{(a+b)(b+c)(c+a)}}","['multivariable-calculus', 'inequality']"
68,Problem in Double Integral by Change of Order,Problem in Double Integral by Change of Order,,"$$\text{Evaluate by changing the order} \int^1_0\int^y_{4y}e^{x^{2}}dx\ dy.$$ I am unable to solve the following question. I have tried using the following approach. First I formed the equation of lines using the limits of the inner integral which gave equations of $2$ lines. $$ y = x $$ $$ y = x/4 $$ and the limits of the outer integral gave me the total bounded region as below From what I understood, I have to calculate the volume of function $e^{x^2}$ within the bounded area. Since, from the current order, calculating the integral is difficult, I tried to change the order of integral (as given in question) so this is what I did : First I thought the current integral divides the current region into small $dA$ and first integrating by $dx$ means that we are taking a strip parallel to $X$ -axis with length between $y = x$ and $y = x/4$ and then integrated it all the way above from 0 to 1. Then I tried to change this order and thought of integrating first w.r.t. $dy$ as this will mean I will divide the region into strips parallel to $Y$ -axis but the equation will be divided into two with the inner limits of first being $x/4$ to $x$ and second from $x/4$ to $1$ . and the outer limit will change to 0 to 1 for first and 1 to 4 for second. I was solving the integral but then I encountered a problem. I am unable to integrate the highlighted term any further. Please tell me where I went wrong. Note: I forgot the $x$ in the first term, it will be $\frac{3xe^{x^2}}{4}$","I am unable to solve the following question. I have tried using the following approach. First I formed the equation of lines using the limits of the inner integral which gave equations of lines. and the limits of the outer integral gave me the total bounded region as below From what I understood, I have to calculate the volume of function within the bounded area. Since, from the current order, calculating the integral is difficult, I tried to change the order of integral (as given in question) so this is what I did : First I thought the current integral divides the current region into small and first integrating by means that we are taking a strip parallel to -axis with length between and and then integrated it all the way above from 0 to 1. Then I tried to change this order and thought of integrating first w.r.t. as this will mean I will divide the region into strips parallel to -axis but the equation will be divided into two with the inner limits of first being to and second from to . and the outer limit will change to 0 to 1 for first and 1 to 4 for second. I was solving the integral but then I encountered a problem. I am unable to integrate the highlighted term any further. Please tell me where I went wrong. Note: I forgot the in the first term, it will be",\text{Evaluate by changing the order} \int^1_0\int^y_{4y}e^{x^{2}}dx\ dy. 2  y = x   y = x/4  e^{x^2} dA dx X y = x y = x/4 dy Y x/4 x x/4 1 x \frac{3xe^{x^2}}{4},"['multivariable-calculus', 'definite-integrals', 'multiple-integral']"
69,Converts into $\frac{\partial^2 W}{\partial u^2}+\frac{\partial^2 W}{\partial v^2}=0$,Converts into,\frac{\partial^2 W}{\partial u^2}+\frac{\partial^2 W}{\partial v^2}=0,"Show that the substitution $u=x^2-y^2$ , $v=2xy$ converts the equation $\frac{\partial^2 W}{\partial x^2}+\frac{\partial^2 W}{\partial y^2}=0$ into $\frac{\partial^2 W}{\partial u^2}+\frac{\partial^2 W}{\partial v^2}=0$ I have $$\left\lbrace u=x^2-y^2 \atop v=2xy \right. $$ So $$\frac{\partial W}{\partial x} = \frac{\partial W}{\partial u}\frac{\partial u}{\partial x}+\frac{\partial W}{\partial v}\frac{\partial v}{\partial y} $$ and $$\frac{\partial W}{\partial y} = \frac{\partial W}{\partial u}\frac{\partial u}{\partial y}+\frac{\partial W}{\partial v}\frac{\partial v}{\partial y} $$ I can compute for example $\frac{\partial u}{\partial x}=2x$ , $\frac{\partial u}{\partial y}=-2y $ , $\frac{\partial v}{\partial x}=2y$ and $\frac{\partial v}{\partial y}=2x$ but how to find $\frac{\partial W}{\partial u}$ or $\frac{\partial W}{\partial v}$ ?","Show that the substitution , converts the equation into I have So and I can compute for example , , and but how to find or ?",u=x^2-y^2 v=2xy \frac{\partial^2 W}{\partial x^2}+\frac{\partial^2 W}{\partial y^2}=0 \frac{\partial^2 W}{\partial u^2}+\frac{\partial^2 W}{\partial v^2}=0 \left\lbrace u=x^2-y^2 \atop v=2xy \right.  \frac{\partial W}{\partial x} = \frac{\partial W}{\partial u}\frac{\partial u}{\partial x}+\frac{\partial W}{\partial v}\frac{\partial v}{\partial y}  \frac{\partial W}{\partial y} = \frac{\partial W}{\partial u}\frac{\partial u}{\partial y}+\frac{\partial W}{\partial v}\frac{\partial v}{\partial y}  \frac{\partial u}{\partial x}=2x \frac{\partial u}{\partial y}=-2y  \frac{\partial v}{\partial x}=2y \frac{\partial v}{\partial y}=2x \frac{\partial W}{\partial u} \frac{\partial W}{\partial v},['multivariable-calculus']
70,Do the position and velocity vectors live in the same space?,Do the position and velocity vectors live in the same space?,,"This is a question I've been thinking about for a while. Position vectors are supposed be represented by an arrow from the origin that traces out a path, while one does not need to think of the velocity vector as having any sort of starting point because as long as the length and direction and the same, they represent the same velocity. From a pure math perspective, a vector space is just an object with a dozen or so properties with closure being one of them. But it makes no sense to add a position and a velocity vector right? So that means they do not belong to the same vector space. But at the same time, it is possible to take the dot or cross products of a position and velocity vector. So how do you explain that? Also when we learn multivariable calculus, why do we need the position vector to originate from the origin? $\Bbb R^3$ or $\Bbb R^2$ satisfies all the properties of a vector space without needing to conjure up any sort arrow.","This is a question I've been thinking about for a while. Position vectors are supposed be represented by an arrow from the origin that traces out a path, while one does not need to think of the velocity vector as having any sort of starting point because as long as the length and direction and the same, they represent the same velocity. From a pure math perspective, a vector space is just an object with a dozen or so properties with closure being one of them. But it makes no sense to add a position and a velocity vector right? So that means they do not belong to the same vector space. But at the same time, it is possible to take the dot or cross products of a position and velocity vector. So how do you explain that? Also when we learn multivariable calculus, why do we need the position vector to originate from the origin? or satisfies all the properties of a vector space without needing to conjure up any sort arrow.",\Bbb R^3 \Bbb R^2,"['linear-algebra', 'multivariable-calculus', 'differential-geometry', 'vector-spaces']"
71,"If $D=\{(x,y):x^2+y^2\leq 1\}$. Show there is $p_0\in D$ such that $T(p_0)=(0,0)$",If . Show there is  such that,"D=\{(x,y):x^2+y^2\leq 1\} p_0\in D T(p_0)=(0,0)","Let $D=\{(x,y):x^2+y^2\leq 1\}$ , $T$ transformation of class $C'$ on an open containing $D$ , $$T:\left\lbrace \begin{array}{rcl} u &=& f(x,y) \\ v&=&g(x,y)  \end{array}\right.  $$ whose Jacobian is never $0$ in $D$ , and $|T(p)-p|\leq \frac{1}{3}$ for all $p\in D$ . Show there is $p_0\in D$ such that $T(p_0)=(0,0)$ Idea: Seems like fixed point theorem can help, for example if  I define $H(p)=T(p)-p$ , then $H(p)=p$ iff $T(p)=0$ , so it's enough to find such point for $H$ , by hypothesis $|H(p)|\leq \frac{1}{3}$ . Usually the fixed theorem is stated as: if $f:D\to D$ is continuous then there is a fixed point, or also there is no retraction of class $C'$ from $D$ onto the unit circle (only the boundary of $D$ ). Here I know that the jacobian of $T$ , $J(p)\neq 0$ for all $p\in D$ so there is a neighborhood of any $p\in D$ where $T$ is $1$ -to- $1$ , i.e., in where $T^{-1}$ exists. But at least for now I can't have a precise idea. Any hints are welcome. For context, this is a problem #17 section 7.5 from Buck's Advance Calculus. In this section we have the results: For $T:D\subset\mathbb{R}^n \to \mathbb{R}^n$ of class $C'$ , $D$ open, with $J(p)\neq 0$ for all $p\in D$ then $T$ is locally $1$ -to- $1$ in $D$ . If $J(p_0)\neq 0$ only for a specific value, then $T$ is $1$ -to- $1$ only in a small neighborhood of $p_0$ , where $T^{-1}$ exists. If conditions as in 1.,then $T(D)$ is an open set.","Let , transformation of class on an open containing , whose Jacobian is never in , and for all . Show there is such that Idea: Seems like fixed point theorem can help, for example if  I define , then iff , so it's enough to find such point for , by hypothesis . Usually the fixed theorem is stated as: if is continuous then there is a fixed point, or also there is no retraction of class from onto the unit circle (only the boundary of ). Here I know that the jacobian of , for all so there is a neighborhood of any where is -to- , i.e., in where exists. But at least for now I can't have a precise idea. Any hints are welcome. For context, this is a problem #17 section 7.5 from Buck's Advance Calculus. In this section we have the results: For of class , open, with for all then is locally -to- in . If only for a specific value, then is -to- only in a small neighborhood of , where exists. If conditions as in 1.,then is an open set.","D=\{(x,y):x^2+y^2\leq 1\} T C' D T:\left\lbrace \begin{array}{rcl} u &=& f(x,y) \\ v&=&g(x,y)  \end{array}\right.   0 D |T(p)-p|\leq \frac{1}{3} p\in D p_0\in D T(p_0)=(0,0) H(p)=T(p)-p H(p)=p T(p)=0 H |H(p)|\leq \frac{1}{3} f:D\to D C' D D T J(p)\neq 0 p\in D p\in D T 1 1 T^{-1} T:D\subset\mathbb{R}^n \to \mathbb{R}^n C' D J(p)\neq 0 p\in D T 1 1 D J(p_0)\neq 0 T 1 1 p_0 T^{-1} T(D)",['multivariable-calculus']
72,Rectangle in polar coordinates,Rectangle in polar coordinates,,"Suppose that we have $D=[-a,a]\times [-b,b]\subseteq \mathbb{R}^{2}$ . How can I transform that region into a new region described by polar coordinates? If we start by making a graph, we can see that the graph will be a rectangular region that can be partitioned by the diagonals of the rectangle into 4 isosceles triangles. So the region in polar can be written as 4 regions in polar coordinates and one of them is of the form $$D_{1}=\{(r,\theta): \theta\in [-\arctan(b/a),+\arctan(b/a)]; r\in [0,a/\cos(\theta)] =D_{3}$$ $$D_{2}=\{(r,\theta): \theta\in [-\arctan(a/b),+\arctan(a/b)]; r\in [0,b/\cos(\theta)]=D_{4}$$ Is it correct? Would the other regions have a similar scheme or should I change the approach?","Suppose that we have . How can I transform that region into a new region described by polar coordinates? If we start by making a graph, we can see that the graph will be a rectangular region that can be partitioned by the diagonals of the rectangle into 4 isosceles triangles. So the region in polar can be written as 4 regions in polar coordinates and one of them is of the form Is it correct? Would the other regions have a similar scheme or should I change the approach?","D=[-a,a]\times [-b,b]\subseteq \mathbb{R}^{2} D_{1}=\{(r,\theta): \theta\in [-\arctan(b/a),+\arctan(b/a)]; r\in [0,a/\cos(\theta)] =D_{3} D_{2}=\{(r,\theta): \theta\in [-\arctan(a/b),+\arctan(a/b)]; r\in [0,b/\cos(\theta)]=D_{4}",[]
73,Proof of De la Vallée-Poussin's Test,Proof of De la Vallée-Poussin's Test,,"Here's what I'm trying to prove. Suppose that $A \subseteq \mathbb{R}^n$ and $J = [c,+\infty)$ . Let $f: A \times J \to \mathbb{R}^p$ be continuous and suppose that there exists a function $g: J \to [0,+\infty)$ such that: $$\forall (x,t) \in A \times J: \|f(x,t)\| \leq g(t)$$ Suppose that $\int_{c}^{\infty} g(t) \ dt$ is convergent. Then, $\int_{c}^{\infty} f(x,t) \, dt$ is uniformly convergent for $x \in A$ . Here's the definition of uniform convergence (I'm pretty sure that this is really just an extension of the definition of improper integrals on the real line). Again, let $f$ be as given above. Then, define $F(x) = \int_{c}^{\infty} f(x,t) \, dt$ . We say that $f$ is uniformly convergent for $x \in A$ iff: $$\forall \epsilon > 0: \exists D > c: x \in A \land d \geq D \implies \left\| F(x)-\int_{c}^{d} f(x,t) \right\| < \epsilon$$ Proof Attempt: Observe that for any fixed $x \in A$ , we have: $$\forall i \in \{1,2,\ldots,p\}: |f_i(x,t)| \leq \left\|f(x,t)\right\| \leq g(t)$$ $$\implies \forall d > c: \int_{c}^{d} |f_i(x,t)| \ dt \leq \int_{c}^{d} g(t) \ dt$$ Then, since $\int_{c}^{d} g(t) \ dt$ converges as $d \to \infty$ , it follows that $\int_{c}^{d} |f_i(x,t)| \ dt$ converges as $d \to \infty$ by comparison. Then, this implies that $\int_{c}^{\infty} f_i(x,t) \ dt$ converges and since the integral of each component function of $f$ converges, it follows that $\int_{c}^{\infty} f(x,t) \ dt$ converges. $\Box$ The thing that I'm somewhat worried about is if I'm missing something crucial over here. Like, I'm just wondering if I've overlooked something or if I would actually have to do a more formal argument using the definition directly. I'd appreciate it if someone could have a look at my working.","Here's what I'm trying to prove. Suppose that and . Let be continuous and suppose that there exists a function such that: Suppose that is convergent. Then, is uniformly convergent for . Here's the definition of uniform convergence (I'm pretty sure that this is really just an extension of the definition of improper integrals on the real line). Again, let be as given above. Then, define . We say that is uniformly convergent for iff: Proof Attempt: Observe that for any fixed , we have: Then, since converges as , it follows that converges as by comparison. Then, this implies that converges and since the integral of each component function of converges, it follows that converges. The thing that I'm somewhat worried about is if I'm missing something crucial over here. Like, I'm just wondering if I've overlooked something or if I would actually have to do a more formal argument using the definition directly. I'd appreciate it if someone could have a look at my working.","A \subseteq \mathbb{R}^n J = [c,+\infty) f: A \times J \to \mathbb{R}^p g: J \to [0,+\infty) \forall (x,t) \in A \times J: \|f(x,t)\| \leq g(t) \int_{c}^{\infty} g(t) \ dt \int_{c}^{\infty} f(x,t) \, dt x \in A f F(x) = \int_{c}^{\infty} f(x,t) \, dt f x \in A \forall \epsilon > 0: \exists D > c: x \in A \land d \geq D \implies \left\| F(x)-\int_{c}^{d} f(x,t) \right\| < \epsilon x \in A \forall i \in \{1,2,\ldots,p\}: |f_i(x,t)| \leq \left\|f(x,t)\right\| \leq g(t) \implies \forall d > c: \int_{c}^{d} |f_i(x,t)| \ dt \leq \int_{c}^{d} g(t) \ dt \int_{c}^{d} g(t) \ dt d \to \infty \int_{c}^{d} |f_i(x,t)| \ dt d \to \infty \int_{c}^{\infty} f_i(x,t) \ dt f \int_{c}^{\infty} f(x,t) \ dt \Box","['real-analysis', 'multivariable-calculus', 'convergence-divergence', 'solution-verification', 'uniform-convergence']"
74,"$u(x,t)\cdot(\nabla\rho(x,t))$ versus $(u(x,t)\cdot\nabla)\rho(x,t)).$",versus,"u(x,t)\cdot(\nabla\rho(x,t)) (u(x,t)\cdot\nabla)\rho(x,t)).","I am new in the Navier Stokes Equation In the Eulerian description of the fluid, if I have $g(t)=\rho(x(t),t)$ , then: $$g'(t)=\frac{\partial \rho}{\partial t}(x(t),t)+\frac{\partial \rho}{\partial x}(x(t),t)x'(t)=\frac{\partial \rho}{\partial t}(x(t),t)+ u(x,t)\cdot(\nabla\rho(x,t)).$$ $u(x,t)=x'(t)$ is the velocity field of the flow of the fluid, $\rho:\Omega×[0,T] \rightarrow \mathbb{R}$ is the mass density, and $\nabla$ is the nabla operator. MY QUESTION IS how did $u(x,t)\cdot (\nabla\rho(x,t))$ become $(u(x,t)\cdot\nabla)\rho(x,t))?$ I searched on internet and they said that: Generally the convective derivative of the field u·∇y, the one that contains the covariant derivative of the field, can be interpreted both as involving the streamline tensor derivative of the field u·(∇y), or as involving the streamline directional derivative of the field (u·∇) y, leading to the same result. BUT I don't know why and how did this lead to the same result!!","I am new in the Navier Stokes Equation In the Eulerian description of the fluid, if I have , then: is the velocity field of the flow of the fluid, is the mass density, and is the nabla operator. MY QUESTION IS how did become I searched on internet and they said that: Generally the convective derivative of the field u·∇y, the one that contains the covariant derivative of the field, can be interpreted both as involving the streamline tensor derivative of the field u·(∇y), or as involving the streamline directional derivative of the field (u·∇) y, leading to the same result. BUT I don't know why and how did this lead to the same result!!","g(t)=\rho(x(t),t) g'(t)=\frac{\partial \rho}{\partial t}(x(t),t)+\frac{\partial \rho}{\partial x}(x(t),t)x'(t)=\frac{\partial \rho}{\partial t}(x(t),t)+ u(x,t)\cdot(\nabla\rho(x,t)). u(x,t)=x'(t) \rho:\Omega×[0,T] \rightarrow \mathbb{R} \nabla u(x,t)\cdot (\nabla\rho(x,t)) (u(x,t)\cdot\nabla)\rho(x,t))?","['multivariable-calculus', 'derivatives', 'partial-differential-equations', 'chain-rule', 'fluid-dynamics']"
75,Proof of optimal transport map for 1-d Wasserstein,Proof of optimal transport map for 1-d Wasserstein,,"I'm following some code for implementing Wasserstein distance. They provide a link to this paper https://arxiv.org/pdf/1509.02237.pdf On page 10 they state proposition 1, namely The $p$ -Wasserstein distance between two probability measures $P$ and $Q$ on $\mathbb{R}$ with $p$ -finite moments can be written as $$W_{p}^p(P,Q) = \int^{1}_{0} |F^{-1}(t) - G^{-1}(t)|^p dt $$ where $F^{-1},G^{-1}$ are the quantile functions of $P$ and $Q$ respectively. Now the proof is provided on page 17. I sort of follow most of it but how do they arrive the last result $$\int_{supp \pi^* } |x-y|^p d \pi^*(x,y) = \int^{1}_{0} |F^{-1}(t) - G^{-1}(t)|^p dt$$ My guess is they made the substitution $F(x)=G(y) = t$ so then the substitution works for $|x-y| = |F^{-1}(t) - G^{-1}(t)|$ but i can't get the rest to work out. Can somebody add the steps in logic in?","I'm following some code for implementing Wasserstein distance. They provide a link to this paper https://arxiv.org/pdf/1509.02237.pdf On page 10 they state proposition 1, namely The -Wasserstein distance between two probability measures and on with -finite moments can be written as where are the quantile functions of and respectively. Now the proof is provided on page 17. I sort of follow most of it but how do they arrive the last result My guess is they made the substitution so then the substitution works for but i can't get the rest to work out. Can somebody add the steps in logic in?","p P Q \mathbb{R} p W_{p}^p(P,Q) = \int^{1}_{0} |F^{-1}(t) - G^{-1}(t)|^p dt  F^{-1},G^{-1} P Q \int_{supp \pi^* } |x-y|^p d \pi^*(x,y) = \int^{1}_{0} |F^{-1}(t) - G^{-1}(t)|^p dt F(x)=G(y) = t |x-y| = |F^{-1}(t) - G^{-1}(t)|","['multivariable-calculus', 'metric-spaces', 'convex-optimization', 'optimal-transport']"
76,Integral for Biot-Savart Law on a box,Integral for Biot-Savart Law on a box,,"I'm trying to make a basic computer model of a bar magnet. In the process I came across this question and answer that appears to have an appropriate equation for me to use. I say appears because my maths knowledge is only barely past high-school level, and the answerer stops here: you can simply use the Biot-Savart law to calculate the magnetic field: $$\mathbf B(\mathbf x) = \frac{\mu_0}{4\pi}\int_{\mathbb S}d\mathbf a' \ \mathbf K(\mathbf x') \times \frac{\mathbf{x-x'}}{|\mathbf{x-x'}|^3}$$ I believe you can take it from here. So I understand that in this case I can use the fact that the net magnetic field (the thing I want to model) is composed of the sum of all the magnetic fields produced in this situation. Which is to say I need to add up the field produced by each of the faces with respect to each point I want to model. I also figure that that is what the part of the equation that I don't understand is trying to express, namely the section $\int_{\mathbb S}d\mathbf a' \ \mathbf K(\mathbf x')$ So in the above example a bar magnet is being modeled like so: You can model a bar magnet by a rectangular box with a constant magnetization in one direction. Let's take the box $[0,a]\times[0,b]\times[0,c]$ , with a constant magnetization $\mathbf M(\mathbf x) = M_0 \ \hat{\mathbf k}$ , where $\hat{\mathbf k}$ is the unit vector in the $z$ direction. The bound volume and surface current densities are: $$\mathbf J_b(\mathbf x) = \boldsymbol{\nabla}\times\mathbf M(\mathbf x)$$ $$\mathbf K_b(\mathbf x) = \mathbf M(\mathbf x) \times \hat {\mathbf n}$$ The volume current density is zero because $\mathbf M$ is constant. For the surface current density, the top and bottom faces don't contribute since $M_0 \hat{\mathbf k}\times\hat {\mathbf k}=0$ . For the other four faces we have: $$\mathrm{x=0 \ face:} \ \mathbf K_1 = M_0 \ \hat{\mathbf k}\times (-\hat{\mathbf i}) = -M_0 \ \hat{\mathbf j}$$ $$\mathrm{x=a \ face:} \ \mathbf K_2 = M_0 \ \hat{\mathbf k}\times \hat{\mathbf i} = M_0 \ \hat{\mathbf j}$$ $$\mathrm{y=0 \ face:} \ \mathbf K_3 = M_0 \ \hat{\mathbf k}\times (-\hat{\mathbf j}) = M_0 \ \hat{\mathbf i}$$ $$\mathrm{y=b \ face:} \ \mathbf K_4 = M_0 \ \hat{\mathbf k}\times \hat{\mathbf j} = -M_0 \ \hat{\mathbf i}$$ Now that you know the bound current distribution, you can simply use the Biot-Savart law to calculate the magnetic field: $$\mathbf B(\mathbf x) = \frac{\mu_0}{4\pi}\int_{\mathbb S}d\mathbf a' \ \mathbf K(\mathbf x') \times \frac{\mathbf{x-x'}}{|\mathbf{x-x'}|^3}$$ I believe you can take it from here. My question is how do I evaluate the integral portion of this equation? I'm looking to turn this into a piece of computer code, and my background is pretty shallow when it comes to this level of maths. Edit: I understand Matlab has an integrate function, but I would prefer not to buy a license for that if possible. Edit2: After thinking about this some more, and with the help of Ian's comments I have determined what I think I need to do, which is best expressed graphically by the diagram I have just drawn:","I'm trying to make a basic computer model of a bar magnet. In the process I came across this question and answer that appears to have an appropriate equation for me to use. I say appears because my maths knowledge is only barely past high-school level, and the answerer stops here: you can simply use the Biot-Savart law to calculate the magnetic field: I believe you can take it from here. So I understand that in this case I can use the fact that the net magnetic field (the thing I want to model) is composed of the sum of all the magnetic fields produced in this situation. Which is to say I need to add up the field produced by each of the faces with respect to each point I want to model. I also figure that that is what the part of the equation that I don't understand is trying to express, namely the section So in the above example a bar magnet is being modeled like so: You can model a bar magnet by a rectangular box with a constant magnetization in one direction. Let's take the box , with a constant magnetization , where is the unit vector in the direction. The bound volume and surface current densities are: The volume current density is zero because is constant. For the surface current density, the top and bottom faces don't contribute since . For the other four faces we have: Now that you know the bound current distribution, you can simply use the Biot-Savart law to calculate the magnetic field: I believe you can take it from here. My question is how do I evaluate the integral portion of this equation? I'm looking to turn this into a piece of computer code, and my background is pretty shallow when it comes to this level of maths. Edit: I understand Matlab has an integrate function, but I would prefer not to buy a license for that if possible. Edit2: After thinking about this some more, and with the help of Ian's comments I have determined what I think I need to do, which is best expressed graphically by the diagram I have just drawn:","\mathbf B(\mathbf x) = \frac{\mu_0}{4\pi}\int_{\mathbb S}d\mathbf a' \ \mathbf K(\mathbf x') \times \frac{\mathbf{x-x'}}{|\mathbf{x-x'}|^3} \int_{\mathbb S}d\mathbf a' \ \mathbf K(\mathbf x') [0,a]\times[0,b]\times[0,c] \mathbf M(\mathbf x) = M_0 \ \hat{\mathbf k} \hat{\mathbf k} z \mathbf J_b(\mathbf x) = \boldsymbol{\nabla}\times\mathbf M(\mathbf x) \mathbf K_b(\mathbf x) = \mathbf M(\mathbf x) \times \hat {\mathbf n} \mathbf M M_0 \hat{\mathbf k}\times\hat {\mathbf k}=0 \mathrm{x=0 \ face:} \ \mathbf K_1 = M_0 \ \hat{\mathbf k}\times (-\hat{\mathbf i}) = -M_0 \ \hat{\mathbf j} \mathrm{x=a \ face:} \ \mathbf K_2 = M_0 \ \hat{\mathbf k}\times \hat{\mathbf i} = M_0 \ \hat{\mathbf j} \mathrm{y=0 \ face:} \ \mathbf K_3 = M_0 \ \hat{\mathbf k}\times (-\hat{\mathbf j}) = M_0 \ \hat{\mathbf i} \mathrm{y=b \ face:} \ \mathbf K_4 = M_0 \ \hat{\mathbf k}\times \hat{\mathbf j} = -M_0 \ \hat{\mathbf i} \mathbf B(\mathbf x) = \frac{\mu_0}{4\pi}\int_{\mathbb S}d\mathbf a' \ \mathbf K(\mathbf x') \times \frac{\mathbf{x-x'}}{|\mathbf{x-x'}|^3}","['integration', 'multivariable-calculus', 'physics']"
77,Line Integral - Why are these two integrals the same?,Line Integral - Why are these two integrals the same?,,"I am starting to study calculus III and I came across the following situation Given the following form $$ydx - xdy$$ Why the integral along the semicircle $$P(t) = cos(t)\vec{i} + sin(t)\vec{j},\:0 \leq t \leq \pi$$ It is the same when using the following parameterization $$y = \sqrt{1-x^2}, -1 \leq x \leq 1$$ Since the parameterizations are reversed? When computing the first integral, I got $ - \pi $ . $$\int_c ydx - xdy$$ $$x = cos(t) \Rightarrow \frac{dx}{dt} = -sin(t)$$ $$y = sin(t) \Rightarrow \frac{dt}{dt} = cos(t)$$ $$\int_0^\pi ( sin(t)(-sin(t)) - cos(t)cos(t)) dt = -\pi$$ How do I compute the second one that has an inverted sense of integration than the first one? Thanks in advance!","I am starting to study calculus III and I came across the following situation Given the following form Why the integral along the semicircle It is the same when using the following parameterization Since the parameterizations are reversed? When computing the first integral, I got . How do I compute the second one that has an inverted sense of integration than the first one? Thanks in advance!","ydx - xdy P(t) = cos(t)\vec{i} + sin(t)\vec{j},\:0 \leq t \leq \pi y = \sqrt{1-x^2}, -1 \leq x \leq 1  - \pi  \int_c ydx - xdy x = cos(t) \Rightarrow \frac{dx}{dt} = -sin(t) y = sin(t) \Rightarrow \frac{dt}{dt} = cos(t) \int_0^\pi ( sin(t)(-sin(t)) - cos(t)cos(t)) dt = -\pi","['multivariable-calculus', 'vector-analysis', 'differential-forms', 'line-integrals']"
78,How can we calculate $\int_{r\in \mathbb{S}^2}\ {\rm Area}\ \Delta p_0q_0r\ d{\rm Area}_r$,How can we calculate,\int_{r\in \mathbb{S}^2}\ {\rm Area}\ \Delta p_0q_0r\ d{\rm Area}_r,"Define a function $f : \mathbb{S}^2\times \mathbb{S}^2\times \mathbb{S}^2\rightarrow \mathbb{R}$ by $f(p,q,r)$ to be a area of the geodesic triangle $pqr$ in the unit sphere $\mathbb{S}^2$ . (Here the area of the triangle $pqr$ is $\angle p + \angle q +\angle r -\pi$ ). Question 1 : Here how can we find $$ \int_{\mathbb{S}^2\times \mathbb{S}^2\times \mathbb{S}^2 } \ f(p,q,r) d{\rm Vol}(p,q,r) $$ Question 2 : Furthermore, how can we find the following $$\int_{\mathbb{S}^2 } \ f(p_0,q_0,r) d{\rm Vol}(r) $$ Question 3 : Consider the triangle $\Delta pqr$ where the order of $p,\ q,\ r$ is positively oriented. And define $$A = \frac{ q\times p }{|  q\times p| },\ B = \frac{r\times q}{|r\times q|},\ C =  \frac{p\times r}{|p\times r|} $$ Note that $\angle \ (A,B) =\pi-\angle p $ so that $$ \angle (A,B) + \angle (B,C)+\angle (A,C) = 2\pi - {\rm Area}\ \Delta pqr $$ Hence ${\rm perim}\ \Delta ABC$ , the sum of all side lengths in $\Delta ABC$ , is equal to $2\pi - {\rm Area}\ \Delta pqr$ . Hence we want to calculate $$ \int_{(A,B,C)\in \mathbb{S}^2  \times\mathbb{S}^2\times \mathbb{S}^2}\ {\rm perim}\ \Delta ABC \ d{\rm Vol}_{ (A,B,C) }$$","Define a function by to be a area of the geodesic triangle in the unit sphere . (Here the area of the triangle is ). Question 1 : Here how can we find Question 2 : Furthermore, how can we find the following Question 3 : Consider the triangle where the order of is positively oriented. And define Note that so that Hence , the sum of all side lengths in , is equal to . Hence we want to calculate","f : \mathbb{S}^2\times \mathbb{S}^2\times \mathbb{S}^2\rightarrow \mathbb{R} f(p,q,r) pqr \mathbb{S}^2 pqr \angle p + \angle q +\angle r -\pi  \int_{\mathbb{S}^2\times \mathbb{S}^2\times \mathbb{S}^2 } \ f(p,q,r) d{\rm Vol}(p,q,r)  \int_{\mathbb{S}^2 } \ f(p_0,q_0,r) d{\rm Vol}(r)  \Delta pqr p,\ q,\ r A = \frac{ q\times p }{|
 q\times p| },\ B = \frac{r\times q}{|r\times q|},\ C =
 \frac{p\times r}{|p\times r|}  \angle \ (A,B) =\pi-\angle p   \angle (A,B) +
\angle (B,C)+\angle (A,C) = 2\pi - {\rm Area}\ \Delta pqr  {\rm perim}\ \Delta ABC \Delta ABC 2\pi - {\rm Area}\ \Delta pqr  \int_{(A,B,C)\in \mathbb{S}^2
 \times\mathbb{S}^2\times
\mathbb{S}^2}\ {\rm perim}\ \Delta ABC \ d{\rm Vol}_{ (A,B,C) }","['multivariable-calculus', 'euclidean-geometry', 'spherical-geometry']"
79,How to get the coefficients of Taylor series in several variables when sums are grouped by order of partial derivatives,How to get the coefficients of Taylor series in several variables when sums are grouped by order of partial derivatives,,"Assume $f$ is real analytic in several variables. The Taylor series of $f$ can be represented in two forms (cf. e.g., wikipedia ). The first form is power series form: $$T(x_1,\dots,x_d)=\sum\limits_{(n_1,\dots,n_d)\in\Bbb N^d}\frac{1}{n_1!\cdots n_d!}\frac{\partial^{n_1+\cdots+n_d}f(a_1,\dots,a_d)}{\partial x_1^{n_1}\cdots\partial x_d^{n_d}}(x_1-a_1)^{n_1}\cdots(x_d-a_d)^{n_d}.\tag{1}$$ The second form is grouping summands by the order (number of variables) of partial derivatives: $$T(x_1,\dots,x_d)=f(a_1,\dots,a_d)+\sum\limits_{j=1}^d\frac{\partial f(a_1,\dots,a_d)}{\partial x_j}(x_j-a_j)\\+\frac{1}{2!}\sum\limits_{j=1}^d\sum\limits_{k=1}^d\frac{\partial^2f(a_1,\dots,a_d)}{\partial x_j\partial x_k}(x_j-a_j)(x_k-a_k)+\cdots\\+\frac{1}{n!}\sum\limits_{i_1=1}^d\cdots\sum\limits_{i_n=1}^d\frac{\partial^n f(a_1,\dots,a_d)}{\partial x_{i_1}\cdots\partial x_{i_n}}(x_{i_1}-a_{i_1})\cdots(x_{i_n}-a_{i_n})+\cdots.\tag{2}$$ I can derive the coefficients in power series form $(1)$ by differentiating term-by-term. But I encountered difficulty in deriving coefficients in the second form. Following is my attempt. The general $n$ -th term in the second form is like this (please correct me if I am wrong): $$\sum\limits_{i_1=1}^d\cdots\sum\limits_{i_n=1}^d c_{(i_1,\cdots,i_n)}(x_{i_1}-a_{i_1})\cdots(x_{i_n}-a_{i_n}).\tag{3}$$ $i_1,\dots,i_n$ can take duplicated values in $1..d$ . Assuming in $n$ indices $i_1,\dots,i_n$ , there are $n_1\ 1$ 's, $n_2\ 2$ 's, $\cdots$ , $n_d\ d$ 's, we have actually constructed a histogram mapping $\varphi: (i_1,\cdots,i_n)\to (n_1,\dots,n_d)$ with the restriction that $n_1+\cdots+n_d=n$ . Based on different histogram $d$ -tuple $(n_1,\dots,n_d)$ , we can further subgroup the summands in sum $(3)$ as follows. For a specific histogram distribution $(n_1,\dots,n_d)$ , by merging same factors, the polynomial part of $(3)$ becomes $(x_1-a_1)^{n_1}\cdots(x_d-a_d)^{n_d}$ . So, $(3)$ can be rewritten in accordance with this subgrouping as $$\sum\limits_{\begin{array}{c}(n_1,\dots,n_d)\in\Bbb N^d\\n_1+\cdots+n_d=n\end{array}}c_{(n_1,\dots,n_d)}(x_1-a_1)^{n_1}\cdots(x_d-a_d)^{n_d}.$$ This looks much like the power series form. If we take partial derivative $\frac{\partial^{n}f}{\partial x_1^{n_1}\cdots\partial x_d^{n_d}}$ and plugging in $(a_1,\dots,a_d)$ , all other terms not corresponding to this $(n_1,\dots,n_d)$ tuple will vanish either because the exponent is less or because the factor is evaluated to zero, and not surprisingly we get the coefficient in exactly the same form as ones in the power series form: $c_{(n_1,\dots,n_d)}=\frac{1}{n_1!\cdots n_d!}\frac{\partial^n f(a_1,\dots,a_d)}{\partial x_1^{n_1}\cdots\partial x_d^{n_d}}=\frac{1}{n_1!\cdots n_d!}\frac{\partial^n f(a_1,\dots,a_d)}{\partial x_{i_1}\cdots\partial x_{i_n}}$ since $f$ is $C^\infty$ and therefore changing the order in which variables are taken does not change the result of mixed partial derivative. But the denominator before the mixed partial derivative is supposed to be $n!$ from the ground truth in $(2)$ as opposed to $n_1!\cdots n_d!$ . I know I must have committed some error by messing up something during the derivation, but I cannot spot it. So, could you please help me figure out the error so that I can correct it and eventually derive the coefficients in the second form having expected $n!$ in the denominator. Thank you.","Assume is real analytic in several variables. The Taylor series of can be represented in two forms (cf. e.g., wikipedia ). The first form is power series form: The second form is grouping summands by the order (number of variables) of partial derivatives: I can derive the coefficients in power series form by differentiating term-by-term. But I encountered difficulty in deriving coefficients in the second form. Following is my attempt. The general -th term in the second form is like this (please correct me if I am wrong): can take duplicated values in . Assuming in indices , there are 's, 's, , 's, we have actually constructed a histogram mapping with the restriction that . Based on different histogram -tuple , we can further subgroup the summands in sum as follows. For a specific histogram distribution , by merging same factors, the polynomial part of becomes . So, can be rewritten in accordance with this subgrouping as This looks much like the power series form. If we take partial derivative and plugging in , all other terms not corresponding to this tuple will vanish either because the exponent is less or because the factor is evaluated to zero, and not surprisingly we get the coefficient in exactly the same form as ones in the power series form: since is and therefore changing the order in which variables are taken does not change the result of mixed partial derivative. But the denominator before the mixed partial derivative is supposed to be from the ground truth in as opposed to . I know I must have committed some error by messing up something during the derivation, but I cannot spot it. So, could you please help me figure out the error so that I can correct it and eventually derive the coefficients in the second form having expected in the denominator. Thank you.","f f T(x_1,\dots,x_d)=\sum\limits_{(n_1,\dots,n_d)\in\Bbb N^d}\frac{1}{n_1!\cdots n_d!}\frac{\partial^{n_1+\cdots+n_d}f(a_1,\dots,a_d)}{\partial x_1^{n_1}\cdots\partial x_d^{n_d}}(x_1-a_1)^{n_1}\cdots(x_d-a_d)^{n_d}.\tag{1} T(x_1,\dots,x_d)=f(a_1,\dots,a_d)+\sum\limits_{j=1}^d\frac{\partial f(a_1,\dots,a_d)}{\partial x_j}(x_j-a_j)\\+\frac{1}{2!}\sum\limits_{j=1}^d\sum\limits_{k=1}^d\frac{\partial^2f(a_1,\dots,a_d)}{\partial x_j\partial x_k}(x_j-a_j)(x_k-a_k)+\cdots\\+\frac{1}{n!}\sum\limits_{i_1=1}^d\cdots\sum\limits_{i_n=1}^d\frac{\partial^n f(a_1,\dots,a_d)}{\partial x_{i_1}\cdots\partial x_{i_n}}(x_{i_1}-a_{i_1})\cdots(x_{i_n}-a_{i_n})+\cdots.\tag{2} (1) n \sum\limits_{i_1=1}^d\cdots\sum\limits_{i_n=1}^d c_{(i_1,\cdots,i_n)}(x_{i_1}-a_{i_1})\cdots(x_{i_n}-a_{i_n}).\tag{3} i_1,\dots,i_n 1..d n i_1,\dots,i_n n_1\ 1 n_2\ 2 \cdots n_d\ d \varphi: (i_1,\cdots,i_n)\to (n_1,\dots,n_d) n_1+\cdots+n_d=n d (n_1,\dots,n_d) (3) (n_1,\dots,n_d) (3) (x_1-a_1)^{n_1}\cdots(x_d-a_d)^{n_d} (3) \sum\limits_{\begin{array}{c}(n_1,\dots,n_d)\in\Bbb N^d\\n_1+\cdots+n_d=n\end{array}}c_{(n_1,\dots,n_d)}(x_1-a_1)^{n_1}\cdots(x_d-a_d)^{n_d}. \frac{\partial^{n}f}{\partial x_1^{n_1}\cdots\partial x_d^{n_d}} (a_1,\dots,a_d) (n_1,\dots,n_d) c_{(n_1,\dots,n_d)}=\frac{1}{n_1!\cdots n_d!}\frac{\partial^n f(a_1,\dots,a_d)}{\partial x_1^{n_1}\cdots\partial x_d^{n_d}}=\frac{1}{n_1!\cdots n_d!}\frac{\partial^n f(a_1,\dots,a_d)}{\partial x_{i_1}\cdots\partial x_{i_n}} f C^\infty n! (2) n_1!\cdots n_d! n!","['combinatorics', 'multivariable-calculus', 'taylor-expansion', 'partial-derivative']"
80,A differentiable function with $df = 0$ is locally constant,A differentiable function with  is locally constant,df = 0,"Let $U$ be an open set and $f : U \subset\mathbb{R}^d \to \mathbb{R}^m$ be a differentiable function with $df=0, \forall x \in U.$ Show that $f$ is locally constant. So my idea was, given some arbitrary $x_0 \in U$ , first taking some ball $B_{\varepsilon}(x_0) \subset U$ which exists since $U$ is open, we can assume $f$ isn't constant there i.e there is $y_o$ s.t $f(x_0) \neq f(y_0)$ . Then defining $\gamma(t) = (1-t)x_0 +t y_0$ , we have $f(\gamma(0)) \neq f(\gamma(1))$ meaning they are different in at least one coordiante, so without loss of generality $f^1(\gamma(0)) \neq f^1(\gamma(1))$ . and then using lagrange for an $\mathbb{R} \to \mathbb{R}$ function we have $(f^1(\gamma(c)))' \neq 0$ . But on the other hand using chain rule $(f^1(\gamma(c)))' = d(f^1(\gamma(c))) = \underbrace{df^1(\gamma(c))}_\text{0 since $df=0$}    \circ d \gamma(c) = 0$ I don't really like my solution since it relies on coordinates, if someone would like to share a different solution it would be great.","Let be an open set and be a differentiable function with Show that is locally constant. So my idea was, given some arbitrary , first taking some ball which exists since is open, we can assume isn't constant there i.e there is s.t . Then defining , we have meaning they are different in at least one coordiante, so without loss of generality . and then using lagrange for an function we have . But on the other hand using chain rule I don't really like my solution since it relies on coordinates, if someone would like to share a different solution it would be great.","U f : U \subset\mathbb{R}^d \to \mathbb{R}^m df=0, \forall x \in U. f x_0 \in U B_{\varepsilon}(x_0) \subset U U f y_o f(x_0) \neq f(y_0) \gamma(t) = (1-t)x_0 +t y_0 f(\gamma(0)) \neq f(\gamma(1)) f^1(\gamma(0)) \neq f^1(\gamma(1)) \mathbb{R} \to \mathbb{R} (f^1(\gamma(c)))' \neq 0 (f^1(\gamma(c)))' = d(f^1(\gamma(c))) =
\underbrace{df^1(\gamma(c))}_\text{0 since df=0} 
  \circ d \gamma(c) = 0",['multivariable-calculus']
81,"Double integral over $f(x,y) = y^2x+x^3$",Double integral over,"f(x,y) = y^2x+x^3","Compute the integral $$\int_D f \ dx \ dy$$ where $f : D\to \mathbb{R}, f(x,y) = y^2x+x^3$ and $D= \{(x,y) \mid \sqrt{x^2+y^2} \leqslant 5, y >0\}$ Converting to polar coordinates I got $f(r,\theta) = r^3\cos(\theta)$ and $D = \{(r,\theta) \mid 0 \leqslant r \leqslant 5, 0\leqslant \theta \leqslant 2\pi \} $ So the integral would become $$\int_{0}^{2\pi}\int_{0}^{5} r^4\cos(\theta) \ dr \ d\theta = \int _0^{2\pi }\cos \left(θ\right)\cdot \:625 \ d\theta = 0.$$ I guess I have a mistake here since isn't this just a cone whose projection on to the $xy$ plane is a circle of radius $5$ and the area shouldn't evaluate to zero?",Compute the integral where and Converting to polar coordinates I got and So the integral would become I guess I have a mistake here since isn't this just a cone whose projection on to the plane is a circle of radius and the area shouldn't evaluate to zero?,"\int_D f \ dx \ dy f : D\to \mathbb{R}, f(x,y) = y^2x+x^3 D= \{(x,y) \mid \sqrt{x^2+y^2} \leqslant 5, y >0\} f(r,\theta) = r^3\cos(\theta) D = \{(r,\theta) \mid 0 \leqslant r \leqslant 5, 0\leqslant \theta \leqslant 2\pi \}  \int_{0}^{2\pi}\int_{0}^{5} r^4\cos(\theta) \ dr \ d\theta = \int _0^{2\pi }\cos \left(θ\right)\cdot \:625 \ d\theta = 0. xy 5",['integration']
82,An estimate involving Polar Coordinates,An estimate involving Polar Coordinates,,"I encountered the following computation in a paper: The highlighted $t$ is mysterious to me, how did it end up there? For some guidance: the first line is fundamental theorem of calculus and change of variables. The last equality is change of variables and fubini. Thanks!","I encountered the following computation in a paper: The highlighted is mysterious to me, how did it end up there? For some guidance: the first line is fundamental theorem of calculus and change of variables. The last equality is change of variables and fubini. Thanks!",t,"['multivariable-calculus', 'differential-geometry', 'partial-differential-equations', 'polar-coordinates']"
83,"""Differential of"" as an operator","""Differential of"" as an operator",,"I am reading the paper Learning Dynamical Systems from Partial Observations and I am having a hard time following the calculations in Appendix B. Let $F: \mathbb{R}^n \rightarrow \mathbb{R}^m$ be a multivariate function. The paper defines the differential $\partial_u F(u_0)$ of $F$ with respect to $u$ as the operator such that $$F(u_0 + \delta u) = F(u_0) + \partial_u F(u_0) \delta u + o(\delta u).$$ Now, let $F_{\theta}$ to mean that $F$ is a function of $\theta$ . Likewise, let $X^{\theta}$ mean that $X$ is a function of $\theta$ . The paper states that $$\partial_{\theta} F_{\theta} (X_t^{\theta} + \partial_{\theta} X_t^{\theta} \cdot \delta \theta + o(\delta \theta)) = \partial_{\theta} F_{\theta}(X_t^{\theta}) + \partial_{X} \partial_{\theta} F_{\theta}(X_t^{\theta}) \cdot \partial_{\theta} X_t^{\theta} \cdot \delta \theta + o(\delta \theta).$$ Here are my questions: Is the definition of the differential accurate? Shouldn't it be $$F(u_0 + \delta u) = F(u_0) + \partial_u F(u_0) \delta u + o(\|\delta u\|^2)$$ instead? Can anyone guide me on the calculation of $$\partial_{\theta} F_{\theta} (X_t^{\theta} + \partial_{\theta} X_t^{\theta} \cdot \delta \theta + o(\delta \theta))?$$ I can't figure out the calculation. Just looking at the resulting expression above, I wonder why the argument $(X_t^{\theta} + \partial_{\theta} X_t^{\theta} \cdot \delta \theta + o(\delta \theta)$ is not present at all in the $F_{\theta}$ 's that appear on the right hand side if the chain rule was indeed used. Suggestions appreciated. Thanks!","I am reading the paper Learning Dynamical Systems from Partial Observations and I am having a hard time following the calculations in Appendix B. Let be a multivariate function. The paper defines the differential of with respect to as the operator such that Now, let to mean that is a function of . Likewise, let mean that is a function of . The paper states that Here are my questions: Is the definition of the differential accurate? Shouldn't it be instead? Can anyone guide me on the calculation of I can't figure out the calculation. Just looking at the resulting expression above, I wonder why the argument is not present at all in the 's that appear on the right hand side if the chain rule was indeed used. Suggestions appreciated. Thanks!",F: \mathbb{R}^n \rightarrow \mathbb{R}^m \partial_u F(u_0) F u F(u_0 + \delta u) = F(u_0) + \partial_u F(u_0) \delta u + o(\delta u). F_{\theta} F \theta X^{\theta} X \theta \partial_{\theta} F_{\theta} (X_t^{\theta} + \partial_{\theta} X_t^{\theta} \cdot \delta \theta + o(\delta \theta)) = \partial_{\theta} F_{\theta}(X_t^{\theta}) + \partial_{X} \partial_{\theta} F_{\theta}(X_t^{\theta}) \cdot \partial_{\theta} X_t^{\theta} \cdot \delta \theta + o(\delta \theta). F(u_0 + \delta u) = F(u_0) + \partial_u F(u_0) \delta u + o(\|\delta u\|^2) \partial_{\theta} F_{\theta} (X_t^{\theta} + \partial_{\theta} X_t^{\theta} \cdot \delta \theta + o(\delta \theta))? (X_t^{\theta} + \partial_{\theta} X_t^{\theta} \cdot \delta \theta + o(\delta \theta) F_{\theta},"['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative', 'chain-rule']"
84,"Sketch the region of integration for the integral $\int_{-2}^2 \int_0^{2y} f(x,y)dx\,dy$",Sketch the region of integration for the integral,"\int_{-2}^2 \int_0^{2y} f(x,y)dx\,dy","I am slightly confused when sketching the region for the double integral $$\int_{-2}^2 \int_0^{2y} f(x,y)dx\,dy$$ When I sketch the region I get a triangle in the top right quadrant, however my textbook says the region is two triangles, one in the top right quadrant, one in the bottom left quadrant. I don't understand how this can be if the inequalities for the regions are $0<x<2y$ and $-2<y<2$ .","I am slightly confused when sketching the region for the double integral When I sketch the region I get a triangle in the top right quadrant, however my textbook says the region is two triangles, one in the top right quadrant, one in the bottom left quadrant. I don't understand how this can be if the inequalities for the regions are and .","\int_{-2}^2 \int_0^{2y} f(x,y)dx\,dy 0<x<2y -2<y<2","['multivariable-calculus', 'multiple-integral']"
85,Locally Lipschitz with respect to a variable and uniformly respect the other,Locally Lipschitz with respect to a variable and uniformly respect the other,,"Let: $\mathbf{f}:D\subseteq \mathbb{R}^{n+1} \to  \mathbb{R}^{n} \ \  \ D \ \text{open}$ $\  \ \ \ \ \ (t,\mathbf{y}) \mapsto \mathbf{f}(t,\mathbf{y})$ Where $t\in \mathbb{R}$ and $\mathbf{y} \in \mathbb{R}^{n}$ . $\mathbf{f}$ is locally Lipschitz in $D$ with respect to $\mathbf{y}$ and uniformly respect $t$ if and only if: $$\forall (t,\mathbf{y})\in D   \ \exists B_r(t,\mathbf{y}) : \exists L\in\mathbb{R}_+ : \ \forall (t,\mathbf{z}) \in B_r(t,\mathbf{y}) \\ ||\mathbf{f}(t,\mathbf{y})-\mathbf{f}(t,\mathbf{y})||\leq L||\mathbf{y}-\mathbf{z}||$$ Where $L$ doesn't depend upon $t$ . The following proof is left as exercise to the reader Let $\mathbf{f}(t,\mathbf{y})=(f_1(t,\mathbf{y}),...,f_n(t,\mathbf{y}))$ and $\mathbf{y}=(y_1,...,y_n)$ then: $$\mathbf{f}\text{ and }\frac{\partial f_j}{\partial y_s } \ \text{continous in } D \ \  \forall j,s\in\{1,...,n\} \implies \mathbf{f} \text{  is locally Lipschitz in }D\text{ with respect to }\mathbf{y}\text{ and uniformly respect }t $$ I'm trying to prove it. Here is my attempt: Lemma Since mean value theorem is not true for vectorial functions, my idea was to use this weaker inequality: $\mathbf{g}:A\subseteq \mathbb{R}^n \to \mathbb{R}^n \ \ \ \mathbf{g}\in C^1(A)$ $\text{The segment } [\mathbf{y},\mathbf{z}]\subset A$ $\implies ||\mathbf{g}(\mathbf{y})-\mathbf{g}(\mathbf{z})||\leq \sqrt{n}\alpha||\mathbf{y}-\mathbf{z}||$ Where $$\alpha=\max \left\{\max_{[\mathbf{y},\mathbf{z}]} (||\nabla g_1(\mathbf{x})||),\max_{[\mathbf{y},\mathbf{z}]} (||\nabla g_2(\mathbf{x})||),...,\max_{[\mathbf{y},\mathbf{z}]} (||\nabla g_n(\mathbf{x})||)\right\}$$ The existences of the maximum of the $||\nabla g_j||$ is granted by Weierstrass Theorem, since $[\mathbf{y},\mathbf{z}]$ is compact. Attempt of proof I can define a family of functions $\mathbf{g}_t: \mathbf{y} \mapsto \mathbf{f}(t,\mathbf{y})$ (I'll indicate $g_{t,j}$ its $j$ -th component). One can easily verify the following: $$ \nabla f_j(t_0,\mathbf{y}_0)=(D_t f_j(t_0,\mathbf{y}_0),\nabla g_{t_0,j}(\mathbf{y}_0)) \ \ \forall(t_0,\mathbf{y}_0)\in D $$ In fact: $$D_{y_s} f_j(t_0,\mathbf{y}_0)=\lim_{h \to 0} \frac{f_j(t_0,\mathbf{y}_0+h\mathbf{e}_s)-f_j(t_0,\mathbf{y}_0)}{h}=\lim_{h \to 0} \frac{g_{t_0,j}(\mathbf{y}_0+h\mathbf{e}_s)-g_{t_0,j}(\mathbf{y}_0)}{h}=D_{y_s} g_{t_0,j}(\mathbf{y}_0)$$ This means that the following implication holds: $$D_{y_s} f_j(t,\mathbf{y})\leq N \ \ \ \forall(t,\mathbf{y})\in K\subset D \implies ||\nabla g_{t,j}(\mathbf{y})||\leq M  \ \ \ \forall(t,\mathbf{y})\in K $$ In particular the LHS proposition is true for every $j$ (and this obviously makes true also RHS for every $j$ ) by Weierstrass theorem if I choose $K$ to be a closed ball of generic center $(\overline{t},\mathbf{\overline{y}})$ (this ball  exists for every choice of $(\overline{t},\mathbf{\overline{y}})$ , since $D$ is open). After this premises, since $D_{y_s} \mathbf{g}_t:t \mapsto D_{y_s} \mathbf{f}(t,\mathbf{y})  $ and thanks to our hyphothesis it's easy to see that: $\mathbf{g}_t:A_t\subseteq \mathbb{R}^n \to \mathbb{R}^n \ \ \ A_t=\{\mathbf{y}:(t,\mathbf{y})\in D \} $ $\mathbf{g}_t \in C^1(A_t)$ Moreover since a ball is connected $\forall (t,\mathbf{z})\in K \ \ $ the segment $[ (\overline{t},\overline{\mathbf{y}}),(t,\mathbf{z})]\subseteq K $ and this implies that $[\mathbf{\overline{\mathbf{y}}},\mathbf{z}]\subseteq A_t$ (this can be easily proved showing that the pythagorean distance of $(\overline{t},\overline{\mathbf{y}})$ from $(t,q\overline{\mathbf{y}}+(1-q)\mathbf{z})$ with $q\in [0,1]$ is $\leq$ than the distance $d((\overline{t},\overline{\mathbf{y}}),(t,\mathbf{z}))$ ) , so by our lemma: $$\forall (t,\mathbf{z})\in K \ \ ||\mathbf{g}_t(\mathbf{\overline{y}})-\mathbf{g}_t(\mathbf{z})||\leq \sqrt{n}\alpha_t||\mathbf{\mathbf{\overline{y}}}-\mathbf{z}||$$ If we prove that $\alpha_t$ is bounded we are done because we substitute it with its upper bound, so that the inequality doesn't depend upon $t$ .Notice that: $$\alpha_t=\max \left\{\max_{[\mathbf{\overline{y}},\mathbf{z}]} (||\nabla g_{t,1}(\mathbf{y})||),\max_{[\mathbf{\overline{y}},\mathbf{z}]} (||\nabla g_{t,2}(\mathbf{y})||),...,\max_{[\mathbf{\overline{y}},\mathbf{z}]} (||\nabla g_{t,n}(\mathbf{y})||)\right\}$$ Since we know that for every $j \ \ $ , $||\nabla g_{t,j}(\mathbf{y})||\leq M \ \ \forall (t,\mathbf{y})\in  K$ ,we have just to prove that $\forall \mathbf{y} \in [\mathbf{\overline{y}},\mathbf{z}], (t,\mathbf{y})\in K$ , this can be done particularly easily showing that $ d((t,\mathbf{y}),(\overline{t},\mathbf{\overline{y}}))\leq d((t,\mathbf{z}),(\overline{t},\mathbf{\overline{y}}))$ (it's straight forward after you use the fact that $\mathbf{y}$ is convex combination of $\mathbf{\overline{y}}$ and $\mathbf{z}$ ). In conclusion: $$\alpha_t \leq M$$ And this implies that: $$\forall (t,\mathbf{z})\in K \ \ ||\mathbf{g}_t(\mathbf{\overline{y}})-\mathbf{g}_t(\mathbf{z})||\leq \sqrt{n}M||\mathbf{\mathbf{\overline{y}}}-\mathbf{z}||$$ By the definition of $\mathbf{g}_t$ and defining $L=M \sqrt{n}$ : $$\forall (t,\mathbf{z})\in K \ \ ||\mathbf{f}(t,\mathbf{\overline{y}})-\mathbf{f}(t,\mathbf{z})||\leq L||\mathbf{\mathbf{\overline{y}}}-\mathbf{z}||$$ Since $(\overline{t},\overline{\mathbf{y}})$ was a generic point of $D$ , this concludes the proof. Is it correct?","Let: Where and . is locally Lipschitz in with respect to and uniformly respect if and only if: Where doesn't depend upon . The following proof is left as exercise to the reader Let and then: I'm trying to prove it. Here is my attempt: Lemma Since mean value theorem is not true for vectorial functions, my idea was to use this weaker inequality: Where The existences of the maximum of the is granted by Weierstrass Theorem, since is compact. Attempt of proof I can define a family of functions (I'll indicate its -th component). One can easily verify the following: In fact: This means that the following implication holds: In particular the LHS proposition is true for every (and this obviously makes true also RHS for every ) by Weierstrass theorem if I choose to be a closed ball of generic center (this ball  exists for every choice of , since is open). After this premises, since and thanks to our hyphothesis it's easy to see that: Moreover since a ball is connected the segment and this implies that (this can be easily proved showing that the pythagorean distance of from with is than the distance ) , so by our lemma: If we prove that is bounded we are done because we substitute it with its upper bound, so that the inequality doesn't depend upon .Notice that: Since we know that for every , ,we have just to prove that , this can be done particularly easily showing that (it's straight forward after you use the fact that is convex combination of and ). In conclusion: And this implies that: By the definition of and defining : Since was a generic point of , this concludes the proof. Is it correct?","\mathbf{f}:D\subseteq \mathbb{R}^{n+1} \to  \mathbb{R}^{n} \ \  \ D \ \text{open} \  \ \ \ \ \ (t,\mathbf{y}) \mapsto \mathbf{f}(t,\mathbf{y}) t\in \mathbb{R} \mathbf{y} \in \mathbb{R}^{n} \mathbf{f} D \mathbf{y} t \forall (t,\mathbf{y})\in D   \ \exists B_r(t,\mathbf{y}) : \exists L\in\mathbb{R}_+ : \ \forall (t,\mathbf{z}) \in B_r(t,\mathbf{y}) \\ ||\mathbf{f}(t,\mathbf{y})-\mathbf{f}(t,\mathbf{y})||\leq L||\mathbf{y}-\mathbf{z}|| L t \mathbf{f}(t,\mathbf{y})=(f_1(t,\mathbf{y}),...,f_n(t,\mathbf{y})) \mathbf{y}=(y_1,...,y_n) \mathbf{f}\text{ and }\frac{\partial f_j}{\partial y_s } \ \text{continous in } D \ \  \forall j,s\in\{1,...,n\} \implies \mathbf{f} \text{  is locally Lipschitz in }D\text{ with respect to }\mathbf{y}\text{ and uniformly respect }t  \mathbf{g}:A\subseteq \mathbb{R}^n \to \mathbb{R}^n \ \ \ \mathbf{g}\in C^1(A) \text{The segment } [\mathbf{y},\mathbf{z}]\subset A \implies ||\mathbf{g}(\mathbf{y})-\mathbf{g}(\mathbf{z})||\leq \sqrt{n}\alpha||\mathbf{y}-\mathbf{z}|| \alpha=\max \left\{\max_{[\mathbf{y},\mathbf{z}]} (||\nabla g_1(\mathbf{x})||),\max_{[\mathbf{y},\mathbf{z}]} (||\nabla g_2(\mathbf{x})||),...,\max_{[\mathbf{y},\mathbf{z}]} (||\nabla g_n(\mathbf{x})||)\right\} ||\nabla g_j|| [\mathbf{y},\mathbf{z}] \mathbf{g}_t: \mathbf{y} \mapsto \mathbf{f}(t,\mathbf{y}) g_{t,j} j  \nabla f_j(t_0,\mathbf{y}_0)=(D_t f_j(t_0,\mathbf{y}_0),\nabla g_{t_0,j}(\mathbf{y}_0)) \ \ \forall(t_0,\mathbf{y}_0)\in D  D_{y_s} f_j(t_0,\mathbf{y}_0)=\lim_{h \to 0} \frac{f_j(t_0,\mathbf{y}_0+h\mathbf{e}_s)-f_j(t_0,\mathbf{y}_0)}{h}=\lim_{h \to 0} \frac{g_{t_0,j}(\mathbf{y}_0+h\mathbf{e}_s)-g_{t_0,j}(\mathbf{y}_0)}{h}=D_{y_s} g_{t_0,j}(\mathbf{y}_0) D_{y_s} f_j(t,\mathbf{y})\leq N \ \ \ \forall(t,\mathbf{y})\in K\subset D \implies ||\nabla g_{t,j}(\mathbf{y})||\leq M  \ \ \ \forall(t,\mathbf{y})\in K  j j K (\overline{t},\mathbf{\overline{y}}) (\overline{t},\mathbf{\overline{y}}) D D_{y_s} \mathbf{g}_t:t \mapsto D_{y_s} \mathbf{f}(t,\mathbf{y})   \mathbf{g}_t:A_t\subseteq \mathbb{R}^n \to \mathbb{R}^n \ \ \ A_t=\{\mathbf{y}:(t,\mathbf{y})\in D \}  \mathbf{g}_t \in C^1(A_t) \forall (t,\mathbf{z})\in K \ \  [ (\overline{t},\overline{\mathbf{y}}),(t,\mathbf{z})]\subseteq K  [\mathbf{\overline{\mathbf{y}}},\mathbf{z}]\subseteq A_t (\overline{t},\overline{\mathbf{y}}) (t,q\overline{\mathbf{y}}+(1-q)\mathbf{z}) q\in [0,1] \leq d((\overline{t},\overline{\mathbf{y}}),(t,\mathbf{z})) \forall (t,\mathbf{z})\in K \ \ ||\mathbf{g}_t(\mathbf{\overline{y}})-\mathbf{g}_t(\mathbf{z})||\leq \sqrt{n}\alpha_t||\mathbf{\mathbf{\overline{y}}}-\mathbf{z}|| \alpha_t t \alpha_t=\max \left\{\max_{[\mathbf{\overline{y}},\mathbf{z}]} (||\nabla g_{t,1}(\mathbf{y})||),\max_{[\mathbf{\overline{y}},\mathbf{z}]} (||\nabla g_{t,2}(\mathbf{y})||),...,\max_{[\mathbf{\overline{y}},\mathbf{z}]} (||\nabla g_{t,n}(\mathbf{y})||)\right\} j \ \  ||\nabla g_{t,j}(\mathbf{y})||\leq M \ \ \forall (t,\mathbf{y})\in  K \forall \mathbf{y} \in [\mathbf{\overline{y}},\mathbf{z}], (t,\mathbf{y})\in K  d((t,\mathbf{y}),(\overline{t},\mathbf{\overline{y}}))\leq d((t,\mathbf{z}),(\overline{t},\mathbf{\overline{y}})) \mathbf{y} \mathbf{\overline{y}} \mathbf{z} \alpha_t \leq M \forall (t,\mathbf{z})\in K \ \ ||\mathbf{g}_t(\mathbf{\overline{y}})-\mathbf{g}_t(\mathbf{z})||\leq \sqrt{n}M||\mathbf{\mathbf{\overline{y}}}-\mathbf{z}|| \mathbf{g}_t L=M \sqrt{n} \forall (t,\mathbf{z})\in K \ \ ||\mathbf{f}(t,\mathbf{\overline{y}})-\mathbf{f}(t,\mathbf{z})||\leq L||\mathbf{\mathbf{\overline{y}}}-\mathbf{z}|| (\overline{t},\overline{\mathbf{y}}) D","['multivariable-calculus', 'solution-verification', 'partial-derivative', 'lipschitz-functions']"
86,Differentiation of $x^TAx$ (Multivariate Calculus),Differentiation of  (Multivariate Calculus),x^TAx,"Suppose $A$ is an $n\times n$ symmetric matrix. Consider a function $f : \mathbb{R}^n \to \mathbb{R}$ such that $$f(x) = \langle Ax , x \rangle = x^TAx\quad\quad\text{for all}~~~ x \in \mathbb{R}^n$$ I want to find its derivative, i.e. $Df$ . Most probably, it can be done by expanding $x^TAx$ by usual matrix multiplication. But I'm looking for a procedure which doesn't involve this sort of expansion. However, any elementary proof will help. Thanks in advance. Sorry if someone had asked this problem before.","Suppose is an symmetric matrix. Consider a function such that I want to find its derivative, i.e. . Most probably, it can be done by expanding by usual matrix multiplication. But I'm looking for a procedure which doesn't involve this sort of expansion. However, any elementary proof will help. Thanks in advance. Sorry if someone had asked this problem before.","A n\times n f : \mathbb{R}^n \to \mathbb{R} f(x) = \langle Ax , x \rangle = x^TAx\quad\quad\text{for all}~~~ x \in \mathbb{R}^n Df x^TAx","['calculus', 'multivariable-calculus', 'derivatives', 'linear-transformations']"
87,When will level sets be closed/Jordan curves?,When will level sets be closed/Jordan curves?,,"Are there any conditions on a $C^{1}$ function $f$ , for example from $\mathbb{R}^2$ to $\mathbb{R}$ , and constant $c$ which would guarantee that the level set $\left\{(x,y) \in \mathbb{R}^2: f(x,y)=c\right\}$ is a closed curve? By ""closed curve"" I mean a curve which is a closed loop (homeomorphic to the circle), rather than merely a curve which, seen as a set of points, is a closed set (as discussed in comments here: Do the curves of a level set of a continuous function have to be closed? ). Similarly, what about conditions for the level set to be a simple (non-self-intersecting) closed curve, i.e. a Jordan curve in the plane?","Are there any conditions on a function , for example from to , and constant which would guarantee that the level set is a closed curve? By ""closed curve"" I mean a curve which is a closed loop (homeomorphic to the circle), rather than merely a curve which, seen as a set of points, is a closed set (as discussed in comments here: Do the curves of a level set of a continuous function have to be closed? ). Similarly, what about conditions for the level set to be a simple (non-self-intersecting) closed curve, i.e. a Jordan curve in the plane?","C^{1} f \mathbb{R}^2 \mathbb{R} c \left\{(x,y) \in \mathbb{R}^2: f(x,y)=c\right\}","['real-analysis', 'multivariable-calculus']"
88,"Region D is bounded by below by $z=0$, and above by $x^2+y^2+z^2=4$, and on sides $x^2+y^2=1$ is required to be setup in spherical coordinate","Region D is bounded by below by , and above by , and on sides  is required to be setup in spherical coordinate",z=0 x^2+y^2+z^2=4 x^2+y^2=1,"I started Spherical coordinate literally yesterday so please bear with me, im still fairly new to the topic, so i came upon this question and graphically it should look like this to me I already have the final answer given by : $\displaystyle\int _0^{2\pi }\int _0^2\int _0^{\frac{\pi }{6}}\rho \ ^2\sin \phi \ d\phi \ d\rho \ d\theta +\ \ \int _0^{2\pi }\int _0^1\int _{\frac{\pi }{6}}^{\frac{\pi }{2}}\rho \ ^2\sin \phi \ d\phi \ d\rho \ d\theta +\ \int _0^{2\pi }\int _1^2\int _{\frac{\pi }{6}}^{\sin ^{-1}\left(\frac{1}{\rho \ }\right)}\rho \ ^2\sin \phi \ d\phi \ d\rho \ d\theta \ $ i understood the limit for $\theta$ which is $0<\theta <2\pi $ , I even understood why we took $0<\rho \ <1$ and $0<\rho \ <2$ and $1<\rho <2$ what i didn't understand at all are these $\phi $ angles limits ( $\sin ^{-1}\left(\frac{1}{\rho }\ \right), \frac{\pi }{6},\frac{\pi }{2}$ ) I want to be able to form the limits of these $\phi$ on my own but I'm thoroughly confused. Please if anybody can help explain the limits of the $\phi$ , it'd be greatly appreciated. Also side note: this particular example has already been asked once but i didnt understand the answer that was given to that question which is why im asking again. So please kindly don't mark this as duplicate, thank you.","I started Spherical coordinate literally yesterday so please bear with me, im still fairly new to the topic, so i came upon this question and graphically it should look like this to me I already have the final answer given by : i understood the limit for which is , I even understood why we took and and what i didn't understand at all are these angles limits ( ) I want to be able to form the limits of these on my own but I'm thoroughly confused. Please if anybody can help explain the limits of the , it'd be greatly appreciated. Also side note: this particular example has already been asked once but i didnt understand the answer that was given to that question which is why im asking again. So please kindly don't mark this as duplicate, thank you.","\displaystyle\int _0^{2\pi }\int _0^2\int _0^{\frac{\pi }{6}}\rho \ ^2\sin \phi \ d\phi \ d\rho \ d\theta +\ \ \int _0^{2\pi }\int _0^1\int _{\frac{\pi }{6}}^{\frac{\pi }{2}}\rho \ ^2\sin \phi \ d\phi \ d\rho \ d\theta +\ \int _0^{2\pi }\int _1^2\int _{\frac{\pi }{6}}^{\sin ^{-1}\left(\frac{1}{\rho \ }\right)}\rho \ ^2\sin \phi \ d\phi \ d\rho \ d\theta \  \theta 0<\theta <2\pi  0<\rho \ <1 0<\rho \ <2 1<\rho <2 \phi  \sin ^{-1}\left(\frac{1}{\rho }\ \right), \frac{\pi }{6},\frac{\pi }{2} \phi \phi","['multivariable-calculus', 'multiple-integral']"
89,Matrix Multiplication in Index Notation,Matrix Multiplication in Index Notation,,"I am trying to express the $i$ and $j$ th component of the product $AB^{T}C$ in terms of the components of $A$ , $B$ and $C$ in index notation/Einstein summation convention, where $A\in\Bbb{R}^{n\times p}$ , $B\in\Bbb{R}^{q\times p}$ and $C\in\Bbb{R}^{q\times s}$ . I am aware of the standard notation for a supposed matrix product $C=AB$ , which is $c_{ik}=a_{ij}b_{jk}$ , but afterwards I am not sure how to proceed - appreciate any help I can get, thanks.","I am trying to express the and th component of the product in terms of the components of , and in index notation/Einstein summation convention, where , and . I am aware of the standard notation for a supposed matrix product , which is , but afterwards I am not sure how to proceed - appreciate any help I can get, thanks.",i j AB^{T}C A B C A\in\Bbb{R}^{n\times p} B\in\Bbb{R}^{q\times p} C\in\Bbb{R}^{q\times s} C=AB c_{ik}=a_{ij}b_{jk},"['calculus', 'multivariable-calculus']"
90,Solving a line integral,Solving a line integral,,"A vector field $\overrightarrow{E}=7x^2\hat{e}_x +3y\hat{e}_y-2xz\hat{e}_z$ Evaluate the line integral I $=\int_{c} \overrightarrow{E}\bullet d\overrightarrow{I}$ where the contour C is the straight line from the point (0,0,0) to (1,2,0) I was wondering if we are allowed to split the intergral up, so for example $I= \int_{0}^{1} 7x^2 dx + \int_{0}^{2} 3yzdx$ So do the line integral from (0,0,0) to (1,0,0) in the first integral then (1,0,0) to (1,2,0). or is there an easier method.","A vector field Evaluate the line integral I where the contour C is the straight line from the point (0,0,0) to (1,2,0) I was wondering if we are allowed to split the intergral up, so for example So do the line integral from (0,0,0) to (1,0,0) in the first integral then (1,0,0) to (1,2,0). or is there an easier method.",\overrightarrow{E}=7x^2\hat{e}_x +3y\hat{e}_y-2xz\hat{e}_z =\int_{c} \overrightarrow{E}\bullet d\overrightarrow{I} I= \int_{0}^{1} 7x^2 dx + \int_{0}^{2} 3yzdx,"['multivariable-calculus', 'vector-fields', 'line-integrals']"
91,Weighted sum of diagonal values is dominated by the sum of the singular values,Weighted sum of diagonal values is dominated by the sum of the singular values,,"Let $A$ be a $2 \times 2$ real matrix with $\det A \ge 0$ , and let $\sigma_1 \le \sigma_2$ be its singular values. Let $0 \le x_1 \le x_2$ . How to prove that $x_1 A_{11} +x_2A_{22} \le x_1 \sigma_1+x_2 \sigma_2$ ? I have a proof, but it uses Riemannian geometry. I am looking for a more elementary proof. Equivalent formulation: Set $K=\{ A \in M_2 \, | \, \det A \ge 0 \, \, \text{ and the singular values of } A \, \text{are } \sigma_1,\sigma_2 \}$ . Then $$\max_{A \in K} x_1 A_{11} +x_2A_{22}=x_1 \sigma_1+x_2 \sigma_2.$$ It suffices to prove that the maximum is obtained at a diagonal matrix; for a diagonal matrix with nonnegative entries $A=\operatorname{diag}(\sigma_{\alpha(i)})$ , the claim reduces to the rearrangement inequality $\sum_i x_i\sigma_{\alpha(i)} \le \sum_i x_i\sigma_i$ , where $\alpha \in S_2$ is a permutation.(for dimension $2$ this can be verified directly by hand.) I guess this should be well-known. Is there any reference in the literature? Is it true for $n \times n$ matrices? If $x_1=x_2$ , then this reduces to $\text{tr}(A) \le \sigma_1+\sigma_2$ which is a classic easy result.","Let be a real matrix with , and let be its singular values. Let . How to prove that ? I have a proof, but it uses Riemannian geometry. I am looking for a more elementary proof. Equivalent formulation: Set . Then It suffices to prove that the maximum is obtained at a diagonal matrix; for a diagonal matrix with nonnegative entries , the claim reduces to the rearrangement inequality , where is a permutation.(for dimension this can be verified directly by hand.) I guess this should be well-known. Is there any reference in the literature? Is it true for matrices? If , then this reduces to which is a classic easy result.","A 2 \times 2 \det A \ge 0 \sigma_1 \le \sigma_2 0 \le x_1 \le x_2 x_1 A_{11} +x_2A_{22} \le x_1 \sigma_1+x_2 \sigma_2 K=\{ A \in M_2 \, | \, \det A \ge 0 \, \, \text{ and the singular values of } A \, \text{are } \sigma_1,\sigma_2 \} \max_{A \in K} x_1 A_{11} +x_2A_{22}=x_1 \sigma_1+x_2 \sigma_2. A=\operatorname{diag}(\sigma_{\alpha(i)}) \sum_i x_i\sigma_{\alpha(i)} \le \sum_i x_i\sigma_i \alpha \in S_2 2 n \times n x_1=x_2 \text{tr}(A) \le \sigma_1+\sigma_2","['multivariable-calculus', 'inequality', 'optimization', 'matrix-calculus', 'svd']"
92,"Showing that $\frac{\|f(x)\|}{\|x\|}=0, \text{ as $x \to 0$}$",Showing that x \to 0,"\frac{\|f(x)\|}{\|x\|}=0, \text{ as  }","Let $f : \Bbb R^2 \to \Bbb R^2$ be differentiable at $x=0.$ Show that $$\frac{\|f(x)\|}{\|x\|}=0, \text{ as $x \to 0$}$$ if and only if $f(0) = 0$ and $Df(0) = 0.$ Using the definition $f(x)-f(a) = Df(a)(x-a)+\|x-a\|\varepsilon(x-a)$ I get that $$f(x)-0=0+\|x-0\|\varepsilon(x-0) \Rightarrow f(x) = \|x\|\varepsilon(x).$$ Dividing by $\|x\|$ I have that $\frac{f(x)}{\|x\|}=\varepsilon(x)$ . Now doesn't $\varepsilon(x) \to 0$ by definition? Thus $\frac{f(x)}{\|x\|} \to 0$ ?",Let be differentiable at Show that if and only if and Using the definition I get that Dividing by I have that . Now doesn't by definition? Thus ?,"f : \Bbb R^2 \to \Bbb R^2 x=0. \frac{\|f(x)\|}{\|x\|}=0, \text{ as x \to 0} f(0) = 0 Df(0) = 0. f(x)-f(a) = Df(a)(x-a)+\|x-a\|\varepsilon(x-a) f(x)-0=0+\|x-0\|\varepsilon(x-0) \Rightarrow f(x) = \|x\|\varepsilon(x). \|x\| \frac{f(x)}{\|x\|}=\varepsilon(x) \varepsilon(x) \to 0 \frac{f(x)}{\|x\|} \to 0",['real-analysis']
93,Can someone help me to compute this integral with a delta function,Can someone help me to compute this integral with a delta function,,"I don't know how to compute this integral: $$\int_{0}^{\infty}  \prod_{i=1}^a dx_i \,\delta \left(\sum_{i=1}^a x_i - a\right)$$ The result should be: $$\frac{a^{a-1} }{(a-1)!}$$ Thanks very much for helping! edit: Thanks to the link I am one step further: $$\int_0^\infty dx_a\delta \left(\sum_{i=1}^a x_i - a\right)=1$$ if $$x_a=a-\sum_{i=1}^{a-1}x_i\geq 0\\ \Leftrightarrow \quad \sum_{i=1}^{a-1}x_i \leq a$$ So: $$\int_{0}^{\infty}  \prod_{i=1}^a dx_i \,\delta (\sum_{i=1}^a x_i - a)=\int_0^a dx_1 \int_0^{a-x_1} dx_2 \ldots \int_0^{a-x_1 - \ldots - x_{a-2}} dx_{a-1}$$ But how do I now show: $$\int_0^a dx_1 \int_0^{a-x_1} dx_2 \ldots \int_0^{a-x_1 - \ldots - x_{a-2}} dx_{a-1}=\frac{a^{a-1} }{(a-1)!}$$",I don't know how to compute this integral: The result should be: Thanks very much for helping! edit: Thanks to the link I am one step further: if So: But how do I now show:,"\int_{0}^{\infty}  \prod_{i=1}^a dx_i \,\delta \left(\sum_{i=1}^a x_i - a\right) \frac{a^{a-1} }{(a-1)!} \int_0^\infty dx_a\delta \left(\sum_{i=1}^a x_i - a\right)=1 x_a=a-\sum_{i=1}^{a-1}x_i\geq 0\\
\Leftrightarrow \quad \sum_{i=1}^{a-1}x_i \leq a \int_{0}^{\infty}  \prod_{i=1}^a dx_i \,\delta (\sum_{i=1}^a x_i - a)=\int_0^a dx_1 \int_0^{a-x_1} dx_2 \ldots \int_0^{a-x_1 - \ldots - x_{a-2}} dx_{a-1} \int_0^a dx_1 \int_0^{a-x_1} dx_2 \ldots \int_0^{a-x_1 - \ldots - x_{a-2}} dx_{a-1}=\frac{a^{a-1} }{(a-1)!}","['multivariable-calculus', 'dirac-delta']"
94,Double Integral of f(y) over region in xy plane,Double Integral of f(y) over region in xy plane,,"I was given that $\int_{0}^{1} \int_{x}^{1-x}f(y)dydx$ is equal to a specific value for all integratable functions $f(y)$ , I was able to work out that the value is $0$ , but when I try to visualise this I just get confused. Is it possible I'm missing something? Can someone provide a nice visual explanation?","I was given that is equal to a specific value for all integratable functions , I was able to work out that the value is , but when I try to visualise this I just get confused. Is it possible I'm missing something? Can someone provide a nice visual explanation?",\int_{0}^{1} \int_{x}^{1-x}f(y)dydx f(y) 0,['multivariable-calculus']
95,Constrained minimization: characterizing derivatives of optimum with respect to parameters,Constrained minimization: characterizing derivatives of optimum with respect to parameters,,"Suppose I have a function $f(x,y)$ defined on $[0,1]^2$ that is equal to the value of the following constrained minimization problem: $$f(x,y) = min_{a\in [0,1-x),\ b\in[0,x)}\ \left\{ h(x,a) + g(x,b) \right\}\ \text{subject to } a+b=y$$ Here is some relevant information about $h$ and $g$ . First, $h(x,0)=g(x,0)$ for all $x$ . Second, $\lim_{a\rightarrow 1-x} h(x,a)=\infty$ and $\lim_{b\rightarrow x}g(x,b)=\infty$ (this is why I write the feasible sets as partially open intervals). I am interested in an analytical characterization of the partial derivatives $f_x(x,y)$ and $f_y(x,y)$ in terms of the partial derivatives of $h$ and $g$ . My envelope theorem is super rusty, and the simplicity of both the objective and the constraint make me think there is a relatively easy solution here that I am missing. Note this is not homework (I am not a student, but rather an old professor who has come to rely perhaps too heavily on numerical methods for his own good). Here is my logic so far: (i) The simplest case is if $x=0$ , where $b=0$ and $a=y$ by construction. Obviously in this case $f(0,y)$ = $h(0,y)$ for all $y$ . Then $f_x(0,y) = h_x(0,y)$ and $f_y(0,y) = h_a(0,y)$ . (ii) If $x>0$ but we have a corner solution (either $a=0$ or $b=0$ ), the answer is still pretty simple. If $a=0$ then $f_x(x,y)=g_x(x,y)$ and $f_y(x,y)=g_b(x,y)$ . If $b=0$ then $f_x(x,y) = h_x(x,y)$ and $f_y(x,y) = h_a(x,y)$ as in (i). (iii) For interior solutions, the first-order conditions imply $h_a(x,a^*(x,y))=g_b(x,b^*(x,y))$ , where $a^*(x,y)$ and $b^*(x,y)$ are the argmins. I am thinking $f_y(x,y)=h_a(x,a^*(x,y))=g_b(x,b^*(x,y))$ in this case. At the margin, allocating a small increase in y to a or b should have the same effect on the objective. [iv] The only thing I am left with is: what is $f_x(x,y)$ for interior solutions?","Suppose I have a function defined on that is equal to the value of the following constrained minimization problem: Here is some relevant information about and . First, for all . Second, and (this is why I write the feasible sets as partially open intervals). I am interested in an analytical characterization of the partial derivatives and in terms of the partial derivatives of and . My envelope theorem is super rusty, and the simplicity of both the objective and the constraint make me think there is a relatively easy solution here that I am missing. Note this is not homework (I am not a student, but rather an old professor who has come to rely perhaps too heavily on numerical methods for his own good). Here is my logic so far: (i) The simplest case is if , where and by construction. Obviously in this case = for all . Then and . (ii) If but we have a corner solution (either or ), the answer is still pretty simple. If then and . If then and as in (i). (iii) For interior solutions, the first-order conditions imply , where and are the argmins. I am thinking in this case. At the margin, allocating a small increase in y to a or b should have the same effect on the objective. [iv] The only thing I am left with is: what is for interior solutions?","f(x,y) [0,1]^2 f(x,y) = min_{a\in [0,1-x),\ b\in[0,x)}\ \left\{ h(x,a) + g(x,b) \right\}\ \text{subject to } a+b=y h g h(x,0)=g(x,0) x \lim_{a\rightarrow 1-x} h(x,a)=\infty \lim_{b\rightarrow x}g(x,b)=\infty f_x(x,y) f_y(x,y) h g x=0 b=0 a=y f(0,y) h(0,y) y f_x(0,y) = h_x(0,y) f_y(0,y) = h_a(0,y) x>0 a=0 b=0 a=0 f_x(x,y)=g_x(x,y) f_y(x,y)=g_b(x,y) b=0 f_x(x,y) = h_x(x,y) f_y(x,y) = h_a(x,y) h_a(x,a^*(x,y))=g_b(x,b^*(x,y)) a^*(x,y) b^*(x,y) f_y(x,y)=h_a(x,a^*(x,y))=g_b(x,b^*(x,y)) f_x(x,y)","['multivariable-calculus', 'optimization', 'partial-derivative', 'chain-rule', 'constraints']"
96,$\sup_{x \neq y}\frac{||f(x)-f(y)||}{||x-y||}= \sup_{z \in U} ||f'(z)||$,,\sup_{x \neq y}\frac{||f(x)-f(y)||}{||x-y||}= \sup_{z \in U} ||f'(z)||,"Let $f:U \to \mathbb{R}^{m}$ differentiable in the open convex $U \subset \mathbb{R}^{m}$ . Prove that $$\sup_{x \neq y}\frac{\|f(x)-f(y)\|}{\|x-y\|}= \sup_{z \in U} \|f'(z)\|$$ by the mean value theorem we have $$\sup_{x \neq y}\frac{\|f(x)-f(y)\|}{\|x-y\|} \leq \sup_{z \in U} \|f'(z)\|$$ the other implication I can't do it, any ideas?","Let differentiable in the open convex . Prove that by the mean value theorem we have the other implication I can't do it, any ideas?",f:U \to \mathbb{R}^{m} U \subset \mathbb{R}^{m} \sup_{x \neq y}\frac{\|f(x)-f(y)\|}{\|x-y\|}= \sup_{z \in U} \|f'(z)\| \sup_{x \neq y}\frac{\|f(x)-f(y)\|}{\|x-y\|} \leq \sup_{z \in U} \|f'(z)\|,"['real-analysis', 'multivariable-calculus', 'lipschitz-functions']"
97,Why is the Volume integration & Surface Area integration of a sphere different?,Why is the Volume integration & Surface Area integration of a sphere different?,,"For both volume & surface area, the sphere is split into many discs and the area or circumference of the discs are summed up in an integral. But the summation process uses $dy$ for volume & $r\,d\theta$ (arc-length) for surface area. Why this discrepancy? Supposing we have a sphere in the $x$ - $y$ - $z$ plane where you split the sphere into discs along the $y$ axis.. If you visualise the problem from $z$ axis looking down over the $x$ - $y$ plane.. The sphere will look like a circle and the disc will be a line segment inside the circle (chord). The length of the line segment will be the diameter of the disc. And the point where the line segment and circle meet - (x,y) can be solved by plugging in the value of y and the x we solve for will then be the radius of the disc. Now to calculate surface area, we need to sum up the circumference of each disc $ s(x) = 2\pi x$ & and for volume, we need to sum up the area of each disc $ v(x) = \pi x^2 $ Say, the point $(x,y)$ makes an angle $\theta$ with the origin. Then for surface area, we assume for length $r\,d\theta$ , the disc radius is not changing (across arc length) & we integrate it as: $$\int s(x)\, rd\theta $$ But for volume, instead of using the arc length, we use the diameter $dy$ to integrate it as: $$\int v(x) \,dy$$ Why this discrepancy? In both cases, the number of discs is the same so why should the summation be different? I tried interchanging the summation process and when i converted everything into polar co-ordinates ( $x = r\,cos\theta, y = r\,sin\theta $ ) i get an extra $cos\theta$ since $ dy = rd\theta.cos\theta$ The same happens to me when i calculate Moment of Inertia for a solid sphere & hollow sphere. Similarly when i calculate gravity for a point outside a solid sphere & hollow sphere. Can someone please tell me, why we need to change the summation process?? What decides the summation process, why the difference?","For both volume & surface area, the sphere is split into many discs and the area or circumference of the discs are summed up in an integral. But the summation process uses for volume & (arc-length) for surface area. Why this discrepancy? Supposing we have a sphere in the - - plane where you split the sphere into discs along the axis.. If you visualise the problem from axis looking down over the - plane.. The sphere will look like a circle and the disc will be a line segment inside the circle (chord). The length of the line segment will be the diameter of the disc. And the point where the line segment and circle meet - (x,y) can be solved by plugging in the value of y and the x we solve for will then be the radius of the disc. Now to calculate surface area, we need to sum up the circumference of each disc & and for volume, we need to sum up the area of each disc Say, the point makes an angle with the origin. Then for surface area, we assume for length , the disc radius is not changing (across arc length) & we integrate it as: But for volume, instead of using the arc length, we use the diameter to integrate it as: Why this discrepancy? In both cases, the number of discs is the same so why should the summation be different? I tried interchanging the summation process and when i converted everything into polar co-ordinates ( ) i get an extra since The same happens to me when i calculate Moment of Inertia for a solid sphere & hollow sphere. Similarly when i calculate gravity for a point outside a solid sphere & hollow sphere. Can someone please tell me, why we need to change the summation process?? What decides the summation process, why the difference?","dy r\,d\theta x y z y z x y  s(x) = 2\pi x  v(x) = \pi x^2  (x,y) \theta r\,d\theta \int s(x)\, rd\theta  dy \int v(x) \,dy x = r\,cos\theta, y = r\,sin\theta  cos\theta  dy = rd\theta.cos\theta","['integration', 'multivariable-calculus', 'physics', 'spheres', 'multiple-integral']"
98,Books for vector analysis,Books for vector analysis,,"In the beginning of Griffiths electrodynamics there is a section for Vector analysis. All that was taught in very brief and I would like to read it in detail from a mathematical perspective rather than a Physics. I want a book on VECTOR ANALYSIS which has all topics such as Dirac delta function vector fields theory, Spherical coordinates, curl,divergence, gradient, potentials etc and basically has all topics. I have a good understanding on single variable calculus. Please help","In the beginning of Griffiths electrodynamics there is a section for Vector analysis. All that was taught in very brief and I would like to read it in detail from a mathematical perspective rather than a Physics. I want a book on VECTOR ANALYSIS which has all topics such as Dirac delta function vector fields theory, Spherical coordinates, curl,divergence, gradient, potentials etc and basically has all topics. I have a good understanding on single variable calculus. Please help",,"['multivariable-calculus', 'vector-analysis', 'book-recommendation']"
99,"Prove that $f(W)$ is the graph of $y_{n+1} = \varphi(y_1,\cdots,y_n)$",Prove that  is the graph of,"f(W) y_{n+1} = \varphi(y_1,\cdots,y_n)","Let $f: U \subseteq \mathbb R^n \rightarrow \mathbb R^{n+1}$ be of class $C^k,k\geq 1,$ and $U$ open. If for every $x\in U$ , $$f(x) = (f_1(x),\cdots, f_{n+1}(x)) \text{    and   }\det\bigg(\frac{\partial f_i}{\partial x_j}\bigg)_{1\leq i,j \leq n} \neq0,$$ then, for every $x\in U$ , there exists a neighbourhood $W\subseteq U$ of $x$ such that $f(W)$ is the graph of a $C^k$ function $y_{n+1} = \varphi (y_1,\cdots,y_n)$ . Saying that $f(W)$ is the graph of $\varphi$ is the same as saying the following sets are equal: $\{(f_1(x),\cdots,f_{n+1}(x)): x\in W\} = \{((y_1,\cdots,y_n,\varphi(y_1,\cdots,y_n)): (y_1,\cdots,y_n)\in \text{Domain of $\varphi$}\}$ . In other words, I have to prove that locally there is a function $\varphi$ depending on the previous coordinates $f_1,\cdots,f_n$ . The first thing that I've noticed is that $f'(x)$ is an injective linear transformation. Indeed, we have $n = \dim \ker f'(x) + \dim \text{im}f'(x) \geq n + \dim \ker f'(x) \geq n \implies \dim\ker f'(x) =0,$ since $f'(x)$ has at least $n$ linearly independent lines. Now I don't know how to proceed exactly. Initially, I was wondering of using the local immersion theorem (since $f'(x)$ is injective), but I couldnt see a way to use this theorem to express $f_{n+1}$ in terms of the others. I also considered the function $\pi:\mathbb R^{n+1} \rightarrow \mathbb R^n, (x_1,\cdots, x_{n+1}) \mapsto (x_1,\cdots, x_n).$ So, $\pi(f(x)) = (f_1(x),\cdots,f_n(x)).$ Writing $g = \pi \circ f$ , its derivative $g'(x)$ is invertible, hence, it is a local diffeomorphism with inverse $h$ . Therefore, $\pi \circ f \circ h = g \circ h = I_d$ and we have $\pi(f(h(x_1,\dots,x_n) ) = (x_1,\cdots,x_n).$ If I could ""get rid"" of $\pi$ somehow, this equation would give me that $f(h(x_1,\cdots,x_n)) = (x_1,\cdots,x_n,\varphi(x_1,\cdots,x_n))$ and thats what whe need to show. But I cant find a clear way to say or jusitfy this. Any insight, hint? Thank you.","Let be of class and open. If for every , then, for every , there exists a neighbourhood of such that is the graph of a function . Saying that is the graph of is the same as saying the following sets are equal: . In other words, I have to prove that locally there is a function depending on the previous coordinates . The first thing that I've noticed is that is an injective linear transformation. Indeed, we have since has at least linearly independent lines. Now I don't know how to proceed exactly. Initially, I was wondering of using the local immersion theorem (since is injective), but I couldnt see a way to use this theorem to express in terms of the others. I also considered the function So, Writing , its derivative is invertible, hence, it is a local diffeomorphism with inverse . Therefore, and we have If I could ""get rid"" of somehow, this equation would give me that and thats what whe need to show. But I cant find a clear way to say or jusitfy this. Any insight, hint? Thank you.","f: U \subseteq \mathbb R^n \rightarrow \mathbb R^{n+1} C^k,k\geq 1, U x\in U f(x) = (f_1(x),\cdots, f_{n+1}(x)) \text{    and   }\det\bigg(\frac{\partial f_i}{\partial x_j}\bigg)_{1\leq i,j \leq n} \neq0, x\in U W\subseteq U x f(W) C^k y_{n+1} = \varphi (y_1,\cdots,y_n) f(W) \varphi \{(f_1(x),\cdots,f_{n+1}(x)): x\in W\} = \{((y_1,\cdots,y_n,\varphi(y_1,\cdots,y_n)): (y_1,\cdots,y_n)\in \text{Domain of \varphi}\} \varphi f_1,\cdots,f_n f'(x) n = \dim \ker f'(x) + \dim \text{im}f'(x) \geq n + \dim \ker f'(x) \geq n \implies \dim\ker f'(x) =0, f'(x) n f'(x) f_{n+1} \pi:\mathbb R^{n+1} \rightarrow \mathbb R^n, (x_1,\cdots, x_{n+1}) \mapsto (x_1,\cdots, x_n). \pi(f(x)) = (f_1(x),\cdots,f_n(x)). g = \pi \circ f g'(x) h \pi \circ f \circ h = g \circ h = I_d \pi(f(h(x_1,\dots,x_n) ) = (x_1,\cdots,x_n). \pi f(h(x_1,\cdots,x_n)) = (x_1,\cdots,x_n,\varphi(x_1,\cdots,x_n))","['real-analysis', 'multivariable-calculus']"
