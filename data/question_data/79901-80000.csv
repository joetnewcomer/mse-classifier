,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Complexity of matrix multiplication over $\mathbb F_2$,Complexity of matrix multiplication over,\mathbb F_2,"I've been reading about theoretical bounds on the computational complexity of $n\times n$ matrix-matrix multiplication in floating-point arithmetic, about how the complexity is known to fall between $\mathcal O(n^2)$ and $\mathcal O(n^3)$ , and how the exponent has been slowly whittled down over time by various researchers. It's currently unknown whether a $\mathcal O(n^2)$ algorithm is possible. I was wondering about what the complexity of multiplying two bit-matrices over the finite field $\mathbb F_2$ (or other finite fields - my intuition tells me the complexity will be the same), but I haven't found any references online. Are there any known ""tricks"" for multiplying matrices in $\mathbb F_2^{n\times n}$ that makes it less expensive than multiplication in larger fields or in floating point? What current bounds are known? Is there a known $\mathcal O(n^2)$ algorithm? Can anyone provide any relevant references for these sort of questions?","I've been reading about theoretical bounds on the computational complexity of matrix-matrix multiplication in floating-point arithmetic, about how the complexity is known to fall between and , and how the exponent has been slowly whittled down over time by various researchers. It's currently unknown whether a algorithm is possible. I was wondering about what the complexity of multiplying two bit-matrices over the finite field (or other finite fields - my intuition tells me the complexity will be the same), but I haven't found any references online. Are there any known ""tricks"" for multiplying matrices in that makes it less expensive than multiplication in larger fields or in floating point? What current bounds are known? Is there a known algorithm? Can anyone provide any relevant references for these sort of questions?",n\times n \mathcal O(n^2) \mathcal O(n^3) \mathcal O(n^2) \mathbb F_2 \mathbb F_2^{n\times n} \mathcal O(n^2),"['linear-algebra', 'matrices', 'reference-request', 'algorithms', 'finite-fields']"
1,Using matrix to analyze an ODE system $\begin{cases}\dot a_k(t)=2(b_k^2-b_{k-1}^2)\\\dot b_k(t)=b_k(a_{k+1}-a_{k})\end{cases}$ with $b_0(t)=b_n(t)=0$.,Using matrix to analyze an ODE system  with .,\begin{cases}\dot a_k(t)=2(b_k^2-b_{k-1}^2)\\\dot b_k(t)=b_k(a_{k+1}-a_{k})\end{cases} b_0(t)=b_n(t)=0,"Let $a_k(t), b_k (t) \in \mathbb{R} \ (k = 1, 2, \dots, n)$ satisfy the following differential equations: $$\begin{aligned} \frac{d}{dt}a_k(t) &= 2 \left( b_k^2 - b_{k-1}^2 \right) \\ \frac{d}{dt} b_k(t) &= b_k \left( a_{k+1} - a_{k} \right)\end{aligned} \qquad k = 1, 2, \dots , n $$ where $b_0(t) = b_n(t) = 0$ . Consider the $n \times n$ tri-diagonal matrix $$L(a,b)=\begin{bmatrix}     a_1 & b_1 & 0 & \dots  & 0 &0\\     b_1 & a_2 & b_2 & \dots  & 0 &0\\     \vdots & \vdots & \vdots & \ddots & \vdots& \vdots \\ 0 & 0 & 0 & \dots  & a_{n-1} & b_{n-1}\\     0 & 0 & 0 & \dots  & b_{n-1} & a_n \end{bmatrix}$$ show that: The eigenvalues of $L(t) = L(a(t), b(t))$ are independent of $t$ . $\lim_{t \to \infty}b_k(t) = 0$ for $k=1,2,\dots,n-1$ . For P1, Let $L(a,b)=\begin{bmatrix}     0 & b_1 & 0 & \dots  & 0 &0\\     -b_1 & 0 & b_2 & \dots  & 0 &0\\     \vdots & \vdots & \vdots & \ddots & \vdots& \vdots \\ 0 & 0 & 0 & \dots  & 0 & b_{n-1}\\     0 & 0 & 0 & \dots  & -b_{n-1} & 0 \end{bmatrix}$ , then all above becomes $\frac{dL}{dt}=PL-LP$ . Since $L$ exists and is unique, $P$ also exists and is unique. Hence there exists only one $U$ such that $\frac{d}{dt}U(t,s)=P(t)U(t,s),U(s,s)=I$ . Since $U(t,s)L(s)U(t,s)^{-1}$ is also the solution to $\frac{dL}{dt}=PL-LP$ , we must have $L(t)=U(t,s)L(s)U(t,s)^{-1}$ and P1 is solved. However now I'm stucking on P2, which is equivalent to prove $\lim_{t\rightarrow \infty}P=0$ or $L$ tends to be diagonal, in which the solution of ODE tends to be constant, and I don't know how to handle with analyzing matrix ODE. Any help, hint or solution on Problem 2 would be appreciated!","Let satisfy the following differential equations: where . Consider the tri-diagonal matrix show that: The eigenvalues of are independent of . for . For P1, Let , then all above becomes . Since exists and is unique, also exists and is unique. Hence there exists only one such that . Since is also the solution to , we must have and P1 is solved. However now I'm stucking on P2, which is equivalent to prove or tends to be diagonal, in which the solution of ODE tends to be constant, and I don't know how to handle with analyzing matrix ODE. Any help, hint or solution on Problem 2 would be appreciated!","a_k(t), b_k (t) \in \mathbb{R} \ (k = 1, 2, \dots, n) \begin{aligned} \frac{d}{dt}a_k(t) &= 2 \left( b_k^2 - b_{k-1}^2 \right) \\ \frac{d}{dt} b_k(t) &= b_k \left( a_{k+1} - a_{k} \right)\end{aligned} \qquad k = 1, 2, \dots , n  b_0(t) = b_n(t) = 0 n \times n L(a,b)=\begin{bmatrix}
    a_1 & b_1 & 0 & \dots  & 0 &0\\
    b_1 & a_2 & b_2 & \dots  & 0 &0\\
    \vdots & \vdots & \vdots & \ddots & \vdots& \vdots \\
0 & 0 & 0 & \dots  & a_{n-1} & b_{n-1}\\
    0 & 0 & 0 & \dots  & b_{n-1} & a_n
\end{bmatrix} L(t) = L(a(t), b(t)) t \lim_{t \to \infty}b_k(t) = 0 k=1,2,\dots,n-1 L(a,b)=\begin{bmatrix}
    0 & b_1 & 0 & \dots  & 0 &0\\
    -b_1 & 0 & b_2 & \dots  & 0 &0\\
    \vdots & \vdots & \vdots & \ddots & \vdots& \vdots \\
0 & 0 & 0 & \dots  & 0 & b_{n-1}\\
    0 & 0 & 0 & \dots  & -b_{n-1} & 0
\end{bmatrix} \frac{dL}{dt}=PL-LP L P U \frac{d}{dt}U(t,s)=P(t)U(t,s),U(s,s)=I U(t,s)L(s)U(t,s)^{-1} \frac{dL}{dt}=PL-LP L(t)=U(t,s)L(s)U(t,s)^{-1} \lim_{t\rightarrow \infty}P=0 L","['matrices', 'ordinary-differential-equations', 'limits', 'derivatives', 'tridiagonal-matrices']"
2,Define the smallest subspace of $3\times 3$ matrix vector space that contains the set of all invertible matricies.,Define the smallest subspace of  matrix vector space that contains the set of all invertible matricies.,3\times 3,"Define the smallest subspace of $3\times 3$ matrix vector space that contains the set of all invertible matrices. My attempt: $A$ is invertible if and only if its columns form a basis in $F^3$ . The only subspace I can think of is the vector space of all $3\times 3$ matrices, which I cannot prove is the smallest. Note: We are not allowed to use determinant.","Define the smallest subspace of matrix vector space that contains the set of all invertible matrices. My attempt: is invertible if and only if its columns form a basis in . The only subspace I can think of is the vector space of all matrices, which I cannot prove is the smallest. Note: We are not allowed to use determinant.",3\times 3 A F^3 3\times 3,"['linear-algebra', 'matrices', 'invariant-subspace']"
3,Changing the basis in diagonalization: why doesn't it work?,Changing the basis in diagonalization: why doesn't it work?,,"I read the theorem  on Apostol Calculus II : Theorem $4.10$ Let $T : V \to V$ be a linear transformation, where $V$ has scalars in $F$ , and $\dim V = n$ . Assume that the characteristic polynomial of $T$ has $n$ distinct roots $\lambda_1,\dots, \lambda_n$ in $F$ . Then we have: The corresponding eigenvectors $u_1,\dots,u_n$ form a basis for $V$ . The matrix of $T$ relative to the ordered basis $U = [u_1,\dots,u_n]$ is the diagonal   matrix $\Lambda$ having the eigenvalues as diagonal entries: $$\Lambda = \text{diag}(\lambda_1,\dots,\lambda_n) $$ If $A$ is the matrix of $T$ relative to another basis $E = [e_1,\dots, e_n]$ then $$\Lambda = C^{-1}AC$$ where $C$ is the nonsingular matrix relating the two bases by the equation $U = EC$ . But something went wrong when I applied it: I took the matrix $A= \begin{bmatrix}1 & 2 \\5 & 4\end{bmatrix}$ and I tried to find the matrix $C$ in $\Lambda =C^{-1}AC$ : First, I found the eigenvalues which are $6$ and $-1$ , then I chose the eigenvectors $\begin{pmatrix}2  \\5 \end{pmatrix}$ and $\begin{pmatrix}1  \\-1 \end{pmatrix}$ which then give me the matrix $C = U =\begin{bmatrix}2 & 1 \\5 & -1\end{bmatrix}$ if I choose for the $E$ mentioned in the theorem above the basis composed by the unit coordinate vectors. Then $C^{-1} = \frac{1}{7} \begin{bmatrix}1 & 1 \\ 5 & -2\end{bmatrix}$ which indeed gives me $$\Lambda  = C^{-1} A C =  \begin{bmatrix}                               6 & 0 \\                               0 & -1                              \end{bmatrix}$$ which is the correct result. But then I tried to use a different basis instead of the one composed by the unit coordinate vectors, I chose $E = \begin{bmatrix}1 & 0 \\1 & 1\end{bmatrix}$ and I used for $U$ the same matrix as before as I think it should be done, so $U = \begin{bmatrix}2 & 1 \\5 & -1\end{bmatrix}$ , then, since $C = E^{-1}U$ and $C^{-1} = U^{-1}E$ , we have that $\Lambda = U^{-1}EAE^{-1}U$ . However, calculating it on Wolfram unfortunately doesn't return a diagonal matrix . Where did I make a mistake? Am I misapplying the theorem?","I read the theorem  on Apostol Calculus II : Theorem Let be a linear transformation, where has scalars in , and . Assume that the characteristic polynomial of has distinct roots in . Then we have: The corresponding eigenvectors form a basis for . The matrix of relative to the ordered basis is the diagonal   matrix having the eigenvalues as diagonal entries: If is the matrix of relative to another basis then where is the nonsingular matrix relating the two bases by the equation . But something went wrong when I applied it: I took the matrix and I tried to find the matrix in : First, I found the eigenvalues which are and , then I chose the eigenvectors and which then give me the matrix if I choose for the mentioned in the theorem above the basis composed by the unit coordinate vectors. Then which indeed gives me which is the correct result. But then I tried to use a different basis instead of the one composed by the unit coordinate vectors, I chose and I used for the same matrix as before as I think it should be done, so , then, since and , we have that . However, calculating it on Wolfram unfortunately doesn't return a diagonal matrix . Where did I make a mistake? Am I misapplying the theorem?","4.10 T : V \to V V F \dim V = n T n \lambda_1,\dots, \lambda_n F u_1,\dots,u_n V T U = [u_1,\dots,u_n] \Lambda \Lambda = \text{diag}(\lambda_1,\dots,\lambda_n)  A T E = [e_1,\dots, e_n] \Lambda = C^{-1}AC C U = EC A= \begin{bmatrix}1 & 2 \\5 & 4\end{bmatrix} C \Lambda =C^{-1}AC 6 -1 \begin{pmatrix}2  \\5 \end{pmatrix} \begin{pmatrix}1  \\-1 \end{pmatrix} C = U =\begin{bmatrix}2 & 1 \\5 & -1\end{bmatrix} E C^{-1} = \frac{1}{7} \begin{bmatrix}1 & 1 \\ 5 & -2\end{bmatrix} \Lambda  = C^{-1} A C =  \begin{bmatrix}
                              6 & 0 \\
                              0 & -1
                             \end{bmatrix} E = \begin{bmatrix}1 & 0 \\1 & 1\end{bmatrix} U U = \begin{bmatrix}2 & 1 \\5 & -1\end{bmatrix} C = E^{-1}U C^{-1} = U^{-1}E \Lambda = U^{-1}EAE^{-1}U","['linear-algebra', 'matrices', 'linear-transformations', 'diagonalization', 'change-of-basis']"
4,Showing that a certain complex matrix is positive definite,Showing that a certain complex matrix is positive definite,,"Consider the $n$ -variable complex function $f(z)=f(z_1,\dots,z_n)=\log (1+|z|^2)=\log (1+|z_1|^2+\cdots+|z_n|^2)$ . I am trying to show that the Hessian (the $n\times n$ matrix whose $(i,j)$ -entry is $\frac{\partial^2 f}{\partial z_i\partial \bar{z}_j}$ ) is complex positive definite. By calculation I've shown that the Hessian is given by $\frac{1}{(1+|z|^2)^2} (a_{ij})$ where $a_{ij}=-z_j\bar{z}_i$ if $i\neq j$ and $a_{ii}=1+\sum_{j\neq i}|z_j|^2$ . Clearly it suffices to show that the matrix $(a_{ij})$ is positive definite, but it seems that the calculation showing this directly is quite complicated. Any hints please?","Consider the -variable complex function . I am trying to show that the Hessian (the matrix whose -entry is ) is complex positive definite. By calculation I've shown that the Hessian is given by where if and . Clearly it suffices to show that the matrix is positive definite, but it seems that the calculation showing this directly is quite complicated. Any hints please?","n f(z)=f(z_1,\dots,z_n)=\log (1+|z|^2)=\log (1+|z_1|^2+\cdots+|z_n|^2) n\times n (i,j) \frac{\partial^2 f}{\partial z_i\partial \bar{z}_j} \frac{1}{(1+|z|^2)^2} (a_{ij}) a_{ij}=-z_j\bar{z}_i i\neq j a_{ii}=1+\sum_{j\neq i}|z_j|^2 (a_{ij})","['linear-algebra', 'matrices', 'complex-analysis', 'positive-definite', 'hessian-matrix']"
5,Projection onto the convex hull of the set of permuted values of a given vector,Projection onto the convex hull of the set of permuted values of a given vector,,"Let $v$ be a vector in $\mathbb{R}^n$ , such that $v_i \geq 0$ . I am interested in the convex hull of $\{v_{\sigma} : \sigma \in \Sigma_n\}$ where $\Sigma_n$ is the set of permutations of $\{1,...,n\}$ and $v_{\sigma} = (v_{\sigma(1)},... ,v_{\sigma(n)})$ . I know that the convex hull of permutation matrices is well-studied (since it is the Birkhoff polytope) and that projections onto it are possible via the Sinkhorn-Knopp algorithm. What about $C = \text{conv}\{v_{\sigma} : \sigma \in \Sigma_n\}$ ? We can write $C$ as $C=\{Qv : Q \text{ is doubly stochastic}\}$ , but are there simpler ways than existing algorithms to project onto $C$ (w.r.t. the Euclidean distance)? Thanks in advance!","Let be a vector in , such that . I am interested in the convex hull of where is the set of permutations of and . I know that the convex hull of permutation matrices is well-studied (since it is the Birkhoff polytope) and that projections onto it are possible via the Sinkhorn-Knopp algorithm. What about ? We can write as , but are there simpler ways than existing algorithms to project onto (w.r.t. the Euclidean distance)? Thanks in advance!","v \mathbb{R}^n v_i \geq 0 \{v_{\sigma} : \sigma \in \Sigma_n\} \Sigma_n \{1,...,n\} v_{\sigma} = (v_{\sigma(1)},... ,v_{\sigma(n)}) C = \text{conv}\{v_{\sigma} : \sigma \in \Sigma_n\} C C=\{Qv : Q \text{ is doubly stochastic}\} C","['matrices', 'permutations', 'convex-analysis', 'projection']"
6,A matrix with $a_{ij}\equiv \delta_{ij}\pmod p$,A matrix with,a_{ij}\equiv \delta_{ij}\pmod p,"Let $p$ be an odd prime number. Let $A\in M_{n\times n}(\mathbb{Z})$ be a matrix satisfiying $a_{ij}\equiv \delta_{ij}\pmod{p}$ . Prove that: if $|\det(A)|=1$ and $A^m=I$ for some $m\in\mathbb{N}^+$ , then we have $A=I$ . I tried to write $A$ as $pX+I$ where $X\in M_{n\times n}(\mathbb{Z})$ . Then there is an annihilation polynomial of $X$ by $(pX+I)^m=I$ , and the problem turns to be proving $X=O$ . But I don't know how to complete this proof.","Let be an odd prime number. Let be a matrix satisfiying . Prove that: if and for some , then we have . I tried to write as where . Then there is an annihilation polynomial of by , and the problem turns to be proving . But I don't know how to complete this proof.",p A\in M_{n\times n}(\mathbb{Z}) a_{ij}\equiv \delta_{ij}\pmod{p} |\det(A)|=1 A^m=I m\in\mathbb{N}^+ A=I A pX+I X\in M_{n\times n}(\mathbb{Z}) X (pX+I)^m=I X=O,"['linear-algebra', 'matrices']"
7,How many integer matrices have spectral radius bounded by a constant?,How many integer matrices have spectral radius bounded by a constant?,,"How many integer matrices with the spectral radius bounded by a fixed constant are there? Without any restrictions the answer is infinitely many. Indeed, $nJ_0$ , where $J_0$ is the Jordan cell with the eigenvalue $0$ and $n$ is any integer, all have spectral radius $0$ . On the other hand, for symmetric matrices the spectral radius is equal to the spectral norm so there are only finitely symmetric integer matrices with bounded spectral radius. What about other conditions that rule out Jordan cells and other nilpotents? For example, matrices with strictly positive entries, invertible or diagonalizable matrices. I suspect that there are infinitely many invertible or diagonalizable ones, but cannot think of any general construction. The motivation comes from trying to constructively generate large collections of invertible ""random"" integer matrices whose spectral radius stays between $1$ and $2$ (it cannot be less than $1$ ), so that their powers do not explode too quickly. This comes up in encryption. EDIT: For integer matrices with strictly positive entries this is answered in Does small Perron-Frobenius eigenvalue imply small entries for integral matrices? on MathOverflow There are finitely many because the sum of all entries is bounded by the square of the spectral radius.","How many integer matrices with the spectral radius bounded by a fixed constant are there? Without any restrictions the answer is infinitely many. Indeed, , where is the Jordan cell with the eigenvalue and is any integer, all have spectral radius . On the other hand, for symmetric matrices the spectral radius is equal to the spectral norm so there are only finitely symmetric integer matrices with bounded spectral radius. What about other conditions that rule out Jordan cells and other nilpotents? For example, matrices with strictly positive entries, invertible or diagonalizable matrices. I suspect that there are infinitely many invertible or diagonalizable ones, but cannot think of any general construction. The motivation comes from trying to constructively generate large collections of invertible ""random"" integer matrices whose spectral radius stays between and (it cannot be less than ), so that their powers do not explode too quickly. This comes up in encryption. EDIT: For integer matrices with strictly positive entries this is answered in Does small Perron-Frobenius eigenvalue imply small entries for integral matrices? on MathOverflow There are finitely many because the sum of all entries is bounded by the square of the spectral radius.",nJ_0 J_0 0 n 0 1 2 1,"['linear-algebra', 'matrices', 'spectral-radius']"
8,"Show that $(E_{i,j})_{i,j=1}^n$ is a positive matrix.",Show that  is a positive matrix.,"(E_{i,j})_{i,j=1}^n","Let $\{E_{i,j}\}_{i,j=1}^n$ be the matrix units of $M_n(\mathbb{C})$ . Consider the matrix $$A=(E_{i,j})_{i,j=1}^n \in M_n(M_n(\mathbb{C})) \cong M_{n^2}(\mathbb{C}).$$ I want to show that this matrix is positive (= self-adjoint and positive eigenvalues). Attempt : Let $\xi_1, \dots, \xi_n \in \mathbb{C}^n$ . I then  calculated $$\left\langle A \begin{pmatrix}\xi_1 \\ \vdots \\ \xi_n\end{pmatrix},\begin{pmatrix}\xi_1 \\ \vdots \\ \xi_n\end{pmatrix} \right\rangle= \sum_{k,l=1}^n (\xi_l)_l\overline{(\xi_k)_k}$$ but I don't see why this expression should be positive. Maybe my calculation is wrong?",Let be the matrix units of . Consider the matrix I want to show that this matrix is positive (= self-adjoint and positive eigenvalues). Attempt : Let . I then  calculated but I don't see why this expression should be positive. Maybe my calculation is wrong?,"\{E_{i,j}\}_{i,j=1}^n M_n(\mathbb{C}) A=(E_{i,j})_{i,j=1}^n \in M_n(M_n(\mathbb{C})) \cong M_{n^2}(\mathbb{C}). \xi_1, \dots, \xi_n \in \mathbb{C}^n \left\langle A \begin{pmatrix}\xi_1 \\ \vdots \\ \xi_n\end{pmatrix},\begin{pmatrix}\xi_1 \\ \vdots \\ \xi_n\end{pmatrix} \right\rangle= \sum_{k,l=1}^n (\xi_l)_l\overline{(\xi_k)_k}","['matrices', 'c-star-algebras']"
9,Determinant as the volume of a box in n-dimensions,Determinant as the volume of a box in n-dimensions,,"Below is a proof found in Gilbert Strang's book and here Why is the determinant the volume of a parallelepiped in any dimensions? that the determinant equals the volume of a box : To find the volume of a box whose edges are given by a  set of vectors $\{v_{1},\ldots,v_{n}\}$ we apply Gram-Schmidt orthogonalization to $\{v_{1},\ldots,v_{n}\}$ , so that \begin{eqnarray*} v_{1} & = & v_{1}\\ v_{2} & = & c_{12}v_{1}+v_{2}^{\perp}\\ v_{3} & = & c_{13}v_{1}+c_{23}v_{2}+v_{3}^{\perp}\\  & \vdots \end{eqnarray*} where $v_{2}^{\perp}$ is orthogonal to $v_{1}$ ; and $v_{3}^{\perp}$ is orthogonal to $span\left\{ v_{1},v_{2}\right\} $ , etc. Since determinant is multilinear, anti-symmetric, then \begin{eqnarray*} \det\left(v_{1},v_{2},v_{3},\ldots,v_{n}\right) & = & \det\left(v_{1},c_{12}v_{1}+v_{2}^{\perp},c_{13}v_{1}+c_{23}v_{2}+v_{3}^{\perp},\ldots\right)\\  & = & \det\left(v_{1},v_{2}^{\perp},v_{3}^{\perp},\ldots,v_{n}^{\perp}\right)\\  & =?& \mbox{signed volume}\left(v_{1},\ldots,v_{n}\right) \end{eqnarray*} Question : This only proves that the determinant of the original edges equals the volume of the new created box. That is we don't show $\det(v_1,...,v_n)$ = signed volume $(v_1,...v_n)$ . What we do prove is $\det(v_1,...,v_n)$ = Volume of  the box with orthogonal edges.","Below is a proof found in Gilbert Strang's book and here Why is the determinant the volume of a parallelepiped in any dimensions? that the determinant equals the volume of a box : To find the volume of a box whose edges are given by a  set of vectors we apply Gram-Schmidt orthogonalization to , so that where is orthogonal to ; and is orthogonal to , etc. Since determinant is multilinear, anti-symmetric, then Question : This only proves that the determinant of the original edges equals the volume of the new created box. That is we don't show = signed volume . What we do prove is = Volume of  the box with orthogonal edges.","\{v_{1},\ldots,v_{n}\} \{v_{1},\ldots,v_{n}\} \begin{eqnarray*}
v_{1} & = & v_{1}\\
v_{2} & = & c_{12}v_{1}+v_{2}^{\perp}\\
v_{3} & = & c_{13}v_{1}+c_{23}v_{2}+v_{3}^{\perp}\\
 & \vdots
\end{eqnarray*} v_{2}^{\perp} v_{1} v_{3}^{\perp} span\left\{ v_{1},v_{2}\right\}  \begin{eqnarray*}
\det\left(v_{1},v_{2},v_{3},\ldots,v_{n}\right) & = & \det\left(v_{1},c_{12}v_{1}+v_{2}^{\perp},c_{13}v_{1}+c_{23}v_{2}+v_{3}^{\perp},\ldots\right)\\
 & = & \det\left(v_{1},v_{2}^{\perp},v_{3}^{\perp},\ldots,v_{n}^{\perp}\right)\\
 & =?& \mbox{signed volume}\left(v_{1},\ldots,v_{n}\right)
\end{eqnarray*} \det(v_1,...,v_n) (v_1,...v_n) \det(v_1,...,v_n)","['linear-algebra', 'matrices', 'determinant', 'volume']"
10,Shorthand limits of matrices?,Shorthand limits of matrices?,,"Is there any relatively easy way to solve limits with matrices? $$ \lim_{h\rightarrow \infty} \begin{pmatrix} 1 & 0 & 0\\ \frac{1}{r} & 0 & \frac{-1}{r}\\ 0 & \frac{1}{hc} & 1 \end{pmatrix}^h = \begin{pmatrix} 1 & 0 & 0\\ \frac{1}{r} & 0 & \frac{-1}{r}\\ \frac{1}{rc} & 0 & 1-\frac{1}{rc} \end{pmatrix} $$ I have painstakingly identified the solution for this particular input matrix by identifying numbers with gigantic values of h in matlab. I would prefer to not square a matrix about a hundred times and manually identify the numbers. It's like calculating the derivative of a function with limits, I rather figure out the tricks than always going through the limit. The 4th power of the input matrix looks like this and I suppose you can make certain deductions, such as $\frac{1}{h}$ and $\frac{1}{h^2}=$ 0 and $\frac{3}{h}$ = 1. $$ \begin{pmatrix} 1 & 0 & 0\\ \frac{1}{r} & 0 & \frac{-1}{r}\\ 0 & \frac{1}{hc} & 1 \end{pmatrix}^4 = \begin{pmatrix} 1 & 0 & 0\\ \frac{1}{r} & \frac{-1}{rhc} & \frac{-1}{r}\\ \frac{1}{rhc} & \frac{1}{hc} & 1-\frac{1}{rhc}\\ \end{pmatrix}^2 = \begin{pmatrix} 1 & 0 & 0\\ \frac{1}{r}-\frac{1}{(rhc)^2}-\frac{1}{r^2hc} & \frac{1}{(rhc)^2}-\frac{1}{rhc} & \frac{-1}{r}+\frac{2}{r^2hc}\\ \frac{3}{rhc}-\frac{1}{(rhc)^2} & \frac{1}{hc}-\frac{2}{r(hc)^2} & 1-\frac{3}{rhc}+\frac{1}{(rhc)^2}\\ \end{pmatrix} $$ I would like to understand how to go from the input matrix straight to the solution , I can reach the solution if I take the 4th power of the input matrix and look for the number 3. But that will become very time consuming for big matrices, on the order of 100 x 100. Edit The $\frac{1}{rc}$ part in the previous limit result had to receive another limit so here's the solution to that. $$ \lim_{h\rightarrow \infty} \begin{pmatrix} 1 & 0 & 0\\ \frac{1}{r} & 0 & \frac{-1}{r}\\ \frac{1}{rhc} & 0 & 1-\frac{1}{rhc} \end{pmatrix}^h = \begin{pmatrix} 1 & 0 & 0\\ \frac{1}{r} & 0 & \frac{-1}{r}\\ 1-e^\frac{-1}{rc} & 0 & e^\frac{-1}{rc} \end{pmatrix} $$ Again, I found the solution by squaring many times and seeing error patterns and using wolframalpha to turn error sums to e functions. Maybe there is no simple way to find limits to matrices.","Is there any relatively easy way to solve limits with matrices? I have painstakingly identified the solution for this particular input matrix by identifying numbers with gigantic values of h in matlab. I would prefer to not square a matrix about a hundred times and manually identify the numbers. It's like calculating the derivative of a function with limits, I rather figure out the tricks than always going through the limit. The 4th power of the input matrix looks like this and I suppose you can make certain deductions, such as and 0 and = 1. I would like to understand how to go from the input matrix straight to the solution , I can reach the solution if I take the 4th power of the input matrix and look for the number 3. But that will become very time consuming for big matrices, on the order of 100 x 100. Edit The part in the previous limit result had to receive another limit so here's the solution to that. Again, I found the solution by squaring many times and seeing error patterns and using wolframalpha to turn error sums to e functions. Maybe there is no simple way to find limits to matrices.","
\lim_{h\rightarrow \infty}
\begin{pmatrix}
1 & 0 & 0\\
\frac{1}{r} & 0 & \frac{-1}{r}\\
0 & \frac{1}{hc} & 1
\end{pmatrix}^h
=
\begin{pmatrix}
1 & 0 & 0\\
\frac{1}{r} & 0 & \frac{-1}{r}\\
\frac{1}{rc} & 0 & 1-\frac{1}{rc}
\end{pmatrix}
 \frac{1}{h} \frac{1}{h^2}= \frac{3}{h} 
\begin{pmatrix}
1 & 0 & 0\\
\frac{1}{r} & 0 & \frac{-1}{r}\\
0 & \frac{1}{hc} & 1
\end{pmatrix}^4
=
\begin{pmatrix}
1 & 0 & 0\\
\frac{1}{r} & \frac{-1}{rhc} & \frac{-1}{r}\\
\frac{1}{rhc} & \frac{1}{hc} & 1-\frac{1}{rhc}\\
\end{pmatrix}^2
=
\begin{pmatrix}
1 & 0 & 0\\
\frac{1}{r}-\frac{1}{(rhc)^2}-\frac{1}{r^2hc} & \frac{1}{(rhc)^2}-\frac{1}{rhc} & \frac{-1}{r}+\frac{2}{r^2hc}\\
\frac{3}{rhc}-\frac{1}{(rhc)^2} & \frac{1}{hc}-\frac{2}{r(hc)^2} & 1-\frac{3}{rhc}+\frac{1}{(rhc)^2}\\
\end{pmatrix}
 \frac{1}{rc} 
\lim_{h\rightarrow \infty}
\begin{pmatrix}
1 & 0 & 0\\
\frac{1}{r} & 0 & \frac{-1}{r}\\
\frac{1}{rhc} & 0 & 1-\frac{1}{rhc}
\end{pmatrix}^h
=
\begin{pmatrix}
1 & 0 & 0\\
\frac{1}{r} & 0 & \frac{-1}{r}\\
1-e^\frac{-1}{rc} & 0 & e^\frac{-1}{rc}
\end{pmatrix}
","['matrices', 'limits']"
11,Find spectral norm via power iteration,Find spectral norm via power iteration,,"I have a question about this algorithm to find the spectral norm via power iteration from the paper Spectral Normalization for Generative Adversarial Networks . I don't know if I understood it correctly: $v$ approximates the dominant eigenvector of $W^\mathbf{\top}$ , and $u$ approximates the dominant eigenvector of $W W^\mathbf{\top}$ . The largest singular value of $W$ is $u^\mathbf{\top} Wv$ , but why is this the case? More specifically, I thought that $v$ should be the dominant eigenvector of $W^\mathbf{\top}W$ instead of $W^\mathbf{\top}$ , according to the SVD of $W$ (in other words, $v$ is the first right singular vector of the matrix $W=USV^\mathbf{\top}$ . Please let me know if I can revise the question for clarity. Thank you!","I have a question about this algorithm to find the spectral norm via power iteration from the paper Spectral Normalization for Generative Adversarial Networks . I don't know if I understood it correctly: approximates the dominant eigenvector of , and approximates the dominant eigenvector of . The largest singular value of is , but why is this the case? More specifically, I thought that should be the dominant eigenvector of instead of , according to the SVD of (in other words, is the first right singular vector of the matrix . Please let me know if I can revise the question for clarity. Thank you!",v W^\mathbf{\top} u W W^\mathbf{\top} W u^\mathbf{\top} Wv v W^\mathbf{\top}W W^\mathbf{\top} W v W=USV^\mathbf{\top},"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'svd', 'spectral-norm']"
12,Writing Back-Propogation out in terms of Matrix Multiplication,Writing Back-Propogation out in terms of Matrix Multiplication,,"I am trying to write my own backpropogation algorithm for neural networks for a class. For any specific weight of my network I could easily take the derivative, but for computational speed I want to write the derivatives out in terms of matrix expressions that could be simply coded and executed. Let us use the following notation: let $x\in\mathbb{R}^p$ be our set of $p$ inputs to our network. Let us have $m$ hidden layers of nodes each consisting of $n$ nodes. We will write the values at the $i^{th}$ hidden layer as $z_i$ , which are retieved from multipling the $i^{th}$ weight matrix $W_i$ by the previous layer a chosen function $f$ (sigmoid, RELU, etc). Finally, we consider only $1$ output node (since we could loop through the output nodes and do this for all of them, although it'd be nice to generalize). As such we can say that our network is given as $$ z_1 = f(W_1 x) $$ $$ z_2 = f(W_2 z_1) $$ $$ \vdots $$ $$ z_n = f(W_n z_{n-1}) $$ $$ \hat{y} = f(W_{n+1} z_n) $$ Using Mean Squared Error we get that our objective function for a given point is $$ MSE = (\hat{y} - y)^2 $$ The first update matrix (which is actually a vector) was fairly easy to find the update for. I found that $$ \frac{\partial(MSE)}{\partial W_{n+1}} = 2(\hat{y}-y)f'(W_{n+1}z_n) z_n = k_n z_n $$ where $k_n=2(\hat{y}-y)f'(W_{n+1}z_n)$ . From here the next layer is also special I found that if we let $$ \frac{\partial(MSE)}{\partial W_n} = k_n (W_{n+1}\circ f'(W_n z_{n-1}))\otimes z_{n-1} = k_{n-1}\otimes z_{n-1} $$ where $\circ$ represents the element wise multiplication and $\otimes$ represents the outer product. The final formula I get is the one that should (in theory) work for every layer following, however I am getting a code error on the final step that the matrix dimensions I have do not properly line up, which means I must have a problem with my formula. I find that if $$ k_i = (k_{i+1}^T W_i)\circ f'(W_i z_{i-1})  $$ then we can say that $$ \frac{\partial (MSE)}{\partial W_i} = k_i \otimes z_{i-1} $$ If anyone can help me find an error that'd be much appreicated!","I am trying to write my own backpropogation algorithm for neural networks for a class. For any specific weight of my network I could easily take the derivative, but for computational speed I want to write the derivatives out in terms of matrix expressions that could be simply coded and executed. Let us use the following notation: let be our set of inputs to our network. Let us have hidden layers of nodes each consisting of nodes. We will write the values at the hidden layer as , which are retieved from multipling the weight matrix by the previous layer a chosen function (sigmoid, RELU, etc). Finally, we consider only output node (since we could loop through the output nodes and do this for all of them, although it'd be nice to generalize). As such we can say that our network is given as Using Mean Squared Error we get that our objective function for a given point is The first update matrix (which is actually a vector) was fairly easy to find the update for. I found that where . From here the next layer is also special I found that if we let where represents the element wise multiplication and represents the outer product. The final formula I get is the one that should (in theory) work for every layer following, however I am getting a code error on the final step that the matrix dimensions I have do not properly line up, which means I must have a problem with my formula. I find that if then we can say that If anyone can help me find an error that'd be much appreicated!","x\in\mathbb{R}^p p m n i^{th} z_i i^{th} W_i f 1 
z_1 = f(W_1 x)
 
z_2 = f(W_2 z_1)
 
\vdots
 
z_n = f(W_n z_{n-1})
 
\hat{y} = f(W_{n+1} z_n)
 
MSE = (\hat{y} - y)^2
 
\frac{\partial(MSE)}{\partial W_{n+1}} = 2(\hat{y}-y)f'(W_{n+1}z_n) z_n = k_n z_n
 k_n=2(\hat{y}-y)f'(W_{n+1}z_n) 
\frac{\partial(MSE)}{\partial W_n} = k_n (W_{n+1}\circ f'(W_n z_{n-1}))\otimes z_{n-1} = k_{n-1}\otimes z_{n-1}
 \circ \otimes 
k_i = (k_{i+1}^T W_i)\circ f'(W_i z_{i-1}) 
 
\frac{\partial (MSE)}{\partial W_i} = k_i \otimes z_{i-1}
","['calculus', 'matrices', 'multivariable-calculus', 'matrix-calculus', 'machine-learning']"
13,Help finding Eigenvectors,Help finding Eigenvectors,,"The matrix is \begin{equation*} A =  \begin{pmatrix} 1 & 0 & 0 \\ 2 & 1 & -2 \\ 3 & 2 & 1 \end{pmatrix} \end{equation*} I got the eigenvalues $\lambda_1 = 1, \lambda_2 = 1 + 2i$ , and $\lambda_3 = 1-2i$ . I am only concerned with the complex valued eigenvectors. For $\lambda_2$ , I got the eigenvector \begin{equation*} v_2= \begin{pmatrix} 0 \\ i \\ 1 \end{pmatrix}\end{equation*} and for $\lambda_3$ , I got the eigenvector \begin{equation*}v_3= \begin{pmatrix} 0 \\ -i \\ 1 \end{pmatrix}\end{equation*} In the back of the book, it is saying the eigenvectors for $\lambda_2$ and $\lambda_3$ are \begin{equation*} v_2= \begin{pmatrix} 0 \\ 1 \\ -i \end{pmatrix}\end{equation*} and \begin{equation*}v_3= \begin{pmatrix} 0 \\ 1 \\ i \end{pmatrix}\end{equation*} When I checked on Wolfram Alpha, it is saying that my answers are correct. Did I do something wrong or is the back of my book wrong?","The matrix is I got the eigenvalues , and . I am only concerned with the complex valued eigenvectors. For , I got the eigenvector and for , I got the eigenvector In the back of the book, it is saying the eigenvectors for and are and When I checked on Wolfram Alpha, it is saying that my answers are correct. Did I do something wrong or is the back of my book wrong?","\begin{equation*}
A = 
\begin{pmatrix}
1 & 0 & 0 \\
2 & 1 & -2 \\
3 & 2 & 1
\end{pmatrix}
\end{equation*} \lambda_1 = 1, \lambda_2 = 1 + 2i \lambda_3 = 1-2i \lambda_2 \begin{equation*} v_2=
\begin{pmatrix}
0 \\
i \\
1
\end{pmatrix}\end{equation*} \lambda_3 \begin{equation*}v_3=
\begin{pmatrix}
0 \\
-i \\
1
\end{pmatrix}\end{equation*} \lambda_2 \lambda_3 \begin{equation*} v_2=
\begin{pmatrix}
0 \\
1 \\
-i
\end{pmatrix}\end{equation*} \begin{equation*}v_3=
\begin{pmatrix}
0 \\
1 \\
i
\end{pmatrix}\end{equation*}","['linear-algebra', 'matrices', 'ordinary-differential-equations', 'eigenvalues-eigenvectors', 'generalized-eigenvector']"
14,"A matrix equation $Ax=0$ has infinite solutions, does $A^Tx = 0$ have infinite solutions?","A matrix equation  has infinite solutions, does  have infinite solutions?",Ax=0 A^Tx = 0,"I'm wondering whether a system with a transpose of a matrix has the same type of solution that the original matrix system has. If an equation $Ax=0$ equation has a unique solution, would a system with $A$ transpose instead of $A$ also have a unique solution? And what about with no solution, and infinite solutions?","I'm wondering whether a system with a transpose of a matrix has the same type of solution that the original matrix system has. If an equation equation has a unique solution, would a system with transpose instead of also have a unique solution? And what about with no solution, and infinite solutions?",Ax=0 A A,"['linear-algebra', 'matrices', 'transpose']"
15,"If $A^{n-1}$ is not diagonalizable but $A^n$ is, show $A^n$ is zero","If  is not diagonalizable but  is, show  is zero",A^{n-1} A^n A^n,"Let $n$ be an integer greater than or equal to 2 and let $A$ be a $n^{th}$ order complex square matrix. Show that if $A^{n-1}$ is not diagonalizable and $A^n$ is diagonalizable, then $A^n=0$ . I have tried to think of a way to prove that but couldn't. I know that, If a matrix is diagonalizable over a field $F$ if and only if its minimal polynomial is a product of distinct linear factors over $F$ . Can we use this to prove it or what is the proper way to think about it?","Let be an integer greater than or equal to 2 and let be a order complex square matrix. Show that if is not diagonalizable and is diagonalizable, then . I have tried to think of a way to prove that but couldn't. I know that, If a matrix is diagonalizable over a field if and only if its minimal polynomial is a product of distinct linear factors over . Can we use this to prove it or what is the proper way to think about it?",n A n^{th} A^{n-1} A^n A^n=0 F F,"['linear-algebra', 'matrices', 'diagonalization']"
16,Proof of the theorem that there is no counterexample,Proof of the theorem that there is no counterexample,,"If $\rho(A) $ is a spectral radius and $ \|\cdot\|$ is norm, the relation $\rho(A) \leqslant \|A\| $ is always established. I want to prove that if $\rho(A) <1$ then $ \|A\|<1$ . Note that using the computer, I checked a large number of random matrices and there were no Counterexample. This is probably a mathematical theorem, but I can not prove it. please help. Thankful $$ $$ A friend found a counterexample for it so I change the condition to $0<\rho(A) <1$ . Can the theorem be proved now?","If is a spectral radius and is norm, the relation is always established. I want to prove that if then . Note that using the computer, I checked a large number of random matrices and there were no Counterexample. This is probably a mathematical theorem, but I can not prove it. please help. Thankful A friend found a counterexample for it so I change the condition to . Can the theorem be proved now?",\rho(A)   \|\cdot\| \rho(A) \leqslant \|A\|  \rho(A) <1  \|A\|<1   0<\rho(A) <1,"['matrices', 'examples-counterexamples']"
17,$O$ is not dense in $\mathbb R^n$,is not dense in,O \mathbb R^n,"$A$ is a matrix of order $n$ , and $x$ is an $n$ -dimensional column vector. The set $$ O=\{x,Ax,A^2x,\cdots,A^mx,\cdots\} $$ Prove that, $O$ is not dense in $\mathbb R^n$ . This claim seems to be trivial: it just looks like $\mathbb N$ in $\mathbb R$ . I recalled how we verify the denseness of $\mathbb Q$ in $\mathbb R$ , but it seeems that the same method doesn't work. Can anyone help?","is a matrix of order , and is an -dimensional column vector. The set Prove that, is not dense in . This claim seems to be trivial: it just looks like in . I recalled how we verify the denseness of in , but it seeems that the same method doesn't work. Can anyone help?","A n x n 
O=\{x,Ax,A^2x,\cdots,A^mx,\cdots\}
 O \mathbb R^n \mathbb N \mathbb R \mathbb Q \mathbb R","['linear-algebra', 'matrices', 'vector-spaces']"
18,"Given a particular matrix $A \in \operatorname{GL}_n(F)$, what is an easy way to determine $A^n$ $\forall n \in \Bbb Z$?","Given a particular matrix , what is an easy way to determine  ?",A \in \operatorname{GL}_n(F) A^n \forall n \in \Bbb Z,"$ \newcommand{\GL}{\operatorname{GL}} \newcommand{\R}{\mathbb{R}} \newcommand{\Z}{\mathbb{Z}} \newcommand{\m}[1]{\left( \begin{matrix} #1 \end{matrix} \right)} \newcommand{\ds}{\displaystyle} $ Original Problem: Consider the matrix $A \in \GL_3(\R)$ given by, for $x > 0$ , $$A = \m{-1 & x & -x \\ 1 & -1 & 0 \\ 1 & 0 & -1 }$$ What is the general form of $A^n$ for any $n \in \Z$ ? My Work So Far: By doing some particular cases by hand and doing some pattern matching, I was able to observe that $$A^n= (-1)^n \m{ 1 & -nx & nx \\ -n &\ds \frac{n(n-1)}{2} x + 1 &\ds  -\frac{n(n-1)}{2} x \\ -n & \ds \frac{n(n-1)}{2} x &\ds 1 - \frac{n(n-1)}{2} x }$$ Proving this through a sort of ""bidirectional induction"" (i.e. show it with $A$ as the base case for $n \in \Z_{>0}$ , then with $A^{-1}$ as the base case for $\Z_{<0}$ , and then $A^0 = I_3$ trivially) seems more than doable, if a little tedious. However, I don't think that this is the way it was intended to be solved. This comes as a homework problem and was tied to some others I was given. Namely: Find the characteristic polynomial $\mu_A$ of $A$ Deduce $A$ is not diagonalizable Find vectors $U_i \in \R^3$ such that $$AU_1 = -U_1 \qquad AU_2 = U_1 - U_2 \qquad AU_3 = U_1 + U_2 - U_3$$ Show $A$ is similar to the matrix $$B = \m{ -1 & 1 & 1 \\ 0 & -1 & 1 \\ 0 & 0 & -1}$$ (I did so by showing $\mu_A = \mu_B$ .) I've done all of these, so I don't really need help there. It would not be unusual for these to tie into the problem somehow, based on how this professor's homework tends to go, but I'm not sure where the connection lies. So I'm left wondering: based on this information, what is some more elegant way to calculate $A^n$ or prove its general form as given, if any? Be it for this specific matrix tied to the information given, or a more general case of $A \in \GL_n(F)$ .","Original Problem: Consider the matrix given by, for , What is the general form of for any ? My Work So Far: By doing some particular cases by hand and doing some pattern matching, I was able to observe that Proving this through a sort of ""bidirectional induction"" (i.e. show it with as the base case for , then with as the base case for , and then trivially) seems more than doable, if a little tedious. However, I don't think that this is the way it was intended to be solved. This comes as a homework problem and was tied to some others I was given. Namely: Find the characteristic polynomial of Deduce is not diagonalizable Find vectors such that Show is similar to the matrix (I did so by showing .) I've done all of these, so I don't really need help there. It would not be unusual for these to tie into the problem somehow, based on how this professor's homework tends to go, but I'm not sure where the connection lies. So I'm left wondering: based on this information, what is some more elegant way to calculate or prove its general form as given, if any? Be it for this specific matrix tied to the information given, or a more general case of .","
\newcommand{\GL}{\operatorname{GL}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\m}[1]{\left( \begin{matrix} #1 \end{matrix} \right)}
\newcommand{\ds}{\displaystyle}
 A \in \GL_3(\R) x > 0 A = \m{-1 & x & -x \\ 1 & -1 & 0 \\ 1 & 0 & -1 } A^n n \in \Z A^n= (-1)^n \m{ 1 & -nx & nx \\
-n &\ds \frac{n(n-1)}{2} x + 1 &\ds  -\frac{n(n-1)}{2} x \\
-n & \ds \frac{n(n-1)}{2} x &\ds 1 - \frac{n(n-1)}{2} x } A n \in \Z_{>0} A^{-1} \Z_{<0} A^0 = I_3 \mu_A A A U_i \in \R^3 AU_1 = -U_1 \qquad AU_2 = U_1 - U_2 \qquad AU_3 = U_1 + U_2 - U_3 A B = \m{ -1 & 1 & 1 \\ 0 & -1 & 1 \\ 0 & 0 & -1} \mu_A = \mu_B A^n A \in \GL_n(F)","['linear-algebra', 'matrices', 'exponentiation']"
19,Prove that if $A^{t}A$ is idempotent then $A^{t}=A^{+}$,Prove that if  is idempotent then,A^{t}A A^{t}=A^{+},"Let $A_{m\times{n}}$ prove that if $A^{t}A$ is idempotent, then $A^{t}=A^{+}$ . I already proved the reciprocal, but I am having some troubles with this one. I think that I have to prove that the 4 conditions of the Moore-Penrose inverse holds. I would like to know if my prove is correct. My attempt: Because $A^{t}A$ is idempotent, then $A^{t}AA^{t}A=A^{t}A$ $\hspace{1cm}$ associating we have $(A^{t}AA^{t})A=A^{t}A$ Then $A^{t}AA^{t}=A^{t}$ Is this correct? To prove that $AA^{t}A=A$ is analogous. And to prove that $AA^{t}$ and $A^{t}A$ is symmetric is a property already proved and I can use it. My question is: what property guarentees that if $(A^{t}AA^{t})A=A^{t}A$ then $A^{t}AA^{t}=A^{t}$ ?","Let prove that if is idempotent, then . I already proved the reciprocal, but I am having some troubles with this one. I think that I have to prove that the 4 conditions of the Moore-Penrose inverse holds. I would like to know if my prove is correct. My attempt: Because is idempotent, then associating we have Then Is this correct? To prove that is analogous. And to prove that and is symmetric is a property already proved and I can use it. My question is: what property guarentees that if then ?",A_{m\times{n}} A^{t}A A^{t}=A^{+} A^{t}A A^{t}AA^{t}A=A^{t}A \hspace{1cm} (A^{t}AA^{t})A=A^{t}A A^{t}AA^{t}=A^{t} AA^{t}A=A AA^{t} A^{t}A (A^{t}AA^{t})A=A^{t}A A^{t}AA^{t}=A^{t},"['matrices', 'matrix-decomposition', 'pseudoinverse']"
20,Chain rule for matrix derivatives,Chain rule for matrix derivatives,,"I have a function $f: \mathbb{R}^{n \times n} \to \mathbb{R}$ , where $\mathbb{R}^{n \times n}$ denotes the set of $n \times n $ real matrices. I have a closed form expression for $$ g(A) := \frac{\partial}{\partial A} f(A). $$ The goal is to calculate $\frac{\partial}{\partial B} f(C'BC)$ where $B$ and $C$ are $k \times k$ and $k \times n$ matrices, respectively (so that $C'BC$ is a $n \times n $ matrix as it should be). I am thinking it must equal $ C'g(C'BC)C $ but I want to make sure and get some reference for this chain rule. Thanks a lot for your help. [EDIT] It seems clear that my conjecture is wrong. Any help to get me on the correct path would be greatly appreciated.","I have a function , where denotes the set of real matrices. I have a closed form expression for The goal is to calculate where and are and matrices, respectively (so that is a matrix as it should be). I am thinking it must equal but I want to make sure and get some reference for this chain rule. Thanks a lot for your help. [EDIT] It seems clear that my conjecture is wrong. Any help to get me on the correct path would be greatly appreciated.","f: \mathbb{R}^{n \times n} \to \mathbb{R} \mathbb{R}^{n \times n} n \times n  
g(A) := \frac{\partial}{\partial A} f(A).
 \frac{\partial}{\partial B} f(C'BC) B C k \times k k \times n C'BC n \times n   C'g(C'BC)C ","['calculus', 'matrices', 'derivatives', 'matrix-calculus', 'chain-rule']"
21,Finding the basis from two subspaces,Finding the basis from two subspaces,,"This problem involves finding the basis of the union and intersection of two subspaces. We have $V$ and $U$ which are subspaces of $\mathbb R^4$ $$V = \begin{Bmatrix} {(x_1, x_2, x_3, x_4) : x_1 + x_2 = x_3 + x_4}\end{Bmatrix}$$ $$U = span \{ \begin{bmatrix}0\\0 \\1 \\1 \\ \end{bmatrix}, \begin{bmatrix}3\\0 \\1 \\1 \\ \end{bmatrix}, \begin{bmatrix}0\\-1 \\2 \\-1 \\ \end{bmatrix} , \begin{bmatrix} 0\\ 1 \\ 0 \\ 3 \\ \end{bmatrix} \} \\ $$ We want to find the dimension and basis for: $a)\text{ } U + V$ $b)\text{ } U \cap V$ My attempt: Let me first try to find the column space of U and a basis for V The $$rref(U) = \begin{bmatrix} 1 & 0 & 0 & 2\\ 0 & 1 & 0 & 0\\ 0 & 0 & 1 & -1\\ 0 & 0 & 0 & 0\\ \end{bmatrix}$$ Since only the first three columns have pivot elements, only the first three rows of $U$ make up the column space: $\begin{bmatrix}0\\0 \\1 \\1 \end{bmatrix}, \begin{bmatrix}3\\0 \\1 \\1 \end{bmatrix}, \begin{bmatrix}0\\-1 \\2 \\-1\end{bmatrix}$ Now let's find a basis for $V = \begin{Bmatrix} {(x_1, x_2, x_3, x_4) : x_1 + x_2 = x_3 + x_4}\end{Bmatrix}$ . I did this by just making vectors that satisfied the constraint until I couldn't anymore. If this is the wrong way to do so, please let me know! The vectors I found were: $\begin{bmatrix}1\\1 \\2 \\0 \end{bmatrix}, \begin{bmatrix}1\\1 \\1 \\1 \end{bmatrix}, \begin{bmatrix}1\\1 \\0 \\2\end{bmatrix}$ . So I know we want to find the the dimension and basis for $U + V$ . I think what I should do now is find the linearly independent vectors of all seven vectors above. The number of vectors is going to be the dimension and the basis is just the independent vectors. Correct? How should I go about doing the same for $U \cap V$ . I'm stumped by the ""intersect"" symbol. Do they just want me to list the basis of vectors that the two have in common?","This problem involves finding the basis of the union and intersection of two subspaces. We have and which are subspaces of We want to find the dimension and basis for: My attempt: Let me first try to find the column space of U and a basis for V The Since only the first three columns have pivot elements, only the first three rows of make up the column space: Now let's find a basis for . I did this by just making vectors that satisfied the constraint until I couldn't anymore. If this is the wrong way to do so, please let me know! The vectors I found were: . So I know we want to find the the dimension and basis for . I think what I should do now is find the linearly independent vectors of all seven vectors above. The number of vectors is going to be the dimension and the basis is just the independent vectors. Correct? How should I go about doing the same for . I'm stumped by the ""intersect"" symbol. Do they just want me to list the basis of vectors that the two have in common?","V U \mathbb R^4 V = \begin{Bmatrix} {(x_1, x_2, x_3, x_4) : x_1 + x_2 = x_3 + x_4}\end{Bmatrix} U = span \{ \begin{bmatrix}0\\0 \\1 \\1 \\
\end{bmatrix}, \begin{bmatrix}3\\0 \\1 \\1 \\
\end{bmatrix}, \begin{bmatrix}0\\-1 \\2 \\-1 \\
\end{bmatrix} , \begin{bmatrix}
0\\
1 \\
0 \\
3 \\
\end{bmatrix} \} \\  a)\text{ } U + V b)\text{ } U \cap V rref(U) = \begin{bmatrix}
1 & 0 & 0 & 2\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & -1\\
0 & 0 & 0 & 0\\
\end{bmatrix} U \begin{bmatrix}0\\0 \\1 \\1 \end{bmatrix}, \begin{bmatrix}3\\0 \\1 \\1 \end{bmatrix}, \begin{bmatrix}0\\-1 \\2 \\-1\end{bmatrix} V = \begin{Bmatrix} {(x_1, x_2, x_3, x_4) : x_1 + x_2 = x_3 + x_4}\end{Bmatrix} \begin{bmatrix}1\\1 \\2 \\0 \end{bmatrix}, \begin{bmatrix}1\\1 \\1 \\1 \end{bmatrix}, \begin{bmatrix}1\\1 \\0 \\2\end{bmatrix} U + V U \cap V","['linear-algebra', 'matrices', 'vector-spaces', 'self-learning']"
22,Determine a in the system such that the system is consistent.,Determine a in the system such that the system is consistent.,,"The system is: $x_1 + 2x_2 - x_3 = 2, $ $2x_1 - x_2 + x_3 = 1 $ , $-x_1 + 4x_2 -2x_3 = a $ To begin solving the system, I did Row 2 - 2(Row 1). Row 3 + Row 1, then Row 3 + Row 2. This left me with \begin{bmatrix}1&2&-1&2\\0&-5&3&-3\\0&0&0&a-1\end{bmatrix} If all I am trying to find is if the system is inconsistent, do I need to continue reducing it completely to reduced echelon form? Or is it that since I solved the row with a, I can stop there leaving the system as consistent when a = 1.","The system is: , To begin solving the system, I did Row 2 - 2(Row 1). Row 3 + Row 1, then Row 3 + Row 2. This left me with If all I am trying to find is if the system is inconsistent, do I need to continue reducing it completely to reduced echelon form? Or is it that since I solved the row with a, I can stop there leaving the system as consistent when a = 1.","x_1 + 2x_2 - x_3 = 2,  2x_1 - x_2 + x_3 = 1  -x_1 + 4x_2 -2x_3 = a  \begin{bmatrix}1&2&-1&2\\0&-5&3&-3\\0&0&0&a-1\end{bmatrix}","['linear-algebra', 'matrices', 'systems-of-equations']"
23,Is it true that the infinity norm of the matrix exponential $\|e^{At}\|_{l^\infty} \leq 1$ if $A$ is a negative diagonally dominant matrix?,Is it true that the infinity norm of the matrix exponential  if  is a negative diagonally dominant matrix?,\|e^{At}\|_{l^\infty} \leq 1 A,"Assume $A = (a_{ij}) \in \mathbb{R}^{N\times N}$ is negative diagonally dominant matrix, i.e. $|a_{ii}| \geq \sum_{j = 1, j\neq i}^{N} |a_{ij}|$ with $ a_{ii} < 0, 1 \leq i \leq N$ . For example: \begin{equation} A =  \left[ \begin{array}{ccccc} -2 & 1  &   &  & 1 \\ 1  & -2 & 1 &  & \\  & \cdots & \cdots & \cdots &  \\  &        &  1 & -2 & 1\\  1&     &   & 1 & -2 \\ \end{array} \right]_{N\times N} \end{equation} Is it true that the infinity norm of the matrix exponential $\|\mathrm{e}^{A t}\|_{l^\infty} \leq 1, \forall t \geq 0$ ? EDIT 1 The $l^\infty$ norm of a matrix $B  = (b_{ij}) \in \mathbb{R}^{N\times N}$ is given by \begin{align*} \|B\|_{l^\infty} = \mathop{max}_{i = 1, \cdots, N}\{\sum_{j = 1}^{N} |b_{ij}| \}. \end{align*} How to prove it? Lemma 3.1 of (Du Qiang, et al., 2019, MAXIMUM PRINCIPLE PRESERVING EXPONENTIAL TIME DIFFERENCING SCHEMES FOR THE NONLOCAL ALLEN-CAHN EQUATION)[https://arxiv.org/pdf/1902.04998.pdf] shows that (I think there is a typo in the stricit diagonally dominant condition, $j \neq i$ is missing) When $\kappa = 0$ is it still true that $\|\mathrm{e}^{A t}\|_{l^\infty} \leq 1$ ? EDIT 2 I wrote a short matlab code to verify the inequality N = 10; for i = 1:100000     A0 = 2*rand(N, N) - 1; % random value in [-1, 1]     A = A0 + A0'; % construct symmetric matrix;     v = -(sum(abs(A), 2) - abs(diag(A)));     for i = 1:N         A(i,i) = v(i); % Assign v to the diagonal elements     end     tmp = norm(expm(A), inf);     if  tmp > 1         tmp     end end Thank you very much!","Assume is negative diagonally dominant matrix, i.e. with . For example: Is it true that the infinity norm of the matrix exponential ? EDIT 1 The norm of a matrix is given by How to prove it? Lemma 3.1 of (Du Qiang, et al., 2019, MAXIMUM PRINCIPLE PRESERVING EXPONENTIAL TIME DIFFERENCING SCHEMES FOR THE NONLOCAL ALLEN-CAHN EQUATION)[https://arxiv.org/pdf/1902.04998.pdf] shows that (I think there is a typo in the stricit diagonally dominant condition, is missing) When is it still true that ? EDIT 2 I wrote a short matlab code to verify the inequality N = 10; for i = 1:100000     A0 = 2*rand(N, N) - 1; % random value in [-1, 1]     A = A0 + A0'; % construct symmetric matrix;     v = -(sum(abs(A), 2) - abs(diag(A)));     for i = 1:N         A(i,i) = v(i); % Assign v to the diagonal elements     end     tmp = norm(expm(A), inf);     if  tmp > 1         tmp     end end Thank you very much!","A = (a_{ij}) \in \mathbb{R}^{N\times N} |a_{ii}| \geq \sum_{j = 1, j\neq i}^{N} |a_{ij}|  a_{ii} < 0, 1 \leq i \leq N \begin{equation}
A = 
\left[
\begin{array}{ccccc}
-2 & 1  &   &  & 1 \\
1  & -2 & 1 &  & \\
 & \cdots & \cdots & \cdots &  \\
 &        &  1 & -2 & 1\\
 1&     &   & 1 & -2 \\
\end{array}
\right]_{N\times N}
\end{equation} \|\mathrm{e}^{A t}\|_{l^\infty} \leq 1, \forall t \geq 0 l^\infty B  = (b_{ij}) \in \mathbb{R}^{N\times N} \begin{align*}
\|B\|_{l^\infty} = \mathop{max}_{i = 1, \cdots, N}\{\sum_{j = 1}^{N} |b_{ij}| \}.
\end{align*} j \neq i \kappa = 0 \|\mathrm{e}^{A t}\|_{l^\infty} \leq 1","['matrices', 'normed-spaces', 'numerical-linear-algebra', 'positive-semidefinite', 'matrix-exponential']"
24,"Let $P$ be a $3\times 3$ matrix such that all the entries are from $\{1, 0, 1\}$. What is the maximum possible value of the determinant of $P$?",Let  be a  matrix such that all the entries are from . What is the maximum possible value of the determinant of ?,"P 3\times 3 \{1, 0, 1\} P","Let $P$ be a matrix of order $ 3 \times 3$ such that all the entries in $P$ are from the set $\{1, 0, 1\}$ . What is the maximum possible value of the determinant of $P$ ? The original question This question is from JEE ADVANCED 2018, paper 2 My Attempt: $det(P)$ = $$     \begin{vmatrix}     a_1 & a_2 & a_3 \\     b_1 & b_2 & b_3 \\     c_1 & c_2 & c_3 \\     \end{vmatrix}$$ $$a_1(b_2c_3  b_3c_2) a_2(b_1c_3  b_3c_1) + a_3(b_1c_2  b_2c_1)\le6$$ value can be 6 only if $a_1 = 1, a_2 = \ 1, a_3 = 1 ,  b_2c_3 = b_1c_3 = b_1c_2 = 1 , b_3c_2 = b_3c_1 = b_2c_1 = \  1$ and $(b_2c_3) (b_3c_1) (b_1c_2) = \  1$ and $ (b_1c_3)(b_3c_2) (b_2c_1) = 1$ i.e $b_1b_2b_3c_1c_2c_3 = 1$ and $ 1$ hence not possible Similar contradiction occurs when $ a_1 = 1, a_2 = \ 1, a_3 = 1, b_2c_2 = b_3c_1 = b_1c_2 = 1, b_3c_2 = b_3c_1 = b_2c_1 =\  1$ Now for value to be $5$ one the terms must be zero but that will make $2$ terms zero which means answer cannot be $5$ Now, $$     \begin{vmatrix}     1 & 0 & 1 \\     -1 & 1 & 1 \\     -1 & -1 & -1 \\      \end{vmatrix} =4$$ Hence max value = $4$ I need the best way to solve this question(the process through which this question could be done in the least time possible and it should not be similar to my  approach (i.e any approach other than substitution and checking)","Let be a matrix of order such that all the entries in are from the set . What is the maximum possible value of the determinant of ? The original question This question is from JEE ADVANCED 2018, paper 2 My Attempt: = value can be 6 only if and and i.e and hence not possible Similar contradiction occurs when Now for value to be one the terms must be zero but that will make terms zero which means answer cannot be Now, Hence max value = I need the best way to solve this question(the process through which this question could be done in the least time possible and it should not be similar to my  approach (i.e any approach other than substitution and checking)","P  3 \times 3 P \{1, 0, 1\} P det(P) 
    \begin{vmatrix}
    a_1 & a_2 & a_3 \\
    b_1 & b_2 & b_3 \\
    c_1 & c_2 & c_3 \\
    \end{vmatrix} a_1(b_2c_3  b_3c_2) a_2(b_1c_3  b_3c_1) + a_3(b_1c_2  b_2c_1)\le6 a_1 = 1, a_2 = \ 1, a_3 = 1 ,  b_2c_3 = b_1c_3 = b_1c_2 = 1 , b_3c_2 = b_3c_1 = b_2c_1 = \  1 (b_2c_3) (b_3c_1) (b_1c_2) = \  1  (b_1c_3)(b_3c_2) (b_2c_1) = 1 b_1b_2b_3c_1c_2c_3 = 1  1  a_1 = 1, a_2 = \ 1, a_3 = 1, b_2c_2 = b_3c_1 = b_1c_2 = 1, b_3c_2 = b_3c_1 = b_2c_1 =\  1 5 2 5 
    \begin{vmatrix}
    1 & 0 & 1 \\
    -1 & 1 & 1 \\
    -1 & -1 & -1 \\ 
    \end{vmatrix}
=4 4","['linear-algebra', 'matrices', 'linear-programming', 'maxima-minima', 'discrete-optimization']"
25,Upper bound on condition number of row-normalized matrices,Upper bound on condition number of row-normalized matrices,,"I would like to study the condition number of a non-square normalized matrices as function of the original non row-normalized matrix. Let $X \in \mathbb{R}^{a \times b}$ (for $a > b$ ). We obtain $\hat X$ by taking all the rows of $X$ and normalizing them such that the $\ell_2$ norms of each row is $1$ . We can further assume that all the rows of $X$ are $1 \leq \|x_i| \leq \alpha$ Question I believe (and I would like to prove) that: $$\kappa(\hat X) \leq \kappa(X).$$ For me, the condition number of a matrix $X$ is defined as the ratio between the biggest and the smallest singular value of $X$ , i.e.: $$ \kappa(X) = \frac{\sigma_1}{\sigma_k} $$ where $k$ is the rank of the matrix (which can be smaller than $b$ ) I think a property of matrix norm might come handy: its submultiplicativity: $$ \|AB\| \leq \|A\|\|B\|$$ from which it possible to derive the property that $$ \kappa(AB) \leq \kappa(A)\kappa(B)$$ (this is true only in some casis, see referenced questions at the bottom) We recall that the norm of a matrix can be defined from a ( $\ell_p$ which in our case is $\ell_2$ ) $$ \| A \| = \max_{x \neq 0, } \frac{\|Ax\|}{\|x\|}  = \max_{\|x\|=1} \|Ax\| = \sigma_1$$ My attempt: In general, it is easy to see that $\sigma_1(\hat X) < \sigma_1(X)$ , while I cannot prove that $\sigma_{min}(\hat X) > \sigma_{min}(S)$ for the smallest singular values. This is how I approached the proof: Let me recall you that the condition number, which usually for square matrices is defined as $\kappa(X) = \|X\| \|X^{-1}\|$ for the case of non-square matrices can be better defined as the ratio between the biggest and the smallest singular value. In other words: $\kappa(X)= \|X\|\|X^+\|$ (where $X^+$ is the Moore-Penrose pseudoinverse of $X$ , i.e. the matrix obtained by taking the inverse $1/\sigma_i$ of the singular values of $\sigma_i$ of $X$ ). We can think of $X$ as the product of $\hat X$ where I left multiply by $N_X \in \mathbb{R}^{n \times n}$ , a diagonal matrix where the entry in position $ii$ is just $\|x_i\|$ , i.e. the norm of row $i$ . $$ X = N_X \hat X.$$ I thought that I can express the condition number as product of the norm. Unfortunately, this direction seems to lead me astray, as the inequality is in the wrong direction. So: $\kappa(X) = \|N_X\hat  X\| \|(N_X \hat X)^{+}\|=\|N_X\hat  X\| \|\hat  X^{+}N_X^{+} \| \leq \|N_X\| \|\hat  X\| \|\hat  X^{+}\|\|N_X^{+} \| $ and also $\kappa(\hat X) = \|\hat X\| \|\hat X^{-1}\|$ . Also note that $\kappa(N_X) = \kappa(N_X^{-1}) \leq \alpha$ , because of our assumption on the value of the norms of the rows of $X$ . This is equivalent of asking if these two conditions are satisfied: $\|\hat X\| \|\hat X^{+}\| \leq \|N_X\hat  X\| \|(N_X \hat X)^{+}\| $ $\| \hat X^+ \| \leq \|(N\hat X)^+ \| $ It is simple to observe that: $\|\hat X\| \leq \|N\hat X\|$ . This is because, by using the definition of norm of a matrix, $$\forall y \text{ s.t. } \|y\|=1 \text{ we have that } \|\hat Xy\| \leq  \|N \hat Xy\|$$ because each element on the diagonal is bigger than 1. We need to see if $\|\hat X^{+}\| \leq \|(N\hat X)^+\| = \|(\hat X^{+}N^{+})\|$ I am looking here for some monotonicty properties of matrix norms, or property that can be derived from the inverse of a matrix. Am I going in the right direction? Thanks. Reply to comment What if we start from $N^{-1}_XX = \hat X$ ? Then, I would obtain $\kappa(\hat X) = \kappa(N^{-1}_X X) \leq \kappa(N^{-1}_X)\kappa(X) \leq \alpha \kappa(X) $ This does not seems helpful, because we get to the point where $$\kappa(\hat X) \leq \alpha \kappa(X)$$ and, from the previous observation, $$ \kappa(X) \leq \alpha \kappa(\hat X) $$ Experiments I checked that this property is satisfied in two cases: if we have a diagonal matrix $X$ with some random scalar in it, then the normalized version is just the identity matrix, whose condition number is 1. for random matrices (random in sense of numpy.random.rand() ) it holds true that $\kappa(\hat X) \leq \kappa(X)$ Related questions There are numerous questions around the condition number of product of matrices: Condition number of a product of two matrices Counter example or proof that $\kappa(AB) \leq \kappa(A)\kappa(B) $ In the last question they show a counterexample for $\kappa(AB)\leq\kappa(A)\kappa(B) $ which apparently does not hold for non-square matrices.","I would like to study the condition number of a non-square normalized matrices as function of the original non row-normalized matrix. Let (for ). We obtain by taking all the rows of and normalizing them such that the norms of each row is . We can further assume that all the rows of are Question I believe (and I would like to prove) that: For me, the condition number of a matrix is defined as the ratio between the biggest and the smallest singular value of , i.e.: where is the rank of the matrix (which can be smaller than ) I think a property of matrix norm might come handy: its submultiplicativity: from which it possible to derive the property that (this is true only in some casis, see referenced questions at the bottom) We recall that the norm of a matrix can be defined from a ( which in our case is ) My attempt: In general, it is easy to see that , while I cannot prove that for the smallest singular values. This is how I approached the proof: Let me recall you that the condition number, which usually for square matrices is defined as for the case of non-square matrices can be better defined as the ratio between the biggest and the smallest singular value. In other words: (where is the Moore-Penrose pseudoinverse of , i.e. the matrix obtained by taking the inverse of the singular values of of ). We can think of as the product of where I left multiply by , a diagonal matrix where the entry in position is just , i.e. the norm of row . I thought that I can express the condition number as product of the norm. Unfortunately, this direction seems to lead me astray, as the inequality is in the wrong direction. So: and also . Also note that , because of our assumption on the value of the norms of the rows of . This is equivalent of asking if these two conditions are satisfied: It is simple to observe that: . This is because, by using the definition of norm of a matrix, because each element on the diagonal is bigger than 1. We need to see if I am looking here for some monotonicty properties of matrix norms, or property that can be derived from the inverse of a matrix. Am I going in the right direction? Thanks. Reply to comment What if we start from ? Then, I would obtain This does not seems helpful, because we get to the point where and, from the previous observation, Experiments I checked that this property is satisfied in two cases: if we have a diagonal matrix with some random scalar in it, then the normalized version is just the identity matrix, whose condition number is 1. for random matrices (random in sense of numpy.random.rand() ) it holds true that Related questions There are numerous questions around the condition number of product of matrices: Condition number of a product of two matrices Counter example or proof that $\kappa(AB) \leq \kappa(A)\kappa(B) $ In the last question they show a counterexample for which apparently does not hold for non-square matrices.","X \in \mathbb{R}^{a \times b} a > b \hat X X \ell_2 1 X 1 \leq \|x_i| \leq \alpha \kappa(\hat X) \leq \kappa(X). X X  \kappa(X) = \frac{\sigma_1}{\sigma_k}  k b  \|AB\| \leq \|A\|\|B\|  \kappa(AB) \leq \kappa(A)\kappa(B) \ell_p \ell_2  \| A \| = \max_{x \neq 0, } \frac{\|Ax\|}{\|x\|}  = \max_{\|x\|=1} \|Ax\| = \sigma_1 \sigma_1(\hat X) < \sigma_1(X) \sigma_{min}(\hat X) > \sigma_{min}(S) \kappa(X) = \|X\| \|X^{-1}\| \kappa(X)= \|X\|\|X^+\| X^+ X 1/\sigma_i \sigma_i X X \hat X N_X \in \mathbb{R}^{n \times n} ii \|x_i\| i  X = N_X \hat X. \kappa(X) = \|N_X\hat  X\| \|(N_X \hat X)^{+}\|=\|N_X\hat  X\| \|\hat  X^{+}N_X^{+} \| \leq \|N_X\| \|\hat  X\| \|\hat  X^{+}\|\|N_X^{+} \|  \kappa(\hat X) = \|\hat X\| \|\hat X^{-1}\| \kappa(N_X) = \kappa(N_X^{-1}) \leq \alpha X \|\hat X\| \|\hat X^{+}\| \leq \|N_X\hat  X\| \|(N_X \hat X)^{+}\|  \| \hat X^+ \| \leq \|(N\hat X)^+ \|  \|\hat X\| \leq \|N\hat X\| \forall y \text{ s.t. } \|y\|=1 \text{ we have that } \|\hat Xy\| \leq  \|N \hat Xy\| \|\hat X^{+}\| \leq \|(N\hat X)^+\| = \|(\hat X^{+}N^{+})\| N^{-1}_XX = \hat X \kappa(\hat X) = \kappa(N^{-1}_X X) \leq \kappa(N^{-1}_X)\kappa(X) \leq \alpha \kappa(X)  \kappa(\hat X) \leq \alpha \kappa(X)  \kappa(X) \leq \alpha \kappa(\hat X)  X \kappa(\hat X) \leq \kappa(X) \kappa(AB)\leq\kappa(A)\kappa(B) ","['linear-algebra', 'matrices', 'condition-number']"
26,"Good analogies between types in programming and structures, spaces and fields in mathematics?","Good analogies between types in programming and structures, spaces and fields in mathematics?",,"I am a programmer, and so my knowledge of math is limited to bare-bones calculus, linear algebra etc. Basically I know how to do the basic stuff, but never learned how to look at it in a more abstract sense. I am trying to see if I can leverage my understanding of concepts within programming to get a better intuition about concepts in mathematics. I am well aware that there is no 1-to-1 mapping. So I am not looking for precise analogies but very rough analogies to help me build some intuition and make my journey into math-world a bit easier. In programming , we operate with hierarchies of types, some which are abstract and others which are concrete. Concrete types can be instantiated to create objects. We can do thing with objects based on the type they belong to. So if I have an abstract type A, which concrete subtypes B and C, then instances of B and C will have some shared abilities because they share an abstract type A. I am trying to map this thinking into my math reading. In programming , there is often a base type for everything called e.g. Object or Any . It seems you have the same in math. Everything is a mathematical object . In many programming languages , types are also objects themselves. There seems to be an analogy in mathematics. mathematical structures look like types to me. As a programmer , I would say these have mathematical objects as instances. However , the mathematical structure is itself also an object, just like a type can be an object in programming. And like programming there is a type hierarchy. If I understand correctly an algebraic structure is kind of like a subtype of refinement of the mathematical structure concept. When I look at things like fields and spaces , this reminds of parameterized types in programming. E.g. Vector2D{Integer} or Vector2D<Int> depending on the language you use are 2D vectors parameterized on the Integer type, meaning their x and y coordinates are integers, rather than say floating point. As far as I understand , you express vector spaces in a similar fashion. You say a vector space over some field. The field seem a bit analogous to a type parameter to me. The field would be the natural numbers, real numbers of whatever. The field defines the components making up the  individual vectors in the vector space as well as the numbers which you may multiply a vector with e.g. Again I know these kinds of analogies are flawed, but if we were to attempt to create analogies, is there some truth to what I am guessing here or do I have a profound misunderstanding of these concepts?","I am a programmer, and so my knowledge of math is limited to bare-bones calculus, linear algebra etc. Basically I know how to do the basic stuff, but never learned how to look at it in a more abstract sense. I am trying to see if I can leverage my understanding of concepts within programming to get a better intuition about concepts in mathematics. I am well aware that there is no 1-to-1 mapping. So I am not looking for precise analogies but very rough analogies to help me build some intuition and make my journey into math-world a bit easier. In programming , we operate with hierarchies of types, some which are abstract and others which are concrete. Concrete types can be instantiated to create objects. We can do thing with objects based on the type they belong to. So if I have an abstract type A, which concrete subtypes B and C, then instances of B and C will have some shared abilities because they share an abstract type A. I am trying to map this thinking into my math reading. In programming , there is often a base type for everything called e.g. Object or Any . It seems you have the same in math. Everything is a mathematical object . In many programming languages , types are also objects themselves. There seems to be an analogy in mathematics. mathematical structures look like types to me. As a programmer , I would say these have mathematical objects as instances. However , the mathematical structure is itself also an object, just like a type can be an object in programming. And like programming there is a type hierarchy. If I understand correctly an algebraic structure is kind of like a subtype of refinement of the mathematical structure concept. When I look at things like fields and spaces , this reminds of parameterized types in programming. E.g. Vector2D{Integer} or Vector2D<Int> depending on the language you use are 2D vectors parameterized on the Integer type, meaning their x and y coordinates are integers, rather than say floating point. As far as I understand , you express vector spaces in a similar fashion. You say a vector space over some field. The field seem a bit analogous to a type parameter to me. The field would be the natural numbers, real numbers of whatever. The field defines the components making up the  individual vectors in the vector space as well as the numbers which you may multiply a vector with e.g. Again I know these kinds of analogies are flawed, but if we were to attempt to create analogies, is there some truth to what I am guessing here or do I have a profound misunderstanding of these concepts?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'soft-question', 'computer-science']"
27,Equality in trace duality,Equality in trace duality,,"For $A,B\in\mathbb{R}^{n\times m}$ we have the trace duality property $$|\langle A, B \rangle|\leq \|A\|_1 \|B\|_{\infty}$$ where $\|A\|_p$ is the Schatten $p$ -norm (i.e. $\|\cdot \|_1$ is the nuclear norm equal to the sum of singular values, and $\|\cdot\|_{\infty}$ is the operator norm equal to the largest singular value) and the inner product is $\langle A, B \rangle = \text{tr}(A^{\top}B)$ . There are at least two methods to prove this inequality. One is using the Fischer-Courant min-max principle (see for example this question ), and the other is by the aid of symmetric gauge functions (see Chapter 4 of Matrix Analysis (1997) form Bathia). None of these proofs establish sufficient (or necessary) conditions to get an equality. Do anyone know a way to get equality?","For we have the trace duality property where is the Schatten -norm (i.e. is the nuclear norm equal to the sum of singular values, and is the operator norm equal to the largest singular value) and the inner product is . There are at least two methods to prove this inequality. One is using the Fischer-Courant min-max principle (see for example this question ), and the other is by the aid of symmetric gauge functions (see Chapter 4 of Matrix Analysis (1997) form Bathia). None of these proofs establish sufficient (or necessary) conditions to get an equality. Do anyone know a way to get equality?","A,B\in\mathbb{R}^{n\times m} |\langle A, B \rangle|\leq \|A\|_1 \|B\|_{\infty} \|A\|_p p \|\cdot \|_1 \|\cdot\|_{\infty} \langle A, B \rangle = \text{tr}(A^{\top}B)","['real-analysis', 'linear-algebra', 'matrices', 'duality-theorems', 'matrix-analysis']"
28,Gaussian integral over all possible real matrices,Gaussian integral over all possible real matrices,,"I am trying to compute the following gaussian integral over all possible real matrices $J$ : $$I=\int \exp\left\{-\frac{N}{2}\text{Tr}\left[\mathbf{J}\mathbf{A}\;\mathbf{J}^T+2\mathbf{BJ}-\gamma \mathbf{JJ} \right]\right\}\mathrm{d}\mathbf{J}$$ Where $\mathbf{A}$ and $\mathbf{B}$ are Hermitian matrices. When $\gamma=0$ I can complete the square and integrate this Gaussian integral without any problem (assuming I know the eigenvalues and determinant of $\mathbf{A}$ ): $$\mathbf{J}\mathbf{A}\;\mathbf{J}^T+2\mathbf{BJ}=\left(\mathbf{J}^T-\mathbf{B}\mathbf{A}^{-1}\right)\mathbf{A}\left(\mathbf{J}-\mathbf{A}^{-1}\mathbf{B}\right)-\mathbf{B}\mathbf{A}^{-1}\mathbf{B}$$ However for general $\gamma\in \mathbb{R}$ I cannot seem to know how to evaluate this integral by completing the square: $\mathbf{J}^T\mathbf{A}\;\mathbf{J}+2\mathbf{BJ}-\gamma \mathbf{JJ}$ $\mathbf{J}$ is real but not symmetric. when $\gamma=0$ this integral converges so I do not see any reason why it would not be generalised to general $\gamma$ with an appropriate $\mathbf{A}$ . Any remark or advice is always appreciated. Thank you. Edit : A different way to express the integral $I$ is the following: $$I=\int \left(\prod_{ij}\mathrm{d}J_{ij}\right)\exp\left\{-\frac{N}{2} \sum_{i, j, k} J_{k i} A_{i j} J_{k j}+N\sum_{k, j} B_{k j} J_{k j}+\frac{N\gamma}{2}\sum_{ij}J_{ij}J_{ji}\right\}$$ Assuming I already know the eigenvalues of $\mathbf{A}$ and thus $\det(\mathbf{A})$ , how can I compute the integral $I$ ?","I am trying to compute the following gaussian integral over all possible real matrices : Where and are Hermitian matrices. When I can complete the square and integrate this Gaussian integral without any problem (assuming I know the eigenvalues and determinant of ): However for general I cannot seem to know how to evaluate this integral by completing the square: is real but not symmetric. when this integral converges so I do not see any reason why it would not be generalised to general with an appropriate . Any remark or advice is always appreciated. Thank you. Edit : A different way to express the integral is the following: Assuming I already know the eigenvalues of and thus , how can I compute the integral ?","J I=\int \exp\left\{-\frac{N}{2}\text{Tr}\left[\mathbf{J}\mathbf{A}\;\mathbf{J}^T+2\mathbf{BJ}-\gamma \mathbf{JJ} \right]\right\}\mathrm{d}\mathbf{J} \mathbf{A} \mathbf{B} \gamma=0 \mathbf{A} \mathbf{J}\mathbf{A}\;\mathbf{J}^T+2\mathbf{BJ}=\left(\mathbf{J}^T-\mathbf{B}\mathbf{A}^{-1}\right)\mathbf{A}\left(\mathbf{J}-\mathbf{A}^{-1}\mathbf{B}\right)-\mathbf{B}\mathbf{A}^{-1}\mathbf{B} \gamma\in \mathbb{R} \mathbf{J}^T\mathbf{A}\;\mathbf{J}+2\mathbf{BJ}-\gamma \mathbf{JJ} \mathbf{J} \gamma=0 \gamma \mathbf{A} I I=\int \left(\prod_{ij}\mathrm{d}J_{ij}\right)\exp\left\{-\frac{N}{2} \sum_{i, j, k} J_{k i} A_{i j} J_{k j}+N\sum_{k, j} B_{k j} J_{k j}+\frac{N\gamma}{2}\sum_{ij}J_{ij}J_{ji}\right\} \mathbf{A} \det(\mathbf{A}) I","['matrices', 'gaussian-integral']"
29,Multivariate Application of Slutsky's lemma,Multivariate Application of Slutsky's lemma,,"Let $(A_n)_{n \geq 1}$ be a sequence of random matrices in $\mathbb{R}^{d \times d}$ and $(T_n)_{n \geq 1}$ and $(Z_n)_{n \geq 1}$ be  two sequences of random vectors in $\mathbb{R}^{d}$ . We assume that: $\forall n \geq 1$ , $Z_n = A_n T_n$ $A_n$ converges in probability towards a deterministic non singular random matrice $A$ $Z_n$ converges in distribution towards a random vector $Z$ . Show that $T_n$ converges in distribution towards $A^{-1}Z$ . We do not assume that the $A_n$ are non-singular. If the $A_n$ were non-singular it would be easy to say $T_n = A_n^{-1}A_nT_n$ and apply Slutsky's lemma to obtain the result. Without assuming non-singularity but if we could assume $A_n$ nonnegative I would use $T_n = (A_n+ \frac{1}{n})^{-1}(A_n+ \frac{1}{n})T_n$ to show the result. In the general case, I do not know how to prove it. Any hint ?","Let be a sequence of random matrices in and and be  two sequences of random vectors in . We assume that: , converges in probability towards a deterministic non singular random matrice converges in distribution towards a random vector . Show that converges in distribution towards . We do not assume that the are non-singular. If the were non-singular it would be easy to say and apply Slutsky's lemma to obtain the result. Without assuming non-singularity but if we could assume nonnegative I would use to show the result. In the general case, I do not know how to prove it. Any hint ?",(A_n)_{n \geq 1} \mathbb{R}^{d \times d} (T_n)_{n \geq 1} (Z_n)_{n \geq 1} \mathbb{R}^{d} \forall n \geq 1 Z_n = A_n T_n A_n A Z_n Z T_n A^{-1}Z A_n A_n T_n = A_n^{-1}A_nT_n A_n T_n = (A_n+ \frac{1}{n})^{-1}(A_n+ \frac{1}{n})T_n,"['linear-algebra', 'matrices', 'probability-theory', 'weak-convergence']"
30,Can we find / express all matrices so that ${\bf P}^2 = {\bf P+I}$?,Can we find / express all matrices so that ?,{\bf P}^2 = {\bf P+I},"Can we find/express all matrices, so that: $${\bf P}^2={\bf P+I}$$ Own work: For the eigenvalues must then hold: $$\lambda^2-\lambda-1=0$$ In other words : $$\lambda_1 = \frac{1-\sqrt{5}}2\\\lambda_2 = \frac{1+\sqrt{5}}2$$ So as long as we construct a matrix with any combination of such eigenvalues, we shall be fine. $${\bf P =SDS}^{-1},\\  {\bf D} = \text{diag}(\text{AnyCombinationOf}\{\lambda_1,\lambda_2\})$$ for any matrix $\bf S$ . For my experiments this does seem to give numerically reasonable results for lots of such generated $\bf P$ of sizes $2-6$ . These numerical experiments do however not disprove that such matrices $\bf P$ could exist which do not fit this description. Does this description or representation catch all possible such matrices, or do we miss some? How to prove?","Can we find/express all matrices, so that: Own work: For the eigenvalues must then hold: In other words : So as long as we construct a matrix with any combination of such eigenvalues, we shall be fine. for any matrix . For my experiments this does seem to give numerically reasonable results for lots of such generated of sizes . These numerical experiments do however not disprove that such matrices could exist which do not fit this description. Does this description or representation catch all possible such matrices, or do we miss some? How to prove?","{\bf P}^2={\bf P+I} \lambda^2-\lambda-1=0 \lambda_1 = \frac{1-\sqrt{5}}2\\\lambda_2 = \frac{1+\sqrt{5}}2 {\bf P =SDS}^{-1},\\
 {\bf D} = \text{diag}(\text{AnyCombinationOf}\{\lambda_1,\lambda_2\}) \bf S \bf P 2-6 \bf P","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'minimal-polynomials', 'cayley-hamilton']"
31,Gershgorin circle theorem and similarity transformations,Gershgorin circle theorem and similarity transformations,,"Consider the following problem, that was part of an old exam I am studying for: Let $$ A = \begin{pmatrix} 4 & 0 & 2\\  -2 & 8 & 2\\ 0 & 2 & -4 \end{pmatrix}$$ Using the Gershgorin circle theorem, show that $A$ has exactly one eigenvalue with a negative real part. Find three disjunct circles, such that each contains exactly one eigenvalue. Give an approximation for the biggest eigenvalue that is as close as possible. Hint: For 2. and 3., consider $\hat{A} = D^{-1}AD$ with $D = diag(1,c,1),\: c>0$ . For 3., choose $c$ to yield an optimal   approximation. Now, we dealt with the Gershgorin circle theorem in class, and I can apply it well in the first part of the problem. My issue is with the second and last parts - I have never seen the use of similarity transforms in combination with the theorem. I only know that such transformations keep the eigenvalues intact, and that in some cases, it can be used to extract more information from the Gershgorin circles. My question is though: How do I select such a $c$ to retrieve an optimal approximation? What is the strategy here? What I have tried so far: I calculated the Matrix $D^{-1}AD$ parameterized by $c$ : $$D^{-1}AD = \begin{pmatrix}4 & 0 & 2 \\ -\frac{2}{c} & 8 & \frac{2}{c}  \\ 0 & 2c & -4\end{pmatrix}$$ The three resulting Gershgorin circles are therefore: $C_1 = (4, 2)$ $C_2 = (8, \frac{4}{c})$ $C_3 = (-4, 2c)$ $C_3$ immediately solves the first part of the problem, since for $c=1$ it's the only circle that is on the negative side of the real axis. For the second part, I need to find a $c$ such that $C_1$ and $C_2$ dont overlap anymore (which they do for $A$ ), which means that: $$\frac{4}{c} < 2$$ This is easily the case for $c > 2$ , for example $c = 2.1$ . But how do I pick an optimal $c$ for the last part of the problem? The circle with the biggest ""magnitude"" $C_2 = (8, \frac{4}{c})$ just gets smaller in radius for bigger $c$ .","Consider the following problem, that was part of an old exam I am studying for: Let Using the Gershgorin circle theorem, show that has exactly one eigenvalue with a negative real part. Find three disjunct circles, such that each contains exactly one eigenvalue. Give an approximation for the biggest eigenvalue that is as close as possible. Hint: For 2. and 3., consider with . For 3., choose to yield an optimal   approximation. Now, we dealt with the Gershgorin circle theorem in class, and I can apply it well in the first part of the problem. My issue is with the second and last parts - I have never seen the use of similarity transforms in combination with the theorem. I only know that such transformations keep the eigenvalues intact, and that in some cases, it can be used to extract more information from the Gershgorin circles. My question is though: How do I select such a to retrieve an optimal approximation? What is the strategy here? What I have tried so far: I calculated the Matrix parameterized by : The three resulting Gershgorin circles are therefore: immediately solves the first part of the problem, since for it's the only circle that is on the negative side of the real axis. For the second part, I need to find a such that and dont overlap anymore (which they do for ), which means that: This is easily the case for , for example . But how do I pick an optimal for the last part of the problem? The circle with the biggest ""magnitude"" just gets smaller in radius for bigger ."," A = \begin{pmatrix} 4 & 0 & 2\\
 -2 & 8 & 2\\ 0 & 2 & -4 \end{pmatrix} A \hat{A} = D^{-1}AD D = diag(1,c,1),\: c>0 c c D^{-1}AD c D^{-1}AD = \begin{pmatrix}4 & 0 & 2 \\ -\frac{2}{c} & 8 & \frac{2}{c}  \\ 0 & 2c & -4\end{pmatrix} C_1 = (4, 2) C_2 = (8, \frac{4}{c}) C_3 = (-4, 2c) C_3 c=1 c C_1 C_2 A \frac{4}{c} < 2 c > 2 c = 2.1 c C_2 = (8, \frac{4}{c}) c","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'gershgorin-sets', 'similar-matrices']"
32,Eigendecomposing real matrices without knowing complex numbers,Eigendecomposing real matrices without knowing complex numbers,,"Let $A$ be a real squared matrix. A scalar $\lambda\in\mathbb C$ is an eigenvalue for $A$ iff $$Av=\lambda v \tag A$$ for some complex vector $v$ . This condition can be equivalently written in terms of purely real quantities as the following system: $$\begin{cases}   (A-\lambda_1 I)v_1 = - \lambda_2 v_2, \\   (A-\lambda_1 I)v_2 = \phantom{-}\lambda_2 v_1, \end{cases} \tag B$$ as can be seen by decomposing the quantities in (A) into real and imaginary parts: $\lambda=\lambda_1+i\lambda_2$ and $v=v_1+i v_2$ . If we didn't know anything about complex numbers, we would be working directly on (B), asking for a pair of reals $\lambda_1,\lambda_2\in\mathbb R$ such that (B) is satisfied for some real vectors $v_1,v_2$ . This pair of conditions can be seen to imply to the following ones: $$\begin{cases}   [(A-\lambda_1 I)^2 + \lambda_2^2 I ]v_1 = 0, \\   [(A-\lambda_1 I)^2 + \lambda_2^2 I ]v_2 = 0. \end{cases} \tag C$$ This follows from applying $(A-\lambda_1 I)$ twice to either $v_1$ or $v_2$ , and using (B). This, on the other hand, is equivalent to the condition $$\det[(A-\lambda_1 I)^2 + \lambda_2^2 I] = 0. \tag D$$ See also this post about the equivalence of (A) and (D). From complex analysis we know that, given an arbitrary real matrix $A$ , there must be a pair of reals $\lambda_1,\lambda_2$ such that (D) is verified. Not knowing what complex numbers are, how would we go in finding such values for a given $A$ ? The determinant equation gives a polynomial of two variables which I'm not sure how to handle.","Let be a real squared matrix. A scalar is an eigenvalue for iff for some complex vector . This condition can be equivalently written in terms of purely real quantities as the following system: as can be seen by decomposing the quantities in (A) into real and imaginary parts: and . If we didn't know anything about complex numbers, we would be working directly on (B), asking for a pair of reals such that (B) is satisfied for some real vectors . This pair of conditions can be seen to imply to the following ones: This follows from applying twice to either or , and using (B). This, on the other hand, is equivalent to the condition See also this post about the equivalence of (A) and (D). From complex analysis we know that, given an arbitrary real matrix , there must be a pair of reals such that (D) is verified. Not knowing what complex numbers are, how would we go in finding such values for a given ? The determinant equation gives a polynomial of two variables which I'm not sure how to handle.","A \lambda\in\mathbb C A Av=\lambda v \tag A v \begin{cases}
  (A-\lambda_1 I)v_1 = - \lambda_2 v_2, \\
  (A-\lambda_1 I)v_2 = \phantom{-}\lambda_2 v_1,
\end{cases} \tag B \lambda=\lambda_1+i\lambda_2 v=v_1+i v_2 \lambda_1,\lambda_2\in\mathbb R v_1,v_2 \begin{cases}
  [(A-\lambda_1 I)^2 + \lambda_2^2 I ]v_1 = 0, \\
  [(A-\lambda_1 I)^2 + \lambda_2^2 I ]v_2 = 0.
\end{cases} \tag C (A-\lambda_1 I) v_1 v_2 \det[(A-\lambda_1 I)^2 + \lambda_2^2 I] = 0. \tag D A \lambda_1,\lambda_2 A","['linear-algebra', 'matrices', 'complex-analysis', 'eigenvalues-eigenvectors']"
33,How to find characteristic polynomial of $B$ in terms of $A$?,How to find characteristic polynomial of  in terms of ?,B A,"$$B := \begin{pmatrix} A+nI && -E\\ -E^T&& nI\end{pmatrix}$$ where $E$ is the all-ones matrix. If the eigenvalues of $n \times n$ matrix $A$ are known, is it possible to find the eigenvalues of $B$ ? I find the characteristic polynomial of $B$ to be $$\begin{pmatrix} xI-(A+nI) && -E\\ -E^T&& (x-n)I\end{pmatrix}$$ I found that $$\det(xI-B)= \det(x-n)I\times \{(xI-(A+nI)-E(x-n)^{-1}E^T\}$$ but cant find the characteristic polynomial of $B$ in terms of $A$ . Can I get some help here?","where is the all-ones matrix. If the eigenvalues of matrix are known, is it possible to find the eigenvalues of ? I find the characteristic polynomial of to be I found that but cant find the characteristic polynomial of in terms of . Can I get some help here?",B := \begin{pmatrix} A+nI && -E\\ -E^T&& nI\end{pmatrix} E n \times n A B B \begin{pmatrix} xI-(A+nI) && -E\\ -E^T&& (x-n)I\end{pmatrix} \det(xI-B)= \det(x-n)I\times \{(xI-(A+nI)-E(x-n)^{-1}E^T\} B A,"['matrices', 'eigenvalues-eigenvectors', 'determinant', 'block-matrices']"
34,Prove that $A$ and $B$ are nilpotent - proof checking,Prove that  and  are nilpotent - proof checking,A B,"Let $A$ and $B$ be $n \times n$ matrices with real entries and $c_1, c_2, \dots ,c_{n+1}$ distinct real numbers such that $A+c_1B, A+c_2B \dots, A+c_{n+1}B$ are nilpotent matrices. Prove that $A$ and $B$ are nilpotent. Is my proof alright? This is my approach: For $k$ chosen arbitrarily from $\{1,2, \dots, n\}$ define the polynomial : $ P_k(x) = \operatorname{tr}[ (A+xB)^k]$ . Suppose that $P_k$ is not constant taking the value $0$ . $A+c_iB \text{ is nilpotent} \iff tr[ (A+c_iB)^k ] = 0 \forall k \in \mathbb{N}^* \text{ and } \forall i \in \{1,2,\dots, n+1\} \Rightarrow P_k(c_1) = P_k(c_2) = \cdots = P_k(c_{n+1}) = 0 \Rightarrow$ $$\operatorname{deg} P_k \geq n+1.$$ However the elements of $(A+xB)^k$ are polynomials in $x$ of degree at most $k$ . So $$ \operatorname{deg} P_k \leq k. $$ But from these two statements $k \geq n+1$ which is a contradiction because we chose $ k \leq n$ . So $P_k(x) = 0 \forall x \in \mathbb{R} $ . That means that all of its coefficients are $0$ . But, because $(A+xB)^k = x^k B^k + \dots + A^k$ , the leading coefficient of $P_k$ is $\operatorname{tr} (B^k)$ and the constant term is $\operatorname{tr} (A^k) \Rightarrow \operatorname{tr} (A^k) = \operatorname{tr} (B^k) = 0 \forall k \in \mathbb\{1,2 \dots n\} \Rightarrow $ $$ A \text{ and } B \text{ are nilpotent} $$","Let and be matrices with real entries and distinct real numbers such that are nilpotent matrices. Prove that and are nilpotent. Is my proof alright? This is my approach: For chosen arbitrarily from define the polynomial : . Suppose that is not constant taking the value . However the elements of are polynomials in of degree at most . So But from these two statements which is a contradiction because we chose . So . That means that all of its coefficients are . But, because , the leading coefficient of is and the constant term is","A B n \times n c_1, c_2, \dots ,c_{n+1} A+c_1B, A+c_2B \dots, A+c_{n+1}B A B k \{1,2, \dots, n\}  P_k(x) = \operatorname{tr}[ (A+xB)^k] P_k 0 A+c_iB \text{ is nilpotent} \iff tr[ (A+c_iB)^k ] = 0 \forall k \in \mathbb{N}^* \text{ and } \forall i \in \{1,2,\dots, n+1\} \Rightarrow P_k(c_1) = P_k(c_2) = \cdots = P_k(c_{n+1}) = 0 \Rightarrow \operatorname{deg} P_k \geq n+1. (A+xB)^k x k  \operatorname{deg} P_k \leq k.  k \geq n+1  k \leq n P_k(x) = 0 \forall x \in \mathbb{R}  0 (A+xB)^k = x^k B^k + \dots + A^k P_k \operatorname{tr} (B^k) \operatorname{tr} (A^k) \Rightarrow \operatorname{tr} (A^k) = \operatorname{tr} (B^k) = 0 \forall k \in \mathbb\{1,2 \dots n\} \Rightarrow   A \text{ and } B \text{ are nilpotent} ","['linear-algebra', 'matrices', 'solution-verification', 'nilpotence']"
35,How to prove this formula for the determinant of a $4 \times 4$ tridiagonal matrix?,How to prove this formula for the determinant of a  tridiagonal matrix?,4 \times 4,"This following is a problem from B. S. Grewal's Higher Engineering Mathematics . Show $$\begin{vmatrix} 2\cos(\theta) & 1 & 0 & 0 \\ 1 & 2 \cos(\theta) & 1  & 0 \\ 0 & 1 & 2 \cos(\theta) & 1 \\ 0 & 0 & 1 & 2 \cos(\theta)  \end{vmatrix} = \frac{\sin(5\theta)}{\sin(\theta)}.$$ If I take the $3 \times 3$ matrix after deleting the first row and first column, the value is $\frac{\sin(4\theta)}{\sin(\theta)}$ , but I am unable to solve this $4\times 4$ matrix. I tried solving the RHS but I am still unable to solve.","This following is a problem from B. S. Grewal's Higher Engineering Mathematics . Show If I take the matrix after deleting the first row and first column, the value is , but I am unable to solve this matrix. I tried solving the RHS but I am still unable to solve.","\begin{vmatrix} 2\cos(\theta) & 1 & 0 & 0 \\ 1 & 2 \cos(\theta) & 1
 & 0 \\ 0 & 1 & 2 \cos(\theta) & 1 \\ 0 & 0 & 1 & 2 \cos(\theta)
 \end{vmatrix} = \frac{\sin(5\theta)}{\sin(\theta)}. 3 \times 3 \frac{\sin(4\theta)}{\sin(\theta)} 4\times 4","['linear-algebra', 'matrices', 'trigonometry', 'determinant', 'tridiagonal-matrices']"
36,Subgroups of $GL_n$ containing upper triangular matrices,Subgroups of  containing upper triangular matrices,GL_n,"EDIT: I rephrased the claim for clarity. Let $k$ be a field (that we may assume to be algebraically closed, but I don't think it is necessary). Let $n\geq 1$ and $T$ denote the subgroup of $GL_n$ consisting of invertible upper triangular matrices. Let $1\leq r \leq n$ and consider a sequence $a_1,\ldots,a_r$ of positive numbers such that $a_1+\ldots+a_r=n$ . Consider the subgroup $P_{(a_1,\ldots,a_r)}$ of $GL_n$ consisting exactly of all matrices $M$ of the form $$M =  \begin{bmatrix}     M_1 & * & \dots  & * \\     0 & M_2 & \ddots  & \vdots \\     \vdots & \ddots & \ddots & * \\     0 & \dots & 0  & M_r \end{bmatrix}$$ with $M_i\in GL_{a_i}(k)$ for all $i$ , and the $*$ 's being any elements of $k$ (or rather, any matrices with coefficients in $k$ and of appropriate dimensions). In other words, $P_{(a_1,\ldots,a_r)}$ consists of all invertible upper-triangular by blocks matrices with diagonal blocks being squares of dimensions $a_1,\ldots,a_r$ . The claim I am considering is the following: Any subgroup $P$ of $GL_n$ containing $T$ must have the form $P=P_{(a_1,\ldots,a_r)}$ for some $r$ and $a_1,\ldots,a_r$ . I suspect that this result may be true, however I can't find a way to prove it. In particular, given a group $P$ containing $T$ , I have trouble seeing how I could characterize $r$ and the (ordered !) sequence $a_1,\ldots,a_r$ solely in terms of $P$ . The motivation behind this lies in the theory of algebraic groups. We know that $T$ is a connected closed solvable subgroup of $GL_n$ . With the above result, I could deduce that $T$ is maximal with respect to such properties, because all subgroups described above with $r<n$ are unsolvable.","EDIT: I rephrased the claim for clarity. Let be a field (that we may assume to be algebraically closed, but I don't think it is necessary). Let and denote the subgroup of consisting of invertible upper triangular matrices. Let and consider a sequence of positive numbers such that . Consider the subgroup of consisting exactly of all matrices of the form with for all , and the 's being any elements of (or rather, any matrices with coefficients in and of appropriate dimensions). In other words, consists of all invertible upper-triangular by blocks matrices with diagonal blocks being squares of dimensions . The claim I am considering is the following: Any subgroup of containing must have the form for some and . I suspect that this result may be true, however I can't find a way to prove it. In particular, given a group containing , I have trouble seeing how I could characterize and the (ordered !) sequence solely in terms of . The motivation behind this lies in the theory of algebraic groups. We know that is a connected closed solvable subgroup of . With the above result, I could deduce that is maximal with respect to such properties, because all subgroups described above with are unsolvable.","k n\geq 1 T GL_n 1\leq r \leq n a_1,\ldots,a_r a_1+\ldots+a_r=n P_{(a_1,\ldots,a_r)} GL_n M M = 
\begin{bmatrix}
    M_1 & * & \dots  & * \\
    0 & M_2 & \ddots  & \vdots \\
    \vdots & \ddots & \ddots & * \\
    0 & \dots & 0  & M_r
\end{bmatrix} M_i\in GL_{a_i}(k) i * k k P_{(a_1,\ldots,a_r)} a_1,\ldots,a_r P GL_n T P=P_{(a_1,\ldots,a_r)} r a_1,\ldots,a_r P T r a_1,\ldots,a_r P T GL_n T r<n","['matrices', 'group-theory', 'algebraic-groups', 'block-matrices', 'linear-groups']"
37,Questions on $A^mBA^n=I$ and $\small B=\left[\begin{smallmatrix}1&-1&3&1\\1&1&2&1\\2&-1&3&2\\-1&-2&1&2\end{smallmatrix}\right]$,Questions on  and,A^mBA^n=I \small B=\left[\begin{smallmatrix}1&-1&3&1\\1&1&2&1\\2&-1&3&2\\-1&-2&1&2\end{smallmatrix}\right],"Let $m,n\in \mathbb N$ and $A,B\in M_n(\mathbb R)$ so that: $$A^mBA^n=I$$ $$B=\begin{bmatrix} \;1&-1&\;3&\;1\\\;1&\;1&\;2&\;1\\\;2&-1&\;3&\;2\\-1&-2&\;1&\;2\end{bmatrix}$$ $(a)$ Is $A$ regular? $(b)$ Calculate $A^{m+n}$ $(a)$ $A$ is regular because $\det {(A^mBA^n)}=\det A\cdot\det {(A^{m-1}BA^n)}\ne0$ $(b)$ $A^{m+n}=A^mIA^n=A^m(B\frac{1}{B})A^n=\frac{1}{B}$ I inverted $B$ blockwise . As on Wikipedia (in notation here I used $B$ for a block-matrix, not to mix itwith the initial matrix): $${\begin{bmatrix} A&B\\C&D\end{bmatrix}}^{-1}=\begin{bmatrix} A^{-1}+A^{-1}B{(D-CA^{-1}B)}^{-1}CA^{-1} &- A^{-1}B{(D-CA^{-1}B)}^{-1}\\{(D-CA^{-1}B)}^{-1}CA^{-1} & {(D-CA^{-1}B)}^{-1}\end{bmatrix}$$ For the sake of simplicity, I transformed B into upper-triangular matrix: $$B=\begin{bmatrix} \;1&-1&\;3&\;1\\\;1&\;1&\;2&\;1\\\;2&-1&\;3&\;2\\-1&-2&\;1&\;2\end{bmatrix}\rightarrow\begin{bmatrix} \;1&-1&\;3&\;1\\\;0&\;2&-1&\;0\\0&\;1&-3&\;0\\\;0&-3&\;4&\;3\end{bmatrix}\rightarrow\begin{bmatrix} \;1&-1&\;3&\;1\\\;0&\;0&\;5&\;0\\\;0&\;1&-3&\;0\\\;0&\;0&-5&\;3\end{bmatrix}\rightarrow\begin{bmatrix} \;1&-1&\;3&\;1\\\;0&\;1&-3&\;0\\\;0&\;0&5&\;0\\\;0&\;0&-5&\;3\end{bmatrix}\rightarrow\begin{bmatrix} \;1&-1&\;3&\;1\\\;0&\;1&-3&\;0\\\;0&\;0&\;5&\;0\\\;0&\;0&\;0&\;3\end{bmatrix}\rightarrow\begin{bmatrix} \;1&\;0&\;0&\;1\\\;0&\;1&-3&\;0\\\;0&\;0&\;5&\;0\\\;0&\;0&\;0&\;3\end{bmatrix}$$ /edited: here I could've get $I_4$ , but I would like to go through different ways/ $$A=I, B=\begin{bmatrix}\;0&\;1\\-3&\;0\end{bmatrix}, C=0_2,D=\begin{bmatrix}\;5&\;0\\\;0&\;3\end{bmatrix}$$ Then, the Schur complement of A : $$D-CA^{-1}B=D$$ I got $$D^{-1}=\begin{bmatrix}\frac{1}{5}&\;0\\\;0&\frac{1}{3}\end{bmatrix}$$ Then $$-BD^{-1}=\begin{bmatrix}\;0&-\frac{1}{3}\\\frac{3}{5}&\;0\end{bmatrix}$$ Finally: $$B^{-1}=\begin{bmatrix}\;1&\;0&\;0&\frac{1}{3}\\\;0&\;1&-\frac{3}{5}&0\\\;0&\;0&\frac{1}{5}&\;0\\\;0&\;0&\;0&\frac{1}{3}\end{bmatrix}$$ This appears to be just a little different than $B$ transformed. Is this correct?","Let and so that: Is regular? Calculate is regular because I inverted blockwise . As on Wikipedia (in notation here I used for a block-matrix, not to mix itwith the initial matrix): For the sake of simplicity, I transformed B into upper-triangular matrix: /edited: here I could've get , but I would like to go through different ways/ Then, the Schur complement of A : I got Then Finally: This appears to be just a little different than transformed. Is this correct?","m,n\in \mathbb N A,B\in M_n(\mathbb R) A^mBA^n=I B=\begin{bmatrix} \;1&-1&\;3&\;1\\\;1&\;1&\;2&\;1\\\;2&-1&\;3&\;2\\-1&-2&\;1&\;2\end{bmatrix} (a) A (b) A^{m+n} (a) A \det {(A^mBA^n)}=\det A\cdot\det {(A^{m-1}BA^n)}\ne0 (b) A^{m+n}=A^mIA^n=A^m(B\frac{1}{B})A^n=\frac{1}{B} B B {\begin{bmatrix} A&B\\C&D\end{bmatrix}}^{-1}=\begin{bmatrix} A^{-1}+A^{-1}B{(D-CA^{-1}B)}^{-1}CA^{-1} &- A^{-1}B{(D-CA^{-1}B)}^{-1}\\{(D-CA^{-1}B)}^{-1}CA^{-1} & {(D-CA^{-1}B)}^{-1}\end{bmatrix} B=\begin{bmatrix} \;1&-1&\;3&\;1\\\;1&\;1&\;2&\;1\\\;2&-1&\;3&\;2\\-1&-2&\;1&\;2\end{bmatrix}\rightarrow\begin{bmatrix} \;1&-1&\;3&\;1\\\;0&\;2&-1&\;0\\0&\;1&-3&\;0\\\;0&-3&\;4&\;3\end{bmatrix}\rightarrow\begin{bmatrix} \;1&-1&\;3&\;1\\\;0&\;0&\;5&\;0\\\;0&\;1&-3&\;0\\\;0&\;0&-5&\;3\end{bmatrix}\rightarrow\begin{bmatrix} \;1&-1&\;3&\;1\\\;0&\;1&-3&\;0\\\;0&\;0&5&\;0\\\;0&\;0&-5&\;3\end{bmatrix}\rightarrow\begin{bmatrix} \;1&-1&\;3&\;1\\\;0&\;1&-3&\;0\\\;0&\;0&\;5&\;0\\\;0&\;0&\;0&\;3\end{bmatrix}\rightarrow\begin{bmatrix} \;1&\;0&\;0&\;1\\\;0&\;1&-3&\;0\\\;0&\;0&\;5&\;0\\\;0&\;0&\;0&\;3\end{bmatrix} I_4 A=I, B=\begin{bmatrix}\;0&\;1\\-3&\;0\end{bmatrix}, C=0_2,D=\begin{bmatrix}\;5&\;0\\\;0&\;3\end{bmatrix} D-CA^{-1}B=D D^{-1}=\begin{bmatrix}\frac{1}{5}&\;0\\\;0&\frac{1}{3}\end{bmatrix} -BD^{-1}=\begin{bmatrix}\;0&-\frac{1}{3}\\\frac{3}{5}&\;0\end{bmatrix} B^{-1}=\begin{bmatrix}\;1&\;0&\;0&\frac{1}{3}\\\;0&\;1&-\frac{3}{5}&0\\\;0&\;0&\frac{1}{5}&\;0\\\;0&\;0&\;0&\frac{1}{3}\end{bmatrix} B","['linear-algebra', 'matrices', 'inverse', 'matrix-calculus', 'solution-verification']"
38,Vector derivative of $ f(x)= (A+B\operatorname{diag}(x))^{-1} b$,Vector derivative of, f(x)= (A+B\operatorname{diag}(x))^{-1} b,"How to find a vector derivative with respect to $x\in \mathbb{R}^n$ of \begin{align}f(x)=  (A+B \operatorname{diag}(x))^{-1} b \end{align} where $\operatorname{diag}(x)$ is a diagonal matrix where $x$ is a main diagonal, $A\in \mathbb{R}^{n \times n}$ , $B\in \mathbb{R}^{n \times n}$ , $b \in \mathbb{R}^n$ . This question is similar to what I have asked here .  However, there are some differences with matrix multiplication that lead to some confusion for me. I am also wondering if this can be shown using $\epsilon$ -definition of the derivative.","How to find a vector derivative with respect to of where is a diagonal matrix where is a main diagonal, , , . This question is similar to what I have asked here .  However, there are some differences with matrix multiplication that lead to some confusion for me. I am also wondering if this can be shown using -definition of the derivative.","x\in \mathbb{R}^n \begin{align}f(x)=  (A+B \operatorname{diag}(x))^{-1} b
\end{align} \operatorname{diag}(x) x A\in \mathbb{R}^{n \times n} B\in \mathbb{R}^{n \times n} b \in \mathbb{R}^n \epsilon","['linear-algebra', 'matrices', 'multivariable-calculus', 'vector-analysis', 'jacobian']"
39,Linear Independent Eigenvectors and Generalized Eigenvectors,Linear Independent Eigenvectors and Generalized Eigenvectors,,"I did the following question: Show that if $A$ is a square matrix and $A^n = I$ , the identity matrix, for some $n>0$ , then $A$ has a basis of eigenvectors. The solution actually shows that A has no generalized eigenvectors that are not genuine/ordinary eigenvectors. That is, $A$ only has ""ordinary"" eigenvectors. But I don't understand why showing this can conclude that $A$ has a basis of eigenvectors? Thanks in advance for any help.","I did the following question: Show that if is a square matrix and , the identity matrix, for some , then has a basis of eigenvectors. The solution actually shows that A has no generalized eigenvectors that are not genuine/ordinary eigenvectors. That is, only has ""ordinary"" eigenvectors. But I don't understand why showing this can conclude that has a basis of eigenvectors? Thanks in advance for any help.",A A^n = I n>0 A A A,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
40,Correspondence between infinite matrices and $\ell_2$ operators,Correspondence between infinite matrices and  operators,\ell_2,"When calculating the numerical range of the matrix $$ C := \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix} $$ and the left-shift operator on the Hilbert space $\ell_2$ $$ T: \ell_2(\mathbb{N}) \to \ell_2(\mathbb{N}), \   (x_1, x_2, \ldots) \mapsto (x_2, \ldots) $$ I noticed that the latter can be considered an infinite-dimensional generalisation of the first as $$ \begin{pmatrix} 0 & 1 \\ 0 & 0  \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} x_2 \\ 0 \end{pmatrix} \quad \text{and} \quad \begin{pmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0  \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} = \begin{pmatrix} x_2 \\ x_3 \\ 0 \end{pmatrix} $$ and so on. If we now continue this pattern to infinity (I know this is not perfectly rigorous, but considering the norm of the difference of $T x$ and $T_n x$ , where $T_n$ are the matrices, we see it goes to zero as $\ell_2$ sequences are zero sequences), we end up with $T$ ! A similar example of course is the right shift operator on $\ell_2$ with ""finite-dimensional analogon"" $$\begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}.$$ For $$T: \ell_2 \to \ell_2, \ (x_1, x_2, x_3, \ldots) \mapsto \left(x_1, \frac{x_2}{2}, \frac{x_3}{3}, \ldots \right)$$ this is more difficult to find such an analogon, since one could argue like above that $$ T_1 :=  \begin{pmatrix} 1 & 0 \\ 0 & \frac{1}{2} \end{pmatrix} $$ is the finite-dimensional analogon as $$ T_1 \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} x_1 \\ \frac{x_2}{2} \end{pmatrix}, $$ but this somewhat doesn't contain the ""essence"" of the operator like in the examples of shift-operators above, as $\begin{pmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0\end{pmatrix}$ exhibits a certain self-similarity, in the way that you can find the lower dimensional matrix $C$ in it two times (though they are overlapping). Questions Are there other examples of such a corresponded (one could also consider $(x_1, x_2, x_3) \mapsto \left(x_1, \frac{x_2}{2!}, \frac{x_3}{3!}\right)$ and similar variations) and what might be a reason that $T$ has a ""finite-dimensional analogon""? Consider $D := \{ y \in \ell_2: \exists N \in \mathbb{N}: y_n = 0 \ \forall n > N\}$ , pick $x \in \ell_2 \setminus D$ and define $$ \hat{T}: \text{span}(x) + D \to \text{span}(x), \  cx + d \mapsto cx, $$ where $c \in \mathbb{C}$ and $d \in D$ . $\hat{T}$ is still linear, but unbounded (and not closable). Can there still be a ""finite-dimensional analogon""?","When calculating the numerical range of the matrix and the left-shift operator on the Hilbert space I noticed that the latter can be considered an infinite-dimensional generalisation of the first as and so on. If we now continue this pattern to infinity (I know this is not perfectly rigorous, but considering the norm of the difference of and , where are the matrices, we see it goes to zero as sequences are zero sequences), we end up with ! A similar example of course is the right shift operator on with ""finite-dimensional analogon"" For this is more difficult to find such an analogon, since one could argue like above that is the finite-dimensional analogon as but this somewhat doesn't contain the ""essence"" of the operator like in the examples of shift-operators above, as exhibits a certain self-similarity, in the way that you can find the lower dimensional matrix in it two times (though they are overlapping). Questions Are there other examples of such a corresponded (one could also consider and similar variations) and what might be a reason that has a ""finite-dimensional analogon""? Consider , pick and define where and . is still linear, but unbounded (and not closable). Can there still be a ""finite-dimensional analogon""?","
C := \begin{pmatrix}
0 & 1 \\ 0 & 0 \end{pmatrix}
 \ell_2 
T: \ell_2(\mathbb{N}) \to \ell_2(\mathbb{N}), \ 
 (x_1, x_2, \ldots) \mapsto (x_2, \ldots)
 
\begin{pmatrix}
0 & 1 \\ 0 & 0 
\end{pmatrix}
\begin{pmatrix} x_1 \\ x_2 \end{pmatrix}
= \begin{pmatrix} x_2 \\ 0 \end{pmatrix}
\quad \text{and} \quad
\begin{pmatrix}
0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 
\end{pmatrix}
\begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix}
= \begin{pmatrix} x_2 \\ x_3 \\ 0 \end{pmatrix}
 T x T_n x T_n \ell_2 T \ell_2 \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}. T: \ell_2 \to \ell_2, \ (x_1, x_2, x_3, \ldots) \mapsto \left(x_1, \frac{x_2}{2}, \frac{x_3}{3}, \ldots \right) 
T_1 := 
\begin{pmatrix}
1 & 0 \\
0 & \frac{1}{2}
\end{pmatrix}
 
T_1 \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}
= \begin{pmatrix} x_1 \\ \frac{x_2}{2} \end{pmatrix},
 \begin{pmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0\end{pmatrix} C (x_1, x_2, x_3) \mapsto \left(x_1, \frac{x_2}{2!}, \frac{x_3}{3!}\right) T D := \{ y \in \ell_2: \exists N \in \mathbb{N}: y_n = 0 \ \forall n > N\} x \in \ell_2 \setminus D 
\hat{T}: \text{span}(x) + D \to \text{span}(x), \ 
cx + d \mapsto cx,
 c \in \mathbb{C} d \in D \hat{T}","['matrices', 'functional-analysis', 'examples-counterexamples']"
41,"Prove $\mbox{rank} (A  B) = \mbox{rank} (A  AB) + \mbox{rank}(B  AB)$, if $A^2 = A$ and $B^2 = B$","Prove , if  and",\mbox{rank} (A  B) = \mbox{rank} (A  AB) + \mbox{rank}(B  AB) A^2 = A B^2 = B,"How would one prove following? If $A^2 = A$ and $B^2 = B$ , then show that $$\mbox{rank} (A  B) = \mbox{rank} (A  AB) + \mbox{rank} (B  AB)$$","How would one prove following? If and , then show that",A^2 = A B^2 = B \mbox{rank} (A  B) = \mbox{rank} (A  AB) + \mbox{rank} (B  AB),"['linear-algebra', 'matrices', 'matrix-rank', 'idempotents']"
42,$\det(A+tB)=\det(f(A)+tf(B))$ [closed],[closed],\det(A+tB)=\det(f(A)+tf(B)),"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Let $f:\mathcal{M}_n(\mathbb{C})\longrightarrow\mathcal{M}_n(\mathbb{C})$ a surjective function such that for all $A,B\in\mathcal{M}_n(\mathbb{C})$ and for all $t\in\mathbb{C}$ , $$ \det(A+tB)=\det(f(A)+tf(B)) $$ How can one show that $f$ is bijective and that $\mathrm{rank}(f(A))=\mathrm{rank}(A)$ for all $A\in\mathcal{M}_n(\mathbb{C})$ ?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Let a surjective function such that for all and for all , How can one show that is bijective and that for all ?","f:\mathcal{M}_n(\mathbb{C})\longrightarrow\mathcal{M}_n(\mathbb{C}) A,B\in\mathcal{M}_n(\mathbb{C}) t\in\mathbb{C}  \det(A+tB)=\det(f(A)+tf(B))  f \mathrm{rank}(f(A))=\mathrm{rank}(A) A\in\mathcal{M}_n(\mathbb{C})","['linear-algebra', 'matrices', 'determinant']"
43,Getting contradiction for $\left[\begin{array}{rrr|r}1&1&1&3\\2&-1&1&0\\-3&5&7&7\end{array}\right]$,Getting contradiction for,\left[\begin{array}{rrr|r}1&1&1&3\\2&-1&1&0\\-3&5&7&7\end{array}\right],"I'm trying to solve the following matrix and I'm getting a contradiction yet I know there is a solution as I've graphed it and the book has the same answer.  What am I doing wrong? $$\left[\begin{array}{rrr|r}1&1&1&3\\2&-1&1&0\\-3&5&7&7\end{array}\right]$$ Now the book gets (1,2,0) as an answer.  And here is my attempt. Replace R2 with -2R1+R2 and I also replace R3 with 3R1+R3: $$\left[\begin{array}{rrr|r}1&1&1&3\\0&3&1&6\\0&8&10&16\end{array}\right]$$ Then I multiply R2 by 1/3 as to get a 1 in the leading term: $$\left[\begin{array}{rrr|r}1&1&1&3\\0&1&1/3&2\\0&8&10&16\end{array}\right]$$ Now I want to eliminate the leading term of R3 so I replace it with -8R2+R3 and get: $$\left[\begin{array}{rrr|r}1&1&1&3\\0&1&1/3&2\\0&0&22/3&0\end{array}\right]$$ And so 22/3 = 0 is a contradiction.  What the heck am I doing wrong?","I'm trying to solve the following matrix and I'm getting a contradiction yet I know there is a solution as I've graphed it and the book has the same answer.  What am I doing wrong? Now the book gets (1,2,0) as an answer.  And here is my attempt. Replace R2 with -2R1+R2 and I also replace R3 with 3R1+R3: Then I multiply R2 by 1/3 as to get a 1 in the leading term: Now I want to eliminate the leading term of R3 so I replace it with -8R2+R3 and get: And so 22/3 = 0 is a contradiction.  What the heck am I doing wrong?",\left[\begin{array}{rrr|r}1&1&1&3\\2&-1&1&0\\-3&5&7&7\end{array}\right] \left[\begin{array}{rrr|r}1&1&1&3\\0&3&1&6\\0&8&10&16\end{array}\right] \left[\begin{array}{rrr|r}1&1&1&3\\0&1&1/3&2\\0&8&10&16\end{array}\right] \left[\begin{array}{rrr|r}1&1&1&3\\0&1&1/3&2\\0&0&22/3&0\end{array}\right],"['linear-algebra', 'matrices']"
44,How can the row rank of a matrix with complex elements be calculated over $\mathbb{Q}$?,How can the row rank of a matrix with complex elements be calculated over ?,\mathbb{Q},"I have a matrix whose elements are complex or real numbers or expressions of complex-valued or real-valued functions. How can the row rank of that matrix be calculated over $\mathbb{Q}$ ? The row rank over $\mathbb{Q}$ is the number of rows that are linearly independent over $\mathbb{Q}$ . for example: $$\left( \begin{array}{c} 1\\\sqrt2\\\sqrt3\\\sqrt6 \end{array}\right)$$ or $$\left(\begin{array}{ccc} 1+x & \sqrt{2}x & e^x \\ 1 & \sqrt{2} & e^x \\ 0 & 0 & e^x \\ \end{array}\right)$$ Usually, Gaussian elimination is used. But what is the algorithm for Gaussian elimination over $\mathbb{Q}$ ? How must the usual Gaussian elimination algorithm (see e.g. this short and easy program from the MathWorks Matlab forum ) be changed for calculating the rank over $\mathbb{Q}$ ? I also could calculate the Wronskian matrix or the Gramian matrix and calculate their determinant over $\mathbb{Q}$ . Example 2 is the Wronskian matrix of the first row of that matrix. MAPLE has a procedure for calculating determinants over $\mathbb{Q}(i)$ . See e.g. MAPLE: LinearAlgebra[Determinant] - method=rational . Is this only for choosing the fastest algorithm by the user? What is the algorithm for determinant calculation over $\mathbb{Q}$ for matrices with complex elements? I could build all combinations of up to three rows of the matrix and check if they are linearly dependent over $\mathbb{Q}$ because for up to 3 rows, I need to calculate only one coefficient of the linear combination and check if it is rational. But what is with larger matrices? The answer to this questions help to solve the problem in Algebraic independence of the values of algebraic functions? .","I have a matrix whose elements are complex or real numbers or expressions of complex-valued or real-valued functions. How can the row rank of that matrix be calculated over ? The row rank over is the number of rows that are linearly independent over . for example: or Usually, Gaussian elimination is used. But what is the algorithm for Gaussian elimination over ? How must the usual Gaussian elimination algorithm (see e.g. this short and easy program from the MathWorks Matlab forum ) be changed for calculating the rank over ? I also could calculate the Wronskian matrix or the Gramian matrix and calculate their determinant over . Example 2 is the Wronskian matrix of the first row of that matrix. MAPLE has a procedure for calculating determinants over . See e.g. MAPLE: LinearAlgebra[Determinant] - method=rational . Is this only for choosing the fastest algorithm by the user? What is the algorithm for determinant calculation over for matrices with complex elements? I could build all combinations of up to three rows of the matrix and check if they are linearly dependent over because for up to 3 rows, I need to calculate only one coefficient of the linear combination and check if it is rational. But what is with larger matrices? The answer to this questions help to solve the problem in Algebraic independence of the values of algebraic functions? .","\mathbb{Q} \mathbb{Q} \mathbb{Q} \left( \begin{array}{c}
1\\\sqrt2\\\sqrt3\\\sqrt6
\end{array}\right) \left(\begin{array}{ccc}
1+x & \sqrt{2}x & e^x \\
1 & \sqrt{2} & e^x \\
0 & 0 & e^x \\
\end{array}\right) \mathbb{Q} \mathbb{Q} \mathbb{Q} \mathbb{Q}(i) \mathbb{Q} \mathbb{Q}","['linear-algebra', 'matrices']"
45,Is there a formula to calculate the euclidean distance of two matrices?,Is there a formula to calculate the euclidean distance of two matrices?,,"Wiki gives this formula ${\displaystyle {\begin{aligned}d(\mathbf {p} ,\mathbf {q} )=d(\mathbf {q} ,\mathbf {p} )&={\sqrt {(q_{1}-p_{1})^{2}+(q_{2}-p_{2})^{2}+\cdots +(q_{n}-p_{n})^{2}}}\\[8pt]&={\sqrt {\sum _{i=1}^{n}(q_{i}-p_{i})^{2}}}.\end{aligned}}}$ to calculate the euclidean distance of two vectors. Is there a similar formula to calculate the euclidean distance of two matrices?",Wiki gives this formula to calculate the euclidean distance of two vectors. Is there a similar formula to calculate the euclidean distance of two matrices?,"{\displaystyle {\begin{aligned}d(\mathbf {p} ,\mathbf {q} )=d(\mathbf {q} ,\mathbf {p} )&={\sqrt {(q_{1}-p_{1})^{2}+(q_{2}-p_{2})^{2}+\cdots +(q_{n}-p_{n})^{2}}}\\[8pt]&={\sqrt {\sum _{i=1}^{n}(q_{i}-p_{i})^{2}}}.\end{aligned}}}","['linear-algebra', 'matrices']"
46,"Showing $\det\big[ (B+K)^{-1} (A+K) \big] = O(1) $ when $A,B$ are rank 1 updates of $I_n$ and $K$ is symmetric PD with positive entries",Showing  when  are rank 1 updates of  and  is symmetric PD with positive entries,"\det\big[ (B+K)^{-1} (A+K) \big] = O(1)  A,B I_n K","In general, given $n$ define $m_A, m_B \in\{1,...,n-1\}$ by $$ m_A = floor(a \times n) $$ $$ m_B = floor(b \times n ) $$ where the constants $a,b \in (0,1)$ are independent of $n$ with $a \ne b$ . Define two matrices as rank 1 updates of the identity matrix: $$A=I_n +u_A u_A^\top\; \text{where}\; (u_A)_i=\left\{\begin{array}{cc} 0, & i\leq n-m_A \\ 1 & \text{else} \end{array}\right.,$$ $$B=I_n +u_B u_B^\top\; \text{where}\; (u_B)_i=\left\{\begin{array}{cc} 0, & i\leq n-m_B \\ 1 & \text{else} \end{array}\right.$$ or equivalently, \begin{equation}  A = \begin{pmatrix} I_{n-m_A} & 0 \\ 0 & I_{m_A} + J_{m_A} \\ \end{pmatrix}, B = \begin{pmatrix} I_{n-m_B} & 0 \\ 0 & I_{m_B} + J_{m_B} \\ \end{pmatrix}, \end{equation} where $J_m$ is a $m \times m$ matrix of ones. My goal Now, let $K$ be a $n \times n$ symmetric positive definite matrix with positive entries. My goal is to show that $\det\left[ (B+K)^{-1} (A+K) \right]$ is $O(1)$ as $n \to \infty$ . Hence, I would like to find bounds which are $O(1)$ . Findings so far From link1 , I know that 1 as an eigenvalue of the matrix $B^{-1}A$ has multiplicity $n-2$ . From link2 , I also know that $\det(B^{-1}A) =\frac{m_A+1}{m_B+1}$ and $\det(A^{-1}B) =\frac{m_B+1}{m_A+1}$ . Thank to the suggestion ( link3 ) by @Semiclassical, $$\det[(B+K)^{-1})(A+K)] =\frac{\det(A+K)}{\det(B+K)} =\frac{\det(K+I_n+u_A u_A^\top)}{\det(K+I_n+u_B u_B^\top)} =\frac{(1+u_A^\top(K+I_n)^{-1} u_A)\det(K+I_n)}{(1+u_B^\top(K+I_n)^{-1} u_B)\det(K+I_n)}=\frac{1+u_A^\top(K+I_n)^{-1} u_A}{1+u_B^\top(K+I_n)^{-1} u_B}$$ where the third equality holds due to the identity $\det(X+uv^\top)=(1+u^\top X^{-1}v)\det X$ . My attempts and Questions (Question 1) Through numerical experiments in Matlab, I found candidate bounds that seem to work for various versions of $K$ (the Matlab code can be found below).  So my question is: is the following statement true for all $n$ and $K$ (any symmetric positive definite matrix with only positive entries)? I. If $m_B<m_A$ , then \begin{align*} \det (A^{-1}B) \leq  \det\left[ (B+K)^{-1} (A+K) \right]  \leq  \det (B^{-1}A) \end{align*} II. If $m_B>m_A$ , then \begin{align*} \det (B^{-1}A)  \leq  \det\left[ (B+K)^{-1} (A+K) \right]  \leq  \det (A^{-1}B) \end{align*} or equivalently, I. If $m_B<m_A$ , then \begin{align*} \frac{1+m_B}{1+m_A}  \leq \frac{1+u_A^\top(K+I_n)^{-1} u_A}{1+u_B^\top(K+I_n)^{-1} u_B} \leq  \frac{1+m_A}{1+m_B} \end{align*} II. If $m_B>m_A$ , then \begin{align*} \frac{1+m_A}{1+m_B}  \leq  \frac{1+u_A^\top(K+I_n)^{-1} u_A}{1+u_B^\top(K+I_n)^{-1} u_B} \leq  \frac{1+m_B}{1+m_A}  \end{align*} where $\frac{1+m_A}{1+m_B}\approx \frac{1+a\times n}{1+b \times n}=\frac{1/n + a}{1/n +b}$ and $\frac{1+m_B}{1+m_A} \approx \frac{1/n+b}{1/n+a}$ are $O(1)$ , so the inequalities would imply that $\det\left[ (B+K)^{-1} (A+K) \right]=O(1)$ which is my goal. (Question 2) Are there any other bounds for $\det\left[ (B+K)^{-1} (A+K) \right]$ that are $O(1)$ (possibly obvious bounds that I am missing)? Note I initially thought a sharper bound by $1$ might be possible, but it was not. Suppose $m_B<m_A$ . It is not guaranteed that $u_A^T(K+I_n)^{-1}u_A -u_B^T(K+I_n)^{-1}u_B \geq 0$ . To see this, for instance, consider the example provided here with the matrix $$K = \begin{bmatrix} 1 & 1 & 1\\ 1 & 100 & 99\\ 1 & 99 & 100\\ \end{bmatrix}, \\ $$ and the vectors $u_A = (0, 1, 1)$ and $u_B =(0, 0, 1)$ . This means that the sharper lower bound by $1$ : \begin{align*} \frac{1+m_B}{1+m_A}  < 1 \leq \frac{1+u_A^T(K+I_n)^{-1}u_A}{1+u_B^T(K+I_n)^{-1}u_B} \end{align*} is not possible. However, the proposed bounds by $\frac{1+m_B}{1+m_A}$ and $\frac{1+m_A}{1+m_B}$ still work even with the $K$ , $u_A$ , and $u_B$ in the example above. Code Matlab code for a fixed $n$ : % 1. Specify n,a,b  n=5; a=0.7;b=0.3; mA=floor(a*n); mB=floor(b*n);  % 2. Define matrices A and B  % Define a vector uA whose first n-mA entries = 0 and the last mA entries =1    uA=ones(n,1);uA(1:n-mA)=0;  A=eye(n)+uA*uA'; % Do the same for B uB=ones(n,1);uB(1:n-mB)=0;  B=eye(n)+uB*uB'; % 3. Define a (this can be any) symmetric PD matrix K with positive entires  K = rand(n,n);K = 0.5*(K+K'); K = K + n*eye(n);  % 4. Check that det(A) = m_A +1. Same for B. det(A) mA+1 det(B) mB+1 % 5. Compare three items (mB+1)/(mA+1) det(inv(B+K)*(A+K)) (mA+1)/(mB+1) Matlab code for varying $n$ : n_grid=10:100:1000; a=0.7;b=0.3; for i=1:length(n_grid)    n=n_grid(i);    mA=floor(a*n);    mB=floor(b*n);    uA=ones(n,1);uA(1:n-mA)=0;     A=eye(n)+uA*uA';    uB=ones(n,1);uB(1:n-mB)=0;     B=eye(n)+uB*uB';    K = rand(n,n);K = 0.5*(K+K'); K = K + n*eye(n);     determinant(i) = det(inv(B+K)*(A+K));    det_invBA(i)=(mA+1)/(mB+1); % determinant of inv(B)*A    det_invAB(i)=(mB+1)/(mA+1); % determinant of inv(A)*B end   figure plot(n_grid,determinant,'*');xlabel('n'); hold on  plot(n_grid,det_invBA,'*'); hold on plot(n_grid,det_invAB,'*'); legend('det (B+K)^{-1}(A+K)','det B^{-1}A','det A^{-1}B'); xlim([n_grid(1),n_grid(end)]);xlabel('n') title(['a =',num2str(a),'  b =',num2str(b)] );","In general, given define by where the constants are independent of with . Define two matrices as rank 1 updates of the identity matrix: or equivalently, where is a matrix of ones. My goal Now, let be a symmetric positive definite matrix with positive entries. My goal is to show that is as . Hence, I would like to find bounds which are . Findings so far From link1 , I know that 1 as an eigenvalue of the matrix has multiplicity . From link2 , I also know that and . Thank to the suggestion ( link3 ) by @Semiclassical, where the third equality holds due to the identity . My attempts and Questions (Question 1) Through numerical experiments in Matlab, I found candidate bounds that seem to work for various versions of (the Matlab code can be found below).  So my question is: is the following statement true for all and (any symmetric positive definite matrix with only positive entries)? I. If , then II. If , then or equivalently, I. If , then II. If , then where and are , so the inequalities would imply that which is my goal. (Question 2) Are there any other bounds for that are (possibly obvious bounds that I am missing)? Note I initially thought a sharper bound by might be possible, but it was not. Suppose . It is not guaranteed that . To see this, for instance, consider the example provided here with the matrix and the vectors and . This means that the sharper lower bound by : is not possible. However, the proposed bounds by and still work even with the , , and in the example above. Code Matlab code for a fixed : % 1. Specify n,a,b  n=5; a=0.7;b=0.3; mA=floor(a*n); mB=floor(b*n);  % 2. Define matrices A and B  % Define a vector uA whose first n-mA entries = 0 and the last mA entries =1    uA=ones(n,1);uA(1:n-mA)=0;  A=eye(n)+uA*uA'; % Do the same for B uB=ones(n,1);uB(1:n-mB)=0;  B=eye(n)+uB*uB'; % 3. Define a (this can be any) symmetric PD matrix K with positive entires  K = rand(n,n);K = 0.5*(K+K'); K = K + n*eye(n);  % 4. Check that det(A) = m_A +1. Same for B. det(A) mA+1 det(B) mB+1 % 5. Compare three items (mB+1)/(mA+1) det(inv(B+K)*(A+K)) (mA+1)/(mB+1) Matlab code for varying : n_grid=10:100:1000; a=0.7;b=0.3; for i=1:length(n_grid)    n=n_grid(i);    mA=floor(a*n);    mB=floor(b*n);    uA=ones(n,1);uA(1:n-mA)=0;     A=eye(n)+uA*uA';    uB=ones(n,1);uB(1:n-mB)=0;     B=eye(n)+uB*uB';    K = rand(n,n);K = 0.5*(K+K'); K = K + n*eye(n);     determinant(i) = det(inv(B+K)*(A+K));    det_invBA(i)=(mA+1)/(mB+1); % determinant of inv(B)*A    det_invAB(i)=(mB+1)/(mA+1); % determinant of inv(A)*B end   figure plot(n_grid,determinant,'*');xlabel('n'); hold on  plot(n_grid,det_invBA,'*'); hold on plot(n_grid,det_invAB,'*'); legend('det (B+K)^{-1}(A+K)','det B^{-1}A','det A^{-1}B'); xlim([n_grid(1),n_grid(end)]);xlabel('n') title(['a =',num2str(a),'  b =',num2str(b)] );","n m_A, m_B \in\{1,...,n-1\}  m_A = floor(a \times n)   m_B = floor(b \times n )  a,b \in (0,1) n a \ne b A=I_n +u_A u_A^\top\; \text{where}\; (u_A)_i=\left\{\begin{array}{cc} 0, & i\leq n-m_A \\ 1 & \text{else} \end{array}\right., B=I_n +u_B u_B^\top\; \text{where}\; (u_B)_i=\left\{\begin{array}{cc} 0, & i\leq n-m_B \\ 1 & \text{else} \end{array}\right. \begin{equation} 
A =
\begin{pmatrix}
I_{n-m_A} & 0 \\
0 & I_{m_A} + J_{m_A} \\
\end{pmatrix},
B =
\begin{pmatrix}
I_{n-m_B} & 0 \\
0 & I_{m_B} + J_{m_B} \\
\end{pmatrix},
\end{equation} J_m m \times m K n \times n \det\left[ (B+K)^{-1} (A+K) \right] O(1) n \to \infty O(1) B^{-1}A n-2 \det(B^{-1}A) =\frac{m_A+1}{m_B+1} \det(A^{-1}B) =\frac{m_B+1}{m_A+1} \det[(B+K)^{-1})(A+K)]
=\frac{\det(A+K)}{\det(B+K)}
=\frac{\det(K+I_n+u_A u_A^\top)}{\det(K+I_n+u_B u_B^\top)}
=\frac{(1+u_A^\top(K+I_n)^{-1} u_A)\det(K+I_n)}{(1+u_B^\top(K+I_n)^{-1} u_B)\det(K+I_n)}=\frac{1+u_A^\top(K+I_n)^{-1} u_A}{1+u_B^\top(K+I_n)^{-1} u_B} \det(X+uv^\top)=(1+u^\top X^{-1}v)\det X K n K m_B<m_A \begin{align*}
\det (A^{-1}B)
\leq 
\det\left[ (B+K)^{-1} (A+K) \right] 
\leq 
\det (B^{-1}A)
\end{align*} m_B>m_A \begin{align*}
\det (B^{-1}A) 
\leq 
\det\left[ (B+K)^{-1} (A+K) \right] 
\leq 
\det (A^{-1}B)
\end{align*} m_B<m_A \begin{align*}
\frac{1+m_B}{1+m_A} 
\leq
\frac{1+u_A^\top(K+I_n)^{-1} u_A}{1+u_B^\top(K+I_n)^{-1} u_B}
\leq 
\frac{1+m_A}{1+m_B}
\end{align*} m_B>m_A \begin{align*}
\frac{1+m_A}{1+m_B} 
\leq 
\frac{1+u_A^\top(K+I_n)^{-1} u_A}{1+u_B^\top(K+I_n)^{-1} u_B}
\leq 
\frac{1+m_B}{1+m_A} 
\end{align*} \frac{1+m_A}{1+m_B}\approx \frac{1+a\times n}{1+b \times n}=\frac{1/n + a}{1/n +b} \frac{1+m_B}{1+m_A} \approx \frac{1/n+b}{1/n+a} O(1) \det\left[ (B+K)^{-1} (A+K) \right]=O(1) \det\left[ (B+K)^{-1} (A+K) \right] O(1) 1 m_B<m_A u_A^T(K+I_n)^{-1}u_A -u_B^T(K+I_n)^{-1}u_B \geq 0 K =
\begin{bmatrix}
1 & 1 & 1\\
1 & 100 & 99\\
1 & 99 & 100\\
\end{bmatrix}, \\
 u_A = (0, 1, 1) u_B =(0, 0, 1) 1 \begin{align*}
\frac{1+m_B}{1+m_A} 
<
1
\leq
\frac{1+u_A^T(K+I_n)^{-1}u_A}{1+u_B^T(K+I_n)^{-1}u_B}
\end{align*} \frac{1+m_B}{1+m_A} \frac{1+m_A}{1+m_B} K u_A u_B n n","['matrices', 'eigenvalues-eigenvectors', 'determinant', 'inverse', 'upper-lower-bounds']"
47,Matrix representation when the vector space is infinite dimensional,Matrix representation when the vector space is infinite dimensional,,"How do we represent a linear map when the vector space is infinite dimensional? Will the matrix itself become infinite dimensional as well? Say I consider linear map $T:\mathbb{F}[X]\to \mathbb{F}[X]$ as $T(f(x))=xf(x)$ , which is obviously infinite dimensional. How can I construct such matrix with respect to the basis $\{1, x, x^2,...\}$ ? Will this matrix be infinite dimensional with $1$ 's below the main diagonal and other entries just $0$ ?","How do we represent a linear map when the vector space is infinite dimensional? Will the matrix itself become infinite dimensional as well? Say I consider linear map as , which is obviously infinite dimensional. How can I construct such matrix with respect to the basis ? Will this matrix be infinite dimensional with 's below the main diagonal and other entries just ?","T:\mathbb{F}[X]\to \mathbb{F}[X] T(f(x))=xf(x) \{1, x, x^2,...\} 1 0","['linear-algebra', 'matrices']"
48,Proof of De Gua's Theorem using Cauchy Binet,Proof of De Gua's Theorem using Cauchy Binet,,"I was trying to prove the generalized version of De Gua's theorem using the Cauchy-Binet formula , and I ran into a bit of trouble. We are considering a right-angled simplex in $\Bbb R^n$ whose corners are the origin and the coordinates $a_i e_i$ (where $a_i > 0$ and $e_1,\dots,e_n$ denotes the canonical basis of $\Bbb R^n$ ). The ( $n$ -dimensional) volume of a simplex whose corners are the origin and (column-vectors) $v_1,\dots,v_n$ can be computed with the formula $$ V = \frac 1{n!}\det \pmatrix{v_1 & \cdots & v_n} $$ Similarly, the ( $(n-1)$ -dimensional) volume of a simplex whose corners are $v_1,\dots,v_n$ can be computed via $V^2 = \frac 1{((n-1)!)^2}\det M^TM$ where $$ M = \pmatrix{v_1 & v_2 & \cdots & v_n\\ 1 & 1 & \cdots & 1} $$ (or so I believe). Note that the above $M$ is not square: it has size $(n+1) \times n$ . With the above in mind: let $A$ denote the diagonal matrix $$ A = \pmatrix{a_1 \\ & \ddots \\ && a_n} $$ and let $x = (1,1,\dots,1)^T \in \Bbb R^n$ . De Gua's theorem should tell us that $$ ((n-1)!)^2 V^2 = \det \left[\pmatrix{A\\x^T}^T\pmatrix{A\\x^T}\right] = \sum_{i=1}^n (P/a_i)^2 $$ where $P = \det A = a_1 a_2 \cdots a_n$ .  However, applying the Cauchy-Binet formula yields $$ \det \left[\pmatrix{A\\x^T}^T\pmatrix{A\\x^T}\right] = P^2 + \sum_{i=1}^n (P/a_i)^2. $$ The question: So, where did I go wrong?  I suspect that there's an issue with the $M^TM$ formula I give above, but I'm not confident that this is the issue.  If that is the issue, I'm not sure if the above proof is salvageable. Any feedback here is appreciated.","I was trying to prove the generalized version of De Gua's theorem using the Cauchy-Binet formula , and I ran into a bit of trouble. We are considering a right-angled simplex in whose corners are the origin and the coordinates (where and denotes the canonical basis of ). The ( -dimensional) volume of a simplex whose corners are the origin and (column-vectors) can be computed with the formula Similarly, the ( -dimensional) volume of a simplex whose corners are can be computed via where (or so I believe). Note that the above is not square: it has size . With the above in mind: let denote the diagonal matrix and let . De Gua's theorem should tell us that where .  However, applying the Cauchy-Binet formula yields The question: So, where did I go wrong?  I suspect that there's an issue with the formula I give above, but I'm not confident that this is the issue.  If that is the issue, I'm not sure if the above proof is salvageable. Any feedback here is appreciated.","\Bbb R^n a_i e_i a_i > 0 e_1,\dots,e_n \Bbb R^n n v_1,\dots,v_n 
V = \frac 1{n!}\det \pmatrix{v_1 & \cdots & v_n}
 (n-1) v_1,\dots,v_n V^2 = \frac 1{((n-1)!)^2}\det M^TM 
M = \pmatrix{v_1 & v_2 & \cdots & v_n\\ 1 & 1 & \cdots & 1}
 M (n+1) \times n A 
A = \pmatrix{a_1 \\ & \ddots \\ && a_n}
 x = (1,1,\dots,1)^T \in \Bbb R^n 
((n-1)!)^2 V^2 = \det \left[\pmatrix{A\\x^T}^T\pmatrix{A\\x^T}\right] = \sum_{i=1}^n (P/a_i)^2
 P = \det A = a_1 a_2 \cdots a_n 
\det \left[\pmatrix{A\\x^T}^T\pmatrix{A\\x^T}\right] = P^2 + \sum_{i=1}^n (P/a_i)^2.
 M^TM","['linear-algebra', 'matrices', 'geometry', 'proof-verification']"
49,Constructing the identity element of a matrix algebra generated by a single matrix,Constructing the identity element of a matrix algebra generated by a single matrix,,"Setup : Let us define the matrix algebra $\mathcal{A}$ generated by a single self-adjoint matrix $M$ to be the span of all natural powers of $M$ (the matrix and the span are over complex numbers) $$\mathcal A :=\left<M\right>=span\{M,M^2,M^3...\}.$$ Question : Is there an element $I_\mathcal{A}\in\mathcal{A}$ that acts as the multiplicative identity on the elements of $\mathcal{A}$ ? Is there a constructive way to show it? Remarks : I know that when we have the identity matrix as part of the algebra $\left<I,M\right>$ the algebra is just the span of spectral projections (including the kernel projection) of $M$ . I expect the same to be true for $\left<M\right>$ (excluding the kernel projection) and the identity $I_\mathcal{A}$ to be just the sum of all the spectral projections. The problems start when I try to construct the spectral projections in $\left<M\right>$ without using the identity or construct the identity $I_\mathcal{A}$ without using the spectral projections (if I have one it is easy to construct the other). Thank you.",Setup : Let us define the matrix algebra generated by a single self-adjoint matrix to be the span of all natural powers of (the matrix and the span are over complex numbers) Question : Is there an element that acts as the multiplicative identity on the elements of ? Is there a constructive way to show it? Remarks : I know that when we have the identity matrix as part of the algebra the algebra is just the span of spectral projections (including the kernel projection) of . I expect the same to be true for (excluding the kernel projection) and the identity to be just the sum of all the spectral projections. The problems start when I try to construct the spectral projections in without using the identity or construct the identity without using the spectral projections (if I have one it is easy to construct the other). Thank you.,"\mathcal{A} M M \mathcal A :=\left<M\right>=span\{M,M^2,M^3...\}. I_\mathcal{A}\in\mathcal{A} \mathcal{A} \left<I,M\right> M \left<M\right> I_\mathcal{A} \left<M\right> I_\mathcal{A}","['linear-algebra', 'matrices', 'operator-algebras', 'von-neumann-algebras']"
50,$<$ and $>$ symbols used with matrices,and  symbols used with matrices,< >,"In this Wikipedia page, in the first property, it says $a_1$ < $a_2$ , where $a_1$ and $a_2$ are elements of a C*-algebra. The easiest way for me to think of $a_1$ and $a_2$ are as matrices. Does that statement mean that the smallest eigenvalue of $a_1$ is lesser than the smallest eigenvalue of $a_2$ ?","In this Wikipedia page, in the first property, it says < , where and are elements of a C*-algebra. The easiest way for me to think of and are as matrices. Does that statement mean that the smallest eigenvalue of is lesser than the smallest eigenvalue of ?",a_1 a_2 a_1 a_2 a_1 a_2 a_1 a_2,"['linear-algebra', 'matrices', 'c-star-algebras']"
51,Matrix optimization problem,Matrix optimization problem,,"I am having some difficulty understanding an argument in a book. The authors claim that the following Theorem is a direct consequence of the preceding lemma, but fail to give details. Either it is completely trivial and I am not seeing it, or there are some details missing. Lemma : Let $A$ be a $n\times n$ symmetric positive definite matrix over $\mathbb{R}$ with eigenvalues $\lambda_1>\lambda_2>\ldots>\lambda_n$ and associated eigenvectors $v_1,v_2,\ldots v_n$ . Then we have $$  \max\{x^TAx: ||x||=1, \langle x,v_j\rangle=0 \,\,\text{for}\,\, 1\leq j\leq i-1\}=\lambda_i, $$ where the maximum is attained precisely at the points $v_i$ and $-v_i$ . Theorem :   Let $p\leq n$ . Consider the following optimization problem \begin{align} \max\sum_{k=1}^p &\langle Au_k,u_k\rangle\\  s.t:\,\,(u_1,\ldots u_p)&\,\,\,\text{is an orthonormal system}  \end{align} The claim is that the optimal value is $\sum_{k=1}^p \lambda_k$ with   optimal solution $(v_1,v_2,\ldots,v_p)$ , and that the solution is unique up to sign and permutation. It seems to me that the optimization is carried out by successively maximizing each summand. I fail to understand why this is legitimate. What am I missing here? Thanks","I am having some difficulty understanding an argument in a book. The authors claim that the following Theorem is a direct consequence of the preceding lemma, but fail to give details. Either it is completely trivial and I am not seeing it, or there are some details missing. Lemma : Let be a symmetric positive definite matrix over with eigenvalues and associated eigenvectors . Then we have where the maximum is attained precisely at the points and . Theorem :   Let . Consider the following optimization problem The claim is that the optimal value is with   optimal solution , and that the solution is unique up to sign and permutation. It seems to me that the optimization is carried out by successively maximizing each summand. I fail to understand why this is legitimate. What am I missing here? Thanks","A n\times n \mathbb{R} \lambda_1>\lambda_2>\ldots>\lambda_n v_1,v_2,\ldots v_n 
 \max\{x^TAx: ||x||=1, \langle x,v_j\rangle=0 \,\,\text{for}\,\, 1\leq
j\leq i-1\}=\lambda_i,  v_i -v_i p\leq n \begin{align} \max\sum_{k=1}^p &\langle Au_k,u_k\rangle\\
 s.t:\,\,(u_1,\ldots u_p)&\,\,\,\text{is an orthonormal system}
 \end{align} \sum_{k=1}^p \lambda_k (v_1,v_2,\ldots,v_p)","['linear-algebra', 'matrices', 'optimization', 'nonlinear-optimization', 'matrix-decomposition']"
52,Alternative proofs Matrix Determinant Lemma,Alternative proofs Matrix Determinant Lemma,,"Well as many of you know wiki has a beautiful proof for the Matrix Determinant Lemma Wiki's Proof But: How the hell is one supposed to get there on his own? There is no way that when a professor would ask you to proof the lemma in your let's say 4th semester, you will come up with the idea for that proof. So my question is: Are there alternative methods to show the Lemma which are more intuitiv or let's say realistic to come up with?","Well as many of you know wiki has a beautiful proof for the Matrix Determinant Lemma Wiki's Proof But: How the hell is one supposed to get there on his own? There is no way that when a professor would ask you to proof the lemma in your let's say 4th semester, you will come up with the idea for that proof. So my question is: Are there alternative methods to show the Lemma which are more intuitiv or let's say realistic to come up with?",,"['linear-algebra', 'matrices', 'numerical-methods', 'determinant', 'matrix-calculus']"
53,How to complete a primitive vector to a unimodular matrix,How to complete a primitive vector to a unimodular matrix,,"I would like to understand the following relation between unimodular matrices and its columns in some sense: if $x$ is a primitive vector (that is to say an integer column of $n$ rows whose entries are coprime), then it can be completed to an $n\times n$ unimodular matrix. In the case of $2 \times 2$ matrices, I can see that it is equivalent to a Bezout relation, but is there a generalisation of this proof to show this property for all $n$ ?","I would like to understand the following relation between unimodular matrices and its columns in some sense: if is a primitive vector (that is to say an integer column of rows whose entries are coprime), then it can be completed to an unimodular matrix. In the case of matrices, I can see that it is equivalent to a Bezout relation, but is there a generalisation of this proof to show this property for all ?",x n n\times n 2 \times 2 n,"['matrices', 'gcd-and-lcm']"
54,Integral of $M^TM$ for matrix $M=B\exp(-At)$,Integral of  for matrix,M^TM M=B\exp(-At),"For non-commuting positive definite matrices $A$ , $B$ , is there a simple expression for $$\int_{0}^{\infty} M_t^T M_t \,\mathrm d t$$ where $M_t:=B\exp(-At)$ ? Based on the commutative case, I'd hope it's something like $\frac{1}{2}A^{-1/2}B^2A^{-1/2}$ but I cannot prove it.","For non-commuting positive definite matrices , , is there a simple expression for where ? Based on the commutative case, I'd hope it's something like but I cannot prove it.","A B \int_{0}^{\infty} M_t^T M_t \,\mathrm d t M_t:=B\exp(-At) \frac{1}{2}A^{-1/2}B^2A^{-1/2}","['matrices', 'improper-integrals', 'matrix-equations', 'matrix-calculus', 'matrix-exponential']"
55,Question on Wiki proof of Vandermonde matrix,Question on Wiki proof of Vandermonde matrix,,"On this page, https://proofwiki.org/wiki/Vandermonde_Determinant#Proof_3 , I understand how they are creating the $n-1$ degree polynomial $P(x)$ by calculating the determinant based on the final row. I understand how they find the $n-1$ zeroes of $P(x)$ , and I understand how they got that $P(x) = C(x-x_1)\ldots(x-x_{n-1})$ but I don't understand how they found that $C=V_{n-1}$ . It's not very clear on that step. Even if the coefficient of the $x^{n-1}$ is $V_{n-1}$ , how are we sure that the other coefficients will match up ? If the above is true, does that mean that $$\begin{vmatrix} x_1 & x_1^2 & \ldots & x_{1}^{n-1} \\ x_2 & x_2^2 & \ldots & x_{2}^{n-1} \\ \vdots & & \ddots & \vdots \\ x_{n-1} & x_{n-1}^2 & \ddots & x_{n-1}^{n-1}  \end{vmatrix}$$ is equal to $x_1x_2x_3\ldots x_{n-1}$ because they are both the constant term of $P(x)$ ? If so, are these identities useful in any way? Thanks for any help!","On this page, https://proofwiki.org/wiki/Vandermonde_Determinant#Proof_3 , I understand how they are creating the degree polynomial by calculating the determinant based on the final row. I understand how they find the zeroes of , and I understand how they got that but I don't understand how they found that . It's not very clear on that step. Even if the coefficient of the is , how are we sure that the other coefficients will match up ? If the above is true, does that mean that is equal to because they are both the constant term of ? If so, are these identities useful in any way? Thanks for any help!","n-1 P(x) n-1 P(x) P(x) = C(x-x_1)\ldots(x-x_{n-1}) C=V_{n-1} x^{n-1} V_{n-1} \begin{vmatrix} x_1 & x_1^2 & \ldots & x_{1}^{n-1} \\
x_2 & x_2^2 & \ldots & x_{2}^{n-1} \\
\vdots & & \ddots & \vdots \\
x_{n-1} & x_{n-1}^2 & \ddots & x_{n-1}^{n-1} 
\end{vmatrix} x_1x_2x_3\ldots x_{n-1} P(x)",['matrices']
56,How to prove that the following matrices in $M_p(\Bbb F_p)$ is similar,How to prove that the following matrices in  is similar,M_p(\Bbb F_p),How to prove that the following matrices in $M_p(\Bbb F_p)$ is similar: Consider two matrices $$(a_{ij})=     \begin{pmatrix}     1 & 1 & 0 & \cdots & 0 & 0 \\     0 & 1 & 1 & \cdots & 0 & 0\\     0 & 0 & 1 & \cdots & 0 & 0\\     \vdots & \vdots & \vdots& \ddots  & \vdots &\vdots\\     0 & 0 & 0 &  \cdots &  1 & 1\\     0 & 0 & 0 &  \cdots &  0 & 1\\     \end{pmatrix}$$ and $$(b_{ij})=     \begin{pmatrix}     0 & 0 & 0 & \cdots & 0 & 1 \\     1 & 0 & 0 & \cdots & 0 & 0\\     0 & 1 & 0 & \cdots & 0 & 0\\     \vdots & \vdots & \vdots& \vdots  & \vdots &\vdots\\     0 & 0 & 0 &  \cdots &  1 & 0\\     \end{pmatrix}$$ I can observe that the characteristic polynomial of $(a_{ij})$ =minimal polynomial of $(a_{ij})$ = $(x-1)^p$ and the characteristic polynomial of $(b_{ij})=x^p-1=(x-1)^p$ Now how do I argue from here that the minimal polynomial of $b_{ij}$ is also $(x-1)^p$ ?,How to prove that the following matrices in is similar: Consider two matrices and I can observe that the characteristic polynomial of =minimal polynomial of = and the characteristic polynomial of Now how do I argue from here that the minimal polynomial of is also ?,"M_p(\Bbb F_p) (a_{ij})=
    \begin{pmatrix}
    1 & 1 & 0 & \cdots & 0 & 0 \\
    0 & 1 & 1 & \cdots & 0 & 0\\
    0 & 0 & 1 & \cdots & 0 & 0\\
    \vdots & \vdots & \vdots& \ddots  & \vdots &\vdots\\
    0 & 0 & 0 &  \cdots &  1 & 1\\
    0 & 0 & 0 &  \cdots &  0 & 1\\
    \end{pmatrix} (b_{ij})=
    \begin{pmatrix}
    0 & 0 & 0 & \cdots & 0 & 1 \\
    1 & 0 & 0 & \cdots & 0 & 0\\
    0 & 1 & 0 & \cdots & 0 & 0\\
    \vdots & \vdots & \vdots& \vdots  & \vdots &\vdots\\
    0 & 0 & 0 &  \cdots &  1 & 0\\
    \end{pmatrix} (a_{ij}) (a_{ij}) (x-1)^p (b_{ij})=x^p-1=(x-1)^p b_{ij} (x-1)^p","['linear-algebra', 'matrices', 'modules', 'jordan-normal-form', 'minimal-polynomials']"
57,How to show that the given matrix has non-zero determinant,How to show that the given matrix has non-zero determinant,,"Given $p,q$ to be primes where $p<q$ . Show that the following marix has non-zero determinant, \begin{bmatrix} 	1&2 & 2 & 2 &\dotso & 2\\ 	2&q-p+1 & 1 & 1 &\dotso & 1\\ 2& 1 & q-p+1 & 1 & \dotso  & 1\\2&1 & 1 & q-p+1 &\dotso & 1 \\ \dotso &\dotso & \dotso & \dotso & \dotso \\ \dotso & \dotso & \dotso & \dotso & \dotso \\ \dotso & \dotso & \dotso & \dotso &\dotso 	\\2&1 &1 &1 &\dotso & q-p+1 	\end{bmatrix} I am able to show that the submatrix of this matrix \begin{bmatrix}q-p+1 & 1 & 1 & 1 & \dotso  & 1\\ 1 &q-p+1& 1 &\dotso & \dotso &1 \\ \dotso &\dotso & \dotso & \dotso & \dotso \\ \dotso & \dotso & \dotso & \dotso & \dotso \\ \dotso & \dotso & \dotso & \dotso &\dotso 	\\1 &1 &1 &\dotso & \dotso &q-p+1\end{bmatrix} has determinant non-zero. How I can show that the original matrix has determinant non-zero? I tried using Laplace Expansion but not getting anything. Please help.","Given to be primes where . Show that the following marix has non-zero determinant, I am able to show that the submatrix of this matrix has determinant non-zero. How I can show that the original matrix has determinant non-zero? I tried using Laplace Expansion but not getting anything. Please help.","p,q p<q \begin{bmatrix}
	1&2 & 2 & 2 &\dotso & 2\\
	2&q-p+1 & 1 & 1 &\dotso & 1\\
2& 1 & q-p+1 & 1 & \dotso  & 1\\2&1 & 1 & q-p+1 &\dotso & 1 \\ \dotso &\dotso & \dotso & \dotso & \dotso \\ \dotso & \dotso & \dotso & \dotso & \dotso \\ \dotso & \dotso & \dotso & \dotso &\dotso
	\\2&1 &1 &1 &\dotso & q-p+1
	\end{bmatrix} \begin{bmatrix}q-p+1 & 1 & 1 & 1 & \dotso  & 1\\ 1 &q-p+1& 1 &\dotso & \dotso &1 \\ \dotso &\dotso & \dotso & \dotso & \dotso \\ \dotso & \dotso & \dotso & \dotso & \dotso \\ \dotso & \dotso & \dotso & \dotso &\dotso
	\\1 &1 &1 &\dotso & \dotso &q-p+1\end{bmatrix}","['linear-algebra', 'matrices']"
58,"Let $A, B$ be two positive definite $2 \times 2$ matrices. Prove or disprove: $AB+BA$ is positive definite.",Let  be two positive definite  matrices. Prove or disprove:  is positive definite.,"A, B 2 \times 2 AB+BA","I know that $AB+BA$ is not necessarily positive definite, as this question has been asked before on here. What I don't understand is how one would go about constructing counter-examples. Previous answers to this question just state the counter-examples, such as $A = \begin{bmatrix} 6 & 0 \\ 0 & 1 \\ \end{bmatrix}$ , $B = \begin{bmatrix} 2 & 1 \\ 1 & 1 \\ \end{bmatrix}$ . Is there some intuition behind how these matrices are chosen, or is it more a matter of just fiddling with $2 \times 2$ matrices until we find a counter-example? Thanks in advance.","I know that is not necessarily positive definite, as this question has been asked before on here. What I don't understand is how one would go about constructing counter-examples. Previous answers to this question just state the counter-examples, such as , . Is there some intuition behind how these matrices are chosen, or is it more a matter of just fiddling with matrices until we find a counter-example? Thanks in advance.","AB+BA A = \begin{bmatrix} 6 & 0 \\
0 & 1 \\ \end{bmatrix} B = \begin{bmatrix} 2 & 1 \\
1 & 1 \\ \end{bmatrix} 2 \times 2","['linear-algebra', 'matrices', 'positive-definite']"
59,A linear space of degenerate matrices.,A linear space of degenerate matrices.,,"Denote by $M_n(\mathbb{R})$ the set of all $n$ by $n$ real matrices. It's a linear space over $\mathbb{R}$ . Can we find a nontrivial vector subspace in which all the matrices are degenerate? If possible, what is the maximum possible dimension of this subspace? I thought about the set of upper triangular matrices, where all the the numbers at a certain position on the diagonal line of all matrices are always zero. This meets the requirements, but I don't how to deal with the 'maximum' problem. Any hints? Thank you in advance.","Denote by the set of all by real matrices. It's a linear space over . Can we find a nontrivial vector subspace in which all the matrices are degenerate? If possible, what is the maximum possible dimension of this subspace? I thought about the set of upper triangular matrices, where all the the numbers at a certain position on the diagonal line of all matrices are always zero. This meets the requirements, but I don't how to deal with the 'maximum' problem. Any hints? Thank you in advance.",M_n(\mathbb{R}) n n \mathbb{R},"['linear-algebra', 'matrices', 'vector-spaces']"
60,Integration by substitution in different dimensions,Integration by substitution in different dimensions,,"I want to solve a specific integral, by using substitution. As it is too specific to describe my situation and probably also not of general interest, let me give a toy example. Let $\overline{\Omega} \subseteq \mathbb{R}^3$ and $\Omega \subseteq \mathbb{R}^2$ be two domains of different dimension. Note that the intrinsic dimension of $\overline{\Omega}$ is two, although the ambient space is three dimensional. Furthermore, I know a bijective map $\varphi: \overline{\Omega} \rightarrow \Omega$ . We may assume that all partial derivatives of $\varphi$ exist. I want to solve the following integral as follows: $$\int_{x\in \Omega} f(x)dx = \int_{x\in \varphi(\overline{\Omega})} f(x)dx = \int_{y\in \overline{\Omega}} f(\varphi(y)) \ ? ? ? \ dy.$$ Where I have written the three question marks, there should be a dependence on $\varphi$ . According to Wikipedia, if $\varphi$ would be a function from $\mathbb{R}^n$ to $\mathbb{R}^n$ , I should take the absolute value of the determinant of the Jacobian.  ( https://en.wikipedia.org/wiki/Integration_by_substitution ) But $\varphi'$ is not a square matrix. So something else needs to be done. I feel there should be a general theorem that one can look up if one knows integrals better. Question 1: What should be at the three question marks? Question 2: Can someone give me a citeable source? many thanks Till","I want to solve a specific integral, by using substitution. As it is too specific to describe my situation and probably also not of general interest, let me give a toy example. Let and be two domains of different dimension. Note that the intrinsic dimension of is two, although the ambient space is three dimensional. Furthermore, I know a bijective map . We may assume that all partial derivatives of exist. I want to solve the following integral as follows: Where I have written the three question marks, there should be a dependence on . According to Wikipedia, if would be a function from to , I should take the absolute value of the determinant of the Jacobian.  ( https://en.wikipedia.org/wiki/Integration_by_substitution ) But is not a square matrix. So something else needs to be done. I feel there should be a general theorem that one can look up if one knows integrals better. Question 1: What should be at the three question marks? Question 2: Can someone give me a citeable source? many thanks Till",\overline{\Omega} \subseteq \mathbb{R}^3 \Omega \subseteq \mathbb{R}^2 \overline{\Omega} \varphi: \overline{\Omega} \rightarrow \Omega \varphi \int_{x\in \Omega} f(x)dx = \int_{x\in \varphi(\overline{\Omega})} f(x)dx = \int_{y\in \overline{\Omega}} f(\varphi(y)) \ ? ? ? \ dy. \varphi \varphi \mathbb{R}^n \mathbb{R}^n \varphi',"['integration', 'matrices', 'derivatives', 'determinant', 'substitution']"
61,"find $\max \left\{ \operatorname{rank}((A+B)^2) : A,B \in \mathbb R^{2n,2n} \wedge \operatorname{rank} A + \operatorname{rank} B \le n \right\}$",find,"\max \left\{ \operatorname{rank}((A+B)^2) : A,B \in \mathbb R^{2n,2n} \wedge \operatorname{rank} A + \operatorname{rank} B \le n \right\}","how to find: $\max \left\{  \operatorname{rank}((A+B)^2) : A,B \in \mathbb R^{2n,2n} \wedge  \operatorname{rank} A +  \operatorname{rank} B \le n  \right\}$ I assume that there is needed to prove some additional theorems. One of them I think that I got: $$ \operatorname{rank} A^2 \le  \operatorname{rank} A$$ Take $x \in \ker A$ then $Ax=0$ Ok, now let's multiplicate from the left side: $$ A^2x=A\cdot 0 = 0 $$ so $$\ker A \subset \ker A^2$$ and the result follows. But I don't know how to deal with sum of $A,B$ :( $$ $$","how to find: I assume that there is needed to prove some additional theorems. One of them I think that I got: Take then Ok, now let's multiplicate from the left side: so and the result follows. But I don't know how to deal with sum of :(","\max \left\{  \operatorname{rank}((A+B)^2) : A,B \in \mathbb R^{2n,2n} \wedge  \operatorname{rank} A +  \operatorname{rank} B \le n  \right\}  \operatorname{rank} A^2 \le  \operatorname{rank} A x \in \ker A Ax=0  A^2x=A\cdot 0 = 0  \ker A \subset \ker A^2 A,B  ",['linear-algebra']
62,Matrix Multiplication on Riemannian Manifolds,Matrix Multiplication on Riemannian Manifolds,,"I am having a hard time understanding the concept of matrix (and / or vector) multiplication on a Riemannian Manifold $(M, g)$ . On $\mathbb R^n $ we can multiply a matrix for a vector in the usual way. How do I translate that on $M$ ? The naive way would be to just do the multiplication on the local coordinates, but this entirely disregards the metric, which seems wrong. Is the matrix multiplication something that lives on $T_vM$ ? Intuitively yes, but why?","I am having a hard time understanding the concept of matrix (and / or vector) multiplication on a Riemannian Manifold . On we can multiply a matrix for a vector in the usual way. How do I translate that on ? The naive way would be to just do the multiplication on the local coordinates, but this entirely disregards the metric, which seems wrong. Is the matrix multiplication something that lives on ? Intuitively yes, but why?","(M, g) \mathbb R^n  M T_vM","['matrices', 'riemannian-geometry']"
63,Can't understand step in LU decomposition proof,Can't understand step in LU decomposition proof,,"UPDATE : The author of the linked article has updated the proof and now it's perfectly clear. I'm reading about the LU decomposition on this page and cannot understand one of the final steps in the proof to the following: Let $A$ be a $K\times K$ matrix. Then, there exists a permutation matrix $P$ such that $PA$ has an LU decomposition: $$PA=LU$$ where $L$ is a lower triangular $K\times K$ matrix and $U$ is an upper triangular $K\times K$ matrix. I'll reproduce the proof here: Through Gaussian elimination, $A$ can be reduced to a row-echelon (hence upper triangular) matrix $U$ via a series of $n$ elementary operations: $$E_n\bullet\ldots\bullet E_1\bullet A = U$$ Any elementary matrix $E_i$ used in Gaussian elimination is either a permutation matrix $P_i$ or a matrix $L_i$ used to add a multiple of one row to a row below it. Thus, $L_i$ will be lower triangular with non-zero diagonal $\implies$ invertible. Suppose that the first permutation matrix we encounter is in position $j$ , so that we have: $$E_n\bullet\ldots\bullet E_{j+1}\bullet P_j\bullet L_{j-1}\bullet\ldots\bullet L_1\bullet A = U$$ Since a permutation matrix is orthogonal, $P_j^T P_j = I$ and hence $$E_n\bullet\ldots\bullet E_{j+1}\bullet P_j\bullet L_{j-1}\bullet P_j^TP_j \bullet\ldots\bullet P_j^TP_j\bullet L_1\bullet P_j^TP_j\bullet A = U$$ or $$E_n\bullet\ldots\bullet E_{j+1}\bullet\tilde{L}_{j-1}\bullet\ldots\bullet\tilde{L}_1\bullet P_j\bullet A = U$$ for $i=1,\ldots,j-1$ . A matrix $L_i$ , used to add $\alpha$ times the $i$ -th row to the $s$ -th row (in this case $s>i$ ), can be written as a rank one update to the identity matrix: $L_i=I+M$ , where $M$ is a matrix whose entries are all zeros except $M_{si} = \alpha$ . We have that $$\tilde{L}_i = P_jL_iP_j^T = P_j(I+M)P_j^T = P_jP_j^T + P_jMP_j^T = I+P_jMP_j^T$$ So far so good. Then we have: The permutations performed on the rows and columns of $M$ by $P_j$ and $P_j^T$ can move the only non-zero element of $M$ , but that element remains below the main diagonal (because $j>i$ ). I can't understand this. $M$ is derived from $L_i$ , which means that $M_{si} = \alpha \neq 0$ and all other elements of $M$ are $0$ . Nothing has been said about $P_j$ . I'm guessing $P_j$ swaps $j$ -th row with $r$ -th row, where $r>j$ . What do the indices $s,i$ have anything to do with the indices $r,j$ ? Would be grateful if anyone could clear this up!","UPDATE : The author of the linked article has updated the proof and now it's perfectly clear. I'm reading about the LU decomposition on this page and cannot understand one of the final steps in the proof to the following: Let be a matrix. Then, there exists a permutation matrix such that has an LU decomposition: where is a lower triangular matrix and is an upper triangular matrix. I'll reproduce the proof here: Through Gaussian elimination, can be reduced to a row-echelon (hence upper triangular) matrix via a series of elementary operations: Any elementary matrix used in Gaussian elimination is either a permutation matrix or a matrix used to add a multiple of one row to a row below it. Thus, will be lower triangular with non-zero diagonal invertible. Suppose that the first permutation matrix we encounter is in position , so that we have: Since a permutation matrix is orthogonal, and hence or for . A matrix , used to add times the -th row to the -th row (in this case ), can be written as a rank one update to the identity matrix: , where is a matrix whose entries are all zeros except . We have that So far so good. Then we have: The permutations performed on the rows and columns of by and can move the only non-zero element of , but that element remains below the main diagonal (because ). I can't understand this. is derived from , which means that and all other elements of are . Nothing has been said about . I'm guessing swaps -th row with -th row, where . What do the indices have anything to do with the indices ? Would be grateful if anyone could clear this up!","A K\times K P PA PA=LU L K\times K U K\times K A U n E_n\bullet\ldots\bullet E_1\bullet A = U E_i P_i L_i L_i \implies j E_n\bullet\ldots\bullet E_{j+1}\bullet P_j\bullet L_{j-1}\bullet\ldots\bullet L_1\bullet A = U P_j^T P_j = I E_n\bullet\ldots\bullet E_{j+1}\bullet P_j\bullet L_{j-1}\bullet P_j^TP_j \bullet\ldots\bullet P_j^TP_j\bullet L_1\bullet P_j^TP_j\bullet A = U E_n\bullet\ldots\bullet E_{j+1}\bullet\tilde{L}_{j-1}\bullet\ldots\bullet\tilde{L}_1\bullet P_j\bullet A = U i=1,\ldots,j-1 L_i \alpha i s s>i L_i=I+M M M_{si} = \alpha \tilde{L}_i = P_jL_iP_j^T = P_j(I+M)P_j^T = P_jP_j^T + P_jMP_j^T = I+P_jMP_j^T M P_j P_j^T M j>i M L_i M_{si} = \alpha \neq 0 M 0 P_j P_j j r r>j s,i r,j","['linear-algebra', 'matrices']"
64,"Find rank of AB, given that A has linearly independent columns and B has rank 2","Find rank of AB, given that A has linearly independent columns and B has rank 2",,"I'm trying to prove to myself that given... Matrix A, which has linearly independent columns, and at least 2 columns... Matrix B, which has rank of 2 Their product, AB, will have rank of 2. I believe this is because... Matrix B has two linearly independent columns. Each column of AB will be a combination of the columns of A When multiplying matrix A by matrix B, each of the two independent columns of B will create a unique combination of the columns in A. Is this true? If so, can this be made more rigorous? Thanks for the help in advance!","I'm trying to prove to myself that given... Matrix A, which has linearly independent columns, and at least 2 columns... Matrix B, which has rank of 2 Their product, AB, will have rank of 2. I believe this is because... Matrix B has two linearly independent columns. Each column of AB will be a combination of the columns of A When multiplying matrix A by matrix B, each of the two independent columns of B will create a unique combination of the columns in A. Is this true? If so, can this be made more rigorous? Thanks for the help in advance!",,"['linear-algebra', 'matrices', 'matrix-rank']"
65,"If $Q_kR_k$ converges to $QR$, where this represents their respective $QR$ decompositions, then $Q_k \rightarrow Q$ and $R_k \rightarrow R$?","If  converges to , where this represents their respective  decompositions, then  and ?",Q_kR_k QR QR Q_k \rightarrow Q R_k \rightarrow R,"Suppose $Q_kR_k \rightarrow QR$ as $k \rightarrow \infty$ , where $Q_k, Q$ are orthogonal matrices and $R_k, R$ are upper triangular with positive diagonal entries, then would the uniqueness of the $QR$ decomposition imply that $Q_k \rightarrow Q$ and $R_k \rightarrow R?$ I need this detail for a proof, but I wasn't able to prove it. It would suffice to show that if $Q_kR_k \rightarrow I$ , then $Q_k \rightarrow I$ and $R_k \rightarrow I$ . Edit: If the limit of $Q_k$ and $R_k$ exist, then they must be $I$ , but how would one show that these limits exist, if they do?","Suppose as , where are orthogonal matrices and are upper triangular with positive diagonal entries, then would the uniqueness of the decomposition imply that and I need this detail for a proof, but I wasn't able to prove it. It would suffice to show that if , then and . Edit: If the limit of and exist, then they must be , but how would one show that these limits exist, if they do?","Q_kR_k \rightarrow QR k \rightarrow \infty Q_k, Q R_k, R QR Q_k \rightarrow Q R_k \rightarrow R? Q_kR_k \rightarrow I Q_k \rightarrow I R_k \rightarrow I Q_k R_k I","['matrices', 'matrix-decomposition']"
66,$Av=\lambda v \Rightarrow A^*v=\bar \lambda v$ (general case),(general case),Av=\lambda v \Rightarrow A^*v=\bar \lambda v,"Suppose $V$ is a finite-dimensional complex inner product space and $v_1,v_2,...,v_3$ is an orthonormal basis of $V$ . Define $A:V \to V$ by $Av_i=\lambda_i v_i$ for some $\lambda_i \in \mathbb{C}.$ Show that $A^*v_i=\bar \lambda v_i.$ My attempt: $(Av_i,v_i)=(\lambda_iv_i,v_i)=(v_i,\bar\lambda_i v_i)=(v_i,A^*v_i)$ $\Rightarrow(v_i,(A^*-\bar \lambda_i)v_i)=0 \Rightarrow (A^*-\bar \lambda_i)v_i \perp v_i $ How to show that $(A^*-\bar\lambda_i)v_i=0$ ?",Suppose is a finite-dimensional complex inner product space and is an orthonormal basis of . Define by for some Show that My attempt: How to show that ?,"V v_1,v_2,...,v_3 V A:V \to V Av_i=\lambda_i v_i \lambda_i \in \mathbb{C}. A^*v_i=\bar \lambda v_i. (Av_i,v_i)=(\lambda_iv_i,v_i)=(v_i,\bar\lambda_i v_i)=(v_i,A^*v_i) \Rightarrow(v_i,(A^*-\bar \lambda_i)v_i)=0 \Rightarrow (A^*-\bar \lambda_i)v_i \perp v_i  (A^*-\bar\lambda_i)v_i=0","['linear-algebra', 'matrices', 'adjoint-operators']"
67,Proving that $det(A) \ne 0$ with $A$ satisfying following conditions.,Proving that  with  satisfying following conditions.,det(A) \ne 0 A,"I am given $A \in M_n(\mathbb{R})$ which satisfies the following conditions. $A_{i,i} \gt 0$ for all $1 \le i \le n$ $A_{i,j} \le 0$ for all distinct $1 \le i, j \le n$ $\sum_{j=1}^n A_{i,j} \gt 0$ for all $1 \le i \le n$ Then, I am supposed to show that $det(A) \ne 0$ Now, I am frankly not sure where to even start. However, I was given the following hint: If not, there is a non-zero solution of $Ax = 0$ . If $x_i$ has largest absolute value, show that the $i$ th linear equation from $Ax=0$ leads to a contradiction. I don't really quite get how to apply this hint either. Could someone help? Thanks.","I am given which satisfies the following conditions. for all for all distinct for all Then, I am supposed to show that Now, I am frankly not sure where to even start. However, I was given the following hint: If not, there is a non-zero solution of . If has largest absolute value, show that the th linear equation from leads to a contradiction. I don't really quite get how to apply this hint either. Could someone help? Thanks.","A \in M_n(\mathbb{R}) A_{i,i} \gt 0 1 \le i \le n A_{i,j} \le 0 1 \le i, j \le n \sum_{j=1}^n A_{i,j} \gt 0 1 \le i \le n det(A) \ne 0 Ax = 0 x_i i Ax=0","['linear-algebra', 'abstract-algebra', 'matrices', 'determinant']"
68,Condition number bound using block LU factorization,Condition number bound using block LU factorization,,"Let $A \in \mathbb{C}^{n \times n}$ be invertible and $P$ be a permutation matrix such that $$PA = \begin{pmatrix}A_{11} & A_{12} \\ A_{21} & A_{22} \end{pmatrix} $$ Where $A_{11} \in \mathbb{C}^{n \times n}$ , such that $1 \leq k < n$ , is also invertible. I want to show that if all the elements of $A_{21}A_{11}^{-1}$ have an absolute value that is less than or equal to one, then $K_{\infty}(A_{22}-A_{21}A_{11}^{-1}A_{12}) \leq n K_{\infty}(A)$ Where $K_{\infty}(X) = ||X||_{\infty}||X^{-1}||_{\infty} $ is the condition number of $X$ with respect to the infinity norm. $\textbf{Proof Attempt:}$ I believe that I need to write $PA$ in terms of its $LU$ factorization, such that $$\begin{pmatrix}A_{11} & A_{12} \\ A_{21} & A_{22} \end{pmatrix} = \begin{pmatrix}L_{11} & 0 \\ L_{21} & L_{22} \end{pmatrix}\begin{pmatrix}U_{11} & U_{12} \\ 0 & U_{22} \end{pmatrix}$$ This gives $$A_{11}=L_{11}U_{11} \\   A_{12}=L_{11}U_{12} \\   A_{21}=L_{21}U_{11} \\   A_{22}=L_{21}U_{12} + L_{22}U_{22} $$ This implies $$K_{\infty}(A_{22}-A_{21}A_{11}^{-1}A_{12}) = K_{\infty}(L_{22}U_{12})$$ By the hypothesis, we can also say $||A_{21}A_{11}^{-1}||_{\infty} = ||L_{21}L_{11}^{-1}||_{\infty} < n$ , by our assumption and because the infinity norm is equivalent to the maximum absolute row sum. This is the point where I get stuck, since I don't see how I can get the inequality. I don't see how to to apply the property of the matrix A_{21}A_{11}^{-1} to the problem.","Let be invertible and be a permutation matrix such that Where , such that , is also invertible. I want to show that if all the elements of have an absolute value that is less than or equal to one, then Where is the condition number of with respect to the infinity norm. I believe that I need to write in terms of its factorization, such that This gives This implies By the hypothesis, we can also say , by our assumption and because the infinity norm is equivalent to the maximum absolute row sum. This is the point where I get stuck, since I don't see how I can get the inequality. I don't see how to to apply the property of the matrix A_{21}A_{11}^{-1} to the problem.","A \in \mathbb{C}^{n \times n} P PA = \begin{pmatrix}A_{11} & A_{12} \\ A_{21} & A_{22} \end{pmatrix}  A_{11} \in \mathbb{C}^{n \times n} 1 \leq k < n A_{21}A_{11}^{-1} K_{\infty}(A_{22}-A_{21}A_{11}^{-1}A_{12}) \leq n K_{\infty}(A) K_{\infty}(X) = ||X||_{\infty}||X^{-1}||_{\infty}  X \textbf{Proof Attempt:} PA LU \begin{pmatrix}A_{11} & A_{12} \\ A_{21} & A_{22} \end{pmatrix} = \begin{pmatrix}L_{11} & 0 \\ L_{21} & L_{22} \end{pmatrix}\begin{pmatrix}U_{11} & U_{12} \\ 0 & U_{22} \end{pmatrix} A_{11}=L_{11}U_{11} \\
  A_{12}=L_{11}U_{12} \\
  A_{21}=L_{21}U_{11} \\
  A_{22}=L_{21}U_{12} + L_{22}U_{22}  K_{\infty}(A_{22}-A_{21}A_{11}^{-1}A_{12}) = K_{\infty}(L_{22}U_{12}) ||A_{21}A_{11}^{-1}||_{\infty} = ||L_{21}L_{11}^{-1}||_{\infty} < n","['linear-algebra', 'matrices', 'normed-spaces', 'numerical-linear-algebra', 'matrix-decomposition']"
69,Question related to the trace and the commutator of matrices,Question related to the trace and the commutator of matrices,,"Let $K$ be any field and $n\in \mathbb N$ . For every $A\in M_n(K)$ , define a linear form $\lambda_A: M_n(K) \rightarrow K$ by sending the matrix $M$ to $\lambda_A(M):= \operatorname{Tr}(AM)$ . The map $\lambda: M_n(K) \rightarrow M_n(K)^*$ defined this way is an isomorphism, as can be seen from injectivity and equality of dimensions (which are finite). The question is the following. Let $A, B\in M_n(K)$ be two matrices such that for every $M\in M_n(K)$ that commutes with $A$ (that is, such that $AM=MA$ ), we have $\lambda_B(M)=0$ . Prove that $B=AC-CA$ for some matrix $C\in M_n(K)$ . The reciprocal being clearly true, the statement above in fact exactly characterizes all the matrices that can be written as $AC-CA$ for some matrix $C\in M_n(K)$ . As a preliminary question, I managed to show that $$\operatorname{Ker}(\operatorname{Tr})=\operatorname{Span}\{AB-BA\,|\,A,B\in M_n(K)\}$$ The inclusion of the right-hand space - which I call $E$ - inside the kernel is indeed clear. However, it may not be clear that the right-hand space is a hyperplane. If we assumed it were not, we may find another hyperplane $H$ such that $E\subset H$ , and $H\not = \operatorname{Ker}(\operatorname{Tr})$ . The hyperplane $H$ is the kernel of some linear form which is linearly independant on $\operatorname{Tr}$ . It can be uniquely written as $\lambda_X$ for some matrix $X\in M_n(K)$ which is not a multiple of the identity matrix $I_n$ . However, the condition that $\lambda_X$ vanishes on $E$ implies that $X$ is a multiple of the identity, so we get a contradiction. Hence, $E$ is a hyperplane and so by equality of dimensions, we must have the equality above (note that the kernel of $\operatorname{Tr}$ is never trivial). I have been trying to solve the problem using this result, however I can't progress further. Taking $M=I_n$ , we may express $B$ as $XY-YX$ for some matrices $X,Y \in M_n(K)$ . Considering $A$ , we may also express $BA$ as a commutator. But the problem is that we have no idea what these commutators would look like. In particular, I can't think of any way to prove that one of the matrices could be $A$ . Could you please give me any hint/advices that could lead me to the solution?","Let be any field and . For every , define a linear form by sending the matrix to . The map defined this way is an isomorphism, as can be seen from injectivity and equality of dimensions (which are finite). The question is the following. Let be two matrices such that for every that commutes with (that is, such that ), we have . Prove that for some matrix . The reciprocal being clearly true, the statement above in fact exactly characterizes all the matrices that can be written as for some matrix . As a preliminary question, I managed to show that The inclusion of the right-hand space - which I call - inside the kernel is indeed clear. However, it may not be clear that the right-hand space is a hyperplane. If we assumed it were not, we may find another hyperplane such that , and . The hyperplane is the kernel of some linear form which is linearly independant on . It can be uniquely written as for some matrix which is not a multiple of the identity matrix . However, the condition that vanishes on implies that is a multiple of the identity, so we get a contradiction. Hence, is a hyperplane and so by equality of dimensions, we must have the equality above (note that the kernel of is never trivial). I have been trying to solve the problem using this result, however I can't progress further. Taking , we may express as for some matrices . Considering , we may also express as a commutator. But the problem is that we have no idea what these commutators would look like. In particular, I can't think of any way to prove that one of the matrices could be . Could you please give me any hint/advices that could lead me to the solution?","K n\in \mathbb N A\in M_n(K) \lambda_A: M_n(K) \rightarrow K M \lambda_A(M):= \operatorname{Tr}(AM) \lambda: M_n(K) \rightarrow M_n(K)^* A, B\in M_n(K) M\in M_n(K) A AM=MA \lambda_B(M)=0 B=AC-CA C\in M_n(K) AC-CA C\in M_n(K) \operatorname{Ker}(\operatorname{Tr})=\operatorname{Span}\{AB-BA\,|\,A,B\in M_n(K)\} E H E\subset H H\not = \operatorname{Ker}(\operatorname{Tr}) H \operatorname{Tr} \lambda_X X\in M_n(K) I_n \lambda_X E X E \operatorname{Tr} M=I_n B XY-YX X,Y \in M_n(K) A BA A","['linear-algebra', 'matrices']"
70,What is the greatest possible number of nonzero terms in a the determinant of a matrix with exactly $N$ zeroes?,What is the greatest possible number of nonzero terms in a the determinant of a matrix with exactly  zeroes?,N,"Suppose we have an $n \times n$ matrix and we know there are exactly $N$ zero entries. The determinant is the sum of $n!$ terms, each formed by selecting exactly one entry from each row and column and multiplying them. If a given term comes from selecting one or more zeroes then that term becomes zero. This raises the question of how many of the terms can be nonzero. Intuitively I imagine the maximum numbers are achieved if we take a matrix of all $1$ s and start putting in zeroes, starting with entries $(1,2), (1,3), \ldots, (1,n)$ so the only nonzero entry in that row is at $(1,1)$ , and then moving to the next row and putting zeroes in $(2,1), (2,3), \ldots, (2,n)$ so the only nonzero entry in that row is at $(2,2)$ , and so on until we're filling in zeroes on the last row at $(n,1), (n,2), \ldots, (n,n-1)$ . At this stage we have the identity matrix and there is exactly one nonzero term. I imagine this has already been proved somewhere but cannot find a good reference. Could someone please provide a proof, or better yet, a book where this is proved as part of some wider theory? Ideally by something more elegant than induction?","Suppose we have an matrix and we know there are exactly zero entries. The determinant is the sum of terms, each formed by selecting exactly one entry from each row and column and multiplying them. If a given term comes from selecting one or more zeroes then that term becomes zero. This raises the question of how many of the terms can be nonzero. Intuitively I imagine the maximum numbers are achieved if we take a matrix of all s and start putting in zeroes, starting with entries so the only nonzero entry in that row is at , and then moving to the next row and putting zeroes in so the only nonzero entry in that row is at , and so on until we're filling in zeroes on the last row at . At this stage we have the identity matrix and there is exactly one nonzero term. I imagine this has already been proved somewhere but cannot find a good reference. Could someone please provide a proof, or better yet, a book where this is proved as part of some wider theory? Ideally by something more elegant than induction?","n \times n N n! 1 (1,2), (1,3), \ldots, (1,n) (1,1) (2,1), (2,3), \ldots, (2,n) (2,2) (n,1), (n,2), \ldots, (n,n-1)","['matrices', 'reference-request', 'determinant']"
71,Which relationship between trace and determinant is established using density?,Which relationship between trace and determinant is established using density?,,"I read in some lecture notes that ""as an example for the intersection between linear algebra and calculus, one can establish the relationship between trace and determinant of a matrix using a density-argument"". Which relationship is meant? And what would the argument be?","I read in some lecture notes that ""as an example for the intersection between linear algebra and calculus, one can establish the relationship between trace and determinant of a matrix using a density-argument"". Which relationship is meant? And what would the argument be?",,"['linear-algebra', 'matrices', 'analysis', 'determinant', 'trace']"
72,What does oplus symbol  do for 2 images in Convolution Neural Networks,What does oplus symbol  do for 2 images in Convolution Neural Networks,,"So I'm reading this paper on optical flow prediction from two image frames, and I'm having a difficult time finding what this operator does. This paper, and some other ones uses it on the outputs of convolution neural networks. http://openaccess.thecvf.com/content_cvpr_2017/papers/Ranjan_Optical_Flow_Estimation_CVPR_2017_paper.pdf I looked up math symbols and saw that it was an xor operator in logic, but that doesn't make any sense to me in this context. I would also like to know what to call this symbol for future reference.","So I'm reading this paper on optical flow prediction from two image frames, and I'm having a difficult time finding what this operator does. This paper, and some other ones uses it on the outputs of convolution neural networks. http://openaccess.thecvf.com/content_cvpr_2017/papers/Ranjan_Optical_Flow_Estimation_CVPR_2017_paper.pdf I looked up math symbols and saw that it was an xor operator in logic, but that doesn't make any sense to me in this context. I would also like to know what to call this symbol for future reference.",,"['matrices', 'image-processing', 'neural-networks']"
73,Rank comparison for different low rank approximations,Rank comparison for different low rank approximations,,"I want to approximate a correlation matrix by low-rank components such that $$\Sigma \approx \sum_{i=1}^{k_1} \sigma_ib_ib_i^T$$ where $\Sigma$ is of size $n \times n$ , $b$ is a $n$ dimensional vector and $k$ is the number of components. I was wondering if subtracting a diagonal matrix $D \in R^{n\times n}$ from the matrix would give a better low rank approximation, below is the decomposition $$\Sigma \approx D + \sum_{i=1}^{k_2} \sigma_{i}^{'} b_{i}^{'}b_i^{'T}$$ Can we estimate $D, \sigma, b$ such that $k_2 \leq k_1$ holds true? Both the above approximations have different $\sigma$ and $b$ . Also, I know that the $\Sigma$ matrix has non-zero diagonal entries and the matrix after removing the diagonal term is sparse. Edit 1: I was thinking of simulating correlation matrix and then test the above hypothesis. Is there is a way to simulate a low rank, sparse and full diagonal matrix? I have asked this question here https://stats.stackexchange.com/questions/368868/simulation-of-low-rank-and-sparse-matrix","I want to approximate a correlation matrix by low-rank components such that where is of size , is a dimensional vector and is the number of components. I was wondering if subtracting a diagonal matrix from the matrix would give a better low rank approximation, below is the decomposition Can we estimate such that holds true? Both the above approximations have different and . Also, I know that the matrix has non-zero diagonal entries and the matrix after removing the diagonal term is sparse. Edit 1: I was thinking of simulating correlation matrix and then test the above hypothesis. Is there is a way to simulate a low rank, sparse and full diagonal matrix? I have asked this question here https://stats.stackexchange.com/questions/368868/simulation-of-low-rank-and-sparse-matrix","\Sigma \approx \sum_{i=1}^{k_1} \sigma_ib_ib_i^T \Sigma n \times n b n k D \in R^{n\times n} \Sigma \approx D + \sum_{i=1}^{k_2} \sigma_{i}^{'} b_{i}^{'}b_i^{'T} D, \sigma, b k_2 \leq k_1 \sigma b \Sigma","['linear-algebra', 'matrices', 'numerical-methods', 'numerical-linear-algebra', 'matrix-decomposition']"
74,How to find lines of invariant points?,How to find lines of invariant points?,,"Every time I try a question on this topic I get it wrong. My textbook says: Invariant points satisfy $B\begin{pmatrix}u\\ v\end{pmatrix}=\begin{pmatrix}u\\ v\end{pmatrix}$ Re-write this as a system of equations. Check whether both equations are in fact the same. If so, they give a line of invariant points. So I tried this question using that method (and the method needed for finding invariant lines): For [this] matrix, find any lines of invariant points and any other invariant lines through the origin. $\begin{pmatrix}3&-2\\ 4&-3\end{pmatrix}$ What I did was: $\begin{pmatrix}3&-2\\ 4&-3\end{pmatrix}\begin{pmatrix}u\\ mu\end{pmatrix}=\begin{pmatrix}u\\ v\end{pmatrix}=\begin{pmatrix}3u-2mu\\ 4u-3mu\end{pmatrix}$ These equations are not the same, so no lines of invariant points. Then, to find invariant lines: $\begin{pmatrix}3&-2\\ 4&-3\end{pmatrix}\begin{pmatrix}u\\ mu\end{pmatrix}=\begin{pmatrix}u'\\ v'\end{pmatrix}=\begin{pmatrix}3u-2mu\\ 4u-3mu\end{pmatrix}$ $4u-3mu=m\left(3u-2mu\right)$ $2m^2-6m+4\:=0\:\Rightarrow \:m\:=1\: $ or $ \:m=2$ So the invariant lines should be $y=2x$ or $y=x$ . As far as I know this is sort of right, except $y=x$ is apparently also a line of invariant points. What did I do wrong?","Every time I try a question on this topic I get it wrong. My textbook says: Invariant points satisfy Re-write this as a system of equations. Check whether both equations are in fact the same. If so, they give a line of invariant points. So I tried this question using that method (and the method needed for finding invariant lines): For [this] matrix, find any lines of invariant points and any other invariant lines through the origin. What I did was: These equations are not the same, so no lines of invariant points. Then, to find invariant lines: or So the invariant lines should be or . As far as I know this is sort of right, except is apparently also a line of invariant points. What did I do wrong?",B\begin{pmatrix}u\\ v\end{pmatrix}=\begin{pmatrix}u\\ v\end{pmatrix} \begin{pmatrix}3&-2\\ 4&-3\end{pmatrix} \begin{pmatrix}3&-2\\ 4&-3\end{pmatrix}\begin{pmatrix}u\\ mu\end{pmatrix}=\begin{pmatrix}u\\ v\end{pmatrix}=\begin{pmatrix}3u-2mu\\ 4u-3mu\end{pmatrix} \begin{pmatrix}3&-2\\ 4&-3\end{pmatrix}\begin{pmatrix}u\\ mu\end{pmatrix}=\begin{pmatrix}u'\\ v'\end{pmatrix}=\begin{pmatrix}3u-2mu\\ 4u-3mu\end{pmatrix} 4u-3mu=m\left(3u-2mu\right) 2m^2-6m+4\:=0\:\Rightarrow \:m\:=1\:   \:m=2 y=2x y=x y=x,"['matrices', 'geometry', 'linear-transformations']"
75,Making a matrix as sparse as possible,Making a matrix as sparse as possible,,"Consider a matrix $A \in \mathbb{R}^{m \times n}$ where $n >> m$. In other words, $A$ has much more columns than rows. Also, consider we are given a fixed number (integer) $m \leq r < n$. I'm trying to find a matrix $B \in \mathbb{R}^{r \times m}$ such that $B' B = I_m$ (i.e., $B'$ is the left inverse of $B$) and $BA$ is very sparse, as much as possible. My first idea was to consider the case $r = m$ and take the $QR$ factorization of $A$. Then set $B = Q^T$ to get  $$BA = Q^TA = Q^TQR = I_mR = R,$$  a triangular (and rectangular) matrix. This matrix has a few zeros in the first $m$ columns, but since $m$ is much smaller than $n$, there are a lot of non zero terms left. Also, in my context usually $r$ will be bigger than $m$ and smaller than $n$, so I don't want to choose specif values for $r$. I really want a general approach that maximizes the number of zero entries in $BA$ for a generic $r$ between $m$ and $n$. I wonder if there is a better matrix to do the job. Hope you can help me. Thank you.","Consider a matrix $A \in \mathbb{R}^{m \times n}$ where $n >> m$. In other words, $A$ has much more columns than rows. Also, consider we are given a fixed number (integer) $m \leq r < n$. I'm trying to find a matrix $B \in \mathbb{R}^{r \times m}$ such that $B' B = I_m$ (i.e., $B'$ is the left inverse of $B$) and $BA$ is very sparse, as much as possible. My first idea was to consider the case $r = m$ and take the $QR$ factorization of $A$. Then set $B = Q^T$ to get  $$BA = Q^TA = Q^TQR = I_mR = R,$$  a triangular (and rectangular) matrix. This matrix has a few zeros in the first $m$ columns, but since $m$ is much smaller than $n$, there are a lot of non zero terms left. Also, in my context usually $r$ will be bigger than $m$ and smaller than $n$, so I don't want to choose specif values for $r$. I really want a general approach that maximizes the number of zero entries in $BA$ for a generic $r$ between $m$ and $n$. I wonder if there is a better matrix to do the job. Hope you can help me. Thank you.",,"['linear-algebra', 'matrices', 'linear-transformations', 'sparse-matrices']"
76,Why is the spectral norm of a weighting matrix for a Hopfield network less than or equal to $1$?,Why is the spectral norm of a weighting matrix for a Hopfield network less than or equal to ?,1,"We define weighting matrix $W$ as follows $$W := \frac{1}{Md}\left[\sum_{m=1}^{M}x^{(m)}(x^{(m)})^{T}\right] - \frac 1d \Bbb I_d$$ where $\Bbb I_d$ is the $d \times d$ identity matrix and $x^{(m)} \in \{\pm 1\}^d$. Why is $\| W \|_2 \leq 1$? For reference, the claim that $\| W \|_2 \leq 1$ is made on page 4 here , on the bottom left.","We define weighting matrix $W$ as follows $$W := \frac{1}{Md}\left[\sum_{m=1}^{M}x^{(m)}(x^{(m)})^{T}\right] - \frac 1d \Bbb I_d$$ where $\Bbb I_d$ is the $d \times d$ identity matrix and $x^{(m)} \in \{\pm 1\}^d$. Why is $\| W \|_2 \leq 1$? For reference, the claim that $\| W \|_2 \leq 1$ is made on page 4 here , on the bottom left.",,"['linear-algebra', 'matrices']"
77,Smiths normal form is similar to $xI-A.$,Smiths normal form is similar to,xI-A.,"I am reading Rational Canonical form from The Abstract Algebra book by Dummit and Foote. I have some doubt in Smith normal form. Smiths normal for says for any $n\times n$ square matrix $A$ over an arbitrary field $F,$ $xI-A$ is equivalent to diagonal matrix in $F[x]$ whose diagonal elements are either $1$ or the invariant factors of the pair $(F^n,A)$. But after looking at other references it seems to me that $xI-A$ is not only equivalent but Similar to such diagonal matrix in $F[x].$  I can't understand how they are similar. I need some help to understand the similarity. And also I want to know if there are references for the Canonical form in the modern approach by what I mean using the results of modules over PID. Thank you.","I am reading Rational Canonical form from The Abstract Algebra book by Dummit and Foote. I have some doubt in Smith normal form. Smiths normal for says for any $n\times n$ square matrix $A$ over an arbitrary field $F,$ $xI-A$ is equivalent to diagonal matrix in $F[x]$ whose diagonal elements are either $1$ or the invariant factors of the pair $(F^n,A)$. But after looking at other references it seems to me that $xI-A$ is not only equivalent but Similar to such diagonal matrix in $F[x].$  I can't understand how they are similar. I need some help to understand the similarity. And also I want to know if there are references for the Canonical form in the modern approach by what I mean using the results of modules over PID. Thank you.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'modules', 'smith-normal-form']"
78,Classification of all matrix transformations on $\mathbb{R}^2$ plane,Classification of all matrix transformations on  plane,\mathbb{R}^2,"What is a complete exhaustive classification of all geometric transforms on the $\mathbb{R}^2$ plane obtained with: 2x2 matrices$$A = \pmatrix{a & b \\ c & d}$$ applied to a point $X= (x, y)$. 3x3 matrices $$A = \pmatrix{a & b& c \\ d & e & f \\ g & h & i}$$  applied to a point $(x, y)$ noted $X = (x, y, 1)$ using homogeneous coordinates. ? Note: I've already looked at Transformation matrix Wikipedia page, which is good, but it mainly gives examples (stretching, squeezing, etc.), and doesn't state it as a full classification of all possible transforms. In most lecture notes / resources I find, it usually goes this way: here is a list of geometric transforms => here are their representation as matrix In this question I'm more looking for: here is a random 3x3 matrix => what geometric transform is it? I'm looking for an exhaustive classification like ""All 2x2 matrices can be either a rotation matrix with parameter $\theta$, a scaling matrix of parameter $\lambda_1, \lambda_2$, a blahblah matrix of parameter $\delta$, or a composition of any 2 of them"" (nonsense, just an example).","What is a complete exhaustive classification of all geometric transforms on the $\mathbb{R}^2$ plane obtained with: 2x2 matrices$$A = \pmatrix{a & b \\ c & d}$$ applied to a point $X= (x, y)$. 3x3 matrices $$A = \pmatrix{a & b& c \\ d & e & f \\ g & h & i}$$  applied to a point $(x, y)$ noted $X = (x, y, 1)$ using homogeneous coordinates. ? Note: I've already looked at Transformation matrix Wikipedia page, which is good, but it mainly gives examples (stretching, squeezing, etc.), and doesn't state it as a full classification of all possible transforms. In most lecture notes / resources I find, it usually goes this way: here is a list of geometric transforms => here are their representation as matrix In this question I'm more looking for: here is a random 3x3 matrix => what geometric transform is it? I'm looking for an exhaustive classification like ""All 2x2 matrices can be either a rotation matrix with parameter $\theta$, a scaling matrix of parameter $\lambda_1, \lambda_2$, a blahblah matrix of parameter $\delta$, or a composition of any 2 of them"" (nonsense, just an example).",,"['linear-algebra', 'matrices', 'geometry', 'linear-transformations', 'euclidean-geometry']"
79,Matrix Notation Form of Roots of a Quadratic Equation,Matrix Notation Form of Roots of a Quadratic Equation,,"We know that the quadratic equation  $$f(x)=ax^2+bx+c=0$$  has roots  $$x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}=-\frac b{2a}\pm \frac 1a\sqrt{-\left(ac-\frac {b^2}4\right)}$$ Also, $f(x)$ can be written in matrix notation as follows: $$f(x)= \left(\begin{matrix}x&1\\\end{matrix}\right) \left(\begin{matrix}a&\frac b2\\\frac b2&c\end{matrix}\right) \left(\begin{matrix}x\\1\end{matrix}\right)=\mathbf{x^T Q x}$$ where the determinant of $\mathbf Q$ is $\left(ac-\frac {b^2}4\right)=-\frac 14\left(b^2-4ac\right)$, where coincidentally the familiar $(b^2-4ac)$ is the discriminant of the quadratic $f(x)$. Hence the roots of the quadratic $f(x)=0$ may be written as $$x=-\frac b{2a}\pm \frac 1a\sqrt{-\det(\mathbf Q)}$$ This is equivalent to $$\left(x+\frac b{2a}\right)^2=\frac {-\det(\mathbf Q)}{a^2}$$ Or in neater form,  $$\left(ax+\frac b{2}\right)^2={-\det(\mathbf Q)}$$ Question Can the roots of $f(x)=0$ be derived and written completely in matrix notation, given the link between the determinant and discriminant as shown above?","We know that the quadratic equation  $$f(x)=ax^2+bx+c=0$$  has roots  $$x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}=-\frac b{2a}\pm \frac 1a\sqrt{-\left(ac-\frac {b^2}4\right)}$$ Also, $f(x)$ can be written in matrix notation as follows: $$f(x)= \left(\begin{matrix}x&1\\\end{matrix}\right) \left(\begin{matrix}a&\frac b2\\\frac b2&c\end{matrix}\right) \left(\begin{matrix}x\\1\end{matrix}\right)=\mathbf{x^T Q x}$$ where the determinant of $\mathbf Q$ is $\left(ac-\frac {b^2}4\right)=-\frac 14\left(b^2-4ac\right)$, where coincidentally the familiar $(b^2-4ac)$ is the discriminant of the quadratic $f(x)$. Hence the roots of the quadratic $f(x)=0$ may be written as $$x=-\frac b{2a}\pm \frac 1a\sqrt{-\det(\mathbf Q)}$$ This is equivalent to $$\left(x+\frac b{2a}\right)^2=\frac {-\det(\mathbf Q)}{a^2}$$ Or in neater form,  $$\left(ax+\frac b{2}\right)^2={-\det(\mathbf Q)}$$ Question Can the roots of $f(x)=0$ be derived and written completely in matrix notation, given the link between the determinant and discriminant as shown above?",,"['matrices', 'determinant', 'quadratics', 'discriminant']"
80,"determine a orthogonal basis $(v_1,v_2,v_3)$ of $\mathbb R^3$, such that $Av_1,Av_2,Av_3$ are pairwise orthogonal","determine a orthogonal basis  of , such that  are pairwise orthogonal","(v_1,v_2,v_3) \mathbb R^3 Av_1,Av_2,Av_3","Let $A=  \begin{bmatrix} 1& 0& -4\\ 2& 1& 1\\ 2& -1& 1 \end{bmatrix} $ $A^TA=\begin{bmatrix} 9& 0& 0\\ 0& 2& 0\\ 0& 0& 18 \end{bmatrix}$ I am asked to determine a orthogonal basis $(v_1,v_2,v_3)$ of $\mathbb R^3$, such that $Av_1,Av_2,Av_3$ are pairwise orthogonal. If we take the columns of $A^TA$ as the basis $(v_1,v_2,v_3)$, they are obviously an orthogonal basis of $\mathbb R^3$  and we can calculate $A\begin{bmatrix} 9\\ 0 \\ 0  \end{bmatrix}=$$\begin{bmatrix} 9\\ 18 \\ 18  \end{bmatrix}$. $A\begin{bmatrix} 0\\ 0 \\ 18  \end{bmatrix}=$$\begin{bmatrix} -72\\ 18 \\ 18  \end{bmatrix}$ $A\begin{bmatrix} 0\\ 2 \\ 0  \end{bmatrix}=$$\begin{bmatrix} 0\\ 2 \\ -2  \end{bmatrix}$ These vectors are pairwise orthogonal again. Could someone explain why this works?","Let $A=  \begin{bmatrix} 1& 0& -4\\ 2& 1& 1\\ 2& -1& 1 \end{bmatrix} $ $A^TA=\begin{bmatrix} 9& 0& 0\\ 0& 2& 0\\ 0& 0& 18 \end{bmatrix}$ I am asked to determine a orthogonal basis $(v_1,v_2,v_3)$ of $\mathbb R^3$, such that $Av_1,Av_2,Av_3$ are pairwise orthogonal. If we take the columns of $A^TA$ as the basis $(v_1,v_2,v_3)$, they are obviously an orthogonal basis of $\mathbb R^3$  and we can calculate $A\begin{bmatrix} 9\\ 0 \\ 0  \end{bmatrix}=$$\begin{bmatrix} 9\\ 18 \\ 18  \end{bmatrix}$. $A\begin{bmatrix} 0\\ 0 \\ 18  \end{bmatrix}=$$\begin{bmatrix} -72\\ 18 \\ 18  \end{bmatrix}$ $A\begin{bmatrix} 0\\ 2 \\ 0  \end{bmatrix}=$$\begin{bmatrix} 0\\ 2 \\ -2  \end{bmatrix}$ These vectors are pairwise orthogonal again. Could someone explain why this works?",,['linear-algebra']
81,Calculating the Moore-Penrose inverse of an incidence matrix,Calculating the Moore-Penrose inverse of an incidence matrix,,"I'm trying to understand a method for calculating the Moore-Penrose inverse of an incidence matrix of a graph as outlined in ""Graphs and Matrices"" (you can find the book here: https://link.springer.com/book/10.1007/978-1-4471-6569-9 ). I can execute all the steps for calculating the inverse but I haven't yet fully understood why the method works: The line I don't understand is on page 20 and reads ""By Lemma 2.14 it follows that $Y=D'X$."" I don't see how Lemma 2.14 is connected to this statement and would appreciate it if anyone could help me understand this line. If you cannot access the book or don't want to get familiar with the notation the author uses, here's a summary of what I don't understand which hopefully includes all the relevant information: Summary of the problem The transpose of a matrix $A$ will be denoted by $A'$. Consider a connected directed graph $G$ with vertices $V=\left\{1,\ldots,n\right\}$ and edges $E=\left\{e_1,\ldots,e_m\right\}$ with the edges ordered in such a way that $\left\{e_1,\ldots,e_{n-1}\right\}$ form a spanning tree of $G$. Let $Q$ be the $n\times m$ incidence matrix of $G$. To cite ""Graphs and Matrices"", ""the rows and the columns of $Q$ are indexed by $V$ and $E$, respectively. The $\left(i,j\right)$- entry of $Q$ is $0$ if vertex $i$ and edge $e_j$ are not incident, and otherwise it is $1$ or $-1$ according as $e_j$ originates or terminates at $i$, respectively."" See the following example where the spanning tree is highlighted in red: Now, one wants to calculate the Moore-Penrose inverse $Q^+$ of $Q$ (which implies that the following four conditions hold: (1) $QQ^+Q=Q$, (2) $Q^+QQ^+=Q^+$, (3) $\left(QQ^+\right)'=QQ^+$, (4) $\left(Q^+Q\right)'=Q^+Q$). To do so, the author partitions $Q$ as $Q=\begin{bmatrix} 	U & V \end{bmatrix}$ where $U$ is $n\times\left(n-1\right)$ and $V$ is $n\times\left(m-n+1\right)$ and $Q^+$ as $Q^+=\begin{bmatrix} 	X \\ Y \end{bmatrix}$ where $X$ is $\left(n-1\right)\times n$ and $Y$ is $\left(m-n+1\right)\times n$. He then notes that ""there exists an $\left(n-1\right)\times\left(m-n+1\right)$ matrix $D$ such that $V=UD$."" I still understand why this is the case. However, I'm stuck at the next sentence, which says ""By Lemma 2.14 it follows that $Y=D'X$."" Lemma 2.14 says: If $A$ is an $m\times n$ matrix, then for an $n\times1$ vector $x$,   $Ax=0$ if and only if $x'A^+=0$. I don't see how the Lemma can be applied to see that $Y=D'X$. I tried a few approaches but none led to any result and I don't want to list them all here. Just leave a comment for more details. Maybe I'm missing something obvious but I really can't seem to find a way to reach the same conclusion as the author. Thanks in advance for any help!","I'm trying to understand a method for calculating the Moore-Penrose inverse of an incidence matrix of a graph as outlined in ""Graphs and Matrices"" (you can find the book here: https://link.springer.com/book/10.1007/978-1-4471-6569-9 ). I can execute all the steps for calculating the inverse but I haven't yet fully understood why the method works: The line I don't understand is on page 20 and reads ""By Lemma 2.14 it follows that $Y=D'X$."" I don't see how Lemma 2.14 is connected to this statement and would appreciate it if anyone could help me understand this line. If you cannot access the book or don't want to get familiar with the notation the author uses, here's a summary of what I don't understand which hopefully includes all the relevant information: Summary of the problem The transpose of a matrix $A$ will be denoted by $A'$. Consider a connected directed graph $G$ with vertices $V=\left\{1,\ldots,n\right\}$ and edges $E=\left\{e_1,\ldots,e_m\right\}$ with the edges ordered in such a way that $\left\{e_1,\ldots,e_{n-1}\right\}$ form a spanning tree of $G$. Let $Q$ be the $n\times m$ incidence matrix of $G$. To cite ""Graphs and Matrices"", ""the rows and the columns of $Q$ are indexed by $V$ and $E$, respectively. The $\left(i,j\right)$- entry of $Q$ is $0$ if vertex $i$ and edge $e_j$ are not incident, and otherwise it is $1$ or $-1$ according as $e_j$ originates or terminates at $i$, respectively."" See the following example where the spanning tree is highlighted in red: Now, one wants to calculate the Moore-Penrose inverse $Q^+$ of $Q$ (which implies that the following four conditions hold: (1) $QQ^+Q=Q$, (2) $Q^+QQ^+=Q^+$, (3) $\left(QQ^+\right)'=QQ^+$, (4) $\left(Q^+Q\right)'=Q^+Q$). To do so, the author partitions $Q$ as $Q=\begin{bmatrix} 	U & V \end{bmatrix}$ where $U$ is $n\times\left(n-1\right)$ and $V$ is $n\times\left(m-n+1\right)$ and $Q^+$ as $Q^+=\begin{bmatrix} 	X \\ Y \end{bmatrix}$ where $X$ is $\left(n-1\right)\times n$ and $Y$ is $\left(m-n+1\right)\times n$. He then notes that ""there exists an $\left(n-1\right)\times\left(m-n+1\right)$ matrix $D$ such that $V=UD$."" I still understand why this is the case. However, I'm stuck at the next sentence, which says ""By Lemma 2.14 it follows that $Y=D'X$."" Lemma 2.14 says: If $A$ is an $m\times n$ matrix, then for an $n\times1$ vector $x$,   $Ax=0$ if and only if $x'A^+=0$. I don't see how the Lemma can be applied to see that $Y=D'X$. I tried a few approaches but none led to any result and I don't want to list them all here. Just leave a comment for more details. Maybe I'm missing something obvious but I really can't seem to find a way to reach the same conclusion as the author. Thanks in advance for any help!",,"['linear-algebra', 'matrices', 'graph-theory', 'pseudoinverse']"
82,Find a linear transformation such that $S^2 = T$ (general case),Find a linear transformation such that  (general case),S^2 = T,"Suppose a linear transformation $T:\mathbb{R}^3\rightarrow \mathbb{R}^3$, $T\begin{pmatrix} x\\  y\\  z \end{pmatrix} = \begin{pmatrix} x+y\\  y+z\\  z+x \end{pmatrix}$ (just an example). How do i find a linear transformation S such that $S^2=S\circ S=T$ ? it's part of a basic linear algebra course. Would appreciate a general answer (not specific to the example above). Iv'e already tried several ways, didn't manage to get somewhere.. Thanks. EDIT: The answer is supposed to be based only on basic matrices and linear transformations material, no Diagonalization and eigenvalues","Suppose a linear transformation $T:\mathbb{R}^3\rightarrow \mathbb{R}^3$, $T\begin{pmatrix} x\\  y\\  z \end{pmatrix} = \begin{pmatrix} x+y\\  y+z\\  z+x \end{pmatrix}$ (just an example). How do i find a linear transformation S such that $S^2=S\circ S=T$ ? it's part of a basic linear algebra course. Would appreciate a general answer (not specific to the example above). Iv'e already tried several ways, didn't manage to get somewhere.. Thanks. EDIT: The answer is supposed to be based only on basic matrices and linear transformations material, no Diagonalization and eigenvalues",,"['linear-algebra', 'matrices', 'linear-transformations']"
83,Determinant of Union Jack matrix without Scottish diagonal,Determinant of Union Jack matrix without Scottish diagonal,,"Let $n \ge 1$ be an integer , and let $A_n$ be the matrix in $M_{2n-1}\mathbb(F)$ with entries $(a_{ij})$ where $a_{ij}=1 $ if $i+j=2n$  or $i=n$ or $j=n,$ and $a_{ij}=0$ otherwise . Find det$(A_n)$ my idea: for $n=4$ the matrix of the form from the given data $$\begin{bmatrix}  &  &  & 1 &  &  &1 \\   &  &  &  1&  &  1& \\   &  &  &  1& 1 &  & \\   1&1 & 1 & 1 & 1 &1  &1 \\   &  &  1&  1&  &  & \\   &  1&  &  1&  &  & \\   1&  &  &1  &  &  &  \end{bmatrix}$$ How to find genearl formula for determent :","Let $n \ge 1$ be an integer , and let $A_n$ be the matrix in $M_{2n-1}\mathbb(F)$ with entries $(a_{ij})$ where $a_{ij}=1 $ if $i+j=2n$  or $i=n$ or $j=n,$ and $a_{ij}=0$ otherwise . Find det$(A_n)$ my idea: for $n=4$ the matrix of the form from the given data $$\begin{bmatrix}  &  &  & 1 &  &  &1 \\   &  &  &  1&  &  1& \\   &  &  &  1& 1 &  & \\   1&1 & 1 & 1 & 1 &1  &1 \\   &  &  1&  1&  &  & \\   &  1&  &  1&  &  & \\   1&  &  &1  &  &  &  \end{bmatrix}$$ How to find genearl formula for determent :",,"['linear-algebra', 'matrices', 'determinant']"
84,"rank of $PQ$, rank of $QP$","rank of , rank of",PQ QP,"Given that $m$ and $n$ are natural numbers and $m < n$. $P$ is an $n{\times}m$ real matrix, and $Q$ is an $m{\times}n$ real matrix. Then which of the following is/are not possible. a)$\;\text{rank}(PQ)=n$ b)$\;\text{rank}(QP)=m$ c)$\;\text{rank}(PQ)=m$ d)$\;\text{rank}(QP)=[(m+n)/2]$ ,where $[x]$ is defined as the smallest integer greater or equal to $x$. option a) is not possible because of the theorem $\text{rank}(PQ) \le \min\{\text{rank}(P),\text{rank}(Q)\}$. Now $[(m+n)/2] > m$, but $QP$ is an $m{\times}m$ matrix, so option d) should not be possible. The answer is given only option a). Am I doing any mistake for option d).","Given that $m$ and $n$ are natural numbers and $m < n$. $P$ is an $n{\times}m$ real matrix, and $Q$ is an $m{\times}n$ real matrix. Then which of the following is/are not possible. a)$\;\text{rank}(PQ)=n$ b)$\;\text{rank}(QP)=m$ c)$\;\text{rank}(PQ)=m$ d)$\;\text{rank}(QP)=[(m+n)/2]$ ,where $[x]$ is defined as the smallest integer greater or equal to $x$. option a) is not possible because of the theorem $\text{rank}(PQ) \le \min\{\text{rank}(P),\text{rank}(Q)\}$. Now $[(m+n)/2] > m$, but $QP$ is an $m{\times}m$ matrix, so option d) should not be possible. The answer is given only option a). Am I doing any mistake for option d).",,['matrices']
85,Inside Roots of Determinant of Polynomial Matrix,Inside Roots of Determinant of Polynomial Matrix,,"Let ${\bf A}(x)$ be an $(n-1) \times n$ polynomial matrix and ${\bf b}(x)$ be a $1 \times n$ polynomial vector. Suppose that $$ \det\begin{bmatrix} {\bf A}(x) \\ {\bf b}(x) \end{bmatrix}=0 $$ does not have a root inside the unit circle. What condition must a $1 \times n$ constant vector ${\bf c}$ satisfy in order for $$ \det\begin{bmatrix} {\bf A}(x) \\ {\bf b}(x) - {\bf c} \end{bmatrix}=0 $$ to have a root inside the unit circle? I guess it is useful to use the fact that $$ \det\begin{bmatrix} {\bf A}(x) \\ {\bf b}(x) - {\bf c} \end{bmatrix} = \det\begin{bmatrix} {\bf A}(x) \\ {\bf b}(x)  \end{bmatrix}-\det\begin{bmatrix} {\bf A}(x) \\  {\bf c} \end{bmatrix},$$ which means, in particular, that ${\bf c}={\bf b}(x_0)$ for some $x_0$ inside the unit circle is a sufficient condition.","Let ${\bf A}(x)$ be an $(n-1) \times n$ polynomial matrix and ${\bf b}(x)$ be a $1 \times n$ polynomial vector. Suppose that $$ \det\begin{bmatrix} {\bf A}(x) \\ {\bf b}(x) \end{bmatrix}=0 $$ does not have a root inside the unit circle. What condition must a $1 \times n$ constant vector ${\bf c}$ satisfy in order for $$ \det\begin{bmatrix} {\bf A}(x) \\ {\bf b}(x) - {\bf c} \end{bmatrix}=0 $$ to have a root inside the unit circle? I guess it is useful to use the fact that $$ \det\begin{bmatrix} {\bf A}(x) \\ {\bf b}(x) - {\bf c} \end{bmatrix} = \det\begin{bmatrix} {\bf A}(x) \\ {\bf b}(x)  \end{bmatrix}-\det\begin{bmatrix} {\bf A}(x) \\  {\bf c} \end{bmatrix},$$ which means, in particular, that ${\bf c}={\bf b}(x_0)$ for some $x_0$ inside the unit circle is a sufficient condition.",,"['linear-algebra', 'matrices', 'determinant']"
86,Linear Algebra - Matrix Inverse,Linear Algebra - Matrix Inverse,,"Prove that if $A$ is invertible, and $B$ can be obtained from $A$ by applying elementary row operations, then $B$ is invertible. This is what i did: Let $A$ be any nxn square matrix, Given: $E_{k}E_{(k-1)}...E_{2}E_{1}A=B$ We know that each Elementary Matrix is invertible because it can be obtained by applying one elementary row operation to identity matrix. For $k=1$, $$EA=B$$ Given that $A$ and $E$ are both invertible,  $$(EA)^{-1}=A^{-1}E^{-1}$$ then, $$(B)^{-1}=A^{-1}E^{-1}$$ For $k=2$, $$E_{2}E_{1}A=B$$ Given that $A$ and $E_{1}$ and $E_{2}$ are invertible, $$(E_{2}E_{1}A)^{-1}=(E_{2}(E_{1}A))^{-1}=(E_{1}A)^{-1}E_{2}^{-1}=A^{-1}E_{1}^{-1}E_{2}^{-1}$$ then, $$(B)^{-1}=A^{-1}E_{1}^{-1}E_{2}^{-1}$$ For $k>2$, inductively we can assume that, $$(E_{k}E_{(k-1)}...E_{2}E_{1}A)^{-1}=A^{-1}E_{1}^{-1}E_{2}^{-1}...E_{k-1}^{-1}E_{k}^{-1}$$ Then, $$B^{-1}=A^{-1}E_{1}^{-1}E_{2}^{-1}...E_{k-1}^{-1}E_{k}^{-1}$$ Therefore, $B$ is invertible. Does this proof makes sense.","Prove that if $A$ is invertible, and $B$ can be obtained from $A$ by applying elementary row operations, then $B$ is invertible. This is what i did: Let $A$ be any nxn square matrix, Given: $E_{k}E_{(k-1)}...E_{2}E_{1}A=B$ We know that each Elementary Matrix is invertible because it can be obtained by applying one elementary row operation to identity matrix. For $k=1$, $$EA=B$$ Given that $A$ and $E$ are both invertible,  $$(EA)^{-1}=A^{-1}E^{-1}$$ then, $$(B)^{-1}=A^{-1}E^{-1}$$ For $k=2$, $$E_{2}E_{1}A=B$$ Given that $A$ and $E_{1}$ and $E_{2}$ are invertible, $$(E_{2}E_{1}A)^{-1}=(E_{2}(E_{1}A))^{-1}=(E_{1}A)^{-1}E_{2}^{-1}=A^{-1}E_{1}^{-1}E_{2}^{-1}$$ then, $$(B)^{-1}=A^{-1}E_{1}^{-1}E_{2}^{-1}$$ For $k>2$, inductively we can assume that, $$(E_{k}E_{(k-1)}...E_{2}E_{1}A)^{-1}=A^{-1}E_{1}^{-1}E_{2}^{-1}...E_{k-1}^{-1}E_{k}^{-1}$$ Then, $$B^{-1}=A^{-1}E_{1}^{-1}E_{2}^{-1}...E_{k-1}^{-1}E_{k}^{-1}$$ Therefore, $B$ is invertible. Does this proof makes sense.",,"['linear-algebra', 'matrices', 'proof-verification']"
87,Does every polynomial with a Perron root have a primitive matrix representation?,Does every polynomial with a Perron root have a primitive matrix representation?,,Let $p(x)=x^6-13x^4-20x^3+x^2-x+2$ and $C$ be the companion matrix of $p(x)$. How can I find a primitive matrix similar to $C$ ? Is there a general method  to transform the companion matrix with a Perron root into a primitive matrix?,Let $p(x)=x^6-13x^4-20x^3+x^2-x+2$ and $C$ be the companion matrix of $p(x)$. How can I find a primitive matrix similar to $C$ ? Is there a general method  to transform the companion matrix with a Perron root into a primitive matrix?,,"['linear-algebra', 'matrices', 'polynomials', 'companion-matrices', 'similar-matrices']"
88,Q of the QR decomposition is an upper Hessenberg matrix,Q of the QR decomposition is an upper Hessenberg matrix,,"Let $A=\{a_{ij}\}_{i,j=1}^n$ be a nonsingular upper hessenberg matrix, i.e., $a_{i,j}=0$ for $i>j+1$. I want to show that if $A=QR$ is the QR-decomposition of $A$ then $Q$ is also an upper hessenberg matrix. $$$$ For the QR decomposition, we can apply the Gram-Schmidt Algorithm:  $$\tilde{q}_1=a_1 \Rightarrow q_1=\frac{\tilde{q}_1}{\|\tilde{q}_1\|_2} \\ \tilde{q}_{k+1}=a_{k+1}-\sum_{i=1}^k(a_{k+1}, q_i)q_i \Rightarrow q_{k+1}=\frac{\tilde{q}_{k+1}}{\|\tilde{q}_{k+1}\|_2}  $$ That means that each column of the $Q$ matrix involves a linear combination of all the previous columns, right? We have that $A$ is an upper Hessenberg matrix, i.e. it holds that $a_{ij}=0$ for $i>j+1$. How could we continue? Could yoyu giive me a hint?","Let $A=\{a_{ij}\}_{i,j=1}^n$ be a nonsingular upper hessenberg matrix, i.e., $a_{i,j}=0$ for $i>j+1$. I want to show that if $A=QR$ is the QR-decomposition of $A$ then $Q$ is also an upper hessenberg matrix. $$$$ For the QR decomposition, we can apply the Gram-Schmidt Algorithm:  $$\tilde{q}_1=a_1 \Rightarrow q_1=\frac{\tilde{q}_1}{\|\tilde{q}_1\|_2} \\ \tilde{q}_{k+1}=a_{k+1}-\sum_{i=1}^k(a_{k+1}, q_i)q_i \Rightarrow q_{k+1}=\frac{\tilde{q}_{k+1}}{\|\tilde{q}_{k+1}\|_2}  $$ That means that each column of the $Q$ matrix involves a linear combination of all the previous columns, right? We have that $A$ is an upper Hessenberg matrix, i.e. it holds that $a_{ij}=0$ for $i>j+1$. How could we continue? Could yoyu giive me a hint?",,"['matrices', 'numerical-methods', 'matrix-decomposition']"
89,"Looking for a $2\times2$ real matrix $A$ with $Ax$ a contraction for the supremum norm, and not a contraction for the one norm","Looking for a  real matrix  with  a contraction for the supremum norm, and not a contraction for the one norm",2\times2 A Ax,"I am looking for a $2\times2$ real matrix $A$, such that  $ x\longmapsto Ax $ is a contraction considering  $\|\cdot\|_\infty$ and a noncontraction considering $\|\cdot\|_1$. I have now idea how to solve this. Thank you.","I am looking for a $2\times2$ real matrix $A$, such that  $ x\longmapsto Ax $ is a contraction considering  $\|\cdot\|_\infty$ and a noncontraction considering $\|\cdot\|_1$. I have now idea how to solve this. Thank you.",,"['linear-algebra', 'matrices', 'contraction-operator']"
90,What is the apex angle of the cone of positive semidefinite matrices?,What is the apex angle of the cone of positive semidefinite matrices?,,"Let $\def\S{\mathbf S}\S^n$ be the linear space of symmetric $n \times n$ matrices and $\S_+^n$ be the subset of positive semidefinite matrices. It is well-known that $\S_+^n$ is a convex cone in $\S^n$. In order to get a better geometric understanding of this object, I asked myself what the apex angle of this cone might be. We use the inner product $\DeclareMathOperator{\tr}{tr}\def\<{\langle}\def\>{\rangle}\<A,B\>=\tr(AB)$, where $\tr(A)$ is the trace of $A$. The apex angle $\theta$ of $\S_+^n$ is the biggest value of $\arccos\<A_1,A_2\>$ for $A_i\in\S_+^n$ with $\<A_i,A_i\>=1$. My best result so far Let $\def\E{\mathbf E}\E$ be some Euclidean space and $S\subset \E$ a proper subspace. Let $A_1\in\S_+^n$ be the orthogonal projection onto $S$ and $A_2\in\S_+^n$ the orthogonal projection onto the orthogonal complement $S^\bot$. Then $\<A_i,A_i\>=1$ but $A_1A_2=0$, hence $\<A_1,A_2\>=\tr(A_1A_2)=0$. So we have that $\theta\ge 90^\circ$. Can we do better? Especially, can we have $\tr(A_1A_2)<0$?","Let $\def\S{\mathbf S}\S^n$ be the linear space of symmetric $n \times n$ matrices and $\S_+^n$ be the subset of positive semidefinite matrices. It is well-known that $\S_+^n$ is a convex cone in $\S^n$. In order to get a better geometric understanding of this object, I asked myself what the apex angle of this cone might be. We use the inner product $\DeclareMathOperator{\tr}{tr}\def\<{\langle}\def\>{\rangle}\<A,B\>=\tr(AB)$, where $\tr(A)$ is the trace of $A$. The apex angle $\theta$ of $\S_+^n$ is the biggest value of $\arccos\<A_1,A_2\>$ for $A_i\in\S_+^n$ with $\<A_i,A_i\>=1$. My best result so far Let $\def\E{\mathbf E}\E$ be some Euclidean space and $S\subset \E$ a proper subspace. Let $A_1\in\S_+^n$ be the orthogonal projection onto $S$ and $A_2\in\S_+^n$ the orthogonal projection onto the orthogonal complement $S^\bot$. Then $\<A_i,A_i\>=1$ but $A_1A_2=0$, hence $\<A_1,A_2\>=\tr(A_1A_2)=0$. So we have that $\theta\ge 90^\circ$. Can we do better? Especially, can we have $\tr(A_1A_2)<0$?",,"['linear-algebra', 'matrices', 'angle', 'positive-semidefinite', 'convex-cone']"
91,quadratic inequality for non-symmetric with real eigenvalues,quadratic inequality for non-symmetric with real eigenvalues,,"If $A \in \mathbb{R}^{n \times n}$ is real and non-symmetric with real eigenvalues, then what is correct inequality ordering that connects $\lambda_{min}(A)$ and $\lambda_{\max}(A)$ with $\hat{\lambda}_{min}(S)$ and $\hat{\lambda}_{max}(S)$ where $S=\frac{A+A^T}{2}$. Here, ${\lambda}_{\min}(A)$ and ${\lambda}_{\max}(A)$ are the eigenvalues of $A$, respectively and $\hat{\lambda}_{\min}(S)$ and $\hat{\lambda}_{\max}(S)$ are the minimum and maximum eigenvalues of $S$, respectively. I found this result in one of the math-stackexchange posts which is apparently not true.  $$\lambda_{min}||x||^2 \le \hat{\lambda}_{min}||x||^2\le x^TAx \le \hat{\lambda}_{max}||x||^2\le \lambda_{max}||x||^2$$ for all $x \in \mathbb{R}^n$. So, I wonder what is the correct inequality that can be proved?","If $A \in \mathbb{R}^{n \times n}$ is real and non-symmetric with real eigenvalues, then what is correct inequality ordering that connects $\lambda_{min}(A)$ and $\lambda_{\max}(A)$ with $\hat{\lambda}_{min}(S)$ and $\hat{\lambda}_{max}(S)$ where $S=\frac{A+A^T}{2}$. Here, ${\lambda}_{\min}(A)$ and ${\lambda}_{\max}(A)$ are the eigenvalues of $A$, respectively and $\hat{\lambda}_{\min}(S)$ and $\hat{\lambda}_{\max}(S)$ are the minimum and maximum eigenvalues of $S$, respectively. I found this result in one of the math-stackexchange posts which is apparently not true.  $$\lambda_{min}||x||^2 \le \hat{\lambda}_{min}||x||^2\le x^TAx \le \hat{\lambda}_{max}||x||^2\le \lambda_{max}||x||^2$$ for all $x \in \mathbb{R}^n$. So, I wonder what is the correct inequality that can be proved?",,"['linear-algebra', 'matrices']"
92,"If $A,B$ are positive definite, is $(A+B)^{-1} - B^{-1}$ negative definite?","If  are positive definite, is  negative definite?","A,B (A+B)^{-1} - B^{-1}","In $\mathbb{R}$, given $a,b>0$, you have $a+b>b\iff \frac{1}{a+b} < \frac{1}{b} \iff (a+b)^{-1} - b^{-1}<0$. Is this true for positive definite $A,B\in\mathbb{R}^{n\times n}$ of arbitrary $n$?","In $\mathbb{R}$, given $a,b>0$, you have $a+b>b\iff \frac{1}{a+b} < \frac{1}{b} \iff (a+b)^{-1} - b^{-1}<0$. Is this true for positive definite $A,B\in\mathbb{R}^{n\times n}$ of arbitrary $n$?",,"['matrices', 'positive-definite']"
93,"Let $A,B,C$ in $M_n(\mathbb C).$ Suppose $CA = BC$ and $rank(C) = r$. Show that $A$ and $B$ have at least $r$ eigenvalues in common.",Let  in  Suppose  and . Show that  and  have at least  eigenvalues in common.,"A,B,C M_n(\mathbb C). CA = BC rank(C) = r A B r","Let $A,B,C$ in $M_n(\mathbb C).$ Suppose $CA = BC$ and $rank(C) = r$.   Show that $A$ and $B$ have at least $r$ eigenvalues (counted with   multiplicity) in common. I do not see how to use characteristic polynomial, and minimal polynomial does not give the number of eigenvalues counted with multiplicity. If someone has an hint... There exist $P$ and $Q$ invertible matrices such that $C = PJ_rQ$ with $J_r$ having first $r$ diagonal coefficients equal to $1$, and $0$ elsewhere. N.B. : It is supposed to be able to be solved without Jordan reduction since it is not in my curriculum (anyway, it is possible it simplifies the problem).","Let $A,B,C$ in $M_n(\mathbb C).$ Suppose $CA = BC$ and $rank(C) = r$.   Show that $A$ and $B$ have at least $r$ eigenvalues (counted with   multiplicity) in common. I do not see how to use characteristic polynomial, and minimal polynomial does not give the number of eigenvalues counted with multiplicity. If someone has an hint... There exist $P$ and $Q$ invertible matrices such that $C = PJ_rQ$ with $J_r$ having first $r$ diagonal coefficients equal to $1$, and $0$ elsewhere. N.B. : It is supposed to be able to be solved without Jordan reduction since it is not in my curriculum (anyway, it is possible it simplifies the problem).",,['linear-algebra']
94,Inequalities on matrix norm,Inequalities on matrix norm,,Let $K=(A+D)^{-1}A$ where $A$ is symmetric positive definite and $D$ is a diagonal matrix with positive elements. Is it true that $\|K\|\leq 1$ where $\|\cdot\|$ is the induced $2$-norm? Thank you.,Let $K=(A+D)^{-1}A$ where $A$ is symmetric positive definite and $D$ is a diagonal matrix with positive elements. Is it true that $\|K\|\leq 1$ where $\|\cdot\|$ is the induced $2$-norm? Thank you.,,"['linear-algebra', 'matrices', 'normed-spaces']"
95,Find a lower bound on $a^2(\boldsymbol{x}^\textrm{T}\boldsymbol{A}\boldsymbol{y})^2-(\boldsymbol{x}^\textrm{T}\boldsymbol{A}\boldsymbol{x})^2$,Find a lower bound on,a^2(\boldsymbol{x}^\textrm{T}\boldsymbol{A}\boldsymbol{y})^2-(\boldsymbol{x}^\textrm{T}\boldsymbol{A}\boldsymbol{x})^2,"I wonder if there is a good to find the lower bound of the following term: \begin{equation} \min_{\boldsymbol{x}}\,a^2(\boldsymbol{x}^\textrm{T}\boldsymbol{A}\boldsymbol{y})^2-(\boldsymbol{x}^\textrm{T}\boldsymbol{A}\boldsymbol{x})^2 \end{equation} where $a>0$ is some constant; $\boldsymbol{x},\boldsymbol{y}\in\mathbb{R}^n$ have unit norm; and $\boldsymbol{A}\in\mathbb{R}^{n\times n}$ is a symmetric matrix. $\boldsymbol{A}$ and $\boldsymbol{y}$ are known. It can also be written as: \begin{equation} \min_{\boldsymbol{x}}\,(a\boldsymbol{y}-\boldsymbol{x})^\textrm{T}\boldsymbol{A}\boldsymbol{x}\boldsymbol{x}^\textrm{T}\boldsymbol{A}(a\boldsymbol{y}+\boldsymbol{x}) \end{equation} or  \begin{equation} \min_{\boldsymbol{x}}\,\boldsymbol{x}^\textrm{T}\boldsymbol{A}(a^2\boldsymbol{y}\boldsymbol{y}^\textrm{T}-\boldsymbol{x}\boldsymbol{x}^\textrm{T})\boldsymbol{A}\boldsymbol{x} \end{equation} This is related to an earlier trivial question I asked, any help is greatly appreciated, thanks so much! Edit : In other words, is there an $a$ that could make the above term greater than $0$?","I wonder if there is a good to find the lower bound of the following term: \begin{equation} \min_{\boldsymbol{x}}\,a^2(\boldsymbol{x}^\textrm{T}\boldsymbol{A}\boldsymbol{y})^2-(\boldsymbol{x}^\textrm{T}\boldsymbol{A}\boldsymbol{x})^2 \end{equation} where $a>0$ is some constant; $\boldsymbol{x},\boldsymbol{y}\in\mathbb{R}^n$ have unit norm; and $\boldsymbol{A}\in\mathbb{R}^{n\times n}$ is a symmetric matrix. $\boldsymbol{A}$ and $\boldsymbol{y}$ are known. It can also be written as: \begin{equation} \min_{\boldsymbol{x}}\,(a\boldsymbol{y}-\boldsymbol{x})^\textrm{T}\boldsymbol{A}\boldsymbol{x}\boldsymbol{x}^\textrm{T}\boldsymbol{A}(a\boldsymbol{y}+\boldsymbol{x}) \end{equation} or  \begin{equation} \min_{\boldsymbol{x}}\,\boldsymbol{x}^\textrm{T}\boldsymbol{A}(a^2\boldsymbol{y}\boldsymbol{y}^\textrm{T}-\boldsymbol{x}\boldsymbol{x}^\textrm{T})\boldsymbol{A}\boldsymbol{x} \end{equation} This is related to an earlier trivial question I asked, any help is greatly appreciated, thanks so much! Edit : In other words, is there an $a$ that could make the above term greater than $0$?",,"['linear-algebra', 'matrices', 'optimization', 'upper-lower-bounds']"
96,"Equation via matrix, having no solution, one solution and infinite solutions.","Equation via matrix, having no solution, one solution and infinite solutions.",,"$$ \left[ \begin{array}{cc|c}   1&-2&4\\   a&4&5 \end{array} \right] $$ I came across this question on one of my course slides, and I am having trouble understanding the whole concept of an equation having no solution, one solution or infinitely many solutions. $$ \left[ \begin{array}{cc|c}   1&-2&4\\   0&4+2a&5-4a \end{array} \right] $$ this was the resulting matrix. What I don't understand is how you get $(4+2a)$ and $(5-4a)$? And what needs to be done so that the same equation has no solution, one unique solution, and infinitely many solutions. Can anybody please help?","$$ \left[ \begin{array}{cc|c}   1&-2&4\\   a&4&5 \end{array} \right] $$ I came across this question on one of my course slides, and I am having trouble understanding the whole concept of an equation having no solution, one solution or infinitely many solutions. $$ \left[ \begin{array}{cc|c}   1&-2&4\\   0&4+2a&5-4a \end{array} \right] $$ this was the resulting matrix. What I don't understand is how you get $(4+2a)$ and $(5-4a)$? And what needs to be done so that the same equation has no solution, one unique solution, and infinitely many solutions. Can anybody please help?",,"['linear-algebra', 'matrices']"
97,Prove that $\operatorname{null}(A)+\operatorname{null}(B)\operatorname{null}(AB)$ in matrix formulation,Prove that  in matrix formulation,\operatorname{null}(A)+\operatorname{null}(B)\operatorname{null}(AB),"I've already seen this question , but I'd like to prove the case in a matrix form as below; however, I have no justification for a particular part of my proof: Let $A_{m*n}$, and $B_{n*k}$, so $AB_{m*k}$. According to the rank-nullity theorem, we have: $n(A) + r(A) = n$ $n(B) + r(B) = k$ $n(AB) + r(AB) = k$ Adding the first two equations and using the third one to get rid of $k$ yields: $n(A) + n(B) + r(A) +r(B) = n + n(AB) + r(AB)$ Since $r(B) \ge r(AB)$, we have $n(A) + n(B) + r(A) \le n + n(AB)$ Here is the problem: I know that $r(A) \le n$, so I can't justify how to eliminate $r(A)$ and $n$ to end up with $n(A) + n(B) \ge n(AB)$","I've already seen this question , but I'd like to prove the case in a matrix form as below; however, I have no justification for a particular part of my proof: Let $A_{m*n}$, and $B_{n*k}$, so $AB_{m*k}$. According to the rank-nullity theorem, we have: $n(A) + r(A) = n$ $n(B) + r(B) = k$ $n(AB) + r(AB) = k$ Adding the first two equations and using the third one to get rid of $k$ yields: $n(A) + n(B) + r(A) +r(B) = n + n(AB) + r(AB)$ Since $r(B) \ge r(AB)$, we have $n(A) + n(B) + r(A) \le n + n(AB)$ Here is the problem: I know that $r(A) \le n$, so I can't justify how to eliminate $r(A)$ and $n$ to end up with $n(A) + n(B) \ge n(AB)$",,['linear-algebra']
98,"When determining the solution of a system of equations with free variables, can any set of variables be selected as the free variable?","When determining the solution of a system of equations with free variables, can any set of variables be selected as the free variable?",,"In this example $x_3$ and $x_4$ were the non-pivot variables and therefore used as the free variables. Is it possible to select any two variables as free variables and if not, is there a general rule/way to tell if the variables you want to use as the free variable can actually be used as free variables? I attempted manipulate the solution so $x_1$ and $x_2$ are the free variables but it seems I cannot escape using at least one of $x_3$ or $x_4$ as the free variable as one equation has $x_3 - x_4$ and another has $x_4 - x_3$.","In this example $x_3$ and $x_4$ were the non-pivot variables and therefore used as the free variables. Is it possible to select any two variables as free variables and if not, is there a general rule/way to tell if the variables you want to use as the free variable can actually be used as free variables? I attempted manipulate the solution so $x_1$ and $x_2$ are the free variables but it seems I cannot escape using at least one of $x_3$ or $x_4$ as the free variable as one equation has $x_3 - x_4$ and another has $x_4 - x_3$.",,"['linear-algebra', 'matrices', 'matrix-equations']"
99,Find all real square matrix $A$ which $A=\operatorname{adj}(A)$,Find all real square matrix  which,A A=\operatorname{adj}(A),"Find all real square matrix $A$ which $$A=\operatorname{adj}(A)$$ My thoughts: We have the well-known equation $$ A\operatorname{adj}(A)=\operatorname{det}(A)I_n$$ Since $A=\operatorname{adj}(A)$, it comes to $$ A^2=\operatorname{det}(A)I_n$$ So, if $\operatorname{det}(A)=0$, then $A^2=O$. if $\operatorname{det}(A)\neq0$, then we get (if $n\neq2$)  $$(\operatorname{det}(A))^2=(\operatorname{det}(A))^n$$ $$ \operatorname{det}(A)=\pm1$$ $$ A^2=\pm I_n$$ In this case, $A$ seems like a involutory matrix. How to characterize the matrics mentioned above? Or what further properties should $A$ follows so we can completely characterize $A$? Thanks for your help!","Find all real square matrix $A$ which $$A=\operatorname{adj}(A)$$ My thoughts: We have the well-known equation $$ A\operatorname{adj}(A)=\operatorname{det}(A)I_n$$ Since $A=\operatorname{adj}(A)$, it comes to $$ A^2=\operatorname{det}(A)I_n$$ So, if $\operatorname{det}(A)=0$, then $A^2=O$. if $\operatorname{det}(A)\neq0$, then we get (if $n\neq2$)  $$(\operatorname{det}(A))^2=(\operatorname{det}(A))^n$$ $$ \operatorname{det}(A)=\pm1$$ $$ A^2=\pm I_n$$ In this case, $A$ seems like a involutory matrix. How to characterize the matrics mentioned above? Or what further properties should $A$ follows so we can completely characterize $A$? Thanks for your help!",,['linear-algebra']
