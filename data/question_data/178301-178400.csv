,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How does one solve for a generalized quadratic in multiple dimensions as in $a w^Tw + b^T w + c = 0$?,How does one solve for a generalized quadratic in multiple dimensions as in ?,a w^Tw + b^T w + c = 0,"Consider: $$a w^Tw + b^T w + c = 0 $$ I would like to find a vector $w \in \mathbb{R}^D$ that satisfies the above equation. I realize that I need to choose D numbers to satisfy 1 equation, so my intuition (from linear algebra) tells me that there might be an infinite set of solutions for this (1 equation D unknowns). Thus, I thought that maybe additional constraints are needed. I am honestly not too worried about which one to choose as long as it satisfies the equation. However, if I really had to choose a criterion to choose a $w$ I would first require it to be real vector and perhaps choose $w$ closest to some other fixed (aprior chosen) $x$ (say for simplicity closest to the origin $x=0$). So the additional constraint is: $$ min_w \| w - x\|^2 $$ or even $$ \| w - x\|^2 = 0 $$ another constraint could be (normalized vector) is also good: $$  \| w \|^2 = 1 $$ I mostly care to satisfy the ""generalized quadratic"", the additional constraint is there just incase its needed (no complex number solutions). Also Ideally I'd like to implement the solution in a maths program, for example, python or matlab.","Consider: $$a w^Tw + b^T w + c = 0 $$ I would like to find a vector $w \in \mathbb{R}^D$ that satisfies the above equation. I realize that I need to choose D numbers to satisfy 1 equation, so my intuition (from linear algebra) tells me that there might be an infinite set of solutions for this (1 equation D unknowns). Thus, I thought that maybe additional constraints are needed. I am honestly not too worried about which one to choose as long as it satisfies the equation. However, if I really had to choose a criterion to choose a $w$ I would first require it to be real vector and perhaps choose $w$ closest to some other fixed (aprior chosen) $x$ (say for simplicity closest to the origin $x=0$). So the additional constraint is: $$ min_w \| w - x\|^2 $$ or even $$ \| w - x\|^2 = 0 $$ another constraint could be (normalized vector) is also good: $$  \| w \|^2 = 1 $$ I mostly care to satisfy the ""generalized quadratic"", the additional constraint is there just incase its needed (no complex number solutions). Also Ideally I'd like to implement the solution in a maths program, for example, python or matlab.",,"['multivariable-calculus', 'algebraic-geometry', 'optimization', 'numerical-methods', 'quadratic-forms']"
1,A finite intersection of open sets is open,A finite intersection of open sets is open,,"I have to prove that a finite intersection of open sets is open. My idea is to do a proof by contradiction. The definition of an open set is as follows: A subset $U \subset \mathbb{R^n}$ is open if for every point $x \in U$, there exists $ r > 0$ such that the open ball $B_r(x)$ is contained in U. Is my starting point valid? Let $A_1, A_2 ... A_k$ open sets and let $x\in \bigcap_{i=1}^{k} A_i$. Suppose the intersection is not open and then there must be an $x\in  \bigcap_{i=1}^{k} A_i$ such that given any $\epsilon>0,\ B_{\epsilon}(x)\subset \bigcup_{i=1}^{k} A_i^c$.  From here I could look for a particular $\epsilon$ to find a contradiction.","I have to prove that a finite intersection of open sets is open. My idea is to do a proof by contradiction. The definition of an open set is as follows: A subset $U \subset \mathbb{R^n}$ is open if for every point $x \in U$, there exists $ r > 0$ such that the open ball $B_r(x)$ is contained in U. Is my starting point valid? Let $A_1, A_2 ... A_k$ open sets and let $x\in \bigcap_{i=1}^{k} A_i$. Suppose the intersection is not open and then there must be an $x\in  \bigcap_{i=1}^{k} A_i$ such that given any $\epsilon>0,\ B_{\epsilon}(x)\subset \bigcup_{i=1}^{k} A_i^c$.  From here I could look for a particular $\epsilon$ to find a contradiction.",,"['analysis', 'multivariable-calculus', 'proof-verification']"
2,"Proving that $\lim_{(x,y) \to (0,0)} (x^2 +y^2 -x^3 y^3)/(x^2 +y^2) =1$",Proving that,"\lim_{(x,y) \to (0,0)} (x^2 +y^2 -x^3 y^3)/(x^2 +y^2) =1","How can I go about proving that $$\lim_{(x,y) \to (0,0)} \frac{x^2 +y^2 -x^3 y^3}{x^2 +y^2} = 1 ?$$ I checked some lines along $x, y$ and $x=y$ and it all gave $1$","How can I go about proving that $$\lim_{(x,y) \to (0,0)} \frac{x^2 +y^2 -x^3 y^3}{x^2 +y^2} = 1 ?$$ I checked some lines along $x, y$ and $x=y$ and it all gave $1$",,"['calculus', 'limits', 'multivariable-calculus']"
3,Partial and total derivative of a multivariable function,Partial and total derivative of a multivariable function,,"Given a function $f(x,y,t)$, is it correct to say $$\frac{d f}{d x} = \frac{\partial f}{\partial x} \text{ ?}$$","Given a function $f(x,y,t)$, is it correct to say $$\frac{d f}{d x} = \frac{\partial f}{\partial x} \text{ ?}$$",,['multivariable-calculus']
4,"integrate $\int_0^1 \int_{\sqrt{x}}^1 e^{\frac{y^3}{3}} \, dy \, dx$",integrate,"\int_0^1 \int_{\sqrt{x}}^1 e^{\frac{y^3}{3}} \, dy \, dx","$$\int_0^1 \int_{\sqrt{x}}^{1}e^{\frac{y^3}{3}} \, dy \, dx$$ So I understand I need to change the integration limits by looking at the domain. I have sketched it But how do I know if I need to take the area under the curve or above it? (in the drawing the area filled with vercital or horizontal lines)","$$\int_0^1 \int_{\sqrt{x}}^{1}e^{\frac{y^3}{3}} \, dy \, dx$$ So I understand I need to change the integration limits by looking at the domain. I have sketched it But how do I know if I need to take the area under the curve or above it? (in the drawing the area filled with vercital or horizontal lines)",,"['integration', 'multivariable-calculus']"
5,"Continuity of $\frac{x^5-4x^3y^2-xy^4}{(x^2+y^2)^2}$ at (0, 0)","Continuity of  at (0, 0)",\frac{x^5-4x^3y^2-xy^4}{(x^2+y^2)^2},"I am having trouble proving that $\dfrac{x^5-4x^3y^2-xy^4}{(x^2+y^2)^2}$ is continuous at $(0, 0)$ if we set the value at $(0, 0)$ to be $0$ . I don't see a way to prove this as I cannot factor this into partial fractions.",I am having trouble proving that is continuous at if we set the value at to be . I don't see a way to prove this as I cannot factor this into partial fractions.,"\dfrac{x^5-4x^3y^2-xy^4}{(x^2+y^2)^2} (0, 0) (0, 0) 0","['multivariable-calculus', 'continuity']"
6,Maximum value of $x$ such that $3x^2 - xy - 2x - 5y + 7 = 0$,Maximum value of  such that,x 3x^2 - xy - 2x - 5y + 7 = 0,"Let $x, y \in \mathbb Z$ such that   $$3x^2 - xy - 2x - 5y + 7 = 0$$   What is the maximum value for $x$? I tried to solve the problem by interpreting the equation as a conic section, but it took too much time and I gained no valuable information. As last resort I differentiated with respect to $x$ and tried to solve the resulting equation with respect to $y$, but I wasn't even sure about the correctness of what I was doing. I wonder, how could one solve such a problem?","Let $x, y \in \mathbb Z$ such that   $$3x^2 - xy - 2x - 5y + 7 = 0$$   What is the maximum value for $x$? I tried to solve the problem by interpreting the equation as a conic section, but it took too much time and I gained no valuable information. As last resort I differentiated with respect to $x$ and tried to solve the resulting equation with respect to $y$, but I wasn't even sure about the correctness of what I was doing. I wonder, how could one solve such a problem?",,"['calculus', 'elementary-number-theory', 'multivariable-calculus']"
7,Dot product between sum and difference between two vectors.,Dot product between sum and difference between two vectors.,,"The solution to a problem I was working on had the following manipulation that I cannot understand: (A + B) · (A − B) = A · A − B · B , where A and B are vectors Could someone please explain the steps in the simplification?","The solution to a problem I was working on had the following manipulation that I cannot understand: (A + B) · (A − B) = A · A − B · B , where A and B are vectors Could someone please explain the steps in the simplification?",,"['calculus', 'linear-algebra', 'multivariable-calculus', 'vectors']"
8,Prove that $\nabla \times \vec F =0 \implies \vec F = \nabla f$,Prove that,\nabla \times \vec F =0 \implies \vec F = \nabla f,"How do I prove that if $\nabla \times \vec F = \vec 0 $ then $\vec F = \nabla f$ for some scalar field $f$? My lecturer only proved the converse, which follows easily from the symmetry of mixed partial derivatives. Preferably I would like a simple method of proof, not something powerful like Stokes theorem.","How do I prove that if $\nabla \times \vec F = \vec 0 $ then $\vec F = \nabla f$ for some scalar field $f$? My lecturer only proved the converse, which follows easily from the symmetry of mixed partial derivatives. Preferably I would like a simple method of proof, not something powerful like Stokes theorem.",,"['calculus', 'multivariable-calculus']"
9,"How to draw a contour map of $ f(x,y)=x^2+y^2+xy$",How to draw a contour map of," f(x,y)=x^2+y^2+xy","I have used a program to see that it is an ellipse but I want to know the process of thinking to actually draw the contour map myself. $x^2+y^2+xy=C$ for $C=0,1,2,3,...$ I can't seem to get it into an ellipse form. What should I do? Thanks!","I have used a program to see that it is an ellipse but I want to know the process of thinking to actually draw the contour map myself. $x^2+y^2+xy=C$ for $C=0,1,2,3,...$ I can't seem to get it into an ellipse form. What should I do? Thanks!",,['multivariable-calculus']
10,"An example of a continuous function on $\mathbb R^2$ with two critical points, both of them minima","An example of a continuous function on  with two critical points, both of them minima",\mathbb R^2,"Knowing you can not use the minimum bound, there exists a function $f ( x , y )$ continuous in $\mathbb R ^ 2$ that has exactly two critical points which are (both) the minimum? Can you give me an example?","Knowing you can not use the minimum bound, there exists a function $f ( x , y )$ continuous in $\mathbb R ^ 2$ that has exactly two critical points which are (both) the minimum? Can you give me an example?",,"['multivariable-calculus', 'optimization']"
11,Computing the Laplacian of $\frac{\mu\ \cdot\ \mathbf{r}}{\|\mathbf{r}\|^3}$,Computing the Laplacian of,\frac{\mu\ \cdot\ \mathbf{r}}{\|\mathbf{r}\|^3},"How does one compute the Laplacian of $$\dfrac{(\mathbf \mu \cdot \mathbf r)}{r^3} \;\; \text{where} \;\; r = \Vert \mathbf r \Vert?$$ I am aware that the Laplacian is defined to be $\Delta f=\sum_i \partial_i^2f$ but am a little confused about this computationally in this case. I have got as far as saying $\partial_i^2[1/r^3(\mu_1r_1+\mu_2r_2+...+\mu_nr_n)]$, but don't know how to compute this.","How does one compute the Laplacian of $$\dfrac{(\mathbf \mu \cdot \mathbf r)}{r^3} \;\; \text{where} \;\; r = \Vert \mathbf r \Vert?$$ I am aware that the Laplacian is defined to be $\Delta f=\sum_i \partial_i^2f$ but am a little confused about this computationally in this case. I have got as far as saying $\partial_i^2[1/r^3(\mu_1r_1+\mu_2r_2+...+\mu_nr_n)]$, but don't know how to compute this.",,"['calculus', 'multivariable-calculus', 'partial-derivative', 'laplacian']"
12,Proving Multivairble Limit Exists [duplicate],Proving Multivairble Limit Exists [duplicate],,"This question already has answers here : Is there a step by step checklist to check if a multivariable limit exists and find its value? (2 answers) Closed 9 years ago . How do you deal with multivariable limits? We'll use the example $f: \mathbb R ^2 \rightarrow \mathbb R$ $$\lim _{(x,y) \rightarrow (0,0)}\frac{\sqrt{|xy|}}{\sqrt{x^2 + y^2}}$$ The limit doesn't exist, if $x=y$ we have the value $1/\sqrt{2}$ and if $y = x^3$ we get $0$. How would we prove it with the $\epsilon - \delta$ proof? Do we even need to prove it through the definition, or does it suffice to show that approaching the point by different paths leads to different answers? Given that there are an infinite amount of paths in which to approach our point, how would you prove that the limit did actually exist? Take for example $f(x,y) = xy$, where we'll take the limit $$\lim _{(x,y)\rightarrow (1,2)} f(x,y) = 2 $$ This is obvious because the function is continuous at our point of interest, but how do you prove it directly from the definition? The same tricks we used when dealing with a single variable won't apply here due to the fact that we're dealing with a point, rather than a single number.","This question already has answers here : Is there a step by step checklist to check if a multivariable limit exists and find its value? (2 answers) Closed 9 years ago . How do you deal with multivariable limits? We'll use the example $f: \mathbb R ^2 \rightarrow \mathbb R$ $$\lim _{(x,y) \rightarrow (0,0)}\frac{\sqrt{|xy|}}{\sqrt{x^2 + y^2}}$$ The limit doesn't exist, if $x=y$ we have the value $1/\sqrt{2}$ and if $y = x^3$ we get $0$. How would we prove it with the $\epsilon - \delta$ proof? Do we even need to prove it through the definition, or does it suffice to show that approaching the point by different paths leads to different answers? Given that there are an infinite amount of paths in which to approach our point, how would you prove that the limit did actually exist? Take for example $f(x,y) = xy$, where we'll take the limit $$\lim _{(x,y)\rightarrow (1,2)} f(x,y) = 2 $$ This is obvious because the function is continuous at our point of interest, but how do you prove it directly from the definition? The same tricks we used when dealing with a single variable won't apply here due to the fact that we're dealing with a point, rather than a single number.",,"['limits', 'multivariable-calculus']"
13,A rigorous statement and proof of the two-path test,A rigorous statement and proof of the two-path test,,"Background In multivariable calculus/complex analysis, one often tries to show that a limit doesn't exist by finding two paths which yield different limits. The justification goes that if the limit exists, it must be the same along any path; hence if it is not, it doesn't exist. On an intuitive level, I think I understand this concept. But I have never seen a rigorous justification for this method, nor what exactly constitutes a ""path"". Attempt at a solution Definition/Theorem I'm going to stick to $\mathbb{R}^2$ here, since the arguments should generalize easily. Let $f: U \subseteq \mathbb{R}^2 \to \mathbb{R}$ be a real-valued function defined on the domain $U$ , an open proper subset of $\mathbb{R}^2$ . Let us define a ""path"" in this context to be a continuous function $\gamma: \mathbb{R} \to \mathbb{R}^2$ , parameterized by $t$ . Suppose that $\lim\limits_{\mathbf{x} \to \mathbf{x_0}} f(\mathbf{x_0})$ exists. Then, for any path $\gamma$ with range in $U$ and $\gamma(t_0) = \mathbf{x_0}$ , $$\lim\limits_{t \to t_0} f(\gamma(t_0)) = L. $$ In terms of $\delta$ 's and $\epsilon$ 's, this says that for any $\epsilon > 0$ , there exists some $\delta > 0$ such that $$\left|t - t_0 \right| < \delta \Rightarrow \left| f(\gamma(t)) - f(\gamma(t_0)) \right| < \epsilon. $$ Proof I don't know how to prove this. Example To demonstrate, I will show that the limit in example (d) from Paul Dawkins' Calculus III notes doesn't exist. I'll follow his proof, exchanging his intuitive definition with the formal one above. To show: $\lim\limits_{(x, y) \to (0, 0)} \frac{x^3y}{x^6 + x^2}$ doesn't exist. Proof : Suppose that the limit exists and is equal to $L$ . First, take the continuous path $\gamma(t) = (t, t)$ (the path $y = x$ ). By our theorem, we conclude that $$\lim\limits_{t \to 0} \frac{t^3t}{t^6 + t^2} = \lim\limits_{t \to 0} \frac{1}{t^2 + 1} = 0 = L.$$ But instead take the path $\gamma(t) = (t, t^3)$ (the path $y = x^3$ ). By our theorem, we conclude that this is also equal to $L = 0$ , but we have $$\lim\limits_{t \to 0} \frac{t^3(t^3)}{t^6 + (t^3)^2} = \lim\limits_{t \to 0} \frac{t^6}{2t^6} = \frac{1}{2},$$ which is a contradiction. Thus, the limit does not exist. Summary So the definition seems to work. My questions are (1) is the definition/theorem I've stated above in terms of continuous paths the most general one and (2) how can we prove it?","Background In multivariable calculus/complex analysis, one often tries to show that a limit doesn't exist by finding two paths which yield different limits. The justification goes that if the limit exists, it must be the same along any path; hence if it is not, it doesn't exist. On an intuitive level, I think I understand this concept. But I have never seen a rigorous justification for this method, nor what exactly constitutes a ""path"". Attempt at a solution Definition/Theorem I'm going to stick to here, since the arguments should generalize easily. Let be a real-valued function defined on the domain , an open proper subset of . Let us define a ""path"" in this context to be a continuous function , parameterized by . Suppose that exists. Then, for any path with range in and , In terms of 's and 's, this says that for any , there exists some such that Proof I don't know how to prove this. Example To demonstrate, I will show that the limit in example (d) from Paul Dawkins' Calculus III notes doesn't exist. I'll follow his proof, exchanging his intuitive definition with the formal one above. To show: doesn't exist. Proof : Suppose that the limit exists and is equal to . First, take the continuous path (the path ). By our theorem, we conclude that But instead take the path (the path ). By our theorem, we conclude that this is also equal to , but we have which is a contradiction. Thus, the limit does not exist. Summary So the definition seems to work. My questions are (1) is the definition/theorem I've stated above in terms of continuous paths the most general one and (2) how can we prove it?","\mathbb{R}^2 f: U \subseteq \mathbb{R}^2 \to \mathbb{R} U \mathbb{R}^2 \gamma: \mathbb{R} \to \mathbb{R}^2 t \lim\limits_{\mathbf{x} \to \mathbf{x_0}} f(\mathbf{x_0}) \gamma U \gamma(t_0) = \mathbf{x_0} \lim\limits_{t \to t_0} f(\gamma(t_0)) = L.  \delta \epsilon \epsilon > 0 \delta > 0 \left|t - t_0 \right| < \delta \Rightarrow \left| f(\gamma(t)) - f(\gamma(t_0)) \right| < \epsilon.  \lim\limits_{(x, y) \to (0, 0)} \frac{x^3y}{x^6 + x^2} L \gamma(t) = (t, t) y = x \lim\limits_{t \to 0} \frac{t^3t}{t^6 + t^2} = \lim\limits_{t \to 0} \frac{1}{t^2 + 1} = 0 = L. \gamma(t) = (t, t^3) y = x^3 L = 0 \lim\limits_{t \to 0} \frac{t^3(t^3)}{t^6 + (t^3)^2} = \lim\limits_{t \to 0} \frac{t^6}{2t^6} = \frac{1}{2},","['limits', 'multivariable-calculus']"
14,Computing double integrals,Computing double integrals,,"If $f(x)$ is continuous on $[0,1]$ and $$\int^{1}_{0} f(x) \ \mathrm{d}x = \sqrt{2}$$ compute $$\int_{0}^{1} \int^{1}_{x} f(x)f(y) \ \mathrm{d}y \ \mathrm{d}x$$ First I change the order of integration: $$\int_{?}^{?} \int^{?}_{?} f(x)f(y) \ \mathrm{d}x \ \mathrm{d}y$$ but what happens to the limits? What do I change them to?","If $f(x)$ is continuous on $[0,1]$ and $$\int^{1}_{0} f(x) \ \mathrm{d}x = \sqrt{2}$$ compute $$\int_{0}^{1} \int^{1}_{x} f(x)f(y) \ \mathrm{d}y \ \mathrm{d}x$$ First I change the order of integration: $$\int_{?}^{?} \int^{?}_{?} f(x)f(y) \ \mathrm{d}x \ \mathrm{d}y$$ but what happens to the limits? What do I change them to?",,['multivariable-calculus']
15,Why is the value of this line integral constant,Why is the value of this line integral constant,,"Consider the line integral given by $$\int_C \frac{(x+y)\,dx-(x-y)\,dy}{x^2+y^2}$$ where $C$ is any simple closed curve around the origin. Can someone explain, without using complex analysis, why this is always $-2 \pi$?","Consider the line integral given by $$\int_C \frac{(x+y)\,dx-(x-y)\,dy}{x^2+y^2}$$ where $C$ is any simple closed curve around the origin. Can someone explain, without using complex analysis, why this is always $-2 \pi$?",,"['calculus', 'integration', 'multivariable-calculus']"
16,How do I find the orthogonal basis for this plane?,How do I find the orthogonal basis for this plane?,,"Question: $P$ is a plane through the origin given by $x + y + 2z = 0$. Find an orthogonal basis v 1 , v 2 ∈ $P$. My answer: I'm assuming the question asks for two vectors that span this plane $P$. But the chapter that this problem is for doesn't say anything about the $x,y,z$ equation of a plane that was given here...so I did some searching online and learned that this helps find the ""normal vector"". In this case it would be $n = (1,1,2)$, right? Then if all the vectors that span this plane are orthogonal to the normal vector, I can use the dot product. I chose the following two vectors: v 1 = $(1,1,-1)$ v 2 = $(3,3,-3)$ Was this question answered correctly?","Question: $P$ is a plane through the origin given by $x + y + 2z = 0$. Find an orthogonal basis v 1 , v 2 ∈ $P$. My answer: I'm assuming the question asks for two vectors that span this plane $P$. But the chapter that this problem is for doesn't say anything about the $x,y,z$ equation of a plane that was given here...so I did some searching online and learned that this helps find the ""normal vector"". In this case it would be $n = (1,1,2)$, right? Then if all the vectors that span this plane are orthogonal to the normal vector, I can use the dot product. I chose the following two vectors: v 1 = $(1,1,-1)$ v 2 = $(3,3,-3)$ Was this question answered correctly?",,['multivariable-calculus']
17,Calculate double integral of ...,Calculate double integral of ...,,"I was doing a homework problem but now I'm stuck. The problem says: Calculate $\iint_{S} \frac{dx dy}{\sqrt{2a - x}}$ where S is a circle of radius $a$ which is tangent to to both coordinate axes and is in the first quadrant The cartesian equation for a circle of radius a, and center C(a,a) is: $$(x-a)^{2}+(y-a)^{2} = a^{2}$$ Sketching this equation for different values of $a$ I obtained the limits of integration for the double integral: $$4\int_{a}^{2a} \int_{a}^{a+\sqrt{2ay-y^{2}}} \frac{1}{\sqrt{2a-x}}dx dy$$ It is multiplied by 4 because that double integral only calculates a quarter of the desired area. Then evaluating the double integral I obtained this: $$4 \left ( 2a^{3/2}+2\int_{a}^{2a} \sqrt{a+\sqrt{2ay-y^{2}}} dy \right )$$ But I was stuck on this (I've tried to put the indefinite  integral$\int \sqrt{a+\sqrt{2ay - y^{2}}} dy$ into Wolfram Alpha and I obtained this ), then I said ""why I don't use polar coordinates?"" and I used them, and I obtained this double integral: $$4\int_{0}^{\pi /2} \int_{0}^{a}\frac{r}{\sqrt{2a-rcos\theta}} dr d\theta$$ I evaluated it, and I obtained: $$4\left ( \int_{0}^{\pi/2} -\frac{2}{3}\left ( \sec ^{2} \theta \sqrt{2a-\arccos {\theta}} \left ( 4a + acos\theta \right ) \right ) d\theta  \right )$$ But Wolfram says this for the indefinite integral so... What can I do? Am I wrong in something? Is there another way to do it? I would appreciate any help.","I was doing a homework problem but now I'm stuck. The problem says: Calculate $\iint_{S} \frac{dx dy}{\sqrt{2a - x}}$ where S is a circle of radius $a$ which is tangent to to both coordinate axes and is in the first quadrant The cartesian equation for a circle of radius a, and center C(a,a) is: $$(x-a)^{2}+(y-a)^{2} = a^{2}$$ Sketching this equation for different values of $a$ I obtained the limits of integration for the double integral: $$4\int_{a}^{2a} \int_{a}^{a+\sqrt{2ay-y^{2}}} \frac{1}{\sqrt{2a-x}}dx dy$$ It is multiplied by 4 because that double integral only calculates a quarter of the desired area. Then evaluating the double integral I obtained this: $$4 \left ( 2a^{3/2}+2\int_{a}^{2a} \sqrt{a+\sqrt{2ay-y^{2}}} dy \right )$$ But I was stuck on this (I've tried to put the indefinite  integral$\int \sqrt{a+\sqrt{2ay - y^{2}}} dy$ into Wolfram Alpha and I obtained this ), then I said ""why I don't use polar coordinates?"" and I used them, and I obtained this double integral: $$4\int_{0}^{\pi /2} \int_{0}^{a}\frac{r}{\sqrt{2a-rcos\theta}} dr d\theta$$ I evaluated it, and I obtained: $$4\left ( \int_{0}^{\pi/2} -\frac{2}{3}\left ( \sec ^{2} \theta \sqrt{2a-\arccos {\theta}} \left ( 4a + acos\theta \right ) \right ) d\theta  \right )$$ But Wolfram says this for the indefinite integral so... What can I do? Am I wrong in something? Is there another way to do it? I would appreciate any help.",,['multivariable-calculus']
18,The normal vector to a surface given by parametric equations,The normal vector to a surface given by parametric equations,,"This is a problem I haven't thought about or encountered in many years but has popped up again incidentally, so please correct anything ahead if something is incorrect. If we are given the condition that $\{f(x,y,z) = 0\}$ for some differentiable function $f$, then the normal vector to the surface governed by said equation is given by $\nabla f(x,y,z)=(f_1(x,y,z), f_2(x,y,z),f_3(x,y,z))$ where $f_i := \frac{\partial f}{\partial x_i}$ is the $i^{\operatorname{th}}$ partial derivative of $f$. Good and all, but say that instead I am given $f$ parametrically in the form $f(u,v) = (x(u,v), y(u,v), z(u,v))$. How would I now go about determining the normal vector at arbitrary $u,v \in \mathbb{R}$?","This is a problem I haven't thought about or encountered in many years but has popped up again incidentally, so please correct anything ahead if something is incorrect. If we are given the condition that $\{f(x,y,z) = 0\}$ for some differentiable function $f$, then the normal vector to the surface governed by said equation is given by $\nabla f(x,y,z)=(f_1(x,y,z), f_2(x,y,z),f_3(x,y,z))$ where $f_i := \frac{\partial f}{\partial x_i}$ is the $i^{\operatorname{th}}$ partial derivative of $f$. Good and all, but say that instead I am given $f$ parametrically in the form $f(u,v) = (x(u,v), y(u,v), z(u,v))$. How would I now go about determining the normal vector at arbitrary $u,v \in \mathbb{R}$?",,['multivariable-calculus']
19,Lagrange multiplier problem of looking for the point on $\frac1x + \frac1y + \frac1z =1$ closest to the origin,Lagrange multiplier problem of looking for the point on  closest to the origin,\frac1x + \frac1y + \frac1z =1,"Use Lagrange multipliers to find the point on the surface  $$\frac1x + \frac1y + \frac1z =1$$ which is closest to the origin. I was wondering if I would start off by using the distance formula, $$d=\sqrt{\left(\frac1x\right)^2+\left(\frac1y\right)^2+\left(\frac1z\right)^2}$$ which would then simplify to, $$d^2=\left(\frac1x\right)^2+\left(\frac1y\right)^2+\left(\frac1z\right)^2$$ I don't know where to go from there, so if someone could help me out, that would be great.","Use Lagrange multipliers to find the point on the surface  $$\frac1x + \frac1y + \frac1z =1$$ which is closest to the origin. I was wondering if I would start off by using the distance formula, $$d=\sqrt{\left(\frac1x\right)^2+\left(\frac1y\right)^2+\left(\frac1z\right)^2}$$ which would then simplify to, $$d^2=\left(\frac1x\right)^2+\left(\frac1y\right)^2+\left(\frac1z\right)^2$$ I don't know where to go from there, so if someone could help me out, that would be great.",,"['multivariable-calculus', 'lagrange-multiplier']"
20,"Double integral $\iint_D |x^3 y^3|\, \mathrm{d}x \mathrm{d}y$",Double integral,"\iint_D |x^3 y^3|\, \mathrm{d}x \mathrm{d}y","Solve the following double integral \begin{equation} \iint_D |x^3 y^3|\, \mathrm{d}x \mathrm{d}y \end{equation} where $D: \{(x,y)\mid x^2+y^2\leq y \}$. Some help please? Thank you very much.","Solve the following double integral \begin{equation} \iint_D |x^3 y^3|\, \mathrm{d}x \mathrm{d}y \end{equation} where $D: \{(x,y)\mid x^2+y^2\leq y \}$. Some help please? Thank you very much.",,['multivariable-calculus']
21,Calculate volume in a 3D sort of space using cartesian coordinates,Calculate volume in a 3D sort of space using cartesian coordinates,,"Find the volume bounded by the cylinder $x^2 + y^2 = 1$, the planes $x=0, z=0, z=y$ and lies in the first octant. (where x, y, and z are all positive)","Find the volume bounded by the cylinder $x^2 + y^2 = 1$, the planes $x=0, z=0, z=y$ and lies in the first octant. (where x, y, and z are all positive)",,"['integration', 'multivariable-calculus']"
22,Can the directional derivative be defined as $\lim_{\|v\|\rightarrow 0}\frac{f(\mathbf{x} + \mathbf{v})-f(\mathbf{x})}{||\mathbf{v}||}$?,Can the directional derivative be defined as ?,\lim_{\|v\|\rightarrow 0}\frac{f(\mathbf{x} + \mathbf{v})-f(\mathbf{x})}{||\mathbf{v}||},"I am thinking about the directional derivative. I think that the easiest way how to express it is \begin{equation} \frac{\partial f(\mathbf{x})}{\partial \mathbf{v}} = \lim_{||v|| \rightarrow 0} \frac{f(\mathbf{x} + \mathbf{v})-f(\mathbf{x})}{||\mathbf{v}||}, \end{equation} but the directional derivative is usually defined as \begin{equation} \frac{\partial f(\mathbf{x})}{\partial  \mathbf{v}} = \lim_{h \rightarrow 0} \frac{f(\mathbf{x} + h \mathbf{v})-f(\mathbf{x})}{h}. \end{equation} Can you rigorously explain the transition from first and second definition? Thanks! Edit: Just to make it clear. The first definition is primarily wrong because the orientation of the directional vector is not fixed. So if I correct it like this (switching to conventional notation) \begin{equation} \nabla_{\mathbf{v}} f(\mathbf{x}) = \lim_{||\mathbf{v}|| \to 0} \frac{f(\mathbf{x} + ||\mathbf{v}||\mathbf{\hat{v}})-f(\mathbf{x})}{||\mathbf{v}||}, \end{equation} it makes a little bit more sense ($ \mathbf{\hat{v}}$ denotes unit vector). BUT the norm allows to get close to zero just from right side ($ ||\mathbf{v}|| \to 0+ $) and the limit makes sense even for vector reversal (~ negative norm). So we can actually use any scalar $h$ scaling the vector and write the derivative as \begin{equation} \nabla_{\mathbf{v}} f(\mathbf{x}) = \lim_{h \to 0} \frac{f(\mathbf{x} + h\mathbf{\hat{v}})-f(\mathbf{x})}{h} \end{equation} or you can find it equivalently written as \begin{equation} \nabla_{\mathbf{v}} f(\mathbf{x}) = \lim_{h \to 0} \frac{f(\mathbf{x} + h\mathbf{v})-f(\mathbf{x})}{h||\mathbf{v}||}. \end{equation}","I am thinking about the directional derivative. I think that the easiest way how to express it is \begin{equation} \frac{\partial f(\mathbf{x})}{\partial \mathbf{v}} = \lim_{||v|| \rightarrow 0} \frac{f(\mathbf{x} + \mathbf{v})-f(\mathbf{x})}{||\mathbf{v}||}, \end{equation} but the directional derivative is usually defined as \begin{equation} \frac{\partial f(\mathbf{x})}{\partial  \mathbf{v}} = \lim_{h \rightarrow 0} \frac{f(\mathbf{x} + h \mathbf{v})-f(\mathbf{x})}{h}. \end{equation} Can you rigorously explain the transition from first and second definition? Thanks! Edit: Just to make it clear. The first definition is primarily wrong because the orientation of the directional vector is not fixed. So if I correct it like this (switching to conventional notation) \begin{equation} \nabla_{\mathbf{v}} f(\mathbf{x}) = \lim_{||\mathbf{v}|| \to 0} \frac{f(\mathbf{x} + ||\mathbf{v}||\mathbf{\hat{v}})-f(\mathbf{x})}{||\mathbf{v}||}, \end{equation} it makes a little bit more sense ($ \mathbf{\hat{v}}$ denotes unit vector). BUT the norm allows to get close to zero just from right side ($ ||\mathbf{v}|| \to 0+ $) and the limit makes sense even for vector reversal (~ negative norm). So we can actually use any scalar $h$ scaling the vector and write the derivative as \begin{equation} \nabla_{\mathbf{v}} f(\mathbf{x}) = \lim_{h \to 0} \frac{f(\mathbf{x} + h\mathbf{\hat{v}})-f(\mathbf{x})}{h} \end{equation} or you can find it equivalently written as \begin{equation} \nabla_{\mathbf{v}} f(\mathbf{x}) = \lim_{h \to 0} \frac{f(\mathbf{x} + h\mathbf{v})-f(\mathbf{x})}{h||\mathbf{v}||}. \end{equation}",,"['multivariable-calculus', 'derivatives']"
23,Are the derivatives of symmetric functions symmetric?,Are the derivatives of symmetric functions symmetric?,,"Suppose $f(x,y)=f(y,x)$. Does it follow that $\frac{\partial f}{\partial x}=\frac{\partial f}{\partial y}$? Intuitively it seems like it must, because taking a ""step"" in the $x$ direction must be the same as taking one in the $y$. But when I try to prove it I get lost as to which is ""really"" x or y. EDIT: I should have been more clear. What I meant was: does $\frac{\partial f(x,y)}{\partial x}=\frac{\partial f(y,x)}{\partial y}$? (At some intuitive level this is like doing a ""find and replace"" s/x/y/, but my intuition fails when taking the derivative.)","Suppose $f(x,y)=f(y,x)$. Does it follow that $\frac{\partial f}{\partial x}=\frac{\partial f}{\partial y}$? Intuitively it seems like it must, because taking a ""step"" in the $x$ direction must be the same as taking one in the $y$. But when I try to prove it I get lost as to which is ""really"" x or y. EDIT: I should have been more clear. What I meant was: does $\frac{\partial f(x,y)}{\partial x}=\frac{\partial f(y,x)}{\partial y}$? (At some intuitive level this is like doing a ""find and replace"" s/x/y/, but my intuition fails when taking the derivative.)",,['multivariable-calculus']
24,Finding an equation and parametric description given 3 points,Finding an equation and parametric description given 3 points,,"Let m be the plane through (0,1,1), (0,1,0) and (-2,-1,-1). This concept has always confused me: How would I find the equation and parametric description given just these points?? I think the parametric description is just (0,0,1)+t(0,0,1)+s(-2,-2,1) for some t and s; but how do you derive a formula for the plane given this information?","Let m be the plane through (0,1,1), (0,1,0) and (-2,-1,-1). This concept has always confused me: How would I find the equation and parametric description given just these points?? I think the parametric description is just (0,0,1)+t(0,0,1)+s(-2,-2,1) for some t and s; but how do you derive a formula for the plane given this information?",,"['calculus', 'multivariable-calculus', 'parametric']"
25,Differentiability of a function and existence of directional derivatives,Differentiability of a function and existence of directional derivatives,,"I have a problem. In the book of Classical Elemental Analysis of Hoffman - Marsden assure that the function $f: \mathbb{R}^2 \to \mathbb{R}$ defined by $f(x,y) = \frac{xy}{x^2+y}$ if $x^2 \neq -y$ and $f(x,y) = 0$ if $x^2 = -y$ is not differentiable because the function $f$ is not continuous in $(0,0)$ but I think it is not true because it is continuous since f(0,0) = 0 and with polar coordinates: $x = r\cos{\theta}, y = r\sin{\theta}$ when $(x,y) \rightarrow (0,0)$ then $r \rightarrow 0^{+}$ , therefore \begin{aligned} \lim_{(x,y) \rightarrow (0,0)}f(x,y) &= \lim_{(x,y) \rightarrow (0,0)} \frac{xy}{x^2+y}\\  &= \lim_{r\rightarrow 0^{+}}\frac{(r\cos{\theta})(r\sin{\theta})}{r^2\cos^2{\theta}+r\sin{\theta}} \\ &= \lim_{r \rightarrow 0^{+}}\frac{r\cos{\theta}\sin{\theta}}{r\cos^2{\theta}+\sin{\theta}} \\ &= 0 \end{aligned} then we have to $\lim_{(x,y) \rightarrow (0,0)}f(x,y) = f(0,0)$ . The book uses that example to show that although all directional derivatives of $f$ exist at $(0,0)$ , the function is not differentiable at $(0,0)$ and use the continuity argument of $f$ in $(0,0)$ but I think it is incorrect. I sense that the function is not differentiable at (0,0) but not with the argument given in the book, I have tried multiple options but I have not been able to. Any suggestions?","I have a problem. In the book of Classical Elemental Analysis of Hoffman - Marsden assure that the function defined by if and if is not differentiable because the function is not continuous in but I think it is not true because it is continuous since f(0,0) = 0 and with polar coordinates: when then , therefore then we have to . The book uses that example to show that although all directional derivatives of exist at , the function is not differentiable at and use the continuity argument of in but I think it is incorrect. I sense that the function is not differentiable at (0,0) but not with the argument given in the book, I have tried multiple options but I have not been able to. Any suggestions?","f: \mathbb{R}^2 \to \mathbb{R} f(x,y) = \frac{xy}{x^2+y} x^2 \neq -y f(x,y) = 0 x^2 = -y f (0,0) x = r\cos{\theta}, y = r\sin{\theta} (x,y) \rightarrow (0,0) r \rightarrow 0^{+} \begin{aligned}
\lim_{(x,y) \rightarrow (0,0)}f(x,y) &= \lim_{(x,y) \rightarrow (0,0)} \frac{xy}{x^2+y}\\
 &= \lim_{r\rightarrow 0^{+}}\frac{(r\cos{\theta})(r\sin{\theta})}{r^2\cos^2{\theta}+r\sin{\theta}} \\
&= \lim_{r \rightarrow 0^{+}}\frac{r\cos{\theta}\sin{\theta}}{r\cos^2{\theta}+\sin{\theta}} \\
&= 0
\end{aligned} \lim_{(x,y) \rightarrow (0,0)}f(x,y) = f(0,0) f (0,0) (0,0) f (0,0)","['real-analysis', 'calculus', 'multivariable-calculus', 'vector-analysis']"
26,Matrix of the derivative of a differentiable function $f$ at the origin with respect to the standard basis of $\Bbb R^2$.,Matrix of the derivative of a differentiable function  at the origin with respect to the standard basis of .,f \Bbb R^2,"Let $f(x,y)=(u(x,y),v(x,y)):\Bbb R^2\rightarrow\Bbb R^2$ be a differentiable function.Let $A$ denote the matrix of derivative of $f$ at the origin with respect to the standard basis of $\Bbb R^2$ .Assume $f(y,-x)=(v(x,y),-u(x,y))$ for all $(x,y)\in\Bbb R^2$ .Which of the following statements are possibly true? $(1)$ . $A=\begin{bmatrix}1&0\\ 0&1 \end{bmatrix}$ $(2)$ . $A=\begin{bmatrix}0&-1\\ 1&0 \end{bmatrix}$ $(3)$ . $A=\begin{bmatrix}1&2\\ -1&-2 \end{bmatrix}$ $(4)$ . $A=\begin{bmatrix}2&1\\ -1&2 \end{bmatrix}$ Let $f(x,y)=(x,y)$ then $f(y,-x)=(y,-x)$ therefore $f$ satisfies all the conditions. Here $u(x,y)=x$ and $v(x,y)=y$ . Therefore matrix of derivative of $f$ at the origin with respect to the standard basis of $\Bbb R^2$ is given by $A=\begin{bmatrix}u_x&u_y\\ v_x&v_y \end{bmatrix}=\begin{bmatrix}1&0\\ 0&1 \end{bmatrix}$ .Therefore option (1) is correct. I am unable to solve this question completely and also unable to use the given information $f(y,-x)=(v(x,y),-u(x,y))$ for all $(x,y)\in\Bbb R^2$ . Is there any general method to solve this problem?",Let be a differentiable function.Let denote the matrix of derivative of at the origin with respect to the standard basis of .Assume for all .Which of the following statements are possibly true? . . . . Let then therefore satisfies all the conditions. Here and . Therefore matrix of derivative of at the origin with respect to the standard basis of is given by .Therefore option (1) is correct. I am unable to solve this question completely and also unable to use the given information for all . Is there any general method to solve this problem?,"f(x,y)=(u(x,y),v(x,y)):\Bbb R^2\rightarrow\Bbb R^2 A f \Bbb R^2 f(y,-x)=(v(x,y),-u(x,y)) (x,y)\in\Bbb R^2 (1) A=\begin{bmatrix}1&0\\
0&1
\end{bmatrix} (2) A=\begin{bmatrix}0&-1\\
1&0
\end{bmatrix} (3) A=\begin{bmatrix}1&2\\
-1&-2
\end{bmatrix} (4) A=\begin{bmatrix}2&1\\
-1&2
\end{bmatrix} f(x,y)=(x,y) f(y,-x)=(y,-x) f u(x,y)=x v(x,y)=y f \Bbb R^2 A=\begin{bmatrix}u_x&u_y\\
v_x&v_y
\end{bmatrix}=\begin{bmatrix}1&0\\
0&1
\end{bmatrix} f(y,-x)=(v(x,y),-u(x,y)) (x,y)\in\Bbb R^2","['real-analysis', 'linear-algebra', 'matrices', 'multivariable-calculus']"
27,"Calculate $I = \int_{0}^{\infty} \int_{0}^{\infty} \frac{\ln(xy)}{(x^2 + x + 1)(y^2 + y + 1)} \,dx \,dy$",Calculate,"I = \int_{0}^{\infty} \int_{0}^{\infty} \frac{\ln(xy)}{(x^2 + x + 1)(y^2 + y + 1)} \,dx \,dy","Question Evaluate $$I = \int_{0}^{\infty} \int_{0}^{\infty} \frac{\ln(xy)}{(x^2 + x + 1)(y^2 + y + 1)} \,dx \,dy$$ My try $$I = \int_{0}^{\infty} \int_{0}^{\infty} \frac{\ln(x)}{(x^2 + x + 1)(y^2 + y + 1)} \,dx \,dy + \int_{0}^{\infty} \int_{0}^{\infty} \frac{\ln(y)}{(x^2 + x + 1)(y^2 + y + 1)} \,dx \,dy$$ $$+ 2 \int_{0}^{\infty} \int_{0}^{\infty} \frac{\ln(x) \cdot \ln(y)}{(x^2 + x + 1)(y^2 + y + 1)} \,dx \,dy$$ $$= 2 \int_{0}^{\infty} \frac{\ln(x)}{x^2 + x + 1} \,dx \cdot \int_{0}^{\infty} \frac{1}{y^2 + y + 1} \,dy \quad (\text{symmetry})$$",Question Evaluate My try,"I = \int_{0}^{\infty} \int_{0}^{\infty} \frac{\ln(xy)}{(x^2 + x + 1)(y^2 + y + 1)} \,dx \,dy I = \int_{0}^{\infty} \int_{0}^{\infty} \frac{\ln(x)}{(x^2 + x + 1)(y^2 + y + 1)} \,dx \,dy + \int_{0}^{\infty} \int_{0}^{\infty} \frac{\ln(y)}{(x^2 + x + 1)(y^2 + y + 1)} \,dx \,dy + 2 \int_{0}^{\infty} \int_{0}^{\infty} \frac{\ln(x) \cdot \ln(y)}{(x^2 + x + 1)(y^2 + y + 1)} \,dx \,dy = 2 \int_{0}^{\infty} \frac{\ln(x)}{x^2 + x + 1} \,dx \cdot \int_{0}^{\infty} \frac{1}{y^2 + y + 1} \,dy \quad (\text{symmetry})","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'improper-integrals']"
28,Finding the Inverse of a Function on the Sphere,Finding the Inverse of a Function on the Sphere,,"I am attempting to find the inverse of the function: $f:S^2 \to S^2$ , defined as: $f(x,y,z)=(x\cos(z)+y\sin(z), x\sin(z)-y\cos(z),z)$ My approach so far has been to use linearity and express the function as a sum of scaled basis vectors: $f(x,y,z)=x(\cos(z),\ sin(z),0)+y(\sin(z), -\cos(z),0)+z(0,0,1)$ However, I'm unsure how to proceed from this point. Any guidance or assistance would be greatly appreciated! Thanks.","I am attempting to find the inverse of the function: , defined as: My approach so far has been to use linearity and express the function as a sum of scaled basis vectors: However, I'm unsure how to proceed from this point. Any guidance or assistance would be greatly appreciated! Thanks.","f:S^2 \to S^2 f(x,y,z)=(x\cos(z)+y\sin(z), x\sin(z)-y\cos(z),z) f(x,y,z)=x(\cos(z),\ sin(z),0)+y(\sin(z), -\cos(z),0)+z(0,0,1)","['calculus', 'linear-algebra', 'multivariable-calculus', 'functions']"
29,Can't logically find critical points but everything works,Can't logically find critical points but everything works,,"I'm trying to find the critical points of: $$ f(x,y) = (x^2 + 2y^2)e^{1-x^2-y^2} $$ I applied the product rule and chain rule for each of the partial derivatives to get these two: $$f_x(x,y) = 2x(e^{1-x^2-y^2}) + (x^2 + 2y^2)(-2x)(e^{1-x^2-y^2})$$ $$f_y(x,y) = 4y(e^{1-x^2-y^2}) + (x^2 + 2y^2)(-2y)(e^{1-x^2-y^2})$$ When I set those two to zero I can reduce them to: $$x^2 + 2y^2 = 1$$ $$x^2 + 2y^2 = 2$$ Which I'm pretty sure is right. Trying to solve those two I only get nonsense like $1 = 2$ or the square root of a negative. I looked at the answer and the points are $(1,0), (-1,0), (0,1), (0,-1), (0,0)$ I can see that those work out with the two formulas right above but I can't get there logically. Is there a flaw in my reasoning so far or do I just need to keep working to solve those two partials ? Also if $y$ is undefined for a value of x does that make it critical value of $x$ ?",I'm trying to find the critical points of: I applied the product rule and chain rule for each of the partial derivatives to get these two: When I set those two to zero I can reduce them to: Which I'm pretty sure is right. Trying to solve those two I only get nonsense like or the square root of a negative. I looked at the answer and the points are I can see that those work out with the two formulas right above but I can't get there logically. Is there a flaw in my reasoning so far or do I just need to keep working to solve those two partials ? Also if is undefined for a value of x does that make it critical value of ?," f(x,y) = (x^2 + 2y^2)e^{1-x^2-y^2}  f_x(x,y) = 2x(e^{1-x^2-y^2}) + (x^2 + 2y^2)(-2x)(e^{1-x^2-y^2}) f_y(x,y) = 4y(e^{1-x^2-y^2}) + (x^2 + 2y^2)(-2y)(e^{1-x^2-y^2}) x^2 + 2y^2 = 1 x^2 + 2y^2 = 2 1 = 2 (1,0), (-1,0), (0,1), (0,-1), (0,0) y x",['multivariable-calculus']
30,The directional derivative equals dot product of gradient and a unit vector. But what if the function is not totally differentiable?,The directional derivative equals dot product of gradient and a unit vector. But what if the function is not totally differentiable?,,"The directional derivative is the dot product of gradient and a unit vector. But what if the function is not totally differentiable? Is it an implicit assumption that formula only applies to totally differentiable functions? Apostol Volume 2 does not really explicitly spell it out, and I am convinced that the formula only holds when the function is totally differentiable, I just want some confirmation in this regard. Furthermore, in many problems when the directional derivate is being asked to be computed, the author simply invokes the above formula, without PROVING total differentiability. So this then begs the question: Does the formula make any sense if the function is not totally differentiable? In other words, is the gradient a concept that 'exists' on its own or is defined 'through' total differentiability, and therefore implicitly subsumes the prerequisite of total differentiability?","The directional derivative is the dot product of gradient and a unit vector. But what if the function is not totally differentiable? Is it an implicit assumption that formula only applies to totally differentiable functions? Apostol Volume 2 does not really explicitly spell it out, and I am convinced that the formula only holds when the function is totally differentiable, I just want some confirmation in this regard. Furthermore, in many problems when the directional derivate is being asked to be computed, the author simply invokes the above formula, without PROVING total differentiability. So this then begs the question: Does the formula make any sense if the function is not totally differentiable? In other words, is the gradient a concept that 'exists' on its own or is defined 'through' total differentiability, and therefore implicitly subsumes the prerequisite of total differentiability?",,"['real-analysis', 'multivariable-calculus', 'partial-derivative', 'vector-analysis']"
31,"If $a,b,c >0 : ab+bc+ca=3,$ find maximal value $\sum\dfrac{a\sqrt{a^2+2}}{a^2+3}$",If  find maximal value,"a,b,c >0 : ab+bc+ca=3, \sum\dfrac{a\sqrt{a^2+2}}{a^2+3}","Question If $a,b,c >0 : ab+bc+ca=3,$ find maximal value $$M=\dfrac{a\sqrt{a^2+2}}{a^2+3}+\dfrac{b\sqrt{b^2+2}}{b^2+3}+\dfrac{c\sqrt{c^2+2}}{c^2+3}$$ By $a=b=c=1,$ I try prove $M\le \dfrac{3\sqrt{3}}{4}$ or $$\sum_{cyc}(ab+ac)\sqrt{a^2+2}\le \dfrac{3\sqrt{3}}{4}(a+b)(b+c)(c+a)$$ I don't know how to find upbound $\sum_{cyc}(ab+ac)\sqrt{a^2+2}$ I need some advices. Thanks.",Question If find maximal value By I try prove or I don't know how to find upbound I need some advices. Thanks.,"a,b,c >0 : ab+bc+ca=3, M=\dfrac{a\sqrt{a^2+2}}{a^2+3}+\dfrac{b\sqrt{b^2+2}}{b^2+3}+\dfrac{c\sqrt{c^2+2}}{c^2+3} a=b=c=1, M\le \dfrac{3\sqrt{3}}{4} \sum_{cyc}(ab+ac)\sqrt{a^2+2}\le \dfrac{3\sqrt{3}}{4}(a+b)(b+c)(c+a) \sum_{cyc}(ab+ac)\sqrt{a^2+2}","['multivariable-calculus', 'inequality', 'cauchy-schwarz-inequality', 'symmetric-polynomials', 'convexity-inequality']"
32,Integration using Stokes' theorem,Integration using Stokes' theorem,,I have a problem that I can't seem to figure out. Given a surface $S$ : \begin{cases} x^2+y^2 \leq 1 \newline z = y^2 \end{cases} Let $C$ be the edge of $S$ . $C$ is oriented so that the projection of $C$ on the $xy$ -plane runs counter-clockwise. Calculate: \begin{equation} \oint_{C}y^2dx + xy^2 dy+ xzdz \end{equation} Directly Using Stokes's theorem I've tried to make a sketch of the area with the orientation: I've at least gotten \begin{equation} \vec{F} = y^2 \vec{i} + xy^2 \vec{j} + xz \vec{k} \end{equation} and \begin{equation} curlF = -z\vec{j} + (y^2-2y)\vec{k} \end{equation} For calculating the integral directly I've tried a parametrization: \begin{cases} x = r\cos(t) \newline y = r\sin(t) \newline z = y^2 = r^2\sin^2(t) \end{cases} with $0 \leq r \leq 1$ and $0 \leq t \leq 2\pi$ . This gave me: \begin{equation} \int_{0}^{1} \int_{0}^{2\pi} r^2\sin^2(t) + r^2\cos(t)\sin^2(t) + r^3\cos(t)\sin^2(t) dtdr = \frac{1}{3} \pi \end{equation} But now for $\iint_{R} curlF \cdot N dS$ I can't figure out how to proceed. I can't seem to figure out what $N$ is and what integration bounds I need to use. The examples in my book and my lecture  aren't all that similar and I don't see how to apply those to this situation. If someone could give me a hint on how to proceed I'd be very grateful.,I have a problem that I can't seem to figure out. Given a surface : Let be the edge of . is oriented so that the projection of on the -plane runs counter-clockwise. Calculate: Directly Using Stokes's theorem I've tried to make a sketch of the area with the orientation: I've at least gotten and For calculating the integral directly I've tried a parametrization: with and . This gave me: But now for I can't figure out how to proceed. I can't seem to figure out what is and what integration bounds I need to use. The examples in my book and my lecture  aren't all that similar and I don't see how to apply those to this situation. If someone could give me a hint on how to proceed I'd be very grateful.,"S \begin{cases}
x^2+y^2 \leq 1 \newline
z = y^2
\end{cases} C S C C xy \begin{equation}
\oint_{C}y^2dx + xy^2 dy+ xzdz
\end{equation} \begin{equation}
\vec{F} = y^2 \vec{i} + xy^2 \vec{j} + xz \vec{k}
\end{equation} \begin{equation}
curlF = -z\vec{j} + (y^2-2y)\vec{k}
\end{equation} \begin{cases}
x = r\cos(t) \newline
y = r\sin(t) \newline
z = y^2 = r^2\sin^2(t)
\end{cases} 0 \leq r \leq 1 0 \leq t \leq 2\pi \begin{equation}
\int_{0}^{1} \int_{0}^{2\pi} r^2\sin^2(t) + r^2\cos(t)\sin^2(t) + r^3\cos(t)\sin^2(t) dtdr = \frac{1}{3} \pi
\end{equation} \iint_{R} curlF \cdot N dS N","['integration', 'multivariable-calculus', '3d', 'stokes-theorem', 'curl']"
33,"Double integral $\iint_{D} \left(x^2+y^2\right)\mathrm{d}x \mathrm{d}y$ over the domain $D$: $x\le x^2+y^2 \le 2x,\ \ y\ge 0$",Double integral  over the domain :,"\iint_{D} \left(x^2+y^2\right)\mathrm{d}x \mathrm{d}y D x\le x^2+y^2 \le 2x,\ \ y\ge 0","$$\iint\limits_{D} \left(x^2+y^2\right)\mathrm{d}x \mathrm{d}y$$ Where the domain is $D$ : $x\le x^2+y^2 \le 2x,\ \ y\ge 0$ So far I have tried getting the bounds by working on the domain and taking the right part i could determine that $y^2+(x-1)^2 = 1$ which would be a circle, but other than that i have no idea how to work the first part of the inequality","Where the domain is : So far I have tried getting the bounds by working on the domain and taking the right part i could determine that which would be a circle, but other than that i have no idea how to work the first part of the inequality","\iint\limits_{D} \left(x^2+y^2\right)\mathrm{d}x \mathrm{d}y D x\le x^2+y^2 \le 2x,\ \ y\ge 0 y^2+(x-1)^2 = 1","['calculus', 'integration', 'multivariable-calculus', 'multiple-integral']"
34,Can this intuitive explanation of the Change of Variable Theorem be turned into a proof?,Can this intuitive explanation of the Change of Variable Theorem be turned into a proof?,,"Reading the following intuitive explanation of the Change of Variable Theorem, I wonder if the explanation provided can be made into a rigorous proof. As of now, I'm merely interested in the case of Riemann integration: Theorem: let $A\subseteq \mathbb{R}^n$ be a $C^1$ injective function such that $\det g'(a) \ne 0$ for any $a\in A$ . If $f:g(A)\to \mathbb{R}$ is (Riemann) integrable, then $$\int_{g(A)}f = \int_A(f\circ g) |\det g'|.$$ Other proofs I've seen take a different, seemingly less intuitive route. As an example, Spivak's Calculus on Manifolds first applies multiple 'reductions' (e.g. showing that it is sufficient to prove the result whenever $g$ is a linear transformation), and continues by applying induction on the dimension $n$ of $\mathbb{R}^n$ .","Reading the following intuitive explanation of the Change of Variable Theorem, I wonder if the explanation provided can be made into a rigorous proof. As of now, I'm merely interested in the case of Riemann integration: Theorem: let be a injective function such that for any . If is (Riemann) integrable, then Other proofs I've seen take a different, seemingly less intuitive route. As an example, Spivak's Calculus on Manifolds first applies multiple 'reductions' (e.g. showing that it is sufficient to prove the result whenever is a linear transformation), and continues by applying induction on the dimension of .",A\subseteq \mathbb{R}^n C^1 \det g'(a) \ne 0 a\in A f:g(A)\to \mathbb{R} \int_{g(A)}f = \int_A(f\circ g) |\det g'|. g n \mathbb{R}^n,"['integration', 'multivariable-calculus', 'proof-explanation', 'alternative-proof', 'riemann-integration']"
35,"Is any function on $(\mathbb S^1)^4$ which is invariant under isometries, invariant under permutations?","Is any function on  which is invariant under isometries, invariant under permutations?",(\mathbb S^1)^4,"$\newcommand{\S}{\mathbb{S}^1}$ This might be silly, but here it goes: Let $E:(\mathbb{S}^1)^4 \to \mathbb{R}$ be a smooth function. Let $f \in \text{Iso}(\mathbb{S}^1)$ be an isometry of $\mathbb{S}^1$ . $f$ induces a map $\tilde f:(\mathbb{S}^1)^4 \to (\mathbb{S}^1)^4$ , given by $$ \tilde f(x_1,x_2,x_3,x_4):=\big(f(x_1),f(x_2),f(x_3),f(x_4)\big). $$ Suppose that $E \circ \tilde f = E$ for any $f \in \text{Iso}(\mathbb{S}^1)$ . Is it true that $$ E(x_1,x_2,x_3,x_4)=E(x_{\sigma(1)},x_{\sigma(2)},x_{\sigma(3)},x_{\sigma(4)}) $$ for any permutation $\sigma \in S^4$ and any $(x_1,x_2,x_3,x_4) \in (\mathbb{S}^1)^4$ ? I guess that the answer is negative, but I am not sure.","This might be silly, but here it goes: Let be a smooth function. Let be an isometry of . induces a map , given by Suppose that for any . Is it true that for any permutation and any ? I guess that the answer is negative, but I am not sure.","\newcommand{\S}{\mathbb{S}^1} E:(\mathbb{S}^1)^4 \to \mathbb{R} f \in \text{Iso}(\mathbb{S}^1) \mathbb{S}^1 f \tilde f:(\mathbb{S}^1)^4 \to (\mathbb{S}^1)^4 
\tilde f(x_1,x_2,x_3,x_4):=\big(f(x_1),f(x_2),f(x_3),f(x_4)\big).
 E \circ \tilde f = E f \in \text{Iso}(\mathbb{S}^1) 
E(x_1,x_2,x_3,x_4)=E(x_{\sigma(1)},x_{\sigma(2)},x_{\sigma(3)},x_{\sigma(4)})
 \sigma \in S^4 (x_1,x_2,x_3,x_4) \in (\mathbb{S}^1)^4","['multivariable-calculus', 'differential-geometry', 'examples-counterexamples', 'symmetry', 'symmetric-functions']"
36,Calculate a triple integral where the integrand has some symmetry,Calculate a triple integral where the integrand has some symmetry,,"Calculate $$I = \iiint\limits_{V_n}x^{n-1}y^{n-1}z^{n-1}\sqrt{1-x^n-y^n-z^n}\,dx\,dy\,dz$$ where $$V_n = \{(x,y,z) \in \mathbb{R}^3 \mid x^n + y^n + z^n \leq 1,\ x \geq 0,\  y \geq 0,\ z \geq 0\},\ n \in \mathbb{Z}^+,\ n \geq 1$$ I tried changing variables, and applying symmetry arguments. I think I can use mathematical induction, but I have to guess what is the final result. Any hints in these directions?","Calculate where I tried changing variables, and applying symmetry arguments. I think I can use mathematical induction, but I have to guess what is the final result. Any hints in these directions?","I = \iiint\limits_{V_n}x^{n-1}y^{n-1}z^{n-1}\sqrt{1-x^n-y^n-z^n}\,dx\,dy\,dz V_n = \{(x,y,z) \in \mathbb{R}^3 \mid x^n + y^n + z^n \leq 1,\ x \geq
0,\  y \geq 0,\ z \geq 0\},\ n \in \mathbb{Z}^+,\ n \geq 1","['multivariable-calculus', 'definite-integrals']"
37,"Evaluate $\lim_{(x,y) \to (0,0)}\frac{-xy}{x^2 + y^2}$",Evaluate,"\lim_{(x,y) \to (0,0)}\frac{-xy}{x^2 + y^2}","Evaluate the following limit: $$\lim_{(x,y) \to (0,0)}\frac{-xy}{x^2 + y^2}$$ My try: Using polar substitution - putting $x =r\cos(\theta)$ , $y = r\sin(\theta)$ , $$\implies\lim_{r \to 0} \frac{-r^2\cos(\theta) \sin(\theta)}{r^2(\sin^2\theta+\cos^2\theta)} =\lim_{r \to 0} -\cos(\theta) \sin(\theta) = -\cos(\theta) \sin(\theta)$$ Does this make any sense? Also, approaching from two different sides, $$\begin{align}\lim_{(x,y) \to (0,0)}\frac{-xy}{x^2 + y^2} & = \lim_{x\to 0}\frac{-x(0)}{x^2 + (0)^2} = 0\\\lim_{(x,y) \to (0,0)}\frac{-xy}{x^2 + y^2} & = \lim_{y\to 0}\frac{-(0)y}{(0)^2 + y^2} = 0 \end{align}$$ Both are equal... but the limit doesn't exist which I verified from Symbolab . What should I do to evaluate this?","Evaluate the following limit: My try: Using polar substitution - putting , , Does this make any sense? Also, approaching from two different sides, Both are equal... but the limit doesn't exist which I verified from Symbolab . What should I do to evaluate this?","\lim_{(x,y) \to (0,0)}\frac{-xy}{x^2 + y^2} x =r\cos(\theta) y = r\sin(\theta) \implies\lim_{r \to 0} \frac{-r^2\cos(\theta) \sin(\theta)}{r^2(\sin^2\theta+\cos^2\theta)} =\lim_{r \to 0} -\cos(\theta) \sin(\theta) = -\cos(\theta) \sin(\theta) \begin{align}\lim_{(x,y) \to (0,0)}\frac{-xy}{x^2 + y^2} & = \lim_{x\to 0}\frac{-x(0)}{x^2 + (0)^2} = 0\\\lim_{(x,y) \to (0,0)}\frac{-xy}{x^2 + y^2} & = \lim_{y\to 0}\frac{-(0)y}{(0)^2 + y^2} = 0 \end{align}",['limits']
38,Is my understanding about this complex contour integral correct?,Is my understanding about this complex contour integral correct?,,I have to evaluate $$\int_{\gamma} \sin(z)\cos(2z)dz$$ over the above contour. My question is: Is the info given sufficient to solve this problem (just the contour image and integral) and can I take $1$ and $-0.5$ as my endpoints/bounds to solve this integral?,I have to evaluate over the above contour. My question is: Is the info given sufficient to solve this problem (just the contour image and integral) and can I take and as my endpoints/bounds to solve this integral?,\int_{\gamma} \sin(z)\cos(2z)dz 1 -0.5,"['calculus', 'complex-analysis', 'multivariable-calculus']"
39,"What are the geometric, harmonic, and quadratic averages of a function?","What are the geometric, harmonic, and quadratic averages of a function?",,"In Mean of a function , they describe the arithmetic mean of a function and at the bottom of the article they said: There is also a harmonic average of functions and a quadratic average (or root mean square) of functions. My question is what is the form of these averages?","In Mean of a function , they describe the arithmetic mean of a function and at the bottom of the article they said: There is also a harmonic average of functions and a quadratic average (or root mean square) of functions. My question is what is the form of these averages?",,"['calculus', 'multivariable-calculus', 'average', 'means']"
40,Partial derivatives of $A^n x$ with respect to the entries of $A$,Partial derivatives of  with respect to the entries of,A^n x A,"Let $A \in \mathbb{R}^{n \times n}$ and $x \in \mathbb{R}^{n \times 1}$ . I am wondering what the partial derivative of each entry of $A^n x$ with respect to the entries of $A$ is. Is there a closed form expression for this? $$\frac{\partial}{\partial A_{i,j}} A^nx$$ I wasn't able to find an answer.",Let and . I am wondering what the partial derivative of each entry of with respect to the entries of is. Is there a closed form expression for this? I wasn't able to find an answer.,"A \in \mathbb{R}^{n \times n} x \in \mathbb{R}^{n \times 1} A^n x A \frac{\partial}{\partial A_{i,j}} A^nx","['matrices', 'multivariable-calculus', 'partial-derivative', 'matrix-calculus']"
41,What exactly is the relationship and are the differences between multivariable limits and complex limits?,What exactly is the relationship and are the differences between multivariable limits and complex limits?,,"Edit : This question is wrong. Ignore it. I've already flagged to request for deletion. Please just go to that question: What exactly are the differences between real multivariable limits and complex limits? Say you want to disprove the existence of either of the ff $\lim_{z \to 0} \frac{Re(z)}{|z|^2}$ , $\lim_{z \to 0} \frac{Im(z)}{|z|^2}$ (as in here ). It seems we just change the limits to $\lim_{(x,y) \to (0,0)} \frac{x \ \text{or} \ y}{x^2+y^2}$ and then go about this calc2 way. So the rule is that complex limit doesn't exist if the $\mathbb R^2$ limit doesn't exist? In general, for $$\lim_{z \to z_0}[u(z)+iv(z)] \ \text{vs} \ \lim_{(x,y) \to (x_0,y_0)}[u(x,y)+iv(x,y)],$$ where $u$ and $v$ are real functions, is it that the LHS doesn't exist if the RHS doesn't exist? Here, the RHS equals by theorem or by definition to $$\lim_{(x,y) \to (x_0,y_0)}u(x,y)+i\lim_{(x,y) \to (x_0,y_0)}v(x,y)$$ BUT if the RHS exists, the LHS may or may not exist, i.e. existence of real limit is necessary but not sufficient for existence of complex limit? Please provide examples. Note : For now, I'll just say the real functions $u,v$ without specifying specific domains and hope the above makes sense. If need be, then I can edit this question to be more specific about $u,v,z_0$ , etc. Related questions : I've found several questions that talk about the relationship of complex derivative and real derivative, but what I'm not quite seeing is the general case/concept of complex vs real limits. Differences between the complex derivative and the multivariable derivative. Difference between the properties of differentiation in $\mathbb{C}$ and $\mathbb{R}^2$ Scalar field Derivative. Real vs Complex Limit defintion of a function that is $\mathbb{R}$-differentiable but not $\mathbb{C}$ differentiable.","Edit : This question is wrong. Ignore it. I've already flagged to request for deletion. Please just go to that question: What exactly are the differences between real multivariable limits and complex limits? Say you want to disprove the existence of either of the ff , (as in here ). It seems we just change the limits to and then go about this calc2 way. So the rule is that complex limit doesn't exist if the limit doesn't exist? In general, for where and are real functions, is it that the LHS doesn't exist if the RHS doesn't exist? Here, the RHS equals by theorem or by definition to BUT if the RHS exists, the LHS may or may not exist, i.e. existence of real limit is necessary but not sufficient for existence of complex limit? Please provide examples. Note : For now, I'll just say the real functions without specifying specific domains and hope the above makes sense. If need be, then I can edit this question to be more specific about , etc. Related questions : I've found several questions that talk about the relationship of complex derivative and real derivative, but what I'm not quite seeing is the general case/concept of complex vs real limits. Differences between the complex derivative and the multivariable derivative. Difference between the properties of differentiation in $\mathbb{C}$ and $\mathbb{R}^2$ Scalar field Derivative. Real vs Complex Limit defintion of a function that is $\mathbb{R}$-differentiable but not $\mathbb{C}$ differentiable.","\lim_{z \to 0} \frac{Re(z)}{|z|^2} \lim_{z \to 0} \frac{Im(z)}{|z|^2} \lim_{(x,y) \to (0,0)} \frac{x \ \text{or} \ y}{x^2+y^2} \mathbb R^2 \lim_{z \to z_0}[u(z)+iv(z)] \ \text{vs} \ \lim_{(x,y) \to (x_0,y_0)}[u(x,y)+iv(x,y)], u v \lim_{(x,y) \to (x_0,y_0)}u(x,y)+i\lim_{(x,y) \to (x_0,y_0)}v(x,y) u,v u,v,z_0","['real-analysis', 'complex-analysis', 'limits', 'analysis', 'multivariable-calculus']"
42,Tangent line to a two variables function,Tangent line to a two variables function,,"Given the function $$f(x,y)=\sqrt{x^2+y^2-9}$$ I have to find the tangent line to it obtained from the intersection of the plane $y=-3$ with its graph at $(4,-3,4)$ . Since $$\frac{\partial f}{\partial x}(4,-3)=1$$ then the tangent line will be $$y-(-3)=1\cdot(x-4)$$ I want to know if it's correct.",Given the function I have to find the tangent line to it obtained from the intersection of the plane with its graph at . Since then the tangent line will be I want to know if it's correct.,"f(x,y)=\sqrt{x^2+y^2-9} y=-3 (4,-3,4) \frac{\partial f}{\partial x}(4,-3)=1 y-(-3)=1\cdot(x-4)","['multivariable-calculus', 'tangent-line']"
43,"Find $\lim\limits_{(x,y)\to(1,-1)}{\frac{e^{x-y}\tan{(x+y)}}{x^2-y^2}}$",Find,"\lim\limits_{(x,y)\to(1,-1)}{\frac{e^{x-y}\tan{(x+y)}}{x^2-y^2}}","I need to find $\lim\limits_{(x,y)\to(1,-1)}{\frac{e^{x-y}\tan{(x+y)}}{x^2-y^2}}$ . I came up with this: Since $x^2-y^2$ changes its sign, I can'y apply $\tan{(x+y)}\sim(x+y)$ just yet. So, $\lim\limits_{(x,y)\to(1,-1)}{|\frac{e^{x-y}\tan{(x+y)}}{x^2-y^2}|}=\lim\limits_{(x,y)\to(1,-1)}{\frac{e^{x-y}|\tan{(x+y)}|}{|x^2-y^2|}}=\lim\limits_{(x,y)\to(1,-1)}{\frac{e^{x-y}|x+y|}{|x-y|.|x+y|}}=\lim\limits_{(x,y)\to(1,-1)}{\frac{e^{x-y}}{|x-y|}}=\frac{e^2}{2}$ Is this correct and is there a better way to approach the problem?","I need to find . I came up with this: Since changes its sign, I can'y apply just yet. So, Is this correct and is there a better way to approach the problem?","\lim\limits_{(x,y)\to(1,-1)}{\frac{e^{x-y}\tan{(x+y)}}{x^2-y^2}} x^2-y^2 \tan{(x+y)}\sim(x+y) \lim\limits_{(x,y)\to(1,-1)}{|\frac{e^{x-y}\tan{(x+y)}}{x^2-y^2}|}=\lim\limits_{(x,y)\to(1,-1)}{\frac{e^{x-y}|\tan{(x+y)}|}{|x^2-y^2|}}=\lim\limits_{(x,y)\to(1,-1)}{\frac{e^{x-y}|x+y|}{|x-y|.|x+y|}}=\lim\limits_{(x,y)\to(1,-1)}{\frac{e^{x-y}}{|x-y|}}=\frac{e^2}{2}","['limits', 'multivariable-calculus', 'solution-verification']"
44,Prove a limit with $\varepsilon$-$\delta$ definition:,Prove a limit with - definition:,\varepsilon \delta,"the limit we want to prove is: $$\lim_{(x,y)\to(0,0)}\frac{x^4}{x^2+y^2}=0$$ It is actually very simple: the denominator is the square of the norm of $(x,y)$ , thus it follows that, for every $\varepsilon>0$ we must find $\delta>0$ for which it holds: $$\frac{x^4}{\delta^{2}}<\varepsilon$$ but I do not see how to get rid of that $x^4$ in order to define a proper $\delta$ . I imagine one has to use some particular inequality based on squares I must have forgot.","the limit we want to prove is: It is actually very simple: the denominator is the square of the norm of , thus it follows that, for every we must find for which it holds: but I do not see how to get rid of that in order to define a proper . I imagine one has to use some particular inequality based on squares I must have forgot.","\lim_{(x,y)\to(0,0)}\frac{x^4}{x^2+y^2}=0 (x,y) \varepsilon>0 \delta>0 \frac{x^4}{\delta^{2}}<\varepsilon x^4 \delta","['limits', 'multivariable-calculus', 'epsilon-delta']"
45,"If $\theta_t=\frac{1}{r}\frac{\partial}{\partial{r}}(r\theta_r)$, show that $\int_0^{\infty} \theta(r,t)rdr=\int_0^{\infty} \theta(r,0)rdr$?","If , show that ?","\theta_t=\frac{1}{r}\frac{\partial}{\partial{r}}(r\theta_r) \int_0^{\infty} \theta(r,t)rdr=\int_0^{\infty} \theta(r,0)rdr","We have $r>0, t>0$ and we're given the conditions: $$r\theta_r \to 0  \text{ as } r\to \infty$$ and $$\theta(r,t)\leq K\in \mathbb{R}\text{ as }r\to 0$$ I tried taking integrals with respect to $r$ over the PDE. Rearranging the PDE and applying the integral: $\int_0^{\infty}\big(r\theta_t-\frac{\partial}{\partial{r}}(r\theta_r)\big)dr=\int_0^{\infty}r\theta_tdr+\int_0^{\infty}\frac{\partial}{\partial{r}}(r\theta_r)dr=\int_0^{\infty}r\theta_tdr+r\theta_r |_{0}^{\infty}=\int_0^{\infty}r\theta_t(r,t)dr=0 $ Is this correct? From here I can't see how to proceed to get the result in the title?",We have and we're given the conditions: and I tried taking integrals with respect to over the PDE. Rearranging the PDE and applying the integral: Is this correct? From here I can't see how to proceed to get the result in the title?,"r>0, t>0 r\theta_r \to 0  \text{ as } r\to \infty \theta(r,t)\leq K\in \mathbb{R}\text{ as }r\to 0 r \int_0^{\infty}\big(r\theta_t-\frac{\partial}{\partial{r}}(r\theta_r)\big)dr=\int_0^{\infty}r\theta_tdr+\int_0^{\infty}\frac{\partial}{\partial{r}}(r\theta_r)dr=\int_0^{\infty}r\theta_tdr+r\theta_r |_{0}^{\infty}=\int_0^{\infty}r\theta_t(r,t)dr=0 ","['calculus', 'integration', 'multivariable-calculus', 'partial-differential-equations', 'partial-derivative']"
46,Finding extrema of a function of multiple variables,Finding extrema of a function of multiple variables,,"I had to find extrema of the following function: \begin{align} z=x^3y^2(2-x-y)\end{align} I identified first order partial derivatives, which are: \begin{align} z_x^{'}=y^2(3x^2(2-x-y)-x^3)  \\\\ z_y^{'}=x^3(2y(2-x-y)-y^2)\end{align} I then equated the two expressions to $0$ to identifiy critical points for further analysis (points which would be checked for extrema, in my case, by the means of cunstructing and evaluating a Hessian matrix). The solutions for the system of equations I contrived were $y=0; x=0; x=1, y=2/3$ . However, as I was solving the system and going through all the possibilities, I got some roots that were special cases of the $y=0, x\in{\Bbb{R}}$ and $x=0, y\in{\Bbb{R}}$ solutions. Here is an example: \begin{cases} &3x^2(2-x-y)-x^3=0 \\ &x^3(2y(2-x-y)-y^2)=0 \\ \end{cases} \begin{align}x_1=0,y_1=2\\\\x_2=3/2, y_2=0\\\\x_3=1, y_3=2/3 \end{align} Are those points ( $x_1=0,y_1=2; x_2=3/2, y_2=0$ ) critical? If not, then what's the gist here, in what way are those points special? Thank you!","I had to find extrema of the following function: I identified first order partial derivatives, which are: I then equated the two expressions to to identifiy critical points for further analysis (points which would be checked for extrema, in my case, by the means of cunstructing and evaluating a Hessian matrix). The solutions for the system of equations I contrived were . However, as I was solving the system and going through all the possibilities, I got some roots that were special cases of the and solutions. Here is an example: Are those points ( ) critical? If not, then what's the gist here, in what way are those points special? Thank you!","\begin{align} z=x^3y^2(2-x-y)\end{align} \begin{align} z_x^{'}=y^2(3x^2(2-x-y)-x^3) 
\\\\
z_y^{'}=x^3(2y(2-x-y)-y^2)\end{align} 0 y=0; x=0; x=1, y=2/3 y=0, x\in{\Bbb{R}} x=0, y\in{\Bbb{R}} \begin{cases}
&3x^2(2-x-y)-x^3=0 \\
&x^3(2y(2-x-y)-y^2)=0 \\ \end{cases} \begin{align}x_1=0,y_1=2\\\\x_2=3/2, y_2=0\\\\x_3=1, y_3=2/3 \end{align} x_1=0,y_1=2; x_2=3/2, y_2=0","['multivariable-calculus', 'maxima-minima']"
47,"Prove that $\lim_{(x, y) \to (0, 0)} \frac{xy}{\sin(x^2+y^2)}$ does not exist.",Prove that  does not exist.,"\lim_{(x, y) \to (0, 0)} \frac{xy}{\sin(x^2+y^2)}","I'm trying to prove that $$\lim_{(x, y) \to (0, 0)} \frac{xy}{\sin(x^2+y^2)}$$ does not exists. To do so, I'm trying to find two different sequences $\{\bar{a_n}\}$ , $\{\bar{b_n}\}$ such that $\{\bar{a_n}\} \to (0, 0), \{\bar{b_n}\} \to (0, 0)$ and $$\lim_{n\to\infty} f(\bar{a_n}) = L_1 \not = L_2 = \lim_{n\to\infty} f(\bar{b_n})$$ Def. Let $f: A\subseteq \mathbb{R^2} \to \mathbb{R}, (x_0, y_0) \in \mathbb{R^2}$ accumulation point of $A.$ We say that $$\lim_{(x, y) \to (x_0, x_0)} f(x, y) = L \Longleftrightarrow$$ if $\forall \{\bar{a_n}\}$ such that $\forall n\in\mathbb{N}, \bar{a_n} \in A, \bar{a_n}\not = (x_0, y_0)$ and $\lim_{n \to \infty} \bar{a_n} = (x_0, y_0),$ then $$\lim_{n \to \infty} f(\bar{a_n}) = L$$","I'm trying to prove that does not exists. To do so, I'm trying to find two different sequences , such that and Def. Let accumulation point of We say that if such that and then","\lim_{(x, y) \to (0, 0)} \frac{xy}{\sin(x^2+y^2)} \{\bar{a_n}\} \{\bar{b_n}\} \{\bar{a_n}\} \to (0, 0), \{\bar{b_n}\} \to (0, 0) \lim_{n\to\infty} f(\bar{a_n}) = L_1 \not = L_2 = \lim_{n\to\infty} f(\bar{b_n}) f: A\subseteq \mathbb{R^2} \to \mathbb{R}, (x_0, y_0) \in \mathbb{R^2} A. \lim_{(x, y) \to (x_0, x_0)} f(x, y) = L \Longleftrightarrow \forall \{\bar{a_n}\} \forall n\in\mathbb{N}, \bar{a_n} \in A, \bar{a_n}\not = (x_0, y_0) \lim_{n \to \infty} \bar{a_n} = (x_0, y_0), \lim_{n \to \infty} f(\bar{a_n}) = L","['limits', 'multivariable-calculus']"
48,Why is a miracle happening on this double integral?,Why is a miracle happening on this double integral?,,"I am trying to solve a double integral of the form $$\int_0^\infty du\int_0^\infty dx \frac{1}{x}\left(\frac{\partial}{\partial x}\right)^{2n+1}f\left(\sqrt{x^2+u}\right),$$ where $f$ is a well-behaved even function that decays rapidly at infinity. The content of this question (and miracle) does not depend on the specific $f$ , as long as it is even and well behaved, but $f(t)=e^{-t^2}$ is a fine example. Note that since $1/x$ multiplies an odd function, the integrand is nonsingular as $x\to0$ . I would like to have a solution for a generic integer $n$ , in terms of the known quantities $f^{(m)}(0)$ (i.e., arbitrary $m$ th derivatives of $f$ evaluated at zero). Amazingly, such a solution seems to always exist (I have tested up to $n = 5$ ), but the method I am using is extremely tedious, relies on Mathematica, and results in infinite terms that miraculously cancel. So the question is, what's a more efficient and clear method to get the same result? Step 1: Evaluate all $2n+1$ derivatives. Step 2: Transform the first integral with the change of variables $u=y^2,du=2ydy$ . Step 3: Convert to polar coordinates. Step 4: Evaluate the $\theta$ -integral. Step 5: What remains is an integral of the form $\int_0^\infty dr\left(c_1\frac{1}{r^{p_1}}f^{(q_1)}(r) + \cdots + c_r r^{p_N}f^{(q_N)}(r)\right)$ . Some of the terms with powers of $x$ in the denominator are infinite, but integrating by parts on some terms always makes these cancel. The terms with positive powers of $x$ can be integrated by parts repeatedly until they become $\int_0^\infty dr f^{(m)}(r) = - f^{(m-1)}(0)$ . Steps 1–4 can be performed by Mathematica using the single line (here $n=2$ ): Integrate[  Assuming[{r, \[Theta]} \[Element] PositiveReals,    2r Tan[\[Theta]] D[f[Sqrt[x^2 + u]], {x, 2*2 + 1}] /. {x ->        r Cos[\[Theta]], u -> r^2 Sin[\[Theta]]^2} //     Simplify], {\[Theta], 0, \[Pi]/2}] I invite you to copy/paste this line in, examine the output and verify what I have claimed in step 5. It may seem pointless for me to ask this question, given that I already have a method for getting to a solution; however, Mathematica starts taking a very long time to compute this even for $n=5$ , and even still the above line does not consider the final integrations by parts that make several more terms vanish and combine, so I think a better method is needed for both efficiency and understanding's sake.","I am trying to solve a double integral of the form where is a well-behaved even function that decays rapidly at infinity. The content of this question (and miracle) does not depend on the specific , as long as it is even and well behaved, but is a fine example. Note that since multiplies an odd function, the integrand is nonsingular as . I would like to have a solution for a generic integer , in terms of the known quantities (i.e., arbitrary th derivatives of evaluated at zero). Amazingly, such a solution seems to always exist (I have tested up to ), but the method I am using is extremely tedious, relies on Mathematica, and results in infinite terms that miraculously cancel. So the question is, what's a more efficient and clear method to get the same result? Step 1: Evaluate all derivatives. Step 2: Transform the first integral with the change of variables . Step 3: Convert to polar coordinates. Step 4: Evaluate the -integral. Step 5: What remains is an integral of the form . Some of the terms with powers of in the denominator are infinite, but integrating by parts on some terms always makes these cancel. The terms with positive powers of can be integrated by parts repeatedly until they become . Steps 1–4 can be performed by Mathematica using the single line (here ): Integrate[  Assuming[{r, \[Theta]} \[Element] PositiveReals,    2r Tan[\[Theta]] D[f[Sqrt[x^2 + u]], {x, 2*2 + 1}] /. {x ->        r Cos[\[Theta]], u -> r^2 Sin[\[Theta]]^2} //     Simplify], {\[Theta], 0, \[Pi]/2}] I invite you to copy/paste this line in, examine the output and verify what I have claimed in step 5. It may seem pointless for me to ask this question, given that I already have a method for getting to a solution; however, Mathematica starts taking a very long time to compute this even for , and even still the above line does not consider the final integrations by parts that make several more terms vanish and combine, so I think a better method is needed for both efficiency and understanding's sake.","\int_0^\infty du\int_0^\infty dx \frac{1}{x}\left(\frac{\partial}{\partial x}\right)^{2n+1}f\left(\sqrt{x^2+u}\right), f f f(t)=e^{-t^2} 1/x x\to0 n f^{(m)}(0) m f n = 5 2n+1 u=y^2,du=2ydy \theta \int_0^\infty dr\left(c_1\frac{1}{r^{p_1}}f^{(q_1)}(r) + \cdots + c_r r^{p_N}f^{(q_N)}(r)\right) x x \int_0^\infty dr f^{(m)}(r) = - f^{(m-1)}(0) n=2 n=5","['integration', 'multivariable-calculus', 'polar-coordinates']"
49,"Integration of $e^{-\langle Ax , x \rangle}$ over $\mathbb{R}^n$ [duplicate]",Integration of  over  [duplicate],"e^{-\langle Ax , x \rangle} \mathbb{R}^n","This question already has answers here : If $A$ is positive definite, then $\int_{\mathbb{R}^n}\mathrm{e}^{-\langle Ax,x\rangle}\text{d}x=\left|\det\left({\pi}^{-1}A\right)\right|^{-1/2}$ (3 answers) Closed 3 years ago . Problem : If $A_{n \times n}$ is a symmetric, positive-definite matrix, show that : $$\int_{\mathbb{R}^n} e^{-\langle Ax , x \rangle}~ dx = \sqrt{\dfrac{\pi^n}{\det(A)}}$$ where $\langle a , b\rangle$ denotes the inner product of $a$ and $b$ . Approach : I was approaching the problem using the Change of Variable Formula, using the function $\varphi(x) = A^{-1}x$ . Since $A$ is p.d., I can show that it is invertible. But I can't proceed anymore. I found a similar-looking problem here , but couldn't understand anything.","This question already has answers here : If $A$ is positive definite, then $\int_{\mathbb{R}^n}\mathrm{e}^{-\langle Ax,x\rangle}\text{d}x=\left|\det\left({\pi}^{-1}A\right)\right|^{-1/2}$ (3 answers) Closed 3 years ago . Problem : If is a symmetric, positive-definite matrix, show that : where denotes the inner product of and . Approach : I was approaching the problem using the Change of Variable Formula, using the function . Since is p.d., I can show that it is invertible. But I can't proceed anymore. I found a similar-looking problem here , but couldn't understand anything.","A_{n \times n} \int_{\mathbb{R}^n} e^{-\langle Ax , x \rangle}~ dx = \sqrt{\dfrac{\pi^n}{\det(A)}} \langle a , b\rangle a b \varphi(x) = A^{-1}x A",['linear-algebra']
50,"CDF of $\frac{T_1}{T_1+T_2}$ where $T_1,T_2$ are the first two delays in a Poisson process?",CDF of  where  are the first two delays in a Poisson process?,"\frac{T_1}{T_1+T_2} T_1,T_2","$P$ is a Poisson Process with rate $\lambda$ . Let $T_1$ be the time of the first event and let $T_2$ be the time of the from the first to the second event. Let $Y = \frac{T_1}{T_1+T_2}$ . Find the density of $Y$ . I think I should find the CDF first and then take the derivative, but I do not know how to find $P(Y\leq t)$ . Do I need to find the joint density?","is a Poisson Process with rate . Let be the time of the first event and let be the time of the from the first to the second event. Let . Find the density of . I think I should find the CDF first and then take the derivative, but I do not know how to find . Do I need to find the joint density?",P \lambda T_1 T_2 Y = \frac{T_1}{T_1+T_2} Y P(Y\leq t),"['probability', 'multivariable-calculus', 'stochastic-processes', 'poisson-process']"
51,Limit multivarible,Limit multivarible,,"How do I solve this limit? $$\lim_{x,y,z) \to (0,0,0)} \frac{\sin(x^2+y^2+z^2)}{x^2+y^2+z^2+xyz} $$ This is equal to $$\frac{\sin(x^2+y^2+z^2)}{x^2+y^2+z^2}\times\frac1{1+\frac{xyz}{x^2+y^2+z^2}}$$ The first one is a standard limit with value one, but I'n not sure about the other term.","How do I solve this limit? This is equal to The first one is a standard limit with value one, but I'n not sure about the other term.","\lim_{x,y,z) \to (0,0,0)} \frac{\sin(x^2+y^2+z^2)}{x^2+y^2+z^2+xyz}  \frac{\sin(x^2+y^2+z^2)}{x^2+y^2+z^2}\times\frac1{1+\frac{xyz}{x^2+y^2+z^2}}",['multivariable-calculus']
52,"Find $ \lim_{(x,y)\to(0,0)} \frac{\sin( |x| + |y|) + |y|(e^x - 1)} {|x| + |y|} $",Find," \lim_{(x,y)\to(0,0)} \frac{\sin( |x| + |y|) + |y|(e^x - 1)} {|x| + |y|} ","$$ \lim_{(x,y)\to(0,0)} \frac{\sin( |x| + |y|) + |y|(e^x - 1)} {|x| + |y|} $$ I tried in this way $ \lim_{(x,y)\to(0,0)} \frac{\sin( |x| + |y|)}{|x| + |y|} + \frac{|y|(e^x - 1)} {|x| + |y|} $ the first term, when $(x,y)\to(0,0)$ , is $1$ . When $x\to 0 $ we have that $(e^x - 1) \to x$ . Now the limit to solve is: $\lim_{(x,y)\to(0,0)} 1 + \frac{|y|x} {|x| + |y|} = \lim_{(x,y)\to(0,0)} f(x,y) $ $ f(x,y) ≤ | \frac{|y|x} {|x| + |y|} |$ = $ \frac{|y||x|} {|x| + |y|} $ = $ \rho \frac{|\sin(\theta)||\cos(\theta)|}{|\sin(\theta)| + |\cos(\theta)|} $ ≤ $ \rho \frac{1}{2m} $ where $\frac{1}{2}$ is the maximum of the function in the numerator and m is the minimum of the function in the denominator and it is a positive number $ \rho \frac{1}{2m}  \to 0 $ when $\rho \to 0^+$ So the initial limit is 1 Is it ok?","I tried in this way the first term, when , is . When we have that . Now the limit to solve is: = = ≤ where is the maximum of the function in the numerator and m is the minimum of the function in the denominator and it is a positive number when So the initial limit is 1 Is it ok?"," \lim_{(x,y)\to(0,0)} \frac{\sin( |x| + |y|) + |y|(e^x - 1)} {|x| + |y|}   \lim_{(x,y)\to(0,0)} \frac{\sin( |x| + |y|)}{|x| + |y|} + \frac{|y|(e^x - 1)} {|x| + |y|}  (x,y)\to(0,0) 1 x\to 0  (e^x - 1) \to x \lim_{(x,y)\to(0,0)} 1 + \frac{|y|x} {|x| + |y|} = \lim_{(x,y)\to(0,0)} f(x,y)   f(x,y) ≤ | \frac{|y|x} {|x| + |y|} |  \frac{|y||x|} {|x| + |y|}   \rho \frac{|\sin(\theta)||\cos(\theta)|}{|\sin(\theta)| + |\cos(\theta)|}   \rho \frac{1}{2m}  \frac{1}{2}  \rho \frac{1}{2m}  \to 0  \rho \to 0^+","['limits', 'multivariable-calculus', 'polar-coordinates']"
53,"Is $f(x,y)=\frac{xy^3}{x^2+y^6}$ differentiable at $(0,0)$? [duplicate]",Is  differentiable at ? [duplicate],"f(x,y)=\frac{xy^3}{x^2+y^6} (0,0)","This question already has an answer here : Prove that $\lim_{(x,y)\rightarrow(0,0)} \frac{|x|^{a}|y|^{b}}{|x|^{c} + |y|^{d}}$ does not exist (1 answer) Closed 3 years ago . Is the following function differentiable at $(0,0)$ ? $$ \  f(x,y) =   \begin{cases}    \frac{xy^3}{x^2+y^6} & \text{if } (x,y) \ne (0,0), \\    0       & \text{if } (x,y) = (0,0).   \end{cases} $$ I found that both of the partial derivatives are $0$ , and then tried to calculate the following limit: $$\lim_{(x,y) \to (0,0)} \frac{\frac{xy^3}{x^2+y^6}}{\sqrt{x^2+y^2}} = \lim_{(x,y) \to (0,0)} \frac{xy^3}{(x^2+y^6) \sqrt{x^2+y^2}}$$ And then I got stuck. I tried the squeeze theorem, but I still couldn't calculate it. How can I calculate this limit?","This question already has an answer here : Prove that $\lim_{(x,y)\rightarrow(0,0)} \frac{|x|^{a}|y|^{b}}{|x|^{c} + |y|^{d}}$ does not exist (1 answer) Closed 3 years ago . Is the following function differentiable at ? I found that both of the partial derivatives are , and then tried to calculate the following limit: And then I got stuck. I tried the squeeze theorem, but I still couldn't calculate it. How can I calculate this limit?","(0,0)  \
 f(x,y) =
  \begin{cases}
   \frac{xy^3}{x^2+y^6} & \text{if } (x,y) \ne (0,0), \\
   0       & \text{if } (x,y) = (0,0).
  \end{cases}
 0 \lim_{(x,y) \to (0,0)} \frac{\frac{xy^3}{x^2+y^6}}{\sqrt{x^2+y^2}} = \lim_{(x,y) \to (0,0)} \frac{xy^3}{(x^2+y^6) \sqrt{x^2+y^2}}","['limits', 'multivariable-calculus', 'derivatives']"
54,Show that $(a^3+a+1)(b^3+b+1)(c^3+c+1)\le 27$,Show that,(a^3+a+1)(b^3+b+1)(c^3+c+1)\le 27,"Let $a,b,c\ge 0$ be such that $a^2+b^2+c^2=3$ . Show that $$(a^3+a+1)(b^3+b+1)(c^3+c+1)\le 27$$ I want to consider the function $$f(x)=\ln{(x^{3/2}+x^{1/2}+1)}$$ Maybe it isn't the case $f''(x)\le 0$ , so I can't use Jensen's inequality.","Let be such that . Show that I want to consider the function Maybe it isn't the case , so I can't use Jensen's inequality.","a,b,c\ge 0 a^2+b^2+c^2=3 (a^3+a+1)(b^3+b+1)(c^3+c+1)\le 27 f(x)=\ln{(x^{3/2}+x^{1/2}+1)} f''(x)\le 0","['multivariable-calculus', 'inequality', 'summation', 'logarithms', 'tangent-line-method']"
55,"$\lim_{(x,y)\to(0,0)} \frac{x^2y^3}{x^4+2y^6}$ limit calculation",limit calculation,"\lim_{(x,y)\to(0,0)} \frac{x^2y^3}{x^4+2y^6}",I have tried to write the limit using polar coordination. but I remain with a $cos(\theta)$ in the denominator. thanks for the help,I have tried to write the limit using polar coordination. but I remain with a in the denominator. thanks for the help,cos(\theta),"['calculus', 'limits', 'multivariable-calculus']"
56,"Finding $\iint_{A}\frac{dx\,dy}{(1+x^2)(1+x^2 y^2)}$ with Fubini's theorem",Finding  with Fubini's theorem,"\iint_{A}\frac{dx\,dy}{(1+x^2)(1+x^2 y^2)}","I have to solve the following double integral $$\iint_{A}\frac{dx\,dy}{(1+x^2)(1+x^2 y^2)}$$ with $A= \left[0,+\infty\right[ \times [0,1].$ So far I've tried to solve it integrating w.r.t. $y$ first. $$\iint_0^1\frac{dy\,dx}{(1+x^2)(1+x^2 y^2)} = \int_0^\infty\frac{1}{1+x^2}\int_0^1\frac{dy}{1+x^2 y^2} \, dx. $$ I've solved the internal integral by substitution, remembering that $\int\frac{du}{1+u^2}=\arctan u$ Substitution: $$x^2 y^2= u^2 \to y=\frac{1}{x}u \to dy=\frac{1}{x}du.$$ $$y=0 \to u=0, \qquad y=1 →u=x$$ So: \begin{align} & \int_0^∞\frac{1}{1+x^2} \left( \int_0^x\frac{1}{x}\frac{1}{1+u^2}\,du \right) \, dx \\[8pt] = {} & \int_0^\infty\frac{1}{x}\frac{1}{1+x^2}[\arctan u] \, dx \\[8pt] = {} & \int_0^\infty\frac{\arctan x}{x(1+x^2)} \, dx. \end{align} Now I have to solve this last integral with the Fubini's theorem but I don't know how to do it.","I have to solve the following double integral with So far I've tried to solve it integrating w.r.t. first. I've solved the internal integral by substitution, remembering that Substitution: So: Now I have to solve this last integral with the Fubini's theorem but I don't know how to do it.","\iint_{A}\frac{dx\,dy}{(1+x^2)(1+x^2 y^2)} A= \left[0,+\infty\right[ \times [0,1]. y \iint_0^1\frac{dy\,dx}{(1+x^2)(1+x^2 y^2)} = \int_0^\infty\frac{1}{1+x^2}\int_0^1\frac{dy}{1+x^2 y^2} \, dx.  \int\frac{du}{1+u^2}=\arctan u x^2 y^2= u^2 \to y=\frac{1}{x}u \to dy=\frac{1}{x}du. y=0 \to u=0, \qquad y=1 →u=x \begin{align}
& \int_0^∞\frac{1}{1+x^2} \left( \int_0^x\frac{1}{x}\frac{1}{1+u^2}\,du \right) \, dx \\[8pt]
= {} & \int_0^\infty\frac{1}{x}\frac{1}{1+x^2}[\arctan u] \, dx \\[8pt]
= {} & \int_0^\infty\frac{\arctan x}{x(1+x^2)} \, dx.
\end{align}","['integration', 'multivariable-calculus', 'multiple-integral', 'fubini-tonelli-theorems']"
57,Calculate $\iint\frac{dxdy}{(1+x^2+y^2)^2}$ over a triangle,Calculate  over a triangle,\iint\frac{dxdy}{(1+x^2+y^2)^2},"Calculate $$\iint\frac{dxdy}{(1+x^2+y^2)^2}$$ over the triangle $(0,0)$ , $(2,0)$ , $(1,\sqrt{3})$ . So I tried changing to polar coordinates and I know that the angle is between $0$ and $\frac{\pi}{3}$ but I couldn't figure how to set the radius because it depends on the angle.","Calculate over the triangle , , . So I tried changing to polar coordinates and I know that the angle is between and but I couldn't figure how to set the radius because it depends on the angle.","\iint\frac{dxdy}{(1+x^2+y^2)^2} (0,0) (2,0) (1,\sqrt{3}) 0 \frac{\pi}{3}","['integration', 'multivariable-calculus', 'polar-coordinates', 'multiple-integral', 'fubini-tonelli-theorems']"
58,Find the volume between the surface $x^2+y^2+z=1$ and $ z=x^2+(y-1)^2$,Find the volume between the surface  and,x^2+y^2+z=1  z=x^2+(y-1)^2,I'm trying to find the volume between the surface $x^2+y^2+z=1$ and $ z=x^2+(y-1)^2$ but nothing works for me. I made the plot and it looks like this: How could you start? Any recommendation?,I'm trying to find the volume between the surface and but nothing works for me. I made the plot and it looks like this: How could you start? Any recommendation?,x^2+y^2+z=1  z=x^2+(y-1)^2,"['integration', 'multivariable-calculus', 'definite-integrals', 'volume']"
59,"Prove $\lim_{(x,y)\to (0,0)}\frac{xy\sin(y)}{x^2+y^2}$ using delta-epsilon argument",Prove  using delta-epsilon argument,"\lim_{(x,y)\to (0,0)}\frac{xy\sin(y)}{x^2+y^2}","I am trying to prove this limit using delta epsilon: $$\lim_{(x,y)\to (0,0)}\frac{xy\sin(y)}{x^2+y^2}$$ I know how the individual components relate to delta etc but I can't put it together. Please help. (Alternatively, is it valid to use polar coordinates?)","I am trying to prove this limit using delta epsilon: I know how the individual components relate to delta etc but I can't put it together. Please help. (Alternatively, is it valid to use polar coordinates?)","\lim_{(x,y)\to (0,0)}\frac{xy\sin(y)}{x^2+y^2}","['limits', 'multivariable-calculus', 'epsilon-delta']"
60,"Is $f(x,y)=\sqrt{|xy|}$ differentiable? [duplicate]",Is  differentiable? [duplicate],"f(x,y)=\sqrt{|xy|}","This question already has an answer here : Check differentiability of multivariable functions (1 answer) Closed 4 years ago . I know that $f(x,y)=\sqrt{|xy|}$ when $(x,y)\neq{(0,0)}$ is differentiable since partial derivatives exist and they're continous. When I do by definition $f_x (0,0)$ and $f_y (0,0)$ , both are equal to $0$ . Due to ""Sufficient Condition for the Differentiability of Functions"", can I conclude that it is differentiable?  My problem is that I think it isn't diff. because of an analogy to single-variable $f(x)=|x|$ , but not sure. Thanks.","This question already has an answer here : Check differentiability of multivariable functions (1 answer) Closed 4 years ago . I know that when is differentiable since partial derivatives exist and they're continous. When I do by definition and , both are equal to . Due to ""Sufficient Condition for the Differentiability of Functions"", can I conclude that it is differentiable?  My problem is that I think it isn't diff. because of an analogy to single-variable , but not sure. Thanks.","f(x,y)=\sqrt{|xy|} (x,y)\neq{(0,0)} f_x (0,0) f_y (0,0) 0 f(x)=|x|","['real-analysis', 'multivariable-calculus']"
61,How to find the slope of $y.\ln x = x.\ln y$ at $x = e$?,How to find the slope of  at ?,y.\ln x = x.\ln y x = e,"Let's say we have the following equation:- $$y.\ln x=x.\ln y$$ After graphing the equation on desmos (which included, not surprisingly, the line $y=x$ ), I realised that the equation has a slope of 1 at all points of the form $(a,a)$ , except it had two slopes at $(e,e)$ . I went on to try and algebraically find its second slope. $$y.\ln x=x.\ln y$$ $$\ln x.y' + y/x = (x/y).y'+\ln y$$ $$y' = (\ln y-y/x)/(\ln x-x/y)$$ Thus I got an expression for $y'$ which becomes indeterminate at $x=e$ . I wasn't sure how to apply a limit with two variables, and wasn't sure how I would get two slopes. How do I approach finding both the slopes in this self-intersecting graph?","Let's say we have the following equation:- After graphing the equation on desmos (which included, not surprisingly, the line ), I realised that the equation has a slope of 1 at all points of the form , except it had two slopes at . I went on to try and algebraically find its second slope. Thus I got an expression for which becomes indeterminate at . I wasn't sure how to apply a limit with two variables, and wasn't sure how I would get two slopes. How do I approach finding both the slopes in this self-intersecting graph?","y.\ln x=x.\ln y y=x (a,a) (e,e) y.\ln x=x.\ln y \ln x.y' + y/x = (x/y).y'+\ln y y' = (\ln y-y/x)/(\ln x-x/y) y' x=e","['calculus', 'multivariable-calculus', 'exponential-function', 'graphing-functions', 'slope']"
62,"For non-negative reals such that $a+b+c\geq x+y+z$, $ab+bc+ca\geq xy+yz+zx$, and $abc\geq xyz$, show $a^k+b^k+c^k\geq x^k+y^k+z^k$ for $0<k<1$","For non-negative reals such that , , and , show  for",a+b+c\geq x+y+z ab+bc+ca\geq xy+yz+zx abc\geq xyz a^k+b^k+c^k\geq x^k+y^k+z^k 0<k<1,"Let $a$ , $b$ , $c$ , $x$ , $y$ , $z$ be non-negative real numbers such that $$a+b+c \geq x+y+z,$$ $$ab+bc+ca \geq xy+yz+zx,$$ $$ abc \geq xyz$$ Show that $$a^k+b^k+c^k \geq x^k+y^k+z^k, \quad 0 < k < 1$$ This is case $n=3$ of a problem posted by Ji Chen in the Art of Problem Solving forums, 2008. I have posted a partial result in an answer below. I have a proof when $r = \frac12,$ for weaker conditons $$a+b+c = x+y+z,$$ $$\min(x, y, z) \leqslant \min(a, b, c),$$ $$\max(a, b, c) \leqslant \max(x, y, z).$$ Indeed, if $u, v > 0$ it's easy check $$\sqrt{u} - \sqrt{v} \leqslant \frac{u-v}{2\sqrt{v}}.$$ Assume $x \geqslant y \geqslant z$ and $a \geqslant b \geqslant c$ then $x \geqslant a, \; c \geqslant z.$ Therefore $$\begin{aligned}\sqrt{x}+\sqrt{y}+\sqrt{z} - \sqrt{a} - \sqrt{b} - \sqrt{c} & \leqslant \frac{x-a}{2\sqrt{a}}+ \frac{y-b}{2\sqrt{b}}+ \frac{z-c}{2\sqrt{c}} \\& \leqslant \frac{x-a}{2\sqrt{b}}+ \frac{y-b}{2\sqrt{b}}+ \frac{z-c}{2\sqrt{c}} \\& =\frac{x+y+z-a-b-c}{2\sqrt{b}}-\frac{(c-z)(\sqrt{b}-\sqrt{c})}{2\sqrt{bc}} \leqslant 0.\end{aligned}$$","Let , , , , , be non-negative real numbers such that Show that This is case of a problem posted by Ji Chen in the Art of Problem Solving forums, 2008. I have posted a partial result in an answer below. I have a proof when for weaker conditons Indeed, if it's easy check Assume and then Therefore","a b c x y z a+b+c \geq x+y+z, ab+bc+ca \geq xy+yz+zx,  abc \geq xyz a^k+b^k+c^k \geq x^k+y^k+z^k, \quad 0 < k < 1 n=3 r = \frac12, a+b+c = x+y+z, \min(x, y, z) \leqslant \min(a, b, c), \max(a, b, c) \leqslant \max(x, y, z). u, v > 0 \sqrt{u} - \sqrt{v} \leqslant \frac{u-v}{2\sqrt{v}}. x \geqslant y \geqslant z a \geqslant b \geqslant c x \geqslant a, \; c \geqslant z. \begin{aligned}\sqrt{x}+\sqrt{y}+\sqrt{z} - \sqrt{a} - \sqrt{b} - \sqrt{c} & \leqslant \frac{x-a}{2\sqrt{a}}+ \frac{y-b}{2\sqrt{b}}+ \frac{z-c}{2\sqrt{c}} \\& \leqslant \frac{x-a}{2\sqrt{b}}+ \frac{y-b}{2\sqrt{b}}+ \frac{z-c}{2\sqrt{c}} \\& =\frac{x+y+z-a-b-c}{2\sqrt{b}}-\frac{(c-z)(\sqrt{b}-\sqrt{c})}{2\sqrt{bc}} \leqslant 0.\end{aligned}","['multivariable-calculus', 'inequality', 'systems-of-equations']"
63,Find the maximum value of a sum of cosines given certain condition,Find the maximum value of a sum of cosines given certain condition,,"In my calculus class, I've come across this problem when we were on the topic of Jensen's Inequality: \begin{multline}A=\{\cos(x_1)\cos(x_2)\dots\cos(x_n)\in\Bbb{R}:\\n\in\Bbb{N},x_1^2+...+x_n^2=1\}.\end{multline} We are tasked with finding $\sup A$ . I have tried the obvious approach of writing $$\cos(x_1)\cos(x_2)\dots\cos(x_n)=e^{\ln(\cos(x_1))+\dots+\ln(\cos(x_n)}$$ and then trying to find an achievable upper bound for the value of $$\ln(\cos(x_1))+\dots+\ln(\cos(x_n)$$ by using Jensen's inequality for the concave function $\ln(\cos(x))$ in hope that those pesky squares could be dealt with using $$\sqrt{\dfrac{a_1^2+...+a_n^2}{n}}\geqslant\dfrac{a_1+...+a_n}{n}$$ but so far that didn't help. I am quite convinced that the answer will be $\dfrac{1}{\sqrt{e}}$ since that is what $e^{\ln(\cos(x_1))+\dots+\ln(\cos(x_n)}$ approaches from below when $x_1=x_2=...=x_n$ and $n$ goes to $\infty$ , but can't prove it. Using different methods I've managed to show that the answer is smaller than $e^{\cos(1)-1}$ which is just barely larger than $\dfrac{1}{\sqrt{e}}$ , but in proving so I have used inequalities with different equality conditions. Any help would be appreciated as I feel I am missing something I should definitely find out by now. By the way, no integrals allowed :) Yay early calc","In my calculus class, I've come across this problem when we were on the topic of Jensen's Inequality: We are tasked with finding . I have tried the obvious approach of writing and then trying to find an achievable upper bound for the value of by using Jensen's inequality for the concave function in hope that those pesky squares could be dealt with using but so far that didn't help. I am quite convinced that the answer will be since that is what approaches from below when and goes to , but can't prove it. Using different methods I've managed to show that the answer is smaller than which is just barely larger than , but in proving so I have used inequalities with different equality conditions. Any help would be appreciated as I feel I am missing something I should definitely find out by now. By the way, no integrals allowed :) Yay early calc","\begin{multline}A=\{\cos(x_1)\cos(x_2)\dots\cos(x_n)\in\Bbb{R}:\\n\in\Bbb{N},x_1^2+...+x_n^2=1\}.\end{multline} \sup A \cos(x_1)\cos(x_2)\dots\cos(x_n)=e^{\ln(\cos(x_1))+\dots+\ln(\cos(x_n)} \ln(\cos(x_1))+\dots+\ln(\cos(x_n) \ln(\cos(x)) \sqrt{\dfrac{a_1^2+...+a_n^2}{n}}\geqslant\dfrac{a_1+...+a_n}{n} \dfrac{1}{\sqrt{e}} e^{\ln(\cos(x_1))+\dots+\ln(\cos(x_n)} x_1=x_2=...=x_n n \infty e^{\cos(1)-1} \dfrac{1}{\sqrt{e}}","['multivariable-calculus', 'optimization', 'maxima-minima', 'products', 'jensen-inequality']"
64,showing $(\arctan(z))' = \frac{1}{1+z^2}$ is true for $z\in C$,showing  is true for,(\arctan(z))' = \frac{1}{1+z^2} z\in C,"I wish to show that $(\arctan(z))' = \frac{1}{1+z^2}$ is true for $z\in C$ . I've found, after some algebra, $$ \arctan(z) = \frac{i(e^{iz} + e^{-iz})}{e^{iz} - e^{-iz}} \Rightarrow \arctan(z)' = \frac{4}{e^{2iz}-2+e^{-2iz}}$$ then using $z=x+iy$ , $$\frac{4}{e^{2ix-2y} +e^{-2ix+2y}-2} = \frac{4}{(e^{-2y}+e^{2y})cos(2x) +(e^{-2y} - e^{2y})i\sin{2x} -2})$$ then using $\sin z = \frac{e^{iz}-e^{-iz}}{2i}$ and $\cos z = \frac{e^{iz} + e^{-iz}}{2}$ , $$\frac{4}{e^{-2y}e^{iz}+e^{2y}e^{-iz}-2} = \frac{4}{e^{-3y}e^{ix} + e^{3y}e^{-ix}-2}   $$ I cannot get past this point, nor do I know if I'm even on the right path. This problem is a metric ton of algebra, but if you have a hint or solution or spot a misstep, please let me know. Thank you.","I wish to show that is true for . I've found, after some algebra, then using , then using and , I cannot get past this point, nor do I know if I'm even on the right path. This problem is a metric ton of algebra, but if you have a hint or solution or spot a misstep, please let me know. Thank you.",(\arctan(z))' = \frac{1}{1+z^2} z\in C  \arctan(z) = \frac{i(e^{iz} + e^{-iz})}{e^{iz} - e^{-iz}} \Rightarrow \arctan(z)' = \frac{4}{e^{2iz}-2+e^{-2iz}} z=x+iy \frac{4}{e^{2ix-2y} +e^{-2ix+2y}-2} = \frac{4}{(e^{-2y}+e^{2y})cos(2x) +(e^{-2y} - e^{2y})i\sin{2x} -2}) \sin z = \frac{e^{iz}-e^{-iz}}{2i} \cos z = \frac{e^{iz} + e^{-iz}}{2} \frac{4}{e^{-2y}e^{iz}+e^{2y}e^{-iz}-2} = \frac{4}{e^{-3y}e^{ix} + e^{3y}e^{-ix}-2}   ,"['complex-analysis', 'multivariable-calculus', 'derivatives']"
65,"How to show that limiting value of the function $f(x,y)=\frac{x^2-xy}{\sqrt{x}-\sqrt{y}}$ is zero by $\epsilon-\delta $ definition",How to show that limiting value of the function  is zero by  definition,"f(x,y)=\frac{x^2-xy}{\sqrt{x}-\sqrt{y}} \epsilon-\delta ","I know that the limit of function when $(x,y)$ approaches $(0,0)$ is zero. But how can I show this limiting value is correct using $\epsilon-\delta$ definition of the limit, i.e, $$0<\sqrt{(x-x_{0})^2+(y-y_{0})^2}<\delta \implies |f(x,y)-L|<\epsilon$$ Now in my case, if I take $$\left|\frac{x^2-xy}{\sqrt{x}-\sqrt{y}}-0\right|<\epsilon$$ I dont know how to proceed further to get the $\delta.$","I know that the limit of function when approaches is zero. But how can I show this limiting value is correct using definition of the limit, i.e, Now in my case, if I take I dont know how to proceed further to get the","(x,y) (0,0) \epsilon-\delta 0<\sqrt{(x-x_{0})^2+(y-y_{0})^2}<\delta \implies |f(x,y)-L|<\epsilon \left|\frac{x^2-xy}{\sqrt{x}-\sqrt{y}}-0\right|<\epsilon \delta.","['calculus', 'multivariable-calculus', 'inequality']"
66,Gradient: field of tangent vectors vs. normal to surface at a point,Gradient: field of tangent vectors vs. normal to surface at a point,,"One definition of the gradient say that its a field of tangent vectors to a surface.  The gradient takes a scalar field f(x,y) (aka. a function), and produces a vector field $\vec{v}(x,y)$ , where the vector at each point of the field points in the the direction of greatest increase. $$\vec{v}(x,y) = \overbrace{\nabla \underbrace{f(x,y)}_\text{scalar field}}^{\text{vector field}} = \overbrace{\begin{bmatrix}\frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \end{bmatrix}}^{\text{vector field}}$$ Another definition of gradient say that its a normal to a surface of the form $F(x,y,z)=c$ . How to know when to apply which definition of gradient?  How is a field of tangent vectors related to the normal of a surface?  They seem like contradictory definitions. Let $\vec{r} = x \hat{\text{i}} + y \hat{\text{j}} + z \hat{\text{k}}$ be the position vector to any point P(x,y,z) on the surface $\phi(x,y,z)=c$ .  Then: $d\vec{r} = dx~\hat{\text{i}} + dy~\hat{\text{j}} + dz~\hat{\text{k}}$ lies in the tangent plane to the surface at P. $\phi(x,y,z)=c$ Taking differential of both sides: $$d\phi = \frac{\partial \phi}{\partial x} dx + \frac{\partial \phi}{\partial y} dy + \frac{\partial \phi}{\partial z} dz = 0$$ Therefore: $$\bigg(\frac{\partial \phi}{\partial x} \hat{\text{i}} + \frac{\phi}{\partial x}\hat{\text{j}} +  \frac{\phi}{\partial x}\hat{\text{k}}\bigg)  \cdot \bigg(dx\hat{\text{i}} +dy\hat{\text{j}} + dz\hat{\text{k} }\bigg) =0$$ $$\nabla \phi \cdot d\vec{r} = 0$$ Therefore $\nabla \phi$ is perpendicular to $d\vec{r}$ or normal to the surface at point P.","One definition of the gradient say that its a field of tangent vectors to a surface.  The gradient takes a scalar field f(x,y) (aka. a function), and produces a vector field , where the vector at each point of the field points in the the direction of greatest increase. Another definition of gradient say that its a normal to a surface of the form . How to know when to apply which definition of gradient?  How is a field of tangent vectors related to the normal of a surface?  They seem like contradictory definitions. Let be the position vector to any point P(x,y,z) on the surface .  Then: lies in the tangent plane to the surface at P. Taking differential of both sides: Therefore: Therefore is perpendicular to or normal to the surface at point P.","\vec{v}(x,y) \vec{v}(x,y) = \overbrace{\nabla \underbrace{f(x,y)}_\text{scalar field}}^{\text{vector field}} = \overbrace{\begin{bmatrix}\frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \end{bmatrix}}^{\text{vector field}} F(x,y,z)=c \vec{r} = x \hat{\text{i}} + y \hat{\text{j}} + z \hat{\text{k}} \phi(x,y,z)=c d\vec{r} = dx~\hat{\text{i}} + dy~\hat{\text{j}} + dz~\hat{\text{k}} \phi(x,y,z)=c d\phi = \frac{\partial \phi}{\partial x} dx + \frac{\partial \phi}{\partial y} dy + \frac{\partial \phi}{\partial z} dz = 0 \bigg(\frac{\partial \phi}{\partial x} \hat{\text{i}} + \frac{\phi}{\partial x}\hat{\text{j}} +  \frac{\phi}{\partial x}\hat{\text{k}}\bigg)  \cdot \bigg(dx\hat{\text{i}} +dy\hat{\text{j}} + dz\hat{\text{k} }\bigg) =0 \nabla \phi \cdot d\vec{r} = 0 \nabla \phi d\vec{r}","['calculus', 'multivariable-calculus']"
67,Minimize $x+2y$ subject to $x^2+y^2\le1$ and $3x+4y\le-5$.,Minimize  subject to  and .,x+2y x^2+y^2\le1 3x+4y\le-5,"I want to minimize $x+2y$ subject to $x^2+y^2\le1$ and $3x+4y\le-5$ . I found the gradient conditions: $1+2x\lambda_1+3\lambda_2=2+2y\lambda_1+4\lambda_2=0$ . Then, by complementary slackness: $\lambda_1(x^2+y^2-1)=\lambda_2(3x+4y+5)=0$ . Wolfram-Alpha gives $(x,y,\lambda_1,\lambda_2) = (\frac1{\sqrt5},\frac2{\sqrt5},-\frac{\sqrt5}2,0)$ or $(-\frac1{\sqrt5},-\frac2{\sqrt5},\frac{\sqrt5}2,0)$ . The second one gives an objective value of $-\sqrt5$ , but both of these violate the second constraint. The actual mimimum is $-\frac{11}5$ at $(x,y) = (-\frac35,-\frac45)$ . Where did I go wrong?","I want to minimize subject to and . I found the gradient conditions: . Then, by complementary slackness: . Wolfram-Alpha gives or . The second one gives an objective value of , but both of these violate the second constraint. The actual mimimum is at . Where did I go wrong?","x+2y x^2+y^2\le1 3x+4y\le-5 1+2x\lambda_1+3\lambda_2=2+2y\lambda_1+4\lambda_2=0 \lambda_1(x^2+y^2-1)=\lambda_2(3x+4y+5)=0 (x,y,\lambda_1,\lambda_2) = (\frac1{\sqrt5},\frac2{\sqrt5},-\frac{\sqrt5}2,0) (-\frac1{\sqrt5},-\frac2{\sqrt5},\frac{\sqrt5}2,0) -\sqrt5 -\frac{11}5 (x,y) = (-\frac35,-\frac45)","['multivariable-calculus', 'optimization']"
68,Show that $e^{|xy|}$ is not differentiable,Show that  is not differentiable,e^{|xy|},"I'm a little surprised and am not sure how to show that $e^{|xy|}$ is not differentiable. I am especially surprised because I know that $|xy|$ is differentiable at $(0, 0)$ . Any help would be greatly appreciated. Edit: I forgot to add that I am in particular interested in differentiability at $(0, 0)$ .",I'm a little surprised and am not sure how to show that is not differentiable. I am especially surprised because I know that is differentiable at . Any help would be greatly appreciated. Edit: I forgot to add that I am in particular interested in differentiability at .,"e^{|xy|} |xy| (0, 0) (0, 0)","['calculus', 'multivariable-calculus']"
69,Evaluating integral delta function,Evaluating integral delta function,,"I am trying to evaluate the integral below (delta function) and not sure if I evaluated correctly? The integral is the following: $$\int^{100}_{-100}x^3\sin(2x)\delta(x^2-9)dx$$ I have the following $x=\pm 3$ , $$\therefore \int^{100}_{-100}x^3\sin(2x)\delta(x^2-9)dx = \int^{0}_{-100}x^3\sin(2x)\delta(x^2-9)dx + \int^{100}_{0}x^3\sin(2x)\delta(x^2-9)dx= \ (-3)^3\sin(2\cdot(-3)) + (3)^3\sin(2\cdot(3)) = \ -27\sin(-6) + 27\sin(6)$$","I am trying to evaluate the integral below (delta function) and not sure if I evaluated correctly? The integral is the following: I have the following ,",\int^{100}_{-100}x^3\sin(2x)\delta(x^2-9)dx x=\pm 3 \therefore \int^{100}_{-100}x^3\sin(2x)\delta(x^2-9)dx = \int^{0}_{-100}x^3\sin(2x)\delta(x^2-9)dx + \int^{100}_{0}x^3\sin(2x)\delta(x^2-9)dx= \ (-3)^3\sin(2\cdot(-3)) + (3)^3\sin(2\cdot(3)) = \ -27\sin(-6) + 27\sin(6),"['calculus', 'multivariable-calculus', 'vector-analysis']"
70,"Is $f(x,y)=\frac{1}{x^2+y^2+1}$ uniformly continuous?",Is  uniformly continuous?,"f(x,y)=\frac{1}{x^2+y^2+1}","Is \begin{align*} f(x,y)=\frac{1}{x^2+y^2+1} \end{align*} uniformly continuous? I was able to show that $f$ has a global maximum at $f(0,0)=1$ , but I can't seem to work out a proper estimate for uniform continuity. Any insights would be greatly appreciated.","Is uniformly continuous? I was able to show that has a global maximum at , but I can't seem to work out a proper estimate for uniform continuity. Any insights would be greatly appreciated.","\begin{align*}
f(x,y)=\frac{1}{x^2+y^2+1}
\end{align*} f f(0,0)=1","['real-analysis', 'multivariable-calculus', 'uniform-continuity']"
71,Trouble with the Proof of a Multi-variable Integration Theorem,Trouble with the Proof of a Multi-variable Integration Theorem,,"One of my classes has online notes containing theorems.  One of the theorems is ""Assume that $R=[a,b]\times[c,d]$ is a rectangle in the $xy$ plane.  Let $f$ be a function on $R$ , and assume that $f$ is integrable on $R$ , and for every $y\in[c,d]$ , the function $f_y:[a,b]\to\mathbb{R}$ defined by $f_y:=f(x,y)$ is integrable on $[a,b]$ .  Then the function $g:[c,d]\to\mathbb{R}$ defined by $g(y):=\int^b_af(x,y)dx$ is integrable on $[c,d]$ , and $$\int\int_RfdA=\int^d_cg(y)dy=\int^d_c\left(\int^b_af(x,y)dx\right)dy.""$$ They say the proof is left as an exercise, but I don't know how to do it. A hint was that I could first show that for every $k\in\{1,\dots,K\}$ and for every $y\in[y_{k-1},y_k]$ , we have that $$g(y)\geq\sum^J_{j=1}m_{jk}(f)(x_j-x_{j-1}).$$ Thanks in advance!","One of my classes has online notes containing theorems.  One of the theorems is ""Assume that is a rectangle in the plane.  Let be a function on , and assume that is integrable on , and for every , the function defined by is integrable on .  Then the function defined by is integrable on , and They say the proof is left as an exercise, but I don't know how to do it. A hint was that I could first show that for every and for every , we have that Thanks in advance!","R=[a,b]\times[c,d] xy f R f R y\in[c,d] f_y:[a,b]\to\mathbb{R} f_y:=f(x,y) [a,b] g:[c,d]\to\mathbb{R} g(y):=\int^b_af(x,y)dx [c,d] \int\int_RfdA=\int^d_cg(y)dy=\int^d_c\left(\int^b_af(x,y)dx\right)dy."" k\in\{1,\dots,K\} y\in[y_{k-1},y_k] g(y)\geq\sum^J_{j=1}m_{jk}(f)(x_j-x_{j-1}).","['real-analysis', 'integration', 'multivariable-calculus', 'integer-partitions']"
72,derivative with respect to a diagonal matrix,derivative with respect to a diagonal matrix,,"Had check some previous questions regarding the derivatives of diagonal matrices, but haven't found a form like this. If $K=Wdiag(s)W^T$ , in which $W$ is an m-by-n matrix, and $diag(s)$ represents an n-by-n diagonal matrix of which diagonal is represented by the vector $s$ . I'm interested in the derivative of the log-determinant of K, ( $\frac{\partial{ln}|K|}{\partial{s}}$ ), but I get stuck at solving this part: $\frac{\partial{K}}{\partial{s}}$","Had check some previous questions regarding the derivatives of diagonal matrices, but haven't found a form like this. If , in which is an m-by-n matrix, and represents an n-by-n diagonal matrix of which diagonal is represented by the vector . I'm interested in the derivative of the log-determinant of K, ( ), but I get stuck at solving this part:",K=Wdiag(s)W^T W diag(s) s \frac{\partial{ln}|K|}{\partial{s}} \frac{\partial{K}}{\partial{s}},"['linear-algebra', 'matrices', 'multivariable-calculus', 'derivatives', 'matrix-calculus']"
73,"Finding extrema, non-critical points, and saddle points given a contour plot","Finding extrema, non-critical points, and saddle points given a contour plot",,"I'm trying to interpret the contour plot as like a mountain, where all the segregated areas are plains that are higher when it proceeds inwards. What is the significance of the numbers on x-axis and y-axis, and wouldn't the points that are most centered be maximums? $\hskip 12 in$","I'm trying to interpret the contour plot as like a mountain, where all the segregated areas are plains that are higher when it proceeds inwards. What is the significance of the numbers on x-axis and y-axis, and wouldn't the points that are most centered be maximums?",\hskip 12 in,"['multivariable-calculus', 'graphing-functions']"
74,I need to find the potential function of a vector field.,I need to find the potential function of a vector field.,,"I was given F = (y+z) i + (x+z) j + (x+y) k . I found said field to be conservative, and I integrated the x partial derivative and got f(x,y,z) = xy + xz + g(y,z). The thing is that I am trying to find g(y,z), and I ended up with something that was expressed in terms of x, y and z (I got x+z-xy-xz). I don't know what to do with this information not that I arrived at something expressed in all three variables.","I was given F = (y+z) i + (x+z) j + (x+y) k . I found said field to be conservative, and I integrated the x partial derivative and got f(x,y,z) = xy + xz + g(y,z). The thing is that I am trying to find g(y,z), and I ended up with something that was expressed in terms of x, y and z (I got x+z-xy-xz). I don't know what to do with this information not that I arrived at something expressed in all three variables.",,"['integration', 'multivariable-calculus', 'vector-fields']"
75,Saddle point or not?,Saddle point or not?,,"Consider the function $f(x,y)=2xy-x^3-y^2$ . One of the stationary points is $(0,0)$ . At this point, $f_{xx}f_{yy}-f_{xy}f_{yx}<0$ . According to me, this indicates that (0,0) is a saddle point. However, the text I am referring to calls this ""neither an extremum nor a saddle point"". Am I missing something? Edit The plot (from GeoGebra) looks like this:","Consider the function . One of the stationary points is . At this point, . According to me, this indicates that (0,0) is a saddle point. However, the text I am referring to calls this ""neither an extremum nor a saddle point"". Am I missing something? Edit The plot (from GeoGebra) looks like this:","f(x,y)=2xy-x^3-y^2 (0,0) f_{xx}f_{yy}-f_{xy}f_{yx}<0",['multivariable-calculus']
76,"Prove that $\lim_{(x,y)\to (0,0)}\frac{x^{2}+xy+y^{2}}{\sqrt{x^{2}+y^{2}}}=0$.",Prove that .,"\lim_{(x,y)\to (0,0)}\frac{x^{2}+xy+y^{2}}{\sqrt{x^{2}+y^{2}}}=0","Prove that $\displaystyle \lim_{(x,y)\to (0,0)}\frac{x^{2}+xy+y^{2}}{\sqrt{x^{2}+y^{2}}}=0$ . This has been my rough work so far, but I am not sure how to go further or if I am doing it the wrong way... $\displaystyle|f(x,y)-L|=\left |\frac{x^{2}+xy+y^{2}}{\sqrt{x^{2}+y^{2}}}  \right | \leq \frac{|x^{2}+xy+y^{2}|}{\big|\sqrt{x^{2}+y^{2}}\big|}\cdot \frac{\sqrt{x^{2}+y^{2}}}{\sqrt{x^{2}+y^{2}}} = \frac{\big(\sqrt{x^{2}+y^{2}}\big)|x^{2}+xy+y^{2}|}{|x^{2}+y^{2}|}$ I wanted the square root on top to help determine what I should set my $\delta$ as. Any ideas on how to finish this or do it in a better way?","Prove that . This has been my rough work so far, but I am not sure how to go further or if I am doing it the wrong way... I wanted the square root on top to help determine what I should set my as. Any ideas on how to finish this or do it in a better way?","\displaystyle \lim_{(x,y)\to (0,0)}\frac{x^{2}+xy+y^{2}}{\sqrt{x^{2}+y^{2}}}=0 \displaystyle|f(x,y)-L|=\left |\frac{x^{2}+xy+y^{2}}{\sqrt{x^{2}+y^{2}}}  \right | \leq \frac{|x^{2}+xy+y^{2}|}{\big|\sqrt{x^{2}+y^{2}}\big|}\cdot \frac{\sqrt{x^{2}+y^{2}}}{\sqrt{x^{2}+y^{2}}} = \frac{\big(\sqrt{x^{2}+y^{2}}\big)|x^{2}+xy+y^{2}|}{|x^{2}+y^{2}|} \delta",['limits']
77,Is $\mathbb{R}^n$ a vector space or a metric space?,Is  a vector space or a metric space?,\mathbb{R}^n,"In my various courses, for instance, linear algebra and vector calculus, I am somewhat confused with what precisely $\mathbb{R}^n$ is. From the definition of the Cartesian product, I would conceptualise $\mathbb{R}^n$ as the metric space with some distance operator, where all the points are just $n$ -tuples. This is surely a distinct notion from vectors as isn't the point $A = (1,2,3)$ , for instance, different from the vector $\vec{a} =\begin{pmatrix} 1\\  2\\  3 \end{pmatrix}$ ? But if we were to consider the points in $\mathbb{R}^n$ as vectors then clearly it is a vector space. However I don't know whether these two conceptions of $\mathbb{R}^n$ are actually equivalent. Surely the vectors do not correspond to a specific point in space, unlike the points in $\mathbb{R}^n$ . Forgive me if this is a silly question, or if my question seems garbled. Also please help me with tags if they are inappropriate.","In my various courses, for instance, linear algebra and vector calculus, I am somewhat confused with what precisely is. From the definition of the Cartesian product, I would conceptualise as the metric space with some distance operator, where all the points are just -tuples. This is surely a distinct notion from vectors as isn't the point , for instance, different from the vector ? But if we were to consider the points in as vectors then clearly it is a vector space. However I don't know whether these two conceptions of are actually equivalent. Surely the vectors do not correspond to a specific point in space, unlike the points in . Forgive me if this is a silly question, or if my question seems garbled. Also please help me with tags if they are inappropriate.","\mathbb{R}^n \mathbb{R}^n n A = (1,2,3) \vec{a} =\begin{pmatrix}
1\\ 
2\\ 
3
\end{pmatrix} \mathbb{R}^n \mathbb{R}^n \mathbb{R}^n","['linear-algebra', 'multivariable-calculus', 'vector-spaces', 'metric-spaces']"
78,Intuition on Double Integrals,Intuition on Double Integrals,,"Frequently, I am met with problem that ask to evaluate a double integral over a bounded region. For example, evaluate the double integral $$\int\int_R 2x\cos(y)+3 \space dA$$ over the region $R$ bounded by $y=2x^2$ , $y=0$ , and $x=1$ . Graphically, Setting up the integral, I get $$\int_0^1\int_0^{2x^2} 2x\cos(y) \space dy \space dx$$ My question is how exactly does this double integral evaluate the volume above the region R, since the limits of integration seem to have no dependence on the z-axis; the limits of integration are in terms of $x$ and $y$ . Evaluating the inner integral, wouldn't you get the area between $y=0$ and $y=2x^2$ of $2x\cos(y)$ . If so, how does integrating that area between $x=0$ and $x=1$ give you the volume underneath the surface? The double integral seems random. The question may be broad, but any intuition on this would be helpful. Thanks.","Frequently, I am met with problem that ask to evaluate a double integral over a bounded region. For example, evaluate the double integral over the region bounded by , , and . Graphically, Setting up the integral, I get My question is how exactly does this double integral evaluate the volume above the region R, since the limits of integration seem to have no dependence on the z-axis; the limits of integration are in terms of and . Evaluating the inner integral, wouldn't you get the area between and of . If so, how does integrating that area between and give you the volume underneath the surface? The double integral seems random. The question may be broad, but any intuition on this would be helpful. Thanks.",\int\int_R 2x\cos(y)+3 \space dA R y=2x^2 y=0 x=1 \int_0^1\int_0^{2x^2} 2x\cos(y) \space dy \space dx x y y=0 y=2x^2 2x\cos(y) x=0 x=1,"['integration', 'multivariable-calculus', 'intuition', 'volume', 'multiple-integral']"
79,"Discontinuity of $f(x,y)=\frac{2(x^3+y^3)}{x^2+2y}$ at $(x,y) = (0,0)$",Discontinuity of  at,"f(x,y)=\frac{2(x^3+y^3)}{x^2+2y} (x,y) = (0,0)","Let $f(x,y)=\frac{2(x^3+y^3)}{x^2+2y}$ , when $(x,y) \neq (0,0)$ and $f(x,y)=0 $ when $(x,y)=(0,0)$ We are required to prove the discontinuity of this function at $(0,0)$ . So, I put $y=mx$ where $x \rightarrow 0$ , to get the limit to be $0$ . But I can't find another instance where the limiti is different. Can you guys help? Thanks","Let , when and when We are required to prove the discontinuity of this function at . So, I put where , to get the limit to be . But I can't find another instance where the limiti is different. Can you guys help? Thanks","f(x,y)=\frac{2(x^3+y^3)}{x^2+2y} (x,y) \neq (0,0) f(x,y)=0  (x,y)=(0,0) (0,0) y=mx x \rightarrow 0 0","['calculus', 'real-analysis']"
80,Volume after transformation.,Volume after transformation.,,"Consider $$ \Omega:\{ (x, y, z): x^2+y^2\leq 1, 0\leq z\leq 2 \} $$ and the transform $$ T:(x, y, z)\to(x, y+\tan(\alpha z), z) ,$$ where $ \alpha\in (0, \pi) $ . What is the volume of $ T(\Omega) $ ? My attempt: I am trying to convert to the cylindrical coordinate system, but I can't find the region where I should integrate since it is transformed.","Consider and the transform where . What is the volume of ? My attempt: I am trying to convert to the cylindrical coordinate system, but I can't find the region where I should integrate since it is transformed."," \Omega:\{ (x, y, z): x^2+y^2\leq 1, 0\leq z\leq 2 \}   T:(x, y, z)\to(x, y+\tan(\alpha z), z) ,  \alpha\in (0, \pi)   T(\Omega) ","['calculus', 'integration', 'multivariable-calculus']"
81,Derivative of a squared definite integral,Derivative of a squared definite integral,,"Differentiation of $\int_{a(x)}^{b(x)} f(x,t)\,\text{d}t$ is done by Leibniz's integral rule: $$\frac{\text{d}}{\text{d}x} \left (\int_{a(x)}^{b(x)} f(x,t)\,\text{d}t \right )= f\big(x,b(x)\big)\cdot \frac{\text{d}}{\text{d}x} b(x) - f\big(x,a(x)\big)\cdot \frac{\text{d}}{\text{d}x} a(x) + \int_{a(x)}^{b(x)}\frac{\partial}{\partial x} f(x,t) \,\text{d}t,$$ if $-\infty<a(x),b(x)<\infty$ . Can we say anything in general about the derivative of the powers of the $\int_{a(x)}^{b(x)} f(x,t)\,\text{d}t$ in a similar fashion? So for example $$\frac{\text{d}}{\text{d}x} \left(\left(\int_{a(x)}^{b(x)} f(x,t)\,\text{d}t\right)^2\right)=~?$$",Differentiation of is done by Leibniz's integral rule: if . Can we say anything in general about the derivative of the powers of the in a similar fashion? So for example,"\int_{a(x)}^{b(x)} f(x,t)\,\text{d}t \frac{\text{d}}{\text{d}x} \left (\int_{a(x)}^{b(x)} f(x,t)\,\text{d}t \right )= f\big(x,b(x)\big)\cdot \frac{\text{d}}{\text{d}x} b(x) - f\big(x,a(x)\big)\cdot \frac{\text{d}}{\text{d}x} a(x) + \int_{a(x)}^{b(x)}\frac{\partial}{\partial x} f(x,t) \,\text{d}t, -\infty<a(x),b(x)<\infty \int_{a(x)}^{b(x)} f(x,t)\,\text{d}t \frac{\text{d}}{\text{d}x} \left(\left(\int_{a(x)}^{b(x)} f(x,t)\,\text{d}t\right)^2\right)=~?","['integration', 'multivariable-calculus', 'derivatives', 'definite-integrals']"
82,"Prove that $f(x,y)=1- \sqrt{|xy|}$ is not differentiable at $(0,0)$",Prove that  is not differentiable at,"f(x,y)=1- \sqrt{|xy|} (0,0)","Let $f(x,y)=1- \sqrt{|xy|}$. Prove that $f$ is not differentiable at $(0,0)$. I don't really know how to tackle this problem. Normally for this kind of problem I would find the candidate to be differential, this is, $L(x,y) = \frac{\partial f}{\partial x}(0,0)x + \frac{\partial f}{\partial y}(0,0)y$ and I would try to find out if $\displaystyle\lim_{(x,y)\to(0,0)}\frac{f(x,y)-f(0,0)-L(x,y)}{ \| (x,y)\|}=0$. But I don't know how to start being $f$ differentiable.","Let $f(x,y)=1- \sqrt{|xy|}$. Prove that $f$ is not differentiable at $(0,0)$. I don't really know how to tackle this problem. Normally for this kind of problem I would find the candidate to be differential, this is, $L(x,y) = \frac{\partial f}{\partial x}(0,0)x + \frac{\partial f}{\partial y}(0,0)y$ and I would try to find out if $\displaystyle\lim_{(x,y)\to(0,0)}\frac{f(x,y)-f(0,0)-L(x,y)}{ \| (x,y)\|}=0$. But I don't know how to start being $f$ differentiable.",,"['real-analysis', 'multivariable-calculus']"
83,what is the difference between $D(g\circ f)(x)$ and $Dg(f(x))$?,what is the difference between  and ?,D(g\circ f)(x) Dg(f(x)),Let $A$ be an open in $\mathbb R^m$. Let $B$ be open in $\mathbb R^n$. Let $f: A \to  \mathbb R^n$ and $g: B \to  \mathbb R^p$ where $B = f(A)$. If $f$ is differentiable at $a$ and $g$ is differentiable at $f(a) = b$. Then $D(g\circ f)(a) = Dg(f(a) Df(a)$. I got this theorem in Mukresh's Analysis of Manifolds . I can not understand what is the difference  between $D(g\circ f)(x)$ and $Dg(f(x))$ . Can anyone please make me understand ? Thank You in Advance.,Let $A$ be an open in $\mathbb R^m$. Let $B$ be open in $\mathbb R^n$. Let $f: A \to  \mathbb R^n$ and $g: B \to  \mathbb R^p$ where $B = f(A)$. If $f$ is differentiable at $a$ and $g$ is differentiable at $f(a) = b$. Then $D(g\circ f)(a) = Dg(f(a) Df(a)$. I got this theorem in Mukresh's Analysis of Manifolds . I can not understand what is the difference  between $D(g\circ f)(x)$ and $Dg(f(x))$ . Can anyone please make me understand ? Thank You in Advance.,,"['calculus', 'multivariable-calculus', 'partial-derivative']"
84,"Calculate the derivative of $f(x)=\int_x^{x^2}\tan(x+y)\,dy$ on the open interval $(0,\frac{\pi}{4})$.",Calculate the derivative of  on the open interval .,"f(x)=\int_x^{x^2}\tan(x+y)\,dy (0,\frac{\pi}{4})","This appears to be a chain rule question, but I'm struggling with the setup. My attempt is as follows: Let $F:\mathbb{R}^2\rightarrow\mathbb{R}$ , $F(u,v)=\int_u^{u^2}$ tan $(v+y)dy$ , $u:\mathbb{R}\rightarrow\mathbb{R}^2$ , and $u(x)=(u(x),v(x))=(x,x)$ . Then writing $f(x)=(F\circ u)(x)$ allows for use of the chain rule: $D(F\circ u)(x)=DF(u(x),v(x))Du(x)=\left[\frac{\partial F}{\partial u}\mbox{ }\frac{\partial F}{\partial v}\right]\begin{bmatrix}u'(x)\\v'(x)\end{bmatrix}=\frac{\partial F}{\partial u}(u(x))u'(x)+\frac{\partial F}{\partial v}(u(x))v'(x)$ Then by the Fundamental Theorem of Calculus, $\frac{\partial F}{\partial u}=\frac{\partial}{\partial u}\int_u^{u^2}tan(v+y)dy=\mbox{tan}(v+u)=\mbox{tan}(x+x)=\mbox{tan(2x)}$ And similarly, $\frac{\partial F}{\partial v}=\frac{\partial}{\partial v}\int_u^{u^2}\mbox{tan}(v+y)dy=\int_u^{u^2}\frac{\partial}{\partial v}\mbox{tan}(v+y)dy=\int_x^{x^2}\frac{\partial}{\partial x}\mbox{tan}(x+y)dy=\int_x^{x^2}\mbox{sec}^2(x+y)dy$ However at this point, I'm relatively convinced I'm making a major mistake, either with my entire substitution or with the way I'm handling the boundaries. Any advice is much appreciated.","This appears to be a chain rule question, but I'm struggling with the setup. My attempt is as follows: Let , tan , , and . Then writing allows for use of the chain rule: Then by the Fundamental Theorem of Calculus, And similarly, However at this point, I'm relatively convinced I'm making a major mistake, either with my entire substitution or with the way I'm handling the boundaries. Any advice is much appreciated.","F:\mathbb{R}^2\rightarrow\mathbb{R} F(u,v)=\int_u^{u^2} (v+y)dy u:\mathbb{R}\rightarrow\mathbb{R}^2 u(x)=(u(x),v(x))=(x,x) f(x)=(F\circ u)(x) D(F\circ u)(x)=DF(u(x),v(x))Du(x)=\left[\frac{\partial F}{\partial u}\mbox{ }\frac{\partial F}{\partial v}\right]\begin{bmatrix}u'(x)\\v'(x)\end{bmatrix}=\frac{\partial F}{\partial u}(u(x))u'(x)+\frac{\partial F}{\partial v}(u(x))v'(x) \frac{\partial F}{\partial u}=\frac{\partial}{\partial u}\int_u^{u^2}tan(v+y)dy=\mbox{tan}(v+u)=\mbox{tan}(x+x)=\mbox{tan(2x)} \frac{\partial F}{\partial v}=\frac{\partial}{\partial v}\int_u^{u^2}\mbox{tan}(v+y)dy=\int_u^{u^2}\frac{\partial}{\partial v}\mbox{tan}(v+y)dy=\int_x^{x^2}\frac{\partial}{\partial x}\mbox{tan}(x+y)dy=\int_x^{x^2}\mbox{sec}^2(x+y)dy","['real-analysis', 'multivariable-calculus', 'proof-verification']"
85,Is real part of a complex differentiable function real differentiable?,Is real part of a complex differentiable function real differentiable?,,"Let $f=u+i v$ be a complex function. Then the real part $u=u(x,y)$ is a multivariable real-valued function. It is well known that if $f$ is holomorphic at $z=x+iy$, then $u$ is $C^1$ near $(x,y) \in \mathbb{R}^2$. I was wondering if $u$ is differentiable at $(x,y)$ when $f$ is complex-differentiable at a point $z=x+iy.$ Would you give me any comment of it. Thanks in advance!","Let $f=u+i v$ be a complex function. Then the real part $u=u(x,y)$ is a multivariable real-valued function. It is well known that if $f$ is holomorphic at $z=x+iy$, then $u$ is $C^1$ near $(x,y) \in \mathbb{R}^2$. I was wondering if $u$ is differentiable at $(x,y)$ when $f$ is complex-differentiable at a point $z=x+iy.$ Would you give me any comment of it. Thanks in advance!",,"['complex-analysis', 'multivariable-calculus']"
86,Total derivative vanishes implies function is constant,Total derivative vanishes implies function is constant,,"I came across this problem: Let $f: \mathbb{R}^n \to \mathbb{R}^m$ be a totally differentiable function, whose derivative $Df$ vanishes for all $x\in \mathbb{R}^n$. Show that $f$ is constant. Hint: Consider the line segment $L$ connecting two arbitrary points $x_{0},y_{0}\in\mathbb{R}^n$ and show that for suitable points $x_{i},y_{i}\in L$, $|f(x_{i})-f(y_{i})|$ is sufficiently small. Use compactness to proof the statement. We aren't supposed to use partial derivatives like this: Functions where the total derivative is zero . Also, we don't have a mean value theorem for multi variable functions and we didn't introduce connectedness yet (as used in the second answer above). For the first part of the hint, one could use continuity of $f$ to force $|f(x_{i})-f(y_{i})|$ small enough. But this would require $\delta$ to be (very) small aswell. Regarding compactness, my first guess was to define a sequence of points $\in L$ and use sequential compactness in some sense. None of my thoughts worked out so far. Regards cerocius","I came across this problem: Let $f: \mathbb{R}^n \to \mathbb{R}^m$ be a totally differentiable function, whose derivative $Df$ vanishes for all $x\in \mathbb{R}^n$. Show that $f$ is constant. Hint: Consider the line segment $L$ connecting two arbitrary points $x_{0},y_{0}\in\mathbb{R}^n$ and show that for suitable points $x_{i},y_{i}\in L$, $|f(x_{i})-f(y_{i})|$ is sufficiently small. Use compactness to proof the statement. We aren't supposed to use partial derivatives like this: Functions where the total derivative is zero . Also, we don't have a mean value theorem for multi variable functions and we didn't introduce connectedness yet (as used in the second answer above). For the first part of the hint, one could use continuity of $f$ to force $|f(x_{i})-f(y_{i})|$ small enough. But this would require $\delta$ to be (very) small aswell. Regarding compactness, my first guess was to define a sequence of points $\in L$ and use sequential compactness in some sense. None of my thoughts worked out so far. Regards cerocius",,"['calculus', 'analysis', 'multivariable-calculus', 'derivatives', 'compactness']"
87,extract x: $((x+y)^n - x^n)^{1/n}$ expressed in the form $ba^{c}$ where all that matters is b contains x,extract x:  expressed in the form  where all that matters is b contains x,((x+y)^n - x^n)^{1/n} ba^{c},"I've done a bit of calculus/diff.eq./lin.alg. in the past. But that was a long time ago and to be honest I was always better at the actual applying than the pure math. Anyway, I have a function which I am trying to get into a certain form. I want to do this by isolating/extracting one variable. I've been trying on paper, Mathematica, Wolfram|Alpha and it's just not making sense to me anymore. Here's the function: $$((x+y)^n - x^n)^{1/n}$$ And I'm trying to extract the x out of this so I get something of the form: $$xa^\text{exponent}$$ Whatever the exponent is or 'a' is or if x has any +something/-something/divided by/exponents, or even if 'a' still contains x's I want x times something to an exponent. What I have come up with myself so far is driving me mad: I get two forms for 'a' depending on if I do $$(xa)^{1/n}$$ or $$x(a^{1/n})$$ As I said, I am really rusty and tried a lot of things which I think are oh-so wrong, but this is what I got trying to extracate x: $$x^n ((1+((1/x)y)^n -1)^{1/n}$$ OR $$x^n ((((1/x) + ((1/(x^n))y))^n)-1)^{1/n}$$ I'm not too proud to say I even got: $$x((1+(1/x)y)^n - x(1/n))^{1/n}$$ I'm doing this all for a hobby programming project of mine (lead android programmer) and I just can't get it to  work out ... plus I would like to be sure the answer is correct! If it helps, $x$,$y$ and $n$ are all positive integers. $y<x$ and $y\geq 1$, $x\geq3$ and $n\geq2$.","I've done a bit of calculus/diff.eq./lin.alg. in the past. But that was a long time ago and to be honest I was always better at the actual applying than the pure math. Anyway, I have a function which I am trying to get into a certain form. I want to do this by isolating/extracting one variable. I've been trying on paper, Mathematica, Wolfram|Alpha and it's just not making sense to me anymore. Here's the function: $$((x+y)^n - x^n)^{1/n}$$ And I'm trying to extract the x out of this so I get something of the form: $$xa^\text{exponent}$$ Whatever the exponent is or 'a' is or if x has any +something/-something/divided by/exponents, or even if 'a' still contains x's I want x times something to an exponent. What I have come up with myself so far is driving me mad: I get two forms for 'a' depending on if I do $$(xa)^{1/n}$$ or $$x(a^{1/n})$$ As I said, I am really rusty and tried a lot of things which I think are oh-so wrong, but this is what I got trying to extracate x: $$x^n ((1+((1/x)y)^n -1)^{1/n}$$ OR $$x^n ((((1/x) + ((1/(x^n))y))^n)-1)^{1/n}$$ I'm not too proud to say I even got: $$x((1+(1/x)y)^n - x(1/n))^{1/n}$$ I'm doing this all for a hobby programming project of mine (lead android programmer) and I just can't get it to  work out ... plus I would like to be sure the answer is correct! If it helps, $x$,$y$ and $n$ are all positive integers. $y<x$ and $y\geq 1$, $x\geq3$ and $n\geq2$.",,"['calculus', 'multivariable-calculus', 'discrete-mathematics', 'continuity']"
88,"Limit as $(x,y) \to (0,0)$ of $\frac{x^2\sin(x)}{x^2 + y^2}$",Limit as  of,"(x,y) \to (0,0) \frac{x^2\sin(x)}{x^2 + y^2}","I need to find $$\lim_{(x.y) \to (0,0)}\frac{x^2 \sin(x)}{x^2 + y^2}$$ Wolfram says that this limit is undefined. However, I attempted to solve this and I got that the limit is $0$. Therefore, I'd be grateful if you could tell me where my reasoning went wrong. Since the limit is at the origin, I can apply polar coordinates: $$\lim_{r \to 0} \frac{r^2 \sin^2(\theta) \sin(r \sin(\theta))}{r^2(\sin^2(\theta)+\cos^2(\theta))} = \lim_{r \to 0}\sin^2(\theta) \sin(r \sin(\theta))$$ Now, we know that this expression $\sin(\theta) $ is bounded. Therefore $r \sin(\theta)$ approaches $0$ since $r$ approaches $0$. This entails that $\sin(r \sin(\theta)) $ approaches $0$ because $\sin(0) = 0$. Finally. $\sin^2(\theta) $ is bounded, and so the limit in questoin becomes $0$. Where is the error in my reasoning?","I need to find $$\lim_{(x.y) \to (0,0)}\frac{x^2 \sin(x)}{x^2 + y^2}$$ Wolfram says that this limit is undefined. However, I attempted to solve this and I got that the limit is $0$. Therefore, I'd be grateful if you could tell me where my reasoning went wrong. Since the limit is at the origin, I can apply polar coordinates: $$\lim_{r \to 0} \frac{r^2 \sin^2(\theta) \sin(r \sin(\theta))}{r^2(\sin^2(\theta)+\cos^2(\theta))} = \lim_{r \to 0}\sin^2(\theta) \sin(r \sin(\theta))$$ Now, we know that this expression $\sin(\theta) $ is bounded. Therefore $r \sin(\theta)$ approaches $0$ since $r$ approaches $0$. This entails that $\sin(r \sin(\theta)) $ approaches $0$ because $\sin(0) = 0$. Finally. $\sin^2(\theta) $ is bounded, and so the limit in questoin becomes $0$. Where is the error in my reasoning?",,"['calculus', 'limits', 'multivariable-calculus']"
89,"Let $f : Q\to \mathbb{R}$ be bounded. Then the statement that $f$ is integrable over $Q$, with $\int_{Q}f = A$, is equivalent to the statement","Let  be bounded. Then the statement that  is integrable over , with , is equivalent to the statement",f : Q\to \mathbb{R} f Q \int_{Q}f = A,"Theorem. Let $f : Q\to \mathbb{R}$ be bounded. Then the statement that $f$ is integrable over $Q$, with $\int_{Q}f = A$, is equivalent to the statement that given $\epsilon>0$, there is a $\delta>0$ such that if $P$ is any partition of mesh less than $\delta$, and if, for each subrectangle $R$ determined by $P$, $x_R$ is a point of $R$, then $|\sum_{R}f(x_R)v(R) - A|<\epsilon$. The exercise of which I am doing one is the following: Let $f : Q\to \mathbb{R}$ be bounded. Then $f$ is integrable over $Q$ if and only if given $\epsilon> 0$, there is a $\delta> 0$ such that that still has no answer and I would like to know how to demonstrate that too. Suppose that $f$ is integrable with $\int_{Q}f$, then by the exercise, given $\epsilon>0$ there exists a $\delta>0$ such that if $P$ is a partition with a norm smaller than $\delta$, we have that $U(f,P)-L(f,P)<\epsilon$. Let $x_R\in R$ then we have the following inequalities $\sum_{R}f(x_R)v(R)\leq U(f,P)$ and $A\geq L(f,P)$ and so $\sum_{R}f(x_R)v(R)-A\leq U(f,P)-L(f,P)<\epsilon$, but I do not know how to prove this so I can keep the absolute value. Could anyone help me, please? For the other direction, suppose the second part of the theorem, then we will have that $U(f,P)-L(f,P)\leq |U(f,P)-L(f,P)|\leq |U(f,P)-A|+|L(f,P)-A|<2\epsilon$ and thus $f$ is integrable.","Theorem. Let $f : Q\to \mathbb{R}$ be bounded. Then the statement that $f$ is integrable over $Q$, with $\int_{Q}f = A$, is equivalent to the statement that given $\epsilon>0$, there is a $\delta>0$ such that if $P$ is any partition of mesh less than $\delta$, and if, for each subrectangle $R$ determined by $P$, $x_R$ is a point of $R$, then $|\sum_{R}f(x_R)v(R) - A|<\epsilon$. The exercise of which I am doing one is the following: Let $f : Q\to \mathbb{R}$ be bounded. Then $f$ is integrable over $Q$ if and only if given $\epsilon> 0$, there is a $\delta> 0$ such that that still has no answer and I would like to know how to demonstrate that too. Suppose that $f$ is integrable with $\int_{Q}f$, then by the exercise, given $\epsilon>0$ there exists a $\delta>0$ such that if $P$ is a partition with a norm smaller than $\delta$, we have that $U(f,P)-L(f,P)<\epsilon$. Let $x_R\in R$ then we have the following inequalities $\sum_{R}f(x_R)v(R)\leq U(f,P)$ and $A\geq L(f,P)$ and so $\sum_{R}f(x_R)v(R)-A\leq U(f,P)-L(f,P)<\epsilon$, but I do not know how to prove this so I can keep the absolute value. Could anyone help me, please? For the other direction, suppose the second part of the theorem, then we will have that $U(f,P)-L(f,P)\leq |U(f,P)-L(f,P)|\leq |U(f,P)-A|+|L(f,P)-A|<2\epsilon$ and thus $f$ is integrable.",,"['real-analysis', 'calculus', 'integration', 'multivariable-calculus', 'riemann-integration']"
90,"How do you show $f(x,y)=\left( \frac{x}{x^2+y^2},\frac{y}{x^2+y^2}\right)$ is a diffeomorphism?",How do you show  is a diffeomorphism?,"f(x,y)=\left( \frac{x}{x^2+y^2},\frac{y}{x^2+y^2}\right)","Given this mapping:$$f(x,y)=\left( \frac{x}{x^2+y^2},\frac{y}{x^2+y^2}\right), \quad (x,y)  \in \mathbb{R}^2$$ $$(\mathbb{R}^2= \{ (x,y):x,y \text{   are real numbers}, \text{ excluding } (x,y)=(0,0) \})$$ How do you check if it is a diffeomorphism? I would say $f$ is continuous on the given domain... now to how would you check for differentiability? Is it true to say that if the determinant of the Jacobian matrix is non-zero for all $(x,y)$ in the given domain then f is differentiable and therefore a diffeomorphism? This is a specific function that is its own inverse but considering a function that wasn't its own inverse does the fact that the determinant of the Jacobian matrix being non-zero (and existing) for the whole domain show that the inverse function exists without restricting the domain?","Given this mapping:$$f(x,y)=\left( \frac{x}{x^2+y^2},\frac{y}{x^2+y^2}\right), \quad (x,y)  \in \mathbb{R}^2$$ $$(\mathbb{R}^2= \{ (x,y):x,y \text{   are real numbers}, \text{ excluding } (x,y)=(0,0) \})$$ How do you check if it is a diffeomorphism? I would say $f$ is continuous on the given domain... now to how would you check for differentiability? Is it true to say that if the determinant of the Jacobian matrix is non-zero for all $(x,y)$ in the given domain then f is differentiable and therefore a diffeomorphism? This is a specific function that is its own inverse but considering a function that wasn't its own inverse does the fact that the determinant of the Jacobian matrix being non-zero (and existing) for the whole domain show that the inverse function exists without restricting the domain?",,"['real-analysis', 'analysis', 'multivariable-calculus', 'differential-geometry']"
91,Total derivative vs chain rule,Total derivative vs chain rule,,The total derivative as I understand it would be a linear map along the lines on the wikipedia page. https://en.wikipedia.org/wiki/Total_derivative However in some instances it looks alot like a chain rule or people use the chain rule to compute it. I dont see why we would assume that the variables are functions of some common parameter $t$ when we talk about total derivative. Why are these thing being mixed everywhere? Are they related somehow?,The total derivative as I understand it would be a linear map along the lines on the wikipedia page. https://en.wikipedia.org/wiki/Total_derivative However in some instances it looks alot like a chain rule or people use the chain rule to compute it. I dont see why we would assume that the variables are functions of some common parameter $t$ when we talk about total derivative. Why are these thing being mixed everywhere? Are they related somehow?,,[]
92,Integral That Requires Differentiating Under the Integral Sign Twice?,Integral That Requires Differentiating Under the Integral Sign Twice?,,"Is it ""legal"" to differentiate under the integral twice? Are there problems where there this is actually useful? If I were to differentiate under the integral sign twice, I would need to integrate the function with respect to the inserted variable twice too to get the original function, but in the process, I would need to find two different constants of integrations too. Does this make sense? Are there problems that actually simplify better by differentiating under the integral twice? Or is this completely not allowed by some definition or theorem that I'm unaware of? Hopefully, this was not too incoherent to understand,  thanks in advance.","Is it ""legal"" to differentiate under the integral twice? Are there problems where there this is actually useful? If I were to differentiate under the integral sign twice, I would need to integrate the function with respect to the inserted variable twice too to get the original function, but in the process, I would need to find two different constants of integrations too. Does this make sense? Are there problems that actually simplify better by differentiating under the integral twice? Or is this completely not allowed by some definition or theorem that I'm unaware of? Hopefully, this was not too incoherent to understand,  thanks in advance.",,"['calculus', 'integration', 'multivariable-calculus']"
93,Does there exist a function $f:\mathbb{R}^2\to\mathbb{R}$ that is discontinuous at a point but all of whose directional derivatives exist there?,Does there exist a function  that is discontinuous at a point but all of whose directional derivatives exist there?,f:\mathbb{R}^2\to\mathbb{R},Can such function exist whose all directional derivatives exist at a point but it isn't continuous at that point? Preferably give examples of $f: \Bbb R^2 \rightarrow \Bbb R$ . Our teacher said such a function can exist but the example he gave had one undefined directional derivative.,Can such function exist whose all directional derivatives exist at a point but it isn't continuous at that point? Preferably give examples of $f: \Bbb R^2 \rightarrow \Bbb R$ . Our teacher said such a function can exist but the example he gave had one undefined directional derivative.,,"['calculus', 'multivariable-calculus', 'partial-derivative']"
94,What is a compact 2-D submanifold?,What is a compact 2-D submanifold?,,"Show that the qeuations $x^3 + y^3 + z^3 + w^3 = 1$ $x^2 +y^2 + z^2 +w^2 =4$ define a compact 2-dimensional submanifold of $\mathbb{R}^4$. Write the equations for its tangent space at a point $(x_0,y_0,z_0,w_0)$. I'm not sure what a 2-dimensional submanifold is. The second equation looks like it would be a parabaloid of some sort which would be 3 dimensional right? I'm a little confused as to what this question is asking for. Can anybody help clarify?","Show that the qeuations $x^3 + y^3 + z^3 + w^3 = 1$ $x^2 +y^2 + z^2 +w^2 =4$ define a compact 2-dimensional submanifold of $\mathbb{R}^4$. Write the equations for its tangent space at a point $(x_0,y_0,z_0,w_0)$. I'm not sure what a 2-dimensional submanifold is. The second equation looks like it would be a parabaloid of some sort which would be 3 dimensional right? I'm a little confused as to what this question is asking for. Can anybody help clarify?",,"['real-analysis', 'multivariable-calculus', 'manifolds', 'compact-manifolds']"
95,Factor rational function of two variables,Factor rational function of two variables,,"Is there a factorization $$\frac{1}{1-wz} = f(w)g(z),$$ where $f$ does not depend on $z$ and $g$ does not depend on $w$? My assumptions are that $w,z \in \mathbb{C}$ and that $f,g:\mathbb{C}\rightarrow\mathbb{C}$. The factorization does not need to hold for all $(w,z)\in\mathbb{C}\times\mathbb{C}$, I would also be interested in subsets of $\mathbb{C}\times\mathbb{C}$ for which it might hold.","Is there a factorization $$\frac{1}{1-wz} = f(w)g(z),$$ where $f$ does not depend on $z$ and $g$ does not depend on $w$? My assumptions are that $w,z \in \mathbb{C}$ and that $f,g:\mathbb{C}\rightarrow\mathbb{C}$. The factorization does not need to hold for all $(w,z)\in\mathbb{C}\times\mathbb{C}$, I would also be interested in subsets of $\mathbb{C}\times\mathbb{C}$ for which it might hold.",,"['real-analysis', 'complex-analysis', 'multivariable-calculus']"
96,"Evaluate limit of as $(x,y)$ approaches $(0,0)$",Evaluate limit of as  approaches,"(x,y) (0,0)","$$ \lim_{(x,y)\to (0,0)}\frac{|y|}{\sqrt{x^2+y^2}}$$ I know that $|y|$ is both $+y$ and/or $-y$ do I evaluate the limit when $|y|$ is $+y$ and when it is $-y$ seperately to see if the limit matches? If so, what methods can I use?","$$ \lim_{(x,y)\to (0,0)}\frac{|y|}{\sqrt{x^2+y^2}}$$ I know that $|y|$ is both $+y$ and/or $-y$ do I evaluate the limit when $|y|$ is $+y$ and when it is $-y$ seperately to see if the limit matches? If so, what methods can I use?",,[]
97,"Show that $\lim_{(x, y)\to(0,0)}\frac{x^4y^4}{(x^2+y^4)^3}$ D.N.E",Show that  D.N.E,"\lim_{(x, y)\to(0,0)}\frac{x^4y^4}{(x^2+y^4)^3}","Show that $$\lim_{(x,y)\to(0,0)}\frac{x^4y^4}{(x^2+y^4)^3}$$ D.N.E In order to do this I should consider two paths and show that they do not reach the same limit. Consider when $x=0$. Then we have $\lim_{y\to 0}\dfrac{y^4}{y^{12}}=\dfrac{1}{y^8}=\infty$ Similarly if we choose $y=0$, then we have Then we have $\lim_{x\to 0}\dfrac{x^4}{x^{6}}=\dfrac{1}{x^2}=\infty$ Since both of these limits approach an asympote, can I even use them to show that the limit does not exist? Or must they approach a finite number?","Show that $$\lim_{(x,y)\to(0,0)}\frac{x^4y^4}{(x^2+y^4)^3}$$ D.N.E In order to do this I should consider two paths and show that they do not reach the same limit. Consider when $x=0$. Then we have $\lim_{y\to 0}\dfrac{y^4}{y^{12}}=\dfrac{1}{y^8}=\infty$ Similarly if we choose $y=0$, then we have Then we have $\lim_{x\to 0}\dfrac{x^4}{x^{6}}=\dfrac{1}{x^2}=\infty$ Since both of these limits approach an asympote, can I even use them to show that the limit does not exist? Or must they approach a finite number?",,"['calculus', 'limits', 'multivariable-calculus']"
98,"Determine whether $ f(x,y)$ exists given the partial derivatives",Determine whether  exists given the partial derivatives," f(x,y)","Find a function $z=f(x,y)$  whose partial derivatives are as given, or explain why this is impossible. We have that $ f_x$ = $ 3x^2y^2-2x$, and $f_y$ = $ 2x^3y+6y$. where $ f_z$ denotes the partial derivative of the function $ f$ with respect to some variable $ z$. I believe that given the partial derivatives, there is not a function $ z=f(x,y)$  whose partial derivatives are as given. Pf:  We will integrate both $f_x$ and $ f_y$ . The integral of $ f_x$  and the integral of $ f_y$  are not equal by calculus. QED. Am I correct?","Find a function $z=f(x,y)$  whose partial derivatives are as given, or explain why this is impossible. We have that $ f_x$ = $ 3x^2y^2-2x$, and $f_y$ = $ 2x^3y+6y$. where $ f_z$ denotes the partial derivative of the function $ f$ with respect to some variable $ z$. I believe that given the partial derivatives, there is not a function $ z=f(x,y)$  whose partial derivatives are as given. Pf:  We will integrate both $f_x$ and $ f_y$ . The integral of $ f_x$  and the integral of $ f_y$  are not equal by calculus. QED. Am I correct?",,"['integration', 'multivariable-calculus']"
99,"Why is there no Jacobian in the definition of the surface integral, $\iint_UfdS = \iint_Df(r(s,t))|r'_s \times r'_t|dsdt$?","Why is there no Jacobian in the definition of the surface integral, ?","\iint_UfdS = \iint_Df(r(s,t))|r'_s \times r'_t|dsdt","I'm extremely confused by Jacobians and when to use them and I think my lack of understanding can be boiled down to this question: Why is there no Jacobian in the definition of the surface integral:   $$\iint_UfdS = \iint_Df(r(s,t))|r'_s \times r'_t|dsdt$$   where U is some surface and D is another surface? Isn't $U$ a surface like any other? Don't we need to compensate for the area when going from $U$ to $D$? Has it got something to do with that $U$ is a ""function surface""(?) and D is a surface in $R^2$?","I'm extremely confused by Jacobians and when to use them and I think my lack of understanding can be boiled down to this question: Why is there no Jacobian in the definition of the surface integral:   $$\iint_UfdS = \iint_Df(r(s,t))|r'_s \times r'_t|dsdt$$   where U is some surface and D is another surface? Isn't $U$ a surface like any other? Don't we need to compensate for the area when going from $U$ to $D$? Has it got something to do with that $U$ is a ""function surface""(?) and D is a surface in $R^2$?",,"['multivariable-calculus', 'definition', 'surface-integrals', 'parametrization', 'jacobian']"
