,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Some questions regarding Ramanujan summation -- Part I,Some questions regarding Ramanujan summation -- Part I,,"The Ramanujan Summation method, is a method through which divergent series can be summed to convergent values. I have several questions regarding this summation method. For more info about the words used to describe divergent series summation methods, see this wiki article. Is it a regular summation method? Is it a linear summation method? Is it a stable summation method? More questions to come in Part II. Furthermore, I would like to point to this question on MSE by Peter Tamaroff, which hasn't been answered yet. I am also curious as to what the answers to his question might be.","The Ramanujan Summation method, is a method through which divergent series can be summed to convergent values. I have several questions regarding this summation method. For more info about the words used to describe divergent series summation methods, see this wiki article. Is it a regular summation method? Is it a linear summation method? Is it a stable summation method? More questions to come in Part II. Furthermore, I would like to point to this question on MSE by Peter Tamaroff, which hasn't been answered yet. I am also curious as to what the answers to his question might be.",,"['analysis', 'divergent-series']"
1,State of the art of the Implicit Function Theorem,State of the art of the Implicit Function Theorem,,What is the most general form of the Implicit Function Theorem? Quite a general form of this theorem was given by Kumagai (1980): An implicit function theorem. So I am  wondering what are the weakest assumption we have to make in order to obtain an implicit function theorem...,What is the most general form of the Implicit Function Theorem? Quite a general form of this theorem was given by Kumagai (1980): An implicit function theorem. So I am  wondering what are the weakest assumption we have to make in order to obtain an implicit function theorem...,,"['real-analysis', 'analysis', 'differential-geometry']"
2,Prove or disprove an inequality with $0 \le a_1 \le a_2 \le \ldots \le a_n$,Prove or disprove an inequality with,0 \le a_1 \le a_2 \le \ldots \le a_n,Let $n \in \mathbb{Z}_+$ be $n \ge 3$ and $0 \le a_1 \le a_2 \le \ldots \le a_n$. Prove or disprove an inequality: $$\large \sqrt{a_1a_2} + \sqrt{a_2a_3} + \ldots + \sqrt{a_na_1} \ge \sqrt[3]{a_1a_2a_3} + \sqrt[3]{a_2a_3a_4} + \ldots + \sqrt[3]{a_na_1a_2}$$,Let $n \in \mathbb{Z}_+$ be $n \ge 3$ and $0 \le a_1 \le a_2 \le \ldots \le a_n$. Prove or disprove an inequality: $$\large \sqrt{a_1a_2} + \sqrt{a_2a_3} + \ldots + \sqrt{a_na_1} \ge \sqrt[3]{a_1a_2a_3} + \sqrt[3]{a_2a_3a_4} + \ldots + \sqrt[3]{a_na_1a_2}$$,,"['analysis', 'inequality']"
3,derivative and cut off functions,derivative and cut off functions,,"Is there any way of constuct a cut off function which is $1$ in $B(0,\epsilon)$ and zero outside the ball $B(0,2\epsilon)$ and it's first and second derivative is smaller than $1/|x|^a$ , with $a<1/4$?","Is there any way of constuct a cut off function which is $1$ in $B(0,\epsilon)$ and zero outside the ball $B(0,2\epsilon)$ and it's first and second derivative is smaller than $1/|x|^a$ , with $a<1/4$?",,['analysis']
4,"Prove that (M,+,*) is a field","Prove that (M,+,*) is a field",,"Prove that the multiplication $*:M \times M \to M$ defined by this table: * | 0  1 -------- 0 | 0 0 1 | 0 1 together with the commutative group (M,+), is a field (M,+,*). Group axioms: 1) Closure: $0*0=0 \in M$ $0*1=0 \in M $ $ 1*0=0 \in M$ $ 1*1=1 \in M $ 2) Inverse element: $1*1=1  $ $ 1*1=1 $ $\forall i,a,e \in M : a*i=i*a=e$ here the inverse element is i=e=1. 3) Identity element: $ 0*1=0 $ $1*0=0$ $1*1=1$ $\forall e,a\in M: e*a=a*e=a $ with $e=1$ 4) Associativity: 4.1) $0*(0*0)=0 \leftrightarrow (0*0)*0=0$ 4.2) $0*(0*1)=0 \leftrightarrow (0*0)*1=0$ 4.3) $0*(1*0)=0 \leftrightarrow (0*1)*0=0$ 4.4) $0*(1*1)=0 \leftrightarrow (0*1)*1=0$ 4.5) $1*(0*0)=0 \leftrightarrow (1*0)*0=0$ 4.6) $1*(0*1)=0 \leftrightarrow (1*0)*1=0$ 4.7) $1*(1*0)=0 \leftrightarrow (1*1)*0=0$ 4.8) $1*(1*1)=1 \leftrightarrow (1*1)*1=1$ $\forall a,b,c\in M : a*(b*c)=(a*b)*c$ the addition $+:M \times M \to M$ defined by: $+ | 0$ $1$ ------------ $0 | 0$  $1$ $1 | 1$  $0$ Field condition: 1F) Commutativity: $0*0=0=0*0$ $ 1*0=0=0*1 $ $ 1*1=1=1*1 $ 2F) Distributivity: 2.1F) $0*(0+0)=0 \leftrightarrow (0*0)+(0*0)=0$ 2.2F) $0*(0+1)=0 \leftrightarrow (0*0)+(0*1)=0$ 2.3F) $0*(1+0)=0 \leftrightarrow (0*1)*(0*0)=0$ 2.4F) $0*(1+1)=0 \leftrightarrow (0*1)+(0*1)=0$ 2.5F) $1*(0+0)=0 \leftrightarrow (1*0)+(1*0)=0$ 2.6F) $1*(0+1)=1 \leftrightarrow (1*0)+(1*1)=1$ 2.7F) $1*(1+0)=1 \leftrightarrow (1*1)+(1*0)=1$ 2.8F) $1*(1+1)=0 \leftrightarrow (1*1)+(1*1)=0$ $\forall a,b,c\in M : a*(b+c)=(a*b)+(a*c)$ Could you please tell me if I have made a mistake? And what about 2) the inverse element is that correct?(because in the ""usual multiplication"" the inverse element i is $i=a^{-1}$ defined?!)","Prove that the multiplication $*:M \times M \to M$ defined by this table: * | 0  1 -------- 0 | 0 0 1 | 0 1 together with the commutative group (M,+), is a field (M,+,*). Group axioms: 1) Closure: $0*0=0 \in M$ $0*1=0 \in M $ $ 1*0=0 \in M$ $ 1*1=1 \in M $ 2) Inverse element: $1*1=1  $ $ 1*1=1 $ $\forall i,a,e \in M : a*i=i*a=e$ here the inverse element is i=e=1. 3) Identity element: $ 0*1=0 $ $1*0=0$ $1*1=1$ $\forall e,a\in M: e*a=a*e=a $ with $e=1$ 4) Associativity: 4.1) $0*(0*0)=0 \leftrightarrow (0*0)*0=0$ 4.2) $0*(0*1)=0 \leftrightarrow (0*0)*1=0$ 4.3) $0*(1*0)=0 \leftrightarrow (0*1)*0=0$ 4.4) $0*(1*1)=0 \leftrightarrow (0*1)*1=0$ 4.5) $1*(0*0)=0 \leftrightarrow (1*0)*0=0$ 4.6) $1*(0*1)=0 \leftrightarrow (1*0)*1=0$ 4.7) $1*(1*0)=0 \leftrightarrow (1*1)*0=0$ 4.8) $1*(1*1)=1 \leftrightarrow (1*1)*1=1$ $\forall a,b,c\in M : a*(b*c)=(a*b)*c$ the addition $+:M \times M \to M$ defined by: $+ | 0$ $1$ ------------ $0 | 0$  $1$ $1 | 1$  $0$ Field condition: 1F) Commutativity: $0*0=0=0*0$ $ 1*0=0=0*1 $ $ 1*1=1=1*1 $ 2F) Distributivity: 2.1F) $0*(0+0)=0 \leftrightarrow (0*0)+(0*0)=0$ 2.2F) $0*(0+1)=0 \leftrightarrow (0*0)+(0*1)=0$ 2.3F) $0*(1+0)=0 \leftrightarrow (0*1)*(0*0)=0$ 2.4F) $0*(1+1)=0 \leftrightarrow (0*1)+(0*1)=0$ 2.5F) $1*(0+0)=0 \leftrightarrow (1*0)+(1*0)=0$ 2.6F) $1*(0+1)=1 \leftrightarrow (1*0)+(1*1)=1$ 2.7F) $1*(1+0)=1 \leftrightarrow (1*1)+(1*0)=1$ 2.8F) $1*(1+1)=0 \leftrightarrow (1*1)+(1*1)=0$ $\forall a,b,c\in M : a*(b+c)=(a*b)+(a*c)$ Could you please tell me if I have made a mistake? And what about 2) the inverse element is that correct?(because in the ""usual multiplication"" the inverse element i is $i=a^{-1}$ defined?!)",,"['analysis', 'axioms']"
5,"Finding two functions (density) $g,f$ satisfying some conditions",Finding two functions (density)  satisfying some conditions,"g,f","Is there a clever way to find two density functions, $f$ and $g$, that satisfy the following conditions? $$\begin{align*} \int_{\infty}^{m}\int_{-\infty}^{\infty}f(w)f(w+z)\,dw\,dz&=\int_{\infty}^{m}\int_{-\infty}^{\infty}g(w)g(w+z)\,dw\,dz\\  \int_{\infty}^{m+2}\int_{-\infty}^{\infty}f(w)f(w+z)\,dw\,dz&=\int_{\infty}^{m+1}\int_{-\infty}^{\infty}g(w)g(w+z)\,dw\,dz\\  \int_{-\infty}^{\infty}f(w)\,dw&=\int_{-\infty}^{\infty}g(w)\,dw=M\\  \end{align*}$$ where $f\gt 0$ and $g\gt 0$ almost everywhere? for $m\in (-\delta,\delta)$ and $\delta$ is some small number. My main intent is to come up with two i.i.d. random variable, $X'$ and $X''$ and $Y$ and $Y''$, such that $\operatorname{\mathbb{Pr}}(m> Y'-Y'')=\operatorname{\mathbb{Pr}}(m>X'-X'')$ for $m \in (-b,b)$ for some $b$ small enough, while $\operatorname{\mathbb{Pr}}(m+2> Y'-Y'')=\operatorname{\mathbb{Pr}}(m+1> X'-X'')$.Is this possible? Thanks so much in advance for your much appreciated help.","Is there a clever way to find two density functions, $f$ and $g$, that satisfy the following conditions? $$\begin{align*} \int_{\infty}^{m}\int_{-\infty}^{\infty}f(w)f(w+z)\,dw\,dz&=\int_{\infty}^{m}\int_{-\infty}^{\infty}g(w)g(w+z)\,dw\,dz\\  \int_{\infty}^{m+2}\int_{-\infty}^{\infty}f(w)f(w+z)\,dw\,dz&=\int_{\infty}^{m+1}\int_{-\infty}^{\infty}g(w)g(w+z)\,dw\,dz\\  \int_{-\infty}^{\infty}f(w)\,dw&=\int_{-\infty}^{\infty}g(w)\,dw=M\\  \end{align*}$$ where $f\gt 0$ and $g\gt 0$ almost everywhere? for $m\in (-\delta,\delta)$ and $\delta$ is some small number. My main intent is to come up with two i.i.d. random variable, $X'$ and $X''$ and $Y$ and $Y''$, such that $\operatorname{\mathbb{Pr}}(m> Y'-Y'')=\operatorname{\mathbb{Pr}}(m>X'-X'')$ for $m \in (-b,b)$ for some $b$ small enough, while $\operatorname{\mathbb{Pr}}(m+2> Y'-Y'')=\operatorname{\mathbb{Pr}}(m+1> X'-X'')$.Is this possible? Thanks so much in advance for your much appreciated help.",,"['analysis', 'measure-theory']"
6,"Characterize the continuous functions with finite right-hand derivative for at least one point of $[0, 1]$",Characterize the continuous functions with finite right-hand derivative for at least one point of,"[0, 1]","Let $(E,d_\infty)$ the metric space of continuous functions defined on $ [0,1] $ , with $$ d_\infty(f,g)=\sup\{ |f(x) - g(x)| : x\in [0,1] \}. $$ For all $ n\in \mathbb{N} $ let $$ F_n = \{ f: \exists x_0\in [0,1-1/n] \forall x\in [x_0,1]\left(|f(x)-f(x_0)|\leq n(x-x_0)\right) \}. $$ Let $D$ be the set of continuous functions which have a finite derivate on the right for at least one point of $[0, 1)$ . I need to show that $$ D = \bigcup_{n\in\mathbb{N}} F_n. $$ This is part of problem 38 of section 8 of Royden's Real Analysis book.","Let the metric space of continuous functions defined on , with For all let Let be the set of continuous functions which have a finite derivate on the right for at least one point of . I need to show that This is part of problem 38 of section 8 of Royden's Real Analysis book.","(E,d_\infty)  [0,1]   d_\infty(f,g)=\sup\{ |f(x) - g(x)| : x\in [0,1] \}.   n\in \mathbb{N}   F_n = \{ f: \exists x_0\in [0,1-1/n] \forall x\in [x_0,1]\left(|f(x)-f(x_0)|\leq n(x-x_0)\right) \}.  D [0, 1)  D = \bigcup_{n\in\mathbb{N}} F_n. ","['real-analysis', 'analysis']"
7,Is there anything wrong with this proposed proof of the irrationality of Euler's constant?,Is there anything wrong with this proposed proof of the irrationality of Euler's constant?,,"Let $\{\lambda_n\}$ be the sequence given by $H_n - \ln n$. We claim that $\lambda_n$ is irrational for every integer $n>1$ and justify this by the following argument: Assume that $\lambda_k$ is rational for some integer $k>1$ such that $H_k - \ln k = p/q$ where $p$ and $q$ are integers. Rearranging the above we arrive at $H_k - p/q = \ln k$, which implies that $\ln k$ is rational since $H_k$ is rational. But we know that $\ln k$ is irrational for all integers $k>1$, hence we reach a contradiction. Therefore, $\lambda_n$ is irrational for all integers $n>1$. Hence the limit as $n$ tends to infinity is irrational, and we are done.","Let $\{\lambda_n\}$ be the sequence given by $H_n - \ln n$. We claim that $\lambda_n$ is irrational for every integer $n>1$ and justify this by the following argument: Assume that $\lambda_k$ is rational for some integer $k>1$ such that $H_k - \ln k = p/q$ where $p$ and $q$ are integers. Rearranging the above we arrive at $H_k - p/q = \ln k$, which implies that $\ln k$ is rational since $H_k$ is rational. But we know that $\ln k$ is irrational for all integers $k>1$, hence we reach a contradiction. Therefore, $\lambda_n$ is irrational for all integers $n>1$. Hence the limit as $n$ tends to infinity is irrational, and we are done.",,[]
8,The functional equation $f(x+x) = f(x)f(x)$,The functional equation,f(x+x) = f(x)f(x),"Consider the exponential functional equation on the diagonal \begin{align} f(x+x) = f(x)f(x)\,, \quad f(0) = 1\,. \end{align} What are the solutions for this problem? Either for all $x $ or for non-negative $x $. EDIT: Apparently the following is wrong:  I guess the exponential functions $x \mapsto e^{cx}$ are the only solutions if we assume measurability, continuity or some other regularity property.","Consider the exponential functional equation on the diagonal \begin{align} f(x+x) = f(x)f(x)\,, \quad f(0) = 1\,. \end{align} What are the solutions for this problem? Either for all $x $ or for non-negative $x $. EDIT: Apparently the following is wrong:  I guess the exponential functions $x \mapsto e^{cx}$ are the only solutions if we assume measurability, continuity or some other regularity property.",,"['real-analysis', 'analysis', 'functional-equations']"
9,Definition of connectedness?,Definition of connectedness?,,"Connectedness is defined as: ""A metric space $E$ is connected if the only subsets of $E$ which are both open and closed are $E$ and $\varnothing$. A subset $S$ of a metric space is a connected subset if the subspace $S$ is connected."" Can someone provide me with a more trivial/simple definition of connectedness?","Connectedness is defined as: ""A metric space $E$ is connected if the only subsets of $E$ which are both open and closed are $E$ and $\varnothing$. A subset $S$ of a metric space is a connected subset if the subspace $S$ is connected."" Can someone provide me with a more trivial/simple definition of connectedness?",,['real-analysis']
10,How prove $\pi^2>2^\pi$,How prove,\pi^2>2^\pi,"show that $$\pi^2>2^\pi$$ I use computer found  $$\pi^2-2^\pi\approx 1.044\cdots,$$ can  see this I know  $$\Longleftrightarrow \dfrac{\ln{\pi}}{\pi}>\dfrac{\ln{2}}{2}$$ so let $$f(x)=\dfrac{\ln{x}}{x}$$ so $$f'(x)=\dfrac{1-\ln{x}}{x^2}=0,x=e$$ so $f(x)$ is Strictly increasing  on $(2,e)$, and is Strictly decreasing  on $(e,3) $ so I can't know $f(2)$ and $f(\pi)$ which is bigger? maybe this problem exsit have easy methods by hand","show that $$\pi^2>2^\pi$$ I use computer found  $$\pi^2-2^\pi\approx 1.044\cdots,$$ can  see this I know  $$\Longleftrightarrow \dfrac{\ln{\pi}}{\pi}>\dfrac{\ln{2}}{2}$$ so let $$f(x)=\dfrac{\ln{x}}{x}$$ so $$f'(x)=\dfrac{1-\ln{x}}{x^2}=0,x=e$$ so $f(x)$ is Strictly increasing  on $(2,e)$, and is Strictly decreasing  on $(e,3) $ so I can't know $f(2)$ and $f(\pi)$ which is bigger? maybe this problem exsit have easy methods by hand",,"['analysis', 'inequality', 'pi']"
11,Improper integral from 1 to infinity $\Rightarrow$ integrated function converges towards zero?,Improper integral from 1 to infinity  integrated function converges towards zero?,\Rightarrow,"Let $f: [1, \infty) \to \mathbb{R}$ be a continuous function such that the improper integral $$\int_1^\infty f(x) \ dx$$ exists. Show or disprove that $\lim \limits _{x \to \infty} f(x) =0$. Our professor said that there are counterexamples but after frying my brain out I still could not find any. It does sound logical.","Let $f: [1, \infty) \to \mathbb{R}$ be a continuous function such that the improper integral $$\int_1^\infty f(x) \ dx$$ exists. Show or disprove that $\lim \limits _{x \to \infty} f(x) =0$. Our professor said that there are counterexamples but after frying my brain out I still could not find any. It does sound logical.",,"['analysis', 'improper-integrals']"
12,Real roots of $3^{x} + 4^{x} = 5^{x}$,Real roots of,3^{x} + 4^{x} = 5^{x},How do I show that $3^{x}+4^{x} = 5^{x}$ has exactly one real root.,How do I show that $3^{x}+4^{x} = 5^{x}$ has exactly one real root.,,"['real-analysis', 'analysis']"
13,partitioning $\mathbb{R}$ into two dense sets of equal cardinality,partitioning  into two dense sets of equal cardinality,\mathbb{R},"Just out of curiosity, is it possible to partition $\mathbb{R}$ into two dense sets of equal cardinality? I was thinking something like this: Let $S$ be the basis of $\mathbb{R}$ over $\mathbb{Q}$. Then $S$ is of equal cardinality as $\mathbb{R}$. Let $A$ be an uncountable subset of $S$ such that $S\setminus A$ is also uncountable. Such a set exists, for example if $f$ is a bijection from $S$ to $\mathbb{R}$, put $a$ in $A$ if and only if $f(a)>0$. Now let $T$ be the subset of $\mathbb{R}$ such that $t\in T$ if and only if $tq\in A$ for some rational number $q$. Then $T$ and $\mathbb{R}\setminus T$ gives the desired partition. But, as far as I know, the existence of a basis of $\mathbb{R}$ over $\mathbb{Q}$ depends on the Axiom of Choice. Is it possible to prove the above statement without assuming the Axiom of Choice?","Just out of curiosity, is it possible to partition $\mathbb{R}$ into two dense sets of equal cardinality? I was thinking something like this: Let $S$ be the basis of $\mathbb{R}$ over $\mathbb{Q}$. Then $S$ is of equal cardinality as $\mathbb{R}$. Let $A$ be an uncountable subset of $S$ such that $S\setminus A$ is also uncountable. Such a set exists, for example if $f$ is a bijection from $S$ to $\mathbb{R}$, put $a$ in $A$ if and only if $f(a)>0$. Now let $T$ be the subset of $\mathbb{R}$ such that $t\in T$ if and only if $tq\in A$ for some rational number $q$. Then $T$ and $\mathbb{R}\setminus T$ gives the desired partition. But, as far as I know, the existence of a basis of $\mathbb{R}$ over $\mathbb{Q}$ depends on the Axiom of Choice. Is it possible to prove the above statement without assuming the Axiom of Choice?",,['analysis']
14,Continuous function with local maxima everywhere but no global maxima,Continuous function with local maxima everywhere but no global maxima,,"Can there be such a function: $f \colon \mathbb R \to \mathbb R$ is continuous and non-constant. It has a local maxima everywhere, i.e., for all $x \in \mathbb R$ there is some $\delta_x>0$ such that $f(x)\geq f(y)$ for all $y \in B(x,\delta_x)$. And, yet $f$ has no global maxima? Thank you. PS: $\mathbb R$ is with the usual topology. This is true for $\mathbb R$ with upper-limit topology.","Can there be such a function: $f \colon \mathbb R \to \mathbb R$ is continuous and non-constant. It has a local maxima everywhere, i.e., for all $x \in \mathbb R$ there is some $\delta_x>0$ such that $f(x)\geq f(y)$ for all $y \in B(x,\delta_x)$. And, yet $f$ has no global maxima? Thank you. PS: $\mathbb R$ is with the usual topology. This is true for $\mathbb R$ with upper-limit topology.",,['analysis']
15,"$\int f_k\to 0 $ but $f_k $ does not converge to $0 $ ae, where $ f_k $ is defined in $[0, 1] $","but  does not converge to  ae, where  is defined in","\int f_k\to 0  f_k  0   f_k  [0, 1] ","Give an exemple, in [0, 1], of a sequence of functions $ f_k $ such that $||f_k||_ 1=\int |f|_k \to 0 $ but $ f_k $ does not converge to $0 $ a.e.","Give an exemple, in [0, 1], of a sequence of functions $ f_k $ such that $||f_k||_ 1=\int |f|_k \to 0 $ but $ f_k $ does not converge to $0 $ a.e.",,"['real-analysis', 'analysis', 'examples-counterexamples', 'lp-spaces']"
16,prove that $(1 + x)^\frac{1}{b}$ is a formal power series,prove that  is a formal power series,(1 + x)^\frac{1}{b},How to prove that $(1 + x)^\frac{1}{b}$ (where $b$ is an integer) can be expressed as a formal power series without using Binomial theorem?,How to prove that $(1 + x)^\frac{1}{b}$ (where $b$ is an integer) can be expressed as a formal power series without using Binomial theorem?,,"['analysis', 'power-series']"
17,"With the epsilon-delta definition of continuity, are the rationals continuous?","With the epsilon-delta definition of continuity, are the rationals continuous?",,"My current knowledge is that a function is continuous at a point $x=a$ if and only if, for any $\epsilon>0$ , there exists some $\delta>0$ such that $$  |x-a|<\delta \implies |f(x)-f(a)|<\varepsilon. $$ If I have a function $f: \mathbb{Q} \to \mathbb{R}$ , say $f(x)=\log\left(\sqrt{x}\right)$ , where the domain is all positive rationals, surely I can always find a $\delta$ for any $\varepsilon$ ? However, I feel like I’ve heard that the rationals aren’t continuous. Is that false (that is, they are actually continuous), or is there an additional restriction to the definition of continuity?","My current knowledge is that a function is continuous at a point if and only if, for any , there exists some such that If I have a function , say , where the domain is all positive rationals, surely I can always find a for any ? However, I feel like I’ve heard that the rationals aren’t continuous. Is that false (that is, they are actually continuous), or is there an additional restriction to the definition of continuity?","x=a \epsilon>0 \delta>0 
 |x-a|<\delta \implies |f(x)-f(a)|<\varepsilon.
 f: \mathbb{Q} \to \mathbb{R} f(x)=\log\left(\sqrt{x}\right) \delta \varepsilon",['analysis']
18,"Give an example of function continuous for $x \in (a, b)$ and not differentiable in $a < x_{1} <\dots < x_{n} < b $, but differentiable in intervals.","Give an example of function continuous for  and not differentiable in , but differentiable in intervals.","x \in (a, b) a < x_{1} <\dots < x_{n} < b ","Give an example of a function continuous for x $\in (a, b)$ and not differentiable in $a < x_{1}< \dots < x_{n} < b $ , but differentiable in intervals $(a, x_{1}), (x_{n}, b), (x_{i}, x_{i+1})$ for $i = 1,  \ldots, n-1 $ My idea: $$ f(x)= \begin{cases} \frac{x-a}{x_{1}-a} & x \in  [a,x_{1}] \\ -\frac{(x-x_{n-1})}{(x_{n}-x_{n-1})} + 1  &  x \in  [x_{n-1}, x_{n}] \text{ and  n odd} \\ \frac{x-x_{n-1}}{x_{n}-x_{n-1}}  & x \in  [x_{n-1}, x_{n}]\text{ and n even }\\ \frac{x-x_{n}}{x_{n}-b} & x \in [x_{n},b] \end{cases} $$","Give an example of a function continuous for x and not differentiable in , but differentiable in intervals for My idea:","\in (a, b) a < x_{1}< \dots < x_{n} < b  (a, x_{1}), (x_{n}, b), (x_{i}, x_{i+1}) i = 1,
 \ldots, n-1  
f(x)=
\begin{cases}
\frac{x-a}{x_{1}-a} & x \in  [a,x_{1}] \\
-\frac{(x-x_{n-1})}{(x_{n}-x_{n-1})} + 1  &  x \in  [x_{n-1}, x_{n}] \text{ and  n odd} \\
\frac{x-x_{n-1}}{x_{n}-x_{n-1}}  & x \in  [x_{n-1}, x_{n}]\text{ and n even }\\
\frac{x-x_{n}}{x_{n}-b} & x \in [x_{n},b]
\end{cases}
","['real-analysis', 'analysis']"
19,What is the main difference between pointwise and uniform convergence as defined here?,What is the main difference between pointwise and uniform convergence as defined here?,,"I have a little confusion here. I have seen the following several times and seem to be a bit confused as to differentiating them. Let $E$ be a non-empty subset of $\Bbb{R}$ . A sequence of functions $\{f_n\}_{n\in \Bbb{N}},$ converges pointwise to $f$ on $E$ if and only if \begin{align}f_n(x)\to f(x),\;\forall\,x\in E.\end{align} On the other hand $\{f_n\}_{n\in \Bbb{N}},$ converges uniformly to $f$ on $E$ if and only if \begin{align}f_n(x)\to f(x),\;\forall\,x\in E.\end{align} QUESTION: Why is $f_n(x)\to f(x),\;\forall\,x\in E,$ is used for both uniform and pointwise convergence or I'm I missing something important? Can't we distinguish them?",I have a little confusion here. I have seen the following several times and seem to be a bit confused as to differentiating them. Let be a non-empty subset of . A sequence of functions converges pointwise to on if and only if On the other hand converges uniformly to on if and only if QUESTION: Why is is used for both uniform and pointwise convergence or I'm I missing something important? Can't we distinguish them?,"E \Bbb{R} \{f_n\}_{n\in \Bbb{N}}, f E \begin{align}f_n(x)\to f(x),\;\forall\,x\in E.\end{align} \{f_n\}_{n\in \Bbb{N}}, f E \begin{align}f_n(x)\to f(x),\;\forall\,x\in E.\end{align} f_n(x)\to f(x),\;\forall\,x\in E,","['real-analysis', 'analysis', 'definition', 'uniform-convergence', 'pointwise-convergence']"
20,Constructive intermediate value theorem,Constructive intermediate value theorem,,"I have given real numbers $x_1,x_2,y_1,y_2$ such that $x_1 > x_2$ and $y_1 < y_2$. The the claim is that there exists some $\lambda \in (0,1)$ such that $\lambda (x_1 - x_2) + (1-\lambda)(y_1-y_2) = 0$. In order to proof this, one needs ( at least in my opinion) the intermediate value theorem. But the intermediate value theorem does not hold in constructive mathematics (that is without the law of excluded middle; or constructive mathematics acts in intuitionistic logic). For a proof of this c.f. this paper. Is there any constructive way to show the above equation?","I have given real numbers $x_1,x_2,y_1,y_2$ such that $x_1 > x_2$ and $y_1 < y_2$. The the claim is that there exists some $\lambda \in (0,1)$ such that $\lambda (x_1 - x_2) + (1-\lambda)(y_1-y_2) = 0$. In order to proof this, one needs ( at least in my opinion) the intermediate value theorem. But the intermediate value theorem does not hold in constructive mathematics (that is without the law of excluded middle; or constructive mathematics acts in intuitionistic logic). For a proof of this c.f. this paper. Is there any constructive way to show the above equation?",,['analysis']
21,Real continuous function with attractive fixed point has range subset of domain,Real continuous function with attractive fixed point has range subset of domain,,"Let $f:\mathbb{R}\to \mathbb{R}$ be $C^1$ with fixed point $p$, such that $|f'(p)|< 1$. Then there exists an interval $I$ containing $p$ such that $f:I\to I$. I'm trying to prove this rigorously, but there's a roadblock. My best attempt is as follows: Let $\varepsilon > 0$, such that $\varepsilon$ is small, and consider $f(p+\varepsilon)=f(p)+f'(p)\varepsilon+\mathcal{O}(\varepsilon^2)$ and $f(p-\varepsilon)=f(p)-f'(p)\varepsilon+\mathcal{O}(\varepsilon^2)$. Then $$|f(p+\varepsilon)-f(p-\varepsilon)|=|\varepsilon(2f'(p))+O(\varepsilon^2)|$$$$\le 2\varepsilon|f'(p)|+|\mathcal{O}(\varepsilon^2)|<2\varepsilon+|\mathcal{O}(\varepsilon^2)|$$ Everything would be as desired if not for the $|\mathcal{O}(\varepsilon^2)|$ term. How does one get rid of it? I could argue that since $\varepsilon$ is small, we can neglect the term, but that is not very rigorous. Would appreciate your insight.","Let $f:\mathbb{R}\to \mathbb{R}$ be $C^1$ with fixed point $p$, such that $|f'(p)|< 1$. Then there exists an interval $I$ containing $p$ such that $f:I\to I$. I'm trying to prove this rigorously, but there's a roadblock. My best attempt is as follows: Let $\varepsilon > 0$, such that $\varepsilon$ is small, and consider $f(p+\varepsilon)=f(p)+f'(p)\varepsilon+\mathcal{O}(\varepsilon^2)$ and $f(p-\varepsilon)=f(p)-f'(p)\varepsilon+\mathcal{O}(\varepsilon^2)$. Then $$|f(p+\varepsilon)-f(p-\varepsilon)|=|\varepsilon(2f'(p))+O(\varepsilon^2)|$$$$\le 2\varepsilon|f'(p)|+|\mathcal{O}(\varepsilon^2)|<2\varepsilon+|\mathcal{O}(\varepsilon^2)|$$ Everything would be as desired if not for the $|\mathcal{O}(\varepsilon^2)|$ term. How does one get rid of it? I could argue that since $\varepsilon$ is small, we can neglect the term, but that is not very rigorous. Would appreciate your insight.",,"['real-analysis', 'analysis', 'functions', 'proof-verification', 'fixed-points']"
22,"Prove that for all $n\in\mathbb{N}$, $\sqrt{n(n+1)}$ is not an integer.","Prove that for all ,  is not an integer.",n\in\mathbb{N} \sqrt{n(n+1)},I'm sure this is a very simple proof but I can't seem to get it right. I tried to do it by induction but get stuck trying to show that $\sqrt{(k+1)(k+2)}$ is not an integer and also cannot seem to do it through other methods. Anyone have any ideas? Thanks so much in advance!,I'm sure this is a very simple proof but I can't seem to get it right. I tried to do it by induction but get stuck trying to show that $\sqrt{(k+1)(k+2)}$ is not an integer and also cannot seem to do it through other methods. Anyone have any ideas? Thanks so much in advance!,,"['real-analysis', 'analysis']"
23,Why is exponentiation defined as $x^y=e^{\ln(x)\cdot y}$?,Why is exponentiation defined as ?,x^y=e^{\ln(x)\cdot y},"There are many curves that extend integer exponentiation to larger domains, so why was this one chosen?","There are many curves that extend integer exponentiation to larger domains, so why was this one chosen?",,"['analysis', 'definition', 'exponentiation']"
24,Problem similar to folland chapter 2 problem 51.,Problem similar to folland chapter 2 problem 51.,,"Let $(X\mathcal M,\mu)$ and $(Y,\mathcal N,v)$ be $\sigma$-finite measure spaces. If $f:X\to\Bbb R$ is $\mathcal M$-measurable, $g:Y\to\Bbb R$ is $\mathcal N$-measurable, and $h(x,y)=f(x)g(y)$, prove that $h$ is $\mathcal M\otimes \mathcal N$-measurable. If $f\in L^1(\mu)$ and $g\in L^1(v)$, prove that $h\in L^1(\mu\times v)$ and $$\int\, h\,d(\mu\times v)=\left[\int\,f\,\mathrm d\mu\right]\left[\int\,g\,\mathrm dv\right].$$ Could someone show me how to prove this. I have been staring at it, and cannot even think where to start. I am a bit desperate here. I will highly rate for your help. Thank you.","Let $(X\mathcal M,\mu)$ and $(Y,\mathcal N,v)$ be $\sigma$-finite measure spaces. If $f:X\to\Bbb R$ is $\mathcal M$-measurable, $g:Y\to\Bbb R$ is $\mathcal N$-measurable, and $h(x,y)=f(x)g(y)$, prove that $h$ is $\mathcal M\otimes \mathcal N$-measurable. If $f\in L^1(\mu)$ and $g\in L^1(v)$, prove that $h\in L^1(\mu\times v)$ and $$\int\, h\,d(\mu\times v)=\left[\int\,f\,\mathrm d\mu\right]\left[\int\,g\,\mathrm dv\right].$$ Could someone show me how to prove this. I have been staring at it, and cannot even think where to start. I am a bit desperate here. I will highly rate for your help. Thank you.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
25,"Finding the value of $f(2)$ if $f\left(\frac{x+y}{2}\right)=\frac{f(x)+f(y)}{2}$, $f(0)=1$ and $f'(0)=-1$","Finding the value of  if ,  and",f(2) f\left(\frac{x+y}{2}\right)=\frac{f(x)+f(y)}{2} f(0)=1 f'(0)=-1,Let $f:\mathbb{R}\to\mathbb{R}$ be a function satisfying the conditions $$\begin{gather} f\left(\frac{x+y}{2}\right)=\frac{f(x)+f(y)}{2} \tag{1}\\ f(0)=1 \tag{2}\\ f'(0)=-1 \tag{3} \end{gather}$$ Find the value of $f(2)$ by proper explanation.,Let be a function satisfying the conditions Find the value of by proper explanation.,"f:\mathbb{R}\to\mathbb{R} \begin{gather}
f\left(\frac{x+y}{2}\right)=\frac{f(x)+f(y)}{2} \tag{1}\\
f(0)=1 \tag{2}\\
f'(0)=-1 \tag{3}
\end{gather} f(2)","['analysis', 'derivatives', 'functional-equations']"
26,"Prove that if $f(x+y)=f(x)f(y)$ and f is continuous in $ x=0$, then it is continuous in all its domain","Prove that if  and f is continuous in , then it is continuous in all its domain",f(x+y)=f(x)f(y)  x=0,"Prove that, for $f:\Bbb{R} \rightarrow \Bbb{R}$,   if $f(x+y)=f(x)f(y)$ and f is continuous at $x=0,$ then it is continuous in all $\Bbb{R}$. I haven't figured out how to prove this. What would you suggest? I've already tried finding and expression for $f(0)$ which is either $0$ or $1$.","Prove that, for $f:\Bbb{R} \rightarrow \Bbb{R}$,   if $f(x+y)=f(x)f(y)$ and f is continuous at $x=0,$ then it is continuous in all $\Bbb{R}$. I haven't figured out how to prove this. What would you suggest? I've already tried finding and expression for $f(0)$ which is either $0$ or $1$.",,"['real-analysis', 'analysis']"
27,Why such function does not exist?,Why such function does not exist?,,"I could not prove the following: A function $f \in \mathscr{C}^2([0, \pi])$ , such that $$f(0) = f(\pi) = 0,\\ \int_0^{\pi} (f'(x))^2dx = 1,\\ \text{and }\int_0^{\pi} (f(x))^2dx = 2$$ Then such function does not exist. I think that I have to use the Rayleigh quotient and have a contradiction for the eigenvalue $\frac{1}{2}$ . Thanks in advance.","I could not prove the following: A function , such that Then such function does not exist. I think that I have to use the Rayleigh quotient and have a contradiction for the eigenvalue . Thanks in advance.","f \in \mathscr{C}^2([0, \pi]) f(0) = f(\pi) = 0,\\
\int_0^{\pi} (f'(x))^2dx = 1,\\
\text{and }\int_0^{\pi} (f(x))^2dx = 2 \frac{1}{2}","['analysis', 'partial-differential-equations']"
28,Analysis Problem: Prove $f$ is bounded on $I$,Analysis Problem: Prove  is bounded on,f I,"Let $I=[a,b]$ and let $f:I\to {\mathbb R}$ be a (not necessarily continuous) function with the property that for every $x∈I$, the function $f$ is bounded on a neighborhood $V_{d_x}(x)$ of $x$. Prove that $f$ is bounded on $I$. Thus far I have that, For all $n∈I$ there exist $x_n∈[a,b]$ such that $|f(x_n)|>n$. By the Bolzano Weierstrass theorem since $I$ is bounded we have the sequence $X=(x_n)$ is bounded. This implies there is a convergent sub-sequence $X'=(x_{n_r})$ of $X$ that converges to $c$, $c∈[a,b]$. Since $I$ is closed and the element of $X'$ belongs to $I$, it follows from a previous theorem that I proved that $c∈I$. Here is where I get stuck, I want to use that the function $f$ is bounded on a neighborhood $V_{d_x}(x)$ somehow to show that $f$ is bounded on $I$. I'm not sure how to proceed. $f$ is bounded on $I$ means if there exist a d-neighborhood $V_d(c)$ of $c$ and a constant $M>0$ such that we have $|f(x)|\leq M$ for all $x$ in $A ∩  V_d(c)$. I would like to do try a proof by contradiction somehow.","Let $I=[a,b]$ and let $f:I\to {\mathbb R}$ be a (not necessarily continuous) function with the property that for every $x∈I$, the function $f$ is bounded on a neighborhood $V_{d_x}(x)$ of $x$. Prove that $f$ is bounded on $I$. Thus far I have that, For all $n∈I$ there exist $x_n∈[a,b]$ such that $|f(x_n)|>n$. By the Bolzano Weierstrass theorem since $I$ is bounded we have the sequence $X=(x_n)$ is bounded. This implies there is a convergent sub-sequence $X'=(x_{n_r})$ of $X$ that converges to $c$, $c∈[a,b]$. Since $I$ is closed and the element of $X'$ belongs to $I$, it follows from a previous theorem that I proved that $c∈I$. Here is where I get stuck, I want to use that the function $f$ is bounded on a neighborhood $V_{d_x}(x)$ somehow to show that $f$ is bounded on $I$. I'm not sure how to proceed. $f$ is bounded on $I$ means if there exist a d-neighborhood $V_d(c)$ of $c$ and a constant $M>0$ such that we have $|f(x)|\leq M$ for all $x$ in $A ∩  V_d(c)$. I would like to do try a proof by contradiction somehow.",,['analysis']
29,Proving $\Bigl[\frac{8n+13}{25}\Bigr] - \Bigl[\frac{n-12 - \Bigl[\frac{n-17}{25}\Bigr]}{3}\Bigr]$ is independent of $n$,Proving  is independent of,\Bigl[\frac{8n+13}{25}\Bigr] - \Bigl[\frac{n-12 - \Bigl[\frac{n-17}{25}\Bigr]}{3}\Bigr] n,This an Exercise from Apostol's analytic number theory. I have been struggling with this problem for quite some time. Looks elementary though. The question is to prove that this quantity $$\Biggl[\frac{8n+13}{25}\Biggr] - \Biggl[\frac{n-12 - \Bigl[\frac{n-17}{25}\Bigr]}{3}\Biggr]$$ is independent of $n$.,This an Exercise from Apostol's analytic number theory. I have been struggling with this problem for quite some time. Looks elementary though. The question is to prove that this quantity $$\Biggl[\frac{8n+13}{25}\Biggr] - \Biggl[\frac{n-12 - \Bigl[\frac{n-17}{25}\Bigr]}{3}\Biggr]$$ is independent of $n$.,,['analysis']
30,Imaginary Numbers in the Ring of $p$ Adic Integers?,Imaginary Numbers in the Ring of  Adic Integers?,p,"I am currently reading the book ""ultrametric calculus: an introduction to $p$ adic analysis"" by Schikhof, and I came across this problem: ""Prove that $x^2+1=0$ has no solutions in $\mathbb{Z}_3$ but has two solutions in $\mathbb{Z}_5$ ."" Here, $\mathbb{Z}_p$ denotes the ring of $p$ adic integers.  The issue that I am having is that the book has never given any theorems for irreducibility in $\mathbb{Z}_p$ or any helpful theorems related to finding solutions to polynomials $f(x)\in\mathbb{Z}_p[x]$ up until this point, so I am assuming that there must be some way to do this solely on the construction of $\mathbb{Z}_p$ ? I am unsure on how I could do this, and could use some help as I have completely run out of ideas.  Any help is appreciated.","I am currently reading the book ""ultrametric calculus: an introduction to adic analysis"" by Schikhof, and I came across this problem: ""Prove that has no solutions in but has two solutions in ."" Here, denotes the ring of adic integers.  The issue that I am having is that the book has never given any theorems for irreducibility in or any helpful theorems related to finding solutions to polynomials up until this point, so I am assuming that there must be some way to do this solely on the construction of ? I am unsure on how I could do this, and could use some help as I have completely run out of ideas.  Any help is appreciated.",p x^2+1=0 \mathbb{Z}_3 \mathbb{Z}_5 \mathbb{Z}_p p \mathbb{Z}_p f(x)\in\mathbb{Z}_p[x] \mathbb{Z}_p,"['number-theory', 'analysis', 'p-adic-number-theory']"
31,Nested intervals theorem - a special case on open intervals,Nested intervals theorem - a special case on open intervals,,"I learned the nested intervals theorem in the class: If $I_n\ (n\in\Bbb N)$ is a sequence of bounded closed intervals, i.e. $[a_n,b_n]$ , then $\bigcap I_n\ (n\in\Bbb N)\neq\varnothing$ . In the proof of the theorem, we used another theorem: monotone non-decreasing sequence (in this case $a_n$ ) is convergent, if bounded above. And we discussed in the class that the theorem does not hold for open intervals, i.e. $(a_n,b_n)$ . We made a counterexample $I_n=(0,\frac{1}{n}),\ n\in\Bbb N$ , where $\bigcap I_n=\varnothing$ . It also makes sense to me. There is no real staying in the intersection of nested intervals, because the candidate $0$ is ruled out by the open intervals. Then, the professor gave us an extended question: What if we have nested $I_n$ as open intervals $(a_n,b_n)$ , but this time, let $a_n$ be a strictly increasing sequence ( $\forall n\in\Bbb N,\ a_n\lt a_{n+1}$ ) and $b_n$ a strictly decreasing sequence? It seems to me that there would be a real finally staying in the nested interval. However, I cannot prove it. Appreciated if anyone can provide me a hint. Thanks.","I learned the nested intervals theorem in the class: If is a sequence of bounded closed intervals, i.e. , then . In the proof of the theorem, we used another theorem: monotone non-decreasing sequence (in this case ) is convergent, if bounded above. And we discussed in the class that the theorem does not hold for open intervals, i.e. . We made a counterexample , where . It also makes sense to me. There is no real staying in the intersection of nested intervals, because the candidate is ruled out by the open intervals. Then, the professor gave us an extended question: What if we have nested as open intervals , but this time, let be a strictly increasing sequence ( ) and a strictly decreasing sequence? It seems to me that there would be a real finally staying in the nested interval. However, I cannot prove it. Appreciated if anyone can provide me a hint. Thanks.","I_n\ (n\in\Bbb N) [a_n,b_n] \bigcap I_n\ (n\in\Bbb N)\neq\varnothing a_n (a_n,b_n) I_n=(0,\frac{1}{n}),\ n\in\Bbb N \bigcap I_n=\varnothing 0 I_n (a_n,b_n) a_n \forall n\in\Bbb N,\ a_n\lt a_{n+1} b_n","['real-analysis', 'analysis']"
32,"Which sets are ""clompact""?","Which sets are ""clompact""?",,"This is an exercise taken verbatim from Abbott's Understanding Analysis : Let’s call a set clompact if it has the property that every   closed cover (i.e., a cover consisting of closed sets) admits a finite subcover.   Describe all of the clompact subsets of $\mathbf R$ . I am unable to fully resolve the problem. So far, I have been able to see that singleton sets are always clompact, because the single element must be in at least one set belonging to the closed cover, and that one set is a sufficient finite subcover. The null set is also clompact for obvious reasons. I know that every non-singleton interval (regardless of if they are open, closed, or half-open) is not clompact. As an example, the closed cover $$ \{0\}\cup\bigcup_1^\infty\left[\frac1{n+1},\frac1n\right] $$ for $[0,1]$ does not have a finite subcover. Similar constructions of closed covers show that $[a,b]$ , $(a,b]$ , $[b,a)$ and $(a,b)$ are not clompact as well. In addition, $\mathbf R$ itself and any unbounded interval is also not clompact. Is anyone able to help in solving this problem? Any assistance is appreciated.","This is an exercise taken verbatim from Abbott's Understanding Analysis : Let’s call a set clompact if it has the property that every   closed cover (i.e., a cover consisting of closed sets) admits a finite subcover.   Describe all of the clompact subsets of . I am unable to fully resolve the problem. So far, I have been able to see that singleton sets are always clompact, because the single element must be in at least one set belonging to the closed cover, and that one set is a sufficient finite subcover. The null set is also clompact for obvious reasons. I know that every non-singleton interval (regardless of if they are open, closed, or half-open) is not clompact. As an example, the closed cover for does not have a finite subcover. Similar constructions of closed covers show that , , and are not clompact as well. In addition, itself and any unbounded interval is also not clompact. Is anyone able to help in solving this problem? Any assistance is appreciated.","\mathbf R  \{0\}\cup\bigcup_1^\infty\left[\frac1{n+1},\frac1n\right]  [0,1] [a,b] (a,b] [b,a) (a,b) \mathbf R","['real-analysis', 'analysis']"
33,"Proof that $\frac{a}{a+3b+3c}+\frac{b}{b+3a+3c}+\frac{c}{c+3a+3b} \ge \frac{3}{7}$ for all $a,b,c > 0$",Proof that  for all,"\frac{a}{a+3b+3c}+\frac{b}{b+3a+3c}+\frac{c}{c+3a+3b} \ge \frac{3}{7} a,b,c > 0","So I am trtying to proof that $\frac{a}{a+3b+3c}+\frac{b}{b+3a+3c}+\frac{c}{c+3a+3b} \ge \frac{3}{7}$ for all  $a,b,c > 0$. First I tried with Cauchy–Schwarz inequality but got nowhere. Now I am trying to find a convex function, so I can use jensen's inequality, but I can't come up with one which works.. Has anyone an idea?","So I am trtying to proof that $\frac{a}{a+3b+3c}+\frac{b}{b+3a+3c}+\frac{c}{c+3a+3b} \ge \frac{3}{7}$ for all  $a,b,c > 0$. First I tried with Cauchy–Schwarz inequality but got nowhere. Now I am trying to find a convex function, so I can use jensen's inequality, but I can't come up with one which works.. Has anyone an idea?",,"['analysis', 'inequality', 'lagrange-multiplier', 'sum-of-squares-method', 'tangent-line-method']"
34,Infinite product involving primes,Infinite product involving primes,,"I just had my first analysis course as an undergraduate, and I'm trying to learn more about analytic number theory. Right now I'm looking at prime numbers in particular--I'm studying (mostly just thinking and scribbling) on my own during my winter break. I've come across an infinite product that I'm trying to evaluate: I'm fairly sure it converges to 0, but I have no idea of how to prove this, since it is a product over the primes and, so far as I know, we have very few techniques for analyzing the behavior of the prime numbers. I haven't learned how to use LaTex yet, but the infinite product is simple: it is the product over all primes of (p-1)/p. It seems intuitive to me that the limit of the partial products is 0, but I have no idea how to prove it, mostly because I don't know how to analyze the behavior of the primes in this setting. Any help would be very much appreciated. Thanks.","I just had my first analysis course as an undergraduate, and I'm trying to learn more about analytic number theory. Right now I'm looking at prime numbers in particular--I'm studying (mostly just thinking and scribbling) on my own during my winter break. I've come across an infinite product that I'm trying to evaluate: I'm fairly sure it converges to 0, but I have no idea of how to prove this, since it is a product over the primes and, so far as I know, we have very few techniques for analyzing the behavior of the prime numbers. I haven't learned how to use LaTex yet, but the infinite product is simple: it is the product over all primes of (p-1)/p. It seems intuitive to me that the limit of the partial products is 0, but I have no idea how to prove it, mostly because I don't know how to analyze the behavior of the primes in this setting. Any help would be very much appreciated. Thanks.",,"['analysis', 'prime-numbers', 'infinite-product']"
35,"Finding a unique continuous $g$ on $[0,1]$ satisfying $g(x) = \frac{1}{2}g\left(\frac{x+1}{2}\right) + f(x)$ for given $f$",Finding a unique continuous  on  satisfying  for given,"g [0,1] g(x) = \frac{1}{2}g\left(\frac{x+1}{2}\right) + f(x) f","Let $f$ be a given continuous function on $[0,1]$. How do you prove that there is a unique continuous function $g$ on $[0,1]$ satisfying $$g(x) = \frac{1}{2}g\left(\frac{x+1}{2}\right) + f(x)$$ for all $x\in[0,1]$?","Let $f$ be a given continuous function on $[0,1]$. How do you prove that there is a unique continuous function $g$ on $[0,1]$ satisfying $$g(x) = \frac{1}{2}g\left(\frac{x+1}{2}\right) + f(x)$$ for all $x\in[0,1]$?",,"['real-analysis', 'analysis', 'functions', 'continuity', 'functional-equations']"
36,"What does ""almost everywhere"" mean in convergence almost everywhere?","What does ""almost everywhere"" mean in convergence almost everywhere?",,"Here is a very intuitive definition of convergence almost anywhere from ProofWiki: Sequence of function $(f_n)_{n \in \mathbb N}$ is said to converge almost everywhere on $ D$ to $ f$ if and only if $$\mu (\{x \in D \mid f_n(x) \text { does not converge to } f(x)\})=0$$ and we write it as $$f_n \xrightarrow {a.e.} f. $$ Intuitively, it means that the measure of the set, containing all the $x$'s that do not make $f_n$ converge to $ f $, is zero. But here comes my question: How does the adjective ""almost everywhere"" fit in?  Am I correct in saying that the adjective ""almost everywhere"" means there is a few being left out because the way $\mu $ is defined? Thank you for your time and effort.","Here is a very intuitive definition of convergence almost anywhere from ProofWiki: Sequence of function $(f_n)_{n \in \mathbb N}$ is said to converge almost everywhere on $ D$ to $ f$ if and only if $$\mu (\{x \in D \mid f_n(x) \text { does not converge to } f(x)\})=0$$ and we write it as $$f_n \xrightarrow {a.e.} f. $$ Intuitively, it means that the measure of the set, containing all the $x$'s that do not make $f_n$ converge to $ f $, is zero. But here comes my question: How does the adjective ""almost everywhere"" fit in?  Am I correct in saying that the adjective ""almost everywhere"" means there is a few being left out because the way $\mu $ is defined? Thank you for your time and effort.",,"['real-analysis', 'analysis', 'measure-theory']"
37,"Prove $f(x) = 0$ for all $x \in [0, \infty)$ when $|f'(x)| \leq |f(x)|$",Prove  for all  when,"f(x) = 0 x \in [0, \infty) |f'(x)| \leq |f(x)|","mathematicians! I want to ask to all wise people about a problem I met at the quiz to obtain some ideas. The problem is following. It may not be accurate since the problem is dependent on my memory Let $f : [0, \infty) \rightarrow \mathbb{R}$ and $f$ is differentiable. $|f'(x) |\; \leq\  |f(x)|$ for all $x \in [0, \infty)$. $f(0) = 0$. Then prove that $f(x) = 0$ for all $x \in [0,\infty)$. At the quiz, I applied the Mean Value Theorem and the Cauchy-Schwarz inequality. For any $x$, $|f(x)| = |\{f(x) - f(0)\}(x-0)| = |f'(c_{1})(x)|$ for some $c_{1} \in (0,x)$, and then  $$|f'(c_{1})(x)| \leq |f'(c_{1})||x| \leq |f(c_{1})||x|$$ by Cauchy-Schwarz and given assumption. By doing so consecutively, we can obtain the inequality that  $$ |f(x)| \leq |f(c_{n})||x||c_{1}|\ldots |c_{n-1}| $$ for any $x \in [0,\infty)$. As $\{c_{i}\}, \; i\in \mathbb{N}$ is decreasing sequence and $f$ is continuous, $|f(c_{n})|$ can be arbitrary close to $0$ when $c_{n} \rightarrow 0$. Thus the inequality above can be bounded by $\epsilon$. I think my answer is right, but a bit uncertain. Could you give me a certainty? Thank you very much.","mathematicians! I want to ask to all wise people about a problem I met at the quiz to obtain some ideas. The problem is following. It may not be accurate since the problem is dependent on my memory Let $f : [0, \infty) \rightarrow \mathbb{R}$ and $f$ is differentiable. $|f'(x) |\; \leq\  |f(x)|$ for all $x \in [0, \infty)$. $f(0) = 0$. Then prove that $f(x) = 0$ for all $x \in [0,\infty)$. At the quiz, I applied the Mean Value Theorem and the Cauchy-Schwarz inequality. For any $x$, $|f(x)| = |\{f(x) - f(0)\}(x-0)| = |f'(c_{1})(x)|$ for some $c_{1} \in (0,x)$, and then  $$|f'(c_{1})(x)| \leq |f'(c_{1})||x| \leq |f(c_{1})||x|$$ by Cauchy-Schwarz and given assumption. By doing so consecutively, we can obtain the inequality that  $$ |f(x)| \leq |f(c_{n})||x||c_{1}|\ldots |c_{n-1}| $$ for any $x \in [0,\infty)$. As $\{c_{i}\}, \; i\in \mathbb{N}$ is decreasing sequence and $f$ is continuous, $|f(c_{n})|$ can be arbitrary close to $0$ when $c_{n} \rightarrow 0$. Thus the inequality above can be bounded by $\epsilon$. I think my answer is right, but a bit uncertain. Could you give me a certainty? Thank you very much.",,"['analysis', 'continuity']"
38,Differentiability of $s=\sum\limits_{-\infty}^{\infty} {1\over (x-n)^2}$,Differentiability of,s=\sum\limits_{-\infty}^{\infty} {1\over (x-n)^2},"How do I show that $s=\sum\limits_{-\infty}^{\infty} {1\over (x-n)^2}$ on $x\not\in \mathbb Z$ is differentiable without using its compact form? I realize that the sequence of sums $s_a=\sum\limits_{-a}^{a} {1\over (x-n)^2}$ is not uniformly convergent. I also tried to prove that it is continuous by using the usual $\varepsilon \over 3$ method. And it seems to apply because each $s_a$ is continuous and they converge pointwise to $s$. But then I realized that this should not be the right proof because I didn't use any special property of the given functions and the general case only works for uniform convergence. I am very confused. Please help! Actually I do realize that if I could prove differentiability, continuity follows. Thanks.","How do I show that $s=\sum\limits_{-\infty}^{\infty} {1\over (x-n)^2}$ on $x\not\in \mathbb Z$ is differentiable without using its compact form? I realize that the sequence of sums $s_a=\sum\limits_{-a}^{a} {1\over (x-n)^2}$ is not uniformly convergent. I also tried to prove that it is continuous by using the usual $\varepsilon \over 3$ method. And it seems to apply because each $s_a$ is continuous and they converge pointwise to $s$. But then I realized that this should not be the right proof because I didn't use any special property of the given functions and the general case only works for uniform convergence. I am very confused. Please help! Actually I do realize that if I could prove differentiability, continuity follows. Thanks.",,"['real-analysis', 'analysis']"
39,Domain of the Gamma function,Domain of the Gamma function,,"I need to find the domain of the Gamma function, that is to say all $z \in \mathbb{C}$, for which the integral: $$\Gamma(z) = \int_0^\infty t^{z-1} e^{-t} \mathrm dt$$ converges. I started by splitting up the integral into an integral running from $0$ to $1$ and another one from $1$ to $\infty$. I first tried to figure out for what $z \in \mathbb{C}$ the integral from $0$ to $1$ converges and I came to the conclusion, that $\Re(z) > 0$ is the condition. The other integral, I believe, converges for every $z$, as the exponential function dominates the monomial eventually. So I concluded: $$\exists \Gamma(z) \iff \Re(z) > 0$$ However, I just learned that this is wrong. I found out that the integral only diverges for non-positive integers. What did I do wrong or what is a better way to find the domain of the Gamma function?","I need to find the domain of the Gamma function, that is to say all $z \in \mathbb{C}$, for which the integral: $$\Gamma(z) = \int_0^\infty t^{z-1} e^{-t} \mathrm dt$$ converges. I started by splitting up the integral into an integral running from $0$ to $1$ and another one from $1$ to $\infty$. I first tried to figure out for what $z \in \mathbb{C}$ the integral from $0$ to $1$ converges and I came to the conclusion, that $\Re(z) > 0$ is the condition. The other integral, I believe, converges for every $z$, as the exponential function dominates the monomial eventually. So I concluded: $$\exists \Gamma(z) \iff \Re(z) > 0$$ However, I just learned that this is wrong. I found out that the integral only diverges for non-positive integers. What did I do wrong or what is a better way to find the domain of the Gamma function?",,"['analysis', 'special-functions', 'gamma-function']"
40,An Identity Concerning the Riemann Zeta Function,An Identity Concerning the Riemann Zeta Function,,"Let $\zeta$ be the Riemann- Zeta function. For any integer, $n \geq 2$, how to prove $$\zeta(2) \zeta(2n-2) + \zeta(4)\zeta(2n-4) + \cdots + \zeta(2n-2)\zeta(2) = \Bigl(n + \frac{1}{2}\Bigr)\zeta(2n)$$","Let $\zeta$ be the Riemann- Zeta function. For any integer, $n \geq 2$, how to prove $$\zeta(2) \zeta(2n-2) + \zeta(4)\zeta(2n-4) + \cdots + \zeta(2n-2)\zeta(2) = \Bigl(n + \frac{1}{2}\Bigr)\zeta(2n)$$",,['number-theory']
41,Prove that $\left|\frac{x}{1+x^2}\right| \leq \frac{1}{2}$,Prove that,\left|\frac{x}{1+x^2}\right| \leq \frac{1}{2},"Prove that $$\left|\frac{x}{1+x^2}\right| \leq \frac{1}{2}$$ for any number $x$ . My attempt: $$\left|\frac{x}{1+x^2}\right| \leq \frac{1}{2} \\ \iff \frac{x}{1+x^2} \geq -\frac{1}{2} \land \frac{x}{1+x^2} \leq \frac{1}{2}$$ $$\iff (x+1)^2 \geq 0 \land (x-1)^2 \geq 0$$ the last two inequalities are obviously true, which concludes my proof attempt. Not sure if this is a correct way to prove the inequality, also it's clearly not very elegant. Could someone please verify my solution, and maybe suggest a more elegant or efficient approach?","Prove that for any number . My attempt: the last two inequalities are obviously true, which concludes my proof attempt. Not sure if this is a correct way to prove the inequality, also it's clearly not very elegant. Could someone please verify my solution, and maybe suggest a more elegant or efficient approach?",\left|\frac{x}{1+x^2}\right| \leq \frac{1}{2} x \left|\frac{x}{1+x^2}\right| \leq \frac{1}{2} \\ \iff \frac{x}{1+x^2} \geq -\frac{1}{2} \land \frac{x}{1+x^2} \leq \frac{1}{2} \iff (x+1)^2 \geq 0 \land (x-1)^2 \geq 0,"['analysis', 'inequality', 'solution-verification', 'proof-writing']"
42,$L^p$ space question,space question,L^p,"Assume $(X,\mathcal{M},\mu)$ is a measure space and for some $1\leq p<\infty$, $1\leq q<\infty$, $L^p(\mu)\subset L^q(\mu)$.  Prove there is a constant $C>0$ so that $\|f\|_q\leq C\|f\|_p$ for all $f\in L^p(\mu)$. I need help getting started on this.","Assume $(X,\mathcal{M},\mu)$ is a measure space and for some $1\leq p<\infty$, $1\leq q<\infty$, $L^p(\mu)\subset L^q(\mu)$.  Prove there is a constant $C>0$ so that $\|f\|_q\leq C\|f\|_p$ for all $f\in L^p(\mu)$. I need help getting started on this.",,"['analysis', 'measure-theory', 'banach-spaces']"
43,Why do we define functions over $open$ subsets of $\mathbb{R}^n$?,Why do we define functions over  subsets of ?,open \mathbb{R}^n,"Oftentimes we see functions defined on open sets; I never gave this much thought, but now I'd like to get a better understanding. Why do we define functions over open subsets of $\mathbb{R}^n$? ""Why"" is not meant to be a deep philosophical question; what I mean is, for example, When is it important to define a function on an open set (and when do we not care whether the set is open or not)? What's the motivation for doing so, i.e. what is it that we're going to use about an open set? Does using an open set (as opposed to, say, its closure) make anything more convenient or more manageable? I realize this is quite a general question, and that I didn't give any context; however, I'm hoping that the principles behind defining functions on open sets are ""universal"" enough that the generality and lack of specific examples won't be too problematic.","Oftentimes we see functions defined on open sets; I never gave this much thought, but now I'd like to get a better understanding. Why do we define functions over open subsets of $\mathbb{R}^n$? ""Why"" is not meant to be a deep philosophical question; what I mean is, for example, When is it important to define a function on an open set (and when do we not care whether the set is open or not)? What's the motivation for doing so, i.e. what is it that we're going to use about an open set? Does using an open set (as opposed to, say, its closure) make anything more convenient or more manageable? I realize this is quite a general question, and that I didn't give any context; however, I'm hoping that the principles behind defining functions on open sets are ""universal"" enough that the generality and lack of specific examples won't be too problematic.",,"['analysis', 'multivariable-calculus']"
44,Why is the change-to-polar-coordinate method valid in computing limits?,Why is the change-to-polar-coordinate method valid in computing limits?,,"For example, when we compute $\displaystyle\lim_{(x,y)\to(0,0)}\frac{x^2y}{x^2+y^2}$, we can use the change to polar coordinates method below: Let $\begin{cases}x=r\cos\theta\\y=r\sin\theta \end{cases}$, then   $\displaystyle\lim_{(x,y)\to(0,0)}\frac{x^2y}{x^2+y^2}=\lim_{r\to0}\frac{r^3\cos^3\theta\sin\theta}{r^2}(=\lim_{r\to0}r\cos^2\theta\sin\theta=0)$. However, why is this method valid? What is the actual reason behind this substitution? Is this related to the limit law of composition functions of continuous function? But what are those two functions that applying behind here? I think it makes use of this change-of-coordinate continuous and one-to-one function $\Psi$: $$\begin{alignat*}{3}\Psi:\ &\mathbb{R}^+\times[0,2\pi)&&\to\mathbb{R}^2\\ &(r,\theta)&&\mapsto(r\cos\theta,r\sin\theta)\end{alignat*}$$ But I can't figure out how to ""chain"" this function with others and get the eager result.","For example, when we compute $\displaystyle\lim_{(x,y)\to(0,0)}\frac{x^2y}{x^2+y^2}$, we can use the change to polar coordinates method below: Let $\begin{cases}x=r\cos\theta\\y=r\sin\theta \end{cases}$, then   $\displaystyle\lim_{(x,y)\to(0,0)}\frac{x^2y}{x^2+y^2}=\lim_{r\to0}\frac{r^3\cos^3\theta\sin\theta}{r^2}(=\lim_{r\to0}r\cos^2\theta\sin\theta=0)$. However, why is this method valid? What is the actual reason behind this substitution? Is this related to the limit law of composition functions of continuous function? But what are those two functions that applying behind here? I think it makes use of this change-of-coordinate continuous and one-to-one function $\Psi$: $$\begin{alignat*}{3}\Psi:\ &\mathbb{R}^+\times[0,2\pi)&&\to\mathbb{R}^2\\ &(r,\theta)&&\mapsto(r\cos\theta,r\sin\theta)\end{alignat*}$$ But I can't figure out how to ""chain"" this function with others and get the eager result.",,"['real-analysis', 'analysis', 'continuity', 'polar-coordinates']"
45,Cartesian Product of Borel Sets is Borel Again,Cartesian Product of Borel Sets is Borel Again,,"Let $E$ and $F$ be Borel measurable subsets of $\mathbb R^{d_1}$ and $\mathbb R^{d_2}$, respectively. Then $E \times F$ is also Borel measurable in $\mathbb R^{d_1 + d_2}$. I suppose it is necessary to show that elements of a generator of Borel $\sigma$-algebra on $\mathbb R^{d_1 + d_2}$ is Borel. That is, show that rectangles, i.e., Cartesian product of intervals, are Borel. It seems that I need to use Fubini' theorem. But I do not know how. Any help, please? Thank you!","Let $E$ and $F$ be Borel measurable subsets of $\mathbb R^{d_1}$ and $\mathbb R^{d_2}$, respectively. Then $E \times F$ is also Borel measurable in $\mathbb R^{d_1 + d_2}$. I suppose it is necessary to show that elements of a generator of Borel $\sigma$-algebra on $\mathbb R^{d_1 + d_2}$ is Borel. That is, show that rectangles, i.e., Cartesian product of intervals, are Borel. It seems that I need to use Fubini' theorem. But I do not know how. Any help, please? Thank you!",,"['analysis', 'measure-theory', 'self-learning', 'lebesgue-measure']"
46,convolution of compactly supported continuous function with schwartz class function is again a Schwartz class function?,convolution of compactly supported continuous function with schwartz class function is again a Schwartz class function?,,"Suppose $f$ is continuous function on $\mathbb R$ with compact support; and $g\in \mathcal{S}(\mathbb R),$ (Schwartz space) My Question is : Can we expect $f\ast g \in \mathcal{S(\mathbb R)}$ ? (Bit roughly speaking, convolution of compactly supported continuous function with schwartz class function is again a Schwartz class function ) [We note that convolution of arbitrary continuous function with schwartz class function need not be Schwartz class function, for instance, see this question , ] Thanks,","Suppose $f$ is continuous function on $\mathbb R$ with compact support; and $g\in \mathcal{S}(\mathbb R),$ (Schwartz space) My Question is : Can we expect $f\ast g \in \mathcal{S(\mathbb R)}$ ? (Bit roughly speaking, convolution of compactly supported continuous function with schwartz class function is again a Schwartz class function ) [We note that convolution of arbitrary continuous function with schwartz class function need not be Schwartz class function, for instance, see this question , ] Thanks,",,"['real-analysis', 'analysis', 'fourier-analysis', 'convolution']"
47,A function continuous on all irrational points,A function continuous on all irrational points,,"Let $h:[0,1]\to\mathbb R$ $h(x)=\begin{cases}0&\text{if }x=1\\\frac{1}{n}& \text{otherwise if }x\in\mathbb Q,x=\frac{m}{n},\;m,n\in\mathbb N,\gcd(m,n)=1\\0&\text{otherwise if }x\in\mathbb R\setminus\mathbb Q\end{cases}$ How do you prove that $h$ is continuous on all irrational points within $[0,1]$ ?",Let How do you prove that is continuous on all irrational points within ?,"h:[0,1]\to\mathbb R h(x)=\begin{cases}0&\text{if }x=1\\\frac{1}{n}& \text{otherwise if }x\in\mathbb Q,x=\frac{m}{n},\;m,n\in\mathbb N,\gcd(m,n)=1\\0&\text{otherwise if }x\in\mathbb R\setminus\mathbb Q\end{cases} h [0,1]","['real-analysis', 'analysis', 'functions']"
48,Rudin Theorem 2.47 - Connected Sets in $\mathbb{R}$,Rudin Theorem 2.47 - Connected Sets in,\mathbb{R},"I need help with the proof of the converse, as given by Rudin in Principles of Mathematical Analysis , to the following theorem: Theorem 2.47 : A subset $E$ of the real line $\mathbb{R}^1$ is connected if and only if it has the following property: If $x \in E$ , $y \in E$ , and $x < z < y$ then $z \in E$ . Proof : To prove the converse suppose that $E$ is not connected. Then there are nonempty separated sets $A \text { and } B$ such that $A \cup B = E$ . Pick $x \in A, y \in B$ and assume (without loss of generality) $x < y$ . Define $$z = \sup(A \cap [x,y])$$ By Theorem 2.28, $z \in \overline{A}$ ; hence $z \not\in B$ . In particular, $x \leq z < y.$ If $z \not \in A$ , it follows that $x < z < y$ , and $z \not\in E$ . If $z \in A$ , then $z \not\in \overline{B}$ , hence there exists $z_1$ such that $z < z_1 < y \text { and } z_1 \not\in B$ . Then $x < z_1 < y$ and $z_1 \not \in E$ . $_\Box$ What I need help with: i) I need help understanding what $z$ is and if there is any relationship to $\sup(A)$ . I initially thought they were equal, but I constructed an example where they weren't. ii) I need help understanding why Theorem 2.28 implies that $z \in \overline{A}$ . It is my understanding that Theorem 2.28 implies that $z \in \overline{A \cap [x,y]}$ . I initially thought that $\overline{A \cap [x,y]} = \overline{A} \cap \overline{[x,y]}$ , but I've come to realize that this isn't generally the case and therefore I'm having troubles seeing the implication. iii) I suppose that I should be able to understand the rest of the proof if I understand i) and ii), but any further added detail would be greatly appreciated. Thank-you. For reference: Definition : Two subsets $A$ and $B$ of a metric space $X$ are said to be separated if $\overline{A} \cap B = \emptyset = A \cap \overline{B}$ . Definition : A set $E \subset X$ is said to be connected if $E$ is not a union of two nonempty separated sets. Theorem 2.28 : Let $E$ be a nonempty set of real numbers which is bounded above. Let $y = \sup(E)$ . Then $y \in \overline{E}$ . Hence $y \in E$ if $E$ is closed.","I need help with the proof of the converse, as given by Rudin in Principles of Mathematical Analysis , to the following theorem: Theorem 2.47 : A subset of the real line is connected if and only if it has the following property: If , , and then . Proof : To prove the converse suppose that is not connected. Then there are nonempty separated sets such that . Pick and assume (without loss of generality) . Define By Theorem 2.28, ; hence . In particular, If , it follows that , and . If , then , hence there exists such that . Then and . What I need help with: i) I need help understanding what is and if there is any relationship to . I initially thought they were equal, but I constructed an example where they weren't. ii) I need help understanding why Theorem 2.28 implies that . It is my understanding that Theorem 2.28 implies that . I initially thought that , but I've come to realize that this isn't generally the case and therefore I'm having troubles seeing the implication. iii) I suppose that I should be able to understand the rest of the proof if I understand i) and ii), but any further added detail would be greatly appreciated. Thank-you. For reference: Definition : Two subsets and of a metric space are said to be separated if . Definition : A set is said to be connected if is not a union of two nonempty separated sets. Theorem 2.28 : Let be a nonempty set of real numbers which is bounded above. Let . Then . Hence if is closed.","E \mathbb{R}^1 x \in E y \in E x < z < y z \in E E A \text { and } B A \cup B = E x \in A, y \in B x < y z = \sup(A \cap [x,y]) z \in \overline{A} z \not\in B x \leq z < y. z \not \in A x < z < y z \not\in E z \in A z \not\in \overline{B} z_1 z < z_1 < y \text { and } z_1 \not\in B x < z_1 < y z_1 \not \in E _\Box z \sup(A) z \in \overline{A} z \in \overline{A \cap [x,y]} \overline{A \cap [x,y]} = \overline{A} \cap \overline{[x,y]} A B X \overline{A} \cap B = \emptyset = A \cap \overline{B} E \subset X E E y = \sup(E) y \in \overline{E} y \in E E",[]
49,"If $f:[a,b]\to \mathbb{R}$ is continuous and nonnegative and $\int_a^b{f}=0$, then $f(x)=0$ for all $x\in [a,b]$","If  is continuous and nonnegative and , then  for all","f:[a,b]\to \mathbb{R} \int_a^b{f}=0 f(x)=0 x\in [a,b]","Suppose that a function $f:[a,b]\to\mathbb{R}$ is continuous and nonnegative. Prove that if $\int_a^b{f}=0$, then $f(x)=0$ for all $x\in [a,b]$. I've been trying to prove it using the extreme value theorem, continuity and the upper and lower sums but can't come up with something tight enough. More specifically, I've been trying to relate $|f(x)-f(t)|<\epsilon$ to the sum of $M_k-m_k(x_k-x_{k-1})$","Suppose that a function $f:[a,b]\to\mathbb{R}$ is continuous and nonnegative. Prove that if $\int_a^b{f}=0$, then $f(x)=0$ for all $x\in [a,b]$. I've been trying to prove it using the extreme value theorem, continuity and the upper and lower sums but can't come up with something tight enough. More specifically, I've been trying to relate $|f(x)-f(t)|<\epsilon$ to the sum of $M_k-m_k(x_k-x_{k-1})$",,"['real-analysis', 'analysis']"
50,Prove $|f(x)-p_{3}(x)| \leq \frac{1}{384} \max\limits_{0 \leq x \leq 1}|f^{(4)}(x)|$,Prove,|f(x)-p_{3}(x)| \leq \frac{1}{384} \max\limits_{0 \leq x \leq 1}|f^{(4)}(x)|,"Given $f(x)\in C^4[0,1]$ , and a polynomial $p_3(x)$ of degree $3$ s.t. $p_3(0)=0,p_3'(0)=f'(0),p_3(1)=f(1),p_3'(1)=f'(1)$ . Prove that, $\left|f(x)-p_{3}(x)\right| \leq \dfrac{1}{384} \max\limits_{0 \leq x \leq 1}\left|f^{(4)}(x)\right|$ . I thought it may relate to Simpson's rule , but it seems to be not the case. Also I thought it may relate to Orthogonal Projection on a Polynomial Space , but I don't know how to proceed either. Can anyone help? And, are there any further suggestions on these kind of problems(i.e. given a function, and a polynomial whose values and derivates are related to the function, then estimate)?","Given , and a polynomial of degree s.t. . Prove that, . I thought it may relate to Simpson's rule , but it seems to be not the case. Also I thought it may relate to Orthogonal Projection on a Polynomial Space , but I don't know how to proceed either. Can anyone help? And, are there any further suggestions on these kind of problems(i.e. given a function, and a polynomial whose values and derivates are related to the function, then estimate)?","f(x)\in C^4[0,1] p_3(x) 3 p_3(0)=0,p_3'(0)=f'(0),p_3(1)=f(1),p_3'(1)=f'(1) \left|f(x)-p_{3}(x)\right| \leq \dfrac{1}{384} \max\limits_{0 \leq x \leq 1}\left|f^{(4)}(x)\right|","['analysis', 'derivatives', 'inequality', 'polynomials', 'numerical-methods']"
51,Minimum of a sum of positive functions is the sum of the minimums of the functions,Minimum of a sum of positive functions is the sum of the minimums of the functions,,"Let $f_1, \dots, f_n$ be positive functions from $\mathbb R^m \rightarrow \mathbb R$ . How do we show that $$\min_x \sum_{i=1}^n f_i(x) = \sum_{i=1}^n \min_x f_i(x)$$ Actually, I am not sure this is true. Maybe adding convexity of the functions helps ?","Let be positive functions from . How do we show that Actually, I am not sure this is true. Maybe adding convexity of the functions helps ?","f_1, \dots, f_n \mathbb R^m \rightarrow \mathbb R \min_x \sum_{i=1}^n f_i(x) = \sum_{i=1}^n \min_x f_i(x)","['real-analysis', 'analysis', 'optimization']"
52,"In a metric space, is every convergent sequence bounded?","In a metric space, is every convergent sequence bounded?",,"In $\mathbb{R}$ and $\mathbb{R}^p$, this is true, but is it true in every metric space? I suppose not, but what other condition would I have to put on the metric space in order for it to have this property? For clarity, a bounded subset $W$ of a metric space $V,d$ is a subset of $W$ with this property: $$\exists M\in\mathbb{R}_0^+,\forall v,w \in V:\ d(v,w) < M$$","In $\mathbb{R}$ and $\mathbb{R}^p$, this is true, but is it true in every metric space? I suppose not, but what other condition would I have to put on the metric space in order for it to have this property? For clarity, a bounded subset $W$ of a metric space $V,d$ is a subset of $W$ with this property: $$\exists M\in\mathbb{R}_0^+,\forall v,w \in V:\ d(v,w) < M$$",,"['analysis', 'metric-spaces']"
53,Proving the chain rule by first principles,Proving the chain rule by first principles,,"I'm currently trying to prove: $(f(g))'(a)=f'(g(a))*g'(a)$ I have been given a proof which manipulates: $f(a+h)=f(a)+f'(a)h+O(h)$ where $O(h)$ is the error function. However, I would like to have a proof in terms of the standard limit definition of $(1/h)*(f(a+h)-f(a) \to f'(a)$ as $h \to 0$ Thanks!","I'm currently trying to prove: $(f(g))'(a)=f'(g(a))*g'(a)$ I have been given a proof which manipulates: $f(a+h)=f(a)+f'(a)h+O(h)$ where $O(h)$ is the error function. However, I would like to have a proof in terms of the standard limit definition of $(1/h)*(f(a+h)-f(a) \to f'(a)$ as $h \to 0$ Thanks!",,"['analysis', 'functions', 'derivatives', 'proof-writing']"
54,"If $f$ is twice differentiable and $f(2^{-n}) = 0 $, for all $n \in \mathbb N$, then $f^\prime(0) = f^{\prime\prime}(0) = 0$.","If  is twice differentiable and , for all , then .",f f(2^{-n}) = 0  n \in \mathbb N f^\prime(0) = f^{\prime\prime}(0) = 0,"Let $f : \mathbb R \to \mathbb R$ be a twice differentiable function, such that $f(2^{-n}) = 0$, for all $n \in \mathbb N$ . Show that $$f^\prime(0) = f^{\prime\prime}(0) = 0.$$ My attempt. First, let us show that $f(0) = 0$ . Since $f$ is twice differentiable, it is also continuous. Suppose $f(0) = k$ where $k \ne 0$ . Then there is a $\delta > 0$ such that whenever $x \in (-\delta,\delta)$ , $f(x) \in (k/2,3k/2)$ . But $f(2^{-n}) = 0 $ for all positive integer values of $n$ . Thus we have a contradiction as no $\delta$ works for $f$ to be continuous at $x=0$ . Thus $f(x) = 0$. $f(x)$ is differentiable , thus $f^\prime(x)$ is continuous. Suppose $f^\prime (0) = m$ such that $m\neq0$ , then by continuity of $f^\prime(x)$ there is a $\delta$ such that whenever $x \in (-\delta,\delta)$ , $f^\prime(x) \in (m/2,3m/2)$ . Choose an $N$ such that  $2^{-N} < \delta$  . Now, $\int_{x=0}^{1/2^N} f^\prime(x) = f(2^{-N}) - f(0) = f(2^{-N})$ (I am confused whether this integral exists or not ,as in if $f^\prime(x)$ is integrable or not in the interval I am using) . Now $\int_{x=0}^{1/2^N} f^\prime(x) > m/2(1/2^N)$ as the integral would be greater than the rectangle formed using a lower bound for the value of $f$ in the interval. Thus  $\int_{x=0}^{2^{-N}} f^\prime(x) > 0$ and thus $f(2^{-N}) > 0$ but this is a contradiction as $f(2^{-N}) = 0$ by the function definition. Thus we have a contradiction and thus, $f^\prime(0)$ can't be non-zero. We can proceed similarly to prove that $f^{\prime\prime}(0) = 0$ . Is my approach correct, is there a better way to prove it ?  I am not sure if my solution is correct as I am not sure of using integrals the way I used it. Thanks.","Let $f : \mathbb R \to \mathbb R$ be a twice differentiable function, such that $f(2^{-n}) = 0$, for all $n \in \mathbb N$ . Show that $$f^\prime(0) = f^{\prime\prime}(0) = 0.$$ My attempt. First, let us show that $f(0) = 0$ . Since $f$ is twice differentiable, it is also continuous. Suppose $f(0) = k$ where $k \ne 0$ . Then there is a $\delta > 0$ such that whenever $x \in (-\delta,\delta)$ , $f(x) \in (k/2,3k/2)$ . But $f(2^{-n}) = 0 $ for all positive integer values of $n$ . Thus we have a contradiction as no $\delta$ works for $f$ to be continuous at $x=0$ . Thus $f(x) = 0$. $f(x)$ is differentiable , thus $f^\prime(x)$ is continuous. Suppose $f^\prime (0) = m$ such that $m\neq0$ , then by continuity of $f^\prime(x)$ there is a $\delta$ such that whenever $x \in (-\delta,\delta)$ , $f^\prime(x) \in (m/2,3m/2)$ . Choose an $N$ such that  $2^{-N} < \delta$  . Now, $\int_{x=0}^{1/2^N} f^\prime(x) = f(2^{-N}) - f(0) = f(2^{-N})$ (I am confused whether this integral exists or not ,as in if $f^\prime(x)$ is integrable or not in the interval I am using) . Now $\int_{x=0}^{1/2^N} f^\prime(x) > m/2(1/2^N)$ as the integral would be greater than the rectangle formed using a lower bound for the value of $f$ in the interval. Thus  $\int_{x=0}^{2^{-N}} f^\prime(x) > 0$ and thus $f(2^{-N}) > 0$ but this is a contradiction as $f(2^{-N}) = 0$ by the function definition. Thus we have a contradiction and thus, $f^\prime(0)$ can't be non-zero. We can proceed similarly to prove that $f^{\prime\prime}(0) = 0$ . Is my approach correct, is there a better way to prove it ?  I am not sure if my solution is correct as I am not sure of using integrals the way I used it. Thanks.",,"['calculus', 'real-analysis', 'analysis', 'derivatives', 'continuity']"
55,Introduction to Pseudodifferential operators,Introduction to Pseudodifferential operators,,"I'm interested in elementary introduction to pseduodifferential operators and its application to hyperbolic PDE's. I know measure theory, Fourier analysis and some elementary(linear) hyperbolic PDE's but not functional analysis, distributions, Sobolev spaces,etc. Can you recommend suitable intro text? Thanks","I'm interested in elementary introduction to pseduodifferential operators and its application to hyperbolic PDE's. I know measure theory, Fourier analysis and some elementary(linear) hyperbolic PDE's but not functional analysis, distributions, Sobolev spaces,etc. Can you recommend suitable intro text? Thanks",,"['analysis', 'reference-request', 'partial-differential-equations']"
56,An inequality of $\int_0^1 |f(x)|dx$,An inequality of,\int_0^1 |f(x)|dx,"Question: If $f\in C^1[0,1]$, show that $$\int_0^1 |f(x)|dx\le\max\left\{\int_0^1 |f'(x)|\,dx,\;\bigg|\int_0^1 f(x)\,dx\bigg|\right\}.$$ I have tried to make connection between $|f|$ and $|f'|$ by using $$(tf(t))'=f(t)+tf'(t),$$ integrate the equation with $t$ on $[0,1]$, it gets $$|f(1)|\leq \bigg|\int_0^1 f(t)\,dt\bigg|+\int_0^1 |f'(t)|\,dt,$$ but it is not the desired inequality.","Question: If $f\in C^1[0,1]$, show that $$\int_0^1 |f(x)|dx\le\max\left\{\int_0^1 |f'(x)|\,dx,\;\bigg|\int_0^1 f(x)\,dx\bigg|\right\}.$$ I have tried to make connection between $|f|$ and $|f'|$ by using $$(tf(t))'=f(t)+tf'(t),$$ integrate the equation with $t$ on $[0,1]$, it gets $$|f(1)|\leq \bigg|\int_0^1 f(t)\,dt\bigg|+\int_0^1 |f'(t)|\,dt,$$ but it is not the desired inequality.",,['analysis']
57,Why doesn't MVT hold when I take the limit as $x\to 0$?,Why doesn't MVT hold when I take the limit as ?,x\to 0,"I'm considering $f(x) = x^2\sin(1/x)$ for $x\ne 0$ , and $f(0)=0$ . MVT gives me $(f(x)-f(0))/ x = x \sin(1/x) = f'(c) = 2c \sin(1/c) - \cos(1/c)$ for some $c$ between $x$ and $0$ . Taking the limit as $x$ goes to $0$ gives me $0 = \lim_{c\to 0} \cos(1/c)$ which doesn't make sense. What did I do wrong?","I'm considering for , and . MVT gives me for some between and . Taking the limit as goes to gives me which doesn't make sense. What did I do wrong?",f(x) = x^2\sin(1/x) x\ne 0 f(0)=0 (f(x)-f(0))/ x = x \sin(1/x) = f'(c) = 2c \sin(1/c) - \cos(1/c) c x 0 x 0 0 = \lim_{c\to 0} \cos(1/c),"['real-analysis', 'calculus']"
58,Walter Rudin's proof: countable union of countable sets is countable,Walter Rudin's proof: countable union of countable sets is countable,,"The capture is from Rudin's Principles of Mathematical Analysis , and I've seen similar proof for this theorem but with a different technique. It uses a single arrow to throw all the elements, the arrow can wiggle and turn around, this kind of proof I could understand. Q1 : When I see Rudin's proof, I was confused about how those arrows have bijections with $\mathbb{N}$ . It said arrange in sequence, it seems that the sequence's terms are increasing(e.g $2$ nd term is a two-element tuple, $3$ rd term is a three-element tuple, etc.) can a sequence have terms for different types?","The capture is from Rudin's Principles of Mathematical Analysis , and I've seen similar proof for this theorem but with a different technique. It uses a single arrow to throw all the elements, the arrow can wiggle and turn around, this kind of proof I could understand. Q1 : When I see Rudin's proof, I was confused about how those arrows have bijections with . It said arrange in sequence, it seems that the sequence's terms are increasing(e.g nd term is a two-element tuple, rd term is a three-element tuple, etc.) can a sequence have terms for different types?",\mathbb{N} 2 3,"['real-analysis', 'analysis', 'proof-explanation']"
59,Are there non-trivial examples of an 'infinitieth' derivative? What is the criteria for 'convergence'?,Are there non-trivial examples of an 'infinitieth' derivative? What is the criteria for 'convergence'?,,"Let me preface this by saying that 'convergence' refers to $\lim_{n\to\infty}\frac{d^nf(x)}{dx^n}$ being well defined. This stands in contrast to functions which, despite having derivatives of all orders, do not approach any particular value as each successive derivative is taken (e.g. $\cos{x}$ or $1/x$ ). There are two trivial cases for the 'infinitieth' derivative - namely $ke^{x+c}$ , for which $\frac{d^\infty f(x)}{dx^\infty}=ke^{x+c}$ , and $\frac{d^\infty f(x)}{dx^\infty}=0$ , which is the case for any function with some constant $n^\text{th}$ derivative (e.g. power/polynomial functions). Intuitively, I can think of a few reasons why there wouldn't be any other examples, but then there might be some remarkable special function which defies all intuition. Are there any non-trivial examples of functions (real or complex) where the 'infinitieth' derivative exists?","Let me preface this by saying that 'convergence' refers to being well defined. This stands in contrast to functions which, despite having derivatives of all orders, do not approach any particular value as each successive derivative is taken (e.g. or ). There are two trivial cases for the 'infinitieth' derivative - namely , for which , and , which is the case for any function with some constant derivative (e.g. power/polynomial functions). Intuitively, I can think of a few reasons why there wouldn't be any other examples, but then there might be some remarkable special function which defies all intuition. Are there any non-trivial examples of functions (real or complex) where the 'infinitieth' derivative exists?",\lim_{n\to\infty}\frac{d^nf(x)}{dx^n} \cos{x} 1/x ke^{x+c} \frac{d^\infty f(x)}{dx^\infty}=ke^{x+c} \frac{d^\infty f(x)}{dx^\infty}=0 n^\text{th},"['analysis', 'derivatives']"
60,Uses of step functions,Uses of step functions,,"My highschool teacher has informally told us about what continuity is and used step functions as an example of a discontinuous function. The Wikipedia page for it links to a lot of other kind of step functions, such as the Heaviside step function. What are some uses of these step functions to mathematicians at higher levels (or even in highschool)?","My highschool teacher has informally told us about what continuity is and used step functions as an example of a discontinuous function. The Wikipedia page for it links to a lot of other kind of step functions, such as the Heaviside step function. What are some uses of these step functions to mathematicians at higher levels (or even in highschool)?",,"['real-analysis', 'analysis', 'functions', 'continuity']"
61,area-preserving iff $|\det |=+1$,area-preserving iff,|\det |=+1,"Why is a (not necessarily linear) mapping $f:\mathbb{R}^n\rightarrow \mathbb{R}^n$ area- and orientation preserving iff the determinant of its jacobian is $\pm 1$ ? (I understand by an area-preserving mapping $f$ a mapping $f$ such that the measure $m(f^{-1}(A))=m(A)$, where $m(\cdot)$ denotes the measure of a measurable set $A$.) I have no idea how to prove this... but I'd also be happy with a reference. (I'd also be happy for a sketch of the proof for a less general definition of ""area-preserving"", where $A$ is not just any measurable set, but a polytope - this definition would work easier with the concept of determinant since the volumen of a polytope is just the absolute value of the determinant of the vector that represent it's edges.) Googling didn't get me anything.","Why is a (not necessarily linear) mapping $f:\mathbb{R}^n\rightarrow \mathbb{R}^n$ area- and orientation preserving iff the determinant of its jacobian is $\pm 1$ ? (I understand by an area-preserving mapping $f$ a mapping $f$ such that the measure $m(f^{-1}(A))=m(A)$, where $m(\cdot)$ denotes the measure of a measurable set $A$.) I have no idea how to prove this... but I'd also be happy with a reference. (I'd also be happy for a sketch of the proof for a less general definition of ""area-preserving"", where $A$ is not just any measurable set, but a polytope - this definition would work easier with the concept of determinant since the volumen of a polytope is just the absolute value of the determinant of the vector that represent it's edges.) Googling didn't get me anything.",,['analysis']
62,Discontinuous Differentiable and One to One,Discontinuous Differentiable and One to One,,"If the derivative of a function (from $\mathbb{R} \rightarrow \mathbb{R}$) at a point $x_0$ is discontinuous, does that imply that the function is not one to one or injective in a neighborhood of $x_0$? If not, how does one go in showing that the function is not injective at $x_0$ given that the derivative of the function is discontinuous at $x_0$. My thoughts: Since the function's derivative is not always positive or negative, then the function is not injective because it is neither always decreasing or always increasing. Does that sound right?","If the derivative of a function (from $\mathbb{R} \rightarrow \mathbb{R}$) at a point $x_0$ is discontinuous, does that imply that the function is not one to one or injective in a neighborhood of $x_0$? If not, how does one go in showing that the function is not injective at $x_0$ given that the derivative of the function is discontinuous at $x_0$. My thoughts: Since the function's derivative is not always positive or negative, then the function is not injective because it is neither always decreasing or always increasing. Does that sound right?",,"['analysis', 'derivatives', 'continuity']"
63,How find this sum $\frac{1}{1^2}+\frac{2}{2^2}+\frac{2}{3^2}+\frac{3}{4^2}+\frac{2}{5^2}+\frac{4}{6^2}+\cdots+\frac{d(n)}{n^2}+\cdots$,How find this sum,\frac{1}{1^2}+\frac{2}{2^2}+\frac{2}{3^2}+\frac{3}{4^2}+\frac{2}{5^2}+\frac{4}{6^2}+\cdots+\frac{d(n)}{n^2}+\cdots,"Question: Find the value   $$\dfrac{1}{1^2}+\dfrac{2}{2^2}+\dfrac{2}{3^2}+\dfrac{3}{4^2}+\dfrac{2}{5^2}+\dfrac{4}{6^2}+\cdots+\dfrac{d(n)}{n^2}+\cdots$$ where $d(n)$ is The total number of positive divisors of $n$ I think we can use  $$\zeta{(2)}=\dfrac{1}{1^2}+\dfrac{1}{2^2}+\cdots+\dfrac{1}{n^2}+\cdots=\dfrac{\pi^2}{6}$$ I know If the prime factorization of  is given by $$n=p^{a_{1}}_{1}\cdot p^{a_{2}}_{2}\cdots p^{a_{n}}_{n}$$ then the number of positive divisors of  is $$d(n)=(a_{1}+1)(a_{2}+1)\cdots(a_{n}+1)$$ But follow is very ugly,I don't understand @Nate idea(my English is poor),can you post detail? Thank you","Question: Find the value   $$\dfrac{1}{1^2}+\dfrac{2}{2^2}+\dfrac{2}{3^2}+\dfrac{3}{4^2}+\dfrac{2}{5^2}+\dfrac{4}{6^2}+\cdots+\dfrac{d(n)}{n^2}+\cdots$$ where $d(n)$ is The total number of positive divisors of $n$ I think we can use  $$\zeta{(2)}=\dfrac{1}{1^2}+\dfrac{1}{2^2}+\cdots+\dfrac{1}{n^2}+\cdots=\dfrac{\pi^2}{6}$$ I know If the prime factorization of  is given by $$n=p^{a_{1}}_{1}\cdot p^{a_{2}}_{2}\cdots p^{a_{n}}_{n}$$ then the number of positive divisors of  is $$d(n)=(a_{1}+1)(a_{2}+1)\cdots(a_{n}+1)$$ But follow is very ugly,I don't understand @Nate idea(my English is poor),can you post detail? Thank you",,"['analysis', 'summation']"
64,Can a self-adjoint operator have no eigenvalues?,Can a self-adjoint operator have no eigenvalues?,,"I have a self-adjoint operator $d$ which acts on vector fields defined on $\mathbb{R}^n$. I am interested on its eigenvalues. That is, I study the equation $d(X)-\lambda X=0$. I have found that if $\lambda\neq 0$ then $X=0$ is the only solution of $d(X)-\lambda X=0$. This would mean that if $\lambda$ is an eigenvalue, then it must be zero. However, I have also shown that the kernel of $d$ is trivial. This then implies, i think, that $d$ has no eigenvalues! Is this really possible? or did something go wrong? thanks. Edit: It is a bit difficult to write the specific operator $d$ in a brief way, but let me try for $\mathbb{R}^3$. Let $S$ the vector field $S=z\partial_x-(y^2+x)\partial_y+0\partial_z$. We define the operator $f$ (acting on polynomial vector fields of degree $k$) as $f(U)=[S,U]$. Note that $f$ is not self adjoint as it maps polynomial vector fields of degree $k$ to polynomial vector fields of degree $k+1$ . Let $f^*$ be the adjoint of $f$ (there is a well defined inner product in the space of polynomial vector fields that allows us to do this). Then $d=ff^*$. As a side note, I've seen here that integral operators may have no eigenvalues.","I have a self-adjoint operator $d$ which acts on vector fields defined on $\mathbb{R}^n$. I am interested on its eigenvalues. That is, I study the equation $d(X)-\lambda X=0$. I have found that if $\lambda\neq 0$ then $X=0$ is the only solution of $d(X)-\lambda X=0$. This would mean that if $\lambda$ is an eigenvalue, then it must be zero. However, I have also shown that the kernel of $d$ is trivial. This then implies, i think, that $d$ has no eigenvalues! Is this really possible? or did something go wrong? thanks. Edit: It is a bit difficult to write the specific operator $d$ in a brief way, but let me try for $\mathbb{R}^3$. Let $S$ the vector field $S=z\partial_x-(y^2+x)\partial_y+0\partial_z$. We define the operator $f$ (acting on polynomial vector fields of degree $k$) as $f(U)=[S,U]$. Note that $f$ is not self adjoint as it maps polynomial vector fields of degree $k$ to polynomial vector fields of degree $k+1$ . Let $f^*$ be the adjoint of $f$ (there is a well defined inner product in the space of polynomial vector fields that allows us to do this). Then $d=ff^*$. As a side note, I've seen here that integral operators may have no eigenvalues.",,"['analysis', 'eigenvalues-eigenvectors', 'operator-theory', 'adjoint-operators']"
65,$\sigma$-finite measure and semi-finite measure,-finite measure and semi-finite measure,\sigma,"Let $ (X, \Sigma, \mu)  $ it will be a space with measure. $\mu$ is $\sigma$ -finite measure if it exist sequence of sets $X_{i} \in \Sigma $ and $\cup_{i=1}^{\infty}X_{i}=X$ and $\mu(X_{i})<\infty$ for all i $\mu$ is semi-finite measure if for all $G \in \Sigma $ and $\mu (G)=\infty$ it exist $H \in \Sigma$ and $H \subset G$ and $0<\mu(H)<\infty$ Show that if $\mu$ is $\sigma$ -finite measure then $\mu$ is semi-finite measure",Let it will be a space with measure. is -finite measure if it exist sequence of sets and and for all i is semi-finite measure if for all and it exist and and Show that if is -finite measure then is semi-finite measure," (X, \Sigma, \mu)   \mu \sigma X_{i} \in \Sigma  \cup_{i=1}^{\infty}X_{i}=X \mu(X_{i})<\infty \mu G \in \Sigma  \mu (G)=\infty H \in \Sigma H \subset G 0<\mu(H)<\infty \mu \sigma \mu","['real-analysis', 'analysis', 'measure-theory']"
66,Nodes of eigenfunctions and Courant's nodal domain theorem,Nodes of eigenfunctions and Courant's nodal domain theorem,,"I am looking for a reference for properties of eigenfunctions of the Laplacian (on the Euclidean plane, and maybe also Laplace-Beltrami on a general manifold): The discreteness of the set of eigenvalues, Nodes of eigenfunctions, Courant's nodal domain theorem, The Faber-Krahn inequality, and other related results. I have tried Methods of Mathematical Physics (Courant, Hilbert) but it contains only some of the above, is quite old and a bit hard to read.","I am looking for a reference for properties of eigenfunctions of the Laplacian (on the Euclidean plane, and maybe also Laplace-Beltrami on a general manifold): The discreteness of the set of eigenvalues, Nodes of eigenfunctions, Courant's nodal domain theorem, The Faber-Krahn inequality, and other related results. I have tried Methods of Mathematical Physics (Courant, Hilbert) but it contains only some of the above, is quite old and a bit hard to read.",,"['analysis', 'reference-request', 'partial-differential-equations']"
67,For all but finitely many $n \in \mathbb N$,For all but finitely many,n \in \mathbb N,"In my book I have the following theorem: A sequence $\langle a_n \rangle$ converges to a real number $A$ if and only if every neighborhood of $A$ contains $a_n$ for all but finitely many $n \in \mathbb N$. Can anyone clarify what the phrase, ""All but finitely many"", means?","In my book I have the following theorem: A sequence $\langle a_n \rangle$ converges to a real number $A$ if and only if every neighborhood of $A$ contains $a_n$ for all but finitely many $n \in \mathbb N$. Can anyone clarify what the phrase, ""All but finitely many"", means?",,"['real-analysis', 'analysis', 'notation', 'definition']"
68,Power series representation/calculation,Power series representation/calculation,,"I am struggling a bit with power series at the moment, and I don't quite understand what this question is asking me to do? Am I meant to form a power series from these, or simply evaluate that series? Any explanation/working is appreciated. Using power series representation, calculate $$\sum_{n=1}^\infty \frac{n2^n}{3^n}.$$","I am struggling a bit with power series at the moment, and I don't quite understand what this question is asking me to do? Am I meant to form a power series from these, or simply evaluate that series? Any explanation/working is appreciated. Using power series representation, calculate $$\sum_{n=1}^\infty \frac{n2^n}{3^n}.$$",,"['real-analysis', 'analysis']"
69,"If a function is Frechet differentiable, does the Frechet derivative equal the Gateaux derivative?","If a function is Frechet differentiable, does the Frechet derivative equal the Gateaux derivative?",,"If a function is Frechet differentiable, does the Frechet derivative equal the Gateaux derivative?","If a function is Frechet differentiable, does the Frechet derivative equal the Gateaux derivative?",,"['calculus', 'analysis', 'derivatives']"
70,"Is it true in general that $\int \dots \int_{0 \le x_1 \le \dots \le x_n,\ 0 \le x_n\le1}dx_1\dots dx_n=\left(\frac{1}{2}\right)^n?$",Is it true in general that,"\int \dots \int_{0 \le x_1 \le \dots \le x_n,\ 0 \le x_n\le1}dx_1\dots dx_n=\left(\frac{1}{2}\right)^n?","I was looking at an example with the following integral: $$\iiiint_{0 \le x \le y \le z \le t,\ 0 \le t \le \frac{1}{2}} 1 \,dx\,dy\,dz\,dt = \frac{1}{16}$$  Is it true in general that  $$\int \dots \int_{0 \le x_1 \le \dots \le x_n,\ 0 \le x_n\le1}dx_1\dots dx_n=\left(\frac{1}{2}\right)^n?$$ (edit: here $0 \le x_n \le 1$ but in the example $0 \le t \le \frac{1}{2}$ so this can't be true...). Intuitively it would seem so since each dimension is ""cut in half"" by each inequality. Are there some other results for multiple integrals with this domain that have other integrand that is not a constant? Is it true that $$\int \dots \int_{0 \le x_1 \le \dots \le x_n,\ a \le x_n\le b}f(\boldsymbol{x})\,dx_1\dots dx_n=\int_a^b \int_0^{x_{n-1}} \int_0^{x_{n-2}} \dots \int_0^{x_2}f(\boldsymbol{x})\,dx_1\dots dx_n$$","I was looking at an example with the following integral: $$\iiiint_{0 \le x \le y \le z \le t,\ 0 \le t \le \frac{1}{2}} 1 \,dx\,dy\,dz\,dt = \frac{1}{16}$$  Is it true in general that  $$\int \dots \int_{0 \le x_1 \le \dots \le x_n,\ 0 \le x_n\le1}dx_1\dots dx_n=\left(\frac{1}{2}\right)^n?$$ (edit: here $0 \le x_n \le 1$ but in the example $0 \le t \le \frac{1}{2}$ so this can't be true...). Intuitively it would seem so since each dimension is ""cut in half"" by each inequality. Are there some other results for multiple integrals with this domain that have other integrand that is not a constant? Is it true that $$\int \dots \int_{0 \le x_1 \le \dots \le x_n,\ a \le x_n\le b}f(\boldsymbol{x})\,dx_1\dots dx_n=\int_a^b \int_0^{x_{n-1}} \int_0^{x_{n-2}} \dots \int_0^{x_2}f(\boldsymbol{x})\,dx_1\dots dx_n$$",,"['calculus', 'analysis', 'multivariable-calculus', 'definite-integrals']"
71,Prove or disprove: if $f$ and $fg$ are continuous then $g$ is continuous.,Prove or disprove: if  and  are continuous then  is continuous.,f fg g,"Prove of provide a counterexample: Suppose that $f$ and $g$ are defined and finite valued on an open interval $I$ which contains $a$, that $f$ is continuous at $a$, and that $f(a)\neq 0$. Then $g$ is continuous at $a$ if and only if $fg$ is continuous at $a$. I don't suppose it's true, based on the fact that the common theorem '$f, g$ continuous implies $fg$ continuous' is not stated as true both ways; obviously, this implies exceptions. The only ones I can think of, however, are one's that don't fit the ""open interval"" or ""$f(a)\neq 0$"" parts, or ones where both f and g are discontinuous. I've also tried proving it, but with no luck. Help? :-S","Prove of provide a counterexample: Suppose that $f$ and $g$ are defined and finite valued on an open interval $I$ which contains $a$, that $f$ is continuous at $a$, and that $f(a)\neq 0$. Then $g$ is continuous at $a$ if and only if $fg$ is continuous at $a$. I don't suppose it's true, based on the fact that the common theorem '$f, g$ continuous implies $fg$ continuous' is not stated as true both ways; obviously, this implies exceptions. The only ones I can think of, however, are one's that don't fit the ""open interval"" or ""$f(a)\neq 0$"" parts, or ones where both f and g are discontinuous. I've also tried proving it, but with no luck. Help? :-S",,"['calculus', 'analysis']"
72,Show that the linear operator $(Tf)(x)=\frac{1}{\pi} \int_0^{\infty} \frac{f(y)}{(x+y)} dy$ satisfies $\|T\|\leq 1$.,Show that the linear operator  satisfies .,(Tf)(x)=\frac{1}{\pi} \int_0^{\infty} \frac{f(y)}{(x+y)} dy \|T\|\leq 1,"Show that the linear operator $T$ given by $(Tf)(x) = \frac{1}{\pi} \int_0^{\infty} \frac{f(y)}{(x+y)} dy$ is bounded on $L^2(0, \infty)$ with norm $||T|| \leq 1$. The professor also wrote, $``$In fact, you should observe that $||T|| = 1$, where $$||T|| = \sup\limits_{f\neq 0}\frac{||Tf||_2}{||f||_2}.""$$","Show that the linear operator $T$ given by $(Tf)(x) = \frac{1}{\pi} \int_0^{\infty} \frac{f(y)}{(x+y)} dy$ is bounded on $L^2(0, \infty)$ with norm $||T|| \leq 1$. The professor also wrote, $``$In fact, you should observe that $||T|| = 1$, where $$||T|| = \sup\limits_{f\neq 0}\frac{||Tf||_2}{||f||_2}.""$$",,"['real-analysis', 'analysis']"
73,proof of Poisson formula by T. Tao,proof of Poisson formula by T. Tao,,"I do not understand one thing in an article on the blog of Terence Tao: For instance, restricting a function $f: G \rightarrow \mathbb{C}$ to   a subgroup $H$ causes the Fourier transform $\hat f$ to be averaged   along the dual group $\widehat{H}$. In particular, restricting a   function $f: \mathbb{R} \rightarrow \mathbb{C}$ to the integers (and   renormalising it to become the measure $\sum_{n \in \mathbb{Z}} f(n) \delta_n$)   causes the Fourier transform $\hat f: \mathbb{R} \rightarrow \mathbb{C}$   to become summed over the dual group $\mathbb{Z}^\perp = \mathbb{Z}$   to become the function $\sum_{m \in \mathbb{Z}} \hat f(\cdot+m)$.   In particular, the zero Fourier coefficient of $\sum_{n \in \mathbb{Z}} f(n) \delta_n$ is $\sum_{m \in \mathbb{Z}} \hat f(m)$. The thing that I do not understand is ""to become the function $\sum_{m \in \mathbb{Z}} \hat f(\cdot+m)$."" Could someone give an explanation of this point?","I do not understand one thing in an article on the blog of Terence Tao: For instance, restricting a function $f: G \rightarrow \mathbb{C}$ to   a subgroup $H$ causes the Fourier transform $\hat f$ to be averaged   along the dual group $\widehat{H}$. In particular, restricting a   function $f: \mathbb{R} \rightarrow \mathbb{C}$ to the integers (and   renormalising it to become the measure $\sum_{n \in \mathbb{Z}} f(n) \delta_n$)   causes the Fourier transform $\hat f: \mathbb{R} \rightarrow \mathbb{C}$   to become summed over the dual group $\mathbb{Z}^\perp = \mathbb{Z}$   to become the function $\sum_{m \in \mathbb{Z}} \hat f(\cdot+m)$.   In particular, the zero Fourier coefficient of $\sum_{n \in \mathbb{Z}} f(n) \delta_n$ is $\sum_{m \in \mathbb{Z}} \hat f(m)$. The thing that I do not understand is ""to become the function $\sum_{m \in \mathbb{Z}} \hat f(\cdot+m)$."" Could someone give an explanation of this point?",,"['analysis', 'fourier-analysis', 'intuition', 'fourier-series']"
74,Uniformly bounded sequence of Riemann integrable functions,Uniformly bounded sequence of Riemann integrable functions,,"Let { $f_n$ } be a uniformly bounded sequence of Riemann int'ble functions on $[a,b]$ .If $f_n\rightarrow 0$ pointwise then does it follow that $\int _{[a,b]}f_n\rightarrow0$ ? My thoughts: The result doesn't follow from the given assumptions. To justify my claim, I choose $f_n(x)=\frac{x^2}{x^2+(1-nx)^2}$ on $[0,1]$ which satisfies all the criteria. Clearly, $f_n\rightarrow 0$ pointwise but I haven't been able to show that $\int _{[a,b]}f_n$ doesn't converge to $0$ although it's clear that it doesn't. Are there any other counter-examples to justify this result?. I came up with $f_n(x)=nx(1-x^2)^n$ on $[0,1]$ but this choice of function doesn't have the uniform boundedness. Can anybody provide me with a relatively easy example to go with?","Let { } be a uniformly bounded sequence of Riemann int'ble functions on .If pointwise then does it follow that ? My thoughts: The result doesn't follow from the given assumptions. To justify my claim, I choose on which satisfies all the criteria. Clearly, pointwise but I haven't been able to show that doesn't converge to although it's clear that it doesn't. Are there any other counter-examples to justify this result?. I came up with on but this choice of function doesn't have the uniform boundedness. Can anybody provide me with a relatively easy example to go with?","f_n [a,b] f_n\rightarrow 0 \int _{[a,b]}f_n\rightarrow0 f_n(x)=\frac{x^2}{x^2+(1-nx)^2} [0,1] f_n\rightarrow 0 \int _{[a,b]}f_n 0 f_n(x)=nx(1-x^2)^n [0,1]","['real-analysis', 'analysis', 'riemann-integration', 'pointwise-convergence']"
75,Prove that the function $f(x)=\frac{\sin(x^3)}{x}$ is uniformly continuous.,Prove that the function  is uniformly continuous.,f(x)=\frac{\sin(x^3)}{x},"Continuous function in the interval $(0,\infty)$ $f(x)=\frac{\sin(x^3)}{x}$ . To prove that the function is uniformly continuous. The function is clearly continuous. Now $|f(x)-f(y)|=|\frac{\sin(x^3)}{x}-\frac{\sin(y^3)}{y}|\leq |\frac{1}{x}|+|\frac{1}{y}|$ . But I don't think whether this will work. I was trying in the other way, using Lagrange Mean Value theorem so that we can apply any Lipschitz condition or not!! but $f'(x)=3x^2\frac{\cos(x^3)}x-\frac{\sin(x^3)}{x^2}$ Any hint...","Continuous function in the interval . To prove that the function is uniformly continuous. The function is clearly continuous. Now . But I don't think whether this will work. I was trying in the other way, using Lagrange Mean Value theorem so that we can apply any Lipschitz condition or not!! but Any hint...","(0,\infty) f(x)=\frac{\sin(x^3)}{x} |f(x)-f(y)|=|\frac{\sin(x^3)}{x}-\frac{\sin(y^3)}{y}|\leq |\frac{1}{x}|+|\frac{1}{y}| f'(x)=3x^2\frac{\cos(x^3)}x-\frac{\sin(x^3)}{x^2}","['real-analysis', 'calculus', 'analysis', 'continuity', 'uniform-continuity']"
76,Prove the support of a real function is countable,Prove the support of a real function is countable,,"This is a problem from my real analysis homework. We are learning the countable sets, and have yet to reach uncountable sets. Let $f$ be a real function defined on $[0, 1]$ . There exists a constant $M$ , such that for each finite $n$ , and $0 \le x_1 < x_2 < \cdots < x_n \le 1$ , we have $$ |f(x_1) + f(x_2) + \cdots + f(x_n)| \le M. $$ Prove $E \stackrel{\text{def}}{=} \{x \in [0, 1] : f(x) \ne 0\}$ is countable. My attempt I split $E$ into two parts, $$ \begin{align} E^{+} &\stackrel{\text{def}}{=} \{x \in [0, 1] : f(x) > 0\} \\ E^{-} &\stackrel{\text{def}}{=} \{x \in [0, 1] : f(x) < 0\} \end{align} $$ A classmate suggested considering $f(x) > \frac{1}{n}$ and $f(x) \le \frac{1}{n}$ from $E^{+}$ separately, and proving for each $n$ both part have finite amount of elements, but I didn't gain much from her hint. Another path I have taken is letting $$ \begin{align} E' &\stackrel{\text{def}}{=} \{x \in E : \exists \delta_x > 0, (x, x+\delta_x) \cap E = \varnothing \} \\ E'' &\stackrel{\text{def}}{=} E \setminus E' \end{align} $$ I can prove $E'$ is countable, but $E''$ is still tricky even though it contains less or equal elements than $E$ . Proof by contradiction didn't yield any significant result, either.","This is a problem from my real analysis homework. We are learning the countable sets, and have yet to reach uncountable sets. Let be a real function defined on . There exists a constant , such that for each finite , and , we have Prove is countable. My attempt I split into two parts, A classmate suggested considering and from separately, and proving for each both part have finite amount of elements, but I didn't gain much from her hint. Another path I have taken is letting I can prove is countable, but is still tricky even though it contains less or equal elements than . Proof by contradiction didn't yield any significant result, either.","f [0, 1] M n 0 \le x_1 < x_2 < \cdots < x_n \le 1 
|f(x_1) + f(x_2) + \cdots + f(x_n)| \le M.
 E \stackrel{\text{def}}{=} \{x \in [0, 1] : f(x) \ne 0\} E 
\begin{align}
E^{+} &\stackrel{\text{def}}{=} \{x \in [0, 1] : f(x) > 0\} \\
E^{-} &\stackrel{\text{def}}{=} \{x \in [0, 1] : f(x) < 0\}
\end{align}
 f(x) > \frac{1}{n} f(x) \le \frac{1}{n} E^{+} n 
\begin{align}
E' &\stackrel{\text{def}}{=} \{x \in E : \exists \delta_x > 0, (x, x+\delta_x) \cap E = \varnothing \} \\
E'' &\stackrel{\text{def}}{=} E \setminus E'
\end{align}
 E' E'' E","['real-analysis', 'analysis']"
77,"Computing the total variation of a function $f:[0,1] \to \mathbb{R}$ using uniform partitions",Computing the total variation of a function  using uniform partitions,"f:[0,1] \to \mathbb{R}","Let $f:[0,1] \to \mathbb{R}$ be a function of bounded variation and let $$ V_0^1(f) = \sup \{ \ \sum_{i=1}^{N-1} |f(x_i)-f(x_{i+1})| : 0<x_1 < \cdots < x_N < 1 \ \} $$ be its total variation where the supremum runs over all partitions of $[0,1]$ . Can we caculate the $V_0^1(f)$ using uniform partitions? That means we set $$ x_i = \frac{i-1}{N-1}, \ \ i=1, \dots,N $$ and $$ v_N = \sum_{i=1}^{N-1} |f(x_i)-f(x_{i+1})|. $$ Does the sequnce $(v_N)_N$ converges to $V_0^1(f)$ (maybe under stronger conditions such as continuity of $f$ ) ?",Let be a function of bounded variation and let be its total variation where the supremum runs over all partitions of . Can we caculate the using uniform partitions? That means we set and Does the sequnce converges to (maybe under stronger conditions such as continuity of ) ?,"f:[0,1] \to \mathbb{R} 
V_0^1(f) = \sup \{ \ \sum_{i=1}^{N-1} |f(x_i)-f(x_{i+1})| : 0<x_1 < \cdots < x_N < 1 \ \}
 [0,1] V_0^1(f) 
x_i = \frac{i-1}{N-1}, \ \ i=1, \dots,N
 
v_N = \sum_{i=1}^{N-1} |f(x_i)-f(x_{i+1})|.
 (v_N)_N V_0^1(f) f","['real-analysis', 'analysis', 'functions', 'bounded-variation']"
78,Implicit Function Theorem implies the Inverse Function Theorem,Implicit Function Theorem implies the Inverse Function Theorem,,"I want to prove that the implicit function theorem implies the inverse function theorem.  I saw in another post what's written below as the proof for this but I don't understand what they've done. $$ \text{ For } f : \mathbb{R}^n \to \mathbb{R}^n \text{, consider } F:\mathbb{R}^n\times\mathbb{R}^n \to \mathbb{R}^n   \text{ given by } F({\bf x}, {\bf y}) = f({\bf y}) - {\bf x}$$ Do we consider $f(x)$ to be the implicit function satisfying $F\big(x,f(x)\big)=0$ , and by the definition of $F$ we get $F\big(x,f(x)\big)=0=f\big(f(x)\big)-x \Longrightarrow f\big(f(x)\big)=x$.  It seems I was wrong by assuming defining $f$ as the implicit function and I should have just let it be $g$.  So if we instead get the final implcation being $f\big(g(x)\big)=x$ is that proof of the inverse function theorem? Thanks!","I want to prove that the implicit function theorem implies the inverse function theorem.  I saw in another post what's written below as the proof for this but I don't understand what they've done. $$ \text{ For } f : \mathbb{R}^n \to \mathbb{R}^n \text{, consider } F:\mathbb{R}^n\times\mathbb{R}^n \to \mathbb{R}^n   \text{ given by } F({\bf x}, {\bf y}) = f({\bf y}) - {\bf x}$$ Do we consider $f(x)$ to be the implicit function satisfying $F\big(x,f(x)\big)=0$ , and by the definition of $F$ we get $F\big(x,f(x)\big)=0=f\big(f(x)\big)-x \Longrightarrow f\big(f(x)\big)=x$.  It seems I was wrong by assuming defining $f$ as the implicit function and I should have just let it be $g$.  So if we instead get the final implcation being $f\big(g(x)\big)=x$ is that proof of the inverse function theorem? Thanks!",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'proof-explanation']"
79,"Given any function $g(x): \mathbb{R} \rightarrow \mathbb{R}$, find a function $f(x)$ such that $f(f(x)) = g(x)$.","Given any function , find a function  such that .",g(x): \mathbb{R} \rightarrow \mathbb{R} f(x) f(f(x)) = g(x),"This is a problem I was casually discussing with friends: Given any function $g(x): \mathbb{R} \rightarrow \mathbb{R}$, find a   function $f(x): \mathbb{R} \rightarrow \mathbb{R}$ such that $f(f(x)) = g(x)$. Is it possible to find a solution for $f(x)$ or prove that there isn't a solution for $f(x)$ such that $f(f(x)) = g(x)$ for any given $g(x)$? For example, if $g(x) = x + 2$, then $f(x)$ could easily be $f(x) = x + 1$. But if $g(x) = x^2 - 1$, then it's not clear to see what $f$ can satisfy such solution. What approach might be promising? My first gut reaction would be some analytical method but can't think of a way still...Discussions are welcomed!","This is a problem I was casually discussing with friends: Given any function $g(x): \mathbb{R} \rightarrow \mathbb{R}$, find a   function $f(x): \mathbb{R} \rightarrow \mathbb{R}$ such that $f(f(x)) = g(x)$. Is it possible to find a solution for $f(x)$ or prove that there isn't a solution for $f(x)$ such that $f(f(x)) = g(x)$ for any given $g(x)$? For example, if $g(x) = x + 2$, then $f(x)$ could easily be $f(x) = x + 1$. But if $g(x) = x^2 - 1$, then it's not clear to see what $f$ can satisfy such solution. What approach might be promising? My first gut reaction would be some analytical method but can't think of a way still...Discussions are welcomed!",,['analysis']
80,Proving existence of at least one root,Proving existence of at least one root,,"The function $f:\mathbb{R}\to\mathbb{R}$, is continuous and $a>0$. How can I prove that there is at least one root of this equation: $f(x)=f(\sqrt{|x^2-a|})$","The function $f:\mathbb{R}\to\mathbb{R}$, is continuous and $a>0$. How can I prove that there is at least one root of this equation: $f(x)=f(\sqrt{|x^2-a|})$",,"['analysis', 'continuity']"
81,Taylor series of the inverse of $x^4+x$,Taylor series of the inverse of,x^4+x,"I would like to expand the inverse function of $$g(x) := x^4+x $$ in a taylor series at the point x = 0. I calculated the first and second derivate at x = 0 with the rule of  the derivation of an inverse function. Theoretically, this process  could be continued for higher derivates. But I would like to have an easier method to calculate higher derivates of an inverse function in order to calculate the taylor series. Any ideas ?","I would like to expand the inverse function of $$g(x) := x^4+x $$ in a taylor series at the point x = 0. I calculated the first and second derivate at x = 0 with the rule of  the derivation of an inverse function. Theoretically, this process  could be continued for higher derivates. But I would like to have an easier method to calculate higher derivates of an inverse function in order to calculate the taylor series. Any ideas ?",,"['analysis', 'taylor-expansion', 'inverse']"
82,"How to prove that $2\sqrt{a^{ea}b^{eb}}\ge a^{eb}+b^{ea}$ for $a > 0, b > 0$?",How to prove that  for ?,"2\sqrt{a^{ea}b^{eb}}\ge a^{eb}+b^{ea} a > 0, b > 0","Let $a,b\in R^{+}$. Show that $$2a^{\frac{ea}{2}}b^{\frac{eb}{2}}\ge a^{eb}+b^{ea} \>.$$ My attempt I know the following inequality is true: $$a^{ea}+b^{eb}\ge a^{eb}+b^{ea} \>.$$ See this post on AoPS or S. Manyama (2010), Solution of One Conjecture on Inequalities with Power-Exponential Functions , The Australian Journal of Mathematical Analysis and Applications , vol. 7, no. 2. But my problem is much stronger (because $a^2+b^2\ge 2ab$) and I can't solve it.","Let $a,b\in R^{+}$. Show that $$2a^{\frac{ea}{2}}b^{\frac{eb}{2}}\ge a^{eb}+b^{ea} \>.$$ My attempt I know the following inequality is true: $$a^{ea}+b^{eb}\ge a^{eb}+b^{ea} \>.$$ See this post on AoPS or S. Manyama (2010), Solution of One Conjecture on Inequalities with Power-Exponential Functions , The Australian Journal of Mathematical Analysis and Applications , vol. 7, no. 2. But my problem is much stronger (because $a^2+b^2\ge 2ab$) and I can't solve it.",,"['analysis', 'inequality']"
83,$(0 \leq f' \leq f$ on $\mathbb R$ and $f(a)=0)$ $\implies f=0$?,on  and  ?,(0 \leq f' \leq f \mathbb R f(a)=0) \implies f=0,"Given that $0 \leq f' \leq f$  on $\mathbb R$  and $f(a)=0\in\mathbb R$ for some $a\in\mathbb R$, how do I prove that $f$ is identically zero?  Using the mean value theorem naively didn't really get me anywhere.","Given that $0 \leq f' \leq f$  on $\mathbb R$  and $f(a)=0\in\mathbb R$ for some $a\in\mathbb R$, how do I prove that $f$ is identically zero?  Using the mean value theorem naively didn't really get me anywhere.",,"['calculus', 'real-analysis', 'analysis']"
84,Example of the equality of an inequality,Example of the equality of an inequality,,"This question is related to Daniel Fischer's answer here . Suppose $f$ is a real $C^{1}$ function on $[0, 1]$ such that $f(0) = 0$ and $\int_{0}^{1}f'(x)^{2}\, dx \leq 1$. Then (essentially by Cauchy-Schwarz), we have $\left|\int_{0}^{1}f(x)\, dx\right| \leq 2/3$ as Daniel Fischer stated in his answer. My question is: Is there an example of a function such that we have equality in $\left|\int_{0}^{1}f(x)\, dx\right| \leq 2/3$? I was thinking of maybe a smoothed version of a function which takes the value $0$ on $[0, 1/2)$ and $4/3$ on $(1/2, 1]$ but that seems complicated to construct, moreover, the derivative in some neighbourhood of $1/2$ might be hard to control.","This question is related to Daniel Fischer's answer here . Suppose $f$ is a real $C^{1}$ function on $[0, 1]$ such that $f(0) = 0$ and $\int_{0}^{1}f'(x)^{2}\, dx \leq 1$. Then (essentially by Cauchy-Schwarz), we have $\left|\int_{0}^{1}f(x)\, dx\right| \leq 2/3$ as Daniel Fischer stated in his answer. My question is: Is there an example of a function such that we have equality in $\left|\int_{0}^{1}f(x)\, dx\right| \leq 2/3$? I was thinking of maybe a smoothed version of a function which takes the value $0$ on $[0, 1/2)$ and $4/3$ on $(1/2, 1]$ but that seems complicated to construct, moreover, the derivative in some neighbourhood of $1/2$ might be hard to control.",,"['real-analysis', 'analysis']"
85,Prove that $\displaystyle\prod_{q\in \mathbb{Q}^{\times}}|q|=1$,Prove that,\displaystyle\prod_{q\in \mathbb{Q}^{\times}}|q|=1,"Prove that $\displaystyle\prod_{q\in \mathbb{Q}^{\times}}|q|=1$. I don't have a lot of experience working with infinite products, but I read a couple of theorems that say that absolute convergence of infinite products requires that $\prod1+|a_n|$ converges, and that $\prod1+|a_n|$ converges iff $\sum a_n$ converges. Now $\sum_{q\in \mathbb{Q}^{\times}}|q|$ certainly does not converge, implying that my original product is not absolutely convergent.  Which leaves me with the problem of being unable to rearrange its terms.  But since I was never given an enumeration of my rationals to begin with, I'm a bit vexed as to how I should proceed. Here is the link to the problem: homework 2 (problem 2).  I'm not in the class, just doing the homeworks.  I'm doing it for the $\mid \cdot \mid_{\infty}$ absolute value.  Which is supposed to be just the normal absolute value (according to homework 1).","Prove that $\displaystyle\prod_{q\in \mathbb{Q}^{\times}}|q|=1$. I don't have a lot of experience working with infinite products, but I read a couple of theorems that say that absolute convergence of infinite products requires that $\prod1+|a_n|$ converges, and that $\prod1+|a_n|$ converges iff $\sum a_n$ converges. Now $\sum_{q\in \mathbb{Q}^{\times}}|q|$ certainly does not converge, implying that my original product is not absolutely convergent.  Which leaves me with the problem of being unable to rearrange its terms.  But since I was never given an enumeration of my rationals to begin with, I'm a bit vexed as to how I should proceed. Here is the link to the problem: homework 2 (problem 2).  I'm not in the class, just doing the homeworks.  I'm doing it for the $\mid \cdot \mid_{\infty}$ absolute value.  Which is supposed to be just the normal absolute value (according to homework 1).",,"['analysis', 'infinite-product']"
86,Is my Fourier Series computation done correctly?,Is my Fourier Series computation done correctly?,,"See my fourier series calculation of this function if you please! $      f(t)=\left\{\begin{array}{ll} 0, & \text{for } \ -\pi<t<0  \\          1, & \text{for } \ 0 < t < \pi\end{array}\right. . $ I start by calculation of the coefficient $a_0,a_k,b_k$: $$a_0=\frac{1}{\pi} \int\limits_{0}^{\pi}1\mathrm dt=1 ;$$ $$ a_k=\frac{1}{\pi}\int\limits_{0}^{\pi}\sin(kt)\mathrm dt=\frac{1-\cos(\pi k)}{\pi k} = \frac{1-(-1)^{k}}{\pi k};$$ $$ b_k= \frac{1}{\pi}\int\limits_{0}^{\pi}\cos(kt)\mathrm dt = \frac{\sin(\pi t)}{k} = 0$$ We will get: $f(t) \sim \frac{1}{2} + \sum\limits_{k=0}^{\infty}\frac{1-(-1)^{k}}{\pi k}\sin(kt) = \frac{1}{2} + \sum\limits_{n=0}^{\infty}\frac{2}{\pi (2n+1)}\sin((2n+1)t) $ How we can find relationship : $\sum\limits_{n=0}^{\infty}\frac{(-1)^n}{2n+1}=\frac{\pi}{4}$ from this fourier series ?","See my fourier series calculation of this function if you please! $      f(t)=\left\{\begin{array}{ll} 0, & \text{for } \ -\pi<t<0  \\          1, & \text{for } \ 0 < t < \pi\end{array}\right. . $ I start by calculation of the coefficient $a_0,a_k,b_k$: $$a_0=\frac{1}{\pi} \int\limits_{0}^{\pi}1\mathrm dt=1 ;$$ $$ a_k=\frac{1}{\pi}\int\limits_{0}^{\pi}\sin(kt)\mathrm dt=\frac{1-\cos(\pi k)}{\pi k} = \frac{1-(-1)^{k}}{\pi k};$$ $$ b_k= \frac{1}{\pi}\int\limits_{0}^{\pi}\cos(kt)\mathrm dt = \frac{\sin(\pi t)}{k} = 0$$ We will get: $f(t) \sim \frac{1}{2} + \sum\limits_{k=0}^{\infty}\frac{1-(-1)^{k}}{\pi k}\sin(kt) = \frac{1}{2} + \sum\limits_{n=0}^{\infty}\frac{2}{\pi (2n+1)}\sin((2n+1)t) $ How we can find relationship : $\sum\limits_{n=0}^{\infty}\frac{(-1)^n}{2n+1}=\frac{\pi}{4}$ from this fourier series ?",,"['real-analysis', 'analysis', 'fourier-analysis', 'fourier-series']"
87,Why is arctangent smooth?,Why is arctangent smooth?,,"I just read a proof by Spivak that $\arctan(x)$ has the $(2n+1)$-th Taylor polynomial at zero $$x - \frac{x^3}{3} + \cdots + (-1)^n\frac{x^{2n+1}}{2n+1}$$ The proof relied on the assumption that $\arctan(x)$ has $2n +1$ derivatives in order for the Taylor polynomial to exist. Spivak also made the point that the existence of a ""good approximation polynomial"" does not imply the existence of a Taylor polynomial if the function isn't differentiable enough. But he never did prove that arctangent is smooth (at zero at least). Is there a simple way to see this?","I just read a proof by Spivak that $\arctan(x)$ has the $(2n+1)$-th Taylor polynomial at zero $$x - \frac{x^3}{3} + \cdots + (-1)^n\frac{x^{2n+1}}{2n+1}$$ The proof relied on the assumption that $\arctan(x)$ has $2n +1$ derivatives in order for the Taylor polynomial to exist. Spivak also made the point that the existence of a ""good approximation polynomial"" does not imply the existence of a Taylor polynomial if the function isn't differentiable enough. But he never did prove that arctangent is smooth (at zero at least). Is there a simple way to see this?",,['analysis']
88,Prove that $\sin(x+\frac{\pi}{n})$ converges uniformly to $\sin(x)$.,Prove that  converges uniformly to .,\sin(x+\frac{\pi}{n}) \sin(x),"I've just starting learning uniform convergence and understand the formal definition. What I've got so far is: $|\sin(x+ \frac{\pi}{n}) - \sin(x)| < \epsilon \ \ \ \ \forall x \in \mathbb{R} \ \ \ \ $ for $n \geq N, \epsilon>0$ LHS = $|2\cos(x+\frac{\pi}{2n})\cdot \sin(\frac{\pi}{2n})| < \epsilon $ Am I going down the right route here? I've done some examples fine, but when trig is involved on all space, I get confused as to what I should be doing... Any help at all would be VERY much appreciated, I have an analysis exam tomorrow and need to be able to practice this. Thanks.","I've just starting learning uniform convergence and understand the formal definition. What I've got so far is: $|\sin(x+ \frac{\pi}{n}) - \sin(x)| < \epsilon \ \ \ \ \forall x \in \mathbb{R} \ \ \ \ $ for $n \geq N, \epsilon>0$ LHS = $|2\cos(x+\frac{\pi}{2n})\cdot \sin(\frac{\pi}{2n})| < \epsilon $ Am I going down the right route here? I've done some examples fine, but when trig is involved on all space, I get confused as to what I should be doing... Any help at all would be VERY much appreciated, I have an analysis exam tomorrow and need to be able to practice this. Thanks.",,"['real-analysis', 'analysis', 'convergence-divergence']"
89,Proof of uniform continuity,Proof of uniform continuity,,"I seem to have hit a dead-end in the following proof. Define $f:\mathbb{R}\to\mathbb{R}$ by: $f(x)=\frac{1}{1+x^2}$ Show that $f$ is uniformly continuous. My proof: Let $x_{0}\in \mathbb{R}$ . Also let $\epsilon >0$ Choose $\delta = ?$ Then, for $x\in \mathbb{R}$ , such that $|x-x_{0}|<\delta$ , we have: | $f(x)-f(x_{0})|=|\frac{1}{1+x^2}-\frac{1}{1+x_{0}^2}|=|\frac{x_{0}^2-x^2}{(1+x^2)(1+x_{0}^2)}|=\frac{|x_{0}-x||x_{0}+x|}{(1+x^2)(1+x_{0}^2)}\le \delta|x+x_{0}|$ The last line uses the fact that $x^2, x_{0}^2\ge 0$ .  How can I finish the proof?","I seem to have hit a dead-end in the following proof. Define by: Show that is uniformly continuous. My proof: Let . Also let Choose Then, for , such that , we have: | The last line uses the fact that .  How can I finish the proof?","f:\mathbb{R}\to\mathbb{R} f(x)=\frac{1}{1+x^2} f x_{0}\in \mathbb{R} \epsilon >0 \delta = ? x\in \mathbb{R} |x-x_{0}|<\delta f(x)-f(x_{0})|=|\frac{1}{1+x^2}-\frac{1}{1+x_{0}^2}|=|\frac{x_{0}^2-x^2}{(1+x^2)(1+x_{0}^2)}|=\frac{|x_{0}-x||x_{0}+x|}{(1+x^2)(1+x_{0}^2)}\le \delta|x+x_{0}| x^2, x_{0}^2\ge 0",['analysis']
90,About Borel's lemma,About Borel's lemma,,"Borel lemma states that for $x_0 \in \mathbf{R}$ and for a real sequence $(a_n)_{n \in \mathbf{N_0}}$ there exists a smooth function $f: \mathbf{R} \rightarrow \mathbf{R}$ such that $f^{(n)}(x_0)=a_n$ for $n \in \mathbf{N_0}$. However is it true for $x_1, x_2 \in \mathbf{R}, x_1 \neq x_2$, and for real sequences $(a_n)_{n \in \mathbf{N_0}}, (b_n)_{n \in \mathbf{N_0}}$ there exists a smooth function $f: \mathbf{R} \rightarrow \mathbf{R}$ such that $f^{(n)}(x_1)=a_n$, $f^{(n)}(x_2)=a_n$ for $n \in \mathbf{N_0}$  ? Thanks","Borel lemma states that for $x_0 \in \mathbf{R}$ and for a real sequence $(a_n)_{n \in \mathbf{N_0}}$ there exists a smooth function $f: \mathbf{R} \rightarrow \mathbf{R}$ such that $f^{(n)}(x_0)=a_n$ for $n \in \mathbf{N_0}$. However is it true for $x_1, x_2 \in \mathbf{R}, x_1 \neq x_2$, and for real sequences $(a_n)_{n \in \mathbf{N_0}}, (b_n)_{n \in \mathbf{N_0}}$ there exists a smooth function $f: \mathbf{R} \rightarrow \mathbf{R}$ such that $f^{(n)}(x_1)=a_n$, $f^{(n)}(x_2)=a_n$ for $n \in \mathbf{N_0}$  ? Thanks",,['analysis']
91,Find the max of this function,Find the max of this function,,"$$f(x) = x(2+x)e^{-x}$$ I have to find $\max_{\mathbb{Q}} \{ f(x) \}$ but I don't know how to proceed. I started with $f'(x) = (-x^2+2)e^{-x}$ whence the stationary points are given when the $x$ coordinate is $x = \pm \sqrt{2}$ Yet those points do not belong to $\mathbb{Q}$ . I thought like ""I hav to find the closest possible point to $\sqrt{2}$ , but it doesn't exist. I can always find a closer rational number from the one I found (call it $p$ ) and $\sqrt{2}$ . My conclusion would be that $f(x)$ has no maximum in $\mathbb{Q}$ , but it does on $\mathbb{R}$ or if we want to be fancy, it even does on $\mathbb{R}\backslash\mathbb{Q}$ . Please correct my wrongs.","I have to find but I don't know how to proceed. I started with whence the stationary points are given when the coordinate is Yet those points do not belong to . I thought like ""I hav to find the closest possible point to , but it doesn't exist. I can always find a closer rational number from the one I found (call it ) and . My conclusion would be that has no maximum in , but it does on or if we want to be fancy, it even does on . Please correct my wrongs.",f(x) = x(2+x)e^{-x} \max_{\mathbb{Q}} \{ f(x) \} f'(x) = (-x^2+2)e^{-x} x x = \pm \sqrt{2} \mathbb{Q} \sqrt{2} p \sqrt{2} f(x) \mathbb{Q} \mathbb{R} \mathbb{R}\backslash\mathbb{Q},"['calculus', 'analysis', 'optimization', 'maxima-minima', 'rational-numbers']"
92,A double integration via Fubini theorem,A double integration via Fubini theorem,,"$$\int_0^1\int_0^\infty ye^{-xy}\sin x\,dx\,dy$$ How can I calculate out the value of this integral? P.S. One easy way is to calculate this integral over $dy$ first, to get an integral form $\frac{1-e^{-x}(x+1)}{x^2}\sin x$ , if I calculated correctly, but I don't know any way to calculate out this value other than a hard work with contour integral.  - so I wonder if there be a way other than this (=integrate over $dy$ and do contour integral) (I tried some integration by parts and substitutions but it seems it does not work well, probably the $\infty$ at the second integral is crucial. I think this requires capturing a term in the integrand and converting to a further integral, and reverting the order of integral by Fubini's theorem.. but I'm not sure)","How can I calculate out the value of this integral? P.S. One easy way is to calculate this integral over first, to get an integral form , if I calculated correctly, but I don't know any way to calculate out this value other than a hard work with contour integral.  - so I wonder if there be a way other than this (=integrate over and do contour integral) (I tried some integration by parts and substitutions but it seems it does not work well, probably the at the second integral is crucial. I think this requires capturing a term in the integrand and converting to a further integral, and reverting the order of integral by Fubini's theorem.. but I'm not sure)","\int_0^1\int_0^\infty ye^{-xy}\sin x\,dx\,dy dy \frac{1-e^{-x}(x+1)}{x^2}\sin x dy \infty","['real-analysis', 'calculus', 'analysis']"
93,Infinity norm is actually a norm : triangle inequality,Infinity norm is actually a norm : triangle inequality,,"I have to prove the following assertion :  Let $V$ be a finit dimentional vector space with dimension $n$ over the field $K$ which is the field of real numbers or complex numbers. Let the map defined by: $\mid \mid . \mid \mid _{\infty} \quad : v \mapsto \mid \mid v \mid \mid_{\infty}=\max\{\mid v_1\mid,...,\mid v_n \mid \} \quad \forall v \in V$ where $v_k \quad k=1,...,n $ are the componant of the vector $v$ . Show that this map is a norm. I proved the posivity of this norm. I also proved that : $\mid \mid \lambda v \mid \mid _{\infty}=\mid \lambda \mid \mid \mid v \mid \mid \quad \forall v \in V, \forall \lambda \in K$ Now, I have to prove the triangle inequality. I have already done something but i am not sure what i have done is correct :  I have to prove that : $$\forall v, w \in V \quad \mid \mid v+w \mid \mid _{\infty}\leq \mid \mid v \mid \mid_{\infty} + \mid \mid w \mid \mid _{\infty}$$ I know that in the real or complex numbers, we have this inequality for all $x$ and $y$ : $\mid x+y\mid \leq \mid x \mid +\mid y\mid $ Therefore : $v=(v_1,...,v_n)$ , $w=(w_1,...,w_n)$ : $$\forall k \in {1,...,n} \quad \mid v_k + w_k \mid \leq \mid v_k \mid +\mid w_k \mid $$ Then, $$\max\{\mid v_1 +w_1\mid,... \mid v_n +w_n\mid \} \leq \max\{\mid v_1 \mid +\mid w_1 \mid ,...,\mid v_n\mid+\mid w_n \mid \}$$ Now the problem is : can i say that $$\max\{\mid v_1 \mid +\mid w_1 \mid ,...,\mid v_n\mid+\mid w_n \mid \} \leq \max\{\mid v_1\mid ,...\mid v_n\mid \}+\max\{\mid w_1\mid ,...\mid w_n\mid \}$$ which would prove the inequality ? Is it trivial ? If it is not, i don't know how to prove the last inequality...","I have to prove the following assertion :  Let be a finit dimentional vector space with dimension over the field which is the field of real numbers or complex numbers. Let the map defined by: where are the componant of the vector . Show that this map is a norm. I proved the posivity of this norm. I also proved that : Now, I have to prove the triangle inequality. I have already done something but i am not sure what i have done is correct :  I have to prove that : I know that in the real or complex numbers, we have this inequality for all and : Therefore : , : Then, Now the problem is : can i say that which would prove the inequality ? Is it trivial ? If it is not, i don't know how to prove the last inequality...","V n K \mid \mid . \mid \mid _{\infty} \quad : v \mapsto \mid \mid v \mid \mid_{\infty}=\max\{\mid v_1\mid,...,\mid v_n \mid \} \quad \forall v \in V v_k \quad k=1,...,n  v \mid \mid \lambda v \mid \mid _{\infty}=\mid \lambda \mid \mid \mid v \mid \mid \quad \forall v \in V, \forall \lambda \in K \forall v, w \in V \quad \mid \mid v+w \mid \mid _{\infty}\leq \mid \mid v \mid \mid_{\infty} + \mid \mid w \mid \mid _{\infty} x y \mid x+y\mid \leq \mid x \mid +\mid y\mid  v=(v_1,...,v_n) w=(w_1,...,w_n) \forall k \in {1,...,n} \quad \mid v_k + w_k \mid \leq \mid v_k \mid +\mid w_k \mid  \max\{\mid v_1 +w_1\mid,... \mid v_n +w_n\mid \} \leq \max\{\mid v_1 \mid +\mid w_1 \mid ,...,\mid v_n\mid+\mid w_n \mid \} \max\{\mid v_1 \mid +\mid w_1 \mid ,...,\mid v_n\mid+\mid w_n \mid \} \leq \max\{\mid v_1\mid ,...\mid v_n\mid \}+\max\{\mid w_1\mid ,...\mid w_n\mid \}","['analysis', 'inequality', 'normed-spaces']"
94,Exercise 4.2.5 [HDP Book Vershynin]: Packing the balls into K,Exercise 4.2.5 [HDP Book Vershynin]: Packing the balls into K,,"Suppose $T$ is a normed space. Prove that packing number $P(K,d,\epsilon)$ of $K \subset T$ is the largest number of closed disjoint balls with centers in $K$ and radii $\epsilon/2$ . Show by example that the previous statement may be false for a general metric space $T$ . Def:(Packing numbers). A subset $\cal{N}$ of a metric space $(T,d)$ is $\epsilon$ -separated if $d(x,y)>\epsilon$ for all distinct points $x,y\in \cal{N}$ . The largest possible cardinality of an $\epsilon$ -separated subset of a given set $K\subset T$ is called the packing number of $K$ and is denoted by $P(K,d,\epsilon)$ Proof: Suppose $P(K,d,\epsilon)= n$ , so there are $n$ points in $K$ which are $\epsilon$ -separated. Each of these $n$ points can be thought as a center of a $\epsilon/2$ radius ball. Then there are $n$ closed disjoint balls of radius $\epsilon/2$ , that have their center in K. It remains to show that $n$ is largest number. Assume that there are $n+1$ such $\epsilon/2$ radius closed disjoint balls that have centers in $K$ , which implies that $K$ has $n+1$ $\epsilon$ -separated points, which is contradictory to the fact that $K$ has a packing number $n$ . Hence there can be at-most $n$ such balls. Doubt: The above proof holds true for any metric space, which is not correct according to the question. Please help.","Suppose is a normed space. Prove that packing number of is the largest number of closed disjoint balls with centers in and radii . Show by example that the previous statement may be false for a general metric space . Def:(Packing numbers). A subset of a metric space is -separated if for all distinct points . The largest possible cardinality of an -separated subset of a given set is called the packing number of and is denoted by Proof: Suppose , so there are points in which are -separated. Each of these points can be thought as a center of a radius ball. Then there are closed disjoint balls of radius , that have their center in K. It remains to show that is largest number. Assume that there are such radius closed disjoint balls that have centers in , which implies that has -separated points, which is contradictory to the fact that has a packing number . Hence there can be at-most such balls. Doubt: The above proof holds true for any metric space, which is not correct according to the question. Please help.","T P(K,d,\epsilon) K \subset T K \epsilon/2 T \cal{N} (T,d) \epsilon d(x,y)>\epsilon x,y\in \cal{N} \epsilon K\subset T K P(K,d,\epsilon) P(K,d,\epsilon)= n n K \epsilon n \epsilon/2 n \epsilon/2 n n+1 \epsilon/2 K K n+1 \epsilon K n n","['analysis', 'random-matrices']"
95,"If $f$ vanishes at all its points of continuity, then $\|f\| = 0.$","If  vanishes at all its points of continuity, then",f \|f\| = 0.,"Denote by $\mathcal R$ the set of all Riemann integrable functions on $[0,2\pi]$ with norm given by $$ \|f\| = \bigg( \frac{1}{2\pi}\int_0^{2\pi}|f(x)|^2dx\bigg)^{1/2}$$ If $f\in \mathcal R$ vanishes at all its points of continuity, then $\|f\| = 0$ . This exercise appeard on a book on Fourier analysis. The unique idea that I had was to use parseval identity: $\|f\|^2 = \sum |c_n|^2$ and then using the identity $f(x)= \sum c_n e^{inx}$ to prove that all Fourier coeffcients must be zero. But this is not possible with $f$ only being continuous at $x$ . We must know more about its derivative. Any help?","Denote by the set of all Riemann integrable functions on with norm given by If vanishes at all its points of continuity, then . This exercise appeard on a book on Fourier analysis. The unique idea that I had was to use parseval identity: and then using the identity to prove that all Fourier coeffcients must be zero. But this is not possible with only being continuous at . We must know more about its derivative. Any help?","\mathcal R [0,2\pi]  \|f\| = \bigg( \frac{1}{2\pi}\int_0^{2\pi}|f(x)|^2dx\bigg)^{1/2} f\in \mathcal R \|f\| = 0 \|f\|^2 = \sum |c_n|^2 f(x)= \sum c_n e^{inx} f x","['analysis', 'fourier-analysis']"
96,Is every monotonic additive function $f \colon \mathbb{R} \to \mathbb{R}$ continuous?,Is every monotonic additive function  continuous?,f \colon \mathbb{R} \to \mathbb{R},"Let a function $f \colon \mathbb{R} \to \mathbb{R}$ have the following two properties: (1) For all $x_1, x_2 \in \mathbb{R}$ such that $x_1 < x_2$, we have  $$f \left( x_1 \right) \leq f \left( x_2 \right). $$ (2) For all $x_1, x_2 \in \mathbb{R}$,  we have  $$f \left( x_1 + x_2 \right) = f \left( x_1 \right) + f \left( x_2 \right).  $$ Is such a function $f$ continuous at every point $c$ of $\mathbb{R}$? My Effort: We can show that for every rational number $q$, we have    $$ f(q) = q f(1). $$ As $f$ is monotonic (increasing), so the set of points of discontinuity of $f$ is at most countable. How to proceed from here? If we could show that $f$ is continuous at every rational point $c \in \mathbb{Q}$, then $f$ would also be continuous at every point of $\mathbb{R}$. Am I right? What next? Context : Sec. 5.6 (in fact immediately after Theorem 5.6.4) in the book Introduction To Real Analysis by Robert G. Bartle & Donald R. Sherbert, 4th edition.","Let a function $f \colon \mathbb{R} \to \mathbb{R}$ have the following two properties: (1) For all $x_1, x_2 \in \mathbb{R}$ such that $x_1 < x_2$, we have  $$f \left( x_1 \right) \leq f \left( x_2 \right). $$ (2) For all $x_1, x_2 \in \mathbb{R}$,  we have  $$f \left( x_1 + x_2 \right) = f \left( x_1 \right) + f \left( x_2 \right).  $$ Is such a function $f$ continuous at every point $c$ of $\mathbb{R}$? My Effort: We can show that for every rational number $q$, we have    $$ f(q) = q f(1). $$ As $f$ is monotonic (increasing), so the set of points of discontinuity of $f$ is at most countable. How to proceed from here? If we could show that $f$ is continuous at every rational point $c \in \mathbb{Q}$, then $f$ would also be continuous at every point of $\mathbb{R}$. Am I right? What next? Context : Sec. 5.6 (in fact immediately after Theorem 5.6.4) in the book Introduction To Real Analysis by Robert G. Bartle & Donald R. Sherbert, 4th edition.",,"['calculus', 'real-analysis', 'analysis', 'continuity', 'monotone-functions']"
97,Condition on two irrational numbers,Condition on two irrational numbers,,"Consider two irrational numbers $a$ and $b$. Are there well known sufficent conditions on the relation between $a$ and $b$ that would allow me to conclude that there is $c \in \mathbb{R}$, $c \neq 0$, such that both $ca$ and $cb$ are rational?","Consider two irrational numbers $a$ and $b$. Are there well known sufficent conditions on the relation between $a$ and $b$ that would allow me to conclude that there is $c \in \mathbb{R}$, $c \neq 0$, such that both $ca$ and $cb$ are rational?",,"['analysis', 'irrational-numbers']"
98,Lim inf $ (x_n + y_n)$ less than or equal to lim inf $x_n +$ lim sup $y_n$,Lim inf  less than or equal to lim inf  lim sup, (x_n + y_n) x_n + y_n,"I have the question: Show that for bounded sequences   $(x_n)$ and   $(y_n)$: $$\liminf\limits_{n \rightarrow \infty} (x_n + y_n) \leq \limsup\limits_{n \rightarrow \infty} x_n + \liminf\limits_{n \rightarrow \infty} y_n$$ So far I have that: $$\limsup\limits_{n \rightarrow \infty} (x_n + y_n) \leq \limsup\limits_{n \rightarrow \infty} x_n + \limsup\limits_{n \rightarrow \infty} y_n$$ and that $$ \liminf\limits_{n \rightarrow \infty} (x_n + y_n) \ge \liminf\limits_{n \rightarrow \infty} x_n + \liminf\limits_{n \rightarrow \infty} y_n\tag{1}$$ But now I'm a bit stuck - I've tried using $$ \limsup\limits_{n \rightarrow \infty} (-x_n) = - \liminf\limits_{n \rightarrow \infty} (x_n) \tag{2}$$ but I'm just going in circles. There are similar questions to this but none that I have found address this specficially, any help?","I have the question: Show that for bounded sequences   $(x_n)$ and   $(y_n)$: $$\liminf\limits_{n \rightarrow \infty} (x_n + y_n) \leq \limsup\limits_{n \rightarrow \infty} x_n + \liminf\limits_{n \rightarrow \infty} y_n$$ So far I have that: $$\limsup\limits_{n \rightarrow \infty} (x_n + y_n) \leq \limsup\limits_{n \rightarrow \infty} x_n + \limsup\limits_{n \rightarrow \infty} y_n$$ and that $$ \liminf\limits_{n \rightarrow \infty} (x_n + y_n) \ge \liminf\limits_{n \rightarrow \infty} x_n + \liminf\limits_{n \rightarrow \infty} y_n\tag{1}$$ But now I'm a bit stuck - I've tried using $$ \limsup\limits_{n \rightarrow \infty} (-x_n) = - \liminf\limits_{n \rightarrow \infty} (x_n) \tag{2}$$ but I'm just going in circles. There are similar questions to this but none that I have found address this specficially, any help?",,"['analysis', 'inequality', 'limsup-and-liminf']"
99,"Is there a function $f''(0)$ exists, $f'$ is not continuous on $(-\delta,\delta)$","Is there a function  exists,  is not continuous on","f''(0) f' (-\delta,\delta)","Is  there a function $f\colon(-\delta,\delta)\to\Bbb R$ satisfying the folowing conditions(real number $ \delta\gt0$)? (i) $f$ is  differentiable on $(-\delta,\delta)$; (ii) the second derivative of $f$ exists  at $0$, that is $f''(0)$ exists (iii) there is a sequence $\{x_n\}$, $-\delta\lt x_n\lt \delta$, $\lim\limits_{n\to\infty}x_n=0$,  such that $f'$ is not continuous  at all $x_n$. I can't construct such  a  example Any help will be appreciated!","Is  there a function $f\colon(-\delta,\delta)\to\Bbb R$ satisfying the folowing conditions(real number $ \delta\gt0$)? (i) $f$ is  differentiable on $(-\delta,\delta)$; (ii) the second derivative of $f$ exists  at $0$, that is $f''(0)$ exists (iii) there is a sequence $\{x_n\}$, $-\delta\lt x_n\lt \delta$, $\lim\limits_{n\to\infty}x_n=0$,  such that $f'$ is not continuous  at all $x_n$. I can't construct such  a  example Any help will be appreciated!",,"['real-analysis', 'analysis']"
