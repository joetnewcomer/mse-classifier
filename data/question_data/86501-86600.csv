,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Determine the orthogonal projection of $f$ onto $K := \{u \in H : |u| \le h \text{ a.e.}\}$,Determine the orthogonal projection of  onto,f K := \{u \in H : |u| \le h \text{ a.e.}\},"Let $(\Omega, \mathcal F, \mu)$ be a $\sigma$ -finite measure space. Let $h:\Omega \to [0, \infty)$ be $\mu$ -measurable. Let $H:= L^2(\Omega)$ and $$ K := \{u \in H : |u| \le h \text{ a.e.}\}. $$ Then $K$ is non-empty closed convex subset of $H$ . I'm trying to solve below exercise Fix $f \in H$ and let $u$ be the orthogonal projection of $f$ onto $K$ . Determine $u$ . Could you verify my below attempt? Let $\Omega_1 := \{ |f| \le h \}, \Omega_2 := \{ f < -h \}$ and $\Omega_3 := \{ f > h \}$ . We define $u$ by $$ u (\omega) := \begin{cases} f(\omega) &\text{if} \quad \omega \in \Omega_1, \\ -h(\omega) &\text{if} \quad \omega \in \Omega_2, \\ h(\omega) &\text{if} \quad \omega \in \Omega_3. \end{cases} $$ Then $u \in K$ . Fix $v \in K$ . It suffices to prove that $\int_\Omega (f-u)(v-u) \le 0$ . First, $\int_{\Omega_1} (f-u)(v-u) =0$ . We have $f+h<0$ on $\Omega_2$ and $v+h \ge 0$ , so $$ \int_{\Omega_2} (f-u)(v-u) = \int_{\Omega_1} (f+h)(v+h) \le 0. $$ We have $f-h>0$ on $\Omega_3$ and $v-h \le 0$ , so $$ \int_{\Omega_3} (f-u)(v-u) = \int_{\Omega_1} (f-h)(v-h) \le 0. $$ This completes the proof.","Let be a -finite measure space. Let be -measurable. Let and Then is non-empty closed convex subset of . I'm trying to solve below exercise Fix and let be the orthogonal projection of onto . Determine . Could you verify my below attempt? Let and . We define by Then . Fix . It suffices to prove that . First, . We have on and , so We have on and , so This completes the proof.","(\Omega, \mathcal F, \mu) \sigma h:\Omega \to [0, \infty) \mu H:= L^2(\Omega) 
K := \{u \in H : |u| \le h \text{ a.e.}\}.
 K H f \in H u f K u \Omega_1 := \{ |f| \le h \}, \Omega_2 := \{ f < -h \} \Omega_3 := \{ f > h \} u 
u (\omega) :=
\begin{cases}
f(\omega) &\text{if} \quad \omega \in \Omega_1, \\
-h(\omega) &\text{if} \quad \omega \in \Omega_2, \\
h(\omega) &\text{if} \quad \omega \in \Omega_3.
\end{cases}
 u \in K v \in K \int_\Omega (f-u)(v-u) \le 0 \int_{\Omega_1} (f-u)(v-u) =0 f+h<0 \Omega_2 v+h \ge 0 
\int_{\Omega_2} (f-u)(v-u) = \int_{\Omega_1} (f+h)(v+h) \le 0.
 f-h>0 \Omega_3 v-h \le 0 
\int_{\Omega_3} (f-u)(v-u) = \int_{\Omega_1} (f-h)(v-h) \le 0.
","['functional-analysis', 'hilbert-spaces', 'lp-spaces']"
1,Application of Banach-Alaoglu theorem to extract convergent subsequence of currents,Application of Banach-Alaoglu theorem to extract convergent subsequence of currents,,"While reading about currents I came across the following lemma in Lectures on Geometric Measure Theory by Leon Simon on page 135: Lemma. If $\left\{T_j\right\}_{j\in\mathbb{N}}$ is a sequence of currents in $\mathcal{D}_k(U)$ such that $$\sup_{j\in\mathbb{N}}\mathbb{M}_W(T_j)<\infty$$ for all $W\subset\kern-2px\subset U$ , then there is a subsequence $\{T_{j_i}\}_{i\in\mathbb{N}}$ and a $T\in\mathcal{D}_k(U)$ such that $T_{j_i}\rightharpoonup T$ in $U$ as $i\to\infty$ . For some context, the space $\mathcal{D}_k(U)$ is the space of $k$ -dimensional currents on the open set $U\subseteq\mathbb{R}^n$ , i.e. the (topological) dual space of the space of smooth compactly supported differential $k$ -forms on $U$ , $\mathcal{D}^k(U)$ . Furthermore, for a current $T\in\mathcal{D}_k(U)$ and a set $W\subset\kern-2px\subset U$ , we have that $$\mathbb{M}_W(T)=\sup\left\{T(\omega) : \omega\in\mathcal{D}^k(U),\,\lVert\omega\rVert\leq1,\,\operatorname{supp}\omega\subseteq W\right\}$$ is the mass of $T$ in $W$ . My concern is with how the above theorem is actually proven. In particular, right after defining the notion of weak convergence of currents, i.e. that $T_j\rightharpoonup T$ in $U$ as $j\to\infty$ iff $\lim_{j\to\infty}T_j(\omega)=T(\omega)$ for all $\omega\in\mathcal{D}^k(U)$ , it is quickly mentioned that mass is lower semi-continuous with respect to weak convergence (which I can easily verify), and right after that the lemma is given, with the only ""proof"" being that it's an application of the Banach-Alaoglu theorem on the Banach spaces $$\mathcal{M}_k(W)=\left\{T\in\mathcal{D}_k(W):\mathbb{M}_W(T)<\infty\right\}$$ for $W\subset\kern-2px\subset U$ . Being very inexperienced with functional analysis, I am unable to really make any substantial progress on actually verifying this lemma. While it might be trivial for someone who is more familiar with applications of the Banach-Alaoglu theorem, the only thing I've thought to is to, for all $W\subset\kern-2px\subset U$ , let $\alpha_W>0$ be such that $$\mathbb{M}_W(T_j)\leq\alpha_W$$ for all $j\in\mathbb{N}$ , then define a new sequence $\{R^W_j\}_{j\in\mathbb{N}}$ by setting $R^W_j=\frac{T_j}{\alpha_W}$ , so that $$\mathbb{M}_W(R^W_j)\leq1.$$ If I understand Banach-Alaoglu correctly (which I might not), then I should be able to extract a convergent subsequence $\{R^W_{j_i}\}_{i\in\mathbb{N}}$ converging weakly in $W$ to some $R^W$ , from which I should be able to deduce that $T_{j_i}\rightharpoonup \alpha_W R^W$ in $W$ as $i\to\infty$ . The problem here is that each such subsequence depends on the choice of $W$ , and the subsequences only converge weakly on the respective $W$ , which is not what the lemma entails. Perhaps what I have concluded (assuming it is correct) will imply the lemma, however I doubt it. I have also tried looking for other resources containing the same lemma, however each one I was able to find simply stated that it is a consequence of the Banach-Alaoglu theorem without elaborating further.","While reading about currents I came across the following lemma in Lectures on Geometric Measure Theory by Leon Simon on page 135: Lemma. If is a sequence of currents in such that for all , then there is a subsequence and a such that in as . For some context, the space is the space of -dimensional currents on the open set , i.e. the (topological) dual space of the space of smooth compactly supported differential -forms on , . Furthermore, for a current and a set , we have that is the mass of in . My concern is with how the above theorem is actually proven. In particular, right after defining the notion of weak convergence of currents, i.e. that in as iff for all , it is quickly mentioned that mass is lower semi-continuous with respect to weak convergence (which I can easily verify), and right after that the lemma is given, with the only ""proof"" being that it's an application of the Banach-Alaoglu theorem on the Banach spaces for . Being very inexperienced with functional analysis, I am unable to really make any substantial progress on actually verifying this lemma. While it might be trivial for someone who is more familiar with applications of the Banach-Alaoglu theorem, the only thing I've thought to is to, for all , let be such that for all , then define a new sequence by setting , so that If I understand Banach-Alaoglu correctly (which I might not), then I should be able to extract a convergent subsequence converging weakly in to some , from which I should be able to deduce that in as . The problem here is that each such subsequence depends on the choice of , and the subsequences only converge weakly on the respective , which is not what the lemma entails. Perhaps what I have concluded (assuming it is correct) will imply the lemma, however I doubt it. I have also tried looking for other resources containing the same lemma, however each one I was able to find simply stated that it is a consequence of the Banach-Alaoglu theorem without elaborating further.","\left\{T_j\right\}_{j\in\mathbb{N}} \mathcal{D}_k(U) \sup_{j\in\mathbb{N}}\mathbb{M}_W(T_j)<\infty W\subset\kern-2px\subset U \{T_{j_i}\}_{i\in\mathbb{N}} T\in\mathcal{D}_k(U) T_{j_i}\rightharpoonup T U i\to\infty \mathcal{D}_k(U) k U\subseteq\mathbb{R}^n k U \mathcal{D}^k(U) T\in\mathcal{D}_k(U) W\subset\kern-2px\subset U \mathbb{M}_W(T)=\sup\left\{T(\omega) : \omega\in\mathcal{D}^k(U),\,\lVert\omega\rVert\leq1,\,\operatorname{supp}\omega\subseteq W\right\} T W T_j\rightharpoonup T U j\to\infty \lim_{j\to\infty}T_j(\omega)=T(\omega) \omega\in\mathcal{D}^k(U) \mathcal{M}_k(W)=\left\{T\in\mathcal{D}_k(W):\mathbb{M}_W(T)<\infty\right\} W\subset\kern-2px\subset U W\subset\kern-2px\subset U \alpha_W>0 \mathbb{M}_W(T_j)\leq\alpha_W j\in\mathbb{N} \{R^W_j\}_{j\in\mathbb{N}} R^W_j=\frac{T_j}{\alpha_W} \mathbb{M}_W(R^W_j)\leq1. \{R^W_{j_i}\}_{i\in\mathbb{N}} W R^W T_{j_i}\rightharpoonup \alpha_W R^W W i\to\infty W W","['functional-analysis', 'banach-spaces', 'weak-convergence', 'geometric-measure-theory']"
2,"Can we bound $\sum_{i=1}^ke^{-{\rm i}2\pi\langle\omega,\:x_i\rangle}$ by $\sum_{i=1}^ke^{-{\rm i}2\pi\langle\omega,\:x_i\rangle}\hat g(a_i\omega)$?",Can we bound  by ?,"\sum_{i=1}^ke^{-{\rm i}2\pi\langle\omega,\:x_i\rangle} \sum_{i=1}^ke^{-{\rm i}2\pi\langle\omega,\:x_i\rangle}\hat g(a_i\omega)","Let $x_1,\ldots,x_k\in[0,1)^d$ , $a_1,\ldots,a_d>0$ and $$f(\omega):=\sum_{i=1}^ke^{-{\rm i}2\pi\langle\omega,\:x_i\rangle}\hat g(a_i\omega)\;\;\;\text{for }\omega\in\mathbb R^d,$$ where $g(x)=e^{-(x/\sigma)^2}$ for some $\sigma>0$ . Suppose $|f(\omega)|<\varepsilon$ for all $\omega\in\mathbb R^d$ with $0<\|\omega\|<\delta$ . Can we show that $\left|\sum_{i=1}^ke^{-{\rm i}2\pi\langle\omega,\:x_i\rangle}\right|$ is also bounded by something multiplied by $\varepsilon$ ? This is clearly true when all $a_i$ are equal to the same value, since we can then pull the $\hat\varphi$ out.","Let , and where for some . Suppose for all with . Can we show that is also bounded by something multiplied by ? This is clearly true when all are equal to the same value, since we can then pull the out.","x_1,\ldots,x_k\in[0,1)^d a_1,\ldots,a_d>0 f(\omega):=\sum_{i=1}^ke^{-{\rm i}2\pi\langle\omega,\:x_i\rangle}\hat g(a_i\omega)\;\;\;\text{for }\omega\in\mathbb R^d, g(x)=e^{-(x/\sigma)^2} \sigma>0 |f(\omega)|<\varepsilon \omega\in\mathbb R^d 0<\|\omega\|<\delta \left|\sum_{i=1}^ke^{-{\rm i}2\pi\langle\omega,\:x_i\rangle}\right| \varepsilon a_i \hat\varphi","['functional-analysis', 'inequality', 'fourier-analysis', 'fourier-transform']"
3,"Sufficiency part For the existence of $\bar p(\cdot)$ $\in$ $W(p)$ for which $C([0,1]$) is closed subspace in $L^{\bar p(\cdot)}([0;1])$.",Sufficiency part For the existence of    for which ) is closed subspace in .,"\bar p(\cdot) \in W(p) C([0,1] L^{\bar p(\cdot)}([0;1])","Handwriting this would be impossible, so I apologize. These are the definitions and theorems  which we need for the proof of the theorem : BFS is defined as the Bannach Function Space. Let $W(p)$ denote set of all functions equimeasurable with $p(\cdot)$ . There is the theorem: My question is all about the sufficiency part of the theorem. There is the proof of it: I want to apply this theorem for 2 dimensions. This will be the statement applied to 2 dimensions: For the existence of $\bar p(\cdot)$ $\in$ $W(p)$ for which $C([0,1]^2$ ) is closed subspace in $L^{\bar p(\cdot)}([0;1]^2)$ it is necessary and sufficient that $\lim_{t\to 0+}$ $ sup \frac {p^{*}(t)}{ln(e/t)}$ $\gt$ $0$ . I've already shown that the theorem $2.2$ and $2.3$ work for two dimensions. I have  a trouble in reconstructing this proof for two dimensions. I've added an important definition About the BFS and  subspaces of BFS X. Any help would be appreciated.","Handwriting this would be impossible, so I apologize. These are the definitions and theorems  which we need for the proof of the theorem : BFS is defined as the Bannach Function Space. Let denote set of all functions equimeasurable with . There is the theorem: My question is all about the sufficiency part of the theorem. There is the proof of it: I want to apply this theorem for 2 dimensions. This will be the statement applied to 2 dimensions: For the existence of for which ) is closed subspace in it is necessary and sufficient that . I've already shown that the theorem and work for two dimensions. I have  a trouble in reconstructing this proof for two dimensions. I've added an important definition About the BFS and  subspaces of BFS X. Any help would be appreciated.","W(p) p(\cdot) \bar p(\cdot) \in W(p) C([0,1]^2 L^{\bar p(\cdot)}([0;1]^2) \lim_{t\to 0+}  sup \frac {p^{*}(t)}{ln(e/t)} \gt 0 2.2 2.3","['real-analysis', 'functional-analysis', 'analysis', 'measure-theory', 'banach-spaces']"
4,Convergence in the formulation of the spectral theorem,Convergence in the formulation of the spectral theorem,,"Let $\mathcal H$ be a complex (separable, if needed) Hilbert space and $B(\mathcal H)$ the ring of bounded operators on $\mathcal H$ . I am interested in understanding the formulation of the spectral theorem (for self-adjoint, possibly unbounded operators) in terms of resolutions of the identity. A function $E:\mathbb R\rightarrow B(\mathcal H)$ is a resolution of the identity if for each $\lambda\in\mathbb R$ , $E(\lambda)$ is an orthogonal projection; for each $\lambda_0<\lambda_1$ , $E(\lambda_0)\le E(\lambda_1)$ ; the function $E(\lambda)$ is right-continuous; $\lim_{\lambda\rightarrow-\infty} E(\lambda)=0$ and $\lim_{\lambda\rightarrow\infty}E(\lambda)=I$ where $I$ is the identity operator. One then defines a self-adjoint (possibly unbounded) operator $A$ through $$ A=\int_{-\infty}^{+\infty}\lambda\, \mathrm dE(\lambda). \qquad(\ast)$$ The spectral theorem then states that corresponding to every densely defined self-adjoint  operator on $\mathcal H$ there is a unique resolution of the identity such that $(\ast)$ is true. I do have access to a number of references that treat the spectral theorem this way in a reasonably rigorous manner but nonetheless I am a bit confused about the various definitions of convergence that appear in the statements above. So first, as I understand, if $A_n$ is a sequence of bounded operators on $\mathcal H$ , we have the strong convergence $A_{n}\overset{s}{\longrightarrow}A$ if for each $x\in\mathcal H$ , $A_nx\longrightarrow Ax$ and the weak convergence $ A_{n}\overset{w}{\longrightarrow}A $ if for each $x,y\in\mathcal H$ we have $ \langle x,A_n y\rangle\rightarrow \langle x,Ay\rangle $ (I am a physicist so for me the inner product is linear in the second variable). Now I assume that since each operator $A$ is uniquely determined by the sesquilinear form $\langle x,Ax\rangle$ through the polarization identity, it is true that if $\langle x,A_nx\rangle\rightarrow\langle x,Ax\rangle$ , then $A_n\overset{w}{\longrightarrow A}$ is that correct ? Then the first question is Q1 : In property 3. and 4. of the resolution of identity, the right continuity of the family $E(\lambda)$ and the limits $E(\lambda)$ as $\lambda\rightarrow\pm\infty$ are meant in the sense of weak or strong convergence, and why? I think it is irrelevant, because from what I can tell, for orthogonal projections the weak and strong convergence coincides, but I often find functional analysis so counterintuitive I am not sure I trust my proof. Basically suppose that $P_n$ is a sequence of orthogonal projections converging weakly to $P$ , i.e. $\langle x,P_ny\rangle\rightarrow\langle x,Py\rangle$ , then we have $$ \Vert P_nx-Px\Vert^2=\Vert P_nx\Vert^2+\Vert Px\Vert^2-2\mathrm{Re}\langle P_nx,Px\rangle, $$ but $\langle x,P_nx\rangle=\langle P_nx,P_nx\rangle=\Vert P_nx\Vert^2$ , hence $\Vert P_nx\Vert\rightarrow \Vert Px\Vert$ and thus $$ \lim_{n\rightarrow\infty}\Vert P_nx-Px\Vert^2=2\Vert Px\Vert^2-2\langle Px,Px\rangle=0, $$ thus $P_n\overset{s}{\longrightarrow} P$ as well. Is this correct? I have more problems with interpreting the integral $(\ast)$ defining $A$ . I have often seen it being meant that for any $x\in \mathcal H$ , we have $$ \langle x,Ax\rangle=\int_{-\infty}^{+\infty}\lambda\,\mathrm d\langle x,E(\lambda)x\rangle, \qquad(\ast\ast)$$ where the RHS is an ordinary Riemann-Stieltjes integral with respect to the function $P_x(\lambda):=\langle x,E(\lambda)x\rangle$ . Due to the polarization identity, this does determine $A$ uniquely and I guess $x$ belongs to the domain of $A$ if and only if the above Stieltjes integral converges. Q2 : Is there any direct expression for the action $Ax$ ? I would intuitively think that the formula for $\langle x,Ax\rangle$ implies that for any $x\in\mathcal H$ (or at least in the domain of $A$ ) we have $$ Ax=\int_{-\infty}^{+\infty}\lambda\,\mathrm d(E(\lambda)x), \quad(\ast\ast\ast)$$ and as far as I am aware, Stieltjes integrals with values in a Hilbert space do make sense (hence "" $\mathrm d(E(\lambda)x)$ "" can be interpreted), but the fact that weak convergence does not coincide with strong convergence in general makes me think that $(\ast\ast)$ does not imply $(\ast\ast\ast)$ . So does $(\ast\ast\ast)$ makes sense and if so when?","Let be a complex (separable, if needed) Hilbert space and the ring of bounded operators on . I am interested in understanding the formulation of the spectral theorem (for self-adjoint, possibly unbounded operators) in terms of resolutions of the identity. A function is a resolution of the identity if for each , is an orthogonal projection; for each , ; the function is right-continuous; and where is the identity operator. One then defines a self-adjoint (possibly unbounded) operator through The spectral theorem then states that corresponding to every densely defined self-adjoint  operator on there is a unique resolution of the identity such that is true. I do have access to a number of references that treat the spectral theorem this way in a reasonably rigorous manner but nonetheless I am a bit confused about the various definitions of convergence that appear in the statements above. So first, as I understand, if is a sequence of bounded operators on , we have the strong convergence if for each , and the weak convergence if for each we have (I am a physicist so for me the inner product is linear in the second variable). Now I assume that since each operator is uniquely determined by the sesquilinear form through the polarization identity, it is true that if , then is that correct ? Then the first question is Q1 : In property 3. and 4. of the resolution of identity, the right continuity of the family and the limits as are meant in the sense of weak or strong convergence, and why? I think it is irrelevant, because from what I can tell, for orthogonal projections the weak and strong convergence coincides, but I often find functional analysis so counterintuitive I am not sure I trust my proof. Basically suppose that is a sequence of orthogonal projections converging weakly to , i.e. , then we have but , hence and thus thus as well. Is this correct? I have more problems with interpreting the integral defining . I have often seen it being meant that for any , we have where the RHS is an ordinary Riemann-Stieltjes integral with respect to the function . Due to the polarization identity, this does determine uniquely and I guess belongs to the domain of if and only if the above Stieltjes integral converges. Q2 : Is there any direct expression for the action ? I would intuitively think that the formula for implies that for any (or at least in the domain of ) we have and as far as I am aware, Stieltjes integrals with values in a Hilbert space do make sense (hence "" "" can be interpreted), but the fact that weak convergence does not coincide with strong convergence in general makes me think that does not imply . So does makes sense and if so when?","\mathcal H B(\mathcal H) \mathcal H E:\mathbb R\rightarrow B(\mathcal H) \lambda\in\mathbb R E(\lambda) \lambda_0<\lambda_1 E(\lambda_0)\le E(\lambda_1) E(\lambda) \lim_{\lambda\rightarrow-\infty} E(\lambda)=0 \lim_{\lambda\rightarrow\infty}E(\lambda)=I I A  A=\int_{-\infty}^{+\infty}\lambda\, \mathrm dE(\lambda). \qquad(\ast) \mathcal H (\ast) A_n \mathcal H A_{n}\overset{s}{\longrightarrow}A x\in\mathcal H A_nx\longrightarrow Ax  A_{n}\overset{w}{\longrightarrow}A  x,y\in\mathcal H  \langle x,A_n y\rangle\rightarrow \langle x,Ay\rangle  A \langle x,Ax\rangle \langle x,A_nx\rangle\rightarrow\langle x,Ax\rangle A_n\overset{w}{\longrightarrow A} E(\lambda) E(\lambda) \lambda\rightarrow\pm\infty P_n P \langle x,P_ny\rangle\rightarrow\langle x,Py\rangle  \Vert P_nx-Px\Vert^2=\Vert P_nx\Vert^2+\Vert Px\Vert^2-2\mathrm{Re}\langle P_nx,Px\rangle,  \langle x,P_nx\rangle=\langle P_nx,P_nx\rangle=\Vert P_nx\Vert^2 \Vert P_nx\Vert\rightarrow \Vert Px\Vert  \lim_{n\rightarrow\infty}\Vert P_nx-Px\Vert^2=2\Vert Px\Vert^2-2\langle Px,Px\rangle=0,  P_n\overset{s}{\longrightarrow} P (\ast) A x\in \mathcal H  \langle x,Ax\rangle=\int_{-\infty}^{+\infty}\lambda\,\mathrm d\langle x,E(\lambda)x\rangle, \qquad(\ast\ast) P_x(\lambda):=\langle x,E(\lambda)x\rangle A x A Ax \langle x,Ax\rangle x\in\mathcal H A  Ax=\int_{-\infty}^{+\infty}\lambda\,\mathrm d(E(\lambda)x), \quad(\ast\ast\ast) \mathrm d(E(\lambda)x) (\ast\ast) (\ast\ast\ast) (\ast\ast\ast)","['functional-analysis', 'operator-theory', 'spectral-theory']"
5,What are all $L^pL^q$ estimates for the heat equation (with gain of derivatives)?,What are all  estimates for the heat equation (with gain of derivatives)?,L^pL^q,"The heat equation and the heat kernel. Consider the heat equation on $\mathbb R$ : $$ \left\{\begin{aligned}u_t-\Delta u&=f\\u(0,x)&=0\quad\forall x\in\mathbb R. \end{aligned}\right. $$ It is well-known that under suitable hypotheses there exists a unique solution $u$ given by the Duhamel formula (see below). So, we consider the operator $\Gamma:=(\partial_t-\Delta)^{-1} $ which maps $f$ to the solution $u$ , that is, $u=\Gamma f$ . Given the heat kernel $$ G(t,x)=\frac{1}{(4\pi t)^{1/2}}e^{-\frac{x^2}{4t}}, $$ one can show that $u$ is given as a space-time convolution between $G$ and $f$ : $$ u(t,x)=\left(\Gamma f\right)(t,x)=\int_0^\infty\int_{\mathbb R^d} G(t-s,x-y)f(s,y)\,\,dyds. $$ The problem. Problem. Given $s\in[0,2]$ , find all quadruples $(p,q,r,\sigma)$ such that the following estimate holds for all test functions $f\in\mathcal  D((0,\infty)\times\mathbb R)$ : $$ \||\partial_x|^s\Gamma f  \|_{L^rL^\sigma}\leq C\|f\|_{L^pL^q}. \qquad (1)$$ The norm in the space $L^pL^q$ is given by $$ \|f\|_{L^pL^q}^p:=\int_0^\infty\|f(s,\cdot)\|^p_{L^q}ds, $$ so it's the classical mixed $L^p-L^q$ norm in which you take first the $L^q$ norm in the $x$ variable and then the $L^p$ norm in the $t$ variable. The fractional derivative $|\partial_x|^s$ is defined via the Fourier transform. Basically, the problem is to find all the 'Strichartz estimates' of the heat equation with fractional gain of derivatives. Note that this is not about the fractional heat equation: we simply want to prove estimates on the derivatives of the solution to the classical heat equation. Considerations and known estimates. By scaling considerations, one immediately finds that the coefficients must satisfy $$ \left(\frac{1}{r}+\frac{1/2}{\sigma}\right)=\left(\frac{1}{p}+\frac{1/2}{q}\right)-1+\frac{s}{2}. \qquad (2)$$ Thus, in what follows I will always assume $(2)$ . I will also assume $p\leq r$ and $q\leq\sigma$ (essentially because convolution with a given function maps $L^p$ to $L^q$ with $q\geq p$ ). Preliminarly, I state without proof the following estimates for the heat kernel: $$ \||\partial_x|^sG(t,\cdot)\|_{L_x^p}\lesssim |t|^{-\frac{1}{2}(1+s)+\frac{1}{2p}}, \quad s\geq 0,\,\,1\leq p\leq \infty. \qquad (3)$$ What I already know is the following: The estimate holds in the cases $$ s\in\{0,1\},\quad 1\leq q\leq \sigma\leq \infty,\quad 1<p<r<\infty. $$ This follows from a direct use of the Hardy-Littlewood-Sobolev inequality and the estimates $(3)$ for $s=0$ and $s=1$ (see this book ). This is not in the book, but with the same proof, using the full range of estimates $(3)$ , one covers all the cases $s\in[0,2)$ with the same conditions on $p,q,r,\sigma$ as above. The estimates hold in the case $(r,\sigma)=(\infty,2)$ with $1\leq q\leq 2$ . This follows by integrating the heat equation against $u$ and perform standard energy estimates. What is left is a lot of endpoint cases (most prominently, $s=2$ , $p=r$ , $p=1$ , and $r=\infty$ ). Finally, my question. My question is about the case $r=\infty$ , $1<\sigma<\infty$ and $s\in(0,2)$ . It seems that energy estimates do not help for the full range of exponents $(p,q)$ and work only for certain values of $\sigma$ anyway. Do you know a way to cover all cases? I am reasonably sure that one can invert the roles of $x$ and $t$ and use Hardy-Littlewood-Sobolev, but that would work only for $p\leq q$ . I am especially interested in the case $(r,\sigma)=(\infty,2)$ , $q=2$ for $s\in(0,2)$ (which would basically cover all other cases $(p,q)$ by Sobolev embedding), or even $q<2$ if the case $q=2$ is too hard. Even the case $s=1$ is not clear to me at the moment, except when $(p,q)=(2,2)$ . Besides my specific question, it would be nice to collect some references that address the estimate in all known ranges of exponents (unless you can think of a paper which treats all these estimates at once, which would be very appreciated). So, if you have a reference for any of the remaining ranges of exponents, even for higher dimensions, that would be very appreciated. Related posts and references: A previous post with a similar question (basically the case $s=0$ ) Something about the case $s=2$","The heat equation and the heat kernel. Consider the heat equation on : It is well-known that under suitable hypotheses there exists a unique solution given by the Duhamel formula (see below). So, we consider the operator which maps to the solution , that is, . Given the heat kernel one can show that is given as a space-time convolution between and : The problem. Problem. Given , find all quadruples such that the following estimate holds for all test functions : The norm in the space is given by so it's the classical mixed norm in which you take first the norm in the variable and then the norm in the variable. The fractional derivative is defined via the Fourier transform. Basically, the problem is to find all the 'Strichartz estimates' of the heat equation with fractional gain of derivatives. Note that this is not about the fractional heat equation: we simply want to prove estimates on the derivatives of the solution to the classical heat equation. Considerations and known estimates. By scaling considerations, one immediately finds that the coefficients must satisfy Thus, in what follows I will always assume . I will also assume and (essentially because convolution with a given function maps to with ). Preliminarly, I state without proof the following estimates for the heat kernel: What I already know is the following: The estimate holds in the cases This follows from a direct use of the Hardy-Littlewood-Sobolev inequality and the estimates for and (see this book ). This is not in the book, but with the same proof, using the full range of estimates , one covers all the cases with the same conditions on as above. The estimates hold in the case with . This follows by integrating the heat equation against and perform standard energy estimates. What is left is a lot of endpoint cases (most prominently, , , , and ). Finally, my question. My question is about the case , and . It seems that energy estimates do not help for the full range of exponents and work only for certain values of anyway. Do you know a way to cover all cases? I am reasonably sure that one can invert the roles of and and use Hardy-Littlewood-Sobolev, but that would work only for . I am especially interested in the case , for (which would basically cover all other cases by Sobolev embedding), or even if the case is too hard. Even the case is not clear to me at the moment, except when . Besides my specific question, it would be nice to collect some references that address the estimate in all known ranges of exponents (unless you can think of a paper which treats all these estimates at once, which would be very appreciated). So, if you have a reference for any of the remaining ranges of exponents, even for higher dimensions, that would be very appreciated. Related posts and references: A previous post with a similar question (basically the case ) Something about the case","\mathbb R  \left\{\begin{aligned}u_t-\Delta u&=f\\u(0,x)&=0\quad\forall x\in\mathbb R. \end{aligned}\right.  u \Gamma:=(\partial_t-\Delta)^{-1}  f u u=\Gamma f  G(t,x)=\frac{1}{(4\pi t)^{1/2}}e^{-\frac{x^2}{4t}},  u G f  u(t,x)=\left(\Gamma f\right)(t,x)=\int_0^\infty\int_{\mathbb R^d} G(t-s,x-y)f(s,y)\,\,dyds.  s\in[0,2] (p,q,r,\sigma) f\in\mathcal
 D((0,\infty)\times\mathbb R)  \||\partial_x|^s\Gamma f
 \|_{L^rL^\sigma}\leq C\|f\|_{L^pL^q}. \qquad (1) L^pL^q  \|f\|_{L^pL^q}^p:=\int_0^\infty\|f(s,\cdot)\|^p_{L^q}ds,  L^p-L^q L^q x L^p t |\partial_x|^s  \left(\frac{1}{r}+\frac{1/2}{\sigma}\right)=\left(\frac{1}{p}+\frac{1/2}{q}\right)-1+\frac{s}{2}. \qquad (2) (2) p\leq r q\leq\sigma L^p L^q q\geq p  \||\partial_x|^sG(t,\cdot)\|_{L_x^p}\lesssim |t|^{-\frac{1}{2}(1+s)+\frac{1}{2p}}, \quad s\geq 0,\,\,1\leq p\leq \infty. \qquad (3)  s\in\{0,1\},\quad 1\leq q\leq \sigma\leq \infty,\quad 1<p<r<\infty.  (3) s=0 s=1 (3) s\in[0,2) p,q,r,\sigma (r,\sigma)=(\infty,2) 1\leq q\leq 2 u s=2 p=r p=1 r=\infty r=\infty 1<\sigma<\infty s\in(0,2) (p,q) \sigma x t p\leq q (r,\sigma)=(\infty,2) q=2 s\in(0,2) (p,q) q<2 q=2 s=1 (p,q)=(2,2) s=0 s=2","['real-analysis', 'functional-analysis', 'partial-differential-equations', 'lp-spaces', 'heat-equation']"
6,Representation of the C*-algebra of a Kähler manifold,Representation of the C*-algebra of a Kähler manifold,,"This question is inspired from mathematics of quantum mechanics. In quantum mechanics, we start with a Kähler manifold $\mathcal{M}$ (which is $\mathbb{CP}^n$ ), with the symplectic form $\omega$ and metric $g$ . We have a correspondence between the geometric data and algebraic data as follows. The symplectic form induces a Poisson algebra on the functions $x,y$ on the manifold $$[x,y] := \omega (dx,dy)\,,$$ and the metric induces a Jordan algebra $$x\cdot y := g(dx,dy)+xy\,.$$ Or, we can treat the two structures at the same time and say that the Kähler structure (metric+symplectic) induces a  C*-algebra (Poisson+Jordan). What seems remarkable to me is the representation of the C*-algebra. The representation is a Hilbert space $\mathcal{H}$ by GNS construction. Magically, its projective space turns out to be the manifold we started with: $$\mathbb{P}\mathcal{H} = \mathcal{M}.$$ Somehow, we go back from algebraic data to geometric data by an application of representation theory. Why is this true for $\mathcal M=\mathbb{CP}^n$ ? Is it true for any Kähler manifold $\mathcal{M}$ ? Answers and references are both appreciated.","This question is inspired from mathematics of quantum mechanics. In quantum mechanics, we start with a Kähler manifold (which is ), with the symplectic form and metric . We have a correspondence between the geometric data and algebraic data as follows. The symplectic form induces a Poisson algebra on the functions on the manifold and the metric induces a Jordan algebra Or, we can treat the two structures at the same time and say that the Kähler structure (metric+symplectic) induces a  C*-algebra (Poisson+Jordan). What seems remarkable to me is the representation of the C*-algebra. The representation is a Hilbert space by GNS construction. Magically, its projective space turns out to be the manifold we started with: Somehow, we go back from algebraic data to geometric data by an application of representation theory. Why is this true for ? Is it true for any Kähler manifold ? Answers and references are both appreciated.","\mathcal{M} \mathbb{CP}^n \omega g x,y [x,y] := \omega (dx,dy)\,, x\cdot y := g(dx,dy)+xy\,. \mathcal{H} \mathbb{P}\mathcal{H} = \mathcal{M}. \mathcal M=\mathbb{CP}^n \mathcal{M}","['functional-analysis', 'representation-theory', 'mathematical-physics', 'c-star-algebras', 'kahler-manifolds']"
7,An equivalent norm of $\|\cdot\|_\infty$,An equivalent norm of,\|\cdot\|_\infty,"Let $X$ be a compact topological space and $(E, |\cdot|)$ a Banach space. Let $\mathcal C$ be the space of all continuous functions from $X$ to $E$ . Let $\|\cdot\|_\infty$ be the supremum norm on $\mathcal C$ . Then $(\mathcal C, \|\cdot\|_\infty)$ is a Banach space . We fix a continuous map $g:X \to (0, \infty)$ and define a new norm $[\cdot]$ on $\mathcal C$ by $$ [f] := \sup_{x\in X} g(x) |f(x)| \quad \forall f \in \mathcal C. $$ Because $X$ is compact and $g$ continuous, there are $c_1, c_2 >0$ such that $c_1 \le |g(x)| \le c_2$ for all $x \in X$ . As such, $$ c_1 \|\cdot\|_\infty \le [\cdot] \le c_2 \|\cdot\|_\infty. $$ It follows that $[\cdot]$ is equivalent to $\|\cdot\|_\infty$ . Hence $(\mathcal C, [\cdot])$ is a Banach space. Could you confirm if my above understanding is correct?","Let be a compact topological space and a Banach space. Let be the space of all continuous functions from to . Let be the supremum norm on . Then is a Banach space . We fix a continuous map and define a new norm on by Because is compact and continuous, there are such that for all . As such, It follows that is equivalent to . Hence is a Banach space. Could you confirm if my above understanding is correct?","X (E, |\cdot|) \mathcal C X E \|\cdot\|_\infty \mathcal C (\mathcal C, \|\cdot\|_\infty) g:X \to (0, \infty) [\cdot] \mathcal C 
[f] := \sup_{x\in X} g(x) |f(x)| \quad \forall f \in \mathcal C.
 X g c_1, c_2 >0 c_1 \le |g(x)| \le c_2 x \in X 
c_1 \|\cdot\|_\infty \le [\cdot] \le c_2 \|\cdot\|_\infty.
 [\cdot] \|\cdot\|_\infty (\mathcal C, [\cdot])","['functional-analysis', 'banach-spaces']"
8,Significance of Hahn-Banach theorem.,Significance of Hahn-Banach theorem.,,"I am doing a course on functional analysis. One of the pillars in functional analysis is Hahn-Banach theorem. But in different books I find different versions of the theorem and it is not easy to check that they are equivalent. So, I want to understand why Hahn-Banach theorem is important. Also the proof uses Zorn's lemma which is equivalent to axiom of choice, so I cannot understand it by concrete examples. The statement given by our instructor for Hahn-Banach theorem is as follows: Let $X$ be a normed linear space and $X_0$ be a subspace of $X$ ,let $f_0$ be a bounded linear functional on $X_0$ ,then there is a bounded linear functional $f$ on $X$ such that $f|_{X_0}=f_0$ and $||f||=||f_0||$ i.e. $\sup\limits_{||x||=1,x\in X}|f(x)|=\sup\limits_{||x||=1,x\in X_0}|f_0(x)|$ . I want to understand the usefulness of the fact that $||f||=||f_0||,$ i.e. its physical significance and why it is good to have such an extension. Can someone give me some idea?","I am doing a course on functional analysis. One of the pillars in functional analysis is Hahn-Banach theorem. But in different books I find different versions of the theorem and it is not easy to check that they are equivalent. So, I want to understand why Hahn-Banach theorem is important. Also the proof uses Zorn's lemma which is equivalent to axiom of choice, so I cannot understand it by concrete examples. The statement given by our instructor for Hahn-Banach theorem is as follows: Let be a normed linear space and be a subspace of ,let be a bounded linear functional on ,then there is a bounded linear functional on such that and i.e. . I want to understand the usefulness of the fact that i.e. its physical significance and why it is good to have such an extension. Can someone give me some idea?","X X_0 X f_0 X_0 f X f|_{X_0}=f_0 ||f||=||f_0|| \sup\limits_{||x||=1,x\in X}|f(x)|=\sup\limits_{||x||=1,x\in X_0}|f_0(x)| ||f||=||f_0||,","['functional-analysis', 'normed-spaces', 'intuition', 'motivation', 'hahn-banach-theorem']"
9,Venn diagram for basic types of operators,Venn diagram for basic types of operators,,"This Venn diagram is an attempt to visually classify densely-defined linear operators between Banach spaces (self-adjoint operators are an exception, defined between Hilbert spaces). Operators are first divided between the bounded (B) in the left box and the unbounded (uB) in the right box. Red curve contains closable operators, green ellipse closed operators, grey circle compact operators and blue ellipse self-adjoint operators. Preliminary remarks: bounded operators are closable (Bounded Linear Transformation theorem) compact operators are bounded ( Show that a compact operator is bounded ) self-adjoint operators are closed ( Why is every selfadjoint operator closed? ) Comments for specific subdomains: cf. https://mathoverflow.net/q/47418 e.g. derivative d/dx defined on $C^\infty([a,b]) \subseteq C^0([a,b])$ e.g. https://math.stackexchange.com/a/4170649/142518 e.g. Showing derivative operator is self-adjoint e.g. Prove that $T$ is not compact e.g. Examples of self adjoint compact operators on Hilbert spaces e.g. multiplication with $i$ on finite dimensional complex normed space? e.g. any linear bounded functional with proper dense domain? e.g. multiplication with $i$ on any infinite dimensional complex Banach space? e.g. identity restricted to proper, dense domain of an infinite dimensional Banach space? Is this classification correct? Are there any better, more instructive examples?","This Venn diagram is an attempt to visually classify densely-defined linear operators between Banach spaces (self-adjoint operators are an exception, defined between Hilbert spaces). Operators are first divided between the bounded (B) in the left box and the unbounded (uB) in the right box. Red curve contains closable operators, green ellipse closed operators, grey circle compact operators and blue ellipse self-adjoint operators. Preliminary remarks: bounded operators are closable (Bounded Linear Transformation theorem) compact operators are bounded ( Show that a compact operator is bounded ) self-adjoint operators are closed ( Why is every selfadjoint operator closed? ) Comments for specific subdomains: cf. https://mathoverflow.net/q/47418 e.g. derivative d/dx defined on e.g. https://math.stackexchange.com/a/4170649/142518 e.g. Showing derivative operator is self-adjoint e.g. Prove that $T$ is not compact e.g. Examples of self adjoint compact operators on Hilbert spaces e.g. multiplication with on finite dimensional complex normed space? e.g. any linear bounded functional with proper dense domain? e.g. multiplication with on any infinite dimensional complex Banach space? e.g. identity restricted to proper, dense domain of an infinite dimensional Banach space? Is this classification correct? Are there any better, more instructive examples?","C^\infty([a,b]) \subseteq C^0([a,b]) i i","['functional-analysis', 'big-list', 'compact-operators', 'unbounded-operators']"
10,Polar decomposition for unbounded operators,Polar decomposition for unbounded operators,,"If $T$ is a closed, densely defined linear transformation on a Hilbert space $\mathcal{H}$ , then $T$ has a polar decomposition $T=V(T^{*}T)^{\frac{1}{2}}$ where $V$ is the partial isometry defined by extending the map $(T^{*}T)^{\frac{1}{2}}x\mapsto Tx$ for $x\in D(T^{*}T)$ (where $D(T^{*}T)$ is the domain of $T^{*}T$ and a core for both $T$ and $(T^{*}T)^{\frac{1}{2}}$ ). Since $V$ is bounded it follows that $T^{*}=(T^{*}T)^{\frac{1}{2}}V^{*}$ , and hence $TT^{*}=VT^{*}TV^{*}$ . What I do not understand is the following claim: $T^{*}T$ restricted to the closure of the range of $T^{*}$ is unitarily equivalent to $TT^{*}$ restricted to the closure of the range of $T$ and $V$ implements this equivalence. Specifically, I do not see how it can possibly be true that if $Tx_{n}\rightarrow y$ , then $y\in D(TT^{*})$ (or even that $y\in D(T^{*})$ ). Since $TT^{*}$ is closed, this amounts to showing that $TT^{*}Tx_{n}$ has a limit in $\mathcal{H}$ . Since it is not assumed that $y$ is in the domain of any of these operators, it doesn't seem to me that this can possibly be shown. Edit: I don't even think it's true that $Tx\in D(T^{*})$ for every $x\in D(T)$ . This goes against everything I know about unbounded operators. If this were always true, then  there would be no point in stipulating that $x\in D(T^{*}T)$ when you could just write $x\in D(T)$ .","If is a closed, densely defined linear transformation on a Hilbert space , then has a polar decomposition where is the partial isometry defined by extending the map for (where is the domain of and a core for both and ). Since is bounded it follows that , and hence . What I do not understand is the following claim: restricted to the closure of the range of is unitarily equivalent to restricted to the closure of the range of and implements this equivalence. Specifically, I do not see how it can possibly be true that if , then (or even that ). Since is closed, this amounts to showing that has a limit in . Since it is not assumed that is in the domain of any of these operators, it doesn't seem to me that this can possibly be shown. Edit: I don't even think it's true that for every . This goes against everything I know about unbounded operators. If this were always true, then  there would be no point in stipulating that when you could just write .",T \mathcal{H} T T=V(T^{*}T)^{\frac{1}{2}} V (T^{*}T)^{\frac{1}{2}}x\mapsto Tx x\in D(T^{*}T) D(T^{*}T) T^{*}T T (T^{*}T)^{\frac{1}{2}} V T^{*}=(T^{*}T)^{\frac{1}{2}}V^{*} TT^{*}=VT^{*}TV^{*} T^{*}T T^{*} TT^{*} T V Tx_{n}\rightarrow y y\in D(TT^{*}) y\in D(T^{*}) TT^{*} TT^{*}Tx_{n} \mathcal{H} y Tx\in D(T^{*}) x\in D(T) x\in D(T^{*}T) x\in D(T),"['functional-analysis', 'operator-algebras']"
11,Characterizing the boundary of image of $f:\mathbb{R}^n\to\mathbb{R}^n$,Characterizing the boundary of image of,f:\mathbb{R}^n\to\mathbb{R}^n,"Note: The original question asked how to prove that $f$ maps interior points to interior points when it has a positive Jacobian. This was a silly question as it's an immediate consequence of the Inverse Function Theorem as pointed out in the comment. I modified the question to better reflect the true nature of the issue. Consider a differentiable map $f$ with positive definite Jacobian: $$f(x,y):\Omega = [0,\infty)\times Y\longrightarrow\mathbb{R}^{m+1}, \text{ where } Y\subseteq\mathbb{R}^{m}\,\, \text{is compact, simply connected}.$$ My question is how to accurately describe the boundary of the image of $f.$ We know that from the Inverse Function Theorem that: $$F(\partial\Omega)\subseteq\partial F(\Omega).$$ But it seems that ""new"" boundary can form, if we look at the limit: $$g(y) = \lim\limits_{x\to\infty} f(x,y).$$ This limit exists and finite for any $y\in Y$ and for the particular problem I am working on, $g(y)$ turns out to be piecewise constant i.e.,: $$g(y) = \sum\limits_{s\in S}a_s\mathbf{1}_{\{X_s\}}(y)$$ where $Y = \cup_{s\in S}Y_s$ is a finite partition of $Y.$ So I am tempted to conclude: $$\partial F(\Omega) = F(\partial\Omega)\cup\{a_s\vert\ s\in S \}.$$ However, numerical simulation strongly suggests that the line segments (or hyperplanes) joining the points $a_s$ also lie on the boundary of image of $f.$ So at this point, I want to prove something like for two limit points $a_s,a_t$ : $$\forall\alpha\in[0,1]: \alpha a_s + (1-\alpha)a_t\in\partial F(\Omega).$$ But none of the usual theorems about diffeomorphism I find is of direct help, as they deal with open sets or they are about holomorphic functions. However, my $\Omega$ is not open and my function is real. Can someone point me to a nice reference that deals with results like this?","Note: The original question asked how to prove that maps interior points to interior points when it has a positive Jacobian. This was a silly question as it's an immediate consequence of the Inverse Function Theorem as pointed out in the comment. I modified the question to better reflect the true nature of the issue. Consider a differentiable map with positive definite Jacobian: My question is how to accurately describe the boundary of the image of We know that from the Inverse Function Theorem that: But it seems that ""new"" boundary can form, if we look at the limit: This limit exists and finite for any and for the particular problem I am working on, turns out to be piecewise constant i.e.,: where is a finite partition of So I am tempted to conclude: However, numerical simulation strongly suggests that the line segments (or hyperplanes) joining the points also lie on the boundary of image of So at this point, I want to prove something like for two limit points : But none of the usual theorems about diffeomorphism I find is of direct help, as they deal with open sets or they are about holomorphic functions. However, my is not open and my function is real. Can someone point me to a nice reference that deals with results like this?","f f f(x,y):\Omega = [0,\infty)\times Y\longrightarrow\mathbb{R}^{m+1}, \text{ where } Y\subseteq\mathbb{R}^{m}\,\, \text{is compact, simply connected}. f. F(\partial\Omega)\subseteq\partial F(\Omega). g(y) = \lim\limits_{x\to\infty} f(x,y). y\in Y g(y) g(y) = \sum\limits_{s\in S}a_s\mathbf{1}_{\{X_s\}}(y) Y = \cup_{s\in S}Y_s Y. \partial F(\Omega) = F(\partial\Omega)\cup\{a_s\vert\ s\in S \}. a_s f. a_s,a_t \forall\alpha\in[0,1]: \alpha a_s + (1-\alpha)a_t\in\partial F(\Omega). \Omega","['real-analysis', 'functional-analysis', 'manifolds', 'differential-topology', 'jacobian']"
12,"The miraculous nature of the ""matrix coefficients"" $\langle Tv,v \rangle$ (especially in the context of positive type functions)","The miraculous nature of the ""matrix coefficients""  (especially in the context of positive type functions)","\langle Tv,v \rangle","$\newcommand{\ak}[1]{\langle #1 \rangle}$ I've noticed that for linear operators $T$ and an inner product $ \ak{\bullet, \bullet }$ , the expression $\ak{ Tv,v}$ tends to show up a lot. For instance, it shows up in the min-max theorem to tell us about all the eigenvalues of a sufficiently nice operator. Similarly the expression $\ak{Tx,y}$ (and its dual) appears in the very definition of self-adjointness (which appears by mixing together the Banach space definition of a dual operator + the Riesz representation theorem for Hilbert spaces). I know in the finite dimensional case they correspond to quadratic forms $x^\top A x$ , which appear in an absurd number of scenarios (see Why Study Quadratic Forms? ), in particular optimization (2nd order coefficient of Taylor expansion), all the nice properties of definite matrices , and in number theory (starting from the very beginning of number theory , essentially motivating the entire field of algebraic number theory, and continuing to be very well regarded in the 21st century and beyond ). Even in ""abstract harmonic analysis"",  we have such expressions popping up: for instance the fact that the "" matrix coefficients "" $\phi(x):= \ak{\pi(x)u,u}$ for some unitary representation of a locally compact Hausdorff group $G$ are EXACTLY the functions of positive type on $G$ (Prop. 3.15 in Folland's Abstract Harmonic Analysis ) plays a crucial role in (Folland's proof of) the Gelfand-Raikov theorem. In the abstract harmonic analysis case, one can (slightly) motivate this by saying that $\pi(x)$ is a complicated object, namely a unitary transformation of a Hilbert space; evaluating at some $u\in \cal H$ produces a vector $\pi(x)u$ which is less complicated, but still not as easy a complex number $\ak{\pi(x)u,v}$ . Moreover since ""comparing"" $\pi(x)u$ against all vectors doesn't lose any information, so if one can understand $\ak{\pi(x)u,v}$ for all $u,v\in \cal H$ , one can understand $\pi(x)$ (this is essentially the philosophy of the weak integral, which Folland uses to define $\pi(f) : L^1(G) \to \cal B(H)$ in $\S3.2$ ). Finally, by some sort of polarization identity, one can recover $\ak{\pi(x)u,v}$ from $\ak{\pi(x)u,u}$ , so it suffices to understand those. I can accept this motivation, but I can't accept the miraculous fact that such inner products $\ak{Tv,v}$ behave so nicely. E.g. in the above paragraph they produce positive type functions, which are very related (Prop. 3.35 in Folland) to positive definite matrices (which themselves are very related to such inner products); and as mentioned above they lead to amazing formulas for eigenvalues, nice theorems for optimization, and deep connections to algebraic number theory. Why should this vague notion of ""comparing a vector to its transformed self"" result in such a profound variety of interesting mathematics? A ""perfect answer"" could lie along the lines of someone telling a somewhat cohesive and comprehensive general story about why and when these inner products $\ak{Tv,v}$ appear, to the extent that the special case of the consideration functions $\phi_u(x):=\ak{\pi(x)u,u}$ and their (1-1) connection to positive type functions/""positive definite functions"" is no longer a miraculous leap of thought, but is instead met with ""of course that's what one'd do!"". EDIT 4/30/23: they also have a physical interpretation in quantum mechanics https://physics.stackexchange.com/questions/146005/why-is-the-measured-value-of-some-observable-a-always-an-eigenvalue-of-the-co as the ""expectation of an observable $A$ under the state $\psi$ "".","I've noticed that for linear operators and an inner product , the expression tends to show up a lot. For instance, it shows up in the min-max theorem to tell us about all the eigenvalues of a sufficiently nice operator. Similarly the expression (and its dual) appears in the very definition of self-adjointness (which appears by mixing together the Banach space definition of a dual operator + the Riesz representation theorem for Hilbert spaces). I know in the finite dimensional case they correspond to quadratic forms , which appear in an absurd number of scenarios (see Why Study Quadratic Forms? ), in particular optimization (2nd order coefficient of Taylor expansion), all the nice properties of definite matrices , and in number theory (starting from the very beginning of number theory , essentially motivating the entire field of algebraic number theory, and continuing to be very well regarded in the 21st century and beyond ). Even in ""abstract harmonic analysis"",  we have such expressions popping up: for instance the fact that the "" matrix coefficients "" for some unitary representation of a locally compact Hausdorff group are EXACTLY the functions of positive type on (Prop. 3.15 in Folland's Abstract Harmonic Analysis ) plays a crucial role in (Folland's proof of) the Gelfand-Raikov theorem. In the abstract harmonic analysis case, one can (slightly) motivate this by saying that is a complicated object, namely a unitary transformation of a Hilbert space; evaluating at some produces a vector which is less complicated, but still not as easy a complex number . Moreover since ""comparing"" against all vectors doesn't lose any information, so if one can understand for all , one can understand (this is essentially the philosophy of the weak integral, which Folland uses to define in ). Finally, by some sort of polarization identity, one can recover from , so it suffices to understand those. I can accept this motivation, but I can't accept the miraculous fact that such inner products behave so nicely. E.g. in the above paragraph they produce positive type functions, which are very related (Prop. 3.35 in Folland) to positive definite matrices (which themselves are very related to such inner products); and as mentioned above they lead to amazing formulas for eigenvalues, nice theorems for optimization, and deep connections to algebraic number theory. Why should this vague notion of ""comparing a vector to its transformed self"" result in such a profound variety of interesting mathematics? A ""perfect answer"" could lie along the lines of someone telling a somewhat cohesive and comprehensive general story about why and when these inner products appear, to the extent that the special case of the consideration functions and their (1-1) connection to positive type functions/""positive definite functions"" is no longer a miraculous leap of thought, but is instead met with ""of course that's what one'd do!"". EDIT 4/30/23: they also have a physical interpretation in quantum mechanics https://physics.stackexchange.com/questions/146005/why-is-the-measured-value-of-some-observable-a-always-an-eigenvalue-of-the-co as the ""expectation of an observable under the state "".","\newcommand{\ak}[1]{\langle #1 \rangle} T  \ak{\bullet, \bullet } \ak{ Tv,v} \ak{Tx,y} x^\top A x \phi(x):= \ak{\pi(x)u,u} G G \pi(x) u\in \cal H \pi(x)u \ak{\pi(x)u,v} \pi(x)u \ak{\pi(x)u,v} u,v\in \cal H \pi(x) \pi(f) : L^1(G) \to \cal B(H) \S3.2 \ak{\pi(x)u,v} \ak{\pi(x)u,u} \ak{Tv,v} \ak{Tv,v} \phi_u(x):=\ak{\pi(x)u,u} A \psi","['functional-analysis', 'inner-products', 'harmonic-analysis', 'positive-definite', 'big-picture']"
13,Smoothness of the function $\lambda \mapsto \inf_{x \in \mathcal{C}}\|\lambda x - y\|^2$,Smoothness of the function,\lambda \mapsto \inf_{x \in \mathcal{C}}\|\lambda x - y\|^2,"Let $y\in \mathbb{R}^d$ and suppose that $\mathcal{C}\subset \mathbb{R}^d$ is a compact convex set. I want to know if the function $\psi\colon\lambda \mapsto \inf_{x \in \mathcal{C}}\|\lambda x - y\|^2$ is smooth/Lipschitz-continuous on the interval $[0,1]$ in the sense that $\psi$ is differentiable and there exists some $L>0$ (that may depend on $\mathcal{C}$ and $y$ ) such that \begin{align} |\psi'(\lambda)- \psi'(\gamma)|\leq L |\lambda - \gamma|, \quad \text{for all } \lambda, \gamma \in [0,1]. \end{align} What I know: The function $\psi$ can be written as the (squared) perspective transform of the function $\varphi\colon y \mapsto \inf_{x\in \mathcal{C}}\|x - y\|$ . That is, $\psi(\lambda)= (\lambda\cdot  \varphi(y/\lambda))^2$ . This implies that I) $\psi$ is convex since the perspective transform of a convex function is convex and the composition of a positive convex function with $w \mapsto w^2$ is convex; and II) that $\psi$ is differentiable on $(0,1]$ , since $\varphi^2$ is differentiable. The latter follows by the fact that $\varphi^2$ is the Moreau-Yosida envelope of the indicator function of the set $\mathcal{C}$ , which Moreau showed is $C^1$ . Ultimate goal: My ultimate goal is to be able to numerically find the minimum of the function $\Theta\colon \lambda \mapsto\lambda^2 + \psi(\lambda)$ in the interval $[0,1]$ very efficiently (for different $y$ 's). The function $\Theta$ is 1-strongly convex (since $\psi$ is convex). But it seems that I am missing smoothness/Lipschitz-continuity to be able to show very fast (e.g. linear) convergence to the minimum using known optimization algorithms.","Let and suppose that is a compact convex set. I want to know if the function is smooth/Lipschitz-continuous on the interval in the sense that is differentiable and there exists some (that may depend on and ) such that What I know: The function can be written as the (squared) perspective transform of the function . That is, . This implies that I) is convex since the perspective transform of a convex function is convex and the composition of a positive convex function with is convex; and II) that is differentiable on , since is differentiable. The latter follows by the fact that is the Moreau-Yosida envelope of the indicator function of the set , which Moreau showed is . Ultimate goal: My ultimate goal is to be able to numerically find the minimum of the function in the interval very efficiently (for different 's). The function is 1-strongly convex (since is convex). But it seems that I am missing smoothness/Lipschitz-continuity to be able to show very fast (e.g. linear) convergence to the minimum using known optimization algorithms.","y\in \mathbb{R}^d \mathcal{C}\subset \mathbb{R}^d \psi\colon\lambda \mapsto \inf_{x \in \mathcal{C}}\|\lambda x - y\|^2 [0,1] \psi L>0 \mathcal{C} y \begin{align}
|\psi'(\lambda)- \psi'(\gamma)|\leq L |\lambda - \gamma|, \quad \text{for all } \lambda, \gamma \in [0,1].
\end{align} \psi \varphi\colon y \mapsto \inf_{x\in \mathcal{C}}\|x - y\| \psi(\lambda)= (\lambda\cdot  \varphi(y/\lambda))^2 \psi w \mapsto w^2 \psi (0,1] \varphi^2 \varphi^2 \mathcal{C} C^1 \Theta\colon \lambda \mapsto\lambda^2 + \psi(\lambda) [0,1] y \Theta \psi","['functional-analysis', 'convex-analysis', 'lipschitz-functions', 'smooth-functions']"
14,Is every bounded linear functional on a subspace of a Hilbert space given by a function?,Is every bounded linear functional on a subspace of a Hilbert space given by a function?,,"This question comes from the Limiting Absorption Principle (LAP). I want to obtain the most general statement possible, so I proceed as follows. Let $M$ be a topological space, $(H, \langle \cdot, \cdot \rangle)$ a Hilbert space, and $\mathcal L(D, H)$ the space of linear operators from the dense domain $D \subset H$ into $H$ (not necessarily bounded). Suppose $P : M \to \mathcal L(D, H)$ is continuous in the sense that $P(x) - P(y)$ is a bounded operator and its norm is continuous over $M \times M$ . Assume further that $P(x)$ has a bounded inverse for all $x$ in a subset $\Omega \subset M$ . This inverse may be locally bounded, but it may blow up close to the boundary $\partial \Omega$ . To remedy this, we look for a Banach subspace $Y \subset H$ with a stronger norm $||\cdot ||_Y$ than that of $H$ . By the canonical identification of $H = H ^*$ , we may regard $H$ as a subset of $Y^*$ , and, if we picked the right subspace, we will have that $\sup_{x \in \Omega} ||P(x)^{-1}||_{Y \to Y ^*} < \infty$ . Now, for any $u \in Y$ , the sequence $u_n = P(x_n)^{-1} u$ is bounded in $Y^*$ , so the Banach-Alaoglu theorem furnishes a weak* limit $u_\infty \in Y^*$ s.t. $u_n(y) \to u_\infty(y)$ for all $y \in Y$ . Since $u_\infty$ is a bounded linear functional on the subspace $Y$ , the Hahn-Banach theorem furnishes us an extension $\tilde u_\infty$ of $u_\infty$ to all of $H$ . By the Riesz Representation theorem, $\tilde u_\infty(y) = \langle g, y\rangle$ for some $g \in H$ . If we show that this limit is unique, this $g$ would be a reasonable extension of $P^{-1}$ to a point on the boundary of $\Omega$ . In the classical LAP, $M = \Bbb C$ , $H = L^2(\Bbb R)$ , $P(x)$ would be something like $-\Delta + x$ , with $\Omega = \Bbb C \setminus [0, \infty)$ , and $Y$ an appropriately weighted $L^2$ space. The problem is that in this case, the extension of the resolvent $P(x)^{-1}$ to the real line is a map into the dual of $Y$ (which is just another weighted $L^2$ space with in inverse weight). This evinces a flaw in my previous argument because according to the above paragraph, the extension of the resolvent should take values in the same Hilbert space $H = L^2(\Bbb R)$ . What am I doing wrong?","This question comes from the Limiting Absorption Principle (LAP). I want to obtain the most general statement possible, so I proceed as follows. Let be a topological space, a Hilbert space, and the space of linear operators from the dense domain into (not necessarily bounded). Suppose is continuous in the sense that is a bounded operator and its norm is continuous over . Assume further that has a bounded inverse for all in a subset . This inverse may be locally bounded, but it may blow up close to the boundary . To remedy this, we look for a Banach subspace with a stronger norm than that of . By the canonical identification of , we may regard as a subset of , and, if we picked the right subspace, we will have that . Now, for any , the sequence is bounded in , so the Banach-Alaoglu theorem furnishes a weak* limit s.t. for all . Since is a bounded linear functional on the subspace , the Hahn-Banach theorem furnishes us an extension of to all of . By the Riesz Representation theorem, for some . If we show that this limit is unique, this would be a reasonable extension of to a point on the boundary of . In the classical LAP, , , would be something like , with , and an appropriately weighted space. The problem is that in this case, the extension of the resolvent to the real line is a map into the dual of (which is just another weighted space with in inverse weight). This evinces a flaw in my previous argument because according to the above paragraph, the extension of the resolvent should take values in the same Hilbert space . What am I doing wrong?","M (H, \langle \cdot, \cdot \rangle) \mathcal L(D, H) D \subset H H P : M \to \mathcal L(D, H) P(x) - P(y) M \times M P(x) x \Omega \subset M \partial \Omega Y \subset H ||\cdot ||_Y H H = H ^* H Y^* \sup_{x \in \Omega} ||P(x)^{-1}||_{Y \to Y ^*} < \infty u \in Y u_n = P(x_n)^{-1} u Y^* u_\infty \in Y^* u_n(y) \to u_\infty(y) y \in Y u_\infty Y \tilde u_\infty u_\infty H \tilde u_\infty(y) = \langle g, y\rangle g \in H g P^{-1} \Omega M = \Bbb C H = L^2(\Bbb R) P(x) -\Delta + x \Omega = \Bbb C \setminus [0, \infty) Y L^2 P(x)^{-1} Y L^2 H = L^2(\Bbb R)","['real-analysis', 'functional-analysis', 'mathematical-physics', 'spectral-theory']"
15,Uniqueness of the evaluation isomorphism as a natural isomorphism,Uniqueness of the evaluation isomorphism as a natural isomorphism,,"It is well known that the evaluation map between $V$ , a topological vector space, and $V^{\star\star}$ , the double dual, given by $\epsilon_{v} = f\mapsto f(v)$ is a natural isomorphism between the identity functor and the functor $(-)^{\star\star}$ in the category of real finite dimensional vector spaces or in the category of reflexive Banach spaces. Someone once, perhaps mistakenly, told me that a topological vector space is reflexive it is naturally isomorphic to its double dual. The definition that I recall is that such a space is reflexive if the evaluation map is an isomorphism. I want to know if the previous definition is definitely wrong in a certain sense: Is it possible that for some sub-category of topological vector spaces, there is a natural isomorphism between the identity functor and the double dual functor that does not correspond to the evaluation map?","It is well known that the evaluation map between , a topological vector space, and , the double dual, given by is a natural isomorphism between the identity functor and the functor in the category of real finite dimensional vector spaces or in the category of reflexive Banach spaces. Someone once, perhaps mistakenly, told me that a topological vector space is reflexive it is naturally isomorphic to its double dual. The definition that I recall is that such a space is reflexive if the evaluation map is an isomorphism. I want to know if the previous definition is definitely wrong in a certain sense: Is it possible that for some sub-category of topological vector spaces, there is a natural isomorphism between the identity functor and the double dual functor that does not correspond to the evaluation map?",V V^{\star\star} \epsilon_{v} = f\mapsto f(v) (-)^{\star\star},"['functional-analysis', 'category-theory']"
16,The category $\mathbf{Ban_1}$ equipped with a non-obvious functor $U_1: \mathbf{Ban_1} \to \mathbf{Set}$,The category  equipped with a non-obvious functor,\mathbf{Ban_1} U_1: \mathbf{Ban_1} \to \mathbf{Set},"Wikipedia says that For technical reasons , the category $\mathbf{Ban_1}$ of Banach spaces and linear contractions is often equipped not with the ""obvious"" forgetful functor but the functor $U_1: \mathbf{Ban_1} \to \mathbf{Set}$ which maps a Banach space to its (closed) unit ball. The category $\mathbf{Ban_1}$ , in view of the above functor, is given as an example of a concretizable category. What are these technical reasons ? The choice of the functor seems quite odd and non-obvious. It would be great if someone could throw some light, please!","Wikipedia says that For technical reasons , the category of Banach spaces and linear contractions is often equipped not with the ""obvious"" forgetful functor but the functor which maps a Banach space to its (closed) unit ball. The category , in view of the above functor, is given as an example of a concretizable category. What are these technical reasons ? The choice of the functor seems quite odd and non-obvious. It would be great if someone could throw some light, please!",\mathbf{Ban_1} U_1: \mathbf{Ban_1} \to \mathbf{Set} \mathbf{Ban_1},"['functional-analysis', 'category-theory', 'banach-spaces', 'functors']"
17,Resolvent of Dirichlet Laplacian via Fourier Transform,Resolvent of Dirichlet Laplacian via Fourier Transform,,"For $-\Delta$ considered as a self-adjoint operator on $L^2(\mathbb{R}^d)$ , one may write it's resolvent as the Fourier multiplier $g(\xi)=(|\xi|^2-z)^{-1}$ . Now let $\Omega\subset \mathbb{R}^d$ be a bounded domain and let $-\Delta^\Omega$ be the Dirichlet Laplacian on $\Omega$ , defined, for instance, via the Friedrichs Extension Theorem. Is there an expression for $(-\Delta^\Omega-z)^{-1}$ in terms of the operator $g(\xi)$ and some multiplication operator $f(x)$ ? If not, can we approximate $(-\Delta^\Omega-z)^{-1}$ by operators of the form $f(x)g(\xi)$ ? It seems that something like $f(x)g(\xi)f(x)$ might work for $f$ some bump function of $\Omega$ , but the fact that $-\Delta^\Omega$ has to be defined abstractly leaves me unsure of the details. My motivation is that I would like to prove that $-\Delta^\Omega$ has compact resolvent from the fact that an operator of the form $f(x)g(\xi)$ is compact if $f$ and $g$ vanish at $\infty$ . Since one usually proves that the resolvent is compact from the Rellich embedding theorem and one can prove this theorem from the above compactness criterion, I have reason to believe this should work.","For considered as a self-adjoint operator on , one may write it's resolvent as the Fourier multiplier . Now let be a bounded domain and let be the Dirichlet Laplacian on , defined, for instance, via the Friedrichs Extension Theorem. Is there an expression for in terms of the operator and some multiplication operator ? If not, can we approximate by operators of the form ? It seems that something like might work for some bump function of , but the fact that has to be defined abstractly leaves me unsure of the details. My motivation is that I would like to prove that has compact resolvent from the fact that an operator of the form is compact if and vanish at . Since one usually proves that the resolvent is compact from the Rellich embedding theorem and one can prove this theorem from the above compactness criterion, I have reason to believe this should work.",-\Delta L^2(\mathbb{R}^d) g(\xi)=(|\xi|^2-z)^{-1} \Omega\subset \mathbb{R}^d -\Delta^\Omega \Omega (-\Delta^\Omega-z)^{-1} g(\xi) f(x) (-\Delta^\Omega-z)^{-1} f(x)g(\xi) f(x)g(\xi)f(x) f \Omega -\Delta^\Omega -\Delta^\Omega f(x)g(\xi) f g \infty,"['functional-analysis', 'sobolev-spaces', 'spectral-theory', 'laplacian', 'greens-function']"
18,Is this a typo in Brezis's Ex 3.24?,Is this a typo in Brezis's Ex 3.24?,,"I'm doing Ex 3.24 in Brezis's book of Functional Analysis. The purpose of this exercise is to sketch part of the proof of Theorem 3.29 , i.e., if $E$ is a Banach space such that $B_{E}$ is metrizable with respect to $\sigma\left(E, E^{\star}\right)$ , then $E^{\star}$ is separable. Let $d(x, y)$ be a metric on $B_{E}$ that induces on $B_{E}$ the same topology as $\sigma\left(E, E^{\star}\right)$ . Set $$ U_{n}=\left\{x \in \color{blue}{B_{E}} ; d(x, 0)<\frac{1}{n}\right\} $$ Let $V_{n}$ be a neighborhood of $0$ for $\sigma\left(E, E^{\star}\right)$ such that $\color{blue}{V_{n} \subset U_{n}}$ . We may assume that $V_{n}$ has the form $$ V_{n}=\left\{x \in \color{blue}{E} ;|\langle f, x\rangle|<\varepsilon_{n} \quad \forall f \in \Phi_{n}\right\} $$ with $\varepsilon_{n}>0$ and $\Phi_{n} \subset E^{\star}$ is some finite subset. Let $D=\cup_{n=1}^{\infty} \Phi_{n}$ and let $F$ denote the vector space generated by $D$ . We claim that $F$ is dense in $E^{\star}$ with respect to the strong topology. Suppose, by contradiction, that $\overline{F} \neq E^{\star}$ . Prove that there exist some $\xi \in E^{\star \star}$ and some $f_{0} \in E^{\star}$ such that $$ \left\langle\xi, f_{0}\right\rangle>1, \quad\langle\xi, f\rangle=0 \quad \forall f \in F, \quad \text{ and }\quad\|\xi\|=1 . $$ Let $$ W=\left\{x \in B_{E} ;\left|\left\langle f_{0}, x\right\rangle\right|<\frac{1}{2}\right\} . $$ Prove that there is some integer $n_{0} \geq 1$ such that $V_{n_{0}} \subset W$ . Prove that there exists $x_{1} \in B_{E}$ such that $$ \left\{\begin{array}{l} \left|\left\langle f, x_{1}\right\rangle-\langle\xi, f\rangle\right|<\varepsilon_{n_{0}} \quad \forall f \in \Phi_{n_{0}} \\ \left|\left\langle f_{0}, x_{1}\right\rangle-\left\langle\xi, f_{0}\right\rangle\right|<\frac{1}{2} \end{array}\right. $$ Deduce that $x_{1} \in V_{n_{0}}$ and that $\left\langle f_{0}, x_{1}\right\rangle>\frac{1}{2}$ . Conclude. If $E$ is finite-dimensional the the weak topology coincides with the norm topology. Now consider the case $E$ is infinite dimensional. Then each weakly open set is unbounded , so the set $V_n$ defined by the author above is unbounded and thus $V_{n}$ can not be a subset of $U_{n}$ . Hence I think it should be $$ V_{n}=\left\{x \in \color{blue}{B_E} ;|\langle f, x\rangle|<\varepsilon_{n} \quad \forall f \in \Phi_{n}\right\} $$ Could you confirm if my observation is correct? I solve 3. as follows. It follows from $f_0 \notin \overline F$ that $f_0$ is linearly independent of any finite subset of $F$ . This implies $\bigcap_{f\in \Phi_{n_0}} \ker f \not \subseteq \ker f_0$ . This implies there is $0 \neq a \in \bigcap_{f\in \Phi_{n_0}} \ker f$ such that $a \notin \ker f_0$ . The we can pick $t\in \mathbb R$ such that $x_1 := ta$ satisfies the requirement. Could you confirm if this argument is fine?","I'm doing Ex 3.24 in Brezis's book of Functional Analysis. The purpose of this exercise is to sketch part of the proof of Theorem 3.29 , i.e., if is a Banach space such that is metrizable with respect to , then is separable. Let be a metric on that induces on the same topology as . Set Let be a neighborhood of for such that . We may assume that has the form with and is some finite subset. Let and let denote the vector space generated by . We claim that is dense in with respect to the strong topology. Suppose, by contradiction, that . Prove that there exist some and some such that Let Prove that there is some integer such that . Prove that there exists such that Deduce that and that . Conclude. If is finite-dimensional the the weak topology coincides with the norm topology. Now consider the case is infinite dimensional. Then each weakly open set is unbounded , so the set defined by the author above is unbounded and thus can not be a subset of . Hence I think it should be Could you confirm if my observation is correct? I solve 3. as follows. It follows from that is linearly independent of any finite subset of . This implies . This implies there is such that . The we can pick such that satisfies the requirement. Could you confirm if this argument is fine?","E B_{E} \sigma\left(E, E^{\star}\right) E^{\star} d(x, y) B_{E} B_{E} \sigma\left(E, E^{\star}\right) 
U_{n}=\left\{x \in \color{blue}{B_{E}} ; d(x, 0)<\frac{1}{n}\right\}
 V_{n} 0 \sigma\left(E, E^{\star}\right) \color{blue}{V_{n} \subset U_{n}} V_{n} 
V_{n}=\left\{x \in \color{blue}{E} ;|\langle f, x\rangle|<\varepsilon_{n} \quad \forall f \in \Phi_{n}\right\}
 \varepsilon_{n}>0 \Phi_{n} \subset E^{\star} D=\cup_{n=1}^{\infty} \Phi_{n} F D F E^{\star} \overline{F} \neq E^{\star} \xi \in E^{\star \star} f_{0} \in E^{\star} 
\left\langle\xi, f_{0}\right\rangle>1, \quad\langle\xi, f\rangle=0 \quad \forall f \in F, \quad \text{ and }\quad\|\xi\|=1 .
 
W=\left\{x \in B_{E} ;\left|\left\langle f_{0}, x\right\rangle\right|<\frac{1}{2}\right\} .
 n_{0} \geq 1 V_{n_{0}} \subset W x_{1} \in B_{E} 
\left\{\begin{array}{l}
\left|\left\langle f, x_{1}\right\rangle-\langle\xi, f\rangle\right|<\varepsilon_{n_{0}} \quad \forall f \in \Phi_{n_{0}} \\
\left|\left\langle f_{0}, x_{1}\right\rangle-\left\langle\xi, f_{0}\right\rangle\right|<\frac{1}{2}
\end{array}\right.
 x_{1} \in V_{n_{0}} \left\langle f_{0}, x_{1}\right\rangle>\frac{1}{2} E E V_n V_{n} U_{n} 
V_{n}=\left\{x \in \color{blue}{B_E} ;|\langle f, x\rangle|<\varepsilon_{n} \quad \forall f \in \Phi_{n}\right\}
 f_0 \notin \overline F f_0 F \bigcap_{f\in \Phi_{n_0}} \ker f \not \subseteq \ker f_0 0 \neq a \in \bigcap_{f\in \Phi_{n_0}} \ker f a \notin \ker f_0 t\in \mathbb R x_1 := ta","['functional-analysis', 'banach-spaces', 'separable-spaces']"
19,Can two distinct $L^2$ functions have the same $L^2$ derivative?,Can two distinct  functions have the same  derivative?,L^2 L^2,"Take $f' = g' \in L^2$ for $f, g \in L^2 (\mathbb R)$ . It seems that taking the Fourier transform gives $f = g$ except on a set of measure zero (using the fact that $\hat{f'}(x) = ix \hat f(x)$ ) - does this make sense? I think it does because you can't translate a function up and maintain its $L^2$ integrability, but I was a bit confused because it seems unintuitive.","Take for . It seems that taking the Fourier transform gives except on a set of measure zero (using the fact that ) - does this make sense? I think it does because you can't translate a function up and maintain its integrability, but I was a bit confused because it seems unintuitive.","f' = g' \in L^2 f, g \in L^2 (\mathbb R) f = g \hat{f'}(x) = ix \hat f(x) L^2","['real-analysis', 'functional-analysis', 'fourier-analysis']"
20,Direct limits of topological vector spaces,Direct limits of topological vector spaces,,"It is sometimes useful in functional analysis to take direct limits of function spaces. For instance, the space $\mathcal{D}(\mathbf{R}^d)$ of test functions is the direct limit of the family of function spaces $C_c^\infty(K)$ in the category of locally convex topological vector spaces, i.e. such that a convex subset of $\mathcal{D}(\mathbf{R}^d)$ is open if and only if it's intersection with $C_c^\infty(K)$ is open for each compact set $K \subset \mathbf{R}^d$ . We can also consider the direct limit in the family of topological vector spaces. In this sense, any (not necessarily convex) subset of $\mathcal{D}(\mathbf{R}^d)$ will be open if it's intersection with each $C_c^\infty(K)$ is open. Are these topologies different? If so, what is a set which is open in the latter topology, but not the former. If these two topologies are the same, are there other examples of direct limits where the locally convex limit differs from the general limit?","It is sometimes useful in functional analysis to take direct limits of function spaces. For instance, the space of test functions is the direct limit of the family of function spaces in the category of locally convex topological vector spaces, i.e. such that a convex subset of is open if and only if it's intersection with is open for each compact set . We can also consider the direct limit in the family of topological vector spaces. In this sense, any (not necessarily convex) subset of will be open if it's intersection with each is open. Are these topologies different? If so, what is a set which is open in the latter topology, but not the former. If these two topologies are the same, are there other examples of direct limits where the locally convex limit differs from the general limit?",\mathcal{D}(\mathbf{R}^d) C_c^\infty(K) \mathcal{D}(\mathbf{R}^d) C_c^\infty(K) K \subset \mathbf{R}^d \mathcal{D}(\mathbf{R}^d) C_c^\infty(K),"['functional-analysis', 'distribution-theory']"
21,"If $f_n \overset{\star}{\rightharpoonup} f$ in $\sigma(E^\star, E)$, then $\|f\| \le \liminf \|f_n\|$","If  in , then","f_n \overset{\star}{\rightharpoonup} f \sigma(E^\star, E) \|f\| \le \liminf \|f_n\|","I'm trying to prove this result. Could you have a check on my proof? Let $(E, | \cdot|)$ be a normed linear space and $E^\star$ its topological dual. Let $\sigma(E^\star, E)$ be the weak $^\star$ topology on $E^\star$ . Let $f\in E^\star$ and $(f_n)$ be a sequence in $E^\star$ such that $f_n \overset{\star}{\rightharpoonup} f$ in $\sigma(E^\star, E)$ . Then $\|f\| \le  \liminf \|f_n\|$ . My attempt: Let $B := \{x \in X \mid |x|=1\}$ be the unit sphere. We have $\lim_n \langle f_n, x \rangle = \langle f, x \rangle$ for all $x\in X$ . So $$\sup_{x\in B} \lim_n \langle f_n, x \rangle = \sup_{x\in B} \langle f, x \rangle = \|f\|.$$ Hence it suffices to show that $$\sup_{x\in B} \lim_n \langle f_n, x \rangle \le \liminf_n \|f_n\|.$$ In fact, we have $\langle f_n, x \rangle \le \sup_{x\in B} \langle f_n, x \rangle = \|f_n\|$ and thus $$\lim_n \langle f_n, x \rangle = \liminf_n \langle f_n, x \rangle \le \liminf_n \|f_n\|.$$ The claim then follows by taking the supremum on both sides of above inequality.","I'm trying to prove this result. Could you have a check on my proof? Let be a normed linear space and its topological dual. Let be the weak topology on . Let and be a sequence in such that in . Then . My attempt: Let be the unit sphere. We have for all . So Hence it suffices to show that In fact, we have and thus The claim then follows by taking the supremum on both sides of above inequality.","(E, | \cdot|) E^\star \sigma(E^\star, E) ^\star E^\star f\in E^\star (f_n) E^\star f_n \overset{\star}{\rightharpoonup} f \sigma(E^\star, E) \|f\| \le  \liminf \|f_n\| B := \{x \in X \mid |x|=1\} \lim_n \langle f_n, x \rangle = \langle f, x \rangle x\in X \sup_{x\in B} \lim_n \langle f_n, x \rangle = \sup_{x\in B} \langle f, x \rangle = \|f\|. \sup_{x\in B} \lim_n \langle f_n, x \rangle \le \liminf_n \|f_n\|. \langle f_n, x \rangle \le \sup_{x\in B} \langle f_n, x \rangle = \|f_n\| \lim_n \langle f_n, x \rangle = \liminf_n \langle f_n, x \rangle \le \liminf_n \|f_n\|.","['functional-analysis', 'solution-verification', 'normed-spaces', 'dual-spaces', 'weak-topology']"
22,$E$ is a Banach space for the norm $\| \cdot \|_1$ where $\|x\|_{1} := \|x\|_{E}+\|T x\|_{F}$,is a Banach space for the norm  where,E \| \cdot \|_1 \|x\|_{1} := \|x\|_{E}+\|T x\|_{F},"I'm reading a proof of closed graph theorem in textbook Functional Analysis, Sobolev Spaces and Partial Differential Equations . Let $E$ and $F$ be two Banach spaces. Let $T$ be a linear operator from $E$ into $F .$ Assume that the graph of $T, G(T)$ , is closed in $E \times F$ . Then $T$ is continuous. Proof: Consider, on $E$ , the two norms $$ \|x\|_{1}=\|x\|_{E}+\|T x\|_{F} \quad \text { and } \quad\|x\|_{2}=\|x\|_{E} $$ It is easy to check, using the assumption that $G(T)$ is closed, that $E$ is a Banach space for the norm $\| \cdot \|_1$ . Could you verify if my proof, that $E$ is a Banach space for the norm $\| \cdot \|_1$ , is correct? My attempt: Let $(x_n)$ be a Cauchy sequence w.r.t. $\| \cdot \|_1$ , i.e., for every $\varepsilon >0$ , there is $N \in \mathbb N$ such that $$\|x_n -x_m \|_1 = \|x_n - x_m\|_{E}+\|T x_n -Tx_m\|_{F} < \varepsilon, \quad \forall m,n \ge N.$$ This implies $$\|x_n - x_m\|_{E} < \varepsilon \quad \text{and} \quad\|T x_n -Tx_m\|_{F} < \varepsilon, \quad \forall m,n \ge N.$$ Hence $(x_n)$ is a Cauchy sequence w.r.t. $\| \cdot \|_E$ , and $(Tx_n)$ a Cauchy sequence w.r.t. $\| \cdot \|_F$ . Because $E, F$ are complete. there are $x \in E, y\in F$ such that $x_n \to x$ w.r.t. $\| \cdot \|_E$ and $Tx_n \to y$ w.r.t. $\| \cdot \|_F$ . On the other hand, $(x_n, Tx_n)$ is a sequence in $G(T)$ such that $(x_n, Tx_n) \to (x, y)$ . This is because convergence in product topology is equivalent to component-wise convergence. Because $G(T)$ is closed, $(x, y) \in G(T)$ and thus $Tx = y$ . It follows that $Tx_n \to Tx$ w.r.t. $\| \cdot \|_F$ . Hence $x_n \to x$ w.r.t. $\| \cdot \|_1$ .","I'm reading a proof of closed graph theorem in textbook Functional Analysis, Sobolev Spaces and Partial Differential Equations . Let and be two Banach spaces. Let be a linear operator from into Assume that the graph of , is closed in . Then is continuous. Proof: Consider, on , the two norms It is easy to check, using the assumption that is closed, that is a Banach space for the norm . Could you verify if my proof, that is a Banach space for the norm , is correct? My attempt: Let be a Cauchy sequence w.r.t. , i.e., for every , there is such that This implies Hence is a Cauchy sequence w.r.t. , and a Cauchy sequence w.r.t. . Because are complete. there are such that w.r.t. and w.r.t. . On the other hand, is a sequence in such that . This is because convergence in product topology is equivalent to component-wise convergence. Because is closed, and thus . It follows that w.r.t. . Hence w.r.t. .","E F T E F . T, G(T) E \times F T E 
\|x\|_{1}=\|x\|_{E}+\|T x\|_{F} \quad \text { and } \quad\|x\|_{2}=\|x\|_{E}
 G(T) E \| \cdot \|_1 E \| \cdot \|_1 (x_n) \| \cdot \|_1 \varepsilon >0 N \in \mathbb N \|x_n -x_m \|_1 = \|x_n - x_m\|_{E}+\|T x_n -Tx_m\|_{F} < \varepsilon, \quad \forall m,n \ge N. \|x_n - x_m\|_{E} < \varepsilon \quad \text{and} \quad\|T x_n -Tx_m\|_{F} < \varepsilon, \quad \forall m,n \ge N. (x_n) \| \cdot \|_E (Tx_n) \| \cdot \|_F E, F x \in E, y\in F x_n \to x \| \cdot \|_E Tx_n \to y \| \cdot \|_F (x_n, Tx_n) G(T) (x_n, Tx_n) \to (x, y) G(T) (x, y) \in G(T) Tx = y Tx_n \to Tx \| \cdot \|_F x_n \to x \| \cdot \|_1","['functional-analysis', 'solution-verification', 'banach-spaces']"
23,Continuous bilinear maps on sections of vector bundles,Continuous bilinear maps on sections of vector bundles,,"Let $E \to M$ and $F \to N$ be two vector bundles over smooth manifolds $M, N$ . Denote by $\pi_1, \pi_2$ the projections of $M \times N$ to $M$ and $N$ , respectively. Equip the spaces of sections of a vector bundle with its usual Fréchet space structure to make it into locally convex vector spaces. If we denote by $E \boxtimes F := \pi_1^* E \otimes \pi_2^* F$ the external tensor product vector bundle over $M \times N$ , is there then a natural vector space isomorphism $$ \Gamma(E \boxtimes F)^* \cong \left( \Gamma(E) \otimes_{\mathbb{R}} \Gamma(F) \right)^*?$$ The dual on the left-hand side denotes the continuous dual of a locally convex vector space, the dual on the right-hand side denotes the space of jointly continuous, $\mathbb{R}$ -bilinear maps. I think this is folklore, but I cannot for the life of me find a source proving this precise result. Is there a canonical source to go to? Is this somewhere in Dieudonné's ""Eléments d'analyse""?","Let and be two vector bundles over smooth manifolds . Denote by the projections of to and , respectively. Equip the spaces of sections of a vector bundle with its usual Fréchet space structure to make it into locally convex vector spaces. If we denote by the external tensor product vector bundle over , is there then a natural vector space isomorphism The dual on the left-hand side denotes the continuous dual of a locally convex vector space, the dual on the right-hand side denotes the space of jointly continuous, -bilinear maps. I think this is folklore, but I cannot for the life of me find a source proving this precise result. Is there a canonical source to go to? Is this somewhere in Dieudonné's ""Eléments d'analyse""?","E \to M F \to N M, N \pi_1, \pi_2 M \times N M N E \boxtimes F := \pi_1^* E \otimes \pi_2^* F M \times N  \Gamma(E \boxtimes F)^* \cong \left( \Gamma(E) \otimes_{\mathbb{R}} \Gamma(F) \right)^*? \mathbb{R}","['functional-analysis', 'differential-geometry', 'vector-bundles', 'topological-vector-spaces', 'dual-spaces']"
24,Understanding Quantum Measurement in infinite dimensional systems,Understanding Quantum Measurement in infinite dimensional systems,,"I have been relearning quantum mechanics recently since I realized I got stuck in some misunderstanding of fundamentals when trying to solve certain problems. My question is actually simply the following: What is the relationship between the measurement postulate, stated in terms of projection valued measures, and the eigenvectors of observables in the case of an infinite dimensional Hilbert space? Now here is some context to maybe see where my confusion comes from: Most QM textbooks advertise that observables are postulated to be self-adjoint operators because the spectral theorem assures you that their eigenvectors form a basis, which is connected to the measurement postulate via the idea that when you expand the state of the system in the eigenvectors of an observable, the coefficients of this expansion are the probability amplitudes for the measurement of said observable. I was stuck with the idea that somehow you can always use the eigenvectors of some observable to decompose any state. However, now I know that this is only true for finite dimensional Hilbert spaces. On infinite dimensional Hilbert spaces, the eigenvectors of self-adjoint operators need not even be in the Hilbert space (for example, the momentum operator $\hat{p}:D(\hat{p})\rightarrow L^2(\mathbb{R})$ , $\hat{p}=-i\partial_x$ , which has a self-adjoint extention, has eigenvectors $\psi_p(x) = e^{ipx}$ , which aren't square integrable functions. Even so, while they do not really form a basis in the Hilbert space, they still play an important role (related to Fourier transforms). Now, measurement, formulated in a more rigorous manner, has to do with the actual spectral theorem, namely that for any self-adjoint operator there exists a projection-valued measure $P_A$ such that $A$ can be represented in terms of the Lebesgue-Stieltjes integral as $$A = \int_\mathbb{R} \lambda dP_A(\lambda)$$ Now from my understanding so far, PVMs on finite dimensional spaces and the projectors described in QM texbooks as say $P_\psi = |\psi><\psi|$ in Dirac notation (which again is a notation enabled by working with bases given by the eigenvectors of observables) are actually two sides of the same coin so to say. But I cannot seem to wrap my head around (or find any resource on the internet) about the infinite dimensional case. Here would be some of the questions I'm pondering regarding this: What is the connection between PVM and eigenvectors in the case of the momentum operator for example? Are eigenvectors of operators actually relevant in the infinite dimensional case in general? (or the momentum/position are some very special cases?) If the answer to 2 is affirmative, what happens to operators whose eigenvectors have discontinuities with infinite jumps? (one such an example I can think of is: $\hat{T}:D(T)\rightarrow \{f \in L^2(\left[0,2\pi\right])|f(0)=f(2\pi)\}$ , $\hat{T} = i(cos^2 (\theta) - 1/2)\frac{d}{d \theta} + i\, sin(\theta) cos(\theta)$ , for which there should be a self-adjoint extension, or rather some $D(T)$ for which it is self-adjoint - if I'm wrong scream at me, please. The primitive $\int \frac{sin(\theta)cos(\theta)}{cos^2(\theta)-1/2} d\theta$ diverges to $+\infty$ at the points $\theta_0\in\{\pi/4,\, 3\pi/4,\, 5\pi/4,\, 7\pi/4\}$ ). How would one treat the measurement of the operator described in 3.?","I have been relearning quantum mechanics recently since I realized I got stuck in some misunderstanding of fundamentals when trying to solve certain problems. My question is actually simply the following: What is the relationship between the measurement postulate, stated in terms of projection valued measures, and the eigenvectors of observables in the case of an infinite dimensional Hilbert space? Now here is some context to maybe see where my confusion comes from: Most QM textbooks advertise that observables are postulated to be self-adjoint operators because the spectral theorem assures you that their eigenvectors form a basis, which is connected to the measurement postulate via the idea that when you expand the state of the system in the eigenvectors of an observable, the coefficients of this expansion are the probability amplitudes for the measurement of said observable. I was stuck with the idea that somehow you can always use the eigenvectors of some observable to decompose any state. However, now I know that this is only true for finite dimensional Hilbert spaces. On infinite dimensional Hilbert spaces, the eigenvectors of self-adjoint operators need not even be in the Hilbert space (for example, the momentum operator , , which has a self-adjoint extention, has eigenvectors , which aren't square integrable functions. Even so, while they do not really form a basis in the Hilbert space, they still play an important role (related to Fourier transforms). Now, measurement, formulated in a more rigorous manner, has to do with the actual spectral theorem, namely that for any self-adjoint operator there exists a projection-valued measure such that can be represented in terms of the Lebesgue-Stieltjes integral as Now from my understanding so far, PVMs on finite dimensional spaces and the projectors described in QM texbooks as say in Dirac notation (which again is a notation enabled by working with bases given by the eigenvectors of observables) are actually two sides of the same coin so to say. But I cannot seem to wrap my head around (or find any resource on the internet) about the infinite dimensional case. Here would be some of the questions I'm pondering regarding this: What is the connection between PVM and eigenvectors in the case of the momentum operator for example? Are eigenvectors of operators actually relevant in the infinite dimensional case in general? (or the momentum/position are some very special cases?) If the answer to 2 is affirmative, what happens to operators whose eigenvectors have discontinuities with infinite jumps? (one such an example I can think of is: , , for which there should be a self-adjoint extension, or rather some for which it is self-adjoint - if I'm wrong scream at me, please. The primitive diverges to at the points ). How would one treat the measurement of the operator described in 3.?","\hat{p}:D(\hat{p})\rightarrow L^2(\mathbb{R}) \hat{p}=-i\partial_x \psi_p(x) = e^{ipx} P_A A A = \int_\mathbb{R} \lambda dP_A(\lambda) P_\psi = |\psi><\psi| \hat{T}:D(T)\rightarrow \{f \in L^2(\left[0,2\pi\right])|f(0)=f(2\pi)\} \hat{T} = i(cos^2 (\theta) - 1/2)\frac{d}{d \theta} + i\, sin(\theta) cos(\theta) D(T) \int \frac{sin(\theta)cos(\theta)}{cos^2(\theta)-1/2} d\theta +\infty \theta_0\in\{\pi/4,\, 3\pi/4,\, 5\pi/4,\, 7\pi/4\}","['functional-analysis', 'mathematical-physics', 'spectral-theory', 'quantum-mechanics']"
25,Metrically Cauchy vs. Topologically Cauchy,Metrically Cauchy vs. Topologically Cauchy,,"Let $X$ be a topological vector space. A family $x_i \in X, i\in I,$ is called a topologically Cauchy family if for every NH $U$ of zero, there exists $i_0\in I$ such that $$ i,j\geqslant i_0 \Rightarrow x_i-x_j \in U $$ If the topology on $X$ is induced by a metric $d$ , then the family is called metrically Cauchy if for every $\varepsilon >0$ , there exists $i_0\in I$ such that $$ i,j\geqslant i_0 \Rightarrow d(x_i,x_j)<\varepsilon. $$ Allegedly, neither implies the other in general, which I'm having a hard time understanding. The notions are equivalent if the metric is shift invariant, e.g a norm. Suppose $\tau _X$ is induced by a metric $d$ . Let $x_i\in X, i\in I,$ be metrically Cauchy. Let $B := B(0,\varepsilon)$ be a NH of zero. We can find $i_0\in I$ such that the metric condition is satisfied. But if the implication ""Metrically Cauchy $\Rightarrow$ Topologically Cauchy"" fails in general, that must mean $x_i-x_j\in B$ is not necessarily true for some $i,j\geqslant i_0$ . Conversely, if $x_i$ is topologically Cauchy and $\varepsilon >0$ , then $x_i-x_j\in B(0,\varepsilon)$ , but then again $d(x_i,x_j)< \varepsilon$ has to fail sometimes. What are some examples of non-shift-invariant metrics and families (sequences, even?) for which either implication fails? Is shift invariance necessary for the notions to be equivalent? Are there such topological vector spaces (whose topology is induced by a metric) such that exactly one implication is true?","Let be a topological vector space. A family is called a topologically Cauchy family if for every NH of zero, there exists such that If the topology on is induced by a metric , then the family is called metrically Cauchy if for every , there exists such that Allegedly, neither implies the other in general, which I'm having a hard time understanding. The notions are equivalent if the metric is shift invariant, e.g a norm. Suppose is induced by a metric . Let be metrically Cauchy. Let be a NH of zero. We can find such that the metric condition is satisfied. But if the implication ""Metrically Cauchy Topologically Cauchy"" fails in general, that must mean is not necessarily true for some . Conversely, if is topologically Cauchy and , then , but then again has to fail sometimes. What are some examples of non-shift-invariant metrics and families (sequences, even?) for which either implication fails? Is shift invariance necessary for the notions to be equivalent? Are there such topological vector spaces (whose topology is induced by a metric) such that exactly one implication is true?","X x_i \in X, i\in I, U i_0\in I  i,j\geqslant i_0 \Rightarrow x_i-x_j \in U  X d \varepsilon >0 i_0\in I  i,j\geqslant i_0 \Rightarrow d(x_i,x_j)<\varepsilon.  \tau _X d x_i\in X, i\in I, B := B(0,\varepsilon) i_0\in I \Rightarrow x_i-x_j\in B i,j\geqslant i_0 x_i \varepsilon >0 x_i-x_j\in B(0,\varepsilon) d(x_i,x_j)< \varepsilon","['real-analysis', 'functional-analysis', 'examples-counterexamples', 'topological-vector-spaces']"
26,Existence of measurable maximum map.,Existence of measurable maximum map.,,"Let $Y$ be metric compact, $X$ be a continuous image of complete separable metric space. Assume we have $f: Y \times X \to \mathbb{R}$ (s.a. $f$ is continuous w.r.t. the first argument and borel w.r.t. the second one). We want to prove that there is exist measurable $F : X \to Y$ s.a. $F(x)$ is maximum of $y \to f(y,x)$ for all $x$ . Intuitively we want to show that $\sup_y f(y, x)$ is measurable. But I don't know where should we use compactness of $Y$ ? Should it be covered with finite open sets, and then ...? Any hints. UPD. as it was mentioned, we can consider the following task as measurability of $\pi_X(\{(x,y): f(y,x) > \epsilon\})$ for all $\epsilon$ .","Let be metric compact, be a continuous image of complete separable metric space. Assume we have (s.a. is continuous w.r.t. the first argument and borel w.r.t. the second one). We want to prove that there is exist measurable s.a. is maximum of for all . Intuitively we want to show that is measurable. But I don't know where should we use compactness of ? Should it be covered with finite open sets, and then ...? Any hints. UPD. as it was mentioned, we can consider the following task as measurability of for all .","Y X f: Y \times X \to \mathbb{R} f F : X \to Y F(x) y \to f(y,x) x \sup_y f(y, x) Y \pi_X(\{(x,y): f(y,x) > \epsilon\}) \epsilon","['functional-analysis', 'measure-theory']"
27,How can we prove that a positive-frequency wave and its time-derivative cannot both have compact support in space?,How can we prove that a positive-frequency wave and its time-derivative cannot both have compact support in space?,,"Consider a complex-valued function of the form $$ \newcommand{\bfx}{\mathbf{x}} \newcommand{\bfk}{\mathbf{k}} f(t,\bfx)=\int d^Nk\ g(\bfk)\exp\big(-i\omega(\bfk)t-i\bfk\cdot\bfx\big) \tag{1} $$ where boldface denotes a list of $N$ real variables, the dot-product is defined as usual, and $$ \omega(\bfk)\equiv \sqrt{\strut{}1+\bfk\cdot\bfk}. \tag{2} $$ (I'm calling this a ""wave,"" but notice the constant term under the square root.) Suppose that $f(0,\bfx)$ is nonzero at least for some $\bfx$ . Can we choose $g(\bfk)$ so that $f(t,\bfx)$ and $df(t,\bfx)/dt$ both have compact support in $\bfx$ at $t=0$ ? The answer must be no, because otherwise I could use the Paley-Wiener theorem to construct a contradiction to the Reeh-Schlieder theorem . But that's a very indirect argument that uses relativistic quantum field theory, which surely isn't necessary for the simple question I'm asking here! How can we prove more directly that no such $g(\bfk)$ exists?","Consider a complex-valued function of the form where boldface denotes a list of real variables, the dot-product is defined as usual, and (I'm calling this a ""wave,"" but notice the constant term under the square root.) Suppose that is nonzero at least for some . Can we choose so that and both have compact support in at ? The answer must be no, because otherwise I could use the Paley-Wiener theorem to construct a contradiction to the Reeh-Schlieder theorem . But that's a very indirect argument that uses relativistic quantum field theory, which surely isn't necessary for the simple question I'm asking here! How can we prove more directly that no such exists?","
\newcommand{\bfx}{\mathbf{x}}
\newcommand{\bfk}{\mathbf{k}}
f(t,\bfx)=\int d^Nk\ g(\bfk)\exp\big(-i\omega(\bfk)t-i\bfk\cdot\bfx\big)
\tag{1}
 N 
\omega(\bfk)\equiv \sqrt{\strut{}1+\bfk\cdot\bfk}.
\tag{2}
 f(0,\bfx) \bfx g(\bfk) f(t,\bfx) df(t,\bfx)/dt \bfx t=0 g(\bfk)","['functional-analysis', 'partial-differential-equations', 'asymptotics', 'smooth-functions']"
28,Functional derivative of KL divergence in paper,Functional derivative of KL divergence in paper,,"I have been reading the following paper Black-Box Variational Inference as Distilled Langevin Dynamics . Very early on in the paper is equation one, which considers a base probability density $q_0 : \mathbb{R}^m\to\mathbb{R}_+$ that is transformed according to the change-of-variables formula under a smooth, invertible transformation $g:\mathbb{R}^m\to\mathbb{R}^m$ . Namely, let $q_{g}(\theta) = q(g^{-1}(\theta)) \vert\mathrm{det}(\nabla g^{-1}(\theta))\vert$ be the pushforward density of $q_0$ under the transformation $g$ . Let $p:\mathbb{R}^m\to\mathbb{R}_+$ be another probability density and consider the KL divergence between $q_{g}$ and $p$ : \begin{align} \mathrm{KL}(q_g\Vert p) &= \int q_g(x)\log\frac{q_g(x)}{p(x)}~\mathrm{d}x \\ &= \int q_0(\epsilon) \log \frac{q_g(g(\epsilon))}{p(g(\epsilon))} ~\mathrm{d}\epsilon \\ &= \int q_0(\epsilon) \log \frac{q_0(\epsilon) / \vert \mathrm{det}(\nabla g(\epsilon))\vert}{p(g(\epsilon))} ~\mathrm{d}\epsilon. \end{align} The paper claims that the functional derivative of the KL divergence with respect to the smooth transformation $g$ is $\frac{\delta}{\delta g}\mathrm{KL}(q_g\Vert p) = \nabla \log q_g(g(\cdot)) - \nabla \log p(g(\cdot))$ . I am having trouble deriving this and am looking for some help. Let's write the KL divergence as, \begin{align} \mathrm{KL}(q_g\Vert p) = \int q_0(\epsilon) \log q_0(\epsilon) ~\mathrm{d}\epsilon - \int q_0(\epsilon) \log \vert \mathrm{det}(\nabla g(\epsilon))\vert ~\mathrm{d}\epsilon - \int q_0(\epsilon) \log p(g(\epsilon)). \end{align} The first term has no dependency on $g$ and can therefore be ignored. Let's examine the third term and try to compute its functional derivative \begin{align} -\frac{\delta}{\delta g}\int q_0(\epsilon) \log p(g(\epsilon)) ~\mathrm{d}\epsilon = -q_0(\epsilon) \nabla \log p(g(\epsilon)). \end{align} This is nearly the same as the last term in the claimed functional derivative except that there is a factor of $q_0(\epsilon)$ present. This already makes me think I'm not proceeding along the correct path for the derivation.","I have been reading the following paper Black-Box Variational Inference as Distilled Langevin Dynamics . Very early on in the paper is equation one, which considers a base probability density that is transformed according to the change-of-variables formula under a smooth, invertible transformation . Namely, let be the pushforward density of under the transformation . Let be another probability density and consider the KL divergence between and : The paper claims that the functional derivative of the KL divergence with respect to the smooth transformation is . I am having trouble deriving this and am looking for some help. Let's write the KL divergence as, The first term has no dependency on and can therefore be ignored. Let's examine the third term and try to compute its functional derivative This is nearly the same as the last term in the claimed functional derivative except that there is a factor of present. This already makes me think I'm not proceeding along the correct path for the derivation.","q_0 : \mathbb{R}^m\to\mathbb{R}_+ g:\mathbb{R}^m\to\mathbb{R}^m q_{g}(\theta) = q(g^{-1}(\theta)) \vert\mathrm{det}(\nabla g^{-1}(\theta))\vert q_0 g p:\mathbb{R}^m\to\mathbb{R}_+ q_{g} p \begin{align}
\mathrm{KL}(q_g\Vert p) &= \int q_g(x)\log\frac{q_g(x)}{p(x)}~\mathrm{d}x \\
&= \int q_0(\epsilon) \log \frac{q_g(g(\epsilon))}{p(g(\epsilon))} ~\mathrm{d}\epsilon \\
&= \int q_0(\epsilon) \log \frac{q_0(\epsilon) / \vert \mathrm{det}(\nabla g(\epsilon))\vert}{p(g(\epsilon))} ~\mathrm{d}\epsilon.
\end{align} g \frac{\delta}{\delta g}\mathrm{KL}(q_g\Vert p) = \nabla \log q_g(g(\cdot)) - \nabla \log p(g(\cdot)) \begin{align}
\mathrm{KL}(q_g\Vert p) = \int q_0(\epsilon) \log q_0(\epsilon) ~\mathrm{d}\epsilon - \int q_0(\epsilon) \log \vert \mathrm{det}(\nabla g(\epsilon))\vert ~\mathrm{d}\epsilon - \int q_0(\epsilon) \log p(g(\epsilon)).
\end{align} g \begin{align}
-\frac{\delta}{\delta g}\int q_0(\epsilon) \log p(g(\epsilon)) ~\mathrm{d}\epsilon = -q_0(\epsilon) \nabla \log p(g(\epsilon)).
\end{align} q_0(\epsilon)","['functional-analysis', 'derivatives', 'information-theory']"
29,Extending Ky Fan's eigvenalues inequality to kernel operators,Extending Ky Fan's eigvenalues inequality to kernel operators,,"Base result The following result in Terry Tao's book, 'p. 47, Ky Fan inequality' reads as: $$\sum_i\lambda_i(A+B) \leq \sum_i \lambda_i(A) + \lambda_i(B)$$ where $A, B$ are Hermitian matrices and $\lambda_i(X)$ with $X$ Hermitian denotes the $i$ 'th eigenvalue of $X$ , such that w.l.o.g. $\lambda_1(X) \geq \dots  \geq \lambda_n(X).$ Going from Hermitian matrices to kernel operators Consider the operator induced by a stationary positive semi-definite kernel $k(x, y) := k(\Vert x - y \Vert)$ $$Tf(x) = \int_0^1 k(x,y) f(y) d\mu$$ with $T \in L^2([0, 1], \mu)$ where $\mu$ is some arbitrary measure. I'm interested in Mercer kernels, i.e. kernels which have an eigenfunction expansion of the form $$k(x, y) := \sum_i \lambda_i \phi_i(x) \phi_i(y)$$ where $\big \langle \phi_i, \phi_j \big\rangle_{L^2(\mu)} = \delta_{ij}$ , and $\big\langle k(x, \cdot), \phi_i \big\rangle_{L^2(\mu)} = \lambda_i \phi_i(x)$ , and the  eigenvalues $\lambda_i$ decay very fast (i.e. polynomial and exponential decay). Actual Question I feel that Ky Fan's inequality should apply directly to kernel operators as well, i.e. considering $k_1(\cdot, \cdot), k_2(\cdot, \cdot)$ as before, under what conditions can we say $$\sum_i \lambda_i(k_1 + k_2) \leq \sum_i \lambda_i(k_1) + \lambda_i(k_2) $$ as in the Hermitian matrix case?","Base result The following result in Terry Tao's book, 'p. 47, Ky Fan inequality' reads as: where are Hermitian matrices and with Hermitian denotes the 'th eigenvalue of , such that w.l.o.g. Going from Hermitian matrices to kernel operators Consider the operator induced by a stationary positive semi-definite kernel with where is some arbitrary measure. I'm interested in Mercer kernels, i.e. kernels which have an eigenfunction expansion of the form where , and , and the  eigenvalues decay very fast (i.e. polynomial and exponential decay). Actual Question I feel that Ky Fan's inequality should apply directly to kernel operators as well, i.e. considering as before, under what conditions can we say as in the Hermitian matrix case?","\sum_i\lambda_i(A+B) \leq \sum_i \lambda_i(A) + \lambda_i(B) A, B \lambda_i(X) X i X \lambda_1(X) \geq \dots  \geq \lambda_n(X). k(x, y) := k(\Vert x - y \Vert) Tf(x) = \int_0^1 k(x,y) f(y) d\mu T \in L^2([0, 1], \mu) \mu k(x, y) := \sum_i \lambda_i \phi_i(x) \phi_i(y) \big \langle \phi_i, \phi_j \big\rangle_{L^2(\mu)} = \delta_{ij} \big\langle k(x, \cdot), \phi_i \big\rangle_{L^2(\mu)} = \lambda_i \phi_i(x) \lambda_i k_1(\cdot, \cdot), k_2(\cdot, \cdot) \sum_i \lambda_i(k_1 + k_2) \leq \sum_i \lambda_i(k_1) + \lambda_i(k_2) ","['functional-analysis', 'inequality', 'eigenvalues-eigenvectors', 'spectral-theory', 'eigenfunctions']"
30,When can we write one function as a non-negative integral of another?,When can we write one function as a non-negative integral of another?,,"Let $f(x)$ and $g(x,y)$ be positive real-valued functions. It is often quite useful to write $$f(x) = \int g(x,y)\, {\rm d}\mu(y)$$ for some non-negative measure $\mu(y)$ . For example, this can be very useful for converting bounds on $g$ into bounds on $f$ . I'm interested in necessary and/or sufficient conditions for when this is possible. As a simple example, if $f$ , $g$ , and $\mu$ are nice enough, then if $g(x,y)$ has non-negative Fourier transform in $x$ for all $y$ , $f$ must also have non-negative Fourier transform as well. More generally, let $T_x$ be any linear functional satisfying $\int T_x g(x,y) \,{\rm d} \mu(y) = T_x f(x)$ . Then, if $T_x g(x,y)$ is non-negative for all $y$ , then $T_x f(x)$ must also be non-negative. Is this condition sufficient? Are there simpler necessary and/or sufficient conditions?","Let and be positive real-valued functions. It is often quite useful to write for some non-negative measure . For example, this can be very useful for converting bounds on into bounds on . I'm interested in necessary and/or sufficient conditions for when this is possible. As a simple example, if , , and are nice enough, then if has non-negative Fourier transform in for all , must also have non-negative Fourier transform as well. More generally, let be any linear functional satisfying . Then, if is non-negative for all , then must also be non-negative. Is this condition sufficient? Are there simpler necessary and/or sufficient conditions?","f(x) g(x,y) f(x) = \int g(x,y)\, {\rm d}\mu(y) \mu(y) g f f g \mu g(x,y) x y f T_x \int T_x g(x,y) \,{\rm d} \mu(y) = T_x f(x) T_x g(x,y) y T_x f(x)","['real-analysis', 'functional-analysis']"
31,When is the group $C^*$-algebra of a locally compact group an AF-algebra?,When is the group -algebra of a locally compact group an AF-algebra?,C^*,"It is known that the group $C^*$ -algebra of a compact group is an AF-algebra. I want to know if given a non-compact locally compact group $G$ , does there exist conditions on $G$ which imply that the (full or reduced) group $C^*$ -algebra of $G$ is an AF-algebra? Also, are there any known examples of non-compact locally compact groups whose group $C^*$ -algebra is an AF-algebra, and the group $C^*$ -algebra has been computed? Examples where the group is non-discrete would be of most interest.","It is known that the group -algebra of a compact group is an AF-algebra. I want to know if given a non-compact locally compact group , does there exist conditions on which imply that the (full or reduced) group -algebra of is an AF-algebra? Also, are there any known examples of non-compact locally compact groups whose group -algebra is an AF-algebra, and the group -algebra has been computed? Examples where the group is non-discrete would be of most interest.",C^* G G C^* G C^* C^*,"['functional-analysis', 'group-theory', 'operator-algebras', 'c-star-algebras', 'locally-compact-groups']"
32,"Is the set $S=\{ p(x) e^{-\alpha \|x\|^2} : p(x) \in \mathcal{P}, \alpha >0 \}$ dense in $L^2$ where $\mathcal{P}$ is a set of polynomials.",Is the set  dense in  where  is a set of polynomials.,"S=\{ p(x) e^{-\alpha \|x\|^2} : p(x) \in \mathcal{P}, \alpha >0 \} L^2 \mathcal{P}","Let $\mathcal{P}$ be the set of all realvalued polynomials on $\mathbb{R}^2$ and define $$S=\{ p(x) e^{-\alpha \|x\|^2}, x\in \mathbb{R}^2 : p \in \mathcal{P}, \alpha >0 \}.$$ I have two questions, one minor and one major: (Minor) Is $S$ is a dense subset of $L^2(\mathbb{R}^2)$ ? (Major) What are some minimal assumptions on $\mathcal{P}$ that guarantee that $S$ is dense? For example, is it enough to consider only all homogeneous polynomials plus a constant. I think the answer to the first question follows from here where this result was shown for a one-dimensional case.","Let be the set of all realvalued polynomials on and define I have two questions, one minor and one major: (Minor) Is is a dense subset of ? (Major) What are some minimal assumptions on that guarantee that is dense? For example, is it enough to consider only all homogeneous polynomials plus a constant. I think the answer to the first question follows from here where this result was shown for a one-dimensional case.","\mathcal{P} \mathbb{R}^2 S=\{ p(x) e^{-\alpha \|x\|^2}, x\in \mathbb{R}^2 : p \in \mathcal{P}, \alpha >0 \}. S L^2(\mathbb{R}^2) \mathcal{P} S","['functional-analysis', 'polynomials', 'hilbert-spaces', 'dense-subspaces']"
33,"How to define the grad, div and the curl of a distribution?","How to define the grad, div and the curl of a distribution?",,"I have learned how to define the derivative or partial derivative of a distribution, but I still can't find a clear definition of the grad, div and curl of a distribution, I would appreciate it if you could write down the detailed definition of the definitions of these operators of distribution or recommend me some books about the curl, div and grad of distributions","I have learned how to define the derivative or partial derivative of a distribution, but I still can't find a clear definition of the grad, div and curl of a distribution, I would appreciate it if you could write down the detailed definition of the definitions of these operators of distribution or recommend me some books about the curl, div and grad of distributions",,"['real-analysis', 'functional-analysis', 'distribution-theory']"
34,Vector-valued integral: Equivalence of Riemann sums and Rudin's definition,Vector-valued integral: Equivalence of Riemann sums and Rudin's definition,,"Definition/Notation: In his book ""Functional Analysis"", Rudin defines the integral of a vector-valued function as follows (Definition 3.26). Let $(Q, \mu)$ be a measure space, $X$ a topological vector space on which the dual $X^*$ separates points, and $f:Q\to X$ such that $\lambda f:Q\to\mathbb{R}$ is integrable w.r.t. $\mu$ for any $\lambda\in X^*$ . If there exists some $y\in X$ such that $$ \lambda(y)=\int_Q \lambda f d\mu \qquad (*) $$ for all $\lambda \in X^*$ , define $\int_Qfd\mu=y$ . Question: I would like to show that this is equivalent to defining it as the strong limit of Riemann sums in the following way (Chapter 3, Exercise 23): Suppose $Q$ is a compact Hausdorff space, $\mu$ a Borel probability measure, $X$ a Fréchet space [for my purposes we could even assume Banach, if that makes it easier] and $f:Q\to X$ continuous. (The existence of the integral under these circumstances is shown in Theorem 3.27.) Then for any neighborhood $V\subseteq X$ of $0$ , there exists a partition $E_1, ..., E_n$ of $Q$ (i.e. $E_i\cap E_j = \emptyset$ for $i\neq j$ and $\bigcup_{i=1}^n E_i=Q$ ) such that $$ z:=\int_Q fd\mu - \sum_{i=1}^n \mu(E_i)f(s_i) \in V $$ for any choice of $s_i\in E_i$ . Own work: As mentioned, I assume $(X, \lVert \cdot \rVert_X)$ is Banach. Given some neighborhood $0\in V\subseteq X$ , let $B(0,\varepsilon)\subseteq V$ for some $\varepsilon>0$ . By the Hahn-Banach theorem, there exists $\lambda \in X^*$ with $\lambda(z)=\lVert z \rVert_X$ and $\lVert \lambda \rVert=1$ , so $$ \lVert z \rVert_X = |\lambda(z)| = \left|\int_Q \lambda fd\mu - \sum_{i=1}^n \mu(E_i)\lambda(f(s_i))\right| \le \sum_{i=1}^n \int_{E_i} |\lambda(f(t)-f(s_i))|\mu(dt) \le \varepsilon \qquad (**) $$ given that we define the $E_i$ such that $f(t)-f(s) \in B(0,\varepsilon)$ whenever $t,s \in E_i$ . Since $Q$ is compact, $f$ is uniformly continuous, which I assume helps with finding the partition. I struggle, since it has to be a finite partition (which would be easy if $X$ were totally bounded, but it doesn't have to be). Could someone help me?","Definition/Notation: In his book ""Functional Analysis"", Rudin defines the integral of a vector-valued function as follows (Definition 3.26). Let be a measure space, a topological vector space on which the dual separates points, and such that is integrable w.r.t. for any . If there exists some such that for all , define . Question: I would like to show that this is equivalent to defining it as the strong limit of Riemann sums in the following way (Chapter 3, Exercise 23): Suppose is a compact Hausdorff space, a Borel probability measure, a Fréchet space [for my purposes we could even assume Banach, if that makes it easier] and continuous. (The existence of the integral under these circumstances is shown in Theorem 3.27.) Then for any neighborhood of , there exists a partition of (i.e. for and ) such that for any choice of . Own work: As mentioned, I assume is Banach. Given some neighborhood , let for some . By the Hahn-Banach theorem, there exists with and , so given that we define the such that whenever . Since is compact, is uniformly continuous, which I assume helps with finding the partition. I struggle, since it has to be a finite partition (which would be easy if were totally bounded, but it doesn't have to be). Could someone help me?","(Q, \mu) X X^* f:Q\to X \lambda f:Q\to\mathbb{R} \mu \lambda\in X^* y\in X 
\lambda(y)=\int_Q \lambda f d\mu \qquad (*)
 \lambda \in X^* \int_Qfd\mu=y Q \mu X f:Q\to X V\subseteq X 0 E_1, ..., E_n Q E_i\cap E_j = \emptyset i\neq j \bigcup_{i=1}^n E_i=Q 
z:=\int_Q fd\mu - \sum_{i=1}^n \mu(E_i)f(s_i) \in V
 s_i\in E_i (X, \lVert \cdot \rVert_X) 0\in V\subseteq X B(0,\varepsilon)\subseteq V \varepsilon>0 \lambda \in X^* \lambda(z)=\lVert z \rVert_X \lVert \lambda \rVert=1 
\lVert z \rVert_X = |\lambda(z)| = \left|\int_Q \lambda fd\mu - \sum_{i=1}^n \mu(E_i)\lambda(f(s_i))\right| \le \sum_{i=1}^n \int_{E_i} |\lambda(f(t)-f(s_i))|\mu(dt) \le \varepsilon \qquad (**)
 E_i f(t)-f(s) \in B(0,\varepsilon) t,s \in E_i Q f X",['functional-analysis']
35,Upper bound for a conditional variance (and expectation),Upper bound for a conditional variance (and expectation),,"Let $V(x,y)\in C^\infty$ be a uniformly convex function with polynomial growth, i.e. there exist $\alpha>0$ , $n\in\mathbb N$ , $k>0$ such that $$Hess V(x,y) \,\geq\, \alpha\, I \quad\textrm{for all }(x,y)\in\mathbb R^{d_1}\times\mathbb R^{d_2}$$ $$V(x,y) \,\leq\, k\,(|x|^{2n}+|y|^{2n}) \quad\textrm{as }|x|^2+|y|^2\to\infty \;.$$ We may assume w.l.o.g. $\int\!\int e^{-V(x,y)} dx\,dy=1$ , so that $e^{-V(x,y)}dx\,dy\,$ is a log-concave probability measure. I am interested in the conditional probabily measure. In particular I wonder if -maybe under additional assumption on $V$ and its derivatives- we have $$ \frac{\int |x|^{2p}\,e^{-V(x,y)}\,dx}{\int e^{-V(x,y)} dx}  \,-\,  \left(\frac{\int |x|^{p}\,e^{-V(x,y)}\,dx}{\int e^{-V(x,y)} dx}\right)^2 \,\leq\, h_1\,|y|^{2p} + h_0$$ for any $p \in \mathbb N$ and for suitable constants $h_1,h_0\geq0$ (that depend on $p$ and $V$ ). In other terms: is the variance of $|x|^{p}$ given $y$ bounded by a polynomial in $|y|$ ? Example. The answer is yes in the case of Gaussian measure. Indeed if $e^{-V(x,y)}dx\,dy\,$ is the Gaussian measure with mean $0$ and positive-definite covariance matrix $A$ , then the conditional measure given $y$ turns out to be the Gaussian measure with mean $A_{12}A_{22}^{-1}y$ and covariance matrix $A_{11}-A_{12}A_{22}^{-1}A_{12}^T$ . Then a simple change of variable $x\mapsto x+A_{12}A_{22}^{-1}y$ allows to conclude. Edit. After computations in the case of Gaussian measure, and after some simple numerics for a non-Gaussian measure ( $2n=4$ ), I suspect the same inequality could hold true also for expectations. Namely: $$ \frac{\int |x|^{p}\,e^{-V(x,y)}\,dx}{\int e^{-V(x,y)} dx} \,\leq\, h_1\,|y|^p + h_0$$ for every $p\in\mathbb N$ and for suitable constants $h_0,h_1\geq0$ which depend of $p$ and $V$ . Of course this would imply the previous inequality for variances. Edit 2. By continuity of the functions involved, an equivalent formulation of the previous inequality is: $$ L\equiv \limsup_{|y|\to\infty} \frac{\int |x|^{p}\,e^{-V(x,y)}\,dx}{|y|^p \int e^{-V(x,y)} dx} <\infty \;.$$ Edit 3. Conjecture : if $V(x,y)$ is a uniformly convex polynomial of degree $2n\geq 2$ and $p\in\mathbb N$ , there exists a positive-definite covariance matrix $A$ such that $$ \frac{\int |x|^{2p} e^{-V(x,y)} dx}{\int e^{-V(x,y)} dx} \,\leq\, \frac{\int |x|^{2p} e^{-G(x,y)} dx}{\int e^{-G(x,y)} dx}$$ where $G(x,y) = \frac{1}{2}\langle A^{-1}(x-x_0,y-y_0) , (x-x_0,y-y_0)\rangle $ and $(x_0,y_0)$ is the unique minimum point of $V$ . Now in the Gaussian case we can perform computations explcitly, finding $$ \frac{\int |x|^{2p} e^{-G(x,y)} dx}{\int e^{-G(x,y)} dx} \leq h_1\,|y|^{2p} + h_0 $$ for suitable constants $h_0,h_1$ . Thus if the conjecture holds true, it would prove the desired bound for conditional moments, at least in the case $V$ is a polynomial.","Let be a uniformly convex function with polynomial growth, i.e. there exist , , such that We may assume w.l.o.g. , so that is a log-concave probability measure. I am interested in the conditional probabily measure. In particular I wonder if -maybe under additional assumption on and its derivatives- we have for any and for suitable constants (that depend on and ). In other terms: is the variance of given bounded by a polynomial in ? Example. The answer is yes in the case of Gaussian measure. Indeed if is the Gaussian measure with mean and positive-definite covariance matrix , then the conditional measure given turns out to be the Gaussian measure with mean and covariance matrix . Then a simple change of variable allows to conclude. Edit. After computations in the case of Gaussian measure, and after some simple numerics for a non-Gaussian measure ( ), I suspect the same inequality could hold true also for expectations. Namely: for every and for suitable constants which depend of and . Of course this would imply the previous inequality for variances. Edit 2. By continuity of the functions involved, an equivalent formulation of the previous inequality is: Edit 3. Conjecture : if is a uniformly convex polynomial of degree and , there exists a positive-definite covariance matrix such that where and is the unique minimum point of . Now in the Gaussian case we can perform computations explcitly, finding for suitable constants . Thus if the conjecture holds true, it would prove the desired bound for conditional moments, at least in the case is a polynomial.","V(x,y)\in C^\infty \alpha>0 n\in\mathbb N k>0 Hess V(x,y) \,\geq\, \alpha\, I \quad\textrm{for all }(x,y)\in\mathbb R^{d_1}\times\mathbb R^{d_2} V(x,y) \,\leq\, k\,(|x|^{2n}+|y|^{2n}) \quad\textrm{as }|x|^2+|y|^2\to\infty \;. \int\!\int e^{-V(x,y)} dx\,dy=1 e^{-V(x,y)}dx\,dy\, V  \frac{\int |x|^{2p}\,e^{-V(x,y)}\,dx}{\int e^{-V(x,y)} dx}  \,-\,  \left(\frac{\int |x|^{p}\,e^{-V(x,y)}\,dx}{\int e^{-V(x,y)} dx}\right)^2 \,\leq\, h_1\,|y|^{2p} + h_0 p \in \mathbb N h_1,h_0\geq0 p V |x|^{p} y |y| e^{-V(x,y)}dx\,dy\, 0 A y A_{12}A_{22}^{-1}y A_{11}-A_{12}A_{22}^{-1}A_{12}^T x\mapsto x+A_{12}A_{22}^{-1}y 2n=4  \frac{\int |x|^{p}\,e^{-V(x,y)}\,dx}{\int e^{-V(x,y)} dx} \,\leq\, h_1\,|y|^p + h_0 p\in\mathbb N h_0,h_1\geq0 p V  L\equiv \limsup_{|y|\to\infty} \frac{\int |x|^{p}\,e^{-V(x,y)}\,dx}{|y|^p \int e^{-V(x,y)} dx} <\infty \;. V(x,y) 2n\geq 2 p\in\mathbb N A  \frac{\int |x|^{2p} e^{-V(x,y)} dx}{\int e^{-V(x,y)} dx} \,\leq\, \frac{\int |x|^{2p} e^{-G(x,y)} dx}{\int e^{-G(x,y)} dx} G(x,y) = \frac{1}{2}\langle A^{-1}(x-x_0,y-y_0) , (x-x_0,y-y_0)\rangle  (x_0,y_0) V  \frac{\int |x|^{2p} e^{-G(x,y)} dx}{\int e^{-G(x,y)} dx} \leq h_1\,|y|^{2p} + h_0  h_0,h_1 V","['functional-analysis', 'probability-theory', 'polynomials', 'convex-analysis', 'conditional-probability']"
36,Schauder Basis for Homeo$^+(\mathbb{R}^2)$,Schauder Basis for Homeo,^+(\mathbb{R}^2),"In the 50's through 70's there was a lot of research into the group of orientation-preserving homeomorphisms of the plane, denoted as Homeo $^+(\mathbb{R}^2)$ (in the compact-open topology, which in this case is just the topology of compact convergence).  Long proofs were used to prove certain local properties like local arc-connectedness and local contractibility. But much more recently it was shown that Homeo $^+(\mathbb{R}^2)$ is actually a Hilbert manifold, i.e. locally homeomorphic to separable Hilbert space.  The proof was rather abstract, but it leaves open the question of whether it can just be displayed directly; it wasn't believed to be so nice back when the foundational work was being done, so I'm not sure anyone ever tried. I guess the best way to get a direct proof is to just find the basis! What is an explicit orthonormal basis for a neighborhood of $id \in$ Homeo $^+(\mathbb{R}^2)$ ? I've been trying to do this myself using very simple functions, but haven't been able to come up with it.  Here I don't want to a priori assume that the group is a Hilbert manifold, I want to prove it directly (or at least the local homeomorphism). It seems like this is more a problem for people in functional analysis, so hopefully somebody can help!","In the 50's through 70's there was a lot of research into the group of orientation-preserving homeomorphisms of the plane, denoted as Homeo (in the compact-open topology, which in this case is just the topology of compact convergence).  Long proofs were used to prove certain local properties like local arc-connectedness and local contractibility. But much more recently it was shown that Homeo is actually a Hilbert manifold, i.e. locally homeomorphic to separable Hilbert space.  The proof was rather abstract, but it leaves open the question of whether it can just be displayed directly; it wasn't believed to be so nice back when the foundational work was being done, so I'm not sure anyone ever tried. I guess the best way to get a direct proof is to just find the basis! What is an explicit orthonormal basis for a neighborhood of Homeo ? I've been trying to do this myself using very simple functions, but haven't been able to come up with it.  Here I don't want to a priori assume that the group is a Hilbert manifold, I want to prove it directly (or at least the local homeomorphism). It seems like this is more a problem for people in functional analysis, so hopefully somebody can help!",^+(\mathbb{R}^2) ^+(\mathbb{R}^2) id \in ^+(\mathbb{R}^2),"['real-analysis', 'functional-analysis', 'topological-groups', 'topological-vector-spaces', 'schauder-basis']"
37,When is the heat semigroup Gibbs?,When is the heat semigroup Gibbs?,,"Defining the Laplacian on a region $\Omega$ of $\mathbb{R}^{d}$ with Dirichlet boundary conditions, under what conditions on the region (or other possible assumptions) is the semigroup it generates Gibbs, i.e. trace-class? For example, if $\Omega$ has finite volume, this is the case, and the book ""Heat Kernels and Spectral Theory"" by E. Brian Davies (where this question is inspired from) provides one other set of conditions (involving strongly regular regions). Are there other references that study this question?","Defining the Laplacian on a region of with Dirichlet boundary conditions, under what conditions on the region (or other possible assumptions) is the semigroup it generates Gibbs, i.e. trace-class? For example, if has finite volume, this is the case, and the book ""Heat Kernels and Spectral Theory"" by E. Brian Davies (where this question is inspired from) provides one other set of conditions (involving strongly regular regions). Are there other references that study this question?",\Omega \mathbb{R}^{d} \Omega,"['functional-analysis', 'reference-request', 'operator-theory']"
38,Two notions of vector distributions and differential operators in $\mathbb{R}^3$,Two notions of vector distributions and differential operators in,\mathbb{R}^3,"Two possible ways of defining vector/vector valued distributions in $\mathbb{R}^3$ are: $$ X := [\mathcal{D}(\mathbb{R}^3; (\mathbb{R}^3)^{*})]^{*} = \{ T: \mathcal{D}(\mathbb{R}^3; (\mathbb{R}^3)^{*}) \simeq \mathcal{D}(\mathbb{R}^3; \mathbb{R}^3) \rightarrow \mathbb{R} \}, $$ and $$ Y := \mathcal{D}^{*}(\mathbb{R}^3;\mathbb{R}^3) = \{ T : \mathcal{D}(\mathbb{R}^3) \rightarrow \mathbb{R}^3 \} \simeq \mathcal{D}'(\mathbb{R}^3;\mathbb{R})^3 .  $$ Note that $\mathbf{u} \in L^1_{loc}(\mathbb{R}^3)^3$ induces the distribution $\langle T_{\mathbf{u}}, \phi \rangle = \int_{\mathbb{R}^3} \mathbf{u} \cdot \phi$ , where in the first case the test function is vector valued while in the second case it is scalar valued. As explained here , these two space are ""equal"" (i.e., linearly homeomorphic) in this framework. In general, that is if the ambient space is some infinite dimensional Banach space $U$ , the $Y$ definition is to be preferred. Now if $T$ is a vector valued distribution, one usually defines its curl via duality: $$ \langle \operatorname{curl} T, \phi \rangle :=  \langle T, \operatorname{curl} \phi \rangle, \qquad \phi \in \mathcal{D}(\mathbb{R}^3; \mathbb{R}^3); $$ since $\operatorname{curl} \phi \in \mathcal{D}(\mathbb{R}^3; \mathbb{R}^3)$ and $T$ acts on it, a posteriori it follows that $T$ should be in $X$ and $\operatorname{curl} T \in X$ too (assume continuity holds)! If instead $T = (T_1, T_2, T_3) \in Y$ , I would write $$ \langle \underbrace{\operatorname{curl} T}_{\in Y}, \phi \rangle := (\langle \partial_3 T_2 - \partial_2 T_3, \phi \rangle, \langle \dots, \phi \rangle, \langle \dots, \phi \rangle) \qquad \phi \in \mathcal{D}(\mathbb{R}^3; \mathbb{R}).  $$ For what concerns the gradient of a distribution $F \in \mathcal{D}'(\mathbb{R}^3; \mathbb{R})$ we have the same issue: we can either define it by duality with the divergence, resulting in $\nabla F \in X$ , or as the triplet of distributions $(\partial_{x_1} F, \partial_{x_2} F, \partial_{x_3} F) \in Y$ . All considered, is there a more standard or desirable choice among these definitions? For some reason, my taste would prefer the $X$ definition (with corresponding operators), perhaps because of the ""perfect dualities"", but this doesn't sound like a robust motivation. EDIT: There is a good chance that part of the answer somehow lies in differential geometry, which I am not quite good at. What properties we would like these diffential operators to have? Almost certainly, the validity of a Poincaré lemma, namely that if $T \in \mathcal{D}'$ and $\operatorname{curl} T = 0$ , then $T = \nabla F$ for some distribution $F$ (and similar for div and grad). I know that there is such an abstract result for currents , but I am not able to put all pieces together with the correct definitions and identifications (for instance, in the standard $\mathbb{R}^3$ setting, even if in principle grad, curl and div are differential operators acting on $0,1,2$ -forms resp., we usually identify the divergence with the scalar function it is represented by etc...)","Two possible ways of defining vector/vector valued distributions in are: and Note that induces the distribution , where in the first case the test function is vector valued while in the second case it is scalar valued. As explained here , these two space are ""equal"" (i.e., linearly homeomorphic) in this framework. In general, that is if the ambient space is some infinite dimensional Banach space , the definition is to be preferred. Now if is a vector valued distribution, one usually defines its curl via duality: since and acts on it, a posteriori it follows that should be in and too (assume continuity holds)! If instead , I would write For what concerns the gradient of a distribution we have the same issue: we can either define it by duality with the divergence, resulting in , or as the triplet of distributions . All considered, is there a more standard or desirable choice among these definitions? For some reason, my taste would prefer the definition (with corresponding operators), perhaps because of the ""perfect dualities"", but this doesn't sound like a robust motivation. EDIT: There is a good chance that part of the answer somehow lies in differential geometry, which I am not quite good at. What properties we would like these diffential operators to have? Almost certainly, the validity of a Poincaré lemma, namely that if and , then for some distribution (and similar for div and grad). I know that there is such an abstract result for currents , but I am not able to put all pieces together with the correct definitions and identifications (for instance, in the standard setting, even if in principle grad, curl and div are differential operators acting on -forms resp., we usually identify the divergence with the scalar function it is represented by etc...)","\mathbb{R}^3  X := [\mathcal{D}(\mathbb{R}^3; (\mathbb{R}^3)^{*})]^{*} = \{ T: \mathcal{D}(\mathbb{R}^3; (\mathbb{R}^3)^{*}) \simeq \mathcal{D}(\mathbb{R}^3; \mathbb{R}^3) \rightarrow \mathbb{R} \},   Y := \mathcal{D}^{*}(\mathbb{R}^3;\mathbb{R}^3) = \{ T : \mathcal{D}(\mathbb{R}^3) \rightarrow \mathbb{R}^3 \} \simeq \mathcal{D}'(\mathbb{R}^3;\mathbb{R})^3 .   \mathbf{u} \in L^1_{loc}(\mathbb{R}^3)^3 \langle T_{\mathbf{u}}, \phi \rangle = \int_{\mathbb{R}^3} \mathbf{u} \cdot \phi U Y T  \langle \operatorname{curl} T, \phi \rangle :=  \langle T, \operatorname{curl} \phi \rangle, \qquad \phi \in \mathcal{D}(\mathbb{R}^3; \mathbb{R}^3);  \operatorname{curl} \phi \in \mathcal{D}(\mathbb{R}^3; \mathbb{R}^3) T T X \operatorname{curl} T \in X T = (T_1, T_2, T_3) \in Y  \langle \underbrace{\operatorname{curl} T}_{\in Y}, \phi \rangle := (\langle \partial_3 T_2 - \partial_2 T_3, \phi \rangle, \langle \dots, \phi \rangle, \langle \dots, \phi \rangle) \qquad \phi \in \mathcal{D}(\mathbb{R}^3; \mathbb{R}).   F \in \mathcal{D}'(\mathbb{R}^3; \mathbb{R}) \nabla F \in X (\partial_{x_1} F, \partial_{x_2} F, \partial_{x_3} F) \in Y X T \in \mathcal{D}' \operatorname{curl} T = 0 T = \nabla F F \mathbb{R}^3 0,1,2","['functional-analysis', 'differential-geometry', 'distribution-theory', 'topological-vector-spaces']"
39,"Prove that $\int_0^1\log\left(\Lambda-\sqrt{\Lambda^2+C}\sin(\pi x+D)\right)\,dx\le\frac{\Lambda-1}2+\frac{\Lambda\sinh\pi\sqrt C}{2\pi\sqrt C}$",Prove that,"\int_0^1\log\left(\Lambda-\sqrt{\Lambda^2+C}\sin(\pi x+D)\right)\,dx\le\frac{\Lambda-1}2+\frac{\Lambda\sinh\pi\sqrt C}{2\pi\sqrt C}","Consider the generalised Poincaré inequality [1] $$\left\||u|^2\right\|_{L^1}-\left\||u|^2\right\|_{L^{1/p}}\le(p-1)C_p(\mu)\int|\nabla u|^2\,d\mu$$ for $u\in H^1(\mu)$ , $p\in(1,2]$ and some positive constant $C_p(\mu)$ . The special case $p=2$ over the finite interval $[0,1]\in\Bbb R^1$ yields the convex Sobolev inequality [2] $$\int_0^1\phi(u)\,dx-\phi\left(\int_0^1u\,dx\right)\le\frac1{2\pi^2}\int_0^1\left(\frac{\partial\psi(u)}{\partial x}\right)^2\,dx$$ where $\phi$ is convex, $(\phi'')^{-1/2}$ is concave and $\psi'^2=\phi''$ . In particular, we can take $\psi=-\phi=\log$ to obtain the result $$F(x,u,u’):=\int_0^1\log u\,dx+\frac1{2\pi^2}\int_0^1\frac{u'^2}{u^2}\,dx\ge0$$ which is shown in Prove that $0\le\int_0^1\log(u){\rm d}x+\frac1{2\pi^2}\int_0^1\frac1{u^2}\left(\frac{{\rm d}u}{{\rm d}x}\right)^2{\rm d}x$ . This is of course, an easy consequence of a rather advanced result so I tried a calculus of variations approach. With the additional constraint that $u(x)$ is a positive, differentiable density function, we define the functional $F(x,u,u’)+\lambda\int_0^1u\,dx$ which yields the stationary path $$u_s(x)=\frac1{\Lambda-\sqrt{\Lambda^2+C}\sin(\pi x+D)}$$ where $C,\Lambda=-\lambda>0$ are subject to $\sqrt{\Lambda^2+C}\cos D\tanh(\pi\sqrt C/2)=-\sqrt C$ . Thus the inequality becomes ( details in my answer to linked question ) $$\int_0^1\log\left(\Lambda-\sqrt{\Lambda^2+C}\sin(\pi x+D)\right)\,dx\le\frac{\Lambda-1}2+\frac{\Lambda\sinh\pi\sqrt C}{2\pi\sqrt C}\tag1$$ for all $C,D$ whenever $u_s(x)$ is continuous on $[0,1]$ and $F(x,u_s,u_s')$ is defined and finite. Is there a way to prove $(1)$ directly? References [1] Bartier, J-P., Doubault, J. (2006). Convex Sobolev inequalities and spectral gap. Comptes Rendus Mathematique. 342(5) :307-312. [2] Matthes, D. Lecture Notes on the Course ""Entropy Methods and Related Functional Inequalities"". Available from https://www2.karlin.mff.cuni.cz/~kaplicky/pages/pages/2020l/matthes_entropymethods.pdf .","Consider the generalised Poincaré inequality [1] for , and some positive constant . The special case over the finite interval yields the convex Sobolev inequality [2] where is convex, is concave and . In particular, we can take to obtain the result which is shown in Prove that $0\le\int_0^1\log(u){\rm d}x+\frac1{2\pi^2}\int_0^1\frac1{u^2}\left(\frac{{\rm d}u}{{\rm d}x}\right)^2{\rm d}x$ . This is of course, an easy consequence of a rather advanced result so I tried a calculus of variations approach. With the additional constraint that is a positive, differentiable density function, we define the functional which yields the stationary path where are subject to . Thus the inequality becomes ( details in my answer to linked question ) for all whenever is continuous on and is defined and finite. Is there a way to prove directly? References [1] Bartier, J-P., Doubault, J. (2006). Convex Sobolev inequalities and spectral gap. Comptes Rendus Mathematique. 342(5) :307-312. [2] Matthes, D. Lecture Notes on the Course ""Entropy Methods and Related Functional Inequalities"". Available from https://www2.karlin.mff.cuni.cz/~kaplicky/pages/pages/2020l/matthes_entropymethods.pdf .","\left\||u|^2\right\|_{L^1}-\left\||u|^2\right\|_{L^{1/p}}\le(p-1)C_p(\mu)\int|\nabla u|^2\,d\mu u\in H^1(\mu) p\in(1,2] C_p(\mu) p=2 [0,1]\in\Bbb R^1 \int_0^1\phi(u)\,dx-\phi\left(\int_0^1u\,dx\right)\le\frac1{2\pi^2}\int_0^1\left(\frac{\partial\psi(u)}{\partial x}\right)^2\,dx \phi (\phi'')^{-1/2} \psi'^2=\phi'' \psi=-\phi=\log F(x,u,u’):=\int_0^1\log u\,dx+\frac1{2\pi^2}\int_0^1\frac{u'^2}{u^2}\,dx\ge0 u(x) F(x,u,u’)+\lambda\int_0^1u\,dx u_s(x)=\frac1{\Lambda-\sqrt{\Lambda^2+C}\sin(\pi x+D)} C,\Lambda=-\lambda>0 \sqrt{\Lambda^2+C}\cos D\tanh(\pi\sqrt C/2)=-\sqrt C \int_0^1\log\left(\Lambda-\sqrt{\Lambda^2+C}\sin(\pi x+D)\right)\,dx\le\frac{\Lambda-1}2+\frac{\Lambda\sinh\pi\sqrt C}{2\pi\sqrt C}\tag1 C,D u_s(x) [0,1] F(x,u_s,u_s') (1)","['real-analysis', 'functional-analysis', 'inequality', 'definite-integrals', 'calculus-of-variations']"
40,Book Recommendations for Stochastic Analysis Preliminaries,Book Recommendations for Stochastic Analysis Preliminaries,,"I would like to ask for references that may help me in tackling some of the advanced stochastic analysis books. I am interested in a variety of different areas, namely (1) Malliavin Calculus, (2) Stochastic Differential Geometry, (3) Stochastic Differential Equations, and these are in order of interest. So, I would like to know what books I can read to better comprehend the standard literature in those areas. You can suppose I have a background in real analysis from Folland, probability theory from Williams, stochastic calculus from LeGall/Kuo (which is more or less the first 3 chapters of Karatzas & Shreve, i.e. elementary stochastics knowledge). Now, from the research I have done by myself, I concluded that for (1) I would simply need to go through a text in functional analysis. Something on the lines of ""Sobolev Spaces"" by Adams or Brezis' ""Functional Analysis, Sobolev Spaces and Partial Differential Equations"". Is that all that I should read before tackling the standard texts like Nualart? Now, for (2), I concluded that I would simply require a deeper understanding of riemannian geometry. A text like Lee's ""Riemannian Geometry"" would suffice. Am I safe in assuming this? Given that I remember very little differential geometry, can I 'skim' over manifold theory and riemannian manifolds instead of rigorously tackling the classic differential geometry texts, considering that I do not want to spend/focus my time on differential geometry and would very much like to focus on pure probability theory. For (3), from what I can tell it is an extremely wide area. If so, what would you recommend as solid foundation for tackling the basic texts in the area? Would the same books in (1) suffice? Thank you for your time.","I would like to ask for references that may help me in tackling some of the advanced stochastic analysis books. I am interested in a variety of different areas, namely (1) Malliavin Calculus, (2) Stochastic Differential Geometry, (3) Stochastic Differential Equations, and these are in order of interest. So, I would like to know what books I can read to better comprehend the standard literature in those areas. You can suppose I have a background in real analysis from Folland, probability theory from Williams, stochastic calculus from LeGall/Kuo (which is more or less the first 3 chapters of Karatzas & Shreve, i.e. elementary stochastics knowledge). Now, from the research I have done by myself, I concluded that for (1) I would simply need to go through a text in functional analysis. Something on the lines of ""Sobolev Spaces"" by Adams or Brezis' ""Functional Analysis, Sobolev Spaces and Partial Differential Equations"". Is that all that I should read before tackling the standard texts like Nualart? Now, for (2), I concluded that I would simply require a deeper understanding of riemannian geometry. A text like Lee's ""Riemannian Geometry"" would suffice. Am I safe in assuming this? Given that I remember very little differential geometry, can I 'skim' over manifold theory and riemannian manifolds instead of rigorously tackling the classic differential geometry texts, considering that I do not want to spend/focus my time on differential geometry and would very much like to focus on pure probability theory. For (3), from what I can tell it is an extremely wide area. If so, what would you recommend as solid foundation for tackling the basic texts in the area? Would the same books in (1) suffice? Thank you for your time.",,"['functional-analysis', 'stochastic-calculus', 'book-recommendation', 'stochastic-analysis', 'malliavin-calculus']"
41,How to prove that the map $T \mapsto T^{-1}$ is continuous?,How to prove that the map  is continuous?,T \mapsto T^{-1},"Let $E$ be a Banach space. Let $\mathcal L (E)$ denote the space of all bounded linear operators on $E$ and $\text {GL} (E)$ denote the space of all bounded linear operators on $E$ with bounded inverse. Consider the map $\varphi : \text {GL} (E) \longrightarrow \mathcal L (E)$ defined by $T \longmapsto T^{-1}, T \in \text {GL} (E).$ Prove that $\varphi$ is continuous. What I need to prove is that for a given $\varepsilon \gt 0$ there exists $\delta \gt 0$ such that whenever $\|T - T_0 \|_{\text {op}} \lt \delta$ we have $\|T^{-1} - T_0^{-1} \|_{\text {op}} \lt \varepsilon.$ I tried to write $T^{-1} - T_0^{-1} = T_0^{-1} (T_0 - T) T^{-1}.$ Then we have $$\begin{align*} \|T^{-1} - T_0^{-1} \|_{\text {op}} & \leq \|T_0^{-1}\|_{\text {op}} \|T - T_0\|_{\text {op}} \|T^{-1}\|_{\text {op}} \leq \|T_0^{-1}\|_{\text {op}} \|T - T_0 \|_{\text {op}} \left (\|T^{-1} - T_0^{-1}\|_{\text {op}} + \|T_0^{-1}\|_{\text {op}} \right ). \end{align*}$$ But this shows that $$\begin{align*} \|T^{-1} - T_0^{-1} \|_{\text {op}} \leq \frac {\left (\|T_0^{-1}\|_{\text {op}} \right )^2 \|T - T_0\|_{\text {op}}} {1 - \|T_0^{-1}\|_{\text {op}} \|T - T_0 \|_{\text {op}}} \lt \varepsilon & \iff \|T - T_0 \|_{\text {op}} \lt \frac {\varepsilon} {\|T_0^{-1}\|_{\text {op}} \left ( 1 + \|T_0^{-1}\|_{\text {op}} \right )} \end{align*}$$ which is what we set out to prove. Is the above proof ok? Please check it. Thanks in advance.",Let be a Banach space. Let denote the space of all bounded linear operators on and denote the space of all bounded linear operators on with bounded inverse. Consider the map defined by Prove that is continuous. What I need to prove is that for a given there exists such that whenever we have I tried to write Then we have But this shows that which is what we set out to prove. Is the above proof ok? Please check it. Thanks in advance.,"E \mathcal L (E) E \text {GL} (E) E \varphi : \text {GL} (E) \longrightarrow \mathcal L (E) T \longmapsto T^{-1}, T \in \text {GL} (E). \varphi \varepsilon \gt 0 \delta \gt 0 \|T - T_0 \|_{\text {op}} \lt \delta \|T^{-1} - T_0^{-1} \|_{\text {op}} \lt \varepsilon. T^{-1} - T_0^{-1} = T_0^{-1} (T_0 - T) T^{-1}. \begin{align*} \|T^{-1} - T_0^{-1} \|_{\text {op}} & \leq \|T_0^{-1}\|_{\text {op}} \|T - T_0\|_{\text {op}} \|T^{-1}\|_{\text {op}} \leq \|T_0^{-1}\|_{\text {op}} \|T - T_0 \|_{\text {op}} \left (\|T^{-1} - T_0^{-1}\|_{\text {op}} + \|T_0^{-1}\|_{\text {op}} \right ). \end{align*} \begin{align*} \|T^{-1} - T_0^{-1} \|_{\text {op}} \leq \frac {\left (\|T_0^{-1}\|_{\text {op}} \right )^2 \|T - T_0\|_{\text {op}}} {1 - \|T_0^{-1}\|_{\text {op}} \|T - T_0 \|_{\text {op}}} \lt \varepsilon & \iff \|T - T_0 \|_{\text {op}} \lt \frac {\varepsilon} {\|T_0^{-1}\|_{\text {op}} \left ( 1 + \|T_0^{-1}\|_{\text {op}} \right )} \end{align*}","['functional-analysis', 'solution-verification', 'operator-theory', 'banach-spaces']"
42,Convex analysis question: Cone and convex hull,Convex analysis question: Cone and convex hull,,"I'm reading a publication, where a small bit of convex analysis is used in one of the lemmas. I have Rockafellar's book, but I haven't found anything too helpful yet as I am pretty new to the area. Lemma. Let $I \subset \{1, \dots, n\}$ for some $n \in \mathbb{N}$ with cardinality $\alpha = \#I$ . Define a cone as $$C = \left\{x \in \mathbb{R}^n  : \sum_{i \notin I} |x_i| \leq \sum_{i \in I} |x_i|\right\}.$$ Then for a set $$ G = \left\{x \in \mathbb{R}^n : \sum_{i=1}^n \mathbb{1}_{\{x_i \neq 0\}} \leq \alpha,~~||x|| = 1\right\},$$ we have that $$ C \cap \{x \in \mathbb{R}^n : ||x|| = 1\} \subset 3 \cdot \text{Conv}(G),$$ where $||\cdot||$ is the $L^2$ norm. Proof of Lemma Idea: A Theorem in Rockafellar states that a convex hull is equal to the set of convex combinations of $H$ , where for a vector $\lambda \geq 0$ we can define as $$K(G) := \left\{x \in \mathbb{R}^n : \lambda_1 x_1 + \dots + \lambda_n x_n ~\text{a combination in}~G~\text{s.t.}~~\sum_i^n \lambda_i = 1\right\},$$ and $\text{Conv}(G) = K(G)$ . But I'm not sure how to proceed?","I'm reading a publication, where a small bit of convex analysis is used in one of the lemmas. I have Rockafellar's book, but I haven't found anything too helpful yet as I am pretty new to the area. Lemma. Let for some with cardinality . Define a cone as Then for a set we have that where is the norm. Proof of Lemma Idea: A Theorem in Rockafellar states that a convex hull is equal to the set of convex combinations of , where for a vector we can define as and . But I'm not sure how to proceed?","I \subset \{1, \dots, n\} n \in \mathbb{N} \alpha = \#I C = \left\{x \in \mathbb{R}^n  : \sum_{i \notin I} |x_i| \leq \sum_{i \in I} |x_i|\right\}.  G = \left\{x \in \mathbb{R}^n : \sum_{i=1}^n \mathbb{1}_{\{x_i \neq 0\}} \leq \alpha,~~||x|| = 1\right\},  C \cap \{x \in \mathbb{R}^n : ||x|| = 1\} \subset 3 \cdot \text{Conv}(G), ||\cdot|| L^2 H \lambda \geq 0 K(G) := \left\{x \in \mathbb{R}^n : \lambda_1 x_1 + \dots + \lambda_n x_n ~\text{a combination in}~G~\text{s.t.}~~\sum_i^n \lambda_i = 1\right\}, \text{Conv}(G) = K(G)","['functional-analysis', 'convex-analysis', 'convex-geometry', 'convex-hulls']"
43,"If $(X, \|\cdot\|)$ is a normed vector space, then $(X\setminus\{0\}, d)$ is a metric space for $d(x, y) = \frac{\|x-y\|}{\|x\|+\|y\|}$","If  is a normed vector space, then  is a metric space for","(X, \|\cdot\|) (X\setminus\{0\}, d) d(x, y) = \frac{\|x-y\|}{\|x\|+\|y\|}","Consider the following statement: If $(X, \|\cdot\|)$ is a normed vector space, then $(X\setminus\{0\}, d)$ is a metric space for $$d(x, y) = \frac{\|x-y\|}{\|x\|+\|y\|}$$ I'm trying to figure out whether this statement is true (and if it is, whether it's only true in a special setting, e.g. uniformly convex Banach spaces). The context is that I'm trying to define a ""similarity metric"" between two vectors in a general normed vector space with the invariance $d(x, y) = d(\alpha x, \alpha y)$ for $\alpha\neq 0$ , among some other properties. It seems like it could maybe be a textbook problem, but I'm not quite sure if I've seen it somewhere or not. It's pretty clear that $d(x, y) = 0$ implies $x = y$ and $d(x, y) = d(y, x)$ , but the triangle inequality is a bit sticky. We can rearrange $d(x, y)\leq d(x, z)+d(z, y)$ as $$\|x-y\|\leq \left(\frac{\|x-z\|}{\|x\|+\|z\|}+\frac{\|y-z\|}{\|y\|+\|z\|}\right)(\|x\|+\|y\|)$$ which feels stronger than the standard triangle inequality on $(X, \|\cdot\|)$ . Does anybody have some insights on this? Am I missing a standard trick here? Thanks!","Consider the following statement: If is a normed vector space, then is a metric space for I'm trying to figure out whether this statement is true (and if it is, whether it's only true in a special setting, e.g. uniformly convex Banach spaces). The context is that I'm trying to define a ""similarity metric"" between two vectors in a general normed vector space with the invariance for , among some other properties. It seems like it could maybe be a textbook problem, but I'm not quite sure if I've seen it somewhere or not. It's pretty clear that implies and , but the triangle inequality is a bit sticky. We can rearrange as which feels stronger than the standard triangle inequality on . Does anybody have some insights on this? Am I missing a standard trick here? Thanks!","(X, \|\cdot\|) (X\setminus\{0\}, d) d(x, y) = \frac{\|x-y\|}{\|x\|+\|y\|} d(x, y) = d(\alpha x, \alpha y) \alpha\neq 0 d(x, y) = 0 x = y d(x, y) = d(y, x) d(x, y)\leq d(x, z)+d(z, y) \|x-y\|\leq \left(\frac{\|x-z\|}{\|x\|+\|z\|}+\frac{\|y-z\|}{\|y\|+\|z\|}\right)(\|x\|+\|y\|) (X, \|\cdot\|)","['functional-analysis', 'inequality', 'metric-spaces', 'normed-spaces']"
44,Koopman composition operator theoretical approach to Ergodic Dynamical Systems,Koopman composition operator theoretical approach to Ergodic Dynamical Systems,,"Given a dynamical system $T:X\to X$ , the Koopman  operator $U$ is defined on the space of (complex-valued) functions on $X$ . So for $f:X\to\mathbb C$ , the action of the composition on the function $f$ reads: $U(f):=f\circ T$ . Then, since $U$ is then a linear operator in a vector space, one is interested in the eigenfunctions especially those with eigenvalue 1. But for ergodic transformations $T$ , it is a fact that the only $T$ -invariant function (i.e. $f\circ T=f$ ) is an a.e. constant  (almost everywhere, up to a zero-measure subset). So what is the application of this transition to ""observable space"" for ergodic systems, if there is any? Actually I think, ergodicity is a property mostly assumed for real physical systems, and here the opposite is required? References: Koopman Operator Spectrum and Data Analysis , Igor Mezic: ""Koopman Operator Theory for Dynamical Systems, Control and Data Analytics"" , Introduction to Koopman operator theory ofdynamical systems , Data-driven spectral decomposition and forecasting of ergodic dynamical systems","Given a dynamical system , the Koopman  operator is defined on the space of (complex-valued) functions on . So for , the action of the composition on the function reads: . Then, since is then a linear operator in a vector space, one is interested in the eigenfunctions especially those with eigenvalue 1. But for ergodic transformations , it is a fact that the only -invariant function (i.e. ) is an a.e. constant  (almost everywhere, up to a zero-measure subset). So what is the application of this transition to ""observable space"" for ergodic systems, if there is any? Actually I think, ergodicity is a property mostly assumed for real physical systems, and here the opposite is required? References: Koopman Operator Spectrum and Data Analysis , Igor Mezic: ""Koopman Operator Theory for Dynamical Systems, Control and Data Analytics"" , Introduction to Koopman operator theory ofdynamical systems , Data-driven spectral decomposition and forecasting of ergodic dynamical systems",T:X\to X U X f:X\to\mathbb C f U(f):=f\circ T U T T f\circ T=f,"['functional-analysis', 'operator-theory', 'dynamical-systems', 'ergodic-theory', 'data-analysis']"
45,Upgrading weak-convergence in $\mathbb{R}$ to strong-convergence in compact sets,Upgrading weak-convergence in  to strong-convergence in compact sets,\mathbb{R},"Let $s,s'\in\mathbb{R}$ satisfying $s'<s$ . Consider $u=u(t,x)\in C(\mathbb{R},H^s(\mathbb{R}))$ and suppose that, as $t$ goes to infinity, we have the following weak convergence: $$ u(t,\cdot)\rightharpoonup u^*(x) \quad \hbox{in} \quad H^{s'}(\mathbb{R}), $$ where $s'<s$ and "" $\rightharpoonup$ "" denotes the weak convergence in $H^{s'}$ . If we additionally assume that $u(t,\cdot)$ is uniformly bounded in $H^s$ , say $$ u\in L^{\infty}(\mathbb{R},H^s(\mathbb{R})), $$ does this imply that $u(t,\cdot)$ strongly converge to $u^*$ in $H^{s'}(K)$ for any $K\subset\mathbb{R}$ compact? Moreover, if we additionally assume that $u^*$ belongs to $H^s$ (recall that $s'<s$ ), does the previous hypotheses (weak convergence in $H^{s'}(\mathbb{R})$ and uniform boundedness in $H^s(\mathbb{R})$ ) imply strong convergence in $H^{r}(K)$ for all $s'\leq r<s$ and all $K\subset\mathbb{R}$ compact? In other words, if we have a function is converging in a very weak topology, but this function is also uniformly bounded in a stronger topology, does that implies that the function is locally-strongly converging in the topologies ""in between"" them (whenever ""in between"" has sense, like in this case)?","Let satisfying . Consider and suppose that, as goes to infinity, we have the following weak convergence: where and "" "" denotes the weak convergence in . If we additionally assume that is uniformly bounded in , say does this imply that strongly converge to in for any compact? Moreover, if we additionally assume that belongs to (recall that ), does the previous hypotheses (weak convergence in and uniform boundedness in ) imply strong convergence in for all and all compact? In other words, if we have a function is converging in a very weak topology, but this function is also uniformly bounded in a stronger topology, does that implies that the function is locally-strongly converging in the topologies ""in between"" them (whenever ""in between"" has sense, like in this case)?","s,s'\in\mathbb{R} s'<s u=u(t,x)\in C(\mathbb{R},H^s(\mathbb{R})) t 
u(t,\cdot)\rightharpoonup u^*(x) \quad \hbox{in} \quad H^{s'}(\mathbb{R}),
 s'<s \rightharpoonup H^{s'} u(t,\cdot) H^s 
u\in L^{\infty}(\mathbb{R},H^s(\mathbb{R})),
 u(t,\cdot) u^* H^{s'}(K) K\subset\mathbb{R} u^* H^s s'<s H^{s'}(\mathbb{R}) H^s(\mathbb{R}) H^{r}(K) s'\leq r<s K\subset\mathbb{R}","['real-analysis', 'functional-analysis', 'partial-differential-equations', 'sobolev-spaces', 'weak-convergence']"
46,"""Second order quantization operator"" and its role in the construction of Hida spaces.","""Second order quantization operator"" and its role in the construction of Hida spaces.",,"As premise I know barely anything about physics and thus this question might be absolutely trivial or ill-posed. I am starting to study the White noise theory and when constructing the infinite-dimensional analogue of the Schwartz space $S(\mathbb R^n)$ , i.e. $(S)$ one way is to grab a random variable $\psi\in L^2(S(R),\mu)$ with chaotic representation in term of Wiener-Ito integrals: $$\psi=\sum_n I_n(f_n).$$ At this point an operator is introduced, namely $$\Gamma (A) \psi= \sum_n I_n(A^{\otimes n} f_n),$$ where $A$ is the Hamiltonian of a harmonic oscillator, $A=-d^2/dx^2 +x^2+1$ . The author of the notes I'm reading states that this is the ""second order quantization operator"", also T.Hida makes allusion to such operator. I went to Wikipedia to read about this ""second order quantization"" and  honestly I don't quite understand if the two things are related at all. Do you mind giving a brief (and ""for a dumb"") explanation on why this operator is called like that? I suspect it has to do with the Fock space, since the Homogeneous chaoses are related via an isometry with the  Hilbert  space symmetric tensor power. Thanks in advance.","As premise I know barely anything about physics and thus this question might be absolutely trivial or ill-posed. I am starting to study the White noise theory and when constructing the infinite-dimensional analogue of the Schwartz space , i.e. one way is to grab a random variable with chaotic representation in term of Wiener-Ito integrals: At this point an operator is introduced, namely where is the Hamiltonian of a harmonic oscillator, . The author of the notes I'm reading states that this is the ""second order quantization operator"", also T.Hida makes allusion to such operator. I went to Wikipedia to read about this ""second order quantization"" and  honestly I don't quite understand if the two things are related at all. Do you mind giving a brief (and ""for a dumb"") explanation on why this operator is called like that? I suspect it has to do with the Fock space, since the Homogeneous chaoses are related via an isometry with the  Hilbert  space symmetric tensor power. Thanks in advance.","S(\mathbb R^n) (S) \psi\in L^2(S(R),\mu) \psi=\sum_n I_n(f_n). \Gamma (A) \psi= \sum_n I_n(A^{\otimes n} f_n), A A=-d^2/dx^2 +x^2+1","['functional-analysis', 'stochastic-calculus', 'quantum-mechanics']"
47,Minimum Solution Over Closed Ball of $H_0^1(\Omega)$,Minimum Solution Over Closed Ball of,H_0^1(\Omega),"Let $\Omega\subset \mathbb{R}^n$ an open bounded domain. Let $\kappa:\Omega \to \mathbb{R}$ a continuous function which $\beta\leq \kappa(x)\leq M \quad \forall x \in \Omega$ , where $0<\beta,M.$ Define $$S:=\{v \in H_0^1(\Omega):\quad \|v\|_{H^1(\Omega)}\leq 1\}.$$ Show that for any $u \in H_{0}^1(\Omega)$ exists a unique $g \in S$ such that $$\int_{\Omega}\kappa(x)|\nabla u(x)-\nabla g(x)|^2dx=\min_{v \in S} \int_{\Omega}\kappa(x)|\nabla u(x)-\nabla v(x)|^2dx.$$ My attempts : Of course S is closed and convex subset of $H_0^1(\Omega)$ . According to the Approximation Theorem for Hilbert Spaces there is a unique $g \in S$ $$\| u-g\|^2_{H^1(\Omega)}=\min_{v\in S}\int_{\Omega}|u(x)-v(x)|^2+\int_{\Omega}|\nabla u(x)-\nabla v(x)|^2dx$$ and $\langle u-g,v\rangle_{H^1(\Omega)} = 0 \quad \forall v \in V $ . Now we can employ another inner product such as: $$\langle u,v \rangle_\kappa := \int_{\Omega} \frac{1}{\kappa(x)}\nabla u(x)\nabla v(x)dx+\int_{\Omega} \kappa(x)u(x)v(x)dx$$ and $\|u\|_{\kappa}=\langle u,u \rangle^{1/2}_\kappa$ . Of course, $\min\{\frac{1}{M},\beta\}\|u\|^2_{H^1(\Omega)}\leq \|u\|^2_\kappa \leq \max\{\frac{1}{\beta},M\}\|u\|^2_{H^1(\Omega)}$ , then $H^1_0(\Omega)$ is still a Hilbert space and $S$ remains closed and convex. So we can say that exist a unique $g \in S$ . Such that, $$\| u-g\|^2_{\kappa}=\min_{v\in S}\int_{\Omega}\frac{1}{\kappa(x)}|u(x)-v(x)|^2+\int_{\Omega}\kappa(x)|\nabla u(x)-\nabla v(x)|^2dx$$ Where $\langle u-g,v \rangle_\kappa = 0 \quad \forall v \in S$ . There is a way to get conclusion from here or we need to find another approach?","Let an open bounded domain. Let a continuous function which , where Define Show that for any exists a unique such that My attempts : Of course S is closed and convex subset of . According to the Approximation Theorem for Hilbert Spaces there is a unique and . Now we can employ another inner product such as: and . Of course, , then is still a Hilbert space and remains closed and convex. So we can say that exist a unique . Such that, Where . There is a way to get conclusion from here or we need to find another approach?","\Omega\subset \mathbb{R}^n \kappa:\Omega \to \mathbb{R} \beta\leq \kappa(x)\leq M \quad \forall x \in \Omega 0<\beta,M. S:=\{v \in H_0^1(\Omega):\quad \|v\|_{H^1(\Omega)}\leq 1\}. u \in H_{0}^1(\Omega) g \in S \int_{\Omega}\kappa(x)|\nabla u(x)-\nabla g(x)|^2dx=\min_{v \in S} \int_{\Omega}\kappa(x)|\nabla u(x)-\nabla v(x)|^2dx. H_0^1(\Omega) g \in S \| u-g\|^2_{H^1(\Omega)}=\min_{v\in S}\int_{\Omega}|u(x)-v(x)|^2+\int_{\Omega}|\nabla u(x)-\nabla v(x)|^2dx \langle u-g,v\rangle_{H^1(\Omega)} = 0 \quad \forall v \in V  \langle u,v \rangle_\kappa := \int_{\Omega} \frac{1}{\kappa(x)}\nabla u(x)\nabla v(x)dx+\int_{\Omega} \kappa(x)u(x)v(x)dx \|u\|_{\kappa}=\langle u,u \rangle^{1/2}_\kappa \min\{\frac{1}{M},\beta\}\|u\|^2_{H^1(\Omega)}\leq \|u\|^2_\kappa \leq \max\{\frac{1}{\beta},M\}\|u\|^2_{H^1(\Omega)} H^1_0(\Omega) S g \in S \| u-g\|^2_{\kappa}=\min_{v\in S}\int_{\Omega}\frac{1}{\kappa(x)}|u(x)-v(x)|^2+\int_{\Omega}\kappa(x)|\nabla u(x)-\nabla v(x)|^2dx \langle u-g,v \rangle_\kappa = 0 \quad \forall v \in S","['functional-analysis', 'hilbert-spaces', 'banach-spaces', 'sobolev-spaces']"
48,Proof of Stone's Theorem on unitary groups,Proof of Stone's Theorem on unitary groups,,"I dont understand a particular step in the proof of Stone's Theorem [ B.C. Hall, ""Quantum Theory for Mathematicians"",p.210-213]. Let me state the Theorem and explain where I got stuck. Stone's Theorem : Suppose $U(\cdot)$ is a strongly continuous one-parameter unitary group on a Hilbert space $H$ . Then the infinitesimal generator of $U(\cdot)$ is densely defined and self adjoint and $U(t)= e^{itA}$ for all $t\in\mathbb{R}$ . Proof: After proving that $A$ is densely defined one shows that $A$ is symmetric and essentially self adjoint. Then one defines $V(t):=e^{it\bar{A}}$ , where $\bar{A}$ is the closure of $A$ . Now it suffices to show that for an arbitrary $\psi\in Dom(A)$ the function $w(t):= U(t)\psi-V(t)\psi=0$ for all $t\in \mathbb{R}$ . Now comes the part where I have problems. In the book it is claimed that $$ \frac{d}{dt}w(t)=iAU(t)\psi-iAV(t)\psi $$ Since $A$ is the infinitesimal generator of $U$ it is clear that $\frac{d}{dt}(U(t)\psi)=iAU(t)\psi$ . The infinitesimal generator of $V(t)$ however is $\bar{A}$ , so that $\frac{d}{dt}(V(t)\psi)=i\bar{A}V(t)\psi$ . The equation above would follow if we knew that $V(t)\psi\in Dom(A)$ . It is also true that $\frac{d}{dt}(V(t)\psi)=iV(t)\bar{A}\psi=iV(t)A\psi$ , so it would also suffice to show that $V(t)$ and $A$ commute. I do not see why either of these conditions should be true. Maybe there is something else that I am missing here.","I dont understand a particular step in the proof of Stone's Theorem [ B.C. Hall, ""Quantum Theory for Mathematicians"",p.210-213]. Let me state the Theorem and explain where I got stuck. Stone's Theorem : Suppose is a strongly continuous one-parameter unitary group on a Hilbert space . Then the infinitesimal generator of is densely defined and self adjoint and for all . Proof: After proving that is densely defined one shows that is symmetric and essentially self adjoint. Then one defines , where is the closure of . Now it suffices to show that for an arbitrary the function for all . Now comes the part where I have problems. In the book it is claimed that Since is the infinitesimal generator of it is clear that . The infinitesimal generator of however is , so that . The equation above would follow if we knew that . It is also true that , so it would also suffice to show that and commute. I do not see why either of these conditions should be true. Maybe there is something else that I am missing here.","U(\cdot) H U(\cdot) U(t)= e^{itA} t\in\mathbb{R} A A V(t):=e^{it\bar{A}} \bar{A} A \psi\in Dom(A) w(t):= U(t)\psi-V(t)\psi=0 t\in \mathbb{R} 
\frac{d}{dt}w(t)=iAU(t)\psi-iAV(t)\psi
 A U \frac{d}{dt}(U(t)\psi)=iAU(t)\psi V(t) \bar{A} \frac{d}{dt}(V(t)\psi)=i\bar{A}V(t)\psi V(t)\psi\in Dom(A) \frac{d}{dt}(V(t)\psi)=iV(t)\bar{A}\psi=iV(t)A\psi V(t) A","['functional-analysis', 'proof-explanation', 'operator-theory', 'quantum-mechanics']"
49,Bound length of a curve in a ball,Bound length of a curve in a ball,,"Let $E$ be a $\mathbb R$ -Banach space, $v:E\to[1,\infty)$ be continuous and $v_i:[0,\infty)\to[1,\infty)$ be continuous and nondecreasing with $$v_1(\left\|x\right\|_E)\le v(x)\le v_2(\left\|x\right\|_E)\;\;\;\text{for all }x\in E,\tag1$$ $$v_1(a)\xrightarrow{a\to\infty}\infty\tag2$$ and $$av_2(a)\le C_1v_1^\theta(a)\;\;\;\text{for all }a>0\tag3$$ for some $C_1\ge0$ and $\theta\ge1$ . Now, let $r\in(0,1]$ and $$\rho(x,y):=\inf_{\substack{\gamma\:\in\:C^1([0,\:1],\:E)\\ \gamma(0)\:=\:x\\ \gamma(1)\:=\:y}}\int_0^1v^r\left(\gamma(t)\right)\left\|\gamma'(t)\right\|_E\:{\rm d}t\;\;\;\text{for }x,y\in E.$$ Let $k>0$ and $B_k$ denote the open ball around $0\in E$ with radius $k$ . Let $x,y\in E$ and $\varepsilon>0$ . By definition of the infimum, there is a $\gamma\in C^1([0,1],E)$ with $\gamma(0)=x$ , $\gamma(1)=y$ and $$\rho(x,y)\le\int_0^1v^r\left(\gamma(t)\right)\left\|\gamma'(t)\right\|_E\:{\rm d}t<\rho(x,y)+\varepsilon\tag4.$$ Question : Why can we conclude that $$\int_0^11_{B_k}(\gamma(t))\left\|\gamma'(t)\right\|_E\:{\rm d}t\le 2k\left(\frac{v_2(k)}{v_1(0)}\right)^r+\varepsilon?\tag5$$ The argument should be that we could otherwise replace the corresponding piece of curve by a straight line and obtain a value which differed from $\rho(x,y)$ by more than $\varepsilon$ , but how can we show this rigorously? EDIT : I mean, by $(1)$ , there is the trivial inequality $$1\le\frac{v_2(\left\|z\right\|_E)}{v_1(\left\|z\right\|_E)}\le\frac{v_2(k)}{v_1(0)}\;\;\;\text{for all }z\in B_k(0)\tag6$$ and I guess a variant of this needs to be used. EDIT 2 : The inequality is clearly trivial, when $\gamma$ never enters $B_k$ . Maybe it's useful to consider the entrance and exit times/points: Let $\sigma_0:=\tau_0:=0$ , \begin{align}\sigma_n&:=\inf\{t\in(\tau_{n-1},1):\gamma(t)\in B_k\},\\\tau_n&:=\inf\{t\in(\sigma_n,1):\gamma(t)\not\in B_k\}\wedge 1\end{align} for $n\in\mathbb N$ , $N:=\{n\in\mathbb N:\sigma_n<\infty\}$ and \begin{align}x_n&:=\gamma(\sigma_n),\\y_n&:=\gamma(\tau_n)\end{align} and $$c_n(t):=\frac{t(y_n-x_n)+\tau_nx_n-\sigma_ny_n}{\tau_n-\sigma_n}\;\;\;\text{for }t\in[\sigma_n,\tau_n]$$ be the straight line connecting $x_n$ and $y_n$ for $n\in N$ . The left-hand side of $(5)$ can then be rewritten as $$\int_0^11_{B_k}(\gamma(t))\left\|\gamma'(t)\right\|_E\:{\rm d}t=\sum_{n\in N}\int_{\sigma_n}^{\tau_n}\left\|\gamma'(t)\right\|_E\:{\rm d}t\tag7.$$ Maybe it can be shown that if the desired inequality doesn't hold, we could replace $\gamma$ on $[\sigma_n,\tau_n]$ with $c_n$ and obtain a value of the integral in $(4)$ which differs by more than $\varepsilon$ from $\rho(x,y)$ . Assume, for simplicity, that $N=\{1\}$ and let $$\tilde\gamma(t):=\left.\begin{cases}\gamma(t)&\text{, if }t\in[0,\sigma_1]\\ c_1(t)&\text{, if }t\in[\sigma_1,\tau_1]\\\gamma(t)&\text{, if }t\in[\tau_1,1]\end{cases}\right\}\;\;\;\text{for }t\in[0,1].$$ We may clearly note that, by construction, $$c_1((\sigma_1,\tau_1))\subseteq B_k\tag8$$ and hence $$\int_{\sigma_1}^{\tau_1}v^r(\tilde\gamma(t))\left\|\tilde\gamma'(t)\right\|_E\:{\rm d}t=\frac{\left\|x_1-y_1\right\|_E}{\tau_1-\sigma_1}\int_{\sigma_1}^{\tau_1}v^r(c_1(t))\le 2kv^r_2(k)\tag9$$ by $(1)$ . Now, maybe we need to use $(6)$ , $v_1\ge1$ and $r\le1$ to obtain $$v^r_2(k)\le\left(\frac{v_2(k)}{v_1(0)}\right)^r\le\left(\frac{v_2(\left\|z\right\|_E)}{v_1(\left\|z\right\|_E)}\right)^r\;\;\;\text{for all }z\in B_k\tag{10}.$$ I think I'm close but still can't complete the puzzle. EDIT 3 : I think we can argue in the following way: Assume $\sigma_1<\infty$ so that the curve enters the ball $B_k$ at time $\sigma_1$ . Replace $\gamma$ on $[\sigma_1,\tau_1]$ by $c_1$ , which yields the curve $\tilde\gamma$ (as defined above). If $(5)$ would not hold, then \begin{equation}\begin{split}\int_{\sigma_1}^{\tau_1}v^r(\tilde\gamma(t))\left\|\tilde\gamma'(t)\right\|_E\:{\rm d}t&\le 2kv_2^r(k)\le2ke\left(\frac{v_2(k)}{v_1(0)}\right)^r\\&<2ke\left(\frac{v_2(k)}{v_1(0)}\right)^r+\varepsilon<\int_{\sigma_1}^{\tau_1}1_{B_k}(\gamma(t))\left\|\gamma'(t)\right\|_E\:{\rm d}t\\&\le\int_{\sigma_1}^{\tau_1}\underbrace{v^r(\gamma(t))}_{\ge\:1}\left\|\gamma'(t)\right\|_E\:{\rm d}t\end{split}\tag{11}\end{equation} by $(9)$ and $(10)$ . Is this a contradiction to $(4)$ ?","Let be a -Banach space, be continuous and be continuous and nondecreasing with and for some and . Now, let and Let and denote the open ball around with radius . Let and . By definition of the infimum, there is a with , and Question : Why can we conclude that The argument should be that we could otherwise replace the corresponding piece of curve by a straight line and obtain a value which differed from by more than , but how can we show this rigorously? EDIT : I mean, by , there is the trivial inequality and I guess a variant of this needs to be used. EDIT 2 : The inequality is clearly trivial, when never enters . Maybe it's useful to consider the entrance and exit times/points: Let , for , and and be the straight line connecting and for . The left-hand side of can then be rewritten as Maybe it can be shown that if the desired inequality doesn't hold, we could replace on with and obtain a value of the integral in which differs by more than from . Assume, for simplicity, that and let We may clearly note that, by construction, and hence by . Now, maybe we need to use , and to obtain I think I'm close but still can't complete the puzzle. EDIT 3 : I think we can argue in the following way: Assume so that the curve enters the ball at time . Replace on by , which yields the curve (as defined above). If would not hold, then by and . Is this a contradiction to ?","E \mathbb R v:E\to[1,\infty) v_i:[0,\infty)\to[1,\infty) v_1(\left\|x\right\|_E)\le v(x)\le v_2(\left\|x\right\|_E)\;\;\;\text{for all }x\in E,\tag1 v_1(a)\xrightarrow{a\to\infty}\infty\tag2 av_2(a)\le C_1v_1^\theta(a)\;\;\;\text{for all }a>0\tag3 C_1\ge0 \theta\ge1 r\in(0,1] \rho(x,y):=\inf_{\substack{\gamma\:\in\:C^1([0,\:1],\:E)\\ \gamma(0)\:=\:x\\ \gamma(1)\:=\:y}}\int_0^1v^r\left(\gamma(t)\right)\left\|\gamma'(t)\right\|_E\:{\rm d}t\;\;\;\text{for }x,y\in E. k>0 B_k 0\in E k x,y\in E \varepsilon>0 \gamma\in C^1([0,1],E) \gamma(0)=x \gamma(1)=y \rho(x,y)\le\int_0^1v^r\left(\gamma(t)\right)\left\|\gamma'(t)\right\|_E\:{\rm d}t<\rho(x,y)+\varepsilon\tag4. \int_0^11_{B_k}(\gamma(t))\left\|\gamma'(t)\right\|_E\:{\rm d}t\le 2k\left(\frac{v_2(k)}{v_1(0)}\right)^r+\varepsilon?\tag5 \rho(x,y) \varepsilon (1) 1\le\frac{v_2(\left\|z\right\|_E)}{v_1(\left\|z\right\|_E)}\le\frac{v_2(k)}{v_1(0)}\;\;\;\text{for all }z\in B_k(0)\tag6 \gamma B_k \sigma_0:=\tau_0:=0 \begin{align}\sigma_n&:=\inf\{t\in(\tau_{n-1},1):\gamma(t)\in B_k\},\\\tau_n&:=\inf\{t\in(\sigma_n,1):\gamma(t)\not\in B_k\}\wedge 1\end{align} n\in\mathbb N N:=\{n\in\mathbb N:\sigma_n<\infty\} \begin{align}x_n&:=\gamma(\sigma_n),\\y_n&:=\gamma(\tau_n)\end{align} c_n(t):=\frac{t(y_n-x_n)+\tau_nx_n-\sigma_ny_n}{\tau_n-\sigma_n}\;\;\;\text{for }t\in[\sigma_n,\tau_n] x_n y_n n\in N (5) \int_0^11_{B_k}(\gamma(t))\left\|\gamma'(t)\right\|_E\:{\rm d}t=\sum_{n\in N}\int_{\sigma_n}^{\tau_n}\left\|\gamma'(t)\right\|_E\:{\rm d}t\tag7. \gamma [\sigma_n,\tau_n] c_n (4) \varepsilon \rho(x,y) N=\{1\} \tilde\gamma(t):=\left.\begin{cases}\gamma(t)&\text{, if }t\in[0,\sigma_1]\\ c_1(t)&\text{, if }t\in[\sigma_1,\tau_1]\\\gamma(t)&\text{, if }t\in[\tau_1,1]\end{cases}\right\}\;\;\;\text{for }t\in[0,1]. c_1((\sigma_1,\tau_1))\subseteq B_k\tag8 \int_{\sigma_1}^{\tau_1}v^r(\tilde\gamma(t))\left\|\tilde\gamma'(t)\right\|_E\:{\rm d}t=\frac{\left\|x_1-y_1\right\|_E}{\tau_1-\sigma_1}\int_{\sigma_1}^{\tau_1}v^r(c_1(t))\le 2kv^r_2(k)\tag9 (1) (6) v_1\ge1 r\le1 v^r_2(k)\le\left(\frac{v_2(k)}{v_1(0)}\right)^r\le\left(\frac{v_2(\left\|z\right\|_E)}{v_1(\left\|z\right\|_E)}\right)^r\;\;\;\text{for all }z\in B_k\tag{10}. \sigma_1<\infty B_k \sigma_1 \gamma [\sigma_1,\tau_1] c_1 \tilde\gamma (5) \begin{equation}\begin{split}\int_{\sigma_1}^{\tau_1}v^r(\tilde\gamma(t))\left\|\tilde\gamma'(t)\right\|_E\:{\rm d}t&\le 2kv_2^r(k)\le2ke\left(\frac{v_2(k)}{v_1(0)}\right)^r\\&<2ke\left(\frac{v_2(k)}{v_1(0)}\right)^r+\varepsilon<\int_{\sigma_1}^{\tau_1}1_{B_k}(\gamma(t))\left\|\gamma'(t)\right\|_E\:{\rm d}t\\&\le\int_{\sigma_1}^{\tau_1}\underbrace{v^r(\gamma(t))}_{\ge\:1}\left\|\gamma'(t)\right\|_E\:{\rm d}t\end{split}\tag{11}\end{equation} (9) (10) (4)","['functional-analysis', 'analysis', 'differential-geometry', 'curves', 'supremum-and-infimum']"
50,"Exercise 5.22 in Brezis, ""Functional Analysis Sobolev Spaces and Partial Differential Equations"".","Exercise 5.22 in Brezis, ""Functional Analysis Sobolev Spaces and Partial Differential Equations"".",,"Let $H$ be a Hilbert space, $C\subseteq H$ a nonempty closed convex set and $T:C\to C$ a nonlinear contraction, that is $$ (*)\qquad|Tu - Tv| \leq |u-v|. $$ Let $(u_n)$ be a sequence in $C$ such that $u_n\rightharpoonup u$ weakly in $H$ and $u_n - Tu_n\to f$ strongly in $H$ . I want to prove that $f = u - Tu$ . My attempt : From inequality $(*)$ it is easy to prove that $$ ((w-Tw)-(v-Tv),w-v)\geq 0, \qquad \forall w,v\in C. $$ Put $w = u_n$ , then as $u_n - v\rightharpoonup u-v$ weakly and $u_n - Tu_n - (v - T)\to f-(v-T)$ strongly, we obtain that $$ (**)\qquad (f-(v-Tv),u-v)\geq 0, \qquad \forall v\in C. $$ Because $C$ is convex we have that $u\in C$ . I believe that from inequality $(**)$ we can prove that $(u,f)$ belongs to the graph of $I-T$ , but I'm stuck here. Maybe my attempt is not the right way to prove the result, so any help willbe appreciated. The book then asks to use this result to prove that if $C$ is bounded, then $T$ has a fixed point (this is the reason I'm adding the ""fixed-point-theorems"" tag), but this is a simple consequence of the Banach fixed point theorem and the above result.","Let be a Hilbert space, a nonempty closed convex set and a nonlinear contraction, that is Let be a sequence in such that weakly in and strongly in . I want to prove that . My attempt : From inequality it is easy to prove that Put , then as weakly and strongly, we obtain that Because is convex we have that . I believe that from inequality we can prove that belongs to the graph of , but I'm stuck here. Maybe my attempt is not the right way to prove the result, so any help willbe appreciated. The book then asks to use this result to prove that if is bounded, then has a fixed point (this is the reason I'm adding the ""fixed-point-theorems"" tag), but this is a simple consequence of the Banach fixed point theorem and the above result.","H C\subseteq H T:C\to C 
(*)\qquad|Tu - Tv| \leq |u-v|.
 (u_n) C u_n\rightharpoonup u H u_n - Tu_n\to f H f = u - Tu (*) 
((w-Tw)-(v-Tv),w-v)\geq 0, \qquad \forall w,v\in C.
 w = u_n u_n - v\rightharpoonup u-v u_n - Tu_n - (v - T)\to f-(v-T) 
(**)\qquad (f-(v-Tv),u-v)\geq 0, \qquad \forall v\in C.
 C u\in C (**) (u,f) I-T C T","['functional-analysis', 'hilbert-spaces', 'weak-convergence', 'fixed-point-theorems']"
51,Approximation of positive definite functions by neural networks,Approximation of positive definite functions by neural networks,,"Bochner's theorem shows that probability measures $\mu$ are linked with positive definite functions via Fourier transform: $f(k) = \int_{\mathbb{R}^n} e^{-2 \pi i k x} \,d\mu(x)$ Currently, probability measures can be very well approximated by various generative models based on neural networks. What about positive definite functions? Is there any natural approximation tool, based on neural network? Udp: I can that question myself but this answer does not satisfy me. Let us assume that $f(k)$ is real-valued, i.e. $\mu$ is symmetric w.r.t. to origin. Then, it is natural to approximate $f$ by the single layer neural network with cosine activation fucntion: $f(x) = \sum_{i=1}^N \theta_i cos(\omega_i^T x)$ where $\theta_i\geq 0$ . My question is: are there any more powerfull (many-layer) networks, for this kind of functions?","Bochner's theorem shows that probability measures are linked with positive definite functions via Fourier transform: Currently, probability measures can be very well approximated by various generative models based on neural networks. What about positive definite functions? Is there any natural approximation tool, based on neural network? Udp: I can that question myself but this answer does not satisfy me. Let us assume that is real-valued, i.e. is symmetric w.r.t. to origin. Then, it is natural to approximate by the single layer neural network with cosine activation fucntion: where . My question is: are there any more powerfull (many-layer) networks, for this kind of functions?","\mu f(k) = \int_{\mathbb{R}^n} e^{-2 \pi i k x} \,d\mu(x) f(k) \mu f f(x) = \sum_{i=1}^N \theta_i cos(\omega_i^T x) \theta_i\geq 0","['functional-analysis', 'machine-learning', 'approximation-theory', 'neural-networks']"
52,Lax-Milgram theorem proof in Brezis book.,Lax-Milgram theorem proof in Brezis book.,,"I am reading Brezis book ""FA, Sobolev Sp. and PDEs"" and I am working through the proof of Stampacchia theorem (5.6 page 138 2010 edition) and I am particulary interested in Lax-Milgram theorem (which is given as a Corollary 5.8) . Lax-Milgram theorem is proven as a corollary of Stampacchia theorem invoking corollary 5.4 , which states Suppose $W$ is a closed linear subspace of $H.$ For $x\in H,$ $y=P_Kx$ is characterized by the property that for all $\omega \in W$ $$ y\in W \ \text{and} \ \langle x-y, \omega \rangle =0.$$ How does one prove Lax Milgram from this? My explanation What I would say is that taking $H=K$ we argue as in Stampacchia theorem (keeping in mind that for $H$ the above argument applies with $H=W$ )   to obtain the unique $u \in H$ such that for all $v \in H$ $$a(u,v-u+u)=\ell (v-u+u),$$ where we have an equality ( $= 0$ ) instead of an inequality ( $\leq 0$ ) precisely because we have so in the corollary 5.4, and the same goes about the $+u$ factor. And then when $a$ is symmetric the Stampacchia's argument completely carries over without any change so that the minimizing function is the same but we minimize it over $H$ instead of $K.$ Is it correct? I would like to make it more precise, any suggestions? Note I found this recent question which asks the same thing but has received answers that are not very to the point, since they just give standard proofs of Lax-Milgram without any reference to Stampacchia's theorem, which is not what the other (and my own) question was about at all. So I ask hoping to receive a more on-topic answer.","I am reading Brezis book ""FA, Sobolev Sp. and PDEs"" and I am working through the proof of Stampacchia theorem (5.6 page 138 2010 edition) and I am particulary interested in Lax-Milgram theorem (which is given as a Corollary 5.8) . Lax-Milgram theorem is proven as a corollary of Stampacchia theorem invoking corollary 5.4 , which states Suppose is a closed linear subspace of For is characterized by the property that for all How does one prove Lax Milgram from this? My explanation What I would say is that taking we argue as in Stampacchia theorem (keeping in mind that for the above argument applies with )   to obtain the unique such that for all where we have an equality ( ) instead of an inequality ( ) precisely because we have so in the corollary 5.4, and the same goes about the factor. And then when is symmetric the Stampacchia's argument completely carries over without any change so that the minimizing function is the same but we minimize it over instead of Is it correct? I would like to make it more precise, any suggestions? Note I found this recent question which asks the same thing but has received answers that are not very to the point, since they just give standard proofs of Lax-Milgram without any reference to Stampacchia's theorem, which is not what the other (and my own) question was about at all. So I ask hoping to receive a more on-topic answer.","W H. x\in H, y=P_Kx \omega \in W  y\in W
\ \text{and} \ \langle x-y, \omega \rangle =0. H=K H H=W u \in H v \in H a(u,v-u+u)=\ell (v-u+u), = 0 \leq 0 +u a H K.","['real-analysis', 'functional-analysis']"
53,Need help in proving an inclusion between some subspaces of operators,Need help in proving an inclusion between some subspaces of operators,,"Let $V$ be a closed subspace of $B(H,K)$ such that $xy^*z \in V$ for all $x,y,z \in V$ . Let $I$ be an ideal $(IV^*V+VV^*I \subset I)$ of $V$ . Let $C(I)$ denote the $C^{\ast}$ -algebra generated by $II^{\ast}$ . In a paper the following inclusion is stated without proof: $VI^{\ast} \subset C(I)$ I don’t see how this can be true, since $C(I)$ is generated by $II^{\ast}$ therefore how could $VI^*$ be included inside $C(I)$ ?","Let be a closed subspace of such that for all . Let be an ideal of . Let denote the -algebra generated by . In a paper the following inclusion is stated without proof: I don’t see how this can be true, since is generated by therefore how could be included inside ?","V B(H,K) xy^*z \in V x,y,z \in V I (IV^*V+VV^*I \subset I) V C(I) C^{\ast} II^{\ast} VI^{\ast} \subset C(I) C(I) II^{\ast} VI^* C(I)","['functional-analysis', 'operator-theory', 'c-star-algebras']"
54,Equivalent norms on Sobolev spaces. Only function and last derivative needed to define the norm.,Equivalent norms on Sobolev spaces. Only function and last derivative needed to define the norm.,,"Equip $W_n^{p}[0,1]$ with the norm $$\left\|f\right\|_{W_n^{p}} = \sum_{k=0}^{n}\left\|f^{(k)}\right\|_{L^p}.$$ I want to prove that this norm is equivalent to the norm $$\left\|f\right\|_2 = \left\|f\right\|_{L^p}+\left\|f^{(n)}\right\|_{L^p}.$$ It is obvious that $\left\|f\right\|_2\leq \left\|f\right\|_{W_n^p}$ so I want to find a constant $C>0$ such that $\left\|f\right\|_{W_n^p}\leq C\left\|f\right\|_2$ . I've managed to solve this in the case $n=2$ but I don't know how to generalize this to the case $n>2$ . Solution in the case $n=2$ : Let $f\in W^{2}_p[0,1]$ . Let $\xi_{\mathrm{min}}$ and $\xi_{\mathrm{min}}'$ be points which minimizes $|f|$ and $|f'|$ respectively. Note that these functions are continuous so these points exists. Now Hölder's inequality implies that \begin{align*} 		|f'(x)|& = \left|f'(\xi_{\mathrm{min}}')+\int_{\xi_{\mathrm{min}}'}^{x}f''(t)\, dt\right| \\ 		& \leq |f'(\xi_{\mathrm{min}}')|+\int_{0}^{1}|f''(t)|\, dt \\  		& \leq |f'(\xi_{\mathrm{min}}')|+\left(\int_{0}^{1}|f''(t)|^{p}\,dt\right)^{1/p} 	\end{align*} and by the mean-value theorem \begin{align*} 		2|f(x)|\geq |f(x)-f(\xi_{\mathrm{min}})|=|f'(\xi)||x-\xi_{\mathrm{min}}|\geq |f'(\xi_{\mathrm{min}})||x-\xi_{\mathrm{min}}| 	\end{align*} for some $\xi$ between $x$ and $\xi_{\mathrm{min}}$ . Integrating this inequality yields \begin{equation*} 		\frac{|f'(\xi_{\mathrm{min}}')|}{2} = \int_{0}^{1}|f'(\xi_{\mathrm{min}}')||x-\xi_{\mathrm{min}}|\, dx\leq 2\int_{0}^{1}|f(x)|\, dx \leq 2\|f\|_{L^p} 	\end{equation*} why \begin{equation*} 		|f'(x)|\leq 4\|f\|_{L^p}+\|f''\|_{L^p} \Rightarrow \|f'\|_{L^p}\leq 4\|f\|_{L^p}+\|f''\|_{L^p}	 \leq 4\|f\|_2 	\end{equation*} Now I don't see a way to generalise this method to the case $n>2$ using induction. For instance if we consider the case $n=3$ then applying the method to $f'$ we obtain $$\|f'\|_{L^p} \leq 4\|f\|_{L^p}+\|f''\|_{L^p}$$ but when we do the same estimate on $f''$ we again get a term containing $f'$ so this reasoning becomes circular. This question has been previously asked in Sobolev space and equivalence of norms . The answer there however seems to be incomplete as it assumes that $f^{(k)}(0) = 0$ for every $k$ and the above calculation in the case that $n=2$ can be seen as a way to get around this fact using calculus.",Equip with the norm I want to prove that this norm is equivalent to the norm It is obvious that so I want to find a constant such that . I've managed to solve this in the case but I don't know how to generalize this to the case . Solution in the case : Let . Let and be points which minimizes and respectively. Note that these functions are continuous so these points exists. Now Hölder's inequality implies that and by the mean-value theorem for some between and . Integrating this inequality yields why Now I don't see a way to generalise this method to the case using induction. For instance if we consider the case then applying the method to we obtain but when we do the same estimate on we again get a term containing so this reasoning becomes circular. This question has been previously asked in Sobolev space and equivalence of norms . The answer there however seems to be incomplete as it assumes that for every and the above calculation in the case that can be seen as a way to get around this fact using calculus.,"W_n^{p}[0,1] \left\|f\right\|_{W_n^{p}} = \sum_{k=0}^{n}\left\|f^{(k)}\right\|_{L^p}. \left\|f\right\|_2 = \left\|f\right\|_{L^p}+\left\|f^{(n)}\right\|_{L^p}. \left\|f\right\|_2\leq \left\|f\right\|_{W_n^p} C>0 \left\|f\right\|_{W_n^p}\leq C\left\|f\right\|_2 n=2 n>2 n=2 f\in W^{2}_p[0,1] \xi_{\mathrm{min}} \xi_{\mathrm{min}}' |f| |f'| \begin{align*}
		|f'(x)|& = \left|f'(\xi_{\mathrm{min}}')+\int_{\xi_{\mathrm{min}}'}^{x}f''(t)\, dt\right| \\
		& \leq |f'(\xi_{\mathrm{min}}')|+\int_{0}^{1}|f''(t)|\, dt \\ 
		& \leq |f'(\xi_{\mathrm{min}}')|+\left(\int_{0}^{1}|f''(t)|^{p}\,dt\right)^{1/p}
	\end{align*} \begin{align*}
		2|f(x)|\geq |f(x)-f(\xi_{\mathrm{min}})|=|f'(\xi)||x-\xi_{\mathrm{min}}|\geq |f'(\xi_{\mathrm{min}})||x-\xi_{\mathrm{min}}|
	\end{align*} \xi x \xi_{\mathrm{min}} \begin{equation*}
		\frac{|f'(\xi_{\mathrm{min}}')|}{2} = \int_{0}^{1}|f'(\xi_{\mathrm{min}}')||x-\xi_{\mathrm{min}}|\, dx\leq 2\int_{0}^{1}|f(x)|\, dx \leq 2\|f\|_{L^p}
	\end{equation*} \begin{equation*}
		|f'(x)|\leq 4\|f\|_{L^p}+\|f''\|_{L^p} \Rightarrow \|f'\|_{L^p}\leq 4\|f\|_{L^p}+\|f''\|_{L^p}	 \leq 4\|f\|_2
	\end{equation*} n>2 n=3 f' \|f'\|_{L^p} \leq 4\|f\|_{L^p}+\|f''\|_{L^p} f'' f' f^{(k)}(0) = 0 k n=2","['functional-analysis', 'sobolev-spaces']"
55,$f_n \to f$ a.e. and $\| f_n\|_p \to \|f\|_p$. Is $\{f_n\}$ dominated by some $g$?,a.e. and . Is  dominated by some ?,f_n \to f \| f_n\|_p \to \|f\|_p \{f_n\} g,"Let $E\subset \mathcal{M}(\mathbb{R}^n)$ with $m(E)>0$ , $\{f_j\}_{j\in \mathbb{N}}\subset \mathcal{L}^p(E)$ and $f\in \mathcal{L}^p(E).$ Let $1\leq p < + \infty$ and suppose that $f_j\to f$ almost everywhere on E and $||f_j||_p\to ||f||_p$ . I want to prove the standard fact that $||f_j-f||_p\to 0.$ I know I have the inequality $$|f-f_n|^p \leq (2\max(|f|,|f_n|))^p = 2^p \max(|f|^p,|f_n|^p) \leq 2^p (|f|^p + |f_n|^p)$$ for any $p>0.$ So, supposing the sequence $\{f_n\}$ is dominated by some $g$ , we have the bound: $$\{h_n=|f-f_n|^p\}\leq 2^{p+1}g^p$$ Since $h_n \to 0$ almost everywhere, applying dominated convergence theorem we get: $$\text{lim}||f-f_n||_p^p=\text{lim}\int_E|f-f_n|^p=0$$ hence $$\text{lim}||f-f_n||_p=\left(\text{lim}||f-f_n||_p^p\right)^{1/p}=0.$$ Is it correct until now? Supposing it is correct, one is left to prove that $\{f_n\}$ is indeed bounded. How to show that $f_j\to f$ almost everywhere on E and $||f_j||_p\to ||f||_p$ imply that ${f_n}$ is dominated so that one can apply dominated convergence? Note that there already a few questions on this site that address in some way the statement I want to prove. But I would like to know if my own reasoning is correct and moreover none of them addresses my specific questions.","Let with , and Let and suppose that almost everywhere on E and . I want to prove the standard fact that I know I have the inequality for any So, supposing the sequence is dominated by some , we have the bound: Since almost everywhere, applying dominated convergence theorem we get: hence Is it correct until now? Supposing it is correct, one is left to prove that is indeed bounded. How to show that almost everywhere on E and imply that is dominated so that one can apply dominated convergence? Note that there already a few questions on this site that address in some way the statement I want to prove. But I would like to know if my own reasoning is correct and moreover none of them addresses my specific questions.","E\subset \mathcal{M}(\mathbb{R}^n) m(E)>0 \{f_j\}_{j\in \mathbb{N}}\subset \mathcal{L}^p(E) f\in \mathcal{L}^p(E). 1\leq p < + \infty f_j\to f ||f_j||_p\to ||f||_p ||f_j-f||_p\to 0. |f-f_n|^p \leq (2\max(|f|,|f_n|))^p = 2^p \max(|f|^p,|f_n|^p) \leq 2^p (|f|^p + |f_n|^p) p>0. \{f_n\} g \{h_n=|f-f_n|^p\}\leq 2^{p+1}g^p h_n \to 0 \text{lim}||f-f_n||_p^p=\text{lim}\int_E|f-f_n|^p=0 \text{lim}||f-f_n||_p=\left(\text{lim}||f-f_n||_p^p\right)^{1/p}=0. \{f_n\} f_j\to f ||f_j||_p\to ||f||_p {f_n}","['real-analysis', 'functional-analysis']"
56,Linear dependence of entire functions.,Linear dependence of entire functions.,,"Given two entire function f and g , suppose exp( f ) , exp( g ) and 1 are linearly dependent in the complex field, how can we show that f , g , and 1 are also linearly dependent ?","Given two entire function f and g , suppose exp( f ) , exp( g ) and 1 are linearly dependent in the complex field, how can we show that f , g , and 1 are also linearly dependent ?",,"['complex-analysis', 'functional-analysis']"
57,Sum of Banach spaces norm,Sum of Banach spaces norm,,"Let $ \left ( X_{1},\left \|  \right \|_{1} \right ) $ , $ \left ( X_{2},\left \|  \right \|_{2} \right ) $ two Banach spaces in the vector space X. How to prove that $\left \| x \right \|= \inf \left \{\left \| x_{1} \right \|_{1}+\left \| x_{2} \right \|_{2}:x=x_{1}+x_{2} \ , \ x_{1}\in X_{1} \ \ x_{2}\in X_{2} \right \} $ defines a norm in $ X_{1}+X_{2} $ My attempt : $ \left \| x \right \|=0 \Leftrightarrow \exists \left ( x_{n} \right )_{n\in \mathbb{N}}\subset X_{1} \ ,  \exists \left ( y_{n} \right )_{n\in \mathbb{N}}\subset X_{2}: x=x_{n}+y_{n} \ and \ \lim_{n\rightarrow \infty}\left \| x_{n} \right \|_{1}=0 \ $ and $ \lim_{n\rightarrow \infty}\left \| y_{n} \right \|_{2}=0 $ I can't prove that $x=0$","Let , two Banach spaces in the vector space X. How to prove that defines a norm in My attempt : and I can't prove that"," \left ( X_{1},\left \|  \right \|_{1} \right )   \left ( X_{2},\left \|  \right \|_{2} \right )  \left \| x \right \|= \inf \left \{\left \| x_{1} \right \|_{1}+\left \| x_{2} \right \|_{2}:x=x_{1}+x_{2} \ , \ x_{1}\in X_{1} \ \ x_{2}\in X_{2} \right \}   X_{1}+X_{2}   \left \| x \right \|=0 \Leftrightarrow \exists \left ( x_{n} \right )_{n\in \mathbb{N}}\subset X_{1} \ ,  \exists \left ( y_{n} \right )_{n\in \mathbb{N}}\subset X_{2}: x=x_{n}+y_{n} \ and \ \lim_{n\rightarrow \infty}\left \| x_{n} \right \|_{1}=0 \   \lim_{n\rightarrow \infty}\left \| y_{n} \right \|_{2}=0  x=0","['functional-analysis', 'banach-spaces']"
58,Integral with respect to spectral measure,Integral with respect to spectral measure,,"Let $A:D(A)\subset H \to H$ be a self-adjoint unbounded operator on complex Hilbertspace $H$ with corresponding spectral measure $E:\mathcal{B}(\mathbb{R})\to\mathcal{L}(H)$ . I want to show  that an element $u\in H$ with $\Vert u \Vert =1$ and $E(I)u=u$ for some open interval $I$ is $ u \in D(A)$ , that is $$\int_\mathbb{R} \lambda^2 d\Vert E(\lambda)u \Vert^2  < \infty.\quad (*)$$ I can think of two approaches: measure theory  or generalisation of Riemann–Stieltjes integral. From m.t. point of view the integral in (*) means $$ \int_\mathbb{R} \lambda^2 d \mu_u $$ where $\mu_u$ is a measure on Borel sets  defined by $\mu_u(S)=\langle E(S)u,u\rangle.$ In order to calculate this integral I can use monotone convergence theorem. Since it's only matter of construction, let's assume we found a sequence $f_k:\mathbb{R}\to \mathbb R $ with $f_k \uparrow \lambda^2$ , so we may write $$ \int_\mathbb{R} \lambda^2 d \mu_u=\lim \int_\mathbb{R} f_k d \mu_u .$$ But that isn't really helpfull... If we try second approach, then (*) is defined as $$ \int_\mathbb{R} \lambda^2 d\Vert E(\lambda)u \Vert^2=\lim_{\text{ partition of } \mathbb{R}  \to 0} \sum t_i^2( \Vert E(\lambda_i)u \Vert^2-\Vert E(\lambda_{i-1})u \Vert^2 ), $$ where $t_i\in (\lambda_{i-1}, \lambda_i)$ and $E(\lambda):=E((-\infty,\lambda])$ . Again it's not obvious for me that this leads anywhere. Any help, hints or suggestions are greatly appreciated! Edit 1: Let's try again. @N.S. comment suggests, it suffices to show $E(I)u \in D(A)$ , that is $$\int_\mathbb{R} \lambda^2 d \mu_{E(I)u}< \infty.$$ We note $\mu_{E(I)u}(S)=\langle E(I\cap S)u,u \rangle=\mu_u(I\cap S)=:\nu_u(S)$ for any measurable set $S$ . Coincidentally $\nu_u$ is also a measure! Question 1: Is $$ \int_\mathbb{R} \lambda^2 d \mu_{E(I)u}= \int_\mathbb{R} \lambda^2 d \nu_u=\int_{I} \lambda^2 d \mu_u \leq sup_{\lambda \in I} \{\lambda ^2\} \mu_u(I) <\infty$$ true? If so, how is second ""="" justifiable?","Let be a self-adjoint unbounded operator on complex Hilbertspace with corresponding spectral measure . I want to show  that an element with and for some open interval is , that is I can think of two approaches: measure theory  or generalisation of Riemann–Stieltjes integral. From m.t. point of view the integral in (*) means where is a measure on Borel sets  defined by In order to calculate this integral I can use monotone convergence theorem. Since it's only matter of construction, let's assume we found a sequence with , so we may write But that isn't really helpfull... If we try second approach, then (*) is defined as where and . Again it's not obvious for me that this leads anywhere. Any help, hints or suggestions are greatly appreciated! Edit 1: Let's try again. @N.S. comment suggests, it suffices to show , that is We note for any measurable set . Coincidentally is also a measure! Question 1: Is true? If so, how is second ""="" justifiable?","A:D(A)\subset H \to H H E:\mathcal{B}(\mathbb{R})\to\mathcal{L}(H) u\in H \Vert u \Vert =1 E(I)u=u I  u \in D(A) \int_\mathbb{R} \lambda^2 d\Vert E(\lambda)u \Vert^2  < \infty.\quad (*)  \int_\mathbb{R} \lambda^2 d \mu_u  \mu_u \mu_u(S)=\langle E(S)u,u\rangle. f_k:\mathbb{R}\to \mathbb R  f_k \uparrow \lambda^2  \int_\mathbb{R} \lambda^2 d \mu_u=\lim \int_\mathbb{R} f_k d \mu_u .  \int_\mathbb{R} \lambda^2 d\Vert E(\lambda)u \Vert^2=\lim_{\text{ partition of } \mathbb{R}  \to 0} \sum t_i^2( \Vert E(\lambda_i)u \Vert^2-\Vert E(\lambda_{i-1})u \Vert^2 ),  t_i\in (\lambda_{i-1}, \lambda_i) E(\lambda):=E((-\infty,\lambda]) E(I)u \in D(A) \int_\mathbb{R} \lambda^2 d \mu_{E(I)u}< \infty. \mu_{E(I)u}(S)=\langle E(I\cap S)u,u \rangle=\mu_u(I\cap S)=:\nu_u(S) S \nu_u  \int_\mathbb{R} \lambda^2 d \mu_{E(I)u}= \int_\mathbb{R} \lambda^2 d \nu_u=\int_{I} \lambda^2 d \mu_u \leq sup_{\lambda \in I} \{\lambda ^2\} \mu_u(I) <\infty","['functional-analysis', 'measure-theory', 'spectral-theory', 'unbounded-operators', 'self-adjoint-operators']"
59,"Numerical range of the first derivative operator on $\{ u \in H^1(0,1): u(1)=0 \}$",Numerical range of the first derivative operator on,"\{ u \in H^1(0,1): u(1)=0 \}","I need to calculate the numerical range of the operator $T:D(T)\subseteq L^2(0,1) \to L^2(0,1)$ defined by $$D(T):=\{ u \in H^1(0,1): u(1)=0 \}, \ Tu:=u', \ u \in D(T),$$ where $H^1(0,1)$ is the Sobolev space of functions in $L^2(0,1)$ whose first weak derivative belongs to $L^2(0,1)$ . My attempt For each $u \in D(T)$ with $\|u \|=1$ we have that $$\langle Tu,u \rangle=\int_0^1u' \overline{u}dx=-|u(0)|^2 - \int_0^1u\overline{u'}dx.$$ If we add $\langle Tu,u \rangle$ on both sides of this equality, we get that $$2\langle Tu,u \rangle=-|u(0)|^2 + 2 i \int_0^1 \mbox{Im}(u' \overline{u})dx.$$ I think that the last one equality implies that the numerical range $W(T)$ of $T$ is given by $$W(T):=\{\langle Tu,u \rangle: u \in D(T), \|u \|=1 \}= \{z \in \mathbb{C} : \mbox{Re}(z) \leq 0 \}.$$ Is my argument right? Do you know another way to calculate $W(T)$ ? Thank you.","I need to calculate the numerical range of the operator defined by where is the Sobolev space of functions in whose first weak derivative belongs to . My attempt For each with we have that If we add on both sides of this equality, we get that I think that the last one equality implies that the numerical range of is given by Is my argument right? Do you know another way to calculate ? Thank you.","T:D(T)\subseteq L^2(0,1) \to L^2(0,1) D(T):=\{ u \in H^1(0,1): u(1)=0 \}, \ Tu:=u', \ u \in D(T), H^1(0,1) L^2(0,1) L^2(0,1) u \in D(T) \|u \|=1 \langle Tu,u \rangle=\int_0^1u' \overline{u}dx=-|u(0)|^2 - \int_0^1u\overline{u'}dx. \langle Tu,u \rangle 2\langle Tu,u \rangle=-|u(0)|^2 + 2 i \int_0^1 \mbox{Im}(u' \overline{u})dx. W(T) T W(T):=\{\langle Tu,u \rangle: u \in D(T), \|u \|=1 \}= \{z \in \mathbb{C} : \mbox{Re}(z) \leq 0 \}. W(T)","['functional-analysis', 'operator-theory', 'spectral-theory']"
60,Normed space but not complete [duplicate],Normed space but not complete [duplicate],,"This question already has an answer here : Prove that this space is not Banach (1 answer) Closed 4 years ago . As someone pointed out this could seems to be a duplicate of another question already posted. Is in fact the same question but I have provided also the effort of the solution and additionally I have shown something else, for instante the proof of the triangular inequality via de Cauchy-Schwarz inequality. Hence is definetly a duplicate but with a better description of the problem and the solution. Let $\Omega \subset \mathbb{R}^{n}$ open and bounded and let $\mathcal{A}=\{ v \in C^{2}(\overline{\Omega}) \, | \,v=0\, on \, \partial\Omega\}$ with the following scalar product \begin{equation} (u,v)_{\mathcal{A}}=\int_{\Omega} (\nabla u, \nabla v)_{\mathbb{R}^{n}}dx \end{equation} where $(\cdot, \cdot)_{\mathbb{R}^{n}}$ is the canonical scalar product on $\mathbb{R}^{n}$ . I have to prove that $(\mathcal{A}, \| \cdot \|_{\mathcal{A}})$ is a normed space but not a Banach space, where $\| u \|_{\mathcal{A}}=\sqrt{(u,u)_{\mathcal{A}}}$ . Proving that $(\mathcal{A}, \| \cdot \|_{\mathcal{A}})$ is a normed space is quite straightforward, the only point that make me think was the triangular inequality that I solved using the Chauchy-Schwarz inequality, namely \begin{equation} |(u,v)_{\mathcal{A}}|\leq \|u\|_{\mathcal{A}}\|v\|_{\mathcal{A}}, \end{equation} infact if I consider $\| u+v\|_{\mathcal{A}}^2$ I can proced in this way \begin{split} \| u+v\|_{\mathcal{A}}^2= & \| u\|_{\mathcal{A}}^2+2(u,v)_{\mathcal{A}} +\|v\|_{\mathcal{A}}^2 \\ \leq &\| u\|_{\mathcal{A}}^2+2\|u\|_{\mathcal{A}}\|v\|_{\mathcal{A}} +\|v\|_{\mathcal{A}}^2 \\ = & (\| u\|_{\mathcal{A}}+\|v\|_{\mathcal{A}})^2 \end{split} Hence we have proved $\| u+v\|_{\mathcal{A}}\leq \| u\|_{\mathcal{A}}+\|v\|_{\mathcal{A}} $ . But I was stucked on the noncompleteness, I tried to argue as following but I am not sure if it is right. Let $n=1$ and $\Omega=(-1,1)$ and suppose to take the sequence $(u_n)_n\subset \mathcal{A}$ , given by \begin{equation} u_n=\sqrt{1+\frac{1}{n^2}}-\sqrt{x^2+\frac{1}{n^2}}, \end{equation} I can notice that $u_n \rightarrow u$ when $n\rightarrow \infty$ using for instance the uniform norm, where $u=1-|x|$ that does not belong to $\mathcal{A}$ . Hence I found a sequence that converges to an element that is not in the space and consequentely $\mathcal{A}$ is not complete.","This question already has an answer here : Prove that this space is not Banach (1 answer) Closed 4 years ago . As someone pointed out this could seems to be a duplicate of another question already posted. Is in fact the same question but I have provided also the effort of the solution and additionally I have shown something else, for instante the proof of the triangular inequality via de Cauchy-Schwarz inequality. Hence is definetly a duplicate but with a better description of the problem and the solution. Let open and bounded and let with the following scalar product where is the canonical scalar product on . I have to prove that is a normed space but not a Banach space, where . Proving that is a normed space is quite straightforward, the only point that make me think was the triangular inequality that I solved using the Chauchy-Schwarz inequality, namely infact if I consider I can proced in this way Hence we have proved . But I was stucked on the noncompleteness, I tried to argue as following but I am not sure if it is right. Let and and suppose to take the sequence , given by I can notice that when using for instance the uniform norm, where that does not belong to . Hence I found a sequence that converges to an element that is not in the space and consequentely is not complete.","\Omega \subset \mathbb{R}^{n} \mathcal{A}=\{ v \in C^{2}(\overline{\Omega}) \, | \,v=0\, on \, \partial\Omega\} \begin{equation}
(u,v)_{\mathcal{A}}=\int_{\Omega} (\nabla u, \nabla v)_{\mathbb{R}^{n}}dx
\end{equation} (\cdot, \cdot)_{\mathbb{R}^{n}} \mathbb{R}^{n} (\mathcal{A}, \| \cdot \|_{\mathcal{A}}) \| u \|_{\mathcal{A}}=\sqrt{(u,u)_{\mathcal{A}}} (\mathcal{A}, \| \cdot \|_{\mathcal{A}}) \begin{equation}
|(u,v)_{\mathcal{A}}|\leq \|u\|_{\mathcal{A}}\|v\|_{\mathcal{A}},
\end{equation} \| u+v\|_{\mathcal{A}}^2 \begin{split}
\| u+v\|_{\mathcal{A}}^2= & \| u\|_{\mathcal{A}}^2+2(u,v)_{\mathcal{A}} +\|v\|_{\mathcal{A}}^2 \\
\leq &\| u\|_{\mathcal{A}}^2+2\|u\|_{\mathcal{A}}\|v\|_{\mathcal{A}} +\|v\|_{\mathcal{A}}^2 \\
= & (\| u\|_{\mathcal{A}}+\|v\|_{\mathcal{A}})^2
\end{split} \| u+v\|_{\mathcal{A}}\leq \| u\|_{\mathcal{A}}+\|v\|_{\mathcal{A}}  n=1 \Omega=(-1,1) (u_n)_n\subset \mathcal{A} \begin{equation}
u_n=\sqrt{1+\frac{1}{n^2}}-\sqrt{x^2+\frac{1}{n^2}},
\end{equation} u_n \rightarrow u n\rightarrow \infty u=1-|x| \mathcal{A} \mathcal{A}","['functional-analysis', 'analysis', 'normed-spaces']"
61,Convergence speed of discrete approximation,Convergence speed of discrete approximation,,"Here I asked the question about approximating the function $g(x) := \mathbb{E}(f(x,Y))$ , where $x \in R$ and $Y$ is a random variable. If you follow the link you will see that $g(x)$ can be approximated by finite sum $\frac{1}{n}\sum_{i=1}^nf(x,\mu_i)$ with any precision we desire. My question: whats is the speed of the convergence? Can we deduce how large $n$ should be to make error $\leq \epsilon$ Feel free to impose any reasonable conditions on $f$ and distribution of $Y$","Here I asked the question about approximating the function , where and is a random variable. If you follow the link you will see that can be approximated by finite sum with any precision we desire. My question: whats is the speed of the convergence? Can we deduce how large should be to make error Feel free to impose any reasonable conditions on and distribution of","g(x) := \mathbb{E}(f(x,Y)) x \in R Y g(x) \frac{1}{n}\sum_{i=1}^nf(x,\mu_i) n \leq \epsilon f Y","['functional-analysis', 'approximation-theory', 'linear-regression']"
62,"Compute the derivative $\partial_t\int_{\{u(t,\cdot) >0\} } 1\, dx$ in the sense of distributions",Compute the derivative  in the sense of distributions,"\partial_t\int_{\{u(t,\cdot) >0\} } 1\, dx","Let $u:\Omega\subset \mathbb R^N \to \mathbb R$ be bounded function that solves (in the sense of distributions) an evolution PDE $\partial_t u(t,x)= L(u(t,\cdot))(x)$ , where $L$ is some elliptic operator in divergence form. In a post on MathOverflow , a calculation was made regarding the time derivative $$\partial_t\int_{\{u(t,\cdot) >0\} } 1\,  dx$$ in the following way: $$\partial_t\int_{\{u(t,\cdot) >0\} } 1\,  dx= ∫_\Omega \delta[u(t,x)]\partial_t u(t,x)\,dx=\int_{S(t)}\frac{\partial_t u(t,x)}{|\nabla_x u(t,x)|}d\sigma(x).$$ How can one rewrite this calculation emphasizing the fact that we are doing a derivative in the sense of distributions, i.e. $$\langle \partial_t T, \phi\rangle = -\langle T, \partial_t\phi \rangle $$ where $T$ is the distribution associated with $\int_{\{u(t,\cdot) >0\} } 1\,  dx$ ? And how can we use the fact that $u$ solves the PDE above in the final formula?","Let be bounded function that solves (in the sense of distributions) an evolution PDE , where is some elliptic operator in divergence form. In a post on MathOverflow , a calculation was made regarding the time derivative in the following way: How can one rewrite this calculation emphasizing the fact that we are doing a derivative in the sense of distributions, i.e. where is the distribution associated with ? And how can we use the fact that solves the PDE above in the final formula?","u:\Omega\subset \mathbb R^N \to \mathbb R \partial_t u(t,x)= L(u(t,\cdot))(x) L \partial_t\int_{\{u(t,\cdot) >0\} } 1\,  dx \partial_t\int_{\{u(t,\cdot) >0\} } 1\,  dx= ∫_\Omega \delta[u(t,x)]\partial_t u(t,x)\,dx=\int_{S(t)}\frac{\partial_t u(t,x)}{|\nabla_x u(t,x)|}d\sigma(x). \langle \partial_t T, \phi\rangle = -\langle T, \partial_t\phi \rangle  T \int_{\{u(t,\cdot) >0\} } 1\,  dx u","['real-analysis', 'functional-analysis', 'multivariable-calculus', 'distribution-theory']"
63,Characterization of Reflexive Banach Space.,Characterization of Reflexive Banach Space.,,"Prove that a real Banach Space $X$ is reflexive if and only if each pair of disjoint closed, convex subsets of $X$ , one of which is bounded, can be strictly separated by a hyperplane. The theorem is stated and proved in the book Geometric Functional Analysis and its Applications by Richard Holmes on page-161, which goes like this We shall show that the disjoint closed convex sets $H$ and $U(M)$ cannot be strictly separated. Suppose otherwise; then there would exist $\psi\in X^*$ and a positive number $\gamma$ such that $\psi(x)<\gamma<\psi(y)$ for all $x\in U(M)$ and $y\in H$ . Let us assume that $\phi$ has been extended to all of $X$ via the Hahn-Banach Theorem, and let us call the extension $\phi$ also. Choose any point $\overline{x}\in X$ such that $\phi(\overline{x})=\|\phi\|$ and any $z\in X$ for which $\phi(z)=0$ . Then for all $\lambda\in \mathbb{R}, \psi(\overline{x}+\lambda z)>\gamma$ . Hence, $\psi(z)=0$ and so $\ker(\phi)\subset \ker(\psi)$ . This means that the set $\{\phi,\psi\}$ is linearly dependent. We can therefore choose a constant $\alpha$ so that $\psi=\left(\frac{\alpha \gamma}{\|\phi\|}\right) \phi$ . Now if $y\in H$ then $\gamma<\psi(y)=\alpha \gamma$ , so that $1<\alpha$ . On the other hand, if $x\in U(M)$ then $\psi(x)<\gamma$ , and so $\phi(x)=\left(\frac{\|\phi\|}{\alpha\gamma}\right)\psi(x)<\frac{\|\phi\|}{\alpha}$ ; since $\alpha>1$ , this contradicts the definition $\|\phi\|=\sup\{\phi(x):x\in U(M)\}$ .    I am trying to decipher this proof. It uses the contrapositive logic and is based on the James Theorem a Banach space $X$ is reflexive if and only if every continuous linear functional on $X$ attains its supremum on the closed unit ball in $X$ . And, here is what I could understand in this proof Let $X$ be a Banach Space and $M$ be a subspace. Let $\phi\in M^*$ be a continuous linear functional.  Let $H=\{x\in M:\phi(x)=\|\phi\|\}$ be a hyperplane in $M$ and $U(M)=\{x\in M:\|x\|=1\}$ be a unit ball in $M$ . Suppose that there exists a continuous linear functional $\psi\in X^*$ and a positive number $\gamma$ such that $\psi(x)<\gamma<\psi(y)$ for all $x\in U(M)$ and $y\in H.$ Choose a point $\overline{x}\in X$ such that $\phi(\overline{x})=\|\phi\|$ , i.e. $\overline{x}\in H$ and any $z\in X$ such that $\phi(z)=0$ .    Then, for a real number $\lambda$ , $\phi(\overline{x}+\lambda z)=\phi(\overline{x})+\lambda \phi(z)$ .   Since $\overline{x}\in H$ and $\phi(z)=0$ , $\phi(\overline{x}+\lambda z)=\|\phi\|+\lambda. 0$ . $\implies \phi(\overline{x}+\lambda z)=\|\phi\|$ . $\implies \overline{x}+\lambda z\in H$ . $\implies \psi(\overline{x}+\lambda z)>\gamma$ .   What I couldn't understand is How the idea of extending $\phi$ is applied in all of this? How do you infer $\psi(z)=0$ ?","Prove that a real Banach Space is reflexive if and only if each pair of disjoint closed, convex subsets of , one of which is bounded, can be strictly separated by a hyperplane. The theorem is stated and proved in the book Geometric Functional Analysis and its Applications by Richard Holmes on page-161, which goes like this We shall show that the disjoint closed convex sets and cannot be strictly separated. Suppose otherwise; then there would exist and a positive number such that for all and . Let us assume that has been extended to all of via the Hahn-Banach Theorem, and let us call the extension also. Choose any point such that and any for which . Then for all . Hence, and so . This means that the set is linearly dependent. We can therefore choose a constant so that . Now if then , so that . On the other hand, if then , and so ; since , this contradicts the definition .    I am trying to decipher this proof. It uses the contrapositive logic and is based on the James Theorem a Banach space is reflexive if and only if every continuous linear functional on attains its supremum on the closed unit ball in . And, here is what I could understand in this proof Let be a Banach Space and be a subspace. Let be a continuous linear functional.  Let be a hyperplane in and be a unit ball in . Suppose that there exists a continuous linear functional and a positive number such that for all and Choose a point such that , i.e. and any such that .    Then, for a real number , .   Since and , . . . .   What I couldn't understand is How the idea of extending is applied in all of this? How do you infer ?","X X H U(M) \psi\in X^* \gamma \psi(x)<\gamma<\psi(y) x\in U(M) y\in H \phi X \phi \overline{x}\in X \phi(\overline{x})=\|\phi\| z\in X \phi(z)=0 \lambda\in \mathbb{R}, \psi(\overline{x}+\lambda z)>\gamma \psi(z)=0 \ker(\phi)\subset \ker(\psi) \{\phi,\psi\} \alpha \psi=\left(\frac{\alpha \gamma}{\|\phi\|}\right) \phi y\in H \gamma<\psi(y)=\alpha \gamma 1<\alpha x\in U(M) \psi(x)<\gamma \phi(x)=\left(\frac{\|\phi\|}{\alpha\gamma}\right)\psi(x)<\frac{\|\phi\|}{\alpha} \alpha>1 \|\phi\|=\sup\{\phi(x):x\in U(M)\} X X X X M \phi\in M^* H=\{x\in M:\phi(x)=\|\phi\|\} M U(M)=\{x\in M:\|x\|=1\} M \psi\in X^* \gamma \psi(x)<\gamma<\psi(y) x\in U(M) y\in H. \overline{x}\in X \phi(\overline{x})=\|\phi\| \overline{x}\in H z\in X \phi(z)=0 \lambda \phi(\overline{x}+\lambda z)=\phi(\overline{x})+\lambda \phi(z) \overline{x}\in H \phi(z)=0 \phi(\overline{x}+\lambda z)=\|\phi\|+\lambda. 0 \implies \phi(\overline{x}+\lambda z)=\|\phi\| \implies \overline{x}+\lambda z\in H \implies \psi(\overline{x}+\lambda z)>\gamma \phi \psi(z)=0","['functional-analysis', 'banach-spaces']"
64,"Density of a class of function in $L^2(\mathbb{R}, e^x\,dx)$",Density of a class of function in,"L^2(\mathbb{R}, e^x\,dx)","Consider the class of function defined by $$\mathcal{G}=\operatorname{Span}\left\{e^{-\frac{(x+a)^2}{2}}-e^{-x}e^{-\frac{(x+a)^2}{2}}\mid a\in\mathbb{R}\right\}.$$ Is $\mathcal{G}$ dense in $L^2(\mathbb{R}, e^x\,dx)$ ? Note: If in the expression of $\mathcal{G}$ , we had $e^{-(x+a)}$ instead of $e^{-x}$ , the result would be true (by Wiener's Tauberian theorem). Edit: Here is a motivation why I need a result of this sort. I am working with a differential operator $A$ on $L^2(\mathbb{R}, e^x\,dx)$ such that $$Au(x)=e^{-x}u''(x)$$ for all $u\in C^2_b(\mathbb{R})\cap L^2(\mathbb{R},e^x\,dx)$ . It can be shown that $A$ is the generator of a Markov diffusion semigroup. I need to show that for any $\alpha>0$ , $R^{-1}_\alpha(\mathcal{G})$ is dense in $L^2(\mathbb{R}, e^x\,dx)$ , where $R_\alpha=(\alpha-A)^{-1}$ is the resolvent operator.","Consider the class of function defined by Is dense in ? Note: If in the expression of , we had instead of , the result would be true (by Wiener's Tauberian theorem). Edit: Here is a motivation why I need a result of this sort. I am working with a differential operator on such that for all . It can be shown that is the generator of a Markov diffusion semigroup. I need to show that for any , is dense in , where is the resolvent operator.","\mathcal{G}=\operatorname{Span}\left\{e^{-\frac{(x+a)^2}{2}}-e^{-x}e^{-\frac{(x+a)^2}{2}}\mid a\in\mathbb{R}\right\}. \mathcal{G} L^2(\mathbb{R}, e^x\,dx) \mathcal{G} e^{-(x+a)} e^{-x} A L^2(\mathbb{R}, e^x\,dx) Au(x)=e^{-x}u''(x) u\in C^2_b(\mathbb{R})\cap L^2(\mathbb{R},e^x\,dx) A \alpha>0 R^{-1}_\alpha(\mathcal{G}) L^2(\mathbb{R}, e^x\,dx) R_\alpha=(\alpha-A)^{-1}","['functional-analysis', 'fourier-analysis', 'fourier-transform']"
65,Determining if $f\in L^{p}(\mathbb R)$ from a bound on the measure of the level sets $\{|f|>\lambda\}$ for all $\lambda>0.$,Determining if  from a bound on the measure of the level sets  for all,f\in L^{p}(\mathbb R) \{|f|>\lambda\} \lambda>0.,"$\textbf{The Problem:}$ Let $f$ be a measurable function on $\mathbb R$ with respect to the Lebesgue measure $m$ . $\textbf{a)}$ Suppose that $$m(\{\vert f\vert>\lambda\})\leq(1+\lambda)^{-1}$$ holds for every $\lambda>0.$ For which $p\in\{1,\infty\}$ do we have $f\in L^{p}(\mathbb R)$ ? $\textbf{b)}$ Now suppose that $$m(\{\vert f\vert>\lambda\})\leq(1+\lambda)^{-10}.$$ For which $p\in[1,\infty]$ do we have $f\in L^{p}(\mathbb R)?$ $\textbf{My Thoughts:}$ $\textbf{a)}$ It doesn't necessarily follow that $f\in L^1(\mathbb R)$ or $f\in L^{\infty}(\mathbb R)$ since a counterexample in both cases is given by $f(x)=(2x)^{-1}\cdot\mathbf1_{x>0}.$ $\textbf{b)}$ The following formula for the $L^p-$ norm will be used $$\|f\|_{p}^{p}=\int_{0}^{\infty}p\lambda^{p-1}m(\{|f|>\lambda\})d\lambda.$$ With that in mind we have the following by hypothesis $$\|f\|_{p}^{p}\leq\int_{0}^{\infty}\frac{p\lambda^{p-1}}{(1+\lambda)^{10}}d\lambda,$$ so if $p-1\leq8$ we are golden, and the integral converges, and so $f\in L^{p}(\mathbb R)$ for all $1\leq p\leq9.$ If $p>9$ , then the bound on the $L^p-$ norm seems useless. So I think the claim may not follow, so I tried building up counterexamples from the hypothesis on the level sets. I came up with the function $f(x)=(x^{-1/10}-1)\cdot\mathbf1_{x>1}(x)$ by reverse engineering the bound on the level sets. This function will not be in $L^{p}(\mathbb R)$ for all $p>9.$ Is my work above accurate? Any comments are welcomed, be it about style, lack of details, and of course, correctness of the ideas. Thank you for your time.","Let be a measurable function on with respect to the Lebesgue measure . Suppose that holds for every For which do we have ? Now suppose that For which do we have It doesn't necessarily follow that or since a counterexample in both cases is given by The following formula for the norm will be used With that in mind we have the following by hypothesis so if we are golden, and the integral converges, and so for all If , then the bound on the norm seems useless. So I think the claim may not follow, so I tried building up counterexamples from the hypothesis on the level sets. I came up with the function by reverse engineering the bound on the level sets. This function will not be in for all Is my work above accurate? Any comments are welcomed, be it about style, lack of details, and of course, correctness of the ideas. Thank you for your time.","\textbf{The Problem:} f \mathbb R m \textbf{a)} m(\{\vert f\vert>\lambda\})\leq(1+\lambda)^{-1} \lambda>0. p\in\{1,\infty\} f\in L^{p}(\mathbb R) \textbf{b)} m(\{\vert f\vert>\lambda\})\leq(1+\lambda)^{-10}. p\in[1,\infty] f\in L^{p}(\mathbb R)? \textbf{My Thoughts:} \textbf{a)} f\in L^1(\mathbb R) f\in L^{\infty}(\mathbb R) f(x)=(2x)^{-1}\cdot\mathbf1_{x>0}. \textbf{b)} L^p- \|f\|_{p}^{p}=\int_{0}^{\infty}p\lambda^{p-1}m(\{|f|>\lambda\})d\lambda. \|f\|_{p}^{p}\leq\int_{0}^{\infty}\frac{p\lambda^{p-1}}{(1+\lambda)^{10}}d\lambda, p-1\leq8 f\in L^{p}(\mathbb R) 1\leq p\leq9. p>9 L^p- f(x)=(x^{-1/10}-1)\cdot\mathbf1_{x>1}(x) L^{p}(\mathbb R) p>9.","['real-analysis', 'functional-analysis', 'lebesgue-integral', 'lp-spaces']"
66,convergence RATE of the square root of a self-adjoint operator.,convergence RATE of the square root of a self-adjoint operator.,,"I am assuming $T$ is a compact operator and $\{T_j\}$ is a sequence of compact operators such that $\|T-T_j\| < \epsilon_j$ where $\epsilon_j$ is a quantity that goes to zero as $j \to \infty$ . It is easy to show that this implies $\|T^*T-T^*_jT_j\| \leq C\epsilon_j$ for some constant $C$ . I am wondering if it is also true that $\|\sqrt{T^*T}-\sqrt{T^*_jT_j}\| \leq D\epsilon_j$ for some constant $D$ ? I have seen several proofs that $\sqrt{T^*_jT_j}$ converges to $T^*T$ in the operator norm, but I am wondering about the rate of this convergence. I am pretty sure that just assuming $\|T^*T-T^*_jT_j\| \leq C\epsilon_j$ does not imply that $\|\sqrt{T^*T}-\sqrt{T^*_jT_j}\| \leq D\epsilon_j$ . But I was wondering if $\|T-T_j\| < \epsilon_j$ did imply the result. Any help or direction to a reference would be greatly appreciated. If the result is false, a counterexample would also be great.","I am assuming is a compact operator and is a sequence of compact operators such that where is a quantity that goes to zero as . It is easy to show that this implies for some constant . I am wondering if it is also true that for some constant ? I have seen several proofs that converges to in the operator norm, but I am wondering about the rate of this convergence. I am pretty sure that just assuming does not imply that . But I was wondering if did imply the result. Any help or direction to a reference would be greatly appreciated. If the result is false, a counterexample would also be great.",T \{T_j\} \|T-T_j\| < \epsilon_j \epsilon_j j \to \infty \|T^*T-T^*_jT_j\| \leq C\epsilon_j C \|\sqrt{T^*T}-\sqrt{T^*_jT_j}\| \leq D\epsilon_j D \sqrt{T^*_jT_j} T^*T \|T^*T-T^*_jT_j\| \leq C\epsilon_j \|\sqrt{T^*T}-\sqrt{T^*_jT_j}\| \leq D\epsilon_j \|T-T_j\| < \epsilon_j,"['functional-analysis', 'operator-theory', 'compact-operators']"
67,Using abstract Hilbert spaces to solve differential equations,Using abstract Hilbert spaces to solve differential equations,,"There are techniques for solving PDE's, such as Fock-Schwinger method in physics, which involve translating the problem from the language of distributions to the language of the abstract Hilbert spaces. For example, the Green's function equation is: $$L(i\partial_x,x)G(x,x')= \delta(x-x')$$ A typical approach to deal with such equations is to note that we can choose vectors in Hilbert space as: $$\langle x|x'\rangle  = \delta(x-x')$$ and, having this in mind, move $x$ and $i\partial_x$ ""into"" Hilbert space according to the rule: $$i\partial_x \langle x|x'\rangle  = \langle x|P|x'\rangle  [1]$$ $$x \langle x|x'\rangle  = \langle x|Q|x'\rangle  [2]$$ where $P$ is chosen to commute with $Q$ as $[P,Q]=i * I$ . Therefore, inside bra-ket a linear operator $L$ is represented as: $$L(i\partial_x,x)\langle x|x'\rangle  = \langle x|L(P,Q)|x'\rangle $$ and analytic functions of $L$ , such as $exp(itL)$ , will move into bra-ket with some additional terms due to commutation. My question is the following. On what mathematical grounds can we use rules [1] and [2]? I see that they are somehow related to the Stone - von Neumann theorem, but I don't see the exact justification. Moreover, $\langle x|x'\rangle $ is being normalized in terms of distributions, not scalars, that is kind of a generalization to the usual notion of vector spaces with dot product, which also needs justification . Any textbook on Hilbert spaces or Quantum Mechanics that clarifies these rules? (I mean, in a more or less strict way, not ""on the physical grounds"") Is there any other book which highly exploits this Hilbert space technique to solve different kind of problems? I proved similar relations in terms of distributions: $$\delta(x-x') = \int dX \space \delta(X-x) \space \delta(X-x')$$ $$x \space \delta(x-x') = \int dX \space \delta(X-x) \space x \space \delta(X-x')$$ and by definition of the derivative in the space of distributions $$\partial_x  \delta(X-x) \to -\partial_X  \delta(X-x) \to  +\delta(X-x)\partial_X$$ I finally get: $$i\partial_x \space \delta(x-x') = \int dX \space \delta(X-x) \space i\partial_X \space \delta(X-x')$$ which looks like a dot product in Hilbert space if with make a map: $$\delta(X-x) \to |x\rangle $$ $$\int dX \space \delta(X-x) \space \delta(X-x') \space \to \space \langle x|x'\rangle $$ And also, I have another question. Is there a proof that also justifies this kind of a map between distribution-like objects to Hilbert space vectors?","There are techniques for solving PDE's, such as Fock-Schwinger method in physics, which involve translating the problem from the language of distributions to the language of the abstract Hilbert spaces. For example, the Green's function equation is: A typical approach to deal with such equations is to note that we can choose vectors in Hilbert space as: and, having this in mind, move and ""into"" Hilbert space according to the rule: where is chosen to commute with as . Therefore, inside bra-ket a linear operator is represented as: and analytic functions of , such as , will move into bra-ket with some additional terms due to commutation. My question is the following. On what mathematical grounds can we use rules [1] and [2]? I see that they are somehow related to the Stone - von Neumann theorem, but I don't see the exact justification. Moreover, is being normalized in terms of distributions, not scalars, that is kind of a generalization to the usual notion of vector spaces with dot product, which also needs justification . Any textbook on Hilbert spaces or Quantum Mechanics that clarifies these rules? (I mean, in a more or less strict way, not ""on the physical grounds"") Is there any other book which highly exploits this Hilbert space technique to solve different kind of problems? I proved similar relations in terms of distributions: and by definition of the derivative in the space of distributions I finally get: which looks like a dot product in Hilbert space if with make a map: And also, I have another question. Is there a proof that also justifies this kind of a map between distribution-like objects to Hilbert space vectors?","L(i\partial_x,x)G(x,x')= \delta(x-x') \langle x|x'\rangle  = \delta(x-x') x i\partial_x i\partial_x \langle x|x'\rangle  = \langle x|P|x'\rangle  [1] x \langle x|x'\rangle  = \langle x|Q|x'\rangle  [2] P Q [P,Q]=i * I L L(i\partial_x,x)\langle x|x'\rangle  = \langle x|L(P,Q)|x'\rangle  L exp(itL) \langle x|x'\rangle  \delta(x-x') = \int dX \space \delta(X-x) \space \delta(X-x') x \space \delta(x-x') = \int dX \space \delta(X-x) \space x \space \delta(X-x') \partial_x  \delta(X-x) \to -\partial_X  \delta(X-x) \to  +\delta(X-x)\partial_X i\partial_x \space \delta(x-x') = \int dX \space \delta(X-x) \space i\partial_X \space \delta(X-x') \delta(X-x) \to |x\rangle  \int dX \space \delta(X-x) \space \delta(X-x') \space \to \space \langle x|x'\rangle ","['functional-analysis', 'operator-theory', 'hilbert-spaces', 'mathematical-physics']"
68,"If $f$ is integrable, then $\| f\|$ is also integrable.","If  is integrable, then  is also integrable.",f \| f\|,"As usual, a partition of a compact interval $[a, b]$ is, by definition, an strictly increasing family $\Pi = (t_k)_{k = 0}^m$ ( $m \geq 0$ ) of points in the interval such that $t_0 = a$ and $t_m = b;$ $\tilde\Pi$ is refining $\Pi$ if $\Pi$ is subfamily of $\tilde\Pi$ and $\tilde\Pi$ is also a partition. A valid selection of tags $\tau$ for the partition $\Pi = (t_k)_{k = 0}^m$ is, by definition, a family $(\tau_k)_{k = 1}^m$ for which $\tau_k \in [t_{k - 1}, t_k],$ as $k$ runs from $1$ until $m.$ Definition of Banach space valued Riemann integral. A function $f:[a, b] \to \mathrm{X},$ $\mathrm{X}$ being a Banach space, is Riemann-integrable if there exists a vector $I \in \mathrm{X}$ obeying the following law: for all $\varepsilon > 0,$ there exists a partition $\Pi_\varepsilon$ such that for whatever the partition $\Pi$ of $[a, b]$ refining $\Pi_\varepsilon$ and whatever valid selection of tags $\tau$ for the partition $\Pi$ may be, the Riemman sum of $f$ associated with the partition $\Pi$ under the valid selection of tags $\tau,$ $S(f, \Pi, \tau) = \sum\limits_{k = 1}^m f(\tau_k)(t_k - t_{k - 1}),$ satisfies $\|I - S(f, \Pi, \tau)\| < \varepsilon.$ Problem. How to show that $\|f\|$ will be integrable whenever $f$ is? There is the following ""fundamental criterion"" for Riemann-integration that may be useful but I just couldn't find a way to apply it. Fundamental criterion for existence. For the function $f:[a, b] \to \mathrm{X}$ to be Riemann-integrable it is necessary and sufficient that the following conditions should hold, for every $\varepsilon > 0,$ there exists a partition $\Pi_\varepsilon,$ such that $\left\| S \left(f, \Pi^{(1)}, \tau^{(1)} \right) - S \left(f, \Pi^{(2)}, \tau^{(2)} \right) \right\| < \varepsilon$ for all refinements $\Pi^{(1)}$ and $\Pi^{(2)}$ of $\Pi_\varepsilon$ and all corresponding valid selections of tags $\tau^{(1)}$ and $\tau^{(2)}.$ Sketch of proof of the fundamental criterion. Necessity is obvious. For sufficiency, consider, for each $n,$ a partition $\Pi^{(n)},$ refining all previous $\Pi^{(k)},$ and consider valid tags $\tau^{(n)}$ such that $$\left\| S \left(f, \Pi^{(n)}, \tau^{(n)} \right) - S \left(f, \Pi^{(k)}, \tau^{(k)} \right) \right\| < \dfrac{1}{k}.$$ Set then $I_n = S \left(f, \Pi^{(n)}, \tau^{(n)} \right)$ and notice this defines a fundamental sequence is $\mathrm{X}$ and therefore, converging to some vector $I \in \mathrm{X},$ which can be shown, using triangle inequality, to be the Riemann integral of $f.$ $\square$","As usual, a partition of a compact interval is, by definition, an strictly increasing family ( ) of points in the interval such that and is refining if is subfamily of and is also a partition. A valid selection of tags for the partition is, by definition, a family for which as runs from until Definition of Banach space valued Riemann integral. A function being a Banach space, is Riemann-integrable if there exists a vector obeying the following law: for all there exists a partition such that for whatever the partition of refining and whatever valid selection of tags for the partition may be, the Riemman sum of associated with the partition under the valid selection of tags satisfies Problem. How to show that will be integrable whenever is? There is the following ""fundamental criterion"" for Riemann-integration that may be useful but I just couldn't find a way to apply it. Fundamental criterion for existence. For the function to be Riemann-integrable it is necessary and sufficient that the following conditions should hold, for every there exists a partition such that for all refinements and of and all corresponding valid selections of tags and Sketch of proof of the fundamental criterion. Necessity is obvious. For sufficiency, consider, for each a partition refining all previous and consider valid tags such that Set then and notice this defines a fundamental sequence is and therefore, converging to some vector which can be shown, using triangle inequality, to be the Riemann integral of","[a, b] \Pi = (t_k)_{k = 0}^m m \geq 0 t_0 = a t_m = b; \tilde\Pi \Pi \Pi \tilde\Pi \tilde\Pi \tau \Pi = (t_k)_{k = 0}^m (\tau_k)_{k = 1}^m \tau_k \in [t_{k - 1}, t_k], k 1 m. f:[a, b] \to \mathrm{X}, \mathrm{X} I \in \mathrm{X} \varepsilon > 0, \Pi_\varepsilon \Pi [a, b] \Pi_\varepsilon \tau \Pi f \Pi \tau, S(f, \Pi, \tau) = \sum\limits_{k = 1}^m f(\tau_k)(t_k - t_{k - 1}), \|I - S(f, \Pi, \tau)\| < \varepsilon. \|f\| f f:[a, b] \to \mathrm{X} \varepsilon > 0, \Pi_\varepsilon, \left\| S \left(f, \Pi^{(1)}, \tau^{(1)} \right) - S \left(f, \Pi^{(2)}, \tau^{(2)} \right) \right\| < \varepsilon \Pi^{(1)} \Pi^{(2)} \Pi_\varepsilon \tau^{(1)} \tau^{(2)}. n, \Pi^{(n)}, \Pi^{(k)}, \tau^{(n)} \left\| S \left(f, \Pi^{(n)}, \tau^{(n)} \right) - S \left(f, \Pi^{(k)}, \tau^{(k)} \right) \right\| < \dfrac{1}{k}. I_n = S \left(f, \Pi^{(n)}, \tau^{(n)} \right) \mathrm{X} I \in \mathrm{X}, f. \square","['real-analysis', 'functional-analysis', 'banach-spaces', 'normed-spaces', 'riemann-integration']"
69,Characterization of extreme points of the ball of $B(H)$,Characterization of extreme points of the ball of,B(H),"The following is an exercise in Chapter 5, Section 7 of Conway's Functional Analysis text: If $\mathcal{H}$ is a Hilbert space, show that $T$ is an extreme point of $\text{ball }\mathcal{B}(\mathcal{H})$ if and only if either $T$ or $T^*$ is an isometry. The ( $\Leftarrow$ ) implication is due to both the fact that $\ell^2$ balls are strictly convex and the nature of the adjoint - other stackexchange posts talk about this. I have not been able to find an answer to ( $\Rightarrow$ ) that a beginning student in functional analysis could also find. Here is what I have so far: If $\ker T$ and $\ker T^*$ both happen to be non-trivial - say $0\neq x\in\ker T$ and $0\neq y\in \ker T^*$ are of norm one - then we can perturb $T$ by a rank-one operator $S$ sending $x$ to $y$ . Then if we decompose an arbitrary $z=\lambda x+\sqrt{1-\lambda^2}z_0$ for $z_0$ a norm-one element orthogonal to $x$ and $0\le \lambda\le 1$ , we get \begin{align*}\langle(T\pm S)z,(T\pm S)z\rangle&=\langle S\lambda x,S\lambda x\rangle+\langle T\sqrt{1-\lambda^2}z_0,T\sqrt{1-\lambda^2}z_0\rangle\pm2\langle x,T^*Sx\rangle\\ &\le\lambda^2+1-\lambda^2\pm2\langle x,T^*y\rangle=1.\end{align*} Hence $T=\frac{1}{2}(T+S)+\frac{1}{2}(T-S)$ , so $T$ is not an extreme point. This argument is restricted to $\mathbb{R}$ -Hilbert spaces, and even this does not seem to generalize very nicely to other such spaces: for instance, in $M_2(\mathbb{R})$ if we define $T:=\begin{bmatrix} 1 & 1 \\ 0 & 1\end{bmatrix}$ , then $\tilde{T}:=\frac{T}{\lVert T\rVert}$ is not an isometry and neither is its adjoint, $\tilde{T}\begin{bmatrix} 1 \\ 0\end{bmatrix}$ and $\tilde{T}^*\begin{bmatrix} 0 \\ 1\end{bmatrix}$ are both of norm less than one, but any perturbation of the operator by an operator of the form $\begin{bmatrix} 0 & 0 \\ \xi & 0\end{bmatrix}$ for $\xi\ge 0$ raises the operator norm of the matrix. Murphy mentions this fact on page 137 and references Takesaki, Theorem I.10.2, for the proof. The theorem in Takesaki is done for a general C*-algebra and the proof utilizes the theory of operator algebras. The proof is a bit lengthy and (in my opinion) too clever for a student of Conway. Kadison-Ringrose also has a similar statement to that of Takesaki near the beginning of their second volume (Theorem 7.3.1), and this proof uses partial isometries and the Gelfand transform. I am not contesting the more general proofs given above in (2) but am merely asking whether there is a simpler way to answer this problem in the specific case of our C*-algebra being $\mathcal{B}(\mathcal{H})$ - perhaps one accessible to a reader of Conway.","The following is an exercise in Chapter 5, Section 7 of Conway's Functional Analysis text: If is a Hilbert space, show that is an extreme point of if and only if either or is an isometry. The ( ) implication is due to both the fact that balls are strictly convex and the nature of the adjoint - other stackexchange posts talk about this. I have not been able to find an answer to ( ) that a beginning student in functional analysis could also find. Here is what I have so far: If and both happen to be non-trivial - say and are of norm one - then we can perturb by a rank-one operator sending to . Then if we decompose an arbitrary for a norm-one element orthogonal to and , we get Hence , so is not an extreme point. This argument is restricted to -Hilbert spaces, and even this does not seem to generalize very nicely to other such spaces: for instance, in if we define , then is not an isometry and neither is its adjoint, and are both of norm less than one, but any perturbation of the operator by an operator of the form for raises the operator norm of the matrix. Murphy mentions this fact on page 137 and references Takesaki, Theorem I.10.2, for the proof. The theorem in Takesaki is done for a general C*-algebra and the proof utilizes the theory of operator algebras. The proof is a bit lengthy and (in my opinion) too clever for a student of Conway. Kadison-Ringrose also has a similar statement to that of Takesaki near the beginning of their second volume (Theorem 7.3.1), and this proof uses partial isometries and the Gelfand transform. I am not contesting the more general proofs given above in (2) but am merely asking whether there is a simpler way to answer this problem in the specific case of our C*-algebra being - perhaps one accessible to a reader of Conway.","\mathcal{H} T \text{ball }\mathcal{B}(\mathcal{H}) T T^* \Leftarrow \ell^2 \Rightarrow \ker T \ker T^* 0\neq x\in\ker T 0\neq y\in \ker T^* T S x y z=\lambda x+\sqrt{1-\lambda^2}z_0 z_0 x 0\le \lambda\le 1 \begin{align*}\langle(T\pm S)z,(T\pm S)z\rangle&=\langle S\lambda x,S\lambda x\rangle+\langle T\sqrt{1-\lambda^2}z_0,T\sqrt{1-\lambda^2}z_0\rangle\pm2\langle x,T^*Sx\rangle\\
&\le\lambda^2+1-\lambda^2\pm2\langle x,T^*y\rangle=1.\end{align*} T=\frac{1}{2}(T+S)+\frac{1}{2}(T-S) T \mathbb{R} M_2(\mathbb{R}) T:=\begin{bmatrix} 1 & 1 \\ 0 & 1\end{bmatrix} \tilde{T}:=\frac{T}{\lVert T\rVert} \tilde{T}\begin{bmatrix} 1 \\ 0\end{bmatrix} \tilde{T}^*\begin{bmatrix} 0 \\ 1\end{bmatrix} \begin{bmatrix} 0 & 0 \\ \xi & 0\end{bmatrix} \xi\ge 0 \mathcal{B}(\mathcal{H})",['functional-analysis']
70,Lipschitz constant of L2 reg. logistic regression $\sum_i \log \left(1 + \exp\left\{ -t_i \left(w^T x_i\right)\right\} \right) + \mu \|w \|_2^2$,Lipschitz constant of L2 reg. logistic regression,\sum_i \log \left(1 + \exp\left\{ -t_i \left(w^T x_i\right)\right\} \right) + \mu \|w \|_2^2,"Let the L2 regularized logistic regression function is given by, \begin{align} f(w) &= \frac{1}{N} \sum_i \log \left(1 + \exp\left\{ -t_i \left(w^T x_i\right)\right\} \right) + \mu \|w \|_2^2 \ = \frac{1}{N} \sum_i f_i(w), \end{align} where $t_i \in \mathbb{R}$ , $w, x_i \in \mathbb{R}^n$ , $\mu  \in \mathbb{R}$ , and $f_i(w) := \log \left(1 + \exp\left\{ -t_i \left(w^T x_i\right)\right\} \right) + \mu \|w \|_2^2$ . Questions : How would/could I find the (small) Lipschitz constant $M$ and $\nu$ -strongly  parameter of $f(w)$ ? How can I find the (small) Lipschitz constant $L$ of $\nabla f(w)$ and $L_i$ of $\nabla f_i(w)$ ? I am sorry if this question has already been asked or it is trivial to compute (analytically?).","Let the L2 regularized logistic regression function is given by, where , , , and . Questions : How would/could I find the (small) Lipschitz constant and -strongly  parameter of ? How can I find the (small) Lipschitz constant of and of ? I am sorry if this question has already been asked or it is trivial to compute (analytically?).","\begin{align}
f(w) &= \frac{1}{N} \sum_i \log \left(1 + \exp\left\{ -t_i \left(w^T x_i\right)\right\} \right) + \mu \|w \|_2^2 \ = \frac{1}{N} \sum_i f_i(w),
\end{align} t_i \in \mathbb{R} w, x_i \in \mathbb{R}^n \mu  \in \mathbb{R} f_i(w) := \log \left(1 + \exp\left\{ -t_i \left(w^T x_i\right)\right\} \right) + \mu \|w \|_2^2 M \nu f(w) L \nabla f(w) L_i \nabla f_i(w)","['real-analysis', 'functional-analysis', 'convex-analysis', 'lipschitz-functions']"
71,"Showing $\min\limits_{j=1,\dots,n}|\lambda-\lambda_j|\le ||C||_p||C^{-1}||_p||B||_p$",Showing,"\min\limits_{j=1,\dots,n}|\lambda-\lambda_j|\le ||C||_p||C^{-1}||_p||B||_p","Let $A$ be a diagonalizable $n\times n$ matrix with eigenvalues $\lambda_1,\dots, \lambda_n$ , $B$ an $n\times n$ matrix, and $\lambda$ an eigenvalue of $A+B$ . Show that $$\min\limits_{j=1,\dots,n}|\lambda-\lambda_j|\le ||C||_p||C^{-1}||_p||B||_p$$ where $C$ is a nonsingular matrix such that $C^{-1}AC$ is diagonal and $p=1,2,\infty$ . I'm having difficulty figuring out where to start. If given some guidance I'm sure I can easily get the rest. I know that under the assumption $A$ is diagonalizable gives $C^{-1}AC=diag(\lambda_1,\dots,\lambda_n)$ , but I am failing to see how I will use the other assumptions. Any input would be greatly appreciated!","Let be a diagonalizable matrix with eigenvalues , an matrix, and an eigenvalue of . Show that where is a nonsingular matrix such that is diagonal and . I'm having difficulty figuring out where to start. If given some guidance I'm sure I can easily get the rest. I know that under the assumption is diagonalizable gives , but I am failing to see how I will use the other assumptions. Any input would be greatly appreciated!","A n\times n \lambda_1,\dots, \lambda_n B n\times n \lambda A+B \min\limits_{j=1,\dots,n}|\lambda-\lambda_j|\le ||C||_p||C^{-1}||_p||B||_p C C^{-1}AC p=1,2,\infty A C^{-1}AC=diag(\lambda_1,\dots,\lambda_n)","['functional-analysis', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
72,Composition with Lipschitz map is Lipschitz on Sobolev spaces,Composition with Lipschitz map is Lipschitz on Sobolev spaces,,"Suppose that $F: \mathbb{R}^d \rightarrow \mathbb{R}^d$ is Lipschitz with some constant $L$ and that $F(0)=0$ . Then it is clear that $F$ defines a Lipschitz continuous map $L^2(\mathbb{R}^d) \rightarrow L^2(\mathbb{R}^d)$ by $u \mapsto F(u)$ with the same Lipschitz constant $L$ . We can extend this to Sobolev spaces $H^k(\mathbb{R}^d)$ when $k \in \mathbb{N}$ , by requiring in addition that $F \in C^k$ with the first $k$ derivatives of $F$ Lipschitz and that the first $k$ derivatives vanish at $0$ . For instance, if $k=1$ , we can compute \begin{align} \| \partial_i(F(u)-F(\tilde{u})) \|_{L^2(\mathbb{R}^d)} &= \| \nabla F(u) \partial_i u - \nabla F(\tilde{u}) \partial_i \tilde{u} \|_{L^2} \\ &\leq \| \nabla F(u) \|_{L^2} \|\partial_i u - \partial_i \tilde{u} \|_{L^2} +  \|\partial_i \tilde{u} \|_{L^2} \|\nabla F(u) - \nabla F(\tilde{u}) \|_{L^2} \\ &\leq C \left( \|u\|_{L^2} \|u-\tilde{u} \|_{H^1}+\| \tilde{u} \|_{H^1} \| u- \tilde{u}\|_{L^2} \right) \\ &\leq C (\|u\|_{H^1} + \| \tilde{u} \|_{H^1}) \| u - \tilde{u} \|_{H^1} \end{align} where we have used that $\nabla F$ is again a Lipschitz map (and I denoted all constants by $C$ ). Thus we obtain that the map $u \mapsto F(u)$ is (at least) locally Lipschitz in $H^1$ . Similarly of course for $H^k$ , when $k \in \mathbb{N}$ . Now here is my question: (how) is it possible to extend this result to Sobolev spaces $H^s$ with real index $s \in \mathbb{R}$ ? Is the map $u \mapsto F(u)$ well-defined and locally Lipschitz continuous on the space $H^s(\mathbb{R}^d)$ ? Is there some interpolation argument that easily accomplishes this? Finally, did I miss something, and is the map perhaps even globally Lipschitz?","Suppose that is Lipschitz with some constant and that . Then it is clear that defines a Lipschitz continuous map by with the same Lipschitz constant . We can extend this to Sobolev spaces when , by requiring in addition that with the first derivatives of Lipschitz and that the first derivatives vanish at . For instance, if , we can compute where we have used that is again a Lipschitz map (and I denoted all constants by ). Thus we obtain that the map is (at least) locally Lipschitz in . Similarly of course for , when . Now here is my question: (how) is it possible to extend this result to Sobolev spaces with real index ? Is the map well-defined and locally Lipschitz continuous on the space ? Is there some interpolation argument that easily accomplishes this? Finally, did I miss something, and is the map perhaps even globally Lipschitz?","F: \mathbb{R}^d \rightarrow \mathbb{R}^d L F(0)=0 F L^2(\mathbb{R}^d) \rightarrow L^2(\mathbb{R}^d) u \mapsto F(u) L H^k(\mathbb{R}^d) k \in \mathbb{N} F \in C^k k F k 0 k=1 \begin{align}
\| \partial_i(F(u)-F(\tilde{u})) \|_{L^2(\mathbb{R}^d)} &= \| \nabla F(u) \partial_i u - \nabla F(\tilde{u}) \partial_i \tilde{u} \|_{L^2} \\
&\leq \| \nabla F(u) \|_{L^2} \|\partial_i u - \partial_i \tilde{u} \|_{L^2} +  \|\partial_i \tilde{u} \|_{L^2} \|\nabla F(u) - \nabla F(\tilde{u}) \|_{L^2} \\
&\leq C \left( \|u\|_{L^2} \|u-\tilde{u} \|_{H^1}+\| \tilde{u} \|_{H^1} \| u- \tilde{u}\|_{L^2} \right) \\
&\leq C (\|u\|_{H^1} + \| \tilde{u} \|_{H^1}) \| u - \tilde{u} \|_{H^1}
\end{align} \nabla F C u \mapsto F(u) H^1 H^k k \in \mathbb{N} H^s s \in \mathbb{R} u \mapsto F(u) H^s(\mathbb{R}^d)","['functional-analysis', 'analysis', 'sobolev-spaces', 'lipschitz-functions', 'fractional-sobolev-spaces']"
73,Bounded operator to a space with two different norms,Bounded operator to a space with two different norms,,"$X$ is a Banach space and $Y$ is a normed linear space. $(Y,\lVert\cdot\rVert_1)$ is not complete and $(Y,\lVert\cdot\rVert_2)$ is complete, while $\lVert\cdot\rVert_2\ge\lVert\cdot\rVert_1$ . Let $T: X\to( Y,\lVert\cdot\rVert_1)$ be a bounded linear operator. Prove that $T: X\to( Y,\lVert\cdot\rVert_2)$ is also a bounded linear operator. For example, $Y$ can be $C([0,1])$ . $\lVert\cdot\rVert_1$ is $L_1$ norm and $\lVert\cdot\rVert_2$ is sup norm.","is a Banach space and is a normed linear space. is not complete and is complete, while . Let be a bounded linear operator. Prove that is also a bounded linear operator. For example, can be . is norm and is sup norm.","X Y (Y,\lVert\cdot\rVert_1) (Y,\lVert\cdot\rVert_2) \lVert\cdot\rVert_2\ge\lVert\cdot\rVert_1 T: X\to( Y,\lVert\cdot\rVert_1) T: X\to( Y,\lVert\cdot\rVert_2) Y C([0,1]) \lVert\cdot\rVert_1 L_1 \lVert\cdot\rVert_2",['functional-analysis']
74,Convergence of (unbounded) self-adjoint operators,Convergence of (unbounded) self-adjoint operators,,"I'm learning about the dynamical convergence (i.e, convergence of the unitary group associated with each operator) and resolvent convergence of (unbounded) self-adjoint densely defined operators. I can understand the proof of the initial results, but I can't notice what is the motivation for this. One motivation that I have seen was the following: if $(T_{n})_{n\in\mathbb{N}}$ and $T$ are (unbounded) self-adjoint densely defined operators in a Hilbert space $H$ , then the intersection of the domains could be only the null vector, thus the convergence $T_{n}v\to Tv$ will be true for all, possible, $v$ , since $v$ is just allowed to be $0$ , the convergence is gonna be true. I'm theoretically satisfied with this intuition, but I can't find an example to show that possibility, i.e, a sequence like that with $dom(T)\cap\biggl(\bigcap_{n\in\mathbb{N}}dom(T_{n})\biggr) = \left \{0\right \}$ Just to add some reference on my question, it can be found on ""César R. de Oliveira, Intermediate Spectral Theory and Quantum dynamics, Chapter 10"".","I'm learning about the dynamical convergence (i.e, convergence of the unitary group associated with each operator) and resolvent convergence of (unbounded) self-adjoint densely defined operators. I can understand the proof of the initial results, but I can't notice what is the motivation for this. One motivation that I have seen was the following: if and are (unbounded) self-adjoint densely defined operators in a Hilbert space , then the intersection of the domains could be only the null vector, thus the convergence will be true for all, possible, , since is just allowed to be , the convergence is gonna be true. I'm theoretically satisfied with this intuition, but I can't find an example to show that possibility, i.e, a sequence like that with Just to add some reference on my question, it can be found on ""César R. de Oliveira, Intermediate Spectral Theory and Quantum dynamics, Chapter 10"".",(T_{n})_{n\in\mathbb{N}} T H T_{n}v\to Tv v v 0 dom(T)\cap\biggl(\bigcap_{n\in\mathbb{N}}dom(T_{n})\biggr) = \left \{0\right \},"['functional-analysis', 'hilbert-spaces', 'spectral-theory', 'unbounded-operators', 'self-adjoint-operators']"
75,Characterization of bounded set in test function space?(Rudin),Characterization of bounded set in test function space?(Rudin),,"Let $E$ be a subset of test function space $D(\Omega)$ , then Rudin (functional analysis) in Page 178 says that $E$ is bounded if and only if for any $f\in D'(\Omega)$ , the supremum sup $\{|f(\phi)|:\phi\in E\}$ is finite. How to prove that the latter implies the former?","Let be a subset of test function space , then Rudin (functional analysis) in Page 178 says that is bounded if and only if for any , the supremum sup is finite. How to prove that the latter implies the former?",E D(\Omega) E f\in D'(\Omega) \{|f(\phi)|:\phi\in E\},['functional-analysis']
76,Fractional Hardy inequality,Fractional Hardy inequality,,"From classic literature, I know the following result. Let $\Omega\subset\mathbb{R}^d$ be a bounded open set of class $C^1$ .   Then there exists $C>0$ such that \begin{equation}\label{1} \|\frac{u}{d}  \|_{L^2(\Omega)}\le C\|\nabla u\|_{L^2(\Omega)}\qquad \forall\ u \in  H^1_0(\Omega), \end{equation} where $d(x):=\operatorname{dist}(x,\partial\Omega)$ . I am wondering if a fractional order equivalent of the previous result holds, namely $$ \|\frac{u}{d}  \|_{H^s(\Omega)}\le C\|\nabla u\|_{H^s(\Omega)}\qquad \forall\ u \in  H^1_0(\Omega)\cap H^2(\Omega), $$ for every $0\le s<\frac{1}{2}$ . Does anyone have any ideas or counterexamples? Thanks!","From classic literature, I know the following result. Let be a bounded open set of class .   Then there exists such that where . I am wondering if a fractional order equivalent of the previous result holds, namely for every . Does anyone have any ideas or counterexamples? Thanks!","\Omega\subset\mathbb{R}^d C^1 C>0 \begin{equation}\label{1} \|\frac{u}{d}
 \|_{L^2(\Omega)}\le C\|\nabla u\|_{L^2(\Omega)}\qquad \forall\ u \in
 H^1_0(\Omega), \end{equation} d(x):=\operatorname{dist}(x,\partial\Omega) 
\|\frac{u}{d}
 \|_{H^s(\Omega)}\le C\|\nabla u\|_{H^s(\Omega)}\qquad \forall\ u \in
 H^1_0(\Omega)\cap H^2(\Omega),
 0\le s<\frac{1}{2}","['functional-analysis', 'inequality', 'sobolev-spaces', 'fractional-sobolev-spaces']"
77,Fourier transform and fourth root,Fourier transform and fourth root,,"Given a well-behaved convex function $f(t):\mathbb{R}\to \mathbb{R}$ , its Fourier transform (FT) $\hat{f}(\omega)=\mathcal{F}[f(t)](\omega)$ is positive (and decreasing) proof here . It follows that the fourth square root of the FT $\sqrt[4]{\hat{f}(\omega)}$ is invertible. I was wondering if it is possible to determine the function $g(t)=\mathcal{F}^{-1}\Big[\sqrt[4]{\hat{f}(\omega)}\Big](t)$ directly from $f(t)$ , without making use of the Fourier transform. In other words, knowing $f(t)$ , I would like to determine the function $g(t)$ such that $$\mathcal{F}[g(t)](\omega)=\sqrt[4]{\hat{f}(\omega)}.$$ I had a look at the convolution theorem and at the fractional Fourier transform but I do not see a straightforward application of these tools to solve this problem.","Given a well-behaved convex function , its Fourier transform (FT) is positive (and decreasing) proof here . It follows that the fourth square root of the FT is invertible. I was wondering if it is possible to determine the function directly from , without making use of the Fourier transform. In other words, knowing , I would like to determine the function such that I had a look at the convolution theorem and at the fractional Fourier transform but I do not see a straightforward application of these tools to solve this problem.",f(t):\mathbb{R}\to \mathbb{R} \hat{f}(\omega)=\mathcal{F}[f(t)](\omega) \sqrt[4]{\hat{f}(\omega)} g(t)=\mathcal{F}^{-1}\Big[\sqrt[4]{\hat{f}(\omega)}\Big](t) f(t) f(t) g(t) \mathcal{F}[g(t)](\omega)=\sqrt[4]{\hat{f}(\omega)}.,"['functional-analysis', 'fourier-analysis', 'fourier-transform', 'convolution', 'fractional-iteration']"
78,"Finding an orthonormal basis of $L^2[0,1]$ consisting of elements from $C[0,1]$",Finding an orthonormal basis of  consisting of elements from,"L^2[0,1] C[0,1]","Problem: Let $f \in L^1[0,1]$ but $f\notin L^2[0,1]$ . Is there an orthonormal basis of $L^2[0,1]$ consisting elements {g_n} in $C[0,1]$ such that $\int g_n f = 0$ ? To start with, one can show that $\{g \in L^2[0,1]| \int gf = 0\}$ is dense in $L^2[0,1]$ . It follows that one is able to extract an orthonormal basis out of this subspace. It remains to show further that this basis can be chosen from the subspace of $C[0,1]$ if possible. I wonder whether this is correct and how to finish the last step.","Problem: Let but . Is there an orthonormal basis of consisting elements {g_n} in such that ? To start with, one can show that is dense in . It follows that one is able to extract an orthonormal basis out of this subspace. It remains to show further that this basis can be chosen from the subspace of if possible. I wonder whether this is correct and how to finish the last step.","f \in L^1[0,1] f\notin L^2[0,1] L^2[0,1] C[0,1] \int g_n f = 0 \{g \in L^2[0,1]| \int gf = 0\} L^2[0,1] C[0,1]","['real-analysis', 'functional-analysis']"
79,Direct proof of Closed Graph Theorem (or Bounded Inverse Theorem) from Uniform Boundedness Principle,Direct proof of Closed Graph Theorem (or Bounded Inverse Theorem) from Uniform Boundedness Principle,,"I'm looking for a direct proof of the Closed Graph Theorem (or Bounded Inverse Theorem) from the Uniform Boundedness Principle. But I can't find one in the literature. I'm hoping there's a nice proof of the Closed Graph Theorem of the following form. Let $T:X \to Y$ be a closed linear map between Banach spaces.  Define a family $\{T_{\alpha}\}_{\alpha \in A}$ of bounded linear maps from $X$ to $Y$ (or from $X$ to another normed space $Z$ ) such that $\sup_{\alpha \in A} \| T_{\alpha}(x) \| < \infty$ for all $x \in X$ and $\| T \| \leq \sup_{\alpha \in A} \| T_{\alpha}\|$ . Conclude using Uniform Boundedness Principle. Of course, coming up with the family $\{T_{\alpha}\}_{\alpha \in A}$ is the hard part. Similarly/Alternatively, I'd be very happy to see a proof of the Bounded Inverse Theorem of the following form. Let $T:X \to Y$ be a bounded linear bijection between Banach spaces.  Define a family $\{S_{\alpha}\}_{\alpha \in A}$ of bounded linear maps from $Y$ to $X$ such that $\sup_{\alpha \in A} \| S_{\alpha}(y) \| < \infty$ for all $y \in Y$ and $\| T^{-1} \| \leq \sup_{\alpha \in A} \| S_{\alpha}\|$ . Conclude using Uniform Boundedness Principle. I was inspired by this proof of the Uniform Boundedness Principle from the Closed Graph Theorem: https://math.stackexchange.com/a/1473367/570438 It looks at the map $\Phi(x) = (T_{\alpha}(x))_{\alpha \in A}$ , which maps $X$ to the space of bounded maps in $Y^A$ . A similar question was asked here before, but without a satisfactory answer: Does the Closed Graph Theorem follow from Banach-Steinhaus? Theorem 27.26-27.31 of Schechter's Handbook of Analysis and its Foundations gives a indirect argument. Relatedly, the argument is adapted to give a direct proof of the Open Mapping Theorem from the Uniform Boundedness Principle here: https://mathoverflow.net/questions/190587/is-there-a-simple-direct-proof-of-the-open-mapping-theorem-from-the-uniform-boun I am aware of the standard arguments for the implications Open Mapping Theorem $\Leftrightarrow$ Bounded Inverse Theorem $\Leftrightarrow$ Closed Graph Theorem. Edit: Cross-posted at MO: https://mathoverflow.net/questions/311275/direct-proof-of-closed-graph-theorem-or-bounded-inverse-theorem-from-uniform-b","I'm looking for a direct proof of the Closed Graph Theorem (or Bounded Inverse Theorem) from the Uniform Boundedness Principle. But I can't find one in the literature. I'm hoping there's a nice proof of the Closed Graph Theorem of the following form. Let be a closed linear map between Banach spaces.  Define a family of bounded linear maps from to (or from to another normed space ) such that for all and . Conclude using Uniform Boundedness Principle. Of course, coming up with the family is the hard part. Similarly/Alternatively, I'd be very happy to see a proof of the Bounded Inverse Theorem of the following form. Let be a bounded linear bijection between Banach spaces.  Define a family of bounded linear maps from to such that for all and . Conclude using Uniform Boundedness Principle. I was inspired by this proof of the Uniform Boundedness Principle from the Closed Graph Theorem: https://math.stackexchange.com/a/1473367/570438 It looks at the map , which maps to the space of bounded maps in . A similar question was asked here before, but without a satisfactory answer: Does the Closed Graph Theorem follow from Banach-Steinhaus? Theorem 27.26-27.31 of Schechter's Handbook of Analysis and its Foundations gives a indirect argument. Relatedly, the argument is adapted to give a direct proof of the Open Mapping Theorem from the Uniform Boundedness Principle here: https://mathoverflow.net/questions/190587/is-there-a-simple-direct-proof-of-the-open-mapping-theorem-from-the-uniform-boun I am aware of the standard arguments for the implications Open Mapping Theorem Bounded Inverse Theorem Closed Graph Theorem. Edit: Cross-posted at MO: https://mathoverflow.net/questions/311275/direct-proof-of-closed-graph-theorem-or-bounded-inverse-theorem-from-uniform-b",T:X \to Y \{T_{\alpha}\}_{\alpha \in A} X Y X Z \sup_{\alpha \in A} \| T_{\alpha}(x) \| < \infty x \in X \| T \| \leq \sup_{\alpha \in A} \| T_{\alpha}\| \{T_{\alpha}\}_{\alpha \in A} T:X \to Y \{S_{\alpha}\}_{\alpha \in A} Y X \sup_{\alpha \in A} \| S_{\alpha}(y) \| < \infty y \in Y \| T^{-1} \| \leq \sup_{\alpha \in A} \| S_{\alpha}\| \Phi(x) = (T_{\alpha}(x))_{\alpha \in A} X Y^A \Leftrightarrow \Leftrightarrow,"['functional-analysis', 'analysis']"
80,May I ask why the following operator between Banach spaces is continuous please?,May I ask why the following operator between Banach spaces is continuous please?,,"I am reading the paper ""On the Initial Value Problem of Stochastic Evolution Equations in Hilbert Spaces"" ( Here is the link for electronic version ). But when reading it, I meet a problem and would like to ask for help: Background : 1.The event space is $\Omega$ with sigma algebra and probability measure $P$. 2.$H$ and $K$ are two real Hilberts. 3.$L^2(\Omega,H)$ denotes the collection of strongly measurable, square-integrable, $H$-valued random variables and it is a Banach space with norm: for any element $u$, $\|u(.)\|_{L^2}=(E\|u(.,\omega)\|)^{1/2}$, where the expectation $E$ is defined by $Eu=\int_{\Omega}u(\omega)dP$, $\omega\in\Omega$. 4.$W_{t}$ denotes the cylindrical Wiener process valued in $K$ with covariance operator $Q$. 5.Let $J$ denotes some closed subinterval in $[0,\infty)$, $C(J,L^2(\Omega,H))$ denotes the collection of continuous processes from $J$ to $L^2(\Omega,H)$ such that $sup_{t\in J}E\|u(t)\|^2<\infty$. It is a Banach space with the norm $\|u\|_{C}=(sup_{t \in J}E\|u\|^2)^{1/2}$. 6.$A$ is an operator on $H$ which generates a family of compact operators $(S_{t})_{t>0}$. Problem: Assume $f:J\times H \to L_{HS}(K,H)$ is continuous nonlinear map, where $L_{HS}(K,H)$ denotes the Hilbert-Schmidt operators; the set $\{E\|f(t,u(t))\|^2:t \in J \}$ is bounded for any bounded $E\|u(t) \|^2$. Then consider a subinterval $[t_0,t_0+h]$ of $J$, the map $(Fu)(t) = \int_{to}^tS(t-s)f(s,u(s))dW_s$ is continuous from $C(J,L^2(\Omega,H))$ to itself. (The above map is different from formula (7) in the paper, but the essential should be the same. )  For its proof, it is just pointed out that due to continuity of $f$, map $F$ is continuous, I could not figure out why. Furthermore, if we put randomness on $f$, for instance $f$ is a stochastic process valued in the space of continuous maps between $H$ and $L_{HS}(K,H)$ with continuous sample paths, will the same argument hold please? Thank you very much!","I am reading the paper ""On the Initial Value Problem of Stochastic Evolution Equations in Hilbert Spaces"" ( Here is the link for electronic version ). But when reading it, I meet a problem and would like to ask for help: Background : 1.The event space is $\Omega$ with sigma algebra and probability measure $P$. 2.$H$ and $K$ are two real Hilberts. 3.$L^2(\Omega,H)$ denotes the collection of strongly measurable, square-integrable, $H$-valued random variables and it is a Banach space with norm: for any element $u$, $\|u(.)\|_{L^2}=(E\|u(.,\omega)\|)^{1/2}$, where the expectation $E$ is defined by $Eu=\int_{\Omega}u(\omega)dP$, $\omega\in\Omega$. 4.$W_{t}$ denotes the cylindrical Wiener process valued in $K$ with covariance operator $Q$. 5.Let $J$ denotes some closed subinterval in $[0,\infty)$, $C(J,L^2(\Omega,H))$ denotes the collection of continuous processes from $J$ to $L^2(\Omega,H)$ such that $sup_{t\in J}E\|u(t)\|^2<\infty$. It is a Banach space with the norm $\|u\|_{C}=(sup_{t \in J}E\|u\|^2)^{1/2}$. 6.$A$ is an operator on $H$ which generates a family of compact operators $(S_{t})_{t>0}$. Problem: Assume $f:J\times H \to L_{HS}(K,H)$ is continuous nonlinear map, where $L_{HS}(K,H)$ denotes the Hilbert-Schmidt operators; the set $\{E\|f(t,u(t))\|^2:t \in J \}$ is bounded for any bounded $E\|u(t) \|^2$. Then consider a subinterval $[t_0,t_0+h]$ of $J$, the map $(Fu)(t) = \int_{to}^tS(t-s)f(s,u(s))dW_s$ is continuous from $C(J,L^2(\Omega,H))$ to itself. (The above map is different from formula (7) in the paper, but the essential should be the same. )  For its proof, it is just pointed out that due to continuity of $f$, map $F$ is continuous, I could not figure out why. Furthermore, if we put randomness on $f$, for instance $f$ is a stochastic process valued in the space of continuous maps between $H$ and $L_{HS}(K,H)$ with continuous sample paths, will the same argument hold please? Thank you very much!",,"['functional-analysis', 'stochastic-processes', 'stochastic-pde']"
81,An $n$-norm for the space of continuous functions?,An -norm for the space of continuous functions?,n,"Given a (real) vector space $F$ of dimension $\geq n$, recall that  $\|\cdot,\ldots,\cdot\|:F^{n}\longrightarrow [0,\infty)$ satisfying the following conditions for each $x_{0},x_{1},\ldots,x_{n}\in F$: (1) $\|x_{1},\ldots,x_{n}\|=0$ if, and only if, $x_{1},\ldots,x_{n}$ are linear dependent. (2) $\|x_{1},\ldots,x_{n}\|$ is invariant under permutation. (3)  $\| \lambda x_{1},\ldots,x_{n}\|=|\lambda | \|x_{1},\ldots,x_{n}\|$ for every $\lambda\in\mathbb{R}$. (4) $\|x_{0}+x_{1},\ldots,x_{n}\|\leq \|x_{0},\ldots,x_{n}\|+\|x_{1},\ldots,x_{n}\|$. is said to be an $n$-norm on $F$, and the pair $(F,\|\cdot,\ldots,\cdot\|)$ a $n$-normed space. For instance, if $F:=\mathbb{R}^{d}$, with $d\geq n$, then $\|x_{1},\ldots,x_{n}\|:=\sqrt{ \mathrm{det}\big(\langle x_{i},x_{j} \rangle  \big) }$ is an $n$-norm in $F$. How can we define an $n$-norm in the linear space $F$ of the continuous functions $f:[0,1]\longrightarrow \mathbb{R}$? I am thinking about a mapping of the form: $$\|f_{1},\ldots,f_{n}\|:= \sup\left\{ \left|\, \mathrm{det}\left[ \begin{array}{l} f_{1}(x_{1}) & \cdots &f_{n}(x_{n}) \\ f_{1}(x_{1}) &\cdots &f_{n}(x_{n}) \\ \vdots & \ddots & \vdots \\ f_{1}(x_{1})&\cdots& f_{n}(x_{n})   \end{array}\right] \,\right|:x_{1},\ldots,x_{n} \in [0,1]\right\}$$ which (from the properties of the determinant) it seems that obeys the conditions (2)-(4) of the above definition. I am not sure if the condition (1) holds. What do you think? Do you known some $n$-norm in this space $F$? Many thanks in advance for your comments and suggestions.","Given a (real) vector space $F$ of dimension $\geq n$, recall that  $\|\cdot,\ldots,\cdot\|:F^{n}\longrightarrow [0,\infty)$ satisfying the following conditions for each $x_{0},x_{1},\ldots,x_{n}\in F$: (1) $\|x_{1},\ldots,x_{n}\|=0$ if, and only if, $x_{1},\ldots,x_{n}$ are linear dependent. (2) $\|x_{1},\ldots,x_{n}\|$ is invariant under permutation. (3)  $\| \lambda x_{1},\ldots,x_{n}\|=|\lambda | \|x_{1},\ldots,x_{n}\|$ for every $\lambda\in\mathbb{R}$. (4) $\|x_{0}+x_{1},\ldots,x_{n}\|\leq \|x_{0},\ldots,x_{n}\|+\|x_{1},\ldots,x_{n}\|$. is said to be an $n$-norm on $F$, and the pair $(F,\|\cdot,\ldots,\cdot\|)$ a $n$-normed space. For instance, if $F:=\mathbb{R}^{d}$, with $d\geq n$, then $\|x_{1},\ldots,x_{n}\|:=\sqrt{ \mathrm{det}\big(\langle x_{i},x_{j} \rangle  \big) }$ is an $n$-norm in $F$. How can we define an $n$-norm in the linear space $F$ of the continuous functions $f:[0,1]\longrightarrow \mathbb{R}$? I am thinking about a mapping of the form: $$\|f_{1},\ldots,f_{n}\|:= \sup\left\{ \left|\, \mathrm{det}\left[ \begin{array}{l} f_{1}(x_{1}) & \cdots &f_{n}(x_{n}) \\ f_{1}(x_{1}) &\cdots &f_{n}(x_{n}) \\ \vdots & \ddots & \vdots \\ f_{1}(x_{1})&\cdots& f_{n}(x_{n})   \end{array}\right] \,\right|:x_{1},\ldots,x_{n} \in [0,1]\right\}$$ which (from the properties of the determinant) it seems that obeys the conditions (2)-(4) of the above definition. I am not sure if the condition (1) holds. What do you think? Do you known some $n$-norm in this space $F$? Many thanks in advance for your comments and suggestions.",,"['real-analysis', 'functional-analysis']"
82,When does the continuous dual of the weak operator topology consist only of finite linear combinations of evaluations?,When does the continuous dual of the weak operator topology consist only of finite linear combinations of evaluations?,,"Let $V$ and $W$ be topological vector spaces, say, over $\mathbb C$. Let $L(V, W)$ be the vector space of continuous linear maps equipped with the weak operator topology , which is the initial topology for the maps $$\begin{align*} \phi_{v, \mu} : L(V, W) &\to \mathbb C \\ T & \mapsto \mu(Tv) \end{align*}$$ for $v \in V$ and $\mu \in W^*$ (the continuous dual of $W$). Equivalently, it is the topology induced by the seminorms $p_{v, \mu} = |\phi_{v, \mu}|$. In particular, $L(V, W)$ is locally convex, and Hausdorff iff $W^*$ separates points of $W$. By construction, the $\phi_{v, \mu} : L(V, W) \to \mathbb C$ are continuous linear maps, and so are finite linear combinations of the $\phi_{v, \mu}$. Under which conditions do the $\phi_{v, \mu}$ span the continuous dual $L(V, W)^*$? I know that this is the case when $V = W$ is a Hilbert space. (Takesaki, Theory of Operator Algebras I , Chapter II Theorem 2.6.) In these notes , Paul Garrett suggests that this is more generally true for certain topological vector spaces: the proof of the second corollary on page 3 uses that this is true when $V$ is an LF-space and $W$ quasi-complete and locally convex.) It does not give a precise statement, which makes we wonder if this is true for general topological vector spaces: Is there any good reference which addresses the case of topological vector spaces (not just normed spaces)?","Let $V$ and $W$ be topological vector spaces, say, over $\mathbb C$. Let $L(V, W)$ be the vector space of continuous linear maps equipped with the weak operator topology , which is the initial topology for the maps $$\begin{align*} \phi_{v, \mu} : L(V, W) &\to \mathbb C \\ T & \mapsto \mu(Tv) \end{align*}$$ for $v \in V$ and $\mu \in W^*$ (the continuous dual of $W$). Equivalently, it is the topology induced by the seminorms $p_{v, \mu} = |\phi_{v, \mu}|$. In particular, $L(V, W)$ is locally convex, and Hausdorff iff $W^*$ separates points of $W$. By construction, the $\phi_{v, \mu} : L(V, W) \to \mathbb C$ are continuous linear maps, and so are finite linear combinations of the $\phi_{v, \mu}$. Under which conditions do the $\phi_{v, \mu}$ span the continuous dual $L(V, W)^*$? I know that this is the case when $V = W$ is a Hilbert space. (Takesaki, Theory of Operator Algebras I , Chapter II Theorem 2.6.) In these notes , Paul Garrett suggests that this is more generally true for certain topological vector spaces: the proof of the second corollary on page 3 uses that this is true when $V$ is an LF-space and $W$ quasi-complete and locally convex.) It does not give a precise statement, which makes we wonder if this is true for general topological vector spaces: Is there any good reference which addresses the case of topological vector spaces (not just normed spaces)?",,"['functional-analysis', 'operator-theory', 'operator-algebras', 'topological-vector-spaces', 'weak-topology']"
83,"If E* is separable, so is E. Is this proof correct?","If E* is separable, so is E. Is this proof correct?",,"This is my first time posting here. I already saw a post regarding this result, but I wanted to check if this specific proof is correct or not; as the proof presented in the book is different. Also, I'm not a native English speaker, so excuse me if there's any grammar mistakes. Proposition : Let $(E,||.||)$ be a normed space (over $\mathbb{R}$ or $\mathbb{C}$) such that $E^{*}$  is separable. Then $E$  is separable as well. Proof: Let $D = \{\phi_{n}\}_{n \in \mathbb{N}}$ be a dense in $E^{*}$. Define $x_{n} \in E$ such that $||x_{n}||=1$ and $||\phi_{n}|| < |\phi_{n}(x_{n})|+\frac{1}{n}$, which exists by definition of $||\phi_{n}||$. I claim that $\langle x_{1},...x_{k}...\rangle$ is a dense in $E$. If I could prove that, due to a previous result, this would imply that $E$ is separable. We'll define $S=\overline{\langle x_{1},...x_{k}...\rangle}$. We need to prove that $S=E$. Let's assume otherwise. Then there would exist $x \in E-S$, and given that $S$ is a subspace of $E$ we can assume $||x||=1$. By a corollary of the Hahn-Banach theorem (since $S$ is closed), we can find a continious functional $\phi \in E^{*}$ such that $\phi(S)=\{0\}$ and $\phi(x)=1$. Now, we know that there exists a sequence in $D$ such that $\phi_{n_{k}} \rightarrow \phi$, but if that's the case: $$||\phi_{n_{k}}||-\frac{1}{n_{k}}<|\phi_{n_{k}}(x_{n_{k}})|=|\phi_{n_{k}}(x_{n_{k}})-0|=|\phi_{n_{k}}(x_{n_{k}})-\phi(x_{n_{k}})|\leq ||\phi_{n_{k}}-\phi||,$$ which implies that $||\phi_{n_{k}}|| \rightarrow 0$ as $k \rightarrow \infty$. For $k$ large enough, $|\phi_{n_{k}}(y)| < \frac{1}{2}$ for all $y$ with $||y||=1$, in particular, $|\phi_{n_{k}}(x)| < \frac{1}{2}$. Then, $$||\phi-\phi_{n_{k}}|| \geq |\phi(x)-\phi_{n_{k}}(x)| = |1-\phi_{n_{k}}(x)| >\frac{1}{2}$$ But $||\phi-\phi_{n_{k}}|| \rightarrow 0$? We reached our contradiction and conclude that $S=E$. Thank you in advance.","This is my first time posting here. I already saw a post regarding this result, but I wanted to check if this specific proof is correct or not; as the proof presented in the book is different. Also, I'm not a native English speaker, so excuse me if there's any grammar mistakes. Proposition : Let $(E,||.||)$ be a normed space (over $\mathbb{R}$ or $\mathbb{C}$) such that $E^{*}$  is separable. Then $E$  is separable as well. Proof: Let $D = \{\phi_{n}\}_{n \in \mathbb{N}}$ be a dense in $E^{*}$. Define $x_{n} \in E$ such that $||x_{n}||=1$ and $||\phi_{n}|| < |\phi_{n}(x_{n})|+\frac{1}{n}$, which exists by definition of $||\phi_{n}||$. I claim that $\langle x_{1},...x_{k}...\rangle$ is a dense in $E$. If I could prove that, due to a previous result, this would imply that $E$ is separable. We'll define $S=\overline{\langle x_{1},...x_{k}...\rangle}$. We need to prove that $S=E$. Let's assume otherwise. Then there would exist $x \in E-S$, and given that $S$ is a subspace of $E$ we can assume $||x||=1$. By a corollary of the Hahn-Banach theorem (since $S$ is closed), we can find a continious functional $\phi \in E^{*}$ such that $\phi(S)=\{0\}$ and $\phi(x)=1$. Now, we know that there exists a sequence in $D$ such that $\phi_{n_{k}} \rightarrow \phi$, but if that's the case: $$||\phi_{n_{k}}||-\frac{1}{n_{k}}<|\phi_{n_{k}}(x_{n_{k}})|=|\phi_{n_{k}}(x_{n_{k}})-0|=|\phi_{n_{k}}(x_{n_{k}})-\phi(x_{n_{k}})|\leq ||\phi_{n_{k}}-\phi||,$$ which implies that $||\phi_{n_{k}}|| \rightarrow 0$ as $k \rightarrow \infty$. For $k$ large enough, $|\phi_{n_{k}}(y)| < \frac{1}{2}$ for all $y$ with $||y||=1$, in particular, $|\phi_{n_{k}}(x)| < \frac{1}{2}$. Then, $$||\phi-\phi_{n_{k}}|| \geq |\phi(x)-\phi_{n_{k}}(x)| = |1-\phi_{n_{k}}(x)| >\frac{1}{2}$$ But $||\phi-\phi_{n_{k}}|| \rightarrow 0$? We reached our contradiction and conclude that $S=E$. Thank you in advance.",,"['functional-analysis', 'normed-spaces', 'dual-spaces']"
84,Proving Uniqueness of CCR Algebras,Proving Uniqueness of CCR Algebras,,"I am currently trying to learn about CCR algebras (canonical commutation relations) and I am experiencing some confusion with the proof that the CCR algebra of a (non-degenerate, real) symplectic vector space is unique. I have been reading from chapter 6 of the following, in which the relevant result is Theorem 3: https://www.math.uni-potsdam.de/fileadmin/user_upload/Prof-Geometrie/Dokumente/Publikationen/qft-alg.pdf I shall summarise the proof, and insert numbers where I have questions. Any terminology should be defined in the document linked above. Proof: Given two CCR-representations ($A_1$, $W_1$) and ($A_2$, $W_2$) of the symplectic vector space ($V$, $\omega$), we must show that the $^*$-isomorphism $\pi:\langle W_1(V)\rangle\rightarrow\langle W_2(V)\rangle$ between the $^*$-algebras $\langle W_1(V)\rangle$ and $\langle W_2(V)\rangle$ extends to an isometry between $A_1$ and $A_2$$^{(1)}$. Then the norm $\|x\|=\|\pi(x)\|_2$ is introduced on $A_1$, and it noted that $\|\pi(x)\|_2\leq \|x\|_\text{max}$, where $\|\cdot\|_\text{max}$ is defined in Lemma 9 by $$\|x\|_\text{max}=\sup\{\|x\|\text{; }\|\cdot\|\text{ is a C}^*\text{ norm on } \langle W_1(V)\rangle\}.$$ Then it is concluded that $\varphi$ extends to a $^*$-homomorphism $\overline{\langle W_1(V)\rangle}^\text{max}\rightarrow A_2$$^{(2)}$. Lemma 10 is applied to conclude that this extension is injective, and then it follows that the extension is isometric$^{(3)}$. This is pretty much where the proof finishes, barring one note about the case when $A_1=A_2$. I'll now list my questions. (1) Checking this will be sufficient because we could then do the same for the inverse $\varphi^{-1}$ to get the required map between $A_1$ and $A_2$? (2) The bound $\|\pi(x)\|_2\leq \|x\|_\text{max}$ shows that $\pi$ is continuous, so $\pi$ extends to $\overline{\langle W_1(V)\rangle}^\text{max}$ by continuity and density of $\langle W_1(V)\rangle$? (3) I really don't understand why this is sufficient. To me it seems that finishing at this point requires the completion $\overline{\langle W_1(V)\rangle}^\text{max}$ to coincide with $A_1$, but I haven't really gotten anywhere with showing this on my own. Any help/suggestions/slaps around the face because it's obvious will be greatly appreciated.","I am currently trying to learn about CCR algebras (canonical commutation relations) and I am experiencing some confusion with the proof that the CCR algebra of a (non-degenerate, real) symplectic vector space is unique. I have been reading from chapter 6 of the following, in which the relevant result is Theorem 3: https://www.math.uni-potsdam.de/fileadmin/user_upload/Prof-Geometrie/Dokumente/Publikationen/qft-alg.pdf I shall summarise the proof, and insert numbers where I have questions. Any terminology should be defined in the document linked above. Proof: Given two CCR-representations ($A_1$, $W_1$) and ($A_2$, $W_2$) of the symplectic vector space ($V$, $\omega$), we must show that the $^*$-isomorphism $\pi:\langle W_1(V)\rangle\rightarrow\langle W_2(V)\rangle$ between the $^*$-algebras $\langle W_1(V)\rangle$ and $\langle W_2(V)\rangle$ extends to an isometry between $A_1$ and $A_2$$^{(1)}$. Then the norm $\|x\|=\|\pi(x)\|_2$ is introduced on $A_1$, and it noted that $\|\pi(x)\|_2\leq \|x\|_\text{max}$, where $\|\cdot\|_\text{max}$ is defined in Lemma 9 by $$\|x\|_\text{max}=\sup\{\|x\|\text{; }\|\cdot\|\text{ is a C}^*\text{ norm on } \langle W_1(V)\rangle\}.$$ Then it is concluded that $\varphi$ extends to a $^*$-homomorphism $\overline{\langle W_1(V)\rangle}^\text{max}\rightarrow A_2$$^{(2)}$. Lemma 10 is applied to conclude that this extension is injective, and then it follows that the extension is isometric$^{(3)}$. This is pretty much where the proof finishes, barring one note about the case when $A_1=A_2$. I'll now list my questions. (1) Checking this will be sufficient because we could then do the same for the inverse $\varphi^{-1}$ to get the required map between $A_1$ and $A_2$? (2) The bound $\|\pi(x)\|_2\leq \|x\|_\text{max}$ shows that $\pi$ is continuous, so $\pi$ extends to $\overline{\langle W_1(V)\rangle}^\text{max}$ by continuity and density of $\langle W_1(V)\rangle$? (3) I really don't understand why this is sufficient. To me it seems that finishing at this point requires the completion $\overline{\langle W_1(V)\rangle}^\text{max}$ to coincide with $A_1$, but I haven't really gotten anywhere with showing this on my own. Any help/suggestions/slaps around the face because it's obvious will be greatly appreciated.",,"['functional-analysis', 'mathematical-physics', 'operator-algebras']"
85,blocks of a normalized basis dominated by lp,blocks of a normalized basis dominated by lp,,"I would like to know whether the following conjecture is true, possibly with additional assumptions such as unconditionality. Conjecture 1. Suppose $(x_i)_{i=1}^\infty$ is a normalized basis for a Banach space $X$ which is dominated by the unit vectors in $\ell_p$.  Then $X$ contains a seminormalized basic sequence dominated by the canonical unit vectors $((e_k^{(n)})_{k=1}^n)_{n=1}^\infty$ in $(\oplus\ell_2^n)_p$. Discussion. The conjecture is trivial for $p\geq 2$, so if necessary we may assume $1\leq p<2$. By Dvoretsky's Theorem we can find $(u_k^{(n)})_{k=1}^n\subset X$ which is 2-equivalent to $(e_k^{(n)})_{k=1}^n$ for each $n\in\mathbb{N}$, and such that $\text{supp }u_k^{(m)}<\text{supp }u_j^{(n)}$ whenever  $m<n$. Can we prove that $((u_k^{(n)})_{k=1}^n)_{n=1}^\infty$ is dominated by $((e_k^{(n)})_{k=1}^n)_{n=1}^\infty$?","I would like to know whether the following conjecture is true, possibly with additional assumptions such as unconditionality. Conjecture 1. Suppose $(x_i)_{i=1}^\infty$ is a normalized basis for a Banach space $X$ which is dominated by the unit vectors in $\ell_p$.  Then $X$ contains a seminormalized basic sequence dominated by the canonical unit vectors $((e_k^{(n)})_{k=1}^n)_{n=1}^\infty$ in $(\oplus\ell_2^n)_p$. Discussion. The conjecture is trivial for $p\geq 2$, so if necessary we may assume $1\leq p<2$. By Dvoretsky's Theorem we can find $(u_k^{(n)})_{k=1}^n\subset X$ which is 2-equivalent to $(e_k^{(n)})_{k=1}^n$ for each $n\in\mathbb{N}$, and such that $\text{supp }u_k^{(m)}<\text{supp }u_j^{(n)}$ whenever  $m<n$. Can we prove that $((u_k^{(n)})_{k=1}^n)_{n=1}^\infty$ is dominated by $((e_k^{(n)})_{k=1}^n)_{n=1}^\infty$?",,"['functional-analysis', 'banach-spaces', 'lp-spaces']"
86,Hilbert transform on a finite domain,Hilbert transform on a finite domain,,"While doing physics research, I`ve come across the linear operator $$(K\phi)(t_{2}):=\text{P.V}\int_{-1}^{1}\frac{\phi(t_{1})}{(t_{1}-t_{2})}dt_{1}$$ with arguments and values in functions on $[-1,1]$. The functional of my interest is extremized by eigenfunctions of $K$ - do such functions exist? Is there some good literature on this operator? - preferably, accessible by a physicist.","While doing physics research, I`ve come across the linear operator $$(K\phi)(t_{2}):=\text{P.V}\int_{-1}^{1}\frac{\phi(t_{1})}{(t_{1}-t_{2})}dt_{1}$$ with arguments and values in functions on $[-1,1]$. The functional of my interest is extremized by eigenfunctions of $K$ - do such functions exist? Is there some good literature on this operator? - preferably, accessible by a physicist.",,['functional-analysis']
87,An exercise in Banach Space Theory,An exercise in Banach Space Theory,,"I am currently reading a book and  while i was reading I came across an exercise : Prove that a Banach Space $X$ has finite dimension if and only if every linear subspace of $X$ is closed. My solution goes like this : The direction ($\implies$) is immediate since if we have a subspace $F \subseteq X$ then $F$ must be also of finite dimension hence closed. For the opposite direction I assumed that $X$ has an infinite dimension , then we know that there exist a non trivial linear functional $F:X \to \mathbb{R}$ which is not bounded . Then I proved that $\ker F$ must be dense in $X$ and from hypothesis we have that $\ker F$ is also closed, hence $\ker F=X$ which means that $F$ is trivial and we have a contradiction. Is this proof correct ? I am asking because I didn't use the hypothesis that $X$ is Banach throughout the proof. Thanks in advance !","I am currently reading a book and  while i was reading I came across an exercise : Prove that a Banach Space $X$ has finite dimension if and only if every linear subspace of $X$ is closed. My solution goes like this : The direction ($\implies$) is immediate since if we have a subspace $F \subseteq X$ then $F$ must be also of finite dimension hence closed. For the opposite direction I assumed that $X$ has an infinite dimension , then we know that there exist a non trivial linear functional $F:X \to \mathbb{R}$ which is not bounded . Then I proved that $\ker F$ must be dense in $X$ and from hypothesis we have that $\ker F$ is also closed, hence $\ker F=X$ which means that $F$ is trivial and we have a contradiction. Is this proof correct ? I am asking because I didn't use the hypothesis that $X$ is Banach throughout the proof. Thanks in advance !",,"['functional-analysis', 'proof-verification', 'banach-spaces']"
88,sequence of square root of positive operators is convergent,sequence of square root of positive operators is convergent,,"I'm trying to prove the next: a) If $A_{n}\geq 0,$ $A_{n}\rightarrow A$ in norm, then $\sqrt{A_{n}}\rightarrow\sqrt{A}$ in norm, b) Suppose that $A_{n}\rightarrow A$ strongly for a sequence $\{A_{n}\}.$ Then $\sqrt{A_{n}}\rightarrow\sqrt{A}$ strongly. I'm stuck prove this. I've seen the proof of a) using spectral theorem, but I don't familiar with this. I was thinking in a proof more elementary. I was thinking in something of the form  $$||\sqrt{A_{n}}-\sqrt{A}||=||\frac{A_{n}-A}{\sqrt{A_{n}}+\sqrt{A}}||,$$ and then bounding denominator an use the hypotesis of convegence in norm, but I guess this is not correct and useless. Any kind of help is thanked in advanced.","I'm trying to prove the next: a) If $A_{n}\geq 0,$ $A_{n}\rightarrow A$ in norm, then $\sqrt{A_{n}}\rightarrow\sqrt{A}$ in norm, b) Suppose that $A_{n}\rightarrow A$ strongly for a sequence $\{A_{n}\}.$ Then $\sqrt{A_{n}}\rightarrow\sqrt{A}$ strongly. I'm stuck prove this. I've seen the proof of a) using spectral theorem, but I don't familiar with this. I was thinking in a proof more elementary. I was thinking in something of the form  $$||\sqrt{A_{n}}-\sqrt{A}||=||\frac{A_{n}-A}{\sqrt{A_{n}}+\sqrt{A}}||,$$ and then bounding denominator an use the hypotesis of convegence in norm, but I guess this is not correct and useless. Any kind of help is thanked in advanced.",,"['functional-analysis', 'operator-theory']"
89,Show that no two eigenvectors of adjoint of right shift operator are orthogonal,Show that no two eigenvectors of adjoint of right shift operator are orthogonal,,"Let $$T:\ell^2 \to \ell^2$$ is unilateral shift operator, defined by $$T(x_1,x_2,x_3......)=(0,x_1,x_2,x_3.....),$$ then show that $T$ has no eigenvalue. But every $\lambda \in \mathbb{C}$ such that $|\lambda|<1$ is an eigenvalue of $T^{*}$ with multiplicity one. show that none of eigen vectors of $T^{*}$ are orthogonal to each other. I have found $$T^{*}(x_1,x_2,x_3......)=(x_2,x_3,x_4.....)$$ and eigenspace$\ E_{\lambda}$ corresponding to $\lambda$ equal to linear span of vector $$(1,\lambda,\lambda^2,\lambda^3....)$$ but I am stuck in the last part.how to show no two eigenvectors are orthogonal to each other. Any hint please?","Let $$T:\ell^2 \to \ell^2$$ is unilateral shift operator, defined by $$T(x_1,x_2,x_3......)=(0,x_1,x_2,x_3.....),$$ then show that $T$ has no eigenvalue. But every $\lambda \in \mathbb{C}$ such that $|\lambda|<1$ is an eigenvalue of $T^{*}$ with multiplicity one. show that none of eigen vectors of $T^{*}$ are orthogonal to each other. I have found $$T^{*}(x_1,x_2,x_3......)=(x_2,x_3,x_4.....)$$ and eigenspace$\ E_{\lambda}$ corresponding to $\lambda$ equal to linear span of vector $$(1,\lambda,\lambda^2,\lambda^3....)$$ but I am stuck in the last part.how to show no two eigenvectors are orthogonal to each other. Any hint please?",,"['functional-analysis', 'operator-theory', 'hilbert-spaces', 'banach-spaces']"
90,Regularity of a function from asymptotics of its Fourier transform?,Regularity of a function from asymptotics of its Fourier transform?,,"I know that if $f(t) \in L^1(\mathbb{R}) \cap C^k(\mathbb{R})$ then we must have $\hat{f}(s)=o(s^{-k})$ for the Fourier transform. Is there some sort of converse to this statement? Let $f \in L^1(\mathbb{R})$ be such that  $\hat{f}=o(s^{-k})$, What can we conclude about the regularity of $f$? I'm willing to replace $o(s^{-k})$ with $o(s^{-k-\epsilon})$ for some small $\epsilon > 0$ if that makes a big difference.","I know that if $f(t) \in L^1(\mathbb{R}) \cap C^k(\mathbb{R})$ then we must have $\hat{f}(s)=o(s^{-k})$ for the Fourier transform. Is there some sort of converse to this statement? Let $f \in L^1(\mathbb{R})$ be such that  $\hat{f}=o(s^{-k})$, What can we conclude about the regularity of $f$? I'm willing to replace $o(s^{-k})$ with $o(s^{-k-\epsilon})$ for some small $\epsilon > 0$ if that makes a big difference.",,"['functional-analysis', 'fourier-analysis', 'asymptotics', 'fourier-transform']"
91,Are the canonical representatives of the Hilbert space $L^2$ basis-dependent?,Are the canonical representatives of the Hilbert space  basis-dependent?,L^2,"The space $\mathcal{L}^p(\mathbb{R}^n)$ of functions $f$ such that $\int |f(x)|^p\, d^nx$ converges is only a seminormed rather than a normed vector space, because any function $f$ whose support has Lebesgue measure zero has $||f||_p := \int |f(x)|^p\, d^nx = 0$, even if $f$ is not identically zero. In order to turn it into a normed vector space $L^p(\mathbb{R}^n)$ , we need to mod out by the kernel of the $p$-norm, which is the set of functions whose support has Lebesgue measure zero (or equivalently, we need to identify functions that agree almost everywhere). This raises the natural question of whether this quotient space $L^p(\mathbb{R}^n)$ has a natural section ; that is, whether for every equivalence class $[f] \in L^p(\mathbb{R}^n)$ there is a natural canonical representative square-integrable function $f \in \mathcal{L}^p(\mathbb{R}^n)$. Is there generally a natural section of $\mathcal{L}^p(\mathbb{R}^n)$ space? (I'm using the word ""natural"" loosely, not in any kind of mathematically precise sense.) For the Hilbert space $L^2(\mathbb{R}^n)$ with a given orthonormal basis $\{\phi_n(x)\}$, there does seem to be a natural canonical representative $f_c \in [f]$, given by the generalized Fourier series for $[f]$: $$f_c(x) := \sum_{n=0}^\infty \langle f, \phi_n \rangle \phi_n, \qquad \qquad \langle f, \phi_n \rangle := \int_{\mathbb{R}^n} f(x)\, \phi_n(x)\, d^nx.$$ Does this choice of section depend on the choice of basis $\{ \phi_n \}$? (Obviously it would be ""nicer"" if it didn't.)","The space $\mathcal{L}^p(\mathbb{R}^n)$ of functions $f$ such that $\int |f(x)|^p\, d^nx$ converges is only a seminormed rather than a normed vector space, because any function $f$ whose support has Lebesgue measure zero has $||f||_p := \int |f(x)|^p\, d^nx = 0$, even if $f$ is not identically zero. In order to turn it into a normed vector space $L^p(\mathbb{R}^n)$ , we need to mod out by the kernel of the $p$-norm, which is the set of functions whose support has Lebesgue measure zero (or equivalently, we need to identify functions that agree almost everywhere). This raises the natural question of whether this quotient space $L^p(\mathbb{R}^n)$ has a natural section ; that is, whether for every equivalence class $[f] \in L^p(\mathbb{R}^n)$ there is a natural canonical representative square-integrable function $f \in \mathcal{L}^p(\mathbb{R}^n)$. Is there generally a natural section of $\mathcal{L}^p(\mathbb{R}^n)$ space? (I'm using the word ""natural"" loosely, not in any kind of mathematically precise sense.) For the Hilbert space $L^2(\mathbb{R}^n)$ with a given orthonormal basis $\{\phi_n(x)\}$, there does seem to be a natural canonical representative $f_c \in [f]$, given by the generalized Fourier series for $[f]$: $$f_c(x) := \sum_{n=0}^\infty \langle f, \phi_n \rangle \phi_n, \qquad \qquad \langle f, \phi_n \rangle := \int_{\mathbb{R}^n} f(x)\, \phi_n(x)\, d^nx.$$ Does this choice of section depend on the choice of basis $\{ \phi_n \}$? (Obviously it would be ""nicer"" if it didn't.)",,"['functional-analysis', 'fourier-analysis', 'hilbert-spaces', 'lebesgue-integral', 'lp-spaces']"
92,Arzelá-Ascoli Theorem precompact sets,Arzelá-Ascoli Theorem precompact sets,,"Is the Arzelá-Ascoli Theorem true in a precompact subset of $\mathbb{R}^n$?. If $S \subset \mathbb{R}^n$ is precompact and we have a sequence $(f_n)$ of functions in $C(S)$ (Space of bounded and continuous functions $f: S \to \mathbb{R}$) which is equicontinuous and uniformly bounded, is there a subsequence of $(f_n)$ uniformly convergent? I think that if $S=(a,b)$, with $a,b \in \mathbb{R}$, we can extend continuously each function $f_n$ to $[a,b]$ and apply Arzelá-Ascoli Theorem in $C([a,b])$. Thanks in advance.","Is the Arzelá-Ascoli Theorem true in a precompact subset of $\mathbb{R}^n$?. If $S \subset \mathbb{R}^n$ is precompact and we have a sequence $(f_n)$ of functions in $C(S)$ (Space of bounded and continuous functions $f: S \to \mathbb{R}$) which is equicontinuous and uniformly bounded, is there a subsequence of $(f_n)$ uniformly convergent? I think that if $S=(a,b)$, with $a,b \in \mathbb{R}$, we can extend continuously each function $f_n$ to $[a,b]$ and apply Arzelá-Ascoli Theorem in $C([a,b])$. Thanks in advance.",,"['real-analysis', 'functional-analysis', 'sequence-of-function', 'arzela-ascoli']"
93,Weighted Hardy-Littlewood-Sobolev inequality in one dimesion,Weighted Hardy-Littlewood-Sobolev inequality in one dimesion,,"Weighted Hardy-Littlewood-Sobolev inequality in one dimesion states: Let $1 < p, q < \infty, 0 < t < 1, a, b \in \mathbb{R}$ such that: $$ -t \leq a + b \leq 0, \frac{1}{p} < a + 1, \frac{1}{q} < b + 1, \frac{1}{p} + \frac{1}{q} = a + b + t + 1 $$ Then there exists $C$ (depending on $a,b,p,q,t$ ) such that: $$ \int_{\mathbb{R}}\int_{\mathbb{R}} \frac{|x|^{a}|f(x)||y|^{b}|g(y)|}{|x-y|^{1-t} }dxdy \leq C\left\| f\right\| _{L^p(\mathbb{R})}\left\| g\right\| _{L^q(\mathbb{R})}.$$ The proof can probably be done by applying Marcinkiewicz interpolation. This is similar to the unweighted case, but the kernel is giving me more problems here, so some help is welcome.","Weighted Hardy-Littlewood-Sobolev inequality in one dimesion states: Let such that: Then there exists (depending on ) such that: The proof can probably be done by applying Marcinkiewicz interpolation. This is similar to the unweighted case, but the kernel is giving me more problems here, so some help is welcome.","1 < p, q < \infty, 0 < t < 1, a, b \in \mathbb{R}  -t \leq a + b \leq 0, \frac{1}{p} < a + 1, \frac{1}{q} < b + 1, \frac{1}{p} + \frac{1}{q} = a + b + t + 1  C a,b,p,q,t  \int_{\mathbb{R}}\int_{\mathbb{R}} \frac{|x|^{a}|f(x)||y|^{b}|g(y)|}{|x-y|^{1-t} }dxdy \leq C\left\| f\right\| _{L^p(\mathbb{R})}\left\| g\right\| _{L^q(\mathbb{R})}.","['functional-analysis', 'fourier-analysis', 'harmonic-analysis', 'interpolation']"
94,Multiplication operation of End(X) with strong topology not continuous,Multiplication operation of End(X) with strong topology not continuous,,"Let $(X, ||\ ||)$ be a normed space. $dim X = \infty$. $L(X)$ is a space of continuous (which is equialent to bounded in a normed space) linear operators with strong topology. Strong topology is defined by zero neighborhoods: $U(x, W) = \{A \in L(X) | Ax \in W\}$, where $x \in X, 0 \in W \in T_X, T_X$ is topology on $X$, and these neighborhoods finite intersections. Convergence $A_n \rightarrow A (A_n, A \in L(X))$ is equialent to convergence $A_n x \rightarrow Ax$ over $X$, $\forall x \in X$. Show that multiplication operation of End(X) (сomposition of operators) isn't continuous.","Let $(X, ||\ ||)$ be a normed space. $dim X = \infty$. $L(X)$ is a space of continuous (which is equialent to bounded in a normed space) linear operators with strong topology. Strong topology is defined by zero neighborhoods: $U(x, W) = \{A \in L(X) | Ax \in W\}$, where $x \in X, 0 \in W \in T_X, T_X$ is topology on $X$, and these neighborhoods finite intersections. Convergence $A_n \rightarrow A (A_n, A \in L(X))$ is equialent to convergence $A_n x \rightarrow Ax$ over $X$, $\forall x \in X$. Show that multiplication operation of End(X) (сomposition of operators) isn't continuous.",,"['functional-analysis', 'operator-theory', 'normed-spaces']"
95,An inequality by using general Hölder's inequality,An inequality by using general Hölder's inequality,,"Let $w$ be a weight function and $\frac{1}{p}=\frac{1}{p_1}+\frac{1}{p_2}+\dots+\frac{1}{p_n}$. For suitable functions let $\overrightarrow{f}=(f_1,f_2,\dots,f_n)$ and operator $T$ satisifies the condition  $$ T(\overrightarrow{f})(x)\leq \prod_{i=1}^{n}f_{i}(x).\tag{*} $$ By using (*) and generalization of Hölder's inequality , I get $$ \left(\int_{\mathbb{R}^n}|T(\overrightarrow{f})(x)w(x)|^pdx\right)^{\frac{1}{p}}\leq \left(\int_{\mathbb{R}^n}| \prod_{i=1}^{n}f_{i}(x)w(x)|^pdx\right)^{\frac{1}{p}}\leq \prod_{i=1}^{n}\left(\int_{\mathbb{R}^n}|f_{i}(x)|^{p_i}w(x)^pdx\right)^{\frac{1}{p_i}}, $$ where I apply Hölder's inequality  for the measure $d\mu(x)=w(x)^p$. But the paper which I read now writes this inequality as following $$ \left(\int_{\mathbb{R}^n}|T(\overrightarrow{f})(x)w(x)|^pdx\right)^{\frac{1}{p}}\leq \left(\int_{\mathbb{R}^n}| \prod_{i=1}^{n}f_{i}(x)w(x)|^pdx\right)^{\frac{1}{p}}\leq \prod_{i=1}^{n}\left(\int_{\mathbb{R}^n}|f_{i}(x)w(x)|^{p_i}dx\right)^{\frac{1}{p_i}}. $$ Which one is true?","Let $w$ be a weight function and $\frac{1}{p}=\frac{1}{p_1}+\frac{1}{p_2}+\dots+\frac{1}{p_n}$. For suitable functions let $\overrightarrow{f}=(f_1,f_2,\dots,f_n)$ and operator $T$ satisifies the condition  $$ T(\overrightarrow{f})(x)\leq \prod_{i=1}^{n}f_{i}(x).\tag{*} $$ By using (*) and generalization of Hölder's inequality , I get $$ \left(\int_{\mathbb{R}^n}|T(\overrightarrow{f})(x)w(x)|^pdx\right)^{\frac{1}{p}}\leq \left(\int_{\mathbb{R}^n}| \prod_{i=1}^{n}f_{i}(x)w(x)|^pdx\right)^{\frac{1}{p}}\leq \prod_{i=1}^{n}\left(\int_{\mathbb{R}^n}|f_{i}(x)|^{p_i}w(x)^pdx\right)^{\frac{1}{p_i}}, $$ where I apply Hölder's inequality  for the measure $d\mu(x)=w(x)^p$. But the paper which I read now writes this inequality as following $$ \left(\int_{\mathbb{R}^n}|T(\overrightarrow{f})(x)w(x)|^pdx\right)^{\frac{1}{p}}\leq \left(\int_{\mathbb{R}^n}| \prod_{i=1}^{n}f_{i}(x)w(x)|^pdx\right)^{\frac{1}{p}}\leq \prod_{i=1}^{n}\left(\int_{\mathbb{R}^n}|f_{i}(x)w(x)|^{p_i}dx\right)^{\frac{1}{p_i}}. $$ Which one is true?",,"['real-analysis', 'functional-analysis', 'inequality', 'harmonic-analysis', 'holder-inequality']"
96,Converse of the projection theorem,Converse of the projection theorem,,"I am trying to prove the converse of the projection theorem: If for every $f\in H$ there is a $p\in M$ such that $\|p−f\|=\inf\limits_{v\in M}\|v−f\|$ , then $M$ is closed. Is my proof correct? Let $p_n\in M,n=1,2,\ldots,$ be a sequence converging to $g\in H$ . Then there is a $p\in M$ such that $\|p−g\|=\inf\limits_{v\in M}\|v−g\|$ , so $\|p−g\|\leq \|p_n-g\|\,$ for all $n$ . Now as $n$ goes to infinity the RHS goes to zero, so $g=p\in M$ . Therefore, $M$ contains all its limit points and is closed.","I am trying to prove the converse of the projection theorem: If for every there is a such that , then is closed. Is my proof correct? Let be a sequence converging to . Then there is a such that , so for all . Now as goes to infinity the RHS goes to zero, so . Therefore, contains all its limit points and is closed.","f\in H p\in M \|p−f\|=\inf\limits_{v\in M}\|v−f\| M p_n\in M,n=1,2,\ldots, g\in H p\in M \|p−g\|=\inf\limits_{v\in M}\|v−g\| \|p−g\|\leq \|p_n-g\|\, n n g=p\in M M","['functional-analysis', 'analysis', 'hilbert-spaces']"
97,"Show that if the open ball of normed vector space $E$ is precompact, then $\mathrm{dim}(E)<\infty$.","Show that if the open ball of normed vector space  is precompact, then .",E \mathrm{dim}(E)<\infty,"Show that if the open ball of normed vector space $E$ is precompact, then $\mathrm{dim}(E)<\infty$. Remark: The most common proof in the functional analysis books is the one found in this link . My intention is to carry out another proof using the fact that in all infinite dimensional vector normed space there is a  non-continuous linear form . My attepmt: We suppose that $E$ is infinite dimensional and $f:E\rightarrow \mathbb{K}$ is non-continuous linear form ($\mathbb{K}$ is $\mathbb{R}$ or $\mathbb{C}$), an idea of the proof of the existence of this function can be found in this link . Since $f$ is discontinuous function then for each $n\in \mathbb{N}$ there exists $x_{n}\in E$ such that $\left\|x_{n}\right\|=1$ and $|f(x_{n})|>n$. Let $M_{n}=\mathrm{span}\left\{x_{1},x_{2},\ldots,x_{n}\right\}$, since $M_{n}$ is finite dimensional, then $M_{n}$ is reflexive and $\left.f_{n}\right|_{M_{n}}$ is bounded, let $\alpha_{n}=\left\|\left.f_{n}\right|_{M_{n}}\right\|$. By Hahn-Banach theorem and the fact that $M_{n}$ is reflexive we can choose $y_{n}\in M_{n}$ such that $\left\|y_{n}\right\|=1$ and  $|f(y_{n})|=\alpha_{n}$. Therefore, $\left\{y_{n}\right\}\subseteq \overline{B_{1}(0)}$, If I can prove that $\left\{y_{n}\right\}$ does not have convergent subsequences or that it does not have Cauchy subsequences then the proof will have ended because $\overline{B_{1}(0)}$ would not be compact, which implies that $B_{1}(0)$ would not be precompact, which is a contradiction. The problem: I have not been able to demonstrate the final part of my proof proposal, although I'm not sure it's true either.","Show that if the open ball of normed vector space $E$ is precompact, then $\mathrm{dim}(E)<\infty$. Remark: The most common proof in the functional analysis books is the one found in this link . My intention is to carry out another proof using the fact that in all infinite dimensional vector normed space there is a  non-continuous linear form . My attepmt: We suppose that $E$ is infinite dimensional and $f:E\rightarrow \mathbb{K}$ is non-continuous linear form ($\mathbb{K}$ is $\mathbb{R}$ or $\mathbb{C}$), an idea of the proof of the existence of this function can be found in this link . Since $f$ is discontinuous function then for each $n\in \mathbb{N}$ there exists $x_{n}\in E$ such that $\left\|x_{n}\right\|=1$ and $|f(x_{n})|>n$. Let $M_{n}=\mathrm{span}\left\{x_{1},x_{2},\ldots,x_{n}\right\}$, since $M_{n}$ is finite dimensional, then $M_{n}$ is reflexive and $\left.f_{n}\right|_{M_{n}}$ is bounded, let $\alpha_{n}=\left\|\left.f_{n}\right|_{M_{n}}\right\|$. By Hahn-Banach theorem and the fact that $M_{n}$ is reflexive we can choose $y_{n}\in M_{n}$ such that $\left\|y_{n}\right\|=1$ and  $|f(y_{n})|=\alpha_{n}$. Therefore, $\left\{y_{n}\right\}\subseteq \overline{B_{1}(0)}$, If I can prove that $\left\{y_{n}\right\}$ does not have convergent subsequences or that it does not have Cauchy subsequences then the proof will have ended because $\overline{B_{1}(0)}$ would not be compact, which implies that $B_{1}(0)$ would not be precompact, which is a contradiction. The problem: I have not been able to demonstrate the final part of my proof proposal, although I'm not sure it's true either.",,"['functional-analysis', 'compactness']"
98,Hilbert valued martingales - help with reference,Hilbert valued martingales - help with reference,,"I'm currently studying the theory of SPDEs on the book ""Stochastic equations in infinite dimensions"" by da Prato, Zabczyk. In the book, the theory of stochastic processes with values on a Banach space $E$, and in the particular the notion of $E$-valued brownian motion, is also introduced. When the stochastic integral is constructed, it is defined only with respect to the brownian motion using a common approximation technique (it is firstly defined for elementary processes and then extended to the closure of these processes with respect to a suitable norm; after that it can be further extended by localization). However, when I was first introduced to stochastic analysis, it was done following the book ""Cntinuous martingales and brownian motion"" by Revuz, Yor, in which the (one dimensional) stochastic integral is defined for any continuous, square integrable martingale, and in particular it is shown the key result that $I_t=\int_0^t K_sdM_s$ is the unique process such that $$ \langle I,N\rangle_t = \int_0^t K_s d\langle M,N\rangle_s$$ for any $N$ continuous, square integrable martingale, where $\langle \cdot,\cdot\rangle$ denotes the bracket process, or cross quadratic variation. My question is: can this result be extended to the general setting of $E$-valued martingales? (obviously one needs to define what is the integration with respect to the bracket in this case first) If so, do you know any books in which this is done? I made a few researches online and I found that some books approach stochastic integration in this setting using Doolean measures, which I don't know, so I'd prefer books that avoid using that kind of tool or introduce it softly before using it.","I'm currently studying the theory of SPDEs on the book ""Stochastic equations in infinite dimensions"" by da Prato, Zabczyk. In the book, the theory of stochastic processes with values on a Banach space $E$, and in the particular the notion of $E$-valued brownian motion, is also introduced. When the stochastic integral is constructed, it is defined only with respect to the brownian motion using a common approximation technique (it is firstly defined for elementary processes and then extended to the closure of these processes with respect to a suitable norm; after that it can be further extended by localization). However, when I was first introduced to stochastic analysis, it was done following the book ""Cntinuous martingales and brownian motion"" by Revuz, Yor, in which the (one dimensional) stochastic integral is defined for any continuous, square integrable martingale, and in particular it is shown the key result that $I_t=\int_0^t K_sdM_s$ is the unique process such that $$ \langle I,N\rangle_t = \int_0^t K_s d\langle M,N\rangle_s$$ for any $N$ continuous, square integrable martingale, where $\langle \cdot,\cdot\rangle$ denotes the bracket process, or cross quadratic variation. My question is: can this result be extended to the general setting of $E$-valued martingales? (obviously one needs to define what is the integration with respect to the bracket in this case first) If so, do you know any books in which this is done? I made a few researches online and I found that some books approach stochastic integration in this setting using Doolean measures, which I don't know, so I'd prefer books that avoid using that kind of tool or introduce it softly before using it.",,"['functional-analysis', 'banach-spaces', 'stochastic-integrals', 'stochastic-analysis', 'stochastic-pde']"
99,Maximum value of a function with condition,Maximum value of a function with condition,,"Hello everybody I have a question about this : Let a function $f$ with domain $]0,+\infty[$ and codomain $]0,+\infty[$ and twice differentiable with the following inequality :   $$f'+f''\geq f^2>0$$ Furthermore we now this about $f$ and this condition : 1)$\lim\limits_{x \to \infty}f(x)=0$ 2)$f(x)$ is convex for all $x>0$ or $f'<0$ for all $x>0$ 3)$\lim\limits_{x \to 0}f(x)=\infty$ 4)If we make this substitution $f(x)=ln(g'(e^{-x}))$ so we have (if we apply the inequality) : $$e^{-2x}[\frac{g'''(e^{-x})}{g'(e^{-x})}-\frac{g''(e^{-x})^2}{g'(e^{-x})^2}]\geq ln(g'(e^{-x}))^2>0$$ So there is a link with the Schwarzian derivative 5)We can find $\alpha$ such as we have :  $$\frac{ln(-e^{-\alpha x }+1)}{-\alpha x}\leq f_{\alpha}(x)$$ So now my question is: what's the maximal value of $f(n)$? Second Edit :Geometrically speaking If you take the osculating circle of a function $f(x)$ what's the maximum value that can be taken by the osculating circle of minimum radius ? Finally I give you some examples of functions wich verify the inequality : $$f(x)=\frac{ln(-e^{-\alpha x }+1)}{-\alpha x}$$ for all $\alpha\geq 1$ $$f(x)=ln(|\frac{e^{x^2}-1}{e^{x^2}+1}|)^2$$ $$f(x)=\frac{e^{-x}}{x^2}$$ Edit : Furthermore we know that we have under the initial inequality $f'(x)^2\leq f(x)f''(x)$ for all $x>0$ so this paper could be useful . Third edit : I found a 'maximal' function (maybe there exists an another that's all the question ) wich is : $$M(x)=\frac{3 e^{-x}(x^2+2)}{x^2}+7e^{-2x}+3e^{-3x}$$ So for the moment I can't find better than this : $f(1)\leq M(1)$ If we put the following substitution $f(x)=g''(1-e^{-x})$ we get : $$e^{-2x}g^{(iv)}(1-e^{-x})\geq g''(1-e^{-x})^2$$ So this paper is interesting if we assume that $\lim\limits_{x \to 0}f(x)=l$ where $l$ is a real positive number . We get : $$g^{(iv)}(0)\geq g''(0)^2$$ wich corresponds to $d=1$ in the paper . Thanks a lot .","Hello everybody I have a question about this : Let a function $f$ with domain $]0,+\infty[$ and codomain $]0,+\infty[$ and twice differentiable with the following inequality :   $$f'+f''\geq f^2>0$$ Furthermore we now this about $f$ and this condition : 1)$\lim\limits_{x \to \infty}f(x)=0$ 2)$f(x)$ is convex for all $x>0$ or $f'<0$ for all $x>0$ 3)$\lim\limits_{x \to 0}f(x)=\infty$ 4)If we make this substitution $f(x)=ln(g'(e^{-x}))$ so we have (if we apply the inequality) : $$e^{-2x}[\frac{g'''(e^{-x})}{g'(e^{-x})}-\frac{g''(e^{-x})^2}{g'(e^{-x})^2}]\geq ln(g'(e^{-x}))^2>0$$ So there is a link with the Schwarzian derivative 5)We can find $\alpha$ such as we have :  $$\frac{ln(-e^{-\alpha x }+1)}{-\alpha x}\leq f_{\alpha}(x)$$ So now my question is: what's the maximal value of $f(n)$? Second Edit :Geometrically speaking If you take the osculating circle of a function $f(x)$ what's the maximum value that can be taken by the osculating circle of minimum radius ? Finally I give you some examples of functions wich verify the inequality : $$f(x)=\frac{ln(-e^{-\alpha x }+1)}{-\alpha x}$$ for all $\alpha\geq 1$ $$f(x)=ln(|\frac{e^{x^2}-1}{e^{x^2}+1}|)^2$$ $$f(x)=\frac{e^{-x}}{x^2}$$ Edit : Furthermore we know that we have under the initial inequality $f'(x)^2\leq f(x)f''(x)$ for all $x>0$ so this paper could be useful . Third edit : I found a 'maximal' function (maybe there exists an another that's all the question ) wich is : $$M(x)=\frac{3 e^{-x}(x^2+2)}{x^2}+7e^{-2x}+3e^{-3x}$$ So for the moment I can't find better than this : $f(1)\leq M(1)$ If we put the following substitution $f(x)=g''(1-e^{-x})$ we get : $$e^{-2x}g^{(iv)}(1-e^{-x})\geq g''(1-e^{-x})^2$$ So this paper is interesting if we assume that $\lim\limits_{x \to 0}f(x)=l$ where $l$ is a real positive number . We get : $$g^{(iv)}(0)\geq g''(0)^2$$ wich corresponds to $d=1$ in the paper . Thanks a lot .",,"['real-analysis', 'functional-analysis', 'inequality']"
